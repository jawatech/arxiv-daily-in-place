{"2412.14689": {"publish_time": "2024-12-19", "title": "How to Synthesize Text Data without Model Collapse?", "paper_summary": "Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves data quality and enhances model performance.", "paper_summary_zh": "\u5408\u6210\u8cc7\u6599\u4e2d\u7684\u6a21\u578b\u5d29\u6f70\u8868\u660e\uff0c\u5c0d\u81ea\u6211\u7522\u751f\u7684\u8cc7\u6599\u9032\u884c\u53cd\u8986\u8a13\u7df4\u6703\u5c0e\u81f4\u6548\u80fd\u9010\u6f38\u4e0b\u964d\u3002\u96a8\u8457 AI \u6a21\u578b\u7684\u666e\u53ca\uff0c\u5408\u6210\u8cc7\u6599\u5c07\u5f9e\u6839\u672c\u4e0a\u91cd\u5851\u7db2\u8def\u8cc7\u6599\u751f\u614b\u7cfb\u7d71\u3002\u672a\u4f86\u7684 GPT-$\\{n\\}$ \u6a21\u578b\u5c07\u4e0d\u53ef\u907f\u514d\u5730\u8a13\u7df4\u65bc\u5408\u6210\u8cc7\u6599\u548c\u4eba\u985e\u7522\u751f\u8cc7\u6599\u7684\u6df7\u5408\u8cc7\u6599\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u5169\u500b\u554f\u984c\uff1a\u5408\u6210\u8cc7\u6599\u5c0d\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u7684\u5f71\u97ff\u662f\u4ec0\u9ebc\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u4e0d\u767c\u751f\u6a21\u578b\u5d29\u6f70\u7684\u60c5\u6cc1\u4e0b\u5408\u6210\u8cc7\u6599\uff1f\u6211\u5011\u9996\u5148\u5728\u4e0d\u540c\u6bd4\u4f8b\u7684\u5408\u6210\u8cc7\u6599\u4e2d\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5408\u6210\u8cc7\u6599\u7684\u6bd4\u4f8b\u8207\u6a21\u578b\u6548\u80fd\u4e4b\u9593\u7684\u8ca0\u76f8\u95dc\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c0d\u5408\u6210\u8cc7\u6599\u9032\u884c\u7d71\u8a08\u5206\u6790\uff0c\u4ee5\u767c\u73fe\u5206\u4f48\u8f49\u79fb\u73fe\u8c61\u548c n-gram \u7279\u5fb5\u7684\u904e\u5ea6\u96c6\u4e2d\u3002\u6839\u64da\u4e0a\u8ff0\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u5c0d\u4eba\u985e\u7522\u751f\u7684\u8cc7\u6599\u9032\u884c\u6a19\u8a18\u7de8\u8f2f\u4ee5\u53d6\u5f97\u534a\u5408\u6210\u8cc7\u6599\u3002\u4f5c\u70ba\u6982\u5ff5\u8b49\u660e\uff0c\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8b49\u660e\u4e86\u6a19\u8a18\u5c64\u7d1a\u7de8\u8f2f\u53ef\u4ee5\u9632\u6b62\u6a21\u578b\u5d29\u6f70\uff0c\u56e0\u70ba\u6e2c\u8a66\u8aa4\u5dee\u53d7\u5230\u6709\u9650\u7684\u4e0a\u9650\u7d04\u675f\u3002\u6211\u5011\u5c0d\u5f9e\u982d\u958b\u59cb\u7684\u9810\u8a13\u7df4\u3001\u6301\u7e8c\u9810\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u7d50\u679c\u9a57\u8b49\u4e86\u6211\u5011\u7684\u7406\u8ad6\u8b49\u660e\uff0c\u5373\u6a19\u8a18\u5c64\u7d1a\u7de8\u8f2f\u53ef\u4ee5\u6539\u5584\u8cc7\u6599\u54c1\u8cea\u4e26\u589e\u5f37\u6a21\u578b\u6548\u80fd\u3002", "author": "Xuekai Zhu et.al.", "authors": "Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou", "id": "2412.14689v1", "paper_url": "http://arxiv.org/abs/2412.14689v1", "repo": "null"}}