{"2501.10322": {"publish_time": "2025-01-17", "title": "Hierarchical Autoregressive Transformers: Combining Byte-~and Word-Level Processing for Robust, Adaptable Language Models", "paper_summary": "Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains.", "paper_summary_zh": "\u5206\u8bcd\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u57fa\u672c\u6b65\u9a5f\uff0c\u5c07\u6587\u672c\u5206\u89e3\u6210\u8a08\u7b97\u6a21\u578b\u53ef\u4ee5\u8655\u7406\u7684\u55ae\u4f4d\u3002\u96d6\u7136\u5b78\u7fd2\u7684\u5b50\u8a5e\u5206\u8a5e\u5668\u5df2\u6210\u70ba\u4e8b\u5be6\u4e0a\u7684\u6a19\u6e96\uff0c\u4f46\u5b83\u5011\u6703\u5e36\u4f86\u6311\u6230\uff0c\u4f8b\u5982\u8a5e\u5f59\u91cf\u9f90\u5927\u3001\u5c0d\u65b0\u9818\u57df\u6216\u8a9e\u8a00\u7684\u9069\u61c9\u6027\u6709\u9650\uff0c\u4ee5\u53ca\u5c0d\u62fc\u5beb\u932f\u8aa4\u548c\u8b8a\u5316\u7684\u654f\u611f\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u7814\u7a76\u4e86\u81ea\u8ff4\u6b78\u8a9e\u8a00\u5efa\u6a21\u7684\u5206\u5c64\u67b6\u69cb\uff0c\u5b83\u7d50\u5408\u4e86\u5b57\u5143\u7d1a\u548c\u8a5e\u5f59\u7d1a\u8655\u7406\u3002\u5b83\u63a1\u7528\u8f15\u91cf\u7d1a\u7684\u5b57\u5143\u7d1a\u7de8\u78bc\u5668\uff0c\u5c07\u5b57\u5143\u5e8f\u5217\u8f49\u63db\u70ba\u8a5e\u5f59\u5d4c\u5165\uff0c\u7136\u5f8c\u7531\u8a5e\u5f59\u7d1a\u4e3b\u5e79\u6a21\u578b\u8655\u7406\uff0c\u4e26\u900f\u904e\u4e00\u500b\u7dca\u6e4a\u7684\u5b57\u5143\u7d1a\u89e3\u78bc\u5668\u89e3\u78bc\u56de\u5b57\u5143\u3002\u9019\u7a2e\u65b9\u6cd5\u4fdd\u7559\u4e86\u8a5e\u5f59\u7d1a\u5206\u8a5e\u7684\u5e8f\u5217\u58d3\u7e2e\u512a\u9ede\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u65bc\u50f5\u5316\u3001\u9810\u5b9a\u7fa9\u7684\u8a5e\u5f59\u3002\u6211\u5011\u5728\u9ad8\u9054 70 \u5104\u500b\u53c3\u6578\u7684\u898f\u6a21\u4e0a\u8b49\u660e\uff0c\u5206\u5c64Transformer\u8207\u57fa\u65bc\u5b50\u8a5e\u5206\u8a5e\u5668\u7684\u6a21\u578b\u76f8\u5339\u914d\u7684\u4e0b\u6e38\u4efb\u52d9\u6027\u80fd\uff0c\u540c\u6642\u5c0d\u8f38\u5165\u64fe\u52d5\u8868\u73fe\u51fa\u986f\u8457\u66f4\u9ad8\u7684\u9b6f\u68d2\u6027\u3002\u6b64\u5916\uff0c\u5728\u6301\u7e8c\u5c0d\u9818\u57df\u5916\u8a9e\u8a00\u9032\u884c\u9810\u8a13\u7df4\u671f\u9593\uff0c\u6211\u5011\u7684\u6a21\u578b\u8a13\u7df4\u901f\u5ea6\u5e7e\u4e4e\u5feb\u4e86\u4e00\u500d\uff0c\u5728\u76ee\u6a19\u8a9e\u8a00\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e26\u4fdd\u7559\u4e86\u66f4\u591a\u5148\u524d\u5b78\u7fd2\u7684\u77e5\u8b58\u3002\u5206\u5c64Transformer\u70ba\u8de8\u8a9e\u8a00\u548c\u9818\u57df\u66f4\u5f37\u5927\u3001\u66f4\u9748\u6d3b\u3001\u66f4\u5177\u6982\u62ec\u6027\u7684 NLP \u7cfb\u7d71\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Pit Neitemeier et.al.", "authors": "Pit Neitemeier, Bj\u00f6rn Deiseroth, Constantin Eichenberg, Lukas Balles", "id": "2501.10322v1", "paper_url": "http://arxiv.org/abs/2501.10322v1", "repo": "null"}}