{"2405.20535": {"publish_time": "2024-05-30", "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning", "paper_summary": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost reasoning abilities during LLM pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during the IFT stage? To explore this, we thoroughly examine the\nimpact of coding data across different coding data proportions, model families,\nsizes, and reasoning domains, from various perspectives. Specifically, we\ncreate three IFT datasets with increasing coding data proportions, fine-tune\nsix LLM backbones across different families and scales on these datasets,\nevaluate the tuned models' performance across twelve tasks in three reasoning\ndomains, and analyze the outcomes from three broad-to-granular perspectives:\noverall, domain-level, and task-specific. Our holistic analysis provides\nvaluable insights in each perspective. First, coding data tuning enhances the\noverall reasoning capabilities of LLMs across different model families and\nscales. Moreover, the effect of coding data varies among different domains but\nshows consistent trends across model families and scales within each domain.\nAdditionally, coding data generally yields comparable task-specific benefits\nacross different model families, with the optimal coding data proportions in\nIFT datasets being task-specific.", "paper_summary_zh": "\u6307\u4ee4\u5fae\u8abf (IFT) \u5927\u5e45\u63d0\u5347\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u3002\u5118\u7ba1\u5df2\u77e5\u7de8\u78bc\u8cc7\u6599\u6703\u5728 LLM \u9810\u8a13\u7df4\u671f\u9593\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5728 IFT \u671f\u9593\u555f\u52d5\u5167\u90e8\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u89d2\u8272\u4ecd\u672a\u53d7\u5230\u6df1\u5165\u63a2\u8a0e\u3002\u672c\u8ad6\u6587\u63a2\u8a0e\u4e86\u4e00\u500b\u95dc\u9375\u554f\u984c\uff1a\u7de8\u78bc\u8cc7\u6599\u5728 IFT \u968e\u6bb5\u5982\u4f55\u5f71\u97ff LLM \u7684\u63a8\u7406\u80fd\u529b\uff1f\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u5f9e\u5404\u500b\u89d2\u5ea6\u5fb9\u5e95\u6aa2\u8996\u4e86\u7de8\u78bc\u8cc7\u6599\u5728\u4e0d\u540c\u7de8\u78bc\u8cc7\u6599\u6bd4\u4f8b\u3001\u6a21\u578b\u7cfb\u5217\u3001\u5927\u5c0f\u548c\u63a8\u7406\u9818\u57df\u4e2d\u7684\u5f71\u97ff\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e09\u500b\u7de8\u78bc\u8cc7\u6599\u6bd4\u4f8b\u9010\u6f38\u589e\u52a0\u7684 IFT \u8cc7\u6599\u96c6\uff0c\u5728\u9019\u4e9b\u8cc7\u6599\u96c6\u4e0a\u5fae\u8abf\u4e86\u4e0d\u540c\u7cfb\u5217\u548c\u898f\u6a21\u7684\u516d\u500b LLM \u4e3b\u5e79\uff0c\u4e26\u5728\u4e09\u500b\u63a8\u7406\u9818\u57df\u7684\u5341\u4e8c\u9805\u4efb\u52d9\u4e2d\u8a55\u4f30\u4e86\u5fae\u8abf\u6a21\u578b\u7684\u6548\u80fd\uff0c\u4e26\u5f9e\u4e09\u500b\u5ee3\u6cdb\u5230\u8a73\u7d30\u7684\u89d2\u5ea6\u5206\u6790\u7d50\u679c\uff1a\u6574\u9ad4\u3001\u9818\u57df\u5c64\u7d1a\u548c\u7279\u5b9a\u4efb\u52d9\u3002\u6211\u5011\u7684\u6574\u9ad4\u5206\u6790\u5728\u6bcf\u500b\u89d2\u5ea6\u90fd\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002\u9996\u5148\uff0c\u7de8\u78bc\u8cc7\u6599\u5fae\u8abf\u63d0\u5347\u4e86\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u548c\u898f\u6a21\u7684 LLM \u7684\u6574\u9ad4\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7de8\u78bc\u8cc7\u6599\u7684\u5f71\u97ff\u56e0\u9818\u57df\u800c\u7570\uff0c\u4f46\u5728\u6bcf\u500b\u9818\u57df\u4e2d\uff0c\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u548c\u898f\u6a21\u7684\u8da8\u52e2\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u7de8\u78bc\u8cc7\u6599\u901a\u5e38\u5728\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u4e2d\u7522\u751f\u4e86\u76f8\u7576\u7684\u7279\u5b9a\u4efb\u52d9\u6548\u76ca\uff0cIFT \u8cc7\u6599\u96c6\u4e2d\u6700\u4f73\u7684\u7de8\u78bc\u8cc7\u6599\u6bd4\u4f8b\u56e0\u4efb\u52d9\u800c\u7570\u3002", "author": "Xinlu Zhang et.al.", "authors": "Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, Linda Ruth Petzold", "id": "2405.20535v1", "paper_url": "http://arxiv.org/abs/2405.20535v1", "repo": "null"}}