{"2502.12928": {"publish_time": "2025-02-18", "title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts", "paper_summary": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5bc6\u96c6\u6a21\u578b\u901a\u5e38\u6703\u51fa\u73fe\u7a00\u758f\u6fc0\u6d3b\uff0c\u5176\u4e2d\u8a31\u591a\u6fc0\u6d3b\u503c\u8da8\u8fd1\u65bc\u96f6\uff08\u5373\u8655\u65bc\u975e\u6fc0\u6d3b\u72c0\u614b\uff09\u3002\u6211\u5011\u8a8d\u70ba\u9019\u53ef\u80fd\u6703\u9650\u5236\u6a21\u578b\u8868\u793a\u7a7a\u9593\u7684\u6709\u6548\u63a2\u7d22\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa Finedeep\uff0c\u9019\u662f\u4e00\u7a2e\u91dd\u5c0d\u5bc6\u96c6\u6a21\u578b\u7684\u6df1\u5ea6\u5206\u5c64\u7d30\u7c92\u5ea6\u5c08\u5bb6\u67b6\u69cb\u3002\u6211\u5011\u7684\u6846\u67b6\u5c07\u50b3\u7d71\u5bc6\u96c6\u6a21\u578b\u7684\u524d\u994b\u795e\u7d93\u7db2\u8def\u5c64\u5206\u5272\u6210\u5c0f\u578b\u5c08\u5bb6\uff0c\u4e26\u5c07\u5b83\u5011\u6392\u5217\u5728\u591a\u500b\u5b50\u5c64\u4e2d\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8def\u7531\u6a5f\u5236\u4f86\u78ba\u5b9a\u6bcf\u500b\u5c08\u5bb6\u7684\u8ca2\u737b\u3002\u6211\u5011\u91dd\u5c0d\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u8b49\u660e\u6211\u5011\u7684\u505a\u6cd5\u5728\u56f0\u60d1\u5ea6\u548c\u57fa\u6e96\u6548\u80fd\u65b9\u9762\u986f\u8457\u512a\u65bc\u50b3\u7d71\u7684\u5bc6\u96c6\u67b6\u69cb\uff0c\u540c\u6642\u4fdd\u6301\u4e86\u76f8\u7576\u6578\u91cf\u7684\u53c3\u6578\u548c\u6d6e\u9ede\u904b\u7b97\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe Finedeep \u5728\u5e73\u8861\u6df1\u5ea6\u548c\u5ee3\u5ea6\u6642\u53ef\u4ee5\u9054\u5230\u6700\u4f73\u7d50\u679c\uff0c\u7279\u5225\u662f\u900f\u904e\u8abf\u6574\u5c08\u5bb6\u5b50\u5c64\u7684\u6578\u91cf\u548c\u6bcf\u500b\u5b50\u5c64\u7684\u5c08\u5bb6\u6578\u91cf\u3002\u5be6\u8b49\u7d50\u679c\u8b49\u5be6\uff0cFinedeep \u6709\u6548\u5730\u6e1b\u8f15\u4e86\u7a00\u758f\u6fc0\u6d3b\uff0c\u4e26\u6709\u6548\u5229\u7528\u4e86\u5bc6\u96c6\u6a21\u578b\u4e2d\u7684\u8868\u793a\u80fd\u529b\u3002", "author": "Leiyu Pan et.al.", "authors": "Leiyu Pan, Zhenpeng Su, Minxuan Lv, Yizhe Xiong, Xiangwen Zhang, Zijia Lin, Hui Chen, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Deyi Xiong", "id": "2502.12928v1", "paper_url": "http://arxiv.org/abs/2502.12928v1", "repo": "null"}}