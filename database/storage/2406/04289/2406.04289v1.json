{"2406.04289": {"publish_time": "2024-06-06", "title": "What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages", "paper_summary": "What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u5b78\u7fd2\u4ec0\u9ebc\uff1f\u6839\u64da\u5b9a\u7fa9\uff0c\u8a9e\u8a00\u6a21\u578b (LM) \u662f\u5b57\u4e32\u4e0a\u7684\u5206\u4f48\u3002\u56e0\u6b64\uff0c\u89e3\u6c7a\u4e0a\u8ff0\u554f\u984c\u7684\u4e00\u500b\u76f4\u89c0\u65b9\u6cd5\u662f\u5c07\u5176\u5f62\u5f0f\u5316\u70ba\u5b57\u4e32\u4e0a\u5206\u4f48\u985e\u5225\u7684\u53ef\u5b78\u7fd2\u6027\u554f\u984c\u3002\u96d6\u7136\u904e\u53bb\u671d\u9019\u500b\u65b9\u5411\u7684\u7814\u7a76\u5c08\u6ce8\u65bc\u8a55\u4f30\u7406\u8ad6\u9650\u5236\uff0c\u4f46\u6211\u5011\u8a66\u5716\u4e86\u89e3\u7d93\u9a57\u53ef\u5b78\u7fd2\u6027\u3002\u8207\u904e\u53bb\u7684\u7d93\u9a57\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u5728\u5b83\u5011\u7684\u4e3b\u5834\u4e0a\u8a55\u4f30\u795e\u7d93\u8a9e\u8a00\u6a21\u578b\uff0c\u5b78\u7fd2\u6a5f\u7387\u8a9e\u8a00\uff0c\u800c\u4e0d\u662f\u4f5c\u70ba\u5f62\u5f0f\u8a9e\u8a00\u7684\u5206\u985e\u5668\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7814\u7a76 RNN \u548c Transformer LM \u5c0d\u6b63\u5247\u8a9e\u8a00\u6a21\u578b (RLM) \u7684\u53ef\u5b78\u7fd2\u6027\u3002\u6211\u5011\u6839\u64da RLM \u7684\u5404\u7a2e\u8907\u96dc\u5ea6\u53c3\u6578\u548c\u795e\u7d93\u8a9e\u8a00\u6a21\u578b\u7684\u96b1\u85cf\u72c0\u614b\u5927\u5c0f\uff0c\u5be6\u8b49\u6e2c\u8a66 RLM \u7684\u53ef\u5b78\u7fd2\u6027\u3002\u6211\u5011\u767c\u73fe RLM \u7b49\u7d1a\uff08\u5c0d\u61c9\u65bc\u5176\u689d\u4ef6\u5206\u4f48\u7684\u5c0d\u6578\u6a5f\u7387\u6240\u8de8\u8d8a\u7684\u7dda\u6027\u7a7a\u9593\u5927\u5c0f\uff09\u548c\u62bd\u6a23\u5b57\u4e32\u7684\u9810\u671f\u9577\u5ea6\u662f RNN \u548c Transformer \u53ef\u5b78\u7fd2\u6027\u7684\u5f37\u6709\u529b\u4e14\u986f\u8457\u7684\u9810\u6e2c\u56e0\u5b50\u3002\u5176\u4ed6\u5e7e\u500b\u9810\u6e2c\u56e0\u5b50\u4e5f\u9054\u5230\u986f\u8457\u6027\uff0c\u4f46\u5728 RNN \u548c Transformer \u4e4b\u9593\u6709\u4e0d\u540c\u7684\u6a21\u5f0f\u3002", "author": "Nadav Borenstein et.al.", "authors": "Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell", "id": "2406.04289v1", "paper_url": "http://arxiv.org/abs/2406.04289v1", "repo": "null"}}