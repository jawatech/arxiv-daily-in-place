{"2405.20089": {"publish_time": "2024-05-30", "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities", "paper_summary": "Fine-tuning large language models (LLMs) for machine translation has shown\nimprovements in overall translation quality. However, it is unclear what is the\nimpact of fine-tuning on desirable LLM behaviors that are not present in neural\nmachine translation models, such as steerability, inherent document-level\ntranslation abilities, and the ability to produce less literal translations. We\nperform an extensive translation evaluation on the LLaMA and Falcon family of\nmodels with model size ranging from 7 billion up to 65 billion parameters. Our\nresults show that while fine-tuning improves the general translation quality of\nLLMs, several abilities degrade. In particular, we observe a decline in the\nability to perform formality steering, to produce technical translations\nthrough few-shot examples, and to perform document-level translation. On the\nother hand, we observe that the model produces less literal translations after\nfine-tuning on parallel data. We show that by including monolingual data as\npart of the fine-tuning data we can maintain the abilities while simultaneously\nenhancing overall translation quality. Our findings emphasize the need for\nfine-tuning strategies that preserve the benefits of LLMs for machine\ntranslation.", "paper_summary_zh": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u5df2\u663e\u793a\u5728\u6574\u4f53\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u6709\u6539\u8fdb\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5fae\u8c03\u5bf9\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u4e0d\u5b58\u5728\u7684 LLM \u671f\u671b\u884c\u4e3a\u6709\u4f55\u5f71\u54cd\uff0c\u4f8b\u5982\u53ef\u64cd\u7eb5\u6027\u3001\u56fa\u6709\u7684\u6587\u6863\u7ea7\u7ffb\u8bd1\u80fd\u529b\u4ee5\u53ca\u751f\u6210\u8f83\u5c11\u76f4\u8bd1\u7684\u80fd\u529b\u3002\u6211\u4eec\u5bf9 LLaMA \u548c Falcon \u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7ffb\u8bd1\u8bc4\u4f30\uff0c\u6a21\u578b\u5927\u5c0f\u4ece 70 \u4ebf\u5230 650 \u4ebf\u4e2a\u53c2\u6570\u4e0d\u7b49\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u5fae\u8c03\u63d0\u9ad8\u4e86 LLM \u7684\u4e00\u822c\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f46\u4e00\u4e9b\u80fd\u529b\u5374\u4e0b\u964d\u4e86\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5728\u6267\u884c\u5f62\u5f0f\u64cd\u7eb5\u3001\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u751f\u6210\u6280\u672f\u7ffb\u8bd1\u4ee5\u53ca\u6267\u884c\u6587\u6863\u7ea7\u7ffb\u8bd1\u7684\u80fd\u529b\u65b9\u9762\u6709\u6240\u4e0b\u964d\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8be5\u6a21\u578b\u5728\u5e76\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\u4ea7\u751f\u7684\u76f4\u8bd1\u8f83\u5c11\u3002\u6211\u4eec\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u5355\u8bed\u6570\u636e\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u540c\u65f6\u63d0\u9ad8\u6574\u4f53\u7ffb\u8bd1\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u8fd9\u4e9b\u80fd\u529b\u3002\u6211\u4eec\u7684\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u5fae\u8c03\u7b56\u7565\u6765\u4fdd\u7559 LLM \u5bf9\u673a\u5668\u7ffb\u8bd1\u7684\u597d\u5904\u3002", "author": "David Stap et.al.", "authors": "David Stap, Eva Hasler, Bill Byrne, Christof Monz, Ke Tran", "id": "2405.20089v1", "paper_url": "http://arxiv.org/abs/2405.20089v1", "repo": "null"}}