{"2501.19202": {"publish_time": "2025-01-31", "title": "Improving the Robustness of Representation Misdirection for Large Language Model Unlearning", "paper_summary": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.", "paper_summary_zh": "\u8868\u5fb5\u8aa4\u5c0e (RM) \u53ca\u5176\u8b8a\u9ad4\u662f\u5df2\u5efa\u7acb\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fd8\u8a18\u65b9\u6cd5\uff0c\u5177\u6709\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a RM \u65b9\u6cd5\u672c\u8cea\u4e0a\u6703\u964d\u4f4e\u6a21\u578b\u7684\u7a69\u5065\u6027\uff0c\u5c0e\u81f4\u5b83\u5011\u5728\u4fdd\u7559\u67e5\u8a62\u4e2d\u5373\u4f7f\u53ea\u6709\u4e00\u500b\u975e\u5c0d\u6297\u6027\u7684\u907a\u5fd8\u4ee3\u78bc\u4e5f\u6703\u51fa\u73fe\u7570\u5e38\u884c\u70ba\u3002\u70ba\u4e86\u7406\u89e3\u80cc\u5f8c\u7684\u539f\u56e0\uff0c\u6211\u5011\u5c07\u5fd8\u8a18\u904e\u7a0b\u91cd\u65b0\u5b9a\u7fa9\u70ba\u5f8c\u9580\u653b\u64ca\u548c\u9632\u79a6\uff1a\u907a\u5fd8\u4ee3\u78bc\u5145\u7576\u5f8c\u9580\u89f8\u767c\u5668\uff0c\u7576\u5728\u4fdd\u7559\u67e5\u8a62\u4e2d\u88ab\u555f\u52d5\u6642\uff0c\u6703\u9020\u6210 RM \u6a21\u578b\u884c\u70ba\u7684\u4e2d\u65b7\uff0c\u985e\u4f3c\u65bc\u6210\u529f\u7684\u5f8c\u9580\u653b\u64ca\u3002\u70ba\u4e86\u6e1b\u8f15\u6b64\u6f0f\u6d1e\uff0c\u6211\u5011\u63d0\u51fa\u96a8\u6a5f\u96dc\u8a0a\u64f4\u5145\u2014\u2014\u4e00\u7a2e\u6a21\u578b\u548c\u65b9\u6cd5\u4e0d\u53ef\u77e5\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u63d0\u9ad8 RM \u65b9\u6cd5\u7a69\u5065\u6027\u7684\u7406\u8ad6\u4fdd\u8b49\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0cRNA \u5728\u589e\u5f37\u5fd8\u8a18\u6548\u80fd\u7684\u540c\u6642\uff0c\u986f\u8457\u63d0\u9ad8\u4e86 RM \u6a21\u578b\u7684\u7a69\u5065\u6027\u3002", "author": "Dang Huu-Tien et.al.", "authors": "Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue", "id": "2501.19202v2", "paper_url": "http://arxiv.org/abs/2501.19202v2", "repo": "https://github.com/rebelsnlu-jaist/llmu-robustness"}}