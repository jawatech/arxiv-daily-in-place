{"2407.14417": {"publish_time": "2024-07-19", "title": "Mixture of Experts with Mixture of Precisions for Tuning Quality of Service", "paper_summary": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 2.62 to 2.80, 6.48 to 7.24, and 3.24 to 3.53\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.", "paper_summary_zh": "\u96a8\u8457\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u6df7\u5408\u5c08\u5bb6 (MoE) \u6a21\u578b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u4f86\u89e3\u6c7a\u5176\u9ad8\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u7684\u6311\u6230\u3002\u6b64\u5916\uff0c\u7531\u65bc\u4efb\u52d9\u5177\u6709\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u5b9a\u7fa9\u7d04\u675f\uff0c\u4e14\u5728\u591a\u79df\u6236\u74b0\u5883\u4e2d\u53ef\u7528\u8cc7\u6e90\u6703\u96a8\u8457\u6642\u9593\u800c\u6539\u8b8a\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8a2d\u8a08\u4e00\u7a2e\u63d0\u4f9b\u5f48\u6027\u914d\u7f6e\u7a7a\u9593\u7684\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u9069\u61c9\u670d\u52d9\u65b9\u6cd5\uff0c\u7528\u65bc\u6709\u6548\u90e8\u7f72 MoE \u6a21\u578b\uff0c\u5229\u7528\u5c08\u5bb6\u7684\u90e8\u5206\u91cf\u5316\u3002\u900f\u904e\u52d5\u614b\u78ba\u5b9a\u91cf\u5316\u5c08\u5bb6\u7684\u6578\u91cf\u53ca\u5176\u5728 CPU \u548c GPU \u4e0a\u7684\u5206\u5e03\uff0c\u6211\u5011\u7684\u505a\u6cd5\u63a2\u7d22\u4e86 Pareto \u524d\u7de3\uff0c\u4e26\u63d0\u4f9b\u4e86\u7528\u65bc\u8abf\u6574\u901a\u91cf\u548c\u6a21\u578b\u54c1\u8cea\u7684\u7d30\u7c92\u5ea6\u914d\u7f6e\u7bc4\u570d\u3002\u6211\u5011\u4f7f\u7528 Mixtral 8x7B MoE \u6a21\u578b\u5728 NVIDIA A100 GPU \u4e0a\u91dd\u5c0d\u4e09\u500b\u8a9e\u8a00\u5efa\u6a21\u57fa\u6e96\u9032\u884c\u8a55\u4f30\uff0c\u7d50\u679c\u8868\u660e\uff0c\u4ee4\u724c\u7522\u751f\u7684\u901a\u91cf\u53ef\u4ee5\u5f9e\u6bcf\u79d2 0.63 \u500b\u4ee4\u724c\u8abf\u6574\u5230 13.00 \u500b\u4ee4\u724c\u3002\u6b64\u589e\u5f37\u529f\u80fd\u7684\u56f0\u60d1\u5ea6\u7565\u6709\u589e\u52a0\uff0c\u5728\u6700\u5927\u91cf\u5316\u4e0b\uff0c\u5c0d\u65bc WikiText2\u3001PTB \u548c C4 \u8cc7\u6599\u96c6\u5206\u5225\u70ba 2.62 \u5230 2.80\u30016.48 \u5230 7.24 \u548c 3.24 \u5230 3.53\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u52d5\u614b\u4e14\u5c0d\u6e96\u78ba\u5ea6\u654f\u611f\u7684\u61c9\u7528\u4e2d\u7684\u5be6\u969b\u9069\u7528\u6027\uff0c\u5728\u9019\u4e9b\u61c9\u7528\u4e2d\uff0c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u8f38\u51fa\u54c1\u8cea\u90fd\u5f88\u91cd\u8981\u3002", "author": "HamidReza Imani et.al.", "authors": "HamidReza Imani, Abdolah Amirany, Tarek El-Ghazawi", "id": "2407.14417v1", "paper_url": "http://arxiv.org/abs/2407.14417v1", "repo": "null"}}