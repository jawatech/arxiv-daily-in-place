{"2411.12663": {"publish_time": "2024-11-19", "title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer", "paper_summary": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous\nto generate high quality images and videos. However, encoding an image or a\nvideo as a sequence of patches results in costly attention patterns, as the\nrequirements both in terms of memory and compute grow quadratically. To\nalleviate this problem, we propose a drop-in replacement for MHA called the\nPolynomial Mixer (PoM) that has the benefit of encoding the entire sequence\ninto an explicit state. PoM has a linear complexity with respect to the number\nof tokens. This explicit state also allows us to generate frames in a\nsequential fashion, minimizing memory and compute requirement, while still\nbeing able to train in parallel. We show the Polynomial Mixer is a universal\nsequence-to-sequence approximator, just like regular MHA. We adapt several\nDiffusion Transformers (DiT) for generating images and videos with PoM\nreplacing MHA, and we obtain high quality samples while using less\ncomputational resources. The code is available at\nhttps://github.com/davidpicard/HoMM.", "paper_summary_zh": "\u57fa\u65bc\u591a\u982d\u6ce8\u610f\u529b (MHA) \u7684\u64f4\u6563\u6a21\u578b\u5df2\u5ee3\u6cdb\u7528\u65bc\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u5f71\u50cf\u548c\u5f71\u7247\u3002\u7136\u800c\uff0c\u5c07\u5f71\u50cf\u6216\u5f71\u7247\u7de8\u78bc\u6210\u4e00\u7cfb\u5217\u4fee\u88dc\u7a0b\u5f0f\u6703\u5c0e\u81f4\u6602\u8cb4\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u56e0\u70ba\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u6703\u4ee5\u4e8c\u6b21\u65b9\u7684\u901f\u5ea6\u589e\u9577\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b MHA \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7a31\u70ba\u591a\u9805\u5f0f\u6df7\u5408\u5668 (PoM)\uff0c\u5b83\u5177\u6709\u5c07\u6574\u500b\u5e8f\u5217\u7de8\u78bc\u6210\u660e\u78ba\u72c0\u614b\u7684\u512a\u9ede\u3002PoM \u5177\u6709\u8207 token \u6578\u91cf\u6210\u7dda\u6027\u7684\u8907\u96dc\u5ea6\u3002\u9019\u500b\u660e\u78ba\u7684\u72c0\u614b\u4e5f\u5141\u8a31\u6211\u5011\u4ee5\u9806\u5e8f\u7684\u65b9\u5f0f\u7522\u751f\u756b\u9762\uff0c\u5c07\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u964d\u5230\u6700\u4f4e\uff0c\u540c\u6642\u4ecd\u7136\u80fd\u5920\u4e26\u884c\u8a13\u7df4\u3002\u6211\u5011\u8b49\u660e\u4e86\u591a\u9805\u5f0f\u6df7\u5408\u5668\u662f\u4e00\u500b\u901a\u7528\u7684\u5e8f\u5217\u5c0d\u5e8f\u5217\u903c\u8fd1\u5668\uff0c\u5c31\u50cf\u4e00\u822c\u7684 MHA \u4e00\u6a23\u3002\u6211\u5011\u6539\u7de8\u4e86\u5e7e\u500b\u7528\u65bc\u7522\u751f\u5f71\u50cf\u548c\u5f71\u7247\u7684\u64f4\u6563Transformer (DiT)\uff0c\u4e26\u7528 PoM \u53d6\u4ee3 MHA\uff0c\u6211\u5011\u5728\u4f7f\u7528\u8f03\u5c11\u904b\u7b97\u8cc7\u6e90\u7684\u540c\u6642\u7372\u5f97\u4e86\u9ad8\u54c1\u8cea\u7684\u6a23\u672c\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/davidpicard/HoMM \u53d6\u5f97\u3002", "author": "David Picard et.al.", "authors": "David Picard, Nicolas Dufour", "id": "2411.12663v1", "paper_url": "http://arxiv.org/abs/2411.12663v1", "repo": "https://github.com/davidpicard/homm"}}