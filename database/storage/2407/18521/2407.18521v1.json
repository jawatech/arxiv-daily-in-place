{"2407.18521": {"publish_time": "2024-07-26", "title": "Patched MOA: optimizing inference for diverse software development tasks", "paper_summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u4ecb\u7d39\u4e86\u4fee\u88dc\u5f8c\u7684 MOA\uff08\u4ee3\u7406\u6df7\u5408\uff09\uff0c\u4e00\u7a2e\u63a8\u8ad6\u6700\u4f73\u5316\u6280\u8853\uff0c\u5b83\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u8edf\u9ad4\u958b\u767c\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\u3002\u6211\u5011\u8a55\u4f30\u4e86\u4e09\u7a2e\u63a8\u8ad6\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u5206\u5225\u662f N \u4e2d\u6700\u4f73\u3001\u4ee3\u7406\u6df7\u5408\uff0c\u4ee5\u53ca\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b\uff0c\u4e26\u5c55\u793a\u4fee\u88dc\u5f8c\u7684 MOA \u53ef\u4ee5\u63d0\u5347\u8f03\u5c0f\u578b\u6a21\u578b\u7684\u6548\u80fd\uff0c\u8d85\u8d8a\u8f03\u5927\u578b\u3001\u8f03\u6602\u8cb4\u7684\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728 Arena-Hard-Auto \u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c07 gpt-4o-mini \u6a21\u578b\u7684\u6548\u80fd\u63d0\u5347\u4e86 15.52%\uff0c\u4ee5\u4e0d\u5230 gpt-4-turbo \u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\u5c31\u8d85\u8d8a\u4e86 gpt-4-turbo\u3002\u6211\u5011\u4e5f\u5c07\u4fee\u88dc\u5f8c\u7684 MOA \u61c9\u7528\u65bc\u5404\u7a2e\u8edf\u9ad4\u958b\u767c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u986f\u793a\u5728\u4efb\u52d9\u5b8c\u6210\u7387\u65b9\u9762\u6709\u986f\u8457\u7684\u63d0\u5347\u3002\u6211\u5011\u7684\u65b9\u6cd5\u8207\u6a21\u578b\u7121\u95dc\uff0c\u5c0d\u6700\u7d42\u4f7f\u7528\u8005\u4f86\u8aaa\u662f\u900f\u660e\u7684\uff0c\u800c\u4e14\u53ef\u4ee5\u8f15\u9b06\u6574\u5408\u5230\u73fe\u6709\u7684 LLM \u7ba1\u7dda\u4e2d\u3002\u9019\u9805\u5de5\u4f5c\u6709\u52a9\u65bc LLM \u6700\u4f73\u5316\u9818\u57df\u7684\u767c\u5c55\uff0c\u63d0\u4f9b\u4e00\u7a2e\u5177\u6210\u672c\u6548\u76ca\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u5fae\u8abf\u6216\u4f7f\u7528\u8f03\u5927\u578b\u6a21\u578b\u7684\u60c5\u6cc1\u4e0b\u63d0\u5347\u6a21\u578b\u6548\u80fd\u3002", "author": "Asankhaya Sharma et.al.", "authors": "Asankhaya Sharma", "id": "2407.18521v1", "paper_url": "http://arxiv.org/abs/2407.18521v1", "repo": "null"}}