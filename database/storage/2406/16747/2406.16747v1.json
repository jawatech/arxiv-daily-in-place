{"2406.16747": {"publish_time": "2024-06-24", "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers", "paper_summary": "Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.", "paper_summary_zh": "\u5728\u81ea\u56de\u5f52 Transformer \u4e2d\u6709\u6548\u5bb9\u7eb3\u957f\u5e8f\u5217\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u5185\uff0c\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u56fa\u6709\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5927\u91cf\u7684 KV \u5185\u5b58\u9700\u6c42\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SPARSEK \u6ce8\u610f\u529b\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u8ba1\u7b97\u548c\u5185\u5b58\u969c\u788d\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u96c6\u6210\u4e86\u4e00\u4e2a\u8bc4\u5206\u7f51\u7edc\u548c\u4e00\u4e2a\u53ef\u5fae\u5206\u7684 top-k \u63a9\u7801\u8fd0\u7b97\u7b26 SPARSEK\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u4e00\u4e2a\u6052\u5b9a\u7684 KV \u5bf9\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002\u56e0\u6b64\uff0cSPARSEK \u6ce8\u610f\u529b\u5728\u751f\u6210\u671f\u95f4\u63d0\u4f9b\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u6052\u5b9a\u5185\u5b58\u5360\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSPARSEK \u6ce8\u610f\u529b\u4f18\u4e8e\u5148\u524d\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u63d0\u4f9b\u4e86\u663e\u7740\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\uff0c\u53ea\u9700\u8fdb\u884c\u6700\u5c0f\u7684\u5fae\u8c03\uff0c\u4e3a\u5728\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6709\u6548\u7ba1\u7406\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "author": "Chao Lou et.al.", "authors": "Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu", "id": "2406.16747v1", "paper_url": "http://arxiv.org/abs/2406.16747v1", "repo": "null"}}