{"2502.07982": {"publish_time": "2025-02-11", "title": "Deep Semantic Graph Learning via LLM based Node Enhancement", "paper_summary": "Graph learning has attracted significant attention due to its widespread\nreal-world applications. Current mainstream approaches rely on text node\nfeatures and obtain initial node embeddings through shallow embedding learning\nusing GNNs, which shows limitations in capturing deep textual semantics. Recent\nadvances in Large Language Models (LLMs) have demonstrated superior\ncapabilities in understanding text semantics, transforming traditional text\nfeature processing. This paper proposes a novel framework that combines Graph\nTransformer architecture with LLM-enhanced node features. Specifically, we\nleverage LLMs to generate rich semantic representations of text nodes, which\nare then processed by a multi-head self-attention mechanism in the Graph\nTransformer to capture both local and global graph structural information. Our\nmodel utilizes the Transformer's attention mechanism to dynamically aggregate\nneighborhood information while preserving the semantic richness provided by LLM\nembeddings. Experimental results demonstrate that the LLM-enhanced node\nfeatures significantly improve the performance of graph learning models on node\nclassification tasks. This approach shows promising results across multiple\ngraph learning tasks, offering a practical direction for combining graph\nnetworks with language models.", "paper_summary_zh": "\u5716\u5f62\u5b78\u7fd2\u56e0\u5176\u5ee3\u6cdb\u7684\u73fe\u5be6\u4e16\u754c\u61c9\u7528\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u76ee\u524d\u7684\u71b1\u9580\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u6587\u672c\u7bc0\u9ede\u7279\u5fb5\uff0c\u4e26\u901a\u904e\u4f7f\u7528 GNN \u7684\u6dfa\u5c64\u5d4c\u5165\u5b78\u7fd2\u4f86\u7372\u53d6\u521d\u59cb\u7bc0\u9ede\u5d4c\u5165\uff0c\u9019\u5728\u6355\u6349\u6df1\u5ea6\u6587\u672c\u8a9e\u7fa9\u65b9\u9762\u8868\u73fe\u51fa\u5c40\u9650\u6027\u3002\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u8b49\u660e\u5728\u7406\u89e3\u6587\u672c\u8a9e\u7fa9\u65b9\u9762\u5177\u6709\u512a\u8d8a\u7684\u80fd\u529b\uff0c\u8f49\u63db\u4e86\u50b3\u7d71\u7684\u6587\u672c\u7279\u5fb5\u8655\u7406\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u6846\u67b6\uff0c\u5c07\u5716\u5f62\u8f49\u63db\u5668\u67b6\u69cb\u8207 LLM \u589e\u5f37\u7684\u7bc0\u9ede\u7279\u5fb5\u76f8\u7d50\u5408\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528 LLM \u4f86\u751f\u6210\u6587\u672c\u7bc0\u9ede\u7684\u8c50\u5bcc\u8a9e\u7fa9\u8868\u793a\uff0c\u7136\u5f8c\u5728\u5716\u5f62\u8f49\u63db\u5668\u4e2d\u7531\u591a\u982d\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u8655\u7406\uff0c\u4ee5\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u5716\u5f62\u7d50\u69cb\u4fe1\u606f\u3002\u6211\u5011\u7684\u6a21\u578b\u5229\u7528 Transformer \u7684\u6ce8\u610f\u6a5f\u5236\u4f86\u52d5\u614b\u805a\u5408\u9130\u57df\u4fe1\u606f\uff0c\u540c\u6642\u4fdd\u7559 LLM \u5d4c\u5165\u63d0\u4f9b\u7684\u8a9e\u7fa9\u8c50\u5bcc\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cLLM \u589e\u5f37\u7684\u7bc0\u9ede\u7279\u5fb5\u986f\u8457\u63d0\u9ad8\u4e86\u5716\u5f62\u5b78\u7fd2\u6a21\u578b\u5728\u7bc0\u9ede\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u6027\u80fd\u3002\u9019\u7a2e\u65b9\u6cd5\u5728\u591a\u500b\u5716\u5f62\u5b78\u7fd2\u4efb\u52d9\u4e2d\u986f\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\uff0c\u70ba\u5c07\u5716\u5f62\u7db2\u7d61\u8207\u8a9e\u8a00\u6a21\u578b\u76f8\u7d50\u5408\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u65b9\u5411\u3002", "author": "Chuanqi Shi et.al.", "authors": "Chuanqi Shi, Yiyi Tao, Hang Zhang, Lun Wang, Shaoshuai Du, Yixian Shen, Yanxin Shen", "id": "2502.07982v1", "paper_url": "http://arxiv.org/abs/2502.07982v1", "repo": "null"}}