{"2502.03147": {"publish_time": "2025-02-05", "title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented Large Language Models", "paper_summary": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91dd\u5c0d\u8868\u683c\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u5f8c\uff0c\u53ef\u4ee5\u7372\u5f97\u4e00\u822c\u7684\u8868\u683c\u60c5\u5883\u5b78\u7fd2 (TabICL) \u80fd\u529b\u3002\u9019\u4e9b\u6a21\u578b\u80fd\u5920\u6709\u6548\u5730\u8f49\u79fb\u5230\u4e0d\u540c\u7684\u8cc7\u6599\u67b6\u69cb\u548c\u4e0d\u540c\u7684\u4efb\u52d9\u9818\u57df\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u65bc LLM \u7684 TabICL \u65b9\u6cd5\u7531\u65bc LLM \u7684\u5e8f\u5217\u9577\u5ea6\u9650\u5236\u800c\u53d7\u5230\u5c11\u6a23\u672c\u5834\u666f\u7684\u9650\u5236\uff0c\u56e0\u70ba\u4ee5\u7d14\u6587\u5b57\u8868\u793a\u7684\u8868\u683c\u5be6\u4f8b\u6703\u6d88\u8017\u5927\u91cf\u7684\u7b26\u865f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\u4e26\u70ba\u4efb\u4f55\u8cc7\u6599\u5927\u5c0f\u555f\u7528\u53ef\u64f4\u5145\u7684 TabICL\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u91dd\u5c0d\u8868\u683c\u8cc7\u6599\u91cf\u8eab\u6253\u9020\u7684\u6aa2\u7d22\u589e\u5f37 LLM\u3002\u6211\u5011\u7684\u505a\u6cd5\u7d50\u5408\u4e86\u81ea\u8a02\u7684\u6aa2\u7d22\u6a21\u7d44\uff0c\u4ee5\u53ca\u91dd\u5c0d LLM \u7684\u6aa2\u7d22\u5f15\u5c0e\u6307\u4ee4\u5fae\u8abf\u3002\u9019\u4f7f LLM \u80fd\u5920\u6709\u6548\u5730\u5229\u7528\u66f4\u5927\u7684\u8cc7\u6599\u96c6\uff0c\u5728 69 \u500b\u5ee3\u6cdb\u8a8d\u53ef\u7684\u8cc7\u6599\u96c6\u4e0a\u5be6\u73fe\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4e26\u5c55\u793a\u51fa\u6709\u5e0c\u671b\u7684\u64f4\u5145\u884c\u70ba\u3002\u8207\u6700\u5148\u9032\u7684\u8868\u683c\u6a21\u578b\u9032\u884c\u5ee3\u6cdb\u7684\u6bd4\u8f03\u8868\u660e\uff0c\u5118\u7ba1\u57fa\u65bc LLM \u7684 TabICL \u5728\u6574\u9ad4\u6548\u80fd\u4e0a\u4ecd\u843d\u5f8c\u65bc\u7d93\u904e\u826f\u597d\u5fae\u8abf\u7684\u6578\u503c\u6a21\u578b\uff0c\u4f46\u5b83\u5728\u6709\u9650\u7684\u8a9e\u5883\u4e0b\u63ed\u793a\u4e86\u5f37\u5927\u7684\u6f14\u7b97\u6cd5\uff0c\u589e\u5f37\u4e86\u6574\u9ad4\u591a\u6a23\u6027\uff0c\u4e26\u5728\u7279\u5b9a\u8cc7\u6599\u96c6\u4e0a\u8868\u73fe\u51fa\u8272\u3002\u9019\u4e9b\u7368\u7279\u7684\u7279\u6027\u5f37\u8abf\u4e86\u8a9e\u8a00\u4f5c\u70ba\u53ef\u64f4\u5145\u8868\u683c\u8cc7\u6599\u5b78\u7fd2\u7684\u901a\u7528\u4e14\u53ef\u5b58\u53d6\u4ecb\u9762\u7684\u6f5b\u529b\u3002", "author": "Xumeng Wen et.al.", "authors": "Xumeng Wen, Shun Zheng, Zhen Xu, Yiming Sun, Jiang Bian", "id": "2502.03147v1", "paper_url": "http://arxiv.org/abs/2502.03147v1", "repo": "null"}}