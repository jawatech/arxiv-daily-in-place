{"2408.10189": {"publish_time": "2024-08-19", "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models", "paper_summary": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.", "paper_summary_zh": "\u8b8a\u5f62\u5668\u67b6\u69cb\u5df2\u6210\u70ba\u8a9e\u8a00\u6a21\u578b\u7b49\u9818\u57df\u7684\u4e3b\u6d41\u7bc4\u4f8b\uff0c\u4f46\u7531\u65bc\u5176\u4e8c\u6b21\u65b9\u6642\u9593\u81ea\u6211\u6ce8\u610f\uff0c\u5728\u8a31\u591a\u63a8\u8ad6\u8a2d\u5b9a\u4e2d\u6703\u9047\u5230\u56f0\u96e3\u3002\u6700\u8fd1\u63d0\u51fa\u7684\u6b21\u4e8c\u6b21\u65b9\u67b6\u69cb\uff0c\u4f8b\u5982 Mamba\uff0c\u5df2\u5c55\u73fe\u5176\u524d\u666f\uff0c\u4f46\u9810\u8a13\u7df4\u6642\u6240\u7528\u7684\u904b\u7b97\u8cc7\u6e90\u9060\u4f4e\u65bc\u6700\u5f37\u5927\u7684\u8b8a\u5f62\u5668\u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u80fd\u5920\u5c07\u9810\u8a13\u7df4\u7684\u8b8a\u5f62\u5668\u67b6\u69cb\u8f49\u5316\u70ba\u66ff\u4ee3\u67b6\u69cb\uff0c\u4f8b\u5982\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\u3002\u6211\u5011\u65b9\u6cd5\u7684\u95dc\u9375\u6982\u5ff5\u662f\uff0c\u6211\u5011\u53ef\u4ee5\u5c07\u8b8a\u5f62\u5668\u548c SSM \u8996\u70ba\u5728\u4ee4\u724c\u5e8f\u5217\u4e0a\u5957\u7528\u4e0d\u540c\u5f62\u5f0f\u7684\u6df7\u5408\u77e9\u9663\u3002\u56e0\u6b64\uff0c\u6211\u5011\u53ef\u4ee5\u900f\u904e\u8abf\u6574 SSM \u4e2d\u4e0d\u540c\u7a0b\u5ea6\u7684\u8a73\u7d30\u5ea6\u4f86\u9010\u6b65\u8f49\u5316\u8b8a\u5f62\u5668\u67b6\u69cb\uff1a\u9996\u5148\u8abf\u6574\u6df7\u5408\u77e9\u9663\u672c\u8eab\uff0c\u7136\u5f8c\u8abf\u6574\u6bcf\u500b\u5340\u584a\u4e2d\u7684\u96b1\u85cf\u55ae\u5143\uff0c\u6700\u5f8c\u8abf\u6574\u7aef\u5c0d\u7aef\u9810\u6e2c\u3002\u6211\u5011\u7684\u65b9\u6cd5\u7a31\u70ba MOHAWK\uff0c\u80fd\u5920\u4f7f\u7528\u50c5 3B \u500b\u4ee4\u724c\u5c07\u57fa\u65bc Phi-1.5 \u67b6\u69cb\u7684 Mamba-2 \u8b8a\u9ad4\uff08Phi-Mamba\uff09\u8f49\u5316\u70ba\u6df7\u5408\u7248\u672c\uff08Hybrid Phi-Mamba\uff09\uff0c\u4e26\u4f7f\u7528 5B \u500b\u4ee4\u724c\u3002\u5118\u7ba1\u4f7f\u7528\u5c11\u65bc\u5f9e\u982d\u8a13\u7df4\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u7684 1% \u8a13\u7df4\u8cc7\u6599\uff0c\u4f46\u8207\u904e\u53bb\u6240\u6709\u958b\u6e90\u975e\u8b8a\u5f62\u5668\u6a21\u578b\u76f8\u6bd4\uff0cPhi-Mamba \u64c1\u6709\u986f\u8457\u66f4\u5f37\u7684\u6548\u80fd\u3002MOHAWK \u5141\u8a31\u50cf SSM \u9019\u6a23\u7684\u6a21\u578b\u5229\u7528\u6295\u8cc7\u65bc\u8a13\u7df4\u57fa\u65bc\u8b8a\u5f62\u5668\u7684\u67b6\u69cb\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u7a81\u986f\u4e86\u5efa\u69cb\u6b64\u985e\u6a21\u578b\u7684\u65b0\u9014\u5f91\u3002", "author": "Aviv Bick et.al.", "authors": "Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu", "id": "2408.10189v1", "paper_url": "http://arxiv.org/abs/2408.10189v1", "repo": "null"}}