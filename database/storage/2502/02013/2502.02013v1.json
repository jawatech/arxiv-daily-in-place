{"2502.02013": {"publish_time": "2025-02-04", "title": "Layer by Layer: Uncovering Hidden Representations in Language Models", "paper_summary": "From extracting features to generating text, the outputs of large language\nmodels (LLMs) typically rely on their final layers, following the conventional\nwisdom that earlier layers capture only low-level cues. However, our analysis\nshows that intermediate layers can encode even richer representations, often\nimproving performance on a wide range of downstream tasks. To explain and\nquantify these hidden-layer properties, we propose a unified framework of\nrepresentation quality metrics based on information theory, geometry, and\ninvariance to input perturbations. Our framework highlights how each model\nlayer balances information compression and signal preservation, revealing why\nmid-depth embeddings can exceed the last layer's performance. Through extensive\nexperiments on 32 text-embedding tasks and comparisons across model\narchitectures (transformers, state-space models) and domains (language,\nvision), we demonstrate that intermediate layers consistently provide stronger\nfeatures. These findings challenge the standard focus on final-layer embeddings\nand open new directions for model analysis and optimization, including\nstrategic use of mid-layer representations for more robust and accurate AI\nsystems.", "paper_summary_zh": "\u5f9e\u7279\u5fb5\u8403\u53d6\u5230\u6587\u5b57\u751f\u6210\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u51fa\u901a\u5e38\u4ef0\u8cf4\u5176\u6700\u5f8c\u5e7e\u5c64\uff0c\u9075\u5faa\u65e9\u671f\u5c64\u53ea\u6355\u6349\u4f4e\u968e\u7dda\u7d22\u7684\u50b3\u7d71\u667a\u6167\u3002\u7136\u800c\uff0c\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u4e2d\u9593\u5c64\u53ef\u4ee5\u7de8\u78bc\u66f4\u8c50\u5bcc\u7684\u8868\u5fb5\uff0c\u5e38\u5728\u5ee3\u6cdb\u7684\u4e0b\u6e38\u4efb\u52d9\u4e2d\u6539\u5584\u6548\u80fd\u3002\u70ba\u4e86\u89e3\u91cb\u548c\u91cf\u5316\u9019\u4e9b\u96b1\u85cf\u5c64\u5c6c\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7d71\u4e00\u7684\u8868\u5fb5\u54c1\u8cea\u6307\u6a19\u67b6\u69cb\uff0c\u57fa\u65bc\u8cc7\u8a0a\u7406\u8ad6\u3001\u5e7e\u4f55\u548c\u8f38\u5165\u64fe\u52d5\u4e0d\u8b8a\u6027\u3002\u6211\u5011\u7684\u67b6\u69cb\u5f37\u8abf\u6bcf\u500b\u6a21\u578b\u5c64\u5982\u4f55\u5e73\u8861\u8cc7\u8a0a\u58d3\u7e2e\u548c\u8a0a\u865f\u4fdd\u7559\uff0c\u63ed\u793a\u70ba\u4ec0\u9ebc\u4e2d\u6df1\u5ea6\u5d4c\u5165\u53ef\u4ee5\u8d85\u8d8a\u6700\u5f8c\u4e00\u5c64\u7684\u6548\u80fd\u3002\u900f\u904e\u5728 32 \u500b\u6587\u5b57\u5d4c\u5165\u4efb\u52d9\u548c\u8de8\u6a21\u578b\u67b6\u69cb\uff08Transformer\u3001\u72c0\u614b\u7a7a\u9593\u6a21\u578b\uff09\u548c\u9818\u57df\uff08\u8a9e\u8a00\u3001\u8996\u89ba\uff09\u7684\u6bd4\u8f03\u4e2d\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e2d\u9593\u5c64\u59cb\u7d42\u63d0\u4f9b\u66f4\u5f37\u5927\u7684\u7279\u5fb5\u3002\u9019\u4e9b\u767c\u73fe\u6311\u6230\u4e86\u5c0d\u6700\u5f8c\u4e00\u5c64\u5d4c\u5165\u7684\u6a19\u6e96\u95dc\u6ce8\uff0c\u4e26\u70ba\u6a21\u578b\u5206\u6790\u548c\u6700\u4f73\u5316\u958b\u555f\u4e86\u65b0\u65b9\u5411\uff0c\u5305\u62ec\u7b56\u7565\u6027\u4f7f\u7528\u4e2d\u5c64\u8868\u5fb5\uff0c\u4ee5\u7372\u5f97\u66f4\u5f37\u5065\u3001\u66f4\u6e96\u78ba\u7684 AI \u7cfb\u7d71\u3002", "author": "Oscar Skean et.al.", "authors": "Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, Ravid Shwartz-Ziv", "id": "2502.02013v1", "paper_url": "http://arxiv.org/abs/2502.02013v1", "repo": "null"}}