{"2406.12479": {"publish_time": "2024-06-18", "title": "RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding", "paper_summary": "The remote sensing image intelligence understanding model is undergoing a new\nprofound paradigm shift which has been promoted by multi-modal large language\nmodel (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to\nparadigm learning a pre-trained general foundation model followed by an\nadaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets,\nwhich have led to advances in RSI intelligence understanding in the last\ndecade, are no longer suitable for fire-new tasks. We argued that a new dataset\nmust be designed to lighten tasks with the following features: 1)\nGeneralization: training model to learn shared knowledge among tasks and to\nadapt to different tasks; 2) Understanding complex scenes: training model to\nunderstand the fine-grained attribute of the objects of interest, and to be\nable to describe the scene with natural language; 3) Reasoning: training model\nto be able to realize high-level visual reasoning. In this paper, we designed a\nhigh-quality, diversified, and unified multimodal instruction-following dataset\nfor RSI understanding produced by GPT-4V and existing datasets, which we called\nRS-GPT4V. To achieve generalization, we used a (Question, Answer) which was\ndeduced from GPT-4V via instruction-following to unify the tasks such as\ncaptioning and localization; To achieve complex scene, we proposed a\nhierarchical instruction description with local strategy in which the\nfine-grained attributes of the objects and their spatial relationships are\ndescribed and global strategy in which all the local information are integrated\nto yield detailed instruction descript; To achieve reasoning, we designed\nmultiple-turn QA pair to provide the reasoning ability for a model. The\nempirical results show that the fine-tuned MLLMs by RS-GPT4V can describe\nfine-grained information. The dataset is available at:\nhttps://github.com/GeoX-Lab/RS-GPT4V.", "paper_summary_zh": "\u9059\u611f\u5f71\u50cf\u667a\u6167\u7406\u89e3\u6a21\u578b\u6b63\u7d93\u6b77\u4e00\u5834\u65b0\u7684\u91cd\u5927\u5178\u7bc4\u8f49\u79fb\uff0c\u9019\u662f\u7531\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u63a8\u52d5\u7684\uff0c\u5373\u5f9e\u5b78\u7fd2\u9818\u57df\u6a21\u578b (LaDM) \u7684\u5178\u7bc4\u8f49\u79fb\u5230\u5b78\u7fd2\u9810\u8a13\u7df4\u901a\u7528\u57fa\u790e\u6a21\u578b\uff0c\u7136\u5f8c\u518d\u5b78\u7fd2\u81ea\u9069\u61c9\u9818\u57df\u6a21\u578b (LaGD) \u7684\u5178\u7bc4\u3002\u5728\u65b0\u7684 LaGD \u5178\u7bc4\u4e0b\uff0c\u5728\u904e\u53bb\u5341\u5e74\u4e2d\u5c0e\u81f4 RSI \u667a\u6167\u7406\u89e3\u9032\u6b65\u7684\u820a\u8cc7\u6599\u96c6\u4e0d\u518d\u9069\u5408\u5168\u65b0\u7684\u4efb\u52d9\u3002\u6211\u5011\u8a8d\u70ba\u5fc5\u9808\u8a2d\u8a08\u4e00\u500b\u65b0\u7684\u8cc7\u6599\u96c6\uff0c\u4ee5\u6e1b\u8f15\u5177\u6709\u4ee5\u4e0b\u7279\u5fb5\u7684\u4efb\u52d9\uff1a1) \u6982\u62ec\u5316\uff1a\u8a13\u7df4\u6a21\u578b\u5b78\u7fd2\u4efb\u52d9\u4e4b\u9593\u7684\u5171\u4eab\u77e5\u8b58\uff0c\u4e26\u9069\u61c9\u4e0d\u540c\u7684\u4efb\u52d9\uff1b2) \u7406\u89e3\u8907\u96dc\u5834\u666f\uff1a\u8a13\u7df4\u6a21\u578b\u7406\u89e3\u611f\u8208\u8da3\u7269\u9ad4\u7684\u7d30\u7c92\u5ea6\u5c6c\u6027\uff0c\u4e26\u80fd\u5920\u7528\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u5834\u666f\uff1b3) \u63a8\u7406\uff1a\u8a13\u7df4\u6a21\u578b\u80fd\u5920\u5be6\u73fe\u9ad8\u5c64\u7d1a\u8996\u89ba\u63a8\u7406\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u9ad8\u54c1\u8cea\u3001\u591a\u6a23\u5316\u4e14\u7d71\u4e00\u7684\u591a\u6a21\u614b\u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\uff0c\u7528\u65bc RSI \u7406\u89e3\uff0c\u7531 GPT-4V \u548c\u73fe\u6709\u8cc7\u6599\u96c6\u751f\u6210\uff0c\u6211\u5011\u7a31\u4e4b\u70ba RS-GPT4V\u3002\u70ba\u4e86\u5be6\u73fe\u6982\u62ec\u5316\uff0c\u6211\u5011\u4f7f\u7528\u4e86\u4e00\u500b (\u554f\u984c\u3001\u7b54\u6848)\uff0c\u8a72 (\u554f\u984c\u3001\u7b54\u6848) \u662f\u901a\u904e\u6307\u4ee4\u9075\u5faa\u5f9e GPT-4V \u63a8\u5c0e\u51fa\u4f86\u7684\uff0c\u4ee5\u7d71\u4e00\u8af8\u5982\u6a19\u984c\u548c\u5b9a\u4f4d\u4e4b\u985e\u7684\u4efb\u52d9\uff1b\u70ba\u4e86\u5be6\u73fe\u8907\u96dc\u5834\u666f\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5177\u6709\u5c40\u90e8\u7b56\u7565\u7684\u5206\u5c64\u6307\u4ee4\u63cf\u8ff0\uff0c\u5176\u4e2d\u63cf\u8ff0\u4e86\u7269\u9ad4\u7684\u7d30\u7c92\u5ea6\u5c6c\u6027\u53ca\u5176\u7a7a\u9593\u95dc\u4fc2\uff0c\u4ee5\u53ca\u5c07\u6240\u6709\u5c40\u90e8\u8cc7\u8a0a\u6574\u5408\u8d77\u4f86\u4ee5\u7522\u751f\u8a73\u7d30\u6307\u4ee4\u63cf\u8ff0\u7684\u5168\u5c40\u7b56\u7565\uff1b\u70ba\u4e86\u5be6\u73fe\u63a8\u7406\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u591a\u8f2a\u554f\u7b54\u5c0d\uff0c\u70ba\u6a21\u578b\u63d0\u4f9b\u63a8\u7406\u80fd\u529b\u3002\u7d93\u9a57\u7d50\u679c\u8868\u660e\uff0cRS-GPT4V \u5fae\u8abf\u7684 MLLM \u53ef\u4ee5\u63cf\u8ff0\u7d30\u7c92\u5ea6\u8cc7\u8a0a\u3002\u8cc7\u6599\u96c6\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\nhttps://github.com/GeoX-Lab/RS-GPT4V\u3002", "author": "Linrui Xu et.al.", "authors": "Linrui Xu, Ling Zhao, Wang Guo, Qiujun Li, Kewang Long, Kaiqi Zou, Yuhan Wang, Haifeng Li", "id": "2406.12479v1", "paper_url": "http://arxiv.org/abs/2406.12479v1", "repo": "https://github.com/geox-lab/rs-gpt4v"}}