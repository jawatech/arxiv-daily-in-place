
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-08**|**Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures**|Ziyuan Huang et.al.|[2501.04700v1](http://arxiv.org/abs/2501.04700v1)|null|
|**2025-01-08**|**Grokking at the Edge of Numerical Stability**|Lucas Prieto et.al.|[2501.04697v1](http://arxiv.org/abs/2501.04697v1)|[link](https://github.com/lucasprietoal/grokking-at-the-edge-of-numerical-stability)|
|**2025-01-08**|**EpiCoder: Encompassing Diversity and Complexity in Code Generation**|Yaoxiang Wang et.al.|[2501.04694v1](http://arxiv.org/abs/2501.04694v1)|null|
|**2025-01-08**|**Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding**|Joshua Jones et.al.|[2501.04693v1](http://arxiv.org/abs/2501.04693v1)|null|
|**2025-01-08**|**URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**|Ruilin Luo et.al.|[2501.04686v1](http://arxiv.org/abs/2501.04686v1)|null|
|**2025-01-08**|**Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought**|Violet Xiang et.al.|[2501.04682v1](http://arxiv.org/abs/2501.04682v1)|null|
|**2025-01-08**|**Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations**|Archita Srivastava et.al.|[2501.04675v1](http://arxiv.org/abs/2501.04675v1)|null|
|**2025-01-08**|**DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests**|Charles Corbière et.al.|[2501.04671v1](http://arxiv.org/abs/2501.04671v1)|null|
|**2025-01-08**|**On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena**|Tarek Naous et.al.|[2501.04662v1](http://arxiv.org/abs/2501.04662v1)|null|
|**2025-01-08**|**Assessing Language Comprehension in Large Language Models Using Construction Grammar**|Wesley Scivetti et.al.|[2501.04661v1](http://arxiv.org/abs/2501.04661v1)|null|
|**2025-01-08**|**Multi-task retriever fine-tuning for domain-specific and efficient RAG**|Patrice Béchard et.al.|[2501.04652v1](http://arxiv.org/abs/2501.04652v1)|null|
|**2025-01-08**|**FlairGPT: Repurposing LLMs for Interior Designs**|Gabrielle Littlefair et.al.|[2501.04648v1](http://arxiv.org/abs/2501.04648v1)|null|
|**2025-01-08**|**Knowledge Retrieval Based on Generative AI**|Te-Lun Yang et.al.|[2501.04635v1](http://arxiv.org/abs/2501.04635v1)|null|
|**2025-01-08**|**MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation**|Daniele Molino et.al.|[2501.04614v2](http://arxiv.org/abs/2501.04614v2)|null|
|**2025-01-08**|**Quantum-inspired Embeddings Projection and Similarity Metrics for Representation Learning**|Ivan Kankeu et.al.|[2501.04591v1](http://arxiv.org/abs/2501.04591v1)|null|
|**2025-01-08**|**Federated-Continual Dynamic Segmentation of Histopathology guided by Barlow Continuity**|Niklas Babendererde et.al.|[2501.04588v1](http://arxiv.org/abs/2501.04588v1)|null|
|**2025-01-08**|**InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection**|Yuhang Liu et.al.|[2501.04575v1](http://arxiv.org/abs/2501.04575v1)|[link](https://github.com/reallm-labs/infiguiagent)|
|**2025-01-08**|**Supervision-free Vision-Language Alignment**|Giorgio Giannone et.al.|[2501.04568v1](http://arxiv.org/abs/2501.04568v1)|null|
|**2025-01-08**|**OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis**|Run Luo et.al.|[2501.04561v1](http://arxiv.org/abs/2501.04561v1)|null|
|**2025-01-08**|**rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking**|Xinyu Guan et.al.|[2501.04519v1](http://arxiv.org/abs/2501.04519v1)|null|
|**2025-01-08**|**Improving Image Captioning by Mimicking Human Reformulation Feedback at Inference-time**|Uri Berger et.al.|[2501.04513v1](http://arxiv.org/abs/2501.04513v1)|null|
|**2025-01-08**|**CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection**|Ruijun Feng et.al.|[2501.04510v1](http://arxiv.org/abs/2501.04510v1)|null|
|**2025-01-08**|**Developing a Modular Compiler for a Subset of a C-like Language**|Debasish Dutta et.al.|[2501.04503v1](http://arxiv.org/abs/2501.04503v1)|null|
|**2025-01-08**|**Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction**|Guofeng Yang et.al.|[2501.04487v1](http://arxiv.org/abs/2501.04487v1)|null|
|**2025-01-08**|**Research on environment perception and behavior prediction of intelligent UAV based on semantic communication**|Kechong Ren et.al.|[2501.04480v1](http://arxiv.org/abs/2501.04480v1)|null|
|**2025-01-08**|**When LLMs Struggle: Reference-less Translation Evaluation for Low-resource Languages**|Archchana Sindhujan et.al.|[2501.04473v1](http://arxiv.org/abs/2501.04473v1)|null|
|**2025-01-08**|**Hybrid Artificial Intelligence Strategies for Drone Navigation**|Rubén San-Segundo et.al.|[2501.04472v1](http://arxiv.org/abs/2501.04472v1)|null|
|**2025-01-08**|**Hidden Entity Detection from GitHub Leveraging Large Language Models**|Lu Gan et.al.|[2501.04455v1](http://arxiv.org/abs/2501.04455v1)|[link](https://github.com/louisegan514/hidden-entity-detection-from-github-leveraging-llms)|
|**2025-01-08**|**A novel Facial Recognition technique with Focusing on Masked Faces**|Dana A Abdullah et.al.|[2501.04444v1](http://arxiv.org/abs/2501.04444v1)|null|
|**2025-01-08**|**Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions**|Doaa Mahmud et.al.|[2501.04437v1](http://arxiv.org/abs/2501.04437v1)|null|
|**2025-01-08**|**Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions**|Na Yan et.al.|[2501.04436v1](http://arxiv.org/abs/2501.04436v1)|null|
|**2025-01-08**|**A Digital Shadow for Modeling, Studying and Preventing Urban Crime**|Juan Palma-Borda et.al.|[2501.04435v1](http://arxiv.org/abs/2501.04435v1)|null|
|**2025-01-08**|**End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark: Leveraging Large Language Model Using Integrated Approach**|H. M. Shadman Tabib et.al.|[2501.04425v1](http://arxiv.org/abs/2501.04425v1)|null|
|**2025-01-08**|**NSA: Neuro-symbolic ARC Challenge**|Paweł Batorski et.al.|[2501.04424v1](http://arxiv.org/abs/2501.04424v1)|[link](https://github.com/batorskq/nsa)|
|**2025-01-08**|**User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation**|Krisztian Balog et.al.|[2501.04410v1](http://arxiv.org/abs/2501.04410v1)|null|
|**2025-01-08**|**SEO: Stochastic Experience Optimization for Large Language Models**|Jitao Xu et.al.|[2501.04393v1](http://arxiv.org/abs/2501.04393v1)|null|
|**2025-01-08**|**On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis**|Yekun Ke et.al.|[2501.04377v1](http://arxiv.org/abs/2501.04377v1)|null|
|**2025-01-08**|**DispFormer: Pretrained Transformer for Flexible Dispersion Curve Inversion from Global Synthesis to Regional Applications**|Feng Liu et.al.|[2501.04366v1](http://arxiv.org/abs/2501.04366v1)|[link](https://github.com/liufeng2317/DispFormer)|
|**2025-01-08**|**Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation**|Terrance Yu-Hao Chen et.al.|[2501.04359v1](http://arxiv.org/abs/2501.04359v1)|null|
|**2025-01-08**|**Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting**|Dong-Hai Zhu et.al.|[2501.04341v1](http://arxiv.org/abs/2501.04341v1)|[link](https://github.com/zdhgreat/isp-2)|
|**2025-01-08**|**Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts**|Preethi Seshadri et.al.|[2501.04316v1](http://arxiv.org/abs/2501.04316v1)|null|
|**2025-01-08**|**RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation**|Jun Liu et.al.|[2501.04315v1](http://arxiv.org/abs/2501.04315v1)|null|
|**2025-01-08**|**LLM4SR: A Survey on Large Language Models for Scientific Research**|Ziming Luo et.al.|[2501.04306v1](http://arxiv.org/abs/2501.04306v1)|[link](https://github.com/du-nlp-lab/llm4sr)|
|**2025-01-08**|**Multimodal Graph Constrastive Learning and Prompt for ChartQA**|Yue Dai et.al.|[2501.04303v1](http://arxiv.org/abs/2501.04303v1)|null|
|**2025-01-08**|**H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving**|Siran Chen et.al.|[2501.04302v1](http://arxiv.org/abs/2501.04302v1)|null|
|**2025-01-08**|**Circuit Complexity Bounds for Visual Autoregressive Model**|Yekun Ke et.al.|[2501.04299v1](http://arxiv.org/abs/2501.04299v1)|null|
|**2025-01-08**|**MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge**|Zijiang Yang et.al.|[2501.04292v1](http://arxiv.org/abs/2501.04292v1)|null|
|**2025-01-08**|**Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models**|Bahman Torkamandi et.al.|[2501.04286v1](http://arxiv.org/abs/2501.04286v1)|null|
|**2025-01-08**|**Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images**|Yuze Wang et.al.|[2501.04283v1](http://arxiv.org/abs/2501.04283v1)|null|
|**2025-01-08**|**Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning**|Lang Xu et.al.|[2501.04266v1](http://arxiv.org/abs/2501.04266v1)|null|
|**2025-01-08**|**Integrated Offline and Online Learning to Solve a Large Class of Scheduling Problems**|Anbang Liu et.al.|[2501.04253v1](http://arxiv.org/abs/2501.04253v1)|null|
|**2025-01-08**|**IOLBENCH: Benchmarking LLMs on Linguistic Reasoning**|Satyam Goyal et.al.|[2501.04249v1](http://arxiv.org/abs/2501.04249v1)|null|
|**2025-01-08**|**Agent Laboratory: Using LLM Agents as Research Assistants**|Samuel Schmidgall et.al.|[2501.04227v1](http://arxiv.org/abs/2501.04227v1)|null|
|**2025-01-08**|**Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images**|Ren Tasai et.al.|[2501.04217v1](http://arxiv.org/abs/2501.04217v1)|null|
|**2025-01-08**|**UPAQ: A Framework for Real-Time and Energy-Efficient 3D Object Detection in Autonomous Vehicles**|Abhishek Balasubramaniam et.al.|[2501.04213v1](http://arxiv.org/abs/2501.04213v1)|null|
|**2025-01-08**|**CURing Large Models: Compression via CUR Decomposition**|Sanghyeon Park et.al.|[2501.04211v1](http://arxiv.org/abs/2501.04211v1)|null|
|**2025-01-08**|**Generative Dataset Distillation Based on Self-knowledge Distillation**|Longzhen Li et.al.|[2501.04202v1](http://arxiv.org/abs/2501.04202v1)|null|
|**2025-01-07**|**HIVEX: A High-Impact Environment Suite for Multi-Agent Research (extended version)**|Philipp D. Siedler et.al.|[2501.04180v1](http://arxiv.org/abs/2501.04180v1)|null|
|**2025-01-07**|**Multimodal Multihop Source Retrieval for Web Question Answering**|Navya Yarrabelly et.al.|[2501.04173v1](http://arxiv.org/abs/2501.04173v1)|null|
|**2025-01-07**|**Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation**|Alireza Salemi et.al.|[2501.04167v1](http://arxiv.org/abs/2501.04167v1)|null|
|**2025-01-07**|**MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation**|Siddharth Joshi et.al.|[2501.04155v1](http://arxiv.org/abs/2501.04155v1)|[link](https://github.com/sjoshi804/mm-gen)|
|**2025-01-07**|**Multilingual Open QA on the MIA Shared Task**|Navya Yarrabelly et.al.|[2501.04153v1](http://arxiv.org/abs/2501.04153v1)|null|
|**2025-01-07**|**BiasGuard: Guardrailing Fairness in Machine Learning Production Systems**|Nurit Cohen-Inger et.al.|[2501.04142v1](http://arxiv.org/abs/2501.04142v1)|null|
|**2025-01-07**|**"Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?**|Benjamin Reichman et.al.|[2501.04138v1](http://arxiv.org/abs/2501.04138v1)|null|
|**2025-01-07**|**Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach**|Hicham Assoudi et.al.|[2501.04136v1](http://arxiv.org/abs/2501.04136v1)|null|
|**2025-01-07**|**Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles**|Yuxi Xia et.al.|[2501.03991v1](http://arxiv.org/abs/2501.03991v1)|null|
|**2025-01-07**|**Semantically Cohesive Word Grouping in Indian Languages**|N J Karthika et.al.|[2501.03988v1](http://arxiv.org/abs/2501.03988v1)|null|
|**2025-01-07**|**VLM-driven Behavior Tree for Context-aware Task Planning**|Naoki Wake et.al.|[2501.03968v1](http://arxiv.org/abs/2501.03968v1)|null|
|**2025-01-07**|**Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States**|Jurgita Kapočiūtė-Dzikienė et.al.|[2501.03952v1](http://arxiv.org/abs/2501.03952v1)|null|
|**2025-01-07**|**Synthetic Data Privacy Metrics**|Amy Steier et.al.|[2501.03941v1](http://arxiv.org/abs/2501.03941v1)|null|
|**2025-01-07**|**Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection**|Pablo Miralles-González et.al.|[2501.03940v1](http://arxiv.org/abs/2501.03940v1)|null|
|**2025-01-07**|**Multi-armed Bandit and Backbone boost Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problems**|Long Wang et.al.|[2501.04072v1](http://arxiv.org/abs/2501.04072v1)|[link](https://github.com/jhl-hust/mabb-lkh)|
|**2025-01-07**|**From Newswire to Nexus: Using text-based actor embeddings and transformer networks to forecast conflict dynamics**|Mihai Croicu et.al.|[2501.03928v1](http://arxiv.org/abs/2501.03928v1)|null|
|**2025-01-07**|**Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**|Ramya Jonnala et.al.|[2501.03904v1](http://arxiv.org/abs/2501.03904v1)|null|
|**2025-01-07**|**LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**|Shaolei Zhang et.al.|[2501.03895v1](http://arxiv.org/abs/2501.03895v1)|[link](https://github.com/ictnlp/llava-mini)|
|**2025-01-07**|**Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies**|Kexin Gu Baugh et.al.|[2501.03888v1](http://arxiv.org/abs/2501.03888v1)|null|
|**2025-01-07**|**AlphaPO -- Reward shape matters for LLM alignment**|Aman Gupta et.al.|[2501.03884v1](http://arxiv.org/abs/2501.03884v1)|null|
|**2025-01-07**|**CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds**|Keonwoo Kim et.al.|[2501.03879v1](http://arxiv.org/abs/2501.03879v1)|null|
|**2025-01-07**|**Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on Norwegian Dialectal Slot and Intent Detection**|Verena Blaschke et.al.|[2501.03870v1](http://arxiv.org/abs/2501.03870v1)|null|
|**2025-01-07**|**Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A Multi-Dialectal Bavarian Case Study**|Xaver Maria Krückl et.al.|[2501.03863v1](http://arxiv.org/abs/2501.03863v1)|[link](https://github.com/mainlp/auxtasks-bavarian-sid)|
|**2025-01-07**|**Progressive Document-level Text Simplification via Large Language Models**|Dengzhao Fang et.al.|[2501.03857v1](http://arxiv.org/abs/2501.03857v1)|null|
|**2025-01-07**|**BabyLMs for isiXhosa: Data-Efficient Language Modelling in a Low-Resource Context**|Alexis Matzopoulos et.al.|[2501.03855v1](http://arxiv.org/abs/2501.03855v1)|null|
|**2025-01-07**|**Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control**|Zekai Gu et.al.|[2501.03847v2](http://arxiv.org/abs/2501.03847v2)|[link](https://github.com/igl-hkust/diffusionasshader)|
|**2025-01-07**|**More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives**|Xiaoqing Zhang et.al.|[2501.04070v2](http://arxiv.org/abs/2501.04070v2)|[link](https://github.com/xiaoqzhwhu/dr-icl)|
|**2025-01-07**|**BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study**|Atharva Mutsaddi et.al.|[2501.03843v1](http://arxiv.org/abs/2501.03843v1)|null|
|**2025-01-07**|**SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis**|Runci Bai et.al.|[2501.03836v1](http://arxiv.org/abs/2501.03836v1)|null|
|**2025-01-07**|**Three-dimensional attention Transformer for state evaluation in real-time strategy games**|Yanqing Ye et.al.|[2501.03832v1](http://arxiv.org/abs/2501.03832v1)|null|
|**2025-01-07**|**Investigating the Impact of Data Selection Strategies on Language Model Performance**|Jiayao Gu et.al.|[2501.03826v1](http://arxiv.org/abs/2501.03826v1)|[link](https://github.com/jgu13/hir-hybrid-importance-resampling-for-language-models)|
|**2025-01-07**|**Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in Ultrasound Imaging**|Simon W. Penninga et.al.|[2501.03825v1](http://arxiv.org/abs/2501.03825v1)|null|
|**2025-01-07**|**Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks**|Weilong Yang et.al.|[2501.03824v1](http://arxiv.org/abs/2501.03824v1)|null|
|**2025-01-07**|**Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits**|Sung-Feng Huang et.al.|[2501.03805v1](http://arxiv.org/abs/2501.03805v1)|null|
|**2025-01-07**|**Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model Matching**|Ahmed Maged et.al.|[2501.03795v1](http://arxiv.org/abs/2501.03795v1)|null|
|**2025-01-07**|**Explainable Reinforcement Learning for Formula One Race Strategy**|Devin Thomas et.al.|[2501.04068v1](http://arxiv.org/abs/2501.04068v1)|null|
|**2025-01-07**|**How to Select Pre-Trained Code Models for Reuse? A Learning Perspective**|Zhangqian Bi et.al.|[2501.03783v1](http://arxiv.org/abs/2501.03783v1)|null|
|**2025-01-07**|**SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment**|Siyuan Zhao et.al.|[2501.03764v1](http://arxiv.org/abs/2501.03764v1)|null|
|**2025-01-07**|**Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series**|Yuxiao Hu et.al.|[2501.03747v1](http://arxiv.org/abs/2501.03747v1)|null|
|**2025-01-07**|**Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy**|Jamie Todd et.al.|[2501.04067v1](http://arxiv.org/abs/2501.04067v1)|null|
|**2025-01-07**|**Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein**|Xiaotong Guo et.al.|[2501.03722v1](http://arxiv.org/abs/2501.03722v1)|null|
|**2025-01-07**|**Unsupervised Speech Segmentation: A General Approach Using Speech Language Models**|Avishai Elmakies et.al.|[2501.03711v1](http://arxiv.org/abs/2501.03711v1)|[link](https://github.com/avishaielmakies/unsupervised_speech_segmentation_using_slm)|
|**2025-01-07**|**AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features**|Ruochen Zhang et.al.|[2501.03700v1](http://arxiv.org/abs/2501.03700v1)|null|

#### Abstracts
##### **Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures**
2501.04700v1 by Ziyuan Huang, Mark Newman, Maria Vaida, Srikar Bellur, Roozbeh Sadeghian, Andrew Siu, Hui Wang, Kevin Huggins

This study examined the viability of enhancing the prediction accuracy of
artificial neural networks (ANNs) in image classification tasks by developing
ANNs with evolution patterns similar to those of biological neural networks.
ResNet is a widely used family of neural networks with both deep and wide
variants; therefore, it was selected as the base model for our investigation.
The aim of this study is to improve the image classification performance of
ANNs via a novel approach inspired by the biological nervous system
architecture of planarians, which comprises a brain and two nerve cords. We
believe that the unique neural architecture of planarians offers valuable
insights into the performance enhancement of ANNs. The proposed planarian
neural architecture-based neural network was evaluated on the CIFAR-10 and
CIFAR-100 datasets. Our results indicate that the proposed method exhibits
higher prediction accuracy than the baseline neural network models in image
classification tasks. These findings demonstrate the significant potential of
biologically inspired neural network architectures in improving the performance
of ANNs in a wide range of applications.

摘要：本研究探討了透過開發具有與生物神經網路相似的演化模式的人工神經網路 (ANN)，來提升人工神經網路在影像分類任務中預測精確度的可行性。ResNet 是一個廣泛使用的神經網路系列，具有深層和廣泛的變異；因此，它被選為我們研究的基本模型。本研究的目的是透過一種新穎的方法來提升人工神經網路的影像分類效能，此方法的靈感來自於扁蟲的生物神經系統架構，其中包含一個大腦和兩條神經索。我們相信扁蟲獨特的神經架構能為提升人工神經網路的效能提供有價值的見解。所提出的基於扁蟲神經架構的神經網路在 CIFAR-10 和 CIFAR-100 資料集上進行評估。我們的結果顯示，所提出的方法在影像分類任務中展現出比基準神經網路模型更高的預測精確度。這些發現證明了受生物啟發的神經網路架構在提升人工神經網路在廣泛應用中的效能方面具有顯著的潛力。

##### **Grokking at the Edge of Numerical Stability**
2501.04697v1 by Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal

Grokking, the sudden generalization that occurs after prolonged overfitting,
is a surprising phenomenon challenging our understanding of deep learning.
Although significant progress has been made in understanding grokking, the
reasons behind the delayed generalization and its dependence on regularization
remain unclear. In this work, we argue that without regularization, grokking
tasks push models to the edge of numerical stability, introducing floating
point errors in the Softmax function, which we refer to as Softmax Collapse
(SC). We demonstrate that SC prevents grokking and that mitigating SC enables
grokking without regularization. Investigating the root cause of SC, we find
that beyond the point of overfitting, the gradients strongly align with what we
call the na\"ive loss minimization (NLM) direction. This component of the
gradient does not alter the model's predictions but decreases the loss by
scaling the logits, typically by scaling the weights along their current
direction. We show that this scaling of the logits explains the delay in
generalization characteristic of grokking and eventually leads to SC, halting
further learning. To validate our hypotheses, we introduce two key
contributions that address the challenges in grokking tasks: StableMax, a new
activation function that prevents SC and enables grokking without
regularization, and $\perp$Grad, a training algorithm that promotes quick
generalization in grokking tasks by preventing NLM altogether. These
contributions provide new insights into grokking, elucidating its delayed
generalization, reliance on regularization, and the effectiveness of existing
grokking-inducing methods. Code for this paper is available at
https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.

摘要：<paragraph>過擬合後突然發生的廣泛化，稱為「頓悟」，是一個令人驚訝的現象，挑戰了我們對深度學習的理解。雖然在理解頓悟方面已取得重大進展，但延遲廣泛化背後的原因及其對正則化的依賴性仍不清楚。在這項工作中，我們論證說，在沒有正則化的情況下，頓悟任務會將模型推到數值穩定性的邊緣，在 Softmax 函數中引入浮點誤差，我們稱之為 Softmax 崩潰 (SC)。我們證明 SC 會阻止頓悟，而減輕 SC 可以讓頓悟在沒有正則化的情況下發生。在調查 SC 的根本原因時，我們發現，在過擬合點之外，梯度與我們稱之為「樸素損失最小化」(NLM) 方向強烈對齊。梯度的這個組成部分不會改變模型的預測，但會透過調整 logit 來降低損失，通常是沿著其當前方向調整權重。我們表明，logit 的這種調整解釋了頓悟特有的廣泛化延遲，並最終導致 SC，阻礙進一步學習。為了驗證我們的假設，我們提出了兩個關鍵貢獻，以解決頓悟任務中的挑戰：StableMax，這是一個新的激活函數，可以防止 SC 並在沒有正則化的情況下實現頓悟，以及 $\perp$Grad，這是一種訓練演算法，透過完全防止 NLM，在頓悟任務中促進快速廣泛化。這些貢獻為頓悟提供了新的見解，闡明了其延遲廣泛化、對正則化的依賴性，以及現有頓悟誘導方法的有效性。本文的程式碼可在 https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability 取得。</paragraph>

##### **EpiCoder: Encompassing Diversity and Complexity in Code Generation**
2501.04694v1 by Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, Scarlett Li

Effective instruction tuning is indispensable for optimizing code LLMs,
aligning model behavior with user expectations and enhancing model performance
in real-world applications. However, most existing methods focus on code
snippets, which are limited to specific functionalities and rigid structures,
restricting the complexity and diversity of the synthesized data. To address
these limitations, we introduce a novel feature tree-based synthesis framework
inspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic
structure of code, our framework models semantic relationships between code
elements, enabling the generation of more nuanced and diverse data. The feature
tree is constructed from raw data and refined iteratively to increase the
quantity and diversity of the extracted features. This process enables the
identification of more complex patterns and relationships within the code. By
sampling subtrees with controlled depth and breadth, our framework allows
precise adjustments to the complexity of the generated code, supporting a wide
range of tasks from simple function-level operations to intricate multi-file
scenarios. We fine-tuned widely-used base models to create the EpiCoder series,
achieving state-of-the-art performance at both the function and file levels
across multiple benchmarks. Notably, empirical evidence indicates that our
approach shows significant potential in synthesizing highly complex
repository-level code data. Further analysis elucidates the merits of this
approach by rigorously assessing data complexity and diversity through software
engineering principles and LLM-as-a-judge method.

摘要：有效的指令調整對於最佳化程式碼 LLM 至關重要，可將模型行為與使用者預期保持一致，並提升模型在實際應用中的效能。然而，現有的方法大多著重於程式碼片段，僅限於特定功能和僵化的結構，限制了合成資料的複雜性和多樣性。為了解決這些限制，我們引入一種新穎的基於特徵樹的合成架構，靈感來自抽象語法樹 (AST)。與擷取程式碼語法結構的 AST 不同，我們的架構會對程式碼元素之間的語意關係建模，能夠產生更細緻且多樣化的資料。特徵樹是由原始資料建構而成，並反覆精煉以增加提取特徵的數量和多樣性。此程序能識別程式碼中更複雜的模式和關係。透過以受控深度和廣度取樣子樹，我們的架構允許精確調整產生程式碼的複雜度，支援從簡單函式層級操作到複雜多檔案場景的各種任務。我們微調廣泛使用的基礎模型以建立 EpiCoder 系列，在函式和檔案層級上於多個基準測試中達成最先進的效能。值得注意的是，實證證據顯示，我們的做法在合成高度複雜的儲存庫層級程式碼資料方面具有顯著的潛力。進一步的分析透過軟體工程原則和 LLM 作為評判方法，嚴謹地評估資料的複雜性和多樣性，闡明此方法的優點。

##### **Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding**
2501.04693v1 by Joshua Jones, Oier Mees, Carmelo Sferrazza, Kyle Stachowicz, Pieter Abbeel, Sergey Levine

Interacting with the world is a multi-sensory experience: achieving effective
general-purpose interaction requires making use of all available modalities --
including vision, touch, and audio -- to fill in gaps from partial observation.
For example, when vision is occluded reaching into a bag, a robot should rely
on its senses of touch and sound. However, state-of-the-art generalist robot
policies are typically trained on large datasets to predict robot actions
solely from visual and proprioceptive observations. In this work, we propose
FuSe, a novel approach that enables finetuning visuomotor generalist policies
on heterogeneous sensor modalities for which large datasets are not readily
available by leveraging natural language as a common cross-modal grounding. We
combine a multimodal contrastive loss with a sensory-grounded language
generation loss to encode high-level semantics. In the context of robot
manipulation, we show that FuSe enables performing challenging tasks that
require reasoning jointly over modalities such as vision, touch, and sound in a
zero-shot setting, such as multimodal prompting, compositional cross-modal
prompting, and descriptions of objects it interacts with. We show that the same
recipe is applicable to widely different generalist policies, including both
diffusion-based generalist policies and large vision-language-action (VLA)
models. Extensive experiments in the real world show that FuSeis able to
increase success rates by over 20% compared to all considered baselines.

摘要：與世界互動是一種多感官體驗：要達成有效的通用互動，必須利用所有可用的方式，包括視覺、觸覺和聽覺，以填補部分觀察的空白。例如，當視覺被遮蔽時，機器人應依賴其觸覺和聽覺。然而，最先進的通用機器人策略通常在大型資料集上訓練，以僅從視覺和本體感覺觀察來預測機器人的動作。在這項工作中，我們提出了 FuSe，一種新穎的方法，它能微調視覺運動通用策略，針對大型資料集不易取得的異質感測器模式，利用自然語言作為一種通用的跨模式基礎。我們將多模式對比損失與感官基礎語言生成損失結合，以編碼高層語義。在機器人操作的背景下，我們展示了 FuSe 能夠執行具有挑戰性的任務，這些任務需要在零次學習的設定中，對視覺、觸覺和聲音等模式進行聯合推理，例如多模式提示、組合式跨模式提示，以及與其互動的物體描述。我們展示了相同的配方適用於各種不同的通用策略，包括基於擴散的通用策略和大型視覺語言動作 (VLA) 模型。現實世界中的大量實驗表明，與所有考慮的基準線相比，FuSe 能夠將成功率提高 20% 以上。

##### **URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**
2501.04686v1 by Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang

Chain-of-thought (CoT) reasoning has been widely applied in the mathematical
reasoning of Large Language Models (LLMs). Recently, the introduction of
derivative process supervision on CoT trajectories has sparked discussions on
enhancing scaling capabilities during test time, thereby boosting the potential
of these models. However, in multimodal mathematical reasoning, the scarcity of
high-quality CoT training data has hindered existing models from achieving
high-precision CoT reasoning and has limited the realization of reasoning
potential during test time. In this work, we propose a three-module synthesis
strategy that integrates CoT distillation, trajectory-format rewriting, and
format unification. It results in a high-quality CoT reasoning instruction
fine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively
validate the state-of-the-art (SOTA) performance of the trained URSA-7B model
on multiple multimodal mathematical benchmarks. For test-time scaling, we
introduce a data synthesis strategy that automatically generates process
annotation datasets, known as DualMath-1.1M, focusing on both interpretation
and logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT
reasoning capabilities to robust supervision abilities. The trained URSA-RM-7B
acts as a verifier, effectively enhancing the performance of URSA-7B at test
time. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)
verifying capabilities, showcasing its generalization. Model weights, training
data and code will be open-sourced.

摘要：<paragraph>鏈條思考（CoT）推理已廣泛應用於大型語言模型（LLM）的數學推理中。最近，在 CoT 軌跡中引入導數過程監督，引發了關於在測試期間增強規模化能力的討論，從而提升了這些模型的潛力。然而，在多模態數學推理中，高品質 CoT 訓練資料的稀缺性阻礙了現有模型實現高精度的 CoT 推理，並限制了在測試期間實現推理潛力的可能性。在這項工作中，我們提出了一種三模組合成策略，它整合了 CoT 蒸餾、軌跡格式重寫和格式統一。它產生了一個高品質的 CoT 推理指令微調資料集，用於多模態數學，MMathCoT-1M。我們全面驗證了訓練後的 URSA-7B 模型在多個多模態數學基準上的最新技術（SOTA）效能。對於測試時間縮放，我們引入了一種資料合成策略，它自動產生過程註解資料集，稱為 DualMath-1.1M，重點關注解釋和邏輯。通過進一步訓練 URSA-7B 在 DualMath-1.1M 上，我們從 CoT 推理能力過渡到強大的監督能力。訓練後的 URSA-RM-7B 作為驗證器，有效地增強了 URSA-7B 在測試時間的效能。URSA-RM-7B 還展示了出色的分布外（OOD）驗證能力，展示了它的泛化性。模型權重、訓練資料和程式碼將會開源。</paragraph>

##### **Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought**
2501.04682v1 by Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, Chelsea Finn

We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends
traditional Chain-of-Thought (CoT) by explicitly modeling the underlying
reasoning required to arrive at a particular CoT. We present empirical evidence
from state-of-the-art models exhibiting behaviors consistent with in-context
search, and explore methods for producing Meta-CoT via process supervision,
synthetic data generation, and search algorithms. Finally, we outline a
concrete pipeline for training a model to produce Meta-CoTs, incorporating
instruction tuning with linearized search traces and reinforcement learning
post-training. Finally, we discuss open research questions, including scaling
laws, verifier roles, and the potential for discovering novel reasoning
algorithms. This work provides a theoretical and practical roadmap to enable
Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in
artificial intelligence.

摘要：我們提出了一個新穎的框架，元思考鏈 (Meta-CoT)，它通過明確建模得出特定 CoT 所需的基本推理來擴展傳統的思考鏈 (CoT)。我們展示了來自最先進模型的經驗證據，這些模型表現出與情境中搜尋一致的行為，並探索了通過流程監督、合成資料生成和搜尋演算法來產生 Meta-CoT 的方法。最後，我們概述了一個具體的管道，用於訓練模型以產生 Meta-CoT，其中包含線性化搜尋軌跡和強化學習後訓練的指令調整。最後，我們討論了開放的研究問題，包括規模定律、驗證者角色，以及發現新推理演算法的潛力。這項工作提供了一個理論和實務路線圖，以在 LLM 中啟用 Meta-CoT，為人工智慧中更強大且更類似人類的推理鋪平道路。

##### **Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations**
2501.04675v1 by Archita Srivastava, Abhas Kumar, Rajesh Kumar, Prabhakar Srinivasan

Chart interpretation is crucial for visual data analysis, but accurately
extracting information from charts poses significant challenges for automated
models. This study investigates the fine-tuning of DEPLOT, a modality
conversion module that translates the image of a plot or chart to a linearized
table, on a custom dataset of 50,000 bar charts. The dataset comprises simple,
stacked, and grouped bar charts, targeting the unique structural features of
these visualizations. The finetuned DEPLOT model is evaluated against its base
version using a test set of 1,000 images and two metrics: Relative Mapping
Similarity (RMS), which measures categorical mapping accuracy, and Relative
Number Set Similarity (RNSS), which evaluates numerical interpretation
accuracy. To further explore the reasoning capabilities of large language
models (LLMs), we curate an additional set of 100 bar chart images paired with
question answer sets. Our findings demonstrate that providing a structured
intermediate table alongside the image significantly enhances LLM reasoning
performance compared to direct image queries.

摘要：圖表解讀對於視覺資料分析至關重要，但從圖表中準確擷取資訊對於自動化模型來說是一項重大挑戰。本研究探討了 DEPLOT 的微調，這是一個將繪圖或圖表的影像轉換成線性化表格的模組化轉換模組，針對一個包含 50,000 個長條圖的客製化資料集。該資料集包含簡單、堆疊和群組長條圖，目標為這些視覺化的獨特結構特徵。微調後的 DEPLOT 模型使用一個包含 1,000 個影像和兩個指標的測試集，來評估它與基本版本的差異：相對應對相似度 (RMS)，用於衡量分類對應的準確度，以及相對數字集相似度 (RNSS)，用於評估數值解讀的準確度。為了進一步探索大型語言模型 (LLM) 的推理能力，我們策劃了一個額外的 100 張長條圖影像集，並配對問題解答集。我們的研究結果表明，與直接影像查詢相比，在影像旁邊提供一個結構化的中間表格，可以顯著增強 LLM 的推理效能。

##### **DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests**
2501.04671v1 by Charles Corbière, Simon Roburin, Syrielle Montariol, Antoine Bosselut, Alexandre Alahi

Large vision-language models (LVLMs) augment language models with visual
understanding, enabling multimodal reasoning. However, due to the modality gap
between textual and visual data, they often face significant challenges, such
as over-reliance on text priors, hallucinations, and limited capacity for
complex visual reasoning. Existing benchmarks to evaluate visual reasoning in
LVLMs often rely on schematic or synthetic images and on imprecise
machine-generated explanations. To bridge the modality gap, we present
DrivingVQA, a new benchmark derived from driving theory tests to evaluate
visual chain-of-thought reasoning in complex real-world scenarios. It offers
3,931 expert-crafted multiple-choice problems and interleaved explanations
grounded with entities relevant to the reasoning process. We leverage this
dataset to perform an extensive study of LVLMs' ability to reason about complex
visual scenarios. Our experiments reveal that open-source and proprietary LVLMs
struggle with visual chain-of-thought reasoning under zero-shot settings. We
investigate training strategies that leverage relevant entities to improve
visual reasoning. Notably, we observe a performance boost of up to 7\% when
reasoning over image tokens of cropped regions tied to these entities.

摘要：大型视觉语言模型 (LVLMs) 使用视觉理解来增强语言模型，实现多模态推理。然而，由于文本和视觉数据之间的模态差异，它们通常面临着重大挑战，例如过度依赖文本先验、幻觉以及复杂视觉推理能力有限。现有的基准用于评估 LVLMs 中的视觉推理，通常依赖于示意图或合成图像以及不精确的机器生成的解释。为了弥合模态差距，我们提出了 DrivingVQA，这是一个新的基准，源自驾驶理论测试，用于评估复杂现实世界场景中的视觉思维链推理。它提供了 3,931 个专家制作的多项选择题和穿插的解释，这些解释以与推理过程相关的实体为基础。我们利用此数据集对 LVLMs 推理复杂视觉场景的能力进行了广泛的研究。我们的实验表明，开源和专有 LVLMs 在零次学习设置下难以进行视觉思维链推理。我们研究了利用相关实体来改善视觉推理的训练策略。值得注意的是，当对与这些实体相关的裁剪区域的图像标记进行推理时，我们观察到性能提升高达 7%。

##### **On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena**
2501.04662v1 by Tarek Naous, Wei Xu

Language Models (LMs) have been shown to exhibit a strong preference towards
entities associated with Western culture when operating in non-Western
languages. In this paper, we aim to uncover the origins of entity-related
cultural biases in LMs by analyzing several contributing factors, including the
representation of entities in pre-training data and the impact of variations in
linguistic phenomena across languages. We introduce CAMeL-2, a parallel
Arabic-English benchmark of 58,086 entities associated with Arab and Western
cultures and 367 masked natural contexts for entities. Our evaluations using
CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in
English compared to Arabic. We find that LMs struggle in Arabic with entities
that appear at high frequencies in pre-training, where entities can hold
multiple word senses. This also extends to entities that exhibit high lexical
overlap with languages that are not Arabic but use the Arabic script. Further,
we show how frequency-based tokenization leads to this issue in LMs, which gets
worse with larger Arabic vocabularies. We will make CAMeL-2 available at:
https://github.com/tareknaous/camel2

摘要：語言模型 (LM) 在使用非西方語言時，已被證明對與西方文化相關的實體表現出強烈的偏好。在本文中，我們旨在透過分析幾個促成因素來揭露語言模型中與實體相關的文化偏見的根源，包括實體在預訓練資料中的表示，以及語言中語言現象變化的影響。我們引入了 CAMeL-2，一個包含 58,086 個與阿拉伯和西方文化相關的實體，以及 367 個用於實體的遮蔽自然語境的平行阿拉伯語-英語基準。我們使用 CAMeL-2 進行的評估顯示，與在阿拉伯語中測試相比，語言模型在英語中測試時，不同文化之間的效能差距縮小。我們發現語言模型在阿拉伯語中處理在預訓練中出現頻率高的實體時會遇到困難，因為實體可能有多個詞彙意義。這也延伸到與非阿拉伯語但使用阿拉伯文字的語言具有高度詞彙重疊的實體。此外，我們展示了基於頻率的詞彙化如何導致語言模型中出現此問題，而隨著阿拉伯語詞彙量的增加，情況會變得更糟。我們將在以下位置提供 CAMeL-2：https://github.com/tareknaous/camel2

##### **Assessing Language Comprehension in Large Language Models Using Construction Grammar**
2501.04661v1 by Wesley Scivetti, Melissa Torgbi, Austin Blodgett, Mollie Shichman, Taylor Hudson, Claire Bonial, Harish Tayyar Madabushi

Large Language Models, despite their significant capabilities, are known to
fail in surprising and unpredictable ways. Evaluating their true
`understanding' of language is particularly challenging due to the extensive
web-scale data they are trained on. Therefore, we construct an evaluation to
systematically assess natural language understanding (NLU) in LLMs by
leveraging Construction Grammar (CxG), which provides insights into the meaning
captured by linguistic elements known as constructions (Cxns). CxG is
well-suited for this purpose because provides a theoretical basis to construct
targeted evaluation sets. These datasets are carefully constructed to include
examples which are unlikely to appear in pre-training data, yet intuitive and
easy for humans to understand, enabling a more targeted and reliable
assessment. Our experiments focus on downstream natural language inference and
reasoning tasks by comparing LLMs' understanding of the underlying meanings
communicated through 8 unique Cxns with that of humans. The results show that
while LLMs demonstrate some knowledge of constructional information, even the
latest models including GPT-o1 struggle with abstract meanings conveyed by
these Cxns, as demonstrated in cases where test sentences are dissimilar to
their pre-training data. We argue that such cases provide a more accurate test
of true language understanding, highlighting key limitations in LLMs' semantic
capabilities. We make our novel dataset and associated experimental data
including prompts and model responses publicly available.

摘要：儘管大型語言模型具有顯著的能力，但它們以令人驚訝且無法預測的方式失敗而聞名。由於它們訓練於廣泛的網路規模資料，因此評估它們對語言的真正「理解」特別具有挑戰性。因此，我們建構了一個評量，以透過利用建構文法（CxG）系統性地評估大型語言模型中的自然語言理解（NLU），它提供了對建構（Cxns）等語言元素所擷取的意義的見解。CxG 非常適合此目的，因為它提供了建構目標評量集的理論基礎。這些資料集經過仔細建構，包含不太可能出現在預訓練資料中的範例，但對人類來說卻直觀且易於理解，從而能夠進行更有針對性和更可靠的評量。我們的實驗重點在於下游自然語言推論和推理任務，透過將大型語言模型對透過 8 個獨特的 Cxns 傳達的底層意義的理解與人類的理解進行比較。結果顯示，儘管大型語言模型展示出對建構資訊的某些知識，但即使是包括 GPT-o1 在內的最新模型，在這些 Cxns 傳達的抽象意義上仍有困難，這在測試句子與其預訓練資料不同的情況中得到證明。我們認為，此類案例提供了對真實語言理解更準確的測試，突顯了大型語言模型語義能力中的主要限制。我們公開了我們的新穎資料集和相關實驗資料，包括提示和模型回應。

##### **Multi-task retriever fine-tuning for domain-specific and efficient RAG**
2501.04652v1 by Patrice Béchard, Orlando Marquez Ayala

Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying
Large Language Models (LLMs), as it can address typical limitations such as
generating hallucinated or outdated information. However, when building
real-world RAG applications, practical issues arise. First, the retrieved
information is generally domain-specific. Since it is computationally expensive
to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve
the quality of the data included in the LLM input. Second, as more applications
are deployed in the same real-world system, one cannot afford to deploy
separate retrievers. Moreover, these RAG applications normally retrieve
different kinds of data. Our solution is to instruction fine-tune a small
retriever encoder on a variety of domain-specific tasks to allow us to deploy
one encoder that can serve many use cases, thereby achieving low-cost,
scalability, and speed. We show how this encoder generalizes to out-of-domain
settings as well as to an unseen retrieval task on real-world enterprise use
cases.

摘要：檢索增強生成（RAG）在部署大型語言模型（LLM）時已變得無處不在，因為它可以解決典型的限制，例如生成幻覺或過時的資訊。然而，在建構真實世界的 RAG 應用程式時，會出現實際問題。首先，檢索到的資訊通常是特定領域的。由於微調 LLM 在計算上很昂貴，因此微調檢索器以提高 LLM 輸入中資料品質更可行。其次，由於在同一個真實世界系統中部署了更多應用程式，因此無法負擔部署獨立的檢索器。此外，這些 RAG 應用程式通常檢索不同種類的資料。我們的解決方案是在各種特定領域的任務上對小型檢索器編碼器進行指令微調，讓我們能夠部署一個可以服務於許多使用案例的編碼器，從而實現低成本、可擴充性和速度。我們展示了這個編碼器如何推廣到領域外設定，以及在真實世界的企業使用案例中對未見過的檢索任務進行推廣。

##### **FlairGPT: Repurposing LLMs for Interior Designs**
2501.04648v1 by Gabrielle Littlefair, Niladri Shekhar Dutt, Niloy J. Mitra

Interior design involves the careful selection and arrangement of objects to
create an aesthetically pleasing, functional, and harmonized space that aligns
with the client's design brief. This task is particularly challenging, as a
successful design must not only incorporate all the necessary objects in a
cohesive style, but also ensure they are arranged in a way that maximizes
accessibility, while adhering to a variety of affordability and usage
considerations. Data-driven solutions have been proposed, but these are
typically room- or domain-specific and lack explainability in their design
design considerations used in producing the final layout. In this paper, we
investigate if large language models (LLMs) can be directly utilized for
interior design. While we find that LLMs are not yet capable of generating
complete layouts, they can be effectively leveraged in a structured manner,
inspired by the workflow of interior designers. By systematically probing LLMs,
we can reliably generate a list of objects along with relevant constraints that
guide their placement. We translate this information into a design layout
graph, which is then solved using an off-the-shelf constrained optimization
setup to generate the final layouts. We benchmark our algorithm in various
design configurations against existing LLM-based methods and human designs, and
evaluate the results using a variety of quantitative and qualitative metrics
along with user studies. In summary, we demonstrate that LLMs, when used in a
structured manner, can effectively generate diverse high-quality layouts,
making them a viable solution for creating large-scale virtual scenes. Project
webpage at https://flairgpt.github.io/

摘要：室內設計涉及仔細挑選和安排物件，以創造一個美觀、實用且和諧的空間，符合客戶的設計簡報。這項任務特別具有挑戰性，因為成功的設計不僅必須以一致的風格納入所有必要的物件，還必須確保它們的排列方式能最大化可及性，同時符合各種負擔能力和使用考量。已經提出了資料驅動的解決方案，但這些解決方案通常是特定於房間或領域，而且缺乏在產生最終佈局時所使用的設計考量的可解釋性。在本文中，我們探討大型語言模型 (LLM) 是否可以直接用於室內設計。雖然我們發現 LLM 尚未能夠產生完整的佈局，但它們可以有效地以結構化的方式利用，靈感來自室內設計師的工作流程。透過系統性地探查 LLM，我們可以可靠地產生一個物件清單，以及指導它們放置位置的相关約束。我們將這些資訊轉換成設計佈局圖，然後使用現成的約束式最佳化設定來解決，以產生最終佈局。我們在各種設計配置中將我們的演算法與現有的基於 LLM 的方法和人類設計進行基準測試，並使用各種量化和質化指標以及使用者研究來評估結果。總之，我們證明了 LLM 在以結構化的方式使用時，可以有效地產生多樣化的高品質佈局，使其成為創造大型虛擬場景的可行解決方案。專案網頁在 https://flairgpt.github.io/

##### **Knowledge Retrieval Based on Generative AI**
2501.04635v1 by Te-Lun Yang, Jyi-Shane Liu, Yuen-Hsien Tseng, Jyh-Shing Roger Jang

This study develops a question-answering system based on Retrieval-Augmented
Generation (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.
Using TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for
dense vector retrieval to obtain highly relevant search results and
BGE-reranker to reorder these results based on query relevance. The most
pertinent retrieval outcomes serve as reference knowledge for a Large Language
Model (LLM), enhancing its ability to answer questions and establishing a
knowledge retrieval system grounded in generative AI.
  The system's effectiveness is assessed through a two-stage evaluation:
automatic and assisted performance evaluations. The automatic evaluation
calculates accuracy by comparing the model's auto-generated labels with ground
truth answers, measuring performance under standardized conditions without
human intervention. The assisted performance evaluation involves 20
finance-related multiple-choice questions answered by 20 participants without
financial backgrounds. Initially, participants answer independently. Later,
they receive system-generated reference information to assist in answering,
examining whether the system improves accuracy when assistance is provided.
  The main contributions of this research are: (1) Enhanced LLM Capability: By
integrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly
relevant results, reduces hallucinations, and dynamically accesses authorized
or public knowledge sources. (2) Improved Data Privacy: A customized RAG
architecture enables local operation of the LLM, eliminating the need to send
private data to external servers. This approach enhances data security, reduces
reliance on commercial services, lowers operational costs, and mitigates
privacy risks.

摘要：<paragraph>本研究開發了一個問答系統，該系統基於檢索增強生成 (RAG)，使用中文維基百科和 Lawbank 作為檢索來源。系統使用 TTQA 和 TMMLU+ 作為評估資料集，採用 BGE-M3 進行稠密向量檢索，以取得高度相關的搜尋結果，並使用 BGE-reranker 根據查詢相關性對這些結果重新排序。最相關的檢索結果作為大型語言模型 (LLM) 的參考知識，增強其回答問題的能力，並建立一個基於生成式 AI 的知識檢索系統。系統的有效性通過兩階段評估來評估：自動和輔助性能評估。自動評估通過將模型自動生成的標籤與真實答案進行比較來計算準確性，在沒有人工干預的情況下測量標準化條件下的性能。輔助性能評估包括 20 個與金融相關的多選題，由 20 個沒有金融背景的參與者回答。最初，參與者獨立回答。稍後，他們會收到系統生成的參考資訊以協助回答，檢查在提供協助時系統是否能提高準確性。本研究的主要貢獻有：(1) 增強的 LLM 能力：通過整合 BGE-M3 和 BGE-reranker，系統檢索和重新排序高度相關的結果，減少幻覺，並動態訪問授權或公開的知識來源。(2) 改善資料隱私：自訂的 RAG 架構允許 LLM 本地運作，無需將私人資料傳送至外部伺服器。這種方法增強了資料安全性，減少了對商業服務的依賴，降低了運營成本，並減輕了隱私風險。</paragraph>

##### **MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation**
2501.04614v2 by Daniele Molino, Francesco Di Feola, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Linlin Shen, Valerio Guarrasi, Paolo Soda

Artificial Intelligence is revolutionizing medical practice, enhancing
diagnostic accuracy and healthcare delivery. However, its adaptation in medical
settings still faces significant challenges, related to data availability and
privacy constraints. Synthetic data has emerged as a promising solution to
mitigate these issues, addressing data scarcity while preserving privacy.
Recently, Latent Diffusion Models have emerged as a powerful tool for
generating high-quality synthetic data. Meanwhile, the integration of different
modalities has gained interest, emphasizing the need of models capable of
handle multimodal medical data. Existing approaches struggle to integrate
complementary information and lack the ability to generate modalities
simultaneously. To address this challenge, we present MedCoDi-M, a
6.77-billion-parameter model, designed for multimodal medical data generation,
that, following Foundation Model paradigm, exploits contrastive learning and
large quantity of data to build a shared latent space which capture the
relationships between different data modalities. Further, we introduce the
Multi-Prompt training technique, which significantly boosts MedCoDi-M's
generation under different settings. We extensively validate MedCoDi-M: first
we benchmark it against five competitors on the MIMIC-CXR dataset, a
state-of-the-art dataset for Chest X-ray and radiological report generation.
Secondly, we perform a Visual Turing Test with expert radiologists to assess
the realism and clinical relevance of the generated data, ensuring alignment
with real-world scenarios. Finally, we assess the utility of MedCoDi-M in
addressing key challenges in the medical field, such as anonymization, data
scarcity and imbalance learning. The results are promising, demonstrating the
applicability of MedCoDi-M in medical contexts. Project page is at
https://cosbidev.github.io/MedCoDi-M/.

摘要：人工智能正在革新醫療實務，提升診斷準確度和醫療保健服務。然而，它在醫療場景中的應用仍面臨著重大挑戰，這與資料可用性和隱私限制有關。合成資料已成為緩解這些問題的潛在解決方案，它在保護隱私的同時解決了資料短缺的問題。最近，潛在擴散模型已成為產生高品質合成資料的強大工具。同時，整合不同模態已引起興趣，強調了需要能夠處理多模態醫療資料的模型。現有方法難以整合補充資訊，並且缺乏同時產生模態的能力。為了應對這一挑戰，我們提出了 MedCoDi-M，這是一個 67.7 億參數的模型，專為多模態醫療資料產生而設計，它遵循基礎模型範例，利用對比學習和大量的資料來建立一個共享潛在空間，以捕捉不同資料模態之間的關係。此外，我們引入了多提示訓練技術，它顯著提升了 MedCoDi-M 在不同設定下的產生。我們廣泛驗證了 MedCoDi-M：首先，我們在 MIMIC-CXR 資料集上對它與五個競爭者進行了基準測試，這是胸部 X 光和放射報告產生領域的最新資料集。其次，我們與放射科專家進行了視覺圖靈測試，以評估產生資料的真實性和臨床相關性，確保與真實場景保持一致。最後，我們評估了 MedCoDi-M 在解決醫療領域關鍵挑戰中的效用，例如匿名化、資料短缺和不平衡學習。結果令人滿意，證明了 MedCoDi-M 在醫療環境中的適用性。專案頁面位於 https://cosbidev.github.io/MedCoDi-M/。

##### **Quantum-inspired Embeddings Projection and Similarity Metrics for Representation Learning**
2501.04591v1 by Ivan Kankeu, Stefan Gerd Fritsch, Gunnar Schönhoff, Elie Mounzer, Paul Lukowicz, Maximilian Kiefer-Emmanouilidis

Over the last decade, representation learning, which embeds complex
information extracted from large amounts of data into dense vector spaces, has
emerged as a key technique in machine learning. Among other applications, it
has been a key building block for large language models and advanced computer
vision systems based on contrastive learning. A core component of
representation learning systems is the projection head, which maps the original
embeddings into different, often compressed spaces, while preserving the
similarity relationship between vectors.
  In this paper, we propose a quantum-inspired projection head that includes a
corresponding quantum-inspired similarity metric. Specifically, we map
classical embeddings onto quantum states in Hilbert space and introduce a
quantum circuit-based projection head to reduce embedding dimensionality. To
evaluate the effectiveness of this approach, we extended the BERT language
model by integrating our projection head for embedding compression. We compared
the performance of embeddings, which were compressed using our quantum-inspired
projection head, with those compressed using a classical projection head on
information retrieval tasks using the TREC 2019 and TREC 2020 Deep Learning
benchmarks. The results demonstrate that our quantum-inspired method achieves
competitive performance relative to the classical method while utilizing 32
times fewer parameters. Furthermore, when trained from scratch, it notably
excels, particularly on smaller datasets. This work not only highlights the
effectiveness of the quantum-inspired approach but also emphasizes the utility
of efficient, ad hoc low-entanglement circuit simulations within neural
networks as a powerful quantum-inspired technique.

摘要：在過去十年中，表徵學習已成為機器學習中的一項關鍵技術，它將從大量資料中萃取出的複雜資訊嵌入到稠密向量空間中。在其他應用中，它一直是大型語言模型和基於對比學習的高階電腦視覺系統的關鍵組成部分。表徵學習系統的核心組成部分是投影頭，它將原始嵌入映射到不同的、通常是壓縮的空間中，同時保留向量之間的相似性關係。
在本文中，我們提出一個量子啟發的投影頭，其中包含一個對應的量子啟發相似性度量。具體來說，我們將經典嵌入映射到希爾伯特空間中的量子態，並引入一個基於量子電路的投影頭來降低嵌入維度。為了評估此方法的有效性，我們通過整合我們的投影頭以進行嵌入壓縮來擴充 BERT 語言模型。我們將使用我們的量子啟發投影頭壓縮的嵌入效能與使用經典投影頭壓縮的嵌入效能進行比較，以在使用 TREC 2019 和 TREC 2020 深度學習基準的資訊檢索任務上進行比較。結果表明，我們的量子啟發方法相對於經典方法取得了有競爭力的效能，同時使用的參數減少了 32 倍。此外，當從頭開始訓練時，它特別是在較小的資料集上顯著勝出。這項工作不僅突顯了量子啟發方法的有效性，還強調了在神經網路中使用高效、即席低糾纏電路模擬作為一種強大的量子啟發技術的效用。

##### **Federated-Continual Dynamic Segmentation of Histopathology guided by Barlow Continuity**
2501.04588v1 by Niklas Babendererde, Haozhe Zhu, Moritz Fuchs, Jonathan Stieber, Anirban Mukhopadhyay

Federated- and Continual Learning have been established as approaches to
enable privacy-aware learning on continuously changing data, as required for
deploying AI systems in histopathology images. However, data shifts can occur
in a dynamic world, spatially between institutions and temporally, due to
changing data over time. This leads to two issues: Client Drift, where the
central model degrades from aggregating data from clients trained on shifted
data, and Catastrophic Forgetting, from temporal shifts such as changes in
patient populations. Both tend to degrade the model's performance of previously
seen data or spatially distributed training. Despite both problems arising from
the same underlying problem of data shifts, existing research addresses them
only individually. In this work, we introduce a method that can jointly
alleviate Client Drift and Catastrophic Forgetting by using our proposed
Dynamic Barlow Continuity that evaluates client updates on a public reference
dataset and uses this to guide the training process to a spatially and
temporally shift-invariant model. We evaluate our approach on the
histopathology datasets BCSS and Semicol and prove our method to be highly
effective by jointly improving the dice score as much as from 15.8% to 71.6% in
Client Drift and from 42.5% to 62.8% in Catastrophic Forgetting. This enables
Dynamic Learning by establishing spatio-temporal shift-invariance.

摘要：联邦学习和持续学习已被确立为在不断变化的数据上启用注重隐私的学习的方法，正如在组织病理学图像中部署人工智能系统所要求的那样。然而，数据转移可能发生在一个动态的世界中，在机构之间在空间上和随着时间的推移在时间上，由于数据随着时间的推移而改变。这导致了两个问题：客户端漂移，其中中心模型因汇总来自针对已转移数据进行训练的客户端的数据而退化，以及灾难性遗忘，来自时间转移，例如患者群体的变化。两者都倾向于降低模型对先前看到的数据或空间分布式训练的性能。尽管这两个问题都源于数据转移的相同根本问题，但现有研究仅单独解决它们。在这项工作中，我们引入了一种方法，可以通过使用我们提出的动态 Barlow 连续性来缓解客户端漂移和灾难性遗忘，该连续性在公共参考数据集上评估客户端更新，并使用它来指导训练过程以获得空间和时间平移不变的模型。我们在组织病理学数据集 BCSS 和 Semicol 上评估我们的方法，并证明我们的方法非常有效，通过联合改善骰子分数，在客户端漂移中从 15.8% 提升至 71.6%，在灾难性遗忘中从 42.5% 提升至 62.8%。这通过建立时空平移不变性实现了动态学习。

##### **InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection**
2501.04575v1 by Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, Fei Wu

Graphical User Interface (GUI) Agents, powered by multimodal large language
models (MLLMs), have shown great potential for task automation on computing
devices such as computers and mobile phones. However, existing agents face
challenges in multi-step reasoning and reliance on textual annotations,
limiting their effectiveness. We introduce \textit{InfiGUIAgent}, an MLLM-based
GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1
enhances fundamental skills such as GUI understanding and grounding, while
Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning
skills using synthesized data to enable native reasoning abilities of the
agents. \textit{InfiGUIAgent} achieves competitive performance on several GUI
benchmarks, highlighting the impact of native reasoning skills in enhancing GUI
interaction for automation tasks. Resources are available at
\url{https://github.com/Reallm-Labs/InfiGUIAgent}.

摘要：圖形使用者介面 (GUI) 代理程式由多模態大型語言模型 (MLLM) 提供支援，在電腦和行動電話等運算裝置上展現了任務自動化的龐大潛力。然而，現有的代理程式在多步驟推理和依賴文字註解方面面臨挑戰，限制了它們的效益。我們引入了基於 MLLM 的 GUI 代理程式 \textit{InfiGUIAgent}，並使用兩階段監督微調管道進行訓練。第 1 階段增強了 GUI 理解和基礎等基本技能，而第 2 階段則使用合成資料整合了階層式推理和期望反映推理技能，以讓代理程式具備原生推理能力。\textit{InfiGUIAgent} 在數個 GUI 基準測試中獲得了競爭力表現，突顯了原生推理技能在增強 GUI 互動以進行自動化任務的影響。資源可於 \url{https://github.com/Reallm-Labs/InfiGUIAgent} 取得。

##### **Supervision-free Vision-Language Alignment**
2501.04568v1 by Giorgio Giannone, Ruoteng Li, Qianli Feng, Evgeny Perevodchikov, Rui Chen, Aleix Martinez

Vision-language models (VLMs) have demonstrated remarkable potential in
integrating visual and linguistic information, but their performance is often
constrained by the need for extensive, high-quality image-text training data.
Curation of these image-text pairs is both time-consuming and computationally
expensive. To address this challenge, we introduce SVP (Supervision-free Visual
Projection), a novel framework that enhances vision-language alignment without
relying on curated data or preference annotation. SVP leverages self-captioning
and a pre-trained grounding model as a feedback mechanism to elicit latent
information in VLMs. We evaluate our approach across six key areas: captioning,
referring, visual question answering, multitasking, hallucination control, and
object recall. Results demonstrate significant improvements, including a 14%
average improvement in captioning tasks, up to 12% increase in object recall,
and substantial reduction in hallucination rates. Notably, a small VLM using
SVP achieves hallucination reductions comparable to a model five times larger,
while a VLM with initially poor referring capabilities more than doubles its
performance, approaching parity with a model twice its size.

摘要：視覺語言模型 (VLM) 在整合視覺和語言資訊方面展現了非凡的潛力，但其效能往往受到廣泛、高品質的影像文字訓練資料需求的限制。整理這些影像文字對既耗時又耗費運算資源。為了應對這項挑戰，我們引入了 SVP（無監督視覺投影），這是一種新穎的架構，它增強了視覺語言對齊，而無需依賴整理資料或偏好註解。SVP 利用自我標題和預訓練的基礎模型作為一種回饋機制，以引出 VLM 中的潛在資訊。我們在六個關鍵領域評估了我們的方法：標題、指涉、視覺問題解答、多工處理、幻覺控制和物件回憶。結果顯示出顯著的進步，包括標題任務平均進步 14%，物件回憶增加多達 12%，以及幻覺率大幅降低。值得注意的是，使用 SVP 的小型 VLM 達到的幻覺減少量可與大五倍的模型相媲美，而最初指涉能力較差的 VLM 則將其效能提高了一倍以上，接近於其兩倍大小的模型。

##### **OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis**
2501.04561v1 by Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Yangyi Chen, Hamid Alinejad-Rokny, Fei Huang

Recent advancements in omnimodal learning have been achieved in understanding
and generation across images, text, and speech, though mainly within
proprietary models. Limited omnimodal datasets and the inherent challenges
associated with real-time emotional speech generation have hindered open-source
progress. To address these issues, we propose openomni, a two-stage training
method combining omnimodal alignment and speech generation to develop a
state-of-the-art omnimodal large language model. In the alignment phase, a
pre-trained speech model is further trained on text-image tasks to generalize
from vision to speech in a (near) zero-shot manner, outperforming models
trained on tri-modal datasets. In the speech generation phase, a lightweight
decoder facilitates real-time emotional speech through training on speech tasks
and preference learning. Experiments demonstrate that openomni consistently
improves across omnimodal, vision-language, and speech-language evaluations,
enabling natural, emotion-rich dialogues and real-time emotional speech
generation.

摘要：最近在全模態學習方面取得的進展已在圖像、文字和語音的理解和生成方面取得進展，儘管主要在專有模型中。有限的全模態數據集和與實時情緒語音生成相關的固有挑戰阻礙了開源進度。為了解決這些問題，我們提出了 openomni，這是一種結合全模態對齊和語音生成的兩階段訓練方法，用於開發最先進的全模態大型語言模型。在對齊階段，預訓練的語音模型進一步在文本圖像任務上進行訓練，以在（近乎）零次學習的方式下從視覺推廣到語音，優於在三模態數據集上訓練的模型。在語音生成階段，輕量級解碼器通過在語音任務和偏好學習上進行訓練，促進實時情緒語音。實驗表明，openomni 在全模態、視覺語言和語音語言評估中始終得到改進，實現了自然、富含情緒的對話和實時情緒語音生成。

##### **rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking**
2501.04519v1 by Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang

We present rStar-Math to demonstrate that small language models (SLMs) can
rival or even surpass the math reasoning capability of OpenAI o1, without
distillation from superior models. rStar-Math achieves this by exercising "deep
thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM
performs test-time search guided by an SLM-based process reward model.
rStar-Math introduces three innovations to tackle the challenges in training
the two SLMs: (1) a novel code-augmented CoT data sythesis method, which
performs extensive MCTS rollouts to generate step-by-step verified reasoning
trajectories used to train the policy SLM; (2) a novel process reward model
training method that avoids na\"ive step-level score annotation, yielding a
more effective process preference model (PPM); (3) a self-evolution recipe in
which the policy SLM and PPM are built from scratch and iteratively evolved to
improve reasoning capabilities. Through 4 rounds of self-evolution with
millions of synthesized solutions for 747k math problems, rStar-Math boosts
SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it
improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to
86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad
(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among
the top 20% the brightest high school math students. Code and data will be
available at https://github.com/microsoft/rStar.

摘要：<paragraph>我們提出 rStar-Math，以證明小型語言模型 (SLM) 可以匹敵甚至超越 OpenAI o1 的數學推理能力，而無需從優質模型中進行蒸餾。rStar-Math 通過蒙地卡羅樹狀搜尋 (MCTS) 進行「深度思考」來實現這一目標，其中數學策略 SLM 執行由基於 SLM 的過程獎勵模型指導的測試時間搜尋。rStar-Math 引入了三項創新來應對訓練兩個 SLM 的挑戰：(1) 一種新穎的程式碼增強 CoT 資料合成方法，它執行廣泛的 MCTS 展開以生成用於訓練策略 SLM 的逐步驗證推理軌跡；(2) 一種新穎的過程獎勵模型訓練方法，避免了天真的步驟級別分數註解，產生了更有效的過程偏好模型 (PPM)；(3) 一種自我演化配方，其中策略 SLM 和 PPM 從頭開始構建並反覆演化以提高推理能力。通過 4 輪自我演化，為 747k 個數學問題生成了數百萬個綜合解決方案，rStar-Math 將 SLM 的數學推理提升到了最先進的水平。在 MATH 基準測試中，它將 Qwen2.5-Math-7B 從 58.8% 提高到 90.0%，將 Phi3-mini-3.8B 從 41.4% 提高到 86.4%，超過 o1-preview +4.5% 和 +0.9%。在美國數學奧林匹克競賽 (AIME) 中，rStar-Math 平均解決了 53.3% (8/15) 的問題，名列最優秀高中數學生的前 20%。程式碼和資料將在 https://github.com/microsoft/rStar 上提供。</paragraph>

##### **Improving Image Captioning by Mimicking Human Reformulation Feedback at Inference-time**
2501.04513v1 by Uri Berger, Omri Abend, Lea Frermann, Gabriel Stanovsky

Incorporating automatically predicted human feedback into the process of
training generative models has attracted substantial recent interest, while
feedback at inference time has received less attention. The typical feedback at
training time, i.e., preferences of choice given two samples, does not
naturally transfer to the inference phase. We introduce a novel type of
feedback -- caption reformulations -- and train models to mimic reformulation
feedback based on human annotations. Our method does not require training the
image captioning model itself, thereby demanding substantially less
computational effort. We experiment with two types of reformulation feedback:
first, we collect a dataset of human reformulations that correct errors in the
generated captions. We find that incorporating reformulation models trained on
this data into the inference phase of existing image captioning models results
in improved captions, especially when the original captions are of low quality.
We apply our method to non-English image captioning, a domain where robust
models are less prevalent, and gain substantial improvement. Second, we apply
reformulations to style transfer. Quantitative evaluations reveal
state-of-the-art performance on German image captioning and English style
transfer, while human validation with a detailed comparative framework exposes
the specific axes of improvement.

摘要：將自動預測的人類回饋納入生成模型訓練過程中已引起近期大量關注，而推論時間的回饋則較少受到關注。訓練時間的典型回饋，也就是給定兩個範例的選擇偏好，並未自然轉移到推論階段。我們引入一種新類型的回饋——標題重新表述——並訓練模型根據人類註解來模擬重新表述的回饋。我們的做法不需要訓練影像標題模型本身，因此大幅減少運算需求。我們實驗了兩種類型的重新表述回饋：首先，我們收集了一組人類重新表述的資料集，來修正產生標題中的錯誤。我們發現將訓練於此資料的重新表述模型納入現有影像標題模型的推論階段，會改善標題品質，尤其是在原始標題品質不佳時。我們將我們的做法應用於非英語影像標題，這是一個強健模型較不普遍的領域，並獲得大幅改善。其次，我們將重新表述應用於風格轉移。量化評估顯示在德語影像標題和英語風格轉移方面達到最先進的效能，而使用詳細比較架構進行的人類驗證則揭露了具體的改善面向。

##### **CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection**
2501.04510v1 by Ruijun Feng, Hammond Pearce, Pietro Liguori, Yulei Sui

Large language models (LLMs) have been proposed as powerful tools for
detecting software vulnerabilities, where task-specific fine-tuning is
typically employed to provide vulnerability-specific knowledge to the LLMs for
this purpose. However, traditional full-parameter fine-tuning is inefficient
for modern, complex LLMs, which contain billions of parameters.
  Soft prompt tuning has been suggested as a more efficient alternative for
fine-tuning LLMs in general cases. However, pure soft prompt tuning treats
source code as plain text, losing structural information inherent in source
code. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to
address this issue, are unable to preserve the rich semantic information within
code graphs, as they are primarily designed for general graph-related tasks and
focus more on adjacency information. They also fail to ensure computational
efficiency while accounting for graph-text interactions.
  This paper, therefore, introduces a new code graph-enhanced, structure-aware
soft prompt tuning method for vulnerability detection, referred to as
CGP-Tuning. It employs innovative type-aware embeddings to capture the rich
semantic information within code graphs, along with a novel and efficient
cross-modal alignment module that achieves linear computational cost while
incorporating graph-text interactions. The proposed CGP-Tuning is evaluated on
the latest DiverseVul dataset and the most recent open-source code LLMs,
CodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning
outperforms the best state-of-the-art method by an average of 3.5 percentage
points in accuracy, without compromising its vulnerability detection
capabilities for long source code.

摘要：大型語言模型 (LLM) 已被提出用於偵測軟體漏洞的強大工具，其中任務特定微調通常用於提供漏洞特定知識給 LLM 以達到此目的。然而，傳統的完整參數微調對於包含數十億個參數的現代複雜 LLM 來說效率低下。
軟提示微調已被建議作為一般情況下微調 LLM 的更有效替代方案。然而，純軟提示微調將原始碼視為純文字，失去了原始碼中固有的結構資訊。同時，旨在解決此問題的圖形增強軟提示微調方法無法保留程式碼圖形中的豐富語義資訊，因為它們主要設計用於一般的圖形相關任務，且更專注於鄰接資訊。它們也無法在考量圖形文字互動的同時確保運算效率。
因此，本文介紹了一種新的程式碼圖形增強、結構感知軟提示微調方法來偵測漏洞，稱為 CGP-Tuning。它採用創新的類型感知嵌入來擷取程式碼圖形中的豐富語義資訊，以及一個新穎且有效的跨模態對齊模組，該模組在納入圖形文字互動的同時實現線性運算成本。提議的 CGP-Tuning 在最新的 DiverseVul 資料集和最新的開源程式碼 LLM（CodeLlama 和 CodeGemma）上進行評估。實驗結果證明，CGP-Tuning 在準確度方面平均比最佳的現有技術高出 3.5 個百分點，同時不損害其對長原始碼的漏洞偵測能力。

##### **Developing a Modular Compiler for a Subset of a C-like Language**
2501.04503v1 by Debasish Dutta, Neeharika Sonowal, Irani Hazarika

The paper introduces the development of a modular compiler for a subset of a
C-like language, which addresses the challenges in constructing a compiler for
high-level languages. This modular approach will allow developers to modify a
language by adding or removing subsets as required, resulting in a minimal and
memory-efficient compiler. The development process is divided into small,
incremental steps, where each step yields a fully functioning compiler for an
expanding subset of the language. The paper outlines the iterative
developmental phase of the compiler, emphasizing progressive enhancements in
capabilities and functionality. Adherence to industry best practices of modular
design, code reusability, and documentation has enabled the resulting
compiler's functional efficiency, maintainability, and extensibility. The
compiler proved to be effective not only in managing the language structure but
also in developing optimized code, which demonstrates its practical usability.
This was also further assessed using the compiler on a tiny memory-deficient
single-board computer, again showing the compiler's efficiency and suitability
for resource-constrained devices.

摘要：本文介紹了一個模組化編譯器，它針對 C 類語言的子集進行編譯，並解決了建置高階語言編譯器時所面臨的挑戰。這種模組化方法將允許開發人員依據需求新增或移除子集來修改語言，進而產生精簡且記憶體使用效率高的編譯器。開發流程被劃分為多個小而漸進的步驟，每個步驟都會為語言的擴充子集產生一個功能完整的編譯器。本文概述了編譯器的反覆開發階段，並強調了功能和效能的逐步增強。遵循模組化設計、程式碼可重複使用和文件編寫的業界最佳實務，讓產出的編譯器具備了功能效率、可維護性和可擴充性。編譯器不僅在管理語言結構方面被證明有效，在開發最佳化程式碼方面也一樣，這證明了它的實用性。這也進一步使用編譯器在一個微小的記憶體不足的單板電腦上進行評估，再次顯示出編譯器的效率和對資源受限裝置的適用性。

##### **Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction**
2501.04487v1 by Guofeng Yang, Nanfei Jin, Wenjie Ai, Zhonghua Zheng, Yuhong He, Yong He

Yield is one of the core goals of crop breeding. By predicting the potential
yield of different breeding materials, breeders can screen these materials at
various growth stages to select the best performing. Based on unmanned aerial
vehicle remote sensing technology, high-throughput crop phenotyping data in
breeding areas is collected to provide data support for the breeding decisions
of breeders. However, the accuracy of current yield predictions still requires
improvement, and the usability and user-friendliness of yield forecasting tools
remain suboptimal. To address these challenges, this study introduces a hybrid
method and tool for crop yield prediction, designed to allow breeders to
interactively and accurately predict wheat yield by chatting with a large
language model (LLM). First, the newly designed data assimilation algorithm is
used to assimilate the leaf area index into the WOFOST model. Then, selected
outputs from the assimilation process, along with remote sensing inversion
results, are used to drive the time-series temporal fusion transformer model
for wheat yield prediction. Finally, based on this hybrid method and leveraging
an LLM with retrieval augmented generation technology, we developed an
interactive yield prediction Web tool that is user-friendly and supports
sustainable data updates. This tool integrates multi-source data to assist
breeding decision-making. This study aims to accelerate the identification of
high-yield materials in the breeding process, enhance breeding efficiency, and
enable more scientific and smart breeding decisions.

摘要：產量是作物育種的核心目標之一。透過預測不同育種材料的潛在產量，育種者可以在不同的生長階段篩選這些材料，以選擇表現最佳的材料。基於無人機遙感技術，收集育種區的高通量作物表型數據，為育種者的育種決策提供數據支持。然而，目前產量預測的準確性仍有待提高，產量預測工具的可用性和使用者友善度仍有待優化。為了解決這些挑戰，本研究提出了一種作物產量預測的混合方法和工具，旨在讓育種者透過與大型語言模型 (LLM) 聊天，互動且準確地預測小麥產量。首先，使用新設計的數據同化演算法將葉面積指數同化到 WOFOST 模型中。然後，使用同化過程中的選定輸出，以及遙感反演結果，來驅動時間序列時序融合Transformer模型，以進行小麥產量預測。最後，基於這種混合方法，並利用具備檢索增強生成技術的 LLM，我們開發了一個互動式產量預測 Web 工具，該工具使用者友善，並支援永續的數據更新。此工具整合多來源數據，以協助育種決策。本研究旨在加速在育種過程中識別高產量材料，提高育種效率，並實現更科學、更智慧的育種決策。

##### **Research on environment perception and behavior prediction of intelligent UAV based on semantic communication**
2501.04480v1 by Kechong Ren, Li Gao, Qi Guan

The convergence of drone delivery systems, virtual worlds, and blockchain has
transformed logistics and supply chain management, providing a fast, and
environmentally friendly alternative to traditional ground transportation
methods;Provide users with a real-world experience, virtual service providers
need to collect up-to-the-minute delivery information from edge devices. To
address this challenge, 1) a reinforcement learning approach is introduced to
enable drones with fast training capabilities and the ability to autonomously
adapt to new virtual scenarios for effective resource allocation.2) A semantic
communication framework for meta-universes is proposed, which utilizes the
extraction of semantic information to reduce the communication cost and
incentivize the transmission of information for meta-universe services.3) In
order to ensure that user information security, a lightweight authentication
and key agreement scheme is designed between the drone and the user by
introducing blockchain technology. In our experiments, the drone adaptation
performance is improved by about 35\%, and the local offloading rate can reach
90\% with the increase of the number of base stations. The semantic
communication system proposed in this paper is compared with the Cross Entropy
baseline model. Introducing blockchain technology the throughput of the
transaction is maintained at a stable value with different number of drones.

摘要：無人機配送系統、虛擬世界和區塊鏈的融合，轉變了物流和供應鏈管理，提供快速且對環境友善的傳統地面運輸方法替代方案；為使用者提供真實世界的體驗，虛擬服務供應商需要從邊緣裝置收集最新的配送資訊。為了解決這項挑戰，1) 引進強化學習方法，讓無人機具備快速訓練能力，並能夠自主適應新的虛擬場景，以有效配置資源。2) 提出元宇宙語意溝通架構，利用語意資訊萃取，降低溝通成本，並誘使元宇宙服務傳輸資訊。3) 為了確保使用者資訊安全，在無人機和使用者之間，透過區塊鏈技術設計輕量級驗證和金鑰協商機制。在我們的實驗中，無人機適應效能提升約 35%，而隨著基地台數量增加，本地卸載率可達 90%。本文提出的語意溝通系統與交叉熵基線模型進行比較。導入區塊鏈技術後，交易量能維持在穩定值，且無人機數量不同。

##### **When LLMs Struggle: Reference-less Translation Evaluation for Low-resource Languages**
2501.04473v1 by Archchana Sindhujan, Diptesh Kanojia, Constantin Orasan, Shenbin Qian

This paper investigates the reference-less evaluation of machine translation
for low-resource language pairs, known as quality estimation (QE).
Segment-level QE is a challenging cross-lingual language understanding task
that provides a quality score (0-100) to the translated output. We
comprehensively evaluate large language models (LLMs) in zero/few-shot
scenarios and perform instruction fine-tuning using a novel prompt based on
annotation guidelines. Our results indicate that prompt-based approaches are
outperformed by the encoder-based fine-tuned QE models. Our error analysis
reveals tokenization issues, along with errors due to transliteration and named
entities, and argues for refinement in LLM pre-training for cross-lingual
tasks. We release the data, and models trained publicly for further research.

摘要：本文探討了低資源語言對的機器翻譯無參考評估，又稱為品質評估 (QE)。
區段層級的 QE 是一項具挑戰性的跨語言語言理解任務，它會給予翻譯輸出的品質評分 (0-100)。我們全面評估了零次/少次學習場景中的大型語言模型 (LLM)，並根據標註指南使用新提示執行指令微調。我們的結果顯示，基於提示的方法不如基於編碼器的微調 QE 模型表現出色。我們的錯誤分析揭露了代幣化問題，以及轉寫和命名實體所造成的錯誤，並主張針對跨語言任務改善 LLM 預訓練。我們公開發布資料和模型，以供進一步研究。

##### **Hybrid Artificial Intelligence Strategies for Drone Navigation**
2501.04472v1 by Rubén San-Segundo, Lucía Angulo, Manuel Gil-Martín, David Carramiñana, Ana M. Bernardos

Objective: This paper describes the development of hybrid artificial
intelligence strategies for drone navigation. Methods: The navigation module
combines a deep learning model with a rule-based engine depending on the agent
state. The deep learning model has been trained using reinforcement learning.
The rule-based engine uses expert knowledge to deal with specific situations.
The navigation module incorporates several strategies to explain the drone
decision based on its observation space, and different mechanisms for including
human decisions in the navigation process. Finally, this paper proposes an
evaluation methodology based on defining several scenarios and analyzing the
performance of the different strategies according to metrics adapted to each
scenario. Results: Two main navigation problems have been studied. For the
first scenario (reaching known targets), it has been possible to obtain a 90%
task completion rate, reducing significantly the number of collisions thanks to
the rule-based engine. For the second scenario, it has been possible to reduce
20% of the time required to locate all the targets using the reinforcement
learning model. Conclusions: Reinforcement learning is a very good strategy to
learn policies for drone navigation, but in critical situations, it is
necessary to complement it with a rule-based module to increase task success
rate.

摘要：目標：本文說明無人機導航的混合人工智慧策略的發展。方法：導航模組結合深度學習模型與基於規則的引擎，取決於代理狀態。深度學習模型已使用強化學習進行訓練。基於規則的引擎使用專業知識來處理特定情況。導航模組結合了多種策略來解釋無人機的決策，其根據其觀察空間，以及將人類決策納入導航過程的不同機制。最後，本文提出了一種評估方法，該方法基於定義多種場景並根據適應每個場景的指標分析不同策略的效能。結果：研究了兩個主要的導航問題。對於第一個場景（到達已知目標），已能夠獲得 90% 的任務完成率，由於基於規則的引擎，碰撞次數大幅減少。對於第二個場景，已能夠減少 20% 的時間來使用強化學習模型找出所有目標。結論：強化學習是一種非常好的策略，可以學習無人機導航的政策，但在關鍵情況下，有必要補充一個基於規則的模組來提高任務成功率。

##### **Hidden Entity Detection from GitHub Leveraging Large Language Models**
2501.04455v1 by Lu Gan, Martin Blum, Danilo Dessi, Brigitte Mathiak, Ralf Schenkel, Stefan Dietze

Named entity recognition is an important task when constructing knowledge
bases from unstructured data sources. Whereas entity detection methods mostly
rely on extensive training data, Large Language Models (LLMs) have paved the
way towards approaches that rely on zero-shot learning (ZSL) or few-shot
learning (FSL) by taking advantage of the capabilities LLMs acquired during
pretraining. Specifically, in very specialized scenarios where large-scale
training data is not available, ZSL / FSL opens new opportunities. This paper
follows this recent trend and investigates the potential of leveraging Large
Language Models (LLMs) in such scenarios to automatically detect datasets and
software within textual content from GitHub repositories. While existing
methods focused solely on named entities, this study aims to broaden the scope
by incorporating resources such as repositories and online hubs where entities
are also represented by URLs. The study explores different FSL prompt learning
approaches to enhance the LLMs' ability to identify dataset and software
mentions within repository texts. Through analyses of LLM effectiveness and
learning strategies, this paper offers insights into the potential of advanced
language models for automated entity detection.

摘要：命名實體識別在從非結構化資料來源建構知識庫時是一項重要的任務。雖然實體偵測方法大多依賴大量的訓練資料，但大型語言模型 (LLM) 已為利用零次學習 (ZSL) 或少次學習 (FSL) 的方法鋪路，方法是利用 LLM 在預訓練期間獲得的能力。具體來說，在無法取得大規模訓練資料的非常專業的情境中，ZSL/FSL 開啟了新的機會。本文遵循這項最新趨勢，並探討在這種情境中利用大型語言模型 (LLM) 的潛力，以自動偵測 GitHub 儲存庫中文字內容中的資料集和軟體。現有方法僅專注於命名實體，而本研究旨在透過納入儲存庫和線上中心等資源來擴大範圍，其中實體也由 URL 表示。本研究探討了不同的 FSL 提示學習方法，以增強 LLM 識別儲存庫文字中資料集和軟體提及的能力。透過分析 LLM 的有效性和學習策略，本文提供深入見解，了解進階語言模型在自動實體偵測方面的潛力。

##### **A novel Facial Recognition technique with Focusing on Masked Faces**
2501.04444v1 by Dana A Abdullah, Dana Rasul Hamad, Hakem Beitollahi, Ismail Y Maolood, Abdulhady Abas Abdullah, Aso Khaleel Ameen

Recognizing the same faces with and without masks is important for ensuring
consistent identification in security, access control, and public safety. This
capability is crucial in scenarios like law enforcement, healthcare, and
surveillance, where accurate recognition must be maintained despite facial
occlusion. This research focuses on the challenge of recognizing the same faces
with and without masks by employing cosine similarity as the primary technique.
With the increased use of masks, traditional facial recognition systems face
significant accuracy issues, making it crucial to develop methods that can
reliably identify individuals in masked conditions. For that reason, this study
proposed Masked-Unmasked Face Matching Model (MUFM). This model employs
transfer learning using the Visual Geometry Group (VGG16) model to extract
significant facial features, which are subsequently classified utilizing the
K-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed
to compare masked and unmasked faces of the same individuals. This approach
represents a novel contribution, as the task of recognizing the same individual
with and without a mask using cosine similarity has not been previously
addressed. By integrating these advanced methodologies, the research
demonstrates effective identification of individuals despite the presence of
masks, addressing a significant limitation in traditional systems. Using data
is another essential part of this work, by collecting and preparing an image
dataset from three different sources especially some of those data are real
provided a comprehensive power of this research. The image dataset used were
already collected in three different datasets of masked and unmasked for the
same faces.

摘要：<paragraph>辨識有戴口罩和沒戴口罩的同一張臉孔對於確保在安全、出入管制和公共安全上的一致性非常重要。這種能力在執法、醫療保健和監視等情況中至關重要，在這些情況中，儘管臉部被遮住，仍必須維持準確的辨識。本研究專注於利用餘弦相似性作為主要技術，來解決辨識有戴口罩和沒戴口罩的同一張臉孔的挑戰。隨著口罩使用量增加，傳統的臉部辨識系統面臨顯著的準確性問題，因此開發能夠在戴口罩的條件下可靠辨識個人的方法至關重要。因此，本研究提出了戴口罩-不戴口罩臉部配對模型 (MUFM)。此模型採用使用視覺幾何組 (VGG16) 模型的遷移學習，來萃取重要的臉部特徵，接著利用 K-最近鄰 (K-NN) 演算法進行分類。餘弦相似性度量用於比較同個人的戴口罩和不戴口罩的臉孔。這種方法代表了一項創新的貢獻，因為以前從未處理過使用餘弦相似性來辨識有戴口罩和沒戴口罩的同個人的任務。透過整合這些先進的方法，本研究證明了儘管有口罩，仍然可以有效辨識個人，解決了傳統系統的一項重大限制。使用資料是這項工作的另一個重要部分，透過從三個不同的來源收集和準備影像資料集，特別是其中一些資料是真實提供的，提供了這項研究的全面能力。所使用的影像資料集已經在三個不同的資料集中收集，包含有戴口罩和沒戴口罩的同一張臉孔。</paragraph>

##### **Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions**
2501.04437v1 by Doaa Mahmud, Hadeel Hajmohamed, Shamma Almentheri, Shamma Alqaydi, Lameya Aldhaheri, Ruhul Amin Khalil, Nasir Saeed

Intelligent Transportation Systems (ITS) are crucial for the development and
operation of smart cities, addressing key challenges in efficiency,
productivity, and environmental sustainability. This paper comprehensively
reviews the transformative potential of Large Language Models (LLMs) in
optimizing ITS. Initially, we provide an extensive overview of ITS,
highlighting its components, operational principles, and overall effectiveness.
We then delve into the theoretical background of various LLM techniques, such
as GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.
Following this, we examine the wide-ranging applications of LLMs within ITS,
including traffic flow prediction, vehicle detection and classification,
autonomous driving, traffic sign recognition, and pedestrian detection. Our
analysis reveals how these advanced models can significantly enhance traffic
management and safety. Finally, we explore the challenges and limitations LLMs
face in ITS, such as data availability, computational constraints, and ethical
considerations. We also present several future research directions and
potential innovations to address these challenges. This paper aims to guide
researchers and practitioners through the complexities and opportunities of
integrating LLMs in ITS, offering a roadmap to create more efficient,
sustainable, and responsive next-generation transportation systems.

摘要：智慧型運輸系統 (ITS) 對智慧城市的發展和營運至關重要，能解決效率、生產力和環境永續性的關鍵挑戰。本文全面回顧大型語言模型 (LLM) 在最佳化 ITS 中的轉型潛力。首先，我們提供 ITS 的廣泛概觀，重點說明其組成、運作原理和整體效能。接著深入探討各種 LLM 技術的理論背景，例如 GPT、T5、CTRL 和 BERT，闡明它們與 ITS 應用程式的關聯性。接著，我們探討 LLM 在 ITS 中的廣泛應用，包括交通流量預測、車輛偵測與分類、自動駕駛、交通標誌辨識和行人偵測。我們的分析揭示了這些進階模型如何能大幅提升交通管理和安全性。最後，我們探討 LLM 在 ITS 中面臨的挑戰和限制，例如資料取得、運算限制和道德考量。我們也提出幾個未來的研究方向和潛在創新，以解決這些挑戰。本文旨在引導研究人員和從業人員了解在 ITS 中整合 LLM 的複雜性和機會，提供建立更有效率、永續且靈敏的下一代運輸系統的藍圖。

##### **Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions**
2501.04436v1 by Na Yan, Yang Su, Yansha Deng, Robert Schober

Federated learning (FL) provides a privacy-preserving solution for
fine-tuning pre-trained large language models (LLMs) using distributed private
datasets, enabling task-specific adaptation while preserving data privacy.
However, fine-tuning the extensive parameters in LLMs is particularly
challenging in resource-constrained federated scenarios due to the significant
communication and computational costs. To gain a deeper understanding of how
these challenges can be addressed, this article conducts a comparative analysis
three advanced federated LLM (FedLLM) frameworks that integrate knowledge
distillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,
where clients upload model parameters or gradients to enable straightforward
and effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient
knowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into
two parts, with one part executed on the client and the other one on the
server, to balance the computational load. Each framework is evaluated based on
key performance metrics, including model accuracy, communication overhead, and
client-side computational load, offering insights into their effectiveness for
various federated fine-tuning scenarios. Through this analysis, we identify
framework-specific optimization opportunities to enhance the efficiency of
FedLLMs and discuss broader research directions, highlighting open
opportunities to better adapt FedLLMs for real-world applications. A use case
is presented to demonstrate the performance comparison of these three
frameworks under varying configurations and settings.

摘要：<paragraph>聯合學習 (FL) 提供了一個隱私保護解決方案，用於使用分布式私有資料集微調預先訓練的大型語言模型 (LLM)，同時在保護資料隱私的同時實現特定任務的適應。
然而，在資源受限的聯合場景中微調 LLM 中的廣泛參數特別具有挑戰性，這是因為它會帶來顯著的通訊和計算成本。為了更深入地了解如何應對這些挑戰，本文對三個先進的聯合 LLM (FedLLM) 框架進行了比較分析，這些框架集成了知識蒸餾 (KD) 和分割學習 (SL) 以減輕這些問題：1) FedLLM，客戶端上傳模型參數或梯度以實現直接而有效的微調；2) KD-FedLLM，它利用 KD 通過邏輯值進行有效的知識共享；3) Split-FedLLM，它將 LLM 分成兩部分，一部分在客戶端執行，另一部分在伺服器上執行，以平衡計算負載。每個框架都根據關鍵效能指標進行評估，包括模型準確度、通訊開銷和客戶端計算負載，從而深入了解它們在各種聯合微調場景中的有效性。通過此分析，我們確定了特定於框架的最佳化機會，以提高 FedLLM 的效率，並討論了更廣泛的研究方向，強調了為實際應用更好地適應 FedLLM 的公開機會。提供了一個用例來說明這三個框架在不同配置和設定下的效能比較。</paragraph>

##### **A Digital Shadow for Modeling, Studying and Preventing Urban Crime**
2501.04435v1 by Juan Palma-Borda, Eduardo Guzmán, María-Victoria Belmonte

Crime is one of the greatest threats to urban security. Around 80 percent of
the world's population lives in countries with high levels of criminality. Most
of the crimes committed in the cities take place in their urban environments.
This paper presents the development and validation of a digital shadow platform
for modeling and simulating urban crime. This digital shadow has been
constructed using data-driven agent-based modeling and simulation techniques,
which are suitable for capturing dynamic interactions among individuals and
with their environment. Our approach transforms and integrates well-known
criminological theories and the expert knowledge of law enforcement agencies
(LEA), policy makers, and other stakeholders under a theoretical model, which
is in turn combined with real crime, spatial (cartographic) and socio-economic
data into an urban model characterizing the daily behavior of citizens. The
digital shadow has also been instantiated for the city of Malaga, for which we
had over 300,000 complaints available. This instance has been calibrated with
those complaints and other geographic and socio-economic information of the
city. To the best of our knowledge, our digital shadow is the first for large
urban areas that has been calibrated with a large dataset of real crime reports
and with an accurate representation of the urban environment. The performance
indicators of the model after being calibrated, in terms of the metrics widely
used in predictive policing, suggest that our simulated crime generation
matches the general pattern of crime in the city according to historical data.
Our digital shadow platform could be an interesting tool for modeling and
predicting criminal behavior in an urban environment on a daily basis and,
thus, a useful tool for policy makers, criminologists, sociologists, LEAs, etc.
to study and prevent urban crime.

摘要：犯罪是對城市安全的最大威脅之一。全球約有 80% 的人口生活在犯罪率高的國家。在城市中發生的犯罪大多發生在市區環境中。本文介紹了用於建模和模擬城市犯罪的數位陰影平台的開發和驗證。此數位陰影是使用資料驅動的基於代理的建模和模擬技術建構的，這些技術適用於捕捉個人之間以及個人與其環境之間的動態互動。我們的做法是將著名的犯罪學理論以及執法機構 (LEA)、政策制定者和其他利害關係人的專家知識轉換並整合到一個理論模型中，然後再將其與真實犯罪、空間 (製圖) 和社會經濟資料結合到一個描述公民日常行為的城市模型中。此數位陰影也已在馬拉加市實例化，我們擁有超過 300,000 起可用的申訴。此個案已根據這些申訴以及該市的地理和社會經濟資訊進行校準。據我們所知，我們的數位陰影是第一個針對大型城市地區的數位陰影，已根據大量真實犯罪報告資料集和準確的城市環境描述進行校準。根據預測警務中廣泛使用的指標，在校準後的模型效能指標顯示，我們的模擬犯罪產生與根據歷史資料顯示的城市犯罪一般模式相符。我們的數位陰影平台可以成為一個有趣的工具，用於每天在城市環境中建模和預測犯罪行為，因此，對於政策制定者、犯罪學家、社會學家、執法機構等來說，這是一個有用的工具，可用於研究和預防城市犯罪。

##### **End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark: Leveraging Large Language Model Using Integrated Approach**
2501.04425v1 by H. M. Shadman Tabib, Jaber Ahmed Deedar

This work introduces systematic approach for enhancing large language models
(LLMs) to address Bangla AI mathematical challenges. Through the assessment of
diverse LLM configurations, fine-tuning with specific datasets, and the
implementation of Retrieval-Augmented Generation (RAG), we enhanced the model's
reasoning precision in a multilingual setting. Crucial discoveries indicate
that customized prompting, dataset augmentation, and iterative reasoning
improve the model's efficiency regarding Olympiad-level mathematical
challenges.

摘要：這項工作引入系統化方法來增強大型語言模型 (LLM)，以應對孟加拉 AI 數學挑戰。透過評估不同的 LLM 組態、針對特定資料集進行微調，以及實作檢索增強生成 (RAG)，我們在多語言設定中增強了模型的推理精準度。關鍵發現指出，客製化提示、資料集擴充和反覆推理可提升模型在奧林匹克級數學挑戰中的效率。

##### **NSA: Neuro-symbolic ARC Challenge**
2501.04424v1 by Paweł Batorski, Jannik Brinkmann, Paul Swoboda

The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning
capabilities that are difficult for both machine learning models and
combinatorial search methods. We propose a neuro-symbolic approach that
combines a transformer for proposal generation with combinatorial search using
a domain-specific language. The transformer narrows the search space by
proposing promising search directions, which allows the combinatorial search to
find the actual solution in short time. We pre-train the trainsformer with
synthetically generated data. During test-time we generate additional
task-specific training tasks and fine-tune our model. Our results surpass
comparable state of the art on the ARC evaluation set by 27% and compare
favourably on the ARC train set. We make our code and dataset publicly
available at https://github.com/Batorskq/NSA.

摘要：抽象與推理語料庫 (ARC) 評估一般推理能力，這對機器學習模型和組合搜尋方法來說都很困難。我們提出了一種神經符號方法，結合提案產生Transformer和使用特定領域語言的組合搜尋。Transformer通過提出有希望的搜尋方向來縮小搜尋空間，這允許組合搜尋在短時間內找到實際的解決方案。我們使用合成產生的數據對Transformer進行預訓練。在測試時，我們產生額外的特定任務訓練任務並微調我們的模型。我們的結果在 ARC 評估集中比可比較的最新技術高出 27%，並且在 ARC 訓練集中表現出色。我們在 https://github.com/Batorskq/NSA 上公開我們的代碼和數據集。

##### **User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation**
2501.04410v1 by Krisztian Balog, ChengXiang Zhai

User simulation is an emerging interdisciplinary topic with multiple critical
applications in the era of Generative AI. It involves creating an intelligent
agent that mimics the actions of a human user interacting with an AI system,
enabling researchers to model and analyze user behaviour, generate synthetic
data for training, and evaluate interactive AI systems in a controlled and
reproducible manner. User simulation has profound implications for diverse
fields and plays a vital role in the pursuit of Artificial General
Intelligence. This paper provides an overview of user simulation, highlighting
its key applications, connections to various disciplines, and outlining future
research directions to advance this increasingly important technology.

摘要：使用者模擬是生成式 AI 時代中一個新興的跨領域主題，有多重關鍵應用。它包含創造一個智慧代理人，模仿人類使用者與 AI 系統互動的動作，讓研究人員能夠模擬和分析使用者行為、為訓練產生合成資料，並在受控和可複製的方式中評估互動式 AI 系統。使用者模擬對各種領域有深遠的影響，並在追求人工通用智慧中扮演至關重要的角色。這篇論文提供使用者模擬的概觀，重點說明其關鍵應用、與各個領域的連結，並概述未來的研究方向，以推進這項日益重要的技術。

##### **SEO: Stochastic Experience Optimization for Large Language Models**
2501.04393v1 by Jitao Xu, Hongyun Zhou, Lei Shen, Conghui Zhu, Jin Huang, Yitao Duan

Large Language Models (LLMs) can benefit from useful experiences to improve
their performance on specific tasks. However, finding helpful experiences for
different LLMs is not obvious, since it is unclear what experiences suit
specific LLMs. Previous studies intended to automatically find useful
experiences using LLMs, while it is difficult to ensure the effectiveness of
the obtained experience. In this paper, we propose Stochastic Experience
Optimization (SEO), an iterative approach that finds optimized model-specific
experience without modifying model parameters through experience update in
natural language. In SEO, we propose a stochastic validation method to ensure
the update direction of experience, avoiding unavailing updates. Experimental
results on three tasks for three LLMs demonstrate that experiences optimized by
SEO can achieve consistently improved performance. Further analysis indicates
that SEO-optimized experience can generalize to out-of-distribution data,
boosting the performance of LLMs on similar tasks.

摘要：大型語言模型 (LLM) 可以從有用的經驗中受益，以提升其在特定任務上的表現。然而，要為不同的 LLM 找到有用的經驗並不容易，因為目前尚不清楚哪些經驗適合特定的 LLM。先前的研究旨在使用 LLM 自動尋找有用的經驗，但難以確保獲得的經驗有效。在本文中，我們提出了隨機經驗最佳化 (SEO)，這是一種反覆運算的方法，它會透過以自然語言更新經驗，來找到最佳化的模型特定經驗，而不會修改模型參數。在 SEO 中，我們提出了一種隨機驗證方法，以確保經驗的更新方向，避免無效的更新。針對三種 LLM 的三個任務的實驗結果顯示，由 SEO 最佳化的經驗可以達到持續提升的表現。進一步的分析指出，經過 SEO 最佳化的經驗可以推廣到分佈外資料，提升 LLM 在類似任務上的表現。

##### **On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis**
2501.04377v1 by Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song

Recently, Visual Autoregressive ($\mathsf{VAR}$) Models introduced a
groundbreaking advancement in the field of image generation, offering a
scalable approach through a coarse-to-fine "next-scale prediction" paradigm.
However, the state-of-the-art algorithm of $\mathsf{VAR}$ models in [Tian,
Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^4)$ time, which is
computationally inefficient. In this work, we analyze the computational limits
and efficiency criteria of $\mathsf{VAR}$ Models through a fine-grained
complexity lens. Our key contribution is identifying the conditions under which
$\mathsf{VAR}$ computations can achieve sub-quadratic time complexity.
Specifically, we establish a critical threshold for the norm of input matrices
used in $\mathsf{VAR}$ attention mechanisms. Above this threshold, assuming the
Strong Exponential Time Hypothesis ($\mathsf{SETH}$) from fine-grained
complexity theory, a sub-quartic time algorithm for $\mathsf{VAR}$ models is
impossible. To substantiate our theoretical findings, we present efficient
constructions leveraging low-rank approximations that align with the derived
criteria. This work initiates the study of the computational efficiency of the
$\mathsf{VAR}$ model from a theoretical perspective. Our technique will shed
light on advancing scalable and efficient image generation in $\mathsf{VAR}$
frameworks.

摘要：最近，视觉自回归 ($\mathsf{VAR}$) 模型在图像生成领域引入了一项开创性的进步，通过粗到细的“下一尺度预测”范例提供了一种可扩展的方法。然而，$\mathsf{VAR}$ 模型在 [Tian, Jiang, Yuan, Peng 和 Wang, NeurIPS 2024] 中的最先进算法需要 $O(n^4)$ 时间，这在计算上是低效的。在这项工作中，我们通过细粒度的复杂性透镜分析了 $\mathsf{VAR}$ 模型的计算限制和效率标准。我们的关键贡献是确定 $\mathsf{VAR}$ 计算可以在其中实现次二次时间复杂度的条件。具体来说，我们为 $\mathsf{VAR}$ 注意力机制中使用的输入矩阵范数建立了一个临界阈值。在此阈值之上，假设来自细粒度复杂性理论的强指数时间假设 ($\mathsf{SETH}$)，则 $\mathsf{VAR}$ 模型的次四次时间算法是不可能的。为了证实我们的理论发现，我们提出了利用低秩近似的高效构造，这些近似与派生标准相一致。这项工作从理论角度开始研究 $\mathsf{VAR}$ 模型的计算效率。我们的技术将阐明在 $\mathsf{VAR}$ 框架中推进可扩展且高效的图像生成。

##### **DispFormer: Pretrained Transformer for Flexible Dispersion Curve Inversion from Global Synthesis to Regional Applications**
2501.04366v1 by Feng Liu, Bao Deng, Rui Su, Lei Bai, Wanli Ouyang

Surface wave dispersion curve inversion is essential for estimating
subsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle
to balance computational efficiency with inversion accuracy. While deep
learning approaches show promise, previous studies typically require large
amounts of labeled data and struggle with real-world datasets that have varying
period ranges, missing data, and low signal-to-noise ratios. This study
proposes DispFormer, a transformer-based neural network for inverting the $v_s$
profile from Rayleigh-wave phase and group dispersion curves. DispFormer
processes dispersion data at each period independently, thereby allowing it to
handle data of varying lengths without requiring network modifications or
alignment between training and testing data. The performance is demonstrated by
pre-training it on a global synthetic dataset and testing it on two regional
synthetic datasets using zero-shot and few-shot strategies. Results indicate
that zero-shot DispFormer, even without any labeled data, produces inversion
profiles that match well with the ground truth, providing a deployable initial
model generator to assist traditional methods. When labeled data is available,
few-shot DispFormer outperforms traditional methods with only a small number of
labels. Furthermore, real-world tests indicate that DispFormer effectively
handles varying length data, and yields lower data residuals than reference
models. These findings demonstrate that DispFormer provides a robust foundation
model for dispersion curve inversion and is a promising approach for broader
applications.

摘要：表面波色散曲线反演对于估计地下剪切波速度 ($v_s$) 至关重要，但传统方法常常难以平衡计算效率和反演精度。虽然深度学习方法显示出前景，但以往的研究通常需要大量标记数据，并且难以处理具有不同周期范围、缺失数据和低信噪比的真实世界数据集。本研究提出了 DispFormer，这是一种基于 transformer 的神经网络，用于反演瑞利波相位和群速度曲线中的 $v_s$ 分布。DispFormer 独立处理每个周期的色散数据，从而允许它处理不同长度的数据，而不需要网络修改或训练和测试数据之间的对齐。通过在全球合成数据集上进行预训练，并在两个区域合成数据集上使用零样本和少样本策略进行测试，证明了其性能。结果表明，即使没有任何标记数据，零样本 DispFormer 也会生成与地面实况匹配良好的反演剖面，为辅助传统方法提供可部署的初始模型生成器。当有标记数据可用时，少样本 DispFormer 仅使用少量标签就能优于传统方法。此外，实际测试表明，DispFormer 有效地处理了不同长度的数据，并且比参考模型产生了更低的残差数据。这些发现表明，DispFormer 为色散曲线反演提供了一个稳健的基础模型，并且是一种有前途的更广泛应用方法。

##### **Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation**
2501.04359v1 by Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, Kateryna Shapovalenko

Decoding speech from non-invasive brain signals, such as
electroencephalography (EEG), has the potential to advance brain-computer
interfaces (BCIs), with applications in silent communication and assistive
technologies for individuals with speech impairments. However, EEG-based speech
decoding faces major challenges, such as noisy data, limited datasets, and poor
performance on complex tasks like speech perception. This study attempts to
address these challenges by employing variational autoencoders (VAEs) for EEG
data augmentation to improve data quality and applying a state-of-the-art
(SOTA) sequence-to-sequence deep learning architecture, originally successful
in electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we
adapt this architecture for word classification tasks. Using the Brennan
dataset, which contains EEG recordings of subjects listening to narrated
speech, we preprocess the data and evaluate both classification and
sequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments
show that VAEs have the potential to reconstruct artificial EEG data for
augmentation. Meanwhile, our sequence-to-sequence model achieves more promising
performance in generating sentences compared to our classification model,
though both remain challenging tasks. These findings lay the groundwork for
future research on EEG speech perception decoding, with possible extensions to
speech production tasks such as silent or imagined speech.

摘要：從非侵入式腦信號（例如腦電圖 (EEG)）解碼語言，具有推動腦機介面 (BCI) 的潛力，並可應用於無聲溝通和協助語言障礙人士的輔助技術。然而，基於 EEG 的語言解碼面臨重大挑戰，例如雜訊資料、受限的資料集和在語言知覺等複雜任務上的表現不佳。本研究嘗試透過採用變異自動編碼器 (VAE) 來擴充 EEG 資料以改善資料品質，並應用最先進 (SOTA) 的序列對序列深度學習架構（最初成功應用於肌電圖 (EMG) 任務）到基於 EEG 的語言解碼，來解決這些挑戰。此外，我們調整此架構以適用於詞彙分類任務。我們使用 Brennan 資料集（其中包含受試者聆聽敘述語言的 EEG 記錄），預處理資料並評估分類和序列對序列模型在 EEG 轉換為文字/句子任務中的表現。我們的實驗顯示，VAE 有潛力重建人工 EEG 資料以進行擴充。同時，與分類模型相比，我們的序列對序列模型在產生句子方面表現更具前景，儘管兩者仍是具有挑戰性的任務。這些發現為未來 EEG 語言知覺解碼研究奠定基礎，並可能延伸至語言產生任務，例如無聲或想像的語言。

##### **Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting**
2501.04341v1 by Dong-Hai Zhu, Yu-Jie Xiong, Jia-Chen Zhang, Xi-Jiong Xie, Chun-Ming Xia

Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language
Models (LLMs) to enhance complex reasoning. It guides LLMs to present
multi-step reasoning, rather than generating the final answer directly.
However, CoT encounters difficulties when key information required for
reasoning is implicit or missing. This occurs because CoT emphasizes the
sequence of reasoning steps while overlooking the early extraction of essential
information. We propose a pre-prompting method called Iterative Summarization
Pre-Prompting (ISP^2) to refine LLM reasoning when key information is not
explicitly provided. First, entities and their corresponding descriptions are
extracted to form potential key information pairs. Next, we use a reliability
rating to assess these pairs, then merge the two lowest-ranked pairs into a new
entity description. This process is repeated until a unique key information
pair is obtained. Finally, that pair, along with the original question, is fed
into LLMs to produce the answer. Extensive experiments demonstrate a 7.1%
improvement compared to existing methods. Unlike traditional prompting, ISP^2
adopts an inductive approach with pre-prompting, offering flexible integration
into diverse reasoning frameworks. The code is available at
https://github.com/zdhgreat/ISP-2.

摘要：鏈式思維 (CoT) 提示是大型語言模型 (LLM) 中增強複雜推理的主導範例。它引導 LLM 提出多步驟推理，而不是直接生成最終答案。
然而，當推理所需的重要資訊是隱含或遺失時，CoT 會遇到困難。這是因為 CoT 強調推理步驟的順序，同時忽略了早期萃取基本資訊。我們提出了一種稱為反覆摘要預提示 (ISP^2) 的預提示方法，以在未明確提供關鍵資訊時改善 LLM 推理。首先，萃取實體及其對應描述以形成潛在關鍵資訊對。接下來，我們使用可靠性評分評估這些對，然後將排名最低的兩個對合併成一個新的實體描述。此過程會重複進行，直到獲得唯一的關鍵資訊對。最後，將該對連同原始問題輸入 LLM 以產生答案。廣泛的實驗證明與現有方法相比，改進了 7.1%。與傳統提示不同，ISP^2 採用預提示的歸納方法，提供靈活整合到不同的推理框架中。程式碼可在 https://github.com/zdhgreat/ISP-2 取得。

##### **Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts**
2501.04316v1 by Preethi Seshadri, Seraphina Goldfarb-Tarrant

Large language models (LLMs) are increasingly being deployed in high-stakes
applications like hiring, yet their potential for unfair decision-making and
outcomes remains understudied, particularly in generative settings. In this
work, we examine the fairness of LLM-based hiring systems through two
real-world tasks: resume summarization and retrieval. By constructing a
synthetic resume dataset and curating job postings, we investigate whether
model behavior differs across demographic groups and is sensitive to
demographic perturbations. Our findings reveal that race-based differences
appear in approximately 10% of generated summaries, while gender-based
differences occur in only 1%. In the retrieval setting, all evaluated models
display non-uniform selection patterns across demographic groups and exhibit
high sensitivity to both gender and race-based perturbations. Surprisingly,
retrieval models demonstrate comparable sensitivity to non-demographic changes,
suggesting that fairness issues may stem, in part, from general brittleness
issues. Overall, our results indicate that LLM-based hiring systems, especially
at the retrieval stage, can exhibit notable biases that lead to discriminatory
outcomes in real-world contexts.

摘要：大型語言模型 (LLM)  zunehmend in hochkarätigen Anwendungen wie der Personalbeschaffung eingesetzt, doch ihr Potenzial für unfaire Entscheidungsfindung und Ergebnisse bleibt unerforscht, insbesondere in generativen Umgebungen. In dieser Arbeit untersuchen wir die Fairness von LLM-basierten Einstellungssystemen anhand von zwei realen Aufgaben: Lebenslaufzusammenfassung und -abruf. Durch die Erstellung eines synthetischen Lebenslaufdatensatzes und die Zusammenstellung von Stellenausschreibungen untersuchen wir, ob sich das Modellverhalten zwischen demografischen Gruppen unterscheidet und für demografische Störungen empfindlich ist. Unsere Ergebnisse zeigen, dass rassenbedingte Unterschiede in etwa 10 % der generierten Zusammenfassungen auftreten, während geschlechtsspezifische Unterschiede nur in 1 % auftreten. In der Abrufumgebung zeigen alle bewerteten Modelle ungleichmäßige Auswahlmuster über demografische Gruppen hinweg und weisen eine hohe Empfindlichkeit sowohl gegenüber geschlechtsspezifischen als auch rassenbedingten Störungen auf. Überraschenderweise zeigen Abrufmodelle eine vergleichbare Empfindlichkeit gegenüber nicht-demografischen Veränderungen, was darauf hindeutet, dass Fairnessprobleme teilweise auf allgemeine Sprödigkeitsprobleme zurückzuführen sein könnten. Insgesamt deuten unsere Ergebnisse darauf hin, dass LLM-basierte Einstellungssysteme, insbesondere in der Abrufphase, erhebliche Verzerrungen aufweisen können, die in realen Kontexten zu diskriminierenden Ergebnissen führen.

##### **RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation**
2501.04315v1 by Jun Liu, Zhenglun Kong, Peiyan Dong, Xuan Shen, Pu Zhao, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, Dong Huang, Yanzhi Wang

Fine-tuning helps large language models (LLM) recover degraded information
and enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used
and effective for fine-tuning, we have observed that its scaling factor can
limit or even reduce performance as the rank size increases. To address this
issue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet
effective method for optimizing LoRA's scaling factor. By replacing $\alpha/r$
with $\alpha/\sqrt{r}$, RoRA ensures improved performance as rank size
increases. Moreover, RoRA enhances low-rank adaptation in fine-tuning
uncompressed models and excels in the more challenging task of accuracy
recovery when fine-tuning pruned models. Extensive experiments demonstrate the
effectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA
surpasses the state-of-the-art (SOTA) in average accuracy and robustness on
LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and
DoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,
RoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%
pruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher
than DoRA.

摘要：微調有助大型語言模型 (LLM) 恢復退化的資訊並提升任務效能。儘管低階層次適應 (LoRA) 廣泛用於微調且效果良好，我們觀察到其縮放因子可能會隨著階層次大小增加而限制或甚至降低效能。為了解決此問題，我們提出 RoRA (階層次適應可靠性最佳化)，這是一種用於最佳化 LoRA 縮放因子的簡單但有效的方法。透過將 $\alpha/r$ 替換為 $\alpha/\sqrt{r}$，RoRA 可確保在階層次大小增加時提升效能。此外，RoRA 可在微調未壓縮模型時提升低階層次適應，且在微調修剪模型時精準度復原的更具挑戰性任務中表現優異。廣泛的實驗證明了 RoRA 在微調未壓縮和修剪模型時的有效性。RoRA 在 LLaMA-7B/13B、LLaMA2-7B 和 LLaMA3-8B 上的平均精準度和穩健性方面超越了現有技術 (SOTA)，特別是在 LLaMA-7B 上分別比 LoRA 和 DoRA 高出 6.5% 和 2.9%。在修剪模型微調中，RoRA 顯示出顯著的優勢；對於修剪率為 81.4% 的 LLaMA-7B，即 SHEARED-LLAMA-1.3，RoRA 的平均精準度比 LoRA 高出 5.7%，比 DoRA 高出 3.9%。

##### **LLM4SR: A Survey on Large Language Models for Scientific Research**
2501.04306v1 by Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du

In recent years, the rapid advancement of Large Language Models (LLMs) has
transformed the landscape of scientific research, offering unprecedented
support across various stages of the research cycle. This paper presents the
first systematic survey dedicated to exploring how LLMs are revolutionizing the
scientific research process. We analyze the unique roles LLMs play across four
critical stages of research: hypothesis discovery, experiment planning and
implementation, scientific writing, and peer reviewing. Our review
comprehensively showcases the task-specific methodologies and evaluation
benchmarks. By identifying current challenges and proposing future research
directions, this survey not only highlights the transformative potential of
LLMs, but also aims to inspire and guide researchers and practitioners in
leveraging LLMs to advance scientific inquiry. Resources are available at the
following repository: https://github.com/du-nlp-lab/LLM4SR

摘要：近年來，大型語言模型 (LLM) 的快速進展已轉變科學研究的格局，在研究週期的各個階段提供前所未有的支持。本文提出第一份系統性的調查，專門探討 LLM 如何革新科學研究流程。我們分析 LLM 在研究的四個關鍵階段中所扮演的獨特角色：假說發現、實驗規劃與實施、科學寫作和同行評審。我們的回顧全面展示了特定任務的方法論和評估基準。透過找出當前的挑戰並提出未來的研究方向，本調查不僅強調了 LLM 的轉型潛力，也旨在激勵和引導研究人員和從業人員利用 LLM 來推進科學探究。資源可於以下存放庫取得：https://github.com/du-nlp-lab/LLM4SR

##### **Multimodal Graph Constrastive Learning and Prompt for ChartQA**
2501.04303v1 by Yue Dai, Soyeon Caren Han, Wei Liu

ChartQA presents significant challenges due to the complex distribution of
chart elements and the implicit patterns embedded within the underlying data.
In this chapter, we have developed a joint multimodal scene graph for charts,
explicitly representing the relationships between chart elements and their
associated patterns.
  Our proposed multimodal scene graph consists of two components: a visual
graph and a textual graph, each designed to capture the structural and semantic
information within the chart. To unify representations across these different
modalities, we introduce a multimodal graph contrastive learning approach that
learns unified representations by maximizing similarity between nodes
representing the same object across multimodal graphs. The learned graph
representations can be seamlessly incorporated into a transformer decoder as a
soft prompt.
  Additionally, given the growing need for Multimodal Large Language Models
(MLLMs) in zero-shot scenarios, we have designed Chain-of-Thought (CoT) prompts
for MLLMs to reduce hallucinations. We tested both methods on public benchmarks
such as ChartQA, OpenCQA, and ChartX, demonstrating improved performance and
validating the effectiveness of our proposed methods.

摘要：ChartQA 因圖表元素的複雜分佈和基礎資料中內嵌的隱含模式而面臨重大挑戰。
在本章中，我們為圖表開發了一個聯合多模態場景圖形，明確表示圖表元素之間的關係及其關聯模式。
我們提出的多模態場景圖形包含兩個組成部分：一個視覺圖形和一個文本圖形，每個組成部分都旨在擷取圖表中的結構化和語義資訊。
為了統一這些不同模態的表示，我們引入了一個多模態圖形對比學習方法，透過最大化跨多模態圖形表示相同物件的節點之間的相似性來學習統一的表示。
學習到的圖形表示可以無縫地整合到Transformer解碼器中，作為一個軟提示。
此外，鑑於多模態大型語言模型 (MLLM) 在零次學習場景中的需求日益增加，我們為 MLLM 設計了思考鏈 (CoT) 提示，以減少幻覺。
我們在公眾基準上測試了這兩種方法，例如 ChartQA、OpenCQA 和 ChartX，證明了效能的提升，並驗證了我們提出的方法的有效性。

##### **H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving**
2501.04302v1 by Siran Chen, Yuxiao Luo, Yue Ma, Yu Qiao, Yali Wang

With the prevalence of Multimodal Large Language Models(MLLMs), autonomous
driving has encountered new opportunities and challenges. In particular,
multi-modal video understanding is critical to interactively analyze what will
happen in the procedure of autonomous driving. However, videos in such a
dynamical scene that often contains complex spatial-temporal movements, which
restricts the generalization capacity of the existing MLLMs in this field. To
bridge the gap, we propose a novel Hierarchical Mamba Adaptation (H-MBA)
framework to fit the complicated motion changes in autonomous driving videos.
Specifically, our H-MBA consists of two distinct modules, including Context
Mamba (C-Mamba) and Query Mamba (Q-Mamba). First, C-Mamba contains various
types of structure state space models, which can effectively capture
multi-granularity video context for different temporal resolutions. Second,
Q-Mamba flexibly transforms the current frame as the learnable query, and
attentively selects multi-granularity video context into query. Consequently,
it can adaptively integrate all the video contexts of multi-scale temporal
resolutions to enhance video understanding. Via a plug-and-play paradigm in
MLLMs, our H-MBA shows the remarkable performance on multi-modal video tasks in
autonomous driving, e.g., for risk object detection, it outperforms the
previous SOTA method with 5.5% mIoU improvement.

摘要：隨著多模態大型語言模型 (MLLM) 的普及，自動駕駛迎來了新的機遇和挑戰。特別是，多模態影片理解對於互動式分析自動駕駛過程中將會發生的事情至關重要。然而，在這樣一個經常包含複雜時空移動的動態場景中的影片，限制了現有 MLLM 在這個領域中的泛化能力。為了彌合差距，我們提出了一個新穎的分層 Mamba 適應 (H-MBA) 框架，以適應自動駕駛影片中複雜的動作變化。具體來說，我們的 H-MBA 由兩個不同的模組組成，包括 Context Mamba (C-Mamba) 和 Query Mamba (Q-Mamba)。首先，C-Mamba 包含各種結構狀態空間模型，可以有效地捕捉不同時間解析度的多粒度影片內容。其次，Q-Mamba 靈活地將當前幀轉換為可學習的查詢，並專注於將多粒度影片內容選入查詢中。因此，它可以自適應地整合所有多尺度時間解析度的影片內容，以增強影片理解。透過 MLLM 中的即插即用範例，我們的 H-MBA 在自動駕駛中的多模態影片任務中展現了顯著的效能，例如，對於風險物體偵測，它優於先前的 SOTA 方法，mIoU 提升了 5.5%。

##### **Circuit Complexity Bounds for Visual Autoregressive Model**
2501.04299v1 by Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song

Understanding the expressive ability of a specific model is essential for
grasping its capacity limitations. Recently, several studies have established
circuit complexity bounds for Transformer architecture. Besides, the Visual
AutoRegressive (VAR) model has risen to be a prominent method in the field of
image generation, outperforming previous techniques, such as Diffusion
Transformers, in generating high-quality images. We investigate the circuit
complexity of the VAR model and establish a bound in this study. Our primary
result demonstrates that the VAR model is equivalent to a simulation by a
uniform $\mathsf{TC}^0$ threshold circuit with hidden dimension $d \leq O(n)$
and $\mathrm{poly}(n)$ precision. This is the first study to rigorously
highlight the limitations in the expressive power of VAR models despite their
impressive performance. We believe our findings will offer valuable insights
into the inherent constraints of these models and guide the development of more
efficient and expressive architectures in the future.

摘要：了解特定模型的表達能力對於掌握其容量限制至關重要。最近，一些研究已經為 Transformer 架構建立了電路複雜度界限。此外，視覺自迴歸 (VAR) 模型已成為影像生成領域的顯著方法，在產生高品質影像方面優於先前的技術，例如擴散式 Transformer。我們在研究中調查 VAR 模型的電路複雜度並建立界限。我們的初步結果表明，VAR 模型等於一個均勻的 $\mathsf{TC}^0$ 閾值電路模擬，其隱藏維度 $d \leq O(n)$，且精度為 $\mathrm{poly}(n)$。儘管 VAR 模型有令人印象深刻的效能，但這是第一個嚴謹地強調其表達能力限制的研究。我們相信我們的發現將提供有價值的見解，了解這些模型的內在限制，並在未來引導開發更有效率且更具表達力的架構。

##### **MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge**
2501.04292v1 by Zijiang Yang, Meishu Song, Xin Jing, Haojie Zhang, Kun Qian, Bin Hu, Kota Tamada, Toru Takumi, Björn W. Schuller, Yoshiharu Yamamoto

The Mice Autism Detection via Ultrasound Vocalization (MAD-UV) Challenge
introduces the first INTERSPEECH challenge focused on detecting autism spectrum
disorder (ASD) in mice through their vocalizations. Participants are tasked
with developing models to automatically classify mice as either wild-type or
ASD models based on recordings with a high sampling rate. Our baseline system
employs a simple CNN-based classification using three different spectrogram
features. Results demonstrate the feasibility of automated ASD detection, with
the considered audible-range features achieving the best performance (UAR of
0.600 for segment-level and 0.625 for subject-level classification). This
challenge bridges speech technology and biomedical research, offering
opportunities to advance our understanding of ASD models through machine
learning approaches. The findings suggest promising directions for vocalization
analysis and highlight the potential value of audible and ultrasound
vocalizations in ASD detection.

摘要：小鼠自閉症透過超音波發聲偵測（MAD-UV）挑戰
引入了第一個 INTERSPEECH 挑戰，專注於透過小鼠的發聲偵測自閉症譜系障礙（ASD）。參與者需要開發模型，根據採樣率高的錄音，將小鼠自動分類為野生型或 ASD 模型。我們的基準系統採用簡單的基於 CNN 的分類，使用三個不同的頻譜圖特徵。結果證明了自動化 ASD 偵測的可行性，其中考慮的聲波範圍特徵達到了最佳效能（區段層級為 0.600 的 UAR，主體層級分類為 0.625）。這個挑戰連結了語音技術和生物醫學研究，提供機會透過機器學習方法推進我們對 ASD 模型的理解。研究結果顯示了發聲分析的有前途的方向，並強調了聲波和超音波發聲在 ASD 偵測中的潛在價值。

##### **Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models**
2501.04286v1 by Bahman Torkamandi

In the realm of fractal geometry, intricate structures emerge from simple
iterative processes that partition parameter spaces into regions of stability
and instability. Likewise, training large language models involves iteratively
applying update functions, such as Adam, where even slight hyperparameter
adjustments can shift the training process from convergence to divergence.
Recent evidence from miniature neural networks suggests that the boundary
separating these outcomes displays fractal characteristics [1]. Building on
these insights, this study extends them to medium-sized, decoder-only
transformer architectures by employing a more consistent convergence measure
and examining the learning rate hyperparameter landscape for attention and
fully connected layers. The results show that the trainability frontier is not
a simple threshold; rather, it forms a self-similar yet seemingly random
structure at multiple scales, with statistically consistent and repeating
patterns. Within this landscape, a region of stable convergence is surrounded
by a complex chaotic border, illustrating the sensitive nature of the
underlying training dynamics.

摘要：在分形几何领域，复杂的结构从简单的迭代过程中出现，这些过程将参数空间划分为稳定性和不稳定性区域。同样，训练大型语言模型涉及迭代应用更新函数，例如 Adam，其中即使是轻微的超参数调整也能将训练过程从收敛转变为发散。微型神经网络的最新证据表明，分隔这些结果的边界显示出分形特征 [1]。基于这些见解，本研究通过采用更一致的收敛度量并检查注意力和完全连接层的学习率超参数格局，将它们扩展到中等大小的仅解码器转换器架构。结果表明，可训练性边界不是一个简单的阈值；相反，它在多个尺度上形成一个自相似但看似随机的结构，具有统计一致性和重复模式。在这个格局中，一个稳定的收敛区域被一个复杂的混沌边界包围，说明了底层训练动态的敏感性。

##### **Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images**
2501.04283v1 by Yuze Wang, Rong Xiao, Haifeng Li, Mariana Belgiu, Chao Tao

In remote sensing scene classification, leveraging the transfer methods with
well-trained optical models is an efficient way to overcome label scarcity.
However, cloud contamination leads to optical information loss and significant
impacts on feature distribution, challenging the reliability and stability of
transferred target models. Common solutions include cloud removal for optical
data or directly using Synthetic aperture radar (SAR) data in the target
domain. However, cloud removal requires substantial auxiliary data for support
and pre-training, while directly using SAR disregards the unobstructed portions
of optical data. This study presents a scene classification transfer method
that synergistically combines multi-modality data, which aims to transfer the
source domain model trained on cloudfree optical data to the target domain that
includes both cloudy optical and SAR data at low cost. Specifically, the
framework incorporates two parts: (1) the collaborative transfer strategy,
based on knowledge distillation, enables the efficient prior knowledge transfer
across heterogeneous data; (2) the information regulation mechanism (IRM) is
proposed to address the modality imbalance issue during transfer. It employs
auxiliary models to measure the contribution discrepancy of each modality, and
automatically balances the information utilization of modalities during the
target model learning process at the sample-level. The transfer experiments
were conducted on simulated and real cloud datasets, demonstrating the superior
performance of the proposed method compared to other solutions in cloud-covered
scenarios. We also verified the importance and limitations of IRM, and further
discussed and visualized the modality imbalance problem during the model
transfer. Codes are available at https://github.com/wangyuze-csu/ESCCS

摘要：<paragraph>在遥感场景分类中，利用经过良好训练的光学模型的迁移方法是克服标签稀缺的有效方法。
然而，云污染导致光学信息丢失，并对特征分布产生重大影响，对迁移目标模型的可靠性和稳定性构成挑战。常见的解决方案包括去除光学数据的云或直接在目标域中使用合成孔径雷达 (SAR) 数据。然而，去除云需要大量的辅助数据来支持和预训练，而直接使用 SAR 则忽略了光学数据的畅通部分。本研究提出了一种场景分类迁移方法，该方法协同组合多模态数据，旨在将训练在无云光学数据上的源域模型转移到同时包含多云光学和 SAR 数据的目标域，且成本低。具体来说，该框架包含两个部分：(1) 基于知识蒸馏的协作迁移策略，能够跨异构数据进行有效的先验知识迁移；(2) 提出信息调节机制 (IRM) 以解决迁移期间的模态不平衡问题。它采用辅助模型来测量每个模态的贡献差异，并在样本级目标模型学习过程中自动平衡模态的信息利用。在模拟和真实云数据集上进行了迁移实验，结果表明，与其他解决方案相比，该方法在云覆盖场景中表现出卓越的性能。我们还验证了 IRM 的重要性和局限性，并进一步讨论和可视化了模型迁移过程中的模态不平衡问题。代码可在 https://github.com/wangyuze-csu/ESCCS 获得</paragraph>

##### **Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning**
2501.04266v1 by Lang Xu, Quentin Anthony, Jacob Hatef, Aamir Shafi, Hari Subramoni, Dhabaleswar K., Panda

Scaling up Large Language Model(LLM) training involves fitting a tremendous
amount of training parameters across a limited number of workers. However,
methods like ZeRO-3 that drastically reduce GPU memory pressure often incur
heavy communication to ensure global synchronization and consistency.
Established efforts such as ZeRO++ use secondary partitions to avoid inter-node
communications, given that intra-node GPU-GPU transfer generally has more
bandwidth and lower latency than inter-node connections. However, as more
capable infrastructure like Frontier, equipped with AMD GPUs, emerged with
impressive computing capability, there is a need for investigations on the
hardware topology and to develop targeted strategies to improve training
efficiency. In this work, we propose a collection of communication and
optimization strategies for ZeRO++ to reduce communication costs and improve
memory utilization. In this paper, we propose a 3-level hierarchical
partitioning specifically for the current Top-1 supercomputing cluster,
Frontier, which aims at leveraging various bandwidths across layers of
communications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication
overhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU
when compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for
up to 384 GCDs. To the best of our knowledge, our work is also the first effort
to efficiently optimize LLM workloads on Frontier AMD GPUs.

摘要：大语言模型 (LLM) 训练的扩展涉及在有限数量的 worker 中拟合大量的训练参数。然而，像 ZeRO-3 这样的方法大幅降低了 GPU 内存压力，通常会产生大量的通信，以确保全局同步和一致性。已建立的努力，例如 ZeRO++，使用辅助分区来避免节点间通信，因为节点内 GPU-GPU 传输通常比节点间连接具有更大的带宽和更低的延迟。然而，随着配备 AMD GPU 的 Frontier 等功能更强大的基础设施的出现，具有令人印象深刻的计算能力，需要对硬件拓扑进行调查并制定有针对性的策略来提高训练效率。在这项工作中，我们提出了一组用于 ZeRO++ 的通信和优化策略，以降低通信成本并提高内存利用率。在本文中，我们针对当前的 Top-1 超级计算集群 Frontier 提出了一种 3 级分层分区，其目的是利用跨通信层（GCD-GCD、GPU-GPU 和节点间）的各种带宽来减少通信开销。对于 20B GPT 模型，我们观察到与 ZeRO++ 相比，每个 GPU 的 TFLOPS 增加了 1.71 倍，最多可达 384 个 GCD，并且对于最多 384 个 GCD，缩放效率为 0.94。据我们所知，我们的工作也是第一个在 Frontier AMD GPU 上有效优化 LLM 工作负载的努力。

##### **Integrated Offline and Online Learning to Solve a Large Class of Scheduling Problems**
2501.04253v1 by Anbang Liu, Zhi-Long Chen, Jinyang Jiang, Xi Chen

In this paper, we develop a unified machine learning (ML) approach to predict
high-quality solutions for single-machine scheduling problems with a
non-decreasing min-sum objective function with or without release times. Our ML
approach is novel in three major aspects. First, our approach is developed for
the entire class of the aforementioned problems. To achieve this, we exploit
the fact that the entire class of the problems considered can be formulated as
a time-indexed formulation in a unified manner. We develop a deep neural
network (DNN) which uses the cost parameters in the time-indexed formulation as
the inputs to effectively predict a continuous solution to this formulation,
based on which a feasible discrete solution is easily constructed. The second
novel aspect of our approach lies in how the DNN model is trained. In view of
the NP-hard nature of the problems, labels (i.e., optimal solutions) are hard
to generate for training. To overcome this difficulty, we generate and utilize
a set of special instances, for which optimal solutions can be found with
little computational effort, to train the ML model offline. The third novel
idea we employ in our approach is that we develop an online single-instance
learning approach to fine tune the parameters in the DNN for a given online
instance, with the goal of generating an improved solution for the given
instance. To this end, we develop a feasibility surrogate that approximates the
objective value of a given instance as a continuous function of the outputs of
the DNN, which then enables us to derive gradients and update the learnable
parameters in the DNN. Numerical results show that our approach can efficiently
generate high-quality solutions for a variety of single-machine scheduling
min-sum problems with up to 1000 jobs.

摘要：<paragraph>在本文中，我们开发了一种统一的机器学习 (ML) 方法来预测具有或不具有释放时间的非递减最小和目标函数的单机调度问题的优质解决方案。我们的 ML 方法在三个主要方面是新颖的。首先，我们的方法是为上述所有问题类别开发的。为了实现这一点，我们利用了这样一个事实：所考虑问题的整个类别可以用统一的方式表述为时间索引公式。我们开发了一个深度神经网络 (DNN)，它使用时间索引公式中的成本参数作为输入，以有效地预测该公式的连续解，在此基础上可以轻松构建可行的离散解。我们方法的第二个新颖之处在于 DNN 模型的训练方式。鉴于问题的 NP 困难性质，很难生成标签（即最优解）用于训练。为了克服这个困难，我们生成并利用了一组特殊实例，可以用很少的计算工作找到最优解，以离线训练 ML 模型。我们在方法中采用的第三个新颖思想是，我们开发了一种在线单实例学习方法，以微调给定在线实例的 DNN 中的参数，目的是为给定实例生成改进的解决方案。为此，我们开发了一个可行性代理，它将给定实例的目标值近似为 DNN 输出的连续函数，然后使我们能够导出梯度并更新 DNN 中可学习的参数。数值结果表明，我们的方法可以有效地生成各种单机调度最小和问题的优质解决方案，这些问题最多有 1000 个作业。</paragraph>

##### **IOLBENCH: Benchmarking LLMs on Linguistic Reasoning**
2501.04249v1 by Satyam Goyal, Soham Dan

Despite the remarkable advancements and widespread applications of deep
neural networks, their ability to perform reasoning tasks remains limited,
particularly in domains requiring structured, abstract thought. In this paper,
we investigate the linguistic reasoning capabilities of state-of-the-art large
language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from
International Linguistics Olympiad (IOL) problems. This dataset encompasses
diverse problems testing syntax, morphology, phonology, and semantics, all
carefully designed to be self-contained and independent of external knowledge.
These tasks challenge models to engage in metacognitive linguistic reasoning,
requiring the deduction of linguistic rules and patterns from minimal examples.
Through extensive benchmarking of leading LLMs, we find that even the most
advanced models struggle to handle the intricacies of linguistic complexity,
particularly in areas demanding compositional generalization and rule
abstraction. Our analysis highlights both the strengths and persistent
limitations of current models in linguistic problem-solving, offering valuable
insights into their reasoning capabilities. By introducing IOLBENCH, we aim to
foster further research into developing models capable of human-like reasoning,
with broader implications for the fields of computational linguistics and
artificial intelligence.

摘要：儘管深度神經網路有顯著的進展和廣泛的應用，它們執行推理任務的能力仍然有限，特別是在需要結構化抽象思維的領域。在本文中，我們透過引入 IOLBENCH，一個源自國際語言奧林匹克競賽 (IOL) 問題的新基準，來探討最先進的大語言模型 (LLM) 的語言推理能力。此資料集包含了測試句法、形態、音韻和語義的各種問題，所有問題都經過仔細設計，以自成一體且獨立於外部知識。這些任務挑戰模型從最少的範例中推演出語言規則和模式，從而從事元認知語言推理。透過對領先的 LLM 進行廣泛的基準測試，我們發現即使是最先進的模型也很難處理語言複雜性的錯綜複雜，特別是在需要組合概括和規則抽象的領域。我們的分析突出了當前模型在語言問題解決中的優點和持續存在的限制，為它們的推理能力提供了寶貴的見解。透過引入 IOLBENCH，我們旨在促進進一步的研究，以開發具備類人推理能力的模型，並對計算語言學和人工智慧領域產生更廣泛的影響。

##### **Agent Laboratory: Using LLM Agents as Research Assistants**
2501.04227v1 by Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum

Historically, scientific discovery has been a lengthy and costly process,
demanding substantial time and resources from initial conception to final
results. To accelerate scientific discovery, reduce research costs, and improve
research quality, we introduce Agent Laboratory, an autonomous LLM-based
framework capable of completing the entire research process. This framework
accepts a human-provided research idea and progresses through three
stages--literature review, experimentation, and report writing to produce
comprehensive research outputs, including a code repository and a research
report, while enabling users to provide feedback and guidance at each stage. We
deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple
researchers to assess its quality by participating in a survey, providing human
feedback to guide the research process, and then evaluate the final paper. We
found that: (1) Agent Laboratory driven by o1-preview generates the best
research outcomes; (2) The generated machine learning code is able to achieve
state-of-the-art performance compared to existing methods; (3) Human
involvement, providing feedback at each stage, significantly improves the
overall quality of research; (4) Agent Laboratory significantly reduces
research expenses, achieving an 84% decrease compared to previous autonomous
research methods. We hope Agent Laboratory enables researchers to allocate more
effort toward creative ideation rather than low-level coding and writing,
ultimately accelerating scientific discovery.

摘要：<paragraph>從歷史上來看，科學發現一直是一個漫長且昂貴的過程，
從最初的概念到最終的結果都需要大量的時間和資源。為了加速科學發現、降低研究成本並改善
研究品質，我們引入了 Agent Laboratory，一個基於自主式 LLM 的
架構，能夠完成整個研究過程。此架構
接受人類提供的研究構想，並進行三個階段的進展——文獻回顧、實驗和報告撰寫，以產生
全面的研究成果，包括程式碼儲存庫和研究
報告，同時讓使用者在每個階段提供回饋和指導。我們
使用各種最先進的 LLM 部署 Agent Laboratory，並邀請多位
研究人員透過參與調查、提供人類回饋來指導研究過程，然後評估最終論文的品質。我們
發現：(1) 由 o1-preview 驅動的 Agent Laboratory 產生了最佳
研究成果；(2) 生成的機器學習程式碼能夠達到
與現有方法相比最先進的效能；(3) 人類參與，在每個階段提供回饋，顯著改善
整體研究品質；(4) Agent Laboratory 大幅降低
研究開支，與先前的自主研究方法相比，減少了 84%。我們希望 Agent Laboratory 能讓研究人員將更多
精力分配到創意的構想，而不是低階的編碼和寫作，最終加速科學發現。</paragraph>

##### **Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images**
2501.04217v1 by Ren Tasai, Guang Li, Ren Togo, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Kenji Hirata, Takahiro Ogawa, Kohsuke Kudo, Miki Haseyama

We propose a novel continual self-supervised learning method (CSSL)
considering medical domain knowledge in chest CT images. Our approach addresses
the challenge of sequential learning by effectively capturing the relationship
between previously learned knowledge and new information at different stages.
By incorporating an enhanced DER into CSSL and maintaining both diversity and
representativeness within the rehearsal buffer of DER, the risk of data
interference during pretraining is reduced, enabling the model to learn more
richer and robust feature representations. In addition, we incorporate a mixup
strategy and feature distillation to further enhance the model's ability to
learn meaningful representations. We validate our method using chest CT images
obtained under two different imaging conditions, demonstrating superior
performance compared to state-of-the-art methods.

摘要：我們提出了一種新的持續自我監督學習方法 (CSSL)，考量了胸部電腦斷層影像中的醫學領域知識。我們的做法透過有效捕捉先前學習的知識與不同階段的新資訊之間的關係，來解決循序學習的挑戰。透過將增強的 DER 納入 CSSL，並在 DER 的排練緩衝區內維持多樣性和代表性，預訓練期間資料干擾的風險降低，使模型能夠學習更豐富且強健的特徵表徵。此外，我們納入混淆策略和特徵萃取，進一步增強模型學習有意義表徵的能力。我們使用在兩種不同影像條件下取得的胸部電腦斷層影像驗證我們的模型，證明與現有技術相比具有優異的效能。

##### **UPAQ: A Framework for Real-Time and Energy-Efficient 3D Object Detection in Autonomous Vehicles**
2501.04213v1 by Abhishek Balasubramaniam, Febin P Sunny, Sudeep Pasricha

To enhance perception in autonomous vehicles (AVs), recent efforts are
concentrating on 3D object detectors, which deliver more comprehensive
predictions than traditional 2D object detectors, at the cost of increased
memory footprint and computational resource usage. We present a novel framework
called UPAQ, which leverages semi-structured pattern pruning and quantization
to improve the efficiency of LiDAR point-cloud and camera-based 3D object
detectors on resource-constrained embedded AV platforms. Experimental results
on the Jetson Orin Nano embedded platform indicate that UPAQ achieves up to
5.62x and 5.13x model compression rates, up to 1.97x and 1.86x boost in
inference speed, and up to 2.07x and 1.87x reduction in energy consumption
compared to state-of-the-art model compression frameworks, on the Pointpillar
and SMOKE models respectively.

摘要：為了增強自動駕駛車輛 (AV) 的感知能力，最近的努力集中在 3D 物件偵測器，它提供比傳統 2D 物件偵測器更全面的預測，但代價是增加了記憶體使用量和運算資源使用量。我們提出一個名為 UPAQ 的新框架，它利用半結構化模式剪枝和量化來改善 LiDAR 點雲和基於相機的 3D 物件偵測器在資源受限的嵌入式 AV 平台上的效率。在 Jetson Orin Nano 嵌入式平台上的實驗結果表明，與最先進的模型壓縮框架相比，UPAQ 在 Pointpillar 和 SMOKE 模型上分別實現了高達 5.62 倍和 5.13 倍的模型壓縮率、高達 1.97 倍和 1.86 倍的推理速度提升，以及高達 2.07 倍和 1.87 倍的能耗降低。

##### **CURing Large Models: Compression via CUR Decomposition**
2501.04211v1 by Sanghyeon Park, Soo-Mook Moon

Large deep learning models have achieved remarkable success but are
resource-intensive, posing challenges in computational cost and memory usage.
  We introduce CURing, a novel model compression method based on CUR matrix
decomposition, which approximates weight matrices as the product of selected
columns (C) and rows (R), and a small linking matrix (U). We apply this
decomposition to weights chosen based on the combined influence of their
magnitudes and activations. By identifying and retaining informative rows and
columns, CURing significantly reduces model size with minimal performance loss.
  It preserves the original network's input/output structures, retains
important features such as non-negativity, and the compressed model's
activation patterns align with the original, thereby enhancing
interpretability.

摘要：大型深度學習模型已取得顯著成功，但資源密集，對運算成本和記憶體使用構成挑戰。
我們引入了 CURing，一種基於 CUR 矩陣分解的新型模型壓縮方法，它將權重矩陣近似為所選列 (C) 和行 (R) 的乘積，以及一個小的連結矩陣 (U)。我們將此分解應用於根據其大小和激活的綜合影響選擇的權重。通過識別和保留有資訊的行和列，CURing 大幅減少模型大小，且效能損失極小。
它保留原始網路的輸入/輸出結構，保留非負性等重要特徵，且壓縮模型的激活模式與原始模式一致，從而增強了解釋性。

##### **Generative Dataset Distillation Based on Self-knowledge Distillation**
2501.04202v1 by Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama

Dataset distillation is an effective technique for reducing the cost and
complexity of model training while maintaining performance by compressing large
datasets into smaller, more efficient versions. In this paper, we present a
novel generative dataset distillation method that can improve the accuracy of
aligning prediction logits. Our approach integrates self-knowledge distillation
to achieve more precise distribution matching between the synthetic and
original data, thereby capturing the overall structure and relationships within
the data. To further improve the accuracy of alignment, we introduce a
standardization step on the logits before performing distribution matching,
ensuring consistency in the range of logits. Through extensive experiments, we
demonstrate that our method outperforms existing state-of-the-art methods,
resulting in superior distillation performance.

摘要：資料集蒸餾是一種有效技術，可用於降低模型訓練的成本和複雜性，同時透過將大型資料集壓縮成更小、更有效率的版本來維持效能。在本文中，我們提出了一種新穎的生成資料集蒸餾方法，可以改善對齊預測 logit 的準確性。我們的做法整合自我知識蒸餾，以在合成資料和原始資料之間達成更精確的分布比對，從而擷取資料中的整體結構和關係。為了進一步改善對齊的準確性，我們在執行分布比對之前，於 logit 上引入標準化步驟，確保 logit 範圍的一致性。透過廣泛的實驗，我們證明了我們的方法優於現有的最先進方法，進而產生出色的蒸餾效能。

##### **HIVEX: A High-Impact Environment Suite for Multi-Agent Research (extended version)**
2501.04180v1 by Philipp D. Siedler

Games have been vital test beds for the rapid development of Agent-based
research. Remarkable progress has been achieved in the past, but it is unclear
if the findings equip for real-world problems. While pressure grows, some of
the most critical ecological challenges can find mitigation and prevention
solutions through technology and its applications. Most real-world domains
include multi-agent scenarios and require machine-machine and human-machine
collaboration. Open-source environments have not advanced and are often toy
scenarios, too abstract or not suitable for multi-agent research. By mimicking
real-world problems and increasing the complexity of environments, we hope to
advance state-of-the-art multi-agent research and inspire researchers to work
on immediate real-world problems. Here, we present HIVEX, an environment suite
to benchmark multi-agent research focusing on ecological challenges. HIVEX
includes the following environments: Wind Farm Control, Wildfire Resource
Management, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial
Wildfire Suppression. We provide environments, training examples, and baselines
for the main and sub-tasks. All trained models resulting from the experiments
of this work are hosted on Hugging Face. We also provide a leaderboard on
Hugging Face and encourage the community to submit models trained on our
environment suite.

摘要：遊戲一直是基於代理的研究快速發展的重要測試環境。過去取得了顯著的進展，但目前尚不清楚這些發現是否能解決現實世界中的問題。隨著壓力的增加，一些最關鍵的生態挑戰可以透過技術及其應用找到緩解和預防的解決方案。大多數現實世界的領域都包括多代理場景，並需要機器與機器以及人與機器之間的協作。開源環境尚未進步，而且通常是玩具場景，過於抽象或不適合多代理研究。透過模擬現實世界的問題並增加環境的複雜性，我們希望推進最先進的多代理研究，並激勵研究人員致力於解決現實世界中的問題。在此，我們提出 HIVEX，一個環境套件，用於基準化專注於生態挑戰的多代理研究。HIVEX 包含以下環境：風力發電場控制、野火資源管理、無人機造林、海洋塑膠收集和空中野火撲滅。我們提供環境、訓練範例和主要任務與子任務的基準。所有因本研究的實驗而訓練出的模型都託管在 Hugging Face 上。我們也在 Hugging Face 上提供一個排行榜，並鼓勵社群提交在我們的環境套件上訓練出的模型。

##### **Multimodal Multihop Source Retrieval for Web Question Answering**
2501.04173v1 by Navya Yarrabelly, Saloni Mittal

This work deals with the challenge of learning and reasoning over multi-modal
multi-hop question answering (QA). We propose a graph reasoning network based
on the semantic structure of the sentences to learn multi-source reasoning
paths and find the supporting facts across both image and text modalities for
answering the question. In this paper, we investigate the importance of graph
structure for multi-modal multi-hop question answering. Our analysis is
centered on WebQA. We construct a strong baseline model, that finds relevant
sources using a pairwise classification task. We establish that, with the
proper use of feature representations from pre-trained models, graph structure
helps in improving multi-modal multi-hop question answering. We point out that
both graph structure and adjacency matrix are task-related prior knowledge, and
graph structure can be leveraged to improve the retrieval performance for the
task. Experiments and visualized analysis demonstrate that message propagation
over graph networks or the entire graph structure can replace massive
multimodal transformers with token-wise cross-attention. We demonstrated the
applicability of our method and show a performance gain of \textbf{4.6$\%$}
retrieval F1score over the transformer baselines, despite being a very light
model. We further demonstrated the applicability of our model to a large scale
retrieval setting.

摘要：本研究應對多模態多跳問題解答 (QA) 的學習和推理挑戰。我們提出一個基於句子語義結構的圖推理網路，用於學習多來源推理路徑，並在影像和文字模態中找到支撐事實，以回答問題。在本文中，我們探討圖結構對於多模態多跳問題解答的重要性。我們的分析集中在 WebQA。我們建構了一個強大的基線模型，它使用成對分類任務來找出相關來源。我們確立了，透過適當使用預訓練模型中的特徵表示，圖結構有助於改善多模態多跳問題解答。我們指出，圖結構和鄰接矩陣都是與任務相關的先驗知識，並且圖結構可用於改善任務的檢索效能。實驗和視覺化分析表明，圖網路或整個圖結構上的訊息傳播可以取代具有令牌明智交叉注意力的巨大多模態Transformer。我們展示了我們方法的適用性，並且展示了比Transformer基線高出 \textbf{4.6$\%$} 的檢索 F1 分數，儘管它是一個非常輕量級的模型。我們進一步展示了我們的模型對大規模檢索設定的適用性。

##### **Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation**
2501.04167v1 by Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Tao Chen, Zhuowan Li, Michael Bendersky, Hamed Zamani

Personalized text generation requires a unique ability of large language
models (LLMs) to learn from context that they often do not encounter during
their standard training. One way to encourage LLMs to better use personalized
context for generating outputs that better align with the user's expectations
is to instruct them to reason over the user's past preferences, background
knowledge, or writing style. To achieve this, we propose Reasoning-Enhanced
Self-Training for Personalized Text Generation (REST-PG), a framework that
trains LLMs to reason over personal data during response generation. REST-PG
first generates reasoning paths to train the LLM's reasoning abilities and then
employs Expectation-Maximization Reinforced Self-Training to iteratively train
the LLM based on its own high-reward outputs. We evaluate REST-PG on the
LongLaMP benchmark, consisting of four diverse personalized long-form text
generation tasks. Our experiments demonstrate that REST-PG achieves significant
improvements over state-of-the-art baselines, with an average relative
performance gain of 14.5% on the benchmark.

摘要：個人化文字生成需要大型語言模型 (LLM) 具備一項獨特的能力，即從它們在標準訓練期間通常不會遇到的脈絡中學習。鼓勵 LLM 更佳利用個人化脈絡來產生更符合使用者預期的輸出的方法之一，就是指示它們根據使用者的過往偏好、背景知識或寫作風格進行推理。為達成此目標，我們提出個人化文字生成推理增強自訓練 (REST-PG)，這是一個訓練 LLM 在回應生成期間對個人資料進行推理的架構。REST-PG 首先產生推理路徑來訓練 LLM 的推理能力，然後採用期望最大化強化自訓練，根據 LLM 自身的高獎勵輸出反覆訓練 LLM。我們在 LongLaMP 基準上評估 REST-PG，該基準包含四項不同的個人化長篇文字生成任務。我們的實驗證明 REST-PG 在最先進的基準上取得顯著進步，在基準上平均相對效能提升 14.5%。

##### **MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation**
2501.04155v1 by Siddharth Joshi, Besmira Nushi, Vidhisha Balachandran, Varun Chandrasekaran, Vibhav Vineet, Neel Joshi, Baharan Mirzasoleiman

Vision-language models (VLMs) are highly effective but often underperform on
specialized tasks; for example, Llava-1.5 struggles with chart and diagram
understanding due to scarce task-specific training data. Existing training
data, sourced from general-purpose datasets, fails to capture the nuanced
details needed for these tasks. We introduce MM-Gen, a scalable method that
generates task-specific, high-quality synthetic text for candidate images by
leveraging stronger models. MM-Gen employs a three-stage targeted process:
partitioning data into subgroups, generating targeted text based on task
descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs
with data generated by MM-Gen leads to significant performance gains, including
29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B).
Compared to human-curated caption data, MM-Gen achieves up to 1.6x better
improvements for the original models, proving its effectiveness in enhancing
task-specific VLM performance and bridging the gap between general-purpose
datasets and specialized requirements. Code available at
https://github.com/sjoshi804/MM-Gen.

摘要：視覺語言模型 (VLM) 非常有效，但通常在專門任務上表現不佳；例如，由於缺乏特定任務的訓練資料，Llava-1.5 在理解圖表和圖解時會遇到困難。現有的訓練資料來自一般用途的資料集，無法擷取這些任務所需的細微細節。我們引入了 MM-Gen，這是一種可擴充的方法，它利用更強大的模型為候選影像產生特定於任務的高品質合成文字。MM-Gen 採用一個三階段目標流程：將資料分割成子群組、根據任務說明產生目標文字，並過濾掉多餘和異常資料。使用 MM-Gen 產生的資料微調 VLM 會帶來顯著的效能提升，包括 Llava-1.5 (7B) 的空間推理提升 29%，圖解理解提升 15%。與人工策展的標題資料相比，MM-Gen 為原始模型實現了高達 1.6 倍的改進，證明了其在增強特定任務的 VLM 效能和彌合一般用途資料集與特定需求之間的差距方面的有效性。程式碼可於 https://github.com/sjoshi804/MM-Gen 取得。

##### **Multilingual Open QA on the MIA Shared Task**
2501.04153v1 by Navya Yarrabelly, Saloni Mittal, Ketan Todi, Kimihiro Hasegawa

Cross-lingual information retrieval (CLIR) ~\cite{shi2021cross, asai2021one,
jiang2020cross} for example, can find relevant text in any language such as
English(high resource) or Telugu (low resource) even when the query is posed in
a different, possibly low-resource, language. In this work, we aim to develop
useful CLIR models for this constrained, yet important, setting where we do not
require any kind of additional supervision or labelled data for retrieval task
and hence can work effectively for low-resource languages.
  \par We propose a simple and effective re-ranking method for improving
passage retrieval in open question answering. The re-ranker re-scores retrieved
passages with a zero-shot multilingual question generation model, which is a
pre-trained language model, to compute the probability of the input question in
the target language conditioned on a retrieved passage, which can be possibly
in a different language. We evaluate our method in a completely zero shot
setting and doesn't require any training. Thus the main advantage of our method
is that our approach can be used to re-rank results obtained by any sparse
retrieval methods like BM-25. This eliminates the need for obtaining expensive
labelled corpus required for the retrieval tasks and hence can be used for low
resource languages.

摘要：跨語言資訊檢索 (CLIR) ~\cite{shi2021cross, asai2021one,
jiang2020cross} 例如，可以在任何語言中找到相關文字，例如英文（高資源）或泰盧固語（低資源），即使查詢是用不同的語言（可能是低資源）提出的。在此研究中，我們旨在為此受限但重要的設定開發有用的 CLIR 模型，在此設定中，我們不需要任何類型的額外監督或標籤資料進行檢索任務，因此可以有效地用於低資源語言。
\par 我們提出了一種簡單且有效的重新排序方法，用於改善開放式問答中的段落檢索。重新排序器使用零次學習多語言問題生成模型（一種預訓練語言模型）重新評分檢索到的段落，以計算目標語言中輸入問題在檢索到的段落中的機率，而該段落可能使用不同的語言。我們在完全零次學習的設定中評估我們的模型，不需要任何訓練。因此，我們的方法的主要優點是，我們的方法可以用於重新排序任何稀疏檢索方法（例如 BM-25）獲得的結果。這消除了獲取檢索任務所需昂貴標記語料庫的需要，因此可用於低資源語言。

##### **BiasGuard: Guardrailing Fairness in Machine Learning Production Systems**
2501.04142v1 by Nurit Cohen-Inger, Seffi Cohen, Neomi Rabaev, Lior Rokach, Bracha Shapira

As machine learning (ML) systems increasingly impact critical sectors such as
hiring, financial risk assessments, and criminal justice, the imperative to
ensure fairness has intensified due to potential negative implications. While
much ML fairness research has focused on enhancing training data and processes,
addressing the outputs of already deployed systems has received less attention.
This paper introduces 'BiasGuard', a novel approach designed to act as a
fairness guardrail in production ML systems. BiasGuard leverages Test-Time
Augmentation (TTA) powered by Conditional Generative Adversarial Network
(CTGAN), a cutting-edge generative AI model, to synthesize data samples
conditioned on inverted protected attribute values, thereby promoting equitable
outcomes across diverse groups. This method aims to provide equal opportunities
for both privileged and unprivileged groups while significantly enhancing the
fairness metrics of deployed systems without the need for retraining. Our
comprehensive experimental analysis across diverse datasets reveals that
BiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%
compared to non-mitigated benchmarks. Additionally, BiasGuard outperforms
existing post-processing methods in improving fairness, positioning it as an
effective tool to safeguard against biases when retraining the model is
impractical.

摘要：隨著機器學習 (ML) 系統日益影響招聘、財務風險評估和刑事司法等關鍵部門，由於潛在的負面影響，確保公平性的必要性也隨之加劇。雖然許多 ML 公平性研究都專注於加強訓練資料和流程，但對於已部署系統的輸出，則較少受到關注。本文介紹「BiasGuard」，這是一種新穎的方法，旨在作為生產 ML 系統中的公平性防護措施。BiasGuard 透過條件生成對抗網路 (CTGAN) 驅動的測試時間擴充 (TTA) 來提升效能，CTGAN 是一種尖端的生成式 AI 模型，可根據反向受保護屬性值來合成資料樣本，從而促進不同群體之間的公平結果。此方法旨在為特權和非特權群體提供平等的機會，同時大幅提升已部署系統的公平性指標，而無需重新訓練。我們在各種資料集上進行的全面實驗分析顯示，BiasGuard 可將公平性提升 31%，而與未緩解的基準相比，準確度僅降低 0.09%。此外，BiasGuard 在改善公平性方面優於現有的後處理方法，使其成為在重新訓練模型不切實際時，用於防範偏差的有效工具。

##### **"Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?**
2501.04138v1 by Benjamin Reichman, Kartik Talamadupula

Human communication is a multifaceted and multimodal skill. Communication
requires an understanding of both the surface-level textual content and the
connotative intent of a piece of communication. In humans, learning to go
beyond the surface level starts by learning communicative intent in speech.
Once humans acquire these skills in spoken communication, they transfer those
skills to written communication. In this paper, we assess the ability of
speech+text models and text models trained with special emphasis on
human-to-human conversations to make this multimodal transfer of skill. We
specifically test these models on their ability to detect covert deceptive
communication. We find that with no special prompting speech+text LLMs have an
advantage over unimodal LLMs in performing this task. Likewise, we find that
human-to-human conversation-trained LLMs are also advantaged in this skill.

摘要：人類的溝通是一種多面向且多模態的技能。溝通
需要理解溝通內容的表面文字內容和內涵意圖。在人類中，學習超越表層的起點是學習言語中的溝通意圖。
一旦人類在口語溝通中習得這些技能，他們就會將這些
技能轉移到書面溝通中。在本文中，我們評估了
語音+文字模型和文字模型在特別強調人對人對話的情況下訓練的能力，以進行這種多模態技能轉移。我們
特別測試了這些模型檢測隱蔽欺騙性溝通的能力。我們發現，在沒有特別提示的情況下，語音+文字 LLM 在執行此任務時優於單模態 LLM。同樣，我們發現，接受人對人對話訓練的 LLM 在這項技能上也具有優勢。

##### **Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach**
2501.04136v1 by Hicham Assoudi, Hakim Lounis

Several approaches are proposed to deal with the problem of the Automatic
Schema Matching (ASM). The challenges and difficulties caused by the complexity
and uncertainty characterizing both the process and the outcome of Schema
Matching motivated us to investigate how bio-inspired emerging paradigm can
help with understanding, managing, and ultimately overcoming those challenges.
In this paper, we explain how we approached Automatic Schema Matching as a
systemic and Complex Adaptive System (CAS) and how we modeled it using the
approach of Agent-Based Modeling and Simulation (ABMS). This effort gives birth
to a tool (prototype) for schema matching called Reflex-SMAS. A set of
experiments demonstrates the viability of our approach on two main aspects: (i)
effectiveness (increasing the quality of the found matchings) and (ii)
efficiency (reducing the effort required for this efficiency). Our approach
represents a significant paradigm-shift, in the field of Automatic Schema
Matching.

摘要：為了處理自動模式比對 (ASM) 的問題，提出了幾種方法。模式比對的過程和結果所具有的複雜性和不確定性所造成的挑戰和困難，促使我們探討生物靈感新興典範如何協助理解、管理和最終克服這些挑戰。在本文中，我們說明我們如何將自動模式比對視為一個系統性和複雜適應系統 (CAS)，以及我們如何使用基於代理的建模和模擬 (ABMS) 方法對其進行建模。這項努力產生了一個用於模式比對的工具 (原型)，稱為 Reflex-SMAS。一組實驗證明了我們的方法在兩個主要方面的可行性：(i) 有效性（提高找到的比對品質）和 (ii) 效率（減少此效率所需的努力）。我們的方法代表了自動模式比對領域的重大典範轉移。

##### **Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles**
2501.03991v1 by Yuxi Xia, Pedro Henrique Luz de Araujo, Klim Zaporojets, Benjamin Roth

Calibration, the alignment between model confidence and prediction accuracy,
is critical for the reliable deployment of large language models (LLMs).
Existing works neglect to measure the generalization of their methods to other
prompt styles and different sizes of LLMs. To address this, we define a
controlled experimental setting covering 12 LLMs and four prompt styles. We
additionally investigate if incorporating the response agreement of multiple
LLMs and an appropriate loss function can improve calibration performance.
Concretely, we build Calib-n, a novel framework that trains an auxiliary model
for confidence estimation that aggregates responses from multiple LLMs to
capture inter-model agreement. To optimize calibration, we integrate focal and
AUC surrogate losses alongside binary cross-entropy. Experiments across four
datasets demonstrate that both response agreement and focal loss improve
calibration from baselines. We find that few-shot prompts are the most
effective for auxiliary model-based methods, and auxiliary models demonstrate
robust calibration performance across accuracy variations, outperforming LLMs'
internal probabilities and verbalized confidences. These insights deepen the
understanding of influence factors in LLM calibration, supporting their
reliable deployment in diverse applications.

摘要：校準，模型信心與預測準確度之間的對齊，對於大型語言模型 (LLM) 的可靠部署至關重要。現有研究忽略了衡量其方法對其他提示樣式和不同規模 LLM 的概括性。為了解決這個問題，我們定義了一個受控實驗設定，涵蓋 12 個 LLM 和四種提示樣式。我們進一步研究了是否納入多個 LLM 的回應協議和適當的損失函數可以改善校準性能。具體來說，我們構建了 Calib-n，一個新穎的框架，用於訓練一個輔助模型以進行信心估計，該模型彙總來自多個 LLM 的回應以捕捉模型間協議。為了優化校準，我們整合了焦點和 AUC 替代損失以及二元交叉熵。在四個數據集上的實驗表明，回應協議和焦點損失都改善了基準校準。我們發現，少次提示對於基於輔助模型的方法最有效，並且輔助模型在準確性變化中展示了穩健的校準性能，優於 LLM 的內部概率和言語化信心。這些見解加深了對 LLM 校準中影響因素的理解，支持了它們在各種應用中的可靠部署。

##### **Semantically Cohesive Word Grouping in Indian Languages**
2501.03988v1 by N J Karthika, Adyasha Patra, Nagasai Saketh Naidu, Arnab Bhattacharya, Ganesh Ramakrishnan, Chaitali Dangarikar

Indian languages are inflectional and agglutinative and typically follow
clause-free word order. The structure of sentences across most major Indian
languages are similar when their dependency parse trees are considered. While
some differences in the parsing structure occur due to peculiarities of a
language or its preferred natural way of conveying meaning, several apparent
differences are simply due to the granularity of representation of the smallest
semantic unit of processing in a sentence. The semantic unit is typically a
word, typographically separated by whitespaces. A single whitespace-separated
word in one language may correspond to a group of words in another. Hence,
grouping of words based on semantics helps unify the parsing structure of
parallel sentences across languages and, in the process, morphology. In this
work, we propose word grouping as a major preprocessing step for any
computational or linguistic processing of sentences for Indian languages. Among
Indian languages, since Hindi is one of the least agglutinative, we expect it
to benefit the most from word-grouping. Hence, in this paper, we focus on Hindi
to study the effects of grouping. We perform quantitative assessment of our
proposal with an intrinsic method that perturbs sentences by shuffling words as
well as an extrinsic evaluation that verifies the importance of word grouping
for the task of Machine Translation (MT) using decomposed prompting. We also
qualitatively analyze certain aspects of the syntactic structure of sentences.
Our experiments and analyses show that the proposed grouping technique brings
uniformity in the syntactic structures, as well as aids underlying NLP tasks.

摘要：<paragraph>印度語言是屈折的和黏著的，通常遵循無子句的詞序。當考慮其依存句法分析樹時，大多數主要印度語言的句子結構是相似的。雖然由於語言的特殊性或其傳達含義的自然方式而導致分析結構上存在一些差異，但幾個明顯的差異僅僅是句子中最小的語義處理單元的表示粒度。語義單元通常是一個詞，在排版上由空白分隔。一種語言中單獨的空白分隔詞可能對應於另一種語言中的一組詞。因此，基於語義對詞進行分組有助於統一跨語言的並行句子的分析結構，並在此過程中統一形態。在這項工作中，我們提出詞組作為印度語言的句子進行任何計算或語言處理的主要預處理步驟。在印度語言中，由於印地語是最不黏著的語言之一，我們預計它將從詞組中受益最多。因此，在本文中，我們專注於印地語來研究分組的影響。我們使用一種通過打亂詞彙來擾動句子的內在方法對我們的提案進行定量評估，以及一種驗證詞組對於使用分解提示的機器翻譯 (MT) 任務的重要性。我們還定性地分析了句子的句法結構的某些方面。我們的實驗和分析表明，所提出的分組技術為句法結構帶來了統一性，並有助於 NLP 的基礎任務。</paragraph>

##### **VLM-driven Behavior Tree for Context-aware Task Planning**
2501.03968v1 by Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi

The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)
has recently gained attention in the robotics community, yet remains in its
early stages of development. In this paper, we propose a novel framework that
leverages Vision-Language Models (VLMs) to interactively generate and edit BTs
that address visual conditions, enabling context-aware robot operations in
visually complex environments. A key feature of our approach lies in the
conditional control through self-prompted visual conditions. Specifically, the
VLM generates BTs with visual condition nodes, where conditions are expressed
as free-form text. Another VLM process integrates the text into its prompt and
evaluates the conditions against real-world images during robot execution. We
validated our framework in a real-world cafe scenario, demonstrating both its
feasibility and limitations.

摘要：大型語言模型 (LLM) 用於生成行為樹 (BT) 最近在機器人社群中引起關注，但仍處於發展的早期階段。在本文中，我們提出了一個新穎的架構，利用視覺語言模型 (VLM) 來互動式生成和編輯 BT，以處理視覺條件，並在視覺複雜的環境中實現具備情境感知能力的機器人操作。我們方法的一個關鍵特點在於透過自我提示的視覺條件進行條件控制。具體來說，VLM 會生成帶有視覺條件節點的 BT，其中條件以自由形式文字表示。另一個 VLM 程序會將文字整合到提示中，並在機器人執行期間針對真實世界的影像評估條件。我們在真實世界的咖啡廳場景中驗證了我們的架構，證明了它的可行性和限制。

##### **Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States**
2501.03952v1 by Jurgita Kapočiūtė-Dzikienė, Toms Bergmanis, Mārcis Pinnis

Although large language models (LLMs) have transformed our expectations of
modern language technologies, concerns over data privacy often restrict the use
of commercially available LLMs hosted outside of EU jurisdictions. This limits
their application in governmental, defence, and other data-sensitive sectors.
In this work, we evaluate the extent to which locally deployable open-weight
LLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.
We examine various size and precision variants of the top-performing
multilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine
translation, multiple-choice question answering, and free-form text generation.
The results indicate that while certain models like Gemma~2 perform close to
the top commercially available models, many LLMs struggle with these languages.
Most surprisingly, however, we find that these models, while showing close to
state-of-the-art translation performance, are still prone to lexical
hallucinations with errors in at least 1 in 20 words for all open-weight
multilingual LLMs.

摘要：儘管大型語言模型 (LLM) 已轉變我們對現代語言科技的期待，但對於資料隱私的疑慮經常限制了在歐盟管轄區外託管的商業化 LLM 的使用。這限制了它們在政府、國防和其他資料敏感產業的應用。在這項工作中，我們評估了可本地部署的開放權重 LLM 對立陶宛語、拉脫維亞語和愛沙尼亞語等較少人使用的語言的支援程度。我們檢視了表現最佳的多語言開放權重模型 Llama~3、Gemma~2、Phi 和 NeMo 的各種規模和精準度變體，針對機器翻譯、多選題問答和自由形式文字生成。結果顯示，儘管某些模型（例如 Gemma~2）的表現接近頂尖的商業化模型，但許多 LLM 在處理這些語言時仍有困難。然而，最令人驚訝的是，我們發現這些模型在展現接近最先進的翻譯表現同時，仍容易出現詞彙幻覺，所有開放權重的多語言 LLM 每 20 個字中至少有 1 個字出現錯誤。

##### **Synthetic Data Privacy Metrics**
2501.03941v1 by Amy Steier, Lipika Ramaswamy, Andre Manoel, Alexa Haushalter

Recent advancements in generative AI have made it possible to create
synthetic datasets that can be as accurate as real-world data for training AI
models, powering statistical insights, and fostering collaboration with
sensitive datasets while offering strong privacy guarantees. Effectively
measuring the empirical privacy of synthetic data is an important step in the
process. However, while there is a multitude of new privacy metrics being
published every day, there currently is no standardization. In this paper, we
review the pros and cons of popular metrics that include simulations of
adversarial attacks. We also review current best practices for amending
generative models to enhance the privacy of the data they create (e.g.
differential privacy).

摘要：生成式 AI 近期取得的进步使得创建合成数据集成为可能，这些数据集可以与真实世界数据一样准确，用于训练 AI 模型，为统计洞察力提供支持，并促进对敏感数据集的协作，同时提供强有力的隐私保障。有效衡量合成数据的经验隐私是该过程中的重要一步。然而，虽然每天都会发布大量新的隐私指标，但目前还没有标准化。在本文中，我们回顾了包括对抗攻击模拟在内的流行指标的优点和缺点。我们还回顾了当前为修改生成模型以增强其创建的数据的隐私（例如差分隐私）的最佳实践。

##### **Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection**
2501.03940v1 by Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

The rapid advancement in large language models (LLMs) has significantly
enhanced their ability to generate coherent and contextually relevant text,
raising concerns about the misuse of AI-generated content and making it
critical to detect it. However, the task remains challenging, particularly in
unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution
outputs offers a theoretically appealing approach for detection, as they
encapsulate insights from the models' extensive pre-training on diverse
corpora. Despite its promise, zero-shot methods that attempt to operationalize
these outputs have met with limited success. We hypothesize that one of the
problems is that they use the mean to aggregate next-token distribution metrics
across tokens, when some tokens are naturally easier or harder to predict and
should be weighted differently. Based on this idea, we propose the Perplexity
Attention Weighted Network (PAWN), which uses the last hidden states of the LLM
and positions to weight the sum of a series of features based on metrics from
the next-token distribution across the sequence length. Although not zero-shot,
our method allows us to cache the last hidden states and next-token
distribution metrics on disk, greatly reducing the training resource
requirements. PAWN shows competitive and even better performance
in-distribution than the strongest baselines (fine-tuned LMs) with a fraction
of their trainable parameters. Our model also generalizes better to unseen
domains and source models, with smaller variability in the decision boundary
across distribution shifts. It is also more robust to adversarial attacks, and
if the backbone has multilingual capabilities, it presents decent
generalization to languages not seen during supervised training, with LLaMA3-1B
reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine
languages.

摘要：大型語言模型 (LLM) 的快速進步大幅提升了它們生成連貫且與語境相關文字的能力，引發了對 AI 生成的內容被濫用的擔憂，並使其偵測變得至關重要。然而，這項任務仍然具有挑戰性，尤其是在未見過的領域或不熟悉的 LLM 中。利用 LLM 下一個代幣分佈輸出提供了一個理論上對偵測有吸引力的方法，因為它們概括了模型在各種語料庫上進行廣泛預訓練的見解。儘管有這樣的承諾，但試圖將這些輸出操作化的零次學習方法卻只獲得了有限的成功。我們假設其中一個問題是它們使用平均值來彙總代幣的下一代幣分佈指標，而有些代幣在預測上自然比較容易或困難，並且應該賦予不同的權重。基於這個想法，我們提出了困惑度注意力加權網路 (PAWN)，它使用 LLM 的最後隱藏狀態和位置來根據序列長度中下一代幣分佈的指標加權一系列特徵的總和。儘管不是零次學習，但我們的這種方法允許我們將最後的隱藏狀態和下一代幣分佈指標快取到磁碟中，大幅減少訓練資源需求。PAWN 在分佈中顯示出比最強的基準（微調語言模型）更具競爭力甚至更好的效能，而其可訓練參數只佔它們的一小部分。我們的模型對於未見過的領域和原始模型也有更好的泛化能力，在分佈轉移中決策邊界的變異較小。它對對抗性攻擊也更強健，而且如果主幹具有多語言能力，它會對在監督訓練期間未見過的語言呈現出良好的泛化能力，其中 LLaMA3-1B 在與九種語言的交叉驗證中達到平均巨觀 F1 分數 81.46%。

##### **Multi-armed Bandit and Backbone boost Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problems**
2501.04072v1 by Long Wang, Jiongzhi Zheng, Zhengda Xiong, Kun He

The Lin-Kernighan-Helsguan (LKH) heuristic is a classic local search
algorithm for the Traveling Salesman Problem (TSP). LKH introduces an
$\alpha$-value to replace the traditional distance metric for evaluating the
edge quality, which leads to a significant improvement. However, we observe
that the $\alpha$-value does not make full use of the historical information
during the search, and single guiding information often makes LKH hard to
escape from some local optima. To address the above issues, we propose a novel
way to extract backbone information during the TSP local search process, which
is dynamic and can be updated once a local optimal solution is found. We
further propose to combine backbone information, $\alpha$-value, and distance
to evaluate the edge quality so as to guide the search. Moreover, we abstract
their different combinations to arms in a multi-armed bandit (MAB) and use an
MAB model to help the algorithm select an appropriate evaluation metric
dynamically. Both the backbone information and MAB can provide diverse guiding
information and learn from the search history to suggest the best metric. We
apply our methods to LKH and LKH-3, which is an extension version of LKH that
can be used to solve about 40 variant problems of TSP and Vehicle Routing
Problem (VRP). Extensive experiments show the excellent performance and
generalization capability of our proposed method, significantly improving LKH
for TSP and LKH-3 for two representative TSP and VRP variants, the Colored TSP
(CTSP) and Capacitated VRP with Time Windows (CVRPTW).

摘要：Lin-Kernighan-Helsguan (LKH) 啟發法是一種針對旅行推銷員問題 (TSP) 的經典局部搜尋演算法。LKH 引入一個 $\alpha$-值來取代傳統的距離量度，用於評估邊緣品質，這帶來顯著的改善。然而，我們觀察到 $\alpha$-值並未充分利用搜尋期間的歷史資訊，而且單一的引導資訊通常會讓 LKH 難以逃離某些局部最佳解。為了解決上述問題，我們提出了一種新穎的方法，在 TSP 局部搜尋過程中萃取骨幹資訊，這種資訊是動態的，而且一旦找到局部最佳解就可以更新。我們進一步提出結合骨幹資訊、$\alpha$-值和距離來評估邊緣品質，以引導搜尋。此外，我們將它們不同的組合抽象成多臂老虎機 (MAB) 中的手臂，並使用 MAB 模型來幫助演算法動態選擇適當的評估量度。骨幹資訊和 MAB 都能提供多樣化的引導資訊，並從搜尋歷程中學習，以建議最佳量度。我們將我們的這些方法套用於 LKH 和 LKH-3，後者是 LKH 的延伸版本，可用於解決約 40 個 TSP 和車輛路徑問題 (VRP) 的變體問題。廣泛的實驗顯示了我們提出的方法的優異效能和泛化能力，大幅改善了 LKH 在 TSP 中的表現，以及 LKH-3 在兩個具代表性的 TSP 和 VRP 變體中的表現，分別是著色 TSP (CTSP) 和帶有時間窗的載重 VRP (CVRPTW)。

##### **From Newswire to Nexus: Using text-based actor embeddings and transformer networks to forecast conflict dynamics**
2501.03928v1 by Mihai Croicu, Simon Polichinel von der Maase

This study advances the field of conflict forecasting by using text-based
actor embeddings with transformer models to predict dynamic changes in violent
conflict patterns at the actor level. More specifically, we combine newswire
texts with structured conflict event data and leverage recent advances in
Natural Language Processing (NLP) techniques to forecast escalations and
de-escalations among conflicting actors, such as governments, militias,
separatist movements, and terrorists. This new approach accurately and promptly
captures the inherently volatile patterns of violent conflicts, which existing
methods have not been able to achieve. To create this framework, we began by
curating and annotating a vast international newswire corpus, leveraging
hand-labeled event data from the Uppsala Conflict Data Program. By using this
hybrid dataset, our models can incorporate the textual context of news sources
along with the precision and detail of structured event data. This combination
enables us to make both dynamic and granular predictions about conflict
developments. We validate our approach through rigorous back-testing against
historical events, demonstrating superior out-of-sample predictive power. We
find that our approach is quite effective in identifying and predicting phases
of conflict escalation and de-escalation, surpassing the capabilities of
traditional models. By focusing on actor interactions, our explicit goal is to
provide actionable insights to policymakers, humanitarian organizations, and
peacekeeping operations in order to enable targeted and effective intervention
strategies.

摘要：本研究透過使用基於文字的演員嵌入與轉換器模型來預測演員層級暴力衝突模式的動態變化，推進了衝突預測領域。更具體地說，我們結合新聞電文與結構化的衝突事件資料，並利用自然語言處理 (NLP) 技術的最新進展來預測衝突行為者（例如政府、民兵、分離主義運動和恐怖分子）之間的升級和降級。這種新方法準確且及時地捕捉了暴力衝突固有的不穩定模式，這是現有方法無法達到的。為了建立這個架構，我們首先整理並註解了一個龐大的國際新聞電文語料庫，利用烏普薩拉衝突資料計畫的手動標記事件資料。透過使用這個混合資料集，我們的模型可以結合新聞來源的文字脈絡以及結構化事件資料的精確性和細節。這種組合使我們能夠對衝突發展做出動態且細緻的預測。我們透過針對歷史事件進行嚴格的反向測試來驗證我們的方法，展示了卓越的樣本外預測能力。我們發現，我們的方法在識別和預測衝突升級和降級階段方面非常有效，超越了傳統模型的能力。透過專注於演員互動，我們的明確目標是為政策制定者、人道主義組織和維和行動提供可行的見解，以便制定有針對性和有效的干預策略。

##### **Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**
2501.03904v1 by Ramya Jonnala, Gongbo Liang, Jeong Yang, Izzat Alsmadi

The integration of large language models (LLMs) into public transit systems
presents a transformative opportunity to enhance urban mobility. This study
explores the potential of LLMs to revolutionize public transportation
management within the context of San Antonio's transit system. Leveraging the
capabilities of LLMs in natural language processing and data analysis, we
investigate their capabilities to optimize route planning, reduce wait times,
and provide personalized travel assistance. By utilizing the General Transit
Feed Specification (GTFS) and other relevant data, this research aims to
demonstrate how LLMs can potentially improve resource allocation, elevate
passenger satisfaction, and inform data-driven decision-making in transit
operations. A comparative analysis of different ChatGPT models was conducted to
assess their ability to understand transportation information, retrieve
relevant data, and provide comprehensive responses. Findings from this study
suggest that while LLMs hold immense promise for public transit, careful
engineering and fine-tuning are essential to realizing their full potential.
San Antonio serves as a case study to inform the development of LLM-powered
transit systems in other urban environments.

摘要：大型語言模型 (LLM) 整合到公共交通系統中，為提升城市流動性帶來轉型契機。本研究探討 LLM 在聖安東尼奧交通系統脈絡下，革新大眾運輸管理的潛力。利用 LLM 在自然語言處理和資料分析方面的能力，我們探討其在優化路線規劃、縮短等候時間，以及提供個人化旅遊協助方面的能力。透過利用通用大眾運輸資料規範 (GTFS) 和其他相關資料，本研究旨在證明 LLM 如何潛在提升資源配置、提升乘客滿意度，以及在交通營運中提供資料驅動的決策。針對不同的 ChatGPT 模型進行比較分析，以評估其理解交通資訊、擷取相關資料，以及提供全面回應的能力。本研究的發現顯示，儘管 LLM 對大眾運輸極具前景，但精密的工程和微調對於實現其全部潛力至關重要。聖安東尼奧作為一個案例研究，為在其他都市環境中開發由 LLM 驅動的交通系統提供參考。

##### **LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**
2501.03895v1 by Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng

The advent of real-time large multimodal models (LMMs) like GPT-4o has
sparked considerable interest in efficient LMMs. LMM frameworks typically
encode visual inputs into vision tokens (continuous representations) and
integrate them and textual instructions into the context of large language
models (LLMs), where large-scale parameters and numerous context tokens
(predominantly vision tokens) result in substantial computational overhead.
Previous efforts towards efficient LMMs always focus on replacing the LLM
backbone with smaller models, while neglecting the crucial issue of token
quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal
vision tokens. To achieve a high compression ratio of vision tokens while
preserving visual information, we first analyze how LMMs understand vision
tokens and find that most vision tokens only play a crucial role in the early
layers of LLM backbone, where they mainly fuse visual information into text
tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to
fuse visual information into text tokens in advance, thereby facilitating the
extreme compression of vision tokens fed to LLM backbone into one token.
LLaVA-Mini is a unified large multimodal model that can support the
understanding of images, high-resolution images, and videos in an efficient
manner. Experiments across 11 image-based and 7 video-based benchmarks
demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token
instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by
77%, deliver low-latency responses within 40 milliseconds, and process over
10,000 frames of video on the GPU hardware with 24GB of memory.

摘要：隨著 GPT-4o 等即時大型多模態模型 (LMM) 的出現，對於高效能 LMM 的興趣也大幅提升。LMM 架構通常會將視覺輸入編碼成視覺符號（連續表示），並將其與文字說明整合到大型語言模型 (LLM) 的脈絡中，其中大規模參數和大量的脈絡符號（主要是視覺符號）會造成大量的運算負擔。先前對於高效能 LMM 的努力總是專注於用較小的模型取代 LLM 主幹，而忽略了符號數量這個關鍵問題。在本文中，我們將介紹 LLaVA-Mini，這是一個符號數量最少的 LMM。為了在保留視覺資訊的同時，達成視覺符號的高壓縮率，我們首先分析 LMM 如何理解視覺符號，並發現大部分的視覺符號只在 LLM 主幹的早期層中扮演關鍵角色，在這些層中，它們主要將視覺資訊融合到文字符號中。基於這個發現，LLaVA-Mini 引入了模態預融合，以預先將視覺資訊融合到文字符號中，從而促進將輸入 LLM 主幹的視覺符號極度壓縮成一個符號。LLaVA-Mini 是一個統一的大型多模態模型，能夠以一種高效能的方式理解影像、高解析度影像和影片。在 11 個基於影像的基準和 7 個基於影片的基準的實驗中，證明 LLaVA-Mini 的效能優於 LLaVA-v1.5，僅使用 1 個視覺符號，而非 576 個。效率分析顯示，LLaVA-Mini 可以減少 77% 的 FLOP，在 40 毫秒內提供低延遲的回應，並在配備 24GB 記憶體的 GPU 硬體上處理超過 10,000 幀的影片。

##### **Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies**
2501.03888v1 by Kexin Gu Baugh, Luke Dickens, Alessandra Russo

Although deep reinforcement learning has been shown to be effective, the
model's black-box nature presents barriers to direct policy interpretation. To
address this problem, we propose a neuro-symbolic approach called neural DNF-MT
for end-to-end policy learning. The differentiable nature of the neural DNF-MT
model enables the use of deep actor-critic algorithms for training. At the same
time, its architecture is designed so that trained models can be directly
translated into interpretable policies expressed as standard (bivalent or
probabilistic) logic programs. Moreover, additional layers can be included to
extract abstract features from complex observations, acting as a form of
predicate invention. The logic representations are highly interpretable, and we
show how the bivalent representations of deterministic policies can be edited
and incorporated back into a neural model, facilitating manual intervention and
adaptation of learned policies. We evaluate our approach on a range of tasks
requiring learning deterministic or stochastic behaviours from various forms of
observations. Our empirical results show that our neural DNF-MT model performs
at the level of competing black-box methods whilst providing interpretable
policies.

摘要：儘管深度強化學習已被證明有效，但模型的黑箱性質對直接政策詮釋造成了障礙。為了解決這個問題，我們提出了一種稱為神經符號方法的神經 DNF-MT，用於端對端政策學習。神經 DNF-MT 模型的可微分特性使得能夠使用深度動作者-評論家演算法進行訓練。同時，其架構被設計成訓練好的模型可以直接轉換為可解釋的政策，表示為標準（二值或機率）邏輯程式。此外，可以包含額外的層級，從複雜的觀察中提取抽象特徵，作為一種謂詞發明的形式。邏輯表示高度可解釋，我們展示了確定性政策的二值表示如何被編輯並整合回神經模型中，促進手動介入和學習政策的適應。我們在各種任務上評估我們的方法，這些任務需要從各種形式的觀察中學習確定性或隨機行為。我們的經驗結果表明，我們的神經 DNF-MT 模型在競爭黑箱方法的層級上執行，同時提供可解釋的政策。

##### **AlphaPO -- Reward shape matters for LLM alignment**
2501.03884v1 by Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Jason Zhu, Natesh Pillai, S. Sathiya Keerthi

Reinforcement Learning with Human Feedback (RLHF) and its variants have made
huge strides toward the effective alignment of large language models (LLMs) to
follow instructions and reflect human values. More recently, Direct Alignment
Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is
skipped by characterizing the reward directly as a function of the policy being
learned. Examples include Direct Preference Optimization (DPO) and Simple
Preference Optimization (SimPO). These methods often suffer from likelihood
displacement, a phenomenon by which the probabilities of preferred responses
are often reduced undesirably.
  In this paper, we argue that, for DAAs the reward (function) shape matters.
We introduce AlphaPO, a new DAA method that leverages an $\alpha$-parameter to
help change the shape of the reward function beyond the standard log reward.
AlphaPO helps maintain fine-grained control over likelihood displacement and
over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO
leads to about 7\% to 10\% relative improvement in alignment performance for
the instruct versions of Mistral-7B and Llama3-8B. The analysis and results
presented highlight the importance of the reward shape, and how one can
systematically change it to affect training dynamics, as well as improve
alignment performance.

摘要：人類回饋強化學習 (RLHF) 及其變體已在大型語言模型 (LLM) 的有效對齊方面取得重大進展，以遵循指令並反映人類價值觀。最近，直接對齊演算法 (DAA) 已出現，其中 RLHF 的獎勵模型階段透過將獎勵直接表徵為正在學習的政策的函數來跳過。範例包括直接偏好最佳化 (DPO) 和簡單偏好最佳化 (SimPO)。這些方法通常會受到似然位移的影響，這是一種現象，其中偏好回應的機率通常會不必要地降低。
在本文中，我們論證，對於 DAA，獎勵（函數）形狀很重要。我們引入了 AlphaPO，這是一種新的 DAA 方法，它利用 $\alpha$-參數來幫助改變獎勵函數的形狀，超越標準的對數獎勵。AlphaPO 有助於維持對似然位移和過度最佳化的細緻控制。與表現最好的 DAA 之一 SimPO 相比，AlphaPO 導致 Mistral-7B 和 Llama3-8B 的指令版本在對齊效能方面相對改善了約 7% 到 10%。所呈現的分析和結果突出了獎勵形狀的重要性，以及如何系統地改變它以影響訓練動態，以及改善對齊效能。

##### **CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds**
2501.03879v1 by Keonwoo Kim, Yeongjae Cho, Taebaek Hwang, Minsoo Jo, Sangdo Han

Recent research has demonstrated that Large Language Models (LLMs) are not
limited to text-only tasks but can also function as multimodal models across
various modalities, including audio, images, and videos. In particular,
research on 3D Large Multimodal Models (3D LMMs) is making notable strides,
driven by the potential of processing higher-dimensional data like point
clouds. However, upon closer examination, we find that the visual and textual
content within each sample of existing training datasets lacks both high
informational granularity and clarity, which serve as a bottleneck for precise
cross-modal understanding. To address these issues, we propose CL3DOR,
Contrastive Learning for 3D large multimodal models via Odds ratio on
high-Resolution point clouds, designed to ensure greater specificity and
clarity in both visual and textual content. Specifically, we increase the
density of point clouds per object and construct informative hard negative
responses in the training dataset to penalize unwanted responses. To leverage
hard negative responses, we incorporate the odds ratio as an auxiliary term for
contrastive learning into the conventional language modeling loss. CL3DOR
achieves state-of-the-art performance in 3D scene understanding and reasoning
benchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key
components through extensive experiments.

摘要：最近的研究表明，大语言模型 (LLM) 不仅限于纯文本任务，还可以跨越各种模式（包括音频、图像和视频）作为多模态模型发挥作用。特别是，3D 大多模态模型 (3D LMM) 的研究正在取得显著进展，这得益于处理点云等高维数据的潜力。然而，经过仔细检查，我们发现现有训练数据集的每个样本中的视觉和文本内容都缺乏高信息粒度和清晰度，这成为精确跨模态理解的瓶颈。为了解决这些问题，我们提出了 CL3DOR，即通过高分辨率点云上的比值对 3D 大多模态模型进行对比学习，旨在确保视觉和文本内容具有更高的特异性和清晰度。具体来说，我们增加了每个对象的点云密度，并在训练数据集中构建信息丰富的硬负面响应，以惩罚不需要的响应。为了利用硬负面响应，我们将比值作为对比学习的辅助项纳入传统的语言建模损失中。CL3DOR 在 3D 场景理解和推理基准测试中取得了最先进的性能。此外，我们通过大量的实验展示了 CL3DOR 关键组件的有效性。

##### **Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on Norwegian Dialectal Slot and Intent Detection**
2501.03870v1 by Verena Blaschke, Felicia Körner, Barbara Plank

Slot and intent detection (SID) is a classic natural language understanding
task. Despite this, research has only more recently begun focusing on SID for
dialectal and colloquial varieties. Many approaches for low-resource scenarios
have not yet been applied to dialectal SID data, or compared to each other on
the same datasets. We participate in the VarDial 2025 shared task on slot and
intent detection in Norwegian varieties, and compare multiple set-ups: varying
the training data (English, Norwegian, or dialectal Norwegian), injecting
character-level noise, training on auxiliary tasks, and applying Layer
Swapping, a technique in which layers of models fine-tuned on different
datasets are assembled into a model. We find noise injection to be beneficial
while the effects of auxiliary tasks are mixed. Though some experimentation was
required to successfully assemble a model from layers, it worked surprisingly
well; a combination of models trained on English and small amounts of dialectal
data produced the most robust slot predictions. Our best models achieve 97.6%
intent accuracy and 85.6% slot F1 in the shared task.

摘要：時隙和意圖偵測 (SID) 是經典的自然語言理解任務。儘管如此，研究直到最近才開始專注於方言和口語變體的 SID。許多低資源場景的方法尚未應用於方言 SID 資料，或在同一資料集上進行相互比較。我們參與了 VarDial 2025 共享任務，針對挪威語變體的時隙和意圖偵測，並比較多種設定：變更訓練資料 (英語、挪威語或挪威方言)、注入字元層級雜訊、針對輔助任務進行訓練，以及套用層交換，這是一種將針對不同資料集微調的模型層組裝成一個模型的技術。我們發現雜訊注入有益，而輔助任務的效果則好壞參半。儘管需要一些實驗才能成功從各層組裝模型，但效果卻出人意料地好；針對英語和小量方言資料訓練的模型組合產生了最穩健的時隙預測。我們最佳的模型在共享任務中達到了 97.6% 的意圖準確度和 85.6% 的時隙 F1 分數。

##### **Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A Multi-Dialectal Bavarian Case Study**
2501.03863v1 by Xaver Maria Krückl, Verena Blaschke, Barbara Plank

Reliable slot and intent detection (SID) is crucial in natural language
understanding for applications like digital assistants. Encoder-only
transformer models fine-tuned on high-resource languages generally perform well
on SID. However, they struggle with dialectal data, where no standardized form
exists and training data is scarce and costly to produce. We explore zero-shot
transfer learning for SID, focusing on multiple Bavarian dialects, for which we
release a new dataset for the Munich dialect. We evaluate models trained on
auxiliary tasks in Bavarian, and compare joint multi-task learning with
intermediate-task training. We also compare three types of auxiliary tasks:
token-level syntactic tasks, named entity recognition (NER), and language
modelling. We find that the included auxiliary tasks have a more positive
effect on slot filling than intent classification (with NER having the most
positive effect), and that intermediate-task training yields more consistent
performance gains. Our best-performing approach improves intent classification
performance on Bavarian dialects by 5.1 and slot filling F1 by 8.4 percentage
points.

摘要：可靠的槽位和意图检测 (SID) 在自然语言理解中至关重要，例如数位助理应用程序。仅编码器转换器模型经过微调，在高资源语言上通常在 SID 上表现良好。然而，它们难以处理方言数据，其中不存在标准化形式，并且训练数据稀缺且制作成本高昂。我们探索了 SID 的零次学习迁移学习，重点关注多种巴伐利亚方言，为此我们发布了慕尼黑方言的新数据集。我们评估了在巴伐利亚辅助任务中训练的模型，并将联合多任务学习与中间任务训练进行比较。我们还比较了三种类型的辅助任务：标记级句法任务、命名实体识别 (NER) 和语言建模。我们发现包含的辅助任务对槽位填充的影响比意图分类更积极（NER 影响最积极），并且中间任务训练产生了更一致的性能提升。我们表现最好的方法将巴伐利亚方言的意图分类性能提高了 5.1，槽位填充 F1 提高了 8.4 个百分点。

##### **Progressive Document-level Text Simplification via Large Language Models**
2501.03857v1 by Dengzhao Fang, Jipeng Qiang, Yi Zhu, Yunhao Yuan, Wei Li, Yan Liu

Research on text simplification has primarily focused on lexical and
sentence-level changes. Long document-level simplification (DS) is still
relatively unexplored. Large Language Models (LLMs), like ChatGPT, have
excelled in many natural language processing tasks. However, their performance
on DS tasks is unsatisfactory, as they often treat DS as merely document
summarization. For the DS task, the generated long sequences not only must
maintain consistency with the original document throughout, but complete
moderate simplification operations encompassing discourses, sentences, and
word-level simplifications. Human editors employ a hierarchical complexity
simplification strategy to simplify documents. This study delves into
simulating this strategy through the utilization of a multi-stage collaboration
using LLMs. We propose a progressive simplification method (ProgDS) by
hierarchically decomposing the task, including the discourse-level,
topic-level, and lexical-level simplification. Experimental results demonstrate
that ProgDS significantly outperforms existing smaller models or direct
prompting with LLMs, advancing the state-of-the-art in the document
simplification task.

摘要：文本簡化的研究主要集中在詞彙和句子層級的改變。長篇文件層級的簡化（DS）仍相對未被探討。大型語言模型（LLM），例如 ChatGPT，在許多自然語言處理任務中表現出色。然而，它們在 DS 任務上的表現並不令人滿意，因為它們常常將 DS 視為單純的文件摘要。對於 DS 任務，產生的長序列不僅必須始終與原始文件保持一致，而且必須包含涵蓋語篇、句子和詞彙層級簡化的適度簡化操作。人工編輯採用階層式複雜性簡化策略來簡化文件。本研究深入探討透過利用 LLM 的多階段協作來模擬此策略。我們提出了一種漸進簡化方法（ProgDS），透過階層式分解任務，包括語篇層級、主題層級和詞彙層級的簡化。實驗結果證明，ProgDS 明顯優於現有的較小模型或使用 LLM 直接提示，推動了文件簡化任務的最新技術。

##### **BabyLMs for isiXhosa: Data-Efficient Language Modelling in a Low-Resource Context**
2501.03855v1 by Alexis Matzopoulos, Charl Hendriks, Hishaam Mahomed, Francois Meyer

The BabyLM challenge called on participants to develop sample-efficient
language models. Submissions were pretrained on a fixed English corpus, limited
to the amount of words children are exposed to in development (<100m). The
challenge produced new architectures for data-efficient language modelling,
which outperformed models trained on trillions of words. This is promising for
low-resource languages, where available corpora are limited to much less than
100m words. In this paper, we explore the potential of BabyLMs for low-resource
languages, using the isiXhosa language as a case study. We pretrain two BabyLM
architectures, ELC-BERT and MLSM, on an isiXhosa corpus. They outperform a
vanilla pretrained model on POS tagging and NER, achieving notable gains (+3.2
F1) for the latter. In some instances, the BabyLMs even outperform XLM-R. Our
findings show that data-efficient models are viable for low-resource languages,
but highlight the continued importance, and lack of, high-quality pretraining
data. Finally, we visually analyse how BabyLM architectures encode isiXhosa.

摘要：BabyLM 挑戰要求參與者開發出樣本效率語言模型。提交的模型在固定的英語語料庫上進行預訓練，並限制在兒童在發展過程中接觸到的詞彙量（<100m）。該挑戰產生了用於資料效率語言建模的新架構，其效能優於在數兆個詞彙上訓練的模型。這對於低資源語言來說很有希望，因為可用的語料庫限制在遠少於 100m 個詞彙。在本文中，我們探討了 BabyLM 在低資源語言中的潛力，並以 isiXhosa 語言作為案例研究。我們在 isiXhosa 語料庫上預訓練了兩個 BabyLM 架構，ELC-BERT 和 MLSM。它們在詞性標記和 NER 上的表現優於香草預訓練模型，後者獲得了顯著的增益（+3.2 F1）。在某些情況下，BabyLM 甚至優於 XLM-R。我們的研究結果表明，資料效率模型對於低資源語言是可行的，但強調了高品質預訓練資料的持續重要性和缺乏。最後，我們視覺化分析了 BabyLM 架構如何編碼 isiXhosa。

##### **Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control**
2501.03847v2 by Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu

Diffusion models have demonstrated impressive performance in generating
high-quality videos from text prompts or images. However, precise control over
the video generation process, such as camera manipulation or content editing,
remains a significant challenge. Existing methods for controlled video
generation are typically limited to a single control type, lacking the
flexibility to handle diverse control demands. In this paper, we introduce
Diffusion as Shader (DaS), a novel approach that supports multiple video
control tasks within a unified architecture. Our key insight is that achieving
versatile video control necessitates leveraging 3D control signals, as videos
are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods
limited to 2D control signals, DaS leverages 3D tracking videos as control
inputs, making the video diffusion process inherently 3D-aware. This innovation
allows DaS to achieve a wide range of video controls by simply manipulating the
3D tracking videos. A further advantage of using 3D tracking videos is their
ability to effectively link frames, significantly enhancing the temporal
consistency of the generated videos. With just 3 days of fine-tuning on 8 H800
GPUs using less than 10k videos, DaS demonstrates strong control capabilities
across diverse tasks, including mesh-to-video generation, camera control,
motion transfer, and object manipulation.

摘要：擴散模型已證明在根據文字提示或圖像產生高品質影片方面具有令人印象深刻的效能。然而，對影片產生過程的精確控制，例如相機操作或內容編輯，仍然是一項重大的挑戰。現有的受控影片產生方法通常僅限於單一控制類型，缺乏處理不同控制需求的靈活性。在本文中，我們介紹了擴散為著色器 (DaS)，這是一種新穎的方法，可在統一架構中支援多項影片控制任務。我們的關鍵見解是，要實現多功能影片控制，必須利用 3D 控制訊號，因為影片基本上是動態 3D 內容的 2D 渲染。與僅限於 2D 控制訊號的先前方法不同，DaS 利用 3D 追蹤影片作為控制輸入，使影片擴散過程本質上具備 3D 感知能力。此創新讓 DaS 能夠透過簡單地操作 3D 追蹤影片來實現廣泛的影片控制。使用 3D 追蹤影片的另一個優點是它們有效連結畫面的能力，大幅提升產生影片的時間一致性。DaS 只使用不到 10k 個影片，在 8 個 H800 GPU 上進行 3 天微調，就能在各種任務中展現強大的控制能力，包括網格到影片產生、相機控制、動作轉移和物件操作。

##### **More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives**
2501.04070v2 by Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan

Large language models (LLMs) excel at few-shot in-context learning (ICL)
without requiring parameter updates. However, as the number of ICL
demonstrations increases from a few to many, performance tends to plateau and
eventually decline. We identify two primary causes for this trend: the
suboptimal negative log-likelihood (NLL) optimization objective and the
incremental data noise. To address these issues, we introduce DrICL, a novel
optimization method that enhances model performance through Differentiated
Learning and advantage-based Reweighting objectives. Globally, DrICL utilizes
differentiated learning to optimize the NLL objective, ensuring that many-shot
performance surpasses zero-shot levels. Locally, it dynamically adjusts the
weighting of many-shot demonstrations by leveraging cumulative advantages
inspired by reinforcement learning, thereby improving generalization. This
approach allows the model to handle varying numbers of shots effectively,
mitigating the impact of noisy data. Recognizing the lack of multi-task
datasets with diverse many-shot distributions, we develop the Many-Shot ICL
Benchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers
from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.
ICL-50 facilitates the evaluation of many-shot ICL strategies across seven
prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate
that LLMs enhanced with DrICL achieve significant improvements in many-shot
setups across various tasks, including both in-domain and out-of-domain
scenarios. We release the code and benchmark dataset hoping to facilitate
further research in many-shot ICL.

摘要：大型語言模型 (LLM) 在僅需少量範例的脈絡學習 (ICL) 中表現出色，而無需更新參數。然而，隨著 ICL 示範的數量從少數增加到多數，效能往往會達到平穩期，並最終下降。我們找出此趨勢的兩個主要原因：次佳負對數似然 (NLL) 最佳化目標和遞增資料雜訊。為了解決這些問題，我們引入了 DrICL，一種透過差異化學習和基於優勢的重新加權目標來增強模型效能的新型最佳化方法。在全球層面上，DrICL 利用差異化學習來最佳化 NLL 目標，確保多範例效能超越零範例等級。在局部層面上，它透過利用受強化學習啟發的累積優勢，動態調整多範例示範的加權，從而改善泛化能力。這種方法讓模型能夠有效處理不同數量的範例，減輕雜訊資料的影響。由於缺乏具有多樣化多範例分佈的多任務資料集，我們開發了多範例 ICL 基準 (ICL-50) - 一個包含 50 項任務的大規模基準，涵蓋從 1 到 350 的範例數量，序列長度最長為 8,000 個符號，用於微調目的。ICL-50 促進了跨越七項顯著的 NLP 任務和 50 個不同資料集的多範例 ICL 策略的評估。實驗結果表明，透過 DrICL 增強的 LLM 在各種任務的多範例設定中獲得顯著改善，包括領域內和領域外場景。我們釋出程式碼和基準資料集，希望能促進多範例 ICL 的進一步研究。

##### **BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study**
2501.03843v1 by Atharva Mutsaddi, Anvi Jamkhande, Aryan Thakre, Yashodhara Haribhakta

As short text data in native languages like Hindi increasingly appear in
modern media, robust methods for topic modeling on such data have gained
importance. This study investigates the performance of BERTopic in modeling
Hindi short texts, an area that has been under-explored in existing research.
Using contextual embeddings, BERTopic can capture semantic relationships in
data, making it potentially more effective than traditional models, especially
for short and diverse texts. We evaluate BERTopic using 6 different document
embedding models and compare its performance against 8 established topic
modeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative
Matrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive
Regularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis
(PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec.
The models are assessed using coherence scores across a range of topic counts.
Our results reveal that BERTopic consistently outperforms other models in
capturing coherent topics from short Hindi texts.

摘要：由於印地語等原生的語言中短文本資料越來越常出現在現代媒體中，因此在這些資料上進行主題建模的強健方法變得越來越重要。本研究探討了 BERTopic 在建模印地語短文本中的表現，這是一個現有研究中尚未充分探討的領域。透過使用脈絡嵌入，BERTopic 能夠擷取資料中的語義關係，使其潛在效能優於傳統模型，特別是對於簡短且多樣化的文本。我們使用 6 個不同的文件嵌入模型評估 BERTopic，並將其效能與 8 種已建立的主題建模技術進行比較，例如隱含狄利克雷配置 (LDA)、非負矩陣分解 (NMF)、潛在語義索引 (LSI)、主題模型的加成正則化 (ARTM)、機率潛在語義分析 (PLSA)、嵌入式主題模型 (ETM)、組合式主題模型 (CTM) 和 Top2Vec。這些模型會使用不同主題計數的相干性分數進行評估。我們的結果顯示，BERTopic 在從印地語短文本中擷取相干主題方面始終優於其他模型。

##### **SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis**
2501.03836v1 by Runci Bai

Brain tumors can result in neurological dysfunction, alterations in cognitive
and psychological states, increased intracranial pressure, and the occurrence
of seizures, thereby presenting a substantial risk to human life and health.
The You Only Look Once(YOLO) series models have demonstrated superior accuracy
in object detection for medical imaging. In this paper, we develop a novel
SCC-YOLO architecture by integrating the SCConv attention mechanism into
YOLOv9. The SCConv module reconstructs an efficient convolutional module by
reducing spatial and channel redundancy among features, thereby enhancing the
learning of image features. We investigate the impact of intergrating different
attention mechanisms with the YOLOv9 model on brain tumor image detection using
both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset).
Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3%
improvement in mAp50 compared to YOLOv9, while on our self-made dataset,
SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached
state-of-the-art performance in brain tumor detection. Source code is available
at : https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master

摘要：腦瘤可能導致神經功能障礙、認知和心理狀態改變、顱內壓升高，以及癲癇發作，因此對人類生命和健康構成重大風險。You Only Look Once (YOLO) 系列模型已證明在醫學影像的物件偵測中具有優異的準確度。在本文中，我們透過將 SCConv 注意力機制整合到 YOLOv9 中，開發出新穎的 SCC-YOLO 架構。SCConv 模組透過減少特徵中的空間和通道冗餘來重建一個高效的卷積模組，從而增強影像特徵的學習。我們使用 Br35H 資料集和我們自製的資料集 (Brain_Tumor_Dataset) 探討了將不同的注意力機制與 YOLOv9 模型整合對腦瘤影像偵測的影響。實驗結果顯示，在 Br35H 資料集上，SCC-YOLO 在 mAp50 方面比 YOLOv9 提升了 0.3%，而在我們自製的資料集上，SCC-YOLO 比 YOLOv9 提升了 0.5%。SCC-YOLO 已在腦瘤偵測方面達到最先進的效能。原始碼可於以下網址取得：https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master

##### **Three-dimensional attention Transformer for state evaluation in real-time strategy games**
2501.03832v1 by Yanqing Ye, Weilong Yang, Kai Qiu, Jie Zhang

Situation assessment in Real-Time Strategy (RTS) games is crucial for
understanding decision-making in complex adversarial environments. However,
existing methods remain limited in processing multi-dimensional feature
information and temporal dependencies. Here we propose a tri-dimensional
Space-Time-Feature Transformer (TSTF Transformer) architecture, which
efficiently models battlefield situations through three independent but
cascaded modules: spatial attention, temporal attention, and feature attention.
On a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF
Transformer demonstrates superior performance: achieving 58.7% accuracy in the
early game (~4% progress), significantly outperforming the conventional
Timesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress)
while maintaining low performance variation (standard deviation 0.114).
Meanwhile, this architecture requires fewer parameters (4.75M) compared to the
baseline model (5.54M). Our study not only provides new insights into situation
assessment in RTS games but also presents an innovative paradigm for
Transformer-based multi-dimensional temporal modeling.

摘要：在即時戰略 (RTS) 遊戲中，情境評估對於理解在複雜對抗環境中的決策至關重要。然而，現有方法在處理多維特徵資訊和時間依賴性方面仍然有限。在此，我們提出一個三維時空特徵轉換器 (TSTF Transformer) 架構，它透過三個獨立但串聯的模組有效地模擬戰場情境：空間注意力、時間注意力和特徵注意力。在包含 3,150 個對抗實驗的資料集上，8 層 TSTF Transformer 展現出優異的效能：在遊戲初期獲得 58.7% 的準確度（進步約 4%），明顯優於傳統 Timesformer 的 41.8%；在遊戲中期達到 97.6% 的準確度（進步約 40%），同時維持低效能變異（標準差 0.114）。同時，與基線模型（5.54M）相比，此架構需要的參數更少（4.75M）。我們的研究不僅為 RTS 遊戲中的情境評估提供了新的見解，也為基於 Transformer 的多維時間建模提出了創新的典範。

##### **Investigating the Impact of Data Selection Strategies on Language Model Performance**
2501.03826v1 by Jiayao Gu, Liting Chen, Yihong Li

Data selection is critical for enhancing the performance of language models,
particularly when aligning training datasets with a desired target
distribution. This study explores the effects of different data selection
methods and feature types on model performance. We evaluate whether selecting
data subsets can influence downstream tasks, whether n-gram features improve
alignment with target distributions, and whether embedding-based neural
features provide complementary benefits. Through comparative experiments using
baseline random selection methods and distribution aligned approaches, we
provide insights into the interplay between data selection strategies and model
training efficacy. All code for this study can be found on
\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github
repository}.

摘要：資料選擇對於增強語言模型的效能至關重要，特別是在將訓練資料集與目標目標分佈對齊時。本研究探討不同資料選擇方法和特徵類型對模型效能的影響。我們評估選擇資料子集是否會影響下游任務，n-gram 特徵是否會改善與目標分佈的對齊，以及基於嵌入的神經特徵是否提供額外的優點。透過使用基準隨機選擇方法和分佈對齊方法進行比較實驗，我們深入了解資料選擇策略和模型訓練效能之間的交互作用。本研究的所有程式碼都可以在
\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github 儲存庫}中找到。

##### **Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in Ultrasound Imaging**
2501.03825v1 by Simon W. Penninga, Hans van Gorp, Ruud J. G. van Sloun

Ultrasound images are commonly formed by sequential acquisition of
beam-steered scan-lines. Minimizing the number of required scan-lines can
significantly enhance frame rate, field of view, energy efficiency, and data
transfer speeds. Existing approaches typically use static subsampling schemes
in combination with sparsity-based or, more recently, deep-learning-based
recovery. In this work, we introduce an adaptive subsampling method that
maximizes intrinsic information gain in-situ, employing a Sylvester Normalizing
Flow encoder to infer an approximate Bayesian posterior under partial
observation in real-time. Using the Bayesian posterior and a deep generative
model for future observations, we determine the subsampling scheme that
maximizes the mutual information between the subsampled observations, and the
next frame of the video. We evaluate our approach using the EchoNet cardiac
ultrasound video dataset and demonstrate that our active sampling method
outperforms competitive baselines, including uniform and variable-density
random sampling, as well as equidistantly spaced scan-lines, improving mean
absolute reconstruction error by 15%. Moreover, posterior inference and the
sampling scheme generation are performed in just 0.015 seconds (66Hz), making
it fast enough for real-time 2D ultrasound imaging applications.

摘要：超音波影像通常是由逐行掃描線的序列取得。最小化所需掃描線的數量可以顯著地增強畫面更新率、視野、能源效率以及資料傳輸速度。現有的方法通常使用靜態子取樣方案，結合基於稀疏性或最近基於深度學習的復原。在這項工作中，我們引入了一種適應性子取樣方法，它最大化內在資訊的獲取，採用 Sylvester 正規化流編碼器，以在部分觀察下推斷近似貝氏後驗。使用貝氏後驗和一個用於未來觀察的深度生成模型，我們決定了子取樣方案，它最大化子取樣觀察和影片的下一幀之間的互信息。我們使用 EchoNet 心臟超音波影片資料集評估我們的做法，並證明我們的主動取樣方法優於競爭基準，包括均勻和變數密度隨機取樣，以及等距的掃描線，將平均絕對重建誤差改善了 15%。此外，後驗推論和取樣方案生成僅在 0.015 秒（66Hz）中執行，使其足夠快，可以用於即時 2D 超音波影像應用程式。

##### **Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks**
2501.03824v1 by Weilong Yang, Jie Zhang, Xunyun Liu, Yanqing Ye

Effective evaluation of real-time strategy tasks requires adaptive mechanisms
to cope with dynamic and unpredictable environments. This study proposes a
method to improve evaluation functions for real-time responsiveness to
battle-field situation changes, utilizing an online reinforcement
learning-based dynam-ic weight adjustment mechanism within the real-time
strategy game. Building on traditional static evaluation functions, the method
employs gradient descent in online reinforcement learning to update weights
dynamically, incorporating weight decay techniques to ensure stability.
Additionally, the AdamW optimizer is integrated to adjust the learning rate and
decay rate of online reinforcement learning in real time, further reducing the
dependency on manual parameter tun-ing. Round-robin competition experiments
demonstrate that this method signifi-cantly enhances the application
effectiveness of the Lanchester combat model evaluation function, Simple
evaluation function, and Simple Sqrt evaluation function in planning algorithms
including IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notable
improvement in scores, with the en-hancement becoming more pronounced as the
map size increases. Furthermore, the increase in evaluation function
computation time induced by this method is kept below 6% for all evaluation
functions and planning algorithms. The pro-posed dynamic adaptive evaluation
function demonstrates a promising approach for real-time strategy task
evaluation.

摘要：<paragraph>有效評估即時策略任務需要適應機制來應對動態且不可預測的環境。本研究提出了一種方法，用於改善即時響應戰場狀況變化的評估函數，利用即時策略遊戲中的基於線上強化學習的動態權重調整機制。在傳統靜態評估函數的基礎上，該方法採用線上強化學習中的梯度下降法動態更新權重，並結合權重衰減技術來確保穩定性。此外，AdamW 優化器被整合用於調整線上強化學習的學習率和衰減率，進一步降低了對手動參數調整的依賴性。輪詢競爭實驗表明，這種方法顯著提高了蘭徹斯特戰鬥模型評估函數、簡單評估函數和簡單平方根評估函數在包括 IDABCD、IDRTMinimax 和組合 AI 在內的規劃演算法中的應用效果。該方法在分數上取得了顯著的改進，隨著地圖大小的增加，這種改進變得更加明顯。此外，該方法引起的評估函數計算時間增加對於所有評估函數和規劃演算法都保持在 6% 以下。所提出的動態自適應評估函數展示了即時策略任務評估的有前景的方法。</paragraph>

##### **Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits**
2501.03805v1 by Sung-Feng Huang, Heng-Cheng Kuo, Zhehuai Chen, Xuesong Yang, Chao-Han Huck Yang, Yu Tsao, Yu-Chiang Frank Wang, Hung-yi Lee, Szu-Wei Fu

Neural speech editing advancements have raised concerns about their misuse in
spoofing attacks. Traditional partially edited speech corpora primarily focus
on cut-and-paste edits, which, while maintaining speaker consistency, often
introduce detectable discontinuities. Recent methods, like
A\textsuperscript{3}T and Voicebox, improve transitions by leveraging
contextual information. To foster spoofing detection research, we introduce the
Speech INfilling Edit (SINE) dataset, created with Voicebox. We detailed the
process of re-implementing Voicebox training and dataset creation. Subjective
evaluations confirm that speech edited using this novel technique is more
challenging to detect than conventional cut-and-paste methods. Despite human
difficulty, experimental results demonstrate that self-supervised-based
detectors can achieve remarkable performance in detection, localization, and
generalization across different edit methods. The dataset and related models
will be made publicly available.

摘要：神經語音編輯的進展引發了人們對其在欺騙攻擊中被濫用的擔憂。傳統的部分編輯語音語料庫主要集中在剪貼編輯上，儘管保持了說話者的一致性，但通常會引入可檢測的不連續性。最近的方法，如 A\textsuperscript{3}T 和 Voicebox，通過利用上下文信息改進了過渡。為了促進欺騙檢測研究，我們引入了使用 Voicebox 創建的語音填充編輯 (SINE) 數據集。我們詳細介紹了重新實現 Voicebox 訓練和數據集創建的過程。主觀評估證實，使用這種新技術編輯的語音比傳統的剪貼方法更難檢測。儘管人類有困難，但實驗結果表明，基於自監督的檢測器可以在不同編輯方法的檢測、定位和泛化中實現顯著的性能。該數據集和相關模型將公開提供。

##### **Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model Matching**
2501.03795v1 by Ahmed Maged, Gamal Kassem

Enterprise Resource Planning (ERP) consultants play a vital role in
customizing systems to meet specific business needs by processing large amounts
of data and adapting functionalities. However, the process is
resource-intensive, time-consuming, and requires continuous adjustments as
business demands evolve. This research introduces a Self-Adaptive ERP Framework
that automates customization using enterprise process models and system usage
analysis. It leverages Artificial Intelligence (AI) & Natural Language
Processing (NLP) for Petri nets to transform business processes into adaptable
models, addressing both structural and functional matching. The framework,
built using Design Science Research (DSR) and a Systematic Literature Review
(SLR), reduces reliance on manual adjustments, improving ERP customization
efficiency and accuracy while minimizing the need for consultants.

摘要：企業資源規劃 (ERP) 顧問扮演著重要的角色，透過處理大量資料並調整功能，客製化系統以滿足特定的業務需求。然而，此程序需要大量資源、耗時，且隨著業務需求的演變，需要持續調整。本研究提出了一個自適應 ERP 架構，它使用企業流程模型和系統使用分析自動化客製化。它利用人工智慧 (AI) 和自然語言處理 (NLP) 讓 Petri 網路將業務流程轉換為可適應的模型，解決結構和功能匹配的問題。此架構使用設計科學研究 (DSR) 和系統化文獻回顧 (SLR) 建立，降低對手動調整的依賴，提升 ERP 客製化的效率和準確度，同時將顧問的需求降到最低。

##### **Explainable Reinforcement Learning for Formula One Race Strategy**
2501.04068v1 by Devin Thomas, Junqi Jiang, Avinash Kori, Aaron Russo, Steffen Winkler, Stuart Sale, Joseph McMillan, Francesco Belardinelli, Antonio Rago

In Formula One, teams compete to develop their cars and achieve the highest
possible finishing position in each race. During a race, however, teams are
unable to alter the car, so they must improve their cars' finishing positions
via race strategy, i.e. optimising their selection of which tyre compounds to
put on the car and when to do so. In this work, we introduce a reinforcement
learning model, RSRL (Race Strategy Reinforcement Learning), to control race
strategies in simulations, offering a faster alternative to the industry
standard of hard-coded and Monte Carlo-based race strategies. Controlling cars
with a pace equating to an expected finishing position of P5.5 (where P1
represents first place and P20 is last place), RSRL achieves an average
finishing position of P5.33 on our test race, the 2023 Bahrain Grand Prix,
outperforming the best baseline of P5.63. We then demonstrate, in a
generalisability study, how performance for one track or multiple tracks can be
prioritised via training. Further, we supplement model predictions with feature
importance, decision tree-based surrogate models, and decision tree
counterfactuals towards improving user trust in the model. Finally, we provide
illustrations which exemplify our approach in real-world situations, drawing
parallels between simulations and reality.

摘要：在一级方程式赛车中，车队竞相开发他们的赛车，并在每场比赛中取得尽可能高的完赛名次。然而，在比赛期间，车队无法改变赛车，因此他们必须通过比赛策略来提高赛车的完赛名次，即优化选择何时为赛车更换哪种轮胎配方。在这项工作中，我们引入了一个强化学习模型，RSRL（比赛策略强化学习），来控制模拟中的比赛策略，为业界标准的硬编码和基于蒙特卡罗的比赛策略提供了一个更快的替代方案。用与预期完赛名次 P5.5 相当的速度控制赛车（其中 P1 代表第一名，P20 为最后一名），RSRL 在我们的测试比赛中获得了 P5.33 的平均完赛名次，2023 年巴林大奖赛，优于 P5.63 的最佳基准。然后我们在一个通用性研究中展示了如何通过训练优先考虑一条赛道或多条赛道的性能。此外，我们用特征重要性、基于决策树的替代模型和决策树反事实来补充模型预测，以提高用户对模型的信任。最后，我们提供了说明我们方法在现实世界情况中的示例，在模拟和现实之间画出相似之处。

##### **How to Select Pre-Trained Code Models for Reuse? A Learning Perspective**
2501.03783v1 by Zhangqian Bi, Yao Wan, Zhaoyang Chu, Yufei Hu, Junyi Zhang, Hongyu Zhang, Guandong Xu, Hai Jin

Pre-training a language model and then fine-tuning it has shown to be an
efficient and effective technique for a wide range of code intelligence tasks,
such as code generation, code summarization, and vulnerability detection.
However, pretraining language models on a large-scale code corpus is
computationally expensive. Fortunately, many off-the-shelf Pre-trained Code
Models (PCMs), such as CodeBERT, CodeT5, CodeGen, and Code Llama, have been
released publicly. These models acquire general code understanding and
generation capability during pretraining, which enhances their performance on
downstream code intelligence tasks. With an increasing number of these public
pre-trained models, selecting the most suitable one to reuse for a specific
task is essential. In this paper, we systematically investigate the reusability
of PCMs. We first explore three intuitive model selection methods that select
by size, training data, or brute-force fine-tuning. Experimental results show
that these straightforward techniques either perform poorly or suffer high
costs. Motivated by these findings, we explore learning-based model selection
strategies that utilize pre-trained models without altering their parameters.
Specifically, we train proxy models to gauge the performance of pre-trained
models, and measure the distribution deviation between a model's latent
features and the task's labels, using their closeness as an indicator of model
transferability. We conduct experiments on 100 widely-used opensource PCMs for
code intelligence tasks, with sizes ranging from 42.5 million to 3 billion
parameters. The results demonstrate that learning-based selection methods
reduce selection time to 100 seconds, compared to 2,700 hours with brute-force
fine-tuning, with less than 6% performance degradation across related tasks.

摘要：<paragraph>預先訓練語言模型，然後微調它，已被證明是一種高效且有效的技術，可應用于各種程式碼智能任務，例如程式碼生成、程式碼摘要和漏洞偵測。
然而，在大型程式碼語料庫上預訓練語言模型在計算上非常昂貴。幸運的是，許多現成的預訓練程式碼模型 (PCM)，例如 CodeBERT、CodeT5、CodeGen 和 Code Llama，已經公開發布。這些模型在預訓練期間獲得了通用的程式碼理解和生成能力，這增強了它們在下游程式碼智能任務上的效能。隨著這些公開預訓練模型數量的不斷增加，選擇最適合特定任務重複使用的模型至關重要。在本文中，我們系統地研究了 PCM 的可重複使用性。我們首先探討了三種直觀的模型選擇方法，它們通過大小、訓練資料或蠻力微調進行選擇。實驗結果表明，這些直接技術要么執行得很差，要么成本很高。受這些發現的啟發，我們探索了基於學習的模型選擇策略，它們利用預訓練模型而不改變它們的參數。具體來說，我們訓練代理模型來評估預訓練模型的效能，並使用它們的接近程度作為模型可傳輸性的指標，來衡量模型潛在特徵和任務標籤之間的分布偏差。我們對 100 個廣泛使用的開源 PCM 進行了程式碼智能任務的實驗，其規模從 4250 萬到 30 億個參數不等。結果表明，與蠻力微調的 2700 小時相比，基於學習的選擇方法將選擇時間減少到 100 秒，而相關任務的效能下降不到 6%。</paragraph>

##### **SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment**
2501.03764v1 by Siyuan Zhao, Chenyu Liu, Yi Ding, Xinliang Zhou

In practical sleep stage classification, a key challenge is the variability
of EEG data across different subjects and environments. Differences in
physiology, age, health status, and recording conditions can lead to domain
shifts between data. These domain shifts often result in decreased model
accuracy and reliability, particularly when the model is applied to new data
with characteristics different from those it was originally trained on, which
is a typical manifestation of negative transfer. To address this, we propose
SelectiveFinetuning in this paper. Our method utilizes a pretrained Multi
Resolution Convolutional Neural Network (MRCNN) to extract EEG features,
capturing the distinctive characteristics of different sleep stages. To
mitigate the effect of domain shifts, we introduce a domain aligning mechanism
that employs Earth Mover Distance (EMD) to evaluate and select source domain
data closely matching the target domain. By finetuning the model with selective
source data, our SelectiveFinetuning enhances the model's performance on target
domain that exhibits domain shifts compared to the data used for training.
Experimental results show that our method outperforms existing baselines,
offering greater robustness and adaptability in practical scenarios where data
distributions are often unpredictable.

摘要：在實際的睡眠階段分類中，一個關鍵的挑戰是腦電圖數據在不同受試者和環境中的變異性。生理、年齡、健康狀況和記錄條件的差異可能導致數據之間的領域偏移。這些領域偏移通常會導致模型準確度和可靠性下降，特別是當模型應用於與其最初訓練時不同的特徵的新數據時，這是負遷移的典型表現。為了解決這個問題，我們在本文中提出選擇性微調。我們的模型利用預訓練的多解析度卷積神經網路 (MRCNN) 來提取腦電圖特徵，捕捉不同睡眠階段的獨特特徵。為了減輕領域偏移的影響，我們引入了一個領域對齊機制，它採用地球移動距離 (EMD) 來評估和選擇與目標領域緊密匹配的源領域數據。通過使用選擇性源數據微調模型，我們的選擇性微調增強了模型在與用於訓練的數據相比表現出領域偏移的目標領域上的性能。實驗結果表明，我們的模型優於現有的基準，在數據分佈通常不可預測的實際場景中提供了更大的穩健性和適應性。

##### **Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series**
2501.03747v1 by Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, Yuntian Chen

Recently, leveraging pre-trained Large Language Models (LLMs) for time series
(TS) tasks has gained increasing attention, which involves activating and
enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities
based on token-level alignment but overlook LLMs' inherent strength on natural
language processing -- their deep understanding of linguistic logic and
structure rather than superficial embedding processing. We propose
Context-Alignment, a new paradigm that aligns TS with a linguistic component in
the language environments familiar to LLMs to enable LLMs to contextualize and
comprehend TS data, thereby activating their capabilities. Specifically, such
context-level alignment comprises structural alignment and logical alignment,
which is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to
TS-language multimodal inputs. Structural alignment utilizes dual-scale nodes
to describe hierarchical structure in TS-language, enabling LLMs treat long TS
data as a whole linguistic component while preserving intrinsic token features.
Logical alignment uses directed edges to guide logical relationships, ensuring
coherence in the contextual semantics. Demonstration examples prompt are
employed to construct Demonstration Examples based Context-Alignment (DECA)
following DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated
into various layers of pre-trained LLMs to improve awareness of logic and
structure, thereby enhancing performance. Extensive experiments show the
effectiveness of DECA and the importance of Context-Alignment across tasks,
particularly in few-shot and zero-shot forecasting, confirming that
Context-Alignment provide powerful prior knowledge on context.

摘要：<paragraph>最近，利用预先训练的大语言模型 (LLM) 来执行时间序列 (TS) 任务备受关注，这涉及激活和增强 LLM 的功能。许多方法旨在基于标记级别的对齐来激活 LLM 的功能，但忽略了 LLM 在自然语言处理方面的固有优势——它们对语言逻辑和结构的深刻理解，而不是表面的嵌入式处理。我们提出了语境对齐，这是一种新范例，它将时间序列与 LLM 熟悉的语言环境中的语言组件对齐，使 LLM 能够对时间序列数据进行语境化和理解，从而激活它们的功能。具体来说，这种语境级别的对齐包括结构对齐和逻辑对齐，这是通过应用于时间序列语言多模态输入的双尺度语境对齐 GNN（DSCA-GNN）实现的。结构对齐利用双尺度节点来描述时间序列语言中的层次结构，使 LLM 能够将长的时间序列数据作为一个整体的语言组件来处理，同时保留固有的标记特征。逻辑对齐使用有向边来指导逻辑关系，确保语境语义中的一致性。演示示例提示被用来构建基于语境对齐的演示示例 (DECA)，遵循 DSCA-GNN 框架。DECA 可以灵活且重复地集成到预训练 LLM 的各个层中，以提高对逻辑和结构的认识，从而增强性能。大量的实验表明了 DECA 的有效性以及语境对齐在各个任务中的重要性，特别是在少样本和零样本预测中，证实了语境对齐为语境提供了强大的先验知识。</paragraph>

##### **Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy**
2501.04067v1 by Jamie Todd, Junqi Jiang, Aaron Russo, Steffen Winkler, Stuart Sale, Joseph McMillan, Antonio Rago

Formula One (F1) race strategy takes place in a high-pressure and fast-paced
environment where split-second decisions can drastically affect race results.
Two of the core decisions of race strategy are when to make pit stops (i.e.
replace the cars' tyres) and which tyre compounds (hard, medium or soft, in
normal conditions) to select. The optimal pit stop decisions can be determined
by estimating the tyre degradation of these compounds, which in turn can be
computed from the energy applied to each tyre, i.e. the tyre energy. In this
work, we trained deep learning models, using the Mercedes-AMG PETRONAS F1
team's historic race data consisting of telemetry, to forecast tyre energies
during races. Additionally, we fitted XGBoost, a decision tree-based machine
learning algorithm, to the same dataset and compared the results, with both
giving impressive performance. Furthermore, we incorporated two different
explainable AI methods, namely feature importance and counterfactual
explanations, to gain insights into the reasoning behind the forecasts. Our
contributions thus result in an explainable, automated method which could
assist F1 teams in optimising their race strategy.

摘要：一級方程式 (F1) 賽車策略在高壓和快節奏的環境中進行，其中瞬間的決定可能會劇烈影響比賽結果。賽車策略的兩個核心決定是何時進站 (即更換賽車輪胎) 以及選擇哪種輪胎配方 (在正常情況下為硬胎、中性胎或軟胎)。最佳進站決策可透過估計這些配方的輪胎磨損來決定，而輪胎磨損反過來又可從施加在每個輪胎上的能量 (即輪胎能量) 計算出來。在這項工作中，我們使用 Mercedes-AMG PETRONAS F1 車隊的歷史比賽數據 (包含遙測) 訓練深度學習模型，以預測比賽期間的輪胎能量。此外，我們將 XGBoost (一種基於決策樹的機器學習演算法) 安裝到相同的資料集，並比較結果，兩者都表現出色。此外，我們納入了兩種不同的可解釋 AI 方法，即特徵重要性和反事實解釋，以深入了解預測背後的推理。因此，我們的貢獻造就了一種可解釋的自動化方法，可協助 F1 車隊優化其比賽策略。

##### **Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein**
2501.03722v1 by Xiaotong Guo, Deqian Yang, Dan Wang, Haochen Zhao, Yuan Li, Zhilin Sui, Tao Zhou, Lijun Zhang, Yanda Meng

Accurate segmentation of pulmonary structures iscrucial in clinical
diagnosis, disease study, and treatment planning. Significant progress has been
made in deep learning-based segmentation techniques, but most require much
labeled data for training. Consequently, developing precise segmentation
methods that demand fewer labeled datasets is paramount in medical image
analysis. The emergence of pre-trained vision-language foundation models, such
as CLIP, recently opened the door for universal computer vision tasks.
Exploiting the generalization ability of these pre-trained foundation models on
downstream tasks, such as segmentation, leads to unexpected performance with a
relatively small amount of labeled data. However, exploring these models for
pulmonary artery-vein segmentation is still limited. This paper proposes a
novel framework called Language-guided self-adaptive Cross-Attention Fusion
Framework. Our method adopts pre-trained CLIP as a strong feature extractor for
generating the segmentation of 3D CT scans, while adaptively aggregating the
cross-modality of text and image representations. We propose a s pecially
designed adapter module to fine-tune pre-trained CLIP with a self-adaptive
learning strategy to effectively fuse the two modalities of embeddings. We
extensively validate our method on a local dataset, which is the largest
pulmonary artery-vein CT dataset to date and consists of 718 labeled data in
total. The experiments show that our method outperformed other state-of-the-art
methods by a large margin. Our data and code will be made publicly available
upon acceptance.

摘要：精確分割肺部結構在臨床診斷、疾病研究和治療計畫中至關重要。基於深度學習的分割技術已取得重大進展，但大多數技術在訓練時需要大量的標記資料。因此，開發精確的分割方法，以減少標記資料集的需求，在醫學影像分析中至關重要。預訓練的視覺語言基礎模型（例如 CLIP）的出現，最近為通用電腦視覺任務開啟了大門。利用這些預訓練基礎模型在分割等下游任務中的泛化能力，即使標記資料量相對較少，也能產生意想不到的效能。然而，探索這些模型在肺動脈靜脈分割中的應用仍然有限。本文提出了一個名為語言引導自適應交叉注意力融合框架的新框架。我們的模型採用預訓練的 CLIP 作為強大的特徵萃取器，用於產生 3D 電腦斷層掃描的分割，同時自適應地聚合文本和影像表徵的跨模態。我們提出了一個特別設計的適配器模組，以自適應學習策略微調預訓練的 CLIP，以有效融合兩種嵌入模態。我們在一個本地資料集上廣泛驗證了我們的模型，這是迄今為止最大的肺動脈靜脈電腦斷層掃描資料集，總共包含 718 個標記資料。實驗表明，我們的模型以大幅優於其他最先進模型。我們的資料和程式碼將在獲得接受後公開。

##### **Unsupervised Speech Segmentation: A General Approach Using Speech Language Models**
2501.03711v1 by Avishai Elmakies, Omri Abend, Yossi Adi

In this paper, we introduce an unsupervised approach for Speech Segmentation,
which builds on previously researched approaches, e.g., Speaker Diarization,
while being applicable to an inclusive set of acoustic-semantic distinctions,
paving a path towards a general Unsupervised Speech Segmentation approach.
Unlike traditional speech and audio segmentation, which mainly focuses on
spectral changes in the input signal, e.g., phone segmentation, our approach
tries to segment the spoken utterance into chunks with differing
acoustic-semantic styles, focusing on acoustic-semantic information that does
not translate well into text, e.g., emotion or speaker. While most Speech
Segmentation tasks only handle one style change, e.g., emotion diarization, our
approach tries to handle multiple acoustic-semantic style changes. Leveraging
recent advances in Speech Language Models (SLMs), we propose a simple
unsupervised method to segment a given speech utterance. We empirically
demonstrate the effectiveness of the proposed approach by considering several
setups. Results suggest that the proposed method is superior to the evaluated
baselines on boundary detection, segment purity, and over-segmentation. Code is
available at
https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.

摘要：在本文中，我們介紹了一種用於語音分割的非監督式方法，
這建立在先前研究的方法之上，例如說話者區分，
同時適用於一組包容性的聲學語義區別，
為通用的非監督式語音分割方法鋪平道路。
與傳統的語音和音訊分割不同，傳統的語音和音訊分割主要關注
輸入訊號中的頻譜變化，例如音素分割，我們的做法
試圖將口語語句分割成具有不同
聲學語義風格的區塊，專注於無法順利轉換為文字的聲學語義資訊，例如情緒或說話者。雖然大多數語音
分割任務只處理一種風格變化，例如情緒區分，我們的
方法試圖處理多種聲學語義風格變化。利用
語音語言模型 (SLM) 的最新進展，我們提出一個簡單的
非監督式方法來分割給定的語音語句。我們透過考慮數個
設定，以經驗方式證明所提出方法的有效性。結果表明，所提出的方法在邊界偵測、區段純度和過度分割方面優於評估的基準。程式碼可在
https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm 取得。

##### **AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features**
2501.03700v1 by Ruochen Zhang, Hyeung-Sik Choi, Dongwook Jung, Phan Huy Nam Anh, Sang-Ki Jeong, Zihao Zhu

Monocular 3D object detection is a challenging task in autonomous systems due
to the lack of explicit depth information in single-view images. Existing
methods often depend on external depth estimators or expensive sensors, which
increase computational complexity and hinder real-time performance. To overcome
these limitations, we propose AuxDepthNet, an efficient framework for real-time
monocular 3D object detection that eliminates the reliance on external depth
maps or pre-trained depth models. AuxDepthNet introduces two key components:
the Auxiliary Depth Feature (ADF) module, which implicitly learns
depth-sensitive features to improve spatial reasoning and computational
efficiency, and the Depth Position Mapping (DPM) module, which embeds depth
positional information directly into the detection process to enable accurate
object localization and 3D bounding box regression. Leveraging the DepthFusion
Transformer architecture, AuxDepthNet globally integrates visual and
depth-sensitive features through depth-guided interactions, ensuring robust and
efficient detection. Extensive experiments on the KITTI dataset show that
AuxDepthNet achieves state-of-the-art performance, with $\text{AP}_{3D}$ scores
of 24.72\% (Easy), 18.63\% (Moderate), and 15.31\% (Hard), and
$\text{AP}_{\text{BEV}}$ scores of 34.11\% (Easy), 25.18\% (Moderate), and
21.90\% (Hard) at an IoU threshold of 0.7.

摘要：單目 3D 物件偵測由於單視圖影像中缺乏明確的深度資訊，因此在自駕系統中是一項具有挑戰性的任務。現有方法通常依賴外部深度估測器或昂貴的感測器，這會增加運算複雜度並阻礙即時效能。為了克服這些限制，我們提出 AuxDepthNet，一個用於即時單目 3D 物件偵測的高效架構，它消除了對外部深度圖或預先訓練深度模型的依賴。AuxDepthNet 導入兩個關鍵元件：輔助深度特徵 (ADF) 模組，它隱式學習深度敏感特徵以改善空間推理和運算效率，以及深度位置對應 (DPM) 模組，它將深度位置資訊直接嵌入偵測流程中，以實現準確的物件定位和 3D 邊界框回歸。透過利用 DepthFusion Transformer 架構，AuxDepthNet 透過深度引導互動，將視覺和深度敏感特徵在全球整合，確保穩健且高效的偵測。在 KITTI 資料集上的大量實驗顯示，AuxDepthNet 達到了最先進的效能，在 IoU 閾值為 0.7 時，$\text{AP}_{3D}$ 分數分別為 24.72%（容易）、18.63%（中等）和 15.31%（困難），而 $\text{AP}_{\text{BEV}}$ 分數分別為 34.11%（容易）、25.18%（中等）和 21.90%（困難）。

