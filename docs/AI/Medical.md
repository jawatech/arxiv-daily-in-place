
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-16**|**Schema Matching with Large Language Models: an Experimental Study**|Marcel Parciak et.al.|[2407.11852v1](http://arxiv.org/abs/2407.11852v1)|[link](https://github.com/uhasselt-dsi-data-systems-lab/code-schema-matching-llms-artefacs)|
|**2024-07-16**|**GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**|Kyle Hamilton et.al.|[2407.11827v1](http://arxiv.org/abs/2407.11827v1)|null|
|**2024-07-16**|**Characterizing and Understanding HGNN Training on GPUs**|Dengke Han et.al.|[2407.11790v1](http://arxiv.org/abs/2407.11790v1)|null|
|**2024-07-16**|**CCoE: A Compact LLM with Collaboration of Experts**|Shaomang Huang et.al.|[2407.11686v1](http://arxiv.org/abs/2407.11686v1)|null|
|**2024-07-16**|**CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging**|Sunny Gupta et.al.|[2407.11652v1](http://arxiv.org/abs/2407.11652v1)|null|
|**2024-07-16**|**Improving Engagement and Efficacy of mHealth Micro-Interventions for Stress Coping: an In-The-Wild Study**|Chaya Ben Yehuda et.al.|[2407.11612v1](http://arxiv.org/abs/2407.11612v1)|null|
|**2024-07-16**|**DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training**|Guillermo Jimenez-Perez et.al.|[2407.11594v1](http://arxiv.org/abs/2407.11594v1)|null|
|**2024-07-16**|**Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification**|Naif Alkhunaizi et.al.|[2407.11573v1](http://arxiv.org/abs/2407.11573v1)|null|
|**2024-07-16**|**Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise**|Qimin Yang et.al.|[2407.11536v1](http://arxiv.org/abs/2407.11536v1)|null|
|**2024-07-16**|**Cross-Phase Mutual Learning Framework for Pulmonary Embolism Identification on Non-Contrast CT Scans**|Bizhe Bai et.al.|[2407.11529v1](http://arxiv.org/abs/2407.11529v1)|null|
|**2024-07-16**|**Multi-Channel Masked Autoencoder and Comprehensive Evaluations for Reconstructing 12-Lead ECG from Arbitrary Single-Lead ECG**|Jiarong Chen et.al.|[2407.11481v1](http://arxiv.org/abs/2407.11481v1)|[link](https://github.com/chenjiar3/mcma)|
|**2024-07-16**|**TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering**|Tonmoy Rajkhowa et.al.|[2407.11383v1](http://arxiv.org/abs/2407.11383v1)|null|
|**2024-07-15**|**Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs**|Leonardo Crespi et.al.|[2407.10888v1](http://arxiv.org/abs/2407.10888v1)|null|
|**2024-07-15**|**Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method**|Yi-Wei Chua et.al.|[2407.10828v1](http://arxiv.org/abs/2407.10828v1)|null|
|**2024-07-15**|**Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN**|Seyed Amir Latifi et.al.|[2407.10689v1](http://arxiv.org/abs/2407.10689v1)|null|
|**2024-07-15**|**Spatio-temporal neural distance fields for conditional generative modeling of the heart**|Kristine SÃ¸rensen et.al.|[2407.10663v1](http://arxiv.org/abs/2407.10663v1)|[link](https://github.com/kristineaajuhl/spatio_temporal_generative_cardiac_model)|
|**2024-07-15**|**TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction**|Xingzhi Zhou et.al.|[2407.10510v1](http://arxiv.org/abs/2407.10510v1)|null|
|**2024-07-15**|**A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT**|Chunshi Wang et.al.|[2407.10433v1](http://arxiv.org/abs/2407.10433v1)|null|
|**2024-07-15**|**Static and multivariate-temporal attentive fusion transformer for readmission risk prediction**|Zhe Sun et.al.|[2407.11096v1](http://arxiv.org/abs/2407.11096v1)|null|
|**2024-07-14**|**Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence**|Yintong Zhang et.al.|[2407.10359v1](http://arxiv.org/abs/2407.10359v1)|null|
|**2024-07-14**|**Learning Unlabeled Clients Divergence via Anchor Model Aggregation for Federated Semi-supervised Learning**|Marawan Elbatel et.al.|[2407.10327v1](http://arxiv.org/abs/2407.10327v1)|null|
|**2024-07-14**|**Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine**|Omid Rohanian et.al.|[2407.10086v1](http://arxiv.org/abs/2407.10086v1)|null|
|**2024-07-13**|**Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation**|Kriti Bhattarai et.al.|[2407.10021v1](http://arxiv.org/abs/2407.10021v1)|null|
|**2024-07-13**|**Causality extraction from medical text using Large Language Models (LLMs)**|Seethalakshmi Gopalakrishnan et.al.|[2407.10020v1](http://arxiv.org/abs/2407.10020v1)|null|
|**2024-07-13**|**Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient Multi-Label Skin Lesion Classification**|Peng Tang et.al.|[2407.09999v1](http://arxiv.org/abs/2407.09999v1)|null|
|**2024-07-13**|**Evaluating the Impact of Different Quantum Kernels on the Classification Performance of Support Vector Machine Algorithm: A Medical Dataset Application**|Emine Akpinar et.al.|[2407.09930v1](http://arxiv.org/abs/2407.09930v1)|null|
|**2024-07-13**|**Enhancing Semantic Segmentation with Adaptive Focal Loss: A Novel Approach**|Md Rakibul Islam et.al.|[2407.09828v1](http://arxiv.org/abs/2407.09828v1)|null|
|**2024-07-12**|**Towards Personalised Patient Risk Prediction Using Temporal Hospital Data Trajectories**|Thea Barnes et.al.|[2407.09373v1](http://arxiv.org/abs/2407.09373v1)|null|
|**2024-07-12**|**Enhancing Depressive Post Detection in Bangla: A Comparative Study of TF-IDF, BERT and FastText Embeddings**|Saad Ahmed Sazan et.al.|[2407.09187v1](http://arxiv.org/abs/2407.09187v1)|null|
|**2024-07-12**|**STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs**|Yiheng Huang et.al.|[2407.09096v1](http://arxiv.org/abs/2407.09096v1)|null|
|**2024-07-12**|**FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and Dehiscence Detection from Intraoral Images**|Marawan Elbatel et.al.|[2407.09088v1](http://arxiv.org/abs/2407.09088v1)|null|
|**2024-07-12**|**Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media**|Chen Chen et.al.|[2407.09019v1](http://arxiv.org/abs/2407.09019v1)|null|
|**2024-07-12**|**Application of Artificial Intelligence in Supporting Healthcare Professionals and Caregivers in Treatment of Autistic Children**|Hossein Mohammadi Rouzbahani et.al.|[2407.08902v1](http://arxiv.org/abs/2407.08902v1)|null|
|**2024-07-11**|**SALT: Introducing a Framework for Hierarchical Segmentations in Medical Imaging using Softmax for Arbitrary Label Trees**|Sven Koitka et.al.|[2407.08878v1](http://arxiv.org/abs/2407.08878v1)|null|
|**2024-07-11**|**FedMedICL: Towards Holistic Evaluation of Distribution Shifts in Federated Medical Imaging**|Kumail Alhamoud et.al.|[2407.08822v1](http://arxiv.org/abs/2407.08822v1)|[link](https://github.com/m1k2zoo/fedmedicl)|
|**2024-07-11**|**FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification**|Yu Tian et.al.|[2407.08813v1](http://arxiv.org/abs/2407.08813v1)|[link](https://github.com/harvard-ophthalmology-ai-lab/fairdomain)|
|**2024-07-11**|**Uncertainty Estimation of Large Language Models in Medical Question Answering**|Jiaxin Wu et.al.|[2407.08662v1](http://arxiv.org/abs/2407.08662v1)|null|
|**2024-07-11**|**Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models**|Wanling Gao et.al.|[2407.08554v1](http://arxiv.org/abs/2407.08554v1)|[link](https://github.com/benchcouncil/vc-medai)|
|**2024-07-11**|**How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical Time-Series Imputation**|Linglong Qian et.al.|[2407.08442v1](http://arxiv.org/abs/2407.08442v1)|null|
|**2024-07-11**|**Specialist vision-language models for clinical ophthalmology**|Robbie Holland et.al.|[2407.08410v1](http://arxiv.org/abs/2407.08410v1)|[link](https://github.com/robbieholland/specialistvlms)|
|**2024-07-11**|**Unveiling Disparities in Maternity Care: A Topic Modelling Approach to Analysing Maternity Incident Investigation Reports**|Georgina Cosma et.al.|[2407.08328v1](http://arxiv.org/abs/2407.08328v1)|null|
|**2024-07-11**|**Predicting Heart Failure with Attention Learning Techniques Utilizing Cardiovascular Data**|Ershadul Haque et.al.|[2407.08289v1](http://arxiv.org/abs/2407.08289v1)|null|
|**2024-07-11**|**Leveraging LLMs to Predict Affective States via Smartphone Sensor Features**|Tianyi Zhang et.al.|[2407.08240v1](http://arxiv.org/abs/2407.08240v1)|null|
|**2024-07-11**|**DALL-M: Context-Aware Clinical Data Augmentation with LLMs**|Chihcheng Hsieh et.al.|[2407.08227v1](http://arxiv.org/abs/2407.08227v1)|[link](https://github.com/chihchenghsieh/dall-m)|
|**2024-07-11**|**Synthetic Electroretinogram Signal Generation Using Conditional Generative Adversarial Network for Enhancing Classification of Autism Spectrum Disorder**|Mikhail Kulyabin et.al.|[2407.08166v1](http://arxiv.org/abs/2407.08166v1)|null|
|**2024-07-11**|**Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates**|A. Noorizadegan et.al.|[2407.08134v1](http://arxiv.org/abs/2407.08134v1)|[link](https://github.com/cmmai/resnet_for_pinn)|
|**2024-07-10**|**Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data**|Ritesh Mehta et.al.|[2407.08003v1](http://arxiv.org/abs/2407.08003v1)|null|
|**2024-07-10**|**The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing**|Alice Qian Zhang et.al.|[2407.07786v1](http://arxiv.org/abs/2407.07786v1)|null|
|**2024-07-10**|**A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models : Safety, Consensus, Objectivity, Reproducibility and Explainability**|Ting Fang Tan et.al.|[2407.07666v1](http://arxiv.org/abs/2407.07666v1)|null|
|**2024-07-10**|**Boosting Medical Image Synthesis via Registration-guided Consistency and Disentanglement Learning**|Chuanpu Li et.al.|[2407.07660v1](http://arxiv.org/abs/2407.07660v1)|null|
|**2024-07-10**|**H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for Occlusal Contact Segmentation with Articulating Paper**|Ryan Banks et.al.|[2407.07604v1](http://arxiv.org/abs/2407.07604v1)|[link](https://github.com/banksylel/h-fcbformer)|
|**2024-07-10**|**FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes**|Rajat Kumar Jenamani et.al.|[2407.07561v1](http://arxiv.org/abs/2407.07561v1)|null|
|**2024-07-10**|**Arabic Automatic Story Generation with Large Language Models**|Ahmed Oumar El-Shangiti et.al.|[2407.07551v1](http://arxiv.org/abs/2407.07551v1)|[link](https://github.com/ubc-nlp/arastories)|
|**2024-07-10**|**Weakly-supervised Medical Image Segmentation with Gaze Annotations**|Yuan Zhong et.al.|[2407.07406v1](http://arxiv.org/abs/2407.07406v1)|[link](https://github.com/med-air/gazemedseg)|
|**2024-07-10**|**Interpretable Differential Diagnosis with Dual-Inference Large Language Models**|Shuang Zhou et.al.|[2407.07330v1](http://arxiv.org/abs/2407.07330v1)|null|
|**2024-07-10**|**Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy**|Praveenbalaji Rajendran et.al.|[2407.07296v1](http://arxiv.org/abs/2407.07296v1)|null|
|**2024-07-10**|**Causal Discovery in Semi-Stationary Time Series**|Shanyun Gao et.al.|[2407.07291v1](http://arxiv.org/abs/2407.07291v1)|[link](https://github.com/causalml-lab/pcmci-omega)|
|**2024-07-10**|**Causal Discovery-Driven Change Point Detection in Time Series**|Shanyun Gao et.al.|[2407.07290v1](http://arxiv.org/abs/2407.07290v1)|null|
|**2024-07-09**|**Lifestyle-Informed Personalized Blood Biomarker Prediction via Novel Representation Learning**|A. Ali Heydari et.al.|[2407.07277v1](http://arxiv.org/abs/2407.07277v1)|null|
|**2024-07-09**|**ProtoSAM -- One Shot Medical Image Segmentation With Foundational Models**|Lev Ayzenberg et.al.|[2407.07042v1](http://arxiv.org/abs/2407.07042v1)|null|
|**2024-07-09**|**Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**|Abdul Karim Gizzini et.al.|[2407.07009v1](http://arxiv.org/abs/2407.07009v1)|null|
|**2024-07-09**|**Microsoft Cloud-based Digitization Workflow with Rich Metadata Acquisition for Cultural Heritage Objects**|Krzysztof Kutt et.al.|[2407.06972v1](http://arxiv.org/abs/2407.06972v1)|null|
|**2024-07-09**|**TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's Disease Progression Analysis**|Jacob Thrasher et.al.|[2407.06852v1](http://arxiv.org/abs/2407.06852v1)|[link](https://github.com/jacob-thrasher/te-ssl)|
|**2024-07-09**|**VRDSynth: Synthesizing Programs for Multilingual Visually Rich Document Information Extraction**|Thanh-Dat Nguyen et.al.|[2407.06826v1](http://arxiv.org/abs/2407.06826v1)|null|
|**2024-07-09**|**iASiS: Towards Heterogeneous Big Data Analysis for Personalized Medicine**|Anastasia Krithara et.al.|[2407.06748v1](http://arxiv.org/abs/2407.06748v1)|null|
|**2024-07-09**|**Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations**|Rachael Fleurence et.al.|[2407.11054v1](http://arxiv.org/abs/2407.11054v1)|null|
|**2024-07-09**|**TCKIN: A Novel Integrated Network Model for Predicting Mortality Risk in Sepsis Patients**|Fanglin Dong et.al.|[2407.06560v1](http://arxiv.org/abs/2407.06560v1)|null|
|**2024-07-08**|**AI-driven multi-omics integration for multi-scale predictive modeling of causal genotype-environment-phenotype relationships**|You Wu et.al.|[2407.06405v1](http://arxiv.org/abs/2407.06405v1)|null|
|**2024-07-08**|**Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps**|Chuanbo Hu et.al.|[2407.06309v1](http://arxiv.org/abs/2407.06309v1)|null|
|**2024-07-08**|**Hybrid X-Linker: Automated Data Generation and Extreme Multi-label Ranking for Biomedical Entity Linking**|Pedro Ruas et.al.|[2407.06292v1](http://arxiv.org/abs/2407.06292v1)|null|
|**2024-07-08**|**Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities**|Avinash Anand et.al.|[2407.06125v1](http://arxiv.org/abs/2407.06125v1)|null|
|**2024-07-08**|**Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian**|Tommaso Mario Buonocore et.al.|[2407.06011v1](http://arxiv.org/abs/2407.06011v1)|null|
|**2024-07-08**|**Generation and De-Identification of Indian Clinical Discharge Summaries using LLMs**|Sanjeet Singh et.al.|[2407.05887v1](http://arxiv.org/abs/2407.05887v1)|null|
|**2024-07-08**|**Integrating AI in College Education: Positive yet Mixed Experiences with ChatGPT**|Xinrui Song et.al.|[2407.05810v1](http://arxiv.org/abs/2407.05810v1)|null|
|**2024-07-08**|**FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep Reinforcement Learning for Medical Imaging**|Pranab Sahoo et.al.|[2407.05800v1](http://arxiv.org/abs/2407.05800v1)|[link](https://github.com/pranabiitp/fedmrl)|
|**2024-07-08**|**Large Language Models for Judicial Entity Extraction: A Comparative Study**|Atin Sakkeer Hussain et.al.|[2407.05786v1](http://arxiv.org/abs/2407.05786v1)|null|
|**2024-07-08**|**Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports**|Yutong Zhang et.al.|[2407.05758v1](http://arxiv.org/abs/2407.05758v1)|null|
|**2024-07-08**|**RadiomicsFill-Mammo: Synthetic Mammogram Mass Manipulation with Radiomics Features**|Inye Na et.al.|[2407.05683v1](http://arxiv.org/abs/2407.05683v1)|[link](https://github.com/nainye/radiomicsfill)|
|**2024-07-08**|**WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering**|Pingyi Chen et.al.|[2407.05603v1](http://arxiv.org/abs/2407.05603v1)|[link](https://github.com/cpystan/wsi-vqa)|
|**2024-07-07**|**Accelerating MRI Uncertainty Estimation with Mask-based Bayesian Neural Network**|Zehuan Zhang et.al.|[2407.05521v1](http://arxiv.org/abs/2407.05521v1)|null|
|**2024-07-07**|**A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions**|Fei Wang et.al.|[2407.05458v1](http://arxiv.org/abs/2407.05458v1)|null|
|**2024-07-07**|**Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**|P. N. Karthikayan et.al.|[2407.05440v1](http://arxiv.org/abs/2407.05440v1)|null|
|**2024-07-07**|**FM-OSD: Foundation Model-Enabled One-Shot Detection of Anatomical Landmarks**|Juzheng Miao et.al.|[2407.05412v1](http://arxiv.org/abs/2407.05412v1)|[link](https://github.com/juzhengmiao/fm-osd)|
|**2024-07-06**|**BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records**|Weimin Lyu et.al.|[2407.05213v1](http://arxiv.org/abs/2407.05213v1)|null|
|**2024-07-06**|**RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models**|Peng Xia et.al.|[2407.05131v1](http://arxiv.org/abs/2407.05131v1)|[link](https://github.com/richard-peng-xia/rule)|
|**2024-07-06**|**Linear Attention Based Deep Nonlocal Means Filtering for Multiplicative Noise Removal**|Xiao Siyao et.al.|[2407.05087v1](http://arxiv.org/abs/2407.05087v1)|null|
|**2024-07-05**|**Brain Age Estimation with a Greedy Dual-Stream Model for Limited Datasets**|Iman Kianian et.al.|[2407.04808v1](http://arxiv.org/abs/2407.04808v1)|[link](https://github.com/iman2693/gdsm)|
|**2024-07-05**|**Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework**|Reza Averly et.al.|[2407.04629v1](http://arxiv.org/abs/2407.04629v1)|null|
|**2024-07-05**|**Variational and Explanatory Neural Networks for Encoding Cancer Profiles and Predicting Drug Responses**|Tianshu Feng et.al.|[2407.04486v1](http://arxiv.org/abs/2407.04486v1)|null|
|**2024-07-05**|**Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning**|Saeed Shurrab et.al.|[2407.04449v1](http://arxiv.org/abs/2407.04449v1)|[link](https://github.com/nyuad-cai/cxr-ehr-msn)|
|**2024-07-04**|**Query-Guided Self-Supervised Summarization of Nursing Notes**|Ya Gao et.al.|[2407.04125v1](http://arxiv.org/abs/2407.04125v1)|null|
|**2024-07-04**|**MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis**|Asma Alkhaldi et.al.|[2407.04106v1](http://arxiv.org/abs/2407.04106v1)|[link](https://github.com/vision-cair/minigpt-med)|
|**2024-07-04**|**Mechanisms for Data Sharing in Collaborative Causal Inference (Extended Version)**|BjÃ¶rn Filter et.al.|[2407.11032v1](http://arxiv.org/abs/2407.11032v1)|null|
|**2024-07-04**|**Unsupervised Analysis of Alzheimer's Disease Signatures using 3D Deformable Autoencoders**|Mehmet Yigit Avci et.al.|[2407.03863v1](http://arxiv.org/abs/2407.03863v1)|[link](https://github.com/ci-ber/morphade)|
|**2024-07-04**|**CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation**|Rui Yang et.al.|[2407.07913v1](http://arxiv.org/abs/2407.07913v1)|null|
|**2024-07-04**|**Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content**|Andrew Bouras et.al.|[2407.03582v1](http://arxiv.org/abs/2407.03582v1)|[link](https://github.com/andrewbouras/randomnesspaper)|
|**2024-07-03**|**Accelerated Proton Resonance Frequency-based Magnetic Resonance Thermometry by Optimized Deep Learning Method**|Sijie Xu et.al.|[2407.03308v1](http://arxiv.org/abs/2407.03308v1)|[link](https://github.com/minipuding/fastmrt)|
|**2024-07-03**|**MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition**|Yanjie Cui et.al.|[2407.03131v2](http://arxiv.org/abs/2407.03131v2)|null|
|**2024-07-03**|**Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation**|Yujin Shin et.al.|[2407.03086v1](http://arxiv.org/abs/2407.03086v1)|null|
|**2024-07-03**|**Attention Incorporated Network for Sharing Low-rank, Image and K-space Information during MR Image Reconstruction to Achieve Single Breath-hold Cardiac Cine Imaging**|Siying Xu et.al.|[2407.03034v1](http://arxiv.org/abs/2407.03034v1)|[link](https://github.com/midas-tum/a-liknet)|

#### Abstracts
##### **Schema Matching with Large Language Models: an Experimental Study**
2407.11852v1 by Marcel Parciak, Brecht Vandevoort, Frank Neven, Liesbet M. Peeters, Stijn Vansummeren

Large Language Models (LLMs) have shown useful applications in a variety of
tasks, including data wrangling. In this paper, we investigate the use of an
off-the-shelf LLM for schema matching. Our objective is to identify semantic
correspondences between elements of two relational schemas using only names and
descriptions. Using a newly created benchmark from the health domain, we
propose different so-called task scopes. These are methods for prompting the
LLM to do schema matching, which vary in the amount of context information
contained in the prompt. Using these task scopes we compare LLM-based schema
matching against a string similarity baseline, investigating matching quality,
verification effort, decisiveness, and complementarity of the approaches. We
find that matching quality suffers from a lack of context information, but also
from providing too much context information. In general, using newer LLM
versions increases decisiveness. We identify task scopes that have acceptable
verification effort and succeed in identifying a significant number of true
semantic matches. Our study shows that LLMs have potential in bootstrapping the
schema matching process and are able to assist data engineers in speeding up
this task solely based on schema element names and descriptions without the
need for data instances.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¨åç¨®ä»»åä¸­å±ç¾åºæç¨çæç¨ï¼åæ¬è³ææ´çãå¨æ¬æä¸­ï¼æåæ¢è¨ç¾æ LLM å¨æ¶æ§æ¯å°ä¸­çç¨éãæåçç®æ¨æ¯åä½¿ç¨åç¨±åæè¿°ï¼æ¾åºå©åéè¯å¼æ¶æ§çåç´ ä¹éçèªæå°æãä½¿ç¨å¾å¥åº·é åæ°å»ºç«çåºæºï¼æåæåºä¸åçæè¬ä»»åç¯åãéäºæ¹æ³æ¯ç¨æ¼æç¤º LLM é²è¡æ¶æ§æ¯å°ï¼å¶åå«å¨æç¤ºä¸­çèçµ¡è³è¨éææä¸åãä½¿ç¨éäºä»»åç¯åï¼æåå°åºæ¼ LLM çæ¶æ§æ¯å°èå­ä¸²ç¸ä¼¼æ§åºæºé²è¡æ¯è¼ï¼æ¢è¨æ¯å°åè³ªãé©è­å·¥ä½ãææ·æ§ï¼ä»¥åæ¹æ³çäºè£æ§ãæåç¼ç¾æ¯å°åè³ªæåå°èçµ¡è³è¨ä¸è¶³ä»¥åæä¾éå¤èçµ¡è³è¨çå½±é¿ãä¸è¬ä¾èªªï¼ä½¿ç¨è¼æ°ç LLM çæ¬æå¢å ææ·æ§ãæåæ¾åºå·æå¯æ¥åé©è­å·¥ä½ï¼ä¸¦æåæ¾åºå¤§éçå¯¦èªææ¯å°çä»»åç¯åãæåçç ç©¶é¡¯ç¤ºï¼LLM æå©æ¼å¼å°æ¶æ§æ¯å°æµç¨ï¼ä¸¦ä¸è½å¤ åå©è³æå·¥ç¨å¸«åæ ¹ææ¶æ§åç´ åç¨±åæè¿°å éæ­¤ä»»åï¼èä¸éè¦è³æå¯¦ä¾ã

##### **GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**
2407.11827v1 by Kyle Hamilton, Luca Longo, Bojan Bozic

While the use of machine learning for the detection of propaganda techniques
in text has garnered considerable attention, most approaches focus on
"black-box" solutions with opaque inner workings. Interpretable approaches
provide a solution, however, they depend on careful feature engineering and
costly expert annotated data. Additionally, language features specific to
propagandistic text are generally the focus of rhetoricians or linguists, and
there is no data set labeled with such features suitable for machine learning.
This study codifies 22 rhetorical and linguistic features identified in
literature related to the language of persuasion for the purpose of annotating
an existing data set labeled with propaganda techniques. To help human experts
annotate natural language sentences with these features, RhetAnn, a web
application, was specifically designed to minimize an otherwise considerable
mental effort. Finally, a small set of annotated data was used to fine-tune
GPT-3.5, a generative large language model (LLM), to annotate the remaining
data while optimizing for financial cost and classification accuracy. This
study demonstrates how combining a small number of human annotated examples
with GPT can be an effective strategy for scaling the annotation process at a
fraction of the cost of traditional annotation relying solely on human experts.
The results are on par with the best performing model at the time of writing,
namely GPT-4, at 10x less the cost. Our contribution is a set of features,
their properties, definitions, and examples in a machine-readable format, along
with the code for RhetAnn and the GPT prompts and fine-tuning procedures for
advancing state-of-the-art interpretable propaganda technique detection.

æè¦ï¼<paragraph>åç®¡ä½¿ç¨æ©å¨å­¸ç¿ä¾åµæ¸¬å®£å³æå·§å¨ææ¬ä¸­å·²ç²å¾ç¸ç¶çéæ³¨ï¼ä½å¤§å¤æ¸æ¹æ³é½å°æ³¨æ¼å·æä¸éæå§é¨éä½çãé»çå­ãè§£æ±ºæ¹æ¡ãå¯è§£éçæ¹æ³æä¾äºè§£æ±ºæ¹æ¡ï¼ç¶èï¼å®åä¾è³´æ¼ä»ç´°çç¹å¾µå·¥ç¨åæè²´çå°å®¶è¨»éè³æãæ­¤å¤ï¼å®£å³æ§ææ¬çç¹å®èªè¨ç¹å¾µéå¸¸æ¯ä¿®è¾­å­¸å®¶æèªè¨å­¸å®¶çéæ³¨ç¦é»ï¼ä¸¦ä¸æ²ææ¨è¨ææ­¤é¡ç¹å¾µçè³æéé©åæ©å¨å­¸ç¿ãæ¬ç ç©¶å°åºç¾å¨èèªªæèªè¨ç¸éçæç»ä¸­è­å¥åºç 22 åä¿®è¾­åèªè¨ç¹å¾µç·¨çºææ³å¸ï¼ç®çæ¯çºæ¨è¨æå®£å³æå·§çç¾æè³æéãçºäºå¹«å©äººé¡å°å®¶ä½¿ç¨éäºç¹å¾µè¨»éèªç¶èªè¨å¥å­ï¼å°éè¨­è¨äºç¶²è·¯æç¨ç¨å¼ RhetAnnï¼ä»¥æå¤§ç¨åº¦å°æ¸å°åæ¬ç¸ç¶å¤§çå¿æºè² æãæå¾ï¼ä½¿ç¨ä¸å°çµè¨»éè³æå¾®èª¿äºçæå¼å¤§åèªè¨æ¨¡å (LLM) GPT-3.5ï¼ä»¥è¨»éå©é¤è³æï¼åæéå°è²¡åææ¬ååé¡æºç¢ºåº¦é²è¡æä½³åãæ¬ç ç©¶å±ç¤ºäºå°å°æ¸äººé¡è¨»éç¯ä¾è GPT çµåå¦ä½æçºä»¥å³çµ±åä¾è³´äººé¡å°å®¶çè¨»éææ¬çä¸å°é¨åä¾æ´å±è¨»éç¨åºçææç­ç¥ãå¨æ°å¯«æ¬ææï¼çµæèç¶æè¡¨ç¾æä½³çæ¨¡å GPT-4 ç¸ç¶ï¼ææ¬å»ä½äº 10 åãæåçè²¢ç»æ¯ä¸çµç¹å¾µãå®åçå±¬æ§ãå®ç¾©åç¯ä¾ï¼æ¡ç¨æ©å¨å¯è®æ ¼å¼ï¼ä»¥å RhetAnn çç¨å¼ç¢¼å GPT æç¤ºåå¾®èª¿ç¨åºï¼ç¨æ¼æ¨é²æåé²çå¯è§£éå®£å³æå·§åµæ¸¬ã</paragraph>

##### **Characterizing and Understanding HGNN Training on GPUs**
2407.11790v1 by Dengke Han, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Ninghui Sun

Owing to their remarkable representation capabilities for heterogeneous graph
data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in
many critical real-world domains such as recommendation systems and medical
analysis. Prior to their practical application, identifying the optimal HGNN
model parameters tailored to specific tasks through extensive training is a
time-consuming and costly process. To enhance the efficiency of HGNN training,
it is essential to characterize and analyze the execution semantics and
patterns within the training process to identify performance bottlenecks. In
this study, we conduct an in-depth quantification and analysis of two
mainstream HGNN training scenarios, including single-GPU and multi-GPU
distributed training. Based on the characterization results, we disclose the
performance bottlenecks and their underlying causes in different HGNN training
scenarios and provide optimization guidelines from both software and hardware
perspectives.

æè¦ï¼ç±æ¼ç°è³ªåç¥ç¶ç¶²è·¯ (HGNN) å·æåè¶çç°è³ªåå½¢æ¸æè¡¨ç¤ºè½åï¼å æ­¤å·²å»£æ³æç¨æ¼è¨±å¤éè¦ççå¯¦ä¸çé åï¼ä¾å¦æ¨è¦ç³»çµ±åé«çåæãå¨å¯¦éæç¨ä¹åï¼ééå»£æ³çè¨ç·´ä¾è­å¥éå°ç¹å®ä»»åèª¿æ´çæä½³ HGNN æ¨¡ååæ¸æ¯ä¸åèæä¸æè²´çéç¨ãçºäºæé« HGNN è¨ç·´çæçï¼å¿é æè¿°ååæè¨ç·´éç¨ä¸­çå·è¡èªç¾©åæ¨¡å¼ï¼ä»¥è­å¥æè½ç¶é ¸ãå¨æ¬ç ç©¶ä¸­ï¼æåå°å©åä¸»æµç HGNN è¨ç·´å ´æ¯ï¼åæ¬å® GPU åå¤ GPU åæ£å¼è¨ç·´ï¼é²è¡æ·±å¥çéåååæãæ ¹ææè¿°çµæï¼æåæ­ç¤ºäºä¸å HGNN è¨ç·´å ´æ¯ä¸­çæè½ç¶é ¸åå¶æ ¹æ¬åå ï¼ä¸¦å¾è»é«åç¡¬é«çè§åº¦æä¾äºæä½³åæåã

##### **CCoE: A Compact LLM with Collaboration of Experts**
2407.11686v1 by Shaomang Huang, Jianfeng Pan, Hanzhong Zheng

In the domain of Large Language Model (LLM), LLMs demonstrate significant
capabilities in natural language understanding and generation. With the growing
needs of applying LLMs on various domains, it is a research question that how
to efficiently train and build a model that has expertise in different domains
but with a low training cost. We propose CCoE architecture, a framework of
easily coupling multiple strong domain experts together to fuse into a big LLM,
provides a collective way of utilizing the different domain expert LLMs.
Besides, training a large collaborative of multiple expert LLMs requires a high
requirements on training sources. CCoE bypasses this problem through isolating
other experts and train each expert separately. The design of CCoE assembles
multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE
layer could have one or more expert LLMs. Expert LLMs have different number of
layers and have been well-trained for different domain tasks. Each expert is
fine-tuned to be able to achieve the comparable results with SOTA domain LLMs.
We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and
Medical. The results indicate that our CCoE framework can easily and
efficiently boost nearly 10%-20% performance on original base model in
different domains but using less resources on training, as well as inference.

æè¦ï¼å¨å¤§èªè¨æ¨¡å (LLM) é åä¸­ï¼LLM å¨èªç¶èªè¨çè§£åçææ¹é¢å±ç¾åºé¡¯èçè½åãé¨è LLM å¨åç¨®é åæç¨éæ±çå¢å ï¼å¦ä½ææè¨ç·´åå»ºæ§ä¸åå¨ä¸åé åææå°æ¥­ç¥è­ä½è¨ç·´ææ¬ä½çæ¨¡åï¼æ¯ä¸åç ç©¶åé¡ãæåæåº CCoE æ¶æ§ï¼ä¸åå°å¤åå¼·å¤§çé åå°å®¶è¼é¬çµåå¨ä¸èµ·ä»¥èåæä¸åå¤§å LLM çæ¡æ¶ï¼æä¾ä¸ç¨®å©ç¨ä¸åé åå°å®¶ LLM çéé«æ¹å¼ãæ­¤å¤ï¼è¨ç·´å¤åå°å®¶ LLM çå¤§ååä½éè¦å°è¨ç·´ä¾æºæå¾é«çè¦æ±ãCCoE éééé¢å¶ä»å°å®¶ä¸¦åå¥è¨ç·´æ¯åå°å®¶ä¾ç¹ééååé¡ãCCoE çè¨­è¨éé CoEï¼å°å®¶åä½ï¼å±¤çµè£å¤åå°å®¶ LLMãæ¯å CoE å±¤å¯è½æä¸åæå¤åå°å®¶ LLMãå°å®¶ LLM å·æä¸åæ¸éçå±¤ï¼ä¸¦ä¸å·²éå°ä¸åçé åä»»åæ¥åéè¯å¥½çè¨ç·´ãæ¯åå°å®¶é½ç¶éå¾®èª¿ï¼ä»¥ä¾¿è½å¤ éå°è SOTA é å LLM ç¸ç¶ççµæãæåå¾ç¨å¼ç¢¼ãæ¸å­¸ãæ³å¾ãæå­è½ SQL åé«å­¸é åç 5 ä½å°å®¶éå§ãçµæè¡¨æï¼æåç CCoE æ¡æ¶å¯ä»¥è¼é¬ææå°å¨ä¸åé åçåå§åºç¤æ¨¡åä¸æåè¿ 10%-20% çæè½ï¼ä½å¨è¨ç·´åæ¨è«ä¸ä½¿ç¨çè³æºè¼å°ã

##### **CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging**
2407.11652v1 by Sunny Gupta, Amit Sethi

Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.

æè¦ï¼èé¦å­¦ä¹  (FL) æä¾äºä¸ç§å¨åæ£å¼æ°æ®ä¸è®­ç»æ¨¡åçéç§ä¿æ¤æ¹æ³ãå®å¨å»çä¿å¥ä¸­çæ½åå¾å¤§ï¼ä½ç±äºå»çå¾åæ°æ®ä¸­å­å¨è·¨å®¢æ·ç«¯å·®å¼ï¼å æ­¤å¸¦æ¥äºææï¼èæéçæ³¨éå å§äºè¿ä¸é®é¢ãæ¬æä»ç»äºè·¨å®¢æ·ç«¯å·®å¼èªéåºèé¦å­¦ä¹  (CCVA-FL) æ¥è§£å³è¿äºé®é¢ãCCVA-FL æ¨å¨éè¿å°å¾åè½¬æ¢ä¸ºå¬å±ç¹å¾ç©ºé´æ¥æå°åè·¨å®¢æ·ç«¯å·®å¼ãå®æ¶åä»æ¯ä¸ªå®¢æ·ç«¯æ³¨éå¾åå­éçä¸å®¶æ³¨éï¼ç¶åéæ©æ°æ®å¤ææ§æä½çå®¢æ·ç«¯ä½ä¸ºç®æ ãç¶åä½¿ç¨åºäºç®æ å®¢æ·ç«¯æ³¨éå¾åçå¯æ©å±æ©æ£æ¨¡åä¸ Transformer (DiT) çæåæå»å­¦å¾åãè¿äºåæå¾åææäºå¤æ ·æ§å¹¶ä»£è¡¨äºåå§æ°æ®ï¼ä¸å¶ä»å®¢æ·ç«¯å±äº«ãç¶åï¼æ¯ä¸ªå®¢æ·ç«¯ä½¿ç¨å¾åå°å¾åç¿»è¯å°å¶æ¬å°å¾åè½¬æ¢ä¸ºç®æ å¾åç©ºé´ãç¿»è¯åçå¾åéåå¨èé¦å­¦ä¹ è®¾ç½®ä¸­ç¨äºå¼åæå¡å¨æ¨¡åãæä»¬çç»æè¡¨æï¼CCVA-FL éè¿ææè§£å³è·¨å®¢æ·ç«¯çæ°æ®åå¸å·®å¼å¨ä¸æå®³éç§çæåµä¸ä¼äºé¦èèé¦å¹³åã

##### **Improving Engagement and Efficacy of mHealth Micro-Interventions for Stress Coping: an In-The-Wild Study**
2407.11612v1 by Chaya Ben Yehuda, Ran Gilad-Bachrach, Yarin Udi

Sustaining long-term user engagement with mobile health (mHealth)
interventions while preserving their high efficacy remains an ongoing challenge
in real-world well-being applications. To address this issue, we introduce a
new algorithm, the Personalized, Context-Aware Recommender (PCAR), for
intervention selection and evaluate its performance in a field experiment. In a
four-week, in-the-wild experiment involving 29 parents of young children, we
delivered personalized stress-reducing micro-interventions through a mobile
chatbot. We assessed their impact on stress reduction using momentary stress
level ecological momentary assessments (EMAs) before and after each
intervention. Our findings demonstrate the superiority of PCAR intervention
selection in enhancing the engagement and efficacy of mHealth
micro-interventions to stress coping compared to random intervention selection
and a control group that did not receive any intervention. Furthermore, we show
that even brief, one-minute interventions can significantly reduce perceived
stress levels (p=0.001). We observe that individuals are most receptive to
one-minute interventions during transitional periods between activities, such
as transitioning from afternoon activities to bedtime routines. Our study
contributes to the literature by introducing a personalized context-aware
intervention selection algorithm that improves engagement and efficacy of
mHealth interventions, identifying key timing for stress interventions, and
offering insights into mechanisms to improve stress coping.

æè¦ï¼<paragraph>å¨ç¶­æè¡åå¥åº· (mHealth) å¹²é æªæ½çé·æä½¿ç¨èåèåº¦ï¼åæç¶­æå¶é«åæï¼å¨ç¾å¯¦ä¸ççå¥åº·æç¨ä¸­ä»æ¯ä¸åæçºçææ°ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äºä¸ç¨®æ°çæ¼ç®æ³ï¼ç¨±çºåäººåãæå¢æç¥æ¨è¦å¨ (PCAR)ï¼ç¨æ¼å¹²é é¸æï¼ä¸¦å¨å¯¦å°å¯¦é©ä¸­è©ä¼°å¶æè½ãå¨çºæåé±çéå¤å¯¦é©ä¸­ï¼æ¶å 29 ä½å¹¼åçç¶æ¯ï¼æåééè¡åèå¤©æ©å¨äººå³éåäººåçæ¸å£å¾®åå¹²é æªæ½ãæåééå¨æ¯æ¬¡å¹²é æªæ½åå¾é²è¡çç¬éå£åç­ç´çæç¬æè©ä¼° (EMA)ï¼è©ä¼°å®åå°æ¸å£çå½±é¿ãæåçç ç©¶çµæè­æäº PCAR å¹²é é¸æå¨å¢å¼· mHealth å¾®åå¹²é æªæ½å°å£åæå°çåèåº¦åæè½æ¹é¢çåªè¶æ§ï¼ç¸è¼æ¼é¨æ©å¹²é é¸æåæªæ¥åä»»ä½å¹²é æªæ½çå°ç§çµãæ­¤å¤ï¼æåè­æäºå³ä½¿ç°¡ç­çä¸åéå¹²é æªæ½ä¹è½é¡¯èéä½æç¥å£åç­ç´ (p=0.001)ãæåè§å¯å°ï¼åäººå¨æ´»åä¹éçéæ¸¡æï¼ä¾å¦å¾ä¸åæ´»åéæ¸¡å°å°±å¯¢æéï¼å°ä¸åéçå¹²é æªæ½æå·æ¥ååº¦ãæåçç ç©¶ééå¼å¥ä¸ç¨®åäººåæå¢æç¥å¹²é é¸ææ¼ç®æ³ï¼æ¹å mHealth å¹²é æªæ½çåèåº¦åæè½ï¼æ¾åºå£åå¹²é æªæ½çééµææ©ï¼ä¸¦æä¾æ¹åå£åæå°æ©å¶çè¦è§£ï¼çºæç»ååºè²¢ç»ã</paragraph>

##### **DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training**
2407.11594v1 by Guillermo Jimenez-Perez, Pedro Osorio, Josef Cersovsky, Javier Montalt-Tordera, Jens Hooge, Steffen Vogler, Sadegh Mohammadi

Diffusion models (DMs) have emerged as powerful foundation models for a
variety of tasks, with a large focus in synthetic image generation. However,
their requirement of large annotated datasets for training limits their
applicability in medical imaging, where datasets are typically smaller and
sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for
training latent diffusion models (LDMs) that conditions the generation process
on image embeddings extracted from DiNO. By eliminating the reliance on
annotations, our training leverages over 868k unlabelled images from public
chest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows
comprehensive manifold coverage, with FID scores as low as 4.7, and emerging
properties when evaluated in downstream tasks. It can be used to generate
semantically-diverse synthetic datasets even from small data pools,
demonstrating up to 20% AUC increase in classification performance when used
for data augmentation. Images were generated with different sampling strategies
over the DiNO embedding manifold and using real images as a starting point.
Results suggest, DiNO-Diffusion could facilitate the creation of large datasets
for flexible training of downstream AI models from limited amount of real data,
while also holding potential for privacy preservation. Additionally,
DiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4%
Dice score when evaluating lung lobe segmentation. This evidences good CXR
image-anatomy alignment, akin to segmenting using textual descriptors on
vanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical
imaging modalities or state-of-the-art diffusion models, opening the door for
large-scale, multi-domain image generation pipelines for medical imaging.

æè¦ï¼æ´æ£æ¨¡å (DM) å·²æçºåç¨®ä»»åä¸­å¼·å¤§çåºç¤æ¨¡åï¼ç¹å¥æ¯åæå½±åçæãç¶èï¼å®åå¨è¨ç·´ä¸­å°å¤§åæ¨è¨»è³æéçè¦æ±éå¶äºå®åå¨é«çå½±åä¸­çæç¨ï¼èé«çå½±åçè³æééå¸¸è¼å°ä¸æ¨è¨»ç¨çãæåå¼å¥äº DiNO-Diffusionï¼éæ¯ä¸ç¨®ç¨æ¼è¨ç·´æ¢ä»¶çæéç¨çæ½å¨æ´æ£æ¨¡å (LDM) çèªç£ç£æ¹æ³ï¼è©²éç¨åºæ¼å¾ DiNO ä¸­æåçå½±ååµå¥ãééæ¶é¤å°æ¨è¨»çä¾è³´ï¼æåçè¨ç·´å©ç¨äºä¾èªå¬å±è¸é¨ X å (CXR) è³æéçè¶é 868k å¼µæªæ¨è¨»å½±åãåç®¡æ¯èªç£ç£çï¼ä½ DiNO-Diffusion é¡¯ç¤ºåºå¨é¢çæµå½¢è¦èï¼FID åæ¸ä½è³ 4.7ï¼ä¸¦ä¸å¨è©ä¼°ä¸æ¸¸ä»»åæåºç¾äºæ°èçå±¬æ§ãå®å¯ç¨æ¼å¾å°åè³æåº«çæèªç¾©å¤æ¨£çåæè³æéï¼å¨ç¨æ¼è³ææ´åæï¼åé¡æè½æåå¹åº¦é«é 20% AUCãå½±åæ¯å¨ DiNO åµå¥æµå½¢ä¸ä½¿ç¨ä¸åçåæ¨£ç­ç¥çæçï¼ä¸¦ä½¿ç¨çå¯¦å½±åä½çºèµ·é»ãçµæé¡¯ç¤ºï¼DiNO-Diffusion å¯ä»¥ä¿é²å¾æéççå¯¦è³æä¸­éæ´»è¨ç·´ä¸æ¸¸ AI æ¨¡åçå¤§åè³æéçå»ºç«ï¼åæä¹å·æé±ç§ä¿è­·çæ½åãæ­¤å¤ï¼DiNO-Diffusion å¨è©ä¼°èºèåå²æå±ç¤ºäºé«é 84.4% ç Dice åæ¸çé¶æ¬¡å­¸ç¿åå²æè½ãéè­æäºè¯å¥½ç CXR å½±åè§£åå°é½ï¼é¡ä¼¼æ¼å¨é¦è DM ä¸ä½¿ç¨æå­æè¿°ç¬¦é²è¡åå²ãæå¾ï¼DiNO-Diffusion å¯ä»¥è¼é¬é©æå¶ä»é«çå½±åæ¹å¼ææåé²çæ´æ£æ¨¡åï¼çºé«çå½±åçå¤§è¦æ¨¡ãå¤é åå½±åçæç®¡ééåäºå¤§éã

##### **Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification**
2407.11573v1 by Naif Alkhunaizi, Faris Almalik, Rouqaiah Al-Refai, Muzammal Naseer, Karthik Nandakumar

With the advent of large pre-trained transformer models, fine-tuning these
models for various downstream tasks is a critical problem. Paucity of training
data, the existence of data silos, and stringent privacy constraints exacerbate
this fine-tuning problem in the medical imaging domain, creating a strong need
for algorithms that enable collaborative fine-tuning of pre-trained models.
Moreover, the large size of these models necessitates the use of
parameter-efficient fine-tuning (PEFT) to reduce the communication burden in
federated learning. In this work, we systematically investigate various
federated PEFT strategies for adapting a Vision Transformer (ViT) model
(pre-trained on a large natural image dataset) for medical image
classification. Apart from evaluating known PEFT techniques, we introduce new
federated variants of PEFT algorithms such as visual prompt tuning (VPT),
low-rank decomposition of visual prompts, stochastic block attention
fine-tuning, and hybrid PEFT methods like low-rank adaptation (LoRA)+VPT.
Moreover, we perform a thorough empirical analysis to identify the optimal PEFT
method for the federated setting and understand the impact of data distribution
on federated PEFT, especially for out-of-domain (OOD) and non-IID data. The key
insight of this study is that while most federated PEFT methods work well for
in-domain transfer, there is a substantial accuracy vs. efficiency trade-off
when dealing with OOD and non-IID scenarios, which is commonly the case in
medical imaging. Specifically, every order of magnitude reduction in
fine-tuned/exchanged parameters can lead to a 4% drop in accuracy. Thus, the
initial model choice is crucial for federated PEFT. It is preferable to use
medical foundation models learned from in-domain medical image data (if
available) rather than general vision models.

æè¦ï¼<paragraph>é¨èå¤§åé è¨ç·´è½æå¨æ¨¡åçåºç¾ï¼éå°åç¨®ä¸æ¸¸ä»»åå¾®èª¿éäºæ¨¡åæ¯ä¸åééµåé¡ãè¨ç·´è³æçç¨ç¼ºæ§ãè³æå­¤å³¶çå­å¨ä»¥åå´æ ¼çé±ç§éå¶æå åé«çå½±åé åä¸­çå¾®èª¿åé¡ï¼éå°è½è®é è¨ç·´æ¨¡åé²è¡åä½å¾®èª¿çæ¼ç®æ³ç¢çäºå¼·çéæ±ãæ­¤å¤ï¼éäºæ¨¡åçé¾å¤§è¦æ¨¡éè¦ä½¿ç¨åæ¸ææå¾®èª¿ (PEFT) ä¾éä½è¯åå­¸ç¿ä¸­çéè¨è² æãå¨éé å·¥ä½ä¸­ï¼æåç³»çµ±æ§å°æ¢è¨äºåç¨®è¯å PEFT ç­ç¥ï¼ä»¥èª¿æ´è¦è¦ºè½æå¨ (ViT) æ¨¡åï¼å¨å¤§åèªç¶å½±åè³æéä¸é åè¨ç·´ï¼ä»¥é²è¡é«çå½±ååé¡ãé¤äºè©ä¼°å·²ç¥ç PEFT æè¡å¤ï¼æåéå¼å¥äº PEFT æ¼ç®æ³çæ°è¯åè®é«ï¼ä¾å¦è¦è¦ºæç¤ºèª¿æ´ (VPT)ãè¦è¦ºæç¤ºçä½ç§©åè§£ãé¨æ©åå¡æ³¨æåå¾®èª¿ï¼ä»¥åä½ç§©é©æ (LoRA)+VPT ç­æ··å PEFT æ¹æ³ãæ­¤å¤ï¼æåé²è¡äºå¾¹åºçç¶é©åæï¼ä»¥æ¾åºè¯åè¨­å®çæä½³ PEFT æ¹æ³ï¼ä¸¦äºè§£è³æåä½å°è¯å PEFT çå½±é¿ï¼ç¹å¥æ¯å°æ¼é åå¤ (OOD) åéç¨ç«ååä½ (non-IID) è³æãéé ç ç©¶çä¸»è¦è¦è§£æ¯ï¼åç®¡å¤§å¤æ¸è¯å PEFT æ¹æ³é½é©ç¨æ¼é åå§è½ç§»ï¼ä½å¨èç OOD åéç¨ç«ååä½å ´æ¯æï¼ææå¤§å¹çæºç¢ºåº¦èæçæè¡·ï¼ééå¸¸æ¯é«çå½±åä¸­çææ³ãå·é«ä¾èªªï¼å¾®èª¿/äº¤æåæ¸çæ¯åæ¸éç´æ¸å°é½å¯è½å°è´æºç¢ºåº¦ä¸é 4%ãå æ­¤ï¼åå§æ¨¡åçé¸æå°æ¼è¯å PEFT è³ééè¦ãæå¥½ä½¿ç¨å¾é åå§é«å­¸å½±åè³æï¼å¦ææçè©±ï¼å­¸ç¿çé«å­¸åºç¤æ¨¡åï¼èä¸æ¯ä¸è¬è¦è¦ºæ¨¡åã</paragraph>

##### **Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise**
2407.11536v1 by Qimin Yang, Rongsheng Wang, Jiexin Chen, Runqi Su, Tao Tan

Large Language Models (LLMs) have been widely applied in various professional
fields. By fine-tuning the models using domain specific question and answer
datasets, the professional domain knowledge and Q\&A abilities of these models
have significantly improved, for example, medical professional LLMs that use
fine-tuning of doctor-patient Q\&A data exhibit extraordinary disease
diagnostic abilities. However, we observed that despite improvements in
specific domain knowledge, the performance of medical LLM in long-context
understanding has significantly declined, especially compared to general
language models with similar parameters. The purpose of this study is to
investigate the phenomenon of reduced performance in understanding long-context
in medical LLM. We designed a series of experiments to conduct open-book
professional knowledge exams on all models to evaluate their ability to read
long-context. By adjusting the proportion and quantity of general data and
medical data in the process of fine-tuning, we can determine the best data
composition to optimize the professional model and achieve a balance between
long-context performance and specific domain knowledge.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å»£æ³æç¨æ¼åç¨®å°æ¥­é åãééä½¿ç¨ç¹å®é åçåç­è³æéå¾®èª¿æ¨¡åï¼éäºæ¨¡åçå°æ¥­é åç¥è­ååç­è½åå·²é¡¯èæåï¼ä¾å¦ï¼ä½¿ç¨é«ç-æ£èåç­è³æé²è¡å¾®èª¿çé«çå°æ¥­ LLM å±ç¾åºéå¡çç¾çè¨ºæ·è½åãç¶èï¼æåè§å¯å°ï¼åç®¡ç¹å®é åç¥è­æææåï¼ä½é«ç LLM å¨é·èªå¢çè§£æ¹é¢çè¡¨ç¾å»å¤§å¹ä¸éï¼å°¤å¶æ¯èå·æé¡ä¼¼åæ¸çä¸è¬èªè¨æ¨¡åç¸æ¯ãæ¬ç ç©¶çç®çæ¯æ¢è¨é«ç LLM å¨çè§£é·èªå¢æ¹é¢çè¡¨ç¾ä¸éç¾è±¡ãæåè¨­è¨äºä¸ç³»åå¯¦é©ï¼å°æææ¨¡åé²è¡éæ¾å¼å°æ¥­ç¥è­èè©¦ï¼ä»¥è©ä¼°å®åé±è®é·èªå¢ççè§£è½åãééèª¿æ´å¾®èª¿éç¨ä¸­ä¸è¬è³æåé«çè³æçæ¯ä¾åæ¸éï¼æåå¯ä»¥ç¢ºå®æä½³è³æçµåï¼ä»¥åªåå°æ¥­æ¨¡åï¼ä¸¦å¨é·èªå¢è¡¨ç¾åç¹å®é åç¥è­ä¹éåå¾å¹³è¡¡ã

##### **Cross-Phase Mutual Learning Framework for Pulmonary Embolism Identification on Non-Contrast CT Scans**
2407.11529v1 by Bizhe Bai, Yan-Jie Zhou, Yujian Hu, Tony C. W. Mok, Yilang Xiang, Le Lu, Hongkun Zhang, Minfeng Xu

Pulmonary embolism (PE) is a life-threatening condition where rapid and
accurate diagnosis is imperative yet difficult due to predominantly atypical
symptomatology. Computed tomography pulmonary angiography (CTPA) is
acknowledged as the gold standard imaging tool in clinics, yet it can be
contraindicated for emergency department (ED) patients and represents an
onerous procedure, thus necessitating PE identification through non-contrast CT
(NCT) scans. In this work, we explore the feasibility of applying a
deep-learning approach to NCT scans for PE identification. We propose a novel
Cross-Phase Mutual learNing framework (CPMN) that fosters knowledge transfer
from CTPA to NCT, while concurrently conducting embolism segmentation and
abnormality classification in a multi-task manner. The proposed CPMN leverages
the Inter-Feature Alignment (IFA) strategy that enhances spatial contiguity and
mutual learning between the dual-pathway network, while the Intra-Feature
Discrepancy (IFD) strategy can facilitate precise segmentation of PE against
complex backgrounds for single-pathway networks. For a comprehensive assessment
of the proposed approach, a large-scale dual-phase dataset containing 334 PE
patients and 1,105 normal subjects has been established. Experimental results
demonstrate that CPMN achieves the leading identification performance, which is
95.4\% and 99.6\% in patient-level sensitivity and specificity on NCT scans,
indicating the potential of our approach as an economical, accessible, and
precise tool for PE identification in clinical practice.

æè¦ï¼èºæ å¡ (PE) æ¯ä¸ç¨®å±åçå½çç¾çï¼å¿«éä¸æºç¢ºçè¨ºæ·è³ééè¦ï¼ä½ç±æ¼ççä¸»è¦æ¯éå¸åçï¼å æ­¤å¾é£è¨ºæ·ãé»è¦æ·å±¤èºè¡ç®¡æå½± (CTPA) è¢«å¬èªçºè¨ºæä¸­çé»éæ¨æºå½±åå·¥å·ï¼ä½å®å¯è½æå°æ¥è¨ºé¨é (ED) çæ£èç¦å¿ï¼ä¸¦ä¸ä»£è¡¨èç¹éçç¨åºï¼å æ­¤éè¦éééå°æ¯ CT (NCT) ææä¾è­å¥ PEãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨äºå°æ·±åº¦å­¸ç¿æ¹æ³æç¨æ¼ NCT ææä»¥è­å¥ PE çå¯è¡æ§ãæåæåºäºä¸åæ°ç©çè·¨ç¸ä½äºå­¸ç¿æ¡æ¶ (CPMN)ï¼å®ä¿é²äºå¾ CTPA å° NCT çç¥è­è½ç§»ï¼åæä»¥å¤ä»»åçæ¹å¼é²è¡æ å¡åå²åç°å¸¸åé¡ãææåºç CPMN æ¡ç¨äºç¹å¾µéå°é½ (IFA) ç­ç¥ï¼å®å¢å¼·äºéè·¯å¾ç¶²è·¯ä¹éçç©ºéé£çºæ§åç¸äºå­¸ç¿ï¼èç¹å¾µå§å·®ç° (IFD) ç­ç¥å¯ä»¥ä¿é²å®è·¯å¾ç¶²è·¯å°è¤éèæ¯ä¸­ç PE é²è¡ç²¾ç¢ºåå²ãçºäºå°ææåºçæ¹æ³é²è¡å¨é¢è©ä¼°ï¼å·²ç¶å»ºç«äºä¸ååå« 334 å PE æ£èå 1,105 åæ­£å¸¸åè©¦èçãå¤§è¦æ¨¡éç¸ä½æ¸æéãå¯¦é©çµæè¡¨æï¼CPMN éå°äºé åçè­å¥æè½ï¼å¨ NCT ææä¸­ï¼æ£èå±¤ç´çæææ§åç¹ç°æ§åå¥çº 95.4% å 99.6%ï¼éè¡¨ææåçåæ³æå¯è½æçºä¸ç¨®ç¶æ¿ãææ¼åå¾ä¸ç²¾ç¢ºç PE è­å¥å·¥å·ï¼å¯æç¨æ¼è¨åºå¯¦åã

##### **Multi-Channel Masked Autoencoder and Comprehensive Evaluations for Reconstructing 12-Lead ECG from Arbitrary Single-Lead ECG**
2407.11481v1 by Jiarong Chen, Wanqing Wu, Tong Liu, Shenda Hong

In the context of cardiovascular diseases (CVD) that exhibit an elevated
prevalence and mortality, the electrocardiogram (ECG) is a popular and standard
diagnostic tool for doctors, commonly utilizing a 12-lead configuration in
clinical practice. However, the 10 electrodes placed on the surface would cause
a lot of inconvenience and discomfort, while the rapidly advancing wearable
devices adopt the reduced-lead or single-lead ECG to reduce discomfort as a
solution in long-term monitoring. Since the single-lead ECG is a subset of
12-lead ECG, it provides insufficient cardiac health information and plays a
substandard role in real-world healthcare applications. Hence, it is necessary
to utilize signal generation technologies to reduce their clinical importance
gap by reconstructing 12-lead ECG from the real single-lead ECG. Specifically,
this study proposes a multi-channel masked autoencoder (MCMA) for this goal. In
the experimental results, the visualized results between the generated and real
signals can demonstrate the effectiveness of the proposed framework. At the
same time, this study introduces a comprehensive evaluation benchmark named
ECGGenEval, encompassing the signal-level, feature-level, and diagnostic-level
evaluations, providing a holistic assessment of 12-lead ECG signals and
generative model. Further, the quantitative experimental results are as
follows, the mean square errors of 0.0178 and 0.0658, correlation coefficients
of 0.7698 and 0.7237 in the signal-level evaluation, the average F1-score with
two generated 12-lead ECG is 0.8319 and 0.7824 in the diagnostic-level
evaluation, achieving the state-of-the-art performance. The open-source code is
publicly available at \url{https://github.com/CHENJIAR3/MCMA}.

æè¦ï¼<paragraph>å¨è¡¨ç¾åºé«çè¡çåæ­»äº¡ççå¿è¡ç®¡ç¾ç (CVD) çææ³ä¸ï¼å¿é»å (ECG) æ¯ä¸ç¨®é«çå¸¸ç¨çæ¨æºè¨ºæ·å·¥å·ï¼å¨è¨åºå¯¦åä¸­éå¸¸ä½¿ç¨ 12 å°ç¨çµæãç¶èï¼æ¾ç½®å¨è¡¨é¢ç 10 åé»æ¥µæé æè¨±å¤ä¸ä¾¿åä¸é©ï¼èå¿«éé²æ­¥çå¯ç©¿æ´å¼è£ç½®æ¡ç¨æ¸å°å°ç¨æå®å°ç¨ ECG ä¾éä½ä¸é©ï¼ä½çºé·æç£æ¸¬çè§£æ±ºæ¹æ¡ãç±æ¼å®å°ç¨ ECG æ¯ 12 å°ç¨ ECG çå­éï¼å®æä¾çå¥åº·è³è¨ä¸è¶³ï¼å¨çå¯¦ä¸ççé«çä¿å¥æç¨ä¸­æ®æ¼èæ¬¡æ¨æºçè§è²ãå æ­¤ï¼æå¿è¦å©ç¨è¨èç¢çæè¡ä¾ç¸®å°å¶è¨åºéè¦æ§å·®è·ï¼æ¹æ³æ¯å¾çå¯¦çå®å°ç¨ ECG éå»º 12 å°ç¨ ECGãå·é«ä¾èªªï¼æ¬ç ç©¶æåºäºä¸åå¤ééé®ç½©èªåç·¨ç¢¼å¨ (MCMA) ä¾éææ­¤ç®æ¨ãå¨å¯¦é©çµæä¸­ï¼çæçè¨èèçå¯¦è¨èä¹éçå¯è¦åçµæå¯ä»¥è­æææåºæ¶æ§çæææ§ãåæï¼æ¬ç ç©¶å¼å¥äºç¨±çº ECGGenEval çç¶åè©ä¼°åºæºï¼æ¶µèè¨èå±¤ç´ãç¹å¾µå±¤ç´åè¨ºæ·å±¤ç´è©ä¼°ï¼æä¾ 12 å°ç¨ ECG è¨èåçææ¨¡åçæ´é«è©ä¼°ãæ­¤å¤ï¼å®éçå¯¦é©çµæå¦ä¸ï¼å¨è¨èå±¤ç´è©ä¼°ä¸­ï¼åæ¹èª¤å·®çº 0.0178 å 0.0658ï¼ç¸éä¿æ¸çº 0.7698 å 0.7237ï¼å¨è¨ºæ·å±¤ç´è©ä¼°ä¸­ï¼å©åçæç 12 å°ç¨ ECG çå¹³å F1 åæ¸çº 0.8319 å 0.7824ï¼éå°äºæåé²çæè½ãéæ¾åå§ç¢¼å¯ä»¥å¨ \url{https://github.com/CHENJIAR3/MCMA} å¬éåå¾ã</paragraph>

##### **TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering**
2407.11383v1 by Tonmoy Rajkhowa, Amartya Roy Chowdhury, Sankalp Nagaonkar, Achyut Mani Tripathi

In healthcare and medical diagnostics, Visual Question Answering (VQA)
mayemergeasapivotal tool in scenarios where analysis of intricate medical
images becomes critical for accurate diagnoses. Current text-based VQA systems
limit their utility in scenarios where hands-free interaction and accessibility
are crucial while performing tasks. A speech-based VQA system may provide a
better means of interaction where information can be accessed while performing
tasks simultaneously. To this end, this work implements a speech-based VQA
system by introducing a Textless Multilingual Pathological VQA (TMPathVQA)
dataset, an expansion of the PathVQA dataset, containing spoken questions in
English, German & French. This dataset comprises 98,397 multilingual spoken
questions and answers based on 5,004 pathological images along with 70 hours of
audio. Finally, this work benchmarks and compares TMPathVQA systems implemented
using various combinations of acoustic and visual features.

æè¦ï¼å¨é«çä¿å¥åé«çè¨ºæ·ä¸­ï¼è¦è¦ºåç­ï¼VQAï¼å¯è½æçºééµå·¥å·ï¼å¨åæè¤éçé«çå½±åå°æ¼æºç¢ºè¨ºæ·è³ééè¦çå ´æ¯ä¸­ãç®åçåºæ¼æå­ç VQA ç³»çµ±éå¶äºå®åå¨å·è¡ä»»åæåæäºååå¯åæ§è³ééè¦çå ´æ¯ä¸­çæç¨ãåºæ¼èªé³ç VQA ç³»çµ±å¯è½æä¾æ´å¥½çäºåæ¹å¼ï¼å¯ä»¥å¨å·è¡ä»»åçåæå­åè³è¨ãçºæ­¤ï¼æ¬ç ç©¶ééå°å¥ç¡æå­å¤èªè¨çç VQAï¼TMPathVQAï¼è³æéï¼æ´åäº PathVQA è³æéï¼åå«è±èªãå¾·èªåæ³èªçå£èªªåé¡ï¼å¯¦ä½äºä¸ååºæ¼èªé³ç VQA ç³»çµ±ãæ­¤è³æéåå« 98,397 åå¤èªè¨çå£èªªåé¡åç­æ¡ï¼åºæ¼ 5,004 åççå½±åä»¥å 70 å°æçé³è¨ãæå¾ï¼æ¬ç ç©¶ä½¿ç¨åç¨®é³è¨åè¦è¦ºç¹å¾µççµåä¾å¯¦ä½ TMPathVQA ç³»çµ±ï¼ä¸¦é²è¡åºæºæ¸¬è©¦åæ¯è¼ã

##### **Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs**
2407.10888v1 by Leonardo Crespi, Samuele Camnasio, Damiano Dei, Nicola Lambri, Pietro Mancosu, Marta Scorsetti, Daniele Loiacono

In many clinical settings, the use of both Computed Tomography (CT) and
Magnetic Resonance (MRI) is necessary to pursue a thorough understanding of the
patient's anatomy and to plan a suitable therapeutical strategy; this is often
the case in MRI-based radiotherapy, where CT is always necessary to prepare the
dose delivery, as it provides the essential information about the radiation
absorption properties of the tissues. Sometimes, MRI is preferred to contour
the target volumes. However, this approach is often not the most efficient, as
it is more expensive, time-consuming and, most importantly, stressful for the
patients. To overcome this issue, in this work, we analyse the capabilities of
different configurations of Deep Learning models to generate synthetic CT scans
from MRI, leveraging the power of Generative Adversarial Networks (GANs) and,
in particular, the CycleGAN architecture, capable of working in an unsupervised
manner and without paired images, which were not available. Several CycleGAN
models were trained unsupervised to generate CT scans from different MRI
modalities with and without contrast agents. To overcome the problem of not
having a ground truth, distribution-based metrics were used to assess the
model's performance quantitatively, together with a qualitative evaluation
where physicians were asked to differentiate between real and synthetic images
to understand how realistic the generated images were. The results show how,
depending on the input modalities, the models can have very different
performances; however, models with the best quantitative results, according to
the distribution-based metrics used, can generate very difficult images to
distinguish from the real ones, even for physicians, demonstrating the
approach's potential.

æè¦ï¼å¨è¨±å¤è¨åºç°å¢ä¸­ï¼éè¦ä½¿ç¨é»è¦æ·å±¤ææ (CT) åç£æ¯é å½± (MRI) ä¾å¾¹åºäºè§£æ£èçè§£åçµæ§ï¼ä¸¦è¦åé©ç¶çæ²»çç­ç¥ï¼ééå¸¸ç¼çå¨åºæ¼ MRI çæ¾å°æ²»çä¸­ï¼å¶ä¸­ CT å°æ¼æºååéå³éç¸½æ¯å¿è¦çï¼å çºå®æä¾äºæéçµç¹è¼»å°å¸æ¶ç¹æ§çåºæ¬è³è¨ãææï¼MRI åªåæ¼å¾åç®æ¨é«ç©ãç¶èï¼éç¨®æ¹æ³éå¸¸ä¸æ¯æææççï¼å çºå®æ´æè²´ãèæï¼æéè¦çæ¯æè®æ£èæå°å£åãçºäºåæéååé¡ï¼å¨éé å·¥ä½ä¸­ï¼æååæäºæ·±åº¦å­¸ç¿æ¨¡åçä¸åéç½®ï¼ä»¥å¾ MRI çæåæ CT ææçè½åï¼å©ç¨çæå°æç¶²è·¯ (GAN) çåè½ï¼ç¹å¥æ¯ CycleGAN æ¶æ§ï¼è½å¤ ä»¥ç¡ç£ç£çæ¹å¼å·¥ä½ï¼èä¸ä¸éè¦æå°çå½±åï¼èéäºå½±åä¸¦ä¸å¯ç¨ãå¹¾å CycleGAN æ¨¡åç¶éç¡ç£ç£è¨ç·´ï¼ä»¥å¾ä¸å MRI æ¨¡å¼çæ CT ææï¼ç¡è«æ¯å¦ä½¿ç¨å°æ¯åãçºäºåææ²æåºæ¬äºå¯¦çåé¡ï¼åºæ¼åä½çææ¨è¢«ç¨æ¼å®éè©ä¼°æ¨¡åçæè½ï¼ä»¥åå®æ§è©ä¼°ï¼å¶ä¸­è¦æ±é«çååçå¯¦ååæå½±åï¼ä»¥äºè§£çæçå½±åæå¤é¼çãçµæé¡¯ç¤ºï¼æ ¹æè¼¸å¥æ¨¡å¼ï¼æ¨¡åçæè½å¯è½å¤§ä¸ç¸åï¼ç¶èï¼æ ¹ææä½¿ç¨çåºæ¼åä½çææ¨ï¼å·ææä½³å®éçµæçæ¨¡åå¯ä»¥ç¢çéå¸¸é£ä»¥èçå¯¦å½±åååçå½±åï¼å³ä½¿å°æ¼é«çä¾èªªä¹æ¯å¦æ­¤ï¼éè­æäºéç¨®æ¹æ³çæ½åã

##### **Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method**
2407.10828v1 by Yi-Wei Chua, Yun-Chien Cheng

This study aims to develop an auxiliary diagnostic system for classifying
abnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal
breath sound classification through an innovative multi-label learning approach
and multi-head attention mechanism. Addressing the issue of class imbalance and
lack of diversity in existing respiratory sound datasets, our study employs a
lightweight and highly accurate model, using a two-dimensional label set to
represent multiple respiratory sound characteristics. Our method achieved a
59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,
demonstrating its advantages in terms of lightweight and high accuracy. This
study not only improves the accuracy of automatic diagnosis of lung respiratory
sound abnormalities but also opens new possibilities for clinical applications.

æè¦ï¼æ¬ç ç©¶æ¨å¨éç¼ä¸åè¼å©è¨ºæ·ç³»çµ±ï¼ç¨æ¼åé¡ç°å¸¸çèºé¨å¼å¸é³ï¼ééåµæ°çå¤æ¨ç±¤å­¸ç¿æ¹æ³åå¤é ­æ³¨æåæ©å¶ï¼æåèªåç°å¸¸å¼å¸é³åé¡çæºç¢ºåº¦ãéå°ç¾æå¼å¸é³è³æéä¸­é¡å¥ä¸å¹³è¡¡åç¼ºä¹å¤æ¨£æ§çåé¡ï¼æ¬ç ç©¶æ¡ç¨è¼éä¸é«ç²¾ç¢ºåº¦çæ¨¡åï¼ä½¿ç¨äºç¶­æ¨ç±¤çµä¾è¡¨ç¤ºå¤éå¼å¸é³ç¹å¾µãæåçæ¨¡åå¨ ICBHI2017 è³æéçåé¡å¥ä»»åä¸­ï¼ç²å¾äº 59.2% ç ICBHI åæ¸ï¼è­æäºå¶å¨è¼éååé«æºç¢ºåº¦æ¹é¢çåªå¢ãæ¬ç ç©¶ä¸åæåäºèºé¨å¼å¸é³ç°å¸¸èªåè¨ºæ·çæºç¢ºåº¦ï¼ä¹çºè¨åºæç¨éåäºæ°çå¯è½æ§ã

##### **Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN**
2407.10689v1 by Seyed Amir Latifi, Hassan Ghassemian, Maryam Imani

This paper presents a fast and cost-effective method for diagnosing cardiac
abnormalities with high accuracy and reliability using low-cost systems in
clinics. The primary limitation of automatic diagnosing of cardiac diseases is
the rarity of correct and acceptable labeled samples, which can be expensive to
prepare. To address this issue, two methods are proposed in this work. The
first method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)
architecture inspired by human auditory processing, specifically designed to
optimize feature extraction by employing various sizes of convolutional filters
and audio signal power spectrum as input. In the second method, called as Long
short-term memory-Convolutional Neural (LSCN) model, Additionally, the network
architecture includes Long Short-Term Memory (LSTM) network blocks to improve
feature extraction in the time domain. The innovative approach of combining
multiple parallel branches consisting of the one-dimensional convolutional
layers along with LSTM blocks helps in achieving superior results in audio
signal processing tasks. The experimental results demonstrate superiority of
the proposed methods over the state-of-the-art techniques. The overall
classification accuracy of heart sounds with the LSCN network is more than 96%.
The efficiency of this network is significant compared to common feature
extraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and
wavelet transform. Therefore, the proposed method shows promising results in
the automatic analysis of heart sounds and has potential applications in the
diagnosis and early detection of cardiovascular diseases.

æè¦ï¼æ¬ææåºäºä¸ç¨®å¿«éä¸ç¶æ¿ææçæ¹æ³ï¼ä½¿ç¨ä½ææ¬çç³»çµ±å¨è¨ºæè¨ºæ·å¿èç°å¸¸ï¼ä¸å·æé«æºç¢ºåº¦åå¯é æ§ãèªåè¨ºæ·å¿èç¾ççä¸»è¦éå¶æ¯æ­£ç¢ºä¸å¯æ¥åçæ¨ç±¤æ¨£æ¬ç¨å°ï¼èä¸æºåèµ·ä¾å¯è½å¾æè²´ãçºäºè§£æ±ºéååé¡ï¼éé å·¥ä½æåºäºå©ç¨®æ¹æ³ãç¬¬ä¸ç¨®æ¹æ³æ¯ä¸ç¨®ç¨ç¹çå¤åæ¯æ·±åº¦å·ç©ç¥ç¶ç¶²è·¯ (MBDCN) æ¶æ§ï¼éæä¾èªäººé¡è½è¦ºèçï¼ç¹å¥è¨­è¨çºééæ¡ç¨åç¨®å¤§å°çå·ç©æ¿¾æ³¢å¨åé³è¨è¨èåçè­ä½çºè¼¸å¥ï¼ä¾æä½³åç¹å¾µæåãå¨ç¬¬äºç¨®æ¹æ³ä¸­ï¼ç¨±çºé·ç­æè¨æ¶ - å·ç©ç¥ç¶ (LSCN) æ¨¡åï¼æ­¤å¤ï¼ç¶²è·¯æ¶æ§åæ¬é·ç­æè¨æ¶ (LSTM) ç¶²è·¯åå¡ï¼ä»¥æ¹åæåä¸­çç¹å¾µæåãçµåç±ä¸ç¶­å·ç©å±¤å LSTM åå¡çµæçå¤åä¸¦è¡åæ¯çåµæ°æ¹æ³ï¼æå©æ¼å¨é³è¨è¨èèçä»»åä¸­éæåªç°ççµæãå¯¦é©çµæè­æäºææåºçæ¹æ³åªæ¼æåé²çæè¡ãLSCN ç¶²è·¯å°å¿é³çæ´é«åé¡æºç¢ºåº¦è¶é 96%ãèå¸¸è¦çç¹å¾µæåæ¹æ³ï¼ä¾å¦æ¢ç¾é »çåè­ä¿æ¸ (MFCC) åå°æ³¢è½æï¼ç¸æ¯ï¼æ­¤ç¶²è·¯çæçé¡¯èãå æ­¤ï¼ææåºçæ¹æ³å¨å¿é³çèªååæä¸­é¡¯ç¤ºåºæå¸æççµæï¼ä¸¦ä¸å¨å¿è¡ç®¡ç¾ççè¨ºæ·åæ©ææª¢æ¸¬ä¸­å·ææ½å¨æç¨ã

##### **Spatio-temporal neural distance fields for conditional generative modeling of the heart**
2407.10663v1 by Kristine SÃ¸rensen, Paula Diez, Jan Margeta, Yasmin El Youssef, Michael Pham, Jonas Jalili Pedersen, Tobias KÃ¼hl, Ole de Backer, Klaus Kofoed, Oscar Camara, Rasmus Paulsen

The rhythmic pumping motion of the heart stands as a cornerstone in life, as
it circulates blood to the entire human body through a series of carefully
timed contractions of the individual chambers. Changes in the size, shape and
movement of the chambers can be important markers for cardiac disease and
modeling this in relation to clinical demography or disease is therefore of
interest. Existing methods for spatio-temporal modeling of the human heart
require shape correspondence over time or suffer from large memory
requirements, making it difficult to use for complex anatomies. We introduce a
novel conditional generative model, where the shape and movement is modeled
implicitly in the form of a spatio-temporal neural distance field and
conditioned on clinical demography. The model is based on an auto-decoder
architecture and aims to disentangle the individual variations from that
related to the clinical demography. It is tested on the left atrium (including
the left atrial appendage), where it outperforms current state-of-the-art
methods for anatomical sequence completion and generates synthetic sequences
that realistically mimics the shape and motion of the real left atrium. In
practice, this means we can infer functional measurements from a static image,
generate synthetic populations with specified demography or disease and
investigate how non-imaging clinical data effect the shape and motion of
cardiac anatomies.

æè¦ï¼å¿èæç¯å¥çè·³ååä½æ¯çå½ä¸­çåºç³ï¼å çºå®ééä¸ç³»åä»ç´°è¨æçå®ç¨å¿å®¤æ¶ç¸®ï¼å°è¡æ¶²å¾ªç°å°æ´åèº«é«ãå¿å®¤çå¤§å°ãå½¢çåéåçè®åå¯è½æ¯å¿èç¾ççéè¦æ¨è¨ï¼å æ­¤å°æ­¤é²è¡å»ºæ¨¡ä»¥éè¯è¨åºäººå£çµ±è¨æç¾çï¼å æ­¤å·ææç¾©ãç¾æçæç©ºå»ºæ¨¡æ¹æ³éè¦é¨èæéæ¨ç§»é²è¡å½¢çå°æï¼æéè¦å¤§éçè¨æ¶é«éæ±ï¼éä½¿å¾é£ä»¥ç¨æ¼è¤éçè§£åçµæ§ãæåå¼å¥äºä¸åæ°ç©çæ¢ä»¶çææ¨¡åï¼å¶ä¸­å½¢çåéåä»¥æç©ºç¥ç¶è·é¢å ´çå½¢å¼é±å«å»ºæ¨¡ï¼ä¸¦æ ¹æè¨åºäººå£çµ±è¨é²è¡æ¢ä»¶è¨­å®ãè©²æ¨¡ååºæ¼èªåç·¨ç¢¼å¨æ¶æ§ï¼æ¨å¨è§£éèè¨åºäººå£çµ±è¨ç¸éçåå¥è®ç°ãå®å¨å·¦å¿æ¿ï¼åæ¬å·¦å¿è³ï¼ä¸é²è¡æ¸¬è©¦ï¼å¨è§£ååºåå®ææ¹é¢åªæ¼ç¶åæåé²çæ¹æ³ï¼ä¸¦çæé¼çå°æ¨¡æ¬çå¯¦å·¦å¿æ¿å½¢çåéåçåæåºåãå¯¦éä¸ï¼éæå³èæåå¯ä»¥å¾éæå½±åæ¨æ·åè½æ§æ¸¬éï¼çæå·æç¹å®äººå£çµ±è¨æç¾ççåææç¾¤ï¼ä¸¦èª¿æ¥éå½±åè¨åºè³æå¦ä½å½±é¿å¿èè§£åçµæ§çå½¢çåéåã

##### **TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction**
2407.10510v1 by Xingzhi Zhou, Xin Dong, Chunhao Li, Yuning Bai, Yulong Xu, Ka Chun Cheung, Simon See, Xinpeng Song, Runshun Zhang, Xuezhong Zhou, Nevin L. Zhang

Traditional Chinese medicine (TCM) relies on specific combinations of herbs
in prescriptions to treat symptoms and signs, a practice that spans thousands
of years. Predicting TCM prescriptions presents a fascinating technical
challenge with practical implications. However, this task faces limitations due
to the scarcity of high-quality clinical datasets and the intricate
relationship between symptoms and herbs. To address these issues, we introduce
DigestDS, a new dataset containing practical medical records from experienced
experts in digestive system diseases. We also propose a method, TCM-FTP (TCM
Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)
through supervised fine-tuning on DigestDS. Additionally, we enhance
computational efficiency using a low-rank adaptation technique. TCM-FTP also
incorporates data augmentation by permuting herbs within prescriptions,
capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves
an F1-score of 0.8031, surpassing previous methods significantly. Furthermore,
it demonstrates remarkable accuracy in dosage prediction, achieving a
normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning
perform poorly. Although LLMs have shown capabilities on a wide range of tasks,
this work illustrates the importance of fine-tuning for TCM prescription
prediction, and we have proposed an effective way to do that.

æè¦ï¼ä¸­é«ä¾è³´ç¹å®ä¸­èè¥çµåä¾æ²»çççåå¾µåï¼éé åæ³å·²ææ¸åå¹´çæ­·å²ãé æ¸¬ä¸­é«èæ¹æ¯ä¸åå¼äººå¥åçæè¡ææ°ï¼å·æå¯¦éæç¾©ãç¶èï¼ç±æ¼ç¼ºä¹é«åè³ªçè¨åºæ¸æéä»¥åççèä¸­èè¥ä¹éçè¤ééä¿ï¼éé ä»»åé¢è¨éå¶ãçºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº DigestDSï¼ä¸ååå«æ¶åç³»çµ±ç¾çç¶é©è±å¯å°å®¶å¯¦éçæ­·çæ°æ¸æéãæåéæåºäºä¸ç¨®æ¹æ³ï¼TCM-FTPï¼ä¸­é«å¾®èª¿é è¨ç·´ï¼ï¼ééå¨ DigestDS ä¸é²è¡ç£ç£å¾®èª¿ä¾å©ç¨é è¨ç·´çå¤§èªè¨æ¨¡å (LLM)ãæ­¤å¤ï¼æåä½¿ç¨ä½ç§©é©ææè¡ä¾æé«è¨ç®æçãTCM-FTP éééç½®æèæ¹ä¸­çä¸­èè¥ä¾ç´å¥æ¸ææ´åï¼å©ç¨å®åèé åºç¡éçç¹æ§ãä»¤äººå°è±¡æ·±å»çæ¯ï¼TCM-FTP éå°äº 0.8031 ç F1 åæ¸ï¼é¡¯èè¶è¶äºä»¥åçæ¹æ³ãæ­¤å¤ï¼å®å¨åéé æ¸¬ä¸­è¡¨ç¾åºé¡¯èçæºç¢ºæ§ï¼å¯¦ç¾äº 0.0604 çæ­¸ä¸ååæ¹èª¤å·®ãç¸æ¯ä¹ä¸ï¼æªç¶å¾®èª¿ç LLM è¡¨ç¾ä¸ä½³ãåç®¡ LLM å·²å¨å»£æ³çä»»åä¸­å±ç¾åºè½åï¼ä½éé å·¥ä½èªªæäºå¾®èª¿å°æ¼ä¸­é«èæ¹é æ¸¬çéè¦æ§ï¼èä¸æåæåºäºä¸åææçæ¹æ³ä¾åå°éä¸é»ã

##### **A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT**
2407.10433v1 by Chunshi Wang, Bin Zhao, Shuxue Ding

Cone beam computed tomography (CBCT) is a common way of diagnosing dental
related diseases. Accurate segmentation of 3D tooth is of importance for the
treatment. Although deep learning based methods have achieved convincing
results in medical image processing, they need a large of annotated data for
network training, making it very time-consuming in data collection and
annotation. Besides, domain shift widely existing in the distribution of data
acquired by different devices impacts severely the model generalization. To
resolve the problem, we propose a multi-stage framework for 3D tooth
segmentation in dental CBCT, which achieves the third place in the
"Semi-supervised Teeth Segmentation" 3D (STS-3D) challenge. The experiments on
validation set compared with other semi-supervised segmentation methods further
indicate the validity of our approach.

æè¦ï¼éçåæé»è¦æ·å±¤ææ (CBCT) æ¯ä¸ç¨®å¸¸è¦ççç§ç¸éç¾çè¨ºæ·æ¹å¼ã3D çé½çç²¾ç¢ºåå²å°æ¼æ²»çè³ééè¦ãåç®¡åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³å¨é«å­¸å½±åèçä¸­å·²åå¾ä»¤äººä¿¡æçææï¼ä½å®åéè¦å¤§éçè¨»è§£è³æé²è¡ç¶²è·¯è¨ç·´ï¼éä½¿å¾è³ææ¶éåè¨»è§£éå¸¸èæãæ­¤å¤ï¼å¨ä¸åè£ç½®åå¾çè³æåä½ä¸­å»£æ³å­å¨çé åè½ç§»æå´éå½±é¿æ¨¡åçæ³åè½åãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸åå¤éæ®µæ¶æ§ï¼ç¨æ¼çç§ CBCT ä¸­ç 3D çé½åå²ï¼å¨ãåç£ç£çé½åå²ã3D (STS-3D) ææ°ä¸­ç²å¾ç¬¬ä¸åãèå¶ä»åç£ç£åå²æ¹æ³ç¸æ¯ï¼å¨é©è­éä¸çå¯¦é©é²ä¸æ­¥è­æäºæåæ¹æ³çæææ§ã

##### **Static and multivariate-temporal attentive fusion transformer for readmission risk prediction**
2407.11096v1 by Zhe Sun, Runzhi Li, Jing Wang, Gang Chen, Siyu Yan, Lihong Ma

Background: Accurate short-term readmission prediction of ICU patients is
significant in improving the efficiency of resource assignment by assisting
physicians in making discharge decisions. Clinically, both individual static
static and multivariate temporal data collected from ICU monitors play critical
roles in short-term readmission prediction. Informative static and multivariate
temporal feature representation capturing and fusion present challenges for
accurate readmission prediction. Methods:We propose a novel static and
multivariate-temporal attentive fusion transformer (SMTAFormer) to predict
short-term readmission of ICU patients by fully leveraging the potential of
demographic and dynamic temporal data. In SMTAFormer, we first apply an MLP
network and a temporal transformer network to learn useful static and temporal
feature representations, respectively. Then, the well-designed static and
multivariate temporal feature fusion module is applied to fuse static and
temporal feature representations by modeling intra-correlation among
multivariate temporal features and constructing inter-correlation between
static and multivariate temporal features. Results: We construct a readmission
risk assessment (RRA) dataset based on the MIMIC-III dataset. The extensive
experiments show that SMTAFormer outperforms advanced methods, in which the
accuracy of our proposed method is up to 86.6%, and the area under the receiver
operating characteristic curve (AUC) is up to 0.717. Conclusion: Our proposed
SMTAFormer can efficiently capture and fuse static and multivariate temporal
feature representations. The results show that SMTAFormer significantly
improves the short-term readmission prediction performance of ICU patients
through comparisons to strong baselines.

æè¦ï¼<paragraph>èæ¯ï¼ç²¾æºç­æéè¿é æ¸¬éçå è­·çæ¿ï¼ICUï¼çäººå°æ¼æåè³æºåéæçè³ééè¦ï¼è½åå©é«å¸«ååºåºé¢æ±ºç­ãè¨åºä¸ï¼å¾ ICU ç£æ¸¬å¨æ¶éå°çåå¥éæè³æåå¤è®éæéè³æå¨ç­æéè¿é æ¸¬ä¸­æ®æ¼ééµè§è²ãæ·ååèåææç¾©çéæåå¤è®éæéç¹å¾µè¡¨å¾µå°æ¼ç²¾æºéè¿é æ¸¬æ§æææ°ãæ¹æ³ï¼æåæåºä¸åæ°ç©çéæåå¤è®éæéæ³¨æåèåTransformerï¼SMTAFormerï¼ï¼èç±ååå©ç¨äººå£çµ±è¨ååææéè³æçæ½åï¼ä¾é æ¸¬ ICU çäººçç­æéè¿ãå¨ SMTAFormer ä¸­ï¼æåé¦åæç¨ä¸å MLP ç¶²è·¯åä¸åæéTransformerç¶²è·¯ï¼åå¥å­¸ç¿æç¨çéæåæéç¹å¾µè¡¨å¾µãç¶å¾ï¼æç¨ç²¾å¿è¨­è¨çéæåå¤è®éæéç¹å¾µèåæ¨¡çµï¼èç±å»ºæ¨¡å¤è®éæéç¹å¾µä¹éçå§é¨ç¸éæ§ï¼ä»¥åå»ºæ§éæåå¤è®éæéç¹å¾µä¹éçç¸äºéè¯æ§ï¼ä¾èåéæåæéç¹å¾µè¡¨å¾µãçµæï¼æåæ ¹æ MIMIC-III è³æéå»ºæ§ä¸åéè¿é¢¨éªè©ä¼°ï¼RRAï¼è³æéãå»£æ³çå¯¦é©é¡¯ç¤ºï¼SMTAFormer åªæ¼é²éæ¹æ³ï¼å¶ä¸­æåæåºçæ¹æ³çæºç¢ºåº¦é«é 86.6%ï¼èåè©¦èå·¥ä½ç¹æ§æ²ç·ï¼AUCï¼ä¸çé¢ç©é«é 0.717ãçµè«ï¼æåæåºç SMTAFormer è½æææ·ååèåéæåå¤è®éæéç¹å¾µè¡¨å¾µãçµæé¡¯ç¤ºï¼SMTAFormer èç±èå¼·å¤§çåºç·æ¯è¼ï¼é¡¯èæå ICU çäººçç­æéè¿é æ¸¬æè½ã</paragraph>

##### **Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence**
2407.10359v1 by Yintong Zhang, Jason A. Yoder

Recently, Cartesian Genetic Programming has been used to evolve developmental
programs to guide the formation of artificial neural networks (ANNs). This
approach has demonstrated success in enabling ANNs to perform multiple tasks
while avoiding catastrophic forgetting. One unique aspect of this approach is
the use of separate developmental programs evolved to regulate the development
of separate soma and dendrite units. An opportunity afforded by this approach
is the ability to incorporate Activity Dependence (AD) into the model such that
environmental feedback can help to regulate the behavior of each type of unit.
Previous work has shown a limited version of AD (influencing neural bias) to
provide marginal improvements over non-AD ANNs. In this work, we present
promising results from new extensions to AD. Specifically, we demonstrate a
more significant improvement via AD on new neural parameters including health
and position, as well as a combination of all of these along with bias. We
report on the implications of this work and suggest several promising
directions for future work.

æè¦ï¼æè¿ï¼ç¬å¡å°éä¼ è§åå·²è¢«ç¨äºè¿ååè²ç¨åºï¼ä»¥æå¯¼äººå·¥ç¥ç»ç½ç» (ANN) çå½¢æãè¿ç§æ¹æ³å·²è¯æè½å¤è®© ANN æ§è¡å¤é¡¹ä»»å¡ï¼åæ¶é¿åç¾é¾æ§éå¿ãè¿ç§æ¹æ³çä¸ä¸ªç¬ç¹æ¹é¢æ¯ä½¿ç¨åç¬çåå±ç¨åºæ¥è°èåç¬çèº¯ä½åæ çªååçåå±ãè¿ç§æ¹æ³æä¾äºä¸ä¸ªæºä¼ï¼å³è½å¤å°æ´»å¨ä¾èµæ§ (AD) çº³å¥æ¨¡åï¼ä»¥ä¾¿ç¯å¢åé¦å¯ä»¥å¸®å©è°èæ¯ç§ç±»åçååçè¡ä¸ºãä»¥åçå·¥ä½å·²ç»å±ç¤ºäº AD çä¸ä¸ªæéçæ¬ï¼å½±åç¥ç»åç½®ï¼ï¼ä»¥æä¾å¯¹é AD ANN çè¾¹éæ¹è¿ãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬å±ç¤ºäº AD æ°æ©å±çä»¤äººé¼èçç»æãå·ä½æ¥è¯´ï¼æä»¬éè¿ AD å¨æ°çç¥ç»åæ°ï¼åæ¬å¥åº·åä½ç½®ï¼ä»¥åææè¿äºåæ°ä¸åç½®çç»åä¸å±ç¤ºäºæ´æ¾ççæ¹è¿ãæä»¬æ¥åäºè¿é¡¹å·¥ä½çå½±åï¼å¹¶ä¸ºæªæ¥çå·¥ä½æåºäºå ä¸ªæå¸æçæ¹åã

##### **Learning Unlabeled Clients Divergence via Anchor Model Aggregation for Federated Semi-supervised Learning**
2407.10327v1 by Marawan Elbatel, Hualiang Wang, Jixiang Chen, Hao Wang, Xiaomeng Li

Federated semi-supervised learning (FedSemi) refers to scenarios where there
may be clients with fully labeled data, clients with partially labeled, and
even fully unlabeled clients while preserving data privacy. However, challenges
arise from client drift due to undefined heterogeneous class distributions and
erroneous pseudo-labels. Existing FedSemi methods typically fail to aggregate
models from unlabeled clients due to their inherent unreliability, thus
overlooking unique information from their heterogeneous data distribution,
leading to sub-optimal results. In this paper, we enable unlabeled client
aggregation through SemiAnAgg, a novel Semi-supervised Anchor-Based federated
Aggregation. SemiAnAgg learns unlabeled client contributions via an anchor
model, effectively harnessing their informative value. Our key idea is that by
feeding local client data to the same global model and the same consistently
initialized anchor model (i.e., random model), we can measure the importance of
each unlabeled client accordingly. Extensive experiments demonstrate that
SemiAnAgg achieves new state-of-the-art results on four widely used FedSemi
benchmarks, leading to substantial performance improvements: a 9% increase in
accuracy on CIFAR-100 and a 7.6% improvement in recall on the medical dataset
ISIC-18, compared with prior state-of-the-art. Code is available at:
https://github.com/xmed-lab/SemiAnAgg.

æè¦ï¼è¯é¦åç£ç£å­¸ç¿ (FedSemi) æçæ¯å¨ä¿è­·è³æé±ç§çåæï¼å¯è½å­å¨å·æå®å¨æ¨ç±¤è³æçå®¢æ¶ç«¯ãå·æé¨åæ¨ç±¤çå®¢æ¶ç«¯ï¼çè³å®å¨æ²ææ¨ç±¤çå®¢æ¶ç«¯çææ³ãç¶èï¼ç±æ¼æªå®ç¾©çç°è³ªé¡å¥åä½åé¯èª¤çå½æ¨ç±¤ï¼å®¢æ¶ç«¯æ¼ç§»å¸¶ä¾äºææ°ãç¾æç FedSemi æ¹æ³éå¸¸ç¡æ³å½ç¸½ä¾èªæªæ¨ç±¤å®¢æ¶ç«¯çæ¨¡åï¼å çºå®åæ¬è³ªä¸ä¸å¯é ï¼å æ­¤å¿½ç¥äºå¶ç°è³ªè³æåä½ä¸­çç¨ç¹è³è¨ï¼å°è´æ¬¡ä½³çµæãå¨æ¬æä¸­ï¼æåéé SemiAnAggï¼ä¸ç¨®æ°ç©çåç£ç£é¨å®å¼è¯é¦èåï¼åç¨æªæ¨ç±¤å®¢æ¶ç«¯èåãSemiAnAgg ééé¨å®æ¨¡åå­¸ç¿æªæ¨ç±¤å®¢æ¶ç«¯è²¢ç»ï¼ææå©ç¨å¶è³è¨å¹å¼ãæåçééµæ§æ³æ¯ï¼ééå°æ¬å°å®¢æ¶ç«¯è³ææä¾çµ¦ç¸åçå¨çæ¨¡ååç¸åä¸è´åå§åçé¨å®æ¨¡åï¼å³é¨æ©æ¨¡åï¼ï¼æåå¯ä»¥ç¸æå°è¡¡éæ¯åæªæ¨ç±¤å®¢æ¶ç«¯çéè¦æ§ãå»£æ³çå¯¦é©è­æ SemiAnAgg å¨ååå»£æ³ä½¿ç¨ç FedSemi åºæºä¸ç²å¾äºæ°çæåé²çµæï¼å¸¶ä¾äºé¡¯èçæè½æåï¼èååçæåé²æè¡ç¸æ¯ï¼CIFAR-100 çæºç¢ºåº¦æé«äº 9%ï¼é«çè³æé ISIC-18 çå¬åçæé«äº 7.6%ãç¨å¼ç¢¼å¯å¨ https://github.com/xmed-lab/SemiAnAgg åå¾ã

##### **Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine**
2407.10086v1 by Omid Rohanian, Mohammadmahdi Nouriborji, Olena Seminog, Rodrigo Furst, Thomas Mendy, Shanthi Levanita, Zaharat Kadri-Alab, Nusrat Jabin, Daniela Toale, Georgina Humphreys, Emilia Antonio, Adrian Bucher, Alice Norton, David A. Clifton

This paper introduces the Pandemic PACT Advanced Categorisation Engine
(PPACE) along with its associated dataset. PPACE is a fine-tuned model
developed to automatically classify research abstracts from funded biomedical
projects according to WHO-aligned research priorities. This task is crucial for
monitoring research trends and identifying gaps in global health preparedness
and response. Our approach builds on human-annotated projects, which are
allocated one or more categories from a predefined list. A large language model
is then used to generate `rationales' explaining the reasoning behind these
annotations. This augmented data, comprising expert annotations and rationales,
is subsequently used to fine-tune a smaller, more efficient model. Developed as
part of the Pandemic PACT project, which aims to track and analyse research
funding and clinical evidence for a wide range of diseases with outbreak
potential, PPACE supports informed decision-making by research funders,
policymakers, and independent researchers. We introduce and release both the
trained model and the instruction-based dataset used for its training. Our
evaluation shows that PPACE significantly outperforms its baselines. The
release of PPACE and its associated dataset offers valuable resources for
researchers in multilabel biomedical document classification and supports
advancements in aligning biomedical research with key global health priorities.

æè¦ï¼æ¬æä»ç´¹äºç«æ PACT é²éåé¡å¼æ (PPACE) åå¶ç¸éè³æéãPPACE æ¯ä¸åç¶éå¾®èª¿çæ¨¡åï¼éç¼ç¨æ¼æ ¹æ WHO èªå¯çç ç©¶åªåé åºèªååé¡ç²å¾è³å©ççç©é«å­¸å°æ¡çç ç©¶æè¦ãéé ä»»åå°æ¼ç£æ§ç ç©¶è¶¨å¢åæ¾åºå¨çå¥åº·æºååæè®çå·®è·è³ééè¦ãæåçåæ³å»ºç«å¨äººå·¥æ¨è¨»çå°æ¡ä¸ï¼éäºå°æ¡å¾é åå®ç¾©çæ¸å®ä¸­åéä¸åæå¤åé¡å¥ãç¶å¾ä½¿ç¨å¤§åèªè¨æ¨¡åä¾ç¢çãä¾æãï¼èªªæéäºæ¨è¨»èå¾çæ¨çãéäºæ´å¢è³æåå«å°å®¶æ¨è¨»åä¾æï¼é¨å¾ç¨æ¼å¾®èª¿ä¸åè¼å°ãæ´ææççæ¨¡åãPPACE æ¯ä½çºç«æ PACT å°æ¡çä¸é¨åéç¼çï¼è©²å°æ¡æ¨å¨è¿½è¹¤ååæåç¨®å·æçç¼æ½åçç¾ççç ç©¶è³éåè¨åºè­æï¼ä¸¦æ¯æç ç©¶è³å©èãæ¿ç­å¶å®èåç¨ç«ç ç©¶äººå¡ååºææºçæ±ºç­ãæåä»ç´¹ä¸¦éåºè¨ç·´å¥½çæ¨¡ååç¨æ¼è¨ç·´çåºæ¼æä»¤çè³æéãæåçè©ä¼°é¡¯ç¤ºï¼PPACE çè¡¨ç¾é¡¯èåªæ¼å¶åºç·ãPPACE åå¶ç¸éè³æéçéåºçºå¤æ¨ç±¤çç©é«å­¸æä»¶åé¡çç ç©¶äººå¡æä¾äºå¯¶è²´çè³æºï¼ä¸¦æ¯æå°çç©é«å­¸ç ç©¶èå¨çå¥åº·ééµåªåé åºç¸ç¬¦çé²å±ã

##### **Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation**
2407.10021v1 by Kriti Bhattarai, Inez Y. Oh, Zachary B. Abrams, Albert M. Lai

Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.

æè¦ï¼çæå¼é¢è®­ç»è½¬æ¢å¨ (GPT) æ¨¡åå¨ä¸´åºå®ä½åå³ç³»æ½åä»»å¡ä¸­å±ç°åºæ½åï¼å ä¸ºå®ä»¬å·æç²¾ç¡®æ½ååä¸ä¸æçè§£è½åãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬è¿ä¸æ­¥å©ç¨ç»ä¸å»å­¦è¯­è¨ç³»ç» (UMLS) ç¥è¯åºæ¥åç¡®è¯å«å»å­¦æ¦å¿µï¼å¹¶å¨ææ¡£çº§å«æ¹è¿ä¸´åºå®ä½åå³ç³»æ½åãæä»¬çæ¡æ¶éæ©ä¸ææ¬ç¸å³ç UMLS æ¦å¿µï¼å¹¶å°å®ä»¬ä¸æç¤ºç¸ç»åï¼ä»¥æå¯¼è¯­è¨æ¨¡åæ½åå®ä½ãæä»¬çå®éªè¡¨æï¼ä¸ä¸å©ç¨ UMLS çéç¨è¯­è¨æ¨¡åä¸çå°éæ½åä»»å¡ç¸æ¯ï¼è¿ç§åå§æ¦å¿µæ å°åå¨æç¤ºä¸­åå«è¿äºæ å°æ¦å¿µæ¹è¿äºæ½åç»æãæ­¤å¤ï¼æä»¬çç»æè¡¨æï¼è¿ç§æ¹æ³æ¯æ åçæ£ç´¢å¢å¼ºçæ (RAG) ææ¯æ´ææï¼å¶ä¸­æ£ç´¢å°çæ°æ®ä¸æç¤ºåµå¥è¿è¡æ¯è¾ä»¥çæç»æãæ»ä½èè¨ï¼æä»¬åç°å° UMLS æ¦å¿µä¸ GPT æ¨¡åéæå¯ä»¥æ¾èæ¹åå®ä½åå³ç³»è¯å«ï¼ä¼äºåºçº¿å RAG æ¨¡åãéè¿å° UMLS ç­åºäºç¥è¯çæ¹æ³çç²¾ç¡®æ¦å¿µæ å°è½åä¸ GPT çä¸ä¸æçè§£è½åç¸ç»åï¼æä»¬çæ¹æ³çªåºäºè¿äºæ¹æ³å¨å»çä¿å¥ç­ä¸ä¸é¢åçæ½åã

##### **Causality extraction from medical text using Large Language Models (LLMs)**
2407.10020v1 by Seethalakshmi Gopalakrishnan, Luciana Garbayo, Wlodek Zadrozny

This study explores the potential of natural language models, including large
language models, to extract causal relations from medical texts, specifically
from Clinical Practice Guidelines (CPGs). The outcomes causality extraction
from Clinical Practice Guidelines for gestational diabetes are presented,
marking a first in the field. We report on a set of experiments using variants
of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),
namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better
than other models, including the Large Language Models, with an average
F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less
consistency. We also release the code and an annotated a corpus of causal
statements within the Clinical Practice Guidelines for gestational diabetes.

æè¦ï¼æ¬ç ç©¶æ¢è¨èªç¶èªè¨æ¨¡åï¼åæ¬å¤§åèªè¨æ¨¡åï¼å¾é«å­¸ææ¬ä¸­èåå æéä¿çå¯è½æ§ï¼ç¹å¥æ¯å¾è¨åºå¯¦åæå (CPG) ä¸­ãç ç©¶ææçºå¦å¨ ç³å°¿ççè¨åºå¯¦åæåä¸­å æéä¿èåï¼çºè©²é åé¦ä¾ãæåå ±åäºä¸çµä½¿ç¨ BERT è®é« (BioBERTãDistilBERT å BERT) åä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çå¯¦é©ï¼å³ GPT-4 å LLAMA2ãæåçå¯¦é©é¡¯ç¤ºï¼BioBERT çè¡¨ç¾åªæ¼å¶ä»æ¨¡åï¼åæ¬å¤§åèªè¨æ¨¡åï¼å¹³å F1 åæ¸çº 0.72ãGPT-4 å LLAMA2 ççµæé¡¯ç¤ºåºé¡ä¼¼çè¡¨ç¾ï¼ä½ä¸è´æ§è¼ä½ãæåä¹éåºäºç¨å¼ç¢¼åå¦å¨ ç³å°¿çè¨åºå¯¦åæåä¸­å æé³è¿°çæ¨è¨»èªæåº«ã

##### **Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient Multi-Label Skin Lesion Classification**
2407.09999v1 by Peng Tang, Tobias Lasser

Existing multi-modal approaches primarily focus on enhancing multi-label skin
lesion classification performance through advanced fusion modules, often
neglecting the associated rise in parameters. In clinical settings, both
clinical and dermoscopy images are captured for diagnosis; however, dermoscopy
images exhibit more crucial visual features for multi-label skin lesion
classification. Motivated by this observation, we introduce a novel asymmetric
multi-modal fusion method in this paper for efficient multi-label skin lesion
classification. Our fusion method incorporates two innovative schemes. Firstly,
we validate the effectiveness of our asymmetric fusion structure. It employs a
light and simple network for clinical images and a heavier, more complex one
for dermoscopy images, resulting in significant parameter savings compared to
the symmetric fusion structure using two identical networks for both
modalities. Secondly, in contrast to previous approaches using mutual attention
modules for interaction between image modalities, we propose an asymmetric
attention module. This module solely leverages clinical image information to
enhance dermoscopy image features, considering clinical images as supplementary
information in our pipeline. We conduct the extensive experiments on the
seven-point checklist dataset. Results demonstrate the generality of our
proposed method for both networks and Transformer structures, showcasing its
superiority over existing methods We will make our code publicly available.

æè¦ï¼ç¾æçå¤æ¨¡å¼æ¹æ³ä¸»è¦å°æ³¨æ¼ééåé²çèåæ¨¡çµä¾å¢å¼·å¤æ¨ç±¤ç®èçè®åé¡æè½ï¼å¾å¾å¿½ç¥äºç¸éåæ¸çå¢å ãå¨è¨åºç°å¢ä¸­ï¼è¨åºä¸åç®èé¡å½±åé½æè¢«æ·åç¨æ¼è¨ºæ·ï¼ç¶èï¼ç®èé¡å½±åå±ç¾åºæ´éè¦çè¦è¦ºç¹å¾µï¼ç¨æ¼å¤æ¨ç±¤ç®èçè®åé¡ãåæ­¤è§å¯çµæåç¼ï¼æåå¨æ¬æä¸­ä»ç´¹ä¸ç¨®æ°ç©çä¸å°ç¨±å¤æ¨¡å¼èåæ¹æ³ï¼ç¨æ¼ææçå¤æ¨ç±¤ç®èçè®åé¡ãæåçèåæ¹æ³åå«å©ååµæ°çæ¹æ¡ãé¦åï¼æåé©è­äºæåçä¸å°ç¨±èåçµæ§çæææ§ãå®æ¡ç¨ä¸åè¼éä¸ç°¡å®çç¶²è·¯ç¨æ¼è¨åºå½±åï¼ä»¥åä¸åè¼éä¸è¤éçç¶²è·¯ç¨æ¼ç®èé¡å½±åï¼èä½¿ç¨å©åç¸åçç¶²è·¯ç¨æ¼å©ç¨®æ¨¡å¼çå°ç¨±èåçµæ§ç¸æ¯ï¼éæç¯çå¤§éçåæ¸ãå¶æ¬¡ï¼èååä½¿ç¨ç¸äºæ³¨æåæ¨¡çµç¨æ¼å½±åæ¨¡å¼ä¹éäºåçæ¹æ³ç¸åï¼æåæåºäºä¸åä¸å°ç¨±æ³¨æåæ¨¡çµãéåæ¨¡çµåå©ç¨è¨åºå½±åè³è¨ä¾å¢å¼·ç®èé¡å½±åç¹å¾µï¼å°è¨åºå½±åè¦çºæåæµç¨ä¸­çè£åè³è¨ãæåå¨ä¸é»æ ¸å°æ¸å®è³æéä¸é²è¡äºå»£æ³çå¯¦é©ãçµæè­æäºæåæåºçæ¹æ³å°ç¶²è·¯å Transformer çµæ§çæ®éæ§ï¼å±ç¤ºäºå®åªæ¼ç¾ææ¹æ³çåªè¶æ§ãæåå°å¬éæåçç¨å¼ç¢¼ã

##### **Evaluating the Impact of Different Quantum Kernels on the Classification Performance of Support Vector Machine Algorithm: A Medical Dataset Application**
2407.09930v1 by Emine Akpinar, Sardar M. N. Islam, Murat Oduncuoglu

The support vector machine algorithm with a quantum kernel estimator
(QSVM-Kernel), as a leading example of a quantum machine learning technique,
has undergone significant advancements. Nevertheless, its integration with
classical data presents unique challenges. While quantum computers primarily
interact with data in quantum states, embedding classical data into quantum
states using feature mapping techniques is essential for leveraging quantum
algorithms Despite the recognized importance of feature mapping, its specific
impact on data classification outcomes remains largely unexplored. This study
addresses this gap by comprehensively assessing the effects of various feature
mapping methods on classification results, taking medical data analysis as a
case study. In this study, the QSVM-Kernel method was applied to classification
problems in two different and publicly available medical datasets, namely, the
Wisconsin Breast Cancer (original) and The Cancer Genome Atlas (TCGA) Glioma
datasets. In the QSVM-Kernel algorithm, quantum kernel matrices obtained from 9
different quantum feature maps were used. Thus, the effects of these quantum
feature maps on the classification results of the QSVM-Kernel algorithm were
examined in terms of both classifier performance and total execution time. As a
result, in the Wisconsin Breast Cancer (original) and TCGA Glioma datasets,
when Rx and Ry rotational gates were used, respectively, as feature maps in the
QSVM-Kernel algorithm, the best classification performances were achieved both
in terms of classification performance and total execution time. The
contributions of this study are that (1) it highlights the significant impact
of feature mapping techniques on medical data classification outcomes using the
QSVM-Kernel algorithm, and (2) it also guides undertaking research for improved
QSVM classification performance.

æè¦ï¼<paragraph>ä»¥éå­æ ¸ä¼°è¨å¨çºä¸»çéå­æ©å¨å­¸ç¿æè¡ï¼æ¯æ´åéæ©æ¼ç®æ³ï¼QSVM-Kernelï¼ï¼å·²ç¶æé¡¯èçé²å±ãåç®¡å¦æ­¤ï¼å®èå³çµ±è³æçæ´åï¼åç¾äºç¨ç¹çææ°ãéç¶éå­é»è¦ä¸»è¦èéå­çæä¸­çè³æäºåï¼ä½ä½¿ç¨ç¹å¾µå°ææè¡å°å³çµ±è³æåµå¥éå­çæï¼å°æ¼å©ç¨éå­æ¼ç®æ³è³ééè¦ãåç®¡ç¹å¾µå°æçéè¦æ§ç²å¾èªå¯ï¼ä½å®å°è³æåé¡çµæçå·é«å½±é¿ï¼å¨å¾å¤§ç¨åº¦ä¸ä»æªè¢«æ¢è¨ãæ¬ç ç©¶ééå¨é¢è©ä¼°åç¨®ç¹å¾µå°ææ¹æ³å°åé¡çµæçå½±é¿ï¼ä»¥é«çè³æåæçºæ¡ä¾ç ç©¶ï¼ä¾è§£æ±ºéåå·®è·ãå¨æ¬ç ç©¶ä¸­ï¼QSVM-Kernel æ¹æ³è¢«æç¨æ¼å©åä¸åä¸å¬éçé«çè³æéä¸­çåé¡åé¡ï¼å³å¨æ¯åº·è¾ä¹³çï¼åå§ï¼åççåºå çµåè­ï¼TCGAï¼ç¥ç¶è è³ªç¤è³æéãå¨ QSVM-Kernel æ¼ç®æ³ä¸­ï¼ä½¿ç¨äºå¾ 9 åä¸åçéå­ç¹å¾µå°æä¸­ç²å¾çéå­æ ¸ç©é£ãå æ­¤ï¼éäºéå­ç¹å¾µå°æå° QSVM-Kernel æ¼ç®æ³åé¡çµæçå½±é¿ï¼å¨åé¡å¨æè½åç¸½å·è¡æéæ¹é¢é½å¾å°äºæª¢é©ãçµæï¼å¨å¨æ¯åº·è¾ä¹³çï¼åå§ï¼å TCGA ç¥ç¶è è³ªç¤è³æéä¸­ï¼ç¶ Rx å Ry æè½éåå¥ç¨ä½ QSVM-Kernel æ¼ç®æ³ä¸­çç¹å¾µå°ææï¼å¨åé¡æè½åç¸½å·è¡æéæ¹é¢é½éå°äºæä½³åé¡æè½ãæ¬ç ç©¶çè²¢ç»å¨æ¼ï¼ï¼1ï¼å®å¼·èª¿äºç¹å¾µå°ææè¡å°ä½¿ç¨ QSVM-Kernel æ¼ç®æ³çé«çè³æåé¡çµæçé¡¯èå½±é¿ï¼ä¸¦ä¸ï¼2ï¼å®ä¹æå°é²è¡ç ç©¶ä»¥æ¹å QSVM åé¡æè½ã</paragraph>

##### **Enhancing Semantic Segmentation with Adaptive Focal Loss: A Novel Approach**
2407.09828v1 by Md Rakibul Islam, Riad Hassan, Abdullah Nazib, Kien Nguyen, Clinton Fookes, Md Zahidul Islam

Deep learning has achieved outstanding accuracy in medical image
segmentation, particularly for objects like organs or tumors with smooth
boundaries or large sizes. Whereas, it encounters significant difficulties with
objects that have zigzag boundaries or are small in size, leading to a notable
decrease in segmentation effectiveness. In this context, using a loss function
that incorporates smoothness and volume information into a model's predictions
offers a promising solution to these shortcomings. In this work, we introduce
an Adaptive Focal Loss (A-FL) function designed to mitigate class imbalance by
down-weighting the loss for easy examples that results in up-weighting the loss
for hard examples and giving greater emphasis to challenging examples, such as
small and irregularly shaped objects. The proposed A-FL involves dynamically
adjusting a focusing parameter based on an object's surface smoothness, size
information, and adjusting the class balancing parameter based on the ratio of
targeted area to total area in an image. We evaluated the performance of the
A-FL using ResNet50-encoded U-Net architecture on the Picai 2022 and BraTS 2018
datasets. On the Picai 2022 dataset, the A-FL achieved an Intersection over
Union (IoU) of 0.696 and a Dice Similarity Coefficient (DSC) of 0.769,
outperforming the regular Focal Loss (FL) by 5.5% and 5.4% respectively. It
also surpassed the best baseline Dice-Focal by 2.0% and 1.2%. On the BraTS 2018
dataset, A-FL achieved an IoU of 0.883 and a DSC of 0.931. The comparative
studies show that the proposed A-FL function surpasses conventional methods,
including Dice Loss, Focal Loss, and their hybrid variants, in IoU, DSC,
Sensitivity, and Specificity metrics. This work highlights A-FL's potential to
improve deep learning models for segmenting clinically significant regions in
medical images, leading to more precise and reliable diagnostic tools.

æè¦ï¼æ·±åº¦å­¸ç¿å¨é«å­¸å½±ååå²æ¹é¢åå¾äºååºçæºç¢ºæ§ï¼ç¹å¥æ¯å°æ¼å·æå¹³æ»éçæå¤§å°ºå¯¸çå¨å®æè«ç¤ç­ç©é«ãç¶èï¼å°æ¼å·ææ²æéçæå°ºå¯¸å°çç©é«ï¼å®æéå°å¾å¤§çå°é£ï¼å°è´åå²ææé¡¯èä¸éãå¨æ­¤èæ¯ä¸ï¼ä½¿ç¨å°å¹³æ»åº¦åé«ç©è³è¨ç´å¥æ¨¡åé æ¸¬çæå¤±å½æ¸çºéäºç¼ºé»æä¾äºä¸åæå¸æçè§£æ±ºæ¹æ¡ãå¨éé å·¥ä½ä¸­ï¼æåå¼å¥äºä¸åèªé©æç¦é»æå¤± (A-FL) å½æ¸ï¼æ¨å¨éééä½ææ¼ç¯ä¾çæå¤±ä¾æ¸è¼é¡å¥å¤±è¡¡ï¼å¾èå¢å å°é£ç¯ä¾çæå¤±ï¼ä¸¦æ´å å¼·èª¿å·æææ°æ§çç¯ä¾ï¼ä¾å¦å°ä¸å½¢çä¸è¦åçç©é«ãææåºç A-FL æ¶åæ ¹æç©é«çè¡¨é¢å¹³æ»åº¦ãå°ºå¯¸è³è¨åæèª¿æ´èç¦åæ¸ï¼ä¸¦æ ¹æååä¸­ç®æ¨ååèç¸½ååçæ¯çèª¿æ´é¡å¥å¹³è¡¡åæ¸ãæåä½¿ç¨ ResNet50 ç·¨ç¢¼ç U-Net æ¶æ§å¨ Picai 2022 å BraTS 2018 è³æéä¸è©ä¼°äº A-FL çæè½ãå¨ Picai 2022 è³æéä¸ï¼A-FL çäº¤éæ¯è¯é (IoU) çº 0.696ï¼éª°å­ç¸ä¼¼æ§ä¿æ¸ (DSC) çº 0.769ï¼åå¥åªæ¼å¸¸è¦ç¦é»æå¤± (FL) 5.5% å 5.4%ãå®éè¶è¶äºæä½³åºæº Dice-Focal 2.0% å 1.2%ãå¨ BraTS 2018 è³æéä¸ï¼A-FL ç IoU çº 0.883ï¼DSC çº 0.931ãæ¯è¼ç ç©¶è¡¨æï¼ææåºç A-FL å½æ¸å¨ IoUãDSCãæææ§åç¹ç°æ§ææ¨ä¸åªæ¼å³çµ±æ¹æ³ï¼åæ¬ Dice æå¤±ãç¦é»æå¤±åå¶æ··åè®é«ãéé å·¥ä½çªåºäº A-FL å¨åå²é«å­¸å½±åä¸­å·æè¨åºæç¾©çååä»¥æ¹åæ·±åº¦å­¸ç¿æ¨¡åçæ½åï¼å¾èç¢çæ´ç²¾ç¢ºãæ´å¯é çè¨ºæ·å·¥å·ã

##### **Towards Personalised Patient Risk Prediction Using Temporal Hospital Data Trajectories**
2407.09373v1 by Thea Barnes, Enrico Werner, Jeffrey N. Clark, Raul Santos-Rodriguez

Quantifying a patient's health status provides clinicians with insight into
patient risk, and the ability to better triage and manage resources. Early
Warning Scores (EWS) are widely deployed to measure overall health status, and
risk of adverse outcomes, in hospital patients. However, current EWS are
limited both by their lack of personalisation and use of static observations.
We propose a pipeline that groups intensive care unit patients by the
trajectories of observations data throughout their stay as a basis for the
development of personalised risk predictions. Feature importance is considered
to provide model explainability. Using the MIMIC-IV dataset, six clusters were
identified, capturing differences in disease codes, observations, lengths of
admissions and outcomes. Applying the pipeline to data from just the first four
hours of each ICU stay assigns the majority of patients to the same cluster as
when the entire stay duration is considered. In-hospital mortality prediction
models trained on individual clusters had higher F1 score performance in five
of the six clusters when compared against the unclustered patient cohort. The
pipeline could form the basis of a clinical decision support tool, working to
improve the clinical characterisation of risk groups and the early detection of
patient deterioration.

æè¦ï¼éåæ£èçå¥åº·ç¶åµå¯è®©ä¸´åºå»çæ·±å¥äºè§£æ£èé£é©ï¼å¹¶è½æ´å¥½å°å¯¹èµæºè¿è¡åç±»åç®¡çãæ©æé¢è­¦è¯å (EWS) è¢«å¹¿æ³ç¨äºè¡¡éæ´ä½å¥åº·ç¶åµåä½é¢æ£èçä¸è¯åæé£é©ãç¶èï¼å½åç EWS åéäºå¶ç¼ºä¹ä¸ªæ§ååä½¿ç¨éæè§å¯ãæä»¬æåºäºä¸ä¸ªç®¡éï¼è¯¥ç®¡éæ ¹æ®æ£èå¨æ´ä¸ªä½é¢æé´çè§å¯æ°æ®è½¨è¿¹å¯¹éççæ¤çæ¿æ£èè¿è¡åç»ï¼ä½ä¸ºå¶å®ä¸ªæ§åé£é©é¢æµçåºç¡ãç¹å¾éè¦æ§è¢«èèä¸ºæä¾æ¨¡åå¯è§£éæ§ãä½¿ç¨ MIMIC-IV æ°æ®éï¼è¯å«åºå­ä¸ªéç¾¤ï¼ææç¾çä»£ç ãè§å¯ãå¥é¢æ¶é´åç»æçå·®å¼ãå°ç®¡éåºç¨äºæ¯ä¸ª ICU ä½é¢çååä¸ªå°æ¶çæ°æ®æ¶ï¼å°å¤§å¤æ°æ£èåéå°ä¸èèæ´ä¸ªä½é¢æ¶é´æ¶ç¸åçéç¾¤ãå¨äºä¸ªéç¾¤ä¸­ï¼éå¯¹åä¸ªéç¾¤è®­ç»çé¢åæ­»äº¡çé¢æµæ¨¡åä¸æªåç»æ£èéåç¸æ¯å·ææ´é«ç F1 åæ°è¡¨ç°ãè¯¥ç®¡éå¯ä»¥å½¢æä¸´åºå³ç­æ¯æå·¥å·çåºç¡ï¼ç¨äºæ¹åé£é©ç»çä¸´åºè¡¨å¾åæ£èæ¶åçæ©ææ£æµã

##### **Enhancing Depressive Post Detection in Bangla: A Comparative Study of TF-IDF, BERT and FastText Embeddings**
2407.09187v1 by Saad Ahmed Sazan, Mahdi H. Miraz, A B M Muntasir Rahman

Due to massive adoption of social media, detection of users' depression
through social media analytics bears significant importance, particularly for
underrepresented languages, such as Bangla. This study introduces a
well-grounded approach to identify depressive social media posts in Bangla, by
employing advanced natural language processing techniques. The dataset used in
this work, annotated by domain experts, includes both depressive and
non-depressive posts, ensuring high-quality data for model training and
evaluation. To address the prevalent issue of class imbalance, we utilised
random oversampling for the minority class, thereby enhancing the model's
ability to accurately detect depressive posts. We explored various numerical
representation techniques, including Term Frequency-Inverse Document Frequency
(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)
embedding and FastText embedding, by integrating them with a deep
learning-based Convolutional Neural Network-Bidirectional Long Short-Term
Memory (CNN-BiLSTM) model. The results obtained through extensive
experimentation, indicate that the BERT approach performed better the others,
achieving a F1-score of 84%. This indicates that BERT, in combination with the
CNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts
relevant to depressive contents. Comparative analysis with the existing
state-of-the-art methods demonstrates that our approach with BERT embedding
performs better than others in terms of evaluation metrics and the reliability
of dataset annotations. Our research significantly contribution to the
development of reliable tools for detecting depressive posts in the Bangla
language. By highlighting the efficacy of different embedding techniques and
deep learning models, this study paves the way for improved mental health
monitoring through social media platforms.

æè¦ï¼<paragraph>ç±æ¼ç¤¾ç¾¤åªé«çå»£æ³æ¡ç¨ï¼ééç¤¾ç¾¤åªé«åæä¾åµæ¸¬ä½¿ç¨èçæé¬±çå·æéè¦çæç¾©ï¼ç¹å¥æ¯å°æ¼å­å æèªç­ä»£è¡¨æ§ä¸è¶³çèªè¨ãæ¬ç ç©¶ä»ç´¹äºä¸ç¨®ææ ¹æçæ¹æ³ä¾è­å¥å­å æèªä¸­çæé¬±ç¤¾ç¾¤åªé«è²¼æï¼æ¹æ³æ¯æ¡ç¨åé²çèªç¶èªè¨èçæè¡ãæ¬ç ç©¶ä¸­æä½¿ç¨çè³æéç±é åå°å®¶è¨»è§£ï¼åæ¬æé¬±åéæé¬±è²¼æï¼ç¢ºä¿æ¨¡åè¨ç·´åè©ä¼°è³æçé«åè³ªãçºäºè§£æ±ºé¡å¥ä¸å¹³è¡¡çæ®éåé¡ï¼æåå°å°æ¸é¡å¥æ¡ç¨é¨æ©éåº¦åæ¨£ï¼å¾èå¢å¼·æ¨¡åæºç¢ºåµæ¸¬æé¬±è²¼æçè½åãæåæ¢è¨äºåç¨®æ¸å¼è¡¨ç¤ºæè¡ï¼åæ¬è©é »-éæä»¶é »ç (TF-IDF)ãTransformer (BERT) åµå¥çéåç·¨ç¢¼å¨è¡¨ç¤ºå FastText åµå¥ï¼ä¸¦å°å®åèåºæ¼æ·±åº¦å­¸ç¿çå·ç©ç¥ç¶ç¶²è·¯-éåé·ç­æè¨æ¶ (CNN-BiLSTM) æ¨¡åæ´åå¨ä¸èµ·ãééå»£æ³çå¯¦é©æç²å¾ççµæé¡¯ç¤ºï¼BERT æ¹æ³çè¡¨ç¾åªæ¼å¶ä»æ¹æ³ï¼éå°äº 84% ç F1 åæ¸ãéè¡¨ç¤º BERT è CNN-BiLSTM æ¶æ§ç¸çµåï¼å¯ä»¥ææè­å¥èæé¬±å§å®¹ç¸éçå­å æèªææ¬çç´°å¾®å·®å¥ãèç¾æçæåé²æ¹æ³é²è¡æ¯è¼åæï¼è­ææåæ¡ç¨ BERT åµå¥çæ¹æ³å¨è©ä¼°ææ¨åè³æéè¨»è§£çå¯é æ§æ¹é¢åªæ¼å¶ä»æ¹æ³ãæåçç ç©¶çºéç¼ç¨æ¼åµæ¸¬å­å æèªä¸­æé¬±è²¼æçå¯é å·¥å·ååºäºéå¤§è²¢ç»ãééå¼·èª¿ä¸ååµå¥æè¡åæ·±åº¦å­¸ç¿æ¨¡åçæè½ï¼æ¬ç ç©¶çºééç¤¾ç¾¤åªé«å¹³å°æ¹åå¿çå¥åº·ç£æ§éªå¹³äºéè·¯ã</paragraph>

##### **STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs**
2407.09096v1 by Yiheng Huang, Xiaowei Mao, Shengnan Guo, Yubin Chen, Youfang Lin, Huaiyu Wan

Spatial-temporal forecasting and imputation are important for real-world
dynamic systems such as intelligent transportation, urban planning, and public
health. Most existing methods are tailored for individual forecasting or
imputation tasks but are not designed for both. Additionally, they are less
effective for zero-shot and few-shot learning. While large language models
(LLMs) have exhibited strong pattern recognition and reasoning abilities across
various tasks, including few-shot and zero-shot learning, their development in
understanding spatial-temporal data has been constrained by insufficient
modeling of complex correlations such as the temporal correlations, spatial
connectivity, non-pairwise and high-order spatial-temporal correlations within
data. In this paper, we propose STD-LLM for understanding both spatial and
temporal properties of \underline{S}patial-\underline{T}emporal
\underline{D}ata with \underline{LLM}s, which is capable of implementing both
spatial-temporal forecasting and imputation tasks. STD-LLM understands
spatial-temporal correlations via explicitly designed spatial and temporal
tokenizers as well as virtual nodes. Topology-aware node embeddings are
designed for LLMs to comprehend and exploit the topology structure of data.
Additionally, to capture the non-pairwise and higher-order correlations, we
design a hypergraph learning module for LLMs, which can enhance the overall
performance and improve efficiency. Extensive experiments demonstrate that
STD-LLM exhibits strong performance and generalization capabilities across the
forecasting and imputation tasks on various datasets. Moreover, STD-LLM
achieves promising results on both few-shot and zero-shot learning tasks.

æè¦ï¼æç©ºé æ¸¬åå¡«è£å°æ¼æºæ§äº¤éãé½å¸è¨ç«åå¬å±è¡çç­çå¯¦ä¸çåæç³»çµ±ä¾èªªå¾éè¦ãç¾ææ¹æ³å¤§å¤æ¯éå°åå¥é æ¸¬æå¡«è£ä»»åéèº«æé ï¼ä½ä¸¦ééå°å©èè¨­è¨ãæ­¤å¤ï¼å®åå°æ¼é¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿çææè¼å·®ãåç®¡å¤§åèªè¨æ¨¡å (LLM) å·²å¨åç¨®ä»»åä¸­å±ç¾å¼·å¤§çæ¨¡å¼è­å¥åæ¨çè½åï¼åæ¬å°æ¬¡å­¸ç¿åé¶æ¬¡å­¸ç¿ï¼ä½å®åå¨çè§£æç©ºè³ææ¹é¢çç¼å±åå°éå¶ï¼åå æ¯å°è¤ééè¯æ§çå»ºæ¨¡ä¸è¶³ï¼ä¾å¦è³æä¸­çæééè¯æ§ãç©ºéé£éæ§ãéæå°åé«éæç©ºéè¯æ§ãå¨æ¬æä¸­ï¼æåæåº STD-LLMï¼ç¨æ¼äºè§£æç©ºè³æçç©ºéåæéå±¬æ§ï¼ä¸¦å·åå·è¡æç©ºé æ¸¬åå¡«è£ä»»åçè½åãSTD-LLM ééæç¢ºè¨­è¨çç©ºéåæéæ¨è¨åå¨ä»¥åèæ¬ç¯é»ä¾äºè§£æç©ºéè¯æ§ãææ²æç¥ç¯é»åµå¥æ¯çº LLM è¨­è¨çï¼ç¨æ¼çè§£åå©ç¨è³æçææ²çµæ§ãæ­¤å¤ï¼çºäºææéæå°åé«ééè¯æ§ï¼æåçº LLM è¨­è¨äºä¸åè¶åå­¸ç¿æ¨¡çµï¼å¯ä»¥æåæ´é«æè½ä¸¦æ¹åæçãå¤§éçå¯¦é©è­æ STD-LLM å¨åç¨®è³æéçé æ¸¬åå¡«è£ä»»åä¸­å±ç¾åºå¼·å¤§çæè½åæ³åè½åãæ­¤å¤ï¼STD-LLM å¨å°æ¬¡å­¸ç¿åé¶æ¬¡å­¸ç¿ä»»åä¸­é½åå¾äºä»¤äººæ»¿æçææã

##### **FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and Dehiscence Detection from Intraoral Images**
2407.09088v1 by Marawan Elbatel, Keyuan Liu, Yanqi Yang, Xiaomeng Li

Accurate detection of bone fenestration and dehiscence (FD) is crucial for
effective treatment planning in dentistry. While cone-beam computed tomography
(CBCT) is the gold standard for evaluating FD, it comes with limitations such
as radiation exposure, limited accessibility, and higher cost compared to
intraoral images. In intraoral images, dentists face challenges in the
differential diagnosis of FD. This paper presents a novel and clinically
significant application of FD detection solely from intraoral images. To
achieve this, we propose FD-SOS, a novel open-set object detector for FD
detection from intraoral images. FD-SOS has two novel components: conditional
contrastive denoising (CCDN) and teeth-specific matching assignment (TMA).
These modules enable FD-SOS to effectively leverage external dental semantics.
Experimental results showed that our method outperformed existing detection
methods and surpassed dental professionals by 35% recall under the same level
of precision. Code is available at: https://github.com/xmed-lab/FD-SOS.

æè¦ï¼éª¨éª¼ç©¿å­åéª¨ç¼ºæ (FD) çæºç¢ºåµæ¸¬å°æ¼çç§çæææ²»çè¨ç«è³ééè¦ãéå½¢æé»è¦æ·å±¤ææ (CBCT) éç¶æ¯è©ä¼° FD çé»éæ¨æºï¼ä½å®å­å¨èè«¸å¦è¼»å°æé²ãåå¾ä¸æåèå£èå§å½±åç¸æ¯ææ¬è¼é«ç­éå¶ãå¨å£èå§å½±åä¸­ï¼çé«å¸«å¨ FD çéå¥è¨ºæ·ä¸­é¢è¨ææ°ãæ¬ææåºäºä¸ååµæ°ä¸è¨åºä¸éè¦çæç¨ï¼å¯åå¾å£èå§å½±åä¸­åµæ¸¬ FDãçºéææ­¤ç®æ¨ï¼æåæåº FD-SOSï¼éæ¯ä¸ç¨®ç¨æ¼å¾å£èå§å½±åä¸­åµæ¸¬ FD çæ°åéæ¾å¼ç©ä»¶åµæ¸¬å¨ãFD-SOS æå©åæ°ç©ççµæé¨åï¼æ¢ä»¶å°æ¯å»åª (CCDN) åç¹å®æ¼çé½çå¹éæå® (TMA)ãéäºæ¨¡çµä½¿ FD-SOS è½ææå©ç¨å¤é¨çç§èªç¾©ãå¯¦é©çµæé¡¯ç¤ºï¼æåçæè¡åªæ¼ç¾æçåµæ¸¬æè¡ï¼ä¸å¨ç¸åçæºç¢ºåº¦ä¸ï¼æ¯çç§å°æ¥­äººå¡é«åº 35% çå¬åçãç¨å¼ç¢¼å¯å¨ https://github.com/xmed-lab/FD-SOS åå¾ã

##### **Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media**
2407.09019v1 by Chen Chen, Mingwei Li, Fenghuan Li, Haopeng Chen, Yuankun Lin

Massive social media data can reflect people's authentic thoughts, emotions,
communication, etc., and therefore can be analyzed for early detection of
mental health problems such as depression. Existing works about early
depression detection on social media lacked interpretability and neglected the
heterogeneity of social media data. Furthermore, they overlooked the global
interaction among users. To address these issues, we develop a novel method
that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and
contrastive learning mechanisms. Specifically, prompt learning is employed to
map users' implicit psychological symbols with excellent interpretability while
deep semantic and diverse behavioral features are incorporated by a
heterogeneous information network. Then, the heterogeneous graph network with a
dual attention mechanism is constructed to model the relationships among
heterogeneous social information at the feature level. Furthermore, the
heterogeneous subgraph network integrating subgraph attention and
self-supervised contrastive learning is developed to explore complicated
interactions among users and groups at the user level. Extensive experimental
results demonstrate that our proposed method significantly outperforms
state-of-the-art methods for depression detection on social media.

æè¦ï¼é¾å¤§çç¤¾ç¾¤åªé«è³æå¯ä»¥åæ äººåçå¯¦çæ³æ³ãæç·ãæºéç­ï¼å æ­¤å¯ä»¥åæéäºè³æï¼ä»¥æ©æåµæ¸¬æé¬±çç­å¿çå¥åº·åé¡ãç¾æéæ¼ç¤¾ç¾¤åªé«ä¸æ©ææé¬±çåµæ¸¬çç ç©¶ç¼ºä¹å¯è§£éæ§ï¼ä¸å¿½ç¥äºç¤¾ç¾¤åªé«è³æçç°è³ªæ§ãæ­¤å¤ï¼éäºç ç©¶å¿½è¦äºä½¿ç¨èä¹éçæ´é«äºåãçºäºè§£æ±ºéäºåé¡ï¼æåéç¼äºä¸ç¨®æ°ç©çæ¹æ³ï¼éç¨®æ¹æ³å©ç¨å¸¶ææç¤ºå­¸ç¿ï¼HSNPLï¼çç°è³ªå­åç¶²è·¯åå°æ¯å­¸ç¿æ©å¶ãå·é«èè¨ï¼æç¤ºå­¸ç¿è¢«ç¨æ¼ç¹ªè£½ä½¿ç¨èå·æåºè²å¯è§£éæ§çé±å«å¿çç¬¦èï¼åæééç°è³ªè³è¨ç¶²è·¯æ´åäºæ·±å±¤èªç¾©åå¤æ¨£åçè¡çºç¹å¾µãç¶å¾ï¼æ§å»ºå·æééæ³¨ææ©å¶çç°è³ªåç¶²è·¯ï¼ä»¥å¨ç¹å¾µå±¤ç´å»ºæ¨¡ç°è³ªç¤¾ç¾¤è³è¨ä¹éçéä¿ãæ­¤å¤ï¼éç¼äºæ´åå­åæ³¨æåèªæç£ç£å°æ¯å­¸ç¿çç°è³ªå­åç¶²è·¯ï¼ä»¥æ¢ç´¢ä½¿ç¨èåç¾¤çµä¹éå¨ä½¿ç¨èå±¤ç´çè¤éäºåãå¤§éçå¯¦é©çµæè¡¨æï¼æåæåºçæ¹æ³å¨ç¤¾ç¾¤åªé«ä¸çæé¬±çåµæ¸¬æ¹é¢é¡¯èåªæ¼æåé²çæ¹æ³ã

##### **Application of Artificial Intelligence in Supporting Healthcare Professionals and Caregivers in Treatment of Autistic Children**
2407.08902v1 by Hossein Mohammadi Rouzbahani, Hadis Karimipour

Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental
condition marked by difficulties in social interaction, communication
impediments, and repetitive behaviors. Despite progress in understanding ASD,
its diagnosis and treatment continue to pose significant challenges due to the
variability in symptomatology and the necessity for multidisciplinary care
approaches. This paper investigates the potential of Artificial Intelligence
(AI) to augment the capabilities of healthcare professionals and caregivers in
managing ASD. We have developed a sophisticated algorithm designed to analyze
facial and bodily expressions during daily activities of both autistic and
non-autistic children, leading to the development of a powerful deep
learning-based autism detection system. Our study demonstrated that AI models,
specifically the Xception and ResNet50V2 architectures, achieved high accuracy
in diagnosing Autism Spectrum Disorder (ASD). This research highlights the
transformative potential of AI in improving the diagnosis, treatment, and
comprehensive management of ASD. Our study revealed that AI models, notably the
Xception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing
ASD.

æè¦ï¼èªéçè­ç³»éç¤ (ASD) æ¯ä¸ç¨®å¤é¢åçç¥ç¶ç¼å±çæ³ï¼å¶ç¹å¾µå¨æ¼ç¤¾äº¤äºåå°é£ãæºééç¤åéè¤æ§è¡çºãåç®¡å¨äºè§£ ASD æ¹é¢åå¾é²å±ï¼ä½ç±æ¼çççå¤è®æ§åå°è·¨é åç§è­·æ¹æ³çå¿è¦æ§ï¼å¶è¨ºæ·åæ²»çä»ç¶æ§æéå¤§ææ°ãæ¬ææ¢è¨äººå·¥æºæ§ (AI) å¨æ´å¢é«çä¿å¥å°æ¥­äººå¡åç§è­·èç®¡ç ASD è½åæ¹é¢çæ½åãæåéç¼äºä¸ç¨®ç²¾å¯æ¼ç®æ³ï¼æ¨å¨åæèªéçåéèªéçåç«¥å¨æ¥å¸¸æ´»åä¸­çé¢é¨åèº«é«è¡¨æï¼é²èéç¼åºåè½å¼·å¤§çæ·±åº¦å­¸ç¿èªéçåµæ¸¬ç³»çµ±ãæåçç ç©¶è¡¨æï¼AI æ¨¡åï¼ç¹å¥æ¯ Xception å ResNet50V2 æ¶æ§ï¼å¨è¨ºæ·èªéçè­ç³»éç¤ (ASD) æ¹é¢åå¾é«æºç¢ºåº¦ãéé ç ç©¶çªé¡¯äº AI å¨æ¹å ASD è¨ºæ·ãæ²»çåå¨é¢ç®¡çæ¹é¢çè®é©æ½åãæåçç ç©¶æ­ç¤ºï¼AI æ¨¡åï¼ç¹å¥æ¯ Xception å ResNet50V2 æ¶æ§ï¼å¨è¨ºæ· ASD æ¹é¢è¡¨ç¾åºé«æºç¢ºåº¦ã

##### **SALT: Introducing a Framework for Hierarchical Segmentations in Medical Imaging using Softmax for Arbitrary Label Trees**
2407.08878v1 by Sven Koitka, Giulia Baldini, Cynthia S. Schmidt, Olivia B. Pollok, Obioma Pelka, Judith Kohnke, Katarzyna Borys, Christoph M. Friedrich, Benedikt M. Schaarschmidt, Michael Forsting, Lale Umutlu, Johannes Haubold, Felix Nensa, RenÃ© Hosch

Traditional segmentation networks approach anatomical structures as
standalone elements, overlooking the intrinsic hierarchical connections among
them. This study introduces Softmax for Arbitrary Label Trees (SALT), a novel
approach designed to leverage the hierarchical relationships between labels,
improving the efficiency and interpretability of the segmentations.
  This study introduces a novel segmentation technique for CT imaging, which
leverages conditional probabilities to map the hierarchical structure of
anatomical landmarks, such as the spine's division into lumbar, thoracic, and
cervical regions and further into individual vertebrae. The model was developed
using the SAROS dataset from The Cancer Imaging Archive (TCIA), comprising 900
body region segmentations from 883 patients. The dataset was further enhanced
by generating additional segmentations with the TotalSegmentator, for a total
of 113 labels. The model was trained on 600 scans, while validation and testing
were conducted on 150 CT scans. Performance was assessed using the Dice score
across various datasets, including SAROS, CT-ORG, FLARE22, LCTSC, LUNA16, and
WORD.
  Among the evaluated datasets, SALT achieved its best results on the LUNA16
and SAROS datasets, with Dice scores of 0.93 and 0.929 respectively. The model
demonstrated reliable accuracy across other datasets, scoring 0.891 on CT-ORG
and 0.849 on FLARE22. The LCTSC dataset showed a score of 0.908 and the WORD
dataset also showed good performance with a score of 0.844.
  SALT used the hierarchical structures inherent in the human body to achieve
whole-body segmentations with an average of 35 seconds for 100 slices. This
rapid processing underscores its potential for integration into clinical
workflows, facilitating the automatic and efficient computation of full-body
segmentations with each CT scan, thus enhancing diagnostic processes and
patient care.

æè¦ï¼<paragraph>å³çµ±çåå²ç¶²è·¯å°è§£åçµæ§è¦çºç¨ç«åç´ ï¼å¿½ç¥äºå®åä¹éåºæçå±¤ç´é£æ¥ãæ¬ç ç©¶å¼å¥äºä»»ææ¨ç±¤æ¨¹ç Softmax (SALT)ï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼æ¨å¨å©ç¨æ¨ç±¤ä¹éçå±¤ç´éä¿ï¼æé«åå²çæçåå¯è§£éæ§ã
æ¬ç ç©¶å¼å¥äºä¸ç¨®æ°ç CT å½±ååå²æè¡ï¼å®å©ç¨æ¢ä»¶æ©çä¾å°è§£åæ¨èªçå±¤ç´çµæ§é²è¡å°æï¼ä¾å¦å°èæ¤åçºè°æ¤ãè¸æ¤åé ¸æ¤ååï¼ä¸¦é²ä¸æ­¥åçºåå¥æ¤éª¨ãè©²æ¨¡åæ¯ä½¿ç¨ççå½±åæªæ¡é¤¨ (TCIA) ä¸­ç SAROS è³æééç¼çï¼å¶ä¸­åå«ä¾èª 883 ä½æ£èç 900 åèº«é«åååå²ãè©²è³æéé²ä¸æ­¥éé TotalSegmentator çæäºé¡å¤çåå²ï¼ç¸½å± 113 åæ¨ç±¤ãè©²æ¨¡åå¨ 600 æ¬¡ææä¸­æ¥åäºè¨ç·´ï¼èé©è­åæ¸¬è©¦åå¨ 150 æ¬¡ CT ææä¸­é²è¡ãæè½ä½¿ç¨ Dice åæ¸å¨åç¨®è³æéä¸é²è¡è©ä¼°ï¼åæ¬ SAROSãCT-ORGãFLARE22ãLCTSCãLUNA16 å WORDã
å¨è©ä¼°çè³æéä¸­ï¼SALT å¨ LUNA16 å SAROS è³æéä¸åå¾äºæä½³çµæï¼Dice åæ¸åå¥çº 0.93 å 0.929ãè©²æ¨¡åå¨å¶ä»è³æéä¸è¡¨ç¾åºå¯é çæºç¢ºæ§ï¼å¨ CT-ORG ä¸å¾åçº 0.891ï¼å¨ FLARE22 ä¸å¾åçº 0.849ãLCTSC è³æéçå¾åçº 0.908ï¼WORD è³æéçè¡¨ç¾ä¹å¾å¥½ï¼å¾åçº 0.844ã
SALT å©ç¨äººé«åºæçå±¤ç´çµæ§ï¼ä»¥å¹³å 35 ç§çæéå° 100 ååçé²è¡å¨èº«åå²ãéç¨®å¿«éèççªé¡¯äºå®æ´åå°è¨åºå·¥ä½æµç¨ä¸­çæ½åï¼ä¿é²äºæ¯æ¬¡ CT ææçå¨èº«åå²çèªåååé«æè¨ç®ï¼å¾èå¢å¼·äºè¨ºæ·éç¨åæ£èè­·çã</paragraph>

##### **FedMedICL: Towards Holistic Evaluation of Distribution Shifts in Federated Medical Imaging**
2407.08822v1 by Kumail Alhamoud, Yasir Ghunaim, Motasem Alfarra, Thomas Hartvigsen, Philip Torr, Bernard Ghanem, Adel Bibi, Marzyeh Ghassemi

For medical imaging AI models to be clinically impactful, they must
generalize. However, this goal is hindered by (i) diverse types of distribution
shifts, such as temporal, demographic, and label shifts, and (ii) limited
diversity in datasets that are siloed within single medical institutions. While
these limitations have spurred interest in federated learning, current
evaluation benchmarks fail to evaluate different shifts simultaneously.
However, in real healthcare settings, multiple types of shifts co-exist, yet
their impact on medical imaging performance remains unstudied. In response, we
introduce FedMedICL, a unified framework and benchmark to holistically evaluate
federated medical imaging challenges, simultaneously capturing label,
demographic, and temporal distribution shifts. We comprehensively evaluate
several popular methods on six diverse medical imaging datasets (totaling 550
GPU hours). Furthermore, we use FedMedICL to simulate COVID-19 propagation
across hospitals and evaluate whether methods can adapt to pandemic changes in
disease prevalence. We find that a simple batch balancing technique surpasses
advanced methods in average performance across FedMedICL experiments. This
finding questions the applicability of results from previous, narrow benchmarks
in real-world medical settings.

æè¦ï¼çºäºè®é«å­¸å½±å AI æ¨¡åå¨è¨åºä¸ç¢çå½±é¿ï¼å®åå¿é å·åæ³åæ§ãç¶èï¼æ­¤ç®æ¨åå° (i) åä½è½ç§»çä¸åé¡åï¼ä¾å¦æéãäººå£çµ±è¨åæ¨ç±¤è½ç§»ï¼ä»¥å (ii) ä¾·éæ¼å®ä¸é«çæ©æ§å§è³æéçå¤æ¨£æ§æé»ç¤ãåç®¡éäºéå¶æ¿ç¼äºå°è¯åå­¸ç¿çèè¶£ï¼ä½ç®åçè©ä¼°åºæºç¡æ³åæè©ä¼°ä¸åçè½ç§»ãç¶èï¼å¨å¯¦éçé«çä¿å¥ç°å¢ä¸­ï¼å¤ç¨®é¡åçè½ç§»åæå­å¨ï¼ä½å®åå°é«å­¸å½±åæè½çå½±é¿ä»æªå¾å°ç ç©¶ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº FedMedICLï¼ä¸åçµ±ä¸çæ¶æ§ååºæºï¼ä»¥å¨é¢è©ä¼°è¯åé«å­¸å½±åææ°ï¼åææææ¨ç±¤ãäººå£çµ±è¨åæéåä½è½ç§»ãæåå¨å­åä¸åçé«å­¸å½±åè³æéï¼ç¸½è¨ 550 å GPU å°æï¼ä¸å¨é¢è©ä¼°äºå¹¾ç¨®æµè¡çæ¹æ³ãæ­¤å¤ï¼æåä½¿ç¨ FedMedICL æ¨¡æ¬äº COVID-19 å¨é«é¢éçå³æ­ï¼ä¸¦è©ä¼°æ¹æ³æ¯å¦è½é©æç¾ççè¡ççæµè¡çè®åãæåç¼ç¾ï¼ä¸åç°¡å®çæ¹æ¬¡å¹³è¡¡æè¡å¨ FedMedICL å¯¦é©ä¸­è¶è¶äºåé²çæ¹æ³çå¹³åæè½ãæ­¤ç¼ç¾è³ªçäºååç¹éåºæºå¨ç¾å¯¦ä¸çé«çç°å¢ä¸­çµæçé©ç¨æ§ã

##### **FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification**
2407.08813v1 by Yu Tian, Congcong Wen, Min Shi, Muhammad Muneeb Afzal, Hao Huang, Muhammad Osama Khan, Yan Luo, Yi Fang, Mengyu Wang

Addressing fairness in artificial intelligence (AI), particularly in medical
AI, is crucial for ensuring equitable healthcare outcomes. Recent efforts to
enhance fairness have introduced new methodologies and datasets in medical AI.
However, the fairness issue under the setting of domain transfer is almost
unexplored, while it is common that clinics rely on different imaging
technologies (e.g., different retinal imaging modalities) for patient
diagnosis. This paper presents FairDomain, a pioneering systemic study into
algorithmic fairness under domain shifts, employing state-of-the-art domain
adaptation (DA) and generalization (DG) algorithms for both medical
segmentation and classification tasks to understand how biases are transferred
between different domains. We also introduce a novel plug-and-play fair
identity attention (FIA) module that adapts to various DA and DG algorithms to
improve fairness by using self-attention to adjust feature importance based on
demographic attributes. Additionally, we curate the first fairness-focused
dataset with two paired imaging modalities for the same patient cohort on
medical segmentation and classification tasks, to rigorously assess fairness in
domain-shift scenarios. Excluding the confounding impact of demographic
distribution variation between source and target domains will allow clearer
quantification of the performance of domain transfer models. Our extensive
evaluations reveal that the proposed FIA significantly enhances both model
performance accounted for fairness across all domain shift settings (i.e., DA
and DG) with respect to different demographics, which outperforms existing
methods on both segmentation and classification. The code and data can be
accessed at https://ophai.hms.harvard.edu/datasets/harvard-fairdomain20k.

æè¦ï¼<paragraph>å¨äººå·¥æºæ§ï¼AIï¼ï¼ç¹å¥æ¯é«ç AI ä¸­è§£æ±ºå¬å¹³æ§å°æ¼ç¢ºä¿å¬å¹³çé«çä¿å¥çµæè³ééè¦ãæè¿æåå¬å¹³æ§çåªåå¼å¥äºæ°çæ¹æ³åé«ç AI ä¸­çè³æéãç¶èï¼å¨ç¶²åè½ç§»çè¨­å®ä¸å¬å¹³æ§çè­°é¡å¹¾ä¹æªç¶æ¢è¨ï¼èè¨ºæéå¸¸ä»°è³´ä¸åçå½±åæè¡ï¼ä¾å¦ï¼ä¸åçè¦ç¶²èå½±åæ¹å¼ï¼é²è¡çæ£è¨ºæ·ãæ¬ææåº FairDomainï¼ä¸é éæ¼ç¶²åè½ç§»ä¸æ¼ç®æ³å¬å¹³æ§çåé©ç³»çµ±æ§ç ç©¶ï¼ä½¿ç¨æåé²çç¶²åé©æï¼DAï¼åæ¦åï¼DGï¼æ¼ç®æ³ï¼åæéå°é«çåå²ååé¡ä»»åï¼ä»¥äºè§£åè¦å¦ä½å¨ä¸åç¶²åéè½ç§»ãæåä¹ä»ç´¹äºä¸åæ°ç©çå³æå³ç¨å¬å¹³èº«åæ³¨æåï¼FIAï¼æ¨¡çµï¼å®é©ç¨æ¼åç¨® DA å DG æ¼ç®æ³ï¼ééä½¿ç¨èªææ³¨æåæ ¹æäººå£å±¬æ§èª¿æ´ç¹å¾µéè¦æ§ä¾æåå¬å¹³æ§ãæ­¤å¤ï¼æåç­åäºç¬¬ä¸åå¬å¹³æ§çºéé»çè³æéï¼å¶ä¸­åå«éå°ç¸åçæ£ç¾¤é«çå©ç¨®éå°å½±åæ¹å¼ï¼ç¨æ¼é«çåå²ååé¡ä»»åï¼ä»¥å´è¬¹è©ä¼°ç¶²åè½ç§»æå¢ä¸­çå¬å¹³æ§ãæé¤ä¾æºç¶²ååç®æ¨ç¶²åä¹éäººå£åä½è®ç°çæ··æ·å½±é¿ï¼å°è½æ´æ¸æ¥å°éåç¶²åè½ç§»æ¨¡åçæè½ãæåå»£æ³çè©ä¼°é¡¯ç¤ºï¼ææåºç FIA å¤§å¹æåäºèéå¬å¹³æ§çæ¨¡åæè½ï¼æ¶µèææç¶²åè½ç§»è¨­å®ï¼ä¾å¦ï¼DA å DGï¼ä»¥åä¸åäººå£çµ±è¨è³æï¼å¨åå²ååé¡æ¹é¢é½åªæ¼ç¾ææ¹æ³ãç¨å¼ç¢¼åè³æå¯æ¼ https://ophai.hms.harvard.edu/datasets/harvard-fairdomain20k åå¾ã</paragraph>

##### **Uncertainty Estimation of Large Language Models in Medical Question Answering**
2407.08662v1 by Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou

Large Language Models (LLMs) show promise for natural language generation in
healthcare, but risk hallucinating factually incorrect information. Deploying
LLMs for medical question answering necessitates reliable uncertainty
estimation (UE) methods to detect hallucinations. In this work, we benchmark
popular UE methods with different model sizes on medical question-answering
datasets. Our results show that current approaches generally perform poorly in
this domain, highlighting the challenge of UE for medical applications. We also
observe that larger models tend to yield better results, suggesting a
correlation between model size and the reliability of UE. To address these
challenges, we propose Two-phase Verification, a probability-free Uncertainty
Estimation approach. First, an LLM generates a step-by-step explanation
alongside its initial answer, followed by formulating verification questions to
check the factual claims in the explanation. The model then answers these
questions twice: first independently, and then referencing the explanation.
Inconsistencies between the two sets of answers measure the uncertainty in the
original response. We evaluate our approach on three biomedical
question-answering datasets using Llama 2 Chat models and compare it against
the benchmarked baseline methods. The results show that our Two-phase
Verification method achieves the best overall accuracy and stability across
various datasets and model sizes, and its performance scales as the model size
increases.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥é åçèªç¶èªè¨çææ¹é¢é¡¯ç¤ºåºåæ¯ï¼ä½å­å¨èæ§äºå¯¦ä¸æ­£ç¢ºè³è¨çé¢¨éªãé¨ç½² LLM ä¾åç­é«çåé¡éè¦å¯é çä¸ç¢ºå®æ§ä¼°è¨ (UE) æ¹æ³ä¾åµæ¸¬èæ§ãå¨éé å·¥ä½ä¸­ï¼æåä½¿ç¨ä¸åæ¨¡åå¤§å°å°ç±é UE æ¹æ³é²è¡åºæºæ¸¬è©¦ï¼éå°é«çåé¡åç­è³æéãæåççµæé¡¯ç¤ºï¼ç®åçä½æ³å¨éæ¹é¢éå¸¸è¡¨ç¾ä¸ä½³ï¼çªé¡¯äº UE å¨é«çæç¨ä¸­çææ°ãæåéè§å¯å°ï¼è¼å¤§çæ¨¡åå¾å¾æç¢çæ´å¥½ççµæï¼éè¡¨ææ¨¡åå¤§å°è UE çå¯é æ§ä¹éå­å¨ç¸éæ§ãçºäºæå°éäºææ°ï¼æåæåºäºå©éæ®µé©è­ï¼ä¸ç¨®ç¡æ©ççä¸ç¢ºå®æ§ä¼°è¨æ¹æ³ãé¦åï¼LLM æå¨å¶åå§ç­æ¡æéç¢çéæ­¥èªªæï¼ç¶å¾å¶å®é©è­åé¡ä¾æª¢æ¥èªªæä¸­çäºå¯¦è²æãç¶å¾ï¼æ¨¡ååç­éäºåé¡å©æ¬¡ï¼ç¬¬ä¸æ¬¡ç¨ç«åç­ï¼ç¶å¾åèèªªæãå©çµç­æ¡ä¹éçä¸ä¸è´æ§è¡¡éåå§åæä¸­çä¸ç¢ºå®æ§ãæåä½¿ç¨ Llama 2 Chat æ¨¡åå¨ä¸åçç©é«å­¸åé¡åç­è³æéä¸è©ä¼°æåçä½æ³ï¼ä¸¦å°å¶èåºæºåºæºæ¹æ³é²è¡æ¯è¼ãçµæé¡¯ç¤ºï¼æåçå©éæ®µé©è­æ¹æ³å¨åç¨®è³æéåæ¨¡åå¤§å°ä¸­å¯¦ç¾äºæä½³çæ´é«æºç¢ºæ§åç©©å®æ§ï¼ä¸¦ä¸å¶æè½é¨èæ¨¡åå¤§å°çå¢å èæ´å±ã

##### **Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models**
2407.08554v1 by Wanling Gao, Yunyou Huang, Dandan Cui, Zhuoming Yu, Wenjing Liu, Xiaoshuang Liang, Jiahui Zhao, Jiyue Xie, Hao Li, Li Ma, Ning Ye, Yumiao Kang, Dingfeng Luo, Peng Pan, Wei Huang, Zhongmou Liu, Jizhong Hu, Gangyuan Zhao, Chongrong Jiang, Fan Huang, Tianyi Wei, Suqin Tang, Bingjie Xia, Zhifei Zhang, Jianfeng Zhan

A profound gap persists between artificial intelligence (AI) and clinical
practice in medicine, primarily due to the lack of rigorous and cost-effective
evaluation methodologies. State-of-the-art and state-of-the-practice AI model
evaluations are limited to laboratory studies on medical datasets or direct
clinical trials with no or solely patient-centered controls. Moreover, the
crucial role of clinicians in collaborating with AI, pivotal for determining
its impact on clinical practice, is often overlooked. For the first time, we
emphasize the critical necessity for rigorous and cost-effective evaluation
methodologies for AI models in clinical practice, featuring
patient/clinician-centered (dual-centered) AI randomized controlled trials
(DC-AI RCTs) and virtual clinician-based in-silico trials (VC-MedAI) as an
effective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records from
two-phase inaugural DC-AI RCTs across 14 medical centers with 125 clinicians,
our results demonstrate the necessity of DC-AI RCTs and the effectiveness of
VC-MedAI. Notably, VC-MedAI performs comparably to human clinicians,
replicating insights and conclusions from prospective DC-AI RCTs. We envision
DC-AI RCTs and VC-MedAI as pivotal advancements, presenting innovative and
transformative evaluation methodologies for AI models in clinical practice,
offering a preclinical-like setting mirroring conventional medicine, and
reshaping development paradigms in a cost-effective and fast-iterative manner.
Chinese Clinical Trial Registration: ChiCTR2400086816.

æè¦ï¼<paragraph>äººå·¥æºæ§ï¼AIï¼èè¨åºé«çå¯¦åä¹éå­å¨èå·¨å¤§çé´»æºï¼å¶ä¸»è¦åå å¨æ¼ç¼ºä¹å´è¬¹ä¸å·ææ¬æççè©ä¼°æ¹æ³ãæåé²ä¸ç¬¦åå¯¦åç AI æ¨¡åè©ä¼°åéæ¼éå°é«å­¸è³æéé²è¡çå¯¦é©å®¤ç ç©¶ï¼æåææ£èçºä¸­å¿çå°ç§çµçç´æ¥è¨åºè©¦é©ãæ­¤å¤ï¼è¨åºé«å¸«å¨è AI åä½ä¸­ææ®æ¼çééµè§è²ï¼å°æ¼æ±ºå®å¶å°è¨åºå¯¦åçå½±é¿è³ééè¦ï¼å»ç¶å¸¸è¢«å¿½è¦ãæåé¦åº¦å¼·èª¿å¨è¨åºå¯¦åä¸­æ¡ç¨å´è¬¹ä¸å·ææ¬æçç AI æ¨¡åè©ä¼°æ¹æ³è³ééè¦ï¼å¶ç¹è²å¨æ¼ä»¥æ£èï¼è¨åºé«å¸«çºä¸­å¿çï¼éä¸­å¿ï¼AI é¨æ©å°ç§è©¦é©ï¼DC-AI RCTï¼åèæ¬è¨åºé«å¸«çºåºç¤çé»è¦æ¨¡æ¬è©¦é©ï¼VC-MedAIï¼ï¼åçº DC-AI RCT çæææ¿ä»£æ¹æ¡ãå©ç¨ä¾èª 14 åé«çä¸­å¿ã125 ä½è¨åºé«å¸«çå©éæ®µé¦æ¬¡ DC-AI RCT ä¸­ç 7500 ç­è¨ºæ·ç´éï¼æåççµæè­æäº DC-AI RCT çå¿è¦æ§è VC-MedAI çæææ§ãå¼å¾æ³¨æçæ¯ï¼VC-MedAI çè¡¨ç¾èäººé¡è¨åºé«å¸«ç¸ç¶ï¼è¤è£½äºåç»æ§ DC-AI RCT çè¦è§£åçµè«ãæåå° DC-AI RCT å VC-MedAI è¦çºééµçé²å±ï¼å®åæåºäºåµæ°ä¸å·æè®é©æ§ç AI æ¨¡åè©ä¼°æ¹æ³ï¼å¨è¨åºå¯¦åä¸­æä¾é¡ä¼¼æ¼è¨åºåè¨­å®çç°å¢ï¼åæ å³çµ±é«å­¸ï¼ä¸¦ä»¥å·ææ¬æçä¸å¿«éåè¦éç®çæ¹å¼éæ°å¡é éç¼æ¨¡å¼ãä¸­åè¨åºè©¦é©è¨»åï¼ChiCTR2400086816ã</paragraph>

##### **How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical Time-Series Imputation**
2407.08442v1 by Linglong Qian, Tao Wang, Jun Wang, Hugh Logan Ellis, Robin Mitra, Richard Dobson, Zina Ibrahim

We introduce a novel classification framework for time-series imputation
using deep learning, with a particular focus on clinical data. By identifying
conceptual gaps in the literature and existing reviews, we devise a taxonomy
grounded on the inductive bias of neural imputation frameworks, resulting in a
classification of existing deep imputation strategies based on their
suitability for specific imputation scenarios and data-specific properties. Our
review further examines the existing methodologies employed to benchmark deep
imputation models, evaluating their effectiveness in capturing the missingness
scenarios found in clinical data and emphasising the importance of reconciling
mathematical abstraction with clinical insights. Our classification aims to
serve as a guide for researchers to facilitate the selection of appropriate
deep learning imputation techniques tailored to their specific clinical data.
Our novel perspective also highlights the significance of bridging the gap
between computational methodologies and medical insights to achieve clinically
sound imputation models.

æè¦ï¼æåæåºäºä¸åæ°çæéåºåæè£åé¡æ¶æ§ï¼ä½¿ç¨æ·±åº¦å­¸ç¿ï¼ç¹å¥éæ³¨è¨åºæ¸æãééæ¾åºæç»åç¾æè©è«ä¸­çæ¦å¿µå·®è·ï¼æåè¨­è¨äºä¸ååé¡æ³ï¼è©²åé¡æ³åºæ¼ç¥ç¶æè£æ¡æ¶çæ­¸ç´åèª¤ï¼å¾èå°ç¾æçæ·±åº¦æè£ç­ç¥é²è¡åé¡ï¼åºæ¼å®åå°ç¹å®æè£å ´æ¯åæ¸æç¹å®å±¬æ§çé©ç¨æ§ãæåçåé¡§é²ä¸æ­¥æª¢é©äºç¨æ¼å°æ·±åº¦æè£æ¨¡åé²è¡åºæºæ¸¬è©¦çç¾ææ¹æ³ï¼è©ä¼°äºå®åå¨ææè¨åºæ¸æä¸­ç¼ç¾çç¼ºå¤±å ´æ¯æ¹é¢çæææ§ï¼ä¸¦å¼·èª¿äºèª¿åæ¸å­¸æ½è±¡èè¨åºè¦è§£çéè¦æ§ãæåçåé¡æ¨å¨ä½çºç ç©¶äººå¡çæåï¼ä»¥ä¿é²æ ¹æå¶ç¹å®è¨åºæ¸æé¸æé©ç¶çæ·±åº¦å­¸ç¿æè£æè¡ãæåçæ°è§é»éå¼·èª¿äºå½åè¨ç®æ¹æ³åé«å­¸è¦è§£ä¹éå·®è·ä»¥å¯¦ç¾è¨åºåçæè£æ¨¡åçéè¦æ§ã

##### **Specialist vision-language models for clinical ophthalmology**
2407.08410v1 by Robbie Holland, Thomas R. P. Taylor, Christopher Holmes, Sophie Riedl, Julia Mai, Maria Patsiamanidi, Dimitra Mitsopoulou, Paul Hager, Philip MÃ¼ller, Hendrik P. N. Scholl, Hrvoje BogunoviÄ, Ursula Schmidt-Erfurth, Daniel Rueckert, Sobha Sivaprasad, Andrew J. Lotery, Martin J. Menten

Clinicians spend a significant amount of time reviewing medical images and
transcribing their findings regarding patient diagnosis, referral and treatment
in text form. Vision-language models (VLMs), which automatically interpret
images and summarize their findings as text, have enormous potential to
alleviate clinical workloads and increase patient access to high-quality
medical care. While foundational models have stirred considerable interest in
the medical community, it is unclear whether their general capabilities
translate to real-world clinical utility. In this work, we show that foundation
VLMs markedly underperform compared to practicing ophthalmologists on
specialist tasks crucial to the care of patients with age-related macular
degeneration (AMD). To address this, we initially identified the essential
capabilities required for image-based clinical decision-making, and then
developed a curriculum to selectively train VLMs in these skills. The resulting
model, RetinaVLM, can be instructed to write reports that significantly
outperform those written by leading foundation medical VLMs in disease staging
(F1 score of 0.63 vs. 0.11) and patient referral (0.67 vs. 0.39), and
approaches the diagnostic performance of junior ophthalmologists (who achieve
0.77 and 0.78 on the respective tasks). Furthermore, in a reader study
involving two senior ophthalmologists with up to 32 years of experience,
RetinaVLM's reports were found to be similarly correct (78.6% vs. 82.1%) and
complete (both 78.6%) as reports written by junior ophthalmologists with up to
10 years of experience. These results demonstrate that our curriculum-based
approach provides a blueprint for specializing generalist foundation medical
VLMs to handle real-world clinical tasks.

æè¦ï¼<paragraph>è¨åºé«çè±è²»å¤§éæéæª¢é±é«çå½±åï¼ä¸¦ä»¥æå­å½¢å¼è¨éä»åéæ¼æ£èè¨ºæ·ãè½è¨ºåæ²»ççç¼ç¾ãè¦è¦ºèªè¨æ¨¡å (VLM) æèªåè§£è®å½±åä¸¦å°å¶ç¼ç¾æè¦ææå­ï¼å·ææ¸è¼è¨åºå·¥ä½è² è¼åå¢å æ£èç²å¾åªè³ªé«çä¿å¥çæ©æçå·¨å¤§æ½åãéç¶åºç¤æ¨¡åå¨é«ççå¼èµ·äºç¸ç¶å¤§çèè¶£ï¼ä½å°ä¸æ¸æ¥å®åçä¸è¬è½åæ¯å¦è½è½åçºå¯¦éçè¨åºæç¨ãå¨éé å·¥ä½ä¸­ï¼æåè¡¨æåºç¤ VLM å¨èå¹´é½¡ç¸éæ§é»æé¨çè® (AMD) æ£èç§è­·è³ééè¦çå°éä»»åä¸ï¼è¡¨ç¾æé¡¯ä¸å¦å·æ¥­ç¼ç§é«çãçºäºè§£æ±ºéååé¡ï¼æåæåæ¾åºå½±åå¼è¨åºæ±ºç­æéçå¿è¦è½åï¼ç¶å¾å¶å®èª²ç¨ä¾é¸ææ§å°è¨ç·´ VLM éäºæè½ãæç¢ççæ¨¡å RetinaVLM å¯ä»¥è¢«æç¤ºæ°å¯«å ±åï¼å¶å¨ç¾çåæï¼F1 åæ¸çº 0.63 å° 0.11ï¼åæ£èè½è¨ºï¼0.67 å° 0.39ï¼æ¹é¢æé¡¯åªæ¼é åçåºç¤é«ç VLM ææ°å¯«çå ±åï¼ä¸¦æ¥è¿åç´ç¼ç§é«ççè¨ºæ·è¡¨ç¾ï¼å¨åé ä»»åä¸­åå¥éå° 0.77 å 0.78ï¼ãæ­¤å¤ï¼å¨æ¶åå©ä½ææé·é 32 å¹´ç¶é©çé«ç´ç¼ç§é«ççè®èç ç©¶ä¸­ï¼ç¼ç¾ RetinaVLM çå ±åæ­£ç¢ºæ§ï¼78.6% å° 82.1%ï¼åå®æ´æ§ï¼åçº 78.6%ï¼èææé·é 10 å¹´ç¶é©çåç´ç¼ç§é«çææ°å¯«çå ±åç¸ä¼¼ãéäºçµæè¡¨æï¼æååºæ¼èª²ç¨çæ¹æ³æä¾äºå°éæåºç¤é«ç VLM å°éåä»¥èçå¯¦éè¨åºä»»åçèåã</paragraph>

##### **Unveiling Disparities in Maternity Care: A Topic Modelling Approach to Analysing Maternity Incident Investigation Reports**
2407.08328v1 by Georgina Cosma, Mohit Kumar Singh, Patrick Waterson, Gyuchan Thomas Jun, Jonathan Back

This study applies Natural Language Processing techniques, including Latent
Dirichlet Allocation, to analyse anonymised maternity incident investigation
reports from the Healthcare Safety Investigation Branch. The reports underwent
preprocessing, annotation using the Safety Intelligence Research taxonomy, and
topic modelling to uncover prevalent topics and detect differences in maternity
care across ethnic groups. A combination of offline and online methods was
utilised to ensure data protection whilst enabling advanced analysis, with
offline processing for sensitive data and online processing for non-sensitive
data using the `Claude 3 Opus' language model. Interactive topic analysis and
semantic network visualisation were employed to extract and display thematic
topics and visualise semantic relationships among keywords. The analysis
revealed disparities in care among different ethnic groups, with distinct focus
areas for the Black, Asian, and White British ethnic groups. The study
demonstrates the effectiveness of topic modelling and NLP techniques in
analysing maternity incident investigation reports and highlighting disparities
in care. The findings emphasise the crucial role of advanced data analysis in
improving maternity care quality and equity.

æè¦ï¼æ¬ç ç©¶æç¨èªç¶èªè¨èçæè¡ï¼åæ¬æ½å¨çå©åé·åéï¼åæé«çä¿å¥å®å¨èª¿æ¥å±çå¿åç¢å©¦äºä»¶èª¿æ¥å ±åãéäºå ±åç¶éé èçãä½¿ç¨å®å¨æå ±ç ç©¶åé¡æ³è¨»è§£ï¼ä»¥åä¸»é¡å»ºæ¨¡ï¼ä»¥æ¾åºæ®éçä¸»é¡ä¸¦æ¾åºä¸åæç¾¤å¨ç¢åç§è­·çå·®ç°ãçµåé¢ç·åç·ä¸æ¹æ³ï¼ä»¥ç¢ºä¿è³æä¿è­·ï¼åæé²è¡é²éåæï¼ä½¿ç¨ Claude 3 Opus èªè¨æ¨¡åå°ææè³æé²è¡é¢ç·èçï¼å°éææè³æé²è¡ç·ä¸èçãæ¡ç¨äºåä¸»é¡åæåèªæç¶²è·¯è¦è¦ºåï¼ä»¥èååé¡¯ç¤ºä¸»é¡ä¸»é¡ï¼ä¸¦è¦è¦ºåééµå­ä¹éçèªæéä¿ãåæé¡¯ç¤ºä¸åæç¾¤ä¹éçç§è­·å·®ç°ï¼é»äººãäºæ´²äººåç½äººè±åäººæç¾¤çéæ³¨é åä¸åãæ¬ç ç©¶è­æäºä¸»é¡å»ºæ¨¡åèªç¶èªè¨èçæè¡å¨åæç¢å©¦äºä»¶èª¿æ¥å ±ååå¼·èª¿ç§è­·å·®ç°æ¹é¢çæææ§ãç ç©¶çµæå¼·èª¿äºé²éè³æåæå¨æåç¢åç§è­·åè³ªåå¬å¹³æ§æ¹é¢çééµè§è²ã

##### **Predicting Heart Failure with Attention Learning Techniques Utilizing Cardiovascular Data**
2407.08289v1 by Ershadul Haque, Manoranjan Paul, Faranak Tohidi

Cardiovascular diseases (CVDs) encompass a group of disorders affecting the
heart and blood vessels, including conditions such as coronary artery disease,
heart failure, stroke, and hypertension. In cardiovascular diseases, heart
failure is one of the main causes of death and also long-term suffering in
patients worldwide. Prediction is one of the risk factors that is highly
valuable for treatment and intervention to minimize heart failure. In this
work, an attention learning-based heart failure prediction approach is proposed
on EHR(electronic health record) cardiovascular data such as ejection fraction
and serum creatinine. Moreover, different optimizers with various learning rate
approaches are applied to fine-tune the proposed approach. Serum creatinine and
ejection fraction are the two most important features to predict the patient's
heart failure. The computational result shows that the RMSProp optimizer with
0.001 learning rate has a better prediction based on serum creatinine. On the
other hand, the combination of SGD optimizer with 0.01 learning rate exhibits
optimum performance based on ejection fraction features. Overall, the proposed
attention learning-based approach performs very efficiently in predicting heart
failure compared to the existing state-of-the-art such as LSTM approach.

æè¦ï¼å¿è¡ç®¡ç¾ç (CVD) åå«ä¸çµå½±é¿å¿èåè¡ç®¡çç¾çï¼åæ¬å çåèç¾çãå¿è¡°ç«­ãä¸­é¢¨åé«è¡å£ç­ç¾çãå¨å¿è¡ç®¡ç¾çä¸­ï¼å¿è¡°ç«­æ¯å¨çæ£èæ­»äº¡çä¸»è¦åå ä¹ä¸ï¼ä¹æ¯é·æçè¦çä¾æºãé æ¸¬æ¯å°æ²»çåå¹²é ä»¥æå¤§ç¨åº¦æ¸å°å¿è¡°ç«­æ¥µæå¹å¼çé¢¨éªå ç´ ä¹ä¸ãå¨éé å·¥ä½ä¸­ï¼æåºäºä¸ç¨®åºæ¼æ³¨æåå­¸ç¿çå¿è¡°ç«­é æ¸¬æ¹æ³ï¼è©²æ¹æ³åºæ¼ EHRï¼é»å­å¥åº·è¨éï¼å¿è¡ç®¡æ¸æï¼ä¾å¦å°è¡åæ¸åè¡æ¸èéãæ­¤å¤ï¼æç¨å·æåç¨®å­¸ç¿çæ¹æ³çä¸ååªåå¨å°ææåºçæ¹æ³é²è¡å¾®èª¿ãè¡æ¸èéåå°è¡åæ¸æ¯é æ¸¬æ£èå¿è¡°ç«­çå©åæéè¦çç¹å¾µãè¨ç®çµæè¡¨æï¼å­¸ç¿ççº 0.001 ç RMSProp åªåå¨åºæ¼è¡æ¸èéå·ææ´å¥½çé æ¸¬ãå¦ä¸æ¹é¢ï¼å­¸ç¿ççº 0.01 ç SGD åªåå¨èå°è¡åæ¸ç¹å¾µç¸çµåï¼è¡¨ç¾åºæä½³æ§è½ãç¸½é«èè¨ï¼è LSTM æ¹æ³ç­ç¾ææè¡ç¸æ¯ï¼ææåºçåºæ¼æ³¨æåå­¸ç¿çæ¹æ³å¨é æ¸¬å¿è¡°ç«­æ¹é¢è¡¨ç¾å¾éå¸¸ææã

##### **Leveraging LLMs to Predict Affective States via Smartphone Sensor Features**
2407.08240v1 by Tianyi Zhang, Songyan Teng, Hong Jia, Simon D'Alfonso

As mental health issues for young adults present a pressing public health
concern, daily digital mood monitoring for early detection has become an
important prospect. An active research area, digital phenotyping, involves
collecting and analysing data from personal digital devices such as smartphones
(usage and sensors) and wearables to infer behaviours and mental health. Whilst
this data is standardly analysed using statistical and machine learning
approaches, the emergence of large language models (LLMs) offers a new approach
to make sense of smartphone sensing data. Despite their effectiveness across
various domains, LLMs remain relatively unexplored in digital mental health,
particularly in integrating mobile sensor data. Our study aims to bridge this
gap by employing LLMs to predict affect outcomes based on smartphone sensing
data from university students. We demonstrate the efficacy of zero-shot and
few-shot embedding LLMs in inferring general wellbeing. Our findings reveal
that LLMs can make promising predictions of affect measures using solely
smartphone sensing data. This research sheds light on the potential of LLMs for
affective state prediction, emphasizing the intricate link between smartphone
behavioral patterns and affective states. To our knowledge, this is the first
work to leverage LLMs for affective state prediction and digital phenotyping
tasks.

æè¦ï¼é¨èå¹´è¼äººçå¿çå¥åº·åé¡æçºè¿«åçå¬å±è¡çåé¡ï¼æ¯æ¥æ¸ä½æç·ç£æ§å·²æçºæ©æåµæ¸¬çéè¦åæ¯ãæ¸ä½è¡¨ååæ¯ä¸åç©æ¥µçç ç©¶é åï¼æ¶åæ¶éååæä¾èªåäººæ¸ä½è£ç½®ï¼ä¾å¦æºæ§åææ©ï¼ä½¿ç¨åææ¸¬å¨ï¼åå¯ç©¿æ´è£ç½®ï¼çè³æï¼ä»¥æ¨è«è¡çºåå¿çå¥åº·ãéç¶éäºè³æéå¸¸ä½¿ç¨çµ±è¨åæ©å¨å­¸ç¿æ¹æ³é²è¡åæï¼ä½å¤§åèªè¨æ¨¡å (LLM) çåºç¾æä¾äºä¸ç¨®æ°çæ¹æ³ä¾çè§£æºæ§åææ©ææ¸¬è³æãåç®¡ LLM å¨åç¨®é åé½éå¸¸ææï¼ä½å¶å¨æ¸ä½å¿çå¥åº·é åä»ç¸å°æªè¢«æ¢ç´¢ï¼ç¹å¥æ¯å¨æ´åè¡åææ¸¬å¨è³ææ¹é¢ãæåçç ç©¶æ¨å¨ééä½¿ç¨ LLM ä¾æ ¹æå¤§å­¸ççæºæ§åææ©ææ¸¬è³æé æ¸¬å½±é¿çµæï¼ä»¥å½åéä¸å·®è·ãæåå±ç¤ºäºé¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿åµå¥å¼ LLM å¨æ¨è«ä¸è¬å¹¸ç¦ææ¹é¢çæè½ãæåçç ç©¶çµæé¡¯ç¤ºï¼LLM å¯ä»¥åä½¿ç¨æºæ§åææ©ææ¸¬è³æå°å½±é¿æ¸¬éé²è¡æå¸æçé æ¸¬ãæ¬ç ç©¶æ­ç¤ºäº LLM å¨ææçæé æ¸¬æ¹é¢çæ½åï¼å¼·èª¿äºæºæ§åææ©è¡çºæ¨¡å¼åææçæä¹éçè¤éè¯ç¹«ãææåæç¥ï¼éæ¯ç¬¬ä¸åå©ç¨ LLM é²è¡ææçæé æ¸¬åæ¸ä½è¡¨ååä»»åçç ç©¶ã

##### **DALL-M: Context-Aware Clinical Data Augmentation with LLMs**
2407.08227v1 by Chihcheng Hsieh, Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Joaquim Jorge, Jacinto C. Nascimento

X-ray images are vital in medical diagnostics, but their effectiveness is
limited without clinical context. Radiologists often find chest X-rays
insufficient for diagnosing underlying diseases, necessitating comprehensive
clinical features and data integration. We present a novel technique to enhance
the clinical context through augmentation techniques with clinical tabular
data, thereby improving its applicability and reliability in AI medical
diagnostics. To address this, we introduce a pioneering approach to clinical
data augmentation that employs large language models (LLMs) to generate patient
contextual synthetic data. This methodology is crucial for training more robust
deep learning models in healthcare. It preserves the integrity of real patient
data while enriching the dataset with contextually relevant synthetic features,
significantly enhancing model performance. DALL-M uses a three-phase feature
generation process: (i) clinical context storage, (ii) expert query generation,
and (iii) context-aware feature augmentation. DALL-M generates new, clinically
relevant features by synthesizing chest X-ray images and reports. Applied to
799 cases using nine features from the MIMIC-IV dataset, it created an
augmented set of 91 features. This is the first work to generate contextual
values for existing and new features based on patients' X-ray reports, gender,
and age and to produce new contextual knowledge during data augmentation.
Empirical validation with machine learning models, including Decision Trees,
Random Forests, XGBoost, and TabNET, showed significant performance
improvements. Incorporating augmented features increased the F1 score by 16.5%
and Precision and Recall by approximately 25%. DALL-M addresses a critical gap
in clinical data augmentation, offering a robust framework for generating
contextually enriched datasets.

æè¦ï¼<paragraph>X åå½±åå¨å»å­¦è¯æ­ä¸­è³å³éè¦ï¼ä½å¦ææ²¡æä¸´åºèæ¯ï¼å¶æææ§ä¼åå°éå¶ãæ¾å°ç§å»çç»å¸¸åç°è¸é¨ X åå½±åä¸è¶³ä»¥è¯æ­æ½å¨ç¾çï¼å æ­¤éè¦å¨é¢çä¸´åºç¹å¾åæ°æ®æ´åãæä»¬æåºäºä¸ç§æ°ææ¯ï¼éè¿ä½¿ç¨ä¸´åºè¡¨æ ¼æ°æ®è¿è¡å¢å¼ºææ¯æ¥å¢å¼ºä¸´åºèæ¯ï¼ä»èæé«å¶å¨ AI å»å­¦è¯æ­ä¸­çéç¨æ§åå¯é æ§ãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æä»¬å¼å¥äºä¸ç§å¼åæ§çä¸´åºæ°æ®å¢å¼ºæ¹æ³ï¼è¯¥æ¹æ³éç¨å¤§åè¯­è¨æ¨¡å (LLM) æ¥çææ£èèæ¯åææ°æ®ãè¿ç§æ¹æ³å¯¹äºå¨å»çä¿å¥é¢åè®­ç»æ´å¼ºå¤§çæ·±åº¦å­¦ä¹ æ¨¡åè³å³éè¦ãå®ä¿çäºçå®æ£èæ°æ®çå®æ´æ§ï¼åæ¶ä½¿ç¨ä¸ä¸ä¸æç¸å³çåæç¹å¾ä¸°å¯äºæ°æ®éï¼ä»èæ¾èæé«äºæ¨¡åæ§è½ãDALL-M ä½¿ç¨äºä¸ä¸ªä¸é¶æ®µç¹å¾çæè¿ç¨ï¼(i) ä¸´åºèæ¯å­å¨ï¼(ii) ä¸å®¶æ¥è¯¢çæï¼(iii) ä¸ä¸ææç¥ç¹å¾å¢å¼ºãDALL-M éè¿åæè¸é¨ X åå½±ååæ¥åæ¥çææ°çãä¸ä¸´åºç¸å³çç¹å¾ãå°å¶åºç¨äº MIMIC-IV æ°æ®éä¸­ç 799 ä¸ªæ¡ä¾ï¼ä½¿ç¨ä¹ä¸ªç¹å¾ï¼å®åå»ºäºä¸ä¸ªåå« 91 ä¸ªç¹å¾çå¢å¼ºéåãè¿æ¯ç¬¬ä¸é¡¹åºäºæ£èç X åæ¥åãæ§å«åå¹´é¾ä¸ºç°æåæ°ç¹å¾çæä¸ä¸æå¼çèä½ï¼å¹¶å¨æ°æ®å¢å¼ºæé´äº§çæ°çä¸ä¸æç¥è¯ãä½¿ç¨åæ¬å³ç­æ ãéæºæ£®æãXGBoost å TabNET å¨åçæºå¨å­¦ä¹ æ¨¡åè¿è¡çç»éªéªè¯æ¾ç¤ºåºæ¾èçæ§è½æ¹è¿ãåå¹¶å¢å¼ºåè½å° F1 åæ°æé«äº 16.5%ï¼å¹¶å°ç²¾ç¡®åº¦åå¬åçæé«äºå¤§çº¦ 25%ãDALL-M è§£å³äºä¸ä¸ªä¸´åºæ°æ®å¢å¼ºä¸­çå³é®ç©ºç½ï¼æä¾äºä¸ä¸ªç¨äºçæä¸ä¸æä¸°å¯çæ°æ®éçç¨³å¥æ¡æ¶ã</paragraph>

##### **Synthetic Electroretinogram Signal Generation Using Conditional Generative Adversarial Network for Enhancing Classification of Autism Spectrum Disorder**
2407.08166v1 by Mikhail Kulyabin, Paul A. Constable, Aleksei Zhdanov, Irene O. Lee, David H. Skuse, Dorothy A. Thompson, Andreas Maier

The electroretinogram (ERG) is a clinical test that records the retina's
electrical response to light. The ERG is a promising way to study different
neurodevelopmental and neurodegenerative disorders, including autism spectrum
disorder (ASD) - a neurodevelopmental condition that impacts language,
communication, and reciprocal social interactions. However, in heterogeneous
populations, such as ASD, where the ability to collect large datasets is
limited, the application of artificial intelligence (AI) is complicated.
Synthetic ERG signals generated from real ERG recordings carry similar
information as natural ERGs and, therefore, could be used as an extension for
natural data to increase datasets so that AI applications can be fully
utilized. As proof of principle, this study presents a Generative Adversarial
Network capable of generating synthetic ERG signals of children with ASD and
typically developing control individuals. We applied a Time Series Transformer
and Visual Transformer with Continuous Wavelet Transform to enhance
classification results on the extended synthetic signals dataset. This approach
may support classification models in related psychiatric conditions where the
ERG may help classify disorders.

æè¦ï¼è¦ç¶²èé»å (ERG) æ¯ä¸ç¨®è¨åºæ¸¬è©¦ï¼ç¨æ¼è¨éè¦ç¶²èå°åçé»æ°£åæãERG æ¯ä¸ç¨®å¾æåéçç ç©¶ä¸åç¥ç¶ç¼è²åç¥ç¶éåæ§ç¾ççæ¹æ³ï¼åæ¬èªéçè­ç³»éç¤ (ASD) - ä¸ç¨®å½±é¿èªè¨ãæºéåç¤¾äº¤äºåçç¥ç¶ç¼è²çæ³ãç¶èï¼å¨ç°è³ªäººç¾¤ä¸­ï¼ä¾å¦ ASDï¼æ¶éå¤§åæ¸æéçè½åæéï¼äººå·¥æºè½ (AI) çæç¨å¾è¤éãå¾çå¯¦ ERG è¨éä¸­ç¢ççåæ ERG ä¿¡èæå¸¶èèªç¶ ERG ç¸ä¼¼çä¿¡æ¯ï¼å æ­¤å¯ç¨ä½èªç¶æ¸æçæ´å±ï¼ä»¥å¢å æ¸æéï¼ä»¥ä¾¿ AI æç¨ç¨åºå¯ä»¥å¾å°ååå©ç¨ãä½çºåçè­æï¼æ¬ç ç©¶æåºäºä¸åçæå°æç¶²è·¯ï¼è½å¤ ç¢çèªéçåç«¥åæ­£å¸¸ç¼è²å°ç§åäººçåæ ERG ä¿¡èãæåæç¨æåºè½æå¨åå·æé£çºå°æ³¢è½æçè¦è¦ºè½æå¨ä¾å¢å¼·æ´å±åæä¿¡èæ¸æéä¸çåé¡çµæãéç¨®æ¹æ³å¯ä»¥æ¯æç¸éç²¾ç¥ç¾ççåé¡æ¨¡åï¼å¨éäºç¾çä¸­ï¼ERG å¯è½æå©æ¼å°ç¾çé²è¡åé¡ã

##### **Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates**
2407.08134v1 by A. Noorizadegan, Y. C. Hon, D. L. Young, C. S. Chen

Surface reconstruction from point clouds is a fundamental challenge in
computer graphics and medical imaging. In this paper, we explore the
application of advanced neural network architectures for the accurate and
efficient reconstruction of surfaces from data points. We introduce a novel
variant of the Highway network (Hw) called Square-Highway (SqrHw) within the
context of multilayer perceptrons and investigate its performance alongside
plain neural networks and a simplified Hw in various numerical examples. These
examples include the reconstruction of simple and complex surfaces, such as
spheres, human hands, and intricate models like the Stanford Bunny. We analyze
the impact of factors such as the number of hidden layers, interior and
exterior points, and data distribution on surface reconstruction quality. Our
results show that the proposed SqrHw architecture outperforms other neural
network configurations, achieving faster convergence and higher-quality surface
reconstructions. Additionally, we demonstrate the SqrHw's ability to predict
surfaces over missing data, a valuable feature for challenging applications
like medical imaging. Furthermore, our study delves into further details,
demonstrating that the proposed method based on highway networks yields more
stable weight norms and backpropagation gradients compared to the Plain Network
architecture. This research not only advances the field of computer graphics
but also holds utility for other purposes such as function interpolation and
physics-informed neural networks, which integrate multilayer perceptrons into
their algorithms.

æè¦ï¼å¾é»é²é²è¡æ²é¢éå»ºæ¯é»è¦åå­¸åé«å­¸å½±åä¸­çä¸é åºæ¬ææ°ãå¨æ¬æä¸­ï¼æåæ¢è¨äºåé²ç¥ç¶ç¶²è·¯æ¶æ§å¨å¾è³æé»ç²¾ç¢ºä¸ææéå»ºæ²é¢ä¸­çæç¨ãæåå¨å¤å±¤æç¥å¨çæ¶æ§ä¸­ï¼å¼å¥äºé«éå¬è·¯ç¶²è·¯ï¼Hwï¼çä¸ç¨®æ°è®é«ï¼ç¨±çº Square-Highwayï¼SqrHwï¼ï¼ä¸¦å¨åç¨®æ¸å¼ç¯ä¾ä¸­æ¢è¨å¶æè½ï¼ä»¥åèä¸è¬ç¥ç¶ç¶²è·¯åç°¡åç Hw çæè½ãéäºç¯ä¾åæ¬éå»ºç°¡å®åè¤éçæ²é¢ï¼ä¾å¦çé«ãäººæåå Stanford Bunny é£æ¨£è¤éçæ¨¡åãæååæäºé±èå±¤æ¸ãå§é¨åå¤é¨é»ä»¥åè³æåä½ç­å ç´ å°æ²é¢éå»ºåè³ªçå½±é¿ãæåççµæé¡¯ç¤ºï¼ææåºç SqrHw æ¶æ§åªæ¼å¶ä»ç¥ç¶ç¶²è·¯çµæï¼è½éææ´å¿«çæ¶æéåº¦åæ´é«åè³ªçæ²é¢éå»ºãæ­¤å¤ï¼æåå±ç¤ºäº SqrHw è½å¤ é æ¸¬éºå¤±è³æä¸çæ²é¢ï¼éå°æ¼åé«å­¸å½±åé£æ¨£å·æææ°æ§çæç¨ä¾èªªï¼æ¯ä¸åæå¹å¼çåè½ãæ­¤å¤ï¼æåçç ç©¶æ·±å¥æ¢è¨äºæ´å¤ç´°ç¯ï¼è­æäºåºæ¼é«éå¬è·¯ç¶²è·¯çææåºæ¹æ³ï¼èä¸è¬ç¶²è·¯æ¶æ§ç¸æ¯ï¼ç¢çäºæ´ç©©å®çæ¬éç¯æ¸åååå³æ­æ¢¯åº¦ãéé ç ç©¶ä¸åæ¨åäºé»è¦åå­¸é åï¼ä¹å°å¶ä»ç¨éæå¹«å©ï¼ä¾å¦å½æ¸å§æåç©çè³è¨ç¥ç¶ç¶²è·¯ï¼å°å¤å±¤æç¥å¨æ´åå°å¶æ¼ç®æ³ä¸­ã

##### **Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data**
2407.08003v1 by Ritesh Mehta, Aleksandar Pramov, Shashank Verma

Amyotrophic Lateral Sclerosis (ALS) is characterized as a rapidly progressive
neurodegenerative disease that presents individuals with limited treatment
options in the realm of medical interventions and therapies. The disease
showcases a diverse range of onset patterns and progression trajectories,
emphasizing the critical importance of early detection of functional decline to
enable tailored care strategies and timely therapeutic interventions. The
present investigation, spearheaded by the iDPP@CLEF 2024 challenge, focuses on
utilizing sensor-derived data obtained through an app. This data is used to
construct various machine learning models specifically designed to forecast the
advancement of the ALS Functional Rating Scale-Revised (ALSFRS-R) score,
leveraging the dataset provided by the organizers. In our analysis, multiple
predictive models were evaluated to determine their efficacy in handling ALS
sensor data. The temporal aspect of the sensor data was compressed and
amalgamated using statistical methods, thereby augmenting the interpretability
and applicability of the gathered information for predictive modeling
objectives. The models that demonstrated optimal performance were a naive
baseline and ElasticNet regression. The naive model achieved a Mean Absolute
Error (MAE) of 0.20 and a Root Mean Square Error (RMSE) of 0.49, slightly
outperforming the ElasticNet model, which recorded an MAE of 0.22 and an RMSE
of 0.50. Our comparative analysis suggests that while the naive approach
yielded marginally better predictive accuracy, the ElasticNet model provides a
robust framework for understanding feature contributions.

æè¦ï¼èèç¸®æ§èé«å´ç´¢ç¡¬åç (ALS) çç¹å¾µçºå¿«éé²å±çç¥ç¶éåæ§ç¾çï¼å¨é«çä»å¥åæ²»çé åä¸­ï¼æ£èçæ²»çé¸ææéãæ­¤ç¾çå±ç¤ºåºå¤æ¨£åçç¼çæ¨¡å¼åé²å±è»è·¡ï¼å¼·èª¿æ©æåµæ¸¬åè½è¡°éè³ééè¦ï¼ä»¥å¶å®å®¢è£½åçç§è­·ç­ç¥ååæçæ²»çä»å¥ãæ¬ç ç©¶ç± iDPP@CLEF 2024 ææ°å¸¶é ­ï¼å°æ³¨æ¼å©ç¨ééæç¨ç¨å¼åå¾çææ¸¬å¨è¡çè³æãéäºè³æç¨æ¼å»ºæ§åç¨®æ©å¨å­¸ç¿æ¨¡åï¼ç¹å¥è¨­è¨ç¨æ¼é æ¸¬ ALS åè½è©åéè¡¨ä¿®è¨ç (ALSFRS-R) åæ¸çé²å±ï¼ä¸¦å©ç¨ä¸»è¾¦å®ä½æä¾çè³æéãå¨æåçåæä¸­ï¼è©ä¼°äºå¤ç¨®é æ¸¬æ¨¡åï¼ä»¥ç¢ºå®å®åå¨èç ALS ææ¸¬å¨è³ææ¹é¢çæè½ãææ¸¬å¨è³æçæéé¢åä½¿ç¨çµ±è¨æ¹æ³é²è¡å£ç¸®ååä½µï¼å¾èå¢å¼·æ¶éè³è¨å¨é æ¸¬å»ºæ¨¡ç®æ¨æ¹é¢çå¯è§£éæ§åé©ç¨æ§ãè¡¨ç¾æä½³çæ¨¡åæ¯æ¨¸ç´ åºæºå ElasticNet åæ­¸ãæ¨¸ç´ æ¨¡åéå°äºå¹³åçµå°èª¤å·® (MAE) çº 0.20 ååæ¹æ ¹èª¤å·® (RMSE) çº 0.49ï¼ç¥åæ¼ ElasticNet æ¨¡åï¼å¾èç MAE çº 0.22ï¼RMSE çº 0.50ãæåçæ¯è¼åæè¡¨æï¼éç¶æ¨¸ç´ æ¹æ³ç¢ççé æ¸¬æºç¢ºåº¦ç¥é«ï¼ä½ ElasticNet æ¨¡åæä¾äºä¸åç©©å¥çæ¶æ§ï¼ç¨æ¼ç­è§£ç¹å¾µè²¢ç»ã

##### **The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing**
2407.07786v1 by Alice Qian Zhang, Ryland Shaw, Jacy Reese Anthis, Ashlee Milton, Emily Tseng, Jina Suh, Lama Ahmad, Ram Shankar Siva Kumar, Julian Posada, Benjamin Shestakofsky, Sarah T. Roberts, Mary L. Gray

Rapid progress in general-purpose AI has sparked significant interest in "red
teaming," a practice of adversarial testing originating in military and
cybersecurity applications. AI red teaming raises many questions about the
human factor, such as how red teamers are selected, biases and blindspots in
how tests are conducted, and harmful content's psychological effects on red
teamers. A growing body of HCI and CSCW literature examines related
practices-including data labeling, content moderation, and algorithmic
auditing. However, few, if any, have investigated red teaming itself. This
workshop seeks to consider the conceptual and empirical challenges associated
with this practice, often rendered opaque by non-disclosure agreements. Future
studies may explore topics ranging from fairness to mental health and other
areas of potential harm. We aim to facilitate a community of researchers and
practitioners who can begin to meet these challenges with creativity,
innovation, and thoughtful reflection.

æè¦ï¼ä¸è¬ç¨é AI çå¿«éé²å±å¼ç¼äºå°ãç´éãçæ¿åèè¶£ï¼ç´éæ¯ä¸ç¨®æºèªè»äºåç¶²è·¯å®å¨æç¨ä¸­çå°ææ§æ¸¬è©¦å¯¦åãAI ç´éå°äººé¡å ç´ æåºäºè¨±å¤åé¡ï¼ä¾å¦ç´éæå¡å¦ä½é¸æãæ¸¬è©¦å·è¡æ¹å¼ä¸­çåè¦åç²é»ï¼ä»¥åæå®³å§å®¹å°ç´éæå¡çå¿çå½±é¿ãè¶ä¾è¶å¤çäººæ©äºåå CSCW æç»æ¢è¨äºç¸éå¯¦åï¼åæ¬è³ææ¨è¨ãå§å®¹å¯©æ ¸åæ¼ç®æ³ç¨½æ ¸ãç¶èï¼é®®å°æäººæ¢è¨ç´éæ¬èº«ãæ¬å·¥ä½åæ¨å¨æ¢è¨èæ­¤å¯¦åç¸éçæ¦å¿µåç¶é©ææ°ï¼éäºææ°éå¸¸å ä¿å¯åè­°èè®å¾æ¨¡ç³ä¸æ¸ãæªä¾çç ç©¶å¯è½ææ¢è¨å¾å¬å¹³æ§å°å¿çå¥åº·åå¶ä»æ½å¨å±å®³é åçä¸»é¡ãæåçç®æ¨æ¯ä¿é²ç ç©¶äººå¡åå¯¦åå·¥ä½èçç¤¾ç¾¤ï¼ä»åå¯ä»¥éå§éç¨åµæãåµæ°åæ·±æçæ®çåæä¾æå°éäºææ°ã

##### **A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models : Safety, Consensus, Objectivity, Reproducibility and Explainability**
2407.07666v1 by Ting Fang Tan, Kabilan Elangovan, Jasmine Ong, Nigam Shah, Joseph Sung, Tien Yin Wong, Lan Xue, Nan Liu, Haibo Wang, Chang Fu Kuo, Simon Chesterman, Zee Kin Yeong, Daniel SW Ting

A comprehensive qualitative evaluation framework for large language models
(LLM) in healthcare that expands beyond traditional accuracy and quantitative
metrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,
Consensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We
suggest that S.C.O.R.E. may form the basis for an evaluation framework for
future LLM-based models that are safe, reliable, trustworthy, and ethical for
healthcare and clinical applications.

æè¦ï¼ä¸åå¨é¢çå®æ§è©ä¼°æ¶æ§ï¼é©ç¨æ¼é«çä¿å¥é åçå¤§åèªè¨æ¨¡å (LLM)ï¼å¶ç¯åè¶è¶å³çµ±çæºç¢ºåº¦åå®éææ¨ãæåæåºç¨æ¼è©ä¼° LLM ç 5 åééµé¢åï¼å®å¨æ§ãå±è­ãå®¢è§æ§ãå¯è¤è£½æ§åå¯è§£éæ§ (S.C.O.R.E.)ãæåå»ºè­° S.C.O.R.E. å¯ä»¥ä½çºè©ä¼°æ¶æ§çåºç¤ï¼é©ç¨æ¼æªä¾çåºæ¼ LLM çæ¨¡åï¼éäºæ¨¡åå°æ¼é«çä¿å¥åè¨åºæç¨ä¾èªªæ¯å®å¨ãå¯é ãå¼å¾ä¿¡è³´ä¸åä¹éå¾·çã

##### **Boosting Medical Image Synthesis via Registration-guided Consistency and Disentanglement Learning**
2407.07660v1 by Chuanpu Li, Zeli Chen, Yiwen Zhang, Liming Zhong, Wei Yang

Medical image synthesis remains challenging due to misalignment noise during
training. Existing methods have attempted to address this challenge by
incorporating a registration-guided module. However, these methods tend to
overlook the task-specific constraints on the synthetic and registration
modules, which may cause the synthetic module to still generate spatially
aligned images with misaligned target images during training, regardless of the
registration module's function. Therefore, this paper proposes
registration-guided consistency and incorporates disentanglement learning for
medical image synthesis. The proposed registration-guided consistency
architecture fosters task-specificity within the synthetic and registration
modules by applying identical deformation fields before and after synthesis,
while enforcing output consistency through an alignment loss. Moreover, the
synthetic module is designed to possess the capability of disentangling
anatomical structures and specific styles across various modalities. An anatomy
consistency loss is introduced to further compel the synthetic module to
preserve geometrical integrity within latent spaces. Experiments conducted on
both an in-house abdominal CECT-CT dataset and a publicly available pelvic
MR-CT dataset have demonstrated the superiority of the proposed method.

æè¦ï¼ç±æ¼è¨ç·´æéçé¯ä½éè¨ï¼é«å­¸å½±ååæä»ç¶å·æææ°æ§ãç¾ææ¹æ³å·²åè©¦ééç´å¥è¨»åå°å¼æ¨¡çµä¾è§£æ±ºæ­¤ææ°ãç¶èï¼éäºæ¹æ³å¾å¾å¿½ç¥åæèè¨»åæ¨¡çµçç¹å®ä»»åç´æï¼éå¯è½æå°è´åææ¨¡çµå¨è¨ç·´æéä»ç¢çèé¯ä½ç®æ¨å½±åç©ºéå°é½çå½±åï¼èèè¨»åæ¨¡çµçåè½ç¡éãå æ­¤ï¼æ¬ææåºè¨»åå°å¼ä¸è´æ§ï¼ä¸¦çµåè§£ç³¾çºå­¸ç¿ç¨æ¼é«å­¸å½±ååæãææåºçè¨»åå°å¼ä¸è´æ§æ¶æ§ééå¨åæåå¾æç¨ç¸åçè®å½¢å ´ï¼ä¸¦ééå°é½æå¤±ä¾å¼·å¶å·è¡è¼¸åºä¸è´æ§ï¼ä¾ä¿é²åæèè¨»åæ¨¡çµä¸­çä»»åç¹ç°æ§ãæ­¤å¤ï¼åææ¨¡çµè¢«è¨­è¨çºå·åå¨åç¨®æ¨¡æä¸­è§£éè§£åçµæ§åç¹å®æ¨£å¼çè½åãå¼å¥äºè§£åä¸è´æ§æå¤±ï¼ä»¥é²ä¸æ­¥å¼·å¶åææ¨¡çµå¨æ½å¨ç©ºéä¸­ä¿çå¹¾ä½å®æ´æ§ãå¨å§é¨è¹é¨ CECT-CT è³æéåå¬éå¯ç¨çéª¨ç MR-CT è³æéä¸é²è¡çå¯¦é©å·²è­æäºææåºæ¹æ³çåªè¶æ§ã

##### **H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for Occlusal Contact Segmentation with Articulating Paper**
2407.07604v1 by Ryan Banks, Bernat Rovira-Lastra, Jordi Martinez-Gomis, Akhilanand Chaurasia, Yunpeng Li

Occlusal contacts are the locations at which the occluding surfaces of the
maxilla and the mandible posterior teeth meet. Occlusal contact detection is a
vital tool for restoring the loss of masticatory function and is a mandatory
assessment in the field of dentistry, with particular importance in
prosthodontics and restorative dentistry. The most common method for occlusal
contact detection is articulating paper. However, this method can indicate
significant medically false positive and medically false negative contact
areas, leaving the identification of true occlusal indications to clinicians.
To address this, we propose a multiclass Vision Transformer and Fully
Convolutional Network ensemble semantic segmentation model with a combination
hierarchical loss function, which we name as Hierarchical Fully Convolutional
Branch Transformer (H-FCBFormer). We also propose a method of generating
medically true positive semantic segmentation masks derived from expert
annotated articulating paper masks and gold standard masks. The proposed model
outperforms other machine learning methods evaluated at detecting medically
true positive contacts and performs better than dentists in terms of accurately
identifying object-wise occlusal contact areas while taking significantly less
time to identify them. Code is available at
https://github.com/Banksylel/H-FCBFormer.

æè¦ï¼å¬åæ¥è§¸æ¯ä¸é¡åä¸é¡å¾çå¬åé¢ç¸éçä½ç½®ãå¬åæ¥è§¸åµæ¸¬æ¯æ¢å¾©åå¼åè½åªå¤±çå¿è¦å·¥å·ï¼ä¹æ¯çç§é åä¸­çä¸é å¼·å¶æ§è©ä¼°ï¼ç¹å¥æ¯å¨è´å¾©çç§åä¿®å¾©çç§ä¸­å·æéè¦æç¾©ãæå¸¸è¦çå¬åæ¥è§¸åµæ¸¬æ¹æ³æ¯ä½¿ç¨å¬åç´ãç¶èï¼æ­¤æ¹æ³å¯è½æé¡¯ç¤ºåºé¡¯èçé«å­¸åé½æ§åé«å­¸åé°æ§æ¥è§¸ååï¼è®è¨åºé«å¸«é£ä»¥æ¾åºçæ­£çå¬åè·¡è±¡ãçºäºè§£æ±ºéååé¡ï¼æåæåºä¸åå¤é¡å¥ç Vision Transformer åå¨å·ç©ç¶²è·¯éåèªæåå²æ¨¡åï¼ä¸¦çµååå±¤æå¤±å½æ¸ï¼æåå°å¶å½åçºåå±¤å¨å·ç©åæ¯è½æå¨ (H-FCBFormer)ãæåéæåºäºä¸ç¨®çæé«å­¸çé½æ§èªæåå²é®ç½©çæ¹æ³ï¼è©²æ¹æ³æºèªå°å®¶è¨»è§£çå¬åç´é®ç½©åéæ¨æºé®ç½©ãææåºçæ¨¡åå¨åµæ¸¬é«å­¸çé½æ§æ¥è§¸æ¹é¢åªæ¼å¶ä»æ©å¨å­¸ç¿æ¹æ³ï¼ä¸¦ä¸å¨æºç¢ºè­å¥ç©ä»¶å¼å¬åæ¥è§¸ååæ¹é¢åªæ¼çé«å¸«ï¼åæè­å¥æéæéå»é¡¯èæ¸å°ãç¨å¼ç¢¼å¯å¨ https://github.com/Banksylel/H-FCBFormer åå¾ã

##### **FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes**
2407.07561v1 by Rajat Kumar Jenamani, Priya Sundaresan, Maram Sakr, Tapomayukh Bhattacharjee, Dorsa Sadigh

Robot-assisted feeding has the potential to improve the quality of life for
individuals with mobility limitations who are unable to feed themselves
independently. However, there exists a large gap between the homogeneous,
curated plates existing feeding systems can handle, and truly in-the-wild
meals. Feeding realistic plates is immensely challenging due to the sheer range
of food items that a robot may encounter, each requiring specialized
manipulation strategies which must be sequenced over a long horizon to feed an
entire meal. An assistive feeding system should not only be able to sequence
different strategies efficiently in order to feed an entire meal, but also be
mindful of user preferences given the personalized nature of the task. We
address this with FLAIR, a system for long-horizon feeding which leverages the
commonsense and few-shot reasoning capabilities of foundation models, along
with a library of parameterized skills, to plan and execute user-preferred and
efficient bite sequences. In real-world evaluations across 6 realistic plates,
we find that FLAIR can effectively tap into a varied library of skills for
efficient food pickup, while adhering to the diverse preferences of 42
participants without mobility limitations as evaluated in a user study. We
demonstrate the seamless integration of FLAIR with existing bite transfer
methods [19, 28], and deploy it across 2 institutions and 3 robots,
illustrating its adaptability. Finally, we illustrate the real-world efficacy
of our system by successfully feeding a care recipient with severe mobility
limitations. Supplementary materials and videos can be found at:
https://emprise.cs.cornell.edu/flair .

æè¦ï¼æ©å¨äººè¼å©é²é£ææ½åæ¹åè¡åä¸ä¾¿ãç¡æ³èªè¡é²é£çåäººçæ´»åè³ªãç¶èï¼ç¾æçé²é£ç³»çµ±æè½èççåè³ªãç²¾é¸é¤ç¤èå¯¦éçé¤é»ä¹éå­å¨èå¾å¤§çå·®è·ãé²é£å¯¦éçé¤é»æ¥µå·ææ°æ§ï¼å çºæ©å¨äººå¯è½éå°çé£ç©ç¨®é¡ç¹å¤ï¼æ¯ç¨®é£ç©é½éè¦ç¹å®çæä½ç­ç¥ï¼èéäºç­ç¥å¿é å¨ä¸åé·æçç¯åå§é²è¡æåºï¼æè½é²é£ä¸æ´é¤ãä¸åè¼å©é²é£ç³»çµ±ä¸åæè©²è½å¤ ææå°å°ä¸åçç­ç¥é²è¡æåºï¼ä»¥ä¾¿é²é£ä¸æ´é¤ï¼éæè©²å¨ä»»åçåæ§åæ§è³ªä¸ï¼èéä½¿ç¨èçåå¥½ãæåéé FLAIR ä¾è§£æ±ºéååé¡ï¼FLAIR æ¯éå°é·æç¨é²é£çç³»çµ±ï¼å®å©ç¨åºç¤æ¨¡åçå¸¸è­åå°éæ¨çè½åï¼ä»¥åä¸ååæ¸åæè½åº«ï¼ä¾è¦ååå·è¡ä½¿ç¨èåå¥½ä¸ææçé²é£é åºãå¨ 6 åå¯¦éé¤ç¤ççå¯¦ä¸çè©ä¼°ä¸­ï¼æåç¼ç¾ FLAIR å¯ä»¥ææå°å©ç¨åç¨®æè½åº«é²è¡ææçé£ç©åç¨ï¼åæéµå® 42 ä½è¡åä¸ä¾¿åèèçä¸ååå¥½ï¼éæ¯å¨ä½¿ç¨èç ç©¶ä¸­è©ä¼°çãæåå±ç¤ºäº FLAIR èç¾æé²é£è½ç§»æ¹æ³ [19, 28] çç¡ç¸«æ´åï¼ä¸¦å¨ 2 åæ©æ§å 3 åæ©å¨äººä¸­é¨ç½²å®ï¼èªªæäºå®çé©ææ§ãæå¾ï¼æåééæåé¤µé£ä¸ä½è¡åä¸ä¾¿çåç§è­·èä¾èªªææåç³»çµ±å¨çå¯¦ä¸çä¸­çåæãè£åææåå½±çå¯ä»¥å¨éè£¡æ¾å°ï¼https://emprise.cs.cornell.edu/flairã

##### **Arabic Automatic Story Generation with Large Language Models**
2407.07551v1 by Ahmed Oumar El-Shangiti, Fakhraddin Alwajih, Muhammad Abdul-Mageed

Large language models (LLMs) have recently emerged as a powerful tool for a
wide range of language generation tasks. Nevertheless, this progress has been
slower in Arabic. In this work, we focus on the task of generating stories from
LLMs. For our training, we use stories acquired through machine translation
(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that
ensures we acquire high-quality stories. For our GPT-41 data, we introduce
crafted prompts that allow us to generate data well-suited to the Arabic
context in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian
and Moroccan). For example, we generate stories tailored to various Arab
countries on a wide host of topics. Our manual evaluation shows that our model
fine-tuned on these training datasets can generate coherent stories that adhere
to our instructions. We also conduct an extensive automatic and human
evaluation comparing our models against state-of-the-art proprietary and
open-source models. Our datasets and models will be made publicly available at
https: //github.com/UBC-NLP/arastories.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼æè¿å·²æçºåç¨®èªè¨çæä»»åçå¼·å¤§å·¥å·ãåç®¡å¦æ­¤ï¼éé é²å±å¨é¿æä¼¯èªä¸­è¼çºç·©æ¢ãå¨éé å·¥ä½ä¸­ï¼æåå°æ³¨æ¼å¾ LLM çææäºçä»»åãå°æ¼æåçè¨ç·´ï¼æåä½¿ç¨ééæ©å¨ç¿»è­¯ï¼MTï¼ä»¥å GPT-4 ç²å¾çæäºãå°æ¼ MT è³æï¼æåéç¼äºä¸åä»ç´°çç®¡éï¼ä»¥ç¢ºä¿æåç²å¾é«åè³ªçæäºãå°æ¼æåç GPT-41 è³æï¼æåå¼å¥äºç²¾å¿è£½ä½çæç¤ºï¼ä½¿æåè½å¤ çæéå¸¸é©åé¿æä¼¯èªç°å¢çè³æï¼åæ¬ç¾ä»£æ¨æºé¿æä¼¯èªï¼MSAï¼åå©ç¨®é¿æä¼¯èªæ¹è¨ï¼ååèªåæ©æ´å¥èªï¼ãä¾å¦ï¼æåçæéå°åç¨®é¿æä¼¯åå®¶çæäºï¼ä¸»é¡å»£æ³ãæåçè©ä¼°é¡¯ç¤ºï¼æåéå°éäºè¨ç·´è³æéé²è¡å¾®èª¿çæ¨¡åå¯ä»¥çæç¬¦åæåæç¤ºçé£è²«æäºãæåéé²è¡äºå»£æ³çèªååäººå·¥è©ä¼°ï¼å°æåçæ¨¡åèæåé²çå°æåéæ¾åå§ç¢¼æ¨¡åé²è¡æ¯è¼ãæåçè³æéåæ¨¡åå°å¨ https: //github.com/UBC-NLP/arastories å¬éã

##### **Weakly-supervised Medical Image Segmentation with Gaze Annotations**
2407.07406v1 by Yuan Zhong, Chenhui Tang, Yumeng Yang, Ruoxi Qi, Kang Zhou, Yuqi Gong, Pheng Ann Heng, Janet H. Hsiao, Qi Dou

Eye gaze that reveals human observational patterns has increasingly been
incorporated into solutions for vision tasks. Despite recent explorations on
leveraging gaze to aid deep networks, few studies exploit gaze as an efficient
annotation approach for medical image segmentation which typically entails
heavy annotating costs. In this paper, we propose to collect dense weak
supervision for medical image segmentation with a gaze annotation scheme. To
train with gaze, we propose a multi-level framework that trains multiple
networks from discriminative human attention, simulated with a set of
pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps.
Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to
regularize overfitting noisy labels, steering models toward clean patterns
learned by peer networks. The proposed method is validated on two public
medical datasets of polyp and prostate segmentation tasks. We contribute a
high-quality gaze dataset entitled GazeMedSeg as an extension to the popular
medical segmentation datasets. To the best of our knowledge, this is the first
gaze dataset for medical image segmentation. Our experiments demonstrate that
gaze annotation outperforms previous label-efficient annotation schemes in
terms of both performance and annotation time. Our collected gaze data and code
are available at: https://github.com/med-air/GazeMedSeg.

æè¦ï¼äººç±»è§å¯æ¨¡å¼çç¼çæ³¨è§å·²è¶æ¥è¶å¤å°èå¥è§è§ä»»å¡çè§£å³æ¹æ¡ä¸­ãå°½ç®¡æè¿æ¢ç´¢äºå©ç¨æ³¨è§æ¥è¾å©æ·±åº¦ç½ç»ï¼ä½å¾å°æç ç©¶å©ç¨æ³¨è§ä½ä¸ºå»å­¦å¾ååå²çæææ³¨éæ¹æ³ï¼è¿éå¸¸éè¦å¤§éçæ³¨éææ¬ãå¨æ¬æä¸­ï¼æä»¬æåºæ¶éå¯éçå¼±çç£ï¼ç¨äºå·æåè§æ³¨éæ¹æ¡çå»å­¦å¾ååå²ãä¸ºäºç¨æ³¨è§è¿è¡è®­ç»ï¼æä»¬æåºäºä¸ä¸ªå¤çº§æ¡æ¶ï¼è¯¥æ¡æ¶ä»åºåæ§äººç±»æ³¨æåè®­ç»å¤ä¸ªç½ç»ï¼å¹¶éè¿å¨åè§ç­å¾ä¸åºç¨åå±éå¼æ¥æ¨¡æä¸ç»ä¼ªæ©ç ãæ­¤å¤ï¼ä¸ºäºåè½»æ³¨è§åªå£°ï¼å©ç¨è·¨çº§ä¸è´æ§æ¥æ­£ååè¿åº¦æåçåªå£°æ ç­¾ï¼å°æ¨¡åå¼å¯¼è³ç±å¯¹ç­ç½ç»å­¦ä¹ çå¹²åæ¨¡å¼ãææåºçæ¹æ³å·²å¨ä¸¤ä¸ªå¬å±å»å­¦æ°æ®éçå¤æ¯èåååèºåå²ä»»å¡ä¸å¾å°éªè¯ãæä»¬è´¡ç®äºä¸ä¸ªåä¸º GazeMedSeg çé«è´¨éåè§æ°æ®éï¼ä½ä¸ºæµè¡å»å­¦åå²æ°æ®éçæ©å±ãæ®æä»¬æç¥ï¼è¿æ¯å»å­¦å¾ååå²çç¬¬ä¸ä¸ªåè§æ°æ®éãæä»¬çå®éªè¡¨æï¼å¨æ§è½åæ³¨éæ¶é´æ¹é¢ï¼åè§æ³¨éä¼äºä»¥åçæ ç­¾é«ææ³¨éæ¹æ¡ãæä»¬æ¶éçåè§æ°æ®åä»£ç å¯å¨ä»¥ä¸ä½ç½®è·å¾ï¼https://github.com/med-air/GazeMedSegã

##### **Interpretable Differential Diagnosis with Dual-Inference Large Language Models**
2407.07330v1 by Shuang Zhou, Sirui Ding, Jiashuo Wang, Mingquan Lin, Genevieve B. Melton, Rui Zhang

Methodological advancements to automate the generation of differential
diagnosis (DDx) to predict a list of potential diseases as differentials given
patients' symptom descriptions are critical to clinical reasoning and
applications such as decision support. However, providing reasoning or
interpretation for these differential diagnoses is more meaningful.
Fortunately, large language models (LLMs) possess powerful language processing
abilities and have been proven effective in various related tasks. Motivated by
this potential, we investigate the use of LLMs for interpretable DDx. First, we
develop a new DDx dataset with expert-derived interpretation on 570 public
clinical notes. Second, we propose a novel framework, named Dual-Inf, that
enables LLMs to conduct bidirectional inference for interpretation. Both human
and automated evaluation demonstrate the effectiveness of Dual-Inf in
predicting differentials and diagnosis explanations. Specifically, the
performance improvement of Dual-Inf over the baseline methods exceeds 32%
w.r.t. BERTScore in DDx interpretation. Furthermore, experiments verify that
Dual-Inf (1) makes fewer errors in interpretation, (2) has great
generalizability, (3) is promising for rare disease diagnosis and explanation.

æè¦ï¼æ¹æ³å­¸çé²å±èªååçæå·®ç°è¨ºæ· (DDx)ï¼ä»¥é æ¸¬çµ¦å®æ£èççæè¿°çæ½å¨ç¾çæ¸å®ï¼å°æ¼è¨åºæ¨çåæ±ºç­æ¯æ´ç­æç¨è³ééè¦ãç¶èï¼æä¾éäºå·®ç°è¨ºæ·çæ¨çæè§£éæ´ææç¾©ãå¹¸éçæ¯ï¼å¤§åèªè¨æ¨¡å (LLM) ææå¼·å¤§çèªè¨èçè½åï¼ä¸¦å·²è¢«è­æå¨åç¨®ç¸éä»»åä¸­ææãåæ­¤æ½åçæ¿åµï¼æåç ç©¶äº LLM å¨å¯è§£éç DDx ä¸­çæç¨ãé¦åï¼æåéç¼äºä¸åæ°ç DDx æ¸æéï¼å¶ä¸­åå«å°å®¶å° 570 åå¬å±è¨åºç­è¨çè§£éãå¶æ¬¡ï¼æåæåºäºä¸ååçº Dual-Inf çæ°æ¡æ¶ï¼å®ä½¿ LLM è½å¤ é²è¡éåæ¨çä»¥é²è¡è§£éãäººé¡åèªååè©ä¼°é½è­æäº Dual-Inf å¨é æ¸¬å·®ç°åè¨ºæ·è§£éæ¹é¢çæææ§ãå·é«ä¾èªªï¼Dual-Inf å¨ DDx è§£éä¸­è¶éåºç·æ¹æ³çæ§è½æ¹é²è¶é 32% w.r.t. BERTScoreãæ­¤å¤ï¼å¯¦é©é©è­äº Dual-Inf (1) å¨è§£éä¸­ç¢çè¼å°çé¯èª¤ï¼(2) å·æå¾å¥½çæ¦æ¬æ§ï¼(3) å°ç½è¦ç¾ççè¨ºæ·åè§£éå¾æåæ¯ã

##### **Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy**
2407.07296v1 by Praveenbalaji Rajendran, Yong Yang, Thomas R. Niedermayr, Michael Gensheimer, Beth Beadle, Quynh-Thu Le, Lei Xing, Xianjin Dai

Radiation therapy (RT) is one of the most effective treatments for cancer,
and its success relies on the accurate delineation of targets. However, target
delineation is a comprehensive medical decision that currently relies purely on
manual processes by human experts. Manual delineation is time-consuming,
laborious, and subject to interobserver variations. Although the advancements
in artificial intelligence (AI) techniques have significantly enhanced the
auto-contouring of normal tissues, accurate delineation of RT target volumes
remains a challenge. In this study, we propose a visual language model-based RT
target volume auto-delineation network termed Radformer. The Radformer utilizes
a hierarichal vision transformer as the backbone and incorporates large
language models to extract text-rich features from clinical data. We introduce
a visual language attention module (VLAM) for integrating visual and linguistic
features for language-aware visual encoding (LAVE). The Radformer has been
evaluated on a dataset comprising 2985 patients with head-and-neck cancer who
underwent RT. Metrics, including the Dice similarity coefficient (DSC),
intersection over union (IOU), and 95th percentile Hausdorff distance (HD95),
were used to evaluate the performance of the model quantitatively. Our results
demonstrate that the Radformer has superior segmentation performance compared
to other state-of-the-art models, validating its potential for adoption in RT
practice.

æè¦ï¼æ¾å°æ²»ç (RT) æ¯æææçççæ²»çæ¹æ³ä¹ä¸ï¼å¶æåæè³´æ¼ç®æ¨çæºç¢ºæç¹ªãç¶èï¼ç®æ¨æç¹ªæ¯ä¸é å¨é¢çé«çæ±ºç­ï¼ç®åå®å¨ä¾è³´äººé¡å°å®¶çæåç¨åºãæåæç¹ªèæãè²»åï¼ä¸åè§å¯èéå·®ç°å½±é¿ãåç®¡äººå·¥æºæ§ (AI) æè¡çé²æ­¥å·²é¡¯èå¢å¼·æ­£å¸¸çµç¹çèªåè¼ªå»æç¹ªï¼ä½ RT ç®æ¨é«ç©çæºç¢ºæç¹ªä»æ¯ä¸é ææ°ãå¨æ¬ç ç©¶ä¸­ï¼æåæåºä¸ååºæ¼è¦è¦ºèªè¨æ¨¡åç RT ç®æ¨é«ç©èªåæç¹ªç¶²è·¯ï¼ç¨±çº RadformerãRadformer å©ç¨éå±¤å¼è¦è¦ºTransformerä½çºä¸»å¹¹ï¼ä¸¦æ´åå¤§åèªè¨æ¨¡åå¾è¨åºè³æä¸­æåè±å¯æå­ç¹å¾µãæåå¼å¥ä¸åè¦è¦ºèªè¨æ³¨æåæ¨¡çµ (VLAM)ï¼ç¨æ¼æ´åè¦è¦ºåèªè¨ç¹å¾µï¼ä»¥é²è¡èªè¨æç¥è¦è¦ºç·¨ç¢¼ (LAVE)ãRadformer å·²å¨ä¸ååå« 2985 åæ¥å RT æ²»ççé ­é ¸çæ£èçè³æéä¸é²è¡è©ä¼°ãææ¨ï¼åæ¬ Dice ç¸ä¼¼ä¿æ¸ (DSC)ãè¯éæ¯ (IOU) åç¬¬ 95 åç¾åä½æ¸ Hausdorff è·é¢ (HD95)ï¼ç¨æ¼å®éè©ä¼°æ¨¡åçæè½ãæåççµæè¡¨æï¼èå¶ä»æåé²çæ¨¡åç¸æ¯ï¼Radformer å·æåªç°çåå²æè½ï¼é©è­äºå¶å¨ RT å¯¦åä¸­æç¨çæ½åã

##### **Causal Discovery in Semi-Stationary Time Series**
2407.07291v1 by Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan A. Rossi, Murat Kocaoglu

Discovering causal relations from observational time series without making
the stationary assumption is a significant challenge. In practice, this
challenge is common in many areas, such as retail sales, transportation
systems, and medical science. Here, we consider this problem for a class of
non-stationary time series. The structural causal model (SCM) of this type of
time series, called the semi-stationary time series, exhibits that a finite
number of different causal mechanisms occur sequentially and periodically
across time. This model holds considerable practical utility because it can
represent periodicity, including common occurrences such as seasonality and
diurnal variation. We propose a constraint-based, non-parametric algorithm for
discovering causal relations in this setting. The resulting algorithm,
PCMCI$_{\Omega}$, can capture the alternating and recurring changes in the
causal mechanisms and then identify the underlying causal graph with
conditional independence (CI) tests. We show that this algorithm is sound in
identifying causal relations on discrete time series. We validate the algorithm
with extensive experiments on continuous and discrete simulated data. We also
apply our algorithm to a real-world climate dataset.

æè¦ï¼å¨ä¸ä½å¹³ç©©åè¨­çææ³ä¸å¾è§æ¸¬æéåºåä¸­ç¼ç¾å æéä¿æ¯ä¸é éå¤§ææ°ãå¨å¯¦åä¸­ï¼éåææ°å¨è¨±å¤é åä¸­å¾å¸¸è¦ï¼ä¾å¦é¶å®é·å®ãéè¼¸ç³»çµ±åé«å­¸ç§å­¸ãå¨æ­¤ï¼æåèæ®éå¹³ç©©æéåºåé¡å¥çéååé¡ãéç¨®é¡åçæéåºåççµæ§å ææ¨¡å (SCM)ï¼ç¨±çºåå¹³ç©©æéåºåï¼å±ç¤ºäºæéæ¸éçä¸åå ææ©å¶æé¨èæéé åºä¸é±ææ§å°ç¼çãéåæ¨¡åå·æç¸ç¶å¤§çå¯¦ç¨æ§ï¼å çºå®å¯ä»¥è¡¨ç¤ºé±ææ§ï¼åæ¬å­£ç¯æ§åæå¤è®åç­å¸¸è¦ç¾è±¡ãæåæåºäºä¸ååºæ¼ç´æçéåæ¸æ¼ç®æ³ï¼ç¨æ¼ç¼ç¾éåè¨­å®ä¸­çå æéä¿ãç¢ççæ¼ç®æ³ PCMCI$_{\Omega}$ å¯ä»¥ææå ææ©å¶ä¸­çäº¤æ¿åéè¤è®åï¼ç¶å¾èç±æ¢ä»¶ç¨ç« (CI) æª¢å®ä¾è­å¥åºç¤å æåãæåè­æéåæ¼ç®æ³å¨è­å¥é¢æ£æéåºåä¸çå æéä¿ææ¯åççãæåä½¿ç¨é£çºåé¢æ£æ¨¡æ¬è³æé²è¡å»£æ³çå¯¦é©ï¼ä»¥é©è­æ¼ç®æ³ãæåä¹å°æåçæ¼ç®æ³æç¨æ¼çå¯¦ä¸ççæ°£åè³æéã

##### **Causal Discovery-Driven Change Point Detection in Time Series**
2407.07290v1 by Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan A. Rossi, Murat Kocaoglu

Change point detection in time series seeks to identify times when the
probability distribution of time series changes. It is widely applied in many
areas, such as human-activity sensing and medical science. In the context of
multivariate time series, this typically involves examining the joint
distribution of high-dimensional data: If any one variable changes, the whole
time series is assumed to have changed. However, in practical applications, we
may be interested only in certain components of the time series, exploring
abrupt changes in their distributions in the presence of other time series.
Here, assuming an underlying structural causal model that governs the
time-series data generation, we address this problem by proposing a two-stage
non-parametric algorithm that first learns parts of the causal structure
through constraint-based discovery methods. The algorithm then uses conditional
relative Pearson divergence estimation to identify the change points. The
conditional relative Pearson divergence quantifies the distribution disparity
between consecutive segments in the time series, while the causal discovery
method enables a focus on the causal mechanism, facilitating access to
independent and identically distributed (IID) samples. Theoretically, the
typical assumption of samples being IID in conventional change point detection
methods can be relaxed based on the Causal Markov Condition. Through
experiments on both synthetic and real-world datasets, we validate the
correctness and utility of our approach.

æè¦ï¼æéåºåçè®ç°é»åµæ¸¬æ¨å¨æ¾åºæéåºåçæ©çåä½æ¹è®çæéãå®å»£æ³æç¨æ¼è¨±å¤é åï¼ä¾å¦äººé¡æ´»åææ¸¬èé«å­¸ç§å­¸ãå¨å¤è®éæéåºåçèæ¯ä¸­ï¼ééå¸¸æ¶åæª¢è¦é«ç¶­åº¦è³æçè¯ååä½ï¼å¦æä»»ä½ä¸åè®æ¸æ¹è®ï¼ååè¨­æ´åæéåºåå·²ç¶æ¹è®ãç¶èï¼å¨å¯¦éæç¨ä¸­ï¼æåå¯è½åªå°æéåºåçç¹å®çµæé¨åæèè¶£ï¼æ¢ç´¢å®åçåä½å¨å¶ä»æéåºåå­å¨çææ³ä¸çªç¶æ¹è®ãå¨éè£¡ï¼åè¨­ä¸ååºç¤ççµæ§å ææ¨¡åæ¯éèæéåºåè³æççæï¼æåééæåºä¸åå©éæ®µéåæ¸æ¼ç®æ³ä¾è§£æ±ºéååé¡ï¼è©²æ¼ç®æ³é¦åééåºæ¼ç´æçç¼ç¾æ¹æ³ä¾å­¸ç¿å æçµæ§çé¨åãç¶å¾ï¼è©²æ¼ç®æ³ä½¿ç¨æ¢ä»¶ç¸å° Pearson å·®ç°ä¼°è¨ä¾æ¾åºè®ç°é»ãæ¢ä»¶ç¸å° Pearson å·®ç°éåæéåºåä¸­é£çºåæ®µä¹éçåä½å·®ç°ï¼èå æç¼ç¾æ¹æ³å¯ä»¥å°æ³¨æ¼å ææ©å¶ï¼ä¿é²åå¾ç¨ç«ä¸ååå¸ (IID) çæ¨£æ¬ãçè«ä¸ï¼æ¨£æ¬çº IID çå¸ååè¨­å¨å³çµ±è®ç°é»åµæ¸¬æ¹æ³ä¸­å¯ä»¥æ ¹æå æé¦¬å¯å¤«æ¢ä»¶æ¾å¯¬ãééå¨åæåçå¯¦ä¸çè³æéä¸é²è¡å¯¦é©ï¼æåé©è­äºæåæ¹æ³çæ­£ç¢ºæ§åå¯¦ç¨æ§ã

##### **Lifestyle-Informed Personalized Blood Biomarker Prediction via Novel Representation Learning**
2407.07277v1 by A. Ali Heydari, Naghmeh Rezaei, Javier L. Prieto, Shwetak N. Patel, Ahmed A. Metwally

Blood biomarkers are an essential tool for healthcare providers to diagnose,
monitor, and treat a wide range of medical conditions. Current reference values
and recommended ranges often rely on population-level statistics, which may not
adequately account for the influence of inter-individual variability driven by
factors such as lifestyle and genetics. In this work, we introduce a novel
framework for predicting future blood biomarker values and define personalized
references through learned representations from lifestyle data (physical
activity and sleep) and blood biomarkers. Our proposed method learns a
similarity-based embedding space that captures the complex relationship between
biomarkers and lifestyle factors. Using the UK Biobank (257K participants), our
results show that our deep-learned embeddings outperform traditional and
current state-of-the-art representation learning techniques in predicting
clinical diagnosis. Using a subset of UK Biobank of 6440 participants who have
follow-up visits, we validate that the inclusion of these embeddings and
lifestyle factors directly in blood biomarker models improves the prediction of
future lab values from a single lab visit. This personalized modeling approach
provides a foundation for developing more accurate risk stratification tools
and tailoring preventative care strategies. In clinical settings, this
translates to the potential for earlier disease detection, more timely
interventions, and ultimately, a shift towards personalized healthcare.

æè¦ï¼è¡æ¶²çç©æ¨è¨æ¯é«çä¿å¥æä¾èç¨æ¼è¨ºæ·ãç£æ¸¬åæ²»çåç¨®ç¾ççéè¦å·¥å·ãç®åçåèå¼åå»ºè­°ç¯åéå¸¸ä¾è³´æ¼äººç¾¤çµ±è¨æ¸æï¼èéäºæ¸æå¯è½ç¡æ³ååèªªæç±çæ´»æ¹å¼ååºå ç­å ç´ é©åçåé«éè®ç°çå½±é¿ãå¨éé å·¥ä½ä¸­ï¼æåå¼å¥äºä¸åæ°çæ¡æ¶ä¾é æ¸¬æªä¾çè¡æ¶²çç©æ¨è¨å¼ï¼ä¸¦ééå¾çæ´»æ¹å¼æ¸æï¼èº«é«æ´»ååç¡ç ï¼åè¡æ¶²çç©æ¨è¨ä¸­å­¸ç¿å°çè¡¨å¾µä¾å®ç¾©åæ§ååèãæåæåºçæ¹æ³å­¸ç¿äºä¸ååºæ¼ç¸ä¼¼æ§çåµå¥ç©ºéï¼è©²ç©ºéææäºçç©æ¨è¨åçæ´»æ¹å¼å ç´ ä¹éçè¤ééä¿ãä½¿ç¨è±åçç©éè¡ï¼257K åèèï¼ï¼æåççµæè¡¨æï¼æåæ·±åº¦å­¸ç¿çåµå¥åªæ¼å³çµ±åç¶åæåé²çè¡¨å¾µå­¸ç¿æè¡ï¼å¯ä»¥é æ¸¬è¨åºè¨ºæ·ãä½¿ç¨ææå¾çºè¨ªè¦ç 6440 ååèèçè±åçç©éè¡å­éï¼æåé©è­äºå¨è¡æ¶²çç©æ¨è¨æ¨¡åä¸­ç´æ¥åå«éäºåµå¥åçæ´»æ¹å¼å ç´ å¯ä»¥æ¹åå¾å®æ¬¡å¯¦é©å®¤è¨ªåä¸­é æ¸¬æªä¾å¯¦é©å®¤å¼ãéç¨®åæ§åå»ºæ¨¡æ¹æ³çºéç¼æ´æºç¢ºçé¢¨éªåå±¤å·¥å·åå®å¶é é²ä¿å¥ç­ç¥æä¾äºåºç¤ãå¨è¨åºç°å¢ä¸­ï¼éè½åçºæ©æç¾çæª¢æ¸¬ãæ´åæçå¹²é ï¼æçµè½ååæ§åé«çä¿å¥çæ½åã

##### **ProtoSAM -- One Shot Medical Image Segmentation With Foundational Models**
2407.07042v1 by Lev Ayzenberg, Raja Giryes, Hayit Greenspan

This work introduces a new framework, ProtoSAM, for one-shot medical image
segmentation. It combines the use of prototypical networks, known for few-shot
segmentation, with SAM - a natural image foundation model. The method proposed
creates an initial coarse segmentation mask using the ALPnet prototypical
network, augmented with a DINOv2 encoder. Following the extraction of an
initial mask, prompts are extracted, such as points and bounding boxes, which
are then input into the Segment Anything Model (SAM). State-of-the-art results
are shown on several medical image datasets and demonstrate automated
segmentation capabilities using a single image example (one shot) with no need
for fine-tuning of the foundation model.

æè¦ï¼æ¬ç ç©¶æåºä¸åæ°çæ¶æ§ï¼ProtoSAMï¼ç¨æ¼ä¸æ¬¡æ§é«å­¸å½±ååå²ãå®çµåäºååç¶²è·¯çä½¿ç¨ï¼ä»¥é²è¡å°æ¬¡åå²ï¼ä»¥å SAM - ä¸åèªç¶å½±ååºç¤æ¨¡åãææåºçæ¹æ³ä½¿ç¨ ALPnet ååç¶²è·¯å»ºç«ä¸ååå§çç²ç¥åå²é®ç½©ï¼ä¸¦ä½¿ç¨ DINOv2 ç·¨ç¢¼å¨é²è¡æ´åãå¨æååå§é®ç½©å¾ï¼ææåæç¤ºï¼ä¾å¦é»åéçæ¡ï¼ç¶å¾å°å¶è¼¸å¥å° Segment Anything Model (SAM) ä¸­ãå¨å¤åé«å­¸å½±åè³æéä¸é¡¯ç¤ºäºæåé²ççµæï¼ä¸¦å±ç¤ºäºä½¿ç¨å®ä¸å½±åç¯ä¾ï¼ä¸æ¬¡æ§ï¼çèªååå²åè½ï¼ç¡éå¾®èª¿åºç¤æ¨¡åã

##### **Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**
2407.07009v1 by Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier

The support of artificial intelligence (AI) based decision-making is a key
element in future 6G networks, where the concept of native AI will be
introduced. Moreover, AI is widely employed in different critical applications
such as autonomous driving and medical diagnosis. In such applications, using
AI as black-box models is risky and challenging. Hence, it is crucial to
understand and trust the decisions taken by these models. Tackling this issue
can be achieved by developing explainable AI (XAI) schemes that aim to explain
the logic behind the black-box model behavior, and thus, ensure its efficient
and safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST
framework that is oriented toward channel estimation in wireless
communications. The core idea of the XAI-CHEST framework is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. This
manuscript provides the detailed theoretical foundations of the XAI-CHEST
framework. In particular, we derive the analytical expressions of the XAI-CHEST
loss functions and the noise threshold fine-tuning optimization problem. Hence
the designed XAI-CHEST delivers a smart input feature selection methodology
that can further improve the overall performance while optimizing the
architecture of the employed model. Simulation results show that the XAI-CHEST
framework provides valid interpretations, where it offers an improved bit error
rate performance while reducing the required computational complexity in
comparison to the classical DL-based channel estimation.

æè¦ï¼äººå·¥æºè½ (AI) æ¯æçæ±ºç­å¶å®æ¯æªä¾ 6G ç¶²è·¯ä¸­çééµåç´ ï¼å¶ä¸­å°å¼å¥åç AI çæ¦å¿µãæ­¤å¤ï¼AI å»£æ³ç¨æ¼ä¸åçééµæç¨ä¸­ï¼ä¾å¦èªåé§é§åé«çè¨ºæ·ãå¨éäºæç¨ä¸­ï¼ä½¿ç¨ AI ä½çºé»çæ¨¡åæ¯æé¢¨éªä¸å·æææ°æ§çãå æ­¤ï¼çè§£åä¿¡ä»»éäºæ¨¡åååºçæ±ºç­è³ééè¦ãè§£æ±ºæ­¤åé¡çæ¹æ³æ¯éç¼å¯è§£é AI (XAI) æ¶æ§ï¼æ¨å¨è§£éé»çæ¨¡åè¡çºèå¾çéè¼¯ï¼å¾èç¢ºä¿å¶ææä¸å®å¨çé¨ç½²ãæè¿ï¼æåæåºäºä¸åæ°çåºæ¼æ¾åç XAI-CHEST æ¡æ¶ï¼è©²æ¡æ¶é¢åç¡ç·éä¿¡ä¸­çä¿¡éä¼°è¨ãXAI-CHEST æ¡æ¶çæ ¸å¿ææ³æ¯ééå¨ç¡éè¼¸å¥ä¸å¼å¥é«åªè²ä¾è­å¥ç¸éæ¨¡åè¼¸å¥ãéä»½æç¨¿æä¾äº XAI-CHEST æ¡æ¶çè©³ç´°çè«åºç¤ãç¹å¥æ¯ï¼æåæ¨å°äº XAI-CHEST æå¤±å½æ¸ååªè²é¾å¼å¾®èª¿åªååé¡çè§£æè¡¨éå¼ãå æ­¤ï¼è¨­è¨ç XAI-CHEST æä¾äºä¸ç¨®æºè½è¼¸å¥ç¹å¾µé¸ææ¹æ³ï¼å¯ä»¥å¨åªåæç¨æ¨¡åçæ¶æ§çåæé²ä¸æ­¥æé«æ´é«æ§è½ãæ¨¡æ¬çµæè¡¨æï¼XAI-CHEST æ¡æ¶æä¾äºææçè§£éï¼å¨éä½æéçè¨ç®è¤éåº¦çåæï¼æä¾äºæ¹é²çæ¯ç¹é¯èª¤çæ§è½ï¼èéèåºæ¼å³çµ± DL çä¿¡éä¼°è¨ç¸æ¯ã

##### **Microsoft Cloud-based Digitization Workflow with Rich Metadata Acquisition for Cultural Heritage Objects**
2407.06972v1 by Krzysztof Kutt, Jakub GomuÅka, Luiz do Valle Miranda, Grzegorz J. Nalepa

In response to several cultural heritage initiatives at the Jagiellonian
University, we have developed a new digitization workflow in collaboration with
the Jagiellonian Library (JL). The solution is based on easy-to-access
technological solutions -- Microsoft 365 cloud with MS Excel files as metadata
acquisition interfaces, Office Script for validation, and MS Sharepoint for
storage -- that allows metadata acquisition by domain experts (philologists,
historians, philosophers, librarians, archivists, curators, etc.) regardless of
their experience with information systems. The ultimate goal is to create a
knowledge graph that describes the analyzed holdings, linked to general
knowledge bases, as well as to other cultural heritage collections, so careful
attention is paid to the high accuracy of metadata and proper links to external
sources. The workflow has already been evaluated in two pilots in the DiHeLib
project focused on digitizing the so-called "Berlin Collection" and in two
workshops with international guests, which allowed for its refinement and
confirmation of its correctness and usability for JL. As the proposed workflow
does not interfere with existing systems or domain guidelines regarding
digitization and basic metadata collection in a given institution (e.g., file
type, image quality, use of Dublin Core/MARC-21), but extends them in order to
enable rich metadata collection, not previously possible, we believe that it
could be of interest to all GLAMs (galleries, libraries, archives, and
museums).

æè¦ï¼<paragraph>çºåæ Jagiello å¤§å­¸çæ¸åæåéºç¢å¡è­°ï¼æåè Jagiello åæ¸é¤¨ (JL) åä½éç¼ä¸åæ°çæ¸ä½åå·¥ä½æµç¨ãæ­¤è§£æ±ºæ¹æ¡åºæ¼ææ¼å­åçæè¡è§£æ±ºæ¹æ¡ï¼åæ¬ï¼ä½çºåè³ææ·åä»é¢ç Microsoft 365 é²ç«¯è MS Excel æªæ¡ãç¨æ¼é©è­ç Office Scriptï¼ä»¥åç¨æ¼å²å­ç MS Sharepointï¼å®åè¨±é åå°å®¶ï¼èªè¨å­¸å®¶ãæ­·å²å­¸å®¶ãå²å­¸å®¶ãåæ¸é¤¨å¡ãæªæ¡ç®¡çå¡ãç­å±äººç­ï¼æ·ååè³æï¼èç¡é å·åè³è¨ç³»çµ±æ¹é¢çç¶é©ãæçµç®æ¨æ¯å»ºç«ä¸åç¥è­åè­ï¼ç¨ä»¥æè¿°æåæçé¤¨èï¼ä¸¦é£çµè³ä¸è¬ç¥è­åº«ï¼ä»¥åå¶ä»æåéºç¢é¤¨èï¼å æ­¤æåéå¸¸éè¦åè³æçé«æºç¢ºæ§ï¼ä»¥åèå¤é¨ä¾æºçé©ç¶é£çµãæ­¤å·¥ä½æµç¨å·²å¨ DiHeLib å°æ¡ä¸­å©åè©¦é»è¨ç«ä¸­é²è¡è©ä¼°ï¼è©²å°æ¡å°æ³¨æ¼æ¸ä½åæè¬çãææé¤¨èãï¼ä»¥åèåéè¨ªå®¢é²è¡çå©åå·¥ä½åï¼éè®æåå¾ä»¥æ¹åå·¥ä½æµç¨ï¼ä¸¦ç¢ºèªå¶æ­£ç¢ºæ§ï¼ä»¥åå° JL çå¯ç¨æ§ãç±æ¼ææåºçå·¥ä½æµç¨ä¸æå¹²æ¾æ¢æç³»çµ±æéæ¼æ¸ä½åååºæ¬åè³æèéçé åæåï¼ä¾å¦ï¼æªæ¡é¡åãå½±ååè³ªãä½¿ç¨ Dublin Core/MARC-21ï¼ï¼èæ¯æ´åéäºç³»çµ±ï¼ä»¥æ¯æ´ä»¥åç¡æ³é²è¡çè±å¯åè³æèéï¼å æ­¤æåç¸ä¿¡å®å¯è½æå¼èµ·ææ GLAMï¼ç«å»ãåæ¸é¤¨ãæªæ¡é¤¨ååç©é¤¨ï¼çèè¶£ã</paragraph>

##### **TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's Disease Progression Analysis**
2407.06852v1 by Jacob Thrasher, Alina Devkota, Ahmed Tafti, Binod Bhattarai, Prashnna Gyawali

Alzheimer's Dementia (AD) represents one of the most pressing challenges in
the field of neurodegenerative disorders, with its progression analysis being
crucial for understanding disease dynamics and developing targeted
interventions. Recent advancements in deep learning and various representation
learning strategies, including self-supervised learning (SSL), have shown
significant promise in enhancing medical image analysis, providing innovative
ways to extract meaningful patterns from complex data. Notably, the computer
vision literature has demonstrated that incorporating supervisory signals into
SSL can further augment model performance by guiding the learning process with
additional relevant information. However, the application of such supervisory
signals in the context of disease progression analysis remains largely
unexplored. This gap is particularly pronounced given the inherent challenges
of incorporating both event and time-to-event information into the learning
paradigm. Addressing this, we propose a novel framework, Time and Even-aware
SSL (TE-SSL), which integrates time-to-event and event data as supervisory
signals to refine the learning process. Our comparative analysis with existing
SSL-based methods in the downstream task of survival analysis shows superior
performance across standard metrics.

æè¦ï¼é¿è²æµ·é»çå¤±æºç (AD) æ¯ç¥ç¶éåæ§ç¾çé åä¸­æè¿«åçææ°ä¹ä¸ï¼å¶é²ç¨åæå°æ¼äºè§£ç¾çåæåéç¼ç®æ¨æ§å¹²é æªæ½è³ééè¦ãæ·±åº¦å­¸ç¿ååç¨®è¡¨ç¤ºå­¸ç¿ç­ç¥ï¼åæ¬èªç£ç£å­¸ç¿ (SSL)ï¼çææ°é²å±ï¼å·²å¨å¢å¼·é«å­¸å½±ååææ¹é¢å±ç¾é¡¯èåæ¯ï¼æä¾å¾è¤éè³æä¸­æåææç¾©æ¨¡å¼çåµæ°æ¹æ³ãå¼å¾æ³¨æçæ¯ï¼é»è¦è¦è¦ºæç»å·²è­æå°ç£ç£è¨èç´å¥ SSL å¯ä»¥ééæä¾é¡å¤ç¸éè³è¨ä¾æå°å­¸ç¿éç¨ï¼é²ä¸æ­¥å¢å¼·æ¨¡åæè½ãç¶èï¼æ­¤é¡ç£ç£è¨èå¨ç¾çé²ç¨åæä¸­çæç¨ä»æªå¾å°ååæ¢è¨ãç±æ¼å°äºä»¶åäºä»¶æéè³è¨ç´å¥å­¸ç¿ç¯ä¾çåºæææ°ï¼æ­¤å·®è·ç¹å¥æé¡¯ãéå°æ­¤åé¡ï¼æåæåºä¸ååµæ°çæ¶æ§ï¼æéåäºä»¶æç¥ SSL (TE-SSL)ï¼å®æ´åäºä»¶æéåäºä»¶è³æä½çºç£ç£è¨èï¼ä»¥åªåå­¸ç¿éç¨ãæåå¨çå­åæçä¸æ¸¸ä»»åä¸­ï¼å°å¶èç¾æåºæ¼ SSL çæ¹æ³é²è¡æ¯è¼åæï¼é¡¯ç¤ºå¶å¨æ¨æºææ¨ä¸çæè½åªç°ã

##### **VRDSynth: Synthesizing Programs for Multilingual Visually Rich Document Information Extraction**
2407.06826v1 by Thanh-Dat Nguyen, Tung Do-Viet, Hung Nguyen-Duy, Tuan-Hai Luu, Hung Le, Bach Le, Patanamon, Thongtanunam

Businesses need to query visually rich documents (VRDs) like receipts,
medical records, and insurance forms to make decisions. Existing techniques for
extracting entities from VRDs struggle with new layouts or require extensive
pre-training data. We introduce VRDSynth, a program synthesis method to
automatically extract entity relations from multilingual VRDs without
pre-training data. To capture the complexity of VRD domain, we design a
domain-specific language (DSL) to capture spatial and textual relations to
describe the synthesized programs. Along with this, we also derive a new
synthesis algorithm utilizing frequent spatial relations, search space pruning,
and a combination of positive, negative, and exclusive programs to improve
coverage.
  We evaluate VRDSynth on the FUNSD and XFUND benchmarks for semantic entity
linking, consisting of 1,592 forms in 8 languages. VRDSynth outperforms
state-of-the-art pre-trained models (LayoutXLM, InfoXLMBase, and
XLMRobertaBase) in 5, 6, and 7 out of 8 languages, respectively, improving the
F1 score by 42% over LayoutXLM in English. To test the extensibility of the
model, we further improve VRDSynth with automated table recognition, creating
VRDSynth(Table), and compare it with extended versions of the pre-trained
models, InfoXLM(Large) and XLMRoberta(Large). VRDSynth(Table) outperforms these
baselines in 4 out of 8 languages and in average F1 score. VRDSynth also
significantly reduces memory footprint (1M and 380MB vs. 1.48GB and 3GB for
LayoutXLM) while maintaining similar time efficiency.

æè¦ï¼<paragraph>ä¼æ¥­éè¦æ¥è©¢è¦è¦ºè±å¯çæä»¶ (VRD)ï¼ä¾å¦æ¶æãé«çè¨éåä¿éªå®æï¼æè½ååºæ±ºç­ãç¾æçæè¡ç¨æ¼å¾ VRD ä¸­æåå¯¦é«ï¼æéå°æ°ççé¢åé¡ï¼æèéè¦å¤§éçé è¨ç·´æ¸æãæåä»ç´¹ VRDSynthï¼éæ¯ä¸ç¨®ç¨å¼åææ¹æ³ï¼å¯ä»¥å¨æ²æé è¨ç·´æ¸æçææ³ä¸èªåå¾å¤èªè¨ VRD ä¸­æåå¯¦é«éä¿ãçºäºææ VRD é åçè¤éæ§ï¼æåè¨­è¨äºä¸åç¹å®é åèªè¨ (DSL)ï¼ç¨æ¼ææç©ºéåæå­éä¿ï¼ä»¥æè¿°åæçç¨å¼ãé¤æ­¤ä¹å¤ï¼æåéæ¨å°åºä¸åæ°çåææ¼ç®æ³ï¼å©ç¨é »ç¹çç©ºééä¿ãæå°ç©ºéåªæï¼ä»¥åæ­£ãè² åæä»ç¨å¼ççµåï¼ä»¥æ¹åæ¶µèç¯åã
æåå¨ FUNSD å XFUND åºæºä¸è©ä¼° VRDSynthï¼ç¨æ¼èªç¾©å¯¦é«é£çµï¼åå« 8 ç¨®èªè¨ç 1,592 åè¡¨å®ãVRDSynth å¨ 8 ç¨®èªè¨ä¸­ç 5ã6 å 7 ç¨®èªè¨ä¸­åªæ¼æåé²çé è¨ç·´æ¨¡å (LayoutXLMãInfoXLMBase å XLMRobertaBase)ï¼åå¥å°è±æä¸­ç F1 åæ¸æé«äº 42%ï¼é«æ¼ LayoutXLMãçºäºæ¸¬è©¦æ¨¡åçå¯æ´åæ§ï¼æåé²ä¸æ­¥æ¹é² VRDSynthï¼æ¡ç¨èªååè¡¨æ ¼è­å¥ï¼å»ºç« VRDSynth(Table)ï¼ä¸¦å°å¶èé è¨ç·´æ¨¡å InfoXLM(Large) å XLMRoberta(Large) çå»¶ä¼¸çæ¬é²è¡æ¯è¼ãVRDSynth(Table) å¨ 8 ç¨®èªè¨ä¸­ç 4 ç¨®èªè¨åå¹³å F1 åæ¸ä¸­åªæ¼éäºåºæºãVRDSynth éé¡¯èæ¸å°äºè¨æ¶é«ä½¿ç¨é (1M å 380MBï¼è LayoutXLM çº 1.48GB å 3GB)ï¼åæç¶­æé¡ä¼¼çæéæçã</paragraph>

##### **iASiS: Towards Heterogeneous Big Data Analysis for Personalized Medicine**
2407.06748v1 by Anastasia Krithara, Fotis Aisopos, Vassiliki Rentoumi, Anastasios Nentidis, Konstantinos Bougatiotis, Maria-Esther Vidal, Ernestina Menasalvas, Alejandro Rodriguez-Gonzalez, Eleftherios G. Samaras, Peter Garrard, Maria Torrente, Mariano Provencio Pulla, Nikos Dimakopoulos, Rui Mauricio, Jordi Rambla De Argila, Gian Gaetano Tartaglia, George Paliouras

The vision of IASIS project is to turn the wave of big biomedical data
heading our way into actionable knowledge for decision makers. This is achieved
by integrating data from disparate sources, including genomics, electronic
health records and bibliography, and applying advanced analytics methods to
discover useful patterns. The goal is to turn large amounts of available data
into actionable information to authorities for planning public health
activities and policies. The integration and analysis of these heterogeneous
sources of information will enable the best decisions to be made, allowing for
diagnosis and treatment to be personalised to each individual. The project
offers a common representation schema for the heterogeneous data sources. The
iASiS infrastructure is able to convert clinical notes into usable data,
combine them with genomic data, related bibliography, image data and more, and
create a global knowledge base. This facilitates the use of intelligent methods
in order to discover useful patterns across different resources. Using semantic
integration of data gives the opportunity to generate information that is rich,
auditable and reliable. This information can be used to provide better care,
reduce errors and create more confidence in sharing data, thus providing more
insights and opportunities. Data resources for two different disease categories
are explored within the iASiS use cases, dementia and lung cancer.

æè¦ï¼IASIS é ç®çé¡æ¯æ¯å°ææåèä¾çé¾å¤§çç©é«å­¸æ¸ææµªæ½®è½è®çºæ±ºç­èçå¯è¡ç¥è­ãéæ¯ééæ´åä¾èªä¸åä¾æºçæ¸æï¼åæ¬åºå çµå­¸ãé»å­å¥åº·è¨éåæ¸ç®ï¼ï¼ä¸¦æç¨åé²çåææ¹æ³ä¾ç¼ç¾æç¨çæ¨¡å¼ä¾å¯¦ç¾çãç®æ¨æ¯å°å¤§éå¯ç¨æ¸æè½åçºå¯è¡çè³è¨ï¼ä¾ç¶å±è¦åå¬å±è¡çæ´»ååæ¿ç­ãæ´åååæéäºç°è³ªçè³è¨ä¾æºå°ä½¿æä½³æ±ºç­å¾ä»¥å¶å®ï¼ä¸¦åè¨±å°æ¯åäººçè¨ºæ·åæ²»çé²è¡åäººåãè©²å°æ¡çºç°è³ªæ¸æä¾æºæä¾äºä¸åå±åçè¡¨ç¤ºæ¶æ§ãiASiS åºç¤è¨­æ½è½å¤ å°è¨åºç­è¨è½æçºå¯ç¨æ¸æï¼å°å¶èåºå çµæ¸æãç¸éæ¸ç®ãå½±åæ¸æç­çµåèµ·ä¾ï¼ä¸¦å»ºç«ä¸åå¨çç¥è­åº«ãéæå©æ¼ä½¿ç¨æºæ§æ¹æ³ä¾ç¼ç¾ä¸åè³æºä¹éçæç¨æ¨¡å¼ãä½¿ç¨æ¸æçèªç¾©æ´åæä¾äºç¢çè±å¯ãå¯ç¨½æ ¸ä¸å¯é è³è¨çæ©æãéäºè³è¨å¯ç¨æ¼æä¾æ´å¥½çç§è­·ãæ¸å°é¯èª¤ï¼ä¸¦å°è³æå±äº«å»ºç«æ´å¤ä¿¡å¿ï¼å¾èæä¾æ´å¤è¦è§£åæ©æãå¨ iASiS çä½¿ç¨æ¡ä¾ä¸­ï¼æ¢è¨äºå©ç¨®ä¸åç¾çé¡å¥çæ¸æè³æºï¼å³å¤±æºçåèºçã

##### **Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations**
2407.11054v1 by Rachael Fleurence, Jiang Bian, Xiaoyan Wang, Hua Xu, Dalia Dawoud, Tala Fakhouri, Mitch Higashi, Jagpreet Chhatwal

This review introduces the transformative potential of generative Artificial
Intelligence (AI) and foundation models, including large language models
(LLMs), for health technology assessment (HTA). We explore their applications
in four critical areas, evidence synthesis, evidence generation, clinical
trials and economic modeling: (1) Evidence synthesis: Generative AI has the
potential to assist in automating literature reviews and meta-analyses by
proposing search terms, screening abstracts, and extracting data with notable
accuracy; (2) Evidence generation: These models can potentially facilitate
automating the process and analyze the increasingly available large collections
of real-world data (RWD), including unstructured clinical notes and imaging,
enhancing the speed and quality of real-world evidence (RWE) generation; (3)
Clinical trials: Generative AI can be used to optimize trial design, improve
patient matching, and manage trial data more efficiently; and (4) Economic
modeling: Generative AI can also aid in the development of health economic
models, from conceptualization to validation, thus streamlining the overall HTA
process. Despite their promise, these technologies, while rapidly improving,
are still nascent and continued careful evaluation in their applications to HTA
is required. To ensure their responsible use and implementation, both
developers and users of research incorporating these tools, should familiarize
themselves with their current limitations, including the issues related to
scientific validity, risk of bias, and consider equity and ethical
implications. We also surveyed the current policy landscape and provide
suggestions for HTA agencies on responsibly integrating generative AI into
their workflows, emphasizing the importance of human oversight and the
fast-evolving nature of these tools.

æè¦ï¼æ¬ç¯è©è«ä»ç´¹äºçæå¼äººå·¥æºæ§ (AI) ååºç¤æ¨¡åï¼åæ¬å¤§åèªè¨æ¨¡å (LLM)ï¼å¨å¥åº·æè¡è©ä¼° (HTA) ä¸­çè½åæ½åãæåæ¢è¨äºå®åå¨ååééµé åçæç¨ï¼è­æç¶åãè­æçæãè¨åºè©¦é©åç¶æ¿å»ºæ¨¡ï¼(1) è­æç¶åï¼çæå¼ AI ææ½ååå©èªååæç»åé¡§åçµ±ååæï¼æ¹æ³æ¯æåºæå°è©å½ãç¯©é¸æè¦åæåè³æï¼æºç¢ºåº¦é¡¯èï¼(2) è­æçæï¼éäºæ¨¡åææ½åä¿é²èªååæµç¨ï¼ä¸¦åææ¥çå¯å¾çå¤§éçå¯¦ä¸çè³æ (RWD) éåï¼åæ¬éçµæ§åè¨åºç­è¨åå½±åï¼æåçå¯¦ä¸çè­æ (RWE) çæçéåº¦ååè³ªï¼(3) è¨åºè©¦é©ï¼çæå¼ AI å¯ç¨æ¼æä½³åè©¦é©è¨­è¨ãæ¹åæ£èéå°ï¼ä¸¦æ´ææçå°ç®¡çè©¦é©è³æï¼(4) ç¶æ¿å»ºæ¨¡ï¼çæå¼ AI ä¹å¯ä»¥åå©éç¼å¥åº·ç¶æ¿æ¨¡åï¼å¾æ¦å¿µåå°é©è­ï¼é²èç°¡åæ´é« HTA æµç¨ãåç®¡éäºæè¡åæ¯çå¥½ï¼ä¸é²æ­¥ç¥éï¼ä½ä»èæ¼èè½éæ®µï¼éè¦æçºå°å¿è©ä¼°å®åå¨ HTA ä¸­çæç¨ãçºäºç¢ºä¿è² è²¬ä»»å°ä½¿ç¨åå¯¦æ½éäºæè¡ï¼éäºå·¥å·çç ç©¶éç¼äººå¡åä½¿ç¨èé½æè©²çæå®åç®åçéå¶ï¼åæ¬èç§å­¸æåº¦ãåå·®é¢¨éªç¸éçåé¡ï¼ä¸¦èæ®å¬å¹³æ§åå«çææ¶µãæåä¹èª¿æ¥äºç®åçæ¿ç­ç¾æ³ï¼ä¸¦éå° HTA æ©æ§æä¾å»ºè­°ï¼èªªæå¦ä½è² è²¬ä»»å°å°çæå¼ AI æ´åå°å·¥ä½æµç¨ä¸­ï¼å¼·èª¿äººå·¥ç£ç£åéäºå·¥å·å¿«éæ¼è®çæ¬è³ªã

##### **TCKIN: A Novel Integrated Network Model for Predicting Mortality Risk in Sepsis Patients**
2407.06560v1 by Fanglin Dong

Sepsis poses a major global health threat, accounting for millions of deaths
annually and significant economic costs. Accurate predictions of mortality risk
in sepsis patients facilitate the efficient allocation of medical resources,
thereby enhancing patient survival and quality of life. Through precise risk
assessments, healthcare facilities can effectively distribute intensive care
beds, medical equipment, and staff, ensuring high-risk patients receive timely
and appropriate care. Early identification and intervention significantly
decrease mortality rates and improve patient outcomes. Current methods
typically utilize only one type of data--either constant, temporal, or ICD
codes. This study introduces the Time-Constant KAN Integrated Network(TCKIN),
an innovative model that enhances the accuracy of sepsis mortality risk
predictions by integrating both temporal and constant data from electronic
health records and ICD codes. Validated against the MIMIC-III and MIMIC-IV
datasets, TCKIN surpasses existing machine learning and deep learning methods
in accuracy, sensitivity, and specificity. Notably, TCKIN achieved AUCs of
87.76% and 88.07%, demonstrating superior capability in identifying high-risk
patients. Additionally, TCKIN effectively combats the prevalent issue of data
imbalance in clinical settings, improving the detection of patients at elevated
risk of mortality and facilitating timely interventions. These results confirm
the model's effectiveness and its potential to transform patient management and
treatment optimization in clinical practice. With this advanced risk assessment
tool, healthcare providers can devise more tailored treatment plans, optimize
resource utilization, and ultimately enhance survival rates and quality of life
for sepsis patients.

æè¦ï¼<paragraph>æè¡çæ§æå¨çä¸»è¦çå¥åº·å¨èï¼æ¯å¹´é ææ¸ç¾è¬äººæ­»äº¡ï¼ä¸¦å¸¶ä¾é¾å¤§çç¶æ¿ææ¬ãæºç¢ºé æ¸¬æè¡çæ£èçæ­»äº¡é¢¨éªï¼æå©æ¼ææåéé«çè³æºï¼å¾èæåæ£èå­æ´»çåçæ´»åè³ªãééç²¾ç¢ºçé¢¨éªè©ä¼°ï¼é«çæ©æ§å¯ä»¥ææåéå è­·çæ¿çåºãé«çè¨­ååäººå¡ï¼ç¢ºä¿é«é¢¨éªæ£èè½åæç²å¾é©ç¶çç§è­·ãæ©æç¼ç¾åä»å¥å¯ä»¥é¡¯èéä½æ­»äº¡çï¼ä¸¦æ¹åæ£èé å¾ãç®åçæ¹æ³éå¸¸åä½¿ç¨ä¸ç¨®é¡åçè³æï¼ä¾å¦å¸¸æ¸ãæéæ ICD ç·¨ç¢¼ãæ¬ç ç©¶å¼å¥äºæéå¸¸æ¸ KAN æ´åç¶²è·¯ (TCKIN)ï¼éæ¯ä¸ååµæ°çæ¨¡åï¼ééæ´åé»å­å¥åº·ç´éå ICD ç·¨ç¢¼ä¸­çæéåå¸¸æ¸è³æï¼ä¾æåæè¡çæ­»äº¡é¢¨éªé æ¸¬çæºç¢ºæ§ãå¨ MIMIC-III å MIMIC-IV è³æéé©è­ä¸ï¼TCKIN å¨æºç¢ºæ§ãæææ§åç¹ç°æ§æ¹é¢é½è¶è¶äºç¾æçæ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æ¹æ³ãå¼å¾æ³¨æçæ¯ï¼TCKIN éå°äº 87.76% å 88.07% ç AUCï¼é¡¯ç¤ºåºåªç°çè­å¥é«é¢¨éªæ£èè½åãæ­¤å¤ï¼TCKIN ææå°è§£æ±ºäºè¨åºç°å¢ä¸­æ®éå­å¨çè³æä¸å¹³è¡¡åé¡ï¼æ¹åäºå°æ­»äº¡é¢¨éªè¼é«çæ£èçæª¢æ¸¬ï¼ä¸¦ä¿é²åæä»å¥ãéäºçµæè­å¯¦äºè©²æ¨¡åçæææ§ï¼ä»¥åå¶å¨è¨åºå¯¦åä¸­è½åæ£èç®¡çååªåæ²»ççæ½åãæäºéåé²éçé¢¨éªè©ä¼°å·¥å·ï¼é«çä¿å¥æä¾èå¯ä»¥å¶å®æ´å®¢è£½åçæ²»çè¨ç«ï¼æä½³åè³æºå©ç¨ï¼ä¸¦æçµæåæè¡çæ£èçå­æ´»çåçæ´»åè³ªã</paragraph>

##### **AI-driven multi-omics integration for multi-scale predictive modeling of causal genotype-environment-phenotype relationships**
2407.06405v1 by You Wu, Lei Xie

Despite the wealth of single-cell multi-omics data, it remains challenging to
predict the consequences of novel genetic and chemical perturbations in the
human body. It requires knowledge of molecular interactions at all biological
levels, encompassing disease models and humans. Current machine learning
methods primarily establish statistical correlations between genotypes and
phenotypes but struggle to identify physiologically significant causal factors,
limiting their predictive power. Key challenges in predictive modeling include
scarcity of labeled data, generalization across different domains, and
disentangling causation from correlation. In light of recent advances in
multi-omics data integration, we propose a new artificial intelligence
(AI)-powered biology-inspired multi-scale modeling framework to tackle these
issues. This framework will integrate multi-omics data across biological
levels, organism hierarchies, and species to predict causal
genotype-environment-phenotype relationships under various conditions. AI
models inspired by biology may identify novel molecular targets, biomarkers,
pharmaceutical agents, and personalized medicines for presently unmet medical
needs.

æè¦ï¼åç®¡æè±å¯çå®ç´°èå¤çµå­¸è³æï¼ä½é æ¸¬äººé«ä¸­æ°çéºå³ååå­¸æ¾åçå¾æä»ç¶å·æææ°æ§ãééè¦äºè§£ææçç©å±¤ç´çåå­äº¤äºä½ç¨ï¼åæ¬ç¾çæ¨¡ååäººé¡ãç®åçæ©å¨å­¸ç¿æ¹æ³ä¸»è¦å»ºç«åºå ååè¡¨åä¹éççµ±è¨ç¸éæ§ï¼ä½é£ä»¥è­å¥ççä¸éè¦çå æå ç´ ï¼å¾èéå¶äºå¶é æ¸¬è½åãé æ¸¬å»ºæ¨¡ä¸­çä¸»è¦ææ°åæ¬æ¨è¨è³æçç¨ç¼ºæ§ãè·¨ä¸åé åçæ¦åï¼ä»¥åå°å æéä¿å¾ç¸éæ§ä¸­è§£éãéæ¼å¤çµå­¸è³ææ´åçææ°é²å±ï¼æåæåºäºä¸åæ°çç±äººå·¥æºæ§ (AI) é©åçãåçç©åç¼çå¤å°ºåº¦å»ºæ¨¡æ¡æ¶ä¾è§£æ±ºéäºåé¡ãæ­¤æ¡æ¶å°æ´åè·¨çç©å±¤ç´ãçç©é«å±¤ç´åç©ç¨®çå¤çµå­¸è³æï¼ä»¥é æ¸¬å¨åç¨®æ¢ä»¶ä¸çå æåºå å-ç°å¢-è¡¨åéä¿ãåçç©åç¼ç AI æ¨¡åå¯è½æè­å¥åºæ°çåå­é¶æ¨ãçç©æ¨è¨ãè¥ç©ååæ§åè¥ç©ï¼ä»¥æ»¿è¶³ç®åå°æªæ»¿è¶³çé«çéæ±ã

##### **Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps**
2407.06309v1 by Chuanbo Hu, Bin Liu, Minglei Yin, Yilu Zhou, Xin Li

Mobile applications (Apps) could expose children to inappropriate themes such
as sexual content, violence, and drug use. Maturity rating offers a quick and
effective method for potential users, particularly guardians, to assess the
maturity levels of apps. Determining accurate maturity ratings for mobile apps
is essential to protect children's health in today's saturated digital
marketplace. Existing approaches to maturity rating are either inaccurate
(e.g., self-reported rating by developers) or costly (e.g., manual
examination). In the literature, there are few text-mining-based approaches to
maturity rating. However, each app typically involves multiple modalities,
namely app description in the text, and screenshots in the image. In this
paper, we present a framework for determining app maturity levels that utilize
multimodal large language models (MLLMs), specifically ChatGPT-4 Vision.
Powered by Chain-of-Thought (CoT) reasoning, our framework systematically
leverages ChatGPT-4 to process multimodal app data (i.e., textual descriptions
and screenshots) and guide the MLLM model through a step-by-step reasoning
pathway from initial content analysis to final maturity rating determination.
As a result, through explicitly incorporating CoT reasoning, our framework
enables ChatGPT to understand better and apply maturity policies to facilitate
maturity rating. Experimental results indicate that the proposed method
outperforms all baseline models and other fusion strategies.

æè¦ï¼è¡åæç¨ç¨å¼ (App) å¯è½è®åç«¥æ¥è§¸å°ä¸é©ç¶çä¸»é¡ï¼ä¾å¦æ§å§å®¹ãæ´ååè¥ç©ä½¿ç¨ãæçåº¦è©åæä¾ä¸ç¨®å¿«éä¸ææçæ¹æ³ï¼è®æ½å¨ä½¿ç¨èï¼å°¤å¶æ¯ç£è­·äººï¼è©ä¼°æç¨ç¨å¼çæçåº¦ç­ç´ãå¨ç¶ä»é£½åçæ¸ä½å¸å ´ä¸­ï¼çºè¡åæç¨ç¨å¼ç¢ºå®æºç¢ºçæçåº¦è©åå°æ¼ä¿è­·åç«¥å¥åº·è³ééè¦ãç¾æçæçåº¦è©åæ¹æ³ä¸æ¯ä¸æºç¢ºï¼ä¾å¦ï¼éç¼äººå¡èªè¡å ±åçè©åï¼ï¼å°±æ¯ææ¬é«æï¼ä¾å¦ï¼äººå·¥å¯©æ¥ï¼ãå¨æç»ä¸­ï¼å¾å°æåºæ¼æå­æ¢åçæ¹æ³ä¾è©ä¼°æçåº¦ãç¶èï¼æ¯åæç¨ç¨å¼éå¸¸æ¶åå¤ç¨®æ¨¡å¼ï¼å³æå­ä¸­çæç¨ç¨å¼èªªæåå½±åä¸­çè¢å¹æªåãå¨æ¬æä¸­ï¼æåæåºä¸åæ¡æ¶ï¼ç¨æ¼ç¢ºå®æç¨ç¨å¼çæçåº¦ç­ç´ï¼è©²æ¡æ¶å©ç¨å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM)ï¼ç¹å¥æ¯ ChatGPT-4 Visionãæåçæ¡æ¶æ¡ç¨æç¶­é (CoT) æ¨ççºåºç¤ï¼ç³»çµ±æ§å°å©ç¨ ChatGPT-4 èçå¤æ¨¡ææç¨ç¨å¼è³æï¼å³æå­èªªæåè¢å¹æªåï¼ï¼ä¸¦å¼å° MLLM æ¨¡åéæ­¥é²è¡æ¨çè·¯å¾ï¼å¾åå§å§å®¹åæå°æçµæçåº¦è©åç¢ºå®ãå æ­¤ï¼ééæç¢ºç´å¥ CoT æ¨çï¼æåçæ¡æ¶ä½¿ ChatGPT è½å¤ æ´æ·±å¥å°äºè§£ä¸¦æç¨æçåº¦æ¿ç­ä¾ä¿é²æçåº¦è©åãå¯¦é©çµæè¡¨æï¼ææåºçæ¹æ³åªæ¼ææåºç·æ¨¡ååå¶ä»èåç­ç¥ã

##### **Hybrid X-Linker: Automated Data Generation and Extreme Multi-label Ranking for Biomedical Entity Linking**
2407.06292v1 by Pedro Ruas, Fernando Gallego, Francisco J. Veredas, Francisco M. Couto

State-of-the-art deep learning entity linking methods rely on extensive
human-labelled data, which is costly to acquire. Current datasets are limited
in size, leading to inadequate coverage of biomedical concepts and diminished
performance when applied to new data. In this work, we propose to automatically
generate data to create large-scale training datasets, which allows the
exploration of approaches originally developed for the task of extreme
multi-label ranking in the biomedical entity linking task. We propose the
hybrid X-Linker pipeline that includes different modules to link disease and
chemical entity mentions to concepts in the MEDIC and the CTD-Chemical
vocabularies, respectively. X-Linker was evaluated on several biomedical
datasets: BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical,
BioRED-Chemical, and NLM-Chem, achieving top-1 accuracies of 0.8307, 0.7969,
0.8271, 0.9511, 0.9248, and 0.7895, respectively. X-Linker demonstrated
superior performance in three datasets: BC5CDR-Disease, NCBI-Disease, and
BioRED-Chemical. In contrast, SapBERT outperformed X-Linker in the remaining
three datasets. Both models rely only on the mention string for their
operations. The source code of X-Linker and its associated data are publicly
available for performing biomedical entity linking without requiring
pre-labelled entities with identifiers from specific knowledge organization
systems.

æè¦ï¼æåé²çæ·±åº¦å­¸ç¿å¯¦é«é£çµæ¹æ³ä¾è³´æ¼å¤§éç
äººå·¥æ¨ç±¤è³æï¼èéé¡è³æçåå¾ææ¬å¾é«ãç®åçè³æé
å¤§å°æéï¼å°è´çç©é«å­¸æ¦å¿µæ¶µèä¸è¶³ï¼å¨æç¨æ¼æ°è³æææè½éä½ãå¨éé å·¥ä½ä¸­ï¼æåæè­°èªå
ç¢çè³æï¼ä»¥å»ºç«å¤§è¦æ¨¡çè¨ç·´è³æéï¼éåè¨±æ¢ç´¢åæ¬æ¯çºçç©é«å­¸å¯¦é«é£çµä»»åä¸­æ¥µç«¯å¤æ¨ç±¤æåä»»åèéç¼çæ¹æ³ãæåæè­°æ··å X-Linker ç®¡ç·ï¼å¶ä¸­åå«ä¸åçæ¨¡çµï¼åå¥å°ç¾çååå­¸å¯¦é«æåé£çµå° MEDIC å CTD-Chemical å­å½ä¸­çæ¦å¿µãX-Linker å·²å¨å¤åçç©é«å­¸è³æéä¸é²è¡è©ä¼°ï¼BC5CDR-DiseaseãBioRED-DiseaseãNCBI-DiseaseãBC5CDR-ChemicalãBioRED-Chemical å NLM-Chemï¼åå¥éå° 0.8307ã0.7969ã0.8271ã0.9511ã0.9248 å 0.7895 ç top-1 æºç¢ºåº¦ãX-Linker å¨ä¸åè³æéï¼BC5CDR-DiseaseãNCBI-Disease å BioRED-Chemical ä¸­è¡¨ç¾åºåªç°çæè½ãç¸åå°ï¼SapBERT å¨å¶é¤ä¸åè³æéä¸­åªæ¼ X-Linkerãå©åæ¨¡ååä¾è³´æ¼å®åæä½çæåå­ä¸²ãX-Linker åå¶ç¸éè³æçåå§ç¨å¼ç¢¼å¬éæä¾ï¼å¯å·è¡çç©é«å­¸å¯¦é«é£çµï¼èç¡éç¹å®ç¥è­çµç¹ç³»çµ±ä¸­è­å¥ç¬¦èçé åæ¨ç±¤å¯¦é«ã

##### **Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities**
2407.06125v1 by Avinash Anand, Chayan Tank, Sarthak Pol, Vinayak Katoch, Shaina Mehta, Rajiv Ratn Shah

Depression has proven to be a significant public health issue, profoundly
affecting the psychological well-being of individuals. If it remains
undiagnosed, depression can lead to severe health issues, which can manifest
physically and even lead to suicide. Generally, Diagnosing depression or any
other mental disorder involves conducting semi-structured interviews alongside
supplementary questionnaires, including variants of the Patient Health
Questionnaire (PHQ) by Clinicians and mental health professionals. This
approach places significant reliance on the experience and judgment of trained
physicians, making the diagnosis susceptible to personal biases. Given that the
underlying mechanisms causing depression are still being actively researched,
physicians often face challenges in diagnosing and treating the condition,
particularly in its early stages of clinical presentation. Recently,
significant strides have been made in Artificial neural computing to solve
problems involving text, image, and speech in various domains. Our analysis has
aimed to leverage these state-of-the-art (SOTA) models in our experiments to
achieve optimal outcomes leveraging multiple modalities. The experiments were
performed on the Extended Distress Analysis Interview Corpus Wizard of Oz
dataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)
2019 Challenge. The proposed solutions demonstrate better results achieved by
Proprietary and Open-source Large Language Models (LLMs), which achieved a Root
Mean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC
2019 challenge baseline results and current SOTA regression analysis
architectures. Additionally, the proposed solution achieved an accuracy of
71.43% in the classification task. The paper also includes a novel audio-visual
multi-modal network that predicts PHQ-8 scores with an RMSE of 6.51.

æè¦ï¼æé¬±çå·²è¢«è­å¯¦æ¯ä¸åéå¤§çå¬å±è¡çè­°é¡ï¼æ·±å»å½±é¿åäººå¿çå¥åº·ãå¦ææé¬±çæªç¶è¨ºæ·ï¼å¯è½æå°è´å´éçå¥åº·åé¡ï¼éäºåé¡å¯è½å¨ççä¸é¡¯ç¾ï¼çè³å°è´èªæ®ºãéå¸¸ï¼è¨ºæ·æé¬±çæä»»ä½å¶ä»ç²¾ç¥ç¾çé½æ¶åé²è¡åçµæ§åè¨ªè«ï¼ä»¥åè£ååå·ï¼åæ¬è¨åºé«çåå¿çå¥åº·å°æ¥­äººå¡æä½¿ç¨çæ£èå¥åº·åå· (PHQ) è®é«ãéç¨®æ¹æ³éå¸¸ä¾è³´åéè¨ç·´çé«å¸«çç¶é©åå¤æ·ï¼ä½¿è¨ºæ·å®¹æåå°åäººåè¦çå½±é¿ãç±æ¼å°è´æé¬±ççæ½å¨æ©å¶ä»å¨ç©æ¥µç ç©¶ä¸­ï¼å æ­¤é«å¸«å¨è¨ºæ·åæ²»çéç¨®ç¾çæç¶å¸¸é¢è¨ææ°ï¼å°¤å¶æ¯å¨è¨åºè¡¨ç¾çæ©æéæ®µãæè¿ï¼äººå·¥ç¥ç¶éç®å¨è§£æ±ºæ¶åææ¬ãå½±ååèªè¨çåç¨®é ååé¡æ¹é¢åå¾äºéå¤§é²å±ãæåçåææ¨å¨å©ç¨éäºæåé² (SOTA) æ¨¡åå¨æåçå¯¦é©ä¸­ï¼ééå©ç¨å¤ç¨®æ¨¡å¼ä¾éææä½³çµæãéäºå¯¦é©æ¯å¨ Audio/Visual Emotion Challenge (AVEC) 2019 Challenge ä¸­æåºç Extended Distress Analysis Interview Corpus Wizard of Oz è³æé (E-DAIC) èªæåº«ä¸é²è¡çãææåºçè§£æ±ºæ¹æ¡è­æäºå°æåéæ¾åå§ç¢¼å¤§åèªè¨æ¨¡å (LLM) æåå¾çè¼ä½³çµæï¼éäºæ¨¡åå¨ææ¬æ¨¡å¼ä¸éå°äº 3.98 çåæ¹æ ¹èª¤å·® (RMSE) åæ¸ï¼åªæ¼ AVEC 2019 ææ°åºæºçµæåç®åç SOTA åæ­¸åææ¶æ§ãæ­¤å¤ï¼ææåºçè§£æ±ºæ¹æ¡å¨åé¡ä»»åä¸­éå°äº 71.43% çæºç¢ºçãæ¬æéåæ¬ä¸åæ°ç©çé³è¨è¦è¦ºå¤æ¨¡å¼ç¶²è·¯ï¼å¶ä½¿ç¨ 6.51 ç RMSE é æ¸¬ PHQ-8 åæ¸ã

##### **Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian**
2407.06011v1 by Tommaso Mario Buonocore, Simone Rancati, Enea Parimbelli

The development of domain-specific language models has significantly advanced
natural language processing applications in various specialized fields,
particularly in biomedicine. However, the focus has largely been on
English-language models, leaving a gap for less-resourced languages such as
Italian. This paper introduces Igea, the first decoder-only language model
designed explicitly for biomedical text generation in Italian. Built on the
Minerva model and continually pretrained on a diverse corpus of Italian medical
texts, Igea is available in three model sizes: 350 million, 1 billion, and 3
billion parameters. The models aim to balance computational efficiency and
performance, addressing the challenges of managing the peculiarities of medical
terminology in Italian. We evaluate Igea using a mix of in-domain biomedical
corpora and general-purpose benchmarks, highlighting its efficacy and retention
of general knowledge even after the domain-specific training. This paper
discusses the model's development and evaluation, providing a foundation for
future advancements in Italian biomedical NLP.

æè¦ï¼ç¹å®é åèªè¨æ¨¡åçç¼å±å·²å¤§å¹æåäºåç¨®å°æ¥­é åçèªç¶èªè¨èçæç¨ï¼ç¹å¥æ¯å¨çç©é«å­¸é åãç¶èï¼ç®åçç ç©¶éé»ä¸»è¦æ¾å¨è±èªèªè¨æ¨¡åä¸ï¼éå°ç¾©å¤§å©èªç­è³æºè¼å°çèªè¨ä¾èªªæ¯ä¸å¤§ç¼ºæ¾ãæ¬æä»ç´¹äº Igeaï¼éæ¯ç¬¬ä¸åå°éè¨­è¨ç¨æ¼ç¾©å¤§å©èªçç©é«å­¸ææ¬çæçåè§£ç¢¼å¨èªè¨æ¨¡åãIgea å»ºæ§å¨ Minerva æ¨¡åä¸ï¼ä¸¦æçºå¨å¤§éç¾©å¤§å©é«å­¸ææ¬èªæåº«ä¸é²è¡é è¨ç·´ï¼æä¾ä¸ç¨®æ¨¡åå¤§å°ï¼3.5 åã10 åå 30 åååæ¸ãéäºæ¨¡åæ¨å¨å¹³è¡¡éç®æçåæè½ï¼è§£æ±ºèçç¾©å¤§å©èªé«å­¸è¡èªç¹æ§çææ°ãæåä½¿ç¨æ··åé åçç©é«å­¸èªæåº«åéç¨åºæºå° Igea é²è¡è©ä¼°ï¼å¼·èª¿äºå¶åæåå¨ç¹å®é åè¨ç·´å¾ä»è½ä¿çä¸è¬ç¥è­ãæ¬ææ¢è¨äºæ¨¡åçéç¼åè©ä¼°ï¼çºç¾©å¤§å©èªçç©é«å­¸èªç¶èªè¨èççæªä¾é²å±å¥ å®äºåºç¤ã

##### **Generation and De-Identification of Indian Clinical Discharge Summaries using LLMs**
2407.05887v1 by Sanjeet Singh, Shreya Gupta, Niralee Gupta, Naimish Sharma, Lokesh Srivastava, Vibhu Agarwal, Ashutosh Modi

The consequences of a healthcare data breach can be devastating for the
patients, providers, and payers. The average financial impact of a data breach
in recent months has been estimated to be close to USD 10 million. This is
especially significant for healthcare organizations in India that are managing
rapid digitization while still establishing data governance procedures that
align with the letter and spirit of the law. Computer-based systems for
de-identification of personal information are vulnerable to data drift, often
rendering them ineffective in cross-institution settings. Therefore, a rigorous
assessment of existing de-identification against local health datasets is
imperative to support the safe adoption of digital health initiatives in India.
Using a small set of de-identified patient discharge summaries provided by an
Indian healthcare institution, in this paper, we report the nominal performance
of de-identification algorithms (based on language models) trained on publicly
available non-Indian datasets, pointing towards a lack of cross-institutional
generalization. Similarly, experimentation with off-the-shelf de-identification
systems reveals potential risks associated with the approach. To overcome data
scarcity, we explore generating synthetic clinical reports (using publicly
available and Indian summaries) by performing in-context learning over Large
Language Models (LLMs). Our experiments demonstrate the use of generated
reports as an effective strategy for creating high-performing de-identification
systems with good generalization capabilities.

æè¦ï¼é«çä¿å¥è³æå¤æ´©çå¾æå°æ£èãæä¾èåä»æ¬¾èä¾èªªå¯è½æ¯æ¯æ»æ§çãæè¿å¹¾åæè³æå¤æ´©çå¹³åè²¡åå½±é¿ä¼°è¨æ¥è¿ 1,000 è¬ç¾åãéå°å°åº¦çé«çä¿å¥çµç¹ä¾èªªå°¤å¶éè¦ï¼éäºçµç¹å¨ç®¡çå¿«éæ¸ä½åçåæï¼ä»å¨å»ºç«ç¬¦åæ³å¾æ¢æåç²¾ç¥çè³ææ²»çç¨åºãç¨æ¼å»è­å¥åäººè³è¨çé»è¦ç³»çµ±å®¹æåå°è³ææ¼ç§»çå½±é¿ï¼éå¸¸å°è´å®åå¨è·¨æ©æ§è¨­å®ä¸­ç¡æãå æ­¤ï¼å¿é å´æ ¼è©ä¼°ç¾æçå»è­å¥èç¶å°å¥åº·è³æéï¼æè½æ¯æ´å°åº¦å®å¨æ¡ç¨æ¸ä½å¥åº·è¨ç«ãæ¬æä½¿ç¨å°åº¦é«çä¿å¥æ©æ§æä¾çä¸å°çµå»è­å¥æ£èåºé¢æè¦ï¼å ±åäºå¨å¬éå¯ç¨çéå°åº¦è³æéä¸è¨ç·´çå»è­å¥æ¼ç®æ³ï¼åºæ¼èªè¨æ¨¡åï¼çæ¨ç¨±æè½ï¼æåºç¼ºä¹è·¨æ©æ§æ¦åãåæ¨£å°ï¼å°ç¾æçå»è­å¥ç³»çµ±é²è¡å¯¦é©æ­ç¤ºäºèè©²æ¹æ³ç¸éçæ½å¨é¢¨éªãçºäºåæè³æç¨å°çåé¡ï¼æåæ¢è¨ééå¨å¤§èªè¨æ¨¡å (LLM) ä¸å·è¡æå¢å­¸ç¿ä¾ç¢çåæè¨åºå ±åï¼ä½¿ç¨å¬éå¯ç¨çå°åº¦æè¦ï¼ãæåçå¯¦é©è­æäºä½¿ç¨ç¢ççå ±åä½çºå»ºç«å·æè¯å¥½æ¦åè½åçé«æè½å»è­å¥ç³»çµ±çææç­ç¥ã

##### **Integrating AI in College Education: Positive yet Mixed Experiences with ChatGPT**
2407.05810v1 by Xinrui Song, Jiajin Zhang, Pingkun Yan, Juergen Hahn, Uwe Kruger, Hisham Mohamed, Ge Wang

The integration of artificial intelligence (AI) chatbots into higher
education marks a shift towards a new generation of pedagogical tools,
mirroring the arrival of milestones like the internet. With the launch of
ChatGPT-4 Turbo in November 2023, we developed a ChatGPT-based teaching
application (https://chat.openai.com/g/g-1imx1py4K-chatge-medical-imaging) and
integrated it into our undergraduate medical imaging course in the Spring 2024
semester. This study investigates the use of ChatGPT throughout a semester-long
trial, providing insights into students' engagement, perception, and the
overall educational effectiveness of the technology. We systematically
collected and analyzed data concerning students' interaction with ChatGPT,
focusing on their attitudes, concerns, and usage patterns. The findings
indicate that ChatGPT offers significant advantages such as improved
information access and increased interactivity, but its adoption is accompanied
by concerns about the accuracy of the information provided and the necessity
for well-defined guidelines to optimize its use.

æè¦ï¼äººå·¥æºè½ï¼AIï¼èå¤©æ©å¨äººæ´åå°é«ç­æè²ä¸­ï¼æ¨èªèæ°ä¸ä»£æå­¸å·¥å·çè½è®ï¼åæ äºç¶²è·¯ç­éç¨ç¢çå°ä¾ãé¨è ChatGPT-4 Turbo å¨ 2023 å¹´ 11 ææ¨åºï¼æåéç¼äºä¸ååºæ¼ ChatGPT çæå­¸æç¨ç¨å¼ï¼https://chat.openai.com/g/g-1imx1py4K-chatge-medical-imagingï¼ï¼ä¸¦å¨ 2024 å¹´æ¥å­£å­¸æå°å¶æ´åå°æåçé«å­¸å½±åå­¸æ¬ç§èª²ç¨ä¸­ãæ¬ç ç©¶èª¿æ¥äºå¨ä¸åå­¸æé·çè©¦é©ä¸­ä½¿ç¨ ChatGPT çææ³ï¼æ·±å¥äºè§£å­¸ççåèåº¦ãçæ³åæè¡çæ´é«æè²ææãæåç³»çµ±å°æ¶éååæäºæéå­¸çè ChatGPT äºåçè³æï¼éé»éæ³¨ä»åçæåº¦ãçæ®åä½¿ç¨æ¨¡å¼ãç ç©¶çµæè¡¨æï¼ChatGPT æä¾äºé¡¯èçåªå¢ï¼ä¾å¦æ¹é²è³è¨çåå¾åå¢å äºåæ§ï¼ä½å¶æ¡ç¨ä¹ä¼´é¨èå°ææä¾è³è¨æºç¢ºæ§ççæ®ï¼ä»¥åæä½³åå¶ä½¿ç¨çæç¢ºæºåçå¿è¦æ§ã

##### **FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep Reinforcement Learning for Medical Imaging**
2407.05800v1 by Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal

Despite recent advancements in federated learning (FL) for medical image
diagnosis, addressing data heterogeneity among clients remains a significant
challenge for practical implementation. A primary hurdle in FL arises from the
non-IID nature of data samples across clients, which typically results in a
decline in the performance of the aggregated global model. In this study, we
introduce FedMRL, a novel federated multi-agent deep reinforcement learning
framework designed to address data heterogeneity. FedMRL incorporates a novel
loss function to facilitate fairness among clients, preventing bias in the
final global model. Additionally, it employs a multi-agent reinforcement
learning (MARL) approach to calculate the proximal term $(\mu)$ for the
personalized local objective function, ensuring convergence to the global
optimum. Furthermore, FedMRL integrates an adaptive weight adjustment method
using a Self-organizing map (SOM) on the server side to counteract distribution
shifts among clients' local data distributions. We assess our approach using
two publicly available real-world medical datasets, and the results demonstrate
that FedMRL significantly outperforms state-of-the-art techniques, showing its
efficacy in addressing data heterogeneity in federated learning. The code can
be found here~{\url{https://github.com/Pranabiitp/FedMRL}}.

æè¦ï¼åç®¡å¨ç¨æ¼é«å­¸å½±åè¨ºæ·çè¯é¦å­¸ç¿ (FL) æ¹é¢æè¿æçé²å±ï¼ä½è§£æ±ºå®¢æ¶ç«¯ä¹éçè³æç°è³ªæ§ä»ç¶æ¯å¯¦éå·è¡çéå¤§ææ°ãè¯é¦å­¸ç¿çä¸»è¦éç¤ä¾èªæ¼å®¢æ¶ç«¯ä¹éè³ææ¨£æ¬çéç¨ç«ååå¸ (non-IID) ç¹æ§ï¼ééå¸¸æå°è´å½ç¸½çå¨çæ¨¡åæè½ä¸éãå¨æ¬ç ç©¶ä¸­ï¼æåå¼å¥äº FedMRLï¼ä¸åæ°ç©çè¯é¦å¤æºè½é«æ·±åº¦å¼·åå­¸ç¿æ¡æ¶ï¼æ¨å¨è§£æ±ºè³æç°è³ªæ§ãFedMRL çµåäºä¸åæ°ç©çæå¤±å½æ¸ï¼ä»¥ä¿é²å®¢æ¶ç«¯ä¹éçå¬å¹³æ§ï¼é²æ­¢æçµå¨çæ¨¡åä¸­çåå·®ãæ­¤å¤ï¼å®æ¡ç¨å¤æºè½é«å¼·åå­¸ç¿ (MARL) æ¹æ³ä¾è¨ç®åæ§åå±é¨ç®æ¨å½æ¸çè¿ç«¯é  (Î¼)ï¼ç¢ºä¿æ¶æå°å¨å±æåªå¼ãæ­¤å¤ï¼FedMRL æ´åäºä¸ç¨®èªé©ææ¬éèª¿æ´æ¹æ³ï¼å¨ä¼ºæå¨ç«¯ä½¿ç¨èªçµç¹åå°æ (SOM)ï¼ä»¥æµæ¶å®¢æ¶ç«¯æ¬å°è³æåä½ä¹éçåå¸è½ç§»ãæåä½¿ç¨å©åå¬éå¯ç¨ççå¯¦ä¸çé«å­¸è³æéè©ä¼°æåçåæ³ï¼çµæè¡¨æ FedMRL æé¡¯åªæ¼æåé²çæè¡ï¼é¡¯ç¤ºå¶å¨è§£æ±ºè¯é¦å­¸ç¿ä¸­è³æç°è³ªæ§æ¹é¢çæè½ãç¨å¼ç¢¼å¯ä»¥å¨éè£¡æ¾å°~{\url{https://github.com/Pranabiitp/FedMRL}}ã

##### **Large Language Models for Judicial Entity Extraction: A Comparative Study**
2407.05786v1 by Atin Sakkeer Hussain, Anu Thomas

Domain-specific Entity Recognition holds significant importance in legal
contexts, serving as a fundamental task that supports various applications such
as question-answering systems, text summarization, machine translation,
sentiment analysis, and information retrieval specifically within case law
documents. Recent advancements have highlighted the efficacy of Large Language
Models in natural language processing tasks, demonstrating their capability to
accurately detect and classify domain-specific facts (entities) from
specialized texts like clinical and financial documents. This research
investigates the application of Large Language Models in identifying
domain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents,
FIR nos.) within case law documents, with a specific focus on their aptitude
for handling domain-specific language complexity and contextual variations. The
study evaluates the performance of state-of-the-art Large Language Model
architectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in
the context of extracting judicial facts tailored to Indian judicial texts.
Mistral and Gemma emerged as the top-performing models, showcasing balanced
precision and recall crucial for accurate entity identification. These findings
confirm the value of Large Language Models in judicial documents and
demonstrate how they can facilitate and quicken scientific research by
producing precise, organised data outputs that are appropriate for in-depth
examination.

æè¦ï¼é åç¹å®å¯¦é«è¾¨è­å¨æ³å¾èçµ¡ä¸­è³ééè¦ï¼ä½çºæ¯æ´åç¨®æç¨ç¨å¼çåºç¤ä»»åï¼ä¾å¦å¨æ¡ä¾æ³æä»¶ä¸­é²è¡åç­ç³»çµ±ãæå­æè¦ãæ©å¨ç¿»è­¯ãæç·åæåè³è¨æª¢ç´¢ãæè¿çé²å±çªé¡¯äºå¤§åèªè¨æ¨¡åå¨èªç¶èªè¨èçä»»åä¸­çæè½ï¼å±ç¤ºäºå®åæºç¢ºåµæ¸¬ååé¡ä¾èªå°æ¥­ææ¬ï¼ä¾å¦è¨åºåè²¡åæä»¶ï¼çé åç¹å®äºå¯¦ï¼å¯¦é«ï¼çè½åãæ¬ç ç©¶æ¢è¨äºå¤§åèªè¨æ¨¡åå¨æ¡ä¾æ³æä»¶ä¸­è¾¨è­é åç¹å®å¯¦é«ï¼ä¾å¦æ³é¢ãè«é¡äººãæ³å®ãå¾å¸«ãç­è¾¯äººãFIR ç·¨èï¼çæç¨ï¼ç¹å¥éæ³¨å®åèçé åç¹å®èªè¨è¤éæ§åèçµ¡è®åçè½åãæ¬ç ç©¶è©ä¼°äºæåé²çå¤§åèªè¨æ¨¡åæ¶æ§ï¼åæ¬ Large Language Model Meta AI 3ãMistral å Gemmaï¼å¨æåéå°å°åº¦å¸æ³ææ¬éèº«æé çå¸æ³äºå¯¦æ¹é¢çæè½ãMistral å Gemma æçºè¡¨ç¾æä½³çæ¨¡åï¼å±ç¤ºäºæºç¢ºå¯¦é«è¾¨è­è³ééè¦çå¹³è¡¡ç²¾ç¢ºåº¦åå¬åçãéäºç¼ç¾è­å¯¦äºå¤§åèªè¨æ¨¡åå¨å¸æ³æä»¶ä¸­çå¹å¼ï¼ä¸¦å±ç¤ºäºå®åå¦ä½ééç¢çé©ç¨æ¼æ·±å¥æª¢é©çç²¾ç¢ºãæçµç¹çè³æè¼¸åºï¼ä¾ä¿é²åå éç§å­¸ç ç©¶ã

##### **Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports**
2407.05758v1 by Yutong Zhang, Yi Pan, Tianyang Zhong, Peixin Dong, Kangni Xie, Yuxiao Liu, Hanqi Jiang, Zhengliang Liu, Shijie Zhao, Tuo Zhang, Xi Jiang, Dinggang Shen, Tianming Liu, Xin Zhang

Medical images and radiology reports are crucial for diagnosing medical
conditions, highlighting the importance of quantitative analysis for clinical
decision-making. However, the diversity and cross-source heterogeneity of these
data challenge the generalizability of current data-mining methods. Multimodal
large language models (MLLMs) have recently transformed many domains,
significantly affecting the medical field. Notably, Gemini-Vision-series
(Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in
Artificial General Intelligence (AGI) for computer vision, showcasing their
potential in the biomedical domain. In this study, we evaluated the performance
of the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation
across 14 medical imaging datasets, including 5 medical imaging categories
(dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3
radiology report datasets. The investigated tasks encompass disease
classification, lesion segmentation, anatomical localization, disease
diagnosis, report generation, and lesion detection. Our experimental results
demonstrated that Gemini-series models excelled in report generation and lesion
detection but faces challenges in disease classification and anatomical
localization. Conversely, GPT-series models exhibited proficiency in lesion
segmentation and anatomical localization but encountered difficulties in
disease diagnosis and lesion detection. Additionally, both the Gemini series
and GPT series contain models that have demonstrated commendable generation
efficiency. While both models hold promise in reducing physician workload,
alleviating pressure on limited healthcare resources, and fostering
collaboration between clinical practitioners and artificial intelligence
technologies, substantial enhancements and comprehensive validations remain
imperative before clinical deployment.

æè¦ï¼<paragraph>é«å­¸å½±ååæ¾å°ç§å ±åå°è¨ºæ·é«ççæ³è³ééè¦ï¼çªé¡¯äºå®éåæå¨è¨åºæ±ºç­ä¸­çéè¦æ§ãç¶èï¼éäºæ¸æçå¤æ¨£æ§åè·¨ä¾æºç°è³ªæ§ææ°äºç¶åæ¸ææææ¹æ³çæ¦æ¬æ§ãå¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) è¿ä¾å·²è½è®è¨±å¤é åï¼å°é«å­¸é åå½±é¿éå¤§ãå¼å¾æ³¨æçæ¯ï¼Gemini-Vision ç³»å (Gemini) å GPT-4 ç³»å (GPT-4) æ¨¡åå·²æçºé»è¦è¦è¦ºä¸­äººå·¥éç¨æºæ§ (AGI) çå¸ç¯è½ç§»ï¼å±ç¤ºäºå®åå¨çç©é«å­¸é åçæ½åãå¨éé ç ç©¶ä¸­ï¼æåè©ä¼°äº GeminiãGPT-4 å 4 åç±éå¤§åæ¨¡åå¨ 14 åé«çå½±åæ¸æéä¸çå»£æ³è©ä¼°è¡¨ç¾ï¼åæ¬ 5 åé«çå½±åé¡å¥ï¼ç®èç§ãæ¾å°ç§ãçç§ãç¼ç§åå§è¦é¡æª¢æ¥ï¼ï¼ä»¥å 3 åæ¾å°ç§å ±åæ¸æéãæèª¿æ¥çä»»ååæ¬ç¾çåé¡ãçç¶åå²ãè§£åå®ä½ãç¾çè¨ºæ·ãå ±åçæåçç¶æª¢æ¸¬ãæåçå¯¦é©çµæè¡¨æï¼Gemini ç³»åæ¨¡åå¨å ±åçæåçç¶æª¢æ¸¬æ¹é¢è¡¨ç¾åºè²ï¼ä½å¨ç¾çåé¡åè§£åå®ä½æ¹é¢é¢è¨ææ°ãç¸åï¼GPT ç³»åæ¨¡åå¨çç¶åå²åè§£åå®ä½æ¹é¢è¡¨ç¾åºçç·´åº¦ï¼ä½å¨ç¾çè¨ºæ·åçç¶æª¢æ¸¬æ¹é¢éå°å°é£ãæ­¤å¤ï¼Gemini ç³»åå GPT ç³»åé½åå«å·²è­æå·æå¯åçææççæ¨¡åãåç®¡éå©ç¨®æ¨¡åé½æææ¸å°é«å¸«çå·¥ä½éï¼æ¸è¼æéé«çä¿å¥è³æºçå£åï¼ä¸¦ä¿é²è¨åºå¾æ¥­äººå¡èäººå·¥æºæ§æè¡ä¹éçåä½ï¼ä½å¨è¨åºé¨ç½²ä¹åï¼å¯¦è³ªæ§çå¢å¼·åå¨é¢çé©è­ä»ç¶å¢å¨å¿è¡ã</paragraph>

##### **RadiomicsFill-Mammo: Synthetic Mammogram Mass Manipulation with Radiomics Features**
2407.05683v1 by Inye Na, Jonghun Kim, Eun Sook Ko, Hyunjin Park

Motivated by the question, "Can we generate tumors with desired attributes?''
this study leverages radiomics features to explore the feasibility of
generating synthetic tumor images. Characterized by its low-dimensional yet
biologically meaningful markers, radiomics bridges the gap between complex
medical imaging data and actionable clinical insights. We present
RadiomicsFill-Mammo, the first of the RadiomicsFill series, an innovative
technique that generates realistic mammogram mass images mirroring specific
radiomics attributes using masked images and opposite breast images, leveraging
a recent stable diffusion model. This approach also allows for the
incorporation of essential clinical variables, such as BI-RADS and breast
density, alongside radiomics features as conditions for mass generation.
Results indicate that RadiomicsFill-Mammo effectively generates diverse and
realistic tumor images based on various radiomics conditions. Results also
demonstrate a significant improvement in mass detection capabilities,
leveraging RadiomicsFill-Mammo as a strategy to generate simulated samples.
Furthermore, RadiomicsFill-Mammo not only advances medical imaging research but
also opens new avenues for enhancing treatment planning and tumor simulation.
Our code is available at https://github.com/nainye/RadiomicsFill.

æè¦ï¼<paragraph>æ¬ç ç©¶ä»¥ãæåè½çæå·ææéå±¬æ§çè«ç¤åï¼ãéååé¡çºåæ©ï¼å©ç¨æ¾å°ç¹å¾µä¾æ¢è¨çæåæè«ç¤å½±åçå¯è¡æ§ãæ¾å°ç¹å¾µä»¥å¶ä½ç¶­åº¦ä¸å·æçç©æç¾©çæ¨è¨çºç¹å¾µï¼å½è£äºè¤éé«å­¸å½±åè³æèå¯æä½è¨åºè¦è§£ä¹éçå·®è·ãæåæåº RadiomicsFill-Mammoï¼RadiomicsFill ç³»åçç¬¬ä¸åï¼éæ¯ä¸ç¨®åµæ°çæè¡ï¼å©ç¨é®ç½©å½±ååå°å´ä¹³æ¿å½±åï¼ä¸¦å©ç¨æè¿çç©©å®æ´æ£æ¨¡åï¼çæåæ ç¹å®æ¾å°ç¹å¾µå±¬æ§çé¼çä¹³æ¿æå½±è«å¡å½±åãéç¨®æ¹æ³éåè¨±å°åºæ¬è¨åºè®æ¸ï¼ä¾å¦ BI-RADS åä¹³æ¿å¯åº¦ï¼èæ¾å°ç¹å¾µä¸èµ·ä½çºçæè«å¡çæ¢ä»¶ãçµæè¡¨æï¼RadiomicsFill-Mammo è½ææå°æ ¹æåç¨®æ¾å°æ¢ä»¶çæå¤æ¨£åä¸é¼ççè«ç¤å½±åãçµæéè­æäºè«å¡æª¢æ¸¬è½åçé¡¯èæåï¼å©ç¨ RadiomicsFill-Mammo ä½çºçææ¨¡æ¬æ¨£æ¬çç­ç¥ãæ­¤å¤ï¼RadiomicsFill-Mammo ä¸åæ¨åäºé«å­¸å½±åç ç©¶ï¼éçºå¢å¼·æ²»çè¦ååè«ç¤æ¨¡æ¬éé¢äºæ°éå¾ãæåçç¨å¼ç¢¼å¯å¨ https://github.com/nainye/RadiomicsFill åå¾ã</paragraph>

##### **WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering**
2407.05603v1 by Pingyi Chen, Chenglu Zhu, Sunyi Zheng, Honglin Li, Lin Yang

Whole slide imaging is routinely adopted for carcinoma diagnosis and
prognosis. Abundant experience is required for pathologists to achieve accurate
and reliable diagnostic results of whole slide images (WSI). The huge size and
heterogeneous features of WSIs make the workflow of pathological reading
extremely time-consuming. In this paper, we propose a novel framework (WSI-VQA)
to interpret WSIs by generative visual question answering. WSI-VQA shows
universality by reframing various kinds of slide-level tasks in a
question-answering pattern, in which pathologists can achieve
immunohistochemical grading, survival prediction, and tumor subtyping following
human-machine interaction. Furthermore, we establish a WSI-VQA dataset which
contains 8672 slide-level question-answering pairs with 977 WSIs. Besides the
ability to deal with different slide-level tasks, our generative model which is
named Wsi2Text Transformer (W2T) outperforms existing discriminative models in
medical correctness, which reveals the potential of our model to be applied in
the clinical scenario. Additionally, we also visualize the co-attention mapping
between word embeddings and WSIs as an intuitive explanation for diagnostic
results. The dataset and related code are available at
https://github.com/cpystan/WSI-VQA.

æè¦ï¼å¨åçå½±åéå¸¸ç¨æ¼çççè¨ºæ·åé å¾ãççå­¸å®¶éè¦æè±å¯çç¶é©æè½å°å¨åçå½±å (WSI) ååºæºç¢ºä¸å¯é çè¨ºæ·çµæãWSI çå°ºå¯¸é¾å¤§ä¸ç¹å¾µç°è³ªï¼ä½¿å¾ççå­¸å¤è®çå·¥ä½æµç¨æ¥µçºèæãå¨æ¬æä¸­ï¼æåæåºäºä¸åæ°çæ¡æ¶ (WSI-VQA)ï¼ééçæå¼è¦è¦ºåç­ä¾è©®é WSIãWSI-VQA ééå¨åç­æ¨¡å¼ä¸­éæ°å®ç¾©åç¨®åçå±¤ç´ä»»åï¼å±ç¾å¶éç¨æ§ï¼ççå­¸å®¶å¯ä»¥å¨äººæ©äºåå¾ï¼å®æåç«çµç¹åå­¸åç´ãå­æ´»é æ¸¬åè«ç¤äºååé¡ãæ­¤å¤ï¼æåå»ºç«äºä¸å WSI-VQA è³æéï¼å¶ä¸­åå« 8672 ååçå±¤ç´åç­å°ï¼ä»¥å 977 å WSIãé¤äºè½å¤ èçä¸åçåçå±¤ç´ä»»åå¤ï¼æååçº Wsi2Text Transformer (W2T) ççææ¨¡åå¨é«å­¸æ­£ç¢ºæ§æ¹é¢åªæ¼ç¾æçå¤å¥æ¨¡åï¼éæ­ç¤ºäºæåçæ¨¡åå¨è¨åºå ´æ¯ä¸­æç¨çæ½åãæ­¤å¤ï¼æåéå°è©åµå¥å WSI ä¹éçå±åæ³¨ææ å°è¦è¦ºåï¼ä½çºè¨ºæ·çµæçç´è§è§£éãè³æéåç¸éç¨å¼ç¢¼å¯å¨ https://github.com/cpystan/WSI-VQA åå¾ã

##### **Accelerating MRI Uncertainty Estimation with Mask-based Bayesian Neural Network**
2407.05521v1 by Zehuan Zhang, Matej Genci, Hongxiang Fan, Andreas Wetscherek, Wayne Luk

Accurate and reliable Magnetic Resonance Imaging (MRI) analysis is
particularly important for adaptive radiotherapy, a recent medical advance
capable of improving cancer diagnosis and treatment. Recent studies have shown
that IVIM-NET, a deep neural network (DNN), can achieve high accuracy in MRI
analysis, indicating the potential of deep learning to enhance diagnostic
capabilities in healthcare. However, IVIM-NET does not provide calibrated
uncertainty information needed for reliable and trustworthy predictions in
healthcare. Moreover, the expensive computation and memory demands of IVIM-NET
reduce hardware performance, hindering widespread adoption in realistic
scenarios. To address these challenges, this paper proposes an
algorithm-hardware co-optimization flow for high-performance and reliable MRI
analysis. At the algorithm level, a transformation design flow is introduced to
convert IVIM-NET to a mask-based Bayesian Neural Network (BayesNN),
facilitating reliable and efficient uncertainty estimation. At the hardware
level, we propose an FPGA-based accelerator with several hardware
optimizations, such as mask-zero skipping and operation reordering.
Experimental results demonstrate that our co-design approach can satisfy the
uncertainty requirements of MRI analysis, while achieving 7.5 times and 32.5
times speedup on an Xilinx VU13P FPGA compared to GPU and CPU implementations
with reduced power consumption.

æè¦ï¼ç²¾æºå¯é çç£æ¯é å½± (MRI) åæå°æ¼é©ææ§æ¾å°æ²»çç¹å¥éè¦ï¼éæ¯ä¸ç¨®è¿æé«çé²å±ï¼è½å¤ æ¹åççè¨ºæ·åæ²»çãæè¿çç ç©¶é¡¯ç¤ºï¼æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) ç IVIM-NET è½å¨ MRI åæä¸­éå°é«æºç¢ºåº¦ï¼é¡¯ç¤ºæ·±åº¦å­¸ç¿ææ½åå¢å¼·é«çä¿å¥ä¸­çè¨ºæ·è½åãç¶èï¼IVIM-NET æ²ææä¾å¨é«çä¿å¥ä¸­é²è¡å¯é ä¸å¼å¾ä¿¡è³´çé æ¸¬æéçæ ¡æºä¸ç¢ºå®æ§è³è¨ãæ­¤å¤ï¼IVIM-NET æè²´çéç®åè¨æ¶é«éæ±éä½äºç¡¬é«æè½ï¼é»ç¤äºå¨å¯¦éå ´æ¯ä¸­çå»£æ³æ¡ç¨ãçºäºè§£æ±ºéäºææ°ï¼æ¬ææåºäºä¸ç¨®æ¼ç®æ³èç¡¬é«å±åæä½³åçæµç¨ï¼ä»¥é²è¡é«æ§è½ä¸å¯é ç MRI åæãå¨æ¼ç®æ³å±¤ç´ï¼å¼å¥äºè½æè¨­è¨æµç¨ï¼å° IVIM-NET è½æçºåºæ¼é®ç½©çè²æ°ç¥ç¶ç¶²è·¯ (BayesNN)ï¼ä¿é²å¯é ä¸ææçéç¢ºå®æ§ä¼°è¨ãå¨ç¡¬é«å±¤ç´ï¼æåæåºäºä¸ç¨®åºæ¼ FPGA çå éå¨ï¼å·åå¤é ç¡¬é«æä½³åï¼ä¾å¦é®ç½©é¶è·³éåéç®éæ°æåºãå¯¦é©çµæè­æï¼æåçå±åè¨­è¨æ¹æ³å¯ä»¥æ»¿è¶³ MRI åæçä¸ç¢ºå®æ§éæ±ï¼åæå¨ Xilinx VU13P FPGA ä¸å¯¦ç¾æ¯ GPU å CPU å¯¦ä½å¿«ä¸ 7.5 åå 32.5 åçéåº¦ï¼ä¸åèéä½ã

##### **A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions**
2407.05458v1 by Fei Wang, Weibo Gao, Qi Liu, Jiatong Li, Guanhao Zhao, Zheng Zhang, Zhenya Huang, Mengxiao Zhu, Shijin Wang, Wei Tong, Enhong Chen

Cognitive diagnosis has been developed for decades as an effective
measurement tool to evaluate human cognitive status such as ability level and
knowledge mastery. It has been applied to a wide range of fields including
education, sport, psychological diagnosis, etc. By providing better awareness
of cognitive status, it can serve as the basis for personalized services such
as well-designed medical treatment, teaching strategy and vocational training.
This paper aims to provide a survey of current models for cognitive diagnosis,
with more attention on new developments using machine learning-based methods.
By comparing the model structures, parameter estimation algorithms, model
evaluation methods and applications, we provide a relatively comprehensive
review of the recent trends in cognitive diagnosis models. Further, we discuss
future directions that are worthy of exploration. In addition, we release two
Python libraries: EduData for easy access to some relevant public datasets we
have collected, and EduCDM that implements popular CDMs to facilitate both
applications and research purposes.

æè¦ï¼èªç¥è¨ºæ·å·²ç¼å±æ¸åå¹´ï¼ä½çºè©ä¼°äººé¡èªç¥çæï¼ä¾å¦è½åæ°´å¹³åç¥è­ææ¡ï¼çæææ¸¬éå·¥å·ãå®å·²è¢«æç¨æ¼å»£æ³çé åï¼åæ¬æè²ãé«è²ãå¿çè¨ºæ·ç­ãééæä¾å°èªç¥çæçæ´å¥½èªè­ï¼å®å¯ä»¥ä½çºåäººåæåçåºç¤ï¼ä¾å¦ç²¾å¿è¨­è¨çé«çæ²»çãæå­¸ç­ç¥åè·æ¥­è¨ç·´ãæ¬ææ¨å¨æä¾èªç¥è¨ºæ·ç¶åæ¨¡åçç¶è¿°ï¼ä¸¦æ´éæ³¨ä½¿ç¨åºæ¼æ©å¨å­¸ç¿çæ¹æ³çæ°ç¼å±ãééæ¯è¼æ¨¡åçµæ§ãåæ¸ä¼°è¨æ¼ç®æ³ãæ¨¡åè©ä¼°æ¹æ³åæç¨ï¼æåå°èªç¥è¨ºæ·æ¨¡åçææ°è¶¨å¢æä¾äºç¸å°å¨é¢çåé¡§ãæ­¤å¤ï¼æåè¨è«äºå¼å¾æ¢ç´¢çæªä¾æ¹åãæ­¤å¤ï¼æåç¼å¸äºå©å Python ç¨å¼åº«ï¼EduDataï¼ç¨æ¼è¼é¬å­åæåæ¶éçä¸äºç¸éå¬éè³æéï¼ä»¥å EduCDMï¼ç¨æ¼å¯¦ä½ç±éç CDMï¼ä»¥ä¿é²æç¨åç ç©¶ç®çã

##### **Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**
2407.05440v1 by P. N. Karthikayan, Yoga Sri Varshan V, Hitesh Gupta Kattamuri, Umarani Jayaraman

This paper presents dilated Residual Network (ResNet) models for disease
classification from retinal fundus images. Dilated convolution filters are used
to replace normal convolution filters in the higher layers of the ResNet model
(dilated ResNet) in order to improve the receptive field compared to the normal
ResNet model for disease classification. This study introduces
computer-assisted diagnostic tools that employ deep learning, enhanced with
explainable AI techniques. These techniques aim to make the tool's
decision-making process transparent, thereby enabling medical professionals to
understand and trust the AI's diagnostic decision. They are particularly
relevant in today's healthcare landscape, where there is a growing demand for
transparency in AI applications to ensure their reliability and ethical use.
The dilated ResNet is used as a replacement for the normal ResNet to enhance
the classification accuracy of retinal eye diseases and reduce the required
computing time. The dataset used in this work is the Ocular Disease Intelligent
Recognition (ODIR) dataset which is a structured ophthalmic database with eight
classes covering most of the common retinal eye diseases. The evaluation
metrics used in this work include precision, recall, accuracy, and F1 score. In
this work, a comparative study has been made between normal ResNet models and
dilated ResNet models on five variants namely ResNet-18, ResNet-34, ResNet-50,
ResNet-101, and ResNet-152. The dilated ResNet model shows promising results as
compared to normal ResNet with an average F1 score of 0.71, 0.70, 0.69, 0.67,
and 0.70 respectively for the above respective variants in ODIR multiclass
disease classification.

æè¦ï¼<paragraph>æ¬ææåºäºç¨äºè§ç½èç¼åºå¾åç¾çåç±»çæ©å¼ æ®å·®ç½ç» (ResNet) æ¨¡åãæ©å¼ å·ç§¯æ»¤æ³¢å¨ç¨äºæ¿æ¢ ResNet æ¨¡åè¾é«å±ä¸­çæ­£å¸¸å·ç§¯æ»¤æ³¢å¨ï¼æ©å¼  ResNetï¼ï¼ä»¥æ¹åä¸ç¨äºç¾çåç±»çæ­£å¸¸ ResNet æ¨¡åç¸æ¯çæåéãæ¬ç ç©¶ä»ç»äºéç¨æ·±åº¦å­¦ä¹ çè®¡ç®æºè¾å©è¯æ­å·¥å·ï¼å¹¶éè¿å¯è§£éç AI ææ¯è¿è¡äºå¢å¼ºãè¿äºææ¯æ¨å¨ä½¿è¯¥å·¥å·çå³ç­è¿ç¨éæåï¼ä»èä½¿å»çä¸ä¸äººåè½å¤çè§£åä¿¡ä»» AI çè¯æ­å³ç­ãå®ä»¬å¨å½ä»å»çä¿å¥é¢åå°¤ä¸ºéè¦ï¼å ä¸ºå¯¹ AI åºç¨ç¨åºçéæåº¦éæ±ä¸æ­å¢é¿ï¼ä»¥ç¡®ä¿å¶å¯é æ§åéå¾·ä½¿ç¨ãæ©å¼  ResNet ç¨ä½æ­£å¸¸ ResNet çæ¿ä»£åï¼ä»¥æé«è§ç½èç¼ççåç±»åç¡®æ§å¹¶åå°æéçè®¡ç®æ¶é´ãæ¬å·¥ä½ä¸­ä½¿ç¨çæ°æ®éæ¯ Ocular Disease Intelligent Recognition (ODIR) æ°æ®éï¼è¿æ¯ä¸ä¸ªç»æåçç¼ç§æ°æ®åºï¼åå«å«ç±»æ¶µçå¤§å¤æ°å¸¸è§è§ç½èç¼çãæ¬å·¥ä½ä¸­ä½¿ç¨çè¯ä¼°ææ åæ¬ç²¾ç¡®åº¦ãå¬åçãåç¡®åº¦å F1 åæ°ãå¨è¿é¡¹å·¥ä½ä¸­ï¼å¯¹æ­£å¸¸ ResNet æ¨¡ååæ©å¼  ResNet æ¨¡åå¨äºä¸ªåä½ï¼å³ ResNet-18ãResNet-34ãResNet-50ãResNet-101 å ResNet-152ï¼ä¹é´è¿è¡äºæ¯è¾ç ç©¶ãä¸æ­£å¸¸ ResNet ç¸æ¯ï¼æ©å¼  ResNet æ¨¡åæ¾ç¤ºåºæå¸æçç»æï¼å¨ ODIR å¤ç±»ç¾çåç±»ä¸­ï¼ä¸è¿°åä¸ªåä½çå¹³å F1 åæ°åå«ä¸º 0.71ã0.70ã0.69ã0.67 å 0.70ã</paragraph>

##### **FM-OSD: Foundation Model-Enabled One-Shot Detection of Anatomical Landmarks**
2407.05412v1 by Juzheng Miao, Cheng Chen, Keli Zhang, Jie Chuai, Quanzheng Li, Pheng-Ann Heng

One-shot detection of anatomical landmarks is gaining significant attention
for its efficiency in using minimal labeled data to produce promising results.
However, the success of current methods heavily relies on the employment of
extensive unlabeled data to pre-train an effective feature extractor, which
limits their applicability in scenarios where a substantial amount of unlabeled
data is unavailable. In this paper, we propose the first foundation
model-enabled one-shot landmark detection (FM-OSD) framework for accurate
landmark detection in medical images by utilizing solely a single template
image without any additional unlabeled data. Specifically, we use the frozen
image encoder of visual foundation models as the feature extractor, and
introduce dual-branch global and local feature decoders to increase the
resolution of extracted features in a coarse to fine manner. The introduced
feature decoders are efficiently trained with a distance-aware similarity
learning loss to incorporate domain knowledge from the single template image.
Moreover, a novel bidirectional matching strategy is developed to improve both
robustness and accuracy of landmark detection in the case of scattered
similarity map obtained by foundation models. We validate our method on two
public anatomical landmark detection datasets. By using solely a single
template image, our method demonstrates significant superiority over strong
state-of-the-art one-shot landmark detection methods.

æè¦ï¼è§£åæ¨èªçä¸ç¼åµæ¸¬å å¶ä½¿ç¨æå°æ¨ç±¤è³æç¢çæåæ¯çµæçæçèç²å¾é¡¯èéæ³¨ãç¶èï¼ç®åæ¹æ³çæåæ¥µåº¦ä¾è³´éç¨å»£æ³çæªæ¨ç±¤è³æä¾é åè¨ç·´ä¸åææç¹å¾µèåå¨ï¼ééå¶äºå¶å¨å¤§éæªæ¨ç±¤è³æä¸å¯ç¨çææ³ä¸çé©ç¨æ§ãå¨æ¬æä¸­ï¼æåæåºç¬¬ä¸ååºç¤æ¨¡ååç¨çå®ç¼æ¨èªåµæ¸¬ (FM-OSD) æ¶æ§ï¼èç±ååå©ç¨å®ä¸ç¯æ¬å½±åèç¡ä»»ä½å¶ä»æªæ¨ç±¤è³æï¼å¨é«å­¸å½±åä¸­é²è¡ç²¾ç¢ºæ¨èªåµæ¸¬ãå·é«ä¾èªªï¼æåä½¿ç¨è¦è¦ºåºç¤æ¨¡åçåçµå½±åç·¨ç¢¼å¨ä½çºç¹å¾µèåå¨ï¼ä¸¦å¼å¥éåæ¯å¨å±åå±é¨ç¹å¾µè§£ç¢¼å¨ï¼ä»¥ç²å°ç´°çæ¹å¼å¢å èåç¹å¾µçåè¾¨çãå¼å¥çç¹å¾µè§£ç¢¼å¨å©ç¨è·é¢æç¥ç¸ä¼¼æ§å­¸ç¿æå¤±å½æ¸é²è¡ææè¨ç·´ï¼ä»¥ç´å¥å®ä¸ç¯æ¬å½±åçé åç¥è­ãæ­¤å¤ï¼éç¼äºä¸ç¨®æ°ç©çéåå¹éç­ç¥ï¼ä»¥æé«åºç¤æ¨¡ååå¾çæ£ä½ç¸ä¼¼æ§åå¨æ¨èªåµæ¸¬ä¸­çç©©å¥æ§åæºç¢ºæ§ãæåå¨å©åå¬éçè§£åæ¨èªåµæ¸¬è³æéé©è­æåçæ¨¡åãæåçæ¨¡ååä½¿ç¨å®ä¸ç¯æ¬å½±åï¼è­æå¶åªæ¼ç¾ææè¡ä¸­å¼·å¤§çå®ç¼æ¨èªåµæ¸¬æ¹æ³ã

##### **BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records**
2407.05213v1 by Weimin Lyu, Zexin Bi, Fusheng Wang, Chao Chen

The advent of clinical language models integrated into electronic health
records (EHR) for clinical decision support has marked a significant
advancement, leveraging the depth of clinical notes for improved
decision-making. Despite their success, the potential vulnerabilities of these
models remain largely unexplored. This paper delves into the realm of backdoor
attacks on clinical language models, introducing an innovative attention-based
backdoor attack method, BadCLM (Bad Clinical Language Models). This technique
clandestinely embeds a backdoor within the models, causing them to produce
incorrect predictions when a pre-defined trigger is present in inputs, while
functioning accurately otherwise. We demonstrate the efficacy of BadCLM through
an in-hospital mortality prediction task with MIMIC III dataset, showcasing its
potential to compromise model integrity. Our findings illuminate a significant
security risk in clinical decision support systems and pave the way for future
endeavors in fortifying clinical language models against such vulnerabilities.

æè¦ï¼è¨åºèªè¨æ¨¡åæ´åå°é»å­å¥åº·ç´é (EHR) ä¸­ä»¥é²è¡è¨åºæ±ºç­æ¯æ´ï¼æ¨èªèä¸é éå¤§çé²å±ï¼å©ç¨è¨åºç­è¨çæ·±åº¦ä¾æ¹åæ±ºç­å¶å®ãåç®¡éäºæ¨¡ååå¾äºæåï¼ä½å®åçæ½å¨æ¼æ´å¨å¾å¤§ç¨åº¦ä¸ä»æªå¾å°æ¢ç´¢ãæ¬ææ·±å¥æ¢è¨äºéå°è¨åºèªè¨æ¨¡åçå¾éæ»æé åï¼ä»ç´¹äºä¸ç¨®åµæ°çåºæ¼æ³¨æåçå¾éæ»ææ¹æ³ BadCLMï¼ä¸è¯è¨åºèªè¨æ¨¡åï¼ãéç¨®æè¡ç§å¯å°å¨æ¨¡åä¸­åµå¥äºä¸åå¾éï¼å°è´å®åå¨è¼¸å¥ä¸­å­å¨é å®ç¾©è§¸ç¼å¨æç¢çä¸æ­£ç¢ºçé æ¸¬ï¼èå¶ä»ææ³ä¸åæºç¢ºéä½ãæåééä½¿ç¨ MIMIC III è³æéé²è¡é¢å§æ­»äº¡çé æ¸¬ä»»åä¾è­æ BadCLM çæåï¼å±ç¤ºäºå¶æå®³æ¨¡åå®æ´æ§çæ½åãæåçç¼ç¾æ­ç¤ºäºè¨åºæ±ºç­æ¯æ´ç³»çµ±ä¸­çä¸åéå¤§å®å¨é¢¨éªï¼ä¸¦çºæªä¾å å¼·è¨åºèªè¨æ¨¡åä»¥æå°æ­¤é¡æ¼æ´çåªåéªå¹³äºéè·¯ã

##### **RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models**
2407.05131v1 by Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao

The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on three medical VQA datasets, achieving an average
improvement of 20.8% in factual accuracy. We publicly release our benchmark and
code in https://github.com/richard-peng-xia/RULE.

æè¦ï¼æè¿åºç¾çé«çå¤§åèªè¨æ¨¡å (Med-LVLMs) æåäºé«çè¨ºæ·ãç¶èï¼ç®åç Med-LVLMs ç¶å¸¸éå°äºå¯¦åé¡ï¼éå¸¸æç¢çèå·²ç¢ºç«çé«çäºå¯¦ä¸ç¬¦çåæãå©ç¨å¤é¨ç¥è­çæª¢ç´¢å¢å¼·çæ (RAG) å¯ä»¥æ¹åéäºæ¨¡åçäºå¯¦æºç¢ºæ§ï¼ä½å¼å¥äºå©åä¸»è¦ææ°ãé¦åï¼æéçæª¢ç´¢å§å®¹å¯è½ç¡æ³æ¶µèææå¿è¦çè³è¨ï¼èéåº¦çæª¢ç´¢å¯è½æå¼å¥ä¸ç¸éåä¸æºç¢ºçåèï¼å¹²æ¾æ¨¡åççæãå¶æ¬¡ï¼å¨æ¨¡ååæ¬æ­£ç¢ºåæçææ³ä¸ï¼æç¨ RAG å¯è½æéåº¦ä¾è³´æª¢ç´¢å°çå§å®¹ï¼å°è´ä¸æ­£ç¢ºçç­æ¡ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäº RULEï¼å®åå«å©åçµæé¨åãé¦åï¼æåå¼å¥äºä¸ç¨®å¯è­æææçç­ç¥ï¼ééæ ¡æºæª¢ç´¢å°çå§å®¹æ¸éä¾æ§å¶äºå¯¦é¢¨éªãå¶æ¬¡ï¼æ ¹æéåº¦ä¾è³´æª¢ç´¢å°çå§å®¹å°è´é¯èª¤çç¯ä¾ï¼æåç­åäºä¸ååå¥½è³æéä¾å¾®èª¿æ¨¡åï¼å¹³è¡¡å¶å¨çææå°å§å¨ç¥è­åæª¢ç´¢å°çå§å®¹çä¾è³´æ§ãæåå¨ä¸åé«ç VQA è³æéä¸å±ç¤ºäº RULE çæææ§ï¼å¨äºå¯¦æºç¢ºæ§æ¹é¢å¹³åæåäº 20.8%ãæåå¨ https://github.com/richard-peng-xia/RULE ä¸­å¬éç¼å¸æåçåºæºåç¨å¼ç¢¼ã

##### **Linear Attention Based Deep Nonlocal Means Filtering for Multiplicative Noise Removal**
2407.05087v1 by Xiao Siyao, Huang Libing, Zhang Shunsheng

Multiplicative noise widely exists in radar images, medical images and other
important fields' images. Compared to normal noises, multiplicative noise has a
generally stronger effect on the visual expression of images. Aiming at the
denoising problem of multiplicative noise, we linearize the nonlocal means
algorithm with deep learning and propose a linear attention mechanism based
deep nonlocal means filtering (LDNLM). Starting from the traditional nonlocal
means filtering, we employ deep channel convolution neural networks to extract
the information of the neighborhood matrix and obtain representation vectors of
every pixel. Then we replace the similarity calculation and weighted averaging
processes with the inner operations of the attention mechanism. To reduce the
computational overhead, through the formula of similarity calculation and
weighted averaging, we derive a nonlocal filter with linear complexity.
Experiments on both simulated and real multiplicative noise demonstrate that
the LDNLM is more competitive compared with the state-of-the-art methods.
Additionally, we prove that the LDNLM possesses interpretability close to
traditional NLM.

æè¦ï¼ä¹æ§éè¨å»£æ³å­å¨æ¼é·éå½±åãé«å­¸å½±åç­éè¦é åçå½±åä¸­ãç¸è¼æ¼ä¸è¬éè¨ï¼ä¹æ§éè¨å°å½±åçè¦è¦ºè¡¨ç¾å·ææ®éæ´å¼·çå½±é¿ãéå°ä¹æ§éè¨çå»éè¨åé¡ï¼æåä»¥æ·±åº¦å­¸ç¿ç·æ§åéå±é¨åå¼æ¼ç®æ³ï¼ä¸¦æåºåºæ¼ç·æ§æ³¨æåæ©å¶çæ·±åº¦éå±é¨åå¼æ¿¾æ³¢ï¼LDNLMï¼ãå¾å³çµ±éå±é¨åå¼æ¿¾æ³¢åºç¼ï¼æåæ¡ç¨æ·±åº¦ééå·ç©ç¥ç¶ç¶²è·¯æåé°åç©é£çè³è¨ï¼ä¸¦åå¾æ¯åç«ç´ çè¡¨ç¤ºåéãæ¥èï¼æåä»¥æ³¨æåæ©å¶çå§é¨éç®åä»£ç¸ä¼¼åº¦è¨ç®èå æ¬å¹³åçç¨åºãçºäºéä½éç®è² æï¼æåééç¸ä¼¼åº¦è¨ç®èå æ¬å¹³åçå¬å¼ï¼æ¨å°åºå·æç·æ§è¤éåº¦çéå±é¨æ¿¾æ³¢å¨ãå¨æ¨¡æ¬èçå¯¦ä¹æ§éè¨ä¸çå¯¦é©åè­å¯¦ï¼LDNLM èç®åæåé²çæ¹æ³ç¸æ¯æ´å·ç«¶ç­åãæ­¤å¤ï¼æåè­æ LDNLM å·åæ¥è¿å³çµ± NLM çå¯è§£éæ§ã

##### **Brain Age Estimation with a Greedy Dual-Stream Model for Limited Datasets**
2407.04808v1 by Iman Kianian, Hedieh Sajedi

Brain age estimation involves predicting the biological age of individuals
from their brain images, which offers valuable insights into the aging process
and the progression of neurodegenerative diseases. Conducting large-scale
datasets for medical image analysis is a challenging and time-consuming task.
Existing approaches mostly depend on large datasets, which are hard to come by
and expensive. These approaches also require sophisticated, resource-intensive
models with a large number of parameters, necessitating a considerable amount
of processing power. As a result, there is a vital need to develop innovative
methods that can achieve robust performance with limited datasets and efficient
use of computational resources. This paper proposes a novel slice-based
dual-stream method called GDSM (Greedy Dual-Stream Model) for brain age
estimation. This method addresses the limitations of large dataset requirements
and computational resource intensiveness. The proposed method incorporates
local and global aspects of the brain, thereby refining the focus on specific
target regions. The approach employs four backbones to predict ages based on
local and global features, complemented by a final model for age correction.
Our method demonstrates a Mean Absolute Error (MAE) of 3.25 years on the test
set of IBID, which only contains 289 subjects. To demonstrate the robustness of
our approach for any small dataset, we analyzed the proposed method with the
IXI dataset and achieved an MAE of 4.18 years on the test set of IXI. By
leveraging dual-stream and greedy strategies, this approach achieves efficiency
and robust performance, making it comparable with other state-of-the-art
methods. The code for the GDSM model is available at
https://github.com/iman2693/GDSM.

æè¦ï¼å¤§è¦å¹´é½¡ä¼°è¨æ¶åå¾å¤§è¦å½±åé æ¸¬åé«ççç©å¹´é½¡ï¼éå°èåéç¨åç¥ç¶éåæ§ç¾ççé²å±æä¾äºå¯¶è²´çè¦è§£ãå°é«å­¸å½±ååæé²è¡å¤§è¦æ¨¡çæ¸æéèçæ¯ä¸é å·æææ°æ§ä¸èæçä»»åãç¾æçæ¹æ³å¤§å¤ä¾è³´æ¼å¤§åæ¸æéï¼èéäºæ¸æéé£ä»¥ç²å¾ä¸æè²´ãéäºæ¹æ³ééè¦è¤éçãè³æºå¯éåçæ¨¡åï¼éäºæ¨¡åå·æå¤§éçåæ¸ï¼éè¦å¤§éçèçè½åãå æ­¤ï¼è¿«åéè¦éç¼åµæ°çæ¹æ³ï¼éäºæ¹æ³å¯ä»¥å¨æéçæ¸æéåé«æå©ç¨è¨ç®è³æºçææ³ä¸å¯¦ç¾ç©©å¥çæ§è½ãæ¬ææåºäºä¸ç¨®ç¨±çº GDSMï¼è²ªå©ªéæµæ¨¡åï¼çæ°ååºæ¼åççéæµæ¹æ³ï¼ç¨æ¼å¤§è¦å¹´é½¡ä¼°è¨ãéç¨®æ¹æ³è§£æ±ºäºå°å¤§åæ¸æééæ±åè¨ç®è³æºå¯éæ§çéå¶ãææåºçæ¹æ³çµåäºå¤§è¦çå±é¨åå¨å±æ¹é¢ï¼å¾èç²¾ç¢ºéæ³¨å·é«çç®æ¨ååãè©²æ¹æ³æ¡ç¨ååä¸»å¹¹æ ¹æå±é¨åå¨å±ç¹å¾µé æ¸¬å¹´é½¡ï¼ä¸¦è¼ä»¥ä¸åæçµæ¨¡åé²è¡å¹´é½¡æ ¡æ­£ãæåçæ¨¡åå¨ååå« 289 ååè©¦èç IBID æ¸¬è©¦éä¸­å±ç¤ºäº 3.25 å¹´çå¹³åçµå°èª¤å·® (MAE)ãçºäºè­ææåçæ¨¡åå°ä»»ä½å°åæ¸æéçç©©å¥æ§ï¼æåä½¿ç¨ IXI æ¸æéåæäºææåºçæ¹æ³ï¼ä¸¦å¨ IXI çæ¸¬è©¦éä¸­å¯¦ç¾äº 4.18 å¹´ç MAEãééå©ç¨éæµåè²ªå©ªç­ç¥ï¼éç¨®æ¹æ³å¯¦ç¾äºé«æåç©©å¥çæ§è½ï¼ä½¿å¶èå¶ä»æåé²çæ¹æ³ç¸ç¶ãGDSM æ¨¡åçä»£ç¢¼å¯å¨ https://github.com/iman2693/GDSM ä¸ç²å¾ã

##### **Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework**
2407.04629v1 by Reza Averly, Xia Ning

Clinical named entity recognition (NER) aims to retrieve important entities
within clinical narratives. Recent works have demonstrated that large language
models (LLMs) can achieve strong performance in this task. While previous works
focus on proprietary LLMs, we investigate how open NER LLMs, trained
specifically for entity recognition, perform in clinical NER. In this paper, we
aim to improve them through a novel framework, entity decomposition with
filtering, or EDF. Our key idea is to decompose the entity recognition task
into several retrievals of sub-entity types. We also introduce a filtering
mechanism to remove incorrect entities. Our experimental results demonstrate
the efficacy of our framework across all metrics, models, datasets, and entity
types. Our analysis reveals that entity decomposition can recognize previously
missed entities with substantial improvement. We further provide a
comprehensive evaluation of our framework and an in-depth error analysis to
pave future works.

æè¦ï¼è¨åºå½åå¯¦é«è­å¥ (NER) æ¨å¨æ·åè¨åºæè¿°ä¸­çéè¦å¯¦é«ãæè¿çç ç©¶è¡¨æï¼å¤§åèªè¨æ¨¡å (LLM) å¯ä»¥å¨æ­¤ä»»åä¸­å¯¦ç¾å¼·å¤§çæè½ãéç¶ååçç ç©¶å°æ³¨æ¼å°æç LLMï¼ä½æåæ¢è¨äºå°ééå°å¯¦é«è­å¥è¨ç·´çéæ¾å¼ NER LLM å¨è¨åº NER ä¸­çè¡¨ç¾ãå¨æ¬æä¸­ï¼æåæ¨å¨ééä¸åæ°ç©çæ¶æ§ä¾æ¹åå®åï¼å³å¸¶æéæ¿¾çå¯¦é«åè§£ï¼æ EDFãæåçééµæ³æ³æ¯å°å¯¦é«è­å¥ä»»ååè§£çºå¤åå­å¯¦é«é¡åçæ·åãæåéå¼å¥äºä¸åéæ¿¾æ©å¶ä¾ç§»é¤ä¸æ­£ç¢ºçå¯¦é«ãæåçå¯¦é©çµæè­æäºæåæ¶æ§å¨ææææ¨ãæ¨¡åãè³æéåå¯¦é«é¡åä¸­çæè½ãæåçåæé¡¯ç¤ºï¼å¯¦é«åè§£å¯ä»¥è­å¥ååéºæ¼çå¯¦é«ï¼ä¸¦æé¡¯èçæ¹åãæåé²ä¸æ­¥æä¾äºæåæ¶æ§çå¨é¢è©ä¼°åæ·±å¥çé¯èª¤åæï¼çºæªä¾çç ç©¶éªè·¯ã

##### **Variational and Explanatory Neural Networks for Encoding Cancer Profiles and Predicting Drug Responses**
2407.04486v1 by Tianshu Feng, Rohan Gnanaolivu, Abolfazl Safikhani, Yuanhang Liu, Jun Jiang, Nicholas Chia, Alexander Partin, Priyanka Vasanthakumari, Yitan Zhu, Chen Wang

Human cancers present a significant public health challenge and require the
discovery of novel drugs through translational research. Transcriptomics
profiling data that describes molecular activities in tumors and cancer cell
lines are widely utilized for predicting anti-cancer drug responses. However,
existing AI models face challenges due to noise in transcriptomics data and
lack of biological interpretability. To overcome these limitations, we
introduce VETE (Variational and Explanatory Transcriptomics Encoder), a novel
neural network framework that incorporates a variational component to mitigate
noise effects and integrates traceable gene ontology into the neural network
architecture for encoding cancer transcriptomics data. Key innovations include
a local interpretability-guided method for identifying ontology paths, a
visualization tool to elucidate biological mechanisms of drug responses, and
the application of centralized large scale hyperparameter optimization. VETE
demonstrated robust accuracy in cancer cell line classification and drug
response prediction. Additionally, it provided traceable biological
explanations for both tasks and offers insights into the mechanisms underlying
its predictions. VETE bridges the gap between AI-driven predictions and
biologically meaningful insights in cancer research, which represents a
promising advancement in the field.

æè¦ï¼äººé¡ççå°å¬å±è¡çæ§æéå¤§ææ°ï¼éè¦ééè½è­¯ç ç©¶ç¼ç¾æ°è¥ç©ãæè¿°è«ç¤åçç´°èæ ªåå­æ´»åçè½éçµå­¸åæè³æå»£æ³ç¨æ¼é æ¸¬æçè¥ç©åæãç¶èï¼ç¾æç AI æ¨¡åå è½éçµå­¸è³æä¸­çéè¨åç¼ºä¹çç©å­¸å¯è§£éæ§èé¢è¨ææ°ãçºäºåæéäºéå¶ï¼æåå¼å¥äº VETEï¼è®ç°åè§£éæ§è½éçµå­¸ç·¨ç¢¼å¨ï¼ï¼éæ¯ä¸ç¨®æ°ç©çç¥ç¶ç¶²è·¯æ¶æ§ï¼å®çµåäºè®ç°çµæä»¥æ¸è¼éè¨ææï¼ä¸¦å°å¯è¿½è¹¤çåºå æ¬é«æ´åå°ç¥ç¶ç¶²è·¯æ¶æ§ä¸­ä»¥ç·¨ç¢¼ççè½éçµå­¸è³æãééµåµæ°åæ¬ä¸ç¨®å±é¨å¯è§£éæ§å¼å°æ¹æ³ï¼ç¨æ¼è­å¥æ¬é«è·¯å¾ï¼ä¸ç¨®ç¨æ¼é¡æè¥ç©åæççç©æ©å¶çè¦è¦ºåå·¥å·ï¼ä»¥åéä¸­å¼å¤§è¦æ¨¡è¶åæ¸æä½³åçæç¨ãVETE å¨çç´°èæ ªåé¡åè¥ç©åæé æ¸¬æ¹é¢è¡¨ç¾åºç©©å¥çæºç¢ºæ§ãæ­¤å¤ï¼å®çºéå©åä»»åæä¾äºå¯è¿½è¹¤ççç©å­¸è§£éï¼ä¸¦æä¾äºå°å¶é æ¸¬èå¾æ©å¶çè¦è§£ãVETE å½åäº AI é©åé æ¸¬èççç ç©¶ä¸­å·æçç©å­¸æç¾©çè¦è§£ä¹éçå·®è·ï¼éä»£è¡¨äºè©²é åçä¸é æåéçé²å±ã

##### **Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning**
2407.04449v1 by Saeed Shurrab, Alejandro Guerra-Manzanares, Farah E. Shamout

Self-supervised learning methods for medical images primarily rely on the
imaging modality during pretraining. While such approaches deliver promising
results, they do not leverage associated patient or scan information collected
within Electronic Health Records (EHR). Here, we propose to incorporate EHR
data during self-supervised pretraining with a Masked Siamese Network (MSN) to
enhance the quality of chest X-ray representations. We investigate three types
of EHR data, including demographic, scan metadata, and inpatient stay
information. We evaluate our approach on three publicly available chest X-ray
datasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT)
backbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the
representations via linear evaluation, our proposed method demonstrates
significant improvement compared to vanilla MSN and state-of-the-art
self-supervised learning baselines. Our work highlights the potential of
EHR-enhanced self-supervised pre-training for medical imaging. The code is
publicly available at: https://github.com/nyuad-cai/CXR-EHR-MSN

æè¦ï¼ç¨æ¼é«å­¸å½±åçèªç£ç£å¼å­¸ç¿æ¹æ³ä¸»è¦ä¾è³´æ¼é è¨ç·´æéçå½±åæ¨¡å¼ãéç¶æ­¤é¡æ¹æ³æä¾äºæåæ¯ççµæï¼ä½å®åä¸¦æªå©ç¨é»å­å¥åº·è¨é (EHR) ä¸­æ¶éçç¸éæ£èæææè³è¨ãå¨æ­¤ï¼æåå»ºè­°å¨ä½¿ç¨èé¢é£é«ç¶²è·¯ (MSN) é²è¡èªç£ç£é è¨ç·´æéç´å¥ EHR è³æï¼ä»¥æåè¸é¨ X åçè¡¨å¾µçåè³ªãæåæ¢è¨ä¸ç¨®é¡åç EHR è³æï¼åæ¬äººå£çµ±è¨è³æãææåè³æåä½é¢æéè³è¨ãæåå¨ä¸åå¬éçè¸é¨ X åçè³æéï¼MIMIC-CXRãCheXpert å NIH-14ï¼ä¸è©ä¼°æåçåæ³ï¼ä½¿ç¨å©åè¦è¦ºè½æå¨ (ViT) ä¸»å¹¹ï¼ç¹å¥æ¯ ViT-Tiny å ViT-Smallãå¨ééç·æ§è©ä¼°ä¾è©éè¡¨å¾µçåè³ªæï¼æåæåºçæ¹æ³èå³çµ± MSN åæåé²çèªç£ç£å¼å­¸ç¿åºæºç¸æ¯ï¼è¡¨ç¾åºé¡¯èçé²æ­¥ãæåçç ç©¶éé»èªªæäº EHR å¢å¼·çèªç£ç£é è¨ç·´å¨é«å­¸å½±åæ¹é¢çæ½åãæ­¤ç¨å¼ç¢¼å¯æ¼ä»¥ä¸ç¶²åå¬éåå¾ï¼https://github.com/nyuad-cai/CXR-EHR-MSN

##### **Query-Guided Self-Supervised Summarization of Nursing Notes**
2407.04125v1 by Ya Gao, Hans Moen, Saila Koivusalo, Miika Koskinen, Pekka Marttinen

Nursing notes, an important component of Electronic Health Records (EHRs),
keep track of the progression of a patient's health status during a care
episode. Distilling the key information in nursing notes through text
summarization techniques can improve clinicians' efficiency in understanding
patients' conditions when reviewing nursing notes. However, existing
abstractive summarization methods in the clinical setting have often overlooked
nursing notes and require the creation of reference summaries for supervision
signals, which is time-consuming. In this work, we introduce QGSumm, a
query-guided self-supervised domain adaptation framework for nursing note
summarization. Using patient-related clinical queries as guidance, our approach
generates high-quality, patient-centered summaries without relying on reference
summaries for training. Through automatic and manual evaluation by an expert
clinician, we demonstrate the strengths of our approach compared to the
state-of-the-art Large Language Models (LLMs) in both zero-shot and few-shot
settings. Ultimately, our approach provides a new perspective on conditional
text summarization, tailored to the specific interests of clinical personnel.

æè¦ï¼è­·çè¨éæ¯é»å­å¥åº·ç´é (EHR) çéè¦çµæé¨åï¼
å¨ç§è­·éç¨ä¸­è¿½è¹¤çæ£çå¥åº·çæé²å±ãå©ç¨æå­æè¦æè¡æçè­·çè¨éä¸­çééµè³è¨ï¼å¯ä»¥æåè¨åºé«å¸«å¨æª¢è¦è­·çè¨éæäºè§£çæ£çæ³çæçãç¶èï¼ç¾æçè¨åºæè¦æ¹æ³å¸¸å¸¸å¿½ç¥è­·çè¨éï¼ä¸éè¦å»ºç«åèæè¦ä½çºç£ç£è¨èï¼ééå¸¸èæãå¨éé å·¥ä½ä¸­ï¼æåæåº QGSummï¼ä¸åç¨æ¼è­·çè¨éæè¦çæ¥è©¢å¼å°å¼èªæç£ç£é åé©ææ¶æ§ãæåçåæ³ä½¿ç¨èçæ£ç¸éçè¨åºæ¥è©¢ä½çºæå¼ï¼å¨è¨ç·´ä¸­ä¸ä¾è³´åèæè¦ï¼å°±è½ç¢çé«åè³ªãä»¥çæ£çºä¸­å¿çæè¦ãééå°å®¶è¨åºé«å¸«çèªååæåè©ä¼°ï¼æåå±ç¤ºäºæåçæ¹æ³èæåé²çå¤§èªè¨æ¨¡å (LLM) ç¸æ¯å¨é¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿è¨­å®ä¸­çåªå¢ãæçµï¼æåçåæ³çºæ¢ä»¶å¼æå­æè¦æä¾äºæ°çè§é»ï¼å°ééå°è¨åºäººå¡çç¹å®èè¶£éèº«æé ã

##### **MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis**
2407.04106v1 by Asma Alkhaldi, Raneem Alnajim, Layan Alabdullatef, Rawan Alyahya, Jun Chen, Deyao Zhu, Ahmed Alsinan, Mohamed Elhoseiny

Recent advancements in artificial intelligence (AI) have precipitated
significant breakthroughs in healthcare, particularly in refining diagnostic
procedures. However, previous studies have often been constrained to limited
functionalities. This study introduces MiniGPT-Med, a vision-language model
derived from large-scale language models and tailored for medical applications.
MiniGPT-Med demonstrates remarkable versatility across various imaging
modalities, including X-rays, CT scans, and MRIs, enhancing its utility. The
model is capable of performing tasks such as medical report generation, visual
question answering (VQA), and disease identification within medical imagery.
Its integrated processing of both image and textual clinical data markedly
improves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's
superior performance in disease grounding, medical report generation, and VQA
benchmarks, representing a significant step towards reducing the gap in
assisting radiology practice. Furthermore, it achieves state-of-the-art
performance on medical report generation, higher than the previous best model
by 19\% accuracy. MiniGPT-Med promises to become a general interface for
radiology diagnoses, enhancing diagnostic efficiency across a wide range of
medical imaging applications.

æè¦ï¼é¨èäººå·¥æºæ§ (AI) çææ°é²å±ï¼é«çä¿å¥é ååºç¾äºé¡¯èççªç ´ï¼ç¹å¥æ¯å¨æ¹åè¨ºæ·ç¨åºæ¹é¢ãç¶èï¼ååçç ç©¶éå¸¸åéæ¼æéçåè½ãéé ç ç©¶å¼å¥äº MiniGPT-Medï¼éæ¯ä¸ç¨®æºèªå¤§è¦æ¨¡èªè¨æ¨¡åä¸å°çºé«çæç¨èè¨­è¨çè¦è¦ºèªè¨æ¨¡åãMiniGPT-Med å¨åç¨®å½±åæ¨¡å¼ä¸­å±ç¾åºéå¡çå¤åè½æ§ï¼åæ¬ X åãé»è¦æ·å±¤ææå MRIï¼é²èå¢å¼·å¶æç¨ãè©²æ¨¡åè½å¤ å·è¡è«¸å¦é«çå ±åçæãè¦è¦ºåç­ (VQA) åé«çå½±åä¸­çç¾çè­å¥ç­ä»»åãå®å°å½±ååæå­è¨åºè³ææ´åèçï¼é¡¯èæé«äºè¨ºæ·æºç¢ºæ§ãæåçå¯¦è­è©ä¼°è­å¯¦äº MiniGPT-Med å¨ç¾çåºç¤ãé«çå ±åçæå VQA åºæºä¸çåªç°è¡¨ç¾ï¼éä»£è¡¨äºç¸®å°åå©æ¾å°è¨ºæ·å¯¦åå·®è·çéè¦ä¸æ­¥ãæ­¤å¤ï¼å®å¨é«çå ±åçææ¹é¢éå°äºæåé²çè¡¨ç¾ï¼æ¯ååçæä½³æ¨¡åé«åº 19% çæºç¢ºåº¦ãMiniGPT-Med æææçºæ¾å°è¨ºæ·çéç¨ä»é¢ï¼é²èæååç¨®é«çå½±åæç¨ä¸­çè¨ºæ·æçã

##### **Mechanisms for Data Sharing in Collaborative Causal Inference (Extended Version)**
2407.11032v1 by BjÃ¶rn Filter, Ralf MÃ¶ller, ÃzgÃ¼r LÃ¼tfÃ¼ ÃzÃ§ep

Collaborative causal inference (CCI) is a federated learning method for
pooling data from multiple, often self-interested, parties, to achieve a common
learning goal over causal structures, e.g. estimation and optimization of
treatment variables in a medical setting. Since obtaining data can be costly
for the participants and sharing unique data poses the risk of losing
competitive advantages, motivating the participation of all parties through
equitable rewards and incentives is necessary. This paper devises an evaluation
scheme to measure the value of each party's data contribution to the common
learning task, tailored to causal inference's statistical demands, by comparing
completed partially directed acyclic graphs (CPDAGs) inferred from
observational data contributed by the participants. The Data Valuation Scheme
thus obtained can then be used to introduce mechanisms that incentivize the
agents to contribute data. It can be leveraged to reward agents fairly,
according to the quality of their data, or to maximize all agents' data
contributions.

æè¦ï¼åä½å ææ¨è« (CCI) æ¯ä¸ç¨®è¯é¦å­¸ç¿æ¹æ³ï¼ç¨æ¼å½æ´ä¾èªå¤åéå¸¸æ¯èªå©æ¹æ¸æï¼ä»¥éæå æçµæ§çå±åå­¸ç¿ç®æ¨ï¼ä¾å¦å¨é«çç°å¢ä¸­ä¼°è¨åæä½³åæ²»çè®æ¸ãç±æ¼åå¾æ¸æå°åèèä¾èªªå¯è½æ¯æè²´çï¼èåäº«ç¨ç¹æ¸ææé æå¤±å»ç«¶ç­åªå¢çé¢¨éªï¼å æ­¤ééå¬å¹³ççåµåèªå ä¾æ¿åµæææ¹çåèæ¯å¿è¦çãæ¬æè¨­è¨äºä¸åè©ä¼°æ¹æ¡ï¼ç¨æ¼è¡¡éæ¯åæ¹æ¸æè²¢ç»å°å±åå­¸ç¿ä»»åçå¹å¼ï¼éå°å ææ¨è«ççµ±è¨éæ±é²è¡èª¿æ´ï¼æ¹æ³æ¯æ¯è¼åèèè²¢ç»çè§å¯æ¸ææ¨è«åºçå·²å®æé¨åå°åç¡ç°å (CPDAG)ãå æ­¤ï¼åå¾çæ¸æä¼°å¼æ¹æ¡å¯é²ä¸æ­¥ç¨æ¼å¼å¥æ©å¶ï¼ä»¥æ¿åµä»£çäººè²¢ç»æ¸æãå®å¯è¢«ç¨æ¼æ ¹ææ¸æåè³ªå¬å¹³çåµä»£çäººï¼ææå¤§åææä»£çäººçæ¸æè²¢ç»ã

##### **Unsupervised Analysis of Alzheimer's Disease Signatures using 3D Deformable Autoencoders**
2407.03863v1 by Mehmet Yigit Avci, Emily Chan, Veronika Zimmer, Daniel Rueckert, Benedikt Wiestler, Julia A. Schnabel, Cosmin I. Bercea

With the increasing incidence of neurodegenerative diseases such as
Alzheimer's Disease (AD), there is a need for further research that enhances
detection and monitoring of the diseases. We present MORPHADE (Morphological
Autoencoders for Alzheimer's Disease Detection), a novel unsupervised learning
approach which uses deformations to allow the analysis of 3D T1-weighted brain
images. To the best of our knowledge, this is the first use of deformations
with deep unsupervised learning to not only detect, but also localize and
assess the severity of structural changes in the brain due to AD. We obtain
markedly higher anomaly scores in clinically important areas of the brain in
subjects with AD compared to healthy controls, showcasing that our method is
able to effectively locate AD-related atrophy. We additionally observe a visual
correlation between the severity of atrophy highlighted in our anomaly maps and
medial temporal lobe atrophy scores evaluated by a clinical expert. Finally,
our method achieves an AUROC of 0.80 in detecting AD, out-performing several
supervised and unsupervised baselines. We believe our framework shows promise
as a tool towards improved understanding, monitoring and detection of AD. To
support further research and application, we have made our code publicly
available at github.com/ci-ber/MORPHADE.

æè¦ï¼é¨èç¥ç¶éè¡æ§ç¾çï¼ä¾å¦é¿è²æµ·é»çï¼çç¼ççå¢å ï¼éè¦é²ä¸æ­¥çç ç©¶ä¾å å¼·å°éäºç¾ççåµæ¸¬åç£æ§ãæåæåº MORPHADEï¼é¿è²æµ·é»çåµæ¸¬çå½¢æèªåç·¨ç¢¼å¨ï¼ï¼éæ¯ä¸ç¨®æ°ç©çç¡ç£ç£å­¸ç¿æ¹æ³ï¼å®ä½¿ç¨è®å½¢ä¾åæ 3D T1 å æ¬è¦é¨å½±åãææåæç¥ï¼éæ¯é¦æ¬¡å°è®å½¢èæ·±åº¦ç¡ç£ç£å­¸ç¿çµåä½¿ç¨ï¼ä¸åå¯ä»¥åµæ¸¬ï¼éå¯ä»¥å®ä½åè©ä¼°é¿è²æµ·é»çå°è´çè¦é¨çµæ§è®åå´éç¨åº¦ãæåå¨é¿è²æµ·é»çåè©¦èçè¦é¨è¨åºä¸éè¦ååç²å¾é¡¯èæ´é«çç°å¸¸åæ¸ï¼èå¥åº·å°ç§çµç¸æ¯ï¼é¡¯ç¤ºæåçæ¨¡åè½å¤ ææå®ä½èé¿è²æµ·é»çç¸éçèç¸®ãæ­¤å¤ï¼æåè§å¯å°ç°å¸¸åä¸­çªåºçèç¸®å´éç¨åº¦èè¨åºå°å®¶è©ä¼°çå§å´é¡³èèç¸®åæ¸ä¹éå­å¨è¦è¦ºç¸éæ§ãæå¾ï¼æåçæ¨¡åå¨åµæ¸¬é¿è²æµ·é»çæ¹é¢éå°äº 0.80 ç AUROCï¼åªæ¼å¤åç£ç£å¼åç¡ç£ç£å¼åºæºãæåç¸ä¿¡æåçæ¶æ§é¡¯ç¤ºåºæææçºæ¹åé¿è²æµ·é»ççè§£ãç£æ§ååµæ¸¬çå·¥å·ãçºäºæ¯æé²ä¸æ­¥çç ç©¶åæç¨ï¼æåå·²å¨ github.com/ci-ber/MORPHADE å¬éæåçç¨å¼ç¢¼ã

##### **CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation**
2407.07913v1 by Rui Yang

This paper presents CaseGPT, an innovative approach that combines Large
Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to
enhance case-based reasoning in the healthcare and legal sectors. The system
addresses the challenges of traditional database queries by enabling fuzzy
searches based on imprecise descriptions, thereby improving data searchability
and usability. CaseGPT not only retrieves relevant case data but also generates
insightful suggestions and recommendations based on patterns discerned from
existing case data. This functionality proves especially valuable for tasks
such as medical diagnostics, legal precedent research, and case strategy
formulation. The paper includes an in-depth discussion of the system's
methodology, its performance in both medical and legal domains, and its
potential for future applications. Our experiments demonstrate that CaseGPT
significantly outperforms traditional keyword-based and simple LLM-based
systems in terms of precision, recall, and efficiency.

æè¦ï¼æ¬æä»ç´¹ CaseGPTï¼éæ¯ä¸ç¨®åµæ°çæ¹æ³ï¼çµåå¤§åèªè¨æ¨¡å (LLM) åæª¢ç´¢å¢å¼·çæ (RAG) æè¡ï¼ä»¥å¢å¼·é«çä¿å¥åæ³å¾é åçæ¡ä¾æ¨çãæ­¤ç³»çµ±ééåºæ¼ä¸ç²¾ç¢ºæè¿°åç¨æ¨¡ç³æå°ï¼ä¾è§£æ±ºå³çµ±è³æåº«æ¥è©¢çææ°ï¼é²èæ¹åè³æçå¯æå°æ§åå¯ç¨æ§ãCaseGPT ä¸åææ·åç¸éçæ¡ä¾è³æï¼éææ ¹æå¾ç¾ææ¡ä¾è³æä¸­è¾¨è­åºçæ¨¡å¼ï¼ç¢çæè¦å°çå»ºè­°åå»ºè­°ãæ­¤åè½å°æ¼é«çè¨ºæ·ãæ³å¾åä¾ç ç©¶åæ¡ä¾ç­ç¥å¶å®ç­ä»»åç¹å¥æå¹å¼ãæ¬æåå«å°ç³»çµ±æ¹æ³è«ãå¶å¨é«çåæ³å¾é åçæè½ï¼ä»¥åå¶æªä¾æç¨æ½åçæ·±å¥æ¢è¨ãæåçå¯¦é©è­æï¼CaseGPT å¨æºç¢ºåº¦ãå¬åçåæçæ¹é¢ï¼æé¡¯åªæ¼å³çµ±çåºæ¼ééµå­ååºæ¼ç°¡å® LLM çç³»çµ±ã

##### **Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content**
2407.03582v1 by Andrew Bouras

Generating diverse, high-quality outputs from language models is crucial for
applications in education and content creation. Achieving true randomness and
avoiding repetition remains a significant challenge. This study uses the Linear
Congruential Generator method for systematic fact selection, combined with
AI-powered content generation. We ensured unique combinations of
gastrointestinal physiology and pathology facts across multiple rounds,
integrating these facts into prompts for GPT-4o to create clinically relevant,
vignette-style outputs. Over 14 rounds, 98 unique outputs were generated,
demonstrating LCG's effectiveness in producing diverse and high-quality
content. This method addresses key issues of randomness and repetition,
enhancing the quality and efficiency of language model-generated content for
various applications.

æè¦ï¼å¨æè²åå§å®¹åµä½çæç¨ä¸­ï¼å¾èªè¨æ¨¡åç¢çå¤æ¨£åãé«åè³ªçè¼¸åºè³ééè¦ãå¯¦ç¾çæ­£çé¨æ©æ§åé¿åéè¤ä»ç¶æ¯ä¸é éå¤§çææ°ãæ¬ç ç©¶ä½¿ç¨ç·æ§åé¤ç¢çå¨æ¹æ³é²è¡ç³»çµ±æ§äºå¯¦é¸æï¼ä¸¦çµå AI é©åçå§å®¹çæãæåç¢ºä¿äºå¨å¤è¼ªä¸­èè¸ççåççäºå¯¦çç¨ç¹çµåï¼å°éäºäºå¯¦æ´åå° GPT-4o çæç¤ºä¸­ï¼ä»¥åµå»ºå·æè¨åºç¸éæ§çç­ç¯æäºé¢¨æ ¼è¼¸åºãå¨ 14 è¼ªä¸­ï¼çæäº 98 åç¨ç¹è¼¸åºï¼è­æäº LCG å¨ç¢çå¤æ¨£ååé«åè³ªå§å®¹æ¹é¢çæææ§ãæ­¤æ¹æ³è§£æ±ºäºé¨æ©æ§åéè¤æ§çééµåé¡ï¼æé«äºèªè¨æ¨¡åçæçå§å®¹å¨åç¨®æç¨ä¸­çåè³ªåæçã

##### **Accelerated Proton Resonance Frequency-based Magnetic Resonance Thermometry by Optimized Deep Learning Method**
2407.03308v1 by Sijie Xu, Shenyan Zong, Chang-Sheng Mei, Guofeng Shen, Yueran Zhao, He Wang

Proton resonance frequency (PRF) based MR thermometry is essential for
focused ultrasound (FUS) thermal ablation therapies. This work aims to enhance
temporal resolution in dynamic MR temperature map reconstruction using an
improved deep learning method. The training-optimized methods and five
classical neural networks were applied on the 2-fold and 4-fold under-sampling
k-space data to reconstruct the temperature maps. The enhanced training modules
included offline/online data augmentations, knowledge distillation, and the
amplitude-phase decoupling loss function. The heating experiments were
performed by a FUS transducer on phantom and ex vivo tissues, respectively.
These data were manually under-sampled to imitate acceleration procedures and
trained in our method to get the reconstruction model. The additional dozen or
so testing datasets were separately obtained for evaluating the real-time
performance and temperature accuracy. Acceleration factors of 1.9 and 3.7 were
found for 2 times and 4 times k-space under-sampling strategies and the
ResUNet-based deep learning reconstruction performed exceptionally well. In
2-fold acceleration scenario, the RMSE of temperature map patches provided the
values of 0.888 degree centigrade and 1.145 degree centigrade on phantom and ex
vivo testing datasets. The DICE value of temperature areas enclosed by 43
degree centigrade isotherm was 0.809, and the Bland-Altman analysis showed a
bias of -0.253 degree centigrade with the apart of plus or minus 2.16 degree
centigrade. In 4 times under-sampling case, these evaluating values decreased
by approximately 10%. This study demonstrates that deep learning-based
reconstruction can significantly enhance the accuracy and efficiency of MR
thermometry for clinical FUS thermal therapies.

æè¦ï¼åºæ¼è³ªå­å±æ¯é »ç (PRF) ç MR æº«åº¦æ¸¬éå°æ¼èç¦è¶é³æ³¢ (FUS) ç±æ¶èçæ³è³ééè¦ãéé ç ç©¶æ¨å¨ééæ¹åæ·±åº¦å­¸ç¿æ¹æ³ï¼æååæ MR æº«åº¦åéå»ºä¸­çæéè§£æåº¦ãå¨ 2 åå 4 åç k-space è³æä¸è¶³æ¡æ¨£ä¸­ï¼å°è¨ç·´æä½³åæ¹æ³åäºåå³çµ±ç¥ç¶ç¶²è·¯æç¨æ¼éå»ºæº«åº¦åãå¢å¼·çè¨ç·´æ¨¡çµåæ¬é¢ç·/ç·ä¸è³ææ´åãç¥è­èåï¼ä»¥åæ¯å¹ç¸ä½è§£è¦æå¤±å½æ¸ãå ç±å¯¦é©åå¥ç± FUS æè½å¨å¨æ¨¡æ¬äººé«åé¢é«çµç¹ä¸å·è¡ãéäºè³æç¶éæåä¸è¶³æ¡æ¨£ä»¥æ¨¡æ¬å éç¨åºï¼ä¸¦å¨æåçæ¨¡åä¸­é²è¡è¨ç·´ä»¥åå¾éå»ºæ¨¡åãé¡å¤çåå¹¾åæ¸¬è©¦è³æéåå¦å¤åå¾ï¼ç¨æ¼è©ä¼°å³ææè½åæº«åº¦æºç¢ºåº¦ãå¨ 2 åå 4 å k-space ä¸è¶³æ¡æ¨£ç­ç¥ä¸­ï¼ç¼ç¾å éå å­çº 1.9 å 3.7ï¼èåºæ¼ ResUNet çæ·±åº¦å­¸ç¿éå»ºè¡¨ç¾å¾éå¸¸å¥½ãå¨ 2 åå éæå¢ä¸­ï¼æº«åº¦ååå¡ç RMSE å¨æ¨¡æ¬äººé«åé¢é«æ¸¬è©¦è³æéä¸æä¾ 0.888 åº¦ææ°å 1.145 åº¦ææ°çå¼ãæº«åº¦ååç DICE å¼ï¼ä»¥ 43 åº¦ææ°ç­æº«ç·åè¦ï¼çº 0.809ï¼è Bland-Altman åæé¡¯ç¤ºåå·®çº -0.253 åº¦ææ°ï¼å ä¸ææ¸å» 2.16 åº¦ææ°ãå¨ 4 åä¸è¶³æ¡æ¨£æ¡ä¾ä¸­ï¼éäºè©ä¼°å¼æ¸å°äºå¤§ç´ 10%ãéé ç ç©¶è­æï¼åºæ¼æ·±åº¦å­¸ç¿çéå»ºå¯ä»¥å¤§å¹æåè¨åº FUS ç±çä¸­ MR æº«åº¦æ¸¬éçæºç¢ºåº¦åæçã

##### **MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition**
2407.03131v2 by Yanjie Cui, Xiaohong Liu, Jing Liang, Yamin Fu

Electroencephalography (EEG), a medical imaging technique that captures scalp
electrical activity of brain structures via electrodes, has been widely used in
affective computing. The spatial domain of EEG is rich in affective
information. However, few of the existing studies have simultaneously analyzed
EEG signals from multiple perspectives of geometric and anatomical structures
in spatial domain. In this paper, we propose a multi-view Graph Transformer
(MVGT) based on spatial relations, which integrates information from the
temporal, frequency and spatial domains, including geometric and anatomical
structures, so as to enhance the expressive power of the model comprehensively.
We incorporate the spatial information of EEG channels into the model as
encoding, thereby improving its ability to perceive the spatial structure of
the channels. Meanwhile, experimental results based on publicly available
datasets demonstrate that our proposed model outperforms state-of-the-art
methods in recent years. In addition, the results also show that the MVGT could
extract information from multiple domains and capture inter-channel
relationships in EEG emotion recognition tasks effectively.

æè¦ï¼è¦é»åï¼EEGï¼æ¯ä¸ç¨®é«å­¸å½±åæè¡ï¼ééé»æ¥µæ·åé ­ç®ä¸è¦é¨çµæ§çé»æ°£æ´»åï¼å·²å»£æ³ç¨æ¼ææéç®ä¸­ãEEG çç©ºéåèèè±å¯çææè³è¨ãç¶èï¼ç¾æçç ç©¶é®®å°åæå¾ç©ºéåä¸­çå¹¾ä½çµæ§åè§£åçµæ§ç­å¤éé¢ååæ EEG è¨èãæ¬ææåºä¸ååºæ¼ç©ºééä¿çå¤è¦ååå½¢è½æå¨ï¼MVGTï¼ï¼å®æ´åäºæéãé »çåç©ºéåï¼åæ¬å¹¾ä½çµæ§åè§£åçµæ§ï¼çè³è¨ï¼ä»¥å¨é¢æåæ¨¡åçè¡¨ç¾åãæåå° EEG ééçç©ºéè³è¨ç·¨ç¢¼å¾ç´å¥æ¨¡åä¸­ï¼é²èæåå¶æç¥ééç©ºéçµæ§çè½åãåæï¼åºæ¼å¬éè³æéçå¯¦é©çµæé¡¯ç¤ºï¼æåæåºçæ¨¡ååªæ¼è¿å¹´ä¾çç¾ææè¡ãæ­¤å¤ï¼çµæä¹é¡¯ç¤º MVGT è½ææå¾å¤éåä¸­æ·åè³è¨ï¼ä¸¦å¨ EEG æç·è¾¨è­ä»»åä¸­ææå°éééçéä¿ã

##### **Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation**
2407.03086v1 by Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko

While federated learning leverages distributed client resources, it faces
challenges due to heterogeneous client capabilities. This necessitates
allocating models suited to clients' resources and careful parameter
aggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel
federated learning framework for supporting client heterogeneity by combining a
multi-exit network architecture with hypernetwork-based model weight
generation. This approach aligns the feature spaces of heterogeneous model
layers and resolves per-layer information disparity during weight aggregation.
To practically realize HypeMeFed, we also propose a low-rank factorization
approach to minimize computation and memory overhead associated with
hypernetworks. Our evaluations on a real-world heterogeneous device testbed
indicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the
hypernetwork memory requirements by 98.22%, and accelerates its operations by
1.86 times compared to a naive hypernetwork approach. These results demonstrate
HypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for
federated learning.

æè¦ï¼éç¶è¯åå­¸ç¿å©ç¨åæ£å¼ç¨æ¶ç«¯è³æºï¼ä½ç±æ¼ç¨æ¶ç«¯è½åç°è³ªï¼å æ­¤é¢è¨ææ°ãééè¦åéé©åç¨æ¶ç«¯è³æºçæ¨¡åï¼ä¸¦ä»ç´°åæ¸èåä»¥å®¹ç´éç¨®ç°è³ªæ§ãæåæåº HypeMeFedï¼ä¸ç¨®æ°çè¯åå­¸ç¿æ¡æ¶ï¼ééå°å¤åºå£ç¶²è·¯æ¶æ§èåºæ¼è¶ç¶²è·¯çæ¨¡åæ¬éçæç¸çµåä¾æ¯æ´ç¨æ¶ç«¯ç°è³ªæ§ãæ­¤æ¹æ³å°é½ç°è³ªæ¨¡åå±¤çç¹å¾µç©ºéï¼ä¸¦å¨æ¬éèåæéè§£æ±ºéå±¤è³è¨å·®ç°ãçºäºå¯¦éå¯¦ç¾ HypeMeFedï¼æåéæåºäºä¸ç¨®ä½ç§©åè§£æ¹æ³ï¼ä»¥æå¤§éåº¦å°æ¸å°èè¶ç¶²è·¯ç¸éçè¨ç®åè¨æ¶é«éé·ãæåå¨çå¯¦ä¸çç°è³ªè¨­åæ¸¬è©¦å¹³å°ä¸çè©ä¼°è¡¨æï¼è FedAvg ç¸æ¯ï¼HypeMeFed å°æºç¢ºçæé«äº 5.12%ï¼å°è¶ç¶²è·¯è¨æ¶é«éæ±æ¸å°äº 98.22%ï¼ä¸¦ä¸èå¤©ççè¶ç¶²è·¯æ¹æ³ç¸æ¯ï¼å¶éç®éåº¦æé«äº 1.86 åãéäºçµæè­æäº HypeMeFed å¨å©ç¨åå¸å¼ç°è³ªç¨æ¶ç«¯é²è¡è¯åå­¸ç¿æ¹é¢çæææ§ã

##### **Attention Incorporated Network for Sharing Low-rank, Image and K-space Information during MR Image Reconstruction to Achieve Single Breath-hold Cardiac Cine Imaging**
2407.03034v1 by Siying Xu, Kerstin Hammernik, Andreas Lingg, Jens Kuebler, Patrick Krumm, Daniel Rueckert, Sergios Gatidis, Thomas Kuestner

Cardiac Cine Magnetic Resonance Imaging (MRI) provides an accurate assessment
of heart morphology and function in clinical practice. However, MRI requires
long acquisition times, with recent deep learning-based methods showing great
promise to accelerate imaging and enhance reconstruction quality. Existing
networks exhibit some common limitations that constrain further acceleration
possibilities, including single-domain learning, reliance on a single
regularization term, and equal feature contribution. To address these
limitations, we propose to embed information from multiple domains, including
low-rank, image, and k-space, in a novel deep learning network for MRI
reconstruction, which we denote as A-LIKNet. A-LIKNet adopts a parallel-branch
structure, enabling independent learning in the k-space and image domain.
Coupled information sharing layers realize the information exchange between
domains. Furthermore, we introduce attention mechanisms into the network to
assign greater weights to more critical coils or important temporal frames.
Training and testing were conducted on an in-house dataset, including 91
cardiovascular patients and 38 healthy subjects scanned with 2D cardiac Cine
using retrospective undersampling. Additionally, we evaluated A-LIKNet on the
real-time 8x prospectively undersampled data from the OCMR dataset. The results
demonstrate that our proposed A-LIKNet outperforms existing methods and
provides high-quality reconstructions. The network can effectively reconstruct
highly retrospectively undersampled dynamic MR images up to 24x accelerations,
indicating its potential for single breath-hold imaging.

æè¦ï¼<paragraph>å¿èåæç£å±æ¯å½±å (MRI) æä¾äºå¿èå½¢æååè½å¨è¨åºå¯¦åä¸çç²¾æºè©ä¼°ãç¶èï¼MRI éè¦è¼é·çæ·åæéï¼èæè¿åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³é¡¯ç¤ºåºæ¥µä½³çæ½åï¼ç¨æ¼å éå½±åä¸¦å¢å¼·éå»ºåè³ªãç¾æçç¶²è·¯å±ç¾äºä¸äºå¸¸è¦çéå¶ï¼éå¶äºé²ä¸æ­¥çå éå¯è½æ§ï¼åæ¬å®ä¸é åå­¸ç¿ãä¾è³´å®ä¸æ­£ååé ï¼ä»¥åç¸ç­çç¹å¾µè²¢ç»ãçºäºè§£æ±ºéäºéå¶ï¼æåæè­°å°ä¾èªå¤åé åçè³è¨åµå¥ä¸åç¨æ¼ MRI éå»ºçæ°ç©æ·±åº¦å­¸ç¿ç¶²è·¯ä¸­ï¼åæ¬ä½ç§©ãå½±åï¼ä»¥å k ç©ºéï¼æåå°å¶è¡¨ç¤ºçº A-LIKNetãA-LIKNet æ¡ç¨å¹³è¡åæ¯çµæ§ï¼å¨ k ç©ºéåå½±åé åä¸­å¯¦ç¾ç¨ç«å­¸ç¿ãè¦åè³è¨å±äº«å±¤å¯¦ç¾äºé åä¹éçè³è¨äº¤æãæ­¤å¤ï¼æåå°æ³¨æåæ©å¶å¼å¥ç¶²è·¯ä¸­ï¼ä»¥å°è¼å¤§çæ¬éåéçµ¦æ´éè¦çç·åæéè¦çæéå¹ãè¨ç·´åæ¸¬è©¦æ¯å¨å§é¨è³æéä¸é²è¡çï¼å¶ä¸­åæ¬ 91 ä½å¿è¡ç®¡ç¾çæ£èå 38 ä½å¥åº·åè©¦èï¼ä»åä½¿ç¨ 2D å¿èåæå½±åé²è¡ææï¼ä¸¦ä½¿ç¨åæº¯å¼æ¬ æ¡æ¨£ãæ­¤å¤ï¼æåå¨ OCMR è³æéä¸­çå³æ 8 ååç»æ§æ¬ æ¡æ¨£è³æä¸è©ä¼°äº A-LIKNetãçµæè­æï¼æåæåºç A-LIKNet åªæ¼ç¾ææ¹æ³ï¼ä¸¦æä¾äºé«åè³ªçéå»ºãè©²ç¶²è·¯å¯ä»¥ææéå»ºé«åæº¯æ§æ¬ æ¡æ¨£çåæ MR å½±åï¼å éçé«é 24 åï¼éè¡¨ç¤ºå¶å·æå®æ¬¡éæ°£å½±åçæ½åã</paragraph>

