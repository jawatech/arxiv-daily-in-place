{"2411.03590": {"publish_time": "2024-11-06", "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond", "paper_summary": "Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs.", "paper_summary_zh": "<paragraph>Medprompt \u7b49\u8fd0\u884c\u65f6\u5bfc\u5f15\u7b56\u7565\u5bf9\u4e8e\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u5f88\u6709\u4ef7\u503c\u3002Medprompt \u8bc1\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u63d0\u793a\u6765\u5f15\u53d1\u6d89\u53ca\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u96c6\u6210\u8fd0\u884c\u65f6\u7b56\u7565\uff0c\u5c06\u901a\u7528 LLM \u96c6\u4e2d\u8d77\u6765\uff0c\u4ee5\u5728\u533b\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u63d0\u4f9b\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002OpenAI \u7684 o1-preview \u6a21\u578b\u4ee3\u8868\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\uff0c\u5176\u4e2d\u6a21\u578b\u88ab\u8bbe\u8ba1\u4e3a\u5728\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u4e4b\u524d\u8fdb\u884c\u8fd0\u884c\u65f6\u63a8\u7406\u3002\u6211\u4eec\u5bfb\u6c42\u4e86\u89e3 o1-preview \u5728\u5404\u79cd\u533b\u5b66\u6311\u6218\u95ee\u9898\u57fa\u51c6\u4e0a\u7684\u884c\u4e3a\u3002\u5728\u4f7f\u7528 GPT-4 \u8fdb\u884c Medprompt \u7814\u7a76\u540e\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86 o1-preview \u6a21\u578b\u5728\u5404\u79cd\u533b\u5b66\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\u6280\u672f\uff0co1-preview \u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4e5f\u4f18\u4e8e\u5e26\u6709 Medprompt \u7684 GPT-4 \u7cfb\u5217\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u7ecf\u5178\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff08\u4ee5 Medprompt \u4e3a\u4ee3\u8868\uff09\u5728\u65b0\u8303\u4f8b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u529f\u6548\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5c11\u91cf\u63d0\u793a\u963b\u788d\u4e86 o1 \u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u4e0a\u4e0b\u6587\u5b66\u4e60\u53ef\u80fd\u4e0d\u518d\u662f\u63a8\u7406\u539f\u751f\u6a21\u578b\u7684\u6709\u6548\u5bfc\u5411\u65b9\u6cd5\u3002\u867d\u7136\u96c6\u6210\u4ecd\u7136\u53ef\u884c\uff0c\u4f46\u5b83\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u5e76\u4e14\u9700\u8981\u4ed4\u7ec6\u8fdb\u884c\u6210\u672c\u6027\u80fd\u4f18\u5316\u3002\u6211\u4eec\u5bf9\u8fd0\u884c\u65f6\u7b56\u7565\u7684\u6210\u672c\u548c\u51c6\u786e\u6027\u5206\u6790\u63ed\u793a\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5176\u4e2d GPT-4o \u4ee3\u8868\u4e86\u4e00\u4e2a\u66f4\u5b9e\u60e0\u7684\u9009\u62e9\uff0c\u800c o1-preview \u4ee5\u66f4\u9ad8\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u867d\u7136 o1-preview \u63d0\u4f9b\u4e86\u9876\u7ea7\u6027\u80fd\uff0c\u4f46\u91c7\u7528 Medprompt \u7b49\u5bfc\u5411\u7b56\u7565\u7684 GPT-4o \u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4ecd\u5177\u6709\u4ef7\u503c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6ce8\u610f\u5230 o1-preview \u6a21\u578b\u5728\u8bb8\u591a\u73b0\u6709\u7684\u533b\u5b66\u57fa\u51c6\u4e0a\u5df2\u63a5\u8fd1\u9971\u548c\uff0c\u8fd9\u5f3a\u8c03\u4e86\u5bf9\u65b0\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u7684\u9700\u6c42\u3002\u6211\u4eec\u4ee5\u5bf9\u4f7f\u7528 LLM \u8fdb\u884c\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u7684\u4e00\u822c\u65b9\u5411\u7684\u601d\u8003\u4f5c\u4e3a\u7ed3\u675f\u3002</paragraph>", "author": "Harsha Nori et.al.", "authors": "Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz", "id": "2411.03590v1", "paper_url": "http://arxiv.org/abs/2411.03590v1", "repo": "null"}}