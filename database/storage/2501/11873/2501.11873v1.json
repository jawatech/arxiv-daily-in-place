{"2501.11873": {"publish_time": "2025-01-21", "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models", "paper_summary": "This paper revisits the implementation of\n$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E\n\\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$\nrepresents the frequency of expert $i$ being selected, and $p_i$ denotes the\naverage gating score of the expert $i$. Existing MoE training frameworks\nusually employ the parallel training strategy so that $f_i$ and the LBL are\ncalculated within a $\\textbf{micro-batch}$ and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a $\\textbf{global-batch}$ to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize $f_i$ across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n$\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts.", "paper_summary_zh": "<paragraph>\u672c\u6587\u91cd\u65b0\u63a2\u8a0e\u4e86\u5728\u8a13\u7df4 Mixture-of-Experts (MoEs) \u6a21\u578b\u6642\uff0c$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) \u7684\u5be6\u4f5c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cMoEs \u7684 LBL \u88ab\u5b9a\u7fa9\u70ba $N_E \\sum_{i=1}^{N_E} f_i p_i$\uff0c\u5176\u4e2d $N_E$ \u662f\u5c08\u5bb6\u7684\u7e3d\u6578\uff0c$f_i$ \u8868\u793a\u9078\u64c7\u5c08\u5bb6 $i$ \u7684\u983b\u7387\uff0c\u800c $p_i$ \u8868\u793a\u5c08\u5bb6 $i$ \u7684\u5e73\u5747\u9598\u63a7\u5206\u6578\u3002\u73fe\u6709\u7684 MoE \u8a13\u7df4\u67b6\u69cb\u901a\u5e38\u63a1\u7528\u4e26\u884c\u8a13\u7df4\u7b56\u7565\uff0c\u4ee5\u4fbf\u5728 $\\textbf{\u5fae\u6279\u6b21}$ \u4e2d\u8a08\u7b97 $f_i$ \u548c LBL\uff0c\u7136\u5f8c\u5728\u4e26\u884c\u7fa4\u7d44\u4e2d\u53d6\u5e73\u5747\u503c\u3002\u5be6\u8cea\u4e0a\uff0c\u8a13\u7df4\u5341\u5104\u898f\u6a21 LLM \u7684\u5fae\u6279\u6b21\u901a\u5e38\u53ea\u5305\u542b\u5f88\u5c11\u7684\u5e8f\u5217\u3002\u56e0\u6b64\uff0c\u5fae\u6279\u6b21 LBL \u5e7e\u4e4e\u8655\u65bc\u5e8f\u5217\u5c64\u7d1a\uff0c\u800c\u8def\u7531\u5668\u88ab\u8feb\u5728\u6bcf\u500b\u5e8f\u5217\u4e2d\u5747\u52fb\u5206\u914d\u4ee4\u724c\u3002\u5728\u9019\u500b\u56b4\u683c\u7684\u7d04\u675f\u4e0b\uff0c\u5373\u4f7f\u4f86\u81ea\u7279\u5b9a\u9818\u57df\u5e8f\u5217\uff08$\\textit{e.g.}$\uff0c\u7a0b\u5f0f\u78bc\uff09\u7684\u4ee4\u724c\u4e5f\u6703\u5747\u52fb\u8def\u7531\u5230\u6240\u6709\u5c08\u5bb6\uff0c\u5f9e\u800c\u6291\u5236\u5c08\u5bb6\u5c08\u696d\u5316\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528 $\\textbf{\u5168\u57df\u6279\u6b21}$ \u8a08\u7b97 LBL \u4ee5\u89e3\u9664\u9019\u500b\u7d04\u675f\u3002\u7531\u65bc\u5168\u57df\u6279\u6b21\u5305\u542b\u6bd4\u5fae\u6279\u6b21\u66f4\u591a\u6a23\u5316\u7684\u5e8f\u5217\uff0c\u9019\u5c07\u4fc3\u9032\u8a9e\u6599\u5c64\u7d1a\u7684\u8ca0\u8f09\u5e73\u8861\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u984d\u5916\u7684\u901a\u8a0a\u6b65\u9a5f\uff0c\u4ee5\u5728\u5fae\u6279\u6b21\u4e4b\u9593\u540c\u6b65 $f_i$\uff0c\u7136\u5f8c\u4f7f\u7528\u5b83\u4f86\u8a08\u7b97 LBL\u3002\u900f\u904e\u5c0d\u57fa\u65bc MoE \u7684 LLM\uff08\u7e3d\u53c3\u6578\u91cf\u9ad8\u9054 $\\textbf{42.8B}$\uff0c\u4ee4\u724c\u9054 $\\textbf{400B}$\uff09\u7684\u8a13\u7df4\u9032\u884c\u5be6\u9a57\uff0c\u6211\u5011\u9a5a\u8a1d\u5730\u767c\u73fe\uff0c\u5168\u57df\u6279\u6b21 LBL \u7b56\u7565\u5728\u9810\u8a13\u7df4\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u90fd\u7522\u751f\u4e86\u6975\u4f73\u7684\u6548\u80fd\u63d0\u5347\u3002\u6211\u5011\u7684\u5206\u6790\u8868\u660e\uff0c\u5168\u57df\u6279\u6b21 LBL \u4e5f\u5927\u5e45\u63d0\u5347\u4e86 MoE \u5c08\u5bb6\u7684\u9818\u57df\u5c08\u696d\u5316\u3002</paragraph>", "author": "Zihan Qiu et.al.", "authors": "Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, Junyang Lin", "id": "2501.11873v1", "paper_url": "http://arxiv.org/abs/2501.11873v1", "repo": "null"}}