{"2410.08115": {"publish_time": "2024-10-10", "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System", "paper_summary": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba\u57fa\u790e\u7684\u591a\u91cd\u4ee3\u7406\u7cfb\u7d71 (MAS) \u5728\u5354\u4f5c\u554f\u984c\u89e3\u6c7a\u65b9\u9762\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u6f5b\u529b\uff0c\u4f46\u4ecd\u9762\u81e8\u95dc\u9375\u6311\u6230\uff1a\u4f4e\u6e9d\u901a\u6548\u7387\u3001\u53ef\u64f4\u5145\u6027\u5dee\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u7684\u53c3\u6578\u66f4\u65b0\u6700\u4f73\u5316\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa Optima\uff0c\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u900f\u904e LLM \u8a13\u7df4\u5927\u5e45\u63d0\u5347 LLM \u70ba\u57fa\u790e\u7684 MAS \u4e2d\u7684\u6e9d\u901a\u6548\u7387\u548c\u4efb\u52d9\u6548\u80fd\uff0c\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002Optima \u63a1\u7528\u53cd\u8986\u751f\u6210\u3001\u6392\u540d\u3001\u9078\u64c7\u548c\u8a13\u7df4\u7684\u7bc4\u4f8b\uff0c\u5176\u734e\u52f5\u51fd\u6578\u5e73\u8861\u4e86\u4efb\u52d9\u8868\u73fe\u3001\u7b26\u865f\u6548\u7387\u548c\u6e9d\u901a\u53ef\u8b80\u6027\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u5404\u7a2e RL \u6f14\u7b97\u6cd5\uff0c\u5305\u62ec\u76e3\u7763\u5fae\u8abf\u3001\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff0c\u4ee5\u53ca\u5176\u6df7\u5408\u65b9\u6cd5\uff0c\u9032\u4e00\u6b65\u4e86\u89e3\u5176\u6548\u80fd\u6548\u7387\u7684\u6b0a\u8861\u53d6\u6368\u3002\u6211\u5011\u6574\u5408\u4e86\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b\u555f\u767c\u7684\u6280\u8853\uff0c\u7528\u65bc DPO \u8cc7\u6599\u7522\u751f\uff0c\u5c07\u5c0d\u8a71\u56de\u5408\u8996\u70ba\u6a39\u72c0\u7bc0\u9ede\uff0c\u4ee5\u63a2\u7d22\u591a\u6a23\u5316\u7684\u4e92\u52d5\u8def\u5f91\u3002\u5728\u5e38\u898b\u7684\u591a\u91cd\u4ee3\u7406\u4efb\u52d9\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec\u8cc7\u8a0a\u4e0d\u5c0d\u7a31\u554f\u7b54\u548c\u8907\u96dc\u63a8\u7406\uff0cOptima \u5728\u55ae\u4e00\u4ee3\u7406\u57fa\u6e96\u548c\u57fa\u65bc Llama 3 8B \u7684\u9999\u8349 MAS \u4e0a\u5c55\u73fe\u4e86\u4e00\u81f4\u4e14\u986f\u8457\u7684\u9032\u6b65\uff0c\u5728\u9700\u8981\u5927\u91cf\u8cc7\u8a0a\u4ea4\u63db\u7684\u4efb\u52d9\u4e2d\uff0c\u4ee5\u4e0d\u5230 10% \u7684\u7b26\u865f\u7372\u5f97\u4e86\u9ad8\u9054 2.8 \u500d\u7684\u6548\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0cOptima \u7684\u6548\u7387\u63d0\u5347\u70ba\u66f4\u6709\u6548\u5730\u904b\u7528\u63a8\u7406\u904b\u7b97\u958b\u555f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u9032\u800c\u6539\u5584\u63a8\u7406\u6642\u9593\u7684\u7e2e\u653e\u5b9a\u5f8b\u3002\u900f\u904e\u89e3\u6c7a LLM \u70ba\u57fa\u790e\u7684 MAS \u4e2d\u7684\u57fa\u672c\u6311\u6230\uff0cOptima \u5c55\u73fe\u4e86\u9081\u5411\u53ef\u64f4\u5145\u3001\u9ad8\u6548\u548c\u6709\u6548\u7684 MAS \u7684\u6f5b\u529b\n(https://chenweize1998.github.io/optima-project-page)\u3002</paragraph>", "author": "Weize Chen et.al.", "authors": "Weize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, Maosong Sun", "id": "2410.08115v1", "paper_url": "http://arxiv.org/abs/2410.08115v1", "repo": "null"}}