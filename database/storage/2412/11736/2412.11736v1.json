{"2412.11736": {"publish_time": "2024-12-16", "title": "Personalized LLM for Generating Customized Responses to the Same Query from Different Users", "paper_summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation.", "paper_summary_zh": "\u73fe\u6709\u91dd\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u500b\u4eba\u5316\u7684\u7814\u7a76\uff0c\u70ba LLM \u5206\u914d\u4e86\u4e0d\u540c\u7684\u56de\u61c9\u89d2\u8272\uff0c\u4f46\u5ffd\u7565\u4e86\u63d0\u554f\u8005\u7684\u591a\u6a23\u6027\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u554f\u984c\u611f\u77e5 LLM \u500b\u4eba\u5316\u5f62\u5f0f\uff0c\u5373\u4f7f\u5c0d\u65bc\u4f86\u81ea\u4e0d\u540c\u63d0\u554f\u8005\u7684\u76f8\u540c\u67e5\u8a62\uff0c\u4e5f\u80fd\u7522\u751f\u4e0d\u540c\u7684\u56de\u61c9\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u96d9\u5854\u6a21\u578b\u67b6\u69cb\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b\u8de8\u63d0\u554f\u8005\u901a\u7528\u7de8\u78bc\u5668\u548c\u4e00\u500b\u7279\u5b9a\u65bc\u63d0\u554f\u8005\u7684\u7de8\u78bc\u5668\u3002\u6211\u5011\u9032\u4e00\u6b65\u61c9\u7528\u5c0d\u6bd4\u5b78\u7fd2\u8207\u591a\u8996\u89d2\u64f4\u5145\uff0c\u62c9\u8fd1\u76f8\u540c\u63d0\u554f\u8005\u7684\u5c0d\u8a71\u8868\u5fb5\uff0c\u540c\u6642\u62c9\u958b\u4e0d\u540c\u63d0\u554f\u8005\u7684\u5c0d\u8a71\u8868\u5fb5\u3002\u70ba\u4e86\u6e1b\u8f15\u554f\u984c\u591a\u6a23\u6027\u5c0d\u63d0\u554f\u8005\u5c0d\u6bd4\u5b78\u7fd2\u7684\u5f71\u97ff\uff0c\u6211\u5011\u6839\u64da\u554f\u984c\u76f8\u4f3c\u6027\u5c0d\u8a71\u8a9e\u9032\u884c\u5206\u7fa4\uff0c\u4e26\u5c07\u5c0d\u6bd4\u5b78\u7fd2\u7684\u7bc4\u570d\u9650\u5236\u5728\u6bcf\u500b\u7fa4\u96c6\u5167\u3002\u6211\u5011\u9084\u5f9e\u82f1\u6587\u548c\u4e2d\u6587\u8173\u672c\u4ee5\u53ca\u5fae\u4fe1\u8a18\u9304\u4e2d\u5efa\u7acb\u4e86\u4e00\u500b\u591a\u63d0\u554f\u8005\u8cc7\u6599\u96c6\uff0c\u7a31\u70ba MQDialog\uff0c\u5176\u4e2d\u5305\u542b 173 \u500b\u63d0\u554f\u8005\u548c 12 \u500b\u56de\u61c9\u8005\u3002\u4f7f\u7528\u4e0d\u540c\u7684\u6307\u6a19\u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\uff0c\u986f\u793a\u500b\u4eba\u5316\u56de\u61c9\u7522\u751f\u7684\u54c1\u8cea\u6709\u986f\u8457\u63d0\u5347\u3002", "author": "Hang Zeng et.al.", "authors": "Hang Zeng, Chaoyue Niu, Fan Wu, Chengfei Lv, Guihai Chen", "id": "2412.11736v1", "paper_url": "http://arxiv.org/abs/2412.11736v1", "repo": "https://github.com/nidryen-zh/questionerawareresponder"}}