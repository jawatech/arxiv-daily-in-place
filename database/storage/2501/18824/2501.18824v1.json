{"2501.18824": {"publish_time": "2025-01-31", "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "paper_summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.", "paper_summary_zh": "\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u9488\u5bf9\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e13\u95e8\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u7136\u800c\uff0c\u5fae\u8c03\u901a\u5e38\u4f1a\u4ea7\u751f\u8f83\u9ad8\u7684\u5185\u5b58\u5f00\u9500\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u57fa\u4e8e\u5927\u578b\u8f6c\u6362\u5668\u7684\u6a21\u578b\uff0c\u4f8b\u5982 LLM\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u5fae\u8c03\u6240\u9700\u7684\u67d0\u4e9b\u5185\u5b58\u90e8\u5206\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u9700\u8981\u7f13\u5b58\u6b63\u5411\u4f20\u9012\u4e2d\u8ba1\u7b97\u7684\u6240\u6709\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u4ee5\u4fbf\u5728\u53cd\u5411\u4f20\u9012\u671f\u95f4\u66f4\u65b0\u6743\u91cd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 TokenTune\uff0c\u8fd9\u662f\u4e00\u79cd\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\u7684\u5185\u5b58\uff0c\u7528\u4e8e\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u6a21\u578b\u7684\u5fae\u8c03\u3002\u5728\u53cd\u5411\u4f20\u9012\u671f\u95f4\uff0cTokenTune \u901a\u8fc7\u4ec5\u53cd\u5411\u4f20\u64ad\u8f93\u5165\u4ee4\u724c\u7684\u5b50\u96c6\u6765\u903c\u8fd1\u68af\u5ea6\u8ba1\u7b97\u3002\u56e0\u6b64\uff0c\u4f7f\u7528 TokenTune\uff0c\u4ec5\u5728\u6b63\u5411\u4f20\u9012\u671f\u95f4\u7f13\u5b58\u4e2d\u95f4\u6fc0\u6d3b\u7684\u5b50\u96c6\u3002\u6b64\u5916\uff0cTokenTune \u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u5982 LoRA\uff09\u76f8\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u6210\u672c\u3002\u6211\u4eec\u5bf9\u62e5\u6709\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u8f6c\u6362\u5668\u6a21\u578b\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u5728\u5c11\u6570\u955c\u5934\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5bf9\u6587\u672c\u5206\u7c7b\u548c\u95ee\u9898\u89e3\u7b54\u7b49\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u603b\u4f53\u800c\u8a00\uff0cTokenTune \u5728\u4e0e\u5b8c\u5168\u5fae\u8c03\u6216\u5177\u6709\u4ee3\u8868\u6027\u7684\u8282\u80fd\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u6210\u5c31\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u5176\u4ed6\u5177\u6709\u4e92\u8865\u5185\u5b58\u51cf\u5c11\u673a\u5236\u7684\u65b9\u6cd5\u76f8\u7ed3\u5408\u65f6\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6709\u52a9\u4e8e\u5bf9\u5927\u578b\u8f6c\u6362\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5b83\u4eec\u4e13\u95e8\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u4e0e\u6765\u81ea\u66f4\u5927\u7cfb\u7edf\u4e2d\u7684\u5176\u4ed6\u795e\u7ecf\u7ec4\u4ef6\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/facebookresearch/tokentune \u4e2d\u83b7\u5f97\u3002", "author": "Antoine Simoulin et.al.", "authors": "Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang", "id": "2501.18824v1", "paper_url": "http://arxiv.org/abs/2501.18824v1", "repo": "null"}}