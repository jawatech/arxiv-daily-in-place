{"2412.10848": {"publish_time": "2024-12-14", "title": "Large Language Models for Medical Forecasting -- Foresight 2", "paper_summary": "Foresight 2 (FS2) is a large language model fine-tuned on hospital data for\nmodelling patient timelines (GitHub 'removed for anon'). It can understand\npatients' clinical notes and predict SNOMED codes for a wide range of\nbiomedical use cases, including diagnosis suggestions, risk forecasting, and\nprocedure and medication recommendations. FS2 is trained on the free text\nportion of the MIMIC-III dataset, firstly through extracting biomedical\nconcepts and then creating contextualised patient timelines, upon which the\nmodel is then fine-tuned. The results show significant improvement over the\nprevious state-of-the-art for the next new biomedical concept prediction (P/R -\n0.73/0.66 vs 0.52/0.32) and a similar improvement specifically for the next new\ndisorder prediction (P/R - 0.69/0.62 vs 0.46/0.25). Finally, on the task of\nrisk forecast, we compare our model to GPT-4-turbo (and a range of open-source\nbiomedical LLMs) and show that FS2 performs significantly better on such tasks\n(P@5 - 0.90 vs 0.65). This highlights the need to incorporate hospital data\ninto LLMs and shows that small models outperform much larger ones when\nfine-tuned on high-quality, specialised data.", "paper_summary_zh": "Foresight 2 (FS2) \u662f\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u91dd\u5c0d\u91ab\u9662\u6578\u64da\u9032\u884c\u5fae\u8abf\uff0c\u7528\u65bc\u5efa\u6a21\u60a3\u8005\u6642\u9593\u8ef8\uff08GitHub\u300c\u5df2\u79fb\u9664\u533f\u540d\u300d\uff09\u3002\u5b83\u53ef\u4ee5\u4e86\u89e3\u60a3\u8005\u7684\u81e8\u5e8a\u8a18\u9304\uff0c\u4e26\u9810\u6e2c\u5404\u7a2e\u751f\u7269\u91ab\u5b78\u7528\u4f8b\u7684 SNOMED \u4ee3\u78bc\uff0c\u5305\u62ec\u8a3a\u65b7\u5efa\u8b70\u3001\u98a8\u96aa\u9810\u6e2c\u4ee5\u53ca\u7a0b\u5e8f\u548c\u85e5\u7269\u5efa\u8b70\u3002FS2 \u5728 MIMIC-III \u6578\u64da\u96c6\u7684\u81ea\u7531\u6587\u672c\u90e8\u5206\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u9996\u5148\u901a\u904e\u63d0\u53d6\u751f\u7269\u91ab\u5b78\u6982\u5ff5\uff0c\u7136\u5f8c\u5275\u5efa\u60c5\u5883\u5316\u7684\u60a3\u8005\u6642\u9593\u8ef8\uff0c\u7136\u5f8c\u5c0d\u6a21\u578b\u9032\u884c\u5fae\u8abf\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u5148\u524d\u6700\u5148\u9032\u7684\u4e0b\u4e00\u65b0\u751f\u7269\u91ab\u5b78\u6982\u5ff5\u9810\u6e2c\uff08P/R - 0.73/0.66 \u5c0d 0.52/0.32\uff09\u76f8\u6bd4\uff0c\u6709\u986f\u8457\u6539\u9032\uff0c\u4e26\u4e14\u7279\u5225\u662f\u5c0d\u65bc\u4e0b\u4e00\u65b0\u75be\u75c5\u9810\u6e2c\uff08P/R - 0.69/0.62 \u5c0d 0.46/0.25\uff09\u4e5f\u6709\u985e\u4f3c\u7684\u6539\u9032\u3002\u6700\u5f8c\uff0c\u5728\u98a8\u96aa\u9810\u6e2c\u4efb\u52d9\u4e0a\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u6a21\u578b\u8207 GPT-4-turbo\uff08\u548c\u4e00\u7cfb\u5217\u958b\u6e90\u751f\u7269\u91ab\u5b78 LLM\uff09\u9032\u884c\u6bd4\u8f03\uff0c\u4e26\u8868\u660e FS2 \u5728\u6b64\u985e\u4efb\u52d9\u4e0a\u8868\u73fe\u986f\u8457\u512a\u65bc\uff08P@5 - 0.90 \u5c0d 0.65\uff09\u3002\u9019\u7a81\u986f\u4e86\u5c07\u91ab\u9662\u6578\u64da\u7d0d\u5165 LLM \u7684\u5fc5\u8981\u6027\uff0c\u4e26\u8868\u660e\u5728\u91dd\u5c0d\u9ad8\u54c1\u8cea\u5c08\u9580\u6578\u64da\u9032\u884c\u5fae\u8abf\u6642\uff0c\u5c0f\u578b\u6a21\u578b\u7684\u8868\u73fe\u512a\u65bc\u5927\u578b\u6a21\u578b\u3002", "author": "Zeljko Kraljevic et.al.", "authors": "Zeljko Kraljevic, Joshua Au Yeung, Daniel Bean, James Teo, Richard J. Dobson", "id": "2412.10848v1", "paper_url": "http://arxiv.org/abs/2412.10848v1", "repo": "null"}}