{"2406.07520": {"publish_time": "2024-06-11", "title": "Neural Gaffer: Relighting Any Object via Diffusion", "paper_summary": "Single-image relighting is a challenging task that involves reasoning about\nthe complex interplay between geometry, materials, and lighting. Many prior\nmethods either support only specific categories of images, such as portraits,\nor require special capture conditions, like using a flashlight. Alternatively,\nsome methods explicitly decompose a scene into intrinsic components, such as\nnormals and BRDFs, which can be inaccurate or under-expressive. In this work,\nwe propose a novel end-to-end 2D relighting diffusion model, called Neural\nGaffer, that takes a single image of any object and can synthesize an accurate,\nhigh-quality relit image under any novel environmental lighting condition,\nsimply by conditioning an image generator on a target environment map, without\nan explicit scene decomposition. Our method builds on a pre-trained diffusion\nmodel, and fine-tunes it on a synthetic relighting dataset, revealing and\nharnessing the inherent understanding of lighting present in the diffusion\nmodel. We evaluate our model on both synthetic and in-the-wild Internet imagery\nand demonstrate its advantages in terms of generalization and accuracy.\nMoreover, by combining with other generative methods, our model enables many\ndownstream 2D tasks, such as text-based relighting and object insertion. Our\nmodel can also operate as a strong relighting prior for 3D tasks, such as\nrelighting a radiance field.", "paper_summary_zh": "\u55ae\u5f35\u5f71\u50cf\u91cd\u65b0\u6253\u5149\u662f\u4e00\u9805\u8271\u96e3\u7684\u4efb\u52d9\uff0c\u6d89\u53ca\u63a8\u7406\u5e7e\u4f55\u3001\u6750\u6599\u548c\u5149\u7dda\u4e4b\u9593\u7684\u8907\u96dc\u4ea4\u4e92\u4f5c\u7528\u3002\u8a31\u591a\u5148\u524d\u7684\u6280\u8853\u50c5\u652f\u63f4\u7279\u5b9a\u985e\u5225\u7684\u5f71\u50cf\uff0c\u4f8b\u5982\u4eba\u50cf\uff0c\u6216\u9700\u8981\u7279\u6b8a\u62cd\u651d\u689d\u4ef6\uff0c\u4f8b\u5982\u4f7f\u7528\u624b\u96fb\u7b52\u3002\u6216\u8005\uff0c\u67d0\u4e9b\u6280\u8853\u6703\u660e\u78ba\u5c07\u5834\u666f\u5206\u89e3\u70ba\u5167\u5728\u7d44\u6210\u90e8\u5206\uff0c\u4f8b\u5982\u6cd5\u7dda\u548c BRDF\uff0c\u9019\u53ef\u80fd\u6703\u4e0d\u6e96\u78ba\u6216\u8868\u9054\u4e0d\u8db3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u7aef\u5230\u7aef 2D \u91cd\u65b0\u6253\u5149\u64f4\u6563\u6a21\u578b\uff0c\u7a31\u70ba Neural Gaffer\uff0c\u5b83\u6703\u64f7\u53d6\u4efb\u4f55\u7269\u9ad4\u7684\u55ae\u5f35\u5f71\u50cf\uff0c\u4e26\u53ef\u4ee5\u5728\u4efb\u4f55\u65b0\u7a4e\u7684\u74b0\u5883\u5149\u7167\u689d\u4ef6\u4e0b\u5408\u6210\u6e96\u78ba\u3001\u9ad8\u54c1\u8cea\u7684\u91cd\u65b0\u6253\u5149\u5f71\u50cf\uff0c\u53ea\u9700\u5728\u5f71\u50cf\u7522\u751f\u5668\u4e0a\u52a0\u4e0a\u76ee\u6a19\u74b0\u5883\u8cbc\u5716\uff0c\u800c\u7121\u9700\u660e\u78ba\u7684\u5834\u666f\u5206\u89e3\u3002\u6211\u5011\u7684\u6280\u8853\u5efa\u7acb\u5728\u9810\u5148\u8a13\u7df4\u597d\u7684\u64f4\u6563\u6a21\u578b\u4e0a\uff0c\u4e26\u5728\u5408\u6210\u91cd\u65b0\u6253\u5149\u8cc7\u6599\u96c6\u4e0a\u5fae\u8abf\uff0c\u63ed\u793a\u4e26\u5229\u7528\u64f4\u6563\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u5167\u5728\u5149\u7167\u7406\u89e3\u3002\u6211\u5011\u5728\u5408\u6210\u548c\u91ce\u751f\u7db2\u8def\u5f71\u50cf\u4e0a\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0c\u4e26\u5c55\u793a\u5176\u5728\u6cdb\u5316\u548c\u6e96\u78ba\u6027\u65b9\u9762\u7684\u512a\u9ede\u3002\u6b64\u5916\uff0c\u900f\u904e\u8207\u5176\u4ed6\u751f\u6210\u5f0f\u6280\u8853\u7d50\u5408\uff0c\u6211\u5011\u7684\u6a21\u578b\u652f\u63f4\u8a31\u591a\u4e0b\u6e38 2D \u4efb\u52d9\uff0c\u4f8b\u5982\u57fa\u65bc\u6587\u5b57\u7684\u91cd\u65b0\u6253\u5149\u548c\u7269\u4ef6\u63d2\u5165\u3002\u6211\u5011\u7684\u6a21\u578b\u9084\u53ef\u4ee5\u4f5c\u70ba 3D \u4efb\u52d9\u7684\u5f37\u5927\u91cd\u65b0\u6253\u5149\u5148\u9a57\uff0c\u4f8b\u5982\u91cd\u65b0\u6253\u5149\u8f3b\u7167\u5834\u3002", "author": "Haian Jin et.al.", "authors": "Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, Noah Snavely", "id": "2406.07520v1", "paper_url": "http://arxiv.org/abs/2406.07520v1", "repo": "null"}}