{"2411.03542": {"publish_time": "2024-11-05", "title": "Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry", "paper_summary": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and\nmore) are driving forward novel development of multipurpose AI for a variety of\ntasks, particularly natural language processing (NLP) tasks. These models\ndemonstrate strong performance on a range of tasks; however, there has been\nevidence of brittleness when applied to more niche or narrow domains where\nhallucinations or fluent but incorrect responses reduce performance. Given the\ncomplex nature of scientific domains, it is prudent to investigate the\ntrade-offs of leveraging off-the-shelf versus more targeted foundation models\nfor scientific domains. In this work, we examine the benefits of in-domain\npre-training for a given scientific domain, chemistry, and compare these to\nopen-source, off-the-shelf models with zero-shot and few-shot prompting. Our\nresults show that not only do in-domain base models perform reasonably well on\nin-domain tasks in a zero-shot setting but that further adaptation using\ninstruction fine-tuning yields impressive performance on chemistry-specific\ntasks such as named entity recognition and molecular formula generation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08GPT \u7cfb\u5217\u3001BLOOM\u3001LLaMA \u7b49\uff09\u7684\u6fc0\u589e\u63a8\u52d5\u4e86\u591a\u529f\u80fd AI \u7684\u65b0\u7a4e\u767c\u5c55\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u4efb\u52d9\uff0c\u7279\u5225\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u3002\u9019\u4e9b\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u8868\u73fe\u51fa\u8272\uff1b\u7136\u800c\uff0c\u6709\u8b49\u64da\u8868\u660e\uff0c\u7576\u61c9\u7528\u65bc\u66f4\u5229\u57fa\u6216\u72f9\u7a84\u7684\u9818\u57df\u6642\uff0c\u5b83\u5011\u6703\u51fa\u73fe\u8106\u5f31\u6027\uff0c\u5728\u9019\u4e9b\u9818\u57df\u4e2d\uff0c\u5e7b\u89ba\u6216\u6d41\u66a2\u4f46\u932f\u8aa4\u7684\u53cd\u61c9\u6703\u964d\u4f4e\u6027\u80fd\u3002\u9451\u65bc\u79d1\u5b78\u9818\u57df\u7684\u8907\u96dc\u6027\uff0c\u5be9\u614e\u8abf\u67e5\u5229\u7528\u73fe\u6210\u7684\u57fa\u790e\u6a21\u578b\u8207\u91dd\u5c0d\u79d1\u5b78\u9818\u57df\u7684\u66f4\u5177\u91dd\u5c0d\u6027\u7684\u57fa\u790e\u6a21\u578b\u4e4b\u9593\u7684\u6b0a\u8861\u53d6\u6368\u662f\u660e\u667a\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5728\u7279\u5b9a\u79d1\u5b78\u9818\u57df\uff08\u5316\u5b78\uff09\u4e2d\u9032\u884c\u9818\u57df\u5167\u9810\u8a13\u7df4\u7684\u597d\u8655\uff0c\u4e26\u5c07\u9019\u4e9b\u597d\u8655\u8207\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u6b21\u5b78\u7fd2\u63d0\u793a\u7684\u958b\u6e90\u73fe\u6210\u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u9818\u57df\u5167\u57fa\u790e\u6a21\u578b\u4e0d\u50c5\u5728\u96f6\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\u5728\u9818\u57df\u5167\u4efb\u52d9\u4e0a\u8868\u73fe\u5f97\u76f8\u7576\u597d\uff0c\u800c\u4e14\u4f7f\u7528\u6307\u4ee4\u5fae\u8abf\u9032\u4e00\u6b65\u9069\u61c9\u5728\u5316\u5b78\u7279\u5b9a\u4efb\u52d9\uff08\u4f8b\u5982\u547d\u540d\u5be6\u9ad4\u8b58\u5225\u548c\u5206\u5b50\u5f0f\u751f\u6210\uff09\u4e0a\u7522\u751f\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002", "author": "Anurag Acharya et.al.", "authors": "Anurag Acharya, Shivam Sharma, Robin Cosbey, Megha Subramanian, Scott Howland, Maria Glenski", "id": "2411.03542v1", "paper_url": "http://arxiv.org/abs/2411.03542v1", "repo": "null"}}