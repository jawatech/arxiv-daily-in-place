{"2406.19954": {"publish_time": "2024-06-28", "title": "BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5", "paper_summary": "Incorporating speech understanding capabilities into pretrained\nlarge-language models has become a vital research direction (SpeechLLM). The\nprevious architectures can be categorized as: i) GPT-style, prepend speech\nprompts to the text prompts as a sequence of LLM inputs like a decoder-only\nmodel; ii) T5-style, introduce speech cross-attention to each layer of the\npretrained LLMs. We propose BESTOW architecture to bring the BESt features from\nTwO Worlds into a single model that is highly efficient and has strong\nmultitask capabilities. Moreover, there is no clear streaming solution for\neither style, especially considering the solution should generalize to speech\nmultitask. We reformulate streamable SpeechLLM as a read-write policy problem\nand unifies the offline and streaming research with BESTOW architecture. Hence\nwe demonstrate the first open-source SpeechLLM solution that enables Streaming\nand Multitask at scale (beyond ASR) at the same time. This streamable solution\nachieves very strong performance on a wide range of speech tasks (ASR, AST,\nSQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower\ntraining/inference cost, and demonstrates LLM knowledge transferability to\nspeech.", "paper_summary_zh": "\u5c07\u8a9e\u97f3\u7406\u89e3\u80fd\u529b\u6574\u5408\u5230\u9810\u8a13\u7df4\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u4e2d\u5df2\u6210\u70ba\u4e00\u500b\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411 (SpeechLLM)\u3002\u5148\u524d\u7684\u67b6\u69cb\u53ef\u4ee5\u5206\u985e\u70ba\uff1ai) GPT \u98a8\u683c\uff0c\u5c07\u8a9e\u97f3\u63d0\u793a\u4f5c\u70ba LLM \u8f38\u5165\u7684\u5e8f\u5217\uff0c\u9644\u52a0\u5230\u6587\u5b57\u63d0\u793a\u4e4b\u524d\uff0c\u5c31\u50cf\u50c5\u89e3\u78bc\u5668\u6a21\u578b\uff1bii) T5 \u98a8\u683c\uff0c\u5728\u9810\u8a13\u7df4 LLM \u7684\u6bcf\u4e00\u5c64\u4e2d\u5f15\u5165\u8a9e\u97f3\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u6211\u5011\u63d0\u51fa BESTOW \u67b6\u69cb\uff0c\u5c07\u5169\u500b\u4e16\u754c\u7684\u6700\u4f73\u529f\u80fd\u5e36\u5165\u4e00\u500b\u55ae\u4e00\u6a21\u578b\u4e2d\uff0c\u8a72\u6a21\u578b\u9ad8\u6548\u4e14\u5177\u6709\u5f37\u5927\u7684\u591a\u4efb\u52d9\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u9019\u5169\u7a2e\u98a8\u683c\u90fd\u6c92\u6709\u660e\u78ba\u7684\u4e32\u6d41\u89e3\u6c7a\u65b9\u6848\uff0c\u7279\u5225\u662f\u8003\u616e\u5230\u8a72\u89e3\u6c7a\u65b9\u6848\u61c9\u6982\u62ec\u70ba\u8a9e\u97f3\u591a\u4efb\u52d9\u3002\u6211\u5011\u5c07\u53ef\u4e32\u6d41 SpeechLLM \u91cd\u65b0\u8868\u8ff0\u70ba\u8b80\u5beb\u7b56\u7565\u554f\u984c\uff0c\u4e26\u4f7f\u7528 BESTOW \u67b6\u69cb\u7d71\u4e00\u96e2\u7dda\u548c\u4e32\u6d41\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5c55\u793a\u4e86\u7b2c\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc SpeechLLM \u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u80fd\u540c\u6642\u5927\u898f\u6a21\u555f\u7528\u4e32\u6d41\u548c\u591a\u4efb\u52d9\uff08\u8d85\u8d8a ASR\uff09\u3002\u6b64\u53ef\u4e32\u6d41\u89e3\u6c7a\u65b9\u6848\u5728\u5ee3\u6cdb\u7684\u8a9e\u97f3\u4efb\u52d9\uff08ASR\u3001AST\u3001SQA\u3001\u672a\u898b\u904e\u7684 DynamicSuperb\uff09\u4e0a\u5be6\u73fe\u4e86\u975e\u5e38\u5f37\u5927\u7684\u6548\u80fd\u3002\u5b83\u662f\u7aef\u5230\u7aef\u53ef\u6700\u4f73\u5316\u7684\uff0c\u5177\u6709\u8f03\u4f4e\u7684\u8a13\u7df4/\u63a8\u8ad6\u6210\u672c\uff0c\u4e26\u5c55\u793a\u4e86 LLM \u77e5\u8b58\u53ef\u8f49\u79fb\u5230\u8a9e\u97f3\u3002", "author": "Zhehuai Chen et.al.", "authors": "Zhehuai Chen, He Huang, Oleksii Hrinchuk, Krishna C. Puvvada, Nithin Rao Koluguri, Piotr \u017belasko, Jagadeesh Balam, Boris Ginsburg", "id": "2406.19954v1", "paper_url": "http://arxiv.org/abs/2406.19954v1", "repo": "null"}}