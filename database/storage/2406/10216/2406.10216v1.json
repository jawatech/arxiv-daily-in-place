{"2406.10216": {"publish_time": "2024-06-14", "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs", "paper_summary": "Reward models trained on human preference data have been proven to be\neffective for aligning Large Language Models (LLMs) with human intent within\nthe reinforcement learning from human feedback (RLHF) framework. However, the\ngeneralization capabilities of current reward models to unseen prompts and\nresponses are limited. This limitation can lead to an unexpected phenomenon\nknown as reward over-optimization, where excessive optimization of rewards\nresults in a decline in actual performance. While previous research has\nadvocated for constraining policy optimization, our study proposes a novel\napproach to enhance the reward model's generalization ability against\ndistribution shifts by regularizing the hidden states. Specifically, we retain\nthe base model's language model head and incorporate a suite of text-generation\nlosses to preserve the hidden states' text generation capabilities, while\nconcurrently learning a reward head behind the same hidden states. Our\nexperimental results demonstrate that the introduced regularization technique\nmarkedly improves the accuracy of learned reward models across a variety of\nout-of-distribution (OOD) tasks and effectively alleviate the over-optimization\nissue in RLHF, offering a more reliable and robust preference learning\nparadigm.", "paper_summary_zh": "\u734e\u52f5\u6a21\u578b\u7d93\u904e\u4eba\u985e\u504f\u597d\u6578\u64da\u8a13\u7df4\uff0c\u5df2\u8b49\u660e\u53ef\u6709\u6548\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u610f\u5716\u7d50\u5408\u5728\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u67b6\u69cb\u4e2d\u3002\u7136\u800c\uff0c\u7576\u524d\u734e\u52f5\u6a21\u578b\u5c0d\u672a\u898b\u63d0\u793a\u548c\u56de\u61c9\u7684\u6982\u5316\u80fd\u529b\u6709\u9650\u3002\u9019\u7a2e\u9650\u5236\u53ef\u80fd\u5c0e\u81f4\u4e00\u7a2e\u7a31\u70ba\u734e\u52f5\u904e\u5ea6\u6700\u4f73\u5316\u7684\u73fe\u8c61\uff0c\u5176\u4e2d\u904e\u5ea6\u6700\u4f73\u5316\u734e\u52f5\u6703\u5c0e\u81f4\u5be6\u969b\u6548\u80fd\u4e0b\u964d\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u4e3b\u5f35\u7d04\u675f\u653f\u7b56\u6700\u4f73\u5316\uff0c\u4f46\u6211\u5011\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u898f\u7bc4\u96b1\u85cf\u72c0\u614b\u4f86\u589e\u5f37\u734e\u52f5\u6a21\u578b\u5c0d\u6297\u5206\u4f48\u8f49\u79fb\u7684\u6982\u5316\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4fdd\u7559\u57fa\u790e\u6a21\u578b\u7684\u8a9e\u8a00\u6a21\u578b\u982d\uff0c\u4e26\u7d50\u5408\u4e00\u5957\u6587\u5b57\u751f\u6210\u640d\u5931\uff0c\u4ee5\u4fdd\u7559\u96b1\u85cf\u72c0\u614b\u7684\u6587\u5b57\u751f\u6210\u80fd\u529b\uff0c\u540c\u6642\u5728\u76f8\u540c\u7684\u96b1\u85cf\u72c0\u614b\u5f8c\u9762\u5b78\u7fd2\u734e\u52f5\u982d\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5f15\u5165\u7684\u6b63\u5247\u5316\u6280\u8853\u986f\u8457\u63d0\u9ad8\u4e86\u5404\u7a2e\u5206\u4f48\u5916 (OOD) \u4efb\u52d9\u4e2d\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\u7684\u6e96\u78ba\u6027\uff0c\u4e26\u6709\u6548\u7de9\u89e3\u4e86 RLHF \u4e2d\u7684\u904e\u5ea6\u6700\u4f73\u5316\u554f\u984c\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u53ef\u9760\u4e14\u5f37\u5927\u7684\u504f\u597d\u5b78\u7fd2\u7bc4\u4f8b\u3002", "author": "Rui Yang et.al.", "authors": "Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang", "id": "2406.10216v1", "paper_url": "http://arxiv.org/abs/2406.10216v1", "repo": "null"}}