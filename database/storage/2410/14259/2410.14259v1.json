{"2410.14259": {"publish_time": "2024-10-18", "title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement", "paper_summary": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5982 ChatGPT \u7684\u5feb\u901f\u767c\u5c55\uff0c\u5c0e\u81f4 LLM \u751f\u6210\u7684\u5167\u5bb9\u5ee3\u6cdb\u51fa\u73fe\u5728\u793e\u7fa4\u5a92\u9ad4\u5e73\u53f0\u4e0a\uff0c\u5f15\u767c\u4e86\u95dc\u65bc\u932f\u8aa4\u8cc7\u8a0a\u3001\u8cc7\u6599\u504f\u5dee\u548c\u96b1\u79c1\u4fb5\u72af\u7684\u7591\u616e\uff0c\u9019\u53ef\u80fd\u7834\u58de\u7dda\u4e0a\u8a0e\u8ad6\u7684\u4fe1\u4efb\u611f\u3002\u5118\u7ba1\u5075\u6e2c LLM \u751f\u6210\u7684\u5167\u5bb9\u5c0d\u65bc\u6e1b\u8f15\u9019\u4e9b\u98a8\u96aa\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u8457\u91cd\u65bc\u4e8c\u5143\u5206\u985e\uff0c\u672a\u80fd\u89e3\u6c7a\u73fe\u5be6\u4e16\u754c\u60c5\u5883\uff08\u4f8b\u5982\u4eba\u985e\u8207 AI \u5408\u4f5c\uff09\u7684\u8907\u96dc\u6027\u3002\u70ba\u4e86\u8d85\u8d8a\u4e8c\u5143\u5206\u985e\u4e26\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5075\u6e2c LLM \u751f\u6210\u7684\u5167\u5bb9\u7684\u65b0\u7bc4\u4f8b\u3002\u6b64\u65b9\u6cd5\u5f15\u5165\u4e86\u5169\u500b\u65b0\u4efb\u52d9\uff1aLLM \u89d2\u8272\u8fa8\u8b58 (LLM-RR)\uff0c\u4e00\u500b\u591a\u985e\u5225\u5206\u985e\u4efb\u52d9\uff0c\u7528\u65bc\u8b58\u5225 LLM \u5728\u5167\u5bb9\u7522\u751f\u4e2d\u7684\u7279\u5b9a\u89d2\u8272\uff1b\u4ee5\u53ca LLM \u5f71\u97ff\u529b\u6e2c\u91cf (LLM-IM)\uff0c\u4e00\u500b\u56de\u6b78\u4efb\u52d9\uff0c\u7528\u65bc\u91cf\u5316 LLM \u5728\u5167\u5bb9\u5275\u4f5c\u4e2d\u7684\u53c3\u8207\u7a0b\u5ea6\u3002\u70ba\u4e86\u652f\u63f4\u9019\u4e9b\u4efb\u52d9\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LLMDetect\uff0c\u4e00\u500b\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u5075\u6e2c\u5668\u5728\u9019\u4e9b\u65b0\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002LLMDetect \u5305\u542b\u4e86\u6df7\u5408\u65b0\u805e\u5075\u6e2c\u8a9e\u6599\u5eab (HNDC)\uff0c\u7528\u65bc\u8a13\u7df4\u5075\u6e2c\u5668\uff0c\u4ee5\u53ca DetectEval\uff0c\u4e00\u500b\u5168\u9762\u7684\u8a55\u91cf\u5957\u4ef6\uff0c\u5b83\u8003\u616e\u4e86\u540c\u4e00\u500b LLM \u89d2\u8272\u4e2d\u7684\u4e94\u7a2e\u4e0d\u540c\u7684\u8de8\u60c5\u5883\u8b8a\u5316\u548c\u591a\u5f37\u5ea6\u8b8a\u5316\u3002\u9019\u5141\u8a31\u5c0d\u5075\u6e2c\u5668\u7684\u6cdb\u5316\u6027\u548c\u5728\u4e0d\u540c\u60c5\u5883\u4e2d\u7684\u7a69\u5065\u6027\u9032\u884c\u5fb9\u5e95\u8a55\u4f30\u3002\u6211\u5011\u5c0d 10 \u7a2e\u57fa\u7dda\u5075\u6e2c\u65b9\u6cd5\u7684\u5be6\u8b49\u9a57\u8b49\u8868\u660e\uff0c\u7d93\u904e\u5fae\u8abf\u7684\u57fa\u65bc PLM \u7684\u6a21\u578b\u5728\u5169\u500b\u4efb\u52d9\u4e0a\u90fd\u6301\u7e8c\u512a\u65bc\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u9032\u968e\u7684 LLM \u5728\u6e96\u78ba\u5075\u6e2c\u5176\u81ea\u5df1\u7522\u751f\u7684\u5167\u5bb9\u6642\u9762\u81e8\u6311\u6230\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u898b\u89e3\uff0c\u7528\u65bc\u958b\u767c\u66f4\u6709\u6548\u7684 LLM \u751f\u6210\u7684\u5167\u5bb9\u5075\u6e2c\u6a21\u578b\u3002\u9019\u9805\u7814\u7a76\u589e\u5f37\u4e86\u5c0d LLM \u751f\u6210\u7684\u5167\u5bb9\u7684\u7406\u89e3\uff0c\u4e26\u70ba\u66f4\u7d30\u7dfb\u7684\u5075\u6e2c\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Zihao Cheng et.al.", "authors": "Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li", "id": "2410.14259v1", "paper_url": "http://arxiv.org/abs/2410.14259v1", "repo": "null"}}