{"2405.17888": {"publish_time": "2024-05-28", "title": "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment", "paper_summary": "Aligning human preference and value is an important requirement for\ncontemporary foundation models. State-of-the-art techniques such as\nReinforcement Learning from Human Feedback (RLHF) often consist of two stages:\n1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from\nhuman demonstration data; 2) Preference learning, where preference data is used\nto learn a reward model, which is in turn used by a reinforcement learning (RL)\nstep to fine-tune the model. Such reward model serves as a proxy to human\npreference, and it is critical to guide the RL step towards improving the model\nquality. In this work, we argue that the SFT stage significantly benefits from\nlearning a reward model as well. Instead of using the human demonstration data\ndirectly via supervised learning, we propose to leverage an Inverse\nReinforcement Learning (IRL) technique to (explicitly or implicitly) build an\nreward model, while learning the policy model. This approach leads to new SFT\nalgorithms that are not only efficient to implement, but also promote the\nability to distinguish between the preferred and non-preferred continuations.\nMoreover, we identify a connection between the proposed IRL based approach, and\ncertain self-play approach proposed recently, and showed that self-play is a\nspecial case of modeling a reward-learning agent. Theoretically, we show that\nthe proposed algorithms converge to the stationary solutions of the IRL\nproblem. Empirically, we align 1B and 7B models using proposed methods and\nevaluate them on a reward benchmark model and the HuggingFace Open LLM\nLeaderboard. The proposed methods show significant performance improvement over\nexisting SFT approaches. Our results indicate that it is beneficial to\nexplicitly or implicitly leverage reward learning throughout the entire\nalignment process.", "paper_summary_zh": "\u5c0d\u9f4a\u4eba\u985e\u504f\u597d\u548c\u50f9\u503c\u662f\u7576\u4ee3\u57fa\u790e\u6a21\u578b\u7684\u4e00\u9805\u91cd\u8981\u8981\u6c42\u3002\u6700\u5148\u9032\u7684\u6280\u8853\uff0c\u4f8b\u5982\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u901a\u5e38\u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a1) \u76e3\u7763\u5fae\u8abf (SFT)\uff0c\u5176\u4e2d\u6a21\u578b\u901a\u904e\u5f9e\u4eba\u985e\u793a\u7bc4\u6578\u64da\u4e2d\u5b78\u7fd2\u9032\u884c\u5fae\u8abf\uff1b2) \u504f\u597d\u5b78\u7fd2\uff0c\u5176\u4e2d\u504f\u597d\u6578\u64da\u7528\u65bc\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\uff0c\u800c\u734e\u52f5\u6a21\u578b\u53cd\u904e\u4f86\u53c8\u7531\u5f37\u5316\u5b78\u7fd2 (RL) \u6b65\u9a5f\u7528\u65bc\u5fae\u8abf\u6a21\u578b\u3002\u9019\u7a2e\u734e\u52f5\u6a21\u578b\u4f5c\u70ba\u4eba\u985e\u504f\u597d\u7684\u4ee3\u7406\uff0c\u5c0d\u65bc\u6307\u5c0e RL \u6b65\u9a5f\u6539\u5584\u6a21\u578b\u54c1\u8cea\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a8d\u70ba SFT \u968e\u6bb5\u4e5f\u986f\u8457\u53d7\u76ca\u65bc\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\u3002\u6211\u5011\u6c92\u6709\u76f4\u63a5\u901a\u904e\u76e3\u7763\u5b78\u7fd2\u4f7f\u7528\u4eba\u985e\u793a\u7bc4\u6578\u64da\uff0c\u800c\u662f\u63d0\u51fa\u5229\u7528\u9006\u5411\u5f37\u5316\u5b78\u7fd2 (IRL) \u6280\u8853\uff08\u986f\u5f0f\u6216\u96b1\u5f0f\u5730\uff09\u5efa\u7acb\u734e\u52f5\u6a21\u578b\uff0c\u540c\u6642\u5b78\u7fd2\u7b56\u7565\u6a21\u578b\u3002\u9019\u7a2e\u65b9\u6cd5\u5c0e\u81f4\u65b0\u7684 SFT \u6f14\u7b97\u6cd5\uff0c\u9019\u4e9b\u6f14\u7b97\u6cd5\u4e0d\u50c5\u6613\u65bc\u5be6\u4f5c\uff0c\u9084\u80fd\u4fc3\u9032\u5340\u5206\u9996\u9078\u548c\u975e\u9996\u9078\u5ef6\u7e8c\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u6240\u63d0\u51fa\u7684\u57fa\u65bc IRL \u7684\u65b9\u6cd5\u8207\u6700\u8fd1\u63d0\u51fa\u7684\u67d0\u4e9b\u81ea\u73a9\u65b9\u6cd5\u4e4b\u9593\u5b58\u5728\u806f\u7e6b\uff0c\u4e26\u8868\u660e\u81ea\u73a9\u662f\u5efa\u6a21\u734e\u52f5\u5b78\u7fd2\u4ee3\u7406\u7684\u7279\u6b8a\u60c5\u6cc1\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u8868\u660e\u6240\u63d0\u51fa\u7684\u6f14\u7b97\u6cd5\u6536\u6582\u5230 IRL \u554f\u984c\u7684\u5e73\u7a69\u89e3\u3002\u5728\u7d93\u9a57\u4e0a\uff0c\u6211\u5011\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c0d\u9f4a 1B \u548c 7B \u6a21\u578b\uff0c\u4e26\u5728\u734e\u52f5\u57fa\u6e96\u6a21\u578b\u548c HuggingFace Open LLM \u6392\u884c\u699c\u4e0a\u5c0d\u5b83\u5011\u9032\u884c\u8a55\u4f30\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u986f\u793a\u51fa\u6bd4\u73fe\u6709\u7684 SFT \u65b9\u6cd5\u6709\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5728\u6574\u500b\u5c0d\u9f4a\u904e\u7a0b\u4e2d\u986f\u5f0f\u6216\u96b1\u5f0f\u5730\u5229\u7528\u734e\u52f5\u5b78\u7fd2\u662f\u6709\u76ca\u7684\u3002", "author": "Jiaxiang Li et.al.", "authors": "Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong", "id": "2405.17888v2", "paper_url": "http://arxiv.org/abs/2405.17888v2", "repo": "null"}}