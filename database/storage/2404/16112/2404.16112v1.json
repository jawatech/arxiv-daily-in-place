{"2404.16112": {"publish_time": "2024-04-24", "title": "Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges", "paper_summary": "Sequence modeling is a crucial area across various domains, including Natural\nLanguage Processing (NLP), speech recognition, time series forecasting, music\ngeneration, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short\nTerm Memory Networks (LSTMs) have historically dominated sequence modeling\ntasks like Machine Translation, Named Entity Recognition (NER), etc. However,\nthe advancement of transformers has led to a shift in this paradigm, given\ntheir superior performance. Yet, transformers suffer from $O(N^2)$ attention\ncomplexity and challenges in handling inductive bias. Several variations have\nbeen proposed to address these issues which use spectral networks or\nconvolutions and have performed well on a range of tasks. However, they still\nhave difficulty in dealing with long sequences. State Space Models(SSMs) have\nemerged as promising alternatives for sequence modeling paradigms in this\ncontext, especially with the advent of S4 and its variants, such as S4nd,\nHippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear\nRecurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the\nfoundational SSMs based on three paradigms namely, Gating architectures,\nStructural architectures, and Recurrent architectures. This survey also\nhighlights diverse applications of SSMs across domains such as vision, video,\naudio, speech, language (especially long sequence modeling), medical (including\ngenomics), chemical (like drug design), recommendation systems, and time series\nanalysis, including tabular data. Moreover, we consolidate the performance of\nSSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile,\nImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast,\nCOIN, LVU, and various time series datasets. The project page for Mamba-360\nwork is available on this webpage.\\url{https://github.com/badripatro/mamba360}.", "paper_summary_zh": "\u5e8f\u5217\u5efa\u6a21\u662f\u5404\u7a2e\u9818\u57df\u7684\u95dc\u9375\u9818\u57df\uff0c\u5305\u62ec\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP)\u3001\u8a9e\u97f3\u8fa8\u8b58\u3001\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u3001\u97f3\u6a02\u751f\u6210\u548c\u751f\u7269\u8cc7\u8a0a\u5b78\u3002\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN) \u548c\u9577\u77ed\u671f\u8a18\u61b6\u7db2\u8def (LSTM) \u5728\u6b77\u53f2\u4e0a\u4e00\u76f4\u4e3b\u5c0e\u8457\u6a5f\u5668\u7ffb\u8b6f\u3001\u547d\u540d\u5be6\u9ad4\u8fa8\u8b58 (NER) \u7b49\u5e8f\u5217\u5efa\u6a21\u4efb\u52d9\u3002\u7136\u800c\uff0c\u7531\u65bcTransformer\u7684\u6548\u80fd\u512a\u7570\uff0c\u4fc3\u4f7f\u9019\u7a2e\u6a21\u5f0f\u767c\u751f\u8f49\u8b8a\u3002\u7136\u800c\uff0cTransformer\u6709 $O(N^2)$ \u6ce8\u610f\u529b\u8907\u96dc\u5ea6\u548c\u8655\u7406\u6b78\u7d0d\u504f\u8aa4\u7684\u6311\u6230\u3002\u5df2\u7d93\u63d0\u51fa\u4e86\u4e00\u4e9b\u8b8a\u9ad4\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u9019\u4e9b\u8b8a\u9ad4\u4f7f\u7528\u983b\u8b5c\u7db2\u8def\u6216\u5377\u7a4d\uff0c\u4e26\u4e14\u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u8868\u73fe\u826f\u597d\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u4ecd\u7136\u6709\u56f0\u96e3\u3002\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM) \u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\u5df2\u6210\u70ba\u5e8f\u5217\u5efa\u6a21\u7bc4\u4f8b\u7684\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u5225\u662f\u96a8\u8457 S4 \u53ca\u5176\u8b8a\u9ad4\u7684\u51fa\u73fe\uff0c\u4f8b\u5982 S4nd\u3001Hippo\u3001Hyena\u3001Diagnol State Spaces (DSS)\u3001Gated State Spaces (GSS)\u3001Linear Recurrent Unit (LRU)\u3001Liquid-S4\u3001Mamba \u7b49\u3002\u5728\u672c\u6b21\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u6839\u64da\u4e09\u7a2e\u7bc4\u4f8b\u5c0d\u57fa\u790e SSM \u9032\u884c\u5206\u985e\uff0c\u5373\u9598\u63a7\u67b6\u69cb\u3001\u7d50\u69cb\u67b6\u69cb\u548c\u905e\u8ff4\u67b6\u69cb\u3002\u672c\u6b21\u8abf\u67e5\u9084\u91cd\u9ede\u4ecb\u7d39\u4e86 SSM \u5728\u8996\u89ba\u3001\u5f71\u7247\u3001\u97f3\u8a0a\u3001\u8a9e\u97f3\u3001\u8a9e\u8a00\uff08\u7279\u5225\u662f\u9577\u5e8f\u5217\u5efa\u6a21\uff09\u3001\u91ab\u7642\uff08\u5305\u62ec\u57fa\u56e0\u7d44\u5b78\uff09\u3001\u5316\u5b78\uff08\u4f8b\u5982\u85e5\u7269\u8a2d\u8a08\uff09\u3001\u63a8\u85a6\u7cfb\u7d71\u548c\u6642\u9593\u5e8f\u5217\u5206\u6790\uff08\u5305\u62ec\u8868\u683c\u8cc7\u6599\uff09\u7b49\u9818\u57df\u7684\u5404\u7a2e\u61c9\u7528\u3002\u6b64\u5916\uff0c\u6211\u5011\u6574\u5408\u4e86 SSM \u5728 Long Range Arena (LRA)\u3001WikiText\u3001Glue\u3001Pile\u3001ImageNet\u3001Kinetics-400\u3001sstv2 \u7b49\u57fa\u6e96\u8cc7\u6599\u96c6\u4ee5\u53ca\u65e9\u9910\u3001COIN\u3001LVU \u548c\u5404\u7a2e\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u96c6\u7b49\u5f71\u7247\u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\u3002Mamba-360 \u5c08\u6848\u9801\u9762\u53ef\u5728\u9019\u500b\u7db2\u9801\u4e0a\u627e\u5230\u3002\\url{https://github.com/badripatro/mamba360}\u3002", "author": "Badri Narayana Patro et.al.", "authors": "Badri Narayana Patro, Vijay Srinivas Agneeswaran", "id": "2404.16112v1", "paper_url": "http://arxiv.org/abs/2404.16112v1", "repo": "https://github.com/badripatro/mamba360"}}