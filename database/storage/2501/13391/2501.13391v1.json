{"2501.13391": {"publish_time": "2025-01-23", "title": "Can Large Language Models Understand Preferences in Personalized Recommendation?", "paper_summary": "Large Language Models (LLMs) excel in various tasks, including personalized\nrecommendations. Existing evaluation methods often focus on rating prediction,\nrelying on regression errors between actual and predicted ratings. However,\nuser rating bias and item quality, two influential factors behind rating\nscores, can obscure personal preferences in user-item pair data. To address\nthis, we introduce PerRecBench, disassociating the evaluation from these two\nfactors and assessing recommendation techniques on capturing the personal\npreferences in a grouped ranking manner. We find that the LLM-based\nrecommendation techniques that are generally good at rating prediction fail to\nidentify users' favored and disfavored items when the user rating bias and item\nquality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find\nthat while larger models generally outperform smaller ones, they still struggle\nwith personalized recommendation. Our findings reveal the superiority of\npairwise and listwise ranking approaches over pointwise ranking, PerRecBench's\nlow correlation with traditional regression metrics, the importance of user\nprofiles, and the role of pretraining data distributions. We further explore\nthree supervised fine-tuning strategies, finding that merging weights from\nsingle-format training is promising but improving LLMs' understanding of user\npreferences remains an open research problem. Code and data are available at\nhttps://github.com/TamSiuhin/PerRecBench", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5305\u62ec\u500b\u4eba\u5316\u63a8\u85a6\u3002\u73fe\u6709\u7684\u8a55\u4f30\u65b9\u6cd5\u901a\u5e38\u5c08\u6ce8\u65bc\u8a55\u5206\u9810\u6e2c\uff0c\u4f9d\u8cf4\u65bc\u5be6\u969b\u8a55\u5206\u548c\u9810\u6e2c\u8a55\u5206\u4e4b\u9593\u7684\u56de\u6b78\u8aa4\u5dee\u3002\u7136\u800c\uff0c\u4f7f\u7528\u8005\u8a55\u5206\u504f\u5dee\u548c\u9805\u76ee\u54c1\u8cea\u9019\u5169\u500b\u5f71\u97ff\u8a55\u5206\u80cc\u5f8c\u7684\u56e0\u7d20\uff0c\u53ef\u80fd\u6703\u6a21\u7cca\u4f7f\u7528\u8005-\u9805\u76ee\u914d\u5c0d\u8cc7\u6599\u4e2d\u7684\u500b\u4eba\u504f\u597d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 PerRecBench\uff0c\u5c07\u8a55\u4f30\u8207\u9019\u5169\u500b\u56e0\u7d20\u5206\u96e2\uff0c\u4e26\u8a55\u4f30\u63a8\u85a6\u6280\u8853\u4ee5\u7fa4\u7d44\u6392\u540d\u65b9\u5f0f\u6355\u6349\u500b\u4eba\u504f\u597d\u3002\u6211\u5011\u767c\u73fe\uff0c\u57fa\u65bc LLM \u7684\u63a8\u85a6\u6280\u8853\u901a\u5e38\u5f88\u64c5\u9577\u8a55\u5206\u9810\u6e2c\uff0c\u4f46\u5728\u7fa4\u7d44\u4f7f\u7528\u8005\u6642\u7121\u6cd5\u8b58\u5225\u4f7f\u7528\u8005\u7684\u504f\u597d\u548c\u4e0d\u504f\u597d\u7684\u9805\u76ee\u3002\u4f7f\u7528 PerRecBench \u548c 19 \u500b LLM\uff0c\u6211\u5011\u767c\u73fe\u96d6\u7136\u8f03\u5927\u7684\u6a21\u578b\u901a\u5e38\u512a\u65bc\u8f03\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u5b83\u5011\u5728\u500b\u4eba\u5316\u63a8\u85a6\u65b9\u9762\u4ecd\u7136\u6709\u56f0\u96e3\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u6210\u5c0d\u548c\u6e05\u55ae\u6392\u540d\u65b9\u6cd5\u512a\u65bc\u9010\u9ede\u6392\u540d\u3001PerRecBench \u8207\u50b3\u7d71\u56de\u6b78\u6307\u6a19\u7684\u4f4e\u76f8\u95dc\u6027\u3001\u4f7f\u7528\u8005\u500b\u4eba\u8cc7\u6599\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u9810\u8a13\u7df4\u8cc7\u6599\u5206\u5e03\u7684\u89d2\u8272\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u4e09\u7a2e\u76e3\u7763\u5fae\u8abf\u7b56\u7565\uff0c\u767c\u73fe\u5408\u4f75\u55ae\u4e00\u683c\u5f0f\u8a13\u7df4\u7684\u6b0a\u91cd\u662f\u6709\u5e0c\u671b\u7684\uff0c\u4f46\u6539\u5584 LLM \u5c0d\u4f7f\u7528\u8005\u504f\u597d\u7684\u7406\u89e3\u4ecd\u7136\u662f\u4e00\u500b\u958b\u653e\u7684\u7814\u7a76\u554f\u984c\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/TamSiuhin/PerRecBench \u53d6\u5f97", "author": "Zhaoxuan Tan et.al.", "authors": "Zhaoxuan Tan, Zinan Zeng, Qingkai Zeng, Zhenyu Wu, Zheyuan Liu, Fengran Mo, Meng Jiang", "id": "2501.13391v1", "paper_url": "http://arxiv.org/abs/2501.13391v1", "repo": "null"}}