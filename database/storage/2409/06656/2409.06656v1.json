{"2409.06656": {"publish_time": "2024-09-10", "title": "Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens", "paper_summary": "We propose Sortformer, a novel neural model for speaker diarization, trained\nwith unconventional objectives compared to existing end-to-end diarization\nmodels. The permutation problem in speaker diarization has long been regarded\nas a critical challenge. Most prior end-to-end diarization systems employ\npermutation invariant loss (PIL), which optimizes for the permutation that\nyields the lowest error. In contrast, we introduce Sort Loss, which enables a\ndiarization model to autonomously resolve permutation, with or without PIL. We\ndemonstrate that combining Sort Loss and PIL achieves performance competitive\nwith state-of-the-art end-to-end diarization models trained exclusively with\nPIL. Crucially, we present a streamlined multispeaker ASR architecture that\nleverages Sortformer as a speaker supervision model, embedding speaker label\nestimation within the ASR encoder state using a sinusoidal kernel function.\nThis approach resolves the speaker permutation problem through sorted\nobjectives, effectively bridging speaker-label timestamps and speaker tokens.\nIn our experiments, we show that the proposed multispeaker ASR architecture,\nenhanced with speaker supervision, improves performance via adapter techniques.\nCode and trained models will be made publicly available via the NVIDIA NeMo\nframework", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa Sortformer\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc\u8aaa\u8a71\u8005\u5206\u7d44\u7684\u65b0\u578b\u795e\u7d93\u6a21\u578b\uff0c\u5b83\u8207\u73fe\u6709\u7684\u7aef\u5230\u7aef\u5206\u7d44\u6a21\u578b\u76f8\u6bd4\uff0c\u63a1\u7528\u4e86\u975e\u5e38\u898f\u7684\u76ee\u6a19\u9032\u884c\u8a13\u7df4\u3002\u8aaa\u8a71\u8005\u5206\u7d44\u4e2d\u7684\u6392\u5217\u554f\u984c\u9577\u671f\u4ee5\u4f86\u4e00\u76f4\u88ab\u8a8d\u70ba\u662f\u4e00\u500b\u95dc\u9375\u6311\u6230\u3002\u5927\u591a\u6578\u5148\u524d\u7684\u7aef\u5230\u7aef\u5206\u7d44\u7cfb\u7d71\u63a1\u7528\u6392\u5217\u4e0d\u8b8a\u640d\u5931 (PIL)\uff0c\u5b83\u91dd\u5c0d\u7522\u751f\u6700\u4f4e\u932f\u8aa4\u7684\u6392\u5217\u9032\u884c\u512a\u5316\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5206\u985e\u640d\u5931\uff0c\u5b83\u4f7f\u5206\u7d44\u6a21\u578b\u80fd\u5920\u81ea\u4e3b\u89e3\u6c7a\u6392\u5217\uff0c\u7121\u8ad6\u662f\u5426\u4f7f\u7528 PIL\u3002\u6211\u5011\u8b49\u660e\u4e86\u5c07\u5206\u985e\u640d\u5931\u548c PIL \u7d50\u5408\u8d77\u4f86\uff0c\u53ef\u4ee5\u5be6\u73fe\u8207\u50c5\u4f7f\u7528 PIL \u8a13\u7df4\u7684\u6700\u65b0\u7aef\u5230\u7aef\u5206\u7d44\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6548\u80fd\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u5316\u7684\u591a\u8aaa\u8a71\u8005 ASR \u67b6\u69cb\uff0c\u5b83\u5229\u7528 Sortformer \u4f5c\u70ba\u8aaa\u8a71\u8005\u76e3\u7763\u6a21\u578b\uff0c\u4f7f\u7528\u6b63\u5f26\u6838\u51fd\u6578\u5728 ASR \u7de8\u78bc\u5668\u72c0\u614b\u4e2d\u5d4c\u5165\u8aaa\u8a71\u8005\u6a19\u7c64\u4f30\u8a08\u3002\u9019\u7a2e\u65b9\u6cd5\u901a\u904e\u5206\u985e\u76ee\u6a19\u89e3\u6c7a\u4e86\u8aaa\u8a71\u8005\u6392\u5217\u554f\u984c\uff0c\u6709\u6548\u5730\u6a4b\u63a5\u4e86\u8aaa\u8a71\u8005\u6a19\u7c64\u6642\u9593\u6233\u548c\u8aaa\u8a71\u8005\u6a19\u8a18\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u901a\u904e\u9069\u914d\u5668\u6280\u8853\u589e\u5f37\u7684\u5efa\u8b70\u591a\u8aaa\u8a71\u8005 ASR \u67b6\u69cb\u901a\u904e\u9069\u914d\u5668\u6280\u8853\u6539\u5584\u4e86\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u548c\u8a13\u7df4\u597d\u7684\u6a21\u578b\u5c07\u901a\u904e NVIDIA NeMo \u6846\u67b6\u516c\u958b\u3002", "author": "Taejin Park et.al.", "authors": "Taejin Park, Ivan Medennikov, Kunal Dhawan, Weiqing Wang, He Huang, Nithin Rao Koluguri, Krishna C. Puvvada, Jagadeesh Balam, Boris Ginsburg", "id": "2409.06656v1", "paper_url": "http://arxiv.org/abs/2409.06656v1", "repo": "null"}}