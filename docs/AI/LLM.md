
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-22**|**LLMmap: Fingerprinting For Large Language Models**|Dario Pasquini et.al.|[2407.15847v1](http://arxiv.org/abs/2407.15847v1)|null|
|**2024-07-22**|**Reconstructing Training Data From Real World Models Trained with Transfer Learning**|Yakir Oz et.al.|[2407.15845v1](http://arxiv.org/abs/2407.15845v1)|null|
|**2024-07-22**|**CarFormer: Self-Driving with Learned Object-Centric Representations**|Shadi Hamdan et.al.|[2407.15843v1](http://arxiv.org/abs/2407.15843v1)|null|
|**2024-07-22**|**Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments**|Mansur Arief et.al.|[2407.15839v1](http://arxiv.org/abs/2407.15839v1)|null|
|**2024-07-22**|**Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning**|Yibing Wei et.al.|[2407.15837v1](http://arxiv.org/abs/2407.15837v1)|[link](https://github.com/yibingwei-1/latentmim)|
|**2024-07-22**|**dMel: Speech Tokenization made Simple**|He Bai et.al.|[2407.15835v1](http://arxiv.org/abs/2407.15835v1)|null|
|**2024-07-22**|**NV-Retriever: Improving text embedding models with effective hard-negative mining**|Gabriel de Souza P. Moreira et.al.|[2407.15831v1](http://arxiv.org/abs/2407.15831v1)|null|
|**2024-07-22**|**J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling**|Wataru Nakata et.al.|[2407.15828v1](http://arxiv.org/abs/2407.15828v1)|null|
|**2024-07-22**|**Perceptions of Linguistic Uncertainty by Language Models and Humans**|Catarina G Belem et.al.|[2407.15814v1](http://arxiv.org/abs/2407.15814v1)|[link](https://github.com/ucidatalab/llm-uncertainty-perceptions)|
|**2024-07-22**|**Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget**|Vikash Sehwag et.al.|[2407.15811v1](http://arxiv.org/abs/2407.15811v1)|null|
|**2024-07-22**|**FSboard: Over 3 million characters of ASL fingerspelling collected via smartphones**|Manfred Georg et.al.|[2407.15806v1](http://arxiv.org/abs/2407.15806v1)|null|
|**2024-07-22**|**CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning**|Emanuele Frascaroli et.al.|[2407.15793v1](http://arxiv.org/abs/2407.15793v1)|[link](https://github.com/aimagelab/mammoth)|
|**2024-07-22**|**Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach**|Rian Dolphin et.al.|[2407.15788v1](http://arxiv.org/abs/2407.15788v1)|null|
|**2024-07-22**|**Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels**|Zhuorui Ye et.al.|[2407.15786v1](http://arxiv.org/abs/2407.15786v1)|null|
|**2024-07-22**|**Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems**|Amirhassan Babazadeh Darabi et.al.|[2407.15784v1](http://arxiv.org/abs/2407.15784v1)|null|
|**2024-07-22**|**Explaining Decisions in ML Models: a Parameterized Complexity Analysis**|Sebastian Ordyniak et.al.|[2407.15780v1](http://arxiv.org/abs/2407.15780v1)|null|
|**2024-07-22**|**Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection**|Kangqi Ma et.al.|[2407.15771v1](http://arxiv.org/abs/2407.15771v1)|null|
|**2024-07-22**|**Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning**|Kaiwen Wang et.al.|[2407.15762v1](http://arxiv.org/abs/2407.15762v1)|null|
|**2024-07-22**|**Model editing for distribution shifts in uranium oxide morphological analysis**|Davis Brown et.al.|[2407.15756v1](http://arxiv.org/abs/2407.15756v1)|null|
|**2024-07-22**|**LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding**|Haoning Wu et.al.|[2407.15754v1](http://arxiv.org/abs/2407.15754v1)|[link](https://github.com/longvideobench/longvideobench)|
|**2024-07-22**|**MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**|Marco Simoni et.al.|[2407.15748v1](http://arxiv.org/abs/2407.15748v1)|null|
|**2024-07-22**|**Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond**|Silvio Galesso et.al.|[2407.15739v1](http://arxiv.org/abs/2407.15739v1)|[link](https://github.com/lmb-freiburg/diffusion-for-ood)|
|**2024-07-22**|**Parallel Split Learning with Global Sampling**|Mohammad Kohankhaki et.al.|[2407.15738v1](http://arxiv.org/abs/2407.15738v1)|null|
|**2024-07-22**|**OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context**|Steffen Kleinle et.al.|[2407.15736v1](http://arxiv.org/abs/2407.15736v1)|null|
|**2024-07-22**|**TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**|John Chong Min Tan et.al.|[2407.15734v1](http://arxiv.org/abs/2407.15734v1)|null|
|**2024-07-22**|**DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design**|Zhi Hao Luo et.al.|[2407.15723v1](http://arxiv.org/abs/2407.15723v1)|[link](https://github.com/plstory/ds2d)|
|**2024-07-22**|**Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability**|Zhuoyan Xu et.al.|[2407.15720v1](http://arxiv.org/abs/2407.15720v1)|[link](https://github.com/oliverxuzy/llm_compose)|
|**2024-07-22**|**GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI**|Zhaojie Fang et.al.|[2407.15719v1](http://arxiv.org/abs/2407.15719v1)|[link](https://github.com/tinysqua/gfe-mamba)|
|**2024-07-22**|**Mamba meets crack segmentation**|Zhili He et.al.|[2407.15714v1](http://arxiv.org/abs/2407.15714v1)|[link](https://github.com/hzlbbfrog/crackmamba)|
|**2024-07-22**|**AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?**|Ori Yoran et.al.|[2407.15711v1](http://arxiv.org/abs/2407.15711v1)|null|
|**2024-07-22**|**SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams**|Liangyan Jiang et.al.|[2407.15708v1](http://arxiv.org/abs/2407.15708v1)|null|
|**2024-07-22**|**Predicting the Best of N Visual Trackers**|Basit Alawode et.al.|[2407.15707v1](http://arxiv.org/abs/2407.15707v1)|null|
|**2024-07-22**|**Supporting the Digital Autonomy of Elders Through LLM Assistance**|Jesse Roberts et.al.|[2407.15695v1](http://arxiv.org/abs/2407.15695v1)|null|
|**2024-07-22**|**Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)**|Ishan Kavathekar et.al.|[2407.15694v1](http://arxiv.org/abs/2407.15694v1)|null|
|**2024-07-22**|**AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach**|Abdelaziz Amara korba et.al.|[2407.15688v1](http://arxiv.org/abs/2407.15688v1)|null|
|**2024-07-22**|**HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**|Zhecan Wang et.al.|[2407.15680v1](http://arxiv.org/abs/2407.15680v1)|null|
|**2024-07-22**|**Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps**|Rabbia Asghar et.al.|[2407.15675v1](http://arxiv.org/abs/2407.15675v1)|null|
|**2024-07-22**|**SLVideo: A Sign Language Video Moment Retrieval Framework**|Gon√ßalo Vinagre Martins et.al.|[2407.15668v1](http://arxiv.org/abs/2407.15668v1)|null|
|**2024-07-22**|**Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models**|Joy He-Yueya et.al.|[2407.15645v1](http://arxiv.org/abs/2407.15645v1)|[link](https://github.com/joyheyueya/psychometric-alignment)|
|**2024-07-22**|**RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation**|Soroosh Tayebi Arasteh et.al.|[2407.15621v1](http://arxiv.org/abs/2407.15621v1)|null|
|**2024-07-22**|**Can GPT-4 learn to analyze moves in research article abstracts?**|Danni Yu et.al.|[2407.15612v1](http://arxiv.org/abs/2407.15612v1)|null|
|**2024-07-22**|**StylusAI: Stylistic Adaptation for Robust German Handwritten Text Generation**|Nauman Riaz et.al.|[2407.15608v1](http://arxiv.org/abs/2407.15608v1)|null|
|**2024-07-22**|**A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism**|Yu Xue et.al.|[2407.15600v1](http://arxiv.org/abs/2407.15600v1)|null|
|**2024-07-22**|**Discrete Flow Matching**|Itai Gat et.al.|[2407.15595v1](http://arxiv.org/abs/2407.15595v1)|null|
|**2024-07-22**|**Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**|Soojin Yoon et.al.|[2407.15588v1](http://arxiv.org/abs/2407.15588v1)|[link](https://github.com/eralign/eralign)|
|**2024-07-22**|**An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought**|Yuetong Zhao et.al.|[2407.15569v1](http://arxiv.org/abs/2407.15569v1)|null|
|**2024-07-22**|**SETTP: Style Extraction and Tunable Inference via Dual-level Transferable Prompt Learning**|Chunzhen Jin et.al.|[2407.15556v1](http://arxiv.org/abs/2407.15556v1)|null|
|**2024-07-22**|**Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs**|Abhay Sheshadri et.al.|[2407.15549v1](http://arxiv.org/abs/2407.15549v1)|null|
|**2024-07-22**|**Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks**|Kamesh Korangi et.al.|[2407.15532v1](http://arxiv.org/abs/2407.15532v1)|null|
|**2024-07-22**|**Interpretable Concept-Based Memory Reasoning**|David Debot et.al.|[2407.15527v1](http://arxiv.org/abs/2407.15527v1)|null|
|**2024-07-22**|**Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks**|Eugenio Lomurno et.al.|[2407.15526v1](http://arxiv.org/abs/2407.15526v1)|null|
|**2024-07-22**|**Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management**|Roberto Morabito et.al.|[2407.15520v1](http://arxiv.org/abs/2407.15520v1)|null|
|**2024-07-22**|**Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models**|Georgy Tyukin et.al.|[2407.15516v1](http://arxiv.org/abs/2407.15516v1)|null|
|**2024-07-22**|**Increasing the Robustness of Model Predictions to Missing Sensors in Earth Observation**|Francisco Mena et.al.|[2407.15512v1](http://arxiv.org/abs/2407.15512v1)|null|
|**2024-07-22**|**Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners**|Yifei Gao et.al.|[2407.15508v1](http://arxiv.org/abs/2407.15508v1)|null|
|**2024-07-22**|**Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models**|Adway Girish et.al.|[2407.15504v1](http://arxiv.org/abs/2407.15504v1)|null|
|**2024-07-22**|**Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction**|Dingyao Yu et.al.|[2407.15498v1](http://arxiv.org/abs/2407.15498v1)|null|
|**2024-07-22**|**Two Stacks Are Better Than One: A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives**|Zihao Li et.al.|[2407.15489v1](http://arxiv.org/abs/2407.15489v1)|[link](https://github.com/helsinki-nlp/lm-vs-mt)|
|**2024-07-22**|**In-Context Learning Improves Compositional Understanding of Vision-Language Models**|Matteo Nulli et.al.|[2407.15487v1](http://arxiv.org/abs/2407.15487v1)|[link](https://github.com/hoezey/vlm-compositionality)|
|**2024-07-22**|**A Multi-Level Corroborative Approach for Verification and Validation of Autonomous Robotic Swarms**|Dhaminda B. Abeywickrama et.al.|[2407.15475v1](http://arxiv.org/abs/2407.15475v1)|null|
|**2024-07-22**|**Text-to-Battery Recipe: A language modeling-based protocol for automatic battery recipe extraction and retrieval**|Daeun Lee et.al.|[2407.15459v1](http://arxiv.org/abs/2407.15459v1)|null|
|**2024-07-22**|**Developing a Reliable, General-Purpose Hallucination Detection and Mitigation Service: Insights and Lessons Learned**|Song Wang et.al.|[2407.15441v1](http://arxiv.org/abs/2407.15441v1)|null|
|**2024-07-22**|**Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**|Huanjing Zhao et.al.|[2407.15431v1](http://arxiv.org/abs/2407.15431v1)|null|
|**2024-07-22**|**Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation**|Rashi Sharma et.al.|[2407.15428v1](http://arxiv.org/abs/2407.15428v1)|null|
|**2024-07-22**|**YOLO-pdd: A Novel Multi-scale PCB Defect Detection Method Using Deep Representations with Sequential Images**|Bowen Liu et.al.|[2407.15427v1](http://arxiv.org/abs/2407.15427v1)|null|
|**2024-07-22**|**Empirical Capacity Model for Self-Attention Neural Networks**|Aki H√§rm√§ et.al.|[2407.15425v1](http://arxiv.org/abs/2407.15425v1)|null|
|**2024-07-22**|**Integrating IP Broadcasting with Audio Tags: Workflow and Challenges**|Rhys Burchett-Vass et.al.|[2407.15423v2](http://arxiv.org/abs/2407.15423v2)|null|
|**2024-07-22**|**Planning behavior in a recurrent neural network that plays Sokoban**|Adri√† Garriga-Alonso et.al.|[2407.15421v1](http://arxiv.org/abs/2407.15421v1)|[link](https://github.com/alignmentresearch/train-learned-planner)|
|**2024-07-22**|**LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models**|Xi Chen et.al.|[2407.15415v1](http://arxiv.org/abs/2407.15415v1)|[link](https://github.com/openaudiolab/llast)|
|**2024-07-22**|**Knowledge Mechanisms in Large Language Models: A Survey and Perspective**|Mengru Wang et.al.|[2407.15017v1](http://arxiv.org/abs/2407.15017v1)|null|
|**2024-07-22**|**Tackling Selfish Clients in Federated Learning**|Andrea Augello et.al.|[2407.15402v1](http://arxiv.org/abs/2407.15402v1)|[link](https://github.com/ndslab-group/RFL-Self)|
|**2024-07-22**|**Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models**|Xiao Liu et.al.|[2407.15399v1](http://arxiv.org/abs/2407.15399v1)|null|
|**2024-07-22**|**Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation**|Jaehyeong Jeon et.al.|[2407.15396v1](http://arxiv.org/abs/2407.15396v1)|null|
|**2024-07-22**|**ALLaM: Large Language Models for Arabic and English**|M Saiful Bari et.al.|[2407.15390v1](http://arxiv.org/abs/2407.15390v1)|null|
|**2024-07-22**|**The Development of a Comprehensive Spanish Dictionary for Phonetic and Lexical Tagging in Socio-phonetic Research (ESPADA)**|Simon Gonzalez et.al.|[2407.15375v1](http://arxiv.org/abs/2407.15375v1)|null|
|**2024-07-22**|**ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter Posts**|Simon Gonzalez et.al.|[2407.15374v1](http://arxiv.org/abs/2407.15374v1)|null|
|**2024-07-22**|**A Network Analysis Approach to Conlang Research Literature**|Simon Gonzalez et.al.|[2407.15370v1](http://arxiv.org/abs/2407.15370v1)|null|
|**2024-07-22**|**Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias**|Rongwu Xu et.al.|[2407.15366v1](http://arxiv.org/abs/2407.15366v1)|null|
|**2024-07-22**|**A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model**|Yingxue Xu et.al.|[2407.15362v1](http://arxiv.org/abs/2407.15362v1)|null|
|**2024-07-22**|**Dissecting Multiplication in Transformers: Insights into LLMs**|Luyu Qiu et.al.|[2407.15360v1](http://arxiv.org/abs/2407.15360v1)|null|
|**2024-07-22**|**UF-HOBI at "Discharge Me!": A Hybrid Solution for Discharge Summary Generation Through Prompt-based Tuning of GatorTronGPT Models**|Mengxian Lyu et.al.|[2407.15359v1](http://arxiv.org/abs/2407.15359v1)|null|
|**2024-07-22**|**X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images**|Yunpeng Wang et.al.|[2407.15356v1](http://arxiv.org/abs/2407.15356v1)|[link](https://github.com/wangyunpengbio/x-recon)|
|**2024-07-22**|**Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA**|Yuan Pu et.al.|[2407.15353v1](http://arxiv.org/abs/2407.15353v1)|null|
|**2024-07-22**|**MAVEN-Fact: A Large-scale Event Factuality Detection Dataset**|Chunyang Li et.al.|[2407.15352v1](http://arxiv.org/abs/2407.15352v1)|[link](https://github.com/lcy2723/maven-fact)|
|**2024-07-22**|**LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**|Jiaxing Zhang et.al.|[2407.15351v2](http://arxiv.org/abs/2407.15351v2)|null|
|**2024-07-22**|**Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering with Large Language Models**|Wenbin An et.al.|[2407.15346v1](http://arxiv.org/abs/2407.15346v1)|null|
|**2024-07-22**|**Improving Minimum Bayes Risk Decoding with Multi-Prompt**|David Heineman et.al.|[2407.15343v1](http://arxiv.org/abs/2407.15343v1)|null|
|**2024-07-22**|**ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis with Coarse-to-Fine In-context Learning**|Senbin Zhu et.al.|[2407.15341v1](http://arxiv.org/abs/2407.15341v1)|null|
|**2024-07-22**|**Deep Learning for Economists**|Melissa Dell et.al.|[2407.15339v1](http://arxiv.org/abs/2407.15339v1)|null|
|**2024-07-22**|**Robust personalized pricing under uncertainty of purchase probabilities**|Shunnosuke Ikeda et.al.|[2407.15332v1](http://arxiv.org/abs/2407.15332v1)|null|
|**2024-07-22**|**Odyssey: Empowering Agents with Open-World Skills**|Shunyu Liu et.al.|[2407.15325v1](http://arxiv.org/abs/2407.15325v1)|[link](https://github.com/zju-vipa/odyssey)|
|**2024-07-22**|**FMDNN: A Fuzzy-guided Multi-granular Deep Neural Network for Histopathological Image Classification**|Weiping Ding et.al.|[2407.15312v1](http://arxiv.org/abs/2407.15312v1)|null|
|**2024-07-21**|**Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection**|Kwanyong Park et.al.|[2407.15296v1](http://arxiv.org/abs/2407.15296v1)|null|
|**2024-07-21**|**Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis**|Guangliang Liu et.al.|[2407.15286v1](http://arxiv.org/abs/2407.15286v1)|null|
|**2024-07-21**|**Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms**|Sheila Schoepp et.al.|[2407.15283v1](http://arxiv.org/abs/2407.15283v1)|null|
|**2024-07-21**|**SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking**|Kuan-Yen Lin et.al.|[2407.15281v1](http://arxiv.org/abs/2407.15281v1)|[link](https://github.com/irislin1006/cpkl)|
|**2024-07-21**|**Unifying Invariant and Variant Features for Graph Out-of-Distribution via Probability of Necessity and Sufficiency**|Xuexin Chen et.al.|[2407.15273v1](http://arxiv.org/abs/2407.15273v1)|null|
|**2024-07-21**|**Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation**|Liwen Sun et.al.|[2407.15268v1](http://arxiv.org/abs/2407.15268v1)|null|
|**2024-07-21**|**Explaining Decisions of Agents in Mixed-Motive Games**|Maayan Orner et.al.|[2407.15255v1](http://arxiv.org/abs/2407.15255v1)|null|
|**2024-07-21**|**XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models**|Erik Cambria et.al.|[2407.15248v1](http://arxiv.org/abs/2407.15248v1)|null|

#### Abstracts
##### **LLMmap: Fingerprinting For Large Language Models**
2407.15847v1 by Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese

We introduce LLMmap, a first-generation fingerprinting attack targeted at
LLM-integrated applications. LLMmap employs an active fingerprinting approach,
sending carefully crafted queries to the application and analyzing the
responses to identify the specific LLM model in use. With as few as 8
interactions, LLMmap can accurately identify LLMs with over 95% accuracy. More
importantly, LLMmap is designed to be robust across different application
layers, allowing it to identify LLMs operating under various system prompts,
stochastic sampling hyperparameters, and even complex generation frameworks
such as RAG or Chain-of-Thought.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π LLMmapÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈáùÂ∞çÊï¥Âêà LLM ÁöÑÊáâÁî®Á®ãÂºèÊâÄÈÄ≤Ë°åÁöÑÁ¨¨‰∏Ä‰ª£ÊåáÁ¥ãÊîªÊìä„ÄÇLLMmap Êé°Áî®‰∏ªÂãïÂºèÊåáÁ¥ãËæ®Ë≠òÊñπÊ≥ïÔºåÂ∞áÁ≤æÂøÉË®≠Ë®àÁöÑÊü•Ë©¢ÂÇ≥ÈÄÅËá≥ÊáâÁî®Á®ãÂºèÔºå‰∏¶ÂàÜÊûêÂõûÊáâ‰ª•Ë≠òÂà•ÊâÄ‰ΩøÁî®ÁöÑÁâπÂÆö LLM Ê®°Âûã„ÄÇÈÄèÈÅéÂÉÖ 8 Ê¨°‰∫íÂãïÔºåLLMmap ‰æøËÉΩ‰ª•Ë∂ÖÈÅé 95% ÁöÑÊ∫ñÁ¢∫Â∫¶Ê∫ñÁ¢∫Ë≠òÂà• LLM„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåLLMmap Ë¢´Ë®≠Ë®àÁÇ∫ËÉΩÂ§†Âú®‰∏çÂêåÁöÑÊáâÁî®Á®ãÂºèÂ±§Èù¢‰∏≠‰øùÊåÅÁ©©ÂÅ•ÊÄßÔºåÈÄô‰ΩøÂÖ∂ËÉΩÂ§†Ë≠òÂà•Âú®ÂêÑÁ®ÆÁ≥ªÁµ±ÊèêÁ§∫„ÄÅÈö®Ê©üÊäΩÊ®£Ë∂ÖÂèÉÊï∏ÔºåÁîöËá≥ÊòØË§áÈõúÁîüÊàêÊû∂ÊßãÔºà‰æãÂ¶Ç RAG Êàñ Chain-of-ThoughtÔºâ‰∏ãÈÅã‰ΩúÁöÑ LLM„ÄÇ

##### **Reconstructing Training Data From Real World Models Trained with Transfer Learning**
2407.15845v1 by Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim

Current methods for reconstructing training data from trained classifiers are
restricted to very small models, limited training set sizes, and low-resolution
images. Such restrictions hinder their applicability to real-world scenarios.
In this paper, we present a novel approach enabling data reconstruction in
realistic settings for models trained on high-resolution images. Our method
adapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --
specifically, targeting models trained via transfer learning over image
embeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs
data reconstruction in the embedding space rather than in the image space,
showcasing its applicability beyond visual data. Moreover, we introduce a novel
clustering-based method to identify good reconstructions from thousands of
candidates. This significantly improves on previous works that relied on
knowledge of the training set to identify good reconstructed images. Our
findings shed light on a potential privacy risk for data leakage from models
trained using transfer learning.

ÊëòË¶ÅÔºöÁõÆÂâçÁöÑË®ìÁ∑¥Ë≥áÊñôÂæûË®ìÁ∑¥Â•ΩÁöÑÂàÜÈ°ûÂô®‰∏≠ÈáçÂª∫ÊñπÊ≥ïÂÉÖÈôêÊñºÈùûÂ∏∏Â∞èÁöÑÊ®°Âûã„ÄÅÊúâÈôêÁöÑË®ìÁ∑¥ÈõÜÂ§ßÂ∞èÂíå‰ΩéËß£ÊûêÂ∫¶ÁöÑÂΩ±ÂÉè„ÄÇÈÄô‰∫õÈôêÂà∂Â¶®Á§ôÂÖ∂ÈÅ©Áî®ÊñºÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âú®È´òËß£ÊûêÂ∫¶ÂΩ±ÂÉèË®ìÁ∑¥ÁöÑÊ®°Âûã‰∏≠ÈÄ≤Ë°åË≥áÊñôÈáçÂª∫Ôºå‰ª•ÈÅ©ÊáâÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞á arXiv:2206.07758 ÁöÑÈáçÂª∫ÊñπÊ°àË™øÊï¥Âà∞ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠‚Äî‚ÄîÁâπÂà•ÊòØÈáùÂ∞çÈÄèÈÅéÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÔºàÂ¶Ç DINO-ViT Âíå CLIPÔºâÁöÑÂΩ±ÂÉèÂµåÂÖ•ÈÄ≤Ë°åÈÅ∑ÁßªÂ≠∏ÁøíË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂµåÂÖ•Á©∫Èñì‰∏≠ÈÄ≤Ë°åË≥áÊñôÈáçÂª∫ÔºåËÄå‰∏çÂú®ÂΩ±ÂÉèÁ©∫Èñì‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Ë¶ñË¶∫Ë≥áÊñô‰πãÂ§ñÁöÑÈÅ©Áî®ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫ÊñºÁæ§ÈõÜÁöÑÊñπÊ≥ïÔºåÂæûÊï∏ÂçÉÂÄãÂÄôÈÅ∏ËÄÖ‰∏≠ÊâæÂá∫ËâØÂ•ΩÁöÑÈáçÂª∫„ÄÇÈÄôÈ°ØËëóÊîπÂñÑ‰∫Ü‰ª•ÂæÄ‰æùË≥¥Ë®ìÁ∑¥ÈõÜÁü•Ë≠ò‰æÜÊâæÂá∫ËâØÂ•ΩÈáçÂª∫ÂΩ±ÂÉèÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫Ü‰ΩøÁî®ÈÅ∑ÁßªÂ≠∏ÁøíË®ìÁ∑¥ÁöÑÊ®°Âûã‰∏≠Ë≥áÊñôÂ§ñÊ¥©ÁöÑÊΩõÂú®Èö±ÁßÅÈ¢®Èö™„ÄÇ

##### **CarFormer: Self-Driving with Learned Object-Centric Representations**
2407.15843v1 by Shadi Hamdan, Fatma G√ºney

The choice of representation plays a key role in self-driving. Bird's eye
view (BEV) representations have shown remarkable performance in recent years.
In this paper, we propose to learn object-centric representations in BEV to
distill a complex scene into more actionable information for self-driving. We
first learn to place objects into slots with a slot attention model on BEV
sequences. Based on these object-centric representations, we then train a
transformer to learn to drive as well as reason about the future of other
vehicles. We found that object-centric slot representations outperform both
scene-level and object-level approaches that use the exact attributes of
objects. Slot representations naturally incorporate information about objects
from their spatial and temporal context such as position, heading, and speed
without explicitly providing it. Our model with slots achieves an increased
completion rate of the provided routes and, consequently, a higher driving
score, with a lower variance across multiple runs, affirming slots as a
reliable alternative in object-centric approaches. Additionally, we validate
our model's performance as a world model through forecasting experiments,
demonstrating its capability to predict future slot representations accurately.
The code and the pre-trained models can be found at
https://kuis-ai.github.io/CarFormer/.

ÊëòË¶ÅÔºöË°®Á§∫ÁöÑÈÅ∏ÊìáÂú®Ëá™ÂãïÈßïÈßõ‰∏≠ÊâÆÊºîËëóÈóúÈçµÁöÑËßíËâ≤„ÄÇÈ≥•Áû∞Âúñ (BEV) Ë°®Á§∫Âú®ËøëÂπ¥‰æÜÂ±ïÁèæ‰∫ÜÂçìË∂äÁöÑË°®Áèæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Â≠∏Áøí BEV ‰∏≠‰ª•Áâ©È´îÁÇ∫‰∏≠ÂøÉÁöÑË°®Á§∫Ôºå‰ª•Â∞áË§áÈõúÁöÑÂ†¥ÊôØÊèêÁÖâÊàêÊõ¥ÂÖ∑ÂèØÊìç‰ΩúÊÄßÁöÑË≥áË®äÔºåÁî®ÊñºËá™ÂãïÈßïÈßõ„ÄÇÊàëÂÄëÈ¶ñÂÖàÂ≠∏Áøí‰ΩøÁî® BEV Â∫èÂàó‰∏äÁöÑÊèíÊßΩÊ≥®ÊÑèÂäõÊ®°ÂûãÂ∞áÁâ©È´îÊîæÂÖ•ÊèíÊßΩ‰∏≠„ÄÇÊ†πÊìöÈÄô‰∫õ‰ª•Áâ©È´îÁÇ∫‰∏≠ÂøÉÁöÑË°®Á§∫ÔºåÊàëÂÄëÊé•ËëóË®ìÁ∑¥‰∏ÄÂÄãËΩâÊèõÂô®‰æÜÂ≠∏ÁøíÈßïÈßõÔºå‰ª•ÂèäÊé®Ë´ñÂÖ∂‰ªñËªäËºõÁöÑÊú™‰æÜ„ÄÇÊàëÂÄëÁôºÁèæ‰ª•Áâ©È´îÁÇ∫‰∏≠ÂøÉÁöÑÊèíÊßΩË°®Á§∫ÂÑ™Êñº‰ΩøÁî®Áâ©È´îÁ¢∫ÂàáÂ±¨ÊÄßÁöÑÂ†¥ÊôØÁ¥öÂà•ÂíåÁâ©È´îÁ¥öÂà•ÊñπÊ≥ï„ÄÇÊèíÊßΩË°®Á§∫Ëá™ÁÑ∂Âú∞ÁµêÂêà‰∫Ü‰æÜËá™Áâ©È´îÁ©∫ÈñìÂíåÊôÇÈñìËÑàÁµ°ÁöÑË≥áË®äÔºå‰æãÂ¶Ç‰ΩçÁΩÆ„ÄÅËà™ÂêëÂíåÈÄüÂ∫¶ÔºåËÄåÁÑ°ÈúÄÊòéÁ¢∫Êèê‰æõ„ÄÇÊàëÂÄëÂ∏∂ÊúâÊèíÊßΩÁöÑÊ®°ÂûãÊèêÈ´ò‰∫ÜÊâÄÊèê‰æõË∑ØÁ∑öÁöÑÂÆåÊàêÁéáÔºåÂõ†Ê≠§ÊèêÈ´ò‰∫ÜÈßïÈßõÂàÜÊï∏Ôºå‰∏¶‰∏îÂú®Â§öÊ¨°Âü∑Ë°å‰∏≠ÁöÑÂ∑ÆÁï∞ËºÉ‰ΩéÔºåË≠âÂØ¶ÊèíÊßΩÊòØ‰ª•Áâ©È´îÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ï‰∏≠ÂèØÈù†ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÈ†êÊ∏¨ÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊ®°Âûã‰ΩúÁÇ∫‰∏ñÁïåÊ®°ÂûãÁöÑÊïàËÉΩÔºåË≠âÊòé‰∫ÜÂÖ∂Ê∫ñÁ¢∫È†êÊ∏¨Êú™‰æÜÊèíÊßΩË°®Á§∫ÁöÑËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂíåÈ†êË®ìÁ∑¥Ê®°ÂûãÂèØ‰ª•Âú® https://kuis-ai.github.io/CarFormer/ ÊâæÂà∞„ÄÇ

##### **Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments**
2407.15839v1 by Mansur Arief, Mike Timmerman, Jiachen Li, David Isele, Mykel J Kochenderfer

Training intelligent agents to navigate highly interactive environments
presents significant challenges. While guided meta reinforcement learning (RL)
approach that first trains a guiding policy to train the ego agent has proven
effective in improving generalizability across various levels of interaction,
the state-of-the-art method tends to be overly sensitive to extreme cases,
impairing the agents' performance in the more common scenarios. This study
introduces a novel training framework that integrates guided meta RL with
importance sampling (IS) to optimize training distributions for navigating
highly interactive driving scenarios, such as T-intersections. Unlike
traditional methods that may underrepresent critical interactions or
overemphasize extreme cases during training, our approach strategically adjusts
the training distribution towards more challenging driving behaviors using IS
proposal distributions and applies the importance ratio to de-bias the result.
By estimating a naturalistic distribution from real-world datasets and
employing a mixture model for iterative training refinements, the framework
ensures a balanced focus across common and extreme driving scenarios.
Experiments conducted with both synthetic dataset and T-intersection scenarios
from the InD dataset demonstrate not only accelerated training but also
improvement in agent performance under naturalistic conditions, showcasing the
efficacy of combining IS with meta RL in training reliable autonomous agents
for highly interactive navigation tasks.

ÊëòË¶ÅÔºöË®ìÁ∑¥Êô∫ËÉΩ‰ª£ÁêÜÁ®ãÂºèÂú®È´òÂ∫¶‰∫íÂãïÁöÑÁí∞Â¢É‰∏≠Â∞éËà™ÔºåÊúÉÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÈõñÁÑ∂ÂºïÂ∞éÂÖÉÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÂÖàË®ìÁ∑¥ÂºïÂ∞éÊîøÁ≠ñ‰æÜË®ìÁ∑¥Ëá™Êàë‰ª£ÁêÜÁ®ãÂºèÔºåÂ∑≤Ë¢´Ë≠âÊòéÂèØ‰ª•ÊúâÊïàÊèêÂçáÂú®‰∏çÂêå‰∫íÂãïÂ±§Á¥ö‰πãÈñìÁöÑÊ≥õÂåñÊÄßÔºå‰ΩÜÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÂæÄÂæÄÂ∞çÊ•µÁ´ØÊ°à‰æãÈÅéÂ∫¶ÊïèÊÑüÔºåÊêçÂÆ≥‰ª£ÁêÜÁ®ãÂºèÂú®Êõ¥Â∏∏Ë¶ãÂ†¥ÊôØ‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑË®ìÁ∑¥Êû∂ÊßãÔºåÂ∞áÂºïÂ∞éÂÖÉ RL ËàáÈáçË¶ÅÊÄßÊäΩÊ®£ (IS) Êï¥ÂêàÔºå‰ª•ÊúÄ‰Ω≥ÂåñË®ìÁ∑¥ÂàÜ‰ΩàÔºåÁî®ÊñºÂ∞éËà™È´òÂ∫¶‰∫íÂãïÁöÑÈßïÈßõÂ†¥ÊôØÔºå‰æãÂ¶Ç T ÂûãË∑ØÂè£„ÄÇËàáÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåÂÇ≥Áµ±ÊñπÊ≥ïÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÂèØËÉΩ‰Ωé‰º∞ÈóúÈçµ‰∫íÂãïÊàñÈÅéÂ∫¶Âº∑Ë™øÊ•µÁ´ØÊ°à‰æãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁ≠ñÁï•ÊÄßÂú∞Ë™øÊï¥Ë®ìÁ∑¥ÂàÜ‰ΩàÔºå‰ΩøÁî® IS ÊèêË≠∞ÂàÜ‰ΩàÊúùÂêëÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÁöÑÈßïÈßõË°åÁÇ∫Ôºå‰∏¶Â•óÁî®ÈáçË¶ÅÊÄßÊØîÁéá‰æÜÊ∂àÈô§ÁµêÊûúÂÅèÂ∑Æ„ÄÇÈÄèÈÅéÂæûÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰º∞Ë®àËá™ÁÑ∂ÂàÜ‰ΩàÔºå‰∏¶Êé°Áî®Ê∑∑ÂêàÊ®°ÂûãÈÄ≤Ë°åÂèçË¶ÜË®ìÁ∑¥Ë™øÊï¥ÔºåË©≤Êû∂ÊßãÁ¢∫‰øùÂú®Â∏∏Ë¶ãÂíåÊ•µÁ´ØÈßïÈßõÂ†¥ÊôØ‰πãÈñìÂèñÂæóÂπ≥Ë°°ÁöÑÈóúÊ≥®„ÄÇ‰ΩøÁî®ÂêàÊàêË≥áÊñôÈõÜÂíå InD Ë≥áÊñôÈõÜ‰∏≠ÁöÑ T ÂûãË∑ØÂè£Â†¥ÊôØÈÄ≤Ë°åÁöÑÂØ¶È©óÔºå‰∏çÂÉÖË≠âÊòé‰∫ÜË®ìÁ∑¥Âä†ÈÄüÔºå‰πüË≠âÊòé‰∫ÜÂú®Ëá™ÁÑ∂Ê¢ù‰ª∂‰∏ã‰ª£ÁêÜÁ®ãÂºèÊïàËÉΩÁöÑÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂú®Ë®ìÁ∑¥ÂèØÈù†ÁöÑËá™‰∏ª‰ª£ÁêÜÁ®ãÂºè‰ª•Âü∑Ë°åÈ´òÂ∫¶‰∫íÂãïÂ∞éËà™‰ªªÂãôÊôÇÔºåÁµêÂêà IS ÂíåÂÖÉ RL ÁöÑÊïàËÉΩ„ÄÇ

##### **Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning**
2407.15837v1 by Yibing Wei, Abhinav Gupta, Pedro Morgado

Masked Image Modeling (MIM) has emerged as a promising method for deriving
visual representations from unlabeled image data by predicting missing pixels
from masked portions of images. It excels in region-aware learning and provides
strong initializations for various tasks, but struggles to capture high-level
semantics without further supervised fine-tuning, likely due to the low-level
nature of its pixel reconstruction objective. A promising yet unrealized
framework is learning representations through masked reconstruction in latent
space, combining the locality of MIM with the high-level targets. However, this
approach poses significant training challenges as the reconstruction targets
are learned in conjunction with the model, potentially leading to trivial or
suboptimal solutions.Our study is among the first to thoroughly analyze and
address the challenges of such framework, which we refer to as Latent MIM.
Through a series of carefully designed experiments and extensive analysis, we
identify the source of these challenges, including representation collapsing
for joint online/target optimization, learning objectives, the high region
correlation in latent space and decoding conditioning. By sequentially
addressing these issues, we demonstrate that Latent MIM can indeed learn
high-level representations while retaining the benefits of MIM models.

ÊëòË¶ÅÔºöÈÅÆËîΩÂΩ±ÂÉèÂª∫Ê®° (MIM) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÂèØÁî®ÊñºÈÄèÈÅéÈ†êÊ∏¨ÂΩ±ÂÉèÈÅÆËîΩÈÉ®ÂàÜÁöÑÈÅ∫Â§±ÂÉèÁ¥†ÔºåÂæûÊú™Ê®ôË®òÁöÑÂΩ±ÂÉèË≥áÊñô‰∏≠Ë°çÁîüË¶ñË¶∫Ë°®Á§∫„ÄÇÂÆÉÂú®ÂçÄÂüüÊÑüÁü•Â≠∏ÁøíÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏¶ÁÇ∫ÂêÑÁ®Æ‰ªªÂãôÊèê‰æõÂº∑Â§ßÁöÑÂàùÂßãË®≠ÂÆöÔºå‰ΩÜÂçªÈõ£‰ª•Âú®Ê≤íÊúâÈÄ≤‰∏ÄÊ≠•Áõ£Áù£ÂæÆË™øÁöÑÊÉÖÊ≥Å‰∏ãÊçïÊçâÂà∞È´òÂ±§Ê¨°Ë™ûÊÑèÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÂÖ∂ÂÉèÁ¥†ÈáçÂª∫ÁõÆÊ®ôÁöÑ‰ΩéÂ±§Ê¨°Êú¨Ë≥™ÊâÄËá¥„ÄÇ‰∏ÄÂÄãÊúâÂâçÈÄî‰ΩÜÂ∞öÊú™ÂØ¶ÁèæÁöÑÊû∂ÊßãÊòØÈÄèÈÅéÊΩõÂú®Á©∫Èñì‰∏≠ÁöÑÈÅÆËîΩÈáçÂª∫‰æÜÂ≠∏ÁøíË°®Á§∫ÔºåÁµêÂêà MIM ÁöÑÂ±ÄÈÉ®ÊÄßËàáÈ´òÂ±§Ê¨°ÁõÆÊ®ô„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÊúÉÂ∏∂‰æÜÈáçÂ§ßÁöÑË®ìÁ∑¥ÊåëÊà∞ÔºåÂõ†ÁÇ∫ÈáçÂª∫ÁõÆÊ®ôÊòØËàáÊ®°ÂûãÁµêÂêàÂ≠∏ÁøíÁöÑÔºåÂèØËÉΩÊúÉÂ∞éËá¥ÂæÆ‰∏çË∂≥ÈÅìÊàñÊ¨°‰Ω≥ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊòØÁ¨¨‰∏ÄÊâπÂæπÂ∫ïÂàÜÊûêÂíåËß£Ê±∫Ê≠§È°ûÊû∂ÊßãÊåëÊà∞ÁöÑÁ†îÁ©∂‰πã‰∏ÄÔºåÊàëÂÄëÁ®±‰πãÁÇ∫ÊΩõÂú® MIM„ÄÇÈÄèÈÅé‰∏ÄÁ≥ªÂàóÁ≤æÂøÉË®≠Ë®àÁöÑÂØ¶È©óÂíåÂª£Ê≥õÁöÑÂàÜÊûêÔºåÊàëÂÄëÊâæÂá∫ÈÄô‰∫õÊåëÊà∞ÁöÑÊ†πÊ∫êÔºåÂåÖÊã¨ËÅØÂêàÁ∑ö‰∏ä/ÁõÆÊ®ôÊúÄ‰Ω≥Âåñ„ÄÅÂ≠∏ÁøíÁõÆÊ®ô„ÄÅÊΩõÂú®Á©∫Èñì‰∏≠ÁöÑÈ´òÂçÄÂüüÁõ∏ÈóúÊÄßÂíåËß£Á¢ºÊ¢ù‰ª∂ÁöÑË°®Á§∫Â¥©ÊΩ∞„ÄÇÈÄèÈÅéÂæ™Â∫èÊº∏ÈÄ≤Âú∞Ëß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëË≠âÊòéÊΩõÂú® MIM Á¢∫ÂØ¶ÂèØ‰ª•Âú®‰øùÁïô MIM Ê®°ÂûãÂÑ™ÈªûÁöÑÂêåÊôÇÂ≠∏ÁøíÈ´òÂ±§Ê¨°Ë°®Á§∫„ÄÇ

##### **dMel: Speech Tokenization made Simple**
2407.15835v1 by He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, Navdeep Jaitly

Large language models have revolutionized natural language processing by
leveraging self-supervised pretraining on vast textual data. Inspired by this
success, researchers have investigated complicated speech tokenization methods
to discretize continuous speech signals so that language modeling techniques
can be applied to speech data. However, existing approaches either model
semantic tokens, potentially losing acoustic information, or model acoustic
tokens, risking the loss of semantic information. Having multiple token types
also complicates the architecture and requires additional pretraining. Here we
show that discretizing mel-filterbank channels into discrete intensity bins
produces a simple representation (dMel), that performs better than other
existing speech tokenization methods. Using a transformer decoder-only
architecture for speech-text modeling, we comprehensively evaluate different
speech tokenization methods on speech recognition (ASR), speech synthesis
(TTS). Our results demonstrate the effectiveness of dMel in achieving high
performance on both tasks within a unified framework, paving the way for
efficient and effective joint modeling of speech and text.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄèÈÅéÂú®Â§ßÈáèÁöÑÊñáÊú¨Ë≥áÊñô‰∏äÂà©Áî®Ëá™ÊàëÁõ£Áù£È†êË®ìÁ∑¥ÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ„ÄÇÂèóÂà∞ÈÄôÈ†ÖÊàêÂäüÁöÑÂïüÁôºÔºåÁ†îÁ©∂‰∫∫Âì°Ë™øÊü•‰∫ÜË§áÈõúÁöÑË™ûÈü≥Ê®ôË®òÂåñÊñπÊ≥ïÔºå‰ª•‰æøÂ∞áÈÄ£Á∫åÁöÑË™ûÈü≥Ë®äËôüÈõ¢Êï£ÂåñÔºåËÆìË™ûË®ÄÂª∫Ê®°ÊäÄË°ìÂèØ‰ª•ÊáâÁî®ÊñºË™ûÈü≥Ë≥áÊñô„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ï‰∏çÊòØÂ∞çË™ûÁæ©Ê®ôË®òÂª∫Ê®°ÔºàÂèØËÉΩÈÅ∫Â§±Èü≥Ë®äË≥áË®äÔºâÔºåÂ∞±ÊòØÂ∞çÈü≥Ë®äÊ®ôË®òÂª∫Ê®°ÔºàÊúâÈÅ∫Â§±Ë™ûÁæ©Ë≥áË®äÁöÑÈ¢®Èö™Ôºâ„ÄÇÊìÅÊúâÂ§öÁ®ÆÈ°ûÂûãÁöÑÊ®ôË®ò‰πüÊúÉ‰ΩøÊû∂ÊßãË§áÈõúÂåñÔºå‰∏¶ÈúÄË¶ÅÈ°çÂ§ñÁöÑÈ†êË®ìÁ∑¥„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ±ïÁ§∫Â∞áÊ¢ÖÁàæÊøæÊ≥¢Âô®ÁµÑÈÄöÈÅìÈõ¢Êï£ÂåñÁÇ∫Èõ¢Êï£Âº∑Â∫¶ÂçÄÂ°äÔºåÊúÉÁî¢Áîü‰∏ÄÂÄãÁ∞°ÂñÆÁöÑË°®Á§∫ÔºàdMelÔºâÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÂÖ∂‰ªñÁèæÊúâÁöÑË™ûÈü≥Ê®ôË®òÂåñÊñπÊ≥ï„ÄÇÊàëÂÄë‰ΩøÁî®ÂÉÖÂÖ∑ËΩâÊèõÂô®Ëß£Á¢ºÂô®ÁöÑÊû∂ÊßãÈÄ≤Ë°åË™ûÈü≥ËΩâÊñáÂ≠óÂª∫Ê®°ÔºåÂú®Ë™ûÈü≥Ëæ®Ë≠ò (ASR)„ÄÅË™ûÈü≥ÂêàÊàê (TTS) ‰∏äÂÖ®Èù¢Ë©ï‰º∞‰∏çÂêåÁöÑË™ûÈü≥Ê®ôË®òÂåñÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫Ü dMel Âú®Áµ±‰∏ÄÊû∂Êßã‰∏≠ÈÅîÊàêÈÄôÂÖ©ÂÄã‰ªªÂãôÁöÑÈ´òÊïàËÉΩÔºåÁÇ∫ÊúâÊïàÁéá‰∏îÊúâÊïàÁöÑË™ûÈü≥ÂíåÊñáÂ≠óËÅØÂêàÂª∫Ê®°Èã™Ë∑Ø„ÄÇ

##### **NV-Retriever: Improving text embedding models with effective hard-negative mining**
2407.15831v1 by Gabriel de Souza P. Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, Even Oldridge

Text embedding models have been popular for information retrieval
applications such as semantic search and Question-Answering systems based on
Retrieval-Augmented Generation (RAG). Those models are typically Transformer
models that are fine-tuned with contrastive learning objectives. Many papers
introduced new embedding model architectures and training approaches, however,
one of the key ingredients, the process of mining negative passages, remains
poorly explored or described. One of the challenging aspects of fine-tuning
embedding models is the selection of high quality hard-negative passages for
contrastive learning. In this paper we propose a family of positive-aware
mining methods that leverage the positive relevance score for more effective
false negatives removal. We also provide a comprehensive ablation study on
hard-negative mining methods over their configurations, exploring different
teacher and base models. We demonstrate the efficacy of our proposed methods by
introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval
(BEIR) benchmark and 0.65 points higher than previous methods. The model placed
1st when it was published to MTEB Retrieval on July 07, 2024.

ÊëòË¶ÅÔºöÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÂú®‰ø°ÊÅØÊ£ÄÁ¥¢Â∫îÁî®‰∏≠ÂæàÂèóÊ¨¢ËøéÔºå‰æãÂ¶ÇÂü∫‰∫éÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) ÁöÑËØ≠‰πâÊêúÁ¥¢ÂíåÈóÆÁ≠îÁ≥ªÁªü„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÊòØÁªèËøáÂØπÊØîÂ≠¶‰π†ÁõÆÊ†áÂæÆË∞ÉÁöÑ Transformer Ê®°Âûã„ÄÇËÆ∏Â§öËÆ∫Êñá‰ªãÁªç‰∫ÜÊñ∞ÁöÑÂµåÂÖ•Ê®°ÂûãÊû∂ÊûÑÂíåËÆ≠ÁªÉÊñπÊ≥ïÔºåÁÑ∂ËÄåÔºåÂÖ∂‰∏≠‰∏Ä‰∏™ÂÖ≥ÈîÆË¶ÅÁ¥†ÔºåÂç≥ÊåñÊéòË¥üÈù¢ÊÆµËêΩÁöÑËøáÁ®ãÔºå‰ªçÁÑ∂Êé¢Á¥¢ÊàñÊèèËø∞ÂæóÂæàÂ∑Æ„ÄÇÂæÆË∞ÉÂµåÂÖ•Ê®°ÂûãÁöÑ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊñπÈù¢ÊòØ‰∏∫ÂØπÊØîÂ≠¶‰π†ÈÄâÊã©È´òË¥®ÈáèÁöÑÂõ∞ÈöæË¥üÈù¢ÊÆµËêΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁßØÊûÅÊÑüÁü•ÊåñÊéòÊñπÊ≥ïÔºåÂà©Áî®ÁßØÊûÅÁõ∏ÂÖ≥ÊÄßÂàÜÊï∞Êõ¥ÊúâÊïàÂú∞ÂéªÈô§ÂÅáÈò¥ÊÄß„ÄÇÊàë‰ª¨ËøòÂØπÂõ∞ÈöæË¥üÈù¢ÊåñÊéòÊñπÊ≥ïÂèäÂÖ∂ÈÖçÁΩÆËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑÊïôÂ∏àÂíåÂü∫Á°ÄÊ®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ• NV-Retriever-v1 Ê®°ÂûãÊù•ËØÅÊòéÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåËØ•Ê®°ÂûãÂú® MTEB Ê£ÄÁ¥¢ (BEIR) Âü∫ÂáÜÊµãËØï‰∏≠ÂæóÂàÜ 60.9ÔºåÊØî‰ª•ÂâçÁöÑÊñπÊ≥ïÈ´ò 0.65 ÂàÜ„ÄÇËØ•Ê®°ÂûãÂú® 2024 Âπ¥ 7 Êúà 07 Êó•ÂèëÂ∏ÉÂà∞ MTEB Ê£ÄÁ¥¢Êó∂ÊéíÂêçÁ¨¨‰∏Ä„ÄÇ

##### **J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling**
2407.15828v1 by Wataru Nakata, Kentaro Seki, Hitomi Yanaka, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari

Spoken dialogue plays a crucial role in human-AI interactions, necessitating
dialogue-oriented spoken language models (SLMs). To develop versatile SLMs,
large-scale and diverse speech datasets are essential. Additionally, to ensure
hiqh-quality speech generation, the data must be spontaneous like in-wild data
and must be acoustically clean with noise removed. Despite the critical need,
no open-source corpus meeting all these criteria has been available. This study
addresses this gap by constructing and releasing a large-scale spoken dialogue
corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly
accessible. Furthermore, this paper presents a language-independent method for
corpus construction and describes experiments on dialogue generation using SLMs
trained on J-CHAT. Experimental results indicate that the collected data from
multiple domains by our method improve the naturalness and meaningfulness of
dialogue generation.

ÊëòË¶ÅÔºöÂè£Ë™™Â∞çË©±Âú®‰∫∫Ê©ü‰∫íÂãï‰∏≠ÊâÆÊºîËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ∞çË©±Â∞éÂêëÁöÑÂè£Ë™™Ë™ûË®ÄÊ®°Âûã (SLM)„ÄÇÁÇ∫‰∫ÜÈñãÁôºÈÄöÁî®ÁöÑ SLMÔºåÂ§ßË¶èÊ®°‰∏îÂ§öÊ®£ÁöÑË™ûÈü≥Ë≥áÊñôÈõÜËá≥ÈóúÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÁ¢∫‰øùÈ´òÂìÅË≥™ÁöÑË™ûÈü≥ÁîüÊàêÔºåË≥áÊñôÂøÖÈ†àÂÉèÈáéÂ§ñË≥áÊñô‰∏ÄÊ®£Ëá™ÁÑ∂Ôºå‰∏îÂøÖÈ†àÂú®ÂéªÈô§ÈõúË®äÂæåÈÄ≤Ë°åËÅ≤Â≠∏Ê∏ÖÁêÜ„ÄÇÂÑòÁÆ°ÊúâÊ≠§ÈóúÈçµÈúÄÊ±ÇÔºå‰ΩÜÈÇÑÊ≤íÊúâ‰ªª‰ΩïÈñãÊ∫êË™ûÊñôÂ∫´Á¨¶ÂêàÊâÄÊúâÈÄô‰∫õÊ¢ù‰ª∂„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÂª∫ÊßãÂíåÈáãÂá∫‰∏ÄÂÄãÂêçÁÇ∫„Äå‰∫∫Ê©üÂ∞çË©±Êó•Ë™ûË™ûÊñôÂ∫´„Äç(J-CHAT) ÁöÑÂ§ßË¶èÊ®°Âè£Ë™™Â∞çË©±Ë™ûÊñôÂ∫´‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåË©≤Ë™ûÊñôÂ∫´ÂèØ‰æõÂÖ¨Áúæ‰ΩøÁî®„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËàáË™ûË®ÄÁÑ°ÈóúÁöÑË™ûÊñôÂ∫´Âª∫ÊßãÊñπÊ≥ïÔºå‰∏¶Ë™™Êòé‰∫Ü‰ΩøÁî®Âú® J-CHAT ‰∏äË®ìÁ∑¥ÁöÑ SLM ÈÄ≤Ë°åÂ∞çË©±ÁîüÊàêÁöÑÂØ¶È©ó„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÂæûÂ§öÂÄãÈ†òÂüüÊî∂ÈõÜÁöÑË≥áÊñôÊîπÂñÑ‰∫ÜÂ∞çË©±ÁîüÊàêÁöÑËá™ÁÑ∂ÊÄßÂíåÊÑèÁæ©ÊÄß„ÄÇ

##### **Perceptions of Linguistic Uncertainty by Language Models and Humans**
2407.15814v1 by Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth

Uncertainty expressions such as ``probably'' or ``highly unlikely'' are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans interpret these expressions,
there has been little inquiry into the abilities of language models to
interpret such expressions. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We evaluate
both humans and 10 popular language models on a task created to assess these
abilities. Unexpectedly, we find that 8 out of 10 models are able to map
uncertainty expressions to probabilistic responses in a human-like manner.
However, we observe systematically different behavior depending on whether a
statement is actually true or false. This sensitivity indicates that language
models are substantially more susceptible to bias based on their prior
knowledge (as compared to humans). These findings raise important questions and
have broad implications for human-AI alignment and AI-AI communication.

ÊëòË¶ÅÔºö‰∏çÁ¢∫ÂÆöÊÄßË°®ÈÅîÂºèÔºå‰æãÂ¶Ç„ÄåÂèØËÉΩ„ÄçÊàñ„ÄåÊ•µ‰∏çÂèØËÉΩ„ÄçÔºåÂú®‰∫∫È°ûË™ûË®Ä‰∏≠ÊôÆÈÅçÂ≠òÂú®„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂ∑•‰ΩúÂ∑≤Á∂ìÁ¢∫ÂÆöÔºåÂú®‰∫∫È°ûÂ¶Ç‰ΩïË©ÆÈáãÈÄô‰∫õË°®ÈÅîÂºèÊñπÈù¢Â≠òÂú®Ëëó‰∫∫Âè£Â±§Á¥öÁöÑÂÖ±Ë≠òÔºå‰ΩÜÂ∞çÊñºË™ûË®ÄÊ®°ÂûãË©ÆÈáãÊ≠§È°ûË°®ÈÅîÂºèÁöÑËÉΩÂäõÔºåÂçªÈÆÆÂ∞ëÊúâÊé¢Ë®é„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éË™ûË®ÄÊ®°ÂûãÂ¶Ç‰ΩïÂ∞á‰∏çÁ¢∫ÂÆöÊÄßÁöÑË™ûË®ÄË°®ÈÅîÂºèÂ∞çÊáâÂà∞Êï∏ÂÄºÂõûÊáâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË©ï‰º∞Ë™ûË®ÄÊ®°ÂûãÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÊòØÂê¶ËÉΩÈÅãÁî®ÂøÉÊô∫ÁêÜË´ñÔºö‰∫ÜËß£Âè¶‰∏ÄÂÄã‰∏ªÈ´îÂ∞çÁâπÂÆöÈô≥Ëø∞ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºå‰∏îËàáÊ®°ÂûãÂ∞çË©≤Èô≥Ëø∞Êú¨Ë∫´ÁöÑÁ¢∫ÂÆöÊÄßÁÑ°Èóú„ÄÇÊàëÂÄëÈáùÂ∞ç‰∫∫È°ûÂíå 10 ÂÄãÊµÅË°åÁöÑË™ûË®ÄÊ®°ÂûãÔºåË©ï‰º∞‰∏ÄÈ†ÖÊó®Âú®Ë©ïÈáèÈÄô‰∫õËÉΩÂäõÁöÑ‰ªªÂãô„ÄÇÂá∫‰πéÊÑèÊñôÂú∞ÔºåÊàëÂÄëÁôºÁèæ 10 ÂÄãÊ®°Âûã‰∏≠Êúâ 8 ÂÄãËÉΩÂ§†‰ª•È°û‰ºº‰∫∫È°ûÁöÑÊñπÂºèÔºåÂ∞á‰∏çÁ¢∫ÂÆöÊÄßË°®ÈÅîÂºèÂ∞çÊáâÂà∞Ê©üÁéáÂõûÊáâ„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëËßÄÂØüÂà∞ÔºåË¶ñÈô≥Ëø∞ÂØ¶ÈöõÁÇ∫ÁúüÊàñÂÅáÔºåËÄåÊúâÁ≥ªÁµ±ÊÄßÁöÑ‰∏çÂêåË°åÁÇ∫„ÄÇÈÄôÁ®ÆÊïèÊÑüÊÄßË°®ÊòéÔºåË™ûË®ÄÊ®°ÂûãÊ•µÂÆπÊòìÂèóÂà∞Âü∫ÊñºÂÖ∂ÂÖàÈ©óÁü•Ë≠òÁöÑÂÅèÂ∑ÆÂΩ±ÈüøÔºàÁõ∏ËºÉÊñº‰∫∫È°ûÔºâ„ÄÇÈÄô‰∫õÁôºÁèæÊèêÂá∫‰∫ÜÈáçË¶ÅÁöÑÂïèÈ°åÔºå‰∏¶Â∞ç‰∫∫È°ûËàá AI ÁöÑÂçîË™øÔºå‰ª•Âèä AI Ëàá AI ‰πãÈñìÁöÑÊ∫ùÈÄöÔºåÁî¢ÁîüÂª£Ê≥õÁöÑÂΩ±Èüø„ÄÇ

##### **Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget**
2407.15811v1 by Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, Lingjuan Lyu

As scaling laws in generative AI push performance, they also simultaneously
concentrate the development of these models among actors with large
computational resources. With a focus on text-to-image (T2I) generative models,
we aim to address this bottleneck by demonstrating very low-cost training of
large-scale T2I diffusion transformer models. As the computational cost of
transformers increases with the number of patches in each image, we propose to
randomly mask up to 75% of the image patches during training. We propose a
deferred masking strategy that preprocesses all patches using a patch-mixer
before masking, thus significantly reducing the performance degradation with
masking, making it superior to model downscaling in reducing computational
cost. We also incorporate the latest improvements in transformer architecture,
such as the use of mixture-of-experts layers, to improve performance and
further identify the critical benefit of using synthetic images in micro-budget
training. Finally, using only 37M publicly available real and synthetic images,
we train a 1.16 billion parameter sparse transformer with only \$1,890
economical cost and achieve a 12.7 FID in zero-shot generation on the COCO
dataset. Notably, our model achieves competitive FID and high-quality
generations while incurring 118$\times$ lower cost than stable diffusion models
and 14$\times$ lower cost than the current state-of-the-art approach that costs
\$28,400. We aim to release our end-to-end training pipeline to further
democratize the training of large-scale diffusion models on micro-budgets.

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÁîüÊàêÂºè AI ‰∏≠ÁöÑÁ∏ÆÊîæÂÆöÂæãÊèêÂçáÊïàËÉΩÔºåÂÆÉÂÄë‰πüÂêåÊôÇÂ∞áÈÄô‰∫õÊ®°ÂûãÁöÑÈñãÁôºÈõÜ‰∏≠Âú®ÊìÅÊúâÈæêÂ§ßÈÅãÁÆóË≥áÊ∫êÁöÑÂèÉËàáËÄÖ‰πãÈñì„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÊñáÂ≠óËΩâÂΩ±ÂÉè (T2I) ÁîüÊàêÂºèÊ®°ÂûãÔºåÊó®Âú®ÈÄèÈÅéÂ±ïÁ§∫Â§ßË¶èÊ®° T2I Êì¥Êï£TransformerÊ®°ÂûãÁöÑÊ•µ‰ΩéÊàêÊú¨Ë®ìÁ∑¥‰æÜËß£Ê±∫ÈÄôÂÄãÁì∂È†∏„ÄÇÁî±ÊñºTransformerÁöÑÈÅãÁÆóÊàêÊú¨ÊúÉÈö®ËëóÊØèÂÄãÂΩ±ÂÉè‰∏≠ÁöÑË≤ºÁâáÊï∏ÈáèËÄåÂ¢ûÂä†ÔºåÊàëÂÄëÂª∫Ë≠∞Âú®Ë®ìÁ∑¥ÊúüÈñìÈö®Ê©üÈÅÆËîΩÊúÄÂ§ö 75% ÁöÑÂΩ±ÂÉèË≤ºÁâá„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂª∂ÈÅ≤ÈÅÆËîΩÁ≠ñÁï•ÔºåÂú®ÈÅÆËîΩ‰πãÂâç‰ΩøÁî®Ë≤ºÁâáÊ∑∑ÂêàÂô®È†êËôïÁêÜÊâÄÊúâË≤ºÁâáÔºåÂæûËÄåÈ°ØËëóÈôç‰ΩéÈÅÆËîΩÈÄ†ÊàêÁöÑÊïàËÉΩ‰∏ãÈôçÔºå‰ΩøÂÖ∂Âú®Èôç‰ΩéÈÅãÁÆóÊàêÊú¨ÊñπÈù¢ÂÑ™ÊñºÊ®°ÂûãÁ∏ÆÊîæ„ÄÇÊàëÂÄëÈÇÑÁ¥çÂÖ•‰∫ÜTransformerÊû∂Êßã‰∏≠ÁöÑÊúÄÊñ∞ÊîπÈÄ≤Ôºå‰æãÂ¶Ç‰ΩøÁî®Ê∑∑ÂêàÂ∞àÂÆ∂Â±§Ôºå‰ª•ÊèêÂçáÊïàËÉΩ‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊâæÂá∫Âú®ÂæÆÈ†êÁÆóË®ìÁ∑¥‰∏≠‰ΩøÁî®ÂêàÊàêÂΩ±ÂÉèÁöÑÈáçË¶ÅÂ•ΩËôï„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂÉÖ‰ΩøÁî® 3700 Ëê¨ÂºµÂÖ¨ÈñãÁöÑÁúüÂØ¶ÂíåÂêàÊàêÂΩ±ÂÉèÔºåË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÂÖ∑Êúâ 11.6 ÂÑÑÂÄãÂèÉÊï∏ÁöÑÁ®ÄÁñèTransformerÔºåÁ∂ìÊøüÊàêÊú¨ÂÉÖÁÇ∫ 1,890 ÁæéÂÖÉÔºå‰∏¶Âú® COCO Ë≥áÊñôÈõÜÁöÑÈõ∂Ê¨°ÁîüÊàê‰∏≠ÈÅîÂà∞‰∫Ü 12.7 FID„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑ FID ÂíåÈ´òÂìÅË≥™ÁöÑÁîüÊàêÔºåÂêåÊôÇÁî¢ÁîüÁöÑÊàêÊú¨ÊØîÁ©©ÂÆöÁöÑÊì¥Êï£Ê®°Âûã‰Ωé 118 ÂÄçÔºåÊØîÁõÆÂâçÊúÄÂÖàÈÄ≤‰∏îËä±Ë≤ª 28,400 ÁæéÂÖÉÁöÑÊäÄË°ì‰Ωé 14 ÂÄç„ÄÇÊàëÂÄëÊó®Âú®ÈáãÂá∫ÊàëÂÄëÁöÑÁ´ØÂ∞çÁ´ØË®ìÁ∑¥ÁÆ°ÈÅìÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Ê∞ë‰∏ªÂåñÂú®ÂæÆÈ†êÁÆó‰∏äË®ìÁ∑¥Â§ßË¶èÊ®°Êì¥Êï£Ê®°Âûã„ÄÇ</paragraph>

##### **FSboard: Over 3 million characters of ASL fingerspelling collected via smartphones**
2407.15806v1 by Manfred Georg, Garrett Tanzer, Saad Hassan, Maximus Shengelia, Esha Uboweja, Sam Sepah, Sean Forbes, Thad Starner

Progress in machine understanding of sign languages has been slow and
hampered by limited data. In this paper, we present FSboard, an American Sign
Language fingerspelling dataset situated in a mobile text entry use case,
collected from 147 paid and consenting Deaf signers using Pixel 4A selfie
cameras in a variety of environments. Fingerspelling recognition is an
incomplete solution that is only one small part of sign language translation,
but it could provide some immediate benefit to Deaf/Hard of Hearing signers as
more broadly capable technology develops. At >3 million characters in length
and >250 hours in duration, FSboard is the largest fingerspelling recognition
dataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz
MediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character
Error Rate (CER) on a test set with unique phrases and signers. This quality
degrades gracefully when decreasing frame rate and excluding face/body
landmarks: plausible optimizations to help models run on device in real time.

ÊëòË¶ÅÔºöÊâãË™ûÊ©üÂô®ÁêÜËß£ÁöÑÈÄ≤Â±ïÁ∑©ÊÖ¢Ôºå‰∏îÂèóÂà∞Ë≥áÊñôÊúâÈôêÁöÑÈòªÁ§ô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ FSboardÔºå‰∏ÄÂÄãÁæéÂúãÊâãË™ûÊâãÊåáÊãºÂØ´Ë≥áÊñôÈõÜÔºå‰ΩçÊñºË°åÂãïÊñáÂ≠óËº∏ÂÖ•Áî®‰æã‰∏≠ÔºåÂæû 147 ‰Ωç‰ªòË≤ª‰∏îÂêåÊÑèÁöÑËÅæ‰∫∫ÊâãË™ûËÄÖ‰∏≠Êî∂ÈõÜÔºå‰ΩøÁî® Pixel 4A Ëá™ÊãçÁõ∏Ê©üÂú®ÂêÑÁ®ÆÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÊãçÊîù„ÄÇÊâãÊåáÊãºÂØ´Ëæ®Ë≠òÊòØ‰∏ÄÂÄã‰∏çÂÆåÊï¥ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂÉÖÊòØÊâãË™ûÁøªË≠ØÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰ΩÜÈö®ËëóÊõ¥Âª£Ê≥õÂäüËÉΩÊäÄË°ìÁöÑÁôºÂ±ïÔºåÂÆÉÂèØ‰ª•ÁÇ∫ËÅæ‰∫∫/ËÅΩÈöúÊâãË™ûËÄÖÊèê‰æõ‰∏Ä‰∫õÁ´ãÂç≥ÁöÑÁõäËôï„ÄÇFSboard Èï∑Â∫¶Ë∂ÖÈÅé 300 Ëê¨ÂÄãÂ≠óÂÖÉÔºåÊåÅÁ∫åÊôÇÈñìË∂ÖÈÅé 250 Â∞èÊôÇÔºåÊòØËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂ§ßÁöÑÊâãÊåáÊãºÂØ´Ëæ®Ë≠òË≥áÊñôÈõÜÔºå‰øÇÊï∏ÁÇ∫ >10x„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÂü∫Ê∫ñÔºåÊàëÂÄëÂæÆË™ø 30 Hz ÁöÑ MediaPipe Holistic Âú∞Ê®ôËº∏ÂÖ•Âà∞ ByT5-Small ‰∏≠Ôºå‰∏¶Âú®ÂÖ∑ÊúâÁç®ÁâπÁü≠Ë™ûÂíåÊâãË™ûËÄÖÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÅîÂà∞ 11.1% ÁöÑÂ≠óÂÖÉÈåØË™§Áéá (CER)„ÄÇÁï∂Èôç‰ΩéÂπÄÁéá‰∏¶ÊéíÈô§ËáâÈÉ®/Ë∫´È´îÂú∞Ê®ôÊôÇÔºåÈÄôÁ®ÆÂìÅË≥™ÊúÉÈÄêÊº∏Èôç‰ΩéÔºöÂêàÁêÜÁöÑÊúÄ‰Ω≥ÂåñÊúâÂä©ÊñºÊ®°ÂûãÂú®Ë£ùÁΩÆ‰∏äÂç≥ÊôÇÂü∑Ë°å„ÄÇ

##### **CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning**
2407.15793v1 by Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara

With the emergence of Transformers and Vision-Language Models (VLMs) such as
CLIP, large pre-trained models have become a common strategy to enhance
performance in Continual Learning scenarios. This led to the development of
numerous prompting strategies to effectively fine-tune transformer-based models
without succumbing to catastrophic forgetting. However, these methods struggle
to specialize the model on domains significantly deviating from the
pre-training and preserving its zero-shot capabilities. In this work, we
propose Continual Generative training for Incremental prompt-Learning, a novel
approach to mitigate forgetting while adapting a VLM, which exploits generative
replay to align prompts to tasks. We also introduce a new metric to evaluate
zero-shot capabilities within CL benchmarks. Through extensive experiments on
different domains, we demonstrate the effectiveness of our framework in
adapting to new tasks while improving zero-shot capabilities. Further analysis
reveals that our approach can bridge the gap with joint prompt tuning. The
codebase is available at https://github.com/aimagelab/mammoth.

ÊëòË¶ÅÔºöÈö®Ëëó Transformers Âíå CLIP Á≠âË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÂá∫ÁèæÔºåÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÂ∑≤ÊàêÁÇ∫Âú®ÊåÅÁ∫åÂ≠∏ÁøíÂ†¥ÊôØ‰∏≠Â¢ûÂº∑ÊïàËÉΩÁöÑÂ∏∏Ë¶ãÁ≠ñÁï•„ÄÇÈÄôÂ∞éËá¥‰∫ÜË®±Â§öÊèêÁ§∫Á≠ñÁï•ÁöÑÈñãÁôºÔºå‰ª•ÊúâÊïàÂæÆË™øÂü∫ÊñºËΩâÊèõÂô®ÁöÑÊ®°ÂûãÔºåËÄå‰∏çÊúÉÂ±àÊúçÊñºÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈõ£‰ª•Â∞áÊ®°ÂûãÂ∞àÈñÄÂåñÂà∞ËàáÈ†êË®ìÁ∑¥È°ØËëó‰∏çÂêåÁöÑÈ†òÂüüÔºå‰∏¶‰øùÁïôÂÖ∂Èõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÊåÅÁ∫åÁîüÊàêË®ìÁ∑¥‰ª•ÈÄ≤Ë°åÂ¢ûÈáèÊèêÁ§∫Â≠∏ÁøíÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊ∏õËºïÈÅ∫ÂøòÂêåÊôÇÈÅ©Êáâ VLM ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂà©Áî®ÁîüÊàêÈáçÊí≠Â∞áÊèêÁ§∫Ëàá‰ªªÂãôÂ∞çÈΩä„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÊåáÊ®ô‰æÜË©ï‰º∞ CL Âü∫Ê∫ñ‰∏≠ÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÈÄöÈÅéÂú®‰∏çÂêåÈ†òÂüüÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊ°ÜÊû∂Âú®ÈÅ©ÊáâÊñ∞‰ªªÂãôÂêåÊôÇÊèêÈ´òÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÂàÜÊûêË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèØ‰ª•ÂΩåÂêàÁêÜË´ñÊèêÁ§∫Ë™øÊï¥ÁöÑÂ∑ÆË∑ù„ÄÇÁ®ãÂºèÁ¢ºÂ∫´ÂèØÂú® https://github.com/aimagelab/mammoth ÂèñÂæó„ÄÇ

##### **Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach**
2407.15788v1 by Rian Dolphin, Joe Dursun, Jonathan Chow, Jarrett Blankenship, Katie Adams, Quinton Pike

Financial news plays a crucial role in decision-making processes across the
financial sector, yet the efficient processing of this information into a
structured format remains challenging. This paper presents a novel approach to
financial news processing that leverages Large Language Models (LLMs) to
overcome limitations that previously prevented the extraction of structured
data from unstructured financial news. We introduce a system that extracts
relevant company tickers from raw news article content, performs sentiment
analysis at the company level, and generates summaries, all without relying on
pre-structured data feeds. Our methodology combines the generative capabilities
of LLMs, and recent prompting techniques, with a robust validation framework
that uses a tailored string similarity approach. Evaluation on a dataset of
5530 financial news articles demonstrates the effectiveness of our approach,
with 90% of articles not missing any tickers compared with current data
providers, and 22% of articles having additional relevant tickers. In addition
to this paper, the methodology has been implemented at scale with the resulting
processed data made available through a live API endpoint, which is updated in
real-time with the latest news. To the best of our knowledge, we are the first
data provider to offer granular, per-company sentiment analysis from news
articles, enhancing the depth of information available to market participants.
We also release the evaluation dataset of 5530 processed articles as a static
file, which we hope will facilitate further research leveraging financial news.

ÊëòË¶ÅÔºöË≤°ÂãôÊñ∞ËÅûÂú®Êï¥ÂÄãÈáëËûçÈÉ®ÈñÄÁöÑÊ±∫Á≠ñÈÅéÁ®ã‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÁÑ∂ËÄåÂ∞áÈÄô‰∫õË≥áË®äÊúâÊïàËôïÁêÜÊàêÁµêÊßãÂåñÊ†ºÂºè‰ªçÂÖ∑ÊåëÊà∞ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËôïÁêÜË≤°ÂãôÊñ∞ËÅûÁöÑÊñ∞ÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂÖãÊúçÈÅéÂéªÈòªÁ§ôÂæûÈùûÁµêÊßãÂåñË≤°ÂãôÊñ∞ËÅû‰∏≠Êì∑ÂèñÁµêÊßãÂåñË≥áÊñôÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ≥ªÁµ±ÔºåÂæûÂéüÂßãÊñ∞ËÅûÊñáÁ´†ÂÖßÂÆπ‰∏≠Êì∑ÂèñÁõ∏ÈóúÂÖ¨Âè∏‰ª£ËôüÔºåÂü∑Ë°åÂÖ¨Âè∏Â±§Á¥öÁöÑÊÉÖÁ∑íÂàÜÊûêÔºå‰∏¶Áî¢ÁîüÊëòË¶ÅÔºåÊâÄÊúâÈÄô‰∫õÈÉΩ‰∏ç‰æùË≥¥ÊñºÈ†êÂÖàÁµêÊßãÂåñÁöÑË≥áÊñôÈ•ãÈÄÅ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁµêÂêà‰∫Ü LLM ÁöÑÁîüÊàêËÉΩÂäõÂíåÊúÄÊñ∞ÁöÑÊèêÁ§∫ÊäÄË°ìÔºå‰ª•Âèä‰ΩøÁî®ÂÆ¢Ë£ΩÂåñÂ≠ó‰∏≤Áõ∏‰ººÂ∫¶ÊñπÊ≥ïÁöÑÂº∑ÂÅ•È©óË≠âÊû∂Êßã„ÄÇÂú® 5530 ÁØáË≤°ÂãôÊñ∞ËÅûÊñáÁ´†ÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåËàáÁõÆÂâçË≥áÊñô‰æõÊáâÂïÜÁõ∏ÊØîÔºå90% ÁöÑÊñáÁ´†Ê≤íÊúâÈÅ∫Êºè‰ªª‰Ωï‰ª£ËôüÔºåËÄå 22% ÁöÑÊñáÁ´†ÊúâÈ°çÂ§ñÁöÑÁõ∏Èóú‰ª£Ëôü„ÄÇÈô§‰∫ÜÈÄôÁØáË´ñÊñá‰πãÂ§ñÔºåË©≤ÊñπÊ≥ïÂ∑≤Â§ßË¶èÊ®°ÂØ¶ÊñΩÔºåËôïÁêÜÂæåÁöÑË≥áÊñôÈÄèÈÅéÂç≥ÊôÇ API Á´ØÈªûÊèê‰æõÔºå‰∏¶ÊúÉÊ†πÊìöÊúÄÊñ∞Êñ∞ËÅûÂç≥ÊôÇÊõ¥Êñ∞„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÊàëÂÄëÊòØÁ¨¨‰∏ÄÂÄãÊèê‰æõÊñ∞ËÅûÊñáÁ´†‰∏≠Á¥∞Á∑ªÁöÑ„ÄÅÊåâÂÖ¨Âè∏ÂäÉÂàÜÁöÑÂ∏ÇÂ†¥ÊÉÖÁ∑íÂàÜÊûêË≥áÊñô‰æõÊáâÂïÜÔºåÈÄôÂ¢ûÂä†‰∫ÜÂ∏ÇÂ†¥ÂèÉËàáËÄÖÂèØÂèñÂæóÁöÑË≥áË®äÊ∑±Â∫¶„ÄÇÊàëÂÄë‰πüÈáãÂá∫‰∫Ü 5530 ÁØáÂ∑≤ËôïÁêÜÊñáÁ´†ÁöÑË©ï‰º∞Ë≥áÊñôÈõÜ‰ΩúÁÇ∫ÈùúÊÖãÊ™îÊ°àÔºåÊàëÂÄëÂ∏åÊúõÈÄôÂ∞áÊúâÂä©ÊñºÂà©Áî®Ë≤°ÂãôÊñ∞ËÅûÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ

##### **Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels**
2407.15786v1 by Zhuorui Ye, Stephanie Milani, Geoffrey J. Gordon, Fei Fang

Recent advances in reinforcement learning (RL) have predominantly leveraged
neural network-based policies for decision-making, yet these models often lack
interpretability, posing challenges for stakeholder comprehension and trust.
Concept bottleneck models offer an interpretable alternative by integrating
human-understandable concepts into neural networks. However, a significant
limitation in prior work is the assumption that human annotations for these
concepts are readily available during training, necessitating continuous
real-time input from human annotators. To overcome this limitation, we
introduce a novel training scheme that enables RL algorithms to efficiently
learn a concept-based policy by only querying humans to label a small set of
data, or in the extreme case, without any human labels. Our algorithm,
LICORICE, involves three main contributions: interleaving concept learning and
RL training, using a concept ensembles to actively select informative data
points for labeling, and decorrelating the concept data with a simple strategy.
We show how LICORICE reduces manual labeling efforts to to 500 or fewer concept
labels in three environments. Finally, we present an initial study to explore
how we can use powerful vision-language models to infer concepts from raw
visual inputs without explicit labels at minimal cost to performance.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏Áøí (RL) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰∏ªË¶ÅÂà©Áî®‰∫ÜÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊîøÁ≠ñ‰æÜÈÄ≤Ë°åÊ±∫Á≠ñÂà∂ÂÆöÔºå‰ΩÜÈÄô‰∫õÊ®°ÂûãÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºåÂ∞çÂà©ÁõäÁõ∏ÈóúËÄÖÁöÑÁêÜËß£Âíå‰ø°‰ªªÊßãÊàêÊåëÊà∞„ÄÇÊ¶ÇÂøµÁì∂È†∏Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂ∞á‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑÊ¶ÇÂøµÊï¥ÂêàÂà∞Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÂ∑•‰ΩúÁöÑ‰∏ÄÂÄãÈáçÂ§ßÈôêÂà∂ÊòØÂÅáË®≠Âú®Ë®ìÁ∑¥ÊúüÈñìÈÄô‰∫õÊ¶ÇÂøµÁöÑ‰∫∫Â∑•Ë®ªËß£ÂæàÂÆπÊòìÂèñÂæóÔºåÈÄôÈúÄË¶Å‰∫∫Â∑•Ë®ªËß£ËÄÖÊåÅÁ∫åÁöÑÂç≥ÊôÇËº∏ÂÖ•„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË®ìÁ∑¥ÊñπÊ°àÔºå‰Ωø RL ÊºîÁÆóÊ≥ïËÉΩÂ§†ÊúâÊïàÂú∞Â≠∏ÁøíÂü∫ÊñºÊ¶ÇÂøµÁöÑÊîøÁ≠ñÔºåÂè™ÈúÄÊü•Ë©¢‰∫∫È°ûÊ®ôË®òÂ∞ëÈáèÁöÑË≥áÊñôÈõÜÔºåÊàñÂú®Ê•µÁ´ØÊÉÖÊ≥Å‰∏ãÔºåÂÆåÂÖ®‰∏ç‰ΩøÁî®‰∫∫Â∑•Ê®ôÁ±§„ÄÇÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï LICORICE Ê∂âÂèä‰∏âÂÄã‰∏ªË¶ÅË≤¢ÁçªÔºö‰∫§ÈåØÊ¶ÇÂøµÂ≠∏ÁøíÂíå RL Ë®ìÁ∑¥„ÄÅ‰ΩøÁî®Ê¶ÇÂøµÈõÜÂêà‰∏ªÂãïÈÅ∏ÊìáÊúâÁî®ÁöÑË≥áÊñôÈªûÈÄ≤Ë°åÊ®ôË®òÔºå‰ª•Âèä‰ΩøÁî®Á∞°ÂñÆÁ≠ñÁï•Â∞çÊ¶ÇÂøµË≥áÊñôÈÄ≤Ë°åÂéªÁõ∏Èóú„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü LICORICE Â¶Ç‰ΩïÂ∞á‰∫∫Â∑•Ê®ôË®òÂ∑•‰ΩúÈáèÊ∏õÂ∞ëÂà∞ 500 ÂÄãÊàñÊõ¥Â∞ëÁöÑÊ¶ÇÂøµÊ®ôÁ±§Ôºå‰∏¶ÈÅãÁî®Âú®‰∏âÂÄãÁí∞Â¢É‰∏≠„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂàùÊ≠•Á†îÁ©∂ÔºåÊé¢Ë®éÂ¶Ç‰Ωï‰ΩøÁî®Âº∑Â§ßÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÂæûÂéüÂßãË¶ñË¶∫Ëº∏ÂÖ•‰∏≠Êé®Êñ∑Ê¶ÇÂøµÔºåËÄåÁÑ°ÈúÄÊòéÁ¢∫Ê®ôÁ±§Ôºå‰∏îÂ∞çÊïàËÉΩÁöÑÂΩ±ÈüøÊ•µÂ∞è„ÄÇ

##### **Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems**
2407.15784v1 by Amirhassan Babazadeh Darabi, Sinem Coleri

Diffusion models are vastly used in generative AI, leveraging their
capability to capture complex data distributions. However, their potential
remains largely unexplored in the field of resource allocation in wireless
networks. This paper introduces a novel diffusion model-based resource
allocation strategy for Wireless Networked Control Systems (WNCSs) with the
objective of minimizing total power consumption through the optimization of the
sampling period in the control system, and blocklength and packet error
probability in the finite blocklength regime of the communication system. The
problem is first reduced to the optimization of blocklength only based on the
derivation of the optimality conditions. Then, the optimization theory solution
collects a dataset of channel gains and corresponding optimal blocklengths.
Finally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collected
dataset to train the resource allocation algorithm that generates optimal
blocklength values conditioned on the channel state information (CSI). Via
extensive simulations, the proposed approach is shown to outperform previously
proposed Deep Reinforcement Learning (DRL) based approaches with close to
optimal performance regarding total power consumption. Moreover, an improvement
of up to eighteen-fold in the reduction of critical constraint violations is
observed, further underscoring the accuracy of the solution.

ÊëòË¶ÅÔºöÊì¥Êï£Ê®°ÂûãÂª£Ê≥õÁî®ÊñºÁîüÊàêÂºè AIÔºåÂà©Áî®ÂÖ∂ÊçïÊçâË§áÈõúË≥áÊñôÂàÜ‰ΩàÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÁÑ°Á∑öÁ∂≤Ë∑ØË≥áÊ∫êÈÖçÁΩÆÈ†òÂüüÁöÑÊΩõÂäõ‰ªçÊú™ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑË≥áÊ∫êÈÖçÁΩÆÁ≠ñÁï•ÔºåÁî®ÊñºÁÑ°Á∑öÁ∂≤Ë∑ØÊéßÂà∂Á≥ªÁµ± (WNCS)ÔºåÁõÆÁöÑÊòØÈÄèÈÅéÊúÄ‰Ω≥ÂåñÊéßÂà∂Á≥ªÁµ±‰∏≠ÁöÑÂèñÊ®£ÈÄ±Êúü„ÄÅÂçÄÂ°äÈï∑Â∫¶ÂíåÈÄöË®äÁ≥ªÁµ±ÊúâÈôêÂçÄÂ°äÈï∑Â∫¶ÊñπÊ°à‰∏≠ÁöÑÂ∞ÅÂåÖÈåØË™§Ê©üÁéáÔºå‰æÜÊúÄÂ∞èÂåñÁ∏ΩÂäüËÄó„ÄÇÈ¶ñÂÖàÔºåÂ∞áÂïèÈ°åÁ∞°ÂåñÁÇ∫ÂÉÖÊ†πÊìöÊúÄ‰Ω≥ÂåñÊ¢ù‰ª∂ÁöÑÊé®Â∞é‰æÜÊúÄ‰Ω≥ÂåñÂçÄÂ°äÈï∑Â∫¶„ÄÇÁÑ∂ÂæåÔºåÊúÄ‰Ω≥ÂåñÁêÜË´ñËß£Êî∂ÈõÜ‰∫Ü‰∏ÄÁµÑÈ†ªÈÅìÂ¢ûÁõäÂíåÂ∞çÊáâÁöÑÊúÄ‰Ω≥ÂçÄÂ°äÈï∑Â∫¶„ÄÇÊúÄÂæåÔºåÂéªÂô™Êì¥Êï£Ê©üÁéáÊ®°Âûã (DDPM) ‰ΩøÁî®Ê≠§Êî∂ÈõÜÁöÑË≥áÊñôÈõÜ‰æÜË®ìÁ∑¥Ë≥áÊ∫êÈÖçÁΩÆÊºîÁÆóÊ≥ïÔºåË©≤ÊºîÁÆóÊ≥ïÊúÉÊ†πÊìöÈ†ªÈÅìÁãÄÊÖãË≥áË®ä (CSI) Áî¢ÁîüÊúÄ‰Ω≥ÂçÄÂ°äÈï∑Â∫¶ÂÄº„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÊ®°Êì¨ÔºåÂ∑≤Ë≠âÊòéÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂÑ™ÊñºÂÖàÂâçÊèêÂá∫ÁöÑÂü∫ÊñºÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (DRL) ÁöÑÊñπÊ≥ïÔºåÂú®Á∏ΩÂäüËÄóÊñπÈù¢Êé•ËøëÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåËßÄÂØüÂà∞Âú®Ê∏õÂ∞ëÈóúÈçµÁ¥ÑÊùüÈÅïË¶èÊñπÈù¢ÊúÄÂ§öÂèØÊîπÂñÑÂçÅÂÖ´ÂÄçÔºåÈÄ≤‰∏ÄÊ≠•Âº∑Ë™ø‰∫ÜËß£Ê±∫ÊñπÊ°àÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇ

##### **Explaining Decisions in ML Models: a Parameterized Complexity Analysis**
2407.15780v1 by Sebastian Ordyniak, Giacomo Paesani, Mateusz Rychlicki, Stefan Szeider

This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random
Forests, and Boolean Circuits, and ensembles thereof, each offering unique
explanatory challenges. This research fills a significant gap in explainable AI
(XAI) by providing a foundational understanding of the complexities of
generating explanations for these models. This work provides insights vital for
further research in the domain of XAI, contributing to the broader discourse on
the necessity of transparency and accountability in AI systems.

ÊëòË¶ÅÔºöÊú¨ÊñáÂ∞çÂêÑÁ®ÆÊ©üÂô®Â≠∏Áøí (ML) Ê®°Âûã‰∏≠Ëß£ÈáãÂïèÈ°åÁöÑÂèÉÊï∏ÂåñË§áÈõúÊÄßÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÁêÜË´ñÁ†îÁ©∂„ÄÇËàáÊôÆÈÅçÁöÑÈªëÁõíÊÑüÁü•Áõ∏ÂèçÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÈáçÈªûÂú®ÊñºÂÖ∑ÊúâÈÄèÊòéÂÖßÈÉ®Ê©üÂà∂ÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÂÖ©Á®Æ‰∏ªË¶ÅÁöÑËß£ÈáãÂïèÈ°åÈ°ûÂûãÔºöÊºîÁππÂíåÂ∞çÊØîÔºåÂåÖÊã¨ÂÆÉÂÄëÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËÆäÈ´î„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÊ∂µËìã‰∫ÜÂêÑÁ®Æ ML Ê®°ÂûãÔºåÂåÖÊã¨Ê±∫Á≠ñÊ®π„ÄÅÊ±∫Á≠ñÈõÜ„ÄÅÊ±∫Á≠ñÂàóË°®„ÄÅÊúâÂ∫è‰∫åÂÖÉÊ±∫Á≠ñÂúñ„ÄÅÈö®Ê©üÊ£ÆÊûóÂíåÂ∏ÉÊûóÈõªË∑ØÂèäÂÖ∂ÈõÜÂêàÔºåÊØèÂÄãÈÉΩÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑËß£ÈáãÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂Â°´Ë£ú‰∫ÜÂèØËß£Èáã AI (XAI) ÁöÑ‰∏ÄÂÄãÈáçÂ§ßÁ©∫ÁôΩÔºåÊèê‰æõ‰∫ÜÂ∞çÈÄô‰∫õÊ®°ÂûãÁîüÊàêËß£ÈáãÁöÑË§áÈõúÊÄßÁöÑÂü∫Á§éÁêÜËß£„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ XAI È†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂Êèê‰æõ‰∫ÜËá≥ÈóúÈáçË¶ÅÁöÑË¶ãËß£ÔºåÁÇ∫ÈóúÊñº AI Á≥ªÁµ±‰∏≠ÈÄèÊòéÂ∫¶ÂíåÂïèË≤¨Âà∂ÂøÖË¶ÅÊÄßÁöÑÂª£Ê≥õË®éË´ñÂÅöÂá∫‰∫ÜË≤¢Áçª„ÄÇ

##### **Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection**
2407.15771v1 by Kangqi Ma, Hao Dong, Yadong Mu

This paper addresses the challenge of robotic grasping of general objects.
Similar to prior research, the task reads a single-view 3D observation (i.e.,
point clouds) captured by a depth camera as input. Crucially, the success of
object grasping highly demands a comprehensive understanding of the shape of
objects within the scene. However, single-view observations often suffer from
occlusions (including both self and inter-object occlusions), which lead to
gaps in the point clouds, especially in complex cluttered scenes. This renders
incomplete perception of the object shape and frequently causes failures or
inaccurate pose estimation during object grasping. In this paper, we tackle
this issue with an effective albeit simple solution, namely completing
grasping-related scene regions through local occupancy prediction. Following
prior practice, the proposed model first runs by proposing a number of most
likely grasp points in the scene. Around each grasp point, a module is designed
to infer any voxel in its neighborhood to be either void or occupied by some
object. Importantly, the occupancy map is inferred by fusing both local and
global cues. We implement a multi-group tri-plane scheme for efficiently
aggregating long-distance contextual information. The model further estimates
6-DoF grasp poses utilizing the local occupancy-enhanced object shape
information and returns the top-ranked grasp proposal. Comprehensive
experiments on both the large-scale GraspNet-1Billion benchmark and real
robotic arm demonstrate that the proposed method can effectively complete the
unobserved parts in cluttered and occluded scenes. Benefiting from the
occupancy-enhanced feature, our model clearly outstrips other competing methods
under various performance metrics such as grasping average precision.

ÊëòË¶ÅÔºö<paragraph>ÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫ÜÊ©üÂô®‰∫∫ÊäìÂèñ‰∏ÄËà¨Áâ©È´îÁöÑÊåëÊà∞„ÄÇËàáÂÖàÂâçÁöÑÁ†îÁ©∂È°û‰ººÔºåË©≤‰ªªÂãôËÆÄÂèñÊ∑±Â∫¶Áõ∏Ê©üÊçïÊçâÂà∞ÁöÑÂñÆË¶ñÂúñ 3D ËßÄÊ∏¨ÔºàÂç≥ÔºåÈªûÈõ≤Ôºâ‰ΩúÁÇ∫Ëº∏ÂÖ•„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÁâ©È´îÊäìÂèñÁöÑÊàêÂäüÈ´òÂ∫¶‰æùË≥¥ÊñºÂ∞çÂ†¥ÊôØ‰∏≠Áâ©È´îÂΩ¢ÁãÄÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÁÑ∂ËÄåÔºåÂñÆË¶ñÂúñËßÄÊ∏¨ÈÄöÂ∏∏ÊúÉÂèóÂà∞ÈÅÆÊìãÔºàÂåÖÊã¨Ëá™ÊàëÈÅÆÊìãÂíåÁâ©È´îÈñìÈÅÆÊìãÔºâÁöÑÂΩ±ÈüøÔºåÈÄôÊúÉÂ∞éËá¥ÈªûÈõ≤‰∏≠ÁöÑÈñìÈöôÔºåÁâπÂà•ÊòØÂú®Ë§áÈõúÁöÑÊ∑∑‰∫ÇÂ†¥ÊôØ‰∏≠„ÄÇÈÄôÊúÉÂ∞éËá¥Â∞çÁâ©È´îÂΩ¢ÁãÄÁöÑÊÑüÁü•‰∏çÂÆåÊï¥Ôºå‰∏¶Âú®Áâ©È´îÊäìÂèñÈÅéÁ®ã‰∏≠Á∂ìÂ∏∏Â∞éËá¥Â§±ÊïóÊàñÂßøÂã¢‰º∞Ë®à‰∏çÊ∫ñÁ¢∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé°Áî®‰∏ÄÁ®ÆÊúâÊïà‰∏îÁ∞°ÂñÆÁöÑËß£Ê±∫ÊñπÊ°à‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂç≥ÈÄöÈÅéÂ±ÄÈÉ®‰ΩîÁî®È†êÊ∏¨ÂÆåÊàêËàáÊäìÂèñÁõ∏ÈóúÁöÑÂ†¥ÊôØÂçÄÂüü„ÄÇÈÅµÂæ™ÂÖàÂâçÁöÑÂØ¶Ë∏êÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÈ¶ñÂÖàÈÄöÈÅéÊèêÂá∫Â†¥ÊôØ‰∏≠‰∏Ä‰∫õÊúÄÂèØËÉΩÁöÑÊäìÂèñÈªû‰æÜÈÅãË°å„ÄÇÂú®ÊØèÂÄãÊäìÂèñÈªûÂë®ÂúçÔºåË®≠Ë®à‰∏ÄÂÄãÊ®°ÁµÑ‰æÜÊé®Êñ∑ÂÖ∂ÈÑ∞Âüü‰∏≠ÁöÑ‰ªª‰ΩïÈ´îÁ¥†ÊòØÁ©∫ÁöÑÈÇÑÊòØË¢´ÊüêÂÄãÁâ©È´î‰ΩîÁî®„ÄÇÈáçË¶ÅÁöÑÊòØÔºå‰ΩîÁî®Âú∞ÂúñÊòØÈÄöÈÅéËûçÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁ∑öÁ¥¢Êé®Êñ∑Âá∫‰æÜÁöÑ„ÄÇÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÂ§öÁæ§ÁµÑ‰∏âÂπ≥Èù¢ÊñπÊ°àÔºå‰ª•ÊúâÊïàÂú∞ÂΩôÁ∏ΩÈï∑Ë∑ùÈõ¢ÁöÑ‰∏ä‰∏ãÊñáË≥áË®ä„ÄÇË©≤Ê®°ÂûãÈÄ≤‰∏ÄÊ≠•‰º∞Ë®à 6-DoF ÊäìÂèñÂßøÂã¢ÔºåÂà©Áî®Â±ÄÈÉ®‰ΩîÁî®Â¢ûÂº∑ÁöÑÁâ©È´îÂΩ¢ÁãÄË≥áË®äÔºå‰∏¶ËøîÂõûÊéíÂêçÊúÄÈ´òÁöÑÊäìÂèñÂª∫Ë≠∞„ÄÇÂú®Â§ßÂûã GraspNet-1Billion Âü∫Ê∫ñÂíåÁúüÂØ¶Ê©üÂô®‰∫∫ÊâãËáÇ‰∏äÁöÑÁ∂úÂêàÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•Âú®Ê∑∑‰∫ÇÂíåÈÅÆÊìãÁöÑÂ†¥ÊôØ‰∏≠ÊúâÊïàÂú∞ÂÆåÊàêÊú™ËßÄÂØüÂà∞ÁöÑÈÉ®ÂàÜ„ÄÇÂèóÁõäÊñº‰ΩîÁî®Â¢ûÂº∑ÂäüËÉΩÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂêÑÁ®ÆÊïàËÉΩÊåáÊ®ôÔºà‰æãÂ¶ÇÊäìÂèñÂπ≥ÂùáÁ≤æÊ∫ñÂ∫¶Ôºâ‰∏ãÊòéÈ°ØÂÑ™ÊñºÂÖ∂‰ªñÁ´∂Áà≠ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning**
2407.15762v1 by Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Avinava Dubey, Alexandre Ram√©, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, L√©onard Hussenot, Olivier Bachem, Edouard Leurent

Reward-based finetuning is crucial for aligning language policies with
intended behaviors (e.g., creativity and safety). A key challenge here is to
develop steerable language models that trade-off multiple (conflicting)
objectives in a flexible and efficient manner. This paper presents Conditioned
Language Policy (CLP), a general framework for finetuning language models on
multiple objectives. Building on techniques from multi-task training and
parameter-efficient finetuning, CLP can learn steerable models that effectively
trade-off conflicting objectives at inference time. Notably, this does not
require training or maintaining multiple models to achieve different trade-offs
between the objectives. Through an extensive set of experiments and ablations,
we show that the CLP framework learns steerable models that outperform and
Pareto-dominate the current state-of-the-art approaches for multi-objective
finetuning.

ÊëòË¶ÅÔºöÂü∫ÊñºÁçéÂãµÁöÑÂæÆË™øÂ∞çÊñºÂ∞áË™ûË®ÄÊîøÁ≠ñËàáÈ†êÊúüÁöÑË°åÁÇ∫Ôºà‰æãÂ¶ÇÔºåÂâµÊÑèÂíåÂÆâÂÖ®ÊÄßÔºâÁõ∏Á¨¶Ëá≥ÈóúÈáçË¶Å„ÄÇÊ≠§ËôïÁöÑ‰∏ªË¶ÅÊåëÊà∞ÊòØÈñãÁôºÂèØÊéßË™ûË®ÄÊ®°ÂûãÔºå‰ª•ÈùàÊ¥ª‰∏îÊúâÊïàÁöÑÊñπÂºèÊ¨äË°°Â§öÂÄãÔºàË°ùÁ™ÅÔºâÁõÆÊ®ô„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÊ¢ù‰ª∂Ë™ûË®ÄÊîøÁ≠ñÔºàCLPÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂú®Â§öÂÄãÁõÆÊ®ô‰∏äÂæÆË™øË™ûË®ÄÊ®°ÂûãÁöÑÈÄöÁî®Ê°ÜÊû∂„ÄÇCLP Âª∫Á´ãÂú®Â§ö‰ªªÂãôË®ìÁ∑¥ÂíåÂèÉÊï∏ÊúâÊïàÂæÆË™øÁöÑÊäÄË°ì‰πã‰∏äÔºåÂèØ‰ª•Â≠∏ÁøíÂèØÊéßÊ®°ÂûãÔºåÂú®Êé®ÁêÜÊôÇÊúâÊïàÊ¨äË°°Ë°ùÁ™ÅÁõÆÊ®ô„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÈÄô‰∏çÈúÄË¶ÅË®ìÁ∑¥ÊàñÁ∂≠Ë≠∑Â§öÂÄãÊ®°Âûã‰æÜÂØ¶ÁèæÁõÆÊ®ô‰πãÈñìÁöÑ‰∏çÂêåÊ¨äË°°„ÄÇÈÄöÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÂíåÊ∂àËûçÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü CLP Ê°ÜÊû∂Â≠∏Áøí‰∫ÜÂèØÊéßÊ®°ÂûãÔºåÈÄô‰∫õÊ®°ÂûãÂÑ™Êñº‰∏¶Âú®Â∏ïÁ¥ØÊâòÊÑèÁæ©‰∏äÂÑ™ÊñºÁï∂ÂâçÂ§öÁõÆÊ®ôÂæÆË™øÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ

##### **Model editing for distribution shifts in uranium oxide morphological analysis**
2407.15756v1 by Davis Brown, Cody Nizinski, Madelyn Shapiro, Corey Fallon, Tianzhixi Yin, Henry Kvinge, Jonathan H. Tu

Deep learning still struggles with certain kinds of scientific data. Notably,
pretraining data may not provide coverage of relevant distribution shifts
(e.g., shifts induced via the use of different measurement instruments). We
consider deep learning models trained to classify the synthesis conditions of
uranium ore concentrates (UOCs) and show that model editing is particularly
effective for improving generalization to distribution shifts common in this
domain. In particular, model editing outperforms finetuning on two curated
datasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity
chambers and micrographs acquired with different scanning electron microscopes,
respectively.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí‰ªçÈõ£‰ª•ËôïÁêÜÁâπÂÆöÈ°ûÂûãÁöÑÁßëÂ≠∏Ë≥áÊñô„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÈ†êË®ìÁ∑¥Ë≥áÊñôÂèØËÉΩÁÑ°Ê≥ïÊ∂µËìãÁõ∏ÈóúÂàÜ‰ΩàËΩâÁßªÔºà‰æãÂ¶ÇÔºåÂõ†‰ΩøÁî®‰∏çÂêåÁöÑÊ∏¨ÈáèÂÑÄÂô®ËÄåÂºïÁôºÁöÑËΩâÁßªÔºâ„ÄÇÊàëÂÄëËÄÉÊÖÆË®ìÁ∑¥Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰ª•ÂàÜÈ°ûÈàæÁ§¶Á≤æÁ§¶ÔºàUOCÔºâÁöÑÂêàÊàêÊ¢ù‰ª∂Ôºå‰∏¶Ë°®ÊòéÊ®°ÂûãÁ∑®ËºØÂ∞çÊñºÊîπÂñÑÂú®Ë©≤È†òÂüüÂ∏∏Ë¶ãÁöÑÂàÜ‰ΩàËΩâÁßªÁöÑÊ≥õÂåñÁâπÂà•ÊúâÊïà„ÄÇÁâπÂà•ÊòØÔºåÊ®°ÂûãÁ∑®ËºØÂú®ÂÖ©ÂÄãÁ≠ñÂ±ïË≥áÊñôÈõÜ‰∏äÁöÑË°®ÁèæÂÑ™ÊñºÂæÆË™øÔºåÈÄô‰∫õË≥áÊñôÈõÜÂåÖÂê´Âú®ÊøïÂ∫¶ÂÆ§‰∏≠ËÄÅÂåñÁöÑ U3O8 ÁöÑÈ°ØÂæÆÁÖßÁâá‰ª•ÂèäÂàÜÂà•‰ΩøÁî®‰∏çÂêåÁöÑÊéÉÊèèÈõªÂ≠êÈ°ØÂæÆÈè°Áç≤ÂæóÁöÑÈ°ØÂæÆÁÖßÁâá„ÄÇ

##### **LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding**
2407.15754v1 by Haoning Wu, Dongxu Li, Bei Chen, Junnan Li

Large multimodal models (LMMs) are processing increasingly longer and richer
inputs. Albeit the progress, few public benchmark is available to measure such
development. To mitigate this gap, we introduce LongVideoBench, a
question-answering benchmark that features video-language interleaved inputs up
to an hour long. Our benchmark includes 3,763 varying-length web-collected
videos with their subtitles across diverse themes, designed to comprehensively
evaluate LMMs on long-term multimodal understanding. To achieve this, we
interpret the primary challenge as to accurately retrieve and reason over
detailed multimodal information from long inputs. As such, we formulate a novel
video question-answering task termed referring reasoning. Specifically, as part
of the question, it contains a referring query that references related video
contexts, called referred context. The model is then required to reason over
relevant video details from the referred context. Following the paradigm of
referring reasoning, we curate 6,678 human-annotated multiple-choice questions
in 17 fine-grained categories, establishing one of the most comprehensive
benchmarks for long-form video understanding. Evaluations suggest that the
LongVideoBench presents significant challenges even for the most advanced
proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their
open-source counterparts show an even larger performance gap. In addition, our
results indicate that model performance on the benchmark improves only when
they are capable of processing more frames, positioning LongVideoBench as a
valuable benchmark for evaluating future-generation long-context LMMs.

ÊëòË¶ÅÔºöÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) Ê≠£Âú®ËôïÁêÜË∂ä‰æÜË∂äÈï∑‰∏îË±êÂØåÁöÑËº∏ÂÖ•„ÄÇÂÑòÁÆ°ÈÄ≤Â±ïÈ†ÜÂà©Ôºå‰ΩÜÂæàÂ∞ëÊúâÂÖ¨ÈñãÂü∫Ê∫ñÂèØÁî®ÊñºË°°ÈáèÊ≠§È°ûÈñãÁôº„ÄÇÁÇ∫‰∫ÜÁ∏ÆÂ∞èÈÄôÁ®ÆÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü LongVideoBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÂïèÁ≠îÂü∫Ê∫ñÔºåÂÖ∂ÁâπÈªûÊòØÈï∑ÈÅî‰∏ÄÂ∞èÊôÇÁöÑË¶ñË®äË™ûË®Ä‰∫§ÈåØËº∏ÂÖ•„ÄÇÊàëÂÄëÁöÑÂü∫Ê∫ñÂåÖÊã¨ 3,763 ÂÄãÈï∑Â∫¶‰∏çÂêåÁöÑÁ∂≤Ë∑ØÊî∂ÈõÜË¶ñË®äÂèäÂÖ∂Â≠óÂπïÔºåÊ∂µËìãÂêÑÁ®Æ‰∏ªÈ°åÔºåÊó®Âú®ÂÖ®Èù¢Ë©ï‰º∞ LMM Âú®Èï∑ÊúüÂ§öÊ®°ÊÖãÁêÜËß£‰∏äÁöÑË°®Áèæ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∞á‰∏ªË¶ÅÊåëÊà∞Ë©ÆÈáãÁÇ∫Ê∫ñÁ¢∫Êì∑ÂèñÂíåÊé®Ë´ñÈï∑Ëº∏ÂÖ•‰∏≠ÁöÑË©≥Á¥∞Â§öÊ®°ÊÖãË≥áË®ä„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂà∂ÂÆö‰∫Ü‰∏ÄÈ†ÖÊñ∞Á©éÁöÑË¶ñË®äÂïèÁ≠î‰ªªÂãôÔºåÁ®±ÁÇ∫ÊåáÊ∂âÊé®ÁêÜ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂÆÉÂåÖÂê´‰∏ÄÂÄãÊåáÊ∂âÊü•Ë©¢‰ΩúÁÇ∫ÂïèÈ°åÁöÑ‰∏ÄÈÉ®ÂàÜÔºåË©≤Êü•Ë©¢ÂèÉËÄÉÁõ∏ÈóúË¶ñË®äÂÖßÂÆπÔºåÁ®±ÁÇ∫ÊåáÊ∂âÂÖßÂÆπ„ÄÇÁÑ∂ÂæåÔºåÊ®°ÂûãÈúÄË¶ÅÊ†πÊìöÊåáÊ∂âÂÖßÂÆπ‰∏≠ÁöÑÁõ∏ÈóúË¶ñË®äÁ¥∞ÁØÄÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÈÅµÂæ™ÊåáÊ∂âÊé®ÁêÜÁöÑÁØÑ‰æãÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫Ü 6,678 ÂÄãÁî±‰∫∫Â∑•Ê®ôË®ªÁöÑÂ§öÈáçÈÅ∏ÊìáÈ°åÔºåÂàÜÁÇ∫ 17 ÂÄãÁ¥∞Á∑ªÈ°ûÂà•ÔºåÂª∫Á´ã‰∫ÜÊúÄÂÖ®Èù¢ÁöÑÈï∑ÁØáË¶ñË®äÁêÜËß£Âü∫Ê∫ñ‰πã‰∏Ä„ÄÇË©ï‰º∞Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàÈÄ≤ÁöÑÂ∞àÊúâÊ®°ÂûãÔºà‰æãÂ¶Ç GPT-4o„ÄÅGemini-1.5-Pro„ÄÅGPT-4-TurboÔºâÔºåLongVideoBench ‰πüÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊà∞ÔºåËÄåÂÆÉÂÄëÁöÑÈñãÊ∫êÂ∞çÊáâÊ®°ÂûãÂâáÈ°ØÁ§∫Âá∫Êõ¥Â§ßÁöÑÊïàËÉΩÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÊ®°ÂûãÂú®Âü∫Ê∫ñ‰∏äÁöÑÊïàËÉΩÂè™ÊúâÂú®ÂÆÉÂÄëËÉΩÂ§†ËôïÁêÜÊõ¥Â§öÂπÄÊôÇÊâçÊúÉÊèêÂçáÔºåÈÄô‰ΩøÂæó LongVideoBench ÊàêÁÇ∫Ë©ï‰º∞Êú™‰æÜ‰∏ñ‰ª£Èï∑ÂÖßÂÆπ LMM ÁöÑÂØ∂Ë≤¥Âü∫Ê∫ñ„ÄÇ

##### **MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**
2407.15748v1 by Marco Simoni, Andrea Saracino, Vinod P., Mauro Conti

In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the
first specialised AI chatbot for cybersecurity. MoRSE aims to provide
comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG
(Retrieval Augmented Generation) systems designed to retrieve and organize
information from multidimensional cybersecurity contexts. MoRSE differs from
traditional RAGs by using parallel retrievers that work together to retrieve
semantically related information in different formats and structures. Unlike
traditional Large Language Models (LLMs) that rely on Parametric Knowledge
Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases
in response to user queries. Subsequently, MoRSE uses this information to
generate accurate answers. In addition, MoRSE benefits from real-time updates
to its knowledge bases, enabling continuous knowledge enrichment without
retraining. We have evaluated the effectiveness of MoRSE against other
state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific
questions. The experimental evaluation has shown that the improvement in terms
of relevance and correctness of the answer is more than 10\% compared to known
solutions such as GPT-4 and Mixtral 7x8.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π MoRSEÔºàRAG ÂÆâÂÖ®Â∞àÂÆ∂ÁöÑÊ∑∑ÂêàÔºâÔºåÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄÁî®ÊñºÁ∂≤Ë∑ØÂÆâÂÖ®ÁöÑ AI ËÅäÂ§©Ê©üÂô®‰∫∫„ÄÇMoRSE Êó®Âú®Êèê‰æõÈóúÊñºÁ∂≤Ë∑ØÂÆâÂÖ®ÁöÑÂÖ®Èù¢‰∏îÂÆåÊï¥ÁöÑÁü•Ë≠ò„ÄÇMoRSE ‰ΩøÁî®ÂÖ©ÂÄã RAGÔºàÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºâÁ≥ªÁµ±ÔºåÊó®Âú®ÂæûÂ§öÁ∂≠Á∂≤Ë∑ØÂÆâÂÖ®ÊÉÖÂ¢É‰∏≠Ê™¢Á¥¢ÂíåÊï¥ÁêÜË≥áË®ä„ÄÇMoRSE ËàáÂÇ≥Áµ±ÁöÑ RAG ‰∏çÂêåÔºåÂÆÉ‰ΩøÁî®‰∏¶Ë°åÊ™¢Á¥¢Âô®ÂÖ±ÂêåÈÅã‰ΩúÔºå‰ª•Ê™¢Á¥¢‰∏çÂêåÊ†ºÂºèÂíåÁµêÊßã‰∏≠ÁöÑË™ûÁæ©Áõ∏ÈóúË≥áË®ä„ÄÇËàá‰æùË≥¥ÂèÉÊï∏ÂåñÁü•Ë≠òÂ∫´ÁöÑÂÇ≥Áµ±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏çÂêåÔºåMoRSE Ê†πÊìö‰ΩøÁî®ËÄÖÊü•Ë©¢ÂæûÈùûÂèÉÊï∏ÂåñÁü•Ë≠òÂ∫´‰∏≠Ê™¢Á¥¢Áõ∏ÈóúÊñá‰ª∂„ÄÇÈö®ÂæåÔºåMoRSE ‰ΩøÁî®ÈÄô‰∫õË≥áË®äÁî¢ÁîüÊ∫ñÁ¢∫ÁöÑÁ≠îÊ°à„ÄÇÊ≠§Â§ñÔºåMoRSE ÂèóÁõäÊñºÂÖ∂Áü•Ë≠òÂ∫´ÁöÑÂç≥ÊôÇÊõ¥Êñ∞ÔºåÂèØ‰ª•Âú®‰∏çÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÊåÅÁ∫åË±êÂØåÁü•Ë≠ò„ÄÇÊàëÂÄëÂ∑≤Á∂ìË©ï‰º∞‰∫Ü MoRSE Â∞çÊØîÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑ LLM ÁöÑÊúâÊïàÊÄßÔºåÂú® 600 ÂÄãÁ∂≤Ë∑ØÂÆâÂÖ®ÁâπÂÆöÂïèÈ°å‰∏äË©ï‰º∞Á≥ªÁµ±„ÄÇÂØ¶È©óË©ï‰º∞Ë°®ÊòéÔºåËàáÂ∑≤Áü•ÁöÑËß£Ê±∫ÊñπÊ°àÔºà‰æãÂ¶Ç GPT-4 Âíå Mixtral 7x8ÔºâÁõ∏ÊØîÔºåÁ≠îÊ°àÁõ∏ÈóúÊÄßÂíåÊ≠£Á¢∫ÊÄßÁöÑÊîπÈÄ≤Ë∂ÖÈÅé 10%„ÄÇ

##### **Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond**
2407.15739v1 by Silvio Galesso, Philipp Schr√∂ppel, Hssan Driss, Thomas Brox

In recent years, research on out-of-distribution (OoD) detection for semantic
segmentation has mainly focused on road scenes -- a domain with a constrained
amount of semantic diversity. In this work, we challenge this constraint and
extend the domain of this task to general natural images. To this end, we
introduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and
includes images from diverse domains with a high semantic diversity, and 2. a
novel approach that uses Diffusion score matching for OoD detection (DOoD) and
is robust to the increased semantic diversity. ADE-OoD features indoor and
outdoor images, defines 150 semantic categories as in-distribution, and
contains a variety of OoD objects. For DOoD, we train a diffusion model with an
MLP architecture on semantic in-distribution embeddings and build on the score
matching interpretation to compute pixel-wise OoD scores at inference time. On
common road scene OoD benchmarks, DOoD performs on par or better than the state
of the art, without using outliers for training or making assumptions about the
data domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much
room for future improvements.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈáùÂ∞çË™ûÊÑèÂàÜÂâ≤ÁöÑÂàÜÂ∏ÉÂ§ñ (OoD) ÂÅµÊ∏¨Á†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÈÅìË∑ØÂ†¥ÊôØÔºåÈÄôÊòØ‰∏ÄÂÄãË™ûÊÑèÂ§öÊ®£ÊÄßÂèóÈôêÁöÑÈ†òÂüü„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊåëÊà∞Ê≠§ÈôêÂà∂Ôºå‰∏¶Â∞áÊ≠§‰ªªÂãôÁöÑÈ†òÂüüÊì¥Â±ïÂà∞‰∏ÄËà¨Ëá™ÁÑ∂ÂΩ±ÂÉè„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫Ôºö1. ADE-OoD Âü∫Ê∫ñÔºåÂÆÉÂü∫Êñº ADE20k Ë≥áÊñôÈõÜÔºå‰∏¶ÂåÖÂê´‰æÜËá™ÂÖ∑ÊúâÈ´òÂ∫¶Ë™ûÊÑèÂ§öÊ®£ÊÄßÁöÑ‰∏çÂêåÈ†òÂüüÁöÑÂΩ±ÂÉèÔºå‰ª•Âèä 2. ‰∏ÄÁ®Æ‰ΩøÁî®Êì¥Êï£ÂàÜÊï∏ÈÖçÂ∞çÈÄ≤Ë°å OoD ÂÅµÊ∏¨ (DOoD) ÁöÑÊñ∞Á©éÊñπÊ≥ïÔºå‰∏îÂ∞çÂ¢ûÂä†ÁöÑË™ûÊÑèÂ§öÊ®£ÊÄßÂÖ∑ÊúâÈ≠ØÊ£íÊÄß„ÄÇADE-OoD ÂÖ∑ÊúâÂÆ§ÂÖßÂíåÂÆ§Â§ñÂΩ±ÂÉèÔºåÂÆöÁæ© 150 ÂÄãË™ûÊÑèÈ°ûÂà•‰ΩúÁÇ∫ÂàÜÂ∏ÉÂÖßÔºå‰∏¶ÂåÖÂê´ÂêÑÁ®Æ OoD Áâ©‰ª∂„ÄÇÂ∞çÊñº DOoDÔºåÊàëÂÄë‰ΩøÁî® MLP Êû∂ÊßãË®ìÁ∑¥‰∏ÄÂÄãÊì¥Êï£Ê®°ÂûãÔºåÂú®Ë™ûÊÑèÂàÜÂ∏ÉÂÖßÂµåÂÖ•‰∏≠Ôºå‰∏¶Âª∫Á´ãÂú®ÂàÜÊï∏ÈÖçÂ∞çË©ÆÈáã‰∏äÔºå‰ª•Âú®Êé®ÁêÜÊôÇË®àÁÆóÈÄêÂÉèÁ¥†ÁöÑ OoD ÂàÜÊï∏„ÄÇÂú®Â∏∏Ë¶ãÈÅìË∑ØÂ†¥ÊôØ OoD Âü∫Ê∫ñ‰∏äÔºåDOoD ÁöÑË°®ÁèæËàáÁèæÊúâÊäÄË°ìÁõ∏Áï∂ÊàñÊõ¥Â•ΩÔºåËÄåÁÑ°ÈúÄ‰ΩøÁî®Áï∞Â∏∏ÂÄºÈÄ≤Ë°åË®ìÁ∑¥ÊàñÂ∞çË≥áÊñôÈ†òÂüüÂÅöÂá∫ÂÅáË®≠„ÄÇÂú® ADE-OoD ‰∏äÔºåDOoD ÂÑ™ÊñºÂÖàÂâçÁöÑÂÅöÊ≥ïÔºå‰ΩÜ‰ªçÊúâÂæàÂ§ßÁöÑÊîπÈÄ≤Á©∫Èñì„ÄÇ

##### **Parallel Split Learning with Global Sampling**
2407.15738v1 by Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink

The expansion of IoT devices and the demands of Deep Learning have
highlighted significant challenges in Distributed Deep Learning (DDL) systems.
Parallel Split Learning (PSL) has emerged as a promising derivative of Split
Learning that is well suited for distributed learning on resource-constrained
devices. However, PSL faces several obstacles, such as large effective batch
sizes, non-IID data distributions, and the straggler effect. We view these
issues as a sampling dilemma and propose to address them by orchestrating the
mini-batch sampling process on the server side. We introduce the Uniform Global
Sampling (UGS) method to decouple the effective batch size from the number of
clients and reduce mini-batch deviation in non-IID settings. To address the
straggler effect, we introduce the Latent Dirichlet Sampling (LDS) method,
which generalizes UGS to balance the trade-off between batch deviation and
training time. Our simulations reveal that our proposed methods enhance model
accuracy by up to 34.1% in non-IID settings and reduce the training time in the
presence of stragglers by up to 62%. In particular, LDS effectively mitigates
the straggler effect without compromising model accuracy or adding significant
computational overhead compared to UGS. Our results demonstrate the potential
of our methods as a promising solution for DDL in real applications.

ÊëòË¶ÅÔºöÁâ©ËÅîÁΩëËÆæÂ§áÁöÑÊâ©Âº†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÁöÑÈúÄÊ±ÇÁ™ÅÂá∫‰∫ÜÂàÜÂ∏ÉÂºèÊ∑±Â∫¶Â≠¶‰π† (DDL) Á≥ªÁªü‰∏≠ÁöÑÈáçÂ§ßÊåëÊàò„ÄÇ
Âπ∂Ë°åÊãÜÂàÜÂ≠¶‰π† (PSL) Â∑≤Êàê‰∏∫ÊãÜÂàÜÂ≠¶‰π†ÁöÑ‰∏Ä‰∏™ÊúâÂâçÈÄîÁöÑË°çÁîü‰∫ßÂìÅÔºåÈùûÂ∏∏ÈÄÇÂêàÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äËøõË°åÂàÜÂ∏ÉÂºèÂ≠¶‰π†„ÄÇ
ÁÑ∂ËÄåÔºåPSL Èù¢‰∏¥ÁùÄ‰∏Ä‰∫õÈöúÁ¢çÔºå‰æãÂ¶ÇÊúâÊïàÊâπÊ¨°Â§ßÂ∞èÂ§ß„ÄÅÈùû IID Êï∞ÊçÆÂàÜÂ∏ÉÂíåÊéâÈòüËÄÖÊïàÂ∫î„ÄÇ
Êàë‰ª¨Â∞ÜËøô‰∫õÈóÆÈ¢òËßÜ‰∏∫ÊäΩÊ†∑ÈöæÈ¢òÔºåÂπ∂ÊèêÂá∫ÈÄöËøáÂú®ÊúçÂä°Âô®Á´ØÁºñÊéíÂ∞èÊâπÈáèÊäΩÊ†∑ËøáÁ®ãÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇ
Êàë‰ª¨ÂºïÂÖ•‰∫ÜÁªü‰∏ÄÂÖ®Â±ÄÈááÊ†∑ (UGS) ÊñπÊ≥ïÔºå‰ª•Â∞ÜÊúâÊïàÊâπÊ¨°Â§ßÂ∞è‰∏éÂÆ¢Êà∑Á´ØÊï∞ÈáèËß£ËÄ¶ÔºåÂπ∂ÂáèÂ∞ëÈùû IID ËÆæÁΩÆ‰∏≠ÁöÑÂ∞èÊâπÈáèÂÅèÂ∑Æ„ÄÇ
‰∏∫‰∫ÜËß£ÂÜ≥ÊéâÈòüËÄÖÊïàÂ∫îÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊΩúÂú®ÁãÑÂà©ÂÖãÈõ∑ÈááÊ†∑ (LDS) ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂ∞Ü UGS Ê¶ÇÊã¨‰∏∫Âπ≥Ë°°ÊâπÊ¨°ÂÅèÂ∑ÆÂíåËÆ≠ÁªÉÊó∂Èó¥‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ
Êàë‰ª¨ÁöÑÊ®°ÊãüË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∞ÜÈùû IID ËÆæÁΩÆ‰∏≠ÁöÑÊ®°ÂûãÂáÜÁ°ÆÂ∫¶ÊèêÈ´ò‰∫Ü 34.1%ÔºåÂπ∂Âú®Â≠òÂú®ÊéâÈòüËÄÖÁöÑÊÉÖÂÜµ‰∏ãÂ∞ÜËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë‰∫Ü 62%„ÄÇ
ÁâπÂà´ÊòØÔºå‰∏é UGS Áõ∏ÊØîÔºåLDS ÊúâÊïàÂú∞ÂáèËΩª‰∫ÜÊéâÈòüËÄÖÊïàÂ∫îÔºåËÄå‰∏ç‰ºöÊçüÂÆ≥Ê®°ÂûãÂáÜÁ°ÆÊÄßÊàñÂ¢ûÂä†ÊòæÁùÄÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ
Êàë‰ª¨ÁöÑÁªìÊûúËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ï‰Ωú‰∏∫ÂÆûÈôÖÂ∫îÁî®‰∏≠ DDL ÁöÑÊúâÂâçÈÄîÁöÑËß£ÂÜ≥ÊñπÊ°àÁöÑÊΩúÂäõ„ÄÇ

##### **OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context**
2407.15736v1 by Steffen Kleinle, Jakob Prange, Annemarie Friedrich

When immigrating to a new country, it is easy to feel overwhelmed by the need
to obtain information on financial support, housing, schooling, language
courses, and other issues. If relocation is rushed or even forced, the
necessity for high-quality answers to such questions is all the more urgent.
Official immigration counselors are usually overbooked, and online systems
could guide newcomers to the requested information or a suitable counseling
service.
  To this end, we present OMoS-QA, a dataset of German and English questions
paired with relevant trustworthy documents and manually annotated answers,
specifically tailored to this scenario. Questions are automatically generated
with an open-source large language model (LLM) and answer sentences are
selected by crowd workers with high agreement. With our data, we conduct a
comparison of 5 pretrained LLMs on the task of extractive question answering
(QA) in German and English. Across all models and both languages, we find high
precision and low-to-mid recall in selecting answer sentences, which is a
favorable trade-off to avoid misleading users. This performance even holds up
when the question language does not match the document language. When it comes
to identifying unanswerable questions given a context, there are larger
differences between the two languages.

ÊëòË¶ÅÔºö<paragraph>Âú®ÁßªÊ∞ëÂà∞‰∏Ä‰∏™Êñ∞ÂõΩÂÆ∂Êó∂ÔºåÂæàÂÆπÊòìÂõ†ÈúÄË¶ÅËé∑ÂæóÊúâÂÖ≥Ë¥¢Âä°ÊîØÊåÅ„ÄÅ‰ΩèÊàø„ÄÅÊïôËÇ≤„ÄÅËØ≠Ë®ÄËØæÁ®ãÂíåÂÖ∂‰ªñÈóÆÈ¢òÁöÑ‰ø°ÊÅØËÄåÊÑüÂà∞‰∏çÁü•ÊâÄÊé™„ÄÇÂ¶ÇÊûúÊê¨ËøÅ‰ªì‰øÉÁîöËá≥Ë¢´Ëø´ÔºåÈÇ£‰πàÂØπËøô‰∫õÈóÆÈ¢òÁöÑÈ´òË¥®ÈáèÁ≠îÊ°àÁöÑÈúÄÊ±ÇÂ∞±Êõ¥Âä†Ëø´Âàá„ÄÇÂÆòÊñπÁßªÊ∞ëÈ°æÈóÆÈÄöÂ∏∏ÈÉΩÂæàÂøôÔºåËÄåÂú®Á∫øÁ≥ªÁªüÂèØ‰ª•ÊåáÂØºÊñ∞ÁßªÊ∞ëËé∑ÂæóÊâÄÈúÄ‰ø°ÊÅØÊàñÂêàÈÄÇÁöÑÂí®ËØ¢ÊúçÂä°„ÄÇ
‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü OMoS-QAÔºåËøôÊòØ‰∏Ä‰∏™Âæ∑ËØ≠ÂíåËã±ËØ≠ÈóÆÈ¢òÊï∞ÊçÆÈõÜÔºåÈÖçÊúâÁõ∏ÂÖ≥ÁöÑÂèØ‰ø°Êñá‰ª∂ÂíåÊâãÂä®Ê≥®ÈáäÁöÑÁ≠îÊ°àÔºå‰∏ìÈó®ÈíàÂØπËøôÁßçÊÉÖÂÜµÈáèË∫´ÂÆöÂà∂„ÄÇÈóÆÈ¢òÊòØ‰ΩøÁî®ÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Ëá™Âä®ÁîüÊàêÁöÑÔºåÁ≠îÊ°àÂè•Â≠êÊòØÁî±È´òÂ∫¶‰∏ÄËá¥ÁöÑ‰ºóÂåÖÂ∑•‰ΩúËÄÖÈÄâÊã©ÁöÑ„ÄÇÂà©Áî®Êàë‰ª¨ÁöÑÊï∞ÊçÆÔºåÊàë‰ª¨ÂØπ 5 ‰∏™È¢ÑËÆ≠ÁªÉ LLM Âú®Âæ∑ËØ≠ÂíåËã±ËØ≠ÁöÑÊäΩÂèñÂºèÈóÆÈ¢òËß£Á≠î (QA) ‰ªªÂä°‰∏äËøõË°å‰∫ÜÊØîËæÉ„ÄÇÂú®ÊâÄÊúâÊ®°ÂûãÂíå‰∏§ÁßçËØ≠Ë®Ä‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞ÈÄâÊã©Á≠îÊ°àÂè•Â≠êÁöÑÈ´òÁ≤æÂ∫¶Âíå‰∏≠‰ΩéÂè¨ÂõûÁéáÔºåËøôÊòØÈÅøÂÖçËØØÂØºÁî®Êà∑ÁöÑÊúâÂà©ÊùÉË°°„ÄÇÂç≥‰ΩøÈóÆÈ¢òËØ≠Ë®Ä‰∏éÊñáÊ°£ËØ≠Ë®Ä‰∏çÂåπÈÖçÔºåÊ≠§ÊÄßËÉΩ‰ªçÁÑ∂ÊàêÁ´ã„ÄÇÂΩìÈúÄË¶ÅÂú®ÁªôÂÆö‰∏ä‰∏ãÊñá‰∏≠ËØÜÂà´Êó†Ê≥ïÂõûÁ≠îÁöÑÈóÆÈ¢òÊó∂Ôºå‰∏§ÁßçËØ≠Ë®Ä‰πãÈó¥Â≠òÂú®ËæÉÂ§ßÂ∑ÆÂºÇ„ÄÇ</paragraph>

##### **TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**
2407.15734v1 by John Chong Min Tan, Prince Saroj, Bharat Runwal, Hardik Maheshwari, Brian Lim Yi Sheng, Richard Cottrill, Alankrit Chona, Ambuj Kumar, Mehul Motani

TaskGen is an open-sourced agentic framework which uses an Agent to solve an
arbitrary task by breaking them down into subtasks. Each subtask is mapped to
an Equipped Function or another Agent to execute. In order to reduce verbosity
(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from
the Large Language Model (LLM), along with additional features such as type
checking and iterative error correction. Key to the philosophy of TaskGen is
the management of information/memory on a need-to-know basis. We empirically
evaluate TaskGen on various environments such as 40x40 dynamic maze navigation
with changing obstacle locations (100% solve rate), TextWorld escape room
solving with dense rewards and detailed goals (96% solve rate), web browsing
(69% of actions successful), solving the MATH dataset (71% solve rate over 100
Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset
(F1 score of 47.03%)

ÊëòË¶ÅÔºöTaskGen ÊòØ‰∏ÄÂÄãÈñãÊ∫êÁöÑ‰ª£ÁêÜÊ°ÜÊû∂ÔºåÂÆÉ‰ΩøÁî®‰∏ÄÂÄã‰ª£ÁêÜ‰æÜËß£Ê±∫‰ªªÊÑèÁöÑ‰ªªÂãôÔºåÊñπÊ≥ïÊòØÂ∞áÂÆÉÂÄëÂàÜËß£ÊàêÂ≠ê‰ªªÂãô„ÄÇÊØèÂÄãÂ≠ê‰ªªÂãôÈÉΩÂ∞çÊáâÂà∞‰∏ÄÂÄãË£ùÂÇôÂäüËÉΩÊàñÂè¶‰∏ÄÂÄã‰ª£ÁêÜ‰æÜÂü∑Ë°å„ÄÇÁÇ∫‰∫ÜÊ∏õÂ∞ëÂÜóÈï∑ÔºàÂõ†Ê≠§Ê∏õÂ∞ë‰ª§Áâå‰ΩøÁî®ÔºâÔºåTaskGen ‰ΩøÁî® StrictJSONÔºåÂÆÉÁ¢∫‰øùÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ JSON Ëº∏Âá∫Ôºå‰ª•ÂèäÈ°ûÂûãÊ™¢Êü•ÂíåËø≠‰ª£ÈåØË™§‰øÆÊ≠£Á≠âÈ°çÂ§ñÂäüËÉΩ„ÄÇTaskGen ÁöÑÂì≤Â≠∏ÈóúÈçµÂú®ÊñºÊ†πÊìöÈúÄË¶ÅÁü•ÈÅì‰æÜÁÆ°ÁêÜË≥áË®ä/Ë®òÊÜ∂È´î„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÁí∞Â¢É‰∏≠Â∞ç TaskGen ÈÄ≤Ë°åÁ∂ìÈ©óË©ï‰º∞Ôºå‰æãÂ¶Ç 40x40 ÂãïÊÖãËø∑ÂÆÆÂ∞éËà™ÔºåÈöúÁ§ôÁâ©‰ΩçÁΩÆÊúÉÊîπËÆäÔºà100% Ëß£Ê±∫ÁéáÔºâ„ÄÅTextWorld ÂØÜÂÆ§ÈÄÉËÑ´ÔºåÊúâÂØÜÈõÜÁöÑÁçéÂãµÂíåË©≥Á¥∞ÁöÑÁõÆÊ®ôÔºà96% Ëß£Ê±∫ÁéáÔºâ„ÄÅÁ∂≤È†ÅÁÄèË¶ΩÔºà69% ÁöÑÂãï‰ΩúÊàêÂäüÔºâ„ÄÅËß£Ê±∫ MATH Ë≥áÊñôÈõÜÔºàË∂ÖÈÅé 100 ÂÄã 5 Á¥öÂïèÈ°åÁöÑ 71% Ëß£Ê±∫ÁéáÔºâ„ÄÅËá™ÁÑ∂ÂïèÈ°åË≥áÊñôÈõÜ‰∏äÁöÑÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàêÔºàF1 ÂàÜÊï∏ÁÇ∫ 47.03%Ôºâ

##### **DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design**
2407.15723v1 by Zhi Hao Luo, Luis Lara, Ge Ya Luo, Florian Golemo, Christopher Beckham, Christopher Pal

Text conditioned generative models for images have yielded impressive
results. Text conditioned floorplan generation as a special type of raster
image generation task also received particular attention. However there are
many use cases in floorpla generation where numerical properties of the
generated result are more important than the aesthetics. For instance, one
might want to specify sizes for certain rooms in a floorplan and compare the
generated floorplan with given specifications Current approaches, datasets and
commonly used evaluations do not support these kinds of constraints. As such,
an attractive strategy is to generate an intermediate data structure that
contains numerical properties of a floorplan which can be used to generate the
final floorplan image. To explore this setting we (1) construct a new dataset
for this data-structure to data-structure formulation of floorplan generation
using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and
provide the tools to convert further procedurally generated ProcTHOR floorplan
data into our format. (2) We explore the task of floorplan generation given a
partial or complete set of constraints and we design a series of metrics and
benchmarks to enable evaluating how well samples generated from models respect
the constraints. (3) We create multiple baselines by finetuning a large
language model (LLM), Llama3, and demonstrate the feasibility of using
floorplan data structure conditioned LLMs for the problem of floorplan
generation respecting numerical constraints. We hope that our new datasets and
benchmarks will encourage further research on different ways to improve the
performance of LLMs and other generative modelling techniques for generating
designs where quantitative constraints are only partially specified, but must
be respected.

ÊëòË¶ÅÔºöÊñáÊú¨Êù°‰ª∂ÁîüÊàêÊ®°ÂûãÂ∑≤‰∏∫ÂõæÂÉè‰∫ßÁîü‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÁªìÊûú„ÄÇÊñáÊú¨Êù°‰ª∂Âπ≥Èù¢ÂõæÁîüÊàê‰Ωú‰∏∫ÂÖâÊ†ÖÂõæÂÉèÁîüÊàê‰ªªÂä°ÁöÑ‰∏ÄÁßçÁâπÊÆäÁ±ªÂûã‰πüÂèóÂà∞‰∫ÜÁâπÂà´ÁöÑÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÂú®Âπ≥Èù¢ÂõæÁîüÊàê‰∏≠ÔºåÁîüÊàêÁªìÊûúÁöÑÊï∞Â≠óÂ±ûÊÄßÊØîÁæéÂ≠¶Êõ¥ÈáçË¶ÅÁöÑÁî®‰æãÊúâÂæàÂ§ö„ÄÇ‰æãÂ¶ÇÔºå‰∫∫‰ª¨ÂèØËÉΩÂ∏åÊúõÊåáÂÆöÂπ≥Èù¢Âõæ‰∏≠Êüê‰∫õÊàøÈó¥ÁöÑÂ§ßÂ∞èÔºåÂπ∂Â∞ÜÁîüÊàêÁöÑÂπ≥Èù¢Âõæ‰∏éÁªôÂÆöÁöÑËßÑÊ†ºËøõË°åÊØîËæÉ„ÄÇÂΩìÂâçÁöÑÊñπÊ≥ï„ÄÅÊï∞ÊçÆÈõÜÂíåÂ∏∏Áî®ÁöÑËØÑ‰º∞‰∏çÊîØÊåÅËøô‰∫õÁ±ªÂûãÁöÑÁ∫¶Êùü„ÄÇÂõ†Ê≠§Ôºå‰∏Ä‰∏™ÊúâÂê∏ÂºïÂäõÁöÑÁ≠ñÁï•ÊòØÁîüÊàê‰∏Ä‰∏™ÂåÖÂê´Âπ≥Èù¢ÂõæÊï∞Â≠óÂ±ûÊÄßÁöÑ‰∏≠Èó¥Êï∞ÊçÆÁªìÊûÑÔºåËØ•Êï∞ÊçÆÁªìÊûÑÂèØÁî®‰∫éÁîüÊàêÊúÄÁªàÁöÑÂπ≥Èù¢ÂõæÂõæÂÉè„ÄÇ‰∏∫‰∫ÜÊé¢Á¥¢Ê≠§ËÆæÁΩÆÔºåÊàë‰ª¨ (1) ‰∏∫Ê≠§Êï∞ÊçÆÁªìÊûÑÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•‰ΩøÁî®‰∏§‰∏™ÊµÅË°åÁöÑÂü∫‰∫éÂõæÂÉèÁöÑÂπ≥Èù¢ÂõæÊï∞ÊçÆÈõÜ RPLAN Âíå ProcTHOR-10k ÂØπÂπ≥Èù¢ÂõæÁîüÊàêËøõË°åÊï∞ÊçÆÁªìÊûÑÂà∞Êï∞ÊçÆÁªìÊûÑÁöÑË°®Ëø∞ÔºåÂπ∂Êèê‰æõÂ∞ÜËøõ‰∏ÄÊ≠•Á®ãÂ∫èÁîüÊàêÁöÑ ProcTHOR Âπ≥Èù¢ÂõæÊï∞ÊçÆËΩ¨Êç¢‰∏∫Êàë‰ª¨Ê†ºÂºèÁöÑÂ∑•ÂÖ∑„ÄÇ(2) Êàë‰ª¨Êé¢Á¥¢‰∫ÜÂú®ÁªôÂÆöÈÉ®ÂàÜÊàñÂÆåÊï¥Á∫¶ÊùüÈõÜÁöÑÊÉÖÂÜµ‰∏ãÂπ≥Èù¢ÂõæÁîüÊàêÁöÑ ‰ªªÂä°ÔºåÂπ∂‰∏îÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÊåáÊ†áÂíåÂü∫ÂáÜÊù•ËØÑ‰º∞‰ªéÊ®°ÂûãÁîüÊàêÁöÑÊ†∑Êú¨Â¶Ç‰ΩïÂ∞äÈáçÁ∫¶Êùü„ÄÇ(3) Êàë‰ª¨ÈÄöËøáÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Llama3 Êù•ÂàõÂª∫Â§ö‰∏™Âü∫Á∫øÔºåÂπ∂Â±ïÁ§∫‰∫Ü‰ΩøÁî®Âπ≥Èù¢ÂõæÊï∞ÊçÆÁªìÊûÑÊù°‰ª∂ LLM Ëß£ÂÜ≥Âπ≥Èù¢ÂõæÁîüÊàêÈóÆÈ¢ò‰ª•Â∞äÈáçÊï∞Â≠óÁ∫¶ÊùüÁöÑÂèØË°åÊÄß„ÄÇÊàë‰ª¨Â∏åÊúõÊàë‰ª¨ÁöÑÊñ∞Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÂ∞ÜÈºìÂä±ÂØπÊèêÈ´ò LLM ÂíåÂÖ∂‰ªñÁîüÊàêÂª∫Ê®°ÊäÄÊúØÊÄßËÉΩÁöÑ‰∏çÂêåÊñπÂºèËøõË°åËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Ôºå‰ª•ÁîüÊàêÊï∞ÈáèÁ∫¶Êùü‰ªÖÈÉ®ÂàÜÊåáÂÆö‰ΩÜÂøÖÈ°ªÂ∞äÈáçÁöÑËÆæËÆ°„ÄÇ

##### **Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability**
2407.15720v1 by Zhuoyan Xu, Zhenmei Shi, Yingyu Liang

Large language models (LLMs) have emerged as powerful tools for many AI
problems and exhibit remarkable in-context learning (ICL) capabilities.
Compositional ability, solving unseen complex tasks that combine two or more
simple tasks, is an essential reasoning ability for Artificial General
Intelligence. Despite LLM's tremendous success, how they approach composite
tasks, especially those not encountered during the pretraining phase, remains
an open question and largely ununderstood. In this study, we delve into the ICL
capabilities of LLMs on composite tasks, with only simple tasks as in-context
examples. We develop a test suite of composite tasks that include linguistic
and logical challenges and perform empirical studies across different LLM
families. We observe that models exhibit divergent behaviors: (1) For simpler
composite tasks that apply distinct mapping mechanisms to different input
segments, the models demonstrate decent compositional ability, while scaling up
the model enhances this ability; (2) for more complex composite tasks that
involving reasoning multiple steps, where each step represent one task, models
typically underperform, and scaling up generally provide no improvements. We
offer theoretical analysis in a simplified setting, explaining that models
exhibit compositional capability when the task handles different input parts
separately. We believe our work sheds new light on the capabilities of LLMs in
solving composite tasks regarding the nature of the tasks and model scale. Our
dataset and code are available at
{\url{https://github.com/OliverXUZY/LLM_Compose}}.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫Ë®±Â§ö‰∫∫Â∑•Êô∫ÊÖßÂïèÈ°åÁöÑÂº∑Â§ßÂ∑•ÂÖ∑Ôºå‰∏¶Â±ïÁèæÂá∫È°ØËëóÁöÑË™ûÂ¢ÉÂ≠∏Áøí (ICL) ËÉΩÂäõ„ÄÇÁµÑÂêàËÉΩÂäõÔºåËß£Ê±∫ÁµêÂêàÂÖ©ÂÄãÊàñÊõ¥Â§öÁ∞°ÂñÆ‰ªªÂãôÁöÑÊú™Ë¶ãÈÅéË§áÈõú‰ªªÂãôÔºåÊòØ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖßÁöÑÂøÖË¶ÅÊé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ° LLM Áç≤ÂæóÂ∑®Â§ßÊàêÂäüÔºå‰ΩÜÂÆÉÂÄëÂ¶Ç‰ΩïËôïÁêÜË§áÂêà‰ªªÂãôÔºåÁâπÂà•ÊòØÂú®È†êË®ìÁ∑¥ÈöéÊÆµÊú™ÈÅáÂà∞ÁöÑ‰ªªÂãôÔºå‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊú™Ëß£‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂ∞öÊú™ÁêÜËß£ÁöÑÂïèÈ°å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®é LLM Âú®Ë§áÂêà‰ªªÂãô‰∏äÁöÑ ICL ËÉΩÂäõÔºåÂÉÖ‰ΩøÁî®Á∞°ÂñÆ‰ªªÂãô‰ΩúÁÇ∫Ë™ûÂ¢ÉÁØÑ‰æã„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂ•óÂåÖÂê´Ë™ûË®ÄÂíåÈÇèËºØÊåëÊà∞ÁöÑË§áÂêà‰ªªÂãôÊ∏¨Ë©¶Â•ó‰ª∂Ôºå‰∏¶Â∞ç‰∏çÂêåÁöÑ LLM ÂÆ∂ÊóèÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂„ÄÇÊàëÂÄëËßÄÂØüÂà∞Ê®°ÂûãË°®ÁèæÂá∫‰∏çÂêåÁöÑË°åÁÇ∫Ôºö(1) Â∞çÊñºÂ∞á‰∏çÂêåÁöÑÊò†Â∞ÑÊ©üÂà∂ÊáâÁî®Êñº‰∏çÂêåËº∏ÂÖ•ÂçÄÊÆµÁöÑËºÉÁ∞°ÂñÆÁöÑË§áÂêà‰ªªÂãôÔºåÊ®°ÂûãË°®ÁèæÂá∫ËâØÂ•ΩÁöÑÁµÑÂêàËÉΩÂäõÔºåËÄåÊì¥Â§ßÊ®°ÂûãÊúÉÂ¢ûÂº∑ÈÄôÁ®ÆËÉΩÂäõÔºõ(2) Â∞çÊñºÊ∂âÂèäÂ§öÂÄãÊ≠•È©üÊé®ÁêÜÁöÑÊõ¥Ë§áÈõúÁöÑË§áÂêà‰ªªÂãôÔºåÂÖ∂‰∏≠ÊØèÂÄãÊ≠•È©ü‰ª£Ë°®‰∏ÄÂÄã‰ªªÂãôÔºåÊ®°ÂûãÈÄöÂ∏∏Ë°®Áèæ‰∏ç‰Ω≥ÔºåËÄåÊì¥Â§ßË¶èÊ®°ÈÄöÂ∏∏‰∏çÊúÉÂ∏∂‰æÜ‰ªª‰ΩïÊîπÈÄ≤„ÄÇÊàëÂÄëÂú®Á∞°ÂåñÁöÑÁí∞Â¢É‰∏≠Êèê‰æõÁêÜË´ñÂàÜÊûêÔºåË™™ÊòéÁï∂‰ªªÂãôÂ∞á‰∏çÂêåÁöÑËº∏ÂÖ•ÈÉ®ÂàÜÂàÜÈñãËôïÁêÜÊôÇÔºåÊ®°ÂûãÊúÉË°®ÁèæÂá∫ÁµÑÂêàËÉΩÂäõ„ÄÇÊàëÂÄëÁõ∏‰ø°ÊàëÂÄëÁöÑÁ†îÁ©∂ÁÇ∫ LLM Âú®Ëß£Ê±∫Ë§áÂêà‰ªªÂãôÊñπÈù¢ÁöÑËÉΩÂäõÊèê‰æõ‰∫ÜÊñ∞ÁöÑË¶ãËß£ÔºåÊ∂âÂèä‰ªªÂãôÁöÑÊÄßË≥™ÂíåÊ®°ÂûãË¶èÊ®°„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÂèØÂú®
{\url{https://github.com/OliverXUZY/LLM_Compose}} ÂèñÂæó„ÄÇ

##### **GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI**
2407.15719v1 by Zhaojie Fang, Shenghao Zhu, Yifei Chen, Binfeng Zou, Fan Jia, Linwei Qiu, Chang Liu, Yiyu Huang, Xiang Feng, Feiwei Qin, Changmiao Wang, Yeru Wang, Jin Fan, Changbiao Chu, Wan-Zhen Wu, Hu Zhao

Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that
often progresses from Mild Cognitive Impairment (MCI), leading to memory loss
and significantly impacting patients' lives. Clinical trials indicate that
early targeted interventions for MCI patients can potentially slow or halt the
development and progression of AD. Previous research has shown that accurate
medical classification requires the inclusion of extensive multimodal data,
such as assessment scales and various neuroimaging techniques like Magnetic
Resonance Imaging (MRI) and Positron Emission Tomography (PET). However,
consistently tracking the diagnosis of the same individual over time and
simultaneously collecting multimodal data poses significant challenges. To
address this issue, we introduce GFE-Mamba, a classifier based on Generative
Feature Extraction (GFE). This classifier effectively integrates data from
assessment scales, MRI, and PET, enabling deeper multimodal fusion. It
efficiently extracts both long and short sequence information and incorporates
additional information beyond the pixel space. This approach not only improves
classification accuracy but also enhances the interpretability and stability of
the model. We constructed datasets of over 3000 samples based on the
Alzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training
process. Our experimental results demonstrate that the GFE-Mamba model is
effective in predicting the conversion from MCI to AD and outperforms several
state-of-the-art methods. Our source code and ADNI dataset processing code are
available at https://github.com/Tinysqua/GFE-Mamba.

ÊëòË¶ÅÔºöÈòøËå≤Êµ∑ÈªòÁóá (AD) ÊòØ‰∏ÄÁ®Æ‰∏çÂèØÈÄÜÁöÑÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºå
ÈÄöÂ∏∏ÊúÉÂæûËºïÂ∫¶Ë™çÁü•ÈöúÁ§ô (MCI) ÊÉ°ÂåñÔºåÂ∞éËá¥Ë®òÊÜ∂ÂäõÊ∏õÈÄÄ
‰∏¶Â∞çÁóÖÊÇ£ÁöÑÁîüÊ¥ªÁî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇËá®Â∫äË©¶È©óÈ°ØÁ§∫Ôºå
ÈáùÂ∞ç MCI ÁóÖÊÇ£ÁöÑÊó©ÊúüÁõÆÊ®ôÊÄßÂπ≤È†êÊé™ÊñΩÔºåÊúâÂèØËÉΩÊ∏õÁ∑©ÊàñÂÅúÊ≠¢
AD ÁöÑÁôºÂ±ïÂíåÊÉ°Âåñ„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåÁ≤æÁ¢∫ÁöÑ
ÈÜ´ÁôÇÂàÜÈ°ûÈúÄË¶ÅÁ¥çÂÖ•Âª£Ê≥õÁöÑÂ§öÊ®°ÂºèÊï∏ÊìöÔºå
‰æãÂ¶ÇË©ï‰º∞ÈáèË°®ÂíåÂêÑÁ®ÆÁ•ûÁ∂ìÂΩ±ÂÉèÊäÄË°ìÔºåÂ¶ÇÁ£ÅÊåØÈÄ†ÂΩ± (MRI) ÂíåÊ≠£Â≠êÊñ∑Â±§ÊéÉÊèè (PET)„ÄÇÁÑ∂ËÄåÔºå
ÊåÅÁ∫åËøΩËπ§Âêå‰∏Ä‰ΩçÂÄãÈ´îÁöÑË®∫Êñ∑ÁµêÊûúÔºå‰∏¶ÂêåÊôÇÊî∂ÈõÜÂ§öÊ®°ÂºèÊï∏ÊìöÔºåÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫Ü
Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GFE-MambaÔºå‰∏ÄÁ®ÆÂü∫ÊñºÁîüÊàêÁâπÂæµËêÉÂèñ (GFE) ÁöÑÂàÜÈ°ûÂô®„ÄÇÊ≠§ÂàÜÈ°ûÂô®ÊúâÊïàÊï¥Âêà‰æÜËá™
Ë©ï‰º∞ÈáèË°®„ÄÅMRI Âíå PET ÁöÑÊï∏ÊìöÔºåÂØ¶ÁèæÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÂºèËûçÂêà„ÄÇÂÆÉ
ÊúâÊïàÁéáÂú∞ËêÉÂèñÂá∫Èï∑Â∫èÂàóÂíåÁü≠Â∫èÂàóË≥áË®äÔºå‰∏¶Á¥çÂÖ•ÂÉèÁ¥†Á©∫Èñì‰ª•Â§ñÁöÑÈ°çÂ§ñË≥áË®ä„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰∏çÂÉÖÊèêÂçá‰∫Ü
ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶Ôºå‰πüÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂíåÁ©©ÂÆöÊÄß„ÄÇÊàëÂÄëÊ†πÊìö
ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ÁµÑÁπî (ADNI) Âª∫Á´ã‰∫ÜË∂ÖÈÅé 3000 ÂÄãÊ®£Êú¨ÁöÑË≥áÊñôÈõÜÔºåÁî®ÊñºÂÖ©ÈöéÊÆµË®ìÁ∑¥
ÈÅéÁ®ã„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåGFE-Mamba Ê®°ÂûãÂú®È†êÊ∏¨Âæû MCI ËΩâËÆäÁÇ∫ AD ‰∏äÊòØÊúâÊïàÁöÑÔºå‰∏¶‰∏îÂÑ™ÊñºÂ§öÁ®Æ
ÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ®ãÂºèÁ¢ºÂíå ADNI Ë≥áÊñôÈõÜËôïÁêÜÁ®ãÂºèÁ¢ºÂèØÊñº https://github.com/Tinysqua/GFE-Mamba ÂèñÂæó„ÄÇ

##### **Mamba meets crack segmentation**
2407.15714v1 by Zhili He, Yu-Hsing Wang

Cracks pose safety risks to infrastructure and cannot be overlooked. The
prevailing structures in existing crack segmentation networks predominantly
consist of CNNs or Transformers. However, CNNs exhibit a deficiency in global
modeling capability, hindering the representation to entire crack features.
Transformers can capture long-range dependencies but suffer from high and
quadratic complexity. Recently, Mamba has garnered extensive attention due to
its linear spatial and computational complexity and its powerful global
perception. This study explores the representation capabilities of Mamba to
crack features. Specifically, this paper uncovers the connection between Mamba
and the attention mechanism, providing a profound insight, an attention
perspective, into interpreting Mamba and devising a novel Mamba module
following the principles of attention blocks, namely CrackMamba. We compare
CrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two
datasets comprising asphalt pavement and concrete pavement cracks, and steel
cracks, respectively. The quantitative results show that CrackMamba stands out
as the sole Mamba block consistently enhancing the baseline model's performance
across all evaluation measures, while reducing its parameters and computational
costs. Moreover, this paper substantiates that Mamba can achieve global
receptive fields through both theoretical analysis and visual interpretability.
The discoveries of this study offer a dual contribution. First, as a
plug-and-play and simple yet effective Mamba module, CrackMamba exhibits
immense potential for integration into various crack segmentation models.
Second, the proposed innovative Mamba design concept, integrating Mamba with
the attention mechanism, holds significant reference value for all Mamba-based
computer vision models, not limited to crack segmentation networks, as
investigated in this study.

ÊëòË¶ÅÔºö<paragraph>Ë£ÇÁ∏´ÊúÉÂ∞çÂü∫Á§éË®≠ÊñΩÊßãÊàêÂÆâÂÖ®È¢®Èö™Ôºå‰∏çÂèØÂøΩË¶ñ„ÄÇ
ÁèæÊúâË£ÇÁ∏´ÂàÜÂâ≤Á∂≤Ë∑Ø‰∏≠ÁöÑ‰∏ªË¶ÅÁµêÊßã‰∏ªË¶ÅÁî± CNN Êàñ Transformer ÁµÑÊàê„ÄÇÁÑ∂ËÄåÔºåCNN Âú®ÂÖ®Â±ÄÂª∫Ê®°ËÉΩÂäõÊñπÈù¢Â≠òÂú®Áº∫Èô∑ÔºåÈòªÁ§ô‰∫ÜÂ∞çÊï¥ÂÄãË£ÇÁ∏´ÁâπÂæµÁöÑË°®Á§∫„ÄÇTransformer ÂèØ‰ª•ÊçïÊçâÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇÔºå‰ΩÜÊúÉÂ∏∂‰æÜÈ´ò‰∫åÊ¨°Ë§áÈõúÂ∫¶„ÄÇÊúÄËøëÔºåMamba Âõ†ÂÖ∂Á∑öÊÄßÁ©∫ÈñìÂíåË®àÁÆóË§áÈõúÂ∫¶‰ª•ÂèäÂº∑Â§ßÁöÑÂÖ®Â±ÄÊÑüÁü•ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü Mamba Â∞çË£ÇÁ∏´ÁâπÂæµÁöÑË°®Á§∫ËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊú¨ÊñáÊè≠Á§∫‰∫Ü Mamba ËàáÊ≥®ÊÑèÂäõÊ©üÂà∂‰πãÈñìÁöÑËÅØÁπ´ÔºåÊèê‰æõ‰∫ÜÊ∑±ÂàªÁöÑË¶ãËß£Ôºå‰∏ÄÁ®ÆÊ≥®ÊÑèÂäõË¶ñËßíÔºåÁî®ÊñºËß£Èáã Mamba ‰∏¶Ë®≠Ë®à‰∏ÄÂÄãÈÅµÂæ™Ê≥®ÊÑèÂäõÂçÄÂ°äÂéüÁêÜÁöÑÊñ∞ Mamba Ê®°Â°äÔºåÂç≥ CrackMamba„ÄÇÊàëÂÄëÂú®ÂåÖÂê´ÁÄùÈùíË∑ØÈù¢ÂíåÊ∑∑ÂáùÂúüË∑ØÈù¢Ë£ÇÁ∏´‰ª•ÂèäÈãºÈêµË£ÇÁ∏´ÁöÑÂÖ©ÂÄãÊï∏ÊìöÈõÜ‰∏äÂ∞á CrackMamba ËàáÊúÄÁ™ÅÂá∫ÁöÑË¶ñË¶∫ Mamba Ê®°Â°ä Vim Âíå Vmamba ÈÄ≤Ë°å‰∫ÜÊØîËºÉ„ÄÇÂÆöÈáèÁµêÊûúË°®ÊòéÔºåCrackMamba ËÑ´Á©éËÄåÂá∫ÔºåÊàêÁÇ∫ÂîØ‰∏ÄÂßãÁµÇÂ¶Ç‰∏ÄÂú∞ÊèêÈ´òÂü∫Á∑öÊ®°ÂûãÂú®ÊâÄÊúâË©ï‰º∞ÊåáÊ®ô‰∏äÁöÑÊÄßËÉΩÁöÑ Mamba ÂçÄÂ°äÔºåÂêåÊôÇÊ∏õÂ∞ëÂÖ∂ÂèÉÊï∏ÂíåË®àÁÆóÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÈÄöÈÅéÁêÜË´ñÂàÜÊûêÂíåË¶ñË¶∫ÂèØËß£ÈáãÊÄßË≠âÂØ¶‰∫Ü Mamba ÂèØ‰ª•ÂØ¶ÁèæÂÖ®Â±ÄÊÑüÂèóÈáé„ÄÇÊú¨Á†îÁ©∂ÁöÑÁôºÁèæÊèê‰æõ‰∫ÜÈõôÈáçË≤¢Áçª„ÄÇÈ¶ñÂÖàÔºå‰ΩúÁÇ∫‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®‰∏îÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑ Mamba Ê®°Â°äÔºåCrackMamba Â±ïÁ§∫‰∫ÜÂ∑®Â§ßÁöÑÊΩõÂäõÔºåÂèØ‰ª•Êï¥ÂêàÂà∞ÂêÑÁ®ÆË£ÇÁ∏´ÂàÜÂâ≤Ê®°Âûã‰∏≠„ÄÇÂÖ∂Ê¨°ÔºåÊèêÂá∫ÁöÑÂâµÊñ∞ÁöÑ Mamba Ë®≠Ë®àÊ¶ÇÂøµÔºåÂ∞á Mamba ËàáÊ≥®ÊÑèÂäõÊ©üÂà∂Áõ∏ÁµêÂêàÔºåÂ∞çÊâÄÊúâÂü∫Êñº Mamba ÁöÑË®àÁÆóÊ©üË¶ñË¶∫Ê®°ÂûãÂÖ∑ÊúâÈáçË¶ÅÁöÑÂèÉËÄÉÂÉπÂÄºÔºå‰∏çÂÉÖÈôêÊñºË£ÇÁ∏´ÂàÜÂâ≤Á∂≤Ë∑ØÔºåÂ¶ÇÊú¨Á†îÁ©∂‰∏≠ÊâÄÊé¢Ë®éÁöÑ„ÄÇ</paragraph>

##### **AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?**
2407.15711v1 by Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, Jonathan Berant

Language agents, built on top of language models (LMs), are systems that can
interact with complex environments, such as the open web. In this work, we
examine whether such agents can perform realistic and time-consuming tasks on
the web, e.g., monitoring real-estate markets or locating relevant nearby
businesses. We introduce AssistantBench, a challenging new benchmark consisting
of 214 realistic tasks that can be automatically evaluated, covering different
scenarios and domains. We find that AssistantBench exposes the limitations of
current systems, including language models and retrieval-augmented language
models, as no model reaches an accuracy of more than 25 points. While
closed-book LMs perform well, they exhibit low precision since they tend to
hallucinate facts. State-of-the-art web agents reach a score of near zero.
Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly
outperforms previous agents, and an ensemble of SPA and closed-book models
reaches the best overall performance. Moreover, we analyze failures of current
systems and highlight that web navigation remains a major challenge.

ÊëòË¶ÅÔºöÂª∫ÊßãÊñºË™ûË®ÄÊ®°Âûã (LM) ‰πã‰∏äÁöÑË™ûË®Ä‰ª£ÁêÜÔºåÊòØËÉΩËàáË§áÈõúÁí∞Â¢ÉÔºàÂ¶ÇÈñãÊîæÁ∂≤Ë∑ØÔºâ‰∫íÂãïÁöÑÁ≥ªÁµ±„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊ≠§È°û‰ª£ÁêÜÊòØÂê¶ËÉΩÂú®Á∂≤Ë∑Ø‰∏äÂü∑Ë°åÂØ¶Èöõ‰∏îËÄóÊôÇÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÁõ£ÊéßÊàøÂú∞Áî¢Â∏ÇÂ†¥ÊàñÂ∞ãÊâæÈÑ∞ËøëÁõ∏ÈóúÂïÜÂÆ∂„ÄÇÊàëÂÄëÂºïÈÄ≤‰∫Ü AssistantBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± 214 ÂÄãÂØ¶Èöõ‰ªªÂãôÁµÑÊàêÁöÑÂÖ∑ÊåëÊà∞ÊÄßÊñ∞Âü∫Ê∫ñÔºåÂèØËá™ÂãïË©ï‰º∞ÔºåÊ∂µËìã‰∏çÂêåÁöÑÊÉÖÂ¢ÉÂíåÈ†òÂüü„ÄÇÊàëÂÄëÁôºÁèæ AssistantBench Êè≠Èú≤‰∫ÜÁï∂ÂâçÁ≥ªÁµ±ÁöÑÈôêÂà∂ÔºåÂåÖÊã¨Ë™ûË®ÄÊ®°ÂûãÂíåÊ™¢Á¥¢Â¢ûÂº∑Ë™ûË®ÄÊ®°ÂûãÔºåÂõ†ÁÇ∫Ê≤íÊúâ‰ªª‰ΩïÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÁéáË∂ÖÈÅé 25 ÂàÜ„ÄÇÈõñÁÑ∂ÈñâÂç∑ LM Ë°®ÁèæËâØÂ•ΩÔºå‰ΩÜÂÆÉÂÄëÁöÑÁ≤æÁ¢∫Â∫¶Âæà‰ΩéÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂÇæÂêëÊñºËôõÊßã‰∫ãÂØ¶„ÄÇÊúÄÂÖàÈÄ≤ÁöÑÁ∂≤Ë∑Ø‰ª£ÁêÜÈÅîÂà∞Êé•ËøëÈõ∂ÁöÑÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü SeePlanAct (SPA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÊòéÈ°ØÂÑ™ÊñºÂÖàÂâç‰ª£ÁêÜÁöÑÊñ∞Á∂≤Ë∑Ø‰ª£ÁêÜÔºåËÄå SPA ÂíåÈñâÂç∑Ê®°ÂûãÁöÑÊï¥È´îË°®ÁèæÈÅîÂà∞ÊúÄ‰Ω≥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÁï∂ÂâçÁ≥ªÁµ±ÁöÑÂ§±ÊïóÔºå‰∏¶Âº∑Ë™øÁ∂≤Ë∑ØÂ∞éËà™‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇ

##### **SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams**
2407.15708v1 by Liangyan Jiang, Chuang Zhu, Yanxu Chen

The spike camera, with its high temporal resolution, low latency, and high
dynamic range, addresses high-speed imaging challenges like motion blur. It
captures photons at each pixel independently, creating binary spike streams
rich in temporal information but challenging for image reconstruction. Current
algorithms, both traditional and deep learning-based, still need to be improved
in the utilization of the rich temporal detail and the restoration of the
details of the reconstructed image. To overcome this, we introduce Swin
Spikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike
streams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal
Feature Extraction, and Final Reconstruction Module. It combines shifted window
self-attention and proposed temporal spike attention, ensuring a comprehensive
feature extraction that encapsulates both spatial and temporal dynamics,
leading to a more robust and accurate reconstruction of spike streams.
Furthermore, we build a new synthesized dataset for spike image reconstruction
which matches the resolution of the latest spike camera, ensuring its relevance
and applicability to the latest developments in spike camera imaging.
Experimental results demonstrate that the proposed network SwinSF sets a new
benchmark, achieving state-of-the-art performance across a series of datasets,
including both real-world and synthesized data across various resolutions. Our
codes and proposed dataset will be available soon.

ÊëòË¶ÅÔºöÂ∞ñÂ≥∞Áõ∏Ê©ü‰ª•ÂÖ∂È´òÊôÇÈñìËß£ÊûêÂ∫¶„ÄÅ‰ΩéÂª∂ÈÅ≤ÂíåÈ´òÂãïÊÖãÁØÑÂúçÔºåËß£Ê±∫‰∫ÜÈÅãÂãïÊ®°Á≥äÁ≠âÈ´òÈÄüÂΩ±ÂÉèÊåëÊà∞„ÄÇÂÆÉÁç®Á´ãÊçïÊçâÊØèÂÄãÁï´Á¥†ÁöÑÂÖâÂ≠êÔºåÁî¢ÁîüÂØåÂê´ÊôÇÈñìË≥áË®äÁöÑ‰∫åÈÄ≤‰ΩçÂ∞ñÂ≥∞‰∏≤ÊµÅÔºå‰ΩÜÂ∞çÊñºÂΩ±ÂÉèÈáçÂª∫ËÄåË®ÄÂçªÊòØÈ†ÖÊåëÊà∞„ÄÇÁõÆÂâçÁöÑÊºîÁÆóÊ≥ïÔºåÁÑ°Ë´ñÊòØÂÇ≥Áµ±ÁöÑÈÇÑÊòØÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÔºåÂú®Âà©Áî®Ë±êÂØåÁöÑÊôÇÈñìÁ¥∞ÁØÄÂíåÈáçÂª∫ÂΩ±ÂÉèÁöÑÁ¥∞ÁØÄÊñπÈù¢‰ªçÈúÄË¶ÅÊîπÈÄ≤„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Swin Spikeformer (SwinSF)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂæûÂ∞ñÂ≥∞‰∏≤ÊµÅÈáçÂª∫ÂãïÊÖãÂ†¥ÊôØÁöÑÊñ∞Á©éÊ®°Âûã„ÄÇSwinSF Áî±Â∞ñÂ≥∞ÁâπÂæµÊèêÂèñ„ÄÅÊôÇÁ©∫ÁâπÂæµÊèêÂèñÂíåÊúÄÁµÇÈáçÂª∫Ê®°ÁµÑÁµÑÊàê„ÄÇÂÆÉÁµêÂêà‰∫Ü‰ΩçÁßªË¶ñÁ™óËá™Ê≥®ÊÑèÂäõÂíåÊèêÂá∫ÁöÑÊôÇÂ∫èÂ∞ñÂ≥∞Ê≥®ÊÑèÂäõÔºåÁ¢∫‰øùÂÖ®Èù¢ÁöÑÁâπÂæµÊèêÂèñÔºåÊ∂µËìãÁ©∫ÈñìÂíåÊôÇÈñìÂãïÊÖãÔºåÂæûËÄåÂ∞çÂ∞ñÂ≥∞‰∏≤ÊµÅÈÄ≤Ë°åÊõ¥Âº∑ÂÅ•‰∏îÊ∫ñÁ¢∫ÁöÑÈáçÂª∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂêàÊàêË≥áÊñôÈõÜÔºåÁî®ÊñºÂ∞ñÂ≥∞ÂΩ±ÂÉèÈáçÂª∫ÔºåÂÖ∂Ëß£ÊûêÂ∫¶ËàáÊúÄÊñ∞ÁöÑÂ∞ñÂ≥∞Áõ∏Ê©üÁõ∏Á¨¶ÔºåÁ¢∫‰øùÂÖ∂ËàáÂ∞ñÂ≥∞Áõ∏Ê©üÂΩ±ÂÉèÁöÑÊúÄÊñ∞ÁôºÂ±ïÁõ∏Èóú‰∏îÈÅ©Áî®„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑ SwinSF Á∂≤Ë∑ØÊ®πÁ´ã‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºåÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜÔºàÂåÖÊã¨ÂêÑÁ®ÆËß£ÊûêÂ∫¶ÁöÑÁúüÂØ¶‰∏ñÁïåÂíåÂêàÊàêË≥áÊñôÔºâ‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåÊèêÂá∫ÁöÑË≥áÊñôÈõÜÂ∞áÂæàÂø´Êèê‰æõ„ÄÇ

##### **Predicting the Best of N Visual Trackers**
2407.15707v1 by Basit Alawode, Sajid Javed, Arif Mahmood, Jiri Matas

We observe that the performance of SOTA visual trackers surprisingly strongly
varies across different video attributes and datasets. No single tracker
remains the best performer across all tracking attributes and datasets. To
bridge this gap, for a given video sequence, we predict the "Best of the N
Trackers", called the BofN meta-tracker. At its core, a Tracking Performance
Prediction Network (TP2N) selects a predicted best performing visual tracker
for the given video sequence using only a few initial frames. We also introduce
a frame-level BofN meta-tracker which keeps predicting best performer after
regular temporal intervals. The TP2N is based on self-supervised learning
architectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with
ViT-S as a backbone performs the best. The video-level BofN meta-tracker
outperforms, by a large margin, existing SOTA trackers on nine standard
benchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,
OTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN
meta-tracker effectively handling variations in the tracking scenarios within
long sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is
88.7% and 91.1% with video and frame-level settings respectively. The best
performing tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average
overlap is 67.88% and 70.98% with video and frame level settings, compared to
the best performing ARTrack, 64.12%. This work also presents an extensive
evaluation of competitive tracking methods on all commonly used benchmarks,
following their protocols. The code, the trained models, and the results will
soon be made publicly available on
https://github.com/BasitAlawode/Best_of_N_Trackers.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëËßÄÂØüÂà∞ÔºåSOTA Ë¶ñË¶∫ËøΩËπ§Âô®ÁöÑÊïàËÉΩÈ©ö‰∫∫Âú∞Âõ†‰∏çÂêåÁöÑÂΩ±ÁâáÂ±¨ÊÄßÂíåË≥áÊñôÈõÜËÄåÊúâÂæàÂ§ßÁöÑÂ∑ÆÁï∞„ÄÇÊ≤íÊúâÂñÆ‰∏ÄËøΩËπ§Âô®ËÉΩÂú®ÊâÄÊúâËøΩËπ§Â±¨ÊÄßÂíåË≥áÊñôÈõÜ‰∏ä‰øùÊåÅÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄôÂÄãÂ∑ÆË∑ùÔºåÂ∞çÊñºÁµ¶ÂÆöÁöÑÂΩ±ÁâáÂ∫èÂàóÔºåÊàëÂÄëÈ†êÊ∏¨„ÄåN ÂÄãËøΩËπ§Âô®‰∏≠ÁöÑÊúÄ‰Ω≥ËÄÖ„ÄçÔºåÁ®±ÁÇ∫ BofN ÂÖÉËøΩËπ§Âô®„ÄÇÂÖ∂Ê†∏ÂøÉÊòØ‰∏ÄÂÄãËøΩËπ§ÊïàËÉΩÈ†êÊ∏¨Á∂≤Ë∑Ø (TP2N)ÔºåÂÆÉ‰ΩøÁî®ÂÉÖÊúâÂπæÂÄãÂàùÂßãÁï´Ê†ºÔºåÂ∞±ËÉΩÁÇ∫Áµ¶ÂÆöÁöÑÂΩ±ÁâáÂ∫èÂàóÈÅ∏ÊìáÈ†êÊ∏¨ÁöÑÊúÄ‰Ω≥ÊïàËÉΩË¶ñË¶∫ËøΩËπ§Âô®„ÄÇÊàëÂÄëÈÇÑÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÁï´Ê†ºÂ±§Á¥öÁöÑ BofN ÂÖÉËøΩËπ§Âô®ÔºåÂÆÉÊúÉÂú®Ë¶èÂæãÁöÑÊôÇÈñìÈñìÈöîÂæåÊåÅÁ∫åÈ†êÊ∏¨ÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇTP2N Âü∫ÊñºËá™Áõ£Áù£Â≠∏ÁøíÊû∂Êßã MocoV2„ÄÅSwAv„ÄÅBT Âíå DINOÔºõÂØ¶È©óÈ°ØÁ§∫Ôºå‰ª• ViT-S ‰ΩúÁÇ∫‰∏ªÂππÁöÑ DINO ÊïàËÉΩÊúÄ‰Ω≥„ÄÇÂΩ±ÁâáÂ±§Á¥öÁöÑ BofN ÂÖÉËøΩËπ§Âô®Âú®‰πùÂÄãÊ®ôÊ∫ñÂü∫Ê∫ñ‰∏äÂ§ßÂπÖÂÑ™ÊñºÁèæÊúâÁöÑ SOTA ËøΩËπ§Âô®ÔºåÂåÖÊã¨ LaSOT„ÄÅTrackingNet„ÄÅGOT-10K„ÄÅVOT2019„ÄÅVOT2021„ÄÅVOT2022„ÄÅUAV123„ÄÅOTB100 Âíå WebUAV-3M„ÄÇÁï´Ê†ºÂ±§Á¥öÁöÑ BofN ÂÖÉËøΩËπ§Âô®ÊúâÊïàËôïÁêÜÈï∑Â∫èÂàó‰∏≠ËøΩËπ§Â†¥ÊôØÁöÑËÆäÂåñÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú® GOT-10k ‰∏äÔºåBofN ÂÖÉËøΩËπ§Âô®Âπ≥ÂùáÈáçÁñäÁéáÂàÜÂà•ÁÇ∫ 88.7% Âíå 91.1%Ôºå‰ΩøÁî®ÂΩ±ÁâáÂíåÁï´Ê†ºÂ±§Á¥öË®≠ÂÆö„ÄÇÊïàËÉΩÊúÄ‰Ω≥ÁöÑËøΩËπ§Âô® RTS ÈÅîÂà∞ 85.20% ÁöÑ AO„ÄÇÂú® VOT2022 ‰∏äÔºåBofN È†êÊúüÂπ≥ÂùáÈáçÁñäÁéáÂàÜÂà•ÁÇ∫ 67.88% Âíå 70.98%Ôºå‰ΩøÁî®ÂΩ±ÁâáÂíåÁï´Ê†ºÂ±§Á¥öË®≠ÂÆöÔºåËÄåÊïàËÉΩÊúÄ‰Ω≥ÁöÑ ARTrack ÁÇ∫ 64.12%„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈÇÑÊ†πÊìöÂêÑÂÄãÂü∫Ê∫ñÁöÑÂçîÂÆöÔºåÂ∞çÊâÄÊúâÂ∏∏Áî®ÁöÑÂü∫Ê∫ñÈÄ≤Ë°å‰∫ÜÁ´∂Áà≠ËøΩËπ§ÊñπÊ≥ïÁöÑÂª£Ê≥õË©ï‰º∞„ÄÇÁ®ãÂºèÁ¢º„ÄÅË®ìÁ∑¥Ê®°ÂûãÂíåÁµêÊûúÂ∞áÂæàÂø´Âú® https://github.com/BasitAlawode/Best_of_N_Trackers ‰∏äÂÖ¨Èñã„ÄÇ</paragraph>

##### **Supporting the Digital Autonomy of Elders Through LLM Assistance**
2407.15695v1 by Jesse Roberts, Lindsey Roberts, Alice Reed

The internet offers tremendous access to services, social connections, and
needed products. However, to those without sufficient experience, engaging with
businesses and friends across the internet can be daunting due to the ever
present danger of scammers and thieves, to say nothing of the myriad of
potential computer viruses. Like a forest rich with both edible and poisonous
plants, those familiar with the norms inhabit it safely with ease while
newcomers need a guide. However, reliance on a human digital guide can be
taxing and often impractical. We propose and pilot a simple but unexplored
idea: could an LLM provide the necessary support to help the elderly who are
separated by the digital divide safely achieve digital autonomy?

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØÊèê‰æõ‰∫ÜÊ•µÂ§ßÁöÑÊúçÂãô„ÄÅÁ§æ‰∫§ÈÄ£ÁµêÔºå‰ª•ÂèäÂøÖÈúÄÁî¢ÂìÅÁöÑÂ≠òÂèñÊ¨ä„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÊ≤íÊúâË∂≥Â§†Á∂ìÈ©óÁöÑ‰∫∫‰æÜË™™ÔºåËàá‰ºÅÊ•≠ÂíåÊúãÂèãÈÄèÈÅéÁ∂≤Ë∑Ø‰∫íÂãïÂèØËÉΩÊúÉ‰ª§‰∫∫ÊúõËÄåÁîüÁïèÔºåÂõ†ÁÇ∫Ë©êÈ®ôËÄÖÂíåÂ∞èÂÅ∑ÁÑ°ÊâÄ‰∏çÂú®ÔºåÊõ¥Âà•Ë™™ÈÇÑÊúâÁÑ°Êï∏ÊΩõÂú®ÁöÑÈõªËÖ¶ÁóÖÊØí„ÄÇÂ∞±ÂÉè‰∏ÄÂ∫ßÂØåÂê´ÂèØÈ£üÁî®ÂíåÊúâÊØíÊ§çÁâ©ÁöÑÊ£ÆÊûóÔºåÁÜüÊÇâË¶èÁØÑÁöÑ‰∫∫ÂèØ‰ª•ËºïÈ¨ÜÂÆâÂÖ®Âú∞Â±Ö‰ΩèÂÖ∂‰∏≠ÔºåËÄåÊñ∞ÊâãÂâáÈúÄË¶Å‰∏Ä‰ΩçÂöÆÂ∞é„ÄÇÁÑ∂ËÄåÔºå‰æùË≥¥‰∫∫È°ûÊï∏‰ΩçÂöÆÂ∞éÂèØËÉΩÊúÉÂæàÁ¥Ø‰∫∫ÔºåËÄå‰∏îÂ∏∏Â∏∏‰∏çÂàáÂØ¶Èöõ„ÄÇÊàëÂÄëÊèêÂá∫‰∏¶Ë©¶Ë°å‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÂ∞öÊú™Êé¢Á¥¢ÁöÑÊÉ≥Ê≥ïÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂê¶Êèê‰æõÂøÖË¶ÅÁöÑÊîØÊè¥Ôºå‰ª•Âπ´Âä©Ë¢´Êï∏‰ΩçÈ¥ªÊ∫ùÈöîÈñãÁöÑËÄÅÂπ¥‰∫∫ÂÆâÂÖ®Âú∞ÂØ¶ÁèæÊï∏‰ΩçËá™‰∏ªÔºü

##### **Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)**
2407.15694v1 by Ishan Kavathekar, Anku Rani, Ashmit Chamoli, Ponnurangam Kumaraguru, Amit Sheth, Amitava Das

The widespread adoption of large language models (LLMs) and awareness around
multilingual LLMs have raised concerns regarding the potential risks and
repercussions linked to the misapplication of AI-generated text, necessitating
increased vigilance. While these models are primarily trained for English,
their extensive training on vast datasets covering almost the entire web,
equips them with capabilities to perform well in numerous other languages.
AI-Generated Text Detection (AGTD) has emerged as a topic that has already
received immediate attention in research, with some initial methods having been
proposed, soon followed by the emergence of techniques to bypass detection. In
this paper, we report our investigation on AGTD for an indic language Hindi.
Our major contributions are in four folds: i) examined 26 LLMs to evaluate
their proficiency in generating Hindi text, ii) introducing the AI-generated
news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of
five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and
Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv)
proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to
understand the evolving landscape of eloquence of AI-generated text in Hindi.
We will make the codes and datasets available to encourage further research.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂª£Ê≥õÊé°Áî®Ôºå‰ª•ÂèäÂ∞çÂ§öË™ûË®Ä LLM ÁöÑË™çË≠òÔºå‰∫∫ÂÄëÈñãÂßãÈóúÊ≥®Ëàá AI ÁîüÊàêÁöÑÊñáÂ≠óË™§Áî®Áõ∏ÈóúÁöÑÊΩõÂú®È¢®Èö™ÂíåÂΩ±ÈüøÔºåÈÄôÈúÄË¶ÅÊèêÈ´òË≠¶Ë¶∫„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊ®°Âûã‰∏ªË¶ÅÊòØÈáùÂ∞çËã±Ë™ûË®ìÁ∑¥Ôºå‰ΩÜÁî±ÊñºÂú®Ê∂µËìãÂπæ‰πéÊï¥ÂÄãÁ∂≤Ë∑ØÁöÑÈæêÂ§ßË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂª£Ê≥õË®ìÁ∑¥ÔºåÂõ†Ê≠§ÂÆÉÂÄëÂÖ∑ÂÇôÂú®Ë®±Â§öÂÖ∂‰ªñË™ûË®Ä‰∏≠Ë°®ÁèæËâØÂ•ΩÁöÑËÉΩÂäõ„ÄÇAI ÁîüÊàêÁöÑÊñáÂ≠óÂÅµÊ∏¨ (AGTD) Â∑≤ÊàêÁÇ∫‰∏ÄÂÄãÂú®Á†îÁ©∂‰∏≠Â∑≤Á´ãÂç≥ÂèóÂà∞ÈóúÊ≥®ÁöÑ‰∏ªÈ°åÔºå‰∏Ä‰∫õÊúÄÂàùÁöÑÊñπÊ≥ïÂ∑≤Ë¢´ÊèêÂá∫ÔºåÈö®ÂæåÂæàÂø´Âá∫Áèæ‰∫ÜÁπûÈÅéÂÅµÊ∏¨ÁöÑÊäÄË°ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ†±Âëä‰∫ÜÊàëÂÄëÂ∞çÂç∞Â∫¶Ë™ûË®ÄÂç∞Âú∞Ë™ûÁöÑ AGTD Ë™øÊü•„ÄÇÊàëÂÄëÁöÑÈáçÂ§ßË≤¢ÁçªÊúâÂõõÂÄãÊñπÈù¢Ôºöi) Ê™¢Êü• 26 ÂÄã LLM ‰ª•Ë©ï‰º∞ÂÆÉÂÄëÁîüÊàêÂç∞Âú∞Ë™ûÊñáÂ≠óÁöÑËÉΩÂäõÔºåii) ‰ªãÁ¥πÂç∞Âú∞Ë™û AI ÁîüÊàêÁöÑÊñ∞ËÅûÊñáÁ´† ($AG_{hi}$) Ë≥áÊñôÈõÜÔºåiii) Ë©ï‰º∞‰∫îÁ®ÆÊúÄËøëÊèêÂá∫ÁöÑ AGTD ÊäÄË°ìÁöÑÊúâÊïàÊÄßÔºöConDA„ÄÅJ-Guard„ÄÅRADAR„ÄÅRAIDAR ÂíåÂÖßÂú®Á∂≠Â∫¶‰º∞Ë®àÔºå‰ª•ÂÅµÊ∏¨ AI ÁîüÊàêÁöÑÂç∞Âú∞Ë™ûÊñáÂ≠óÔºåiv) ÊèêÂá∫Âç∞Âú∞Ë™û AI ÂèØÂÅµÊ∏¨ÊÄßÊåáÊ®ô ($ADI_{hi}$)ÔºåÂÆÉÈ°ØÁ§∫‰∫Ü‰∏ÄÂÄãÂÖâË≠úÔºåÁî®Êñº‰∫ÜËß£Âç∞Âú∞Ë™û‰∏≠ AI ÁîüÊàêÁöÑÊñáÂ≠óÁöÑË°®ÈÅîËÉΩÂäõ‰∏çÊñ∑ËÆäÂåñÁöÑÁèæÊ≥Å„ÄÇÊàëÂÄëÂ∞áÊèê‰æõÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÔºå‰ª•ÈºìÂãµÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂„ÄÇ

##### **AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach**
2407.15688v1 by Abdelaziz Amara korba, Aleddine Diaf, Yacine Ghamri-Doudane

In the rapidly evolving landscape of cyber threats targeting the Internet of
Things (IoT) ecosystem, and in light of the surge in botnet-driven Distributed
Denial of Service (DDoS) and brute force attacks, this study focuses on the
early detection of IoT bots. It specifically addresses the detection of stealth
bot communication that precedes and orchestrates attacks. This study proposes a
comprehensive methodology for analyzing IoT network traffic, including
considerations for both unidirectional and bidirectional flow, as well as
packet formats. It explores a wide spectrum of network features critical for
representing network traffic and characterizing benign IoT traffic patterns
effectively. Moreover, it delves into the modeling of traffic using various
semi-supervised learning techniques. Through extensive experimentation with the
IoT-23 dataset - a comprehensive collection featuring diverse botnet types and
traffic scenarios - we have demonstrated the feasibility of detecting botnet
traffic corresponding to different operations and types of bots, specifically
focusing on stealth command and control (C2) communications. The results
obtained have demonstrated the feasibility of identifying C2 communication with
a 100% success rate through packet-based methods and 94% via flow based
approaches, with a false positive rate of 1.53%.

ÊëòË¶ÅÔºöÂú®Á∂≤Ë∑ØÂ®ÅËÑÖÂø´ÈÄüÊºîËÆäÁöÑÁí∞Â¢É‰∏≠ÔºåÈáùÂ∞çÁâ©ËÅØÁ∂≤ (IoT) ÁîüÊÖãÁ≥ªÁµ±Ôºå‰ª•ÂèäÂú®Ê©üÂô®‰∫∫Á®ãÂºèÈ©ÖÂãïÁöÑÂàÜÂ∏ÉÂºèÈòªÊñ∑ÊúçÂãô (DDoS) ÂíåÊö¥ÂäõÁ†¥Ëß£ÊîªÊìäÊøÄÂ¢ûÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÁâ©ËÅØÁ∂≤Ê©üÂô®‰∫∫Á®ãÂºèÁöÑÊó©ÊúüÂÅµÊ∏¨„ÄÇÁâπÂà•ÊòØÂÅµÊ∏¨Âú®ÊîªÊìä‰πãÂâçÂíåÂçîË™øÊîªÊìäÁöÑÂåøËπ§Ê©üÂô®‰∫∫Á®ãÂºèÈÄöË®ä„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÂàÜÊûêÁâ©ËÅØÁ∂≤Á∂≤Ë∑ØÊµÅÈáèÁöÑÂÖ®Èù¢ÊñπÊ≥ïÔºåÂåÖÊã¨ËÄÉÈáèÂñÆÂêëÂíåÈõôÂêëÊµÅÈáèÔºå‰ª•ÂèäÂ∞ÅÂåÖÊ†ºÂºè„ÄÇÂÆÉÊé¢Ë®é‰∫ÜË°®Á§∫Á∂≤Ë∑ØÊµÅÈáèÂíåÊúâÊïàË°®ÂæµËâØÊÄßÁâ©ËÅØÁ∂≤ÊµÅÈáèÊ®°ÂºèÁöÑÂª£Ê≥õÁ∂≤Ë∑ØÂäüËÉΩ„ÄÇÊ≠§Â§ñÔºåÂÆÉÊ∑±ÂÖ•Êé¢Ë®é‰ΩøÁî®ÂêÑÁ®ÆÂçäÁõ£Áù£Â≠∏ÁøíÊäÄË°ìÂ∞çÊµÅÈáèÂª∫Ê®°„ÄÇÈÄèÈÅéÂ∞ç IoT-23 Ë≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÂØ¶È©óÔºåË©≤Ë≥áÊñôÈõÜÊòØ‰∏ÄÂÄãÂåÖÂê´‰∏çÂêåÊ©üÂô®‰∫∫Á®ãÂºèÈ°ûÂûãÂíåÊµÅÈáèÂ†¥ÊôØÁöÑÂÖ®Èù¢ÈõÜÂêàÔºåÊàëÂÄëÂ∑≤Á∂ìË≠âÊòé‰∫ÜÂÅµÊ∏¨Ëàá‰∏çÂêåÊìç‰ΩúÂíåÊ©üÂô®‰∫∫Á®ãÂºèÈ°ûÂûãÁõ∏ÊáâÁöÑÊ©üÂô®‰∫∫Á®ãÂºèÊµÅÈáèÁöÑÂèØË°åÊÄßÔºåÁâπÂà•Â∞àÊ≥®ÊñºÂåøËπ§ÂëΩ‰ª§ÂíåÊéßÂà∂ (C2) ÈÄöË®ä„ÄÇÁç≤ÂæóÁöÑÁµêÊûúË≠âÊòé‰∫ÜÈÄèÈÅéÂ∞ÅÂåÖÊñπÊ≥ï‰ª• 100% ÁöÑÊàêÂäüÁéáË≠òÂà• C2 ÈÄöË®äÁöÑÂèØË°åÊÄßÔºåËÄåÈÄèÈÅéÊµÅÈáèÊñπÊ≥ïÂâáÁÇ∫ 94%ÔºåË™§Âà§ÁéáÁÇ∫ 1.53%„ÄÇ

##### **HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**
2407.15680v1 by Zhecan Wang, Garrett Bingham, Adams Yu, Quoc Le, Thang Luong, Golnaz Ghiasi

Hallucination has been a major problem for large language models and remains
a critical challenge when it comes to multimodality in which vision-language
models (VLMs) have to deal with not just textual but also visual inputs.
Despite rapid progress in VLMs, resources for evaluating and addressing
multimodal hallucination are limited and mostly focused on evaluation. This
work introduces HaloQuest, a novel visual question answering dataset that
captures various aspects of multimodal hallucination such as false premises,
insufficient contexts, and visual challenges. A novel idea from HaloQuest is to
leverage synthetic images, apart from real ones, to enable dataset creation at
scale. With over 7.7K examples spanning across a wide variety of categories,
HaloQuest was designed to be both a challenging benchmark for VLMs and a
fine-tuning dataset for advancing multimodal reasoning. Our experiments reveal
that current models struggle with HaloQuest, with all open-source VLMs
achieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest
significantly reduces hallucination rates while preserving performance on
standard reasoning tasks. Our results discover that benchmarking with generated
images is highly correlated (r=0.97) with real images. Last but not least, we
propose a novel Auto-Eval mechanism that is highly correlated with human raters
(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards
understanding, evaluating, and mitigating hallucination in VLMs, serving as an
important step towards more reliable multimodal AI systems in the future.

ÊëòË¶ÅÔºöÂ∞çÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËÄåË®ÄÔºåÂπªË¶∫‰∏ÄÁõ¥ÊòØ‰∏ÄÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÁï∂Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ‰∏çÂÉÖË¶ÅËôïÁêÜÊñáÂ≠óÔºåÈÇÑË¶ÅËôïÁêÜË¶ñË¶∫Ëº∏ÂÖ•ÊôÇÔºåÈÄô‰ªçÁÑ∂ÊòØÂ§öÊ®°ÊÖãÈù¢Ëá®ÁöÑ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÂÑòÁÆ° VLM Âø´ÈÄüÈÄ≤Ê≠•Ôºå‰ΩÜÁî®ÊñºË©ï‰º∞ÂíåËß£Ê±∫Â§öÊ®°ÊÖãÂπªË¶∫ÁöÑË≥áÊ∫êÊúâÈôêÔºåËÄå‰∏îÂ§ßÂ§öÈõÜ‰∏≠Âú®Ë©ï‰º∞‰∏ä„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü HaloQuestÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑË¶ñË¶∫ÂïèÁ≠îË≥áÊñôÈõÜÔºåÂÆÉÊ∂µËìã‰∫ÜÂ§öÊ®°ÊÖãÂπªË¶∫ÁöÑÂêÑÂÄãÊñπÈù¢Ôºå‰æãÂ¶ÇÈåØË™§ÂâçÊèê„ÄÅËÉåÊôØ‰∏çË∂≥ÂíåË¶ñË¶∫ÊåëÊà∞„ÄÇHaloQuest ÁöÑ‰∏ÄÂÄãÊñ∞Á©éÊÉ≥Ê≥ïÊòØÂà©Áî®ÂêàÊàêÂΩ±ÂÉèÔºàÈô§‰∫ÜÁúüÂØ¶ÂΩ±ÂÉè‰πãÂ§ñÔºâ‰æÜÂ§ßË¶èÊ®°Âª∫Á´ãË≥áÊñôÈõÜ„ÄÇHaloQuest ÊìÅÊúâË∂ÖÈÅé 7.7K ÂÄãÊ©´Ë∑®ÂêÑÁ®ÆÈ°ûÂà•ÁöÑÁØÑ‰æãÔºåË¢´Ë®≠Ë®àÁÇ∫ VLM ÁöÑ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂü∫Ê∫ñÔºå‰ª•Âèä‰∏ÄÂÄãÁî®ÊñºÊé®ÈÄ≤Â§öÊ®°ÊÖãÊé®ÁêÜÁöÑÂæÆË™øË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÁõÆÂâçÁöÑÊ®°ÂûãÂú® HaloQuest ‰∏≠ÊéôÊâéÔºåÊâÄÊúâÈñãÊ∫ê VLM ÁöÑÊ∫ñÁ¢∫Â∫¶ÈÉΩ‰ΩéÊñº 36%„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂú® HaloQuest ‰∏äÈÄ≤Ë°åÂæÆË™øÊúÉÈ°ØËëóÈôç‰ΩéÂπªË¶∫ÁéáÔºåÂêåÊôÇ‰øùÊåÅÊ®ôÊ∫ñÊé®ÁêÜ‰ªªÂãôÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁôºÁèæÔºå‰ΩøÁî®ÁîüÊàêÁöÑÂΩ±ÂÉèÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶ËàáÁúüÂØ¶ÂΩ±ÂÉèÈ´òÂ∫¶Áõ∏Èóú (r=0.97)„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑ‰∏ÄÈªûÊòØÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ Auto-Eval Ê©üÂà∂ÔºåÂÆÉËàá‰∫∫È°ûË©ïÂàÜËÄÖÈ´òÂ∫¶Áõ∏Èóú (r=0.99)ÔºåÁî®ÊñºË©ï‰º∞ VLM„ÄÇÁ∏Ω‰πãÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊúùËëóÁêÜËß£„ÄÅË©ï‰º∞ÂíåÊ∏õËºï VLM ‰∏≠ÁöÑÂπªË¶∫ÈÇÅÂá∫‰∫ÜÂÖ∑È´îÁöÑÊ≠•‰ºêÔºåÊàêÁÇ∫Êú™‰æÜÊõ¥ÂèØÈù†ÁöÑÂ§öÊ®°ÊÖã AI Á≥ªÁµ±ÁöÑÈáçË¶Å‰∏ÄÊ≠•„ÄÇ

##### **Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps**
2407.15675v1 by Rabbia Asghar, Wenqian Liu, Lukas Rummelhard, Anne Spalanzani, Christian Laugier

Accurate prediction of driving scenes is essential for road safety and
autonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene
prediction due to their structured spatial representation, flexibility across
sensor modalities and integration of uncertainty. Recent studies have
successfully combined OGMs with deep learning methods to predict the evolution
of scene and learn complex behaviours. These methods, however, do not consider
prediction of flow or velocity vectors in the scene. In this work, we propose a
novel multi-task framework that leverages dynamic OGMs and semantic information
to predict both future vehicle semantic grids and the future flow of the scene.
This incorporation of semantic flow not only offers intermediate scene features
but also enables the generation of warped semantic grids. Evaluation on the
real-world NuScenes dataset demonstrates improved prediction capabilities and
enhanced ability of the model to retain dynamic vehicles within the scene.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫È†êÊ∏¨ÈßïÈßõÂ†¥ÊôØÂ∞çÊñºÈÅìË∑ØÂÆâÂÖ®ÂíåËá™ÂãïÈßïÈßõËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩîÁî®Á∂≤Ê†ºÂú∞Âúñ (OGM) Áî±ÊñºÂÖ∂ÁµêÊßãÂåñÁöÑÁ©∫ÈñìË°®Á§∫„ÄÅË∑®ÂÇ≥ÊÑüÂô®Ê®°ÂºèÁöÑÈùàÊ¥ªÊÄß‰ª•Âèä‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊï¥ÂêàÔºåÈÄöÂ∏∏Áî®ÊñºÂ†¥ÊôØÈ†êÊ∏¨„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤ÊàêÂäüÂú∞Â∞á OGM ËàáÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁõ∏ÁµêÂêàÔºå‰ª•È†êÊ∏¨Â†¥ÊôØÁöÑÊºîËÆä‰∏¶Â≠∏ÁøíË§áÈõúÁöÑË°åÁÇ∫„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÊ≤íÊúâËÄÉÊÖÆÂ†¥ÊôØ‰∏≠ÊµÅÊàñÈÄüÂ∫¶ÂêëÈáèÁöÑÈ†êÊ∏¨„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§ö‰ªªÂãôÊ°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂Âà©Áî®ÂãïÊÖã OGM ÂíåË™ûÁæ©‰ø°ÊÅØ‰æÜÈ†êÊ∏¨Êú™‰æÜÁöÑËªäËºõË™ûÁæ©Á∂≤Ê†ºÂíåÂ†¥ÊôØÁöÑÊú™‰æÜÊµÅÂãï„ÄÇÈÄôÁ®ÆË™ûÁæ©ÊµÅÁöÑÁ¥çÂÖ•‰∏çÂÉÖÊèê‰æõ‰∫Ü‰∏≠ÈñìÂ†¥ÊôØÁâπÂæµÔºåËÄå‰∏îÈÇÑËÉΩÂ§†ÁîüÊàêËÆäÂΩ¢ÁöÑË™ûÁæ©Á∂≤Ê†º„ÄÇÂ∞çÁúüÂØ¶‰∏ñÁïå NuScenes Êï∏ÊìöÈõÜÁöÑË©ï‰º∞Ë≠âÊòé‰∫ÜÊîπÈÄ≤ÁöÑÈ†êÊ∏¨ËÉΩÂäõÂíåÊ®°ÂûãÂú®Â†¥ÊôØ‰∏≠‰øùÁïôÂãïÊÖãËªäËºõÁöÑËÉΩÂäõÂ¢ûÂº∑„ÄÇ

##### **SLVideo: A Sign Language Video Moment Retrieval Framework**
2407.15668v1 by Gon√ßalo Vinagre Martins, Afonso Quinaz, Carla Viegas, Sofia Cavaco, Jo√£o Magalh√£es

Sign Language Recognition has been studied and developed throughout the years
to help the deaf and hard-of-hearing people in their day-to-day lives. These
technologies leverage manual sign recognition algorithms, however, most of them
lack the recognition of facial expressions, which are also an essential part of
Sign Language as they allow the speaker to add expressiveness to their dialogue
or even change the meaning of certain manual signs. SLVideo is a video moment
retrieval software for Sign Language videos with a focus on both hands and
facial signs. The system extracts embedding representations for the hand and
face signs from video frames to capture the language signs in full. This will
then allow the user to search for a specific sign language video segment with
text queries, or to search by similar sign language videos. To test this
system, a collection of five hours of annotated Sign Language videos is used as
the dataset, and the initial results are promising in a zero-shot
setting.SLVideo is shown to not only address the problem of searching sign
language videos but also supports a Sign Language thesaurus with a search by
similarity technique.
  Project web page: https://novasearch.github.io/SLVideo/

ÊëòË¶ÅÔºöÊâãË™ûËæ®Ë≠òÊäÄË°ìÂ§öÂπ¥‰æÜ‰∏çÊñ∑Á†îÁ©∂ÂíåÈñãÁôºÔºå
ÂçîÂä©ËÅΩÈöú‰∫∫Â£´Âú®Êó•Â∏∏ÁîüÊ¥ª‰∏≠„ÄÇÈÄô‰∫õ
ÊäÄË°ìÂà©Áî®ÊâãÂã¢Ëæ®Ë≠òÊºîÁÆóÊ≥ïÔºåÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ÊäÄË°ì
Ê¨†Áº∫Â∞çËáâÈÉ®Ë°®ÊÉÖÁöÑËæ®Ë≠òÔºåËÄåËáâÈÉ®Ë°®ÊÉÖ‰πüÊòØ
ÊâãË™ûÁöÑÂøÖË¶ÅÈÉ®ÂàÜÔºåÂõ†ÁÇ∫ÂÆÉËÆìË™™Ë©±ËÄÖËÉΩÁÇ∫Â∞çË©±Â¢ûÊ∑ª
Ë°®ÈÅîÂäõÔºåÁîöËá≥ÊîπËÆäÁâπÂÆöÊâãÂã¢ÁöÑÊÑèÁæ©„ÄÇSLVideo ÊòØ‰∏ÄÊ¨æ
ÊâãË™ûÂΩ±ÁâáÁöÑÂΩ±ÁâáÁâáÊÆµÊì∑ÂèñËªüÈ´îÔºåÂ∞àÊ≥®ÊñºÊâãÈÉ®Âíå
ËáâÈÉ®ÊâãÂã¢„ÄÇÁ≥ªÁµ±ÂæûÂΩ±ÁâáÊ†º‰∏≠Êì∑ÂèñÊâãÈÉ®Âíå
ËáâÈÉ®ÊâãÂã¢ÁöÑÂµåÂÖ•ÂºèË°®Á§∫Ôºå‰ª•ÂÆåÊï¥ÊçïÊçâË™ûË®ÄÊâãÂã¢„ÄÇÈÄôÂ∞á
‰ΩøÁî®Êà∂ËÉΩÂ§†‰ª•ÊñáÂ≠óÊü•Ë©¢ÊêúÂ∞ãÁâπÂÆöÊâãË™ûÂΩ±ÁâáÁâáÊÆµÔºåÊàñ‰ª•
È°û‰ººÊâãË™ûÂΩ±ÁâáÊêúÂ∞ã„ÄÇÁÇ∫‰∫ÜÊ∏¨Ë©¶ÈÄôÂÄã
Á≥ªÁµ±ÔºåÊàëÂÄë‰ΩøÁî®‰∏ÄÁ≥ªÂàó‰∫îÂ∞èÊôÇÁöÑË®ªËß£ÊâãË™ûÂΩ±Áâá‰ΩúÁÇ∫
Ë≥áÊñôÈõÜÔºåËÄåÂàùÂßãÁµêÊûúÂú®Èõ∂Ê¨°Â≠∏Áøí
Ë®≠ÂÆö‰∏≠ÂæàÊúâÂ∏åÊúõ„ÄÇSLVideo ‰∏çÂÉÖËß£Ê±∫‰∫ÜÊâãË™ûÂΩ±ÁâáÊêúÂ∞ãÂïèÈ°åÔºåÈÇÑÊîØÊè¥
ÊâãË™ûÂêåÁæ©Ë©ûÂ∫´Ôºå‰∏¶‰ª•Áõ∏‰ººÂ∫¶ÊäÄË°ìÊêúÂ∞ã„ÄÇ
Â∞àÊ°àÁ∂≤È†ÅÔºöhttps://novasearch.github.io/SLVideo/

##### **Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models**
2407.15645v1 by Joy He-Yueya, Wanjing Anya Ma, Kanishk Gandhi, Benjamin W. Domingue, Emma Brunskill, Noah D. Goodman

Language models (LMs) are increasingly used to simulate human-like responses
in scenarios where accurately mimicking a population's behavior can guide
decision-making, such as in developing educational materials and designing
public policies. The objective of these simulations is for LMs to capture the
variations in human responses, rather than merely providing the expected
correct answers. Prior work has shown that LMs often generate unrealistically
accurate responses, but there are no established metrics to quantify how
closely the knowledge distribution of LMs aligns with that of humans. To
address this, we introduce "psychometric alignment," a metric that measures the
extent to which LMs reflect human knowledge distribution. Assessing this
alignment involves collecting responses from both LMs and humans to the same
set of test items and using Item Response Theory to analyze the differences in
item functioning between the groups. We demonstrate that our metric can capture
important variations in populations that traditional metrics, like differences
in accuracy, fail to capture. We apply this metric to assess existing LMs for
their alignment with human knowledge distributions across three real-world
domains. We find significant misalignment between LMs and human populations,
though using persona-based prompts can improve alignment. Interestingly,
smaller LMs tend to achieve greater psychometric alignment than larger LMs.
Further, training LMs on human response data from the target distribution
enhances their psychometric alignment on unseen test items, but the
effectiveness of such training varies across domains.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°ÂûãÔºàLMÔºâË∂ä‰æÜË∂äÂ∏∏Ë¢´Áî®ÊñºÊ®°Êì¨È°û‰ºº‰∫∫È°ûÁöÑÂõûÊáâÔºåÂú®Ê∫ñÁ¢∫Ê®°‰ªøÁæ§È´îË°åÁÇ∫ÂèØ‰ª•ÂºïÂ∞éÊ±∫Á≠ñÂà∂ÂÆöÔºà‰æãÂ¶ÇÈñãÁôºÊïôËÇ≤ÊùêÊñôÂíåË®≠Ë®àÂÖ¨ÂÖ±ÊîøÁ≠ñÔºâÁöÑÂ†¥ÊôØ‰∏≠„ÄÇÈÄô‰∫õÊ®°Êì¨ÁöÑÁõÆÊ®ôÊòØËÆì LM ÊçïÊçâ‰∫∫È°ûÂõûÊáâÁöÑËÆäÂåñÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊèê‰æõÈ†êÊúüÁöÑÊ≠£Á¢∫Á≠îÊ°à„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLM Á∂ìÂ∏∏Áî¢Áîü‰∏çÂàáÂØ¶ÈöõÁöÑÊ∫ñÁ¢∫ÂõûÊáâÔºå‰ΩÜÊ≤íÊúâÊó¢ÂÆöÁöÑÊåáÊ®ôÂèØ‰ª•ÈáèÂåñ LM ÁöÑÁü•Ë≠òÂàÜ‰ΩàËàá‰∫∫È°ûÁöÑÁü•Ë≠òÂàÜ‰ΩàÊúâÂ§öÊé•Ëøë„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü„ÄåÂøÉÁêÜÊ∏¨ÈáèÂ∞çÈΩä„ÄçÔºåÈÄôÊòØ‰∏ÄÂÄãË°°Èáè LM ÂèçÊò†‰∫∫È°ûÁü•Ë≠òÂàÜ‰ΩàÁ®ãÂ∫¶ÁöÑÊåáÊ®ô„ÄÇË©ï‰º∞ÈÄôÁ®ÆÂ∞çÈΩäÊ∂âÂèäÊî∂ÈõÜ LM Âíå‰∫∫È°ûÂ∞çÂêå‰∏ÄÁµÑÊ∏¨Ë©¶È†ÖÁõÆÁöÑÂõûÊáâÔºå‰∏¶‰ΩøÁî®È†ÖÁõÆÂèçÊáâÁêÜË´ñ‰æÜÂàÜÊûêÁæ§ÁµÑ‰πãÈñìÈ†ÖÁõÆÂäüËÉΩÁöÑÂ∑ÆÁï∞„ÄÇÊàëÂÄëË≠âÊòéÊàëÂÄëÁöÑÊåáÊ®ôÂèØ‰ª•ÊçïÊçâÂÇ≥Áµ±ÊåáÊ®ôÔºà‰æãÂ¶ÇÊ∫ñÁ¢∫ÊÄßÂ∑ÆÁï∞ÔºâÁÑ°Ê≥ïÊçïÊçâÁöÑ‰∫∫Áæ§‰∏≠ÁöÑÈáçË¶ÅËÆäÂåñ„ÄÇÊàëÂÄëÊáâÁî®ÈÄôÂÄãÊåáÊ®ô‰æÜË©ï‰º∞ÁèæÊúâÁöÑ LMÔºå‰ª•Ë©ï‰º∞ÂÆÉÂÄëËàá‰∏âÂÄãÁèæÂØ¶‰∏ñÁïåÈ†òÂüüÁöÑ‰∫∫È°ûÁü•Ë≠òÂàÜ‰ΩàÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÁôºÁèæ LM Âíå‰∫∫È°ûÁæ§È´î‰πãÈñìÂ≠òÂú®È°ØËëóÁöÑ‰∏ç‰∏ÄËá¥ÔºåÂÑòÁÆ°‰ΩøÁî®Âü∫ÊñºËßíËâ≤ÁöÑÊèêÁ§∫ÂèØ‰ª•ÊîπÂñÑ‰∏ÄËá¥ÊÄß„ÄÇÊúâË∂£ÁöÑÊòØÔºåËºÉÂ∞èÁöÑ LM ÂæÄÂæÄÊØîËºÉÂ§ßÁöÑ LM ÈÅîÂà∞Êõ¥È´òÁöÑÂøÉÁêÜÊ∏¨Èáè‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÂú®‰æÜËá™ÁõÆÊ®ôÂàÜ‰ΩàÁöÑ‰∫∫È°ûÂõûÊáâÊï∏Êìö‰∏äË®ìÁ∑¥ LM ÂèØ‰ª•Â¢ûÂº∑ÂÆÉÂÄëÂú®Êú™Ë¶ãÊ∏¨Ë©¶È†ÖÁõÆ‰∏äÁöÑÂøÉÁêÜÊ∏¨Èáè‰∏ÄËá¥ÊÄßÔºå‰ΩÜÈÄôÁ®ÆË®ìÁ∑¥ÁöÑÊúâÊïàÊÄßÂõ†È†òÂüüËÄåÁï∞„ÄÇ

##### **RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation**
2407.15621v1 by Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn

Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊèêÂçá‰∫Ü‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®ÈÜ´Â≠∏È†òÂüüÁöÑÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåLLM Á∂ìÂ∏∏Ê†πÊìöÈùúÊÖãË®ìÁ∑¥Ë≥áÊñôÈõÜÁî¢ÁîüÈÅéÊôÇÊàñ‰∏çÊ∫ñÁ¢∫ÁöÑË≥áË®ä„ÄÇÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Ë≥áÊñô‰æÜÊ∫ê‰æÜÊ∏õËºïÈÄôÂÄãÂïèÈ°å„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑ RAG Á≥ªÁµ±‰ΩøÁî®È†êÂÖàÁµÑË£ù„ÄÅÂÖ∑ÊúâÊúâÈôêÂΩàÊÄßÁöÑÂõ∫ÂÆöË≥áÊñôÂ∫´Ôºå‰ΩÜÊàëÂÄëÂ∑≤ÈñãÁôºÊîæÂ∞ÑÁßë RAG (RadioRAG) ‰ΩúÁÇ∫‰∏ÄÂÄãÁ´ØÂ∞çÁ´ØÊû∂ÊßãÔºåÂèØÂæûÊ¨äÂ®ÅÁöÑÁ∑ö‰∏äÊîæÂ∞ÑÁßë‰æÜÊ∫êÂç≥ÊôÇÊ™¢Á¥¢Ë≥áÊñô„ÄÇRadioRAG ‰ΩøÁî®Â∞àÈñÄÁöÑÊîæÂ∞ÑÁßëÂïèÁ≠îË≥áÊñôÈõÜ (RadioQA) ÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëË©ï‰º∞ÂêÑÁ®Æ LLM Âú®ÂõûÁ≠îÊîæÂ∞ÑÁßëÁâπÂÆöÂïèÈ°åÊôÇÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÔºåÁÑ°Ë´ñÊòØÂê¶ÈÄèÈÅé RAG Â≠òÂèñÈ°çÂ§ñÁöÑÁ∑ö‰∏äË≥áË®ä„ÄÇ‰ΩøÁî®‰æÜËá™ RSNA Ê°à‰æãÂΩôÁ∑®ÁöÑ 80 ÂÄãÊ∂µËìãÊîæÂ∞ÑÁßëÊ¨°Â∞àÁßëÁöÑÂïèÈ°åÂíå 24 ÂÄãÈ°çÂ§ñÁöÑÂ∞àÂÆ∂Á≠ñÂäÉÂïèÈ°åÔºàÊúâÊ≠£Á¢∫ÁöÑÈáëÊ®ôÊ∫ñÁ≠îÊ°àÔºâÔºåÊèêÁ§∫ LLMÔºàGPT-3.5-turbo„ÄÅGPT-4„ÄÅMistral-7B„ÄÅMixtral-8x7B Âíå Llama3 [8B Âíå 70B]ÔºâÔºåÁÑ°Ë´ñÊòØÂê¶‰ΩøÁî® RadioRAG„ÄÇRadioRAG Âæû www.radiopaedia.org Âç≥ÊôÇÊ™¢Á¥¢ÁâπÂÆöÊñºËÑàÁµ°ÁöÑË≥áË®äÔºå‰∏¶Â∞áÂÖ∂Á¥çÂÖ•ÂõûË¶Ü‰∏≠„ÄÇRadioRAG ÊåÅÁ∫åÊîπÂñÑÊâÄÊúâ LLM ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÔºåÁõ∏Â∞çÊîπÂñÑÂπÖÂ∫¶Âæû 2% Âà∞ 54%„ÄÇÂÆÉÂú®ÊîæÂ∞ÑÁßëÊ¨°Â∞àÁßë‰∏≠ÈÅîÂà∞ÊàñË∂ÖÈÅéÊ≤íÊúâ RAG ÁöÑÂïèÁ≠îÊ∫ñÁ¢∫ÊÄßÔºåÁâπÂà•ÊòØÂú®‰π≥ÊàøÂΩ±ÂÉèÂíåÊÄ•Ë®∫ÊîæÂ∞ÑÁßë‰∏≠„ÄÇÁÑ∂ËÄåÔºå‰∏çÂêåÊ®°Âûã‰πãÈñìÁöÑÊîπÂñÑÁ®ãÂ∫¶ÊúâÊâÄ‰∏çÂêåÔºõGPT-3.5-turbo Âíå Mixtral-8x7B-instruct-v0.1 ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåËÄå Mistral-7B-instruct-v0.2 Ê≤íÊúâÊîπÂñÑÔºåÁ™ÅÈ°ØÂÖ∂ÊúâÊïàÊÄßÁöÑËÆäÁï∞ÊÄß„ÄÇLLM Âú®Áç≤ÂæóË∂ÖÂá∫ÂÖ∂Ë®ìÁ∑¥Ë≥áÊñôÁöÑÁâπÂÆöÈ†òÂüüË≥áÊñôÊôÇÊúÉÂèóÁõä„ÄÇÂ∞çÊñºÊîæÂ∞ÑÁßëÔºåRadioRAG Âª∫Á´ã‰∫Ü‰∏ÄÂÄãÂº∑Â§ßÁöÑÊû∂ÊßãÔºåÂèØÂ§ßÂπÖÊîπÂñÑÊîæÂ∞ÑÁßëÂïèÁ≠î‰∏≠ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÁúüÂØ¶ÊÄß„ÄÇ

##### **Can GPT-4 learn to analyze moves in research article abstracts?**
2407.15612v1 by Danni Yu, Marina Bondi, Ken Hylannd

One of the most powerful and enduring ideas in written discourse analysis is
that genres can be described in terms of the moves which structure a writer's
purpose. Considerable research has sought to identify these distinct
communicative acts, but analyses have been beset by problems of subjectivity,
reliability and the time-consuming need for multiple coders to confirm
analyses. In this paper we employ the affordances of GPT-4 to automate the
annotation process by using natural language prompts. Focusing on abstracts
from articles in four applied linguistics journals, we devise prompts which
enable the model to identify moves effectively. The annotated outputs of these
prompts were evaluated by two assessors with a third addressing disagreements.
The results show that an 8-shot prompt was more effective than one using two,
confirming that the inclusion of examples illustrating areas of variability can
enhance GPT-4's ability to recognize multiple moves in a single sentence and
reduce bias related to textual position. We suggest that GPT-4 offers
considerable potential in automating this annotation process, when human actors
with domain specific linguistic expertise inform the prompting process.

ÊëòË¶ÅÔºöÊõ∏Èù¢Ë´ñËø∞ÂàÜÊûê‰∏≠ÊúÄÂº∑Â§ß‰∏îÊ≠∑‰πÖ‰∏çË°∞ÁöÑÊ¶ÇÂøµ‰πã‰∏ÄÔºåÊòØÊñáÈ°ûÂèØ‰ª•Ê†πÊìöÊßãÊàê‰ΩúËÄÖÁõÆÁöÑÁöÑÊ≠•È©ü‰æÜÊèèËø∞„ÄÇÂ§ßÈáèÁöÑÁ†îÁ©∂Ë©¶ÂúñÊâæÂá∫ÈÄô‰∫õ‰∏çÂêåÁöÑÊ∫ùÈÄöË°åÁÇ∫Ôºå‰ΩÜÂàÜÊûê‰∏ÄÁõ¥ÂèóÂà∞‰∏ªËßÄÊÄß„ÄÅÂèØÈù†ÊÄß‰ª•ÂèäÁ¢∫Ë™çÂàÜÊûêÈúÄË¶ÅÂ§öÂÄãÁ∑®Á¢ºÂô®ËÄåËÄóÊôÇÁ≠âÂïèÈ°åÁöÑÂõ∞Êìæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé°Áî® GPT-4 ÁöÑÂäüËÉΩÔºå‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÊèêÁ§∫‰æÜËá™ÂãïÂåñË®ªËß£Á®ãÂ∫è„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÂõõÊú¨ÊáâÁî®Ë™ûË®ÄÂ≠∏ÊúüÂàä‰∏≠ÁöÑÊëòË¶ÅÔºåË®≠Ë®àÊèêÁ§∫ËÆìÊ®°ÂûãËÉΩÊúâÊïàË≠òÂà•Ê≠•È©ü„ÄÇÈÄô‰∫õÊèêÁ§∫ÁöÑË®ªËß£Ëº∏Âá∫Áî±ÂÖ©‰ΩçË©ï‰º∞ËÄÖË©ï‰º∞ÔºåÁ¨¨‰∏â‰ΩçË©ï‰º∞ËÄÖÂâáËß£Ê±∫ÂàÜÊ≠ß„ÄÇÁµêÊûúÈ°ØÁ§∫Ôºå‰ΩøÁî® 8 ÂÄãÊ≠•È©üÁöÑÊèêÁ§∫ÊØî‰ΩøÁî® 2 ÂÄãÊ≠•È©üÁöÑÊèêÁ§∫Êõ¥ÊúâÊïàÔºåÈÄôË≠âÂØ¶‰∫ÜÂåÖÂê´Ë™™ÊòéËÆäÁï∞ÊÄßÈ†òÂüüÁöÑÁØÑ‰æãÔºåÂèØ‰ª•Â¢ûÂº∑ GPT-4 Âú®ÂñÆ‰∏ÄÂè•Â≠ê‰∏≠Ë≠òÂà•Â§öÂÄãÊ≠•È©üÁöÑËÉΩÂäõÔºå‰∏¶Ê∏õÂ∞ëËàáÊñáÂ≠ó‰ΩçÁΩÆÁõ∏ÈóúÁöÑÂÅèÂ∑Æ„ÄÇÊàëÂÄëÂª∫Ë≠∞ÔºåÁï∂ÂÖ∑ÊúâÁâπÂÆöË™ûË®ÄÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÁöÑ‰∫∫È°ûË°åÁÇ∫ËÄÖÊèê‰æõÊèêÁ§∫Á®ãÂ∫èË≥áË®äÊôÇÔºåGPT-4 Âú®Ëá™ÂãïÂåñÊ≠§Ë®ªËß£Á®ãÂ∫èÊñπÈù¢ÂÖ∑ÊúâÁõ∏Áï∂Â§ßÁöÑÊΩõÂäõ„ÄÇ

##### **StylusAI: Stylistic Adaptation for Robust German Handwritten Text Generation**
2407.15608v1 by Nauman Riaz, Saifullah Saifullah, Stefan Agne, Andreas Dengel, Sheraz Ahmed

In this study, we introduce StylusAI, a novel architecture leveraging
diffusion models in the domain of handwriting style generation. StylusAI is
specifically designed to adapt and integrate the stylistic nuances of one
language's handwriting into another, particularly focusing on blending English
handwriting styles into the context of the German writing system. This approach
enables the generation of German text in English handwriting styles and German
handwriting styles into English, enriching machine-generated handwriting
diversity while ensuring that the generated text remains legible across both
languages. To support the development and evaluation of StylusAI, we present
the \lq{Deutscher Handschriften-Datensatz}\rq~(DHSD), a comprehensive dataset
encompassing 37 distinct handwriting styles within the German language. This
dataset provides a fundamental resource for training and benchmarking in the
realm of handwritten text generation. Our results demonstrate that StylusAI not
only introduces a new method for style adaptation in handwritten text
generation but also surpasses existing models in generating handwriting samples
that improve both text quality and stylistic fidelity, evidenced by its
performance on the IAM database and our newly proposed DHSD. Thus, StylusAI
represents a significant advancement in the field of handwriting style
generation, offering promising avenues for future research and applications in
cross-linguistic style adaptation for languages with similar scripts.

ÊëòË¶ÅÔºöÂú®Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü StylusAIÔºå‰∏ÄÁ®ÆÂú®ÊâãÂØ´È¢®Ê†ºÁîüÊàêÈ†òÂüüÂà©Áî®Êì¥Êï£Ê®°ÂûãÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇStylusAI Á∂ìÈÅéÁâπÂà•Ë®≠Ë®àÔºåÁî®ÊñºÈÅ©ÊáâÂíåÊï¥Âêà‰∏ÄÁ®ÆË™ûË®ÄÁöÑÊâãÂØ´È¢®Ê†ºÁöÑÈ¢®Ê†ºÁ¥∞ÂæÆÂ∑ÆÂà•Âà∞Âè¶‰∏ÄÁ®ÆË™ûË®Ä‰∏≠ÔºåÁâπÂà•ËëóÈáçÊñºÂ∞áËã±ÊñáÊâãÂØ´È¢®Ê†ºËûçÂÖ•Âæ∑ÊñáÂØ´‰ΩúÁ≥ªÁµ±ÁöÑË™ûÂ¢É‰∏≠„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÉΩÂ§†ÁîüÊàêËã±ÊñáÊâãÂØ´È¢®Ê†ºÁöÑÂæ∑ÊñáÊñáÊú¨ÂíåÂæ∑ÊñáÊâãÂØ´È¢®Ê†ºÁöÑËã±ÊñáÔºåË±êÂØåÊ©üÂô®ÁîüÊàêÁöÑÊñáÂ≠óÁöÑÂ§öÊ®£ÊÄßÔºåÂêåÊôÇÁ¢∫‰øùÁîüÊàêÁöÑÊñáÂ≠óÂú®ÂÖ©Á®ÆË™ûË®Ä‰∏≠ÈÉΩËÉΩ‰øùÊåÅÊòìËÆÄÊÄß„ÄÇÁÇ∫‰∫ÜÊîØÊåÅ StylusAI ÁöÑÈñãÁôºÂíåË©ï‰º∞ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü„ÄåDeutscher Handschriften-Datensatz„ÄçÔºàDHSDÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊï∏ÊìöÈõÜÔºåÊ∂µËìã‰∫ÜÂæ∑Ë™û‰∏≠ 37 Á®Æ‰∏çÂêåÁöÑÊâãÂØ´È¢®Ê†º„ÄÇÊ≠§Êï∏ÊìöÈõÜÁÇ∫ÊâãÂØ´ÊñáÊú¨ÁîüÊàêÁöÑË®ìÁ∑¥ÂíåÂü∫Ê∫ñÊ∏¨Ë©¶Êèê‰æõ‰∫ÜÂü∫Á§éË≥áÊ∫ê„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåStylusAI ‰∏çÂÉÖÁÇ∫ÊâãÂØ´ÊñáÊú¨ÁîüÊàê‰∏≠ÁöÑÈ¢®Ê†ºÈÅ©ÊáâÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåËÄå‰∏îÂú®ÁîüÊàêÊâãÂØ´Ê®£Êú¨ÊñπÈù¢‰πüË∂ÖË∂ä‰∫ÜÁèæÊúâÊ®°ÂûãÔºåÈÄô‰∫õÊ®£Êú¨ÂêåÊôÇÊîπÂñÑ‰∫ÜÊñáÊú¨ÂìÅË≥™ÂíåÈ¢®Ê†º‰øùÁúüÂ∫¶ÔºåÈÄôÂú® IAM Êï∏ÊìöÂ∫´ÂíåÊàëÂÄëÊñ∞ÊèêÂá∫ÁöÑ DHSD ‰∏äÁöÑË°®Áèæ‰∏≠ÂæóÂà∞‰∫ÜË≠âÊòé„ÄÇÂõ†Ê≠§ÔºåStylusAI ‰ª£Ë°®‰∫ÜÊâãÂØ´È¢®Ê†ºÁîüÊàêÈ†òÂüüÁöÑÈáçÂ§ßÈÄ≤Ê≠•ÔºåÁÇ∫ÂÖ∑ÊúâÁõ∏‰ººÂ≠óÂÖÉÁöÑË™ûË®ÄÁöÑË∑®Ë™ûË®ÄÈ¢®Ê†ºÈÅ©ÊáâÊèê‰æõ‰∫ÜÊú™‰æÜÁ†îÁ©∂ÂíåÊáâÁî®ÊñπÈù¢ÁöÑÂâçÊôØ„ÄÇ

##### **A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism**
2407.15600v1 by Yu Xue, Chenchen Zhu, MengChu Zhou, Mohamed Wahib, Moncef Gabbouj

Neural architecture search (NAS) enables re-searchers to automatically
explore vast search spaces and find efficient neural networks. But NAS suffers
from a key bottleneck, i.e., numerous architectures need to be evaluated during
the search process, which requires a lot of computing resources and time. In
order to improve the efficiency of NAS, a series of methods have been proposed
to reduce the evaluation time of neural architectures. However, they are not
efficient enough and still only focus on the accuracy of architectures. In
addition to the classification accuracy, more efficient and smaller network
architectures are required in real-world applications. To address the above
problems, we propose the SMEM-NAS, a pairwise com-parison relation-assisted
multi-objective evolutionary algorithm based on a multi-population mechanism.
In the SMEM-NAS, a surrogate model is constructed based on pairwise compari-son
relations to predict the accuracy ranking of architectures, rather than the
absolute accuracy. Moreover, two populations cooperate with each other in the
search process, i.e., a main population guides the evolution, while a vice
population expands the diversity. Our method aims to provide high-performance
models that take into account multiple optimization objectives. We conduct a
series of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to
verify its effectiveness. With only a single GPU searching for 0.17 days,
competitive architectures can be found by SMEM-NAS which achieves 78.91%
accuracy with the MAdds of 570M on the ImageNet. This work makes a significant
advance in the important field of NAS.

ÊëòË¶ÅÔºö<paragraph>Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÊêúÂ∞ã (NAS) ËÉΩËÆìÁ†îÁ©∂‰∫∫Âì°Ëá™ÂãïÊé¢Á¥¢Âª£Â§ßÁöÑÊêúÂ∞ãÁ©∫ÈñìÔºå‰∏¶ÊâæÂà∞ÊúâÊïàÁéáÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇ‰ΩÜ NAS ÊúÉÈÅáÂà∞‰∏ÄÂÄã‰∏ªË¶ÅÁöÑÁì∂È†∏Ôºå‰πüÂ∞±ÊòØÂú®ÊêúÂ∞ãÈÅéÁ®ã‰∏≠ÈúÄË¶ÅË©ï‰º∞Â§ßÈáèÁöÑÊû∂ÊßãÔºåÈÄôÈúÄË¶ÅÂ§ßÈáèÁöÑÈÅãÁÆóË≥áÊ∫êÂíåÊôÇÈñì„ÄÇÁÇ∫‰∫ÜÊèêÂçá NAS ÁöÑÊïàÁéáÔºåÂ∑≤Á∂ìÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁöÑÊñπÊ≥ï‰æÜÊ∏õÂ∞ëÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÁöÑË©ï‰º∞ÊôÇÈñì„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÇÑ‰∏çÂ§†ÊúâÊïàÁéáÔºåËÄå‰∏î‰ªçÁÑ∂Âè™ËëóÈáçÊñºÊû∂ÊßãÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÈô§‰∫ÜÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶‰πãÂ§ñÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÈÇÑÈúÄË¶ÅÊõ¥ÊúâÊïàÁéá‰∏îÊõ¥Â∞èÁöÑÁ∂≤Ë∑ØÊû∂Êßã„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ SMEM-NASÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂü∫ÊñºÂ§öÊóèÁæ§Ê©üÂà∂ÁöÑÊàêÂ∞çÊØîËºÉÈóú‰øÇËºîÂä©Â§öÁõÆÊ®ôÊºîÂåñÊºîÁÆóÊ≥ï„ÄÇÂú® SMEM-NAS ‰∏≠ÔºåÊúÉÊ†πÊìöÊàêÂ∞çÊØîËºÉÈóú‰øÇÂª∫Êßã‰∏ÄÂÄã‰ª£ÁêÜÊ®°ÂûãÔºåÁî®‰æÜÈ†êÊ∏¨Êû∂ÊßãÁöÑÊ∫ñÁ¢∫Â∫¶ÊéíÂêçÔºåËÄå‰∏çÊòØÁµïÂ∞çÊ∫ñÁ¢∫Â∫¶„ÄÇÊ≠§Â§ñÔºåÂú®ÊêúÂ∞ãÈÅéÁ®ã‰∏≠ÔºåÂÖ©ÂÄãÊóèÁæ§ÊúÉ‰∫íÁõ∏Âêà‰ΩúÔºå‰πüÂ∞±ÊòØË™™Ôºå‰∏ÄÂÄã‰∏ªÊóèÁæ§ÂºïÂ∞éÊºîÂåñÔºåËÄå‰∏ÄÂÄãÂâØÊóèÁæ§ÂâáÊì¥Â±ïÂ§öÊ®£ÊÄß„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÊó®Âú®Êèê‰æõËÄÉÈáèÂ§öÂÄãÊúÄ‰Ω≥ÂåñÁõÆÊ®ôÁöÑÈ´òÊïàËÉΩÊ®°Âûã„ÄÇÊàëÂÄëÂú® CIFAR-10„ÄÅCIFAR-100 Âíå ImageNet Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫Ü‰∏ÄÁ≥ªÂàóÁöÑÂØ¶È©óÔºå‰ª•È©óË≠âÂÖ∂ÊúâÊïàÊÄß„ÄÇSMEM-NAS Âè™‰ΩøÁî®ÂñÆ‰∏Ä GPU ÊêúÂ∞ã 0.17 Â§©ÔºåÂ∞±ËÉΩÊâæÂà∞Á´∂Áà≠ÂäõÁöÑÊû∂ÊßãÔºåÂú® ImageNet ‰∏ä‰ª• 570M ÁöÑ MAdds ÈÅîÂà∞ 78.91% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂú® NAS ÁöÑÈáçË¶ÅÈ†òÂüü‰∏≠ÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇ</paragraph>

##### **Discrete Flow Matching**
2407.15595v1 by Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman

Despite Flow Matching and diffusion models having emerged as powerful
generative paradigms for continuous variables such as images and videos, their
application to high-dimensional discrete data, such as language, is still
limited. In this work, we present Discrete Flow Matching, a novel discrete flow
paradigm designed specifically for generating discrete data. Discrete Flow
Matching offers several key contributions: (i) it works with a general family
of probability paths interpolating between source and target distributions;
(ii) it allows for a generic formula for sampling from these probability paths
using learned posteriors such as the probability denoiser ($x$-prediction) and
noise-prediction ($\epsilon$-prediction); (iii) practically, focusing on
specific probability paths defined with different schedulers considerably
improves generative perplexity compared to previous discrete diffusion and flow
models; and (iv) by scaling Discrete Flow Matching models up to 1.7B
parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1
and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of
generating high-quality discrete data in a non-autoregressive fashion,
significantly closing the gap between autoregressive models and discrete flow
models.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÊµÅÂåπÈÖçÂíåÊì¥Êï£Ê®°ÂûãÂ∑≤ÊàêÁÇ∫ÈÄ£Á∫åËÆäÊï∏Ôºà‰æãÂ¶ÇÂΩ±ÂÉèÂíåÂΩ±ÁâáÔºâÂº∑Â§ßÁöÑÁîüÊàêÁØÑ‰æãÔºå‰ΩÜÂÆÉÂÄëÂú®È´òÁ∂≠Èõ¢Êï£Ë≥áÊñôÔºà‰æãÂ¶ÇË™ûË®ÄÔºâ‰∏≠ÁöÑÊáâÁî®‰ªçÁÑ∂ÊúâÈôê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Èõ¢Êï£ÊµÅÂåπÈÖçÔºå‰∏ÄÁ®ÆÂ∞àÈñÄÁÇ∫ÁîüÊàêÈõ¢Êï£Ë≥áÊñôËÄåË®≠Ë®àÁöÑÊñ∞Á©éÈõ¢Êï£ÊµÅÁØÑ‰æã„ÄÇÈõ¢Êï£ÊµÅÂåπÈÖçÊèê‰æõÂπæÂÄãÈóúÈçµË≤¢ÁçªÔºö(i) ÂÆÉ‰ΩøÁî®‰∏ÄÂÄãÈÄöÁî®ÁöÑÊ©üÁéáË∑ØÂæëÊóèÔºåÂú®‰æÜÊ∫êÂíåÁõÆÊ®ôÂàÜ‰Ωà‰πãÈñìÂÖßÊèíÔºõ(ii) ÂÆÉÂÖÅË®±‰ΩøÁî®Â∑≤Â≠∏ÁøíÁöÑÂæåÈ©óÔºà‰æãÂ¶ÇÊ©üÁéáÂéªÈõúË®äÂô®Ôºà$x$-È†êÊ∏¨ÔºâÂíåÈõúË®äÈ†êÊ∏¨Ôºà$\epsilon$-È†êÊ∏¨ÔºâÔºâÂæûÈÄô‰∫õÊ©üÁéáË∑ØÂæë‰∏≠ÊäΩÊ®£ÁöÑÈÄöÁî®ÂÖ¨ÂºèÔºõ(iii) ÂØ¶Èöõ‰∏äÔºåÂ∞àÊ≥®Êñº‰ΩøÁî®‰∏çÂêåÊéíÁ®ãÂô®ÂÆöÁæ©ÁöÑÁâπÂÆöÊ©üÁéáË∑ØÂæëÔºåËàáÂÖàÂâçÁöÑÈõ¢Êï£Êì¥Êï£ÂíåÊµÅÊ®°ÂûãÁõ∏ÊØîÔºåÈ°ØËëóÊîπÂñÑ‰∫ÜÁîüÊàêÂõ∞ÊÉëÂ∫¶Ôºõ‰ª•Âèä (iv) ÈÄèÈÅéÂ∞áÈõ¢Êï£ÊµÅÂåπÈÖçÊ®°ÂûãÊì¥ÂÖÖËá≥ 1.7B ÂèÉÊï∏ÔºåÊàëÂÄëÂú® HumanEval ‰∏äÈÅîÂà∞ 6.7% Pass@1 Âíå 13.4% Pass@10ÔºåÂú® 1 Ê¨° MBPP Á∑®Á¢ºÂü∫Ê∫ñ‰∏äÈÅîÂà∞ 6.7% Pass@1 Âíå 20.6% Pass@10„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÂ§†‰ª•ÈùûËá™Ëø¥Ê≠∏ÁöÑÊñπÂºèÁîüÊàêÈ´òÂìÅË≥™Èõ¢Êï£Ë≥áÊñôÔºåÈ°ØËëóÁ∏ÆÂ∞èËá™Ëø¥Ê≠∏Ê®°ÂûãËàáÈõ¢Êï£ÊµÅÊ®°Âûã‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**
2407.15588v1 by Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee

Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge.Existing methods, mostly
supervised, face challenges in obtaining labeled entity pairs. To address this,
recent studies have shifted towards a self-supervised and unsupervised
frameworks. Despite their effectiveness, these approaches have limitations: (1)
they mainly focus on entity features, neglecting the semantic information of
relations, (2) they assume isomorphism between source and target graphs,
leading to noise and reduced alignment accuracy, and (3) they are susceptible
to noise in the textual features, especially when encountering inconsistent
translations or Out-Of-Vocabulary (OOV) problems.
  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual
EA framework that jointly performs Entity-level and Relation-level Alignment
using semantic textual features of relations and entities. Its refinement
process iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification
process examines the entities' neighbor triples as the linearized text. This
\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,
achieving near-perfect alignment even in the presence of noisy textual features
of entities. Our extensive experiments demonstrate that robustness and general
applicability of \proposed improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.

ÊëòË¶ÅÔºöË∑®Ë™ûË®ÄÂØ¶È´îÂ∞çÈΩä (EA) ËÉΩÂ§†Êï¥Âêà‰∏çÂêåË™ûË®Ä‰∏≠ÁöÑÂ§öÂÄãÁü•Ë≠òÂúñË≠ú (KG)ÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÁÑ°Á∏´Âú∞Â≠òÂèñÂ§öÂÖÉ‰∏îÂÖ®Èù¢ÁöÑÁü•Ë≠ò„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÊòØÊúâÁõ£Áù£ÁöÑÔºåÂú®ÂèñÂæóÊ®ôË®òÂØ¶È´îÂ∞çÊôÇÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤ËΩâÂêëËá™Áõ£Áù£ÂíåÁÑ°Áõ£Áù£ÁöÑÊû∂Êßã„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÊúâ‰ª•‰∏ãÈôêÂà∂Ôºö(1) ÂÆÉÂÄë‰∏ªË¶ÅÈóúÊ≥®ÂØ¶È´îÁâπÂæµÔºåÂøΩÁï•Èóú‰øÇÁöÑË™ûÁæ©Ë≥áË®äÔºå(2) ÂÆÉÂÄëÂÅáË®≠‰æÜÊ∫êÂúñË≠úÂíåÁõÆÊ®ôÂúñË≠ú‰πãÈñìÂêåÊßãÔºåÂ∞éËá¥ÈõúË®äÂíåÂ∞çÈΩäÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéÔºå(3) ÂÆÉÂÄëÂÆπÊòìÂèóÂà∞ÊñáÂ≠óÁâπÂæµ‰∏≠ÁöÑÈõúË®äÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂú®ÈÅáÂà∞‰∏ç‰∏ÄËá¥ÁöÑÁøªË≠ØÊàñË©ûÂΩôÂ§ñÂïèÈ°å (OOV) ÊôÇ„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ ERAlignÔºå‰∏ÄÂÄãÁÑ°Áõ£Áù£‰∏îÁ©©ÂÅ•ÁöÑË∑®Ë™ûË®Ä EA Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî®Èóú‰øÇÂíåÂØ¶È´îÁöÑË™ûÁæ©ÊñáÂ≠óÁâπÂæµÔºåÂêåÊôÇÂü∑Ë°åÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩä„ÄÇÂÆÉÁöÑÁ≤æÁÖâÁ®ãÂ∫èÈÄèÈÅéÊ†πÊìöÈÑ∞Êé•‰∏âÂÖÉÁµÑÂåπÈÖçËûçÂêàÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩäÔºåÂèçË¶ÜÂ¢ûÂº∑ÁµêÊûú„ÄÇÈ°çÂ§ñÁöÑÈ©óË≠âÁ®ãÂ∫èÂ∞áÂØ¶È´îÁöÑÈÑ∞Êé•‰∏âÂÖÉÁµÑË¶ñÁÇ∫Á∑öÊÄßÂåñÊñáÂ≠óÈÄ≤Ë°åÊ™¢Êü•„ÄÇÈÄôÂÄãÂö¥Ê†ºË©ï‰º∞Â∞çÈΩäÁµêÊûúÁöÑ„ÄåÂ∞çÈΩäÂíåÈ©óË≠â„ÄçÁÆ°Á∑öÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÂØ¶È´îÁöÑÈõúË®äÊñáÂ≠óÁâπÂæµÊôÇ‰πüËÉΩÈÅîÊàêËøë‰πéÂÆåÁæéÁöÑÂ∞çÈΩä„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºå\proposed ÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÈÅ©Áî®ÊÄßÊèêÂçá‰∫Ü EA ‰ªªÂãôÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊúâÊïàÊÄßÔºåÂ∞çÁü•Ë≠òÂ∞éÂêëÊáâÁî®Á®ãÂºèÊúâÈ°ØËëóÁöÑË≤¢Áçª„ÄÇ

##### **An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought**
2407.15569v1 by Yuetong Zhao, Hongyu Cao, Xianyu Zhao, Zhijian Ou

Since the launch of ChatGPT at the end of 2022, generative dialogue models
represented by ChatGPT have quickly become essential tools in daily life. As
user expectations increase, enhancing the capability of generative dialogue
models to solve complex problems has become a focal point of current research.
This paper delves into the effectiveness of the RAFT (Retrieval Augmented
Fine-Tuning) method in improving the performance of Generative dialogue models.
RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and
retrieval augmented generation (RAG), which significantly enhanced the model's
information extraction and logical reasoning abilities. We evaluated the RAFT
method across multiple datasets and analysed its performance in various
reasoning tasks, including long-form QA and short-form QA tasks, tasks in both
Chinese and English, and supportive and comparison reasoning tasks. Notably, it
addresses the gaps in previous research regarding long-form QA tasks and
Chinese datasets. Moreover, we also evaluate the benefit of the
chain-of-thought (CoT) in the RAFT method. This work offers valuable insights
for studies focused on enhancing the performance of generative dialogue models.

ÊëòË¶ÅÔºöËá™ 2022 Âπ¥Â∫ï ChatGPT Êé®Âá∫‰ª•Êù•Ôºå‰ª• ChatGPT ‰∏∫‰ª£Ë°®ÁöÑÁîüÊàêÂºèÂØπËØùÊ®°ÂûãËøÖÈÄüÊàê‰∏∫Êó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑÂøÖÂ§áÂ∑•ÂÖ∑„ÄÇÈöèÁùÄÁî®Êà∑ÊúüÊúõÁöÑÊèêÈ´òÔºåÂ¢ûÂº∫ÁîüÊàêÂºèÂØπËØùÊ®°ÂûãËß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÁöÑËÉΩÂäõÂ∑≤Êàê‰∏∫ÂΩìÂâçÁ†îÁ©∂ÁöÑÈáçÁÇπ„ÄÇÊú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫Ü RAFTÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÂæÆË∞ÉÔºâÊñπÊ≥ïÂú®ÊèêÈ´òÁîüÊàêÂºèÂØπËØùÊ®°ÂûãÊÄßËÉΩÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇRAFT Â∞ÜÊÄùÊÉ≥Èìæ‰∏éÊ®°ÂûãÁõëÁù£ÂæÆË∞É (SFT) ÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Áõ∏ÁªìÂêàÔºåÊòæÁùÄÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑ‰ø°ÊÅØÊèêÂèñÂíåÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Ë∑®Â§ö‰∏™Êï∞ÊçÆÈõÜËØÑ‰º∞‰∫Ü RAFT ÊñπÊ≥ïÔºåÂπ∂ÂàÜÊûê‰∫ÜÂÆÉÂú®ÂêÑÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂåÖÊã¨ÈïøÁØáÈóÆÁ≠îÂíåÁü≠ÁØáÈóÆÁ≠î‰ªªÂä°„ÄÅ‰∏≠Ëã±Êñá‰ªªÂä°‰ª•ÂèäÊîØÊåÅÊÄßÂíåÊØîËæÉÊé®ÁêÜ‰ªªÂä°„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂÆÉËß£ÂÜ≥‰∫ÜÂÖàÂâçÁ†îÁ©∂‰∏≠ÂÖ≥‰∫éÈïøÁØáÈóÆÁ≠î‰ªªÂä°Âíå‰∏≠ÊñáÊï∞ÊçÆÈõÜÁöÑÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòËØÑ‰º∞‰∫Ü RAFT ÊñπÊ≥ï‰∏≠ÊÄùÊÉ≥Èìæ (CoT) ÁöÑÂ•ΩÂ§Ñ„ÄÇËøôÈ°πÂ∑•‰Ωú‰∏∫‰∏ìÊ≥®‰∫éÂ¢ûÂº∫ÁîüÊàêÂºèÂØπËØùÊ®°ÂûãÊÄßËÉΩÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂÆùË¥µÁöÑËßÅËß£„ÄÇ

##### **SETTP: Style Extraction and Tunable Inference via Dual-level Transferable Prompt Learning**
2407.15556v1 by Chunzhen Jin, Yongfeng Huang, Yaqi Wang, Peng Cao, Osmar Zaiane

Text style transfer, an important research direction in natural language
processing, aims to adapt the text to various preferences but often faces
challenges with limited resources. In this work, we introduce a novel method
termed Style Extraction and Tunable Inference via Dual-level Transferable
Prompt Learning (SETTP) for effective style transfer in low-resource scenarios.
First, SETTP learns source style-level prompts containing fundamental style
characteristics from high-resource style transfer. During training, the source
style-level prompts are transferred through an attention module to derive a
target style-level prompt for beneficial knowledge provision in low-resource
style transfer. Additionally, we propose instance-level prompts obtained by
clustering the target resources based on the semantic content to reduce
semantic bias. We also propose an automated evaluation approach of style
similarity based on alignment with human evaluations using ChatGPT-4. Our
experiments across three resourceful styles show that SETTP requires only
1/20th of the data volume to achieve performance comparable to state-of-the-art
methods. In tasks involving scarce data like writing style and role style,
SETTP outperforms previous methods by 16.24\%.

ÊëòË¶ÅÔºöÊñáÂ≠óÊ®£ÂºèËΩâÁßªÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÊó®Âú®Â∞áÊñáÂ≠óË™øÊï¥Ëá≥ÂêÑÁ®ÆÂÅèÂ•ΩÔºå‰ΩÜÂ∏∏Â∏∏Èù¢Ëá®Ë≥áÊ∫êÊúâÈôêÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁ®±ÁÇ∫ÈõôÂ±§ÂèØËΩâÁßªÊèêÁ§∫Â≠∏ÁøíÁöÑÊ®£ÂºèËêÉÂèñÂíåÂèØË™øÂºèÊé®Ë´ñ (SETTP)ÔºåÁî®ÊñºÂú®‰ΩéË≥áÊ∫êÂ†¥ÊôØ‰∏≠ÈÄ≤Ë°åÊúâÊïàÁöÑÊ®£ÂºèËΩâÁßª„ÄÇÈ¶ñÂÖàÔºåSETTP Â≠∏ÁøíÂæûÈ´òË≥áÊ∫êÊ®£ÂºèËΩâÁßª‰∏≠ÂåÖÂê´Âü∫Êú¨Ê®£ÂºèÁâπÂæµÁöÑ‰æÜÊ∫êÊ®£ÂºèÂ±§Á¥öÊèêÁ§∫„ÄÇÂú®Ë®ìÁ∑¥ÊúüÈñìÔºå‰æÜÊ∫êÊ®£ÂºèÂ±§Á¥öÊèêÁ§∫ÊúÉÈÄèÈÅéÊ≥®ÊÑèÂäõÊ®°ÁµÑËΩâÁßªÔºå‰ª•Ë°çÁîü‰∏ÄÂÄãÁõÆÊ®ôÊ®£ÂºèÂ±§Á¥öÊèêÁ§∫ÔºåÂú®‰ΩéË≥áÊ∫êÊ®£ÂºèËΩâÁßª‰∏≠Êèê‰æõÊúâÁõäÁöÑÁü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫ÈÄèÈÅéÊ†πÊìöË™ûÊÑèÂÖßÂÆπÂ∞çÁõÆÊ®ôË≥áÊ∫êÈÄ≤Ë°åÂàÜÁæ§ËÄåÁç≤ÂæóÁöÑÂØ¶‰æãÂ±§Á¥öÊèêÁ§∫Ôºå‰ª•Ê∏õÂ∞ëË™ûÊÑèÂÅèÂ∑Æ„ÄÇÊàëÂÄëÈÇÑÊèêÂá∫‰∏ÄÂÄãËá™ÂãïÂåñÁöÑÊ®£ÂºèÁõ∏‰ººÊÄßË©ï‰º∞ÊñπÊ≥ïÔºåÂü∫ÊñºËàá‰ΩøÁî® ChatGPT-4 ÁöÑ‰∫∫È°ûË©ï‰º∞ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÈáùÂ∞ç‰∏âÁ®ÆË±êÂØåÁöÑÊ®£ÂºèÈÄ≤Ë°åÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåSETTP Âè™ÈúÄË¶Å 1/20 ÁöÑË≥áÊñôÈáèÔºåÂ∞±ËÉΩÈÅîÂà∞ËàáÁèæÊúâÊäÄË°ìÁõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇÂú®Ê∂âÂèäÁ®ÄÊúâË≥áÊñôÔºà‰æãÂ¶ÇÂØ´‰ΩúÊ®£ÂºèÂíåËßíËâ≤Ê®£ÂºèÔºâÁöÑ‰ªªÂãô‰∏≠ÔºåSETTP ÁöÑË°®ÁèæÂÑ™ÊñºÂÖàÂâçÁöÑÊäÄË°ì 16.24%„ÄÇ

##### **Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs**
2407.15549v1 by Abhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry Sleight, Asa Cooper Stickland, Ethan Perez, Dylan Hadfield-Menell, Stephen Casper

Large language models (LLMs) can often be made to behave in undesirable ways
that they are explicitly fine-tuned not to. For example, the LLM red-teaming
literature has produced a wide variety of `jailbreaking' techniques to elicit
harmful text from models that were fine-tuned to be harmless. Recent work on
red-teaming, model editing, and interpretability suggests that this challenge
stems from how (adversarial) fine-tuning largely serves to suppress rather than
remove undesirable capabilities from LLMs. Prior work has introduced latent
adversarial training (LAT) as a way to improve robustness to broad classes of
failures. These prior works have considered untargeted latent space attacks
where the adversary perturbs latent activations to maximize loss on examples of
desirable behavior. Untargeted LAT can provide a generic type of robustness but
does not leverage information about specific failure modes. Here, we experiment
with targeted LAT where the adversary seeks to minimize loss on a specific
competing task. We find that it can augment a wide variety of state-of-the-art
methods. First, we use targeted LAT to improve robustness to jailbreaks,
outperforming a strong R2D2 baseline with orders of magnitude less compute.
Second, we use it to more effectively remove backdoors with no knowledge of the
trigger. Finally, we use it to more effectively unlearn knowledge for specific
undesirable tasks in a way that is also more robust to re-learning. Overall,
our results suggest that targeted LAT can be an effective tool for defending
against harmful behaviors from LLMs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏ÊúÉË¢´Ë™òÂ∞éÂÅöÂá∫‰∏çËâØË°åÁÇ∫ÔºåÂç≥‰ΩøÂÆÉÂÄëÁ∂ìÈÅéÊòéÁ¢∫ÂæÆË™ø‰ª•ÈÅøÂÖçÈÄôÁ®ÆË°åÁÇ∫„ÄÇ‰æãÂ¶ÇÔºåLLM Á¥ÖÈöäÊñáÁçªÁî¢Áîü‰∫ÜÂêÑÁ®Æ„ÄåË∂äÁçÑ„ÄçÊäÄË°ìÔºåÂæûÁ∂ìÈÅéÂæÆË™ø‰ª•Á¢∫‰øùÁÑ°ÂÆ≥ÁöÑÊ®°Âûã‰∏≠ÂºïÂá∫ÊúâÂÆ≥ÊñáÂ≠ó„ÄÇÊúÄËøëÈóúÊñºÁ¥ÖÈöä„ÄÅÊ®°ÂûãÁ∑®ËºØÂíåÂèØËß£ÈáãÊÄßÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈÄôÂÄãÊåëÊà∞Ê∫êÊñºÔºàÂ∞çÊäóÊÄßÁöÑÔºâÂæÆË™øÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÁî®ÊñºÂ£ìÂà∂ÔºåËÄå‰∏çÊòØÁßªÈô§ LLM ‰∏≠‰∏çËâØÁöÑËÉΩÂäõ„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫ÜÊΩõÂú®Â∞çÊäóË®ìÁ∑¥ (LAT) ‰ΩúÁÇ∫‰∏ÄÁ®ÆÊîπÂñÑÂ∞çÂª£Ê≥õÈ°ûÂà•ÁöÑÊïÖÈöúÁöÑÂÅ•Â£ØÊÄßÁöÑÊñπÊ≥ï„ÄÇÈÄô‰∫õÂÖàÂâçÁöÑÁ†îÁ©∂ËÄÉÊÖÆ‰∫ÜÊú™ÈáùÂ∞çÊΩõÂú®Á©∫ÈñìÊîªÊìäÔºåÂÖ∂‰∏≠Â∞çÊâãÊìæÂãïÊΩõÂú®ÊøÄÊ¥ª‰ª•ÊúÄÂ§ßÂåñÂ∞çÁêÜÊÉ≥Ë°åÁÇ∫ÁØÑ‰æãÁöÑÊêçÂ§±„ÄÇÊú™ÈáùÂ∞çÁöÑ LAT ÂèØ‰ª•Êèê‰æõ‰∏ÄÁ®ÆÈÄöÁî®ÁöÑÂÅ•Â£ØÊÄßÔºå‰ΩÜ‰∏çÊúÉÂà©Áî®ÊúâÈóúÁâπÂÆöÊïÖÈöúÊ®°ÂºèÁöÑË≥áË®ä„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÂØ¶È©ó‰∫ÜÁõÆÊ®ô LATÔºåÂÖ∂‰∏≠Â∞çÊâãË©¶ÂúñÊúÄÂ∞èÂåñÂ∞çÁâπÂÆöÁ´∂Áà≠‰ªªÂãôÁöÑÊêçÂ§±„ÄÇÊàëÂÄëÁôºÁèæÂÆÉÂèØ‰ª•Êì¥ÂÖÖÂêÑÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî®ÁõÆÊ®ô LAT ‰æÜÊîπÂñÑÂ∞çË∂äÁçÑÁöÑÂÅ•Â£ØÊÄßÔºå‰ª•ÊØîÂº∑Â§ßÁöÑ R2D2 Âü∫Á∑öÂ∞ëÂπæÂÄãÊï∏ÈáèÁ¥öÁöÑË®àÁÆóÂäõË°®ÁèæÂæóÊõ¥Â•Ω„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄë‰ΩøÁî®ÂÆÉÂú®‰∏çÁû≠Ëß£Ëß∏ÁôºÂô®ÁöÑÁãÄÊ≥Å‰∏ãÊõ¥ÊúâÊïàÂú∞ÁßªÈô§ÂæåÈñÄ„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰ΩøÁî®ÂÆÉ‰ª•Êõ¥ÊúâÊïàÁöÑÊñπÂºèÂèñÊ∂àÂ≠∏ÁøíÁâπÂÆö‰∏çËâØ‰ªªÂãôÁöÑÁü•Ë≠òÔºåËÄå‰∏îÈÄôÁ®ÆÊñπÂºèÂ∞çÊñºÈáçÊñ∞Â≠∏Áøí‰πüÊõ¥ÂÅ•Â£Ø„ÄÇÁ∏ΩÁöÑ‰æÜË™™ÔºåÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÁõÆÊ®ô LAT ÂèØ‰ª•ÊàêÁÇ∫Èò≤Á¶¶ LLM ÊúâÂÆ≥Ë°åÁÇ∫ÁöÑÊúâÊïàÂ∑•ÂÖ∑„ÄÇ

##### **Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks**
2407.15532v1 by Kamesh Korangi, Christophe Mues, Cristi√°n Bravo

Apart from assessing individual asset performance, investors in financial
markets also need to consider how a set of firms performs collectively as a
portfolio. Whereas traditional Markowitz-based mean-variance portfolios are
widespread, network-based optimisation techniques have built upon these
developments. However, most studies do not contain firms at risk of default and
remove any firms that drop off indices over a certain time. This is the first
study to incorporate risky firms and use all the firms in portfolio
optimisation. We propose and empirically test a novel method that leverages
Graph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs).
GNNs, as deep learning-based models, can exploit network data to uncover
nonlinear relationships. Their ability to handle high-dimensional features and
accommodate customised layers for specific purposes makes them particularly
appealing for large-scale problems such as mid- and small-cap portfolio
optimization. This study utilises 30 years of data on mid-cap firms, creating
graphs of firms using distance correlation and the Triangulated Maximally
Filtered Graph approach. These graphs are the inputs to a GAT model that we
train using custom layers which impose weight and allocation constraints and a
loss function derived from the Sharpe ratio, thus directly maximising portfolio
risk-adjusted returns. This new model is benchmarked against a network
characteristic-based portfolio, a mean variance-based portfolio, and an
equal-weighted portfolio. The results show that the portfolio produced by the
GAT-based model outperforms all benchmarks and is consistently superior to
other strategies over a long period while also being informative of market
dynamics.

ÊëòË¶ÅÔºöÈô§‰∫ÜË©ï‰º∞ÂÄãÂà•Ë≥áÁî¢Á∏æÊïàÔºåÈáëËûçÂ∏ÇÂ†¥ÁöÑÊäïË≥á‰∫∫ÈÇÑÈúÄË¶ÅËÄÉÈáè‰∏ÄÁµÑÂÖ¨Âè∏‰ΩúÁÇ∫ÊäïË≥áÁµÑÂêàÁöÑÊï¥È´îË°®Áèæ„ÄÇÂÇ≥Áµ±ÁöÑÈ¶¨ÁßëÁ∂≠Ëå≤ÂùáÂÄº-ËÆäÁï∞Êï∏ÊäïË≥áÁµÑÂêàÈõñÁÑ∂Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÂü∫ÊñºÁ∂≤Ë∑ØÁöÑÊúÄ‰Ω≥ÂåñÊäÄË°ìÂ∑≤Âú®ÈÄô‰∫õÁôºÂ±ïÁöÑÂü∫Á§é‰∏äÊõ¥ÈÄ≤‰∏ÄÊ≠•„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Á†îÁ©∂‰∏¶Êú™Á¥çÂÖ•ÊúâÈÅïÁ¥ÑÈ¢®Èö™ÁöÑÂÖ¨Âè∏Ôºå‰∏¶ÁßªÈô§Âú®ÁâπÂÆöÊôÇÈñìÂÖßË∑åÂá∫ÊåáÊï∏ÁöÑÊâÄÊúâÂÖ¨Âè∏„ÄÇÊú¨Á†îÁ©∂È¶ñÊ¨°Á¥çÂÖ•È¢®Èö™ÂÖ¨Âè∏Ôºå‰∏¶Âú®ÊäïË≥áÁµÑÂêàÊúÄ‰Ω≥Âåñ‰∏≠‰ΩøÁî®ÊâÄÊúâÂÖ¨Âè∏„ÄÇÊàëÂÄëÊèêÂá∫‰∏¶ÂØ¶Ë≠âÊ™¢È©ó‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®ÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑ØÔºàGATÔºâÔºåÈÄôÊòØÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâÁöÑ‰∏ÄÁ®ÆÂ≠êÈ°û„ÄÇGNN ‰ΩúÁÇ∫Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊ®°ÂûãÔºåÂèØ‰ª•Âà©Áî®Á∂≤Ë∑ØË≥áÊñôÊâæÂá∫ÈùûÁ∑öÊÄßÈóú‰øÇ„ÄÇÂÆÉÂÄëËôïÁêÜÈ´òÁ∂≠Â∫¶ÁâπÂæµ‰∏¶ÈáùÂ∞çÁâπÂÆöÁõÆÁöÑÂÆπÁ¥çËá™Ë®ÇÂ±§ÁöÑËÉΩÂäõÔºå‰ΩøÂÖ∂ÁâπÂà•ÈÅ©ÂêàÊñºÂ§ßË¶èÊ®°ÂïèÈ°åÔºå‰æãÂ¶Ç‰∏≠Â∞èÂûãËÇ°ÊäïË≥áÁµÑÂêàÊúÄ‰Ω≥Âåñ„ÄÇÊú¨Á†îÁ©∂Âà©Áî® 30 Âπ¥ÁöÑ‰∏≠ÂûãËÇ°ÂÖ¨Âè∏Ë≥áÊñôÔºå‰ΩøÁî®Ë∑ùÈõ¢Áõ∏ÈóúÊÄßÂíå‰∏âËßíÂΩ¢ÊúÄÂ§ßÊøæÊ≥¢ÂúñÂΩ¢ÊñπÊ≥ïÂª∫Á´ãÂÖ¨Âè∏ÂúñÂΩ¢„ÄÇÈÄô‰∫õÂúñÂΩ¢ÊòØ GAT Ê®°ÂûãÁöÑËº∏ÂÖ•ÔºåÊàëÂÄë‰ΩøÁî®Ëá™Ë®ÇÂ±§Ë®ìÁ∑¥Ë©≤Ê®°ÂûãÔºåÈÄô‰∫õÂ±§ÊúÉÊñΩÂä†Ê¨äÈáçÂíåÈÖçÁΩÆÈôêÂà∂Ôºå‰ª•ÂèäÊ∫êËá™Â§èÊôÆÊØîÁéáÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåÂæûËÄåÁõ¥Êé•ÊúÄÂ§ßÂåñÊäïË≥áÁµÑÂêàÁöÑÈ¢®Èö™Ë™øÊï¥Â†±ÈÖ¨„ÄÇÈÄôÂÄãÊñ∞Ê®°ÂûãËàáÂü∫ÊñºÁ∂≤Ë∑ØÁâπÂæµÁöÑÊäïË≥áÁµÑÂêà„ÄÅÂü∫ÊñºÂùáÂÄºËÆäÁï∞Êï∏ÁöÑÊäïË≥áÁµÑÂêàÂíåÁ≠âÊ¨äÈáçÊäïË≥áÁµÑÂêàÈÄ≤Ë°åÂü∫Ê∫ñÊØîËºÉ„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÂü∫Êñº GAT ÁöÑÊ®°ÂûãÊâÄÁî¢ÁîüÁöÑÊäïË≥áÁµÑÂêàÂÑ™ÊñºÊâÄÊúâÂü∫Ê∫ñÔºå‰∏¶‰∏îÂú®Èï∑ÊúüÂÖßÂßãÁµÇÂÑ™ÊñºÂÖ∂‰ªñÁ≠ñÁï•ÔºåÂêåÊôÇ‰πüËÉΩÊèê‰æõÂ∏ÇÂ†¥ÂãïÊÖãÁöÑË≥áË®ä„ÄÇ

##### **Interpretable Concept-Based Memory Reasoning**
2407.15527v1 by David Debot, Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra

The lack of transparency in the decision-making processes of deep learning
systems presents a significant challenge in modern artificial intelligence
(AI), as it impairs users' ability to rely on and verify these systems. To
address this challenge, Concept Bottleneck Models (CBMs) have made significant
progress by incorporating human-interpretable concepts into deep learning
architectures. This approach allows predictions to be traced back to specific
concept patterns that users can understand and potentially intervene on.
However, existing CBMs' task predictors are not fully interpretable, preventing
a thorough analysis and any form of formal verification of their
decision-making process prior to deployment, thereby raising significant
reliability concerns. To bridge this gap, we introduce Concept-based Memory
Reasoner (CMR), a novel CBM designed to provide a human-understandable and
provably-verifiable task prediction process. Our approach is to model each task
prediction as a neural selection mechanism over a memory of learnable logic
rules, followed by a symbolic evaluation of the selected rule. The presence of
an explicit memory and the symbolic evaluation allow domain experts to inspect
and formally verify the validity of certain global properties of interest for
the task prediction process. Experimental results demonstrate that CMR achieves
comparable accuracy-interpretability trade-offs to state-of-the-art CBMs,
discovers logic rules consistent with ground truths, allows for rule
interventions, and allows pre-deployment verification.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÁ≥ªÁµ±Ê±∫Á≠ñÈÅéÁ®ãÁº∫‰πèÈÄèÊòéÂ∫¶ÔºåÂ∞çÁèæ‰ª£‰∫∫Â∑•Êô∫ÊÖß (AI) ‰æÜË™™ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞ÔºåÂõ†ÁÇ∫ÈÄôÊúÉÊêçÂÆ≥‰ΩøÁî®ËÄÖ‰æùË≥¥ÂíåÈ©óË≠âÈÄô‰∫õÁ≥ªÁµ±ÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞ÔºåÊ¶ÇÂøµÁì∂È†∏Ê®°Âûã (CBM) ÈÄèÈÅéÂ∞á‰∫∫È°ûÂèØËß£ÈáãÁöÑÊ¶ÇÂøµÁ¥çÂÖ•Ê∑±Â∫¶Â≠∏ÁøíÊû∂Êßã‰∏≠ÔºåÂèñÂæó‰∫ÜÈ°ØËëóÈÄ≤Â±ï„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂÖÅË®±Â∞áÈ†êÊ∏¨ËøΩÊ∫ØÂà∞‰ΩøÁî®ËÄÖÂèØ‰ª•ÁêÜËß£‰∏¶ÂèØËÉΩ‰ªãÂÖ•ÁöÑÁâπÂÆöÊ¶ÇÂøµÊ®°Âºè„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ CBM ‰ªªÂãôÈ†êÊ∏¨Âô®‰∏¶ÈùûÂÆåÂÖ®ÂèØËß£ÈáãÔºåÈÄôÊúÉÂ¶®Á§ôÂú®ÈÉ®ÁΩ≤ÂâçÂæπÂ∫ïÂàÜÊûêÂíåÊ≠£ÂºèÈ©óË≠âÂÖ∂Ê±∫Á≠ñÈÅéÁ®ãÁöÑ‰ªª‰ΩïÂΩ¢ÂºèÔºåÂæûËÄåÂºïÁôºÈáçÂ§ßÁöÑÂèØÈù†ÊÄßÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂü∫ÊñºÊ¶ÇÂøµÁöÑË®òÊÜ∂Êé®ÁêÜÂô® (CMR)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ CBMÔºåÊó®Âú®Êèê‰æõ‰∫∫È°ûÂèØÁêÜËß£‰∏îÂèØË≠âÊòéÈ©óË≠âÁöÑ‰ªªÂãôÈ†êÊ∏¨ÈÅéÁ®ã„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÂ∞áÊØèÂÄã‰ªªÂãôÈ†êÊ∏¨Âª∫Ê®°ÁÇ∫Âú®ÂèØÂ≠∏ÁøíÈÇèËºØË¶èÂâáÁöÑË®òÊÜ∂È´î‰∏äÈÄ≤Ë°åÁ•ûÁ∂ìÈÅ∏ÊìáÊ©üÂà∂ÔºåÊé•ËëóÂ∞çÊâÄÈÅ∏Ë¶èÂâáÈÄ≤Ë°åÁ¨¶ËôüË©ï‰º∞„ÄÇÊòéÁ¢∫Ë®òÊÜ∂ÂíåÁ¨¶ËôüË©ï‰º∞ÁöÑÂ≠òÂú®ÔºåËÆìÈ†òÂüüÂ∞àÂÆ∂ÂèØ‰ª•Ê™¢Êü•‰∏¶Ê≠£ÂºèÈ©óË≠â‰ªªÂãôÈ†êÊ∏¨ÈÅéÁ®ãÂ∞çÊüê‰∫õÊÑüËààË∂£ÁöÑÂÖ®Â±ÄÂ±¨ÊÄßÁöÑÊúâÊïàÊÄß„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåCMR ÈÅîÂà∞‰∫ÜËàáÊúÄÂÖàÈÄ≤ÁöÑ CBM Áõ∏Áï∂ÁöÑÊ∫ñÁ¢∫ÊÄßÂèØËß£ÈáãÊÄßÊ¨äË°°ÔºåÁôºÁèæËàáÂü∫Êú¨‰∫ãÂØ¶‰∏ÄËá¥ÁöÑÈÇèËºØË¶èÂâáÔºåÂÖÅË®±Ë¶èÂâá‰ªãÂÖ•Ôºå‰∏¶ÂÖÅË®±È†êÈÉ®ÁΩ≤È©óË≠â„ÄÇ

##### **Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks**
2407.15526v1 by Eugenio Lomurno, Matteo Matteucci

Generative artificial intelligence has transformed the generation of
synthetic data, providing innovative solutions to challenges like data scarcity
and privacy, which are particularly critical in fields such as medicine.
However, the effective use of this synthetic data to train high-performance
models remains a significant challenge. This paper addresses this issue by
introducing Knowledge Recycling (KR), a pipeline designed to optimise the
generation and use of synthetic data for training downstream classifiers. At
the heart of this pipeline is Generative Knowledge Distillation (GKD), the
proposed technique that significantly improves the quality and usefulness of
the information provided to classifiers through a synthetic dataset
regeneration and soft labelling mechanism. The KR pipeline has been tested on a
variety of datasets, with a focus on six highly heterogeneous medical image
datasets, ranging from retinal images to organ scans. The results show a
significant reduction in the performance gap between models trained on real and
synthetic data, with models based on synthetic data outperforming those trained
on real data in some cases. Furthermore, the resulting models show almost
complete immunity to Membership Inference Attacks, manifesting privacy
properties missing in models trained with conventional techniques.

ÊëòË¶ÅÔºöÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÂ∑≤Á∂ìËΩâËÆä‰∫ÜÂêàÊàêË≥áÊñôÁöÑÁîüÊàêÔºåÊèê‰æõ‰∫ÜÂâµÊñ∞ÁöÑËß£Ê±∫ÊñπÊ°à‰æÜÊáâÂ∞çË≥áÊñôÁ®ÄÂ∞ëÂíåÈö±ÁßÅÁ≠âÊåëÊà∞ÔºåÈÄôÂú®ÈÜ´Â≠∏Á≠âÈ†òÂüüÁâπÂà•ÈáçË¶Å„ÄÇ
ÁÑ∂ËÄåÔºåÊúâÊïà‰ΩøÁî®ÈÄô‰∫õÂêàÊàêË≥áÊñô‰æÜË®ìÁ∑¥È´òÊÄßËÉΩÊ®°Âûã‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊú¨ÊñáÈÄöÈÅéÂºïÂÖ•Áü•Ë≠òÂæ™Áí∞ (KR) ‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®ÂÑ™ÂåñÂêàÊàêË≥áÊñôÁöÑÁîüÊàêÂíå‰ΩøÁî®‰ª•Ë®ìÁ∑¥‰∏ãÊ∏∏ÂàÜÈ°ûÂô®ÁöÑÁÆ°ÈÅì„ÄÇÈÄôÂÄãÁÆ°ÈÅìÁöÑÊ†∏ÂøÉÊòØÁîüÊàêÂºèÁü•Ë≠òËí∏È§æ (GKD)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊèêÂá∫ÁöÑÊäÄË°ìÔºåÂÆÉÈÄöÈÅéÂêàÊàêË≥áÊñôÈõÜÂÜçÁîüÂíåËªüÊ®ôÁ±§Ê©üÂà∂È°ØËëóÊèêÈ´ò‰∫ÜÊèê‰æõÁµ¶ÂàÜÈ°ûÂô®ÁöÑË≥áË®äÁöÑÂìÅË≥™ÂíåÊúâÁî®ÊÄß„ÄÇKR ÁÆ°ÈÅìÂ∑≤Á∂ìÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶ÔºåÈáçÈªûÊòØÂÖ≠ÂÄãÈ´òÂ∫¶Áï∞Ë≥™ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÔºåÁØÑÂúçÂæûË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂà∞Âô®ÂÆòÊéÉÊèè„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ÁúüÂØ¶Ë≥áÊñôÂíåÂêàÊàêË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆË∑ùÈ°ØËëóÁ∏ÆÂ∞èÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÂü∫ÊñºÂêàÊàêË≥áÊñôÁöÑÊ®°ÂûãÂÑ™ÊñºÂú®ÁúüÂØ¶Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÁî¢ÁîüÁöÑÊ®°ÂûãÈ°ØÁ§∫Âá∫Â∞çÊàêÂì°Êé®Ë´ñÊîªÊìäÂπæ‰πéÂÆåÂÖ®ÂÖçÁñ´ÔºåË°®ÁèæÂá∫ÂÇ≥Áµ±ÊäÄË°ìË®ìÁ∑¥ÁöÑÊ®°Âûã‰∏≠Áº∫Â∞ëÁöÑÈö±ÁßÅÂ±¨ÊÄß„ÄÇ

##### **Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management**
2407.15520v1 by Roberto Morabito, Bivek Pandey, Paulius Daubaris, Yasith R Wanigarathna, Sasu Tarkoma

Digital Twins (DTs) are set to become a key enabling technology in future
wireless networks, with their use in network management increasing
significantly. We developed a DT framework that leverages the heterogeneity of
network access technologies as a resource for enhanced network performance and
management, enabling smart data handling in the physical network. Tested in a
\textit{Campus Area Network} environment, our framework integrates diverse data
sources to provide real-time, holistic insights into network performance and
environmental sensing. We also envision that traditional analytics will evolve
to rely on emerging AI models, such as Generative AI (GenAI), while leveraging
current analytics capabilities. This capacity can simplify analytics processes
through advanced ML models, enabling descriptive, diagnostic, predictive, and
prescriptive analytics in a unified fashion. Finally, we present specific
research opportunities concerning interoperability aspects and envision
aligning advancements in DT technology with evolved AI integration.

ÊëòË¶ÅÔºöÊï∏‰ΩçÈõôËÉûËÉé (DT) Â∞áÊàêÁÇ∫Êú™‰æÜÁÑ°Á∑öÁ∂≤Ë∑Ø‰∏≠ÁöÑÈóúÈçµÊäÄË°ìÔºåÂÖ∂Âú®Á∂≤Ë∑ØÁÆ°ÁêÜ‰∏≠ÁöÑÊáâÁî®Â∞áÂ§ßÂπÖÂ¢ûÂä†„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã DT Êû∂ÊßãÔºåÂà©Áî®Á∂≤Ë∑ØÂ≠òÂèñÊäÄË°ìÁöÑÁï∞Ë≥™ÊÄß‰ΩúÁÇ∫Ë≥áÊ∫êÔºå‰ª•Â¢ûÂº∑Á∂≤Ë∑ØÊïàËÉΩÂíåÁÆ°ÁêÜÔºå‰∏¶Âú®ÂØ¶È´îÁ∂≤Ë∑Ø‰∏≠ÂïüÁî®Êô∫ÊÖßË≥áÊñôËôïÁêÜ„ÄÇÂú®„ÄåÊ†°ÂúíÂçÄÂüüÁ∂≤Ë∑Ø„ÄçÁí∞Â¢É‰∏≠Ê∏¨Ë©¶ÂæåÔºåÊàëÂÄëÁöÑÊû∂ÊßãÊï¥Âêà‰∫Ü‰∏çÂêåÁöÑË≥áÊñô‰æÜÊ∫êÔºå‰ª•Êèê‰æõÁ∂≤Ë∑ØÊïàËÉΩÂíåÁí∞Â¢ÉÊÑüÊ∏¨ÁöÑÂç≥ÊôÇ„ÄÅÂÖ®Èù¢ÁöÑË¶ãËß£„ÄÇÊàëÂÄë‰πüÈ†êÊúüÂÇ≥Áµ±ÂàÜÊûêÂ∞áÊºîËÆäÁÇ∫‰æùË≥¥Êñ∞ËààÁöÑ AI Ê®°ÂûãÔºå‰æãÂ¶ÇÁîüÊàêÂºè AI (GenAI)ÔºåÂêåÊôÇÂà©Áî®ÁõÆÂâçÁöÑÂàÜÊûêÂäüËÉΩ„ÄÇÊ≠§ÂäüËÉΩÂèØÈÄèÈÅéÈÄ≤Èöé ML Ê®°ÂûãÁ∞°ÂåñÂàÜÊûêÊµÅÁ®ãÔºå‰ª•Áµ±‰∏ÄÁöÑÊñπÂºèÂïüÁî®ÊèèËø∞ÊÄß„ÄÅË®∫Êñ∑ÊÄß„ÄÅÈ†êÊ∏¨ÊÄßÂíåË¶èÁØÑÊÄßÂàÜÊûê„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫ÈóúÊñº‰∫íÊìç‰ΩúÊÄßÊñπÈù¢ÁöÑÂÖ∑È´îÁ†îÁ©∂Ê©üÊúÉÔºå‰∏¶È†êÊúüÂ∞á DT ÊäÄË°ìÁöÑÈÄ≤Â±ïËàáÈÄ≤ÂåñÁöÑ AI Êï¥ÂêàÁõ∏ÁµêÂêà„ÄÇ

##### **Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models**
2407.15516v1 by Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini

The inference demand for LLMs has skyrocketed in recent months, and serving
models with low latencies remains challenging due to the quadratic input length
complexity of the attention layers. In this work, we investigate the effect of
dropping MLP and attention layers at inference time on the performance of
Llama-v2 models. We find that dropping dreeper attention layers only marginally
decreases performance but leads to the best speedups alongside dropping entire
layers. For example, removing 33\% of attention layers in a 13B Llama2 model
results in a 1.8\% drop in average performance over the OpenLLM benchmark. We
also observe that skipping layers except the latter layers reduces performances
for more layers skipped, except for skipping the attention layers.

ÊëòË¶ÅÔºöËøëÂá†‰∏™ÊúàÊù•ÔºåÂØπ LLM ÁöÑÊé®ÁêÜÈúÄÊ±ÇÊøÄÂ¢ûÔºåÁî±‰∫éÊ≥®ÊÑèÂäõÂ±ÇÁöÑ‰∫åÊ¨°ËæìÂÖ•ÈïøÂ∫¶Â§çÊùÇÂ∫¶Ôºå‰ª•‰ΩéÂª∂ËøüÊèê‰æõÊúçÂä°Ê®°Âûã‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÂú®Êé®ÁêÜÊó∂‰∏¢ÂºÉ MLP ÂíåÊ≥®ÊÑèÂäõÂ±ÇÂØπ Llama-v2 Ê®°ÂûãÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂèëÁé∞Ôºå‰∏¢ÂºÉÊõ¥Ê∑±ÁöÑÊ≥®ÊÑèÂäõÂ±ÇÂè™‰ºöËΩªÂæÆÈôç‰ΩéÊÄßËÉΩÔºå‰ΩÜ‰ºöÂØºËá¥ÊúÄ‰Ω≥Âä†ÈÄüÔºåÂêåÊó∂‰∏¢ÂºÉÊï¥‰∏™Â±Ç„ÄÇ‰æãÂ¶ÇÔºåÂú® 13B Llama2 Ê®°Âûã‰∏≠ÁßªÈô§ 33% ÁöÑÊ≥®ÊÑèÂäõÂ±Ç‰ºöÂØºËá¥ OpenLLM Âü∫ÂáÜ‰∏äÁöÑÂπ≥ÂùáÊÄßËÉΩ‰∏ãÈôç 1.8%„ÄÇÊàë‰ª¨ËøòËßÇÂØüÂà∞ÔºåÈô§‰∫ÜÂêé‰∏ÄÂ±Ç‰πãÂ§ñÔºåË∑≥ËøáÂ±Ç‰ºöÈôç‰ΩéÊÄßËÉΩ‰ª•Ë∑≥ËøáÊõ¥Â§öÂ±ÇÔºåÈô§‰∫ÜË∑≥ËøáÊ≥®ÊÑèÂäõÂ±Ç„ÄÇ

##### **Increasing the Robustness of Model Predictions to Missing Sensors in Earth Observation**
2407.15512v1 by Francisco Mena, Diego Arenas, Andreas Dengel

Multi-sensor ML models for EO aim to enhance prediction accuracy by
integrating data from various sources. However, the presence of missing data
poses a significant challenge, particularly in non-persistent sensors that can
be affected by external factors. Existing literature has explored strategies
like temporal dropout and sensor-invariant models to address the generalization
to missing data issues. Inspired by these works, we study two novel methods
tailored for multi-sensor scenarios, namely Input Sensor Dropout (ISensD) and
Ensemble Sensor Invariant (ESensI). Through experimentation on three
multi-sensor temporal EO datasets, we demonstrate that these methods
effectively increase the robustness of model predictions to missing sensors.
Particularly, we focus on how the predictive performance of models drops when
sensors are missing at different levels. We observe that ensemble multi-sensor
models are the most robust to the lack of sensors. In addition, the sensor
dropout component in ISensD shows promising robustness results.

ÊëòË¶ÅÔºöÂ§öÂÇ≥ÊÑüÂô® ML Ê®°ÂûãÁî®Êñº EOÔºåÊó®Âú®ÈÄöÈÅéÊï¥Âêà‰æÜËá™‰∏çÂêå‰æÜÊ∫êÁöÑÊï∏Êìö‰æÜÊèêÈ´òÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ËÄåÔºåÁº∫Â§±Êï∏ÊìöÁöÑÂ≠òÂú®ÊßãÊàê‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÂèØËÉΩÊúÉÂèóÂà∞Â§ñÈÉ®Âõ†Á¥†ÂΩ±ÈüøÁöÑÈùûÊåÅ‰πÖÊÄßÂÇ≥ÊÑüÂô®‰∏≠„ÄÇÁèæÊúâÊñáÁçªÊé¢Á¥¢‰∫ÜË´∏Â¶ÇÊôÇÈñì‰∏≠Êñ∑ÂíåÂÇ≥ÊÑüÂô®‰∏çËÆäÊ®°Âûã‰πãÈ°ûÁöÑÁ≠ñÁï•Ôºå‰ª•Ëß£Ê±∫Êé®Âª£Âà∞Áº∫Â§±Êï∏ÊìöÂïèÈ°å„ÄÇÂèóÈÄô‰∫õÂ∑•‰ΩúÁöÑÂïüÁôºÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂÖ©Á®ÆÈáùÂ∞çÂ§öÂÇ≥ÊÑüÂô®Â†¥ÊôØÈáèË∫´ÂÆöÂà∂ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂç≥Ëº∏ÂÖ•ÂÇ≥ÊÑüÂô®‰∏≠Êñ∑ (ISensD) ÂíåÈõÜÊàêÂÇ≥ÊÑüÂô®‰∏çËÆä (ESensI)„ÄÇÈÄöÈÅéÂ∞ç‰∏âÂÄãÂ§öÂÇ≥ÊÑüÂô®ÊôÇÈñì EO Êï∏ÊìöÈõÜÈÄ≤Ë°åÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰∫ÜÈÄô‰∫õÊñπÊ≥ïÊúâÊïàÂú∞ÊèêÈ´ò‰∫ÜÊ®°ÂûãÈ†êÊ∏¨Â∞çÁº∫Â§±ÂÇ≥ÊÑüÂô®ÁöÑÈ≠ØÊ£íÊÄß„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÈáçÈªûÈóúÊ≥®Âú®‰∏çÂêåÁ¥öÂà•Áº∫Â§±ÂÇ≥ÊÑüÂô®ÊôÇÊ®°ÂûãÁöÑÈ†êÊ∏¨ÊÄßËÉΩÂ¶Ç‰Ωï‰∏ãÈôç„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÔºåÈõÜÊàêÂ§öÂÇ≥ÊÑüÂô®Ê®°ÂûãÂ∞çÂÇ≥ÊÑüÂô®Áº∫Â§±ÊúÄÁÇ∫È≠ØÊ£í„ÄÇÊ≠§Â§ñÔºåISensD ‰∏≠ÁöÑÂÇ≥ÊÑüÂô®‰∏≠Êñ∑ÁµÑ‰ª∂È°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÈ≠ØÊ£íÊÄßÁµêÊûú„ÄÇ

##### **Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners**
2407.15508v1 by Yifei Gao, Jie Ou, Lei Wang, Fanhua Shang, Jaji Wu, Jun Cheng

Large Language Models (LLMs) showcase remarkable performance and robust
deductive capabilities, yet their expansive size complicates deployment and
raises environmental concerns due to substantial resource consumption. The
recent development of a quantization technique known as Learnable
Singular-value Increment (LSI) has addressed some of these quantization
challenges. Leveraging insights from LSI and our extensive research, we have
developed innovative methods that enhance the performance of quantized LLMs,
particularly in low-bit settings. Our methods consistently deliver
state-of-the-art results across various quantization scenarios and offer deep
theoretical insights into the quantization process, elucidating the potential
of quantized models for widespread application.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â±ïÁ§∫Âá∫ÂçìË∂äÁöÑÊïàËÉΩÂíåÂº∑Â§ßÁöÑÊºîÁππËÉΩÂäõÔºå‰ΩÜÂÖ∂ÈæêÂ§ßÁöÑË¶èÊ®°‰ΩøÈÉ®ÁΩ≤ËÆäÂæóË§áÈõúÔºå‰∏¶Âõ†Â§ßÈáèÁöÑË≥áÊ∫êÊ∂àËÄóËÄåÂºïÁôºÁí∞Â¢ÉÂïèÈ°å„ÄÇÊúÄËøëÈñãÁôºÁöÑ‰∏ÄÁ®ÆÁ®±ÁÇ∫ÂèØÂ≠∏ÁøíÂ•áÁï∞ÂÄºÂ¢ûÈáè (LSI) ÁöÑÈáèÂåñÊäÄË°ìÂ∑≤Ëß£Ê±∫‰∫ÜÂÖ∂‰∏≠‰∏Ä‰∫õÈáèÂåñÊåëÊà∞„ÄÇÂà©Áî® LSI ÁöÑË¶ãËß£ÂíåÊàëÂÄëÂª£Ê≥õÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÈñãÁôº‰∫ÜÂâµÊñ∞ÁöÑÊñπÊ≥ï‰æÜÂ¢ûÂº∑ÈáèÂåñ LLM ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®‰Ωé‰ΩçÂÖÉË®≠ÂÆö‰∏≠„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂú®ÂêÑÁ®ÆÈáèÂåñÂ†¥ÊôØ‰∏≠ÊåÅÁ∫åÊèê‰æõÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûúÔºå‰∏¶Â∞çÈáèÂåñÈÅéÁ®ãÊèê‰æõÊ∑±ÂÖ•ÁöÑÁêÜË´ñË¶ãËß£ÔºåÈó°Êòé‰∫ÜÈáèÂåñÊ®°ÂûãÂª£Ê≥õÊáâÁî®ÁöÑÊΩõÂäõ„ÄÇ

##### **Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models**
2407.15504v1 by Adway Girish, Alliot Nagle, Marco Bondaschi, Michael Gastpar, Ashok Vardhan Makkuva, Hyeji Kim

We formalize the problem of prompt compression for large language models
(LLMs) and present a framework to unify token-level prompt compression methods
which create hard prompts for black-box models. We derive the distortion-rate
function for this setup as a linear program, and provide an efficient algorithm
to compute this fundamental limit via the dual of the linear program. Using the
distortion-rate function as the baseline, we study the performance of existing
compression schemes on a synthetic dataset consisting of prompts generated from
a Markov chain, natural language queries, and their respective answers. Our
empirical analysis demonstrates the criticality of query-aware prompt
compression, where the compressor has knowledge of the downstream task/query
for the black-box LLM. We show that there is a large gap between the
performance of current prompt compression methods and the optimal strategy, and
propose a query-aware, variable-rate adaptation of a prior work to close the
gap. We extend our experiments to a small natural language dataset to further
confirm our findings on our synthetic dataset.

ÊëòË¶ÅÔºöÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊèêÁ§∫Â£ìÁ∏ÆÂïèÈ°åÂΩ¢ÂºèÂåñÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ°ÜÊû∂‰æÜÁµ±‰∏ÄÁî®ÊñºÁÇ∫ÈªëÁõíÊ®°ÂûãÂª∫Á´ãÁ°¨ÊèêÁ§∫ÁöÑ‰ª£Âπ£Á¥öÊèêÁ§∫Â£ìÁ∏ÆÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞áÊ≠§Ë®≠ÂÆöÁöÑÂ§±ÁúüÁéáÂáΩÊï∏Â∞éÂá∫ÁÇ∫‰∏ÄÂÄãÁ∑öÊÄßË¶èÂäÉÔºå‰∏¶Êèê‰æõ‰∏ÄÂÄãÊúâÊïàÁéáÁöÑÊºîÁÆóÊ≥ï‰æÜÈÄèÈÅéÁ∑öÊÄßË¶èÂäÉÁöÑÂ∞çÂÅ∂‰æÜË®àÁÆóÈÄôÂÄãÂü∫Êú¨ÈôêÂà∂„ÄÇ‰ΩøÁî®Â§±ÁúüÁéáÂáΩÊï∏‰ΩúÁÇ∫Âü∫Ê∫ñÔºåÊàëÂÄëÁ†îÁ©∂ÁèæÊúâÂ£ìÁ∏ÆÊñπÊ°àÂú®‰∏ÄÂÄãÁî±È¶¨ÂèØÂ§´Èèà„ÄÅËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÂèäÂÖ∂ÂêÑËá™Á≠îÊ°àÊâÄÁî¢ÁîüÁöÑÊèêÁ§∫ÁµÑÊàêÁöÑÂêàÊàêË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÂàÜÊûêË≠âÊòé‰∫ÜÊü•Ë©¢ÊÑüÁü•ÊèêÁ§∫Â£ìÁ∏ÆÁöÑÈáçË¶ÅÊÄßÔºåÂÖ∂‰∏≠Â£ìÁ∏ÆÂô®ÂÖ∑ÊúâÂ∞çÈªëÁõí LLM ÁöÑ‰∏ãÊ∏∏‰ªªÂãô/Êü•Ë©¢ÁöÑÁü•Ë≠ò„ÄÇÊàëÂÄëË°®ÊòéÔºåÁõÆÂâçÁöÑÊèêÁ§∫Â£ìÁ∏ÆÊñπÊ≥ïÁöÑÊïàËÉΩËàáÊúÄ‰Ω≥Á≠ñÁï•‰πãÈñìÂ≠òÂú®ÂæàÂ§ßÁöÑÂ∑ÆË∑ùÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊü•Ë©¢ÊÑüÁü•„ÄÅÂèØËÆäÈÄüÁéáÁöÑÂÖàÂâçÂ∑•‰ΩúÊîπÁ∑®Ôºå‰ª•Á∏ÆÂ∞èÂ∑ÆË∑ù„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÂØ¶È©óÊì¥Â±ïÂà∞‰∏ÄÂÄãÂ∞èÂûãËá™ÁÑ∂Ë™ûË®ÄË≥áÊñôÈõÜÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Á¢∫Ë™çÊàëÂÄëÂú®ÂêàÊàêË≥áÊñôÈõÜ‰∏äÁöÑÁôºÁèæ„ÄÇ

##### **Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction**
2407.15498v1 by Dingyao Yu, Yang An, Wei Ye, Xiongfeng Xiao, Shaoguang Mao, Tao Ge, Shikun Zhang

Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality
corpora, due to the labor-intensive labeling of spelling errors in real-life
human writing or typing scenarios. Two data augmentation methods are widely
adopted: (1) \textit{Random Replacement} with the guidance of confusion sets
and (2) \textit{OCR/ASR-based Generation} that simulates character misusing.
However, both methods inevitably introduce noisy data (e.g., false spelling
errors), potentially leading to over-correction. By carefully analyzing the two
types of corpora, we find that though the latter achieves more robust
generalization performance, the former yields better-calibrated CSC models. We
then provide a theoretical analysis of this empirical observation, based on
which a corpus refining strategy is proposed. Specifically, OCR/ASR-based data
samples are fed into a well-calibrated CSC model trained on random
replacement-based corpora and then filtered based on prediction confidence. By
learning a simple BERT-based model on the refined OCR/ASR-based corpus, we set
up impressive state-of-the-art performance on three widely-used benchmarks,
while significantly alleviating over-correction (e.g., lowering false positive
predictions).

ÊëòË¶ÅÔºö‰∏≠ÊñáÊãºÂØ´Ê†°Ê≠£ (CSC) ÈÄöÂ∏∏Áº∫‰πèÂ§ßÈáèÈ´òÂìÅË≥™ÁöÑË™ûÊñôÂ∫´ÔºåÈÄôÊòØÂõ†ÁÇ∫Âú®ÁèæÂØ¶ÁîüÊ¥ª‰∏≠ÁöÑ‰∫∫È°ûÊõ∏ÂØ´ÊàñÊâìÂ≠óÂ†¥ÊôØ‰∏≠ÔºåÊãºÂØ´ÈåØË™§ÁöÑÊ®ôË®ªÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Âäõ„ÄÇÂª£Ê≥õÊé°Áî®ÁöÑÂÖ©Á®ÆË≥áÊñôÊì¥ÂÖÖÊñπÊ≥ïÔºö(1) Âú®Ê∑∑Ê∑ÜÈõÜÂêàÁöÑÊåáÂ∞é‰∏ãÈÄ≤Ë°åÈö®Ê©üÊõøÊèõÔºå‰ª•Âèä (2) Ê®°Êì¨Â≠óÂÖÉË™§Áî®ÁöÑ OCR/ASR Âü∫ÊñºÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ï‰∏çÂèØÈÅøÂÖçÂú∞ÊúÉÂºïÂÖ•ÈõúË®äË≥áÊñôÔºà‰æãÂ¶ÇÔºåÊãºÂØ´ÈåØË™§ÔºâÔºåÂèØËÉΩÂ∞éËá¥ÈÅéÂ∫¶Ê†°Ê≠£„ÄÇÈÄèÈÅé‰ªîÁ¥∞ÂàÜÊûêÈÄôÂÖ©Á®ÆË™ûÊñôÂ∫´ÔºåÊàëÂÄëÁôºÁèæÈõñÁÑ∂ÂæåËÄÖÈÅîÂà∞‰∫ÜÊõ¥Âº∑Â§ßÁöÑÊ≥õÂåñÊïàËÉΩÔºå‰ΩÜÂâçËÄÖÁî¢ÁîüÁöÑ CSC Ê®°ÂûãÊ†°Ê∫ñÊïàÊûúËºÉ‰Ω≥„ÄÇÊé•ËëóÔºåÊàëÂÄëÈáùÂ∞çÊ≠§Á∂ìÈ©óËßÄÂØüÊèê‰æõÁêÜË´ñÂàÜÊûêÔºå‰∏¶ÊìöÊ≠§ÊèêÂá∫Ë™ûÊñôÂ∫´Á≤æÁÖâÁ≠ñÁï•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂ∞áÂü∫Êñº OCR/ASR ÁöÑË≥áÊñôÁØÑ‰æãËº∏ÂÖ•Âà∞Âü∫ÊñºÈö®Ê©üÊõøÊèõË™ûÊñôÂ∫´Ë®ìÁ∑¥ÁöÑËâØÂ•ΩÊ†°Ê∫ñ CSC Ê®°Âûã‰∏≠ÔºåÁÑ∂ÂæåÊ†πÊìöÈ†êÊ∏¨‰ø°ÂøÉÈÄ≤Ë°åÁØ©ÈÅ∏„ÄÇÈÄèÈÅéÂú®Á≤æÁÖâÁöÑÂü∫Êñº OCR/ASR ÁöÑË™ûÊñôÂ∫´‰∏äÂ≠∏Áøí‰∏ÄÂÄãÁ∞°ÂñÆÁöÑ BERT Ê®°ÂûãÔºåÊàëÂÄëÂú®‰∏âÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñ‰∏äÂª∫Á´ã‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊúÄÊñ∞ÊäÄË°ìÊïàËÉΩÔºåÂêåÊôÇÂ§ßÂπÖÊ∏õËºï‰∫ÜÈÅéÂ∫¶Ê†°Ê≠£Ôºà‰æãÂ¶ÇÔºåÈôç‰ΩéË™§Âà§Ôºâ„ÄÇ

##### **Two Stacks Are Better Than One: A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives**
2407.15489v1 by Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, J√∂rg Tiedemann

Pretrained language models (PLMs) display impressive performances and have
captured the attention of the NLP community. Establishing the best practices in
pretraining has therefore become a major point of focus for much of NLP
research -- especially since the insights developed for monolingual English
models need not carry to more complex multilingual. One significant caveat of
the current state of the art is that different works are rarely comparable:
they often discuss different parameter counts, training data, and evaluation
methodology.
  This paper proposes a comparison of multilingual pretraining objectives in a
controlled methodological environment. We ensure that training data and model
architectures are comparable, and discuss the downstream performances across 6
languages that we observe in probing and fine-tuning scenarios. We make two key
observations: (1) the architecture dictates which pretraining objective is
optimal; (2) multilingual translation is a very effective pre-training
objective under the right conditions. We make our code, data, and model weights
available at \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºàPLMÔºâÂ±ïÁèæ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑË°®ÁèæÔºå‰∏¶Âê∏Âºï‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ§æÁæ§ÁöÑÊ≥®ÊÑè„ÄÇÂõ†Ê≠§ÔºåÂª∫Á´ãÈ†êË®ìÁ∑¥ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÂ∑≤ÊàêÁÇ∫Ë®±Â§öËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ†îÁ©∂ÁöÑ‰∏ªË¶ÅÈáçÈªûÔºåÁâπÂà•ÊòØÈáùÂ∞çÂñÆË™ûËã±Ë™ûÊ®°ÂûãÊâÄÁôºÂ±ïÁöÑË¶ãËß£‰∏çÂøÖÂ•óÁî®Âú®Êõ¥Ë§áÈõúÁöÑÂ§öË™ûÁ®Æ‰∏ä„ÄÇÁõÆÂâçÊäÄË°ìÁöÑ‰∏ÄÂÄãÈáçÂ§ßË≠¶Á§∫ÊòØÔºå‰∏çÂêåÁöÑ‰ΩúÂìÅÂæàÂ∞ëÂÖ∑ÊúâÂèØÊØîÊÄßÔºöÂÆÉÂÄëÁ∂ìÂ∏∏Ë®éË´ñ‰∏çÂêåÁöÑÂèÉÊï∏Êï∏Èáè„ÄÅË®ìÁ∑¥Ë≥áÊñôÂíåË©ï‰º∞ÊñπÊ≥ï„ÄÇ
Êú¨ÊñáÊèêÂá∫Âú®ÂèóÊéßÁöÑÊñπÊ≥ïÁí∞Â¢É‰∏≠ÊØîËºÉÂ§öË™ûÁ®ÆÈ†êË®ìÁ∑¥ÁõÆÊ®ô„ÄÇÊàëÂÄëÁ¢∫‰øùË®ìÁ∑¥Ë≥áÊñôÂíåÊ®°ÂûãÊû∂ÊßãÂÖ∑ÊúâÂèØÊØîÊÄßÔºå‰∏¶Ë®éË´ñÊàëÂÄëÂú®Êé¢Ê∏¨ÂíåÂæÆË™øÂ†¥ÊôØ‰∏≠ËßÄÂØüÂà∞ÁöÑ 6 Á®ÆË™ûË®ÄÁöÑ‰∏ãÊ∏∏Ë°®Áèæ„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ©ÂÄãÈóúÈçµËßÄÂØüÔºö(1) Êû∂ÊßãÊ±∫ÂÆöÂì™ÂÄãÈ†êË®ìÁ∑¥ÁõÆÊ®ôÊúÄ‰Ω≥Ôºõ(2) Âú®Ê≠£Á¢∫ÁöÑÊ¢ù‰ª∂‰∏ãÔºåÂ§öË™ûÁ®ÆÁøªË≠ØÊòØ‰∏ÄÁ®ÆÈùûÂ∏∏ÊúâÊïàÁöÑÈ†êË®ìÁ∑¥ÁõÆÊ®ô„ÄÇÊàëÂÄëÂú® \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}} Êèê‰æõÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÂíåÊ®°ÂûãÊ¨äÈáç„ÄÇ

##### **In-Context Learning Improves Compositional Understanding of Vision-Language Models**
2407.15487v1 by Matteo Nulli, Anesa Ibrahimi, Avik Pal, Hoshe Lee, Ivona Najdenkoska

Vision-Language Models (VLMs) have shown remarkable capabilities in a large
number of downstream tasks. Nonetheless, compositional image understanding
remains a rather difficult task due to the object bias present in training
data. In this work, we investigate the reasons for such a lack of capability by
performing an extensive bench-marking of compositional understanding in VLMs.
We compare contrastive models with generative ones and analyze their
differences in architecture, pre-training data, and training tasks and losses.
Furthermore, we leverage In-Context Learning (ICL) as a way to improve the
ability of VLMs to perform more complex reasoning and understanding given an
image. Our extensive experiments demonstrate that our proposed approach
outperforms baseline models across multiple compositional understanding
datasets.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Â∑≤Âú®Â§ßÈáè‰∏ãÊ∏∏‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñô‰∏≠Â≠òÂú®ÁöÑÁâ©‰ª∂ÂÅèÂ∑ÆÔºåÁµÑÂêàÂºèÂΩ±ÂÉèÁêÜËß£‰ªçÊòØ‰∏ÄÈ†ÖÁõ∏Áï∂Âõ∞Èõ£ÁöÑ‰ªªÂãô„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂ∞ç VLM ‰∏≠ÁµÑÂêàÂºèÁêÜËß£ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂü∫Ê∫ñË©ïÈáèÔºåÊé¢Ë®éÈÄôÁ®ÆËÉΩÂäõ‰∏çË∂≥ÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÊØîËºÉÂ∞çÊØîÊ®°ÂûãËàáÁîüÊàêÊ®°ÂûãÔºå‰∏¶ÂàÜÊûêÂÆÉÂÄëÂú®Êû∂Êßã„ÄÅÈ†êË®ìÁ∑¥Ë≥áÊñô„ÄÅË®ìÁ∑¥‰ªªÂãôÂíåÊêçÂ§±ÊñπÈù¢ÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®ÊÉÖÂ¢É‰∏≠Â≠∏Áøí (ICL) ‰ΩúÁÇ∫‰∏ÄÁ®ÆÊñπÊ≥ïÔºå‰ª•ÊèêÂçá VLM Âú®Áµ¶ÂÆöÂΩ±ÂÉèÁöÑÊÉÖÊ≥Å‰∏ãÂü∑Ë°åÊõ¥Ë§áÈõúÊé®ÁêÜÂíåÁêÜËß£ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§öÂÄãÁµÑÂêàÂºèÁêÜËß£Ë≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÂü∫Ê∫ñÊ®°Âûã„ÄÇ

##### **A Multi-Level Corroborative Approach for Verification and Validation of Autonomous Robotic Swarms**
2407.15475v1 by Dhaminda B. Abeywickrama, Suet Lee, Chris Bennett, Razanne Abu-Aisheh, Tom Didiot-Cook, Simon Jones, Sabine Hauert, Kerstin Eder

Modelling and characterizing emergent behaviour within a swarm can pose
significant challenges in terms of 'assurance'. Assurance tasks encompass
adherence to standards, certification processes, and the execution of
verification and validation (V&V) methods, such as model checking. In this
study, we propose a holistic, multi-level modelling approach for formally
verifying and validating autonomous robotic swarms, which are defined at the
macroscopic formal modelling, low-fidelity simulation, high-fidelity
simulation, and real-robot levels. Our formal macroscopic models, used for
verification, are characterized by data derived from actual simulations,
ensuring both accuracy and traceability across different system models.
Furthermore, our work combines formal verification with experimental validation
involving real robots. In this way, our corroborative approach for V&V seeks to
enhance confidence in the evidence, in contrast to employing these methods
separately. We explore our approach through a case study focused on a swarm of
robots operating within a public cloakroom.

ÊëòË¶ÅÔºöÊ®°Êì¨ÂíåÊèèËø∞Áæ§È´î‰∏≠Âá∫ÁèæÁöÑË°åÁÇ∫Âú®„Äå‰øùË≠â„ÄçÊñπÈù¢ÂèØËÉΩÂ∏∂‰æÜÈáçÂ§ßÊåëÊà∞„ÄÇ‰øùË≠â‰ªªÂãôÂåÖÊã¨ÈÅµÂÆàÊ®ôÊ∫ñ„ÄÅË™çË≠âÁ®ãÂ∫èÔºå‰ª•ÂèäÂü∑Ë°åÈ©óË≠âÂíåÈ©óË≠â (V&V) ÊñπÊ≥ïÔºå‰æãÂ¶ÇÊ®°ÂûãÊ™¢Êü•„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊï¥È´î„ÄÅÂ§öÂ±§Ê¨°Âª∫Ê®°ÊñπÊ≥ïÔºåÁî®ÊñºÊ≠£ÂºèÈ©óË≠âÂíåÈ©óË≠âËá™‰∏ªÊ©üÂô®‰∫∫Áæ§È´îÔºåÈÄô‰∫õÁæ§È´îÂÆöÁæ©Âú®Â∑®ËßÄÂΩ¢ÂºèÂåñÂª∫Ê®°„ÄÅ‰Ωé‰øùÁúüÊ®°Êì¨„ÄÅÈ´ò‰øùÁúüÊ®°Êì¨ÂíåÁúüÂØ¶Ê©üÂô®‰∫∫Â±§Á¥ö„ÄÇÊàëÂÄëÁî®ÊñºÈ©óË≠âÁöÑÊ≠£ÂºèÂ∑®ËßÄÊ®°ÂûãÁöÑÁâπÈªûÊòØ‰æÜËá™ÂØ¶ÈöõÊ®°Êì¨ÁöÑÊï∏ÊìöÔºåÁ¢∫‰øù‰∫Ü‰∏çÂêåÁ≥ªÁµ±Ê®°Âûã‰πãÈñìÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËøΩÊ∫ØÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Â∞áÂΩ¢ÂºèÂåñÈ©óË≠âËàáÊ∂âÂèäÁúüÂØ¶Ê©üÂô®‰∫∫ÁöÑÂØ¶È©óÈ©óË≠âÁõ∏ÁµêÂêà„ÄÇÈÄöÈÅéÈÄôÁ®ÆÊñπÂºèÔºåÊàëÂÄëÁî®Êñº V&V ÁöÑ‰ΩêË≠âÊñπÊ≥ïÊó®Âú®Â¢ûÂº∑Â∞çË≠âÊìöÁöÑ‰ø°ÂøÉÔºåËÄå‰∏çÊòØÂàÜÂà•Êé°Áî®ÈÄô‰∫õÊñπÊ≥ï„ÄÇÊàëÂÄëÈÄöÈÅé‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Êé¢Ë®é‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåË©≤Ê°à‰æãÁ†îÁ©∂ÈõÜ‰∏≠Âú®‰∏ÄÂÄãÂú®ÂÖ¨ÂÖ±Ë°£Â∏ΩÈñìÂÖßÈÅã‰ΩúÁöÑÊ©üÂô®‰∫∫Áæ§È´î„ÄÇ

##### **Text-to-Battery Recipe: A language modeling-based protocol for automatic battery recipe extraction and retrieval**
2407.15459v1 by Daeun Lee, Jaewoong Choi, Hiroshi Mizuseki, Byungju Lee

Recent studies have increasingly applied natural language processing (NLP) to
automatically extract experimental research data from the extensive battery
materials literature. Despite the complex process involved in battery
manufacturing -- from material synthesis to cell assembly -- there has been no
comprehensive study systematically organizing this information. In response, we
propose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for
the automatic extraction of end-to-end battery recipes, validated using a case
study on batteries containing LiFePO4 cathode material. We report machine
learning-based paper filtering models, screening 2,174 relevant papers from the
keyword-based search results, and unsupervised topic models to identify 2,876
paragraphs related to cathode synthesis and 2,958 paragraphs related to cell
assembly. Then, focusing on the two topics, two deep learning-based named
entity recognition models are developed to extract a total of 30 entities --
including precursors, active materials, and synthesis methods -- achieving F1
scores of 88.18% and 94.61%. The accurate extraction of entities enables the
systematic generation of 165 end-toend recipes of LiFePO4 batteries. Our
protocol and results offer valuable insights into specific trends, such as
associations between precursor materials and synthesis methods, or combinations
between different precursor materials. We anticipate that our findings will
serve as a foundational knowledge base for facilitating battery-recipe
information retrieval. The proposed protocol will significantly accelerate the
review of battery material literature and catalyze innovations in battery
design and development.

ÊëòË¶ÅÔºöËøëÊúüÁöÑÁ†îÁ©∂Â∑≤Êó•ÁõäÊáâÁî®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰æÜËá™ÂãïÂæûÂª£Ê≥õÁöÑÈõªÊ±†ÊùêÊñôÊñáÁçª‰∏≠ÊèêÂèñÂØ¶È©óÁ†îÁ©∂Ë≥áÊñô„ÄÇÂÑòÁÆ°ÈõªÊ±†Ë£ΩÈÄ†Ê∂âÂèäË§áÈõúÁöÑÊµÅÁ®ãÔºåÂæûÊùêÊñôÂêàÊàêÂà∞ÈõªÊ±†ÁµÑË£ùÔºå‰ΩÜÁõÆÂâçÂ∞öÊú™ÊúâÁ≥ªÁµ±ÁµÑÁπîÈÄô‰∫õË≥áË®äÁöÑÂÖ®Èù¢ÊÄßÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºË™ûË®ÄÊ®°ÂûãÁöÑÂçîÂÆöÔºåText-to-Battery Recipe (T2BR)ÔºåÁî®ÊñºËá™ÂãïÊèêÂèñÁ´ØÂ∞çÁ´ØÈõªÊ±†ÈÖçÊñπÔºå‰∏¶‰ΩøÁî®ÂåÖÂê´ LiFePO4 Ê≠£Ê•µÊùêÊñôÁöÑÈõªÊ±†Ê°à‰æãÁ†îÁ©∂ÈÄ≤Ë°åÈ©óË≠â„ÄÇÊàëÂÄëÂõûÂ†±‰∫ÜÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑË´ñÊñáÈÅéÊøæÊ®°ÂûãÔºåÂæûÂü∫ÊñºÈóúÈçµÂ≠óÁöÑÊêúÂ∞ãÁµêÊûú‰∏≠ÁØ©ÈÅ∏Âá∫ 2,174 ÁØáÁõ∏ÈóúË´ñÊñáÔºå‰∏¶‰ΩøÁî®ÁÑ°Áõ£Áù£‰∏ªÈ°åÊ®°Âûã‰æÜÊâæÂá∫ 2,876 ÊÆµËàáÊ≠£Ê•µÂêàÊàêÁõ∏ÈóúÁöÑÊÆµËêΩÂíå 2,958 ÊÆµËàáÈõªÊ±†ÁµÑË£ùÁõ∏ÈóúÁöÑÊÆµËêΩ„ÄÇÊé•ËëóÔºåÈáùÂ∞çÈÄôÂÖ©ÂÄã‰∏ªÈ°åÔºåÈñãÁôºÂÖ©ÂÄãÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂêçË©ûÂØ¶È´îËæ®Ë≠òÊ®°Âûã‰æÜÊèêÂèñÁ∏ΩË®à 30 ÂÄãÂØ¶È´îÔºåÂåÖÊã¨ÂâçÈ©ÖÁâ©„ÄÅÊ¥ªÊÄßÊùêÊñôÂíåÂêàÊàêÊñπÊ≥ïÔºå‰∏¶ÈÅîÊàê 88.18% Âíå 94.61% ÁöÑ F1 ÂàÜÊï∏„ÄÇÂØ¶È´îÁöÑÁ≤æÁ¢∫ÊèêÂèñËÉΩÁ≥ªÁµ±ÊÄßÂú∞Áî¢Áîü 165 ÂÄã LiFePO4 ÈõªÊ±†ÁöÑÁ´ØÂ∞çÁ´ØÈÖçÊñπ„ÄÇÊàëÂÄëÁöÑÂçîÂÆöÂíåÁµêÊûúÊèê‰æõÊúâÂÉπÂÄºÁöÑË¶ãËß£ÔºåÊ∑±ÂÖ•Êé¢Ë®éÁâπÂÆöË∂®Âã¢Ôºå‰æãÂ¶ÇÂâçÈ©ÖÊùêÊñôËàáÂêàÊàêÊñπÊ≥ï‰πãÈñìÁöÑÈóúËÅØÊÄßÔºåÊàñ‰∏çÂêåÂâçÈ©ÖÊùêÊñô‰πãÈñìÁöÑÁµÑÂêà„ÄÇÊàëÂÄëÈ†êÊúüÊàëÂÄëÁöÑÁôºÁèæÂ∞á‰ΩúÁÇ∫‰øÉÈÄ≤ÈõªÊ±†ÈÖçÊñπË≥áË®äÊ™¢Á¥¢ÁöÑÂü∫Êú¨Áü•Ë≠òÂ∫´„ÄÇÊâÄÊèêÂá∫ÁöÑÂçîÂÆöÂ∞áÂ§ßÂπÖÂä†ÈÄüÈõªÊ±†ÊùêÊñôÊñáÁçªÁöÑÂØ©Êü•Ôºå‰∏¶ÂÇ¨ÂåñÈõªÊ±†Ë®≠Ë®àËàáÈñãÁôºÁöÑÂâµÊñ∞„ÄÇ

##### **Developing a Reliable, General-Purpose Hallucination Detection and Mitigation Service: Insights and Lessons Learned**
2407.15441v1 by Song Wang, Xun Wang, Jie Mei, Yujia Xie, Sean Muarray, Zhang Li, Lingfeng Wu, Si-Qing Chen, Wayne Xiong

Hallucination, a phenomenon where large language models (LLMs) produce output
that is factually incorrect or unrelated to the input, is a major challenge for
LLM applications that require accuracy and dependability. In this paper, we
introduce a reliable and high-speed production system aimed at detecting and
rectifying the hallucination issue within LLMs. Our system encompasses named
entity recognition (NER), natural language inference (NLI), span-based
detection (SBD), and an intricate decision tree-based process to reliably
detect a wide range of hallucinations in LLM responses. Furthermore, our team
has crafted a rewriting mechanism that maintains an optimal mix of precision,
response time, and cost-effectiveness. We detail the core elements of our
framework and underscore the paramount challenges tied to response time,
availability, and performance metrics, which are crucial for real-world
deployment of these technologies. Our extensive evaluation, utilizing offline
data and live production traffic, confirms the efficacy of our proposed
framework and service.

ÊëòË¶ÅÔºöÂπªË¶∫Ôºå‰∏ÄÁ®ÆÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî¢ÁîüËº∏Âá∫ÁµêÊûú‰∏çÁ¨¶Âêà‰∫ãÂØ¶ÊàñËàáËº∏ÂÖ•ÁÑ°ÈóúÁöÑÁèæË±°ÔºåÂ∞çÊñºÈúÄË¶ÅÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄßÁöÑ LLM ÊáâÁî®Á®ãÂºè‰æÜË™™ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞á‰ªãÁ¥π‰∏ÄÂÄãÂèØÈù†‰∏îÈ´òÈÄüÁöÑÁîüÁî¢Á≥ªÁµ±ÔºåÊó®Âú®ÂÅµÊ∏¨Âíå‰øÆÊ≠£ LLM ‰∏≠ÁöÑÂπªË¶∫ÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±ÂåÖÂê´ÂëΩÂêçÂØ¶È´îËæ®Ë≠ò (NER)„ÄÅËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI)„ÄÅÂü∫ÊñºÂçÄÈñìÁöÑÂÅµÊ∏¨ (SBD) Âíå‰∏ÄÂÄãË§áÈõúÁöÑÊ±∫Á≠ñÊ®πÁÇ∫Âü∫Á§éÁöÑÊµÅÁ®ãÔºå‰ª•ÂèØÈù†Âú∞ÂÅµÊ∏¨ LLM ÂõûÊáâ‰∏≠ÁöÑÂêÑÁ®ÆÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂúòÈöäË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊîπÂØ´Ê©üÂà∂Ôºå‰ª•Á∂≠ÊåÅÊúÄ‰Ω≥ÁöÑÁ≤æÊ∫ñÂ∫¶„ÄÅÂõûÊáâÊôÇÈñìÂíåÊàêÊú¨ÊïàÁõäÁµÑÂêà„ÄÇÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊ†∏ÂøÉÂÖÉÁ¥†Ôºå‰∏¶Âº∑Ë™øËàáÂõûÊáâÊôÇÈñì„ÄÅÂèØÁî®ÊÄßÂíåÊïàËÉΩÊåáÊ®ôÁõ∏ÈóúÁöÑ‰∏ªË¶ÅÊåëÊà∞ÔºåÈÄô‰∫õÊåáÊ®ôÂ∞çÊñºÈÄô‰∫õÊäÄË°ìÁöÑÂØ¶ÈöõÈÉ®ÁΩ≤Ëá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑË©ï‰º∞ÔºåÂà©Áî®Èõ¢Á∑öË≥áÊñôÂíåÂØ¶ÊôÇÁîüÁî¢ÊµÅÈáèÔºåË≠âÂØ¶‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂíåÊúçÂãôÁöÑÊïàËÉΩ„ÄÇ

##### **Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**
2407.15431v1 by Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang

The text-attributed graph (TAG) is one kind of important real-world
graph-structured data with each node associated with raw texts. For TAGs,
traditional few-shot node classification methods directly conduct training on
the pre-processed node features and do not consider the raw texts. The
performance is highly dependent on the choice of the feature pre-processing
method. In this paper, we propose P2TAG, a framework designed for few-shot node
classification on TAGs with graph pre-training and prompting. P2TAG first
pre-trains the language model (LM) and graph neural network (GNN) on TAGs with
self-supervised loss. To fully utilize the ability of language models, we adapt
the masked language modeling objective for our framework. The pre-trained model
is then used for the few-shot node classification with a mixed prompt method,
which simultaneously considers both text and graph information. We conduct
experiments on six real-world TAGs, including paper citation networks and
product co-purchasing networks. Experimental results demonstrate that our
proposed framework outperforms existing graph few-shot learning methods on
these datasets with +18.98% ~ +35.98% improvements.

ÊëòË¶ÅÔºöÊñáÊú¨Â±ûÊÄßÂõæ (TAG) ÊòØ‰∏ÄÁßçÈáçË¶ÅÁöÑÁúüÂÆû‰∏ñÁïåÂõæÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂÖ∂‰∏≠ÊØè‰∏™ËäÇÁÇπÈÉΩ‰∏éÂéüÂßãÊñáÊú¨Áõ∏ÂÖ≥ËÅî„ÄÇÂØπ‰∫é TAGÔºå‰º†ÁªüÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÊñπÊ≥ïÁõ¥Êé•ÂØπÈ¢ÑÂ§ÑÁêÜÁöÑËäÇÁÇπÁâπÂæÅËøõË°åËÆ≠ÁªÉÔºåËÄå‰∏çËÄÉËôëÂéüÂßãÊñáÊú¨„ÄÇÊÄßËÉΩÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÁâπÂæÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ïÁöÑÈÄâÊã©„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü P2TAGÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ TAG ‰∏äÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªËÆæËÆ°ÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÂõæÈ¢ÑËÆ≠ÁªÉÂíåÊèêÁ§∫„ÄÇP2TAG È¶ñÂÖà‰ΩøÁî®Ëá™ÊàëÁõëÁù£ÊçüÂ§±ÂØπ TAG ‰∏äÁöÑËØ≠Ë®ÄÊ®°Âûã (LM) ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏∫Êàë‰ª¨ÁöÑÊ°ÜÊû∂Ë∞ÉÊï¥‰∫ÜÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÁõÆÊ†á„ÄÇÁÑ∂Âêé‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÔºåÈááÁî®Ê∑∑ÂêàÊèêÁ§∫ÊñπÊ≥ïÔºåÂêåÊó∂ËÄÉËôëÊñáÊú¨ÂíåÂõæ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÂØπÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ TAG ËøõË°å‰∫ÜÂÆûÈ™åÔºåÂåÖÊã¨ËÆ∫ÊñáÂºïÁî®ÁΩëÁªúÂíå‰∫ßÂìÅÂÖ±ÂêåË¥≠‰π∞ÁΩëÁªú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÂ∞ëÊï∞ÈïúÂ§¥Â≠¶‰π†ÊñπÊ≥ïÔºåÊîπËøõ‰∫Ü +18.98% ~ +35.98%„ÄÇ

##### **Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation**
2407.15428v1 by Rashi Sharma, Hiroyuki Okada, Tatsumi Oba, Karthikk Subramanian, Naoto Yanai, Sugiri Pranata

The Industrial Control System (ICS) environment encompasses a wide range of
intricate communication protocols, posing substantial challenges for Security
Operations Center (SOC) analysts tasked with monitoring, interpreting, and
addressing network activities and security incidents. Conventional monitoring
tools and techniques often struggle to provide a clear understanding of the
nature and intent of ICS-specific communications. To enhance comprehension, we
propose a software solution powered by a Large Language Model (LLM). This
solution currently focused on BACnet protocol, processes a packet file data and
extracts context by using a mapping database, and contemporary context
retrieval methods for Retrieval Augmented Generation (RAG). The processed
packet information, combined with the extracted context, serves as input to the
LLM, which generates a concise packet file summary for the user. The software
delivers a clear, coherent, and easily understandable summary of network
activities, enabling SOC analysts to better assess the current state of the
control system.

ÊëòË¶ÅÔºöÂ∑•Ê•≠ÊéßÂà∂Á≥ªÁµ± (ICS) Áí∞Â¢ÉÂåÖÂê´ÁØÑÂúçÂª£Ê≥õÁöÑË§áÈõúÈÄöË®äÂçîÂÆöÔºåÂ∞çË≤†Ë≤¨Áõ£Êéß„ÄÅËß£ËÆÄÂíåËôïÁêÜÁ∂≤Ë∑ØÊ¥ªÂãïËàáÂÆâÂÖ®‰∫ã‰ª∂ÁöÑË≥áÂÆâÁáüÈÅã‰∏≠ÂøÉ (SOC) ÂàÜÊûêÂ∏´‰æÜË™™ÔºåÊßãÊàêÊ•µÂ§ßÁöÑÊåëÊà∞„ÄÇÂÇ≥Áµ±ÁöÑÁõ£ÊéßÂ∑•ÂÖ∑ÂíåÊäÄË°ìÈÄöÂ∏∏Èõ£‰ª•Êèê‰æõ ICS ÁâπÂÆöÈÄöË®äÊÄßË≥™ÂíåÊÑèÂúñÁöÑÊ∏ÖÊô∞ÁêÜËß£„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ÁêÜËß£ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊîØÊè¥ÁöÑËªüÈ´îËß£Ê±∫ÊñπÊ°à„ÄÇÈÄôÂÄãËß£Ê±∫ÊñπÊ°àÁõÆÂâçÂ∞àÊ≥®Êñº BACnet ÂçîÂÆöÔºåËôïÁêÜÂ∞ÅÂåÖÊ™îÊ°àË≥áÊñô‰∏¶‰ΩøÁî®Â∞çÊáâË≥áÊñôÂ∫´ÂíåÁî®ÊñºÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) ÁöÑÁï∂‰ª£ËÑàÁµ°Ê™¢Á¥¢ÊñπÊ≥ï‰æÜËêÉÂèñËÑàÁµ°„ÄÇËôïÁêÜÈÅéÁöÑÂ∞ÅÂåÖË≥áË®äËàáËêÉÂèñÁöÑËÑàÁµ°ÁµêÂêàÔºå‰ΩúÁÇ∫ LLM ÁöÑËº∏ÂÖ•ÔºåÁÇ∫‰ΩøÁî®ËÄÖÁî¢ÁîüÁ∞°ÊΩîÁöÑÂ∞ÅÂåÖÊ™îÊ°àÊëòË¶Å„ÄÇÊ≠§ËªüÈ´îÊèê‰æõÊ∏ÖÊô∞„ÄÅ‰∏ÄËá¥‰∏îÊòìÊñºÁêÜËß£ÁöÑÁ∂≤Ë∑ØÊ¥ªÂãïÊëòË¶ÅÔºåËÆì SOC ÂàÜÊûêÂ∏´ËÉΩÂ§†Êõ¥Â¶•ÂñÑË©ï‰º∞ÊéßÂà∂Á≥ªÁµ±ÁöÑÁï∂ÂâçÁãÄÊÖã„ÄÇ

##### **YOLO-pdd: A Novel Multi-scale PCB Defect Detection Method Using Deep Representations with Sequential Images**
2407.15427v1 by Bowen Liu, Dongjie Chen, Xiao Qi

With the rapid growth of the PCB manufacturing industry, there is an
increasing demand for computer vision inspection to detect defects during
production. Improving the accuracy and generalization of PCB defect detection
models remains a significant challenge. This paper proposes a high-precision,
robust, and real-time end-to-end method for PCB defect detection based on deep
Convolutional Neural Networks (CNN). Traditional methods often suffer from low
accuracy and limited applicability. We propose a novel approach combining
YOLOv5 and multiscale modules for hierarchical residual-like connections. In
PCB defect detection, noise can confuse the background and small targets. The
YOLOv5 model provides a strong foundation with its real-time processing and
accurate object detection capabilities. The multi-scale module extends
traditional approaches by incorporating hierarchical residual-like connections
within a single block, enabling multiscale feature extraction. This
plug-and-play module significantly enhances performance by extracting features
at multiple scales and levels, which are useful for identifying defects of
varying sizes and complexities. Our multi-scale architecture integrates feature
extraction, defect localization, and classification into a unified network.
Experiments on a large-scale PCB dataset demonstrate significant improvements
in precision, recall, and F1-score compared to existing methods. This work
advances computer vision inspection for PCB defect detection, providing a
reliable solution for high-precision, robust, real-time, and domain-adaptive
defect detection in the PCB manufacturing industry.

ÊëòË¶ÅÔºöÈö®Ëëó PCB Ë£ΩÈÄ†Áî¢Ê•≠ÁöÑÂø´ÈÄüÊàêÈï∑ÔºåÂ∞çÊñºÈõªËÖ¶Ë¶ñË¶∫Ê™¢Ê∏¨ÁöÑÈúÄÊ±Ç‰πüËàáÊó•‰ø±Â¢ûÔºå‰ª•Âú®ÁîüÁî¢ÈÅéÁ®ã‰∏≠ÂÅµÊ∏¨Âá∫Áº∫Èô∑„ÄÇÊèêÂçá PCB Áº∫Èô∑ÂÅµÊ∏¨Ê®°ÂûãÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊ≥õÂåñËÉΩÂäõÔºå‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÊ∑±Â∫¶Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÈ´òÁ≤æÂ∫¶„ÄÅÂº∑ÂÅ•‰∏îÂç≥ÊôÇÁ´ØÂ∞çÁ´ØÊñπÊ≥ïÔºåÁî®Êñº PCB Áº∫Èô∑ÂÅµÊ∏¨„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÈÅáÂà∞Ê∫ñÁ¢∫Â∫¶‰ΩéÂíåÈÅ©Áî®ÊÄßÊúâÈôêÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁµêÂêà YOLOv5 ÂíåÂ§öÂ∞∫Â∫¶Ê®°ÁµÑÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÁî®ÊñºÂàÜÂ±§ÊÆòÂ∑ÆÈ°ûÈÄ£Êé•„ÄÇÂú® PCB Áº∫Èô∑ÂÅµÊ∏¨‰∏≠ÔºåÈõúË®äÂèØËÉΩÊúÉÊ∑∑Ê∑ÜËÉåÊôØÂíåÂ∞èÂûãÁõÆÊ®ô„ÄÇYOLOv5 Ê®°Âûã‰ª•ÂÖ∂Âç≥ÊôÇËôïÁêÜÂíåÊ∫ñÁ¢∫ÁöÑÁâ©‰ª∂ÂÅµÊ∏¨ËÉΩÂäõÔºåÊèê‰æõ‰∫ÜÂº∑Â§ßÁöÑÂü∫Á§é„ÄÇÂ§öÂ∞∫Â∫¶Ê®°ÁµÑÈÄèÈÅéÂú®ÂñÆ‰∏ÄÂçÄÂ°ä‰∏≠Âä†ÂÖ•ÂàÜÂ±§ÊÆòÂ∑ÆÈ°ûÈÄ£Êé•Ôºå‰æÜÂª∂‰º∏ÂÇ≥Áµ±ÊñπÊ≥ïÔºåÈÄ≤ËÄåÂØ¶ÁèæÂ§öÂ∞∫Â∫¶ÁâπÂæµËêÉÂèñ„ÄÇÈÄôÂÄãÂç≥ÊèíÂç≥Áî®Ê®°ÁµÑÈÄèÈÅéËêÉÂèñÂ§öÂÄãÂ∞∫Â∫¶ÂíåÂ±§Á¥öÁöÑÁâπÂæµÔºåÂ§ßÂπÖÊèêÂçáÊïàËÉΩÔºåÈÄô‰∫õÁâπÂæµÊúâÂä©ÊñºË≠òÂà•ÂêÑÁ®ÆÂ∞∫ÂØ∏ÂíåË§áÈõúÂ∫¶ÁöÑÁº∫Èô∑„ÄÇÊàëÂÄëÁöÑÂ§öÂ∞∫Â∫¶Êû∂ÊßãÂ∞áÁâπÂæµËêÉÂèñ„ÄÅÁº∫Èô∑ÂÆö‰ΩçÂíåÂàÜÈ°ûÊï¥ÂêàÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÁ∂≤Ë∑Ø‰∏≠„ÄÇÂú®Â§ßË¶èÊ®° PCB Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÔºåË≠âÊòéËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÂú®Á≤æÊ∫ñÂ∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏‰∏äÈÉΩÊúâÈ°ØËëóÁöÑÊèêÂçá„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊé®Âãï‰∫ÜÈõªËÖ¶Ë¶ñË¶∫Ê™¢Êü•Âú® PCB Áº∫Èô∑ÂÅµÊ∏¨‰∏äÁöÑÊáâÁî®ÔºåÁÇ∫ PCB Ë£ΩÈÄ†Áî¢Ê•≠Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÁ≤æÂ∫¶„ÄÅÂº∑ÂÅ•„ÄÅÂç≥ÊôÇ‰∏îÈ†òÂüüÈÅ©ÊáâÊÄßÁöÑÁº∫Èô∑ÂÅµÊ∏¨ÂèØÈù†Ëß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Empirical Capacity Model for Self-Attention Neural Networks**
2407.15425v1 by Aki H√§rm√§, Marcin Pietrasik, Anna Wilbik

Large pretrained self-attention neural networks, or transformers, have been
very successful in various tasks recently. The performance of a model on a
given task depends on its ability to memorize and generalize the training data.
Large transformer models, which may have billions of parameters, in theory have
a huge capacity to memorize content. However, the current algorithms for the
optimization fall short of the theoretical capacity, and the capacity is also
highly dependent on the content. In this paper, we focus on the memory capacity
of these models obtained using common training algorithms and synthetic
training data. Based on the results, we derive an empirical capacity model
(ECM) for a generic transformer. The ECM can be used to design task-specific
transformer models with an optimal number of parameters in cases where the
target memorization capability of the task can be defined.

ÊëòË¶ÅÔºöÂ§ßÂûãÈ†êÂÖàË®ìÁ∑¥ÁöÑËá™ÊàëÊ≥®ÊÑèÂäõÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÊàñTransformerÔºåÊúÄËøëÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏äÈÉΩÈùûÂ∏∏ÊàêÂäü„ÄÇÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂãô‰∏äÁöÑÊïàËÉΩÂèñÊ±∫ÊñºÂÖ∂Ë®òÊÜ∂ÂíåÊ¶ÇÊã¨Ë®ìÁ∑¥Ë≥áÊñôÁöÑËÉΩÂäõ„ÄÇÁêÜË´ñ‰∏äÔºåÂèØËÉΩÊúâÊï∏ÂçÅÂÑÑÂÄãÂèÉÊï∏ÁöÑÂ§ßÂûãTransformerÊ®°ÂûãÂÖ∑ÊúâÂ∑®Â§ßÁöÑË®òÊÜ∂ÂÖßÂÆπÂÆπÈáè„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁî®ÊñºÊúÄ‰Ω≥ÂåñÁöÑÊºîÁÆóÊ≥ïÊú™ÈÅîÂà∞ÁêÜË´ñÂÆπÈáèÔºåËÄå‰∏îÂÆπÈáè‰πüÈ´òÂ∫¶‰æùË≥¥ÊñºÂÖßÂÆπ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº‰ΩøÁî®Â∏∏Ë¶ãË®ìÁ∑¥ÊºîÁÆóÊ≥ïÂíåÂêàÊàêË®ìÁ∑¥Ë≥áÊñôÂèñÂæóÁöÑÈÄô‰∫õÊ®°ÂûãÁöÑË®òÊÜ∂ÂÆπÈáè„ÄÇÊ†πÊìöÁµêÊûúÔºåÊàëÂÄëÁÇ∫ÈÄöÁî®TransformerË°çÁîüÂá∫‰∏ÄÂÄãÁ∂ìÈ©óÂÆπÈáèÊ®°Âûã (ECM)„ÄÇÂú®ÂèØ‰ª•ÂÆöÁæ©‰ªªÂãôÁöÑÁõÆÊ®ôË®òÊÜ∂ËÉΩÂäõÁöÑÊÉÖÊ≥Å‰∏ãÔºåECM ÂèØÁî®ÊñºË®≠Ë®àÂÖ∑ÊúâÊúÄ‰Ω≥ÂèÉÊï∏Êï∏ÈáèÁöÑÁâπÂÆö‰ªªÂãôTransformerÊ®°Âûã„ÄÇ

##### **Integrating IP Broadcasting with Audio Tags: Workflow and Challenges**
2407.15423v2 by Rhys Burchett-Vass, Arshdeep Singh, Gabriel Bibb√≥, Mark D. Plumbley

The broadcasting industry is increasingly adopting IP techniques,
revolutionising both live and pre-recorded content production, from news
gathering to live music events. IP broadcasting allows for the transport of
audio and video signals in an easily configurable way, aligning with modern
networking techniques. This shift towards an IP workflow allows for much
greater flexibility, not only in routing signals but with the integration of
tools using standard web development techniques. One possible tool could
include the use of live audio tagging, which has a number of uses in the
production of content. These include from automated closed captioning to
identifying unwanted sound events within a scene. In this paper, we describe
the process of containerising an audio tagging model into a microservice, a
small segregated code module that can be integrated into a multitude of
different network setups. The goal is to develop a modular, accessible, and
flexible tool capable of seamless deployment into broadcasting workflows of all
sizes, from small productions to large corporations. Challenges surrounding
latency of the selected audio tagging model and its effect on the usefulness of
the end product are discussed.

ÊëòË¶ÅÔºöÂª£Êí≠Áî¢Ê•≠Ê≠£ÈÄêÊº∏Êé°Áî® IP ÊäÄË°ìÔºå
ÂæûÊñ∞ËÅûÊé°ÈõÜÂà∞ÁèæÂ†¥Èü≥Ê®ÇÊ¥ªÂãïÔºåÂæπÂ∫ïÈù©Êñ∞ÁèæÂ†¥ÂíåÈ†êÂÖàÈåÑË£ΩÁöÑÂÖßÂÆπË£Ω‰Ωú„ÄÇIP Âª£Êí≠ÂÖÅË®±‰ª•ËºïÈ¨ÜË®≠ÂÆöÁöÑÊñπÂºèÂÇ≥Ëº∏Èü≥Ë®äÂíåË¶ñË®äË®äËôüÔºåÁ¨¶ÂêàÁèæ‰ª£Á∂≤Ë∑ØÊäÄË°ì„ÄÇÈÄôÁ®ÆËΩâÂêë IP Â∑•‰ΩúÊµÅÁ®ãÁöÑÊîπËÆäÂÖÅË®±Êõ¥Â§ßÁöÑÈùàÊ¥ªÊÄßÔºå‰∏çÂÉÖÂú®Ë∑ØÁî±Ë®äËôü‰∏äÔºåËÄå‰∏îÈÇÑËÉΩÊï¥Âêà‰ΩøÁî®Ê®ôÊ∫ñÁ∂≤Ë∑ØÈñãÁôºÊäÄË°ìÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏ÄÁ®ÆÂèØËÉΩÁöÑÂ∑•ÂÖ∑ÂèØ‰ª•ÂåÖÊã¨‰ΩøÁî®ÁèæÂ†¥Èü≥Ë®äÊ®ôË®òÔºåÈÄôÂú®ÂÖßÂÆπË£Ω‰Ωú‰∏≠ÊúâÂ§öÁ®ÆÁî®ÈÄî„ÄÇÈÄô‰∫õÁî®ÈÄîÂåÖÊã¨ÂæûËá™ÂãïÂåñÈö±ËóèÂºèÂ≠óÂπïÂà∞Ë≠òÂà•Â†¥ÊôØ‰∏≠ÁöÑ‰∏çÂøÖË¶ÅËÅ≤Èü≥‰∫ã‰ª∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂ∞áÈü≥Ë®äÊ®ôË®òÊ®°ÂûãÂÆπÂô®ÂåñÁöÑÈÅéÁ®ãÔºåËÆäÊàê‰∏ÄÂÄãÂæÆÊúçÂãôÔºå‰∏ÄÂÄãÂèØ‰ª•Êï¥ÂêàÂà∞Â§öÁ®Æ‰∏çÂêåÁ∂≤Ë∑ØË®≠ÂÆö‰∏≠ÁöÑÂ∞èÂûãÂàÜÈõ¢Á®ãÂºèÁ¢ºÊ®°ÁµÑ„ÄÇÁõÆÊ®ôÊòØÈñãÁôº‰∏ÄÂÄãÊ®°ÁµÑÂåñ„ÄÅÊòìÊñºÂ≠òÂèñ‰∏îÈùàÊ¥ªÁöÑÂ∑•ÂÖ∑ÔºåËÉΩÂ§†ÁÑ°Á∏´ÈÉ®ÁΩ≤Âà∞ÂêÑÁ®ÆË¶èÊ®°ÁöÑÂª£Êí≠Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÂæûÂ∞èÂûãË£Ω‰ΩúÂà∞Â§ßÂûã‰ºÅÊ•≠„ÄÇË®éË´ñ‰∫ÜËàáÊâÄÈÅ∏Èü≥Ë®äÊ®ôË®òÊ®°ÂûãÁöÑÂª∂ÈÅ≤ÂèäÂÖ∂Â∞çÊúÄÁµÇÁî¢ÂìÅÂØ¶Áî®ÊÄßÁöÑÂΩ±ÈüøÁõ∏ÈóúÁöÑÊåëÊà∞„ÄÇ

##### **Planning behavior in a recurrent neural network that plays Sokoban**
2407.15421v1 by Adri√† Garriga-Alonso, Mohammad Taufeeque, Adam Gleave

To predict how advanced neural networks generalize to novel situations, it is
essential to understand how they reason. Guez et al. (2019, "An investigation
of model-free planning") trained a recurrent neural network (RNN) to play
Sokoban with model-free reinforcement learning. They found that adding extra
computation steps to the start of episodes at test time improves the RNN's
success rate. We further investigate this phenomenon, finding that it rapidly
emerges early on in training and then slowly fades, but only for comparatively
easier levels. The RNN also often takes redundant actions at episode starts,
and these are reduced by adding extra computation steps. Our results suggest
that the RNN learns to take time to think by `pacing', despite the per-step
penalties, indicating that training incentivizes planning capabilities. The
small size (1.29M parameters) and interesting behavior of this model make it an
excellent model organism for mechanistic interpretability.

ÊëòË¶ÅÔºöÁÇ∫‰∫ÜÈ†êÊ∏¨ÂÖàÈÄ≤ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÂ¶Ç‰ΩïÊ¶ÇÂåñÂà∞Êñ∞ÊÉÖÊ≥ÅÔºå‰∫ÜËß£ÂÆÉÂÄëÁöÑÊé®ÁêÜÊñπÂºèËá≥ÈóúÈáçË¶Å„ÄÇGuez Á≠â‰∫∫Ôºà2019 Âπ¥Ôºå„ÄåÁÑ°Ê®°ÂûãË¶èÂäÉÁöÑË™øÊü•„ÄçÔºâË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑ØÔºàRNNÔºâ‰ΩøÁî®ÁÑ°Ê®°ÂûãÂº∑ÂåñÂ≠∏Áøí‰æÜÁé©ÂÄâÂ∫´Áï™„ÄÇ‰ªñÂÄëÁôºÁèæÔºåÂú®Ê∏¨Ë©¶ÊôÇÂú®ÂõûÂêàÈñãÂßãÊôÇÂ¢ûÂä†È°çÂ§ñÁöÑÈÅãÁÆóÊ≠•È©üÊúÉÊèêÈ´ò RNN ÁöÑÊàêÂäüÁéá„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂‰∫ÜÈÄôÁ®ÆÁèæË±°ÔºåÁôºÁèæÂÆÉÂú®Ë®ìÁ∑¥ÂàùÊúüËøÖÈÄüÂá∫ÁèæÔºåÁÑ∂ÂæåÊÖ¢ÊÖ¢Ê∂àÂ§±Ôºå‰ΩÜÂÉÖÈôêÊñºÁõ∏Â∞çÂÆπÊòìÁöÑÈóúÂç°„ÄÇRNN Âú®ÂõûÂêàÈñãÂßãÊôÇ‰πüÁ∂ìÂ∏∏Êé°ÂèñÂÜóÈ§òÂãï‰ΩúÔºåËÄåÈÄô‰∫õÂãï‰ΩúÊúÉÂõ†Â¢ûÂä†È°çÂ§ñÁöÑÈÅãÁÆóÊ≠•È©üËÄåÊ∏õÂ∞ë„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåRNN Â≠∏ÊúÉ‰∫Ü„ÄåË™øÊï¥„ÄçÁØÄÂ•è‰æÜÊÄùËÄÉÔºåÂÑòÁÆ°ÊúâÊØèÊ≠•Êá≤ÁΩ∞ÔºåÈÄôË°®ÊòéË®ìÁ∑¥ÊøÄÂãµ‰∫ÜË¶èÂäÉËÉΩÂäõ„ÄÇÈÄôÂÄãÊ®°ÂûãÁöÑÂ∞èÂ∞∫ÂØ∏Ôºà1.29M ÂèÉÊï∏ÔºâÂíåÊúâË∂£ÁöÑË°åÁÇ∫‰ΩøÂÖ∂ÊàêÁÇ∫Ê©üÂà∂ÂèØËß£ÈáãÊÄßÁöÑÁµï‰Ω≥Ê®°ÂûãÁîüÁâ©„ÄÇ

##### **LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models**
2407.15415v1 by Xi Chen, Songyang Zhang, Qibing Bai, Kai Chen, Satoshi Nakamura

We introduces LLaST, a framework for building high-performance Large Language
model based Speech-to-text Translation systems. We address the limitations of
end-to-end speech translation(E2E ST) models by exploring model architecture
design and optimization techniques tailored for LLMs. Our approach includes
LLM-based speech translation architecture design, ASR-augmented training,
multilingual data augmentation, and dual-LoRA optimization. Our approach
demonstrates superior performance on the CoVoST-2 benchmark and showcases
exceptional scaling capabilities powered by LLMs. We believe this effective
method will serve as a strong baseline for speech translation and provide
insights for future improvements of the LLM-based speech translation framework.
We release the data, code and models in https://github.com/openaudiolab/LLaST.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π LLaSTÔºå‰∏ÄÂÄãÁî®ÊñºÂª∫ÊßãÈ´òÊÄßËÉΩÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÂü∫ÊñºË™ûÈü≥ËΩâÊñáÂ≠óÁøªË≠ØÁ≥ªÁµ±ÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÈÄèÈÅéÊé¢Á¥¢Ê®°ÂûãÊû∂ÊßãË®≠Ë®àÂíåÈáùÂ∞ç LLM ÈáèË∫´ÊâìÈÄ†ÁöÑÊúÄ‰Ω≥ÂåñÊäÄË°ìÔºå‰æÜËß£Ê±∫Á´ØÂ∞çÁ´ØË™ûÈü≥ÁøªË≠Ø (E2E ST) Ê®°ÂûãÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Âü∫Êñº LLM ÁöÑË™ûÈü≥ÁøªË≠ØÊû∂ÊßãË®≠Ë®à„ÄÅASR Â¢ûÂº∑Ë®ìÁ∑¥„ÄÅÂ§öË™ûË®ÄË≥áÊñôÊì¥ÂÖÖÔºå‰ª•ÂèäÈõô LoRA ÊúÄ‰Ω≥Âåñ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® CoVoST-2 Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩÔºå‰∏¶Â±ïÁ§∫‰∫ÜÁî± LLM Êèê‰æõÂãïÂäõÁöÑÈùûÂá°Êì¥ÂÖÖËÉΩÂäõ„ÄÇÊàëÂÄëÁõ∏‰ø°ÈÄôÂÄãÊúâÊïàÁöÑÊñπÊ≥ïÂ∞áÊàêÁÇ∫Ë™ûÈü≥ÁøªË≠ØÁöÑÂº∑Â§ßÂü∫Ê∫ñÔºå‰∏¶ÁÇ∫ LLM Âü∫Á§éË™ûÈü≥ÁøªË≠ØÊ°ÜÊû∂ÁöÑÊú™‰æÜÊîπÈÄ≤Êèê‰æõË¶ãËß£„ÄÇÊàëÂÄëÂú® https://github.com/openaudiolab/LLaST ‰∏≠ÈáãÂá∫Ë≥áÊñô„ÄÅÁ®ãÂºèÁ¢ºÂíåÊ®°Âûã„ÄÇ

##### **Knowledge Mechanisms in Large Language Models: A Survey and Perspective**
2407.15017v1 by Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang

Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial
for advancing towards trustworthy AGI. This paper reviews knowledge mechanism
analysis from a novel taxonomy including knowledge utilization and evolution.
Knowledge utilization delves into the mechanism of memorization, comprehension
and application, and creation. Knowledge evolution focuses on the dynamic
progression of knowledge within individual and group LLMs. Moreover, we discuss
what knowledge LLMs have learned, the reasons for the fragility of parametric
knowledge, and the potential dark knowledge (hypothesis) that will be
challenging to address. We hope this work can help understand knowledge in LLMs
and provide insights for future research.

ÊëòË¶ÅÔºöÁêÜËß£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÁü•Ë≠òÊ©üÂà∂Â∞çÊñºÊé®ÈÄ≤ÂÄºÂæó‰ø°Ë≥¥ÁöÑ AGI Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨ÊñáÂæû‰∏ÄÂÄãÊñ∞ÁöÑÂàÜÈ°ûÊ≥ïÂõûÈ°ß‰∫ÜÁü•Ë≠òÊ©üÂà∂ÁöÑÂàÜÊûêÔºåÂåÖÊã¨Áü•Ë≠òÂà©Áî®ÂíåÊºîÂåñ„ÄÇÁü•Ë≠òÂà©Áî®Ê∑±ÂÖ•Êé¢Ë®é‰∫ÜË®òÊÜ∂„ÄÅÁêÜËß£ÂíåÊáâÁî®‰ª•ÂèäÂâµÈÄ†ÁöÑÊ©üÂà∂„ÄÇÁü•Ë≠òÊºîÂåñÂ∞àÊ≥®ÊñºÂÄãÈ´îÂíåÁæ§È´î LLM ‰∏≠Áü•Ë≠òÁöÑÂãïÊÖãÈÄ≤Á®ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM Â≠∏ÁøíÂà∞ÁöÑÁü•Ë≠ò„ÄÅÂèÉÊï∏ÂåñÁü•Ë≠òËÑÜÂº±ÊÄßÁöÑÂéüÂõ†Ôºå‰ª•ÂèäÂ∞áÊúÉÂ∏∂‰æÜÊåëÊà∞ÁöÑÊΩõÂú®ÈªëÊöóÁü•Ë≠òÔºàÂÅáË®≠Ôºâ„ÄÇÊàëÂÄëÂ∏åÊúõÈÄôÈ†ÖÂ∑•‰ΩúÊúâÂä©ÊñºÁêÜËß£ LLM ‰∏≠ÁöÑÁü•Ë≠òÔºå‰∏¶ÁÇ∫Êú™‰æÜÁöÑÁ†îÁ©∂Êèê‰æõË¶ãËß£„ÄÇ

##### **Tackling Selfish Clients in Federated Learning**
2407.15402v1 by Andrea Augello, Ashish Gupta, Giuseppe Lo Re, Sajal K. Das

Federated Learning (FL) is a distributed machine learning paradigm
facilitating participants to collaboratively train a model without revealing
their local data. However, when FL is deployed into the wild, some intelligent
clients can deliberately deviate from the standard training process to make the
global model inclined toward their local model, thereby prioritizing their
local data distribution. We refer to this novel category of misbehaving clients
as selfish. In this paper, we propose a Robust aggregation strategy for FL
server to mitigate the effect of Selfishness (in short RFL-Self). RFL-Self
incorporates an innovative method to recover (or estimate) the true updates of
selfish clients from the received ones, leveraging robust statistics (median of
norms) of the updates at every round. By including the recovered updates in
aggregation, our strategy offers strong robustness against selfishness. Our
experimental results, obtained on MNIST and CIFAR-10 datasets, demonstrate that
just 2% of clients behaving selfishly can decrease the accuracy by up to 36%,
and RFL-Self can mitigate that effect without degrading the global model
performance.

ÊëòË¶ÅÔºöËÅØÁõüÂºèÂ≠∏Áøí (FL) ÊòØ‰∏ÄÁ®ÆÂàÜÊï£ÂºèÊ©üÂô®Â≠∏ÁøíÁØÑ‰æãÔºå
‰øÉÈÄ≤ÂèÉËàáËÄÖÂú®‰∏çÊè≠Èú≤ÂÖ∂Âú®Âú∞Ë≥áÊñôÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂêà‰ΩúË®ìÁ∑¥Ê®°Âûã„ÄÇ
ÁÑ∂ËÄåÔºåÁï∂ FL ÈÉ®ÁΩ≤Âà∞ÂØ¶ÈöõÁí∞Â¢ÉÊôÇÔºå‰∏Ä‰∫õÊô∫ÊÖßÂûãÁî®Êà∂Á´ØÂèØ‰ª•ÊïÖÊÑèÂÅèÈõ¢Ê®ôÊ∫ñË®ìÁ∑¥Á®ãÂ∫èÔºå
‰ΩøÂÖ®ÁêÉÊ®°ÂûãÂÇæÂêëÊñºÂÖ∂Âú®Âú∞Ê®°ÂûãÔºåÂæûËÄåÂÑ™ÂÖàËÄÉÊÖÆÂÖ∂Âú®Âú∞Ë≥áÊñôÂàÜ‰Ωà„ÄÇ
ÊàëÂÄëÂ∞áÈÄôÈ°ûÊñ∞ÂûãÊÖãÁöÑ‰∏çÁï∂Ë°åÁÇ∫Áî®Êà∂Á´ØÁ®±ÁÇ∫Ëá™ÁßÅ„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã FL ‰º∫ÊúçÂô®ÁöÑÁ©©ÂÅ•ËÅöÂêàÁ≠ñÁï•Ôºå
‰ª•Ê∏õËºïËá™ÁßÅÁöÑÂΩ±ÈüøÔºàÁ∞°Á®± RFL-SelfÔºâ„ÄÇ
RFL-Self ÁµêÂêà‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºå
ÂæûÊé•Êî∂Âà∞ÁöÑÊõ¥Êñ∞‰∏≠ÊÅ¢Âæ©ÔºàÊàñ‰º∞Ë®àÔºâËá™ÁßÅÁî®Êà∂Á´ØÁöÑÁúüÂØ¶Êõ¥Êñ∞Ôºå
Âà©Áî®ÊØèËº™Êõ¥Êñ∞ÁöÑÁ©©ÂÅ•Áµ±Ë®àË≥áÊñôÔºàÁØÑÊï∏ÁöÑ‰∏≠‰ΩçÊï∏Ôºâ„ÄÇ
ÈÄèÈÅéÂ∞áÊÅ¢Âæ©ÁöÑÊõ¥Êñ∞Á¥çÂÖ•ËÅöÂêà‰∏≠ÔºåÊàëÂÄëÁöÑÁ≠ñÁï•Êèê‰æõ‰∫ÜÂ∞çËá™ÁßÅÁöÑÂº∑Â§ßÁ©©ÂÅ•ÊÄß„ÄÇ
ÊàëÂÄëÂú® MNIST Âíå CIFAR-10 Ë≥áÊñôÈõÜ‰∏äÁç≤ÂæóÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºå
Âè™Êúâ 2% ÁöÑÁî®Êà∂Á´ØË°®ÁèæÂá∫Ëá™ÁßÅË°åÁÇ∫ÔºåÂ∞±ÊúÉ‰ΩøÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéÂ§öÈÅî 36%Ôºå
ËÄå RFL-Self ÂèØ‰ª•Ê∏õËºïÈÄôÁ®ÆÂΩ±ÈüøÔºåËÄå‰∏çÊúÉÈôç‰ΩéÂÖ®ÁêÉÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇ

##### **Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models**
2407.15399v1 by Xiao Liu, Liangzhi Li, Tong Xiang, Fuying Ye, Lu Wei, Wangyue Li, Noa Garcia

With the development of large language models (LLMs) like ChatGPT, both their
vast applications and potential vulnerabilities have come to the forefront.
While developers have integrated multiple safety mechanisms to mitigate their
misuse, a risk remains, particularly when models encounter adversarial inputs.
This study unveils an attack mechanism that capitalizes on human conversation
strategies to extract harmful information from LLMs. We delineate three pivotal
strategies: (i) decomposing malicious questions into seemingly innocent
sub-questions; (ii) rewriting overtly malicious questions into more covert,
benign-sounding ones; (iii) enhancing the harmfulness of responses by prompting
models for illustrative examples. Unlike conventional methods that target
explicit malicious responses, our approach delves deeper into the nature of the
information provided in responses. Through our experiments conducted on
GPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy
compared to conventional attack methods. In summary, this work introduces a
novel attack method that outperforms previous approaches, raising an important
question: How to discern whether the ultimate intent in a dialogue is
malicious?

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â¶Ç ChatGPT ÁöÑÁôºÂ±ïÔºåÂÖ∂Âª£Ê≥õÊáâÁî®ÂíåÊΩõÂú®ÊºèÊ¥ûÈÉΩÂ∑≤ÊµÆÂá∫Ê™ØÈù¢„ÄÇÈõñÁÑ∂ÈñãÁôº‰∫∫Âì°Â∑≤Êï¥ÂêàÂ§öÁ®ÆÂÆâÂÖ®Ê©üÂà∂‰æÜÊ∏õËºïÂÖ∂Ë¢´Êø´Áî®ÁöÑÈ¢®Èö™Ôºå‰ΩÜÈ¢®Èö™‰ªçÁÑ∂Â≠òÂú®ÔºåÁâπÂà•ÊòØÁï∂Ê®°ÂûãÈÅáÂà∞Â∞çÊäóÊÄßËº∏ÂÖ•ÊôÇ„ÄÇÊú¨Á†îÁ©∂Êè≠Á§∫‰∫Ü‰∏ÄÁ®ÆÊîªÊìäÊ©üÂà∂ÔºåÂà©Áî®‰∫∫È°ûÂ∞çË©±Á≠ñÁï•Âæû LLM ‰∏≠ÊèêÂèñÊúâÂÆ≥Ë≥áË®ä„ÄÇÊàëÂÄëÊèèÁπ™Âá∫‰∏âÂÄãÈóúÈçµÁ≠ñÁï•Ôºö(i) Â∞áÊÉ°ÊÑèÂïèÈ°åÂàÜËß£ÊàêÁúã‰ººÁÑ°ÂÆ≥ÁöÑÂ≠êÂïèÈ°åÔºõ(ii) Â∞áÊòéÈ°ØÊÉ°ÊÑèÁöÑÂïèÈ°åÊîπÂØ´ÊàêÊõ¥Èö±ËîΩ„ÄÅËÅΩËµ∑‰æÜËâØÊÄßÁöÑÂïèÈ°åÔºõ(iii) ÈÄèÈÅéÊèêÁ§∫Ê®°ÂûãÊèê‰æõË™™ÊòéÊÄßÁØÑ‰æãÔºå‰æÜÊèêÂçáÂõûÊáâÁöÑÂç±ÂÆ≥ÊÄß„ÄÇËàáÈáùÂ∞çÊòéÁ¢∫ÊÉ°ÊÑèÂõûÊáâÁöÑÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊõ¥Ê∑±ÂÖ•Êé¢Ë®éÂõûÊáâ‰∏≠Êèê‰æõÁöÑË≥áË®äÁöÑÊú¨Ë≥™„ÄÇÈÄèÈÅéÊàëÂÄëÂú® GPT-3.5-turbo„ÄÅGPT-4 Âíå Llama2 ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÂäüÊïàÔºåÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÊîªÊìäÊñπÊ≥ï„ÄÇÁ∏Ω‰πãÔºåÈÄôÈ†ÖÂ∑•‰ΩúÂºïÈÄ≤‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊîªÊìäÊñπÊ≥ïÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÂÖàÂâçÁöÑÂÅöÊ≥ïÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈáçË¶ÅÁöÑÂïèÈ°åÔºöÂ¶Ç‰ΩïËæ®Âà•Â∞çË©±‰∏≠ÁöÑÊúÄÁµÇÊÑèÂúñÊòØÂê¶ÊÉ°ÊÑèÔºü

##### **Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation**
2407.15396v1 by Jaehyeong Jeon, Kibum Kim, Kanghoon Yoon, Chanyoung Park

The scene graph generation (SGG) task involves detecting objects within an
image and predicting predicates that represent the relationships between the
objects. However, in SGG benchmark datasets, each subject-object pair is
annotated with a single predicate even though a single predicate may exhibit
diverse semantics (i.e., semantic diversity), existing SGG models are trained
to predict the one and only predicate for each pair. This in turn results in
the SGG models to overlook the semantic diversity that may exist in a
predicate, thus leading to biased predictions. In this paper, we propose a
novel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)
framework that enables unbiased predictions based on the understanding of the
semantic diversity of predicates. Specifically, DPL learns the regions in the
semantic space covered by each predicate to distinguish among the various
different semantics that a single predicate can represent. Extensive
experiments demonstrate that our proposed model-agnostic DPL framework brings
significant performance improvement on existing SGG models, and also
effectively understands the semantic diversity of predicates.

ÊëòË¶ÅÔºöÂ†¥ÊôØÂúñÁîüÊàê (SGG) ‰ªªÂãôÂåÖÂê´Âú®ÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨Áâ©‰ª∂Ôºå‰∏¶È†êÊ∏¨Ë°®Á§∫Áâ©‰ª∂ÈñìÈóú‰øÇÁöÑË¨ÇË©û„ÄÇÁÑ∂ËÄåÔºåÂú® SGG Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏≠ÔºåÊØèÂÄã‰∏ªË©û-ÂèóË©ûÂ∞çÈÉΩÊúÉË®ªËß£‰∏ÄÂÄãË¨ÇË©ûÔºåÂç≥‰ΩøÂñÆ‰∏ÄË¨ÇË©ûÂèØËÉΩÂ±ïÁèæÂá∫Â§öÊ®£Ë™ûÊÑèÔºàÂç≥Ë™ûÊÑèÂ§öÊ®£ÊÄßÔºâÔºåÁèæÊúâÁöÑ SGG Ê®°ÂûãÈÉΩË®ìÁ∑¥ÁÇ∫È†êÊ∏¨ÊØèÂÄãÂ∞çÊáâÁöÑÂîØ‰∏ÄË¨ÇË©û„ÄÇÈÄôÂèçÈÅé‰æÜÂ∞éËá¥ SGG Ê®°ÂûãÂøΩÁï•Ë¨ÇË©û‰∏≠ÂèØËÉΩÂ≠òÂú®ÁöÑË™ûÊÑèÂ§öÊ®£ÊÄßÔºåÈÄ≤ËÄåÂ∞éËá¥ÊúâÂÅèÂ∑ÆÁöÑÈ†êÊ∏¨„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑËàáÊ®°ÂûãÁÑ°ÈóúÁöÑË™ûÊÑèÂ§öÊ®£ÊÄßÊÑüÁü•ÂéüÂûãÂü∫Á§éÂ≠∏Áøí (DPL) Êû∂ÊßãÔºåË©≤Êû∂ÊßãÊ†πÊìöÂ∞çË¨ÇË©ûË™ûÊÑèÂ§öÊ®£ÊÄßÁöÑÁêÜËß£ÔºåÂïüÁî®ÁÑ°ÂÅèÂ∑ÆÈ†êÊ∏¨„ÄÇÂÖ∑È´î‰æÜË™™ÔºåDPL ÊúÉÂ≠∏ÁøíÊØèÂÄãË¨ÇË©ûÊ∂µËìãÁöÑË™ûÊÑèÁ©∫ÈñìÂçÄÂüüÔºå‰ª•ÂçÄÂàÜÂñÆ‰∏ÄË¨ÇË©ûÂèØ‰ª•Ë°®Á§∫ÁöÑ‰∏çÂêåË™ûÊÑè„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑËàáÊ®°ÂûãÁÑ°ÈóúÁöÑ DPL Êû∂ÊßãÁÇ∫ÁèæÊúâÁöÑ SGG Ê®°ÂûãÂ∏∂‰æÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºå‰∏îÊúâÊïàÂú∞ÁêÜËß£Ë¨ÇË©ûÁöÑË™ûÊÑèÂ§öÊ®£ÊÄß„ÄÇ

##### **ALLaM: Large Language Models for Arabic and English**
2407.15390v1 by M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan

We present ALLaM: Arabic Large Language Model, a series of large language
models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is
carefully trained considering the values of language alignment and knowledge
transfer at scale. Our autoregressive decoder-only architecture models
demonstrate how second-language acquisition via vocabulary expansion and
pretraining on a mixture of Arabic and English text can steer a model towards a
new language (Arabic) without any catastrophic forgetting in the original
language (English). Furthermore, we highlight the effectiveness of using
parallel/translated data to aid the process of knowledge alignment between
languages. Finally, we show that extensive alignment with human preferences can
significantly enhance the performance of a language model compared to models of
a larger scale with lower quality alignment. ALLaM achieves state-of-the-art
performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and
Arabic Exams. Our aligned models improve both in Arabic and English from their
base aligned models.

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫ ALLaMÔºöÈòøÊãâ‰ºØÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºå‰∏ÄÁ≥ªÂàóÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºå‰ª•ÊîØÊè¥ÈòøÊãâ‰ºØË™ûË®ÄÊäÄË°ì (ALT) ÁîüÊÖãÁ≥ªÁµ±„ÄÇALLaM Á∂ìÈÅé‰ªîÁ¥∞Ë®ìÁ∑¥ÔºåËÄÉÊÖÆ‰∫ÜË™ûË®ÄÂ∞çÈΩäÂíåÁü•Ë≠òËΩâÁßªÁöÑÂÉπÂÄº„ÄÇÊàëÂÄëÁöÑËá™Ëø¥Ê≠∏ÂÉÖËß£Á¢ºÂô®Êû∂ÊßãÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄèÈÅéË©ûÂΩôÊì¥ÂÖÖÂíåÂú®ÈòøÊãâ‰ºØË™ûÂíåËã±Ë™ûÊñáÊú¨Ê∑∑ÂêàÈ´î‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰æÜÂºïÂ∞éÊ®°ÂûãÊúùÂêë‰∏ÄÁ®ÆÊñ∞Ë™ûË®ÄÔºàÈòøÊãâ‰ºØË™ûÔºâÔºåËÄå‰∏çÊúÉÂ∞çÂéüÂßãË™ûË®ÄÔºàËã±Ë™ûÔºâÈÄ†Êàê‰ªª‰ΩïÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂº∑Ë™ø‰ΩøÁî®Âπ≥Ë°å/ÁøªË≠ØË≥áÊñô‰æÜÂçîÂä©Ë™ûË®Ä‰πãÈñìÁü•Ë≠òÂ∞çÈΩäÈÅéÁ®ãÁöÑÊúâÊïàÊÄß„ÄÇÊúÄÂæåÔºåÊàëÂÄëË°®ÊòéËàá‰∫∫È°ûÂÅèÂ•ΩÁöÑÂª£Ê≥õÂ∞çÈΩäÂèØ‰ª•È°ØËëóÊèêÂçáË™ûË®ÄÊ®°ÂûãÁöÑÊïàËÉΩÔºåËàáË¶èÊ®°ËºÉÂ§ß‰ΩÜÂ∞çÈΩäÂìÅË≥™ËºÉ‰ΩéÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇALLaM Âú®ÂêÑÁ®ÆÈòøÊãâ‰ºØË™ûÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÅîÊàêÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂåÖÊã¨ MMLU Arabic„ÄÅACVA Âíå Arabic Exams„ÄÇÊàëÂÄëÂ∞çÈΩäÁöÑÊ®°ÂûãÂæûÂÖ∂Âü∫Á§éÂ∞çÈΩäÊ®°Âûã‰∏≠Âú®ÈòøÊãâ‰ºØË™ûÂíåËã±Ë™ûÊñπÈù¢ÈÉΩÊúâÊâÄÊîπÈÄ≤„ÄÇ

##### **The Development of a Comprehensive Spanish Dictionary for Phonetic and Lexical Tagging in Socio-phonetic Research (ESPADA)**
2407.15375v1 by Simon Gonzalez

Pronunciation dictionaries are an important component in the process of
speech forced alignment. The accuracy of these dictionaries has a strong effect
on the aligned speech data since they help the mapping between orthographic
transcriptions and acoustic signals. In this paper, I present the creation of a
comprehensive pronunciation dictionary in Spanish (ESPADA) that can be used in
most of the dialect variants of Spanish data. Current dictionaries focus on
specific regional variants, but with the flexible nature of our tool, it can be
readily applied to capture the most common phonetic differences across major
dialectal variants. We propose improvements to current pronunciation
dictionaries as well as mapping other relevant annotations such as
morphological and lexical information. In terms of size, it is currently the
most complete dictionary with more than 628,000 entries, representing words
from 16 countries. All entries come with their corresponding pronunciations,
morphological and lexical tagging, and other relevant information for phonetic
analysis: stress patterns, phonotactics, IPA transcriptions, and more. This
aims to equip socio-phonetic researchers with a complete open-source tool that
enhances dialectal research within socio-phonetic frameworks in the Spanish
language.

ÊëòË¶ÅÔºöÁôºÈü≥Â≠óÂÖ∏ÊòØÂº∑Âà∂Ë™ûÈü≥ÊØîÂ∞çÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈáçË¶ÅÁµÑÊàêÈÉ®ÂàÜ„ÄÇÈÄô‰∫õÂ≠óÂÖ∏ÁöÑÊ∫ñÁ¢∫Â∫¶Â∞çÊØîÂ∞çÁöÑË™ûÈü≥Ë≥áÊñôÊúâÂæàÂ§ßÁöÑÂΩ±ÈüøÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊúâÂä©ÊñºÊ≠£Â≠óÊ≥ïËΩâÈåÑÂíåËÅ≤Â≠∏‰ø°Ëôü‰πãÈñìÁöÑÂ∞çÊáâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂ∞á‰ªãÁ¥π‰∏ÄÂÄãË•øÁè≠ÁâôË™ûÁôºÈü≥Â≠óÂÖ∏ÔºàESPADAÔºâÁöÑÂª∫Á´ãÔºåË©≤Â≠óÂÖ∏ÂèØÁî®ÊñºÂ§ßÂ§öÊï∏Ë•øÁè≠ÁâôË™ûË≥áÊñôÁöÑÊñπË®ÄËÆäÈ´î„ÄÇÁõÆÂâçÁöÑÂ≠óÂÖ∏ÂÅ¥ÈáçÊñºÁâπÂÆöÁöÑÂçÄÂüüËÆäÈ´îÔºå‰ΩÜÈÄöÈÅéÊàëÂÄëÂ∑•ÂÖ∑ÁöÑÈùàÊ¥ªÊÄßÔºåÂèØ‰ª•ÂæàÂÆπÊòìÂú∞ÊáâÁî®ÂÆÉ‰æÜÊçïÊçâ‰∏ªË¶ÅÊñπË®ÄËÆäÈ´î‰∏≠ÊúÄÂ∏∏Ë¶ãÁöÑË™ûÈü≥Â∑ÆÁï∞„ÄÇÊàëÂÄëÂª∫Ë≠∞Â∞çÁõÆÂâçÁöÑÁôºÈü≥Â≠óÂÖ∏ÈÄ≤Ë°åÊîπÈÄ≤Ôºå‰∏¶Â∞çÂÖ∂‰ªñÁõ∏ÈóúË®ªÈáãÔºàÂ¶ÇÂΩ¢ÊÖãÂíåË©ûÂΩô‰ø°ÊÅØÔºâÈÄ≤Ë°åÂ∞çÊáâ„ÄÇÂ∞±Ë¶èÊ®°ËÄåË®ÄÔºåÂÆÉÁõÆÂâçÊòØÊúÄÂÆåÊï¥ÁöÑÂ≠óÂÖ∏ÔºåÊìÅÊúâË∂ÖÈÅé 628,000 ÂÄãÊ¢ùÁõÆÔºå‰ª£Ë°®‰∫Ü‰æÜËá™ 16 ÂÄãÂúãÂÆ∂ÁöÑÂñÆË©û„ÄÇÊâÄÊúâÊ¢ùÁõÆÈÉΩÂ∏∂ÊúâÁõ∏ÊáâÁöÑÁôºÈü≥„ÄÅÂΩ¢ÊÖãÂíåË©ûÂΩôÊ®ôË®òÔºå‰ª•ÂèäÂÖ∂‰ªñËàáË™ûÈü≥ÂàÜÊûêÁõ∏ÈóúÁöÑ‰ø°ÊÅØÔºöÈáçÈü≥Ê®°Âºè„ÄÅÈü≥‰ΩçÂ≠∏„ÄÅIPA ËΩâÈåÑÁ≠â„ÄÇÈÄôÊó®Âú®ÁÇ∫Á§æÊúÉË™ûÈü≥Â≠∏Á†îÁ©∂‰∫∫Âì°Êèê‰æõ‰∏ÄÂÄãÂÆåÊï¥ÁöÑÈñãÊ∫êÂ∑•ÂÖ∑ÔºåÂ¢ûÂº∑Ë•øÁè≠ÁâôË™ûÁ§æÊúÉË™ûÈü≥Â≠∏Ê°ÜÊû∂ÂÖßÁöÑÊñπË®ÄÁ†îÁ©∂„ÄÇ

##### **ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter Posts**
2407.15374v1 by Simon Gonzalez

Social Media platforms have offered invaluable opportunities for linguistic
research. The availability of up-to-date data, coming from any part in the
world, and coming from natural contexts, has allowed researchers to study
language in real time. One of the fields that has made great use of social
media platforms is Corpus Linguistics. There is currently a wide range of
projects which have been able to successfully create corpora from social media.
In this paper, we present the development and deployment of a linguistic corpus
from Twitter posts in English, coming from 26 news agencies and 27 individuals.
The main goal was to create a fully annotated English corpus for linguistic
analysis. We include information on morphology and syntax, as well as NLP
features such as tokenization, lemmas, and n- grams. The information is
presented through a range of powerful visualisations for users to explore
linguistic patterns in the corpus. With this tool, we aim to contribute to the
area of language technologies applied to linguistic research.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™í‰ΩìÂπ≥Âè∞‰∏∫ËØ≠Ë®ÄÁ†îÁ©∂Êèê‰æõ‰∫ÜÊó†‰ª∑ÁöÑÊú∫‰ºö„ÄÇÊù•Ëá™‰∏ñÁïå‰ªª‰ΩïÂú∞Êñπ„ÄÅ‰∏îÊ∫êËá™Ëá™ÁÑ∂ËØ≠Â¢ÉÁöÑÊúÄÊñ∞ËµÑÊñôÔºåËÆ©Á†îÁ©∂‰∫∫ÂëòÂæó‰ª•ÂÆûÊó∂Á†îÁ©∂ËØ≠Ë®Ä„ÄÇÂñÑÁî®Á§æÁæ§Â™í‰ΩìÂπ≥Âè∞ÁöÑÈ¢ÜÂüü‰πã‰∏ÄÊòØËØ≠ÊñôÂ∫ìËØ≠Ë®ÄÂ≠¶„ÄÇÁõÆÂâçÊúâËÆ∏Â§ö‰∏ìÊ°àÊàêÂäüÂú∞‰ªéÁ§æÁæ§Â™í‰ΩìÂª∫Á´ãËØ≠ÊñôÂ∫ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰ªé 26 ÂÆ∂Êñ∞ÈóªÊú∫ÊûÑÂíå 27 ‰Ωç‰∏™‰∫∫ÂèëÂá∫ÁöÑ Twitter Ë¥¥Êñá‰∏≠ÔºåÂèëÂ±ïÂπ∂ÈÉ®ÁΩ≤‰∏Ä‰∏™ËØ≠Ë®ÄËØ≠ÊñôÂ∫ì„ÄÇ‰∏ªË¶ÅÁõÆÊ†áÊòØÂª∫Á´ã‰∏Ä‰∏™ÂÆåÊï¥ÁöÑËã±ÊñáÊ†áÊ≥®ËØ≠ÊñôÂ∫ìÔºå‰ª•Âà©ËØ≠Ë®ÄÂàÜÊûê„ÄÇÊàë‰ª¨Á∫≥ÂÖ•ÂΩ¢ÊÄÅÂíåÂè•Ê≥ïËµÑËÆØÔºå‰ª•ÂèäËØçÂÖÉÂåñ„ÄÅËØçÂπ≤Âíå n-gram Á≠â NLP ÂäüËÉΩ„ÄÇËøô‰∫õËµÑËÆØÈÄèËøá‰∏ÄÁ≥ªÂàóÂº∫Â§ßÁöÑËßÜËßâÂåñÊñπÂºèÂëàÁé∞Ôºå‰æõ‰ΩøÁî®ËÄÖÊé¢Á¥¢ËØ≠ÊñôÂ∫ì‰∏≠ÁöÑËØ≠Ë®ÄÊ®°Âºè„ÄÇÊúâ‰∫ÜËøô‰∏™Â∑•ÂÖ∑ÔºåÊàë‰ª¨Â∏åÊúõÂØπÂ∫îÁî®‰∫éËØ≠Ë®ÄÁ†îÁ©∂ÁöÑËØ≠Ë®ÄÊäÄÊúØÈ¢ÜÂüüÊúâÊâÄË¥°ÁåÆ„ÄÇ

##### **A Network Analysis Approach to Conlang Research Literature**
2407.15370v1 by Simon Gonzalez

The field of conlang has evidenced an important growth in the last decades.
This has been the product of a wide interest in the use and study of conlangs
for artistic purposes. However, one important question is what it is happening
with conlang in the academic world. This paper aims to have an overall
understanding of the literature on conlang research. With this we aim to give a
realistic picture of the field in present days. We have implemented a
computational linguistic approach, combining bibliometrics and network analysis
to examine all publications available in the Scopus database. Analysing over
2300 academic publications since 1927 until 2022, we have found that Esperanto
is by far the most documented conlang. Three main authors have contributed to
this: Garv\'ia R., Fiedler S., and Blanke D. The 1970s and 1980s have been the
decades where the foundations of current research have been built. In terms of
methodologies, language learning and experimental linguistics are the ones
contributing to most to the preferred approaches of study in the field. We
present the results and discuss our limitations and future work.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Ë™ûË®ÄÈ†òÂüüÂú®ÈÅéÂéªÊï∏ÂçÅÂπ¥‰æÜÂ∑≤Á∂ìË≠âÂØ¶‰∫ÜÂÖ∂ÈáçË¶ÅÊàêÈï∑„ÄÇ
ÈÄôÊòØÂõ†ÁÇ∫‰∫∫ÂÄëÂ∞çÊñº‰∫∫Â∑•Ë™ûË®ÄÂú®ËóùË°ìÁõÆÁöÑ‰∏äÁöÑ‰ΩøÁî®ÂíåÁ†îÁ©∂Áî¢Áîü‰∫ÜÂª£Ê≥õÁöÑËààË∂£„ÄÇ
ÁÑ∂ËÄåÔºå‰∏ÄÂÄãÈáçË¶ÅÁöÑÂïèÈ°åÊòØ‰∫∫Â∑•Ë™ûË®ÄÂú®Â≠∏Ë°ìÁïå‰∏≠ÁôºÁîü‰∫Ü‰ªÄÈ∫º‰∫ã„ÄÇ
ÈÄôÁØáË´ñÊñáÊó®Âú®Â∞ç‰∫∫Â∑•Ë™ûË®ÄÁ†îÁ©∂ÁöÑÊñáÁçªÊúâ‰∏ÄÂÄãÊï¥È´îÁöÑ‰∫ÜËß£„ÄÇ
Êúâ‰∫ÜÈÄôÂÄãÔºåÊàëÂÄëÂ∏åÊúõÂ∞çÁï∂‰ªäÈ†òÂüüÊúâ‰∏ÄÂÄãÂØ¶ÈöõÁöÑÊèèÁπ™„ÄÇ
ÊàëÂÄëÂØ¶ÊñΩ‰∫Ü‰∏ÄÁ®ÆË®àÁÆóË™ûË®ÄÂ≠∏ÊñπÊ≥ïÔºåÁµêÂêàÊõ∏ÁõÆË®àÈáèÂ≠∏ÂíåÁ∂≤Ë∑ØÂàÜÊûê
‰æÜÊ™¢È©ó Scopus Ë≥áÊñôÂ∫´‰∏≠ÊâÄÊúâÂèØÁî®ÁöÑÂá∫ÁâàÁâ©„ÄÇ
ÂàÜÊûêËá™ 1927 Âπ¥Ëá≥ 2022 Âπ¥‰ª•‰æÜË∂ÖÈÅé 2300 ÁØáÂ≠∏Ë°ìÂá∫ÁâàÁâ©ÔºåÊàëÂÄëÁôºÁèæ
‰∏ñÁïåË™ûËøÑ‰ªäÁÇ∫Ê≠¢ÊòØÊúÄÊúâË®òÈåÑÁöÑ‰∫∫Â∑•Ë™ûË®Ä„ÄÇ
‰∏â‰Ωç‰∏ªË¶Å‰ΩúËÄÖÂÅöÂá∫‰∫ÜË≤¢ÁçªÔºöGarv\'ia R.„ÄÅFiedler S. Âíå Blanke D.
1970 Âπ¥‰ª£Âíå 1980 Âπ¥‰ª£ÊòØÁï∂ÂâçÁ†îÁ©∂Âü∫Á§éÂª∫Á´ãÁöÑÂπ¥‰ª£„ÄÇ
Âú®ÊñπÊ≥ïË´ñÊñπÈù¢ÔºåË™ûË®ÄÂ≠∏ÁøíÂíåÂØ¶È©óË™ûË®ÄÂ≠∏ÊòØÂ∞çË©≤È†òÂüü
È¶ñÈÅ∏ÁöÑÁ†îÁ©∂ÊñπÊ≥ïÂÅöÂá∫ÊúÄÂ§ßË≤¢ÁçªÁöÑÊñπÊ≥ï„ÄÇ
ÊàëÂÄëÂëàÁèæÁµêÊûú‰∏¶Ë®éË´ñÊàëÂÄëÁöÑÈôêÂà∂ÂíåÊú™‰æÜÂ∑•‰Ωú„ÄÇ</paragraph>

##### **Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias**
2407.15366v1 by Rongwu Xu, Zi'an Zhou, Tianwei Zhang, Zehan Qi, Su Yao, Ke Xu, Wei Xu, Han Qiu

The common toxicity and societal bias in contents generated by large language
models (LLMs) necessitate strategies to reduce harm. Present solutions often
demand white-box access to the model or substantial training, which is
impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting
methods depend on external tool feedback and fail to simultaneously lessen
toxicity and bias. Motivated by social psychology principles, we propose a
novel strategy named \textbf{perspective-taking prompting (\textsc{PeT})} that
inspires LLMs to integrate diverse human perspectives and self-regulate their
responses. This self-correction mechanism can significantly diminish toxicity
(up to $89\%$) and bias (up to $73\%$) in LLMs' responses. Rigorous evaluations
and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and
three open-source LLMs, revealing \textsc{PeT}'s superiority in producing less
harmful responses, outperforming five strong baselines.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁî¢ÁîüÁöÑÂÖßÂÆπ‰∏≠Â∏∏Ë¶ãÁöÑÊØíÊÄßËàáÁ§æÊúÉÂÅèË¶ãÔºåÈúÄË¶ÅÊúâÁ≠ñÁï•‰æÜÊ∏õÂ∞ëÂÇ∑ÂÆ≥„ÄÇÁèæÊúâÁöÑËß£Ê±∫ÊñπÊ°àÈÄöÂ∏∏ÈúÄË¶ÅÂ∞çÊ®°ÂûãÊúâÁôΩÁõíÂ≠òÂèñÊ¨äÊàñÂ§ßÈáèÁöÑË®ìÁ∑¥ÔºåÈÄôÂ∞çÊñºÂ∞ñÁ´ØÁöÑÂïÜÊ•≠ LLM ‰æÜË™™ÊòØ‰∏çÂàáÂØ¶ÈöõÁöÑ„ÄÇÊ≠§Â§ñÔºåÊµÅË°åÁöÑÊèêÁ§∫ÊñπÊ≥ï‰æùË≥¥ÊñºÂ§ñÈÉ®Â∑•ÂÖ∑ÂõûÈ•ãÔºå‰∏îÁÑ°Ê≥ïÂêåÊôÇÊ∏õÂ∞ëÊØíÊÄßÂíåÂÅèË¶ã„ÄÇÊàëÂÄëÂèóÂà∞Á§æÊúÉÂøÉÁêÜÂ≠∏ÂéüÁêÜÁöÑÂïüÁôºÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫„ÄåËßÄÈªûÊé°ÂèñÊèêÁ§∫ÔºàPeTÔºâ„ÄçÁöÑÊñ∞Á≠ñÁï•ÔºåÂÆÉÊøÄÂãµ LLM Êï¥ÂêàÂ§öÂÖÉÁöÑ‰∫∫È°ûËßÄÈªû‰∏¶Ëá™ÊàëË™øÁØÄÂÖ∂ÂõûÊáâ„ÄÇÈÄôÁ®ÆËá™Êàë‰øÆÊ≠£Ê©üÂà∂ÂèØ‰ª•È°ØËëóÊ∏õÂ∞ë LLM ÂõûÊáâ‰∏≠ÁöÑÊØíÊÄßÔºàÈ´òÈÅî 89%ÔºâÂíåÂÅèË¶ãÔºàÈ´òÈÅî 73%Ôºâ„ÄÇÂ∞çÂÖ©ÂÄãÂïÜÊ•≠ LLMÔºàChatGPT Âíå GLMÔºâÂíå‰∏âÂÄãÈñãÊ∫ê LLM ÈÄ≤Ë°å‰∫ÜÂö¥Ë¨πÁöÑË©ï‰º∞ÂíåÊ∂àËûçÁ†îÁ©∂ÔºåÊè≠Á§∫‰∫Ü PeT Âú®Áî¢ÁîüËºÉÂ∞ëÊúâÂÆ≥ÂõûÊáâÊñπÈù¢ÁöÑÂÑ™Ë∂äÊÄßÔºåÂÑ™Êñº‰∫îÂÄãÂº∑Â§ßÁöÑÂü∫Á∑ö„ÄÇ

##### **A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model**
2407.15362v1 by Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, Ronald Cheong Kin Chan, Hao Chen

Remarkable strides in computational pathology have been made in the
task-agnostic foundation model that advances the performance of a wide array of
downstream clinical tasks. Despite the promising performance, there are still
several challenges. First, prior works have resorted to either vision-only or
vision-captions data, disregarding invaluable pathology reports and gene
expression profiles which respectively offer distinct knowledge for versatile
clinical applications. Second, the current progress in pathology FMs
predominantly concentrates on the patch level, where the restricted context of
patch-level pretraining fails to capture whole-slide patterns. Here we curated
the largest multimodal dataset consisting of H\&E diagnostic whole slide images
and their associated pathology reports and RNA-Seq data, resulting in 26,169
slide-level modality pairs from 10,275 patients across 32 cancer types. To
leverage these data for CPath, we propose a novel whole-slide pretraining
paradigm which injects multimodal knowledge at the whole-slide context into the
pathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed
paradigm revolutionizes the workflow of pretraining for CPath, which enables
the pathology FM to acquire the whole-slide context. To our knowledge, this is
the first attempt to incorporate multimodal knowledge at the slide level for
enhancing pathology FMs, expanding the modelling context from unimodal to
multimodal knowledge and from patch-level to slide-level. To systematically
evaluate the capabilities of mSTAR, extensive experiments including slide-level
unimodal and multimodal applications, are conducted across 7 diverse types of
tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.
The average performance in various slide-level applications consistently
demonstrates significant performance enhancements for mSTAR compared to SOTA
FMs.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë®àÁÆóÁóÖÁêÜÂ≠∏‰∏≠Ôºå‰ªªÂãô‰∏çÂèØÁü•Âü∫Á§éÊ®°ÂûãÂ∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºåÂèØÊèêÂçáÂª£Ê≥õ‰∏ãÊ∏∏Ëá®Â∫ä‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÂÑòÁÆ°ÊïàËÉΩ‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜ‰ªçÊúâÂπæÂÄãÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÂÉÖÊé°Áî®ÂÉÖÈôêÂΩ±ÂÉèÊàñÂΩ±ÂÉèÊ®ôÈ°åË≥áÊñôÔºåÂøΩÁï•‰∫ÜÂØ∂Ë≤¥ÁöÑÁóÖÁêÜÂ†±ÂëäÂíåÂü∫Âõ†Ë°®ÁèæÁâπÂæµÔºåËÄåÈÄô‰∫õÁâπÂæµÂàÜÂà•ÁÇ∫Â§öÂäüËÉΩËá®Â∫äÊáâÁî®Êèê‰æõ‰∫Ü‰∏çÂêåÁöÑÁü•Ë≠ò„ÄÇÂÖ∂Ê¨°ÔºåÁóÖÁêÜ FM ÁöÑÁï∂ÂâçÈÄ≤Â∫¶‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçÄÂ°äÂ±§Á¥öÔºåÂçÄÂ°äÂ±§Á¥öÈ†êË®ìÁ∑¥ÁöÑÂèóÈôêËÉåÊôØÁÑ°Ê≥ïÊì∑ÂèñÂÖ®ÂàáÁâáÊ®°Âºè„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫ÜÊúÄÂ§ßÁöÑÂ§öÊ®°ÊÖãË≥áÊñôÈõÜÔºåÂåÖÂê´ H&E Ë®∫Êñ∑ÂÖ®ÂàáÁâáÂΩ±ÂÉèÂèäÂÖ∂Áõ∏ÈóúÁóÖÁêÜÂ†±ÂëäÂíå RNA-Seq Ë≥áÊñôÔºåÂÖ±Áî¢Áîü‰æÜËá™ 32 Á®ÆÁôåÁóáÈ°ûÂûãÁöÑ 10,275 ÂêçÊÇ£ËÄÖÁöÑ 26,169 ÂÄãÂàáÁâáÂ±§Á¥öÊ®°ÊÖãÈÖçÂ∞ç„ÄÇÁÇ∫‰∫ÜÂ∞áÈÄô‰∫õË≥áÊñôÁî®Êñº CPathÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂÖ®ÂàáÁâáÈ†êË®ìÁ∑¥ÁØÑ‰æãÔºåÂ∞áÂÖ®ÂàáÁâáËÉåÊôØÁöÑÂ§öÊ®°ÊÖãÁü•Ë≠òÊ≥®ÂÖ•ÁóÖÁêÜ FMÔºåÁ®±ÁÇ∫Â§öÊ®°ÊÖãËá™ÊïôÈ†êË®ìÁ∑¥ (mSTAR)„ÄÇÊâÄÊèêÂá∫ÁöÑÁØÑ‰æãÂæπÂ∫ïÊîπËÆä‰∫Ü CPath ÁöÑÈ†êË®ìÁ∑¥Â∑•‰ΩúÊµÅÁ®ãÔºå‰ΩøÁóÖÁêÜ FM ËÉΩÂ§†Áç≤ÂèñÂÖ®ÂàáÁâáËÉåÊôØ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÈ¶ñÊ¨°ÂòóË©¶Âú®ÂàáÁâáÂ±§Á¥öÁ¥çÂÖ•Â§öÊ®°ÊÖãÁü•Ë≠ò‰ª•Â¢ûÂº∑ÁóÖÁêÜ FMÔºåÂ∞áÂª∫Ê®°ËÉåÊôØÂæûÂñÆ‰∏ÄÊ®°ÊÖãÁü•Ë≠òÊì¥Â±ïÂà∞Â§öÊ®°ÊÖãÁü•Ë≠òÔºå‰ª•ÂèäÂæûÂçÄÂ°äÂ±§Á¥öÊì¥Â±ïÂà∞ÂàáÁâáÂ±§Á¥ö„ÄÇÁÇ∫‰∫ÜÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ mSTAR ÁöÑÂäüËÉΩÔºåÊàëÂÄëÂú® 43 ÂÄãÂ≠ê‰ªªÂãô‰∏≠Â∞ç 7 Á®Æ‰∏çÂêåÈ°ûÂûãÁöÑ‰ªªÂãôÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÂåÖÊã¨ÂàáÁâáÂ±§Á¥öÁöÑÂñÆ‰∏ÄÊ®°ÊÖãÂíåÂ§öÊ®°ÊÖãÊáâÁî®ÔºåÁî¢Áîü‰∫ÜÊúÄÂ§ßÁöÑ‰∏ãÊ∏∏‰ªªÂãôÁØÑÂúç„ÄÇÂú®ÂêÑÁ®ÆÂàáÁâáÂ±§Á¥öÊáâÁî®‰∏≠ÔºåÂπ≥ÂùáÊïàËÉΩÊåÅÁ∫åÈ°ØÁ§∫ mSTAR Ëàá SOTA FM Áõ∏ÊØîÊúâÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇ</paragraph>

##### **Dissecting Multiplication in Transformers: Insights into LLMs**
2407.15360v1 by Luyu Qiu, Jianing Li, Chi Su, Chen Jason Zhang, Lei Chen

Transformer-based large language models have achieved remarkable performance
across various natural language processing tasks. However, they often struggle
with seemingly easy tasks like arithmetic despite their vast capabilities. This
stark disparity raise human's concerns about their safe and ethical use, hinder
their widespread adoption.In this paper, we focus on a typical arithmetic task,
integer multiplication, to explore and explain the imperfection of transformers
in this domain. We provide comprehensive analysis of a vanilla transformer
trained to perform n-digit integer multiplication. Our observations indicate
that the model decomposes multiplication task into multiple parallel subtasks,
sequentially optimizing each subtask for each digit to complete the final
multiplication. Based on observation and analysis, we infer the reasons of
transformers deficiencies in multiplication tasks lies in their difficulty in
calculating successive carryovers and caching intermediate results, and
confirmed this inference through experiments. Guided by these findings, we
propose improvements to enhance transformers performance on multiplication
tasks. These enhancements are validated through rigorous testing and
mathematical modeling, not only enhance transformer's interpretability, but
also improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit
integer multiplication with a tiny transformer, outperform LLMs GPT-4. Our
method contributes to the broader fields of model understanding and
interpretability, paving the way for analyzing more complex tasks and
Transformer models. This work underscores the importance of explainable AI,
helping to build trust in large language models and promoting their adoption in
critical applications.

ÊëòË¶ÅÔºö<paragraph>Âü∫Êñº Transformer ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°ÂÆÉÂÄëÂäüËÉΩÂº∑Â§ßÔºå‰ΩÜÂÆÉÂÄëÁ∂ìÂ∏∏Âú®ÁÆóË°ìÁ≠âÁúã‰ººÁ∞°ÂñÆÁöÑ‰ªªÂãô‰∏äÈÅáÂà∞Âõ∞Èõ£„ÄÇÈÄôÁ®ÆÊòéÈ°ØÁöÑÂ∑ÆÁï∞ÂºïËµ∑‰∫Ü‰∫∫ÂÄëÂ∞çÂÖ∂ÂÆâÂÖ®ÂíåÈÅìÂæ∑‰ΩøÁî®ÊñπÂºèÁöÑÊìîÊÜÇÔºåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÊé°Áî®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº‰∏ÄÈ†ÖÂÖ∏ÂûãÁöÑÁÆóË°ì‰ªªÂãôÔºåÊï¥Êï∏‰πòÊ≥ïÔºå‰ª•Êé¢Á¥¢ÂíåËß£Èáã Transformer Âú®ÈÄôÂÄãÈ†òÂüü‰∏≠ÁöÑ‰∏çÂÆåÁæé„ÄÇÊàëÂÄëÂ∞ç‰∏ÄÂÄãË®ìÁ∑¥ÊúâÁ¥†ÁöÑÈ¶ôËçâ Transformer ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂàÜÊûêÔºå‰ª•Âü∑Ë°å n ‰ΩçÊï¥Êï∏‰πòÊ≥ï„ÄÇÊàëÂÄëÁöÑËßÄÂØüË°®ÊòéÔºåË©≤Ê®°ÂûãÂ∞á‰πòÊ≥ï‰ªªÂãôÂàÜËß£ÁÇ∫Â§öÂÄã‰∏¶Ë°åÂ≠ê‰ªªÂãôÔºåÂ∞çÊØèÂÄãÊï∏Â≠óÁöÑÊØèÂÄãÂ≠ê‰ªªÂãôÈÄ≤Ë°åÈ†ÜÂ∫èÂÑ™ÂåñÔºå‰ª•ÂÆåÊàêÊúÄÁµÇ‰πòÊ≥ï„ÄÇÊ†πÊìöËßÄÂØüÂíåÂàÜÊûêÔºåÊàëÂÄëÊé®Êñ∑Âá∫ Transformer Âú®‰πòÊ≥ï‰ªªÂãô‰∏≠ÁöÑÁº∫Èô∑ÂéüÂõ†Âú®ÊñºÂÆÉÂÄëÈõ£‰ª•Ë®àÁÆóÈÄ£Á∫åÈÄ≤‰ΩçÂíåÂø´Âèñ‰∏≠ÈñìÁµêÊûúÔºå‰∏¶ÈÄöÈÅéÂØ¶È©óÁ¢∫Ë™ç‰∫ÜÈÄô‰∏ÄÊé®Ë´ñ„ÄÇÂú®ÈÄô‰∫õÁôºÁèæÁöÑÊåáÂ∞é‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊîπÈÄ≤Âª∫Ë≠∞Ôºå‰ª•Â¢ûÂº∑ Transformer Âú®‰πòÊ≥ï‰ªªÂãô‰∏äÁöÑÊÄßËÉΩ„ÄÇÈÄô‰∫õÊîπÈÄ≤ÈÄöÈÅéÂö¥Ê†ºÁöÑÊ∏¨Ë©¶ÂíåÊï∏Â≠∏Âª∫Ê®°ÂæóÂà∞È©óË≠âÔºå‰∏çÂÉÖÂ¢ûÂº∑‰∫Ü Transformer ÁöÑÂèØËß£ÈáãÊÄßÔºåÈÇÑÊèêÈ´ò‰∫ÜÂÆÉÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÔºåÊàëÂÄëÂú® 5 ‰ΩçÊï¥Êï∏‰πòÊ≥ï‰∏äÂØ¶Áèæ‰∫ÜË∂ÖÈÅé 99.9% ÁöÑÊ∫ñÁ¢∫ÁéáÔºå‰ΩøÁî®‰∏ÄÂÄãÂæÆÂ∞èÁöÑ TransformerÔºåÂÑ™Êñº LLM GPT-4„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊñπÊ≥ïÊúâÂä©ÊñºÊ®°ÂûãÁêÜËß£ÂíåÂèØËß£ÈáãÊÄßÁöÑÊõ¥Âª£Ê≥õÈ†òÂüüÔºåÁÇ∫ÂàÜÊûêÊõ¥Ë§áÈõúÁöÑ‰ªªÂãôÂíå Transformer Ê®°ÂûãÈã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂº∑Ë™ø‰∫ÜÂèØËß£Èáã AI ÁöÑÈáçË¶ÅÊÄßÔºåÊúâÂä©ÊñºÂª∫Á´ãÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑ‰ø°‰ªªÔºå‰∏¶‰øÉÈÄ≤ÂÆÉÂÄëÂú®ÈóúÈçµÊáâÁî®‰∏≠ÁöÑÊé°Áî®„ÄÇ</paragraph>

##### **UF-HOBI at "Discharge Me!": A Hybrid Solution for Discharge Summary Generation Through Prompt-based Tuning of GatorTronGPT Models**
2407.15359v1 by Mengxian Lyu, Cheng Peng, Daniel Paredes, Ziyi Chen, Aokun Chen, Jiang Bian, Yonghui Wu

Automatic generation of discharge summaries presents significant challenges
due to the length of clinical documentation, the dispersed nature of patient
information, and the diverse terminology used in healthcare. This paper
presents a hybrid solution for generating discharge summary sections as part of
our participation in the "Discharge Me!" Challenge at the BioNLP 2024 Shared
Task. We developed a two-stage generation method using both extractive and
abstractive techniques, in which we first apply name entity recognition (NER)
to extract key clinical concepts, which are then used as input for a
prompt-tuning-based GatorTronGPT model to generate coherent text for two
important sections including "Brief Hospital Course" and "Discharge
Instructions". Our system was ranked 5th in this challenge, achieving an
overall score of 0.284. The results demonstrate the effectiveness of our hybrid
solution in improving the quality of automated discharge section generation.

ÊëòË¶ÅÔºöÁî±ÊñºËá®Â∫äÊñá‰ª∂Èï∑Â∫¶„ÄÅÊÇ£ËÄÖË≥áË®äÂàÜÊï£‰ª•ÂèäÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî®ÁöÑË°ìË™ûÂ§öÊ®£ÔºåËá™ÂãïÁî¢ÁîüÂá∫Èô¢ÊëòË¶ÅÊúÉÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÁî¢ÁîüÂá∫Èô¢ÊëòË¶ÅÈÉ®ÂàÜÔºå‰ΩúÁÇ∫ÊàëÂÄëÂèÉËàá BioNLP 2024 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑ„ÄåDischarge Me!„ÄçÊåëÊà∞ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ËêÉÂèñÂíåÊëòË¶ÅÊäÄË°ìÁöÑÂÖ©ÈöéÊÆµÁî¢ÁîüÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÊàëÂÄëÈ¶ñÂÖàÊáâÁî®ÂëΩÂêçÂØ¶È´îËæ®Ë≠ò (NER) ‰æÜËêÉÂèñÈóúÈçµËá®Â∫äÊ¶ÇÂøµÔºåÁÑ∂ÂæåÂ∞áÂÖ∂Áî®‰ΩúÊèêÁ§∫Ë™øÊï¥Âºè GatorTronGPT Ê®°ÂûãÁöÑËº∏ÂÖ•Ôºå‰ª•Áî¢ÁîüÂÖ©ÂÄãÈáçË¶ÅÈÉ®ÂàÜÁöÑÈÄ£Ë≤´ÊñáÂ≠óÔºåÂåÖÊã¨„ÄåÁ∞°Ë¶Å‰ΩèÈô¢ÁóÖÁ®ã„ÄçÂíå„ÄåÂá∫Èô¢ÊåáÁ§∫„Äç„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±Âú®Ê≠§ÊåëÊà∞‰∏≠ÊéíÂêçÁ¨¨ 5ÔºåÈÅîÂà∞ 0.284 ÁöÑÁ∏ΩÂàÜ„ÄÇÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ∑∑ÂêàËß£Ê±∫ÊñπÊ°àÂú®ÊîπÂñÑËá™ÂãïÂá∫Èô¢ÈÉ®ÂàÜÁî¢ÁîüÁöÑÂìÅË≥™ÊñπÈù¢ÊúâÊïà„ÄÇ

##### **X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images**
2407.15356v1 by Yunpeng Wang, Kang Wang, Yaoyao Zhuo, Weiya Shi, Fei Shan, Lei Liu

Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and
computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is
commonly used for initial localization of pneumothorax, while CT ensures
accurate quantification. However, CT scans involve high radiation doses and can
be costly. To achieve precise quantitative diagnosis while minimizing radiation
exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based
on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial
networks (GANs), including a generator with a multi-scale fusion rendering
module and a discriminator enhanced by 3D coordinate convolutional layers,
designed to facilitate CT reconstruction. To improve precision, a projective
spatial transformer is utilized to incorporate multi-angle projection loss.
Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation
algorithm, combining image processing techniques with deep-learning models for
the segmentation of air-accumulated regions and lung structures. Experiments on
a large-scale dataset demonstrate its superiority over existing approaches.
X-Recon achieved a significantly higher reconstruction resolution with a higher
average spatial resolution and a lower average slice thickness. The
reconstruction metrics achieved state-of-the-art performance in terms of
several metrics including peak signal-to-noise ratio. The zero-shot
segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision
for the air-accumulated region, the left lung, and the right lung. Moreover,
the consistency analysis for the pneumothorax chest occupancy ratio between
reconstructed CT and original CT obtained a high correlation coefficient. Code
will be available at: https://github.com/wangyunpengbio/X-Recon

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®ËÉ∏ÈÉ® X Â∞ÑÁ∑öÂíåÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) Âø´ÈÄüÊ∫ñÁ¢∫Ë®∫Êñ∑Ê∞£ËÉ∏ÔºåÂ∞çÊñºËºîÂä©Ë®∫Êñ∑Ëá≥ÈóúÈáçË¶Å„ÄÇËÉ∏ÈÉ® X Â∞ÑÁ∑öÈÄöÂ∏∏Áî®ÊñºÊ∞£ËÉ∏ÁöÑÂàùÊ≠•ÂÆö‰ΩçÔºåËÄå CT ÂèØÁ¢∫‰øùÊ∫ñÁ¢∫ÈáèÂåñ„ÄÇÁÑ∂ËÄåÔºåCT ÊéÉÊèèÊ∂âÂèäÈ´òËºªÂ∞ÑÂäëÈáè‰∏îÂèØËÉΩÊòÇË≤¥„ÄÇÁÇ∫‰∫ÜÂú®ÊúÄÂ§ßÁ®ãÂ∫¶Èôç‰ΩéËºªÂ∞ÑÊö¥Èú≤ÁöÑÂêåÊôÇÂØ¶ÁèæÁ≤æÁ¢∫ÁöÑÂÆöÈáèË®∫Êñ∑ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü X-ReconÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫ÊñºÊ≠£ÂÅ¥‰ΩçËÉ∏ÈÉ® X Â∞ÑÁ∑öÂΩ±ÂÉèÁöÑ CT Ë∂ÖÁ®ÄÁñèÈáçÂª∫Á∂≤Ë∑Ø„ÄÇX-Recon Êï¥Âêà‰∫ÜÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN)ÔºåÂåÖÊã¨‰∏ÄÂÄãÂÖ∑ÊúâÂ§öÂ∞∫Â∫¶ËûçÂêàÊ∏≤ÊüìÊ®°ÁµÑÁöÑÁîüÊàêÂô®Âíå‰∏ÄÂÄãÁî± 3D Â∫ßÊ®ôÂç∑Á©çÂ±§Â¢ûÂº∑ÁöÑÂà§Âà•Âô®ÔºåÊó®Âú®‰øÉÈÄ≤ CT ÈáçÂª∫„ÄÇÁÇ∫‰∫ÜÊèêÈ´òÁ≤æÂ∫¶ÔºåÂà©Áî®Â∞ÑÂΩ±Á©∫ÈñìËÆäÊèõÂô®‰æÜÁ¥çÂÖ•Â§öËßíÂ∫¶ÊäïÂΩ±ÊêçÂ§±„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü PTX-SegÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈõ∂Ê¨°Â≠∏ÁøíÊ∞£ËÉ∏ÂàÜÂâ≤ÊºîÁÆóÊ≥ïÔºåÁµêÂêà‰∫ÜÂΩ±ÂÉèËôïÁêÜÊäÄË°ìËàáÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÁî®ÊñºÂàÜÂâ≤Á©çÊ∞£ÂçÄÂüüÂíåËÇ∫ÈÉ®ÁµêÊßã„ÄÇÂú®Â§ßË¶èÊ®°Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÂÖ∂ÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇX-Recon ‰ª•Êõ¥È´òÁöÑÂπ≥ÂùáÁ©∫ÈñìËß£ÊûêÂ∫¶ÂíåÊõ¥‰ΩéÁöÑÂπ≥ÂùáÂàáÁâáÂéöÂ∫¶ÔºåÂØ¶Áèæ‰∫ÜÈ°ØËëóÊõ¥È´òÁöÑÈáçÂª∫Ëß£ÊûêÂ∫¶„ÄÇÈáçÂª∫ÊåáÊ®ôÂú®ÂåÖÊã¨Â≥∞ÂÄº‰ø°Âô™ÊØîÂú®ÂÖßÁöÑÂπæÂÄãÊåáÊ®ôÊñπÈù¢ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÈõ∂Ê¨°Â≠∏ÁøíÂàÜÂâ≤ÊºîÁÆóÊ≥ï PTX-Seg ‰πüÂ±ïÁ§∫‰∫ÜÂ∞çÁ©çÊ∞£ÂçÄÂüü„ÄÅÂ∑¶ËÇ∫ÂíåÂè≥ËÇ∫ÁöÑÈ´òÂàÜÂâ≤Á≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåÈáçÂª∫ CT ÂíåÂéüÂßã CT ‰πãÈñìÁöÑÊ∞£ËÉ∏ËÉ∏ËÖî‰ΩîÊúâÁéáÁöÑ‰∏ÄËá¥ÊÄßÂàÜÊûêÁç≤Âæó‰∫ÜÂæàÈ´òÁöÑÁõ∏Èóú‰øÇÊï∏„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂú®‰ª•‰∏ã‰ΩçÁΩÆÊèê‰æõÔºöhttps://github.com/wangyunpengbio/X-Recon</paragraph>

##### **Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA**
2407.15353v1 by Yuan Pu, Zhuolun He, Tairu Qiu, Haoyuan Wu, Bei Yu

Retrieval augmented generation (RAG) enhances the accuracy and reliability of
generative AI models by sourcing factual information from external databases,
which is extensively employed in document-grounded question-answering (QA)
tasks. Off-the-shelf RAG flows are well pretrained on general-purpose
documents, yet they encounter significant challenges when being applied to
knowledge-intensive vertical domains, such as electronic design automation
(EDA). This paper addresses such issue by proposing a customized RAG framework
along with three domain-specific techniques for EDA tool documentation QA,
including a contrastive learning scheme for text embedding model fine-tuning, a
reranker distilled from proprietary LLM, and a generative LLM fine-tuned with
high-quality domain corpus. Furthermore, we have developed and released a
documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced
RTL-to-GDSII design platform. Experimental results demonstrate that our
proposed RAG flow and techniques have achieved superior performance on ORD-QA
as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA
benchmark and the training dataset for our customized RAG flow are open-source
at https://github.com/lesliepy99/RAG-EDA.

ÊëòË¶ÅÔºöÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) ÈÄèËøá‰ªéÂ§ñÈÉ®Êï∞ÊçÆÂ∫ìÊí∑Âèñ‰∫ãÂÆû‰ø°ÊÅØÊù•ÊèêÂçáÁîüÊàêÂºè AI Ê®°ÂûãÁöÑÂáÜÁ°ÆÂ∫¶ÂíåÂèØÈù†Â∫¶ÔºåÂπøÊ≥õÁî®‰∫éÂü∫‰∫éÊñá‰ª∂ÁöÑÈóÆÁ≠î (QA) ‰ªªÂä°„ÄÇÁé∞ÊàêÁöÑ RAG ÊµÅÁ®ãÂú®ÈÄöÁî®Êñá‰ª∂‰∏äÁªèËøáÂÖÖÂàÜÈ¢ÑËÆ≠ÁªÉÔºå‰ΩÜÂú®Â∫îÁî®‰∫éÁü•ËØÜÂØÜÈõÜÂûãÂûÇÁõ¥È¢ÜÂüüÔºà‰æãÂ¶ÇÁîµÂ≠êËÆæËÆ°Ëá™Âä®Âåñ (EDA)ÔºâÊó∂‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàò„ÄÇÊú¨ÊñáÈÄèËøáÊèêÂá∫‰∏Ä‰∏™ÂÆ¢Âà∂Âåñ RAG Êû∂ÊûÑ‰ª•Âèä‰∏â‰∏™ÈíàÂØπ EDA Â∑•ÂÖ∑Êñá‰ª∂ QA ÁöÑÈ¢ÜÂüüÁâπÂÆöÊäÄÊúØÊù•Ëß£ÂÜ≥Ê≠§ÈóÆÈ¢òÔºåÂåÖÊã¨Áî®‰∫éÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÂæÆË∞ÉÁöÑÂØπÊØîÂºèÂ≠¶‰π†ÊñπÊ°à„ÄÅ‰ªé‰∏ìÊúâ LLM Ëí∏È¶èÂá∫ÁöÑÈáçÊñ∞ÊéíÂ∫èÂô®Ôºå‰ª•ÂèäÁªèËøáÈ´òË¥®ÈáèÈ¢ÜÂüüËØ≠ÊñôÂ∫ìÂæÆË∞ÉÁöÑÁîüÊàêÂºè LLM„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Â∑≤ÂºÄÂèëÂπ∂ÂèëÂ∏ÉÊñá‰ª∂ QA ËØÑ‰º∞Âü∫ÂáÜ ORD-QAÔºå‰æõ OpenROAD ‰ΩøÁî®ÔºåOpenROAD ÊòØÂÖàËøõÁöÑ RTL ËΩ¨ GDSII ËÆæËÆ°Âπ≥Âè∞„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏éÊúÄÂÖàËøõÁöÑÊäÄÊúØÁõ∏ÊØîÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑ RAG ÊµÅÁ®ãÂíåÊäÄÊúØÂú® ORD-QA ‰ª•ÂèäÂïÜÁî®Â∑•ÂÖ∑‰∏äÂùáËææÂà∞‰ºòÂºÇÁöÑÊïàËÉΩ„ÄÇORD-QA Âü∫ÂáÜÂíåÊàë‰ª¨ÂÆ¢Âà∂Âåñ RAG ÊµÅÁ®ãÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂèØÂú® https://github.com/lesliepy99/RAG-EDA ÂºÄÊ∫êÂèñÂæó„ÄÇ

##### **MAVEN-Fact: A Large-scale Event Factuality Detection Dataset**
2407.15352v1 by Chunyang Li, Hao Peng, Xiaozhi Wang, Yunjia Qi, Lei Hou, Bin Xu, Juanzi Li

Event Factuality Detection (EFD) task determines the factuality of textual
events, i.e., classifying whether an event is a fact, possibility, or
impossibility, which is essential for faithfully understanding and utilizing
event knowledge. However, due to the lack of high-quality large-scale data,
event factuality detection is under-explored in event understanding research,
which limits the development of EFD community. To address these issues and
provide faithful event understanding, we introduce MAVEN-Fact, a large-scale
and high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes
factuality annotations of 112,276 events, making it the largest EFD dataset.
Extensive experiments demonstrate that MAVEN-Fact is challenging for both
conventional fine-tuned models and large language models (LLMs). Thanks to the
comprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact
also supports some further analyses and we find that adopting event arguments
and relations helps in event factuality detection for fine-tuned models but
does not benefit LLMs. Furthermore, we preliminarily study an application case
of event factuality detection and find it helps in mitigating event-related
hallucination in LLMs. Our dataset and codes can be obtained from
\url{https://github.com/lcy2723/MAVEN-FACT}

ÊëòË¶ÅÔºö‰∫ã‰ª∂ÁúüÂØ¶ÊÄßÂÅµÊ∏¨ (EFD) ‰ªªÂãôÊòØÂà§Êñ∑ÊñáÂ≠ó‰∫ã‰ª∂ÁöÑÁúüÂØ¶ÊÄßÔºå‰∫¶Âç≥Â∞á‰∫ã‰ª∂ÂàÜÈ°ûÁÇ∫‰∫ãÂØ¶„ÄÅÂèØËÉΩÊÄßÊàñ‰∏çÂèØËÉΩÔºåÈÄôÂ∞çÊñºÂø†ÂØ¶ÁêÜËß£ÂíåÂà©Áî®‰∫ã‰ª∂Áü•Ë≠òËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÈ´òÂìÅË≥™ÁöÑÂ§ßË¶èÊ®°Ë≥áÊñôÔºå‰∫ã‰ª∂ÁúüÂØ¶ÊÄßÂÅµÊ∏¨Âú®‰∫ã‰ª∂ÁêÜËß£Á†îÁ©∂‰∏≠‰∏¶Êú™Áç≤ÂæóÂÖÖÂàÜÊé¢Á¥¢ÔºåÈÄôÈôêÂà∂‰∫Ü EFD Á§æÁæ§ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å‰∏¶Êèê‰æõÂø†ÂØ¶ÁöÑ‰∫ã‰ª∂ÁêÜËß£ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MAVEN-FactÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº MAVEN Ë≥áÊñôÈõÜÁöÑÂ§ßË¶èÊ®°È´òÂìÅË≥™ EFD Ë≥áÊñôÈõÜ„ÄÇMAVEN-Fact ÂåÖÂê´ 112,276 ÂÄã‰∫ã‰ª∂ÁöÑÁúüÂØ¶ÊÄßË®ªËß£Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫ÊúÄÂ§ßÁöÑ EFD Ë≥áÊñôÈõÜ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåMAVEN-Fact Â∞çÂÇ≥Áµ±ÂæÆË™øÊ®°ÂûãÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜË™™ÈÉΩÊòØÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ„ÄÇÁî±Êñº MAVEN ‰∏≠‰∫ã‰ª∂Ë´ñÂÖÉÂíåÈóú‰øÇÁöÑÂÖ®Èù¢Ë®ªËß£ÔºåMAVEN-Fact ‰πüÊîØÊè¥‰∏Ä‰∫õÈÄ≤‰∏ÄÊ≠•ÁöÑÂàÜÊûêÔºåÊàëÂÄëÁôºÁèæÊé°Áî®‰∫ã‰ª∂Ë´ñÂÖÉÂíåÈóú‰øÇÊúâÂä©ÊñºÂæÆË™øÊ®°ÂûãÁöÑ‰∫ã‰ª∂ÁúüÂØ¶ÊÄßÂÅµÊ∏¨Ôºå‰ΩÜÂ∞ç LLM Ê≤íÊúâÂπ´Âä©„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàùÊ≠•Á†îÁ©∂‰∫Ü‰∫ã‰ª∂ÁúüÂØ¶ÊÄßÂÅµÊ∏¨ÁöÑ‰∏ÄÂÄãÊáâÁî®Ê°à‰æãÔºåÁôºÁèæÂÆÉÊúâÂä©ÊñºÊ∏õËºï LLM ‰∏≠Ëàá‰∫ã‰ª∂Áõ∏ÈóúÁöÑÂπªË¶∫„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÂèØÂæû \url{https://github.com/lcy2723/MAVEN-FACT} ÂèñÂæó

##### **LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**
2407.15351v2 by Jiaxing Zhang, Jiayi Liu, Dongsheng Luo, Jennifer Neville, Hua Wei

Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.

ÊëòË¶ÅÔºöËøëÊúüÁ†îÁ©∂Ë©¶ÂúñÈÄèÈÅéÂ§öÁ®ÆÈùûÁõ£Áù£ÂºèÂ≠∏ÁøíÊ®°Âûã‰æÜÊèê‰æõÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÂèØËß£ÈáãÊÄß„ÄÇÁî±ÊñºË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÔºåÁõÆÂâçÁöÑÊºîÁÆóÊ≥ïÂÆπÊòìÂèóÂà∞Â≠∏ÁøíÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Áü•Ë≠òÂµåÂÖ•Âà∞ GNN Ëß£ÈáãÁ∂≤Ë∑Ø‰∏≠Ôºå‰ª•ÈÅøÂÖçÂ≠∏ÁøíÂÅèÂ∑ÆÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ∞á LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ (BI) Ê®°ÁµÑÊ≥®ÂÖ•Ôºå‰ª•Ê∏õËºïÂ≠∏ÁøíÂÅèÂ∑Æ„ÄÇBI Ê®°ÁµÑÁöÑÊïàËÉΩÂ∑≤Âú®ÁêÜË´ñ‰∏äÂíåÂØ¶È©ó‰∏äÂæóÂà∞Ë≠âÂØ¶„ÄÇÊàëÂÄëÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄëÂ∑•‰ΩúÁöÑÂâµÊñ∞‰πãËôïÂú®ÊñºÂÖ©ÈÉ®ÂàÜÔºö1. ÊàëÂÄëÊèê‰æõ LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ‰ª•ÊîπÂñÑÁèæÊúâÊºîÁÆóÊ≥ïÊïàËÉΩÁöÑÂèØËÉΩÊÄß‰πãÊñ∞ËßÄÈªûÔºõ2. ÊàëÂÄëÁéáÂÖàË®éË´ñ GNN Ëß£ÈáãÂïèÈ°å‰∏≠ÁöÑÂ≠∏ÁøíÂÅèÂ∑ÆÂïèÈ°å„ÄÇ

##### **Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering with Large Language Models**
2407.15346v1 by Wenbin An, Feng Tian, Jiahao Nie, Wenkai Shi, Haonan Lin, Yan Chen, QianYing Wang, Yaqiang Wu, Guang Dai, Ping Chen

Knowledge-based Visual Question Answering (KVQA) requires both image and
world knowledge to answer questions. Current methods first retrieve knowledge
from the image and external knowledge base with the original complex question,
then generate answers with Large Language Models (LLMs). However, since the
original question contains complex elements that require knowledge from
different sources, acquiring different kinds of knowledge in a coupled manner
may confuse models and hinder them from retrieving precise knowledge.
Furthermore, the ``forward-only'' answering process fails to explicitly capture
the knowledge needs of LLMs, which can further hurt answering quality. To cope
with the above limitations, we propose DKA: Disentangled Knowledge Acquisition
from LLM feedback, a training-free framework that disentangles knowledge
acquisition to avoid confusion and uses LLM's feedback to specify the required
knowledge. Specifically, DKA requires LLMs to specify what knowledge they need
to answer the question and decompose the original complex question into two
simple sub-questions: Image-based sub-question and Knowledge-based
sub-question. Then we use the two sub-questions to retrieve knowledge from the
image and knowledge base, respectively. In this way, two knowledge acquisition
models can focus on the content that corresponds to them and avoid disturbance
of irrelevant elements in the original complex question, which can help to
provide more precise knowledge and better align the knowledge needs of LLMs to
yield correct answers. Experiments on benchmark datasets show that DKA
significantly outperforms SOTA models. To facilitate future research, our data
and code are available at \url{https://github.com/Lackel/DKA}.

ÊëòË¶ÅÔºöÂü∫ÊñºÁü•Ë≠òÁöÑË¶ñË¶∫ÂïèÁ≠î (KVQA) ÈúÄË¶ÅÂΩ±ÂÉèÂíå‰∏ñÁïåÁü•Ë≠òÊâçËÉΩÂõûÁ≠îÂïèÈ°å„ÄÇÁõÆÂâçÁöÑÂÅöÊ≥ïÊòØÂÖàÁî®ÂéüÂßãÁöÑË§áÈõúÂïèÈ°åÂæûÂΩ±ÂÉèÂíåÂ§ñÈÉ®Áü•Ë≠òÂ∫´‰∏≠Êì∑ÂèñÁü•Ë≠òÔºåÁÑ∂Âæå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî¢ÁîüÁ≠îÊ°à„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂéüÂßãÂïèÈ°åÂåÖÂê´ÈúÄË¶Å‰æÜËá™‰∏çÂêå‰æÜÊ∫êÁöÑÁü•Ë≠òÁöÑË§áÈõúÂÖÉÁ¥†Ôºå‰ª•ÁµêÂêàÁöÑÊñπÂºèÁç≤Âèñ‰∏çÂêåÈ°ûÂûãÁöÑÁü•Ë≠òÂèØËÉΩÊúÉËÆìÊ®°ÂûãÊÑüÂà∞Âõ∞ÊÉëÔºå‰∏¶ÈòªÁ§ôÂÆÉÂÄëÊì∑ÂèñÁ≤æÁ¢∫ÁöÑÁü•Ë≠ò„ÄÇÊ≠§Â§ñÔºå„ÄåÂÉÖÂâçÂêë„ÄçÁöÑÂõûÁ≠îÈÅéÁ®ãÁÑ°Ê≥ïÊòéÁ¢∫ÊçïÊçâ LLM ÁöÑÁü•Ë≠òÈúÄÊ±ÇÔºåÈÄôÂèØËÉΩÊúÉÈÄ≤‰∏ÄÊ≠•ÊêçÂÆ≥ÂõûÁ≠îÂìÅË≥™„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç‰∏äËø∞ÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü DKAÔºöÂæû LLM ÂõûÈ•ã‰∏≠Ëß£ÈñãÁü•Ë≠òÁç≤ÂèñÔºåÈÄôÊòØ‰∏ÄÂÄãÁÑ°ÈúÄË®ìÁ∑¥ÁöÑÊû∂ÊßãÔºåÂÆÉËß£ÈñãÁü•Ë≠òÁç≤Âèñ‰ª•ÈÅøÂÖçÊ∑∑Ê∑ÜÔºå‰∏¶‰ΩøÁî® LLM ÁöÑÂõûÈ•ã‰æÜÊåáÂÆöÊâÄÈúÄÁöÑÁü•Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåDKA Ë¶ÅÊ±Ç LLM ÊåáÂÆö‰ªñÂÄëÈúÄË¶Å‰ªÄÈ∫ºÁü•Ë≠ò‰æÜÂõûÁ≠îÂïèÈ°åÔºå‰∏¶Â∞áÂéüÂßãÁöÑË§áÈõúÂïèÈ°åÂàÜËß£ÊàêÂÖ©ÂÄãÁ∞°ÂñÆÁöÑÂ≠êÂïèÈ°åÔºöÂü∫ÊñºÂΩ±ÂÉèÁöÑÂ≠êÂïèÈ°åÂíåÂü∫ÊñºÁü•Ë≠òÁöÑÂ≠êÂïèÈ°å„ÄÇÁÑ∂ÂæåÊàëÂÄë‰ΩøÁî®ÈÄôÂÖ©ÂÄãÂ≠êÂïèÈ°åÂàÜÂà•ÂæûÂΩ±ÂÉèÂíåÁü•Ë≠òÂ∫´‰∏≠Êì∑ÂèñÁü•Ë≠ò„ÄÇÈÄèÈÅéÈÄôÁ®ÆÊñπÂºèÔºåÂÖ©ÂÄãÁü•Ë≠òÁç≤ÂèñÊ®°ÂûãÂèØ‰ª•Â∞àÊ≥®ÊñºËàáÂÆÉÂÄëÂ∞çÊáâÁöÑÂÖßÂÆπÔºå‰∏¶ÈÅøÂÖçÂéüÂßãË§áÈõúÂïèÈ°å‰∏≠ÁÑ°ÈóúÂÖÉÁ¥†ÁöÑÂπ≤ÊìæÔºåÈÄôÊúâÂä©ÊñºÊèê‰æõÊõ¥Á≤æÁ¢∫ÁöÑÁü•Ë≠òÔºå‰∏¶Êõ¥Â•ΩÂú∞Ë™øÊï¥ LLM ÁöÑÁü•Ë≠òÈúÄÊ±Ç‰ª•Áî¢ÁîüÊ≠£Á¢∫ÁöÑÁ≠îÊ°à„ÄÇÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåDKA ÊòéÈ°ØÂÑ™Êñº SOTA Ê®°Âûã„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Êú™‰æÜÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® \url{https://github.com/Lackel/DKA} ÂèñÂæó„ÄÇ

##### **Improving Minimum Bayes Risk Decoding with Multi-Prompt**
2407.15343v1 by David Heineman, Yao Dou, Wei Xu

While instruction fine-tuned LLMs are effective text generators, sensitivity
to prompt construction makes performance unstable and sub-optimal in practice.
Relying on a single "best" prompt cannot capture all differing approaches to a
generation problem. Using this observation, we propose multi-prompt decoding,
where many candidate generations are decoded from a prompt bank at
inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)
decoding, which selects a final output using a trained value metric. We show
multi-prompt improves MBR across a comprehensive set of conditional generation
tasks, and show this is a result of estimating a more diverse and higher
quality candidate space than that of a single prompt. Further experiments
confirm multi-prompt improves generation across tasks, models and metrics.

ÊëòË¶ÅÔºöÈõñÁÑ∂Á∂ìÈÅéÂæÆË™øÁöÑ LLM ÊòØÊúâÊïàÁöÑÊñáÂ≠óÁîüÊàêÂô®Ôºå‰ΩÜÂ∞çÊèêÁ§∫ÁµêÊßãÁöÑÊïèÊÑüÊÄßÊúÉÂ∞éËá¥ÊïàËÉΩ‰∏çÁ©©ÂÆö‰∏îÂú®ÂØ¶Âãô‰∏ä‰ΩéÊñºÊúÄ‰Ω≥ÁãÄÊÖã„ÄÇ‰æùË≥¥ÂñÆ‰∏ÄÁöÑ„ÄåÊúÄ‰Ω≥„ÄçÊèêÁ§∫ÁÑ°Ê≥ïÊ∂µËìãÊâÄÊúâ‰∏çÂêåÁöÑÁîüÊàêÂïèÈ°åÊñπÊ≥ï„ÄÇÂà©Áî®Ê≠§ËßÄÂØüÁµêÊûúÔºåÊàëÂÄëÊèêÂá∫Â§öÊèêÁ§∫Ëß£Á¢ºÔºåÂÖ∂‰∏≠Âú®Êé®Ë´ñÊôÇÈñìÂæûÊèêÁ§∫Â∫´Ëß£Á¢ºÂá∫Ë®±Â§öÂÄôÈÅ∏ÁîüÊàê„ÄÇÁÇ∫‰∫ÜÁµÑÂêàÂÄôÈÅ∏È†ÖÔºåÊàëÂÄë‰ΩøÁî®ÊúÄÂ∞èË≤ùÊ∞èÈ¢®Èö™ (MBR) Ëß£Á¢ºÔºåÂÆÉ‰ΩøÁî®Ë®ìÁ∑¥ÂæåÁöÑÊï∏ÂÄºÊåáÊ®ô‰æÜÈÅ∏ÊìáÊúÄÁµÇËº∏Âá∫„ÄÇÊàëÂÄëÂ±ïÁ§∫Â§öÊèêÁ§∫Âú®ÂÖ®Èù¢ÁöÑÊ¢ù‰ª∂ÁîüÊàê‰ªªÂãô‰∏≠ÊîπÂñÑ MBRÔºå‰∏¶Â±ïÁ§∫ÈÄôÊòØ‰º∞Ë®àÊØîÂñÆ‰∏ÄÊèêÁ§∫Êõ¥ÁÇ∫Â§öÊ®£‰∏îÂìÅË≥™Êõ¥È´òÁöÑÂÄôÈÅ∏Á©∫ÈñìÁöÑÁµêÊûú„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÂØ¶È©óÁ¢∫Ë™çÂ§öÊèêÁ§∫ÊîπÂñÑ‰∫Ü‰∏çÂêå‰ªªÂãô„ÄÅÊ®°ÂûãÂíåÊåáÊ®ôÁöÑÁîüÊàê„ÄÇ

##### **ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis with Coarse-to-Fine In-context Learning**
2407.15341v1 by Senbin Zhu, Hanjie Zhao, Xingren Wang, Shanhong Liu, Yuxiang Jia, Hongying Zan

The DimABSA task requires fine-grained sentiment intensity prediction for
restaurant reviews, including scores for Valence and Arousal dimensions for
each Aspect Term. In this study, we propose a Coarse-to-Fine In-context
Learning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in
the SIGHAN 2024 workshop. Our method improves prediction accuracy through a
two-stage optimization process. In the first stage, we use fixed in-context
examples and prompt templates to enhance the model's sentiment recognition
capability and provide initial predictions for the test data. In the second
stage, we encode the Opinion field using BERT and select the most similar
training data as new in-context examples based on similarity. These examples
include the Opinion field and its scores, as well as related opinion words and
their average scores. By filtering for sentiment polarity, we ensure that the
examples are consistent with the test data. Our method significantly improves
prediction accuracy and consistency by effectively utilizing training data and
optimizing in-context examples, as validated by experimental results.

ÊëòË¶ÅÔºöDimABSA ‰ªªÂãôÈúÄË¶ÅÂ∞çÈ§êÂª≥Ë©ïË´ñÈÄ≤Ë°åÁ¥∞Á∑ªÁöÑÊÉÖÊÑüÂº∑Â∫¶È†êÊ∏¨ÔºåÂåÖÊã¨Â∞çÊØèÂÄãÈù¢ÂêëË°ìË™ûÁöÑÊÉÖÊÑüÂÉπÂíåÂñöÈÜíÁ∂≠Â∫¶ÁöÑË©ïÂàÜ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈáùÂ∞ç SIGHAN 2024 Á†îË®éÊúÉ‰∏≠ÁöÑ DimABSA ‰ªªÂãôÔºåÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÁôæÂ∑ù 2-7B Ê®°ÂûãÁöÑÁ≤óÂà∞Á¥∞ÊÉÖÂ¢ÉÂÖßÂ≠∏Áøí (CFICL) ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈÄèÈÅéÂÖ©ÈöéÊÆµÂÑ™ÂåñÁ®ãÂ∫è‰æÜÊèêÂçáÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÂú®Á¨¨‰∏ÄÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî®Âõ∫ÂÆöÁöÑÊÉÖÂ¢ÉÂÖßÁØÑ‰æãÂíåÊèêÁ§∫ÁØÑÊú¨Ôºå‰æÜÂ¢ûÂº∑Ê®°ÂûãÁöÑÊÉÖÊÑüËæ®Ë≠òËÉΩÂäõÔºå‰∏¶Êèê‰æõÊ∏¨Ë©¶Ë≥áÊñôÁöÑÂàùÂßãÈ†êÊ∏¨„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî® BERT Â∞çÊÑèË¶ãÊ¨Ñ‰ΩçÈÄ≤Ë°åÁ∑®Á¢ºÔºå‰∏¶Ê†πÊìöÁõ∏‰ººÂ∫¶ÔºåÂæûË®ìÁ∑¥Ë≥áÊñô‰∏≠ÈÅ∏Âá∫ÊúÄÁõ∏‰ººÁöÑË≥áÊñô‰ΩúÁÇ∫Êñ∞ÁöÑÊÉÖÂ¢ÉÂÖßÁØÑ‰æã„ÄÇÈÄô‰∫õÁØÑ‰æãÂåÖÊã¨ÊÑèË¶ãÊ¨Ñ‰ΩçÂèäÂÖ∂Ë©ïÂàÜÔºå‰ª•ÂèäÁõ∏ÈóúÁöÑÊÑèË¶ãË©ûÂíåÂÖ∂Âπ≥ÂùáË©ïÂàÜ„ÄÇÈÄèÈÅéÂ∞çÊÉÖÊÑüÊ•µÊÄßÈÄ≤Ë°åÈÅéÊøæÔºåÊàëÂÄëÁ¢∫‰øùÈÄô‰∫õÁØÑ‰æãËàáÊ∏¨Ë©¶Ë≥áÊñô‰∏ÄËá¥„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈÄèÈÅéÊúâÊïàÂà©Áî®Ë®ìÁ∑¥Ë≥áÊñôÂíåÊúÄ‰Ω≥ÂåñÊÉÖÂ¢ÉÂÖßÁØÑ‰æãÔºåÂ§ßÂπÖÊèêÂçáÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶Âíå‰∏ÄËá¥ÊÄßÔºåÂØ¶È©óÁµêÊûú‰πüË≠âÂØ¶‰∫ÜÈÄô‰∏ÄÈªû„ÄÇ

##### **Deep Learning for Economists**
2407.15339v1 by Melissa Dell

Deep learning provides powerful methods to impute structured information from
large-scale, unstructured text and image datasets. For example, economists
might wish to detect the presence of economic activity in satellite images, or
to measure the topics or entities mentioned in social media, the congressional
record, or firm filings. This review introduces deep neural networks, covering
methods such as classifiers, regression models, generative AI, and embedding
models. Applications include classification, document digitization, record
linkage, and methods for data exploration in massive scale text and image
corpora. When suitable methods are used, deep learning models can be cheap to
tune and can scale affordably to problems involving millions or billions of
data points.. The review is accompanied by a companion website, EconDL, with
user-friendly demo notebooks, software resources, and a knowledge base that
provides technical details and additional applications.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÊèê‰æõÂº∑Â§ßÁöÑÊñπÊ≥ïÔºåÂæûÂ§ßË¶èÊ®°„ÄÅÈùûÁµêÊßãÂåñÊñáÂ≠óÂíåÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏≠Ëº∏ÂÖ•ÁµêÊßãÂåñË≥áË®ä„ÄÇ‰æãÂ¶ÇÔºåÁ∂ìÊøüÂ≠∏ÂÆ∂ÂèØËÉΩÂ∏åÊúõÂú®Ë°õÊòüÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨Á∂ìÊøüÊ¥ªÂãïÁöÑÂ≠òÂú®ÔºåÊàñÊ∏¨ÈáèÁ§æÁæ§Â™íÈ´î„ÄÅÂúãÊúÉË®òÈåÑÊàñÂÖ¨Âè∏Ê™îÊ°à‰∏≠ÊâÄÊèêÂà∞ÁöÑ‰∏ªÈ°åÊàñÂØ¶È´î„ÄÇÈÄôÁØáË©ïË´ñ‰ªãÁ¥π‰∫ÜÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÊ∂µËìãÂàÜÈ°ûÂô®„ÄÅËø¥Ê≠∏Ê®°Âûã„ÄÅÁîüÊàêÂºè AI ÂíåÂµåÂÖ•Ê®°ÂûãÁ≠âÊñπÊ≥ï„ÄÇÊáâÁî®ÂåÖÊã¨ÂàÜÈ°û„ÄÅÊñá‰ª∂Êï∏‰ΩçÂåñ„ÄÅË®òÈåÑÈÄ£ÁµêÔºå‰ª•ÂèäÂú®Â§ßÈáèÊñáÂ≠óÂíåÂΩ±ÂÉèË™ûÊñôÂ∫´‰∏≠ÈÄ≤Ë°åË≥áÊñôÊé¢ÂãòÁöÑÊñπÊ≥ï„ÄÇÁï∂‰ΩøÁî®ÂêàÈÅ©ÁöÑÊñπÊ≥ïÊôÇÔºåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂèØ‰ª•‰æøÂÆúÂú∞Ë™øÊï¥Ôºå‰∏¶‰∏îÂèØ‰ª•Ë≤†ÊìîÂæóËµ∑Âú∞Êì¥Â±ïÂà∞Ê∂âÂèäÊï∏ÁôæËê¨ÊàñÊï∏ÂçÅÂÑÑÂÄãË≥áÊñôÈªûÁöÑÂïèÈ°å„ÄÇÈÄôÁØáË©ïË´ñÈôÑÂ∏∂‰∏ÄÂÄãÈÖçÂ•óÁ∂≤Á´ô EconDLÔºåÂÖ∂‰∏≠ÂåÖÂê´‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÁ§∫ÁØÑÁ≠ÜË®òÊú¨„ÄÅËªüÈ´îË≥áÊ∫êÂíå‰∏ÄÂÄãÊèê‰æõÊäÄË°ìÁ¥∞ÁØÄÂíåÈ°çÂ§ñÊáâÁî®Á®ãÂºèÁöÑÁü•Ë≠òÂ∫´„ÄÇ

##### **Robust personalized pricing under uncertainty of purchase probabilities**
2407.15332v1 by Shunnosuke Ikeda, Naoki Nishimura, Noriyoshi Sukegawa, Yuichi Takano

This paper is concerned with personalized pricing models aimed at maximizing
the expected revenues or profits for a single item. While it is essential for
personalized pricing to predict the purchase probabilities for each consumer,
these predicted values are inherently subject to unavoidable errors that can
negatively impact the realized revenues and profits. To address this issue, we
focus on robust optimization techniques that yield reliable solutions to
optimization problems under uncertainty. Specifically, we propose a robust
optimization model for personalized pricing that accounts for the uncertainty
of predicted purchase probabilities. This model can be formulated as a
mixed-integer linear optimization problem, which can be solved exactly using
mathematical optimization solvers. We also develop a Lagrangian decomposition
algorithm combined with line search to efficiently find high-quality solutions
for large-scale optimization problems. Experimental results demonstrate the
effectiveness of our robust optimization model and highlight the utility of our
Lagrangian decomposition algorithm in terms of both computational efficiency
and solution quality.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢ËÆ®‰∫ÜÊó®Âú®ÊúÄÂ§ßÂåñÂçï‰∏™ÂïÜÂìÅÈ¢ÑÊúüÊî∂ÂÖ•ÊàñÂà©Ê∂¶ÁöÑ‰∏™ÊÄßÂåñÂÆö‰ª∑Ê®°Âûã„ÄÇËôΩÁÑ∂‰∏™ÊÄßÂåñÂÆö‰ª∑ÂØπ‰∫éÈ¢ÑÊµãÊØè‰∏™Ê∂àË¥πËÄÖÁöÑË¥≠‰π∞Ê¶ÇÁéáËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜËøô‰∫õÈ¢ÑÊµãÂÄºÊú¨Ë¥®‰∏ä‰ºöÂèóÂà∞‰∏çÂèØÈÅøÂÖçÁöÑÈîôËØØÂΩ±ÂìçÔºåËÄåËøô‰∫õÈîôËØØÂèØËÉΩ‰ºöÂØπÂÆûÁé∞ÁöÑÊî∂ÂÖ•ÂíåÂà©Ê∂¶‰∫ßÁîüË¥üÈù¢ÂΩ±Âìç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰∏ìÊ≥®‰∫éÁ®≥ÂÅ•‰ºòÂåñÊäÄÊúØÔºåËØ•ÊäÄÊúØÂú®‰∏çÁ°ÆÂÆöÊÄß‰∏ã‰∏∫‰ºòÂåñÈóÆÈ¢òÊèê‰æõ‰∫ÜÂèØÈù†ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Á®≥ÂÅ•‰ºòÂåñÊ®°ÂûãÔºåÁî®‰∫é‰∏™ÊÄßÂåñÂÆö‰ª∑ÔºåËØ•Ê®°ÂûãËÄÉËôë‰∫ÜÈ¢ÑÊµãË¥≠‰π∞Ê¶ÇÁéáÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•Ë°®Ëø∞‰∏∫Ê∑∑ÂêàÊï¥Êï∞Á∫øÊÄß‰ºòÂåñÈóÆÈ¢òÔºåÂèØ‰ª•‰ΩøÁî®Êï∞Â≠¶‰ºòÂåñÊ±ÇËß£Âô®Á≤æÁ°ÆÊ±ÇËß£„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏ÄÁßçÊãâÊ†ºÊúóÊó•ÂàÜËß£ÁÆóÊ≥ïÔºåÁªìÂêàÁ∫øÊêúÁ¥¢Ôºå‰ª•ÊúâÊïàÂú∞‰∏∫Â§ßËßÑÊ®°‰ºòÂåñÈóÆÈ¢òÊâæÂà∞È´òË¥®ÈáèÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊàë‰ª¨Á®≥ÂÅ•‰ºòÂåñÊ®°ÂûãÁöÑÊúâÊïàÊÄßÔºåÂπ∂‰ªéËÆ°ÁÆóÊïàÁéáÂíåËß£ÂÜ≥ÊñπÊ°àË¥®Èáè‰∏§ÊñπÈù¢Á™ÅÂá∫‰∫ÜÊàë‰ª¨ÊãâÊ†ºÊúóÊó•ÂàÜËß£ÁÆóÊ≥ïÁöÑÊïàÁî®„ÄÇ

##### **Odyssey: Empowering Agents with Open-World Skills**
2407.15325v1 by Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, Mingli Song

Recent studies have delved into constructing generalist agents for open-world
embodied environments like Minecraft. Despite the encouraging results, existing
efforts mainly focus on solving basic programmatic tasks, e.g., material
collection and tool-crafting following the Minecraft tech-tree, treating the
ObtainDiamond task as the ultimate goal. This limitation stems from the
narrowly defined set of actions available to agents, requiring them to learn
effective long-horizon strategies from scratch. Consequently, discovering
diverse gameplay opportunities in the open world becomes challenging. In this
work, we introduce ODYSSEY, a new framework that empowers Large Language Model
(LLM)-based agents with open-world skills to explore the vast Minecraft world.
ODYSSEY comprises three key parts: (1) An interactive agent with an open-world
skill library that consists of 40 primitive skills and 183 compositional
skills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering
dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A
new open-world benchmark includes thousands of long-term planning tasks, tens
of dynamic-immediate planning tasks, and one autonomous exploration task.
Extensive experiments demonstrate that the proposed ODYSSEY framework can
effectively evaluate the planning and exploration capabilities of agents. All
datasets, model weights, and code are publicly available to motivate future
research on more advanced autonomous agent solutions.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÁ†îÁ©∂Ê∑±ÂÖ•Êé¢Ë®é‰∫ÜÁÇ∫ Minecraft Á≠âÈñãÊîæ‰∏ñÁïåÂÖ∑Ë∫´Áí∞Â¢ÉÂª∫ÊßãÈÄöÊâç‰ª£ÁêÜ„ÄÇÂÑòÁÆ°ÁµêÊûú‰ª§‰∫∫ÈºìËàûÔºåÁèæÊúâÁöÑÂä™Âäõ‰∏ªË¶ÅÈõÜ‰∏≠ÊñºËß£Ê±∫Âü∫Êú¨ÁöÑÁ®ãÂºèÂåñ‰ªªÂãôÔºå‰æãÂ¶ÇÊùêÊñôÊî∂ÈõÜÂíåÊåâÁÖß Minecraft ÊäÄË°ìÊ®πË£Ω‰ΩúÂ∑•ÂÖ∑Ôºå‰∏¶Â∞áÂèñÂæóÈëΩÁü≥‰ªªÂãôË¶ñÁÇ∫ÊúÄÁµÇÁõÆÊ®ô„ÄÇÈÄôÁ®ÆÈôêÂà∂Ê∫êÊñº‰ª£ÁêÜÂèØÁî®ÁöÑÂãï‰ΩúÈõÜÂÆöÁæ©ÁãπÁ™ÑÔºåË¶ÅÊ±ÇÂÆÉÂÄëÂæûÈ†≠Â≠∏ÁøíÊúâÊïàÁöÑÈï∑ÊôÇÁ®ãÁ≠ñÁï•„ÄÇÂõ†Ê≠§ÔºåÂú®ÈñãÊîæ‰∏ñÁïå‰∏≠ÁôºÁèæÂ§öÊ®£ÂåñÁöÑÈÅäÊà≤Ê©üÊúÉËÆäÂæóÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü ODYSSEYÔºå‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂÆÉË≥¶‰∫àÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ‰ª£ÁêÜÂÖ∑ÂÇôÊé¢Á¥¢Âª£Èóä Minecraft ‰∏ñÁïåÁöÑÈñãÊîæ‰∏ñÁïåÊäÄËÉΩ„ÄÇODYSSEY ÂåÖÂê´‰∏âÂÄãÈóúÈçµÈÉ®ÂàÜÔºö(1) ÂÖ∑ÊúâÈñãÊîæ‰∏ñÁïåÊäÄËÉΩÂ∫´ÁöÑ‰∫íÂãï‰ª£ÁêÜÔºåÂåÖÂê´ 40 ÂÄãÂü∫Êú¨ÊäÄËÉΩÂíå 183 ÂÄãÁµÑÂêàÊäÄËÉΩ„ÄÇ(2) Âú®Â§ßÂûãÂïèÁ≠îË≥áÊñôÈõÜ‰∏äÂæÆË™øÁöÑ LLaMA-3 Ê®°ÂûãÔºåÂÖ∂‰∏≠ÂåÖÂê´Âæû Minecraft Wiki Ë°çÁîüÁöÑ 390,000 Â§öÂÄãÊåá‰ª§Ê¢ùÁõÆ„ÄÇ(3) ‰∏ÄÂÄãÊñ∞ÁöÑÈñãÊîæ‰∏ñÁïåÂü∫Ê∫ñÂåÖÊã¨Êï∏ÂçÉÂÄãÈï∑ÊúüË¶èÂäÉ‰ªªÂãô„ÄÅÊï∏ÂçÅÂÄãÂãïÊÖãÂç≥ÊôÇË¶èÂäÉ‰ªªÂãôÂíå‰∏ÄÂÄãËá™‰∏ªÊé¢Á¥¢‰ªªÂãô„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑ ODYSSEY Ê°ÜÊû∂ÂèØ‰ª•ÊúâÊïàË©ï‰º∞‰ª£ÁêÜÁöÑË¶èÂäÉÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇÊâÄÊúâË≥áÊñôÈõÜ„ÄÅÊ®°ÂûãÊ¨äÈáçÂíåÁ®ãÂºèÁ¢ºÈÉΩÊòØÂÖ¨ÈñãÁöÑÔºå‰ª•ÊøÄÂãµÊú™‰æÜÂ∞çÊõ¥ÂÖàÈÄ≤ÁöÑËá™‰∏ª‰ª£ÁêÜËß£Ê±∫ÊñπÊ°àÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **FMDNN: A Fuzzy-guided Multi-granular Deep Neural Network for Histopathological Image Classification**
2407.15312v1 by Weiping Ding, Tianyi Zhou, Jiashuang Huang, Shu Jiang, Tao Hou, Chin-Teng Lin

Histopathological image classification constitutes a pivotal task in
computer-aided diagnostics. The precise identification and categorization of
histopathological images are of paramount significance for early disease
detection and treatment. In the diagnostic process of pathologists, a
multi-tiered approach is typically employed to assess abnormalities in cell
regions at different magnifications. However, feature extraction is often
performed at a single granularity, overlooking the multi-granular
characteristics of cells. To address this issue, we propose the Fuzzy-guided
Multi-granularity Deep Neural Network (FMDNN). Inspired by the multi-granular
diagnostic approach of pathologists, we perform feature extraction on cell
structures at coarse, medium, and fine granularity, enabling the model to fully
harness the information in histopathological images. We incorporate the theory
of fuzzy logic to address the challenge of redundant key information arising
during multi-granular feature extraction. Cell features are described from
different perspectives using multiple fuzzy membership functions, which are
fused to create universal fuzzy features. A fuzzy-guided cross-attention module
guides universal fuzzy features toward multi-granular features. We propagate
these features through an encoder to all patch tokens, aiming to achieve
enhanced classification accuracy and robustness. In experiments on multiple
public datasets, our model exhibits a significant improvement in accuracy over
commonly used classification methods for histopathological image classification
and shows commendable interpretability.

ÊëòË¶ÅÔºöÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèÂàÜÈ°ûÂú®ÈõªËÖ¶ËºîÂä©Ë®∫Êñ∑‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèÁöÑÁ≤æÁ¢∫Ëæ®Ë≠òÂíåÂàÜÈ°ûÂ∞çÊñºÊó©ÊúüÁñæÁóÖÂÅµÊ∏¨ÂíåÊ≤ªÁôÇËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÁóÖÁêÜÂ≠∏ÂÆ∂ÁöÑË®∫Êñ∑ÈÅéÁ®ã‰∏≠ÔºåÈÄöÂ∏∏Êé°Áî®Â§öÂ±§Á¥öÊñπÊ≥ï‰æÜË©ï‰º∞‰∏çÂêåÊîæÂ§ßÂÄçÁéá‰∏ãÁ¥∞ËÉûÂçÄÂüüÁöÑÁï∞Â∏∏„ÄÇÁÑ∂ËÄåÔºåÁâπÂæµËêÉÂèñÈÄöÂ∏∏Âú®ÂñÆ‰∏ÄÈ°ÜÁ≤íÂ∫¶‰∏ãÂü∑Ë°åÔºåÂøΩÁï•‰∫ÜÁ¥∞ËÉûÁöÑÂ§öÈ°ÜÁ≤íÁâπÂæµ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊ®°Á≥äÂ∞éÂºïÂ§öÈ°ÜÁ≤íÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (FMDNN)„ÄÇÂèóÂà∞ÁóÖÁêÜÂ≠∏ÂÆ∂Â§öÈ°ÜÁ≤íË®∫Êñ∑ÊñπÊ≥ïÁöÑÂïüÁôºÔºåÊàëÂÄëÂ∞çÁ≤ó„ÄÅ‰∏≠„ÄÅÁ¥∞È°ÜÁ≤íÂ∫¶ÁöÑÁ¥∞ËÉûÁµêÊßãÂü∑Ë°åÁâπÂæµËêÉÂèñÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†ÂÖÖÂàÜÂà©Áî®ÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉè‰∏≠ÁöÑË≥áË®ä„ÄÇÊàëÂÄëÁµêÂêàÊ®°Á≥äÈÇèËºØÁêÜË´ñ‰æÜËß£Ê±∫Â§öÈ°ÜÁ≤íÁâπÂæµËêÉÂèñÈÅéÁ®ã‰∏≠ÂÜóÈ§òÈóúÈçµË≥áË®äÁöÑÊåëÊà∞„ÄÇÁ¥∞ËÉûÁâπÂæµ‰ΩøÁî®Â§öÂÄãÊ®°Á≥äÈö∏Â±¨ÂáΩÊï∏Âæû‰∏çÂêåËßíÂ∫¶ÊèèËø∞Ôºå‰∏¶ËûçÂêà‰ª•Âª∫Á´ãÈÄöÁî®Ê®°Á≥äÁâπÂæµ„ÄÇÊ®°Á≥äÂ∞éÂºï‰∫§ÂèâÊ≥®ÊÑèÊ®°ÁµÑÂºïÂ∞éÈÄöÁî®Ê®°Á≥äÁâπÂæµËµ∞ÂêëÂ§öÈ°ÜÁ≤íÁâπÂæµ„ÄÇÊàëÂÄëÈÄèÈÅéÁ∑®Á¢ºÂô®Â∞áÈÄô‰∫õÁâπÂæµÂÇ≥Êí≠Âà∞ÊâÄÊúâ patch Ê®ôË®òÔºåÊó®Âú®ÊèêÈ´òÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÂíåÁ©©ÂÅ•ÊÄß„ÄÇÂú®Â§öÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫Â∫¶ÊñπÈù¢Ë°®ÁèæÂá∫È°ØËëóÁöÑÊèêÂçáÔºå‰∏¶Â±ïÁèæÂá∫ÂÄºÂæóËÆöË≥ûÁöÑÂèØËß£ÈáãÊÄß„ÄÇ

##### **Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection**
2407.15296v1 by Kwanyong Park, Kuniaki Saito, Donghyun Kim

Vision-language (VL) models often exhibit a limited understanding of complex
expressions of visual objects (e.g., attributes, shapes, and their relations),
given complex and diverse language queries. Traditional approaches attempt to
improve VL models using hard negative synthetic text, but their effectiveness
is limited. In this paper, we harness the exceptional compositional
understanding capabilities of generative foundational models. We introduce a
novel method for structured synthetic data generation aimed at enhancing the
compositional understanding of VL models in language-based object detection.
Our framework generates densely paired positive and negative triplets (image,
text descriptions, and bounding boxes) in both image and text domains. By
leveraging these synthetic triplets, we transform 'weaker' VL models into
'stronger' models in terms of compositional understanding, a process we call
"Weak-to-Strong Compositional Learning" (WSCL). To achieve this, we propose a
new compositional contrastive learning formulation that discovers semantics and
structures in complex descriptions from synthetic triplets. As a result, VL
models trained with our synthetic data generation exhibit a significant
performance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark
by +6.9AP upon existing baselines.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®Ä (VL) Ê®°ÂûãÂæÄÂæÄÂ∞çË¶ñË¶∫Áâ©‰ª∂ÁöÑË§áÈõúË°®ÈÅîÔºà‰æãÂ¶ÇÔºåÂ±¨ÊÄß„ÄÅÂΩ¢ÁãÄÂèäÂÖ∂Èóú‰øÇÔºâÁêÜËß£ÊúâÈôêÔºåÂõ†ÁÇ∫Ë™ûË®ÄÊü•Ë©¢Ë§áÈõú‰∏îÂ§öÊ®£„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÂòóË©¶‰ΩøÁî®Èõ£‰ª•Âê¶ÂÆöÁöÑÂêàÊàêÊñáÂ≠ó‰æÜÊîπÂñÑ VL Ê®°ÂûãÔºå‰ΩÜÂÖ∂ÊïàÊûúÊúâÈôê„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂà©Áî®ÁîüÊàêÂü∫Á§éÊ®°ÂûãÁöÑÂçìË∂äÁµÑÂêàÁêÜËß£ËÉΩÂäõ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÁî®ÊñºÁµêÊßãÂåñÂêàÊàêË≥áÊñôÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∑ VL Ê®°ÂûãÂú®Âü∫ÊñºË™ûË®ÄÁöÑÁâ©‰ª∂ÂÅµÊ∏¨‰∏≠ÁöÑÁµÑÂêàÁêÜËß£„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®ÂΩ±ÂÉèÂíåÊñáÂ≠óÈ†òÂüü‰∏≠Áî¢ÁîüÂØÜÈõÜÈÖçÂ∞çÁöÑÊ≠£Ë≤†‰∏âÂÖÉÁµÑÔºàÂΩ±ÂÉè„ÄÅÊñáÂ≠óÊèèËø∞ÂíåÈÇäÁïåÊ°ÜÔºâ„ÄÇÈÄèÈÅéÂà©Áî®ÈÄô‰∫õÂêàÊàê‰∏âÂÖÉÁµÑÔºåÊàëÂÄëÂ∞á„ÄåËºÉÂº±„ÄçÁöÑ VL Ê®°ÂûãËΩâËÆäÁÇ∫Âú®ÁµÑÂêàÁêÜËß£ÊñπÈù¢„ÄåËºÉÂº∑„ÄçÁöÑÊ®°ÂûãÔºåÈÄôÂÄãÈÅéÁ®ãÊàëÂÄëÁ®±‰πãÁÇ∫„ÄåÂº±Âà∞Âº∑ÁöÑÁµÑÂêàÂ≠∏Áøí„Äç(Weak-to-Strong Compositional Learning, WSCL)„ÄÇÁÇ∫ÈÅîÊàêÊ≠§ÁõÆÁöÑÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÁµÑÂêàÂ∞çÊØîÂ≠∏ÁøíÂÖ¨ÂºèÔºåÂæûÂêàÊàê‰∏âÂÖÉÁµÑ‰∏≠ÁôºÁèæË§áÈõúÊèèËø∞‰∏≠ÁöÑË™ûÁæ©ÂíåÁµêÊßã„ÄÇÂõ†Ê≠§Ôºå‰ΩøÁî®ÊàëÂÄëÁöÑÂêàÊàêË≥áÊñôÁîüÊàêË®ìÁ∑¥ÁöÑ VL Ê®°ÂûãÂú® Omnilabel Âü∫Ê∫ñ‰∏äË°®ÁèæÊèêÂçáÈ°ØËëóÔºåÈÅî +5APÔºåÂú® D3 Âü∫Ê∫ñ‰∏äÂâáÊèêÂçá +6.9APÔºåÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñ„ÄÇ

##### **Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis**
2407.15286v1 by Guangliang Liu, Haitao Mao, Jiliang Tang, Kristen Marie Johnson

Large Language Models (LLMs) are capable of producing content that
perpetuates stereotypes, discrimination, and toxicity. The recently proposed
moral self-correction is a computationally efficient method for reducing
harmful content in the responses of LLMs. However, the process of how injecting
self-correction instructions can modify the behavior of LLMs remains
under-explored. In this paper, we explore the effectiveness of moral
self-correction by answering three research questions: (1) In what scenarios
does moral self-correction work? (2) What are the internal mechanisms of LLMs,
e.g., hidden states, that are influenced by moral self-correction instructions?
(3) Is intrinsic moral self-correction actually superficial? We argue that
self-correction can help LLMs find a shortcut to more morally correct output,
rather than truly reducing the immorality stored in hidden states. Through
empirical investigation with tasks of language generation and multi-choice
question answering, we conclude: (i) LLMs exhibit good performance across both
tasks, and self-correction instructions are particularly beneficial when the
correct answer is already top-ranked; (ii) The morality levels in intermediate
hidden states are strong indicators as to whether one instruction would be more
effective than another; (iii) Based on our analysis of intermediate hidden
states and task case studies of self-correction behaviors, we are first to
propose the hypothesis that intrinsic moral self-correction is in fact
superficial.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÁî¢ÁîüÂª∂Á∫åÂàªÊùøÂç∞Ë±°„ÄÅÊ≠ßË¶ñÂíåÊØíÊÄßÁöÑÂÖßÂÆπ„ÄÇÊúÄËøëÊèêÂá∫ÁöÑÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£ÊòØ‰∏ÄÁ®ÆË®àÁÆóÊúâÊïàÁéáÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÊ∏õÂ∞ë LLM ÂõûÊáâ‰∏≠ÁöÑÊúâÂÆ≥ÂÖßÂÆπ„ÄÇÁÑ∂ËÄåÔºåÊ≥®ÂÖ•Ëá™Êàë‰øÆÊ≠£Êåá‰ª§Â¶Ç‰Ωï‰øÆÊîπ LLM Ë°åÁÇ∫ÁöÑÈÅéÁ®ã‰ªçÊú™ÂÖÖÂàÜÊé¢Ë®é„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂõûÁ≠î‰∏âÂÄãÁ†îÁ©∂ÂïèÈ°å‰æÜÊé¢Ë®éÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£ÁöÑÊúâÊïàÊÄßÔºö(1) Âú®Âì™‰∫õÊÉÖÊ≥Å‰∏ãÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£ÊúâÊïàÔºü(2) LLM ÁöÑÂì™‰∫õÂÖßÈÉ®Ê©üÂà∂Ôºà‰æãÂ¶ÇÈö±ËóèÁãÄÊÖãÔºâÂèóÂà∞ÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£Êåá‰ª§ÁöÑÂΩ±ÈüøÔºü(3) ÂÖßÂú®ÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£ÊòØÂê¶ÁúüÁöÑÂè™ÊòØË°®Èù¢ÂäüÂ§´ÔºüÊàëÂÄëË™çÁÇ∫Ëá™Êàë‰øÆÊ≠£ÂèØ‰ª•Âπ´Âä© LLM ÊâæÂà∞Êç∑Âæë‰ª•Áî¢ÁîüÊõ¥Á¨¶ÂêàÈÅìÂæ∑ÁöÑËº∏Âá∫ÔºåËÄå‰∏çÊòØÁúüÊ≠£Ê∏õÂ∞ëÂÑ≤Â≠òÂú®Èö±ËóèÁãÄÊÖã‰∏≠ÁöÑ‰∏çÈÅìÂæ∑ÊÄß„ÄÇÈÄèÈÅéË™ûË®ÄÁîüÊàêÂíåÂ§öÈÅ∏È°åÁ≠îÈ°å‰ªªÂãôÁöÑÂØ¶Ë≠âË™øÊü•ÔºåÊàëÂÄëÂæóÂá∫‰ª•‰∏ãÁµêË´ñÔºö(i) LLM Âú®ÈÄôÂÖ©ÂÄã‰ªªÂãô‰∏≠ÈÉΩË°®ÁèæËâØÂ•ΩÔºåËÄå‰∏îÁï∂Ê≠£Á¢∫Á≠îÊ°àÂ∑≤ÂêçÂàóÂâçËåÖÊôÇÔºåËá™Êàë‰øÆÊ≠£Êåá‰ª§ÁâπÂà•ÊúâÁõäÔºõ(ii) ‰∏≠ÈñìÈö±ËóèÁãÄÊÖã‰∏≠ÁöÑÈÅìÂæ∑Ê∞¥Âπ≥ÊòØÂº∑ËÄåÊúâÂäõÁöÑÊåáÊ®ôÔºåÂèØ‰ª•Âà§Êñ∑‰∏ÄÂÄãÊåá‰ª§ÊòØÂê¶ÊØîÂè¶‰∏ÄÂÄãÊåá‰ª§Êõ¥ÊúâÊïàÔºõ(iii) Ê†πÊìöÊàëÂÄëÂ∞ç‰∏≠ÈñìÈö±ËóèÁãÄÊÖãÂíåËá™Êàë‰øÆÊ≠£Ë°åÁÇ∫ÁöÑ‰ªªÂãôÊ°à‰æãÁ†îÁ©∂ÂàÜÊûêÔºåÊàëÂÄëÈ¶ñÊ¨°ÊèêÂá∫ÂÅáË®≠ÔºåÂç≥ÂÖßÂú®ÈÅìÂæ∑Ëá™Êàë‰øÆÊ≠£ÂØ¶Èöõ‰∏äÂè™ÊòØË°®Èù¢ÂäüÂ§´„ÄÇ

##### **Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms**
2407.15283v1 by Sheila Schoepp, Mehran Taghian, Shotaro Miwa, Yoshihiro Mitsuka, Shadan Golestan, Osmar Za√Øane

Industry is rapidly moving towards fully autonomous and interconnected
systems that can detect and adapt to changing conditions, including machine
hardware faults. Traditional methods for adding hardware fault tolerance to
machines involve duplicating components and algorithmically reconfiguring a
machine's processes when a fault occurs. However, the growing interest in
reinforcement learning-based robotic control offers a new perspective on
achieving hardware fault tolerance. However, limited research has explored the
potential of these approaches for hardware fault tolerance in machines. This
paper investigates the potential of two state-of-the-art reinforcement learning
algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), to
enhance hardware fault tolerance into machines. We assess the performance of
these algorithms in two OpenAI Gym simulated environments, Ant-v2 and
FetchReach-v1. Robot models in these environments are subjected to six
simulated hardware faults. Additionally, we conduct an ablation study to
determine the optimal method for transferring an agent's knowledge, acquired
through learning in a normal (pre-fault) environment, to a (post-)fault
environment in a continual learning setting. Our results demonstrate that
reinforcement learning-based approaches can enhance hardware fault tolerance in
simulated machines, with adaptation occurring within minutes. Specifically, PPO
exhibits the fastest adaptation when retaining the knowledge within its models,
while SAC performs best when discarding all acquired knowledge. Overall, this
study highlights the potential of reinforcement learning-based approaches, such
as PPO and SAC, for hardware fault tolerance in machines. These findings pave
the way for the development of robust and adaptive machines capable of
effectively operating in real-world scenarios.

ÊëòË¶ÅÔºö<paragraph>Áî¢Ê•≠Ê≠£Âø´ÈÄüÊúùÂêëËÉΩÂÅµÊ∏¨ÂíåÈÅ©ÊáâËÆäÂãïÁãÄÊ≥ÅÁöÑÂÆåÂÖ®Ëá™‰∏ª‰∏îÁõ∏‰∫íÈÄ£Êé•ÁöÑÁ≥ªÁµ±ÈÇÅÈÄ≤ÔºåÂÖ∂‰∏≠ÂåÖÊã¨Ê©üÂô®Á°¨È´îÊïÖÈöú„ÄÇÂÇ≥Áµ±‰∏äÁÇ∫Ê©üÂô®Â¢ûÂä†Á°¨È´îÂÆπÈåØÁöÑÊñπÊ≥ïÔºåÂåÖÂê´Ë§áË£ΩÁµÑ‰ª∂Ôºå‰ª•ÂèäÂú®ÁôºÁîüÊïÖÈöúÊôÇ‰ª•ÊºîÁÆóÊ≥ïÊñπÂºèÈáçÊñ∞ÈÖçÁΩÆÊ©üÂô®ÁöÑÁ®ãÂ∫è„ÄÇÁÑ∂ËÄåÔºåÂ∞çÂº∑ÂåñÂ≠∏ÁøíÁÇ∫Âü∫Á§éÁöÑÊ©üÂô®‰∫∫ÊéßÂà∂ÁöÑËààË∂£Êó•ÁõäÂ¢ûÂä†ÔºåÊèê‰æõ‰∫ÜÈÅîÊàêÁ°¨È´îÂÆπÈåØÁöÑÊñ∞ËßÄÈªû„ÄÇÁÑ∂ËÄåÔºåÊúâÈôêÁöÑÁ†îÁ©∂Êé¢Ë®é‰∫ÜÈÄô‰∫õÊñπÊ≥ïÂú®Ê©üÂô®Á°¨È´îÂÆπÈåØÁöÑÊΩõÂäõ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÖ©Á®ÆÊúÄÂÖàÈÄ≤ÁöÑÂº∑ÂåñÂ≠∏ÁøíÊºîÁÆóÊ≥ïÔºåËøëÁ´ØÁ≠ñÁï•ÊúÄ‰Ω≥Âåñ (PPO) ÂíåËªüÊÄßÂãï‰Ωú-Ë©ïË´ñÂÆ∂ (SAC)ÔºåÂú®ÊèêÂçáÊ©üÂô®Á°¨È´îÂÆπÈåØÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄã OpenAI Gym Ê®°Êì¨Áí∞Â¢ÉÔºåAnt-v2 Âíå FetchReach-v1ÔºåË©ï‰º∞ÈÄô‰∫õÊºîÁÆóÊ≥ïÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÁí∞Â¢É‰∏≠ÁöÑÊ©üÂô®‰∫∫Ê®°ÂûãÊúÉÂèóÂà∞ÂÖ≠Á®ÆÊ®°Êì¨Á°¨È´îÊïÖÈöúÂΩ±Èüø„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÊ∂àËûçÁ†îÁ©∂‰ª•Ê±∫ÂÆöÂú®ÊåÅÁ∫åÂ≠∏ÁøíË®≠ÂÆö‰∏≠ÔºåÂ∞á‰ª£ÁêÜÁ®ãÂºèÂú®Ê≠£Â∏∏ÔºàÊïÖÈöúÂâçÔºâÁí∞Â¢É‰∏≠ÈÄèÈÅéÂ≠∏ÁøíÁç≤ÂæóÁöÑÁü•Ë≠òÔºåËΩâÁßªÂà∞ÔºàÊïÖÈöúÂæåÔºâÁí∞Â¢ÉÁöÑÊúÄ‰Ω≥ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂü∫ÊñºÂº∑ÂåñÂ≠∏ÁøíÁöÑÊñπÊ≥ïÂèØ‰ª•Âú®Ê®°Êì¨Ê©üÂô®‰∏≠ÊèêÂçáÁ°¨È´îÂÆπÈåØÔºåÈÅ©ÊáâÊúÉÂú®ÂπæÂàÜÈêòÂÖßÁôºÁîü„ÄÇÁâπÂà•ÊòØÔºåPPO Âú®ÂÖ∂Ê®°Âûã‰∏≠‰øùÁïôÁü•Ë≠òÊôÇÂ±ïÁèæÊúÄÂø´ÁöÑÈÅ©ÊáâÈÄüÂ∫¶ÔºåËÄå SAC Âú®Êç®Ê£ÑÊâÄÊúâÂ∑≤Áç≤ÂæóÁöÑÁü•Ë≠òÊôÇË°®ÁèæÊúÄ‰Ω≥„ÄÇÊï¥È´îËÄåË®ÄÔºåÈÄôÈ†ÖÁ†îÁ©∂Á™ÅÈ°Ø‰∫ÜÂü∫ÊñºÂº∑ÂåñÂ≠∏ÁøíÁöÑÊñπÊ≥ïÔºå‰æãÂ¶Ç PPO Âíå SACÔºåÂú®Ê©üÂô®Á°¨È´îÂÆπÈåØÁöÑÊΩõÂäõ„ÄÇÈÄô‰∫õÁôºÁèæÁÇ∫ÈñãÁôºËÉΩÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÊúâÊïàÈÅã‰ΩúÁöÑÂº∑ÂÅ•‰∏îÂÖ∑ÈÅ©ÊáâÊÄßÁöÑÊ©üÂô®Èã™Ë∑Ø„ÄÇ</paragraph>

##### **SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking**
2407.15281v1 by Kuan-Yen Lin

Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.

ÊëòË¶ÅÔºöÁêÜËß£Ë±êÂØåÁöÑÂ∞çË©±ÈÄöÂ∏∏ÈúÄË¶Å NLP Á≥ªÁµ±Â≠òÂèñÁõ∏ÈóúÁöÑÂ∏∏Ë≠ò‰∫∫Áâ©Áü•Ë≠òÔºå‰ΩÜÁî±ÊñºË§áÈõúÁöÑËÑàÁµ°ÂíåÂ∏∏Ë≠òÁöÑÈö±Âê´ÊÄßË≥™ÔºåÂõ†Ê≠§Êì∑ÂèñÊ≠§Áü•Ë≠òÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Êñá‰ªãÁ¥πÊàëÂÄëÂ∞çÂ∏∏Ë≠ò‰∫∫Áâ©Áü•Ë≠òÈÄ£Áµê (CPKL) ÊåëÊà∞ÁöÑÊñπÊ≥ïÔºåËß£Ê±∫Âú®ÈñãÊîæÈ†òÂüüÂ∞çË©±Á≥ªÁµ±‰∏≠Êï¥Âêà‰∫∫Áâ©ÂíåÂ∏∏Ë≠òÁü•Ë≠òÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇÊàëÂÄë‰ªãÁ¥π SynCPKL ÁÆ°Á∑öÔºåÈÄôÊòØ‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁÇ∫Ë®ìÁ∑¥Â∏∏Ë≠ò‰∫∫Áâ©Áü•Ë≠òÈÄ£ÁµêÂô®Áî¢ÁîüÈ´òÂìÅË≥™ÂêàÊàêË≥áÊñôÈõÜÁöÑÁÆ°Á∑ö„ÄÇÁÇ∫‰∫ÜË≠âÊòéÊàëÂÄëÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÊàëÂÄëÊèêÂá∫ SynCPKLÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÈñÄÁÇ∫Ê≠§‰ªªÂãôË®≠Ë®àÁöÑÊñ∞Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü SynCPKL Âú®Ë®ìÁ∑¥Â∏∏Ë≠ò‰∫∫Áâ©Áü•Ë≠òÈÄ£ÁµêÂô®ÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË°®ÁèæÊúÄ‰Ω≥ÁöÑÊ®°Âûã Derberta-SynCPKL Âú® CPKL ÊåëÊà∞‰∏≠Áç≤ÂæóÁ¨¨‰∏ÄÂêçÔºåF1 ÂàÜÊï∏ÊèêÂçá‰∫Ü 16%„ÄÇÊàëÂÄëÂú® https://github.com/irislin1006/CPKL ÁôºÂ∏É‰∫Ü SynCPKL Âíå Derberta-SynCPKL„ÄÇ

##### **Unifying Invariant and Variant Features for Graph Out-of-Distribution via Probability of Necessity and Sufficiency**
2407.15273v1 by Xuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang, Zhifeng Hao, Zijian Li

Graph Out-of-Distribution (OOD), requiring that models trained on biased data
generalize to the unseen test data, has considerable real-world applications.
One of the most mainstream methods is to extract the invariant subgraph by
aligning the original and augmented data with the help of environment
augmentation. However, these solutions might lead to the loss or redundancy of
semantic subgraphs and result in suboptimal generalization. To address this
challenge, we propose exploiting Probability of Necessity and Sufficiency (PNS)
to extract sufficient and necessary invariant substructures. Beyond that, we
further leverage the domain variant subgraphs related to the labels to boost
the generalization performance in an ensemble manner. Specifically, we first
consider the data generation process for graph data. Under mild conditions, we
show that the sufficient and necessary invariant subgraph can be extracted by
minimizing an upper bound, built on the theoretical advance of the probability
of necessity and sufficiency. To further bridge the theory and algorithm, we
devise the model called Sufficiency and Necessity Inspired Graph Learning
(SNIGL), which ensembles an invariant subgraph classifier on top of latent
sufficient and necessary invariant subgraphs, and a domain variant subgraph
classifier specific to the test domain for generalization enhancement.
Experimental results demonstrate that our SNIGL model outperforms the
state-of-the-art techniques on six public benchmarks, highlighting its
effectiveness in real-world scenarios.

ÊëòË¶ÅÔºöÂúñÂΩ¢Áï∞Â∏∏ÂàÜ‰Ωà (OOD) Ë¶ÅÊ±ÇÂú®ÊúâÂÅèÂ∑ÆÁöÑË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÊ¶ÇÂåñÁÇ∫Êú™Ë¶ãÁöÑÊ∏¨Ë©¶Ë≥áÊñôÔºåÈÄôÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÊúâÁõ∏Áï∂Â§öÁöÑÊáâÁî®„ÄÇÊúÄ‰∏ªÊµÅÁöÑÊñπÊ≥ï‰πã‰∏ÄÊòØÈÄèÈÅéÁí∞Â¢ÉÊì¥ÂÖÖ‰æÜÊØîÂ∞çÂéüÂßãË≥áÊñôÂíåÊì¥ÂÖÖË≥áÊñôÔºå‰ª•ËêÉÂèñ‰∏çËÆäÂ≠êÂúñ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÂèØËÉΩÊúÉÂ∞éËá¥Ë™ûÁæ©Â≠êÂúñÁöÑÈÅ∫Â§±ÊàñÂÜóÈ§òÔºå‰∏¶Â∞éËá¥Ê¨°‰Ω≥ÁöÑÊ¶ÇÂåñ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄëÂª∫Ë≠∞Âà©Áî®ÂøÖË¶ÅÊÄßÂíåÂÖÖÂàÜÊÄßÊ©üÁéá (PNS) ‰æÜËêÉÂèñÂÖÖÂàÜ‰∏îÂøÖË¶ÅÁöÑÂ≠êÁµêÊßã„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âà©Áî®ËàáÊ®ôÁ±§Áõ∏ÈóúÁöÑÁ∂≤ÂüüËÆäÁï∞Â≠êÂúñÔºå‰ª•Êï¥È´îÁöÑÊñπÂºèÊèêÂçáÊ¶ÇÂåñÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàËÄÉÊÖÆÂúñÂΩ¢Ë≥áÊñôÁöÑË≥áÊñôÁî¢ÁîüÈÅéÁ®ã„ÄÇÂú®Ê∫´ÂíåÁöÑÊ¢ù‰ª∂‰∏ãÔºåÊàëÂÄëË≠âÊòé‰∫ÜÂÖÖÂàÜ‰∏îÂøÖË¶ÅÁöÑÂ≠êÂúñÂèØ‰ª•ÈÄèÈÅéÊúÄÂ∞èÂåñ‰∏ÄÂÄã‰∏äÈôê‰æÜËêÉÂèñÔºåÈÄôÂÄã‰∏äÈôêÂª∫Á´ãÂú®ÂøÖË¶ÅÊÄßÂíåÂÖÖÂàÜÊÄßÊ©üÁéáÁöÑÁêÜË´ñÈÄ≤Â±ï‰∏ä„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Ê©ãÊé•ÁêÜË´ñÂíåÊºîÁÆóÊ≥ïÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ÂÖÖÂàÜÊÄßÂíåÂøÖË¶ÅÊÄßÂïüÁôºÂúñÂΩ¢Â≠∏Áøí (SNIGL) ÁöÑÊ®°ÂûãÔºåÂÆÉÂú®ÊΩõÂú®ÂÖÖÂàÜ‰∏îÂøÖË¶ÅÁöÑÂ≠êÂúñ‰∏äÁµÑÂêà‰∏ÄÂÄã‰∏çËÆäÂ≠êÂúñÂàÜÈ°ûÂô®Ôºå‰ª•Âèä‰∏ÄÂÄãÁâπÂÆöÊñºÊ∏¨Ë©¶Á∂≤ÂüüÁöÑÁ∂≤ÂüüËÆäÁï∞Â≠êÂúñÂàÜÈ°ûÂô®Ôºå‰ª•Â¢ûÂº∑Ê¶ÇÂåñ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑ SNIGL Ê®°ÂûãÂú®ÂÖ≠ÂÄãÂÖ¨ÈñãÂü∫Ê∫ñ‰∏äÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊäÄË°ìÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÂú®ÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation**
2407.15268v1 by Liwen Sun, James Zhao, Megan Han, Chenyan Xiong

Multimodal foundation models hold significant potential for automating
radiology report generation, thereby assisting clinicians in diagnosing cardiac
diseases. However, generated reports often suffer from serious factual
inaccuracy. In this paper, we introduce a fact-aware multimodal
retrieval-augmented pipeline in generating accurate radiology reports
(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then
integrate factual knowledge to train a universal multimodal retriever. Given a
radiology image, our retriever can identify high-quality reference reports to
augment multimodal foundation models, thus enhancing the factual completeness
and correctness of report generation. Experiments on two benchmark datasets
show that our multimodal retriever outperforms state-of-the-art retrievers on
both language generation and radiology-specific metrics, up to 6.5% and 2%
score in F1CheXbert and F1RadGraph. Further analysis indicates that employing
our factually-informed training strategy imposes an effective supervision
signal, without relying on explicit diagnostic label guidance, and successfully
propagates fact-aware capabilities from the multimodal retriever to the
multimodal foundation model in radiology report generation.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂú®ÊîæÂ∞ÑÊä•ÂëäÁîüÊàêËá™Âä®ÂåñÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõÔºå‰ªéËÄåÂ∏ÆÂä©‰∏¥Â∫äÂåªÁîüËØäÊñ≠ÂøÉËÑèÁñæÁóÖ„ÄÇÁÑ∂ËÄåÔºåÁîüÊàêÁöÑÊä•ÂëäÂ∏∏Â∏∏Â≠òÂú®‰∏•ÈáçÁöÑ‰∫ãÂÆû‰∏çÂáÜÁ°ÆÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∫ãÂÆûÊÑüÁü•ÁöÑÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁÆ°ÈÅìÊù•ÁîüÊàêÂáÜÁ°ÆÁöÑÊîæÂ∞ÑÊä•ÂëäÔºàFactMM-RAGÔºâ„ÄÇÊàë‰ª¨È¶ñÂÖàÂà©Áî® RadGraph Êù•ÊåñÊéò‰∫ãÂÆûÊä•ÂëäÂØπÔºåÁÑ∂ÂêéÊï¥Âêà‰∫ãÂÆûÁü•ËØÜÊù•ËÆ≠ÁªÉ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âô®„ÄÇÁªôÂÆö‰∏Ä‰∏™ÊîæÂ∞ÑÂõæÂÉèÔºåÊàë‰ª¨ÁöÑÊ£ÄÁ¥¢Âô®ÂèØ‰ª•ËØÜÂà´È´òË¥®ÈáèÁöÑÂèÇËÄÉÊä•ÂëäÊù•Â¢ûÂº∫Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÔºå‰ªéËÄåÂ¢ûÂº∫Êä•ÂëäÁîüÊàêÁöÑ‰∫ãÂÆûÂÆåÊï¥ÊÄßÂíåÊ≠£Á°ÆÊÄß„ÄÇÂú®‰∏§‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âô®Âú®ËØ≠Ë®ÄÁîüÊàêÂíåÊîæÂ∞ÑÂ≠¶ÁâπÂÆöÊåáÊ†á‰∏äÈÉΩ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊ£ÄÁ¥¢Âô®ÔºåÂú® F1CheXbert Âíå F1RadGraph ‰∏≠ÂæóÂàÜÈ´òËææ 6.5% Âíå 2%„ÄÇËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûêË°®ÊòéÔºåÈááÁî®Êàë‰ª¨Âü∫‰∫é‰∫ãÂÆûÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÊñΩÂä†‰∫Ü‰∏Ä‰∏™ÊúâÊïàÁöÑÁõëÁù£‰ø°Âè∑ÔºåËÄåÊó†ÈúÄ‰æùËµñÊòéÁ°ÆÁöÑËØäÊñ≠Ê†áÁ≠æÊåáÂØºÔºåÂπ∂ÊàêÂäüÂú∞Â∞ÜÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âô®ÁöÑ‰∫ãÂÆûÊÑüÁü•ËÉΩÂäõ‰º†Êí≠Âà∞ÊîæÂ∞ÑÊä•ÂëäÁîüÊàê‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°Âûã„ÄÇ

##### **Explaining Decisions of Agents in Mixed-Motive Games**
2407.15255v1 by Maayan Orner, Oleg Maksimov, Akiva Kleinerman, Charles Ortiz, Sarit Kraus

In recent years, agents have become capable of communicating seamlessly via
natural language and navigating in environments that involve cooperation and
competition, a fact that can introduce social dilemmas. Due to the interleaving
of cooperation and competition, understanding agents' decision-making in such
environments is challenging, and humans can benefit from obtaining
explanations. However, such environments and scenarios have rarely been
explored in the context of explainable AI. While some explanation methods for
cooperative environments can be applied in mixed-motive setups, they do not
address inter-agent competition, cheap-talk, or implicit communication by
actions. In this work, we design explanation methods to address these issues.
Then, we proceed to demonstrate their effectiveness and usefulness for humans,
using a non-trivial mixed-motive game as a test case. Lastly, we establish
generality and demonstrate the applicability of the methods to other games,
including one where we mimic human game actions using large language models.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•Ôºå‰ª£ÁêÜ‰∫∫Â∑≤ËÉΩÂ§üÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊó†ÁºùÊ≤üÈÄöÔºåÂπ∂Âú®Ê∂âÂèäÂêà‰ΩúÂíåÁ´û‰∫âÁöÑÁéØÂ¢É‰∏≠ÂØºËà™ÔºåËøô‰∏Ä‰∫ãÂÆûÂèØËÉΩ‰ºöÂ∏¶Êù•Á§æ‰ºöÂõ∞Â¢É„ÄÇÁî±‰∫éÂêà‰ΩúÂíåÁ´û‰∫âÁöÑ‰∫§ÁªáÔºåÁêÜËß£‰ª£ÁêÜ‰∫∫Âú®ËøôÁßçÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñÂÖ∑ÊúâÊåëÊàòÊÄßÔºåËÄå‰∫∫Á±ªÂèØ‰ª•‰ªéËé∑ÂæóËß£Èáä‰∏≠ÂèóÁõä„ÄÇÁÑ∂ËÄåÔºåÂú®ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑËÉåÊôØ‰∏ãÔºåËøôÁßçÁéØÂ¢ÉÂíåÂú∫ÊôØÂæàÂ∞ëË¢´Êé¢Á¥¢„ÄÇËôΩÁÑ∂‰∏Ä‰∫õÈíàÂØπÂêà‰ΩúÁéØÂ¢ÉÁöÑËß£ÈáäÊñπÊ≥ïÂèØ‰ª•Âú®Ê∑∑ÂêàÂä®Êú∫ËÆæÁΩÆ‰∏≠Â∫îÁî®Ôºå‰ΩÜÂÆÉ‰ª¨‰∏çËß£ÂÜ≥‰ª£ÁêÜÈó¥Á´û‰∫â„ÄÅÂªâ‰ª∑Ë∞àËØùÊàñÈÄöËøáË°åÂä®ËøõË°åÁöÑÈöêÂºèÊ≤üÈÄö„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜËß£ÈáäÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÁªßÁª≠‰ΩøÁî®‰∏Ä‰∏™ÈùûÂπ≥Âá°ÁöÑÊ∑∑ÂêàÂä®Êú∫Ê∏∏Êàè‰Ωú‰∏∫ÊµãËØïÊ°à‰æãÔºåÂ±ïÁ§∫ÂÆÉ‰ª¨ÂØπ‰∫∫Á±ªÁöÑÊúâÊïàÊÄßÂíåÊúâÁî®ÊÄß„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜÊôÆÈÅçÊÄßÔºåÂπ∂Â±ïÁ§∫‰∫ÜËøô‰∫õÊñπÊ≥ïÂØπÂÖ∂‰ªñÊ∏∏ÊàèÁöÑÈÄÇÁî®ÊÄßÔºåÂåÖÊã¨Êàë‰ª¨‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊ®°‰ªø‰∫∫Á±ªÊ∏∏ÊàèÂä®‰ΩúÁöÑÊ∏∏Êàè„ÄÇ

##### **XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models**
2407.15248v1 by Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Navid Nobani, Andrea Seveso

In this survey, we address the key challenges in Large Language Models (LLM)
research, focusing on the importance of interpretability. Driven by increasing
interest from AI and business sectors, we highlight the need for transparency
in LLMs. We examine the dual paths in current LLM research and eXplainable
Artificial Intelligence (XAI): enhancing performance through XAI and the
emerging focus on model interpretability. Our paper advocates for a balanced
approach that values interpretability equally with functional advancements.
Recognizing the rapid development in LLM research, our survey includes both
peer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of
XAI's role in LLM research. We conclude by urging the research community to
advance both LLM and XAI fields together.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖË™øÊü•‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á†îÁ©∂‰∏≠ÁöÑÈóúÈçµÊåëÊà∞ÔºåÈáçÈªûÈóúÊ≥®ÂèØËß£ÈáãÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇÂú® AI ÂíåÂïÜÊ•≠È†òÂüüÊó•ÁõäÂ¢ûÈï∑ÁöÑËààË∂£È©ÖÂãï‰∏ãÔºåÊàëÂÄëÂº∑Ë™ø‰∫Ü LLM ‰∏≠ÈÄèÊòéÂ∫¶ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊàëÂÄëÂØ©Êü•‰∫ÜÁï∂Ââç LLM Á†îÁ©∂ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÁöÑÈõôÈáçË∑ØÂæëÔºöÈÄèÈÅé XAI ÊèêÂçáÊïàËÉΩÔºå‰ª•ÂèäÂ∞çÊ®°ÂûãÂèØËß£ÈáãÊÄßÁöÑÊñ∞ËààÈóúÊ≥®„ÄÇÊàëÂÄëÁöÑË´ñÊñá‰∏ªÂºµÊé°Âèñ‰∏ÄÁ®ÆÂπ≥Ë°°ÁöÑÊñπÊ≥ïÔºåÂ∞áÂèØËß£ÈáãÊÄßËàáÂäüËÉΩÈÄ≤Ê≠•ÂêåÁ≠âÈáçË¶ñ„ÄÇË™çË≠òÂà∞ LLM Á†îÁ©∂ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÊàëÂÄëÁöÑË™øÊü•ÂåÖÊã¨ÂêåË°åË©ïÂØ©ÂíåÈ†êÂç∞Êú¨ (arXiv) Ë´ñÊñáÔºåÊèê‰æõ XAI Âú® LLM Á†îÁ©∂‰∏≠ËßíËâ≤ÁöÑÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÊàëÂÄëÊúÄÂæåÊï¶‰øÉÁ†îÁ©∂Á§æÁæ§ÂÖ±ÂêåÊé®ÈÄ≤ LLM Âíå XAI È†òÂüü„ÄÇ

