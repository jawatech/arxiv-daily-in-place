{"2405.18009": {"publish_time": "2024-05-28", "title": "Exploring Context Window of Large Language Models via Decomposed Positional Vectors", "paper_summary": "Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u5177\u6709\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\uff0c\u5c0e\u81f4\u5728\u8655\u7406\u8d85\u904e\u4e0a\u4e0b\u6587\u8996\u7a97\u9577\u5ea6\u7684\u6587\u5b57\u6642\uff0c\u6548\u80fd\u6703\u986f\u8457\u4e0b\u964d\u3002\u5df2\u7d93\u63d0\u51fa\u8a31\u591a\u7814\u7a76\u4f86\u5ef6\u4f38\u4e0a\u4e0b\u6587\u8996\u7a97\u4e26\u9054\u6210 LLM \u7684\u9577\u5ea6\u5916\u63a8\uff0c\u4f46\u5c0d\u65bc\u9019\u4e9b\u65b9\u6cd5\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7684\u8a6e\u91cb\u3002\u5728\u9019\u500b\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e0a\u4e0b\u6587\u8996\u7a97\u5167\u5916\u7684\u4f4d\u7f6e\u8cc7\u8a0a\uff0c\u4ee5\u89e3\u78bc LLM \u7684\u5e95\u5c64\u6a5f\u5236\u3002\u900f\u904e\u4f7f\u7528\u57fa\u65bc\u5e73\u5747\u503c\u7684\u5206\u89e3\u65b9\u6cd5\uff0c\u6211\u5011\u5c07\u4f4d\u7f6e\u5411\u91cf\u5f9e LLM \u7684\u96b1\u85cf\u72c0\u614b\u4e2d\u89e3\u958b\uff0c\u4e26\u5206\u6790\u5b83\u5011\u7684\u5f62\u6210\u548c\u5c0d\u6ce8\u610f\u529b\u7684\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u7576\u6587\u5b57\u8d85\u904e\u4e0a\u4e0b\u6587\u8996\u7a97\u6642\uff0c\u6211\u5011\u5206\u6790\u4e86\u4f4d\u7f6e\u5411\u91cf\u5728\u5169\u7a2e\u8a2d\u5b9a\u4e0b\u7684\u8b8a\u5316\uff0c\u5373\u76f4\u63a5\u5916\u63a8\u548c\u4e0a\u4e0b\u6587\u8996\u7a97\u5ef6\u4f38\u3002\u6839\u64da\u6211\u5011\u7684\u767c\u73fe\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u5169\u7a2e\u7121\u9700\u8a13\u7df4\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u5ef6\u4f38\u65b9\u6cd5\uff0c\u4f4d\u7f6e\u5411\u91cf\u66ff\u63db\u548c\u6ce8\u610f\u529b\u8996\u7a97\u5ef6\u4f38\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u9019\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5ef6\u4f38\u4e0a\u4e0b\u6587\u8996\u7a97\u9577\u5ea6\u3002</paragraph>", "author": "Zican Dong et.al.", "authors": "Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, Ji-Rong Wen", "id": "2405.18009v1", "paper_url": "http://arxiv.org/abs/2405.18009v1", "repo": "null"}}