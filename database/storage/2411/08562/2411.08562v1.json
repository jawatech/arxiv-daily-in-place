{"2411.08562": {"publish_time": "2024-11-13", "title": "Neural Corrective Machine Unranking", "paper_summary": "Machine unlearning in neural information retrieval (IR) systems requires\nremoving specific data whilst maintaining model performance. Applying existing\nmachine unlearning methods to IR may compromise retrieval effectiveness or\ninadvertently expose unlearning actions due to the removal of particular items\nfrom the retrieved results presented to users. We formalise corrective\nunranking, which extends machine unlearning in (neural) IR context by\nintegrating substitute documents to preserve ranking integrity, and propose a\nnovel teacher-student framework, Corrective unRanking Distillation (CuRD), for\nthis task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR\nmodel such that its output relevance scores of to-be-forgotten samples mimic\nthose of low-ranking, non-retrievable samples; (2) enables correction by\nfine-tuning the relevance scores for the substitute samples to match those of\ncorresponding to-be-forgotten samples closely; (3) seeks to preserve\nperformance on samples that are not targeted for forgetting. We evaluate CuRD\non four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and\nTREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the\ntraining dataset demonstrate that CuRD outperforms seven state-of-the-art\nbaselines in terms of forgetting and correction while maintaining model\nretention and generalisation capabilities.", "paper_summary_zh": "\u795e\u7d93\u8cc7\u8a0a\u6aa2\u7d22 (IR) \u7cfb\u7d71\u4e2d\u7684\u6a5f\u5668\u53bb\u5b78\u7fd2\u9700\u8981\u5728\u7dad\u6301\u6a21\u578b\u6548\u80fd\u7684\u540c\u6642\u79fb\u9664\u7279\u5b9a\u8cc7\u6599\u3002\u5c07\u73fe\u6709\u7684\u6a5f\u5668\u53bb\u5b78\u7fd2\u65b9\u6cd5\u5957\u7528\u65bc IR \u53ef\u80fd\u6703\u640d\u5bb3\u6aa2\u7d22\u6548\u80fd\uff0c\u6216\u7531\u65bc\u5f9e\u63d0\u4f9b\u7d66\u4f7f\u7528\u8005\u7684\u6aa2\u7d22\u7d50\u679c\u4e2d\u79fb\u9664\u7279\u5b9a\u9805\u76ee\u800c\u610f\u5916\u5730\u63ed\u9732\u53bb\u5b78\u7fd2\u52d5\u4f5c\u3002\u6211\u5011\u6b63\u5f0f\u5316\u4fee\u6b63\u6027\u53d6\u6d88\u6392\u540d\uff0c\u9019\u900f\u904e\u6574\u5408\u66ff\u4ee3\u6587\u4ef6\u4f86\u4fdd\u7559\u6392\u540d\u5b8c\u6574\u6027\uff0c\u4ee5\u64f4\u5145 (\u795e\u7d93) IR \u80cc\u666f\u4e2d\u7684\u6a5f\u5668\u53bb\u5b78\u7fd2\uff0c\u4e26\u70ba\u6b64\u4efb\u52d9\u63d0\u51fa\u4e00\u500b\u5275\u65b0\u7684\u5e2b\u751f\u67b6\u69cb\uff0c\u4fee\u6b63\u6027\u53d6\u6d88\u6392\u540d\u84b8\u993e (CuRD)\u3002CuRD (1) \u900f\u904e\u8abf\u6574 (\u5df2\u8a13\u7df4\u7684) \u795e\u7d93 IR \u6a21\u578b\uff0c\u8b93\u5176\u8f38\u51fa\u5f85\u907a\u5fd8\u7bc4\u4f8b\u7684\u76f8\u95dc\u6027\u5206\u6578\u6a21\u4eff\u4f4e\u6392\u540d\u3001\u4e0d\u53ef\u6aa2\u7d22\u7bc4\u4f8b\u7684\u76f8\u95dc\u6027\u5206\u6578\uff0c\u4ee5\u4fc3\u9032\u907a\u5fd8\uff1b(2) \u900f\u904e\u5fae\u8abf\u66ff\u4ee3\u7bc4\u4f8b\u7684\u76f8\u95dc\u6027\u5206\u6578\uff0c\u4f7f\u5176\u8207\u5c0d\u61c9\u5f85\u907a\u5fd8\u7bc4\u4f8b\u7684\u76f8\u95dc\u6027\u5206\u6578\u7dca\u5bc6\u5339\u914d\uff0c\u4ee5\u9032\u884c\u4fee\u6b63\uff1b(3) \u5c0b\u6c42\u4fdd\u7559\u672a\u91dd\u5c0d\u907a\u5fd8\u7684\u7bc4\u4f8b\u7684\u6548\u80fd\u3002\u6211\u5011\u4f7f\u7528 MS MARCO \u548c TREC CAR \u8cc7\u6599\u96c6\uff0c\u5728\u56db\u500b\u795e\u7d93 IR \u6a21\u578b (BERTcat\u3001BERTdot\u3001ColBERT\u3001PARADE) \u4e0a\u8a55\u4f30 CuRD\u3002\u4f7f\u7528\u4f54\u8a13\u7df4\u8cc7\u6599\u96c6 1% \u548c 20% \u7684\u907a\u5fd8\u96c6\u5927\u5c0f\u7684\u5be6\u9a57\u8b49\u660e\uff0cCuRD \u5728\u907a\u5fd8\u548c\u4fee\u6b63\u65b9\u9762\u512a\u65bc\u4e03\u500b\u6700\u5148\u9032\u7684\u57fa\u6e96\uff0c\u540c\u6642\u7dad\u6301\u6a21\u578b\u4fdd\u7559\u548c\u6982\u5316\u80fd\u529b\u3002", "author": "Jingrui Hou et.al.", "authors": "Jingrui Hou, Axel Finke, Georgina Cosma", "id": "2411.08562v1", "paper_url": "http://arxiv.org/abs/2411.08562v1", "repo": "null"}}