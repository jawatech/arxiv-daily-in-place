{"2502.13142": {"publish_time": "2025-02-18", "title": "Pre-training Auto-regressive Robotic Models with 4D Representations", "paper_summary": "Foundation models pre-trained on massive unlabeled datasets have\nrevolutionized natural language and computer vision, exhibiting remarkable\ngeneralization capabilities, thus highlighting the importance of pre-training.\nYet, efforts in robotics have struggled to achieve similar success, limited by\neither the need for costly robotic annotations or the lack of representations\nthat effectively model the physical world. In this paper, we introduce ARM4R,\nan Auto-regressive Robotic Model that leverages low-level 4D Representations\nlearned from human video data to yield a better pre-trained robotic model.\nSpecifically, we focus on utilizing 3D point tracking representations from\nvideos derived by lifting 2D representations into 3D space via monocular depth\nestimation across time. These 4D representations maintain a shared geometric\nstructure between the points and robot state representations up to a linear\ntransformation, enabling efficient transfer learning from human video data to\nlow-level robotic control. Our experiments show that ARM4R can transfer\nefficiently from human video data to robotics and consistently improves\nperformance on tasks across various robot environments and configurations.", "paper_summary_zh": "\u9810\u5148\u5728\u5927\u91cf\u672a\u6a19\u8a18\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u597d\u7684\u57fa\u790e\u6a21\u578b\u5df2\u7d93\u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u548c\u96fb\u8166\u8996\u89ba\uff0c\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6982\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u7a81\u986f\u4e86\u9810\u5148\u8a13\u7df4\u7684\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u6a5f\u5668\u4eba\u9818\u57df\u7684\u52aa\u529b\u4e00\u76f4\u96e3\u4ee5\u53d6\u5f97\u985e\u4f3c\u7684\u6210\u529f\uff0c\u53d7\u5230\u6602\u8cb4\u7684\u6a5f\u5668\u4eba\u6a19\u8a3b\u9700\u6c42\u6216\u7f3a\u4e4f\u6709\u6548\u5efa\u6a21\u7269\u7406\u4e16\u754c\u7684\u8868\u5fb5\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 ARM4R\uff0c\u4e00\u7a2e\u81ea\u8ff4\u6b78\u6a5f\u5668\u4eba\u6a21\u578b\uff0c\u5b83\u5229\u7528\u5f9e\u4eba\u985e\u5f71\u7247\u8cc7\u6599\u4e2d\u5b78\u7fd2\u5230\u7684\u4f4e\u968e 4D \u8868\u5fb5\uff0c\u4ee5\u7522\u751f\u66f4\u597d\u7684\u9810\u5148\u8a13\u7df4\u6a5f\u5668\u4eba\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u5229\u7528\u5f9e\u5f71\u7247\u4e2d\u7372\u5f97\u7684 3D \u9ede\u8ffd\u8e64\u8868\u5fb5\uff0c\u9019\u4e9b\u8868\u5fb5\u662f\u900f\u904e\u55ae\u773c\u6df1\u5ea6\u4f30\u8a08\u8de8\u6642\u9593\u5c07 2D \u8868\u5fb5\u63d0\u5347\u5230 3D \u7a7a\u9593\u800c\u5c0e\u51fa\u7684\u3002\u9019\u4e9b 4D \u8868\u5fb5\u5728\u9ede\u548c\u6a5f\u5668\u4eba\u72c0\u614b\u8868\u5fb5\u4e4b\u9593\u4fdd\u6301\u4e00\u500b\u5171\u7528\u7684\u5e7e\u4f55\u7d50\u69cb\uff0c\u76f4\u5230\u4e00\u500b\u7dda\u6027\u8f49\u63db\uff0c\u9019\u4f7f\u5f97\u5f9e\u4eba\u985e\u5f71\u7247\u8cc7\u6599\u5230\u4f4e\u968e\u6a5f\u5668\u4eba\u63a7\u5236\u7684\u6709\u6548\u9077\u79fb\u5b78\u7fd2\u6210\u70ba\u53ef\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cARM4R \u53ef\u4ee5\u6709\u6548\u5730\u5f9e\u4eba\u985e\u5f71\u7247\u8cc7\u6599\u8f49\u79fb\u5230\u6a5f\u5668\u4eba\u6280\u8853\uff0c\u4e26\u6301\u7e8c\u6539\u5584\u5404\u7a2e\u6a5f\u5668\u4eba\u74b0\u5883\u548c\u7d44\u614b\u4e2d\u7684\u4efb\u52d9\u6548\u80fd\u3002", "author": "Dantong Niu et.al.", "authors": "Dantong Niu, Yuvan Sharma, Haoru Xue, Giscard Biamby, Junyi Zhang, Ziteng Ji, Trevor Darrell, Roei Herzig", "id": "2502.13142v1", "paper_url": "http://arxiv.org/abs/2502.13142v1", "repo": "null"}}