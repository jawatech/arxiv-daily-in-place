{"2411.16991": {"publish_time": "2024-11-25", "title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models", "paper_summary": "Knowledge distillation (KD) has become a widely adopted approach for\ncompressing large language models (LLMs) to reduce computational costs and\nmemory footprints. However, the availability of complex teacher models is a\nprerequisite for running most KD pipelines. Thus, the traditional KD procedure\ncan be unachievable or budget-unfriendly, particularly when relying on\ncommercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as\nan advisable alternative, enabling student models to learn without teachers'\nguidance. Nonetheless, existing SelfD approaches for LMs often involve\narchitectural modifications, assuming the models are open-source, which may not\nalways be practical. In this work, we introduce a model-agnostic and\ntask-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB),\nwhich realizes current iterations' distillation from the last ones' generated\nlogits. Additionally, to address prediction inaccuracies during the early\niterations, we dynamically adjust the distillation influence and temperature\nvalues to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a\nnovel fine-tuning policy that facilitates the seamless integration of existing\nself-correction and self-training techniques for small language models (SLMs)\nbecause they all require updating SLMs' parameters. We demonstrate the superior\nperformance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and\ndecoder-only LMs (e.g., LLaMA model families), validating its effectiveness\nacross natural language understanding (NLU) and natural language generation\n(NLG) benchmarks.", "paper_summary_zh": "\u77e5\u8b58\u84b8\u993e (KD) \u5df2\u6210\u70ba\u5ee3\u6cdb\u63a1\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u58d3\u7e2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u964d\u4f4e\u904b\u7b97\u6210\u672c\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u7136\u800c\uff0c\u8907\u96dc\u6559\u5e2b\u6a21\u578b\u7684\u53ef\u7528\u6027\u662f\u57f7\u884c\u5927\u591a\u6578 KD \u7ba1\u7dda\u7684\u5148\u6c7a\u689d\u4ef6\u3002\u56e0\u6b64\uff0c\u50b3\u7d71\u7684 KD \u7a0b\u5e8f\u53ef\u80fd\u7121\u6cd5\u5be6\u73fe\u6216\u4e0d\u7b26\u5408\u9810\u7b97\uff0c\u7279\u5225\u662f\u5728\u4f9d\u8cf4 GPT4 \u7b49\u5546\u696d LLM \u6642\u3002\u5728\u9019\u65b9\u9762\uff0c\u81ea\u84b8\u993e (SelfD) \u6210\u70ba\u5efa\u8b70\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u5b78\u751f\u6a21\u578b\u80fd\u5920\u5728\u6c92\u6709\u6559\u5e2b\u6307\u5c0e\u7684\u60c5\u6cc1\u4e0b\u5b78\u7fd2\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u73fe\u6709\u7684 LLM \u81ea\u84b8\u993e\u65b9\u6cd5\u901a\u5e38\u6d89\u53ca\u67b6\u69cb\u4fee\u6539\uff0c\u5047\u8a2d\u6a21\u578b\u662f\u958b\u6e90\u7684\uff0c\u9019\u53ef\u80fd\u4e26\u4e0d\u7e3d\u662f\u5be6\u7528\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u8207\u6a21\u578b\u7121\u95dc\u4e14\u8207\u4efb\u52d9\u7121\u95dc\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba\u52d5\u614b\u81ea\u84b8\u993e\u4f86\u81ea\u524d\u4e00\u500b\u5c0f\u6279\u6b21 (DynSDPB)\uff0c\u5b83\u5be6\u73fe\u4e86\u5f9e\u6700\u5f8c\u4e00\u500b\u751f\u6210\u7684\u908f\u8f2f\u4e2d\u84b8\u993e\u7576\u524d\u53cd\u8986\u904b\u7b97\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u89e3\u6c7a\u65e9\u671f\u53cd\u8986\u904b\u7b97\u671f\u9593\u7684\u9810\u6e2c\u4e0d\u6e96\u78ba\u6027\uff0c\u6211\u5011\u52d5\u614b\u8abf\u6574\u84b8\u993e\u5f71\u97ff\u548c\u6eab\u5ea6\u503c\uff0c\u4ee5\u589e\u5f37\u5fae\u8abf\u7684\u9069\u61c9\u6027\u3002\u6b64\u5916\uff0cDynSDPB \u662f\u4e00\u9805\u65b0\u7a4e\u7684\u5fae\u8abf\u7b56\u7565\uff0c\u5b83\u4fc3\u9032\u4e86\u73fe\u6709\u81ea\u6821\u6b63\u548c\u81ea\u8a13\u7df4\u6280\u8853\u8207\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u7684\u7121\u7e2b\u6574\u5408\uff0c\u56e0\u70ba\u5b83\u5011\u90fd\u9700\u8981\u66f4\u65b0 SLM \u7684\u53c3\u6578\u3002\u6211\u5011\u5728\u50c5\u7de8\u78bc\u5668 LLM\uff08\u4f8b\u5982 BERT \u6a21\u578b\u7cfb\u5217\uff09\u548c\u50c5\u89e3\u78bc\u5668 LLM\uff08\u4f8b\u5982 LLaMA \u6a21\u578b\u7cfb\u5217\uff09\u4e0a\u5c55\u793a\u4e86 DynSDPB \u7684\u5353\u8d8a\u6548\u80fd\uff0c\u9a57\u8b49\u4e86\u5176\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u548c\u81ea\u7136\u8a9e\u8a00\u751f\u6210 (NLG) \u57fa\u6e96\u4e2d\u7684\u6709\u6548\u6027\u3002", "author": "Yao Fu et.al.", "authors": "Yao Fu, Yin Yu, Xiaotian Han, Runchao Li, Xianxuan Long, Haotian Yu, Pan Li", "id": "2411.16991v1", "paper_url": "http://arxiv.org/abs/2411.16991v1", "repo": "null"}}