{"2407.07801": {"publish_time": "2024-07-10", "title": "AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning", "paper_summary": "In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose \\textbf{AVCap}, an \\textbf{A}udio-\\textbf{V}isual \\textbf{Cap}tioning\nframework, a simple yet powerful baseline approach applicable to audio-visual\ncaptioning. AVCap utilizes audio-visual features as text tokens, which has many\nadvantages not only in performance but also in the extensibility and\nscalability of the model. AVCap is designed around three pivotal dimensions:\nthe exploration of optimal audio-visual encoder architectures, the adaptation\nof pre-trained models according to the characteristics of generated text, and\nthe investigation into the efficacy of modality fusion in captioning. Our\nmethod outperforms existing audio-visual captioning methods across all metrics\nand the code is available on https://github.com/JongSuk1/AVCap", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u8868\u5f81\u5b66\u4e60\u548c\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u5df2\u5c06\u81ea\u52a8\u5b57\u5e55 (AC) \u63a8\u5411\u65b0\u9ad8\u5ea6\uff0c\u80fd\u591f\u751f\u6210\u4eba\u7c7b\u7b49\u7ea7\u7684\u63cf\u8ff0\u3002\u5229\u7528\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5 \\textbf{AVCap}\uff0c\u4e00\u4e2a\u9002\u7528\u4e8e\u97f3\u9891\u89c6\u89c9\u5b57\u5e55\u7684\\textbf{A}udio-\\textbf{V}isual \\textbf{Cap}tioning \u6846\u67b6\u3002AVCap \u5c06\u97f3\u9891\u89c6\u89c9\u7279\u5f81\u7528\u4f5c\u6587\u672c\u6807\u8bb0\uff0c\u8fd9\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u5177\u6709\u8bb8\u591a\u4f18\u52bf\uff0c\u800c\u4e14\u5728\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4e5f\u5177\u6709\u4f18\u52bf\u3002AVCap \u7684\u8bbe\u8ba1\u56f4\u7ed5\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u63a2\u7d22\u6700\u4f18\u7684\u97f3\u9891\u89c6\u89c9\u7f16\u7801\u5668\u67b6\u6784\u3001\u6839\u636e\u751f\u6210\u6587\u672c\u7684\u7279\u5f81\u8c03\u6574\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u53ca\u8c03\u67e5\u6a21\u6001\u878d\u5408\u5728\u5b57\u5e55\u4e2d\u7684\u529f\u6548\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u97f3\u9891\u89c6\u89c9\u5b57\u5e55\u65b9\u6cd5\uff0c\u5e76\u4e14\u4ee3\u7801\u53ef\u5728 https://github.com/JongSuk1/AVCap \u4e0a\u83b7\u53d6", "author": "Jongsuk Kim et.al.", "authors": "Jongsuk Kim, Jiwon Shin, Junmo Kim", "id": "2407.07801v1", "paper_url": "http://arxiv.org/abs/2407.07801v1", "repo": "null"}}