{"2405.10292": {"publish_time": "2024-05-16", "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "paper_summary": "Large vision-language models (VLMs) fine-tuned on specialized visual\ninstruction-following data have exhibited impressive language reasoning\ncapabilities across various scenarios. However, this fine-tuning paradigm may\nnot be able to efficiently learn optimal decision-making agents in multi-step\ngoal-directed tasks from interactive environments. To address this challenge,\nwe propose an algorithmic framework that fine-tunes VLMs with reinforcement\nlearning (RL). Specifically, our framework provides a task description and then\nprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM\nto efficiently explore intermediate reasoning steps that lead to the final\ntext-based action. Next, the open-ended text output is parsed into an\nexecutable action to interact with the environment to obtain goal-directed task\nrewards. Finally, our framework uses these task rewards to fine-tune the entire\nVLM with RL. Empirically, we demonstrate that our proposed framework enhances\nthe decision-making capabilities of VLM agents across various tasks, enabling\n7b models to outperform commercial models such as GPT4-V or Gemini.\nFurthermore, we find that CoT reasoning is a crucial component for performance\nimprovement, as removing the CoT reasoning results in a significant decrease in\nthe overall performance of our method.", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u7ecf\u8fc7\u4e13\u95e8\u7684\u89c6\u89c9\u6307\u4ee4\u9075\u5faa\u6570\u636e\u7684\u5fae\u8c03\u540e\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5fae\u8c03\u8303\u5f0f\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5730\u4ece\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u5b66\u4e60\u591a\u6b65\u9aa4\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u51b3\u7b56\u4ee3\u7406\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60 (RL) \u5bf9 VLM \u8fdb\u884c\u5fae\u8c03\u7684\u7b97\u6cd5\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4efb\u52a1\u63cf\u8ff0\uff0c\u7136\u540e\u63d0\u793a VLM \u751f\u6210\u601d\u7ef4\u94fe (CoT) \u63a8\u7406\uff0c\u4f7f VLM \u80fd\u591f\u6709\u6548\u5730\u63a2\u7d22\u5bfc\u81f4\u6700\u7ec8\u57fa\u4e8e\u6587\u672c\u7684\u52a8\u4f5c\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002\u63a5\u4e0b\u6765\uff0c\u5c06\u5f00\u653e\u5f0f\u6587\u672c\u8f93\u51fa\u89e3\u6790\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\uff0c\u4ee5\u4e0e\u73af\u5883\u4ea4\u4e92\u4ee5\u83b7\u5f97\u76ee\u6807\u5bfc\u5411\u7684\u4efb\u52a1\u5956\u52b1\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f7f\u7528\u8fd9\u4e9b\u4efb\u52a1\u5956\u52b1\u5bf9\u6574\u4e2a VLM \u8fdb\u884c RL \u5fae\u8c03\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u589e\u5f3a\u4e86 VLM \u4ee3\u7406\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4f7f 7b \u6a21\u578b\u80fd\u591f\u4f18\u4e8e GPT4-V \u6216 Gemini \u7b49\u5546\u4e1a\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0 CoT \u63a8\u7406\u662f\u6027\u80fd\u6539\u8fdb\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u56e0\u4e3a\u79fb\u9664 CoT \u63a8\u7406\u4f1a\u5bfc\u81f4\u6211\u4eec\u65b9\u6cd5\u7684\u6574\u4f53\u6027\u80fd\u663e\u7740\u4e0b\u964d\u3002", "author": "Yuexiang Zhai et.al.", "authors": "Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine", "id": "2405.10292v1", "paper_url": "http://arxiv.org/abs/2405.10292v1", "repo": "null"}}