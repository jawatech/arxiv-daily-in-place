{"2501.09355": {"publish_time": "2025-01-16", "title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks", "paper_summary": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks.", "paper_summary_zh": "\u591a\u6a21\u6001 AI \u4ee3\u7406\u662f AI \u6a21\u578b\uff0c\u5b83\u6709\u80fd\u529b\u4ea4\u4e92\u5f0f\u5730\u548c\u534f\u4f5c\u5f0f\u5730\u5e2e\u52a9\u4eba\u7c7b\u7528\u6237\u89e3\u51b3\u65e5\u5e38\u4efb\u52a1\u3002\u589e\u5f3a\u73b0\u5b9e (AR) \u5934\u6234\u8bbe\u5907\u53ef\u4ee5\u901a\u8fc7\u5411 AI \u4ee3\u7406\u63d0\u4f9b\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u591a\u79cd\u6a21\u5f0f\uff08\u97f3\u9891\u548c\u89c6\u9891\uff09\u89c2\u5bdf\u80fd\u529b\uff0c\u6765\u72ec\u7279\u5730\u6539\u5584\u7528\u6237\u89e3\u51b3\u65e5\u5e38\u7a0b\u5e8f\u4efb\u52a1\u7684\u4f53\u9a8c\u3002\u6b64\u7c7b AR \u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9 AI \u4ee3\u7406\u770b\u5230\u548c\u542c\u5230\u7528\u6237\u91c7\u53d6\u7684\u52a8\u4f5c\uff0c\u800c\u8fd9\u4e9b\u52a8\u4f5c\u53ef\u4ee5\u4e0e\u4eba\u7c7b\u7528\u6237\u7684\u591a\u79cd\u6a21\u5f0f\u80fd\u529b\u76f8\u5173\u3002\u73b0\u6709\u7684 AI \u4ee3\u7406\uff0c\u65e0\u8bba\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fd8\u662f\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\uff0c\u672c\u8d28\u4e0a\u90fd\u662f\u53cd\u5e94\u6027\u7684\uff0c\u8fd9\u610f\u5473\u7740\u6a21\u578b\u5728\u4e0d\u9605\u8bfb\u6216\u8046\u542c\u4eba\u7c7b\u7528\u6237\u7684\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u91c7\u53d6\u884c\u52a8\u3002\u53e6\u4e00\u65b9\u9762\uff0cAI \u4ee3\u7406\u7684\u4e3b\u52a8\u6027\u53ef\u4ee5\u5e2e\u52a9\u4eba\u7c7b\u7528\u6237\u68c0\u6d4b\u548c\u7ea0\u6b63\u4ee3\u7406\u89c2\u5bdf\u5230\u7684\u4efb\u52a1\u4e2d\u7684\u4efb\u4f55\u9519\u8bef\uff0c\u5728\u7528\u6237\u6b63\u786e\u5b8c\u6210\u4efb\u52a1\u65f6\u9f13\u52b1\u4ed6\u4eec\uff0c\u6216\u8005\u7b80\u5355\u5730\u4e0e\u7528\u6237\u8fdb\u884c\u5bf9\u8bdd\u2014\u2014\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6559\u6388\u6216\u534f\u52a9\u7528\u6237\u3002\u6211\u4eec\u63d0\u51fa\u7684 YET \u5e72\u9884 (YETI) \u591a\u6a21\u6001\u4ee3\u7406\u4e13\u6ce8\u4e8e\u8bc6\u522b\u53ef\u80fd\u9700\u8981\u4ee3\u7406\u4e3b\u52a8\u5e72\u9884\u7684\u60c5\u51b5\u7684\u7814\u7a76\u95ee\u9898\u3002\u8fd9\u4f7f\u5f97\u4ee3\u7406\u80fd\u591f\u7406\u89e3\u4f55\u65f6\u53ef\u4ee5\u5728\u4e0e\u4eba\u7c7b\u7528\u6237\u7684\u5bf9\u8bdd\u4e2d\u8fdb\u884c\u5e72\u9884\uff0c\u8fd9\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u7ea0\u6b63\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff0c\u4f8b\u5982\u4f7f\u7528 AR \u505a\u996d\u3002\u6211\u4eec\u7684 YETI \u4ee3\u7406\u6839\u636e\u8fde\u7eed\u89c6\u9891\u5e27\u4e0a\u7ed3\u6784\u76f8\u4f3c\u6027 (SSIM) \u7684\u53ef\u89e3\u91ca\u6982\u5ff5\u5b66\u4e60\u573a\u666f\u7406\u89e3\u4fe1\u53f7\u3002\u6211\u4eec\u8fd8\u5b9a\u4e49\u4e86\u5bf9\u9f50\u4fe1\u53f7\uff0cAI \u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u8bc6\u522b\u4e0e\u7528\u6237\u5728\u4efb\u52a1\u4e0a\u7684\u52a8\u4f5c\u76f8\u5bf9\u5e94\u7684\u89c6\u9891\u5e27\u662f\u5426\u4e0e\u9884\u671f\u52a8\u4f5c\u4e00\u81f4\u3002\u6211\u4eec\u7684 AI \u4ee3\u7406\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u53f7\u6765\u786e\u5b9a\u4f55\u65f6\u5e94\u4e3b\u52a8\u5e72\u9884\u3002\u6211\u4eec\u5728 HoloAssist \u591a\u6a21\u6001\u57fa\u51c6\u4e2d\u6bd4\u8f83\u4e86\u6211\u4eec\u5173\u4e8e\u4e3b\u52a8\u5e72\u9884\u5b9e\u4f8b\u7684\u7ed3\u679c\uff0c\u8be5\u57fa\u51c6\u7528\u4e8e\u4e13\u5bb6\u4ee3\u7406\u6307\u5bfc\u7528\u6237\u5b8c\u6210\u7a0b\u5e8f\u4efb\u52a1\u3002", "author": "Saptarashmi Bandyopadhyay et.al.", "authors": "Saptarashmi Bandyopadhyay, Vikas Bahirwani, Lavisha Aggarwal, Bhanu Guda, Lin Li, Andrea Colaco", "id": "2501.09355v1", "paper_url": "http://arxiv.org/abs/2501.09355v1", "repo": "null"}}