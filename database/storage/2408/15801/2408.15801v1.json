{"2408.15801": {"publish_time": "2024-08-28", "title": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization", "paper_summary": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.", "paper_summary_zh": "\u5728\u6578\u4f4d\u6587\u5b57\u4ee5\u7a7a\u524d\u901f\u5ea6\u6fc0\u589e\u7684\u6642\u4ee3\uff0c\u9ad8\u6548\u7684\u6458\u8981\u5de5\u5177\u6b63\u8b8a\u5f97\u4e0d\u53ef\u6216\u7f3a\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u529f\u61c9\u7528\u65bc\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\uff0c\u4f46\u5b83\u5011\u5728\u8403\u53d6\u5f0f\u6587\u5b57\u6458\u8981\u4e2d\u7684\u89d2\u8272\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u672c\u6587\u4ecb\u7d39 EYEGLAXS\uff08\u7528\u65bc\u8403\u53d6\u5f0f\u6458\u8981\u7684\u7c21\u6613\u9ad8\u6548\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\uff0c\u4e00\u500b\u5229\u7528 LLM\uff08\u7279\u5225\u662f LLAMA2-7B \u548c ChatGLM2-6B\uff09\u7684\u67b6\u69cb\uff0c\u91dd\u5c0d\u5197\u9577\u7684\u6587\u5b57\u6587\u4ef6\u9032\u884c\u8403\u53d6\u5f0f\u6458\u8981\u3002EYEGLAXS \u5c08\u6ce8\u65bc\u8403\u53d6\u5f0f\u6458\u8981\uff0c\u4ee5\u78ba\u4fdd\u4e8b\u5be6\u548c\u8a9e\u6cd5\u7684\u5b8c\u6574\u6027\uff0c\u800c\u975e\u62bd\u8c61\u65b9\u6cd5\uff0c\u5f8c\u8005\u901a\u5e38\u6703\u51fa\u73fe\u4e8b\u5be6\u4e0d\u6b63\u78ba\u548c\u5e7b\u89ba\u7b49\u554f\u984c\u3002EYEGLAXS \u5229\u7528\u4e86\u6700\u5148\u9032\u7684\u6280\u8853\uff0c\u4f8b\u5982 Flash Attention \u548c\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT)\uff0c\u89e3\u6c7a\u4e86\u901a\u5e38\u8207 LLM \u76f8\u95dc\u7684\u904b\u7b97\u548c\u8cc7\u6e90\u6311\u6230\u3002\u8a72\u7cfb\u7d71\u5728 PubMed \u548c ArXiv \u7b49\u77e5\u540d\u8cc7\u6599\u96c6\u4e0a\u8a2d\u5b9a\u4e86\u65b0\u7684\u6548\u80fd\u57fa\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u984d\u5916\u7684\u5206\u6790\u64f4\u5c55\u4e86\u6211\u5011\u7684\u7814\u7a76\uff0c\u63a2\u8a0e LLM \u5728\u8655\u7406\u4e0d\u540c\u5e8f\u5217\u9577\u5ea6\u6642\u7684\u9069\u61c9\u6027\uff0c\u4ee5\u53ca\u5b83\u5011\u5728\u8f03\u5c0f\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u6548\u7387\u3002\u9019\u4e9b\u8ca2\u737b\u4e0d\u50c5\u70ba\u8a72\u9818\u57df\u6a39\u7acb\u4e86\u65b0\u7684\u6a19\u6e96\uff0c\u4e5f\u70ba\u8403\u53d6\u5f0f\u6587\u5b57\u6458\u8981\u7684\u672a\u4f86\u7814\u7a76\u958b\u95e2\u4e86\u6709\u524d\u666f\u7684\u9014\u5f91\u3002", "author": "L\u00e9o Hemamou et.al.", "authors": "L\u00e9o Hemamou, Mehdi Debiane", "id": "2408.15801v1", "paper_url": "http://arxiv.org/abs/2408.15801v1", "repo": "null"}}