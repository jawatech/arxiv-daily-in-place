{"2410.17711": {"publish_time": "2024-10-23", "title": "Beware of Calibration Data for Pruning Large Language Models", "paper_summary": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5ee3\u6cdb\u61c9\u7528\u65bc\u5404\u500b\u9818\u57df\uff0c\n\u6a21\u578b\u58d3\u7e2e\u5c0d\u65bc\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u8a13\u7df4\u5f8c\u526a\u679d\u662f\u4e00\u7a2e\u5f88\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u5b83\u4e0d\u9700\u8981\u8cc7\u6e90\u5bc6\u96c6\u7684\u8fed\u4ee3\u8a13\u7df4\uff0c\u53ea\u9700\u8981\u5c11\u91cf\u6821\u6e96\u6578\u64da\u4f86\u8a55\u4f30\u53c3\u6578\u7684\u91cd\u8981\u6027\u3002\n\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u8a2d\u8a08\u5148\u9032\u7684\u526a\u679d\u65b9\u6cd5\uff0c\n\u800c\u4e0d\u540c\u6821\u6e96\u6578\u64da\u5c0d\u526a\u679d\u6548\u80fd\u7684\u5f71\u97ff\u4ecd\u7136\u7f3a\u4e4f\u7cfb\u7d71\u6027\u7684\u63a2\u8a0e\u3002\u6211\u5011\u586b\u88dc\u4e86\u9019\u500b\u7a7a\u767d\uff0c\u4e26\u9a5a\u8a1d\u5730\u89c0\u5bdf\u5230\u6821\u6e96\u6578\u64da\u7684\u5f71\u97ff\u751a\u81f3\u6bd4\u8a2d\u8a08\u5148\u9032\u7684\u526a\u679d\u7b56\u7565\u66f4\u6709\u50f9\u503c\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u9ad8\u7a00\u758f\u6027\u3002\u6211\u5011\u7684\u521d\u6b65\u63a2\u8a0e\u4e5f\u63ed\u793a\u4e86\u4f7f\u7528\u985e\u4f3c\u65bc\u8a13\u7df4\u6578\u64da\u7684\u6821\u6e96\u6578\u64da\u53ef\u4ee5\u7522\u751f\u66f4\u597d\u7684\u6548\u80fd\u3002\u7531\u65bc\u9810\u8a13\u7df4\u6578\u64da\u901a\u5e38\u7121\u6cd5\u7528\u65bc\u5148\u9032\u7684 LLM\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u4f9b\u4e86\u4e00\u500b\u81ea\u751f\u6821\u6e96\u6578\u64da\u5408\u6210\u7b56\u7565\u4f86\u5efa\u69cb\u53ef\u884c\u7684\u6821\u6e96\u6578\u64da\u3002\u6211\u5011\u5c0d\u6700\u8fd1\u7684\u5f37\u5927\u958b\u6e90 LLM\uff08\u4f8b\u5982\uff0cDCLM \u548c LLaMA-3\uff09\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u7d50\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u512a\u65bc\u5e38\u7528\u7684\u6821\u6e96\u6578\u64da\uff0c\u4e26\u4e14\u53ef\u4ee5\u6709\u6548\u589e\u5f37\u5f37\u5927\u7684\u526a\u679d\u65b9\u6cd5\uff08\u4f8b\u5982\uff0cWanda\u3001OWL\uff09\u3002", "author": "Yixin Ji et.al.", "authors": "Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang", "id": "2410.17711v1", "paper_url": "http://arxiv.org/abs/2410.17711v1", "repo": "null"}}