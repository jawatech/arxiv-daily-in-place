{"2501.09425": {"publish_time": "2025-01-16", "title": "Vision-Language Models Do Not Understand Negation", "paper_summary": "Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and 79k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 40%\nboost in accuracy on multiple-choice questions with negated captions.", "paper_summary_zh": "\u8a31\u591a\u5be6\u7528\u7684\u8996\u89ba\u8a9e\u8a00\u61c9\u7528\u7a0b\u5f0f\u9700\u8981\u6a21\u578b\u4f86\u7406\u89e3\u5426\u5b9a\uff0c\u4f8b\u5982\uff0c\u5728\u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u4f86\u64f7\u53d6\u5305\u542b\u7279\u5b9a\u7269\u4ef6\u4f46\u53c8\u4e0d\u5305\u542b\u5176\u4ed6\u7269\u4ef6\u7684\u5f71\u50cf\u6642\u3002\u5118\u7ba1\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u900f\u904e\u5927\u898f\u6a21\u8a13\u7df4\u800c\u7372\u5f97\u9032\u5c55\uff0c\u5b83\u5011\u7406\u89e3\u5426\u5b9a\u80fd\u529b\u7684\u8b70\u984c\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u4ee5\u4e0b\u554f\u984c\uff1a\u73fe\u4eca\u7684 VLM \u5c0d\u5426\u5b9a\u7684\u7406\u89e3\u7a0b\u5ea6\u5982\u4f55\uff1f\u6211\u5011\u5f15\u5165\u4e86 NegBench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u65e8\u5728\u8a55\u4f30 18 \u7a2e\u4efb\u52d9\u8b8a\u7570\u548c\u6a6b\u8de8\u5f71\u50cf\u3001\u5f71\u7247\u548c\u91ab\u7642\u8cc7\u6599\u96c6\u7684 79k \u500b\u7bc4\u4f8b\u4e2d\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002\u8a72\u57fa\u6e96\u6e2c\u8a66\u5305\u542b\u5169\u500b\u6838\u5fc3\u4efb\u52d9\uff0c\u65e8\u5728\u8a55\u4f30\u5728\u4e0d\u540c\u7684\u591a\u6a21\u614b\u8a2d\u5b9a\u4e2d\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\uff1a\u5e36\u6709\u5426\u5b9a\u7684\u64f7\u53d6\u548c\u5e36\u6709\u5426\u5b9a\u5b57\u5e55\u7684\u591a\u91cd\u9078\u64c7\u984c\u3002\u6211\u5011\u7684\u8a55\u4f30\u986f\u793a\uff0c\u73fe\u4ee3 VLM \u5728\u5426\u5b9a\u65b9\u9762\u986f\u8457\u5730\u6399\u624e\uff0c\u901a\u5e38\u8868\u73fe\u5f97\u50cf\u78b0\u904b\u6c23\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u7a2e\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u9014\u5f91\uff0c\u5176\u4e2d\u6211\u5011\u5c0d\u5305\u542b\u6578\u767e\u842c\u500b\u5426\u5b9a\u5b57\u5e55\u7684\u5927\u898f\u6a21\u5408\u6210\u8cc7\u6599\u96c6\u5fae\u8abf CLIP \u6a21\u578b\u3002\u6211\u5011\u5c55\u793a\u4e86\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u5728\u5426\u5b9a\u7684\u67e5\u8a62\u4e2d\u7522\u751f 10% \u7684\u53ec\u56de\u7387\u63d0\u5347\uff0c\u4ee5\u53ca\u5728\u5e36\u6709\u5426\u5b9a\u5b57\u5e55\u7684\u591a\u91cd\u9078\u64c7\u984c\u4e2d\u7522\u751f 40% \u7684\u6e96\u78ba\u5ea6\u63d0\u5347\u3002", "author": "Kumail Alhamoud et.al.", "authors": "Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi", "id": "2501.09425v1", "paper_url": "http://arxiv.org/abs/2501.09425v1", "repo": "null"}}