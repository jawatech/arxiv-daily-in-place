{"2407.04459": {"publish_time": "2024-07-05", "title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "paper_summary": "In this paper, we compare general-purpose pretrained models, GPT-4-Turbo and\nLlama-3-8b-Instruct with special-purpose models fine-tuned on specific tasks,\nXLM-Roberta-large, mT5-large, and Llama-3-8b-Instruct. We focus on seven\nclassification and six generation tasks to evaluate the performance of these\nmodels on Urdu language. Urdu has 70 million native speakers, yet it remains\nunderrepresented in Natural Language Processing (NLP). Despite the frequent\nadvancements in Large Language Models (LLMs), their performance in low-resource\nlanguages, including Urdu, still needs to be explored. We also conduct a human\nevaluation for the generation tasks and compare the results with the\nevaluations performed by GPT-4-Turbo and Llama-3-8b-Instruct. We find that\nspecial-purpose models consistently outperform general-purpose models across\nvarious tasks. We also find that the evaluation done by GPT-4-Turbo for\ngeneration tasks aligns more closely with human evaluation compared to the\nevaluation by Llama-3-8b-Instruct. This paper contributes to the NLP community\nby providing insights into the effectiveness of general and specific-purpose\nLLMs for low-resource languages.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6bd4\u8f03\u4e86\u901a\u7528\u9810\u8a13\u7df4\u6a21\u578b GPT-4-Turbo \u548c\nLlama-3-8b-Instruct\uff0c\u4ee5\u53ca\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u7684\u7279\u6b8a\u7528\u9014\u6a21\u578b\uff0c\nXLM-Roberta-large\u3001mT5-large \u548c Llama-3-8b-Instruct\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u4e03\u9805\n\u5206\u985e\u548c\u516d\u9805\u751f\u6210\u4efb\u52d9\uff0c\u4ee5\u8a55\u4f30\u9019\u4e9b\n\u6a21\u578b\u5728\u70cf\u723e\u90fd\u8a9e\u4e2d\u7684\u8868\u73fe\u3002\u70cf\u723e\u90fd\u8a9e\u6709 7000 \u842c\u6bcd\u8a9e\u4f7f\u7528\u8005\uff0c\u4f46\u5b83\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u4ecd\u7136\n\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u5e38\u53d6\u5f97\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5728\u4f4e\u8cc7\u6e90\n\u8a9e\u8a00\uff08\u5305\u62ec\u70cf\u723e\u90fd\u8a9e\uff09\u4e2d\u7684\u8868\u73fe\u4ecd\u6709\u5f85\u63a2\u8a0e\u3002\u6211\u5011\u9084\u5c0d\u751f\u6210\u4efb\u52d9\u9032\u884c\u4e86\u4eba\u5de5\n\u8a55\u4f30\uff0c\u4e26\u5c07\u7d50\u679c\u8207 GPT-4-Turbo \u548c Llama-3-8b-Instruct \u57f7\u884c\u7684\u8a55\u4f30\u9032\u884c\u4e86\u6bd4\u8f03\u3002\u6211\u5011\u767c\u73fe\n\u7279\u6b8a\u7528\u9014\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u59cb\u7d42\u512a\u65bc\u901a\u7528\u6a21\u578b\u3002\u6211\u5011\u9084\u767c\u73fe\uff0cGPT-4-Turbo \u5c0d\n\u751f\u6210\u4efb\u52d9\u6240\u505a\u7684\u8a55\u4f30\u8207\u4eba\u5de5\u8a55\u4f30\u76f8\u6bd4\uff0c\u8207 Llama-3-8b-Instruct \u7684\u8a55\u4f30\u66f4\u70ba\u63a5\u8fd1\u3002\u672c\u6587\u901a\u904e\u63d0\u4f9b\u5c0d\u4e00\u822c\u548c\u7279\u5b9a\u7528\u9014\nLLM \u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7684\u6709\u6548\u6027\u7684\u898b\u89e3\uff0c\u70ba NLP \u793e\u7fa4\u505a\u51fa\u8ca2\u737b\u3002</paragraph>", "author": "Samee Arif et.al.", "authors": "Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar", "id": "2407.04459v1", "paper_url": "http://arxiv.org/abs/2407.04459v1", "repo": "null"}}