{"2501.01039": {"publish_time": "2025-01-02", "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention", "paper_summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684 LLM \u5728\u5ee3\u6cdb\u7684 NLP \u4efb\u52d9\u4e2d\u5df2\u53d6\u5f97\u5353\u8d8a\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u6a19\u6e96\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u6703\u7522\u751f\u4e8c\u6b21\u6642\u9593\u8907\u96dc\u5ea6\u548c\u7dda\u6027\u589e\u52a0\u7684\u5feb\u53d6\u5927\u5c0f\u3002\u6ed1\u52d5\u8996\u7a97\u6ce8\u610f\u529b (SWA) \u900f\u904e\u5c07\u6ce8\u610f\u529b\u7bc4\u570d\u9650\u5236\u5728\u56fa\u5b9a\u5927\u5c0f\u7684\u5c40\u90e8\u5167\u5bb9\u8996\u7a97\u4e2d\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u5118\u7ba1\u5982\u6b64\uff0cSWA \u5728\u6bcf\u4e00\u5c64\u7684\u6bcf\u500b\u982d\u90e8\u90fd\u4f7f\u7528\u7d71\u4e00\u7684\u8996\u7a97\u5927\u5c0f\uff0c\u9019\u4f7f\u5f97\u5b83\u5728\u64f7\u53d6\u4e0d\u540c\u898f\u6a21\u7684\u5167\u5bb9\u6642\u6548\u7387\u4e0d\u5f70\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u591a\u5c3a\u5ea6\u8996\u7a97\u6ce8\u610f\u529b (MSWA)\uff0c\u5b83\u5728 Transformer \u4e2d\u5c0d\u982d\u90e8\u548c\u5c64\u7d1a\u5957\u7528\u4e0d\u540c\u7684\u8996\u7a97\u5927\u5c0f\u3002\u5b83\u4e0d\u50c5\u5141\u8a31\u5728\u540c\u4e00\u5c64\u4e2d\u7684\u982d\u90e8\u4e4b\u9593\u4f7f\u7528\u4e0d\u540c\u7684\u8996\u7a97\u5927\u5c0f\uff0c\u800c\u4e14\u9084\u9010\u6b65\u589e\u52a0\u5f9e\u6dfa\u5c64\u5230\u6df1\u5c64\u7684\u8996\u7a97\u5927\u5c0f\u914d\u7f6e\uff0c\u5f9e\u800c\u4f7f\u6a21\u578b\u80fd\u5920\u64f7\u53d6\u5177\u6709\u4e0d\u540c\u9577\u5ea6\u548c\u8ddd\u96e2\u7684\u5167\u5bb9\u8cc7\u8a0a\u3002\u5728\u8a9e\u8a00\u5efa\u6a21\u548c\u5e38\u8b58\u63a8\u7406\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u5be6\uff0cMSWA \u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u512a\u65bc\u50b3\u7d71\u7684\u5c40\u90e8\u6ce8\u610f\u529b\u3002", "author": "Yixing Xu et.al.", "authors": "Yixing Xu, Shivank Nag, Dong Li, Lu Tian, Emad Barsoum", "id": "2501.01039v1", "paper_url": "http://arxiv.org/abs/2501.01039v1", "repo": "null"}}