{"2405.03547": {"publish_time": "2024-05-06", "title": "Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions", "paper_summary": "Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave\nof innovation in the machine learning research domain, resulting in substantial\nimpact across diverse fields such as reinforcement learning, robotics, and\ncomputer vision. Their incorporation has been rapid and transformative, marking\na significant paradigm shift in the field of machine learning research.\n  However, the field of experimental design, grounded on black-box\noptimization, has been much less affected by such a paradigm shift, even though\nintegrating LLMs with optimization presents a unique landscape ripe for\nexploration. In this position paper, we frame the field of black-box\noptimization around sequence-based foundation models and organize their\nrelationship with previous literature. We discuss the most promising ways\nfoundational language models can revolutionize optimization, which include\nharnessing the vast wealth of information encapsulated in free-form text to\nenrich task comprehension, utilizing highly flexible sequence models such as\nTransformers to engineer superior optimization strategies, and enhancing\nperformance prediction over previously unseen search spaces.", "paper_summary_zh": "", "author": "Xingyou Song et.al.", "authors": "Xingyou Song,Yingtao Tian,Robert Tjarko Lange,Chansoo Lee,Yujin Tang,Yutian Chen", "id": "2405.03547v1", "paper_url": "http://arxiv.org/abs/2405.03547v1", "repo": "null"}}