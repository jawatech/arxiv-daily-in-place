{"2502.12859": {"publish_time": "2025-02-18", "title": "PAFT: Prompt-Agnostic Fine-Tuning", "paper_summary": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5fae\u8abf\u5f8c\u80fd\u5f88\u597d\u5730\u9069\u61c9\u4e0b\u6e38\u4efb\u52d9\uff0c\u4f46\u9019\u7a2e\u9069\u61c9\u6027\u901a\u5e38\u6703\u640d\u5bb3\u63d0\u793a\u7684\u7a69\u5065\u6027\uff0c\u56e0\u70ba\u5373\u4f7f\u5fae\u5c0f\u7684\u63d0\u793a\u8b8a\u7570\u4e5f\u6703\u5927\u5e45\u964d\u4f4e\u6548\u80fd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u63d0\u793a\u4e0d\u53ef\u77e5\u5fae\u8abf (PAFT)\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u537b\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5fae\u8abf\u671f\u9593\u52d5\u614b\u8abf\u6574\u63d0\u793a\u3002\u9019\u9f13\u52f5\u6a21\u578b\u5b78\u7fd2\u5e95\u5c64\u4efb\u52d9\u539f\u5247\uff0c\u800c\u4e0d\u662f\u904e\u5ea6\u64ec\u5408\u7279\u5b9a\u7684\u63d0\u793a\u8868\u8ff0\u3002PAFT \u5206\u70ba\u5169\u500b\u968e\u6bb5\u904b\u4f5c\uff1a\u9996\u5148\uff0c\u69cb\u5efa\u4e00\u7d44\u591a\u6a23\u5316\u3001\u6709\u610f\u7fa9\u7684\u5408\u6210\u5019\u9078\u63d0\u793a\u3002\u5176\u6b21\uff0c\u5728\u5fae\u8abf\u671f\u9593\uff0c\u5f9e\u6b64\u96c6\u5408\u4e2d\u96a8\u6a5f\u62bd\u53d6\u63d0\u793a\u4ee5\u5efa\u7acb\u52d5\u614b\u8a13\u7df4\u8f38\u5165\u3002\u91dd\u5c0d\u5404\u7a2e\u8cc7\u6599\u96c6\u548c LLM \u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u4f7f\u7528 PAFT \u8a13\u7df4\u7684\u6a21\u578b\u5728\u5404\u7a2e\u63d0\u793a\u4e2d\u8868\u73fe\u51fa\u5f37\u5927\u7684\u7a69\u5065\u6027\u548c\u6982\u62ec\u6027\uff0c\u5305\u62ec\u672a\u898b\u904e\u7684\u63d0\u793a\u3002\u9019\u7a2e\u589e\u5f37\u7684\u7a69\u5065\u6027\u540c\u6642\u6539\u5584\u4e86\u6a21\u578b\u6548\u80fd\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u6642\u7dad\u6301\u8a13\u7df4\u6548\u7387\u3002\u6d88\u878d\u7814\u7a76\u9032\u4e00\u6b65\u8b49\u5be6\u4e86 PAFT \u7684\u6709\u6548\u6027\u3002", "author": "Chenxing Wei et.al.", "authors": "Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu", "id": "2502.12859v1", "paper_url": "http://arxiv.org/abs/2502.12859v1", "repo": "null"}}