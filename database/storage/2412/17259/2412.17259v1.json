{"2412.17259": {"publish_time": "2024-12-23", "title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain", "paper_summary": "With the increasing intelligence and autonomy of LLM agents, their potential\napplications in the legal domain are becoming increasingly apparent. However,\nexisting general-domain benchmarks cannot fully capture the complexity and\nsubtle nuances of real-world judicial cognition and decision-making. Therefore,\nwe propose LegalAgentBench, a comprehensive benchmark specifically designed to\nevaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17\ncorpora from real-world legal scenarios and provides 37 tools for interacting\nwith external knowledge. We designed a scalable task construction framework and\ncarefully annotated 300 tasks. These tasks span various types, including\nmulti-hop reasoning and writing, and range across different difficulty levels,\neffectively reflecting the complexity of real-world legal scenarios. Moreover,\nbeyond evaluating final success, LegalAgentBench incorporates keyword analysis\nduring intermediate processes to calculate progress rates, enabling more\nfine-grained evaluation. We evaluated eight popular LLMs, highlighting the\nstrengths, limitations, and potential areas for improvement of existing models\nand methods. LegalAgentBench sets a new benchmark for the practical application\nof LLMs in the legal domain, with its code and data available at\n\\url{https://github.com/CSHaitao/LegalAgentBench}.", "paper_summary_zh": "\u96a8\u8457 LLM \u4ee3\u7406\u7684\u667a\u6167\u8207\u81ea\u4e3b\u6027\u65e5\u76ca\u63d0\u5347\uff0c\u5b83\u5011\u5728\u6cd5\u5f8b\u9818\u57df\u7684\u6f5b\u5728\u61c9\u7528\u4e5f\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u660e\u986f\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u901a\u7528\u9818\u57df\u57fa\u6e96\u7121\u6cd5\u5b8c\u5168\u6355\u6349\u5230\u73fe\u5be6\u4e16\u754c\u53f8\u6cd5\u8a8d\u77e5\u548c\u6c7a\u7b56\u7684\u8907\u96dc\u6027\u548c\u5fae\u5999\u5dee\u5225\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa LegalAgentBench\uff0c\u4e00\u500b\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u8a55\u4f30 LLM \u4ee3\u7406\u5728\u4e2d\u570b\u6cd5\u5f8b\u9818\u57df\u7684\u7d9c\u5408\u57fa\u6e96\u3002LegalAgentBench \u5305\u542b\u4f86\u81ea\u73fe\u5be6\u4e16\u754c\u6cd5\u5f8b\u5834\u666f\u7684 17 \u500b\u8a9e\u6599\u5eab\uff0c\u4e26\u63d0\u4f9b 37 \u500b\u8207\u5916\u90e8\u77e5\u8b58\u4e92\u52d5\u7684\u5de5\u5177\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u7684\u4efb\u52d9\u5efa\u69cb\u6846\u67b6\uff0c\u4e26\u4ed4\u7d30\u6a19\u8a3b\u4e86 300 \u500b\u4efb\u52d9\u3002\u9019\u4e9b\u4efb\u52d9\u6db5\u84cb\u5404\u7a2e\u985e\u578b\uff0c\u5305\u62ec\u591a\u8df3\u63a8\u7406\u548c\u5beb\u4f5c\uff0c\u4e26\u8de8\u8d8a\u4e0d\u540c\u7684\u96e3\u5ea6\u7b49\u7d1a\uff0c\u6709\u6548\u5730\u53cd\u6620\u4e86\u73fe\u5be6\u4e16\u754c\u6cd5\u5f8b\u5834\u666f\u7684\u8907\u96dc\u6027\u3002\u6b64\u5916\uff0c\u9664\u4e86\u8a55\u4f30\u6700\u7d42\u6210\u529f\u4e4b\u5916\uff0cLegalAgentBench \u9084\u7d50\u5408\u4e86\u4e2d\u9593\u904e\u7a0b\u4e2d\u7684\u95dc\u9375\u5b57\u5206\u6790\u4f86\u8a08\u7b97\u9032\u5ea6\u7387\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u7cbe\u7d30\u7684\u8a55\u4f30\u3002\u6211\u5011\u8a55\u4f30\u4e86\u516b\u500b\u6d41\u884c\u7684 LLM\uff0c\u7a81\u51fa\u4e86\u73fe\u6709\u6a21\u578b\u548c\u65b9\u6cd5\u7684\u512a\u52e2\u3001\u9650\u5236\u548c\u6f5b\u5728\u6539\u9032\u9818\u57df\u3002LegalAgentBench \u70ba LLM \u5728\u6cd5\u5f8b\u9818\u57df\u7684\u5be6\u969b\u61c9\u7528\u8a2d\u5b9a\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u5176\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 \\url{https://github.com/CSHaitao/LegalAgentBench} \u53d6\u5f97\u3002", "author": "Haitao Li et.al.", "authors": "Haitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu, Wuyue Wang, Yiqun Liu, Minlie Huang", "id": "2412.17259v1", "paper_url": "http://arxiv.org/abs/2412.17259v1", "repo": "https://github.com/cshaitao/legalagentbench"}}