{"2405.03131": {"publish_time": "2024-05-06", "title": "WDMoE: Wireless Distributed Large Language Models with Mixture of Experts", "paper_summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but how wireless communications can support\nLLMs has not been extensively studied. In this paper, we propose a wireless\ndistributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE,\ndeploying LLMs collaboratively across edge servers of base station (BS) and\nmobile devices in the wireless communications system. Specifically, we\ndecompose the MoE layer in LLMs by deploying the gating network and the\npreceding neural network layer at BS, while distributing the expert networks\nacross the devices. This arrangement leverages the parallel capabilities of\nexpert networks on distributed devices. Moreover, to overcome the instability\nof wireless communications, we design an expert selection policy by taking into\naccount both the performance of the model and the end-to-end latency, which\nincludes both transmission delay and inference delay. Evaluations conducted\nacross various LLMs and multiple datasets demonstrate that WDMoE not only\noutperforms existing models, such as Llama 2 with 70 billion parameters, but\nalso significantly reduces end-to-end latency.", "paper_summary_zh": "", "author": "Nan Xue et.al.", "authors": "Nan Xue,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Liang Qian,Shuguang Cui,Ping Zhang", "id": "2405.03131v1", "paper_url": "http://arxiv.org/abs/2405.03131v1", "repo": "null"}}