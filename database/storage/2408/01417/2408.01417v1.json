{"2408.01417": {"publish_time": "2024-08-02", "title": "Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs", "paper_summary": "Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.", "paper_summary_zh": "\u4eba\u985e\u5728\u4e92\u52d5\u904e\u7a0b\u4e2d\u6703\u81ea\u767c\u5730\u4f7f\u7528\u8d8a\u4f86\u8d8a\u6709\u6548\u7387\u7684\u8a9e\u8a00\uff0c\u900f\u904e\u9069\u61c9\u548c\u5f62\u6210\u81e8\u6642\u6163\u4f8b\u3002\u9019\u500b\u73fe\u8c61\u5df2\u7d93\u900f\u904e\u53c3\u8003\u904a\u6232\u5ee3\u6cdb\u5730\u7814\u7a76\u904e\uff0c\u986f\u793a\u51fa\u4eba\u985e\u8a9e\u8a00\u7684\u7279\u6027\u8d85\u8d8a\u4e86\u50b3\u905e\u610f\u5716\u3002\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u662f\u5426\u4e5f\u6703\u5728\u4e92\u52d5\u904e\u7a0b\u4e2d\u63d0\u5347\u6e9d\u901a\u6548\u7387\uff0c\u4ee5\u53ca\u5b83\u5011\u53ef\u80fd\u63a1\u7528\u4ec0\u9ebc\u6a5f\u5236\u4f86\u9054\u6210\u6b64\u76ee\u7684\uff0c\u9019\u90e8\u5206\u4ecd\u672a\u88ab\u63a2\u8a0e\u904e\u3002\u6211\u5011\u5f15\u5165\u4e86 ICCA\uff0c\u4e00\u500b\u81ea\u52d5\u5316\u67b6\u69cb\uff0c\u7528\u4f86\u8a55\u4f30\u9019\u7a2e\u5c0d\u8a71\u9069\u61c9\uff0c\u4f5c\u70ba MLLM \u7684\u60c5\u5883\u884c\u70ba\u3002\u6211\u5011\u8a55\u4f30\u4e86\u5e7e\u500b\u6700\u5148\u9032\u7684 MLLM\uff0c\u4e26\u89c0\u5bdf\u5230\uff0c\u5118\u7ba1\u5b83\u5011\u53ef\u80fd\u7406\u89e3\u5c0d\u8a71\u8005\u7684\u8a9e\u8a00\u8d8a\u4f86\u8d8a\u6709\u6548\u7387\uff0c\u4f46\u5b83\u5011\u4e26\u4e0d\u6703\u81ea\u767c\u5730\u8b93\u81ea\u5df1\u7684\u8a9e\u8a00\u96a8\u8457\u6642\u9593\u8b8a\u5f97\u66f4\u6709\u6548\u7387\u3002\u5f8c\u8005\u7684\u80fd\u529b\u53ea\u80fd\u5728\u67d0\u4e9b\u6a21\u578b\uff08\u4f8b\u5982 GPT-4\uff09\u4e2d\u900f\u904e\u5f37\u70c8\u7684\u63d0\u793a\u5f15\u767c\u3002\u9019\u986f\u793a\u51fa\u8a9e\u8a00\u4e92\u52d5\u7684\u9019\u500b\u7279\u6027\u4e26\u975e\u4f86\u81ea\u76ee\u524d\u7684\u8a13\u7df4\u6a5f\u5236\uff0c\u5118\u7ba1\u5b83\u662f\u4eba\u985e\u8a9e\u8a00\u7684\u5171\u540c\u7279\u5fb5\u3002ICCA \u53ef\u5728 https://github.com/lil-lab/ICCA \u53d6\u5f97\u3002", "author": "Yilun Hua et.al.", "authors": "Yilun Hua, Yoav Artzi", "id": "2408.01417v1", "paper_url": "http://arxiv.org/abs/2408.01417v1", "repo": "null"}}