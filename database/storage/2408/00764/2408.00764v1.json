{"2408.00764": {"publish_time": "2024-08-01", "title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation", "paper_summary": "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u57fa\u65bc\u4ee3\u7406\u5df2\u5f15\u8d77\u5ee3\u6cdb\u95dc\u6ce8\uff0c\u4e26\u6b63\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u6d41\u884c\u3002\u6b64\u5916\uff0c\u898f\u5283\u80fd\u529b\u662f LLM \u57fa\u65bc\u4ee3\u7406\u7684\u91cd\u8981\u7d44\u6210\u90e8\u5206\uff0c\u6d89\u53ca\u8207\u74b0\u5883\u4e92\u52d5\u4e26\u57f7\u884c\u52d5\u4f5c\u4ee5\u5b8c\u6210\u898f\u5283\u4efb\u52d9\uff0c\u9019\u901a\u5e38\u9700\u8981\u5f9e\u521d\u59cb\u72c0\u614b\u5be6\u73fe\u9810\u671f\u76ee\u6a19\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u901a\u904e\u6307\u4ee4\u8abf\u6574\u4f86\u589e\u5f37 LLM \u7684\u898f\u5283\u80fd\u529b\uff0c\u7a31\u70ba\u4ee3\u7406\u8a13\u7df4\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u5c08\u5bb6\u7d1a\u8ecc\u8de1\u9032\u884c\u6307\u4ee4\u8abf\u6574 LLM \u6709\u6548\u5730\u589e\u5f37\u4e86\u5176\u898f\u5283\u80fd\u529b\u3002\u7136\u800c\uff0c\u73fe\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u65bc\u5f9e\u4eba\u5de5\u8a2d\u8a08\u7684\u898f\u5283\u4efb\u52d9\u548c\u74b0\u5883\u4e2d\u5408\u6210\u8ecc\u8de1\u3002\u5275\u5efa\u9019\u4e9b\u74b0\u5883\u548c\u4efb\u52d9\u7684\u52de\u52d5\u5bc6\u96c6\u6027\u963b\u7919\u4e86\u7522\u751f\u8db3\u5920\u591a\u6a23\u5316\u548c\u5ee3\u6cdb\u7684\u8ecc\u8de1\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u672c\u6587\u63a2\u8a0e\u4e86\u591a\u6a23\u5316\u74b0\u5883\u548c\u5f9e\u5bb9\u6613\u5230\u56f0\u96e3\u7684\u9010\u6b65\u898f\u5283\u4efb\u52d9\u7684\u81ea\u52d5\u5408\u6210\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u6846\u67b6 AgentGen\uff0c\u5b83\u5229\u7528 LLM \u9996\u5148\u751f\u6210\u74b0\u5883\uff0c\u7136\u5f8c\u6839\u64da\u9019\u4e9b\u74b0\u5883\u751f\u6210\u898f\u5283\u4efb\u52d9\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u70ba\u4e86\u63d0\u9ad8\u74b0\u5883\u7684\u591a\u6a23\u6027\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u7531\u5404\u7a2e\u7279\u5b9a\u9818\u57df\u6587\u672c\u7247\u6bb5\u7d44\u6210\u7684\u9748\u611f\u8a9e\u6599\u5eab\u4f5c\u70ba\u5408\u6210\u74b0\u5883\u7684\u80cc\u666f\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u589e\u52a0\u751f\u6210\u898f\u5283\u4efb\u52d9\u7684\u96e3\u5ea6\u591a\u6a23\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u96d9\u5411\u6f14\u5316\u65b9\u6cd5 Bi-Evol\uff0c\u5b83\u5f9e\u66f4\u5bb9\u6613\u548c\u66f4\u56f0\u96e3\u7684\u65b9\u5411\u6f14\u5316\u898f\u5283\u4efb\u52d9\uff0c\u4ee5\u5408\u6210\u4e00\u500b\u5177\u6709\u66f4\u5e73\u6ed1\u96e3\u5ea6\u66f2\u7dda\u7684\u4efb\u52d9\u96c6\u3002\u5f9e AgentBoard \u884d\u751f\u7684\u8a55\u4f30\u7d50\u679c\u8868\u660e\uff0cAgentGen \u5927\u5927\u63d0\u9ad8\u4e86 LLM \u7684\u898f\u5283\u80fd\u529b\uff0c\u4f8b\u5982\uff0cAgentGen \u6307\u4ee4\u8abf\u6574\u7684 Llama-3 8B \u5728\u6574\u9ad4\u6027\u80fd\u4e0a\u8d85\u904e\u4e86 GPT-3.5\u3002\u6b64\u5916\uff0c\u5728\u67d0\u4e9b\u4efb\u52d9\u4e2d\uff0c\u5b83\u751a\u81f3\u512a\u65bc GPT-4\u3002", "author": "Mengkang Hu et.al.", "authors": "Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei Lin, Ping Luo, Saravan Rajmohan, Dongmei Zhang", "id": "2408.00764v1", "paper_url": "http://arxiv.org/abs/2408.00764v1", "repo": "null"}}