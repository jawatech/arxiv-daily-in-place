{"2405.14366": {"publish_time": "2024-05-23", "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models", "paper_summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.", "paper_summary_zh": "\u4e00\u7a2e\u6709\u6548\u7387\u5730\u90e8\u7f72\u8a08\u7b97\u9700\u6c42\u91cf\u5927\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u95dc\u9375\u65b9\u6cd5\u662f Key-Value (KV) \u5feb\u53d6\u3002KV \u5feb\u53d6\u5132\u5b58\u5148\u524d\u7522\u751f\u7684\u6b0a\u6756\u7684 key-value \u72c0\u614b\uff0c\u5927\u5e45\u6e1b\u5c11\u91cd\u8907\u904b\u7b97\u7684\u9700\u6c42\uff0c\u9032\u800c\u964d\u4f4e\u81ea\u8ff4\u6b78\u751f\u6210\u4e2d\u7684\u5ef6\u9072\u3002\u7136\u800c\uff0cKV \u5feb\u53d6\u7684\u5927\u5c0f\u6703\u96a8\u8457\u5e8f\u5217\u9577\u5ea6\u7dda\u6027\u589e\u52a0\uff0c\u5c0d\u9700\u8981\u9577\u8108\u7d61\u8f38\u5165\u548c\u5ee3\u6cdb\u5e8f\u5217\u751f\u6210\u7684\u61c9\u7528\u7a0b\u5f0f\u69cb\u6210\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba MiniCache\uff0c\u5f9e\u65b0\u7a4e\u7684\u6df1\u5ea6\u89c0\u9ede\u58d3\u7e2e\u5404\u5c64\u7684 KV \u5feb\u53d6\uff0c\u5927\u5e45\u6e1b\u5c11 LLM \u63a8\u8ad6\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u6211\u5011\u7684\u505a\u6cd5\u57fa\u65bc\u4ee5\u4e0b\u89c0\u5bdf\uff1aKV \u5feb\u53d6\u72c0\u614b\u5728 LLM \u7684\u4e2d\u5230\u6df1\u5c64\u90e8\u5206\u7684\u76f8\u9130\u5c64\u4e4b\u9593\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u6027\u3002\u70ba\u4e86\u4fc3\u9032\u5408\u4f75\uff0c\u6211\u5011\u63d0\u51fa\u5c07\u72c0\u614b\u89e3\u958b\u6210\u5927\u5c0f\u548c\u65b9\u5411\u7d44\u6210\uff0c\u5728\u4fdd\u7559\u72c0\u614b\u5411\u91cf\u9577\u5ea6\u4e0d\u8b8a\u7684\u60c5\u6cc1\u4e0b\u5167\u63d2\u72c0\u614b\u5411\u91cf\u7684\u65b9\u5411\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u6b0a\u6756\u4fdd\u7559\u7b56\u7565\uff0c\u4ee5\u4fdd\u6301\u9ad8\u5ea6\u4e0d\u540c\u7684\u72c0\u614b\u5c0d\u4e0d\u5408\u4f75\uff0c\u5f9e\u800c\u4ee5\u6700\u5c0f\u7684\u984d\u5916\u5132\u5b58\u7a7a\u9593\u958b\u92b7\u4fdd\u7559\u8cc7\u8a0a\u3002\u6211\u5011\u7684 MiniCache \u4e0d\u9700\u8981\u8a13\u7df4\u4e14\u901a\u7528\uff0c\u53ef\u88dc\u5145\u73fe\u6709\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u7b56\u7565\uff0c\u4f8b\u5982\u91cf\u5316\u548c\u7a00\u758f\u6027\u3002\u6211\u5011\u5c0d MiniCache \u9032\u884c\u4e86\u5168\u9762\u8a55\u4f30\uff0c\u5229\u7528\u5404\u7a2e\u6a21\u578b\uff0c\u5305\u62ec LLaMA-2\u3001LLaMA-3\u3001Phi-3\u3001Mistral \u548c Mixtral\uff0c\u8de8\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\uff0c\u8b49\u660e\u4e86\u5b83\u5728\u9054\u6210\u5353\u8d8a\u58d3\u7e2e\u7387\u548c\u9ad8\u8655\u7406\u91cf\u65b9\u9762\u7684\u51fa\u8272\u6548\u80fd\u3002\u5728 ShareGPT \u8cc7\u6599\u96c6\u4e0a\uff0c\u5177\u6709 4 \u4f4d\u5143 MiniCache \u7684 LLaMA-2-7B \u9054\u5230\u4e86\u9ad8\u9054 5.02 \u500d\u7684\u986f\u8457\u58d3\u7e2e\u7387\uff0c\u5c07\u63a8\u8ad6\u8655\u7406\u91cf\u63d0\u5347\u4e86\u5927\u7d04 5 \u500d\uff0c\u4e26\u5c07\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 41%\uff0c\u8207 FP16 \u5b8c\u6574\u5feb\u53d6\u57fa\u6e96\u76f8\u6bd4\uff0c\u540c\u6642\u7dad\u6301\u8fd1\u4e4e\u7121\u640d\u5931\u7684\u6548\u80fd\u3002", "author": "Akide Liu et.al.", "authors": "Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang", "id": "2405.14366v1", "paper_url": "http://arxiv.org/abs/2405.14366v1", "repo": "null"}}