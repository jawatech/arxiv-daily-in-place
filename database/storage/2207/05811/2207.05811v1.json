{"2207.05811": {"publish_time": "2022-07-12", "title": "Revealing Unfair Models by Mining Interpretable Evidence", "paper_summary": "The popularity of machine learning has increased the risk of unfair models\ngetting deployed in high-stake applications, such as justice system,\ndrug/vaccination design, and medical diagnosis. Although there are effective\nmethods to train fair models from scratch, how to automatically reveal and\nexplain the unfairness of a trained model remains a challenging task. Revealing\nunfairness of machine learning models in interpretable fashion is a critical\nstep towards fair and trustworthy AI. In this paper, we systematically tackle\nthe novel task of revealing unfair models by mining interpretable evidence\n(RUMIE). The key idea is to find solid evidence in the form of a group of data\ninstances discriminated most by the model. To make the evidence interpretable,\nwe also find a set of human-understandable key attributes and decision rules\nthat characterize the discriminated data instances and distinguish them from\nthe other non-discriminated data. As demonstrated by extensive experiments on\nmany real-world data sets, our method finds highly interpretable and solid\nevidence to effectively reveal the unfairness of trained models. Moreover, it\nis much more scalable than all of the baseline methods.", "paper_summary_zh": "", "author": "Mohit Bajaj et.al.", "authors": "Mohit Bajaj,Lingyang Chu,Vittorio Romaniello,Gursimran Singh,Jian Pei,Zirui Zhou,Lanjun Wang,Yong Zhang", "id": "2207.05811v1", "paper_url": "http://arxiv.org/abs/2207.05811v1", "repo": "null"}}