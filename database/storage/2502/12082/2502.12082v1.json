{"2502.12082": {"publish_time": "2025-02-17", "title": "AdaSplash: Adaptive Sparse Flash Attention", "paper_summary": "The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.", "paper_summary_zh": "\u57fa\u65bc softmax \u7684\u6ce8\u610f\u529b\u5728 Transformer \u4e2d\u7684\u904b\u7b97\u6210\u672c\u9650\u5236\u4e86\u5b83\u5011\u5728\u9577\u5167\u5bb9\u4efb\u52d9\u4e2d\u7684\u61c9\u7528\u6027\u3002\u9069\u61c9\u6027\u7a00\u758f\u6027\uff0c\u5176\u4e2d $\\alpha$-entmax \u6ce8\u610f\u529b\u662f\u4e00\u500b\u4f8b\u5b50\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u9748\u6d3b\u7684\u8cc7\u6599\u76f8\u95dc\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73fe\u6709\u7684\u5be6\u4f5c\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7121\u6cd5\u5229\u7528\u7a00\u758f\u6027\u4f86\u7372\u5f97\u57f7\u884c\u6642\u9593\u548c\u8a18\u61b6\u9ad4\u7684\u589e\u76ca\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 AdaSplash\uff0c\u5b83\u7d50\u5408\u4e86 GPU \u6700\u4f73\u5316\u6f14\u7b97\u6cd5\u7684\u6548\u7387\u548c $\\alpha$-entmax \u7684\u7a00\u758f\u6027\u512a\u9ede\u3002\u6211\u5011\u9996\u5148\u5f15\u5165\u4e86\u4e00\u500b\u6df7\u5408 Halley-\u4e8c\u5206\u6cd5\u6f14\u7b97\u6cd5\uff0c\u5c0e\u81f4\u8a08\u7b97 $\\alpha$-entmax \u8f49\u63db\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6578\u6e1b\u5c11\u4e86 7 \u500d\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5be6\u4f5c\u81ea\u8a02 Triton \u6838\u5fc3\uff0c\u4ee5\u6709\u6548\u8655\u7406\u9069\u61c9\u6027\u7a00\u758f\u6027\u3002\u91dd\u5c0d\u6587\u5b57\u5206\u985e\u548c\u55ae\u4e00\u5411\u91cf\u64f7\u53d6\u7684 RoBERTa \u548c ModernBERT\uff0c\u4ee5\u53ca\u7528\u65bc\u8a9e\u8a00\u5efa\u6a21\u7684 GPT-2 \u7684\u5be6\u9a57\u986f\u793a\uff0c\u8207\u73fe\u6709\u7684 $\\alpha$-entmax \u5be6\u4f5c\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u57f7\u884c\u6642\u9593\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u65b9\u9762\u7372\u5f97\u4e86\u986f\u8457\u7684\u6539\u5584\u3002\u5b83\u63a5\u8fd1\u4e86 -- \u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\u8d85\u8d8a\u4e86 -- \u9ad8\u5ea6\u6700\u4f73\u5316 softmax \u5be6\u4f5c\uff08\u4f8b\u5982 FlashAttention-2\uff09\u7684\u6548\u7387\uff0c\u540c\u6642\u5728\u7dad\u6301\u5f37\u5927\u4efb\u52d9\u6548\u80fd\u7684\u540c\u6642\uff0c\u80fd\u5920\u9032\u884c\u9577\u5167\u5bb9\u8a13\u7df4\u3002", "author": "Nuno Gon\u00e7alves et.al.", "authors": "Nuno Gon\u00e7alves, Marcos Treviso, Andr\u00e9 F. T. Martins", "id": "2502.12082v1", "paper_url": "http://arxiv.org/abs/2502.12082v1", "repo": "null"}}