{"2405.10637": {"publish_time": "2024-05-17", "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models", "paper_summary": "Huge memory consumption has been a major bottleneck for deploying\nhigh-throughput large language models in real-world applications. In addition\nto the large number of parameters, the key-value (KV) cache for the attention\nmechanism in the transformer architecture consumes a significant amount of\nmemory, especially when the number of layers is large for deep language models.\nIn this paper, we propose a novel method that only computes and caches the KVs\nof a small number of layers, thus significantly saving memory consumption and\nimproving inference throughput. Our experiments on large language models show\nthat our method achieves up to 26$\\times$ higher throughput than standard\ntransformers and competitive performance in language modeling and downstream\ntasks. In addition, our method is orthogonal to existing transformer\nmemory-saving techniques, so it is straightforward to integrate them with our\nmodel, achieving further improvement in inference efficiency. Our code is\navailable at https://github.com/whyNLP/LCKV.", "paper_summary_zh": "\u9f90\u5927\u7684\u8a18\u61b6\u9ad4\u6d88\u8017\u4e00\u76f4\u662f\u90e8\u7f72\u9ad8\u541e\u5410\u91cf\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u65bc\u5be6\u969b\u61c9\u7528\u4e2d\u7684\u4e00\u5927\u74f6\u9838\u3002\u9664\u4e86\u5927\u91cf\u7684\u53c3\u6578\u5916\uff0cTransformer\u67b6\u69cb\u4e2d\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u9375\u503c (KV) \u5feb\u53d6\u6703\u6d88\u8017\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\uff0c\u7279\u5225\u662f\u5728\u6df1\u5ea6\u8a9e\u8a00\u6a21\u578b\u4e2d\u5c64\u6578\u9f90\u5927\u7684\u60c5\u6cc1\u4e0b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u50c5\u8a08\u7b97\u4e26\u5feb\u53d6\u5c11\u6578\u5c64\u7684 KV\uff0c\u5f9e\u800c\u986f\u8457\u7bc0\u7701\u8a18\u61b6\u9ad4\u6d88\u8017\u4e26\u63d0\u9ad8\u63a8\u8ad6\u541e\u5410\u91cf\u3002\u6211\u5011\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8207\u6a19\u6e96Transformer\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u5be6\u73fe\u9ad8\u9054 26 \u500d\u7684\u541e\u5410\u91cf\uff0c\u4e14\u5728\u8a9e\u8a00\u5efa\u6a21\u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5177\u6709\u7af6\u722d\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u8207\u73fe\u6709\u7684Transformer\u7701\u8a18\u61b6\u9ad4\u6280\u8853\u6b63\u4ea4\uff0c\u56e0\u6b64\u53ef\u4ee5\u8f15\u9b06\u5730\u5c07\u5b83\u5011\u8207\u6211\u5011\u7684\u6a21\u578b\u6574\u5408\uff0c\u9032\u4e00\u6b65\u63d0\u9ad8\u63a8\u8ad6\u6548\u7387\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/whyNLP/LCKV \u53d6\u5f97\u3002", "author": "Haoyi Wu et.al.", "authors": "Haoyi Wu, Kewei Tu", "id": "2405.10637v1", "paper_url": "http://arxiv.org/abs/2405.10637v1", "repo": "null"}}