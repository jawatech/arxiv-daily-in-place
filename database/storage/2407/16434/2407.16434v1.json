{"2407.16434": {"publish_time": "2024-07-23", "title": "Enhancing LLM's Cognition via Structurization", "paper_summary": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.", "paper_summary_zh": "\u5728\u95b1\u8b80\u9577\u7bc7\u6587\u5b57\u6642\uff0c\u4eba\u985e\u8a8d\u77e5\u662f\u8907\u96dc\u4e14\u7d50\u69cb\u5316\u7684\u3002\n\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u56e0\u679c\u548c\u9806\u5e8f\u89c0\u9ede\u8655\u7406\u8f38\u5165\u80cc\u666f\uff0c\u4f46\u9019\u7a2e\u65b9\u6cd5\u53ef\u80fd\u6703\u9650\u5236\u5176\u6709\u6548\u8655\u7406\u8907\u96dc\u4e14\u932f\u7d9c\u8907\u96dc\u8f38\u5165\u7684\u80fd\u529b\u3002\u70ba\u4e86\u589e\u5f37 LLM \u7684\u8a8d\u77e5\u80fd\u529b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u80cc\u666f\u7d50\u69cb\u5316\u6982\u5ff5\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u7d14\u7cb9\u7684\u3001\u7121\u5e8f\u7684\u4e0a\u4e0b\u6587\u53e5\u5b50\u8f49\u63db\u70ba\u4e95\u7136\u6709\u5e8f\u4e14\u5c64\u6b21\u7d50\u69cb\u5316\u7684\u5143\u7d20\u3002\u85c9\u6b64\uff0cLLM \u53ef\u4ee5\u900f\u904e\u6cbf\u8457\u7d44\u7e54\u7d50\u69cb\u9032\u884c\u7cbe\u78ba\u7684\u6ce8\u610f\u548c\u8cc7\u8a0a\u641c\u5c0b\uff0c\u66f4\u597d\u5730\u638c\u63e1\u8907\u96dc\u4e14\u5ee3\u6cdb\u7684\u80cc\u666f\u3002\u5728\u5404\u7a2e\u6a21\u578b\u67b6\u69cb\u548c\u5927\u5c0f\uff08\u5305\u62ec\u5e7e\u500b 7B \u81f3 72B \u5927\u5c0f\u7684\u81ea\u8ff4\u6b78 LLM \u4ee5\u53ca\u985e\u4f3c BERT \u7684\u906e\u7f69\u6a21\u578b\uff09\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u8a55\u4f30\uff0c\u91dd\u5c0d\u5404\u7a2e NLP \u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u57fa\u65bc\u80cc\u666f\u7684\u554f\u984c\u89e3\u7b54\u3001\u8a73\u76e1\u7684\u5e7b\u89ba\u8a55\u4f30\u548c\u6bb5\u843d\u5c64\u7d1a\u7684\u5bc6\u96c6\u6aa2\u7d22\uff09\u9032\u884c\u8a55\u4f30\u3002\u7d93\u9a57\u7d50\u679c\u986f\u793a\uff0c\u55ae\u8f2a\u7d50\u69cb\u5316\u63d0\u4f9b\u4e86\u6301\u7e8c\u4e14\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u63d0\u5347\u4e86\u4e00\u500b 72B \u53c3\u6578\u7684\u958b\u6e90\u6a21\u578b\uff0c\u4ee5\u9054\u5230\u8207 GPT-3.5-Turbo \u76f8\u7576\u7684\u6548\u80fd\uff0c\u4f5c\u70ba\u5e7b\u89ba\u8a55\u4f30\u5668\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5c07\u5148\u9032 LLM \u7684\u8a9e\u8a00\u8655\u7406\u80fd\u529b\u63d0\u7149\u5230\u66f4\u5c0f\u4f46\u6709\u6548\u7684 StruXGPT-7B \u4ee5\u57f7\u884c\u7d50\u69cb\u5316\u7684\u53ef\u884c\u6027\uff0c\u89e3\u6c7a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u5be6\u7528\u6027\u3002\u4ee3\u78bc\u5c07\u5f88\u5feb\u516c\u958b\u3002", "author": "Kai Liu et.al.", "authors": "Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, Jieping Ye", "id": "2407.16434v1", "paper_url": "http://arxiv.org/abs/2407.16434v1", "repo": "null"}}