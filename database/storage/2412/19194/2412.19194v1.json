{"2412.19194": {"publish_time": "2024-12-26", "title": "Provably Efficient Exploration in Reward Machines with Low Regret", "paper_summary": "We study reinforcement learning (RL) for decision processes with\nnon-Markovian reward, in which high-level knowledge of the task in the form of\nreward machines is available to the learner. We consider probabilistic reward\nmachines with initially unknown dynamics, and investigate RL under the\naverage-reward criterion, where the learning performance is assessed through\nthe notion of regret. Our main algorithmic contribution is a model-based RL\nalgorithm for decision processes involving probabilistic reward machines that\nis capable of exploiting the structure induced by such machines. We further\nderive high-probability and non-asymptotic bounds on its regret and demonstrate\nthe gain in terms of regret over existing algorithms that could be applied, but\nobliviously to the structure. We also present a regret lower bound for the\nstudied setting. To the best of our knowledge, the proposed algorithm\nconstitutes the first attempt to tailor and analyze regret specifically for RL\nwith probabilistic reward machines.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u4e86\u5177\u6709\u975e\u99ac\u53ef\u592b\u734e\u52f5\u7684\u6c7a\u7b56\u904e\u7a0b\u7684\u5f37\u5316\u5b78\u7fd2 (RL)\uff0c\u5176\u4e2d\u4efb\u52d9\u7684\u9ad8\u968e\u77e5\u8b58\u4ee5\u734e\u52f5\u6a5f\u5668\u7684\u5f62\u5f0f\u63d0\u4f9b\u7d66\u5b78\u7fd2\u8005\u3002\u6211\u5011\u8003\u616e\u4e86\u6700\u521d\u52d5\u614b\u672a\u77e5\u7684\u6a5f\u7387\u734e\u52f5\u6a5f\u5668\uff0c\u4e26\u5728\u5e73\u5747\u734e\u52f5\u6e96\u5247\u4e0b\u7814\u7a76 RL\uff0c\u5176\u4e2d\u5b78\u7fd2\u6548\u80fd\u662f\u900f\u904e\u907a\u61be\u7684\u6982\u5ff5\u4f86\u8a55\u4f30\u3002\u6211\u5011\u7684\u4e3b\u8981\u6f14\u7b97\u6cd5\u8ca2\u737b\u662f\u4e00\u7a2e\u57fa\u65bc\u6a21\u578b\u7684 RL \u6f14\u7b97\u6cd5\uff0c\u9069\u7528\u65bc\u6d89\u53ca\u6a5f\u7387\u734e\u52f5\u6a5f\u5668\u7684\u6c7a\u7b56\u904e\u7a0b\uff0c\u80fd\u5920\u5229\u7528\u6b64\u985e\u6a5f\u5668\u6240\u5f15\u767c\u7684\u7d50\u69cb\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a8\u5c0e\u51fa\u5176\u907a\u61be\u7684\u9ad8\u6a5f\u7387\u548c\u975e\u6f38\u8fd1\u754c\u9650\uff0c\u4e26\u5c55\u793a\u51fa\u76f8\u8f03\u65bc\u53ef\u4ee5\u61c9\u7528\u4f46\u5ffd\u7565\u7d50\u69cb\u7684\u73fe\u6709\u6f14\u7b97\u6cd5\uff0c\u5728\u907a\u61be\u65b9\u9762\u7684\u6536\u76ca\u3002\u6211\u5011\u4e5f\u91dd\u5c0d\u6240\u7814\u7a76\u7684\u8a2d\u5b9a\u63d0\u51fa\u907a\u61be\u4e0b\u754c\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6240\u63d0\u51fa\u7684\u6f14\u7b97\u6cd5\u69cb\u6210\u4e86\u91dd\u5c0d RL \u91cf\u8eab\u6253\u9020\u4e26\u5206\u6790\u907a\u61be\u7684\u9996\u6b21\u5617\u8a66\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u6a5f\u7387\u734e\u52f5\u6a5f\u5668\u3002", "author": "Hippolyte Bourel et.al.", "authors": "Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi", "id": "2412.19194v1", "paper_url": "http://arxiv.org/abs/2412.19194v1", "repo": "null"}}