{"2408.12315": {"publish_time": "2024-08-22", "title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations", "paper_summary": "Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more.", "paper_summary_zh": "\u4f7f\u7528\u4e00\u7d44\u7531\u4eba\u985e\u64b0\u5beb\u7684\u793a\u7bc4\u4f86\u6307\u5c0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u662f\u4e00\u7a2e\u6539\u5584 LLM \u61c9\u7528\u7a0b\u5f0f\u7684\u5e38\u898b\u505a\u6cd5\u3002\u7136\u800c\uff0c\u4eba\u529b\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\uff0c\u7279\u5225\u662f\u5728\u5c08\u696d\u9818\u57df\uff08\u4f8b\u5982\u81e8\u5e8a\u8a3a\u65b7\uff09\u4e2d\uff0c\u800c\u4e14\u7531\u65bc\u9078\u5b9a\u7684\u793a\u7bc4\u8207\u5be6\u969b\u6e2c\u8a66\u5be6\u4f8b\u4e4b\u9593\u76ee\u6a19\u6280\u80fd\u7684\u6f5b\u5728\u5dee\u7570\uff0c\u4e26\u4e0d\u80fd\u4fdd\u8b49\u6700\u4f73\u6548\u80fd\u3002\u57fa\u65bc\u9019\u4e9b\u52d5\u6a5f\uff0c\u672c\u6587\u63a2\u8a0e\u4e86\u81ea\u52d5\u5efa\u7acb\u81ea\u8a02\u793a\u7bc4\uff0c\u5176\u76ee\u6a19\u6280\u80fd\u8207\u7d66\u5b9a\u7684\u76ee\u6a19\u5be6\u4f8b\u4e00\u81f4\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u554f\u984c\u89e3\u6c7a\u67b6\u69cb SELF-TAUGHT\uff0c\u5b83\u53ef\u4ee5\u4fc3\u9032\u300c\u91dd\u5c0d\u300d\u76ee\u6a19\u554f\u984c\u300c\u7be9\u9078\u300d\u51fa\u66f4\u9ad8\u54c1\u8cea\uff08\u5373\u6b63\u78ba\u6027\uff09\u7684\u793a\u7bc4\uff0c\u4e14\u63a1\u7528\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u3002\u5728\u591a\u500b\u9818\u57df\u7684\u591a\u9078\u984c\u4efb\u52d9\u548c\u5c0d\u771f\u5be6\u4e16\u754c\u60a3\u8005\u9032\u884c\u963f\u8332\u6d77\u9ed8\u75c7 (AD) \u8a3a\u65b7\u7684 15 \u9805\u4efb\u52d9\u4e2d\uff0cSELF-TAUGHT \u9054\u5230\u4e86\u512a\u65bc\u5f37\u5927\u57fa\u6e96\uff08\u4f8b\u5982 Few-shot CoT\u3001Plan-and-Solve\u3001Auto-CoT\uff09\u7684\u6548\u80fd\u3002\u6211\u5011\u5c0d SELF-TAUGHT \u9032\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5305\u62ec\u5176\u5c0d\u73fe\u6709\u63d0\u793a\u65b9\u6cd5\u548c\u4e0d\u540c LLM \u7684\u6982\u62ec\u6027\u3001\u5176\u4e2d\u9593\u7522\u751f\u7684\u54c1\u8cea\u7b49\u7b49\u3002", "author": "Kai Tzu-iunn Ong et.al.", "authors": "Kai Tzu-iunn Ong, Taeyoon Kwon, Jinyoung Yeo", "id": "2408.12315v1", "paper_url": "http://arxiv.org/abs/2408.12315v1", "repo": "null"}}