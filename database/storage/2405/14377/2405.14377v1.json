{"2405.14377": {"publish_time": "2024-05-23", "title": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization", "paper_summary": "Training large AI models such as deep learning recommendation systems and\nfoundation language (or multi-modal) models costs massive GPUs and computing\ntime. The high training cost has become only affordable to big tech companies,\nmeanwhile also causing increasing concerns about the environmental impact. This\npaper presents CoMERA, a Computing- and Memory-Efficient training method via\nRank-Adaptive tensor optimization. CoMERA achieves end-to-end rank-adaptive\ntensor-compressed training via a multi-objective optimization formulation, and\nimproves the training to provide both a high compression ratio and excellent\naccuracy in the training process. Our optimized numerical computation (e.g.,\noptimized tensorized embedding and tensor-vector contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. With further HPC optimization, CoMERA may significantly\nreduce the training cost of large language models.", "paper_summary_zh": "\u8a13\u7df4\u5927\u578b AI \u6a21\u578b\uff0c\u4f8b\u5982\u6df1\u5ea6\u5b78\u7fd2\u63a8\u85a6\u7cfb\u7d71\u548c\u57fa\u790e\u8a9e\u8a00\uff08\u6216\u591a\u6a21\u614b\uff09\u6a21\u578b\uff0c\u9700\u8981\u5927\u91cf\u7684 GPU \u548c\u904b\u7b97\u6642\u9593\u3002\u9ad8\u6602\u7684\u8a13\u7df4\u6210\u672c\u5df2\u6210\u70ba\u53ea\u6709\u5927\u578b\u79d1\u6280\u516c\u53f8\u8ca0\u64d4\u5f97\u8d77\uff0c\u540c\u6642\u4e5f\u5f15\u8d77\u4eba\u5011\u5c0d\u74b0\u5883\u5f71\u97ff\u7684\u65e5\u76ca\u95dc\u6ce8\u3002\u672c\u6587\u63d0\u51fa CoMERA\uff0c\u4e00\u7a2e\u900f\u904e\u79e9\u81ea\u9069\u61c9\u5f35\u91cf\u6700\u4f73\u5316\u9032\u884c\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9ad8\u6548\u8a13\u7df4\u7684\u65b9\u6cd5\u3002CoMERA \u900f\u904e\u591a\u76ee\u6a19\u6700\u4f73\u5316\u516c\u5f0f\u9054\u6210\u7aef\u5c0d\u7aef\u79e9\u81ea\u9069\u61c9\u5f35\u91cf\u58d3\u7e2e\u8a13\u7df4\uff0c\u4e26\u6539\u5584\u8a13\u7df4\u4ee5\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u63d0\u4f9b\u9ad8\u58d3\u7e2e\u6bd4\u548c\u512a\u7570\u7684\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u6700\u4f73\u5316\u7684\u6578\u503c\u904b\u7b97\uff08\u4f8b\u5982\u6700\u4f73\u5316\u7684\u5f35\u91cf\u5316\u5d4c\u5165\u548c\u5f35\u91cf\u5411\u91cf\u6536\u7e2e\uff09\u548c GPU \u5be6\u4f5c\u6d88\u9664\u4e86\u90e8\u5206\u5f35\u91cf\u5316\u8a13\u7df4\u5728 GPU \u4e0a\u7684\u57f7\u884c\u6642\u9593\u958b\u92b7\u3002\u9019\u5c0e\u81f4\u6bcf\u8a13\u7df4\u9031\u671f\u901f\u5ea6\u63d0\u5347 $2-3\\times$\uff0c\u8207\u6a19\u6e96\u8a13\u7df4\u76f8\u6bd4\u662f\u9996\u6b21\u3002CoMERA \u5728\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u6548\u7387\u65b9\u9762\u4e5f\u512a\u65bc\u6700\u8fd1\u7684 GaLore\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cCoMERA \u6bcf\u500b\u8a13\u7df4\u9031\u671f\u5feb $2\\times$\uff0c\u8a18\u61b6\u9ad4\u6548\u7387\u6bd4\u5728\u55ae\u4e00\u6279\u6b21\u8a13\u7df4\u4e2d\u6e2c\u8a66\u7684\u516d\u500b\u7de8\u78bc\u5668Transformer\u9ad8 $9\\times$\u3002\u900f\u904e\u9032\u4e00\u6b65\u7684 HPC \u6700\u4f73\u5316\uff0cCoMERA \u53ef\u4ee5\u5927\u5e45\u964d\u4f4e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u8a13\u7df4\u6210\u672c\u3002", "author": "Zi Yang et.al.", "authors": "Zi Yang, Samridhi Choudhary, Xinfeng Xie, Cao Gao, Siegfried Kunzmann, Zheng Zhang", "id": "2405.14377v1", "paper_url": "http://arxiv.org/abs/2405.14377v1", "repo": "null"}}