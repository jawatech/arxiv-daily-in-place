{"2502.12771": {"publish_time": "2025-02-18", "title": "Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach", "paper_summary": "Self-supervised language and audio models effectively predict brain responses\nto speech. However, traditional prediction models rely on linear mappings from\nunimodal features, despite the complex integration of auditory signals with\nlinguistic and semantic information across widespread brain networks during\nspeech comprehension. Here, we introduce a nonlinear, multimodal prediction\nmodel that combines audio and linguistic features from pre-trained models\n(e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in\nprediction performance (unnormalized and normalized correlation) over\ntraditional unimodal linear models, as well as a 7.7% and 14.4% improvement,\nrespectively, over prior state-of-the-art models. These improvements represent\na major step towards future robust in-silico testing and improved decoding\nperformance. They also reveal how auditory and semantic information are fused\nin motor, somatosensory, and higher-level semantic regions, aligning with\nexisting neurolinguistic theories. Overall, our work highlights the often\nneglected potential of nonlinear and multimodal approaches to brain modeling,\npaving the way for future studies to embrace these strategies in naturalistic\nneurolinguistics research.", "paper_summary_zh": "\u81ea\u6211\u76e3\u7763\u7684\u8a9e\u8a00\u548c\u97f3\u8a0a\u6a21\u578b\u6709\u6548\u9810\u6e2c\u5927\u8166\u5c0d\u8a9e\u8a00\u7684\u53cd\u61c9\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684\u9810\u6e2c\u6a21\u578b\u4f9d\u8cf4\u65bc\u55ae\u6a21\u614b\u7279\u5fb5\u7684\u7dda\u6027\u6620\u5c04\uff0c\u5118\u7ba1\u5728\u8a9e\u8a00\u7406\u89e3\u904e\u7a0b\u4e2d\uff0c\u807d\u89ba\u4fe1\u865f\u8207\u8a9e\u8a00\u548c\u8a9e\u7fa9\u8cc7\u8a0a\u5728\u5ee3\u6cdb\u7684\u8166\u7db2\u8def\u4e2d\u9032\u884c\u8907\u96dc\u7684\u6574\u5408\u3002\u5728\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u975e\u7dda\u6027\u3001\u591a\u6a21\u614b\u9810\u6e2c\u6a21\u578b\uff0c\u7d50\u5408\u9810\u5148\u8a13\u7df4\u6a21\u578b\uff08\u4f8b\u5982\uff0cLLAMA\u3001Whisper\uff09\u4e2d\u7684\u97f3\u8a0a\u548c\u8a9e\u8a00\u7279\u5fb5\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u9810\u6e2c\u6548\u80fd\u4e0a\uff08\u672a\u6b63\u898f\u5316\u548c\u6b63\u898f\u5316\u76f8\u95dc\u6027\uff09\u5206\u5225\u6bd4\u50b3\u7d71\u7684\u55ae\u6a21\u614b\u7dda\u6027\u6a21\u578b\u63d0\u5347\u4e86 17.2% \u548c 17.9%\uff0c\u5206\u5225\u6bd4\u5148\u524d\u7684\u6700\u5148\u9032\u6a21\u578b\u63d0\u5347\u4e86 7.7% \u548c 14.4%\u3002\u9019\u4e9b\u6539\u9032\u4ee3\u8868\u4e86\u672a\u4f86\u7a69\u5065\u7684\u96fb\u8166\u6a21\u64ec\u6e2c\u8a66\u548c\u6539\u9032\u7684\u89e3\u78bc\u6548\u80fd\u9081\u51fa\u4e86\u4e00\u5927\u6b65\u3002\u5b83\u5011\u4e5f\u63ed\u793a\u4e86\u807d\u89ba\u548c\u8a9e\u7fa9\u8cc7\u8a0a\u5982\u4f55\u5728\u904b\u52d5\u3001\u9ad4\u611f\u548c\u66f4\u9ad8\u5c64\u6b21\u7684\u8a9e\u7fa9\u5340\u57df\u4e2d\u878d\u5408\uff0c\u8207\u73fe\u6709\u7684\u795e\u7d93\u8a9e\u8a00\u5b78\u7406\u8ad6\u4e00\u81f4\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u7814\u7a76\u7a81\u51fa\u4e86\u975e\u7dda\u6027\u548c\u591a\u6a21\u614b\u5927\u8166\u5efa\u6a21\u65b9\u6cd5\u7d93\u5e38\u88ab\u5ffd\u7565\u7684\u6f5b\u529b\uff0c\u70ba\u672a\u4f86\u7814\u7a76\u5728\u81ea\u7136\u4e3b\u7fa9\u795e\u7d93\u8a9e\u8a00\u5b78\u7814\u7a76\u4e2d\u63a1\u7528\u9019\u4e9b\u7b56\u7565\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Danny Dongyeop Han et.al.", "authors": "Danny Dongyeop Han, Yunju Cho, Jiook Cha, Jay-Yoon Lee", "id": "2502.12771v1", "paper_url": "http://arxiv.org/abs/2502.12771v1", "repo": "null"}}