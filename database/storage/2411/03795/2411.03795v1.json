{"2411.03795": {"publish_time": "2024-11-06", "title": "VQA$^2$:Visual Question Answering for Video Quality Assessment", "paper_summary": "The advent and proliferation of large multi-modal models (LMMs) have\nintroduced a new paradigm to video-related computer vision fields, including\ntraining and inference methods based on visual question answering (VQA). These\nmethods enable models to handle multiple downstream tasks robustly. Video\nQuality Assessment (VQA), a classic field in low-level visual quality\nevaluation, originally focused on quantitative video quality scoring. However,\ndriven by advances in LMMs, it is now evolving towards more comprehensive\nvisual quality understanding tasks. Visual question answering has significantly\nimproved low-level visual evaluation within the image domain recently. However,\nrelated work is almost nonexistent in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset the first visual question answering instruction dataset entirely\nfocuses on video quality assessment, and based on it, we propose the VQA2\nseries models The VQA2 Instruction Dataset consists of three stages and covers\nvarious video types, containing 157,735 instruction question-answer pairs,\nincluding both manually annotated and synthetic data. We conduct extensive\nexperiments on both video quality scoring and video quality understanding\ntasks. Results demonstrate that the VQA2 series models achieve state-of-the-art\n(SOTA) performance in quality scoring tasks, and their performance in visual\nquality question answering surpasses the renowned GPT-4o. Additionally, our\nfinal model, the VQA2-Assistant, performs well across both scoring and\nquestion-answering tasks, validating its versatility.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u51fa\u73b0\u548c\u666e\u53ca\u4e3a\u89c6\u9891\u76f8\u5173\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\uff0c\u5305\u62ec\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54 (VQA) \u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u7a33\u5065\u5730\u5904\u7406\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30 (VQA) \u662f\u4f4e\u7ea7\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u4e00\u4e2a\u7ecf\u5178\u9886\u57df\uff0c\u6700\u521d\u4e13\u6ce8\u4e8e\u5b9a\u91cf\u89c6\u9891\u8d28\u91cf\u8bc4\u5206\u3002\u7136\u800c\uff0c\u5728 LMM \u7684\u63a8\u52a8\u4e0b\uff0c\u5b83\u73b0\u5728\u6b63\u671d\u7740\u66f4\u5168\u9762\u7684\u89c6\u89c9\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u53d1\u5c55\u3002\u89c6\u89c9\u95ee\u7b54\u6700\u8fd1\u663e\u7740\u6539\u5584\u4e86\u56fe\u50cf\u57df\u4e2d\u7684\u4f4e\u7ea7\u89c6\u89c9\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u5728\u89c6\u9891\u57df\u4e2d\u51e0\u4e4e\u4e0d\u5b58\u5728\u76f8\u5173\u5de5\u4f5c\uff0c\u7559\u4e0b\u4e86\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 VQA2 \u6307\u4ee4\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u4e13\u6ce8\u4e8e\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u7684\u89c6\u89c9\u95ee\u7b54\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VQA2 \u7cfb\u5217\u6a21\u578b\u3002VQA2 \u6307\u4ee4\u6570\u636e\u96c6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff0c\u6db5\u76d6\u5404\u79cd\u89c6\u9891\u7c7b\u578b\uff0c\u5305\u542b 157,735 \u4e2a\u6307\u4ee4\u95ee\u7b54\u5bf9\uff0c\u5305\u62ec\u4eba\u5de5\u6ce8\u91ca\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u3002\u6211\u4eec\u5bf9\u89c6\u9891\u8d28\u91cf\u8bc4\u5206\u548c\u89c6\u9891\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cVQA2 \u7cfb\u5217\u6a21\u578b\u5728\u8d28\u91cf\u8bc4\u5206\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u89c6\u89c9\u8d28\u91cf\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\u8d85\u8fc7\u4e86\u8457\u540d\u7684 GPT-4o\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6700\u7ec8\u6a21\u578b VQA2-Assistant \u5728\u8bc4\u5206\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86\u5b83\u7684\u591a\u529f\u80fd\u6027\u3002", "author": "Ziheng Jia et.al.", "authors": "Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min", "id": "2411.03795v1", "paper_url": "http://arxiv.org/abs/2411.03795v1", "repo": "null"}}