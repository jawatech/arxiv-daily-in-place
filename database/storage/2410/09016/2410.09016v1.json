{"2410.09016": {"publish_time": "2024-10-11", "title": "Parameter-Efficient Fine-Tuning of State Space Models", "paper_summary": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA.", "paper_summary_zh": "\u6df1\u5ea6\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\uff0c\u4f8b\u5982 Mamba (Gu & Dao, 2024)\uff0c\u5df2\u6210\u70ba\u8a9e\u8a00\u5efa\u6a21\u7684\u5f37\u5927\u5de5\u5177\uff0c\u63d0\u4f9b\u9ad8\u6548\u63a8\u7406\u548c\u7dda\u6027\u5e8f\u5217\u9577\u5ea6\u7e2e\u653e\u7684\u9ad8\u6548\u80fd\u3002\u7136\u800c\uff0c\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u5728\u57fa\u65bc SSM \u7684\u6a21\u578b\u4e2d\u7684\u61c9\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7d71\u6027\u5730\u7814\u7a76\u5169\u500b\u95dc\u9375\u554f\u984c\uff1a(i) \u73fe\u6709\u7684 PEFT \u65b9\u6cd5\u5728\u57fa\u65bc SSM \u7684\u6a21\u578b\u4e0a\u8868\u73fe\u5982\u4f55\uff1f(ii) \u54ea\u4e9b\u6a21\u7d44\u5c0d\u65bc\u5fae\u8abf\u6700\u6709\u6548\uff1f\u6211\u5011\u5c0d\u57fa\u65bc SSM \u7684\u6a21\u578b\u9032\u884c\u4e86\u56db\u7a2e\u57fa\u672c PEFT \u65b9\u6cd5\u7684\u7d93\u9a57\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u57fa\u65bc\u63d0\u793a\u7684\u65b9\u6cd5\uff08\u4f8b\u5982\u524d\u7db4\u5fae\u8abf\uff09\u4e0d\u518d\u6709\u6548\uff0c\u7406\u8ad6\u5206\u6790\u9032\u4e00\u6b65\u652f\u6301\u4e86\u9019\u4e00\u7d93\u9a57\u7d50\u679c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cLoRA \u4ecd\u7136\u9069\u7528\u65bc\u57fa\u65bc SSM \u7684\u6a21\u578b\u3002\u6211\u5011\u9032\u4e00\u6b65\u7814\u7a76\u4e86 LoRA \u5728\u9019\u4e9b\u6a21\u578b\u4e2d\u7684\u6700\u4f73\u61c9\u7528\uff0c\u5728\u7406\u8ad6\u548c\u5be6\u9a57\u4e0a\u8b49\u660e\u4e86\u5c07 LoRA \u61c9\u7528\u65bc\u7dda\u6027\u6295\u5f71\u77e9\u9663\u800c\u4e0d\u4fee\u6539 SSM \u6a21\u7d44\u6703\u7522\u751f\u6700\u4f73\u7d50\u679c\uff0c\u56e0\u70ba LoRA \u5728\u8abf\u6574 SSM \u6a21\u7d44\u65b9\u9762\u7121\u6548\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5177\u6709\u9078\u64c7\u6027\u7dad\u5ea6\u8abf\u6574 (SDLoRA) \u7684 LoRA\uff0c\u5b83\u5728\u5c07 LoRA \u61c9\u7528\u65bc\u7dda\u6027\u6295\u5f71\u77e9\u9663\u7684\u540c\u6642\u6709\u9078\u64c7\u5730\u66f4\u65b0 SSM \u6a21\u7d44\u4e0a\u7684\u67d0\u4e9b\u901a\u9053\u548c\u72c0\u614b\u3002\u5927\u91cf\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u9019\u7a2e\u65b9\u6cd5\u512a\u65bc\u6a19\u6e96\u7684 LoRA\u3002", "author": "Kevin Galim et.al.", "authors": "Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee", "id": "2410.09016v1", "paper_url": "http://arxiv.org/abs/2410.09016v1", "repo": "https://github.com/furiosa-ai/ssm-peft"}}