{"2406.04313": {"publish_time": "2024-06-06", "title": "Improving Alignment and Robustness with Short Circuiting", "paper_summary": "AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that \"short-circuits\" models as they respond with harmful outputs.\nExisting techniques aimed at improving alignment, such as refusal training, are\noften bypassed. Techniques such as adversarial training try to plug these holes\nby countering specific attacks. As an alternative to refusal training and\nadversarial training, short-circuiting directly controls the representations\nthat are responsible for harmful outputs in the first place. Our technique can\nbe applied to both text-only and multimodal language models to prevent the\ngeneration of harmful outputs without sacrificing utility -- even in the\npresence of powerful unseen attacks. Notably, while adversarial robustness in\nstandalone image recognition remains an open challenge, short-circuiting allows\nthe larger multimodal system to reliably withstand image \"hijacks\" that aim to\nproduce harmful content. Finally, we extend our approach to AI agents,\ndemonstrating considerable reductions in the rate of harmful actions when they\nare under attack. Our approach represents a significant step forward in the\ndevelopment of reliable safeguards to harmful behavior and adversarial attacks.", "paper_summary_zh": "AI \u7cfb\u7d71\u53ef\u80fd\u6703\u63a1\u53d6\u6709\u5bb3\u7684\u884c\u52d5\uff0c\u800c\u4e14\u6975\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u9748\u611f\u4f86\u81ea\u6700\u8fd1\u5728\u8868\u793a\u5de5\u7a0b\u65b9\u9762\u7684\u9032\u5c55\uff0c\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u300c\u77ed\u8def\u300d\u6a21\u578b\uff0c\u56e0\u70ba\u5b83\u5011\u6703\u7522\u751f\u6709\u5bb3\u7684\u8f38\u51fa\u3002\u65e8\u5728\u6539\u5584\u5c0d\u9f4a\u7684\u73fe\u6709\u6280\u8853\uff0c\u4f8b\u5982\u62d2\u7d55\u8a13\u7df4\uff0c\u901a\u5e38\u6703\u88ab\u7e5e\u904e\u3002\u5c0d\u6297\u6027\u8a13\u7df4\u7b49\u6280\u8853\u8a66\u5716\u901a\u904e\u53cd\u5236\u7279\u5b9a\u653b\u64ca\u4f86\u586b\u88dc\u9019\u4e9b\u6f0f\u6d1e\u3002\u4f5c\u70ba\u62d2\u7d55\u8a13\u7df4\u548c\u5c0d\u6297\u6027\u8a13\u7df4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u77ed\u8def\u76f4\u63a5\u63a7\u5236\u6700\u521d\u5c0d\u6709\u5bb3\u8f38\u51fa\u8ca0\u8cac\u7684\u8868\u793a\u3002\u6211\u5011\u7684\u6280\u8853\u53ef\u4ee5\u61c9\u7528\u65bc\u7d14\u6587\u672c\u548c\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u9632\u6b62\u7522\u751f\u6709\u5bb3\u8f38\u51fa\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u6548\u7528\u2014\u2014\u5373\u4f7f\u5728\u5b58\u5728\u5f37\u5927\u7684\u672a\u77e5\u653b\u64ca\u7684\u60c5\u6cc1\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5118\u7ba1\u7368\u7acb\u5716\u50cf\u8b58\u5225\u4e2d\u7684\u5c0d\u6297\u6027\u9b6f\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u6c7a\u7684\u6311\u6230\uff0c\u4f46\u77ed\u8def\u5141\u8a31\u66f4\u5927\u7684\u591a\u6a21\u614b\u7cfb\u7d71\u53ef\u9760\u5730\u627f\u53d7\u65e8\u5728\u7522\u751f\u6709\u5bb3\u5167\u5bb9\u7684\u5716\u50cf\u300c\u52ab\u6301\u300d\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u6280\u8853\u64f4\u5c55\u5230 AI \u4ee3\u7406\uff0c\u8b49\u660e\u4e86\u5728\u53d7\u5230\u653b\u64ca\u6642\u6709\u5bb3\u884c\u70ba\u7684\u767c\u751f\u7387\u5927\u5e45\u964d\u4f4e\u3002\u6211\u5011\u7684\u505a\u6cd5\u4ee3\u8868\u4e86\u5728\u958b\u767c\u91dd\u5c0d\u6709\u5bb3\u884c\u70ba\u548c\u5c0d\u6297\u6027\u653b\u64ca\u7684\u53ef\u9760\u9632\u8b77\u63aa\u65bd\u65b9\u9762\u9081\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002", "author": "Andy Zou et.al.", "authors": "Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks", "id": "2406.04313v1", "paper_url": "http://arxiv.org/abs/2406.04313v1", "repo": "null"}}