{"2409.09007": {"publish_time": "2024-09-13", "title": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity", "paper_summary": "Learning representations on large graphs is a long-standing challenge due to\nthe inter-dependence nature. Transformers recently have shown promising\nperformance on small graphs thanks to its global attention for capturing\nall-pair interactions beyond observed structures. Existing approaches tend to\ninherit the spirit of Transformers in language and vision tasks, and embrace\ncomplicated architectures by stacking deep attention-based propagation layers.\nIn this paper, we attempt to evaluate the necessity of adopting multi-layer\nattentions in Transformers on graphs, which considerably restricts the\nefficiency. Specifically, we analyze a generic hybrid propagation layer,\ncomprised of all-pair attention and graph-based propagation, and show that\nmulti-layer propagation can be reduced to one-layer propagation, with the same\ncapability for representation learning. It suggests a new technical path for\nbuilding powerful and efficient Transformers on graphs, particularly through\nsimplifying model architectures without sacrificing expressiveness. As\nexemplified by this work, we propose a Simplified Single-layer Graph\nTransformers (SGFormer), whose main component is a single-layer global\nattention that scales linearly w.r.t. graph sizes and requires none of any\napproximation for accommodating all-pair interactions. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M, yielding\norders-of-magnitude inference acceleration over peer Transformers on\nmedium-sized graphs, and demonstrates competitiveness with limited labeled\ndata.", "paper_summary_zh": "\u5728\u5927\u578b\u5716\u8868\u4e0a\u5b78\u7fd2\u8868\u5fb5\u7531\u65bc\u76f8\u4e92\u4f9d\u8cf4\u7684\u6027\u8cea\u800c\u6210\u70ba\u4e00\u9805\u9577\u671f\u7684\u6311\u6230\u3002\u7531\u65bc Transfomer \u80fd\u5920\u91dd\u5c0d\u6240\u6709\u6210\u5c0d\u4e92\u52d5\u9032\u884c\u5168\u5c40\u95dc\u6ce8\uff0c\u8d85\u8d8a\u89c0\u6e2c\u7d50\u69cb\uff0c\u56e0\u6b64\u6700\u8fd1\u5728\u5c0f\u578b\u5716\u8868\u4e0a\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\u3002\u73fe\u6709\u7684\u65b9\u6cd5\u50be\u5411\u65bc\u7e7c\u627f Transformer \u5728\u8a9e\u8a00\u548c\u8996\u89ba\u4efb\u52d9\u4e2d\u7684\u7cbe\u795e\uff0c\u4e26\u901a\u904e\u5806\u758a\u57fa\u65bc\u6df1\u5ea6\u95dc\u6ce8\u7684\u50b3\u64ad\u5c64\u4f86\u63a1\u7528\u8907\u96dc\u7684\u67b6\u69cb\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5617\u8a66\u8a55\u4f30\u5728\u5716\u8868\u4e0a\u63a1\u7528\u591a\u5c64\u6ce8\u610f\u529b Transformer \u7684\u5fc5\u8981\u6027\uff0c\u9019\u6975\u5927\u5730\u9650\u5236\u4e86\u6548\u7387\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5206\u6790\u4e86\u4e00\u500b\u901a\u7528\u7684\u6df7\u5408\u50b3\u64ad\u5c64\uff0c\u5b83\u5305\u542b\u6240\u6709\u6210\u5c0d\u6ce8\u610f\u529b\u548c\u57fa\u65bc\u5716\u8868\u7684\u50b3\u64ad\uff0c\u4e26\u8868\u660e\u591a\u5c64\u50b3\u64ad\u53ef\u4ee5\u7c21\u5316\u70ba\u55ae\u5c64\u50b3\u64ad\uff0c\u5177\u6709\u76f8\u540c\u7684\u8868\u5fb5\u5b78\u7fd2\u80fd\u529b\u3002\u9019\u70ba\u5728\u5716\u8868\u4e0a\u69cb\u5efa\u5f37\u5927\u800c\u9ad8\u6548\u7684 Transformer \u63d0\u4f9b\u4e86\u4e00\u689d\u65b0\u7684\u6280\u8853\u8def\u5f91\uff0c\u7279\u5225\u662f\u901a\u904e\u7c21\u5316\u6a21\u578b\u67b6\u69cb\uff0c\u800c\u7121\u9700\u72a7\u7272\u8868\u9054\u80fd\u529b\u3002\u6b63\u5982\u9019\u9805\u5de5\u4f5c\u6240\u4f8b\u8b49\u7684\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u5316\u7684\u55ae\u5c64\u5716\u5f62 Transformer (SGFormer)\uff0c\u5176\u4e3b\u8981\u7d44\u6210\u90e8\u5206\u662f\u4e00\u500b\u55ae\u5c64\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u5b83\u8207\u5716\u5f62\u5927\u5c0f\u6210\u7dda\u6027\u6bd4\u4f8b\uff0c\u4e26\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u8fd1\u4f3c\u4f86\u9069\u61c9\u6240\u6709\u6210\u5c0d\u4e92\u52d5\u3002\u6839\u64da\u7d93\u9a57\uff0cSGFormer \u6210\u529f\u5730\u64f4\u5c55\u5230\u7db2\u8def\u898f\u6a21\u7684\u5716\u8868 ogbn-papers100M\uff0c\u5728\u4e2d\u7b49\u5927\u5c0f\u7684\u5716\u8868\u4e0a\u7522\u751f\u4e86\u6bd4\u540c\u5115 Transformer \u5feb\u5e7e\u500b\u6578\u91cf\u7d1a\u7684\u63a8\u8ad6\u52a0\u901f\uff0c\u4e26\u8b49\u660e\u4e86\u5728\u6a19\u7c64\u8cc7\u6599\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\u5177\u6709\u7af6\u722d\u529b\u3002", "author": "Qitian Wu et.al.", "authors": "Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan", "id": "2409.09007v1", "paper_url": "http://arxiv.org/abs/2409.09007v1", "repo": "https://github.com/qitianwu/sgformer"}}