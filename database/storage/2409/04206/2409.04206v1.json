{"2409.04206": {"publish_time": "2024-09-06", "title": "Fast Forwarding Low-Rank Training", "paper_summary": "Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to\nreduce the computational costs of finetuning pretrained Language Models (LMs).\nEnabled by these low-rank settings, we propose an even more efficient\noptimization strategy: Fast Forward, a simple and effective approach to\naccelerate large segments of training. In a Fast Forward stage, we repeat the\nmost recent optimizer step until the loss stops improving on a tiny validation\nset. By alternating between regular optimization steps and Fast Forward stages,\nFast Forward provides up to an 87\\% reduction in FLOPs and up to an 81\\%\nreduction in train time over standard SGD with Adam. We validate Fast Forward\nby finetuning various models on different tasks and demonstrate that it speeds\nup training without compromising model performance. Additionally, we analyze\nwhen and how to apply Fast Forward.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf\u65b9\u6cd5\uff0c\u4f8b\u5982\u4f4e\u968e\u6539\u7de8 (LoRA)\uff0c\u65e8\u5728\u964d\u4f4e\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (LM) \u5fae\u8abf\u7684\u8a08\u7b97\u6210\u672c\u3002\u5728\u9019\u4e9b\u4f4e\u968e\u8a2d\u5b9a\u7684\u52a0\u6301\u4e0b\uff0c\u6211\u5011\u63d0\u51fa\u66f4\u6709\u6548\u7387\u7684\u6700\u4f73\u5316\u7b56\u7565\uff1a\u5feb\u8f49\uff0c\u4e00\u7a2e\u52a0\u901f\u5927\u90e8\u5206\u8a13\u7df4\u7684\u7c21\u55ae\u6709\u6548\u65b9\u6cd5\u3002\u5728\u5feb\u8f49\u968e\u6bb5\uff0c\u6211\u5011\u91cd\u8907\u6700\u8fd1\u7684\u6700\u4f73\u5316\u6b65\u9a5f\uff0c\u76f4\u5230\u640d\u5931\u5728\u5fae\u5c0f\u7684\u9a57\u8b49\u96c6\u4e2d\u505c\u6b62\u6539\u5584\u3002\u900f\u904e\u5728\u4e00\u822c\u6700\u4f73\u5316\u6b65\u9a5f\u548c\u5feb\u8f49\u968e\u6bb5\u4e4b\u9593\u4ea4\u66ff\uff0c\u5feb\u8f49\u53ef\u63d0\u4f9b\u9ad8\u9054 87% \u7684 FLOP \u6e1b\u5c11\u91cf\uff0c\u4ee5\u53ca\u9ad8\u9054 81% \u7684\u8a13\u7df4\u6642\u9593\u6e1b\u5c11\u91cf\uff0c\u512a\u65bc\u4f7f\u7528 Adam \u7684\u6a19\u6e96 SGD\u3002\u6211\u5011\u900f\u904e\u5728\u4e0d\u540c\u4efb\u52d9\u4e0a\u5fae\u8abf\u5404\u7a2e\u6a21\u578b\u4f86\u9a57\u8b49\u5feb\u8f49\uff0c\u4e26\u8b49\u660e\u5b83\u80fd\u5728\u4e0d\u640d\u5bb3\u6a21\u578b\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\u52a0\u901f\u8a13\u7df4\u3002\u6b64\u5916\uff0c\u6211\u5011\u5206\u6790\u4f55\u6642\u4ee5\u53ca\u5982\u4f55\u61c9\u7528\u5feb\u8f49\u3002", "author": "Adir Rahamim et.al.", "authors": "Adir Rahamim, Naomi Saphra, Sara Kangaslahti, Yonatan Belinkov", "id": "2409.04206v1", "paper_url": "http://arxiv.org/abs/2409.04206v1", "repo": "null"}}