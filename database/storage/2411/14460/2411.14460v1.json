{"2411.14460": {"publish_time": "2024-11-16", "title": "LLaSA: Large Language and Structured Data Assistant", "paper_summary": "Structured data, such as tables, graphs, and databases, play a critical role\nin plentiful NLP tasks such as question answering and dialogue system.\nRecently, inspired by Vision-Language Models, Graph Neutral Networks (GNNs)\nhave been introduced as an additional modality into the input of Large Language\nModels (LLMs) to improve their performance on Structured Knowledge Grounding\n(SKG) tasks. However, those GNN-enhanced LLMs have the following limitations:\n(1) They employ diverse GNNs to model varying types of structured data,\nrendering them unable to uniformly process various forms of structured data.\n(2) The pretraining of GNNs is coupled with specific LLMs, which prevents GNNs\nfrom fully aligning with the textual space and limits their adaptability to\nother LLMs. To address these issues, we propose \\textbf{L}arge\n\\textbf{L}anguage and \\textbf{S}tructured Data \\textbf{A}ssistant (LLaSA), a\ngeneral framework for enhancing LLMs' ability to handle structured data.\nSpecifically, we represent various types of structured data in a unified\nhypergraph format, and use self-supervised learning to pretrain a hypergraph\nencoder, and a G-Former compressing encoded hypergraph representations with\ncross-attention. The compressed hypergraph representations are appended to the\nserialized inputs during training and inference stages of LLMs. Experimental\nresults on multiple SKG tasks show that our pretrained hypergraph encoder can\nadapt to various LLMs and enhance their ability to process different types of\nstructured data. Besides, LLaSA, with LoRA fine-tuning, outperforms previous\nSOTA method using full parameters tuning.", "paper_summary_zh": "<paragraph>\u7d50\u69cb\u5316\u8cc7\u6599\uff0c\u4f8b\u5982\u8868\u683c\u3001\u5716\u8868\u548c\u8cc7\u6599\u5eab\uff0c\u5728\u8c50\u5bcc\u7684 NLP \u4efb\u52d9\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f8b\u5982\u554f\u7b54\u548c\u5c0d\u8a71\u7cfb\u7d71\u3002\n\u6700\u8fd1\uff0c\u53d7\u5230\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u555f\u767c\uff0c\u5716\u5f62\u4e2d\u7acb\u7db2\u8def (GNN) \u5df2\u88ab\u5f15\u5165\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u5165\u4e2d\u4f5c\u70ba\u4e00\u7a2e\u984d\u5916\u7684\u6a21\u5f0f\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u7d50\u69cb\u5316\u77e5\u8b58\u57fa\u790e (SKG) \u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u9019\u4e9b GNN \u589e\u5f37\u7684 LLM \u5177\u6709\u4ee5\u4e0b\u9650\u5236\uff1a\n(1) \u5b83\u5011\u4f7f\u7528\u4e0d\u540c\u7684 GNN \u4f86\u5efa\u6a21\u5404\u7a2e\u7d50\u69cb\u5316\u8cc7\u6599\u985e\u578b\uff0c\u5c0e\u81f4\u5b83\u5011\u7121\u6cd5\u7d71\u4e00\u8655\u7406\u5404\u7a2e\u5f62\u5f0f\u7684\u7d50\u69cb\u5316\u8cc7\u6599\u3002\n(2) GNN \u7684\u9810\u8a13\u7df4\u8207\u7279\u5b9a\u7684 LLM \u7d50\u5408\u5728\u4e00\u8d77\uff0c\u9019\u6703\u963b\u6b62 GNN \u8207\u6587\u672c\u7a7a\u9593\u5b8c\u5168\u5c0d\u9f4a\uff0c\u4e26\u9650\u5236\u5176\u9069\u61c9\u5176\u4ed6 LLM\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86**L**arge **L**anguage and **S**tructured Data **A**ssistant (LLaSA)\uff0c\u4e00\u500b\u7528\u65bc\u589e\u5f37 LLM \u8655\u7406\u7d50\u69cb\u5316\u8cc7\u6599\u80fd\u529b\u7684\u901a\u7528\u6846\u67b6\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4ee5\u7d71\u4e00\u7684\u8d85\u5716\u683c\u5f0f\u8868\u793a\u5404\u7a2e\u7d50\u69cb\u5316\u8cc7\u6599\u985e\u578b\uff0c\u4e26\u4f7f\u7528\u81ea\u6211\u76e3\u7763\u5b78\u7fd2\u4f86\u9810\u8a13\u7df4\u8d85\u5716\u7de8\u78bc\u5668\uff0c\u4ee5\u53ca\u4f7f\u7528\u8de8\u6ce8\u610f\u529b\u58d3\u7e2e\u7de8\u78bc\u8d85\u5716\u8868\u793a\u7684 G-Former\u3002\u58d3\u7e2e\u7684\u8d85\u5716\u8868\u793a\u6703\u9644\u52a0\u5230 LLM \u7684\u8a13\u7df4\u548c\u63a8\u8ad6\u968e\u6bb5\u7684\u5e8f\u5217\u5316\u8f38\u5165\u4e2d\u3002\u591a\u500b SKG \u4efb\u52d9\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u9810\u8a13\u7df4\u7684\u8d85\u5716\u7de8\u78bc\u5668\u53ef\u4ee5\u9069\u61c9\u5404\u7a2e LLM\uff0c\u4e26\u589e\u5f37\u5176\u8655\u7406\u4e0d\u540c\u985e\u578b\u7d50\u69cb\u5316\u8cc7\u6599\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0cLLaSA \u4f7f\u7528 LoRA \u5fae\u8abf\uff0c\u512a\u65bc\u4f7f\u7528\u5168\u53c3\u6578\u5fae\u8abf\u7684\u5148\u524d SOTA \u65b9\u6cd5\u3002</paragraph>", "author": "Yao Xu et.al.", "authors": "Yao Xu, Shizhu He, Zeng Xiangrong, Jiabei Chen, Guang Liu, Bingning Wang, Jun Zhao, Kang Liu", "id": "2411.14460v1", "paper_url": "http://arxiv.org/abs/2411.14460v1", "repo": "null"}}