
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection**|Michael Regan et.al.|[2405.19285v1](http://arxiv.org/abs/2405.19285v1)|null|
|**2024-05-29**|**PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications**|Dingkang Yang et.al.|[2405.19266v1](http://arxiv.org/abs/2405.19266v1)|null|
|**2024-05-29**|**Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation**|Jose Tupayachi et.al.|[2405.19255v1](http://arxiv.org/abs/2405.19255v1)|null|
|**2024-05-29**|**Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery**|Sounak Lahiri et.al.|[2405.19164v1](http://arxiv.org/abs/2405.19164v1)|null|
|**2024-05-28**|**Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models**|Hyunjin Seo et.al.|[2405.18581v1](http://arxiv.org/abs/2405.18581v1)|null|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414v1](http://arxiv.org/abs/2405.18414v1)|null|
|**2024-05-28**|**Knowledge Circuits in Pretrained Transformers**|Yunzhi Yao et.al.|[2405.17969v1](http://arxiv.org/abs/2405.17969v1)|[link](https://github.com/zjunlp/knowledgecircuits)|
|**2024-05-28**|**Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs**|Yong Qi et.al.|[2405.17846v1](http://arxiv.org/abs/2405.17846v1)|null|
|**2024-05-27**|**Cost-efficient Knowledge-based Question Answering with Large Language Models**|Junnan Dong et.al.|[2405.17337v1](http://arxiv.org/abs/2405.17337v1)|null|
|**2024-05-27**|**Assessing LLMs Suitability for Knowledge Graph Completion**|Vasile Ionut Remus Iga et.al.|[2405.17249v1](http://arxiv.org/abs/2405.17249v1)|[link](https://github.com/ionutiga/llms-for-kgc)|
|**2024-05-27**|**Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning**|Xun Liang et.al.|[2405.16933v1](http://arxiv.org/abs/2405.16933v1)|[link](https://github.com/iaar-shanghai/pgrag)|
|**2024-05-27**|**Entity Alignment with Noisy Annotations from Large Language Models**|Shengyuan Chen et.al.|[2405.16806v2](http://arxiv.org/abs/2405.16806v2)|[link](https://github.com/chensycn/llm4ea_official)|
|**2024-05-27**|**TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations**|Zheng Zhang et.al.|[2405.16800v1](http://arxiv.org/abs/2405.16800v1)|null|
|**2024-05-26**|**KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge**|Pengcheng Jiang et.al.|[2405.16412v1](http://arxiv.org/abs/2405.16412v1)|null|
|**2024-05-26**|**Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level**|Runlin Lei et.al.|[2405.16405v1](http://arxiv.org/abs/2405.16405v1)|null|
|**2024-05-25**|**COLT: Towards Completeness-Oriented Tool Retrieval for Large Language Models**|Changle Qu et.al.|[2405.16089v1](http://arxiv.org/abs/2405.16089v1)|null|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739v2](http://arxiv.org/abs/2405.15739v2)|[link](https://github.com/andresalgaba/llm_citation_patterns)|
|**2024-05-24**|**Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph**|Runsong Jia et.al.|[2405.15374v1](http://arxiv.org/abs/2405.15374v1)|null|
|**2024-05-24**|**Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models**|Cong Lu et.al.|[2405.15143v2](http://arxiv.org/abs/2405.15143v2)|[link](https://github.com/conglu1997/intelligent-go-explore)|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831v1](http://arxiv.org/abs/2405.14831v1)|null|
|**2024-05-23**|**Fisher Flow Matching for Generative Modeling over Discrete Data**|Oscar Davis et.al.|[2405.14664v3](http://arxiv.org/abs/2405.14664v3)|null|
|**2024-05-23**|**GLaD: Synergizing Molecular Graphs and Language Descriptors for Enhanced Power Conversion Efficiency Prediction in Organic Photovoltaic Devices**|Thao Nguyen et.al.|[2405.14203v1](http://arxiv.org/abs/2405.14203v1)|null|
|**2024-05-23**|**Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning**|Jiapu Wang et.al.|[2405.14170v1](http://arxiv.org/abs/2405.14170v1)|null|
|**2024-05-22**|**Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models**|Tolga Çöplü et.al.|[2405.14012v1](http://arxiv.org/abs/2405.14012v1)|null|
|**2024-05-22**|**LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework**|Yiran Qiao et.al.|[2405.13902v1](http://arxiv.org/abs/2405.13902v1)|null|
|**2024-05-22**|**FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering**|Yuan Sui et.al.|[2405.13873v1](http://arxiv.org/abs/2405.13873v1)|null|
|**2024-05-22**|**Large Language Models are Effective Priors for Causal Graph Discovery**|Victor-Alexandru Darvariu et.al.|[2405.13551v1](http://arxiv.org/abs/2405.13551v1)|null|
|**2024-05-22**|**TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models**|Pengzhou Cheng et.al.|[2405.13401v2](http://arxiv.org/abs/2405.13401v2)|null|
|**2024-05-21**|**Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction**|Yu-Hsiang Lin et.al.|[2405.12656v1](http://arxiv.org/abs/2405.12656v1)|[link](https://github.com/exiled1143/retrieval-augmented-language-model-for-multi-label-knowledge-graph-link-prediction)|
|**2024-05-21**|**Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation**|Qingyao Li et.al.|[2405.12442v1](http://arxiv.org/abs/2405.12442v1)|null|
|**2024-05-20**|**KG-RAG: Bridging the Gap Between Knowledge and Creativity**|Diego Sanmartin et.al.|[2405.12035v1](http://arxiv.org/abs/2405.12035v1)|null|
|**2024-05-20**|**"Set It Up!": Functional Object Arrangement with Compositional Generative Models**|Yiqing Xu et.al.|[2405.11928v1](http://arxiv.org/abs/2405.11928v1)|null|
|**2024-05-20**|**Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!**|Dean Allemang et.al.|[2405.11706v1](http://arxiv.org/abs/2405.11706v1)|null|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745v1](http://arxiv.org/abs/2405.10745v1)|null|
|**2024-05-17**|**Automatic News Generation and Fact-Checking System Based on Language Processing**|Xirui Peng et.al.|[2405.10492v2](http://arxiv.org/abs/2405.10492v2)|null|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305v1](http://arxiv.org/abs/2405.10305v1)|[link](https://github.com/jingkang50/psg4d)|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288v1](http://arxiv.org/abs/2405.10288v1)|null|
|**2024-05-15**|**SCI 3.0: A Web-based Schema Curation Interface for Graphical Event Representations**|Reece Suchocki et.al.|[2405.09733v2](http://arxiv.org/abs/2405.09733v2)|null|
|**2024-05-15**|**SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge**|Andong Wang et.al.|[2405.09713v2](http://arxiv.org/abs/2405.09713v2)|null|
|**2024-05-15**|**STAR: A Benchmark for Situated Reasoning in Real-World Videos**|Bo Wu et.al.|[2405.09711v1](http://arxiv.org/abs/2405.09711v1)|null|
|**2024-05-14**|**Falcon 7b for Software Mention Detection in Scholarly Documents**|AmeerAli Khan et.al.|[2405.08514v1](http://arxiv.org/abs/2405.08514v1)|null|
|**2024-05-14**|**Could Chemical LLMs benefit from Message Passing**|Jiaqing Xie et.al.|[2405.08334v1](http://arxiv.org/abs/2405.08334v1)|null|
|**2024-05-13**|**AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models**|Shuo Liu et.al.|[2405.07626v1](http://arxiv.org/abs/2405.07626v1)|[link](https://github.com/anomalyllm/anomalyllm)|
|**2024-05-13**|**DynLLM: When Large Language Models Meet Dynamic Graph Recommendation**|Ziwei Zhao et.al.|[2405.07580v1](http://arxiv.org/abs/2405.07580v1)|null|
|**2024-05-10**|**LLM-Generated Black-box Explanations Can Be Adversarially Helpful**|Rohan Ajwani et.al.|[2405.06800v2](http://arxiv.org/abs/2405.06800v2)|[link](https://github.com/ziningzhu/adversarial_helpfulness)|
|**2024-05-10**|**A Survey of Large Language Models for Graphs**|Xubin Ren et.al.|[2405.08011v1](http://arxiv.org/abs/2405.08011v1)|[link](https://github.com/hkuds/awesome-llm4graph-papers)|
|**2024-05-10**|**Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark**|Evan M. Williams et.al.|[2405.06634v1](http://arxiv.org/abs/2405.06634v1)|null|
|**2024-05-10**|**Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval**|Mengjia Niu et.al.|[2405.06545v1](http://arxiv.org/abs/2405.06545v1)|null|
|**2024-05-10**|**Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts**|Wenyu Huang et.al.|[2405.06524v1](http://arxiv.org/abs/2405.06524v1)|null|
|**2024-05-09**|**RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**|Sourav Garg et.al.|[2405.05792v1](http://arxiv.org/abs/2405.05792v1)|null|
|**2024-05-09**|**G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning**|Ruiting Dai et.al.|[2405.05616v1](http://arxiv.org/abs/2405.05616v1)|null|
|**2024-05-08**|**MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning**|Inderjeet Nair et.al.|[2405.05189v1](http://arxiv.org/abs/2405.05189v1)|null|
|**2024-05-08**|**Lightweight Spatial Modeling for Combinatorial Information Extraction From Documents**|Yanfei Dong et.al.|[2405.06701v1](http://arxiv.org/abs/2405.06701v1)|null|
|**2024-05-08**|**DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature**|Dawei Li et.al.|[2405.04819v2](http://arxiv.org/abs/2405.04819v2)|[link](https://github.com/david-li0406/dalk)|
|**2024-05-08**|**BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models**|Chu Fei Luo et.al.|[2405.04756v1](http://arxiv.org/abs/2405.04756v1)|[link](https://github.com/VectorInstitute/biaskg)|
|**2024-05-08**|**AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models**|Yongheng Zhang et.al.|[2405.04753v1](http://arxiv.org/abs/2405.04753v1)|null|
|**2024-05-07**|**Enriched BERT Embeddings for Scholarly Publication Classification**|Benjamin Wolff et.al.|[2405.04136v1](http://arxiv.org/abs/2405.04136v1)|null|
|**2024-05-06**|**FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering**|Silan Hu et.al.|[2405.03734v1](http://arxiv.org/abs/2405.03734v1)|null|
|**2024-05-05**|**E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods**|Şükrü Öztürk et.al.|[2405.02984v1](http://arxiv.org/abs/2405.02984v1)|null|
|**2024-05-04**|**Relations Prediction for Knowledge Graph Completion using Large Language Models**|Sakher Khalil Alqaaidi et.al.|[2405.02738v1](http://arxiv.org/abs/2405.02738v1)|null|
|**2024-05-04**|**IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data**|Sami Azirar et.al.|[2405.15792v1](http://arxiv.org/abs/2405.15792v1)|null|
|**2024-05-04**|**R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models**|Taolin Zhang et.al.|[2405.02659v1](http://arxiv.org/abs/2405.02659v1)|null|
|**2024-05-03**|**Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph**|Vladyslav Nechakhin et.al.|[2405.02105v1](http://arxiv.org/abs/2405.02105v1)|null|
|**2024-05-03**|**Protein binding affinity prediction under multiple substitutions applying eGNNs on Residue and Atomic graphs combined with Language model information: eGRAL**|Arturo Fiorellini-Bernardis et.al.|[2405.02374v1](http://arxiv.org/abs/2405.02374v1)|null|
|**2024-05-03**|**CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation**|Kounianhua Du et.al.|[2405.02355v1](http://arxiv.org/abs/2405.02355v1)|null|
|**2024-05-02**|**ALCM: Autonomous LLM-Augmented Causal Discovery Framework**|Elahe Khatibi et.al.|[2405.01744v1](http://arxiv.org/abs/2405.01744v1)|null|
|**2024-05-02**|**Improving Complex Reasoning over Knowledge Graph with Logic-Aware Curriculum Tuning**|Tianle Xia et.al.|[2405.01649v3](http://arxiv.org/abs/2405.01649v3)|null|
|**2024-05-02**|**Identification of Entailment and Contradiction Relations between Natural Language Sentences: A Neurosymbolic Approach**|Xuyao Feng et.al.|[2405.01259v1](http://arxiv.org/abs/2405.01259v1)|null|
|**2024-05-01**|**RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models**|Mohamed Manzour Hussien et.al.|[2405.00449v1](http://arxiv.org/abs/2405.00449v1)|null|
|**2024-04-30**|**Graph Neural Network Approach to Semantic Type Detection in Tables**|Ehsan Hoseinzade et.al.|[2405.00123v1](http://arxiv.org/abs/2405.00123v1)|[link](https://github.com/hoseinzadeehsan/gait)|
|**2024-04-30**|**PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification**|Leon Garza et.al.|[2404.19744v1](http://arxiv.org/abs/2404.19744v1)|null|
|**2024-04-30**|**A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications**|Steph Buongiorno et.al.|[2404.19729v1](http://arxiv.org/abs/2404.19729v1)|null|
|**2024-04-30**|**Octopus v4: Graph of language models**|Wei Chen et.al.|[2404.19296v1](http://arxiv.org/abs/2404.19296v1)|null|
|**2024-04-30**|**Multi-hop Question Answering over Knowledge Graphs using Large Language Models**|Abir Chakraborty et.al.|[2404.19234v1](http://arxiv.org/abs/2404.19234v1)|null|
|**2024-04-29**|**Automated Construction of Theme-specific Knowledge Graphs**|Linyi Ding et.al.|[2404.19146v1](http://arxiv.org/abs/2404.19146v1)|null|
|**2024-04-29**|**QANA: LLM-based Question Generation and Network Analysis for Zero-shot Key Point Analysis and Beyond**|Tomoki Fukuma et.al.|[2404.18371v1](http://arxiv.org/abs/2404.18371v1)|null|
|**2024-04-28**|**Parameter-Efficient Tuning Large Language Models for Graph Representation Learning**|Qi Zhu et.al.|[2404.18271v1](http://arxiv.org/abs/2404.18271v1)|null|
|**2024-04-28**|**Generative AI for Visualization: State of the Art and Future Directions**|Yilin Ye et.al.|[2404.18144v1](http://arxiv.org/abs/2404.18144v1)|null|
|**2024-04-26**|**Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering**|Zhentao Xu et.al.|[2404.17723v2](http://arxiv.org/abs/2404.17723v2)|null|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662v1](http://arxiv.org/abs/2404.17662v1)|[link](https://github.com/alickzhu/player)|
|**2024-04-26**|**Language Interaction Network for Clinical Trial Approval Estimation**|Chufan Gao et.al.|[2405.06662v1](http://arxiv.org/abs/2405.06662v1)|null|
|**2024-04-25**|**CyNetDiff -- A Python Library for Accelerated Implementation of Network Diffusion Models**|Eliot W. Robson et.al.|[2404.17059v1](http://arxiv.org/abs/2404.17059v1)|[link](https://github.com/eliotwrobson/cynetdiff)|
|**2024-04-25**|**Player-Driven Emergence in LLM-Driven Game Narrative**|Xiangyu Peng et.al.|[2404.17027v2](http://arxiv.org/abs/2404.17027v2)|null|
|**2024-04-25**|**Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models**|Bradley P. Allen et.al.|[2404.17000v1](http://arxiv.org/abs/2404.17000v1)|[link](https://github.com/bradleypallen/evaluating-kg-class-memberships-using-llms)|
|**2024-04-25**|**Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer**|Jianyu Zheng et.al.|[2404.16627v1](http://arxiv.org/abs/2404.16627v1)|[link](https://github.com/tian14267/ls_mbert)|
|**2024-04-24**|**Semgrex and Ssurgeon, Searching and Manipulating Dependency Graphs**|John Bauer et.al.|[2404.16250v1](http://arxiv.org/abs/2404.16250v1)|null|
|**2024-04-24**|**Knowledge Graph Completion using Structural and Textual Embeddings**|Sakher Khalil Alqaaidi et.al.|[2404.16206v1](http://arxiv.org/abs/2404.16206v1)|[link](https://github.com/sa5r/kgrp)|
|**2024-04-24**|**Improving Multi-label Recognition using Class Co-Occurrence Probabilities**|Samyak Rawlekar et.al.|[2404.16193v1](http://arxiv.org/abs/2404.16193v1)|null|
|**2024-04-24**|**From Local to Global: A Graph RAG Approach to Query-Focused Summarization**|Darren Edge et.al.|[2404.16130v1](http://arxiv.org/abs/2404.16130v1)|null|
|**2024-04-24**|**KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction**|Jack Boylan et.al.|[2404.15923v1](http://arxiv.org/abs/2404.15923v1)|null|
|**2024-04-23**|**Graph Machine Learning in the Era of Large Language Models (LLMs)**|Wenqi Fan et.al.|[2404.14928v1](http://arxiv.org/abs/2404.14928v1)|null|
|**2024-04-23**|**A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications**|Wenbo Shang et.al.|[2404.14809v1](http://arxiv.org/abs/2404.14809v1)|null|
|**2024-04-23**|**RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model**|Peiwen Li et.al.|[2404.14786v2](http://arxiv.org/abs/2404.14786v2)|null|
|**2024-04-23**|**Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models**|Chris Samarinas et.al.|[2404.14772v1](http://arxiv.org/abs/2404.14772v1)|[link](https://github.com/algoprog/syntod)|
|**2024-04-23**|**Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering**|Yao Xu et.al.|[2404.14741v1](http://arxiv.org/abs/2404.14741v1)|[link](https://github.com/yaooxu/gog)|
|**2024-04-22**|**Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph**|Xiaochen Kev Gao et.al.|[2404.14372v1](http://arxiv.org/abs/2404.14372v1)|[link](https://github.com/shangdatalab/flan-graph)|
|**2024-04-22**|**LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots**|Dongge Han et.al.|[2404.14285v1](http://arxiv.org/abs/2404.14285v1)|null|
|**2024-04-22**|**Context-Enhanced Language Models for Generating Multi-Paper Citations**|Avinash Anand et.al.|[2404.13865v1](http://arxiv.org/abs/2404.13865v1)|null|
|**2024-04-21**|**Test-Time Training on Graphs with Large Language Models (LLMs)**|Jiaxin Zhang et.al.|[2404.13571v1](http://arxiv.org/abs/2404.13571v1)|null|
|**2024-04-20**|**Evaluation of Machine Translation Based on Semantic Dependencies and Keywords**|Kewei Yuan et.al.|[2404.14443v1](http://arxiv.org/abs/2404.14443v1)|null|

#### Abstracts
##### **MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection**
2405.19285v1 by Michael Regan, Shira Wein, George Baker, Emilio Monti

Abstract Meaning Representation (AMR) is a semantic formalism that captures
the core meaning of an utterance. There has been substantial work developing
AMR corpora in English and more recently across languages, though the limited
size of existing datasets and the cost of collecting more annotations are
prohibitive. With both engineering and scientific questions in mind, we
introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph
annotations, currently the largest and most diverse of its kind: AMR graphs for
1,685 information-seeking utterances mapped to 50+ typologically diverse
languages. We describe how we built our resource and its unique features before
reporting on experiments using large language models for multilingual AMR and
SPARQL parsing as well as applying AMRs for hallucination detection in the
context of knowledge base question answering, with results shedding light on
persistent issues using LLMs for structured parsing.

摘要：抽象語意表示（AMR）是一種語意形式化，用於捕捉語句的核心含義。
目前已有大量工作致力於開發英文 AMR 語料庫，最近更擴展到跨語言，儘管現有資料集規模有限，且收集更多標註的成本過高。
考量到工程和科學方面的問題，我們引入了 MASSIVE-AMR，這是一個包含超過 84,000 個文字轉圖形標註的資料集，目前是同類資料集中規模最大且最多樣化的：AMR 圖形涵蓋 1,685 個資訊尋求語句，對應到 50 多種語言類型多樣的語言。
我們將說明如何建構資源及其獨特功能，然後再報告使用大型語言模型進行多語言 AMR 和 SPARQL 解析的實驗，以及在知識庫問答的背景下應用 AMR 進行幻覺偵測，結果有助於釐清使用 LLM 進行結構化解析時持續存在的問題。

##### **PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications**
2405.19266v1 by Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang

Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.

摘要：<paragraph>開發智能兒童諮詢系統，為提高診斷效率提供了有希望的前景，特別是在醫療資源稀缺的中國。儘管中文醫學的大語言模型（LLM）最近取得進展，但由於教學資料不足和培訓程序脆弱，它們在兒科應用中的表現並非最佳。為了解決上述問題，本文構建了 PedCorpus，一個由超過 30 萬條來自兒科教科書、指南和知識圖譜資源的多任務指令組成的優質數據集，以滿足不同的診斷需求。在設計良好的 PedCorpus 上，我們提出了 PediatricsGPT，這是第一個建立在系統且強大的訓練管道上的中文兒科 LLM 助手。在持續的預訓練階段，我們引入了一個混合指令預訓練機制，以減輕 LLM 在醫學領域適應中的內部注入知識不一致。緊接著，利用全參數監督微調（SFT）將一般醫學知識架構納入模型中。在那之後，我們設計了一個直接遵循偏好最佳化，以增強類兒科醫生的人文反應生成。在參數效率的次要 SFT 階段，提出了一個通用特定專家策略的混合，以解決內科醫生和兒科專業掌握之間的能力衝突。基於指標、GPT-4 和醫生對不同醫生下游任務的評估的廣泛結果表明，PediatricsGPT 持續優於先前的中文醫學 LLM。我們的模型和數據集將對社群開發開放原始碼。</paragraph>

##### **Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation**
2405.19255v1 by Jose Tupayachi, Haowen Xu, Olufemi A. Omitaomu, Mustafa Can Camur, Aliza Sharmin, Xueping Li

The incorporation of Artificial Intelligence (AI) models into various
optimization systems is on the rise. Yet, addressing complex urban and
environmental management problems normally requires in-depth domain science and
informatics expertise. This expertise is essential for deriving data and
simulation-driven for informed decision support. In this context, we
investigate the potential of leveraging the pre-trained Large Language Models
(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated
workflow that encompasses natural language processing, methontology-based
prompt tuning, and transformers. This workflow automates the creation of
scenario-based ontology using existing research articles and technical manuals
of urban datasets and simulations. The outcomes of our methodology are
knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing
the data and metadata modeling, the integration of complex datasets, the
coupling of multi-domain simulation models, and the formulation of
decision-making metrics and workflow. The feasibility of our methodology is
evaluated through a comparative analysis that juxtaposes our AI-generated
ontology with the well-known Pizza Ontology employed in tutorials for popular
ontology software (e.g., prot\'eg\'e). We close with a real-world case study of
optimizing the complex urban system of multi-modal freight transportation by
generating anthologies of various domain data and simulations to support
informed decision-making.

摘要：人工智能 (AI) 模型整合到各種最佳化系統中正方興未艾。然而，解決複雜的都市和環境管理問題通常需要深入的領域科學和資訊專業知識。這種專業知識對於從資料和模擬中推導出資料驅動的明智決策支援至關重要。在此脈絡下，我們探討利用預先訓練的大語言模型 (LLM) 的潛力。透過採用 ChatGPT API 作為推理核心，我們概述一個整合的工作流程，其中包含自然語言處理、基於方法論的提示調整和轉換器。此工作流程自動化使用現有研究文章和都市資料集及模擬技術手冊建立基於情境的本体。我們方法論的成果是廣泛採用的本体語言（例如 OWL、RDF、SPARQL）中的知識圖譜。這些知識圖譜透過增強資料和元資料建模、整合複雜的資料集、結合多領域模擬模型，以及制定決策指標和工作流程，促進都市決策支援系統的發展。我們透過比較分析評估我們方法論的可行性，該分析將我們 AI 生成的本体與廣泛用於熱門本体軟體 (例如 prot\'eg\'e) 教學課程的知名 Pizza 本体並置。我們以一個真實世界的案例研究作結，透過產生各種領域資料和模擬的選集來最佳化多式聯運貨運的複雜都市系統，以支援明智的決策制定。

##### **Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery**
2405.19164v1 by Sounak Lahiri, Sumit Pai, Tim Weninger, Sanmitra Bhattacharya

Electronic Discovery (eDiscovery) involves identifying relevant documents
from a vast collection based on legal production requests. The integration of
artificial intelligence (AI) and natural language processing (NLP) has
transformed this process, helping document review and enhance efficiency and
cost-effectiveness. Although traditional approaches like BM25 or fine-tuned
pre-trained models are common in eDiscovery, they face performance,
computational, and interpretability challenges. In contrast, Large Language
Model (LLM)-based methods prioritize interpretability but sacrifice performance
and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid
approach that combines the strengths of two worlds: a heterogeneous graph-based
method for accurate document relevance prediction and subsequent LLM-driven
approach for reasoning. Graph representational learning generates embeddings
and predicts links, ranking the corpus for a given request, and the LLMs
provide reasoning for document relevance. Our approach handles datasets with
balanced and imbalanced distributions, outperforming baselines in F1-score,
precision, and recall by an average of 12%, 3%, and 16%, respectively. In an
enterprise context, our approach drastically reduces document review costs by
99.9% compared to manual processes and by 95% compared to LLM-based
classification methods

摘要：電子發現 (eDiscovery) 涉及根據法律製作要求從大量集合中識別相關文件。人工智慧 (AI) 和自然語言處理 (NLP) 的整合已轉變此程序，協助文件檢閱並提升效率和成本效益。儘管傳統方法（例如 BM25 或微調預先訓練的模型）在電子發現中很常見，但它們面臨效能、運算和可解釋性方面的挑戰。相對地，大型語言模型 (LLM) 為基礎的方法優先考量可解釋性，但犧牲效能和處理量。本文介紹 DISCOvery Graph (DISCOG)，這是一種結合兩個世界的優勢的混合方法：一種用於準確文件相關性預測的異質圖形為基礎的方法和後續的 LLM 驅動方法用於推理。圖形表徵學習會產生嵌入和預測連結，針對特定要求對語料庫進行排名，而 LLM 則提供文件相關性的推理。我們的做法處理具有平衡和不平衡分佈的資料集，在 F1 分數、準確度和召回率方面優於基準，平均分別高出 12%、3% 和 16%。在企業環境中，與手動程序相比，我們的做法大幅減少文件審查成本 99.9%，與基於 LLM 的分類方法相比，減少 95%。

##### **Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models**
2405.18581v1 by Hyunjin Seo, Taewon Kim, June Yong Yang, Eunho Yang

Recent advancements in text-attributed graphs (TAGs) have significantly
improved the quality of node features by using the textual modeling
capabilities of language models. Despite this success, utilizing text
attributes to enhance the predefined graph structure remains largely
unexplored. Our extensive analysis reveals that conventional edges on TAGs,
treated as a single relation (e.g., hyperlinks) in previous literature,
actually encompass mixed semantics (e.g., "advised by" and "participates in").
This simplification hinders the representation learning process of Graph Neural
Networks (GNNs) on downstream tasks, even when integrated with advanced node
features. In contrast, we discover that decomposing these edges into distinct
semantic relations significantly enhances the performance of GNNs. Despite
this, manually identifying and labeling of edges to corresponding semantic
relations is labor-intensive, often requiring domain expertise. To this end, we
introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel
framework that leverages the capability of Large Language Models (LLMs) to
decompose the graph structure by analyzing raw text attributes - in a fully
automated manner. RoSE operates in two stages: (1) identifying meaningful
relations using an LLM-based generator and discriminator, and (2) categorizing
each edge into corresponding relations by analyzing textual contents associated
with connected nodes via an LLM-based decomposer. Extensive experiments
demonstrate that our model-agnostic framework significantly enhances node
classification performance across various datasets, with improvements of up to
16% on the Wisconsin dataset.

摘要：文本属性图 (TAG) 的最新进展显著提升了节点特征的质量，方法是利用语言模型的文本建模能力。尽管取得了这一成功，但利用文本属性来增强预定义的图结构在很大程度上仍未得到探索。我们的广泛分析表明，TAG 上的传统边在先前的文献中被视为单一关系（例如超链接），实际上包含混合语义（例如“建议”和“参与”）。这种简化阻碍了图神经网络 (GNN) 在下游任务中的表示学习过程，即使与高级节点特征集成也是如此。相比之下，我们发现将这些边分解为不同的语义关系会显著增强 GNN 的性能。尽管如此，手动识别和标记对应语义关系的边是一项劳动密集型工作，通常需要领域专业知识。为此，我们引入了 RoSE（面向关系的语义边分解），这是一个新颖的框架，它利用大型语言模型 (LLM) 的能力以全自动的方式通过分析原始文本属性来分解图结构。RoSE 分为两个阶段：（1）使用基于 LLM 的生成器和鉴别器识别有意义的关系，以及（2）通过基于 LLM 的分解器分析与连接节点关联的文本内容，将每个边分类到相应的语义关系中。广泛的实验表明，我们的模型无关框架显著增强了各个数据集上的节点分类性能，在 Wisconsin 数据集上提高了 16%。

##### **Don't Forget to Connect! Improving RAG with Graph-based Reranking**
2405.18414v1 by Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, Anton Tsitsulin

Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.

摘要：檢索增強生成 (RAG) 已大幅提升大型語言模型 (LLM) 回應的效能，方法是將生成基礎於現有文件中的內容。當文件與問題內容明確相關時，這些系統運作良好。但當文件有部分資訊或與內容的關聯性較不顯著時呢？我們又該如何推論文件之間的關聯性？在這項研究中，我們尋求回答這兩個關於 RAG 生成的核心問題。我們引入了 G-RAG，一種基於 RAG 中檢索器和讀取器之間的圖形神經網路 (GNN) 的重新排序器。我們的技術結合了文件之間的關聯性與語義資訊（透過抽象意義表徵圖形），為 RAG 提供一個有脈絡的排序器。G-RAG 的表現超越現有技術，同時運算資源需求較小。此外，我們評估了 PaLM 2 作為重新排序器的表現，發現其表現顯著低於 G-RAG。這個結果強調了重新排序對 RAG 的重要性，即使在使用大型語言模型時也是如此。

##### **Knowledge Circuits in Pretrained Transformers**
2405.17969v1 by Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen

The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, has allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuit holds
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.

摘要：現代大型語言模型的卓越能力源於其參數中編碼的龐大知識庫，讓它們能夠感知世界並參與推理。這些模型如何儲存知識的內部運作方式一直是研究人員高度關注和研究的主題。迄今為止，大多數研究都集中在這些模型中的孤立元件，例如多層感知器和注意力頭。在本文中，我們深入探討語言模型的計算圖，以揭示表達特定知識中至關重要的知識電路。使用 GPT2 和 TinyLLAMA 進行的實驗讓我們能夠觀察某些資訊頭、關係頭和多層感知器如何在模型中協同編碼知識。此外，我們評估了當前知識編輯技術對這些知識電路的影響，對這些編輯方法的功能和限制提供了更深入的見解。最後，我們利用知識電路分析和詮釋語言模型行為，例如幻覺和情境學習。我們相信知識電路有潛力促進我們對 Transformer 的理解，並指導知識編輯的改進設計。代碼和資料可在 https://github.com/zjunlp/KnowledgeCircuits 中取得。

##### **Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs**
2405.17846v1 by Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng Wang, Shuo Jiang

Safety limitations in service robotics across various industries have raised
significant concerns about the need for robust mechanisms ensuring that robots
adhere to safe practices, thereby preventing actions that might harm humans or
cause property damage. Despite advances, including the integration of Knowledge
Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring
consistent safety in autonomous robot actions persist. In this paper, we
propose a novel integration of Large Language Models with Embodied Robotic
Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the
safety framework for service robots. ERCPs are designed as predefined
instructions that ensure LLMs generate safe and precise responses. These
responses are subsequently validated by EKGs, which provide a comprehensive
knowledge base ensuring that the actions of the robot are continuously aligned
with safety protocols, thereby promoting safer operational practices in varied
contexts. Our experimental setup involved diverse real-world tasks, where
robots equipped with our framework demonstrated significantly higher compliance
with safety standards compared to traditional methods. This integration fosters
secure human-robot interactions and positions our methodology at the forefront
of AI-driven safety innovations in service robotics.

摘要：各產業中的服務機器人安全限制已引發重大疑慮，認為有必要建立強健的機制，確保機器人遵守安全規範，進而防止可能傷害人類或造成財產損失的行為。儘管有進展，包括將知識圖譜 (KG) 與大型語言模型 (LLM) 整合，但仍有挑戰存在於確保自主機器人行為的一致性安全。在本文中，我們提出將大型語言模型與具體機器人控制提示 (ERCP) 和具體知識圖譜 (EKG) 進行創新整合，以增強服務機器人的安全架構。ERCP 被設計為預先定義的指令，可確保 LLM 產生安全且精確的回應。這些回應隨後由 EKG 驗證，EKG 提供了一個全面的知識庫，確保機器人的行為持續符合安全協定，進而促進在各種情境中更安全的運作實務。我們的實驗設置涉及多樣化的真實世界任務，配備我們架構的機器人與傳統方法相比，展現出顯著更高的安全標準遵循度。此整合促進了安全的人機互動，並將我們的技術定位於服務機器人中 AI 驅動安全創新的最前線。

##### **Cost-efficient Knowledge-based Question Answering with Large Language Models**
2405.17337v1 by Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, Xiao Huang

Knowledge-based question answering (KBQA) is widely used in many scenarios
that necessitate domain knowledge. Large language models (LLMs) bring
opportunities to KBQA, while their costs are significantly higher and absence
of domain-specific knowledge during pre-training. We are motivated to combine
LLMs and prior small models on knowledge graphs (KGMs) for both inferential
accuracy and cost saving. However, it remains challenging since accuracy and
cost are not readily combined in the optimization as two distinct metrics. It
is also laborious for model selection since different models excel in diverse
knowledge. To this end, we propose Coke, a novel cost-efficient strategy for
KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize
calls to LLMs within limited budgets. We first formulate the accuracy
expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A
context-aware policy is optimized to further distinguish the expert model
subject to the question semantics. The overall decision is bounded by the cost
regret according to historical expenditure on failures. Extensive experiments
showcase the superior performance of Coke, which moves the Pareto frontier with
up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on
the benchmark datasets.

摘要：<paragraph>基於知識的問答 (KBQA) 廣泛應用於許多需要領域知識的場景中。大型語言模型 (LLM) 為 KBQA 帶來了機會，但其成本顯著提高，且在預訓練期間缺乏特定領域的知識。我們有動力將 LLM 和先前的知識圖譜 (KGM) 上的小模型結合起來，以提高推理準確性和節省成本。然而，由於準確性和成本無法在優化中作為兩個不同的指標輕易地結合在一起，因此這仍然具有挑戰性。由於不同的模型擅長於不同的知識，因此模型選擇也很費力。為此，我們提出了 Coke，一種針對 LLM 的新穎且具有成本效益的 KBQA 策略，它被建模為一個定制的多臂賭博機問題，以在有限的預算內最大程度地減少對 LLM 的呼叫。我們首先使用針對 KGM 或 LLM 的群集級 Thompson 採樣來制定準確性期望。優化了一個上下文感知策略，以進一步區分問題語義的主題模型。根據失敗的歷史支出，總體決策受到成本遺憾的約束。大量的實驗展示了 Coke 的優越性能，它將帕累托前沿移動了多達 20.89%，同時在基準數據集上實現了 2.74% 的更高準確性。</paragraph>

##### **Assessing LLMs Suitability for Knowledge Graph Completion**
2405.17249v1 by Vasile Ionut Remus Iga, Gheorghe Cosmin Silaghi

Recent work shown the capability of Large Language Models (LLMs) to solve
tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in
Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or
output results in a non-deterministic manner, thus leading to wrongly reasoned
responses, even if they satisfy the user's demands. To highlight opportunities
and challenges in knowledge graphs-related tasks, we experiment with two
distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125,
on Knowledge Graph Completion for static knowledge graphs, using prompts
constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a
Task-Oriented Dialogue system use case. When evaluated using both strict and
flexible metrics measurement manners, our results show that LLMs could be fit
for such a task if prompts encapsulate sufficient information and relevant
examples.

摘要：最近的研究显示，大型语言模型 (LLM) 具备解决知识图谱相关任务的能力，例如知识图谱补全，即使在零次或小样本的情况下也是如此。然而，众所周知，它们会产生幻觉答案，或以非确定性的方式输出结果，从而导致推理错误的响应，即使它们满足了用户的需求。为了突出知识图谱相关任务中的机遇和挑战，我们对两种杰出的 LLM 进行了实验，分别是 Mixtral-8x7B-Instruct-v0.1 和 gpt-3.5-turbo-0125，在静态知识图谱的知识图谱补全上，使用根据 TELeR 分类法构建的提示，在零次和一次上下文中，在面向任务的对话系统用例中。当使用严格和灵活的度量方式进行评估时，我们的结果表明，如果提示包含足够的信息和相关示例，则 LLM 适用于此类任务。

##### **Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning**
2405.16933v1 by Xun Liang, Simin Niu, Zhiyu li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi

Retrieval-Augmented Generation (RAG) offers a cost-effective approach to
injecting real-time knowledge into large language models (LLMs). Nevertheless,
constructing and validating high-quality knowledge repositories require
considerable effort. We propose a pre-retrieval framework named Pseudo-Graph
Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students
by providing them with abundant raw reading materials and encouraging them to
engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through
common topics or complementary facts to form a pseudo-graph database. During
the retrieval phase, PG-RAG mimics the human behavior in flipping through
notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates
highly corroborated fact paths to provide a structured and refined sub-graph
assisting LLMs. We validated PG-RAG on three specialized question-answering
datasets. In single-document tasks, PG-RAG significantly outperformed the
current best baseline, KGP-LLaMA, across all key evaluation metrics, with an
average overall performance improvement of 11.6%. Specifically, its BLEU score
increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In
multi-document scenarios, the average metrics of PG-RAG were at least 2.35%
higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed
stable improvements of around 7.55% and 12.75%, respectively. Our code:
https://github.com/IAAR-Shanghai/PGRAG.

摘要：<paragraph>檢索增強生成（RAG）提供了一種具成本效益的方法，可以將即時知識注入大型語言模型（LLM）。儘管如此，建構和驗證高品質的知識儲存庫需要相當大的努力。我們提出了一個名為偽圖形檢索增強生成（PG-RAG）的預檢索架構，它將 LLM 概念化為學生，為他們提供豐富的原始閱讀材料，並鼓勵他們從事自主閱讀，用自己的話記錄事實資訊。由此產生的簡潔、組織良好的心智索引通過共同的主題或補充事實相互連接，形成一個偽圖形資料庫。在檢索階段，PG-RAG 模仿人類在翻閱筆記、識別事實路徑並隨後探索相關背景中的行為。遵循眾人走的路是最好的原則，它整合了高度證實的事實路徑，以提供一個結構化且精煉的子圖，協助 LLM。我們在三個專業問答資料集上驗證了 PG-RAG。在單一文件任務中，PG-RAG 在所有關鍵評估指標上都顯著優於目前的最佳基準 KGP-LLaMA，平均整體效能提升了 11.6%。具體來說，它的 BLEU 分數提高了大約 14.3%，QE-F1 指標提高了 23.7%。在多文件場景中，PG-RAG 的平均指標至少比最佳基準高 2.35%。值得注意的是，BLEU 分數和 QE-F1 指標分別顯示出約 7.55% 和 12.75% 的穩定提升。我們的程式碼：https://github.com/IAAR-Shanghai/PGRAG。</paragraph>

##### **Entity Alignment with Noisy Annotations from Large Language Models**
2405.16806v2 by Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang

Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying
equivalent entity pairs. While existing methods heavily rely on human-generated
labels, it is prohibitively expensive to incorporate cross-domain experts for
annotation in real-world scenarios. The advent of Large Language Models (LLMs)
presents new avenues for automating EA with annotations, inspired by their
comprehensive capability to process semantic information. However, it is
nontrivial to directly apply LLMs for EA since the annotation space in
real-world KGs is large. LLMs could also generate noisy labels that may mislead
the alignment. To this end, we propose a unified framework, LLM4EA, to
effectively leverage LLMs for EA. Specifically, we design a novel active
learning policy to significantly reduce the annotation space by prioritizing
the most valuable entities based on the entire inter-KG and intra-KG structure.
Moreover, we introduce an unsupervised label refiner to continuously enhance
label accuracy through in-depth probabilistic reasoning. We iteratively
optimize the policy based on the feedback from a base EA model. Extensive
experiments demonstrate the advantages of LLM4EA on four benchmark datasets in
terms of effectiveness, robustness, and efficiency. Codes are available via
https://github.com/chensyCN/llm4ea_official.

摘要：實體比對 (EA) 旨在透過識別等效的實體對來合併兩個知識圖譜 (KG)。雖然現有方法極度依賴人工產生的標籤，但在現實世界的場景中，要納入跨領域的專家進行註解是難以負擔的成本。大型語言模型 (LLM) 的出現為自動化 EA 提供了新的途徑，其靈感來自於 LLM 全面處理語義資訊的能力。然而，由於現實世界 KG 中的註解空間很大，因此直接應用 LLM 於 EA 並非易事。LLM 也有可能產生雜訊標籤，進而誤導比對。為了解決這個問題，我們提出了一個統一的架構 LLM4EA，以有效利用 LLM 進行 EA。具體來說，我們設計了一種新穎的主動學習政策，透過根據整個跨 KG 和內部 KG 結構，優先處理最有價值的實體，從而大幅減少註解空間。此外，我們引入了一個無監督標籤精煉器，以透過深入的機率推理持續提升標籤的準確性。我們根據基礎 EA 模型的回饋，反覆最佳化政策。廣泛的實驗證明了 LLM4EA 在四個基準資料集上的優勢，包括在效能、穩健性和效率方面。程式碼可透過 https://github.com/chensyCN/llm4ea_official 取得。

##### **TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations**
2405.16800v1 by Zheng Zhang, Yuntong Hu, Bo Pan, Chen Ling, Liang Zhao

Text-Attributed Graphs (TAGs) enhance graph structures with natural language
descriptions, enabling detailed representation of data and their relationships
across a broad spectrum of real-world scenarios. Despite the potential for
deeper insights, existing TAG representation learning primarily relies on
supervised methods, necessitating extensive labeled data and limiting
applicability across diverse contexts. This paper introduces a new
self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA),
which overcomes these constraints by integrating TAGs' structural and semantic
dimensions. TAGA constructs two complementary views: Text-of-Graph view, which
organizes node texts into structured documents based on graph topology, and the
Graph-of-Text view, which converts textual nodes and connections into graph
data. By aligning representations from both views, TAGA captures joint textual
and structural information. In addition, a novel structure-preserving random
walk algorithm is proposed for efficient training on large-sized TAGs. Our
framework demonstrates strong performance in zero-shot and few-shot scenarios
across eight real-world datasets.

摘要：文本属性图 (TAG) 使用自然语言描述增强图结构，能够详细表示数据及其在广泛的真实世界场景中的关系。尽管有更深入见解的潜力，但现有的 TAG 表示学习主要依赖于监督方法，需要大量的标记数据，并限制了在不同上下文中的适用性。本文介绍了一个新的自监督学习框架，文本和图多视图对齐 (TAGA)，通过整合 TAG 的结构和语义维度来克服这些限制。TAGA 构建了两个互补的视图：图文本视图，它根据图拓扑将节点文本组织成结构化文档；文本图视图，它将文本节点和连接转换为图数据。通过对齐来自两个视图的表示，TAGA 捕获了联合文本和结构信息。此外，提出了一种新颖的结构保持随机游走算法，用于对大型 TAG 进行高效训练。我们的框架在八个真实世界数据集的零样本和少样本场景中展示了强大的性能。

##### **KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge**
2405.16412v1 by Pengcheng Jiang, Lang Cao, Cao Xiao, Parminder Bhatia, Jimeng Sun, Jiawei Han

Knowledge Graph Embedding (KGE) techniques are crucial in learning compact
representations of entities and relations within a knowledge graph,
facilitating efficient reasoning and knowledge discovery. While existing
methods typically focus either on training KGE models solely based on graph
structure or fine-tuning pre-trained language models with classification data
in KG, KG-FIT leverages LLM-guided refinement to construct a semantically
coherent hierarchical structure of entity clusters. By incorporating this
hierarchical knowledge along with textual information during the fine-tuning
process, KG-FIT effectively captures both global semantics from the LLM and
local semantics from the KG. Extensive experiments on the benchmark datasets
FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over
state-of-the-art pre-trained language model-based methods, achieving
improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link
prediction task, respectively. Furthermore, KG-FIT yields substantial
performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based
base models upon which it is built. These results highlight the effectiveness
of KG-FIT in incorporating open-world knowledge from LLMs to significantly
enhance the expressiveness and informativeness of KG embeddings.

摘要：知識圖譜嵌入 (KGE) 技術對於學習知識圖譜中實體和關係的緊湊表示至關重要，促進了高效的推理和知識發現。雖然現有方法通常專注於僅基於圖形結構訓練 KGE 模型或使用 KG 中的分類數據微調預訓練的語言模型，但 KG-FIT 利用 LLM 指導的精煉來構建語義上連貫的實體群集層次結構。通過在微調過程中將這種層次知識與文本信息結合起來，KG-FIT 有效地從 LLM 中捕獲全局語義，從 KG 中捕獲局部語義。在基準數據集 FB15K-237、YAGO3-10 和 PrimeKG 上進行的廣泛實驗證明了 KG-FIT 優於最先進的基於預訓練語言模型的方法，在鏈路預測任務的 Hits@10 指標中分別取得了 14.4%、13.5% 和 11.9% 的改進。此外，與其構建的基於結構的基礎模型相比，KG-FIT 在性能上顯著提升了 12.6%、6.7% 和 17.7%。這些結果突出了 KG-FIT 在整合來自 LLM 的開放世界知識以顯著增強 KG 嵌入的表達力和信息量方面的有效性。

##### **Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level**
2405.16405v1 by Runlin Lei, Yuwei Hu, Yuchen Ren, Zhewei Wei

Graph Neural Networks (GNNs) excel across various applications but remain
vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),
which inject malicious nodes into the original graph and pose realistic
threats. Text-attributed graphs (TAGs), where nodes are associated with textual
features, are crucial due to their prevalence in real-world applications and
are commonly used to evaluate these vulnerabilities. However, existing research
only focuses on embedding-level GIAs, which inject node embeddings rather than
actual textual content, limiting their applicability and simplifying detection.
In this paper, we pioneer the exploration of GIAs at the text level, presenting
three novel attack designs that inject textual content into the graph. Through
theoretical and empirical analysis, we demonstrate that text interpretability,
a factor previously overlooked at the embedding level, plays a crucial role in
attack strength. Among the designs we investigate, the Word-frequency-based
Text-level GIA (WTGIA) is particularly notable for its balance between
performance and interpretability. Despite the success of WTGIA, we discover
that defenders can easily enhance their defenses with customized text embedding
methods or large language model (LLM)--based predictors. These insights
underscore the necessity for further research into the potential and practical
significance of text-level GIAs.

摘要：圖形神經網路 (GNN) 在各種應用中表現出色，但仍容易受到對抗性攻擊，特別是圖形注入攻擊 (GIA)，它會將惡意節點注入原始圖形並構成現實威脅。文字屬性圖形 (TAG) 由於在實際應用中很普遍，因此至關重要，並且通常用於評估這些漏洞。然而，現有的研究僅關注嵌入層級的 GIA，它注入節點嵌入而非實際文本內容，限制了它們的適用性並簡化了檢測。在本文中，我們率先探索文本層級的 GIA，提出了三種將文本內容注入圖形的創新攻擊設計。透過理論和實證分析，我們證明了文本可解釋性（在嵌入層級以前被忽略的因素）在攻擊強度中扮演了至關重要的角色。在我們調查的設計中，基於詞頻的文本層級 GIA (WTGIA) 特別以其效能和可解釋性之間的平衡而著稱。儘管 WTGIA 成功的，我們發現防禦者可以透過自訂文字嵌入方法或大型語言模型 (LLM) 為基礎的預測器輕鬆加強他們的防禦。這些見解強調進一步研究文本層級 GIA 的潛力和實際意義的必要性。

##### **COLT: Towards Completeness-Oriented Tool Retrieval for Large Language Models**
2405.16089v1 by Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, the integration of external tools with Large Language Models (LLMs)
has emerged as a promising approach to overcome the inherent constraints of
their pre-training data. However, realworld applications often involve a
diverse range of tools, making it infeasible to incorporate all tools directly
into LLMs due to constraints on input length and response time. Therefore, to
fully exploit the potential of tool-augmented LLMs, it is crucial to develop an
effective tool retrieval system. Existing tool retrieval methods techniques
mainly rely on semantic matching between user queries and tool descriptions,
which often results in the selection of redundant tools. As a result, these
methods fail to provide a complete set of diverse tools necessary for
addressing the multifaceted problems encountered by LLMs. In this paper, we
propose a novel modelagnostic COllaborative Learning-based Tool Retrieval
approach, COLT, which captures not only the semantic similarities between user
queries and tool descriptions but also takes into account the collaborative
information of tools. Specifically, we first fine-tune the PLM-based retrieval
models to capture the semantic relationships between queries and tools in the
semantic learning stage. Subsequently, we construct three bipartite graphs
among queries, scenes, and tools and introduce a dual-view graph collaborative
learning framework to capture the intricate collaborative relationships among
tools during the collaborative learning stage. Extensive experiments on both
the open benchmark and the newly introduced ToolLens dataset show that COLT
achieves superior performance. Notably, the performance of BERT-mini (11M) with
our proposed model framework outperforms BERT-large (340M), which has 30 times
more parameters. Additionally, we plan to publicly release the ToolLens dataset
to support further research in tool retrieval.

摘要：<paragraph>最近，外部工具与大型语言模型 (LLM) 的整合已成为克服其预训练数据固有约束的一种有前途的方法。然而，现实世界的应用程序通常涉及各种工具，由于输入长度和响应时间的限制，将所有工具直接整合到 LLM 中是不可行的。因此，为了充分利用工具增强型 LLM 的潜力，至关重要的是开发一个有效的工具检索系统。现有的工具检索方法技术主要依赖于用户查询和工具描述之间的语义匹配，这通常会导致选择冗余工具。因此，这些方法无法提供解决 LLM 遇到的多方面问题所需的完整且多样化的工具集。在本文中，我们提出了一种新颖的与模型无关的基于协作学习的工具检索方法 COLT，它不仅捕获了用户查询和工具描述之间的语义相似性，还考虑了工具的协作信息。具体来说，我们首先微调基于 PLM 的检索模型，以在语义学习阶段捕获查询和工具之间的语义关系。随后，我们在查询、场景和工具之间构建了三个二分图，并引入了一个双视图图协作学习框架，以在协作学习阶段捕获工具之间错综复杂的协作关系。在开放基准和新引入的 ToolLens 数据集上的大量实验表明，COLT 取得了卓越的性能。值得注意的是，我们提出的模型框架中 BERT-mini（11M）的性能优于 BERT-large（340M），而后者的参数多 30 倍。此外，我们计划公开发布 ToolLens 数据集，以支持工具检索的进一步研究。</paragraph>

##### **Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**
2405.15739v2 by Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis

Citation practices are crucial in shaping the structure of scientific
knowledge, yet they are often influenced by contemporary norms and biases. The
emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic
to these practices. Interestingly, the characteristics and potential biases of
references recommended by LLMs that entirely rely on their parametric
knowledge, and not on search or retrieval-augmented generation, remain
unexplored. Here, we analyze these characteristics in an experiment using a
dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after
GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our
experiment, GPT-4 was tasked with suggesting scholarly references for the
anonymized in-text citations within these papers. Our findings reveal a
remarkable similarity between human and LLM citation patterns, but with a more
pronounced high citation bias in GPT-4, which persists even after controlling
for publication year, title length, number of authors, and venue. Additionally,
we observe a large consistency between the characteristics of GPT-4's existing
and non-existent generated references, indicating the model's internalization
of citation patterns. By analyzing citation graphs, we show that the references
recommended by GPT-4 are embedded in the relevant citation context, suggesting
an even deeper conceptual internalization of the citation networks. While LLMs
can aid in citation generation, they may also amplify existing biases and
introduce new ones, potentially skewing scientific knowledge dissemination. Our
results underscore the need for identifying the model's biases and for
developing balanced methods to interact with LLMs in general.

摘要：<paragraph>引證實務對於形塑科學知識的結構至關重要，然而往往會受到當代規範和偏見的影響。大型語言模型 (LLM) 如 GPT-4 的出現，為這些實務帶來了新的動態。有趣的是，完全依賴其參數化知識（而非搜尋或檢索增強的生成）的 LLM 所推薦的參考文獻，其特性和潛在偏見仍未被探討。在此，我們使用一個包含 166 篇論文的資料集進行實驗，分析這些特性，這些論文來自 AAAI、NeurIPS、ICML 和 ICLR，並於 GPT-4 知識截止日期後發表，總共涵蓋 3,066 篇參考文獻。在我們的實驗中，GPT-4 的任務是為這些論文中匿名的內文引文建議學術參考文獻。我們的研究結果顯示，人類和 LLM 的引證模式有顯著的相似性，但 GPT-4 的高引證偏見較為明顯，即使在控制了出版年份、標題長度、作者人數和發表地點後，這種偏見仍然存在。此外，我們觀察到 GPT-4 現有和不存在的生成參考文獻之間的一致性很高，這表示模型內化了引證模式。透過分析引證圖，我們發現 GPT-4 推薦的參考文獻嵌入在相關的引證脈絡中，這表示對引證網路有更深入的概念內化。雖然 LLM 可以協助生成引證，但它們也可能放大現有的偏見並引入新的偏見，進而可能扭曲科學知識的傳播。我們的研究結果強調了識別模型偏見和開發與 LLM 互動的平衡方法的需求。</paragraph>

##### **Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph**
2405.15374v1 by Runsong Jia, Bowen Zhang, Sergio J. Rodríguez Méndez, Pouya G. Omran

The proposed research aims to develop an innovative semantic query processing
system that enables users to obtain comprehensive information about research
works produced by Computer Science (CS) researchers at the Australian National
University (ANU). The system integrates Large Language Models (LLMs) with the
ANU Scholarly Knowledge Graph (ASKG), a structured repository of all
research-related artifacts produced at ANU in the CS field. Each artifact and
its parts are represented as textual nodes stored in a Knowledge Graph (KG).
  To address the limitations of traditional scholarly KG construction and
utilization methods, which often fail to capture fine-grained details, we
propose a novel framework that integrates the Deep Document Model (DDM) for
comprehensive document representation and the KG-enhanced Query Processing
(KGQP) for optimized complex query handling. DDM enables a fine-grained
representation of the hierarchical structure and semantic relationships within
academic papers, while KGQP leverages the KG structure to improve query
accuracy and efficiency with LLMs.
  By combining the ASKG with LLMs, our approach enhances knowledge utilization
and natural language understanding capabilities. The proposed system employs an
automatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from
the ASKG. Initial experiments demonstrate that our framework is superior to
baseline methods in terms of accuracy retrieval and query efficiency.
  We showcase the practical application of our framework in academic research
scenarios, highlighting its potential to revolutionize scholarly knowledge
management and discovery. This work empowers researchers to acquire and utilize
knowledge from documents more effectively and provides a foundation for
developing precise and reliable interactions with LLMs.

摘要：<paragraph>擬議的研究旨在開發創新的語義查詢處理系統，使用戶能夠獲得由澳洲國立大學（ANU）電腦科學（CS）研究人員產出的研究著作的全面資訊。此系統整合大型語言模型（LLM）與澳洲國立大學學術知識圖譜（ASKG），後者為在澳洲國立大學 CS 領域產出的所有研究相關人工製品的結構化儲存庫。每個人工製品及其部分以儲存在知識圖譜（KG）中的文字節點表示。
為了解決傳統學術 KG 建構和使用方式的限制，這些方式通常無法擷取細微的細節，我們提出一個新的架構，它整合深度文件模型（DDM）以進行全面的文件表示，以及 KG 增強查詢處理（KGQP）以進行最佳化的複雜查詢處理。DDM 能夠細微地表示學術論文中的階層結構和語義關係，而 KGQP 則利用 KG 結構透過 LLM 來改善查詢的準確性和效率。
透過將 ASKG 與 LLM 結合，我們的做法增強了知識利用和自然語言理解能力。擬議的系統採用自動 LLM-SPARQL 融合來從 ASKG 擷取相關事實和文字節點。初步實驗顯示，我們的架構在準確性擷取和查詢效率方面優於基準方法。
我們展示了我們的架構在學術研究情境中的實際應用，強調其革新學術知識管理和發現的潛力。這項工作賦能研究人員更有效地取得和利用文件中的知識，並為與 LLM 進行精確且可靠的互動奠定基礎。</paragraph>

##### **Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models**
2405.15143v2 by Cong Lu, Shengran Hu, Jeff Clune

Go-Explore is a powerful family of algorithms designed to solve
hard-exploration problems, built on the principle of archiving discovered
states, and iteratively returning to and exploring from the most promising
states. This approach has led to superhuman performance across a wide variety
of challenging problems including Atari games and robotic control, but requires
manually designing heuristics to guide exploration, which is time-consuming and
infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE)
which greatly extends the scope of the original Go-Explore by replacing these
heuristics with the intelligence and internalized human notions of
interestingness captured by giant foundation models (FMs). This provides IGE
with a human-like ability to instinctively identify how interesting or
promising any new state is (e.g. discovering new objects, locations, or
behaviors), even in complex environments where heuristics are hard to define.
Moreover, IGE offers the exciting and previously impossible opportunity to
recognize and capitalize on serendipitous discoveries that cannot be predicted
ahead of time. We evaluate IGE on a range of language-based tasks that require
search and exploration. In Game of 24, a multistep mathematical reasoning
problem, IGE reaches 100% success rate 70.8% faster than the best classic graph
search baseline. Next, in BabyAI-Text, a challenging partially observable
gridworld, IGE exceeds the previous SOTA with orders of magnitude fewer online
samples. Finally, in TextWorld, we show the unique ability of IGE to succeed in
settings requiring long-horizon exploration where prior SOTA FM agents like
Reflexion completely fail. Overall, IGE combines the tremendous strengths of
FMs and the powerful Go-Explore algorithm, opening up a new frontier of
research into creating more generally capable agents with impressive
exploration capabilities.

摘要：Go-Explore 是一系列強大的演算法，旨在解決困難的探索問題，建立在封存已發現狀態的原則上，並反覆返回並從最有希望的狀態進行探索。這種方法已在包括 Atari 遊戲和機器人控制在內的各種具有挑戰性的問題中取得了超人的表現，但需要人工設計啟發式方法來指導探索，這既耗時又通常不可行。為了解決這個問題，我們提出了 Intelligent Go-Explore (IGE)，它通過用巨型基礎模型 (FM) 捕捉到的智慧和內化的人類有趣概念取代這些啟發式方法，極大地擴展了原始 Go-Explore 的範圍。這為 IGE 提供了類似人類的能力，可以本能地識別任何新狀態有多有趣或有希望（例如發現新物件、位置或行為），即使在難以定義啟發式方法的複雜環境中也是如此。此外，IGE 提供了令人興奮且以前不可能的機會，可以識別並利用無法預先預測的意外發現。我們在需要搜尋和探索的一系列基於語言的任務上評估了 IGE。在 24 點遊戲中，一個多步驟的數學推理問題，IGE 的成功率達到 100%，比最佳經典圖形搜尋基準快 70.8%。接下來，在 BabyAI-Text 中，一個具有挑戰性的部分可觀察網格世界，IGE 超越了先前的 SOTA，在線範例減少了幾個數量級。最後，在 TextWorld 中，我們展示了 IGE 在需要長期探索的設定中取得成功的獨特能力，而先前的 SOTA FM 代理（如 Reflexion）則完全失敗。總的來說，IGE 結合了 FM 的巨大優勢和強大的 Go-Explore 演算法，開闢了一項新的研究領域，旨在創造具有令人印象深刻的探索能力的更通用的代理。

##### **HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**
2405.14831v1 by Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su

In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.

摘要：為了在惡劣且瞬息萬變的自然環境中茁壯成長，
哺乳動物的大腦演化出儲存大量關於世界知識的能力，
並在避免災難性遺忘的同時持續整合新資訊。儘管有令人印象深刻的成就，
大型語言模型 (LLM)，即使具備檢索增強生成 (RAG)，仍難以
在預訓練後有效率且有效地整合大量新經驗。在這項工作中，我們介紹 HippoRAG，一種創新的檢索架構，
靈感來自人類長期記憶的海馬迴索引理論，以在新的經驗中實現更深入且更有效率的知識整合。HippoRAG
協同編排 LLM、知識圖譜和個人化 PageRank 演算法，以模擬人類記憶中新皮質和海馬迴的不同角色。我們將 HippoRAG 與現有的 RAG 方法進行多跳式問答比較，並展示我們的
方法顯著優於最先進的方法，最多達 20%。使用 HippoRAG 的單步檢索可實現
與 IRCoT 等迭代檢索相當或更好的效能，同時成本降低 10-30 倍，速度加快 6-13 倍，將 HippoRAG 整合到
IRCoT 中可帶來進一步的實質收益。最後，我們展示我們的
方法可以處理現有方法無法達到的新型態場景。程式碼
和資料可在 https://github.com/OSU-NLP-Group/HippoRAG 取得。

##### **Fisher Flow Matching for Generative Modeling over Discrete Data**
2405.14664v3 by Oscar Davis, Samuel Kessler, Mircea Petrache, İsmail İlkan Ceylan, Michael Bronstein, Avishek Joey Bose

Generative modeling over discrete data has recently seen numerous success
stories, with applications spanning language modeling, biological sequence
design, and graph-structured molecular data. The predominant generative
modeling paradigm for discrete data is still autoregressive, with more recent
alternatives based on diffusion or flow-matching falling short of their
impressive performance in continuous data settings, such as image or video
generation. In this work, we introduce Fisher-Flow, a novel flow-matching model
for discrete data. Fisher-Flow takes a manifestly geometric perspective by
considering categorical distributions over discrete data as points residing on
a statistical manifold equipped with its natural Riemannian metric: the
$\textit{Fisher-Rao metric}$. As a result, we demonstrate discrete data itself
can be continuously reparameterised to points on the positive orthant of the
$d$-hypersphere $\mathbb{S}^d_+$, which allows us to define flows that map any
source distribution to target in a principled manner by transporting mass along
(closed-form) geodesics of $\mathbb{S}^d_+$. Furthermore, the learned flows in
Fisher-Flow can be further bootstrapped by leveraging Riemannian optimal
transport leading to improved training dynamics. We prove that the gradient
flow induced by Fisher-Flow is optimal in reducing the forward KL divergence.
  We evaluate Fisher-Flow on an array of synthetic and diverse real-world
benchmarks, including designing DNA Promoter, and DNA Enhancer sequences.
Empirically, we find that Fisher-Flow improves over prior diffusion and
flow-matching models on these benchmarks.

摘要：<paragraph>離散資料的生成式模型最近獲得許多成功案例，應用範圍涵蓋語言模型、生物序列設計和圖形結構分子資料。離散資料的主要生成式模型範例仍然是自迴歸，而最近基於擴散或流匹配的替代方案無法達到在連續資料設定（例如影像或影片生成）中的驚人效能。在這項工作中，我們介紹 Fisher-Flow，這是一種針對離散資料的新穎流匹配模型。Fisher-Flow 採用明顯的幾何觀點，將離散資料上的類別分佈視為駐留在具有其自然黎曼度規的統計流形的點：$\textit{Fisher-Rao 度量}$. 因此，我們證明離散資料本身可以連續重新參數化為 $d$-超球面 $\mathbb{S}^d_+$ 的正軸向上的點，這讓我們可以定義流，以有原則的方式將任何源分佈映射到目標，方法是沿著 $\mathbb{S}^d_+$ 的（閉合形式）測地線傳輸質量。此外，Fisher-Flow 中學習到的流可以進一步透過利用黎曼最佳傳輸進行引導，從而改善訓練動態。我們證明 Fisher-Flow 誘導的梯度流在減少前向 KL 散度方面是最理想的。我們在各種合成和多元的真實世界基準上評估 Fisher-Flow，包括設計 DNA 啟動子，以及 DNA 增強子序列。根據經驗，我們發現 Fisher-Flow 在這些基準上改進了先前的擴散和流匹配模型。</paragraph>

##### **GLaD: Synergizing Molecular Graphs and Language Descriptors for Enhanced Power Conversion Efficiency Prediction in Organic Photovoltaic Devices**
2405.14203v1 by Thao Nguyen, Tiara Torres-Flores, Changhyun Hwang, Carl Edwards, Ying Diao, Heng Ji

This paper presents a novel approach for predicting Power Conversion
Efficiency (PCE) of Organic Photovoltaic (OPV) devices, called GLaD:
synergizing molecular Graphs and Language Descriptors for enhanced PCE
prediction. Due to the lack of high-quality experimental data, we collect a
dataset consisting of 500 pairs of OPV donor and acceptor molecules along with
their corresponding PCE values, which we utilize as the training data for our
predictive model. In this low-data regime, GLaD leverages properties learned
from large language models (LLMs) pretrained on extensive scientific literature
to enrich molecular structural representations, allowing for a multimodal
representation of molecules. GLaD achieves precise predictions of PCE, thereby
facilitating the synthesis of new OPV molecules with improved efficiency.
Furthermore, GLaD showcases versatility, as it applies to a range of molecular
property prediction tasks (BBBP, BACE, ClinTox, and SIDER), not limited to
those concerning OPV materials. Especially, GLaD proves valuable for tasks in
low-data regimes within the chemical space, as it enriches molecular
representations by incorporating molecular property descriptions learned from
large-scale pretraining. This capability is significant in real-world
scientific endeavors like drug and material discovery, where access to
comprehensive data is crucial for informed decision-making and efficient
exploration of the chemical space.

摘要：這篇論文提出了一個新穎的方法來預測有機光電 (OPV) 裝置的功率轉換效率 (PCE)，稱為 GLaD：結合分子圖形和語言描述符以增強 PCE 預測。由於缺乏高品質的實驗數據，我們收集了一個包含 500 對 OPV 供體和受體分子的數據集，以及對應的 PCE 值，我們將其用作預測模型的訓練數據。在這個低數據模式中，GLaD 利用從大型語言模型 (LLM) 中學習到的特性，這些 LLM 在廣泛的科學文獻上進行預訓練，以豐富分子結構表示，從而允許對分子進行多模態表示。GLaD 對 PCE 進行精確預測，從而促進合成具有更高效率的新型 OPV 分子。此外，GLaD 展示了其多功能性，因為它適用於一系列分子特性預測任務 (BBBP、BACE、ClinTox 和 SIDER)，而不僅限於那些涉及 OPV 材料的任務。特別是，GLaD 證明對於化學空間中的低數據模式任務非常有價值，因為它通過結合從大規模預訓練中學習到的分子特性描述來豐富分子表示。這種能力在像藥物和材料發現這樣的現實世界科學工作中非常重要，在這些工作中，獲得全面數據對於明智的決策制定和化學空間的有效探索至關重要。

##### **Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning**
2405.14170v1 by Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan Wee-Chung Liew, Shirui Pan, Baocai Yin

Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing
temporal information to capture complex relations within a Temporal Knowledge
Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically
depend on deep learning algorithms or temporal logical rules. However, deep
learning-based TKGRs often lack interpretability, whereas rule-based TKGRs
struggle to effectively learn temporal rules that capture temporal patterns.
Recently, Large Language Models (LLMs) have demonstrated extensive knowledge
and remarkable proficiency in temporal reasoning. Consequently, the employment
of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing
interest among researchers. Nonetheless, LLMs are known to function as black
boxes, making it challenging to comprehend their reasoning process.
Additionally, due to the resource-intensive nature of fine-tuning, promptly
updating LLMs to integrate evolving knowledge within TKGs for reasoning is
impractical. To address these challenges, in this paper, we propose a Large
Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on
TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze
historical data and extract temporal logical rules. These rules unveil temporal
patterns and facilitate interpretable reasoning. To account for the evolving
nature of TKGs, a dynamic adaptation strategy is proposed to update the
LLM-generated rules with the latest events. This ensures that the extracted
rules always incorporate the most recent knowledge and better generalize to the
predictions on future events. Experimental results show that without the need
of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over
several common datasets, providing a robust framework for TKGR tasks.

摘要：時序知識圖表推理 (TKGR) 是利用時序資訊來擷取時序知識圖表 (TKG) 中複雜關係的過程，以推論出新的知識。TKGR 中的傳統方法通常依賴深度學習演算法或時序邏輯規則。然而，基於深度學習的 TKGR 經常缺乏可解釋性，而基於規則的 TKGR 則難以有效學習擷取時序模式的時序規則。最近，大型語言模型 (LLM) 已展現出廣泛的知識和卓越的時序推理能力。因此，使用 LLM 進行時序知識圖表推理 (TKGR) 已引起研究人員越來越大的興趣。儘管如此，眾所周知 LLM 的功能就像黑盒子，這使得理解其推理過程具有挑戰性。此外，由於微調的資源密集性，即時更新 LLM 以整合 TKG 中不斷演化的知識進行推理是不切實際的。為了應對這些挑戰，我們在本文中提出了一種大型語言模型引導的動態適應 (LLM-DA) 方法，用於對 TKG 進行推理。具體來說，LLM-DA 利用 LLM 的能力來分析歷史資料並提取時序邏輯規則。這些規則揭示了時序模式並促進可解釋的推理。為了說明 TKG 的演化性質，提出了一種動態適應策略，以使用最新事件更新 LLM 生成的規則。這確保了提取的規則始終包含最新的知識，並更好地概括對未來事件的預測。實驗結果表明，無需微調，LLM-DA 就顯著提高了對幾個常見資料集的推理準確性，為 TKGR 任務提供了一個穩健的框架。

##### **Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models**
2405.14012v1 by Tolga Çöplü, Arto Bendiken, Andrii Skomorokhov, Eduard Bateiko, Stephen Cobb

In applications such as personal assistants, large language models (LLMs)
must consider the user's personal information and preferences. However, LLMs
lack the inherent ability to learn from user interactions. This paper explores
capturing personal information from user prompts using ontology and
knowledge-graph approaches. We use a subset of the KNOW ontology, which models
personal information, to train the language model on these concepts. We then
evaluate the success of knowledge capture using a specially constructed
dataset. Our code and datasets are publicly available at
https://github.com/HaltiaAI/paper-PTODSKC

摘要：在個人助理等應用程式中，大型語言模型 (LLM)
必須考量使用者的個人資訊與偏好。然而，LLM
缺乏從使用者互動中學習的內建能力。本文探討使用本體論和
知識圖譜方法從使用者提示中擷取個人資訊。我們使用 KNOW 本體論的子集，它建構
個人資訊，針對這些概念訓練語言模型。然後我們
使用特別建構的資料集評估知識擷取的成功率。我們的程式碼和資料集公開於
https://github.com/HaltiaAI/paper-PTODSKC

##### **LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework**
2405.13902v1 by Yiran Qiao, Xiang Ao, Yang Liu, Jiarong Xu, Xiaoqian Sun, Qing He

Recent prevailing works on graph machine learning typically follow a similar
methodology that involves designing advanced variants of graph neural networks
(GNNs) to maintain the superior performance of GNNs on different graphs. In
this paper, we aim to streamline the GNN design process and leverage the
advantages of Large Language Models (LLMs) to improve the performance of GNNs
on downstream tasks. We formulate a new paradigm, coined "LLMs-as-Consultants,"
which integrates LLMs with GNNs in an interactive manner. A framework named
LOGIN (LLM Consulted GNN training) is instantiated, empowering the interactive
utilization of LLMs within the GNN training process. First, we attentively
craft concise prompts for spotted nodes, carrying comprehensive semantic and
topological information, and serving as input to LLMs. Second, we refine GNNs
by devising a complementary coping mechanism that utilizes the responses from
LLMs, depending on their correctness. We empirically evaluate the effectiveness
of LOGIN on node classification tasks across both homophilic and heterophilic
graphs. The results illustrate that even basic GNN architectures, when employed
within the proposed LLMs-as-Consultants paradigm, can achieve comparable
performance to advanced GNNs with intricate designs. Our codes are available at
https://github.com/QiaoYRan/LOGIN.

摘要：近期的主流图机器学习研究通常遵循类似的方法，即设计图神经网络 (GNN) 的高级变体，以维持 GNN 在不同图上的优异性能。在本文中，我们旨在简化 GNN 设计流程，并利用大型语言模型 (LLM) 的优势来提升 GNN 在下游任务上的性能。我们制定了一个新的范例，称为“LLM 作为顾问”，它以交互方式将 LLM 与 GNN 集成在一起。实例化了一个名为 LOGIN（LLM 咨询的 GNN 训练）的框架，赋能了 LLM 在 GNN 训练过程中的交互式利用。首先，我们专注于为发现的节点精心设计简洁的提示，承载全面的语义和拓扑信息，并作为 LLM 的输入。其次，我们通过设计一种利用 LLM 响应的补充应对机制来优化 GNN，这取决于它们的正确性。我们对 LOGIN 在同质和异质图上的节点分类任务的有效性进行了实证评估。结果表明，即使是基本的 GNN 架构，在所提出的 LLM 作为顾问范例中使用时，也可以实现与具有复杂设计的先进 GNN 相媲美的性能。我们的代码可在 https://github.com/QiaoYRan/LOGIN 获取。

##### **FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering**
2405.13873v1 by Yuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, Bryan Hooi

While large language models (LLMs) have achieved significant success in
various applications, they often struggle with hallucinations, especially in
scenarios that require deep and responsible reasoning. These issues could be
partially mitigate by integrating external knowledge graphs (KG) in LLM
reasoning. However, the method of their incorporation is still largely
unexplored. In this paper, we propose a retrieval-exploration interactive
method, FiDelis to handle intermediate steps of reasoning grounded by KGs.
Specifically, we propose Path-RAG module for recalling useful intermediate
knowledge from KG for LLM reasoning. We incorporate the logic and common-sense
reasoning of LLMs and topological connectivity of KGs into the knowledge
retrieval process, which provides more accurate recalling performance.
Furthermore, we propose to leverage deductive reasoning capabilities of LLMs as
a better criterion to automatically guide the reasoning process in a stepwise
and generalizable manner. Deductive verification serve as precise indicators
for when to cease further reasoning, thus avoiding misleading the chains of
reasoning and unnecessary computation. Extensive experiments show that our
method, as a training-free method with lower computational cost and better
generality outperforms the existing strong baselines in three benchmarks.

摘要：儘管大型語言模型 (LLM) 已在各種應用中取得顯著成功，但它們經常會遇到幻覺問題，特別是在需要深入且負責任的推理場景中。這些問題可以透過在 LLM 推理中整合外部知識圖譜 (KG) 來部分緩解。然而，整合的方法在很大程度上仍未開發。在本文中，我們提出一個檢索探索互動方法 FiDelis 來處理由 KG 為基礎的推理中間步驟。具體來說，我們提出 Path-RAG 模組，用於從 KG 中召回對 LLM 推理有用的中間知識。我們將 LLM 的邏輯和常識推理以及 KG 的拓撲連接納入知識檢索過程中，這提供了更準確的召回效能。此外，我們建議利用 LLM 的演繹推理能力作為一個更好的準則，以逐步且可概括的方式自動引導推理過程。演繹驗證作為何時停止進一步推理的精確指標，從而避免誤導推理鏈和不必要的計算。大量的實驗表明，我們的演算法作為一種訓練免費的方法，具有較低的運算成本和更好的普遍性，在三個基準測試中優於現有的強大基準。

##### **Large Language Models are Effective Priors for Causal Graph Discovery**
2405.13551v1 by Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi

Causal structure discovery from observations can be improved by integrating
background knowledge provided by an expert to reduce the hypothesis space.
Recently, Large Language Models (LLMs) have begun to be considered as sources
of prior information given the low cost of querying them relative to a human
expert. In this work, firstly, we propose a set of metrics for assessing LLM
judgments for causal graph discovery independently of the downstream algorithm.
Secondly, we systematically study a set of prompting designs that allows the
model to specify priors about the structure of the causal graph. Finally, we
present a general methodology for the integration of LLM priors in graph
discovery algorithms, finding that they help improve performance on
common-sense benchmarks and especially when used for assessing edge
directionality. Our work highlights the potential as well as the shortcomings
of the use of LLMs in this problem space.

摘要：透過整合專家提供的背景知識，可以縮小假說空間，進而改善從觀測中發現因果結構。
近期，考量到相較於人類專家，查詢大型語言模型 (LLM) 的成本較低，因此 LLM 已開始被視為先驗資訊的來源。在這項研究中，我們首先提出了一組指標，用於評估 LLM 判斷因果圖形發現的獨立性，與下游演算法無關。其次，我們系統性地研究了一組提示設計，允許模型指定關於因果圖形結構的先驗。最後，我們提出了一種整合 LLM 先驗的通用方法，用於圖形發現演算法，發現它們有助於改善常識基準的效能，特別是在用於評估邊緣方向性時。我們的研究重點說明了在這個問題空間中使用 LLM 的潛力以及缺點。

##### **TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models**
2405.13401v2 by Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu

Large language models (LLMs) have raised concerns about potential security
threats despite performing significantly in Natural Language Processing (NLP).
Backdoor attacks initially verified that LLM is doing substantial harm at all
stages, but the cost and robustness have been criticized. Attacking LLMs is
inherently risky in security review, while prohibitively expensive. Besides,
the continuous iteration of LLMs will degrade the robustness of backdoors. In
this paper, we propose TrojanRAG, which employs a joint backdoor attack in the
Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack
scenarios. Specifically, the adversary constructs elaborate target contexts and
trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized
by contrastive learning, thus constraining the triggering conditions to a
parameter subspace to improve the matching. To improve the recall of the RAG
for the target contexts, we introduce a knowledge graph to construct structured
data to achieve hard matching at a fine-grained level. Moreover, we normalize
the backdoor scenarios in LLMs to analyze the real harm caused by backdoors
from both attackers' and users' perspectives and further verify whether the
context is a favorable tool for jailbreaking models. Extensive experimental
results on truthfulness, language understanding, and harmfulness show that
TrojanRAG exhibits versatility threats while maintaining retrieval capabilities
on normal queries.

摘要：大型語言模型 (LLM) 儘管在自然語言處理 (NLP) 中表現出色，但也引發了潛在安全威脅的擔憂。後門攻擊最初驗證了 LLM 在所有階段都造成實質性損害，但成本和穩健性也受到批評。在安全審查中攻擊 LLM 本質上是有風險的，同時成本高得令人望而卻步。此外，LLM 的持續迭代將降低後門的穩健性。在本文中，我們提出了 TrojanRAG，它在檢索增強生成中採用聯合後門攻擊，從而操縱 LLM 在通用攻擊場景中。具體來說，對手構建精細的目標上下文和觸發器集。通過對比學習正交優化多對後門捷徑，從而將觸發條件約束在參數子空間中以改善匹配。為了提高 RAG 對目標上下文的召回率，我們引入了一個知識圖譜來構建結構化數據，以在細粒度級別實現硬匹配。此外，我們對 LLM 中的後門場景進行標準化，以從攻擊者和用戶的角度分析後門造成的實際危害，並進一步驗證上下文是否是用於越獄模型的有利工具。在真實性、語言理解和危害性方面的廣泛實驗結果表明，TrojanRAG 在維護對正常查詢的檢索能力的同時表現出多功能威脅。

##### **Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction**
2405.12656v1 by Yu-Hsiang Lin, Huang-Ting Shieh, Chih-Yu Liu, Kuang-Ting Lee, Hsiao-Cheng Chang, Jing-Lun Yang, Yu-Sheng Lin

Extrapolation in Large language models (LLMs) for open-ended inquiry
encounters two pivotal issues: (1) hallucination and (2) expensive training
costs. These issues present challenges for LLMs in specialized domains and
personalized data, requiring truthful responses and low fine-tuning costs.
Existing works attempt to tackle the problem by augmenting the input of a
smaller language model with information from a knowledge graph (KG). However,
they have two limitations: (1) failing to extract relevant information from a
large one-hop neighborhood in KG and (2) applying the same augmentation
strategy for KGs with different characteristics that may result in low
performance. Moreover, open-ended inquiry typically yields multiple responses,
further complicating extrapolation. We propose a new task, the extreme
multi-label KG link prediction task, to enable a model to perform extrapolation
with multiple responses using structured real-world knowledge. Our retriever
identifies relevant one-hop neighbors by considering entity, relation, and
textual data together. Our experiments demonstrate that (1) KGs with different
characteristics require different augmenting strategies, and (2) augmenting the
language model's input with textual data improves task performance
significantly. By incorporating the retrieval-augmented framework with KG, our
framework, with a small parameter size, is able to extrapolate based on a given
KG. The code can be obtained on GitHub:
https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git

摘要：<paragraph>大型語言模型 (LLM) 中用於開放式探究的推斷會遭遇兩個關鍵問題：(1) 幻覺和 (2) 昂貴的訓練成本。這些問題為專門領域和個人化數據中的 LLM 帶來挑戰，需要真實的回應和低微調成本。現有作品嘗試通過使用來自知識圖 (KG) 的資訊擴充較小型語言模型的輸入來解決問題。然而，它們有兩個限制：(1) 無法從 KG 中的廣大一跳鄰域中提取相關資訊，以及 (2) 對具有不同特徵的 KG 應用相同的擴充策略，這可能會導致低效能。此外，開放式探究通常會產生多重回應，進一步複雜化推斷。我們提出一個新任務，即極端多標籤 KG 連結預測任務，以使模型能夠使用結構化的真實世界知識執行具有多重回應的推斷。我們的檢索器通過同時考慮實體、關係和文字資料來識別相關的一跳鄰居。我們的實驗證明：(1) 具有不同特徵的 KG 需要不同的擴充策略，以及 (2) 使用文字資料擴充語言模型的輸入會顯著改善任務效能。透過將檢索擴充框架與 KG 整合，我們的框架在參數規模較小的情況下，能夠根據給定的 KG 進行推斷。代碼可以在 GitHub 上取得：
https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git</paragraph>

##### **Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation**
2405.12442v1 by Qingyao Li, Wei Xia, Kounianhua Du, Qiji Zhang, Weinan Zhang, Ruiming Tang, Yong Yu

Concept recommendation aims to suggest the next concept for learners to study
based on their knowledge states and the human knowledge system. While knowledge
states can be predicted using knowledge tracing models, previous approaches
have not effectively integrated the human knowledge system into the process of
designing these educational models. In the era of rapidly evolving Large
Language Models (LLMs), many fields have begun using LLMs to generate and
encode text, introducing external knowledge. However, integrating LLMs into
concept recommendation presents two urgent challenges: 1) How to construct text
for concepts that effectively incorporate the human knowledge system? 2) How to
adapt non-smooth, anisotropic text encodings effectively for concept
recommendation? In this paper, we propose a novel Structure and Knowledge Aware
Representation learning framework for concept Recommendation (SKarREC). We
leverage factual knowledge from LLMs as well as the precedence and succession
relationships between concepts obtained from the knowledge graph to construct
textual representations of concepts. Furthermore, we propose a graph-based
adapter to adapt anisotropic text embeddings to the concept recommendation
task. This adapter is pre-trained through contrastive learning on the knowledge
graph to get a smooth and structure-aware concept representation. Then, it's
fine-tuned through the recommendation task, forming a
text-to-knowledge-to-recommendation adaptation pipeline, which effectively
constructs a structure and knowledge-aware concept representation. Our method
does a better job than previous adapters in transforming text encodings for
application in concept recommendation. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed approach.

摘要：概念推薦旨在根據學習者的知識狀態和人類知識系統，建議學習者學習的下一個概念。雖然知識狀態可以使用知識追蹤模型來預測，但先前的做法並未有效地將人類知識系統整合到設計這些教育模型的過程中。在快速發展的大型語言模型 (LLM) 時代，許多領域已開始使用 LLM 來產生和編碼文字，引入外部知識。然而，將 LLM 整合到概念推薦中會出現兩個迫切的挑戰：1) 如何建構有效地整合人類知識系統的概念文字？2) 如何有效地調整非平滑、各向異性的文字編碼以進行概念推薦？在本文中，我們提出了一個新的結構和知識感知表示學習架構，用於概念推薦 (SKarREC)。我們利用 LLM 的事實知識以及從知識圖譜中獲得的概念之間的先後關係來建構概念的文字表示。此外，我們提出了一個基於圖表的適配器，以將各向異性的文字嵌入調整到概念推薦任務。這個適配器透過對比學習在知識圖譜上進行預訓練，以獲得平滑且具有結構感知的概念表示。然後，透過推薦任務進行微調，形成從文字到知識再到推薦的適配管道，有效地建構一個結構和知識感知的概念表示。我們的方法在轉換文字編碼以應用於概念推薦方面，比先前的適配器做得更好。在真實世界資料集上的廣泛實驗證明了所提出方法的有效性。

##### **KG-RAG: Bridging the Gap Between Knowledge and Creativity**
2405.12035v1 by Diego Sanmartin

Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.

摘要：在確保事實準確性的同時，保持大型語言模型代理（LMA）的創建能力，對智慧型代理系統的開發構成了重大的挑戰。LMA 面臨普遍的問題，例如資訊幻覺、災難性遺忘，以及在處理知識密集型任務時處理長脈絡的限制。本文介紹了 KG-RAG（知識圖譜檢索增強生成）管道，這是一個新穎的框架，旨在透過將結構化的知識圖譜（KG）與 LLM 的功能整合在一起，來增強 LMA 的知識能力，從而顯著減少對 LLM 潛在知識的依賴。KG-RAG 管道從非結構化文字中建構一個 KG，然後對新建立的圖譜執行資訊檢索，以執行 KGQA（知識圖譜問答）。檢索方法利用了一種稱為探索鏈（CoE）的新演算法，該演算法受益於 LLM 推理，以循序探索 KG 中的節點和關係。在 ComplexWebQuestions 資料集上的初步實驗證明了在減少幻覺內容方面有顯著的改進，並暗示了一條有希望的道路，朝著開發擅長處理知識密集型任務的智慧型系統邁進。

##### **"Set It Up!": Functional Object Arrangement with Compositional Generative Models**
2405.11928v1 by Yiqing Xu, Jiayuan Mao, Yilun Du, Tomas Lozáno-Pérez, Leslie Pack Kaebling, David Hsu

This paper studies the challenge of developing robots capable of
understanding under-specified instructions for creating functional object
arrangements, such as "set up a dining table for two"; previous arrangement
approaches have focused on much more explicit instructions, such as "put object
A on the table." We introduce a framework, SetItUp, for learning to interpret
under-specified instructions. SetItUp takes a small number of training examples
and a human-crafted program sketch to uncover arrangement rules for specific
scene types. By leveraging an intermediate graph-like representation of
abstract spatial relationships among objects, SetItUp decomposes the
arrangement problem into two subproblems: i) learning the arrangement patterns
from limited data and ii) grounding these abstract relationships into object
poses. SetItUp leverages large language models (LLMs) to propose the abstract
spatial relationships among objects in novel scenes as the constraints to be
satisfied; then, it composes a library of diffusion models associated with
these abstract relationships to find object poses that satisfy the constraints.
We validate our framework on a dataset comprising study desks, dining tables,
and coffee tables, with the results showing superior performance in generating
physically plausible, functional, and aesthetically pleasing object
arrangements compared to existing models.

摘要：這篇論文探討了開發機器人的挑戰，這些機器人能夠理解未明確規定的指示，以建立功能性的物件排列，例如「為兩個人擺好餐桌」；先前的排列方法著重於更明確的指示，例如「將物件 A 放在桌上」。我們引進一個架構 SetItUp，用於學習詮釋未明確規定的指示。SetItUp 採用少數訓練範例和人類製作的程式草圖，以找出特定場景類型的排列規則。透過利用物件之間抽象空間關係的中間圖形化表示，SetItUp 將排列問題分解成兩個子問題：i) 從有限資料中學習排列模式，以及 ii) 將這些抽象關係基礎化為物件姿勢。SetItUp 利用大型語言模型 (LLM) 來提出新場景中物件之間的抽象空間關係，作為要滿足的約束；然後，它編寫一個與這些抽象關係相關的擴散模型庫，以找出滿足約束的物件姿勢。我們在包含書桌、餐桌和咖啡桌的資料集上驗證我們的架構，結果顯示在產生物理上合理、功能性和美觀的物件排列方面，與現有模型相比具有優異的效能。

##### **Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!**
2405.11706v1 by Dean Allemang, Juan Sequeda

There is increasing evidence that question-answering (QA) systems with Large
Language Models (LLMs), which employ a knowledge graph/semantic representation
of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy
compared to systems that answer questions directly on SQL databases (i.e.
Text-to-SQL). Our previous benchmark research showed that by using a knowledge
graph, the accuracy improved from 16% to 54%. The question remains: how can we
further improve the accuracy and reduce the error rate? Building on the
observations of our previous research where the inaccurate LLM-generated SPARQL
queries followed incorrect paths, we present an approach that consists of 1)
Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of
the knowledge graph to check if the LLM-generated SPARQL query matches the
semantic of ontology and 2) LLM Repair: use the error explanations with an LLM
to repair the SPARQL query. Using the chat with the data benchmark, our primary
finding is that our approach increases the overall accuracy to 72% including an
additional 8% of "I don't know" unknown results. Thus, the overall error rate
is 20%. These results provide further evidence that investing knowledge graphs,
namely the ontology, provides higher accuracy for LLM powered question
answering systems.

摘要：有越來越多的證據顯示，使用大型語言模型 (LLM) 的問答 (QA) 系統，它採用企業 SQL 資料庫的知識圖譜/語義表示（即 Text-to-SPARQL），與直接在 SQL 資料庫上回答問題的系統（即 Text-to-SQL）相比，能達到更高的準確度。我們先前的基準研究顯示，透過使用知識圖譜，準確度從 16% 提升至 54%。問題仍然存在：我們如何進一步提升準確度並降低錯誤率？根據我們先前研究的觀察，其中不準確的 LLM 生成的 SPARQL 查詢遵循不正確的路徑，我們提出了一種方法，其中包含 1) 基於本体的查詢檢查 (OBQC)：利用知識圖譜的本体來檢查 LLM 生成的 SPARQL 查詢是否符合本体的語義，從而偵測錯誤，以及 2) LLM 修復：使用帶有 LLM 的錯誤說明來修復 SPARQL 查詢。使用與資料基準的聊天，我們的初步發現是，我們的做法將整體準確度提升至 72%，包括額外的 8% 的「我不知道」未知結果。因此，整體錯誤率為 20%。這些結果進一步證明投資知識圖譜，即本体，能為 LLM 驅動的問答系統提供更高的準確度。

##### **Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**
2405.10745v1 by Albert Sawczyn, Jakub Binkowski, Piotr Bielak, Tomasz Kajdanowicz

Knowledge-intensive tasks pose a significant challenge for Machine Learning
(ML) techniques. Commonly adopted methods, such as Large Language Models
(LLMs), often exhibit limitations when applied to such tasks. Nevertheless,
there have been notable endeavours to mitigate these challenges, with a
significant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While
KGs provide many advantages for representing knowledge, their development costs
can deter extensive research and applications. Addressing this limitation, we
introduce a framework for enriching embeddings of small-scale domain-specific
Knowledge Graphs with well-established general-purpose KGs. Adopting our
method, a modest domain-specific KG can benefit from a performance boost in
downstream tasks when linked to a substantial general-purpose KG. Experimental
evaluations demonstrate a notable enhancement, with up to a 44% increase
observed in the Hits@10 metric. This relatively unexplored research direction
can catalyze more frequent incorporation of KGs in knowledge-intensive tasks,
resulting in more robust, reliable ML implementations, which hallucinates less
than prevalent LLM solutions.
  Keywords: knowledge graph, knowledge graph completion, entity alignment,
representation learning, machine learning

摘要：知識密集型任務對機器學習 (ML) 技術構成重大挑戰。常見採用的方法，例如大型語言模型 (LLM)，在應用於此類任務時，通常會表現出限制。儘管如此，已經有顯著的努力來減輕這些挑戰，重點在於透過知識圖譜 (KG) 來擴充 LLM。雖然 KG 在表示知識方面提供了許多優點，但其開發成本可能會阻礙廣泛的研究和應用。為了解決這個限制，我們引入了一個架構，用完善的通用 KG 來豐富小規模特定領域知識圖譜的嵌入。採用我們的這個方法，一個適度的特定領域 KG 可以從與一個大量的通用 KG 連結時，在下游任務中受益於效能提升。實驗評估證明了顯著的改進，在 Hits@10 指標中觀察到高達 44% 的提升。這個相對未開發的研究方向可以催化在知識密集型任務中更頻繁地納入 KG，從而產生更強大、更可靠的 ML 實作，其產生的幻覺少於普遍的 LLM 解决方案。
關鍵字：知識圖譜、知識圖譜完成功能、實體對齊、表示學習、機器學習

##### **Automatic News Generation and Fact-Checking System Based on Language Processing**
2405.10492v2 by Xirui Peng, Qiming Xu, Zheng Feng, Haopeng Zhao, Lianghao Tan, Yan Zhou, Zecheng Zhang, Chenwei Gong, Yingqiao Zheng

This paper explores an automatic news generation and fact-checking system
based on language processing, aimed at enhancing the efficiency and quality of
news production while ensuring the authenticity and reliability of the news
content. With the rapid development of Natural Language Processing (NLP) and
deep learning technologies, automatic news generation systems are capable of
extracting key information from massive data and generating well-structured,
fluent news articles. Meanwhile, by integrating fact-checking technology, the
system can effectively prevent the spread of false news and improve the
accuracy and credibility of news. This study details the key technologies
involved in automatic news generation and factchecking, including text
generation, information extraction, and the application of knowledge graphs,
and validates the effectiveness of these technologies through experiments.
Additionally, the paper discusses the future development directions of
automatic news generation and fact-checking systems, emphasizing the importance
of further integration and innovation of technologies. The results show that
with continuous technological optimization and practical application, these
systems will play an increasingly important role in the future news industry,
providing more efficient and reliable news services.

摘要：本文探討了一種基於語言處理的自動新聞生成與事實查核系統，旨在提升新聞產製的效率與品質，同時確保新聞內容的真實性與可信度。隨著自然語言處理（NLP）與深度學習技術的快速發展，自動新聞生成系統能夠從海量資料中萃取關鍵資訊，並生成結構良好、流暢的新聞文章。同時，透過整合事實查核技術，系統能有效防止假新聞的散播，提升新聞的準確度與可信度。本研究詳細說明自動新聞生成與事實查核所涉及的關鍵技術，包括文字生成、資訊萃取、知識圖譜的應用，並透過實驗驗證這些技術的有效性。此外，本文探討自動新聞生成與事實查核系統未來的發展方向，強調技術進一步整合與創新的重要性。結果顯示，隨著技術持續優化與實務應用，這些系統將在未來的新聞產業中扮演日益重要的角色，提供更有效率、更可信賴的新聞服務。

##### **4D Panoptic Scene Graph Generation**
2405.10305v1 by Jingkang Yang, Jun Cen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, Ziwei Liu

We are living in a three-dimensional space while moving forward through a
fourth dimension: time. To allow artificial intelligence to develop a
comprehensive understanding of such a 4D environment, we introduce 4D Panoptic
Scene Graph (PSG-4D), a new representation that bridges the raw visual data
perceived in a dynamic 4D world and high-level visual understanding.
Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent
entities with precise location and status information, and edges, which capture
the temporal relations. To facilitate research in this new area, we build a
richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of
1M frames, each of which is labeled with 4D panoptic segmentation masks as well
as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,
a Transformer-based model that can predict panoptic segmentation masks, track
masks along the time axis, and generate the corresponding scene graphs via a
relation component. Extensive experiments on the new dataset show that our
method can serve as a strong baseline for future research on PSG-4D. In the
end, we provide a real-world application example to demonstrate how we can
achieve dynamic scene understanding by integrating a large language model into
our PSG-4D system.

摘要：我們生活在三維空間中，同時在第四維度：時間中前進。為了讓人工智慧發展對這種 4D 環境的全面理解，我們引入了 4D 全景場景圖 (PSG-4D)，這是一種新的表示方式，它彌合了在動態 4D 世界中感知到的原始視覺數據和高層級視覺理解。具體來說，PSG-4D 將豐富的 4D 感測數據抽象為節點，這些節點表示具有精確位置和狀態信息的實體，以及邊緣，這些邊緣捕獲時間關係。為了促進這一新領域的研究，我們構建了一個豐富註釋的 PSG-4D 數據集，其中包含 3K RGB-D 視頻，總共 1M 幀，每個幀都標記有 4D 全景分割蒙版以及細粒度、動態場景圖。為了解決 PSG-4D，我們提出了 PSG4DFormer，這是一個基於 Transformer 的模型，它可以預測全景分割蒙版、沿時間軸追蹤蒙版，並通過關係組成產生對應的場景圖。在新的數據集上進行的廣泛實驗表明，我們的模型可以用作未來 PSG-4D 研究的強大基線。最後，我們提供了一個真實世界的應用範例，以展示如何通過將大型語言模型整合到我們的 PSG-4D 系統中來實現動態場景理解。

##### **Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**
2405.10288v1 by Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu

Facts extraction is pivotal for constructing knowledge graphs. Recently, the
increasing demand for temporal facts in downstream tasks has led to the
emergence of the task of temporal fact extraction. In this paper, we
specifically address the extraction of temporal facts from natural language
text. Previous studies fail to handle the challenge of establishing
time-to-fact correspondences in complex sentences. To overcome this hurdle, we
propose a timeline-based sentence decomposition strategy using large language
models (LLMs) with in-context learning, ensuring a fine-grained understanding
of the timeline associated with various facts. In addition, we evaluate the
performance of LLMs for direct temporal fact extraction and get unsatisfactory
results. To this end, we introduce TSDRE, a method that incorporates the
decomposition capabilities of LLMs into the traditional fine-tuning of smaller
pre-trained language models (PLMs). To support the evaluation, we construct
ComplexTRED, a complex temporal fact extraction dataset. Our experiments show
that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and
ComplexTRED datasets.

摘要：事實抽取對於建構知識圖譜至關重要。最近，下游任務對時間事實的需求增加，導致了時間事實抽取任務的出現。在本文中，我們特別探討從自然語言文本中抽取時間事實。先前的研究無法處理在複雜句子中建立時間到事實對應的挑戰。為了克服這個障礙，我們提出了一個基於時間軸的句子分解策略，使用具備上下文學習功能的大語言模型 (LLM)，確保對與各種事實相關的時間軸進行細粒度的理解。此外，我們評估了 LLM 直接時間事實抽取的效能，並獲得了不令人滿意的結果。為此，我們引入了 TSDRE，一種將 LLM 的分解能力整合到較小預訓練語言模型 (PLM) 的傳統微調中的方法。為了支援評估，我們構建了 ComplexTRED，一個複雜的時間事實抽取資料集。我們的實驗表明，TSDRE 在 HyperRED-Temporal 和 ComplexTRED 資料集上都達到了最先進的結果。

##### **SCI 3.0: A Web-based Schema Curation Interface for Graphical Event Representations**
2405.09733v2 by Reece Suchocki, Mary Martin, Martha Palmer, Susan Brown

To understand the complexity of global events, one must navigate a web of
interwoven sub-events, identifying those most impactful elements within the
larger, abstract macro-event framework at play. This concept can be extended to
the field of natural language processing (NLP) through the creation of
structured event schemas which can serve as representations of these abstract
events. Central to our approach is the Schema Curation Interface 3.0 (SCI 3.0),
a web application that facilitates real-time editing of event schema properties
within a generated graph e.g., adding, removing, or editing sub-events,
entities, and relations directly through an interface.

摘要：<paragraph>要了解全球事件的复杂性，必须在相互交织的子事件网络中穿梭，在抽象的宏观事件框架中识别那些影响最大的元素。这个概念可以通过创建结构化的事件模式扩展到自然语言处理 (NLP) 领域，这些模式可以作为这些抽象事件的表示。我们方法的核心是模式管理界面 3.0 (SCI 3.0)，这是一个 Web 应用程序，它通过界面直接添加、删除或编辑子事件、实体和关系，从而方便实时编辑生成的图形中的事件模式属性。</paragraph>

##### **SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge**
2405.09713v2 by Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, Chuang Gan

Learning commonsense reasoning from visual contexts and scenes in real-world
is a crucial step toward advanced artificial intelligence. However, existing
video reasoning benchmarks are still inadequate since they were mainly designed
for factual or situated reasoning and rarely involve broader knowledge in the
real world. Our work aims to delve deeper into reasoning evaluations,
specifically within dynamic, open-world, and structured context knowledge. We
propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K
situations with instance-level annotations depicted in the videos. The
reasoning process is required to understand and apply situated knowledge and
general knowledge for problem-solving. To create such a dataset, we propose an
automatic and scalable generation method to generate question-answer pairs,
knowledge graphs, and rationales by instructing the combinations of LLMs and
MLLMs. Concretely, we first extract observable situated entities, relations,
and processes from videos for situated knowledge and then extend to open-world
knowledge beyond the visible content. The task generation is facilitated
through multiple dialogues as iterations and subsequently corrected and refined
by our designed self-promptings and demonstrations. With a corpus of both
explicit situated facts and implicit commonsense, we generate associated
question-answer pairs and reasoning processes, finally followed by manual
reviews for quality assurance. We evaluated recent mainstream large
vision-language models on the benchmark and found several insightful
conclusions. For more information, please refer to our benchmark at
www.bobbywu.com/SOKBench.

摘要：從真實世界的視覺脈絡和場景中學習常識推理是邁向先進人工智慧的關鍵一步。然而，現有的影片推理基準仍然不足，因為它們主要設計用於事實或情境推理，很少涉及現實世界中的更廣泛知識。我們的研究旨在深入探討推理評估，特別是在動態、開放世界和結構化背景知識中。我們提出一個新的基準 (SOK-Bench)，包含 44K 個問題和 10K 個情況，其中包含影片中描繪的實例級註解。推理過程需要理解並應用情境知識和一般知識來解決問題。為了建立這樣的資料集，我們提出了一種自動且可擴充的生成方法，透過指導 LLM 和 MLLM 的組合來生成問答對、知識圖譜和依據。具體來說，我們首先從影片中提取可觀察的情境實體、關係和過程以獲得情境知識，然後擴展到可見內容之外的開放世界知識。任務生成透過多重對話作為迭代來促進，隨後由我們設計的自提示和示範進行更正和優化。透過包含明確情境事實和隱含常識的語料庫，我們生成了相關的問答對和推理過程，最後進行人工審查以確保品質。我們在基準上評估了近期主流的大型視覺語言模型，並發現了幾個有見地的結論。有關更多資訊，請參閱我們的基準，網址為 www.bobbywu.com/SOKBench。

##### **STAR: A Benchmark for Situated Reasoning in Real-World Videos**
2405.09711v1 by Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, Chuang Gan

Reasoning in the real world is not divorced from situations. How to capture
the present knowledge from surrounding situations and perform reasoning
accordingly is crucial and challenging for machine intelligence. This paper
introduces a new benchmark that evaluates the situated reasoning ability via
situation abstraction and logic-grounded question answering for real-world
videos, called Situated Reasoning in Real-World Videos (STAR Benchmark). This
benchmark is built upon the real-world videos associated with human actions or
interactions, which are naturally dynamic, compositional, and logical. The
dataset includes four types of questions, including interaction, sequence,
prediction, and feasibility. We represent the situations in real-world videos
by hyper-graphs connecting extracted atomic entities and relations (e.g.,
actions, persons, objects, and relationships). Besides visual perception,
situated reasoning also requires structured situation comprehension and logical
reasoning. Questions and answers are procedurally generated. The answering
logic of each question is represented by a functional program based on a
situation hyper-graph. We compare various existing video reasoning models and
find that they all struggle on this challenging situated reasoning task. We
further propose a diagnostic neuro-symbolic model that can disentangle visual
perception, situation abstraction, language understanding, and functional
reasoning to understand the challenges of this benchmark.

摘要：現實世界中的推理並非脫離情境。如何從周遭情境中擷取當前知識並據此進行推理，對機器智能而言至關重要且具有挑戰性。本文介紹了一個新的基準，透過情境抽象和以邏輯為基礎的問答，評估真實世界影片中的情境推理能力，稱為真實世界影片中的情境推理（STAR基準）。此基準建立於與人類動作或互動相關的真實世界影片之上，這些影片本質上是動態、組合且合乎邏輯的。該資料集包含四種類型的問題，包括互動、順序、預測和可行性。我們透過連接提取的原子實體和關係（例如動作、人物、物件和關係）的超圖形，來表示真實世界影片中的情境。除了視覺感知之外，情境推理還需要結構化的情境理解和邏輯推理。問題和答案是程序化產生的。每個問題的回答邏輯由基於情境超圖形的函式程式表示。我們比較了現有的各種影片推理模型，發現它們在這個具有挑戰性的情境推理任務中都表現不佳。我們進一步提出了一個診斷性神經符號模型，它可以解開視覺感知、情境抽象、語言理解和函式推理，以了解此基準的挑戰。

##### **Falcon 7b for Software Mention Detection in Scholarly Documents**
2405.08514v1 by AmeerAli Khan, Qusai Ramadan, Cong Yang, Zeyd Boukhers

This paper aims to tackle the challenge posed by the increasing integration
of software tools in research across various disciplines by investigating the
application of Falcon-7b for the detection and classification of software
mentions within scholarly texts. Specifically, the study focuses on solving
Subtask I of the Software Mention Detection in Scholarly Publications (SOMD),
which entails identifying and categorizing software mentions from academic
literature. Through comprehensive experimentation, the paper explores different
training strategies, including a dual-classifier approach, adaptive sampling,
and weighted loss scaling, to enhance detection accuracy while overcoming the
complexities of class imbalance and the nuanced syntax of scholarly writing.
The findings highlight the benefits of selective labelling and adaptive
sampling in improving the model's performance. However, they also indicate that
integrating multiple strategies does not necessarily result in cumulative
improvements. This research offers insights into the effective application of
large language models for specific tasks such as SOMD, underlining the
importance of tailored approaches to address the unique challenges presented by
academic text analysis.

摘要：這篇論文旨在解決軟體工具在各個領域研究中日益整合所帶來的挑戰，研究 Falcon-7b 在學術文本中偵測和分類軟體提及的應用。具體來說，這項研究專注於解決學術出版物中軟體提及偵測 (SOMD) 的子任務一，其中包括識別和分類學術文獻中的軟體提及。透過全面的實驗，本文探討了不同的訓練策略，包括雙分類器方法、自適應抽樣和加權損失縮放，以在克服類別不平衡的複雜性和學術寫作的細微語法的情況下，提升偵測準確度。研究結果突顯了選擇性標記和自適應抽樣在提升模型效能方面的優點。然而，研究結果也指出，整合多種策略並不一定會帶來累積的進步。這項研究提供了對大型語言模型在特定任務（例如 SOMD）中的有效應用之見解，強調了量身打造方法對於解決學術文本分析所呈現之獨特挑戰的重要性。

##### **Could Chemical LLMs benefit from Message Passing**
2405.08334v1 by Jiaqing Xie, Ziheng Chi

Pretrained language models (LMs) showcase significant capabilities in
processing molecular text, while concurrently, message passing neural networks
(MPNNs) demonstrate resilience and versatility in the domain of molecular
science. Despite these advancements, we find there are limited studies
investigating the bidirectional interactions between molecular structures and
their corresponding textual representations. Therefore, in this paper, we
propose two strategies to evaluate whether an information integration can
enhance the performance: contrast learning, which involves utilizing an MPNN to
supervise the training of the LM, and fusion, which exploits information from
both models. Our empirical analysis reveals that the integration approaches
exhibit superior performance compared to baselines when applied to smaller
molecular graphs, while these integration approaches do not yield performance
enhancements on large scale graphs.

摘要：預先訓練的語言模型 (LM) 在處理分子文本方面展示出顯著的能力，同時，訊息傳遞神經網路 (MPNN) 在分子科學領域中展現出韌性和多功能性。儘管有這些進展，我們發現探討分子結構與其對應文本表示之間雙向交互作用的研究有限。因此，在本文中，我們提出兩種策略來評估資訊整合是否能提升效能：對比學習，涉及利用 MPNN 監督 LM 的訓練，以及融合，利用來自兩個模型的資訊。我們的實證分析顯示，與應用於較小分子圖形的基準線相比，整合方法展現出優異的效能，而這些整合方法並未在大型圖形上產生效能提升。

##### **AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models**
2405.07626v1 by Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi

Detecting anomaly edges for dynamic graphs aims to identify edges
significantly deviating from the normal pattern and can be applied in various
domains, such as cybersecurity, financial transactions and AIOps. With the
evolving of time, the types of anomaly edges are emerging and the labeled
anomaly samples are few for each type. Current methods are either designed to
detect randomly inserted edges or require sufficient labeled data for model
training, which harms their applicability for real-world applications. In this
paper, we study this problem by cooperating with the rich knowledge encoded in
large language models(LLMs) and propose a method, namely AnomalyLLM. To align
the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to
generate the representations of edges and reprograms the edges using the
prototypes of word embeddings. Along with the encoder, we design an in-context
learning framework that integrates the information of a few labeled samples to
achieve few-shot anomaly detection. Experiments on four datasets reveal that
AnomalyLLM can not only significantly improve the performance of few-shot
anomaly detection, but also achieve superior results on new anomalies without
any update of model parameters.

摘要：偵測動態圖形的異常邊緣旨在識別顯著偏離正常模式的邊緣，並可用於各種領域，例如網路安全、金融交易和 AIOps。隨著時間的推移，異常邊緣的類型不斷出現，而每種類型的標籤異常樣本很少。目前的技術方法旨在偵測隨機插入的邊緣，或需要足夠的標籤資料進行模型訓練，這會損害其在實際應用中的適用性。在本文中，我們透過與大型語言模型 (LLM) 中編碼的豐富知識合作來研究這個問題，並提出了一種名為 AnomalyLLM 的方法。為了將動態圖形與 LLM 對齊，AnomalyLLM 預先訓練了一個動態感知編碼器，以產生邊緣的表示，並使用詞嵌入的原型重新編寫邊緣。我們與編碼器一起設計了一個情境內學習框架，整合了少數標籤樣本的資訊，以實現少發異常偵測。在四個資料集上的實驗表明，AnomalyLLM 不僅可以顯著改善少發異常偵測的效能，還能對沒有任何模型參數更新的新異常情況取得優異的結果。

##### **DynLLM: When Large Language Models Meet Dynamic Graph Recommendation**
2405.07580v1 by Ziwei Zhao, Fake Lin, Xi Zhu, Zhi Zheng, Tong Xu, Shitian Shen, Xueying Li, Zikai Yin, Enhong Chen

Last year has witnessed the considerable interest of Large Language Models
(LLMs) for their potential applications in recommender systems, which may
mitigate the persistent issue of data sparsity. Though large efforts have been
made for user-item graph augmentation with better graph-based recommendation
performance, they may fail to deal with the dynamic graph recommendation task,
which involves both structural and temporal graph dynamics with inherent
complexity in processing time-evolving data. To bridge this gap, in this paper,
we propose a novel framework, called DynLLM, to deal with the dynamic graph
recommendation task with LLMs. Specifically, DynLLM harnesses the power of LLMs
to generate multi-faceted user profiles based on the rich textual features of
historical purchase records, including crowd segments, personal interests,
preferred categories, and favored brands, which in turn supplement and enrich
the underlying relationships between users and items. Along this line, to fuse
the multi-faceted profiles with temporal graph embedding, we engage LLMs to
derive corresponding profile embeddings, and further employ a distilled
attention mechanism to refine the LLM-generated profile embeddings for
alleviating noisy signals, while also assessing and adjusting the relevance of
each distilled facet embedding for seamless integration with temporal graph
embedding from continuous time dynamic graphs (CTDGs). Extensive experiments on
two real e-commerce datasets have validated the superior improvements of DynLLM
over a wide range of state-of-the-art baseline methods.

摘要：去年，大型語言模型 (LLM) 因其在推薦系統中的潛在應用而備受關注，這可能會緩解數據稀疏性的持續問題。儘管已為用戶項目圖增強做出巨大努力，以提高基於圖形的推薦性能，但它們可能無法處理動態圖形推薦任務，其中涉及結構和時間圖形動態，在處理時間演化數據時具有固有的複雜性。為了彌合這一差距，在本文中，我們提出了一個名為 DynLLM 的新框架，以使用 LLM 處理動態圖形推薦任務。具體來說，DynLLM 利用 LLM 的能力，根據歷史購買記錄的豐富文本特徵生成多方面的用戶資料，包括人群區塊、個人興趣、首選類別和喜愛的品牌，進而補充和豐富用戶和項目之間的底層關係。順著這條思路，為了將多方面的資料與時間圖形嵌入融合，我們使用 LLM 得出相應的資料嵌入，並進一步採用精煉的注意力機制來精煉 LLM 生成的資料嵌入，以減輕雜訊信號，同時評估和調整每個精煉的方面嵌入與來自連續時間動態圖形 (CTDG) 的時間圖形嵌入的無縫整合。在兩個真實電子商務數據集上進行的廣泛實驗驗證了 DynLLM 對廣泛的最新基線方法的優越改進。

##### **LLM-Generated Black-box Explanations Can Be Adversarially Helpful**
2405.06800v2 by Rohan Ajwani, Shashidhar Reddy Javaji, Frank Rudzicz, Zining Zhu

Large Language Models (LLMs) are becoming vital tools that help us solve and
understand complex problems by acting as digital assistants. LLMs can generate
convincing explanations, even when only given the inputs and outputs of these
problems, i.e., in a ``black-box'' approach. However, our research uncovers a
hidden risk tied to this approach, which we call *adversarial helpfulness*.
This happens when an LLM's explanations make a wrong answer look right,
potentially leading people to trust incorrect solutions. In this paper, we show
that this issue affects not just humans, but also LLM evaluators. Digging
deeper, we identify and examine key persuasive strategies employed by LLMs. Our
findings reveal that these models employ strategies such as reframing the
questions, expressing an elevated level of confidence, and cherry-picking
evidence to paint misleading answers in a credible light. To examine if LLMs
are able to navigate complex-structured knowledge when generating adversarially
helpful explanations, we create a special task based on navigating through
graphs. Most LLMs are not able to find alternative paths along simple graphs,
indicating that their misleading explanations aren't produced by only logical
deductions using complex knowledge. These findings shed light on the
limitations of the black-box explanation setting and allow us to provide advice
on the safe usage of LLMs.

摘要：大型語言模型 (LLM) 正成為幫助我們解決和理解複雜問題的重要工具，因為它們可以作為數位助理。LLM 可以產生令人信服的解釋，即使僅提供這些問題的輸入和輸出，即在「黑盒子」方法中。然而，我們的研究揭露了與這種方法相關的隱藏風險，我們稱之為「對抗性幫助」。這會發生在 LLM 的解釋讓錯誤答案看起來正確時，可能會導致人們相信不正確的解決方案。在本文中，我們展示了這個問題不僅影響人類，也影響 LLM 評估者。深入探討後，我們找出並檢視 LLM 使用的主要說服策略。我們的研究結果顯示，這些模型採用了重新建構問題、表達高度信心，以及挑選證據來以令人信服的方式描繪誤導答案等策略。為了探討 LLM 在產生對抗性有用的解釋時是否能夠導航複雜結構的知識，我們根據導航圖形建立了一個特殊任務。大多數 LLM 無法在簡單的圖形中找到替代路徑，這表示它們的誤導性解釋並非僅僅透過使用複雜知識的邏輯推論產生。這些研究結果闡明了黑盒子解釋設定的限制，並讓我們能夠提供有關安全使用 LLM 的建議。

##### **A Survey of Large Language Models for Graphs**
2405.08011v1 by Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Chao Huang

Graphs are an essential data structure utilized to represent relationships in
real-world scenarios. Prior research has established that Graph Neural Networks
(GNNs) deliver impressive outcomes in graph-centric tasks, such as link
prediction and node classification. Despite these advancements, challenges like
data sparsity and limited generalization capabilities continue to persist.
Recently, Large Language Models (LLMs) have gained attention in natural
language processing. They excel in language comprehension and summarization.
Integrating LLMs with graph learning techniques has attracted interest as a way
to enhance performance in graph learning tasks. In this survey, we conduct an
in-depth review of the latest state-of-the-art LLMs applied in graph learning
and introduce a novel taxonomy to categorize existing methods based on their
framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as
Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key
methodologies within each category. We explore the strengths and limitations of
each framework, and emphasize potential avenues for future research, including
overcoming current integration challenges between LLMs and graph learning
techniques, and venturing into new application areas. This survey aims to serve
as a valuable resource for researchers and practitioners eager to leverage
large language models in graph learning, and to inspire continued progress in
this dynamic field. We consistently maintain the related open-source materials
at \url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.

摘要：圖形是一種重要的資料結構，用於表示真實世界場景中的關係。先前的研究已確立圖神經網路 (GNN) 在以圖形為中心的任務中提供令人印象深刻的成果，例如連結預測和節點分類。儘管有這些進展，但資料稀疏性和有限的概化能力等挑戰仍然持續存在。最近，大型語言模型 (LLM) 在自然語言處理中備受關注。它們在語言理解和摘要方面表現出色。將 LLM 與圖形學習技術整合已引起興趣，作為增強圖形學習任務效能的一種方式。在這項調查中，我們對應用於圖形學習的最新最先進的 LLM 進行深入探討，並引入一種新穎的分類法，根據其架構設計對現有方法進行分類。我們詳細說明四種獨特設計：i) GNN 作為前綴，ii) LLM 作為前綴，iii) LLM-圖形整合，以及 iv) 僅 LLM，強調每個類別中的關鍵方法。我們探討每個架構的優點和限制，並強調未來研究的潛在途徑，包括克服 LLM 和圖形學習技術之間目前的整合挑戰，並進軍新的應用領域。這項調查旨在作為研究人員和實務工作者的寶貴資源，他們渴望在圖形學習中利用大型語言模型，並激勵這個動態領域持續進步。我們持續維護相關的開源材料，網址為 \url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}。

##### **Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark**
2405.06634v1 by Evan M. Williams, Kathleen M. Carley

We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual
Network Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision
Language Models (VLMs) on 5 tasks related to three foundational network science
concepts: identifying nodes of maximal degree on a rendered graph, identifying
whether signed triads are balanced or unbalanced, and counting components. The
tasks are structured to be easy for a human who understands the underlying
graph theoretic concepts, and can all be solved by counting the appropriate
elements in graphs. We find that while GPT-4 consistently outperforms LLaVa,
both models struggle with every visual network analysis task we propose. We
publicly release the first benchmark for the evaluation of VLMs on foundational
VNA tasks.

摘要：我們評估 GPT-4 和 LLaVa 的零次學習能力，以執行小規模圖形上的簡單視覺網路分析 (VNA) 任務。我們評估視覺語言模型 (VLM) 在 5 個與三個基礎網路科學概念相關的任務上：識別渲染圖形上最大程度的節點、識別帶正負號的三元組是平衡還是不平衡，以及計算組成部分。這些任務的結構對於理解基礎圖形理論概念的人類來說很容易，並且都可以透過計算圖形中的適當元素來解決。我們發現，雖然 GPT-4 持續優於 LLaVa，但這兩個模型都難以處理我們提出的每個視覺網路分析任務。我們公開發布了第一個用於評估 VLM 在基礎 VNA 任務上的基準。

##### **Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval**
2405.06545v1 by Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, Fan Mo

Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, although their susceptibility to hallucination poses
significant challenges for their deployment in critical areas such as
healthcare. To address this issue, retrieving relevant facts from knowledge
graphs (KGs) is considered a promising method. Existing KG-augmented approaches
tend to be resource-intensive, requiring multiple rounds of retrieval and
verification for each factoid, which impedes their application in real-world
scenarios.
  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval
(Re-KGR) to augment the factuality of LLMs' responses with less retrieval
efforts in the medical field. Our approach leverages the attribution of
next-token predictive probability distributions across different tokens, and
various model layers to primarily identify tokens with a high potential for
hallucination, reducing verification rounds by refining knowledge triples
associated with these tokens. Moreover, we rectify inaccurate content using
retrieved knowledge in the post-processing stage, which improves the
truthfulness of generated responses. Experimental results on a medical dataset
demonstrate that our approach can enhance the factual capability of LLMs across
various foundational models as evidenced by the highest scores on truthfulness.

摘要：大型語言模型 (LLM) 在各個領域展現出卓越的能力，儘管它們容易出現幻覺，對其在醫療保健等關鍵領域的部署構成重大挑戰。為了解決這個問題，從知識圖譜 (KG) 中擷取相關事實被認為是一種有前途的方法。現有的 KG 增強方法往往需要大量資源，需要對每個事實進行多輪檢索和驗證，這阻礙了它們在實際場景中的應用。
在這項研究中，我們提出了自修正增強知識圖譜檢索 (Re-KGR)，以減少醫療領域中 LLM 回應的事實性檢索工作。我們的做法利用了不同標記之間下一個標記預測概率分佈的歸因，以及各種模型層，以主要識別具有高幻覺潛力的標記，通過修正與這些標記相關的知識三元組來減少驗證輪次。此外，我們在後處理階段使用檢索到的知識來修正不準確的內容，這提高了生成回應的真實性。在醫療數據集上的實驗結果表明，我們的做法可以增強 LLM 在各種基礎模型上的事實能力，最高真實性分數證明了這一點。

##### **Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts**
2405.06524v1 by Wenyu Huang, Guancheng Zhou, Mirella Lapata, Pavlos Vougiouklis, Sebastien Montella, Jeff Z. Pan

Although Large Language Models (LLMs) are effective in performing various NLP
tasks, they still struggle to handle tasks that require extensive, real-world
knowledge, especially when dealing with long-tail facts (facts related to
long-tail entities). This limitation highlights the need to supplement LLMs
with non-parametric knowledge. To address this issue, we analysed the effects
of different types of non-parametric knowledge, including textual passage and
knowledge graphs (KGs). Since LLMs have probably seen the majority of factual
question-answering datasets already, to facilitate our analysis, we proposed a
fully automatic pipeline for creating a benchmark that requires knowledge of
long-tail facts for answering the involved questions. Using this pipeline, we
introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different
knowledge settings using the proposed benchmark. Our experiments show that LLMs
alone struggle with answering these questions, especially when the long-tail
level is high or rich knowledge is required. Nonetheless, the performance of
the same models improved significantly when they were prompted with
non-parametric knowledge. We observed that, in most cases, prompting LLMs with
KG triples surpasses passage-based prompting using a state-of-the-art
retriever. In addition, while prompting LLMs with both KG triples and documents
does not consistently improve knowledge coverage, it can dramatically reduce
hallucinations in the generated content.

摘要：儘管大型語言模型 (LLM) 在執行各種自然語言處理 (NLP) 任務時很有效，它們在處理需要廣泛的真實世界知識的任務時仍有困難，特別是在處理長尾事實（與長尾實體相關的事實）時。此限制突顯了補充 LLM 以獲得非參數知識的必要性。為了解決此問題，我們分析了不同類型非參數知識的影響，包括文字段落和知識圖 (KG)。由於 LLM 可能已經看過大多數事實性的問答資料集，為了方便我們的分析，我們提出了一個全自動管道，用於建立一個基準，需要了解長尾事實才能回答相關問題。使用此管道，我們引入了 LTGen 基準。我們使用所提出的基準，在不同的知識設定中評估最先進的 LLM。我們的實驗表明，LLM 單獨難以回答這些問題，特別是在長尾層級很高或需要豐富的知識時。儘管如此，當使用非參數知識提示這些模型時，它們的效能顯著提升。我們觀察到，在大多數情況下，使用最先進的檢索器提示 LLM 使用 KG 三元組會優於基於段落的提示。此外，儘管同時提示 LLM 使用 KG 三元組和文件並不會持續改善知識涵蓋範圍，但它可以大幅減少產生的內容中的幻覺。

##### **RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**
2405.05792v1 by Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, Ian Reid

Mapping is crucial for spatial reasoning, planning and robot navigation.
Existing approaches range from metric, which require precise geometry-based
optimization, to purely topological, where image-as-node based graphs lack
explicit object-level reasoning and interconnectivity. In this paper, we
propose a novel topological representation of an environment based on "image
segments", which are semantically meaningful and open-vocabulary queryable,
conferring several advantages over previous works based on pixel-level
features. Unlike 3D scene graphs, we create a purely topological graph with
segments as nodes, where edges are formed by a) associating segment-level
descriptors between pairs of consecutive images and b) connecting neighboring
segments within an image using their pixel centroids. This unveils a
"continuous sense of a place", defined by inter-image persistence of segments
along with their intra-image neighbours. It further enables us to represent and
update segment-level descriptors through neighborhood aggregation using graph
convolution layers, which improves robot localization based on segment-level
retrieval. Using real-world data, we show how our proposed map representation
can be used to i) generate navigation plans in the form of "hops over segments"
and ii) search for target objects using natural language queries describing
spatial relations of objects. Furthermore, we quantitatively analyze data
association at the segment level, which underpins inter-image connectivity
during mapping and segment-level localization when revisiting the same place.
Finally, we show preliminary trials on segment-level `hopping' based zero-shot
real-world navigation. Project page with supplementary details:
oravus.github.io/RoboHop/

摘要：<paragraph>繪製地圖對於空間推理、規劃和機器人導航至關重要。
現有方法從度量（需要精確的基於幾何的最佳化）到純拓撲（其中以影像為節點的圖形缺乏明確的物件層級推理和互連性）不等。在本文中，我們提出一個基於「影像區段」的新穎拓撲表示，這些區段在語意上是有意義的，並且可以進行開放式詞彙查詢，與基於像素層級特徵的先前工作相比，具有多項優點。與 3D 場景圖形不同，我們使用區段作為節點建立一個純拓撲圖形，其中邊緣是由 a) 將區段層級描述符與連續兩張影像之間的配對關聯起來，以及 b) 使用其像素質心連接影像中的鄰近區段所形成的。這揭示了一個「連續的地方感」，它是由區段在影像之間的持續性及其在影像內的鄰近元素所定義的。它進一步使我們能夠使用圖形卷積層透過鄰域聚合來表示和更新區段層級描述符，這改進了基於區段層級擷取的機器人定位。使用真實世界資料，我們展示了我們提出的地圖表示如何用於 i) 以「區段跳躍」的形式產生導航計畫，以及 ii) 使用描述物件空間關係的自然語言查詢來搜尋目標物件。此外，我們定量分析了區段層級的資料關聯性，這在繪製地圖期間支撐了影像之間的連接性，以及在再次造訪同一個地方時的區段層級定位。最後，我們展示了基於區段層級「跳躍」的零次學習真實世界導航的初步試驗。包含補充詳細資訊的專案頁面：
oravus.github.io/RoboHop/</paragraph>

##### **G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning**
2405.05616v1 by Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, Yao Cheng

Commonsense question answering has demonstrated considerable potential across
various applications like assistants and social robots. Although fully
fine-tuned pre-trained Language Models(LM) have achieved remarkable performance
in commonsense reasoning, their tendency to excessively prioritize textual
information hampers the precise transfer of structural knowledge and undermines
interpretability. Some studies have explored combining LMs with Knowledge
Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural
Network(GNN)-based reasoning that lacks a profound interaction between
heterogeneous modalities. In this paper, we propose a novel Graph-based
Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,
aiming to maintain a balance between heterogeneous knowledge and enhance the
cross-modal interaction within the LM+GNNs model. In particular, an evidence
graph is constructed by integrating multiple knowledge sources, i.e.
ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.
Afterward, a structure-aware frozen PLM is employed to fully incorporate the
structured and textual information from the evidence graph, where the
generation of prompts is driven by graph entities and relations. Finally, a
heterogeneous message-passing reasoning module is used to facilitate deep
interaction of knowledge between the LM and graph-based networks. Empirical
validation, conducted through extensive experiments on three benchmark
datasets, demonstrates the notable performance of the proposed model. The
results reveal a significant advancement over the existing models, especially,
with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.

摘要：常識問答在各種應用程式中展現了相當大的潛力，例如助理和社交機器人。儘管經過微調的預訓練語言模型 (LM) 在常識推理中取得了顯著的表現，但它們過度優先考慮文本資訊的傾向會阻礙結構知識的精確傳輸，並損害可解釋性。一些研究探索了將 LM 與知識圖譜 (KG) 結合起來，通過粗略地融合這兩種模式來執行基於圖神經網路 (GNN) 的推理，而這種推理缺乏異質模式之間的深入互動。在本文中，我們提出了一個新的基於圖的結構感知提示學習模型，用於常識推理，稱為 G-SAP，旨在維持異質知識之間的平衡，並增強 LM+GNN 模型內的跨模式互動。特別是，通過整合多個知識來源（即 ConceptNet、Wikipedia 和劍橋詞典）來構建一個證據圖，以提升效能。隨後，採用結構感知凍結 PLM 來充分整合來自證據圖的結構化和文本資訊，其中提示的產生是由圖實體和關係驅動的。最後，使用異質訊息傳遞推理模組來促進 LM 和基於圖的網路之間的深度互動。透過在三個基準資料集上進行廣泛的實驗進行的經驗驗證，證明了所提出的模型的顯著效能。結果顯示出比現有模型有顯著的進步，特別是在 OpenbookQA 資料集上比 SoTA LM+GNN 模型提升了 6.12%。

##### **MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning**
2405.05189v1 by Inderjeet Nair, Lu Wang

We study the task of conducting structured reasoning as generating a
reasoning graph from natural language input using large language models (LLMs).
Previous approaches have explored various prompting schemes, yet they suffer
from error propagation due to the autoregressive nature and single-pass-based
decoding, which lack error correction capability. Additionally, relying solely
on a single sample may result in the omission of true nodes and edges. To
counter this, we draw inspiration from self-consistency (SC), which involves
sampling a diverse set of reasoning chains and taking the majority vote as the
final answer. To tackle the substantial challenge of applying SC on generated
graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of
Reasoning in Directed acyclic graph) that leverages Minimum Description Length
(MDL)-based formulation to identify consistent properties among the different
graph samples generated by an LLM. This formulation helps reject properties
that appear in only a few samples, which are likely to be erroneous, while
enabling the inclusion of missing elements without compromising precision. Our
method demonstrates superior performance than comparisons across various
structured reasoning tasks, including argument structure extraction,
explanation graph generation, inferring dependency relations among actions for
everyday tasks, and semantic graph generation from natural texts.

摘要：我們研究將結構化推理的任務作為使用大型語言模型 (LLM) 從自然語言輸入產生推理圖。
先前的做法已探討各種提示方案，但由於自迴歸性質和基於單次傳遞的解碼，它們會遭受錯誤傳播，而這缺乏錯誤修正能力。此外，僅依賴單一範例可能會導致遺漏真實節點和邊緣。為了解決此問題，我們從自一致性 (SC) 中汲取靈感，其中涉及取樣多元推理鏈並將多數決作為最終答案。為了應對在生成圖形上應用 SC 的重大挑戰，我們提出了 MIDGARD (無向非循環圖中推理的最小描述長度引導聚合)，它利用基於最小描述長度 (MDL) 的公式來識別 LLM 生成的不同圖形範例之間的一致屬性。此公式有助於拒絕僅出現在少數範例中且可能錯誤的屬性，同時在不損及精準度的前提下納入遺失元素。我們的模型在各種結構化推理任務中表現出優於比較的效能，包括論證結構提取、解釋圖形生成、推論日常任務中動作之間的依賴關係，以及從自然文本生成語義圖形。

##### **Lightweight Spatial Modeling for Combinatorial Information Extraction From Documents**
2405.06701v1 by Yanfei Dong, Lambert Deng, Jiazheng Zhang, Xiaodong Yu, Ting Lin, Francesco Gelli, Soujanya Poria, Wee Sun Lee

Documents that consist of diverse templates and exhibit complex spatial
structures pose a challenge for document entity classification. We propose
KNN-former, which incorporates a new kind of spatial bias in attention
calculation based on the K-nearest-neighbor (KNN) graph of document entities.
We limit entities' attention only to their local radius defined by the KNN
graph. We also use combinatorial matching to address the one-to-one mapping
property that exists in many documents, where one field has only one
corresponding entity. Moreover, our method is highly parameter-efficient
compared to existing approaches in terms of the number of trainable parameters.
Despite this, experiments across various datasets show our method outperforms
baselines in most entity types. Many real-world documents exhibit combinatorial
properties which can be leveraged as inductive biases to improve extraction
accuracy, but existing datasets do not cover these documents. To facilitate
future research into these types of documents, we release a new ID document
dataset that covers diverse templates and languages. We also release enhanced
annotations for an existing dataset.

摘要：<paragraph>由不同範本組成且呈現複雜空間結構的文件，會對文件實體分類構成挑戰。我們提出 KNN-former，它在注意力計算中結合了一種基於文件實體的 K-最近鄰 (KNN) 圖的新型空間偏差。我們將實體的注意力限制在其由 KNN 圖定義的局部半徑內。我們還使用組合匹配來解決許多文件中存在的單對單對應屬性，其中一個欄位只有一個對應實體。此外，與現有方法相比，我們的方法在可訓練參數數量方面具有很高的參數效率。儘管如此，在各種資料集上的實驗表明，我們的模型在大部分實體類型中都優於基準。許多真實世界的文件都表現出組合屬性，這些屬性可以用作歸納偏差來提高提取準確度，但現有資料集並未涵蓋這些文件。為了促進對這些類型文件的未來研究，我們發布了一個新的身分證件文件資料集，其中涵蓋了不同的範本和語言。我們還為現有資料集發布了增強的註釋。</paragraph>

##### **DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature**
2405.04819v2 by Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen

Recent advancements in large language models (LLMs) have achieved promising
performances across various applications. Nonetheless, the ongoing challenge of
integrating long-tail knowledge continues to impede the seamless adoption of
LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic
Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its
ability on studying Alzheimer's Disease (AD), a specialized sub-field in
biomedicine and a global health priority. With a synergized framework of LLM
and KG mutually enhancing each other, we first leverage LLM to construct an
evolving AD-specific knowledge graph (KG) sourced from AD-related scientific
literature, and then we utilize a coarse-to-fine sampling method with a novel
self-aware knowledge retrieval approach to select appropriate knowledge from
the KG to augment LLM inference capabilities. The experimental results,
conducted on our constructed AD question answering (ADQA) benchmark, underscore
the efficacy of DALK. Additionally, we perform a series of detailed analyses
that can offer valuable insights and guidelines for the emerging topic of
mutually enhancing KG and LLM. We will release the code and data at
https://github.com/David-Li0406/DALK.

摘要：大型語言模型 (LLM) 的近期進展在各種應用中都取得了有望的表現。儘管如此，整合長尾知識的持續挑戰仍然阻礙了 LLM 在專業領域中的無縫採用。在這項工作中，我們介紹了 DALK，又名 LLM 和 KG 的動態共增強，以解決這個限制，並展示其在研究阿茲海默症 (AD) 的能力，這是生物醫學中的專業子領域，也是全球健康優先事項。利用 LLM 和 KG 相互增強的協同框架，我們首先利用 LLM 從與 AD 相關的科學文獻中構建一個不斷演化的 AD 特定知識圖譜 (KG)，然後我們利用一種粗到細的抽樣方法，採用一種新穎的自知知識檢索方法，從 KG 中選擇適當的知識來增強 LLM 推論能力。在我們構建的 AD 問題解答 (ADQA) 基準上進行的實驗結果強調了 DALK 的功效。此外，我們進行了一系列詳細的分析，可以為 KG 和 LLM 相互增強的新興主題提供有價值的見解和指導方針。我們將在 https://github.com/David-Li0406/DALK 上發布代碼和數據。

##### **BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models**
2405.04756v1 by Chu Fei Luo, Ahmad Ghawanmeh, Xiaodan Zhu, Faiza Khan Khattak

Modern large language models (LLMs) have a significant amount of world
knowledge, which enables strong performance in commonsense reasoning and
knowledge-intensive tasks when harnessed properly. The language model can also
learn social biases, which has a significant potential for societal harm. There
have been many mitigation strategies proposed for LLM safety, but it is unclear
how effective they are for eliminating social biases. In this work, we propose
a new methodology for attacking language models with knowledge graph augmented
generation. We refactor natural language stereotypes into a knowledge graph,
and use adversarial attacking strategies to induce biased responses from
several open- and closed-source language models. We find our method increases
bias in all models, even those trained with safety guardrails. This
demonstrates the need for further research in AI safety, and further work in
this new adversarial space.

摘要：現代的大型語言模型（LLM）擁有大量的世界知識，在適當利用時，這使它們在常識推理和知識密集型任務中表現出色。語言模型還可以學習社會偏見，這對社會危害具有重大潛力。已經提出了許多針對 LLM 安全性的緩解策略，但目前尚不清楚它們在消除社會偏見方面的有效性。在這項工作中，我們提出了一種新的方法，使用知識圖譜增強生成來攻擊語言模型。我們將自然語言刻板印象重構為知識圖譜，並使用對抗性攻擊策略來誘導來自幾個開源和閉源語言模型的偏差回應。我們發現我們的模型增加了所有模型中的偏差，即使是那些使用安全防護措施進行訓練的模型。這表明需要進一步研究人工智能安全，並在這個新的對抗空間中進一步開展工作。

##### **AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models**
2405.04753v1 by Yongheng Zhang, Tingwen Du, Yunshan Ma, Xiang Wang, Yi Xie, Guozheng Yang, Yuliang Lu, Ee-Chien Chang

Attack knowledge graph construction seeks to convert textual cyber threat
intelligence (CTI) reports into structured representations, portraying the
evolutionary traces of cyber attacks. Even though previous research has
proposed various methods to construct attack knowledge graphs, they generally
suffer from limited generalization capability to diverse knowledge types as
well as requirement of expertise in model design and tuning. Addressing these
limitations, we seek to utilize Large Language Models (LLMs), which have
achieved enormous success in a broad range of tasks given exceptional
capabilities in both language understanding and zero-shot task fulfillment.
Thus, we propose a fully automatic LLM-based framework to construct attack
knowledge graphs named: AttacKG+. Our framework consists of four consecutive
modules: rewriter, parser, identifier, and summarizer, each of which is
implemented by instruction prompting and in-context learning empowered by LLMs.
Furthermore, we upgrade the existing attack knowledge schema and propose a
comprehensive version. We represent a cyber attack as a temporally unfolding
event, each temporal step of which encapsulates three layers of representation,
including behavior graph, MITRE TTP labels, and state summary. Extensive
evaluation demonstrates that: 1) our formulation seamlessly satisfies the
information needs in threat event analysis, 2) our construction framework is
effective in faithfully and accurately extracting the information defined by
AttacKG+, and 3) our attack graph directly benefits downstream security
practices such as attack reconstruction. All the code and datasets will be
released upon acceptance.

摘要：攻擊知識圖譜建構旨在將文字形式的網路威脅情報 (CTI) 報告轉換成結構化表示，描繪網路攻擊的演化軌跡。儘管先前的研究已提出各種建構攻擊知識圖譜的方法，但它們普遍存在對不同知識類型的概化能力有限，以及對模型設計和調整專業知識的需求等問題。為了解決這些限制，我們尋求利用大型語言模型 (LLM)，它在廣泛的任務中取得了巨大的成功，在語言理解和零次學習任務完成方面都具備出色的能力。因此，我們提出一個完全自動化的基於 LLM 的框架來建構攻擊知識圖譜，名為：AttacKG+。我們的框架包含四個連續的模組：改寫器、解析器、識別器和摘要器，每個模組都是透過指令提示和 LLM 賦能的脈絡學習來實現。此外，我們升級了現有的攻擊知識架構，並提出了一個全面的版本。我們將網路攻擊表示為一個時間展開的事件，每個時間步驟都包含三層表示，包括行為圖、MITRE TTP 標籤和狀態摘要。廣泛的評估表明：1) 我們的表述無縫地滿足了威脅事件分析中的資訊需求，2) 我們的建構框架有效地忠實且準確地提取 AttacKG+ 定義的資訊，以及 3) 我們的攻擊圖直接有利於下游安全實務，例如攻擊重建。所有程式碼和資料集將在被接受後發布。

##### **Enriched BERT Embeddings for Scholarly Publication Classification**
2405.04136v1 by Benjamin Wolff, Eva Seidlmayer, Konrad U. Förstner

With the rapid expansion of academic literature and the proliferation of
preprints, researchers face growing challenges in manually organizing and
labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses
this challenge organized as a competition. The goal is to develop a classifier
capable of predicting one of 123 predefined classes from the Open Research
Knowledge Graph (ORKG) taxonomy of research fields for a given article.This
paper presents our results. Initially, we enrich the dataset (containing
English scholarly articles sourced from ORKG and arXiv), then leverage
different pre-trained language Models (PLMs), specifically BERT, and explore
their efficacy in transfer learning for this downstream task. Our experiments
encompass feature-based and fine-tuned transfer learning approaches using
diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and
SPECTER2. We conduct hyperparameter tuning and investigate the impact of data
augmentation from bibliographic databases such as OpenAlex, Semantic Scholar,
and Crossref. Our results demonstrate that fine-tuning pre-trained models
substantially enhances classification performance, with SPECTER2 emerging as
the most accurate model. Moreover, enriching the dataset with additional
metadata improves classification outcomes significantly, especially when
integrating information from S2AG, OpenAlex and Crossref. Our best-performing
approach achieves a weighted F1-score of 0.7415. Overall, our study contributes
to the advancement of reliable automated systems for scholarly publication
categorization, offering a potential solution to the laborious manual curation
process, thereby facilitating researchers in efficiently locating relevant
resources.

摘要：<paragraph>隨著學術文獻的快速擴展和預印本的激增，研究人員在手動組織和標記大量文章時面臨著越來越大的挑戰。NSLP 2024 FoRC 共享任務一以競賽的形式解決了這一挑戰。目標是開發一個分類器，能夠從給定文章的研究領域的開放研究知識圖譜 (ORKG) 分類法中預測 123 個預定義類別之一。本文展示了我們的成果。最初，我們豐富了數據集（包含來自 ORKG 和 arXiv 的英文學術文章），然後利用不同的預訓練語言模型 (PLM)，特別是 BERT，並探索它們在用於此下游任務的遷移學習中的功效。我們的實驗包括使用針對科學任務進行優化的各種 PLM 的基於特徵和微調的遷移學習方法，包括 SciBERT、SciNCL 和 SPECTER2。我們進行超參數調整，並研究來自 OpenAlex、Semantic Scholar 和 Crossref 等書目數據庫的數據增強的影響。我們的結果表明，微調預訓練模型可以顯著提高分類性能，而 SPECTER2 成為最準確的模型。此外，使用額外的元數據豐富數據集可以顯著改善分類結果，特別是在整合來自 S2AG、OpenAlex 和 Crossref 的信息時。我們性能最好的方法實現了 0.7415 的加權 F1 分數。總的來說，我們的研究有助於推進用於學術出版物分類的可靠自動化系統，為繁瑣的手動策展過程提供了一個潛在的解決方案，從而幫助研究人員有效地找到相關資源。</paragraph>

##### **FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering**
2405.03734v1 by Silan Hu, Xiaoning Wang

Integrating large language models (LLMs) and knowledge graphs (KGs) holds
great promise for revolutionizing intelligent education, but challenges remain
in achieving personalization, interactivity, and explainability. We propose
FOKE, a Forest Of Knowledge and Education framework that synergizes foundation
models, knowledge graphs, and prompt engineering to address these challenges.
FOKE introduces three key innovations: (1) a hierarchical knowledge forest for
structured domain knowledge representation; (2) a multi-dimensional user
profiling mechanism for comprehensive learner modeling; and (3) an interactive
prompt engineering scheme for generating precise and tailored learning
guidance.
  We showcase FOKE's application in programming education, homework assessment,
and learning path planning, demonstrating its effectiveness and practicality.
Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.
Our research highlights the potential of integrating foundation models,
knowledge graphs, and prompt engineering to revolutionize intelligent education
practices, ultimately benefiting learners worldwide. FOKE provides a principled
and unified approach to harnessing cutting-edge AI technologies for
personalized, interactive, and explainable educational services, paving the way
for further research and development in this critical direction.

摘要：整合大型語言模型 (LLM) 和知識圖譜 (KG) 對於革新智慧教育具有極大的前景，但在實現個人化、互動性和可解釋性方面仍存在挑戰。我們提出 FOKE，一個知識和教育森林架構，它協同了基礎模型、知識圖譜和提示工程，以應對這些挑戰。FOKE 引入了三項關鍵創新：(1) 一個用於結構化領域知識表示的分層知識森林；(2) 一個用於全面學習者建模的多維用戶分析機制；(3) 一個用於產生精確且量身定制的學習指導的互動式提示工程方案。我們展示了 FOKE 在程式教育、作業評量和學習路徑規劃中的應用，證明了它的有效性和實用性。此外，我們實作了 Scholar Hero，一個 FOKE 的真實世界實例。我們的研究強調了整合基礎模型、知識圖譜和提示工程以革新智慧教育實務的潛力，最終使全球學習者受益。FOKE 提供了一種基於原則且統一的方法，用於利用尖端的 AI 技術提供個人化、互動且可解釋的教育服務，為這個關鍵領域的進一步研究和發展鋪平了道路。

##### **E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods**
2405.02984v1 by Şükrü Öztürk, Hacer Yalim Keles

This study introduces the continuous Educational Turkish Sign Language
(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,
and 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and
includes performances from 11 signers. Turkish, an agglutinative language,
poses unique challenges for sign language translation, particularly with a
vocabulary where 64% are singleton words and 85% are rare words, appearing less
than five times. We developed two baseline models to address these challenges:
the Pose to Text Transformer (P2T-T) and the Graph Neural Network based
Transformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and
3.28% BLEU-4 score, presenting a significant challenge compared to existing
benchmarks. The P2T-T model, while demonstrating slightly lower performance in
BLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we
benchmarked our model using the well-known PHOENIX-Weather 2014T dataset to
validate our approach.

摘要：本研究介紹連續教育土耳其手語 (E-TSL) 資料集，此資料集是從線上土耳其語課程中收集而來，對象為 5、6、8 年級。該資料集包含 1,410 部影片，總長近 24 小時，並包含 11 位手語翻譯員的表演。土耳其語是一種黏著語，對手語翻譯構成獨特挑戰，特別是其詞彙中有 64% 是單身詞，而 85% 是罕見詞彙，出現次數不到五次。我們開發了兩個基線模型來應對這些挑戰：姿勢轉文字轉換器 (P2T-T) 和基於圖形神經網路的轉換器 (GNN-T) 模型。GNN-T 模型獲得 19.13% 的 BLEU-1 分數和 3.28% 的 BLEU-4 分數，與現有基準相比，是一個重大的挑戰。P2T-T 模型雖然在 BLEU 分數上的表現稍低，但獲得了更高的 ROUGE-L 分數，為 22.09%。此外，我們使用著名的 PHOENIX-Weather 2014T 資料集對我們的模型進行基準測試，以驗證我們的做法。

##### **Relations Prediction for Knowledge Graph Completion using Large Language Models**
2405.02738v1 by Sakher Khalil Alqaaidi, Krzysztof Kochut

Knowledge Graphs have been widely used to represent facts in a structured
format. Due to their large scale applications, knowledge graphs suffer from
being incomplete. The relation prediction task obtains knowledge graph
completion by assigning one or more possible relations to each pair of nodes.
In this work, we make use of the knowledge graph node names to fine-tune a
large language model for the relation prediction task. By utilizing the node
names only we enable our model to operate sufficiently in the inductive
settings. Our experiments show that we accomplish new scores on a widely used
knowledge graph benchmark.

摘要：知識圖譜已被廣泛用於以結構化格式表示事實。由於其大規模應用，知識圖譜存在不完整的問題。關係預測任務通過為每對節點分配一個或多個可能的關係來獲得知識圖譜的完成。在這項工作中，我們利用知識圖譜節點名稱來微調關係預測任務的大語言模型。僅通過使用節點名稱，我們就能讓模型在歸納設置中充分運作。我們的實驗表明，我們在廣泛使用的知識圖譜基準測試中獲得了新的分數。

##### **IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data**
2405.15792v1 by Sami Azirar, Hossam A. Gabbar, Chaouki Regoui

As the amount and complexity of data grows, retrieving it has become a more
difficult task that requires greater knowledge and resources. This is
especially true for the logistics industry, where new technologies for data
collection provide tremendous amounts of interconnected real-time data. The
Intelligent Query and Learning System (IQLS) simplifies the process by allowing
natural language use to simplify data retrieval . It maps structured data into
a framework based on the available metadata and available data models. This
framework creates an environment for an agent powered by a Large Language
Model. The agent utilizes the hierarchical nature of the data to filter
iteratively by making multiple small context-aware decisions instead of
one-shot data retrieval. After the Data filtering, the IQLS enables the agent
to fulfill tasks given by the user query through interfaces. These interfaces
range from multimodal transportation information retrieval to route planning
under multiple constraints. The latter lets the agent define a dynamic object,
which is determined based on the query parameters. This object represents a
driver capable of navigating a road network. The road network is depicted as a
graph with attributes based on the data. Using a modified version of the
Dijkstra algorithm, the optimal route under the given constraints can be
determined. Throughout the entire process, the user maintains the ability to
interact and guide the system. The IQLS is showcased in a case study on the
Canadian logistics sector, allowing geospatial, visual, tabular and text data
to be easily queried semantically in natural language.

摘要：随着数据量和复杂性不断增加，检索数据已成为一项更困难的任务，需要更多的知识和资源。对于物流行业来说尤其如此，该行业用于数据收集的新技术提供了大量相互关联的实时数据。智能查询和学习系统 (IQLS) 通过允许使用自然语言来简化数据检索，从而简化了这一过程。它将结构化数据映射到基于可用元数据和可用数据模型的框架中。此框架为由大型语言模型提供支持的代理创建了一个环境。该代理利用数据的层次结构通过多次进行小的上下文感知决策而不是一次性数据检索来进行迭代过滤。在数据过滤之后，IQLS 使代理能够通过界面完成用户查询给定的任务。这些界面范围从多模式交通信息检索到在多重约束下的路线规划。后者让代理能够定义一个动态对象，该对象根据查询参数确定。此对象表示能够在道路网络中导航的驾驶员。道路网络被描述为具有基于数据的属性的图表。使用 Dijkstra 算法的修改版本，可以在给定约束下确定最佳路线。在整个过程中，用户始终能够与系统交互并指导系统。IQLS 在加拿大物流部门的案例研究中得到展示，允许在地理空间、视觉、表格和文本数据中使用自然语言轻松地进行语义查询。

##### **R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models**
2405.02659v1 by Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Jun Huang

Retrieval-augmented large language models (LLMs) leverage relevant content
retrieved by information retrieval systems to generate correct responses,
aiming to alleviate the hallucination problem. However, existing
retriever-responder methods typically append relevant documents to the prompt
of LLMs to perform text generation tasks without considering the interaction of
fine-grained structural semantics between the retrieved documents and the LLMs.
This issue is particularly important for accurate response generation as LLMs
tend to ``lose in the middle'' when dealing with input prompts augmented with
lengthy documents. In this work, we propose a new pipeline named ``Reinforced
Retriever-Reorder-Responder'' (R$^4$) to learn document orderings for
retrieval-augmented LLMs, thereby further enhancing their generation abilities
while the large numbers of parameters of LLMs remain frozen. The reordering
learning process is divided into two steps according to the quality of the
generated responses: document order adjustment and document representation
enhancement. Specifically, document order adjustment aims to organize retrieved
document orderings into beginning, middle, and end positions based on graph
attention learning, which maximizes the reinforced reward of response quality.
Document representation enhancement further refines the representations of
retrieved documents for responses of poor quality via document-level gradient
adversarial learning. Extensive experiments demonstrate that our proposed
pipeline achieves better factual question-answering performance on
knowledge-intensive tasks compared to strong baselines across various public
datasets. The source codes and trained models will be released upon paper
acceptance.

摘要：<paragraph>檢索增強大型語言模型 (LLM) 利用資訊檢索系統檢索到的相關內容來產生正確的回應，旨在減輕幻覺問題。然而，現有的檢索回應方法通常將相關文件附加到 LLM 的提示中，以執行文字生成任務，而沒有考慮檢索到的文件和 LLM 之間細粒度結構語義的互動。這個問題對於準確的回應生成特別重要，因為 LLM 在處理以冗長文件增強的輸入提示時往往會「迷失在中間」。在這項工作中，我們提出了一個名為「強化檢索重排序回應器」(R$^4$) 的新管道，用於學習檢索增強 LLM 的文件排序，從而進一步增強它們的生成能力，同時 LLM 的大量參數保持凍結。根據生成回應的品質，重排序學習過程分為兩個步驟：文件順序調整和文件表示增強。具體來說，文件順序調整旨在根據圖注意力學習將檢索到的文件排序組織成開始、中間和結束位置，這最大化了回應品質的強化獎勵。文件表示增強進一步通過文件級別梯度對抗學習，改善了品質不佳的回應的檢索文件表示。廣泛的實驗表明，與各種公共數據集上的強大基線相比，我們提出的管道在知識密集型任務上實現了更好的事實問答性能。原始碼和訓練好的模型將在論文被接受後發布。</paragraph>

##### **Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph**
2405.02105v1 by Vladyslav Nechakhin, Jennifer D'Souza, Steffen Eger

Structured science summaries or research contributions using properties or
dimensions beyond traditional keywords enhances science findability. Current
methods, such as those used by the Open Research Knowledge Graph (ORKG),
involve manually curating properties to describe research papers' contributions
in a structured manner, but this is labor-intensive and inconsistent between
the domain expert human curators. We propose using Large Language Models (LLMs)
to automatically suggest these properties. However, it's essential to assess
the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before
application. Our study performs a comprehensive comparative analysis between
ORKG's manually curated properties and those generated by the aforementioned
state-of-the-art LLMs. We evaluate LLM performance through four unique
perspectives: semantic alignment and deviation with ORKG properties,
fine-grained properties mapping accuracy, SciNCL embeddings-based cosine
similarity, and expert surveys comparing manual annotations with LLM outputs.
These evaluations occur within a multidisciplinary science setting. Overall,
LLMs show potential as recommendation systems for structuring science, but
further finetuning is recommended to improve their alignment with scientific
tasks and mimicry of human expertise.

摘要：結構化的科學摘要或研究貢獻使用超越傳統關鍵字的屬性或維度，提升科學的可查找性。目前的技術，像是開放研究知識圖譜 (ORKG) 所使用的技術，涉及人工策展屬性以結構化方式描述研究論文的貢獻，但這項工作費時且在領域專家的人工策展者之間不一致。我們提出使用大型語言模型 (LLM) 自動建議這些屬性。然而，在應用之前，評估 GPT-3.5、Llama 2 和 Mistral 等 LLM 的準備度對這項任務至關重要。我們的研究對 ORKG 人工策展的屬性和前述最先進的 LLM 所產生的屬性之間進行全面的比較分析。我們透過四個獨特的觀點評估 LLM 的效能：語意對齊和與 ORKG 屬性的偏差、細粒度屬性對應準確度、基於 SciNCL 嵌入的餘弦相似度，以及比較人工註解和 LLM 輸出的專家調查。這些評量發生在多學科科學環境中。總體而言，LLM 作為結構化科學的推薦系統顯示出潛力，但建議進一步微調以改善它們與科學任務的一致性，並模擬人類的專業知識。

##### **Protein binding affinity prediction under multiple substitutions applying eGNNs on Residue and Atomic graphs combined with Language model information: eGRAL**
2405.02374v1 by Arturo Fiorellini-Bernardis, Sebastien Boyer, Christoph Brunken, Bakary Diallo, Karim Beguir, Nicolas Lopez-Carranza, Oliver Bent

Protein-protein interactions (PPIs) play a crucial role in numerous
biological processes. Developing methods that predict binding affinity changes
under substitution mutations is fundamental for modelling and re-engineering
biological systems. Deep learning is increasingly recognized as a powerful tool
capable of bridging the gap between in-silico predictions and in-vitro
observations. With this contribution, we propose eGRAL, a novel SE(3)
equivariant graph neural network (eGNN) architecture designed for predicting
binding affinity changes from multiple amino acid substitutions in protein
complexes. eGRAL leverages residue, atomic and evolutionary scales, thanks to
features extracted from protein large language models. To address the limited
availability of large-scale affinity assays with structural information, we
generate a simulated dataset comprising approximately 500,000 data points. Our
model is pre-trained on this dataset, then fine-tuned and tested on
experimental data.

摘要：蛋白-蛋白交互作用 (PPIs) 在許多生物過程中扮演關鍵角色。開發可預測取代突變下結合親和力變化的方法，對於建模和重新設計生物系統至關重要。深度學習逐漸被認為是一種強大的工具，能夠彌合電腦模擬預測和體外觀察之間的差距。透過這項貢獻，我們提出 eGRAL，一種新穎的 SE(3) 等變圖神經網路 (eGNN) 架構，專門設計用來預測蛋白質複合體中多個胺基酸取代的結合親和力變化。eGRAL 透過從蛋白質大型語言模型中萃取的特徵，善用殘基、原子和演化尺度。為了解決大規模親和力測定法與結構資訊可用性有限的問題，我們產生了一個模擬的資料集，包含大約 500,000 個資料點。我們的模型在這組資料集上進行預訓練，然後在實驗資料上微調和測試。

##### **CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation**
2405.02355v1 by Kounianhua Du, Renting Rui, Huacan Chai, Lingyue Fu, Wei Xia, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang

Utilizing large language models to generate codes has shown promising meaning
in software development revolution. Despite the intelligence shown by the
general large language models, their specificity in code generation can still
be improved due to the syntactic gap and mismatched vocabulary existing among
natural language and different programming languages. In addition, programming
languages are inherently logical and complex, making them hard to be correctly
generated. Existing methods rely on multiple prompts to the large language
model to explore better solutions, which is expensive. In this paper, we
propose Syntax Graph Retrieval Augmented Code Generation (CodeGRAG) to enhance
the performance of LLMs in single-round code generation tasks. CodeGRAG
extracts and summarizes the control flow and data flow of code blocks to fill
the gap between programming languages and natural language. The extracted
external structural knowledge models the inherent flows of code blocks, which
can facilitate LLMs for better understanding of code syntax and serve as a
bridge among different programming languages. CodeGRAG significantly improves
the code generation ability of LLMs and can even offer performance gain for
cross-lingual code generation, e.g., C++ for Python.

摘要：利用大型语言模型来生成代码在软件开发革命中显示出有希望的意义。尽管通用大型语言模型显示出智能，但由于自然语言和不同编程语言之间存在的句法差距和词汇不匹配，它们在代码生成中的特异性仍然可以得到改善。此外，编程语言本质上是逻辑且复杂的，这使得它们难以被正确生成。现有方法依赖于向大型语言模型发出多个提示以探索更好的解决方案，这是昂贵的。在本文中，我们提出了语法图检索增强代码生成 (CodeGRAG) 来增强 LLM 在单轮代码生成任务中的性能。CodeGRAG 提取和总结代码块的控制流和数据流，以填补编程语言和自然语言之间的空白。提取的外部结构知识对代码块的固有流进行建模，这可以促进 LLM 更好地理解代码语法，并在不同的编程语言之间架起桥梁。CodeGRAG 显着提高了 LLM 的代码生成能力，甚至可以为跨语言代码生成提供性能提升，例如，C++ 用于 Python。

##### **ALCM: Autonomous LLM-Augmented Causal Discovery Framework**
2405.01744v1 by Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani

To perform effective causal inference in high-dimensional datasets,
initiating the process with causal discovery is imperative, wherein a causal
graph is generated based on observational data. However, obtaining a complete
and accurate causal graph poses a formidable challenge, recognized as an
NP-hard problem. Recently, the advent of Large Language Models (LLMs) has
ushered in a new era, indicating their emergent capabilities and widespread
applicability in facilitating causal reasoning across diverse domains, such as
medicine, finance, and science. The expansive knowledge base of LLMs holds the
potential to elevate the field of causal reasoning by offering
interpretability, making inferences, generalizability, and uncovering novel
causal structures. In this paper, we introduce a new framework, named
Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize
data-driven causal discovery algorithms and LLMs, automating the generation of
a more resilient, accurate, and explicable causal graph. The ALCM consists of
three integral components: causal structure learning, causal wrapper, and
LLM-driven causal refiner. These components autonomously collaborate within a
dynamic environment to address causal discovery questions and deliver plausible
causal graphs. We evaluate the ALCM framework by implementing two
demonstrations on seven well-known datasets. Experimental results demonstrate
that ALCM outperforms existing LLM methods and conventional data-driven causal
reasoning mechanisms. This study not only shows the effectiveness of the ALCM
but also underscores new research directions in leveraging the causal reasoning
capabilities of LLMs.

摘要：<paragraph>為了在高維度資料集中執行有效的因果推論，以因果發現啟動流程至關重要，其中因果圖表是根據觀察資料產生的。然而，取得完整且準確的因果圖表是一項艱鉅的挑戰，被視為 NP 難問題。最近，大型語言模型 (LLM) 的出現開啟了一個新紀元，顯示出它們在促進不同領域（例如醫學、金融和科學）的因果推理方面的顯著能力和廣泛適用性。LLM 廣泛的知識庫有潛力提升因果推理領域，方法是提供可解釋性、進行推論、通用化和發現新的因果結構。在本文中，我們介紹了一個名為自適應 LLM 增強因果發現框架 (ALCM) 的新框架，以協同資料驅動的因果發現演算法和 LLM，自動產生更具韌性、準確且可解釋的因果圖表。ALCM 包含三個整合元件：因果結構學習、因果包裝器和 LLM 驅動的因果精煉器。這些元件在動態環境中自主協作，以解決因果發現問題並提供合理的因果圖表。我們透過在七個知名資料集上執行兩個示範來評估 ALCM 框架。實驗結果證明 ALCM 優於現有的 LLM 方法和傳統的資料驅動因果推理機制。這項研究不僅顯示 ALCM 的有效性，也強調了利用 LLM 因果推理能力的新研究方向。</paragraph>

##### **Improving Complex Reasoning over Knowledge Graph with Logic-Aware Curriculum Tuning**
2405.01649v3 by Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao

Answering complex queries over incomplete knowledge graphs (KGs) is a
challenging job. Most previous works have focused on learning entity/relation
embeddings and simulating first-order logic operators with various neural
networks. However, they are bottlenecked by the inability to share world
knowledge to improve logical reasoning, thus resulting in suboptimal
performance. In this paper, we propose a complex reasoning schema over KG upon
large language models (LLMs), containing a curriculum-based logical-aware
instruction tuning framework, named LACT. Specifically, we augment the
arbitrary first-order logical queries via binary tree decomposition, to
stimulate the reasoning capability of LLMs. To address the difficulty gap among
different types of complex queries, we design a simple and flexible logic-aware
curriculum learning framework. Experiments across widely used datasets
demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR
score) over advanced methods, achieving the new state-of-the-art. Our code and
model will be released at GitHub and huggingface soon.

摘要：回答不完整知识图谱 (KG) 上的复杂查询是一项具有挑战性的工作。大多数以前的作品都专注于学习实体/关系嵌入，并使用各种神经网络模拟一阶逻辑运算符。然而，它们因无法共享世界知识来改进逻辑推理而成为瓶颈，从而导致次优性能。在本文中，我们提出了一个基于大语言模型 (LLM) 的 KG 上的复杂推理模式，其中包含一个基于课程的逻辑感知指令调整框架，名为 LACT。具体来说，我们通过二叉树分解来扩充任意一阶逻辑查询，以激发 LLM 的推理能力。为了解决不同类型复杂查询之间的难度差异，我们设计了一个简单且灵活的逻辑感知课程学习框架。在广泛使用的数据集上进行的实验表明，LACT 在先进方法上有了实质性的改进（带来了平均 +5.5% 的 MRR 分数），达到了新的最先进水平。我们的代码和模型将很快在 GitHub 和 huggingface 上发布。

##### **Identification of Entailment and Contradiction Relations between Natural Language Sentences: A Neurosymbolic Approach**
2405.01259v1 by Xuyao Feng, Anthony Hunter

Natural language inference (NLI), also known as Recognizing Textual
Entailment (RTE), is an important aspect of natural language understanding.
Most research now uses machine learning and deep learning to perform this task
on specific datasets, meaning their solution is not explainable nor explicit.
To address the need for an explainable approach to RTE, we propose a novel
pipeline that is based on translating text into an Abstract Meaning
Representation (AMR) graph. For this we use a pre-trained AMR parser. We then
translate the AMR graph into propositional logic and use a SAT solver for
automated reasoning. In text, often commonsense suggests that an entailment (or
contradiction) relationship holds between a premise and a claim, but because
different wordings are used, this is not identified from their logical
representations. To address this, we introduce relaxation methods to allow
replacement or forgetting of some propositions. Our experimental results show
this pipeline performs well on four RTE datasets.

摘要：自然語言推論 (NLI)，也稱為識別文本蘊涵關係 (RTE)，是自然語言理解的重要面向。
目前大多數的研究使用機器學習和深度學習在特定資料集上執行這項任務，這表示他們的解決方案無法解釋或明確說明。
為了滿足 RTE 可解釋方法的需求，我們提出一個新穎的管道，其基礎是將文字轉換為抽象意義表示 (AMR) 圖表。為此，我們使用預先訓練好的 AMR 解析器。接著，我們將 AMR 圖表轉換為命題邏輯，並使用 SAT 求解器進行自動推理。在文字中，常識通常會暗示前提和主張之間存在蘊涵（或矛盾）關係，但由於使用了不同的措辭，因此無法從其邏輯表示中辨識出來。為了解決這個問題，我們引進放鬆方法，允許替換或遺忘某些命題。我們的實驗結果顯示，這個管道在四個 RTE 資料集上表現良好。

##### **RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models**
2405.00449v1 by Mohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Maldonado, Rubén Izquierdo, Miguel Ángel Sotelo

Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.

摘要：在自动驾驶背景下预测道路使用者行为近年来已获得科学界的广泛关注。大多数研究专注于仅基于运动学信息来预测行为，这是对现实的简化，因为道路使用者是人类，因此他们会受到周围环境的极大影响。此外，大量研究工作依赖于强大的深度学习技术，这些技术在预测任务中表现出很高的性能指标，但可能缺乏充分理解和利用道路场景中包含的上下文语义信息的能力，更不用说它们无法提供人类可以理解的可解释预测的能力。在这项工作中，我们提出了一种可解释的道路使用者行为预测系统，该系统通过使用检索增强生成 (RAG) 技术集成了知识图谱 (KG) 的推理能力和大型语言模型 (LLM) 的表达能力。为此，将知识图谱嵌入 (KGE) 和贝叶斯推理相结合，以允许部署一个完全归纳推理系统，该系统能够发出依赖于图中包含的传统信息以及车载传感器实时收集的当前证据的预测。已经按照所提出的方法实现了两个用例：1）预测行人的过马路动作；2）预测变道动作。在这两种情况下，所达到的性能在预期和 F1 分数方面都超过了当前的最新水平，为该领域的未来研究展示了一条有前途的途径。

##### **Graph Neural Network Approach to Semantic Type Detection in Tables**
2405.00123v1 by Ehsan Hoseinzade, Ke Wang

This study addresses the challenge of detecting semantic column types in
relational tables, a key task in many real-world applications. While language
models like BERT have improved prediction accuracy, their token input
constraints limit the simultaneous processing of intra-table and inter-table
information. We propose a novel approach using Graph Neural Networks (GNNs) to
model intra-table dependencies, allowing language models to focus on
inter-table information. Our proposed method not only outperforms existing
state-of-the-art algorithms but also offers novel insights into the utility and
functionality of various GNN types for semantic type detection. The code is
available at https://github.com/hoseinzadeehsan/GAIT

摘要：本研究解决了在关系表中检测语义列类型的挑战，这是许多实际应用中的关键任务。虽然像 BERT 这样的语言模型提高了预测准确性，但它们的标记输入限制了对表内和表间信息的同步处理。我们提出了一种使用图神经网络 (GNN) 的新方法来建模表内依赖关系，从而允许语言模型专注于表间信息。我们提出的方法不仅优于现有的最先进算法，还为语义类型检测提供各种 GNN 类型的实用性和功能的新见解。代码可在 https://github.com/hoseinzadeehsan/GAIT 获得

##### **PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification**
2404.19744v1 by Leon Garza, Lavanya Elluri, Anantaa Kotal, Aritran Piplai, Deepti Gupta, Anupam Joshi

Data protection and privacy is becoming increasingly crucial in the digital
era. Numerous companies depend on third-party vendors and service providers to
carry out critical functions within their operations, encompassing tasks such
as data handling and storage. However, this reliance introduces potential
vulnerabilities, as these vendors' security measures and practices may not
always align with the standards expected by regulatory bodies. Businesses are
required, often under the penalty of law, to ensure compliance with the
evolving regulatory rules. Interpreting and implementing these regulations pose
challenges due to their complexity. Regulatory documents are extensive,
demanding significant effort for interpretation, while vendor-drafted privacy
policies often lack the detail required for full legal compliance, leading to
ambiguity. To ensure a concise interpretation of the regulatory requirements
and compliance of organizational privacy policy with said regulations, we
propose a Large Language Model (LLM) and Semantic Web based approach for
privacy compliance. In this paper, we develop the novel Privacy Policy
Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to
efficiently store and retrieve comprehensive information concerning privacy
policies, regulatory frameworks, and domain-specific knowledge pertaining to
the legal landscape of privacy. Using Retrieval Augmented Generation, we
identify the relevant sections in a privacy policy with corresponding
regulatory rules. This information about individual privacy policies is
populated into the PrivComp-KG. Combining this with the domain context and
rules, the PrivComp-KG can be queried to check for compliance with privacy
policies by each vendor against relevant policy regulations. We demonstrate the
relevance of the PrivComp-KG, by verifying compliance of privacy policy
documents for various organizations.

摘要：<paragraph>在數位時代，資料保護和隱私正變得越來越重要。許多公司依賴第三方供應商和服務供應商，以執行其營運中的關鍵功能，包括資料處理和儲存等任務。然而，這種依賴會引發潛在的漏洞，因為這些供應商的安全措施和做法可能並不總是符合法規機關預期的標準。企業有義務（通常在法律的處罰下）確保符合不斷變化的法規規定。由於法規文件複雜，因此解釋和實施這些法規會構成挑戰。法規文件很廣泛，需要大量的解釋工作，而供應商起草的隱私政策通常缺乏符合完全法律規範所需的細節，導致模稜兩可。為了確保對法規要求的簡潔解釋，以及組織隱私政策符合法規，我們提出了一個基於大型語言模型 (LLM) 和語義網路的方法，用於隱私合規。在本文中，我們開發了新穎的隱私政策合規驗證知識圖譜 PrivComp-KG。它旨在有效儲存和檢索有關隱私政策、法規架構和與隱私法律環境相關的特定領域知識的綜合資訊。使用檢索擴充產生，我們使用對應的法規規則來識別隱私政策中的相關部分。有關個別隱私政策的這些資訊會填入 PrivComp-KG。將此與網域內容和規則結合後，可以對 PrivComp-KG 進行查詢，以檢查每個供應商的隱私政策是否符合相關政策法規。我們透過驗證各種組織的隱私政策文件，來證明 PrivComp-KG 的相關性。</paragraph>

##### **A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications**
2404.19729v1 by Steph Buongiorno, Corey Clark

External knowledge graphs (KGs) can be used to augment large language models
(LLMs), while simultaneously providing an explainable knowledge base of facts
that can be inspected by a human. This approach may be particularly valuable in
domains where explainability is critical, like human trafficking data analysis.
However, creating KGs can pose challenges. KGs parsed from documents may
comprise explicit connections (those directly stated by a document) but miss
implicit connections (those obvious to a human although not directly stated).
To address these challenges, this preliminary research introduces the GAME-KG
framework, standing for "Gaming for Augmenting Metadata and Enhancing Knowledge
Graphs." GAME-KG is a federated approach to modifying explicit as well as
implicit connections in KGs by using crowdsourced feedback collected through
video games. GAME-KG is shown through two demonstrations: a Unity test scenario
from Dark Shadows, a video game that collects feedback on KGs parsed from US
Department of Justice (DOJ) Press Releases on human trafficking, and a
following experiment where OpenAI's GPT-4 is prompted to answer questions based
on a modified and unmodified KG. Initial results suggest that GAME-KG can be an
effective framework for enhancing KGs, while simultaneously providing an
explainable set of structured facts verified by humans.

摘要：外部知識圖譜 (KG) 可用於擴充大型語言模型 (LLM)，同時提供人類可以檢視的事實可解釋知識庫。這種方法在可解釋性至關重要的領域中特別有價值，例如人口販運資料分析。然而，建立 KG 可能會帶來挑戰。從文件中解析的 KG 可能包含明確的連接（文件中直接陳述的連接），但會遺漏隱含的連接（對人類來說很明顯，但並未直接陳述）。為了應對這些挑戰，這項初步研究引入了 GAME-KG 架構，代表「透過遊戲擴充元資料並增強知識圖譜」。GAME-KG 是一種透過使用透過電玩遊戲收集的外包意見來修改 KG 中明確和隱含連接的聯合方法。GAME-KG 透過兩個示範說明：一個來自黑暗陰影的 Unity 測試場景，這是一款收集美國司法部 (DOJ) 人口販運新聞稿中解析的 KG 回饋的電玩遊戲，以及後續的實驗，其中提示 OpenAI 的 GPT-4 根據已修改和未修改的 KG 回答問題。初步結果表明，GAME-KG 可以成為增強 KG 的有效架構，同時提供人類驗證的可解釋結構化事實集。

##### **Octopus v4: Graph of language models**
2404.19296v1 by Wei Chen, Zhiyuan Li

Language models have been effective in a wide range of applications, yet the
most sophisticated models are often proprietary. For example, GPT-4 by OpenAI
and various models by Anthropic are expensive and consume substantial energy.
In contrast, the open-source community has produced competitive models, like
Llama3. Furthermore, niche-specific smaller language models, such as those
tailored for legal, medical or financial tasks, have outperformed their
proprietary counterparts. This paper introduces a novel approach that employs
\textit{functional tokens} to integrate \textbf{multiple open-source models},
each optimized for particular tasks. Our newly developed Octopus v4 model
leverages \textit{functional tokens} to intelligently direct user queries to
the most appropriate vertical model and reformat the query to achieve the best
performance. Octopus v4, an evolution of the Octopus v1, v2, and v3 models,
excels in selection and parameter understanding and reformatting. Additionally,
we explore the use of graph as a versatile data structure that effectively
coordinates multiple open-source models by harnessing the capabilities of the
Octopus model and \textit{functional tokens}. Use our open-sourced GitHub
(\url{https://www.nexa4ai.com/}) to try Octopus v4 models
(\url{https://huggingface.co/NexaAIDev/Octopus-v4}), and contrite to a larger
graph of language models. By activating models less than 10B parameters, we
achieved SOTA MMLU score of 74.8 among the same level models.

摘要：語言模型在廣泛的應用中非常有效，但最精密的模型通常是專有的。例如，OpenAI 的 GPT-4 和 Anthropic 的各種模型價格昂貴且消耗大量能源。相比之下，開源社群已經產出具有競爭力的模型，例如 Llama3。此外，針對特定領域的小型語言模型，例如為法律、醫療或財務任務量身打造的模型，已經超越了它們的專有對應模型。本文介紹一種新方法，它採用「功能性標記」來整合「多個開源模型」，每個模型都針對特定任務進行最佳化。我們新開發的 Octopus v4 模型利用「功能性標記」將使用者查詢智能導向最合適的垂直模型，並重新格式化查詢以達成最佳效能。Octopus v4 是 Octopus v1、v2 和 v3 模型的進化版本，在選擇、參數理解和重新格式化方面表現優異。此外，我們探討將圖形用作通用資料結構，它透過利用 Octopus 模型和「功能性標記」的能力，有效地協調多個開源模型。使用我們的開源 GitHub (\url{https://www.nexa4ai.com/}) 來試用 Octopus v4 模型 (\url{https://huggingface.co/NexaAIDev/Octopus-v4})，並為更大的語言模型圖形做出貢獻。透過啟用參數小於 10B 的模型，我們在同級別模型中達到了 74.8 的 SOTA MMLU 分數。

##### **Multi-hop Question Answering over Knowledge Graphs using Large Language Models**
2404.19234v1 by Abir Chakraborty

Knowledge graphs (KGs) are large datasets with specific structures
representing large knowledge bases (KB) where each node represents a key entity
and relations amongst them are typed edges. Natural language queries formed to
extract information from a KB entail starting from specific nodes and reasoning
over multiple edges of the corresponding KG to arrive at the correct set of
answer nodes. Traditional approaches of question answering on KG are based on
(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL
query, etc.) is generated using node and edge embeddings and then reasoning
over these representations or tuning language models to generate the final
answer directly, or (b) information-retrieval based that works by extracting
entities and relations sequentially. In this work, we evaluate the capability
of (LLMs) to answer questions over KG that involve multiple hops. We show that
depending upon the size and nature of the KG we need different approaches to
extract and feed the relevant information to an LLM since every LLM comes with
a fixed context window. We evaluate our approach on six KGs with and without
the availability of example-specific sub-graphs and show that both the IR and
SP-based methods can be adopted by LLMs resulting in an extremely competitive
performance.

摘要：知識圖譜 (KG) 是具有特定結構的大型資料集，表示大型知識庫 (KB)，其中每個節點代表一個關鍵實體，而它們之間的關係是輸入邊緣。從特定節點開始並推理對應 KG 的多個邊緣以到達正確的答案節點集合，形成用於從 KB 中提取資訊的自然語言查詢。傳統的 KG 問答方法基於 (a) 語義解析 (SP)，其中使用節點和邊緣嵌入產生邏輯形式（例如 S 表達式、SPARQL 查詢等），然後推理這些表示或調整語言模型以直接產生最終答案，或 (b) 基於資訊檢索，其通過按順序提取實體和關係來工作。在這項工作中，我們評估了 (LLM) 回答涉及多跳的 KG 問題的能力。我們表明，根據 KG 的大小和性質，我們需要不同的方法來提取相關資訊並將其提供給 LLM，因為每個 LLM 都有一個固定的上下文視窗。我們在有和沒有範例特定子圖的情況下對六個 KG 評估了我們的方法，並表明 IR 和基於 SP 的方法都可以被 LLM 採用，從而產生極具競爭力的效能。

##### **Automated Construction of Theme-specific Knowledge Graphs**
2404.19146v1 by Linyi Ding, Sizhe Zhou, Jinfeng Xiao, Jiawei Han

Despite widespread applications of knowledge graphs (KGs) in various tasks
such as question answering and intelligent conversational systems, existing KGs
face two major challenges: information granularity and deficiency in
timeliness. These hinder considerably the retrieval and analysis of in-context,
fine-grained, and up-to-date knowledge from KGs, particularly in highly
specialized themes (e.g., specialized scientific research) and rapidly evolving
contexts (e.g., breaking news or disaster tracking). To tackle such challenges,
we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed
from a theme-specific corpus, and design an unsupervised framework for ThemeKG
construction (named TKGCon). The framework takes raw theme-specific corpus and
generates a high-quality KG that includes salient entities and relations under
the theme. Specifically, we start with an entity ontology of the theme from
Wikipedia, based on which we then generate candidate relations by Large
Language Models (LLMs) to construct a relation ontology. To parse the documents
from the theme corpus, we first map the extracted entity pairs to the ontology
and retrieve the candidate relations. Finally, we incorporate the context and
ontology to consolidate the relations for entity pairs. We observe that
directly prompting GPT-4 for theme-specific KG leads to inaccurate entities
(such as "two main types" as one entity in the query result) and unclear (such
as "is", "has") or wrong relations (such as "have due to", "to start"). In
contrast, by constructing the theme-specific KG step by step, our model
outperforms GPT-4 and could consistently identify accurate entities and
relations. Experimental results also show that our framework excels in
evaluations compared with various KG construction baselines.

摘要：儘管知識圖譜 (KG) 廣泛應用於各種任務，例如問答和智慧對話系統，現有的 KG 仍面臨兩項重大挑戰：資訊粒度和即時性不足。這些問題嚴重阻礙了從 KG 中擷取和分析情境中的、細粒度的和最新的知識，特別是在高度專業的主題（例如，專業科學研究）和快速變化的情境（例如，突發新聞或災難追蹤）中。為了應對這些挑戰，我們提出了一個主題特定知識圖譜（亦即 ThemeKG），一個由主題特定語料庫建構的 KG，並設計了一個非監督式框架 TKGCon，用於建構 ThemeKG。此框架採用原始的主題特定語料庫，並產生一個高品質的 KG，其中包含該主題下的顯著實體和關係。具體來說，我們從維基百科中提取該主題的實體本体，然後根據該本体，我們再由大型語言模型 (LLM) 產生候選關係，以建構關係本体。為了解析主題語料庫中的文件，我們首先將提取的實體對應到本体，並擷取候選關係。最後，我們結合情境和本体，以合併實體對的關係。我們觀察到，直接提示 GPT-4 產生主題特定 KG 會導致不準確的實體（例如，查詢結果中將「兩個主要類型」視為一個實體），以及不明確（例如，「是」、「有」）或錯誤的關係（例如，「歸因於」、「開始」）。相比之下，透過逐步建構主題特定 KG，我們的模型優於 GPT-4，並能持續識別準確的實體和關係。實驗結果也顯示，與各種 KG 建構基準相比，我們的框架在評估中表現出色。

##### **QANA: LLM-based Question Generation and Network Analysis for Zero-shot Key Point Analysis and Beyond**
2404.18371v1 by Tomoki Fukuma, Koki Noda, Toshihide Ubukata Kousuke Hoso, Yoshiharu Ichikawa, Kyosuke Kambe, Yu Masubuch, Fujio Toriumi

The proliferation of social media has led to information overload and
increased interest in opinion mining. We propose "Question-Answering Network
Analysis" (QANA), a novel opinion mining framework that utilizes Large Language
Models (LLMs) to generate questions from users' comments, constructs a
bipartite graph based on the comments' answerability to the questions, and
applies centrality measures to examine the importance of opinions. We
investigate the impact of question generation styles, LLM selections, and the
choice of embedding model on the quality of the constructed QA networks by
comparing them with annotated Key Point Analysis datasets. QANA achieves
comparable performance to previous state-of-the-art supervised models in a
zero-shot manner for Key Point Matching task, also reducing the computational
cost from quadratic to linear. For Key Point Generation, questions with high
PageRank or degree centrality align well with manually annotated key points.
Notably, QANA enables analysts to assess the importance of key points from
various aspects according to their selection of centrality measure. QANA's
primary contribution lies in its flexibility to extract key points from a wide
range of perspectives, which enhances the quality and impartiality of opinion
mining.

摘要：社交媒體的興起導致資訊過載，以及對於意見探勘的興趣增加。我們提出「問答網路分析」(QANA)，一個新穎的意見探勘架構，利用大型語言模型 (LLM) 從使用者的留言產生問題，根據留言對問題的可回答性建構一個二部圖，並應用中心性測量來檢視意見的重要性。我們透過將它們與標註關鍵點分析資料集進行比較，來探討問題產生風格、LLM 選擇和嵌入模型的選擇對建構的問答網路品質的影響。QANA 在關鍵點配對任務中以零次學習的方式，達成與先前最先進的監督式模型相當的效能，同時將運算成本從二次方降低到一次方。對於關鍵點產生，具有高 PageRank 或度中心性的問題與手動標註的關鍵點非常吻合。值得注意的是，QANA 能讓分析師根據他們選擇的中心性測量，從各種面向評估關鍵點的重要性。QANA 的主要貢獻在於它從廣泛的觀點中擷取關鍵點的靈活性，這提升了意見探勘的品質和公正性。

##### **Parameter-Efficient Tuning Large Language Models for Graph Representation Learning**
2404.18271v1 by Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, George Karypis

Text-rich graphs, which exhibit rich textual information on nodes and edges,
are prevalent across a wide range of real-world business applications. Large
Language Models (LLMs) have demonstrated remarkable abilities in understanding
text, which also introduced the potential for more expressive modeling in
text-rich graphs. Despite these capabilities, efficiently applying LLMs to
representation learning on graphs presents significant challenges. Recently,
parameter-efficient fine-tuning methods for LLMs have enabled efficient new
task generalization with minimal time and memory consumption. Inspired by this,
we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel
approach for efficient graph representation learning with LLMs on text-rich
graphs. Specifically, we utilize a graph neural network (GNN) to encode
structural information from neighboring nodes into a graph prompt. This prompt
is then inserted at the beginning of the text sequence. To improve the quality
of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting
the next token in the node text. Compared with existing joint GNN and LMs, our
method directly generate the node embeddings from large language models with an
affordable fine-tuning cost. We validate our approach through comprehensive
experiments conducted on 8 different text-rich graphs, observing an average
improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction
evaluations. Our results demonstrate the efficacy and efficiency of our model,
showing that it can be smoothly integrated with various large language models,
including OPT, LLaMA and Falcon.

摘要：<paragraph>文本丰富的图表在广泛的实际商业应用中普遍存在，它们在节点和边上展示丰富的文本信息。大型语言模型 (LLM) 已在理解文本方面展现出非凡的能力，这也为在文本丰富的图表中进行更具表现力的建模带来了可能性。尽管有这些能力，但有效地将 LLM 应用于图表上的表示学习仍然面临着重大挑战。最近，针对 LLM 的参数高效微调方法已实现高效的新任务泛化，且时间和内存消耗最小。受此启发，我们引入了图感知参数高效微调 - GPEFT，一种在文本丰富的图表上使用 LLM 进行高效图表示学习的新方法。具体而言，我们利用图神经网络 (GNN) 将来自相邻节点的结构信息编码到图提示中。然后将此提示插入文本序列的开头。为了提高图提示的质量，我们预训练了 GNN 以帮助冻结的 LLM 预测节点文本中的下一个标记。与现有的联合 GNN 和 LM 相比，我们的方法直接从大型语言模型生成节点嵌入，且微调成本低廉。我们通过在 8 个不同的文本丰富图表上进行的综合实验验证了我们的方法，观察到在链接预测评估中，hit@1 和平均倒数秩 (MRR) 平均提高了 2%。我们的结果证明了我们模型的有效性和效率，表明它可以与各种大型语言模型（包括 OPT、LLaMA 和 Falcon）平滑集成。</paragraph>

##### **Generative AI for Visualization: State of the Art and Future Directions**
2404.18144v1 by Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, Wei Zeng

Generative AI (GenAI) has witnessed remarkable progress in recent years and
demonstrated impressive performance in various generation tasks in different
domains such as computer vision and computational design. Many researchers have
attempted to integrate GenAI into visualization framework, leveraging the
superior generative capacity for different operations. Concurrently, recent
major breakthroughs in GenAI like diffusion model and large language model have
also drastically increase the potential of GenAI4VIS. From a technical
perspective, this paper looks back on previous visualization studies leveraging
GenAI and discusses the challenges and opportunities for future research.
Specifically, we cover the applications of different types of GenAI methods
including sequence, tabular, spatial and graph generation techniques for
different tasks of visualization which we summarize into four major stages:
data enhancement, visual mapping generation, stylization and interaction. For
each specific visualization sub-task, we illustrate the typical data and
concrete GenAI algorithms, aiming to provide in-depth understanding of the
state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based
on the survey, we discuss three major aspects of challenges and research
opportunities including evaluation, dataset, and the gap between end-to-end
GenAI and generative algorithms. By summarizing different generation
algorithms, their current applications and limitations, this paper endeavors to
provide useful insights for future GenAI4VIS research.

摘要：生成式 AI (GenAI) 近年來見證了顯著的進展，並在不同的領域（例如電腦視覺和計算設計）中，展現了在各種生成任務上的驚人表現。許多研究人員已嘗試將 GenAI 整合到視覺化架構中，利用其優越的生成能力進行不同的操作。同時，GenAI 最近在擴散模型和大型語言模型等方面的重大突破，也大幅提升了 GenAI4VIS 的潛力。從技術的角度來看，本文回顧了利用 GenAI 的先前的視覺化研究，並探討了未來研究的挑戰和機會。具體來說，我們涵蓋了不同類型 GenAI 方法的應用，包括序列、表格、空間和圖形生成技術，用於不同的視覺化任務，我們將其總結為四個主要階段：資料增強、視覺對應生成、樣式化和互動。對於每個特定的視覺化子任務，我們說明了典型的資料和具體的 GenAI 演算法，旨在深入了解最先進的 GenAI4VIS 技術及其限制。此外，根據調查，我們探討了挑戰和研究機會的三個主要方面，包括評估、資料集，以及端到端 GenAI 和生成演算法之間的差距。透過總結不同的生成演算法、它們目前的應用和限制，本文致力於為未來的 GenAI4VIS 研究提供有用的見解。

##### **Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering**
2404.17723v2 by Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li

In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.

摘要：在客戶服務技術支援中，快速且準確地擷取相關過往問題對於有效解決客戶詢問至關重要。大型語言模型 (LLM) 中用於檢索增強式生成 (RAG) 的傳統檢索方法將大量過往問題追蹤單據視為純文字，忽略了至關重要的議題內結構和議題間關係，這限制了效能。我們引入一種新穎的客戶服務問答方法，將 RAG 與知識圖譜 (KG) 結合。我們的做法是從歷史問題中建構 KG 以用於檢索，保留議題內結構和議題間關係。在問答階段，我們的方法會分析消費者查詢，並從 KG 中擷取相關子圖形來產生答案。這種 KG 整合不僅透過保留客戶服務結構資訊來提升檢索準確度，而且透過減輕文字分段的效果來提升回答品質。在我們的基準資料集上進行經驗評估，利用關鍵檢索 (MRR、Recall@K、NDCG@K) 和文字生成 (BLEU、ROUGE、METEOR) 指標，顯示我們的做法在 MRR 上優於基準線 77.6%，在 BLEU 上優於基準線 0.32。我們的做法已在 LinkedIn 的客戶服務團隊中部署約六個月，已將每個問題的平均解決時間縮短了 28.6%。

##### **PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**
2404.17662v1 by Qinglin Zhu, Runcong Zhao, Jinhua Du, Lin Gui, Yulan He

Recent advancements in Large Language Models (LLMs) have enhanced the
efficacy of agent communication and social interactions. Despite these
advancements, building LLM-based agents for reasoning in dynamic environments
involving competition and collaboration remains challenging due to the
limitations of informed graph-based search methods. We propose PLAYER*, a novel
framework based on an anytime sampling-based planner, which utilises sensors
and pruners to enable a purely question-driven searching framework for complex
reasoning tasks. We also introduce a quantifiable evaluation method using
multiple-choice questions and construct the WellPlay dataset with 1,482 QA
pairs. Experiments demonstrate PLAYER*'s efficiency and performance
enhancements compared to existing methods in complex, dynamic environments with
quantifiable results.

摘要：大型語言模型 (LLM) 的最新進展增強了代理溝通和社交互動的效能。儘管有這些進展，由於基於資訊圖形的搜尋方法的限制，在涉及競爭和協作的動態環境中建立基於 LLM 的代理以進行推理仍然具有挑戰性。我們提出 PLAYER*，一個基於隨時取樣規劃器的全新框架，它利用感測器和修剪器來啟用一個純粹由問題驅動的複雜推理任務搜尋框架。我們還使用多選題引入一種可量化的評估方法，並構建了一個包含 1,482 個問答對的 WellPlay 資料集。實驗證明了 PLAYER* 的效率和效能，與現有方法在具有可量化結果的複雜動態環境中相比有了提升。

##### **Language Interaction Network for Clinical Trial Approval Estimation**
2405.06662v1 by Chufan Gao, Tianfan Fu, Jimeng Sun

Clinical trial outcome prediction seeks to estimate the likelihood that a
clinical trial will successfully reach its intended endpoint. This process
predominantly involves the development of machine learning models that utilize
a variety of data sources such as descriptions of the clinical trials,
characteristics of the drug molecules, and specific disease conditions being
targeted. Accurate predictions of trial outcomes are crucial for optimizing
trial planning and prioritizing investments in a drug portfolio. While previous
research has largely concentrated on small-molecule drugs, there is a growing
need to focus on biologics-a rapidly expanding category of therapeutic agents
that often lack the well-defined molecular properties associated with
traditional drugs. Additionally, applying conventional methods like graph
neural networks to biologics data proves challenging due to their complex
nature. To address these challenges, we introduce the Language Interaction
Network (LINT), a novel approach that predicts trial outcomes using only the
free-text descriptions of the trials. We have rigorously tested the
effectiveness of LINT across three phases of clinical trials, where it achieved
ROC-AUC scores of 0.770, 0.740, and 0.748 for phases I, II, and III,
respectively, specifically concerning trials involving biologic interventions.

摘要：臨床試驗結果預測旨在估計臨床試驗成功達到預期終點的可能性。此程序主要涉及開發機器學習模型，利用各種數據來源，例如臨床試驗的說明、藥物分子的特徵和目標特定疾病的狀況。準確預測試驗結果對於優化試驗計畫和優先投資藥物組合至關重要。雖然先前的研究主要集中於小分子藥物，但有越來越多的需求專注於生物製劑，這是一個快速擴展的治療劑類別，通常缺乏與傳統藥物相關的明確分子特性。此外，由於生物製劑數據的複雜性，將圖形神經網路等傳統方法應用於生物製劑數據具有挑戰性。為了應對這些挑戰，我們引入了語言互動網路 (LINT)，這是一種僅使用試驗的自由文本描述來預測試驗結果的新方法。我們嚴格測試了 LINT 在臨床試驗的三个階段中的有效性，在涉及生物製劑干預的試驗中，分別在 I、II 和 III 期取得了 0.770、0.740 和 0.748 的 ROC-AUC 分數。

##### **CyNetDiff -- A Python Library for Accelerated Implementation of Network Diffusion Models**
2404.17059v1 by Eliot W. Robson, Dhemath Reddy, Abhishek K. Umrawal

In recent years, there has been increasing interest in network diffusion
models and related problems. The most popular of these are the independent
cascade and linear threshold models. Much of the recent experimental work done
on these models requires a large number of simulations conducted on large
graphs, a computationally expensive task suited for low-level languages.
However, many researchers prefer the use of higher-level languages (such as
Python) for their flexibility and shorter development times. Moreover, in many
research tasks, these simulations are the most computationally intensive task,
so it would be desirable to have a library for these with an interface to a
high-level language with the performance of a low-level language. To fill this
niche, we introduce CyNetDiff, a Python library with components written in
Cython to provide improved performance for these computationally intensive
diffusion tasks.

摘要：近年来，网络扩散模型及相关问题越来越受到关注。其中最流行的是独立级联和线性阈值模型。这些模型上进行的许多近期实验工作需要对大型图进行大量的模拟，这是一项适合低级语言的计算成本高昂的任务。然而，许多研究人员更喜欢使用高级语言（如 Python），因为它们具有灵活性且开发时间较短。此外，在许多研究任务中，这些模拟是最耗费计算资源的任务，因此最好有一个针对这些任务的库，该库具有高级语言的接口和低级语言的性能。为了填补这一空白，我们引入了 CyNetDiff，这是一个 Python 库，其中包含用 Cython 编写的组件，以提高这些计算密集型扩散任务的性能。

##### **Player-Driven Emergence in LLM-Driven Game Narrative**
2404.17027v2 by Xiangyu Peng, Jessica Quaye, Weijia Xu, Portia Botchway, Chris Brockett, Bill Dolan, Nebojsa Jojic, Gabriel DesGarennes, Ken Lobb, Michael Xu, Jorge Leandro, Claire Jin, Sudha Rao

We explore how interaction with large language models (LLMs) can give rise to
emergent behaviors, empowering players to participate in the evolution of game
narratives. Our testbed is a text-adventure game in which players attempt to
solve a mystery under a fixed narrative premise, but can freely interact with
non-player characters generated by GPT-4, a large language model. We recruit 28
gamers to play the game and use GPT-4 to automatically convert the game logs
into a node-graph representing the narrative in the player's gameplay. We find
that through their interactions with the non-deterministic behavior of the LLM,
players are able to discover interesting new emergent nodes that were not a
part of the original narrative but have potential for being fun and engaging.
Players that created the most emergent nodes tended to be those that often
enjoy games that facilitate discovery, exploration and experimentation.

摘要：我們探討與大型語言模型 (LLM) 互動如何產生新興行為，讓玩家能夠參與遊戲敘事的演變。我們的測試平台是一個文字冒險遊戲，玩家在固定的敘事前提下嘗試解開謎團，但可以自由與 GPT-4（一種大型語言模型）產生的非玩家角色互動。我們招募了 28 位玩家來玩遊戲，並使用 GPT-4 自動將遊戲記錄轉換成代表玩家遊戲過程中敘事的節點圖。我們發現，玩家透過與 LLM 的非確定性行為互動，能夠發現有趣的全新新興節點，這些節點並非原始敘事的一部分，但有潛力帶來樂趣和吸引力。創造最多新興節點的玩家往往是那些經常享受促進發現、探索和實驗的遊戲的人。

##### **Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models**
2404.17000v1 by Bradley P. Allen, Paul T. Groth

A backbone of knowledge graphs are their class membership relations, which
assign entities to a given class. As part of the knowledge engineering process,
we propose a new method for evaluating the quality of these relations by
processing descriptions of a given entity and class using a zero-shot
chain-of-thought classifier that uses a natural language intensional definition
of a class. We evaluate the method using two publicly available knowledge
graphs, Wikidata and CaLiGraph, and 7 large language models. Using the
gpt-4-0125-preview large language model, the method's classification
performance achieves a macro-averaged F1-score of 0.830 on data from Wikidata
and 0.893 on data from CaLiGraph. Moreover, a manual analysis of the
classification errors shows that 40.9% of errors were due to the knowledge
graphs, with 16.0% due to missing relations and 24.9% due to incorrectly
asserted relations. These results show how large language models can assist
knowledge engineers in the process of knowledge graph refinement. The code and
data are available on Github.

摘要：知識圖譜的骨幹是其類別成員關係，它將實體分配給特定類別。作為知識工程流程的一部分，我們提出了一種新的方法來評估這些關係的品質，方法是使用零次學習思維鏈分類器處理給定實體和類別的描述，該分類器使用類別的自然語言內涵定義。我們使用兩個公開可用的知識圖譜 Wikidata 和 CaLiGraph 以及 7 個大型語言模型來評估該方法。使用 gpt-4-0125-preview 大型語言模型，該方法的分類性能在 Wikidata 的資料上達到 0.830 的巨觀平均 F1 分數，在 CaLiGraph 的資料上達到 0.893。此外，對分類錯誤的手動分析顯示，40.9% 的錯誤是知識圖譜造成的，其中 16.0% 是由於關係遺失，24.9% 是由於錯誤斷言的關係。這些結果顯示了大型語言模型如何協助知識工程師進行知識圖譜優化的過程。程式碼和資料可在 Github 上取得。

##### **Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer**
2404.16627v1 by Jianyu Zheng, Fengfei Fan, Jianquan Li

Unsupervised cross-lingual transfer involves transferring knowledge between
languages without explicit supervision. Although numerous studies have been
conducted to improve performance in such tasks by focusing on cross-lingual
knowledge, particularly lexical and syntactic knowledge, current approaches are
limited as they only incorporate syntactic or lexical information. Since each
type of information offers unique advantages and no previous attempts have
combined both, we attempt to explore the potential of this approach. In this
paper, we present a novel framework called "Lexicon-Syntax Enhanced
Multilingual BERT" that combines both lexical and syntactic knowledge.
Specifically, we use Multilingual BERT (mBERT) as the base model and employ two
techniques to enhance its learning capabilities. The code-switching technique
is used to implicitly teach the model lexical alignment information, while a
syntactic-based graph attention network is designed to help the model encode
syntactic structure. To integrate both types of knowledge, we input
code-switched sequences into both the syntactic module and the mBERT base model
simultaneously. Our extensive experimental results demonstrate this framework
can consistently outperform all baselines of zero-shot cross-lingual transfer,
with the gains of 1.0~3.7 points on text classification, named entity
recognition (ner), and semantic parsing tasks. Keywords:cross-lingual transfer,
lexicon, syntax, code-switching, graph attention network

摘要：無監督跨語言轉移涉及在語言之間傳遞知識，而無需明確監督。儘管已進行許多研究，專注於跨語言知識（特別是詞彙和句法知識）來改善此類任務的性能，但目前的做法受到限制，因為它們僅包含句法或詞彙信息。由於每種類型的信息都提供獨特的優勢，並且以前沒有嘗試將兩者結合起來，因此我們嘗試探索這種方法的潛力。在本文中，我們提出了一個名為「詞彙句法增強多語言 BERT」的新框架，結合了詞彙和句法知識。具體來說，我們使用多語言 BERT (mBERT) 作為基礎模型，並採用兩種技術來增強其學習能力。代碼切換技術用於隱式教授模型詞彙對齊信息，而基於句法的圖注意力網路旨在幫助模型編碼句法結構。為了整合這兩種知識類型，我們同時將代碼切換序列輸入句法模組和 mBERT 基礎模型。我們廣泛的實驗結果表明，這個框架可以持續優於所有零次跨語言轉移基準，在文本分類、命名實體識別 (ner) 和語義解析任務上獲得 1.0~3.7 分的增益。關鍵字：跨語言轉移、詞彙、句法、代碼切換、圖注意力網路

##### **Semgrex and Ssurgeon, Searching and Manipulating Dependency Graphs**
2404.16250v1 by John Bauer, Chloe Kiddon, Eric Yeh, Alex Shan, Christopher D. Manning

Searching dependency graphs and manipulating them can be a time consuming and
challenging task to get right. We document Semgrex, a system for searching
dependency graphs, and introduce Ssurgeon, a system for manipulating the output
of Semgrex. The compact language used by these systems allows for easy command
line or API processing of dependencies. Additionally, integration with publicly
released toolkits in Java and Python allows for searching text relations and
attributes over natural text.

摘要：搜尋依賴關係圖形並操作它們可能是一項耗時且具有挑戰性的任務。我們記錄了 Semgrex，這是一個用於搜尋依賴關係圖形的系統，並介紹了 Ssurgeon，這是一個用於操作 Semgrex 輸出的系統。這些系統使用的簡潔語言允許輕鬆地對依賴關係進行命令列或 API 處理。此外，與 Java 和 Python 中公開發布的工具包整合允許搜尋自然語言文字關係和屬性。

##### **Knowledge Graph Completion using Structural and Textual Embeddings**
2404.16206v1 by Sakher Khalil Alqaaidi, Krzysztof Kochut

Knowledge Graphs (KGs) are widely employed in artificial intelligence
applications, such as question-answering and recommendation systems. However,
KGs are frequently found to be incomplete. While much of the existing
literature focuses on predicting missing nodes for given incomplete KG triples,
there remains an opportunity to complete KGs by exploring relations between
existing nodes, a task known as relation prediction. In this study, we propose
a relations prediction model that harnesses both textual and structural
information within KGs. Our approach integrates walks-based embeddings with
language model embeddings to effectively represent nodes. We demonstrate that
our model achieves competitive results in the relation prediction task when
evaluated on a widely used dataset.

摘要：知識圖譜 (KG) 廣泛應用於人工智慧應用程式，例如問答和推薦系統。然而，KG 經常被發現是不完整的。雖然現有文獻大多著重於預測給定不完整 KG 三元組的遺失節點，但仍有機會透過探索現有節點之間的關係來完成 KG，這項任務稱為關係預測。在本研究中，我們提出一個關係預測模型，它利用 KG 中的文字和結構資訊。我們的做法將基於遊走的嵌入與語言模型嵌入整合，以有效地表示節點。我們證明我們的模型在關係預測任務中取得具競爭力的結果，並在廣泛使用的資料集上進行評估。

##### **Improving Multi-label Recognition using Class Co-Occurrence Probabilities**
2404.16193v1 by Samyak Rawlekar, Shubhang Bhatnagar, Vishnuvardhan Pogunulu Srinivasulu, Narendra Ahuja

Multi-label Recognition (MLR) involves the identification of multiple objects
within an image. To address the additional complexity of this problem, recent
works have leveraged information from vision-language models (VLMs) trained on
large text-images datasets for the task. These methods learn an independent
classifier for each object (class), overlooking correlations in their
occurrences. Such co-occurrences can be captured from the training data as
conditional probabilities between a pair of classes. We propose a framework to
extend the independent classifiers by incorporating the co-occurrence
information for object pairs to improve the performance of independent
classifiers. We use a Graph Convolutional Network (GCN) to enforce the
conditional probabilities between classes, by refining the initial estimates
derived from image and text sources obtained using VLMs. We validate our method
on four MLR datasets, where our approach outperforms all state-of-the-art
methods.

摘要：多標籤辨識 (MLR) 涉及識別影像中的多個物件。為了解決這個問題的額外複雜性，最近的研究利用了針對大型文字影像資料集訓練的視覺語言模型 (VLM) 中的資訊來執行這項任務。這些方法針對每個物件 (類別) 學習一個獨立的分類器，忽略它們發生時的關聯性。這種共現可以從訓練資料中擷取為一對類別之間的條件機率。我們提出一個架構，透過納入物件對的共現資訊來擴充獨立分類器，以改善獨立分類器的效能。我們使用圖形卷積網路 (GCN) 來強制執行類別之間的條件機率，方法是利用 VLM 取得的影像和文字來源所衍生的初始估計值。我們在四個 MLR 資料集上驗證了我們的模型，我們的做法優於所有最先進的方法。

##### **From Local to Global: A Graph RAG Approach to Query-Focused Summarization**
2404.16130v1 by Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson

The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as "What are the main themes in the dataset?", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities
of text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose a Graph RAG approach to question answering over
private text corpora that scales with both the generality of user questions and
the quantity of source text to be indexed. Our approach uses an LLM to build a
graph-based text index in two stages: first to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely-related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that Graph RAG
leads to substantial improvements over a na\"ive RAG baseline for both the
comprehensiveness and diversity of generated answers. An open-source,
Python-based implementation of both global and local Graph RAG approaches is
forthcoming at https://aka.ms/graphrag.

摘要：使用檢索增強生成 (RAG) 從外部知識來源檢索相關資訊，讓大型語言模型 (LLM) 能回答私人和/或先前未見的文件集合中的問題。然而，針對整個文字語料庫的全球問題（例如「資料集中的主要主題是什麼？」），RAG 會失敗，因為這本質上是查詢重點摘要 (QFS) 任務，而不是明確的檢索任務。同時，先前的 QFS 方法無法擴充到典型 RAG 系統索引的文字量。為了結合這些對比方法的優勢，我們提出了一種圖形 RAG 方法，用於回答私人文字語料庫中的問題，該方法可隨著使用者問題的普遍性和要索引的原始文字量的增加而擴充。我們的做法使用 LLM 分兩個階段建立基於圖形的文字索引：首先從原始文件衍生出實體知識圖形，然後為所有密切相關實體的群組預先產生社群摘要。針對一個問題，每個社群摘要用於產生部分回應，然後在最終回應中再次摘要所有部分回應給使用者。對於 100 萬個代幣範圍內資料集的一類全球意義建構問題，我們展示了圖形 RAG 在生成答案的全面性和多樣性方面，都比天真的 RAG 基準線有顯著的進步。全球和本地圖形 RAG 方法的開源 Python 實作即將於 https://aka.ms/graphrag 上提供。

##### **KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction**
2404.15923v1 by Jack Boylan, Shashank Mangla, Dominic Thorn, Demian Gholipour Ghalandari, Parsa Ghaffari, Chris Hokamp

This study explores the use of Large Language Models (LLMs) for automatic
evaluation of knowledge graph (KG) completion models. Historically, validating
information in KGs has been a challenging task, requiring large-scale human
annotation at prohibitive cost. With the emergence of general-purpose
generative AI and LLMs, it is now plausible that human-in-the-loop validation
could be replaced by a generative agent. We introduce a framework for
consistency and validation when using generative models to validate knowledge
graphs. Our framework is based upon recent open-source developments for
structural and semantic validation of LLM outputs, and upon flexible approaches
to fact checking and verification, supported by the capacity to reference
external knowledge sources of any kind. The design is easy to adapt and extend,
and can be used to verify any kind of graph-structured data through a
combination of model-intrinsic knowledge, user-supplied context, and agents
capable of external knowledge retrieval.

摘要：本研究探討使用大型語言模型 (LLM) 自動評估知識圖譜 (KG) 完成模型。歷來，驗證 KG 中的資訊是一項艱鉅的任務，需要大量的人工標註，成本高昂。隨著通用生成式 AI 和 LLM 的出現，現在有可能用生成式代理取代人工迴圈驗證。我們引入了一個架構，用於在使用生成式模型驗證知識圖譜時確保一致性和驗證。我們的架構基於最近的結構化和語義驗證 LLM 輸出的開源開發，以及靈活的事實查核和驗證方法，並由參考任何種類的外部知識來源的能力所支援。此設計易於調整和擴充，且可透過結合模型內在知識、使用者提供的內容以及能夠擷取外部知識的代理，來驗證任何類型的圖形結構化資料。

##### **Graph Machine Learning in the Era of Large Language Models (LLMs)**
2404.14928v1 by Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li

Graphs play an important role in representing complex relationships in
various domains like social networks, knowledge graphs, and molecular
discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have
emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the
representation and processing of graph structures. Recently, LLMs have
demonstrated unprecedented capabilities in language tasks and are widely
adopted in a variety of applications such as computer vision and recommender
systems. This remarkable success has also attracted interest in applying LLMs
to the graph domain. Increasing efforts have been made to explore the potential
of LLMs in advancing Graph ML's generalization, transferability, and few-shot
learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in
reliable factual knowledge, which can be utilized to enhance the reasoning
capabilities of LLMs and potentially alleviate their limitations such as
hallucinations and the lack of explainability. Given the rapid progress of this
research direction, a systematic review summarizing the latest advancements for
Graph ML in the era of LLMs is necessary to provide an in-depth understanding
to researchers and practitioners. Therefore, in this survey, we first review
the recent developments in Graph ML. We then explore how LLMs can be utilized
to enhance the quality of graph features, alleviate the reliance on labeled
data, and address challenges such as graph heterogeneity and
out-of-distribution (OOD) generalization. Afterward, we delve into how graphs
can enhance LLMs, highlighting their abilities to enhance LLM pre-training and
inference. Furthermore, we investigate various applications and discuss the
potential future directions in this promising field.

摘要：圖形在表示各種領域中複雜的關係中扮演著重要的角色，例如社交網路、知識圖譜和分子發現。隨著深度學習的出現，圖神經網路 (GNN) 已成為圖形機器學習 (Graph ML) 的基石，促進圖形結構的表示和處理。最近，大型語言模型 (LLM) 在語言任務中展現了前所未有的能力，並廣泛應用於各種應用程式中，例如電腦視覺和推薦系統。這項非凡的成功也引起了將 LLM 應用於圖形領域的興趣。人們正投入越來越多的精力來探索 LLM 在提升圖形 ML 的概括化、可轉移性和少量學習能力方面的潛力。同時，圖形（尤其是知識圖譜）富含可靠的事實知識，可用於增強 LLM 的推理能力，並有可能減輕其幻覺和缺乏可解釋性等限制。鑑於此研究方向的快速進展，有必要對 LLM 時代圖形 ML 的最新進展進行系統性回顧，以向研究人員和從業者提供深入的理解。因此，在本調查中，我們首先回顧圖形 ML 的最新發展。然後，我們探討如何利用 LLM 來提升圖形特徵的品質、減輕對標籤資料的依賴，並解決圖形異質性和分布外 (OOD) 概括化等挑戰。之後，我們深入探討圖形如何增強 LLM，強調它們增強 LLM 預訓練和推理的能力。此外，我們探討各種應用程式，並討論這個有前途領域的潛在未來方向。

##### **A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications**
2404.14809v1 by Wenbo Shang, Xin Huang

A graph is a fundamental data model to represent various entities and their
complex relationships in society and nature, such as social networks,
transportation networks, financial networks, and biomedical systems. Recently,
large language models (LLMs) have showcased a strong generalization ability to
handle various NLP and multi-mode tasks to answer users' arbitrary questions
and specific-domain content generation. Compared with graph learning models,
LLMs enjoy superior advantages in addressing the challenges of generalizing
graph tasks by eliminating the need for training graph learning models and
reducing the cost of manual annotation. In this survey, we conduct a
comprehensive investigation of existing LLM studies on graph data, which
summarizes the relevant graph analytics tasks solved by advanced LLM models and
points out the existing remaining challenges and future directions.
Specifically, we study the key problems of LLM-based generative graph analytics
(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),
LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based
applications. LLM-GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge graph (KG) based
augmented retrieval, while LLM-GIL focuses on learning and reasoning over
graphs, including graph learning, graph-formed reasoning and graph
representation. We summarize the useful prompts incorporated into LLM to handle
different graph downstream tasks. Moreover, we give a summary of LLM model
evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM
models. We also explore open problems and future directions in this exciting
interdisciplinary research area of LLMs and graph analytics.

摘要：圖形是一種基本資料模型，用於表示社會和自然界中各種實體及其複雜關係，例如社交網路、運輸網路、金融網路和生物醫學系統。最近，大型語言模型 (LLM) 展示了強大的概括能力，可以處理各種 NLP 和多模式任務，以回答使用者的任意問題和特定領域的內容生成。與圖形學習模型相比，LLM 在解決圖形任務的概括挑戰方面享有優越的優勢，因為它消除了訓練圖形學習模型的需要並降低了人工註解的成本。在這項調查中，我們對現有的 LLM 在圖形資料上的研究進行了全面的調查，總結了由先進 LLM 模型解決的相關圖形分析任務，並指出了現有的剩餘挑戰和未來方向。具體來說，我們研究了基於 LLM 的生成圖形分析 (LLM-GGA) 的關鍵問題，分為三類：基於 LLM 的圖形查詢處理 (LLM-GQP)、基於 LLM 的圖形推理和學習 (LLM-GIL) 以及基於圖形 LLM 的應用程式。LLM-GQP 專注於圖形分析技術和 LLM 提示的整合，包括圖形理解和基於知識圖譜 (KG) 的擴增檢索，而 LLM-GIL 專注於在圖形上學習和推理，包括圖形學習、圖形形成推理和圖形表示。我們總結了整合到 LLM 中以處理不同圖形下游任務的有用提示。此外，我們總結了 LLM 模型評估、基準資料集/任務以及 LLM 模型的深入優缺點分析。我們還探討了 LLM 和圖形分析這個令人興奮的跨學科研究領域中的開放問題和未來方向。

##### **RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model**
2404.14786v2 by Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenweu Zhu

In the field of Artificial Intelligence for Information Technology
Operations, causal discovery is pivotal for operation and maintenance of graph
construction, facilitating downstream industrial tasks such as root cause
analysis. Temporal causal discovery, as an emerging method, aims to identify
temporal causal relationships between variables directly from observations by
utilizing interventional data. However, existing methods mainly focus on
synthetic datasets with heavy reliance on intervention targets and ignore the
textual information hidden in real-world systems, failing to conduct causal
discovery for real industrial scenarios. To tackle this problem, in this paper
we propose to investigate temporal causal discovery in industrial scenarios,
which faces two critical challenges: 1) how to discover causal relationships
without the interventional targets that are costly to obtain in practice, and
2) how to discover causal relations via leveraging the textual information in
systems which can be complex yet abundant in industrial contexts. To address
these challenges, we propose the RealTCD framework, which is able to leverage
domain knowledge to discover temporal causal relationships without
interventional targets. Specifically, we first develop a score-based temporal
causal discovery method capable of discovering causal relations for root cause
analysis without relying on interventional targets through strategic masking
and regularization. Furthermore, by employing Large Language Models (LLMs) to
handle texts and integrate domain knowledge, we introduce LLM-guided
meta-initialization to extract the meta-knowledge from textual information
hidden in systems to boost the quality of discovery. We conduct extensive
experiments on simulation and real-world datasets to show the superiority of
our proposed RealTCD framework over existing baselines in discovering temporal
causal structures.

摘要：<paragraph>在信息技術運算的人工智慧領域中，因果發現對於圖形建構的運作和維護至關重要，有助於促進下游產業任務，例如根本原因分析。時序因果發現作為一種新興方法，旨在透過利用介入數據，直接從觀察中找出變數之間的時序因果關係。然而，現有方法主要集中於合成資料集，過度依賴介入目標，且忽略了現實世界系統中隱藏的文字資訊，無法對真實產業場景進行因果發現。為了解決這個問題，我們在本文中提出探討產業場景中的時序因果發現，這面臨兩項關鍵挑戰：1) 如何在沒有介入目標的情況下發現因果關係，而介入目標在實務上取得成本很高，以及 2) 如何透過利用系統中的文字資訊來發現因果關係，而這些資訊在產業情境中可能很複雜，但也很豐富。為了應對這些挑戰，我們提出 RealTCD 架構，它能夠利用領域知識來發現時序因果關係，而不需要介入目標。具體來說，我們首先開發一種基於分數的時序因果發現方法，它能夠透過策略性遮罩和正則化來發現因果關係，以進行根本原因分析，而不需要依賴介入目標。此外，透過採用大型語言模型 (LLM) 來處理文字並整合領域知識，我們引入了 LLM 引導的元初始化，以從系統中隱藏的文字資訊中萃取元知識，以提升發現的品質。我們在模擬和真實世界資料集上進行了廣泛的實驗，以展示我們提出的 RealTCD 架構在發現時序因果結構方面優於現有的基準。</paragraph>

##### **Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models**
2404.14772v1 by Chris Samarinas, Pracha Promthaw, Atharva Nijasure, Hansi Zeng, Julian Killingback, Hamed Zamani

This paper explores SynTOD, a new synthetic data generation approach for
developing end-to-end Task-Oriented Dialogue (TOD) Systems capable of handling
complex tasks such as intent classification, slot filling, conversational
question-answering, and retrieval-augmented response generation, without
relying on crowdsourcing or real-world data. SynTOD utilizes a state transition
graph to define the desired behavior of a TOD system and generates diverse,
structured conversations through random walks and response simulation using
large language models (LLMs). In our experiments, using graph-guided response
simulations leads to significant improvements in intent classification, slot
filling and response relevance compared to naive single-prompt simulated
conversations. We also investigate the end-to-end TOD effectiveness of
different base and instruction-tuned LLMs, with and without the constructed
synthetic conversations. Finally, we explore how various LLMs can evaluate
responses in a TOD system and how well they are correlated with human
judgments. Our findings pave the path towards quick development and evaluation
of domain-specific TOD systems. We release our datasets, models, and code for
research purposes.

摘要：這篇論文探討 SynTOD，一種新的合成資料產生方式，用於開發端對端任務導向對話 (TOD) 系統，此系統能夠處理複雜的任務，例如意圖分類、插槽填補、對話式問答和檢索增強回應產生，而無需依賴群眾外包或真實世界資料。SynTOD 利用狀態轉換圖形來定義 TOD 系統的所需行為，並透過使用大型語言模型 (LLM) 進行隨機漫步和回應模擬來產生多樣化、結構化的對話。在我們的實驗中，使用圖形引導的回應模擬會大幅改善意圖分類、插槽填補和回應相關性，相較於單純提示模擬的對話。我們也探討了不同基礎和指令調整的 LLM 的端對端 TOD 效能，無論是否使用建構的合成對話。最後，我們探討各種 LLM 如何在 TOD 系統中評估回應，以及它們與人類判斷的相關程度。我們的研究結果為特定領域 TOD 系統的快速開發和評估鋪路。我們釋出我們的資料集、模型和程式碼，供研究用途。

##### **Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering**
2404.14741v1 by Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Kang Liu, Jun Zhao

To address the issue of insufficient knowledge and the tendency to generate
hallucination in Large Language Models (LLMs), numerous studies have endeavored
to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are
evaluated on conventional Knowledge Graph Question Answering (KGQA) with
complete KGs, where the factual triples involved in each question are entirely
covered by the given KG. In this situation, LLM mainly acts as an agent to find
answer entities by exploring the KG, rather than effectively integrating
internal and external knowledge sources. However, in real-world scenarios, KGs
are often incomplete to cover all the knowledge required to answer questions.
To simulate real-world scenarios and evaluate the ability of LLMs to integrate
internal and external knowledge, in this paper, we propose leveraging LLMs for
QA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include
all the factual triples involved in each question. To handle IKGQA, we propose
a training-free method called Generate-on-Graph (GoG) that can generate new
factual triples while exploring on KGs. Specifically, we propose a
selecting-generating-answering framework, which not only treat the LLM as an
agent to explore on KGs, but also treat it as a KG to generate new facts based
on the explored subgraph and its inherent knowledge. Experimental results on
two datasets demonstrate that our GoG can solve IKGQA to a certain extent,
while almost all previous methods cannot perform well on IKGQA.

摘要：<paragraph>為了解決大型語言模型 (LLM) 知識不足和傾向產生幻覺的問題，許多研究致力於將 LLM 與知識圖譜 (KG) 整合。然而，所有這些方法都是根據具有完整 KG 的傳統知識圖譜問答 (KGQA) 進行評估，其中每個問題所涉及的事實三元組完全由給定的 KG 涵蓋。在這種情況下，LLM 主要充當代理，通過探索 KG 來尋找答案實體，而不是有效整合內部和外部知識來源。然而，在現實世界場景中，KG 通常不完整，無法涵蓋回答問題所需的所有知識。為了模擬現實世界場景並評估 LLM 整合內部和外部知識的能力，在本文中，我們提出利用 LLM 在不完整知識圖譜 (IKGQA) 下進行問答，其中給定的 KG 不包括每個問題所涉及的所有事實三元組。為了處理 IKGQA，我們提出了一種稱為生成圖形 (GoG) 的無訓練方法，它可以在探索 KG 時生成新的事實三元組。具體來說，我們提出了一個選擇生成答案框架，它不僅將 LLM 視為探索 KG 的代理，還將其視為一個 KG，根據探索的子圖及其固有知識生成新事實。在兩個數據集上的實驗結果表明，我們的 GoG 可以在一定程度上解決 IKGQA，而幾乎所有以前的方法在 IKGQA 上都無法很好地執行。</paragraph>

##### **Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph**
2404.14372v1 by Xiaochen Kev Gao, Feng Yao, Kewen Zhao, Beilei He, Animesh Kumar, Vish Krishnan, Jingbo Shang

Model scaling is becoming the default choice for many language tasks due to
the success of large language models (LLMs). However, it can fall short in
specific scenarios where simple customized methods excel. In this paper, we
delve into the patent approval pre-diction task and unveil that simple
domain-specific graph methods outperform enlarging the model, using the
intrinsic dependencies within the patent data. Specifically, we first extend
the embedding-based state-of-the-art (SOTA) by scaling up its backbone model
with various sizes of open-source LLMs, then explore prompt-based methods to
harness proprietary LLMs' potential, but find the best results close to random
guessing, underlining the ineffectiveness of model scaling-up. Hence, we
propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous
patent data analyses, capturing the inherent dependencies across segments of
the patent text. As it is model-agnostic, we apply cost-effective graph models
to our FLAN Graph to obtain representations for approval prediction. Extensive
experiments and detailed analyses prove that incorporating FLAN Graph via
various graph models consistently outperforms all LLM baselines significantly.
We hope that our observations and analyses in this paper can bring more
attention to this challenging task and prompt further research into the
limitations of LLMs. Our source code and dataset can be obtained from
http://github.com/ShangDataLab/FLAN-Graph.

摘要：<paragraph>由於大型語言模型 (LLM) 的成功，模型擴充正成為許多語言任務的預設選擇。然而，在簡單自訂方法表現優異的特定情境中，它可能會有所不足。在本文中，我們深入探討專利核准預測任務，並揭示出簡單的特定領域圖形方法優於擴充模型，利用專利資料中的內在依賴性。具體來說，我們首先透過使用各種規模的開源 LLM 擴充其主幹模型，擴展基於嵌入的現有技術 (SOTA)，然後探索基於提示的方法，以利用專有 LLM 的潛力，但發現最佳結果接近隨機猜測，強調了模型擴充的無效性。因此，我們透過細緻的專利資料分析，提出了一個新穎的細粒度主張依賴 (FLAN) 圖形，捕捉專利文字片段之間的內在依賴性。由於它與模型無關，我們將經濟高效的圖形模型應用於我們的 FLAN 圖形，以取得核准預測的表示。廣泛的實驗和詳細的分析證明，透過各種圖形模型整合 FLAN 圖形，持續顯著優於所有 LLM 基準。我們希望本文中的觀察和分析能為這個具有挑戰性的任務帶來更多關注，並促使進一步研究 LLM 的限制。我們的原始程式碼和資料集可從 http://github.com/ShangDataLab/FLAN-Graph 取得。</paragraph>

##### **LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots**
2404.14285v1 by Dongge Han, Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Peter Bell, Amos Storkey

Large language models (LLMs) have shown significant potential for robotics
applications, particularly task planning, by harnessing their language
comprehension and text generation capabilities. However, in applications such
as household robotics, a critical gap remains in the personalization of these
models to individual user preferences. We introduce LLM-Personalize, a novel
framework with an optimization pipeline designed to personalize LLM planners
for household robotics. Our LLM-Personalize framework features an LLM planner
that performs iterative planning in multi-room, partially-observable household
scenarios, making use of a scene graph constructed with local observations. The
generated plan consists of a sequence of high-level actions which are
subsequently executed by a controller. Central to our approach is the
optimization pipeline, which combines imitation learning and iterative
self-training to personalize the LLM planner. In particular, the imitation
learning phase performs initial LLM alignment from demonstrations, and
bootstraps the model to facilitate effective iterative self-training, which
further explores and aligns the model to user preferences. We evaluate
LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark
for household rearrangements, and show that LLM-Personalize achieves more than
a 30 percent increase in success rate over existing LLM planners, showcasing
significantly improved alignment with human preferences. Project page:
https://donggehan.github.io/projectllmpersonalize/.

摘要：大型語言模型 (LLM) 已展現出在機器人應用中，特別是在任務規劃中，利用其語言理解和文字生成能力的巨大潛力。然而，在家庭機器人等應用中，這些模型無法針對個別使用者的偏好進行個人化，這仍是一個嚴重的問題。我們介紹 LLM-Personalize，一個新穎的架構，其中包含一個最佳化管道，旨在針對家庭機器人個人化 LLM 規劃器。我們的 LLM-Personalize 架構具備一個 LLM 規劃器，該規劃器在多房間、部分可觀察的家庭場景中執行反覆規劃，利用由局部觀察所建構的場景圖。產生的計畫包含一系列高級動作，隨後由控制器執行。我們的做法核心是最佳化管道，它結合了模仿學習和反覆自我訓練，以個人化 LLM 規劃器。特別是，模仿學習階段從示範中執行初始 LLM 對齊，並引導模型以利於有效反覆自我訓練，進一步探索並將模型與使用者偏好對齊。我們在 Housekeep 上評估 LLM-Personalize，這是一個具有挑戰性的模擬真實世界 3D 家庭重新安排基準，並顯示 LLM-Personalize 在成功率上比現有的 LLM 規劃器提高了 30% 以上，展示出與人類偏好的顯著改善對齊。專案頁面：https://donggehan.github.io/projectllmpersonalize/。

##### **Context-Enhanced Language Models for Generating Multi-Paper Citations**
2404.13865v1 by Avinash Anand, Kritarth Prasad, Ujjwal Goel, Mohit Gupta, Naman Lal, Astha Verma, Rajiv Ratn Shah

Citation text plays a pivotal role in elucidating the connection between
scientific documents, demanding an in-depth comprehension of the cited paper.
Constructing citations is often time-consuming, requiring researchers to delve
into extensive literature and grapple with articulating relevant content. To
address this challenge, the field of citation text generation (CTG) has
emerged. However, while earlier methods have primarily centered on creating
single-sentence citations, practical scenarios frequently necessitate citing
multiple papers within a single paragraph. To bridge this gap, we propose a
method that leverages Large Language Models (LLMs) to generate multi-citation
sentences. Our approach involves a single source paper and a collection of
target papers, culminating in a coherent paragraph containing multi-sentence
citation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,
composed of English-language academic research papers in Computer Science,
showcasing multiple citation instances. In our experiments, we evaluate three
LLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this
endeavor. Additionally, we exhibit enhanced performance by integrating
knowledge graphs from target papers into the prompts for generating citation
text. This research underscores the potential of harnessing LLMs for citation
generation, opening a compelling avenue for exploring the intricate connections
between scientific documents.

摘要：引文文本在阐明科学文献之间的联系中扮演着关键角色，需要对被引论文有深入的理解。构建引文通常很耗时，需要研究人员深入研究大量文献并努力阐明相关内容。为了应对这一挑战，引文文本生成 (CTG) 领域应运而生。然而，虽然早期的研究方法主要集中在创建单句引文上，但实际情况通常需要在单个段落中引用多篇论文。为了弥合这一差距，我们提出了一种利用大型语言模型 (LLM) 来生成多引文句子的方法。我们的方法涉及一篇单一来源论文和一系列目标论文，最终形成一个包含多句引文文本的连贯段落。此外，我们引入了一个名为 MCG-S2ORC 的精选数据集，该数据集由计算机科学领域的英语学术研究论文组成，展示了多个引文实例。在我们的实验中，我们评估了三个 LLM LLaMA、Alpaca 和 Vicuna，以确定最适合这项工作的模型。此外，我们通过将目标论文的知识图谱集成到生成引文文本的提示中来展示增强的性能。这项研究强调了利用 LLM 进行引文生成的潜力，为探索科学文献之间的复杂联系开辟了一条引人注目的途径。

##### **Test-Time Training on Graphs with Large Language Models (LLMs)**
2404.13571v1 by Jiaxin Zhang, Yiqi Wang, Xihong Yang, Siwei Wang, Yu Feng, Yu Shi, Ruicaho Ren, En Zhu, Xinwang Liu

Graph Neural Networks have demonstrated great success in various fields of
multimedia. However, the distribution shift between the training and test data
challenges the effectiveness of GNNs. To mitigate this challenge, Test-Time
Training (TTT) has been proposed as a promising approach. Traditional TTT
methods require a demanding unsupervised training strategy to capture the
information from test to benefit the main task. Inspired by the great
annotation ability of Large Language Models (LLMs) on Text-Attributed Graphs
(TAGs), we propose to enhance the test-time training on graphs with LLMs as
annotators. In this paper, we design a novel Test-Time Training pipeline,
LLMTTT, which conducts the test-time adaptation under the annotations by LLMs
on a carefully-selected node set. Specifically, LLMTTT introduces a hybrid
active node selection strategy that considers not only node diversity and
representativeness, but also prediction signals from the pre-trained model.
Given annotations from LLMs, a two-stage training strategy is designed to
tailor the test-time model with the limited and noisy labels. A theoretical
analysis ensures the validity of our method and extensive experiments
demonstrate that the proposed LLMTTT can achieve a significant performance
improvement compared to existing Out-of-Distribution (OOD) generalization
methods.

摘要：圖神經網路已在多媒體的各個領域中展現出極大的成功。然而，訓練資料和測試資料之間的分布轉移對 GNN 的效能構成挑戰。為了減輕此項挑戰，測試時間訓練 (TTT) 已被提出作為一種有前景的方法。傳統的 TTT 方法需要嚴苛的無監督訓練策略，以擷取來自測試的資訊，以利於主要任務。受到大型語言模型 (LLM) 在文字屬性圖 (TAG) 上的出色註解能力啟發，我們提出使用 LLM 作為註解者來增強圖形上的測試時間訓練。在本文中，我們設計了一個新穎的測試時間訓練管線 LLMTTT，它在 LLM 對精心挑選的節點集合進行註解的情況下執行測試時間適應。具體來說，LLMTTT 採用一種混合主動節點選擇策略，它不僅考慮節點的多樣性和代表性，還考慮預訓練模型的預測訊號。根據 LLM 的註解，設計了一個兩階段訓練策略，以使用有限且有雜訊的標籤來調整測試時間模型。理論分析確保了我們方法的有效性，而廣泛的實驗證明，與現有的非分佈 (OOD) 概化方法相比，所提出的 LLMTTT 能夠實現顯著的效能提升。

##### **Evaluation of Machine Translation Based on Semantic Dependencies and Keywords**
2404.14443v1 by Kewei Yuan, Qiurong Zhao, Yang Xu, Xiao Zhang, Huansheng Ning

In view of the fact that most of the existing machine translation evaluation
algorithms only consider the lexical and syntactic information, but ignore the
deep semantic information contained in the sentence, this paper proposes a
computational method for evaluating the semantic correctness of machine
translations based on reference translations and incorporating semantic
dependencies and sentence keyword information. Use the language technology
platform developed by the Social Computing and Information Retrieval Research
Center of Harbin Institute of Technology to conduct semantic dependency
analysis and keyword analysis on sentences, and obtain semantic dependency
graphs, keywords, and weight information corresponding to keywords. It includes
all word information with semantic dependencies in the sentence and keyword
information that affects semantic information. Construct semantic association
pairs including word and dependency multi-features. The key semantics of the
sentence cannot be highlighted in the semantic information extracted through
semantic dependence, resulting in vague semantics analysis. Therefore, the
sentence keyword information is also included in the scope of machine
translation semantic evaluation. To achieve a comprehensive and in-depth
evaluation of the semantic correctness of sentences, the experimental results
show that the accuracy of the evaluation algorithm has been improved compared
with similar methods, and it can more accurately measure the semantic
correctness of machine translation.

摘要：由於現有機器翻譯評估演算法大多僅考慮詞彙和句法資訊，忽略句子中包含的深層語意資訊，本文提出了一種基於參考翻譯，並融合語意依存和句子關鍵字資訊的機器翻譯語意正確性評估計算方法。利用哈爾濱工業大學社會計算與資訊檢索研究中心研發的語言技術平臺，對句子進行語意依存分析和關鍵字分析，得到語意依存圖、關鍵字及關鍵字對應的權重資訊。包含了句子中所有具備語意依存關係的詞彙資訊和影響語意資訊的關鍵字資訊。構建包含詞彙和依存關係多特徵的語意關聯對。通過語意依存提取的語意資訊無法突出句子的關鍵語意，導致語意分析模糊。因此，將句子關鍵字資訊也納入機器翻譯語意評估的範圍內。為了對句子的語意正確性進行全面深入的評估，實驗結果表明，與同類方法相比，該評估演算法的準確率有所提升，能更準確地衡量機器翻譯的語意正確性。

