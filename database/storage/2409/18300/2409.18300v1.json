{"2409.18300": {"publish_time": "2024-09-26", "title": "SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining", "paper_summary": "We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial\nfootage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human\nobject knowledge throughout the pretraining process to enhance UAV video\npretraining efficiency and downstream action recognition performance. This is\nin contrast to prior works that primarily incorporate object information during\nthe fine-tuning stage. Specifically, we first propose a novel object-aware\nmasking strategy designed to retain the visibility of certain patches related\nto objects throughout the pretraining phase. Second, we introduce an\nobject-aware loss function that utilizes object information to adjust the\nreconstruction loss, preventing bias towards less informative background\npatches. In practice, SOAR with a vanilla ViT backbone, outperforms best UAV\naction recognition models, recording a 9.7% and 21.4% boost in top-1 accuracy\non the NEC-Drone and UAV-Human datasets, while delivering an inference speed of\n18.7ms per video, making it 2x to 5x faster. Additionally, SOAR obtains\ncomparable accuracy to prior self-supervised learning (SSL) methods while\nrequiring 87.5% less pretraining time and 25% less memory usage", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86 SOAR\uff0c\u4e00\u7a2e\u91dd\u5c0d\u7531\u7121\u4eba\u6a5f (UAV) \u6355\u6349\u7684\u7a7a\u4e2d\u5f71\u50cf\u7684\u5168\u65b0\u81ea\u76e3\u7763\u9810\u8a13\u7df4\u6f14\u7b97\u6cd5\u3002\u6211\u5011\u5728\u6574\u500b\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u7d0d\u5165\u4e86\u4eba\u985e\u7269\u9ad4\u77e5\u8b58\uff0c\u4ee5\u63d0\u5347\u7121\u4eba\u6a5f\u5f71\u7247\u9810\u8a13\u7df4\u6548\u7387\u548c\u4e0b\u6e38\u52d5\u4f5c\u8fa8\u8b58\u6548\u80fd\u3002\u9019\u8207\u5148\u524d\u4e3b\u8981\u5728\u5fae\u8abf\u968e\u6bb5\u7d0d\u5165\u7269\u9ad4\u8cc7\u8a0a\u7684\u4f5c\u54c1\u5f62\u6210\u5c0d\u6bd4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u7269\u9ad4\u611f\u77e5\u906e\u7f69\u7b56\u7565\uff0c\u65e8\u5728\u4fdd\u7559\u8207\u7269\u9ad4\u76f8\u95dc\u7684\u7279\u5b9a\u5340\u584a\u5728\u6574\u500b\u9810\u8a13\u7df4\u968e\u6bb5\u7684\u53ef\u8996\u6027\u3002\u5176\u6b21\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5229\u7528\u7269\u9ad4\u8cc7\u8a0a\u8abf\u6574\u91cd\u5efa\u640d\u5931\u7684\u7269\u9ad4\u611f\u77e5\u640d\u5931\u51fd\u6578\uff0c\u9632\u6b62\u504f\u5411\u65bc\u8cc7\u8a0a\u91cf\u8f03\u5c11\u7684\u80cc\u666f\u5340\u584a\u3002\u5728\u5be6\u52d9\u4e2d\uff0c\u63a1\u7528\u9999\u8349 ViT \u4e3b\u5e79\u7684 SOAR \u512a\u65bc\u6700\u4f73\u7684\u7121\u4eba\u6a5f\u52d5\u4f5c\u8fa8\u8b58\u6a21\u578b\uff0c\u5728 NEC-Drone \u548c UAV-Human \u8cc7\u6599\u96c6\u4e0a\u5275\u4e0b\u6700\u9ad8 1% \u7cbe\u78ba\u5ea6\u63d0\u5347 9.7% \u548c 21.4%\uff0c\u540c\u6642\u63d0\u4f9b\u6bcf\u90e8\u5f71\u7247 18.7ms \u7684\u63a8\u8ad6\u901f\u5ea6\uff0c\u4f7f\u5176\u5feb 2 \u5230 5 \u500d\u3002\u6b64\u5916\uff0cSOAR \u5728\u9700\u8981\u5c11 87.5% \u9810\u8a13\u7df4\u6642\u9593\u548c\u5c11 25% \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7684\u60c5\u6cc1\u4e0b\uff0c\u7372\u5f97\u8207\u5148\u524d\u7684\u81ea\u76e3\u7763\u5b78\u7fd2 (SSL) \u65b9\u6cd5\u76f8\u7576\u7684\u6e96\u78ba\u5ea6", "author": "Ruiqi Xian et.al.", "authors": "Ruiqi Xian, Xiyang Wu, Tianrui Guan, Xijun Wang, Boqing Gong, Dinesh Manocha", "id": "2409.18300v1", "paper_url": "http://arxiv.org/abs/2409.18300v1", "repo": "null"}}