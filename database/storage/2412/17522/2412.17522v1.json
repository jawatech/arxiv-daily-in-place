{"2412.17522": {"publish_time": "2024-12-23", "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak", "paper_summary": "Large Language Models (LLMs) are susceptible to generating harmful content\nwhen prompted with carefully crafted inputs, a vulnerability known as LLM\njailbreaking. As LLMs become more powerful, studying jailbreak methods is\ncritical to enhancing security and aligning models with human values.\nTraditionally, jailbreak techniques have relied on suffix addition or prompt\ntemplates, but these methods suffer from limited attack diversity. This paper\nintroduces DiffusionAttacker, an end-to-end generative approach for jailbreak\nrewriting inspired by diffusion models. Our method employs a\nsequence-to-sequence (seq2seq) text diffusion model as a generator,\nconditioning on the original prompt and guiding the denoising process with a\nnovel attack loss. Unlike previous approaches that use autoregressive LLMs to\ngenerate jailbreak prompts, which limit the modification of already generated\ntokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq\ndiffusion model, allowing more flexible token modifications. This approach\npreserves the semantic content of the original prompt while producing harmful\ncontent. Additionally, we leverage the Gumbel-Softmax technique to make the\nsampling process from the diffusion model's output distribution differentiable,\neliminating the need for iterative token search. Extensive experiments on\nAdvbench and Harmbench demonstrate that DiffusionAttacker outperforms previous\nmethods across various evaluation metrics, including attack success rate (ASR),\nfluency, and diversity.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6536\u5230\u7cbe\u5fc3\u8a2d\u8a08\u7684\u8f38\u5165\u6642\u5bb9\u6613\u7522\u751f\u6709\u5bb3\u5167\u5bb9\uff0c\u6b64\u6f0f\u6d1e\u7a31\u70ba LLM \u8d8a\u7344\u3002\u96a8\u8457 LLM \u8b8a\u5f97\u66f4\u5f37\u5927\uff0c\u7814\u7a76\u8d8a\u7344\u65b9\u6cd5\u5c0d\u65bc\u589e\u5f37\u5b89\u5168\u6027\u4e26\u4f7f\u6a21\u578b\u8207\u4eba\u985e\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\u81f3\u95dc\u91cd\u8981\u3002\u50b3\u7d71\u4e0a\uff0c\u8d8a\u7344\u6280\u8853\u4f9d\u8cf4\u65bc\u5f8c\u7db4\u6dfb\u52a0\u6216\u63d0\u793a\u7bc4\u672c\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u7684\u653b\u64ca\u591a\u6a23\u6027\u6709\u9650\u3002\u672c\u6587\u4ecb\u7d39\u4e86 DiffusionAttacker\uff0c\u4e00\u7a2e\u9748\u611f\u4f86\u81ea\u64f4\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u8d8a\u7344\u91cd\u5beb\u65b9\u6cd5\u3002\u6211\u5011\u7684\u6280\u8853\u63a1\u7528\u5e8f\u5217\u5230\u5e8f\u5217 (seq2seq) \u6587\u672c\u64f4\u6563\u6a21\u578b\u4f5c\u70ba\u751f\u6210\u5668\uff0c\u4ee5\u539f\u59cb\u63d0\u793a\u70ba\u689d\u4ef6\uff0c\u4e26\u4f7f\u7528\u65b0\u7a4e\u7684\u653b\u64ca\u640d\u5931\u4f86\u5f15\u5c0e\u53bb\u566a\u904e\u7a0b\u3002\u8207\u4f7f\u7528\u81ea\u8ff4\u6b78 LLM \u4f86\u7522\u751f\u8d8a\u7344\u63d0\u793a\u7684\u4e0d\u540c\uff0c\u9019\u6703\u9650\u5236\u5df2\u7d93\u7522\u751f\u7684\u6a19\u8a18\u7684\u4fee\u6539\u4e26\u9650\u5236\u91cd\u5beb\u7a7a\u9593\uff0cDiffusionAttacker \u4f7f\u7528 seq2seq \u64f4\u6563\u6a21\u578b\uff0c\u5141\u8a31\u66f4\u9748\u6d3b\u7684\u6a19\u8a18\u4fee\u6539\u3002\u9019\u7a2e\u65b9\u6cd5\u4fdd\u7559\u4e86\u539f\u59cb\u63d0\u793a\u7684\u8a9e\u7fa9\u5167\u5bb9\uff0c\u540c\u6642\u7522\u751f\u6709\u5bb3\u5167\u5bb9\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528 Gumbel-Softmax \u6280\u8853\u4f7f\u5f9e\u64f4\u6563\u6a21\u578b\u8f38\u51fa\u5206\u4f48\u4e2d\u63a1\u6a23\u7684\u904e\u7a0b\u53ef\u5fae\u5206\uff0c\u5f9e\u800c\u6d88\u9664\u4e86\u5c0d\u53cd\u8986\u6a19\u8a18\u641c\u7d22\u7684\u9700\u6c42\u3002\u5728 Advbench \u548c Harmbench \u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cDiffusionAttacker \u5728\u5404\u7a2e\u8a55\u4f30\u6307\u6a19\uff08\u5305\u62ec\u653b\u64ca\u6210\u529f\u7387 (ASR)\u3001\u6d41\u66a2\u5ea6\u548c\u591a\u6a23\u6027\uff09\u4e0a\u512a\u65bc\u4ee5\u524d\u7684\u65b9\u6cd5\u3002", "author": "Hao Wang et.al.", "authors": "Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha", "id": "2412.17522v1", "paper_url": "http://arxiv.org/abs/2412.17522v1", "repo": "null"}}