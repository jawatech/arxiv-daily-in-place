{"2407.17469": {"publish_time": "2024-07-24", "title": "I Could've Asked That: Reformulating Unanswerable Questions", "paper_summary": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.", "paper_summary_zh": "\u5728\u5f9e\u4e0d\u719f\u6089\u7684\u6587\u6a94\u4e2d\u5c0b\u6c42\u8cc7\u8a0a\u6642\uff0c\u4f7f\u7528\u8005\u7d93\u5e38\u6703\u63d0\u51fa\u6587\u6a94\u7121\u6cd5\u56de\u7b54\u7684\u554f\u984c\u3002\u96d6\u7136\u73fe\u6709\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u8b58\u5225\u9019\u4e9b\u7121\u6cd5\u56de\u7b54\u7684\u554f\u984c\uff0c\u4f46\u5b83\u5011\u4e26\u672a\u5354\u52a9\u4f7f\u7528\u8005\u91cd\u65b0\u8868\u8ff0\u554f\u984c\uff0c\u56e0\u6b64\u964d\u4f4e\u4e86\u5b83\u5011\u7684\u6574\u9ad4\u6548\u7528\u3002\u6211\u5011\u6574\u7406\u4e86 CouldAsk\uff0c\u9019\u662f\u4e00\u500b\u8a55\u4f30\u57fa\u6e96\uff0c\u7531\u73fe\u6709\u548c\u65b0\u7684\u8cc7\u6599\u96c6\u7d44\u6210\uff0c\u7528\u65bc\u57fa\u65bc\u6587\u6a94\u7684\u554f\u7b54\uff0c\u7279\u5225\u8a2d\u8a08\u7528\u65bc\u7814\u7a76\u91cd\u65b0\u8868\u8ff0\u7121\u6cd5\u56de\u7b54\u7684\u554f\u984c\u3002\u6211\u5011\u5728 CouldAsk \u4e0a\u8a55\u4f30\u4e86\u6700\u5148\u9032\u7684\u958b\u6e90\u548c\u5c08\u6709 LLM\u3002\u7d50\u679c\u8b49\u660e\u4e86\u9019\u4e9b\u6a21\u578b\u5728\u91cd\u65b0\u8868\u8ff0\u554f\u984c\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cGPT-4 \u548c Llama2-7B \u5206\u5225\u50c5\u5728 26% \u548c 12% \u7684\u6642\u9593\u5167\u6210\u529f\u91cd\u65b0\u8868\u8ff0\u554f\u984c\u3002\u932f\u8aa4\u5206\u6790\u986f\u793a\uff0c62% \u7684\u4e0d\u6210\u529f\u91cd\u65b0\u8868\u8ff0\u6e90\u65bc\u6a21\u578b\u50c5\u50c5\u91cd\u65b0\u8868\u8ff0\u554f\u984c\uff0c\u751a\u81f3\u7522\u751f\u76f8\u540c\u7684\u554f\u984c\u3002\u6211\u5011\u516c\u958b\u767c\u5e03\u57fa\u6e96\u548c\u4ee3\u78bc\u4ee5\u91cd\u73fe\u5be6\u9a57\u3002", "author": "Wenting Zhao et.al.", "authors": "Wenting Zhao, Ge Gao, Claire Cardie, Alexander M. Rush", "id": "2407.17469v1", "paper_url": "http://arxiv.org/abs/2407.17469v1", "repo": "https://github.com/wenting-zhao/couldask"}}