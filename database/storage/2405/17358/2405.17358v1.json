{"2405.17358": {"publish_time": "2024-05-27", "title": "Rethinking Transformers in Solving POMDPs", "paper_summary": "Sequential decision-making algorithms such as reinforcement learning (RL) in\nreal-world scenarios inevitably face environments with partial observability.\nThis paper scrutinizes the effectiveness of a popular architecture, namely\nTransformers, in Partially Observable Markov Decision Processes (POMDPs) and\nreveals its theoretical limitations. We establish that regular languages, which\nTransformers struggle to model, are reducible to POMDPs. This poses a\nsignificant challenge for Transformers in learning POMDP-specific inductive\nbiases, due to their lack of inherent recurrence found in other models like\nRNNs. This paper casts doubt on the prevalent belief in Transformers as\nsequence models for RL and proposes to introduce a point-wise recurrent\nstructure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited\nalternative for Partially Observable RL, with empirical results highlighting\nthe sub-optimal performance of the Transformer and considerable strength of\nLRU.", "paper_summary_zh": "\u5728\u73fe\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u8af8\u5982\u5f37\u5316\u5b78\u7fd2 (RL) \u7b49\u9806\u5e8f\u6c7a\u7b56\u6f14\u7b97\u6cd5\u4e0d\u53ef\u907f\u514d\u5730\u9762\u81e8\u5177\u6709\u90e8\u5206\u53ef\u89c0\u5bdf\u6027\u7684\u74b0\u5883\u3002\u672c\u6587\u5be9\u8996\u4e86\u5728\u90e8\u5206\u53ef\u89c0\u5bdf\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b (POMDP) \u4e2d\u4e00\u7a2e\u6d41\u884c\u7684\u67b6\u69cb\uff0c\u5373 Transformer \u7684\u6709\u6548\u6027\uff0c\u4e26\u63ed\u793a\u5176\u7406\u8ad6\u9650\u5236\u3002\u6211\u5011\u5efa\u7acb\u4e86 Transformer \u96e3\u4ee5\u5efa\u6a21\u7684\u6b63\u5247\u8a9e\u8a00\u53ef\u7c21\u5316\u70ba POMDP\u3002\u7531\u65bc Transformer \u7f3a\u4e4f\u5728\u5176\u4ed6\u6a21\u578b\uff08\u5982 RNN\uff09\u4e2d\u767c\u73fe\u7684\u56fa\u6709\u905e\u8ff4\uff0c\u56e0\u6b64\u9019\u5c0d Transformer \u5728\u5b78\u7fd2 POMDP \u7279\u5b9a\u7684\u6b78\u7d0d\u504f\u5dee\u65b9\u9762\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u672c\u6587\u5c0d Transformer \u4f5c\u70ba RL \u7684\u5e8f\u5217\u6a21\u578b\u666e\u904d\u5b58\u5728\u7684\u4fe1\u5ff5\u63d0\u51fa\u8cea\u7591\uff0c\u4e26\u5efa\u8b70\u5f15\u5165\u9ede\u72c0\u905e\u8ff4\u7d50\u69cb\u3002\u6df1\u5ea6\u7dda\u6027\u905e\u8ff4\u55ae\u5143 (LRU) \u6210\u70ba\u90e8\u5206\u53ef\u89c0\u5bdf RL \u7684\u5408\u9069\u66ff\u4ee3\u65b9\u6848\uff0c\u5be6\u8b49\u7d50\u679c\u7a81\u986f\u4e86 Transformer \u7684\u6b21\u512a\u6027\u80fd\u548c LRU \u7684\u986f\u8457\u512a\u52e2\u3002", "author": "Chenhao Lu et.al.", "authors": "Chenhao Lu, Ruizhe Shi, Yuyao Liu, Kaizhe Hu, Simon S. Du, Huazhe Xu", "id": "2405.17358v1", "paper_url": "http://arxiv.org/abs/2405.17358v1", "repo": "https://github.com/ctp314/tfporl"}}