{"2502.09940": {"publish_time": "2025-02-14", "title": "A Preliminary Exploration with GPT-4o Voice Mode", "paper_summary": "With the rise of multimodal large language models, GPT-4o stands out as a\npioneering model, driving us to evaluate its capabilities. This report assesses\nGPT-4o across various tasks to analyze its audio processing and reasoning\nabilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and\nmusic understanding, performing well in tasks like intent classification,\nspoken command classification, semantic and grammatical reasoning.,\nmultilingual speech recognition, and singing analysis. It also shows greater\nrobustness against hallucinations than other large audio-language models\n(LALMs). However, it struggles with tasks such as audio duration prediction and\ninstrument classification. Additionally, GPT-4o's safety mechanisms cause it to\ndecline tasks like speaker identification, age classification, MOS prediction,\nand audio deepfake detection. Notably, the model exhibits a significantly\ndifferent refusal rate when responding to speaker verification tasks on\ndifferent datasets. This is likely due to variations in the accompanying\ninstructions or the quality of the input audio, suggesting the sensitivity of\nits built-in safeguards. Finally, we acknowledge that model performance varies\nwith evaluation protocols. This report only serves as a preliminary exploration\nof the current state of LALMs.", "paper_summary_zh": "\u96a8\u8457\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u8208\u8d77\uff0cGPT-4o \u4f5c\u70ba\u958b\u5275\u6027\u7684\u6a21\u578b\u812b\u7a4e\u800c\u51fa\uff0c\u9a45\u4f7f\u6211\u5011\u8a55\u4f30\u5176\u80fd\u529b\u3002\u672c\u5831\u544a\u8a55\u4f30\u4e86 GPT-4o \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\uff0c\u4ee5\u5206\u6790\u5176\u97f3\u8a0a\u8655\u7406\u548c\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u767c\u73fe GPT-4o \u5728\u97f3\u8a0a\u3001\u8a9e\u97f3\u548c\u97f3\u6a02\u7406\u89e3\u65b9\u9762\u5c55\u73fe\u4e86\u5f37\u5927\u7684\u77e5\u8b58\uff0c\u5728\u8af8\u5982\u610f\u5716\u5206\u985e\u3001\u53e3\u8a9e\u6307\u4ee4\u5206\u985e\u3001\u8a9e\u7fa9\u548c\u8a9e\u6cd5\u63a8\u7406\u3001\u591a\u8a9e\u8a00\u8a9e\u97f3\u8b58\u5225\u548c\u6b4c\u5531\u5206\u6790\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u826f\u597d\u3002\u5b83\u9084\u6bd4\u5176\u4ed6\u5927\u578b\u97f3\u8a0a\u8a9e\u8a00\u6a21\u578b (LALM) \u5177\u6709\u66f4\u5f37\u7684\u6297\u5e7b\u89ba\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5728\u97f3\u8a0a\u6301\u7e8c\u6642\u9593\u9810\u6e2c\u548c\u6a02\u5668\u5206\u985e\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u4e0d\u4f73\u3002\u6b64\u5916\uff0cGPT-4o \u7684\u5b89\u5168\u6a5f\u5236\u5c0e\u81f4\u5176\u62d2\u7d55\u57f7\u884c\u8af8\u5982\u8aaa\u8a71\u8005\u8b58\u5225\u3001\u5e74\u9f61\u5206\u985e\u3001MOS \u9810\u6e2c\u548c\u97f3\u8a0a\u6df1\u5ea6\u507d\u9020\u6aa2\u6e2c\u7b49\u4efb\u52d9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8a72\u6a21\u578b\u5728\u4e0d\u540c\u8cc7\u6599\u96c6\u4e0a\u5c0d\u8aaa\u8a71\u8005\u9a57\u8b49\u4efb\u52d9\u505a\u51fa\u56de\u61c9\u6642\uff0c\u62d2\u7d55\u7387\u6709\u986f\u8457\u5dee\u7570\u3002\u9019\u53ef\u80fd\u662f\u7531\u65bc\u9644\u5e36\u8aaa\u660e\u6216\u8f38\u5165\u97f3\u8a0a\u54c1\u8cea\u7684\u5dee\u7570\u6240\u81f4\uff0c\u8868\u660e\u5176\u5167\u5efa\u9632\u8b77\u63aa\u65bd\u7684\u654f\u611f\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u627f\u8a8d\u6a21\u578b\u6548\u80fd\u6703\u96a8\u8457\u8a55\u4f30\u5354\u5b9a\u800c\u6709\u6240\u4e0d\u540c\u3002\u672c\u5831\u544a\u50c5\u4f5c\u70ba\u5c0d LALM \u76ee\u524d\u72c0\u614b\u7684\u521d\u6b65\u63a2\u8a0e\u3002", "author": "Yu-Xiang Lin et.al.", "authors": "Yu-Xiang Lin, Chih-Kai Yang, Wei-Chih Chen, Chen-An Li, Chien-yu Huang, Xuanjun Chen, Hung-yi Lee", "id": "2502.09940v1", "paper_url": "http://arxiv.org/abs/2502.09940v1", "repo": "null"}}