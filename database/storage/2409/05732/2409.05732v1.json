{"2409.05732": {"publish_time": "2024-09-09", "title": "Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach", "paper_summary": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future.", "paper_summary_zh": "\u958b\u653e\u539f\u59cb\u78bc\u3001\u591a\u8a9e\u8a00\u5927\u578b\u91ab\u5b78\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u70ba\u4e0d\u540c\u5730\u5340\u7684\u8a9e\u8a00\u591a\u5143\u5316\u4eba\u7fa4\u670d\u52d9\u7684\u6f5b\u529b\u3002\u5c07\u901a\u7528 LLM \u9069\u61c9\u65bc\u91ab\u7642\u4fdd\u5065\u901a\u5e38\u9700\u8981\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u4f46\u6b64\u65b9\u6cd5\u5728\u904b\u7b97\u4e0a\u5f88\u6602\u8cb4\uff0c\u6709\u6642\u4e5f\u4e0d\u5207\u5be6\u969b\u3002\u7531\u65bc\u6a21\u578b\u5728\u5404\u7a2e\u60c5\u6cc1\u4e0b\u7406\u89e3\u548c\u6709\u6548\u63a8\u7406\u6642\u7f3a\u4e4f\u66f4\u5ee3\u6cdb\u7684\u9818\u57df\u77e5\u8b58\uff0c\u56e0\u6b64\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u7684\u6307\u793a\u5fae\u8abf\u4e26\u4e0d\u80fd\u7e3d\u662f\u4fdd\u8b49\u6700\u4f73\u6548\u80fd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5169\u500b\u591a\u8a9e\u8a00\u6307\u793a\u5fae\u8abf\u8cc7\u6599\u96c6\uff0c\u5373 MMed-IFT \u548c MMed-IFT-MC\uff0c\u5176\u4e2d\u5305\u542b\u516d\u7a2e\u8a9e\u8a00\u4e2d\u8d85\u904e 20 \u842c\u500b\u9ad8\u54c1\u8cea\u7684\u91ab\u7642\u7bc4\u4f8b\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5169\u968e\u6bb5\u8a13\u7df4\u7bc4\u4f8b\uff1a\u7b2c\u4e00\u968e\u6bb5\u4f7f\u7528 MMed-IFT \u6ce8\u5165\u4e00\u822c\u91ab\u7642\u77e5\u8b58\uff0c\u800c\u7b2c\u4e8c\u968e\u6bb5\u4f7f\u7528 MMed-IFT-MC \u5fae\u8abf\u7279\u5b9a\u4efb\u52d9\u7684\u591a\u9078\u984c\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u82f1\u6587\u548c\u591a\u8a9e\u8a00\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7686\u53d6\u5f97\u5177\u7af6\u722d\u529b\u7684\u7d50\u679c\uff0c\u5728\u904b\u7b97\u6548\u7387\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u6211\u5011\u8a08\u756b\u5728\u672a\u4f86\u65bc \\url{https://github.com/SpassMed/Med-Llama3} \u516c\u958b\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u6b0a\u91cd\u3002", "author": "Meng Zhou et.al.", "authors": "Meng Zhou, Surajsinh Parmar, Anubhav Bhatti", "id": "2409.05732v1", "paper_url": "http://arxiv.org/abs/2409.05732v1", "repo": "null"}}