{"2406.06279": {"publish_time": "2024-06-10", "title": "Multi-Prompting Decoder Helps Better Language Understanding", "paper_summary": "Recent Pre-trained Language Models (PLMs) usually only provide users with the\ninference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt\nMaaS PLMs to downstream tasks without accessing their parameters and gradients,\nsome existing methods focus on the output-side adaptation of PLMs, viewing the\nPLM as an encoder and then optimizing a task-specific decoder for decoding the\noutput hidden states and class scores of the PLM. Despite the effectiveness of\nthese methods, they only use a single prompt to query PLMs for decoding,\nleading to a heavy reliance on the quality of the adopted prompt. In this\npaper, we propose a simple yet effective Multi-Prompting Decoder (MPD)\nframework for MaaS adaptation. The core idea is to query PLMs with multiple\ndifferent prompts for each sample, thereby obtaining multiple output hidden\nstates and class scores for subsequent decoding. Such multi-prompting decoding\nparadigm can simultaneously mitigate reliance on the quality of a single\nprompt, alleviate the issue of data scarcity under the few-shot setting, and\nprovide richer knowledge extracted from PLMs. Specifically, we propose two\ndecoding strategies: multi-prompting decoding with optimal transport for hidden\nstates and calibrated decoding for class scores. Extensive experiments\ndemonstrate that our method achieves new state-of-the-art results on multiple\nnatural language understanding datasets under the few-shot setting.", "paper_summary_zh": "\u8fd1\u671f\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u901a\u5e38\u53ea\u4e3a\u7528\u6237\u63d0\u4f9b\u63a8\u7406 API\uff0c\u5373\u65b0\u5174\u7684\u6a21\u578b\u5373\u670d\u52a1 (MaaS) \u8bbe\u7f6e\u3002\u4e3a\u4e86\u5728\u4e0d\u8bbf\u95ee\u5176\u53c2\u6570\u548c\u68af\u5ea6\u7684\u60c5\u51b5\u4e0b\u5c06 MaaS PLM \u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e00\u4e9b\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e PLM \u7684\u8f93\u51fa\u7aef\u9002\u5e94\uff0c\u5c06 PLM \u89c6\u4e3a\u7f16\u7801\u5668\uff0c\u7136\u540e\u4f18\u5316\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u89e3\u7801\u5668\u6765\u89e3\u7801 PLM \u7684\u8f93\u51fa\u9690\u85cf\u72b6\u6001\u548c\u7c7b\u5206\u6570\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u5b83\u4eec\u53ea\u4f7f\u7528\u4e00\u4e2a\u63d0\u793a\u6765\u67e5\u8be2 PLM \u8fdb\u884c\u89e3\u7801\uff0c\u5bfc\u81f4\u4e25\u91cd\u4f9d\u8d56\u6240\u91c7\u7528\u63d0\u793a\u7684\u8d28\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u7528\u4e8e MaaS \u9002\u5e94\u7684\u591a\u63d0\u793a\u89e3\u7801\u5668 (MPD) \u6846\u67b6\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u4f7f\u7528\u591a\u4e2a\u4e0d\u540c\u7684\u63d0\u793a\u4e3a\u6bcf\u4e2a\u6837\u672c\u67e5\u8be2 PLM\uff0c\u4ece\u800c\u83b7\u5f97\u591a\u4e2a\u8f93\u51fa\u9690\u85cf\u72b6\u6001\u548c\u7c7b\u5206\u6570\u4ee5\u8fdb\u884c\u540e\u7eed\u89e3\u7801\u3002\u8fd9\u79cd\u591a\u63d0\u793a\u89e3\u7801\u8303\u4f8b\u53ef\u4ee5\u540c\u65f6\u51cf\u8f7b\u5bf9\u5355\u4e2a\u63d0\u793a\u8d28\u91cf\u7684\u4f9d\u8d56\uff0c\u7f13\u89e3\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4ece PLM \u4e2d\u63d0\u53d6\u7684\u66f4\u4e30\u5bcc\u7684\u77e5\u8bc6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u89e3\u7801\u7b56\u7565\uff1a\u7528\u4e8e\u9690\u85cf\u72b6\u6001\u7684\u6700\u4f18\u4f20\u8f93\u7684\u591a\u63d0\u793a\u89e3\u7801\u548c\u7528\u4e8e\u7c7b\u5206\u6570\u7684\u6821\u51c6\u89e3\u7801\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "author": "Zifeng Cheng et.al.", "authors": "Zifeng Cheng, Zhaoling Chen, Zhiwei Jiang, Yafeng Yin, Shiping Ge, Yuliang Liu, Qing Gu", "id": "2406.06279v1", "paper_url": "http://arxiv.org/abs/2406.06279v1", "repo": "null"}}