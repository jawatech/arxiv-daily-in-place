{"2412.06198": {"publish_time": "2024-12-09", "title": "SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs", "paper_summary": "As Large Language Models (LLMs) scale to longer context windows, the\ncomputational cost of attention mechanisms, which traditionally grows\nquadratically with input length, presents a critical challenge for real-time\nand memory-constrained deployments. Existing sparse attention techniques have\nsought to reduce this complexity, but they often incur significant overhead or\ncompromise accuracy, making them less practical for large contexts on mid-range\nhardware. In this paper, we introduce SparseAccelerate, a dynamic sparse\nattention method that adapts its sparsity patterns based on input\ncharacteristics, effectively flattening the attention complexity curve. Our\napproach is effective for input lengths starting at 16K tokens and scales\nefficiently up to 128K tokens on dual NVIDIA A5000 GPUs (24GB each).\nExperimental results show that SparseAccelerate achieves up to a 1.04x\nreduction in Time-To-First-Token (TTFT) latency at 32K tokens, while also\nproviding substantial memory savings. These improvements yield practical gains\nfor memory-intensive applications and long-context tasks that were previously\ninfeasible with standard attention. Beyond latency reductions, SparseAccelerate\nfundamentally shifts the scaling trend, demonstrating the smallest TTFT growth\ngradient relative to context length among competing methods. Ongoing\nevaluations on diverse benchmarks confirm its scalability, positioning\nSparseAccelerate as a critical advancement toward efficient, real-time, and\nlarge-context LLM inference on accessible hardware.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64f4\u5c55\u5230\u66f4\u9577\u7684\u5167\u5bb9\u8996\u7a97\uff0c\u50b3\u7d71\u4e0a\u96a8\u8457\u8f38\u5165\u9577\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u9577\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u8a08\u7b97\u6210\u672c\uff0c\u5c0d\u5373\u6642\u548c\u8a18\u61b6\u9ad4\u53d7\u9650\u7684\u90e8\u7f72\u69cb\u6210\u4e86\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u73fe\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6280\u8853\u5df2\u5c0b\u6c42\u964d\u4f4e\u6b64\u8907\u96dc\u6027\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u6703\u7522\u751f\u986f\u8457\u7684\u958b\u92b7\u6216\u640d\u5bb3\u6e96\u78ba\u6027\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u5c0d\u65bc\u4e2d\u968e\u786c\u9ad4\u4e0a\u7684\u5927\u578b\u5167\u5bb9\u4e0d\u592a\u5be6\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 SparseAccelerate\uff0c\u9019\u662f\u4e00\u7a2e\u52d5\u614b\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5b83\u6839\u64da\u8f38\u5165\u7279\u5fb5\u8abf\u6574\u5176\u7a00\u758f\u6a21\u5f0f\uff0c\u6709\u6548\u5730\u5c55\u5e73\u4e86\u6ce8\u610f\u529b\u8907\u96dc\u6027\u66f2\u7dda\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c0d\u65bc\u5f9e 16K \u500b\u4ee3\u5e63\u958b\u59cb\u7684\u8f38\u5165\u9577\u5ea6\u6709\u6548\uff0c\u4e26\u4e14\u53ef\u4ee5\u5728\u5169\u500b NVIDIA A5000 GPU\uff08\u6bcf\u500b 24GB\uff09\u4e0a\u6709\u6548\u5730\u64f4\u5c55\u5230 128K \u500b\u4ee3\u5e63\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cSparseAccelerate \u5728 32K \u500b\u4ee3\u5e63\u6642\u5c07\u9996\u6b21\u4ee3\u5e63\u6642\u9593 (TTFT) \u5ef6\u9072\u6e1b\u5c11\u4e86 1.04 \u500d\uff0c\u540c\u6642\u4e5f\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\u7bc0\u7701\u3002\u9019\u4e9b\u6539\u9032\u70ba\u8a18\u61b6\u9ad4\u5bc6\u96c6\u578b\u61c9\u7528\u7a0b\u5f0f\u548c\u9577\u5167\u5bb9\u4efb\u52d9\u5e36\u4f86\u4e86\u5be6\u969b\u6536\u76ca\uff0c\u800c\u9019\u4e9b\u4efb\u52d9\u4ee5\u524d\u4f7f\u7528\u6a19\u6e96\u6ce8\u610f\u529b\u662f\u4e0d\u53ef\u884c\u7684\u3002\u9664\u4e86\u6e1b\u5c11\u5ef6\u9072\u4e4b\u5916\uff0cSparseAccelerate \u5f9e\u6839\u672c\u4e0a\u6539\u8b8a\u4e86\u64f4\u5c55\u8da8\u52e2\uff0c\u5c55\u793a\u4e86\u76f8\u5c0d\u65bc\u5167\u5bb9\u9577\u5ea6\u800c\u8a00\uff0c\u5728\u7af6\u722d\u65b9\u6cd5\u4e2d TTFT \u589e\u9577\u68af\u5ea6\u6700\u5c0f\u3002\u5c0d\u5404\u7a2e\u57fa\u6e96\u7684\u6301\u7e8c\u8a55\u4f30\u78ba\u8a8d\u4e86\u5176\u53ef\u64f4\u5c55\u6027\uff0c\u5c07 SparseAccelerate \u5b9a\u4f4d\u70ba\u671d\u8457\u5728\u53ef\u5b58\u53d6\u786c\u9ad4\u4e0a\u9032\u884c\u9ad8\u6548\u3001\u5373\u6642\u4e14\u5927\u5167\u5bb9 LLM \u63a8\u8ad6\u7684\u95dc\u9375\u9032\u5c55\u3002", "author": "James Vo et.al.", "authors": "James Vo", "id": "2412.06198v1", "paper_url": "http://arxiv.org/abs/2412.06198v1", "repo": "null"}}