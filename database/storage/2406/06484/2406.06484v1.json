{"2406.06484": {"publish_time": "2024-06-10", "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length", "paper_summary": "Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive outer-product update in linear transformers with the delta rule\nhave been found to be more effective at associative recall, existing algorithms\nfor training such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks (including on\ntasks that focus on recall). We also experiment with two hybrid models which\ncombine DeltaNet layers with (1) sliding-window attention layers every other\nlayer or (2) two global attention layers, and find that these hybrid models\noutperform strong transformer baselines.", "paper_summary_zh": "\u5e36\u6709\u7dda\u6027\u6ce8\u610f\u529b\uff08\u5373\u7dda\u6027Transformer\uff09\u7684Transformer\u548c\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u6700\u8fd1\u5df2\u88ab\u5efa\u8b70\u4f5c\u70ba\u5177\u6709 softmax \u6ce8\u610f\u529b\u7684Transformer\u7684\u53ef\u884c\u7684\u7dda\u6027\u6642\u9593\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u4ecd\u7136\u8868\u73fe\u4e0d\u4f73\uff0c\u7279\u5225\u662f\u5728\u9700\u8981\u4e0a\u4e0b\u6587\u6aa2\u7d22\u7684\u4efb\u52d9\u4e0a\u3002\u96d6\u7136\u5df2\u7d93\u767c\u73fe\u7528 delta \u898f\u5247\u66ff\u63db\u7dda\u6027Transformer\u4e2d\u7684\u52a0\u6cd5\u5916\u7a4d\u66f4\u65b0\u7684\u7dda\u6027Transformer\u7684\u66f4\u5177\u8868\u73fe\u529b\u7684\u8b8a\u9ad4\u5728\u806f\u60f3\u53ec\u56de\u65b9\u9762\u66f4\u6709\u6548\uff0c\u4f46\u73fe\u6709\u7684\u8a13\u7df4\u6b64\u985e\u6a21\u578b\u7684\u6f14\u7b97\u6cd5\u4e26\u672a\u5728\u5e8f\u5217\u9577\u5ea6\u4e0a\u4e26\u884c\u5316\uff0c\u56e0\u6b64\u5728\u73fe\u4ee3\u786c\u9ad4\u4e0a\u8a13\u7df4\u6548\u7387\u4f4e\u4e0b\u3002\u9019\u9805\u5de5\u4f5c\u63cf\u8ff0\u4e86\u4e00\u7a2e\u7528\u65bc\u8a13\u7df4\u5177\u6709 delta \u898f\u5247\u7684\u7dda\u6027Transformer\u7684\u786c\u9ad4\u9ad8\u6548\u6f14\u7b97\u6cd5\uff0c\u8a72\u6f14\u7b97\u6cd5\u5229\u7528\u8a18\u61b6\u9ad4\u9ad8\u6548\u7684\u8868\u793a\u4f86\u8a08\u7b97 Householder \u77e9\u9663\u7684\u4e58\u7a4d\u3002\u6b64\u6f14\u7b97\u6cd5\u5141\u8a31\u6211\u5011\u5c07 DeltaNet \u64f4\u5c55\u5230\u6a19\u6e96\u8a9e\u8a00\u5efa\u6a21\u8a2d\u5b9a\u3002\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b 1.3B \u6a21\u578b\uff0c\u7528\u65bc 100B \u500b\u4ee3\u5e63\uff0c\u4e26\u767c\u73fe\u5b83\u5728\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u4efb\u52d9\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u65b9\u9762\u512a\u65bc\u6700\u8fd1\u7684\u7dda\u6027\u6642\u9593\u57fa\u7dda\uff0c\u4f8b\u5982 Mamba \u548c GLA\uff08\u5305\u62ec\u5c08\u6ce8\u65bc\u53ec\u56de\u7684\u4efb\u52d9\uff09\u3002\u6211\u5011\u9084\u5c0d\u5169\u7a2e\u6df7\u5408\u6a21\u578b\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u9019\u4e9b\u6a21\u578b\u5c07 DeltaNet \u5c64\u8207\uff081\uff09\u6bcf\u9694\u4e00\u5c64\u7684\u6ed1\u52d5\u8996\u7a97\u6ce8\u610f\u529b\u5c64\u6216\uff082\uff09\u5169\u500b\u5168\u5c40\u6ce8\u610f\u529b\u5c64\u7d50\u5408\u5728\u4e00\u8d77\uff0c\u4e26\u767c\u73fe\u9019\u4e9b\u6df7\u5408\u6a21\u578b\u512a\u65bc\u5f37\u5927\u7684Transformer\u57fa\u7dda\u3002", "author": "Songlin Yang et.al.", "authors": "Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim", "id": "2406.06484v1", "paper_url": "http://arxiv.org/abs/2406.06484v1", "repo": "null"}}