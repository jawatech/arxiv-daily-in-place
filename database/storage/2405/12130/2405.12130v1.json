{"2405.12130": {"publish_time": "2024-05-20", "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning", "paper_summary": "Low-rank adaptation is a popular parameter-efficient fine-tuning method for\nlarge language models. In this paper, we analyze the impact of low-rank\nupdating, as implemented in LoRA. Our findings suggest that the low-rank\nupdating mechanism may limit the ability of LLMs to effectively learn and\nmemorize new knowledge. Inspired by this observation, we propose a new method\ncalled MoRA, which employs a square matrix to achieve high-rank updating while\nmaintaining the same number of trainable parameters. To achieve it, we\nintroduce the corresponding non-parameter operators to reduce the input\ndimension and increase the output dimension for the square matrix. Furthermore,\nthese operators ensure that the weight can be merged back into LLMs, which\nmakes our method can be deployed like LoRA. We perform a comprehensive\nevaluation of our method across five tasks: instruction tuning, mathematical\nreasoning, continual pretraining, memory and pretraining. Our method\noutperforms LoRA on memory-intensive tasks and achieves comparable performance\non other tasks.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9\u662f\u4e00\u7a2e\u5ee3\u53d7\u6b61\u8fce\u4e14\u53c3\u6578\u6548\u7387\u9ad8\u7684\u5fae\u8abf\u65b9\u6cd5\uff0c\u9069\u7528\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5206\u6790\u4e86\u4f4e\u79e9\u66f4\u65b0\u7684\u5f71\u97ff\uff0c\u5c31\u50cf\u5728 LoRA \u4e2d\u5be6\u4f5c\u7684\u90a3\u6a23\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u4f4e\u79e9\u66f4\u65b0\u6a5f\u5236\u53ef\u80fd\u6703\u9650\u5236\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6709\u6548\u5b78\u7fd2\u548c\u8a18\u61b6\u65b0\u77e5\u8b58\u7684\u80fd\u529b\u3002\u53d7\u5230\u9019\u4e00\u89c0\u5bdf\u7d50\u679c\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba MoRA \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u63a1\u7528\u4e00\u500b\u65b9\u9663\u4f86\u5be6\u73fe\u9ad8\u79e9\u66f4\u65b0\uff0c\u540c\u6642\u4fdd\u6301\u76f8\u540c\u6578\u91cf\u7684\u53ef\u8a13\u7df4\u53c3\u6578\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u76f8\u61c9\u7684\u975e\u53c3\u6578\u904b\u7b97\u5b50\uff0c\u4ee5\u964d\u4f4e\u8f38\u5165\u7dad\u5ea6\u4e26\u589e\u52a0\u65b9\u9663\u7684\u8f38\u51fa\u7dad\u5ea6\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u904b\u7b97\u5b50\u78ba\u4fdd\u6b0a\u91cd\u53ef\u4ee5\u5408\u4f75\u56de\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u9019\u4f7f\u5f97\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u50cf LoRA \u4e00\u6a23\u90e8\u7f72\u3002\u6211\u5011\u5c0d\u6211\u5011\u7684\u6a21\u578b\u9032\u884c\u4e86\u5168\u9762\u7684\u8a55\u4f30\uff0c\u6db5\u84cb\u4e86\u4e94\u9805\u4efb\u52d9\uff1a\u6307\u4ee4\u5fae\u8abf\u3001\u6578\u5b78\u63a8\u7406\u3001\u6301\u7e8c\u9810\u8a13\u7df4\u3001\u8a18\u61b6\u548c\u9810\u8a13\u7df4\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u8a18\u61b6\u5bc6\u96c6\u578b\u4efb\u52d9\u4e0a\u512a\u65bc LoRA\uff0c\u4e26\u5728\u5176\u4ed6\u4efb\u52d9\u4e0a\u5be6\u73fe\u4e86\u53ef\u6bd4\u7684\u6548\u80fd\u3002", "author": "Ting Jiang et.al.", "authors": "Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang", "id": "2405.12130v1", "paper_url": "http://arxiv.org/abs/2405.12130v1", "repo": "null"}}