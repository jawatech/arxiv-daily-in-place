{"2410.22678": {"publish_time": "2024-10-30", "title": "Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion", "paper_summary": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetworks (CNN) across various computer vision tasks. However, akin to CNN, ViTs\nare vulnerable to backdoor attacks, where the adversary embeds the backdoor\ninto the victim model, causing it to make wrong predictions about testing\nsamples containing a specific trigger. Existing backdoor attacks against ViTs\nhave the limitation of failing to strike an optimal balance between attack\nstealthiness and attack effectiveness.\n  In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB)\ntargeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively\nerodes pixels in areas of maximal attention gradient, embedding a covert\nbackdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves\nan optimal balance between attack stealthiness and attack effectiveness,\nensuring the trigger remains invisible to human detection while preserving the\nmodel's accuracy on clean samples. Extensive experimental evaluations across\nvarious ViT architectures and datasets confirm the effectiveness of AGEB,\nachieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data\nAccuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated,\ndemonstrating minimal visual discrepancies between the clean and the triggered\nimages.", "paper_summary_zh": "\u8996\u89ba Transformer (ViT) \u5728\u5404\u7a2e\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u4e2d\u512a\u65bc\u50b3\u7d71\u7684\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN)\u3002\u7136\u800c\uff0c\u8207 CNN \u985e\u4f3c\uff0cViT \u5bb9\u6613\u53d7\u5230\u5f8c\u9580\u653b\u64ca\uff0c\u5176\u4e2d\u5c0d\u624b\u5c07\u5f8c\u9580\u5d4c\u5165\u53d7\u5bb3\u8005\u6a21\u578b\uff0c\u5c0e\u81f4\u5176\u5c0d\u5305\u542b\u7279\u5b9a\u89f8\u767c\u5668\u7684\u6e2c\u8a66\u6a23\u672c\u505a\u51fa\u932f\u8aa4\u9810\u6e2c\u3002\u73fe\u6709\u7684\u91dd\u5c0d ViT \u7684\u5f8c\u9580\u653b\u64ca\u5b58\u5728\u7121\u6cd5\u5728\u653b\u64ca\u96b1\u853d\u6027\u548c\u653b\u64ca\u6709\u6548\u6027\u4e4b\u9593\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u7684\u9650\u5236\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u91dd\u5c0d ViT \u7684\u57fa\u65bc\u6ce8\u610f\u68af\u5ea6\u7684\u4fb5\u8755\u5f8c\u9580 (AGEB)\u3002\u8003\u616e\u5230 ViT \u7684\u6ce8\u610f\u6a5f\u5236\uff0cAGEB \u9078\u64c7\u6027\u5730\u4fb5\u8755\u6700\u5927\u6ce8\u610f\u68af\u5ea6\u7684\u5340\u57df\u50cf\u7d20\uff0c\u5d4c\u5165\u4e00\u500b\u96b1\u853d\u7684\u5f8c\u9580\u89f8\u767c\u5668\u3002\u8207\u5148\u524d\u91dd\u5c0d ViT \u7684\u5f8c\u9580\u653b\u64ca\u4e0d\u540c\uff0cAGEB \u5728\u653b\u64ca\u96b1\u853d\u6027\u548c\u653b\u64ca\u6709\u6548\u6027\u4e4b\u9593\u5be6\u73fe\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u78ba\u4fdd\u89f8\u767c\u5668\u5c0d\u4eba\u985e\u6aa2\u6e2c\u4fdd\u6301\u4e0d\u53ef\u898b\uff0c\u540c\u6642\u4fdd\u6301\u6a21\u578b\u5c0d\u4e7e\u6de8\u6a23\u672c\u7684\u6e96\u78ba\u6027\u3002\u8de8\u5404\u7a2e ViT \u67b6\u69cb\u548c\u8cc7\u6599\u96c6\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8a55\u4f30\u8b49\u5be6\u4e86 AGEB \u7684\u6709\u6548\u6027\uff0c\u5be6\u73fe\u4e86\u986f\u8457\u7684\u653b\u64ca\u6210\u529f\u7387 (ASR)\uff0c\u800c\u4e0d\u6703\u964d\u4f4e\u4e7e\u6de8\u6578\u64da\u6e96\u78ba\u5ea6 (CDA)\u3002\u6b64\u5916\uff0cAGEB \u7684\u96b1\u853d\u6027\u5f97\u5230\u4e86\u56b4\u683c\u9a57\u8b49\uff0c\u8b49\u660e\u4e86\u4e7e\u6de8\u5716\u50cf\u548c\u89f8\u767c\u5716\u50cf\u4e4b\u9593\u6700\u5c0f\u7684\u8996\u89ba\u5dee\u7570\u3002", "author": "Ji Guo et.al.", "authors": "Ji Guo, Hongwei Li, Wenbo Jiang, Guoming Lu", "id": "2410.22678v1", "paper_url": "http://arxiv.org/abs/2410.22678v1", "repo": "null"}}