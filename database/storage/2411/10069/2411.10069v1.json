{"2411.10069": {"publish_time": "2024-11-15", "title": "Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity", "paper_summary": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs.", "paper_summary_zh": "\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u4e0d\u540c\u5c64\u7d1a\u7684\u91cd\u8981\u6027\u5c0d\u65bc\u6700\u4f73\u5316\u6a21\u578b\u6548\u80fd\u548c\u53ef\u89e3\u91cb\u6027\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u9996\u5148\u4f7f\u7528\u555f\u52d5\u65b9\u5dee\u7a00\u758f\u5ea6\u8a55\u5206 (AVSS) \u4f86\u63a2\u8a0e\u5c64\u7d1a\u7684\u91cd\u8981\u6027\uff0c\u5b83\u7d50\u5408\u4e86\u6a19\u6e96\u5316\u555f\u52d5\u65b9\u5dee\u548c\u7a00\u758f\u5ea6\uff0c\u4ee5\u91cf\u5316\u6bcf\u4e00\u5c64\u5c0d\u6574\u9ad4\u6a21\u578b\u6548\u80fd\u7684\u8ca2\u737b\u3002\u900f\u904e\u6839\u64da AVSS \u5c0d\u5c64\u7d1a\u9032\u884c\u6392\u5e8f\uff0c\u4e26\u4fee\u526a\u5f71\u97ff\u6700\u5c0f\u7684 25%\uff0c\u6211\u5011\u5728\u554f\u7b54\u3001\u8a9e\u8a00\u5efa\u6a21\u548c\u60c5\u7dd2\u5206\u985e\u7b49\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u986f\u793a\uff0c\u4fdd\u7559\u4e86\u8d85\u904e 90% \u7684\u539f\u59cb\u6548\u80fd\uff0c\u7a81\u986f\u4e86 LLM \u67b6\u69cb\u4e2d\u6f5b\u5728\u7684\u5197\u9918\u3002\u5efa\u7acb\u5728 AVSS \u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u589e\u5f37\u7248\u672c\uff0c\u5c08\u9580\u7528\u65bc\u8a55\u4f30\u8de8\u5c64\u7d1a\u7684\u5e7b\u89ba\u50be\u5411 (EAVSS)\u3002\u9019\u7a2e\u6539\u826f\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u5e7b\u89ba\u7279\u5b9a\u555f\u52d5\u65b9\u5dee (HSAV) \u548c\u5e7b\u89ba\u7279\u5b9a\u7a00\u758f\u5ea6 (HSS) \u6307\u6a19\uff0c\u53ef\u4ee5\u7cbe\u6e96\u8fa8\u8b58\u5bb9\u6613\u7522\u751f\u5e7b\u89ba\u7684\u5c64\u7d1a\u3002\u900f\u904e\u5728\u9019\u4e9b\u5c64\u7d1a\u4e2d\u7d0d\u5165\u5c0d\u6bd4\u5b78\u7fd2\uff0c\u6211\u5011\u6709\u6548\u6e1b\u8f15\u4e86\u5e7b\u89ba\u7684\u7522\u751f\uff0c\u6709\u52a9\u65bc\u5efa\u7acb\u66f4\u5f37\u5927\u4e14\u66f4\u6709\u6548\u7684 LLM\uff08\u6548\u80fd\u6700\u5927\u7684\u63d0\u5347\u5e45\u5ea6\u70ba 12%\uff09\u3002\u6211\u5011\u5728 NQ\u3001SciQ\u3001TriviaQA\u3001TruthfulQA \u548c WikiQA \u8cc7\u6599\u96c6\u4e0a\u7684\u7d50\u679c\u8b49\u660e\u4e86\u6b64\u65b9\u6cd5\u7684\u6548\u7528\uff0c\u70ba LLM \u4e2d\u7684\u5c64\u7d1a\u91cd\u8981\u6027\u8a55\u4f30\u548c\u5e7b\u89ba\u6e1b\u8f15\u63d0\u4f9b\u4e86\u4e00\u500b\u5168\u9762\u7684\u67b6\u69cb\u3002", "author": "Zichen Song et.al.", "authors": "Zichen Song, Sitan Huang, Yuxin Wu, Zhongfeng Kang", "id": "2411.10069v1", "paper_url": "http://arxiv.org/abs/2411.10069v1", "repo": "null"}}