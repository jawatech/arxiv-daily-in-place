{"2406.09308": {"publish_time": "2024-06-13", "title": "Transformers meet Neural Algorithmic Reasoners", "paper_summary": "Transformers have revolutionized machine learning with their simple yet\neffective architecture. Pre-training Transformers on massive text datasets from\nthe Internet has led to unmatched generalization for natural language\nunderstanding (NLU) tasks. However, such language models remain fragile when\ntasked with algorithmic forms of reasoning, where computations must be precise\nand robust. To address this limitation, we propose a novel approach that\ncombines the Transformer's language understanding with the robustness of graph\nneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs\nproved effective as generic solvers for algorithmic tasks, when specified in\ngraph form. To make their embeddings accessible to a Transformer, we propose a\nhybrid architecture with a two-phase training procedure, allowing the tokens in\nthe language model to cross-attend to the node embeddings from the NAR. We\nevaluate our resulting TransNAR model on CLRS-Text, the text-based version of\nthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-only\nmodels for algorithmic reasoning, both in and out of distribution.", "paper_summary_zh": "Transformer \u6191\u85c9\u5176\u7c21\u55ae\u4f46\u6709\u6548\u7684\u67b6\u69cb\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u6a5f\u5668\u5b78\u7fd2\u3002\u5728\u7db2\u969b\u7db2\u8def\u4e0a\u7684\u5927\u91cf\u6587\u5b57\u8cc7\u6599\u96c6\u4e0a\u5c0d Transformer \u9032\u884c\u9810\u8a13\u7df4\uff0c\u5df2\u70ba\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u4efb\u52d9\u5e36\u4f86\u4e86\u7121\u8207\u502b\u6bd4\u7684\u6cdb\u5316\u3002\u7136\u800c\uff0c\u5728\u9700\u8981\u6f14\u7b97\u6cd5\u5f62\u5f0f\u63a8\u7406\u7684\u4efb\u52d9\u4e2d\uff0c\u6b64\u985e\u8a9e\u8a00\u6a21\u578b\u4ecd\u7136\u8106\u5f31\uff0c\u56e0\u70ba\u8a08\u7b97\u5fc5\u9808\u7cbe\u78ba\u4e14\u7a69\u5065\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u7d50\u5408\u4e86 Transformer \u7684\u8a9e\u8a00\u7406\u89e3\u8207\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u70ba\u57fa\u790e\u7684\u795e\u7d93\u6f14\u7b97\u6cd5\u63a8\u7406\u5668 (NAR) \u7684\u7a69\u5065\u6027\u3002\u6b64\u985e NAR \u88ab\u8b49\u660e\u5728\u4ee5\u5716\u5f62\u5f62\u5f0f\u6307\u5b9a\u6642\uff0c\u53ef\u4f5c\u70ba\u6f14\u7b97\u6cd5\u4efb\u52d9\u7684\u901a\u7528\u89e3\u7b97\u5668\u3002\u70ba\u4e86\u8b93 Transformer \u53ef\u4ee5\u5b58\u53d6\u5176\u5d4c\u5165\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5177\u6709\u5169\u968e\u6bb5\u8a13\u7df4\u7a0b\u5e8f\u7684\u6df7\u5408\u67b6\u69cb\uff0c\u8b93\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u7b26\u865f\u53ef\u4ee5\u4ea4\u53c9\u95dc\u6ce8 NAR \u4e2d\u7684\u7bc0\u9ede\u5d4c\u5165\u3002\u6211\u5011\u5728 CLRS-Text\uff08CLRS-30 \u57fa\u6e96\u7684\u6587\u5b57\u7248\u672c\uff09\u4e0a\u8a55\u4f30\u6211\u5011\u5f97\u5230\u7684 TransNAR \u6a21\u578b\uff0c\u4e26\u8b49\u660e\u4e86\u5728\u6f14\u7b97\u6cd5\u63a8\u7406\u4e2d\uff0c\u7121\u8ad6\u662f\u5728\u5206\u4f48\u5167\u6216\u5206\u4f48\u5916\uff0c\u90fd\u6bd4\u50c5\u4f7f\u7528 Transformer \u7684\u6a21\u578b\u6709\u986f\u8457\u7684\u9032\u6b65\u3002", "author": "Wilfried Bounsi et.al.", "authors": "Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, Petar Veli\u010dkovi\u0107", "id": "2406.09308v1", "paper_url": "http://arxiv.org/abs/2406.09308v1", "repo": "null"}}