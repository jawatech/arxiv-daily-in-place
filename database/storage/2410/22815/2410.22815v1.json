{"2410.22815": {"publish_time": "2024-10-30", "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients", "paper_summary": "Federated fine-tuning for Large Language Models (LLMs) has recently gained\nattention due to the heavy communication overhead of transmitting large model\nupdates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its\napplication in federated learning is complicated by discordance in aggregation.\nExisting methods addressing this discordance often suffer from performance\ndegradation at low ranks in heterogeneous data settings. In response, we\nintroduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive\nrank selection), which demonstrates robustness in challenging settings with low\nranks and high data heterogeneity. Our experimental findings reveal that\nLoRA-A2 maintains performance even under extreme heterogeneity and low rank\nconditions, achieving up to a 99.8% reduction in uploaded parameters compared\nto full fine-tuning without compromising performance. This adaptive mechanism\nboosts robustness and communication efficiency in federated fine-tuning,\nenabling the practical deployment of LLMs in resource-constrained environments.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u806f\u90a6\u5fae\u8abf\u6700\u8fd1\u56e0\u50b3\u8f38\u5927\u578b\u6a21\u578b\u66f4\u65b0\u7684\u9f90\u5927\u901a\u8a0a\u958b\u92b7\u800c\u53d7\u5230\u95dc\u6ce8\u3002\u4f4e\u79e9\u9069\u61c9 (LoRA) \u5df2\u88ab\u63d0\u51fa\u4f5c\u70ba\u4e00\u7a2e\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u5176\u5728\u806f\u90a6\u5b78\u7fd2\u4e2d\u7684\u61c9\u7528\u56e0\u805a\u5408\u4e2d\u7684\u4e0d\u4e00\u81f4\u800c\u8b8a\u5f97\u8907\u96dc\u3002\u89e3\u6c7a\u9019\u7a2e\u4e0d\u4e00\u81f4\u7684\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u7570\u8cea\u6578\u64da\u8a2d\u7f6e\u7684\u4f4e\u79e9\u4e2d\u6703\u51fa\u73fe\u6548\u80fd\u4e0b\u964d\u7684\u554f\u984c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LoRA-A2\uff08\u4ea4\u66ff\u51cd\u7d50\u548c\u81ea\u9069\u61c9\u79e9\u9078\u64c7\u7684\u4f4e\u79e9\u9069\u61c9\uff09\uff0c\u5b83\u5728\u4f4e\u79e9\u548c\u9ad8\u6578\u64da\u7570\u8cea\u6027\u7684\u6311\u6230\u6027\u8a2d\u7f6e\u4e2d\u5c55\u73fe\u51fa\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6975\u7aef\u7684\u7570\u8cea\u6027\u548c\u4f4e\u79e9\u689d\u4ef6\u4e0b\uff0cLoRA-A2 \u4ecd\u80fd\u7dad\u6301\u6548\u80fd\uff0c\u8207\u5b8c\u5168\u5fae\u8abf\u76f8\u6bd4\uff0c\u4e0a\u50b3\u53c3\u6578\u6e1b\u5c11\u4e86 99.8%\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u6548\u80fd\u3002\u9019\u7a2e\u81ea\u9069\u61c9\u6a5f\u5236\u63d0\u5347\u4e86\u806f\u90a6\u5fae\u8abf\u4e2d\u7684\u7a69\u5065\u6027\u548c\u901a\u8a0a\u6548\u7387\uff0c\u4f7f LLM \u80fd\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u5be6\u969b\u90e8\u7f72\u3002", "author": "Jabin Koo et.al.", "authors": "Jabin Koo, Minwoo Jang, Jungseul Ok", "id": "2410.22815v1", "paper_url": "http://arxiv.org/abs/2410.22815v1", "repo": "null"}}