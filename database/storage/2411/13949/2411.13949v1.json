{"2411.13949": {"publish_time": "2024-11-21", "title": "Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning", "paper_summary": "Visual instruction tuning (VIT) enables multimodal large language models\n(MLLMs) to effectively handle a wide range of vision tasks by framing them as\nlanguage-based instructions. Building on this, continual visual instruction\ntuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks,\naccommodating evolving functionalities. While prior work has advanced CVIT\nthrough the development of new benchmarks and approaches to mitigate\ncatastrophic forgetting, these efforts largely follow traditional continual\nlearning paradigms, neglecting the unique challenges specific to CVIT. We\nidentify a dual form of catastrophic forgetting in CVIT, where MLLMs not only\nforget previously learned visual understanding but also experience a decline in\ninstruction following abilities as they acquire new tasks. To address this, we\nintroduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework,\nwhich employs separable routing through two distinct modules - one for visual\nunderstanding and another for instruction following. This dual-routing design\nenables specialized adaptation in both domains, preventing forgetting while\nimproving performance. Furthermore, we propose a novel CVIT benchmark that goes\nbeyond existing benchmarks by additionally evaluating a model's ability to\ngeneralize to unseen tasks and handle diverse instructions across various\ntasks. Extensive experiments demonstrate that SMoLoRA outperforms existing\nmethods in mitigating dual forgetting, improving generalization to unseen\ntasks, and ensuring robustness in following diverse instructions.", "paper_summary_zh": "\u8996\u89ba\u6307\u4ee4\u5fae\u8abf (VIT) \u80fd\u8b93\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u6709\u6548\u8655\u7406\u5ee3\u6cdb\u7684\u8996\u89ba\u4efb\u52d9\uff0c\u65b9\u6cd5\u662f\u5c07\u5b83\u5011\u5efa\u69cb\u70ba\u57fa\u65bc\u8a9e\u8a00\u7684\u6307\u4ee4\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6301\u7e8c\u8996\u89ba\u6307\u4ee4\u5fae\u8abf (CVIT) \u64f4\u5c55\u4e86 MLLM \u7684\u80fd\u529b\uff0c\u8b93\u5b83\u5011\u80fd\u9010\u6b65\u5b78\u7fd2\u65b0\u4efb\u52d9\uff0c\u4ee5\u9069\u61c9\u4e0d\u65b7\u8b8a\u5316\u7684\u529f\u80fd\u3002\u5118\u7ba1\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u900f\u904e\u958b\u767c\u65b0\u7684\u57fa\u6e96\u548c\u65b9\u6cd5\u4f86\u7de9\u89e3\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u9032\u800c\u63a8\u52d5 CVIT \u7684\u9032\u6b65\uff0c\u4f46\u9019\u4e9b\u52aa\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u9075\u5faa\u50b3\u7d71\u7684\u6301\u7e8c\u5b78\u7fd2\u7bc4\u4f8b\uff0c\u5ffd\u7565\u4e86 CVIT \u7279\u6709\u7684\u7368\u7279\u6311\u6230\u3002\u6211\u5011\u5728 CVIT \u4e2d\u767c\u73fe\u4e86\u4e00\u7a2e\u707d\u96e3\u6027\u907a\u5fd8\u7684\u96d9\u91cd\u5f62\u5f0f\uff0c\u5176\u4e2d MLLM \u4e0d\u50c5\u907a\u5fd8\u4e86\u5148\u524d\u5b78\u7fd2\u7684\u8996\u89ba\u7406\u89e3\uff0c\u800c\u4e14\u5728\u7372\u5f97\u65b0\u4efb\u52d9\u6642\uff0c\u5176\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u4e5f\u6703\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u53ef\u5206\u96e2\u4f4e\u79e9\u9069\u61c9\u6df7\u5408 (SMoLoRA) \u6846\u67b6\uff0c\u5b83\u63a1\u7528\u53ef\u5206\u96e2\u8def\u7531\uff0c\u900f\u904e\u5169\u500b\u4e0d\u540c\u7684\u6a21\u7d44\u4f86\u57f7\u884c\uff0c\u4e00\u500b\u6a21\u7d44\u7528\u65bc\u8996\u89ba\u7406\u89e3\uff0c\u53e6\u4e00\u500b\u6a21\u7d44\u7528\u65bc\u9075\u5faa\u6307\u4ee4\u3002\u9019\u7a2e\u96d9\u91cd\u8def\u7531\u8a2d\u8a08\u53ef\u4ee5\u5728\u5169\u500b\u9818\u57df\u4e2d\u9032\u884c\u5c08\u9580\u9069\u61c9\uff0c\u9632\u6b62\u907a\u5fd8\uff0c\u540c\u6642\u63d0\u5347\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684 CVIT \u57fa\u6e96\uff0c\u5b83\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u57fa\u6e96\uff0c\u9032\u4e00\u6b65\u8a55\u4f30\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u6982\u62ec\u5230\u672a\u898b\u4efb\u52d9\u548c\u8655\u7406\u4e0d\u540c\u6307\u4ee4\u7684\u80fd\u529b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cSMoLoRA \u5728\u6e1b\u8f15\u96d9\u91cd\u907a\u5fd8\u3001\u6539\u5584\u5c0d\u672a\u898b\u4efb\u52d9\u7684\u6982\u62ec\u4ee5\u53ca\u78ba\u4fdd\u9075\u5faa\u4e0d\u540c\u6307\u4ee4\u7684\u7a69\u5065\u6027\u65b9\u9762\uff0c\u90fd\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "author": "Ziqi Wang et.al.", "authors": "Ziqi Wang, Chang Che, Qi Wang, Yangyang Li, Zenglin Shi, Meng Wang", "id": "2411.13949v1", "paper_url": "http://arxiv.org/abs/2411.13949v1", "repo": "null"}}