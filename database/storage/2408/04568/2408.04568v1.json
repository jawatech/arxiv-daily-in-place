{"2408.04568": {"publish_time": "2024-08-08", "title": "Learning Fine-Grained Grounded Citations for Attributed Large Language Models", "paper_summary": "Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.", "paper_summary_zh": "\u5118\u7ba1\u5728\u8cc7\u8a0a\u641c\u5c0b\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ecd\u96e3\u4ee5\u514b\u670d\u5e7b\u89ba\u554f\u984c\u3002\u5177\u6b78\u56e0\u529f\u80fd\u7684 LLM \u53ef\u5728\u7522\u751f\u7684\u6587\u5b57\u4e2d\u52a0\u5165\u5167\u6587\u5f15\u6587\uff0c\u5df2\u5c55\u73fe\u51fa\u6e1b\u8f15\u5e7b\u89ba\u4e26\u63d0\u5347\u53ef\u9a57\u8b49\u6027\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u73fe\u884c\u7684\u505a\u6cd5\u4ef0\u8cf4\u65bc\u60c5\u5883\u5b78\u7fd2\uff0c\u56e0\u6b64\u5f15\u6587\u54c1\u8cea\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u50c5\u5f15\u8ff0\u7c97\u7565\u7684\u6587\u4ef6\u8b58\u5225\u78bc\uff0c\u4f7f\u7528\u6236\u96e3\u4ee5\u9032\u884c\u7d30\u5fae\u7684\u9a57\u8b49\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 FRONT\uff0c\u4e00\u7a2e\u8a13\u7df4\u67b6\u69cb\uff0c\u65e8\u5728\u6559\u5c0e LLM \u7522\u751f\u7d30\u5fae\u7684 \u043e\u0431\u043e\u0441\u043d\u043e\u5f15\u6587\u3002\u900f\u904e\u5c07\u6a21\u578b\u8f38\u51fa\u5efa\u7acb\u5728\u7d30\u5fae\u7684\u652f\u63f4\u5f15\u6587\u4e2d\uff0c\u9019\u4e9b\u5f15\u6587\u53ef\u5f15\u5c0e\u7522\u751f \u043e\u0431\u043e\u0441\u043d\u043e\u4e14\u4e00\u81f4\u7684\u56de\u61c9\uff0c\u4e0d\u50c5\u63d0\u5347\u5f15\u6587\u54c1\u8cea\uff0c\u9084\u80fd\u4fc3\u9032\u7d30\u5fae\u7684\u9a57\u8b49\u3002\u5728 ALCE \u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\u4e86 FRONT \u5728\u7522\u751f\u512a\u7570\u7684 \u043e\u0431\u043e\u0441\u043d\u043e\u56de\u61c9\u548c\u9ad8\u5ea6\u652f\u6301\u6027\u5f15\u6587\u65b9\u9762\u7684\u6548\u80fd\u3002\u900f\u904e LLaMA-2-7B\uff0c\u6b64\u67b6\u69cb\u5927\u5e45\u512a\u65bc\u6240\u6709\u57fa\u7dda\uff0c\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\u7684\u5f15\u6587\u54c1\u8cea\u5e73\u5747\u63d0\u5347 14.21%\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86 ChatGPT\u3002", "author": "Lei Huang et.al.", "authors": "Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, Bing Qin", "id": "2408.04568v1", "paper_url": "http://arxiv.org/abs/2408.04568v1", "repo": "https://github.com/luckyyysta/fine-grained-attribution"}}