{"2409.02889": {"publish_time": "2024-09-04", "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture", "paper_summary": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.", "paper_summary_zh": "\u64f4\u5c55\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u9577\u6587\u672c\u80fd\u529b\u5c0d\u65bc\u5f71\u7247\u7406\u89e3\u3001\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u7406\u89e3\u548c\u591a\u6a21\u614b\u4ee3\u7406\u4f86\u8aaa\u81f3\u95dc\u91cd\u8981\u3002\u9019\u6d89\u53ca\u4e00\u7cfb\u5217\u7684\u7cfb\u7d71\u6027\u6700\u4f73\u5316\uff0c\u5305\u62ec\u6a21\u578b\u67b6\u69cb\u3001\u8cc7\u6599\u5efa\u7f6e\u548c\u8a13\u7df4\u7b56\u7565\uff0c\u7279\u5225\u662f\u89e3\u6c7a\u8af8\u5982\u300c\u96a8\u8457\u5f71\u50cf\u589e\u52a0\u800c\u964d\u4f4e\u6548\u80fd\u300d\u548c\u300c\u9ad8\u904b\u7b97\u6210\u672c\u300d\u7b49\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u6a21\u578b\u67b6\u69cb\u8abf\u6574\u70ba Mamba \u548c Transformer \u5340\u584a\u7684\u6df7\u5408\u9ad4\uff0c\u4f7f\u7528\u591a\u500b\u5f71\u50cf\u4e4b\u9593\u7684\u6642\u5e8f\u548c\u7a7a\u9593\u4f9d\u8cf4\u6027\u4f86\u5efa\u7f6e\u8cc7\u6599\uff0c\u4e26\u63a1\u7528\u6f38\u9032\u5f0f\u8a13\u7df4\u7b56\u7565\u3002\u6240\u767c\u5e03\u7684\u6a21\u578b \\textbf{LongLLaVA}\uff08\\textbf{Long}-Context \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant\uff09\u662f\u7b2c\u4e00\u500b\u6df7\u5408 MLLM\uff0c\u5728\u6548\u7387\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002LongLLaVA \u4e0d\u50c5\u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e2d\u53d6\u5f97\u7af6\u722d\u529b\u7684\u7d50\u679c\uff0c\u9084\u80fd\u7dad\u6301\u9ad8\u8655\u7406\u91cf\u548c\u4f4e\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u7279\u5225\u662f\uff0c\u5b83\u53ef\u4ee5\u5728\u55ae\u4e00 A100 80GB GPU \u4e0a\u8655\u7406\u8fd1\u5343\u5f35\u5f71\u50cf\uff0c\u5c55\u73fe\u51fa\u5ee3\u6cdb\u4efb\u52d9\u7684\u61c9\u7528\u524d\u666f\u3002", "author": "Xidong Wang et.al.", "authors": "Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang", "id": "2409.02889v1", "paper_url": "http://arxiv.org/abs/2409.02889v1", "repo": "https://github.com/freedomintelligence/longllava"}}