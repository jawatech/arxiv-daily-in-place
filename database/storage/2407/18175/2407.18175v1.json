{"2407.18175": {"publish_time": "2024-07-25", "title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers", "paper_summary": "Vision transformers (ViTs) have demonstrated their superior accuracy for\ncomputer vision tasks compared to convolutional neural networks (CNNs).\nHowever, ViT models are often computation-intensive for efficient deployment on\nresource-limited edge devices. This work proposes Quasar-ViT, a\nhardware-oriented quantization-aware architecture search framework for ViTs, to\ndesign efficient ViT models for hardware implementation while preserving the\naccuracy. First, Quasar-ViT trains a supernet using our row-wise flexible\nmixed-precision quantization scheme, mixed-precision weight entanglement, and\nsupernet layer scaling techniques. Then, it applies an efficient\nhardware-oriented search algorithm, integrated with hardware latency and\nresource modeling, to determine a series of optimal subnets from supernet under\ndifferent inference latency targets. Finally, we propose a series of\nmodel-adaptive designs on the FPGA platform to support the architecture search\nand mitigate the gap between the theoretical computation reduction and the\npractical inference speedup. Our searched models achieve 101.5, 159.6, and\n251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA\nwith 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet\ndataset, consistently outperforming prior works.", "paper_summary_zh": "<paragraph>\u8207\u5377\u7a4d\u795e\u7d93\u7db2\u8def\uff08CNN\uff09\u76f8\u6bd4\uff0c\u8996\u89ba\u8f49\u63db\u5668\uff08ViT\uff09\u5df2\u8b49\u660e\u5176\u5728\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u4e0a\u7684\u512a\u7570\u6e96\u78ba\u5ea6\u3002\u7136\u800c\uff0cViT \u6a21\u578b\u901a\u5e38\u5728\u8a08\u7b97\u4e0a\u5f88\u5bc6\u96c6\uff0c\u7121\u6cd5\u5728\u8cc7\u6e90\u6709\u9650\u7684\u908a\u7de3\u88dd\u7f6e\u4e0a\u6709\u6548\u7387\u5730\u90e8\u7f72\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u51fa\u4e86 Quasar-ViT\uff0c\u4e00\u500b\u9762\u5411\u786c\u9ad4\u7684\u91cf\u5316\u611f\u77e5\u67b6\u69cb\u641c\u5c0b\u6846\u67b6\uff0c\u7528\u65bc ViT\uff0c\u4ee5\u8a2d\u8a08\u9ad8\u6548\u7684 ViT \u6a21\u578b\u9032\u884c\u786c\u9ad4\u5be6\u4f5c\uff0c\u540c\u6642\u4fdd\u6301\u6e96\u78ba\u5ea6\u3002\u9996\u5148\uff0cQuasar-ViT \u4f7f\u7528\u6211\u5011\u5217\u5f0f\u5f48\u6027\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\u3001\u6df7\u5408\u7cbe\u5ea6\u6b0a\u91cd\u7cfe\u7e8f\u548c\u8d85\u7db2\u8def\u5c64\u7e2e\u653e\u6280\u8853\u4f86\u8a13\u7df4\u8d85\u7db2\u8def\u3002\u7136\u5f8c\uff0c\u5b83\u61c9\u7528\u4e00\u500b\u6709\u6548\u7387\u7684\u9762\u5411\u786c\u9ad4\u7684\u641c\u5c0b\u6f14\u7b97\u6cd5\uff0c\u6574\u5408\u786c\u9ad4\u5ef6\u9072\u548c\u8cc7\u6e90\u5efa\u6a21\uff0c\u4ee5\u5728\u4e0d\u540c\u7684\u63a8\u8ad6\u5ef6\u9072\u76ee\u6a19\u4e0b\u5f9e\u8d85\u7db2\u8def\u4e2d\u78ba\u5b9a\u4e00\u7cfb\u5217\u6700\u4f73\u5b50\u7db2\u8def\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728 FPGA \u5e73\u81fa\u4e0a\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\u81ea\u9069\u61c9\u8a2d\u8a08\uff0c\u4ee5\u652f\u63f4\u67b6\u69cb\u641c\u5c0b\u4e26\u7e2e\u5c0f\u7406\u8ad6\u904b\u7b97\u6e1b\u5c11\u548c\u5be6\u969b\u63a8\u8ad6\u52a0\u901f\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u6211\u5011\u641c\u5c0b\u7684\u6a21\u578b\u5728 AMD/Xilinx ZCU102 FPGA \u4e0a\u5206\u5225\u4ee5 80.4%\u300178.6% \u548c 74.9% \u7684 top-1 \u6e96\u78ba\u5ea6\uff0c\u9054\u5230 101.5\u3001159.6 \u548c 251.6 \u5e40\u6bcf\u79d2 (FPS) \u7684\u63a8\u8ad6\u901f\u5ea6\uff0c\u7528\u65bc ImageNet \u8cc7\u6599\u96c6\uff0c\u59cb\u7d42\u512a\u65bc\u5148\u524d\u7684\u7814\u7a76\u3002</paragraph>", "author": "Zhengang Li et.al.", "authors": "Zhengang Li, Alec Lu, Yanyue Xie, Zhenglun Kong, Mengshu Sun, Hao Tang, Zhong Jia Xue, Peiyan Dong, Caiwen Ding, Yanzhi Wang, Xue Lin, Zhenman Fang", "id": "2407.18175v1", "paper_url": "http://arxiv.org/abs/2407.18175v1", "repo": "null"}}