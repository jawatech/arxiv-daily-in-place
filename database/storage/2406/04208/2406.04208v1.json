{"2406.04208": {"publish_time": "2024-06-06", "title": "Aligning Agents like Large Language Models", "paper_summary": "Training agents to behave as desired in complex 3D environments from\nhigh-dimensional sensory information is challenging. Imitation learning from\ndiverse human behavior provides a scalable approach for training an agent with\na sensible behavioral prior, but such an agent may not perform the specific\nbehaviors of interest when deployed. To address this issue, we draw an analogy\nbetween the undesirable behaviors of imitation learning agents and the\nunhelpful responses of unaligned large language models (LLMs). We then\ninvestigate how the procedure for aligning LLMs can be applied to aligning\nagents in a 3D environment from pixels. For our analysis, we utilize an\nacademically illustrative part of a modern console game in which the human\nbehavior distribution is multi-modal, but we want our agent to imitate a single\nmode of this behavior. We demonstrate that we can align our agent to\nconsistently perform the desired mode, while providing insights and advice for\nsuccessfully applying this approach to training agents. Project webpage at\nhttps://adamjelley.github.io/aligning-agents-like-llms .", "paper_summary_zh": "\u8a13\u7df4\u4ee3\u7406\u4eba\u6839\u64da\u9ad8\u7dad\u5ea6\u611f\u5b98\u8cc7\u8a0a\u5728\u8907\u96dc\u7684 3D \u74b0\u5883\u4e2d\u8868\u73fe\u51fa\u5982\u9810\u671f\u822c\u7684\u884c\u70ba\u662f\u4e00\u9805\u6311\u6230\u3002\u5f9e\u591a\u6a23\u5316\u7684\u4eba\u985e\u884c\u70ba\u4e2d\u9032\u884c\u6a21\u4eff\u5b78\u7fd2\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u7684\u9014\u5f91\uff0c\u7528\u65bc\u8a13\u7df4\u5177\u5099\u660e\u667a\u884c\u70ba\u5148\u9a57\u7684\u4ee3\u7406\u4eba\uff0c\u4f46\u9019\u6a23\u7684\u4ee3\u7406\u4eba\u5728\u90e8\u7f72\u6642\u53ef\u80fd\u7121\u6cd5\u57f7\u884c\u7279\u5b9a\u7684\u611f\u8208\u8da3\u884c\u70ba\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5c07\u6a21\u4eff\u5b78\u7fd2\u4ee3\u7406\u4eba\u7684\u4e0d\u826f\u884c\u70ba\u8207\u672a\u5c0d\u9f4a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7121\u76ca\u56de\u61c9\u4e4b\u9593\u756b\u4e0a\u985e\u6bd4\u3002\u7136\u5f8c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u5c07\u5c0d\u9f4a LLM \u7684\u7a0b\u5e8f\u61c9\u7528\u65bc\u5f9e\u50cf\u7d20\u4e2d\u5c0d\u9f4a 3D \u74b0\u5883\u4e2d\u7684\u4ee3\u7406\u4eba\u3002\u5c0d\u65bc\u6211\u5011\u7684\u5206\u6790\uff0c\u6211\u5011\u5229\u7528\u73fe\u4ee3\u904a\u6232\u6a5f\u904a\u6232\u4e2d\u4e00\u500b\u5b78\u8853\u4e0a\u5177\u6709\u8aaa\u660e\u6027\u7684\u90e8\u5206\uff0c\u5176\u4e2d\u4eba\u985e\u884c\u70ba\u5206\u4f48\u662f\u591a\u6a21\u614b\u7684\uff0c\u4f46\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u4ee3\u7406\u4eba\u6a21\u4eff\u6b64\u884c\u70ba\u7684\u55ae\u4e00\u6a21\u5f0f\u3002\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u53ef\u4ee5\u5c07\u6211\u5011\u7684\u4ee3\u7406\u4eba\u5c0d\u9f4a\uff0c\u4ee5\u6301\u7e8c\u57f7\u884c\u6240\u9700\u7684\u6a21\u5f0f\uff0c\u540c\u6642\u63d0\u4f9b\u898b\u89e3\u548c\u5efa\u8b70\uff0c\u4ee5\u6210\u529f\u5c07\u6b64\u65b9\u6cd5\u61c9\u7528\u65bc\u8a13\u7df4\u4ee3\u7406\u4eba\u3002\u5c08\u6848\u7db2\u9801\uff1ahttps://adamjelley.github.io/aligning-agents-like-llms\u3002", "author": "Adam Jelley et.al.", "authors": "Adam Jelley, Yuhan Cao, Dave Bignell, Sam Devlin, Tabish Rashid", "id": "2406.04208v1", "paper_url": "http://arxiv.org/abs/2406.04208v1", "repo": "null"}}