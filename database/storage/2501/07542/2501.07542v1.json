{"2501.07542": {"publish_time": "2025-01-13", "title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought", "paper_summary": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.", "paper_summary_zh": "\u93c8\u5f0f\u601d\u8003 (CoT) \u63d0\u793a\u5df2\u88ab\u8b49\u5be6\u5c0d\u65bc\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e2d\u7684\u8907\u96dc\u63a8\u7406\u975e\u5e38\u6709\u6548\u3002\u7136\u800c\uff0c\u5b83\u5728\u8907\u96dc\u7684\u7a7a\u9593\u63a8\u7406\u4efb\u52d9\u4e2d\u537b\u5f88\u5403\u529b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u4eba\u985e\u7684\u8a8d\u77e5\u4e0d\u50c5\u9650\u65bc\u8a9e\u8a00\uff0c\u9084\u80fd\u4ee5\u8a00\u8a9e\u548c\u5f71\u50cf\u601d\u8003\u3002\u53d7\u5230\u6b64\u6a5f\u5236\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u63a8\u7406\u7bc4\u4f8b\uff0c\u5373\u591a\u6a21\u614b\u601d\u60f3\u8996\u89ba\u5316 (MVoT)\u3002\u5b83\u80fd\u8b93 MLLM \u7522\u751f\u63a8\u7406\u8ecc\u8de1\u7684\u5f71\u50cf\u8996\u89ba\u5316\uff0c\u5f9e\u800c\u5be6\u73fe\u8996\u89ba\u601d\u8003\u3002\u70ba\u4e86\u78ba\u4fdd\u9ad8\u54c1\u8cea\u7684\u8996\u89ba\u5316\uff0c\u6211\u5011\u5c07\u6a19\u8a18\u5dee\u7570\u640d\u5931\u5f15\u5165\u81ea\u8ff4\u6b78 MLLM\u3002\u9019\u9805\u5275\u65b0\u986f\u8457\u63d0\u5347\u4e86\u8996\u89ba\u4e00\u81f4\u6027\u548c\u4fdd\u771f\u5ea6\u3002\u6211\u5011\u900f\u904e\u591a\u9805\u52d5\u614b\u7a7a\u9593\u63a8\u7406\u4efb\u52d9\u9a57\u8b49\u4e86\u6b64\u65b9\u6cd5\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cMVoT \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u5728 CoT \u5931\u6548\u7684\u6700\u5177\u6311\u6230\u6027\u5834\u666f\u4e2d\uff0c\u5b83\u5c55\u73fe\u51fa\u5f37\u5065\u4e14\u53ef\u9760\u7684\u9032\u6b65\u3002\u6700\u7d42\uff0cMVoT \u70ba\u8996\u89ba\u601d\u8003\u80fd\u6709\u6548\u88dc\u5145\u8a9e\u8a00\u63a8\u7406\u7684\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u5efa\u7acb\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "author": "Chengzu Li et.al.", "authors": "Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli\u0107, Furu Wei", "id": "2501.07542v1", "paper_url": "http://arxiv.org/abs/2501.07542v1", "repo": "null"}}