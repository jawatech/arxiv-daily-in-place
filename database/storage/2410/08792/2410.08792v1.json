{"2410.08792": {"publish_time": "2024-10-11", "title": "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model", "paper_summary": "Vision Language Models (VLMs) have recently been adopted in robotics for\ntheir capability in common sense reasoning and generalizability. Existing work\nhas applied VLMs to generate task and motion planning from natural language\ninstructions and simulate training data for robot learning. In this work, we\nexplore using VLM to interpret human demonstration videos and generate robot\ntask planning. Our method integrates keyframe selection, visual perception, and\nVLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to\n''see'' human demonstrations and explain the corresponding plans to the robot\nfor it to ''do''. To validate our approach, we collected a set of long-horizon\nhuman videos demonstrating pick-and-place tasks in three diverse categories and\ndesigned a set of metrics to comprehensively benchmark SeeDo against several\nbaselines, including state-of-the-art video-input VLMs. The experiments\ndemonstrate SeeDo's superior performance. We further deployed the generated\ntask plans in both a simulation environment and on a real robot arm.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\u6700\u8fd1\u5df2\u5728\u6a5f\u5668\u4eba\u6280\u8853\u4e2d\u63a1\u7528\uff0c\u56e0\u5176\u5728\u5e38\u8b58\u63a8\u7406\u548c\u6982\u62ec\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002\u73fe\u6709\u5de5\u4f5c\u5df2\u5c07 VLM \u61c9\u7528\u65bc\u5f9e\u81ea\u7136\u8a9e\u8a00\u6307\u4ee4\u7522\u751f\u4efb\u52d9\u548c\u52d5\u4f5c\u898f\u5283\uff0c\u4e26\u6a21\u64ec\u6a5f\u5668\u4eba\u5b78\u7fd2\u7684\u8a13\u7df4\u8cc7\u6599\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4f7f\u7528 VLM \u4f86\u8a6e\u91cb\u4eba\u985e\u793a\u7bc4\u5f71\u7247\u4e26\u7522\u751f\u6a5f\u5668\u4eba\u4efb\u52d9\u898f\u5283\u3002\u6211\u5011\u7684\u6574\u5408\u95dc\u9375\u5f71\u683c\u9078\u64c7\u3001\u8996\u89ba\u611f\u77e5\u548c VLM \u63a8\u7406\u7684\u65b9\u6cd5\u5230\u4e00\u500b\u7ba1\u9053\u4e2d\u3002\u6211\u5011\u5c07\u5176\u547d\u540d\u70ba SeeDo\uff0c\u56e0\u70ba\u5b83\u4f7f VLM \u80fd\u5920\u300c\u770b\u898b\u300d\u4eba\u985e\u793a\u7bc4\uff0c\u4e26\u5411\u6a5f\u5668\u4eba\u89e3\u91cb\u5c0d\u61c9\u7684\u8a08\u756b\uff0c\u8b93\u6a5f\u5668\u4eba\u300c\u57f7\u884c\u300d\u3002\u70ba\u4e86\u9a57\u8b49\u6211\u5011\u7684\u505a\u6cd5\uff0c\u6211\u5011\u6536\u96c6\u4e86\u4e00\u7d44\u9577\u6642\u7a0b\u4eba\u985e\u5f71\u7247\uff0c\u5c55\u793a\u4e86\u4e09\u7a2e\u985e\u5225\u4e2d\u7684\u62fe\u53d6\u548c\u653e\u7f6e\u4efb\u52d9\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u7d44\u6307\u6a19\uff0c\u4ee5\u5168\u9762\u6bd4\u8f03 SeeDo \u8207\u5e7e\u500b\u57fa\u6e96\uff0c\u5305\u62ec\u6700\u5148\u9032\u7684\u5f71\u7247\u8f38\u5165 VLM\u3002\u5be6\u9a57\u8b49\u660e\u4e86 SeeDo \u7684\u5353\u8d8a\u6548\u80fd\u3002\u6211\u5011\u9032\u4e00\u6b65\u5728\u6a21\u64ec\u74b0\u5883\u548c\u771f\u5be6\u6a5f\u5668\u4eba\u624b\u81c2\u4e0a\u90e8\u7f72\u7522\u751f\u7684\u4efb\u52d9\u8a08\u756b\u3002", "author": "Beichen Wang et.al.", "authors": "Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, Chen Feng", "id": "2410.08792v1", "paper_url": "http://arxiv.org/abs/2410.08792v1", "repo": "null"}}