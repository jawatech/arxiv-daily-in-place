{"2409.03454": {"publish_time": "2024-09-05", "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes", "paper_summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.", "paper_summary_zh": "<paragraph>\u50c5\u89e3\u78bc\u7684 LLM \u5728\u6a5f\u5668\u7ffb\u8b6f\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u9a5a\u8c54\u7684\u6548\u80fd\uff0c\u56e0\u70ba\u5b83\u5011\u80fd\u5f9e\u5ee3\u6cdb\u7684\u8cc7\u6599\u96c6\u5b78\u7fd2\uff0c\u4e26\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u7ffb\u8b6f\u3002\u7136\u800c\uff0cLLM \u901a\u5e38\u96e3\u4ee5\u8655\u7406\u7d44\u7e54\u7279\u5b9a\u7ffb\u8b6f\u6240\u9700\u7684\u7d30\u5fae\u5dee\u5225\u548c\u98a8\u683c\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f Llama 3 8B Instruct\uff0c\u5229\u7528\u7ffb\u8b6f\u8a18\u61b6\u9ad4 (TM) \u4f5c\u70ba\u6709\u50f9\u503c\u7684\u8cc7\u6e90\uff0c\u4ee5\u63d0\u5347\u6e96\u78ba\u5ea6\u548c\u6548\u7387\u3002\u6211\u5011\u63a2\u8a0e\u4f7f\u7528\u4f86\u81ea\u8edf\u9ad4\u90e8\u9580\u7279\u5b9a\u7d44\u7e54\u7684 TM \u5fae\u8abf Llama 3 \u6a21\u578b\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u5be6\u9a57\u6db5\u84cb\u4e94\u7a2e\u7ffb\u8b6f\u65b9\u5411\uff0c\u6a6b\u8de8\u4e0d\u540c\u8cc7\u6e90\u5c64\u7d1a\u7684\u8a9e\u8a00\uff08\u82f1\u8a9e\u5230\u5df4\u897f\u8461\u8404\u7259\u8a9e\u3001\u6377\u514b\u8a9e\u3001\u5fb7\u8a9e\u3001\u82ac\u862d\u8a9e\u548c\u97d3\u8a9e\uff09\u3002\u6211\u5011\u5206\u6790\u4e0d\u540c\u5927\u5c0f\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\uff081k \u5230 207k \u6bb5\u843d\uff09\uff0c\u4ee5\u8a55\u4f30\u5176\u5c0d\u7ffb\u8b6f\u54c1\u8cea\u7684\u5f71\u97ff\u3002\u6211\u5011\u91dd\u5c0d\u6bcf\u500b\u8a13\u7df4\u96c6\u5fae\u8abf\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u4e26\u6839\u64da\u81ea\u52d5\u5316\u6307\u6a19\uff08BLEU\u3001chrF++\u3001TER \u548c COMET\uff09\u8a55\u4f30\u5176\u6548\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5728\u6240\u6709\u6307\u6a19\u4e2d\uff0c\u96a8\u8457\u8cc7\u6599\u96c6\u7684\u64f4\u5927\uff0c\u7ffb\u8b6f\u6548\u80fd\u90fd\u6709\u6240\u63d0\u5347\u3002\u5e73\u5747\u800c\u8a00\uff0c\u5728\u6700\u5927\u7684\u8a13\u7df4\u96c6\u4e0a\uff0c\u8207\u57fa\u6e96\u6a21\u578b\u76f8\u6bd4\uff0cBLEU \u548c COMET \u5206\u6578\u5206\u5225\u589e\u52a0\u4e86 13 \u548c 25 \u5206\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7576\u50c5\u91dd\u5c0d 1k \u548c 2k \u500b\u7bc4\u4f8b\u9032\u884c\u5fae\u8abf\u6642\uff0c\u8207\u57fa\u6e96\u6a21\u578b\u76f8\u6bd4\uff0c\u6548\u80fd\u6703\u4e0b\u964d\uff1b\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u96a8\u8457\u8a13\u7df4\u8cc7\u6599\u96c6\u5927\u5c0f\u7684\u589e\u52a0\uff0c\u6548\u80fd\u6709\u986f\u8457\u7684\u63d0\u5347\u3002\u9019\u9805\u7814\u7a76\u5f37\u8abf\u4e86\u5c07 TM \u8207 LLM \u6574\u5408\u4ee5\u5efa\u7acb\u5ba2\u88fd\u5316\u7ffb\u8b6f\u6a21\u578b\u7684\u6f5b\u529b\uff0c\u9019\u4e9b\u6a21\u578b\u5c08\u9580\u91dd\u5c0d\u4f01\u696d\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u5f9e\u800c\u63d0\u5347\u7ffb\u8b6f\u54c1\u8cea\u4e26\u7e2e\u77ed\u9031\u8f49\u6642\u9593\u3002\u6b64\u65b9\u6cd5\u70ba\u5c0b\u6c42\u5229\u7528 TM \u548c LLM \u4ee5\u7372\u5f97\u6700\u4f73\u7ffb\u8b6f\u6210\u679c\u7684\u7d44\u7e54\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u5c24\u5176\u662f\u5728\u8f03\u72f9\u7a84\u7684\u9818\u57df\u4e2d\u3002</paragraph>", "author": "Inacio Vieira et.al.", "authors": "Inacio Vieira, Will Allred, Seamus Lankford, Sheila Castilho Monteiro De Sousa, Andy Way", "id": "2409.03454v1", "paper_url": "http://arxiv.org/abs/2409.03454v1", "repo": "null"}}