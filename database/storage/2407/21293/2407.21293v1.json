{"2407.21293": {"publish_time": "2024-07-31", "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving", "paper_summary": "Many fields could benefit from the rapid development of the large language\nmodels (LLMs). The end-to-end autonomous driving (e2eAD) is one of the\ntypically fields facing new opportunities as the LLMs have supported more and\nmore modalities. Here, by utilizing vision-language model (VLM), we proposed an\ne2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided\ninto four stages, which are perception, prediction, planning, and behavior.\nEach stage consists of several visual question answering (VQA) pairs and VQA\npairs interconnect with each other constructing a graph called Graph VQA\n(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our\nmethod could achieve e2e driving with language. In our method, vision\ntransformers (ViT) models are employed to process nuScenes visual data, while\nVLM are utilized to interpret and reason about the information extracted from\nthe visual inputs. In the perception stage, the system identifies and\nclassifies objects from the driving environment. The prediction stage involves\nforecasting the potential movements of these objects. The planning stage\nutilizes the gathered information to develop a driving strategy, ensuring the\nsafety and efficiency of the autonomous vehicle. Finally, the behavior stage\ntranslates the planned actions into executable commands for the vehicle. Our\nexperiments demonstrate that SimpleLLM4AD achieves competitive performance in\ncomplex driving scenarios.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\u53ef\u80fd\u4f7f\u8a31\u591a\u9818\u57df\u53d7\u76ca\u3002\u7aef\u5230\u7aef\u81ea\u52d5\u99d5\u99db (e2eAD) \u662f\u5178\u578b\u9818\u57df\u4e4b\u4e00\uff0c\u56e0\u70ba LLM \u652f\u63f4\u8d8a\u4f86\u8d8a\u591a\u7684\u6a21\u5f0f\uff0c\u56e0\u6b64\u9762\u81e8\u65b0\u7684\u6a5f\u6703\u3002\u5728\u6b64\uff0c\u900f\u904e\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7a31\u70ba SimpleLLM4AD \u7684 e2eAD \u65b9\u6cd5\u3002\u5728\u6211\u5011\u7684\u6a21\u578b\u4e2d\uff0ce2eAD \u4efb\u52d9\u5206\u70ba\u56db\u500b\u968e\u6bb5\uff0c\u5206\u5225\u662f\u611f\u77e5\u3001\u9810\u6e2c\u3001\u898f\u5283\u548c\u884c\u70ba\u3002\u6bcf\u500b\u968e\u6bb5\u5305\u542b\u591a\u500b\u8996\u89ba\u554f\u7b54 (VQA) \u914d\u5c0d\uff0c\u4e14 VQA \u914d\u5c0d\u76f8\u4e92\u9023\u63a5\uff0c\u69cb\u5efa\u4e00\u500b\u7a31\u70ba\u5716\u5f62 VQA (GVQA) \u7684\u5716\u5f62\u3002\u900f\u904e VLM \u5206\u968e\u6bb5\u63a8\u7406 GVQA \u4e2d\u7684\u6bcf\u500b VQA \u914d\u5c0d\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u900f\u904e\u8a9e\u8a00\u5be6\u73fe\u7aef\u5230\u7aef\u99d5\u99db\u3002\u5728\u6211\u5011\u7684\u6a21\u578b\u4e2d\uff0c\u63a1\u7528\u8996\u89baTransformer (ViT) \u6a21\u578b\u4f86\u8655\u7406 nuScenes \u8996\u89ba\u8cc7\u6599\uff0c\u540c\u6642\u5229\u7528 VLM \u4f86\u8a6e\u91cb\u548c\u63a8\u7406\u5f9e\u8996\u89ba\u8f38\u5165\u4e2d\u63d0\u53d6\u7684\u8cc7\u8a0a\u3002\u5728\u611f\u77e5\u968e\u6bb5\uff0c\u7cfb\u7d71\u8b58\u5225\u548c\u5206\u985e\u99d5\u99db\u74b0\u5883\u4e2d\u7684\u7269\u4ef6\u3002\u9810\u6e2c\u968e\u6bb5\u6d89\u53ca\u9810\u6e2c\u9019\u4e9b\u7269\u4ef6\u7684\u6f5b\u5728\u79fb\u52d5\u3002\u898f\u5283\u968e\u6bb5\u5229\u7528\u6536\u96c6\u7684\u8cc7\u8a0a\u4f86\u5236\u5b9a\u99d5\u99db\u7b56\u7565\uff0c\u78ba\u4fdd\u81ea\u52d5\u99d5\u99db\u6c7d\u8eca\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002\u6700\u5f8c\uff0c\u884c\u70ba\u968e\u6bb5\u5c07\u898f\u5283\u7684\u52d5\u4f5c\u8f49\u63db\u70ba\u8eca\u8f1b\u53ef\u57f7\u884c\u7684\u547d\u4ee4\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0cSimpleLLM4AD \u5728\u8907\u96dc\u7684\u99d5\u99db\u5834\u666f\u4e2d\u5be6\u73fe\u4e86\u7af6\u722d\u529b\u3002", "author": "Peiru Zheng et.al.", "authors": "Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu", "id": "2407.21293v1", "paper_url": "http://arxiv.org/abs/2407.21293v1", "repo": "null"}}