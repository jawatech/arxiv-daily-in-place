{"2407.18552": {"publish_time": "2024-07-26", "title": "Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention", "paper_summary": "Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.", "paper_summary_zh": "\u7406\u89e3\u60c5\u7dd2\u662f\u4eba\u985e\u6e9d\u901a\u7684\u57fa\u672c\u9762\u5411\u3002\n\u76f8\u8f03\u65bc\u4f9d\u8cf4\u55ae\u4e00\u8cc7\u6599\u4f86\u6e90\uff08\u4f8b\u5982\u8a9e\u97f3\u6216\u9762\u90e8\u8868\u60c5\uff09\u7684\u50b3\u7d71\u65b9\u6cd5\uff0c\u6574\u5408\u97f3\u8a0a\u548c\u8996\u8a0a\u8a0a\u865f\u80fd\u63d0\u4f9b\u5c0d\u60c5\u7dd2\u72c0\u614b\u66f4\u5168\u9762\u7684\u7406\u89e3\u3002\u5118\u7ba1\u5177\u6709\u6f5b\u529b\uff0c\u591a\u6a21\u614b\u60c5\u7dd2\u8fa8\u8b58\u4ecd\u9762\u81e8\u91cd\u5927\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u540c\u6b65\u5316\u3001\u7279\u5fb5\u8403\u53d6\u548c\u4e0d\u540c\u8cc7\u6599\u4f86\u6e90\u7684\u878d\u5408\u65b9\u9762\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u540d\u70ba\u97f3\u8a0a\u8996\u8a0a\u8f49\u63db\u5668\u878d\u5408\u5e36\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u65b0\u7a4e\u8f49\u63db\u5668\u6a21\u578b\uff08AVT-CA\uff09\u3002AVT-CA \u6a21\u578b\u63a1\u7528\u8f49\u63db\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u64f7\u53d6\u548c\u540c\u6b65\u4f86\u81ea\u97f3\u8a0a\u548c\u8996\u8a0a\u8f38\u5165\u7684\u76f8\u4e92\u9023\u7d50\u7279\u5fb5\uff0c\u5f9e\u800c\u89e3\u6c7a\u540c\u6b65\u5316\u554f\u984c\u3002\u6b64\u5916\uff0cAVT-CA \u5167\u90e8\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a5f\u5236\u6703\u9078\u64c7\u6027\u5730\u8403\u53d6\u548c\u5f37\u8abf\u95dc\u9375\u7279\u5fb5\uff0c\u540c\u6642\u6368\u68c4\u4f86\u81ea\u5169\u7a2e\u6a21\u614b\u7684\u4e0d\u76f8\u95dc\u7279\u5fb5\uff0c\u89e3\u6c7a\u4e86\u7279\u5fb5\u8403\u53d6\u548c\u878d\u5408\u7684\u6311\u6230\u3002\u5728 CMU-MOSEI\u3001RAVDESS \u548c CREMA-D \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u5206\u6790\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6548\u80fd\u3002\u7d50\u679c\u5f37\u8abf\u4e86 AVT-CA \u5728\u958b\u767c\u7cbe\u78ba\u4e14\u53ef\u9760\u7684\u591a\u6a21\u614b\u60c5\u7dd2\u8fa8\u8b58\u7cfb\u7d71\u4ee5\u4f9b\u5be6\u969b\u61c9\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002", "author": "Joe Dhanith P R et.al.", "authors": "Joe Dhanith P R, Shravan Venkatraman, Vigya Sharma, Santhosh Malarvannan", "id": "2407.18552v1", "paper_url": "http://arxiv.org/abs/2407.18552v1", "repo": "null"}}