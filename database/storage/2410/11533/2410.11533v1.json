{"2410.11533": {"publish_time": "2024-10-15", "title": "Multi-round jailbreak attack on large language models", "paper_summary": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters.", "paper_summary_zh": "\u78ba\u4fdd\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5b89\u5168\u6027\u548c\u8207\u4eba\u985e\u50f9\u503c\u89c0\u7684\u4e00\u81f4\u6027\u5c0d\u65bc\u7522\u751f\u5c0d\u4eba\u985e\u6709\u76ca\u7684\u56de\u61c9\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136 LLM \u6709\u80fd\u529b\u8b58\u5225\u548c\u907f\u514d\u6709\u5bb3\u67e5\u8a62\uff0c\u4f46\u5b83\u5011\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u300c\u8d8a\u7344\u300d\u653b\u64ca\uff0c\u7cbe\u5fc3\u88fd\u4f5c\u7684\u63d0\u793a\u53ef\u80fd\u6703\u8a98\u767c\u6709\u6bd2\u5167\u5bb9\u7684\u7522\u751f\u3002\u50b3\u7d71\u7684\u55ae\u8f2a\u8d8a\u7344\u653b\u64ca\uff0c\u4f8b\u5982 GCG \u548c AutoDAN\uff0c\u4e0d\u6703\u6539\u8b8a\u5371\u96aa\u63d0\u793a\u4e2d\u7684\u654f\u611f\u8a5e\u3002\u5118\u7ba1\u4ed6\u5011\u53ef\u4ee5\u901a\u904e\u63d0\u793a\u5de5\u7a0b\u66ab\u6642\u7e5e\u904e\u6a21\u578b\u7684\u9632\u8b77\u63aa\u65bd\uff0c\u4f46\u96a8\u8457 LLM \u9032\u4e00\u6b65\u5fae\u8abf\uff0c\u4ed6\u5011\u7684\u6210\u529f\u7387\u6703\u986f\u8457\u4e0b\u964d\uff0c\u800c\u4e14\u4ed6\u5011\u7121\u6cd5\u6709\u6548\u898f\u907f\u522a\u9664\u5371\u96aa\u8a5e\u5f59\u7684\u975c\u614b\u57fa\u65bc\u898f\u5247\u7684\u904e\u6ffe\u5668\u3002\u5728\u672c\u6b21\u7814\u7a76\u4e2d\uff0c\u70ba\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8d8a\u7344\u653b\u64ca\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u591a\u8f2a\u8d8a\u7344\u65b9\u6cd5\u3002\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u6539\u5beb\u5371\u96aa\u63d0\u793a\uff0c\u5c07\u5b83\u5011\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u5371\u5bb3\u8f03\u5c0f\u7684\u5b50\u554f\u984c\uff0c\u4ee5\u7e5e\u904e LLM \u7684\u5b89\u5168\u6aa2\u67e5\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528 LLM \u57f7\u884c\u5206\u89e3\u4efb\u52d9\uff0c\u5c07\u4e00\u7d44\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u6f38\u9032\u7684\u5b50\u554f\u984c\uff0c\u7136\u5f8c\u7528\u65bc\u5fae\u8abf Llama3-8B \u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5920\u5206\u89e3\u5371\u96aa\u63d0\u793a\u3002\u7136\u5f8c\u4f7f\u7528\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u4f86\u5206\u89e3\u6709\u554f\u984c\u7684\u63d0\u793a\uff0c\u4e26\u5c07\u7522\u751f\u7684\u5b50\u554f\u984c\u6309\u9806\u5e8f\u8a62\u554f\u53d7\u5bb3\u8005\u6a21\u578b\u3002\u5982\u679c\u53d7\u5bb3\u8005\u6a21\u578b\u62d2\u7d55\u4e00\u500b\u5b50\u554f\u984c\uff0c\u5247\u6703\u751f\u6210\u4e00\u500b\u65b0\u7684\u5206\u89e3\uff0c\u4e26\u4e14\u91cd\u8907\u9019\u500b\u904e\u7a0b\uff0c\u76f4\u5230\u6700\u7d42\u76ee\u6a19\u9054\u6210\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\u5728 llama2-7B \u4e0a\u7684\u6210\u529f\u7387\u70ba 94%\uff0c\u4e26\u8b49\u660e\u4e86\u9019\u7a2e\u65b9\u6cd5\u5728\u898f\u907f\u975c\u614b\u57fa\u65bc\u898f\u5247\u7684\u904e\u6ffe\u5668\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Yihua Zhou et.al.", "authors": "Yihua Zhou, Xiaochuan Shi", "id": "2410.11533v1", "paper_url": "http://arxiv.org/abs/2410.11533v1", "repo": "null"}}