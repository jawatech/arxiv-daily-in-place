{"2501.01426": {"publish_time": "2025-01-02", "title": "Unifying Specialized Visual Encoders for Video Language Models", "paper_summary": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u554f\u4e16\uff0c\u900f\u904e\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b (VideoLLM) \u5c07\u7cbe\u5bc6\u7684\u63a8\u7406\u80fd\u529b\u5f15\u9032\u5f71\u7247\u9818\u57df\u3002\u7136\u800c\uff0cVideoLLM \u76ee\u524d\u4f9d\u8cf4\u55ae\u4e00\u8996\u89ba\u7de8\u78bc\u5668\u8655\u7406\u6240\u6709\u8996\u89ba\uff0c\u9019\u9650\u5236\u4e86\u50b3\u9054\u7d66 LLM \u7684\u8996\u89ba\u8cc7\u8a0a\u91cf\u548c\u985e\u578b\u3002\u6211\u5011\u7684 MERV \u65b9\u6cd5\uff0c\u5f71\u7247\u7684\u591a\u7de8\u78bc\u5668\u8868\u5fb5\uff0c\u6539\u7528\u591a\u500b\u51cd\u7d50\u7684\u8996\u89ba\u7de8\u78bc\u5668\u4f86\u5efa\u7acb\u5f71\u7247\u7684\u7d71\u4e00\u8868\u5fb5\uff0c\u70ba VideoLLM \u63d0\u4f9b\u4e00\u5957\u5168\u9762\u7684\u5c08\u696d\u8996\u89ba\u77e5\u8b58\u3002\u900f\u904e\u7a7a\u9593\u6642\u9593\u5c0d\u9f4a\u6bcf\u500b\u7de8\u78bc\u5668\u7684\u7279\u5fb5\uff0c\u8b93\u6211\u5011\u80fd\u5920\u8655\u7406\u66f4\u5ee3\u6cdb\u7684\u958b\u653e\u5f0f\u548c\u591a\u9078\u9805\u5f71\u7247\u7406\u89e3\u554f\u984c\uff0c\u4e26\u8d85\u8d8a\u5148\u524d\u7684\u6700\u5148\u9032\u6280\u8853\u3002\u5728\u6a19\u6e96\u5957\u4ef6\u5f71\u7247\u7406\u89e3\u57fa\u6e96\u4e0a\uff0cMERV \u7684\u6e96\u78ba\u5ea6\u6bd4 Video-LLaVA \u9ad8\u9054 3.7%\uff0c\u540c\u6642\u4e5f\u6709\u66f4\u597d\u7684 Video-ChatGPT \u5206\u6578\u3002\u6211\u5011\u4e5f\u6539\u9032\u4e86 SeViLA\uff0c\u5728\u96f6\u6b21\u5b78\u7fd2\u611f\u77e5\u6e2c\u8a66\u6e96\u78ba\u5ea6\u4e0a\uff0c\u6bd4\u524d\u4e00\u500b\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa 2.2%\u3002MERV \u5f15\u5165\u6700\u5c0f\u7684\u984d\u5916\u53c3\u6578\uff0c\u4e26\u6bd4\u7b49\u6548\u7684\u55ae\u4e00\u7de8\u78bc\u5668\u65b9\u6cd5\u8a13\u7df4\u5f97\u66f4\u5feb\uff0c\u540c\u6642\u4e26\u884c\u8655\u7406\u8996\u89ba\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u4f9b\u5b9a\u6027\u7684\u8b49\u64da\uff0c\u8b49\u660e MERV \u6210\u529f\u5f9e\u6bcf\u500b\u7de8\u78bc\u5668\u64f7\u53d6\u9818\u57df\u77e5\u8b58\u3002\u6211\u5011\u7684\u7d50\u679c\u70ba\u5229\u7528\u591a\u500b\u8996\u89ba\u7de8\u78bc\u5668\u9032\u884c\u5168\u9762\u7684\u5f71\u7247\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "author": "Jihoon Chung et.al.", "authors": "Jihoon Chung, Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou, Olga Russakovsky", "id": "2501.01426v1", "paper_url": "http://arxiv.org/abs/2501.01426v1", "repo": "null"}}