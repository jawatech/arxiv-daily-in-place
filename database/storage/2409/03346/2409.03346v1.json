{"2409.03346": {"publish_time": "2024-09-05", "title": "Sketch: A Toolkit for Streamlining LLM Operations", "paper_summary": "Large language models (LLMs) represented by GPT family have achieved\nremarkable success. The characteristics of LLMs lie in their ability to\naccommodate a wide range of tasks through a generative approach. However, the\nflexibility of their output format poses challenges in controlling and\nharnessing the model's outputs, thereby constraining the application of LLMs in\nvarious domains. In this work, we present Sketch, an innovative toolkit\ndesigned to streamline LLM operations across diverse fields. Sketch comprises\nthe following components: (1) a suite of task description schemas and prompt\ntemplates encompassing various NLP tasks; (2) a user-friendly, interactive\nprocess for building structured output LLM services tailored to various NLP\ntasks; (3) an open-source dataset for output format control, along with tools\nfor dataset construction; and (4) an open-source model based on\nLLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting\ninstructions. We anticipate this initiative to bring considerable convenience\nto LLM users, achieving the goal of ''plug-and-play'' for various applications.\nThe components of Sketch will be progressively open-sourced at\nhttps://github.com/cofe-ai/Sketch.", "paper_summary_zh": "\u7531 GPT \u5bb6\u65cf\u4ee3\u8868\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\u3002LLM \u7684\u7279\u9ede\u5728\u65bc\u5b83\u5011\u80fd\u5920\u900f\u904e\u751f\u6210\u5f0f\u65b9\u6cd5\u4f86\u9069\u61c9\u5ee3\u6cdb\u7684\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u8f38\u51fa\u683c\u5f0f\u9748\u6d3b\u6027\u5728\u63a7\u5236\u548c\u5229\u7528\u6a21\u578b\u8f38\u51fa\u65b9\u9762\u69cb\u6210\u4e86\u6311\u6230\uff0c\u5f9e\u800c\u9650\u5236\u4e86 LLM \u5728\u5404\u7a2e\u9818\u57df\u4e2d\u7684\u61c9\u7528\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 Sketch\uff0c\u9019\u662f\u4e00\u500b\u5275\u65b0\u7684\u5de5\u5177\u5305\uff0c\u65e8\u5728\u7c21\u5316 LLM \u5728\u4e0d\u540c\u9818\u57df\u4e2d\u7684\u64cd\u4f5c\u3002Sketch \u5305\u542b\u4ee5\u4e0b\u7d44\u4ef6\uff1a(1) \u4e00\u5957\u4efb\u52d9\u63cf\u8ff0\u67b6\u69cb\u548c\u63d0\u793a\u7bc4\u672c\uff0c\u6db5\u84cb\u5404\u7a2e NLP \u4efb\u52d9\uff1b(2) \u4e00\u500b\u53cb\u5584\u3001\u4e92\u52d5\u7684\u6d41\u7a0b\uff0c\u7528\u65bc\u5efa\u7acb\u91dd\u5c0d\u5404\u7a2e NLP \u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684\u7d50\u69cb\u5316\u8f38\u51fa LLM \u670d\u52d9\uff1b(3) \u4e00\u500b\u7528\u65bc\u8f38\u51fa\u683c\u5f0f\u63a7\u5236\u7684\u958b\u6e90\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca\u7528\u65bc\u8cc7\u6599\u96c6\u5efa\u69cb\u7684\u5de5\u5177\uff1b(4) \u4e00\u500b\u57fa\u65bc LLaMA3-8B-Instruct \u7684\u958b\u6e90\u6a21\u578b\uff0c\u5b83\u80fd\u9748\u6d3b\u5730\u7406\u89e3\u4e26\u9075\u5b88\u8f38\u51fa\u683c\u5f0f\u5316\u6307\u793a\u3002\u6211\u5011\u9810\u671f\u9019\u500b\u8a08\u756b\u5c07\u70ba LLM \u4f7f\u7528\u8005\u5e36\u4f86\u6975\u5927\u7684\u4fbf\u5229\u6027\uff0c\u5be6\u73fe\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u7684\u300c\u5373\u63d2\u5373\u7528\u300d\u76ee\u6a19\u3002Sketch \u7684\u7d44\u4ef6\u5c07\u5728 https://github.com/cofe-ai/Sketch \u9010\u6b65\u958b\u653e\u539f\u59cb\u78bc\u3002", "author": "Xin Jiang et.al.", "authors": "Xin Jiang, Xiang Li, Wenjia Ma, Xuezhi Fang, Yiqun Yao, Naitong Yu, Xuying Meng, Peng Han, Jing Li, Aixin Sun, Yequan Wang", "id": "2409.03346v1", "paper_url": "http://arxiv.org/abs/2409.03346v1", "repo": "null"}}