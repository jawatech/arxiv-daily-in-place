{"2411.18021": {"publish_time": "2024-11-27", "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?", "paper_summary": "Over the past few decades, Artificial Intelligence(AI) has progressed from\nthe initial machine learning stage to the deep learning stage, and now to the\nstage of foundational models. Foundational models have the characteristics of\npre-training, transfer learning, and self-supervised learning, and pre-trained\nmodels can be fine-tuned and applied to various downstream tasks. Under the\nframework of foundational models, models such as Bidirectional Encoder\nRepresentations from Transformers(BERT) and Generative Pre-trained\nTransformer(GPT) have greatly advanced the development of natural language\nprocessing(NLP), especially the emergence of many models based on BERT. BERT\nbroke through the limitation of only using one-way methods for language\nmodeling in pre-training by using a masked language model. It can capture\nbidirectional context information to predict the masked words in the sequence,\nthis can improve the feature extraction ability of the model. This makes the\nmodel very useful for downstream tasks, especially for specialized\napplications. The model using the bidirectional encoder can better understand\nthe domain knowledge and be better applied to these downstream tasks. So we\nhope to help understand how this technology has evolved and improved model\nperformance in various natural language processing tasks under the background\nof foundational models and reveal its importance in capturing context\ninformation and improving the model's performance on downstream tasks. This\narticle analyzes one-way and bidirectional models based on GPT and BERT and\ncompares their differences based on the purpose of the model. It also briefly\nanalyzes BERT and the improvements of some models based on BERT. The model's\nperformance on the Stanford Question Answering Dataset(SQuAD) and General\nLanguage Understanding Evaluation(GLUE) was compared.", "paper_summary_zh": "<paragraph>\u5728\u904e\u53bb\u7684\u5e7e\u5341\u5e74\u4e2d\uff0c\u4eba\u5de5\u667a\u6167 (AI) \u5df2\u5f9e\u6700\u521d\u7684\u6a5f\u5668\u5b78\u7fd2\u968e\u6bb5\u9032\u6b65\u5230\u6df1\u5ea6\u5b78\u7fd2\u968e\u6bb5\uff0c\u73fe\u5728\u9032\u5165\u4e86\u57fa\u790e\u6a21\u578b\u968e\u6bb5\u3002\u57fa\u790e\u6a21\u578b\u5177\u6709\u9810\u8a13\u7df4\u3001\u9077\u79fb\u5b78\u7fd2\u548c\u81ea\u76e3\u7763\u5b78\u7fd2\u7684\u7279\u5fb5\uff0c\u4e26\u4e14\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u4ee5\u5fae\u8abf\u4e26\u61c9\u7528\u65bc\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u3002\u5728\u57fa\u790e\u6a21\u578b\u7684\u6846\u67b6\u4e0b\uff0c\u4f8b\u5982 Transformer \u7684\u96d9\u5411\u7de8\u78bc\u5668\u8868\u793a (BERT) \u548c\u751f\u6210\u5f0f\u9810\u8a13\u7df4 Transformer (GPT) \u7b49\u6a21\u578b\u5df2\u7d93\u6975\u5927\u5730\u63a8\u52d5\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7684\u767c\u5c55\uff0c\u5c24\u5176\u662f\u57fa\u65bc BERT \u7684\u8a31\u591a\u6a21\u578b\u7684\u51fa\u73fe\u3002BERT \u900f\u904e\u4f7f\u7528\u906e\u7f69\u8a9e\u8a00\u6a21\u578b\u7a81\u7834\u4e86\u5728\u9810\u8a13\u7df4\u4e2d\u50c5\u4f7f\u7528\u55ae\u5411\u65b9\u6cd5\u9032\u884c\u8a9e\u8a00\u5efa\u6a21\u7684\u9650\u5236\u3002\u5b83\u53ef\u4ee5\u64f7\u53d6\u96d9\u5411\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u4f86\u9810\u6e2c\u5e8f\u5217\u4e2d\u7684\u906e\u7f69\u5b57\u8a5e\uff0c\u9019\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7279\u5fb5\u63d0\u53d6\u80fd\u529b\u3002\u9019\u4f7f\u5f97\u8a72\u6a21\u578b\u5c0d\u65bc\u4e0b\u6e38\u4efb\u52d9\u975e\u5e38\u6709\u7528\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u5c08\u696d\u61c9\u7528\u3002\u4f7f\u7528\u96d9\u5411\u7de8\u78bc\u5668\u7684\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u9818\u57df\u77e5\u8b58\uff0c\u4e26\u66f4\u597d\u5730\u61c9\u7528\u65bc\u9019\u4e9b\u4e0b\u6e38\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5e0c\u671b\u5354\u52a9\u4e86\u89e3\u5728\u57fa\u790e\u6a21\u578b\u7684\u80cc\u666f\u4e0b\uff0c\u9019\u9805\u6280\u8853\u5982\u4f55\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u6f14\u9032\u4e26\u63d0\u5347\u6a21\u578b\u6548\u80fd\uff0c\u4e26\u63ed\u793a\u5176\u5728\u64f7\u53d6\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u548c\u63d0\u5347\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u57fa\u65bc GPT \u548c BERT \u5206\u6790\u55ae\u5411\u548c\u96d9\u5411\u6a21\u578b\uff0c\u4e26\u6839\u64da\u6a21\u578b\u7684\u76ee\u7684\u6bd4\u8f03\u5b83\u5011\u7684\u5dee\u7570\u3002\u5b83\u4e5f\u7c21\u8981\u5206\u6790 BERT \u548c\u4e00\u4e9b\u57fa\u65bc BERT \u7684\u6a21\u578b\u7684\u6539\u9032\u3002\u6bd4\u8f03\u4e86\u8a72\u6a21\u578b\u5728\u53f2\u4e39\u4f5b\u554f\u7b54\u8cc7\u6599\u96c6 (SQuAD) \u548c\u901a\u7528\u8a9e\u8a00\u7406\u89e3\u8a55\u4f30 (GLUE) \u4e0a\u7684\u6548\u80fd\u3002</paragraph>", "author": "Lewen Yang et.al.", "authors": "Lewen Yang, Xuanyu Zhou, Juao Fan, Xinyi Xie, Shengxin Zhu", "id": "2411.18021v1", "paper_url": "http://arxiv.org/abs/2411.18021v1", "repo": "null"}}