{"2408.10111": {"publish_time": "2024-08-19", "title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities", "paper_summary": "Financial time series modeling is crucial for understanding and predicting\nmarket behaviors but faces challenges such as non-linearity, non-stationarity,\nand high noise levels. Traditional models struggle to capture complex patterns\ndue to these issues, compounded by limitations in computational resources and\nmodel capacity. Inspired by the success of large language models in NLP, we\nintroduce $\\textbf{PLUTUS}$, a $\\textbf{P}$re-trained $\\textbf{L}$arge\n$\\textbf{U}$nified $\\textbf{T}$ransformer-based model that $\\textbf{U}$nveils\nregularities in financial time $\\textbf{S}$eries. PLUTUS uses an invertible\nembedding module with contrastive learning and autoencoder techniques to create\nan approximate one-to-one mapping between raw data and patch embeddings.\nTimeFormer, an attention based architecture, forms the core of PLUTUS,\neffectively modeling high-noise time series. We incorporate a novel attention\nmechanisms to capture features across both variable and temporal dimensions.\nPLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,\ndesigned to thrive in noisy financial environments. To our knowledge, PLUTUS is\nthe first open-source, large-scale, pre-trained financial time series model\nwith over one billion parameters. It achieves state-of-the-art performance in\nvarious tasks, demonstrating strong transferability and establishing a robust\nfoundational model for finance. Our research provides technical guidance for\npre-training financial time series data, setting a new standard in the field.", "paper_summary_zh": "<paragraph>\u8ca1\u52d9\u6642\u9593\u5e8f\u5217\u5efa\u6a21\u5c0d\u65bc\u7406\u89e3\u548c\u9810\u6e2c\u5e02\u5834\u884c\u70ba\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u9762\u81e8\u975e\u7dda\u6027\u3001\u975e\u5e73\u7a69\u548c\u9ad8\u96dc\u8a0a\u7b49\u7d1a\u7b49\u6311\u6230\u3002\u7531\u65bc\u9019\u4e9b\u554f\u984c\uff0c\u50b3\u7d71\u6a21\u578b\u96e3\u4ee5\u6355\u6349\u8907\u96dc\u6a21\u5f0f\uff0c\u4e26\u53d7\u5230\u8a08\u7b97\u8cc7\u6e90\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u9650\u5236\u3002\u53d7\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728 NLP \u4e2d\u6210\u529f\u7684\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e86 $\\textbf{PLUTUS}$\uff0c\u4e00\u500b $\\textbf{P}$re-trained $\\textbf{L}$arge $\\textbf{U}$nified $\\textbf{T}$ransformer-based \u6a21\u578b\uff0c\u5b83 $\\textbf{U}$nveils \u8ca1\u52d9\u6642\u9593 $\\textbf{S}$eries \u4e2d\u7684\u898f\u5f8b\u6027\u3002PLUTUS \u4f7f\u7528\u53ef\u9006\u5d4c\u5165\u6a21\u7d44\uff0c\u7d50\u5408\u5c0d\u6bd4\u5b78\u7fd2\u548c\u81ea\u52d5\u7de8\u78bc\u5668\u6280\u8853\uff0c\u5728\u539f\u59cb\u8cc7\u6599\u548c\u8cbc\u7247\u5d4c\u5165\u4e4b\u9593\u5efa\u7acb\u8fd1\u4f3c\u7684\u4e00\u5c0d\u4e00\u5c0d\u61c9\u3002TimeFormer \u662f\u4e00\u500b\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u67b6\u69cb\uff0c\u69cb\u6210 PLUTUS \u7684\u6838\u5fc3\uff0c\u6709\u6548\u5730\u5c0d\u9ad8\u96dc\u8a0a\u6642\u9593\u5e8f\u5217\u9032\u884c\u5efa\u6a21\u3002\u6211\u5011\u7d0d\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4ee5\u6355\u6349\u8b8a\u6578\u548c\u6642\u9593\u7dad\u5ea6\u4e2d\u7684\u7279\u5fb5\u3002PLUTUS \u5728\u4e00\u500b\u524d\u6240\u672a\u6709\u7684 1000 \u5104\u500b\u89c0\u6e2c\u503c\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u65e8\u5728\u5728\u5608\u96dc\u7684\u91d1\u878d\u74b0\u5883\u4e2d\u84ec\u52c3\u767c\u5c55\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cPLUTUS \u662f\u7b2c\u4e00\u500b\u958b\u6e90\u3001\u5927\u898f\u6a21\u3001\u9810\u8a13\u7df4\u7684\u8ca1\u52d9\u6642\u9593\u5e8f\u5217\u6a21\u578b\uff0c\u64c1\u6709\u8d85\u904e 10 \u5104\u500b\u53c3\u6578\u3002\u5b83\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5c55\u793a\u4e86\u5f37\u5927\u7684\u53ef\u8f49\u79fb\u6027\uff0c\u4e26\u70ba\u91d1\u878d\u5efa\u7acb\u4e86\u4e00\u500b\u5f37\u5927\u7684\u57fa\u790e\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u70ba\u9810\u8a13\u7df4\u8ca1\u52d9\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u63d0\u4f9b\u4e86\u6280\u8853\u6307\u5c0e\uff0c\u70ba\u8a72\u9818\u57df\u6a39\u7acb\u4e86\u65b0\u7684\u6a19\u6e96\u3002</paragraph>", "author": "Yuanjian Xu et.al.", "authors": "Yuanjian Xu, Anxian Liu, Jianing Hao, Zhenzhuo Li, Shichang Meng, Guang Zhang", "id": "2408.10111v2", "paper_url": "http://arxiv.org/abs/2408.10111v2", "repo": "null"}}