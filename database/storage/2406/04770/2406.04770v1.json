{"2406.04770": {"publish_time": "2024-06-07", "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild", "paper_summary": "We introduce WildBench, an automated evaluation framework designed to\nbenchmark large language models (LLMs) using challenging, real-world user\nqueries. WildBench consists of 1,024 tasks carefully selected from over one\nmillion human-chatbot conversation logs. For automated evaluation with\nWildBench, we have developed two metrics, WB-Reward and WB-Score, which are\ncomputable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses\ntask-specific checklists to evaluate model outputs systematically and provides\nstructured explanations that justify the scores and comparisons, resulting in\nmore reliable and interpretable automatic judgments. WB-Reward employs\nfine-grained pairwise comparisons between model responses, generating five\npotential outcomes: much better, slightly better, slightly worse, much worse,\nor a tie. Unlike previous evaluations that employed a single baseline model, we\nselected three baseline models at varying performance levels to ensure a\ncomprehensive pairwise evaluation. Additionally, we propose a simple method to\nmitigate length bias, by converting outcomes of ``slightly better/worse'' to\n``tie'' if the winner response exceeds the loser one by more than $K$\ncharacters. WB-Score evaluates the quality of model outputs individually,\nmaking it a fast and cost-efficient evaluation metric. WildBench results\ndemonstrate a strong correlation with the human-voted Elo ratings from Chatbot\nArena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of\n0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing\nboth ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates,\nas well as the 0.87 for regular win rates.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa WildBench\uff0c\u4e00\u500b\u81ea\u52d5\u5316\u8a55\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u4f7f\u7528\u5177\u6709\u6311\u6230\u6027\u7684\u771f\u5be6\u4e16\u754c\u4f7f\u7528\u8005\u67e5\u8a62\u4f86\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002WildBench \u5305\u542b 1,024 \u9805\u4efb\u52d9\uff0c\u5f9e\u8d85\u904e\u4e00\u767e\u842c\u7b46\u4eba\u6a5f\u5c0d\u8a71\u8a18\u9304\u4e2d\u4ed4\u7d30\u6311\u9078\u3002\u5c0d\u65bc\u4f7f\u7528 WildBench \u9032\u884c\u81ea\u52d5\u5316\u8a55\u4f30\uff0c\u6211\u5011\u958b\u767c\u4e86\u5169\u500b\u6307\u6a19\uff0cWB-Reward \u548c WB-Score\uff0c\u53ef\u4ee5\u4f7f\u7528 GPT-4-turbo \u7b49\u5148\u9032 LLM \u9032\u884c\u904b\u7b97\u3002WildBench \u8a55\u4f30\u4f7f\u7528\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6aa2\u67e5\u6e05\u55ae\u4f86\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u6a21\u578b\u8f38\u51fa\uff0c\u4e26\u63d0\u4f9b\u7d50\u69cb\u5316\u7684\u8aaa\u660e\u4f86\u8b49\u660e\u8a55\u5206\u548c\u6bd4\u8f03\uff0c\u5f9e\u800c\u7522\u751f\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91cb\u7684\u81ea\u52d5\u5224\u65b7\u3002WB-Reward \u63a1\u7528\u6a21\u578b\u56de\u61c9\u4e4b\u9593\u7684\u7d30\u7dfb\u6210\u5c0d\u6bd4\u8f03\uff0c\u7522\u751f\u4e94\u7a2e\u6f5b\u5728\u7d50\u679c\uff1a\u597d\u5f88\u591a\u3001\u7a0d\u597d\u3001\u7a0d\u5dee\u3001\u5dee\u5f88\u591a\u6216\u5e73\u624b\u3002\u8207\u63a1\u7528\u55ae\u4e00\u57fa\u6e96\u6a21\u578b\u7684\u5148\u524d\u8a55\u4f30\u4e0d\u540c\uff0c\u6211\u5011\u9078\u64c7\u4e86\u4e09\u500b\u6548\u80fd\u7b49\u7d1a\u4e0d\u540c\u7684\u57fa\u6e96\u6a21\u578b\uff0c\u4ee5\u78ba\u4fdd\u5168\u9762\u7684\u6210\u5c0d\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u65b9\u6cd5\u4f86\u6e1b\u8f15\u9577\u5ea6\u504f\u5dee\uff0c\u65b9\u6cd5\u662f\u5c07\u300c\u7a0d\u597d/\u7a0d\u5dee\u300d\u7684\u7d50\u679c\u8f49\u63db\u70ba\u300c\u5e73\u624b\u300d\uff0c\u5982\u679c\u7372\u52dd\u56de\u61c9\u8d85\u904e\u5931\u6557\u8005\u8d85\u904e $K$ \u500b\u5b57\u5143\u3002WB-Score \u500b\u5225\u8a55\u4f30\u6a21\u578b\u8f38\u51fa\u7684\u54c1\u8cea\uff0c\u4f7f\u5176\u6210\u70ba\u5feb\u901f\u4e14\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u8a55\u4f30\u6307\u6a19\u3002WildBench \u7d50\u679c\u986f\u793a\u8207 Chatbot Arena \u4e2d\u7531\u4eba\u985e\u6295\u7968\u7684 Elo \u8a55\u5206\u5728\u56f0\u96e3\u4efb\u52d9\u4e2d\u5177\u6709\u5f37\u70c8\u7684\u76f8\u95dc\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cWB-Reward \u8207\u6392\u540d\u6700\u9ad8\u7684\u6a21\u578b\u9054\u5230 0.98 \u7684 Pearson \u76f8\u95dc\u6027\u3002\u6b64\u5916\uff0cWB-Score \u9054\u5230 0.95\uff0c\u5728\u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\u65b9\u9762\u8d85\u8d8a\u4e86 ArenaHard \u7684 0.91 \u548c AlpacaEval2.0 \u7684 0.89\uff0c\u4ee5\u53ca\u5e38\u898f\u7372\u52dd\u7387\u7684 0.87\u3002</paragraph>", "author": "Bill Yuchen Lin et.al.", "authors": "Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, Yejin Choi", "id": "2406.04770v1", "paper_url": "http://arxiv.org/abs/2406.04770v1", "repo": "https://github.com/allenai/wildbench"}}