{"2406.09719": {"publish_time": "2024-06-14", "title": "Self-Knowledge Distillation for Learning Ambiguity", "paper_summary": "Recent language models have shown remarkable performance on natural language\nunderstanding (NLU) tasks. However, they are often sub-optimal when faced with\nambiguous samples that can be interpreted in multiple ways, over-confidently\npredicting a single label without consideration for its correctness. To address\nthis issue, we propose a novel self-knowledge distillation method that enables\nmodels to learn label distributions more accurately by leveraging knowledge\ndistilled from their lower layers. This approach also includes a learning phase\nthat re-calibrates the unnecessarily strengthened confidence for training\nsamples judged as extremely ambiguous based on the distilled distribution\nknowledge. We validate our method on diverse NLU benchmark datasets and the\nexperimental results demonstrate its effectiveness in producing better label\ndistributions. Particularly, through the process of re-calibrating the\nconfidence for highly ambiguous samples, the issue of over-confidence when\npredictions for unseen samples do not match with their ground-truth labels has\nbeen significantly alleviated. This has been shown to contribute to generating\nbetter distributions than the existing state-of-the-art method. Moreover, our\nmethod is more efficient in training the models compared to the existing\nmethod, as it does not involve additional training processes to refine label\ndistributions.", "paper_summary_zh": "\u8fd1\u671f\u7684\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3 (NLU) \u4efb\u52a1\u4e0a\u8868\u73b0\u4eae\u773c\u3002\u7136\u800c\uff0c\u5f53\u9762\u5bf9\u53ef\u6709\u591a\u79cd\u89e3\u8bfb\u7684\u6a21\u68f1\u4e24\u53ef\u6837\u672c\u65f6\uff0c\u5b83\u4eec\u901a\u5e38\u8868\u73b0\u4e0d\u4f73\uff0c\u4f1a\u8fc7\u4e8e\u81ea\u4fe1\u5730\u9884\u6d4b\u5355\u4e00\u6807\u7b7e\uff0c\u800c\u672a\u8003\u8651\u5176\u6b63\u786e\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5229\u7528\u4ece\u5176\u8f83\u4f4e\u5c42\u84b8\u998f\u7684\u77e5\u8bc6\uff0c\u66f4\u51c6\u786e\u5730\u5b66\u4e60\u6807\u7b7e\u5206\u5e03\u3002\u6b64\u65b9\u6cd5\u8fd8\u5305\u62ec\u4e00\u4e2a\u5b66\u4e60\u9636\u6bb5\uff0c\u8be5\u9636\u6bb5\u4f1a\u91cd\u65b0\u6821\u51c6\u5bf9\u57fa\u4e8e\u84b8\u998f\u5206\u5e03\u77e5\u8bc6\u5224\u65ad\u4e3a\u6781\u5ea6\u6a21\u68f1\u4e24\u53ef\u7684\u8bad\u7ec3\u6837\u672c\u7684\u4e0d\u5fc5\u8981\u589e\u5f3a\u4fe1\u5fc3\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7684 NLU \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u66f4\u597d\u7684\u6807\u7b7e\u5206\u5e03\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7279\u522b\u662f\uff0c\u901a\u8fc7\u91cd\u65b0\u6821\u51c6\u9ad8\u5ea6\u6a21\u68f1\u4e24\u53ef\u6837\u672c\u7684\u4fe1\u5fc3\u7684\u8fc7\u7a0b\uff0c\u5f53\u5bf9\u672a\u89c1\u6837\u672c\u7684\u9884\u6d4b\u4e0e\u5176\u771f\u5b9e\u6807\u7b7e\u4e0d\u5339\u914d\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u5f97\u5230\u4e86\u663e\u8457\u7f13\u89e3\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u8fd9\u6709\u52a9\u4e8e\u751f\u6210\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u597d\u7684\u5206\u5e03\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u6548\u7387\u66f4\u9ad8\uff0c\u56e0\u4e3a\u5b83\u4e0d\u6d89\u53ca\u989d\u5916\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6765\u4f18\u5316\u6807\u7b7e\u5206\u5e03\u3002", "author": "Hancheol Park et.al.", "authors": "Hancheol Park, Soyeong Jeong, Sukmin Cho, Jong C. Park", "id": "2406.09719v1", "paper_url": "http://arxiv.org/abs/2406.09719v1", "repo": "null"}}