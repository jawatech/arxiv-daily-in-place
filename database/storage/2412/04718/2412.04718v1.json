{"2412.04718": {"publish_time": "2024-12-06", "title": "Adaptive Optimization for Enhanced Efficiency in Large-Scale Language Model Training", "paper_summary": "With the rapid development of natural language processing technology,\nlarge-scale language models (LLM) have achieved remarkable results in a variety\nof tasks. However, how to effectively train these huge models and improve their\nperformance and computational efficiency remains an important challenge. This\npaper proposes an improved method based on adaptive optimization algorithm,\naiming to improve the training efficiency and final performance of LLM. Through\ncomparative experiments on the SQuAD and GLUE data sets, the experimental\nresults show that compared with traditional optimization algorithms (such as\nSGD, Momentum, AdaGrad, RMSProp and Adam), the adaptive optimization algorithm\nwe proposed has better accuracy and F1 score. Both have achieved significant\nimprovements, especially showed stronger training capabilities when processed\nlarge-scale texts and complex tasks. The research results verify the advantages\nof adaptive optimization algorithms in large-scale language model training and\nprovide new ideas and directions for future optimization methods.", "paper_summary_zh": "\u96a8\u8457\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6280\u8853\u7684\u5feb\u901f\u767c\u5c55\uff0c\n\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6210\u679c\u3002\n\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u5730\u8a13\u7df4\u9019\u4e9b\u9f90\u5927\u7684\u6a21\u578b\u4e26\u63d0\u9ad8\u5176\n\u6548\u80fd\u548c\u8a08\u7b97\u6548\u7387\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u8981\u7684\u6311\u6230\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u81ea\u9069\u61c9\u512a\u5316\u6f14\u7b97\u6cd5\u7684\u6539\u9032\u65b9\u6cd5\uff0c\n\u65e8\u5728\u63d0\u9ad8 LLM \u7684\u8a13\u7df4\u6548\u7387\u548c\u6700\u7d42\u6548\u80fd\u3002\u901a\u904e\n\u5728 SQuAD \u548c GLUE \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u6bd4\u8f03\u5be6\u9a57\uff0c\u5be6\u9a57\n\u7d50\u679c\u8868\u660e\uff0c\u8207\u50b3\u7d71\u512a\u5316\u6f14\u7b97\u6cd5\uff08\u4f8b\u5982\nSGD\u3001Momentum\u3001AdaGrad\u3001RMSProp \u548c Adam\uff09\u76f8\u6bd4\uff0c\u6211\u5011\u63d0\u51fa\u7684\u81ea\u9069\u61c9\u512a\u5316\u6f14\u7b97\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u3002\u5169\u8005\u90fd\u53d6\u5f97\u4e86\u986f\u8457\u7684\n\u9032\u6b65\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\n\u5927\u898f\u6a21\u6587\u5b57\u548c\u8907\u96dc\u4efb\u52d9\u6642\u8868\u73fe\u51fa\u66f4\u5f37\u7684\u8a13\u7df4\u80fd\u529b\u3002\u7814\u7a76\u7d50\u679c\u9a57\u8b49\u4e86\u81ea\u9069\u61c9\u512a\u5316\u6f14\u7b97\u6cd5\u5728\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u4e2d\u7684\u512a\u9ede\uff0c\u4e26\u70ba\u672a\u4f86\u7684\u512a\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u5411\u3002", "author": "Jiajing Chen et.al.", "authors": "Jiajing Chen, Bingying Liu, Xiaoxuan Liao, Jia Gao, Hongye Zheng, Yue Li", "id": "2412.04718v1", "paper_url": "http://arxiv.org/abs/2412.04718v1", "repo": "null"}}