{"2406.02969": {"publish_time": "2024-06-05", "title": "Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models", "paper_summary": "We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained\nexpert Large Language Models (LLMs) in online time-series prediction tasks by\nadaptively forecasting the best weighting of LLM predictions at every time\nstep. Our mechanism leverages the conditional information in each expert's\nrunning performance to forecast the best combination of LLMs for predicting the\ntime series in its next step. Diverging from static (learned) Mixture of\nExperts (MoE) methods, MoE-F employs time-adaptive stochastic filtering\ntechniques to combine experts. By framing the expert selection problem as a\nfinite state-space, continuous-time Hidden Markov model (HMM), we can leverage\nthe Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters\ncorresponding to each of the $N$ individual LLMs. Each filter proposes its best\ncombination of LLMs, given the information that they have access to.\nSubsequently, the $N$ filter outputs are aggregated to optimize a lower bound\nfor the loss of the aggregated LLMs, which can be optimized in closed-form,\nthus generating our ensemble predictor. Our contributions here are: (I) the\nMoE-F algorithm -- deployable as a plug-and-play filtering harness, (II)\ntheoretical optimality guarantees of the proposed filtering-based gating\nalgorithm, and (III) empirical evaluation and ablative results using state of\nthe art foundational and MoE LLMs on a real-world Financial Market Movement\ntask where MoE-F attains a remarkable 17% absolute and 48.5% relative F1\nmeasure improvement over the next best performing individual LLM expert.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa MoE-F\uff0c\u9019\u662f\u4e00\u7a2e\u5f62\u5f0f\u5316\u7684\u6a5f\u5236\uff0c\u7528\u65bc\u5728\u7dda\u4e0a\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u4efb\u52d9\u4e2d\u7d50\u5408 $N$ \u500b\u9810\u5148\u8a13\u7df4\u7684\u5c08\u5bb6\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u65b9\u6cd5\u662f\u81ea\u9069\u61c9\u5730\u9810\u6e2c\u6bcf\u500b\u6642\u9593\u6b65\u9577\u4e2d LLM \u9810\u6e2c\u7684\u6700\u4f73\u52a0\u6b0a\u3002\u6211\u5011\u7684\u6a5f\u5236\u5229\u7528\u6bcf\u500b\u5c08\u5bb6\u7684\u57f7\u884c\u6548\u80fd\u4e2d\u7684\u689d\u4ef6\u8cc7\u8a0a\uff0c\u9810\u6e2c\u7528\u65bc\u9810\u6e2c\u5176\u4e0b\u4e00\u500b\u6b65\u9a5f\u4e2d\u6642\u9593\u5e8f\u5217\u7684\u6700\u4f73 LLM \u7d44\u5408\u3002\u8207\u975c\u614b\uff08\u5df2\u5b78\u7fd2\uff09\u5c08\u5bb6\u6df7\u5408 (MoE) \u65b9\u6cd5\u4e0d\u540c\uff0cMoE-F \u63a1\u7528\u6642\u9593\u81ea\u9069\u61c9\u96a8\u6a5f\u6ffe\u6ce2\u6280\u8853\u4f86\u7d50\u5408\u5c08\u5bb6\u3002\u900f\u904e\u5c07\u5c08\u5bb6\u9078\u64c7\u554f\u984c\u5efa\u69cb\u70ba\u6709\u9650\u72c0\u614b\u7a7a\u9593\u3001\u9023\u7e8c\u6642\u9593\u96b1\u85cf\u99ac\u53ef\u592b\u6a21\u578b (HMM)\uff0c\u6211\u5011\u53ef\u4ee5\u5229\u7528 Wohman-Shiryaev \u6ffe\u6ce2\u5668\u3002\u6211\u5011\u7684\u505a\u6cd5\u9996\u5148\u5efa\u7acb $N$ \u500b\u5e73\u884c\u6ffe\u6ce2\u5668\uff0c\u5206\u5225\u5c0d\u61c9\u65bc $N$ \u500b\u500b\u5225 LLM\u3002\u6bcf\u500b\u6ffe\u6ce2\u5668\u6839\u64da\u5176\u53ef\u5b58\u53d6\u7684\u8cc7\u8a0a\uff0c\u63d0\u51fa\u5176\u6700\u4f73\u7684 LLM \u7d44\u5408\u3002\u96a8\u5f8c\uff0c\u5c07 $N$ \u500b\u6ffe\u6ce2\u5668\u8f38\u51fa\u805a\u5408\uff0c\u4ee5\u6700\u4f73\u5316\u805a\u5408 LLM \u7684\u640d\u5931\u4e0b\u754c\uff0c\u8a72\u640d\u5931\u4e0b\u754c\u53ef\u4ee5\u5728\u9589\u5408\u5f62\u5f0f\u4e2d\u6700\u4f73\u5316\uff0c\u5f9e\u800c\u7522\u751f\u6211\u5011\u7684\u6574\u9ad4\u9810\u6e2c\u5668\u3002\u6211\u5011\u5728\u6b64\u7684\u8ca2\u737b\u5305\u62ec\uff1a(I) MoE-F \u6f14\u7b97\u6cd5\u2014\u2014\u53ef\u4f5c\u70ba\u5373\u63d2\u5373\u7528\u7684\u6ffe\u6ce2\u5668\u5957\u4ef6\u9032\u884c\u90e8\u7f72\uff0c(II) \u6240\u63d0\u51fa\u7684\u57fa\u65bc\u6ffe\u6ce2\u7684\u9598\u63a7\u6f14\u7b97\u6cd5\u7684\u7406\u8ad6\u6700\u4f73\u6027\u4fdd\u8b49\uff0c\u4ee5\u53ca (III) \u4f7f\u7528\u6700\u5148\u9032\u7684\u57fa\u672c LLM \u548c MoE LLM \u5c0d\u771f\u5be6\u4e16\u754c\u7684\u91d1\u878d\u5e02\u5834\u8b8a\u52d5\u4efb\u52d9\u9032\u884c\u7d93\u9a57\u8a55\u4f30\u548c\u6d88\u878d\u7d50\u679c\uff0c\u5176\u4e2d MoE-F \u5728 F1 \u6e2c\u91cf\u4e0a\u7372\u5f97\u4e86\u986f\u8457\u7684 17% \u7d55\u5c0d\u503c\u548c 48.5% \u76f8\u5c0d\u503c\u6539\u9032\uff0c\u512a\u65bc\u8868\u73fe\u7b2c\u4e8c\u4f73\u7684\u500b\u5225 LLM \u5c08\u5bb6\u3002</paragraph>", "author": "Raeid Saqur et.al.", "authors": "Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Jacob-Junqi Tian, John Willes, Blanka Horvath, Frank Rudzicz", "id": "2406.02969v1", "paper_url": "http://arxiv.org/abs/2406.02969v1", "repo": "null"}}