{"2502.11925": {"publish_time": "2025-02-17", "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs", "paper_summary": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4fc3\u8fdb\u4e86\u6587\u672c\u548c\u56fe\u50cf\u7b49\u591a\u79cd\u6a21\u6001\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6846\u67b6\u5185\u7684\u6574\u5408\u3002\u7136\u800c\uff0c\u6587\u672c\u548c\u56fe\u50cf\u901a\u5e38\u662f\u76f8\u4e92\u5173\u8054\u7684\uff0c\u5f62\u6210\u591a\u6a21\u6001\u5c5e\u6027\u56fe (MMAG)\u3002\u5bf9\u4e8e MLLM \u5982\u4f55\u6574\u5408\u6b64\u7c7b\u56fe\u4e0a\u7684\u5173\u7cfb\u4fe1\u606f\uff08\u5373\u56fe\u7ed3\u6784\uff09\u548c\u8bed\u4e49\u4fe1\u606f\uff08\u5373\u6587\u672c\u548c\u56fe\u50cf\uff09\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u76ee\u524d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GraphGPT-o\uff0c\u5b83\u652f\u6301\u5728 MMAG \u4e0a\u8fdb\u884c\u5168\u65b9\u4f4d\u591a\u6a21\u6001\u7406\u89e3\u548c\u521b\u5efa\u3002\u6211\u4eec\u9996\u5148\u5168\u9762\u7814\u7a76\u4e86\u7ebf\u6027\u5316\u53d8\u4f53\uff0c\u4ee5\u5c06\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u8f6c\u6362\u4e3a MLLM \u7684\u8f93\u5165\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u5bf9\u9f50\u5668\uff0c\u5b83\u652f\u6301\u6df1\u5ea6\u56fe\u7f16\u7801\uff0c\u5f25\u5408\u4e86 MMAG \u548c MLLM \u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6700\u540e\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u63a8\u7406\u9009\u62e9\uff0c\u4f7f MLLM \u9002\u5e94\u56fe\u573a\u666f\u4e2d\u4ea4\u9519\u7684\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u3002\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u4e09\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5728\u88ab\u63a5\u53d7\u540e\u5f00\u6e90\u3002", "author": "Yi Fang et.al.", "authors": "Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han", "id": "2502.11925v1", "paper_url": "http://arxiv.org/abs/2502.11925v1", "repo": "null"}}