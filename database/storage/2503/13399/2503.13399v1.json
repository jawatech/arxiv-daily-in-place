{"2503.13399": {"publish_time": "2025-03-17", "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research", "paper_summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.", "paper_summary_zh": "<paragraph>\u79d1\u5b78\u7814\u7a76\u9700\u8981\u5c0d\u591a\u6a21\u614b\u6578\u64da\u9032\u884c\u8907\u96dc\u7684\u63a8\u7406\uff0c\u9019\u5728\u751f\u7269\u5b78\u9818\u57df\u5c24\u70ba\u7a81\u51fa\u3002\u5118\u7ba1\u7528\u65bc AI \u8f14\u52a9\u7814\u7a76\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u8fd1\u671f\u6709\u6240\u9032\u5c55\uff0c\u4f46\u73fe\u6709\u7684\u591a\u6a21\u614b\u63a8\u7406\u57fa\u6e96\u6e2c\u8a66\u50c5\u91dd\u5c0d\u5927\u5b78\u7a0b\u5ea6\u7684\u96e3\u5ea6\uff0c\u800c\u7814\u7a76\u7d1a\u7684\u57fa\u6e96\u6e2c\u8a66\u5247\u5f37\u8abf\u8f03\u4f4e\u5c64\u6b21\u7684\u611f\u77e5\uff0c\u672a\u80fd\u6eff\u8db3\u79d1\u5b78\u767c\u73fe\u6240\u9700\u7684\u8907\u96dc\u591a\u6a21\u614b\u63a8\u7406\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 MicroVQA\uff0c\u9019\u662f\u4e00\u500b\u8996\u89ba\u554f\u7b54 (VQA) \u57fa\u6e96\u6e2c\u8a66\uff0c\u65e8\u5728\u8a55\u4f30\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u4e09\u7a2e\u63a8\u7406\u80fd\u529b\uff1a\u5c08\u5bb6\u5716\u50cf\u7406\u89e3\u3001\u5047\u8a2d\u751f\u6210\u548c\u5be6\u9a57\u65b9\u6848\u8a2d\u8a08\u3002MicroVQA \u7531\u751f\u7269\u5b78\u5c08\u5bb6\u91dd\u5c0d\u5404\u7a2e\u986f\u5fae\u93e1\u6a21\u5f0f\u7cbe\u5fc3\u8a2d\u8a08\u7684 1,042 \u9053\u591a\u9805\u9078\u64c7\u984c (MCQ) \u7d44\u6210\uff0c\u78ba\u4fdd VQA \u6a23\u672c\u4ee3\u8868\u771f\u5be6\u7684\u79d1\u5b78\u5be6\u8e10\u3002\u5728\u69cb\u5efa\u57fa\u6e96\u6e2c\u8a66\u7684\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u6a19\u6e96\u7684 MCQ \u751f\u6210\u65b9\u6cd5\u6703\u5c0e\u81f4\u8a9e\u8a00\u6377\u5f91\uff0c\u9019\u4fc3\u4f7f\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7684\u5169\u968e\u6bb5\u6d41\u7a0b\uff1a\u4e00\u500b\u512a\u5316\u7684 LLM \u63d0\u793a\u5c07\u554f\u7b54\u5c0d\u69cb\u5efa\u6210 MCQ\uff1b\u7136\u5f8c\uff0c\u4e00\u500b\u57fa\u65bc\u4ee3\u7406\u7684\u300cRefineBot\u300d\u66f4\u65b0\u5b83\u5011\u4ee5\u79fb\u9664\u6377\u5f91\u3002\u5c0d\u6700\u5148\u9032\u7684 MLLM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u986f\u793a\uff0c\u5176\u5cf0\u503c\u6027\u80fd\u70ba 53%\uff1b\u5177\u6709\u8f03\u5c0f LLM \u7684\u6a21\u578b\u7684\u6027\u80fd\u50c5\u7565\u4f4e\u65bc\u9802\u7d1a\u6a21\u578b\uff0c\u9019\u8868\u660e\u57fa\u65bc\u8a9e\u8a00\u7684\u63a8\u7406\u4e0d\u5982\u591a\u6a21\u614b\u63a8\u7406\u90a3\u9ebc\u5177\u6709\u6311\u6230\u6027\uff1b\u4f7f\u7528\u79d1\u5b78\u6587\u7ae0\u9032\u884c\u5fae\u8abf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u5c08\u5bb6\u5c0d\u601d\u7dad\u93c8\u97ff\u61c9\u7684\u5206\u6790\u8868\u660e\uff0c\u611f\u77e5\u932f\u8aa4\u6700\u70ba\u5e38\u898b\uff0c\u5176\u6b21\u662f\u77e5\u8b58\u932f\u8aa4\uff0c\u7136\u5f8c\u662f\u904e\u5ea6\u6982\u62ec\u932f\u8aa4\u3002\u9019\u4e9b\u898b\u89e3\u7a81\u986f\u4e86\u591a\u6a21\u614b\u79d1\u5b78\u63a8\u7406\u4e2d\u7684\u6311\u6230\uff0c\u8868\u660e MicroVQA \u662f\u63a8\u9032 AI \u9a45\u52d5\u7684\u751f\u7269\u91ab\u5b78\u7814\u7a76\u7684\u5bf6\u8cb4\u8cc7\u6e90\u3002MicroVQA \u53ef\u5728 https://huggingface.co/datasets/jmhb/microvqa \u7372\u53d6\uff0c\u9805\u76ee\u9801\u9762\u4f4d\u65bc https://jmhb0.github.io/microvqa\u3002</paragraph>\n", "author": "James Burgess et.al.", "authors": "James Burgess, Jeffrey J Nirschl, Laura Bravo-S\u00e1nchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy", "id": "2503.13399v1", "paper_url": "http://arxiv.org/abs/2503.13399v1", "repo": "null"}}