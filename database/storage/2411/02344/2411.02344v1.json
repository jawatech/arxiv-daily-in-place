{"2411.02344": {"publish_time": "2024-11-04", "title": "Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning", "paper_summary": "Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.", "paper_summary_zh": "\u50c5\u89e3\u78bc\u5668 Transformer \u901a\u5e38\u96e3\u4ee5\u61c9\u4ed8\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\uff0c\u7279\u5225\u662f\u9700\u8981\u591a\u500b\u9806\u5e8f\u904b\u7b97\u7684\u7b97\u8853\u63a8\u7406\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b\u4e2d\u9593\u5c64\u7684\u8868\u5fb5\u5d29\u6f70\u662f\u9650\u5236\u5176\u63a8\u7406\u80fd\u529b\u7684\u4e00\u500b\u95dc\u9375\u56e0\u7d20\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u9806\u5e8f\u8b8a\u7570\u5354\u65b9\u5dee\u898f\u7bc4\u5316 (Seq-VCR)\uff0c\u5b83\u589e\u5f37\u4e86\u4e2d\u9593\u8868\u5fb5\u7684\u71b5\u4e26\u9632\u6b62\u5d29\u6f70\u3002\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u7d50\u5408\u4e86\u4f5c\u70ba\u601d\u60f3\u93c8 (CoT) \u6a19\u8a18\u7684\u66ff\u63db\u9805\u7684\u865b\u64ec\u66ab\u505c\u6a19\u8a18\uff0c\u5927\u5e45\u6539\u5584\u4e86\u7b97\u8853\u63a8\u7406\u554f\u984c\u7684\u6548\u80fd\u3002\u5728\u5177\u6709\u6311\u6230\u6027\u7684 $5 \\times 5$ \u6574\u6578\u4e58\u6cd5\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u7684\u505a\u6cd5\u9054\u5230\u4e86 $99.5\\%$ \u7684\u7cbe\u78ba\u5339\u914d\u6e96\u78ba\u5ea6\uff0c\u512a\u65bc\u540c\u7b49\u898f\u6a21\u7684\u6a21\u578b\uff08\u7522\u751f $0\\%$ \u7684\u6e96\u78ba\u5ea6\uff09\u548c\u5177\u6709\u4e94\u6b21 CoT \u63d0\u793a\u7684 GPT-4\uff08$44\\%$\uff09\u3002\u6211\u5011\u4e5f\u5728\u7b97\u8853\u8868\u9054\u5f0f\u548c\u6700\u9577\u905e\u589e\u5b50\u5e8f\u5217 (LIS) \u8cc7\u6599\u96c6\u4e0a\u8b49\u660e\u4e86\u512a\u7570\u7684\u7d50\u679c\u3002\u6211\u5011\u7684\u767c\u73fe\u5f37\u8abf\u4e86\u9632\u6b62\u4e2d\u9593\u5c64\u8868\u5fb5\u5d29\u6f70\u4ee5\u589e\u5f37 Transformer \u7684\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u4e26\u986f\u793a Seq-VCR \u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u800c\u4e0d\u9700\u8981\u660e\u78ba\u7684 CoT \u76e3\u7763\u3002", "author": "Md Rifat Arefin et.al.", "authors": "Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal", "id": "2411.02344v1", "paper_url": "http://arxiv.org/abs/2411.02344v1", "repo": "null"}}