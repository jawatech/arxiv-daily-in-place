{"2503.21435": {"publish_time": "2025-03-27", "title": "Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models", "paper_summary": "Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured\nlearning, have long faced dual challenges of exponentially escalating\ncomputational complexity and inadequate cross-scenario generalization\ncapability. With the rapid advancement of multimodal learning, Vision-Language\nModels (VLMs) have demonstrated exceptional cross-modal relational reasoning\ncapabilities and generalization capacities, thereby opening up novel pathways\nfor overcoming the inherent limitations of conventional graph learning\nparadigms. However, current research predominantly concentrates on\ninvestigating the single-graph reasoning capabilities of VLMs, which\nfundamentally fails to address the critical requirement for coordinated\nreasoning across multiple heterogeneous graph data in real-world application\nscenarios. To address these limitations, we propose the first multi-graph joint\nreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:\nknowledge graphs, flowcharts, mind maps, and route maps,with each graph group\naccompanied by three progressively challenging instruction-response pairs.\nLeveraging this benchmark, we conducted comprehensive capability assessments of\nstate-of-the-art VLMs and performed fine-tuning on open-source models. This\nstudy not only addresses the underexplored evaluation gap in multi-graph\nreasoning for VLMs but also empirically validates their generalization\nsuperiority in graph-structured learning.", "paper_summary_zh": "<paragraph>\u5716\u795e\u7d93\u7db2\u8def (GNNs) \u4f5c\u70ba\u5716\u7d50\u69cb\u5b78\u7fd2\u7684\u4e3b\u6d41\u7bc4\u5f0f\uff0c\u9577\u671f\u4ee5\u4f86\u4e00\u76f4\u9762\u81e8\u8457\u8a08\u7b97\u8907\u96dc\u5ea6\u5448\u6307\u6578\u7d1a\u589e\u9577\u548c\u8de8\u5834\u666f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u96d9\u91cd\u6311\u6230\u3002\u96a8\u8457\u591a\u6a21\u614b\u5b78\u7fd2\u7684\u5feb\u901f\u767c\u5c55\uff0c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLMs) \u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u8de8\u6a21\u614b\u95dc\u4fc2\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5f9e\u800c\u70ba\u514b\u670d\u50b3\u7d71\u5716\u5b78\u7fd2\u7bc4\u5f0f\u7684\u56fa\u6709\u5c40\u9650\u6027\u958b\u95e2\u4e86\u65b0\u7684\u9014\u5f91\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u63a2\u8a0e VLM \u7684\u55ae\u5716\u63a8\u7406\u80fd\u529b\uff0c\u9019\u6839\u672c\u672a\u80fd\u6eff\u8db3\u73fe\u5be6\u61c9\u7528\u5834\u666f\u4e2d\u8de8\u591a\u500b\u7570\u69cb\u5716\u6578\u64da\u9032\u884c\u5354\u540c\u63a8\u7406\u7684\u95dc\u9375\u9700\u6c42\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u91dd\u5c0d VLM \u7684\u591a\u5716\u806f\u5408\u63a8\u7406\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u57fa\u6e96\u6e2c\u8a66\u6db5\u84cb\u56db\u7a2e\u985e\u578b\u7684\u5716\uff1a\u77e5\u8b58\u5716\u8b5c\u3001\u6d41\u7a0b\u5716\u3001\u601d\u7dad\u5c0e\u5716\u548c\u8def\u7dda\u5716\uff0c\u6bcf\u500b\u5716\u7d44\u90fd\u4f34\u96a8\u8457\u4e09\u7d44\u9010\u6b65\u63d0\u5347\u96e3\u5ea6\u7684\u6307\u4ee4-\u56de\u61c9\u5c0d\u3002\u5229\u7528\u9019\u500b\u57fa\u6e96\u6e2c\u8a66\uff0c\u6211\u5011\u5c0d\u6700\u5148\u9032\u7684 VLM \u9032\u884c\u4e86\u5168\u9762\u7684\u80fd\u529b\u8a55\u4f30\uff0c\u4e26\u5c0d\u958b\u6e90\u6a21\u578b\u9032\u884c\u4e86\u5fae\u8abf\u3002\u9019\u9805\u7814\u7a76\u4e0d\u50c5\u5f4c\u88dc\u4e86 VLM \u5728\u591a\u5716\u63a8\u7406\u65b9\u9762\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u8a55\u4f30\u7f3a\u53e3\uff0c\u800c\u4e14\u4ee5\u7d93\u9a57\u9a57\u8b49\u4e86\u5b83\u5011\u5728\u5716\u7d50\u69cb\u5b78\u7fd2\u4e2d\u7684\u6cdb\u5316\u512a\u52e2\u3002</paragraph>\n", "author": "Ruizhou Li et.al.", "authors": "Ruizhou Li, Haiyun Jiang", "id": "2503.21435v1", "paper_url": "http://arxiv.org/abs/2503.21435v1", "repo": "null"}}