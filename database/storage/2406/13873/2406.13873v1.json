{"2406.13873": {"publish_time": "2024-06-19", "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs", "paper_summary": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.", "paper_summary_zh": "\u9810\u8a13\u7df4\u5728\u5f9e\u5927\u578b\u8cc7\u6599\u4e2d\u7372\u53d6\u5ee3\u6cdb\u77e5\u8b58\u65b9\u9762\u767c\u63ee\u4e86\u95dc\u9375\u4f5c\u7528\uff0c\u5f9e CV \u548c NLP \u4e2d\u7684\u5927\u578b\u6a21\u578b\u6240\u8b49\u660e\u7684\u986f\u8457\u6210\u529f\u4e2d\u5373\u53ef\u898b\u4e00\u6591\u3002\u7136\u800c\uff0c\u7531\u65bc\u7279\u5fb5\u7570\u8cea\u6027\u548c\u7d50\u69cb\u7570\u8cea\u6027\u7b49\u57fa\u672c\u6311\u6230\uff0c\u5716\u5f62\u9818\u57df\u7684\u9032\u5c55\u4ecd\u7136\u6709\u9650\u3002\u6700\u8fd1\uff0c\u4eba\u5011\u5728\u6587\u672c\u5c6c\u6027\u5716 (TAG) \u4e0a\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u589e\u5f37\u7bc0\u9ede\u7279\u5fb5\u54c1\u8cea\uff0c\u4e26\u5df2\u505a\u51fa\u8d8a\u4f86\u8d8a\u591a\u52aa\u529b\uff0c\u8b49\u660e\u5176\u512a\u65bc\u50b3\u7d71\u7684\u8a5e\u888b\u6216 word2vec \u6280\u8853\u3002\u9019\u4e9b\u9ad8\u54c1\u8cea\u7bc0\u9ede\u7279\u5fb5\u964d\u4f4e\u4e86\u5716\u5f62\u7d50\u69cb\u5148\u524d\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5c0e\u81f4\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u548c\u8207\u7d50\u69cb\u7121\u95dc\u7684\u591a\u5c64\u611f\u77e5\u5668 (MLP) \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u7e2e\u5c0f\u3002\u53d7\u5230\u6b64\u555f\u767c\uff0c\u6211\u5011\u900f\u904e\u5c07\u5716\u5f62\u7d50\u69cb\u8996\u70ba\u5148\u9a57\uff0c\u4e26\u5229\u7528\u8c50\u5bcc\u7684\u7d71\u4e00\u7279\u5fb5\u7a7a\u9593\u4f86\u5b78\u7fd2\u5728\u5716\u5f62\u4e2d\u6982\u62ec\u7684\u7cbe\u7dfb\u4e92\u52d5\u6a21\u5f0f\uff0c\u5f15\u5165\u4e86\u4ee5\u7279\u5fb5\u70ba\u4e2d\u5fc3\u7684\u9810\u8a13\u7df4\u89c0\u9ede\u3002\u6211\u5011\u7684\u67b6\u69cb\u5716\u5f62\u5e8f\u5217\u9810\u8a13\u7df4\u8207 Transformer (GSPT)\uff0c\u900f\u904e\u96a8\u6a5f\u904a\u8d70\u53d6\u6a23\u7bc0\u9ede\u8108\u7d61\uff0c\u4e26\u63a1\u7528\u906e\u853d\u7279\u5fb5\u91cd\u5efa\uff0c\u4ee5\u4f7f\u7528\u6a19\u6e96 Transformer \u5728 LLM \u7d71\u4e00\u7279\u5fb5\u7a7a\u9593\u4e2d\u64f7\u53d6\u6210\u5c0d\u63a5\u8fd1\u5ea6\u3002\u900f\u904e\u5229\u7528\u7d71\u4e00\u7684\u6587\u5b57\u8868\u5fb5\uff0c\u800c\u975e\u8b8a\u5316\u7684\u7d50\u69cb\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728\u540c\u4e00\u500b\u7db2\u57df\u4e2d\u7684\u5716\u5f62\u4e4b\u9593\u9054\u5230\u4e86\u986f\u8457\u66f4\u597d\u7684\u53ef\u50b3\u905e\u6027\u3002GSPT \u53ef\u4ee5\u8f15\u9b06\u5730\u8abf\u6574\u5230\u7bc0\u9ede\u5206\u985e\u548c\u9023\u7d50\u9810\u6e2c\uff0c\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u6709\u5e0c\u671b\u7684\u5be6\u8b49\u6210\u529f\u3002", "author": "Yu Song et.al.", "authors": "Yu Song, Haitao Mao, Jiachen Xiao, Jingzhe Liu, Zhikai Chen, Wei Jin, Carl Yang, Jiliang Tang, Hui Liu", "id": "2406.13873v1", "paper_url": "http://arxiv.org/abs/2406.13873v1", "repo": "https://github.com/songyyyy/gspt"}}