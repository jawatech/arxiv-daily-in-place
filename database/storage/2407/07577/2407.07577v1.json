{"2407.07577": {"publish_time": "2024-07-10", "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model", "paper_summary": "The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u7684\u5feb\u901f\u9032\u5c55\u5df2\u5c55\u73fe\u4e00\u7cfb\u5217\u65b0\u8208\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6a21\u578b\u50c5\u5c08\u6ce8\u65bc\u55ae\u4e00\u5834\u666f\u7684\u8996\u89ba\u5167\u5bb9\uff0c\u800c\u5b83\u5011\u8de8\u4e0d\u540c\u5834\u666f\u95dc\u806f\u5be6\u4f8b\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u9019\u5c0d\u65bc\u7406\u89e3\u8907\u96dc\u7684\u8996\u89ba\u5167\u5bb9\uff08\u4f8b\u5982\u5177\u6709\u591a\u500b\u89d2\u8272\u548c\u8907\u96dc\u60c5\u7bc0\u7684\u96fb\u5f71\uff09\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u7406\u89e3\u96fb\u5f71\uff0cLVLMs \u7684\u95dc\u9375\u7b2c\u4e00\u6b65\u662f\u767c\u63ee\u89d2\u8272\u8eab\u5206\u8a18\u61b6\u548c\u8de8\u591a\u500b\u8996\u89ba\u5834\u666f\u8b58\u5225\u7684\u6f5b\u529b\u3002\u70ba\u9054\u6210\u76ee\u6a19\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528 ID \u53c3\u8003\u9032\u884c\u8996\u89ba\u6307\u4ee4\u8abf\u6574\uff0c\u4e26\u958b\u767c\u51fa ID \u611f\u77e5\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b IDA-VLM\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u57fa\u6e96 MM-ID\uff0c\u4ee5\u5728\u56db\u500b\u9762\u5411\uff08\u5339\u914d\u3001\u4f4d\u7f6e\u3001\u554f\u7b54\u548c\u5b57\u5e55\uff09\u4e0a\u6aa2\u9a57 LVLMs \u5728\u5be6\u4f8b ID \u8a18\u61b6\u548c\u8b58\u5225\u65b9\u9762\u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u73fe\u6709 LVLMs \u5728\u8b58\u5225\u548c\u95dc\u806f\u5be6\u4f8b\u8eab\u5206\u8207 ID \u53c3\u8003\u65b9\u9762\u7684\u9650\u5236\u3002\u672c\u6587\u70ba\u672a\u4f86\u7684 AI \u7cfb\u7d71\u92ea\u5e73\u4e86\u9053\u8def\uff0c\u8b93\u5b83\u5011\u64c1\u6709\u591a\u91cd\u8eab\u5206\u8996\u89ba\u8f38\u5165\uff0c\u5f9e\u800c\u4fc3\u9032\u5c0d\u96fb\u5f71\u7b49\u8907\u96dc\u8996\u89ba\u6558\u4e8b\u7684\u7406\u89e3\u3002", "author": "Yatai Ji et.al.", "authors": "Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo", "id": "2407.07577v1", "paper_url": "http://arxiv.org/abs/2407.07577v1", "repo": "https://github.com/jiyt17/ida-vlm"}}