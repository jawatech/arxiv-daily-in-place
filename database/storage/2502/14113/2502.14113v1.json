{"2502.14113": {"publish_time": "2025-02-19", "title": "Object-centric Binding in Contrastive Language-Image Pretraining", "paper_summary": "Recent advances in vision language models (VLM) have been driven by\ncontrastive models such as CLIP, which learn to associate visual information\nwith their corresponding text descriptions. However, these models have\nlimitations in understanding complex compositional scenes involving multiple\nobjects and their spatial relationships. To address these challenges, we\npropose a novel approach that diverges from commonly used strategies, which\nrely on the design of hard-negative augmentations. Instead, our work focuses on\nintegrating inductive biases into pre-trained CLIP-like models to improve their\ncompositional understanding without using any additional hard-negatives. To\nthat end, we introduce a binding module that connects a scene graph, derived\nfrom a text description, with a slot-structured image representation,\nfacilitating a structured similarity assessment between the two modalities. We\nalso leverage relationships as text-conditioned visual constraints, thereby\ncapturing the intricate interactions between objects and their contextual\nrelationships more effectively. Our resulting model not only enhances the\nperformance of CLIP-based models in multi-object compositional understanding\nbut also paves the way towards more accurate and sample-efficient image-text\nmatching of complex scenes.", "paper_summary_zh": "\u6700\u8fd1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u8fdb\u6b65\u662f\u7531\u5bf9\u6bd4\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u63a8\u52a8\u7684\uff0c\u8be5\u6a21\u578b\u5b66\u4e60\u5c06\u89c6\u89c9\u4fe1\u606f\u4e0e\u5176\u5bf9\u5e94\u7684\u6587\u672c\u63cf\u8ff0\u8054\u7cfb\u8d77\u6765\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u89e3\u6d89\u53ca\u591a\u4e2a\u5bf9\u8c61\u53ca\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u7ec4\u5408\u573a\u666f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u504f\u79bb\u4e86\u5e38\u7528\u7684\u7b56\u7565\uff0c\u5373\u4f9d\u8d56\u4e8e\u786c\u8d1f\u589e\u5f3a\u8bbe\u8ba1\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u91cd\u70b9\u662f\u5c06\u5f52\u7eb3\u504f\u5dee\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u7c7b\u4f3c CLIP \u7684\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5176\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u4efb\u4f55\u5176\u4ed6\u786c\u5426\u5b9a\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7ed1\u5b9a\u6a21\u5757\uff0c\u5b83\u5c06\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u6d3e\u751f\u7684\u573a\u666f\u56fe\u4e0e\u69fd\u7ed3\u6784\u56fe\u50cf\u8868\u793a\u8fde\u63a5\u8d77\u6765\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e24\u79cd\u6a21\u5f0f\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u76f8\u4f3c\u6027\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u5229\u7528\u5173\u7cfb\u4f5c\u4e3a\u6587\u672c\u6761\u4ef6\u7684\u89c6\u89c9\u7ea6\u675f\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u6355\u6349\u5bf9\u8c61\u53ca\u5176\u4e0a\u4e0b\u6587\u5173\u7cfb\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002\u6211\u4eec\u7531\u6b64\u4ea7\u751f\u7684\u6a21\u578b\u4e0d\u4ec5\u589e\u5f3a\u4e86\u57fa\u4e8e CLIP \u7684\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u7ec4\u5408\u7406\u89e3\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u4e14\u8fd8\u4e3a\u590d\u6742\u573a\u666f\u7684\u66f4\u51c6\u786e\u548c\u6837\u672c\u9ad8\u6548\u7684\u56fe\u50cf\u6587\u672c\u5339\u914d\u94fa\u5e73\u4e86\u9053\u8def\u3002", "author": "Rim Assouel et.al.", "authors": "Rim Assouel, Pietro Astolfi, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano", "id": "2502.14113v1", "paper_url": "http://arxiv.org/abs/2502.14113v1", "repo": "null"}}