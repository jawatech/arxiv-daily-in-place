{"2501.12602": {"publish_time": "2025-01-22", "title": "BLR-MoE: Boosted Language-Routing Mixture of Experts for Domain-Robust Multilingual E2E ASR", "paper_summary": "Recently, the Mixture of Expert (MoE) architecture, such as LR-MoE, is often\nused to alleviate the impact of language confusion on the multilingual ASR\n(MASR) task. However, it still faces language confusion issues, especially in\nmismatched domain scenarios. In this paper, we decouple language confusion in\nLR-MoE into confusion in self-attention and router. To alleviate the language\nconfusion in self-attention, based on LR-MoE, we propose to apply attention-MoE\narchitecture for MASR. In our new architecture, MoE is utilized not only on\nfeed-forward network (FFN) but also on self-attention. In addition, to improve\nthe robustness of the LID-based router on language confusion, we propose expert\npruning and router augmentation methods. Combining the above, we get the\nboosted language-routing MoE (BLR-MoE) architecture. We verify the\neffectiveness of the proposed BLR-MoE in a 10,000-hour MASR dataset.", "paper_summary_zh": "\u8fd1\u671f\uff0c\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\uff0c\u4f8b\u5982 LR-MoE\uff0c\u901a\u5e38\u7528\u4e8e\u51cf\u8f7b\u8bed\u8a00\u6df7\u6dc6\u5bf9\u591a\u8bed\u8a00 ASR\uff08MASR\uff09\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u5b83\u4ecd\u7136\u9762\u4e34\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5339\u914d\u7684\u9886\u57df\u573a\u666f\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 LR-MoE \u4e2d\u7684\u8bed\u8a00\u6df7\u6dc6\u89e3\u8026\u4e3a\u81ea\u6ce8\u610f\u529b\u548c\u8def\u7531\u5668\u4e2d\u7684\u6df7\u6dc6\u3002\u4e3a\u4e86\u51cf\u8f7b\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u8bed\u8a00\u6df7\u6dc6\uff0c\u57fa\u4e8e LR-MoE\uff0c\u6211\u4eec\u63d0\u51fa\u4e3a MASR \u5e94\u7528\u6ce8\u610f\u529b-MoE \u67b6\u6784\u3002\u5728\u6211\u4eec\u7684\u65b0\u67b6\u6784\u4e2d\uff0cMoE \u4e0d\u4ec5\u7528\u4e8e\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\uff0c\u8fd8\u7528\u4e8e\u81ea\u6ce8\u610f\u529b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u57fa\u4e8e LID \u7684\u8def\u7531\u5668\u5bf9\u8bed\u8a00\u6df7\u6dc6\u7684\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e13\u5bb6\u526a\u679d\u548c\u8def\u7531\u5668\u589e\u5f3a\u65b9\u6cd5\u3002\u7ed3\u5408\u4e0a\u8ff0\u65b9\u6cd5\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u589e\u5f3a\u8bed\u8a00\u8def\u7531 MoE\uff08BLR-MoE\uff09\u67b6\u6784\u3002\u6211\u4eec\u5728\u4e00\u4e2a 10,000 \u5c0f\u65f6\u7684 MASR \u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 BLR-MoE \u7684\u6709\u6548\u6027\u3002", "author": "Guodong Ma et.al.", "authors": "Guodong Ma, Wenxuan Wang, Lifeng Zhou, Yuting Yang, Yuke Li, Binbin Du", "id": "2501.12602v1", "paper_url": "http://arxiv.org/abs/2501.12602v1", "repo": "null"}}