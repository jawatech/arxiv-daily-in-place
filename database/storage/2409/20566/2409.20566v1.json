{"2409.20566": {"publish_time": "2024-09-30", "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning", "paper_summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa MM1.5\uff0c\u4e00\u7a2e\u65b0\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5bb6\u65cf\uff0c\u65e8\u5728\u589e\u5f37\u6587\u672c\u8c50\u5bcc\u5f71\u50cf\u7406\u89e3\u3001\u8996\u89ba\u6307\u6d89\u548c\u57fa\u790e\uff0c\u4ee5\u53ca\u591a\u5f71\u50cf\u63a8\u7406\u7684\u80fd\u529b\u3002\u5728 MM1 \u67b6\u69cb\u7684\u57fa\u790e\u4e0a\uff0cMM1.5 \u63a1\u7528\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u6a21\u578b\u8a13\u7df4\u65b9\u6cd5\uff0c\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u5404\u7a2e\u8cc7\u6599\u6df7\u5408\u5c0d\u6574\u500b\u6a21\u578b\u8a13\u7df4\u751f\u547d\u9031\u671f\u7684\u5f71\u97ff\u3002\u9019\u5305\u62ec\u7528\u65bc\u6301\u7e8c\u9810\u8a13\u7df4\u7684\u9ad8\u54c1\u8cea OCR \u8cc7\u6599\u548c\u5408\u6210\u5f0f\u5b57\u5e55\uff0c\u4ee5\u53ca\u7528\u65bc\u76e3\u7763\u5fae\u8abf\u7684\u6700\u4f73\u5316\u8996\u89ba\u6307\u4ee4\u8abf\u6821\u8cc7\u6599\u6df7\u5408\u3002\u6211\u5011\u7684\u6a21\u578b\u7bc4\u570d\u5f9e 1B \u5230 30B \u53c3\u6578\uff0c\u5305\u542b\u5bc6\u96c6\u548c\u5c08\u5bb6\u6df7\u5408 (MoE) \u8b8a\u9ad4\uff0c\u4e26\u8b49\u660e\u4ed4\u7d30\u7684\u8cc7\u6599\u7b56\u5c55\u548c\u8a13\u7df4\u7b56\u7565\u5373\u4f7f\u5728\u5c0f\u898f\u6a21 (1B \u548c 3B) \u4e0b\u4e5f\u80fd\u7522\u751f\u5f37\u5927\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u5169\u500b\u5c08\u9580\u7684\u8b8a\u9ad4\uff1aMM1.5-Video\uff0c\u5c08\u70ba\u5f71\u7247\u7406\u89e3\u800c\u8a2d\u8a08\uff0c\u4ee5\u53ca MM1.5-UI\uff0c\u5c08\u70ba\u884c\u52d5\u88dd\u7f6e UI \u7406\u89e3\u800c\u8a2d\u8a08\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u8b49\u7814\u7a76\u548c\u6d88\u878d\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5c0d\u8a13\u7df4\u6d41\u7a0b\u548c\u6c7a\u7b56\u7684\u8a73\u7d30\u898b\u89e3\uff0c\u9019\u4e9b\u898b\u89e3\u8aaa\u660e\u4e86\u6211\u5011\u7684\u6700\u7d42\u8a2d\u8a08\uff0c\u70ba MLLM \u958b\u767c\u7684\u672a\u4f86\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u6307\u5c0e\u3002", "author": "Haotian Zhang et.al.", "authors": "Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, Yinfei Yang", "id": "2409.20566v1", "paper_url": "http://arxiv.org/abs/2409.20566v1", "repo": "null"}}