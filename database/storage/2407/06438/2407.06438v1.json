{"2407.06438": {"publish_time": "2024-07-08", "title": "A Single Transformer for Scalable Vision-Language Modeling", "paper_summary": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e86 SOLO\uff0c\u4e00\u500b\u7528\u65bc\u53ef\u64f4\u5145\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u55ae\u4e00Transformer\u3002\n\u76ee\u524d\u7684\u5de8\u91cf\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs)\uff0c\u4f8b\u5982 LLaVA\uff0c\u5927\u591a\u63a1\u7528\u7570\u8cea\u67b6\u69cb\uff0c\u5c07\u9810\u5148\u8a13\u7df4\u597d\u7684\u8996\u89ba\u7de8\u78bc\u5668\u8207\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLMs) \u9023\u63a5\u8d77\u4f86\uff0c\u4ee5\u4fc3\u9032\u8996\u89ba\u8b58\u5225\u548c\u8907\u96dc\u63a8\u7406\u3002\n\u5118\u7ba1\u5728\u76f8\u5c0d\u8f15\u91cf\u7d1a\u7684\u8a13\u7df4\u4e2d\u5be6\u73fe\u4e86\u986f\u8457\u7684\u6548\u80fd\uff0c\u6211\u5011\u767c\u73fe\u4e86\u56db\u500b\u4e3b\u8981\u7684\u64f4\u5145\u6027\u9650\u5236\uff1a(1) \u8996\u89ba\u5bb9\u91cf\u53d7\u5230\u9810\u5148\u8a13\u7df4\u597d\u7684\u8996\u89ba\u7de8\u78bc\u5668\u7684\u9650\u5236\uff0c\u800c\u8996\u89ba\u7de8\u78bc\u5668\u901a\u5e38\u6bd4 LLM \u5c0f\u4e00\u500b\u6578\u91cf\u7d1a\u3002(2) \u7570\u8cea\u67b6\u69cb\u4f7f\u65e2\u6709\u786c\u9ad4\u548c\u8edf\u9ad4\u57fa\u790e\u8a2d\u65bd\u7684\u4f7f\u7528\u8b8a\u5f97\u8907\u96dc\u3002(3) \u6b64\u985e\u67b6\u69cb\u4e0a\u64f4\u5145\u5b9a\u5f8b\u7684\u7814\u7a76\u5fc5\u9808\u8003\u91cf\u4e09\u500b\u7368\u7acb\u7684\u5143\u4ef6\uff0c\u5373\u8996\u89ba\u7de8\u78bc\u5668\u3001\u9023\u63a5\u5668\u548c LLM\uff0c\u9019\u4f7f\u5f97\u5206\u6790\u8b8a\u5f97\u8907\u96dc\u3002(4) \u4f7f\u7528\u73fe\u6709\u7684\u8996\u89ba\u7de8\u78bc\u5668\u901a\u5e38\u9700\u8981\u9075\u5faa\u9810\u5148\u5b9a\u7fa9\u7684\u5f71\u50cf\u8f38\u5165\u524d\u8655\u7406\u898f\u683c\uff0c\u4f8b\u5982\uff0c\u5c07\u8f38\u5165\u91cd\u65b0\u8abf\u6574\u70ba\u56fa\u5b9a\u89e3\u6790\u5ea6\u7684\u6b63\u65b9\u5f62\u5f71\u50cf\uff0c\u9019\u5728\u8655\u7406\u548c\u8a13\u7df4\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u6216\u5177\u6709\u7570\u5e38\u9577\u5bec\u6bd4\u7684\u5f71\u50cf\u6642\u6703\u9020\u6210\u56f0\u96e3\u3002\u7d71\u4e00\u7684\u55ae\u4e00Transformer\u67b6\u69cb\uff0c\u4f8b\u5982 SOLO\uff0c\u6709\u6548\u5730\u89e3\u6c7a\u4e86 LVLMs \u4e2d\u7684\u9019\u4e9b\u64f4\u5145\u6027\u554f\u984c\uff1b\u7136\u800c\uff0c\u5b83\u5728\u73fe\u4ee3\u74b0\u5883\u4e2d\u7684\u63a1\u7528\u6709\u9650\uff0c\u9019\u53ef\u80fd\u662f\u56e0\u70ba\u7f3a\u4e4f\u5e73\u8861\u5169\u7a2e\u6a21\u5f0f\u4e26\u78ba\u4fdd\u5341\u5104\u898f\u6a21\u6a21\u578b\u7a69\u5b9a\u8a13\u7df4\u7684\u53ef\u9760\u8a13\u7df4\u914d\u65b9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u7b2c\u4e00\u500b\u7528\u65bc\u958b\u767c SOLO \u7684\u958b\u6e90\u8a13\u7df4\u914d\u65b9\uff0cSOLO \u662f\u4e00\u500b\u4f7f\u7528\u9069\u5ea6\u7684\u5b78\u8853\u8cc7\u6e90\u7684\u958b\u6e90 7B LVLM\u3002\u8a13\u7df4\u914d\u65b9\u6d89\u53ca\u5f9e LLM \u521d\u59cb\u5316\u3001\u5728 ImageNet \u548c\u7db2\u8def\u898f\u6a21\u8cc7\u6599\u4e0a\u9032\u884c\u9806\u5e8f\u9810\u8a13\u7df4\uff0c\u4ee5\u53ca\u5728\u6211\u5011\u7b56\u5283\u7684\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u6307\u4ee4\u5fae\u8abf\u3002\u5728\u5ee3\u6cdb\u7684\u8a55\u4f30\u4e2d\uff0cSOLO \u8868\u73fe\u51fa\u8207 LLaVA-v1.5-7B \u76f8\u7576\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u8996\u89ba\u6578\u5b78\u63a8\u7406\u65b9\u9762\u8868\u73fe\u51fa\u8272\u3002</paragraph>", "author": "Yangyi Chen et.al.", "authors": "Yangyi Chen, Xingyao Wang, Hao Peng, Heng Ji", "id": "2407.06438v1", "paper_url": "http://arxiv.org/abs/2407.06438v1", "repo": "https://github.com/yangyi-chen/solo"}}