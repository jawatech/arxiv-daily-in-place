
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-05**|**Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**|Sarah Al-Shareeda et.al.|[2502.03396v1](http://arxiv.org/abs/2502.03396v1)|null|
|**2025-02-05**|**RadVLM: A Multitask Conversational Vision-Language Model for Radiology**|Nicolas Deperrois et.al.|[2502.03333v1](http://arxiv.org/abs/2502.03333v1)|null|
|**2025-02-05**|**MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**|Amin Dada et.al.|[2502.03298v1](http://arxiv.org/abs/2502.03298v1)|null|
|**2025-02-05**|**Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**|Matthias Schwab et.al.|[2502.03272v1](http://arxiv.org/abs/2502.03272v1)|null|
|**2025-02-05**|**Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**|Li Pan et.al.|[2502.03238v1](http://arxiv.org/abs/2502.03238v1)|null|
|**2025-02-05**|**MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**|Seonok Kim et.al.|[2502.03004v1](http://arxiv.org/abs/2502.03004v1)|null|
|**2025-02-04**|**3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**|Weicheng Zhu et.al.|[2502.02779v1](http://arxiv.org/abs/2502.02779v1)|null|
|**2025-02-04**|**Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**|Obed Korshie Dzikunu et.al.|[2502.02756v1](http://arxiv.org/abs/2502.02756v1)|null|
|**2025-02-04**|**MedRAX: Medical Reasoning Agent for Chest X-ray**|Adibvafa Fallahpour et.al.|[2502.02673v1](http://arxiv.org/abs/2502.02673v1)|null|
|**2025-02-04**|**Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**|Shayan Kiyani et.al.|[2502.02561v1](http://arxiv.org/abs/2502.02561v1)|null|
|**2025-02-04**|**A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**|Edward Ellis et.al.|[2502.02489v1](http://arxiv.org/abs/2502.02489v1)|null|
|**2025-02-04**|**Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**|Yaling Shen et.al.|[2502.02438v1](http://arxiv.org/abs/2502.02438v1)|null|
|**2025-02-04**|**Test Time Training for 4D Medical Image Interpolation**|Qikang Zhang et.al.|[2502.02341v1](http://arxiv.org/abs/2502.02341v1)|null|
|**2025-02-04**|**Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**|Atharva Mangeshkumar Agrawal et.al.|[2502.02249v1](http://arxiv.org/abs/2502.02249v1)|null|
|**2025-02-04**|**Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**|F. Xavier Gaya-Morey et.al.|[2502.02618v1](http://arxiv.org/abs/2502.02618v1)|null|
|**2025-02-04**|**Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**|Yuxiao Cheng et.al.|[2502.02109v1](http://arxiv.org/abs/2502.02109v1)|null|
|**2025-02-03**|**An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**|Jiazi Tian et.al.|[2502.01789v1](http://arxiv.org/abs/2502.01789v1)|null|
|**2025-02-03**|**Improving Transformer World Models for Data-Efficient RL**|Antoine Dedieu et.al.|[2502.01591v1](http://arxiv.org/abs/2502.01591v1)|null|
|**2025-02-03**|**Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**|Zhi Zhang et.al.|[2502.01377v1](http://arxiv.org/abs/2502.01377v1)|null|
|**2025-02-03**|**OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**|Chengfeng Zhou et.al.|[2502.01243v1](http://arxiv.org/abs/2502.01243v1)|null|
|**2025-02-03**|**MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**|Alejandro Guerra-Manzanares et.al.|[2502.01158v1](http://arxiv.org/abs/2502.01158v1)|null|
|**2025-02-03**|**Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**|Qian Chen et.al.|[2502.01141v1](http://arxiv.org/abs/2502.01141v1)|null|
|**2025-02-03**|**Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**|Mithun Saha et.al.|[2502.01108v1](http://arxiv.org/abs/2502.01108v1)|null|
|**2025-02-02**|**Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**|Hadas Ben-Atya et.al.|[2502.01691v1](http://arxiv.org/abs/2502.01691v1)|null|
|**2025-02-02**|**Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**|Si-Ioi Ng et.al.|[2502.01685v1](http://arxiv.org/abs/2502.01685v1)|null|
|**2025-02-02**|**Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**|Shengtian Sang et.al.|[2502.00712v1](http://arxiv.org/abs/2502.00712v1)|null|
|**2025-02-02**|**TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**|Linglong Wu et.al.|[2502.00695v1](http://arxiv.org/abs/2502.00695v1)|null|
|**2025-02-02**|**Enhanced Convolutional Neural Networks for Improved Image Classification**|Xiaoran Yang et.al.|[2502.00663v1](http://arxiv.org/abs/2502.00663v1)|null|
|**2025-02-02**|**Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**|Yujin Oh et.al.|[2502.00619v1](http://arxiv.org/abs/2502.00619v1)|null|
|**2025-02-01**|**Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**|Samiran Dey et.al.|[2502.00568v1](http://arxiv.org/abs/2502.00568v1)|null|
|**2025-02-01**|**Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**|Mohammad Saleh Torkestani et.al.|[2502.00495v1](http://arxiv.org/abs/2502.00495v1)|null|
|**2025-02-01**|**Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**|Aishik Mandal et.al.|[2502.00451v1](http://arxiv.org/abs/2502.00451v1)|null|
|**2025-01-31**|**EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**|Omar H. Khater et.al.|[2502.00205v1](http://arxiv.org/abs/2502.00205v1)|null|
|**2025-01-31**|**DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**|Abdurrahim Yilmaz et.al.|[2502.00196v1](http://arxiv.org/abs/2502.00196v1)|null|
|**2025-01-31**|**Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**|Hassan Jahanandish et.al.|[2502.00146v1](http://arxiv.org/abs/2502.00146v1)|null|
|**2025-01-31**|**AIN: The Arabic INclusive Large Multimodal Model**|Ahmed Heakl et.al.|[2502.00094v2](http://arxiv.org/abs/2502.00094v2)|null|
|**2025-01-31**|**Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**|Misha P. T Kaandorp et.al.|[2501.19338v1](http://arxiv.org/abs/2501.19338v1)|null|
|**2025-01-31**|**Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**|Halil Ibrahim Aysel et.al.|[2501.19271v1](http://arxiv.org/abs/2501.19271v1)|null|
|**2025-01-31**|**Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**|Aurora Rofena et.al.|[2501.19176v1](http://arxiv.org/abs/2501.19176v1)|null|
|**2025-01-31**|**Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**|Xiangyu Sun et.al.|[2501.19086v1](http://arxiv.org/abs/2501.19086v1)|null|
|**2025-01-30**|**Survey and Improvement Strategies for Gene Prioritization with Large Language Models**|Matthew Neeley et.al.|[2501.18794v1](http://arxiv.org/abs/2501.18794v1)|null|
|**2025-01-30**|**Synthetic Data Generation for Augmenting Small Samples**|Dan Liu et.al.|[2501.18741v1](http://arxiv.org/abs/2501.18741v1)|null|
|**2025-01-30**|**A Multi-Layered Large Language Model Framework for Disease Prediction**|Malak Mohamed et.al.|[2502.00063v1](http://arxiv.org/abs/2502.00063v1)|null|
|**2025-01-30**|**A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**|Yifan Wang et.al.|[2501.18367v1](http://arxiv.org/abs/2501.18367v1)|null|
|**2025-01-30**|**MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**|Yuxin Zuo et.al.|[2501.18362v1](http://arxiv.org/abs/2501.18362v1)|null|
|**2025-01-30**|**CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**|Yicheng Wu et.al.|[2501.18328v1](http://arxiv.org/abs/2501.18328v1)|null|
|**2025-01-30**|**A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**|Shayli Farshchiha et.al.|[2501.18294v1](http://arxiv.org/abs/2501.18294v1)|null|
|**2025-01-30**|**The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**|Anup Saha et.al.|[2501.18270v1](http://arxiv.org/abs/2501.18270v1)|null|
|**2025-01-30**|**Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**|Malte Tölle et.al.|[2501.18237v1](http://arxiv.org/abs/2501.18237v1)|null|
|**2025-01-30**|**Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**|Min Hun Lee et.al.|[2501.18108v1](http://arxiv.org/abs/2501.18108v1)|null|
|**2025-01-30**|**Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**|Pratik S. Sachdeva et.al.|[2501.18081v1](http://arxiv.org/abs/2501.18081v1)|null|
|**2025-01-30**|**Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**|Pir Bakhsh Khokhar et.al.|[2501.18071v1](http://arxiv.org/abs/2501.18071v1)|null|
|**2025-01-29**|**Current Pathology Foundation Models are unrobust to Medical Center Differences**|Edwin D. de Jong et.al.|[2501.18055v2](http://arxiv.org/abs/2501.18055v2)|null|
|**2025-01-29**|**Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**|Zijie Liu et.al.|[2501.17860v1](http://arxiv.org/abs/2501.17860v1)|null|
|**2025-01-29**|**GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**|Ziang Liu et.al.|[2501.17855v1](http://arxiv.org/abs/2501.17855v1)|null|
|**2025-01-29**|**Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**|Ljubisa Bojic et.al.|[2502.00055v1](http://arxiv.org/abs/2502.00055v1)|null|
|**2025-01-29**|**Tonguescape: Exploring Language Models Understanding of Vowel Articulation**|Haruki Sakajo et.al.|[2501.17643v1](http://arxiv.org/abs/2501.17643v1)|[link](https://github.com/sj-h4/tonguescape-builder)|
|**2025-01-29**|**Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**|Manish Sanwal et.al.|[2501.18645v2](http://arxiv.org/abs/2501.18645v2)|null|
|**2025-01-29**|**An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**|Wenqi Li et.al.|[2501.17555v1](http://arxiv.org/abs/2501.17555v1)|null|
|**2025-01-29**|**LLM Assistance for Pediatric Depression**|Mariia Ignashina et.al.|[2501.17510v1](http://arxiv.org/abs/2501.17510v1)|null|
|**2025-01-28**|**Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**|Gonzalo Iñaki Quintana et.al.|[2502.00052v1](http://arxiv.org/abs/2502.00052v1)|null|
|**2025-01-28**|**Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**|Chongyu Qu et.al.|[2501.17343v1](http://arxiv.org/abs/2501.17343v1)|null|
|**2025-01-28**|**Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**|Mingyu Derek Ma et.al.|[2501.17338v1](http://arxiv.org/abs/2501.17338v1)|null|
|**2025-01-28**|**Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**|Mingyu Derek Ma et.al.|[2501.17326v1](http://arxiv.org/abs/2501.17326v1)|null|
|**2025-01-28**|**Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**|Peilong Wang et.al.|[2501.17286v1](http://arxiv.org/abs/2501.17286v1)|null|
|**2025-01-28**|**ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**|Mohammadreza Saraei et.al.|[2501.17260v1](http://arxiv.org/abs/2501.17260v1)|[link](https://github.com/mrsaraei/vit-2spn)|
|**2025-01-28**|**A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**|Suresh Babu Nettur et.al.|[2501.17160v1](http://arxiv.org/abs/2501.17160v1)|null|
|**2025-01-28**|**Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**|Reza Ghorbani et.al.|[2501.17152v1](http://arxiv.org/abs/2501.17152v1)|null|
|**2025-01-28**|**Irony Detection, Reasoning and Understanding in Zero-shot Learning**|Peiling Yi et.al.|[2501.16884v1](http://arxiv.org/abs/2501.16884v1)|null|
|**2025-01-28**|**Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**|Keqi Han et.al.|[2501.17207v1](http://arxiv.org/abs/2501.17207v1)|[link](https://github.com/learningkeqi/rethinkingbca)|
|**2025-01-28**|**Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**|Fengpei Yuan et.al.|[2501.17206v1](http://arxiv.org/abs/2501.17206v1)|null|
|**2025-01-28**|**Efficient Knowledge Distillation of SAM for Medical Image Segmentation**|Kunal Dasharath Patil et.al.|[2501.16740v1](http://arxiv.org/abs/2501.16740v1)|null|
|**2025-01-28**|**VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**|Philip Chung et.al.|[2501.16672v1](http://arxiv.org/abs/2501.16672v1)|[link](https://github.com/philipchung/verifact)|
|**2025-01-28**|**Vision-based autonomous structural damage detection using data-driven methods**|Seyyed Taghi Ataei et.al.|[2501.16662v2](http://arxiv.org/abs/2501.16662v2)|null|
|**2025-01-28**|**Molecular-driven Foundation Model for Oncologic Pathology**|Anurag Vaidya et.al.|[2501.16652v1](http://arxiv.org/abs/2501.16652v1)|null|
|**2025-01-27**|**Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**|Yi Mao et.al.|[2502.00045v1](http://arxiv.org/abs/2502.00045v1)|null|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282v1](http://arxiv.org/abs/2501.16282v1)|null|
|**2025-01-27**|**Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**|Huayu Li et.al.|[2501.16215v1](http://arxiv.org/abs/2501.16215v1)|[link](https://github.com/HuayuLiArizona/Conformalized-Multiple-Instance-Learning-For-MedTS)|
|**2025-01-27**|**Atla Selene Mini: A General Purpose Evaluation Model**|Andrei Alexandru et.al.|[2501.17195v1](http://arxiv.org/abs/2501.17195v1)|[link](https://huggingface.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B)|
|**2025-01-27**|**An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**|Shaheer Ahmad Khan et.al.|[2501.15969v1](http://arxiv.org/abs/2501.15969v1)|null|
|**2025-01-27**|**Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**|Taymaz Akan et.al.|[2501.15733v1](http://arxiv.org/abs/2501.15733v1)|null|
|**2025-01-27**|**A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**|Dong Li et.al.|[2501.15724v1](http://arxiv.org/abs/2501.15724v1)|null|
|**2025-01-26**|**Beyond Benchmarks: On The False Promise of AI Regulation**|Gabriel Stanovsky et.al.|[2501.15693v1](http://arxiv.org/abs/2501.15693v1)|null|
|**2025-01-26**|**Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**|Mahshid shiri et.al.|[2501.15572v1](http://arxiv.org/abs/2501.15572v1)|null|
|**2025-01-26**|**AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**|Muhammad Aftab et.al.|[2501.15489v1](http://arxiv.org/abs/2501.15489v1)|null|
|**2025-01-26**|**Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**|Solha Kang et.al.|[2501.15452v1](http://arxiv.org/abs/2501.15452v1)|null|
|**2025-01-25**|**An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**|Arya Rahgozar et.al.|[2501.17181v1](http://arxiv.org/abs/2501.17181v1)|null|
|**2025-01-25**|**Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**|Harshita Chopra et.al.|[2501.15056v1](http://arxiv.org/abs/2501.15056v1)|null|
|**2025-01-24**|**Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**|Md. Kamrul Hasan et.al.|[2501.14929v1](http://arxiv.org/abs/2501.14929v1)|null|
|**2025-01-24**|**Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**|Hang Luo et.al.|[2501.14892v1](http://arxiv.org/abs/2501.14892v1)|null|
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719v1](http://arxiv.org/abs/2501.14719v1)|null|
|**2025-01-24**|**GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**|Ziwen Li et.al.|[2501.16382v1](http://arxiv.org/abs/2501.16382v1)|[link](https://github.com/aaronli43/grappi)|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693v1](http://arxiv.org/abs/2501.14693v1)|null|
|**2025-01-24**|**Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**|Dmitry Ryabtsev et.al.|[2501.14689v1](http://arxiv.org/abs/2501.14689v1)|null|
|**2025-01-24**|**Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**|Fuping Wu et.al.|[2501.14685v1](http://arxiv.org/abs/2501.14685v1)|null|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654v1](http://arxiv.org/abs/2501.14654v1)|[link](https://github.com/stanfordmlgroup/medagentbench)|
|**2025-01-24**|**Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**|Xu Chen et.al.|[2501.18614v1](http://arxiv.org/abs/2501.18614v1)|null|
|**2025-01-24**|**Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**|Walid Yassine et.al.|[2501.14483v1](http://arxiv.org/abs/2501.14483v1)|null|
|**2025-01-24**|**Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**|Taehan Kim et.al.|[2501.14469v1](http://arxiv.org/abs/2501.14469v1)|null|
|**2025-01-24**|**ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**|Yoni Schirris et.al.|[2501.14379v1](http://arxiv.org/abs/2501.14379v1)|[link](https://github.com/nki-ai/ectil)|

#### Abstracts
##### **Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**
2502.03396v1 by Sarah Al-Shareeda, Yasar Celik, Bilge Bilgili, Ahmed Al-Dubai, Berk Canberk

Creating a Digital Twin (DT) for Healthcare Intelligent Transportation
Systems (HITS) is a hot research trend focusing on enhancing HITS management,
particularly in emergencies where ambulance vehicles must arrive at the crash
scene on time and track their real-time location is crucial to the medical
authorities. Despite the claim of real-time representation, a temporal
misalignment persists between the physical and virtual domains, leading to
discrepancies in the ambulance's location representation. This study proposes
integrating AI predictive models, specifically Support Vector Regression (SVR)
and Deep Neural Networks (DNN), within a constructed mock DT data pipeline
framework to anticipate the medical vehicle's next location in the virtual
world. These models align virtual representations with their physical
counterparts, i.e., metaphorically offsetting the synchronization delay between
the two worlds. Trained meticulously on a historical geospatial dataset, SVR
and DNN exhibit exceptional prediction accuracy in MATLAB and Python
environments. Through various testing scenarios, we visually demonstrate the
efficacy of our methodology, showcasing SVR and DNN's key role in significantly
reducing the witnessed gap within the HITS's DT. This transformative approach
enhances real-time synchronization in emergency HITS by approximately 88% to
93%.

摘要：建立醫療智慧交通系統（HITS）的數位分身（DT）是熱門的研究趨勢，其重點在於提升 HITS 管理，特別是在救護車必須準時抵達車禍現場的緊急情況中，追蹤其即時位置對於醫療單位至關重要。儘管聲稱即時呈現，但實體和虛擬領域之間仍存在時間上的錯位，導致救護車位置呈現上的差異。本研究建議在建構的虛擬 DT 資料管道架構中整合人工智慧預測模型，特別是支援向量回歸（SVR）和深度神經網路（DNN），以預測醫療車輛在虛擬世界的下一個位置。這些模型將虛擬呈現與其實體對應物對齊，也就是說，在兩個世界之間比喻性地抵銷同步延遲。在歷史地理空間資料集上經過仔細訓練，SVR 和 DNN 在 MATLAB 和 Python 環境中展現出卓越的預測準確性。透過各種測試情境，我們視覺化展示了我們方法論的效能，展示了 SVR 和 DNN 在顯著縮小 HITS 的 DT 中見證到的差距方面的關鍵作用。這種變革性的方法將緊急 HITS 中的即時同步提升了大約 88% 到 93%。

##### **RadVLM: A Multitask Conversational Vision-Language Model for Radiology**
2502.03333v1 by Nicolas Deperrois, Hidetoshi Matsuo, Samuel Ruipérez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas M. Sutter, Julia E. Vogt, Jonas Kluckert, Thomas Frauenfelder, Christian Blüthgen, Farhad Nooralahzadeh, Michael Krauthammer

The widespread use of chest X-rays (CXRs), coupled with a shortage of
radiologists, has driven growing interest in automated CXR analysis and
AI-assisted reporting. While existing vision-language models (VLMs) show
promise in specific tasks such as report generation or abnormality detection,
they often lack support for interactive diagnostic capabilities. In this work
we present RadVLM, a compact, multitask conversational foundation model
designed for CXR interpretation. To this end, we curate a large-scale
instruction dataset comprising over 1 million image-instruction pairs
containing both single-turn tasks -- such as report generation, abnormality
classification, and visual grounding -- and multi-turn, multi-task
conversational interactions. After fine-tuning RadVLM on this instruction
dataset, we evaluate it across different tasks along with re-implemented
baseline VLMs. Our results show that RadVLM achieves state-of-the-art
performance in conversational capabilities and visual grounding while remaining
competitive in other radiology tasks. Ablation studies further highlight the
benefit of joint training across multiple tasks, particularly for scenarios
with limited annotated data. Together, these findings highlight the potential
of RadVLM as a clinically relevant AI assistant, providing structured CXR
interpretation and conversational capabilities to support more effective and
accessible diagnostic workflows.

摘要：胸部 X 光 (CXR) 的广泛使用，加上放射科醫師短缺，促使人們對自動化 CXR 分析和 AI 輔助報告產生越來越濃厚的興趣。雖然現有的視覺語言模型 (VLM) 在特定任務中顯示出前景，例如報告生成或異常偵測，但它們通常缺乏對互動式診斷功能的支持。在這項工作中，我們提出 RadVLM，這是一個緊湊的多任務對話式基礎模型，專為 CXR 解釋而設計。為此，我們策劃了一個大型指令資料集，包含超過 100 萬個影像指令對，其中包含單輪任務（例如報告生成、異常分類和視覺基礎），以及多輪、多任務對話互動。在對這個指令資料集進行微調後，我們對 RadVLM 進行評估，並與重新實作的基準 VLM 一起執行不同的任務。我們的結果顯示，RadVLM 在對話能力和視覺基礎方面取得了最先進的效能，同時在其他放射學任務中仍具有競爭力。消融研究進一步突顯了跨多個任務進行聯合訓練的好處，特別是對於帶有標註資料有限的場景。這些發現共同突顯了 RadVLM 作為臨床相關 AI 助理的潛力，提供結構化的 CXR 解釋和對話能力，以支援更有效且可存取的診斷工作流程。

##### **MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**
2502.03298v1 by Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich

While increasing patients' access to medical documents improves medical care,
this benefit is limited by varying health literacy levels and complex medical
terminology. Large language models (LLMs) offer solutions by simplifying
medical information. However, evaluating LLMs for safe and patient-friendly
text generation is difficult due to the lack of standardized evaluation
resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset
created from MIMIC-IV discharge summaries through an automated pipeline
combining LLM-based question-answer generation with manual quality checks. We
use this dataset to evaluate various LLMs on patient-oriented
question-answering. Our findings reveal that general-purpose LLMs frequently
surpass biomedical-adapted models, while automated metrics correlate with human
judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the
development of LLMs to enhance patient understanding and ultimately improve
care outcomes.

摘要：儘管讓患者更能取得醫療文件有助於改善醫療照護，
但此優點受到不同的健康素養程度和複雜的醫療術語所限制。大型語言模型 (LLM) 提供了簡化醫療資訊的解決方案。然而，由於缺乏標準化的評估資源，因此難以評估 LLM 以確保其安全且對患者友善的文字產生。為了填補此缺口，我們開發了 MeDiSumQA。MeDiSumQA 是透過自動化流程從 MIMIC-IV 出院摘要中建立的資料集，結合了基於 LLM 的問答產生和手動品質檢查。我們使用此資料集來評估各種 LLM 在以患者為導向的問答中。我們的發現顯示，通用 LLM 經常超越生物醫學適應模型，而自動化指標與人類判斷相關。透過在 PhysioNet 上發布 MeDiSumQA，我們旨在推動 LLM 的發展，以增進患者理解，並最終改善照護成果。

##### **Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**
2502.03272v1 by Matthias Schwab, Mathias Pamminger, Christian Kremser, Agnes Mayr

Purpose: To develop and evaluate a deep learning-based method that allows to
perform myocardial infarct segmentation in a fully-automated way.
  Materials and Methods: For this retrospective study, a cascaded framework of
two and three-dimensional convolutional neural networks (CNNs), specialized on
identifying ischemic myocardial scars on late gadolinium enhancement (LGE)
cardiac magnetic resonance (CMR) images, was trained on an in-house training
dataset consisting of 144 examinations. On a separate test dataset from the
same institution, including images from 152 examinations obtained between 2021
and 2023, a quantitative comparison between artificial intelligence (AI)-based
segmentations and manual segmentations was performed. Further, qualitative
assessment of segmentation accuracy was evaluated for both human and
AI-generated contours by two CMR experts in a blinded experiment.
  Results: Excellent agreement could be found between manually and
automatically calculated infarct volumes ($\rho_c$ = 0.9). The qualitative
evaluation showed that compared to human-based measurements, the experts rated
the AI-based segmentations to better represent the actual extent of infarction
significantly (p < 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On
the contrary, for segmentation of microvascular obstruction (MVO), manual
measurements were still preferred (11.3% AI, 55.6% human, 33.1% equal).
  Conclusion: This fully-automated segmentation pipeline enables CMR infarct
size to be calculated in a very short time and without requiring any
pre-processing of the input images while matching the segmentation quality of
trained human observers. In a blinded experiment, experts preferred automated
infarct segmentations more often than manual segmentations, paving the way for
a potential clinical application.

摘要：<paragraph>目的：開發和評估一種基於深度學習的方法，允許以全自動的方式執行心肌梗塞分割。
材料和方法：對於這項回顧性研究，一個由二維和三維卷積神經網路 (CNN) 組成的串聯架構，專門用於識別晚期釓增強 (LGE) 心臟磁振造影 (CMR) 影像上的缺血性心肌疤痕，並在包含 144 項檢查的內部訓練資料集上受訓。在來自同一家機構的獨立測試資料集上，包括 2021 年至 2023 年間獲得的 152 項檢查的影像，執行基於人工智慧 (AI) 的分割和手動分割之間的定量比較。此外，由兩位 CMR 專家在盲測實驗中評估人類和 AI 生成的輪廓的分割準確度。
結果：在手動和自動計算的梗塞體積之間可以發現極佳的一致性（ρ_c = 0.9）。定性評估顯示，與基於人類的測量相比，專家評估 AI 基於分割能更能代表梗塞的實際範圍，顯著（p < 0.001）更常發生（33.4% AI，25.1% 人類，41.5% 相等）。相反，對於微血管阻塞 (MVO) 的分割，手動測量仍然較受青睞（11.3% AI，55.6% 人類，33.1% 相等）。
結論：這個全自動分割管道可以在很短的時間內計算 CMR 梗塞大小，而且無需對輸入影像進行任何前處理，同時匹配受過訓練的人類觀察者的分割品質。在盲測實驗中，專家比手動分割更常偏好自動梗塞分割，為潛在的臨床應用鋪平了道路。</paragraph>

##### **Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**
2502.03238v1 by Li Pan, Yupei Zhang, Qiushi Yang, Tan Li, Zhen Chen

Recently computer-aided diagnosis has demonstrated promising performance,
effectively alleviating the workload of clinicians. However, the inherent
sample imbalance among different diseases leads algorithms biased to the
majority categories, leading to poor performance for rare categories. Existing
works formulated this challenge as a long-tailed problem and attempted to
tackle it by decoupling the feature representation and classification. Yet, due
to the imbalanced distribution and limited samples from tail classes, these
works are prone to biased representation learning and insufficient classifier
calibration. To tackle these problems, we propose a new Long-tailed Medical
Diagnosis (LMD) framework for balanced medical image classification on
long-tailed datasets. In the initial stage, we develop a Relation-aware
Representation Learning (RRL) scheme to boost the representation ability by
encouraging the encoder to capture intrinsic semantic features through
different data augmentations. In the subsequent stage, we propose an Iterative
Classifier Calibration (ICC) scheme to calibrate the classifier iteratively.
This is achieved by generating a large number of balanced virtual features and
fine-tuning the encoder using an Expectation-Maximization manner. The proposed
ICC compensates for minority categories to facilitate unbiased classifier
optimization while maintaining the diagnostic knowledge in majority classes.
Comprehensive experiments on three public long-tailed medical datasets
demonstrate that our LMD framework significantly surpasses state-of-the-art
approaches. The source code can be accessed at
https://github.com/peterlipan/LMD.

摘要：<paragraph>近期，计算机辅助诊断已展现出良好的性能，有效减轻了临床医师的工作量。然而，不同疾病之间固有的样本不平衡导致算法偏向于多数类别，从而导致罕见类别性能不佳。现有工作将这一挑战表述为长尾问题，并尝试通过解耦特征表示和分类来解决它。然而，由于不平衡分布和尾部类别的样本有限，这些工作容易出现偏差的表示学习和不足的分类器校准。为了解决这些问题，我们提出了一种新的长尾医学诊断 (LMD) 框架，用于对长尾数据集进行平衡的医学图像分类。在初始阶段，我们开发了一种关系感知表示学习 (RRL) 方案，通过鼓励编码器通过不同的数据增强来捕获内在语义特征，从而提升表示能力。在后续阶段，我们提出了一种迭代分类器校准 (ICC) 方案，以迭代校准分类器。这是通过生成大量平衡的虚拟特征并使用期望最大化方式微调编码器来实现的。所提出的 ICC 补偿了少数类别，以促进无偏差的分类器优化，同时保持多数类别的诊断知识。在三个公开的长尾医学数据集上的综合实验表明，我们的 LMD 框架明显优于最先进的方法。源代码可在 https://github.com/peterlipan/LMD 获取。</paragraph>

##### **MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**
2502.03004v1 by Seonok Kim

Large Language Models (LLMs) have demonstrated impressive capabilities across
natural language processing tasks. However, their application to specialized
domains such as medicine and biology requires further optimization to ensure
factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a
domain-adapted biomedical question-answering model designed to enhance both
short-form and long-form queries. By integrating fine-tuning and
retrieval-augmented generation (RAG), MedBioLM dynamically incorporates
domain-specific knowledge, improving reasoning abilities and factual accuracy.
To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA
datasets, covering structured multiple-choice assessments and complex clinical
reasoning tasks. Fine-tuning significantly improves accuracy on benchmark
datasets, while RAG enhances factual consistency. These results highlight the
potential of domain-optimized LLMs in advancing biomedical research, medical
education, and clinical decision support.

摘要：大型語言模型 (LLM) 已展現出在自然語言處理任務中令人印象深刻的能力。然而，要將其應用於醫學和生物學等特定領域，需要進一步最佳化，以確保事實的準確性、可靠性以及脈絡的深度。我們引進了 MedBioLM，這是一個適應領域的生物醫學問答模型，旨在增強短式和長式查詢。透過整合微調和檢索增強生成 (RAG)，MedBioLM 能動態地納入領域特定的知識，從而提升推理能力和事實準確性。為了評估其有效性，我們對模型進行微調，使其涵蓋結構化的多重選擇評量和複雜的臨床推理任務等多樣化的生物醫學問答資料集。微調顯著提升了基準資料集的準確性，而 RAG 則增強了事實的一致性。這些結果突顯了領域最佳化的 LLM 在推進生物醫學研究、醫學教育和臨床決策支援方面的潛力。

##### **3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**
2502.02779v1 by Weicheng Zhu, Haoxu Huang, Huanze Tang, Rushabh Musthyala, Boyang Yu, Long Chen, Emilio Vega, Thomas O'Donnell, Seena Dehkharghani, Jennifer A. Frontera, Arjun V. Masurkar, Kara Melmed, Narges Razavian

Head computed tomography (CT) imaging is a widely-used imaging modality with
multitudes of medical indications, particularly in assessing pathology of the
brain, skull, and cerebrovascular system. It is commonly the first-line imaging
in neurologic emergencies given its rapidity of image acquisition, safety,
cost, and ubiquity. Deep learning models may facilitate detection of a wide
range of diseases. However, the scarcity of high-quality labels and
annotations, particularly among less common conditions, significantly hinders
the development of powerful models. To address this challenge, we introduce
FM-CT: a Foundation Model for Head CT for generalizable disease detection,
trained using self-supervised learning. Our approach pre-trains a deep learning
model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans
without the need for manual annotations, enabling the model to learn robust,
generalizable features. To investigate the potential of self-supervised
learning in head CT, we employed both discrimination with self-distillation and
masked image modeling, and we construct our model in 3D rather than at the
slice level (2D) to exploit the structure of head CT scans more comprehensively
and efficiently. The model's downstream classification performance is evaluated
using internal and three external datasets, encompassing both in-distribution
(ID) and out-of-distribution (OOD) data. Our results demonstrate that the
self-supervised foundation model significantly improves performance on
downstream diagnostic tasks compared to models trained from scratch and
previous 3D CT foundation models on scarce annotated datasets. This work
highlights the effectiveness of self-supervised learning in medical imaging and
sets a new benchmark for head CT image analysis in 3D, enabling broader use of
artificial intelligence for head CT-based diagnosis.

摘要：頭部電腦斷層掃描（CT）影像是一種廣泛使用的影像模式，具有
大量的醫療適應症，特別是在評估腦部、頭骨和腦血管系統的病理時。由於其影像擷取速度快、安全性、成本低和普遍性，通常是神經緊急情況下的第一線影像。深度學習模型可以促進對各種疾病的檢測。然而，高品質標籤和註釋的稀缺，特別是在較不常見的疾病中，顯著地阻礙了強大模型的發展。為了應對這一挑戰，我們引入了 FM-CT：一個用於頭部 CT 的基礎模型，用於可概化的疾病檢測，並使用自我監督學習進行訓練。我們的做法在一個包含 361,663 個非對比 3D 頭部 CT 掃描的大型、多樣化的數據集上預訓練一個深度學習模型，而無需手動註釋，使模型能夠學習強健、可概化的特徵。為了探討自我監督學習在頭部 CT 中的潛力，我們同時採用了帶有自我蒸餾的判別和遮罩影像建模，並且我們以 3D 而不是切片層級（2D）構建我們的模型，以更全面、有效地利用頭部 CT 掃描的結構。該模型的下游分類效能使用內部和三個外部數據集進行評估，包括分佈內 (ID) 和分佈外 (OOD) 資料。我們的結果表明，與從頭開始訓練的模型和先前在稀疏註釋數據集上訓練的 3D CT 基礎模型相比，自我監督基礎模型顯著改善了下游診斷任務的效能。這項工作突顯了自我監督學習在醫學影像中的有效性，並為 3D 頭部 CT 影像分析設定了一個新的基準，讓人工智慧能夠更廣泛地用於基於頭部 CT 的診斷。

##### **Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**
2502.02756v1 by Obed Korshie Dzikunu, Shadab Ahamed, Amirhossein Toosi, Xiaoxiao Li, Arman Rahmim

This study proposes a new loss function for deep neural networks, L1-weighted
Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of
voxels based on their classification difficulty, towards automated detection
and segmentation of metastatic prostate cancer lesions in PET/CT scans. We
obtained 380 PSMA [18-F] DCFPyL PET/CT scans of patients diagnosed with
biochemical recurrence metastatic prostate cancer. We trained two 3D
convolutional neural networks, Attention U-Net and SegResNet, and concatenated
the PET and CT volumes channel-wise as input. The performance of our custom
loss function was evaluated against the Dice and Dice Focal Loss functions. For
clinical significance, we considered a detected region of interest (ROI) as a
true positive if at least the voxel with the maximum standardized uptake value
falls within the ROI. We assessed the models' performance based on the number
of lesions in an image, tumour volume, activity, and extent of spread. The
L1DFL outperformed the comparative loss functions by at least 13% on the test
set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were
lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal
Loss yielded more false positives, whereas the Dice Loss was more sensitive to
smaller volumes and struggled to segment larger lesions accurately. They also
exhibited network-specific variations and yielded declines in segmentation
accuracy with increased tumour spread. Our results demonstrate the potential of
L1DFL to yield robust segmentation of metastatic prostate cancer lesions in
PSMA PET/CT images. The results further highlight potential complexities
arising from the variations in lesion characteristics that may influence
automated prostate cancer tumour detection and segmentation. The code is
publicly available at: https://github.com/ObedDzik/pca_segment.git.

摘要：<paragraph>本研究針對深度神經網路提出一個新的損失函數，L1 加權 Dice 焦點損失 (L1DFL)，它利用 L1 範數根據體素的分類難度進行自適應加權，用於自動偵測和分割 PET/CT 掃描中轉移性前列腺癌病灶。我們取得 380 個經診斷為生化復發轉移性前列腺癌的患者的 PSMA [18-F] DCFPyL PET/CT 掃描。我們訓練了兩個 3D 捲積神經網路，Attention U-Net 和 SegResNet，並將 PET 和 CT 體積按通道連接作為輸入。我們自訂的損失函數的效能與 Dice 和 Dice 焦點損失函數進行評估。為了臨床意義，我們將一個偵測到的感興趣區域 (ROI) 視為真陽性，如果至少具有最大標準攝取值的體素落在 ROI 內。我們根據影像中的病灶數量、腫瘤體積、活性，以及擴散程度評估模型的效能。L1DFL 在測試組中至少比比較損失函數高出 13%。此外，Dice 損失和 Dice 焦點損失的 F1 分數分別比 L1DFL 低至少 6% 和 34%。Dice 焦點損失產生更多假陽性，而 Dice 損失對較小體積較為敏感，且難以準確分割較大病灶。它們也展現出網路特定的變化，並隨著腫瘤擴散而導致分割準確度下降。我們的結果證明 L1DFL 具有在 PSMA PET/CT 影像中產生轉移性前列腺癌病灶的強健分割的潛力。結果進一步強調由病灶特徵變化所產生的潛在複雜性，這可能會影響自動化前列腺癌腫瘤偵測和分割。程式碼公開於：https://github.com/ObedDzik/pca_segment.git。</paragraph>

##### **MedRAX: Medical Reasoning Agent for Chest X-ray**
2502.02673v1 by Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang

Chest X-rays (CXRs) play an integral role in driving critical decisions in
disease management and patient care. While recent innovations have led to
specialized models for various CXR interpretation tasks, these solutions often
operate in isolation, limiting their practical utility in clinical practice. We
present MedRAX, the first versatile AI agent that seamlessly integrates
state-of-the-art CXR analysis tools and multimodal large language models into a
unified framework. MedRAX dynamically leverages these models to address complex
medical queries without requiring additional training. To rigorously evaluate
its capabilities, we introduce ChestAgentBench, a comprehensive benchmark
containing 2,500 complex medical queries across 7 diverse categories. Our
experiments demonstrate that MedRAX achieves state-of-the-art performance
compared to both open-source and proprietary models, representing a significant
step toward the practical deployment of automated CXR interpretation systems.
Data and code have been publicly available at
https://github.com/bowang-lab/MedRAX

摘要：胸部 X 光片 (CXR) 在疾病管理和患者照護中扮演著不可或缺的角色，推動著關鍵決策的制定。儘管近期的創新已針對各種 CXR 解讀任務開發出專門的模型，但這些解決方案通常獨立運作，限制了它們在臨床實務中的實際效用。我們提出 MedRAX，這是一款首創的多功能 AI 代理，它將最先進的 CXR 分析工具和多模態大型語言模型無縫整合到一個統一的架構中。MedRAX 動態運用這些模型來解決複雜的醫療查詢，而無需額外的訓練。為了嚴格評估其功能，我們引入了 ChestAgentBench，這是一個全面的基準，包含 7 個不同類別的 2,500 個複雜醫療查詢。我們的實驗證明，與開源和專有模型相比，MedRAX 達到了最先進的效能，這代表了自動化 CXR 解讀系統實際部署的重要一步。資料和程式碼已公開於 https://github.com/bowang-lab/MedRAX

##### **Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**
2502.02561v1 by Shayan Kiyani, George Pappas, Aaron Roth, Hamed Hassani

A fundamental question in data-driven decision making is how to quantify the
uncertainty of predictions in ways that can usefully inform downstream action.
This interface between prediction uncertainty and decision-making is especially
important in risk-sensitive domains, such as medicine. In this paper, we
develop decision-theoretic foundations that connect uncertainty quantification
using prediction sets with risk-averse decision-making. Specifically, we answer
three fundamental questions: (1) What is the correct notion of uncertainty
quantification for risk-averse decision makers? We prove that prediction sets
are optimal for decision makers who wish to optimize their value at risk. (2)
What is the optimal policy that a risk averse decision maker should use to map
prediction sets to actions? We show that a simple max-min decision policy is
optimal for risk-averse decision makers. Finally, (3) How can we derive
prediction sets that are optimal for such decision makers? We provide an exact
characterization in the population regime and a distribution free finite-sample
construction. Answering these questions naturally leads to an algorithm,
Risk-Averse Calibration (RAC), which follows a provably optimal design for
deriving action policies from predictions. RAC is designed to be both
practical-capable of leveraging the quality of predictions in a black-box
manner to enhance downstream utility-and safe-adhering to a user-defined risk
threshold and optimizing the corresponding risk quantile of the user's
downstream utility. Finally, we experimentally demonstrate the significant
advantages of RAC in applications such as medical diagnosis and recommendation
systems. Specifically, we show that RAC achieves a substantially improved
trade-off between safety and utility, offering higher utility compared to
existing methods while maintaining the safety guarantee.

摘要：<paragraph>在資料驅動決策中，一個基本問題是，如何量化預測的不確定性，以能有用地告知下游行動。
預測不確定性和決策制定之間的這種介面，在風險敏感領域中特別重要，例如醫學。在本文中，我們
發展了決策理論基礎，它利用預測集合將不確定性量化與風險規避決策制定聯繫起來。具體來說，我們回答
了三個基本問題：(1) 對於風險規避決策者來說，不確定性量化的正確概念是什麼？我們證明，對於希望最佳化其風險價值的決策者來說，預測集合是最佳的。(2)
風險規避決策者應使用什麼最佳政策，將預測集合映射到行動？我們表明，對於風險規避決策者來說，一個簡單的最大最小決策政策是最佳的。最後，(3) 我們如何推導出對此類決策者來說最佳的預測集合？我們在總體範圍內提供了一個確切的表徵，並提供了一個不依賴分佈的有限樣本建構。回答這些問題自然會導致一個演算法，風險規避校準 (RAC)，它遵循一個可證明最佳的設計，從預測中推導出行動政策。RAC 被設計為既實用——能夠以黑盒方式利用預測的品質來增強下游效用——又安全——遵守使用者定義的風險閾值，並最佳化使用者的下游效用的對應風險分位數。最後，我們在醫學診斷和推薦系統等應用中，以實驗方式證明了 RAC 的顯著優點。具體來說，我們表明，與現有方法相比，RAC 在安全性和效用之間實現了顯著改善的折衷，在維持安全保證的同時，提供了更高的效用。</paragraph>

##### **A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**
2502.02489v1 by Edward Ellis, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali

Ultrasound (US) imaging is clinically invaluable due to its noninvasive and
safe nature. However, interpreting US images is challenging, requires
significant expertise, and time, and is often prone to errors. Deep learning
offers assistive solutions such as segmentation. Supervised methods rely on
large, high-quality, and consistently labeled datasets, which are challenging
to curate. Moreover, these methods tend to underperform on out-of-distribution
data, limiting their clinical utility. Self-supervised learning (SSL) has
emerged as a promising alternative, leveraging unlabeled data to enhance model
performance and generalisability. We introduce a contrastive SSL approach
tailored for B-mode US images, incorporating a novel Relation Contrastive Loss
(RCL). RCL encourages learning of distinct features by differentiating positive
and negative sample pairs through a learnable metric. Additionally, we propose
spatial and frequency-based augmentation strategies for the representation
learning on US images. Our approach significantly outperforms traditional
supervised segmentation methods across three public breast US datasets,
particularly in data-limited scenarios. Notable improvements on the Dice
similarity metric include a 4% increase on 20% and 50% of the BUSI dataset,
nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4%
and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.
Furthermore, we demonstrate superior generalisability on the
out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6%
compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST
training data, respectively. Our research highlights that domain-inspired SSL
can improve US segmentation, especially under data-limited conditions.

摘要：超音波 (US) 影像由於其非侵入性且安全的特性，在臨床上極具價值。然而，解讀超音波影像具有挑戰性，需要大量的專業知識和時間，而且經常容易出錯。深度學習提供了輔助解決方案，例如分割。監督式方法依賴於大量、高品質且標籤一致的資料集，而這在策劃上具有挑戰性。此外，這些方法在分佈外資料上的表現往往不佳，這限制了它們的臨床效用。自監督學習 (SSL) 已成為一種有前途的替代方案，它利用未標籤資料來增強模型效能和泛化能力。我們提出了一種對比式 SSL 方法，專門針對 B 模式超音波影像，並納入了新穎的關係對比損失 (RCL)。RCL 透過一個可學習的指標區分正負樣本對，來鼓勵學習不同的特徵。此外，我們提出了用於超音波影像上表徵學習的空間和頻率增強策略。我們的做法在三個公開的乳房超音波資料集上顯著優於傳統的監督式分割方法，特別是在資料有限的情況下。在 Dice 相似性指標上的顯著改進包括在 BUSI 資料集的 20% 和 50% 上增加了 4%，在 BrEaST 資料集的 20% 和 50% 上增加了近 6% 和 9%，以及在 UDIAT 資料集的 20% 和 50% 上分別增加了 6.4% 和 3.7%。此外，我們在分佈外的 UDIAT 資料集上展示了卓越的泛化能力，與使用 BUSI 和 BrEaST 訓練資料的 20% 和 50% 的監督式基準相比，效能分別提升了 20.6% 和 13.6%。我們的研究強調，領域啟發的 SSL 可以改善超音波分割，特別是在資料有限的條件下。

##### **Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**
2502.02438v1 by Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz

Medical multimodal large language models (MLLMs) are becoming an instrumental
part of healthcare systems, assisting medical personnel with decision making
and results analysis. Models for radiology report generation are able to
interpret medical imagery, thus reducing the workload of radiologists. As
medical data is scarce and protected by privacy regulations, medical MLLMs
represent valuable intellectual property. However, these assets are potentially
vulnerable to model stealing, where attackers aim to replicate their
functionality via black-box access. So far, model stealing for the medical
domain has focused on classification; however, existing attacks are not
effective against MLLMs. In this paper, we introduce Adversarial Domain
Alignment (ADA-STEAL), the first stealing attack against medical MLLMs.
ADA-STEAL relies on natural images, which are public and widely available, as
opposed to their medical counterparts. We show that data augmentation with
adversarial noise is sufficient to overcome the data distribution gap between
natural images and the domain-specific distribution of the victim MLLM.
Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that
Adversarial Domain Alignment enables attackers to steal the medical MLLM
without any access to medical data.

摘要：醫療多模態大型語言模型 (MLLM) 正在成為醫療保健系統中不可或缺的一部分，協助醫療人員進行決策和結果分析。放射報告生成的模型能夠解釋醫學影像，從而減輕放射科醫師的工作負擔。由於醫療資料稀少且受隱私法規保護，醫療 MLLM 代表了有價值的智慧財產。然而，這些資產潛在地容易受到模型竊取的攻擊，攻擊者旨在透過黑盒存取來複製其功能。到目前為止，針對醫療領域的模型竊取一直專注於分類；然而，現有的攻擊對 MLLM 沒有效。在本文中，我們介紹了對抗域對齊 (ADA-STEAL)，這是針對醫療 MLLM 的第一個竊取攻擊。與醫療對應物相反，ADA-STEAL 依賴於公開且廣泛可用的自然影像。我們表明，對抗雜訊的資料擴充足以克服自然影像與受害者 MLLM 的特定領域分佈之間的資料分佈差距。在 IU X-RAY 和 MIMIC-CXR 放射學資料集上進行的實驗表明，對抗域對齊使攻擊者能夠在不存取任何醫療資料的情況下竊取醫療 MLLM。

##### **Test Time Training for 4D Medical Image Interpolation**
2502.02341v1 by Qikang Zhang, Yingjie Lei, Zihao Zheng, Ziyang Chen, Zhonghao Xie

4D medical image interpolation is essential for improving temporal resolution
and diagnostic precision in clinical applications. Previous works ignore the
problem of distribution shifts, resulting in poor generalization under
different distribution. A natural solution would be to adapt the model to a new
test distribution, but this cannot be done if the test input comes without a
ground truth label. In this paper, we propose a novel test time training
framework which uses self-supervision to adapt the model to a new distribution
without requiring any labels. Indeed, before performing frame interpolation on
each test video, the model is trained on the same instance using a
self-supervised task, such as rotation prediction or image reconstruction. We
conduct experiments on two publicly available 4D medical image interpolation
datasets, Cardiac and 4D-Lung. The experimental results show that the proposed
method achieves significant performance across various evaluation metrics on
both datasets. It achieves higher peak signal-to-noise ratio values, 33.73dB on
Cardiac and 34.02dB on 4D-Lung. Our method not only advances 4D medical image
interpolation but also provides a template for domain adaptation in other
fields such as image segmentation and image registration.

摘要：4D 醫學影像插值對於提升時間解析度及臨床應用中的診斷精準度至關重要。過往的研究忽略了分佈轉移問題，導致在不同分佈下泛化能力不佳。一個自然的解決方案是將模型適應到新的測試分佈，但如果測試輸入沒有真實標籤，就無法做到這一點。在本文中，我們提出了一個新的測試時間訓練架構，它使用自我監督來適應模型到一個新的分佈，而不需要任何標籤。事實上，在對每個測試影片執行幀插值之前，使用自我監督任務（例如旋轉預測或影像重建）在同一個實例上訓練模型。我們在兩個公開的 4D 醫學影像插值資料集（Cardiac 和 4D-Lung）上進行實驗。實驗結果表明，所提出的方法在兩個資料集上的各種評估指標中都取得了顯著的效能。它達到了更高的峰值信噪比值，在 Cardiac 上為 33.73dB，在 4D-Lung 上為 34.02dB。我們的技術不僅推動了 4D 醫學影像插值，還為其他領域（例如影像分割和影像配準）中的領域適應提供了一個範本。

##### **Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**
2502.02249v1 by Atharva Mangeshkumar Agrawal, Rutika Pandurang Shinde, Vasanth Kumar Bhukya, Ashmita Chakraborty, Sagar Bharat Shah, Tanmay Shukla, Sree Pradeep Kumar Relangi, Nilesh Mutyam

Large language models (LLMs) have shown impressive capabilities in natural
language processing tasks, including dialogue generation. This research aims to
conduct a novel comparative analysis of two prominent techniques, fine-tuning
with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)
framework, in the context of doctor-patient chat conversations with multiple
datasets of mixed medical domains. The analysis involves three state-of-the-art
models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient
dialogues, we comprehensively evaluate the performance of models, assessing key
metrics such as language quality (perplexity, BLEU score), factual accuracy
(fact-checking against medical knowledge bases), adherence to medical
guidelines, and overall human judgments (coherence, empathy, safety). The
findings provide insights into the strengths and limitations of each approach,
shedding light on their suitability for healthcare applications. Furthermore,
the research investigates the robustness of the models in handling diverse
patient queries, ranging from general health inquiries to specific medical
conditions. The impact of domain-specific knowledge integration is also
explored, highlighting the potential for enhancing LLM performance through
targeted data augmentation and retrieval strategies.

摘要：大型語言模型 (LLM) 在自然語言處理任務中展現了令人印象深刻的能力，包括對話生成。本研究旨在對兩種著名的技術進行新穎的比較分析，即微調 LoRA (低秩適應) 和檢索增強生成 (RAG) 框架，在具有混合醫療領域的多個資料集的醫患聊天對話中。分析涉及三個最先進的模型：Llama-2、GPT 和 LSTM 模型。採用真實世界的醫患對話，我們全面評估模型的性能，評估語言品質（困惑度、BLEU 分數）、事實準確性（對照醫學知識庫進行事實查核）、遵守醫療指南以及整體人類判斷（連貫性、同理心、安全性）等關鍵指標。研究結果深入了解了每種方法的優點和限制，闡明了它們適用於醫療保健應用的適當性。此外，該研究調查了模型在處理多樣化患者查詢時的穩健性，範圍從一般健康詢問到特定醫療狀況。還探討了特定領域知識整合的影響，強調了通過有針對性的資料擴充和檢索策略來增強 LLM 性能的潛力。

##### **Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**
2502.02618v1 by F. Xavier Gaya-Morey, Jose M. Buades-Rubio, Philippe Palanque, Raquel Lacuesta, Cristina Manresa-Yee

The rapid aging of the global population has highlighted the need for
technologies to support elderly, particularly in healthcare and emotional
well-being. Facial expression recognition (FER) systems offer a non-invasive
means of monitoring emotional states, with applications in assisted living,
mental health support, and personalized care. This study presents a systematic
review of deep learning-based FER systems, focusing on their applications for
the elderly population. Following a rigorous methodology, we analyzed 31
studies published over the last decade, addressing challenges such as the
scarcity of elderly-specific datasets, class imbalances, and the impact of
age-related facial expression differences. Our findings show that convolutional
neural networks remain dominant in FER, and especially lightweight versions for
resource-constrained environments. However, existing datasets often lack
diversity in age representation, and real-world deployment remains limited.
Additionally, privacy concerns and the need for explainable artificial
intelligence emerged as key barriers to adoption. This review underscores the
importance of developing age-inclusive datasets, integrating multimodal
solutions, and adopting XAI techniques to enhance system usability,
reliability, and trustworthiness. We conclude by offering recommendations for
future research to bridge the gap between academic progress and real-world
implementation in elderly care.

摘要：全球人口快速老龄化突显了对技术的需求，以支持老年人，尤其是在医疗保健和情绪健康方面。面部表情识别 (FER) 系统提供了一种非侵入性的情绪状态监测手段，在辅助生活、心理健康支持和个性化护理中得到应用。本研究对基于深度学习的 FER 系统进行了系统的回顾，重点关注它们在老年人群中的应用。遵循严格的方法，我们分析了在过去十年中发表的 31 项研究，解决了诸如老年人特定数据集的稀缺性、类别不平衡以及与年龄相关的面部表情差异的影响等挑战。我们的研究结果表明，卷积神经网络在 FER 中仍然占主导地位，特别是针对资源受限环境的轻量级版本。然而，现有数据集往往缺乏年龄代表性的多样性，并且现实世界的部署仍然有限。此外，隐私问题和对可解释人工智能的需求已成为采用过程中的主要障碍。本次审查强调了开发包容年龄的数据集、整合多模式解决方案以及采用 XAI 技术以增强系统可用性、可靠性和可信度的重要性。最后，我们提出了未来研究的建议，以弥合学术进展与老年护理中的现实世界实施之间的差距。

##### **Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**
2502.02109v1 by Yuxiao Cheng, Xinxin Song, Ziqian Wang, Qin Zhong, Kunlun He, Jinli Suo

Recent advances in deep learning (DL) have prompted the development of
high-performing early warning score (EWS) systems, predicting clinical
deteriorations such as acute kidney injury, acute myocardial infarction, or
circulatory failure. DL models have proven to be powerful tools for various
tasks but come with the cost of lacking interpretability and limited
generalizability, hindering their clinical applications. To develop a practical
EWS system applicable to various outcomes, we propose causally-informed
explainable early prediction model, which leverages causal discovery to
identify the underlying causal relationships of prediction and thus owns two
unique advantages: demonstrating the explicit interpretation of the prediction
while exhibiting decent performance when applied to unfamiliar environments.
Benefiting from these features, our approach achieves superior accuracy for 6
different critical deteriorations and achieves better generalizability across
different patient groups, compared to various baseline algorithms. Besides, we
provide explicit causal pathways to serve as references for assistant clinical
diagnosis and potential interventions. The proposed approach enhances the
practical application of deep learning in various medical scenarios.

摘要：深度學習 (DL) 的最新進展促使開發出高性能早期預警評分 (EWS) 系統，預測急性腎臟損傷、急性心肌梗塞或循環衰竭等臨床惡化。DL 模型已被證明是各種任務的強大工具，但代價是缺乏可解釋性和有限的概括性，阻礙了其臨床應用。為了開發適用於各種結果的實用 EWS 系統，我們提出了因果關係解釋性早期預測模型，它利用因果發現來識別預測的潛在因果關係，從而擁有兩個獨特的優點：展示預測的明確解釋，同時在應用於不熟悉的環境時表現出良好的性能。得益於這些特性，與各種基線演算法相比，我們的模型在 6 種不同的危重惡化中實現了更高的準確度，並在不同的患者群體中實現了更好的概括性。此外，我們提供了明確的因果途徑，作為輔助臨床診斷和潛在干預措施的參考。所提出的方法增強了深度學習在各種醫療場景中的實際應用。

##### **An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**
2502.01789v1 by Jiazi Tian, Liqin Wang, Pedram Fard, Valdery Moura Junior, Deborah Blacker, Jennifer S. Haas, Chirag Patel, Shawn N. Murphy, Lidia M. V. R. Moura, Hossein Estiri

Early identification of cognitive concerns is critical but often hindered by
subtle symptom presentation. This study developed and validated a fully
automated, multi-agent AI workflow using LLaMA 3 8B to identify cognitive
concerns in 3,338 clinical notes from Mass General Brigham. The agentic
workflow, leveraging task-specific agents that dynamically collaborate to
extract meaningful insights from clinical notes, was compared to an
expert-driven benchmark. Both workflows achieved high classification
performance, with F1-scores of 0.90 and 0.91, respectively. The agentic
workflow demonstrated improved specificity (1.00) and achieved prompt
refinement in fewer iterations. Although both workflows showed reduced
performance on validation data, the agentic workflow maintained perfect
specificity. These findings highlight the potential of fully automated
multi-agent AI workflows to achieve expert-level accuracy with greater
efficiency, offering a scalable and cost-effective solution for detecting
cognitive concerns in clinical settings.

摘要：及早辨識認知問題至關重要，但常常受到症狀呈現過於細微的阻礙。本研究開發並驗證了一個全自動化、多重代理的 AI 工作流程，使用 LLaMA 3 8B 來辨識來自麻省總醫院布萊根分院的 3,338 則臨床筆記中的認知問題。這個代理工作流程利用了特定任務的代理，這些代理會動態合作從臨床筆記中萃取出有意義的見解，並與專家驅動的基準進行比較。這兩個工作流程都達到了很高的分類效能，F1 分數分別為 0.90 和 0.91。代理工作流程展現出更好的特異性（1.00），並且在更少的反覆運算中達到了提示精煉。儘管這兩個工作流程在驗證資料上的效能都降低了，但代理工作流程維持了完美的特異性。這些發現突顯了全自動化多重代理 AI 工作流程的潛力，它們能以更高的效率達到專家級的準確度，為在臨床環境中偵測認知問題提供了一個可擴充且具成本效益的解決方案。

##### **Improving Transformer World Models for Data-Efficient RL**
2502.01591v1 by Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy

We present an approach to model-based RL that achieves a new state of the art
performance on the challenging Craftax-classic benchmark, an open-world 2D
survival game that requires agents to exhibit a wide range of general abilities
-- such as strong generalization, deep exploration, and long-term reasoning.
With a series of careful design choices aimed at improving sample efficiency,
our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps,
significantly outperforming DreamerV3, which achieves 53.2%, and, for the first
time, exceeds human performance of 65.0%. Our method starts by constructing a
SOTA model-free baseline, using a novel policy architecture that combines CNNs
and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna
with warmup", which trains the policy on real and imaginary data, (b) "nearest
neighbor tokenizer" on image patches, which improves the scheme to create the
transformer world model (TWM) inputs, and (c) "block teacher forcing", which
allows the TWM to reason jointly about the future tokens of the next timestep.

摘要：我們提出了一個基於模型的 RL 方法，在具有挑戰性的 Craftax-classic 基準上實現了新的技術水準，這是一個開放世界的 2D 生存遊戲，要求代理人展現廣泛的一般能力，例如強大的概括能力、深入探索和長期推理。通過一系列旨在提高樣本效率的仔細設計選擇，我們的 MBRL 演算法在僅 1M 環境步驟後就實現了 67.4% 的獎勵，顯著優於 DreamerV3（實現 53.2%），並且首次超過了人類的 65.0% 的表現。我們的演算法首先通過使用結合 CNN 和 RNN 的新穎策略架構來建構一個 SOTA 無模型基線。然後，我們對標準 MBRL 設定新增了三項改進：(a)「帶熱身的 Dyna」，它在真實和假想資料上訓練策略，(b) 影像貼片的「最近鄰代碼化器」，它改進了建立轉換器世界模型 (TWM) 輸入的方案，以及 (c)「區塊教師強制」，它允許 TWM 共同推理下一個時間步長的未來代碼。

##### **Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**
2502.01377v1 by Zhi Zhang, Yan Liu, Mengxia Gao, Yu Yang, Jiannong Cao, Wai Kai Hou, Shirley Li, Sonata Yau, Yun Kwok Wing, Tatia M. C. Lee

Psychological resilience, defined as the ability to rebound from adversity,
is crucial for mental health. Compared with traditional resilience assessments
through self-reported questionnaires, resilience assessments based on
neurological data offer more objective results with biological markers, hence
significantly enhancing credibility. This paper proposes a novel data-efficient
model to address the scarcity of neurological data. We employ Neuro
Kolmogorov-Arnold Networks as the structure of the prediction model. In the
training stage, a new trait-informed multimodal representation algorithm with a
smart chunk technique is proposed to learn the shared latent space with limited
data. In the test stage, a new noise-informed inference algorithm is proposed
to address the low signal-to-noise ratio of the neurological data. The proposed
model not only shows impressive performance on both public datasets and
self-constructed datasets but also provides some valuable psychological
hypotheses for future research.

摘要：心理韌性，定義為從逆境中反彈的能力，對心理健康至關重要。與通過自我報告問卷的傳統韌性評估相比，基於神經數據的韌性評估提供了更客觀的結果和生物標記，從而顯著提高了可信度。本文提出了一個新穎的數據高效模型來解決神經數據的稀缺性。我們採用神經科爾莫哥羅夫-阿諾德網路作為預測模型的結構。在訓練階段，提出了一種新的特徵信息多模態表示算法，採用智能塊技術，以有限的數據學習共享潛在空間。在測試階段，提出了一種新的噪聲信息推理算法，以解決神經數據的信噪比低的問題。所提出的模型不僅在公共數據集和自構數據集上都顯示出令人印象深刻的性能，還為未來的研究提供了一些有價值的心理假設。

##### **OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**
2502.01243v1 by Chengfeng Zhou, Ji Wang, Juanjuan Qin, Yining Wang, Ling Sun, Weiwei Dai

Large language models (LLMs) have shown significant promise across various
medical applications, with ophthalmology being a notable area of focus. Many
ophthalmic tasks have shown substantial improvement through the integration of
LLMs. However, before these models can be widely adopted in clinical practice,
evaluating their capabilities and identifying their limitations is crucial. To
address this research gap and support the real-world application of LLMs, we
introduce the OphthBench, a specialized benchmark designed to assess LLM
performance within the context of Chinese ophthalmic practices. This benchmark
systematically divides a typical ophthalmic clinical workflow into five key
scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each
scenario, we developed multiple tasks featuring diverse question types,
resulting in a comprehensive benchmark comprising 9 tasks and 591 questions.
This comprehensive framework allows for a thorough assessment of LLMs'
capabilities and provides insights into their practical application in Chinese
ophthalmology. Using this benchmark, we conducted extensive experiments and
analyzed the results from 39 popular LLMs. Our evaluation highlights the
current gap between LLM development and its practical utility in clinical
settings, providing a clear direction for future advancements. By bridging this
gap, we aim to unlock the potential of LLMs and advance their development in
ophthalmology.

摘要：大型語言模型 (LLM) 在各種醫療應用中已展現出顯著的潛力，其中眼科是一個值得關注的重要領域。許多眼科任務已透過整合 LLM 而大幅進步。然而，在這些模型能廣泛應用於臨床實務之前，評估其能力並找出其限制至關重要。為了解決這個研究差距並支援 LLM 的實際應用，我們引入了 OphthBench，這是一個專門的基準測試，旨在評估 LLM 在中國眼科實務中的表現。此基準測試系統性地將典型眼科臨床工作流程劃分為五個關鍵情境：教育、分流、診斷、治療和預後。對於每個情境，我們開發了多項任務，包含多樣化的問題類型，最後組成一個包含 9 項任務和 591 個問題的綜合基準測試。此綜合架構可徹底評估 LLM 的能力，並提供其在中國眼科的實際應用見解。使用此基準測試，我們進行了廣泛的實驗，並分析了來自 39 個熱門 LLM 的結果。我們的評估強調了 LLM 開發與其在臨床環境中的實際效用之間的差距，為未來的進展提供了明確的方向。透過彌合此差距，我們旨在釋放 LLM 的潛力，並促進其在眼科的發展。

##### **MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**
2502.01158v1 by Alejandro Guerra-Manzanares, Farah E. Shamout

Multimodal fusion leverages information across modalities to learn better
feature representations with the goal of improving performance in fusion-based
tasks. However, multimodal datasets, especially in medical settings, are
typically smaller than their unimodal counterparts, which can impede the
performance of multimodal models. Additionally, the increase in the number of
modalities is often associated with an overall increase in the size of the
multimodal network, which may be undesirable in medical use cases. Utilizing
smaller unimodal encoders may lead to sub-optimal performance, particularly
when dealing with high-dimensional clinical data. In this paper, we propose the
Modality-INformed knowledge Distillation (MIND) framework, a multimodal model
compression approach based on knowledge distillation that transfers knowledge
from ensembles of pre-trained deep neural networks of varying sizes into a
smaller multimodal student. The teacher models consist of unimodal networks,
allowing the student to learn from diverse representations. MIND employs
multi-head joint fusion models, as opposed to single-head models, enabling the
use of unimodal encoders in the case of unimodal samples without requiring
imputation or masking of absent modalities. As a result, MIND generates an
optimized multimodal model, enhancing both multimodal and unimodal
representations. It can also be leveraged to balance multimodal learning during
training. We evaluate MIND on binary and multilabel clinical prediction tasks
using time series data and chest X-ray images. Additionally, we assess the
generalizability of the MIND framework on three non-medical multimodal
multiclass datasets. Experimental results demonstrate that MIND enhances the
performance of the smaller multimodal network across all five tasks, as well as
various fusion methods and multimodal architectures, compared to
state-of-the-art baselines.

摘要：多模态融合利用跨模态的信息来学习更好的特征表示，目标是提升基于融合的任务的性能。然而，多模态数据集，尤其是在医疗环境中，通常比它们的单模态对应数据集小，这会阻碍多模态模型的性能。此外，模态数量的增加通常与多模态网络尺寸的整体增加相关，这在医疗用例中可能是不可取的。利用较小的单模态编码器可能会导致次优性能，尤其是在处理高维临床数据时。在本文中，我们提出了模态信息知识蒸馏 (MIND) 框架，这是一种基于知识蒸馏的多模态模型压缩方法，它将来自不同大小的预训练深度神经网络的集合中的知识转移到一个较小的多模态学生中。教师模型由单模态网络组成，允许学生从不同的表示中学习。MIND 采用多头联合融合模型，而不是单头模型，从而能够在单模态样本的情况下使用单模态编码器，而不需要缺失模态的插补或掩蔽。因此，MIND 生成了一个经过优化的多模态模型，增强了多模态和单模态表示。它还可以用来在训练期间平衡多模态学习。我们使用时间序列数据和胸部 X 射线图像对二元和多标签临床预测任务评估了 MIND。此外，我们评估了 MIND 框架在三个非医疗多模态多分类数据集上的泛化性。实验结果表明，与最先进的基线相比，MIND 增强了较小的多模态网络在所有五个任务以及各种融合方法和多模态架构中的性能。

##### **Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**
2502.01141v1 by Qian Chen, Stefanie Rinderle-Ma, Lijie Wen

Most existing process compliance monitoring approaches detect compliance
violations in an ex post manner. Only predicate prediction focuses on
predicting them. However, predicate prediction provides a binary yes/no notion
of compliance, lacking the ability to measure to which extent an ongoing
process instance deviates from the desired state as specified in constraints.
Here, being able to quantify the magnitude of violation would provide
organizations with deeper insights into their operational performance, enabling
informed decision making to reduce or mitigate the risk of non-compliance.
Thus, we propose two predictive compliance monitoring approaches to close this
research gap. The first approach reformulates the binary classification problem
as a hybrid task that considers both classification and regression, while the
second employs a multi-task learning method to explicitly predict the
compliance status and the magnitude of violation for deviant cases
simultaneously. In this work, we focus on temporal constraints as they are
significant in almost any application domain, e.g., health care. The evaluation
on synthetic and real-world event logs demonstrates that our approaches are
capable of quantifying the magnitude of violations while maintaining comparable
performance for compliance predictions achieved by state-of-the-art approaches.

摘要：現有的流程合規監控方法大多會在事後偵測到合規違規。只有謂詞預測專注於預測這些違規。然而，謂詞預測提供的是合規與否的二元概念，無法衡量正在進行的流程實例偏離約束中所指定之理想狀態的程度。在此，能夠量化違規的嚴重程度，將能讓組織深入了解其營運績效，並能據此做出明智的決策，以降低或減輕不合規的風險。因此，我們提出兩種預測合規監控方法來填補此研究空白。第一種方法將二元分類問題重新表述為同時考量分類和回歸的混合任務，而第二種方法則採用多任務學習方法，同時明確預測合規狀態和偏差案例的違規嚴重程度。在這項工作中，我們專注於時間約束，因為它們幾乎在任何應用領域（例如醫療保健）中都很重要。在合成和真實世界事件記錄上的評估顯示，我們的做法能夠量化違規的嚴重程度，同時維持與現有方法所達成的合規預測相當的績效。

##### **Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**
2502.01108v1 by Mithun Saha, Maxwell A. Xu, Wanting Mao, Sameer Neupane, James M. Rehg, Santosh Kumar

Photoplethysmography (PPG)-based foundation models are gaining traction due
to the widespread use of PPG in biosignal monitoring and their potential to
generalize across diverse health applications. In this paper, we introduce
Pulse-PPG, the first open-source PPG foundation model trained exclusively on
raw PPG data collected over a 100-day field study with 120 participants.
Existing PPG foundation models are either open-source but trained on clinical
data or closed-source, limiting their applicability in real-world settings. We
evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its
performance against a state-of-the-art foundation model trained on clinical
data. Our results demonstrate that Pulse-PPG, trained on uncurated field data,
exhibits superior generalization across clinical and mobile health applications
in both lab and field settings. This suggests that exposure to real-world
variability enables the model to learn fine-grained representations, making it
more adaptable across tasks. Furthermore, pre-training on field data
surprisingly outperforms its pre-training on clinical data in many tasks,
reinforcing the importance of training on real-world, diverse datasets. To
encourage further advancements in robust foundation models leveraging field
data, we plan to release Pulse-PPG, providing researchers with a powerful
resource for developing more generalizable PPG-based models.

摘要：基於光電容積描記術 (PPG) 的基礎模型由於 PPG 在生物訊號監控中的廣泛使用及其在各種健康應用中推廣的潛力而備受關注。在本文中，我們介紹 Pulse-PPG，這是第一個開放原始碼 PPG 基礎模型，專門針對在為期 100 天的現場研究中收集的 120 位參與者的原始 PPG 資料進行訓練。現有的 PPG 基礎模型要不是開放原始碼，但訓練於臨床資料，不然就是閉源，這限制了它們在真實世界中的應用性。我們評估了 Pulse-PPG 在多個資料集和下游任務中的表現，並將其效能與訓練於臨床資料的最新基礎模型進行比較。我們的結果表明，訓練於未整理現場資料的 Pulse-PPG 在實驗室和現場環境中，在臨床和行動健康應用中展現出優異的泛化能力。這表明接觸真實世界的變異性使模型能夠學習細粒度的表示，使其更能適應各種任務。此外，令人驚訝的是，現場資料的預訓練在許多任務中優於臨床資料的預訓練，這強化了在真實世界、多樣化的資料集上訓練的重要性。為了鼓勵在利用現場資料的強健基礎模型方面進一步發展，我們計畫發布 Pulse-PPG，為研究人員提供一個強大的資源，用於開發更具泛化性的基於 PPG 的模型。

##### **Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**
2502.01691v1 by Hadas Ben-Atya, Naama Gavrielov, Zvi Badash, Gili Focht, Ruth Cytter-Kuint, Talar Hagopian, Dan Turner, Moti Freiman

Reliable extraction of structured data from radiology reports using Large
Language Models (LLMs) remains challenging, especially for complex, non-English
texts like Hebrew. This study introduces an agent-based uncertainty-aware
approach to improve the trustworthiness of LLM predictions in medical
applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease
patients (from 2010 to 2023) across three medical centers. A subset of 512
reports was manually annotated for six gastrointestinal organs and 15
pathological findings, while the remaining reports were automatically annotated
using HSMP-BERT. Structured data extraction was performed using Llama 3.1
(Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed
six semantically equivalent prompts to estimate uncertainty. An Agent-Based
Decision Model integrated multiple prompt outputs into five confidence levels
for calibrated uncertainty and was compared against three entropy-based models.
Performance was evaluated using accuracy, F1 score, precision, recall, and
Cohen's Kappa before and after filtering high-uncertainty cases. The
agent-based model outperformed the baseline across all metrics, achieving an F1
score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering
high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to
0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated
clear separation between correct and incorrect predictions, with the
agent-based model providing the most well-calibrated uncertainty estimates. By
incorporating uncertainty-aware prompt ensembles and an agent-based decision
model, this approach enhances the performance and reliability of LLMs in
structured data extraction from radiology reports, offering a more
interpretable and trustworthy solution for high-stakes medical applications.

摘要：<paragraph>使用大型語言模型 (LLM) 從放射科報告中可靠地提取結構化數據仍然具有挑戰性，尤其是對於希伯來語等複雜的非英語文本。本研究引入了一種基於代理的不確定性感知方法，以提高 LLM 預測在醫療應用中的可信度。我們分析了來自三個醫療中心的 9,683 份克隆氏症患者的希伯來語放射科報告（從 2010 年到 2023 年）。其中 512 份報告的手動註釋包括六個胃腸器官和 15 個病理發現，而其餘報告則使用 HSMP-BERT 自動註釋。結構化數據提取使用 Llama 3.1（Llama 3-8b-instruct）與貝葉斯提示集合（BayesPE）進行，它採用六個語義等效提示來估計不確定性。基於代理的決策模型將多個提示輸出整合到五個置信度級別中以校準不確定性，並與三個基於熵的模型進行比較。在過濾掉高度不確定性的情況之前和之後，使用準確度、F1 分數、精確度、召回率和 Cohen's Kappa 評估性能。基於代理的模型在所有指標上都優於基線，F1 分數達到 0.3967，召回率達到 0.6437，Cohen's Kappa 達到 0.3006。在過濾掉高度不確定性的情況（大於或等於 0.5）後，F1 分數提高到 0.4787，Kappa 提高到 0.4258。不確定性直方圖顯示了正確預測和不正確預測之間的明顯區別，基於代理的模型提供了校準最好的不確定性估計。通過結合不確定性感知提示集合和基於代理的決策模型，這種方法增強了 LLM 在放射科報告中結構化數據提取中的性能和可靠性，為高風險醫療應用提供了更具可解釋性和可信度的解決方案。</paragraph>

##### **Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**
2502.01685v1 by Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha

Existing methods for analyzing linguistic content from picture descriptions
for assessment of cognitive-linguistic impairment often overlook the
participant's visual narrative path, which typically requires eye tracking to
assess. Spatio-semantic graphs are a useful tool for analyzing this narrative
path from transcripts alone, however they are limited by the need for manual
tagging of content information units (CIUs). In this paper, we propose an
automated approach for estimation of spatio-semantic graphs (via automated
extraction of CIUs) from the Cookie Theft picture commonly used in
cognitive-linguistic analyses. The method enables the automatic
characterization of the visual semantic path during picture description.
Experiments demonstrate that the automatic spatio-semantic graphs effectively
differentiate between cognitively impaired and unimpaired speakers. Statistical
analyses reveal that the features derived by the automated method produce
comparable results to the manual method, with even greater group differences
between clinical groups of interest. These results highlight the potential of
the automated approach for extracting spatio-semantic features in developing
clinical speech models for cognitive impairment assessment.

摘要：現有的用於分析圖像描述中的語言內容的方法，用於評估認知語言障礙，通常會忽略參與者的視覺敘事路徑，這通常需要眼球追蹤來評估。時空語義圖是一種有用的工具，可以僅從轉錄本中分析此敘事路徑，但是它們受到手動標記內容資訊單元 (CIU) 的需求所限制。在本文中，我們提出了一種自動化方法，用於從認知語言分析中常用的 Cookie Theft 圖像估計時空語義圖（通過自動提取 CIU）。該方法能夠自動表徵圖片描述期間的視覺語義路徑。實驗表明，自動時空語義圖有效地區分了認知受損和未受損的說話者。統計分析表明，自動化方法衍生的特徵產生了與手動方法相當的結果，甚至在感興趣的臨床組之間產生了更大的組差異。這些結果突出了自動化方法在提取時空語義特徵以開發用於認知障礙評估的臨床語音模型方面的潛力。

##### **Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**
2502.00712v1 by Shengtian Sang, Hassan Jahanandish, Cynthia Xinran Li, Indrani Bhattachary, Jeong Hoon Lee, Lichun Zhang, Sulaiman Vesal, Pejman Ghanouni, Richard Fan, Geoffrey A. Sonn, Mirabela Rusu

Prostate cancer is a major cause of cancer-related deaths in men, where early
detection greatly improves survival rates. Although MRI-TRUS fusion biopsy
offers superior accuracy by combining MRI's detailed visualization with TRUS's
real-time guidance, it is a complex and time-intensive procedure that relies
heavily on manual annotations, leading to potential errors. To address these
challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation
method that identifies prostate tumors directly in TRUS images without
requiring manual annotations. Unlike traditional multimodal fusion approaches
that rely on naive data concatenation, our method integrates a
registration-segmentation framework to align and leverage spatial information
between MRI and TRUS modalities. This alignment enhances segmentation accuracy
and reduces reliance on manual effort. Our approach was validated on a dataset
of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient
of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132)
methods, with significant improvements (p $<$ 0.01). This framework
demonstrates the potential for reducing the complexity of prostate cancer
diagnosis and provides a flexible architecture applicable to other multimodal
medical imaging tasks.

摘要：前列腺癌是男性癌症相關死亡的主要原因，早期發現可大幅提升存活率。儘管 MRI-TRUS 融合切片檢查結合了 MRI 的詳細視覺化與 TRUS 的即時導引，可提供更高的準確度，但它是一種仰賴大量手動註解的複雜且耗時的程序，容易導致錯誤。為了解決這些挑戰，我們提出了一種全自動的 MRI-TRUS 融合式分割方法，它可以在 TRUS 影像中直接辨識出前列腺腫瘤，而不需要手動註解。與依賴於天真資料串接的傳統多模態融合方法不同，我們的方法整合了一個配準分割架構，以對齊並利用 MRI 與 TRUS 模態之間的空間資訊。這種對齊提升了分割準確度，並減少了對手動作業的依賴。我們的方法已通過來自 Stanford 醫院的 1,747 位患者的資料集進行驗證，達到了 0.212 的平均 Dice 係數，優於僅使用 TRUS (0.117) 和天真的 MRI-TRUS 融合 (0.132) 方法，並有顯著的改善（p < 0.01）。這個架構證明了降低前列腺癌診斷複雜性的潛力，並提供了一個適用於其他多模態醫學影像任務的彈性架構。

##### **TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**
2502.00695v1 by Linglong Wu, Xuhao Shan, Ruiquan Ge, Ruoyu Liang, Chi Zhang, Yonghong Li, Ahmed Elazab, Huoling Luo, Yunbi Liu, Changmiao Wang

Chronic liver disease represents a significant health challenge worldwide and
accurate prognostic evaluations are essential for personalized treatment plans.
Recent evidence suggests that integrating multimodal data, such as computed
tomography imaging, radiomic features, and clinical information, can provide
more comprehensive prognostic information. However, modalities have an inherent
heterogeneity, and incorporating additional modalities may exacerbate the
challenges of heterogeneous data fusion. Moreover, existing multimodal fusion
methods often struggle to adapt to richer medical modalities, making it
difficult to capture inter-modal relationships. To overcome these limitations,
We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).
Specifically, we develop an Intra-Modality Aggregation module and a
Triple-Modal Cross-Attention Fusion module, which are designed to eliminate
intra-modality redundancy and extract cross-modal information, respectively.
Furthermore, we design a Triple-Modal Feature Fusion loss function to align
feature representations across modalities. Extensive experiments on the liver
prognosis dataset demonstrate that our approach significantly outperforms
existing state-of-the-art unimodal models and other multi-modal techniques. Our
code is available at https://github.com/Mysterwll/liver.git.

摘要：慢性肝病在全球范围内代表著重大的健康挑戰，而準確的預後評估對於個人化治療計畫至關重要。最近的證據表明，整合多模態資料（例如電腦斷層影像、放射特徵和臨床資訊）可以提供更全面的預後資訊。然而，模態具有內在異質性，而納入額外的模態可能會加劇異質化資料融合的挑戰。此外，現有的多模態融合方法通常難以適應更豐富的醫療模態，這使得難以捕捉模態間的關係。為了克服這些限制，我們提出了三模態交互慢性肝臟網路 (TMI-CLNet)。具體來說，我們開發了一個模態內聚合模組和一個三模態交叉注意力融合模組，它們分別旨在消除模態內冗餘和提取跨模態資訊。此外，我們設計了一個三模態特徵融合損失函數，以對齊跨模態的特徵表示。在肝臟預後資料集上的廣泛實驗表明，我們的做法顯著優於現有的最先進單模態模型和其他多模態技術。我們的程式碼可以在 https://github.com/Mysterwll/liver.git 上取得。

##### **Enhanced Convolutional Neural Networks for Improved Image Classification**
2502.00663v1 by Xiaoran Yang, Shuhan Yu, Wenxi Xu

Image classification is a fundamental task in computer vision with diverse
applications, ranging from autonomous systems to medical imaging. The CIFAR-10
dataset is a widely used benchmark to evaluate the performance of
classification models on small-scale, multi-class datasets. Convolutional
Neural Networks (CNNs) have demonstrated state-of-the-art results; however,
they often suffer from overfitting and suboptimal feature representation when
applied to challenging datasets like CIFAR-10. In this paper, we propose an
enhanced CNN architecture that integrates deeper convolutional blocks, batch
normalization, and dropout regularization to achieve superior performance. The
proposed model achieves a test accuracy of 84.95%, outperforming baseline CNN
architectures. Through detailed ablation studies, we demonstrate the
effectiveness of the enhancements and analyze the hierarchical feature
representations. This work highlights the potential of refined CNN
architectures for tackling small-scale image classification problems
effectively.

摘要：影像分類是電腦視覺中的一項基本任務，應用範圍廣泛，從自動系統到醫學影像皆有。CIFAR-10 資料集是一個廣泛使用的基準，用於評估分類模型在小規模、多類別資料集上的效能。卷積神經網路 (CNN) 已展現出最先進的成果；然而，當應用於 CIFAR-10 等具挑戰性的資料集時，它們常常會發生過度擬合和次佳特徵表示的問題。在本文中，我們提出一個增強的 CNN 架構，它整合了更深的卷積區塊、批次正規化和中斷正規化，以達成卓越的效能。所提出的模型達到了 84.95% 的測試準確度，優於基準 CNN 架構。透過詳細的消融研究，我們證明了這些增強功能的有效性，並分析了階層式特徵表示。這項工作突顯了精進的 CNN 架構在有效解決小規模影像分類問題上的潛力。

##### **Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**
2502.00619v1 by Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon, Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li

Ensuring fairness in medical image segmentation is critical due to biases in
imbalanced clinical data acquisition caused by demographic attributes (e.g.,
age, sex, race) and clinical factors (e.g., disease severity). To address these
challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired
by optimal control theory. We provide a comprehensive analysis of its
underlying mechanisms and clarify dMoE's role in adapting to heterogeneous
distributions in medical image segmentation. Furthermore, we integrate dMoE
into multiple network architectures, demonstrating its broad applicability
across diverse medical image analysis tasks. By incorporating demographic and
clinical factors, dMoE achieves state-of-the-art performance on two 2D
benchmark datasets and a 3D in-house dataset. Our results highlight the
effectiveness of dMoE in mitigating biases from imbalanced distributions,
offering a promising approach to bridging control theory and medical image
segmentation within fairness learning paradigms. The source code will be made
available.

摘要：在医学影像分割中，由於人口屬性（例如年齡、性別、種族）和臨床因素（例如疾病嚴重程度）導致不平衡的臨床數據採集中存在偏差，因此確保公平性至關重要。為了應對這些挑戰，我們引入了受最優控制理論啟發的感知混合專家 (dMoE)。我們對其底層機制進行了全面分析，並釐清了 dMoE 在適應醫學影像分割中的異質分佈中的作用。此外，我們將 dMoE 整合到多個網路架構中，展示了其在各種醫學影像分析任務中的廣泛適用性。通過納入人口統計和臨床因素，dMoE 在兩個 2D 基準數據集和一個 3D 內部數據集上實現了最先進的性能。我們的結果突出了 dMoE 在減輕不平衡分佈的偏差方面的有效性，為在公平性學習範例中橋接控制理論和醫學影像分割提供了一個有前景的方法。原始碼將會公開。

##### **Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**
2502.00568v1 by Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti

Emerging research has highlighted that artificial intelligence based
multimodal fusion of digital pathology and transcriptomic features can improve
cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.
However, such direct fusion for joint decision is impractical in real clinical
settings, where histopathology is still the gold standard for diagnosis and
transcriptomic tests are rarely requested, at least in the public healthcare
system. With our novel diffusion based crossmodal generative AI model PathoGen,
we show that genomic expressions synthesized from digital histopathology
jointly predicts cancer grading and patient survival risk with high accuracy
(state-of-the-art performance), certainty (through conformal coverage
guarantee) and interpretability (through distributed attention maps). PathoGen
code is available for open use by the research community through GitHub at
https://github.com/Samiran-Dey/PathoGen.

摘要：新興研究強調，基於人工智慧的多模態融合數位病理學和轉錄組特徵，可以改善癌症診斷（分級/分型）和預後（存活風險）預測。
然而，這種直接融合對於聯合決策在實際臨床環境中是不切實際的，因為在實際臨床環境中，組織病理學仍然是診斷的黃金標準，而轉錄組檢測很少被要求，至少在公共醫療系統中是如此。透過我們新穎的基於擴散的跨模態生成式 AI 模型 PathoGen，我們展示了從數位組織病理學合成的基因體表達，可以共同預測癌症分級和患者存活風險，具有高準確度（最先進的效能）、確定性（透過保形覆蓋保證）和可解釋性（透過分佈式注意力圖）。PathoGen 程式碼可透過 GitHub 上的 https://github.com/Samiran-Dey/PathoGen，開放供研究社群使用。

##### **Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**
2502.00495v1 by Mohammad Saleh Torkestani, Robert Davis, Abdolhossein Sarrafzadeh

Time constraints on doctor patient interaction and restricted access to
specialists under the managed care system led to increasingly referring to
computers as a medical information source and a self-health-care management
tool. However, research show that less than 40% of information seekers
indicated that online information helped them to make a decision about their
health. Searching multiple web sites that need basic computer skills, lack of
interaction and no face to face interaction in most search engines and some
social issues, led us to develop a specialized life-like agent that would
overcome mentioned problems.

摘要：由於管理式醫療保健系統中醫師與病患互動時間有限，且專科醫師的取得受限，因此越來越多人將電腦視為醫療資訊來源和自我保健管理工具。然而，研究顯示，不到 40% 的資訊尋求者表示，線上資訊有助於他們做出健康決策。搜尋需要基本電腦技能的許多網站、缺乏互動，以及大多數搜尋引擎和一些社交議題中沒有面對面的互動，這些原因促使我們開發出一個特殊且逼真的代理人，以克服上述問題。

##### **Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**
2502.00451v1 by Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych

Mental illness is a widespread and debilitating condition with substantial
societal and personal costs. Traditional diagnostic and treatment approaches,
such as self-reported questionnaires and psychotherapy sessions, often impose
significant burdens on both patients and clinicians, limiting accessibility and
efficiency. Recent advances in Artificial Intelligence (AI), particularly in
Natural Language Processing and multimodal techniques, hold great potential for
recognizing and addressing conditions such as depression, anxiety, bipolar
disorder, schizophrenia, and post-traumatic stress disorder. However, privacy
concerns, including the risk of sensitive data leakage from datasets and
trained models, remain a critical barrier to deploying these AI systems in
real-world clinical settings. These challenges are amplified in multimodal
methods, where personal identifiers such as voice and facial data can be
misused. This paper presents a critical and comprehensive study of the privacy
challenges associated with developing and deploying AI models for mental
health. We further prescribe potential solutions, including data anonymization,
synthetic data generation, and privacy-preserving model training, to strengthen
privacy safeguards in practical applications. Additionally, we discuss
evaluation frameworks to assess the privacy-utility trade-offs in these
approaches. By addressing these challenges, our work aims to advance the
development of reliable, privacy-aware AI tools to support clinical
decision-making and improve mental health outcomes.

摘要：精神疾病是一種廣泛且會使人衰弱的疾病，會造成重大的社會和個人成本。傳統的診斷與治療方法，例如自我報告問卷和心理治療療程，通常會對患者和臨床醫生造成重大負擔，限制了可及性和效率。人工智慧 (AI) 的最新進展，特別是在自然語言處理和多模式技術方面，在辨識和處理憂鬱症、焦慮症、躁鬱症、精神分裂症和創傷後壓力症候群等疾病方面具有極大的潛力。然而，隱私問題，包括資料集和訓練模型中敏感資料外洩的風險，仍然是這些 AI 系統在真實臨床環境中部署的一項關鍵障礙。這些挑戰在多模式方法中會被放大，因為語音和面部資料等個人識別資料可能會被濫用。本文對與開發和部署用於心理健康的 AI 模型相關的隱私挑戰進行了一項批判且全面的研究。我們進一步提出了潛在的解決方案，包括資料匿名化、合成資料產生和隱私保護模型訓練，以加強實際應用中的隱私保障。此外，我們討論了評估這些方法中隱私與效用取捨的評估架構。透過解決這些挑戰，我們的研究旨在推進可靠、重視隱私的 AI 工具的開發，以支持臨床決策制定並改善心理健康成果。

##### **EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**
2502.00205v1 by Omar H. Khater, Abdul Jabbar Siddiqui, M. Shamim Hossain

Sustainable agriculture plays a crucial role in ensuring world food security
for consumers. A critical challenge faced by sustainable precision agriculture
is weed growth, as weeds share essential resources with the crops, such as
water, soil nutrients, and sunlight, which notably affect crop yields. The
traditional methods employed to combat weeds include the usage of chemical
herbicides and manual weed removal methods. However, these could damage the
environment and pose health hazards. The adoption of automated computer vision
technologies and ground agricultural consumer electronic vehicles in precision
agriculture offers sustainable, low-carbon solutions. However, prior works
suffer from issues such as low accuracy and precision and high computational
expense. This work proposes EcoWeedNet, a novel model with enhanced weed
detection performance without adding significant computational complexity,
aligning with the goals of low-carbon agricultural practices. Additionally, our
model is lightweight and optimal for deployment on ground-based consumer
electronic agricultural vehicles and robots. The effectiveness of the proposed
model is demonstrated through comprehensive experiments on the CottonWeedDet12
benchmark dataset reflecting real-world scenarios. EcoWeedNet achieves
performance close to that of large models yet with much fewer parameters.
(approximately 4.21% of the parameters and 6.59% of the GFLOPs of YOLOv4). This
work contributes effectively to the development of automated weed detection
methods for next-generation agricultural consumer electronics featuring lower
energy consumption and lower carbon footprint. This work paves the way forward
for sustainable agricultural consumer technologies.

摘要：永續農業在確保世界糧食安全方面扮演著至關重要的角色
對於永續精準農業來說，雜草的生長是一個重大的挑戰，因為雜草與農作物共享水、土壤養分和陽光等基本資源，這會顯著影響農作物的產量。傳統上用於對抗雜草的方法包括使用化學除草劑和人工除草方法。然而，這些方法可能會損害環境並造成健康危害。在精準農業中採用自動化電腦視覺技術和地面農業用消費電子車輛提供了永續的低碳解決方案。然而，先前的研究存在準確度和精確度低以及計算成本高等問題。這項研究提出了 EcoWeedNet，這是一個新的模型，具有增強的雜草偵測效能，而不會增加顯著的計算複雜度，符合低碳農業實務的目標。此外，我們的模型輕巧，最適於部署在地面消費電子農業車輛和機器人上。所提出的模型的有效性已通過在反映真實世界場景的 CottonWeedDet12 基準資料集上進行的全面實驗得到證明。EcoWeedNet 的效能接近大型模型，但參數卻少得多。(大約是 YOLOv4 參數的 4.21% 和 GFLOP 的 6.59%)。這項研究有效地促進了下一代農業消費電子產品的自動化雜草偵測方法的開發，這些產品的特點是能耗更低、碳足跡更低。這項研究為永續農業消費技術鋪平了道路。

##### **DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**
2502.00196v1 by Abdurrahim Yilmaz, Furkan Yuceyalcin, Ece Gokyayla, Donghee Choi, Ozan Erdem Ali Anil Demircali, Rahmetullah Varol, Ufuk Gorkem Kirabali, Gulsum Gencoglan, Joram M. Posma, Burak Temelkuran

A major barrier to developing vision large language models (LLMs) in
dermatology is the lack of large image--text pairs dataset. We introduce
DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated
from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for
dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using
Gemini 2.0, we used clinically related prompts and self-instruct method to
generate diverse and rich synthetic texts. Metadata of the datasets were
incorporated into the input prompts by targeting to reduce potential
hallucinations. The resulting dataset builds upon open access dermatological
image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have
permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary
Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We
anticipate this dataset to support and accelerate AI research in dermatology.
Data and code underlying this work are accessible at
https://github.com/abdurrahimyilmaz/DermaSynth.

摘要：大型語言模型（LLM）在皮膚科發展的一大障礙是缺乏大量的影像文字對應資料集。我們引進 DermaSynth，這是一個由 92,020 個合成影像文字對應資料組成的資料集，這些資料對應資料是從 45,205 個影像（13,568 個臨床影像和 35,561 個皮膚鏡影像）中策劃而來的，用於皮膚科相關的臨床任務。利用最先進的 LLM，使用 Gemini 2.0，我們使用與臨床相關的提示和自我指導方法來產生多樣且豐富的合成文字。資料集的元資料會納入輸入提示中，目標是減少潛在的幻覺。產生的資料集建立在開放取用的皮膚科影像儲存庫（DERM12345、BCN20000、PAD-UFES-20、SCIN 和 HIBA）之上，這些儲存庫擁有寬鬆的 CC-BY-4.0 licenses。我們還對一個初步的 Llama-3.2-11B-Vision-Instruct 模型，DermatoLlama 1.0，在 5,000 個樣本上進行微調。我們預計這個資料集將支援和加速皮膚科的 AI 研究。這項工作的資料和程式碼可在 https://github.com/abdurrahimyilmaz/DermaSynth 取得。

##### **Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**
2502.00146v1 by Hassan Jahanandish, Shengtian Sang, Cynthia Xinran Li, Sulaiman Vesal, Indrani Bhattacharya, Jeong Hoon Lee, Richard Fan, Geoffrey A. Sonna, Mirabela Rusu

Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target
suspicious prostate lesions. This has led to artificial intelligence (AI)
applications improving MRI-based detection of clinically significant prostate
cancer (CsPCa). However, MRI-detected lesions must still be mapped to
transrectal ultrasound (TRUS) images during biopsy, which results in missing
CsPCa. This study systematically evaluates a multimodal AI framework
integrating MRI and TRUS image sequences to enhance CsPCa identification. The
study included 3110 patients from three cohorts across two institutions who
underwent prostate biopsy. The proposed framework, based on the 3D UNet
architecture, was evaluated on 1700 test cases, comparing performance to
unimodal AI models that use either MRI or TRUS alone. Additionally, the
proposed model was compared to radiologists in a cohort of 110 patients. The
multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice
(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared
to radiologists, the multimodal model showed higher specificity (88% vs. 78%)
and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings
demonstrate the potential of multimodal AI to improve CsPCa lesion targeting
during biopsy and treatment planning, surpassing current unimodal models and
radiologists; ultimately improving outcomes for prostate cancer patients.

摘要：<paragraph>在活檢前，磁振造影（MRI）正越來越常被用於鎖定可疑的攝護腺病灶。這導致人工智慧（AI）應用程式改善了以 MRI 為基礎的臨床顯著攝護腺癌（CsPCa）檢測。然而，在活檢期間，由 MRI 偵測到的病灶仍必須對應到經直腸超音波（TRUS）影像，這導致錯失 CsPCa。本研究系統性地評估了一個多模態 AI 架構，整合 MRI 和 TRUS 影像序列，以增強 CsPCa 識別。這項研究納入了來自兩家機構的三個群組中的 3110 名患者，他們接受了攝護腺活檢。所提出的架構基於 3D UNet 架構，在 1700 個測試案例中進行評估，並將其效能與僅使用 MRI 或 TRUS 的單模態 AI 模型進行比較。此外，所提出的模型與一個由 110 名患者組成的群組中的放射科醫師進行比較。與單模態 MRI（73%、30%）和 TRUS 模型（49%、27%）相比，多模態 AI 方法達到了更高的敏感度（80%）和病灶 Dice（42%）。與放射科醫師相比，多模態模型顯示出更高的特異性（88% 對 78%）和病灶 Dice（38% 對 33%），且敏感度相當（79%）。我們的研究結果證明了多模態 AI 在活檢和治療計畫期間改善 CsPCa 病灶鎖定的潛力，超越了目前的單模態模型和放射科醫師；最終改善了攝護腺癌患者的治療成果。</paragraph>

##### **AIN: The Arabic INclusive Large Multimodal Model**
2502.00094v2 by Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan

Amid the swift progress of large language models (LLMs) and their evolution
into large multimodal models (LMMs), significant strides have been made in
high-resource languages such as English and Chinese. While Arabic LLMs have
seen notable progress, Arabic LMMs remain largely unexplored, often narrowly
focusing on a few specific aspects of the language and visual understanding. To
bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal
Model-designed to excel across diverse domains. AIN is an English-Arabic
bilingual LMM designed to excel in English and Arabic, leveraging carefully
constructed 3.6 million high-quality Arabic-English multimodal data samples.
AIN demonstrates state-of-the-art Arabic performance, while also possessing
strong English-language visual capabilities. On the recent CAMEL-Bench
benchmark comprising 38 sub-domains including, multi-image understanding,
complex visual perception, handwritten document understanding, video
understanding, medical imaging, plant diseases, and remote sensing-based land
use understanding, our AIN demonstrates strong performance with the 7B model
outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains
and 38 sub-domains. AIN's superior capabilities position it as a significant
step toward empowering Arabic speakers with advanced multimodal generative AI
tools across diverse applications.

摘要：在大型語言模型 (LLM) 快速發展，並演變成大型多模態模型 (LMM) 的過程中，英語和中文等高資源語言已取得重大進展。雖然阿拉伯語 LLM 已取得顯著進展，但阿拉伯語 LMM 仍未被廣泛探索，通常只狹隘地關注語言和視覺理解的幾個特定方面。為了彌合這項差距，我們推出了 AIN，即阿拉伯語包容性多模態模型，旨在在不同領域中表現出色。AIN 是一個英語-阿拉伯語雙語 LMM，旨在精通英語和阿拉伯語，利用精心建構的 360 萬個高品質阿拉伯語-英語多模態數據樣本。AIN 展示了最先進的阿拉伯語效能，同時也具備強大的英語視覺能力。在最近的 CAMEL-Bench 基準測試中，包含 38 個子領域，包括多影像理解、複雜視覺感知、手寫文件理解、影片理解、醫學影像、植物疾病和基於遙測的土地使用理解，我們的 AIN 表現出色，其中 7B 模型在八個領域和 38 個子領域的平均絕對增益為 3.4%，優於 GPT-4o。AIN 的卓越能力使其成為朝著賦予阿拉伯語使用者進階多模態生成式 AI 工具邁出的重要一步，可用於各種應用。

##### **Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**
2501.19338v1 by Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente István Lánczi, Andras Jakab

Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.

摘要：<paragraph>開發用於自動分析臨床胎兒和新生兒 MRI 資料的新方法受到標註病理資料集稀少和隱私問題的限制，這些問題通常會限制資料共享，從而阻礙深度學習模型的有效性。我們以兩種方式解決這個問題。首先，我們引入了 Fetal&Neonatal-DDPM，這是一個新穎的擴散模型架構，旨在從語義標籤影像生成高品質的合成病理胎兒和新生兒 MRI。其次，我們透過形態改變來修改健康的標籤影像，以模擬腦室擴大、小腦和橋腦小腦發育不全以及小頭畸形等情況，從而增強訓練資料。透過利用 Fetal&Neonatal-DDPM，我們從這些修改後的病理標籤影像中合成了逼真的病理 MRI。放射科醫師評估合成 MRI 的品質和診斷價值顯著優於真實 MRI（p < 0.05），展示了血管和脈絡叢等特徵，並改善了與標籤註解的一致性。合成病理資料增強了最先進的 nnUNet 分割效能，特別是對於嚴重的腦室擴大病例，其中腦室分割（Dice 分數：0.9253 對 0.7317）的改善最大。這項研究強調了生成式 AI 作為資料擴充轉型工具的潛力，在病理病例中提供了改善的分割效能。這項發展代表了改善產前影像分析和分割準確性的重要一步，也為透過生成病理影像資料來進行資料匿名化提供了新方法。</paragraph>

##### **Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**
2501.19271v1 by Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett

Concept-based explanation methods, such as concept bottleneck models (CBMs),
aim to improve the interpretability of machine learning models by linking their
decisions to human-understandable concepts, under the critical assumption that
such concepts can be accurately attributed to the network's feature space.
However, this foundational assumption has not been rigorously validated, mainly
because the field lacks standardised metrics and benchmarks to assess the
existence and spatial alignment of such concepts. To address this, we propose
three metrics: the concept global importance metric, the concept existence
metric, and the concept location metric, including a technique for visualising
concept activations, i.e., concept activation mapping. We benchmark post-hoc
CBMs to illustrate their capabilities and challenges. Through qualitative and
quantitative experiments, we demonstrate that, in many cases, even the most
important concepts determined by post-hoc CBMs are not present in input images;
moreover, when they are present, their saliency maps fail to align with the
expected regions by either activating across an entire object or misidentifying
relevant concept-specific regions. We analyse the root causes of these
limitations, such as the natural correlation of concepts. Our findings
underscore the need for more careful application of concept-based explanation
techniques especially in settings where spatial interpretability is critical.

摘要：基於概念的解釋方法，例如概念瓶頸模型 (CBM)，旨在透過將機器學習模型的決策與人類可理解的概念連結，來提升機器學習模型的可解釋性，其關鍵假設為此類概念可以準確地歸因於網路的特徵空間。然而，此項基礎假設尚未經過嚴格驗證，主要是因為該領域缺乏標準化指標和基準來評估此類概念的存在和空間對齊。為了解決這個問題，我們提出三項指標：概念整體重要性指標、概念存在指標和概念位置指標，包括一種用於視覺化概念活化，即概念活化對應的技術。我們對事後 CBM 進行基準測試，以說明它們的能力和挑戰。透過定性和定量實驗，我們證明，在許多情況下，即使是由事後 CBM 確定的最重要概念也不存在於輸入影像中；此外，當它們存在時，它們的顯著性圖無法與預期的區域對齊，原因可能是它們在整個物件中活化，或錯誤辨識出相關的概念特定區域。我們分析了這些限制的根本原因，例如概念的自然相關性。我們的研究結果強調需要更小心地應用基於概念的解釋技術，特別是在空間可解釋性至關重要的設定中。

##### **Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**
2501.19176v1 by Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi

Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.

摘要：全視野數位乳房攝影 (FFDM) 是常規乳癌篩檢的主要影像模式；然而，對於乳房組織緻密或纖維囊腫病變的患者，其有效性受到限制。對比增強光譜乳房攝影 (CESM) 是一種二級影像技術，可提升腫瘤偵測的準確度。儘管如此，由於較高的輻射曝露、對比劑的使用和有限的可及性，限制了其應用。因此，CESM 通常僅保留在特定情況下使用，儘管 CESM 的診斷效能較佳，但許多患者仍只能依賴 FFDM。雖然切片檢查仍然是明確診斷的黃金標準，但這是一種侵入性程序，可能會讓患者感到不適。我們引入一種多模式、多視圖的深度學習方法進行虛擬切片檢查，將 FFDM 和 CESM 模式整合在頭尾向和內外側斜視圖中，以將病灶分類為惡性或良性。為了解決 CESM 資料缺失的挑戰，我們利用生成式人工智慧從 FFDM 掃描中推算 CESM 影像。實驗結果證明，整合 CESM 模式對於提升虛擬切片檢查的效能至關重要。當真實 CESM 資料缺失時，合成 CESM 影像被證明是有效的，其效能優於單獨使用 FFDM，特別是在結合 FFDM 和 CESM 模式的多模式配置中。所提出的方法有潛力改善診斷工作流程，為臨床醫師提供增強的智慧工具，以提高診斷準確度和患者照護。此外，作為對研究社群的貢獻，我們公開發布在實驗中使用的資料集，以促進此領域的進一步進展。

##### **Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**
2501.19086v1 by Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang

X-ray imaging is pivotal in medical diagnostics, offering non-invasive
insights into a range of health conditions. Recently, vision-language models,
such as the Contrastive Language-Image Pretraining (CLIP) model, have
demonstrated potential in improving diagnostic accuracy by leveraging
large-scale image-text datasets. However, since CLIP was not initially designed
for medical images, several CLIP-like models trained specifically on medical
images have been developed. Despite their enhanced performance, issues of
fairness - particularly regarding demographic attributes - remain largely
unaddressed. In this study, we perform a comprehensive fairness analysis of
CLIP-like models applied to X-ray image classification. We assess their
performance and fairness across diverse patient demographics and disease
categories using zero-shot inference and various fine-tuning techniques,
including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation
(LoRA), and full fine-tuning. Our results indicate that while fine-tuning
improves model accuracy, fairness concerns persist, highlighting the need for
further fairness interventions in these foundational models.

摘要：X 光影像在醫療診斷中至關重要，能提供各種健康狀況的非侵入性見解。最近，視覺語言模型（例如對比語言影像預訓練 (CLIP) 模型）已證明有潛力透過利用大規模影像文字資料集來改善診斷準確性。然而，由於 CLIP 最初並非設計用於醫療影像，因此已經開發了數個特別針對醫療影像訓練的類似 CLIP 模型。儘管它們的效能有所提升，但公平性的問題（特別是關於人口統計屬性）仍大多未獲解決。在本研究中，我們對應用於 X 光影像分類的類似 CLIP 模型執行全面的公平性分析。我們使用零次學習推論和各種微調技術（包括線性探查、多層感知器 (MLP)、低秩適應 (LoRA) 和完整微調）來評估它們在不同患者人口統計和疾病類別中的效能和公平性。我們的結果表明，雖然微調會改善模型準確性，但公平性問題仍然存在，強調需要在這些基礎模型中進一步採取公平性干預措施。

##### **Survey and Improvement Strategies for Gene Prioritization with Large Language Models**
2501.18794v1 by Matthew Neeley, Guantong Qi, Guanchu Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, Fan Xia, Pengfei Liu, Zhandong Liu, Xia Hu

Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.

摘要：罕見疾病由於患者數據有限和遺傳多樣性，診斷起來具有挑戰性。儘管變異優先級排序技術進步，但許多病例仍未得到診斷。儘管大型語言模型 (LLM) 在醫學考試中表現良好，但它們在診斷罕見遺傳疾病方面的有效性尚未得到評估。為了識別致病基因，我們對各種 LLM 進行了基因優先級排序基準測試。使用多智能體和人類表型本体 (HPO) 分類，我們根據表型和可解決性對患者進行了分類。隨著基因組大小的增加，LLM 性能下降，因此我們使用分而治之策略將任務分解為更小的子集。在基線中，GPT-4 優於其他 LLM，在正確排序致病基因方面達到近 30% 的準確度。多智能體和 HPO 方法有助於區分解決有信心的病例和具有挑戰性的病例，強調已知基因-表型關聯和表型特異性的重要性。我們發現具有特定表型或明確關聯的病例得到更準確的解決。然而，我們觀察到對研究充分的基因和輸入順序敏感性的偏差，這阻礙了基因優先級排序。我們的分而治之策略通過克服這些偏差來提高準確性。通過利用 HPO 分類、新穎的多智能體技術和我們的 LLM 策略，我們與我們的基線評估相比提高了致病基因識別準確性。這種方法簡化了罕見疾病的診斷，促進了對未解決病例的重新分析，並加速了基因發現，支持了靶向診斷和治療的開發。

##### **Synthetic Data Generation for Augmenting Small Samples**
2501.18741v1 by Dan Liu, Samer El Kababji, Nicholas Mitsakakis, Lisa Pilgram, Thomas Walters, Mark Clemons, Greg Pond, Alaa El-Hussuna, Khaled El Emam

Small datasets are common in health research. However, the generalization
performance of machine learning models is suboptimal when the training datasets
are small. To address this, data augmentation is one solution. Augmentation
increases sample size and is seen as a form of regularization that increases
the diversity of small datasets, leading them to perform better on unseen data.
We found that augmentation improves prognostic performance for datasets that:
have fewer observations, with smaller baseline AUC, have higher cardinality
categorical variables, and have more balanced outcome variables. No specific
generative model consistently outperformed the others. We developed a decision
support model that can be used to inform analysts if augmentation would be
useful. For seven small application datasets, augmenting the existing data
results in an increase in AUC between 4.31% (AUC from 0.71 to 0.75) and 43.23%
(AUC from 0.51 to 0.73), with an average 15.55% relative improvement,
demonstrating the nontrivial impact of augmentation on small datasets
(p=0.0078). Augmentation AUC was higher than resampling only AUC (p=0.016). The
diversity of augmented datasets was higher than the diversity of resampled
datasets (p=0.046).

摘要：在健康研究中，小型数据集很常见。然而，当训练数据集较小时，机器学习模型的泛化性能并不理想。为了解决这个问题，数据增强是一种解决方案。增强增加了样本量，并被视为一种正则化形式，它增加了小型数据集的多样性，从而使其在未见数据上表现得更好。我们发现，增强提高了以下数据集的预测性能：具有较少的观测值、较小的基线 AUC、较高的基数分类变量以及更平衡的结果变量。没有特定的生成模型始终优于其他模型。我们开发了一个决策支持模型，可用于告知分析师增强是否有用。对于七个小型应用程序数据集，增强现有数据导致 AUC 增加 4.31%（AUC 从 0.71 增加到 0.75）和 43.23%（AUC 从 0.51 增加到 0.73），平均相对改进 15.55%，这表明了增强对小型数据集的非平凡影响（p=0.0078）。增强 AUC 高于仅重新采样的 AUC（p=0.016）。增强数据集的多样性高于重新采样数据集的多样性（p=0.046）。

##### **A Multi-Layered Large Language Model Framework for Disease Prediction**
2502.00063v1 by Malak Mohamed, Rokaia Emad, Ali Hamdi

Social telehealth has revolutionized healthcare by enabling patients to share
symptoms and receive medical consultations remotely. Users frequently post
symptoms on social media and online health platforms, generating a vast
repository of medical data that can be leveraged for disease classification and
symptom severity assessment. Large language models (LLMs), such as LLAMA3,
GPT-3.5 Turbo, and BERT, process complex medical data to enhance disease
classification. This study explores three Arabic medical text preprocessing
techniques: text summarization, text refinement, and Named Entity Recognition
(NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best
performance was achieved using CAMeL-BERT with NER-augmented text (83% type
classification, 69% severity assessment). Non-fine-tuned models performed
poorly (13%-20% type classification, 40%-49% severity assessment). Integrating
LLMs into social telehealth systems enhances diagnostic accuracy and treatment
outcomes.

摘要：社交遠距醫療透過讓患者可以遠距分享症狀並接受醫療諮詢，徹底改變了醫療保健。使用者經常在社群媒體和線上健康平台上發布症狀，產生了龐大的醫療資料庫，可利用於疾病分類和症狀嚴重性評估。大型語言模型 (LLM)，例如 LLAMA3、GPT-3.5 Turbo 和 BERT，處理複雜的醫療資料以增強疾病分類。本研究探討了三種阿拉伯語醫療文字前處理技術：文字摘要、文字精煉和命名實體辨識 (NER)。使用 LoRA 評估 CAMeL-BERT、AraBERT 和 Asafaya-BERT，使用具 NER 增強文字的 CAMeL-BERT 獲得最佳效能（83% 類型分類，69% 嚴重性評估）。未經過微調的模型效能不佳（13%-20% 類型分類，40%-49% 嚴重性評估）。將 LLM 整合到社交遠距醫療系統中可增強診斷準確性和治療結果。

##### **A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**
2501.18367v1 by Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li

In medical time series disease diagnosis, two key challenges are
identified.First, the high annotation cost of medical data leads to overfitting
in models trained on label-limited, single-center datasets. To address this, we
propose incorporating external data from related tasks and leveraging AE-GAN to
extract prior knowledge,providing valuable references for downstream tasks.
Second, many existing studies employ contrastive learning to derive more
generalized medical sequence representations for diagnostic tasks, usually
relying on manually designed diverse positive and negative sample
pairs.However, these approaches are complex, lack generalizability, and fail to
adaptively capture disease-specific features across different conditions.To
overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework),
a framework that integrates a multi-head attention mechanism and adaptively
learns representations from different views through inter-view and intra-view
contrastive learning strategies.Additionally, the pre-trained AE-GAN is used to
reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process.Experiments on three
target datasets demonstrate that our method consistently outperforms seven
other baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease.

摘要：在医疗时间序列疾病诊断中，确定了两个关键挑战。首先，医疗数据的标注成本高，导致在标签受限的单中心数据集上训练的模型出现过拟合。为了解决这个问题，我们建议合并来自相关任务的外部数据，并利用 AE-GAN 提取先验知识，为下游任务提供有价值的参考。其次，许多现有的研究采用对比学习来推导出更通用的医疗序列表示，用于诊断任务，通常依赖于手动设计的各种正负样本对。然而，这些方法复杂，缺乏通用性，并且无法自适应地捕获不同条件下的特定疾病特征。为了克服这个问题，我们引入了 LMCF（可学习的多视图对比框架），这是一个集成了多头注意机制的框架，并通过视图间和视图内对比学习策略自适应地学习来自不同视图的表示。此外，预训练的 AE-GAN 用于重建目标数据中的差异作为疾病概率，然后将其集成到对比学习过程中。在三个目标数据集上的实验表明，我们的方法始终优于其他七个基线，突出了其对医疗保健应用（如心肌梗塞、阿尔茨海默病和帕金森病的诊断）的重大影响。

##### **MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**
2501.18362v1 by Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou

We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.

摘要：我們推出了 MedXpertQA，這是一個極具挑戰性且全面的基準，用於評估專家級的醫學知識和先進的推理能力。MedXpertQA 包含 4,460 個問題，涵蓋 17 個專科和 11 個身體系統。它包含兩個子集，文本用於文本評估，MM 用於多模式評估。值得注意的是，MM 引入了專家級考試題目，其中包含多樣化的影像和豐富的臨床資訊，包括患者記錄和檢查結果，這讓它有別於傳統的醫學多模式基準，後者是從影像標題中產生的簡單問答對。MedXpertQA 採用嚴格的過濾和擴充，以解決 MedQA 等現有基準的難度不足問題，並納入專科委員會問題以提高臨床相關性和全面性。我們執行資料合成以降低資料外洩風險，並進行多輪專家審查以確保準確性和可靠性。我們在 MedXpertQA 上評估了 16 個領先的模型。此外，醫學與現實世界的決策制定有密切的聯繫，提供了豐富且具代表性的環境，用於評估超越數學和程式碼的推理能力。為此，我們開發了一個以推理為導向的子集，以利於評估類 o1 的模型。

##### **CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**
2501.18328v1 by Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai

MRI imputation aims to synthesize the missing modality from one or more
available ones, which is highly desirable since it reduces scanning costs and
delivers comprehensive MRI information to enhance clinical diagnosis. In this
paper, we propose a unified model, CodeBrain, designed to adapt to various
brain MRI imputation scenarios. The core design lies in casting various
inter-modality transformations as a full-modality code prediction task. To this
end, CodeBrain is trained in two stages: Reconstruction and Code Prediction.
First, in the Reconstruction stage, we reconstruct each MRI modality, which is
mapped into a shared latent space followed by a scalar quantization. Since such
quantization is lossy and the code is low dimensional, another MRI modality
belonging to the same subject is randomly selected to generate common features
to supplement the code and boost the target reconstruction. In the second
stage, we train another encoder by a customized grading loss to predict the
full-modality codes from randomly masked MRI samples, supervised by the
corresponding quantized codes generated from the first stage. In this way, the
inter-modality transformation is achieved by mapping the instance-specific
codes in a finite scalar space. We evaluated the proposed CodeBrain model on
two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments
demonstrate that our CodeBrain model achieves superior imputation performance
compared to four existing methods, establishing a new state of the art for
unified brain MRI imputation. Codes will be released.

摘要：MRI 補完旨在從一個或多個可用方式中合成遺失的模態，這是非常理想的，因為它降低了掃描成本，並提供了全面的 MRI 資訊以增強臨床診斷。在本文中，我們提出了一個統一模型 CodeBrain，旨在適應各種腦部 MRI 補完場景。核心設計在於將各種模態間轉換轉換為全模態碼預測任務。為此，CodeBrain 分兩個階段進行訓練：重建和碼預測。首先，在重建階段，我們重建每個 MRI 模態，它被映射到一個共享潛在空間，然後進行標量量化。由於這種量化是有損的，並且碼的維度很低，因此隨機選擇屬於同一個受試者的另一個 MRI 模態來產生共同特徵以補充碼並提升目標重建。在第二階段，我們通過自訂分級損失訓練另一個編碼器，從隨機遮罩的 MRI 樣本預測全模態碼，並由第一階段產生的對應量化碼進行監督。這樣，模態間轉換是通過將特定於例項的碼映射到一個有限的標量空間來實現的。我們在兩個公開的腦部 MRI 資料集（即 IXI 和 BraTS 2023）上評估了所提出的 CodeBrain 模型。大量的實驗證明，與四種現有方法相比，我們的 CodeBrain 模型實現了優異的補完效能，為統一的腦部 MRI 補完建立了新的技術水準。碼將會釋出。

##### **A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**
2501.18294v1 by Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki

Lung cancer is a major issue in worldwide public health, requiring early
diagnosis using stable techniques. This work begins a thorough investigation of
the use of machine learning (ML) methods for precise classification of lung
cancer stages. A cautious analysis is performed to overcome overfitting issues
in model performance, taking into account minimum child weight and learning
rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM,
Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF),
CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted.
Furthermore, the correlation between features and targets is examined using the
deep neural network (DNN) model and thus their capability in detecting complex
patternsis established. It is argued that several ML models can be capable of
classifying lung cancer stages with great accuracy. In spite of the complexity
of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic
Regression excel with superior performance. The models perform better than the
others in lung cancer prediction on the complete set of comparative metrics
like accuracy, precision, recall, and F-1 score

摘要：肺癌是全球公共衛生的一大問題，需要使用穩定的技術進行早期診斷。這項工作開始徹底調查使用機器學習 (ML) 方法精確分類肺癌分期的使用情況。執行謹慎的分析以克服模型效能中的過度擬合問題，並考慮最小子權重和學習率。一組機器學習 (ML) 模型，包括 XGBoost (XGB)、LGBM、Adaboost、邏輯迴歸 (LR)、決策樹 (DT)、隨機森林 (RF)、CatBoost 和 k 最近鄰 (k-NN)，以有條理的方式執行並進行對比。此外，使用深度神經網路 (DNN) 模型檢查特徵和目標之間的關聯性，從而建立它們在檢測複雜模式中的能力。有人認為，多個 ML 模型能夠以很高的準確度對肺癌分期進行分類。儘管 DNN 架構很複雜，但傳統 ML 模型（如 XGBoost、LGBM 和邏輯迴歸）表現出色，效能優異。這些模型在肺癌預測中表現優於其他模型，在準確度、精確度、召回率和 F-1 分數等完整的比較指標中表現出色。

##### **The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**
2501.18270v1 by Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, Séraphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szabó, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia

Artificial intelligence has significantly advanced skin cancer diagnosis by
enabling rapid and accurate detection of malignant lesions. In this domain,
most publicly available image datasets consist of single, isolated skin lesions
positioned at the center of the image. While these lesion-centric datasets have
been fundamental for developing diagnostic algorithms, they lack the context of
the surrounding skin, which is critical for improving lesion detection. The
iToBoS dataset was created to address this challenge. It includes 16,954 images
of skin regions from 100 participants, captured using 3D total body
photography. Each image roughly corresponds to a $7 \times 9$ cm section of
skin with all suspicious lesions annotated using bounding boxes. Additionally,
the dataset provides metadata such as anatomical location, age group, and sun
damage score for each image. This dataset aims to facilitate training and
benchmarking of algorithms, with the goal of enabling early detection of skin
cancer and deployment of this technology in non-clinical environments.

摘要：人工智慧透過快速且準確偵測惡性病灶，大幅提升皮膚癌的診斷。在這個領域中，大多數公開的影像資料集都包含單一、孤立的皮膚病灶，置於影像的中央。儘管這些以病灶為中心的資料集對於開發診斷演算法至關重要，但它們卻缺乏周圍皮膚的背景，這對於改善病灶偵測至關重要。iToBoS 資料集的建立就是為了應對這個挑戰。它包含 100 位參與者的 16,954 張皮膚區域影像，使用 3D 全身攝影技術擷取。每張影像大致對應於 $7 \times 9$ 公分的皮膚區域，所有可疑病灶都使用邊界框標註。此外，該資料集還提供每張影像的元資料，例如解剖位置、年齡組和日曬損傷評分。此資料集旨在促進演算法的訓練和基準測試，目標是實現皮膚癌的早期偵測，並將此技術部署在非臨床環境中。

##### **Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**
2501.18237v1 by Malte Tölle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt

A patient undergoes multiple examinations in each hospital stay, where each
provides different facets of the health status. These assessments include
temporal data with varying sampling rates, discrete single-point measurements,
therapeutic interventions such as medication administration, and images. While
physicians are able to process and integrate diverse modalities intuitively,
neural networks need specific modeling for each modality complicating the
training procedure. We demonstrate that this complexity can be significantly
reduced by visualizing all information as images along with unstructured text
and subsequently training a conventional vision-text transformer. Our approach,
Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not
only simplifies data preprocessing and modeling but also outperforms current
state-of-the-art methods in predicting in-hospital mortality and phenotyping,
as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities
include patient's clinical measurements, medications, X-ray images, and
electrocardiography scans. We hope our work inspires advancements in
multi-modal medical AI by reducing the training complexity to (visual) prompt
engineering, thus lowering entry barriers and enabling no-code solutions for
training. The source code will be made publicly available.

摘要：在每次住院期間，患者會接受多項檢查，每一項檢查都能提供健康狀態的不同面向。這些評估包括具有不同取樣率的時間資料、離散單點測量值、治療介入（如藥物管理）和影像。雖然醫生能夠直觀地處理和整合不同的模式，但神經網路需要針對每種模式進行特定的建模，這使得訓練程序變得複雜。我們證明，通過將所有資訊視覺化為影像，並結合非結構化文字，隨後訓練一個傳統的視覺文字轉換器，可以大幅降低這種複雜性。我們的做法，即用於不規則採樣多模式測量的視覺轉換器 (ViTiMM)，不僅簡化了資料預處理和建模，而且在預測院內死亡率和表型方面也優於目前的最新方法，這是根據 MIMIC-IV 資料集中的 6,175 名患者評估的。這些模式包括患者的臨床測量值、藥物、X 光影像和心電圖掃描。我們希望我們的工作能透過降低訓練複雜度到（視覺）提示工程，從而降低進入門檻，並為訓練啟用無程式碼解決方案，進而激勵多模式醫療 AI 的進步。原始程式碼將公開提供。

##### **Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**
2501.18108v1 by Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino

Despite the growing potential of older adult care technologies, the adoption
of these technologies remains challenging. In this work, we conducted a
focus-group session with family caregivers to scope designs of the older adult
care technology. We then developed a high-fidelity prototype and conducted its
qualitative study with professional caregivers and older adults to understand
their perspectives on the system functionalities. This system monitors abnormal
activity patterns of older adults using wireless motion sensors and machine
learning models and supports interactive dialogue responses to explain abnormal
activity patterns of older adults to caregivers and allow older adults
proactively sharing their status with caregivers for an adequate intervention.
Both older adults and professional caregivers appreciated that our system can
provide a faster, personalized service while proactively controlling what
information is to be shared through interactive dialogue responses. We further
discuss other considerations to realize older adult technology in practice.

摘要：儘管老年人照護技術的潛力日益增長，但採用這些技術仍具有挑戰性。在這項研究中，我們與家庭照護者進行焦點小組會議，以界定老年人照護技術的設計範圍。接著，我們開發了一個高保真原型，並與專業照護者和老年人進行質性研究，以了解他們對系統功能的觀點。此系統使用無線動作感測器和機器學習模型監控老年人的異常活動模式，並支援互動式對話回應，向照護者解釋老年人的異常活動模式，並讓老年人主動與照護者分享他們的狀態，以進行適當的介入。老年人和專業照護者都讚賞我們的系統能提供更快速、個人化的服務，同時透過互動式對話回應主動控制要分享哪些資訊。我們進一步討論其他考量因素，以在實務中實現老年人技術。

##### **Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**
2501.18081v1 by Pratik S. Sachdeva, Tom van Nuenen

The rapid adoption of large language models (LLMs) has spurred extensive
research into their encoded moral norms and decision-making processes. Much of
this research relies on prompting LLMs with survey-style questions to assess
how well models are aligned with certain demographic groups, moral beliefs, or
political ideologies. While informative, the adherence of these approaches to
relatively superficial constructs tends to oversimplify the complexity and
nuance underlying everyday moral dilemmas. We argue that auditing LLMs along
more detailed axes of human interaction is of paramount importance to better
assess the degree to which they may impact human beliefs and actions. To this
end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am
I the Asshole" (AITA) community on Reddit, where users seek moral judgments on
everyday conflicts from other community members. We prompted seven LLMs to
assign blame and provide explanations for over 10,000 AITA moral dilemmas. We
then compared the LLMs' judgments and explanations to those of Redditors and to
each other, aiming to uncover patterns in their moral reasoning. Our results
demonstrate that large language models exhibit distinct patterns of moral
judgment, varying substantially from human evaluations on the AITA subreddit.
LLMs demonstrate moderate to high self-consistency but low inter-model
agreement. Further analysis of model explanations reveals distinct patterns in
how models invoke various moral principles. These findings highlight the
complexity of implementing consistent moral reasoning in artificial systems and
the need for careful evaluation of how different models approach ethical
judgment. As LLMs continue to be used in roles requiring ethical
decision-making such as therapists and companions, careful evaluation is
crucial to mitigate potential biases and limitations.

摘要：大型語言模型 (LLM) 的快速採用已促使人們深入研究其編碼的道德規範和決策過程。許多這類研究依賴於以調查式問題提示 LLM，以評估模型與特定人口群體、道德信念或政治意識形態的契合程度。儘管有提供資訊，但這些方法對相對膚淺的結構的堅持傾向於過度簡化日常道德困境背後的複雜性和細微差別。我們認為，沿著更詳細的人類互動軸線審查 LLM 對於更好地評估它們可能影響人類信念和行為的程度至關重要。為此，我們根據 Reddit 上「我是混蛋嗎」(AITA) 社群評估 LLM 在複雜的日常道德困境中，使用者在其中尋求其他社群成員對日常衝突的道德判斷。我們提示七個 LLM 對超過 10,000 個 AITA 道德困境分配責任並提供解釋。然後，我們將 LLM 的判斷和解釋與 Reddit 使用者的判斷和解釋以及彼此進行比較，旨在揭示其道德推理中的模式。我們的結果表明，大型語言模型展現出不同的道德判斷模式，與 AITA 子版塊上的人類評估有很大差異。LLM 表現出中度到高度的自我一致性，但模型間協議低。進一步分析模型解釋揭示了模型如何援引各種道德原則的不同模式。這些發現突顯了在人工系統中實施一致的道德推理的複雜性，以及仔細評估不同模型如何進行道德判斷的必要性。隨著 LLM 持續用於需要道德決策的角色，例如治療師和伴侶，仔細評估對於減輕潛在偏見和限制至關重要。

##### **Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**
2501.18071v1 by Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino

Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.

摘要：糖尿病 (DM) 是一項重要的全球健康議題，必須盡早診斷並妥善管理。本研究提出一個糖尿病預測架構，使用機器學習 (ML) 模型，並搭配可解釋人工智慧 (XAI) 工具，來探討 ML 模型預測的準確度和可解釋性。資料前處理基於合成少數過採樣技術 (SMOTE) 和特徵縮放，用於糖尿病二元健康指標資料集，以處理類別不平衡和臨床特徵的可變性。整合模型提供了高準確度，測試準確度為 92.50%，ROC-AUC 為 0.975。根據模型解釋，BMI、年齡、一般健康狀況、收入和身體活動是最具影響力的預測因子。本研究結果表明，ML 結合 XAI 是一種有前途的方式，可以開發出準確且在運算上透明的工具，用於醫療保健系統。

##### **Current Pathology Foundation Models are unrobust to Medical Center Differences**
2501.18055v2 by Edwin D. de Jong, Eric Marcus, Jonas Teuwen

Pathology Foundation Models (FMs) hold great promise for healthcare. Before
they can be used in clinical practice, it is essential to ensure they are
robust to variations between medical centers. We measure whether pathology FMs
focus on biological features like tissue and cancer type, or on the well known
confounding medical center signatures introduced by staining procedure and
other differences. We introduce the Robustness Index. This novel robustness
metric reflects to what degree biological features dominate confounding
features. Ten current publicly available pathology FMs are evaluated. We find
that all current pathology foundation models evaluated represent the medical
center to a strong degree. Significant differences in the robustness index are
observed. Only one model so far has a robustness index greater than one,
meaning biological features dominate confounding features, but only slightly. A
quantitative approach to measure the influence of medical center differences on
FM-based prediction performance is described. We analyze the impact of
unrobustness on classification performance of downstream models, and find that
cancer-type classification errors are not random, but specifically attributable
to same-center confounders: images of other classes from the same medical
center. We visualize FM embedding spaces, and find these are more strongly
organized by medical centers than by biological factors. As a consequence, the
medical center of origin is predicted more accurately than the tissue source
and cancer type. The robustness index introduced here is provided with the aim
of advancing progress towards clinical adoption of robust and reliable
pathology FMs.

摘要：病理基礎模型 (FM) 對醫療保健而言極具潛力。在臨床實務中使用之前，必須確保它們能應對醫療中心之間的差異。我們衡量病理 FM 是否著重於組織和癌症類型等生物特徵，或著重於染色程序和其他差異所導致的眾所周知混淆醫療中心特徵。我們引進了穩健性指標。這項新穎的穩健性指標反映了生物特徵主導混淆特徵的程度。評估了十項目前公開提供的病理 FM。我們發現，所有目前評估的病理基礎模型在很大程度上代表了醫療中心。觀察到穩健性指標有顯著差異。到目前為止，只有一項模型的穩健性指標大於一，表示生物特徵主導混淆特徵，但僅略微主導。描述了衡量醫療中心差異對基於 FM 的預測效能影響的量化方法。我們分析了不穩健性對下游模型分類效能的影響，發現癌症類型分類錯誤並非隨機，而是特別歸因於同中心混淆因子：來自同一醫療中心的其他類別影像。我們視覺化 FM 嵌入空間，發現這些空間是由醫療中心而非生物因素更強有力地組織。因此，比組織來源和癌症類型更準確地預測了醫療中心的來源。在此引入穩健性指標，目的是推進朝著臨床採用穩健且可靠的病理 FM 的進展。

##### **Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**
2501.17860v1 by Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen

Current medical AI systems often fail to replicate real-world clinical
reasoning, as they are predominantly trained and evaluated on static text and
question-answer tasks. These tuning methods and benchmarks overlook critical
aspects like evidence-based reasoning and handling distracting information. To
bridge this gap, we introduce a novel benchmark that simulates real-world
diagnostic scenarios, integrating noise and difficulty levels aligned with
USMLE standards. Moreover, we explore dialogue-based fine-tuning, which
transforms static datasets into conversational formats to better capture
iterative reasoning processes. Experiments show that dialogue-tuned models
outperform traditional methods, with improvements of $9.64\%$ in multi-round
reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our
findings highlight dialogue tuning as a promising approach for advancing
clinically aligned and robust medical AI systems.

摘要：目前的醫療 AI 系統常無法複製真實世界的臨床推理，因為它們主要在靜態文字和問答任務上受訓和評估。這些調整方法和基準忽略了基於證據的推理和處理分散資訊等關鍵面向。為了彌補這個差距，我們提出一個模擬真實世界診斷情境的全新基準，整合與 USMLE 標準一致的雜訊和難度等級。此外，我們探索以對話為基礎的微調，將靜態資料集轉換為對話格式，以更好地捕捉反覆的推理過程。實驗顯示，對話微調模型優於傳統方法，在多輪推理情境中提升了 9.64%，在有雜訊的環境中提升了 6.18% 的準確度。我們的發現強調對話微調是一種有望推進與臨床相符且強健的醫療 AI 系統的方法。

##### **GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**
2501.17855v1 by Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee

Robot caregiving should be personalized to meet the diverse needs of care
recipients -- assisting with tasks as needed, while taking user agency in
action into account. In physical tasks such as handover, bathing, dressing, and
rehabilitation, a key aspect of this diversity is the functional range of
motion (fROM), which can vary significantly between individuals. In this work,
we learn to predict personalized fROM as a way to generalize robot
decision-making in a wide range of caregiving tasks. We propose a novel
data-driven method for predicting personalized fROM using functional assessment
scores from occupational therapy. We develop a neural model that learns to
embed functional assessment scores into a latent representation of the user's
physical function. The model is trained using motion capture data collected
from users with emulated mobility limitations. After training, the model
predicts personalized fROM for new users without motion capture. Through
simulated experiments and a real-robot user study, we show that the
personalized fROM predictions from our model enable the robot to provide
personalized and effective assistance while improving the user's agency in
action. See our website for more visualizations:
https://emprise.cs.cornell.edu/grace/.

摘要：機器人照護應根據照護對象的不同需求進行客製化，在需要時協助執行任務，同時考量使用者的自主行動。在移交、沐浴、穿衣和復健等身體任務中，這種多樣性的關鍵面向是功能性動作範圍 (fROM)，而這在不同個體之間可能差異很大。在這項工作中，我們學習預測客製化 fROM，作為在廣泛照護任務中概化機器人決策制定的一種方式。我們提出了一種使用職能治療功能評估分數來預測客製化 fROM 的新穎資料驅動方法。我們開發了一個神經模型，學習將功能評估分數嵌入到使用者的身體功能潛在表徵中。該模型使用從具有模擬行動限制的使用者收集的動作擷取資料進行訓練。訓練後，該模型會為沒有動作擷取的新使用者預測客製化 fROM。透過模擬實驗和真實機器人使用者研究，我們展示了我們模型的客製化 fROM 預測使機器人能夠提供客製化且有效的協助，同時提高使用者的自主行動。請參閱我們的網站以取得更多視覺化資料：https://emprise.cs.cornell.edu/grace/。

##### **Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**
2502.00055v1 by Ljubisa Bojic, Zorica Dodevska, Yashar Deldjoo, Nenad Pantelic

Given the exponential advancement in AI technologies and the potential
escalation of harmful effects from recommendation systems, it is crucial to
simulate and evaluate these effects early on. Doing so can help prevent
possible damage to both societies and technology companies. This paper
introduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel
simulation framework leveraging Large Language Models (LLMs) to explore the
impacts of different content recommendation setups on user engagement and
polarization in social networks. By creating diverse AI agents (AgentPrompts)
with descriptive, static, and dynamic attributes, we assess their autonomous
behaviour across three scenarios: Plurality, Balanced, and Similarity. Our
findings reveal that the Similarity Scenario, which aligns content with user
preferences, maximizes engagement while potentially fostering echo chambers.
Conversely, the Plurality Scenario promotes diverse interactions but produces
mixed engagement results. Our study emphasizes the need for a careful balance
in recommender system designs to enhance user satisfaction while mitigating
societal polarization. It underscores the unique value and challenges of
incorporating LLMs into simulation environments. The benefits of RecSysLLMsP
lie in its potential to calculate polarization effects, which is crucial for
assessing societal impacts and determining user engagement levels with diverse
recommender system setups. This advantage is essential for developing and
maintaining a successful business model for social media companies. However,
the study's limitations revolve around accurately emulating reality. Future
efforts should validate the similarity in behaviour between real humans and
AgentPrompts and establish metrics for measuring polarization scores.

摘要：<paragraph>隨著 AI 技術的指數級進步，以及推薦系統造成的有害影響的潛在升級，提早模擬和評估這些影響至關重要。這麼做有助於防止對社會和科技公司造成的潛在損害。這篇論文介紹了推薦系統 LLM 遊樂場 (RecSysLLMsP)，一個創新的模擬架構，利用大型語言模型 (LLM) 來探索不同內容推薦設定對社群網路中的使用者參與度和極化現象的影響。藉由創造具有描述性、靜態和動態屬性的多樣化 AI 代理程式 (AgentPrompts)，我們評估它們在三種情境中的自主行為：多元性、平衡性和相似性。我們的研究結果顯示，與使用者偏好一致的相似性情境，最大化了參與度，同時潛在促進了同溫層。相反地，多元性情境促進了多樣化的互動，但產生了參差不齊的參與度結果。我們的研究強調了在推薦系統設計中仔細取得平衡的必要性，以提升使用者滿意度，同時減輕社會極化現象。它強調了將 LLM 納入模擬環境的獨特價值和挑戰。RecSysLLMsP 的優勢在於它計算極化效應的潛力，這對於評估社會影響和確定使用者參與度與多樣化推薦系統設定的層級至關重要。這個優勢對於社群媒體公司開發和維持成功的商業模式至關重要。然而，這項研究的限制在於精確地模擬現實。未來的努力應該驗證真實人類和 AgentPrompts 之間行為的相似性，並建立衡量極化分數的指標。</paragraph>

##### **Tonguescape: Exploring Language Models Understanding of Vowel Articulation**
2501.17643v1 by Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe

Vowels are primarily characterized by tongue position. Humans have discovered
these features of vowel articulation through their own experience and explicit
objective observation such as using MRI. With this knowledge and our
experience, we can explain and understand the relationship between tongue
positions and vowels, and this knowledge is helpful for language learners to
learn pronunciation. Since language models (LMs) are trained on a large amount
of data that includes linguistic and medical fields, our preliminary studies
indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align
textual information with visual information. One question arises: do LMs
associate real tongue positions with vowel articulation? In this study, we
created video and image datasets from the existing real-time MRI dataset and
investigated whether LMs can understand vowel articulation based on tongue
positions using vision-based information. Our findings suggest that LMs exhibit
potential for understanding vowels and tongue positions when reference examples
are provided while they have difficulties without them. Our code for dataset
building is available on GitHub.

摘要：元音主要由舌頭位置決定。人類透過自己的經驗和明確的客觀觀察（例如使用 MRI）發現了元音發音的這些特徵。有了這些知識和經驗，我們可以解釋和理解舌頭位置和元音之間的關係，而這些知識對語言學習者學習發音很有幫助。由於語言模型 (LM) 是在包含語言學和醫學領域的大量資料上訓練的，我們的初步研究表明，LM 能夠解釋元音的發音機制。然而，尚不清楚多模態 LM（例如視覺 LM）是否將文字資訊與視覺資訊對齊。一個問題產生了：LM 是否將真實的舌頭位置與元音發音聯繫起來？在這項研究中，我們從現有的即時 MRI 資料集中建立了影片和影像資料集，並探討 LM 是否能根據舌頭位置使用基於視覺的資訊來理解元音發音。我們的研究結果表明，當提供參考範例時，LM 具有理解元音和舌頭位置的潛力，而沒有參考範例時則有困難。我們用於建立資料集的程式碼可在 GitHub 上取得。

##### **Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**
2501.18645v2 by Manish Sanwal

Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to
provide step-by-step rationales, improving performance on complex tasks.
Despite its benefits, vanilla CoT often fails to fully verify intermediate
inferences and can produce misleading explanations. In this work, we propose
Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that
systematically segments the reasoning process into multiple layers, each
subjected to external checks and optional user feedback. We expand on the key
concepts, present three scenarios -- medical triage, financial risk assessment,
and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT
in terms of transparency, correctness, and user engagement. By integrating
references from recent arXiv papers on interactive explainability, multi-agent
frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves
the way for more reliable and grounded explanations in high-stakes domains.

摘要：大型語言模型（LLM）利用思考鏈（CoT）提示提供逐步的理由，提升複雜任務的表現。儘管有其好處，但香草 CoT 常常無法完全驗證中間推論，且可能會產生誤導性的解釋。在這項研究中，我們提出分層思考鏈（分層 CoT）提示，一個新穎的架構，它有系統地將推理過程區隔成多個層級，每個層級都經過外部檢查和可選擇的使用者回饋。我們擴展關鍵概念，提出三個場景——醫療分流、財務風險評估和敏捷工程——並展示分層 CoT 在透明度、正確性和使用者參與度方面如何超越香草 CoT。透過整合來自近期 arXiv 論文中關於互動可解釋性、多重代理架構和基於代理的協作的參考文獻，我們說明分層 CoT 如何為高風險領域中更可靠且有根據的解釋鋪路。

##### **An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**
2501.17555v1 by Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian

Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms
that account for less than 5% of all pancreatic malignancies, with an incidence
of only 1-1.5 cases per 100,000. Early detection of pNETs is critical for
improving patient survival, but the rarity of pNETs makes segmenting them from
CT a very challenging problem. So far, there has not been a dataset
specifically for pNETs available to researchers. To address this issue, we
propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography
(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,
containing data from 469 patients. This is the first dataset solely dedicated
to pNETs, distinguishing it from previous collections. Additionally, we provide
the baseline detection networks with a new slice-wise weight loss function
designed for the UNet-based model, improving the overall pNET segmentation
performance. We hope that our dataset can enhance the understanding and
diagnosis of pNET Tumors within the medical community, facilitate the
development of more accurate diagnostic tools, and ultimately improve patient
outcomes and advance the field of oncology.

摘要：胰臟神經內分泌腫瘤 (pNETs) 是非常罕見的內分泌腫瘤，僅佔所有胰臟惡性腫瘤的不到 5%，每 100,000 人中僅發生 1-1.5 個病例。早期發現 pNETs 對改善患者存活率至關重要，但 pNETs 的罕見性使得從 CT 中分割它們成為一個非常具有挑戰性的問題。到目前為止，還沒有專門針對 pNETs 的數據集可供研究人員使用。為了解決這個問題，我們提出了一個 pNETs 數據集，一個專注於胰臟神經內分泌腫瘤的標註良好的對比增強電腦斷層掃描 (CECT) 數據集，包含來自 469 名患者的數據。這是第一個專門針對 pNETs 的數據集，這使其有別於之前的收集。此外，我們為基線檢測網路提供了一個新的基於 UNet 模型設計的切片加權損失函數，改善了整體 pNET 分割性能。我們希望我們的數據集能夠增強醫學界對 pNET 腫瘤的理解和診斷，促進更準確的診斷工具的開發，最終改善患者的預後並推進腫瘤學領域。

##### **LLM Assistance for Pediatric Depression**
2501.17510v1 by Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive

Traditional depression screening methods, such as the PHQ-9, are particularly
challenging for children in pediatric primary care due to practical
limitations. AI has the potential to help, but the scarcity of annotated
datasets in mental health, combined with the computational costs of training,
highlights the need for efficient, zero-shot approaches. In this work, we
investigate the feasibility of state-of-the-art LLMs for depressive symptom
extraction in pediatric settings (ages 6-24). This approach aims to complement
traditional screening and minimize diagnostic errors.
  Our findings show that all LLMs are 60% more efficient than word match, with
Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the
extraction of more rare symptoms like "sleep problems" (F1: 0.92) and
"self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and
recall (0.60), performing well in categories like "Feeling depressed" (0.69)
and "Weight change" (0.78). Llama 3, with the highest recall (0.90),
overgeneralizes symptoms, making it less suitable for this type of analysis.
Challenges include the complexity of clinical notes and overgeneralization from
PHQ-9 scores. The main challenges faced by LLMs include navigating the complex
structure of clinical notes with content from different times in the patient
trajectory, as well as misinterpreting elevated PHQ-9 scores.
  We finally demonstrate the utility of symptom annotations provided by Flan as
features in an ML algorithm, which differentiates depression cases from
controls with high precision of 0.78, showing a major performance boost
compared to a baseline that does not use these features.

摘要：<paragraph>傳統的憂鬱症篩檢方法，例如 PHQ-9，由於實際限制，對於小兒科初級照護中的兒童來說特別具有挑戰性。AI 有可能提供幫助，但心理健康中註解資料集的稀少，加上訓練的運算成本，突顯了對有效率的零次學習方法的需求。在這項工作中，我們探討了最先進的 LLM 在小兒科環境（6-24 歲）中提取憂鬱症狀的可行性。這種方法旨在補充傳統篩檢並將診斷錯誤降至最低。我們的研究結果顯示，所有 LLM 的效率都比字詞比對高出 60%，而 Flan 在精確度方面領先（平均 F1：0.65，精確度：0.78），在提取較罕見的症狀方面表現出色，例如「睡眠問題」（F1：0.92）和「自我厭惡」（F1：0.8）。Phi 在精確度（0.44）和召回率（0.60）之間取得平衡，在「感到沮喪」（0.69）和「體重改變」（0.78）等類別中表現良好。擁有最高召回率（0.90）的 Llama 3 會過度概括症狀，使其不太適合此類分析。挑戰包括臨床筆記的複雜性和 PHQ-9 分數的過度概括。LLM 面臨的主要挑戰包括在患者歷程中不同時間的內容中導航臨床筆記的複雜結構，以及誤解 PHQ-9 分數升高。我們最後展示了 Flan 提供的症狀註解作為機器學習演算法中特徵的效用，它以 0.78 的高精確度將憂鬱症病例與對照組區分開來，與不使用這些特徵的基準相比，顯示出主要的效能提升。</paragraph>

##### **Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**
2502.00052v1 by Gonzalo Iñaki Quintana, Laurence Vancamberg, Vincent Jugnon, Agnès Desolneux, Mathilde Mougeot

This work studies the relationship between Contrastive Learning and Domain
Adaptation from a theoretical perspective. The two standard contrastive losses,
NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to
the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely
used for Domain Adaptation. Our work shows that minimizing the contrastive
losses decreases the CMMD and simultaneously improves class-separability,
laying the theoretical groundwork for the use of Contrastive Learning in the
context of Domain Adaptation. Due to the relevance of Domain Adaptation in
medical imaging, we focused the experiments on mammography images. Extensive
experiments on three mammography datasets - synthetic patches, clinical (real)
patches, and clinical (real) images - show improved Domain Adaptation,
class-separability, and classification performance, when minimizing the
Supervised Contrastive loss.

摘要：本研究從理論角度探討對比學習與領域適應之間的關係。兩種標準對比損失，NT-Xent 損失（自我監督）和監督對比損失，與廣泛用於領域適應的差異性測量標準類別平均最大差異（CMMD）有關。我們的研究表明，最小化對比損失會降低 CMMD，同時提高類別可分離性，為在領域適應中使用對比學習奠定理論基礎。由於領域適應在醫學影像中的相關性，我們將實驗重點放在乳房攝影圖像上。在三個乳房攝影數據集（合成貼片、臨床（真實）貼片和臨床（真實）圖像）上進行的廣泛實驗表明，在最小化監督對比損失時，領域適應、類別可分離性和分類性能得到改善。

##### **Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**
2501.17343v1 by Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo

Quantizing deep neural networks ,reducing the precision (bit-width) of their
computations, can remarkably decrease memory usage and accelerate processing,
making these models more suitable for large-scale medical imaging applications
with limited computational resources. However, many existing methods studied
"fake quantization", which simulates lower precision operations during
inference, but does not actually reduce model size or improve real-world
inference speed. Moreover, the potential of deploying real 3D low-bit
quantization on modern GPUs is still unexplored. In this study, we introduce a
real post-training quantization (PTQ) framework that successfully implements
true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation
models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,
ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use
TensorRT to perform fake quantization for both weights and activations with
unlabeled calibration dataset. Second, we convert this fake quantization into
real quantization via TensorRT engine on real GPUs, resulting in real-world
reductions in model size and inference latency. Extensive experiments
demonstrate that our framework effectively performs 8-bit quantization on GPUs
without sacrificing model performance. This advancement enables the deployment
of efficient deep learning models in medical imaging applications where
computational resources are constrained. The code and models have been
released, including U-Net, TransUNet pretrained on the BTCV dataset for
abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset
for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and
VISTA3D pretrained on TotalSegmentator V2 for full body (104-label)
segmentation. https://github.com/hrlblab/PTQ.

摘要：<paragraph>量化深度神经网络，降低其计算的精度（位宽），可以显著减少内存使用量并加速处理，使这些模型更适合于具有有限计算资源的大规模医学影像应用。然而，许多现有方法研究了“伪量化”，它在推理期间模拟较低精度的操作，但实际上并没有减少模型大小或提高实际推理速度。此外，在现代 GPU 上部署真正的 3D 低位量化的潜力仍未得到探索。在这项研究中，我们引入了一个真正的训练后量化 (PTQ) 框架，该框架成功地在最先进的 (SOTA) 3D 医学分割模型（即 U-Net、SegResNet、SwinUNETR、nnU-Net、UNesT、TransUNet、ST-UNet 和 VISTA3D）上实现了真正的 8 位量化。我们的方法涉及两个主要步骤。首先，我们使用 TensorRT 对权重和激活进行伪量化，并使用未标记的校准数据集。其次，我们将这种伪量化通过真实 GPU 上的 TensorRT 引擎转换为真正的量化，从而在模型大小和推理延迟方面实现了实际的减少。大量的实验表明，我们的框架在 GPU 上有效地执行 8 位量化，而不会牺牲模型性能。这一进步使得在计算资源受限的医学影像应用中部署高效的深度学习模型成为可能。代码和模型已经发布，包括 U-Net、TransUNET，在 BTCV 数据集上预训练用于腹部（13 标签）分割，UNesT 在 Whole Brain 数据集上预训练用于全脑（133 标签）分割，以及 nnU-Net、SegResNet、SwinUNETR 和 VISTA3D 在 TotalSegmentator V2 上预训练用于全身（104 标签）分割。https://github.com/hrlblab/PTQ。</paragraph>

##### **Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**
2501.17338v1 by Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang

Generative Language Models rely on autoregressive decoding to produce the
output sequence token by token. Many tasks such as preference optimization,
require the model to produce task-level output consisting of multiple tokens
directly by selecting candidates from a pool as predictions. Determining a
task-level prediction from candidates using the ordinary token-level decoding
mechanism is constrained by time-consuming decoding and interrupted gradients
by discrete token selection. Existing works have been using decoding-free
candidate selection methods to obtain candidate probability from initial output
logits over vocabulary. Though these estimation methods are widely used, they
are not systematically evaluated, especially on end tasks. We introduce an
evaluation of a comprehensive collection of decoding-free candidate selection
approaches on a comprehensive set of tasks, including five multiple-choice QA
tasks with a small candidate pool and four clinical decision tasks with a
massive amount of candidates, some with 10k+ options. We evaluate the
estimation methods paired with a wide spectrum of foundation LMs covering
different architectures, sizes and training paradigms. The results and insights
from our analysis inform the future model design.

摘要：生成語言模型依靠自迴歸解碼來逐個符號產生輸出序列。許多任務（如偏好最佳化）要求模型直接從候選池中選擇預測，產生由多個符號組成的任務級別輸出。使用一般的符號級別解碼機制從候選者中確定任務級別預測受到耗時的解碼和離散符號選擇中斷的梯度的約束。現有工作一直使用無解碼候選者選擇方法從初始輸出邏輯值中獲得候選者機率。儘管這些估計方法被廣泛使用，但它們並未經過系統評估，特別是在最終任務上。我們針對全面的任務集，包括五個具有小型候選者池的多選題問答任務和四個具有大量候選者的臨床決策任務（其中一些有 10k+ 選項），對全面的無解碼候選者選擇方法進行評估。我們評估與廣泛基礎語言模型配對的估計方法，這些模型涵蓋不同的架構、大小和訓練範例。我們分析的結果和見解為未來的模型設計提供了資訊。

##### **Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**
2501.17326v1 by Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang

Clinical diagnosis prediction models, when provided with a patient's medical
history, aim to detect potential diseases early, facilitating timely
intervention and improving prognostic outcomes. However, the inherent scarcity
of patient data and large disease candidate space often pose challenges in
developing satisfactory models for this intricate task. The exploration of
leveraging Large Language Models (LLMs) for encapsulating clinical decision
processes has been limited. We introduce MERA, a clinical diagnosis prediction
model that bridges pertaining natural language knowledge with medical practice.
We apply hierarchical contrastive learning on a disease candidate ranking list
to alleviate the large decision space issue. With concept memorization through
fine-tuning, we bridge the natural language clinical knowledge with medical
codes. Experimental results on MIMIC-III and IV datasets show that MERA
achieves the state-of-the-art diagnosis prediction performance and dramatically
elevates the diagnosis prediction capabilities of generative LMs.

摘要：臨床診斷預測模型在提供患者病歷的同時，旨在及早發現潛在疾病，促進及時干預並改善預後結果。然而，患者數據的固有稀缺性和大量的疾病候選空間通常對開發令人滿意的模型以應對這項複雜的任務構成挑戰。利用大型語言模型 (LLM) 來封裝臨床決策流程的探索受到限制。我們引入了 MERA，這是一個臨床診斷預測模型，它將相關的自然語言知識與醫療實踐聯繫起來。我們在疾病候選排名清單上應用分層對比學習，以緩解大型決策空間問題。通過微調概念記憶，我們將自然語言臨床知識與醫療代碼聯繫起來。在 MIMIC-III 和 IV 數據集上的實驗結果表明，MERA 達到了最先進的診斷預測性能，並顯著提升了生成式 LM 的診斷預測能力。

##### **Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**
2501.17286v1 by Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu

Background: The radiation oncology clinical practice involves many steps
relying on the dynamic interplay of abundant text data. Large language models
have displayed remarkable capabilities in processing complex text information.
But their direct applications in specific fields like radiation oncology remain
underexplored.
  Purpose: This study aims to investigate whether fine-tuning LLMs with domain
knowledge can improve the performance on Task (1) treatment regimen generation,
Task (2) treatment modality selection (photon, proton, electron, or
brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.
  Methods: Data for 15,724 patient cases were extracted. Cases where patients
had a single diagnostic record, and a clearly identifiable primary treatment
plan were selected for preprocessing and manual annotation to have 7,903 cases
of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.
Each case was used to construct a pair consisting of patient diagnostics
details and an answer (treatment regimen, treatment modality, or ICD-10 code
respectively) for the supervised fine-tuning of these three tasks. Open source
LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the
Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for
the fine-tuned models and original models. Clinical evaluation was performed on
Task (1) by radiation oncologists, while precision, recall, and F-1 score were
evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used
to statistically analyze the results.
  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with
p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the
fine-tuned LLMs-generated treatment regimens were clinically acceptable.
Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.

摘要：<paragraph>背景：放射肿瘤临床实践涉及许多步骤，这些步骤依赖于丰富文本数据的动态交互。大型语言模型在处理复杂的文本信息方面表现出了卓越的能力。但它们在放射肿瘤等特定领域的直接应用仍未得到充分探索。
目的：本研究旨在调查通过领域知识微调 LLM 是否可以提高任务 (1) 治疗方案生成、任务 (2) 治疗方式选择（光子、质子、电子或近距离放射治疗）和任务 (3) 放射肿瘤中 ICD-10 代码预测的性能。
方法：提取了 15,724 例患者病例的数据。选择了患者有单一诊断记录且有明确可识别的主要治疗计划的病例，进行预处理和手动注释，得到 7,903 例患者诊断、治疗计划、治疗方式和 ICD-10 代码。每个病例都用于构建一对，包括患者诊断详情和答案（分别是治疗方案、治疗方式或 ICD-10 代码），用于这三个任务的监督微调。开源 LLaMA2-7B 和 Mistral-7B 模型被用于使用低秩逼近方法进行微调。报告了微调模型和原始模型的准确性和 ROUGE-1 分数。任务 (1) 由放射肿瘤科医师进行临床评估，而任务 (2) 和 (3) 则评估了精确度、召回率和 F-1 分数。单侧 Wilcoxon 符号秩检验用于对结果进行统计分析。
结果：微调后的 LLM 在所有任务中都优于原始 LLM，p 值 <= 0.001。临床评估表明，超过 60% 的微调 LLM 生成的治疗方案在临床上是可接受的。精确度、召回率和 F1 分数显示微调后的 LLM 性能得到改善。</paragraph>

##### **ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**
2501.17260v1 by Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee

Optical Coherence Tomography (OCT) is a non-invasive imaging modality
essential for diagnosing various eye diseases. Despite its clinical
significance, developing OCT-based diagnostic tools faces challenges, such as
limited public datasets, sparse annotations, and privacy concerns. Although
deep learning has made progress in automating OCT analysis, these challenges
remain unresolved. To address these limitations, we introduce the Vision
Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a
novel framework designed to enhance feature extraction and improve diagnostic
accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,
Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining
phase leverages the OCTMNIST dataset (97,477 unlabeled images across four
disease classes) with data augmentation to create dual-augmented views. A
Vision Transformer (ViT-Base) backbone extracts features, while a negative
cosine similarity loss aligns feature representations. Pretraining is conducted
over 50 epochs with a learning rate of 0.0001 and momentum of 0.999.
Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using
10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of
0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming
existing SSP-based methods.

摘要：光學相干斷層掃描（OCT）是一種非侵入式影像模式，對於診斷各種眼疾至關重要。儘管其臨床意義重大，但開發基於 OCT 的診斷工具面臨挑戰，例如公共數據集有限、註解稀疏和隱私問題。儘管深度學習在自動化 OCT 分析方面取得了進展，但這些挑戰仍然沒有解決。為了應對這些限制，我們引入了基於 Vision Transformer 的雙流自監督預訓練網路（ViT-2SPN），這是一個新穎的框架，旨在增強特徵提取並提高診斷準確性。ViT-2SPN 採用三階段工作流程：監督預訓練、自監督預訓練（SSP）和監督微調。預訓練階段利用 OCTMNIST 數據集（跨越四個疾病類別的 97,477 張未標記影像）和數據擴充來建立雙重擴充的檢視。視覺轉換器（ViT-Base）主幹提取特徵，而負餘弦相似度損失則校準特徵表示。預訓練在 50 個世代中進行，學習率為 0.0001，動能為 0.999。微調在 OCTMNIST 的分層 5.129% 子集上執行，使用 10 倍交叉驗證。ViT-2SPN 達到了 0.93 的平均 AUC、0.77 的準確率、0.81 的精確度、0.75 的召回率和 0.76 的 F1 分數，優於現有的基於 SSP 的方法。

##### **A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**
2501.17160v1 by Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham

Early detection of COVID-19 is crucial for effective treatment and
controlling its spread. This study proposes a novel hybrid deep learning model
for detecting COVID-19 from CT scan images, designed to assist overburdened
medical professionals. Our proposed model leverages the strengths of VGG16,
DenseNet121, and MobileNetV2 to extract features, followed by Principal
Component Analysis (PCA) for dimensionality reduction, after which the features
are stacked and classified using a Support Vector Classifier (SVC). We
conducted comparative analysis between the proposed hybrid model and individual
pre-trained CNN models, using a dataset of 2,108 training images and 373 test
images comprising both COVID-positive and non-COVID images. Our proposed hybrid
model achieved an accuracy of 98.93%, outperforming the individual models in
terms of precision, recall, F1 scores, and ROC curve performance.

摘要：早期偵測 COVID-19 對有效治療和控制其傳播至關重要。本研究提出一個新穎的深度學習混合模型，用於從電腦斷層掃描影像中偵測 COVID-19，旨在協助負擔過重的醫療專業人員。我們提出的模型利用 VGG16、DenseNet121 和 MobileNetV2 的優點來萃取特徵，接著進行主成分分析 (PCA) 以進行降維，然後將特徵堆疊並使用支持向量分類器 (SVC) 進行分類。我們對提出的混合模型和個別預訓練的 CNN 模型進行比較分析，使用包含 2,108 張訓練影像和 373 張測試影像的資料集，其中包含 COVID-19 陽性影像和非 COVID-19 影像。我們提出的混合模型達到了 98.93% 的準確度，在精準度、召回率、F1 分數和 ROC 曲線效能方面優於個別模型。

##### **Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**
2501.17152v1 by Reza Ghorbani, Jyothi Rikhab Chand, Chu-Yu Lee, Mathews Jacob, Merry Mani

Three-dimensional (3D) multi-slab acquisition is a technique frequently
employed in high-resolution diffusion-weighted MRI in order to achieve the best
signal-to-noise ratio (SNR) efficiency. However, this technique is limited by
slab boundary artifacts that cause intensity fluctuations and aliasing between
slabs which reduces the accuracy of anatomical imaging. Addressing this issue
is crucial for advancing diffusion MRI quality and making high-resolution
imaging more feasible for clinical and research applications. In this work, we
propose a regularized slab profile encoding (PEN) method within a Plug-and-Play
ADMM framework, incorporating multi-scale energy (MuSE) regularization to
effectively improve the slab combined reconstruction. Experimental results
demonstrate that the proposed method significantly improves image quality
compared to non-regularized and TV-regularized PEN approaches. The regularized
PEN framework provides a more robust and efficient solution for high-resolution
3D diffusion MRI, potentially enabling clearer, more reliable anatomical
imaging across various applications.

摘要：三維 (3D) 多層板擷取是一種技術，經常使用於高解析度擴散加權 MRI，以達到最佳的訊號雜訊比 (SNR) 效率。然而，此技術受到層板邊界偽影的限制，會造成強度波動和層板之間的混疊，降低解剖影像的準確度。解決這個問題對於提升擴散 MRI 品質至關重要，並使高解析度影像更適用於臨床和研究應用。在這項工作中，我們在 Plug-and-Play ADMM 架構內提出正規化的層板輪廓編碼 (PEN) 方法，並結合多尺度能量 (MuSE) 正規化，以有效改善層板組合重建。實驗結果證明，與非正規化和 TV 正規化 PEN 方法相比，所提出的方法顯著提升了影像品質。正規化的 PEN 架構為高解析度 3D 擴散 MRI 提供更強固且有效率的解決方案，潛在可實現更清晰、更可靠的解剖影像，適用於各種應用。

##### **Irony Detection, Reasoning and Understanding in Zero-shot Learning**
2501.16884v1 by Peiling Yi, Yuhan Xia

Irony is a powerful figurative language (FL) on social media that can
potentially mislead various NLP tasks, such as recommendation systems,
misinformation checks, and sentiment analysis. Understanding the implicit
meaning of this kind of subtle language is essential to mitigate irony's
negative impact on NLP tasks. However, building models to understand irony
presents a unique set of challenges, because irony is a complex form of
language that often relies on context, tone, and subtle cues to convey meaning
that is opposite or different from the literal interpretation. Large language
models, such as ChatGPT, are increasingly able to capture implicit and
contextual information. In this study, we investigate the generalization,
reasoning and understanding ability of ChatGPT on irony detection across six
different genre irony detection datasets. Our findings suggest that ChatGPT
appears to show an enhanced language understanding and reasoning ability. But
it needs to be very careful in prompt engineering design. Thus, we propose a
prompt engineering design framework IDADP to achieve higher irony detection
accuracy, improved understanding of irony, and more effective explanations
compared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain
via experiments that the practice generated under the framework is likely to be
the promised solution to resolve the generalization issues of LLMs.

摘要：反諷是一種強大的社交媒體比喻語言 (FL)，可能會誤導各種 NLP 任務，例如推薦系統、錯誤訊息檢查和情緒分析。理解這種微妙語言的隱含含義對於減輕反諷對 NLP 任務的負面影響至關重要。然而，建立模型來理解反諷會帶來一系列獨特的挑戰，因為反諷是一種複雜的語言形式，通常依賴於上下文、語氣和微妙的線索來傳達與字面解釋相反或不同的含義。大型語言模型，例如 ChatGPT，越來越能夠捕捉隱含和上下文信息。在本研究中，我們探討了 ChatGPT 在六個不同類型反諷檢測數據集上的反諷檢測的概括、推理和理解能力。我們的研究結果表明，ChatGPT 似乎表現出增強的語言理解和推理能力。但它需要在提示工程設計中非常小心。因此，我們提出了一個提示工程設計框架 IDADP，以實現更高的反諷檢測準確度、改進的反諷理解以及與其他最先進的 ChatGPT 零次學習方法相比更有效的解釋。並通過實驗確定在該框架下產生的實踐很可能是解決 LLM 概括問題的承諾解決方案。

##### **Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**
2501.17207v1 by Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang

Functional brain connectome is crucial for deciphering the neural mechanisms
underlying cognitive functions and neurological disorders. Graph deep learning
models have recently gained tremendous popularity in this field. However, their
actual effectiveness in modeling the brain connectome remains unclear. In this
study, we re-examine graph deep learning models based on four large-scale
neuroimaging studies encompassing diverse cognitive and clinical outcomes.
Surprisingly, we find that the message aggregation mechanism, a hallmark of
graph deep learning models, does not help with predictive performance as
typically assumed, but rather consistently degrades it. To address this issue,
we propose a hybrid model combining a linear model with a graph attention
network through dual pathways, achieving robust predictions and enhanced
interpretability by revealing both localized and global neural connectivity
patterns. Our findings urge caution in adopting complex deep learning models
for functional brain connectome analysis, emphasizing the need for rigorous
experimental designs to establish tangible performance gains and perhaps more
importantly, to pursue improvements in model interpretability.

摘要：功能性腦連接體對於破譯認知功能和神經疾病背後的機制至關重要。圖形深度學習模型最近在這個領域獲得極大的歡迎。然而，它們在建模腦連接體的實際效能仍不明確。在這項研究中，我們根據四項涵蓋不同認知和臨床結果的大規模神經影像研究，重新檢視圖形深度學習模型。令人驚訝的是，我們發現訊息聚合機制（圖形深度學習模型的標誌）並不像通常假設的那樣有助於預測效能，反而持續降低效能。為了解決這個問題，我們提出一個混合模型，透過雙路徑結合線性模型與圖形注意力網路，達成穩健的預測和增強的可解釋性，方法是揭露局部和整體的神經連接模式。我們的發現敦促在採用複雜的深度學習模型進行功能性腦連接體分析時保持謹慎，強調需要嚴謹的實驗設計，以建立具體的效能增益，或許更重要的是，追求模型可解釋性的改進。

##### **Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**
2501.17206v1 by Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao

This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.

摘要：本研究探索一種創新的方法，透過整合社會輔助機器人、強化學習 (RL)、大型語言模型 (LLM) 和臨床領域專業知識於模擬環境中，以推進失智症照護。這種整合解決了失智症照護中社會輔助機器人實驗數據有限的重大挑戰，提供了一個動態的模擬環境，真實地模擬失智症患者 (PLWD) 和機器人照護者之間的互動。所提出的架構引入了機率模型來表示 PLWD 的認知和情緒狀態，並結合了基於 LLM 的行為模擬來模擬他們的反應。我們進一步開發並訓練了一個適應性 RL 系統，使 Pepper 等人形機器人能夠根據 PLWD 的認知和情緒狀態提供情境感知和個人化的互動和協助。該架構也概括到電腦代理，突顯了它的多功能性。結果表明，由 LLM 增強的 RL 系統有效地解釋和回應 PLWD 的複雜需求，提供量身打造的照護策略。這項研究透過提供一個可自訂的 AI 驅動照護平台，促進對失智症相關挑戰的了解，並促進輔助技術的協作創新，為人機互動和人機互動做出貢獻。所提出的方法有可能提高 PLWD 的獨立性和生活品質，同時減輕照護者的負擔，強調了互動導向 AI 系統在失智症照護中的轉型作用。

##### **Efficient Knowledge Distillation of SAM for Medical Image Segmentation**
2501.16740v1 by Kunal Dasharath Patil, Gowthamaan Palani, Ganapathy Krishnamurthi

The Segment Anything Model (SAM) has set a new standard in interactive image
segmentation, offering robust performance across various tasks. However, its
significant computational requirements limit its deployment in real-time or
resource-constrained environments. To address these challenges, we propose a
novel knowledge distillation approach, KD SAM, which incorporates both encoder
and decoder optimization through a combination of Mean Squared Error (MSE) and
Perceptual Loss. This dual-loss framework captures structural and semantic
features, enabling the student model to maintain high segmentation accuracy
while reducing computational complexity. Based on the model evaluation on
datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast
Ultrasound, we demonstrate that KD SAM achieves comparable or superior
performance to the baseline models, with significantly fewer parameters. KD SAM
effectively balances segmentation accuracy and computational efficiency, making
it well-suited for real-time medical image segmentation applications in
resource-constrained environments.

摘要：分段任何模型 (SAM) 已在互動式影像分割中樹立新標準，在各項任務中皆能提供穩健的效能。然而，其龐大的運算需求限制了它在即時或資源受限環境中的部署。為了應對這些挑戰，我們提出了一種新穎的知識蒸餾方法，KD SAM，它透過結合均方誤差 (MSE) 和感知損失，將編碼器和解碼器最佳化納入其中。此雙重損失架構擷取結構和語義特徵，讓學生模型能夠在降低運算複雜度的同時，維持高分割準確度。根據在 Kvasir-SEG、ISIC 2017、胎兒頭部超音波和乳房超音波等資料集上的模型評估，我們證明 KD SAM 達到了與基準模型相當或更優異的效能，且參數明顯更少。KD SAM 有效地平衡了分割準確度和運算效率，使其非常適合在資源受限環境中的即時醫學影像分割應用。

##### **VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**
2501.16672v1 by Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour

Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.

摘要：大型語言模型 (LLM) 在臨床醫學中生成文本的事實準確性，缺乏確保的方法。VeriFact 是一種人工智慧系統，結合了檢索增強生成和 LLM-as-a-Judge，用於驗證 LLM 生成的文本是否基於病人的電子健康記錄 (EHR) 獲得病人的病歷事實支持。為了評估這個系統，我們引入了 VeriFact-BHC，這是一個新的資料集，將出院摘要中的簡要住院病程分解成一組簡單的陳述，並由臨床醫生註解每一個陳述是否獲得病人的 EHR 病歷摘要支持。儘管臨床醫生之間的最高一致性為 88.5%，但與去噪和裁決的平均人類臨床醫生基本事實相比，VeriFact 的一致性高達 92.7%，這表明 VeriFact 超越了平均臨床醫生根據病人的病歷檢查文本事實的能力。VeriFact 可能會透過移除目前的評估瓶頸，加速基於 LLM 的 EHR 應用程式的開發。

##### **Vision-based autonomous structural damage detection using data-driven methods**
2501.16662v2 by Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei

This study addresses the urgent need for efficient and accurate damage
detection in wind turbine structures, a crucial component of renewable energy
infrastructure. Traditional inspection methods, such as manual assessments and
non-destructive testing (NDT), are often costly, time-consuming, and prone to
human error. To tackle these challenges, this research investigates advanced
deep learning algorithms for vision-based structural health monitoring (SHM). A
dataset of wind turbine surface images, featuring various damage types and
pollution, was prepared and augmented for enhanced model training. Three
algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to
detect and classify surface damage. The models were trained and evaluated on a
dataset split into training, testing, and evaluation subsets (80%-10%-10%).
Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50
and high processing speed, making it suitable for real-time inspections. By
optimizing hyperparameters like learning rate and batch size, the models'
accuracy and efficiency improved further. YOLOv7 demonstrated significant
advancements in detection precision and execution speed, especially for
real-time applications. However, challenges such as dataset limitations and
environmental variability were noted, suggesting future work on segmentation
methods and larger datasets. This research underscores the potential of
vision-based deep learning techniques to transform SHM practices by reducing
costs, enhancing safety, and improving reliability, thus contributing to the
sustainable maintenance of critical infrastructure and supporting the longevity
of wind energy systems.

摘要：本研究解決了風力渦輪機結構中迫切需要的有效且準確的損傷檢測，這是可再生能源基礎設施的關鍵組成部分。傳統的檢查方法，例如手動評估和非破壞性檢測 (NDT)，通常成本高昂、耗時且容易出錯。為了應對這些挑戰，本研究調查了用於基於視覺的結構健康監測 (SHM) 的先進深度學習演算法。準備了一組風力渦輪機表面影像的資料集，其中包含各種損壞類型和污染，並擴充了增強模型訓練。採用了三種演算法——YOLOv7、其輕量級變體和 Faster R-CNN——來檢測和分類表面損壞。這些模型在分割成訓練、測試和評估子集（80%-10%-10%）的資料集上進行訓練和評估。結果表明，YOLOv7 優於其他演算法，實現了 82.4% 的 mAP@50 和較高的處理速度，使其適用於即時檢查。通過最佳化學習率和批次大小等超參數，模型的準確性和效率進一步提高。YOLOv7 在檢測精度和執行速度方面表現出顯著的進步，特別是對於即時應用程式。然而，注意到資料集限制和環境變異性等挑戰，這表明未來在分割方法和更大的資料集方面的工作。本研究強調了基於視覺的深度學習技術在轉換 SHM 實務方面的潛力，方法是降低成本、增強安全性並提高可靠性，從而有助於維護關鍵基礎設施的可持續性並支持風能系統的長壽命。

##### **Molecular-driven Foundation Model for Oncologic Pathology**
2501.16652v1 by Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood

Foundation models are reshaping computational pathology by enabling transfer
learning, where models pre-trained on vast datasets can be adapted for
downstream diagnostic, prognostic, and therapeutic response tasks. Despite
these advances, foundation models are still limited in their ability to encode
the entire gigapixel whole-slide images without additional training and often
lack complementary multimodal data. Here, we introduce Threads, a slide-level
foundation model capable of generating universal representations of whole-slide
images of any size. Threads was pre-trained using a multimodal learning
approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained
tissue sections, paired with corresponding genomic and transcriptomic profiles
- the largest such paired dataset to be used for foundation model development
to date. This unique training paradigm enables Threads to capture the tissue's
underlying molecular composition, yielding powerful representations applicable
to a wide array of downstream tasks. In extensive benchmarking across 54
oncology tasks, including clinical subtyping, grading, mutation prediction,
immunohistochemistry status determination, treatment response prediction, and
survival prediction, Threads outperformed all baselines while demonstrating
remarkable generalizability and label efficiency. It is particularly well
suited for predicting rare events, further emphasizing its clinical utility. We
intend to make the model publicly available for the broader community.

摘要：基礎模型透過啟用轉移學習來重塑計算病理學，其中預先在龐大資料集上訓練的模型可適應於下游診斷、預後和治療反應任務。儘管有這些進展，基礎模型在編碼整個千兆像素全幻燈片影像的能力上仍有限，且經常缺乏補充多模式資料。在此，我們介紹 Threads，這是一個幻燈片層級基礎模型，能夠產生任何大小的全幻燈片影像的通用表示。Threads 使用多模式學習方法預先訓練，並針對 47,171 個蘇木精和曙紅 (H&E) 染色的組織切片的多元群組進行訓練，並搭配對應的基因體和轉錄組特徵檔，這是迄今為止用於基礎模型開發的最大此類配對資料集。這種獨特的訓練範例使 Threads 能夠擷取組織的基礎分子組成，產生強大的表示，適用於廣泛的下游任務。在涵蓋 54 個腫瘤學任務的廣泛基準測試中，包括臨床分型、分級、突變預測、免疫組織化學狀態判定、治療反應預測和存活預測，Threads 優於所有基準，同時展現出顯著的概括性和標籤效率。它特別適合預測罕見事件，進一步強調其臨床效用。我們打算讓該模型公開，供更廣泛的社群使用。

##### **Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**
2502.00045v1 by Yi Mao, Andrew Perrault

Municipal inspections are an important part of maintaining the quality of
goods and services. In this paper, we approach the problem of intelligently
scheduling service inspections to maximize their impact, using the case of food
establishment inspections in Chicago as a case study. The Chicago Department of
Public Health (CDPH) inspects thousands of establishments each year, with a
substantial fail rate (over 3,000 failed inspection reports in 2023). To
balance the objectives of ensuring adherence to guidelines, minimizing
disruption to establishments, and minimizing inspection costs, CDPH assigns
each establishment an inspection window every year and guarantees that they
will be inspected exactly once during that window. These constraints create a
challenge for a restless multi-armed bandit (RMAB) approach, for which there
are no existing methods. We develop an extension to Whittle index-based systems
for RMABs that can guarantee action window constraints and frequencies, and
furthermore can be leveraged to optimize action window assignments themselves.
Briefly, we combine MDP reformulation and integer programming-based lookahead
to maximize the impact of inspections subject to constraints. A neural
network-based supervised learning model is developed to model state transitions
of real Chicago establishments using public CDPH inspection records, which
demonstrates 10\% AUC improvements compared with directly predicting
establishments' failures. Our experiments not only show up to 24\% (in
simulation) or 33\% (on real data) reward improvements resulting from our
approach but also give insight into the impact of scheduling constraints.

摘要：市政檢查是維持商品和服務品質的重要一環。在本文中，我們以芝加哥的食品機構檢查為案例研究，探討如何透過智慧排程服務檢查以最大化其影響力。芝加哥公共衛生部 (CDPH) 每年檢查數千家機構，且有相當高的不合格率（2023 年有超過 3,000 份不合格檢查報告）。為了平衡確保遵循準則、最大程度減少對機構的干擾和最大程度減少檢查成本的目標，CDPH 每年為每家機構分配一個檢查時段，並保證在該時段內只檢查一次。這些限制對不間斷多臂賭博機 (RMAB) 方法構成挑戰，目前尚無現有方法。我們針對 RMAB 的 Whittle 指數系統開發了一個延伸，可以保證動作時段限制和頻率，此外還可以利用它來優化動作時段分配本身。簡而言之，我們結合 MDP 重新制定和基於整數規劃的前瞻性，以最大化符合限制條件的檢查影響。使用公開的 CDPH 檢查記錄開發了一個基於神經網路的監督式學習模型，用於模擬芝加哥實際機構的狀態轉換，與直接預測機構的失敗相比，顯示出 AUC 提升了 10%。我們的實驗不僅顯示我們的做法帶來的獎勵提升高達 24%（模擬中）或 33%（實際資料中），還深入了解了排程限制的影響。

##### **Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**
2501.16282v1 by Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu

Understanding brain disorders is crucial for accurate clinical diagnosis and
treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a
promising approach to interpreting medical images with the support of text
descriptions. However, previous research has primarily focused on 2D medical
images, leaving richer spatial information of 3D images under-explored, and
single-modality-based methods are limited by overlooking the critical clinical
information contained in other modalities. To address this issue, this paper
proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck
layer to learn new knowledge and instill it into the original pre-trained
knowledge. The major idea is to incorporate a lightweight bottleneck layer to
train fewer parameters while capturing essential information and utilize a
Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal
data within a unified representation space. Extensive experiments demonstrated
the effectiveness of our approach in integrating multimodal data to
significantly improve the diagnosis accuracy without high computational costs,
highlighting the potential to enhance real-world diagnostic workflows.

摘要：了解腦部疾病對於準確的臨床診斷和治療至關重要。多模態大型語言模型 (MLLM) 的最新進展提供了一個有前途的方法，可以在文本描述的支援下詮釋醫學影像。然而，先前的研究主要集中在 2D 醫學影像，忽略了 3D 影像更豐富的空間資訊，而單一模態方法受到忽視其他模態中關鍵臨床資訊的限制。為了解決這個問題，本文提出了 Brain-Adapter，這是一種新的方法，它結合了一個額外的瓶頸層來學習新知識並將其灌輸到原始預訓練的知識中。主要的想法是結合一個輕量級瓶頸層，在擷取必要資訊的同時訓練較少的參數，並利用對比語言影像預訓練 (CLIP) 策略在統一的表示空間中對齊多模態資料。廣泛的實驗證明了我們的方法在整合多模態資料以顯著提高診斷準確性方面的有效性，而不會造成高運算成本，突顯了增強真實世界診斷工作流程的潛力。

##### **Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**
2501.16215v1 by Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li

Large language models (LLMs) exhibit remarkable capabilities in visual
inspection of medical time-series data, achieving proficiency comparable to
human clinicians. However, their broad scope limits domain-specific precision,
and proprietary weights hinder fine-tuning for specialized datasets. In
contrast, small specialized models (SSMs) excel in targeted tasks but lack the
contextual reasoning required for complex clinical decision-making. To address
these challenges, we propose ConMIL (Conformalized Multiple Instance Learning),
a decision-support SSM that integrates seamlessly with LLMs. By using Multiple
Instance Learning (MIL) to identify clinically significant signal segments and
conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'
interpretative capabilities for medical time-series analysis. Experimental
results demonstrate that ConMIL significantly improves the performance of
state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,
\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for
confident samples in arrhythmia detection and sleep staging, compared to
standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the
potential of ConMIL to bridge task-specific precision and broader contextual
reasoning, enabling more reliable and interpretable AI-driven clinical decision
support.

摘要：大型語言模型 (LLM) 在醫療時間序列資料的視覺檢查中展現出非凡的能力，達到了與人類臨床醫生相當的熟練度。然而，它們的廣泛範圍限制了特定領域的精確度，而專有權重阻礙了針對特定資料集的微調。相比之下，小型專用模型 (SSM) 在目標任務中表現出色，但缺乏複雜臨床決策制定所需的背景推理。為了應對這些挑戰，我們提出了 ConMIL（共形多實例學習），這是一個與 LLM 無縫整合的決策支援 SSM。透過使用多實例學習 (MIL) 來識別臨床顯著訊號區段，並對校準的集合值輸出進行共形預測，ConMIL 增強了 LLM 對醫療時間序列分析的解釋能力。實驗結果表明，ConMIL 明顯改善了最先進 LLM 的效能，例如 ChatGPT4.0 和 Qwen2-VL-7B。具體來說，\ConMIL{}- 支援的 Qwen2-VL-7B 在心律不整偵測和睡眠分期中，對於有信心的樣本達到了 94.92% 和 96.82% 的精確度，而獨立 LLM 的準確度僅為 46.13% 和 13.16%。這些發現突顯了 ConMIL 在橋接特定任務的精確度和更廣泛的背景推理方面的潛力，從而實現更可靠且可解釋的 AI 驅動臨床決策支援。

##### **Atla Selene Mini: A General Purpose Evaluation Model**
2501.17195v1 by Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park

We introduce Atla Selene Mini, a state-of-the-art small language
model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that
outperforms the best SLMJs and GPT-4o-mini on overall performance across 11
out-of-distribution benchmarks, spanning absolute scoring, classification, and
pairwise preference tasks. It is the highest-scoring 8B generative model on
RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To
achieve this, we develop a principled data curation strategy that augments
public datasets with synthetically generated critiques and ensures high quality
through filtering and dataset ablations. We train our model on a combined
direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and
produce a highly promptable evaluator that excels in real-world scenarios.
Selene Mini shows dramatically improved zero-shot agreement with human expert
evaluations on financial and medical industry datasets. It is also robust to
variations in prompt format. Preliminary results indicate that Selene Mini is
the top-ranking evaluator in a live, community-driven Judge Arena. We release
the model weights on HuggingFace
(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage
widespread community adoption.

摘要：我們介紹 Atla Selene Mini，這是一個最先進的小型語言模型評審 (SLMJ)。Selene Mini 是一個通用評估器，在 11 個分佈外基準測試（包含絕對評分、分類和成對偏好任務）的整體效能上優於最佳的 SLMJ 和 GPT-4o-mini。它是 RewardBench 上得分最高的 8B 生成模型，超越了 GPT-4o 和專業評審等強大的基準。為了達成此目標，我們開發了一種有原則的資料策展策略，利用合成產生的評論擴充公開資料集，並透過篩選和資料集消融確保高品質。我們在結合直接偏好最佳化 (DPO) 和監督微調 (SFT) 損失的資料集上訓練我們的模型，並產生一個高度可提示的評估器，在實際場景中表現出色。Selene Mini 在財務和醫療產業資料集上顯示出顯著改善的零次學習與人類專家評估的一致性。它也對提示格式的變化具有穩健性。初步結果顯示 Selene Mini 是社群驅動的 Judge Arena 中排名第一的評估器。我們在 HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) 和 Ollama 上釋出模型權重，以鼓勵廣泛的社群採用。

##### **An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**
2501.15969v1 by Shaheer Ahmad Khan, Muhammad Usamah Shahid, Ahmad Abdullah, Ibrahim Hashmat, Muddassar Farooq

This study addresses a critical gap in the healthcare system by developing a
clinically meaningful, practical, and explainable disease surveillance system
for multiple chronic diseases, utilizing routine EHR data from multiple U.S.
practices integrated with CureMD's EMR/EHR system. Unlike traditional
systems--using AI models that rely on features from patients' labs--our
approach focuses on routinely available data, such as medical history, vitals,
diagnoses, and medications, to preemptively assess the risks of chronic
diseases in the next year. We trained three distinct models for each chronic
disease: prediction models that forecast the risk of a disease 3, 6, and 12
months before a potential diagnosis. We developed Random Forest models, which
were internally validated using F1 scores and AUROC as performance metrics and
further evaluated by a panel of expert physicians for clinical relevance based
on inferences grounded in medical knowledge. Additionally, we discuss our
implementation of integrating these models into a practical EMR system. Beyond
using Shapley attributes and surrogate models for explainability, we also
introduce a new rule-engineering framework to enhance the intrinsic
explainability of Random Forests.

摘要：本研究透過開發一個臨床有意義、實用且可解釋的多重慢性疾病疾病監測系統，來解決醫療保健系統中的重大缺口，利用整合 CureMD 的 EMR/EHR 系統，來自多個美國實務的例行 EHR 資料。與傳統系統不同的是，我們的做法著重在例行可得的資料，例如病歷、生命徵象、診斷和藥物，以預先評估未來一年慢性疾病的風險，而非仰賴病患實驗室特徵的 AI 模型。我們針對每種慢性疾病訓練了三個不同的模型：預測模型，用以預測在潛在診斷前 3、6 和 12 個月的疾病風險。我們開發了隨機森林模型，並使用 F1 分數和 AUROC 作為效能指標，進行內部驗證，並進一步由專家醫師小組根據植基於醫學知識的推論，評估其臨床相關性。此外，我們討論了將這些模型整合到實用 EMR 系統中的實作方式。除了使用 Shapley 屬性和代理模型來解釋外，我們還引進了一個新的規則工程架構，以增強隨機森林的內在可解釋性。

##### **Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**
2501.15733v1 by Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan

Alzheimer's disease (AD) is a neurodegenerative disorder affecting millions
worldwide, necessitating early and accurate diagnosis for optimal patient
management. In recent years, advancements in deep learning have shown
remarkable potential in medical image analysis. Methods In this study, we
present "ViTranZheimer," an AD diagnosis approach which leverages video vision
transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as
videos, we exploit the temporal dependencies between slices to capture
intricate structural relationships. The video vision transformer's
self-attention mechanisms enable the model to learn long-range dependencies and
identify subtle patterns that may indicate AD progression. Our proposed deep
learning framework seeks to enhance the accuracy and sensitivity of AD
diagnosis, empowering clinicians with a tool for early detection and
intervention. We validate the performance of the video vision transformer using
the ADNI dataset and conduct comparative analyses with other relevant models.
Results The proposed ViTranZheimer model is compared with two hybrid models,
CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional
neural network (CNN) and a bidirectional long-short-term memory network
(BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT)
with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and
ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer
demonstrated the highest accuracy at 98.6%, outperforming other models in this
evaluation metric, indicating its superior performance in this specific
evaluation metric. Conclusion This research advances the understanding of
applying deep learning techniques in neuroimaging and Alzheimer's disease
research, paving the way for earlier and less invasive clinical diagnosis.

摘要：阿茲海默症 (AD) 是一種神經退化性疾病，影響著全球數百萬人，因此需要早期準確診斷以進行最佳患者管理。近年來，深度學習的進步在醫學影像分析中展現了非凡的潛力。方法在這個研究中，我們提出「ViTranZheimer」，一種 AD 診斷方法，它利用影片視覺轉換器來分析 3D 大腦 MRI 資料。透過將 3D MRI 體積視為影片，我們利用切片之間的時間依賴性來捕捉複雜的結構關係。影片視覺轉換器的自注意力機制使模型能夠學習長程依賴性，並識別可能表示 AD 進展的細微模式。我們提出的深度學習架構旨在提高 AD 診斷的準確性和敏感性，為臨床醫生提供早期檢測和干預的工具。我們使用 ADNI 資料集驗證影片視覺轉換器的效能，並與其他相關模型進行比較分析。結果所提出的 ViTranZheimer 模型與兩個混合模型 CNN-BiLSTM 和 ViT-BiLSTM 進行比較。CNN-BiLSTM 是卷積神經網路 (CNN) 和雙向長短期記憶網路 (BiLSTM) 的組合，而 ViT-BiLSTM 是視覺轉換器 (ViT) 與 BiLSTM 的組合。在 ViTranZheimer、CNN-BiLSTM 和 ViT-BiLSTM 模型中達到的準確度分別為 98.6%、96.479% 和 97.465%。ViTranZheimer 以 98.6% 的準確度表現最佳，在這個評估指標中優於其他模型，顯示出其在這個特定評估指標中的卓越效能。結論這項研究促進了在神經影像和阿茲海默症研究中應用深度學習技術的理解，為更早、更低侵入性的臨床診斷鋪路。

##### **A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**
2501.15724v1 by Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Ajit J. Nirmal, Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao

Computational pathology foundation models (CPathFMs) have emerged as a
powerful approach for analyzing histopathological data, leveraging
self-supervised learning to extract robust feature representations from
unlabeled whole-slide images. These models, categorized into uni-modal and
multi-modal frameworks, have demonstrated promise in automating complex
pathology tasks such as segmentation, classification, and biomarker discovery.
However, the development of CPathFMs presents significant challenges, such as
limited data accessibility, high variability across datasets, the necessity for
domain-specific adaptation, and the lack of standardized evaluation benchmarks.
This survey provides a comprehensive review of CPathFMs in computational
pathology, focusing on datasets, adaptation strategies, and evaluation tasks.
We analyze key techniques, such as contrastive learning and multi-modal
integration, and highlight existing gaps in current research. Finally, we
explore future directions from four perspectives for advancing CPathFMs. This
survey serves as a valuable resource for researchers, clinicians, and AI
practitioners, guiding the advancement of CPathFMs toward robust and clinically
applicable AI-driven pathology solutions.

摘要：計算病理基礎模型 (CPathFM) 已成為分析組織病理學數據的一種強大方法，利用自我監督學習從未標記的全幻燈片影像中提取穩健的特色表徵。這些模型分為單模態和多模態框架，已證明有望自動化複雜的病理任務，例如分割、分類和生物標記發現。然而，CPathFM 的開發提出了重大挑戰，例如數據可及性有限、數據集之間變異性高、需要特定領域的適應性，以及缺乏標準化的評估基準。這項調查對計算病理學中的 CPathFM 進行了全面的回顧，重點關注數據集、適應策略和評估任務。我們分析了對比學習和多模態整合等關鍵技術，並強調了當前研究中存在的差距。最後，我們從四個角度探討了推進 CPathFM 的未來方向。這項調查作為研究人員、臨床醫生和人工智能從業人員的寶貴資源，指導 CPathFM 朝著穩健且臨床上適用的 AI 驅動病理解決方案邁進。

##### **Beyond Benchmarks: On The False Promise of AI Regulation**
2501.15693v1 by Gabriel Stanovsky, Renana Keydar, Gadi Perl, Eliya Habba

The rapid advancement of artificial intelligence (AI) systems in critical
domains like healthcare, justice, and social services has sparked numerous
regulatory initiatives aimed at ensuring their safe deployment. Current
regulatory frameworks, exemplified by recent US and EU efforts, primarily focus
on procedural guidelines while presuming that scientific benchmarking can
effectively validate AI safety, similar to how crash tests verify vehicle
safety or clinical trials validate drug efficacy. However, this approach
fundamentally misunderstands the unique technical challenges posed by modern AI
systems. Through systematic analysis of successful technology regulation case
studies, we demonstrate that effective scientific regulation requires a causal
theory linking observable test outcomes to future performance - for instance,
how a vehicle's crash resistance at one speed predicts its safety at lower
speeds. We show that deep learning models, which learn complex statistical
patterns from training data without explicit causal mechanisms, preclude such
guarantees. This limitation renders traditional regulatory approaches
inadequate for ensuring AI safety. Moving forward, we call for regulators to
reckon with this limitation, and propose a preliminary two-tiered regulatory
framework that acknowledges these constraints: mandating human oversight for
high-risk applications while developing appropriate risk communication
strategies for lower-risk uses. Our findings highlight the urgent need to
reconsider fundamental assumptions in AI regulation and suggest a concrete path
forward for policymakers and researchers.

摘要：人工智慧 (AI) 系統在醫療保健、司法和社會服務等關鍵領域的快速進展，引發了許多法規倡議，旨在確保其安全部署。以最近美國和歐盟的努力為例，當前的法規框架主要關注程序指南，同時假設科學基準測試可以有效驗證 AI 安全性，類似於碰撞測試驗證車輛安全性或臨床試驗驗證藥物功效。然而，這種方法根本誤解了現代 AI 系統帶來的獨特技術挑戰。透過對成功的技術法規案例研究進行系統分析，我們證明有效的科學法規需要一個因果理論，將可觀察的測試結果與未來的效能聯繫起來——例如，車輛在某一速度下的抗撞性如何預測其在較低速度下的安全性。我們表明深度學習模型從訓練資料中學習複雜的統計模式，而沒有明確的因果機制，排除了這樣的保證。這種限制使得傳統的法規方法不足以確保 AI 安全性。展望未來，我們呼籲監管機構正視這一限制，並提出一個初步的兩層法規框架，承認這些約束：對高風險應用強制人工監督，同時為低風險用途制定適當的風險溝通策略。我們的發現強調了重新考慮 AI 法規中基本假設的迫切需要，並為政策制定者和研究人員提出了具體的進展道路。

##### **Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**
2501.15572v1 by Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, Lorenzo Preda

Introduction: Generative Adversarial Networks (GANs) are increasingly used to
generate synthetic medical images, addressing the critical shortage of
annotated data for training Artificial Intelligence (AI) systems. This study
introduces a novel memory-efficient GAN architecture, incorporating Conditional
Random Fields (CRFs) to generate high-resolution 3D medical images and
evaluates its performance against the state-of-the-art hierarchical (HA)-GAN
model.
  Materials and Methods: The CRF-GAN was trained using the open-source lung CT
LUNA16 dataset. The architecture was compared to HA-GAN through a quantitative
evaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy
(MMD) metrics, and a qualitative evaluation, through a two-alternative forced
choice (2AFC) test completed by a pool of 12 resident radiologists, in order to
assess the realism of the generated images.
  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD
(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test
showed a significant preference for images generated by CRF-Gan over those
generated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN
demonstrated 9.34% lower memory usage at 256 resolution and achieved up to
14.6% faster training speeds, offering substantial computational savings.
  Discussion: CRF-GAN model successfully generates high-resolution 3D medical
images with non-inferior quality to conventional models, while being more
memory-efficient and faster. Computational power and time saved can be used to
improve the spatial resolution and anatomical accuracy of generated images,
which is still a critical factor limiting their direct clinical applicability.

摘要：<paragraph>引言：生成對抗網路 (GAN) 愈來愈常被用於生成合成醫學影像，以解決人工智慧 (AI) 系統訓練中標註資料嚴重短缺的問題。本研究提出了一種新穎的記憶體高效 GAN 架構，結合條件隨機場 (CRF) 來生成高解析度 3D 醫學影像，並評估其相對於最先進的分層 (HA)-GAN 模型的效能。
材料和方法：CRF-GAN 使用開源肺部電腦斷層掃描 LUNA16 資料集進行訓練。透過使用 Fréchet Inception Distance (FID) 和 Maximum Mean Discrepancy (MMD) 指標進行量化評估，以及由 12 位住院放射科醫師組成的團隊完成的兩擇一強制選擇 (2AFC) 測試進行定性評估，將該架構與 HA-GAN 進行比較，以評估生成影像的真實性。
結果：CRF-GAN 以較低的 FID (0.047 對 0.061) 和 MMD (0.084 對 0.086) 分數優於 HA-GAN，表示影像保真度較佳。2AFC 測試顯示，受試者顯著偏好由 CRF-GAN 生成的影像，而非由 HA-GAN 生成的影像，p 值為 1.93e-05。此外，CRF-GAN 在解析度為 256 時的記憶體使用量降低了 9.34%，並將訓練速度提升了 14.6%，提供了大量的運算節省。
討論：CRF-GAN 模型成功地生成了高解析度 3D 醫學影像，其品質不遜於傳統模型，同時更省記憶體且速度更快。節省的運算能力和時間可用於提升生成影像的空間解析度和解剖精確度，這仍然是限制其直接臨床應用性的關鍵因素。</paragraph>

##### **AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**
2501.15489v1 by Muhammad Aftab, Faisal Mehmood, Chengjuan Zhang, Alishba Nadeem, Zigang Dong, Yanan Jiang, Kangdongs Liu

Artificial intelligence (AI) has potential to revolutionize the field of
oncology by enhancing the precision of cancer diagnosis, optimizing treatment
strategies, and personalizing therapies for a variety of cancers. This review
examines the limitations of conventional diagnostic techniques and explores the
transformative role of AI in diagnosing and treating cancers such as lung,
breast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate,
and skin cancers. The primary objective of this paper is to highlight the
significant advancements that AI algorithms have brought to oncology within the
medical industry. By enabling early cancer detection, improving diagnostic
accuracy, and facilitating targeted treatment delivery, AI contributes to
substantial improvements in patient outcomes. The integration of AI in medical
imaging, genomic analysis, and pathology enhances diagnostic precision and
introduces a novel, less invasive approach to cancer screening. This not only
boosts the effectiveness of medical facilities but also reduces operational
costs. The study delves into the application of AI in radiomics for detailed
cancer characterization, predictive analytics for identifying associated risks,
and the development of algorithm-driven robots for immediate diagnosis.
Furthermore, it investigates the impact of AI on addressing healthcare
challenges, particularly in underserved and remote regions. The overarching
goal of this platform is to support the development of expert recommendations
and to provide universal, efficient diagnostic procedures. By reviewing
existing research and clinical studies, this paper underscores the pivotal role
of AI in improving the overall cancer care system. It emphasizes how AI-enabled
systems can enhance clinical decision-making and expand treatment options,
thereby underscoring the importance of AI in advancing precision oncology

摘要：人工智能（AI）有潛力透過提升癌症診斷的準確性、優化治療策略，以及為各種癌症提供個人化療法，來徹底改變腫瘤學領域。本篇評論探討傳統診斷技術的限制，並探討 AI 在診斷和治療肺癌、乳癌、大腸直腸癌、肝癌、胃癌、食道癌、子宮頸癌、甲狀腺癌、攝護腺癌和皮膚癌等癌症中轉變性的角色。本文的主要目的是強調 AI 演算法在醫療產業腫瘤學領域帶來的重大進展。透過實現早期癌症偵測、提升診斷準確性，以及促進標靶治療的提供，AI 有助於大幅改善病患的治療結果。AI 整合在醫學影像、基因體分析和病理學中，提升了診斷的準確性，並引進了一種創新、侵入性較低的方法來進行癌症篩檢。這不僅提升了醫療機構的效率，也降低了營運成本。本研究探討了 AI 在放射線組學中的應用，以進行詳細的癌症特徵分析、預測分析以找出相關風險，以及開發演算法驅動的機器人進行立即診斷。此外，它還探討了 AI 在解決醫療保健挑戰中的影響，特別是在服務不足和偏遠地區。此平台的總體目標是支援專家建議的發展，並提供普遍、有效的診斷程序。透過檢視現有的研究和臨床試驗，本文強調了 AI 在改善整體癌症照護系統中的關鍵作用。它強調了 AI 驅動的系統如何能提升臨床決策制定，並擴展治療選項，從而強調了 AI 在推進精準腫瘤學中的重要性。

##### **Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**
2501.15452v1 by Solha Kang, Joris Vankerschaver, Utku Ozbulak

With the advancements in self-supervised learning (SSL), transformer-based
computer vision models have recently demonstrated superior results compared to
convolutional neural networks (CNNs) and are poised to dominate the field of
artificial intelligence (AI)-based medical imaging in the upcoming years.
Nevertheless, similar to CNNs, unveiling the decision-making process of
transformer-based models remains a challenge. In this work, we take a step
towards demystifying the decision-making process of transformer-based medical
imaging models and propose Token Insight, a novel method that identifies the
critical tokens that contribute to the prediction made by the model. Our method
relies on the principled approach of token discarding native to
transformer-based models, requires no additional module, and can be applied to
any transformer model. Using the proposed approach, we quantify the importance
of each token based on its contribution to the prediction and enable a more
nuanced understanding of the model's decisions. Our experimental results which
are showcased on the problem of colonic polyp identification using both
supervised and self-supervised pretrained vision transformers indicate that
Token Insight contributes to a more transparent and interpretable
transformer-based medical imaging model, fostering trust and facilitating
broader adoption in clinical settings.

摘要：隨著自監督學習 (SSL) 的進展，基於 Transformer 的電腦視覺模型最近已展示出優於卷積神經網路 (CNN) 的卓越成果，並準備在未來幾年主導基於人工智慧 (AI) 的醫學影像領域。儘管如此，與 CNN 類似，揭示基於 Transformer 的模型的決策過程仍然是一項挑戰。在這項工作中，我們朝著揭開基於 Transformer 的醫學影像模型的決策過程邁出了一步，並提出了 Token Insight，這是一種新穎的方法，可以識別對模型所做的預測有貢獻的關鍵 Token。我們的模型依賴於基於 Transformer 的模型的原生 Token 捨棄原理化方法，不需要額外的模組，並且可以應用於任何 Transformer 模型。使用所提出的方法，我們根據每個 Token 對預測的貢獻量化其重要性，並能更細緻地了解模型的決策。我們的實驗結果展示在使用有監督和自監督預訓練視覺 Transformer 的結腸息肉識別問題上，表明 Token Insight 有助於建立更透明且可解釋的基於 Transformer 的醫學影像模型，培養信任並促進在臨床環境中的更廣泛採用。

##### **An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**
2501.17181v1 by Arya Rahgozar, Pouria Mortezaagha, Jodi Edwards, Douglas Manuel, Jessie McGowen, Merrick Zwarenstein, Dean Fergusson, Andrea Tricco, Kelly Cobey, Margaret Sampson, Malcolm King, Dawn Richards, Alexandra Bodnaruc, David Moher

The Brain-Heart Interconnectome (BHI) combines neurology and cardiology but
is hindered by inefficiencies in evidence synthesis, poor adherence to quality
standards, and research waste. To address these challenges, we developed an
AI-driven system to enhance systematic reviews in the BHI domain. The system
integrates automated detection of Population, Intervention, Comparator,
Outcome, and Study design (PICOS), semantic search using vector embeddings,
graph-based querying, and topic modeling to identify redundancies and
underexplored areas. Core components include a Bi-LSTM model achieving 87%
accuracy for PICOS compliance, a study design classifier with 95.7% accuracy,
and Retrieval-Augmented Generation (RAG) with GPT-3.5, which outperformed GPT-4
for graph-based and topic-driven queries. The system provides real-time
updates, reducing research waste through a living database and offering an
interactive interface with dashboards and conversational AI. While initially
developed for BHI, the system's adaptable architecture enables its application
across various biomedical fields, supporting rigorous evidence synthesis,
efficient resource allocation, and informed clinical decision-making.

摘要：腦心交互組學 (BHI) 結合神經學和心臟學，但受到證據合成效率低、對品質標準的遵守度不佳和研究浪費的阻礙。為了應對這些挑戰，我們開發了一個 AI 驅動系統，以增強 BHI 領域中的系統性回顧。此系統整合了人口、干預、比較、結果和研究設計 (PICOS) 的自動檢測，使用向量嵌入的語義搜尋、基於圖形查詢和主題建模，以識別冗餘和未充分探索的領域。核心組成包括一個雙向 LSTM 模型，PICOS 合規性達到 87% 的準確度，研究設計分類器準確度為 95.7%，以及使用 GPT-3.5 的檢索增強生成 (RAG)，其在基於圖形和主題驅動的查詢方面優於 GPT-4。該系統提供即時更新，透過動態資料庫減少研究浪費，並提供具有儀表板和對話式 AI 的互動介面。儘管最初是為 BHI 開發，但該系統的可適應架構使其能夠應用於各種生物醫學領域，支援嚴謹的證據合成、有效的資源配置和明智的臨床決策制定。

##### **Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**
2501.15056v1 by Harshita Chopra, Chirag Shah

The ability to identify and acquire missing information is a critical
component of effective decision making and problem solving. With the rise of
conversational artificial intelligence (AI) systems, strategically formulating
information-seeking questions becomes crucial and demands efficient methods to
guide the search process. We introduce a novel approach to adaptive
question-asking through a combination of Large Language Models (LLM) for
generating questions that maximize information gain, Monte Carlo Tree Search
(MCTS) for constructing and leveraging a decision tree across multiple samples,
and a hierarchical feedback mechanism to learn from past interactions. We
present two key innovations: (1) an adaptive MCTS algorithm that balances
exploration and exploitation for efficient search over potential questions; and
(2) a clustering-based feedback algorithm that leverages prior experience to
guide future interactions. Each incoming sample is assigned to a cluster based
on its semantic similarity with previously observed samples. Our UCT (Upper
Confidence bound for Trees) formulation selects optimal questions by combining
expected rewards, a function of information gain, with a cluster-specific bonus
that decays with depth, to emphasize the importance of early-stage questions
that have proven effective for narrowing the solution space in similar samples.
Experiments across three domains, including medical diagnosis and
troubleshooting, demonstrate that our method leads to an average of 12%
improvement in success rates and a 10x reduction in the average number of LLM
calls made per conversation for the search process, in comparison to the state
of the art.

摘要：具備辨識與取得遺漏資訊的能力是有效決策和問題解決的關鍵組成部分。隨著對話式人工智慧 (AI) 系統的崛起，策略性地擬定尋求資訊的問題變得至關重要，並需要有效率的方法來引導搜尋流程。我們透過結合大型語言模型 (LLM) 來產生最大化資訊獲取的問題、蒙地卡羅樹狀搜尋 (MCTS) 來建構並利用多個樣本的決策樹，以及分層回饋機制來從過去的互動中學習，提出了一種自適應提問的新方法。我們提出了兩項關鍵創新：(1) 一種自適應 MCTS 演算法，用於平衡探索和利用，以有效搜尋潛在問題；以及 (2) 一種基於聚類的回饋演算法，用於利用先前的經驗來引導未來的互動。每個輸入樣本會根據其與先前觀察到的樣本的語義相似性分配到一個群集。我們的 UCT (樹狀結構的上置信界) 公式透過結合預期的回饋（資訊獲取函數）和隨著深度衰減的特定群集加成，來選擇最佳問題，以強調早期階段問題對於縮小類似樣本中的解空間的重要性。在包括醫療診斷和故障排除在內的三個領域中的實驗證明，與現有技術相比，我們的技術可將成功率平均提升 12%，並將搜尋流程中每次對話所進行的 LLM 呼叫平均次數減少 10 倍。

##### **Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**
2501.14929v1 by Md. Kamrul Hasan, Guang Yang, Choon Hwai Yap

Cardiac anatomy segmentation is essential for clinical assessment of cardiac
function and disease diagnosis to inform treatment and intervention. In
performing segmentation, deep learning (DL) algorithms improved accuracy
significantly compared to traditional image processing approaches. More
recently, studies showed that enhancing DL segmentation with motion information
can further improve it. A range of methods for injecting motion information has
been proposed, but many of them increase the dimensionality of input images
(which is computationally expensive) or have not used an optimal method to
insert motion information, such as non-DL registration, non-attention-based
networks or single-headed attention. Here, we present a novel,
computation-efficient alternative where a novel, scalable temporal attention
module (TAM) extracts temporal feature interactions multiple times and where
TAM has a multi-headed, KQV projection cross-attention architecture. The module
can be seamlessly integrated into a wide range of existing CNN- or
Transformer-based networks, providing novel flexibility for inclusion in future
implementations. Extensive evaluations on different cardiac datasets, 2D
echocardiography (CAMUS), and 3D echocardiography (MITEA) demonstrate the
model's effectiveness when integrated into well-established backbone networks
like UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that
the optimized TAM-enhanced FCN8s network performs well compared to contemporary
alternatives. Our results confirm TAM's robustness, scalability, and
generalizability across diverse datasets and backbones.

摘要：<paragraph>心臟解剖分割對於評估心臟功能和疾病診斷以提供治療和介入資訊至關重要。在執行分割時，深度學習 (DL) 演算法與傳統影像處理方法相比，準確度顯著提升。最近的研究顯示，透過動作資訊強化 DL 分割可以進一步提升分割效果。已經提出許多用於注入動作資訊的方法，但其中許多方法會增加輸入影像的維度（在運算上很耗費成本），或未使用最佳方法來插入動作資訊，例如非 DL 配準、非基於注意力的網路或單頭注意力。在此，我們提出一個新穎且運算效率高的替代方案，其中一個新穎且可擴充的時序注意力模組 (TAM) 多次提取時序特徵互動，且 TAM 具有多頭、KQV 投影交叉注意力的架構。此模組可以無縫整合到各種現有的基於 CNN 或 Transformer 的網路中，為未來實作的納入提供新穎的靈活性。在不同的心臟資料集、2D 超音波心動圖 (CAMUS) 和 3D 超音波心動圖 (MITEA) 上進行的廣泛評估證明了此模型在整合到 UNet、FCN8s、UNetR、SwinUNetR 和最近的 I2UNet 等完善的主幹網路中的效能。我們進一步發現，經過最佳化的 TAM 增強 FCN8s 網路與當代替代方案相比表現良好。我們的結果證實了 TAM 在不同資料集和主幹中的穩健性、可擴充性和泛化性。</paragraph>

##### **Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**
2501.14892v1 by Hang Luo, Jian Zhang, Chujun Li

In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.

摘要：在知識密集型任務中，特別是在醫學和法律等高風險領域，不僅檢索相關資訊至關重要，還必須提供因果推理和可解釋性。大型語言模型 (LLM) 在自然語言理解和生成任務中取得了顯著的表現。然而，它們通常會遇到一些限制，例如難以納入新知識、產生幻覺，以及解釋其推理過程。為了應對這些挑戰，將知識圖與圖形檢索增強生成 (Graph RAG) 整合在一起已成為一種有效的解決方案。傳統的 Graph RAG 方法通常依賴於簡單的圖形遍歷或語義相似性，這無法捕捉因果關係或與模型的內部推理步驟很好地對齊。本文提出了一個新穎的管道，該管道過濾大型知識圖以強調因果邊緣，將檢索過程與模型的思想鏈 (CoT) 對齊，並通過多階段路徑改進來增強推理。在醫療問題解答任務上的實驗顯示出一致的收益，在多個大型語言模型 (LLM) 中絕對改進幅度高達 10%。這種方法展示了將因果推理與逐步檢索相結合的價值，從而為複雜查詢提供更具可解釋性和邏輯依據的解決方案。

##### **Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**
2501.14719v1 by Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso

Equitable access to reliable health information is vital for public health,
but the quality of online health resources varies by language, raising concerns
about inconsistencies in Large Language Models (LLMs) for healthcare. In this
study, we examine the consistency of responses provided by LLMs to
health-related questions across English, German, Turkish, and Chinese. We
largely expand the HealthFC dataset by categorizing health-related questions by
disease type and broadening its multilingual scope with Turkish and Chinese
translations. We reveal significant inconsistencies in responses that could
spread healthcare misinformation. Our main contributions are 1) a multilingual
health-related inquiry dataset with meta-information on disease categories, and
2) a novel prompt-based evaluation workflow that enables sub-dimensional
comparisons between two languages through parsing. Our findings highlight key
challenges in deploying LLM-based tools in multilingual contexts and emphasize
the need for improved cross-lingual alignment to ensure accurate and equitable
healthcare information.

摘要：可靠的健康資訊的公平取得對公共衛生至關重要，
但網路健康資源的品質因語言而異，這引發了對大型語言模型 (LLM) 在醫療保健方面的不一致性的擔憂。在這項研究中，我們探討了 LLM 對英語、德語、土耳其語和中文的健康相關問題所提供回應的一致性。我們透過依疾病類型分類健康相關問題，並透過土耳其語和中文翻譯擴展其多語言範圍，大幅擴展了 HealthFC 資料集。我們揭露了回應中存在顯著的不一致性，這可能會散布醫療保健錯誤資訊。我們的貢獻主要有 1) 一個包含疾病類別元資訊的多語言健康相關查詢資料集，以及 2) 一個新穎的提示式評估工作流程，它能透過解析在兩種語言之間進行次維度比較。我們的研究結果突顯了在多語言環境中部署基於 LLM 的工具的主要挑戰，並強調需要改善跨語言對齊以確保準確且公平的醫療保健資訊。

##### **GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**
2501.16382v1 by Ziwen Li, Xiang 'Anthony' Chen, Youngseung Jeon

Drug discovery (DD) has tremendously contributed to maintaining and improving
public health. Hypothesizing that inhibiting protein misfolding can slow
disease progression, researchers focus on target identification (Target ID) to
find protein structures for drug binding. While Large Language Models (LLMs)
and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug
discovery, integrating models into cohesive workflows remains challenging. We
conducted a user study with drug discovery researchers to identify the
applicability of LLMs and RAGs in Target ID. We identified two main findings:
1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on
an initial protein and protein candidates that have a therapeutic impact; 2)
the model must provide the PPI and relevant explanations for better
understanding. Based on these observations, we identified three limitations in
previous approaches for Target ID: 1) semantic ambiguity, 2) lack of
explainability, and 3) short retrieval units. To address these issues, we
propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve
agent pipeline RAG framework to support large-scale PPI signaling pathway
exploration in understanding therapeutic impacts by decomposing the analysis of
entire PPI pathways into sub-tasks focused on the analysis of PPI edges.

摘要：药物发现 (DD) 极大地促进了公共卫生的维护和改善。研究人员假设抑制蛋白质错误折叠可以减缓疾病进展，因此专注于靶点识别 (Target ID) 以找到用于药物结合的蛋白质结构。虽然大型语言模型 (LLM) 和检索增强生成 (RAG) 框架加速了药物发现，但将模型整合到内聚工作流中仍然具有挑战性。我们与药物发现研究人员进行了一项用户研究，以确定 LLM 和 RAG 在 Target ID 中的适用性。我们确定了两个主要发现：1) LLM 应该基于初始蛋白质和具有治疗作用的蛋白质候选物提供多个蛋白质-蛋白质相互作用 (PPI)；2) 该模型必须提供 PPI 和相关解释以更好地理解。基于这些观察，我们发现了先前 Target ID 方法中的三个局限性：1) 语义歧义，2) 缺乏可解释性，3) 检索单元短。为了解决这些问题，我们提出了 GraPPI，这是一种基于大规模知识图 (KG) 的检索-分解-求解代理管道 RAG 框架，以支持大规模 PPI 信号通路探索，通过将整个 PPI 通路的分析分解为专注于 PPI 边缘分析的子任务来理解治疗影响。

##### **Rethinking Table Instruction Tuning**
2501.14693v1 by Naihao Deng, Rada Mihalcea

Recent advances in table understanding have focused on instruction-tuning
large language models (LLMs) for table-related tasks. However, existing
research has overlooked the impact of hyperparameter choices and lacks a
comprehensive evaluation of the out-of-domain table understanding ability and
the general capabilities of these table LLMs. In this paper, we evaluate these
abilities in existing table LLMs, and reveal significant declines in both
out-of-domain table understanding and general capabilities compared to their
base models. Through systematic analysis, we show that hyperparameters, such as
learning rate, can significantly influence both table-specific and general
capabilities. Contrary to the existing table instruction-tuning works, we
demonstrate that smaller learning rates and fewer training instances can
enhance table understanding while preserving general capabilities. Based on our
findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B
Instruct, which achieves performance on par with, or surpassing GPT-3.5 and
GPT-4 on table tasks, while maintaining strong out-of-domain generalization and
general capabilities. Our findings highlight the potential for reduced data
annotation costs and more efficient model development through careful
hyperparameter selection.

摘要：最近表理解的進展集中在指令調校大型語言模型 (LLM) 以執行與表格相關的任務。然而，現有的研究忽略了超參數選擇的影響，並且缺乏對領域外表格理解能力和這些表格 LLM 的一般能力的全面評估。在本文中，我們評估了現有表格 LLM 中的這些能力，並揭示了與其基礎模型相比，領域外表格理解和一般能力都有顯著下降。透過系統分析，我們表明超參數（例如學習率）可以顯著影響特定表格和一般能力。與現有表格指令調校工作相反，我們證明較小的學習率和較少的訓練實例可以在保留一般能力的同時增強表格理解。根據我們的發現，我們引入了 TAMA，這是一個從 LLaMA 3.1 8B Instruct 調校的表格 LLM，它在表格任務上實現了與 GPT-3.5 和 GPT-4 相當或超越的效能，同時保持強大的領域外概化和一般能力。我們的發現強調了透過仔細選擇超參數，降低資料標註成本和更有效率的模型開發的可能性。

##### **Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**
2501.14689v1 by Dmitry Ryabtsev, Boris Vasilyev, Sergey Shershakov

This paper introduces an innovative software system for fundus image analysis
that deliberately diverges from the conventional screening approach, opting not
to predict specific diagnoses. Instead, our methodology mimics the diagnostic
process by thoroughly analyzing both normal and pathological features of fundus
structures, leaving the ultimate decision-making authority in the hands of
healthcare professionals. Our initiative addresses the need for objective
clinical analysis and seeks to automate and enhance the clinical workflow of
fundus image examination. The system, from its overarching architecture to the
modular analysis design powered by artificial intelligence (AI) models, aligns
seamlessly with ophthalmological practices. Our unique approach utilizes a
combination of state-of-the-art deep learning methods and traditional computer
vision algorithms to provide a comprehensive and nuanced analysis of fundus
structures. We present a distinctive methodology for designing medical
applications, using our system as an illustrative example. Comprehensive
verification and validation results demonstrate the efficacy of our approach in
revolutionizing fundus image analysis, with potential applications across
various medical domains.

摘要：本論文介紹了一種創新的軟體系統，用於眼底影像分析，它刻意偏離傳統的篩檢方法，選擇不預測具體的診斷。相反地，我們的分析方法模擬診斷過程，徹底分析眼底結構的正常和病理特徵，將最終的決策權交到醫療保健專業人員手中。我們的計畫旨在滿足客觀臨床分析的需求，並尋求自動化和強化眼底影像檢查的臨床工作流程。該系統從其整體架構到由人工智慧 (AI) 模型驅動的模組化分析設計，都與眼科實務無縫對齊。我們獨特的方法結合了最先進的深度學習方法和傳統的電腦視覺演算法，提供眼底結構的全面且細緻的分析。我們提出了一種獨特的設計醫療應用方法，並以我們的系統作為說明範例。全面的驗證和驗證結果證明了我們的方法在革新眼底影像分析方面的效力，並具有在各種醫療領域的潛在應用。

##### **Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**
2501.14685v1 by Fuping Wu, Bartlomiej W. Papiez

Foundation models are widely employed in medical image analysis, due to their
high adaptability and generalizability for downstream tasks. With the
increasing number of foundation models being released, model selection has
become an important issue. In this work, we study the capabilities of
foundation models in medical image classification tasks by conducting a
benchmark study on the MedMNIST dataset. Specifically, we adopt various
foundation models ranging from convolutional to Transformer-based models and
implement both end-to-end training and linear probing for all classification
tasks. The results demonstrate the significant potential of these pre-trained
models when transferred for medical image classification. We further conduct
experiments with different image sizes and various sizes of training data. By
analyzing all the results, we provide preliminary, yet useful insights and
conclusions on this topic.

摘要：基礎模型廣泛用於醫學影像分析，因為它們對下游任務具有高度的適應性和概括性。隨著發布的基礎模型數量越來越多，模型選擇已成為一個重要問題。在這項工作中，我們通過對 MedMNIST 資料集進行基準研究來研究基礎模型在醫學影像分類任務中的能力。具體來說，我們採用了從卷積到基於 Transformer 的模型等各種基礎模型，並對所有分類任務實施端到端訓練和線性探測。結果證明了這些預訓練模型在轉移到醫學影像分類時具有顯著的潛力。我們進一步進行了不同影像大小和各種訓練資料大小的實驗。通過分析所有結果，我們對此主題提供了初步但有用的見解和結論。

##### **MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**
2501.14654v1 by Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen

Recent large language models (LLMs) have demonstrated significant
advancements, particularly in their ability to serve as agents thereby
surpassing their traditional role as chatbots. These agents can leverage their
planning and tool utilization capabilities to address tasks specified at a high
level. However, a standardized dataset to benchmark the agent capabilities of
LLMs in medical applications is currently lacking, making the evaluation of
LLMs on complex tasks in interactive healthcare environments challenging. To
address this gap, we introduce MedAgentBench, a broad evaluation suite designed
to assess the agent capabilities of large language models within medical
records contexts. MedAgentBench encompasses 100 patient-specific
clinically-derived tasks from 10 categories written by human physicians,
realistic profiles of 100 patients with over 700,000 data elements, a
FHIR-compliant interactive environment, and an accompanying codebase. The
environment uses the standard APIs and communication infrastructure used in
modern EMR systems, so it can be easily migrated into live EMR systems.
MedAgentBench presents an unsaturated agent-oriented benchmark that current
state-of-the-art LLMs exhibit some ability to succeed at. The best model
(GPT-4o) achieves a success rate of 72%. However, there is still substantial
space for improvement to give the community a next direction to optimize.
Furthermore, there is significant variation in performance across task
categories. MedAgentBench establishes this and is publicly available at
https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable
framework for model developers to track progress and drive continuous
improvements in the agent capabilities of large language models within the
medical domain.

摘要：<paragraph>最近的大型语言模型 (LLM) 已展示出显著的进步，特别是在其作为代理的能力方面，从而超越了其作为聊天机器人的传统角色。这些代理可以利用其规划和工具利用能力来解决在高层指定的任务。然而，目前缺乏用于对医疗应用中 LLM 的代理能力进行基准测试的标准化数据集，这使得在交互式医疗保健环境中对 LLM 在复杂任务上的评估具有挑战性。为了解决这一差距，我们引入了 MedAgentBench，这是一个广泛的评估套件，旨在评估大型语言模型在医疗记录背景下的代理能力。MedAgentBench 包含 100 个由人类医生编写的来自 10 个类别的特定于患者的临床任务、100 个患者的真实个人资料（包含超过 700,000 个数据元素）、一个符合 FHIR 的交互式环境以及一个配套的代码库。该环境使用现代 EMR 系统中使用的标准 API 和通信基础设施，因此可以轻松地迁移到实时 EMR 系统中。MedAgentBench 呈现了一个未饱和的以代理为导向的基准，当前最先进的 LLM 表现出一定程度的成功能力。最好的模型 (GPT-4o) 的成功率达到 72%。然而，仍然有很大的改进空间，可以为社区提供优化方向。此外，不同任务类别之间的性能差异很大。MedAgentBench 建立了这一点，并在 https://github.com/stanfordmlgroup/MedAgentBench 公开提供，为模型开发者提供了一个有价值的框架，用于跟踪进度并推动大型语言模型在医疗领域的代理能力的持续改进。</paragraph>

##### **Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**
2501.18614v1 by Xu Chen, Yuan Huang, Benn Jessney, Jason Sangha, Sophie Gu, Carola-Bibiane Schönlieb, Martin Bennett, Michael Roberts

Artificial intelligence (AI) methodologies hold great promise for the rapid
and accurate diagnosis of coronary artery disease (CAD) from intravascular
optical coherent tomography (IVOCT) images. Numerous papers have been published
describing AI-based models for different diagnostic tasks, yet it remains
unclear which models have potential clinical utility and have been properly
validated. This systematic review considered published literature between
January 2015 and February 2023 describing AI-based diagnosis of CAD using
IVOCT. Our search identified 5,576 studies, with 513 included after initial
screening and 35 studies included in the final systematic review after quality
screening. Our findings indicate that most of the identified models are not
currently suitable for clinical use, primarily due to methodological flaws and
underlying biases. To address these issues, we provide recommendations to
improve model quality and research practices to enhance the development of
clinically useful AI products.

摘要：人工智慧 (AI) 方法論對於從血管內光學相干斷層掃描 (IVOCT) 影像快速且準確診斷冠狀動脈疾病 (CAD) 而言，具有很大的前景。許多論文已發表，說明了用於不同診斷任務的 AI 基礎模型，但仍不清楚哪些模型具有潛在的臨床用途，且已獲得適當驗證。這項系統性回顧考量了 2015 年 1 月至 2023 年 2 月間發表的文獻，說明了使用 IVOCT 的 CAD AI 基礎診斷。我們的搜尋辨識出 5,576 項研究，在初步篩選後納入 513 項，在品質篩選後納入 35 項研究進行最終的系統性回顧。我們的發現顯示，大多數已辨識出的模型目前不適合用於臨床，主要是因為方法論上的缺陷和潛在偏差。為了解決這些問題，我們提供建議來改善模型品質和研究實務，以增強臨床有用 AI 產品的開發。

##### **Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**
2501.14483v1 by Walid Yassine, Martin Charachon, Céline Hudelot, Roberto Ardon

Assessing cancer progression in liver CT scans is a clinical challenge,
requiring a comparison of scans at different times for the same patient.
Practitioners must identify existing tumors, compare them with prior exams,
identify new tumors, and evaluate overall disease evolution. This process is
particularly complex in liver examinations due to misalignment between exams
caused by several factors. Indeed, longitudinal liver examinations can undergo
different non-pathological and pathological changes due to non-rigid
deformations, the appearance or disappearance of pathologies, and other
variations. In such cases, existing registration approaches, mainly based on
intrinsic features may distort tumor regions, biasing the tumor progress
evaluation step and the corresponding diagnosis. This work proposes a
registration method based only on geometrical and anatomical information from
liver segmentation, aimed at aligning longitudinal liver images for aided
diagnosis. The proposed method is trained and tested on longitudinal liver CT
scans, with 317 patients for training and 53 for testing. Our experimental
results support our claims by showing that our method is better than other
registration techniques by providing a smoother deformation while preserving
the tumor burden (total volume of tissues considered as tumor) within the
volume. Qualitative results emphasize the importance of smooth deformations in
preserving tumor appearance.

摘要：評估肝臟電腦斷層掃描中的癌症進程是一項臨床上的挑戰，
需要比較同一病患在不同時間點的掃描結果。
從業人員必須辨識現有的腫瘤，將其與先前的檢查結果進行比較，
辨識新的腫瘤，並評估整體疾病的演變。由於種種因素造成檢查結果之間的錯位，這個過程在肝臟檢查中特別複雜。事實上，縱向的肝臟檢查可能會因為非剛性變形、病理的出現或消失，以及其他變化而產生不同的非病理性和病理性的變化。在這種情況下，現有的配準方法（主要基於內在特徵）可能會扭曲腫瘤區域，造成腫瘤進程評估步驟和相應診斷的偏差。本研究提出了一種僅基於肝臟分割的幾何和解剖資訊的配準方法，旨在對縱向肝臟影像進行配準，以協助診斷。所提出的方法在縱向肝臟電腦斷層掃描上進行訓練和測試，訓練資料有 317 位病患，測試資料有 53 位。我們的實驗結果支持我們的說法，證明我們的配準方法比其他配準技術更好，因為它在保留腫瘤負擔（被視為腫瘤的組織總體積）的同時，提供了更平滑的變形。定性結果強調了平滑變形在保留腫瘤外觀方面的重要性。

##### **Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**
2501.14469v1 by Taehan Kim, Wonduk Seo

Global climate change has reduced crop resilience and pesticide efficacy,
making reliance on synthetic pesticides inevitable, even though their
widespread use poses significant health and environmental risks. While these
pesticides remain a key tool in pest management, previous machine-learning
applications in pesticide and agriculture have focused on classification or
regression, leaving the fundamental challenge of generating new molecular
structures or designing novel candidates unaddressed. In this paper, we propose
Pesti-Gen, a novel generative model based on variational auto-encoders,
designed to create pesticide candidates with optimized properties for the first
time. Specifically, Pesti-Gen leverages a two-stage learning process: an
initial pre-training phase that captures a generalized chemical structure
representation, followed by a fine-tuning stage that incorporates
toxicity-specific information. The model simultaneously optimizes over multiple
toxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to
generate environmentally friendly pesticide candidates. Notably, Pesti-Gen
achieves approximately 68\% structural validity in generating new molecular
structures, demonstrating the model's effectiveness in producing optimized and
feasible pesticide candidates, thereby providing a new way for safer and more
sustainable pest management solutions.

摘要：全球氣候變遷降低了作物的復原力與殺蟲劑的效力，
使得仰賴合成殺蟲劑成為無可避免的趨勢，儘管它們的廣泛使用會帶來重大的健康和環境風險。儘管這些殺蟲劑仍然是蟲害管理中的關鍵工具，過去在殺蟲劑和農業方面的機器學習應用都著重於分類或迴歸，而未解決產生新的分子結構或設計新候選藥劑的基本挑戰。在本文中，我們提出 Pesti-Gen，一種基於變異自動編碼器的創新生成模型，旨在首次建立具有最佳化特性的殺蟲劑候選藥劑。具體來說，Pesti-Gen 採用兩階段學習流程：一個擷取廣義化學結構表示的初始預訓練階段，接著是一個納入毒性特定資訊的微調階段。此模型同時針對多種毒性指標進行最佳化，例如 (1) 牲畜毒性和 (2) 水生毒性，以產生對環境友善的殺蟲劑候選藥劑。值得注意的是，Pesti-Gen 在產生新的分子結構方面達到了約 68% 的結構效度，證明了此模型在產生最佳化且可行的殺蟲劑候選藥劑方面的效能，進而為更安全且更永續的蟲害管理解決方案提供了一種新方法。

##### **ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**
2501.14379v1 by Yoni Schirris, Rosie Voorthuis, Mark Opdam, Marte Liefaard, Gabe S Sonke, Gwen Dackus, Vincent de Jong, Yuwei Wang, Annelot Van Rossum, Tessa G Steenbruggen, Lars C Steggink, Liesbeth G. E. de Vries, Marc van de Vijver, Roberto Salgado, Efstratios Gavves, Paul J van Diest, Sabine C Linn, Jonas Teuwen, Renee Menezes, Marleen Kok, Hugo Horlings

The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor
for patients with (triple-negative) breast cancer (BC). Computational TIL
assessment (CTA) has the potential to assist pathologists in this
labour-intensive task, but current CTA models rely heavily on many detailed
annotations. We propose and validate a fundamentally simpler deep learning
based CTA that can be trained in only ten minutes on hundredfold fewer
pathologist annotations. We collected whole slide images (WSIs) with TILs
scores and clinical data of 2,340 patients with BC from six cohorts including
three randomised clinical trials. Morphological features were extracted from
whole slide images (WSIs) using a pathology foundation model. Our
label-efficient Computational stromal TIL assessment model (ECTIL) directly
regresses the TILs score from these features. ECTIL trained on only a few
hundred samples (ECTIL-TCGA) showed concordance with the pathologist over five
heterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all
slides of five cohorts (ECTIL-combined) improved results on a held-out test set
(r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that
every 10% increase of ECTIL scores was associated with improved overall
survival independent of clinicopathological variables (HR 0.86, p<0.01),
similar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL
is highly concordant with an expert pathologist and obtains a similar hazard
ratio. ECTIL has a fundamentally simpler design than existing methods and can
be trained on orders of magnitude fewer annotations. Such a CTA may be used to
pre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a
tool to assist clinicians in the diagnostic work-up of patients with BC. Our
model is available under an open source licence
(https://github.com/nki-ai/ectil).

摘要：肿瘤浸润淋巴细胞 (TIL) 的水平是 (三阴性) 乳腺癌 (BC) 患者的预后因素。计算 TIL 评估 (CTA) 有可能协助病理学家完成这项劳动密集型任务，但目前的 CTA 模型严重依赖于许多详细的注释。我们提出并验证了一个基于深度学习的 CTA，它可以在几百倍更少的病理学家注释上仅在十分钟内进行训练。我们从六个队列中收集了 2,340 名 BC 患者的 TILs 评分和临床数据的全玻片图像 (WSI)，其中包括三项随机临床试验。使用病理基础模型从全玻片图像 (WSI) 中提取形态学特征。我们的标签高效计算基质 TIL 评估模型 (ECTIL) 直接从这些特征中回归 TILs 评分。仅在几百个样本上进行训练的 ECTIL（ECTIL-TCGA）显示出与病理学家在五个异质外部队列中的一致性（r=0.54-0.74，AUROC=0.80-0.94）。在五个队列的所有玻片上进行训练（ECTIL-combined）改善了保留测试集上的结果（r=0.69，AUROC=0.85）。多变量 Cox 回归分析表明，ECTIL 评分每增加 10%，与临床病理学变量无关的总体生存率就会提高（HR 0.86，p<0.01），类似于病理学家评分（HR 0.87，p<0.001）。我们证明 ECTIL 与专家病理学家高度一致，并获得了类似的风险比。ECTIL 的设计比现有方法从根本上更简单，并且可以在数量级更少的注释上进行训练。这种 CTA 可用于对患者进行预筛选，例如免疫治疗临床试验纳入，或作为一种工具来帮助临床医生对 BC 患者进行诊断检查。我们的模型可在开放源代码许可下获得 (https://github.com/nki-ai/ectil)。

