{"2501.03884": {"publish_time": "2025-01-07", "title": "AlphaPO -- Reward shape matters for LLM alignment", "paper_summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Examples include Direct Preference Optimization (DPO) and Simple\nPreference Optimization (SimPO). These methods often suffer from likelihood\ndisplacement, a phenomenon by which the probabilities of preferred responses\nare often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to\nhelp change the shape of the reward function beyond the standard log reward.\nAlphaPO helps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B. The analysis and results\npresented highlight the importance of the reward shape, and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u53ca\u5176\u8b8a\u9ad4\u5df2\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6709\u6548\u5c0d\u9f4a\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u4ee5\u9075\u5faa\u6307\u4ee4\u4e26\u53cd\u6620\u4eba\u985e\u50f9\u503c\u89c0\u3002\u6700\u8fd1\uff0c\u76f4\u63a5\u5c0d\u9f4a\u6f14\u7b97\u6cd5 (DAA) \u5df2\u51fa\u73fe\uff0c\u5176\u4e2d RLHF \u7684\u734e\u52f5\u6a21\u578b\u968e\u6bb5\u900f\u904e\u5c07\u734e\u52f5\u76f4\u63a5\u8868\u5fb5\u70ba\u6b63\u5728\u5b78\u7fd2\u7684\u653f\u7b56\u7684\u51fd\u6578\u4f86\u8df3\u904e\u3002\u7bc4\u4f8b\u5305\u62ec\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u548c\u7c21\u55ae\u504f\u597d\u6700\u4f73\u5316 (SimPO)\u3002\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u53d7\u5230\u4f3c\u7136\u4f4d\u79fb\u7684\u5f71\u97ff\uff0c\u9019\u662f\u4e00\u7a2e\u73fe\u8c61\uff0c\u5176\u4e2d\u504f\u597d\u56de\u61c9\u7684\u6a5f\u7387\u901a\u5e38\u6703\u4e0d\u5fc5\u8981\u5730\u964d\u4f4e\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8ad6\u8b49\uff0c\u5c0d\u65bc DAA\uff0c\u734e\u52f5\uff08\u51fd\u6578\uff09\u5f62\u72c0\u5f88\u91cd\u8981\u3002\u6211\u5011\u5f15\u5165\u4e86 AlphaPO\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684 DAA \u65b9\u6cd5\uff0c\u5b83\u5229\u7528 $\\alpha$-\u53c3\u6578\u4f86\u5e6b\u52a9\u6539\u8b8a\u734e\u52f5\u51fd\u6578\u7684\u5f62\u72c0\uff0c\u8d85\u8d8a\u6a19\u6e96\u7684\u5c0d\u6578\u734e\u52f5\u3002AlphaPO \u6709\u52a9\u65bc\u7dad\u6301\u5c0d\u4f3c\u7136\u4f4d\u79fb\u548c\u904e\u5ea6\u6700\u4f73\u5316\u7684\u7d30\u7dfb\u63a7\u5236\u3002\u8207\u8868\u73fe\u6700\u597d\u7684 DAA \u4e4b\u4e00 SimPO \u76f8\u6bd4\uff0cAlphaPO \u5c0e\u81f4 Mistral-7B \u548c Llama3-8B \u7684\u6307\u4ee4\u7248\u672c\u5728\u5c0d\u9f4a\u6548\u80fd\u65b9\u9762\u76f8\u5c0d\u6539\u5584\u4e86\u7d04 7% \u5230 10%\u3002\u6240\u5448\u73fe\u7684\u5206\u6790\u548c\u7d50\u679c\u7a81\u51fa\u4e86\u734e\u52f5\u5f62\u72c0\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u7cfb\u7d71\u5730\u6539\u8b8a\u5b83\u4ee5\u5f71\u97ff\u8a13\u7df4\u52d5\u614b\uff0c\u4ee5\u53ca\u6539\u5584\u5c0d\u9f4a\u6548\u80fd\u3002", "author": "Aman Gupta et.al.", "authors": "Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Jason Zhu, Natesh Pillai, S. Sathiya Keerthi", "id": "2501.03884v1", "paper_url": "http://arxiv.org/abs/2501.03884v1", "repo": "null"}}