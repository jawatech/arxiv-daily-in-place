{"2501.09705": {"publish_time": "2025-01-16", "title": "Practical Continual Forgetting for Pre-trained Vision Models", "paper_summary": "For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners, and these requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify three key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal.\n(iii) In real-world scenarios, the training samples may be scarce or partially\nmissing during the process of forgetting. To address them, we first propose\nGroup Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA\nmodules to fine-tune the FFN layers in Transformer blocks for each forgetting\ntask independently, and towards (ii), a simple group sparse regularization is\nadopted, enabling automatic selection of specific LoRA groups and zeroing out\nthe others. To further extend GS-LoRA to more practical scenarios, we\nincorporate prototype information as additional supervision and introduce a\nmore practical approach, GS-LoRA++. For each forgotten class, we move the\nlogits away from its original prototype. For the remaining classes, we pull the\nlogits closer to their respective prototypes. We conduct extensive experiments\non face recognition, object detection and image classification and demonstrate\nthat our method manages to forget specific classes with minimal impact on other\nclasses. Codes have been released on https://github.com/bjzhb666/GS-LoRA.", "paper_summary_zh": "<paragraph>\u7531\u65bc\u96b1\u79c1\u548c\u5b89\u5168\u6027\u8003\u91cf\uff0c\u5982\u4eca\u6e05\u9664\u9810\u8a13\u7df4\u8996\u89ba\u6a21\u578b\u4e2d\u4e0d\u9700\u8981\u7684\u8cc7\u8a0a\u7684\u9700\u6c42\u5df2\u8b8a\u5f97\u660e\u986f\u3002\u5728\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u6e05\u9664\u8acb\u6c42\u96a8\u6642\u4f86\u81ea\u4f7f\u7528\u8005\u548c\u6a21\u578b\u6240\u6709\u8005\uff0c\u800c\u4e14\u9019\u4e9b\u8acb\u6c42\u901a\u5e38\u6703\u5f62\u6210\u4e00\u500b\u5e8f\u5217\u3002\u56e0\u6b64\uff0c\u5728\u9019\u6a23\u7684\u8a2d\u5b9a\u4e0b\uff0c\u9810\u671f\u6703\u6301\u7e8c\u5f9e\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u79fb\u9664\u9078\u64c7\u6027\u8cc7\u8a0a\uff0c\u540c\u6642\u4fdd\u7559\u5176\u9918\u90e8\u5206\u3002\u6211\u5011\u5c07\u9019\u500b\u554f\u984c\u5b9a\u7fa9\u70ba\u6301\u7e8c\u907a\u5fd8\uff0c\u4e26\u627e\u51fa\u4e09\u500b\u95dc\u9375\u6311\u6230\u3002(i) \u5c0d\u65bc\u4e0d\u9700\u8981\u7684\u77e5\u8b58\uff0c\u6709\u6548\u7387\u4e14\u6709\u6548\u7684\u522a\u9664\u81f3\u95dc\u91cd\u8981\u3002(ii) \u5c0d\u65bc\u4fdd\u7559\u7684\u77e5\u8b58\uff0c\u907a\u5fd8\u7a0b\u5e8f\u5e36\u4f86\u7684\u5f71\u97ff\u61c9\u964d\u5230\u6700\u4f4e\u3002(iii) \u5728\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u8a13\u7df4\u6a23\u672c\u5728\u907a\u5fd8\u904e\u7a0b\u4e2d\u53ef\u80fd\u7a00\u5c11\u6216\u90e8\u5206\u907a\u5931\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u7fa4\u7d44\u7a00\u758f LoRA (GS-LoRA)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u91dd\u5c0d (i)\uff0c\u6211\u5011\u5c0e\u5165 LoRA \u6a21\u7d44\uff0c\u4ee5\u91dd\u5c0d\u6bcf\u500b\u907a\u5fd8\u4efb\u52d9\u7368\u7acb\u5fae\u8abf Transformer \u5340\u584a\u4e2d\u7684 FFN \u5c64\uff0c\u800c\u91dd\u5c0d (ii)\uff0c\u63a1\u7528\u7c21\u55ae\u7684\u7fa4\u7d44\u7a00\u758f\u6b63\u5247\u5316\uff0c\u80fd\u5920\u81ea\u52d5\u9078\u64c7\u7279\u5b9a LoRA \u7fa4\u7d44\u4e26\u5c07\u5176\u4ed6\u7fa4\u7d44\u6b78\u96f6\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u5c07 GS-LoRA \u5ef6\u4f38\u5230\u66f4\u5be6\u969b\u7684\u5834\u666f\uff0c\u6211\u5011\u5c07\u539f\u578b\u8cc7\u8a0a\u7d0d\u5165\u4f5c\u70ba\u984d\u5916\u7684\u76e3\u7763\uff0c\u4e26\u5c0e\u5165\u66f4\u5be6\u7528\u7684\u65b9\u6cd5\uff0cGS-LoRA++\u3002\u5c0d\u65bc\u6bcf\u500b\u88ab\u907a\u5fd8\u7684\u985e\u5225\uff0c\u6211\u5011\u5c07 logit \u9060\u96e2\u5176\u539f\u59cb\u539f\u578b\u3002\u5c0d\u65bc\u5176\u9918\u985e\u5225\uff0c\u6211\u5011\u5c07 logit \u62c9\u8fd1\u5b83\u5011\u5404\u81ea\u7684\u539f\u578b\u3002\u6211\u5011\u5c0d\u4eba\u81c9\u8fa8\u8b58\u3001\u7269\u4ef6\u5075\u6e2c\u548c\u5f71\u50cf\u5206\u985e\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4e26\u5c55\u793a\u6211\u5011\u7684\u6a21\u578b\u8a2d\u6cd5\u907a\u5fd8\u7279\u5b9a\u985e\u5225\uff0c\u540c\u6642\u5c0d\u5176\u4ed6\u985e\u5225\u7684\u5f71\u97ff\u964d\u5230\u6700\u4f4e\u3002\u7a0b\u5f0f\u78bc\u5df2\u5728 https://github.com/bjzhb666/GS-LoRA \u4e0a\u767c\u5e03\u3002</paragraph>", "author": "Hongbo Zhao et.al.", "authors": "Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang", "id": "2501.09705v1", "paper_url": "http://arxiv.org/abs/2501.09705v1", "repo": "https://github.com/bjzhb666/GS-LoRA"}}