{"2501.12243": {"publish_time": "2025-01-21", "title": "FOCUS: First Order Concentrated Updating Scheme", "paper_summary": "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u800c\u6539\u5584\u5176\u9810\u8a13\u7df4\u7a0b\u5e8f\u4f3c\u4e4e\u662f\u9032\u4e00\u6b65\u63d0\u5347\u5176\u529f\u80fd\u7684\u95dc\u9375\u3002\u6839\u64da Adam\u3001\u5b78\u7fd2\u7387\u8870\u6e1b\u548c\u6b0a\u91cd\u8870\u6e1b\u7684\u5df2\u8a18\u9304\u6210\u529f\uff0c\u6211\u5011\u5047\u8a2d\u9810\u8a13\u7df4\u640d\u5931\u666f\u89c0\u5177\u6709\u7e2e\u5c0f\u7684\u8c37\u5730\u7d50\u69cb\u3002\u900f\u904e\u5c0d\u5408\u6210\u640d\u5931\u51fd\u6578\u7684\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\u7576\u68af\u5ea6\u67e5\u8a62\u96dc\u8a0a\u76f8\u5c0d\u65bc\u8c37\u5730\u7684\u92b3\u5229\u5ea6\u8f03\u9ad8\u6642\uff0cAdam \u7684\u6548\u80fd\u6703\u843d\u5f8c\u65bc Signum\uff0c\u56e0\u70ba Adam \u6703\u904e\u5ea6\u5927\u5e45\u5ea6\u5730\u6e1b\u5c11\u6709\u6548\u6b65\u9a5f\u5927\u5c0f\u3002\u9019\u500b\u89c0\u5bdf\u8b93\u6211\u5011\u958b\u767c\u51fa FOCUS\uff0c\u4e00\u7a2e\u900f\u904e\u7d0d\u5165\u5c0d\u79fb\u52d5\u5e73\u5747\u53c3\u6578\u7684\u5438\u5f15\u529b\u4f86\u589e\u5f37 Signum \u7684\u6700\u4f73\u5316\u5668\uff0c\u8b93\u5b83\u53ef\u4ee5\u5728\u7dad\u6301\u8f03\u5927\u6b65\u9a5f\u5927\u5c0f\u7684\u540c\u6642\u66f4\u597d\u5730\u8655\u7406\u96dc\u8a0a\u3002\u5728\u8a13\u7df4 GPT-2 \u6642\uff0cFOCUS \u8b49\u660e\u6bd4 Signum \u66f4\u7a69\u5b9a\uff0c\u800c\u4e14\u6bd4 Adam \u66f4\u5feb\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0c\u68af\u5ea6\u96dc\u8a0a\u53ef\u80fd\u662f LLM \u8a13\u7df4\u4e2d\u4e00\u500b\u672a\u88ab\u5145\u5206\u91cd\u8996\u7684\u9650\u5236\u56e0\u7d20\uff0c\u800c FOCUS \u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Yizhou Liu et.al.", "authors": "Yizhou Liu, Ziming Liu, Jeff Gore", "id": "2501.12243v1", "paper_url": "http://arxiv.org/abs/2501.12243v1", "repo": "null"}}