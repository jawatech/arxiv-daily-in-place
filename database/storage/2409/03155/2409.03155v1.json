{"2409.03155": {"publish_time": "2024-09-05", "title": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models", "paper_summary": "Large Language Models (LLMs) may suffer from hallucinations in real-world\napplications due to the lack of relevant knowledge. In contrast, knowledge\ngraphs encompass extensive, multi-relational structures that store a vast array\nof symbolic facts. Consequently, integrating LLMs with knowledge graphs has\nbeen extensively explored, with Knowledge Graph Question Answering (KGQA)\nserving as a critical touchstone for the integration. This task requires LLMs\nto answer natural language questions by retrieving relevant triples from\nknowledge graphs. However, existing methods face two significant challenges:\n\\textit{excessively long reasoning paths distracting from the answer\ngeneration}, and \\textit{false-positive relations hindering the path\nrefinement}. In this paper, we propose an iterative interactive KGQA framework\nthat leverages the interactive learning capabilities of LLMs to perform\nreasoning and Debating over Graphs (DoG). Specifically, DoG employs a\nsubgraph-focusing mechanism, allowing LLMs to perform answer trying after each\nreasoning step, thereby mitigating the impact of lengthy reasoning paths. On\nthe other hand, DoG utilizes a multi-role debate team to gradually simplify\ncomplex questions, reducing the influence of false-positive relations. This\ndebate mechanism ensures the reliability of the reasoning process. Experimental\nresults on five public datasets demonstrate the effectiveness and superiority\nof our architecture. Notably, DoG outperforms the state-of-the-art method ToG\nby 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively.\nFurthermore, the integration experiments with various LLMs on the mentioned\ndatasets highlight the flexibility of DoG. Code is available at\n\\url{https://github.com/reml-group/DoG}.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u7f3a\u4e4f\u76f8\u95dc\u77e5\u8b58\uff0c\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u53ef\u80fd\u6703\u7522\u751f\u5e7b\u89ba\u3002\u76f8\u8f03\u4e4b\u4e0b\uff0c\u77e5\u8b58\u5716\u8b5c\u5305\u542b\u5ee3\u6cdb\u7684\u591a\u91cd\u95dc\u4fc2\u7d50\u69cb\uff0c\u5132\u5b58\u5927\u91cf\u7b26\u865f\u4e8b\u5be6\u3002\u56e0\u6b64\uff0c\u5c07 LLM \u8207\u77e5\u8b58\u5716\u8b5c\u6574\u5408\u5df2\u5ee3\u6cdb\u63a2\u8a0e\uff0c\u5176\u4e2d\u77e5\u8b58\u5716\u8b5c\u554f\u984c\u89e3\u7b54 (KGQA) \u6210\u70ba\u6574\u5408\u7684\u91cd\u8981\u8a66\u91d1\u77f3\u3002\u6b64\u4efb\u52d9\u8981\u6c42 LLM \u900f\u904e\u5f9e\u77e5\u8b58\u5716\u8b5c\u4e2d\u64f7\u53d6\u76f8\u95dc\u4e09\u5143\u7d44\u4f86\u56de\u7b54\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u9762\u81e8\u5169\u9805\u91cd\u5927\u6311\u6230\uff1a\\textit{\u904e\u9577\u7684\u63a8\u7406\u8def\u5f91\u6703\u5206\u6563\u56de\u7b54\u7522\u751f}\uff0c\u4ee5\u53ca\\textit{\u932f\u8aa4\u6b63\u5411\u95dc\u4fc2\u963b\u7919\u8def\u5f91\u7cbe\u7149}\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u53cd\u8986\u4e92\u52d5\u7684 KGQA \u6846\u67b6\uff0c\u5b83\u5229\u7528 LLM \u7684\u4e92\u52d5\u5b78\u7fd2\u80fd\u529b\u4f86\u57f7\u884c\u63a8\u7406\u548c\u5716\u5f62\u8faf\u8ad6 (DoG)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cDoG \u63a1\u7528\u5b50\u5716\u805a\u7126\u6a5f\u5236\uff0c\u5141\u8a31 LLM \u5728\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u5f8c\u57f7\u884c\u7b54\u6848\u5617\u8a66\uff0c\u5f9e\u800c\u6e1b\u8f15\u5197\u9577\u63a8\u7406\u8def\u5f91\u7684\u5f71\u97ff\u3002\u53e6\u4e00\u65b9\u9762\uff0cDoG \u5229\u7528\u591a\u89d2\u8272\u8faf\u8ad6\u5c0f\u7d44\u9010\u6f38\u7c21\u5316\u8907\u96dc\u554f\u984c\uff0c\u6e1b\u5c11\u932f\u8aa4\u6b63\u5411\u95dc\u4fc2\u7684\u5f71\u97ff\u3002\u9019\u7a2e\u8faf\u8ad6\u6a5f\u5236\u78ba\u4fdd\u4e86\u63a8\u7406\u904e\u7a0b\u7684\u53ef\u9760\u6027\u3002\u5728\u4e94\u500b\u516c\u5171\u6578\u64da\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u67b6\u69cb\u7684\u6709\u6548\u6027\u548c\u512a\u8d8a\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDoG \u5728 WebQuestions \u548c GrailQA \u4e0a\u7684\u6e96\u78ba\u5ea6\u5206\u5225\u6bd4\u6700\u5148\u9032\u7684\u65b9\u6cd5 ToG \u9ad8\u51fa 23.7% \u548c 9.1%\u3002\u6b64\u5916\uff0c\u5728\u4e0a\u8ff0\u6578\u64da\u96c6\u4e0a\u8207\u5404\u7a2e LLM \u7684\u6574\u5408\u5be6\u9a57\u7a81\u986f\u4e86 DoG \u7684\u9748\u6d3b\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728\\url{https://github.com/reml-group/DoG}\u53d6\u5f97\u3002</paragraph>", "author": "Jie Ma et.al.", "authors": "Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui", "id": "2409.03155v1", "paper_url": "http://arxiv.org/abs/2409.03155v1", "repo": "https://github.com/reml-group/dog"}}