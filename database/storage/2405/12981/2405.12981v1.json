{"2405.12981": {"publish_time": "2024-05-21", "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention", "paper_summary": "Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible", "paper_summary_zh": "\u95dc\u9375\u503c (KV) \u5feb\u53d6\u5728\u52a0\u901f\u8f49\u63db\u5668\u5f0f\u81ea\u8ff4\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u89e3\u78bc\u904e\u7a0b\u4e2d\u626e\u6f14\u8457\u4e0d\u53ef\u6216\u7f3a\u7684\u89d2\u8272\u3002\u7136\u800c\uff0c\u5728\u5e8f\u5217\u9577\u5ea6\u548c\u6279\u6b21\u5927\u5c0f\u8f03\u5927\u7684\u60c5\u6cc1\u4e0b\uff0c\u5132\u5b58 KV \u5feb\u53d6\u6240\u9700\u7684\u8a18\u61b6\u9ad4\u91cf\u53ef\u80fd\u904e\u65bc\u9f90\u5927\u3002\u81ea\u8f49\u63db\u5668\u767c\u660e\u4ee5\u4f86\uff0c\u5df2\u767c\u73fe\u7528\u65bc\u7e2e\u6e1b KV \u5feb\u53d6\u5927\u5c0f\u6700\u6709\u6548\u7684\u5169\u500b\u4ecb\u5165\u63aa\u65bd\u5206\u5225\u70ba\u591a\u91cd\u67e5\u8a62\u6ce8\u610f\u529b (MQA) \u53ca\u5176\u5ee3\u7fa9\u5316\u7248\u672c\u7fa4\u7d44\u67e5\u8a62\u6ce8\u610f\u529b (GQA)\u3002MQA \u548c GQA \u90fd\u4fee\u6539\u4e86\u6ce8\u610f\u529b\u5340\u584a\u7684\u8a2d\u8a08\uff0c\u8b93\u591a\u500b\u67e5\u8a62\u982d\u90e8\u53ef\u4ee5\u5171\u7528\u4e00\u500b\u91d1\u9470/\u503c\u982d\u90e8\uff0c\u5927\u5e45\u6e1b\u5c11\u76f8\u7570\u91d1\u9470/\u503c\u982d\u90e8\u7684\u6578\u91cf\uff0c\u540c\u6642\u50c5\u9020\u6210\u6975\u5c0f\u7684\u6e96\u78ba\u5ea6\u964d\u4f4e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9032\u4e00\u6b65\u5171\u7528\u76f8\u9130\u5c64\u4e4b\u9593\u7684\u91d1\u9470\u548c\u503c\u982d\u90e8\uff0c\u8b93\u591a\u91cd\u67e5\u8a62\u6ce8\u610f\u529b\u66f4\u4e0a\u4e00\u5c64\u6a13\uff0c\u4e26\u7522\u751f\u4e00\u7a2e\u6211\u5011\u7a31\u4e4b\u70ba\u8de8\u5c64\u6ce8\u610f\u529b (CLA) \u7684\u65b0\u6ce8\u610f\u529b\u8a2d\u8a08\u3002\u900f\u904e CLA\uff0c\u6211\u5011\u767c\u73fe\u53ef\u4ee5\u518d\u5c07 KV \u5feb\u53d6\u7684\u5c3a\u5bf8\u7e2e\u5c0f 2 \u500d\uff0c\u540c\u6642\u7dad\u6301\u8207\u672a\u4fee\u6539 MQA \u8fd1\u4e4e\u76f8\u540c\u7684\u6e96\u78ba\u5ea6\u3002\u5728\u5f9e\u982d\u8a13\u7df4 1B \u548c 3B \u53c3\u6578\u6a21\u578b\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86 CLA \u5728\u8a18\u61b6\u9ad4/\u6e96\u78ba\u5ea6\u6b0a\u8861\u65b9\u9762\u63d0\u4f9b\u4e86\u5e15\u7d2f\u6258\u6539\u5584\uff0c\u8d85\u8d8a\u4e86\u50b3\u7d71 MQA \u7684\u53ef\u80fd\u6027\uff0c\u8b93\u63a8\u8ad6\u5f97\u4ee5\u4f7f\u7528\u66f4\u9577\u7684\u5e8f\u5217\u9577\u5ea6\u548c\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u800c\u9019\u5728\u5176\u4ed6\u60c5\u6cc1\u4e0b\u662f\u7121\u6cd5\u5be6\u73fe\u7684", "author": "William Brandon et.al.", "authors": "William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, Jonathan Ragan Kelly", "id": "2405.12981v1", "paper_url": "http://arxiv.org/abs/2405.12981v1", "repo": "null"}}