{"2410.10739": {"publish_time": "2024-10-14", "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs", "paper_summary": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f9b\u516c\u773e\u4f7f\u7528\u9700\u8981\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u624d\u80fd\u8207\u6700\u65b0\u8cc7\u6599\u4fdd\u6301\u540c\u6b65\u3002\u9019\u4e9b\u6a21\u578b\u4e5f\u9700\u8981\u91dd\u5c0d\u7279\u5b9a\u6307\u793a\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u7dad\u6301\u5176\u6e96\u78ba\u9075\u5faa\u6307\u793a\u7684\u80fd\u529b\u3002LLM \u901a\u5e38\u5206\u70ba\u5169\u500b\u7248\u672c\u767c\u5e03\uff1a\u9810\u5148\u91dd\u5c0d\u5404\u7a2e\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u7684\u57fa\u672c LLM\uff0c\u4ee5\u53ca\u91dd\u5c0d\u7279\u5b9a\u6307\u793a\u9032\u884c\u984d\u5916\u8a13\u7df4\u7684\u6307\u793a\u7cbe\u7149 LLM\uff0c\u4ee5\u66f4\u4f73\u5730\u9075\u5faa\u6307\u793a\u3002\u554f\u984c\u5728\u65bc\u54ea\u500b\u6a21\u578b\u61c9\u8a72\u9032\u884c\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u4ee5\u7dad\u6301\u5176\u9075\u5faa\u6307\u793a\u7684\u80fd\u529b\uff0c\u540c\u6642\u4e5f\u80fd\u8207\u6700\u65b0\u8cc7\u6599\u4fdd\u6301\u540c\u6b65\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e LLM \u7684\u6301\u7e8c\u9810\u8a13\u7df4\u8207\u6307\u793a\u5fae\u8abf\u4e4b\u9593\u7684\u8907\u96dc\u95dc\u4fc2\uff0c\u4e26\u63a2\u8a0e\u6301\u7e8c\u9810\u8a13\u7df4\u5c0d\u57fa\u672c\u6a21\u578b\u53ca\u5176\u6307\u793a\u5fae\u8abf\u6a21\u578b\u9075\u5faa\u6307\u793a\u80fd\u529b\u7684\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u6307\u793a\u5fae\u8abf\u904e\u7a0b\u5728\u904b\u7b97\u4e0a\u5f88\u5bc6\u96c6\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6a19\u8a3b\u7bc4\u4f8b\uff0c\u624d\u80fd\u8b93\u6a21\u578b\u6709\u6548\u5b78\u7fd2\u3002\u672c\u7814\u7a76\u65e8\u5728\u627e\u51fa\u6700\u5177\u904b\u7b97\u6548\u7387\u7684\u7b56\u7565\uff0c\u4ee5\u7372\u5f97\u6700\u65b0\u7684\u77e5\u8b58\u548c\u9075\u5faa\u6307\u793a\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u9700\u8981\u4efb\u4f55\u6307\u793a\u8cc7\u6599\u548c\u5fae\u8abf\u3002\u6211\u5011\u6839\u64da LLaMa 3\u30013.1 \u548c Qwen 2\u30012.5 \u7cfb\u5217\u7684\u57fa\u672c\u6a21\u578b\u548c\u6307\u793a\u6a21\u578b\uff0c\u5be6\u8b49\u8b49\u660e\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\uff0c\u5168\u9762\u63a2\u8a0e\u6211\u5011\u7684\u5047\u8a2d\uff0c\u6db5\u84cb\u4e0d\u540c\u898f\u6a21\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u8a9e\u6599\u5eab\u548c\u4e0d\u540c\u7684 LLM \u8a2d\u5b9a\u3002", "author": "Ishan Jindal et.al.", "authors": "Ishan Jindal, Chandana Badrinath, Pranjal Bharti, Lakkidi Vinay, Sachin Dev Sharma", "id": "2410.10739v1", "paper_url": "http://arxiv.org/abs/2410.10739v1", "repo": "null"}}