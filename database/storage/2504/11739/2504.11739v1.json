{"2504.11739": {"publish_time": "2025-04-16", "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "paper_summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.", "paper_summary_zh": "<paragraph>\u5728\u5927\u578b\u6578\u64da\u96c6\u4e0a\u8a13\u7df4\u7684\u6587\u672c\u5230\u8996\u983b (T2V) \u751f\u6210\u6a21\u578b\u7684\u767c\u5c55\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0cT2V \u751f\u6210\u6a21\u578b\u5c0d\u8f38\u5165\u63d0\u793a\u7684\u654f\u611f\u6027\u51f8\u986f\u4e86\u63d0\u793a\u8a2d\u8a08\u5728\u5f71\u97ff\u751f\u6210\u7d50\u679c\u4e2d\u7684\u95dc\u9375\u4f5c\u7528\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u9760\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u4f7f\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u63d0\u793a\u8207\u8a13\u7df4\u63d0\u793a\u7684\u5206\u4f48\u4fdd\u6301\u4e00\u81f4\uff0c\u5118\u7ba1\u6c92\u6709\u5305\u542b\u63d0\u793a\u8a5e\u5f59\u548c\u53e5\u5b50\u7d50\u69cb\u7d30\u5fae\u5dee\u5225\u7684\u5b9a\u5236\u6307\u5c0e\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 \\textbf{RAPO}\uff0c\u4e00\u500b\u65b0\u7a4e\u7684 \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{P}rompt \\textbf{O}ptimization \u6846\u67b6\u3002\u70ba\u4e86\u61c9\u5c0d\u7531 LLM \u751f\u6210\u7684\u63d0\u793a\u6240\u7522\u751f\u7684\u6f5b\u5728\u4e0d\u6e96\u78ba\u6027\u548c\u6a21\u7cca\u7d30\u7bc0\uff0cRAPO\u900f\u904e\u5169\u500b\u512a\u5316\u5206\u652f\u6539\u9032\u4e86\u539f\u59cb\u63d0\u793a\uff0c\u9078\u64c7\u4e86\u7528\u65bc T2V \u751f\u6210\u7684\u512a\u8d8a\u63d0\u793a\u3002\u7b2c\u4e00\u500b\u5206\u652f\u4f7f\u7528\u5f9e\u5b78\u7fd2\u7684\u95dc\u4fc2\u5716\u4e2d\u63d0\u53d6\u7684\u5404\u7a2e\u4fee\u98fe\u7b26\u4f86\u589e\u5f37\u4f7f\u7528\u8005\u63d0\u793a\uff0c\u900f\u904e\u5fae\u8abf\u7684 LLM \u5c07\u5176\u6539\u9032\u4ee5\u8207\u8a13\u7df4\u63d0\u793a\u7684\u683c\u5f0f\u4fdd\u6301\u4e00\u81f4\u3002\u76f8\u53cd\u5730\uff0c\u7b2c\u4e8c\u500b\u5206\u652f\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u7684 LLM\uff0c\u6839\u64da\u5b9a\u7fa9\u660e\u78ba\u7684\u6307\u4ee4\u96c6\u91cd\u5beb\u539f\u59cb\u63d0\u793a\u3002\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0cRAPO \u53ef\u4ee5\u6709\u6548\u5730\u589e\u5f37\u751f\u6210\u8996\u983b\u7684\u975c\u614b\u548c\u52d5\u614b\u7dad\u5ea6\uff0c\u8b49\u660e\u4e86\u63d0\u793a\u512a\u5316\u5c0d\u65bc\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002\u5c08\u6848\u7db2\u7ad9\uff1a\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}\u3002</paragraph>\n", "author": "Bingjie Gao et.al.", "authors": "Bingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, Yaohui Wang", "id": "2504.11739v1", "paper_url": "http://arxiv.org/abs/2504.11739v1", "repo": "null"}}