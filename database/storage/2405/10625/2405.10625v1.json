{"2405.10625": {"publish_time": "2024-05-17", "title": "Specialising and Analysing Instruction-Tuned and Byte-Level Language Models for Organic Reaction Prediction", "paper_summary": "Transformer-based encoder-decoder models have demonstrated impressive results\nin chemical reaction prediction tasks. However, these models typically rely on\npretraining using tens of millions of unlabelled molecules, which can be\ntime-consuming and GPU-intensive. One of the central questions we aim to answer\nin this work is: Can FlanT5 and ByT5, the encode-decoder models pretrained\nsolely on language data, be effectively specialised for organic reaction\nprediction through task-specific fine-tuning? We conduct a systematic empirical\nstudy on several key issues of the process, including tokenisation, the impact\nof (SMILES-oriented) pretraining, fine-tuning sample efficiency, and decoding\nalgorithms at inference. Our key findings indicate that although being\npretrained only on language tasks, FlanT5 and ByT5 provide a solid foundation\nto fine-tune for reaction prediction, and thus become `chemistry domain\ncompatible' in the process. This suggests that GPU-intensive and expensive\npretraining on a large dataset of unlabelled molecules may be useful yet not\nessential to leverage the power of language models for chemistry. All our\nmodels achieve comparable Top-1 and Top-5 accuracy although some variation\nacross different models does exist. Notably, tokenisation and vocabulary\ntrimming slightly affect final performance but can speed up training and\ninference; The most efficient greedy decoding strategy is very competitive\nwhile only marginal gains can be achieved from more sophisticated decoding\nalgorithms. In summary, we evaluate FlanT5 and ByT5 across several dimensions\nand benchmark their impact on organic reaction prediction, which may guide more\neffective use of these state-of-the-art language models for chemistry-related\ntasks in the future.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\u5df2\u5728\u5316\u5b78\u53cd\u61c9\u9810\u6e2c\u4efb\u52d9\u4e2d\u5c55\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u4f9d\u8cf4\u65bc\u4f7f\u7528\u6578\u5343\u842c\u500b\u672a\u6a19\u8a18\u5206\u5b50\u7684\u9810\u8a13\u7df4\uff0c\u9019\u6703\u8017\u6642\u4e14\u8017\u8cbb GPU \u8cc7\u6e90\u3002\u6211\u5011\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\u65e8\u5728\u56de\u7b54\u7684\u4e2d\u5fc3\u554f\u984c\u4e4b\u4e00\u662f\uff1a\u50c5\u5728\u8a9e\u8a00\u8cc7\u6599\u4e0a\u9810\u8a13\u7df4\u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b FlanT5 \u548c ByT5\uff0c\u662f\u5426\u80fd\u900f\u904e\u7279\u5b9a\u4efb\u52d9\u7684\u5fae\u8abf\uff0c\u6709\u6548\u5730\u5c08\u9580\u7528\u65bc\u6709\u6a5f\u53cd\u61c9\u9810\u6e2c\uff1f\u6211\u5011\u5c0d\u9019\u500b\u904e\u7a0b\u7684\u5e7e\u500b\u95dc\u9375\u554f\u984c\u9032\u884c\u4e86\u7cfb\u7d71\u6027\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u5305\u62ec\u6a19\u8a18\u5316\u3001\uff08\u9762\u5411 SMILES \u7684\uff09\u9810\u8a13\u7df4\u3001\u5fae\u8abf\u6a23\u672c\u6548\u7387\u548c\u63a8\u7406\u4e2d\u7684\u89e3\u78bc\u6f14\u7b97\u6cd5\u3002\u6211\u5011\u7684\u95dc\u9375\u767c\u73fe\u8868\u660e\uff0c\u5118\u7ba1\u50c5\u5728\u8a9e\u8a00\u4efb\u52d9\u4e0a\u9810\u8a13\u7df4\uff0cFlanT5 \u548c ByT5 \u70ba\u53cd\u61c9\u9810\u6e2c\u7684\u5fae\u8abf\u63d0\u4f9b\u4e86\u7a69\u56fa\u7684\u57fa\u790e\uff0c\u56e0\u6b64\u5728\u9019\u500b\u904e\u7a0b\u4e2d\u8b8a\u5f97\u300c\u76f8\u5bb9\u65bc\u5316\u5b78\u9818\u57df\u300d\u3002\u9019\u8868\u660e\u5728\u5927\u91cf\u7684\u672a\u6a19\u8a18\u5206\u5b50\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8017\u8cbb GPU \u8cc7\u6e90\u4e14\u6602\u8cb4\u7684\u9810\u8a13\u7df4\u53ef\u80fd\u5f88\u6709\u7528\uff0c\u4f46\u5c0d\u65bc\u767c\u63ee\u8a9e\u8a00\u6a21\u578b\u5728\u5316\u5b78\u4e0a\u7684\u80fd\u529b\u4e26\u975e\u5fc5\u8981\u3002\u5118\u7ba1\u4e0d\u540c\u6a21\u578b\u4e4b\u9593\u78ba\u5be6\u5b58\u5728\u4e00\u4e9b\u5dee\u7570\uff0c\u4f46\u6211\u5011\u6240\u6709\u7684\u6a21\u578b\u90fd\u9054\u5230\u4e86\u76f8\u7576\u7684 Top-1 \u548c Top-5 \u6e96\u78ba\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6a19\u8a18\u5316\u548c\u8a5e\u5f59\u4fee\u526a\u6703\u8f15\u5fae\u5f71\u97ff\u6700\u7d42\u6548\u80fd\uff0c\u4f46\u53ef\u4ee5\u52a0\u901f\u8a13\u7df4\u548c\u63a8\u7406\uff1b\u6700\u6709\u6548\u7387\u7684\u8caa\u5a6a\u89e3\u78bc\u7b56\u7565\u6975\u5177\u7af6\u722d\u529b\uff0c\u800c\u66f4\u7cbe\u5bc6\u7684\u89e3\u78bc\u6f14\u7b97\u6cd5\u53ea\u80fd\u7372\u5f97\u908a\u969b\u6536\u76ca\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u5728\u5e7e\u500b\u9762\u5411\u8a55\u4f30 FlanT5 \u548c ByT5\uff0c\u4e26\u8a55\u4f30\u5b83\u5011\u5c0d\u6709\u6a5f\u53cd\u61c9\u9810\u6e2c\u7684\u5f71\u97ff\uff0c\u9019\u53ef\u80fd\u6709\u52a9\u65bc\u5728\u672a\u4f86\u66f4\u6709\u6548\u5730\u4f7f\u7528\u9019\u4e9b\u6700\u5148\u9032\u7684\u8a9e\u8a00\u6a21\u578b\u4f86\u57f7\u884c\u8207\u5316\u5b78\u76f8\u95dc\u7684\u4efb\u52d9\u3002</paragraph>", "author": "Jiayun Pang et.al.", "authors": "Jiayun Pang, Ivan Vuli\u0107", "id": "2405.10625v1", "paper_url": "http://arxiv.org/abs/2405.10625v1", "repo": "null"}}