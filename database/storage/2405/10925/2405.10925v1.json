{"2405.10925": {"publish_time": "2024-05-17", "title": "High-dimensional multiple imputation (HDMI) for partially observed confounders including natural language processing-derived auxiliary covariates", "paper_summary": "Multiple imputation (MI) models can be improved by including auxiliary\ncovariates (AC), but their performance in high-dimensional data is not well\nunderstood. We aimed to develop and compare high-dimensional MI (HDMI)\napproaches using structured and natural language processing (NLP)-derived AC in\nstudies with partially observed confounders. We conducted a plasmode simulation\nstudy using data from opioid vs. non-steroidal anti-inflammatory drug (NSAID)\ninitiators (X) with observed serum creatinine labs (Z2) and time-to-acute\nkidney injury as outcome. We simulated 100 cohorts with a null treatment\neffect, including X, Z2, atrial fibrillation (U), and 13 other\ninvestigator-derived confounders (Z1) in the outcome generation. We then\nimposed missingness (MZ2) on 50% of Z2 measurements as a function of Z2 and U\nand created different HDMI candidate AC using structured and NLP-derived\nfeatures. We mimicked scenarios where U was unobserved by omitting it from all\nAC candidate sets. Using LASSO, we data-adaptively selected HDMI covariates\nassociated with Z2 and MZ2 for MI, and with U to include in propensity score\nmodels. The treatment effect was estimated following propensity score matching\nin MI datasets and we benchmarked HDMI approaches against a baseline imputation\nand complete case analysis with Z1 only. HDMI using claims data showed the\nlowest bias (0.072). Combining claims and sentence embeddings led to an\nimprovement in the efficiency displaying the lowest root-mean-squared-error\n(0.173) and coverage (94%). NLP-derived AC alone did not perform better than\nbaseline MI. HDMI approaches may decrease bias in studies with partially\nobserved confounders where missingness depends on unobserved factors.", "paper_summary_zh": "\u591a\u91cd\u63d2\u88dc (MI) \u6a21\u578b\u53ef\u900f\u904e\u7d0d\u5165\u8f14\u52a9\u5171\u8b8a\u91cf (AC) \u4f86\u6539\u5584\uff0c\u4f46\u5176\u5728\u9ad8\u7dad\u5ea6\u8cc7\u6599\u4e2d\u7684\u8868\u73fe\u5c1a\u672a\u5145\u5206\u4e86\u89e3\u3002\u6211\u5011\u65e8\u5728\u958b\u767c\u548c\u6bd4\u8f03\u9ad8\u7dad\u5ea6 MI (HDMI) \u65b9\u6cd5\uff0c\u5728\u90e8\u5206\u89c0\u5bdf\u5230\u7684\u6df7\u6dc6\u56e0\u5b50\u7814\u7a76\u4e2d\u4f7f\u7528\u7d50\u69cb\u5316\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u884d\u751f\u7684 AC\u3002\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u985e\u6a21\u5f0f\u6a21\u64ec\u7814\u7a76\uff0c\u4f7f\u7528\u985e\u9d09\u7247\u85e5\u7269\u8207\u975e\u985e\u56fa\u9187\u6d88\u708e\u85e5 (NSAID) \u5f15\u767c\u8005 (X) \u7684\u8cc7\u6599\uff0c\u5176\u4e2d\u89c0\u5bdf\u5230\u8840\u6e05\u808c\u9150\u5be6\u9a57\u5ba4 (Z2) \u548c\u6025\u6027\u814e\u81df\u640d\u50b7\u7684\u6642\u9593\u4f5c\u70ba\u7d50\u679c\u3002\u6211\u5011\u6a21\u64ec\u4e86 100 \u500b\u5177\u6709\u7a7a\u503c\u8655\u7406\u6548\u679c\u7684\u7fa4\u7d44\uff0c\u5305\u62ec X\u3001Z2\u3001\u5fc3\u623f\u986b\u52d5 (U) \u548c\u7d50\u679c\u7522\u751f\u4e2d\u7684 13 \u500b\u5176\u4ed6\u7814\u7a76\u4eba\u54e1\u884d\u751f\u7684\u6df7\u6dc6\u56e0\u5b50 (Z1)\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d 50% \u7684 Z2 \u6e2c\u91cf\u65bd\u52a0\u7f3a\u5931 (MZ2) \u4f5c\u70ba Z2 \u548c U \u7684\u51fd\u6578\uff0c\u4e26\u4f7f\u7528\u7d50\u69cb\u5316\u548c NLP \u884d\u751f\u7684\u7279\u5fb5\u5efa\u7acb\u4e0d\u540c\u7684 HDMI \u5019\u9078 AC\u3002\u6211\u5011\u6a21\u64ec\u4e86 U \u672a\u88ab\u89c0\u5bdf\u5230\u7684\u5834\u666f\uff0c\u65b9\u6cd5\u662f\u5f9e\u6240\u6709 AC \u5019\u9078\u96c6\u4e2d\u7701\u7565\u5b83\u3002\u4f7f\u7528 LASSO\uff0c\u6211\u5011\u8cc7\u6599\u81ea\u9069\u61c9\u5730\u9078\u64c7\u4e86\u8207 Z2 \u548c MZ2 \u76f8\u95dc\u7684 HDMI \u5171\u8b8a\u91cf\uff0c\u7528\u65bc MI\uff0c\u4e26\u4f7f\u7528 U \u7d0d\u5165\u50be\u5411\u5f97\u5206\u6a21\u578b\u3002\u5728 MI \u8cc7\u6599\u96c6\u4e2d\u9032\u884c\u50be\u5411\u5f97\u5206\u5339\u914d\u5f8c\u4f30\u8a08\u8655\u7406\u6548\u679c\uff0c\u4e26\u5c07 HDMI \u65b9\u6cd5\u8207\u50c5\u4f7f\u7528 Z1 \u7684\u57fa\u7dda\u63d2\u88dc\u548c\u5b8c\u6574\u6848\u4f8b\u5206\u6790\u9032\u884c\u6bd4\u8f03\u3002\u4f7f\u7528\u7d22\u8ce0\u8cc7\u6599\u7684 HDMI \u986f\u793a\u51fa\u6700\u4f4e\u504f\u5dee (0.072)\u3002\u7d50\u5408\u7d22\u8ce0\u548c\u53e5\u5b50\u5d4c\u5165\u6703\u63d0\u9ad8\u6548\u7387\uff0c\u986f\u793a\u51fa\u6700\u4f4e\u7684\u5747\u65b9\u6839\u8aa4\u5dee (0.173) \u548c\u8986\u84cb\u7387 (94%)\u3002\u55ae\u7368\u7684 NLP \u884d\u751f AC \u7684\u8868\u73fe\u4e26\u672a\u512a\u65bc\u57fa\u7dda MI\u3002\u5728\u7f3a\u5931\u53d6\u6c7a\u65bc\u672a\u89c0\u5bdf\u5230\u7684\u56e0\u7d20\u7684\u90e8\u5206\u89c0\u5bdf\u5230\u7684\u6df7\u6dc6\u56e0\u5b50\u7814\u7a76\u4e2d\uff0cHDMI \u65b9\u6cd5\u53ef\u80fd\u6703\u964d\u4f4e\u504f\u5dee\u3002", "author": "Janick Weberpals et.al.", "authors": "Janick Weberpals, Pamela A. Shaw, Kueiyu Joshua Lin, Richard Wyss, Joseph M Plasek, Li Zhou, Kerry Ngan, Thomas DeRamus, Sudha R. Raman, Bradley G. Hammill, Hana Lee, Sengwee Toh, John G. Connolly, Kimberly J. Dandreo, Fang Tian, Wei Liu, Jie Li, Jos\u00e9 J. Hern\u00e1ndez-Mu\u00f1oz, Sebastian Schneeweiss, Rishi J. Desai", "id": "2405.10925v1", "paper_url": "http://arxiv.org/abs/2405.10925v1", "repo": "null"}}