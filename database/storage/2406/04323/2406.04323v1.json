{"2406.04323": {"publish_time": "2024-06-06", "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories", "paper_summary": "Training autonomous agents with sparse rewards is a long-standing problem in\nonline reinforcement learning (RL), due to low data efficiency. Prior work\novercomes this challenge by extracting useful knowledge from offline data,\noften accomplished through the learning of action distribution from offline\ndata and utilizing the learned distribution to facilitate online RL. However,\nsince the offline data are given and fixed, the extracted knowledge is\ninherently limited, making it difficult to generalize to new tasks. We propose\na novel approach that leverages offline data to learn a generative diffusion\nmodel, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates\nsynthetic trajectories, serving as a form of data augmentation and consequently\nenhancing the performance of online RL methods. The key strength of our\ndiffuser lies in its adaptability, allowing it to effectively handle varying\ntrajectory lengths and mitigate distribution shifts between online and offline\ndata. Because of its simplicity, ATraDiff seamlessly integrates with a wide\nspectrum of RL methods. Empirical evaluation shows that ATraDiff consistently\nachieves state-of-the-art performance across a variety of environments, with\nparticularly pronounced improvements in complicated settings. Our code and demo\nvideo are available at https://atradiff.github.io .", "paper_summary_zh": "\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60 (RL) \u4e2d\uff0c\u4f7f\u7528\u7a00\u758f\u5956\u52b1\u6765\u8bad\u7ec3\u81ea\u4e3b\u4ee3\u7406\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u7531\u4e8e\u6570\u636e\u6548\u7387\u4f4e\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u901a\u8fc7\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u7528\u7684\u77e5\u8bc6\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u901a\u5e38\u662f\u901a\u8fc7\u4ece\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u52a8\u4f5c\u5206\u5e03\u5e76\u5229\u7528\u5b66\u4e60\u5230\u7684\u5206\u5e03\u6765\u4fc3\u8fdb\u5728\u7ebf RL \u6765\u5b9e\u73b0\u7684\u3002\u7136\u800c\uff0c\u7531\u4e8e\u79bb\u7ebf\u6570\u636e\u662f\u7ed9\u5b9a\u7684\u5e76\u4e14\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u6b64\u63d0\u53d6\u7684\u77e5\u8bc6\u672c\u8d28\u4e0a\u662f\u6709\u9650\u7684\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u63a8\u5e7f\u5230\u65b0\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u79bb\u7ebf\u6570\u636e\u6765\u5b66\u4e60\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u79f0\u4e3a\u81ea\u9002\u5e94\u8f68\u8ff9\u6269\u6563\u5668 (ATraDiff)\u3002\u6b64\u6a21\u578b\u751f\u6210\u5408\u6210\u8f68\u8ff9\uff0c\u4f5c\u4e3a\u6570\u636e\u6269\u5145\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u7ebf RL \u65b9\u6cd5\u7684\u6027\u80fd\u3002\u6211\u4eec\u6269\u6563\u5668\u7684\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u5176\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u4e0d\u540c\u7684\u8f68\u8ff9\u957f\u5ea6\u5e76\u51cf\u8f7b\u5728\u7ebf\u548c\u79bb\u7ebf\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u3002\u7531\u4e8e\u5176\u7b80\u5355\u6027\uff0cATraDiff \u53ef\u4ee5\u4e0e\u5e7f\u6cdb\u7684 RL \u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cATraDiff \u5728\u5404\u79cd\u73af\u5883\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u663e\u7740\u7684\u6539\u8fdb\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6f14\u793a\u89c6\u9891\u53ef\u5728 https://atradiff.github.io \u83b7\u5f97\u3002", "author": "Qianlan Yang et.al.", "authors": "Qianlan Yang, Yu-Xiong Wang", "id": "2406.04323v1", "paper_url": "http://arxiv.org/abs/2406.04323v1", "repo": "null"}}