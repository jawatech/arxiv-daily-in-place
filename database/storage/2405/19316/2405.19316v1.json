{"2405.19316": {"publish_time": "2024-05-29", "title": "Robust Preference Optimization through Reward Model Distillation", "paper_summary": "Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, typical preference datasets have only a\nsingle, or at most a few, annotation per preference pair, which causes DPO to\noverconfidently assign rewards that trend towards infinite magnitude. This\nfrequently leads to degenerate policies, sometimes causing even the\nprobabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and propose distillation to get a better proxy for the\ntrue preference distribution over generation pairs: we train the LM to produce\nprobabilities that match the distribution induced by a reward model trained on\nthe preference data. Moreover, to account for uncertainty in the reward model\nwe are distilling from, we optimize against a family of reward models that, as\na whole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b (LM) \u5f8c\u8a13\u7df4 (\u6216\u5c0d\u9f4a) \u6d89\u53ca\u6700\u5927\u5316\u5f9e\u504f\u597d\u6a19\u8a3b\u884d\u751f\u7684\u734e\u52f5\u51fd\u6578\u3002\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u662f\u4e00\u7a2e\u6d41\u884c\u7684\u96e2\u7dda\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u5b83\u76f4\u63a5\u5728\u504f\u597d\u8cc7\u6599\u4e0a\u8a13\u7df4\u653f\u7b56\uff0c\u800c\u7121\u9700\u8a13\u7df4\u734e\u52f5\u6a21\u578b\u6216\u61c9\u7528\u5f37\u5316\u5b78\u7fd2\u3002\u7136\u800c\uff0c\u5178\u578b\u7684\u504f\u597d\u8cc7\u6599\u96c6\u5c0d\u65bc\u6bcf\u500b\u504f\u597d\u5c0d\u53ea\u6709\u4e00\u500b\uff0c\u6216\u6700\u591a\u53ea\u6709\u5e7e\u500b\u6a19\u8a3b\uff0c\u9019\u5c0e\u81f4 DPO \u904e\u65bc\u81ea\u4fe1\u5730\u5206\u914d\u8da8\u5411\u65bc\u7121\u9650\u5927\u7684\u734e\u52f5\u3002\u9019\u5e38\u5e38\u5c0e\u81f4\u7b56\u7565\u9000\u5316\uff0c\u6709\u6642\u751a\u81f3\u5c0e\u81f4\u504f\u597d\u751f\u6210\u7684\u6a5f\u7387\u8da8\u8fd1\u65bc\u96f6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5206\u6790\u4e86\u9019\u7a2e\u73fe\u8c61\u4e26\u63d0\u51fa\u84b8\u993e\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u751f\u6210\u5c0d\u504f\u597d\u5206\u4f48\u4ee3\u7406\uff1a\u6211\u5011\u8a13\u7df4 LM \u7522\u751f\u8207\u6839\u64da\u504f\u597d\u8cc7\u6599\u8a13\u7df4\u7684\u734e\u52f5\u6a21\u578b\u6240\u5f15\u767c\u5206\u4f48\u76f8\u5339\u914d\u7684\u6a5f\u7387\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u8aaa\u660e\u6211\u5011\u8981\u5f9e\u4e2d\u9032\u884c\u84b8\u993e\u7684\u734e\u52f5\u6a21\u578b\u4e2d\u7684\u4e0d\u78ba\u5b9a\u6027\uff0c\u6211\u5011\u91dd\u5c0d\u4e00\u500b\u734e\u52f5\u6a21\u578b\u5bb6\u65cf\u9032\u884c\u6700\u4f73\u5316\uff0c\u9019\u500b\u5bb6\u65cf\u6574\u9ad4\u800c\u8a00\u5f88\u53ef\u80fd\u81f3\u5c11\u5305\u542b\u4e00\u500b\u504f\u597d\u5206\u4f48\u7684\u5408\u7406\u4ee3\u7406\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5f9e\u9019\u6a23\u7684\u734e\u52f5\u6a21\u578b\u5bb6\u65cf\u9032\u884c\u84b8\u993e\u6709\u52a9\u65bc\u63d0\u9ad8\u504f\u597d\u6a19\u8a3b\u4e2d\u5206\u4f48\u8f49\u79fb\u7684\u7a69\u5065\u6027\uff0c\u540c\u6642\u4fdd\u7559 DPO \u7c21\u55ae\u7684\u76e3\u7763\u6027\u8cea\u3002", "author": "Adam Fisch et.al.", "authors": "Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant", "id": "2405.19316v1", "paper_url": "http://arxiv.org/abs/2405.19316v1", "repo": "null"}}