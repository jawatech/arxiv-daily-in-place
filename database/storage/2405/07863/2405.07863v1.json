{"2405.07863": {"publish_time": "2024-05-13", "title": "RLHF Workflow: From Reward Modeling to Online RLHF", "paper_summary": "We present the workflow of Online Iterative Reinforcement Learning from Human\nFeedback (RLHF) in this technical report, which is widely reported to\noutperform its offline counterpart by a large margin in the recent large\nlanguage model (LLM) literature. However, existing open-source RLHF projects\nare still largely confined to the offline learning setting. In this technical\nreport, we aim to fill in this gap and provide a detailed recipe that is easy\nto reproduce for online iterative RLHF. In particular, since online human\nfeedback is usually infeasible for open-source communities with limited\nresources, we start by constructing preference models using a diverse set of\nopen-source datasets and use the constructed proxy preference model to\napproximate human feedback. Then, we discuss the theoretical insights and\nalgorithmic principles behind online iterative RLHF, followed by a detailed\npractical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R,\nachieves impressive performance on LLM chatbot benchmarks, including\nAlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks\nsuch as HumanEval and TruthfulQA. We have shown that supervised fine-tuning\n(SFT) and iterative RLHF can obtain state-of-the-art performance with fully\nopen-source datasets. Further, we have made our models, curated datasets, and\ncomprehensive step-by-step code guidebooks publicly available. Please refer to\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling and\nhttps://github.com/RLHFlow/Online-RLHF for more detailed information.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5728\u9019\u4efd\u6280\u8853\u5831\u544a\u4e2d\u4ecb\u7d39\u4e86\u4eba\u985e\u56de\u994b\u5728\u7dda\u8fed\u4ee3\u5f37\u5316\u5b78\u7fd2 (RLHF) \u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u64da\u5ee3\u6cdb\u5831\u5c0e\uff0c\u5b83\u5728\u6700\u8fd1\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u6587\u737b\u4e2d\u5927\u5e45\u512a\u65bc\u5176\u96e2\u7dda\u5c0d\u61c9\u9805\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u958b\u6e90 RLHF \u9805\u76ee\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u5c40\u9650\u65bc\u96e2\u7dda\u5b78\u7fd2\u8a2d\u7f6e\u3002\u5728\u9019\u4efd\u6280\u8853\u5831\u544a\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u586b\u88dc\u9019\u4e00\u7a7a\u767d\uff0c\u4e26\u63d0\u4f9b\u4e00\u500b\u6613\u65bc\u8907\u88fd\u7684\u5728\u7dda\u8fed\u4ee3 RLHF \u7684\u8a73\u7d30\u914d\u65b9\u3002\u7279\u5225\u662f\uff0c\u7531\u65bc\u5728\u7dda\u4eba\u985e\u56de\u994b\u901a\u5e38\u5c0d\u65bc\u8cc7\u6e90\u6709\u9650\u7684\u958b\u6e90\u793e\u5340\u4f86\u8aaa\u4e0d\u53ef\u884c\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u591a\u6a23\u5316\u7684\u958b\u6e90\u6578\u64da\u96c6\u69cb\u5efa\u504f\u597d\u6a21\u578b\uff0c\u4e26\u4f7f\u7528\u69cb\u5efa\u7684\u4ee3\u7406\u504f\u597d\u6a21\u578b\u4f86\u8fd1\u4f3c\u4eba\u985e\u56de\u994b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a0e\u8ad6\u5728\u7dda\u8fed\u4ee3 RLHF \u80cc\u5f8c\u7684\u7406\u8ad6\u898b\u89e3\u548c\u6f14\u7b97\u6cd5\u539f\u7406\uff0c\u7136\u5f8c\u9032\u884c\u8a73\u7d30\u7684\u5be6\u969b\u5be6\u4f5c\u3002\u6211\u5011\u8a13\u7df4\u7684 LLM\uff0cSFR-Iterative-DPO-LLaMA-3-8B-R\uff0c\u5728 LLM \u804a\u5929\u6a5f\u5668\u4eba\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\uff0c\u5305\u62ec AlpacaEval-2\u3001Arena-Hard \u548c MT-Bench\uff0c\u4ee5\u53ca\u5176\u4ed6\u5b78\u8853\u57fa\u6e96\uff0c\u4f8b\u5982 HumanEval \u548c TruthfulQA\u3002\u6211\u5011\u5df2\u7d93\u8b49\u660e\uff0c\u76e3\u7763\u5fae\u8abf (SFT) \u548c\u8fed\u4ee3 RLHF \u53ef\u4ee5\u4f7f\u7528\u5b8c\u5168\u958b\u6e90\u6578\u64da\u96c6\u7372\u5f97\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5df2\u7d93\u516c\u958b\u4e86\u6211\u5011\u7684\u6a21\u578b\u3001\u7cbe\u9078\u6578\u64da\u96c6\u548c\u5168\u9762\u7684\u9010\u6b65\u7a0b\u5f0f\u78bc\u6307\u5357\u3002\u6709\u95dc\u66f4\u591a\u8a73\u7d30\u8cc7\u8a0a\uff0c\u8acb\u53c3\u95b1 https://github.com/RLHFlow/RLHF-Reward-Modeling \u548c https://github.com/RLHFlow/Online-RLHF\u3002</paragraph>", "author": "Hanze Dong et.al.", "authors": "Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang", "id": "2405.07863v1", "paper_url": "http://arxiv.org/abs/2405.07863v1", "repo": "https://github.com/rlhflow/online-rlhf"}}