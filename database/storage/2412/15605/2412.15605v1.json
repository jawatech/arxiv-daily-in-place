{"2412.15605": {"publish_time": "2024-12-20", "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks", "paper_summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.", "paper_summary_zh": "\u64f7\u53d6\u589e\u5f37\u751f\u6210 (RAG) \u4f5c\u70ba\u4e00\u7a2e\u5f37\u5927\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u6574\u5408\u5916\u90e8\u77e5\u8b58\u4f86\u6e90\u4f86\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\uff0c\u5df2\u7372\u5f97\u5ee3\u6cdb\u63a1\u7528\u3002\u7136\u800c\uff0cRAG \u5f15\u5165\u4e86\u8af8\u5982\u64f7\u53d6\u5ef6\u9072\u3001\u6587\u4ef6\u9078\u53d6\u6f5b\u5728\u932f\u8aa4\u548c\u7cfb\u7d71\u8907\u96dc\u6027\u589e\u52a0\u7b49\u6311\u6230\u3002\u96a8\u8457\u5177\u5099\u986f\u8457\u5ef6\u4f38\u5167\u5bb9\u8996\u7a97\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u66ff\u4ee3\u7bc4\u4f8b\uff0c\u5feb\u53d6\u589e\u5f37\u751f\u6210 (CAG)\uff0c\u5b83\u7e5e\u904e\u4e86\u5373\u6642\u64f7\u53d6\u3002\u6211\u5011\u7684\u505a\u6cd5\u6d89\u53ca\u9810\u5148\u8f09\u5165\u6240\u6709\u76f8\u95dc\u8cc7\u6e90\uff0c\u5c24\u5176\u662f\u7576\u6587\u4ef6\u6216\u64f7\u53d6\u77e5\u8b58\u7684\u898f\u6a21\u6709\u9650\u4e14\u53ef\u63a7\u6642\uff0c\u8f09\u5165\u5230 LLM \u7684\u5ef6\u4f38\u5167\u5bb9\u548c\u5feb\u53d6\u5176\u57f7\u884c\u6642\u671f\u53c3\u6578\u4e2d\u3002\u5728\u63a8\u8ad6\u671f\u9593\uff0c\u6a21\u578b\u5229\u7528\u9019\u4e9b\u9810\u5148\u8f09\u5165\u7684\u53c3\u6578\u4f86\u56de\u7b54\u67e5\u8a62\uff0c\u800c\u7121\u9700\u984d\u5916\u7684\u64f7\u53d6\u6b65\u9a5f\u3002\u6bd4\u8f03\u5206\u6790\u986f\u793a\uff0cCAG \u6d88\u9664\u4e86\u64f7\u53d6\u5ef6\u9072\u4e26\u5c07\u64f7\u53d6\u932f\u8aa4\u964d\u5230\u6700\u4f4e\uff0c\u540c\u6642\u7dad\u6301\u5167\u5bb9\u76f8\u95dc\u6027\u3002\u8de8\u591a\u500b\u57fa\u6e96\u7684\u6548\u80fd\u8a55\u4f30\u7a81\u51fa\u4e86\u9577\u5167\u5bb9 LLM \u512a\u65bc\u6216\u88dc\u5145\u50b3\u7d71 RAG \u7ba1\u7dda\u7684\u5834\u666f\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0c\u5c0d\u65bc\u67d0\u4e9b\u61c9\u7528\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u5177\u6709\u53d7\u9650\u77e5\u8b58\u5eab\u7684\u61c9\u7528\uff0cCAG \u63d0\u4f9b\u4e86\u4e00\u500b\u7c21\u5316\u4e14\u6709\u6548\u7684 RAG \u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u964d\u4f4e\u8907\u96dc\u6027\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u4e86\u76f8\u7576\u6216\u66f4\u4f73\u7684\u7d50\u679c\u3002", "author": "Brian J Chan et.al.", "authors": "Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, Hen-Hsen Huang", "id": "2412.15605v1", "paper_url": "http://arxiv.org/abs/2412.15605v1", "repo": "null"}}