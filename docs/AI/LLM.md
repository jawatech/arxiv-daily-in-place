
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-13**|**GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction**|Sicheng Zuo et.al.|[2412.10373v1](http://arxiv.org/abs/2412.10373v1)|[link](https://github.com/zuosc19/gaussianworld)|
|**2024-12-13**|**A Grounded Typology of Word Classes**|Coleman Haley et.al.|[2412.10369v1](http://arxiv.org/abs/2412.10369v1)|null|
|**2024-12-13**|**Apollo: An Exploration of Video Understanding in Large Multimodal Models**|Orr Zohar et.al.|[2412.10360v1](http://arxiv.org/abs/2412.10360v1)|null|
|**2024-12-13**|**A Library for Learning Neural Operators**|Jean Kossaifi et.al.|[2412.10354v1](http://arxiv.org/abs/2412.10354v1)|null|
|**2024-12-13**|**A dual contrastive framework**|Yuan Sun et.al.|[2412.10348v1](http://arxiv.org/abs/2412.10348v1)|null|
|**2024-12-13**|**COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**|Yuchen Ren et.al.|[2412.10347v1](http://arxiv.org/abs/2412.10347v1)|null|
|**2024-12-13**|**TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies**|Ruijie Zheng et.al.|[2412.10345v1](http://arxiv.org/abs/2412.10345v1)|null|
|**2024-12-13**|**Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**|Zhiqi Ge et.al.|[2412.10342v1](http://arxiv.org/abs/2412.10342v1)|null|
|**2024-12-13**|**Generative AI in Medicine**|Divya Shanmugam et.al.|[2412.10337v1](http://arxiv.org/abs/2412.10337v1)|null|
|**2024-12-13**|**AdvPrefix: An Objective for Nuanced LLM Jailbreaks**|Sicheng Zhu et.al.|[2412.10321v1](http://arxiv.org/abs/2412.10321v1)|null|
|**2024-12-13**|**SCBench: A KV Cache-Centric Analysis of Long-Context Methods**|Yucheng Li et.al.|[2412.10319v1](http://arxiv.org/abs/2412.10319v1)|null|
|**2024-12-13**|**BrushEdit: All-In-One Image Inpainting and Editing**|Yaowei Li et.al.|[2412.10316v1](http://arxiv.org/abs/2412.10316v1)|null|
|**2024-12-13**|**Interlocking-free Selective Rationalization Through Genetic-based Learning**|Federico Ruggeri et.al.|[2412.10312v1](http://arxiv.org/abs/2412.10312v1)|null|
|**2024-12-13**|**DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding**|Zhiyu Wu et.al.|[2412.10302v1](http://arxiv.org/abs/2412.10302v1)|[link](https://github.com/deepseek-ai/deepseek-vl2)|
|**2024-12-13**|**Still "Talking About Large Language Models": Some Clarifications**|Murray Shanahan et.al.|[2412.10291v1](http://arxiv.org/abs/2412.10291v1)|null|
|**2024-12-13**|**One world, one opinion? The superstar effect in LLM responses**|Sofie Goethals et.al.|[2412.10281v1](http://arxiv.org/abs/2412.10281v1)|null|
|**2024-12-13**|**Envisioning National Resources for Artificial Intelligence Research: NSF Workshop Report**|Shantenu Jha et.al.|[2412.10278v1](http://arxiv.org/abs/2412.10278v1)|null|
|**2024-12-13**|**Benchmarking Linguistic Diversity of Large Language Models**|Yanzhu Guo et.al.|[2412.10271v1](http://arxiv.org/abs/2412.10271v1)|[link](https://github.com/yanzhuguo/llm-diversity)|
|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270v1](http://arxiv.org/abs/2412.10270v1)|null|
|**2024-12-13**|**Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT**|Danielle R. Thomas et.al.|[2412.10267v1](http://arxiv.org/abs/2412.10267v1)|[link](https://github.com/cmu-plus/lak2025-advocacy)|
|**2024-12-13**|**Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media**|Jiaqing Yuan et.al.|[2412.10266v1](http://arxiv.org/abs/2412.10266v1)|null|
|**2024-12-13**|**Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models**|Harry J. Davies et.al.|[2412.10257v1](http://arxiv.org/abs/2412.10257v1)|null|
|**2024-12-13**|**Exploring the Frontiers of Animation Video Generation in the Sora Era: Method, Dataset and Benchmark**|Yudong Jiang et.al.|[2412.10255v1](http://arxiv.org/abs/2412.10255v1)|null|
|**2024-12-13**|**Efficient Continual Pre-training of LLMs for Low-resource Languages**|Arijit Nag et.al.|[2412.10244v1](http://arxiv.org/abs/2412.10244v1)|null|
|**2024-12-13**|**Physics Instrument Design with Reinforcement Learning**|Shah Rukh Qasim et.al.|[2412.10237v1](http://arxiv.org/abs/2412.10237v1)|null|
|**2024-12-13**|**How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives**|Timour Ichmoukhamedov et.al.|[2412.10220v1](http://arxiv.org/abs/2412.10220v1)|null|
|**2024-12-13**|**GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion**|Jiapeng Tang et.al.|[2412.10209v1](http://arxiv.org/abs/2412.10209v1)|null|
|**2024-12-13**|**Retrieval-Augmented Semantic Parsing: Using Large Language Models to Improve Generalization**|Xiao Zhang et.al.|[2412.10207v1](http://arxiv.org/abs/2412.10207v1)|null|
|**2024-12-13**|**From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection**|Haowei Wang et.al.|[2412.10198v1](http://arxiv.org/abs/2412.10198v1)|null|
|**2024-12-13**|**Solving Robust Markov Decision Processes: Generic, Reliable, Efficient**|Tobias Meggendorfer et.al.|[2412.10185v1](http://arxiv.org/abs/2412.10185v1)|null|
|**2024-12-13**|**Multi-Head Encoding for Extreme Label Classification**|Daojun Liang et.al.|[2412.10182v1](http://arxiv.org/abs/2412.10182v1)|[link](https://github.com/anoise/mhe)|
|**2024-12-13**|**SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models**|Hung Nguyen et.al.|[2412.10178v1](http://arxiv.org/abs/2412.10178v1)|null|
|**2024-12-13**|**Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation**|Federico Julian Camerota Verd√π et.al.|[2412.10163v1](http://arxiv.org/abs/2412.10163v1)|null|
|**2024-12-13**|**Direct Encoding of Declare Constraints in ASP**|Francesco Chiariello et.al.|[2412.10152v1](http://arxiv.org/abs/2412.10152v1)|null|
|**2024-12-13**|**VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation**|Hyeonseok Lim et.al.|[2412.10151v1](http://arxiv.org/abs/2412.10151v1)|null|
|**2024-12-13**|**TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse Analysis with Prompt Engineering**|Bingru Li et.al.|[2412.10139v1](http://arxiv.org/abs/2412.10139v1)|null|
|**2024-12-13**|**ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**|Yang Qin et.al.|[2412.10138v1](http://arxiv.org/abs/2412.10138v1)|[link](https://github.com/alibaba/route)|
|**2024-12-13**|**Can LLMs Convert Graphs to Text-Attributed Graphs?**|Zehong Wang et.al.|[2412.10136v1](http://arxiv.org/abs/2412.10136v1)|[link](https://github.com/zehong-wang/tans)|
|**2024-12-13**|**ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers**|Junyan Hu et.al.|[2412.10135v1](http://arxiv.org/abs/2412.10135v1)|null|
|**2024-12-13**|**You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**|Islem Bouzenia et.al.|[2412.10133v1](http://arxiv.org/abs/2412.10133v1)|null|
|**2024-12-13**|**Familiarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data**|Jonas Golde et.al.|[2412.10121v1](http://arxiv.org/abs/2412.10121v1)|null|
|**2024-12-13**|**CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models**|Zhihao Du et.al.|[2412.10117v1](http://arxiv.org/abs/2412.10117v1)|null|
|**2024-12-13**|**Label-template based Few-Shot Text Classification with Contrastive Learning**|Guanghua Hou et.al.|[2412.10110v1](http://arxiv.org/abs/2412.10110v1)|null|
|**2024-12-13**|**NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models**|Asmaa Abdallah et.al.|[2412.10107v1](http://arxiv.org/abs/2412.10107v1)|null|
|**2024-12-13**|**A Cascaded Dilated Convolution Approach for Mpox Lesion Classification**|Ayush Deshmukh et.al.|[2412.10106v1](http://arxiv.org/abs/2412.10106v1)|null|
|**2024-12-13**|**MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset**|Sagi Shaier et.al.|[2412.10105v1](http://arxiv.org/abs/2412.10105v1)|null|
|**2024-12-13**|**RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector**|Zhensheng Wang et.al.|[2412.10104v1](http://arxiv.org/abs/2412.10104v1)|[link](https://github.com/jensen-w/retqa)|
|**2024-12-13**|**AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm Detection Incorporating Bi-modal Data Augmentation**|Xiyuan Gao et.al.|[2412.10103v1](http://arxiv.org/abs/2412.10103v1)|null|
|**2024-12-13**|**HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation**|Jaione Bengoetxea et.al.|[2412.10095v1](http://arxiv.org/abs/2412.10095v1)|null|
|**2024-12-13**|**AI in the Cosmos**|N. Sahakyan et.al.|[2412.10093v1](http://arxiv.org/abs/2412.10093v1)|null|
|**2024-12-13**|**Lost in the Middle, and In-Between: Enhancing Language Models' Ability to Reason Over Long Contexts in Multi-Hop QA**|George Arthur Baker et.al.|[2412.10079v1](http://arxiv.org/abs/2412.10079v1)|null|
|**2024-12-13**|**Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity**|Dongyun Kam et.al.|[2412.10059v1](http://arxiv.org/abs/2412.10059v1)|null|
|**2024-12-13**|**GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?**|Zhikai Lei et.al.|[2412.10056v1](http://arxiv.org/abs/2412.10056v1)|null|
|**2024-12-13**|**Unsupervised Named Entity Disambiguation for Low Resource Domains**|Debarghya Datta et.al.|[2412.10054v1](http://arxiv.org/abs/2412.10054v1)|[link](https://github.com/deba-iitbh/gst-ned)|
|**2024-12-13**|**TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views**|Liang Zhao et.al.|[2412.10051v1](http://arxiv.org/abs/2412.10051v1)|null|
|**2024-12-13**|**Large Action Models: From Inception to Implementation**|Lu Wang et.al.|[2412.10047v1](http://arxiv.org/abs/2412.10047v1)|[link](https://github.com/microsoft/UFO)|
|**2024-12-13**|**Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework**|Niloy Kumar Kundu et.al.|[2412.10011v1](http://arxiv.org/abs/2412.10011v1)|null|
|**2024-12-13**|**Automated Collection of Evaluation Dataset for Semantic Search in Low-Resource Domain Language**|Anastasia Zhukova et.al.|[2412.10008v1](http://arxiv.org/abs/2412.10008v1)|null|
|**2024-12-13**|**The role of inhibitory control in garden-path sentence processing: A Chinese-English bilingual perspective**|Xiaohui Rao et.al.|[2412.10006v1](http://arxiv.org/abs/2412.10006v1)|null|
|**2024-12-13**|**Cycle-Consistent Bridge Diffusion Model for Accelerated MRI Reconstruction**|Tao Song et.al.|[2412.09998v1](http://arxiv.org/abs/2412.09998v1)|null|
|**2024-12-13**|**A Comparative Study of LLMs, NMT Models, and Their Combination in Persian-English Idiom Translation**|Sara Rezaeimanesh et.al.|[2412.09993v1](http://arxiv.org/abs/2412.09993v1)|null|
|**2024-12-13**|**Visual Object Tracking across Diverse Data Modalities: A Review**|Mengmeng Wang et.al.|[2412.09991v1](http://arxiv.org/abs/2412.09991v1)|null|
|**2024-12-13**|**Small Language Model as Data Prospector for Large Language Model**|Shiwen Ni et.al.|[2412.09990v1](http://arxiv.org/abs/2412.09990v1)|null|
|**2024-12-13**|**AI and the Future of Digital Public Squares**|Beth Goldberg et.al.|[2412.09988v1](http://arxiv.org/abs/2412.09988v1)|null|
|**2024-12-13**|**Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective**|Yuchen Fang et.al.|[2412.09972v1](http://arxiv.org/abs/2412.09972v1)|[link](https://github.com/lmissher/patchstg)|
|**2024-12-13**|**EP-CFG: Energy-Preserving Classifier-Free Guidance**|Kai Zhang et.al.|[2412.09966v1](http://arxiv.org/abs/2412.09966v1)|null|
|**2024-12-13**|**Romanized to Native Malayalam Script Transliteration Using an Encoder-Decoder Framework**|Bajiyo Baiju et.al.|[2412.09957v1](http://arxiv.org/abs/2412.09957v1)|null|
|**2024-12-13**|**Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework**|Qiao Sun et.al.|[2412.09946v1](http://arxiv.org/abs/2412.09946v1)|null|
|**2024-12-13**|**Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation**|Yanxu Mao et.al.|[2412.09922v1](http://arxiv.org/abs/2412.09922v1)|null|
|**2024-12-13**|**B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens**|Zhuqiang Lu et.al.|[2412.09919v1](http://arxiv.org/abs/2412.09919v1)|null|
|**2024-12-13**|**Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning**|Jing Bi et.al.|[2412.09906v1](http://arxiv.org/abs/2412.09906v1)|[link](https://github.com/bijings/sgft)|
|**2024-12-13**|**Analyzing Fairness of Computer Vision and Natural Language Processing Models**|Ahmed Rashed et.al.|[2412.09900v1](http://arxiv.org/abs/2412.09900v1)|null|
|**2024-12-13**|**Analyzing Fairness of Classification Machine Learning Model with Structured Dataset**|Ahmed Rashed et.al.|[2412.09896v1](http://arxiv.org/abs/2412.09896v1)|null|
|**2024-12-13**|**Semi-Periodic Activation for Time Series Classification**|Jos√© Gilberto Barbosa de Medeiros J√∫nior et.al.|[2412.09889v1](http://arxiv.org/abs/2412.09889v1)|[link](https://github.com/jose-gilberto/leakysinelu)|
|**2024-12-13**|**CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls**|Li Chai et.al.|[2412.09887v1](http://arxiv.org/abs/2412.09887v1)|null|
|**2024-12-13**|**Benchmarking Table Comprehension In The Wild**|Yikang Pan et.al.|[2412.09884v1](http://arxiv.org/abs/2412.09884v1)|null|
|**2024-12-13**|**On the Limit of Language Models as Planning Formalizers**|Cassie Huang et.al.|[2412.09879v1](http://arxiv.org/abs/2412.09879v1)|null|
|**2024-12-13**|**Byte Latent Transformer: Patches Scale Better Than Tokens**|Artidoro Pagnoni et.al.|[2412.09871v1](http://arxiv.org/abs/2412.09871v1)|[link](https://github.com/facebookresearch/blt)|
|**2024-12-13**|**Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for Supervised Fine-tuning**|Abraham Atsiwo et.al.|[2412.09859v1](http://arxiv.org/abs/2412.09859v1)|null|
|**2024-12-13**|**RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning**|Charles Xu et.al.|[2412.09858v1](http://arxiv.org/abs/2412.09858v1)|null|
|**2024-12-13**|**LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity**|Hongjie Wang et.al.|[2412.09856v1](http://arxiv.org/abs/2412.09856v1)|null|
|**2024-12-13**|**Learning Structural Causal Models from Ordering: Identifiable Flow Models**|Minh Khoa Le et.al.|[2412.09843v1](http://arxiv.org/abs/2412.09843v1)|null|
|**2024-12-13**|**Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models**|Changqun Li et.al.|[2412.09827v1](http://arxiv.org/abs/2412.09827v1)|null|
|**2024-12-13**|**Precise Antigen-Antibody Structure Predictions Enhance Antibody Development with HelixFold-Multimer**|Jie Gao et.al.|[2412.09826v1](http://arxiv.org/abs/2412.09826v1)|null|
|**2024-12-13**|**MERaLiON-AudioLLM: Technical Report**|Yingxu He et.al.|[2412.09818v1](http://arxiv.org/abs/2412.09818v1)|null|
|**2024-12-13**|**Enhancing Multimodal Large Language Models Complex Reason via Similarity Computation**|Xiaofeng Zhang et.al.|[2412.09817v1](http://arxiv.org/abs/2412.09817v1)|[link](https://github.com/fanshuozeng/simignore)|
|**2024-12-13**|**Temporal Causal Discovery in Dynamic Bayesian Networks Using Federated Learning**|Jianhong Chen et.al.|[2412.09814v1](http://arxiv.org/abs/2412.09814v1)|null|
|**2024-12-13**|**ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic LayerReplace and Selective Rank Compression**|Kai Yao et.al.|[2412.09812v1](http://arxiv.org/abs/2412.09812v1)|null|
|**2024-12-13**|**LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering**|Patrick Sutanto et.al.|[2412.09807v1](http://arxiv.org/abs/2412.09807v1)|null|
|**2024-12-13**|**Universal Inceptive GNNs by Eliminating the Smoothness-generalization Dilemma**|Ming Gu et.al.|[2412.09805v1](http://arxiv.org/abs/2412.09805v1)|null|
|**2024-12-13**|**CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object Detection**|Qibo Chen et.al.|[2412.09799v1](http://arxiv.org/abs/2412.09799v1)|null|
|**2024-12-13**|**AutoPatent: A Multi-Agent Framework for Automatic Patent Generation**|Qiyao Wang et.al.|[2412.09796v1](http://arxiv.org/abs/2412.09796v1)|[link](https://github.com/qiyao-wang/autopatent)|
|**2024-12-13**|**Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation**|Jonghyuk Park et.al.|[2412.09770v1](http://arxiv.org/abs/2412.09770v1)|[link](https://github.com/jpstyle/ns-arch-unity)|
|**2024-12-12**|**Memory Layers at Scale**|Vincent-Pierre Berges et.al.|[2412.09764v1](http://arxiv.org/abs/2412.09764v1)|[link](https://github.com/facebookresearch/memory)|
|**2024-12-12**|**Congruence-based Learning of Probabilistic Deterministic Finite Automata**|Mat√≠as Carrasco et.al.|[2412.09760v1](http://arxiv.org/abs/2412.09760v1)|null|
|**2024-12-12**|**AI Red-Teaming is a Sociotechnical System. Now What?**|Tarleton Gillespie et.al.|[2412.09751v1](http://arxiv.org/abs/2412.09751v1)|null|
|**2024-12-12**|**Let Curves Speak: A Continuous Glucose Monitor based Large Sensor Foundation Model for Diabetes Management**|Junjie Luo et.al.|[2412.09727v1](http://arxiv.org/abs/2412.09727v1)|null|
|**2024-12-12**|**The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications**|Binxu Wang et.al.|[2412.09726v1](http://arxiv.org/abs/2412.09726v1)|null|
|**2024-12-12**|**GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers**|Sarkar Snigdha Sarathi Das et.al.|[2412.09722v1](http://arxiv.org/abs/2412.09722v1)|null|
|**2024-12-12**|**Human vs. AI: A Novel Benchmark and a Comparative Study on the Detection of Generated Images and the Impact of Prompts**|Philipp Moe√üner et.al.|[2412.09715v1](http://arxiv.org/abs/2412.09715v1)|[link](https://github.com/heikeadel/cocoxgen)|

#### Abstracts
##### **GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction**
2412.10373v1 by Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu

3D occupancy prediction is important for autonomous driving due to its
comprehensive perception of the surroundings. To incorporate sequential inputs,
most existing methods fuse representations from previous frames to infer the
current 3D occupancy. However, they fail to consider the continuity of driving
scenarios and ignore the strong prior provided by the evolution of 3D scenes
(e.g., only dynamic objects move). In this paper, we propose a
world-model-based framework to exploit the scene evolution for perception. We
reformulate 3D occupancy prediction as a 4D occupancy forecasting problem
conditioned on the current sensor input. We decompose the scene evolution into
three factors: 1) ego motion alignment of static scenes; 2) local movements of
dynamic objects; and 3) completion of newly-observed scenes. We then employ a
Gaussian world model (GaussianWorld) to explicitly exploit these priors and
infer the scene evolution in the 3D Gaussian space considering the current RGB
observation. We evaluate the effectiveness of our framework on the widely used
nuScenes dataset. Our GaussianWorld improves the performance of the
single-frame counterpart by over 2% in mIoU without introducing additional
computations. Code: https://github.com/zuosc19/GaussianWorld.

ÊëòË¶ÅÔºö3D ‰ΩîÁî®È†êÊ∏¨Â∞çÊñºËá™ÂãïÈßïÈßõÂæàÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉËÉΩÂÖ®Èù¢ÊÑüÁü•Âë®ÂúçÁí∞Â¢É„ÄÇÁÇ∫‰∫ÜÁ¥çÂÖ•Â∫èÂàóËº∏ÂÖ•ÔºåÂ§ßÂ§öÊï∏ÁèæÊúâÊñπÊ≥ïËûçÂêà‰∫ÜÂâçÂπæÂπÄÁöÑË°®Á§∫Ôºå‰ª•Êé®Ë´ñÁï∂ÂâçÁöÑ 3D ‰ΩîÁî®„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊú™ËÉΩËÄÉÊÖÆÈßïÈßõÂ†¥ÊôØÁöÑÈÄ£Á∫åÊÄßÔºå‰∏¶‰∏îÂøΩÁï•‰∫Ü 3D Â†¥ÊôØÊºîÂåñÊâÄÊèê‰æõÁöÑÂº∑ÂÖàÈ©óÔºà‰æãÂ¶ÇÔºåÂè™ÊúâÂãïÊÖãÁâ©È´îÊúÉÁßªÂãïÔºâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰∏ñÁïåÊ®°ÂûãÁöÑÊ°ÜÊû∂Ôºå‰ª•Âà©Áî®Â†¥ÊôØÊºîÂåñÈÄ≤Ë°åÊÑüÁü•„ÄÇÊàëÂÄëÂ∞á 3D ‰ΩîÁî®È†êÊ∏¨ÈáçÊñ∞Ë°®Ëø∞ÁÇ∫ 4D ‰ΩîÁî®È†êÊ∏¨ÂïèÈ°åÔºåÊ¢ù‰ª∂ÂèñÊ±∫ÊñºÁï∂ÂâçÁöÑÊÑüÊ∏¨Âô®Ëº∏ÂÖ•„ÄÇÊàëÂÄëÂ∞áÂ†¥ÊôØÊºîÂåñÂàÜËß£ÁÇ∫‰∏âÂÄãÂõ†Á¥†Ôºö1ÔºâÈùúÊÖãÂ†¥ÊôØÁöÑËá™ÈÅãÂãïÂ∞çÈΩäÔºõ2ÔºâÂãïÊÖãÁâ©È´îÁöÑÂ±ÄÈÉ®ÈÅãÂãïÔºõ‰ª•Âèä 3ÔºâÊñ∞ËßÄÂØüÂ†¥ÊôØÁöÑÂÆåÊàê„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊé°Áî®È´òÊñØ‰∏ñÁïåÊ®°Âûã (GaussianWorld) ‰æÜÊòéÁ¢∫Âà©Áî®ÈÄô‰∫õÂÖàÈ©óÔºå‰∏¶Âú®ËÄÉÊÖÆÁï∂Ââç RGB ËßÄÊ∏¨ÂÄºÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊé®Ë´ñ 3D È´òÊñØÁ©∫Èñì‰∏≠ÁöÑÂ†¥ÊôØÊºîÂåñ„ÄÇÊàëÂÄëÂú®Âª£Ê≥õ‰ΩøÁî®ÁöÑ nuScenes Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÁöÑ GaussianWorld Âú®‰∏çÂºïÂÖ•È°çÂ§ñË®àÁÆóÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞áÂñÆÂπÄÂ∞çÊáâÈ†ÖÁöÑ mIoU ÊïàËÉΩÊèêÂçá‰∫Ü 2% ‰ª•‰∏ä„ÄÇÁ®ãÂºèÁ¢ºÔºöhttps://github.com/zuosc19/GaussianWorld„ÄÇ

##### **A Grounded Typology of Word Classes**
2412.10369v1 by Coleman Haley, Sharon Goldwater, Edoardo Ponti

We propose a grounded approach to meaning in language typology. We treat data
from perceptual modalities, such as images, as a language-agnostic
representation of meaning. Hence, we can quantify the function--form
relationship between images and captions across languages. Inspired by
information theory, we define "groundedness", an empirical measure of
contextual semantic contentfulness (formulated as a difference in surprisal)
which can be computed with multilingual multimodal language models. As a proof
of concept, we apply this measure to the typology of word classes. Our measure
captures the contentfulness asymmetry between functional (grammatical) and
lexical (content) classes across languages, but contradicts the view that
functional classes do not convey content. Moreover, we find universal trends in
the hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that
our measure partly correlates with psycholinguistic concreteness norms in
English. We release a dataset of groundedness scores for 30 languages. Our
results suggest that the grounded typology approach can provide quantitative
evidence about semantic function in language.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË™ûË®ÄÈ°ûÂûãÂ≠∏ÊÑèÁæ©ÁöÑÁ¥ÆÂØ¶ÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞á‰æÜËá™ÊÑüÁü•ÊñπÂºèÁöÑÊï∏ÊìöÔºà‰æãÂ¶ÇÂúñÂÉèÔºâË¶ñÁÇ∫‰∏ÄÁ®ÆËàáË™ûË®ÄÁÑ°ÈóúÁöÑÊÑèÁæ©Ë°®Á§∫„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂèØ‰ª•ÈáèÂåñË∑®Ë™ûË®ÄÁöÑÂúñÂÉèÂíåÂ≠óÂπï‰πãÈñìÁöÑÂäüËÉΩÂΩ¢ÂºèÈóú‰øÇ„ÄÇÂèóË≥áË®äÁêÜË´ñÁöÑÂïüÁôºÔºåÊàëÂÄëÂÆöÁæ©‰∫Ü„ÄåÁ¥ÆÂØ¶ÊÄß„ÄçÔºåÈÄôÊòØ‰∏ÄÂÄã‰∏ä‰∏ãÊñáË™ûÁæ©ÂÖßÂÆπË±êÂØåÊÄßÁöÑÁ∂ìÈ©óÊ∏¨ÈáèÔºàË°®Ëø∞ÁÇ∫È©öÂ•áÁöÑÂ∑ÆÁï∞ÔºâÔºåÂèØ‰ª•Áî®Â§öË™ûË®ÄÂ§öÊ®°ÊÖãË™ûË®ÄÊ®°ÂûãË®àÁÆó„ÄÇ‰ΩúÁÇ∫Ê¶ÇÂøµÈ©óË≠âÔºåÊàëÂÄëÂ∞áÊ≠§Ê∏¨ÈáèÊáâÁî®ÊñºË©ûÈ°ûÁöÑÈ°ûÂûãÂ≠∏„ÄÇÊàëÂÄëÁöÑÊ∏¨ÈáèÊçïÊçâ‰∫ÜË∑®Ë™ûË®ÄÁöÑÂäüËÉΩÔºàË™ûÊ≥ïÔºâÂíåË©ûÂΩôÔºàÂÖßÂÆπÔºâÈ°ûÂà•‰πãÈñìÁöÑÂÖßÂÆπË±êÂØåÊÄß‰∏çÂ∞çÁ®±Ôºå‰ΩÜËàáÂäüËÉΩÈ°ûÂà•‰∏çÂÇ≥ÈÅîÂÖßÂÆπÁöÑËßÄÈªûÁõ∏ÁüõÁõæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÁ¥ÆÂØ¶ÊÄßÂ±§Á¥ö‰∏≠ÁöÑÊôÆÈÅçË∂®Âã¢Ôºà‰æãÂ¶ÇÔºåÂêçË©û>ÂΩ¢ÂÆπË©û>ÂãïË©ûÔºâÔºå‰∏¶Ë°®ÊòéÊàëÂÄëÁöÑÊ∏¨ÈáèÈÉ®ÂàÜËàáËã±Ë™ûÁöÑÂøÉÁêÜË™ûË®ÄÂÖ∑È´îÊÄßË¶èÁØÑÁõ∏Èóú„ÄÇÊàëÂÄëÁôºÂ∏É‰∫Ü‰∏ÄÂÄãÂåÖÂê´ 30 Á®ÆË™ûË®ÄÁöÑÁ¥ÆÂØ¶ÊÄßÂàÜÊï∏ÁöÑÊï∏ÊìöÈõÜ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÁ¥ÆÂØ¶È°ûÂûãÂ≠∏ÊñπÊ≥ïÂèØ‰ª•Êèê‰æõÈóúÊñºË™ûË®Ä‰∏≠Ë™ûÁæ©ÂäüËÉΩÁöÑÈáèÂåñË≠âÊìö„ÄÇ

##### **Apollo: An Exploration of Video Understanding in Large Multimodal Models**
2412.10360v1 by Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia

Despite the rapid integration of video perception capabilities into Large
Multimodal Models (LMMs), the underlying mechanisms driving their video
understanding remain poorly understood. Consequently, many design decisions in
this domain are made without proper justification or analysis. The high
computational cost of training and evaluating such models, coupled with limited
open research, hinders the development of video-LMMs. To address this, we
present a comprehensive study that helps uncover what effectively drives video
understanding in LMMs.
  We begin by critically examining the primary contributors to the high
computational requirements associated with video-LMM research and discover
Scaling Consistency, wherein design and training decisions made on smaller
models and datasets (up to a critical size) effectively transfer to larger
models. Leveraging these insights, we explored many video-specific aspects of
video-LMMs, including video sampling, architectures, data composition, training
schedules, and more. For example, we demonstrated that fps sampling during
training is vastly preferable to uniform frame sampling and which vision
encoders are the best for video representation.
  Guided by these findings, we introduce Apollo, a state-of-the-art family of
LMMs that achieve superior performance across different model sizes. Our models
can perceive hour-long videos efficiently, with Apollo-3B outperforming most
existing $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is
state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on
Video-MME.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂΩ±ÁâáÊÑüÁü•ËÉΩÂäõÂ∑≤Âø´ÈÄüÊï¥ÂêàËá≥Â§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM)Ôºå‰ΩÜÈ©ÖÂãïÂÖ∂ÂΩ±ÁâáÁêÜËß£ÁöÑÂ∫ïÂ±§Ê©üÂà∂‰ªçÈÆÆÁÇ∫‰∫∫Áü•„ÄÇÂõ†Ê≠§ÔºåÊ≠§È†òÂüü‰∏≠ÁöÑË®±Â§öË®≠Ë®àÊ±∫Á≠ñÈÉΩÊòØÂú®Áº∫‰πèÈÅ©Áï∂‰æùÊìöÊàñÂàÜÊûêÁöÑÊÉÖÊ≥Å‰∏ãÂÅöÂá∫ÁöÑ„ÄÇË®ìÁ∑¥ÂíåË©ï‰º∞Ê≠§È°ûÊ®°ÂûãÁöÑÈ´òÈÅãÁÆóÊàêÊú¨ÔºåÂä†‰∏äÊúâÈôêÁöÑÂÖ¨ÈñãÁ†îÁ©∂ÔºåÈòªÁ§ô‰∫ÜÂΩ±Áâá LMM ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂÖ®Èù¢ÊÄßÁ†îÁ©∂ÔºåÊúâÂä©ÊñºÊè≠Á§∫ÊúâÊïàÈ©ÖÂãï LMM ‰∏≠ÂΩ±ÁâáÁêÜËß£ÁöÑÂõ†Á¥†„ÄÇ
ÊàëÂÄëÈ¶ñÂÖàÊâπÂà§ÊÄßÂú∞ÂØ©Ë¶ñËàáÂΩ±Áâá LMM Á†îÁ©∂Áõ∏ÈóúÁöÑÈ´òÈÅãÁÆóÈúÄÊ±ÇÁöÑ‰∏ªË¶ÅÂõ†Á¥†Ôºå‰∏¶ÁôºÁèæË¶èÊ®°‰∏ÄËá¥ÊÄßÔºåÂÖ∂‰∏≠Âú®ËºÉÂ∞èÂûãËôüÂíåË≥áÊñôÈõÜÔºàÈÅîÂà∞Ëá®ÁïåË¶èÊ®°Ôºâ‰∏äÂÅöÂá∫ÁöÑË®≠Ë®àÂíåË®ìÁ∑¥Ê±∫Á≠ñÔºåÂèØÊúâÊïàËΩâÁßªÂà∞ËºÉÂ§ßÂûãËôü„ÄÇÈÅãÁî®ÈÄô‰∫õË¶ãËß£ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂΩ±Áâá LMM ÁöÑË®±Â§öÂΩ±ÁâáÁâπÂÆöÈù¢ÂêëÔºåÂåÖÊã¨ÂΩ±ÁâáÂèñÊ®£„ÄÅÊû∂Êßã„ÄÅË≥áÊñôÁµÑÊàê„ÄÅË®ìÁ∑¥ÊôÇÁ®ãÁ≠â„ÄÇ‰æãÂ¶ÇÔºåÊàëÂÄëË≠âÊòé‰∫ÜË®ìÁ∑¥ÊúüÈñìÁöÑ fps ÂèñÊ®£ÈÅ†ÊØîÂùáÂãªÁöÑÂΩ±Ê†ºÂèñÊ®£‰æÜÂæóÁêÜÊÉ≥Ôºå‰ª•ÂèäÂì™Á®ÆË¶ñË¶∫Á∑®Á¢ºÂô®ÊúÄÈÅ©ÂêàÂΩ±ÁâáË°®Âæµ„ÄÇ
Âú®ÈÄô‰∫õÁôºÁèæÁöÑÊåáÂ∞é‰∏ãÔºåÊàëÂÄëÊé®Âá∫‰∫Ü ApolloÔºå‰∏ÄÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LMM Á≥ªÂàóÔºåÂèØÂú®‰∏çÂêåÊ®°ÂûãË¶èÊ®°‰∏≠ÂØ¶ÁèæÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÊúâÊïàÊÑüÁü•Èï∑ÈÅî‰∏ÄÂ∞èÊôÇÁöÑÂΩ±ÁâáÔºåÂÖ∂‰∏≠ Apollo-3B ‰ª•‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑ LongVideoBench 55.1 ÂàÜÊï∏ÔºåË∂ÖË∂ä‰∫ÜÁèæÊúâÁöÑ $7$B Ê®°Âûã„ÄÇApollo-7B ‰ª• MLVU 70.9 ÂàÜÂíå Video-MME 63.3 ÂàÜÔºåËàá 7B LMM Áõ∏ÊØîÊòØÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑ„ÄÇ

##### **A Library for Learning Neural Operators**
2412.10354v1 by Jean Kossaifi, Nikola Kovachki, Zongyi Li, Davit Pitt, Miguel Liu-Schiaffini, Robert Joseph George, Boris Bonev, Kamyar Azizzadenesheli, Julius Berner, Anima Anandkumar

We present NeuralOperator, an open-source Python library for operator
learning. Neural operators generalize neural networks to maps between function
spaces instead of finite-dimensional Euclidean spaces. They can be trained and
inferenced on input and output functions given at various discretizations,
satisfying a discretization convergence properties. Built on top of PyTorch,
NeuralOperator provides all the tools for training and deploying neural
operator models, as well as developing new ones, in a high-quality, tested,
open-source package. It combines cutting-edge models and customizability with a
gentle learning curve and simple user interface for newcomers.

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫‰∫Ü NeuralOperatorÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÁÆóÂ≠êÂ≠∏ÁøíÁöÑÈñãÊ∫ê Python ÂáΩÂºèÂ∫´„ÄÇÁ•ûÁ∂ìÁÆóÂ≠êÂ∞áÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ¶ÇÊã¨ÁÇ∫ÂáΩÊï∏Á©∫Èñì‰πãÈñìÁöÑÊò†Â∞ÑÔºåËÄå‰∏çÊòØÊúâÈôêÁ∂≠ÁöÑÊ≠êÂπæÈáåÂæóÁ©∫Èñì„ÄÇÂÆÉÂÄëÂèØ‰ª•Âú®ÂêÑÁ®ÆÈõ¢Êï£Âåñ‰∏≠Áµ¶Âá∫ÁöÑËº∏ÂÖ•ÂíåËº∏Âá∫ÂáΩÊï∏‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÂíåÊé®Ë´ñÔºåÊªøË∂≥Èõ¢Êï£ÂåñÊî∂ÊñÇÁâπÊÄß„ÄÇNeuralOperator Âª∫ÊßãÂú® PyTorch ‰πã‰∏äÔºåÊèê‰æõË®ìÁ∑¥ÂíåÈÉ®ÁΩ≤Á•ûÁ∂ìÁÆóÂ≠êÊ®°Âûã‰ª•ÂèäÈñãÁôºÊñ∞Ê®°ÂûãÁöÑÊâÄÊúâÂ∑•ÂÖ∑Ôºå‰∏¶‰ª•È´òÂìÅË≥™„ÄÅÁ∂ìÈÅéÊ∏¨Ë©¶ÁöÑÈñãÊ∫êÂ•ó‰ª∂Êèê‰æõ„ÄÇÂÆÉÁµêÂêà‰∫ÜÂ∞ñÁ´ØÁöÑÊ®°ÂûãÂíåÂèØËá™Ë®ÇÊÄßÔºå‰∏¶ÁÇ∫Êñ∞ÊâãÊèê‰æõ‰∫ÜÂπ≥Á∑©ÁöÑÂ≠∏ÁøíÊõ≤Á∑öÂíåÁ∞°ÂñÆÁöÑ‰ΩøÁî®ËÄÖ‰ªãÈù¢„ÄÇ

##### **A dual contrastive framework**
2412.10348v1 by Yuan Sun, Zhao Zhang, Jorge Ortiz

In current multimodal tasks, models typically freeze the encoder and decoder
while adapting intermediate layers to task-specific goals, such as region
captioning. Region-level visual understanding presents significant challenges
for large-scale vision-language models. While limited spatial awareness is a
known issue, coarse-grained pretraining, in particular, exacerbates the
difficulty of optimizing latent representations for effective encoder-decoder
alignment. We propose AlignCap, a framework designed to enhance region-level
understanding through fine-grained alignment of latent spaces. Our approach
introduces a novel latent feature refinement module that enhances conditioned
latent space representations to improve region-level captioning performance. We
also propose an innovative alignment strategy, the semantic space alignment
module, which boosts the quality of multimodal representations. Additionally,
we incorporate contrastive learning in a novel manner within both modules to
further enhance region-level captioning performance. To address spatial
limitations, we employ a General Object Detection (GOD) method as a data
preprocessing pipeline that enhances spatial reasoning at the regional level.
Extensive experiments demonstrate that our approach significantly improves
region-level captioning performance across various tasks

ÊëòË¶ÅÔºöÂú®Áï∂ÂâçÁöÑÂ§öÊ®°ÊÖã‰ªªÂãô‰∏≠ÔºåÊ®°ÂûãÈÄöÂ∏∏ÊúÉÂáçÁµêÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÔºåÂêåÊôÇË™øÊï¥‰∏≠ÈñìÂ±§‰ª•ÈÅ©ÊáâÁâπÂÆö‰ªªÂãôÁöÑÁõÆÊ®ôÔºå‰æãÂ¶ÇÂçÄÂüüÊ®ôÈ°å„ÄÇÂçÄÂüüÁ¥öÂà•ÁöÑË¶ñË¶∫ÁêÜËß£Â∞çÂ§ßË¶èÊ®°Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊà∞„ÄÇÂÑòÁÆ°Â∑≤Áü•Á©∫ÈñìÊÑüÁü•ÊúâÈôêÊòØ‰∏ÄÂÄãÂïèÈ°åÔºå‰ΩÜÁ≤óÁ≤íÂ∫¶È†êË®ìÁ∑¥Â∞§ÂÖ∂ÊúÉÂä†ÂäáÂÑ™ÂåñÊΩõÂú®Ë°®Á§∫‰ª•ÂØ¶ÁèæÊúâÊïàÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Â∞çÈΩäÁöÑÈõ£Â∫¶„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü AlignCapÔºå‰∏ÄÂÄãÊó®Âú®ÈÄöÈÅéÊΩõÂú®Á©∫ÈñìÁöÑÁ¥∞Á≤íÂ∫¶Â∞çÈΩä‰æÜÂ¢ûÂº∑ÂçÄÂüüÁ¥öÂà•ÁêÜËß£ÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊΩõÂú®ÁâπÂæµÁ¥∞ÂåñÊ®°ÁµÑÔºåÂÆÉÂ¢ûÂº∑‰∫ÜÊ¢ù‰ª∂ÊΩõÂú®Á©∫ÈñìË°®Á§∫‰ª•ÊîπÂñÑÂçÄÂüüÁ¥öÂà•Ê®ôÈ°åÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∞çÈΩäÁ≠ñÁï•ÔºåË™ûÁæ©Á©∫ÈñìÂ∞çÈΩäÊ®°ÁµÑÔºåÂÆÉÊèêÂçá‰∫ÜÂ§öÊ®°ÊÖãË°®Á§∫ÁöÑÂìÅË≥™„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ÂÖ©ÂÄãÊ®°ÁµÑ‰∏≠‰ª•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÂºèÁ¥çÂÖ•‰∫ÜÂ∞çÊØîÂ≠∏ÁøíÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑ÂçÄÂüüÁ¥öÂà•ÁöÑÊ®ôÈ°åÊÄßËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Á©∫ÈñìÈôêÂà∂ÔºåÊàëÂÄëÊé°Áî®ÈÄöÁî®Áâ©‰ª∂ÂÅµÊ∏¨ (GOD) ÊñπÊ≥ï‰ΩúÁÇ∫Ë≥áÊñôÈ†êËôïÁêÜÁÆ°ÈÅìÔºåÂÆÉÂ¢ûÂº∑‰∫ÜÂçÄÂüüÁ¥öÂà•ÁöÑÁ©∫ÈñìÊé®ÁêÜ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÊîπÂñÑ‰∫ÜÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÂçÄÂüüÁ¥öÂà•Ê®ôÈ°åÊÄßËÉΩ

##### **COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**
2412.10347v1 by Yuchen Ren, Wenwei Han, Qianyuan Zhang, Yining Tang, Weiqiang Bai, Yuchen Cai, Lifeng Qiao, Hao Jiang, Dong Yuan, Tao Chen, Siqi Sun, Pan Tan, Wanli Ouyang, Nanqing Dong, Xinzhu Ma, Peng Ye

As key elements within the central dogma, DNA, RNA, and proteins play crucial
roles in maintaining life by guaranteeing accurate genetic expression and
implementation. Although research on these molecules has profoundly impacted
fields like medicine, agriculture, and industry, the diversity of machine
learning approaches-from traditional statistical methods to deep learning
models and large language models-poses challenges for researchers in choosing
the most suitable models for specific tasks, especially for cross-omics and
multi-omics tasks due to the lack of comprehensive benchmarks. To address this,
we introduce the first comprehensive multi-omics benchmark COMET (Benchmark for
Biological COmprehensive Multi-omics Evaluation Tasks and Language Models),
designed to evaluate models across single-omics, cross-omics, and multi-omics
tasks. First, we curate and develop a diverse collection of downstream tasks
and datasets covering key structural and functional aspects in DNA, RNA, and
proteins, including tasks that span multiple omics levels. Then, we evaluate
existing foundational language models for DNA, RNA, and proteins, as well as
the newly proposed multi-omics method, offering valuable insights into their
performance in integrating and analyzing data from different biological
modalities. This benchmark aims to define critical issues in multi-omics
research and guide future directions, ultimately promoting advancements in
understanding biological processes through integrated and different omics data
analysis.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫‰∏≠ÂøÉÊ≥ïÂâá‰∏≠ÁöÑÈóúÈçµÂÖÉÁ¥†ÔºåDNA„ÄÅRNA ÂíåËõãÁôΩË≥™Âú®Á∂≠ÊåÅÁîüÂëΩÊñπÈù¢ÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®ÔºåÂÆÉÂÄë‰øùË≠â‰∫ÜÊ∫ñÁ¢∫ÁöÑÂü∫Âõ†Ë°®ÈÅîÂíåÂØ¶ÊñΩ„ÄÇÂÑòÁÆ°Â∞çÈÄô‰∫õÂàÜÂ≠êÁöÑÁ†îÁ©∂Â∞çÈÜ´Â≠∏„ÄÅËæ≤Ê•≠ÂíåÂ∑•Ê•≠Á≠âÈ†òÂüüÁî¢Áîü‰∫ÜÊ∑±ÈÅ†ÁöÑÂΩ±ÈüøÔºå‰ΩÜÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÂ§öÊ®£ÊÄßÔºàÂæûÂÇ≥Áµ±ÁöÑÁµ±Ë®àÊñπÊ≥ïÂà∞Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÔºâÂ∞çÁ†îÁ©∂‰∫∫Âì°Âú®ÁÇ∫ÁâπÂÆö‰ªªÂãôÈÅ∏ÊìáÊúÄÂêàÈÅ©ÁöÑÊ®°ÂûãÊèêÂá∫‰∫ÜÊåëÊà∞ÔºåÁâπÂà•ÊòØÂ∞çÊñºÁµÑÂ≠∏ÈñìÂíåÂ§öÁµÑÂ≠∏‰ªªÂãôÔºåÈÄôÊòØÁî±ÊñºÁº∫‰πèÂÖ®Èù¢ÁöÑÂü∫Ê∫ñ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ¨¨‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂ§öÁµÑÂ≠∏Âü∫Ê∫ñ COMETÔºàÁîüÁâ©Á∂úÂêàÂ§öÁµÑÂ≠∏Ë©ï‰º∞‰ªªÂãôÂíåË™ûË®ÄÊ®°ÂûãÂü∫Ê∫ñÔºâÔºåÊó®Âú®Ë©ï‰º∞Ê®°ÂûãÂú®ÂñÆÁµÑÂ≠∏„ÄÅÁµÑÂ≠∏ÈñìÂíåÂ§öÁµÑÂ≠∏‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÁ≠ñÂäÉ‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÁ≥ªÂàóÂ§öÊ®£ÁöÑ‰∏ãÊ∏∏‰ªªÂãôÂíåÊï∏ÊìöÈõÜÔºåÊ∂µËìã‰∫Ü DNA„ÄÅRNA ÂíåËõãÁôΩË≥™‰∏≠ÁöÑÈóúÈçµÁµêÊßãÂíåÂäüËÉΩÊñπÈù¢ÔºåÂåÖÊã¨Ë∑®Ë∂äÂ§öÂÄãÁµÑÂ≠∏Â±§Á¥öÁöÑ‰ªªÂãô„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÁèæÊúâÁöÑ DNA„ÄÅRNA ÂíåËõãÁôΩË≥™Âü∫Á§éË™ûË®ÄÊ®°ÂûãÔºå‰ª•ÂèäÊñ∞ÊèêÂá∫ÁöÑÂ§öÁµÑÂ≠∏ÊñπÊ≥ïÔºåÂ∞çÂÆÉÂÄëÊï¥ÂêàÂíåÂàÜÊûê‰æÜËá™‰∏çÂêåÁîüÁâ©Ê®°ÂºèÁöÑÊï∏ÊìöÁöÑÊÄßËÉΩÊèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇÈÄôÂÄãÂü∫Ê∫ñÊó®Âú®ÂÆöÁæ©Â§öÁµÑÂ≠∏Á†îÁ©∂‰∏≠ÁöÑÈóúÈçµÂïèÈ°å‰∏¶ÊåáÂ∞éÊú™‰æÜÁöÑÊñπÂêëÔºåÊúÄÁµÇÈÄöÈÅéÊï¥ÂêàÂíå‰∏çÂêåÁöÑÁµÑÂ≠∏Êï∏ÊìöÂàÜÊûê‰øÉÈÄ≤Â∞çÁîüÁâ©ÈÅéÁ®ãÁöÑÁêÜËß£„ÄÇ

##### **TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies**
2412.10345v1 by Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daum√© III, Andrey Kolobov, Furong Huang, Jianwei Yang

Although large vision-language-action (VLA) models pretrained on extensive
robot datasets offer promising generalist policies for robotic learning, they
still struggle with spatial-temporal dynamics in interactive robotics, making
them less effective in handling complex tasks, such as manipulation. In this
work, we introduce visual trace prompting, a simple yet effective approach to
facilitate VLA models' spatial-temporal awareness for action prediction by
encoding state-action trajectories visually. We develop a new TraceVLA model by
finetuning OpenVLA on our own collected dataset of 150K robot manipulation
trajectories using visual trace prompting. Evaluations of TraceVLA across 137
configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate
state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and
3.5x on real-robot tasks and exhibiting robust generalization across diverse
embodiments and scenarios. To further validate the effectiveness and generality
of our method, we present a compact VLA model based on 4B Phi-3-Vision,
pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B
OpenVLA baseline while significantly improving inference efficiency.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®Âª£Ê≥õÊ©üÂô®‰∫∫Ë≥áÊñôÈõÜ‰∏äÈ†êË®ìÁ∑¥ÁöÑÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÂãï‰Ωú (VLA) Ê®°ÂûãÁÇ∫Ê©üÂô®‰∫∫Â≠∏ÁøíÊèê‰æõ‰∫ÜÊúâÂâçÈÄîÁöÑÈÄöÊâçÁ≠ñÁï•Ôºå‰ΩÜÂÆÉÂÄëÂú®‰∫íÂãïÊ©üÂô®‰∫∫‰∏≠ÁöÑÊôÇÁ©∫ÂãïÊÖã‰∏ä‰ªçÊúâÂõ∞Èõ£ÔºåÈÄô‰ΩøÂæóÂÆÉÂÄëÂú®ËôïÁêÜË§áÈõú‰ªªÂãôÔºà‰æãÂ¶ÇÊìç‰ΩúÔºâÊôÇÊïàÁéáËºÉ‰Ωé„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜË¶ñË¶∫ËªåË∑°ÊèêÁ§∫ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÈÄöÈÅéË¶ñË¶∫Á∑®Á¢ºÁãÄÊÖãÂãï‰ΩúËªåË∑°‰æÜ‰øÉÈÄ≤ VLA Ê®°ÂûãÁöÑÊôÇÁ©∫ÊÑüÁü•Ôºå‰ª•ÈÄ≤Ë°åÂãï‰ΩúÈ†êÊ∏¨„ÄÇÊàëÂÄëÈÄöÈÅé‰ΩøÁî®Ë¶ñË¶∫ËªåË∑°ÊèêÁ§∫ÔºåÂú®ÊàëÂÄëËá™Â∑±Êî∂ÈõÜÁöÑ 150K Ê©üÂô®‰∫∫Êìç‰ΩúËªåË∑°Êï∏ÊìöÈõÜ‰∏äÂ∞ç OpenVLA ÈÄ≤Ë°åÂæÆË™øÔºåÈñãÁôº‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑ TraceVLA Ê®°Âûã„ÄÇTraceVLA Âú® SimplerEnv ‰∏≠ÁöÑ 137 Á®ÆÈÖçÁΩÆÂíåÁâ©ÁêÜ WidowX Ê©üÂô®‰∫∫‰∏äÁöÑ 4 È†Ö‰ªªÂãôÁöÑË©ï‰º∞Ë≠âÊòé‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂú® SimplerEnv ‰∏äÊØî OpenVLA È´òÂá∫ 10%ÔºåÂú®ÁúüÂØ¶Ê©üÂô®‰∫∫‰ªªÂãô‰∏äÈ´òÂá∫ 3.5 ÂÄçÔºå‰∏¶Âú®‰∏çÂêåÁöÑÂÖ∑È´îÂåñÂíåÂ†¥ÊôØ‰∏≠Ë°®ÁèæÂá∫Âº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•È©óË≠âÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÊôÆÈÅçÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº 4B Phi-3-Vision ÁöÑÁ∑äÊπä VLA Ê®°ÂûãÔºåÂú® Open-X-Embodiment ‰∏äÈ†êË®ìÁ∑¥‰∏¶Âú®ÊàëÂÄëÁöÑÊï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åÂæÆË™øÔºåËàá 7B OpenVLA Âü∫Á∑öÁõ∏Â™≤ÁæéÔºåÂêåÊôÇÈ°ØËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇ

##### **Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**
2412.10342v1 by Zhiqi Ge, Juncheng Li, Xinglei Pang, Minghe Gao, Kaihang Pan, Wang Lin, Hao Fei, Wenqiao Zhang, Siliang Tang, Yueting Zhuang

Digital agents are increasingly employed to automate tasks in interactive
digital environments such as web pages, software applications, and operating
systems. While text-based agents built on Large Language Models (LLMs) often
require frequent updates due to platform-specific APIs, visual agents
leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability
by interacting directly with Graphical User Interfaces (GUIs). However, these
agents face significant challenges in visual perception, particularly when
handling high-resolution, visually complex digital environments. This paper
introduces Iris, a foundational visual agent that addresses these challenges
through two key innovations: Information-Sensitive Cropping (ISC) and
Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes
visually dense regions using a edge detection algorithm, enabling efficient
processing by allocating more computational resources to areas with higher
information density. SRDL enhances the agent's ability to handle complex tasks
by leveraging a dual-learning loop, where improvements in referring (describing
UI elements) reinforce grounding (locating elements) and vice versa, all
without requiring additional annotated data. Empirical evaluations demonstrate
that Iris achieves state-of-the-art performance across multiple benchmarks with
only 850K GUI annotations, outperforming methods using 10x more training data.
These improvements further translate to significant gains in both web and OS
agent downstream tasks.

ÊëòË¶ÅÔºöÊï∏‰Ωç‰ª£ÁêÜ‰∫∫ÊÑà‰æÜÊÑàÂ∏∏Ë¢´Áî®‰æÜËá™ÂãïÂåñ‰∫íÂãïÂºèÊï∏‰ΩçÁí∞Â¢É‰∏≠ÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÁ∂≤È†Å„ÄÅËªüÈ´îÊáâÁî®Á®ãÂºèÂíå‰ΩúÊ•≠Á≥ªÁµ±„ÄÇÈõñÁÑ∂Âª∫Á´ãÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏äÁöÑÊñáÂ≠ó‰ª£ÁêÜ‰∫∫ÈÄöÂ∏∏ÈúÄË¶ÅÈ†ªÁπÅÊõ¥Êñ∞Ôºå‰ª•Á¨¶ÂêàÁâπÂÆöÂπ≥Âè∞ÁöÑ APIÔºå‰ΩÜÂà©Áî®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÁöÑË¶ñË¶∫‰ª£ÁêÜ‰∫∫ÈÄèÈÅéÁõ¥Êé•ËàáÂúñÂΩ¢‰ΩøÁî®ËÄÖ‰ªãÈù¢ (GUI) ‰∫íÂãïÔºåÊèê‰æõÂ¢ûÂº∑ÁöÑÈÅ©ÊáâÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õ‰ª£ÁêÜ‰∫∫Âú®Ë¶ñË¶∫ÊÑüÁü•ÊñπÈù¢Èù¢Ëá®ÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ËôïÁêÜÈ´òËß£ÊûêÂ∫¶„ÄÅË¶ñË¶∫Ë§áÈõúÁöÑÊï∏‰ΩçÁí∞Â¢ÉÊôÇ„ÄÇÊú¨Êñá‰ªãÁ¥π IrisÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Á§éË¶ñË¶∫‰ª£ÁêÜ‰∫∫ÔºåÈÄèÈÅéÂÖ©È†Ö‰∏ªË¶ÅÂâµÊñ∞‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºöË≥áË®äÊïèÊÑüË£ÅÂàá (ISC) ÂíåËá™ÊàëÁ≤æÈÄ≤ÈõôÈáçÂ≠∏Áøí (SRDL)„ÄÇISC ‰ΩøÁî®ÈÇäÁ∑£ÂÅµÊ∏¨ÊºîÁÆóÊ≥ïÂãïÊÖãË≠òÂà•ÂíåÂÑ™ÂÖàËôïÁêÜË¶ñË¶∫ÂØÜÈõÜÂçÄÂüüÔºåÈÄèÈÅéÂ∞áÊõ¥Â§öÈÅãÁÆóË≥áÊ∫êÈÖçÁΩÆÂà∞Ë≥áË®äÂØÜÂ∫¶ËºÉÈ´òÁöÑÂçÄÂüüÔºåÂØ¶ÁèæÊúâÊïàÁéáÁöÑËôïÁêÜ„ÄÇSRDL ÈÄèÈÅéÂà©Áî®ÈõôÈáçÂ≠∏ÁøíËø¥Âúà‰æÜÂ¢ûÂº∑‰ª£ÁêÜ‰∫∫ËôïÁêÜË§áÈõú‰ªªÂãôÁöÑËÉΩÂäõÔºåÂÖ∂‰∏≠ÂèÉÁÖßÔºàÊèèËø∞ UI ÂÖÉÁ¥†ÔºâÁöÑÊîπÈÄ≤Âº∑Âåñ‰∫ÜÂü∫Á§éÔºàÂÆö‰ΩçÂÖÉÁ¥†ÔºâÔºåÂèç‰πã‰∫¶ÁÑ∂ÔºåËÄå‰∏îÊâÄÊúâÈÄô‰∫õÈÉΩ‰∏çÈúÄË¶ÅÈ°çÂ§ñÁöÑË®ªËß£Ë≥áÊñô„ÄÇÁ∂ìÈ©óË©ï‰º∞È°ØÁ§∫ÔºåIris ÂÉÖ‰ΩøÁî® 850K GUI Ë®ªËß£ÔºåÂ∞±Âú®Â§öÂÄãÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåË°®ÁèæÂÑ™Êñº‰ΩøÁî®Â§ö 10 ÂÄçË®ìÁ∑¥Ë≥áÊñôÁöÑÊñπÊ≥ï„ÄÇÈÄô‰∫õÊîπÈÄ≤ÈÄ≤‰∏ÄÊ≠•ËΩâÂåñÁÇ∫Á∂≤È†ÅÂíå‰ΩúÊ•≠Á≥ªÁµ±‰ª£ÁêÜ‰∫∫‰∏ãÊ∏∏‰ªªÂãôÁöÑÈ°ØËëóÊî∂Áõä„ÄÇ

##### **Generative AI in Medicine**
2412.10337v1 by Divya Shanmugam, Monica Agrawal, Rajiv Movva, Irene Y. Chen, Marzyeh Ghassemi, Emma Pierson

The increased capabilities of generative AI have dramatically expanded its
possible use cases in medicine. We provide a comprehensive overview of
generative AI use cases for clinicians, patients, clinical trial organizers,
researchers, and trainees. We then discuss the many challenges -- including
maintaining privacy and security, improving transparency and interpretability,
upholding equity, and rigorously evaluating models -- which must be overcome to
realize this potential, and the open research directions they give rise to.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI ÁöÑËÉΩÂäõÊèêÂçáÂ§ßÂπÖÊì¥Â±ï‰∫ÜÂÖ∂Âú®ÈÜ´Â≠∏‰∏≠ÁöÑÊΩõÂú®ÊáâÁî®Ê°à‰æã„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊ¶ÇËßÄÔºåË™™ÊòéÁîüÊàêÂºè AI Âú®Ëá®Â∫äÈÜ´Áîü„ÄÅÊÇ£ËÄÖ„ÄÅËá®Â∫äË©¶È©óÁµÑÁπîËÄÖ„ÄÅÁ†îÁ©∂‰∫∫Âì°ÂíåÂèóË®ì‰∫∫Âì°ÁöÑÊáâÁî®Ê°à‰æã„ÄÇÊé•ËëóÔºåÊàëÂÄëË®éË´ñ‰∫ÜË®±Â§öÊåëÊà∞ÔºåÂåÖÊã¨Á∂≠Ë≠∑Èö±ÁßÅÂíåÂÆâÂÖ®ÊÄß„ÄÅÊèêÂçáÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÅÁ∂≠Ë≠∑ÂÖ¨Âπ≥ÊÄßÔºå‰ª•ÂèäÂö¥Ê†ºË©ï‰º∞Ê®°ÂûãÔºåÈÄô‰∫õÊåëÊà∞ÂøÖÈ†àÂÖãÊúçÊâçËÉΩÂØ¶ÁèæÈÄôÁ®ÆÊΩõÂäõÔºå‰ª•ÂèäÂÆÉÂÄëÂºïÁôºÁöÑÈñãÊîæÁ†îÁ©∂ÊñπÂêë„ÄÇ

##### **AdvPrefix: An Objective for Nuanced LLM Jailbreaks**
2412.10321v1 by Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, Ivan Evtimov

Many jailbreak attacks on large language models (LLMs) rely on a common
objective: making the model respond with the prefix "Sure, here is (harmful
request)". While straightforward, this objective has two limitations: limited
control over model behaviors, often resulting in incomplete or unrealistic
responses, and a rigid format that hinders optimization. To address these
limitations, we introduce AdvPrefix, a new prefix-forcing objective that
enables more nuanced control over model behavior while being easy to optimize.
Our objective leverages model-dependent prefixes, automatically selected based
on two criteria: high prefilling attack success rates and low negative
log-likelihood. It can further simplify optimization by using multiple prefixes
for a single user request. AdvPrefix can integrate seamlessly into existing
jailbreak attacks to improve their performance for free. For example, simply
replacing GCG attack's target prefixes with ours on Llama-3 improves nuanced
attack success rates from 14% to 80%, suggesting that current alignment
struggles to generalize to unseen prefixes. Our work demonstrates the
importance of jailbreak objectives in achieving nuanced jailbreaks.

ÊëòË¶ÅÔºöË®±Â§öÈáùÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË∂äÁçÑÊîªÊìäÈÉΩ‰æùË≥¥Êñº‰∏ÄÂÄãÂÖ±ÂêåÁõÆÊ®ôÔºöËÆìÊ®°Âûã‰ª•„ÄåÂ•ΩÁöÑÔºå‰ª•‰∏ãÊòØ (ÊúâÂÆ≥Ë´ãÊ±Ç)„ÄçÁöÑÂâçÁ∂¥ÂõûÊáâ„ÄÇÈõñÁÑ∂Áõ¥Êé•‰∫ÜÁï∂Ôºå‰ΩÜÊ≠§ÁõÆÊ®ôÊúâÂÖ©ÂÄãÈôêÂà∂ÔºöÂ∞çÊ®°ÂûãË°åÁÇ∫ÁöÑÊéßÂà∂ÊúâÈôêÔºåÈÄöÂ∏∏ÊúÉÂ∞éËá¥‰∏çÂÆåÊï¥Êàñ‰∏çÂàáÂØ¶ÈöõÁöÑÂõûÊáâÔºå‰ª•ÂèäÈòªÁ§ôÊúÄ‰Ω≥ÂåñÁöÑÂÉµÂåñÊ†ºÂºè„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü AdvPrefixÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑÂâçÁ∂¥Âº∑Âà∂ÁõÆÊ®ôÔºåÂèØ‰ª•Âú®ÊòìÊñºÊúÄ‰Ω≥ÂåñÁöÑÂêåÊôÇÔºåÂ∞çÊ®°ÂûãË°åÁÇ∫ÈÄ≤Ë°åÊõ¥Á¥∞Á∑ªÁöÑÊéßÂà∂„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÂà©Áî®‰∫Ü‰æùÊìöÂÖ©ÂÄãÊ®ôÊ∫ñËá™ÂãïÈÅ∏ÂèñÁöÑÊ®°Âûã‰æùË≥¥ÂâçÁ∂¥ÔºöÈ´òÈ†êÂ°´ÂÖÖÊîªÊìäÊàêÂäüÁéáÂíå‰ΩéË≤†Â∞çÊï∏‰ººÁÑ∂Â∫¶„ÄÇÂÆÉÂèØ‰ª•ÈÄèÈÅéÂ∞çÂñÆ‰∏Ä‰ΩøÁî®ËÄÖË´ãÊ±Ç‰ΩøÁî®Â§öÂÄãÂâçÁ∂¥ÔºåÈÄ≤‰∏ÄÊ≠•Á∞°ÂåñÊúÄ‰Ω≥Âåñ„ÄÇAdvPrefix ÂèØ‰ª•ÁÑ°Á∏´Êï¥ÂêàÂà∞ÁèæÊúâÁöÑË∂äÁçÑÊîªÊìä‰∏≠Ôºå‰ª•ÂÖçË≤ªÊèêÂçáÂÖ∂ÊïàËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂÉÖÂÉÖÁî®ÊàëÂÄë Llama-3 ‰∏äÁöÑÁõÆÊ®ôÂâçÁ∂¥Âèñ‰ª£ GCG ÊîªÊìäÁöÑÁõÆÊ®ôÂâçÁ∂¥ÔºåÂ∞±ËÉΩÂ∞áÁ¥∞Á∑ªÁöÑÊîªÊìäÊàêÂäüÁéáÂæû 14% ÊèêÂçáËá≥ 80%ÔºåÈÄôË°®Á§∫ÁõÆÂâçÁöÑÊØîÂ∞çÈõ£‰ª•Ê¶ÇÊã¨Âà∞Êú™Ë¶ãÁöÑÂâçÁ∂¥„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â±ïÁ§∫‰∫ÜË∂äÁçÑÁõÆÊ®ôÂú®ÈÅîÊàêÁ¥∞Á∑ªË∂äÁçÑ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **SCBench: A KV Cache-Centric Analysis of Long-Context Methods**
2412.10319v1 by Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu

Long-context LLMs have enabled numerous downstream applications but also
introduced significant challenges related to computational and memory
efficiency. To address these challenges, optimizations for long-context
inference have been developed, centered around the KV cache. However, existing
benchmarks often evaluate in single-request, neglecting the full lifecycle of
the KV cache in real-world use. This oversight is particularly critical, as KV
cache reuse has become widely adopted in LLMs inference frameworks, such as
vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,
Google, and Anthropic. To address this gap, we introduce
SCBench(SharedContextBench), a comprehensive benchmark for evaluating
long-context methods from a KV cachecentric perspective: 1) KV cache
generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache
loading. Specifically, SCBench uses test examples with shared context, ranging
12 tasks with two shared context modes, covering four categories of
long-context capabilities: string retrieval, semantic retrieval, global
information, and multi-task. With it, we provide an extensive KV cache-centric
analysis of eight categories long-context solutions, including Gated Linear
RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,
KV cache dropping, quantization, retrieval, loading, and prompt compression.
The evaluation is conducted on 8 long-context LLMs. Our findings show that
sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding
with O(n) memory and sub-O(n^2) pre-filling computation perform robustly.
Dynamic sparsity yields more expressive KV caches than static patterns, and
layer-level sparsity in hybrid architectures reduces memory usage with strong
performance. Additionally, we identify attention distribution shift issues in
long-generation scenarios. https://aka.ms/SCBench.

ÊëòË¶ÅÔºöÈï∑Ë™ûÂ¢É LLM Â∑≤ÂïüÁî®Ë®±Â§ö‰∏ãÊ∏∏ÊáâÁî®Á®ãÂºèÔºå‰ΩÜ‰πüÂºïÂÖ•‰∫ÜËàáÈÅãÁÆóÂíåË®òÊÜ∂È´îÊïàÁéáÁõ∏ÈóúÁöÑÈáçÂ§ßÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÂ∑≤ÈáùÂ∞çÈï∑Ë™ûÂ¢ÉÊé®Ë´ñÈñãÁôºÊúÄ‰Ω≥ÂåñÔºåÈõÜ‰∏≠Âú® KV Âø´Âèñ‰∏ä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÈÄöÂ∏∏Âú®ÂñÆ‰∏ÄË¶ÅÊ±Ç‰∏≠ÈÄ≤Ë°åË©ï‰º∞ÔºåÂøΩÁï•‰∫Ü KV Âø´ÂèñÂú®ÂØ¶Èöõ‰ΩøÁî®‰∏≠ÁöÑÂÆåÊï¥ÁîüÂëΩÈÄ±Êúü„ÄÇÈÄôÁ®ÆÁñèÂøΩÁâπÂà•ÈóúÈçµÔºåÂõ†ÁÇ∫ KV Âø´ÂèñÈáçË§á‰ΩøÁî®Â∑≤Âú® LLM Êé®Ë´ñÊû∂Êßã‰∏≠Âª£Ê≥õÊé°Áî®Ôºå‰æãÂ¶Ç vLLM Âíå SGLangÔºå‰ª•Âèä LLM ‰æõÊáâÂïÜÔºåÂåÖÊã¨ OpenAI„ÄÅMicrosoft„ÄÅGoogle Âíå Anthropic„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü SCBenchÔºàSharedContextBenchÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÁî® KV Âø´ÂèñÁÇ∫‰∏≠ÂøÉÁöÑËßÄÈªûË©ï‰º∞Èï∑Ë™ûÂ¢ÉÊñπÊ≥ïÔºö1) KV Âø´ÂèñÁî¢ÁîüÔºå2) KV Âø´ÂèñÂ£ìÁ∏ÆÔºå3) KV Âø´ÂèñÊì∑ÂèñÔºå4) KV Âø´ÂèñËºâÂÖ•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåSCBench ‰ΩøÁî®ÂÖ∑ÊúâÂÖ±Áî®Ë™ûÂ¢ÉÁöÑÊ∏¨Ë©¶ÁØÑ‰æãÔºåÁØÑÂúçÊ∂µËìãÂÖ©Á®ÆÂÖ±Áî®Ë™ûÂ¢ÉÊ®°ÂºèÁöÑ 12 ÂÄã‰ªªÂãôÔºåÊ∂µËìãÈï∑Ë™ûÂ¢ÉÂäüËÉΩÁöÑÂõõÁ®ÆÈ°ûÂà•ÔºöÂ≠ó‰∏≤Êì∑Âèñ„ÄÅË™ûÊÑèÊì∑Âèñ„ÄÅÂÖ®ÂüüË≥áË®äÂíåÂ§ö‰ªªÂãô„ÄÇÈÄèÈÅéÂÆÉÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂ∞çÂÖ´Á®ÆÈ°ûÂà•Èï∑Ë™ûÂ¢ÉËß£Ê±∫ÊñπÊ°àÁöÑÂª£Ê≥õ KV Âø´ÂèñÁÇ∫‰∏≠ÂøÉÂàÜÊûêÔºåÂåÖÊã¨ÈñòÊéßÁ∑öÊÄß RNN„ÄÅMamba-Attention Ê∑∑ÂêàÈ´îÔºå‰ª•ÂèäÁ®ÄÁñèÊ≥®ÊÑèÂäõ„ÄÅKV Âø´ÂèñÊç®Ê£Ñ„ÄÅÈáèÂåñ„ÄÅÊì∑Âèñ„ÄÅËºâÂÖ•ÂíåÊèêÁ§∫Â£ìÁ∏ÆÁ≠âÊúâÊïàÊñπÊ≥ï„ÄÇË©ï‰º∞ÊòØÂú® 8 ÂÄãÈï∑Ë™ûÂ¢É LLM ‰∏äÈÄ≤Ë°åÁöÑ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºåsub-O(n) Ë®òÊÜ∂È´îÊñπÊ≥ïÂú®Â§öËº™ÊÉÖÊ≥Å‰∏ãÊúÉÂèóÂà∞ÂΩ±ÈüøÔºåËÄåÂÖ∑Êúâ O(n) Ë®òÊÜ∂È´îÂíå sub-O(n^2) È†êÂÖàÂ°´ÂÖ•ÈÅãÁÆóÁöÑÁ®ÄÁñèÁ∑®Á¢ºÂâáË°®ÁèæÂæóÂæàÂ•Ω„ÄÇÂãïÊÖãÁ®ÄÁñèÊÄßÁî¢ÁîüÊØîÈùúÊÖãÊ®°ÂºèÊõ¥ÂÖ∑Ë°®ÁèæÂäõÁöÑ KV Âø´ÂèñÔºåËÄåÊ∑∑ÂêàÊû∂Êßã‰∏≠ÁöÑÂ±§Á¥öÁ®ÄÁñèÊÄßÂâá‰ª•Âº∑ÂãÅÁöÑÊïàËÉΩÈôç‰ΩéË®òÊÜ∂È´î‰ΩøÁî®Èáè„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®Èï∑ÁîüÊàêÊÉÖÊ≥Å‰∏≠ÁôºÁèæ‰∫ÜÊ≥®ÊÑèÂäõÂàÜ‰ΩàËΩâÁßªÂïèÈ°å„ÄÇhttps://aka.ms/SCBench„ÄÇ

##### **BrushEdit: All-In-One Image Inpainting and Editing**
2412.10316v1 by Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, Qiang Xu

Image editing has advanced significantly with the development of diffusion
models using both inversion-based and instruction-based methods. However,
current inversion-based approaches struggle with big modifications (e.g.,
adding or removing objects) due to the structured nature of inversion noise,
which hinders substantial changes. Meanwhile, instruction-based methods often
constrain users to black-box operations, limiting direct interaction for
specifying editing regions and intensity. To address these limitations, we
propose BrushEdit, a novel inpainting-based instruction-guided image editing
paradigm, which leverages multimodal large language models (MLLMs) and image
inpainting models to enable autonomous, user-friendly, and interactive
free-form instruction editing. Specifically, we devise a system enabling
free-form instruction editing by integrating MLLMs and a dual-branch image
inpainting model in an agent-cooperative framework to perform editing category
classification, main object identification, mask acquisition, and editing area
inpainting. Extensive experiments show that our framework effectively combines
MLLMs and inpainting models, achieving superior performance across seven
metrics including mask region preservation and editing effect coherence.

ÊëòË¶ÅÔºöÂΩ±ÂÉèÁ∑®ËºØÈö®Ëëó‰ΩøÁî®Âü∫ÊñºÂèçÊºîÂíåÂü∫ÊñºÊåá‰ª§ÊñπÊ≥ïÁöÑÊì¥Êï£Ê®°ÂûãÁöÑÁôºÂ±ïËÄåÈ°ØËëóÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁï∂ÂâçÁöÑÂü∫ÊñºÂèçÊºîÁöÑÊñπÊ≥ïÁî±ÊñºÂèçÊºîÈõúË®äÁöÑÁµêÊßãÊÄßË≥™ËÄåÈõ£‰ª•ÈÄ≤Ë°åÂ§ßÂπÖ‰øÆÊîπÔºà‰æãÂ¶ÇÔºåÂ¢ûÂä†ÊàñÁßªÈô§Áâ©‰ª∂ÔºâÔºåÈÄôÈòªÁ§ô‰∫ÜÂØ¶Ë≥™ÊÄßÁöÑËÆäÊõ¥„ÄÇÂêåÊôÇÔºåÂü∫ÊñºÊåá‰ª§ÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈôêÂà∂‰ΩøÁî®ËÄÖÂè™ËÉΩÈÄ≤Ë°åÈªëÁõíÂ≠êÊìç‰ΩúÔºåÈôêÂà∂‰∫ÜÁõ¥Êé•‰∫íÂãï‰ª•ÊåáÂÆöÁ∑®ËºØÂçÄÂüüÂíåÂº∑Â∫¶„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü BrushEditÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÂÖßÊèíÁöÑÊåá‰ª§ÂºïÂ∞éÂΩ±ÂÉèÁ∑®ËºØÁØÑ‰æãÔºåÂÆÉÂà©Áî®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÂíåÂΩ±ÂÉèÂÖßÊèíÊ®°Âûã‰æÜÂØ¶ÁèæËá™‰∏ª„ÄÅ‰ΩøÁî®ËÄÖÂèãÂñÑ‰∏î‰∫íÂãïÁöÑËá™Áî±ÂΩ¢ÂºèÊåá‰ª§Á∑®ËºØ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁ≥ªÁµ±ÔºåÈÄèÈÅéÊï¥Âêà MLLM Âíå‰∏ÄÂÄãÈõôÂàÜÊîØÂΩ±ÂÉèÂÖßÊèíÊ®°ÂûãÂú®‰∏ÄÂÄã‰ª£ÁêÜÂêà‰ΩúÊû∂Êßã‰∏≠‰æÜÂü∑Ë°åÁ∑®ËºØÈ°ûÂà•ÂàÜÈ°û„ÄÅ‰∏ªÁâ©‰ª∂Ë≠òÂà•„ÄÅÈÅÆÁΩ©Êì∑ÂèñÂíåÁ∑®ËºØÂçÄÂüüÂÖßÊèíÔºåÂæûËÄåÂØ¶ÁèæËá™Áî±ÂΩ¢ÂºèÊåá‰ª§Á∑®ËºØ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊû∂ÊßãÊúâÊïàÂú∞ÁµêÂêà‰∫Ü MLLM ÂíåÂÖßÊèíÊ®°ÂûãÔºåÂú®‰∏ÉÈ†ÖÊåáÊ®ô‰∏≠ÂØ¶Áèæ‰∫ÜÂçìË∂äÁöÑÊïàËÉΩÔºåÂåÖÊã¨ÈÅÆÁΩ©ÂçÄÂüü‰øùÁïôÂíåÁ∑®ËºØÊïàÊûú‰∏ÄËá¥ÊÄß„ÄÇ

##### **Interlocking-free Selective Rationalization Through Genetic-based Learning**
2412.10312v1 by Federico Ruggeri, Gaetano Signorelli

A popular end-to-end architecture for selective rationalization is the
select-then-predict pipeline, comprising a generator to extract highlights fed
to a predictor. Such a cooperative system suffers from suboptimal equilibrium
minima due to the dominance of one of the two modules, a phenomenon known as
interlocking. While several contributions aimed at addressing interlocking,
they only mitigate its effect, often by introducing feature-based heuristics,
sampling, and ad-hoc regularizations. We present GenSPP, the first
interlocking-free architecture for selective rationalization that does not
require any learning overhead, as the above-mentioned. GenSPP avoids
interlocking by performing disjoint training of the generator and predictor via
genetic global search. Experiments on a synthetic and a real-world benchmark
show that our model outperforms several state-of-the-art competitors.

ÊëòË¶ÅÔºöÈÅ∏ÊìáÂºèÁ∞°ÂåñÁöÑ‰∏ÄÁ®ÆÊµÅË°åÁ´ØÂ∞çÁ´ØÊû∂ÊßãÊòØÈÅ∏ÊìáÂÜçÈ†êÊ∏¨ÁÆ°Á∑öÔºåÂåÖÂê´‰∏ÄÂÄãÁî¢ÁîüÂô®Áî®ÊñºËêÉÂèñ‰∫ÆÈªû‰∏¶Êèê‰æõÁµ¶È†êÊ∏¨Âô®„ÄÇÈÄôÁ®ÆÂêà‰ΩúÁ≥ªÁµ±ÊúÉÂõ†ÂÖ©ÂÄãÊ®°ÁµÑ‰πã‰∏ÄÁöÑÊîØÈÖçËÄåÂ∞éËá¥Ê¨°‰Ω≥Âπ≥Ë°°Ê•µÂ∞èÂÄºÔºåÈÄôÁ®ÆÁèæË±°Á®±ÁÇ∫‰∫íÈéñ„ÄÇÈõñÁÑ∂ÊúâË®±Â§öË≤¢ÁçªÊó®Âú®Ëß£Ê±∫‰∫íÈéñÔºå‰ΩÜÂÆÉÂÄëÂÉÖËÉΩÊ∏õËºïÂÖ∂ÂΩ±ÈüøÔºåÈÄöÂ∏∏ÊòØÈÄèÈÅéÂºïÂÖ•Âü∫ÊñºÁâπÂæµÁöÑÂïüÁôºÊ≥ï„ÄÅÂèñÊ®£ÂíåÁâπÂÆöË¶èÁØÑÂåñ„ÄÇÊàëÂÄëÊèêÂá∫ GenSPPÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÁÑ°‰∫íÈéñÊû∂ÊßãÔºåÁî®ÊñºÈÅ∏ÊìáÂºèÁ∞°ÂåñÔºå‰∏çÈúÄË¶Å‰ªª‰ΩïÂ≠∏ÁøíÈñãÈä∑ÔºåÂ¶Ç‰∏äËø∞„ÄÇGenSPP ÈÄèÈÅéÈÅ∫ÂÇ≥ÂÖ®ÁêÉÊêúÂ∞ãÂü∑Ë°åÁî¢ÁîüÂô®ÂíåÈ†êÊ∏¨Âô®ÁöÑÂàÜÈõ¢Ë®ìÁ∑¥Ôºå‰æÜÈÅøÂÖç‰∫íÈéñ„ÄÇÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåÂü∫Ê∫ñ‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂ§öÂÄãÊúÄÂÖàÈÄ≤ÁöÑÁ´∂Áà≠ËÄÖ„ÄÇ

##### **DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding**
2412.10302v1 by Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan

We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)
Vision-Language Models that significantly improves upon its predecessor,
DeepSeek-VL, through two key major upgrades. For the vision component, we
incorporate a dynamic tiling vision encoding strategy designed for processing
high-resolution images with different aspect ratios. For the language
component, we leverage DeepSeekMoE models with the Multi-head Latent Attention
mechanism, which compresses Key-Value cache into latent vectors, to enable
efficient inference and high throughput. Trained on an improved vision-language
dataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,
including but not limited to visual question answering, optical character
recognition, document/table/chart understanding, and visual grounding. Our
model series is composed of three variants: DeepSeek-VL2-Tiny,
DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated
parameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art
performance with similar or fewer activated parameters compared to existing
open-source dense and MoE-based models. Codes and pre-trained models are
publicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü DeepSeek-VL2ÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖàÈÄ≤ÁöÑÂ§ßÂûãÊ∑∑ÂêàÂ∞àÂÆ∂ (MoE) Á≥ªÂàóË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºåÈÄèÈÅéÂÖ©È†Ö‰∏ªË¶ÅÂçáÁ¥öÔºåÂ§ßÂπÖÊîπÈÄ≤ÂÖ∂ÂâçË∫´ DeepSeek-VL„ÄÇÂ∞çÊñºË¶ñË¶∫ÂÖÉ‰ª∂ÔºåÊàëÂÄëÁµêÂêàÂãïÊÖãÊãºË≤ºË¶ñË¶∫Á∑®Á¢ºÁ≠ñÁï•ÔºåÊó®Âú®ËôïÁêÜÂÖ∑Êúâ‰∏çÂêåÈï∑ÂØ¨ÊØîÁöÑÈ´òËß£ÊûêÂ∫¶ÂΩ±ÂÉè„ÄÇÂ∞çÊñºË™ûË®ÄÂÖÉ‰ª∂ÔºåÊàëÂÄëÂà©Áî®ÂÖ∑ÂÇôÂ§öÈ†≠ÊΩõÂú®Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑ DeepSeekMoE Ê®°ÂûãÔºåÂ∞áÈçµÂÄºÂø´ÂèñÂ£ìÁ∏ÆÊàêÊΩõÂú®ÂêëÈáèÔºå‰ª•ÂØ¶ÁèæÊúâÊïàÁéáÁöÑÊé®Ë´ñÂíåÈ´òÈÄöÈáè„ÄÇÂú®ÊîπËâØÁöÑË¶ñË¶∫Ë™ûË®ÄË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåDeepSeek-VL2 Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑËÉΩÂäõÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñºË¶ñË¶∫ÂïèÈ°åËß£Á≠î„ÄÅÂÖâÂ≠∏Â≠óÂÖÉËæ®Ë≠ò„ÄÅÊñá‰ª∂/Ë°®Ê†º/ÂúñË°®ÁêÜËß£ÂíåË¶ñË¶∫Âü∫Á§é„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÁ≥ªÂàóÁî±‰∏âÁ®ÆËÆäÈ´îÁµÑÊàêÔºöDeepSeek-VL2-Tiny„ÄÅDeepSeek-VL2-Small Âíå DeepSeek-VL2ÔºåÂàÜÂà•ÂÖ∑Êúâ 1.0B„ÄÅ2.8B Âíå 4.5B ÂÄãÂ∑≤ÂïüÁî®ÁöÑÂèÉÊï∏„ÄÇËàáÁèæÊúâÁöÑÈñãÊ∫êÂØÜÈõÜÂíåÂü∫Êñº MoE ÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåDeepSeek-VL2 ‰ª•Áõ∏ÂêåÊàñÊõ¥Â∞ëÁöÑÂ∑≤ÂïüÁî®ÂèÉÊï∏ÔºåÂèñÂæóÂÖ∑Á´∂Áà≠ÂäõÊàñÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂíåÈ†êË®ìÁ∑¥Ê®°ÂûãÂèØÂú® https://github.com/deepseek-ai/DeepSeek-VL2 ÂÖ¨ÈñãÂèñÂæó„ÄÇ

##### **Still "Talking About Large Language Models": Some Clarifications**
2412.10291v1 by Murray Shanahan

My paper "Talking About Large Language Models" has more than once been
interpreted as advocating a reductionist stance towards large language models.
But the paper was not intended that way, and I do not endorse such positions.
This short note situates the paper in the context of a larger philosophical
project that is concerned with the (mis)use of words rather than metaphysics,
in the spirit of Wittgenstein's later writing.

ÊëòË¶ÅÔºöÊàëÁöÑË´ñÊñá„ÄåË´áÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄçÂ∑≤‰∏çÊ≠¢‰∏ÄÊ¨°Ë¢´Ëß£ËÆÄÁÇ∫‰∏ªÂºµÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÊé°ÂèñÈÇÑÂéüË´ñÁ´ãÂ†¥„ÄÇ‰ΩÜÈÄôÁØáË´ñÊñá‰∏¶ÈùûÂ¶ÇÊ≠§Áî®ÊÑèÔºåÊàë‰πü‰∏çË™çÂêåÈÄôÁ®ÆÁ´ãÂ†¥„ÄÇÈÄôÁØáÁ∞°Áü≠ÁöÑË®ªËß£Â∞áÈÄôÁØáË´ñÊñáÂÆö‰ΩçÂú®‰∏ÄÂÄãËºÉÂ§ßÁöÑÂì≤Â≠∏Â∞àÊ°àÁöÑËÉåÊôØ‰∏ãÔºåÈÄôÂÄãÂ∞àÊ°àÈóúÊ≥®ÁöÑÊòØÔºàË™§ÔºâÁî®Ë©ûÂΩôÔºåËÄåÈùûÂΩ¢‰∏äÂ≠∏Ôºå‰∏¶ÁßâÊåÅÁ∂≠Ê†πÊñØÂù¶ÂæåÊúüËëó‰ΩúÁöÑÁ≤æÁ•û„ÄÇ

##### **One world, one opinion? The superstar effect in LLM responses**
2412.10281v1 by Sofie Goethals, Lauren Rhue

As large language models (LLMs) are shaping the way information is shared and
accessed online, their opinions have the potential to influence a wide
audience. This study examines who the LLMs view as the most prominent figures
across various fields, using prompts in ten different languages to explore the
influence of linguistic diversity. Our findings reveal low diversity in
responses, with a small number of figures dominating recognition across
languages (also known as the "superstar effect"). These results highlight the
risk of narrowing global knowledge representation when LLMs retrieve subjective
information.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂΩ¢Â°ëÁ∑ö‰∏äË≥áË®äÂàÜ‰∫´ËàáÂèñÂæóÁöÑÊñπÂºèÔºåÂÆÉÂÄëÁöÑËßÄÈªûÊúâÊΩõÂäõÂΩ±ÈüøÂª£Â§ßÂèóÁúæ„ÄÇÊú¨Á†îÁ©∂‰ΩøÁî®ÂçÅÁ®Æ‰∏çÂêåË™ûË®ÄÁöÑÊèêÁ§∫‰æÜÊé¢Ë®éË™ûË®ÄÂ§öÊ®£ÊÄßÁöÑÂΩ±ÈüøÔºåÊé¢Ë®é LLM Ë¶ñÁÇ∫ÂêÑÈ†òÂüüÊúÄÂÇëÂá∫‰∫∫Áâ©ÁöÑ‰∫∫ÈÅ∏„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂõûÊáâÁöÑÂ§öÊ®£ÊÄß‰ΩéÔºåÂ∞ëÊï∏‰∫∫Áâ©Âú®ÂêÑË™ûË®Ä‰∏≠ÈÉΩÁç≤ÂæóÊ•µÈ´òÁöÑË™çÂèØÔºà‰πüÁ®±ÁÇ∫„ÄåË∂ÖÁ¥öÂ∑®ÊòüÊïàÊáâ„ÄçÔºâ„ÄÇÈÄô‰∫õÁµêÊûúÂá∏È°Ø‰∫ÜÁï∂ LLM Êì∑Âèñ‰∏ªËßÄË≥áË®äÊôÇÔºåÂÖ®ÁêÉÁü•Ë≠òÂëàÁèæÂèØËÉΩÊúÉÈÅéÊñºÁãπÈöòÁöÑÈ¢®Èö™„ÄÇ

##### **Envisioning National Resources for Artificial Intelligence Research: NSF Workshop Report**
2412.10278v1 by Shantenu Jha, Yolanda Gil

This is a report of an NSF workshop titled "Envisioning National Resources
for Artificial Intelligence Research" held in Alexandria, Virginia, in May
2024. The workshop aimed to identify initial challenges and opportunities for
national resources for AI research (e.g., compute, data, models, etc.) and to
facilitate planning for the envisioned National AI Research Resource.
Participants included AI and cyberinfrastructure (CI) experts. The report
outlines significant findings and identifies needs and recommendations from the
workshop.

ÊëòË¶ÅÔºöÈÄô‰ªΩÂ†±ÂëäÊòØ NSF Â∑•‰ΩúÂùäÁöÑÂ†±ÂëäÔºåÊ®ôÈ°åÁÇ∫„ÄåÊßãÊÉ≥‰∫∫Â∑•Êô∫ÊÖßÁ†îÁ©∂ÁöÑÂúãÂÆ∂Ë≥áÊ∫ê„ÄçÔºåÊñº 2024 Âπ¥ 5 ÊúàÂú®Á∂≠ÂêâÂ∞º‰∫ûÂ∑û‰∫ûÊ≠∑Â±±ÂçìÂ∏ÇËàâË°å„ÄÇË©≤Â∑•‰ΩúÂùäÊó®Âú®ÊâæÂá∫ÂúãÂÆ∂ AI Á†îÁ©∂Ë≥áÊ∫êÔºà‰æãÂ¶ÇÈÅãÁÆó„ÄÅË≥áÊñô„ÄÅÊ®°ÂûãÁ≠âÔºâÁöÑÂàùÊ≠•ÊåëÊà∞ÂíåÊ©üÊúÉÔºå‰∏¶‰øÉÈÄ≤Ë¶èÂäÉÊßãÊÉ≥‰∏≠ÁöÑÂúãÂÆ∂ AI Á†îÁ©∂Ë≥áÊ∫ê„ÄÇÂèÉËàáËÄÖÂåÖÊã¨ AI ÂíåÁ∂≤Ë∑ØÂü∫Á§éÂª∫Ë®≠ (CI) Â∞àÂÆ∂„ÄÇÈÄô‰ªΩÂ†±ÂëäÊ¶ÇËø∞‰∫ÜÈáçË¶ÅÁöÑÁôºÁèæÔºå‰∏¶ÊâæÂá∫Â∑•‰ΩúÂùäÁöÑÈúÄÊ±ÇÂíåÂª∫Ë≠∞„ÄÇ

##### **Benchmarking Linguistic Diversity of Large Language Models**
2412.10271v1 by Yanzhu Guo, Guokan Shang, Chlo√© Clavel

The development and evaluation of Large Language Models (LLMs) has primarily
focused on their task-solving capabilities, with recent models even surpassing
human performance in some areas. However, this focus often neglects whether
machine-generated language matches the human level of diversity, in terms of
vocabulary choice, syntactic construction, and expression of meaning, raising
questions about whether the fundamentals of language generation have been fully
addressed. This paper emphasizes the importance of examining the preservation
of human linguistic richness by language models, given the concerning surge in
online content produced or aided by LLMs. We propose a comprehensive framework
for evaluating LLMs from various linguistic diversity perspectives including
lexical, syntactic, and semantic dimensions. Using this framework, we benchmark
several state-of-the-art LLMs across all diversity dimensions, and conduct an
in-depth case study for syntactic diversity. Finally, we analyze how different
development and deployment choices impact the linguistic diversity of LLM
outputs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈñãÁôºÂíåË©ï‰º∞‰∏ªË¶ÅÈõÜ‰∏≠ÊñºÂÖ∂Ëß£Ê±∫‰ªªÂãôÁöÑËÉΩÂäõÔºåÊúÄËøëÁöÑÊ®°ÂûãÁîöËá≥Âú®Êüê‰∫õÈ†òÂüüË∂ÖË∂ä‰∫Ü‰∫∫È°ûÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÈóúÊ≥®ÂæÄÂæÄÂøΩÁï•‰∫ÜÊ©üÂô®Áî¢ÁîüÁöÑË™ûË®ÄÂú®Ë©ûÂΩôÈÅ∏Êìá„ÄÅÂè•Ê≥ïÁµêÊßãÂíåÊÑèÁæ©Ë°®ÈÅîÊñπÈù¢ÁöÑÂ§öÊ®£ÊÄßÊòØÂê¶ÈÅîÂà∞‰∫∫È°ûÁöÑÊ∞¥Âπ≥ÔºåÈÄôÂºïÁôº‰∫ÜÈóúÊñºË™ûË®ÄÁîüÊàêÁöÑÂü∫Êú¨ÂéüÁêÜÊòØÂê¶Â∑≤ÂæóÂà∞ÂÖÖÂàÜËß£Ê±∫ÁöÑÂïèÈ°å„ÄÇÊú¨ÊñáÂº∑Ë™ø‰∫ÜÂú® LLM ÁîüÁî¢ÊàñËºîÂä©ÁöÑÁ∑ö‰∏äÂÖßÂÆπÊøÄÂ¢ûÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊ™¢Ë¶ñË™ûË®ÄÊ®°ÂûãÂ∞ç‰∫∫È°ûË™ûË®ÄË±êÂØåÊÄßÁöÑ‰øùÁïôÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊ°ÜÊû∂ÔºåÂæûË©ûÂΩô„ÄÅÂè•Ê≥ïÂíåË™ûÁæ©Á∂≠Â∫¶Á≠âÂêÑÁ®ÆË™ûË®ÄÂ§öÊ®£ÊÄßËßíÂ∫¶Ë©ï‰º∞ LLM„ÄÇ‰ΩøÁî®ÈÄôÂÄãÊ°ÜÊû∂ÔºåÊàëÂÄëÂ∞çÊâÄÊúâÂ§öÊ®£ÊÄßÁ∂≠Â∫¶ÈÄ≤Ë°å‰∫ÜÊï∏ÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLM Âü∫Ê∫ñÊ∏¨Ë©¶Ôºå‰∏¶Â∞çÂè•Ê≥ïÂ§öÊ®£ÊÄßÈÄ≤Ë°å‰∫ÜÊ∑±ÂÖ•ÁöÑÊ°à‰æãÁ†îÁ©∂„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂàÜÊûê‰∫Ü‰∏çÂêåÁöÑÈñãÁôºÂíåÈÉ®ÁΩ≤ÈÅ∏ÊìáÂ¶Ç‰ΩïÂΩ±Èüø LLM Ëº∏Âá∫ÁöÑË™ûË®ÄÂ§öÊ®£ÊÄß„ÄÇ

##### **Cultural Evolution of Cooperation among LLM Agents**
2412.10270v1 by Aron Vallinder, Edward Hughes

Large language models (LLMs) provide a compelling foundation for building
generally-capable AI agents. These agents may soon be deployed at scale in the
real world, representing the interests of individual humans (e.g., AI
assistants) or groups of humans (e.g., AI-accelerated corporations). At
present, relatively little is known about the dynamics of multiple LLM agents
interacting over many generations of iterative deployment. In this paper, we
examine whether a "society" of LLM agents can learn mutually beneficial social
norms in the face of incentives to defect, a distinctive feature of human
sociality that is arguably crucial to the success of civilization. In
particular, we study the evolution of indirect reciprocity across generations
of LLM agents playing a classic iterated Donor Game in which agents can observe
the recent behavior of their peers. We find that the evolution of cooperation
differs markedly across base models, with societies of Claude 3.5 Sonnet agents
achieving significantly higher average scores than Gemini 1.5 Flash, which, in
turn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an
additional mechanism for costly punishment to achieve yet higher scores, while
Gemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also
observe variation in emergent behavior across random seeds, suggesting an
understudied sensitive dependence on initial conditions. We suggest that our
evaluation regime could inspire an inexpensive and informative new class of LLM
benchmarks, focussed on the implications of LLM agent deployment for the
cooperative infrastructure of society.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫Âª∫ÊßãÂÖ∑ÂÇô‰∏ÄËà¨ËÉΩÂäõÁöÑ AI ‰ª£ÁêÜÊèê‰æõ‰∫Ü‰ª§‰∫∫‰ø°ÊúçÁöÑÂü∫Á§é„ÄÇÈÄô‰∫õ‰ª£ÁêÜ‰∫∫ÂèØËÉΩÂæàÂø´Â∞±ÊúÉÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠Â§ßË¶èÊ®°ÈÉ®ÁΩ≤Ôºå‰ª£Ë°®ÂÄã‰∫∫‰∫∫È°ûÔºà‰æãÂ¶ÇÔºåAI Âä©ÁêÜÔºâÊàñ‰∫∫È°ûÁæ§È´îÔºà‰æãÂ¶ÇÔºåAI Âä†ÈÄüÂÖ¨Âè∏ÔºâÁöÑÂà©Áõä„ÄÇÁõÆÂâçÔºåÂ∞çÊñºÂ§öÂÄã LLM ‰ª£ÁêÜÂú®Â§ö‰ª£Áñä‰ª£ÈÉ®ÁΩ≤‰∏≠‰∫íÂãïÁöÑÂãïÊÖãÁü•‰πãÁîöÂ∞ë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏ÄÂÄã LLM ‰ª£ÁêÜ„ÄåÁ§æÊúÉ„ÄçÊòØÂê¶ËÉΩÂ§†Âú®ËÉåÂèõË™òÂõ†Èù¢ÂâçÂ≠∏ÁøíÂà∞Â∞çÂΩºÊ≠§ÊúâÁõäÁöÑÁ§æÊúÉË¶èÁØÑÔºåÈÄôÊòØ‰∫∫È°ûÁ§æÊúÉÊÄßÁöÑ‰∏ÄÂÄãÈ°ØËëóÁâπÂæµÔºåÂèØ‰ª•Ë™™ÊòØÊñáÊòéÊàêÂäüÁöÑÈáçË¶ÅÈóúÈçµ„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü LLM ‰ª£ÁêÜÂú®Á∂ìÂÖ∏ÁöÑÈáçË§áÊçêË¥àËÄÖÈÅäÊà≤‰∏≠Ë∑®‰∏ñ‰ª£ÁöÑÈñìÊé•‰∫íÊÉ†ÊºîËÆäÔºåÂú®Ë©≤ÈÅäÊà≤‰∏≠Ôºå‰ª£ÁêÜ‰∫∫ÂèØ‰ª•ËßÄÂØüÂà∞ÂêåÂÑïÊúÄËøëÁöÑË°åÁÇ∫„ÄÇÊàëÂÄëÁôºÁèæÂêà‰ΩúÁöÑÊºîËÆäÂú®Âü∫Á§éÊ®°Âûã‰πãÈñìÊúâÈ°ØËëóÂ∑ÆÁï∞ÔºåClaude 3.5 Sonnet ‰ª£ÁêÜÁöÑÁ§æÊúÉÊØî Gemini 1.5 Flash Áç≤ÂæóÈ°ØËëóÊõ¥È´òÁöÑÂπ≥ÂùáÂàÜÊï∏ÔºåËÄåÂæåËÄÖÂèàÂÑ™Êñº GPT-4o„ÄÇÊ≠§Â§ñÔºåClaude 3.5 Sonnet ÂèØ‰ª•Âà©Áî®È°çÂ§ñÁöÑ‰ª£ÂÉπÊá≤ÁΩ∞Ê©üÂà∂‰æÜÁç≤ÂæóÊõ¥È´òÁöÑÂàÜÊï∏ÔºåËÄå Gemini 1.5 Flash Âíå GPT-4o ÂâáÁÑ°Ê≥ïÂÅöÂà∞ÈÄô‰∏ÄÈªû„ÄÇÂ∞çÊñºÊØèÂÄãÊ®°ÂûãÈ°ûÂà•ÔºåÊàëÂÄëÈÇÑËßÄÂØüÂà∞Èö®Ê©üÁ®ÆÂ≠ê‰∏≠Âá∫ÁèæË°åÁÇ∫ÁöÑËÆäÂåñÔºåÈÄôË°®ÊòéÂ∞çÂàùÂßãÊ¢ù‰ª∂ÁöÑÊïèÊÑü‰æùË≥¥ÊÄßÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàëÂÄëÂª∫Ë≠∞ÊàëÂÄëÁöÑË©ï‰º∞Âà∂Â∫¶ÂèØ‰ª•ÊøÄÂãµ‰∏ÄÁ®ÆÂªâÂÉπ‰∏îÊúâÁõäÁöÑÊñ∞Âûã LLM Âü∫Ê∫ñÔºåÂ∞àÊ≥®Êñº LLM ‰ª£ÁêÜÈÉ®ÁΩ≤Â∞çÁ§æÊúÉÂêà‰ΩúÂü∫Á§éË®≠ÊñΩÁöÑÂΩ±Èüø„ÄÇ

##### **Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT**
2412.10267v1 by Danielle R. Thomas, Conrad Borchers, Sanjit Kakarla, Jionghao Lin, Shambhavi Bhushan, Boyuan Guo, Erin Gatz, Kenneth R. Koedinger

The role of multiple-choice questions (MCQs) as effective learning tools has
been debated in past research. While MCQs are widely used due to their ease in
grading, open response questions are increasingly used for instruction, given
advances in large language models (LLMs) for automated grading. This study
evaluates MCQs effectiveness relative to open-response questions, both
individually and in combination, on learning. These activities are embedded
within six tutor lessons on advocacy. Using a posttest-only randomized control
design, we compare the performance of 234 tutors (790 lesson completions)
across three conditions: MCQ only, open response only, and a combination of
both. We find no significant learning differences across conditions at
posttest, but tutors in the MCQ condition took significantly less time to
complete instruction. These findings suggest that MCQs are as effective, and
more efficient, than open response tasks for learning when practice time is
limited. To further enhance efficiency, we autograded open responses using
GPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of
low-stakes assessment, though further research is needed for broader use. This
study contributes a dataset of lesson log data, human annotation rubrics, and
LLM prompts to promote transparency and reproducibility.

ÊëòË¶ÅÔºöÂ§öÈÅ∏È°å (MCQ) ‰ΩúÁÇ∫ÊúâÊïàÁöÑÂ≠∏ÁøíÂ∑•ÂÖ∑ÁöÑËßíËâ≤Âú®ÈÅéÂéªÁöÑÁ†îÁ©∂‰∏≠‰∏ÄÁõ¥ÂÇôÂèóÁà≠Ë≠∞„ÄÇÈõñÁÑ∂ MCQ Âõ†ÂÖ∂ÊòìÊñºË©ïÂàÜËÄåË¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÂãïË©ïÂàÜÊñπÈù¢ÁöÑÈÄ≤Ê≠•ÔºåÈñãÊîæÂºèÂïèÈ°åÊ≠£Ë∂ä‰æÜË∂äË¢´Áî®ÊñºÊïôÂ≠∏„ÄÇÊú¨Á†îÁ©∂Ë©ï‰º∞‰∫Ü MCQ Áõ∏Â∞çÊñºÈñãÊîæÂºèÂïèÈ°åÁöÑÊúâÊïàÊÄßÔºåÁÑ°Ë´ñÊòØÂñÆÁç®ÈÇÑÊòØÁµÑÂêàÔºåÂ∞çÂ≠∏ÁøíÁöÑÂΩ±Èüø„ÄÇÈÄô‰∫õÊ¥ªÂãïÂµåÂÖ•Âú®ÂÖ≠ÂÄãÈóúÊñºÂÄ°Â∞éÁöÑËºîÂ∞éË™≤Á®ã‰∏≠„ÄÇ‰ΩøÁî®ÂÉÖÈôêÂæåÊ∏¨ÁöÑÈö®Ê©üÂ∞çÁÖßË®≠Ë®àÔºåÊàëÂÄëÊØîËºÉ‰∫Ü 234 ‰ΩçËºîÂ∞éÂì° (790 ÂÄãË™≤Á®ãÂÆåÊàê) Âú®‰∏âÁ®ÆÊ¢ù‰ª∂‰∏ãÁöÑË°®ÁèæÔºöÂÉÖ MCQ„ÄÅÂÉÖÈñãÊîæÂºèÂõûÊáâ‰ª•ÂèäÂÖ©ËÄÖÁöÑÁµÑÂêà„ÄÇÊàëÂÄëÁôºÁèæÂæåÊ∏¨ÊôÇÂêÑÊ¢ù‰ª∂‰πãÈñìÊ≤íÊúâÈ°ØËëóÁöÑÂ≠∏ÁøíÂ∑ÆÁï∞Ôºå‰ΩÜÂú® MCQ Ê¢ù‰ª∂‰∏ãÁöÑËºîÂ∞éÂì°ÂÆåÊàêÊïôÂ≠∏ÊâÄÈúÄÁöÑÊôÇÈñìÈ°ØËëóÊ∏õÂ∞ë„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåÁï∂Á∑¥ÁøíÊôÇÈñìÊúâÈôêÊôÇÔºåMCQ ËàáÈñãÊîæÂºè‰ªªÂãô‰∏ÄÊ®£ÊúâÊïàÔºåËÄå‰∏îÊïàÁéáÊõ¥È´ò„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊïàÁéáÔºåÊàëÂÄë‰ΩøÁî® GPT-4o Âíå GPT-4-turbo Ëá™ÂãïË©ïÂàÜÈñãÊîæÂºèÂõûÊáâ„ÄÇGPT Ê®°ÂûãÂ±ïÁ§∫‰∫Ü‰ΩéÈ¢®Èö™Ë©ï‰º∞ÁõÆÁöÑÁöÑÁÜüÁ∑¥Â∫¶ÔºåÂÑòÁÆ°ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂‰ª•Êõ¥Âª£Ê≥õÂú∞‰ΩøÁî®„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫Ü‰∏ÄÁµÑË™≤Á®ãÊó•Ë™åÊï∏Êìö„ÄÅ‰∫∫Â∑•Ë®ªÈáãÊ∫ñÂâáÂíå LLM ÊèêÁ§∫Ôºå‰ª•‰øÉÈÄ≤ÈÄèÊòéÂ∫¶ÂíåÂèØË§áË£ΩÊÄß„ÄÇ

##### **Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media**
2412.10266v1 by Jiaqing Yuan, Ruijie Xi, Munindar P. Singh

Stance detection is crucial for fostering a human-centric Web by analyzing
user-generated content to identify biases and harmful narratives that undermine
trust. With the development of Large Language Models (LLMs), existing
approaches treat stance detection as a classification problem, providing robust
methodologies for modeling complex group interactions and advancing
capabilities in natural language tasks. However, these methods often lack
interpretability, limiting their ability to offer transparent and
understandable justifications for predictions. This study adopts a generative
approach, where stance predictions include explicit, interpretable rationales,
and integrates them into smaller language models through single-task and
multitask learning. We find that incorporating reasoning into stance detection
enables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot
performance, achieving an improvement of up to 9.57%. Moreover, our results
show that reasoning capabilities enhance multitask learning performance but may
reduce effectiveness in single-task settings. Crucially, we demonstrate that
faithful rationales improve rationale distillation into SLMs, advancing efforts
to build interpretable, trustworthy systems for addressing discrimination,
fostering trust, and promoting equitable engagement on social media.

ÊëòË¶ÅÔºöÁ´ãÂ†¥ÂÅµÊ∏¨Â∞çÊñº‰øÉÈÄ≤‰ª•‰∫∫ÁÇ∫Êú¨ÁöÑÁ∂≤Ë∑ØËá≥ÈóúÈáçË¶ÅÔºåÈÄèÈÅéÂàÜÊûê‰ΩøÁî®ËÄÖÁî¢ÁîüÁöÑÂÖßÂÆπ‰æÜËæ®Ë≠òÁ†¥Â£û‰ø°‰ªªÁöÑÂÅèË¶ãÂíåÊúâÂÆ≥ÊïòËø∞„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁôºÂ±ïÔºåÁèæÊúâÁöÑÊñπÊ≥ïÂ∞áÁ´ãÂ†¥ÂÅµÊ∏¨Ë¶ñÁÇ∫ÂàÜÈ°ûÂïèÈ°åÔºåÊèê‰æõÂº∑ÂÅ•ÁöÑÊñπÊ≥ï‰æÜÂª∫Ê®°Ë§áÈõúÁöÑÁæ§È´î‰∫íÂãïÔºå‰∏¶ÊèêÂçáËá™ÁÑ∂Ë™ûË®Ä‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÊèê‰æõÈÄèÊòé‰∏îÂèØÁêÜËß£ÁöÑÈ†êÊ∏¨‰æùÊìöÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂Êé°Áî®ÁîüÊàêÂºèÊñπÊ≥ïÔºåÂÖ∂‰∏≠Á´ãÂ†¥È†êÊ∏¨ÂåÖÊã¨ÊòéÁ¢∫„ÄÅÂèØËß£ÈáãÁöÑ‰æùÊìöÔºå‰∏¶ÈÄèÈÅéÂñÆ‰∏Ä‰ªªÂãôÂíåÂ§ö‰ªªÂãôÂ≠∏ÁøíÂ∞áÂÆÉÂÄëÊï¥ÂêàÂà∞ËºÉÂ∞èÁöÑË™ûË®ÄÊ®°Âûã‰∏≠„ÄÇÊàëÂÄëÁôºÁèæÂ∞áÊé®ÁêÜÁ¥çÂÖ•Á´ãÂ†¥ÂÅµÊ∏¨ËÉΩËÆìËºÉÂ∞èÁöÑÊ®°Âûã (FlanT5) ÂÑ™Êñº GPT-3.5 ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩÔºåÊèêÂçáÂπÖÂ∫¶ÊúÄÈ´òÈÅî 9.57%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫Êé®ÁêÜËÉΩÂäõÊèêÂçá‰∫ÜÂ§ö‰ªªÂãôÂ≠∏ÁøíÊïàËÉΩÔºå‰ΩÜÂèØËÉΩÈôç‰ΩéÂñÆ‰∏Ä‰ªªÂãôË®≠ÂÆö‰∏≠ÁöÑÊïàËÉΩ„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëË≠âÊòéÂø†ÂØ¶ÁöÑ‰æùÊìöËÉΩÊîπÂñÑÂ∞á‰æùÊìöËêÉÂèñÂà∞ SLM ‰∏≠Ôºå‰øÉÈÄ≤Âª∫ÊßãÂèØËß£Èáã„ÄÅÂÄºÂæó‰ø°Ë≥¥ÁöÑÁ≥ªÁµ±Ôºå‰ª•Ëß£Ê±∫Ê≠ßË¶ñ„ÄÅÂª∫Á´ã‰ø°‰ªªÔºå‰∏¶Âú®Á§æÁæ§Â™íÈ´î‰∏äÊé®Âª£ÂÖ¨Âπ≥ÁöÑÂèÉËàá„ÄÇ

##### **Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models**
2412.10257v1 by Harry J. Davies, Giorgos Iacovides, Danilo P. Mandic

The sheer scale of data required to train modern large language models (LLMs)
poses significant risks, as models are likely to gain knowledge of sensitive
topics such as bio-security, as well the ability to replicate copyrighted
works. Methods designed to remove such knowledge must do so from all prompt
directions, in a multi-lingual capacity and without degrading general model
performance. To this end, we introduce the targeted angular reversal (TARS)
method of knowledge removal from LLMs. The TARS method firstly leverages the
LLM in combination with a detailed prompt to aggregate information about a
selected concept in the internal representation space of the LLM. It then
refines this approximate concept vector to trigger the concept token with high
probability, by perturbing the approximate concept vector with noise and
transforming it into token scores with the language model head. The feedforward
weight vectors in the LLM which operate directly on the internal representation
space, and have the highest cosine similarity with this targeting vector, are
then replaced by a reversed targeting vector, thus limiting the ability of the
concept to propagate through the model. The modularity of the TARS method
allows for a sequential removal of concepts from Llama 3.1 8B, such as the
famous literary detective Sherlock Holmes, and the planet Saturn. It is
demonstrated that the probability of triggering target concepts can be reduced
to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the
knowledge bi-directionally. Moreover, knowledge is shown to be removed across
all languages despite only being targeted in English. Importantly, TARS has
minimal impact on the general model capabilities, as after removing 5 diverse
concepts in a modular fashion, there is minimal KL divergence in the next token
probabilities of the LLM on large corpora of Wikipedia text (median of 0.002).

ÊëòË¶ÅÔºö<paragraph>Ë®ìÁ∑¥Áèæ‰ª£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊâÄÈúÄÁöÑÈæêÂ§ßÊï∏ÊìöË¶èÊ®°Â∏∂‰æÜÈáçÂ§ßÈ¢®Èö™ÔºåÂõ†ÁÇ∫Ê®°ÂûãÂæàÂèØËÉΩÊúÉÁç≤ÂèñÊïèÊÑü‰∏ªÈ°åÁöÑÁü•Ë≠òÔºå‰æãÂ¶ÇÁîüÁâ©ÂÆâÂÖ®Ôºå‰ª•ÂèäË§áË£ΩÂèóÁâàÊ¨ä‰øùË≠∑‰ΩúÂìÅÁöÑËÉΩÂäõ„ÄÇÊó®Âú®ÁßªÈô§Ê≠§È°ûÁü•Ë≠òÁöÑÊñπÊ≥ïÂøÖÈ†àÂæûÊâÄÊúâÊèêÁ§∫ÊñπÂêë‰ª•Â§öË™ûË®ÄÊñπÂºèÂü∑Ë°åÊ≠§Êìç‰ΩúÔºå‰∏î‰∏çÊúÉÈôç‰Ωé‰∏ÄËà¨Ê®°ÂûãÊïàËÉΩ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂæû LLM ‰∏≠ÁßªÈô§Áü•Ë≠òÁöÑÁõÆÊ®ôËßíÂèçËΩâ (TARS) ÊñπÊ≥ï„ÄÇTARS ÊñπÊ≥ïÈ¶ñÂÖàÂà©Áî® LLM ÁµêÂêàË©≥Á¥∞ÊèêÁ§∫ÔºåÂú® LLM ÁöÑÂÖßÈÉ®Ë°®Á§∫Á©∫Èñì‰∏≠ÂΩôÊï¥ÊúâÈóúÊâÄÈÅ∏Ê¶ÇÂøµÁöÑË≥áË®ä„ÄÇÁÑ∂ÂæåÔºåÂÆÉÊúÉÂæÆË™øÊ≠§Ëøë‰ººÊ¶ÇÂøµÂêëÈáèÔºåËóâÁî±‰ΩøÁî®ÈõúË®äÊìæÂãïËøë‰ººÊ¶ÇÂøµÂêëÈáèÔºå‰∏¶‰ΩøÁî®Ë™ûË®ÄÊ®°ÂûãÈ†≠Â∞áÂÖ∂ËΩâÊèõÁÇ∫‰ª£Âπ£ÂàÜÊï∏Ôºå‰ª•È´òÊ©üÁéáËß∏ÁôºÊ¶ÇÂøµ‰ª£Âπ£„ÄÇÂú® LLM ‰∏≠Áõ¥Êé•‰ΩúÁî®ÊñºÂÖßÈÉ®Ë°®Á§∫Á©∫ÈñìÔºå‰∏îËàáÊ≠§ÁõÆÊ®ôÂêëÈáèÂÖ∑ÊúâÊúÄÈ´òÈ§òÂº¶Áõ∏‰ººÂ∫¶ÁöÑÂâçÈ•ãÊ¨äÈáçÂêëÈáèÔºåÈö®ÂæåÊúÉË¢´ÂèçÂêëÁõÆÊ®ôÂêëÈáèÂèñ‰ª£ÔºåÂõ†Ê≠§ÈôêÂà∂‰∫ÜÊ¶ÇÂøµÂú®Ê®°Âûã‰∏≠ÂÇ≥Êí≠ÁöÑËÉΩÂäõ„ÄÇTARS ÊñπÊ≥ïÁöÑÊ®°ÁµÑÂåñÂÖÅË®±Âæû Llama 3.1 8B ‰∏≠Âæ™Â∫èÁßªÈô§Ê¶ÇÂøµÔºå‰æãÂ¶ÇËëóÂêçÁöÑÊñáÂ≠∏ÂÅµÊé¢Â§èÊ¥õÂÖãÁ¶èÁàæÊë©ÊñØÂíåÂúüÊòü„ÄÇÁµêÊûúË°®ÊòéÔºåËß∏ÁôºÁõÆÊ®ôÊ¶ÇÂøµÁöÑÊ©üÁéáÂèØ‰ª•ÈÄèÈÅéÂ∞ëËá≥ 1 Ê¨° TARS Á∑®ËºØÈôç‰ΩéËá≥ 0.00ÔºåÂêåÊôÇÈõôÂêëÁßªÈô§Áü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåÂÑòÁÆ°ÂÉÖ‰ª•Ëã±ÊñáÁÇ∫ÁõÆÊ®ôÔºå‰ΩÜÂ∑≤È°ØÁ§∫Áü•Ë≠òÂ∑≤Ë∑®ÊâÄÊúâË™ûË®ÄÁßªÈô§„ÄÇÈáçË¶ÅÁöÑÊòØÔºåTARS Â∞ç‰∏ÄËà¨Ê®°ÂûãÂäüËÉΩÁöÑÂΩ±ÈüøÂæàÂ∞èÔºåÂõ†ÁÇ∫Âú®‰ª•Ê®°ÁµÑÂåñÊñπÂºèÁßªÈô§ 5 ÂÄã‰∏çÂêåÁöÑÊ¶ÇÂøµÂæåÔºåLLM Âú®Â§ßÈáèÁ∂≠Âü∫ÁôæÁßëÊñáÂ≠ó‰∏≠ÁöÑ‰∏ã‰∏ÄÂÄã‰ª£Âπ£Ê©üÁéáÁöÑ KL ÂàÜÊ≠ßÂæàÂ∞èÔºà‰∏≠‰ΩçÊï∏ÁÇ∫ 0.002Ôºâ„ÄÇ</paragraph>

##### **Exploring the Frontiers of Animation Video Generation in the Sora Era: Method, Dataset and Benchmark**
2412.10255v1 by Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun

Animation has gained significant interest in the recent film and TV industry.
Despite the success of advanced video generation models like Sora, Kling, and
CogVideoX in generating natural videos, they lack the same effectiveness in
handling animation videos. Evaluating animation video generation is also a
great challenge due to its unique artist styles, violating the laws of physics
and exaggerated motions. In this paper, we present a comprehensive system,
AniSora, designed for animation video generation, which includes a data
processing pipeline, a controllable generation model, and an evaluation
dataset. Supported by the data processing pipeline with over 10M high-quality
data, the generation model incorporates a spatiotemporal mask module to
facilitate key animation production functions such as image-to-video
generation, frame interpolation, and localized image-guided animation. We also
collect an evaluation benchmark of 948 various animation videos, the evaluation
on VBench and human double-blind test demonstrates consistency in character and
motion, achieving state-of-the-art results in animation video generation. %We
also collect an evaluation benchmark of 948 various animation videos, with
specifically developed metrics for animation video generation. Our model access
API and evaluation benchmark will be publicly available.

ÊëòË¶ÅÔºöÂãïÁï´Âú®ÊúÄËøëÁöÑÈõªÂΩ±ÂíåÈõªË¶ñÁî¢Ê•≠‰∏≠Áç≤Âæó‰∫ÜÈ°ØËëóÁöÑÈóúÊ≥®„ÄÇ
ÂÑòÁÆ°ÂÉè Sora„ÄÅKling Âíå CogVideoX ÈÄôÊ®£ÁöÑÂÖàÈÄ≤ÂΩ±ÁâáÁîüÊàêÊ®°ÂûãÂú®ÁîüÊàêËá™ÁÑ∂ÂΩ±ÁâáÊñπÈù¢ÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂú®ËôïÁêÜÂãïÁï´ÂΩ±ÁâáÊñπÈù¢ÂçªÁº∫‰πèÁõ∏ÂêåÁöÑÊïàËÉΩ„ÄÇÁî±ÊñºÂÖ∂Áç®ÁâπÁöÑËóùË°ìÈ¢®Ê†º„ÄÅÈÅïÂèçÁâ©ÁêÜÂÆöÂæãÂíåË™áÂºµÁöÑÂãï‰ΩúÔºåË©ï‰º∞ÂãïÁï´ÂΩ±ÁâáÁîüÊàê‰πüÊòØ‰∏ÄÂÄãÂ∑®Â§ßÁöÑÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÁ≥ªÁµ± AniSoraÔºåÂ∞àÈñÄÁî®ÊñºÂãïÁï´ÂΩ±ÁâáÁîüÊàêÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰∏ÄÂÄãË≥áÊñôËôïÁêÜÁÆ°Á∑ö„ÄÅ‰∏ÄÂÄãÂèØÊéßÁöÑÁîüÊàêÊ®°ÂûãÂíå‰∏ÄÂÄãË©ï‰º∞Ë≥áÊñôÈõÜ„ÄÇÂú®ÊìÅÊúâË∂ÖÈÅé 10M È´òÂìÅË≥™Ë≥áÊñôÁöÑË≥áÊñôËôïÁêÜÁÆ°Á∑öÁöÑÊîØÊè¥‰∏ãÔºåÁîüÊàêÊ®°ÂûãÁµêÂêà‰∫ÜÊôÇÁ©∫ÈÅÆÁΩ©Ê®°ÁµÑÔºå‰ª•‰øÉÈÄ≤ÈóúÈçµÂãïÁï´Ë£Ω‰ΩúÂäüËÉΩÔºå‰æãÂ¶ÇÂΩ±ÂÉèËΩâÂΩ±ÁâáÁîüÊàê„ÄÅÂπÄÊèíË£úÂíåÂ±ÄÈÉ®ÂΩ±ÂÉèÂ∞éÂêëÂãïÁï´„ÄÇÊàëÂÄëÈÇÑÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÁî± 948 ÂÄãÂêÑÁ®ÆÂãïÁï´ÂΩ±ÁâáÁµÑÊàêÁöÑË©ï‰º∞Âü∫Ê∫ñÔºåÂú® VBench Âíå‰∫∫È°ûÈõôÁõ≤Ê∏¨Ë©¶‰∏äÁöÑË©ï‰º∞Ë≠âÊòé‰∫ÜËßíËâ≤ÂíåÂãï‰ΩúÁöÑ‰∏ÄËá¥ÊÄßÔºåÂú®ÂãïÁï´ÂΩ±ÁâáÁîüÊàê‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇ% ÊàëÂÄëÈÇÑÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÁî± 948 ÂÄãÂêÑÁ®ÆÂãïÁï´ÂΩ±ÁâáÁµÑÊàêÁöÑË©ï‰º∞Âü∫Ê∫ñÔºå‰∏¶ÁÇ∫ÂãïÁï´ÂΩ±ÁâáÁîüÊàêÁâπÂà•ÈñãÁôº‰∫ÜÊåáÊ®ô„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ≠òÂèñ API ÂíåË©ï‰º∞Âü∫Ê∫ñÂ∞áÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Efficient Continual Pre-training of LLMs for Low-resource Languages**
2412.10244v1 by Arijit Nag, Soumen Chakrabarti, Animesh Mukherjee, Niloy Ganguly

Open-source Large Language models (OsLLMs) propel the democratization of
natural language research by giving the flexibility to augment or update model
parameters for performance improvement. Nevertheless, like proprietary LLMs,
Os-LLMs offer poorer performance on low-resource languages (LRLs) than
high-resource languages (HRLs), owing to smaller amounts of training data and
underrepresented vocabulary. On the other hand, continual pre-training (CPT)
with large amounts of language-specific data is a costly proposition in terms
of data acquisition and computational resources. Our goal is to drastically
reduce CPT cost. To that end, we first develop a new algorithm to select a
subset of texts from a larger corpus. We show the effectiveness of our
technique using very little CPT data. In search of further improvement, we
design a new algorithm to select tokens to include in the LLM vocabulary. We
experiment with the recent Llama-3 model and nine Indian languages with diverse
scripts and extent of resource availability. For evaluation, we use
IndicGenBench, a generation task benchmark dataset for Indic languages. We
experiment with various CPT corpora and augmented vocabulary size and offer
insights across language families.

ÊëòË¶ÅÔºöÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (OsLLM) Êé®ÂãïËá™ÁÑ∂Ë™ûË®ÄÁ†îÁ©∂ÁöÑÊ∞ë‰∏ªÂåñÔºåËÆìÊ®°ÂûãÂèÉÊï∏ÂèØ‰ª•ÈùàÊ¥ªÂú∞Êì¥ÂÖÖÊàñÊõ¥Êñ∞‰ª•ÊèêÂçáÊïàËÉΩ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåËàáÂ∞àÊúâ LLM È°û‰ººÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñôÈáèËºÉÂ∞ë‰∏îË©ûÂΩôÈáè‰∏çË∂≥ÔºåOs-LLM Âú®‰ΩéË≥áÊ∫êË™ûË®Ä (LRL) ‰∏äÁöÑË°®ÁèæÊØîÈ´òË≥áÊ∫êË™ûË®Ä (HRL) Â∑Æ„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÈáùÂ∞çÁâπÂÆöË™ûË®ÄË≥áÊñôÈÄ≤Ë°åÊåÅÁ∫åÈ†êË®ìÁ∑¥ (CPT) Âú®Ë≥áÊñôÂèñÂæóÂíåÈÅãÁÆóË≥áÊ∫êÊñπÈù¢ÊòØ‰∏ÄÈ†ÖÊòÇË≤¥ÁöÑÊèêË≠∞„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂ§ßÂπÖÈôç‰Ωé CPT ÊàêÊú¨„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈ¶ñÂÖàÈñãÁôº‰∏ÄÁ®ÆÊñ∞ÊºîÁÆóÊ≥ïÔºåÂæûËºÉÂ§ßÁöÑË™ûÊñôÂ∫´‰∏≠ÈÅ∏Âèñ‰∏ÄÂÄãÂ≠êÈõÜÁöÑÊñáÂ≠ó„ÄÇÊàëÂÄë‰ΩøÁî®Ê•µÂ∞ëÁöÑ CPT Ë≥áÊñô‰æÜÂ±ïÁ§∫ÊàëÂÄëÊäÄË°ìÁöÑÊúâÊïàÊÄß„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÔºåÊàëÂÄëË®≠Ë®à‰∏ÄÁ®ÆÊñ∞ÊºîÁÆóÊ≥ï‰æÜÈÅ∏ÂèñË¶ÅÂåÖÂê´Âú® LLM Ë©ûÂΩô‰∏≠ÁöÑË©ûÂΩô„ÄÇÊàëÂÄë‰ΩøÁî®ÊúÄÊñ∞ÁöÑ Llama-3 Ê®°ÂûãÂíå‰πùÁ®ÆÂç∞Â∫¶Ë™ûË®ÄÈÄ≤Ë°åÂØ¶È©óÔºåÈÄô‰∫õË™ûË®ÄÂÖ∑Êúâ‰∏çÂêåÁöÑÊñáÂ≠óÁ≥ªÁµ±ÂíåË≥áÊ∫êÂèØÁî®ÊÄßÁ®ãÂ∫¶„ÄÇÂú®Ë©ï‰º∞ÊñπÈù¢ÔºåÊàëÂÄë‰ΩøÁî® IndicGenBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÈáùÂ∞çÂç∞Â∫¶Ë™ûË®ÄÁöÑÁî¢Áîü‰ªªÂãôÂü∫Ê∫ñË≥áÊñôÈõÜ„ÄÇÊàëÂÄë‰ΩøÁî®ÂêÑÁ®Æ CPT Ë™ûÊñôÂ∫´ÂíåÊì¥ÂÖÖÁöÑË©ûÂΩôÈáèÈÄ≤Ë°åÂØ¶È©óÔºå‰∏¶Êèê‰æõË∑®Ë™ûË®ÄÁ≥ªÂàóÁöÑË¶ãËß£„ÄÇ

##### **Physics Instrument Design with Reinforcement Learning**
2412.10237v1 by Shah Rukh Qasim, Patrick Owen, Nicola Serra

We present a case for the use of Reinforcement Learning (RL) for the design
of physics instrument as an alternative to gradient-based
instrument-optimization methods. It's applicability is demonstrated using two
empirical studies. One is longitudinal segmentation of calorimeters and the
second is both transverse segmentation as well longitudinal placement of
trackers in a spectrometer. Based on these experiments, we propose an
alternative approach that offers unique advantages over differentiable
programming and surrogate-based differentiable design optimization methods.
First, Reinforcement Learning (RL) algorithms possess inherent exploratory
capabilities, which help mitigate the risk of convergence to local optima.
Second, this approach eliminates the necessity of constraining the design to a
predefined detector model with fixed parameters. Instead, it allows for the
flexible placement of a variable number of detector components and facilitates
discrete decision-making. We then discuss the road map of how this idea can be
extended into designing very complex instruments. The presented study sets the
stage for a novel framework in physics instrument design, offering a scalable
and efficient framework that can be pivotal for future projects such as the
Future Circular Collider (FCC), where most optimized detectors are essential
for exploring physics at unprecedented energy scales.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ°à‰æãÔºåË™™Êòé‰ΩøÁî®Âº∑ÂåñÂ≠∏Áøí (RL) ‰æÜË®≠Ë®àÁâ©ÁêÜÂÑÄÂô®Ôºå‰ΩúÁÇ∫Âü∫ÊñºÊ¢ØÂ∫¶ÁöÑÂÑÄÂô®ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂÆÉÁöÑÈÅ©Áî®ÊÄßÂ∑≤ÈÄöÈÅéÂÖ©È†ÖÂØ¶Ë≠âÁ†îÁ©∂ÂæóÂà∞Ë≠âÂØ¶„ÄÇ‰∏ÄÂÄãÊòØÈáèÁÜ±Âô®ÁöÑÁ∏±ÂêëÂàÜÂâ≤ÔºåÂè¶‰∏ÄÂÄãÊòØÊ©´ÂêëÂàÜÂâ≤ÂíåÂÖâË≠úÂÑÄ‰∏≠ËøΩËπ§Âô®ÁöÑÁ∏±ÂêëÊîæÁΩÆ„ÄÇÂü∫ÊñºÈÄô‰∫õÂØ¶È©óÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊõø‰ª£ÊñπÊ≥ïÔºåÂÆÉÊØîÂèØÂæÆÂàÜÁ®ãÂºèË®≠Ë®àÂíåÂü∫Êñº‰ª£ÁêÜÁöÑÂèØÂæÆÂàÜË®≠Ë®àÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÂÖ∑ÊúâÁç®ÁâπÂÑ™Âã¢„ÄÇÈ¶ñÂÖàÔºåÂº∑ÂåñÂ≠∏Áøí (RL) ÊºîÁÆóÊ≥ïÂÖ∑ÊúâÂÖßÂú®ÁöÑÊé¢Á¥¢ËÉΩÂäõÔºåÊúâÂä©ÊñºÈôç‰ΩéÊî∂ÊñÇÂà∞Â±ÄÈÉ®ÊúÄÂÑ™ÁöÑÈ¢®Èö™„ÄÇÂÖ∂Ê¨°ÔºåÈÄôÁ®ÆÊñπÊ≥ïÊ∂àÈô§‰∫ÜÂ∞áË®≠Ë®àÈôêÂà∂Âú®ÂÖ∑ÊúâÂõ∫ÂÆöÂèÉÊï∏ÁöÑÈ†êÂÆöÁæ©ÂÅµÊ∏¨Âô®Ê®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÁõ∏ÂèçÔºåÂÆÉÂÖÅË®±ÈùàÊ¥ªÊîæÁΩÆÂèØËÆäÊï∏ÈáèÁöÑÂÅµÊ∏¨Âô®ÁµÑ‰ª∂Ôºå‰∏¶‰øÉÈÄ≤Èõ¢Êï£Ê±∫Á≠ñÂà∂ÂÆö„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÂ¶Ç‰ΩïÂ∞áÈÄôÂÄãÊÉ≥Ê≥ïÊì¥Â±ïÂà∞Ë®≠Ë®àÈùûÂ∏∏Ë§áÈõúÁöÑÂÑÄÂô®ÁöÑË∑ØÁ∑öÂúñ„ÄÇÊèêÂá∫ÁöÑÁ†îÁ©∂ÁÇ∫Áâ©ÁêÜÂÑÄÂô®Ë®≠Ë®à‰∏≠ÁöÑÊñ∞Ê°ÜÊû∂Â•†ÂÆö‰∫ÜÂü∫Á§éÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÊì¥ÂÖÖ‰∏îÈ´òÊïàÁöÑÊ°ÜÊû∂ÔºåÂ∞çÊñºÊú™‰æÜÁöÑÂ∞àÊ°àÔºà‰æãÂ¶ÇÊú™‰æÜÁí∞ÂΩ¢Â∞çÊíûÊ©ü (FCC)ÔºâËá≥ÈóúÈáçË¶ÅÔºåÂú®ÈÄô‰∫õÂ∞àÊ°à‰∏≠ÔºåÊúÄÊúÄ‰Ω≥ÂåñÁöÑÂÅµÊ∏¨Âô®Â∞çÊñºÊé¢Á¥¢ÂâçÊâÄÊú™ÊúâÁöÑËÉΩÈáèÂ∞∫Â∫¶ÁöÑÁâ©ÁêÜÁèæË±°Ëá≥ÈóúÈáçË¶Å„ÄÇ

##### **How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives**
2412.10220v1 by Timour Ichmoukhamedov, James Hinns, David Martens

A rapidly developing application of LLMs in XAI is to convert quantitative
explanations such as SHAP into user-friendly narratives to explain the
decisions made by smaller prediction models. Evaluating the narratives without
relying on human preference studies or surveys is becoming increasingly
important in this field. In this work we propose a framework and explore
several automated metrics to evaluate LLM-generated narratives for explanations
of tabular classification tasks. We apply our approach to compare several
state-of-the-art LLMs across different datasets and prompt types. As a
demonstration of their utility, these metrics allow us to identify new
challenges related to LLM hallucinations for XAI narratives.

ÊëòË¶ÅÔºöLLM Âú® XAI ‰∏≠Âø´ÈÄüÁôºÂ±ïÁöÑÊáâÁî®ÊòØÂ∞áÂÆöÈáèËß£ÈáãÔºà‰æãÂ¶Ç SHAPÔºâËΩâÊèõÁÇ∫‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÊïòËø∞Ôºå‰ª•Ëß£ÈáãËºÉÂ∞èÁöÑÈ†êÊ∏¨Ê®°ÂûãÊâÄÂÅöÁöÑÊ±∫Á≠ñ„ÄÇÂú®ÈÄôÂÄãÈ†òÂüü‰∏≠ÔºåÂú®‰∏ç‰æùË≥¥‰∫∫È°ûÂÅèÂ•ΩÁ†îÁ©∂ÊàñË™øÊü•ÁöÑÊÉÖÊ≥Å‰∏ãË©ï‰º∞ÊïòËø∞ËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊû∂Êßã‰∏¶Êé¢Ë®éÂ§öÁ®ÆËá™ÂãïÂåñÊåáÊ®ôÔºå‰ª•Ë©ï‰º∞ LLM ÁîüÊàêÁöÑÊïòËø∞ÔºåÁî®ÊñºËß£ÈáãË°®Ê†ºÂàÜÈ°û‰ªªÂãô„ÄÇÊàëÂÄëÊáâÁî®ÊàëÂÄëÁöÑÂÅöÊ≥ï‰æÜÊØîËºÉ‰∏çÂêåË≥áÊñôÈõÜÂíåÊèêÁ§∫È°ûÂûã‰∏≠ÁöÑÂπæÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLM„ÄÇ‰ΩúÁÇ∫ÂÖ∂ÊïàÁî®ÁöÑÁ§∫ÁØÑÔºåÈÄô‰∫õÊåáÊ®ôËÆìÊàëÂÄëËÉΩÂ§†ÊâæÂá∫Ëàá XAI ÊïòËø∞ÁöÑ LLM ÂπªË¶∫Áõ∏ÈóúÁöÑÊñ∞ÊåëÊà∞„ÄÇ

##### **GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion**
2412.10209v1 by Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner

We propose a novel approach for reconstructing animatable 3D Gaussian avatars
from monocular videos captured by commodity devices like smartphones.
Photorealistic 3D head avatar reconstruction from such recordings is
challenging due to limited observations, which leaves unobserved regions
under-constrained and can lead to artifacts in novel views. To address this
problem, we introduce a multi-view head diffusion model, leveraging its priors
to fill in missing regions and ensure view consistency in Gaussian splatting
renderings. To enable precise viewpoint control, we use normal maps rendered
from FLAME-based head reconstruction, which provides pixel-aligned inductive
biases. We also condition the diffusion model on VAE features extracted from
the input image to preserve details of facial identity and appearance. For
Gaussian avatar reconstruction, we distill multi-view diffusion priors by using
iteratively denoised images as pseudo-ground truths, effectively mitigating
over-saturation issues. To further improve photorealism, we apply latent
upsampling to refine the denoised latent before decoding it into an image. We
evaluate our method on the NeRSemble dataset, showing that GAF outperforms the
previous state-of-the-art methods in novel view synthesis by a 5.34\% higher
SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions
from monocular videos captured on commodity devices.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÁî®Êô∫ÊÖßÂûãÊâãÊ©üÁ≠âÂïÜÂìÅË®≠ÂÇôÊâÄÊãçÊîùÁöÑÂñÆÁúºÂΩ±ÁâáÔºåÈáçÂª∫Âá∫ÂèØÂãïÁï´ÁöÑ 3D È´òÊñØÈ†≠ÂÉè„ÄÇ
ÂæûÈÄôÈ°ûÈåÑË£ΩÂΩ±Áâá‰∏≠ÈáçÂª∫Âá∫ÈÄºÁúüÁöÑ 3D È†≠ÂÉèÈ†≠ÂÉèÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫ËßÄÂØüÊúâÈôêÔºåÈÄô‰ΩøÂæóÊú™ËßÄÂØüÂà∞ÁöÑÂçÄÂüüÂèóÂà∞Á¥ÑÊùü‰∏çË∂≥Ôºå‰∏¶ÂèØËÉΩÂ∞éËá¥Êñ∞Ë¶ñÂúñ‰∏≠Âá∫Áèæ‰∫∫Â∑•Ë£ΩÂìÅ„ÄÇ
ÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§öË¶ñÂúñÈ†≠ÈÉ®Êì¥Êï£Ê®°ÂûãÔºåÂà©Áî®ÂÖ∂ÂÖàÈ©ó‰æÜÂ°´Ë£úÁº∫Â§±ÂçÄÂüü‰∏¶Á¢∫‰øùÈ´òÊñØÊΩëÁÅëÊ∏≤Êüì‰∏≠ÁöÑË¶ñÂúñ‰∏ÄËá¥ÊÄß„ÄÇ
ÁÇ∫‰∫ÜÂØ¶ÁèæÁ≤æÁ¢∫ÁöÑË¶ñÈªûÊéßÂà∂ÔºåÊàëÂÄë‰ΩøÁî®‰∫ÜÂæûÂü∫Êñº FLAME ÁöÑÈ†≠ÈÉ®ÈáçÂª∫ÊâÄÊ∏≤ÊüìÁöÑÊ≥ïÁ∑öË≤ºÂúñÔºåÂÆÉÊèê‰æõ‰∫ÜÂÉèÁ¥†Â∞çÈΩäÁöÑÊ≠∏Á¥çÂÅèÂ∑Æ„ÄÇ
ÊàëÂÄëÈÇÑÊ†πÊìöÂæûËº∏ÂÖ•ÂΩ±ÂÉè‰∏≠ÊèêÂèñÁöÑ VAE ÁâπÂæµÂ∞çÊì¥Êï£Ê®°ÂûãÈÄ≤Ë°åÊ¢ù‰ª∂ÂåñÔºå‰ª•‰øùÁïôÈù¢ÈÉ®ÁâπÂæµÂíåÂ§ñËßÄÁöÑÁ¥∞ÁØÄ„ÄÇ
Â∞çÊñºÈ´òÊñØÈ†≠ÂÉèÈáçÂª∫ÔºåÊàëÂÄë‰ΩøÁî®ÂèçË¶ÜÈôçÂô™ÂΩ±ÂÉè‰ΩúÁÇ∫ÂÅΩÂú∞Èù¢ÂØ¶Ê≥ÅÔºå‰æÜÊèêÂèñÂ§öË¶ñÂúñÊì¥Êï£ÂÖàÈ©óÔºåÊúâÊïàÂú∞Ê∏õËºïÈÅéÈ£ΩÂíåÂïèÈ°å„ÄÇ
ÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÈÄºÁúüÂ∫¶ÔºåÊàëÂÄëÊáâÁî®ÊΩõÂú®Âêë‰∏äÂèñÊ®£‰æÜÊîπÂñÑÈôçÂô™ÊΩõÂú®ÂÄºÔºåÁÑ∂ÂæåÂ∞áÂÖ∂Ëß£Á¢ºÊàêÂΩ±ÂÉè„ÄÇ
ÊàëÂÄëÂú® NeRSemble Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÔºåÈ°ØÁ§∫ GAF Âú®Êñ∞Ë¶ñÂúñÂêàÊàê‰∏≠ÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÊñ∞ÊñπÊ≥ïÔºåSSIM ÂàÜÊï∏È´òÂá∫ 5.34%„ÄÇ
Ê≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂæûÂïÜÂìÅË®≠ÂÇô‰∏äÊãçÊîùÁöÑÂñÆÁúºÂΩ±Áâá‰∏≠ÈáçÂª∫Âá∫Êõ¥È´ò‰øùÁúüÂ∫¶ÁöÑÈ†≠ÂÉè„ÄÇ

##### **Retrieval-Augmented Semantic Parsing: Using Large Language Models to Improve Generalization**
2412.10207v1 by Xiao Zhang, Qianru Meng, Johan Bos

Open-domain semantic parsing remains a challenging task, as models often rely
on heuristics and struggle to handle unseen concepts. In this paper, we
investigate the potential of large language models (LLMs) for this task and
introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective
approach that integrates external lexical knowledge into the parsing process.
Our experiments not only show that LLMs outperform previous encoder-decoder
baselines for semantic parsing, but that RASP further enhances their ability to
predict unseen concepts, nearly doubling the performance of previous models on
out-of-distribution concepts. These findings highlight the promise of
leveraging large language models and retrieval mechanisms for robust and
open-domain semantic parsing.

ÊëòË¶ÅÔºöÈñãÊîæÈ†òÂüüË™ûÁæ©Ëß£Êûê‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÂõ†ÁÇ∫Ê®°ÂûãÈÄöÂ∏∏‰æùË≥¥ÂïüÁôºÊ≥ïÔºå‰∏¶‰∏îÈõ£‰ª•ËôïÁêÜÊú™Ë¶ãÈÅéÁöÑÊ¶ÇÂøµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÄôÈ†Ö‰ªªÂãô‰∏≠ÁöÑÊΩõÂäõÔºå‰∏¶ÊèêÂá∫Ê™¢Á¥¢Â¢ûÂº∑Ë™ûÁæ©Ëß£Êûê (RASP)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Â∞áÂ§ñÈÉ®Ë©ûÂΩôÁü•Ë≠òÊï¥ÂêàÂà∞Ëß£ÊûêÈÅéÁ®ã‰∏≠„ÄÇÊàëÂÄëÁöÑÂØ¶È©ó‰∏çÂÉÖË°®Êòé LLM ÂÑ™ÊñºË™ûÁæ©Ëß£ÊûêÁöÑÂÖàÂâçÁ∑®Á¢ºÂô® - Ëß£Á¢ºÂô®Âü∫Á∑öÔºåËÄå‰∏î RASP ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑‰∫ÜÂÆÉÂÄëÈ†êÊ∏¨Êú™Ë¶ãÈÅéÊ¶ÇÂøµÁöÑËÉΩÂäõÔºåÂπæ‰πéÂ∞áÂÖàÂâçÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂ§ñÊ¶ÇÂøµ‰∏äÁöÑÊïàËÉΩÊèêÈ´ò‰∫Ü‰∏ÄÂÄç„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂíåÊ™¢Á¥¢Ê©üÂà∂ÈÄ≤Ë°åÂº∑ÂÅ•‰∏îÈñãÊîæÈ†òÂüüË™ûÁæ©Ëß£ÊûêÁöÑÊΩõÂäõ„ÄÇ

##### **From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection**
2412.10198v1 by Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang

Tool-calling has changed Large Language Model (LLM) applications by
integrating external tools, significantly enhancing their functionality across
diverse tasks. However, this integration also introduces new security
vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which
have not been extensively studied. To fill this gap, we present ToolCommander,
a novel framework designed to exploit vulnerabilities in LLM tool-calling
systems through adversarial tool injection. Our framework employs a
well-designed two-stage attack strategy. Firstly, it injects malicious tools to
collect user queries, then dynamically updates the injected tools based on the
stolen information to enhance subsequent attacks. These stages enable
ToolCommander to execute privacy theft, launch denial-of-service attacks, and
even manipulate business competition by triggering unscheduled tool-calling.
Notably, the ASR reaches 91.67% for privacy theft and hits 100% for
denial-of-service and unscheduled tool calling in certain cases. Our work
demonstrates that these vulnerabilities can lead to severe consequences beyond
simple misuse of tool-calling systems, underscoring the urgent need for robust
defensive strategies to secure LLM Tool-calling systems.

ÊëòË¶ÅÔºöÂ∑•ÂÖ∑ÂëºÂè´Â∑≤ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Â∑•ÂÖ∑‰æÜÊîπËÆäÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊáâÁî®Á®ãÂºèÔºåÂ§ßÂπÖÊèêÂçáÂÖ∂Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÂäüËÉΩ„ÄÇ‰∏çÈÅéÔºåÈÄôÁ®ÆÊï¥Âêà‰πüÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÂÆâÂÖ®ÊÄßÊºèÊ¥ûÔºåÁâπÂà•ÊòØÂú® LLM ÁöÑÂ∑•ÂÖ∑ÊéíÁ®ãÊ©üÂà∂‰∏≠ÔºåËÄåÈÄôÂ∞öÊú™Áç≤ÂæóÂª£Ê≥õÁöÑÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÊèêÂá∫ ToolCommanderÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÊó®Âú®ÈÄèÈÅéÂ∞çÊäóÊÄßÁöÑÂ∑•ÂÖ∑Ê≥®ÂÖ•‰æÜÂà©Áî® LLM Â∑•ÂÖ∑ÂëºÂè´Á≥ªÁµ±‰∏≠ÁöÑÊºèÊ¥û„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊé°Áî®Á≤æÂøÉË®≠Ë®àÁöÑÂÖ©ÈöéÊÆµÊîªÊìäÁ≠ñÁï•„ÄÇÈ¶ñÂÖàÔºåÂÆÉÊúÉÊ≥®ÂÖ•ÊÉ°ÊÑèÂ∑•ÂÖ∑‰æÜÊî∂ÈõÜ‰ΩøÁî®ËÄÖÊü•Ë©¢ÔºåÁÑ∂ÂæåÊ†πÊìöÁ´äÂèñÁöÑË≥áË®äÂãïÊÖãÊõ¥Êñ∞Ê≥®ÂÖ•ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•Âä†Âº∑ÂæåÁ∫åÊîªÊìä„ÄÇÈÄô‰∫õÈöéÊÆµËÆì ToolCommander ËÉΩÂ§†Âü∑Ë°åÈö±ÁßÅÁ´äÂèñ„ÄÅÁôºÂãïÈòªÊñ∑ÊúçÂãôÊîªÊìäÔºåÁîöËá≥ÈÄèÈÅéËß∏ÁôºÈùûÈ†êÂÆöÁöÑÂ∑•ÂÖ∑ÂëºÂè´‰æÜÊìçÁ∏±ÂïÜÊ•≠Á´∂Áà≠„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåASR ÈÅîÂà∞ 91.67% ÁöÑÈö±ÁßÅÁ´äÂèñÔºå‰∏¶ÈÅîÂà∞ 100% ÁöÑÈòªÊñ∑ÊúçÂãôÂíåÈùûÈ†êÂÆöÂ∑•ÂÖ∑ÂëºÂè´„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈÄô‰∫õÊºèÊ¥ûÂèØËÉΩÊúÉÂ∞éËá¥Âö¥ÈáçÁöÑÂæåÊûúÔºåËÄåÈÄô‰∏çÂè™ÊòØÂ∑•ÂÖ∑ÂëºÂè´Á≥ªÁµ±ÁöÑÊø´Áî®ÔºåÈÄôÂº∑Ë™ø‰∫ÜÁ¢∫‰øù LLM Â∑•ÂÖ∑ÂëºÂè´Á≥ªÁµ±ÂÆâÂÖ®ÁöÑÁ©©ÂÅ•Èò≤Á¶¶Á≠ñÁï•ÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇ

##### **Solving Robust Markov Decision Processes: Generic, Reliable, Efficient**
2412.10185v1 by Tobias Meggendorfer, Maximilian Weininger, Patrick Wienh√∂ft

Markov decision processes (MDP) are a well-established model for sequential
decision-making in the presence of probabilities. In robust MDP (RMDP), every
action is associated with an uncertainty set of probability distributions,
modelling that transition probabilities are not known precisely. Based on the
known theoretical connection to stochastic games, we provide a framework for
solving RMDPs that is generic, reliable, and efficient. It is *generic* both
with respect to the model, allowing for a wide range of uncertainty sets,
including but not limited to intervals, $L^1$- or $L^2$-balls, and polytopes;
and with respect to the objective, including long-run average reward,
undiscounted total reward, and stochastic shortest path. It is *reliable*, as
our approach not only converges in the limit, but provides precision guarantees
at any time during the computation. It is *efficient* because -- in contrast to
state-of-the-art approaches -- it avoids explicitly constructing the underlying
stochastic game. Consequently, our prototype implementation outperforms
existing tools by several orders of magnitude and can solve RMDPs with a
million states in under a minute.

ÊëòË¶ÅÔºöÈ¶¨ÂèØÂ§´Ê±∫Á≠ñÈÅéÁ®ã (MDP) ÊòØ‰∏ÄÁ®ÆÂú®Ê©üÁéáÂ≠òÂú®ÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åÈ†ÜÂ∫èÊ±∫Á≠ñÁöÑÊó¢ÂÆöÊ®°Âûã„ÄÇÂú®Âº∑ÂÅ• MDP (RMDP) ‰∏≠ÔºåÊØèÂÄãÂãï‰ΩúÈÉΩËàáÊ©üÁéáÂàÜ‰ΩàÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈõÜÂêàÁõ∏ÈóúÔºåÂª∫Ê®°ÁÇ∫ËΩâÁßªÊ©üÁéá‰∏¶ÈùûÁ≤æÁ¢∫Â∑≤Áü•„ÄÇÊ†πÊìöÂ∑≤Áü•ÁöÑËàáÈö®Ê©üÂçöÂºàÁöÑÁêÜË´ñÈÄ£ÁµêÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãËß£Ê±∫ RMDP ÁöÑÊû∂ÊßãÔºåÂÆÉÂÖ∑ÊúâÈÄöÁî®ÊÄß„ÄÅÂèØÈù†ÊÄßËàáÊïàÁéá„ÄÇÂÆÉÂú®Ê®°ÂûãÊñπÈù¢ÊòØ„ÄåÈÄöÁî®ÁöÑ„ÄçÔºåÂÖÅË®±‰ΩøÁî®Âª£Ê≥õÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈõÜÂêàÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñºÂçÄÈñì„ÄÅ$L^1$ Êàñ $L^2$ ÁêÉÈ´îÂíåÂ§öÈù¢È´îÔºõÂú®ÁõÆÊ®ôÊñπÈù¢‰πüÊòØÈÄöÁî®ÁöÑÔºåÂåÖÊã¨Èï∑ÊúüÂπ≥ÂùáÁçéÂãµ„ÄÅÊú™Ë≤ºÁèæÁ∏ΩÁçéÂãµÂíåÈö®Ê©üÊúÄÁü≠Ë∑ØÂæë„ÄÇÂÆÉÂæà„ÄåÂèØÈù†„ÄçÔºåÂõ†ÁÇ∫ÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÊúÉÂú®Ê•µÈôê‰∏≠Êî∂ÊñÇÔºåÈÇÑÊúÉÂú®ÈÅãÁÆóÈÅéÁ®ã‰∏≠Èö®ÊôÇÊèê‰æõÁ≤æÁ¢∫Â∫¶‰øùË≠â„ÄÇÂÆÉÂæà„ÄåÊúâÊïà„ÄçÔºåÂõ†ÁÇ∫ËàáÊúÄÂÖàÈÄ≤ÁöÑÂÅöÊ≥ïÁõ∏ÂèçÔºåÂÆÉÈÅøÂÖçÊòéÁ¢∫Âª∫ÊßãÂü∫Á§éÈö®Ê©üÂçöÂºà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑÂéüÂûãÂØ¶‰ΩúÂú®ÂπæÂÄãÊï∏ÈáèÁ¥ö‰∏äÈÉΩÂÑ™ÊñºÁèæÊúâÂ∑•ÂÖ∑ÔºåËÄå‰∏îÂèØ‰ª•Âú®‰∏çÂà∞‰∏ÄÂàÜÈêòÁöÑÊôÇÈñìÂÖßËß£Ê±∫ÊìÅÊúâÊï∏ÁôæËê¨ÂÄãÁãÄÊÖãÁöÑ RMDP„ÄÇ

##### **Multi-Head Encoding for Extreme Label Classification**
2412.10182v1 by Daojun Liang, Haixia Zhang, Dongfeng Yuan, Minggao Zhang

The number of categories of instances in the real world is normally huge, and
each instance may contain multiple labels. To distinguish these massive labels
utilizing machine learning, eXtreme Label Classification (XLC) has been
established. However, as the number of categories increases, the number of
parameters and nonlinear operations in the classifier also rises. This results
in a Classifier Computational Overload Problem (CCOP). To address this, we
propose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla
classifier with a multi-head classifier. During the training process, MHE
decomposes extreme labels into the product of multiple short local labels, with
each head trained on these local labels. During testing, the predicted labels
can be directly calculated from the local predictions of each head. This
reduces the computational load geometrically. Then, according to the
characteristics of different XLC tasks, e.g., single-label, multi-label, and
model pretraining tasks, three MHE-based implementations, i.e., Multi-Head
Product, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more
effectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can
achieve performance approximately equivalent to that of the vanilla classifier
by generalizing the low-rank approximation problem from Frobenius-norm to
Cross-Entropy. Experimental results show that the proposed methods achieve
state-of-the-art performance while significantly streamlining the training and
inference processes of XLC tasks. The source code has been made public at
https://github.com/Anoise/MHE.

ÊëòË¶ÅÔºö<paragraph>ÁèæÂØ¶‰∏ñÁïå‰∏≠ÂØ¶‰æãÈ°ûÂà•ÁöÑÊï∏ÈáèÈÄöÂ∏∏ÈæêÂ§ßÔºå‰∏îÊØèÂÄãÂØ¶‰æãÂèØËÉΩÂåÖÂê´Â§öÂÄãÊ®ôÁ±§„ÄÇÁÇ∫‰∫ÜÂà©Áî®Ê©üÂô®Â≠∏Áøí‰æÜÂçÄÂàÜÈÄô‰∫õÂ§ßÈáèÁöÑÊ®ôÁ±§ÔºåÂ∑≤Âª∫Á´ãÊ•µÁ´ØÊ®ôÁ±§ÂàÜÈ°û (XLC)„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóÈ°ûÂà•Êï∏ÈáèÁöÑÂ¢ûÂä†ÔºåÂàÜÈ°ûÂô®‰∏≠ÁöÑÂèÉÊï∏ÂíåÈùûÁ∑öÊÄßÈÅãÁÆóÊï∏Èáè‰πüÊúÉÂ¢ûÂä†„ÄÇÈÄôÂ∞éËá¥ÂàÜÈ°ûÂô®Ë®àÁÆóÈÅéËºâÂïèÈ°å (CCOP)„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öÈ†≠Á∑®Á¢º (MHE) Ê©üÂà∂ÔºåÂÆÉ‰ª•‰∏ÄÂÄãÂ§öÈ†≠ÂàÜÈ°ûÂô®Âèñ‰ª£‰∫ÜÈ¶ôËçâÂàÜÈ°ûÂô®„ÄÇÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÔºåMHE Â∞áÊ•µÁ´ØÊ®ôÁ±§ÂàÜËß£ÁÇ∫Â§öÂÄãËºÉÁü≠ÁöÑÂ±ÄÈÉ®Ê®ôÁ±§ÁöÑ‰πòÁ©çÔºåÊØèÂÄãÈ†≠ÈÉ®ÈÉΩÂú®ÈÄô‰∫õÂ±ÄÈÉ®Ê®ôÁ±§‰∏äÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÂú®Ê∏¨Ë©¶ÊúüÈñìÔºåÂèØ‰ª•Ê†πÊìöÊØèÂÄãÈ†≠ÈÉ®ÁöÑÂ±ÄÈÉ®È†êÊ∏¨Áõ¥Êé•Ë®àÁÆóÈ†êÊ∏¨Ê®ôÁ±§„ÄÇÈÄôÂú®Âπæ‰Ωï‰∏äÊ∏õÂ∞ë‰∫ÜË®àÁÆóË≤†Ëºâ„ÄÇÁÑ∂ÂæåÔºåÊ†πÊìö‰∏çÂêåÁöÑ XLC ‰ªªÂãôÁöÑÁâπÂæµÔºå‰æãÂ¶ÇÂñÆÊ®ôÁ±§„ÄÅÂ§öÊ®ôÁ±§ÂíåÊ®°ÂûãÈ†êË®ìÁ∑¥‰ªªÂãôÔºåÊèêÂá∫‰∫Ü‰∏âÁ®ÆÂü∫Êñº MHE ÁöÑÂØ¶ÁèæÔºåÂç≥Â§öÈ†≠‰πòÁ©ç„ÄÅÂ§öÈ†≠‰∏≤ËÅØÂíåÂ§öÈ†≠ÊäΩÊ®£Ôºå‰ª•Êõ¥ÊúâÊïàÂú∞ÊáâÂ∞ç CCOP„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÁêÜË´ñ‰∏äË≠âÊòé‰∫Ü MHE ÂèØ‰ª•ÈÄöÈÅéÂ∞á‰ΩéÁß©ÈÄºËøëÂïèÈ°åÂæû Frobenius ÁØÑÊï∏Êé®Âª£Âà∞‰∫§ÂèâÁÜµ‰æÜÂØ¶ÁèæËàáÈ¶ôËçâÂàÜÈ°ûÂô®Ëøë‰ººÁöÑÊÄßËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®È°ØËëóÁ∞°Âåñ XLC ‰ªªÂãôÁöÑË®ìÁ∑¥ÂíåÊé®ÁêÜÈÅéÁ®ãÁöÑÂêåÊôÇÔºåÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÊ∫ê‰ª£Á¢ºÂ∑≤ÂÖ¨ÈñãÂú® https://github.com/Anoise/MHE„ÄÇ</paragraph>

##### **SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models**
2412.10178v1 by Hung Nguyen, Quang Qui-Vinh Nguyen, Khoi Nguyen, Rang Nguyen

Given an input video of a person and a new garment, the objective of this
paper is to synthesize a new video where the person is wearing the specified
garment while maintaining spatiotemporal consistency. While significant
advances have been made in image-based virtual try-ons, extending these
successes to video often results in frame-to-frame inconsistencies. Some
approaches have attempted to address this by increasing the overlap of frames
across multiple video chunks, but this comes at a steep computational cost due
to the repeated processing of the same frames, especially for long video
sequence. To address these challenges, we reconceptualize video virtual try-on
as a conditional video inpainting task, with garments serving as input
conditions. Specifically, our approach enhances image diffusion models by
incorporating temporal attention layers to improve temporal coherence. To
reduce computational overhead, we introduce ShiftCaching, a novel technique
that maintains temporal consistency while minimizing redundant computations.
Furthermore, we introduce the \dataname~dataset, a new video try-on dataset
featuring more complex backgrounds, challenging movements, and higher
resolution compared to existing public datasets. Extensive experiments show
that our approach outperforms current baselines, particularly in terms of video
consistency and inference speed. Data and code are available at
https://github.com/VinAIResearch/swift-try

ÊëòË¶ÅÔºöÁµ¶ÂÆö‰∏ÄÂÄã‰∫∫ÁöÑËº∏ÂÖ•ÂΩ±ÁâáÂíå‰∏Ä‰ª∂Êñ∞Ë°£ÊúçÔºåÊú¨ÊñáÁöÑÁõÆÊ®ôÊòØÂêàÊàê‰∏ÄÂÄãÊñ∞ÂΩ±ÁâáÔºåÂΩ±Áâá‰∏≠ÁöÑ‰∫∫Á©øËëóÊåáÂÆöÁöÑË°£ÊúçÔºåÂêåÊôÇ‰øùÊåÅÊôÇÁ©∫‰∏ÄËá¥ÊÄß„ÄÇÈõñÁÑ∂Âú®Âü∫ÊñºÂΩ±ÂÉèÁöÑËôõÊì¨Ë©¶Á©øÊñπÈù¢Â∑≤ÊúâÈ°ØËëóÈÄ≤Â±ïÔºå‰ΩÜÂ∞áÈÄô‰∫õÊàêÂäüÊé®Âª£Âà∞ÂΩ±Áâá‰∏≠ÔºåÈÄöÂ∏∏ÊúÉÂ∞éËá¥ÈÄêÂπÄ‰∏ç‰∏ÄËá¥„ÄÇ‰∏Ä‰∫õÊñπÊ≥ïÂ∑≤ÂòóË©¶ÈÄèÈÅéÂ¢ûÂä†Â§öÂÄãÂΩ±ÁâáÂçÄÂ°ä‰∏≠ÂπÄÁöÑÈáçÁñä‰æÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºå‰ΩÜÁî±ÊñºÈáçË§áËôïÁêÜÁõ∏ÂêåÁöÑÂπÄÔºåÁâπÂà•ÊòØÂ∞çÊñºÈï∑ÁöÑÂΩ±ÁâáÂ∫èÂàóÔºåÈÄôÊúÉÂ∏∂‰æÜÈ´òÊòÇÁöÑÈÅãÁÆóÊàêÊú¨„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂ∞áÂΩ±ÁâáËôõÊì¨Ë©¶Á©øÈáçÊñ∞Ê¶ÇÂøµÂåñÁÇ∫Ê¢ù‰ª∂ÂΩ±Áâá‰øÆÂæ©‰ªªÂãôÔºå‰∏¶Â∞áÊúçË£ù‰ΩúÁÇ∫Ëº∏ÂÖ•Ê¢ù‰ª∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÈÄèÈÅéÂä†ÂÖ•ÊôÇÈñìÊ≥®ÊÑèÂäõÂ±§‰æÜÂ¢ûÂº∑ÂΩ±ÂÉèÊì¥Êï£Ê®°ÂûãÔºå‰ª•ÊîπÂñÑÊôÇÈñìÁõ∏Âπ≤ÊÄß„ÄÇÁÇ∫‰∫ÜÊ∏õÂ∞ëÈÅãÁÆóË≤†ÊìîÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ShiftCachingÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÊäÄË°ìÔºåÂèØÂú®ÊúÄÂ§ßÁ®ãÂ∫¶Ê∏õÂ∞ëÈáçË§áÈÅãÁÆóÁöÑÂêåÊôÇÔºåÁ∂≠ÊåÅÊôÇÈñì‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü \dataname~Ë≥áÊñôÈõÜÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑÂΩ±ÁâáË©¶Á©øË≥áÊñôÈõÜÔºåËàáÁèæÊúâÁöÑÂÖ¨ÈñãË≥áÊñôÈõÜÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥Ë§áÈõúÁöÑËÉåÊôØ„ÄÅÂÖ∑ÊåëÊà∞ÊÄßÁöÑÂãï‰ΩúÂíåÊõ¥È´òÁöÑËß£ÊûêÂ∫¶„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÁõÆÂâçÁöÑÂü∫Ê∫ñÔºåÁâπÂà•ÊòØÂú®ÂΩ±Áâá‰∏ÄËá¥ÊÄßÂíåÊé®Ë´ñÈÄüÂ∫¶ÊñπÈù¢„ÄÇË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/VinAIResearch/swift-try ÂèñÂæó

##### **Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation**
2412.10163v1 by Federico Julian Camerota Verd√π, Lorenzo Castelli, Luca Bortolussi

We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for
deep reinforcement learning (DRL) based combinatorial optimization improvement
heuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson
Problem, LRBS significantly enhances both in-distribution performance and
generalization to larger problem instances, achieving optimality gaps that
outperform existing improvement heuristics and narrowing the gap with
state-of-the-art constructive methods. We also extend our analysis to two
pickup and delivery TSP variants to validate our results. Finally, we employ
our search strategy for offline and online adaptation of the pre-trained
improvement policy, leading to improved search performance and surpassing
recent adaptive methods for constructive heuristics.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫ÜÊúâÈôêÂ±ïÈñãÊ≥¢ÊùüÊêúÂ∞ã (LRBS)Ôºå‰∏ÄÁ®ÆÂü∫ÊñºÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (DRL) ÁöÑÁµÑÂêàÂÑ™ÂåñÊîπÂñÑÂïüÁôºÂºèÊºîÁÆóÊ≥ïÁöÑÊ≥¢ÊùüÊêúÂ∞ãÁ≠ñÁï•„ÄÇÂà©Áî®Âú®Ê≠êÂπæÈáåÂæóÊóÖË°åÂïÜÂïèÈ°å‰∏äÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºåLRBS Â§ßÂπÖÊèêÂçá‰∫ÜÂàÜ‰ΩàÂÖßÊïàËÉΩÂíåÂ∞çËºÉÂ§ßÂûãÂïèÈ°åÂØ¶‰æãÁöÑÊ≥õÂåñÔºåÈÅîÂà∞‰∫ÜÂÑ™ÊñºÁèæÊúâÊîπÂñÑÂïüÁôºÂºèÊºîÁÆóÊ≥ïÁöÑÊúÄ‰Ω≥ÂåñÂ∑ÆË∑ùÔºå‰∏¶Á∏ÆÂ∞è‰∫ÜËàáÊúÄÂÖàÈÄ≤Âª∫ÊßãÊñπÊ≥ïÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄë‰πüÂ∞áÂàÜÊûêÂª∂‰º∏Âà∞ÂÖ©ÂÄãÂèñË≤®ÂíåÈÅûÈÄÅ TSP ËÆäÈ´îÔºå‰ª•È©óË≠âÊàëÂÄëÁöÑÁµêÊûú„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé°Áî®ÊàëÂÄëÁöÑÊêúÂ∞ãÁ≠ñÁï•ÈÄ≤Ë°åÈ†êÂÖàË®ìÁ∑¥ÁöÑÊîπÂñÑÁ≠ñÁï•ÁöÑÈõ¢Á∑öÂíåÁ∑ö‰∏äÈÅ©ÊáâÔºåÊèêÂçá‰∫ÜÊêúÂ∞ãÊïàËÉΩÔºå‰∏¶Ë∂ÖË∂ä‰∫ÜÂª∫ÊßãÂïüÁôºÂºèÊºîÁÆóÊ≥ïÁöÑËøëÊúüÈÅ©ÊáâÊñπÊ≥ï„ÄÇ

##### **Direct Encoding of Declare Constraints in ASP**
2412.10152v1 by Francesco Chiariello, Valeria Fionda, Antonio Ielo, Francesco Ricca

Answer Set Programming (ASP), a well-known declarative logic programming
paradigm, has recently found practical application in Process Mining. In
particular, ASP has been used to model tasks involving declarative
specifications of business processes. In this area, Declare stands out as the
most widely adopted declarative process modeling language, offering a means to
model processes through sets of constraints valid traces must satisfy, that can
be expressed in Linear Temporal Logic over Finite Traces (LTLf). Existing
ASP-based solutions encode Declare constraints by modeling the corresponding
LTLf formula or its equivalent automaton which can be obtained using
established techniques. In this paper, we introduce a novel encoding for
Declare constraints that directly models their semantics as ASP rules,
eliminating the need for intermediate representations. We assess the
effectiveness of this novel approach on two Process Mining tasks by comparing
it with alternative ASP encodings and a Python library for Declare. Under
consideration in Theory and Practice of Logic Programming (TPLP).

ÊëòË¶ÅÔºö<paragraph>Answer Set Programming (ASP)Ôºå‰∏ÄÁ®ÆËëóÂêçÁöÑÂÆ£ÂëäÂºèÈÇèËºØÁ®ãÂºèË®≠Ë®àÁØÑ‰æãÔºåÊúÄËøëÂú®ÊµÅÁ®ãÊé¢Âãò‰∏≠ÊâæÂà∞ÂØ¶Áî®ÊáâÁî®„ÄÇÁâπÂà•ÊòØÔºåASP Â∑≤Áî®ÊñºÂª∫Ê®°Ê∂âÂèäÊ•≠ÂãôÊµÅÁ®ãÂÆ£ÂëäÂºèË¶èÁØÑÁöÑ‰ªªÂãô„ÄÇÂú®ÈÄôÂÄãÈ†òÂüüÔºåDeclare ‰ΩúÁÇ∫ÊúÄÂª£Ê≥õÊé°Áî®ÁöÑÂÆ£ÂëäÂºèÊµÅÁ®ãÂª∫Ê®°Ë™ûË®ÄËÄåËÑ´Á©éËÄåÂá∫ÔºåÊèê‰æõ‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÈÄèÈÅéÂøÖÈ†àÊªøË∂≥ÁöÑÁ¥ÑÊùüÈõÜ‰æÜÂª∫Ê®°ÊµÅÁ®ãÔºåÈÄô‰∫õÁ¥ÑÊùüÈõÜÂèØ‰ª•Áî®ÊúâÈôêËªåË∑°‰∏äÁöÑÁ∑öÊÄßÊôÇÂ∫èÈÇèËºØ (LTLf) ‰æÜË°®Á§∫„ÄÇÁèæÊúâÁöÑÂü∫Êñº ASP ÁöÑËß£Ê±∫ÊñπÊ°àÈÄèÈÅéÂª∫Ê®°Â∞çÊáâÁöÑ LTLf ÂÖ¨ÂºèÊàñÂèØ‰ª•‰ΩøÁî®Â∑≤Âª∫Á´ãÊäÄË°ìÂèñÂæóÁöÑÁ≠âÊïàËá™ÂãïÊ©ü‰æÜÁ∑®Á¢º Declare Á¥ÑÊùü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞ÁöÑ Declare Á¥ÑÊùüÁ∑®Á¢ºÔºåÁõ¥Êé•Â∞áÂÖ∂Ë™ûÊÑèÂª∫Ê®°ÁÇ∫ ASP Ë¶èÂâáÔºåÊ∂àÈô§‰∫ÜÂ∞ç‰∏≠ÈñìË°®Á§∫ÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÈÄèÈÅéËàáÂÇôÁî®ÁöÑ ASP Á∑®Á¢ºÂíå Declare ÁöÑ Python ÂáΩÂºèÂ∫´ÈÄ≤Ë°åÊØîËºÉÔºåË©ï‰º∞ÈÄôÁ®ÆÊñ∞ÊñπÊ≥ïÂú®ÂÖ©ÂÄãÊµÅÁ®ãÊé¢Âãò‰ªªÂãô‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÂú®ÈÇèËºØÁ®ãÂºèË®≠Ë®àÁêÜË´ñËàáÂØ¶Âãô (TPLP) ‰∏≠ËÄÉÈáè‰∏≠„ÄÇ</paragraph>

##### **VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation**
2412.10151v1 by Hyeonseok Lim, Dongjae Shin, Seohyun Song, Inho Won, Minjun Kim, Junghun Yuk, Haneol Jang, KyungTae Lim

We propose the VLR-Bench, a visual question answering (VQA) benchmark for
evaluating vision language models (VLMs) based on retrieval augmented
generation (RAG). Unlike existing evaluation datasets for external
knowledge-based VQA, the proposed VLR-Bench includes five input passages. This
allows testing of the ability to determine which passage is useful for
answering a given query, a capability lacking in previous research. In this
context, we constructed a dataset of 32,000 automatically generated
instruction-following examples, which we denote as VLR-IF. This dataset is
specifically designed to enhance the RAG capabilities of VLMs by enabling them
to learn how to generate appropriate answers based on input passages. We
evaluated the validity of the proposed benchmark and training data and verified
its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3
model. The proposed VLR-Bench and VLR-IF datasets are publicly available
online.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ VLR-BenchÔºå‰∏ÄÁ®ÆË¶ñË¶∫ÂïèÁ≠î (VQA) Âü∫Ê∫ñÔºåÁî®ÊñºÊ†πÊìöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Ë©ï‰º∞Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)„ÄÇËàáÁèæÊúâÂ§ñÈÉ®Áü•Ë≠òÂ∫´ VQA Ë©ï‰º∞Ë≥áÊñôÈõÜ‰∏çÂêåÔºåÊâÄÊèêÂá∫ÁöÑ VLR-Bench ÂåÖÂê´‰∫îÂÄãËº∏ÂÖ•ÊÆµËêΩ„ÄÇÈÄôÂÖÅË®±Ê∏¨Ë©¶Á¢∫ÂÆöÂì™ÂÄãÊÆµËêΩÊúâÂä©ÊñºÂõûÁ≠îÁâπÂÆöÊü•Ë©¢ÁöÑËÉΩÂäõÔºåÈÄôÈ†ÖÂäüËÉΩÂú®ÂÖàÂâçÁöÑÁ†îÁ©∂‰∏≠ÊúâÊâÄÊ¨†Áº∫„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜÔºåÂåÖÂê´ 32,000 ÂÄãËá™ÂãïÁî¢ÁîüÁöÑÈÅµÂæ™Êåá‰ª§ÁØÑ‰æãÔºåÊàëÂÄëÂ∞áÂÖ∂Ë°®Á§∫ÁÇ∫ VLR-IF„ÄÇÊ≠§Ë≥áÊñôÈõÜÁâπÂà•Ë®≠Ë®àÁî®ÊñºÂ¢ûÂº∑ VLM ÁöÑ RAG ÂäüËÉΩÔºåËÆìÂÆÉÂÄëËÉΩÂ§†Â≠∏ÁøíÂ¶Ç‰ΩïÊ†πÊìöËº∏ÂÖ•ÊÆµËêΩÁî¢ÁîüÈÅ©Áï∂ÁöÑÁ≠îÊ°à„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÊâÄÊèêÂá∫ÁöÑÂü∫Ê∫ñÂíåË®ìÁ∑¥Ë≥áÊñôÁöÑÊúâÊïàÊÄßÔºå‰∏¶‰ΩøÁî®ÊúÄÂÖàÈÄ≤ÁöÑÂü∫Êñº Llama3 ÁöÑ VLMÔºåÂç≥ Llava-Llama-3 Ê®°ÂûãÔºåÈ©óË≠âÂÖ∂ÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑ VLR-Bench Âíå VLR-IF Ë≥áÊñôÈõÜÂ∑≤ÂÖ¨ÈñãÂú®Á∑ö‰∏ä„ÄÇ

##### **TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse Analysis with Prompt Engineering**
2412.10139v1 by Bingru Li, Han Wang

The capacity of LLMs to carry out automated qualitative analysis has been
questioned by corpus linguists, and it has been argued that corpus-based
discourse analysis incorporating LLMs is hindered by issues of unsatisfying
performance, hallucination, and irreproducibility. Our proposed method,
TACOMORE, aims to address these concerns by serving as an effective prompting
framework in this domain. The framework consists of four principles, i.e.,
Task, Context, Model and Reproducibility, and specifies five fundamental
elements of a good prompt, i.e., Role Description, Task Definition, Task
Procedures, Contextual Information and Output Format. We conduct experiments on
three LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that
TACOMORE helps improve LLM performance in three representative discourse
analysis tasks, i.e., the analysis of keywords, collocates and concordances,
based on an open corpus of COVID-19 research articles. Our findings show the
efficacy of the proposed prompting framework TACOMORE in corpus-based discourse
analysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and
provide novel insights into the application and evaluation of LLMs in automated
qualitative studies.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂü∑Ë°åËá™ÂãïÂåñÂÆöÊÄßÂàÜÊûêÁöÑËÉΩÂäõÂ∑≤ÂèóÂà∞Ë™ûÊñôÂ∫´Ë™ûË®ÄÂ≠∏ÂÆ∂ÁöÑË≥™ÁñëÔºåÊúâ‰∫∫Ë™çÁÇ∫ÔºåÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑË™ûÊñôÂ∫´ÁÇ∫Âü∫Á§éÁöÑË™ûÁØáÂàÜÊûêÂèóÂà∞‰∏ç‰ª§‰∫∫ÊªøÊÑèÁöÑË°®Áèæ„ÄÅÂπªË¶∫Âíå‰∏çÂèØË§áË£ΩÊÄßÁ≠âÂïèÈ°åÁöÑÈòªÁ§ô„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ï TACOMORE Êó®Âú®ÈÄèÈÅé‰ΩúÁÇ∫Ê≠§È†òÂüü‰∏≠ÊúâÊïàÁöÑÊèêÁ§∫Êû∂Êßã‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊ≠§Êû∂ÊßãÂåÖÂê´ÂõõÈ†ÖÂéüÂâáÔºåÂç≥‰ªªÂãô„ÄÅËÑàÁµ°„ÄÅÊ®°ÂûãÂíåÂèØË§áË£ΩÊÄßÔºå‰∏¶ÊåáÂÆöËâØÂ•ΩÊèêÁ§∫ÁöÑ‰∫îÂÄãÂü∫Êú¨ÂÖÉÁ¥†ÔºåÂç≥ËßíËâ≤ÊèèËø∞„ÄÅ‰ªªÂãôÂÆöÁæ©„ÄÅ‰ªªÂãôÁ®ãÂ∫è„ÄÅËÑàÁµ°Ë≥áË®äÂíåËº∏Âá∫Ê†ºÂºè„ÄÇÊàëÂÄëÂ∞ç‰∏âÂÄãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂü∑Ë°åÂØ¶È©óÔºåÂç≥ GPT-4o„ÄÅGemini-1.5-Pro Âíå Gemini-1.5.FlashÔºåÁôºÁèæ TACOMORE ÊúâÂä©ÊñºÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®‰∏âÈ†Ö‰ª£Ë°®ÊÄßË™ûÁØáÂàÜÊûê‰ªªÂãô‰∏≠ÁöÑË°®ÁèæÔºåÂç≥ÈóúÈçµÂ≠ó„ÄÅÊê≠ÈÖçË©ûÂíåÂÖ±ÁèæÂàÜÊûêÔºåÈÄô‰∫õ‰ªªÂãôÂü∫Êñº COVID-19 Á†îÁ©∂ÊñáÁ´†ÁöÑÈñãÊîæË™ûÊñôÂ∫´„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÊèêÂá∫ÁöÑÊèêÁ§∫Êû∂Êßã TACOMORE Âú®Ë™ûÊñôÂ∫´ÁÇ∫Âü∫Á§éÁöÑË™ûÁØáÂàÜÊûê‰∏≠ÔºåÂú®Ê∫ñÁ¢∫ÊÄß„ÄÅÂÄ´ÁêÜÊÄß„ÄÅÊé®ÁêÜÂíåÂèØË§áË£ΩÊÄßÊñπÈù¢ÂÖ∑ÊúâÊïàËÉΩÔºå‰∏¶Êèê‰æõÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ëá™ÂãïÂåñÂÆöÊÄßÁ†îÁ©∂‰∏≠ÊáâÁî®ÂíåË©ï‰º∞ÁöÑÊñ∞Ë¶ãËß£„ÄÇ

##### **ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**
2412.10138v1 by Yang Qin, Chao Chen, Zhihang Fu, Ze Chen, Dezhong Peng, Peng Hu, Jieping Ye

Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by
large language models (LLMs), the latest state-of-the-art techniques are still
trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which
limits their applicability in open scenarios. To address this challenge, we
propose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to
improve the comprehensive capabilities of open-source LLMs for Text2SQL,
thereby providing a more practical solution. Our approach begins with
multi-task supervised fine-tuning (SFT) using various synthetic training data
related to SQL generation. Unlike existing SFT-based Text2SQL methods, we
introduced several additional SFT tasks, including schema linking, noise
correction, and continuation writing. Engaging in a variety of SQL generation
tasks enhances the model's understanding of SQL syntax and improves its ability
to generate high-quality SQL queries. Additionally, inspired by the
collaborative modes of LLM agents, we introduce a Multitask Collaboration
Prompting (MCP) strategy. This strategy leverages collaboration across several
SQL-related tasks to reduce hallucinations during SQL generation, thereby
maximizing the potential of enhancing Text2SQL performance through explicit
multitask capabilities. Extensive experiments and in-depth analyses have been
performed on eight open-source LLMs and five widely-used benchmarks. The
results demonstrate that our proposal outperforms the latest Text2SQL methods
and yields leading performance.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰øÉÈÄ≤‰∫ÜÊñáÂ≠óËΩâ SQL (Text2SQL) ÁöÑÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜÊúÄÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÊäÄË°ì‰ªçÂèóÈôêÊñºÂ∞ÅÈñâÂéüÂßãÁ¢º LLM (‰æãÂ¶Ç GPT-4) ÁöÑÊÉÖÂ¢ÉÂ≠∏Áøí‰∏≠ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÈñãÊîæÂ†¥ÊôØ‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂº∑ÂÅ•Â§ö‰ªªÂãôË™øÊï¥ÂíåÂçî‰ΩúÊñπÊ≥ï (ROUTE)Ôºå‰ª•ÊèêÈ´òÈñãÊîæÂéüÂßãÁ¢º LLM Â∞ç Text2SQL ÁöÑÁ∂úÂêàËÉΩÂäõÔºåÂæûËÄåÊèê‰æõÊõ¥ÂØ¶Áî®ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂæû‰ΩøÁî®Ëàá SQL ÁîüÊàêÁõ∏ÈóúÁöÑÂêÑÁ®ÆÂêàÊàêË®ìÁ∑¥Ë≥áÊñôÁöÑÂ§ö‰ªªÂãôÁõ£Áù£ÂæÆË™ø (SFT) ÈñãÂßã„ÄÇËàáÁèæÊúâÁöÑÂü∫Êñº SFT ÁöÑ Text2SQL ÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂπæÂÄãÈ°çÂ§ñÁöÑ SFT ‰ªªÂãôÔºåÂåÖÊã¨Ê®°ÂºèÈÄ£Áµê„ÄÅÈõúË®äÊ†°Ê≠£ÂíåÂª∂Á∫åÂØ´‰Ωú„ÄÇÂèÉËàáÂêÑÁ®Æ SQL ÁîüÊàê‰ªªÂãôÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÂ∞ç SQL Ë™ûÊ≥ïÁöÑÁêÜËß£Ôºå‰∏¶ÊèêÈ´ò‰∫ÜÂÆÉÁîüÊàêÈ´òÂìÅË≥™ SQL Êü•Ë©¢ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂú® LLM ‰ª£ÁêÜÁöÑÂçî‰ΩúÊ®°ÂºèÁöÑÂïüÁôº‰∏ãÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§ö‰ªªÂãôÂçî‰ΩúÊèêÁ§∫ (MCP) Á≠ñÁï•„ÄÇÊ≠§Á≠ñÁï•Âà©Áî®Ë∑®Â§öÂÄãËàá SQL Áõ∏Èóú‰ªªÂãôÁöÑÂçî‰Ωú‰æÜÊ∏õÂ∞ë SQL ÁîüÊàêÈÅéÁ®ã‰∏≠ÁöÑÂπªË¶∫ÔºåÂæûËÄåÈÄöÈÅéÊòéÁ¢∫ÁöÑÂ§ö‰ªªÂãôËÉΩÂäõÊúÄÂ§ßÈôêÂ∫¶Âú∞ÁôºÊèÆÂ¢ûÂº∑ Text2SQL ÊïàËÉΩÁöÑÊΩõÂäõ„ÄÇÂ∑≤Á∂ìÂ∞çÂÖ´ÂÄãÈñãÊîæÂéüÂßãÁ¢º LLM Âíå‰∫îÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÂíåÊ∑±ÂÖ•ÁöÑÂàÜÊûê„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊèêÊ°àÂÑ™ÊñºÊúÄÊñ∞ÁöÑ Text2SQL ÊñπÊ≥ïÔºå‰∏¶Áî¢Áîü‰∫ÜÈ†òÂÖàÁöÑÊïàËÉΩ„ÄÇ

##### **Can LLMs Convert Graphs to Text-Attributed Graphs?**
2412.10136v1 by Zehong Wang, Sidney Liu, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye

Graphs are ubiquitous data structures found in numerous real-world
applications, such as drug discovery, recommender systems, and social network
analysis. Graph neural networks (GNNs) have become a popular tool to learn node
embeddings through message passing on these structures. However, a significant
challenge arises when applying GNNs to multiple graphs with different feature
spaces, as existing GNN architectures are not designed for cross-graph feature
alignment. To address this, recent approaches introduce text-attributed graphs,
where each node is associated with a textual description, enabling the use of a
shared textual encoder to project nodes from different graphs into a unified
feature space. While promising, this method relies heavily on the availability
of text-attributed data, which can be difficult to obtain in practice. To
bridge this gap, we propose a novel method named Topology-Aware Node
description Synthesis (TANS), which leverages large language models (LLMs) to
automatically convert existing graphs into text-attributed graphs. The key idea
is to integrate topological information with each node's properties, enhancing
the LLMs' ability to explain how graph topology influences node semantics. We
evaluate our TANS on text-rich, text-limited, and text-free graphs,
demonstrating that it enables a single GNN to operate across diverse graphs.
Notably, on text-free graphs, our method significantly outperforms existing
approaches that manually design node features, showcasing the potential of LLMs
for preprocessing graph-structured data, even in the absence of textual
information. The code and data are available at
https://github.com/Zehong-Wang/TANS.

ÊëòË¶ÅÔºöÂúñÂΩ¢ÊòØÊôÆÈÅçÂ≠òÂú®ÊñºË®±Â§öÁúüÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÁöÑË≥áÊñôÁµêÊßãÔºå‰æãÂ¶ÇËó•Áâ©ÁôºÁèæ„ÄÅÊé®Ëñ¶Á≥ªÁµ±ÂíåÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûê„ÄÇÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊµÅË°åÁöÑÂ∑•ÂÖ∑ÔºåÂèØÈÄèÈÅéÂú®ÈÄô‰∫õÁµêÊßã‰∏äÂÇ≥ÈÅûË®äÊÅØ‰æÜÂ≠∏ÁøíÁØÄÈªûÂµåÂÖ•„ÄÇÁÑ∂ËÄåÔºåÁï∂Â∞á GNN ÊáâÁî®ÊñºÂÖ∑Êúâ‰∏çÂêåÁâπÂæµÁ©∫ÈñìÁöÑÂ§öÂÄãÂúñÂΩ¢ÊôÇÔºåÊúÉÂá∫Áèæ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫ÁèæÊúâÁöÑ GNN Êû∂Êßã‰∏¶ÈùûË®≠Ë®àÁî®ÊñºË∑®ÂúñÂΩ¢ÁâπÂæµÂ∞çÈΩä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊúÄËøëÁöÑÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊñáÂ≠óÂ±¨ÊÄßÂúñÂΩ¢ÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÈÉΩËàáÊñáÂ≠óÊèèËø∞Áõ∏ÈóúËÅØÔºåÂæûËÄåÂèØ‰ª•‰ΩøÁî®ÂÖ±Áî®ÊñáÂ≠óÁ∑®Á¢ºÂô®Â∞á‰æÜËá™‰∏çÂêåÂúñÂΩ¢ÁöÑÁØÄÈªûÊäïÂΩ±Âà∞Áµ±‰∏ÄÁöÑÁâπÂæµÁ©∫Èñì‰∏≠„ÄÇÂÑòÁÆ°ÊúâÂ∏åÊúõÔºå‰ΩÜÊ≠§ÊñπÊ≥ïÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÊñáÂ≠óÂ±¨ÊÄßË≥áÊñôÁöÑÂèØÁî®ÊÄßÔºåÈÄôÂú®ÂØ¶Âãô‰∏äÂèØËÉΩÈõ£‰ª•ÂèñÂæó„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫ÊãìÊí≤ÊÑüÁü•ÁØÄÈªûÊèèËø∞ÂêàÊàê (TANS) ÁöÑÊñ∞ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÁèæÊúâÂúñÂΩ¢Ëá™ÂãïËΩâÊèõÁÇ∫ÊñáÂ≠óÂ±¨ÊÄßÂúñÂΩ¢„ÄÇÂÖ∂ÈóúÈçµÊÄùÊÉ≥ÊòØÂ∞áÊãìÊí≤Ë≥áË®äËàáÊØèÂÄãÁØÄÈªûÁöÑÂ±¨ÊÄßÊï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÂ¢ûÂº∑ LLM Ëß£ÈáãÂúñÂΩ¢ÊãìÊí≤Â¶Ç‰ΩïÂΩ±ÈüøÁØÄÈªûË™ûÁæ©ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®ÊñáÂ≠óË±êÂØå„ÄÅÊñáÂ≠óÂèóÈôêÂíåÁÑ°ÊñáÂ≠óÂúñÂΩ¢‰∏äË©ï‰º∞ÊàëÂÄëÁöÑ TANSÔºåË≠âÊòéÂÆÉËÉΩËÆìÂñÆ‰∏Ä GNN Âú®‰∏çÂêåÁöÑÂúñÂΩ¢‰∏≠ÈÅã‰Ωú„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®ÁÑ°ÊñáÂ≠óÂúñÂΩ¢‰∏äÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÂÑ™ÊñºÊâãÂãïË®≠Ë®àÁØÄÈªûÁâπÂæµÁöÑÁèæÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫Ü LLM Âú®È†êËôïÁêÜÂúñÂΩ¢ÁµêÊßãË≥áÊñôÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂç≥‰ΩøÂú®Ê≤íÊúâÊñáÂ≠óË≥áË®äÁöÑÊÉÖÊ≥Å‰∏ã‰πüÊòØÂ¶ÇÊ≠§„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/Zehong-Wang/TANS ÂèñÂæó„ÄÇ

##### **ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers**
2412.10135v1 by Junyan Hu, Xue Xiao, Mengqi Zhang, Xiao Chen, Zhaochun Ren, Zhumin Chen, Pengjie Ren

As large language models (LLMs) grow in size, traditional full fine-tuning
becomes increasingly impractical due to its high computational and storage
costs. Although popular parameter-efficient fine-tuning methods, such as LoRA,
have significantly reduced the number of tunable parameters, there is still
room for further optimization. In this work, we propose ASLoRA, a cross-layer
parameter-sharing strategy combining global sharing with partial adaptive
sharing. Specifically, we share the low-rank matrix A across all layers and
adaptively merge matrix B during training. This sharing mechanism not only
mitigates overfitting effectively but also captures inter-layer dependencies,
significantly enhancing the model's representational capability. We conduct
extensive experiments on various NLP tasks, showing that ASLoRA outperforms
LoRA while using less than 25% of the parameters, highlighting its flexibility
and superior parameter efficiency. Furthermore, in-depth analyses of the
adaptive sharing strategy confirm its significant advantages in enhancing both
model flexibility and task adaptability.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë¶èÊ®°ÁöÑÊì¥Â§ßÔºåÁî±ÊñºÂÖ∂È´òÈÅãÁÆóÂíåÂÑ≤Â≠òÊàêÊú¨ÔºåÂÇ≥Áµ±ÁöÑÂÖ®ÂæÆË™øËÆäÂæóË∂ä‰æÜË∂ä‰∏çÂàáÂØ¶Èöõ„ÄÇÂÑòÁÆ°ÊµÅË°åÁöÑÂèÉÊï∏ÊúâÊïàÂæÆË™øÊñπÊ≥ïÔºå‰æãÂ¶Ç LoRAÔºåÂ∑≤È°ØËëóÊ∏õÂ∞ëÂèØË™øÂèÉÊï∏ÁöÑÊï∏ÈáèÔºå‰ΩÜ‰ªçÊúâÈÄ≤‰∏ÄÊ≠•ÊúÄ‰Ω≥ÂåñÁöÑÁ©∫Èñì„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ ASLoRAÔºå‰∏ÄÁ®ÆË∑®Â±§ÂèÉÊï∏ÂÖ±‰∫´Á≠ñÁï•ÔºåÁµêÂêà‰∫ÜÂÖ®Â±ÄÂÖ±‰∫´ÂíåÈÉ®ÂàÜËá™ÈÅ©ÊáâÂÖ±‰∫´„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂú®ÊâÄÊúâÂ±§‰∏≠ÂÖ±‰∫´‰ΩéÁß©Áü©Èô£ AÔºå‰∏¶Âú®Ë®ìÁ∑¥ÊúüÈñìËá™ÈÅ©ÊáâÂú∞Âêà‰ΩµÁü©Èô£ B„ÄÇÈÄôÁ®ÆÂÖ±‰∫´Ê©üÂà∂‰∏çÂÉÖÊúâÊïàÂú∞Ê∏õËºï‰∫ÜÈÅéÂ∫¶Êì¨ÂêàÔºåËÄå‰∏îÈÇÑÊçïÁç≤‰∫ÜÂ±§Èñì‰æùË≥¥ÊÄßÔºåÈ°ØËëóÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÁöÑË°®Á§∫ËÉΩÂäõ„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®Æ NLP ‰ªªÂãôÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåË°®Êòé ASLoRA Âú®‰ΩøÁî®‰∏çÂà∞ 25% ÁöÑÂèÉÊï∏ÊôÇÂÑ™Êñº LoRAÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂ÈùàÊ¥ªÊÄßËàáÂçìË∂äÁöÑÂèÉÊï∏ÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÂ∞çËá™ÈÅ©ÊáâÂÖ±‰∫´Á≠ñÁï•ÁöÑÊ∑±ÂÖ•ÂàÜÊûêË≠âÂØ¶‰∫ÜÂÖ∂Âú®Â¢ûÂº∑Ê®°ÂûãÈùàÊ¥ªÊÄßËàá‰ªªÂãôÈÅ©ÊáâÊÄßÊñπÈù¢ÁöÑÈ°ØËëóÂÑ™Âã¢„ÄÇ

##### **You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**
2412.10133v1 by Islem Bouzenia, Michael Pradel

The ability to execute the test suite of a project is essential in many
scenarios, e.g., to assess code quality and code coverage, to validate code
changes made by developers or automated tools, and to ensure compatibility with
dependencies. Despite its importance, executing the test suite of a project can
be challenging in practice because different projects use different programming
languages, software ecosystems, build systems, testing frameworks, and other
tools. These challenges make it difficult to create a reliable, universal test
execution method that works across different projects. This paper presents
ExecutionAgent, an automated technique that installs arbitrary projects,
configures them to run test cases, and produces project-specific scripts to
reproduce the setup. Inspired by the way a human developer would address this
task, our approach is a large language model-based agent that autonomously
executes commands and interacts with the host system. The agent uses
meta-prompting to gather guidelines on the latest technologies related to the
given project, and it iteratively refines its process based on feedback from
the previous steps. Our evaluation applies ExecutionAgent to 50 open-source
projects that use 14 different programming languages and many different build
and testing tools. The approach successfully executes the test suites of 33/55
projects, while matching the test results of ground truth test suite executions
with a deviation of only 7.5\%. These results improve over the best previously
available technique by 6.6x. The costs imposed by the approach are reasonable,
with an execution time of 74 minutes and LLM costs of 0.16 dollars, on average
per project. We envision ExecutionAgent to serve as a valuable tool for
developers, automated programming tools, and researchers that need to execute
tests across a wide variety of projects.

ÊëòË¶ÅÔºöÂú®Ë®±Â§öÂ†¥ÊôØ‰∏≠ÔºåÂü∑Ë°åÂ∞àÊ°àÁöÑÊ∏¨Ë©¶Â•ó‰ª∂ÁöÑËÉΩÂäõËá≥ÈóúÈáçË¶ÅÔºå‰æãÂ¶ÇË©ï‰º∞Á®ãÂºèÁ¢ºÂìÅË≥™ÂíåÁ®ãÂºèÁ¢ºÊ∂µËìãÁéá„ÄÅÈ©óË≠âÈñãÁôº‰∫∫Âì°ÊàñËá™ÂãïÂåñÂ∑•ÂÖ∑ÊâÄÂÅöÁöÑÁ®ãÂºèÁ¢ºËÆäÊõ¥Ôºå‰ª•ÂèäÁ¢∫‰øùËàá‰æùË≥¥È†ÖÁõ∏ÂÆπ„ÄÇÂÑòÁÆ°ÂÖ∂ÈáçË¶ÅÊÄßÔºåÂú®ÂØ¶Âãô‰∏äÂü∑Ë°åÂ∞àÊ°àÁöÑÊ∏¨Ë©¶Â•ó‰ª∂ÂèØËÉΩÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫‰∏çÂêåÁöÑÂ∞àÊ°à‰ΩøÁî®‰∏çÂêåÁöÑÁ®ãÂºèË™ûË®Ä„ÄÅËªüÈ´îÁîüÊÖãÁ≥ªÁµ±„ÄÅÂª∫ÁΩÆÁ≥ªÁµ±„ÄÅÊ∏¨Ë©¶Êû∂ÊßãÂíåÂÖ∂‰ªñÂ∑•ÂÖ∑„ÄÇÈÄô‰∫õÊåëÊà∞‰ΩøÂæóÈõ£‰ª•Âª∫Á´ã‰∏ÄÂÄãÂèØÈù†ÁöÑÈÄöÁî®Ê∏¨Ë©¶Âü∑Ë°åÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂêåÁöÑÂ∞àÊ°à‰∏≠ÈÅã‰Ωú„ÄÇÊú¨ÊñáÊèêÂá∫ ExecutionAgentÔºå‰∏ÄÁ®ÆËá™ÂãïÂåñÊäÄË°ìÔºåÂÆÉÊúÉÂÆâË£ù‰ªªÊÑèÂ∞àÊ°à„ÄÅÂ∞áÂÖ∂ÁµÑÊÖãÁÇ∫Âü∑Ë°åÊ∏¨Ë©¶Ê°à‰æãÔºå‰∏¶Áî¢ÁîüÂ∞àÊ°àÁâπÂÆöÁöÑËÖ≥Êú¨‰ª•ÈáçÁèæË®≠ÂÆö„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂèóÂà∞‰∫∫È°ûÈñãÁôº‰∫∫Âì°ËôïÁêÜÊ≠§‰ªªÂãôÁöÑÊñπÂºèÂïüÁôºÔºåÊòØ‰∏ÄÁ®ÆÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂü∫Á§éÁöÑ‰ª£ÁêÜÔºåÂÆÉÊúÉËá™ÂãïÂü∑Ë°åÊåá‰ª§‰∏¶Ëàá‰∏ªÊ©üÁ≥ªÁµ±‰∫íÂãï„ÄÇ‰ª£ÁêÜ‰ΩøÁî®ÂÖÉÊèêÁ§∫‰æÜÊî∂ÈõÜÊúâÈóúÁµ¶ÂÆöÂ∞àÊ°àÁöÑÊúÄÊñ∞ÊäÄË°ìÁöÑÊåáÂçóÔºå‰∏¶‰∏îÊ†πÊìöÂâç‰∏ÄÈöéÊÆµÁöÑÂõûÈ•ãÔºåÂèçË¶ÜÊîπÂñÑÂÖ∂ÊµÅÁ®ã„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Â∞á ExecutionAgent Â•óÁî®Âà∞ 50 ÂÄã‰ΩøÁî® 14 Á®Æ‰∏çÂêåÁ®ãÂºèË™ûË®ÄÂíåË®±Â§ö‰∏çÂêåÂª∫ÁΩÆÂíåÊ∏¨Ë©¶Â∑•ÂÖ∑ÁöÑÈñãÊ∫êÂ∞àÊ°à„ÄÇÊ≠§ÂÅöÊ≥ïÊàêÂäüÂü∑Ë°å 33/55 ÂÄãÂ∞àÊ°àÁöÑÊ∏¨Ë©¶Â•ó‰ª∂ÔºåÂêåÊôÇÂÉÖÊúâ 7.5% ÁöÑÂÅèÂ∑ÆÔºåËàáÂü∫Êú¨ÂØ¶Ê≥ÅÊ∏¨Ë©¶Â•ó‰ª∂Âü∑Ë°åÁµêÊûúÁõ∏Á¨¶„ÄÇÈÄô‰∫õÁµêÊûúÊØîÂÖàÂâçÂèØÁî®ÁöÑÊúÄ‰Ω≥ÊäÄË°ìÈÄ≤Ê≠•‰∫Ü 6.6 ÂÄç„ÄÇÊ≠§ÂÅöÊ≥ïÁî¢ÁîüÁöÑÊàêÊú¨ÂêàÁêÜÔºåÂπ≥ÂùáÊØèÂÄãÂ∞àÊ°àÁöÑÂü∑Ë°åÊôÇÈñìÁÇ∫ 74 ÂàÜÈêòÔºåLLM ÊàêÊú¨ÁÇ∫ 0.16 ÁæéÂÖÉ„ÄÇÊàëÂÄëÈ†êÊÉ≥ ExecutionAgent ÂèØ‰ΩúÁÇ∫ÈñãÁôº‰∫∫Âì°„ÄÅËá™ÂãïÂåñÁ®ãÂºèË®≠Ë®àÂ∑•ÂÖ∑ÂíåÈúÄË¶ÅÂü∑Ë°åÂêÑÁ®ÆÂ∞àÊ°àÊ∏¨Ë©¶ÁöÑÁ†îÁ©∂‰∫∫Âì°ÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑„ÄÇ

##### **Familiarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data**
2412.10121v1 by Jonas Golde, Patrick Haller, Max Ploner, Fabio Barth, Nicolaas Jedema, Alan Akbik

Zero-shot named entity recognition (NER) is the task of detecting named
entities of specific types (such as 'Person' or 'Medicine') without any
training examples. Current research increasingly relies on large synthetic
datasets, automatically generated to cover tens of thousands of distinct entity
types, to train zero-shot NER models. However, in this paper, we find that
these synthetic datasets often contain entity types that are semantically
highly similar to (or even the same as) those in standard evaluation
benchmarks. Because of this overlap, we argue that reported F1 scores for
zero-shot NER overestimate the true capabilities of these approaches. Further,
we argue that current evaluation setups provide an incomplete picture of
zero-shot abilities since they do not quantify the label shift (i.e., the
similarity of labels) between training and evaluation datasets. To address
these issues, we propose Familiarity, a novel metric that captures both the
semantic similarity between entity types in training and evaluation, as well as
their frequency in the training data, to provide an estimate of label shift. It
allows researchers to contextualize reported zero-shot NER scores when using
custom synthetic training datasets. Further, it enables researchers to generate
evaluation setups of various transfer difficulties for fine-grained analysis of
zero-shot NER.

ÊëòË¶ÅÔºöÈõ∂Ê¨°Â≠∏ÁøíÂëΩÂêçÂØ¶È´îËæ®Ë≠ò (NER) ÊòØ‰∏ÄÈ†Ö‰ªªÂãôÔºåÂú®Ê≤íÊúâ‰ªª‰ΩïË®ìÁ∑¥ÁØÑ‰æãÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂÅµÊ∏¨ÁâπÂÆöÈ°ûÂûãÁöÑÂëΩÂêçÂØ¶È´îÔºà‰æãÂ¶Ç„Äå‰∫∫Áâ©„ÄçÊàñ„ÄåËó•Áâ©„ÄçÔºâ„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄË∂ä‰æÜË∂ä‰æùË≥¥Â§ßÂûãÂêàÊàêË≥áÊñôÈõÜÔºåËá™ÂãïÁî¢ÁîüÊ∂µËìãÊï∏Ëê¨ÂÄã‰∏çÂêåÂØ¶È´îÈ°ûÂûãÁöÑË≥áÊñôÔºå‰æÜË®ìÁ∑¥Èõ∂Ê¨°Â≠∏Áøí NER Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁôºÁèæÈÄô‰∫õÂêàÊàêË≥áÊñôÈõÜÈÄöÂ∏∏ÂåÖÂê´Ë™ûÁæ©‰∏äËàáÊ®ôÊ∫ñË©ïÈáèÂü∫Ê∫ñ‰∏≠ÁöÑÂØ¶È´îÈ°ûÂûãÈ´òÂ∫¶Áõ∏‰ººÔºàÁîöËá≥Áõ∏ÂêåÔºâÁöÑÂØ¶È´îÈ°ûÂûã„ÄÇÁî±ÊñºÈÄôÁ®ÆÈáçÁñäÔºåÊàëÂÄëË™çÁÇ∫Èõ∂Ê¨°Â≠∏Áøí NER ÊâÄÂ†±ÂëäÁöÑ F1 ÂàÜÊï∏È´ò‰º∞‰∫ÜÈÄô‰∫õÊñπÊ≥ïÁöÑÁúüÂØ¶ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË™çÁÇ∫ÁõÆÂâçÁöÑË©ïÈáèË®≠ÂÆöÊèê‰æõ‰∫Ü‰∏çÂÆåÊï¥ÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊ≤íÊúâÈáèÂåñË®ìÁ∑¥ÂíåË©ïÈáèË≥áÊñôÈõÜ‰πãÈñìÁöÑÊ®ôÁ±§ËΩâÁßªÔºàÂç≥Ê®ôÁ±§ÁöÑÁõ∏‰ººÊÄßÔºâ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁÜüÊÇâÂ∫¶ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑÊåáÊ®ôÔºåÂÆÉÂêåÊôÇÊì∑Âèñ‰∫ÜË®ìÁ∑¥ÂíåË©ïÈáè‰∏≠ÂØ¶È´îÈ°ûÂûã‰πãÈñìÁöÑË™ûÁæ©Áõ∏‰ººÊÄßÔºå‰ª•ÂèäÂÆÉÂÄëÂú®Ë®ìÁ∑¥Ë≥áÊñô‰∏≠ÁöÑÈ†ªÁéáÔºå‰ª•Êèê‰æõÊ®ôÁ±§ËΩâÁßªÁöÑ‰º∞Ë®àÂÄº„ÄÇÂÆÉÂÖÅË®±Á†îÁ©∂‰∫∫Âì°Âú®‰ΩøÁî®Ëá™Ë®ÇÂêàÊàêË®ìÁ∑¥Ë≥áÊñôÈõÜÊôÇÔºåÂ∞áÊâÄÂ†±ÂëäÁöÑÈõ∂Ê¨°Â≠∏Áøí NER ÂàÜÊï∏ËÑàÁµ°Âåñ„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ΩøÁ†îÁ©∂‰∫∫Âì°ËÉΩÂ§†Áî¢ÁîüÂêÑÁ®ÆËΩâÁßªÈõ£Â∫¶ÁöÑË©ïÈáèË®≠ÂÆöÔºå‰ª•ÈÄ≤Ë°åÈõ∂Ê¨°Â≠∏Áøí NER ÁöÑÁ¥∞Á≤íÂ∫¶ÂàÜÊûê„ÄÇ

##### **CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models**
2412.10117v1 by Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou

In our previous work, we introduced CosyVoice, a multilingual speech
synthesis model based on supervised discrete speech tokens. By employing
progressive semantic decoding with two popular generative models, language
models (LMs) and Flow Matching, CosyVoice demonstrated high prosody
naturalness, content consistency, and speaker similarity in speech in-context
learning. Recently, significant progress has been made in multi-modal large
language models (LLMs), where the response latency and real-time factor of
speech synthesis play a crucial role in the interactive experience. Therefore,
in this report, we present an improved streaming speech synthesis model,
CosyVoice 2, which incorporates comprehensive and systematic optimizations.
Specifically, we introduce finite-scalar quantization to improve the codebook
utilization of speech tokens. For the text-speech LM, we streamline the model
architecture to allow direct use of a pre-trained LLM as the backbone. In
addition, we develop a chunk-aware causal flow matching model to support
various synthesis scenarios, enabling both streaming and non-streaming
synthesis within a single model. By training on a large-scale multilingual
dataset, CosyVoice 2 achieves human-parity naturalness, minimal response
latency, and virtually lossless synthesis quality in the streaming mode. We
invite readers to listen to the demos at
https://funaudiollm.github.io/cosyvoice2.

ÊëòË¶ÅÔºöÂú®ÊàëÂÄë‰πãÂâçÁöÑÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CosyVoiceÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫ÊñºÁõ£Áù£Èõ¢Êï£Ë™ûÈü≥Á¨¶ËôüÁöÑÂ§öË™ûË®ÄË™ûÈü≥ÂêàÊàêÊ®°Âûã„ÄÇÈÄöÈÅé‰ΩøÁî®ÂÖ©ÂÄãÊµÅË°åÁöÑÁîüÊàêÊ®°ÂûãÔºàË™ûË®ÄÊ®°Âûã (LM) ÂíåÊµÅÂåπÈÖçÔºâ‰æÜÊé°Áî®Êº∏ÈÄ≤Ë™ûÁæ©Ëß£Á¢ºÔºåCosyVoice Â±ïÁ§∫‰∫ÜË™ûÂ¢ÉÂ≠∏Áøí‰∏≠Ë™ûË™øÁöÑËá™ÁÑ∂ÊÄß„ÄÅÂÖßÂÆπÁöÑ‰∏ÄËá¥ÊÄßÂíåË™™Ë©±ËÄÖÁöÑÁõ∏‰ººÊÄß„ÄÇÊúÄËøëÔºåÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèñÂæó‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºåÂÖ∂‰∏≠Ë™ûÈü≥ÂêàÊàêÁöÑÈüøÊáâÂª∂ÈÅ≤ÂíåÂØ¶ÊôÇÂõ†Á¥†Âú®‰∫íÂãïÈ´îÈ©ó‰∏≠ÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊîπÈÄ≤ÁöÑ‰∏≤ÊµÅË™ûÈü≥ÂêàÊàêÊ®°Âûã CosyVoice 2ÔºåÂÆÉÂåÖÂê´‰∫ÜÂÖ®Èù¢ÂíåÁ≥ªÁµ±ÊÄßÁöÑÊúÄ‰Ω≥Âåñ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊúâÈôêÊ®ôÈáèÈáèÂåñ‰æÜÊîπÂñÑË™ûÈü≥Á¨¶ËôüÁöÑÁ¢ºÊú¨Âà©Áî®Áéá„ÄÇÂ∞çÊñºÊñáÂ≠óË™ûÈü≥ LMÔºåÊàëÂÄëÁ∞°Âåñ‰∫ÜÊ®°ÂûãÊû∂ÊßãÔºåÂÖÅË®±Áõ¥Êé•‰ΩøÁî®È†êË®ìÁ∑¥ÁöÑ LLM ‰ΩúÁÇ∫‰∏ªÂππ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂàÜÂ°äÊÑüÁü•Âõ†ÊûúÊµÅÂåπÈÖçÊ®°Âûã‰æÜÊîØÊè¥ÂêÑÁ®ÆÂêàÊàêÂ†¥ÊôØÔºåÂú®ÂñÆ‰∏ÄÊ®°Âûã‰∏≠ÂØ¶Áèæ‰∏≤ÊµÅÂíåÈùû‰∏≤ÊµÅÂêàÊàê„ÄÇÈÄöÈÅéÂú®‰∏ÄÂÄãÂ§ßË¶èÊ®°Â§öË™ûË®ÄË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÔºåCosyVoice 2 Âú®‰∏≤ÊµÅÊ®°Âºè‰∏ãÂØ¶Áèæ‰∫Ü‰∫∫È°ûÂêåÁ≠âÁöÑËá™ÁÑ∂ÊÄß„ÄÅÊúÄÂ∞èÁöÑÈüøÊáâÂª∂ÈÅ≤ÂíåÂπæ‰πéÁÑ°ÊêçÁöÑÂêàÊàêÂìÅË≥™„ÄÇÊàëÂÄëÈÇÄË´ãËÆÄËÄÖÂú® https://funaudiollm.github.io/cosyvoice2 ‰∏äÊî∂ËÅΩÁ§∫ÁØÑ„ÄÇ

##### **Label-template based Few-Shot Text Classification with Contrastive Learning**
2412.10110v1 by Guanghua Hou, Shuhui Cao, Deqiang Ouyang, Ning Wang

As an algorithmic framework for learning to learn, meta-learning provides a
promising solution for few-shot text classification. However, most existing
research fail to give enough attention to class labels. Traditional basic
framework building meta-learner based on prototype networks heavily relies on
inter-class variance, and it is easily influenced by noise. To address these
limitations, we proposes a simple and effective few-shot text classification
framework. In particular, the corresponding label templates are embed into
input sentences to fully utilize the potential value of class labels, guiding
the pre-trained model to generate more discriminative text representations
through the semantic information conveyed by labels. With the continuous
influence of label semantics, supervised contrastive learning is utilized to
model the interaction information between support samples and query samples.
Furthermore, the averaging mechanism is replaced with an attention mechanism to
highlight vital semantic information. To verify the proposed scheme, four
typical datasets are employed to assess the performance of different methods.
Experimental results demonstrate that our method achieves substantial
performance enhancements and outperforms existing state-of-the-art models on
few-shot text classification tasks.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫‰∏ÄÁ®ÆÁî®ÊñºÂ≠∏ÁøíÂ≠∏ÁøíÁöÑÊºîÁÆóÊ≥ïÊû∂ÊßãÔºåÂÖÉÂ≠∏ÁøíÁÇ∫Â∞ëÁôºÊñáÂ≠óÂàÜÈ°ûÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂ§ßÂ§öÊï∏Á†îÁ©∂Êú™ËÉΩÁµ¶‰∫àÈ°ûÂà•Ê®ôÁ±§Ë∂≥Â§†ÁöÑÈóúÊ≥®„ÄÇÂÇ≥Áµ±ÁöÑÂü∫Êú¨Êû∂ÊßãÂª∫Á´ãÂü∫ÊñºÂéüÂûãÁ∂≤Ë∑ØÁöÑÂÖÉÂ≠∏ÁøíËÄÖÔºåÂö¥Èáç‰æùË≥¥ÊñºÈ°ûÈñìÂ∑ÆÁï∞Ôºå‰∏¶‰∏îÂÆπÊòìÂèóÂà∞ÈõúË®äÁöÑÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆËÄåÊúâÊïàÁöÑÂ∞ëÁôºÊñáÂ≠óÂàÜÈ°ûÊû∂Êßã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂ∞áÂ∞çÊáâÁöÑÊ®ôÁ±§ÁØÑÊú¨ÂµåÂÖ•Ëº∏ÂÖ•Âè•Â≠ê‰∏≠Ôºå‰ª•ÂÖÖÂàÜÂà©Áî®È°ûÂà•Ê®ôÁ±§ÁöÑÊΩõÂú®ÂÉπÂÄºÔºåÊåáÂ∞éÈ†êË®ìÁ∑¥Ê®°ÂûãÈÄöÈÅéÊ®ôÁ±§ÂÇ≥ÈÅîÁöÑË™ûÁæ©Ë≥áË®äÁîüÊàêÊõ¥ÂÖ∑ÂçÄÂà•ÊÄßÁöÑÊñáÂ≠óË°®Á§∫„ÄÇÂú®Ê®ôÁ±§Ë™ûÁæ©ÁöÑÊåÅÁ∫åÂΩ±Èüø‰∏ãÔºåÂà©Áî®Áõ£Áù£Â∞çÊØîÂ≠∏ÁøíÂ∞çÊîØÊè¥Ê®£Êú¨ÂíåÊü•Ë©¢Ê®£Êú¨‰πãÈñìÁöÑ‰∫íÂãïË≥áË®äÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊ≠§Â§ñÔºåÂ∞áÂπ≥ÂùáÊ©üÂà∂ÊõøÊèõÁÇ∫Ê≥®ÊÑèÂäõÊ©üÂà∂Ôºå‰ª•Á™ÅÂá∫ÈáçË¶ÅÁöÑË™ûÁæ©Ë≥áË®ä„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÊâÄÊèêÂá∫ÁöÑÊñπÊ°àÔºåÊé°Áî®‰∫ÜÂõõÂÄãÂÖ∏ÂûãÁöÑË≥áÊñôÈõÜ‰æÜË©ï‰º∞‰∏çÂêåÊñπÊ≥ïÁöÑÊïàËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â∞ëÁôºÊñáÂ≠óÂàÜÈ°û‰ªªÂãô‰∏≠ÂØ¶Áèæ‰∫ÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºå‰∏¶‰∏îÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Ê®°Âûã„ÄÇ

##### **NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models**
2412.10107v1 by Asmaa Abdallah, Abdullatif Albaseer, Abdulkadir Celik, Mohamed Abdallah, Ahmed M. Eltawil

The transition to 6G networks promises unprecedented advancements in wireless
communication, with increased data rates, ultra-low latency, and enhanced
capacity. However, the complexity of managing and optimizing these
next-generation networks presents significant challenges. The advent of large
language models (LLMs) has revolutionized various domains by leveraging their
sophisticated natural language understanding capabilities. However, the
practical application of LLMs in wireless network orchestration and management
remains largely unexplored. Existing literature predominantly offers visionary
perspectives without concrete implementations, leaving a significant gap in the
field. To address this gap, this paper presents NETORCHLLM, a wireless NETwork
ORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse
wireless-specific models from wireless communication communities using their
language understanding and generation capabilities. A comprehensive framework
is introduced, demonstrating the practical viability of our approach and
showcasing how LLMs can be effectively harnessed to optimize dense network
operations, manage dynamic environments, and improve overall network
performance. NETORCHLLM bridges the theoretical aspirations of prior research
with practical, actionable solutions, paving the way for future advancements in
integrating generative AI technologies within the wireless communications
sector.

ÊëòË¶ÅÔºö6G Á∂≤Ë∑ØËΩâÂûãÊâøË´æÁÑ°Á∑öÈÄöË®äÂâçÊâÄÊú™ÊúâÁöÑÈÄ≤Â±ïÔºåÂåÖÊã¨Êõ¥È´òÁöÑË≥áÊñôÈÄüÁéá„ÄÅÊ•µ‰ΩéÁöÑÂª∂ÈÅ≤ÂíåÂ¢ûÂº∑ÁöÑÂÆπÈáè„ÄÇÁÑ∂ËÄåÔºåÁÆ°ÁêÜÂíåÊúÄ‰Ω≥ÂåñÈÄô‰∫õÊ¨°‰∏ñ‰ª£Á∂≤Ë∑ØÁöÑË§áÈõúÊÄßÔºåÂ∏∂‰æÜ‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÈÄèÈÅéÈÅãÁî®ÂÖ∂Á≤æÂØÜËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÂêÑÁ®ÆÈ†òÂüü„ÄÇÁÑ∂ËÄåÔºåLLM Âú®ÁÑ°Á∑öÁ∂≤Ë∑ØÂçîË™øÂíåÁÆ°ÁêÜ‰∏≠ÁöÑÂØ¶ÈöõÊáâÁî®Ôºå‰ªçÁÑ∂ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Á∂ìÊé¢Á¥¢„ÄÇÁèæÊúâÁöÑÊñáÁçª‰∏ªË¶ÅÊèê‰æõÊúâÈÅ†Ë¶ãÁöÑËßÄÈªûÔºåËÄåÊ≤íÊúâÂÖ∑È´îÁöÑÂØ¶‰ΩúÔºåÂú®Ë©≤È†òÂüüÁïô‰∏ã‰∫ÜÈáçÂ§ßÁöÑÈ¥ªÊ∫ù„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈ¥ªÊ∫ùÔºåÊú¨ÊñáÊèêÂá∫ NETORCHLLMÔºå‰∏ÄÂÄãÁÑ°Á∑öÁ∂≤Ë∑ØÂçîË™øÂô® LLM Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî® LLM ÂæûÁÑ°Á∑öÈÄöË®äÁ§æÁæ§‰∏≠ÁÑ°Á∏´Âú∞ÂçîË™øÂêÑÁ®ÆÁâπÂÆöÊñºÁÑ°Á∑öÁöÑÊ®°ÂûãÔºå‰ΩøÁî®ÂÆÉÂÄëÁöÑË™ûË®ÄÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ„ÄÇÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊû∂ÊßãÔºåÂ±ïÁ§∫ÊàëÂÄëÊñπÊ≥ïÁöÑÂØ¶ÈöõÂèØË°åÊÄßÔºå‰∏¶Â±ïÁ§∫Â¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî® LLM ‰æÜÊúÄ‰Ω≥ÂåñÂØÜÈõÜÁ∂≤Ë∑ØÊìç‰Ωú„ÄÅÁÆ°ÁêÜÂãïÊÖãÁí∞Â¢ÉÔºå‰ª•ÂèäÊîπÂñÑÊï¥È´îÁ∂≤Ë∑ØÊïàËÉΩ„ÄÇNETORCHLLM Â∞áÂÖàÂâçÁ†îÁ©∂ÁöÑÁêÜË´ñÈ°òÊôØËàáÂØ¶Áî®‰∏îÂèØË°åÁöÑËß£Ê±∫ÊñπÊ°àÈÄ£ÁµêËµ∑‰æÜÔºåÁÇ∫Êú™‰æÜÂú®ÁÑ°Á∑öÈÄöË®äÈ†òÂüüÊï¥ÂêàÁîüÊàêÂºè AI ÊäÄË°ìÁöÑÈÄ≤Â±ïÈã™Ë∑Ø„ÄÇ

##### **A Cascaded Dilated Convolution Approach for Mpox Lesion Classification**
2412.10106v1 by Ayush Deshmukh

The global outbreak of Mpox virus, classified as a Public Health Emergency of
International Concern by WHO, presents significant diagnostic challenges due to
its visual similarity to other skin lesion diseases. Current clinical detection
techniques face limitations in accuracy and efficiency, necessitating improved
automated diagnostic solutions. This study introduces a novel Cascaded Atrous
Group Attention (CAGA) module, specifically designed to enhance multi-scale
feature representation while optimizing computational efficiency. By
integrating CAGA with EfficientViT-L1 as the backbone architecture, our
approach achieves state-of-the-art performance with a score of 0.98% on the
MCSI dataset, while reducing model parameters by 37.5% compared to the original
EfficientViT-L1. This reduction in computational complexity maintains
diagnostic accuracy while enabling broader deployment across
resource-constrained healthcare settings. Extensive validation across two other
benchmark datasets, including MSID and MSLD, demonstrate the model's
robustness, consistently outperforming existing approaches. Our findings
suggest that CAGA's efficient feature extraction mechanism could be adapted for
other medical imaging tasks requiring fine-grained visual discrimination.

ÊëòË¶ÅÔºöÁî±Êñº‰∏ñÁïåË°õÁîüÁµÑÁπîÂ∞áÁå¥ÁóòÁóÖÊØíÂÖ®ÁêÉÁàÜÁôºÂÆöÁÇ∫ÂúãÈöõÈóúÊ≥®ÁöÑÂÖ¨ÂÖ±Ë°õÁîüÁ∑äÊÄ•‰∫ã‰ª∂ÔºåÂõ†Ê≠§Áå¥ÁóòÁóÖÊØíËàáÂÖ∂‰ªñÁöÆËÜöÁóÖËÆäÁñæÁóÖÂú®Ë¶ñË¶∫‰∏äÁöÑÁõ∏‰ººÊÄßÔºåÂ∞çË®∫Êñ∑Â∏∂‰æÜÈáçÂ§ßÊåëÊà∞„ÄÇÁõÆÂâçÁöÑËá®Â∫äÊ™¢Ê∏¨ÊäÄË°ìÂú®Ê∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÊñπÈù¢Èù¢Ëá®ÈôêÂà∂ÔºåÂõ†Ê≠§ÈúÄË¶ÅÊîπÈÄ≤ÁöÑËá™ÂãïÂåñË®∫Êñ∑Ëß£Ê±∫ÊñπÊ°à„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑ‰∏≤ËÅØÁ©∫Ê¥ûÁµÑÊ≥®ÊÑèÂäõ (CAGA) Ê®°ÁµÑÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÂ¢ûÂº∑Â§öÂ∞∫Â∫¶ÁâπÂæµË°®Á§∫ÔºåÂêåÊôÇÊúÄ‰Ω≥ÂåñÈÅãÁÆóÊïàÁéá„ÄÇÈÄèÈÅéÂ∞á CAGA Ëàá EfficientViT-L1 Êï¥Âêà‰ΩúÁÇ∫‰∏ªÂππÊû∂ÊßãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® MCSI Ë≥áÊñôÈõÜ‰∏ä‰ª• 0.98% ÁöÑÂàÜÊï∏ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇËàáÂéüÂßã EfficientViT-L1 Áõ∏ÊØîÔºåÊ®°ÂûãÂèÉÊï∏Ê∏õÂ∞ë‰∫Ü 37.5%„ÄÇÈÄôÁ®ÆÈÅãÁÆóË§áÈõúÂ∫¶ÁöÑÈôç‰ΩéÁ∂≠ÊåÅ‰∫ÜË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÔºåÂêåÊôÇÊîØÊè¥Âú®Ë≥áÊ∫êÂèóÈôêÁöÑÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠Êõ¥Âª£Ê≥õÂú∞ÈÉ®ÁΩ≤„ÄÇÂú®ÂåÖÊã¨ MSID Âíå MSLD Âú®ÂÖßÁöÑÂè¶Â§ñÂÖ©ÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÈ©óË≠âË≠âÊòé‰∫ÜÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÔºåÂßãÁµÇÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåCAGA ÁöÑÈ´òÊïàÁâπÂæµÊèêÂèñÊ©üÂà∂ÂèØ‰ª•Ë™øÊï¥ÁÇ∫ÂÖ∂‰ªñÈúÄË¶ÅÁ¥∞Á∑ªË¶ñË¶∫Ëæ®Âà•ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉè‰ªªÂãô„ÄÇ

##### **MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset**
2412.10105v1 by Sagi Shaier, George Arthur Baker, Chiranthan Sridhar, Lawrence E Hunter, Katharina von der Wense

Language models (LMs) have excelled in various broad domains. However, to
ensure their safe and effective integration into real-world educational
settings, they must demonstrate proficiency in specific, granular areas of
knowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs'
knowledge, have three major limitations. They: 1) do not cover the educational
domain; 2) typically focus on low-complexity, generic knowledge or broad
domains, which do not adequately assess the models' knowledge in specific
subjects; and 3) often rely on templates that can bias model predictions. Here,
we introduce MALAMUTE, a multilingual, template-free, and highly granular
probing dataset comprising expert-written, peer-reviewed probes from 71
university-level textbooks across three languages (English, Spanish, and
Polish). MALAMUTE is the first education-based cloze-style dataset. It covers
eight domains, each with up to 14 subdomains, further broken down into concepts
and concept-based prompts, totaling 33,361 university curriculum concepts and
116,887 prompts. MALAMUTE's fine granularity, educational focus, and inclusion
of both sentence-level and paragraph-level prompts make it an ideal tool for
evaluating LMs' course-related knowledge. Our evaluation of masked and causal
LMs on MALAMUTE shows that despite overall proficiency, they have significant
gaps in knowledge when examined closely on specific subjects, hindering their
safe use in classrooms and underscoring the need for further development.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°Âûã (LM) Âú®ÂêÑÁ®ÆÂª£Ê≥õÁöÑÈ†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÁÇ∫‰∫ÜÁ¢∫‰øùÂÆÉÂÄëÂÆâÂÖ®ÊúâÊïàÂú∞Êï¥ÂêàÂà∞ÁèæÂØ¶‰∏ñÁïåÁöÑÊïôËÇ≤Áí∞Â¢É‰∏≠ÔºåÂÆÉÂÄëÂøÖÈ†àÂú®ÁâπÂÆö„ÄÅÁ¥∞Á∑ªÁöÑÁü•Ë≠òÈ†òÂüü‰∏≠Â±ïÁèæÂá∫ÁÜüÁ∑¥Â∫¶„ÄÇÁèæÊúâÁöÑÂÆåÂΩ¢Â°´Á©∫Âü∫Ê∫ñÔºåÈÄöÂ∏∏Áî®ÊñºË©ï‰º∞ LM ÁöÑÁü•Ë≠òÔºåÊúâ‰∏âÂÄã‰∏ªË¶ÅÁöÑÈôêÂà∂„ÄÇÂÆÉÂÄëÔºö1) Ê≤íÊúâÊ∂µËìãÊïôËÇ≤È†òÂüüÔºõ2) ÈÄöÂ∏∏Â∞àÊ≥®Êñº‰ΩéË§áÈõúÊÄß„ÄÅÈÄöÁî®Áü•Ë≠òÊàñÂª£Ê≥õÈ†òÂüüÔºåÁÑ°Ê≥ïÂÖÖÂàÜË©ï‰º∞Ê®°ÂûãÂú®ÁâπÂÆö‰∏ªÈ°å‰∏≠ÁöÑÁü•Ë≠òÔºõ3) Á∂ìÂ∏∏‰æùË≥¥ÂèØËÉΩÂÅèÂêëÊ®°ÂûãÈ†êÊ∏¨ÁöÑÁØÑÊú¨„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π MALAMUTEÔºå‰∏ÄÂÄãÂ§öË™ûË®Ä„ÄÅÁÑ°ÁØÑÊú¨‰∏îÈ´òÂ∫¶Á¥∞Á∑ªÁöÑÊé¢Ê∏¨Ë≥áÊñôÈõÜÔºåÂåÖÂê´‰æÜËá™ 71 Êú¨Â§ßÂ≠∏ÊïôÁßëÊõ∏ÁöÑÂ∞àÂÆ∂Êí∞ÂØ´„ÄÅÂêåË°åË©ïÂØ©ÁöÑÊé¢Ê∏¨ÔºåÊ∂µËìã‰∏âÁ®ÆË™ûË®ÄÔºàËã±Ë™û„ÄÅË•øÁè≠ÁâôË™ûÂíåÊ≥¢Ëò≠Ë™ûÔºâ„ÄÇMALAMUTE ÊòØÁ¨¨‰∏ÄÂÄãÂü∫ÊñºÊïôËÇ≤ÁöÑÂÆåÂΩ¢Â°´Á©∫Ë≥áÊñôÈõÜ„ÄÇÂÆÉÊ∂µËìãÂÖ´ÂÄãÈ†òÂüüÔºåÊØèÂÄãÈ†òÂüüÊúÄÂ§öÊúâ 14 ÂÄãÂ≠êÈ†òÂüüÔºåÈÄ≤‰∏ÄÊ≠•Á¥∞ÂàÜÁÇ∫Ê¶ÇÂøµÂíåÂü∫ÊñºÊ¶ÇÂøµÁöÑÊèêÁ§∫ÔºåÁ∏ΩË®à 33,361 ÂÄãÂ§ßÂ≠∏Ë™≤Á®ãÊ¶ÇÂøµÂíå 116,887 ÂÄãÊèêÁ§∫„ÄÇMALAMUTE ÁöÑÁ¥∞Á∑ªÁ≤íÂ∫¶„ÄÅÊïôËÇ≤ÁÑ¶Èªû‰ª•ÂèäÂåÖÂê´Âè•Â≠êÂ±§Á¥öÂíåÊÆµËêΩÂ±§Á¥öÊèêÁ§∫Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫Ë©ï‰º∞ LM ËàáË™≤Á®ãÁõ∏ÈóúÁü•Ë≠òÁöÑÁêÜÊÉ≥Â∑•ÂÖ∑„ÄÇÊàëÂÄëÂ∞ç MALAMUTE ‰∏äÁöÑÈÅÆËîΩÂíåÂõ†Êûú LM ÁöÑË©ï‰º∞Ë°®ÊòéÔºåÂÑòÁÆ°Êï¥È´îÁÜüÁ∑¥Ôºå‰ΩÜ‰ªîÁ¥∞Ê™¢Ë¶ñÁâπÂÆö‰∏ªÈ°åÊôÇÔºåÂÆÉÂÄëÂú®Áü•Ë≠ò‰∏äÂ≠òÂú®È°ØËëóÂ∑ÆË∑ùÔºåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÂú®Ë™≤Â†Ç‰∏äÁöÑÂÆâÂÖ®‰ΩøÁî®Ôºå‰∏¶Âº∑Ë™øÈÄ≤‰∏ÄÊ≠•ÈñãÁôºÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector**
2412.10104v1 by Zhensheng Wang, Wenmian Yang, Kun Zhou, Yiquan Zhang, Weijia Jia

The real estate market relies heavily on structured data, such as property
details, market trends, and price fluctuations. However, the lack of
specialized Tabular Question Answering datasets in this domain limits the
development of automated question-answering systems. To fill this gap, we
introduce RETQA, the first large-scale open-domain Chinese Tabular Question
Answering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762
question-answer pairs across 16 sub-fields within three major domains: property
information, real estate company finance information and land auction
information. Compared with existing tabular question answering datasets, RETQA
poses greater challenges due to three key factors: long-table structures,
open-domain retrieval, and multi-domain queries. To tackle these challenges, we
propose the SLUTQA framework, which integrates large language models with
spoken language understanding tasks to enhance retrieval and answering
accuracy. Extensive experiments demonstrate that SLUTQA significantly improves
the performance of large language models on RETQA by in-context learning. RETQA
and SLUTQA provide essential resources for advancing tabular question answering
research in the real estate domain, addressing critical challenges in
open-domain and long-table question-answering. The dataset and code are
publicly available at \url{https://github.com/jensen-w/RETQA}.

ÊëòË¶ÅÔºöÊàøÂú∞Áî¢Â∏ÇÂ†¥Ê•µÂ∫¶‰æùË≥¥ÁµêÊßãÂåñË≥áÊñôÔºå‰æãÂ¶ÇÂú∞Áî¢ÊòéÁ¥∞„ÄÅÂ∏ÇÂ†¥Ë∂®Âã¢ÂíåÂÉπÊ†ºÊ≥¢Âãï„ÄÇÁÑ∂ËÄåÔºåÊ≠§È†òÂüüÁº∫‰πèÂ∞àÊ•≠ÁöÑË°®Ê†ºÂïèÁ≠îË≥áÊñôÈõÜÔºåÈôêÂà∂‰∫ÜËá™ÂãïÂåñÂïèÁ≠îÁ≥ªÁµ±ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÊ≠§Áº∫Âè£ÔºåÊàëÂÄëÂºïÈÄ≤ RETQAÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈáùÂ∞çÊàøÂú∞Áî¢ÁöÑÂ§ßË¶èÊ®°ÈñãÊîæÈ†òÂüü‰∏≠ÊñáË°®Ê†ºÂïèÁ≠îË≥áÊñôÈõÜ„ÄÇRETQA ÂåÖÂê´ 4,932 ÂÄãË°®Ê†ºÂíå 20,762 ÂÄãÂïèÁ≠îÁµÑÔºåÊ∂µËìã‰∏âÂÄã‰∏ªË¶ÅÈ†òÂüüÂÖßÁöÑ 16 ÂÄãÂ≠êÈ†òÂüüÔºöÂú∞Áî¢Ë≥áË®ä„ÄÅÊàøÂú∞Áî¢ÂÖ¨Âè∏Ë≤°ÂãôË≥áË®äÂíåÂúüÂú∞ÊãçË≥£Ë≥áË®ä„ÄÇËàáÁèæÊúâÁöÑË°®Ê†ºÂïèÁ≠îË≥áÊñôÈõÜÁõ∏ÊØîÔºåRETQA Áî±Êñº‰∏âÂÄãÈóúÈçµÂõ†Á¥†ËÄåÂ∏∂‰æÜÊõ¥Â§ßÁöÑÊåëÊà∞ÔºöÈï∑Ë°®Ê†ºÁµêÊßã„ÄÅÈñãÊîæÈ†òÂüüÊì∑ÂèñÂíåÂ§öÈ†òÂüüÊü•Ë©¢„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SLUTQA Ê°ÜÊû∂ÔºåÂÆÉÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËàáÂè£Ë™ûÁêÜËß£‰ªªÂãôÊï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•Â¢ûÂº∑Êì∑ÂèñÂíåÂõûÁ≠îÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåSLUTQA ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÔºåÂ§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú® RETQA ‰∏äÁöÑÊïàËÉΩ„ÄÇRETQA Âíå SLUTQA Êèê‰æõ‰∫ÜÂøÖË¶ÅÁöÑË≥áÊ∫êÔºåÁî®ÊñºÊé®ÈÄ≤ÊàøÂú∞Áî¢È†òÂüüÁöÑË°®Ê†ºÂïèÁ≠îÁ†îÁ©∂ÔºåËß£Ê±∫ÈñãÊîæÈ†òÂüüÂíåÈï∑Ë°®Ê†ºÂïèÁ≠î‰∏≠ÁöÑÈóúÈçµÊåëÊà∞„ÄÇË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº \url{https://github.com/jensen-w/RETQA}„ÄÇ

##### **AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm Detection Incorporating Bi-modal Data Augmentation**
2412.10103v1 by Xiyuan Gao, Shubhi Bansal, Kushaan Gowda, Zhu Li, Shekhar Nayak, Nagendra Kumar, Matt Coler

Detecting sarcasm effectively requires a nuanced understanding of context,
including vocal tones and facial expressions. The progression towards
multimodal computational methods in sarcasm detection, however, faces
challenges due to the scarcity of data. To address this, we present AMuSeD
(Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating
bi-modal Data augmentation). This approach utilizes the Multimodal Sarcasm
Detection Dataset (MUStARD) and introduces a two-phase bimodal data
augmentation strategy. The first phase involves generating varied text samples
through Back Translation from several secondary languages. The second phase
involves the refinement of a FastSpeech 2-based speech synthesis system,
tailored specifically for sarcasm to retain sarcastic intonations. Alongside a
cloud-based Text-to-Speech (TTS) service, this Fine-tuned FastSpeech 2 system
produces corresponding audio for the text augmentations. We also investigate
various attention mechanisms for effectively merging text and audio data,
finding self-attention to be the most efficient for bimodal integration. Our
experiments reveal that this combined augmentation and attention approach
achieves a significant F1-score of 81.0% in text-audio modalities, surpassing
even models that use three modalities from the MUStARD dataset.

ÊëòË¶ÅÔºöÂÅµÊ∏¨Ë´∑Âà∫ÈúÄË¶ÅÂ∞çËÑàÁµ°ÊúâÁ¥∞Á∑ªÁöÑÁêÜËß£ÔºåÂåÖÊã¨Ë™ûË™øÂíåË°®ÊÉÖ„ÄÇÁÑ∂ËÄåÔºåÊúùÂêëÂ§öÊ®°ÊÖãË®àÁÆóÊñπÊ≥ïÁöÑÈÄ≤Â±ïÂú®Ë´∑Âà∫ÂÅµÊ∏¨‰∏äÊúÉÈù¢Ëá®ÊåëÊà∞ÔºåÂõ†ÁÇ∫Ë≥áÊñôÁ®ÄÂ∞ë„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü AMuSeD (Áî®ÊñºÂ§öÊ®°ÊÖãË´∑Âà∫ÂÅµÊ∏¨ÁöÑÊ≥®ÊÑèÂäõÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÁµêÂêàÈõôÊ®°ÊÖãË≥áÊñôÊì¥ÂÖÖ)„ÄÇÊ≠§ÊñπÊ≥ïÂà©Áî®Â§öÊ®°ÊÖãË´∑Âà∫ÂÅµÊ∏¨Ë≥áÊñôÈõÜ (MUStARD)Ôºå‰∏¶ÂºïÂÖ•‰∏ÄÂÄãÂÖ©ÈöéÊÆµÁöÑÈõôÊ®°ÊÖãË≥áÊñôÊì¥ÂÖÖÁ≠ñÁï•„ÄÇÁ¨¨‰∏ÄÈöéÊÆµÂåÖÂê´ÈÄèÈÅéÂæûÂ§öÁ®ÆÊ¨°Ë¶ÅË™ûË®ÄÈÄ≤Ë°åÂèçÂêëÁøªË≠Ø‰æÜÁî¢Áîü‰∏çÂêåÁöÑÊñáÂ≠óÁØÑ‰æã„ÄÇÁ¨¨‰∫åÈöéÊÆµÂåÖÂê´ÊîπÈÄ≤ FastSpeech 2 ÁÇ∫Âü∫Á§éÁöÑË™ûÈü≥ÂêàÊàêÁ≥ªÁµ±ÔºåÁâπÂà•ÈáùÂ∞çË´∑Âà∫ÈÄ≤Ë°åË™øÊï¥Ôºå‰ª•‰øùÁïôË´∑Âà∫ÊÄßÁöÑË™ûË™ø„ÄÇÈÄôÂÄãÂæÆË™øÈÅéÁöÑ FastSpeech 2 Á≥ªÁµ±ËàáÈõ≤Á´ØÊñáÂ≠óËΩâË™ûÈü≥ (TTS) ÊúçÂãô‰∏ÄËµ∑ÔºåÁÇ∫ÊñáÂ≠óÊì¥ÂÖÖÁî¢ÁîüÂ∞çÊáâÁöÑÈü≥Ë®ä„ÄÇÊàëÂÄë‰πüÁ†îÁ©∂‰∫ÜÂêÑÁ®ÆÊ≥®ÊÑèÂäõÊ©üÂà∂Ôºå‰ª•ÊúâÊïàÂú∞Âêà‰ΩµÊñáÂ≠óÂíåÈü≥Ë®äË≥áÊñôÔºåÁôºÁèæËá™ÊàëÊ≥®ÊÑèÂäõÂ∞çÊñºÈõôÊ®°ÊÖãÊï¥ÂêàÊúÄÊúâÊïàÁéá„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÈÄôÁ®ÆÁµêÂêàÊì¥ÂÖÖÂíåÊ≥®ÊÑèÂäõÁöÑÊñπÊ≥ïÂú®ÊñáÂ≠óÈü≥Ë®äÊ®°ÊÖã‰∏≠ÈÅîÂà∞È°ØËëóÁöÑ F1 ÂàÜÊï∏ 81.0%ÔºåÁîöËá≥Ë∂ÖË∂ä‰ΩøÁî® MUStARD Ë≥áÊñôÈõÜ‰∏âÂÄãÊ®°ÊÖãÁöÑÊ®°Âûã„ÄÇ

##### **HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation**
2412.10095v1 by Jaione Bengoetxea, Mikel Zubillaga, Ekhi Azurmendi, Maite Heredia, Julen Etxaniz, Markel Ferro, Jeremy Barnes

In this paper we present our submission for the NorSID Shared Task as part of
the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:
Intent Detection, Slot Filling and Dialect Identification, evaluated using data
in different dialects of the Norwegian language. For Intent Detection and Slot
Filling, we have fine-tuned a multitask model in a cross-lingual setting, to
leverage the xSID dataset available in 17 languages. In the case of Dialect
Identification, our final submission consists of a model fine-tuned on the
provided development set, which has obtained the highest scores within our
experiments. Our final results on the test set show that our models do not drop
in performance compared to the development set, likely due to the
domain-specificity of the dataset and the similar distribution of both subsets.
Finally, we also report an in-depth analysis of the provided datasets and their
artifacts, as well as other sets of experiments that have been carried out but
did not yield the best results. Additionally, we present an analysis on the
reasons why some methods have been more successful than others; mainly the
impact of the combination of languages and domain-specificity of the training
data on the results.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰∫§‰∫ÜÊàëÂÄëÂ∞ç NorSID ÂÖ±‰∫´‰ªªÂãôÁöÑÊèê‰∫§Ôºå‰ΩúÁÇ∫ 2025 VarDial Á†îË®éÊúÉÔºàScherrer Á≠â‰∫∫Ôºå2025 Âπ¥ÔºâÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂåÖÊã¨‰∏âÂÄã‰ªªÂãôÔºöÊÑèÂúñÂÅµÊ∏¨„ÄÅÊßΩ‰ΩçÂ°´Ë£úÂíåÊñπË®ÄËæ®Ë≠òÔºå‰ΩøÁî®Êå™Â®ÅË™û‰∏çÂêåÊñπË®ÄÁöÑË≥áÊñôÈÄ≤Ë°åË©ï‰º∞„ÄÇÂ∞çÊñºÊÑèÂúñÂÅµÊ∏¨ÂíåÊßΩ‰ΩçÂ°´Ë£úÔºåÊàëÂÄëÂú®Ë∑®Ë™ûË®ÄË®≠ÂÆö‰∏≠ÂæÆË™ø‰∫Ü‰∏ÄÂÄãÂ§ö‰ªªÂãôÊ®°ÂûãÔºå‰ª•Âà©Áî® 17 Á®ÆË™ûË®Ä‰∏≠ÂèØÁî®ÁöÑ xSID Ë≥áÊñôÈõÜ„ÄÇÂú®ÊñπË®ÄËæ®Ë≠òÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÁöÑÊúÄÁµÇÊèê‰∫§ÂåÖÂê´‰∏ÄÂÄãÂú®Êèê‰æõÁöÑÈñãÁôºË®≠ÂÆö‰∏äÂæÆË™øÁöÑÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠Áç≤Âæó‰∫ÜÊúÄÈ´òÂàÜ„ÄÇÊàëÂÄëÂú®Ê∏¨Ë©¶Ë®≠ÂÆö‰∏äÁöÑÊúÄÁµÇÁµêÊûúÈ°ØÁ§∫ÔºåËàáÈñãÁôºË®≠ÂÆöÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÁöÑÊïàËÉΩÊ≤íÊúâ‰∏ãÈôçÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºË≥áÊñôÈõÜÁöÑÁâπÂÆöÈ†òÂüüÂíåÂÖ©ÂÄãÂ≠êÈõÜÁöÑÁõ∏‰ººÂàÜ‰Ωà„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÇÑÂ†±Âëä‰∫ÜÂ∞çÊèê‰æõÁöÑË≥áÊñôÈõÜÂèäÂÖ∂‰∫∫Â∑•Ë£ΩÂìÅÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºå‰ª•ÂèäÂ∑≤Âü∑Ë°å‰ΩÜÊú™Áî¢ÁîüÊúÄ‰Ω≥ÁµêÊûúÁöÑÂÖ∂‰ªñÂØ¶È©óÁµÑ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÊüê‰∫õÊñπÊ≥ïÊØîÂÖ∂‰ªñÊñπÊ≥ïÊõ¥ÊàêÂäüÁöÑÂéüÂõ†Ôºõ‰∏ªË¶ÅÊòØË™ûË®ÄÁµÑÂêàÂíåË®ìÁ∑¥Ë≥áÊñôÁöÑÁâπÂÆöÈ†òÂüüÂ∞çÁµêÊûúÁöÑÂΩ±Èüø„ÄÇ</paragraph>

##### **AI in the Cosmos**
2412.10093v1 by N. Sahakyan

Artificial intelligence (AI) is revolutionizing research by enabling the
efficient analysis of large datasets and the discovery of hidden patterns. In
astrophysics, AI has become essential, transforming the classification of
celestial sources, data modeling, and the interpretation of observations. In
this review, I highlight examples of AI applications in astrophysics, including
source classification, spectral energy distribution modeling, and discuss the
advancements achievable through generative AI. However, the use of AI
introduces challenges, including biases, errors, and the "black box" nature of
AI models, which must be resolved before their application. These issues can be
addressed through the concept of Human-Guided AI (HG-AI), which integrates
human expertise and domain-specific knowledge into AI applications. This
approach aims to ensure that AI is applied in a robust, interpretable, and
ethical manner, leading to deeper insights and fostering scientific excellence.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÈÄèÈÅéÂçîÂä©ÊúâÊïàÂàÜÊûêÂ§ßÂûãË≥áÊñôÈõÜÂíåÁôºÁèæÈö±ËóèÊ®°ÂºèÔºåÈÄ≤ËÄåÈù©Êñ∞Á†îÁ©∂„ÄÇÂú®Â§©ÊñáÁâ©ÁêÜÂ≠∏‰∏≠ÔºåAI Â∑≤ÊàêÁÇ∫‰∏çÂèØÊàñÁº∫ÁöÑÂ∑•ÂÖ∑ÔºåËΩâËÆä‰∫ÜÂ§©È´î‰æÜÊ∫êÁöÑÂàÜÈ°û„ÄÅË≥áÊñôÂª∫Ê®°ÂíåËßÄÊ∏¨ÁµêÊûúÁöÑË©ÆÈáã„ÄÇÂú®ÈÄôÁØáË©ïË´ñ‰∏≠ÔºåÊàëÈáçÈªû‰ªãÁ¥π‰∫Ü AI Âú®Â§©ÊñáÁâ©ÁêÜÂ≠∏‰∏≠ÁöÑÊáâÁî®ÁØÑ‰æãÔºåÂåÖÊã¨‰æÜÊ∫êÂàÜÈ°û„ÄÅÂÖâË≠úËÉΩÈáèÂàÜÂ∏ÉÂª∫Ê®°Ôºå‰∏¶Êé¢Ë®éÈÄèÈÅéÁîüÊàêÂºè AI ÊâÄËÉΩÈÅîÂà∞ÁöÑÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåAI ÁöÑ‰ΩøÁî®‰πüÂ∏∂‰æÜ‰∫ÜÊåëÊà∞ÔºåÂåÖÊã¨ÂÅèÂ∑Æ„ÄÅÈåØË™§Âíå AI Ê®°ÂûãÁöÑ„ÄåÈªëÁÆ±„ÄçÊÄßË≥™ÔºåÂú®ÊáâÁî®‰πãÂâçÂøÖÈ†àËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÈÄô‰∫õÂïèÈ°åÂèØÈÄèÈÅé‰∫∫È°ûÂºïÂ∞é AI (HG-AI) ÁöÑÊ¶ÇÂøµ‰æÜËß£Ê±∫ÔºåÈÄôÂ∞á‰∫∫È°ûÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÊï¥ÂêàÂà∞ AI ÊáâÁî®‰∏≠„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÊó®Âú®Á¢∫‰øù AI ‰ª•Âº∑ÂÅ•„ÄÅÂèØË©ÆÈáãÂíåÂêà‰πéÈÅìÂæ∑ÁöÑÊñπÂºèÊáâÁî®ÔºåÈÄ≤ËÄåÁç≤ÂæóÊõ¥Ê∑±ÂÖ•ÁöÑË¶ãËß£‰∏¶‰øÉÈÄ≤ÁßëÂ≠∏ÂçìË∂ä„ÄÇ

##### **Lost in the Middle, and In-Between: Enhancing Language Models' Ability to Reason Over Long Contexts in Multi-Hop QA**
2412.10079v1 by George Arthur Baker, Ankush Raut, Sagi Shaier, Lawrence E Hunter, Katharina von der Wense

Previous work finds that recent long-context language models fail to make
equal use of information in the middle of their inputs, preferring pieces of
information located at the tail ends which creates an undue bias in situations
where we would like models to be equally capable of using different parts of
the input. Thus far, the problem has mainly only been considered in settings
with single pieces of critical information, leading us to question what happens
when multiple necessary pieces of information are spread out over the inputs.
Here, we demonstrate the effects of the "lost in the middle" problem in the
multi-hop question answering setting -- in which multiple reasoning "hops" over
disconnected documents are required -- and show that performance degrades not
only with respect to the distance of information from the edges of the context,
but also between pieces of information. Additionally, we experiment with means
of alleviating the problem by reducing superfluous document contents through
knowledge graph triple extraction and summarization, and prompting models to
reason more thoroughly using chain-of-thought prompting.

ÊëòË¶ÅÔºöÂÖàÂâçÁöÑÁ†îÁ©∂ÁôºÁèæÔºåÊúÄËøëÁöÑÈï∑Ë™ûÂ¢ÉË™ûË®ÄÊ®°ÂûãÁÑ°Ê≥ïÂπ≥ÂùáÂà©Áî®ÂÖ∂Ëº∏ÂÖ•‰∏≠ÊÆµÁöÑË≥áË®äÔºåÂÅèÂ•Ω‰ΩçÊñºÂ∞æÁ´ØÁöÑË≥áË®äÁâáÊÆµÔºåÈÄôÊúÉÈÄ†Êàê‰∏çÁï∂ÁöÑÂÅèÂ∑ÆÔºåÂú®ÊàëÂÄëÂ∏åÊúõÊ®°ÂûãËÉΩÂπ≥Âùá‰ΩøÁî®Ëº∏ÂÖ•‰∏çÂêåÈÉ®ÂàÜÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂà∞ÁõÆÂâçÁÇ∫Ê≠¢ÔºåÈÄôÂÄãÂïèÈ°å‰∏ªË¶ÅÂè™Âú®ÂÖ∑ÊúâÂñÆ‰∏ÄÈóúÈçµË≥áË®äÁâáÊÆµÁöÑË®≠ÂÆö‰∏≠Ë¢´ËÄÉÊÖÆÔºåÂ∞éËá¥ÊàëÂÄëË≥™ÁñëÁï∂Â§öÂÄãÂøÖË¶ÅÁöÑË≥áË®äÁâáÊÆµÊï£‰ΩàÂú®Ëº∏ÂÖ•‰∏≠ÊôÇÊúÉÁôºÁîü‰ªÄÈ∫ºÊÉÖÊ≥Å„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÁ§∫ÁØÑ‰∫Ü„ÄåÈÅ∫Â§±Âú®‰∏≠Èñì„ÄçÂïèÈ°åÂú®Â§öË∑≥ÂïèÁ≠îË®≠ÂÆö‰∏≠ÁöÑÂΩ±ÈüøÔºåÂÖ∂‰∏≠ÈúÄË¶ÅË∑®Ë∂äÊú™ÈÄ£Êé•Êñá‰ª∂ÁöÑÂ§öÊ¨°Êé®ÁêÜ„ÄåË∑≥Ë∫ç„ÄçÔºå‰∏¶È°ØÁ§∫ÊïàËÉΩ‰∏çÂÉÖÊúÉÈö®ËëóË≥áË®äËàáË™ûÂ¢ÉÈÇäÁ∑£ÁöÑË∑ùÈõ¢ËÄå‰∏ãÈôçÔºå‰πüÊúÉÈö®ËëóË≥áË®äÁâáÊÆµ‰πãÈñìÁöÑË∑ùÈõ¢ËÄå‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂØ¶È©ó‰∫ÜÈÄèÈÅéÁü•Ë≠òÂúñË≠ú‰∏âÂÖÉÁµÑËêÉÂèñÂíåÊëòË¶Å‰æÜÊ∏õÂ∞ëÂ§öÈ§òÊñá‰ª∂ÂÖßÂÆπÔºå‰∏¶ÊèêÁ§∫Ê®°Âûã‰ΩøÁî®ÊÄùËÄÉÈèàÊèêÁ§∫‰æÜÊõ¥ÂæπÂ∫ïÂú∞Êé®ÁêÜÔºå‰ª•Ê∏õËºïÂïèÈ°åÁöÑÊñπÊ≥ï„ÄÇ

##### **Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity**
2412.10059v1 by Dongyun Kam, Myeongji Yun, Sunwoo Yoo, Seungwoo Hong, Zhengya Zhang, Youngjoo Lee

Low bit-precisions and their bit-slice sparsity have recently been studied to
accelerate general matrix-multiplications (GEMM) during large-scale deep neural
network (DNN) inferences. While the conventional symmetric quantization
facilitates low-resolution processing with bit-slice sparsity for both weight
and activation, its accuracy loss caused by the activation's asymmetric
distributions cannot be acceptable, especially for large-scale DNNs. In efforts
to mitigate this accuracy loss, recent studies have actively utilized
asymmetric quantization for activations without requiring additional
operations. However, the cutting-edge asymmetric quantization produces numerous
nonzero slices that cannot be compressed and skipped by recent bit-slice GEMM
accelerators, naturally consuming more processing energy to handle the
quantized DNN models.
  To simultaneously achieve high accuracy and hardware efficiency for
large-scale DNN inferences, this paper proposes an Asymmetrically-Quantized
bit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous
bit-slice computing, which only skips operations of zero slices, the AQS-GEMM
compresses frequent nonzero slices, generated by asymmetric quantization, and
skips their operations. To increase the slice-level sparsity of activations, we
also introduce two algorithm-hardware co-optimization methods: a zero-point
manipulation and a distribution-based bit-slicing. To support the proposed
AQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN
accelerator, Panacea, which efficiently handles sparse/dense workloads of the
tiled AQS-GEMM to increase data reuse and utilization. Panacea supports a
specialized dataflow and run-length encoding to maximize data reuse and
minimize external memory accesses, significantly improving its hardware
efficiency. Our benchmark evaluations show Panacea outperforms existing DNN
accelerators.

ÊëòË¶ÅÔºö<paragraph>‰Ωé‰ΩçÂÖÉÁ≤æÂ∫¶ÂèäÂÖ∂‰ΩçÂÖÉÂàáÁâáÁ®ÄÁñèÊÄßÊúÄËøëÂ∑≤Ë¢´Á†îÁ©∂Áî®‰∫éÂú®Â§ßÂûãÊ∑±Â∫¶Á•ûÁªèÁΩëÁªú (DNN) Êé®ËÆ∫ÊúüÈó¥Âä†ÈÄüÈÄöÁî®Áü©Èòµ‰πòÊ≥ï (GEMM)„ÄÇËôΩÁÑ∂‰º†ÁªüÁöÑÂØπÁß∞ÈáèÂåñ‰øÉËøõ‰∫Ü‰ΩçÂÖÉÂàáÁâáÁ®ÄÁñèÊÄßÁöÑ‰ΩéÂàÜËæ®ÁéáÂ§ÑÁêÜÔºåÂêåÊó∂ÈÄÇÁî®‰∫éÊùÉÈáçÂíåÊøÄÊ¥ªÔºå‰ΩÜÁî±‰∫éÊøÄÊ¥ªÁöÑ‰∏çÂØπÁß∞ÂàÜÂ∏ÉËÄåÈÄ†ÊàêÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±ÊòØ‰∏çÂèØÊé•ÂèóÁöÑÔºåÁâπÂà´ÊòØÂØπ‰∫éÂ§ßÂûã DNN„ÄÇ‰∏∫‰∫ÜÂáèËΩªËøôÁßçÂáÜÁ°ÆÊÄßÊçüÂ§±ÔºåÊúÄËøëÁöÑÁ†îÁ©∂ÁßØÊûÅÂà©Áî®‰∫Ü‰∏çÂØπÁß∞ÈáèÂåñÊù•ÊøÄÊ¥ªÔºåËÄå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÊìç‰Ωú„ÄÇÁÑ∂ËÄåÔºåÊúÄÂÖàËøõÁöÑ‰∏çÂØπÁß∞ÈáèÂåñ‰∫ßÁîü‰∫ÜÂ§ßÈáèÈùûÈõ∂ÂàáÁâáÔºåËøô‰∫õÂàáÁâáÊó†Ê≥ïË¢´ÊúÄËøëÁöÑ‰ΩçÂÖÉÂàáÁâá GEMM Âä†ÈÄüÂô®ÂéãÁº©ÂíåË∑≥ËøáÔºåËá™ÁÑ∂‰ºöÊ∂àËÄóÊõ¥Â§öÂ§ÑÁêÜËÉΩÈáèÊù•Â§ÑÁêÜÈáèÂåñÁöÑ DNN Ê®°Âûã„ÄÇ
‰∏∫‰∫ÜÂêåÊó∂ÂÆûÁé∞Â§ßÂûã DNN Êé®ËÆ∫ÁöÑÈ´òÁ≤æÂ∫¶ÂíåÁ°¨‰ª∂ÊïàÁéáÔºåÊú¨ÊñáÈ¶ñÊ¨°ÊèêÂá∫‰∫Ü‰∏çÂØπÁß∞ÈáèÂåñÁöÑ‰ΩçÂÖÉÂàáÁâá GEMM (AQS-GEMM)„ÄÇ‰∏é‰ª•Ââç‰ªÖË∑≥ËøáÈõ∂ÂàáÁâáÊìç‰ΩúÁöÑ‰ΩçÂÖÉÂàáÁâáËÆ°ÁÆó‰∏çÂêåÔºåAQS-GEMM ÂéãÁº©‰∫ÜÁî±‰∏çÂØπÁß∞ÈáèÂåñÁîüÊàêÁöÑÈ´òÈ¢ëÈùûÈõ∂ÂàáÁâáÔºåÂπ∂Ë∑≥ËøáÂÆÉ‰ª¨ÁöÑËøêÁÆó„ÄÇ‰∏∫‰∫ÜÂ¢ûÂä†ÊøÄÊ¥ªÁöÑÂàáÁâáÁ∫ßÁ®ÄÁñèÊÄßÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏§ÁßçÁÆóÊ≥ï-Á°¨‰ª∂ÂçèÂêå‰ºòÂåñÊñπÊ≥ïÔºöÈõ∂ÁÇπÊìç‰ΩúÂíåÂü∫‰∫éÂàÜÂ∏ÉÁöÑ‰ΩçÂÖÉÂàáÁâá„ÄÇ‰∏∫‰∫ÜÂú®Á°¨‰ª∂Á∫ßÂà´ÊîØÊåÅÊâÄÊèêÂá∫ÁöÑ AQS-GEMM Âíå‰ºòÂåñÔºåÊàë‰ª¨Êñ∞Êé®Âá∫‰∫Ü‰∏ÄÁßç DNN Âä†ÈÄüÂô® PanaceaÔºåÂÆÉÂèØ‰ª•ÊúâÊïàÂú∞Â§ÑÁêÜÂπ≥Èì∫ AQS-GEMM ÁöÑÁ®ÄÁñè/ÂØÜÈõÜÂ∑•‰ΩúË¥üËΩΩÔºå‰ª•Â¢ûÂä†Êï∞ÊçÆÈáçÁî®ÂíåÂà©Áî®Áéá„ÄÇPanacea ÊîØÊåÅ‰∏ìÈó®ÁöÑÊï∞ÊçÆÊµÅÂíåÊ∏∏Á®ãÁºñÁ†ÅÔºå‰ª•ÊúÄÂ§ßÂåñÊï∞ÊçÆÈáçÁî®Âπ∂ÊúÄÂ∞èÂåñÂ§ñÈÉ®ÂÜÖÂ≠òËÆøÈóÆÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÂÖ∂Á°¨‰ª∂ÊïàÁéá„ÄÇÊàë‰ª¨ÁöÑÂü∫ÂáÜËØÑ‰º∞Ë°®ÊòéÔºåPanacea ‰ºò‰∫éÁé∞ÊúâÁöÑ DNN Âä†ÈÄüÂô®„ÄÇ</paragraph>

##### **GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?**
2412.10056v1 by Zhikai Lei, Tianyi Liang, Hanglei Hu, Jin Zhang, Yunhua Zhou, Yunfan Shao, Linyang Li, Chenchui Li, Changbo Wang, Hang Yan, Qipeng Guo

Large Language Models (LLMs) are commonly evaluated using human-crafted
benchmarks, under the premise that higher scores implicitly reflect stronger
human-like performance. However, there is growing concern that LLMs may ``game"
these benchmarks due to data leakage, achieving high scores while struggling
with tasks simple for humans. To substantively address the problem, we create
GAOKAO-Eval, a comprehensive benchmark based on China's National College
Entrance Examination (Gaokao), and conduct ``closed-book" evaluations for
representative models released prior to Gaokao. Contrary to prevailing
consensus, even after addressing data leakage and comprehensiveness,
GAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned
capabilities. To better understand this mismatch, We introduce the Rasch model
from cognitive psychology to analyze LLM scoring patterns and identify two key
discrepancies: 1) anomalous consistent performance across various question
difficulties, and 2) high variance in performance on questions of similar
difficulty. In addition, We identified inconsistent grading of LLM-generated
answers among teachers and recurring mistake patterns. we find that the
phenomenons are well-grounded in the motivations behind OpenAI o1, and o1's
reasoning-as-difficulties can mitigate the mismatch. These results show that
GAOKAO-Eval can reveal limitations in LLM capabilities not captured by current
benchmarks and highlight the need for more LLM-aligned difficulty analysis.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÈÄöÂ∏∏‰ΩøÁî®‰∫∫Â∑•Ë©ïÂàÜÂü∫Ê∫ñÈÄ≤Ë°åË©ï‰º∞ÔºåÂâçÊèêÊòØËºÉÈ´òÁöÑÂàÜÊï∏Èö±Âê´ËëóÊõ¥Âº∑ÁöÑ‰∫∫È°ûË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåË∂ä‰æÜË∂ä‰ª§‰∫∫ÊìîÊÜÇÁöÑÊòØÔºåLLM ÂèØËÉΩÊúÉÁî±ÊñºË≥áÊñôÂ§ñÊ¥©ËÄå„ÄåÁé©ÂºÑ„ÄçÈÄô‰∫õË©ïÂàÜÂü∫Ê∫ñÔºåÂú®‰∫∫È°ûÂü∑Ë°åÁ∞°ÂñÆ‰ªªÂãôÊôÇËã¶Ëã¶ÊéôÊâéÔºåÂçªËÉΩÁç≤ÂæóÈ´òÂàÜ„ÄÇÁÇ∫‰∫ÜÂØ¶Ë≥™Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊ†πÊìö‰∏≠ÂúãÂúãÂÆ∂Â§ßÂ≠∏ÂÖ•Â≠∏ËÄÉË©¶ÔºàÈ´òËÄÉÔºâÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË©ïÂàÜÂü∫Ê∫ñ GAOKAO-EvalÔºå‰∏¶Â∞çÂú®È´òËÄÉ‰πãÂâçÁôºÂ∏ÉÁöÑ‰ª£Ë°®ÊÄßÊ®°ÂûãÈÄ≤Ë°å„ÄåÈñâÂç∑„ÄçË©ï‰º∞„ÄÇËàáÊôÆÈÅçÂÖ±Ë≠òÁõ∏ÂèçÔºåÂç≥‰ΩøÂú®Ëß£Ê±∫Ë≥áÊñôÂ§ñÊ¥©ÂíåÂÖ®Èù¢ÊÄßÂïèÈ°åÂæåÔºåGAOKAO-Eval Êè≠Á§∫È´òÂàÜ‰ªçÁÑ∂ÁÑ°Ê≥ïÁúüÊ≠£ÂèçÊò†Ëàá‰∫∫È°û‰∏ÄËá¥ÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊõ¥Â•ΩÂú∞ÁêÜËß£ÈÄôÁ®Æ‰∏çÂåπÈÖçÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜË™çÁü•ÂøÉÁêÜÂ≠∏‰∏≠ÁöÑ Rasch Ê®°Âûã‰æÜÂàÜÊûê LLM Ë©ïÂàÜÊ®°Âºè‰∏¶ÊâæÂá∫ÂÖ©ÂÄãÈóúÈçµÂ∑ÆÁï∞Ôºö1ÔºâÂú®ÂêÑÁ®ÆÂïèÈ°åÈõ£Â∫¶‰∏≠Áï∞Â∏∏‰∏ÄËá¥ÁöÑË°®ÁèæÔºå‰ª•Âèä 2ÔºâÂú®Èõ£Â∫¶Áõ∏‰ººÁöÑÂïèÈ°å‰∏äË°®ÁèæÂá∫È´òËÆäÁï∞ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæÊïôÂ∏´Â∞ç LLM ÁîüÊàêÁöÑÁ≠îÊ°àË©ïÂàÜ‰∏ç‰∏ÄËá¥Ôºå‰∏¶‰∏îÂ≠òÂú®ÈáçË§áÁöÑÈåØË™§Ê®°Âºè„ÄÇÊàëÂÄëÁôºÁèæÈÄô‰∫õÁèæË±°Ê∑±Ê∑±Ê§çÊ†πÊñº OpenAI o1 ËÉåÂæåÁöÑÂãïÊ©üÔºåo1 ÁöÑÊé®ÁêÜÂç≥Âõ∞Èõ£ÔºåÂèØ‰ª•Ê∏õËºïÈÄôÁ®Æ‰∏çÂåπÈÖç„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåGAOKAO-Eval ÂèØ‰ª•Êè≠Á§∫ LLM ËÉΩÂäõ‰∏≠ÁöÑÈôêÂà∂ÔºåËÄåÁõÆÂâçÁöÑË©ïÂàÜÂü∫Ê∫ñÁÑ°Ê≥ïÊçïÊçâÂà∞ÈÄô‰∫õÈôêÂà∂Ôºå‰∏¶Âº∑Ë™øÈúÄË¶ÅÈÄ≤Ë°åÊõ¥Â§öËàá LLM ‰∏ÄËá¥ÁöÑÈõ£Â∫¶ÂàÜÊûê„ÄÇ

##### **Unsupervised Named Entity Disambiguation for Low Resource Domains**
2412.10054v1 by Debarghya Datta, Soumajit Pramanik

In the ever-evolving landscape of natural language processing and information
retrieval, the need for robust and domain-specific entity linking algorithms
has become increasingly apparent. It is crucial in a considerable number of
fields such as humanities, technical writing and biomedical sciences to enrich
texts with semantics and discover more knowledge. The use of Named Entity
Disambiguation (NED) in such domains requires handling noisy texts, low
resource settings and domain-specific KBs. Existing approaches are mostly
inappropriate for such scenarios, as they either depend on training data or are
not flexible enough to work with domain-specific KBs. Thus in this work, we
present an unsupervised approach leveraging the concept of Group Steiner Trees
(GST), which can identify the most relevant candidates for entity
disambiguation using the contextual similarities across candidate entities for
all the mentions present in a document. We outperform the state-of-the-art
unsupervised methods by more than 40\% (in avg.) in terms of Precision@1 across
various domain-specific datasets.

ÊëòË¶ÅÔºöÂú®‰∏çÊñ≠ÊºîÈÄ≤ÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåË≥áË®äÊ™¢Á¥¢È†òÂüü‰∏≠ÔºåÂ∞çÂº∑ÂÅ•‰∏îÁâπÂÆöÊñºÈ†òÂüüÁöÑÂØ¶È´îÈÄ£ÁµêÊºîÁÆóÊ≥ïÁöÑÈúÄÊ±ÇÂ∑≤ËÆäÂæóË∂ä‰æÜË∂äÊòéÈ°Ø„ÄÇÂú®Â§ßÈáèÈ†òÂüü‰∏≠Ôºå‰æãÂ¶Ç‰∫∫ÊñáÂ≠∏Áßë„ÄÅÊäÄË°ìÂØ´‰ΩúÂíåÁîüÁâ©ÈÜ´Â≠∏ÁßëÂ≠∏ÔºåË±êÂØåË™ûÊÑèÂíåÁôºÁèæÊõ¥Â§öÁü•Ë≠òËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄô‰∫õÈ†òÂüü‰∏≠‰ΩøÁî®ÂëΩÂêçÂØ¶È´îÊ∂àÊ≠ß (NED) ÈúÄË¶ÅËôïÁêÜÈõúË®äÊñáÂ≠ó„ÄÅ‰ΩéË≥áÊ∫êË®≠ÂÆöÂíåÁâπÂÆöÊñºÈ†òÂüüÁöÑÁü•Ë≠òÂ∫´„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÂ§ßÂ§ö‰∏çÈÅ©ÂêàÊ≠§È°ûÂ†¥ÊôØÔºåÂõ†ÁÇ∫ÂÆÉÂÄë‰æùË≥¥ÊñºË®ìÁ∑¥Ë≥áÊñôÊàñ‰∏çÂ§†ÈùàÊ¥ªÔºåÁÑ°Ê≥ïËàáÁâπÂÆöÊñºÈ†òÂüüÁöÑÁü•Ë≠òÂ∫´Êê≠ÈÖç‰ΩøÁî®„ÄÇÂõ†Ê≠§ÔºåÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁÑ°Áõ£Áù£ÁöÑÊñπÊ≥ïÔºåÂà©Áî®Áæ§ÁµÑÂè≤Âù¶Á¥çÊ®π (GST) ÁöÑÊ¶ÇÂøµÔºåÂÆÉÂèØ‰ª•‰ΩøÁî®Êñá‰ª∂‰∏≠ÊâÄÊúâÊèêÂèäÁöÑÂÄôÈÅ∏ÂØ¶È´î‰πãÈñìÁöÑ‰∏ä‰∏ãÊñáÁõ∏‰ººÊÄß‰æÜË≠òÂà•ÂØ¶È´îÊ∂àÊ≠ßÊúÄÁõ∏ÈóúÁöÑÂÄôÈÅ∏È†Ö„ÄÇÂú®ÂêÑÁ®ÆÁâπÂÆöÊñºÈ†òÂüüÁöÑË≥áÊñôÈõÜ‰∏äÔºåÊàëÂÄëÂú® Precision@1 ÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÁÑ°Áõ£Áù£ÊñπÊ≥ïË∂ÖÈÅé 40%ÔºàÂπ≥ÂùáÂÄºÔºâ„ÄÇ

##### **TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views**
2412.10051v1 by Liang Zhao, Zehan Bao, Yi Xie, Hong Chen, Yaohui Chen, Weifu Li

Recent advances in Gaussian Splatting have significantly advanced the field,
achieving both panoptic and interactive segmentation of 3D scenes. However,
existing methodologies often overlook the critical need for reconstructing
specified targets with complex structures from sparse views. To address this
issue, we introduce TSGaussian, a novel framework that combines semantic
constraints with depth priors to avoid geometry degradation in challenging
novel view synthesis tasks. Our approach prioritizes computational resources on
designated targets while minimizing background allocation. Bounding boxes from
YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask
predictions, ensuring semantic accuracy and cost efficiency. TSGaussian
effectively clusters 3D gaussians by introducing a compact identity encoding
for each Gaussian ellipsoid and incorporating 3D spatial consistency
regularization. Leveraging these modules, we propose a pruning strategy to
effectively reduce redundancy in 3D gaussians. Extensive experiments
demonstrate that TSGaussian outperforms state-of-the-art methods on three
standard datasets and a new challenging dataset we collected, achieving
superior results in novel view synthesis of specific objects. Code is available
at: https://github.com/leon2000-ai/TSGaussian.

ÊëòË¶ÅÔºöÈ´òÊñØÊï£Â∞ÑÁöÑÊúÄÊñ∞ËøõÂ±ïÊòæËëóÊé®Âä®‰∫ÜËØ•È¢ÜÂüüÁöÑÂèëÂ±ïÔºåÂÆûÁé∞‰∫Ü 3D Âú∫ÊôØÁöÑÂÖ®ÊôØÂíå‰∫§‰∫íÂºèÂàÜÂâ≤„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÂøΩËßÜ‰∫Ü‰ªéÁ®ÄÁñèËßÜÂõæÈáçÂª∫ÂÖ∑ÊúâÂ§çÊùÇÁªìÊûÑÁöÑÁâπÂÆöÁõÆÊ†áÁöÑÂÖ≥ÈîÆÈúÄÊ±Ç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü TSGaussianÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜËØ≠‰πâÁ∫¶Êùü‰∏éÊ∑±Â∫¶ÂÖàÈ™åÁõ∏ÁªìÂêàÔºå‰ª•ÈÅøÂÖçÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊñ∞ËßÜÂõæÂêàÊàê‰ªªÂä°‰∏≠Âá†‰ΩïÈÄÄÂåñ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜËÆ°ÁÆóËµÑÊ∫ê‰ºòÂÖàÂàÜÈÖçÁªôÊåáÂÆöÁõÆÊ†áÔºåÂêåÊó∂ÊúÄÂ∞èÂåñËÉåÊôØÂàÜÈÖç„ÄÇÊù•Ëá™ YOLOv9 ÁöÑËæπÁïåÊ°ÜÁî®‰ΩúÊèêÁ§∫Ôºå‰ª•‰Ωø Segment Anything Model ÁîüÊàê 2D Êé©Á†ÅÈ¢ÑÊµãÔºåÁ°Æ‰øùËØ≠‰πâÂáÜÁ°ÆÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇTSGaussian ÈÄöËøá‰∏∫ÊØè‰∏™È´òÊñØÊ§≠ÁêÉ‰ΩìÂºïÂÖ•Á¥ßÂáëÁöÑË∫´‰ªΩÁºñÁ†ÅÂπ∂ÁªìÂêà 3D Á©∫Èó¥‰∏ÄËá¥ÊÄßÊ≠£ÂàôÂåñÔºåÊúâÊïàÂú∞ÂØπ 3D È´òÊñØËøõË°åËÅöÁ±ª„ÄÇÂà©Áî®Ëøô‰∫õÊ®°ÂùóÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰øÆÂâ™Á≠ñÁï•Ôºå‰ª•ÊúâÊïàÂáèÂ∞ë 3D È´òÊñØ‰∏≠ÁöÑÂÜó‰Ωô„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåTSGaussian Âú®‰∏â‰∏™Ê†áÂáÜÊï∞ÊçÆÈõÜÂíåÊàë‰ª¨Êî∂ÈõÜÁöÑ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊñ∞Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú®ÁâπÂÆöÂØπË±°ÁöÑ novel view synthesis ‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÊûú„ÄÇ‰ª£Á†ÅÂèØ‰ªé‰ª•‰∏ãÁΩëÂùÄËé∑ÂæóÔºöhttps://github.com/leon2000-ai/TSGaussian„ÄÇ

##### **Large Action Models: From Inception to Implementation**
2412.10047v1 by Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang, Si Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

As AI continues to advance, there is a growing demand for systems that go
beyond language-based assistance and move toward intelligent agents capable of
performing real-world actions. This evolution requires the transition from
traditional Large Language Models (LLMs), which excel at generating textual
responses, to Large Action Models (LAMs), designed for action generation and
execution within dynamic environments. Enabled by agent systems, LAMs hold the
potential to transform AI from passive language understanding to active task
completion, marking a significant milestone in the progression toward
artificial general intelligence.
  In this paper, we present a comprehensive framework for developing LAMs,
offering a systematic approach to their creation, from inception to deployment.
We begin with an overview of LAMs, highlighting their unique characteristics
and delineating their differences from LLMs. Using a Windows OS-based agent as
a case study, we provide a detailed, step-by-step guide on the key stages of
LAM development, including data collection, model training, environment
integration, grounding, and evaluation. This generalizable workflow can serve
as a blueprint for creating functional LAMs in various application domains. We
conclude by identifying the current limitations of LAMs and discussing
directions for future research and industrial deployment, emphasizing the
challenges and opportunities that lie ahead in realizing the full potential of
LAMs in real-world applications.
  The code for the data collection process utilized in this paper is publicly
available at: https://github.com/microsoft/UFO/tree/main/dataflow, and
comprehensive documentation can be found at
https://microsoft.github.io/UFO/dataflow/overview/.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÊåÅÁ∫åÈÄ≤Ê≠•ÔºåÂ∞çÊñºË∂ÖË∂äÂü∫ÊñºË™ûË®ÄÁöÑÂçîÂä©Ôºå‰∏¶ÊúùÂêëËÉΩÂ§†Âü∑Ë°åÂØ¶ÈöõË°åÂãïÁöÑÊô∫ÊÖß‰ª£ÁêÜ‰∫∫ÁöÑÁ≥ªÁµ±ÈúÄÊ±ÇÊó•ÁõäÂ¢ûÈï∑„ÄÇÊ≠§ÊºîÈÄ≤ÈúÄË¶ÅÂæûÊìÖÈï∑Áî¢ÁîüÊñáÂ≠óÂõûÊáâÁöÑÂÇ≥Áµ±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËΩâËÆäÁÇ∫Â§ßÂûãÂãï‰ΩúÊ®°Âûã (LAM)ÔºåËÄåÂæåËÄÖÂâáÂ∞àÁÇ∫Âú®ÂãïÊÖãÁí∞Â¢É‰∏≠Áî¢ÁîüÂãï‰ΩúÂíåÂü∑Ë°åÂãï‰ΩúËÄåË®≠Ë®à„ÄÇÈÄèÈÅé‰ª£ÁêÜÁ≥ªÁµ±ÁöÑÂçîÂä©ÔºåLAM ÊìÅÊúâÂ∞á‰∫∫Â∑•Êô∫ÊÖßÂæûË¢´ÂãïË™ûË®ÄÁêÜËß£ËΩâËÆäÁÇ∫‰∏ªÂãï‰ªªÂãôÂÆåÊàêÁöÑÊΩõÂäõÔºå‰∏¶Âú®ÊúùÂêë‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖßÈÇÅÈÄ≤ÁöÑÈÅéÁ®ã‰∏≠Ê®πÁ´ãÈáçË¶ÅÁöÑÈáåÁ®ãÁ¢ë„ÄÇ
  Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®ÊñºÈñãÁôº LAM ÁöÑÂÖ®Èù¢Êû∂ÊßãÔºå‰∏¶Êèê‰æõÂæûÊßãÊÄùÂà∞ÈÉ®ÁΩ≤ÁöÑÁ≥ªÁµ±ÊñπÊ≥ï‰æÜÂª∫Á´ã LAM„ÄÇÊàëÂÄëÈ¶ñÂÖàÊ¶ÇËø∞ LAMÔºåÈáçÈªûË™™ÊòéÂÖ∂Áç®ÁâπÁâπÊÄßÔºå‰∏¶Ë™™ÊòéÂÖ∂Ëàá LLM ÁöÑ‰∏çÂêå‰πãËôï„ÄÇÊàëÂÄë‰ΩøÁî®Âü∫Êñº Windows ‰ΩúÊ•≠Á≥ªÁµ±ÁöÑ‰ª£ÁêÜ‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂Ôºå‰∏¶Êèê‰æõÊúâÈóú LAM ÈñãÁôºÈóúÈçµÈöéÊÆµÁöÑË©≥Á¥∞ÈÄêÊ≠•ÊåáÂçóÔºåÂåÖÊã¨Ë≥áÊñôÊî∂ÈõÜ„ÄÅÊ®°ÂûãË®ìÁ∑¥„ÄÅÁí∞Â¢ÉÊï¥Âêà„ÄÅÂü∫Á§éÂíåË©ï‰º∞„ÄÇÈÄôÂÄãÂèØÊ¶ÇÊã¨ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÂèØ‰ΩúÁÇ∫Âú®ÂêÑÁ®ÆÊáâÁî®È†òÂüüÂª∫Á´ãÂäüËÉΩÊÄß LAM ÁöÑËóçÂúñ„ÄÇÊàëÂÄëÊúÄÂæåÊâæÂá∫ LAM ÁõÆÂâçÁöÑÈôêÂà∂Ôºå‰∏¶Ë®éË´ñÊú™‰æÜÁ†îÁ©∂ÂíåÁî¢Ê•≠ÈÉ®ÁΩ≤ÁöÑÊñπÂêëÔºåÂº∑Ë™øÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂØ¶Áèæ LAM ÂÖ®ÈÉ®ÊΩõÂäõÁöÑÊåëÊà∞ÂíåÊ©üÊúÉ„ÄÇ
  Êú¨Êñá‰∏≠‰ΩøÁî®ÁöÑË≥áÊñôÊî∂ÈõÜÁ®ãÂ∫èÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñºÔºöhttps://github.com/microsoft/UFO/tree/main/dataflowÔºåËÄåÂÖ®Èù¢ÁöÑÊñá‰ª∂ÂèØÊñº https://microsoft.github.io/UFO/dataflow/overview/ ÊâæÂà∞„ÄÇ

##### **Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework**
2412.10011v1 by Niloy Kumar Kundu, Sarah Kobir, Md. Rayhan Ahmed, Tahmina Aktar, Niloya Roy

Speech emotion recognition (SER) is crucial for enhancing affective computing
and enriching the domain of human-computer interaction. However, the main
challenge in SER lies in selecting relevant feature representations from speech
signals with lower computational costs. In this paper, we propose a lightweight
SER architecture that integrates attention-based local feature blocks (ALFBs)
to capture high-level relevant feature vectors from speech signals. We also
incorporate a global feature block (GFB) technique to capture sequential,
global information and long-term dependencies in speech signals. By aggregating
attention-based local and global contextual feature vectors, our model
effectively captures the internal correlation between salient features that
reflect complex human emotional cues. To evaluate our approach, we extracted
four types of spectral features from speech audio samples: mel-frequency
cepstral coefficients, mel-spectrogram, root mean square value, and
zero-crossing rate. Through a 5-fold cross-validation strategy, we tested the
proposed method on five multi-lingual standard benchmark datasets: TESS,
RAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of
99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate
that our model achieves state-of-the-art (SOTA) performance compared to most
existing methods.

ÊëòË¶ÅÔºöË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) Â∞çÊñºÂä†Âº∑ÊÉÖÊÑüÈÅãÁÆóÂíåË±êÂØå‰∫∫Ê©ü‰∫íÂãïÈ†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåSER ÁöÑ‰∏ªË¶ÅÊåëÊà∞Âú®Êñº‰ª•ËºÉ‰ΩéÁöÑÈÅãÁÆóÊàêÊú¨ÂæûË™ûÈü≥Ë®äËôü‰∏≠ÈÅ∏ÊìáÁõ∏ÈóúÁâπÂæµË°®Âæµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËºïÈáèÁ¥ö SER Êû∂ÊßãÔºåÂÆÉÊï¥Âêà‰∫ÜÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂ±ÄÈÉ®ÁâπÂæµÂçÄÂ°ä (ALFB) ‰ª•ÂæûË™ûÈü≥Ë®äËôü‰∏≠Êì∑ÂèñÈ´òÂ±§Á¥öÁõ∏ÈóúÁâπÂæµÂêëÈáè„ÄÇÊàëÂÄëÈÇÑÁµêÂêà‰∫ÜÂÖ®Â±ÄÁâπÂæµÂçÄÂ°ä (GFB) ÊäÄË°ìÔºå‰ª•Êì∑ÂèñË™ûÈü≥Ë®äËôü‰∏≠ÁöÑÈ†ÜÂ∫è„ÄÅÂÖ®Â±ÄË≥áË®äÂíåÈï∑Êúü‰æùË≥¥ÊÄß„ÄÇÈÄèÈÅéÂΩôÁ∏ΩÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËÑàÁµ°ÁâπÂæµÂêëÈáèÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊúâÊïàÂú∞Êì∑Âèñ‰∫ÜÂèçÊò†Ë§áÈõú‰∫∫È°ûÊÉÖÁ∑íÁ∑öÁ¥¢ÁöÑÈ°ØËëóÁâπÂæµ‰πãÈñìÁöÑÂÖßÈÉ®ÈóúËÅØÊÄß„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÁöÑ‰ΩúÊ≥ïÔºåÊàëÂÄëÂæûË™ûÈü≥Èü≥Ë®äÁØÑ‰æã‰∏≠ËêÉÂèñ‰∫ÜÂõõÁ®ÆÈ°ûÂûãÁöÑÈ†ªË≠úÁâπÂæµÔºöÊ¢ÖÁàæÈ†ªÁéáÂÄíÈ†ªÁéáË≠ú‰øÇÊï∏„ÄÅÊ¢ÖÁàæÈ†ªË≠úÂúñ„ÄÅÂùáÊñπÊ†πÂÄºÂíåÈõ∂‰∫§Ë∂äÁéá„ÄÇÈÄèÈÅé 5 ÂÄç‰∫§ÂèâÈ©óË≠âÁ≠ñÁï•ÔºåÊàëÂÄëÂú®‰∫îÂÄãÂ§öË™ûË®ÄÊ®ôÊ∫ñÂü∫Ê∫ñË≥áÊñôÈõÜÔºöTESS„ÄÅRAVDESS„ÄÅBanglaSER„ÄÅSUBESCO Âíå Emo-DB ‰∏äÊ∏¨Ë©¶‰∫ÜÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºå‰∏¶ÂàÜÂà•Áç≤Âæó‰∫Ü 99.65%„ÄÅ94.88%„ÄÅ98.12%„ÄÅ97.94% Âíå 97.19% ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶„ÄÇÁµêÊûúË°®ÊòéÔºåËàáÂ§ßÂ§öÊï∏ÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ (SOTA) ÁöÑÊïàËÉΩ„ÄÇ

##### **Automated Collection of Evaluation Dataset for Semantic Search in Low-Resource Domain Language**
2412.10008v1 by Anastasia Zhukova, Christian E. Matt, Bela Gipp

Domain-specific languages that use a lot of specific terminology often fall
into the category of low-resource languages. Collecting test datasets in a
narrow domain is time-consuming and requires skilled human resources with
domain knowledge and training for the annotation task. This study addresses the
challenge of automated collecting test datasets to evaluate semantic search in
low-resource domain-specific German language of the process industry. Our
approach proposes an end-to-end annotation pipeline for automated query
generation to the score reassessment of query-document pairs. To overcome the
lack of text encoders trained in the German chemistry domain, we explore a
principle of an ensemble of "weak" text encoders trained on common knowledge
datasets. We combine individual relevance scores from diverse models to
retrieve document candidates and relevance scores generated by an LLM, aiming
to achieve consensus on query-document alignment. Evaluation results
demonstrate that the ensemble method significantly improves alignment with
human-assigned relevance scores, outperforming individual models in both
inter-coder agreement and accuracy metrics. These findings suggest that
ensemble learning can effectively adapt semantic search systems for
specialized, low-resource languages, offering a practical solution to resource
limitations in domain-specific contexts.

ÊëòË¶ÅÔºö<paragraph>‰ΩøÁî®Â§ßÈáèÁâπÂÆöË°ìË™ûÁöÑÁâπÂÆöÈ†òÂüüË™ûË®ÄÈÄöÂ∏∏Â±¨Êñº‰ΩéË≥áÊ∫êË™ûË®ÄÈ°ûÂà•„ÄÇÂú®ÁãπÁ™ÑÁöÑÈ†òÂüü‰∏≠Êî∂ÈõÜÊ∏¨Ë©¶Êï∏ÊìöÈõÜÈùûÂ∏∏ËÄóÊôÇÔºå‰∏¶‰∏îÈúÄË¶ÅÂÖ∑ÂÇôÈ†òÂüüÁü•Ë≠òÂíåÊ®ôË®ª‰ªªÂãôË®ìÁ∑¥ÁöÑÁÜüÁ∑¥‰∫∫ÂäõË≥áÊ∫ê„ÄÇÊú¨Á†îÁ©∂Ëß£Ê±∫‰∫ÜËá™ÂãïÊî∂ÈõÜÊ∏¨Ë©¶Êï∏ÊìöÈõÜÁöÑÊåëÊà∞Ôºå‰ª•Ë©ï‰º∞ÊµÅÁ®ãÁî¢Ê•≠‰∏≠‰ΩéË≥áÊ∫êÁâπÂÆöÈ†òÂüüÂæ∑Ë™ûÁöÑË™ûÁæ©ÊêúÁ¥¢„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ´ØÂà∞Á´ØÁöÑÊ®ôË®ªÁÆ°ÈÅìÔºåÁî®ÊñºËá™ÂãïÊü•Ë©¢ÁîüÊàêÔºå‰ª•ÈáçÊñ∞Ë©ïÂàÜÊü•Ë©¢Êñá‰ª∂Â∞ç„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÂú®Âæ∑Ë™ûÂåñÂ≠∏È†òÂüü‰∏≠Ë®ìÁ∑¥ÁöÑÊñáÊú¨Á∑®Á¢ºÂô®ÁöÑ‰∏çË∂≥ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏ÄÂÄãÁî±Âú®Â∏∏Ë≠òÊï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥ÁöÑ„ÄåÂº±„ÄçÊñáÊú¨Á∑®Á¢ºÂô®ÁµÑÊàêÁöÑÈõÜÂêàÂéüÁêÜ„ÄÇÊàëÂÄëÁµêÂêà‰∫Ü‰æÜËá™‰∏çÂêåÊ®°ÂûãÁöÑÂÄãÂà•Áõ∏ÈóúÊÄßÂàÜÊï∏Ôºå‰ª•Êì∑ÂèñÊñá‰ª∂ÂÄôÈÅ∏È†ÖÂíå LLM ÁîüÊàêÁöÑÁõ∏ÈóúÊÄßÂàÜÊï∏ÔºåÊó®Âú®ÈÅîÊàêÊü•Ë©¢Êñá‰ª∂ÊØîÂ∞çÁöÑÂÖ±Ë≠ò„ÄÇË©ï‰º∞ÁµêÊûúË°®ÊòéÔºåÈõÜÂêàÊñπÊ≥ïÈ°ØËëóÊîπÂñÑ‰∫ÜËàá‰∫∫Â∑•ÊåáÂÆöÁöÑÁõ∏ÈóúÊÄßÂàÜÊï∏ÁöÑÊØîÂ∞çÔºåÂú®Á∑®Á¢ºÂô®ÈñìÁöÑ‰∏ÄËá¥ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÊåáÊ®ôÊñπÈù¢ÈÉΩÂÑ™ÊñºÂÄãÂà•Ê®°Âûã„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåÈõÜÂêàÂ≠∏ÁøíÂèØ‰ª•ÊúâÊïàÂú∞Ë™øÊï¥Ë™ûÁæ©ÊêúÁ¥¢Á≥ªÁµ±‰ª•ÈÅ©ÊáâÂ∞àÊ•≠ÁöÑ‰ΩéË≥áÊ∫êË™ûË®ÄÔºåÁÇ∫ÁâπÂÆöÈ†òÂüüËÉåÊôØ‰∏≠ÁöÑË≥áÊ∫êÈôêÂà∂Êèê‰æõÂØ¶Áî®ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ</paragraph>

##### **The role of inhibitory control in garden-path sentence processing: A Chinese-English bilingual perspective**
2412.10006v1 by Xiaohui Rao, Haoze Li, Xiaofang Lin, Lijuan Liang

In reading garden-path sentences, people must resolve competing
interpretations, though initial misinterpretations can linger despite
reanalysis. This study examines the role of inhibitory control (IC) in managing
these misinterpretations among Chinese-English bilinguals. Using self-paced
reading tasks, we investigated how IC influences recovery from garden-path
sentences in Chinese (L1) and its interaction with language proficiency during
English (L2) processing. Results indicate that IC does not affect garden-path
recovery in Chinese, suggesting reliance on semantic context may reduce the
need for IC. In contrast, findings for English L2 learners reveal a complex
relationship between language proficiency and IC: Participants with low L2
proficiency but high IC showed lingering misinterpretations, while those with
high proficiency exhibited none. These results support and extend the Model of
Cognitive Control (Ness et al., 2023). Moreover, our comparison of three Stroop
task versions identifies L1 colour-word Stroop task as the preferred measure of
IC in bilingual research.

ÊëòË¶ÅÔºöÂú®Èñ±ËÆÄËä±ÂúíÂ∞èÂæëÂè•ÊôÇÔºå‰∫∫ÂÄëÂøÖÈ†àËß£Ê±∫Áõ∏‰∫íÁ´∂Áà≠ÁöÑËß£ÈáãÔºåÂÑòÁÆ°ÈáçÊñ∞ÂàÜÊûêÔºåÊúÄÂàùÁöÑË™§Ëß£ÂèØËÉΩÊúÉÊåÅÁ∫åÂ≠òÂú®„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊäëÂà∂ÊÄßÊéßÂà∂ (IC) Âú®ÁÆ°ÁêÜ‰∏≠ÂúãËã±Ë™ûÈõôË™ûËÄÖ‰πãÈñìÈÄô‰∫õË™§Ëß£‰∏≠ÁöÑ‰ΩúÁî®„ÄÇ‰ΩøÁî®Ëá™ÂÆöÈÄ≤Â∫¶Èñ±ËÆÄ‰ªªÂãôÔºåÊàëÂÄëË™øÊü•‰∫Ü IC Â¶Ç‰ΩïÂΩ±ÈüøÂæû‰∏≠Êñá (L1) ÁöÑËä±ÂúíÂ∞èÂæëÂè•‰∏≠ÊÅ¢Âæ©Ôºå‰ª•ÂèäÂÆÉÂú®Ëã±Ë™û (L2) ËôïÁêÜÈÅéÁ®ã‰∏≠ËàáË™ûË®ÄËÉΩÂäõÁöÑ‰∫íÂãï„ÄÇÁµêÊûúË°®ÊòéÔºåIC Ê≤íÊúâÂΩ±Èüø‰∏≠Êñá‰∏≠ÁöÑËä±ÂúíÂ∞èÂæëÊÅ¢Âæ©ÔºåÈÄôË°®Êòé‰æùË≥¥Ë™ûÁæ©‰∏ä‰∏ãÊñáÂèØ‰ª•Ê∏õÂ∞ëÂ∞ç IC ÁöÑÈúÄÊ±Ç„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåËã±Ë™û L2 Â≠∏ÁøíËÄÖÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜË™ûË®ÄËÉΩÂäõÂíå IC ‰πãÈñìÁöÑË§áÈõúÈóú‰øÇÔºöL2 Ê∞¥Âπ≥‰Ωé‰ΩÜ IC È´òÁöÑÂèÉËàáËÄÖË°®ÁèæÂá∫ÊåÅÁ∫åÁöÑË™§Ëß£ÔºåËÄåÈÇ£‰∫õÊ∞¥Âπ≥È´òÁöÑÂèÉËàáËÄÖÂâáÊ≤íÊúâ„ÄÇÈÄô‰∫õÁµêÊûúÊîØÊåÅ‰∏¶Êì¥Â±ï‰∫ÜË™çÁü•ÊéßÂà∂Ê®°Âûã (Ness Á≠â‰∫∫Ôºå2023)„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞ç‰∏âÂÄã Stroop ‰ªªÂãôÁâàÊú¨ÁöÑÊØîËºÉÂ∞á L1 È°èËâ≤Ë©û Stroop ‰ªªÂãôÁ¢∫ÂÆöÁÇ∫ÈõôË™ûÁ†îÁ©∂‰∏≠ IC ÁöÑÈ¶ñÈÅ∏Ê∏¨ÈáèÊñπÊ≥ï„ÄÇ

##### **Cycle-Consistent Bridge Diffusion Model for Accelerated MRI Reconstruction**
2412.09998v1 by Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang

Accelerated MRI reconstruction techniques aim to reduce examination time
while maintaining high image fidelity, which is highly desirable in clinical
settings for improving patient comfort and hospital efficiency. Existing deep
learning methods typically reconstruct images from under-sampled data with
traditional reconstruction approaches, but they still struggle to provide
high-fidelity results. Diffusion models show great potential to improve
fidelity of generated images in recent years. However, their inference process
starting with a random Gaussian noise introduces instability into the results
and usually requires thousands of sampling steps, resulting in sub-optimal
reconstruction quality and low efficiency. To address these challenges, we
propose Cycle-Consistent Bridge Diffusion Model (CBDM). CBDM employs two bridge
diffusion models to construct a cycle-consistent diffusion process with a
consistency loss, enhancing the fine-grained details of reconstructed images
and reducing the number of diffusion steps. Moreover, CBDM incorporates a
Contourlet Decomposition Embedding Module (CDEM) which captures multi-scale
structural texture knowledge in images through frequency domain decomposition
pyramids and directional filter banks to improve structural fidelity. Extensive
experiments demonstrate the superiority of our model by higher reconstruction
quality and fewer training iterations, achieving a new state of the art for
accelerated MRI reconstruction in both fastMRI and IXI datasets.

ÊëòË¶ÅÔºöÂä†ÈÄüÂºè MRI ÈáçÂª∫ÊäÄË°ìÊó®Âú®Á∏ÆÁü≠Ê™¢Êü•ÊôÇÈñìÔºåÂêåÊôÇÁ∂≠ÊåÅÈ´òÂΩ±ÂÉè‰øùÁúüÂ∫¶ÔºåÈÄôÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÈùûÂ∏∏ÁêÜÊÉ≥ÔºåÂèØÊèêÂçáÁóÖÊÇ£ËàíÈÅ©Â∫¶ÂíåÈÜ´Èô¢ÊïàÁéá„ÄÇÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®ÂÇ≥Áµ±ÈáçÂª∫ÊñπÊ≥ïÂæûÊ¨†Êé°Ê®£Êï∏ÊìöÈáçÂª∫ÂΩ±ÂÉèÔºå‰ΩÜ‰ªçÈõ£‰ª•Êèê‰æõÈ´ò‰øùÁúüÂ∫¶ÁµêÊûú„ÄÇÊì¥Êï£Ê®°ÂûãÂú®ËøëÂπ¥Â±ïÁèæÂá∫ÊèêÂçáÁîüÊàêÂΩ±ÂÉè‰øùÁúüÂ∫¶ÁöÑÁµï‰Ω≥ÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÂæûÈö®Ê©üÈ´òÊñØÈõúË®äÈñãÂßãÁöÑÊé®Ë´ñÈÅéÁ®ãÊúÉÁÇ∫ÁµêÊûúÂ∏∂‰æÜ‰∏çÁ©©ÂÆöÊÄßÔºå‰∏îÈÄöÂ∏∏ÈúÄË¶ÅÊï∏ÂçÉÂÄãÊé°Ê®£Ê≠•È©üÔºåÂ∞éËá¥Ê¨°ÊúÄ‰Ω≥ÈáçÂª∫ÂìÅË≥™Âíå‰ΩéÊïàÁéá„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫Âæ™Áí∞‰∏ÄËá¥Ê©ãÊé•Êì¥Êï£Ê®°Âûã (CBDM)„ÄÇCBDM ‰ΩøÁî®ÂÖ©ÂÄãÊ©ãÊé•Êì¥Êï£Ê®°ÂûãÔºåÂª∫Êßã‰∏ÄÂÄãÂÖ∑ÊúâÁõ∏ÂÆπÊÄßÊêçÂ§±ÁöÑÂæ™Áí∞‰∏ÄËá¥Êì¥Êï£ÈÅéÁ®ãÔºåÂ¢ûÂº∑ÈáçÂª∫ÂΩ±ÂÉèÁöÑÁ≤æÁ¥∞Á¥∞ÁØÄ‰∏¶Ê∏õÂ∞ëÊì¥Êï£Ê≠•È©üÁöÑÊï∏Èáè„ÄÇÊ≠§Â§ñÔºåCBDM Êï¥Âêà‰∫Ü‰∏ÄÂÄãËº™ÂªìÂàÜËß£ÂµåÂÖ•Ê®°ÁµÑ (CDEM)ÔºåÈÄèÈÅéÈ†ªÂüüÂàÜËß£ÈáëÂ≠óÂ°îÂíåÊñπÂêëÊøæÊ≥¢Âô®ÁµÑÂú®ÂΩ±ÂÉè‰∏≠Êì∑ÂèñÂ§öÂ∞∫Â∫¶ÁµêÊßãÁ¥ãÁêÜÁü•Ë≠òÔºå‰ª•ÊèêÂçáÁµêÊßã‰øùÁúüÂ∫¶„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊ®°ÂûãÁöÑÂÑ™Áï∞ÊÄßÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÈáçÂª∫ÂìÅË≥™ÂíåÊõ¥Â∞ëÁöÑË®ìÁ∑¥ÂèçË¶ÜÈÅãÁÆóÔºåÂú® fastMRI Âíå IXI Ë≥áÊñôÈõÜÁöÑÂä†ÈÄüÂºè MRI ÈáçÂª∫‰∏≠ÈÅîÊàêÊñ∞ÁöÑÊäÄË°ìÊ∞¥Ê∫ñ„ÄÇ

##### **A Comparative Study of LLMs, NMT Models, and Their Combination in Persian-English Idiom Translation**
2412.09993v1 by Sara Rezaeimanesh, Faezeh Hosseini, Yadollah Yaghoobzadeh

Large language models (LLMs) have shown superior capabilities in translating
figurative language compared to neural machine translation (NMT) systems.
However, the impact of different prompting methods and LLM-NMT combinations on
idiom translation has yet to be thoroughly investigated. This paper introduces
two parallel datasets of sentences containing idiomatic expressions for
Persian$\rightarrow$English and English$\rightarrow$Persian translations, with
Persian idioms sampled from our PersianIdioms resource, a collection of 2,200
idioms and their meanings. Using these datasets, we evaluate various open- and
closed-source LLMs, NMT models, and their combinations. Translation quality is
assessed through idiom translation accuracy and fluency. We also find that
automatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are
effective for comparing different aspects of model performance. Our experiments
reveal that Claude-3.5-Sonnet delivers outstanding results in both translation
directions. For English$\rightarrow$Persian, combining weaker LLMs with Google
Translate improves results, while Persian$\rightarrow$English translations
benefit from single prompts for simpler models and complex prompts for advanced
ones.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÁøªË≠ØÊØîÂñªË™ûË®ÄÊñπÈù¢Â±ïÁèæÂá∫ÊØîÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠Ø (NMT) Á≥ªÁµ±Êõ¥ÂÑ™Ë∂äÁöÑËÉΩÂäõ„ÄÇ
ÁÑ∂ËÄåÔºå‰∏çÂêåÁöÑÊèêÁ§∫ÊñπÊ≥ïÂíå LLM-NMT ÁµÑÂêàÂ∞çÊÖ£Áî®Ë™ûÁøªË≠ØÁöÑÂΩ±ÈüøÂ∞öÊú™ÂæóÂà∞ÂæπÂ∫ïÁ†îÁ©∂„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÂÖ©ÂÄãÂåÖÂê´ÊÖ£Áî®Ë™ûÂè•Â≠êÁöÑÂπ≥Ë°åÊï∏ÊìöÈõÜÔºåÁî®ÊñºÊ≥¢ÊñØË™û$\rightarrow$Ëã±Ë™ûÂíåËã±Ë™û$\rightarrow$Ê≥¢ÊñØË™ûÁøªË≠ØÔºåÂÖ∂‰∏≠Ê≥¢ÊñØË™ûÊÖ£Áî®Ë™ûÂèñÊ®£Ëá™ÊàëÂÄëÁöÑ PersianIdioms Ë≥áÊ∫êÔºåË©≤Ë≥áÊ∫êÊî∂ÈõÜ‰∫Ü 2,200 ÂÄãÊÖ£Áî®Ë™ûÂèäÂÖ∂Âê´Áæ©„ÄÇ‰ΩøÁî®ÈÄô‰∫õÊï∏ÊìöÈõÜÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÂêÑÁ®ÆÈñãÊ∫êÂíåÈñâÊ∫ê LLM„ÄÅNMT Ê®°ÂûãÂèäÂÖ∂ÁµÑÂêà„ÄÇÁøªË≠ØÂìÅË≥™ÈÄöÈÅéÊÖ£Áî®Ë™ûÁøªË≠ØÊ∫ñÁ¢∫Â∫¶ÂíåÊµÅÊö¢Â∫¶ÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåÂÉè LLM-as-a-judge„ÄÅBLEU Âíå BERTScore Á≠âËá™ÂãïË©ï‰º∞ÊñπÊ≥ïÂ∞çÊñºÊØîËºÉÊ®°ÂûãÊïàËÉΩÁöÑ‰∏çÂêåÊñπÈù¢ÊòØÊúâÊïàÁöÑ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåClaude-3.5-Sonnet Âú®ÂÖ©ÂÄãÁøªË≠ØÊñπÂêë‰∏äÈÉΩÊèê‰æõ‰∫ÜÂÇëÂá∫ÁöÑÁµêÊûú„ÄÇÂ∞çÊñºËã±Ë™û$\rightarrow$Ê≥¢ÊñØË™ûÔºåÂ∞áËºÉÂº±ÁöÑ LLM Ëàá Google Translate ÁµêÂêàËµ∑‰æÜÂèØ‰ª•ÊîπÂñÑÁµêÊûúÔºåËÄåÊ≥¢ÊñØË™û$\rightarrow$Ëã±Ë™ûÁøªË≠ØÂâáÂèóÁõäÊñºËºÉÁ∞°ÂñÆÊ®°ÂûãÁöÑÂñÆ‰∏ÄÊèêÁ§∫ÂíåËºÉÈ´òÁ¥öÊ®°ÂûãÁöÑË§áÈõúÊèêÁ§∫„ÄÇ

##### **Visual Object Tracking across Diverse Data Modalities: A Review**
2412.09991v1 by Mengmeng Wang, Teli Ma, Shuo Xin, Xiaojun Hou, Jiazheng Xing, Guang Dai, Jingdong Wang, Yong Liu

Visual Object Tracking (VOT) is an attractive and significant research area
in computer vision, which aims to recognize and track specific targets in video
sequences where the target objects are arbitrary and class-agnostic. The VOT
technology could be applied in various scenarios, processing data of diverse
modalities such as RGB, thermal infrared and point cloud. Besides, since no one
sensor could handle all the dynamic and varying environments, multi-modal VOT
is also investigated. This paper presents a comprehensive survey of the recent
progress of both single-modal and multi-modal VOT, especially the deep learning
methods. Specifically, we first review three types of mainstream single-modal
VOT, including RGB, thermal infrared and point cloud tracking. In particular,
we conclude four widely-used single-modal frameworks, abstracting their schemas
and categorizing the existing inheritors. Then we summarize four kinds of
multi-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language.
Moreover, the comparison results in plenty of VOT benchmarks of the discussed
modalities are presented. Finally, we provide recommendations and insightful
observations, inspiring the future development of this fast-growing literature.

ÊëòË¶ÅÔºöË¶ñË¶∫Áâ©‰ª∂ËøΩËπ§ (VOT) ÊòØÈõªËÖ¶Ë¶ñË¶∫‰∏≠‰∏ÄÂÄãÊúâÂê∏ÂºïÂäõ‰∏îÈáçË¶ÅÁöÑÁ†îÁ©∂È†òÂüüÔºåÂÖ∂ÁõÆÊ®ôÊòØË≠òÂà•‰∏¶ËøΩËπ§ÂΩ±ÁâáÂ∫èÂàó‰∏≠ÁâπÂÆöÁöÑÁõÆÊ®ôÔºåÂÖ∂‰∏≠ÁõÆÊ®ôÁâ©‰ª∂ÊòØ‰ªªÊÑèÁöÑ‰∏îËàáÈ°ûÂà•ÁÑ°Èóú„ÄÇVOT ÊäÄË°ìÂèØÊáâÁî®ÊñºÂêÑÁ®ÆÂ†¥ÊôØÔºåËôïÁêÜÂêÑÁ®ÆÊ®°ÂºèÁöÑË≥áÊñôÔºå‰æãÂ¶Ç RGB„ÄÅÁÜ±Á¥ÖÂ§ñÁ∑öÂíåÈªûÈõ≤„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÊ≤íÊúâ‰ªª‰Ωï‰∏ÄÁ®ÆÊÑüÊ∏¨Âô®ÂèØ‰ª•ËôïÁêÜÊâÄÊúâÂãïÊÖã‰∏îÂ§öËÆäÁöÑÁí∞Â¢ÉÔºåÂõ†Ê≠§Â§öÊ®°Âºè VOT ‰πüÂú®Á†îÁ©∂‰∏≠„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÂñÆÊ®°ÂºèÂíåÂ§öÊ®°Âºè VOT ÊúÄËøëÈÄ≤Â±ïÁöÑÂÖ®Èù¢Ë™øÊü•ÔºåÁâπÂà•ÊòØÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂõûÈ°ß‰∫Ü‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰∏ªÊµÅÂñÆÊ®°Âºè VOTÔºåÂåÖÊã¨ RGB„ÄÅÁÜ±Á¥ÖÂ§ñÁ∑öÂíåÈªûÈõ≤ËøΩËπ§„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÂõõÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂñÆÊ®°ÂºèÊû∂ÊßãÔºåÊäΩË±°ÂÖ∂Êû∂Êßã‰∏¶ÂàÜÈ°ûÁèæÊúâÁöÑÁπºÊâøËÄÖ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÂõõÁ®ÆÈ°ûÂûãÁöÑÂ§öÊ®°Âºè VOTÔºåÂåÖÊã¨ RGB-Ê∑±Â∫¶„ÄÅRGB-ÁÜ±„ÄÅRGB-LiDAR Âíå RGB-Ë™ûË®Ä„ÄÇÊ≠§Â§ñÔºåÈÇÑÊèê‰æõ‰∫ÜÂú®Â§ßÈáè VOT Âü∫Ê∫ñ‰∏≠Ë®éË´ñÊ®°ÂºèÁöÑÊØîËºÉÁµêÊûú„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂª∫Ë≠∞ÂíåÊúâË¶ãÂú∞ÁöÑËßÄÂØüÔºåÊøÄÂãµ‰∫ÜÈÄôÂÄãÂø´ÈÄüÊàêÈï∑ÁöÑÈ†òÂüüÁöÑÊú™‰æÜÁôºÂ±ï„ÄÇ

##### **Small Language Model as Data Prospector for Large Language Model**
2412.09990v1 by Shiwen Ni, Haihong Wu, Di Yang, Qiang Qu, Hamid Alinejad-Rokny, Min Yang

The quality of instruction data directly affects the performance of
fine-tuned Large Language Models (LLMs). Previously, \cite{li2023one} proposed
\texttt{NUGGETS}, which identifies and selects high-quality quality data from a
large dataset by identifying those individual instruction examples that can
significantly improve the performance of different tasks after being learnt as
one-shot instances. In this work, we propose \texttt{SuperNUGGETS}, an improved
variant of \texttt{NUGGETS} optimised for efficiency and performance. Our
\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large
language model (LLM) to filter the data for outstanding one-shot instances and
refines the predefined set of tests. The experimental results show that the
performance of \texttt{SuperNUGGETS} only decreases by 1-2% compared to
\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.
Compared to the original \texttt{NUGGETS}, our \texttt{SuperNUGGETS} has a
higher utility value due to the significantly lower resource consumption.

ÊëòË¶ÅÔºöÊïôÂ≠∏Ë≥áÊñôÁöÑÂìÅË≥™Áõ¥Êé•ÂΩ±ÈüøÂæÆË™øÂæåÁöÑÂ∑®ÈáèË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊïàËÉΩ„ÄÇÂÖàÂâçÔºå\cite{li2023one} ÊèêÂá∫ \texttt{NUGGETS}ÔºåÂÆÉÊúÉÂæûÂ§ßÂûãË≥áÊñôÈõÜ‰∏≠Ë≠òÂà•‰∏¶ÈÅ∏ÂèñÈ´òÂìÅË≥™Ë≥áÊñôÔºåÊñπÊ≥ïÊòØÊâæÂá∫ÈÇ£‰∫õÂÄãÂà•ÊïôÂ≠∏ÁØÑ‰æãÔºåÂú®Â≠∏ÁøíÁÇ∫‰∏ÄÊ¨°ÊÄßÂØ¶‰æãÂæåÔºåËÉΩÂ§ßÂπÖÊèêÂçá‰∏çÂêå‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ \texttt{SuperNUGGETS}ÔºåÈÄôÊòØ \texttt{NUGGETS} ÁöÑÊîπËâØÁâàÊú¨ÔºåÈáùÂ∞çÊïàÁéáÂíåÊïàËÉΩÈÄ≤Ë°å‰∫ÜÊúÄ‰Ω≥Âåñ„ÄÇÊàëÂÄëÁöÑ \texttt{SuperNUGGETS} ‰ΩøÁî®Â∞èÂûãË™ûË®ÄÊ®°Âûã (SLM)ÔºåËÄåÈùûÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æÜÈÅéÊøæË≥áÊñô‰ª•ÊâæÂá∫ÂÇëÂá∫ÁöÑÂñÆÊ¨°ÂØ¶‰æãÔºå‰∏¶ÊîπÂñÑÈ†êÂÖàÂÆöÁæ©ÁöÑÊ∏¨Ë©¶ÈõÜ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåËàá \texttt{NUGGETS} Áõ∏ÊØîÔºå\texttt{SuperNUGGETS} ÁöÑÊïàËÉΩÂÉÖ‰∏ãÈôç 1-2%Ôºå‰ΩÜÊïàÁéáÂçªËÉΩÊèêÂçá 58 ÂÄç„ÄÇËàáÂéüÂßã \texttt{NUGGETS} Áõ∏ÊØîÔºåÊàëÂÄëÁöÑ \texttt{SuperNUGGETS} ÂÖ∑ÊúâÊõ¥È´òÁöÑÂØ¶Áî®ÂÉπÂÄºÔºåÂõ†ÁÇ∫Ë≥áÊ∫êÊ∂àËÄóÈ°ØËëóÈôç‰Ωé„ÄÇ

##### **AI and the Future of Digital Public Squares**
2412.09988v1 by Beth Goldberg, Diana Acosta-Navas, Michiel Bakker, Ian Beacock, Matt Botvinick, Prateek Buch, Ren√©e DiResta, Nandika Donthi, Nathanael Fast, Ravi Iyer, Zaria Jalan, Andrew Konya, Grace Kwak Danciu, H√©l√®ne Landemore, Alice Marwick, Carl Miller, Aviv Ovadya, Emily Saltz, Lisa Schirch, Dalit Shalom, Divya Siddarth, Felix Sieker, Christopher Small, Jonathan Stray, Audrey Tang, Michael Henry Tessler, Amy Zhang

Two substantial technological advances have reshaped the public square in
recent decades: first with the advent of the internet and second with the
recent introduction of large language models (LLMs). LLMs offer opportunities
for a paradigm shift towards more decentralized, participatory online spaces
that can be used to facilitate deliberative dialogues at scale, but also create
risks of exacerbating societal schisms. Here, we explore four applications of
LLMs to improve digital public squares: collective dialogue systems, bridging
systems, community moderation, and proof-of-humanity systems. Building on the
input from over 70 civil society experts and technologists, we argue that LLMs
both afford promising opportunities to shift the paradigm for conversations at
scale and pose distinct risks for digital public squares. We lay out an agenda
for future research and investments in AI that will strengthen digital public
squares and safeguard against potential misuses of AI.

ÊëòË¶ÅÔºöËøëÂçÅÂπ¥‰æÜÔºåÂÖ©È†ÖÈáçÂ§ßÁöÑÊäÄË°ìÈÄ≤Ê≠•ÈáçÂ°ë‰∫ÜÂÖ¨ÂÖ±È†òÂüüÔºöÈ¶ñÂÖàÊòØÁ∂≤ÈöõÁ∂≤Ë∑ØÁöÑÂá∫ÁèæÔºåÂÖ∂Ê¨°ÊòØÊúÄËøëÂºïÂÖ•ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇLLM ÁÇ∫ËΩâÂêëÊõ¥ÂàÜÊï£„ÄÅÊõ¥ÂÖ∑ÂèÉËàáÊÄßÁöÑÁ∑ö‰∏äÁ©∫ÈñìÊèê‰æõ‰∫ÜÊ©üÊúÉÔºåÂèØË¢´Áî®Êñº‰øÉÈÄ≤Â§ßË¶èÊ®°ÁöÑÂØ©Ë≠∞Â∞çË©±Ôºå‰ΩÜ‰πüÁî¢Áîü‰∫ÜÂä†ÂäáÁ§æÊúÉÂàÜË£ÇÁöÑÈ¢®Èö™„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®ÊîπÂñÑÊï∏‰ΩçÂÖ¨ÂÖ±È†òÂüüÁöÑÂõõÁ®ÆÊáâÁî®ÔºöÈõÜÈ´îÂ∞çË©±Á≥ªÁµ±„ÄÅÊ©ãÊé•Á≥ªÁµ±„ÄÅÁ§æÁæ§ÂØ©Ê†∏Âíå‰∫∫È°ûË≠âÊòéÁ≥ªÁµ±„ÄÇÊ†πÊìö 70 Â§ö‰ΩçÂÖ¨Ê∞ëÁ§æÊúÉÂ∞àÂÆ∂ÂíåÊäÄË°ì‰∫∫Âì°ÁöÑÊÑèË¶ãÔºåÊàëÂÄëË™çÁÇ∫ LLM Êó¢Êèê‰æõ‰∫ÜËΩâËÆäÂ§ßË¶èÊ®°Â∞çË©±ÂÖ∏ÁØÑÁöÑÊúâÂ∏åÊúõÊ©üÊúÉÔºå‰πüÂ∞çÊï∏‰ΩçÂÖ¨ÂÖ±È†òÂüüÊßãÊàêÊòéÁ¢∫ÁöÑÈ¢®Èö™„ÄÇÊàëÂÄëÂà∂ÂÆö‰∫Ü‰∏ÄÈ†ÖÊú™‰æÜÁ†îÁ©∂Âíå AI ÊäïË≥áË≠∞Á®ãÔºåÂ∞áÂº∑ÂåñÊï∏‰ΩçÂÖ¨ÂÖ±È†òÂüü‰∏¶Èò≤Ê≠¢ AI ÈÅ≠Âà∞ÊΩõÂú®ÁöÑË™§Áî®„ÄÇ

##### **Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective**
2412.09972v1 by Yuchen Fang, Yuxuan Liang, Bo Hui, Zezhi Shao, Liwei Deng, Xu Liu, Xinke Jiang, Kai Zheng

Road traffic forecasting is crucial in real-world intelligent transportation
scenarios like traffic dispatching and path planning in city management and
personal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as
the mainstream solution in this task. Nevertheless, the quadratic complexity of
remarkable dynamic spatial modeling-based STGNNs has become the bottleneck over
large-scale traffic data. From the spatial data management perspective, we
present a novel Transformer framework called PatchSTG to efficiently and
dynamically model spatial dependencies for large-scale traffic forecasting with
interpretability and fidelity. Specifically, we design a novel irregular
spatial patching to reduce the number of points involved in the dynamic
calculation of Transformer. The irregular spatial patching first utilizes the
leaf K-dimensional tree (KDTree) to recursively partition irregularly
distributed traffic points into leaf nodes with a small capacity, and then
merges leaf nodes belonging to the same subtree into occupancy-equaled and
non-overlapped patches through padding and backtracking. Based on the patched
data, depth and breadth attention are used interchangeably in the encoder to
dynamically learn local and global spatial knowledge from points in a patch and
points with the same index of patches. Experimental results on four real world
large-scale traffic datasets show that our PatchSTG achieves train speed and
memory utilization improvements up to $10\times$ and $4\times$ with the
state-of-the-art performance.

ÊëòË¶ÅÔºöÈÅìË∑Ø‰∫§ÈÄöÈ†êÊ∏¨Âú®ÂØ¶ÈöõÁöÑÊô∫ÊÖßÈÅãËº∏Â†¥ÊôØ‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºå‰æãÂ¶ÇÂüéÂ∏ÇÁÆ°ÁêÜÂíåÂÄã‰∫∫ÊóÖË°å‰∏≠ÁöÑ‰∫§ÈÄöË™øÂ∫¶ÂíåË∑ØÂæëË¶èÂäÉ„ÄÇÊôÇÁ©∫ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (STGNN) ÊàêÁÇ∫ÈÄôÈ†Ö‰ªªÂãôÁöÑ‰∏ªÊµÅËß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÂü∫ÊñºÈ°ØËëóÂãïÊÖãÁ©∫ÈñìÂª∫Ê®°ÁöÑ STGNN ÁöÑ‰∫åÊ¨°Ë§áÈõúÂ∫¶Â∑≤ÊàêÁÇ∫Â§ßË¶èÊ®°‰∫§ÈÄöÊï∏ÊìöÁöÑÁì∂È†∏„ÄÇÂæûÁ©∫ÈñìÊï∏ÊìöÁÆ°ÁêÜÁöÑËßíÂ∫¶‰æÜÁúãÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ PatchSTG ÁöÑÊñ∞Âûã Transformer Ê°ÜÊû∂Ôºå‰ª•‰æøÊúâÊïà‰∏îÂãïÊÖãÂú∞Â∞çÂ§ßË¶èÊ®°‰∫§ÈÄöÈ†êÊ∏¨Âª∫Ê®°Á©∫Èñì‰æùË≥¥ÊÄßÔºåÂêåÊôÇÂÖ∑ÂÇôÂèØËß£ÈáãÊÄßÂíå‰øùÁúüÂ∫¶„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑ‰∏çË¶èÂâáÁ©∫Èñì‰øÆË£úÔºå‰ª•Ê∏õÂ∞ë Transformer ÂãïÊÖãË®àÁÆó‰∏≠Ê∂âÂèäÁöÑÈªûÊï∏„ÄÇ‰∏çË¶èÂâáÁ©∫Èñì‰øÆË£úÈ¶ñÂÖàÂà©Áî®Ëëâ K Á∂≠Ê®π (KDTree) Â∞á‰∏çË¶èÂâáÂàÜ‰ΩàÁöÑ‰∫§ÈÄöÈªûÈÅûËø¥ÂàÜÂâ≤ÊàêÂÆπÈáèÂ∞èÁöÑËëâÁØÄÈªûÔºåÁÑ∂ÂæåÈÄèÈÅéÂ°´ÂÖÖÂíåÂõûÊ∫ØÂ∞áÂ±¨ÊñºÁõ∏ÂêåÂ≠êÊ®πÁöÑËëâÁØÄÈªûÂêà‰ΩµÊàê‰ΩîÁî®Áõ∏Á≠â‰∏î‰∏çÈáçÁñäÁöÑ‰øÆË£úÁ®ãÂºè„ÄÇÂü∫Êñº‰øÆË£úÁöÑÊï∏ÊìöÔºåÊ∑±Â∫¶ÂíåÂª£Â∫¶Ê≥®ÊÑèÂäõÂú®Á∑®Á¢ºÂô®‰∏≠‰∫íÊèõ‰ΩøÁî®Ôºå‰ª•ÂãïÊÖãÂú∞Âæû‰øÆË£úÁ®ãÂºè‰∏≠ÁöÑÈªûÂíåÂÖ∑ÊúâÁõ∏Âêå‰øÆË£úÁ®ãÂºèÁ¥¢ÂºïÁöÑÈªû‰∏≠Â≠∏ÁøíÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁ©∫ÈñìÁü•Ë≠ò„ÄÇÂú®ÂõõÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÂ§ßË¶èÊ®°‰∫§ÈÄöÊï∏ÊìöÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ PatchSTG Âú®Ë®ìÁ∑¥ÈÄüÂ∫¶ÂíåË®òÊÜ∂È´îÂà©Áî®ÁéáÊñπÈù¢ÂàÜÂà•ÊèêÂçá‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÈÅî $10\times$ Âíå $4\times$„ÄÇ

##### **EP-CFG: Energy-Preserving Classifier-Free Guidance**
2412.09966v1 by Kai Zhang, Fujun Luan, Sai Bi, Jianming Zhang

Classifier-free guidance (CFG) is widely used in diffusion models but often
introduces over-contrast and over-saturation artifacts at higher guidance
strengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),
which addresses these issues by preserving the energy distribution of the
conditional prediction during the guidance process. Our method simply rescales
the energy of the guided output to match that of the conditional prediction at
each denoising step, with an optional robust variant for improved artifact
suppression. Through experiments, we show that EP-CFG maintains natural image
quality and preserves details across guidance strengths while retaining CFG's
semantic alignment benefits, all with minimal computational overhead.

ÊëòË¶ÅÔºöÁÑ°ÂàÜÈ°ûÂô®ÂºïÂ∞é (CFG) Âª£Ê≥õÁî®ÊñºÊì¥Êï£Ê®°ÂûãÔºå‰ΩÜÈÄöÂ∏∏ÊúÉÂú®ËºÉÈ´òÁöÑÂºïÂ∞éÂº∑Â∫¶‰∏ãÂºïÂÖ•ÈÅéÂ∫¶Â∞çÊØîÂíåÈÅéÂ∫¶È£ΩÂíåÁöÑÂÅΩÂÉè„ÄÇÊàëÂÄëÊèêÂá∫ EP-CFGÔºàËÉΩÈáè‰øùÁïôÁÑ°ÂàÜÈ°ûÂô®ÂºïÂ∞éÔºâÔºåÂÆÉÈÄèÈÅéÂú®ÂºïÂ∞éÈÅéÁ®ã‰∏≠‰øùÁïôÊ¢ù‰ª∂È†êÊ∏¨ÁöÑËÉΩÈáèÂàÜ‰Ωà‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÉÖÂÉÖÂú®ÊØèÂÄãÂéªÂô™Ê≠•È©ü‰∏≠ÈáçÊñ∞Ë™øÊï¥ÂºïÂ∞éËº∏Âá∫ÁöÑËÉΩÈáèÔºå‰ª•ÂåπÈÖçÊ¢ù‰ª∂È†êÊ∏¨ÁöÑËÉΩÈáèÔºå‰∏¶Êèê‰æõ‰∏ÄÂÄãÂèØÈÅ∏ÁöÑÁ©©ÂÅ•ËÆäÈ´î‰ª•ÊîπÂñÑÂÅΩÂÉèÊäëÂà∂„ÄÇÈÄèÈÅéÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé EP-CFG ËÉΩÂ§†Âú®‰øùÊåÅ CFG ÁöÑË™ûÁæ©Â∞çÈΩäÂÑ™Âã¢ÁöÑÂêåÊôÇÔºåÂú®ÊâÄÊúâÂºïÂ∞éÂº∑Â∫¶‰∏ãÁ∂≠ÊåÅËá™ÁÑ∂ÂΩ±ÂÉèÂìÅË≥™Âíå‰øùÁïôÁ¥∞ÁØÄÔºå‰∏îË®àÁÆóÈáèÈñãÈä∑Ê•µÂ∞è„ÄÇ

##### **Romanized to Native Malayalam Script Transliteration Using an Encoder-Decoder Framework**
2412.09957v1 by Bajiyo Baiju, Kavya Manohar, Leena G Pillai, Elizabeth Sherly

In this work, we present the development of a reverse transliteration model
to convert romanized Malayalam to native script using an encoder-decoder
framework built with attention-based bidirectional Long Short Term Memory
(Bi-LSTM) architecture. To train the model, we have used curated and combined
collection of 4.3 million transliteration pairs derived from publicly available
Indic language translitertion datasets, Dakshina and Aksharantar. We evaluated
the model on two different test dataset provided by IndoNLP-2025-Shared-Task
that contain, (1) General typing patterns and (2) Adhoc typing patterns,
respectively. On the Test Set-1, we obtained a character error rate (CER) of
7.4%. However upon Test Set-2, with adhoc typing patterns, where most vowel
indicators are missing, our model gave a CER of 22.7%.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÂÄãÂèçËΩâÈü≥Ë≠ØÊ®°ÂûãÁöÑÈñãÁôºÔºå‰ΩøÁî®‰∏ÄÂÄãÂª∫ÊßãÊñºÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÈõôÂêëÈï∑Áü≠ÊúüË®òÊÜ∂ (Bi-LSTM) Êû∂ÊßãÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ê°ÜÊû∂ÔºåÂ∞áÁæÖÈ¶¨ÂåñÁöÑÈ¶¨ÊãâÈõÖÊãâÂßÜË™ûËΩâÊèõÊàêÂéüÁîüÊñáÂ≠ó„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥Ê®°ÂûãÔºåÊàëÂÄë‰ΩøÁî®‰∫Ü‰∏ÄÂÄãÁî±ÂÖ¨ÈñãÁöÑÂç∞Â∫¶Ë™ûË®ÄËΩâÂØ´Ë≥áÊñôÈõÜ Dakshina Âíå Aksharantar ‰∏≠Ë°çÁîüÁöÑ 430 Ëê¨ÂÄãËΩâÂØ´Â∞çÁöÑÁ≤æÈÅ∏ÂíåÁµÑÂêàÈõÜÂêà„ÄÇÊàëÂÄëÂú® IndoNLP-2025-Shared-Task Êèê‰æõÁöÑÂÖ©ÂÄã‰∏çÂêåÁöÑÊ∏¨Ë©¶Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊ®°ÂûãÔºåÂàÜÂà•ÂåÖÂê´ (1) ‰∏ÄËà¨Ëº∏ÂÖ•Ê®°ÂºèÂíå (2) ÁâπÊÆäËº∏ÂÖ•Ê®°Âºè„ÄÇÂú®Ê∏¨Ë©¶ÈõÜ 1 ‰∏äÔºåÊàëÂÄëÁç≤Âæó‰∫Ü 7.4% ÁöÑÂ≠óÂÖÉÈåØË™§Áéá (CER)„ÄÇÁÑ∂ËÄåÂú®Ê∏¨Ë©¶ÈõÜ 2 ‰∏äÔºåÂú®ÁâπÊÆäËº∏ÂÖ•Ê®°Âºè‰∏≠ÔºåÂ§ßÂ§öÊï∏ÁöÑÂÖÉÈü≥ÊåáÁ§∫Á¨¶ËôüÈÉΩÈÅ∫Â§±ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÁµ¶Âá∫‰∫Ü 22.7% ÁöÑ CER„ÄÇ

##### **Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework**
2412.09946v1 by Qiao Sun, Jiexin Xie, Nanyang Ye, Qinying Gu, Shijie Guo

This paper explores the application of large language models (LLMs) in
nursing and elderly care, focusing on AI-driven patient monitoring and
interaction. We introduce a novel Chinese nursing dataset and implement
incremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to
enhance LLM performance in specialized tasks. Using LangChain, we develop a
dynamic nursing assistant capable of real-time care and personalized
interventions. Experimental results demonstrate significant improvements,
paving the way for AI-driven solutions to meet the growing demands of
healthcare in aging populations.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ë≠∑ÁêÜÂíåËÄÅÂπ¥ÁÖßË≠∑‰∏≠ÁöÑÊáâÁî®ÔºåÈáçÈªûÂú®Êñº AI È©ÖÂãïÁöÑÁóÖ‰∫∫Áõ£ÊéßÂíå‰∫íÂãï„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑ‰∏≠ÊñáË≠∑ÁêÜË≥áÊñôÈõÜÔºå‰∏¶ÂØ¶ÊñΩÂ¢ûÈáèÈ†êË®ìÁ∑¥ (IPT) ÂíåÁõ£Áù£ÂæÆË™ø (SFT) ÊäÄË°ìÔºå‰ª•Â¢ûÂº∑ LLM Âú®Â∞àÊ•≠‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇ‰ΩøÁî® LangChainÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂãïÊÖãË≠∑ÁêÜÂä©ÁêÜÔºåËÉΩÂ§†Êèê‰æõÂç≥ÊôÇÁÖßË≠∑ÂíåÂÄã‰∫∫ÂåñÂπ≤È†êÊé™ÊñΩ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÈ°ØËëóÁöÑÊîπÈÄ≤ÔºåÁÇ∫ AI È©ÖÂãïÁöÑËß£Ê±∫ÊñπÊ°àÈã™Âπ≥‰∫ÜÈÅìË∑ØÔºå‰ª•ÊªøË∂≥ËÄÅÈΩ°Âåñ‰∫∫Âè£Â∞çÈÜ´ÁôÇ‰øùÂÅ•Êó•ÁõäÂ¢ûÈï∑ÁöÑÈúÄÊ±Ç„ÄÇ

##### **Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation**
2412.09922v1 by Yanxu Mao, Peipei Liu, Tiehan Cui, Congying Liu, Datao You

In recent years, text classification methods based on neural networks and
pre-trained models have gained increasing attention and demonstrated excellent
performance. However, these methods still have some limitations in practical
applications: (1) They typically focus only on the matching similarity between
sentences. However, there exists implicit high-value information both within
sentences of the same class and across different classes, which is very crucial
for classification tasks. (2) Existing methods such as pre-trained language
models and graph-based approaches often consume substantial memory for training
and text-graph construction. (3) Although some low-resource methods can achieve
good performance, they often suffer from excessively long processing times. To
address these challenges, we propose a low-resource and fast text
classification model called LFTC. Our approach begins by constructing a
compressor list for each class to fully mine the regularity information within
intra-class data. We then remove redundant information irrelevant to the target
classification to reduce processing time. Finally, we compute the similarity
distance between text pairs for classification. We evaluate LFTC on 9 publicly
available benchmark datasets, and the results demonstrate significant
improvements in performance and processing time, especially under limited
computational and data resources, highlighting its superior advantages.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÂíåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊñáÊú¨ÂàÜÁ±ªÊñπÊ≥ïË∂äÊù•Ë∂äÂèóÂà∞ÂÖ≥Ê≥®ÔºåÂπ∂Ë°®Áé∞Âá∫‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠‰ªçÁÑ∂Â≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÊÄßÔºö(1) ÂÆÉ‰ª¨ÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Âè•Â≠ê‰πãÈó¥ÁöÑÂåπÈÖçÁõ∏‰ººÊÄß„ÄÇÁÑ∂ËÄåÔºåÂêåÁ±ªÂè•Â≠êÂÜÖÈÉ®Âíå‰∏çÂêåÁ±ªÂè•Â≠ê‰πãÈó¥ÈÉΩÂ≠òÂú®ÈöêÂê´ÁöÑÈ´ò‰ª∑ÂÄº‰ø°ÊÅØÔºåËøôÂØπÂàÜÁ±ª‰ªªÂä°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ(2) È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÂíåÂü∫‰∫éÂõæÁöÑÊñπÊ≥ïÁ≠âÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÂÜÖÂ≠òÁî®‰∫éËÆ≠ÁªÉÂíåÊñáÊú¨ÂõæÊûÑÂª∫„ÄÇ(3) ËôΩÁÑ∂‰∏Ä‰∫õ‰ΩéËµÑÊ∫êÊñπÊ≥ïÂèØ‰ª•ËææÂà∞ËâØÂ•ΩÁöÑÊÄßËÉΩÔºå‰ΩÜÂÆÉ‰ª¨ÈÄöÂ∏∏Â§ÑÁêÜÊó∂Èó¥ËøáÈïø„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ΩéËµÑÊ∫ê‰∏îÂø´ÈÄüÁöÑÊñáÊú¨ÂàÜÁ±ªÊ®°ÂûãÔºåÁß∞‰∏∫ LFTC„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈ¶ñÂÖà‰∏∫ÊØè‰∏™Á±ªÂà´ÊûÑÂª∫‰∏Ä‰∏™ÂéãÁº©Âô®ÂàóË°®Ôºå‰ª•ÂÖÖÂàÜÊåñÊéòÁ±ªÂÜÖÊï∞ÊçÆ‰∏≠ÁöÑËßÑÂæãÊÄß‰ø°ÊÅØ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Âà†Èô§‰∏éÁõÆÊ†áÂàÜÁ±ªÊó†ÂÖ≥ÁöÑÂÜó‰Ωô‰ø°ÊÅØÔºå‰ª•ÂáèÂ∞ëÂ§ÑÁêÜÊó∂Èó¥„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆ°ÁÆóÊñáÊú¨ÂØπ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßË∑ùÁ¶ªËøõË°åÂàÜÁ±ª„ÄÇÊàë‰ª¨Âú® 9 ‰∏™ÂÖ¨ÂºÄÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫Ü LFTCÔºåÁªìÊûúË°®ÊòéÂú®ÊúâÈôêÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆËµÑÊ∫ê‰∏ãÔºåÂÖ∂ÊÄßËÉΩÂíåÂ§ÑÁêÜÊó∂Èó¥ÈÉΩÊúâÊòæËëóÊèêÂçáÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂‰ºòË∂äÁöÑ‰ºòÂäø„ÄÇ

##### **B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens**
2412.09919v1 by Zhuqiang Lu, Zhenfei Yin, Mengwei He, Zhihui Wang, Zicheng Liu, Zhiyong Wang, Kun Hu

Recently, Vision Large Language Models (VLLMs) integrated with vision
encoders have shown promising performance in vision understanding. The key of
VLLMs is to encode visual content into sequences of visual tokens, enabling
VLLMs to simultaneously process both visual and textual content. However,
understanding videos, especially long videos, remain a challenge to VLLMs as
the number of visual tokens grows rapidly when encoding videos, resulting in
the risk of exceeding the context window of VLLMs and introducing heavy
computation burden. To restrict the number of visual tokens, existing VLLMs
either: (1) uniformly downsample videos into a fixed number of frames or (2)
reducing the number of visual tokens encoded from each frame. We argue the
former solution neglects the rich temporal cue in videos and the later
overlooks the spatial details in each frame. In this work, we present
Balanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively
leverage task relevant spatio-temporal cues while restricting the number of
visual tokens under the VLLM context window length. At the core of our method,
we devise a text-conditioned adaptive frame selection module to identify frames
relevant to the visual understanding task. The selected frames are then
de-duplicated using a temporal frame token merging technique. The visual tokens
of the selected frames are processed through a spatial token sampling module
and an optional spatial token merging strategy to achieve precise control over
the token count. Experimental results show that B-VLLM is effective in
balancing the number of frames and visual tokens in video understanding,
yielding superior performance on various video understanding benchmarks. Our
code is available at https://github.com/zhuqiangLu/B-VLLM.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÁªìÂêàËßÜËßâÁºñÁ†ÅÂô®ÁöÑËßÜËßâÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (VLLM) Âú®ËßÜËßâÁêÜËß£‰∏≠Â±ïÁé∞‰∫Ü‰ª§‰∫∫Êª°ÊÑèÁöÑÊÄßËÉΩ„ÄÇVLLM ÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ∞ÜËßÜËßâÂÜÖÂÆπÁºñÁ†ÅÊàêËßÜËßâÊ†áËÆ∞Â∫èÂàóÔºåËÆ© VLLM ËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜËßÜËßâÂíåÊñáÊú¨ÂÜÖÂÆπ„ÄÇÁÑ∂ËÄåÔºåÁêÜËß£ËßÜÈ¢ëÔºåÂ∞§ÂÖ∂ÊòØÈïøËßÜÈ¢ëÔºåÂØπ‰∫é VLLM Êù•ËØ¥‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊåëÊàòÔºåÂõ†‰∏∫Âú®ÂØπËßÜÈ¢ëËøõË°åÁºñÁ†ÅÊó∂ÔºåËßÜËßâÊ†áËÆ∞ÁöÑÊï∞Èáè‰ºöËøÖÈÄüÂ¢ûÈïøÔºå‰ªéËÄåÂØºËá¥Ë∂ÖÂá∫ VLLM ÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£Âπ∂Â∏¶Êù•Ê≤âÈáçÁöÑËÆ°ÁÆóË¥üÊãÖÁöÑÈ£éÈô©„ÄÇ‰∏∫‰∫ÜÈôêÂà∂ËßÜËßâÊ†áËÆ∞ÁöÑÊï∞ÈáèÔºåÁé∞ÊúâÁöÑ VLLM Ë¶Å‰πàÔºö(1) Â∞ÜËßÜÈ¢ëÂùáÂåÄÈôçÈááÊ†∑ÊàêÂõ∫ÂÆöÊï∞ÈáèÁöÑÂ∏ßÔºåË¶Å‰πàÔºö(2) ÂáèÂ∞ë‰ªéÊØèÂ∏ßÁºñÁ†ÅÁöÑËßÜËßâÊ†áËÆ∞Êï∞Èáè„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ââç‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àÂøΩÁï•‰∫ÜËßÜÈ¢ë‰∏≠‰∏∞ÂØåÁöÑÊó∂Â∫èÁ∫øÁ¥¢ÔºåËÄåÂêé‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àÂøΩÁï•‰∫ÜÊØèÂ∏ß‰∏≠ÁöÑÁ©∫Èó¥ÁªÜËäÇ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂπ≥Ë°° VLLM (B-VLLM)Ôºö‰∏ÄÁßçÊñ∞È¢ñÁöÑ VLLM Ê°ÜÊû∂ÔºåÊó®Âú®ÊúâÊïàÂà©Áî®‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊó∂Á©∫Á∫øÁ¥¢ÔºåÂêåÊó∂ÈôêÂà∂ VLLM ‰∏ä‰∏ãÊñáÁ™óÂè£ÈïøÂ∫¶‰∏ãÁöÑËßÜËßâÊ†áËÆ∞Êï∞Èáè„ÄÇÂú®Êàë‰ª¨ÁöÑÊñπÊ≥ïÁöÑÊ†∏ÂøÉÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÊñáÊú¨Êù°‰ª∂Ëá™ÈÄÇÂ∫îÂ∏ßÈÄâÊã©Ê®°ÂùóÊù•ËØÜÂà´‰∏éËßÜËßâÁêÜËß£‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂ∏ß„ÄÇÁÑ∂Âêé‰ΩøÁî®Êó∂Â∫èÂ∏ßÊ†áËÆ∞ÂêàÂπ∂ÊäÄÊúØÂØπÈÄâÂÆöÁöÑÂ∏ßËøõË°åÂéªÈáçÂ§ÑÁêÜ„ÄÇÈÄâÂÆöÂ∏ßÁöÑËßÜËßâÊ†áËÆ∞ÈÄöËøáÁ©∫Èó¥Ê†áËÆ∞ÈááÊ†∑Ê®°ÂùóÂíåÂèØÈÄâÁöÑÁ©∫Èó¥Ê†áËÆ∞ÂêàÂπ∂Á≠ñÁï•ËøõË°åÂ§ÑÁêÜÔºå‰ª•ÂÆûÁé∞ÂØπÊ†áËÆ∞ËÆ°Êï∞ÁöÑÁ≤æÁ°ÆÊéßÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåB-VLLM Âú®Âπ≥Ë°°ËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÂ∏ßÊï∞ÂíåËßÜËßâÊ†áËÆ∞Êï∞ÈáèÊñπÈù¢ÊòØÊúâÊïàÁöÑÔºåÂú®ÂêÑÁßçËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜ‰∏ä‰∫ßÁîü‰∫ÜÊõ¥‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂèØÂú® https://github.com/zhuqiangLu/B-VLLM Ëé∑Âæó„ÄÇ</paragraph>

##### **Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning**
2412.09906v1 by Jing Bi, Yuting Wu, Weiwei Xing, Zhenjie Wei

Large language models (LLMs) have demonstrated remarkable performance across
a wide range of tasks. Advances in prompt engineering and fine-tuning
techniques have further enhanced their ability to address complex reasoning
challenges. However, these advanced capabilities are often exclusive to models
exceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning
methods have been explored for smaller models (under 10 billion parameters),
they typically depend on extensive CoT training data, which can introduce
inconsistencies and limit effectiveness in low-data settings. To overcome these
limitations, this paper introduce a new reasoning strategy Solution Guidance
(SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT)
for enhancing the reasoning capabilities of small language models. SG focuses
on problem understanding and decomposition at the semantic and logical levels,
rather than specific computations, which can effectively improve the SLMs'
generalization and reasoning abilities. With only a small amount of SG training
data, SGFT can fine-tune a SLM to produce accurate problem-solving guidances,
which can then be flexibly fed to any SLM as prompts, enabling it to generate
correct answers directly. Experimental results demonstrate that our method
significantly improves the performance of SLMs on various reasoning tasks,
enhancing both their practicality and efficiency within resource-constrained
environments.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Âª£Ê≥õÁöÑ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊèêÁ§∫Â∑•Á®ãÂíåÂæÆË™øÊäÄË°ìÁöÑÈÄ≤Â±ïÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂÆÉÂÄëËôïÁêÜË§áÈõúÊé®ÁêÜÊåëÊà∞ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÈÄ≤ÈöéÂäüËÉΩÈÄöÂ∏∏Â∞àÂ±¨ÊñºË∂ÖÈÅé 1000 ÂÑÑÂÄãÂèÉÊï∏ÁöÑÊ®°Âûã„ÄÇÂÑòÁÆ°Â∑≤ÈáùÂ∞çËºÉÂ∞èÊ®°ÂûãÔºàÂ∞èÊñº 100 ÂÑÑÂÄãÂèÉÊï∏ÔºâÊé¢Á¥¢ÊÄùÁ∂≠Èèà (CoT) ÂæÆË™øÊñπÊ≥ïÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥ÊñºÂ§ßÈáèÁöÑ CoT Ë®ìÁ∑¥Ë≥áÊñôÔºåÈÄôÂèØËÉΩÊúÉÂú®Ë≥áÊñôÈáèÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ãÈÄ†Êàê‰∏ç‰∏ÄËá¥ÊÄß‰∏¶ÈôêÂà∂ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄåËß£Ê±∫ÊñπÊ°àÊåáÂ∞é„Äç(SG) Âíå‰∏ÄÁ®ÆÂç≥ÊèíÂç≥Áî®Ë®ìÁ∑¥ÁØÑ‰æã„ÄåËß£Ê±∫ÊñπÊ°àÊåáÂ∞éÂæÆË™ø„Äç(SGFT)ÔºåÁî®ÊñºÂ¢ûÂº∑Â∞èÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇSG Â∞àÊ≥®ÊñºË™ûÊÑèÂíåÈÇèËºØÂ±§Èù¢ÁöÑÂïèÈ°åÁêÜËß£ÂíåÂàÜËß£ÔºåËÄåÈùûÁâπÂÆöÈÅãÁÆóÔºåÈÄôÂèØ‰ª•ÊúâÊïàÊèêÂçá SLM ÁöÑÊ¶ÇÂåñÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇSGFT ÂÉÖÈúÄÂ∞ëÈáèÁöÑ SG Ë®ìÁ∑¥Ë≥áÊñôÔºåÂ∞±ËÉΩÂæÆË™ø SLM ‰ª•Áî¢ÁîüÊ∫ñÁ¢∫ÁöÑËß£Ê±∫ÂïèÈ°åÊåáÂ∞éÔºåÁÑ∂ÂæåÂèØ‰ª•ÈùàÊ¥ªÂú∞Â∞áÂÖ∂Êèê‰æõÁµ¶‰ªª‰Ωï SLM ‰ΩúÁÇ∫ÊèêÁ§∫Ôºå‰ΩøÂÖ∂ËÉΩÂ§†Áõ¥Êé•Áî¢ÁîüÊ≠£Á¢∫ÁöÑÁ≠îÊ°à„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÊèêÂçá‰∫Ü SLM Âú®ÂêÑÁ®ÆÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºåÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠ÊèêÂçá‰∫ÜÂÖ∂ÂØ¶Áî®ÊÄßÂíåÊïàÁéá„ÄÇ

##### **Analyzing Fairness of Computer Vision and Natural Language Processing Models**
2412.09900v1 by Ahmed Rashed, Abdelkrim Kallich, Mohamed Eltayeb

Machine learning (ML) algorithms play a crucial role in decision making
across diverse fields such as healthcare, finance, education, and law
enforcement. Despite their widespread adoption, these systems raise ethical and
social concerns due to potential biases and fairness issues. This study focuses
on evaluating and improving the fairness of Computer Vision and Natural
Language Processing (NLP) models applied to unstructured datasets, emphasizing
how biased predictions can reinforce existing systemic inequalities. A publicly
available dataset from Kaggle was utilized to simulate a practical scenario for
examining fairness in ML workflows. To address and mitigate biases, the study
employed two leading fairness libraries: Fairlearn by Microsoft, and AIF360 by
IBM. These tools offer comprehensive frameworks for fairness analysis,
including metrics evaluation, result visualization, and bias mitigation
techniques. The research aims to measure bias levels in ML models, compare the
effectiveness of these fairness libraries, and provide actionable
recommendations for practitioners. The results demonstrate that each library
possesses distinct strengths and limitations in evaluating and mitigating
fairness. By systematically analyzing these tools, the study contributes
valuable insights to the growing field of ML fairness, offering practical
guidance for integrating fairness solutions into real world applications. This
research underscores the importance of building more equitable and responsible
machine learning systems.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) ÊºîÁÆóÊ≥ïÂú®ÈÜ´ÁôÇ‰øùÂÅ•„ÄÅÈáëËûç„ÄÅÊïôËÇ≤ÂíåÂü∑Ê≥ïÁ≠â‰∏çÂêåÈ†òÂüüÁöÑÊ±∫Á≠ñÂà∂ÂÆö‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÂÑòÁÆ°ÈÄô‰∫õÁ≥ªÁµ±Ë¢´Âª£Ê≥õÊé°Áî®Ôºå‰ΩÜÁî±ÊñºÊΩõÂú®ÁöÑÂÅèË¶ãÂíåÂÖ¨Âπ≥ÊÄßÂïèÈ°åÔºåÈÄô‰∫õÁ≥ªÁµ±ÂºïÁôº‰∫ÜÂÄ´ÁêÜÂíåÁ§æÊúÉÂïèÈ°å„ÄÇÊú¨Á†îÁ©∂ÈáçÈªûÂú®ÊñºË©ï‰º∞ÂíåÊîπÂñÑÊáâÁî®ÊñºÈùûÁµêÊßãÂåñË≥áÊñôÈõÜÁöÑÈõªËÖ¶Ë¶ñË¶∫ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Ê®°ÂûãÁöÑÂÖ¨Âπ≥ÊÄßÔºåÂº∑Ë™øÊúâÂÅèË¶ãÁöÑÈ†êÊ∏¨Â¶Ç‰ΩïÂä†ÂäáÁèæÊúâÁöÑÁ≥ªÁµ±ÊÄß‰∏çÂπ≥Á≠â„ÄÇÂà©Áî® Kaggle ‰∏äÂÖ¨ÈñãÊèê‰æõÁöÑË≥áÊñôÈõÜ‰æÜÊ®°Êì¨ÂØ¶ÈöõÂ†¥ÊôØÔºå‰ª•Ê™¢Ë¶ñ ML Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÂíåÊ∏õËºïÂÅèË¶ãÔºåÊú¨Á†îÁ©∂Êé°Áî®‰∫ÜÂÖ©ÂÄãÈ†òÂÖàÁöÑÂÖ¨Âπ≥ÊÄßÂáΩÂºèÂ∫´ÔºöMicrosoft ÁöÑ Fairlearn Âíå IBM ÁöÑ AIF360„ÄÇÈÄô‰∫õÂ∑•ÂÖ∑Êèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂÖ¨Âπ≥ÊÄßÂàÜÊûêÊû∂ÊßãÔºåÂåÖÊã¨ÊåáÊ®ôË©ï‰º∞„ÄÅÁµêÊûúË¶ñË¶∫ÂåñÂíåÂÅèË¶ãÁ∑©Ëß£ÊäÄË°ì„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Ë°°Èáè ML Ê®°Âûã‰∏≠ÁöÑÂÅèË¶ãÁ®ãÂ∫¶ÔºåÊØîËºÉÈÄô‰∫õÂÖ¨Âπ≥ÊÄßÂáΩÂºèÂ∫´ÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÁÇ∫ÂæûÊ•≠‰∫∫Âì°Êèê‰æõÂèØË°åÁöÑÂª∫Ë≠∞„ÄÇÁµêÊûúË°®ÊòéÔºåÊØèÂÄãÂáΩÂºèÂ∫´Âú®Ë©ï‰º∞ÂíåÊ∏õËºïÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÈÉΩÂÖ∑Êúâ‰∏çÂêåÁöÑÂÑ™Âã¢ÂíåÈôêÂà∂„ÄÇÈÄèÈÅéÁ≥ªÁµ±ÊÄßÂú∞ÂàÜÊûêÈÄô‰∫õÂ∑•ÂÖ∑ÔºåÊú¨Á†îÁ©∂ÁÇ∫ ML ÂÖ¨Âπ≥ÊÄßÈÄôÂÄã‰∏çÊñ∑ÁôºÂ±ïÁöÑÈ†òÂüüÂÅöÂá∫‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£ÔºåÁÇ∫Â∞áÂÖ¨Âπ≥ÊÄßËß£Ê±∫ÊñπÊ°àÊï¥ÂêàÂà∞ÂØ¶ÈöõÊáâÁî®‰∏≠Êèê‰æõ‰∫ÜÂØ¶Áî®ÁöÑÊåáÂ∞é„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂª∫ÊßãÊõ¥ÂÖ¨Âπ≥„ÄÅÊõ¥Ë≤†Ë≤¨‰ªªÁöÑÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Analyzing Fairness of Classification Machine Learning Model with Structured Dataset**
2412.09896v1 by Ahmed Rashed, Abdelkrim Kallich, Mohamed Eltayeb

Machine learning (ML) algorithms have become integral to decision making in
various domains, including healthcare, finance, education, and law enforcement.
However, concerns about fairness and bias in these systems pose significant
ethical and social challenges. This study investigates the fairness of ML
models applied to structured datasets in classification tasks, highlighting the
potential for biased predictions to perpetuate systemic inequalities. A
publicly available dataset from Kaggle was selected for analysis, offering a
realistic scenario for evaluating fairness in machine learning workflows.
  To assess and mitigate biases, three prominent fairness libraries; Fairlearn
by Microsoft, AIF360 by IBM, and the What If Tool by Google were employed.
These libraries provide robust frameworks for analyzing fairness, offering
tools to evaluate metrics, visualize results, and implement bias mitigation
strategies. The research aims to assess the extent of bias in the ML models,
compare the effectiveness of these libraries, and derive actionable insights
for practitioners.
  The findings reveal that each library has unique strengths and limitations in
fairness evaluation and mitigation. By systematically comparing their
capabilities, this study contributes to the growing field of ML fairness by
providing practical guidance for integrating fairness tools into real world
applications. These insights are intended to support the development of more
equitable machine learning systems.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) ÊºîÁÆóÊ≥ïÂ∑≤ÊàêÁÇ∫ÂêÑÂÄãÈ†òÂüüÔºàÂåÖÊã¨ÈÜ´ÁôÇ‰øùÂÅ•„ÄÅÈáëËûç„ÄÅÊïôËÇ≤ÂíåÂü∑Ê≥ïÔºâÊ±∫Á≠ñ‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ
ÁÑ∂ËÄåÔºåÂ∞çÊñºÈÄô‰∫õÁ≥ªÁµ±‰∏≠ÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑ÆÁöÑÁñëÊÖÆÔºåÂ∏∂‰æÜ‰∫ÜÈáçÂ§ßÁöÑÂÄ´ÁêÜÂíåÁ§æÊúÉÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü ML Ê®°ÂûãÂú®ÂàÜÈ°û‰ªªÂãô‰∏≠ÊáâÁî®ÊñºÁµêÊßãÂåñË≥áÊñôÈõÜÁöÑÂÖ¨Âπ≥ÊÄßÔºåÂº∑Ë™ø‰∫ÜÊúâÂÅèÂ∑ÆÁöÑÈ†êÊ∏¨ÂèØËÉΩ‰ΩøÁ≥ªÁµ±ÊÄß‰∏çÂπ≥Á≠âÊ∞∏Á∫åÂåñÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÈÅ∏Êìá‰∫Ü Kaggle ‰∏ä‰∏ÄÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÂàÜÊûêÔºåÊèê‰æõ‰∫ÜË©ï‰º∞Ê©üÂô®Â≠∏ÁøíÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÂÖ¨Âπ≥ÊÄßÁöÑÂØ¶ÈöõÊÉÖÂ¢É„ÄÇ
ÁÇ∫‰∫ÜË©ï‰º∞ÂíåÊ∏õËºïÂÅèÂ∑ÆÔºåÊàëÂÄë‰ΩøÁî®‰∫Ü‰∏âÂÄãËëóÂêçÁöÑÂÖ¨Âπ≥ÊÄßÂáΩÂºèÂ∫´ÔºöMicrosoft ÁöÑ Fairlearn„ÄÅIBM ÁöÑ AIF360 Âíå Google ÁöÑ What If Tool„ÄÇÈÄô‰∫õÂáΩÂºèÂ∫´Êèê‰æõ‰∫ÜÂº∑Â§ßÁöÑÊ°ÜÊû∂‰æÜÂàÜÊûêÂÖ¨Âπ≥ÊÄßÔºåÊèê‰æõ‰∫ÜÁî®ÊñºË©ï‰º∞ÊåáÊ®ô„ÄÅË¶ñË¶∫ÂåñÁµêÊûúÂíåÂØ¶‰ΩúÂÅèÂ∑ÆÊ∏õÁ∑©Á≠ñÁï•ÁöÑÂ∑•ÂÖ∑„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êó®Âú®Ë©ï‰º∞ ML Ê®°Âûã‰∏≠ÂÅèÂ∑ÆÁöÑÁ®ãÂ∫¶ÔºåÊØîËºÉÈÄô‰∫õÂáΩÂºèÂ∫´ÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÁÇ∫ÂæûÊ•≠‰∫∫Âì°ÂæóÂá∫ÂèØË°åÁöÑË¶ãËß£„ÄÇ
Á†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÊØèÂÄãÂáΩÂºèÂ∫´Âú®ÂÖ¨Âπ≥ÊÄßË©ï‰º∞ÂíåÊ∏õÁ∑©ÊñπÈù¢ÈÉΩÊúâÂÖ∂Áç®ÁâπÁöÑÂÑ™ÈªûÂíåÁº∫Èªû„ÄÇÈÄèÈÅéÁ≥ªÁµ±ÊÄßÂú∞ÊØîËºÉÂÆÉÂÄëÁöÑËÉΩÂäõÔºåÊú¨Á†îÁ©∂ÈÄèÈÅéÊèê‰æõÂ∞áÂÖ¨Âπ≥ÊÄßÂ∑•ÂÖ∑Êï¥ÂêàÂà∞ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÂØ¶ÂãôÊåáÂçóÔºåÁÇ∫ ML ÂÖ¨Âπ≥ÊÄßÈÄôÂÄã‰∏çÊñ∑ÊàêÈï∑ÁöÑÈ†òÂüüÂÅöÂá∫Ë≤¢Áçª„ÄÇÈÄô‰∫õË¶ãËß£Êó®Âú®ÊîØÊè¥ÈñãÁôºÊõ¥ÂÖ¨Âπ≥ÁöÑÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±„ÄÇ

##### **Semi-Periodic Activation for Time Series Classification**
2412.09889v1 by Jos√© Gilberto Barbosa de Medeiros J√∫nior, Andre Guarnier de Mitri, Diego Furtado Silva

This paper investigates the lack of research on activation functions for
neural network models in time series tasks. It highlights the need to identify
essential properties of these activations to improve their effectiveness in
specific domains. To this end, the study comprehensively analyzes properties,
such as bounded, monotonic, nonlinearity, and periodicity, for activation in
time series neural networks. We propose a new activation that maximizes the
coverage of these properties, called LeakySineLU. We empirically evaluate the
LeakySineLU against commonly used activations in the literature using 112
benchmark datasets for time series classification, obtaining the best average
ranking in all comparative scenarios.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÊé¢Ë®éÊôÇÈñìÂ∫èÂàó‰ªªÂãô‰∏≠Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑÊøÄÊ¥ªÂáΩÊï∏Á†îÁ©∂‰∏çË∂≥ÁöÑÂïèÈ°å„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜË≠òÂà•ÈÄô‰∫õÊøÄÊ¥ªÂáΩÊï∏Âü∫Êú¨ÁâπÊÄßÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÂÆÉÂÄëÂú®ÁâπÂÆöÈ†òÂüüÁöÑÊúâÊïàÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂ÂÖ®Èù¢ÂàÜÊûê‰∫ÜÊôÇÈñìÂ∫èÂàóÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÊøÄÊ¥ªÂáΩÊï∏ÁöÑÊÄßË≥™Ôºå‰æãÂ¶ÇÊúâÁïå„ÄÅÂñÆË™ø„ÄÅÈùûÁ∑öÊÄßÂíåÈÄ±ÊúüÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊøÄÊ¥ªÂáΩÊï∏ LeakySineLUÔºåÂÆÉÊúÄÂ§ßÂåñ‰∫ÜÈÄô‰∫õÊÄßË≥™ÁöÑË¶ÜËìãÁØÑÂúç„ÄÇÊàëÂÄë‰ΩøÁî® 112 ÂÄãÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûÂü∫Ê∫ñË≥áÊñôÈõÜÔºåÊ†πÊìöÊñáÁçª‰∏≠Â∏∏Áî®ÁöÑÊøÄÊ¥ªÂáΩÊï∏Â∞ç LeakySineLU ÈÄ≤Ë°åÂØ¶Ë≠âË©ï‰º∞ÔºåÂú®ÊâÄÊúâÊØîËºÉÊÉÖÂ¢É‰∏≠Áç≤ÂæóÊúÄ‰Ω≥Âπ≥ÂùáÊéíÂêç„ÄÇ

##### **CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls**
2412.09887v1 by Li Chai, Donglin Wang

Lyric-to-melody generation is a highly challenging task in the field of AI
music generation. Due to the difficulty of learning strict yet weak
correlations between lyrics and melodies, previous methods have suffered from
weak controllability, low-quality and poorly structured generation. To address
these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody
generation method based on an in-attention Transformer decoder with
fine-grained lyric and musical controls, which is able to generate full-song
melodies matched with the given lyrics and user-specified musical attributes.
Specifically, we first introduce REMI-Aligned, a novel music representation
that incorporates strict syllable- and sentence-level alignments between lyrics
and melodies, facilitating precise alignment modeling. Subsequently,
sentence-level semantic lyric embeddings independently extracted from a
sentence-wise Transformer encoder are combined with word-level part-of-speech
embeddings and syllable-level tone embeddings as fine-grained controls to
enhance the controllability of lyrics over melody generation. Then we introduce
human-labeled musical tags, sentence-level statistical musical attributes, and
learned musical features extracted from a pre-trained VQ-VAE as coarse-grained,
fine-grained and high-fidelity controls, respectively, to the generation
process, thereby enabling user control over melody generation. Finally, an
in-attention Transformer decoder technique is leveraged to exert fine-grained
control over the full-song melody generation with the aforementioned lyric and
musical conditions. Experimental results demonstrate that our proposed CSL-L2M
outperforms the state-of-the-art models, generating melodies with higher
quality, better controllability and enhanced structure. Demos and source code
are available at https://lichaiustc.github.io/CSL-L2M/.

ÊëòË¶ÅÔºöÊ≠åË©ûËΩâÊóãÂæãÁîüÊàêÊòØ AI Èü≥Ê®ÇÁîüÊàêÈ†òÂüü‰∏≠Ê•µÂÖ∑ÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÁî±ÊñºÈõ£‰ª•Â≠∏ÁøíÊ≠åË©ûÂíåÊóãÂæã‰πãÈñìÂö¥Ê†º‰ΩÜËñÑÂº±ÁöÑÁõ∏ÈóúÊÄßÔºå‰ª•ÂâçÁöÑÁîüÊàêÊñπÊ≥ï‰∏ÄÁõ¥È£ΩÂèóÂèØÊéßÊÄßÂ∑Æ„ÄÅÂìÅË≥™‰ΩéËêΩ‰∏îÁµêÊßã‰∏çËâØÁöÑÂõ∞Êìæ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CSL-L2MÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂèØÊéßÁöÑÊ≠åÊõ≤Á¥öÊ≠åË©ûËΩâÊóãÂæãÁîüÊàêÊñπÊ≥ïÔºåÂÆÉÂü∫ÊñºÂ∏∂ÊúâÁ¥∞Á≤íÂ∫¶Ê≠åË©ûÂíåÈü≥Ê®ÇÊéßÂà∂ÁöÑÈùûÊ≥®ÊÑèÂäõ Transformer Ëß£Á¢ºÂô®ÔºåËÉΩÂ§†ÁîüÊàêËàáÁµ¶ÂÆöÊ≠åË©ûÂíå‰ΩøÁî®ËÄÖÊåáÂÆöÁöÑÈü≥Ê®ÇÂ±¨ÊÄßÁõ∏ÂåπÈÖçÁöÑÂÖ®Êõ≤ÊóãÂæã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥π REMI-AlignedÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈü≥Ê®ÇË°®Á§∫ÔºåÂÆÉÁµêÂêà‰∫ÜÊ≠åË©ûÂíåÊóãÂæã‰πãÈñìÂö¥Ê†ºÁöÑÈü≥ÁØÄÂíåÂè•Â≠êÁ¥öÂà•Â∞çÈΩäÔºå‰øÉÈÄ≤‰∫ÜÁ≤æÁ¢∫Â∞çÈΩäÂª∫Ê®°„ÄÇÈö®ÂæåÔºåÂæûÂè•Â≠êÁ¥ö Transformer Á∑®Á¢ºÂô®‰∏≠Áç®Á´ãÊèêÂèñÁöÑÂè•Â≠êÁ¥öË™ûÁæ©Ê≠åË©ûÂµåÂÖ•ËàáË©ûÁ¥öË©ûÊÄßÂµåÂÖ•ÂíåÈü≥ÁØÄÁ¥öÈü≥Ë™øÂµåÂÖ•Áõ∏ÁµêÂêàÔºå‰ΩúÁÇ∫Á¥∞Á≤íÂ∫¶ÊéßÂà∂Ôºå‰ª•Â¢ûÂº∑Ê≠åË©ûÂ∞çÊóãÂæãÁîüÊàêÁöÑÊéßÂà∂Âäõ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂú®ÁîüÊàêÈÅéÁ®ã‰∏≠ÂàÜÂà•Â∞á‰∫∫Â∑•Ê®ôË®òÁöÑÈü≥Ê®ÇÊ®ôÁ±§„ÄÅÂè•Â≠êÁ¥öÁµ±Ë®àÈü≥Ê®ÇÂ±¨ÊÄß‰ª•ÂèäÂæûÈ†êË®ìÁ∑¥ÁöÑ VQ-VAE ‰∏≠ÊèêÂèñÁöÑÂ≠∏ÁøíÈü≥Ê®ÇÁâπÂæµÂºïÂÖ•ÁÇ∫Á≤óÁ≤íÂ∫¶„ÄÅÁ¥∞Á≤íÂ∫¶ÂíåÈ´ò‰øùÁúüÊéßÂà∂ÔºåÂæûËÄå‰ΩøÁî®Êà∂ËÉΩÂ§†ÊéßÂà∂ÊóãÂæãÁîüÊàê„ÄÇÊúÄÂæåÔºåÂà©Áî®ÈùûÊ≥®ÊÑèÂäõ Transformer Ëß£Á¢ºÂô®ÊäÄË°ìÔºåÂ∞çÂÖ®Êõ≤ÊóãÂæãÁîüÊàêÊñΩÂä†Á¥∞Á≤íÂ∫¶ÊéßÂà∂Ôºå‰∏¶ÂÖ∑ÂÇô‰∏äËø∞Ê≠åË©ûÂíåÈü≥Ê®ÇÊ¢ù‰ª∂„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑ CSL-L2M ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÁîüÊàêÁöÑÊóãÂæãÂÖ∑ÊúâÊõ¥È´òÁöÑÂìÅË≥™„ÄÅÊõ¥Â•ΩÁöÑÂèØÊéßÊÄß‰ª•ÂèäÂ¢ûÂº∑ÁöÑÁµêÊßã„ÄÇÂ±ïÁ§∫ÂíåÊ∫ê‰ª£Á¢ºÂèØÂú® https://lichaiustc.github.io/CSL-L2M/ Áç≤Âæó„ÄÇ

##### **Benchmarking Table Comprehension In The Wild**
2412.09884v1 by Yikang Pan, Yi Zhu, Rand Xie, Yizhi Liu

Large Language Models (LLMs), while being increasingly dominant on a myriad
of knowledge-intensive activities, have only had limited success understanding
lengthy table-text mixtures, such as academic papers and financial reports.
Recent advances of long-context LLMs have opened up new possibilities for this
field. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of table
question answering (TableQA) have focused on isolated tables without context,
making it hard to evaluate models in real-world scenarios. (2) Prior benchmarks
have focused on some narrow skill sets of table comprehension such as table
recognition, data manipulation/calculation, table summarization etc., while a
skilled human employs those skills collectively. In this work, we introduce
TableQuest, a new benchmark designed to evaluate the holistic table
comprehension capabilities of LLMs in the natural table-rich context of
financial reports. We employ a rigorous data processing and filtering procedure
to ensure that the question-answer pairs are logical, reasonable, and diverse.
We experiment with 7 state-of-the-art models, and find that despite reasonable
accuracy in locating facts, they often falter when required to execute more
sophisticated reasoning or multi-step calculations. We conclude with a
qualitative study of the failure modes and discuss the challenges of
constructing a challenging benchmark. We make the evaluation data, judging
procedure and results of this study publicly available to facilitate research
in this field.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÑòÁÆ°Âú®ÂêÑÁ®ÆÁü•Ë≠òÂØÜÈõÜÊ¥ªÂãï‰∏≠Ë∂ä‰æÜË∂ä‰Ωî‰∏ªÂ∞éÂú∞‰ΩçÔºå‰ΩÜÂú®ÁêÜËß£ÂÜóÈï∑ÁöÑË°®Ê†ºÊñáÂ≠óÊ∑∑ÂêàÁâ©Ôºà‰æãÂ¶ÇÂ≠∏Ë°ìË´ñÊñáÂíåË≤°ÂãôÂ†±ÂëäÔºâÊñπÈù¢ÂçªÂÉÖÁç≤ÂæóÊúâÈôêÁöÑÊàêÂäü„ÄÇÈï∑Ë™ûÂ¢É LLM ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÁÇ∫ÈÄôÂÄãÈ†òÂüüÈñãÂïü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂÖ©ÂÄãÈöúÁ§ôÔºö(1) ÂÖàÂâçÁöÑË°®Ê†ºÂïèÈ°åÂõûÁ≠î (TableQA) Âü∫Ê∫ñÂÅ¥ÈáçÊñºÊ≤íÊúâË™ûÂ¢ÉÁöÑÂ≠§Á´ãË°®Ê†ºÔºåÈÄô‰ΩøÂæóÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠Ë©ï‰º∞Ê®°ÂûãËÆäÂæóÂõ∞Èõ£„ÄÇ(2) ÂÖàÂâçÁöÑÂü∫Ê∫ñÂÅ¥ÈáçÊñºË°®Ê†ºÁêÜËß£ÁöÑ‰∏Ä‰∫õÁãπÁ™ÑÊäÄËÉΩÔºå‰æãÂ¶ÇË°®Ê†ºË≠òÂà•„ÄÅÊï∏ÊìöËôïÁêÜ/Ë®àÁÆó„ÄÅË°®Ê†ºÊëòË¶ÅÁ≠âÔºåËÄåÁÜüÁ∑¥ÁöÑ‰∫∫Âì°ÊúÉÁ∂úÂêàÈÅãÁî®ÈÄô‰∫õÊäÄËÉΩ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü TableQuestÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºåÊó®Âú®Ë©ï‰º∞ LLM Âú®Ë≤°ÂãôÂ†±ÂëäÁöÑËá™ÁÑ∂Ë°®Ê†ºË±êÂØåË™ûÂ¢É‰∏≠ÁöÑÊï¥È´îË°®Ê†ºÁêÜËß£ËÉΩÂäõ„ÄÇÊàëÂÄëÊé°Áî®Âö¥Ê†ºÁöÑÊï∏ÊìöËôïÁêÜÂíåÈÅéÊøæÁ®ãÂ∫èÔºå‰ª•Á¢∫‰øùÂïèÈ°åÁ≠îÊ°àÂ∞çÂÖ∑ÊúâÈÇèËºØÊÄß„ÄÅÂêàÁêÜÊÄßÂíåÂ§öÊ®£ÊÄß„ÄÇÊàëÂÄë‰ΩøÁî® 7 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÈÄ≤Ë°åÂØ¶È©óÔºåÁôºÁèæÂÑòÁÆ°Âú®ÂÆö‰Ωç‰∫ãÂØ¶ÊñπÈù¢ÂÖ∑ÊúâÂêàÁêÜÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÂÆÉÂÄëÂú®ÈúÄË¶ÅÂü∑Ë°åÊõ¥Ë§áÈõúÁöÑÊé®ÁêÜÊàñÂ§öÊ≠•È©üË®àÁÆóÊôÇÂ∏∏Â∏∏ÊúÉÂ§±Êïó„ÄÇÊàëÂÄë‰ª•Â§±ÊïóÊ®°ÂºèÁöÑÂÆöÊÄßÁ†îÁ©∂‰ΩúÁÇ∫ÁµêË´ñÔºå‰∏¶Ë®éË´ñ‰∫ÜÊßãÂª∫ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂü∫Ê∫ñÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÂÖ¨ÈñãË©ï‰º∞Êï∏Êìö„ÄÅË©ïÂØ©Á®ãÂ∫èÂíåÊú¨Á†îÁ©∂ÁöÑÁµêÊûúÔºå‰ª•‰øÉÈÄ≤Ë©≤È†òÂüüÁöÑÁ†îÁ©∂„ÄÇ

##### **On the Limit of Language Models as Planning Formalizers**
2412.09879v1 by Cassie Huang, Li Zhang

Large Language Models have been shown to fail to create executable and
verifiable plans in grounded environments. An emerging line of work shows
success in using LLM as a formalizer to generate a formal representation (e.g.,
PDDL) of the planning domain, which can be deterministically solved to find a
plan. We systematically evaluate this methodology while bridging some major
gaps. While previous work only generates a partial PDDL representation given
templated and thus unrealistic environment descriptions, we generate the
complete representation given descriptions of various naturalness levels. Among
an array of observations critical to improve LLMs' formal planning ability, we
note that large enough models can effectively formalize descriptions as PDDL,
outperforming those directly generating plans, while being robust to lexical
perturbation. As the descriptions become more natural-sounding, we observe a
decrease in performance and provide detailed error analysis.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∑≤Ë¢´Ë≠âÊòéÁÑ°Ê≥ïÂú®Êé•Âú∞ÁöÑÁí∞Â¢É‰∏≠Âª∫Á´ãÂèØÂü∑Ë°åÂíåÂèØÈ©óË≠âÁöÑË®àÁï´„ÄÇÊñ∞ËààÁöÑÂ∑•‰ΩúË∑ØÁ∑öÈ°ØÁ§∫Ôºå‰ΩøÁî® LLM ‰ΩúÁÇ∫ÂΩ¢ÂºèÂåñÂô®‰æÜÁî¢ÁîüË¶èÂäÉÈ†òÂüüÁöÑÂΩ¢ÂºèÂåñË°®Á§∫Ôºà‰æãÂ¶ÇÔºåPDDLÔºâÊòØÊàêÂäüÁöÑÔºåË©≤Ë°®Á§∫ÂèØ‰ª•Á¢∫ÂÆöÊÄßÂú∞Ê±ÇËß£‰ª•ÊâæÂà∞‰∏ÄÂÄãË®àÁï´„ÄÇÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞Ê≠§ÊñπÊ≥ïÔºåÂêåÊôÇÂΩåÂêà‰∏Ä‰∫õ‰∏ªË¶ÅÂ∑ÆË∑ù„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂ∑•‰ΩúÂè™Áî¢ÁîüÈÉ®ÂàÜ PDDL Ë°®Á§∫ÔºåÁµ¶ÂÆöÊ®°ÊùøÂåñ‰∏î‰∏çÂàáÂØ¶ÈöõÁöÑÁí∞Â¢ÉÊèèËø∞Ôºå‰ΩÜÊàëÂÄëÁî¢Áîü‰∫ÜÁµ¶ÂÆöÂêÑÁ®ÆËá™ÁÑ∂Á®ãÂ∫¶ÊèèËø∞ÁöÑÂÆåÊï¥Ë°®Á§∫„ÄÇÂú®ÊîπÈÄ≤ LLM ÂΩ¢ÂºèÂåñË¶èÂäÉËÉΩÂäõÁöÑÁúæÂ§öËßÄÂØü‰∏≠ÔºåÊàëÂÄëÊ≥®ÊÑèÂà∞Ë∂≥Â§†Â§ßÁöÑÊ®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞Â∞áÊèèËø∞ÂΩ¢ÂºèÂåñÁÇ∫ PDDLÔºåÂÑ™ÊñºÁõ¥Êé•Áî¢ÁîüË®àÁï´ÁöÑÊ®°ÂûãÔºåÂêåÊôÇÂ∞çË©ûÂΩôÊìæÂãïÂÖ∑ÊúâÈ≠ØÊ£íÊÄß„ÄÇÈö®ËëóÊèèËø∞ËÆäÂæóÊõ¥Ëá™ÁÑ∂ÔºåÊàëÂÄëËßÄÂØüÂà∞ÊÄßËÉΩ‰∏ãÈôç‰∏¶Êèê‰æõË©≥Á¥∞ÁöÑÈåØË™§ÂàÜÊûê„ÄÇ

##### **Byte Latent Transformer: Patches Scale Better Than Tokens**
2412.09871v1 by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srinivasan Iyer

We introduce the Byte Latent Transformer (BLT), a new byte-level LLM
architecture that, for the first time, matches tokenization-based LLM
performance at scale with significant improvements in inference efficiency and
robustness. BLT encodes bytes into dynamically sized patches, which serve as
the primary units of computation. Patches are segmented based on the entropy of
the next byte, allocating more compute and model capacity where increased data
complexity demands it. We present the first FLOP controlled scaling study of
byte-level models up to 8B parameters and 4T training bytes. Our results
demonstrate the feasibility of scaling models trained on raw bytes without a
fixed vocabulary. Both training and inference efficiency improve due to
dynamically selecting long patches when data is predictable, along with
qualitative improvements on reasoning and long tail generalization. Overall,
for fixed inference costs, BLT shows significantly better scaling than
tokenization-based models, by simultaneously growing both patch and model size.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π Byte Latent Transformer (BLT)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑ‰ΩçÂÖÉÁµÑÁ¥öÂà• LLM Êû∂ÊßãÔºåÂÆÉÈ¶ñÊ¨°‰ª•È°ØËëóÊèêÂçáÁöÑÊé®Ë´ñÊïàÁéáÂíåÁ©©ÂÅ•ÊÄßÔºåÂåπÈÖç‰∫ÜÂü∫ÊñºÊ®ôË®òÂåñÁöÑ LLM ÁöÑË¶èÊ®°ÂåñÊïàËÉΩ„ÄÇBLT Â∞á‰ΩçÂÖÉÁµÑÁ∑®Á¢ºÊàêÂãïÊÖãÂ§ßÂ∞èÁöÑÂçÄÂ°äÔºå‰ΩúÁÇ∫ÈÅãÁÆóÁöÑ‰∏ªË¶ÅÂñÆ‰Ωç„ÄÇÂçÄÂ°äÊ†πÊìö‰∏ã‰∏ÄÂÄã‰ΩçÂÖÉÁµÑÁöÑÁÜµÈÄ≤Ë°åÂàÜÂâ≤ÔºåÂú®Â¢ûÂä†ÁöÑË≥áÊñôË§áÈõúÂ∫¶ÈúÄË¶ÅÊôÇÔºåÂàÜÈÖçÊõ¥Â§öÈÅãÁÆóÂíåÊ®°ÂûãÂÆπÈáè„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜÁ¨¨‰∏ÄÂÄã FLOP ÊéßÂà∂ÁöÑ‰ΩçÂÖÉÁµÑÁ¥öÂà•Ê®°ÂûãÊì¥ÂÖÖÁ†îÁ©∂ÔºåÂèÉÊï∏È´òÈÅî 8BÔºåË®ìÁ∑¥‰ΩçÂÖÉÁµÑÈ´òÈÅî 4T„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫ÜÂú®Ê≤íÊúâÂõ∫ÂÆöË©ûÂΩôË°®ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊì¥ÂÖÖÂú®ÂéüÂßã‰ΩçÂÖÉÁµÑ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÁöÑÂèØË°åÊÄß„ÄÇÁî±ÊñºÂú®Ë≥áÊñôÂèØÈ†êÊ∏¨ÊôÇÂãïÊÖãÈÅ∏ÊìáÈï∑ÂçÄÂ°äÔºå‰ª•ÂèäÂú®Êé®ÁêÜÂíåÈï∑Â∞æÊ¶ÇÊã¨‰∏äÁöÑË≥™ÂåñÊîπÈÄ≤ÔºåË®ìÁ∑¥ÂíåÊé®Ë´ñÊïàÁéáÈÉΩÊúâÊâÄÊèêÂçá„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÂ∞çÊñºÂõ∫ÂÆöÁöÑÊé®Ë´ñÊàêÊú¨ÔºåBLT ÈÄèÈÅéÂêåÊôÇÂ¢ûÂä†ÂçÄÂ°äÂíåÊ®°ÂûãÂ§ßÂ∞èÔºåÂ±ïÁèæÂá∫ÊØîÂü∫ÊñºÊ®ôË®òÂåñÁöÑÊ®°ÂûãÈ°ØËëóÊõ¥Â•ΩÁöÑÊì¥ÂÖÖ„ÄÇ

##### **Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for Supervised Fine-tuning**
2412.09859v1 by Abraham Atsiwo

The Efficient Market Hypothesis (EMH) highlights the essence of financial
news in stock price movement. Financial news comes in the form of corporate
announcements, news titles, and other forms of digital text. The generation of
insights from financial news can be done with sentiment analysis.
General-purpose language models are too general for sentiment analysis in
finance. Curated labeled data for fine-tuning general-purpose language models
are scare, and existing fine-tuned models for sentiment analysis in finance do
not capture the maximum context width. We hypothesize that using actual and
synthetic data can improve performance. We introduce BertNSP-finance to
concatenate shorter financial sentences into longer financial sentences, and
finbert-lc to determine sentiment from digital text. The results show improved
performance on the accuracy and the f1 score for the financial phrasebank data
with $50\%$ and $100\%$ agreement levels.

ÊëòË¶ÅÔºöÊúâÊïàÂ∏ÇÂ†¥ÂÅáË®≠ (EMH) Âº∑Ë™ø‰∫ÜË≤°Á∂ìÊñ∞ËÅûÂú®ËÇ°ÂÉπËÆäÂãï‰∏≠ÁöÑÁ≤æÈ´ì„ÄÇË≤°Á∂ìÊñ∞ËÅû‰ª•ÂÖ¨Âè∏ÂÖ¨Âëä„ÄÅÊñ∞ËÅûÊ®ôÈ°åÂíåÂÖ∂‰ªñÂΩ¢ÂºèÁöÑÊï∏‰ΩçÊñáÂ≠óÂëàÁèæ„ÄÇÂæûË≤°Á∂ìÊñ∞ËÅû‰∏≠Áî¢ÁîüË¶ãËß£ÂèØ‰ª•‰ΩøÁî®ÊÉÖÁ∑íÂàÜÊûê‰æÜÂÆåÊàê„ÄÇ
ÈÄöÁî®Ë™ûË®ÄÊ®°ÂûãÂ∞çÊñºË≤°Âãô‰∏≠ÁöÑÊÉÖÁ∑íÂàÜÊûê‰æÜË™™ÈÅéÊñºÈÄöÁî®„ÄÇÁî®ÊñºÂæÆË™øÈÄöÁî®Ë™ûË®ÄÊ®°ÂûãÁöÑÁ≤æÈÅ∏Ê®ôÁ±§Êï∏ÊìöÂæàÁ®ÄÂ∞ëÔºåËÄåÁèæÊúâÁöÑÁî®ÊñºË≤°ÂãôÊÉÖÁ∑íÂàÜÊûêÁöÑÂæÆË™øÊ®°ÂûãÁÑ°Ê≥ïÊçïÊçâÊúÄÂ§ßÁöÑ‰∏ä‰∏ãÊñáÂØ¨Â∫¶„ÄÇÊàëÂÄëÂÅáË®≠‰ΩøÁî®ÂØ¶ÈöõÂíåÂêàÊàêÊï∏ÊìöÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩ„ÄÇÊàëÂÄëÂºïÂÖ• BertNSP-finance Â∞áËºÉÁü≠ÁöÑË≤°ÂãôÂè•Â≠ê‰∏≤ËÅØÊàêËºÉÈï∑ÁöÑË≤°ÂãôÂè•Â≠êÔºå‰ª•Âèä finbert-lc ÂæûÊï∏‰ΩçÊñáÂ≠ó‰∏≠Á¢∫ÂÆöÊÉÖÁ∑í„ÄÇÁµêÊûúÈ°ØÁ§∫Âú®Ê∫ñÁ¢∫ÊÄßÂíå f1 ÂàÜÊï∏‰∏äÊîπÈÄ≤‰∫ÜË≤°ÂãôË©ûÂΩôÂ∫´Êï∏ÊìöÁöÑË°®ÁèæÔºåÂêåÊÑèÁ®ãÂ∫¶ÁÇ∫ 50% Âíå 100%„ÄÇ

##### **RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning**
2412.09858v1 by Charles Xu, Qiyang Li, Jianlan Luo, Sergey Levine

Recent advances in robotic foundation models have enabled the development of
generalist policies that can adapt to diverse tasks. While these models show
impressive flexibility, their performance heavily depends on the quality of
their training data. In this work, we propose Reinforcement Learning Distilled
Generalists (RLDG), a method that leverages reinforcement learning to generate
high-quality training data for finetuning generalist policies. Through
extensive real-world experiments on precise manipulation tasks like connector
insertion and assembly, we demonstrate that generalist policies trained with
RL-generated data consistently outperform those trained with human
demonstrations, achieving up to 40% higher success rates while generalizing
better to new tasks. We also provide a detailed analysis that reveals this
performance gain stems from both optimized action distributions and improved
state coverage. Our results suggest that combining task-specific RL with
generalist policy distillation offers a promising approach for developing more
capable and efficient robotic manipulation systems that maintain the
flexibility of foundation models while achieving the performance of specialized
controllers. Videos and code can be found on our project website
https://generalist-distillation.github.io

ÊëòË¶ÅÔºöÊ©üÂô®‰∫∫Âü∫Á§éÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤ËÉΩÈñãÁôºÂá∫ÈÅ©ÊáâÂêÑÁ®Æ‰ªªÂãôÁöÑÈÄöÊâçÁ≠ñÁï•„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊ®°ÂûãÂ±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈùàÊ¥ªÊÄßÔºå‰ΩÜÂÖ∂ÊïàËÉΩÂçªÈ´òÂ∫¶‰æùË≥¥ÊñºË®ìÁ∑¥Ë≥áÊñôÁöÑÂìÅË≥™„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Âº∑ÂåñÂ≠∏ÁøíËí∏È§æÈÄöÊâç (RLDG)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÅãÁî®Âº∑ÂåñÂ≠∏Áøí‰æÜÁî¢ÁîüÈ´òÂìÅË≥™Ë®ìÁ∑¥Ë≥áÊñô‰ª•ÂæÆË™øÈÄöÊâçÁ≠ñÁï•ÁöÑÊñπÊ≥ï„ÄÇÈÄèÈÅéÈáùÂ∞çÁ≤æÊ∫ñÊìç‰Ωú‰ªªÂãôÔºàÂ¶ÇÈÄ£Êé•Âô®ÊèíÂÖ•ÂíåÁµÑË£ùÔºâÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶ÈöõÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰ΩøÁî® RL ÁîüÊàêÁöÑË≥áÊñôË®ìÁ∑¥Âá∫ÁöÑÈÄöÊâçÁ≠ñÁï•ÔºåÂÖ∂ÊïàËÉΩÂßãÁµÇÂÑ™Êñº‰ΩøÁî®‰∫∫È°ûÁ§∫ÁØÑË®ìÁ∑¥Âá∫ÁöÑÁ≠ñÁï•ÔºåÂú®Êé®Âª£Âà∞Êñ∞‰ªªÂãôÊôÇÂèØÂ∞áÊàêÂäüÁéáÊèêÈ´òÂ§öÈÅî 40%„ÄÇÊàëÂÄë‰πüÊèê‰æõË©≥Á¥∞ÂàÜÊûêÔºåÊè≠Á§∫ÈÄôÁ®ÆÊïàËÉΩÊèêÂçá‰æÜËá™ÊñºÊúÄ‰Ω≥ÂåñÁöÑÂãï‰ΩúÂàÜÈÖçÂíåÊîπÂñÑÁöÑÁãÄÊÖãË¶ÜËìãÁéá„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂ∞áÁâπÂÆö‰ªªÂãôÁöÑ RL ËàáÈÄöÊâçÁ≠ñÁï•Ëí∏È§æÁµêÂêàÔºåÁÇ∫ÈñãÁôºÊõ¥Âº∑Â§ß„ÄÅÊõ¥ÊúâÊïàÁéáÁöÑÊ©üÂô®‰∫∫Êìç‰ΩúÁ≥ªÁµ±Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÈÄôÁ®ÆÁ≥ªÁµ±Âú®Á∂≠ÊåÅÂü∫Á§éÊ®°ÂûãÁöÑÈùàÊ¥ªÊÄßÁöÑÂêåÊôÇÔºå‰πüËÉΩÈÅîÂà∞Â∞àÁî®ÊéßÂà∂Âô®ÁöÑÊïàËÉΩ„ÄÇÂèØ‰ª•Âú®ÊàëÂÄëÁöÑÂ∞àÊ°àÁ∂≤Á´ô https://generalist-distillation.github.io ÊâæÂà∞ÂΩ±ÁâáÂíåÁ®ãÂºèÁ¢º

##### **LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity**
2412.09856v1 by Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, Peter Vajda, Niraj K. Jha, Xiaoliang Dai

Text-to-video generation enhances content creation but is highly
computationally intensive: The computational cost of Diffusion Transformers
(DiTs) scales quadratically in the number of pixels. This makes minute-length
video generation extremely expensive, limiting most existing models to
generating videos of only 10-20 seconds length. We propose a Linear-complexity
text-to-video Generation (LinGen) framework whose cost scales linearly in the
number of pixels. For the first time, LinGen enables high-resolution
minute-length video generation on a single GPU without compromising quality. It
replaces the computationally-dominant and quadratic-complexity block,
self-attention, with a linear-complexity block called MATE, which consists of
an MA-branch and a TE-branch. The MA-branch targets short-to-long-range
correlations, combining a bidirectional Mamba2 block with our token
rearrangement method, Rotary Major Scan, and our review tokens developed for
long video generation. The TE-branch is a novel TEmporal Swin Attention block
that focuses on temporal correlations between adjacent tokens and medium-range
tokens. The MATE block addresses the adjacency preservation issue of Mamba and
improves the consistency of generated videos significantly. Experimental
results show that LinGen outperforms DiT (with a 75.6% win rate) in video
quality with up to 15$\times$ (11.5$\times$) FLOPs (latency) reduction.
Furthermore, both automatic metrics and human evaluation demonstrate our
LinGen-4B yields comparable video quality to state-of-the-art models (with a
50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,
respectively). This paves the way to hour-length movie generation and real-time
interactive video generation. We provide 68s video generation results and more
examples in our project website: https://lineargen.github.io/.

ÊëòË¶ÅÔºöÊñáÊú¨Âà∞ÂΩ±ÁâáÁîüÊàêÊúÉÂ¢ûÂº∑ÂÖßÂÆπÂâµ‰ΩúÔºå‰ΩÜË®àÁÆóÊàêÊú¨Ê•µÈ´òÔºöÊì¥Êï£ÂºèTransformer (DiT) ÁöÑË®àÁÆóÊàêÊú¨ÊúÉÈö®ËëóÂÉèÁ¥†Êï∏Âπ≥ÊñπÂ¢ûÂä†„ÄÇÈÄô‰ΩøÂæóÂΩ±ÁâáÁîüÊàêÊ•µÁÇ∫ÊòÇË≤¥ÔºåÈôêÂà∂Â§ßÂ§öÊï∏ÁèæÊúâÊ®°ÂûãÂè™ËÉΩÁî¢ÁîüÈï∑Â∫¶ÂÉÖ 10-20 ÁßíÁöÑÂΩ±Áâá„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ∑öÊÄßË§áÈõúÂ∫¶ÊñáÊú¨Âà∞ÂΩ±ÁâáÁîüÊàê (LinGen) Êû∂ÊßãÔºåÂÖ∂ÊàêÊú¨ÊúÉÈö®ËëóÂÉèÁ¥†Êï∏Á∑öÊÄßÂ¢ûÂä†„ÄÇLinGen È¶ñÊ¨°Âú®ÂñÆ‰∏Ä GPU ‰∏äÂØ¶ÁèæÈ´òËß£ÊûêÂ∫¶ÂàÜÈêòÈï∑Â∫¶ÂΩ±ÁâáÁîüÊàêÔºå‰∏î‰∏çÊêçÂÆ≥ÂìÅË≥™„ÄÇÂÆÉ‰ΩøÁî®Á∑öÊÄßË§áÈõúÂ∫¶ÂçÄÂ°ä MATE Âèñ‰ª£Ë®àÁÆóÈáèÈæêÂ§ß‰∏îË§áÈõúÂ∫¶ÁÇ∫Âπ≥ÊñπÁöÑËá™Ê≥®ÊÑèÂäõÂçÄÂ°äÔºåMATE ÂåÖÂê´ MA ÂàÜÊîØÂíå TE ÂàÜÊîØ„ÄÇMA ÂàÜÊîØÈéñÂÆöÁü≠Ë∑ùÈõ¢Âà∞Èï∑Ë∑ùÈõ¢ÈóúËÅØÔºåÁµêÂêàÈõôÂêë Mamba2 ÂçÄÂ°äËàáÊàëÂÄëÁöÑ‰ª£Âπ£ÈáçÊñ∞ÊéíÂàóÊñπÊ≥ï„ÄÅÊóãËΩâ‰∏ªÊéÉÊèèÔºå‰ª•ÂèäÊàëÂÄëÈáùÂ∞çÈï∑ÂΩ±ÁâáÁîüÊàêÊâÄÈñãÁôºÁöÑÂØ©Êü•‰ª£Âπ£„ÄÇTE ÂàÜÊîØÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊôÇÂ∫è Swin Ê≥®ÊÑèÂäõÂçÄÂ°äÔºåÂ∞àÊ≥®ÊñºÁõ∏ÈÑ∞‰ª£Âπ£Âíå‰∏≠Ë∑ùÈõ¢‰ª£Âπ£‰πãÈñìÁöÑÊôÇÂ∫èÈóúËÅØ„ÄÇMATE ÂçÄÂ°äËß£Ê±∫‰∫Ü Mamba ÁöÑÈÑ∞Êé•‰øùÁïôÂïèÈ°åÔºå‰∏¶Â§ßÂπÖÊîπÂñÑÁîüÊàêÂΩ±ÁâáÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåLinGen Âú®ÂΩ±ÁâáÂìÅË≥™‰∏äÂÑ™Êñº DiTÔºàÂãùÁéá 75.6%ÔºâÔºå‰∏î FLOPÔºàÂª∂ÈÅ≤ÔºâÊ∏õÂ∞ëÂ§öÈÅî 15 ÂÄçÔºà11.5 ÂÄçÔºâ„ÄÇÊ≠§Â§ñÔºåËá™ÂãïÊåáÊ®ôÂíå‰∫∫ÁÇ∫Ë©ï‰º∞ÈÉΩË≠âÊòéÊàëÂÄëÁöÑ LinGen-4B Áî¢ÁîüËàáÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏Áï∂ÁöÑÂΩ±ÁâáÂìÅË≥™ÔºàËàá Gen-3„ÄÅLumaLabs Âíå Kling Áõ∏ÊØîÔºåÂãùÁéáÂàÜÂà•ÁÇ∫ 50.5%„ÄÅ52.1%„ÄÅ49.1%Ôºâ„ÄÇÈÄôÁÇ∫Â∞èÊôÇÈï∑Â∫¶ÈõªÂΩ±ÁîüÊàêÂíåÂç≥ÊôÇ‰∫íÂãïÂΩ±ÁâáÁîüÊàêÈã™Ë∑Ø„ÄÇÊàëÂÄëÂú®Â∞àÊ°àÁ∂≤Á´ô‰∏≠Êèê‰æõ 68 ÁßíÂΩ±ÁâáÁîüÊàêÁµêÊûúÂíåÊõ¥Â§öÁØÑ‰æãÔºöhttps://lineargen.github.io/„ÄÇ

##### **Learning Structural Causal Models from Ordering: Identifiable Flow Models**
2412.09843v1 by Minh Khoa Le, Kien Do, Truyen Tran

In this study, we address causal inference when only observational data and a
valid causal ordering from the causal graph are available. We introduce a set
of flow models that can recover component-wise, invertible transformation of
exogenous variables. Our flow-based methods offer flexible model design while
maintaining causal consistency regardless of the number of discretization
steps. We propose design improvements that enable simultaneous learning of all
causal mechanisms and reduce abduction and prediction complexity to linear O(n)
relative to the number of layers, independent of the number of causal
variables. Empirically, we demonstrate that our method outperforms previous
state-of-the-art approaches and delivers consistent performance across a wide
range of structural causal models in answering observational, interventional,
and counterfactual questions. Additionally, our method achieves a significant
reduction in computational time compared to existing diffusion-based
techniques, making it practical for large structural causal models.

ÊëòË¶ÅÔºöÂú®ÈÄôÂÄãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂú®Âè™ÊúâËßÄÂØüË≥áÊñôÂíåÂõ†ÊûúÂúñ‰∏≠ÊúâÊïàÁöÑÂõ†ÊûúÈ†ÜÂ∫èÊôÇÔºåÊé¢Ë®éÂõ†ÊûúÊé®Ë´ñ„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÁµÑÊµÅÂãïÊ®°ÂûãÔºåÂèØ‰ª•ÊÅ¢Âæ©Â§ñÁîüËÆäÊï∏ÁöÑÁµÑÊàêÈÉ®ÂàÜ„ÄÅÂèØÈÄÜËΩâÊèõ„ÄÇÊàëÂÄëÁöÑÂü∫ÊñºÊµÅÂãïÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÈùàÊ¥ªÁöÑÊ®°ÂûãË®≠Ë®àÔºåÂêåÊôÇÁÑ°Ë´ñÈõ¢Êï£ÂåñÊ≠•È©üÁöÑÊï∏ÈáèÂ¶Ç‰ΩïÔºåÈÉΩËÉΩÁ∂≠ÊåÅÂõ†Êûú‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜË®≠Ë®àÊîπÈÄ≤ÔºåËÉΩÂ§†ÂêåÊôÇÂ≠∏ÁøíÊâÄÊúâÂõ†ÊûúÊ©üÂà∂Ôºå‰∏¶Â∞áÂ§ñÊé®ÂíåÈ†êÊ∏¨Ë§áÈõúÂ∫¶Èôç‰ΩéÁÇ∫Á∑öÊÄß O(n)ÔºåÁõ∏Â∞çÊñºÂ±§Êï∏ÔºåËàáÂõ†ÊûúËÆäÊï∏ÁöÑÊï∏ÈáèÁÑ°Èóú„ÄÇÊ†πÊìöÁ∂ìÈ©óÔºåÊàëÂÄëË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÔºå‰∏¶‰∏îÂú®ÂõûÁ≠îËßÄÂØü„ÄÅ‰ªãÂÖ•ÂíåÂèç‰∫ãÂØ¶ÂïèÈ°åÊôÇÔºåÂú®ÂêÑÁ®ÆÁµêÊßãÂõ†ÊûúÊ®°Âûã‰∏≠Êèê‰æõ‰∫ÜÁ©©ÂÆöÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåËàáÁèæÊúâÁöÑÂü∫ÊñºÊì¥Êï£ÁöÑÊäÄË°ìÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Ë®àÁÆóÊôÇÈñì‰∏äÂØ¶Áèæ‰∫ÜÈ°ØËëóÁöÑÊ∏õÂ∞ëÔºåÈÄô‰ΩøÂæóÂÆÉÈÅ©Áî®ÊñºÂ§ßÂûãÁµêÊßãÂõ†ÊûúÊ®°Âûã„ÄÇ

##### **Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models**
2412.09827v1 by Changqun Li, Chaofan Ding, Kexin Luan, Xinhan Di

Fine-tuning pre-trained large language models in a parameter-efficient manner
is widely studied for its effectiveness and efficiency. LoRA is one of the most
widely used methods, which assumes that the optimization process is essentially
low dimensional. Although LoRA has demonstrated commendable performance, there
remains a significant performance gap between LoRA and full fine-tuning when
learning new tasks. In this work, we propose Low-Rank Adaptation with
Task-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features
from the perspective of editing neural network representations. To prioritize
task-relevant features, a task-aware filter that selectively extracts valuable
knowledge from hidden representations for the target or current task is
designed. As the experiments on a vareity of datasets including NLU,
commonsense reasoning and mathematical reasoning tasks demonstrates, our method
reduces 33.71% parameters and achieves better performance on a variety of
datasets in comparison with SOTA low-rank methods.

ÊëòË¶ÅÔºö‰ª•ÂèÉÊï∏ÊúâÊïàÁéáÁöÑÊñπÂºèÂæÆË™øÈ†êÂÖàË®ìÁ∑¥ÁöÑÂ§ßË™ûË®ÄÊ®°ÂûãÂõ†ÂÖ∂ÊúâÊïàÊÄßÂíåÊïàÁéáËÄåÂª£ÂèóÁ†îÁ©∂„ÄÇLoRA ÊòØÂÖ∂‰∏≠‰∏ÄÁ®ÆÊúÄÂª£Ê≥õ‰ΩøÁî®ÁöÑÊñπÊ≥ïÔºåÂÆÉÂÅáË®≠ÊúÄ‰Ω≥ÂåñÈÅéÁ®ãÊú¨Ë≥™‰∏äÊòØ‰ΩéÁ∂≠Â∫¶ÁöÑ„ÄÇÂÑòÁÆ° LoRA Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫Á®±ÈÅìÁöÑÊïàËÉΩÔºå‰ΩÜÂú®Â≠∏ÁøíÊñ∞‰ªªÂãôÊôÇÔºåLoRA ÂíåÂÆåÂÖ®ÂæÆË™ø‰πãÈñì‰ªçÂ≠òÂú®È°ØËëóÁöÑÊïàËÉΩÂ∑ÆË∑ù„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÂÖ∑Êúâ‰ªªÂãôÁõ∏ÈóúÁâπÂæµÂ¢ûÂº∑ÁöÑ‰ΩéÁß©ÈÅ©Êáâ (LoRATRF)Ôºå‰ª•Á∑®ËºØÁ•ûÁ∂ìÁ∂≤Ë∑ØË°®Á§∫ÁöÑÊñπÂºèÂ¢ûÂº∑Ëàá‰ªªÂãôÁõ∏ÈóúÁöÑÁâπÂæµ„ÄÇÁÇ∫‰∫ÜÂÑ™ÂÖàËÄÉÊÖÆËàá‰ªªÂãôÁõ∏ÈóúÁöÑÁâπÂæµÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄã‰ªªÂãôÊÑüÁü•ÁØ©ÈÅ∏Âô®ÔºåÁî®ÊñºÊúâÈÅ∏ÊìáÂú∞ÂæûÈö±ËóèË°®Á§∫‰∏≠ÊèêÂèñÊúâÂÉπÂÄºÁöÑÁü•Ë≠òÔºå‰ª•‰æõÁõÆÊ®ôÊàñÁï∂Ââç‰ªªÂãô‰ΩøÁî®„ÄÇÊ≠£Â¶ÇÂú®ÂåÖÂê´ NLU„ÄÅÂ∏∏Ë≠òÊé®ÁêÜÂíåÊï∏Â≠∏Êé®ÁêÜ‰ªªÂãôÂú®ÂÖßÁöÑÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÊâÄÁ§∫ÔºåËàá SOTA ‰ΩéÁß©ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊ∏õÂ∞ë‰∫Ü 33.71% ÁöÑÂèÉÊï∏Ôºå‰∏¶Âú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩ„ÄÇ

##### **Precise Antigen-Antibody Structure Predictions Enhance Antibody Development with HelixFold-Multimer**
2412.09826v1 by Jie Gao, Jing Hu, Lihang Liu, Yang Xue, Kunrui Zhu, Xiaonan Zhang, Xiaomin Fang

The accurate prediction of antigen-antibody structures is essential for
advancing immunology and therapeutic development, as it helps elucidate
molecular interactions that underlie immune responses. Despite recent progress
with deep learning models like AlphaFold and RoseTTAFold, accurately modeling
antigen-antibody complexes remains a challenge due to their unique evolutionary
characteristics. HelixFold-Multimer, a specialized model developed for this
purpose, builds on the framework of AlphaFold-Multimer and demonstrates
improved precision for antigen-antibody structures. HelixFold-Multimer not only
surpasses other models in accuracy but also provides essential insights into
antibody development, enabling more precise identification of binding sites,
improved interaction prediction, and enhanced design of therapeutic antibodies.
These advances underscore HelixFold-Multimer's potential in supporting antibody
research and therapeutic innovation.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫È†êÊ∏¨ÊäóÂéüÊäóÈ´îÁµêÊßãÂ∞çÊñºÂÖçÁñ´Â≠∏ÂíåÊ≤ªÁôÇÈñãÁôºËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÊúâÂä©ÊñºÈó°ÊòéÂÖçÁñ´ÂèçÊáâÁöÑÂü∫Á§éÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®„ÄÇÂÑòÁÆ°ÂÉè AlphaFold Âíå RoseTTAFold Á≠âÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂèñÂæó‰∫ÜÈÄ≤Â±ïÔºå‰ΩÜÁî±ÊñºÊäóÂéüÊäóÈ´îË§áÂêàÁâ©ÁöÑÁç®ÁâπÊºîÂåñÁâπÊÄßÔºåÊ∫ñÁ¢∫Âª∫Ê®°‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊåëÊà∞„ÄÇHelixFold-Multimer ÊòØÁÇ∫Ê≠§ÁõÆÁöÑÈñãÁôºÁöÑÂ∞àÁî®Ê®°ÂûãÔºåÂÆÉÂª∫Á´ãÂú® AlphaFold-Multimer ÁöÑÊ°ÜÊû∂‰πã‰∏äÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊäóÂéüÊäóÈ´îÁµêÊßãÁöÑÊîπÈÄ≤Á≤æÂ∫¶„ÄÇHelixFold-Multimer ‰∏çÂÉÖÂú®Ê∫ñÁ¢∫ÊÄß‰∏äË∂ÖË∂äÂÖ∂‰ªñÊ®°ÂûãÔºåÈÇÑÊèê‰æõ‰∫ÜÂ∞çÊäóÈ´îÈñãÁôºÁöÑÂøÖË¶ÅË¶ãËß£ÔºåÂæûËÄåËÉΩÂ§†Êõ¥Á≤æÁ¢∫Âú∞Ë≠òÂà•ÁµêÂêà‰ΩçÈªû„ÄÅÊîπÈÄ≤‰∫§‰∫í‰ΩúÁî®È†êÊ∏¨‰ª•ÂèäÂ¢ûÂº∑Ê≤ªÁôÇÊÄßÊäóÈ´îÁöÑË®≠Ë®à„ÄÇÈÄô‰∫õÈÄ≤Â±ïÂº∑Ë™ø‰∫Ü HelixFold-Multimer Âú®ÊîØÊåÅÊäóÈ´îÁ†îÁ©∂ÂíåÊ≤ªÁôÇÂâµÊñ∞ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **MERaLiON-AudioLLM: Technical Report**
2412.09818v1 by Yingxu He, Zhuohan Liu, Shuo Sun, Bin Wang, Wenyu Zhang, Xunlong Zou, Nancy F. Chen, Ai Ti Aw

We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning
in One Network), the first speech-text model tailored for Singapore's
multilingual and multicultural landscape. Developed under the National Large
Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates
advanced speech and text processing to address the diverse linguistic nuances
of local accents and dialects, enhancing accessibility and usability in
complex, multilingual environments. Our results demonstrate improvements in
both speech recognition and task-specific understanding, positioning
MERaLiON-AudioLLM as a pioneering solution for region specific AI applications.
We envision this release to set a precedent for future models designed to
address localised linguistic and cultural contexts in a global framework.

ÊëòË¶ÅÔºöÊàëÂÄëÊé®Âá∫ MERaLiON-AudioLLMÔºàÂ§öÊ®°ÊÖãÂêåÁêÜÊé®ÁêÜÂíåÂ≠∏ÁøíÊñº‰∏ÄÈ´îÁ∂≤Ë∑ØÔºâÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÁÇ∫Êñ∞Âä†Âù°ÁöÑÂ§öË™ûË®ÄÂíåÂ§öÂÖÉÊñáÂåñÁí∞Â¢ÉÈáèË∫´ÊâìÈÄ†ÁöÑË™ûÈü≥ÊñáÊú¨Ê®°Âûã„ÄÇÂú®Êñ∞Âä†Âù°ÂúãÂÆ∂Â§ßÂûãË™ûË®ÄÊ®°ÂûãË≥áÂä©Ë®àÁï´‰∏ãÈñãÁôºÁöÑ MERaLiON-AudioLLM Êï¥Âêà‰∫ÜÈÄ≤ÈöéÁöÑË™ûÈü≥ÂíåÊñáÂ≠óËôïÁêÜÔºå‰ª•ÊáâÂ∞çÁï∂Âú∞Âè£Èü≥ÂíåÊñπË®ÄÁöÑÂ§öÊ®£ÂåñË™ûË®ÄÂ∑ÆÁï∞ÔºåÊèêÂçáÂú®Ë§áÈõúÁöÑÂ§öË™ûË®ÄÁí∞Â¢É‰∏≠ÁöÑÂèØÂèäÊÄßÂíåÂèØÁî®ÊÄß„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫ÜË™ûÈü≥Ëæ®Ë≠òÂíåÁâπÂÆö‰ªªÂãôÁêÜËß£ÈÉΩÊúâÊâÄÈÄ≤Ê≠•ÔºåËÆì MERaLiON-AudioLLM ÊàêÁÇ∫ÂçÄÂüüÁâπÂÆö AI ÊáâÁî®ÁöÑ‰∏ÄÈ†ÖÂÖàÈ©ÖËß£Ê±∫ÊñπÊ°à„ÄÇÊàëÂÄëÈ†êË¶ãÊ≠§ÁâàÊú¨Â∞áÁÇ∫Êú™‰æÜÊ®°ÂûãÊ®πÁ´ãÂÖà‰æãÔºåÈÄô‰∫õÊ®°ÂûãÊó®Âú®Âú®ÂÖ®ÁêÉÊû∂Êßã‰∏≠ÊáâÂ∞çÂú®Âú∞ÂåñÁöÑË™ûË®ÄÂíåÊñáÂåñËÉåÊôØ„ÄÇ

##### **Enhancing Multimodal Large Language Models Complex Reason via Similarity Computation**
2412.09817v1 by Xiaofeng Zhang, Fanshuo Zeng, Yihao Quan, Zheng Hui, Jiawei Yao

Multimodal large language models have experienced rapid growth, and numerous
different models have emerged. The interpretability of LVLMs remains an
under-explored area. Especially when faced with more complex tasks such as
chain-of-thought reasoning, its internal mechanisms still resemble a black box
that is difficult to decipher. By studying the interaction and information flow
between images and text, we noticed that in models such as LLaVA1.5, image
tokens that are semantically related to text are more likely to have
information flow convergence in the LLM decoding layer, and these image tokens
receive higher attention scores. However, those image tokens that are less
relevant to the text do not have information flow convergence, and they only
get very small attention scores. To efficiently utilize the image information,
we propose a new image token reduction method, Simignore, which aims to improve
the complex reasoning ability of LVLMs by computing the similarity between
image and text embeddings and ignoring image tokens that are irrelevant and
unimportant to the text. Through extensive experiments, we demonstrate the
effectiveness of our method for complex reasoning tasks. The paper's source
code can be accessed from \url{https://github.com/FanshuoZeng/Simignore}.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁªèÂéÜ‰∫ÜÂø´ÈÄüÂèëÂ±ïÔºåÂπ∂‰∏îÂá∫Áé∞‰∫ÜËÆ∏Â§ö‰∏çÂêåÁöÑÊ®°Âûã„ÄÇLLVM ÁöÑÂèØËß£ÈáäÊÄß‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Â∞öÊú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÈ¢ÜÂüü„ÄÇÁâπÂà´ÊòØÂΩìÈù¢‰∏¥ÈìæÂºèÊÄùÁª¥Êé®ÁêÜÁ≠âÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Êó∂ÔºåÂÖ∂ÂÜÖÈÉ®Êú∫Âà∂‰ªçÁÑ∂Á±ª‰ºº‰∫é‰∏Ä‰∏™Èöæ‰ª•Á†¥ËØëÁöÑÈªëÂå£Â≠ê„ÄÇÈÄöËøáÁ†îÁ©∂ÂõæÂÉèÂíåÊñáÊú¨‰πãÈó¥ÁöÑ‰∫§‰∫íÂíå‰ø°ÊÅØÊµÅÔºåÊàë‰ª¨Ê≥®ÊÑèÂà∞Âú® LLaVA1.5 Á≠âÊ®°Âûã‰∏≠Ôºå‰∏éÊñáÊú¨ËØ≠‰πâÁõ∏ÂÖ≥ÁöÑÂõæÂÉèÊ†áËÆ∞Êõ¥ÊúâÂèØËÉΩÂú® LLM Ëß£Á†ÅÂ±Ç‰∏≠ÂÖ∑Êúâ‰ø°ÊÅØÊµÅÊî∂ÊïõÔºåÂπ∂‰∏îËøô‰∫õÂõæÂÉèÊ†áËÆ∞Êé•Êî∂Êõ¥È´òÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞„ÄÇÁÑ∂ËÄåÔºåÈÇ£‰∫õ‰∏éÊñáÊú¨Áõ∏ÂÖ≥ÊÄßËæÉ‰ΩéÁöÑÂõæÂÉèÊ†áËÆ∞Ê≤°Êúâ‰ø°ÊÅØÊµÅÊî∂ÊïõÔºåËÄå‰∏îÂÆÉ‰ª¨Âè™Ëé∑ÂæóÈùûÂ∏∏Â∞èÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞„ÄÇ‰∏∫‰∫ÜÊúâÊïàÂà©Áî®ÂõæÂÉè‰ø°ÊÅØÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂÉèÊ†áËÆ∞ÁÆÄÂåñÊñπÊ≥ï SimignoreÔºåÂÖ∂Êó®Âú®ÈÄöËøáËÆ°ÁÆóÂõæÂÉèÂíåÊñáÊú¨ÂµåÂÖ•‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÂπ∂ÂøΩÁï•‰∏éÊñáÊú¨Êó†ÂÖ≥‰∏î‰∏çÈáçË¶ÅÁöÑÂõæÂÉèÊ†áËÆ∞Êù•ÊèêÈ´ò LLVMs ÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂØπÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÁöÑÊúâÊïàÊÄß„ÄÇÊú¨ÊñáÁöÑÊ∫ê‰ª£Á†ÅÂèØ‰ª•‰ªé \url{https://github.com/FanshuoZeng/Simignore} ËÆøÈóÆ„ÄÇ

##### **Temporal Causal Discovery in Dynamic Bayesian Networks Using Federated Learning**
2412.09814v1 by Jianhong Chen, Ying Ma, Xubo Yue

Traditionally, learning the structure of a Dynamic Bayesian Network has been
centralized, with all data pooled in one location. However, in real-world
scenarios, data are often dispersed among multiple parties (e.g., companies,
devices) that aim to collaboratively learn a Dynamic Bayesian Network while
preserving their data privacy and security. In this study, we introduce a
federated learning approach for estimating the structure of a Dynamic Bayesian
Network from data distributed horizontally across different parties. We propose
a distributed structure learning method that leverages continuous optimization
so that only model parameters are exchanged during optimization. Experimental
results on synthetic and real datasets reveal that our method outperforms other
state-of-the-art techniques, particularly when there are many clients with
limited individual sample sizes.

ÊëòË¶ÅÔºöÂÇ≥Áµ±‰∏äÔºåÂ≠∏ÁøíÂãïÊÖãË≤ùÊ∞èÁ∂≤Ë∑ØÁöÑÁµêÊßãÊòØÈõÜ‰∏≠ÂºèÁöÑÔºåÊâÄÊúâË≥áÊñôÈÉΩÈõÜ‰∏≠Âú®‰∏ÄÂÄã‰ΩçÁΩÆ„ÄÇÁÑ∂ËÄåÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠ÔºåË≥áÊñôÈÄöÂ∏∏ÂàÜÊï£Âú®Â§öÂÄãÂèÉËàáËÄÖÔºà‰æãÂ¶ÇÂÖ¨Âè∏„ÄÅË£ùÁΩÆÔºâ‰∏≠Ôºå‰ªñÂÄëÂ∏åÊúõÂú®‰øùÁïôË≥áÊñôÈö±ÁßÅÂíåÂÆâÂÖ®ÊÄßÁöÑÂêåÊôÇÔºåÂçî‰ΩúÂ≠∏ÁøíÂãïÊÖãË≤ùÊ∞èÁ∂≤Ë∑Ø„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆËÅØÈÇ¶Â≠∏ÁøíÊñπÊ≥ïÔºåÁî®Êñº‰º∞Ë®àÊ©´Ë∑®‰∏çÂêåÂèÉËàáËÄÖÁöÑË≥áÊñô‰∏≠ÂãïÊÖãË≤ùÊ∞èÁ∂≤Ë∑ØÁöÑÁµêÊßã„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂàÜ‰ΩàÂºèÁµêÊßãÂ≠∏ÁøíÊñπÊ≥ïÔºåÂà©Áî®ÈÄ£Á∫åÊúÄ‰Ω≥ÂåñÔºåÈÄôÊ®£Âú®ÊúÄ‰Ω≥ÂåñÈÅéÁ®ã‰∏≠Âè™ÊúÉ‰∫§ÊèõÊ®°ÂûãÂèÉÊï∏„ÄÇÂú®ÂêàÊàêÂíåÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÊäÄË°ìÔºåÁâπÂà•ÊòØÂú®ÊúâË®±Â§öÂÆ¢Êà∂‰∏îÂÄãÂà•Ê®£Êú¨Â§ßÂ∞èÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇ

##### **ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic LayerReplace and Selective Rank Compression**
2412.09812v1 by Kai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, Jianke Zhu

Offsite-tuning is a privacy-preserving method for tuning large language
models (LLMs) by sharing a lossy compressed emulator from the LLM owners with
data owners for downstream task tuning. This approach protects the privacy of
both the model and data owners. However, current offsite tuning methods often
suffer from adaptation degradation, high computational costs, and limited
protection strength due to uniformly dropping LLM layers or relying on
expensive knowledge distillation. To address these issues, we propose ScaleOT,
a novel privacy-utility-scalable offsite-tuning framework that effectively
balances privacy and utility. ScaleOT introduces a novel layerwise lossy
compression algorithm that uses reinforcement learning to obtain the importance
of each layer. It employs lightweight networks, termed harmonizers, to replace
the raw LLM layers. By combining important original LLM layers and harmonizers
in different ratios, ScaleOT generates emulators tailored for optimal
performance with various model scales for enhanced privacy protection.
Additionally, we present a rank reduction method to further compress the
original LLM layers, significantly enhancing privacy with negligible impact on
utility. Comprehensive experiments show that ScaleOT can achieve nearly
lossless offsite tuning performance compared with full fine-tuning while
obtaining better model privacy.

ÊëòË¶ÅÔºöÂ†¥Â§ñÂæÆË™øÊòØ‰∏ÄÁ®ÆÈö±ÁßÅ‰øùË≠∑ÊñπÊ≥ïÔºåÁî®ÊñºÈÄèÈÅé LLM ÊâÄÊúâËÄÖËàáË≥áÊñôÊâÄÊúâËÄÖÂàÜ‰∫´ÊúâÊêçÂ§±Â£ìÁ∏ÆÁöÑ LLM Ê®°Êì¨Âô®Ôºå‰ª•ÈÄ≤Ë°å‰∏ãÊ∏∏‰ªªÂãôÂæÆË™øÔºåÈÄ≤ËÄåÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇÊ≠§ÊñπÊ≥ï‰øùË≠∑Ê®°ÂûãÂíåË≥áÊñôÊâÄÊúâËÄÖÁöÑÈö±ÁßÅ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÂ†¥Â§ñÂæÆË™øÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂõ†ÁÇ∫ÈÅ©ÊáâÊÄßÈôç‰Ωé„ÄÅÈ´òÈÅãÁÆóÊàêÊú¨ÂíåÊúâÈôêÁöÑ‰øùË≠∑Âº∑Â∫¶ËÄåÂèóËã¶ÔºåÈÄôÊòØÂõ†ÁÇ∫ÂùáÂãªÂú∞Êç®Ê£Ñ LLM Â±§Êàñ‰æùË≥¥ÊòÇË≤¥ÁöÑÁü•Ë≠òËêÉÂèñ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ ScaleOTÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÈö±ÁßÅÂÖ¨Áî®‰∫ãÊ•≠ÂèØÊì¥ÂÖÖÂ†¥Â§ñÂæÆË™øÊû∂ÊßãÔºåËÉΩÊúâÊïàÂπ≥Ë°°Èö±ÁßÅÂíåÂÖ¨Áî®‰∫ãÊ•≠„ÄÇScaleOT ‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈÄêÂ±§ÊúâÊêçÂ§±Â£ìÁ∏ÆÊºîÁÆóÊ≥ïÔºåË©≤ÊºîÁÆóÊ≥ï‰ΩøÁî®Âº∑ÂåñÂ≠∏Áøí‰æÜÂèñÂæóÊØè‰∏ÄÂ±§ÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆÉÊé°Áî®Á®±ÁÇ∫Ë™øÂíåÂô®ÁöÑËºïÈáèÁ¥öÁ∂≤Ë∑ØÔºå‰ª•Âèñ‰ª£ÂéüÂßãÁöÑ LLM Â±§„ÄÇÈÄèÈÅé‰ª•‰∏çÂêåÁöÑÊØî‰æãÁµêÂêàÈáçË¶ÅÁöÑÂéüÂßã LLM Â±§ÂíåË™øÂíåÂô®ÔºåScaleOT ËÉΩÁî¢ÁîüÈáùÂ∞çÊúÄ‰Ω≥ÊïàËÉΩÈáèË∫´ÊâìÈÄ†ÁöÑÊ®°Êì¨Âô®Ôºå‰∏¶ÂÖ∑ÂÇôÂêÑÁ®ÆÊ®°ÂûãË¶èÊ®°‰ª•Â¢ûÂº∑Èö±ÁßÅ‰øùË≠∑„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁß©Ê¨°Á∞°Á¥ÑÊ≥ïÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Â£ìÁ∏ÆÂéüÂßãÁöÑ LLM Â±§ÔºåÂ§ßÂπÖÊèêÂçáÈö±ÁßÅÔºå‰∏îÂ∞çÂÖ¨Áî®‰∫ãÊ•≠ÁöÑÂΩ±ÈüøÂèØ‰ª•ÂøΩÁï•‰∏çË®à„ÄÇÂÖ®Èù¢ÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåËàáÂÆåÂÖ®ÂæÆË™øÁõ∏ÊØîÔºåScaleOT ËÉΩÂ§†ÈÅîÊàêËøë‰πéÁÑ°ÊêçÂ§±ÁöÑÂ†¥Â§ñÂæÆË™øÊïàËÉΩÔºåÂêåÊôÇÈÇÑËÉΩÂèñÂæóÊõ¥Â•ΩÁöÑÊ®°ÂûãÈö±ÁßÅ„ÄÇ

##### **LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering**
2412.09807v1 by Patrick Sutanto, Joan Santoso

Multiple Choice Question Answering (MCQA) is an important problem with
numerous real-world applications, such as medicine, law, and education. The
high cost of building MCQA datasets makes few-shot learning pivotal in this
domain. While Large Language Models (LLMs) can enable few-shot learning, their
direct application in real-world scenarios is often hindered by their high
computational cost. To address this challenge, we propose a simple yet
effective approach that uses LLMs for data generation and scoring. Our approach
utilizes LLMs to create MCQA data which contains questions and choices, and to
assign probability scores to the generated choices. We then use the generated
data and LLM-assigned scores to finetune a smaller and more efficient
encoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive
experiments on the Massive Multitask Language Understanding (MMLU) benchmark
demonstrate that our method improves accuracy from 28.9% to 39.3%, representing
a gain of over 10% compared to a baseline finetuned directly on 5-shot
examples. This shows the effectiveness of LLM-driven data generation and
knowledge distillation for few-shot MCQA.

ÊëòË¶ÅÔºöÂ§öÈÅ∏È°åÂïèÁ≠î (MCQA) ÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÂïèÈ°åÔºåÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÊúâË®±Â§öÊáâÁî®Ôºå‰æãÂ¶ÇÈÜ´Â≠∏„ÄÅÊ≥ïÂæãÂíåÊïôËÇ≤„ÄÇÂª∫Á´ã MCQA Ë≥áÊñôÈõÜÁöÑÊàêÊú¨ÂæàÈ´òÔºåÈÄô‰ΩøÂæóÂ∞èÊ®£Êú¨Â≠∏ÁøíÂú®ÈÄôÂÄãÈ†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØ‰ª•ÊîØÊè¥Â∞èÊ®£Êú¨Â≠∏ÁøíÔºå‰ΩÜÂÆÉÂÄëÂú®ÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÁöÑÁõ¥Êé•ÊáâÁî®Â∏∏Â∏∏ÂèóÂà∞ÂÖ∂È´òÊòÇÁöÑÈÅãÁÆóÊàêÊú¨ÊâÄÈòªÁ§ô„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî® LLM ‰æÜÈÄ≤Ë°åË≥áÊñôÁî¢ÁîüÂíåË©ïÂàÜ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî® LLM ‰æÜÂª∫Á´ãÂåÖÂê´ÂïèÈ°åÂíåÈÅ∏È†ÖÁöÑ MCQA Ë≥áÊñôÔºå‰∏¶ÁÇ∫Áî¢ÁîüÁöÑÈÅ∏È†ÖÂàÜÈÖçÊ©üÁéáÂàÜÊï∏„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ΩøÁî®Áî¢ÁîüÁöÑË≥áÊñôÂíå LLM ÂàÜÈÖçÁöÑÂàÜÊï∏ÔºåËóâÁî±Âà©Áî®Ëí∏È§æÊêçÂ§±‰æÜÂæÆË™ø‰∏ÄÂÄãÊõ¥Â∞è‰∏îÊõ¥ÊúâÊïàÁéáÁöÑÁ∑®Á¢ºÂô®Â∞àÁî®Ê®°Âûã DeBERTa-v3-base„ÄÇÂú® Massive Multitask Language Understanding (MMLU) Âü∫Ê∫ñ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÂ∞áÊ∫ñÁ¢∫ÁéáÂæû 28.9% ÊèêÂçáËá≥ 39.3%ÔºåËàáÁõ¥Êé•ÂæÆË™øÂú® 5 Ê¨°ÂòóË©¶ÁöÑÁØÑ‰æã‰∏äÁõ∏ÊØîÔºåÂ¢ûÂä†‰∫ÜË∂ÖÈÅé 10%„ÄÇÈÄôÈ°ØÁ§∫‰∫Ü LLM È©ÖÂãïÁöÑË≥áÊñôÁî¢ÁîüÂíåÁü•Ë≠òËí∏È§æÂú®Â∞èÊ®£Êú¨ MCQA ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Universal Inceptive GNNs by Eliminating the Smoothness-generalization Dilemma**
2412.09805v1 by Ming Gu, Zhuonan Zheng, Sheng Zhou, Meihan Liu, Jiawei Chen, Tanyu Qiao, Liangcheng Li, Jiajun Bu

Graph Neural Networks (GNNs) have demonstrated remarkable success in various
domains, such as transaction and social net-works. However, their application
is often hindered by the varyinghomophily levels across different orders of
neighboring nodes, ne-cessitating separate model designs for homophilic and
heterophilicgraphs. In this paper, we aim to develop a unified framework
ca-pable of handling neighborhoods of various orders and homophilylevels.
Through theoretical exploration, we identify a previouslyoverlooked
architectural aspect in multi-hop learning: the cascadedependency, which leads
to asmoothness-generalization dilemma.This dilemma significantly affects the
learning process, especiallyin the context of high-order neighborhoods and
heterophilic graphs.To resolve this issue, we propose an Inceptive Graph Neural
Net-work (IGNN), a universal message-passing framework that replacesthe cascade
dependency with an inceptive architecture. IGNN pro-vides independent
representations for each hop, allowing personal-ized generalization
capabilities, and captures neighborhood-wiserelationships to select appropriate
receptive fields. Extensive ex-periments show that our IGNN outperforms 23
baseline methods,demonstrating superior performance on both homophilic and
het-erophilic graphs, while also scaling efficiently to large graphs.

ÊëòË¶ÅÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∑≤Âú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠Â±ïÁèæÂá∫È°ØËëóÁöÑÊàêÂäüÔºå‰æãÂ¶Ç‰∫§ÊòìÂíåÁ§æ‰∫§Á∂≤Ë∑Ø„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÊáâÁî®ÈÄöÂ∏∏ÊúÉÂèóÂà∞‰∏çÂêåÈ†ÜÂ∫èÈÑ∞Êé•ÁØÄÈªû‰πãÈñì‰∏çÂêåÂêåË≥™ÊÄßÂ±§Á¥öÁöÑÈòªÁ§ôÔºåÂõ†Ê≠§ÈúÄË¶ÅÈáùÂ∞çÂêåË≥™ÊÄßËàáÁï∞Ë≥™ÊÄßÂúñÂΩ¢Ë®≠Ë®à‰∏çÂêåÁöÑÊ®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÈñãÁôº‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÊû∂ÊßãÔºåËÉΩÂ§†ËôïÁêÜ‰∏çÂêåÈ†ÜÂ∫èÂíåÂêåË≥™ÊÄßÂ±§Á¥öÁöÑÈÑ∞Âüü„ÄÇÈÄèÈÅéÁêÜË´ñÊé¢Ë®éÔºåÊàëÂÄëË≠òÂà•Âá∫Â§öË∑≥Â≠∏Áøí‰∏≠ÂÖàÂâçË¢´ÂøΩÁï•ÁöÑÊû∂ÊßãÂ±§Èù¢Ôºö‰∏≤ËÅØ‰æùË≥¥ÊÄßÔºåÈÄôÊúÉÂ∞éËá¥Âπ≥ÊªëÊÄßÊ≥õÂåñÂÖ©Èõ£„ÄÇÊ≠§ÂÖ©Èõ£È°ØËëóÂΩ±ÈüøÂ≠∏ÁøíÈÅéÁ®ãÔºåÁâπÂà•ÊòØÂú®È´òÈöéÈÑ∞ÂüüÂíåÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑±Â∫¶ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (IGNN)Ôºå‰∏ÄÂÄãÈÄöÁî®ÁöÑË®äÊÅØÂÇ≥ÈÅûÊû∂ÊßãÔºå‰ª•Ê∑±Â∫¶Êû∂ÊßãÂèñ‰ª£‰∏≤ËÅØ‰æùË≥¥ÊÄß„ÄÇIGNN ÁÇ∫ÊØèÂÄãË∑≥Ë∫çÊèê‰æõÁç®Á´ãÁöÑË°®Á§∫ÔºåÂÖÅË®±ÂÄã‰∫∫ÂåñÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰∏¶Êì∑ÂèñÈÑ∞ÂüüÊõ¥ÊòéÊô∫ÁöÑÈóú‰øÇ‰ª•ÈÅ∏ÊìáÈÅ©Áï∂ÁöÑÊÑüÂèóÈáé„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑ IGNN ÂÑ™Êñº 23 Á®ÆÂü∫Ê∫ñÊñπÊ≥ïÔºåË≠âÊòé‰∫ÜÂú®ÂêåË≥™ÊÄßÂíåÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏äÈÉΩÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÂêåÊôÇ‰πüËÉΩÊúâÊïàÊì¥ÂÖÖÂà∞Â§ßÂûãÂúñÂΩ¢„ÄÇ

##### **CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object Detection**
2412.09799v1 by Qibo Chen, Weizhong Jin, Jianyue Ge, Mengdi Liu, Yuchao Yan, Jian Jiang, Li Yu, Xuanjiang Guo, Shuchang Li, Jianzhong Chen

Recent research on universal object detection aims to introduce language in a
SoTA closed-set detector and then generalize the open-set concepts by
constructing large-scale (text-region) datasets for training. However, these
methods face two main challenges: (i) how to efficiently use the prior
information in the prompts to genericise objects and (ii) how to reduce
alignment bias in the downstream tasks, both leading to sub-optimal performance
in some scenarios beyond pre-training. To address these challenges, we propose
a strong universal detection foundation model called CP-DETR, which is
competitive in almost all scenarios, with only one pre-training weight.
Specifically, we design an efficient prompt visual hybrid encoder that enhances
the information interaction between prompt and visual through scale-by-scale
and multi-scale fusion modules. Then, the hybrid encoder is facilitated to
fully utilize the prompted information by prompt multi-label loss and auxiliary
detection head. In addition to text prompts, we have designed two practical
concept prompt generation methods, visual prompt and optimized prompt, to
extract abstract concepts through concrete visual examples and stably reduce
alignment bias in downstream tasks. With these effective designs, CP-DETR
demonstrates superior universal detection performance in a broad spectrum of
scenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on
LVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35.
Furthermore, our visual prompt generation method achieves 68.4 AP on COCO val
by interactive detection, and the optimized prompt achieves 73.1 fully-shot AP
on ODinW13.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÈÄöÁî®Áâ©‰ΩìÊ£ÄÊµãÁ†îÁ©∂Êó®Âú®Âú® SoTA Èó≠ÈõÜÊ£ÄÊµãÂô®‰∏≠ÂºïÂÖ•ËØ≠Ë®ÄÔºåÁÑ∂ÂêéÈÄöËøáÊûÑÂª∫Áî®‰∫éËÆ≠ÁªÉÁöÑÂ§ßËßÑÊ®°ÔºàÊñáÊú¨Âå∫ÂüüÔºâÊï∞ÊçÆÈõÜÊù•Ê¶ÇÊã¨ÂºÄÊîæÈõÜÊ¶ÇÂøµ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÔºàiÔºâÂ¶Ç‰ΩïÊúâÊïàÂú∞‰ΩøÁî®ÊèêÁ§∫‰∏≠ÁöÑÂÖàÈ™å‰ø°ÊÅØÊù•Ê≥õÂåñÂØπË±°Ôºå‰ª•ÂèäÔºàiiÔºâÂ¶Ç‰ΩïÂáèÂ∞ë‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÂØπÈΩêÂÅèÂ∑ÆÔºåËøô‰∏§ËÄÖÈÉΩ‰ºöÂØºËá¥Âú®Êüê‰∫õË∂ÖÂá∫È¢ÑËÆ≠ÁªÉÁöÑÂú∫ÊôØ‰∏≠Âá∫Áé∞Ê¨°‰ºòÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ CP-DETR ÁöÑÂº∫Â§ßÁöÑÈÄöÁî®Ê£ÄÊµãÂü∫Á°ÄÊ®°ÂûãÔºåËØ•Ê®°ÂûãÂú®Âá†‰πéÊâÄÊúâÂú∫ÊôØ‰∏≠ÈÉΩÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåÂπ∂‰∏îÂè™Êúâ‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÊùÉÈáç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™È´òÊïàÁöÑÊèêÁ§∫ËßÜËßâÊ∑∑ÂêàÁºñÁ†ÅÂô®ÔºåÂÆÉÈÄöËøáÊåâÊØî‰æãËûçÂêàÊ®°ÂùóÂíåÂ§öÂ∞∫Â∫¶ËûçÂêàÊ®°ÂùóÂ¢ûÂº∫‰∫ÜÊèêÁ§∫ÂíåËßÜËßâ‰πãÈó¥ÁöÑ‰ø°ÊÅØ‰∫§‰∫í„ÄÇÁÑ∂ÂêéÔºåÈÄöËøáÊèêÁ§∫Â§öÊ†áÁ≠æÊçüÂ§±ÂíåËæÖÂä©Ê£ÄÊµãÂ§¥Ôºå‰øÉËøõÊ∑∑ÂêàÁºñÁ†ÅÂô®ÂÖÖÂàÜÂà©Áî®ÊèêÁ§∫‰ø°ÊÅØ„ÄÇÈô§‰∫ÜÊñáÊú¨ÊèêÁ§∫‰πãÂ§ñÔºåÊàë‰ª¨ËøòËÆæËÆ°‰∫Ü‰∏§ÁßçÂÆûÁî®ÁöÑÊ¶ÇÂøµÊèêÁ§∫ÁîüÊàêÊñπÊ≥ïÔºåÂç≥ËßÜËßâÊèêÁ§∫Âíå‰ºòÂåñÊèêÁ§∫Ôºå‰ª•ÈÄöËøáÂÖ∑‰ΩìÁöÑËßÜËßâÁ§∫‰æãÊèêÂèñÊäΩË±°Ê¶ÇÂøµÂπ∂Á®≥ÂÆöÂú∞ÂáèÂ∞ë‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÂØπÈΩêÂÅèÂ∑Æ„ÄÇÈÄöËøáËøô‰∫õÊúâÊïàÁöÑËÆæËÆ°ÔºåCP-DETR Âú®ÂπøÊ≥õÁöÑÂú∫ÊôØ‰∏≠Â±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÈÄöÁî®Ê£ÄÊµãÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÊàë‰ª¨ÁöÑ Swin-T ‰∏ªÂπ≤Ê®°ÂûãÂú® LVIS ‰∏äÂÆûÁé∞‰∫Ü 47.6 ÁöÑÈõ∂Ê¨° APÔºåËÄå Swin-L ‰∏ªÂπ≤Ê®°ÂûãÂú® ODinW35 ‰∏äÂÆûÁé∞‰∫Ü 32.2 ÁöÑÈõ∂Ê¨° AP„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑËßÜËßâÊèêÁ§∫ÁîüÊàêÊñπÊ≥ïÈÄöËøá‰∫§‰∫íÂºèÊ£ÄÊµãÂú® COCO val ‰∏äÂÆûÁé∞‰∫Ü 68.4 APÔºåËÄå‰ºòÂåñÊèêÁ§∫Âú® ODinW13 ‰∏äÂÆûÁé∞‰∫Ü 73.1 ÁöÑÂÖ®Ê¨° AP„ÄÇ</paragraph>

##### **AutoPatent: A Multi-Agent Framework for Automatic Patent Generation**
2412.09796v1 by Qiyao Wang, Shiwen Ni, Huaren Liu, Shule Lu, Guhong Chen, Xi Feng, Chi Wei, Qiang Qu, Hamid Alinejad-Rokny, Yuan Lin, Min Yang

As the capabilities of Large Language Models (LLMs) continue to advance, the
field of patent processing has garnered increased attention within the natural
language processing community. However, the majority of research has been
concentrated on classification tasks, such as patent categorization and
examination, or on short text generation tasks like patent summarization and
patent quizzes. In this paper, we introduce a novel and practical task known as
Draft2Patent, along with its corresponding D2P benchmark, which challenges LLMs
to generate full-length patents averaging 17K tokens based on initial drafts.
Patents present a significant challenge to LLMs due to their specialized
nature, standardized terminology, and extensive length. We propose a
multi-agent framework called AutoPatent which leverages the LLM-based planner
agent, writer agents, and examiner agent with PGTree and RRAG to generate
lengthy, intricate, and high-quality complete patent documents. The
experimental results demonstrate that our AutoPatent framework significantly
enhances the ability to generate comprehensive patents across various LLMs.
Furthermore, we have discovered that patents generated solely with the
AutoPatent framework based on the Qwen2.5-7B model outperform those produced by
larger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,
in both objective metrics and human evaluations. We will make the data and code
available upon acceptance at \url{https://github.com/QiYao-Wang/AutoPatent}.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËÉΩÂäõ‰∏çÊñ∑ÈÄ≤Ê≠•ÔºåÂ∞àÂà©ËôïÁêÜÈ†òÂüüÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ§æÁæ§‰∏≠ÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÂ§ßÈÉ®ÂàÜÁ†îÁ©∂ÈÉΩÈõÜ‰∏≠Âú®ÂàÜÈ°û‰ªªÂãô‰∏äÔºå‰æãÂ¶ÇÂ∞àÂà©ÂàÜÈ°ûÂíåÂØ©Êü•ÔºåÊàñÁ∞°Áü≠ÊñáÂ≠óÁîüÊàê‰ªªÂãôÔºå‰æãÂ¶ÇÂ∞àÂà©ÊëòË¶ÅÂíåÂ∞àÂà©Ê∏¨È©ó„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÈ†ÖÊñ∞Á©é‰∏îÂØ¶Áî®ÁöÑ‰ªªÂãôÔºåÁ®±ÁÇ∫ Draft2PatentÔºå‰ª•ÂèäÂÖ∂Â∞çÊáâÁöÑ D2P Âü∫Ê∫ñÔºåÂÆÉÊåëÊà∞ LLM Ê†πÊìöÂàùÂßãËçâÁ®øÁîüÊàêÂπ≥Âùá 17K ÂÄã‰ª£Á¢ºÁöÑÂÖ®ÊñáÂ∞àÂà©„ÄÇÂ∞àÂà©Áî±ÊñºÂÖ∂Â∞àÊ•≠ÊÄßË≥™„ÄÅÊ®ôÊ∫ñÂåñË°ìË™ûÂíåÁØáÂπÖÂÜóÈï∑ÔºåÂ∞ç LLM ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ AutoPatent ÁöÑÂ§ö‰ª£ÁêÜÊû∂ÊßãÔºåÂÆÉÂà©Áî®Âü∫Êñº LLM ÁöÑË¶èÂäÉÂô®‰ª£ÁêÜ„ÄÅÂØ´‰ΩúËÄÖ‰ª£ÁêÜÂíåÂØ©Êü•Âì°‰ª£ÁêÜËàá PGTree Âíå RRAG ‰æÜÁîüÊàêÂÜóÈï∑„ÄÅË§áÈõú‰∏îÈ´òÂìÅË≥™ÁöÑÂÆåÊï¥Â∞àÂà©Êñá‰ª∂„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ AutoPatent Êû∂ÊßãÈ°ØËëóÂ¢ûÂº∑‰∫ÜË∑®ÂêÑÁ®Æ LLM ÁîüÊàêÁ∂úÂêàÂ∞àÂà©ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæÂÉÖ‰ΩøÁî®Âü∫Êñº Qwen2.5-7B Ê®°ÂûãÁöÑ AutoPatent Êû∂ÊßãÁîüÊàêÁöÑÂ∞àÂà©Âú®ÂÆ¢ËßÄÊåáÊ®ôÂíå‰∫∫È°ûË©ï‰º∞‰∏≠ÈÉΩÂÑ™ÊñºÁî±Êõ¥Â§ß„ÄÅÊõ¥Âº∑Â§ßÁöÑ LLMÔºà‰æãÂ¶Ç GPT-4o„ÄÅQwen2.5-72B Âíå LLAMA3.1-70BÔºâÁî¢ÁîüÁöÑÂ∞àÂà©„ÄÇÊàëÂÄëÂ∞áÂú® \url{https://github.com/QiYao-Wang/AutoPatent} ‰∏äÊé•ÂèóÂæåÊèê‰æõÊï∏ÊìöÂíå‰ª£Á¢º„ÄÇ

##### **Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation**
2412.09770v1 by Jonghyuk Park, Alex Lascarides, Subramanian Ramamoorthy

In this paper, we offer a learning framework in which the agent's knowledge
gaps are overcome through corrective feedback from a teacher whenever the agent
explains its (incorrect) predictions. We test it in a low-resource visual
processing scenario, in which the agent must learn to recognize distinct types
of toy truck. The agent starts the learning process with no ontology about what
types of trucks exist nor which parts they have, and a deficient model for
recognizing those parts from visual input. The teacher's feedback to the
agent's explanations addresses its lack of relevant knowledge in the ontology
via a generic rule (e.g., "dump trucks have dumpers"), whereas an inaccurate
part recognition is corrected by a deictic statement (e.g., "this is not a
dumper"). The learner utilizes this feedback not only to improve its estimate
of the hypothesis space of possible domain ontologies and probability
distributions over them, but also to use those estimates to update its visual
interpretation of the scene. Our experiments demonstrate that teacher-learner
pairs utilizing explanations and corrections are more data-efficient than those
without such a faculty.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂ≠∏ÁøíÊû∂ÊßãÔºåÂÖ∂‰∏≠‰ª£ÁêÜÁöÑÁü•Ë≠òÂ∑ÆË∑ùÊúÉÈÄèÈÅéÊïôÂ∏´Âú®‰ª£ÁêÜËß£ÈáãÂÖ∂ÔºàÈåØË™§ÁöÑÔºâÈ†êÊ∏¨ÊôÇÊèê‰æõÁöÑÁ≥æÊ≠£ÂõûÈ•ã‰æÜÂÖãÊúç„ÄÇÊàëÂÄëÂú®‰ΩéË≥áÊ∫êË¶ñË¶∫ËôïÁêÜÂ†¥ÊôØ‰∏≠Ê∏¨Ë©¶‰∫ÜÂÆÉÔºåÂÖ∂‰∏≠‰ª£ÁêÜÂøÖÈ†àÂ≠∏ÊúÉËæ®Ë≠ò‰∏çÂêåÈ°ûÂûãÁöÑÁé©ÂÖ∑Âç°Ëªä„ÄÇ‰ª£ÁêÜÂæûÂ≠∏ÁøíÈÅéÁ®ã‰∏≠ÈñãÂßãÔºåÊ≤íÊúâÈóúÊñºÂç°ËªäÈ°ûÂûãÊàñÂÖ∂Èõ∂‰ª∂ÁöÑÊú¨È´îË´ñÔºå‰ª•Âèä‰∏ÄÂÄãÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠Ëæ®Ë≠òÈÄô‰∫õÈõ∂‰ª∂ÁöÑÁº∫Èô∑Ê®°Âûã„ÄÇÊïôÂ∏´Â∞ç‰ª£ÁêÜËß£ÈáãÁöÑÂõûÈ•ãÈÄèÈÅé‰∏ÄÂÄãÈÄöÁî®Ë¶èÂâáÔºà‰æãÂ¶ÇÔºå„ÄåËá™Âç∏Âç°ËªäÊúâËá™Âç∏ËªäÊñó„ÄçÔºâ‰æÜËß£Ê±∫ÂÖ∂Âú®Êú¨È´îË´ñ‰∏≠Áº∫‰πèÁõ∏ÈóúÁü•Ë≠òÔºåËÄå‰∏ÄÂÄã‰∏çÊ∫ñÁ¢∫ÁöÑÈõ∂‰ª∂Ëæ®Ë≠òÂâáÊúÉÈÄèÈÅé‰∏ÄÂÄãÊåáÁ§∫ÊÄßÈô≥Ëø∞Ôºà‰æãÂ¶ÇÔºå„ÄåÈÄô‰∏çÊòØËá™Âç∏ËªäÊñó„ÄçÔºâ‰æÜ‰øÆÊ≠£„ÄÇÂ≠∏ÁøíËÄÖÂà©Áî®Ê≠§ÂõûÈ•ã‰∏çÂÉÖÂèØ‰ª•ÊîπÂñÑÂÖ∂Â∞çÂèØËÉΩÁöÑÈ†òÂüüÊú¨È´îË´ñÁöÑÂÅáË®≠Á©∫ÈñìÂíåÂÖ∂‰∏äÊ©üÁéáÂàÜÂ∏ÉÁöÑ‰º∞Ë®àÔºåÈÇÑÂèØ‰ª•Âà©Áî®ÈÄô‰∫õ‰º∞Ë®à‰æÜÊõ¥Êñ∞ÂÖ∂Â∞çÂ†¥ÊôØÁöÑË¶ñË¶∫Ë©ÆÈáã„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåÂà©Áî®Ëß£ÈáãÂíå‰øÆÊ≠£ÁöÑËÄÅÂ∏´ËàáÂ≠∏ÁøíËÄÖÈÖçÂ∞çÊØîÊ≤íÊúâÈÄôÁ®ÆËÉΩÂäõÁöÑÈÖçÂ∞çÊõ¥ÂÖ∑Ë≥áÊñôÊïàÁéá„ÄÇ

##### **Memory Layers at Scale**
2412.09764v1 by Vincent-Pierre Berges, Barlas Oƒüuz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, Gargi Gosh

Memory layers use a trainable key-value lookup mechanism to add extra
parameters to a model without increasing FLOPs. Conceptually, sparsely
activated memory layers complement compute-heavy dense feed-forward layers,
providing dedicated capacity to store and retrieve information cheaply. This
work takes memory layers beyond proof-of-concept, proving their utility at
contemporary scale. On downstream tasks, language models augmented with our
improved memory layer outperform dense models with more than twice the
computation budget, as well as mixture-of-expert models when matched for both
compute and parameters. We find gains are especially pronounced for factual
tasks. We provide a fully parallelizable memory layer implementation,
demonstrating scaling laws with up to 128B memory parameters, pretrained to 1
trillion tokens, comparing to base models with up to 8B parameters.

ÊëòË¶ÅÔºöË®òÊÜ∂Â±§‰ΩøÁî®ÂèØË®ìÁ∑¥ÁöÑÈçµÂÄºÊü•Ë©¢Ê©üÂà∂ÔºåÂú®‰∏çÂ¢ûÂä† FLOP ÁöÑÊÉÖÊ≥Å‰∏ãÁÇ∫Ê®°ÂûãÊñ∞Â¢ûÈ°çÂ§ñÂèÉÊï∏„ÄÇÂæûÊ¶ÇÂøµ‰∏ä‰æÜË™™ÔºåÁ®ÄÁñèÊøÄÊ¥ªË®òÊÜ∂Â±§Ë£úÂÖÖ‰∫ÜË®àÁÆóÂØÜÈõÜÁöÑÁ®†ÂØÜÂâçÈ•ãÂ±§ÔºåÊèê‰æõ‰∫ÜÂ∞àÈñÄÁöÑÂÆπÈáè‰æÜÂªâÂÉπÂÑ≤Â≠òÂíåÊì∑ÂèñË≥áË®ä„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂ∞áË®òÊÜ∂Â±§Â∏∂Èõ¢‰∫ÜÊ¶ÇÂøµÈ©óË≠âÔºåË≠âÊòé‰∫ÜÂÆÉÂÄëÂú®Áï∂‰ª£Ë¶èÊ®°‰∏äÁöÑÊïàÁî®„ÄÇÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠Ôºå‰ΩøÁî®ÊàëÂÄëÊîπËâØÁöÑË®òÊÜ∂Â±§Â¢ûÂº∑ÁöÑË™ûË®ÄÊ®°ÂûãÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºË®àÁÆóÈ†êÁÆóÂ¢ûÂä†ÂÖ©ÂÄç‰ª•‰∏äÁöÑÁ®†ÂØÜÊ®°ÂûãÔºå‰ª•ÂèäÂú®Ë®àÁÆóÂíåÂèÉÊï∏ÊñπÈù¢ÈÉΩÂåπÈÖçÁöÑÂ∞àÂÆ∂Ê∑∑ÂêàÊ®°Âûã„ÄÇÊàëÂÄëÁôºÁèæÊî∂ÁõäÂú®‰∫ãÂØ¶‰ªªÂãô‰∏≠ÁâπÂà•ÊòéÈ°Ø„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÆåÂÖ®ÂèØ‰∏¶Ë°åÁöÑË®òÊÜ∂Â±§ÂØ¶‰ΩúÔºåÂ±ïÁ§∫‰∫ÜÈ´òÈÅî 128B Ë®òÊÜ∂È´îÂèÉÊï∏ÁöÑÁ∏ÆÊîæÂÆöÂæãÔºåÈ†êË®ìÁ∑¥Âà∞ 1 ÂÖÜÂÄã tokenÔºåËàáÈ´òÈÅî 8B ÂèÉÊï∏ÁöÑÂü∫Êú¨Ê®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **Congruence-based Learning of Probabilistic Deterministic Finite Automata**
2412.09760v1 by Mat√≠as Carrasco, Franz Mayr, Sergio Yovine

This work studies the question of learning probabilistic deterministic
automata from language models. For this purpose, it focuses on analyzing the
relations defined on algebraic structures over strings by equivalences and
similarities on probability distributions. We introduce a congruence that
extends the classical Myhill-Nerode congruence for formal languages. This new
congruence is the basis for defining regularity over language models. We
present an active learning algorithm that computes the quotient with respect to
this congruence whenever the language model is regular. The paper also defines
the notion of recognizability for language models and shows that it coincides
with regularity for congruences. For relations which are not congruences, it
shows that this is not the case. Finally, it discusses the impact of this
result on learning in the context of language models.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÂæûË™ûË®ÄÊ®°ÂûãÂ≠∏ÁøíÊ©üÁéáÊÄßÁ¢∫ÂÆöËá™ÂãïÊ©üÁöÑÂïèÈ°å„ÄÇÁÇ∫Ê≠§ÔºåÂÆÉÂ∞àÊ≥®ÊñºÂàÜÊûêÁî±Ê©üÁéáÂàÜ‰Ωà‰∏äÁöÑÁ≠âÂÉπÊÄßÂíåÁõ∏‰ººÊÄßÂú®Â≠ó‰∏≤‰∏äÁöÑ‰ª£Êï∏ÁµêÊßãÊâÄÂÆöÁæ©ÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêåÈ§òÔºåÂÆÉÊì¥ÂÖÖ‰∫ÜÂΩ¢ÂºèË™ûË®ÄÁöÑÁ∂ìÂÖ∏ Myhill-Nerode ÂêåÈ§ò„ÄÇÈÄôÂÄãÊñ∞ÁöÑÂêåÈ§òÊòØÂÆöÁæ©Ë™ûË®ÄÊ®°Âûã‰∏äÊ≠£ÂâáÊÄßÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰∏ªÂãïÂ≠∏ÁøíÊºîÁÆóÊ≥ïÔºåÂè™Ë¶ÅË™ûË®ÄÊ®°ÂûãÊòØÊ≠£ÂâáÁöÑÔºåÂÆÉÂ∞±ÊúÉË®àÁÆóÁõ∏Â∞çÊñºÈÄôÂÄãÂêåÈ§òÁöÑÂïÜ„ÄÇÊú¨ÊñáÈÇÑÂÆöÁæ©‰∫ÜË™ûË®ÄÊ®°ÂûãÁöÑÂèØËæ®Ë≠òÊ¶ÇÂøµÔºå‰∏¶Ë°®ÊòéÂÆÉËàáÂêåÈ§òÁöÑÊ≠£ÂâáÊÄß‰∏ÄËá¥„ÄÇÂ∞çÊñº‰∏çÊòØÂêåÈ§òÁöÑÈóú‰øÇÔºåÂÆÉË°®ÊòéÊÉÖÊ≥Å‰∏¶ÈùûÂ¶ÇÊ≠§„ÄÇÊúÄÂæåÔºåÂÆÉË®éË´ñ‰∫ÜÈÄôÂÄãÁµêÊûúÂ∞çË™ûË®ÄÊ®°Âûã‰∏≠Â≠∏ÁøíÁöÑÂΩ±Èüø„ÄÇ

##### **AI Red-Teaming is a Sociotechnical System. Now What?**
2412.09751v1 by Tarleton Gillespie, Ryland Shaw, Mary L. Gray, Jina Suh

As generative AI technologies find more and more real-world applications, the
importance of testing their performance and safety seems paramount.
``Red-teaming'' has quickly become the primary approach to test AI
models--prioritized by AI companies, and enshrined in AI policy and regulation.
Members of red teams act as adversaries, probing AI systems to test their
safety mechanisms and uncover vulnerabilities. Yet we know too little about
this work and its implications. This essay calls for collaboration between
computer scientists and social scientists to study the sociotechnical systems
surrounding AI technologies, including the work of red-teaming, to avoid
repeating the mistakes of the recent past. We highlight the importance of
understanding the values and assumptions behind red-teaming, the labor
involved, and the psychological impacts on red-teamers.

ÊëòË¶ÅÔºöÈö®ËëóÁîüÊàêÂºè AI ÊäÄË°ìÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÊâæÂà∞Ë∂ä‰æÜË∂äÂ§öÁöÑÊáâÁî®ÔºåÊ∏¨Ë©¶ÂÖ∂ÊïàËÉΩÂíåÂÆâÂÖ®ÊÄßÈ°ØÂæóËá≥ÈóúÈáçË¶Å„ÄÇ
„ÄåÁ¥ÖÈöä„ÄçÂ∑≤ËøÖÈÄüÊàêÁÇ∫Ê∏¨Ë©¶ AI Ê®°ÂûãÁöÑ‰∏ªË¶ÅÊñπÊ≥ïÔºåÁî± AI ÂÖ¨Âè∏ÂÑ™ÂÖàËÄÉÊÖÆÔºå‰∏¶ËºâÂÖ• AI ÊîøÁ≠ñÂíåÊ≥ïË¶è‰∏≠„ÄÇ
Á¥ÖÈöäÊàêÂì°ÊâÆÊºîÂ∞çÊâãÔºåÊé¢Ê∏¨ AI Á≥ªÁµ±‰ª•Ê∏¨Ë©¶ÂÖ∂ÂÆâÂÖ®Ê©üÂà∂‰∏¶ÊâæÂá∫ÊºèÊ¥û„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÂ∞çÈÄôÈ†ÖÂ∑•‰ΩúÂèäÂÖ∂ÂΩ±ÈüøÊâÄÁü•ÁîöÂ∞ë„ÄÇÊú¨ÊñáÂëºÁ±≤ÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÂíåÁ§æÊúÉÁßëÂ≠∏ÂÆ∂Âêà‰ΩúÁ†îÁ©∂ÂúçÁπû AI ÊäÄË°ìÁöÑÁ§æÊúÉÊäÄË°ìÁ≥ªÁµ±ÔºåÂåÖÊã¨Á¥ÖÈöäÂ∑•‰ΩúÔºå‰ª•ÈÅøÂÖçÈáçËπàËøëÊúüÈåØË™§„ÄÇÊàëÂÄëÂº∑Ë™ø‰∫ÜËß£Á¥ÖÈöäËÉåÂæåÁöÑÂÉπÂÄºËßÄÂíåÂÅáË®≠„ÄÅÊâÄÊ∂âÂèäÁöÑÂãûÂãïÂäõ‰ª•ÂèäÂ∞çÁ¥ÖÈöäÊàêÂì°ÁöÑÂøÉÁêÜÂΩ±ÈüøÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Let Curves Speak: A Continuous Glucose Monitor based Large Sensor Foundation Model for Diabetes Management**
2412.09727v1 by Junjie Luo, Abhimanyu Kumbara, Mansur Shomali, Rui Han, Anand Iyer, Ritu Agarwal, Gordon Gao

While previous studies of AI in diabetes management focus on long-term risk,
research on near-future glucose prediction remains limited but important as it
enables timely diabetes self-management. Integrating AI with continuous glucose
monitoring (CGM) holds promise for near-future glucose prediction. However,
existing models have limitations in capturing patterns of blood glucose
fluctuations and demonstrate poor generalizability. A robust approach is needed
to leverage massive CGM data for near-future glucose prediction. We propose
large sensor models (LSMs) to capture knowledge in CGM data by modeling
patients as sequences of glucose. CGM-LSM is pretrained on 15.96 million
glucose records from 592 diabetes patients for near-future glucose prediction.
We evaluated CGM-LSM against state-of-the-art methods using the OhioT1DM
dataset across various metrics, prediction horizons, and unseen patients.
Additionally, we assessed its generalizability across factors like diabetes
type, age, gender, and hour of day. CGM-LSM achieved exceptional performance,
with an rMSE of 29.81 mg/dL for type 1 diabetes patients and 23.49 mg/dL for
type 2 diabetes patients in a two-hour prediction horizon. For the OhioT1DM
dataset, CGM-LSM achieved a one-hour rMSE of 15.64 mg/dL, halving the previous
best of 31.97 mg/dL. Robustness analyses revealed consistent performance not
only for unseen patients and future periods, but also across diabetes type,
age, and gender. The model demonstrated adaptability to different hours of day,
maintaining accuracy across periods of various activity intensity levels.
CGM-LSM represents a transformative step in diabetes management by leveraging
pretraining to uncover latent glucose generation patterns in sensor data. Our
findings also underscore the broader potential of LSMs to drive innovation
across domains involving complex sensor data.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂÖàÂâçÁöÑÁ≥ñÂ∞øÁóÖÁÆ°ÁêÜ‰∫∫Â∑•Êô∫ÊÖßÁ†îÁ©∂ËëóÈáçÊñºÈï∑ÊúüÈ¢®Èö™Ôºå
‰ΩÜÂ∞çÊñºËøëÊúüË°ÄÁ≥ñÈ†êÊ∏¨ÁöÑÁ†îÁ©∂‰ªçÁÑ∂ÊúâÈôêÔºå‰ΩÜÁî±ÊñºÂÆÉËÉΩÂèäÊôÇÈÄ≤Ë°åÁ≥ñÂ∞øÁóÖËá™ÊàëÁÆ°ÁêÜÔºåÂõ†Ê≠§ÈùûÂ∏∏ÈáçË¶Å„ÄÇÂ∞á‰∫∫Â∑•Êô∫ÊÖßËàáÈÄ£Á∫åË°ÄÁ≥ñÁõ£Ê∏¨ (CGM) ÁµêÂêàÔºåÊúâÊúõÈÄ≤Ë°åËøëÊúüË°ÄÁ≥ñÈ†êÊ∏¨„ÄÇÁÑ∂ËÄåÔºå
ÁèæÊúâÊ®°ÂûãÂú®ÊçïÊçâË°ÄÁ≥ñÊ≥¢ÂãïÊ®°ÂºèÊñπÈù¢ÊúâÂÖ∂ÈôêÂà∂Ôºå‰∏îÈ°ØÁ§∫Âá∫‰∏ç‰Ω≥ÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÈúÄË¶Å‰∏ÄÁ®ÆÁ©©ÂÅ•ÁöÑÊñπÊ≥ï‰æÜÂà©Áî®Â§ßÈáèÁöÑ CGM Ë≥áÊñôÈÄ≤Ë°åËøëÊúüË°ÄÁ≥ñÈ†êÊ∏¨„ÄÇÊàëÂÄëÊèêÂá∫Â§ßÂûãÊÑüÊ∏¨Âô®Ê®°Âûã (LSM) ‰æÜÊçïÊçâ CGM Ë≥áÊñô‰∏≠ÁöÑÁü•Ë≠òÔºåÊñπÊ≥ïÊòØÂ∞áÊÇ£ËÄÖÂª∫Ê®°ÁÇ∫Ëë°ËêÑÁ≥ñÂ∫èÂàó„ÄÇCGM-LSM Âú® 592 ‰ΩçÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÁöÑ 1596 Ëê¨Á≠ÜËë°ËêÑÁ≥ñË®òÈåÑ‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•ÈÄ≤Ë°åËøëÊúüË°ÄÁ≥ñÈ†êÊ∏¨„ÄÇ
ÊàëÂÄë‰ΩøÁî® OhioT1DM Ë≥áÊñôÈõÜÊ†πÊìöÂêÑÁ®ÆÊåáÊ®ô„ÄÅÈ†êÊ∏¨ÁØÑÂúçÂíåÊú™Ë¶ãÊÇ£ËÄÖÔºåÈáùÂ∞çÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïË©ï‰º∞ CGM-LSM„ÄÇ
Ê≠§Â§ñÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÂÆÉÂú®Á≥ñÂ∞øÁóÖÈ°ûÂûã„ÄÅÂπ¥ÈΩ°„ÄÅÊÄßÂà•Âíå‰∏ÄÂ§©‰∏≠ÁöÑÂ∞èÊôÇÁ≠âÂõ†Á¥†‰∏≠ÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇCGM-LSM ÈÅîÂà∞‰∫ÜÈùûÂá°ÁöÑÊïàËÉΩÔºåÂú®ÂÖ©Â∞èÊôÇÈ†êÊ∏¨ÁØÑÂúçÂÖßÔºå1 ÂûãÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÁöÑ rMSE ÁÇ∫ 29.81 mg/dLÔºå2 ÂûãÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÁöÑ rMSE ÁÇ∫ 23.49 mg/dL„ÄÇÂ∞çÊñº OhioT1DM Ë≥áÊñôÈõÜÔºåCGM-LSM ÈÅîÂà∞‰∏ÄÂ∞èÊôÇ rMSE ÁÇ∫ 15.64 mg/dLÔºåÂ∞áÂÖàÂâçÁöÑÊúÄ‰Ω≥ÂÄº 31.97 mg/dL Ê∏õÂçä„ÄÇÁ©©ÂÅ•ÊÄßÂàÜÊûêÈ°ØÁ§∫ÔºåÊïàËÉΩ‰∏çÂÉÖÂ∞çÊñºÊú™Ë¶ãÊÇ£ËÄÖÂíåÊú™‰æÜÊúüÈñì‰∏ÄËá¥ÔºåËÄå‰∏îÂú®Á≥ñÂ∞øÁóÖÈ°ûÂûã„ÄÅÂπ¥ÈΩ°ÂíåÊÄßÂà•ÊñπÈù¢‰πü‰∏ÄËá¥„ÄÇË©≤Ê®°ÂûãË≠âÊòé‰∫ÜÂ∞ç‰∏ÄÂ§©‰∏≠‰∏çÂêåÂ∞èÊôÇÁöÑÈÅ©ÊáâÊÄßÔºåÂú®ÂêÑÁ®ÆÊ¥ªÂãïÂº∑Â∫¶ÊôÇÊúü‰øùÊåÅÊ∫ñÁ¢∫ÊÄß„ÄÇ
CGM-LSM ÈÄèÈÅéÂà©Áî®È†êË®ìÁ∑¥‰æÜÊè≠Á§∫ÊÑüÊ∏¨Âô®Ë≥áÊñô‰∏≠ÁöÑÊΩõÂú®Ëë°ËêÑÁ≥ñÁîüÊàêÊ®°ÂºèÔºå‰ª£Ë°®‰∫ÜÁ≥ñÂ∞øÁóÖÁÆ°ÁêÜÁöÑËΩâÂûãÊ≠•È©ü„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûú‰πüÂº∑Ë™ø‰∫Ü LSM Âú®Êé®ÂãïÊ∂âÂèäË§áÈõúÊÑüÊ∏¨Âô®Ë≥áÊñôÁöÑÈ†òÂüüÂâµÊñ∞ÁöÑÊõ¥Âª£Ê≥õÊΩõÂäõ„ÄÇ

##### **The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications**
2412.09726v1 by Binxu Wang, John J. Vastola

By learning the gradient of smoothed data distributions, diffusion models can
iteratively generate samples from complex distributions. The learned score
function enables their generalization capabilities, but how the learned score
relates to the score of the underlying data manifold remains largely unclear.
Here, we aim to elucidate this relationship by comparing learned neural scores
to the scores of two kinds of analytically tractable distributions: Gaussians
and Gaussian mixtures. The simplicity of the Gaussian model makes it
theoretically attractive, and we show that it admits a closed-form solution and
predicts many qualitative aspects of sample generation dynamics. We claim that
the learned neural score is dominated by its linear (Gaussian) approximation
for moderate to high noise scales, and supply both theoretical and empirical
arguments to support this claim. Moreover, the Gaussian approximation
empirically works for a larger range of noise scales than naive theory suggests
it should, and is preferentially learned early in training. At smaller noise
scales, we observe that learned scores are better described by a coarse-grained
(Gaussian mixture) approximation of training data than by the score of the
training distribution, a finding consistent with generalization. Our findings
enable us to precisely predict the initial phase of trained models' sampling
trajectories through their Gaussian approximations. We show that this allows
the skipping of the first 15-30% of sampling steps while maintaining high
sample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10
unconditional generation). This forms the foundation of a novel hybrid sampling
method, termed analytical teleportation, which can seamlessly integrate with
and accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our
findings suggest ways to improve the design and training of diffusion models.

ÊëòË¶ÅÔºöÈÄèÈÅéÂ≠∏ÁøíÂπ≥ÊªëË≥áÊñôÂàÜ‰ΩàÁöÑÊ¢ØÂ∫¶ÔºåÊì¥Êï£Ê®°ÂûãÂèØ‰ª•ÂèçË¶ÜÁîüÊàêË§áÈõúÂàÜ‰ΩàÁöÑÊ®£Êú¨„ÄÇÂ≠∏ÁøíÂà∞ÁöÑÂàÜÊï∏ÂáΩÊï∏ËÉΩËÆìÂÆÉÂÄëÂÖ∑ÂÇôÊ¶ÇÂåñËÉΩÂäõÔºå‰ΩÜÂ≠∏ÁøíÂà∞ÁöÑÂàÜÊï∏Â¶Ç‰ΩïËàáÂ∫ïÂ±§Ë≥áÊñôÊµÅÂΩ¢ÁöÑÂàÜÊï∏Áõ∏ÈóúÔºåÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÂ∞áÂ≠∏ÁøíÂà∞ÁöÑÁ•ûÁ∂ìÂàÜÊï∏ËàáÂÖ©Á®ÆËß£ÊûêÂèØËôïÁêÜÂàÜ‰ΩàÔºàÈ´òÊñØÂàÜ‰ΩàÂíåÈ´òÊñØÊ∑∑ÂêàÂàÜ‰ΩàÔºâÁöÑÂàÜÊï∏ÈÄ≤Ë°åÊØîËºÉÔºå‰æÜÈó°ÊòéÈÄôÁ®ÆÈóú‰øÇ„ÄÇÈ´òÊñØÊ®°ÂûãÁöÑÁ∞°ÊΩîÊÄß‰ΩøÂÖ∂Âú®ÁêÜË´ñ‰∏äÂÖ∑ÊúâÂê∏ÂºïÂäõÔºåÊàëÂÄëË°®ÊòéÂÆÉÊâøË™çÈñâÂêàÂΩ¢ÂºèÁöÑËß£Ôºå‰∏¶È†êÊ∏¨‰∫ÜÊ®£Êú¨ÁîüÊàêÂãïÊÖãÁöÑË®±Â§öÂÆöÊÄßÊñπÈù¢„ÄÇÊàëÂÄëËÅ≤Á®±ÔºåÂ∞çÊñº‰∏≠Á≠âËá≥È´òÂô™ËÅ≤Â∞∫Â∫¶ÔºåÂ≠∏ÁøíÂà∞ÁöÑÁ•ûÁ∂ìÂàÜÊï∏Áî±ÂÖ∂Á∑öÊÄßÔºàÈ´òÊñØÔºâËøë‰ºº‰∏ªÂ∞éÔºå‰∏¶Êèê‰æõÁêÜË´ñÂíåÁ∂ìÈ©óË´ñË≠â‰æÜÊîØÊåÅÊ≠§Ë™™Ê≥ï„ÄÇÊ≠§Â§ñÔºåÈ´òÊñØËøë‰ººÂú®ÊØîÊ®∏Á¥†ÁêÜË´ñÊâÄÂª∫Ë≠∞ÁöÑÊõ¥Âª£Ê≥õÁöÑÂô™ËÅ≤Â∞∫Â∫¶ÁØÑÂúçÂÖßÁ∂ìÈ©ó‰∏äÊúâÊïàÔºå‰∏¶‰∏îÂú®Ë®ìÁ∑¥Êó©ÊúüÂÑ™ÂÖàÂ≠∏Áøí„ÄÇÂú®ËºÉÂ∞èÁöÑÂô™ËÅ≤Â∞∫Â∫¶‰∏ãÔºåÊàëÂÄëËßÄÂØüÂà∞Â≠∏ÁøíÂà∞ÁöÑÂàÜÊï∏Áî±Ë®ìÁ∑¥Ë≥áÊñôÁöÑÁ≤óÁ≤íÂ∫¶ÔºàÈ´òÊñØÊ∑∑ÂêàÔºâËøë‰ººÔºåËÄå‰∏çÊòØË®ìÁ∑¥ÂàÜ‰ΩàÁöÑÂàÜÊï∏‰æÜÊõ¥Â•ΩÂú∞ÊèèËø∞ÔºåÈÄô‰∏ÄÁôºÁèæËàáÊ¶ÇÂåñÁõ∏‰∏ÄËá¥„ÄÇÊàëÂÄëÁöÑÁôºÁèæ‰ΩøÊàëÂÄëËÉΩÂ§†ÈÄöÈÅéÈ´òÊñØËøë‰ººÊ∫ñÁ¢∫È†êÊ∏¨Ë®ìÁ∑¥Ê®°ÂûãÊé°Ê®£ËªåË∑°ÁöÑÂàùÂßãÈöéÊÆµ„ÄÇÊàëÂÄëË°®ÊòéÔºåÈÄôÂÖÅË®±Ë∑≥ÈÅéÂâç 15-30% ÁöÑÊé°Ê®£Ê≠•È©üÔºåÂêåÊôÇ‰øùÊåÅÈ´òÊ®£Êú¨ÂìÅË≥™ÔºàÂú® CIFAR-10 ÁÑ°Ê¢ù‰ª∂ÁîüÊàê‰∏äÁç≤ÂæóÊé•ËøëÊúÄÂÖàÈÄ≤ÁöÑ 1.93 FID ÂàÜÊï∏Ôºâ„ÄÇÈÄôÊßãÊàê‰∫ÜÂâµÊñ∞Ê∑∑ÂêàÊé°Ê®£ÊñπÊ≥ïÁöÑÂü∫Á§éÔºåÁ®±ÁÇ∫ÂàÜÊûêÂÇ≥ÈÄÅÔºåÂÆÉÂèØ‰ª•ÁÑ°Á∏´Êï¥Âêà‰∏¶Âä†ÈÄüÁèæÊúâÊé°Ê®£Âô®ÔºåÂåÖÊã¨ DPM-Solver-v3 Âíå UniPC„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊèêÂá∫‰∫ÜÊîπÈÄ≤Êì¥Êï£Ê®°ÂûãË®≠Ë®àÂíåË®ìÁ∑¥ÁöÑÊñπÊ≥ï„ÄÇ

##### **GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers**
2412.09722v1 by Sarkar Snigdha Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming Xiong, Rui Zhang

The effectiveness of large language models (LLMs) is closely tied to the
design of prompts, making prompt optimization essential for enhancing their
performance across a wide range of tasks. Many existing approaches to
automating prompt engineering rely exclusively on textual feedback, refining
prompts based solely on inference errors identified by large, computationally
expensive LLMs. Unfortunately, smaller models struggle to generate high-quality
feedback, resulting in complete dependence on large LLM judgment. Moreover,
these methods fail to leverage more direct and finer-grained information, such
as gradients, due to operating purely in text space. To this end, we introduce
GReaTer, a novel prompt optimization technique that directly incorporates
gradient information over task-specific reasoning. By utilizing task loss
gradients, GReaTer enables self-optimization of prompts for open-source,
lightweight language models without the need for costly closed-source LLMs.
This allows high-performance prompt optimization without dependence on massive
LLMs, closing the gap between smaller models and the sophisticated reasoning
often needed for prompt refinement. Extensive evaluations across diverse
reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer
consistently outperforms previous state-of-the-art prompt optimization methods,
even those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts
frequently exhibit better transferability and, in some cases, boost task
performance to levels comparable to or surpassing those achieved by larger
language models, highlighting the effectiveness of prompt optimization guided
by gradients over reasoning. Code of GReaTer is available at
https://github.com/psunlpgroup/GreaTer.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊïàËÉΩËàáÊèêÁ§∫Ë®≠Ë®àÊÅØÊÅØÁõ∏ÈóúÔºåÂõ†Ê≠§ÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÂ∞çÊñºÊèêÂçá LLM Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑË°®ÁèæËá≥ÈóúÈáçË¶Å„ÄÇË®±Â§öÁèæÊúâÁöÑËá™ÂãïÂåñÊèêÁ§∫Â∑•Á®ãÊñπÊ≥ïÂÉÖ‰æùË≥¥ÊñáÂ≠óÂõûÈ•ãÔºåÊ†πÊìöÂ§ßÂûã„ÄÅÈÅãÁÆóÊàêÊú¨È´òÊòÇÁöÑ LLM Ë≠òÂà•Âá∫ÁöÑÊé®Ë´ñÈåØË™§‰æÜÊîπÈÄ≤ÊèêÁ§∫„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåËºÉÂ∞èÁöÑÊ®°ÂûãÈõ£‰ª•Áî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂõûÈ•ãÔºåÂ∞éËá¥ÂÆåÂÖ®‰æùË≥¥Â§ßÂûã LLM ÁöÑÂà§Êñ∑„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊñπÊ≥ïÁî±ÊñºÁ¥îÁ≤πÂú®ÊñáÂ≠óÁ©∫Èñì‰∏≠ÈÅã‰ΩúÔºåÁÑ°Ê≥ïÂà©Áî®Êõ¥Áõ¥Êé•„ÄÅÊõ¥Á¥∞Á∑ªÁöÑË≥áË®äÔºå‰æãÂ¶ÇÊ¢ØÂ∫¶„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GReaTerÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊäÄË°ìÔºåÂèØÁõ¥Êé•Â∞áÊ¢ØÂ∫¶Ë≥áË®äÁ¥çÂÖ•ÁâπÂÆö‰ªªÂãôÁöÑÊé®ÁêÜ‰∏≠„ÄÇÈÄèÈÅéÂà©Áî®‰ªªÂãôÊêçÂ§±Ê¢ØÂ∫¶ÔºåGReaTer ÂèØ‰ª•ÁÇ∫ÈñãÊîæÂéüÂßãÁ¢º„ÄÅËºïÈáèÁ¥öË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åÊèêÁ§∫Ëá™ÊúÄ‰Ω≥ÂåñÔºåÁÑ°ÈúÄ‰æùË≥¥ÊòÇË≤¥ÁöÑÈñâÊ∫ê LLM„ÄÇÈÄôÂÖÅË®±Âü∑Ë°åÈ´òÊÄßËÉΩÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÔºåËÄåÁÑ°ÈúÄ‰æùË≥¥Â§ßÂûã LLMÔºåÁ∏ÆÂ∞è‰∫ÜËºÉÂ∞èÊ®°ÂûãËàáÊèêÁ§∫ÊîπÈÄ≤ÈÄöÂ∏∏ÈúÄË¶ÅÁöÑË§áÈõúÊé®ÁêÜ‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÂú®ÂåÖÊã¨ BBH„ÄÅGSM8k Âíå FOLIO Âú®ÂÖßÁöÑÂêÑÁ®ÆÊé®ÁêÜ‰ªªÂãô‰∏≠ÈÄ≤Ë°åÁöÑÂª£Ê≥õË©ï‰º∞Ë°®ÊòéÔºåGReaTer ‰∏ÄËá¥ÂÑ™ÊñºÂÖàÂâçÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÔºåÂç≥‰ΩøÊòØ‰æùË≥¥Âº∑Â§ß LLM ÁöÑÊñπÊ≥ï‰πüÊòØÂ¶ÇÊ≠§„ÄÇÊ≠§Â§ñÔºåGReaTer ÊúÄ‰Ω≥ÂåñÁöÑÊèêÁ§∫ÈÄöÂ∏∏Ë°®ÁèæÂá∫Êõ¥Â•ΩÁöÑÂèØÁßªÊ§çÊÄßÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºå‰ªªÂãôÊïàËÉΩÊèêÂçáËá≥ËàáËºÉÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁõ∏Áï∂ÊàñË∂ÖË∂äÂÖ∂ÊïàËÉΩÁöÑÁ®ãÂ∫¶ÔºåÁ™ÅÈ°Ø‰∫ÜÈÄèÈÅéÊé®ÁêÜÊ¢ØÂ∫¶ÂºïÂ∞éÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÁöÑÊïàËÉΩ„ÄÇGReaTer ÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/psunlpgroup/GreaTer ÂèñÂæó„ÄÇ

##### **Human vs. AI: A Novel Benchmark and a Comparative Study on the Detection of Generated Images and the Impact of Prompts**
2412.09715v1 by Philipp Moe√üner, Heike Adel

With the advent of publicly available AI-based text-to-image systems, the
process of creating photorealistic but fully synthetic images has been largely
democratized. This can pose a threat to the public through a simplified spread
of disinformation. Machine detectors and human media expertise can help to
differentiate between AI-generated (fake) and real images and counteract this
danger. Although AI generation models are highly prompt-dependent, the impact
of the prompt on the fake detection performance has rarely been investigated
yet. This work therefore examines the influence of the prompt's level of detail
on the detectability of fake images, both with an AI detector and in a user
study. For this purpose, we create a novel dataset, COCOXGEN, which consists of
real photos from the COCO dataset as well as images generated with SDXL and
Fooocus using prompts of two standardized lengths. Our user study with 200
participants shows that images generated with longer, more detailed prompts are
detected significantly more easily than those generated with short prompts.
Similarly, an AI-based detection model achieves better performance on images
generated with longer prompts. However, humans and AI models seem to pay
attention to different details, as we show in a heat map analysis.

ÊëòË¶ÅÔºöÈö®ËëóÂÖ¨ÈñãÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂü∫ÊñºÊñáÂ≠óËΩâÊèõÂúñÂÉèÁ≥ªÁµ±ÁöÑÂá∫ÁèæÔºåÂâµÈÄ†ÂØ´ÂØ¶‰ΩÜÂÆåÂÖ®ÂêàÊàêÁöÑÂúñÂÉèÁöÑÈÅéÁ®ãÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂ∑≤Á∂ìÊ∞ë‰∏ªÂåñ„ÄÇÈÄôÂèØËÉΩÊúÉÈÄèÈÅéÁ∞°ÂåñÈåØË™§Ë®äÊÅØÁöÑÊï£Â∏ÉÂ∞çÂÖ¨ÁúæÊßãÊàêÂ®ÅËÑÖ„ÄÇÊ©üÂô®ÂÅµÊ∏¨Âô®Âíå‰∫∫È°ûÂ™íÈ´îÂ∞àÊ•≠Áü•Ë≠òÊúâÂä©ÊñºÂçÄÂàÜ‰∫∫Â∑•Êô∫ÊÖßÁî¢ÁîüÁöÑÔºàÂÅáÁöÑÔºâÂíåÁúüÂØ¶ÁöÑÂúñÂÉèÔºå‰∏¶Â∞çÊäóÈÄôÁ®ÆÂç±Èö™„ÄÇÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖßÁîüÊàêÊ®°ÂûãÈ´òÂ∫¶‰æùË≥¥ÊèêÁ§∫Ôºå‰ΩÜÊèêÁ§∫Â∞çÂÅáÂÅµÊ∏¨ÊïàËÉΩÁöÑÂΩ±ÈüøÂçªÂæàÂ∞ëÂèóÂà∞Ë™øÊü•„ÄÇÂõ†Ê≠§ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Ë®é‰∫ÜÊèêÁ§∫ÁöÑË©≥Á¥∞Á®ãÂ∫¶Â∞çÂÅáÂúñÂÉèÁöÑÂèØÂÅµÊ∏¨ÊÄßÁöÑÂΩ±ÈüøÔºåÂêåÊôÇ‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖßÂÅµÊ∏¨Âô®Âíå‰ΩøÁî®ËÄÖÁ†îÁ©∂„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂâµÂª∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑË≥áÊñôÈõÜ COCOXGENÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ COCO Ë≥áÊñôÈõÜÁöÑÁúüÂØ¶ÁÖßÁâáÔºå‰ª•Âèä‰ΩøÁî®ÂÖ©Á®ÆÊ®ôÊ∫ñÂåñÈï∑Â∫¶ÁöÑÊèêÁ§∫Áî¢ÁîüÁöÑ SDXL Âíå Fooocus ÂΩ±ÂÉè„ÄÇÊàëÂÄëÂ∞ç 200 ÂêçÂèÉËàáËÄÖÈÄ≤Ë°åÁöÑ‰ΩøÁî®ËÄÖÁ†îÁ©∂È°ØÁ§∫Ôºå‰ΩøÁî®ËºÉÈï∑„ÄÅÊõ¥Ë©≥Á¥∞ÁöÑÊèêÁ§∫Áî¢ÁîüÁöÑÂúñÂÉèÊØî‰ΩøÁî®Á∞°Áü≠ÊèêÁ§∫Áî¢ÁîüÁöÑÂúñÂÉèÊõ¥ÂÆπÊòìË¢´ÂÅµÊ∏¨Âà∞„ÄÇÂêåÊ®£Âú∞ÔºåÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÂÅµÊ∏¨Ê®°ÂûãÂú®‰ΩøÁî®ËºÉÈï∑ÊèêÁ§∫Áî¢ÁîüÁöÑÂúñÂÉè‰∏äÁç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊ≠£Â¶ÇÊàëÂÄëÂú®ÁÜ±ÈªûÂúñÂàÜÊûê‰∏≠ÊâÄÁ§∫Ôºå‰∫∫È°ûÂíå‰∫∫Â∑•Êô∫ÊÖßÊ®°Âûã‰ºº‰πéÈóúÊ≥®‰∏çÂêåÁöÑÁ¥∞ÁØÄ„ÄÇ

