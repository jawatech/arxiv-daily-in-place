{"2406.09827": {"publish_time": "2024-06-14", "title": "HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning", "paper_summary": "In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.", "paper_summary_zh": "<paragraph>\u5728\u73fe\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\uff0c\u589e\u52a0\u5e8f\u5217\u9577\u5ea6\u5c0d\u65bc\u63d0\u5347\u5176\u5728\u8655\u7406\u591a\u6a21\u614b\u554f\u984c\u89e3\u7b54\u7b49\u8907\u96dc\u4efb\u52d9\u6642\u7684\u7406\u89e3\u529b\u548c\u9023\u8cab\u6027\u4f86\u8aaa\u662f\u4e00\u9805\u81f3\u95dc\u91cd\u8981\u7684\u6311\u6230\u3002\u7136\u800c\uff0c\u7531\u65bc\u50b3\u7d71\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u5177\u6709\u4e8c\u6b21\u6642\u9593\u548c\u7a7a\u9593\u8907\u96dc\u5ea6\uff0c\u4e14\u4e0a\u4e0b\u6587\u8996\u7a97\u5927\u5c0f\u53d7\u5230 GPU \u8a18\u61b6\u9ad4\u7684\u9650\u5236\uff0c\u56e0\u6b64\u4f7f\u7528 LLM \u8655\u7406\u9577\u4e0a\u4e0b\u6587\u5e8f\u5217\u7684\u6210\u672c\u904e\u65bc\u9ad8\u6602\u3002\u5118\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u7dda\u6027\u548c\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4f46\u5b83\u5011\u7684\u5be6\u969b\u61c9\u7528\u901a\u5e38\u53d7\u5230\u91cd\u65b0\u8a13\u7df4\u9810\u8a13\u7df4\u6a21\u578b\u7684\u9700\u8981\u6240\u9650\u5236\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u968e\u5c64\u5316\u4fee\u526a\u6ce8\u610f\u529b (HiP)\uff0c\u5b83\u540c\u6642\u5c07\u8a13\u7df4\u548c\u63a8\u8ad6\u6642\u9593\u8907\u96dc\u5ea6\u5f9e $O(T^2)$ \u964d\u4f4e\u5230 $O(T \\log T)$\uff0c\u4e26\u5c07\u7a7a\u9593\u8907\u96dc\u5ea6\u5f9e $O(T^2)$ \u964d\u4f4e\u5230 $O(T)$\u3002\u70ba\u6b64\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u52d5\u614b\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5b83\u901a\u904e\u4e00\u7a2e\u65b0\u7a4e\u7684\u985e\u6a39\u72c0\u641c\u5c0b\u6f14\u7b97\u6cd5\u70ba\u7d66\u5b9a\u7684\u67e5\u8a62\u52d5\u614b\u751f\u6210\u6ce8\u610f\u529b\u906e\u7f69\u3002HiP \u662f\u514d\u8a13\u7df4\u7684\uff0c\u56e0\u70ba\u5b83\u53ea\u5229\u7528\u9810\u8a13\u7df4\u7684\u6ce8\u610f\u529b\u5206\u6578\u4f86\u627e\u51fa\u6bcf\u500b\u67e5\u8a62\u4e2d\u6700\u91cd\u8981\u7684\u524d $k$ \u500b\u5143\u7d20\u7684\u4f4d\u7f6e\u3002\u6b64\u5916\uff0c\u5b83\u78ba\u4fdd\u4e0d\u6703\u907a\u6f0f\u4efb\u4f55\u7b26\u865f\uff0c\u9019\u8207\u57fa\u65bc\u6ed1\u52d5\u8996\u7a97\u7684\u6b21\u4e8c\u6b21\u6ce8\u610f\u529b\u65b9\u6cd5\uff08\u4f8b\u5982 StreamingLLM\uff09\u4e0d\u540c\u3002\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u57fa\u6e96\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0cHiP \u5927\u5e45\u6e1b\u5c11\u4e86\u63d0\u793a\uff08\u5373\u9810\u586b\u5145\uff09\u548c\u89e3\u78bc\u5ef6\u9072\u4ee5\u53ca\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u540c\u6642\u5728\u5e7e\u4e4e\u6c92\u6709\u6216\u6c92\u6709\u964d\u4f4e\u7684\u60c5\u6cc1\u4e0b\u7dad\u6301\u4e86\u9ad8\u751f\u6210\u6548\u80fd\u3002\u7531\u65bc HiP \u5141\u8a31\u9810\u8a13\u7df4\u7684 LLM \u5728\u6c92\u6709\u984d\u5916\u5de5\u7a0b\u7684\u689d\u4ef6\u4e0b\u64f4\u5145\u5230\u6578\u767e\u842c\u500b\u7b26\u865f\u7684\u5546\u54c1 GPU \u4e0a\uff0c\u800c\u4e14\u90e8\u7f72\u8d77\u4f86\u7c21\u55ae\u6613\u884c\uff0c\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u9019\u9805\u5de5\u4f5c\u5c07\u7522\u751f\u5de8\u5927\u7684\u5be6\u969b\u5f71\u97ff\uff0c\u70ba\u8a31\u591a\u4ee5\u524d\u4e0d\u53ef\u884c\u7684\u9577\u4e0a\u4e0b\u6587 LLM \u61c9\u7528\u958b\u555f\u4e86\u53ef\u80fd\u6027\u3002</paragraph>", "author": "Heejun Lee et.al.", "authors": "Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang", "id": "2406.09827v1", "paper_url": "http://arxiv.org/abs/2406.09827v1", "repo": "null"}}