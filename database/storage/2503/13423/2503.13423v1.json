{"2503.13423": {"publish_time": "2025-03-17", "title": "SuperBPE: Space Travel for Language Models", "paper_summary": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.", "paper_summary_zh": "<paragraph>\u5e7e\u4e4e\u6240\u6709\u8a9e\u8a00\u6a21\u578b (LM) \u5206\u8a5e\u65b9\u6848\u90fd\u5047\u8a2d\u5206\u8a5e\u61c9\u8a72\u70ba\u5b50\u8a5e\uff0c\u4ea6\u5373\u5305\u542b\u5728\u8a5e\u5f59\u908a\u754c\u5167\u3002\u96d6\u7136\u9019\u63d0\u4f9b\u4e86\u4e00\u500b\u770b\u4f3c\u5408\u7406\u7684\u6b78\u7d0d\u504f\u5dee\uff0c\u4f46\u9019\u7a2e\u5e38\u898b\u505a\u6cd5\u662f\u5426\u9650\u5236\u4e86\u73fe\u4ee3 LM \u7684\u6f5b\u529b\uff1f\u7a7a\u683c\u4e26\u4e0d\u662f\u4e00\u500b\u53ef\u9760\u7684\u8a9e\u7fa9\u5206\u9694\u7b26\uff0c\u591a\u8a5e\u8868\u9054\u5f0f\uff08\u4f8b\u5982\u300c\u9806\u4fbf\u4e00\u63d0\u300d\uff09\u3001\u8de8\u8a9e\u8a00\u8868\u9054\u6982\u5ff5\u6240\u9700\u8a5e\u5f59\u6578\u91cf\u7684\u5dee\u7570\uff08\u4f8b\u5982\uff0c\u5fb7\u8a9e\u4e2d\u7684\u300c\u592a\u7a7a\u8863\u982d\u76d4\u300d\u662f\u300craumanzughelm\u300d\uff09\uff0c\u4ee5\u53ca\u5b8c\u5168\u4e0d\u4f7f\u7528\u7a7a\u683c\u7684\u8a9e\u8a00\uff08\u4f8b\u5982\u4e2d\u6587\uff09\u90fd\u8b49\u660e\u4e86\u9019\u4e00\u9ede\u3002\u70ba\u4e86\u63a2\u7d22\u5b50\u8a5e\u4ee5\u5916\u7684\u5206\u8a5e\u7684\u6f5b\u529b\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u300c\u8d85\u7d1a\u8a5e\u300d\u5206\u8a5e\u5668 SuperBPE\uff0c\u5b83\u5728\u5b57\u7bc0\u5c0d\u7de8\u78bc (BPE) \u6f14\u7b97\u6cd5\u4e2d\u52a0\u5165\u4e86\u4e00\u500b\u7c21\u55ae\u7684\u9810\u5206\u8a5e\u8ab2\u7a0b\uff0c\u9996\u5148\u5b78\u7fd2\u5b50\u8a5e\uff0c\u7136\u5f8c\u5b78\u7fd2\u8de8\u8d8a\u7a7a\u683c\u7684\u8d85\u7d1a\u8a5e\u3002\u9019\u986f\u8457\u63d0\u9ad8\u4e86\u7de8\u78bc\u6548\u7387\uff1a\u7576\u8a5e\u5f59\u91cf\u56fa\u5b9a\u70ba 20 \u842c\u6642\uff0cSuperBPE \u5c0d\u4e00\u6bb5\u56fa\u5b9a\u6587\u672c\u7684\u7de8\u78bc\u5e73\u5747\u6bd4 BPE \u5c11 33% \u7684\u5206\u8a5e\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4 8Btransformer LM\uff0c\u540c\u6642\u56fa\u5b9a\u6a21\u578b\u5927\u5c0f\u3001\u8a5e\u5f59\u91cf\u548c\u8a13\u7df4\u8a08\u7b97\u91cf\uff0c*\u50c5*\u6539\u8b8a\u5b78\u7fd2\u8a5e\u5f59\u7684\u6f14\u7b97\u6cd5\u3002\u4f7f\u7528 SuperBPE \u8a13\u7df4\u7684\u6a21\u578b\u5728 30 \u500b\u4e0b\u6e38\u4efb\u52d9\u4e2d\u6bd4 BPE \u57fa\u7dda\u5e73\u5747\u63d0\u9ad8\u4e86 +4.0%\uff08\u5305\u62ec\u5728 MMLU \u4e0a\u63d0\u9ad8\u4e86 +8.2%\uff09\uff0c\u540c\u6642\u5728\u63a8\u7406\u6642\u6240\u9700\u7684\u8a08\u7b97\u91cf\u6e1b\u5c11\u4e86 27%\u3002\u5728\u5206\u6790\u4e2d\uff0c\u6211\u5011\u767c\u73fe SuperBPE \u7522\u751f\u7684\u6587\u672c\u5206\u6bb5\u5728\u6bcf\u500b\u5206\u8a5e\u7684\u96e3\u5ea6\u4e0a\u66f4\u70ba\u4e00\u81f4\u3002\u5b9a\u6027\u5730\u8aaa\uff0c\u9019\u53ef\u80fd\u662f\u56e0\u70ba SuperBPE \u5206\u8a5e\u901a\u5e38\u6703\u6355\u6349\u5230\u8a9e\u7fa9\u4e0a\u4f5c\u70ba\u55ae\u4e00\u55ae\u5143\u904b\u4f5c\u7684\u5e38\u898b\u591a\u8a5e\u8868\u9054\u5f0f\u3002SuperBPE \u662f\u4e00\u7a2e\u7c21\u55ae\u7684\u3001\u5c40\u90e8\u7684\u5206\u8a5e\u4fee\u6539\uff0c\u5b83\u63d0\u9ad8\u4e86\u7de8\u78bc\u6548\u7387\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u5f9e\u800c\u7522\u751f\u4e86\u66f4\u597d\u7684\u8a9e\u8a00\u6a21\u578b\u3002</paragraph>\n", "author": "Alisa Liu et.al.", "authors": "Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi", "id": "2503.13423v1", "paper_url": "http://arxiv.org/abs/2503.13423v1", "repo": "null"}}