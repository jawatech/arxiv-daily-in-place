{"2405.03064": {"publish_time": "2024-05-05", "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation", "paper_summary": "Deep reinforcement learning (DRL) is playing an increasingly important role\nin real-world applications. However, obtaining an optimally performing DRL\nagent for complex tasks, especially with sparse rewards, remains a significant\nchallenge. The training of a DRL agent can be often trapped in a bottleneck\nwithout further progress. In this paper, we propose RICE, an innovative\nrefining scheme for reinforcement learning that incorporates explanation\nmethods to break through the training bottlenecks. The high-level idea of RICE\nis to construct a new initial state distribution that combines both the default\ninitial states and critical states identified through explanation methods,\nthereby encouraging the agent to explore from the mixed initial states. Through\ncareful design, we can theoretically guarantee that our refining scheme has a\ntighter sub-optimality bound. We evaluate RICE in various popular RL\nenvironments and real-world applications. The results demonstrate that RICE\nsignificantly outperforms existing refining schemes in enhancing agent\nperformance.", "paper_summary_zh": "", "author": "Zelei Cheng et.al.", "authors": "Zelei Cheng,Xian Wu,Jiahao Yu,Sabrina Yang,Gang Wang,Xinyu Xing", "id": "2405.03064v1", "paper_url": "http://arxiv.org/abs/2405.03064v1", "repo": "null"}}