{"2407.06542": {"publish_time": "2024-07-09", "title": "LIONs: An Empirically Optimized Approach to Align Language Models", "paper_summary": "Alignment is a crucial step to enhance the instruction-following and\nconversational abilities of language models. Despite many recent work proposing\nnew algorithms, datasets, and training pipelines, there is a lack of\ncomprehensive studies measuring the impact of various design choices throughout\nthe whole training process. We first conduct a rigorous analysis over a\nthree-stage training pipeline consisting of supervised fine-tuning, offline\npreference learning, and online preference learning. We have found that using\ntechniques like sequence packing, loss masking in SFT, increasing the\npreference dataset size in DPO, and online DPO training can significantly\nimprove the performance of language models. We then train from Gemma-2b-base\nand LLama-3-8b-base, and find that our best models exceed the performance of\nthe official instruct models tuned with closed-source data and algorithms. Our\ncode and models can be found at\nhttps://github.com/Columbia-NLP-Lab/LionAlignment.", "paper_summary_zh": "\u5c0d\u9f4a\u662f\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\u9075\u5faa\u6307\u4ee4\u548c\u5c0d\u8a71\u80fd\u529b\u7684\u95dc\u9375\u6b65\u9a5f\u3002\u5118\u7ba1\u8a31\u591a\u8fd1\u671f\u7814\u7a76\u63d0\u51fa\u65b0\u7684\u6f14\u7b97\u6cd5\u3001\u8cc7\u6599\u96c6\u548c\u8a13\u7df4\u7ba1\u7dda\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u8861\u91cf\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u5404\u7a2e\u8a2d\u8a08\u9078\u64c7\u5f71\u97ff\u7684\u5168\u9762\u7814\u7a76\u3002\u6211\u5011\u9996\u5148\u5c0d\u7531\u76e3\u7763\u5fae\u8abf\u3001\u96e2\u7dda\u504f\u597d\u5b78\u7fd2\u548c\u7dda\u4e0a\u504f\u597d\u5b78\u7fd2\u7d44\u6210\u7684\u4e09\u968e\u6bb5\u8a13\u7df4\u7ba1\u7dda\u9032\u884c\u56b4\u8b39\u7684\u5206\u6790\u3002\u6211\u5011\u767c\u73fe\u4f7f\u7528\u5e8f\u5217\u5c01\u88dd\u3001SFT \u4e2d\u7684\u640d\u5931\u906e\u853d\u3001\u589e\u52a0 DPO \u4e2d\u7684\u504f\u597d\u8cc7\u6599\u96c6\u5927\u5c0f\u548c\u7dda\u4e0a DPO \u8a13\u7df4\u7b49\u6280\u8853\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u8a9e\u8a00\u6a21\u578b\u7684\u6548\u80fd\u3002\u63a5\u8457\u6211\u5011\u5f9e Gemma-2b-base \u548c LLama-3-8b-base \u9032\u884c\u8a13\u7df4\uff0c\u767c\u73fe\u6211\u5011\u6700\u597d\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u4f7f\u7528\u9589\u6e90\u8cc7\u6599\u548c\u6f14\u7b97\u6cd5\u5fae\u8abf\u7684\u5b98\u65b9\u6307\u4ee4\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/Columbia-NLP-Lab/LionAlignment \u627e\u5230\u3002", "author": "Xiao Yu et.al.", "authors": "Xiao Yu, Qingyang Wu, Yu Li, Zhou Yu", "id": "2407.06542v1", "paper_url": "http://arxiv.org/abs/2407.06542v1", "repo": "null"}}