{"2503.04697": {"publish_time": "2025-03-06", "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning", "paper_summary": "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps://www.cmu-l3.github.io/l1", "paper_summary_zh": "<paragraph>\u63a8\u7406\u8a9e\u8a00\u6a21\u578b\u5c55\u73fe\u4e86\u4e00\u7a2e\u9a5a\u4eba\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u900f\u904e\u300c\u66f4\u9577\u6642\u9593\u7684\u601d\u8003\u300d\u4f86\u63d0\u9ad8\u6e2c\u8a66\u6642\u7684\u6548\u80fd\u2014\u2014\u4e5f\u5c31\u662f\u8aaa\uff0c\u900f\u904e\u7522\u751f\u66f4\u9577\u7684\u601d\u7dad\u93c8\u5e8f\u5217\uff0c\u5f9e\u800c\u4f7f\u7528\u66f4\u591a\u7684\u8a08\u7b97\u8cc7\u6e90\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u601d\u7dad\u93c8\u9577\u5ea6\u662f\u4e0d\u53ef\u63a7\u7684\uff0c\u9019\u4f7f\u5f97\u6211\u5011\u7121\u6cd5\u5206\u914d\u6e2c\u8a66\u6642\u7684\u8a08\u7b97\u8cc7\u6e90\u4f86\u9054\u5230\u9810\u671f\u7684\u6548\u80fd\u6c34\u6e96\u3002\u6211\u5011\u5f15\u5165\u4e86\u9577\u5ea6\u63a7\u5236\u7b56\u7565\u512a\u5316 (LCPO)\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u7684\u5f37\u5316\u5b78\u7fd2\u65b9\u6cd5\uff0c\u7528\u65bc\u512a\u5316\u6e96\u78ba\u6027\u548c\u9075\u5b88\u4f7f\u7528\u8005\u6307\u5b9a\u7684\u9577\u5ea6\u9650\u5236\u3002\u6211\u5011\u4f7f\u7528 LCPO \u4f86\u8a13\u7df4 L1\uff0c\u9019\u662f\u4e00\u500b\u63a8\u7406\u8a9e\u8a00\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u7522\u751f\u6eff\u8db3\u63d0\u793a\u4e2d\u7d66\u5b9a\u9577\u5ea6\u9650\u5236\u7684\u8f38\u51fa\u3002L1 \u7684\u9577\u5ea6\u63a7\u5236\u5141\u8a31\u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u5e73\u6ed1\u5730\u6b0a\u8861\u8a08\u7b97\u6210\u672c\u548c\u6e96\u78ba\u6027\uff0c\u4e26\u4e14\u5728\u9577\u5ea6\u63a7\u5236\u65b9\u9762\u512a\u65bc\u6700\u5148\u9032\u7684 S1 \u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u4f7f\u7528 LCPO \u8a13\u7df4\u7684\u6a21\u578b\u4e2d\u767c\u73fe\u4e86\u4e00\u7a2e\u610f\u60f3\u4e0d\u5230\u7684\u77ed\u601d\u7dad\u93c8\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684 1.5B L1 \u6a21\u578b\u5728\u76f8\u540c\u7684\u63a8\u7406\u9577\u5ea6\u4e0b\u8d85\u8d8a\u4e86 GPT-4o\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cLCPO \u53ef\u4ee5\u7cbe\u78ba\u63a7\u5236\u63a8\u7406\u9577\u5ea6\uff0c\u5f9e\u800c\u53ef\u4ee5\u7cbe\u7d30\u5730\u5206\u914d\u6e2c\u8a66\u6642\u8a08\u7b97\u8cc7\u6e90\u548c\u6e96\u78ba\u6027\u3002\u6211\u5011\u5728 https://www.cmu-l3.github.io/l1 \u767c\u5e03\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b</paragraph>\n", "author": "Pranjal Aggarwal et.al.", "authors": "Pranjal Aggarwal, Sean Welleck", "id": "2503.04697v1", "paper_url": "http://arxiv.org/abs/2503.04697v1", "repo": "null"}}