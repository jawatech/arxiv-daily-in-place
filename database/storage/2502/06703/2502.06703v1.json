{"2502.06703": {"publish_time": "2025-02-10", "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling", "paper_summary": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.", "paper_summary_zh": "\u6e2c\u8a66\u6642\u7e2e\u653e (TTS) \u662f\u4e00\u7a2e\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u53ef\u900f\u904e\u5728\u63a8\u8ad6\u968e\u6bb5\u4f7f\u7528\u984d\u5916\u904b\u7b97\u4f86\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u6240\u6c92\u6709\u7cfb\u7d71\u6027\u5730\u5206\u6790\u653f\u7b56\u6a21\u578b\u3001\u7a0b\u5e8f\u734e\u52f5\u6a21\u578b (PRM) \u548c\u554f\u984c\u96e3\u5ea6\u5982\u4f55\u5f71\u97ff TTS\u3002\u9019\u7a2e\u5206\u6790\u7684\u7f3a\u4e4f\u9650\u5236\u4e86\u5c0d TTS \u65b9\u6cd5\u7684\u7406\u89e3\u548c\u5be6\u969b\u4f7f\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u5169\u500b\u6838\u5fc3\u554f\u984c\uff1a(1) \u4ec0\u9ebc\u662f\u8de8\u4e0d\u540c\u653f\u7b56\u6a21\u578b\u3001PRM \u548c\u554f\u984c\u96e3\u5ea6\u5c64\u7d1a\u7e2e\u653e\u6e2c\u8a66\u6642\u904b\u7b97\u7684\u6700\u4f73\u65b9\u6cd5\uff1f(2) \u64f4\u5c55\u904b\u7b97\u53ef\u4ee5\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u63d0\u5347 LLM \u5728\u8907\u96dc\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\uff0c\u8f03\u5c0f\u7684\u8a9e\u8a00\u6a21\u578b\u662f\u5426\u80fd\u900f\u904e\u9019\u7a2e\u65b9\u6cd5\u52dd\u904e\u8f03\u5927\u7684\u6a21\u578b\uff1f\u900f\u904e\u5c0d MATH-500 \u548c\u5177\u6709\u6311\u6230\u6027\u7684 AIME24 \u4efb\u52d9\u9032\u884c\u5168\u9762\u7684\u5be6\u9a57\uff0c\u6211\u5011\u6709\u4ee5\u4e0b\u89c0\u5bdf\uff1a(1) \u6700\u4f73\u904b\u7b97 TTS \u7b56\u7565\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u653f\u7b56\u6a21\u578b\u3001PRM \u548c\u554f\u984c\u96e3\u5ea6\u7684\u9078\u64c7\u3002(2) \u4f7f\u7528\u6211\u5011\u6700\u4f73\u904b\u7b97 TTS \u7b56\u7565\uff0c\u6975\u5c0f\u7684\u653f\u7b56\u6a21\u578b\u53ef\u4ee5\u52dd\u904e\u8f03\u5927\u7684\u6a21\u578b\u3002\u4f8b\u5982\uff0c1B LLM \u53ef\u4ee5\u8d85\u904e MATH-500 \u4e0a\u7684 405B LLM\u3002\u6b64\u5916\uff0c\u5728 MATH-500 \u548c AIME24 \u4e0a\uff0c0.5B LLM \u52dd\u904e GPT-4o\uff0c3B LLM \u52dd\u904e 405B LLM\uff0c7B LLM \u52dd\u904e o1 \u548c DeepSeek-R1\uff0c\u540c\u6642\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u8ad6\u6548\u7387\u3002\u9019\u4e9b\u767c\u73fe\u986f\u793a\u4e86\u8abf\u6574 TTS \u7b56\u7565\u4ee5\u9069\u61c9\u6bcf\u500b\u4efb\u52d9\u548c\u6a21\u578b\u7684\u7279\u5b9a\u7279\u5fb5\u7684\u91cd\u8981\u6027\uff0c\u4e26\u8868\u660e TTS \u662f\u589e\u5f37 LLM \u63a8\u7406\u80fd\u529b\u7684\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002", "author": "Runze Liu et.al.", "authors": "Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou", "id": "2502.06703v1", "paper_url": "http://arxiv.org/abs/2502.06703v1", "repo": "null"}}