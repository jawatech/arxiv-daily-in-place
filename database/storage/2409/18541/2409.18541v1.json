{"2409.18541": {"publish_time": "2024-09-27", "title": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation", "paper_summary": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the total training sample size from 158k\nto 14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u6700\u65b0\u8fdb\u5c55\uff08\u4f8b\u5982 LLaVA \u7cfb\u5217\u6a21\u578b\uff09\u7531\u5927\u91cf\u673a\u5668\u751f\u6210\u7684\u6307\u4ee4\u9075\u5faa\u6570\u636e\u8c03\u4f18\u63a8\u52a8\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u81ea\u52a8\u6307\u4ee4\u6536\u96c6\u7ba1\u9053\u5728\u6570\u636e\u8d28\u91cf\u4e2d\u65e0\u610f\u4e2d\u5f15\u5165\u4e86\u663e\u8457\u7684\u53ef\u53d8\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6307\u4ee4\u6574\u7406\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6e90\u81ea\u4eba\u7c7b\u548c LLM \u504f\u597d\u5bf9\u9f50\u7684\u4e24\u4e2a\u72ec\u7279\u89c6\u89d2\uff0c\u4ee5\u5c06\u5927\u91cf\u673a\u5668\u751f\u6210\u7684\u6a21\u6001\u6307\u4ee4\u538b\u7f29\u6210\u7d27\u51d1\u4e14\u9ad8\u8d28\u91cf\u7684\u5f62\u5f0f\uff1a(i) \u5bf9\u4e8e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u673a\u5668\u751f\u6210\u7684\u6a21\u6001\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u5957\u5168\u9762\u7684\u4e3b\u89c2\u548c\u5ba2\u89c2\u6807\u51c6\uff0c\u4ee5\u6307\u5bfc\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\uff0c\u5e76\u4e25\u683c\u5730\u4ece\u4eba\u7c7b\u4e13\u5bb6\u90a3\u91cc\u83b7\u5f97\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u5728\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u5c06\u7ec6\u5fae\u7684\u4eba\u7c7b\u6307\u4ee4\u5bf9\u9f50\u7406\u89e3\u5185\u5316\u3002(ii) \u5bf9\u4e8e LLM \u504f\u597d\u5bf9\u9f50\uff0c\u9274\u4e8e\u5956\u52b1\u6a21\u578b\u9009\u62e9\u7684\u6307\u4ee4\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528 MLLM \u4e2d\u4f7f\u7528\u7684\u5185\u90e8 LLM \u5c06\u89c6\u89c9\u6307\u4ee4\u7684\u5199\u4f5c\u98ce\u683c\u4e0e\u5185\u90e8 LLM \u672c\u8eab\u5bf9\u9f50\uff0c\u4ece\u800c\u5b9e\u73b0 LLM \u5bf9\u9f50\u7684\u6307\u4ee4\u6539\u8fdb\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5c06\u5408\u6210\u6a21\u6001\u6307\u4ee4\u538b\u7f29\u591a\u8fbe 90% \u6765\u7ef4\u6301\u751a\u81f3\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u662f\uff0c\u901a\u8fc7\u5c06\u603b\u8bad\u7ec3\u6837\u672c\u91cf\u4ece 158k \u79ef\u6781\u51cf\u5c11\u5230 14k\uff08\u5c0f 9 \u500d\uff09\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5404\u79cd MLLM \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5176\u5168\u5c3a\u5bf8\u6570\u636e\u96c6\u5bf9\u5e94\u6a21\u578b\u3002\u6211\u4eec\u7684\u9879\u76ee\u53ef\u5728 https://github.com/DCDmllm/Align2LLaVA \u83b7\u5f97\u3002", "author": "Hongzhe Huang et.al.", "authors": "Hongzhe Huang, Zhewen Yu, Jiang Liu, Li Cai, Dian Jiao, Wenqiao Zhang, Siliang Tang, Juncheng Li, Hao Jiang, Haoyuan Li, Yueting Zhuang", "id": "2409.18541v1", "paper_url": "http://arxiv.org/abs/2409.18541v1", "repo": "null"}}