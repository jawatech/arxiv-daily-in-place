{"2406.11566": {"publish_time": "2024-06-17", "title": "MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation", "paper_summary": "Knowledge editing aims to adjust the knowledge within large language models\n(LLMs) to prevent their responses from becoming obsolete or inaccurate.\nHowever, existing works on knowledge editing are primarily conducted in a\nsingle language, which is inadequate for multilingual language models. In this\npaper, we focus on multilingual knowledge editing (MKE), which requires\npropagating updates across multiple languages. This necessity poses a\nsignificant challenge for the task. Furthermore, the limited availability of a\ncomprehensive dataset for MKE exacerbates this challenge, hindering progress in\nthis area. Hence, we introduce the Multilingual Knowledge Editing Benchmark\n(MKEB), a novel dataset comprising 12 languages and providing a complete\nevaluation framework. Additionally, we propose a method that enhances\nMultilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA).\nSpecifically, we identify two categories of knowledge neurons to improve\nediting precision. Moreover, we perform LoRA-based editing with neuron masks to\nefficiently modify parameters and facilitate the propagation of updates across\nmultiple languages. Experiments demonstrate that our method outperforms\nexisting baselines and significantly enhances the multi-hop reasoning\ncapability of the edited model, with minimal impact on its downstream task\nperformance. The dataset and code will be made publicly available.", "paper_summary_zh": "\u77e5\u8b58\u7de8\u8f2f\u65e8\u5728\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u77e5\u8b58\uff0c\u4ee5\u9632\u6b62\u5176\u56de\u61c9\u904e\u6642\u6216\u4e0d\u6e96\u78ba\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u77e5\u8b58\u7de8\u8f2f\u5de5\u4f5c\u4e3b\u8981\u4ee5\u55ae\u4e00\u8a9e\u8a00\u9032\u884c\uff0c\u9019\u5c0d\u65bc\u591a\u8a9e\u8a00\u8a9e\u8a00\u6a21\u578b\u4f86\u8aaa\u662f\u4e0d\u5920\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u591a\u8a9e\u8a00\u77e5\u8b58\u7de8\u8f2f (MKE)\uff0c\u9019\u9700\u8981\u8de8\u591a\u7a2e\u8a9e\u8a00\u50b3\u64ad\u66f4\u65b0\u3002\u9019\u7a2e\u5fc5\u8981\u6027\u5c0d\u9019\u9805\u4efb\u52d9\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\u3002\u6b64\u5916\uff0c\u7d9c\u5408 MKE \u6578\u64da\u96c6\u7684\u53ef\u7528\u6027\u6709\u9650\uff0c\u52a0\u5287\u4e86\u9019\u4e00\u6311\u6230\uff0c\u963b\u7919\u4e86\u8a72\u9818\u57df\u7684\u9032\u5c55\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u8a9e\u8a00\u77e5\u8b58\u7de8\u8f2f\u57fa\u6e96 (MKEB)\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 12 \u7a2e\u8a9e\u8a00\u7684\u65b0\u7a4e\u6578\u64da\u96c6\uff0c\u4e26\u63d0\u4f9b\u4e86\u4e00\u500b\u5b8c\u6574\u7684\u8a55\u4f30\u6846\u67b6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u4f7f\u7528\u795e\u7d93\u5143\u63a9\u78bc\u4f4e\u79e9\u9069\u61c9 (MEMLA) \u589e\u5f37\u4e86\u591a\u8a9e\u8a00\u77e5\u8b58\u7de8\u8f2f\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8b58\u5225\u51fa\u5169\u985e\u77e5\u8b58\u795e\u7d93\u5143\u4ee5\u63d0\u9ad8\u7de8\u8f2f\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528\u795e\u7d93\u5143\u63a9\u78bc\u57f7\u884c\u57fa\u65bc LoRA \u7684\u7de8\u8f2f\uff0c\u4ee5\u6709\u6548\u4fee\u6539\u53c3\u6578\u4e26\u4fc3\u9032\u8de8\u591a\u7a2e\u8a9e\u8a00\u7684\u66f4\u65b0\u50b3\u64ad\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u73fe\u6709\u7684\u57fa\u6e96\uff0c\u4e26\u986f\u8457\u589e\u5f37\u4e86\u5df2\u7de8\u8f2f\u6a21\u578b\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u540c\u6642\u5c0d\u5176\u4e0b\u6e38\u4efb\u52d9\u6027\u80fd\u7684\u5f71\u97ff\u5f88\u5c0f\u3002\u8a72\u6578\u64da\u96c6\u548c\u4ee3\u78bc\u5c07\u516c\u958b\u767c\u5e03\u3002", "author": "Jiakuan Xie et.al.", "authors": "Jiakuan Xie, Pengfei Cao, Yuheng Chen, Yubo Chen, Kang Liu, Jun Zhao", "id": "2406.11566v1", "paper_url": "http://arxiv.org/abs/2406.11566v1", "repo": "null"}}