{"2407.15793": {"publish_time": "2024-07-22", "title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning", "paper_summary": "With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.", "paper_summary_zh": "\u96a8\u8457 Transformers \u548c CLIP \u7b49\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73fe\uff0c\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\u5df2\u6210\u70ba\u5728\u6301\u7e8c\u5b78\u7fd2\u5834\u666f\u4e2d\u589e\u5f37\u6548\u80fd\u7684\u5e38\u898b\u7b56\u7565\u3002\u9019\u5c0e\u81f4\u4e86\u8a31\u591a\u63d0\u793a\u7b56\u7565\u7684\u958b\u767c\uff0c\u4ee5\u6709\u6548\u5fae\u8abf\u57fa\u65bc\u8f49\u63db\u5668\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u6703\u5c48\u670d\u65bc\u707d\u96e3\u6027\u907a\u5fd8\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u96e3\u4ee5\u5c07\u6a21\u578b\u5c08\u9580\u5316\u5230\u8207\u9810\u8a13\u7df4\u986f\u8457\u4e0d\u540c\u7684\u9818\u57df\uff0c\u4e26\u4fdd\u7559\u5176\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u6301\u7e8c\u751f\u6210\u8a13\u7df4\u4ee5\u9032\u884c\u589e\u91cf\u63d0\u793a\u5b78\u7fd2\uff0c\u9019\u662f\u4e00\u7a2e\u6e1b\u8f15\u907a\u5fd8\u540c\u6642\u9069\u61c9 VLM \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u751f\u6210\u91cd\u64ad\u5c07\u63d0\u793a\u8207\u4efb\u52d9\u5c0d\u9f4a\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u6307\u6a19\u4f86\u8a55\u4f30 CL \u57fa\u6e96\u4e2d\u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u3002\u901a\u904e\u5728\u4e0d\u540c\u9818\u57df\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684\u6846\u67b6\u5728\u9069\u61c9\u65b0\u4efb\u52d9\u540c\u6642\u63d0\u9ad8\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u5f4c\u5408\u7406\u8ad6\u63d0\u793a\u8abf\u6574\u7684\u5dee\u8ddd\u3002\u7a0b\u5f0f\u78bc\u5eab\u53ef\u5728 https://github.com/aimagelab/mammoth \u53d6\u5f97\u3002", "author": "Emanuele Frascaroli et.al.", "authors": "Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara", "id": "2407.15793v1", "paper_url": "http://arxiv.org/abs/2407.15793v1", "repo": "https://github.com/aimagelab/mammoth"}}