{"2411.15113": {"publish_time": "2024-11-22", "title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion", "paper_summary": "As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems", "paper_summary_zh": "\u96a8\u8457\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u8d8a\u4f86\u8d8a\u5f37\u5927\u4e14\u8907\u96dc\uff0c\u5176\u9f90\u5927\u7684\u898f\u6a21\u5c0d\u5ee3\u6cdb\u63a1\u7528\u69cb\u6210\u91cd\u5927\u969c\u7919\uff0c\u7279\u5225\u662f\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u3002\u672c\u6587\u91dd\u5c0d Stable Diffusion 2 \u7684\u8a13\u7df4\u5f8c\u526a\u679d\u63d0\u51fa\u958b\u5275\u6027\u7684\u7814\u7a76\uff0c\u63a2\u8a0e\u6587\u5b57\u8f49\u5716\u50cf\u9818\u57df\u4e2d\u6a21\u578b\u58d3\u7e2e\u7684\u95dc\u9375\u9700\u6c42\u3002\u6211\u5011\u7684\u7814\u7a76\u63a2\u8a0e\u5148\u524d\u672a\u63a2\u7d22\u7684\u591a\u6a21\u5f0f\u751f\u6210\u6a21\u578b\u7684\u526a\u679d\u6280\u8853\uff0c\u4e26\u7279\u5225\u5206\u5225\u63a2\u8a0e\u526a\u679d\u5c0d\u6587\u5b57\u5143\u4ef6\u548c\u5f71\u50cf\u751f\u6210\u5143\u4ef6\u7684\u5f71\u97ff\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u7a00\u758f\u5ea6\u7684\u6a21\u578b\u6216\u6a21\u578b\u55ae\u4e00\u5143\u4ef6\u526a\u679d\u9032\u884c\u5168\u9762\u6bd4\u8f03\u3002\u6211\u5011\u7684\u7d50\u679c\u7522\u751f\u4e86\u5148\u524d\u672a\u8a18\u9304\u7684\u767c\u73fe\u3002\u4f8b\u5982\uff0c\u8207\u8a9e\u8a00\u6a21\u578b\u526a\u679d\u4e2d\u7684\u65e2\u5b9a\u8da8\u52e2\u76f8\u53cd\uff0c\u6211\u5011\u767c\u73fe\u7c21\u55ae\u7684\u5e45\u5ea6\u526a\u679d\u5728\u6587\u5b57\u8f49\u5716\u50cf\u7684\u80cc\u666f\u4e0b\u512a\u65bc\u66f4\u9032\u968e\u7684\u6280\u8853\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7d50\u679c\u986f\u793a Stable Diffusion 2 \u53ef\u4ee5\u526a\u679d\u5230 38.5% \u7684\u7a00\u758f\u5ea6\uff0c\u4e14\u54c1\u8cea\u640d\u5931\u6975\u5c0f\uff0c\u5927\u5e45\u7e2e\u5c0f\u6a21\u578b\u898f\u6a21\u3002\u6211\u5011\u63d0\u51fa\u6700\u4f73\u526a\u679d\u914d\u7f6e\uff0c\u5c07\u6587\u5b57\u7de8\u78bc\u5668\u526a\u679d\u5230 47.5%\uff0c\u64f4\u6563\u751f\u6210\u5668\u526a\u679d\u5230 35%\u3002\u6b64\u914d\u7f6e\u7dad\u6301\u5f71\u50cf\u751f\u6210\u54c1\u8cea\uff0c\u540c\u6642\u5927\u5e45\u964d\u4f4e\u904b\u7b97\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u4e5f\u63ed\u9732\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u4e2d\u8cc7\u8a0a\u7de8\u78bc\u7684\u6709\u8da3\u554f\u984c\uff1a\u6211\u5011\u89c0\u5bdf\u5230\u526a\u679d\u8d85\u904e\u7279\u5b9a\u95be\u503c\u6703\u5c0e\u81f4\u6548\u80fd\u7a81\u7136\u4e0b\u964d\uff08\u7121\u6cd5\u8fa8\u8b58\u7684\u5f71\u50cf\uff09\uff0c\u9019\u8868\u793a\u7279\u5b9a\u7684\u6b0a\u91cd\u7de8\u78bc\u95dc\u9375\u7684\u8a9e\u610f\u8cc7\u8a0a\u3002\u6b64\u767c\u73fe\u70ba\u672a\u4f86\u5728\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u58d3\u7e2e\u3001\u4e92\u64cd\u4f5c\u6027\u4ee5\u53ca\u504f\u5dee\u8b58\u5225\u7684\u7814\u7a76\u958b\u555f\u65b0\u9014\u5f91\u3002\u900f\u904e\u63d0\u4f9b\u5c0d\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u526a\u679d\u884c\u70ba\u7684\u91cd\u8981\u898b\u89e3\uff0c\u6211\u5011\u7684\u7814\u7a76\u70ba\u958b\u767c\u66f4\u6709\u6548\u7387\u4e14\u6613\u65bc\u53d6\u5f97\u7684 AI \u9a45\u52d5\u5f71\u50cf\u751f\u6210\u7cfb\u7d71\u5960\u5b9a\u57fa\u790e", "author": "Samarth N Ramesh et.al.", "authors": "Samarth N Ramesh, Zhixue Zhao", "id": "2411.15113v1", "paper_url": "http://arxiv.org/abs/2411.15113v1", "repo": "null"}}