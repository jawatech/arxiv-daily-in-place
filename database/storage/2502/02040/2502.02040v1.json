{"2502.02040": {"publish_time": "2025-02-04", "title": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference", "paper_summary": "Residual transformations enhance the representational depth and expressive\npower of large language models (LLMs). However, applying static residual\ntransformations across all tokens in auto-regressive generation leads to a\nsuboptimal trade-off between inference efficiency and generation fidelity.\nExisting methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth\naddress this by modulating the residual transformation based on token-level\ncomplexity. Nevertheless, these approaches predominantly consider the distance\ntraversed by tokens through the model layers, neglecting the underlying\nvelocity of residual evolution. We introduce Mixture of Multi-rate Residuals\n(M2R2), a framework that dynamically modulates residual velocity to improve\nearly alignment, enhancing inference efficiency. Evaluations on reasoning\noriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2\nsurpasses state-of-the-art distance-based strategies, balancing generation\nquality and speedup. In self-speculative decoding setup, M2R2 achieves up to\n2.8x speedups on MT-Bench, outperforming methods like 2-model speculative\ndecoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE)\narchitectures, integrating early residual alignment with ahead-of-time expert\nloading into high-bandwidth memory (HBM) accelerates decoding, reduces\nexpert-switching bottlenecks, and achieves a 2.9x speedup, making it highly\neffective in resource-constrained environments.", "paper_summary_zh": "\u6b98\u5dee\u8f49\u63db\u589e\u5f37\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8868\u5fb5\u6df1\u5ea6\u548c\u8868\u9054\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u81ea\u8ff4\u6b78\u751f\u6210\u4e2d\u5c0d\u6240\u6709\u7b26\u865f\u61c9\u7528\u975c\u614b\u6b98\u5dee\u8f49\u63db\u6703\u5c0e\u81f4\u63a8\u7406\u6548\u7387\u548c\u751f\u6210\u4fdd\u771f\u5ea6\u4e4b\u9593\u7684\u6b0a\u8861\u4e0d\u4f73\u3002\u73fe\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u65e9\u671f\u9000\u51fa\u3001\u8df3\u904e\u89e3\u78bc\u548c\u6df7\u5408\u6df1\u5ea6\uff0c\u901a\u904e\u6839\u64da\u7b26\u865f\u5c64\u7d1a\u8907\u96dc\u5ea6\u8abf\u7bc0\u6b98\u5dee\u8f49\u63db\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u8003\u616e\u7b26\u865f\u901a\u904e\u6a21\u578b\u5c64\u6240\u8de8\u8d8a\u7684\u8ddd\u96e2\uff0c\u800c\u5ffd\u7565\u4e86\u6b98\u5dee\u6f14\u5316\u7684\u57fa\u672c\u901f\u5ea6\u3002\u6211\u5011\u5f15\u5165\u4e86\u591a\u901f\u7387\u6b98\u5dee\u6df7\u5408 (M2R2)\uff0c\u9019\u662f\u4e00\u500b\u52d5\u614b\u8abf\u7bc0\u6b98\u5dee\u901f\u5ea6\u7684\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u65e9\u671f\u5c0d\u9f4a\uff0c\u589e\u5f37\u63a8\u7406\u6548\u7387\u3002\u5728\u9762\u5411\u63a8\u7406\u7684\u4efb\u52d9\uff08\u4f8b\u5982 Koala\u3001Self-Instruct\u3001WizardLM \u548c MT-Bench\uff09\u4e0a\u7684\u8a55\u4f30\u8868\u660e\uff0cM2R2 \u8d85\u8d8a\u4e86\u6700\u5148\u9032\u7684\u57fa\u65bc\u8ddd\u96e2\u7684\u7b56\u7565\uff0c\u5e73\u8861\u4e86\u751f\u6210\u54c1\u8cea\u548c\u52a0\u901f\u3002\u5728\u81ea\u63a8\u6e2c\u89e3\u78bc\u8a2d\u7f6e\u4e2d\uff0cM2R2 \u5728 MT-Bench \u4e0a\u5be6\u73fe\u4e86\u9ad8\u9054 2.8 \u500d\u7684\u52a0\u901f\uff0c\u512a\u65bc 2 \u6a21\u578b\u63a8\u6e2c\u89e3\u78bc\u3001Medusa\u3001LookAhead \u89e3\u78bc\u548c DEED \u7b49\u65b9\u6cd5\u3002\u5728\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\u4e2d\uff0c\u5c07\u65e9\u671f\u6b98\u5dee\u5c0d\u9f4a\u8207\u9810\u5148\u5c08\u5bb6\u8f09\u5165\u5230\u9ad8\u983b\u5bec\u8a18\u61b6\u9ad4 (HBM) \u4e2d\u6574\u5408\uff0c\u53ef\u52a0\u901f\u89e3\u78bc\uff0c\u6e1b\u5c11\u5c08\u5bb6\u5207\u63db\u74f6\u9838\uff0c\u4e26\u5be6\u73fe 2.9 \u500d\u7684\u52a0\u901f\uff0c\u4f7f\u5176\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u975e\u5e38\u6709\u6548\u3002", "author": "Nikhil Bhendawade et.al.", "authors": "Nikhil Bhendawade, Mahyar Najibi, Devang Naik, Irina Belousova", "id": "2502.02040v1", "paper_url": "http://arxiv.org/abs/2502.02040v1", "repo": "null"}}