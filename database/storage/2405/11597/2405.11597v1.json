{"2405.11597": {"publish_time": "2024-05-19", "title": "Language Reconstruction with Brain Predictive Coding from fMRI Data", "paper_summary": "Many recent studies have shown that the perception of speech can be decoded\nfrom brain signals and subsequently reconstructed as continuous language.\nHowever, there is a lack of neurological basis for how the semantic information\nembedded within brain signals can be used more effectively to guide language\nreconstruction. The theory of predictive coding suggests that human brain\nnaturally engages in continuously predicting future word representations that\nspan multiple timescales. This implies that the decoding of brain signals could\npotentially be associated with a predictable future. To explore the predictive\ncoding theory within the context of language reconstruction, this paper\nproposes a novel model \\textsc{PredFT} for jointly modeling neural decoding and\nbrain prediction. It consists of a main decoding network for language\nreconstruction and a side network for predictive coding. The side network\nobtains brain predictive coding representation from related brain regions of\ninterest with a multi-head self-attention module. This representation is fused\ninto the main decoding network with cross-attention to facilitate the language\nmodels' generation process. Experiments are conducted on the largest\nnaturalistic language comprehension fMRI dataset Narratives. \\textsc{PredFT}\nachieves current state-of-the-art decoding performance with a maximum BLEU-1\nscore of $27.8\\%$.", "paper_summary_zh": "\u8a31\u591a\u8fd1\u671f\u7814\u7a76\u986f\u793a\uff0c\u53ef\u4ee5\u5f9e\u8166\u90e8\u8a0a\u865f\u89e3\u78bc\u8a00\u8a9e\u611f\u77e5\uff0c\u4e26\u9032\u4e00\u6b65\u91cd\u5efa\u70ba\u9023\u7e8c\u7684\u8a9e\u8a00\u3002\n\u7136\u800c\uff0c\u7f3a\u4e4f\u795e\u7d93\u5b78\u57fa\u790e\u4f86\u89e3\u91cb\u5982\u4f55\u66f4\u6709\u6548\u5730\u4f7f\u7528\u5d4c\u5165\u5728\u8166\u90e8\u8a0a\u865f\u4e2d\u7684\u8a9e\u7fa9\u8cc7\u8a0a\uff0c\u4ee5\u5f15\u5c0e\u8a9e\u8a00\u91cd\u5efa\u3002\n\u9810\u6e2c\u7de8\u78bc\u7406\u8ad6\u8868\u660e\uff0c\u4eba\u985e\u5927\u8166\u6703\u81ea\u7136\u5730\u6301\u7e8c\u9810\u6e2c\u8de8\u8d8a\u591a\u500b\u6642\u9593\u5c3a\u5ea6\u7684\u672a\u4f86\u8a5e\u5f59\u8868\u5fb5\u3002\n\u9019\u8868\u793a\u8166\u90e8\u8a0a\u865f\u7684\u89e3\u78bc\u53ef\u80fd\u8207\u53ef\u9810\u6e2c\u7684\u672a\u4f86\u6709\u95dc\u3002\n\u70ba\u4e86\u5728\u8a9e\u8a00\u91cd\u5efa\u7684\u8108\u7d61\u4e2d\u63a2\u7d22\u9810\u6e2c\u7de8\u78bc\u7406\u8ad6\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u6a21\u578b \\textsc{PredFT}\uff0c\u7528\u65bc\u806f\u5408\u5efa\u6a21\u795e\u7d93\u89e3\u78bc\u548c\u8166\u90e8\u9810\u6e2c\u3002\n\u5b83\u5305\u542b\u4e00\u500b\u7528\u65bc\u8a9e\u8a00\u91cd\u5efa\u7684\u4e3b\u8981\u89e3\u78bc\u7db2\u8def\uff0c\u4ee5\u53ca\u4e00\u500b\u7528\u65bc\u9810\u6e2c\u7de8\u78bc\u7684\u8f14\u52a9\u7db2\u8def\u3002\n\u8f14\u52a9\u7db2\u8def\u5f9e\u76f8\u95dc\u7684\u611f\u8208\u8da3\u8166\u5340\u53d6\u5f97\u8166\u90e8\u9810\u6e2c\u7de8\u78bc\u8868\u5fb5\uff0c\u4e26\u4f7f\u7528\u591a\u982d\u81ea\u6211\u6ce8\u610f\u529b\u6a21\u7d44\u3002\n\u6b64\u8868\u5fb5\u8207\u4e3b\u8981\u89e3\u78bc\u7db2\u8def\u878d\u5408\uff0c\u4e26\u900f\u904e\u4ea4\u53c9\u6ce8\u610f\u529b\u4f86\u4fc3\u9032\u8a9e\u8a00\u6a21\u578b\u7684\u7522\u751f\u904e\u7a0b\u3002\n\u5be6\u9a57\u662f\u5728\u6700\u5927\u7684\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 fMRI \u8cc7\u6599\u96c6 Narratives \u4e0a\u9032\u884c\u3002\\textsc{PredFT} \u9054\u5230\u76ee\u524d\u6700\u5148\u9032\u7684\u89e3\u78bc\u6548\u80fd\uff0c\u5176 BLEU-1 \u6700\u9ad8\u5206\u70ba $27.8\\%$\u3002", "author": "Congchi Yin et.al.", "authors": "Congchi Yin, Ziyi Ye, Piji Li", "id": "2405.11597v1", "paper_url": "http://arxiv.org/abs/2405.11597v1", "repo": "null"}}