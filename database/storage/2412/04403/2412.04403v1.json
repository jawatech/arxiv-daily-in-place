{"2412.04403": {"publish_time": "2024-12-05", "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders", "paper_summary": "We develop task scaling laws and model ladders to predict the individual task\nperformance of pretrained language models (LMs) in the overtrained setting.\nStandard power laws for language modeling loss cannot accurately model task\nperformance. Therefore, we leverage a two-step prediction approach: first use\nmodel and data size to predict a task-specific loss, and then use this task\nloss to predict task performance. We train a set of small-scale \"ladder\"\nmodels, collect data points to fit the parameterized functions of the two\nprediction steps, and make predictions for two target models: a 7B model\ntrained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder\nmodels only costs 1% of the compute used for the target models. On four\nmultiple-choice tasks written in ranked classification format, we can predict\nthe accuracy of both target models within 2 points of absolute error. We have\nhigher prediction error on four other tasks (average absolute error 6.9) and\nfind that these are often tasks with higher variance in task metrics. We also\nfind that using less compute to train fewer ladder models tends to deteriorate\npredictions. Finally, we empirically show that our design choices and the\ntwo-step approach lead to superior performance in establishing scaling laws.", "paper_summary_zh": "<paragraph>\u6211\u5011\u958b\u767c\u4efb\u52d9\u64f4\u5145\u6cd5\u5247\u548c\u6a21\u578b\u968e\u68af\uff0c\u4ee5\u9810\u6e2c\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (LM) \u5728\u904e\u5ea6\u8a13\u7df4\u8a2d\u5b9a\u4e2d\u7684\u500b\u5225\u4efb\u52d9\u57f7\u884c\u6548\u80fd\u3002\n\u8a9e\u8a00\u6a21\u578b\u640d\u5931\u7684\u6a19\u6e96\u51aa\u6b21\u6cd5\u5247\u7121\u6cd5\u7cbe\u6e96\u6a21\u64ec\u4efb\u52d9\u6548\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5229\u7528\u5169\u6b65\u9a5f\u9810\u6e2c\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u6a21\u578b\u548c\u8cc7\u6599\u5927\u5c0f\u9810\u6e2c\u7279\u5b9a\u4efb\u52d9\u7684\u640d\u5931\uff0c\u7136\u5f8c\u4f7f\u7528\u6b64\u4efb\u52d9\u640d\u5931\u9810\u6e2c\u4efb\u52d9\u6548\u80fd\u3002\u6211\u5011\u8a13\u7df4\u4e00\u7d44\u5c0f\u898f\u6a21\u300c\u968e\u68af\u300d\u6a21\u578b\uff0c\u6536\u96c6\u8cc7\u6599\u9ede\u4ee5\u7b26\u5408\u5169\u500b\u9810\u6e2c\u6b65\u9a5f\u7684\u53c3\u6578\u5316\u51fd\u6578\uff0c\u4e26\u5c0d\u5169\u500b\u76ee\u6a19\u6a21\u578b\u9032\u884c\u9810\u6e2c\uff1a\u4e00\u500b\u8a13\u7df4\u81f3 4T \u4ee4\u724c\u7684 7B \u6a21\u578b\u548c\u4e00\u500b\u8a13\u7df4\u81f3 5T \u4ee4\u724c\u7684 13B \u6a21\u578b\u3002\u8a13\u7df4\u968e\u68af\u6a21\u578b\u50c5\u8017\u8cbb\u76ee\u6a19\u6a21\u578b\u904b\u7b97\u7684 1%\u3002\u5728\u56db\u500b\u4ee5\u6392\u540d\u5206\u985e\u683c\u5f0f\u64b0\u5beb\u7684\u591a\u91cd\u9078\u64c7\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u53ef\u4ee5\u9810\u6e2c\u5169\u500b\u76ee\u6a19\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\uff0c\u7d55\u5c0d\u8aa4\u5dee\u5728 2 \u9ede\u4ee5\u5167\u3002\u6211\u5011\u5728\u5176\u4ed6\u56db\u500b\u4efb\u52d9\u4e0a\u5177\u6709\u8f03\u9ad8\u7684\u9810\u6e2c\u8aa4\u5dee\uff08\u5e73\u5747\u7d55\u5c0d\u8aa4\u5dee 6.9\uff09\uff0c\u4e26\u767c\u73fe\u9019\u4e9b\u4efb\u52d9\u901a\u5e38\u662f\u4efb\u52d9\u6307\u6a19\u8b8a\u7570\u8f03\u9ad8\u7684\u4efb\u52d9\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u4f7f\u7528\u8f03\u5c11\u7684\u904b\u7b97\u8a13\u7df4\u8f03\u5c11\u7684\u968e\u68af\u6a21\u578b\u5f80\u5f80\u6703\u5c0e\u81f4\u9810\u6e2c\u60e1\u5316\u3002\u6700\u5f8c\uff0c\u6211\u5011\u900f\u904e\u7d93\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u8a2d\u8a08\u9078\u64c7\u548c\u5169\u6b65\u9a5f\u65b9\u6cd5\u6709\u52a9\u65bc\u5728\u5efa\u7acb\u64f4\u5145\u6cd5\u5247\u6642\u7372\u5f97\u512a\u7570\u7684\u6548\u80fd\u3002</paragraph>", "author": "Akshita Bhagia et.al.", "authors": "Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, Hannaneh Hajishirzi", "id": "2412.04403v1", "paper_url": "http://arxiv.org/abs/2412.04403v1", "repo": "null"}}