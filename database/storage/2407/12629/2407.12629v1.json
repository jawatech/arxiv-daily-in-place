{"2407.12629": {"publish_time": "2024-07-17", "title": "A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality", "paper_summary": "Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.", "paper_summary_zh": "\u81ea\u9069\u61c9\u68af\u5ea6\u4e0b\u964d\u6700\u4f73\u5316\u5668\u662f\u8a13\u7df4\u795e\u7d93\u7db2\u8def\u6a21\u578b\u7684\u6a19\u6e96\u9078\u64c7\u3002\u5118\u7ba1\u5b83\u5011\u6bd4\u68af\u5ea6\u4e0b\u964d\u6536\u6582\u5f97\u66f4\u5feb\uff0c\u800c\u4e14\u5728\u5be6\u52d9\u4e0a\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u81ea\u9069\u61c9\u6700\u4f73\u5316\u5668\u4e0d\u5982\u50b3\u7d71\u68af\u5ea6\u4e0b\u964d\u90a3\u9ebc\u5bb9\u6613\u7406\u89e3\u3002\u539f\u56e0\u5728\u65bc\uff0c\u6709\u52a9\u65bc\u52a0\u901f\u9019\u4e9b\u65b9\u6cd5\u6536\u6582\u7684\u5b78\u7fd2\u7387\u52d5\u614b\u66f4\u65b0\uff0c\u4e5f\u8b93\u5b83\u5011\u7684\u5206\u6790\u8b8a\u5f97\u8907\u96dc\u3002\u7279\u5225\u662f\uff0c\u5c0d\u65bc\u4e00\u985e\u6700\u4f73\u5316\u554f\u984c\uff0c\u7c21\u55ae\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u4ee5\u7dda\u6027\u901f\u5ea6\u6536\u6582\uff0c\u800c\u5be6\u969b\u4e0a\u66f4\u5feb\u7684\u81ea\u9069\u61c9\u68af\u5ea6\u65b9\u6cd5\u537b\u7f3a\u4e4f\u9019\u6a23\u7684\u7406\u8ad6\u4fdd\u8b49\u3002Polyak-{\\L}ojasiewicz (PL) \u4e0d\u7b49\u5f0f\u662f\u6700\u5f31\u5df2\u77e5\u985e\u5225\uff0c\u5df2\u8b49\u660e\u68af\u5ea6\u4e0b\u964d\u53ca\u5176\u52d5\u91cf\u8b8a\u7570\u7684\u7dda\u6027\u6536\u6582\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86 AdaGrad \u548c Adam \u9019\u5169\u7a2e\u8457\u540d\u7684\u81ea\u9069\u61c9\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u6210\u672c\u51fd\u6578\u5e73\u6ed1\u4e14\u6eff\u8db3 PL \u4e0d\u7b49\u5f0f\u6642\uff0c\u6703\u7dda\u6027\u6536\u6582\u3002\u6211\u5011\u7684\u7406\u8ad6\u67b6\u69cb\u9075\u5faa\u4e00\u7a2e\u7c21\u55ae\u4e14\u7d71\u4e00\u7684\u65b9\u6cd5\uff0c\u9069\u7528\u65bc\u6279\u6b21\u68af\u5ea6\u548c\u96a8\u6a5f\u68af\u5ea6\uff0c\u9019\u6709\u53ef\u80fd\u7528\u65bc\u5206\u6790 Adam \u5176\u4ed6\u8b8a\u9ad4\u7684\u7dda\u6027\u6536\u6582\u3002", "author": "Kushal Chakrabarti et.al.", "authors": "Kushal Chakrabarti, Mayank Baranwal", "id": "2407.12629v1", "paper_url": "http://arxiv.org/abs/2407.12629v1", "repo": "null"}}