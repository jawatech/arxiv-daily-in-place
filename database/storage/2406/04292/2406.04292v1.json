{"2406.04292": {"publish_time": "2024-06-06", "title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval", "paper_summary": "Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.", "paper_summary_zh": "\u591a\u6a21\u6001\u68c0\u7d22\u5728\u5b9e\u8df5\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u68c0\u7d22\u5668\u5927\u591a\u662f\u6587\u672c\u5bfc\u5411\u7684\uff0c\u7f3a\u4e4f\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5c3d\u7ba1\u6709 CLIP \u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u8868\u793a\u4ec5\u6587\u672c\u548c\u4ec5\u56fe\u50cf\u6570\u636e\u65b9\u9762\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5d4c\u5165\u6a21\u578b VISTA\uff0c\u7528\u4e8e\u901a\u7528\u591a\u6a21\u6001\u68c0\u7d22\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5e26\u6765\u4e86\u4e09\u65b9\u9762\u7684\u6280\u672f\u8d21\u732e\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\uff0c\u5c06\u4e00\u4e2a\u5f3a\u5927\u7684\u6587\u672c\u7f16\u7801\u5668\u6269\u5c55\u4e3a\u5177\u6709\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e24\u79cd\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5e26\u6765\u4e86\u9ad8\u8d28\u91cf\u7684\u7ec4\u5408\u56fe\u50cf\u6587\u672c\uff0c\u4ee5\u4fc3\u8fdb\u5d4c\u5165\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u9996\u5148\u4f7f\u7528\u5927\u91cf\u5f31\u6807\u8bb0\u6570\u636e\u5c06\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u4e0e\u6587\u672c\u7f16\u7801\u5668\u5bf9\u9f50\uff0c\u7136\u540e\u4f7f\u7528\u751f\u6210\u7684\u7ec4\u5408\u56fe\u50cf\u6587\u672c\u6570\u636e\u5f00\u53d1\u591a\u6a21\u6001\u8868\u793a\u80fd\u529b\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0cVISTA \u5728\u5404\u79cd\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u65e2\u9002\u7528\u4e8e\u96f6\u6837\u672c\u8bbe\u7f6e\uff0c\u4e5f\u9002\u7528\u4e8e\u6709\u76d1\u7763\u8bbe\u7f6e\u3002\u6211\u4eec\u7684\u6a21\u578b\u3001\u6570\u636e\u548c\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/FlagOpen/FlagEmbedding \u83b7\u5f97\u3002", "author": "Junjie Zhou et.al.", "authors": "Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, Yongping Xiong", "id": "2406.04292v1", "paper_url": "http://arxiv.org/abs/2406.04292v1", "repo": "https://github.com/flagopen/flagembedding"}}