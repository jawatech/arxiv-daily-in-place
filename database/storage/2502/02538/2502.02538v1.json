{"2502.02538": {"publish_time": "2025-02-04", "title": "Flow Q-Learning", "paper_summary": "We present flow Q-learning (FQL), a simple and performant offline\nreinforcement learning (RL) method that leverages an expressive flow-matching\npolicy to model arbitrarily complex action distributions in data. Training a\nflow policy with RL is a tricky problem, due to the iterative nature of the\naction generation process. We address this challenge by training an expressive\none-step policy with RL, rather than directly guiding an iterative flow policy\nto maximize values. This way, we can completely avoid unstable recursive\nbackpropagation, eliminate costly iterative action generation at test time, yet\nstill mostly maintain expressivity. We experimentally show that FQL leads to\nstrong performance across 73 challenging state- and pixel-based OGBench and\nD4RL tasks in offline RL and offline-to-online RL. Project page:\nhttps://seohong.me/projects/fql/", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u6d41 Q \u5b78\u7fd2 (FQL)\uff0c\u4e00\u7a2e\u7c21\u55ae\u4e14\u9ad8\u6548\u7684\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2 (RL) \u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u8868\u9054\u5f0f\u6d41\u5339\u914d\u7b56\u7565\u4f86\u5efa\u6a21\u8cc7\u6599\u4e2d\u4efb\u610f\u8907\u96dc\u7684\u52d5\u4f5c\u5206\u4f48\u3002\u4f7f\u7528 RL \u8a13\u7df4\u6d41\u7b56\u7565\u662f\u4e00\u500b\u68d8\u624b\u7684\u554f\u984c\uff0c\u56e0\u70ba\u52d5\u4f5c\u751f\u6210\u904e\u7a0b\u7684\u8fed\u4ee3\u6027\u8cea\u3002\u6211\u5011\u900f\u904e\u8a13\u7df4\u4e00\u500b\u5177\u6709\u8868\u9054\u5f0f\u7684\u55ae\u6b65\u7b56\u7565\u4f86\u89e3\u6c7a\u9019\u500b\u6311\u6230\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u6307\u5c0e\u4e00\u500b\u8fed\u4ee3\u6d41\u7b56\u7565\u4f86\u6700\u5927\u5316\u503c\u3002\u9019\u6a23\uff0c\u6211\u5011\u53ef\u4ee5\u5b8c\u5168\u907f\u514d\u4e0d\u7a69\u5b9a\u7684\u905e\u8ff4\u53cd\u5411\u50b3\u64ad\uff0c\u6d88\u9664\u6e2c\u8a66\u6642\u7684\u6602\u8cb4\u8fed\u4ee3\u52d5\u4f5c\u751f\u6210\uff0c\u4f46\u4ecd\u7136\u53ef\u4ee5\u4fdd\u6301\u5927\u90e8\u5206\u7684\u8868\u9054\u529b\u3002\u6211\u5011\u900f\u904e\u5be6\u9a57\u8b49\u660e\uff0cFQL \u5728 73 \u500b\u5177\u6709\u6311\u6230\u6027\u7684\u72c0\u614b\u548c\u50cf\u7d20\u70ba\u57fa\u790e\u7684 OGBench \u548c D4RL \u96e2\u7dda RL \u548c\u96e2\u7dda\u5230\u7dda\u4e0a RL \u4efb\u52d9\u4e2d\u8868\u73fe\u512a\u7570\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://seohong.me/projects/fql/", "author": "Seohong Park et.al.", "authors": "Seohong Park, Qiyang Li, Sergey Levine", "id": "2502.02538v1", "paper_url": "http://arxiv.org/abs/2502.02538v1", "repo": "null"}}