{"2412.12893": {"publish_time": "2024-12-17", "title": "Question: How do Large Language Models perform on the Question Answering tasks? Answer:", "paper_summary": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e NLP \u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6210\u679c\uff0c\u800c\u7121\u9700\u4f7f\u7528\u5c11\u6a23\u672c\u6216\u96f6\u6a23\u672c\u63d0\u793a\u6280\u8853\u91dd\u5c0d\u9019\u4e9b\u4efb\u52d9\u9032\u884c\u660e\u78ba\u8a13\u7df4\u3002\u5e38\u898b\u7684 NLP \u4efb\u52d9\u662f\u554f\u7b54 (QA)\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u91dd\u5c0d\u8f03\u5c0f\u7684\u5fae\u8abf\u6a21\u578b\u548c\u73fe\u6210\u7684\u3001\u9075\u5faa\u6307\u4ee4\u7684 LLM \u5728\u53f2\u4e39\u4f5b\u554f\u7b54\u8cc7\u6599\u96c6 2.0 (SQuAD2) \u4e0a\u9032\u884c\u5168\u9762\u7684\u6548\u80fd\u6bd4\u8f03\uff0c\u7279\u5225\u662f\u5728\u4f7f\u7528\u55ae\u4e00\u63a8\u8ad6\u63d0\u793a\u6280\u8853\u6642\u3002\u7531\u65bc\u8cc7\u6599\u96c6\u5305\u542b\u7121\u6cd5\u56de\u7b54\u7684\u554f\u984c\uff0c\u56e0\u6b64\u5148\u524d\u7684\u5de5\u4f5c\u4f7f\u7528\u96d9\u91cd\u63a8\u8ad6\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u63d0\u793a\u98a8\u683c\uff0c\u65e8\u5728\u5f15\u51fa\u76f8\u540c\u7684\u6280\u80fd\uff0c\u800c\u7121\u9700\u96d9\u91cd\u63a8\u8ad6\uff0c\u5f9e\u800c\u7bc0\u7701\u904b\u7b97\u6642\u9593\u548c\u8cc7\u6e90\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u6bd4\u8f03\u5b83\u5011\u5728\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684 QA \u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\uff0c\u4f86\u63a2\u8a0e\u5b83\u5011\u7684\u6982\u5316\u80fd\u529b\uff0c\u800c\u7121\u9700\u5fae\u8abf\u4efb\u4e00\u6a21\u578b\uff0c\u6a21\u64ec\u771f\u5be6\u4e16\u754c\u7684\u4f7f\u7528\u60c5\u6cc1\uff0c\u5176\u4e2d\u8a9e\u5883\u548c\u63d0\u51fa\u7684\u554f\u984c\u53ef\u80fd\u8207\u539f\u59cb\u8a13\u7df4\u5206\u4f48\u4e0d\u540c\uff0c\u4f8b\u5982\u7528\u65b0\u805e\u6587\u7ae0\u66ff\u63db\u7dad\u57fa\u767e\u79d1\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u8f03\u5c0f\u7684\u5fae\u8abf\u6a21\u578b\u5728\u5fae\u8abf\u4efb\u52d9\u4e0a\u512a\u65bc\u76ee\u524d\u7684\u6700\u65b0\u6280\u8853 (SOTA) LLM\uff0c\u4f46\u6700\u8fd1\u7684 SOTA \u6a21\u578b\u80fd\u5920\u5728\u5206\u4f48\u5916\u6e2c\u8a66\u4e2d\u7e2e\u5c0f\u5dee\u8ddd\uff0c\u751a\u81f3\u5728 5 \u500b\u6e2c\u8a66\u7684 QA \u8cc7\u6599\u96c6\u4e2d\u6709 3 \u500b\u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u5fae\u8abf\u6a21\u578b\u3002", "author": "Kevin Fischer et.al.", "authors": "Kevin Fischer, Darren F\u00fcrst, Sebastian Steindl, Jakob Lindner, Ulrich Sch\u00e4fer", "id": "2412.12893v1", "paper_url": "http://arxiv.org/abs/2412.12893v1", "repo": "null"}}