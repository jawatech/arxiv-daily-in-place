{"2408.05758": {"publish_time": "2024-08-11", "title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing", "paper_summary": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u70ba\u8de8\u6a21\u614b\u8868\u5fb5\u5b78\u7fd2\u9818\u57df\u5e36\u4f86\u4e86\u986f\u8457\u7684\u9032\u6b65\u3002\u5c0d\u65bc\u8af8\u5982\u6587\u5b57\u8f49\u8a9e\u97f3 (TTS)\u3001\u8a9e\u97f3\u8f49\u63db (VC) \u548c\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u7b49\u4efb\u52d9\uff0c\u9700\u8981\u8de8\u6a21\u614b\u7d30\u7c92\u5ea6\uff08\u5e40\u7d1a\u5225\uff09\u5e8f\u5217\u8868\u5fb5\uff0c\u5f37\u8abf\u6587\u5b57\u6a21\u614b\u7684\u8a9e\u7fa9\u5167\u5bb9\uff0c\u540c\u6642\u6de1\u5316\u8a9e\u97f3\u6a21\u614b\u7684\u526f\u8a9e\u8a00\u8cc7\u8a0a\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u300c\u5411\u91cf\u91cf\u5316\u5c0d\u6bd4\u6a19\u8a18-\u8072\u5b78\u9810\u8a13\u7df4 (VQ-CTAP)\u300d\u7684\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u8de8\u6a21\u614b\u5c0d\u9f4a\u5e8f\u5217\u8f49\u78bc\u5668\u5c07\u6587\u5b57\u548c\u8a9e\u97f3\u5e36\u5165\u4e00\u500b\u806f\u5408\u591a\u6a21\u614b\u7a7a\u9593\uff0c\u5b78\u7fd2\u5982\u4f55\u5728\u5e40\u7d1a\u5225\u9023\u63a5\u6587\u5b57\u548c\u8a9e\u97f3\u3002\u6240\u63d0\u51fa\u7684 VQ-CTAP \u662f\u8de8\u6a21\u614b\u5e8f\u5217\u8868\u5fb5\u5b78\u7fd2\u7684\u5178\u7bc4\uff0c\u70ba\u8a9e\u97f3\u8655\u7406\u4e2d\u7684\u7d30\u7c92\u5ea6\u751f\u6210\u548c\u8fa8\u8b58\u4efb\u52d9\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u3002VQ-CTAP \u53ef\u4ee5\u76f4\u63a5\u61c9\u7528\u65bc VC \u548c ASR \u4efb\u52d9\uff0c\u800c\u7121\u9700\u5fae\u8abf\u6216\u984d\u5916\u7684\u7d50\u69cb\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5e8f\u5217\u611f\u77e5\u8a9e\u7fa9\u9023\u63a5\u5668\uff0c\u5b83\u9023\u63a5\u4e86\u591a\u500b\u51cd\u7d50\u7684\u9810\u8a13\u7df4\u6a21\u7d44\u4ee5\u9032\u884c TTS \u4efb\u52d9\uff0c\u5c55\u73fe\u4e86\u5373\u63d2\u5373\u7528\u7684\u80fd\u529b\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u9010\u6b65\u512a\u5316\u7b56\u7565\uff0c\u85c9\u7531\u9010\u6f38\u6ce8\u5165\u548c\u8abf\u6574\u5404\u7a2e\u640d\u5931\u7d44\u6210\u7684\u5f71\u97ff\uff0c\u4ee5\u78ba\u4fdd\u6a21\u578b\u6709\u6548\u6536\u6582\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8a9e\u7fa9\u8f49\u79fb\u5f0f\u7684\u526f\u8a9e\u8a00\u4e00\u81f4\u6027\u640d\u5931\uff0c\u4ee5\u589e\u5f37\u8868\u5fb5\u80fd\u529b\uff0c\u8b93\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6982\u62ec\u5230\u672a\u898b\u904e\u7684\u8cc7\u6599\uff0c\u4e26\u6355\u6349\u526f\u8a9e\u8a00\u8cc7\u8a0a\u7684\u7d30\u5fae\u5dee\u5225\u3002\u6b64\u5916\uff0cVQ-CTAP \u4ee5 25Hz \u7684\u901f\u7387\u5f9e 24kHz \u8f38\u5165\u6ce2\u5f62\u4e2d\u5be6\u73fe\u9ad8\u58d3\u7e2e\u8a9e\u97f3\u7de8\u78bc\uff0c\u9019\u662f\u63a1\u6a23\u7387\u7684 960 \u500d\u7e2e\u6e1b\u3002\u97f3\u8a0a\u793a\u7bc4\u53ef\u5728 https://qiangchunyu.github.io/VQCTAP/ \u53d6\u5f97", "author": "Chunyu Qiang et.al.", "authors": "Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong, Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che, Longbiao Wang, Jianwu Dang, Jianhua Tao", "id": "2408.05758v1", "paper_url": "http://arxiv.org/abs/2408.05758v1", "repo": "null"}}