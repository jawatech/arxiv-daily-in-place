{"2407.10366": {"publish_time": "2024-07-15", "title": "Accessing Vision Foundation Models at ImageNet-level Costs", "paper_summary": "Vision foundation models are renowned for their generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could advance research in this field.\nIn this work, we offer a very simple and general solution, named Proteus, to\ndistill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nLeveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of\nthe Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and\noutperforms other vision foundation models including CLIP-L/14 (400M),\nOpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M).", "paper_summary_zh": "\u8996\u89ba\u57fa\u790e\u6a21\u578b\u7531\u65bc\u5177\u6709\u9f90\u5927\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u800c\u4ee5\u5176\u6cdb\u5316\u80fd\u529b\u805e\u540d\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5b83\u5011\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u8cc7\u6e90\uff0c\u800c\u8a13\u7df4\u8cc7\u6599\u901a\u5e38\u7121\u6cd5\u53d6\u5f97\uff0c\u4f8b\u5982 CLIP\u3001DINOv2\uff0c\u5c0d\u767c\u5c55\u53ef\u4ee5\u63a8\u9032\u6b64\u9818\u57df\u7814\u7a76\u7684\u884d\u751f\u54c1\u69cb\u6210\u6975\u5927\u7684\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u975e\u5e38\u7c21\u55ae\u4e14\u901a\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7a31\u70ba Proteus\uff0c\u53ef\u4ee5\u5728\u6c92\u6709\u539f\u59cb\u8a13\u7df4\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\uff0c\u5c07\u57fa\u790e\u6a21\u578b\u63d0\u7149\u6210 ImageNet-1K \u4e0a\u8f03\u5c0f\u7684\u7b49\u50f9\u7269\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f9e\u50b3\u7d71\u77e5\u8b58\u63d0\u7149\u8a2d\u5b9a\u4e2d\u79fb\u9664\u5c0e\u81f4\u8cc7\u6599\u96c6\u504f\u5dee\u7684\u8a2d\u8a08\uff0c\u4e26\u63d0\u51fa\u4e09\u5c64\u8a13\u7df4\u76ee\u6a19\uff0c\u5373 token\u3001patch \u548c\u7279\u5fb5\uff0c\u4ee5\u6700\u5927\u5316\u77e5\u8b58\u50b3\u8f38\u7684\u529f\u6548\u3002\u900f\u904e\u9019\u7a2e\u65b9\u5f0f\uff0cProteus \u4ee5\u9a5a\u4eba\u7684\u80fd\u529b\u5728 ImageNet \u7b49\u7d1a\u7684\u6210\u672c\u4e0b\u9032\u884c\u8a13\u7df4\uff0c\u6709\u52a9\u65bc\u66f4\u5ee3\u6cdb\u7684\u7814\u7a76\u793e\u7fa4\u53d6\u5f97\u8a13\u7df4\u57fa\u790e\u6a21\u578b\u7684\u7ba1\u9053\u3002\u5229\u7528 DINOv2-g/14 \u4f5c\u70ba\u6559\u5e2b\uff0cProteus-L/14 \u5728 15 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8207 Oracle \u65b9\u6cd5 DINOv2-L/14\uff08142M \u8a13\u7df4\u8cc7\u6599\uff09\u7684\u6548\u80fd\u76f8\u5339\u914d\uff0c\u4e26\u4e14\u512a\u65bc\u5176\u4ed6\u8996\u89ba\u57fa\u790e\u6a21\u578b\uff0c\u5305\u62ec CLIP-L/14\uff08400M\uff09\u3001OpenCLIP-L/14\uff08400M/2B\uff09\u548c SynCLR-L/14\uff08600M\uff09\u3002", "author": "Yitian Zhang et.al.", "authors": "Yitian Zhang, Xu Ma, Yue Bai, Huan Wang, Yun Fu", "id": "2407.10366v1", "paper_url": "http://arxiv.org/abs/2407.10366v1", "repo": "https://github.com/bespontaneous/proteus-pytorch"}}