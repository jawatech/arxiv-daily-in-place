{"2406.16690": {"publish_time": "2024-06-24", "title": "Scaling Laws for Linear Complexity Language Models", "paper_summary": "The interest in linear complexity models for large language models is on the\nrise, although their scaling capacity remains uncertain. In this study, we\npresent the scaling laws for linear complexity language models to establish a\nfoundation for their scalability. Specifically, we examine the scaling\nbehaviors of three efficient linear architectures. These include TNL, a linear\nattention model with data-independent decay; HGRN2, a linear RNN with\ndata-dependent decay; and cosFormer2, a linear attention model without decay.\nWe also include LLaMA as a baseline architecture for softmax attention for\ncomparison. These models were trained with six variants, ranging from 70M to 7B\nparameters on a 300B-token corpus, and evaluated with a total of 1,376\nintermediate checkpoints on various downstream tasks. These tasks include\nvalidation loss, commonsense reasoning, and information retrieval and\ngeneration. The study reveals that existing linear complexity language models\nexhibit similar scaling capabilities as conventional transformer-based models\nwhile also demonstrating superior linguistic proficiency and knowledge\nretention.", "paper_summary_zh": "\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u7dda\u6027\u8907\u96dc\u5ea6\u6a21\u578b\u7684\u8208\u8da3\u6b63\u5728\u589e\u52a0\uff0c\u5118\u7ba1\u5b83\u5011\u7684\u64f4\u5145\u80fd\u529b\u4ecd\u7136\u4e0d\u78ba\u5b9a\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7dda\u6027\u8907\u96dc\u5ea6\u8a9e\u8a00\u6a21\u578b\u7684\u64f4\u5145\u5b9a\u5f8b\uff0c\u70ba\u5b83\u5011\u7684\u53ef\u64f4\u5145\u6027\u5960\u5b9a\u57fa\u790e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u6aa2\u9a57\u4e86\u4e09\u7a2e\u9ad8\u6548\u7dda\u6027\u67b6\u69cb\u7684\u64f4\u5145\u884c\u70ba\u3002\u9019\u4e9b\u67b6\u69cb\u5305\u62ec TNL\uff0c\u4e00\u7a2e\u5177\u6709\u8207\u8cc7\u6599\u7121\u95dc\u8870\u6e1b\u7684\u7dda\u6027\u6ce8\u610f\u529b\u6a21\u578b\uff1bHGRN2\uff0c\u4e00\u7a2e\u5177\u6709\u8207\u8cc7\u6599\u76f8\u95dc\u8870\u6e1b\u7684\u7dda\u6027 RNN\uff1b\u4ee5\u53ca cosFormer2\uff0c\u4e00\u7a2e\u6c92\u6709\u8870\u6e1b\u7684\u7dda\u6027\u6ce8\u610f\u529b\u6a21\u578b\u3002\u6211\u5011\u9084\u5c07 LLaMA \u4f5c\u70ba softmax \u6ce8\u610f\u529b\u7684\u57fa\u6e96\u67b6\u69cb\u9032\u884c\u6bd4\u8f03\u3002\u9019\u4e9b\u6a21\u578b\u4f7f\u7528\u516d\u7a2e\u8b8a\u9ad4\u9032\u884c\u8a13\u7df4\uff0c\u7bc4\u570d\u5f9e 70M \u5230 7B \u53c3\u6578\uff0c\u5728 300B \u4ee4\u724c\u8a9e\u6599\u5eab\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u4f7f\u7528\u7e3d\u5171 1,376 \u500b\u4e2d\u9593\u6aa2\u67e5\u9ede\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e0a\u9032\u884c\u8a55\u4f30\u3002\u9019\u4e9b\u4efb\u52d9\u5305\u62ec\u9a57\u8b49\u640d\u5931\u3001\u5e38\u8b58\u63a8\u7406\u3001\u8cc7\u8a0a\u6aa2\u7d22\u548c\u751f\u6210\u3002\u7814\u7a76\u8868\u660e\uff0c\u73fe\u6709\u7684\u7dda\u6027\u8907\u96dc\u5ea6\u8a9e\u8a00\u6a21\u578b\u5c55\u73fe\u51fa\u8207\u50b3\u7d71\u57fa\u65bc\u8f49\u63db\u5668\u7684\u6a21\u578b\u985e\u4f3c\u7684\u64f4\u5145\u80fd\u529b\uff0c\u540c\u6642\u4e5f\u5c55\u73fe\u51fa\u512a\u7570\u7684\u8a9e\u8a00\u80fd\u529b\u548c\u77e5\u8b58\u4fdd\u7559\u80fd\u529b\u3002", "author": "Xuyang Shen et.al.", "authors": "Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong", "id": "2406.16690v1", "paper_url": "http://arxiv.org/abs/2406.16690v1", "repo": "null"}}