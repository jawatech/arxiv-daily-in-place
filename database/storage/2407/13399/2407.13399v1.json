{"2407.13399": {"publish_time": "2024-07-18", "title": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overparameterization via Chi-squared Preference Optimization", "paper_summary": "Language model alignment methods, such as reinforcement learning from human\nfeedback (RLHF), have led to impressive advances in language model\ncapabilities, but existing techniques are limited by a widely observed\nphenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization\nis often attributed to overfitting to an inaccurate reward model, and while it\ncan be mitigated through online data collection, this is infeasible in many\nsettings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency\nbe improved further?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f8b\u5982\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u5df2\u5c0e\u81f4\u8a9e\u8a00\u6a21\u578b\u80fd\u529b\u7684\u986f\u8457\u9032\u6b65\uff0c\u4f46\u73fe\u6709\u6280\u8853\u53d7\u5230\u5ee3\u6cdb\u89c0\u5bdf\u5230\u7684\u73fe\u8c61\u9650\u5236\uff0c\u7a31\u70ba\u904e\u5ea6\u6700\u4f73\u5316\uff0c\u5176\u4e2d\u8a9e\u8a00\u6a21\u578b\u7684\u54c1\u8cea\u5728\u5c0d\u9f4a\u904e\u7a0b\u4e2d\u9054\u5230\u5e73\u7a69\u671f\u6216\u4e0b\u964d\u3002\u904e\u5ea6\u6700\u4f73\u5316\u901a\u5e38\u6b78\u56e0\u65bc\u904e\u5ea6\u64ec\u5408\u4e0d\u6e96\u78ba\u7684\u734e\u52f5\u6a21\u578b\uff0c\u96d6\u7136\u5b83\u53ef\u4ee5\u900f\u904e\u7dda\u4e0a\u8cc7\u6599\u6536\u96c6\u4f86\u6e1b\u8f15\uff0c\u4f46\u5728\u8a31\u591a\u8a2d\u5b9a\u4e2d\u9019\u662f\u4e0d\u53ef\u884c\u7684\u3002\u9019\u5f15\u767c\u4e86\u4e00\u500b\u57fa\u672c\u554f\u984c\uff1a\u73fe\u6709\u7684\u96e2\u7dda\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u662f\u5426\u5145\u5206\u5229\u7528\u4e86\u4ed6\u5011\u64c1\u6709\u7684\u8cc7\u6599\uff0c\u6216\u8005\u4ed6\u5011\u7684\u6a23\u672c\u6548\u7387\u53ef\u4ee5\u9032\u4e00\u6b65\u63d0\u9ad8\uff1f\n\u6211\u5011\u4f7f\u7528\u4e00\u7a2e\u65b0\u7684\u96e2\u7dda\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5373 $\\chi^2$ \u504f\u597d\u6700\u4f73\u5316\uff08$\\chi$PO\uff09\u3002$\\chi$PO \u662f\u5c0d\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08DPO\uff1bRafailov \u7b49\u4eba\uff0c2023 \u5e74\uff09\u7684\u4e00\u884c\u8b8a\u66f4\uff0c\u5b83\u53ea\u6d89\u53ca\u4fee\u6539 DPO \u76ee\u6a19\u4e2d\u7684\u5c0d\u6578\u9023\u7d50\u51fd\u6578\u3002\u5118\u7ba1\u6709\u9019\u500b\u6700\u5c0f\u7684\u8b8a\u66f4\uff0c$\\chi$PO \u900f\u904e\u4f7f\u7528 $\\chi^2$ \u8ddd\u96e2\u9032\u884c\u6b63\u5247\u5316\uff0c\u5728\u9762\u5c0d\u4e0d\u78ba\u5b9a\u6027\u6642\u96b1\u542b\u5730\u5be6\u4f5c\u4e86\u60b2\u89c0\u539f\u5247\uff0c\u800c $\\chi^2$ \u8ddd\u96e2\u6bd4 KL \u6b63\u5247\u5316\u66f4\u6709\u6548\u5730\u91cf\u5316\u4e86\u4e0d\u78ba\u5b9a\u6027\uff0c\u4e26\u4e14\u53ef\u4ee5\u8b49\u660e\u5730\u6e1b\u8f15\u904e\u5ea6\u6700\u4f73\u5316\uff0c\u6839\u64da\u55ae\u4e00\u653f\u7b56\u96c6\u4e2d\u5ea6\u5be6\u73fe\u6a23\u672c\u8907\u96dc\u5ea6\u4fdd\u8b49\uff0c\u9019\u662f\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u4e2d\u7684\u9ec3\u91d1\u6a19\u6e96\u3002$\\chi$PO \u7684\u7c21\u6f54\u6027\u548c\u5f37\u6709\u529b\u7684\u4fdd\u8b49\u4f7f\u5176\u6210\u70ba\u7b2c\u4e00\u500b\u5be6\u7528\u7684\u901a\u7528\u96e2\u7dda\u5c0d\u9f4a\u6f14\u7b97\u6cd5\uff0c\u5b83\u53ef\u4ee5\u8b49\u660e\u5c0d\u904e\u5ea6\u6700\u4f73\u5316\u5177\u6709\u9b6f\u68d2\u6027\u3002", "author": "Audrey Huang et.al.", "authors": "Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, Dylan J. Foster", "id": "2407.13399v1", "paper_url": "http://arxiv.org/abs/2407.13399v1", "repo": "null"}}