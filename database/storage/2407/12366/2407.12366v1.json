{"2407.12366": {"publish_time": "2024-07-17", "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models", "paper_summary": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.", "paper_summary_zh": "\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u986f\u8457\u9032\u6b65\uff0c\u6709\u4e00\u9805\u84ec\u52c3\u767c\u5c55\u7684\u5021\u8b70\u65e8\u5728\u5229\u7528 LLM \u9032\u884c\u6307\u4ee4\u5f8c\u7e8c\u6a5f\u5668\u4eba\u5c0e\u822a\u3002\u9019\u7a2e\u8da8\u52e2\u5f37\u8abf\u4e86 LLM \u5728\u6982\u62ec\u5c0e\u822a\u63a8\u7406\u548c\u591a\u6a23\u5316\u8a9e\u8a00\u7406\u89e3\u65b9\u9762\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5728\u5c07 LLM \u6574\u5408\u5230\u8996\u89ba\u548c\u8a9e\u8a00\u5c0e\u822a (VLN) \u4efb\u52d9\u4e2d\u6642\uff0c\u8207\u5148\u524d\u7684\u4e0b\u6e38\u5c08\u5bb6\u6a21\u578b\u76f8\u6bd4\uff0c\u89c0\u5bdf\u5230\u4ee3\u7406\u6548\u80fd\u7684\u986f\u8457\u5dee\u7570\u3002\u6b64\u5916\uff0c\u8a9e\u8a00\u5728\u4ee3\u7406\u4e92\u52d5\u4e2d\u89e3\u91cb\u548c\u4fc3\u9032\u6e9d\u901a\u7684\u5167\u5728\u80fd\u529b\u5728\u9019\u4e9b\u6574\u5408\u4e2d\u5e38\u5e38\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u52aa\u529b\u5f4c\u5408 VLN \u5c08\u696d\u6a21\u578b\u548c\u57fa\u65bc LLM \u7684\u5c0e\u822a\u7bc4\u4f8b\u4e4b\u9593\u7684\u9d3b\u6e9d\uff0c\u540c\u6642\u4fdd\u6301 LLM \u5728\u7522\u751f\u8a9e\u8a00\u5c0e\u822a\u63a8\u7406\u65b9\u9762\u7684\u8a6e\u91cb\u80fd\u529b\u3002\u901a\u904e\u5728\u51cd\u7d50\u7684 LLM \u4e2d\u5c0d\u9f4a\u8996\u89ba\u5167\u5bb9\uff0c\u6211\u5011\u6db5\u84cb\u4e86 LLM \u7684\u8996\u89ba\u89c0\u5bdf\u7406\u89e3\uff0c\u4e26\u63a2\u7d22\u4e86\u4e00\u7a2e\u5c07 LLM \u548c\u5c0e\u822a\u7b56\u7565\u7db2\u8def\u7d0d\u5165\u8003\u91cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u9032\u884c\u6709\u6548\u7684\u52d5\u4f5c\u9810\u6e2c\u548c\u5c0e\u822a\u63a8\u7406\u3002\u6211\u5011\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u8cc7\u6599\u6548\u7387\uff0c\u4e26\u6d88\u9664\u4e86\u57fa\u65bc LM \u7684\u4ee3\u7406\u8207\u6700\u5148\u9032\u7684 VLN \u5c08\u5bb6\u4e4b\u9593\u7684\u5dee\u8ddd\u3002", "author": "Gengze Zhou et.al.", "authors": "Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, Qi Wu", "id": "2407.12366v1", "paper_url": "http://arxiv.org/abs/2407.12366v1", "repo": "https://github.com/gengzezhou/navgpt-2"}}