{"2409.09501": {"publish_time": "2024-09-14", "title": "Synthetic4Health: Generating Annotated Synthetic Clinical Letters", "paper_summary": "Since clinical letters contain sensitive information, clinical-related\ndatasets can not be widely applied in model training, medical research, and\nteaching. This work aims to generate reliable, various, and de-identified\nsynthetic clinical letters. To achieve this goal, we explored different\npre-trained language models (PLMs) for masking and generating text. After that,\nwe worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with\ndifferent masking strategies. Both qualitative and quantitative methods were\nused for evaluation. Additionally, a downstream task, Named Entity Recognition\n(NER), was also implemented to assess the usability of these synthetic letters.\n  The results indicate that 1) encoder-only models outperform encoder-decoder\nmodels. 2) Among encoder-only models, those trained on general corpora perform\ncomparably to those trained on clinical data when clinical information is\npreserved. 3) Additionally, preserving clinical entities and document structure\nbetter aligns with our objectives than simply fine-tuning the model. 4)\nFurthermore, different masking strategies can impact the quality of synthetic\nclinical letters. Masking stopwords has a positive impact, while masking nouns\nor verbs has a negative effect. 5) For evaluation, BERTScore should be the\nprimary quantitative evaluation metric, with other metrics serving as\nsupplementary references. 6) Contextual information does not significantly\nimpact the models' understanding, so the synthetic clinical letters have the\npotential to replace the original ones in downstream tasks.", "paper_summary_zh": "\u7531\u65bc\u81e8\u5e8a\u4fe1\u4ef6\u5305\u542b\u654f\u611f\u8cc7\u8a0a\uff0c\u56e0\u6b64\u8207\u81e8\u5e8a\u76f8\u95dc\u7684\u8cc7\u6599\u96c6\u7121\u6cd5\u5ee3\u6cdb\u61c9\u7528\u65bc\u6a21\u578b\u8a13\u7df4\u3001\u91ab\u5b78\u7814\u7a76\u548c\u6559\u5b78\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u7522\u751f\u53ef\u9760\u3001\u591a\u6a23\u4e14\u53bb\u8b58\u5225\u5316\u7684\u5408\u6210\u81e8\u5e8a\u4fe1\u4ef6\u3002\u70ba\u4e86\u9054\u6210\u6b64\u76ee\u6a19\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e0d\u540c\u7684\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u4f86\u906e\u853d\u548c\u7522\u751f\u6587\u5b57\u3002\u4e4b\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u9ad8\u6027\u80fd\u6a21\u578b Bio_ClinicalBERT\uff0c\u4e26\u91dd\u5c0d\u4e0d\u540c\u7684\u906e\u853d\u7b56\u7565\u9032\u884c\u5be6\u9a57\u3002\u6211\u5011\u4f7f\u7528\u5b9a\u6027\u548c\u5b9a\u91cf\u65b9\u6cd5\u9032\u884c\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u6211\u5011\u4e5f\u5be6\u4f5c\u4e86\u4e0b\u6e38\u4efb\u52d9\uff0c\u5373\u547d\u540d\u5be6\u9ad4\u8b58\u5225 (NER) \u4f86\u8a55\u4f30\u9019\u4e9b\u5408\u6210\u4fe1\u4ef6\u7684\u53ef\u7528\u6027\u3002\u7d50\u679c\u986f\u793a\uff1a1) \u7de8\u78bc\u5668\u5c08\u7528\u6a21\u578b\u512a\u65bc\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\u30022) \u5728\u7de8\u78bc\u5668\u5c08\u7528\u6a21\u578b\u4e2d\uff0c\u7576\u4fdd\u7559\u81e8\u5e8a\u8cc7\u8a0a\u6642\uff0c\u5728\u4e00\u822c\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u8868\u73fe\u8207\u5728\u81e8\u5e8a\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u76f8\u7576\u30023) \u6b64\u5916\uff0c\u4fdd\u7559\u81e8\u5e8a\u5be6\u9ad4\u548c\u6587\u4ef6\u7d50\u69cb\u6bd4\u55ae\u7d14\u5fae\u8abf\u6a21\u578b\u66f4\u7b26\u5408\u6211\u5011\u7684\u76ee\u6a19\u30024) \u6b64\u5916\uff0c\u4e0d\u540c\u7684\u906e\u853d\u7b56\u7565\u6703\u5f71\u97ff\u5408\u6210\u81e8\u5e8a\u4fe1\u4ef6\u7684\u54c1\u8cea\u3002\u906e\u853d\u505c\u6b62\u8a5e\u6709\u6b63\u9762\u7684\u5f71\u97ff\uff0c\u800c\u906e\u853d\u540d\u8a5e\u6216\u52d5\u8a5e\u6709\u8ca0\u9762\u7684\u5f71\u97ff\u30025) \u5c0d\u65bc\u8a55\u4f30\uff0cBERTScore \u61c9\u70ba\u4e3b\u8981\u7684\u5b9a\u91cf\u8a55\u4f30\u6307\u6a19\uff0c\u5176\u4ed6\u6307\u6a19\u4f5c\u70ba\u88dc\u5145\u53c3\u8003\u30026) \u80cc\u666f\u8cc7\u8a0a\u4e0d\u6703\u986f\u8457\u5f71\u97ff\u6a21\u578b\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u5408\u6210\u81e8\u5e8a\u4fe1\u4ef6\u6709\u6f5b\u529b\u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u53d6\u4ee3\u539f\u59cb\u4fe1\u4ef6\u3002", "author": "Libo Ren et.al.", "authors": "Libo Ren, Samuel Belkadi, Lifeng Han, Warren Del-Pinto, Goran Nenadic", "id": "2409.09501v1", "paper_url": "http://arxiv.org/abs/2409.09501v1", "repo": "null"}}