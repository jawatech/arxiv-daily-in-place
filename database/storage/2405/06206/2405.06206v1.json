{"2405.06206": {"publish_time": "2024-05-10", "title": "Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning", "paper_summary": "Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on Byzantine-robust aggregation\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign model updates. To effectively conceal\nmalicious model updates among benign ones, we propose DPOT, a backdoor attack\nstrategy in FL that dynamically constructs backdoor objectives by optimizing a\nbackdoor trigger, making backdoor data have minimal effect on model updates. We\nprovide theoretical justifications for DPOT's attacking principle and display\nexperimental results showing that DPOT, via only a data-poisoning attack,\neffectively undermines state-of-the-art defenses and outperforms existing\nbackdoor attack techniques on various datasets.", "paper_summary_zh": "\u806f\u90a6\u5b78\u7fd2 (FL) \u662f\u4e00\u7a2e\u5206\u6563\u5f0f\u6a5f\u5668\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4f7f\u53c3\u8207\u8005\u80fd\u5920\u5728\u4e0d\u5206\u4eab\u5176\u79c1\u6709\u6578\u64da\u7684\u60c5\u6cc1\u4e0b\u5354\u4f5c\u8a13\u7df4\u6a21\u578b\u3002\u5118\u7ba1\u5177\u6709\u96b1\u79c1\u548c\u53ef\u64f4\u5145\u6027\u512a\u52e2\uff0c\u4f46 FL \u5bb9\u6613\u53d7\u5230\u5f8c\u9580\u653b\u64ca\uff0c\u5176\u4e2d\u5c0d\u624b\u4f7f\u7528\u5f8c\u9580\u89f8\u767c\u5668\u6bd2\u5bb3\u90e8\u5206\u5ba2\u6236\u7684\u672c\u5730\u8a13\u7df4\u6578\u64da\uff0c\u76ee\u7684\u662f\u5728\u63a8\u7406\u6642\u9593\u8f38\u5165\u6eff\u8db3\u76f8\u540c\u5f8c\u9580\u689d\u4ef6\u6642\u8b93\u805a\u5408\u6a21\u578b\u7522\u751f\u60e1\u610f\u7d50\u679c\u3002\u73fe\u6709\u7684 FL \u4e2d\u7684\u5f8c\u9580\u653b\u64ca\u5b58\u5728\u5e38\u898b\u7684\u7f3a\u9677\uff1a\u56fa\u5b9a\u7684\u89f8\u767c\u6a21\u5f0f\u548c\u4f9d\u8cf4\u6a21\u578b\u6bd2\u5316\u7684\u5354\u52a9\u3002\u57fa\u65bc\u62dc\u5360\u5ead\u9b6f\u68d2\u805a\u5408\u7684\u6700\u65b0\u9632\u79a6\u5728\u9019\u4e9b\u653b\u64ca\u4e2d\u8868\u73fe\u51fa\u826f\u597d\u7684\u9632\u79a6\u6027\u80fd\uff0c\u56e0\u70ba\u60e1\u610f\u548c\u826f\u6027\u6a21\u578b\u66f4\u65b0\u4e4b\u9593\u5b58\u5728\u986f\u8457\u5dee\u7570\u3002\u70ba\u4e86\u5728\u826f\u6027\u66f4\u65b0\u4e2d\u6709\u6548\u96b1\u85cf\u60e1\u610f\u6a21\u578b\u66f4\u65b0\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DPOT\uff0c\u9019\u662f\u4e00\u7a2e FL \u4e2d\u7684\u5f8c\u9580\u653b\u64ca\u7b56\u7565\uff0c\u901a\u904e\u512a\u5316\u5f8c\u9580\u89f8\u767c\u5668\u52d5\u614b\u69cb\u5efa\u5f8c\u9580\u76ee\u6a19\uff0c\u4f7f\u5f8c\u9580\u6578\u64da\u5c0d\u6a21\u578b\u66f4\u65b0\u7684\u5f71\u97ff\u6700\u5c0f\u3002\u6211\u5011\u70ba DPOT \u7684\u653b\u64ca\u539f\u7406\u63d0\u4f9b\u4e86\u7406\u8ad6\u4f9d\u64da\uff0c\u4e26\u5c55\u793a\u4e86\u5be6\u9a57\u7d50\u679c\uff0c\u8868\u660e DPOT \u50c5\u901a\u904e\u6578\u64da\u6bd2\u5316\u653b\u64ca\uff0c\u5c31\u80fd\u6709\u6548\u7834\u58de\u6700\u5148\u9032\u7684\u9632\u79a6\uff0c\u4e26\u4e14\u5728\u5404\u7a2e\u6578\u64da\u96c6\u4e0a\u512a\u65bc\u73fe\u6709\u7684\u5f8c\u9580\u653b\u64ca\u6280\u8853\u3002", "author": "Yujie Zhang et.al.", "authors": "Yujie Zhang, Neil Gong, Michael K. Reiter", "id": "2405.06206v1", "paper_url": "http://arxiv.org/abs/2405.06206v1", "repo": "null"}}