{"2409.20053": {"publish_time": "2024-09-30", "title": "GUNDAM: Aligning Large Language Models with Graph Understanding", "paper_summary": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8655\u7406\u6587\u672c\u6578\u64da\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u9019\u6fc0\u767c\u4e86\u5c07\u9019\u4e9b\u6a21\u578b\u61c9\u7528\u65bc\u6587\u672c\u6578\u64da\u4e4b\u5916\u9818\u57df\uff08\u4f8b\u5982\u5716\u8868\uff09\u7684\u8208\u8da3\u3002\u5728\u5716\u8868\u5b78\u7fd2\u9818\u57df\uff0c\u5229\u7528 LLM \u4f86\u7406\u89e3\u548c\u64cd\u4f5c\u5716\u8868\u7d50\u69cb\u6578\u64da\u7684\u8208\u8da3\u8207\u65e5\u4ff1\u589e\u3002\u73fe\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u5177\u6709\u8c50\u5bcc\u6587\u672c\u7279\u5fb5\u7684\u5716\u8868\uff0c\u4f8b\u5982\u77e5\u8b58\u5716\u8868\u6216\u6587\u672c\u5c6c\u6027\u5716\u8868\uff0c\u5229\u7528 LLM \u8655\u7406\u6587\u672c\u7684\u80fd\u529b\uff0c\u4f46\u672a\u80fd\u5145\u5206\u89e3\u6c7a\u5716\u8868\u7d50\u69cb\u3002\u9019\u9805\u5de5\u4f5c\u7279\u5225\u65e8\u5728\u8a55\u4f30\u548c\u589e\u5f37 LLM \u7406\u89e3\u548c\u5229\u7528\u5716\u8868\u6578\u64da\u672c\u8eab\u56fa\u6709\u7684\u7d50\u69cb\u77e5\u8b58\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u50c5\u95dc\u6ce8\u5bcc\u542b\u6587\u672c\u5167\u5bb9\u7684\u5716\u8868\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u81ea\u7136\u8a9e\u8a00\u9a45\u52d5\u5206\u6790\u6a21\u578b\u7684\u5716\u8868\u7406\u89e3 (\\model)\u3002\u6b64\u6a21\u578b\u6539\u9032\u4e86 LLM \u4ee5\u4fbf\u66f4\u597d\u5730\u7406\u89e3\u548c\u53c3\u8207\u5716\u8868\u6578\u64da\u7684\u7d50\u69cb\uff0c\u4f7f\u5176\u80fd\u5920\u901a\u904e\u5229\u7528\u5716\u8868\u7684\u7d50\u69cb\u672c\u8eab\u4f86\u57f7\u884c\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\u3002\u6211\u5011\u5c0d\u5716\u8868\u63a8\u7406\u57fa\u6e96\u7684\u5be6\u9a57\u8a55\u4f30\u4e0d\u50c5\u8b49\u5be6\u4e86 \\model~ \u512a\u65bc\u7528\u65bc\u6bd4\u8f03\u7684 SOTA \u57fa\u6e96\uff0c\u9084\u63ed\u793a\u4e86\u5f71\u97ff LLM \u5716\u8868\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u7406\u8ad6\u5206\u6790\uff0c\u8aaa\u660e\u63a8\u7406\u8def\u5f91\u5982\u4f55\u589e\u5f37 LLM \u7684\u63a8\u7406\u80fd\u529b\u3002", "author": "Sheng Ouyang et.al.", "authors": "Sheng Ouyang, Yulan Hu, Ge Chen, Yong Liu", "id": "2409.20053v1", "paper_url": "http://arxiv.org/abs/2409.20053v1", "repo": "null"}}