# arxiv-daily
 Automated deployment @ 2024-09-19 09:03:27 Asia/Taipei
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/storage).

## AI

### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**|Yukang Lin et.al.|[2409.11147v1](http://arxiv.org/abs/2409.11147v1)|[link](https://github.com/yukang-lin/rger)|
|**2024-09-17**|**Semformer: Transformer Language Models with Semantic Planning**|Yongjing Yin et.al.|[2409.11143v1](http://arxiv.org/abs/2409.11143v1)|null|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|Yanbei Jiang et.al.|[2409.10921v1](http://arxiv.org/abs/2409.10921v1)|[link](https://github.com/yanbei-jiang/artwork-interpretation)|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**|Shanshan Wang et.al.|[2409.10294v1](http://arxiv.org/abs/2409.10294v1)|null|
|**2024-09-16**|**LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**|Le Xiao et.al.|[2409.10077v1](http://arxiv.org/abs/2409.10077v1)|null|
|**2024-09-16**|**On the Diagram of Thought**|Yifan Zhang et.al.|[2409.10038v1](http://arxiv.org/abs/2409.10038v1)|[link](https://github.com/diagram-of-thought/diagram-of-thought)|
|**2024-09-14**|**Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**|Yuanjie Lyu et.al.|[2409.09362v1](http://arxiv.org/abs/2409.09362v1)|null|
|**2024-09-14**|**ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**|Yahan Tu et.al.|[2409.09318v1](http://arxiv.org/abs/2409.09318v1)|null|
|**2024-09-13**|**Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**|Florian Gr√∂tschla et.al.|[2409.09026v1](http://arxiv.org/abs/2409.09026v1)|null|
|**2024-09-13**|**Contri(e)ve: Context + Retrieve for Scholarly Question Answering**|Kanchan Shivashankar et.al.|[2409.09010v1](http://arxiv.org/abs/2409.09010v1)|null|
|**2024-09-13**|**SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**|Qitian Wu et.al.|[2409.09007v1](http://arxiv.org/abs/2409.09007v1)|[link](https://github.com/qitianwu/sgformer)|
|**2024-09-13**|**Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**|Zhiqiang Zhong et.al.|[2409.08864v1](http://arxiv.org/abs/2409.08864v1)|null|
|**2024-09-13**|**A RAG Approach for Generating Competency Questions in Ontology Engineering**|Xueli Pan et.al.|[2409.08820v1](http://arxiv.org/abs/2409.08820v1)|null|
|**2024-09-13**|**ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**|Zezheng Qin et.al.|[2409.08543v1](http://arxiv.org/abs/2409.08543v1)|null|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202v1](http://arxiv.org/abs/2409.08202v1)|null|
|**2024-09-12**|**Towards a graph-based foundation model for network traffic analysis**|Louis Van Langendonck et.al.|[2409.08111v1](http://arxiv.org/abs/2409.08111v1)|null|
|**2024-09-12**|**Learning Rules from KGs Guided by Language Models**|Zihang Peng et.al.|[2409.07869v1](http://arxiv.org/abs/2409.07869v1)|[link](https://github.com/pzh97/learning-rules-from-kgs-guided-by-language-models)|
|**2024-09-12**|**Multi-object event graph representation learning for Video Question Answering**|Yanan Wang et.al.|[2409.07747v1](http://arxiv.org/abs/2409.07747v1)|null|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368v2](http://arxiv.org/abs/2409.07368v2)|null|
|**2024-09-11**|**Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**|Daehee Kim et.al.|[2409.07088v1](http://arxiv.org/abs/2409.07088v1)|[link](https://github.com/daehuikim/WikiOFGraph)|
|**2024-09-11**|**Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**|Jiun-Ting Li et.al.|[2409.07064v1](http://arxiv.org/abs/2409.07064v1)|null|
|**2024-09-11**|**FreeRide: Harvesting Bubbles in Pipeline Parallelism**|Jiashu Zhang et.al.|[2409.06941v1](http://arxiv.org/abs/2409.06941v1)|null|
|**2024-09-10**|**Generative Hierarchical Materials Search**|Sherry Yang et.al.|[2409.06762v1](http://arxiv.org/abs/2409.06762v1)|null|
|**2024-09-10**|**Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**|Gollam Rabby et.al.|[2409.06433v1](http://arxiv.org/abs/2409.06433v1)|null|
|**2024-09-09**|**Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**|Dongyue Li et.al.|[2409.06091v1](http://arxiv.org/abs/2409.06091v1)|[link](https://github.com/virtuosoresearch/scalablemtl)|
|**2024-09-09**|**OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**|Ningyu Zhang et.al.|[2409.07497v1](http://arxiv.org/abs/2409.07497v1)|[link](https://github.com/zjunlp/oneedit)|
|**2024-09-09**|**SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**|Alireza Ghafarollahi et.al.|[2409.05556v1](http://arxiv.org/abs/2409.05556v1)|[link](https://github.com/lamm-mit/SciAgentsDiscovery)|
|**2024-09-09**|**Assessing SPARQL capabilities of Large Language Models**|Lars-Peter Meyer et.al.|[2409.05925v1](http://arxiv.org/abs/2409.05925v1)|[link](https://github.com/aksw/llm-kg-bench)|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-07**|**Action is the primary key: a categorical framework for episode description and logical reasoning**|Yoshiki Fukada et.al.|[2409.04793v1](http://arxiv.org/abs/2409.04793v1)|null|
|**2024-09-06**|**Accelerating Training with Neuron Interaction and Nowcasting Networks**|Boris Knyazev et.al.|[2409.04434v1](http://arxiv.org/abs/2409.04434v1)|[link](https://github.com/samsungsailmontreal/nino)|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286v1](http://arxiv.org/abs/2409.04286v1)|null|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183v1](http://arxiv.org/abs/2409.04183v1)|null|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181v1](http://arxiv.org/abs/2409.04181v1)|null|
|**2024-09-06**|**Refining Wikidata Taxonomy using Large Language Models**|Yiwen Peng et.al.|[2409.04056v1](http://arxiv.org/abs/2409.04056v1)|[link](https://github.com/peng-yiwen/WiKC)|
|**2024-09-06**|**Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**|Miao Fan et.al.|[2409.04009v1](http://arxiv.org/abs/2409.04009v1)|null|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440v1](http://arxiv.org/abs/2409.03440v1)|null|
|**2024-09-05**|**iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**|Yassir Lairgi et.al.|[2409.03284v1](http://arxiv.org/abs/2409.03284v1)|[link](https://github.com/AuvaLab/itext2kg)|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258v1](http://arxiv.org/abs/2409.03258v1)|null|
|**2024-09-05**|**Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**|Jie Ma et.al.|[2409.03155v1](http://arxiv.org/abs/2409.03155v1)|[link](https://github.com/reml-group/dog)|
|**2024-09-04**|**Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**|Junyoung Lee et.al.|[2409.02481v1](http://arxiv.org/abs/2409.02481v1)|null|
|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|Xiongkun Linghu et.al.|[2409.02389v1](http://arxiv.org/abs/2409.02389v1)|null|
|**2024-09-02**|**Grounding Language Models in Autonomous Loco-manipulation Tasks**|Jin Wang et.al.|[2409.01326v1](http://arxiv.org/abs/2409.01326v1)|null|
|**2024-09-02**|**LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**|Haoran Yang et.al.|[2409.01145v1](http://arxiv.org/abs/2409.01145v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**|Yuxiang Wang et.al.|[2409.00727v1](http://arxiv.org/abs/2409.00727v1)|null|
|**2024-08-31**|**WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**|Oktie Hassanzadeh et.al.|[2409.00331v1](http://arxiv.org/abs/2409.00331v1)|[link](https://github.com/IBM/wikicausal)|
|**2024-08-29**|**HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**|Rishi Kalra et.al.|[2409.09046v1](http://arxiv.org/abs/2409.09046v1)|null|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|Jingyi Wang et.al.|[2408.16224v2](http://arxiv.org/abs/2408.16224v2)|null|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903v1](http://arxiv.org/abs/2408.15903v1)|null|
|**2024-08-27**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.|[2408.14895v2](http://arxiv.org/abs/2408.14895v2)|[link](https://github.com/aistairc/virtualhome_aist)|
|**2024-08-27**|**XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**|Yasir Ali Farrukh et.al.|[2408.16021v1](http://arxiv.org/abs/2408.16021v1)|[link](https://github.com/yasir-ali-farrukh/gnn4id)|
|**2024-08-26**|**Process Trace Querying using Knowledge Graphs and Notation3**|William Van Woensel et.al.|[2409.04452v1](http://arxiv.org/abs/2409.04452v1)|null|
|**2024-08-26**|**PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**|Runtao Ren et.al.|[2409.00092v1](http://arxiv.org/abs/2409.00092v1)|null|
|**2024-08-26**|**DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**|Ziai Zhou et.al.|[2408.14185v1](http://arxiv.org/abs/2408.14185v1)|null|
|**2024-08-26**|**Exploring the Potential of Large Language Models for Heterophilic Graphs**|Yuxia Wu et.al.|[2408.14134v1](http://arxiv.org/abs/2408.14134v1)|null|
|**2024-08-26**|**Towards Graph Prompt Learning: A Survey and Beyond**|Qingqing Long et.al.|[2408.14520v2](http://arxiv.org/abs/2408.14520v2)|null|
|**2024-08-25**|**CodeGraph: Enhancing Graph Reasoning of LLMs with Code**|Qiaolong Cai et.al.|[2408.13863v1](http://arxiv.org/abs/2408.13863v1)|null|
|**2024-08-25**|**LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**|Duo Wang et.al.|[2408.14512v1](http://arxiv.org/abs/2408.14512v1)|null|
|**2024-08-24**|**Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**|Sakhinana Sagar Srinivas et.al.|[2408.13661v1](http://arxiv.org/abs/2408.13661v1)|null|
|**2024-08-24**|**GNN: Graph Neural Network and Large Language Model for Data Discovery**|Thomas Hoang et.al.|[2408.13609v2](http://arxiv.org/abs/2408.13609v2)|null|
|**2024-08-24**|**HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**|Azmine Toushik Wasi et.al.|[2408.13521v1](http://arxiv.org/abs/2408.13521v1)|[link](https://github.com/azminewasi/hrgraph)|
|**2024-08-24**|**Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**|Yi-Hui Chen et.al.|[2408.13432v1](http://arxiv.org/abs/2408.13432v1)|null|
|**2024-08-23**|**CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**|Ekaterina Trofimova et.al.|[2408.13366v1](http://arxiv.org/abs/2408.13366v1)|null|
|**2024-08-23**|**Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**|Sakhinana Sagar Srinivas et.al.|[2408.14494v1](http://arxiv.org/abs/2408.14494v1)|null|
|**2024-08-22**|**A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**|Ekdeep Singh Lubana et.al.|[2408.12578v2](http://arxiv.org/abs/2408.12578v2)|[link](https://github.com/ekdeepslubana/conceptpercolation)|
|**2024-08-22**|**Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**|Arief Purnama Muharram et.al.|[2409.00061v1](http://arxiv.org/abs/2409.00061v1)|null|
|**2024-08-22**|**Cell-ontology guided transcriptome foundation model**|Xinyu Yuan et.al.|[2408.12373v1](http://arxiv.org/abs/2408.12373v1)|null|
|**2024-08-22**|**Graph Retrieval Augmented Trustworthiness Reasoning**|Ying Zhu et.al.|[2408.12333v2](http://arxiv.org/abs/2408.12333v2)|[link](https://github.com/EvoNexusX/Graph-Retrieval-Augmented-Trustworthiness-Reasoning)|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**|Junlin He et.al.|[2408.12116v1](http://arxiv.org/abs/2408.12116v1)|null|
|**2024-08-21**|**Enabling Small Models for Zero-Shot Classification through Model Label Learning**|Jia Zhang et.al.|[2408.11449v1](http://arxiv.org/abs/2408.11449v1)|null|
|**2024-08-20**|**Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**|Zhilong Wang et.al.|[2408.11182v1](http://arxiv.org/abs/2408.11182v1)|null|
|**2024-08-20**|**Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**|Thomas Hoang et.al.|[2408.11133v1](http://arxiv.org/abs/2408.11133v1)|null|
|**2024-08-20**|**Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**|Rui Yang et.al.|[2408.10819v1](http://arxiv.org/abs/2408.10819v1)|null|
|**2024-08-20**|**Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**|Litian Huang et.al.|[2408.10592v1](http://arxiv.org/abs/2408.10592v1)|[link](https://github.com/ferretdoll/hgr)|
|**2024-08-19**|**Query languages for neural networks**|Martin Grohe et.al.|[2408.10362v2](http://arxiv.org/abs/2408.10362v2)|null|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124v1](http://arxiv.org/abs/2408.10124v1)|[link](https://github.com/zhangtia16/molgraph-lardo)|
|**2024-08-19**|**Geometry Informed Tokenization of Molecules for Language Model Generation**|Xiner Li et.al.|[2408.10120v1](http://arxiv.org/abs/2408.10120v1)|null|
|**2024-08-19**|**GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**|Ran Liu et.al.|[2408.10115v1](http://arxiv.org/abs/2408.10115v1)|[link](https://github.com/oswald1997/glimmer)|
|**2024-08-19**|**SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**|Pengjie Liu et.al.|[2408.09717v1](http://arxiv.org/abs/2408.09717v1)|null|
|**2024-08-18**|**Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**|Xinnan Dai et.al.|[2408.09529v1](http://arxiv.org/abs/2408.09529v1)|null|
|**2024-08-18**|**Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**|Geethan Sannidhi et.al.|[2408.13273v1](http://arxiv.org/abs/2408.13273v1)|null|
|**2024-08-18**|**Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**|Kening Zheng et.al.|[2408.09429v1](http://arxiv.org/abs/2408.09429v1)|null|
|**2024-08-16**|**ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**|Debashis Gupta et.al.|[2408.08972v1](http://arxiv.org/abs/2408.08972v1)|null|
|**2024-08-16**|**EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**|Chenwei Wan et.al.|[2408.08782v1](http://arxiv.org/abs/2408.08782v1)|[link](https://github.com/cw-wan/EmoDynamiX-v2)|
|**2024-08-16**|**Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**|Zhongjian Zhang et.al.|[2408.08685v1](http://arxiv.org/abs/2408.08685v1)|null|
|**2024-08-16**|**RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**|Meng Chen et.al.|[2408.08933v1](http://arxiv.org/abs/2408.08933v1)|[link](https://github.com/matchyc/RoarGraph)|
|**2024-08-16**|**Handling abort commands for household kitchen robots**|Darius Has et.al.|[2408.14480v1](http://arxiv.org/abs/2408.14480v1)|null|
|**2024-08-16**|**CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**|Rong-Ching Chang et.al.|[2408.08535v1](http://arxiv.org/abs/2408.08535v1)|null|
|**2024-08-15**|**VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**|Chia-Tung Ho et.al.|[2408.08927v1](http://arxiv.org/abs/2408.08927v1)|null|
|**2024-08-15**|**Graph Retrieval-Augmented Generation: A Survey**|Boci Peng et.al.|[2408.08921v2](http://arxiv.org/abs/2408.08921v2)|[link](https://github.com/pengboci/graphrag-survey)|
|**2024-08-14**|**Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**|Jiri Hron et.al.|[2408.07852v1](http://arxiv.org/abs/2408.07852v1)|null|
|**2024-08-14**|**ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**|Xuanqing Yu et.al.|[2408.07840v1](http://arxiv.org/abs/2408.07840v1)|null|
|**2024-08-14**|**WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**|Weijian Xie et.al.|[2408.07611v2](http://arxiv.org/abs/2408.07611v2)|null|
|**2024-08-14**|**Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**|Tobias A. Opsahl et.al.|[2408.07453v1](http://arxiv.org/abs/2408.07453v1)|[link](https://github.com/tobias-opsahl/fact-or-fiction)|
|**2024-08-13**|**LLMs can Schedule**|Henrik Abgaryan et.al.|[2408.06993v1](http://arxiv.org/abs/2408.06993v1)|[link](https://github.com/starjob42/datasetjsp)|
|**2024-08-13**|**Causal Agent based on Large Language Model**|Kairong Han et.al.|[2408.06849v1](http://arxiv.org/abs/2408.06849v1)|[link](https://github.com/kairong-han/causal_agent)|

#### Abstracts
##### **Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**
2409.11147v1 by Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen

Large language models(LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning(ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÂ∞ëÈáèÂ≠∏ÁøíËÉΩÂäõÔºå‰∏¶ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÊäÄË°ìÁµ±‰∏Ä‰∫Ü NLP ‰ªªÂãôÁöÑÁØÑ‰æã„ÄÇÂÑòÁÆ° ICL Â∑≤ÊàêÂäüÔºåÁØÑ‰æãÁ§∫ÁØÑÁöÑÂìÅË≥™ÊúÉÈ°ØËëóÂΩ±Èüø LLM ÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÁØÑ‰æãÈÅ∏ÊìáÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñºÊü•Ë©¢ËàáÂÄôÈÅ∏ÁØÑ‰æã‰πãÈñìÁöÑË™ûÊÑèÁõ∏‰ººÊÄß„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÊé®ÁêÜÊ≠•È©ü‰πãÈñìÁöÑÈÇèËºØÈÄ£ÁµêÊúâÂä©ÊñºÊèèÁπ™ÂïèÈ°åËß£Ê±∫ÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êé®ÁêÜÂúñÂ¢ûÂº∑ÁØÑ‰æãÊ™¢Á¥¢ (RGER) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇRGER È¶ñÂÖàË¶ÅÊ±Ç LLM Áî¢Áîü‰∏ÄÂÄãÂàùÂßãÂõûÊáâÔºåÁÑ∂ÂæåÂ∞á‰∏≠ÈñìÂïèÈ°åËß£Ê±∫Ê≠•È©üË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁµêÊßã„ÄÇ‰πãÂæåÔºåÂÆÉÊé°Áî®ÂúñÂΩ¢Ê†∏ÈÅ∏ÂèñÂÖ∑ÊúâË™ûÊÑèÂíåÁµêÊßãÁõ∏‰ººÊÄßÁöÑÁØÑ‰æã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÁµêÊßãÈóú‰øÇÊúâÂä©ÊñºÊü•Ë©¢ÂíåÂÄôÈÅ∏ÁØÑ‰æãÁöÑÂ∞çÈΩä„ÄÇRGER Âú®Êï∏Â≠∏ÂíåÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏äÁöÑÂäüÊïàÂ±ïÁ§∫‰∫ÜÂÆÉÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÁôºÂ∏ÉÊñº https://github.com/Yukang-Lin/RGER„ÄÇ

##### **Semformer: Transformer Language Models with Semantic Planning**
2409.11143v1 by Yongjing Yin, Junran Ding, Kai Song, Yue Zhang

Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.

ÊëòË¶ÅÔºöÂú®Áï∂ÂâçÁöÑË™ûË®ÄÊ®°Âûã‰∏≠Ôºå‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨ÊòØ‰∏ªÂ∞éÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂú®Ë®ìÁ∑¥ÈöéÊÆµÔºåÊ®°ÂûãÊé°Áî®ÊïôÂ∏´Âº∑Âà∂Ê≥ïÔºåÊ†πÊìöÊâÄÊúâÂâç‰∏ÄÂÄãÁöÑÁúüÂØ¶Ë©ûÂΩô‰æÜÈ†êÊ∏¨Ë©ûÂΩô„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÈÄôÁ®ÆÊñπÊ≥ïÊúÉÁî¢ÁîüÊç∑ÂæëÔºåÂà©Áî®Â∑≤Êè≠Èú≤ÁöÑÂâçÁ∂¥‰æÜËôõÂÅáÂú∞Á¨¶ÂêàÂæåÁ∫åÁöÑË©ûÂΩôÔºåÊΩõÂú®ÊúÉÂç±ÂÆ≥‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨Âô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π SemformerÔºå‰∏ÄÁ®ÆË®ìÁ∑¥ Transformer Ë™ûË®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ïÔºåÊòéÁ¢∫Âú∞Âª∫ÊßãÂõûÊáâÁöÑË™ûÊÑèË¶èÂäÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàóË¶èÂäÉË©ûÂΩôÁ¥çÂÖ•ÂâçÁ∂¥ÔºåÂºïÂ∞éË¶èÂäÉË©ûÂΩôÁöÑË°®ÂæµÂéªÈ†êÊ∏¨ÂõûÊáâÁöÑÊΩõÂú®Ë™ûÊÑèË°®ÂæµÔºåÈÄô‰∫õË°®ÂæµÊòØÁî±Ëá™ÂãïÁ∑®Á¢ºÂô®Ë™òÂ∞éÁöÑ„ÄÇÂú®‰∏ÄÂÄãÊúÄÂ∞èÁöÑË¶èÂäÉ‰ªªÂãôÔºàÂç≥ÂúñÂΩ¢Ë∑ØÂæëÂ∞ãÊâæÔºâ‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæÂá∫Êé•ËøëÂÆåÁæéÁöÑÊïàËÉΩÔºå‰∏¶ÊúâÊïàÂú∞Ê∏õËºïÊç∑ÂæëÂ≠∏ÁøíÔºåÈÄôÊòØÊ®ôÊ∫ñË®ìÁ∑¥ÊñπÊ≥ïÂíåÂü∫Á∑öÊ®°ÂûãÁÑ°Ê≥ïÈÅîÊàêÁöÑÂ£ØËàâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÈ†≠ÈñãÂßã‰ΩøÁî® 1.25 ÂÑÑÂÄãÂèÉÊï∏È†êË®ìÁ∑¥ SemformerÔºåÈÄèÈÅéÂõ∞ÊÉëÂ∫¶„ÄÅË™ûÂ¢ÉÂ≠∏ÁøíÂíåÂú®ÊëòË¶Å‰ªªÂãô‰∏äÁöÑÂæÆË™ø‰æÜË≠âÊòéÂÖ∂ÂäüÊïà„ÄÇ

##### **KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**
2409.10921v1 by Yanbei Jiang, Krista A. Ehinger, Jey Han Lau

Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.

ÊëòË¶ÅÔºöÊé¢Á¥¢Áî±ÁæéÊúØÁªòÁîª‰º†ËææÁöÑÂèô‰∫ãÊòØÂõæÂÉèÂ≠óÂπï‰∏≠ÁöÑÊåëÊàòÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏ç‰ªÖÂáÜÁ°ÆÂú∞Ë°®Á§∫ËßÜËßâÂÜÖÂÆπËÄå‰∏îËøòÊèê‰æõÂØπËâ∫ÊúØÂìÅÂê´‰πâÁöÑÊ∑±ÂÖ•Ëß£ÈáäÁöÑÊèèËø∞„ÄÇÁî±‰∫éÂÖ∂‰∏çÂêåÁöÑËß£ÈáäÂíåË∑®‰∏çÂêåËâ∫ÊúØÊµÅÊ¥æÂíåÈ£éÊ†ºÁöÑ‰∏çÂêåÁæéÂ≠¶ÂéüÂàôÔºåËøôÈ°π‰ªªÂä°ÂØπ‰∫éËâ∫ÊúØÂìÅÂõæÂÉèÊù•ËØ¥Â∞§ÂÖ∂Â§çÊùÇ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü KALE Áü•ËØÜÂ¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁî®‰∫éËâ∫ÊúØÂìÅÈòêÈáäÔºå‰∏ÄÁßçÈÄöËøáÂ∞ÜËâ∫ÊúØÂìÅÂÖÉÊï∞ÊçÆ‰Ωú‰∏∫ÈôÑÂä†Áü•ËØÜÊù•Â¢ûÂº∫Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇKALE ‰ª•‰∏§ÁßçÊñπÂºèÂêàÂπ∂ÂÖÉÊï∞ÊçÆÔºöÈ¶ñÂÖà‰Ωú‰∏∫Áõ¥Êé•ÊñáÊú¨ËæìÂÖ•ÔºåÂÖ∂Ê¨°ÈÄöËøáÂ§öÊ®°ÊÄÅÂºÇÊûÑÁü•ËØÜÂõæ„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÂõæË°®ÁöÑÂ≠¶‰π†Ë°®Á§∫ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÊçüÂ§±ÔºåÂÆÉÊúÄÂ§ßÂåñÂõæÂÉè‰∏éÂÖ∂ÂØπÂ∫îÂÖÉÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKALE Âú®‰ΩøÁî® CIDEr ËØÑ‰º∞Êó∂ÔºåÂú®Âá†‰∏™Ëâ∫ÊúØÂìÅÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩÔºàÁâπÂà´ÊòØ‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂ∑•‰ΩúÁõ∏ÊØîÔºâ„ÄÇËØ•È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/Yanbei-Jiang/Artwork-Interpretation Ëé∑Âæó„ÄÇ

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊèêÁ§∫Â≠¶‰π†Ê°ÜÊû∂ÁöÑÁü•ËØÜÂ¢ûÂº∫ÁñæÁóÖËØäÊñ≠ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ï‰ªé‰∏é‰∏¥Â∫äÁóÖ‰æãÁõ∏ÂÖ≥ÁöÑÂ§ñÈÉ®Áü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢ÁªìÊûÑÂåñÁü•ËØÜÔºåÂØπÂÖ∂ËøõË°åÁºñÁ†ÅÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞ÊèêÁ§∫Ê®°Êùø‰∏≠Ôºå‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÂØπ‰ªªÂä°ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®‰∏â‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºöCHIP-CTC„ÄÅIMCS-V2-NER Âíå KUAKE-QTR„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÊòéÊòæ‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÂú® CHIP-CTC Êï∞ÊçÆÈõÜ‰∏äÁöÑ F1 ÂæóÂàÜÊèêÈ´ò‰∫Ü 2.4%ÔºåÂú® IMCS-V2-NER Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 3.1%ÔºåÂú® KUAKE-QTR Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 4.2%„ÄÇÊ≠§Â§ñÔºåÊ∂àËûçÁ†îÁ©∂ËØÅÂÆû‰∫ÜÁü•ËØÜÊ≥®ÂÖ•Ê®°ÂùóÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂõ†‰∏∫ÁßªÈô§Ê≠§Ê®°Âùó‰ºöÂØºËá¥ F1 ÂæóÂàÜÊòæÁùÄ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏ç‰ªÖÊúâÊïàÊèêÈ´ò‰∫ÜÁñæÁóÖËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÂ¢ûÂº∫‰∫ÜÈ¢ÑÊµãÁöÑÂèØËß£ÈáäÊÄßÔºå‰∏∫‰∏¥Â∫äËØäÊñ≠Êèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑÊîØÊåÅÂíåËØÅÊçÆ„ÄÇ

##### **MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**
2409.10294v1 by Shanshan Wang, Chun Zhang, Ning Zhang

The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÂà∞ÊñáÂ≠óÁîüÊàê‰ªªÂãôÊó®Âú®Â∞áÁµêÊßãÂåñÁü•Ë≠òÂúñË≠úËΩâÊèõÁÇ∫ÈÄ£Ë≤´‰∏î‰∫∫È°ûÂèØËÆÄÁöÑËá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÊúÄËøëÂú®ÈÄôÂÄãÈ†òÂüüÁöÑÁ†îÁ©∂ÈõÜ‰∏≠ÊñºÈÄèÈÅéÁ¥çÂÖ•ÂúñÂΩ¢ÁµêÊßãË≥áË®ä‰æÜÂ¢ûÂº∑È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM)Ôºå‰ª•Êì∑ÂèñÁü•Ë≠òÂúñË≠úÁöÑË§áÈõúÁµêÊßãÁ¥∞ÁØÄ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂ§ßÂ§öÂÇæÂêëÊñºÂÉÖÊì∑ÂèñÂñÆ‰∏ÄÁ≤íÂ∫¶ÁöÑÁµêÊßãË≥áË®äÔºåÂ∞àÊ≥®ÊñºÂéüÂßãÂúñÂΩ¢‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇÊàñÂêå‰∏ÄÂÄãÂØ¶È´îÊàñ‰∏çÂêåÂØ¶È´î‰πãÈñìÁöÑÂñÆÂ≠óÈóú‰øÇ„ÄÇÈÄôÁ®ÆÁãπÈöòÁöÑÁÑ¶ÈªûÂ∞éËá¥‰∏ÄÂÄãÈ°ØËëóÁöÑÈôêÂà∂ÔºöÂÉÖÂ∞àÊ≥®ÊñºÂØ¶È´îÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÁÑ°Ê≥ïÊì∑ÂèñÂñÆÂ≠ó‰πãÈñìÁ¥∞ÂæÆÁöÑË™ûÁæ©Èóú‰øÇÔºåËÄåÂÉÖÂ∞àÊ≥®ÊñºÂñÆÂ≠óÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÂâáÂøΩÁï•‰∫ÜÂéüÂßãÊï¥ÂÄãÂØ¶È´î‰πãÈñìÁöÑÊõ¥Âª£Ê≥õÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÂºïÂÖ•‰∫ÜÂü∫Êñº PLM ÁöÑÂ§öÁ≤íÂ∫¶ÂúñÂΩ¢ÁµêÊßãÊ≥®ÊÑèÂäõ (MGSA)„ÄÇÊ®°ÂûãÊû∂ÊßãÁöÑÁ∑®Á¢ºÂô®ÂÖ∑ÊúâÂØ¶È´îÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑ„ÄÅÂñÆÂ≠óÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑÂíå‰∏ÄÂÄãÂæûÂÖ©ÂÄãÁµêÊßã‰∏≠Á∂úÂêàË≥áË®äÁöÑËÅöÂêàÊ®°ÁµÑ„ÄÇÈÄôÁ®ÆÂ§öÁ≤íÂ∫¶ÁµêÊßãÁ∑®Á¢ºÊñπÊ≥ïÂÖÅË®±Ê®°ÂûãÂêåÊôÇÊì∑ÂèñÂØ¶È´îÂ±§Á¥öÂíåÂñÆÂ≠óÂ±§Á¥öÁµêÊßãË≥áË®äÔºåÊèê‰æõÂ∞çÁü•Ë≠òÂúñË≠úÁµêÊßãË≥áË®äÊõ¥ÂÖ®Èù¢ÁöÑÁêÜËß£ÔºåÂæûËÄåÈ°ØËëóÊèêÂçáÁîüÊàêÊñáÂ≠óÁöÑÂìÅË≥™„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÂª£Ê≥õË™çÂèØÁöÑ KG Âà∞ÊñáÂ≠óÁîüÊàêÂü∫Ê∫ñË≥áÊñôÈõÜ WebNLG Âíå EventNarrative Â∞ç MGSA Ê®°ÂûãÈÄ≤Ë°åÂª£Ê≥õË©ï‰º∞ÔºåÂú®ÈÄô‰∫õË≥áÊñôÈõÜ‰∏äÔºåÂÆÉÂßãÁµÇÂÑ™ÊñºÂÉÖ‰æùË≥¥ÂñÆ‰∏ÄÁ≤íÂ∫¶ÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**
2409.10077v1 by Le Xiao, Yunfei Xu, Jing Zhao

Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.

ÊëòË¶ÅÔºö<paragraph>È†òÂüüÁâπÂÆöÂëΩÂêçÂØ¶È´îËæ®Ë≠òÔºàNERÔºâÔºåÂÖ∂ÁõÆÊ®ôÊòØËæ®Ë≠òÈ†òÂüüÁâπÂÆöÂØ¶È´îÂèäÂÖ∂È°ûÂà•ÔºåÁÇ∫Âª∫ÊßãÈ†òÂüüÁü•Ë≠òÂúñË≠úÊèê‰æõÈáçË¶ÅÁöÑÊîØÊè¥„ÄÇÁõÆÂâçÔºåÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂª£Ê≥õÁî®Êñº NER ‰ªªÂãô‰∏îÂçÅÂàÜÊúâÊïàÔºå‰ΩÜÁî±Êñº‰æùË≥¥ÊñºÂ§ßË¶èÊ®°Ê®ôË®òË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÁâπÂÆöÈ†òÂüü‰∏≠Ê®ôË®òË≥áÊñôÁöÑÁ®ÄÂ∞ëÊúÉÈôêÂà∂ÂÖ∂ÊáâÁî®„ÄÇÂõ†Ê≠§ÔºåË®±Â§öÁ†îÁ©∂ÈñãÂßãÂºïÂÖ•Â∞ëÈáèÊ®£Êú¨ÊñπÊ≥ï‰∏¶Áç≤Âæó‰∏Ä‰∫õÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÁâπÂÆöÈ†òÂüü‰∏≠ÁöÑÂØ¶È´îÁµêÊßãÈÄöÂ∏∏ÂæàË§áÈõúÔºåËÄåÁõÆÂâçÁöÑÂ∞ëÈáèÊ®£Êú¨ÊñπÊ≥ïÈõ£‰ª•ÈÅ©ÊáâÂÖ∑ÊúâË§áÈõúÁâπÂæµÁöÑ NER ‰ªªÂãô„ÄÇ‰ª•‰∏≠ÂúãÁÖ§ÂåñÂ∑•Áî¢Ê•≠È†òÂüüÁÇ∫‰æãÔºåÂ≠òÂú®Â§öÂÄãÂØ¶È´îÂÖ±Áî®ÂñÆ‰∏ÄÂØ¶È´îÁöÑË§áÈõúÁµêÊßãÔºå‰ª•ÂèäÂêå‰∏ÄÂ∞çÂØ¶È´îÊúâÂ§öÈáçÈóú‰øÇÔºåÈÄôÊúÉÂΩ±ÈüøÊ®£Êú¨ËºÉÂ∞ëÊ¢ù‰ª∂‰∏ãÁöÑ NER ‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂØ¶È´îËæ®Ë≠òÊû∂Êßã LLM-DERÔºåÁî®Êñº‰∏≠ÊñáÈ†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠òÂïèÈ°åÔºåÈÄöÈÅé LLM ÁîüÊàêÂåÖÂê´ÂØ¶È´îÈ°ûÂûãÁöÑÈóú‰øÇÊ∏ÖÂñÆÔºå‰∏¶Ë®≠Ë®à‰∏ÄÂÄãÂêàÁêÜÊÄßÂíå‰∏ÄËá¥ÊÄßË©ï‰º∞ÊñπÊ≥ï‰æÜÁßªÈô§Ëæ®Ë≠òÈåØË™§ÁöÑÂØ¶È´îÔºåÂæûËÄåÂèØ‰ª•ÊúâÊïàËß£Ê±∫ÁâπÂÆöÈ†òÂüü‰∏≠Ë§áÈõúÁµêÊßãÂØ¶È´îËæ®Ë≠òÂïèÈ°å„ÄÇÊú¨ÊñáÂú® Resume Ë≥áÊñôÈõÜÂíåËá™Âª∫ÁÖ§ÂåñÂ∑•Ë≥áÊñôÈõÜ Coal ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLM-DER Âú®È†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠ò‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏çÂÉÖÂÑ™ÊñºÁèæÊúâÁöÑ GPT-3.5-turbo Âü∫Ê∫ñÔºåÈÇÑË∂ÖÈÅé‰∫ÜÂÆåÂÖ®Áõ£Áù£ÁöÑÂü∫Á∑öÔºåÈ©óË≠â‰∫ÜÂÖ∂Âú®ÂØ¶È´îËæ®Ë≠ò‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ</paragraph>

##### **On the Diagram of Thought**
2409.10038v1 by Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao

We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫ÜÊÄùÊÉ≥ÂúñÔºàDoTÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂÆÉÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑËø≠‰ª£Êé®ÁêÜÂª∫Ê®°ÁÇ∫Âú®ÂñÆ‰∏ÄÊ®°ÂûãÂÖßÂª∫Êßã‰∏ÄÂÄãÊúâÂêëÁÑ°Áí∞ÂúñÔºàDAGÔºâ„ÄÇËàáÂ∞áÊé®ÁêÜË°®Á§∫ÁÇ∫Á∑öÊÄßÈèàÊàñÊ®πÁöÑÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåDoT Â∞áÂëΩÈ°å„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÂíåÈ©óË≠âÁµÑÁπîÊàê‰∏ÄÂÄãÊúâÂáùËÅöÂäõÁöÑ DAG ÁµêÊßãÔºåÂÖÅË®±Ê®°ÂûãÊé¢Á¥¢Ë§áÈõúÁöÑÊé®ÁêÜË∑ØÂæëÔºåÂêåÊôÇ‰øùÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄß„ÄÇÂúñË°®‰∏≠ÁöÑÊØèÂÄãÁØÄÈªûÂ∞çÊáâÊñº‰∏ÄÂÄãÂ∑≤Ë¢´ÊèêÂá∫„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÊàñÈ©óË≠âÁöÑÂëΩÈ°åÔºå‰Ωø LLM ËÉΩÂ§†ÈÄöÈÅéËá™ÁÑ∂Ë™ûË®ÄÂõûÈ•ãËø≠‰ª£Âú∞ÊîπÈÄ≤ÂÖ∂Êé®ÁêÜ„ÄÇÈÄöÈÅéÂà©Áî®ÂÖ∑ÊúâËßíËâ≤ÁâπÂÆöÊ®ôË®òÁöÑËá™ÂãïÂõûÊ≠∏‰∏ã‰∏ÄÂÄãÊ®ôË®òÈ†êÊ∏¨ÔºåDoT ‰øÉÈÄ≤‰∫ÜÊèêÂá∫ÊÉ≥Ê≥ïÂíåÊâπÂà§ÊÄßË©ï‰º∞ÂÆÉÂÄë‰πãÈñìÁöÑÁÑ°Á∏´ÈÅéÊ∏°ÔºåÊèê‰æõ‰∫ÜÊØî‰∫åÂÖÉ‰ø°ËôüÊõ¥Ë±êÂØåÁöÑÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî®ÊãìÊí≤ÁêÜË´ñÂΩ¢ÂºèÂåñ‰∫Ü DoT Ê°ÜÊû∂ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊï∏Â≠∏Âü∫Á§éÔºå‰ª•Á¢∫‰øùÊé®ÁêÜÈÅéÁ®ã‰∏≠ÁöÑÈÇèËºØ‰∏ÄËá¥ÊÄßÂíåÂÅ•ÂÖ®ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÂñÆ‰∏Ä LLM ÂÖßÁöÑË®ìÁ∑¥ÂíåÊé®ÁêÜÈÅéÁ®ãÔºåÊ∂àÈô§‰∫ÜÂ∞çÂ§öÂÄãÊ®°ÂûãÊàñÂ§ñÈÉ®ÊéßÂà∂Ê©üÂà∂ÁöÑÈúÄË¶Å„ÄÇDoT ÁÇ∫Ë®≠Ë®à‰∏ã‰∏Ä‰ª£Êé®ÁêÜÂ∞àÁî®Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊ¶ÇÂøµÊ°ÜÊû∂ÔºåÂº∑Ë™øË®ìÁ∑¥ÊïàÁéá„ÄÅÂº∑Â§ßÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁêÜË´ñÂü∫Á§é„ÄÇ‰ª£Á¢ºÂèØÂú® https://github.com/diagram-of-thought/diagram-of-thought Áç≤Âæó„ÄÇ

##### **Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**
2409.09362v1 by Yuanjie Lyu, Tong Xu, Zihan Niu, Bo Peng, Jing Ke, Enhong Chen

The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑËì¨ÂãÉÁôºÂ±ïÔºåÊèêÂçá‰∫ÜÂ∞çË™ûÊÑèË±êÂØåÊúçÂãôÔºà‰æãÂ¶Ç‰∫ã‰ª∂ÂíåÊïÖ‰∫ãÁ∑öÊ≠∏Âõ†ÔºâÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂Â§ßÂ§öËëóÈáçÊñºÁâáÊÆµÂ±§Á¥öÁöÑ‰∫ã‰ª∂ÁêÜËß£Ôºå‰∏ªË¶ÅÊòØÈÄèÈÅéÂü∫Á§éÁöÑÂ≠óÂπï‰ªªÂãôÔºåËÄåÊú™ÂàÜÊûêÊï¥ÈÉ®ÈõªÂΩ±‰∏≠‰∫ã‰ª∂ÁôºÁîüÁöÑÂéüÂõ†„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫Âç≥‰ΩøÊòØÈÄ≤ÈöéÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰πüÊúÉÂõ†ÁÇ∫ÂèóÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ËÄåÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑÂ§öÊ®°ÊÖãË≥áË®ä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÂâçÁΩÆË©ûÂ¢ûÂº∑ MLLM (TSPE) ÊñπÊ≥ïÔºåÁî®ÊñºÈõªÂΩ±ÂΩ±Áâá‰∏≠ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†Ôºå‰πüÂ∞±ÊòØÂ∞áÁõ∏Èóú‰∫ã‰ª∂ËàáÂÖ∂Âõ†ÊûúË™ûÊÑèÈÄ£ÁµêËµ∑‰æÜ„ÄÇÂú®Â±ÄÈÉ®ÈöéÊÆµÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰∫íÂãïÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁâáÊÆµ‰∏≠ÁöÑÁõ∏ÈóúÂ§öÊ®°ÊÖãË≥áË®äÔºåÁ∞°Ë¶ÅÂú∞Á∏ΩÁµêÂñÆ‰∏Ä‰∫ã‰ª∂„ÄÇÁõ∏ÊáâÂú∞ÔºåÂú®Êï¥È´îÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî®Êé®ÁêÜÁü•Ë≠òÂúñË≠úÂº∑ÂåñÁõ∏Èóú‰∫ã‰ª∂‰πãÈñìÁöÑÈÄ£ÁµêÔºå‰∏¶Ë®≠Ë®à‰∫Ü‰∏ÄÂÄã‰∫ã‰ª∂ÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÁõ∏Èóú‰∫ã‰ª∂ÔºåËÄåÈùûÊâÄÊúâÂâçÁΩÆÁâáÊÆµÔºåÈÄ≤ËÄåÁî¢ÁîüÊ∫ñÁ¢∫ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†„ÄÇÂ∞çÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÁöÑÂÖ®Èù¢Ë©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇ

##### **ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**
2409.09318v1 by Yahan Tu, Rui Hu, Jitao Sang

Hallucination poses a significant challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
static, which can lead to potential data contamination. This paper introduces
ODE, an open-set, dynamic protocol for evaluating object existence
hallucinations in MLLMs. Our framework employs graph structures to model
associations between real-word concepts and generates novel samples for both
general and domain-specific scenarios. The dynamic combination of concepts,
along with various combination principles, ensures a broad sample distribution.
Experimental results show that MLLMs exhibit higher hallucination rates with
ODE-generated samples, effectively avoiding data contamination. Moreover, these
samples can also be used for fine-tuning to improve MLLM performance on
existing benchmarks.

ÊëòË¶ÅÔºöÂπªË¶∫Â∞çÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË©ï‰º∞ÂπªË¶∫Âü∫Ê∫ñÊòØÈùúÊÖãÁöÑÔºåÈÄôÂèØËÉΩÂ∞éËá¥ÊΩõÂú®ÁöÑË≥áÊñôÊ±°Êüì„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü ODEÔºå‰∏ÄÁ®ÆÈñãÊîæÂºè„ÄÅÂãïÊÖãÁöÑÂçîÂÆöÔºåÁî®ÊñºË©ï‰º∞ MLLM ‰∏≠ÁöÑÁâ©‰ª∂Â≠òÂú®ÂπªË¶∫„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊé°Áî®ÂúñÂΩ¢ÁµêÊßã‰æÜÂª∫Ê®°ÁúüÂØ¶‰∏ñÁïåÊ¶ÇÂøµ‰πãÈñìÁöÑÈóúËÅØÔºå‰∏¶ÁÇ∫‰∏ÄËà¨ÂíåÁâπÂÆöÈ†òÂüüÊÉÖÂ¢ÉÁî¢ÁîüÊñ∞ÁöÑÁØÑ‰æã„ÄÇÊ¶ÇÂøµÁöÑÂãïÊÖãÁµÑÂêàÔºå‰ª•ÂèäÂêÑÁ®ÆÁµÑÂêàÂéüÂâáÔºåÁ¢∫‰øù‰∫ÜÂª£Ê≥õÁöÑÁØÑ‰æãÂàÜ‰Ωà„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåMLLM Âú® ODE ÁîüÊàêÁöÑÁØÑ‰æã‰∏≠Ë°®ÁèæÂá∫ËºÉÈ´òÁöÑÂπªË¶∫ÁéáÔºåÊúâÊïàÈÅøÂÖç‰∫ÜË≥áÊñôÊ±°Êüì„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÁØÑ‰æã‰πüÂèØË¢´Áî®ÊñºÂæÆË™øÔºå‰ª•ÊîπÂñÑ MLLM Âú®ÁèæÊúâÂü∫Ê∫ñ‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**
2409.09026v1 by Florian Gr√∂tschla, Luca Str√§ssle, Luca A. Lanzend√∂rfer, Roger Wattenhofer

Music recommender systems frequently utilize network-based models to capture
relationships between music pieces, artists, and users. Although these
relationships provide valuable insights for predictions, new music pieces or
artists often face the cold-start problem due to insufficient initial
information. To address this, one can extract content-based information
directly from the music to enhance collaborative-filtering-based methods. While
previous approaches have relied on hand-crafted audio features for this
purpose, we explore the use of contrastively pretrained neural audio embedding
models, which offer a richer and more nuanced representation of music. Our
experiments demonstrate that neural embeddings, particularly those generated
with the Contrastive Language-Audio Pretraining (CLAP) model, present a
promising approach to enhancing music recommendation tasks within graph-based
frameworks.

ÊëòË¶ÅÔºöÈü≥Ê®ÇÊé®Ëñ¶Á≥ªÁµ±Á∂ìÂ∏∏‰ΩøÁî®Âü∫ÊñºÁ∂≤Ë∑ØÁöÑÊ®°Âûã‰æÜÊì∑ÂèñÈü≥Ê®Ç‰ΩúÂìÅ„ÄÅËóùË°ìÂÆ∂Âíå‰ΩøÁî®ËÄÖ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÈóú‰øÇÁÇ∫È†êÊ∏¨Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ΩÜÁî±ÊñºÂàùÂßãË≥áË®ä‰∏çË∂≥ÔºåÊñ∞ÁöÑÈü≥Ê®Ç‰ΩúÂìÅÊàñËóùË°ìÂÆ∂Á∂ìÂ∏∏Èù¢Ëá®ÂÜ∑ÂïüÂãïÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂèØ‰ª•ÂæûÈü≥Ê®Ç‰∏≠Áõ¥Êé•Êì∑ÂèñÂü∫ÊñºÂÖßÂÆπÁöÑË≥áË®äÔºå‰ª•Â¢ûÂº∑Âü∫ÊñºÂçîÂêåÈÅéÊøæÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰æùË≥¥ÊâãÂ∑•Ë£Ω‰ΩúÁöÑÈü≥Ë®äÁâπÂæµ‰æÜÈÅîÊàêÊ≠§ÁõÆÁöÑÔºå‰ΩÜÊàëÂÄëÊé¢Á¥¢‰ΩøÁî®Â∞çÊØîÈ†êË®ìÁ∑¥Á•ûÁ∂ìÈü≥Ë®äÂµåÂÖ•Ê®°ÂûãÔºåÈÄôÊèê‰æõ‰∫ÜÊõ¥Ë±êÂØå‰∏îÊõ¥Á¥∞Á∑ªÁöÑÈü≥Ê®ÇË°®Á§∫„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÁ•ûÁ∂ìÂµåÂÖ•ÔºåÁâπÂà•ÊòØ‰ΩøÁî®Â∞çÊØîË™ûË®ÄÈü≥Ë®äÈ†êË®ìÁ∑¥ (CLAP) Ê®°ÂûãÁî¢ÁîüÁöÑÂµåÂÖ•ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Áî®ÊñºÂ¢ûÂº∑ÂúñÂΩ¢ÂåñÊ°ÜÊû∂‰∏≠ÁöÑÈü≥Ê®ÇÊé®Ëñ¶‰ªªÂãô„ÄÇ

##### **Contri(e)ve: Context + Retrieve for Scholarly Question Answering**
2409.09010v1 by Kanchan Shivashankar, Nadine Steinmetz

Scholarly communication is a rapid growing field containing a wealth of
knowledge. However, due to its unstructured and document format, it is
challenging to extract useful information from them through conventional
document retrieval methods. Scholarly knowledge graphs solve this problem, by
representing the documents in a semantic network, providing, hidden insights,
summaries and ease of accessibility through queries. Naturally, question
answering for scholarly graphs expands the accessibility to a wider audience.
But some of the knowledge in this domain is still presented as unstructured
text, thus requiring a hybrid solution for question answering systems. In this
paper, we present a two step solution using open source Large Language
Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the
context pertaining to the question from different structured and unstructured
data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,
we implement prompt engineering to improve the information retrieval
performance of the LLM. Our approach achieved an F1 score of 40% and also
observed some anomalous responses from the LLM, that are discussed in the final
part of the paper.

ÊëòË¶ÅÔºöÂ≠∏Ë°ì‰∫§ÊµÅÊòØ‰∏ÄÂÄãÂø´ÈÄüÊàêÈï∑ÁöÑÈ†òÂüüÔºåÂåÖÂê´‰∫ÜË±êÂØåÁöÑÁü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÖ∂ÈùûÁµêÊßãÂåñÂíåÊñá‰ª∂Ê†ºÂºèÔºåÈÄèÈÅéÂÇ≥Áµ±ÁöÑÊñá‰ª∂Ê™¢Á¥¢ÊñπÊ≥ïÂæàÈõ£Âæû‰∏≠ËêÉÂèñÂá∫ÊúâÁî®ÁöÑË≥áË®ä„ÄÇÂ≠∏Ë°ìÁü•Ë≠òÂúñË≠úËß£Ê±∫‰∫ÜÈÄôÂÄãÂïèÈ°åÔºåÂÆÉ‰ª•Ë™ûÁæ©Á∂≤Ë∑ØÂëàÁèæÊñá‰ª∂ÔºåÊèê‰æõÈö±ËóèÁöÑË¶ãËß£„ÄÅÊëòË¶ÅÂíåÈÄèÈÅéÊü•Ë©¢ËºïÈ¨ÜÂ≠òÂèñ„ÄÇËá™ÁÑ∂Âú∞ÔºåÂ≠∏Ë°ìÂúñË≠úÁöÑÂïèÁ≠îÊì¥Â±ï‰∫ÜÂ∞çÊõ¥Âª£Ê≥õÂèóÁúæÁöÑÂ≠òÂèñÊÄß„ÄÇ‰ΩÜÈÄôÂÄãÈ†òÂüü‰∏≠ÁöÑ‰∏Ä‰∫õÁü•Ë≠ò‰ªçÁÑ∂‰ª•ÈùûÁµêÊßãÂåñÊñáÂ≠óÂëàÁèæÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÂÄãÊ∑∑ÂêàËß£Ê±∫ÊñπÊ°à‰æÜÈÄ≤Ë°åÂïèÁ≠îÁ≥ªÁµ±„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÖ©Ê≠•È©üËß£Ê±∫ÊñπÊ°àÔºöLlama3.1 for Scholarly-QALD Ë≥áÊñôÈõÜ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæû‰∏çÂêåÁöÑÁµêÊßãÂåñÂíåÈùûÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫ê‰∏≠ËêÉÂèñËàáÂïèÈ°åÁõ∏ÈóúÁöÑËÑàÁµ°ÔºöDBLP„ÄÅSemOpenAlex Áü•Ë≠òÂúñË≠úÂíåÁ∂≠Âü∫ÁôæÁßëÊñáÂ≠ó„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂØ¶‰ΩúÊèêÁ§∫Â∑•Á®ã‰ª•ÊîπÂñÑ LLM ÁöÑË≥áË®äÊ™¢Á¥¢ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫Ü 40% ÁöÑ F1 ÂàÜÊï∏Ôºå‰∏¶‰∏î‰πüËßÄÂØüÂà∞ LLM ÁöÑ‰∏Ä‰∫õÁï∞Â∏∏ÂõûÊáâÔºåÈÄô‰∫õÂõûÊáâÂú®Êú¨ÊñáÁöÑÊúÄÂæå‰∏ÄÈÉ®ÂàÜ‰∏≠ÈÄ≤Ë°å‰∫ÜË®éË´ñ„ÄÇ

##### **SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**
2409.09007v1 by Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan

Learning representations on large graphs is a long-standing challenge due to
the inter-dependence nature. Transformers recently have shown promising
performance on small graphs thanks to its global attention for capturing
all-pair interactions beyond observed structures. Existing approaches tend to
inherit the spirit of Transformers in language and vision tasks, and embrace
complicated architectures by stacking deep attention-based propagation layers.
In this paper, we attempt to evaluate the necessity of adopting multi-layer
attentions in Transformers on graphs, which considerably restricts the
efficiency. Specifically, we analyze a generic hybrid propagation layer,
comprised of all-pair attention and graph-based propagation, and show that
multi-layer propagation can be reduced to one-layer propagation, with the same
capability for representation learning. It suggests a new technical path for
building powerful and efficient Transformers on graphs, particularly through
simplifying model architectures without sacrificing expressiveness. As
exemplified by this work, we propose a Simplified Single-layer Graph
Transformers (SGFormer), whose main component is a single-layer global
attention that scales linearly w.r.t. graph sizes and requires none of any
approximation for accommodating all-pair interactions. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M, yielding
orders-of-magnitude inference acceleration over peer Transformers on
medium-sized graphs, and demonstrates competitiveness with limited labeled
data.

ÊëòË¶ÅÔºöÂú®Â§ßÂûãÂúñË°®‰∏äÂ≠∏ÁøíË°®ÂæµÁî±ÊñºÁõ∏‰∫í‰æùË≥¥ÁöÑÊÄßË≥™ËÄåÊàêÁÇ∫‰∏ÄÈ†ÖÈï∑ÊúüÁöÑÊåëÊà∞„ÄÇÁî±Êñº Transfomer ËÉΩÂ§†ÈáùÂ∞çÊâÄÊúâÊàêÂ∞ç‰∫íÂãïÈÄ≤Ë°åÂÖ®Â±ÄÈóúÊ≥®ÔºåË∂ÖË∂äËßÄÊ∏¨ÁµêÊßãÔºåÂõ†Ê≠§ÊúÄËøëÂú®Â∞èÂûãÂúñË°®‰∏äÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÂÇæÂêëÊñºÁπºÊâø Transformer Âú®Ë™ûË®ÄÂíåË¶ñË¶∫‰ªªÂãô‰∏≠ÁöÑÁ≤æÁ•ûÔºå‰∏¶ÈÄöÈÅéÂ†ÜÁñäÂü∫ÊñºÊ∑±Â∫¶ÈóúÊ≥®ÁöÑÂÇ≥Êí≠Â±§‰æÜÊé°Áî®Ë§áÈõúÁöÑÊû∂Êßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Ë©ï‰º∞Âú®ÂúñË°®‰∏äÊé°Áî®Â§öÂ±§Ê≥®ÊÑèÂäõ Transformer ÁöÑÂøÖË¶ÅÊÄßÔºåÈÄôÊ•µÂ§ßÂú∞ÈôêÂà∂‰∫ÜÊïàÁéá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü‰∏ÄÂÄãÈÄöÁî®ÁöÑÊ∑∑ÂêàÂÇ≥Êí≠Â±§ÔºåÂÆÉÂåÖÂê´ÊâÄÊúâÊàêÂ∞çÊ≥®ÊÑèÂäõÂíåÂü∫ÊñºÂúñË°®ÁöÑÂÇ≥Êí≠Ôºå‰∏¶Ë°®ÊòéÂ§öÂ±§ÂÇ≥Êí≠ÂèØ‰ª•Á∞°ÂåñÁÇ∫ÂñÆÂ±§ÂÇ≥Êí≠ÔºåÂÖ∑ÊúâÁõ∏ÂêåÁöÑË°®ÂæµÂ≠∏ÁøíËÉΩÂäõ„ÄÇÈÄôÁÇ∫Âú®ÂúñË°®‰∏äÊßãÂª∫Âº∑Â§ßËÄåÈ´òÊïàÁöÑ Transformer Êèê‰æõ‰∫Ü‰∏ÄÊ¢ùÊñ∞ÁöÑÊäÄË°ìË∑ØÂæëÔºåÁâπÂà•ÊòØÈÄöÈÅéÁ∞°ÂåñÊ®°ÂûãÊû∂ÊßãÔºåËÄåÁÑ°ÈúÄÁäßÁâ≤Ë°®ÈÅîËÉΩÂäõ„ÄÇÊ≠£Â¶ÇÈÄôÈ†ÖÂ∑•‰ΩúÊâÄ‰æãË≠âÁöÑÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂåñÁöÑÂñÆÂ±§ÂúñÂΩ¢ Transformer (SGFormer)ÔºåÂÖ∂‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÊòØ‰∏ÄÂÄãÂñÆÂ±§ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºåÂÆÉËàáÂúñÂΩ¢Â§ßÂ∞èÊàêÁ∑öÊÄßÊØî‰æãÔºå‰∏¶‰∏î‰∏çÈúÄË¶Å‰ªª‰ΩïËøë‰ºº‰æÜÈÅ©ÊáâÊâÄÊúâÊàêÂ∞ç‰∫íÂãï„ÄÇÊ†πÊìöÁ∂ìÈ©óÔºåSGFormer ÊàêÂäüÂú∞Êì¥Â±ïÂà∞Á∂≤Ë∑ØË¶èÊ®°ÁöÑÂúñË°® ogbn-papers100MÔºåÂú®‰∏≠Á≠âÂ§ßÂ∞èÁöÑÂúñË°®‰∏äÁî¢Áîü‰∫ÜÊØîÂêåÂÑï Transformer Âø´ÂπæÂÄãÊï∏ÈáèÁ¥öÁöÑÊé®Ë´ñÂä†ÈÄüÔºå‰∏¶Ë≠âÊòé‰∫ÜÂú®Ê®ôÁ±§Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇ

##### **Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**
2409.08864v1 by Zhiqiang Zhong, Davide Mottin

Large Language Models (LLMs) have shown remarkable capabilities in processing
various data structures, including graphs. While previous research has focused
on developing textual encoding methods for graph representation, the emergence
of multimodal LLMs presents a new frontier for graph comprehension. These
advanced models, capable of processing both text and images, offer potential
improvements in graph understanding by incorporating visual representations
alongside traditional textual data. This study investigates the impact of graph
visualisations on LLM performance across a range of benchmark tasks at node,
edge, and graph levels. Our experiments compare the effectiveness of multimodal
approaches against purely textual graph representations. The results provide
valuable insights into both the potential and limitations of leveraging visual
graph modalities to enhance LLMs' graph structure comprehension abilities.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËôïÁêÜÂêÑÁ®ÆÊï∏ÊìöÁµêÊßãÔºàÂåÖÊã¨ÂúñÂΩ¢ÔºâÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑÁ†îÁ©∂ËëóÈáçÊñºÈñãÁôºÂúñÂΩ¢Ë°®Á§∫ÁöÑÊñáÊú¨Á∑®Á¢ºÊñπÊ≥ïÔºå‰ΩÜÂ§öÊ®°ÊÖã LLM ÁöÑÂá∫ÁèæÁÇ∫ÂúñÂΩ¢ÁêÜËß£Êèê‰æõ‰∫ÜÊñ∞ÁöÑÈ†òÂüü„ÄÇÈÄô‰∫õÂÖàÈÄ≤ÁöÑÊ®°ÂûãËÉΩÂ§†ËôïÁêÜÊñáÊú¨ÂíåÂúñÂÉèÔºåÈÄèÈÅéÁµêÂêàË¶ñË¶∫Ë°®Á§∫ËàáÂÇ≥Áµ±ÊñáÊú¨Ë≥áÊñôÔºåÊèê‰æõÂúñÂΩ¢ÁêÜËß£ÁöÑÊΩõÂú®ÊîπÈÄ≤„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂúñÂΩ¢Ë¶ñË¶∫ÂåñÂ∞ç LLM Âú®ÁØÄÈªû„ÄÅÈÇäÁ∑£ÂíåÂúñÂΩ¢Â±§Á¥ö‰∏ÄÁ≥ªÂàóÂü∫Ê∫ñ‰ªªÂãôÁöÑÊïàËÉΩÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊØîËºÉ‰∫ÜÂ§öÊ®°ÊÖãÊñπÊ≥ïËàáÁ¥îÊñáÊú¨ÂúñÂΩ¢Ë°®Á§∫ÁöÑÊúâÊïàÊÄß„ÄÇÁµêÊûúÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£Âà©Áî®Ë¶ñË¶∫ÂúñÂΩ¢Ê®°ÊÖã‰æÜÂ¢ûÂº∑ LLM ÂúñÂΩ¢ÁµêÊßãÁêÜËß£ËÉΩÂäõÁöÑÊΩõÂäõËàáÈôêÂà∂„ÄÇ

##### **A RAG Approach for Generating Competency Questions in Ontology Engineering**
2409.08820v1 by Xueli Pan, Jacco van Ossenbruggen, Victor de Boer, Zhisheng Huang

Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.

ÊëòË¶ÅÔºöËÉΩÂäõÂïèÈ°å (CQ) ÁöÑÂà∂ÂÆöÊòØÂπæÂÄãÊú¨‰ΩìË´ñÁôºÂ±ïÂíåË©ï‰º∞ÊñπÊ≥ïÁöÑ‰∏≠ÂøÉ„ÄÇÂÇ≥Áµ±‰∏äÔºåÂà∂ÂÆöÈÄô‰∫õËÉΩÂäõÂïèÈ°åÁöÑ‰ªªÂãôÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÈ†òÂüüÂ∞àÂÆ∂ÂíåÁü•Ë≠òÂ∑•Á®ãÂ∏´ÁöÑÂä™ÂäõÔºåÈÄôÈÄöÂ∏∏ÊòØËÄóÊôÇ‰∏îÂãûÂäõÂØÜÈõÜÁöÑ„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåËá™ÂãïÂåñÂíåÂ¢ûÂº∑Ê≠§ÈÅéÁ®ãÁöÑÂèØËÉΩÊÄßÂá∫Áèæ‰∫Ü„ÄÇËàáÂÖ∂‰ªñ‰ΩøÁî®ÁèæÊúâÊú¨‰ΩìË´ñÊàñÁü•Ë≠òÂúñË≠ú‰ΩúÁÇ∫ LLM Ëº∏ÂÖ•ÁöÑÈ°û‰ººÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî® LLM Ëá™ÂãïÁîüÊàêË¢´Ë™çÁÇ∫ÊòØÈ†òÂüüÁü•Ë≠òÂ∫´ÁöÑ‰∏ÄÁµÑÁßëÂ≠∏Ë´ñÊñáÁöÑ CQ„ÄÇÊàëÂÄëÁ†îÁ©∂ÂÖ∂ÊÄßËÉΩÔºåÁâπÂà•ÊòØÊàëÂÄëÁ†îÁ©∂‰∏çÂêåÊï∏ÈáèÁöÑË´ñÊñáÂ∞ç RAG ÁöÑÂΩ±ÈüøÂíå LLM ÁöÑ‰∏çÂêåÊ∫´Â∫¶Ë®≠ÁΩÆ„ÄÇÊàëÂÄë‰ΩøÁî® GPT-4 Â∞çÂÖ©ÂÄãÈ†òÂüüÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÈÄ≤Ë°åÂØ¶È©óÔºå‰∏¶Â∞áÁµêÊûúËàáÁî±È†òÂüüÂ∞àÂÆ∂ÊßãÈÄ†ÁöÑÁúüÂØ¶ CQ ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂà©Áî®Ë©ï‰º∞ÊåáÊ®ôÔºàÁ≤æÁ¢∫Â∫¶Âíå‰∏ÄËá¥ÊÄßÔºâÂ∞çÁµêÊûúÈÄ≤Ë°åÁöÑÂØ¶Ë≠âË©ï‰º∞Ë°®ÊòéÔºåËàáÈõ∂Ê¨°ÊèêÁ§∫Áõ∏ÊØîÔºåÂ∞áÁõ∏ÈóúÈ†òÂüüÁü•Ë≠òÊ∑ªÂä†Âà∞ RAG ÂèØ‰ª•ÊèêÈ´ò LLM Âú®ÁÇ∫ÂÖ∑È´îÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÁîüÊàê CQ ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ

##### **ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**
2409.08543v1 by Zezheng Qin

Recommender Systems (RS) play a pivotal role in boosting user satisfaction by
providing personalized product suggestions in domains such as e-commerce and
entertainment. This study examines the integration of multimodal data text and
audio into large language models (LLMs) with the aim of enhancing
recommendation performance. Traditional text and audio recommenders encounter
limitations such as the cold-start problem, and recent advancements in LLMs,
while promising, are computationally expensive. To address these issues,
Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without
compromising performance. The ATFLRec framework is proposed to integrate audio
and text modalities into a multimodal recommendation system, utilizing various
LoRA configurations and modality fusion techniques. Results indicate that
ATFLRec outperforms baseline models, including traditional and graph neural
network-based approaches, achieving higher AUC scores. Furthermore, separate
fine-tuning of audio and text data with distinct LoRA modules yields optimal
performance, with different pooling methods and Mel filter bank numbers
significantly impacting performance. This research offers valuable insights
into optimizing multimodal recommender systems and advancing the integration of
diverse data modalities in LLMs.

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶‰∏≠ÊâÆÊºîËëóËàâË∂≥ËºïÈáçÁöÑËßíËâ≤ÔºåÂÆÉÂú®ÈõªÂ≠êÂïÜÂãôÂíåÂ®õÊ®ÇÁ≠âÈ†òÂüüÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÁî¢ÂìÅÂª∫Ë≠∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂ∞áÂ§öÊ®°ÊÖãË≥áÊñôÊñáÂ≠óÂíåÈü≥Ë®äÊï¥ÂêàÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ôºå‰ª•Â¢ûÂº∑Êé®Ëñ¶ÊïàËÉΩ„ÄÇÂÇ≥Áµ±ÁöÑÊñáÂ≠óÂíåÈü≥Ë®äÊé®Ëñ¶Âô®ÊúÉÈÅáÂà∞ÂÜ∑ÂïüÂãïÂïèÈ°åÁ≠âÈôêÂà∂ÔºåËÄå LLM ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÈõñÁÑ∂ÂæàÊúâÂâçÊôØÔºå‰ΩÜË®àÁÆóÊàêÊú¨ÂæàÈ´ò„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂºïÂÖ•‰∫Ü‰ΩéÁß©ÈÅ©Êáâ (LoRA)ÔºåÂÆÉÂú®‰∏çÂΩ±ÈüøÊïàËÉΩÁöÑÊÉÖÊ≥Å‰∏ãÊèêÂçá‰∫ÜÊïàÁéá„ÄÇATFLRec Ê°ÜÊû∂Ë¢´ÊèêÂá∫‰æÜÂ∞áÈü≥Ë®äÂíåÊñáÂ≠óÊ®°ÊÖãÊï¥ÂêàÂà∞Â§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÔºåÂà©Áî®ÂêÑÁ®Æ LoRA ÈÖçÁΩÆÂíåÊ®°ÊÖãËûçÂêàÊäÄË°ì„ÄÇÁµêÊûúË°®ÊòéÔºåATFLRec ÂÑ™ÊñºÂü∫Á∑öÊ®°ÂûãÔºåÂåÖÊã¨ÂÇ≥Áµ±ÂíåÂü∫ÊñºÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊñπÊ≥ïÔºåÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑ AUC ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®‰∏çÂêåÁöÑ LoRA Ê®°ÁµÑÂ∞çÈü≥Ë®äÂíåÊñáÂ≠óË≥áÊñôÈÄ≤Ë°åÂñÆÁç®ÂæÆË™øÊúÉÁî¢ÁîüÊúÄ‰Ω≥ÊïàËÉΩÔºå‰∏çÂêåÁöÑÊ±†ÂåñÊñπÊ≥ïÂíå Mel ÊøæÊ≥¢Âô®ÁµÑÊï∏ÊúÉÂ∞çÊïàËÉΩÁî¢ÁîüÈ°ØËëóÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£ÔºåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂ§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±Ôºå‰∏¶Êé®ÂãïÂ∞á‰∏çÂêåÁöÑË≥áÊñôÊ®°ÊÖãÊï¥ÂêàÂà∞ LLM ‰∏≠„ÄÇ

##### **What Makes a Maze Look Like a Maze?**
2409.08202v1 by Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu

A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.

ÊëòË¶ÅÔºö‰∫∫È°ûË¶ñË¶∫ÁêÜËß£ÁöÑÁç®ÁâπÈù¢ÂêëÂú®ÊñºÈùàÊ¥ªË©ÆÈáãÊäΩË±°Ê¶ÇÂøµÁöÑËÉΩÂäõÔºöÁç≤ÂèñËß£ÈáãÂÖ∂Ë±°ÂæµÊÑèÁæ©ÁöÑÊèêÂçáË¶èÂâáÔºåÂú®ÁÜüÊÇâÂíå‰∏çÁÜüÊÇâÁöÑËÉåÊôØ‰∏ãÂ•†ÂÆöÂÖ∂Âü∫Á§éÔºå‰∏¶Â∞çÂÖ∂ÈÄ≤Ë°åÈ†êÊ∏¨ÊàñÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÁèæÊàêÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÊìÖÈï∑Â∞çÂΩ±ÂÉèÈÄ≤Ë°åÂ≠óÈù¢Ë©ÆÈáãÔºà‰æãÂ¶ÇËæ®Ë≠òÊ®πÊûùÁ≠âÁâ©È´îÈ°ûÂà•ÔºâÔºå‰ΩÜÂÆÉÂÄëÂú®ÁêÜËß£Ê≠§È°ûË¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÊôÇ‰ªçÊúâÂõ∞Èõ£Ôºà‰æãÂ¶ÇÊ®πÊûùÁöÑÊéíÂàóÂ¶Ç‰ΩïÂΩ¢ÊàêËø∑ÂÆÆÁöÑÁâÜÂ£ÅÔºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊ∑±Â∫¶Ê®°ÂºèÂü∫Á§éÔºàDSGÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂà©Áî®Ë¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÁöÑÊòéÁ¢∫ÁµêÊßãÂåñË°®Á§∫‰æÜÈÄ≤Ë°åÂü∫Á§éÂíåÊé®ÁêÜ„ÄÇDSG ÁöÑÊ†∏ÂøÉÊòØÊ®°Âºè‚Äî‚ÄîÊäΩË±°Ê¶ÇÂøµÁöÑ‰æùË≥¥ÂúñÊèèËø∞ÔºåÂ∞áÂÖ∂ÂàÜËß£ÁÇ∫Êõ¥ÂéüÂßãÂ±§Á¥öÁöÑÁ¨¶Ëôü„ÄÇDSG ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÊèêÂèñÊ®°ÂºèÔºåÁÑ∂ÂæåÂ∞áÊ®°ÂºèÁöÑÂÖ∑È´îÁµÑÊàêÈÉ®ÂàÜÂàÜÂ±§Âü∫Á§éÂà∞ÂΩ±ÂÉè‰∏äÔºå‰∏¶‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã„ÄÇÂü∫Á§éÊ®°ÂºèÁî®ÊñºÊì¥ÂÖÖË¶ñË¶∫ÊäΩË±°ÁêÜËß£„ÄÇÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫Ü DSG ÂíåÊàëÂÄëÁöÑÊñ∞Ë¶ñË¶∫ÊäΩË±°Ë≥áÊñôÈõÜ‰∏äÁöÑ‰∏çÂêåÊé®ÁêÜÊñπÊ≥ïÔºåË©≤Ë≥áÊñôÈõÜÂåÖÂê´ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåÁöÑÊäΩË±°Ê¶ÇÂøµÂΩ±ÂÉèÔºå‰ª•ÂèäÁî±‰∫∫È°ûÊ®ôË®òÁöÑÂ∞çÊáâÂïèÈ°åËß£Á≠îÂ∞ç„ÄÇÊàëÂÄëË≠âÊòé DSG Â§ßÂπÖÊèêÂçá‰∫ÜË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÊäΩË±°Ë¶ñË¶∫Êé®ÁêÜÊïàËÉΩÔºå‰∏¶‰∏îÊúùËëóËàá‰∫∫È°û‰∏ÄËá¥ÁöÑË¶ñË¶∫ÊäΩË±°ÁêÜËß£ÈÇÅÈÄ≤‰∏ÄÊ≠•„ÄÇ

##### **Towards a graph-based foundation model for network traffic analysis**
2409.08111v1 by Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros

Foundation models have shown great promise in various fields of study. A
potential application of such models is in computer network traffic analysis,
where these models can grasp the complexities of network traffic dynamics and
adapt to any specific task or network environment with minimal fine-tuning.
Previous approaches have used tokenized hex-level packet data and the model
architecture of large language transformer models. We propose a new, efficient
graph-based alternative at the flow-level. Our approach represents network
traffic as a dynamic spatio-temporal graph, employing a self-supervised link
prediction pretraining task to capture the spatial and temporal dynamics in
this network graph framework. To evaluate the effectiveness of our approach, we
conduct a few-shot learning experiment for three distinct downstream network
tasks: intrusion detection, traffic classification, and botnet classification.
Models finetuned from our pretrained base achieve an average performance
increase of 6.87\% over training from scratch, demonstrating their ability to
effectively learn general network traffic dynamics during pretraining. This
success suggests the potential for a large-scale version to serve as an
operational foundational model.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤Âú®ÂêÑÂÄãÁ†îÁ©∂È†òÂüü‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÂâçÊôØ„ÄÇÊ≠§È°ûÊ®°ÂûãÁöÑÊΩõÂú®ÊáâÁî®‰πã‰∏ÄÂú®ÊñºÈõªËÖ¶Á∂≤Ë∑ØÊµÅÈáèÂàÜÊûêÔºåÂÖ∂‰∏≠ÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÊéåÊè°Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑË§áÈõúÊÄßÔºå‰∏¶‰ª•ÊúÄÂ∞èÁöÑÂæÆË™øÈÅ©Êáâ‰ªª‰ΩïÁâπÂÆö‰ªªÂãôÊàñÁ∂≤Ë∑ØÁí∞Â¢É„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰ΩøÁî®Ê®ôË®òÂåñÂçÅÂÖ≠ÈÄ≤‰ΩçÂ±§Á¥öÂ∞ÅÂåÖË≥áÊñôÂíåÂ§ßÂûãË™ûË®ÄËΩâÊèõÂô®Ê®°ÂûãÁöÑÊ®°ÂûãÊû∂Êßã„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑ„ÄÅÊúâÊïàÁöÑÊµÅÁ®ãÂ±§Á¥öÂúñÂΩ¢ÂåñÊõø‰ª£ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÁ∂≤Ë∑ØÊµÅÈáèË°®Á§∫ÁÇ∫ÂãïÊÖãÊôÇÁ©∫ÂúñÂΩ¢ÔºåÊé°Áî®Ëá™ÊàëÁõ£Áù£ÈÄ£ÁµêÈ†êÊ∏¨È†êË®ìÁ∑¥‰ªªÂãô‰æÜÊçïÊçâÊ≠§Á∂≤Ë∑ØÂúñÂΩ¢Êû∂Êßã‰∏≠ÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÂãïÊÖã„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂ∞ç‰∏âÂÄã‰∏çÂêåÁöÑ‰∏ãÊ∏∏Á∂≤Ë∑Ø‰ªªÂãôÔºàÂÖ•‰æµÂÅµÊ∏¨„ÄÅÊµÅÈáèÂàÜÈ°ûÂíåÊÆ≠Â±çÁ∂≤Ë∑ØÂàÜÈ°ûÔºâÈÄ≤Ë°åÂ∞ëÈáèÂ≠∏ÁøíÂØ¶È©ó„ÄÇÂæûÊàëÂÄëÁöÑÈ†êË®ìÁ∑¥Âü∫Á§éÂæÆË™øÁöÑÊ®°ÂûãÔºåÂÖ∂Âπ≥ÂùáÊïàËÉΩÊèêÂçá 6.87%ÔºåÈ´òÊñºÂæûÈ†≠Ë®ìÁ∑¥ÔºåÈÄôË≠âÊòé‰∫ÜÂÆÉÂÄëÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂ≠∏Áøí‰∏ÄËà¨Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑËÉΩÂäõ„ÄÇÈÄôÈ†ÖÊàêÂäüÈ°ØÁ§∫Âá∫Â§ßË¶èÊ®°ÁâàÊú¨ÊúâÊΩõÂäõ‰ΩúÁÇ∫ÈÅã‰ΩúÂü∫Á§éÊ®°Âûã„ÄÇ

##### **Learning Rules from KGs Guided by Language Models**
2409.07869v1 by Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott

Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.

ÊëòË¶ÅÔºöË≥áË®äËêÉÂèñÁöÑÈÄ≤Â±ïÂ∑≤ËÉΩËá™ÂãïÂª∫ÊßãÂ§ßÂûãÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶Ç Yago„ÄÅWikidata Êàñ Google KGÔºâÔºåÈÄô‰∫õÁü•Ë≠òÂúñË≠úÂª£Ê≥õÁî®ÊñºË®±Â§öÊáâÁî®Á®ãÂºèÔºå‰æãÂ¶ÇË™ûÊÑèÊêúÂ∞ãÊàñË≥áÊñôÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÄô‰∫õÁü•Ë≠òÂúñË≠úÊòØÂçäËá™ÂãïÂª∫ÊßãÁöÑÔºåÂõ†Ê≠§ÈÄöÂ∏∏‰∏¶‰∏çÂÆåÊï¥„ÄÇË¶èÂâáÂ≠∏ÁøíÊñπÊ≥ïËëóÈáçÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñÈ†ªÁπÅÊ®°ÂºèÔºå‰∏¶Â∞áÂÆÉÂÄëËΩâÊèõÁÇ∫Ë¶èÂâáÔºåÂèØÊáâÁî®ÊñºÈ†êÊ∏¨ÊΩõÂú®ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÊ≠•È©üÊòØË¶èÂâáÊéíÂ∫è„ÄÇË¶èÂâáÊéíÂ∫èÂú®È´òÂ∫¶‰∏çÂÆåÊï¥ÊàñÊúâÂÅèÂ∑ÆÁöÑÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶ÇÔºå‰∏ªË¶ÅÂÑ≤Â≠òÂêç‰∫∫‰∫ãÂØ¶ÁöÑÁü•Ë≠òÂúñË≠úÔºâ‰∏≠ÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Âú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÊúâÂÅèÂ∑ÆÁöÑË¶èÂâáÂèØËÉΩÊúÄÁ¨¶ÂêàË≥áÊñôÔºå‰∏¶Ê†πÊìöÊ®ôÊ∫ñÁµ±Ë®àÈáèÂ∫¶Ôºà‰æãÂ¶ÇË¶èÂâá‰ø°ÂøÉÔºâÊéíÂú®ÊúÄÂâçÈù¢„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÊèêÂá∫‰∏çÂè™‰æùË≥¥ÂéüÂßãÁü•Ë≠òÂúñË≠úÔºåÈÇÑË¶Å‰æùË≥¥Áü•Ë≠òÂúñË≠úÂµåÂÖ•Ê®°ÂûãÈ†êÊ∏¨ÁöÑ‰∫ãÂØ¶‰æÜÂ∞çË¶èÂâáÈÄ≤Ë°åÊéíÂ∫è„ÄÇÂêåÊôÇÔºåÈö®ËëóË™ûË®ÄÊ®°Âûã (LM) ÁöÑËààËµ∑Ôºå‰∏Ä‰∫õÁ†îÁ©∂ËÅ≤Á®± LM ÂèØÁî®‰ΩúÁü•Ë≠òÂúñË≠úÂÆåÊàêÁöÑÊõø‰ª£ÊñπÊ≥ï„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÈ©óË≠âÂà©Áî® LM Âú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÊúâÂä©ÊñºÊèêÂçáË¶èÂâáÂ≠∏ÁøíÁ≥ªÁµ±ÁöÑÂìÅË≥™„ÄÇ

##### **Multi-object event graph representation learning for Video Question Answering**
2409.07747v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.

ÊëòË¶ÅÔºöÂΩ±ÁâáÂïèÁ≠î (VideoQA) ÊòØ‰∏ÄÈ†Ö‰ªªÂãôÔºåÁî®ÊñºÈ†êÊ∏¨ÈáùÂ∞çÁµ¶ÂÆöÂΩ±ÁâáÊèêÂá∫ÁöÑÂïèÈ°åÁöÑÊ≠£Á¢∫Á≠îÊ°à„ÄÇÁ≥ªÁµ±ÂøÖÈ†à‰∫ÜËß£ÂæûÂΩ±Áâá‰∏≠ÊèêÂèñÁöÑÁâ©‰ª∂‰πãÈñìÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÈóú‰øÇÔºåÊâçËÉΩÂü∑Ë°åÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂ÈõÜ‰∏≠Êñº‰ΩøÁî®Âü∫ÊñºTransformerÁöÑÊ®°Âûã‰æÜÂª∫Ê®°ÂÄãÂà•Áâ©‰ª∂ÁöÑÂãï‰ΩúÔºå‰ΩÜÂú®ÊçïÊçâÊ∂âÂèäÂ§öÂÄãÁâ©‰ª∂ÁöÑË§áÈõúÂ†¥ÊôØÔºà‰æãÂ¶Ç„Äå‰∏ÄÂÄãÁî∑Â≠©Ê≠£Âú®Â∞áÁêÉÊäïÈÄ≤Á±ÉÊ°Ü„ÄçÔºâÊôÇÔºåÂÆÉÂÄëÊúÉÂá∫ÁèæÂïèÈ°å„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çÊØîÂºèË™ûË®Ä‰∫ã‰ª∂ÂúñË°®Ë°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºåÁ®±ÁÇ∫ CLanGÔºå‰ª•Ëß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊçïÊçâËàáÂ§öÂÄãÁâ©‰ª∂Áõ∏ÈóúÁöÑ‰∫ã‰ª∂Ë°®Á§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî®Â§öÂ±§ GNN ÈõÜÁæ§Ê®°ÁµÑÈÄ≤Ë°åÂ∞çÊäóÂºèÂúñË°®Ë°®Á§∫Â≠∏ÁøíÔºå‰ΩøÂïèÈ°åÊñáÂ≠óÂèäÂÖ∂Áõ∏ÈóúÁöÑÂ§öÁâ©‰ª∂‰∫ã‰ª∂ÂúñË°®‰πãÈñìËÉΩÂ§†ÈÄ≤Ë°åÂ∞çÊØîÂºèÂ≠∏Áøí„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂº∑Â§ßÁöÑÂü∫Ê∫ñÔºåÂú®ÂÖ©ÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ VideoQA Ë≥áÊñôÈõÜ NExT-QA Âíå TGIF-QA-R ‰∏äÈÅîÂà∞‰∫ÜÈ´òÈÅî 2.2% ÁöÑÊõ¥È´òÊ∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÂú®ËôïÁêÜÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÂïèÈ°åÊñπÈù¢ÊØîÂü∫Ê∫ñÈ´òÂá∫ 2.8%ÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÂú®Êé®ÁêÜÂ§öÂÄãÂü∫ÊñºÁâ©‰ª∂ÁöÑ‰∫ã‰ª∂ÊñπÈù¢ÁöÑÂÑ™Âã¢„ÄÇ

##### **Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**
2409.07368v2 by Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

This paper introduces SGCode, a flexible prompt-optimizing system to generate
secure code with large language models (LLMs). SGCode integrates recent
prompt-optimization approaches with LLMs in a unified system accessible through
front-end and back-end APIs, enabling users to 1) generate secure code, which
is free of vulnerabilities, 2) review and share security analysis, and 3)
easily switch from one prompt optimization approach to another, while providing
insights on model and system performance. We populated SGCode on an AWS server
with PromSec, an approach that optimizes prompts by combining an LLM and
security tools with a lightweight generative adversarial graph neural network
to detect and fix security vulnerabilities in the generated code. Extensive
experiments show that SGCode is practical as a public tool to gain insights
into the trade-offs between model utility, secure code generation, and system
cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is
available at: http://3.131.141.63:8501/.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñá‰ªãÁ¥π‰∫Ü SGCodeÔºå‰∏ÄÂÄãÈùàÊ¥ªÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÁ≥ªÁµ±ÔºåÁî®ÊñºÁîüÊàêÂÖ∑ÂÇôÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÆâÂÖ®Á®ãÂºèÁ¢º„ÄÇSGCode Â∞áÊúÄËøëÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïËàá LLM Êï¥ÂêàÂú®‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÁ≥ªÁµ±‰∏≠ÔºåÂèØÈÄèÈÅéÂâçÁ´ØÂíåÂæåÁ´Ø API Â≠òÂèñÔºå‰ΩøÁî®Êà∂ËÉΩÂ§† 1) ÁîüÊàêÂÆâÂÖ®ÁöÑÁ®ãÂºèÁ¢ºÔºåÊ≤íÊúâÊºèÊ¥ûÔºå2) Ê™¢Èñ±ÂíåÂàÜ‰∫´ÂÆâÂÖ®ÊÄßÂàÜÊûêÔºå‰ª•Âèä 3) ËºïÈ¨ÜÂæû‰∏ÄÁ®ÆÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÂàáÊèõÂà∞Âè¶‰∏ÄÁ®ÆÔºåÂêåÊôÇÊèê‰æõÊ®°ÂûãÂíåÁ≥ªÁµ±ÊïàËÉΩÁöÑË¶ãËß£„ÄÇÊàëÂÄëÂú® AWS ‰º∫ÊúçÂô®‰∏ä‰ΩøÁî® PromSec Â°´ÂÖÖ SGCodeÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÁµêÂêà LLM ÂíåÂÆâÂÖ®ÊÄßÂ∑•ÂÖ∑ËàáËºïÈáèÁ¥öÁîüÊàêÂºèÂ∞çÊäóÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰æÜÊúÄ‰Ω≥ÂåñÊèêÁ§∫ÁöÑÊñπÊ≥ïÔºå‰ª•ÂÅµÊ∏¨‰∏¶‰øÆÂæ©ÁîüÊàêÁ®ãÂºèÁ¢º‰∏≠ÁöÑÂÆâÂÖ®ÊÄßÊºèÊ¥û„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåSGCode ‰ΩúÁÇ∫‰∏ÄÂÄãÂÖ¨Áî®Â∑•ÂÖ∑ÊòØÂØ¶Áî®ÁöÑÔºåÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÂØ¶Áî®ÊÄß„ÄÅÂÆâÂÖ®Á®ãÂºèÁ¢ºÁîüÊàêÂíåÁ≥ªÁµ±ÊàêÊú¨‰πãÈñìÁöÑÂèñÊç®„ÄÇËàáÊèêÁ§∫ LLM Áõ∏ÊØîÔºåSGCode ÂÉÖÊúâÂæÆÂ∞èÁöÑÊàêÊú¨„ÄÇSGCode ÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttp://3.131.141.63:8501/„ÄÇ

##### **Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**
2409.07088v1 by Daehee Kim, Deokhyung Kang, Sangwon Ryu, Gary Geunbae Lee

Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÂà∞ÊñáÂ≠ó (G2T) ÁîüÊàêÊ∂âÂèäÂ∞áÁµêÊßãÂåñÁü•Ë≠òÂúñË≠úË°®ÈÅîÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊîπÂñÑ‰∫Ü G2T ÁöÑÊïàËÉΩÔºå‰ΩÜÂÖ∂ÊúâÊïàÊÄßÂèñÊ±∫ÊñºÂÖ∑ÊúâÁ≤æÁ¢∫ÂúñÂΩ¢ÊñáÂ≠óÂ∞çÈΩäÁöÑË≥áÊñôÈõÜ„ÄÇÁÑ∂ËÄåÔºåÈ´òÂìÅË≥™„ÄÅ‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÊÄßÈôêÂà∂‰∫Ü‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêÁ†îÁ©∂ÁöÑÈÄ≤Â±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ∂≠Âü∫ÁôæÁßëÊú¨‰ΩìÂÖçË≤ªÂúñÂΩ¢ÊñáÂ≠óË≥áÊñôÈõÜ (WikiOFGraph)ÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå Data-QuestEval ÁöÑÊñ∞ÊñπÊ≥ïÁîüÊàêÁöÑÊñ∞Â§ßÂûã G2T Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñ∞Ë≥áÊñôÈõÜÂåÖÂê´ 585 Ëê¨ÂÄã‰∏ÄËà¨È†òÂüüÁöÑÂúñÂΩ¢ÊñáÂ≠óÂ∞çÔºåÊèê‰æõÈ´òÂúñÂΩ¢ÊñáÂ≠ó‰∏ÄËá¥ÊÄßÔºåËÄå‰∏ç‰æùË≥¥ÊñºÂ§ñÈÉ®Êú¨‰Ωì„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂú® WikiOFGraph ‰∏äÂæÆË™øÁöÑ PLM Âú®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏äÂÑ™ÊñºÂú®ÂÖ∂‰ªñË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ PLM„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñπÊ≥ïË¢´Ë≠âÊòéÊòØ‰∏ÄÂÄãÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÁîüÊàêÈ´òÂìÅË≥™ÁöÑ G2T Ë≥áÊñôÔºåÈ°ØËëóÊé®Âãï‰∫Ü G2T ÁîüÊàêÈ†òÂüüÁöÑÁôºÂ±ï„ÄÇ

##### **Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**
2409.07064v1 by Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen

Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.

ÊëòË¶ÅÔºöËá™ÂãïÂ∞çË©±Ë©ïÈáè‰∏≠ÁöÑËá™ÂãïÂåñÂè£Ë™™Ë©ïÈáèÔºàASACÔºâÊó®Âú®Ë©ï‰º∞ L2ÔºàÁ¨¨‰∫åË™ûË®ÄÔºâË©±ËÄÖÂú®Ëàá‰∏Ä‰ΩçÊàñÂ§ö‰ΩçÊáâË©¶ËÄÖ‰∫íÂãïÁöÑÁí∞Â¢É‰∏≠ÔºåÊï¥È´îÁöÑÂè£Ë™™ËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑ ASAC ÊñπÊ≥ïÂú®ÂÖ∂ÂêÑËá™ÁöÑË≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫ÊúâÂâçÈÄîÁöÑË°®ÁèæÔºå‰ΩÜ‰ªçÁº∫‰πèÂ∞àÊ≥®ÊñºÂ∞áÂ∞çË©±‰∏≠ÈÇèËºØÊµÅÁ®ãÁöÑÈÄ£Ë≤´ÊÄßÁ¥çÂÖ•Ë©ïÂàÜÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÈóúÈçµÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈöéÂ±§ÂºèÂúñÂΩ¢Ê®°ÂûãÔºåÂÆÉÈÅ©Áï∂Âú∞ÁµêÂêà‰∫ÜÂª£Ê≥õÁöÑÂõûÊáâÈñì‰∫íÂãïÔºà‰æãÂ¶ÇÔºöË™ûÁØáÈóú‰øÇÔºâÂíåÁ¥∞ÂæÆÁöÑË™ûÁæ©Ë≥áË®äÔºà‰æãÂ¶ÇÔºöË™ûÁæ©Â≠óË©ûÂíåË™™Ë©±ËÄÖÊÑèÂúñÔºâÔºåÈö®ÂæåËàáËÑàÁµ°Ë≥áË®äËûçÂêàÔºå‰ª•ÈÄ≤Ë°åÊúÄÁµÇÈ†êÊ∏¨„ÄÇÂú® NICT-JLE Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàá‰∏Ä‰∫õÂº∑Â§ßÁöÑÂü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÂª∫Ê®°ÊñπÊ≥ïÂèØ‰ª•È°ØËëóÊèêÂçáÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®ÂêÑÁ®ÆË©ïÈáèÊåáÊ®ôÊñπÈù¢„ÄÇÈÄô‰πüÈó°Êòé‰∫ÜÂú® ASAC ‰∏≠Êé¢Ë®éÂè£Ë™ûÂõûÊáâÁöÑÈÄ£Ë≤´ÊÄßÁõ∏ÈóúÈù¢ÂêëÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **FreeRide: Harvesting Bubbles in Pipeline Parallelism**
2409.06941v1 by Jiashu Zhang, Zihan Pan, Molly, Xu, Khuzaima Daudjee, Sihang Liu

The occurrence of bubbles in pipeline parallelism is an inherent limitation
that can account for more than 40% of the large language model (LLM) training
time and is one of the main reasons for the underutilization of GPU resources
in LLM training. Harvesting these bubbles for GPU side tasks can increase
resource utilization and reduce training costs but comes with challenges.
First, because bubbles are discontinuous with various shapes, programming side
tasks becomes difficult while requiring excessive engineering effort. Second, a
side task can compete with pipeline training for GPU resources and incur
significant overhead. To address these challenges, we propose FreeRide, a
system designed to harvest bubbles in pipeline parallelism for side tasks.
FreeRide provides programmers with interfaces to implement side tasks easily,
manages bubbles and side tasks during pipeline training, and controls access to
GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide
achieves 7.8% average cost savings with a negligible overhead of about 1% in
training LLMs while serving model training, graph analytics, and image
processing side tasks.

ÊëòË¶ÅÔºöÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁôºÁîüÊ∞£Ê≥°ÊòØ‰∏ÄÂÄãÂõ∫ÊúâÈôêÂà∂ÔºåÂèØËÉΩ‰ΩîÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë®ìÁ∑¥ÊôÇÈñìÁöÑ 40% ‰ª•‰∏äÔºå‰∏¶‰∏îÊòØ LLM Ë®ìÁ∑¥‰∏≠ GPU Ë≥áÊ∫êÂà©Áî®‰∏çË∂≥ÁöÑ‰∏ªË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇÊî∂ÈõÜÈÄô‰∫õÊ∞£Ê≥°‰ª•ÈÄ≤Ë°å GPU ÂÅ¥Èù¢‰ªªÂãôÂèØ‰ª•ÊèêÈ´òË≥áÊ∫êÂà©Áî®Áéá‰∏¶Èôç‰ΩéË®ìÁ∑¥ÊàêÊú¨Ôºå‰ΩÜÊúÉÂ∏∂‰æÜÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÁî±ÊñºÊ∞£Ê≥°ÊòØ‰∏çÈÄ£Á∫åÁöÑ‰∏îÂΩ¢ÁãÄÂêÑÁï∞ÔºåÂõ†Ê≠§Á∑®ÂØ´Á®ãÂºèÂÅ¥Èù¢‰ªªÂãôËÆäÂæóÂõ∞Èõ£ÔºåÂêåÊôÇÈúÄË¶ÅÈÅéÂ§öÁöÑÂ∑•Á®ãÂ∑•‰Ωú„ÄÇÂÖ∂Ê¨°ÔºåÂÅ¥Èù¢‰ªªÂãôÂèØËÉΩÊúÉËàáÁÆ°Á∑öË®ìÁ∑¥Á´∂Áà≠ GPU Ë≥áÊ∫êÔºå‰∏¶ÈÄ†ÊàêÈ°ØËëóÁöÑÈñãÈä∑„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü FreeRideÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®Êî∂ÈõÜÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁöÑÊ∞£Ê≥°‰ª•ÈÄ≤Ë°åÂÅ¥Èù¢‰ªªÂãôÁöÑÁ≥ªÁµ±„ÄÇFreeRide ÁÇ∫Á®ãÂºèË®≠Ë®àÂ∏´Êèê‰æõ‰∫ÜËºïÈ¨ÜÂØ¶‰ΩúÂÅ¥Èù¢‰ªªÂãôÁöÑ‰ªãÈù¢ÔºåÂú®ÁÆ°Á∑öË®ìÁ∑¥ÊúüÈñìÁÆ°ÁêÜÊ∞£Ê≥°ÂíåÂÅ¥Èù¢‰ªªÂãôÔºå‰∏¶ÊéßÂà∂ÂÅ¥Èù¢‰ªªÂãôÂ∞ç GPU Ë≥áÊ∫êÁöÑÂ≠òÂèñ‰ª•Ê∏õÂ∞ëÈñãÈä∑„ÄÇÊàëÂÄëË≠âÊòé FreeRide Âú®Ë®ìÁ∑¥ LLM ÊôÇÂèØÁØÄÁúÅ 7.8% ÁöÑÂπ≥ÂùáÊàêÊú¨ÔºåÂêåÊôÇÂú®Âü∑Ë°åÊ®°ÂûãË®ìÁ∑¥„ÄÅÂúñÂΩ¢ÂàÜÊûêÂíåÂΩ±ÂÉèËôïÁêÜÂÅ¥Èù¢‰ªªÂãôÊôÇÔºåÈñãÈä∑ÂèØÂøΩÁï•‰∏çË®àÔºåÁ¥ÑÁÇ∫ 1%„ÄÇ

##### **Generative Hierarchical Materials Search**
2409.06762v1 by Sherry Yang, Simon Batzner, Ruiqi Gao, Muratahan Aykol, Alexander L. Gaunt, Brendan McMorrow, Danilo J. Rezende, Dale Schuurmans, Igor Mordatch, Ekin D. Cubuk

Generative models trained at scale can now produce text, video, and more
recently, scientific data such as crystal structures. In applications of
generative approaches to materials science, and in particular to crystal
structures, the guidance from the domain expert in the form of high-level
instructions can be essential for an automated system to output candidate
crystals that are viable for downstream research. In this work, we formulate
end-to-end language-to-structure generation as a multi-objective optimization
problem, and propose Generative Hierarchical Materials Search (GenMS) for
controllable generation of crystal structures. GenMS consists of (1) a language
model that takes high-level natural language as input and generates
intermediate textual information about a crystal (e.g., chemical formulae), and
(2) a diffusion model that takes intermediate information as input and
generates low-level continuous value crystal structures. GenMS additionally
uses a graph neural network to predict properties (e.g., formation energy) from
the generated crystal structures. During inference, GenMS leverages all three
components to conduct a forward tree search over the space of possible
structures. Experiments show that GenMS outperforms other alternatives of
directly using language models to generate structures both in satisfying user
request and in generating low-energy structures. We confirm that GenMS is able
to generate common crystal structures such as double perovskites, or spinels,
solely from natural language input, and hence can form the foundation for more
complex structure generation in near future.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°Ë®ìÁ∑¥ÁöÑÁîüÊàêÊ®°ÂûãÁèæÂú®ÂèØ‰ª•Áî¢ÁîüÊñáÂ≠ó„ÄÅÂΩ±ÁâáÔºå‰ª•ÂèäÊúÄËøëÁöÑÁßëÂ≠∏Ë≥áÊñôÔºå‰æãÂ¶ÇÊô∂È´îÁµêÊßã„ÄÇÂú®ÁîüÊàêÊñπÊ≥ïÊáâÁî®ÊñºÊùêÊñôÁßëÂ≠∏ÔºåÂ∞§ÂÖ∂ÊòØÊô∂È´îÁµêÊßãÊôÇÔºåÈ†òÂüüÂ∞àÂÆ∂ÁöÑÊåáÂ∞éÔºå‰ª•È´òÈöéÊåá‰ª§ÁöÑÂΩ¢ÂºèÔºåÂ∞çÊñºËá™ÂãïÂåñÁ≥ªÁµ±Ëº∏Âá∫ÂèØË°åÊñº‰∏ãÊ∏∏Á†îÁ©∂ÁöÑÂÄôÈÅ∏Êô∂È´îËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞áÁ´ØÂ∞çÁ´ØË™ûË®ÄÂà∞ÁµêÊßãÁîüÊàêÂà∂ÂÆöÁÇ∫Â§öÁõÆÊ®ôÊúÄ‰Ω≥ÂåñÂïèÈ°åÔºå‰∏¶ÊèêÂá∫ÁîüÊàêÂàÜÂ±§ÊùêÊñôÊêúÂ∞ã (GenMS) ‰ª•ÊéßÂà∂Êô∂È´îÁµêÊßãÁöÑÁîüÊàê„ÄÇGenMS ÂåÖÂê´ (1) ‰∏ÄÂÄãË™ûË®ÄÊ®°ÂûãÔºåÂÆÉÂ∞áÈ´òÈöéËá™ÁÑ∂Ë™ûË®Ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàêÊúâÈóúÊô∂È´îÁöÑ‰∏≠ÈñìÊñáÂ≠óË≥áË®äÔºà‰æãÂ¶ÇÂåñÂ≠∏ÂÖ¨ÂºèÔºâÔºå‰ª•Âèä (2) ‰∏ÄÂÄãÊì¥Êï£Ê®°ÂûãÔºåÂÆÉÂ∞á‰∏≠ÈñìË≥áË®ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàê‰ΩéÈöéÈÄ£Á∫åÂÄºÊô∂È´îÁµêÊßã„ÄÇGenMS Ê≠§Â§ñ‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂæûÁîüÊàêÁöÑÊô∂È´îÁµêÊßãÈ†êÊ∏¨Â±¨ÊÄßÔºà‰æãÂ¶ÇÂΩ¢ÊàêËÉΩÔºâ„ÄÇÂú®Êé®ÁêÜÊúüÈñìÔºåGenMS Âà©Áî®ÊâÄÊúâ‰∏âÂÄãÁµÑ‰ª∂Â∞çÂèØËÉΩÁöÑÁµêÊßãÁ©∫ÈñìÈÄ≤Ë°åÂâçÂêëÊ®πÁãÄÊêúÂ∞ã„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåGenMS ÂÑ™ÊñºÁõ¥Êé•‰ΩøÁî®Ë™ûË®ÄÊ®°Âûã‰æÜÁîüÊàêÁµêÊßãÁöÑÂÖ∂‰ªñÊõø‰ª£ÊñπÊ°àÔºåÁÑ°Ë´ñÊòØÂú®ÊªøË∂≥‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊàñÁîüÊàê‰ΩéËÉΩÁµêÊßãÊñπÈù¢„ÄÇÊàëÂÄëÁ¢∫Ë™ç GenMS ËÉΩÂ§†ÂÉÖÂæûËá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•ÁîüÊàêÂ∏∏Ë¶ãÁöÑÊô∂È´îÁµêÊßãÔºå‰æãÂ¶ÇÈõôÈà£Èà¶Á§¶ÊàñÂ∞ñÊô∂Áü≥ÔºåÂõ†Ê≠§ÂèØ‰ª•Âú®‰∏ç‰πÖÁöÑÂ∞á‰æÜÂΩ¢ÊàêÊõ¥Ë§áÈõúÁµêÊßãÁîüÊàêÁöÑÂü∫Á§é„ÄÇ</paragraph>

##### **Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**
2409.06433v1 by Gollam Rabby, S√∂ren Auer, Jennifer D'Souza, Allard Oelen

The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.

ÊëòË¶ÅÔºö<paragraph>ÊØèÂπ¥Ë∂ÖÈÅé 250 Ëê¨ÁØáÁöÑÂ≠∏Ë°ìÊñáÁ´†ÁôºË°®Êï∏ÈáèÊåÅÁ∫åÂ¢ûÂä†ÔºåÂ∞çÁ†îÁ©∂‰∫∫Âì°ËøΩËπ§ÁßëÂ≠∏ÈÄ≤Â±ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂ∞áÂ≠∏Ë°ìÊñáÁ´†ÁöÑË≤¢ÁçªÊï¥ÂêàÂà∞Êñ∞ÂûãÊÖãÁöÑË™çÁü•Áü•Ë≠òÂúñË≠ú (CKG) ‰∏≠ÔºåÂ∞áÊàêÁÇ∫Â≠òÂèñÂíåÁµÑÁπîÂ≠∏Ë°ìÁü•Ë≠òÁöÑÈóúÈçµË¶ÅÁ¥†ÔºåË∂ÖË∂äÊ®ôÈ°åÂíåÊëòË¶ÅÊèê‰æõÁöÑË¶ãËß£„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÊúâÊïàÂÇ≥ÈÅîÁµêÊßãÂåñÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂàÜÈ°ûÂ≠∏Ë°ìÊñáÁ´†Ôºå‰∏¶‰ª•ÁµêÊßãÂåñ‰∏îÂèØÊØîËºÉÁöÑÂΩ¢ÂºèÊèèËø∞ÂÖ∂Ë≤¢Áçª„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂Âú®ÁâπÂÆöÁ†îÁ©∂È†òÂüü‰∏≠Êé¢Á¥¢Ë™ûË®ÄÊ®°ÂûãÔºå‰ΩÜ LLM ÊçïÊçâÂà∞ÁöÑÂª£Ê≥õÈ†òÂüüÁÑ°ÈóúÁü•Ë≠òÔºåÁÇ∫Áî¢ÁîüÁµêÊßãÂåñÁöÑË≤¢ÁçªÊèèËø∞Êèê‰æõ‰∫ÜÂØ¶Ë≥™Ê©üÊúÉÔºå‰æãÂ¶Ç CKG„ÄÇÊ≠§Â§ñÔºåLLM ÈÄèÈÅéÊèêÁ§∫Â∑•Á®ãÊàñÂæÆË™øÊèê‰æõÂèØËá™Ë®ÇË∑ØÂæëÔºåÂæûËÄå‰øÉÈÄ≤Âà©Áî®‰ª•ÊïàÁéá„ÄÅÊàêÊú¨ÊïàÁõäÂíåÁí∞Â¢ÉËÄÉÈáèËÅûÂêçÁöÑËºÉÂ∞èÂûã LLM„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Âà©Áî® LLM Áü•Ë≠òÔºå‰∏¶ÈÄèÈÅé CKG ‰æÜÊ∫êÁöÑÈ†òÂüüÂ∞àÂÆ∂È©óË≠âÂ≠∏Ë°ìË≥áÊñô‰æÜË£úÂÖÖ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•ËûçÂêàÈ°ØËëóÊèêÂçá LLM ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®Â≠∏Ë°ìÊñáÁ´†ÂàÜÈ°ûÂíåË¨ÇË©ûÊé®Ëñ¶Á≠â‰ªªÂãô‰∏≠„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂåÖÊã¨‰ª• CKG Áü•Ë≠òÂæÆË™ø LLMÔºå‰∏¶ÈÄèÈÅéÊñ∞ÁöÑÊèêÁ§∫ÊäÄË°ìÊ≥®ÂÖ• CKG ÁöÑÁü•Ë≠òÔºåÈ°ØËëóÊèêÂçáÂ≠∏Ë°ìÁü•Ë≠òËêÉÂèñÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÂÅöÊ≥ïÊï¥ÂêàÂà∞ÈñãÊîæÁ†îÁ©∂Áü•Ë≠òÂúñË≠ú (ORKG) ‰∏≠ÔºåÂæûËÄåËÉΩÁ≤æÊ∫ñÂ≠òÂèñÂ∑≤ÁµÑÁπîÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÈÄôÂ∞çÊîøÁ≠ñÂà∂ÂÆöËÄÖ„ÄÅÁî¢Ê•≠ÂæûÊ•≠‰∫∫Âì°Âíå‰∏ÄËà¨Â§ßÁúæ‰πãÈñìÁöÑÈ†òÂüüÁÑ°ÈóúÂ≠∏Ë°ìÁü•Ë≠ò‰∫§ÊµÅÂíåÂÇ≥Êí≠Ëá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

##### **Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**
2409.06091v1 by Dongyue Li, Aneesh Sharma, Hongyang R. Zhang

Multitask learning is a widely used paradigm for training models on diverse
tasks, with applications ranging from graph neural networks to language model
fine-tuning. Since tasks may interfere with each other, a key notion for
modeling their relationships is task affinity. This includes pairwise task
affinity, computed among pairs of tasks, and higher-order affinity, computed
among subsets of tasks. Naively computing either of them requires repeatedly
training on data from various task combinations, which is computationally
intensive. We present a new algorithm Grad-TAG that can estimate task
affinities without this repeated training.
  The key idea of Grad-TAG is to train a "base" model for all tasks and then
use a linearization technique to estimate the loss of the model for a specific
task combination. The linearization works by computing a gradient-based
approximation of the loss, using low-dimensional projections of gradients as
features in a logistic regression to predict labels for the task combination.
We show that the linearized model can provably approximate the loss when the
gradient-based approximation is accurate, and also empirically verify that on
several large models. Then, given the estimated task affinity, we design a
semi-definite program for clustering similar tasks by maximizing the average
density of clusters.
  We evaluate Grad-TAG's performance across seven datasets, including
multi-label classification on graphs, and instruction fine-tuning of language
models. Our task affinity estimates are within 2.7% distance to the true
affinities while needing only 3% of FLOPs in full training. On our largest
graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates
within 5% distance to the true affinities, using only 112 GPU hours. Our
results show that Grad-TAG achieves excellent performance and runtime tradeoffs
compared to existing approaches.

ÊëòË¶ÅÔºöÂ§ö‰ªªÂãôÂ≠∏ÁøíÊòØ‰∏ÄÁ®ÆÂª£Ê≥õ‰ΩøÁî®ÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂú®‰∏çÂêåÁöÑ‰ªªÂãô‰∏äË®ìÁ∑¥Ê®°ÂûãÔºåÂÖ∂ÊáâÁî®ÁØÑÂúçÂæûÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂà∞Ë™ûË®ÄÊ®°ÂûãÂæÆË™ø„ÄÇÁî±Êñº‰ªªÂãôÂèØËÉΩÊúÉÁõ∏‰∫íÂπ≤ÊìæÔºåÂõ†Ê≠§Âª∫Ê®°ÂÆÉÂÄëÈóú‰øÇÁöÑ‰∏ÄÂÄãÈóúÈçµÊ¶ÇÂøµÊòØ‰ªªÂãôË¶™ÂíåÊÄß„ÄÇÈÄôÂåÖÊã¨ÊàêÂ∞ç‰ªªÂãôË¶™ÂíåÊÄßÔºåÂú®ÊàêÂ∞ç‰ªªÂãô‰πãÈñìË®àÁÆóÔºå‰ª•ÂèäÈ´òÈöéË¶™ÂíåÊÄßÔºåÂú®‰ªªÂãôÂ≠êÈõÜ‰πãÈñìË®àÁÆó„ÄÇÂ§©ÁúüÂú∞Ë®àÁÆóÂÖ∂‰∏≠‰ªª‰Ωï‰∏ÄÂÄãÈÉΩÈúÄË¶ÅÈáçË§áË®ìÁ∑¥‰æÜËá™ÂêÑÁ®Æ‰ªªÂãôÁµÑÂêàÁöÑË≥áÊñôÔºåÈÄôÂú®Ë®àÁÆó‰∏äÂæàÂØÜÈõÜ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊºîÁÆóÊ≥ï Grad-TAGÔºåÂÆÉÂèØ‰ª•Âú®Ê≤íÊúâÈáçË§áË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ã‰º∞Ë®à‰ªªÂãôË¶™ÂíåÊÄß„ÄÇ
Grad-TAG ÁöÑÈóúÈçµÊÄùÊÉ≥ÊòØÁÇ∫ÊâÄÊúâ‰ªªÂãôË®ìÁ∑¥‰∏ÄÂÄã„ÄåÂü∫Á§é„ÄçÊ®°ÂûãÔºåÁÑ∂Âæå‰ΩøÁî®Á∑öÊÄßÂåñÊäÄË°ì‰æÜ‰º∞Ë®àÊ®°ÂûãÂ∞çÁâπÂÆö‰ªªÂãôÁµÑÂêàÁöÑÊêçÂ§±„ÄÇÁ∑öÊÄßÂåñÈÄöÈÅéË®àÁÆóÊêçÂ§±ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄº‰æÜÂ∑•‰ΩúÔºå‰ΩøÁî®Ê¢ØÂ∫¶ÁöÑ‰ΩéÁ∂≠ÊäïÂΩ±‰ΩúÁÇ∫ÁâπÂæµÔºåÂú®ÈÇèËºØËø¥Ê≠∏‰∏≠È†êÊ∏¨‰ªªÂãôÁµÑÂêàÁöÑÊ®ôÁ±§„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÁï∂Âü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄºÊ∫ñÁ¢∫ÊôÇÔºåÁ∑öÊÄßÂåñÊ®°ÂûãÂèØ‰ª•Ë≠âÊòéÂú∞Ëøë‰ººÊêçÂ§±Ôºå‰∏¶‰∏îÂú®ÂπæÂÄãÂ§ßÂûãÊ®°Âûã‰∏äÁ∂ìÈ©óÈ©óË≠â‰∫ÜÈÄô‰∏ÄÈªû„ÄÇÁÑ∂ÂæåÔºåÁµ¶ÂÆö‰º∞Ë®àÁöÑ‰ªªÂãôË¶™ÂíåÊÄßÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂçäÂÆöÁ®ãÂºèÔºåÈÄöÈÅéÊúÄÂ§ßÂåñÂè¢ÈõÜÁöÑÂπ≥ÂùáÂØÜÂ∫¶‰æÜÂ∞çÈ°û‰ººÁöÑ‰ªªÂãôÈÄ≤Ë°åÂè¢ÈõÜ„ÄÇ
ÊàëÂÄëË©ï‰º∞‰∫Ü Grad-TAG Âú®‰∏ÉÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºåÂåÖÊã¨ÂúñÂΩ¢‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÔºå‰ª•ÂèäË™ûË®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÂæÆË™ø„ÄÇÊàëÂÄëÁöÑ‰ªªÂãôË¶™ÂíåÊÄß‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 2.7% ‰ª•ÂÖßÔºåÂêåÊôÇÂè™ÈúÄË¶Å 3% ÁöÑ FLOP ÈÄ≤Ë°åÂÆåÊï¥Ë®ìÁ∑¥„ÄÇÂú®ÊàëÂÄëÊúÄÂ§ßÁöÑÂúñÂΩ¢ÔºàÊúâ 2100 Ëê¨Ê¢ùÈÇäÂíå 500 ÂÄãÊ®ôÁ±§‰ªªÂãôÔºâ‰∏äÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ïÊèê‰æõÁöÑ‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 5% ‰ª•ÂÖßÔºåÂè™‰ΩøÁî® 112 ÂÄã GPU Â∞èÊôÇ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGrad-TAG Âú®ÊïàËÉΩÂíåÂü∑Ë°åÊôÇÈñìÊ¨äË°°ÊñπÈù¢ÂèñÂæó‰∫ÜÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇ

##### **OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**
2409.07497v1 by Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.

ÊëòË¶ÅÔºöÁü•Ë≠òË°®ÂæµËá™‰∫∫Â∑•Êô∫ÊÖßË™ïÁîü‰ª•‰æÜ‰∏ÄÁõ¥ÊòØÂÖ∂Ê†∏ÂøÉÁõÆÊ®ô„ÄÇ
Á¨¶ËôüÁü•Ë≠òÂúñË≠ú (KG) ÂíåÁ•ûÁ∂ìË™ûË®ÄÂ§ßÊ®°Âûã (LLM) ÈÉΩÂèØ‰ª•Ë°®ÂæµÁü•Ë≠ò„ÄÇKG Êèê‰æõÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÊòéÁ¢∫ÁöÑÁü•Ë≠òË°®ÂæµÔºå‰ΩÜÈù¢Ëá®ÂèØÊì¥ÂÖÖÊÄßÁöÑÂïèÈ°åÔºõËÄå LLM Êèê‰æõÂª£Ê≥õÁöÑÁü•Ë≠òÊ∂µËìãÁØÑÂúçÔºå‰ΩÜÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑË®ìÁ∑¥ÊàêÊú¨Ôºå‰∏¶‰∏îÂú®Á≤æÁ¢∫‰∏îÂèØÈù†ÁöÑÁü•Ë≠òÊìç‰ΩúÊñπÈù¢ÈÅáÂà∞Âõ∞Èõ£„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü OneEditÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°åÂçî‰ΩúÁü•Ë≠òÁ∑®ËºØÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÂéüÂûãÁ≥ªÁµ±ÔºåÂÆÉ‰øÉÈÄ≤‰∫Ü‰ΩøÁî® KG Âíå LLM ÈÄ≤Ë°åÊòìÊñº‰ΩøÁî®ÁöÑÁü•Ë≠òÁÆ°ÁêÜ„ÄÇOneEdit ÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö1) Ëß£Ë≠ØÂô®Áî®Êñº‰ΩøÁî®ËÄÖÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°å‰∫íÂãïÔºõ2) ÊéßÂà∂Âô®ÁÆ°ÁêÜ‰æÜËá™‰∏çÂêå‰ΩøÁî®ËÄÖÁöÑÁ∑®ËºØË´ãÊ±ÇÔºåÂà©Áî® KG ÂíåÂõûÊªæ‰æÜËôïÁêÜÁü•Ë≠òË°ùÁ™Å‰∏¶Èò≤Ê≠¢ÊúâÊØíÁöÑÁü•Ë≠òÊîªÊìäÔºõ3) Á∑®ËºØÂô®Âà©Áî®‰æÜËá™ÊéßÂà∂Âô®ÁöÑÁü•Ë≠ò‰æÜÁ∑®ËºØ KG Âíå LLM„ÄÇÊàëÂÄëÂ∞çÂÖ©ÂÄãÂÖ∑Êúâ KG ÁöÑÊñ∞Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåË≠âÊòé OneEdit ÂèØ‰ª•ÂØ¶ÁèæÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ

##### **SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**
2409.05556v1 by Alireza Ghafarollahi, Markus J. Buehler

A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ‰∏≠Ôºå‰∏ÄÂÄãÈóúÈçµÁöÑÊåëÊà∞ÊòØÂâµÈÄ†Âá∫ÊúâËÉΩÂäõÈÄèÈÅéÊé¢Á¥¢Êñ∞È†òÂüü„ÄÅË≠òÂà•Ë§áÈõúÊ®°ÂºèÔºå‰ª•ÂèäÂú®Â§ßÈáèÁöÑÁßëÂ≠∏Êï∏Êìö‰∏≠ÁôºÁèæÂâçÊâÄÊú™Ë¶ãÁöÑÈóúËÅØÔºå‰æÜËá™‰∏ªÊé®ÈÄ≤ÁßëÂ≠∏ÁêÜËß£ÁöÑÁ≥ªÁµ±„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SciAgentsÔºå‰∏ÄÁ®ÆÂà©Áî®‰∏âÂÄãÊ†∏ÂøÉÊ¶ÇÂøµÁöÑÊñπÊ≥ïÔºö(1) ‰ΩøÁî®Â§ßË¶èÊ®°ÁöÑÊú¨‰ΩìÁü•Ë≠òÂúñË≠ú‰æÜÊï¥ÁêÜÂíåÈÄ£Áµê‰∏çÂêåÁöÑÁßëÂ≠∏Ê¶ÇÂøµÔºå(2) ‰∏ÄÂ•óÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊï∏ÊìöÊ™¢Á¥¢Â∑•ÂÖ∑Ôºå‰ª•Âèä (3) ÂÖ∑ÊúâÂéü‰ΩçÂ≠∏ÁøíËÉΩÂäõÁöÑÂ§ö‰ª£ÁêÜÁ≥ªÁµ±„ÄÇÊáâÁî®ÊñºÁîüÁâ©ÂïüÁôºÊùêÊñôÔºåSciAgents Êè≠Á§∫‰∫Ü‰ª•ÂâçË¢´Ë™çÁÇ∫ÁÑ°ÈóúÁöÑÈö±ËóèË∑®Â≠∏ÁßëÈóú‰øÇÔºåÈÅîÂà∞‰∫ÜË∂ÖË∂äÂÇ≥Áµ±‰∫∫ÁÇ∫Á†îÁ©∂ÊñπÊ≥ïÁöÑË¶èÊ®°„ÄÅÁ≤æÁ¢∫Â∫¶ÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇË©≤Ê°ÜÊû∂Ëá™‰∏ªÁîüÊàêÂíåÂÑ™ÂåñÁ†îÁ©∂ÂÅáË®≠ÔºåÈó°ÊòéÂü∫Á§éÊ©üÂà∂„ÄÅË®≠Ë®àÂéüÁêÜÂíåÊÑèÂ§ñÁöÑÊùêÊñôÁâπÊÄß„ÄÇÈÄèÈÅé‰ª•Ê®°ÁµÑÂåñÊñπÂºèÊï¥ÂêàÈÄô‰∫õËÉΩÂäõÔºåÊô∫ËÉΩÁ≥ªÁµ±Áî¢ÁîüÊùêÊñôÁôºÁèæ„ÄÅÊâπÂà§ÂíåÊîπÈÄ≤ÁèæÊúâÂÅáË®≠„ÄÅÊ™¢Á¥¢ÈóúÊñºÁèæÊúâÁ†îÁ©∂ÁöÑÊúÄÊñ∞Êï∏ÊìöÔºå‰∏¶Âº∑Ë™øÂÆÉÂÄëÁöÑÂÑ™ÈªûÂíåÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÁµêÂêàÁîüÊàêÂºè AI„ÄÅÊú¨‰ΩìË°®Á§∫ÂíåÂ§ö‰ª£ÁêÜÂª∫Ê®°ÁöÑÂèØÊì¥ÂÖÖËÉΩÂäõÔºåÂà©Áî®È°û‰ººÊñºÁîüÁâ©Á≥ªÁµ±ÁöÑ„ÄåÊô∫ÊÖßÁæ§È´î„Äç„ÄÇÈÄôÁÇ∫ÊùêÊñôÁôºÁèæÊèê‰æõ‰∫ÜÊñ∞ÈÄîÂæëÔºå‰∏¶ÈÄèÈÅéËß£ÈéñÂ§ßËá™ÁÑ∂ÁöÑË®≠Ë®àÂéüÁêÜ‰æÜÂä†ÈÄüÂÖàÈÄ≤ÊùêÊñôÁöÑÈñãÁôº„ÄÇ

##### **Assessing SPARQL capabilities of Large Language Models**
2409.05925v1 by Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊï¥ÂêàÁÇ∫Áü•Ë≠òÈ©ÖÂãïÊáâÁî®Á®ãÂºèÊèê‰æõ‰∫ÜÈ°ØËëóÁöÑÁ∂úÊïàÊΩõÂäõ„ÄÇ‰∏ÄÁ®ÆÂèØËÉΩÁöÑÊï¥ÂêàÊòØËß£ÈáãÂíåÁî¢ÁîüÂΩ¢ÂºèÂåñË™ûË®ÄÔºå‰æãÂ¶ÇË™ûÁæ©Á∂≤Ë∑Ø‰∏≠‰ΩøÁî®ÁöÑË™ûË®ÄÔºåËÄå SPARQL ÊòØÂ≠òÂèñ KG ÁöÑÊ†∏ÂøÉÊäÄË°ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºË°°Èáè LLM ÈñãÁÆ±Âç≥Áî®ÁöÑËÉΩÂäõÔºå‰ª•‰ΩøÁî® SPARQLÔºåÊõ¥ÂÖ∑È´îÂú∞Ë™™Ôºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢ÊáâÁî®ÈáèÂåñÊñπÊ≥ï„ÄÇ
  ÊàëÂÄëÂú® LLM-KG-Bench Êû∂Êßã‰∏≠ÂØ¶‰Ωú‰∫ÜÂêÑÁ®ÆÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºå‰ª•Ëá™ÂãïÂü∑Ë°åÂíåË©ï‰º∞Â§öÂÄã LLM„ÄÇÈÄô‰∫õ‰ªªÂãôË©ï‰º∞‰∫ÜË™ûÊ≥ï„ÄÅË™ûÁæ©ËÆÄÂèñ„ÄÅË™ûÁæ©Âª∫Á´ãÂíåÁü•Ë≠òÂúñË≠úÊèêÁ§∫ÂåÖÂê´ÁöÑËßíËâ≤Á≠âÈù¢ÂêëÁöÑËÉΩÂäõ„ÄÇ
  Êúâ‰∫ÜÈÄô‰∫õÊñ∞ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºåÊàëÂÄëË©ï‰º∞‰∫Ü GPT„ÄÅGemini Âíå Claude Ê®°ÂûãÁöÑÈÅ∏È†Ö„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢Â∞çÊñº LLM ‰æÜË™™‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏¶‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÂÖ∑È´îÁöÑ LLM ‰ª•Âèä‰ªªÂãôÁöÑË§áÈõúÊÄß„ÄÇÂÑòÁÆ°‰øÆÂæ©Âü∫Êú¨ÁöÑË™ûÊ≥ïÈåØË™§‰ºº‰πéÂ∞çÁõÆÂâçË©ï‰º∞ÁöÑÊúÄ‰Ω≥ LLM ‰æÜË™™‰∏çÊàêÂïèÈ°åÔºå‰ΩÜÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÂª∫Á´ãË™ûÁæ©Ê≠£Á¢∫ÁöÑ SPARQL SELECT Êü•Ë©¢ÂæàÂõ∞Èõ£„ÄÇ

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âº∑Â§ßÁöÑÂäüËÉΩÔºåÈÄ≤Ë°åÊïò‰∫ãÁîüÊàê„ÄÅÈÇèËºØÊé®ÁêÜÂíåÂ∏∏Ë≠òÁü•Ë≠òÊï¥ÂêàÔºåÊú¨Á†îÁ©∂Ê∑±ÂÖ•Êé¢Ë®éÂà©Áî® LLM ‰æÜÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàê (R2Gen)„ÄÇÂÑòÁÆ° LLM ÊìÅÊúâË±êÂØåÁöÑÁü•Ë≠òÔºå‰ΩÜË¶ÅÊúâÊïàËß∏ÁôºÈÄô‰∫õÂ§ßÂûãÊ®°Âûã‰∏≠ËàáÁâπÂÆö‰ªªÂãôÔºàÂ¶Ç R2GenÔºâÁõ∏ÈóúÁöÑÁü•Ë≠òÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü KARGENÔºå‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÁü•Ë≠òÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàêÊ°ÜÊû∂„ÄÇÂà©Áî®ÂáçÁµêÁöÑ LLM ‰æÜÁîüÊàêÂ†±ÂëäÔºåË©≤Ê°ÜÊû∂Êï¥Âêà‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂúñË≠úÔºå‰ª•Ëß£Èéñ LLM ‰∏≠ËàáËÉ∏ÈÉ®ÁñæÁóÖÁõ∏ÈóúÁöÑÁü•Ë≠òÔºå‰ª•Â¢ûÂº∑ÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊïàÁî®„ÄÇÈÄôÊòØÈÄèÈÅéÂà©Áî®Áü•Ë≠òÂúñË≠ú‰ª•Ë®≠Ë®àÁöÑÊñπÂºèÊèêÂèñËàáÁñæÁóÖÁõ∏ÈóúÁöÑÁâπÂæµ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÁî±ÊñºÊîæÂ∞ÑÂ†±ÂëäÂåÖÂê´Ê≠£Â∏∏ÂíåÁñæÁóÖÁõ∏ÈóúÁöÑÁôºÁèæÔºåÂõ†Ê≠§ÊèêÂèñÁöÑÂúñÂΩ¢Â¢ûÂº∑ÁñæÁóÖÁõ∏ÈóúÁâπÂæµËàáÂçÄÂüüÂΩ±ÂÉèÁâπÂæµÊï¥ÂêàÔºåÂÖºÈ°ßÂÖ©ÂÄãÊñπÈù¢„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫ÜÂÖ©Á®ÆËûçÂêàÊñπÊ≥ïÔºå‰ª•Ëá™ÂãïÂÑ™ÂÖàÊéíÂ∫èÂíåÈÅ∏ÊìáÊúÄÁõ∏ÈóúÁöÑÁâπÂæµ„ÄÇËûçÂêàÁöÑÁâπÂæµÁî± LLM ‰ΩøÁî®Ôºå‰ª•ÁîüÊàêÂ∞çÁñæÁóÖÊõ¥ÊïèÊÑü‰∏îÂìÅË≥™Êõ¥È´òÁöÑÂ†±Âëä„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® MIMIC-CXR Âíå IU-Xray Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ</paragraph>

##### **Action is the primary key: a categorical framework for episode description and logical reasoning**
2409.04793v1 by Yoshiki Fukada

This research presents a computational framework for describing and
recognizing episodes and for logical reasoning. This framework, named
cognitive-logs, consists of a set of relational and graph databases.
Cognitive-logs record knowledge, particularly in episodes that consist of
"actions" represented by verbs in natural languages and "participants" who
perform the actions. These objects are connected by arrows (morphisms) that
link each action to its participant and link cause to effect. Operations based
on category theory enable comparisons between episodes and deductive
inferences, including abstractions of stories. One of the goals of this study
is to develop a database-driven artificial intelligence. This artificial
intelligence thinks like a human but possesses the accuracy and rigour of a
machine. The vast capacities of databases (up to petabyte scales in current
technologies) enable the artificial intelligence to store a greater volume of
knowledge than neural-network based artificial intelligences. Cognitive-logs
serve as a model of human cognition and designed with references to cognitive
linguistics. Cognitive-logs also have the potential to model various human mind
activities.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãË®àÁÆóÊ°ÜÊû∂ÔºåÁî®‰æÜÊèèËø∞ÂíåËæ®Ë≠ò‰∫ã‰ª∂‰ª•ÂèäÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜ„ÄÇÈÄôÂÄãÊ°ÜÊû∂ÂêçÁÇ∫Ë™çÁü•Êó•Ë™åÔºåÂåÖÂê´‰∏ÄÁµÑÈóúËÅØÂºèÂíåÂúñÂΩ¢Ë≥áÊñôÂ∫´„ÄÇË™çÁü•Êó•Ë™åË®òÈåÑÁü•Ë≠òÔºåÁâπÂà•ÊòØÂåÖÂê´Áî±Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÂãïË©ûË°®Á§∫ÁöÑ„ÄåÂãï‰Ωú„ÄçÂíåÂü∑Ë°åÂãï‰ΩúÁöÑ„ÄåÂèÉËàáËÄÖ„ÄçÁöÑ‰∫ã‰ª∂„ÄÇÈÄô‰∫õÁâ©‰ª∂Áî±ÁÆ≠È†≠ÔºàÊÖãÂ∞ÑÔºâÈÄ£Êé•ÔºåÂ∞áÊØèÂÄãÂãï‰ΩúÈÄ£ÁµêÂà∞ÂÖ∂ÂèÉËàáËÄÖÔºå‰∏¶Â∞áÂéüÂõ†ÈÄ£ÁµêÂà∞ÁµêÊûú„ÄÇÂü∫ÊñºÁØÑÁñáË´ñÁöÑÈÅãÁÆóÂèØÊØîËºÉ‰∫ã‰ª∂ÂíåÊºîÁππÊé®Ë´ñÔºåÂåÖÊã¨ÊïÖ‰∫ãÁöÑÊäΩË±°Âåñ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ô‰πã‰∏ÄÊòØÈñãÁôº‰∏ÄÂÄãË≥áÊñôÂ∫´È©ÖÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÈÄôÂÄã‰∫∫Â∑•Êô∫ÊÖßÊÄùËÄÉÊñπÂºèÂÉè‰∫∫È°ûÔºå‰ΩÜÊìÅÊúâÊ©üÂô®Ëà¨ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂö¥Ë¨πÊÄß„ÄÇË≥áÊñôÂ∫´ÁöÑÈæêÂ§ßÂÆπÈáèÔºàÂú®ÁõÆÂâçÁöÑÊäÄË°ì‰∏≠ÂèØÈÅîÁöÆ‰ΩçÂÖÉÁµÑÁ≠âÁ¥öÔºâ‰Ωø‰∫∫Â∑•Êô∫ÊÖßËÉΩÂ§†ÂÑ≤Â≠òÊØîÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊõ¥Â§ßÁöÑÁü•Ë≠òÈáè„ÄÇË™çÁü•Êó•Ë™å‰ΩúÁÇ∫‰∫∫È°ûË™çÁü•ÁöÑÊ®°ÂûãÔºå‰∏¶ÂèÉËÄÉË™çÁü•Ë™ûË®ÄÂ≠∏ÈÄ≤Ë°åË®≠Ë®à„ÄÇË™çÁü•Êó•Ë™å‰πüÊúâÊΩõÂäõÊ®°Êì¨ÂêÑÁ®Æ‰∫∫È°ûÂøÉÊô∫Ê¥ªÂãï„ÄÇ

##### **Accelerating Training with Neuron Interaction and Nowcasting Networks**
2409.04434v1 by Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien

Neural network training can be accelerated when a learnable update rule is
used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable
update rules can be costly and unstable to train and use. A simpler recently
proposed approach to accelerate training is to use Adam for most of the
optimization steps and periodically, only every few steps, nowcast (predict
future) parameters. We improve this approach by Neuron interaction and
Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural
networks to more accurately nowcast parameters by learning in a supervised way
from a set of training trajectories over multiple tasks. We show that in some
networks, such as Transformers, neuron connectivity is non-trivial. By
accurately modeling neuron connectivity, we allow NiNo to accelerate Adam
training by up to 50\% in vision and language tasks.

ÊëòË¶ÅÔºöÁ•ûÁªèÁΩëÁªúËÆ≠ÁªÉÂèØ‰ª•Âä†ÈÄüÔºåÂΩì‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôË¢´Áî®Êù•‰ª£ÊõøÁªèÂÖ∏ÁöÑËá™ÈÄÇÂ∫î‰ºòÂåñÂô®Ôºà‰æãÂ¶Ç AdamÔºâ„ÄÇÁÑ∂ËÄåÔºåÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôÂèØËÉΩÊòØÊòÇË¥µ‰∏î‰∏çÁ®≥ÂÆöÁöÑÔºåÈúÄË¶ÅËÆ≠ÁªÉÂíå‰ΩøÁî®„ÄÇ‰∏ÄÁßçÊúÄËøëÊèêÂá∫ÁöÑÊõ¥ÁÆÄÂçïÁöÑÂä†ÈÄüËÆ≠ÁªÉÁöÑÊñπÊ≥ïÊòØÔºåÂØπ‰∫éÂ§ßÂ§öÊï∞ÁöÑ‰ºòÂåñÊ≠•È™§‰ΩøÁî® AdamÔºåÂπ∂‰∏îÂÆöÊúüÂú∞Ôºå‰ªÖÊØèÈöîÂá†Ê≠•ÔºåÈ¢ÑÊµãÔºàÈ¢ÑÊµãÊú™Êù•ÔºâÂèÇÊï∞„ÄÇÊàë‰ª¨ÈÄöËøáÁ•ûÁªèÂÖÉ‰∫§‰∫íÂíåÈ¢ÑÊµãÔºàNiNoÔºâÁΩëÁªúÊù•ÊîπËøõËøôÁßçÊñπÊ≥ï„ÄÇNiNo Âà©Áî®Á•ûÁªèÂÖÉËøûÊé•ÂíåÂõæÁ•ûÁªèÁΩëÁªúÔºåÈÄöËøá‰ªéÂ§ö‰∏™‰ªªÂä°‰∏≠ÁöÑ‰∏ÄÁªÑËÆ≠ÁªÉËΩ®Ëøπ‰∏≠‰ª•ÁõëÁù£ÊñπÂºèÂ≠¶‰π†ÔºåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµãÂèÇÊï∞„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÂú®‰∏Ä‰∫õÁΩëÁªú‰∏≠Ôºå‰æãÂ¶Ç TransformerÔºåÁ•ûÁªèÂÖÉËøûÊé•ÊòØÈùûÂπ≥Âá°ÁöÑ„ÄÇÈÄöËøáÂáÜÁ°ÆÂú∞Âª∫Ê®°Á•ûÁªèÂÖÉËøûÊé•ÔºåÊàë‰ª¨ÂÖÅËÆ∏ NiNo Â∞Ü Adam ËÆ≠ÁªÉÂä†ÈÄüÈ´òËææ 50%ÔºåÁî®‰∫éËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°„ÄÇ

##### **Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**
2409.04286v1 by Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel

Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÂâçÂÖ¨ÈñãÂèØÁî®ÁöÑÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôËíêÈõÜÁº∫‰πèÂ§öÂÖÉÊÄß„ÄÅÂª£Ê≥õË®ªËß£Âíå‰ΩøÁî®ËÄÖÂèäÂÖ∂Êñá‰ª∂ËÉåÊôØË≥áË®ä„ÄÇÈÄô‰∫õÂïèÈ°åÈòªÁ§ô‰∫ÜÂÆ¢ËßÄ‰∏îÂèØÊØîËºÉÁöÑË≥áÊñôÈ©ÖÂãïË©ï‰º∞Ôºå‰ª•ÂèäÁü•Ë≠òÂ∑•‰ΩúÂçîÂä©Á≥ªÁµ±ÁöÑÊúÄ‰Ω≥Âåñ„ÄÇÁî±ÊñºÂú®ÁèæÂØ¶ÁîüÊ¥ª‰∏≠ËíêÈõÜÊ≠§È°ûË≥áÊñôÈúÄË¶ÅÂ§ßÈáèË≥áÊ∫êÔºåËÄå‰∏îÂøÖÈ†àÂØ©Êü•Ë≥áÊñôÔºåËíêÈõÜÊ≠§È°ûË≥áÊñôÁµÑÈ°ØÁÑ∂Âπæ‰πé‰∏çÂèØËÉΩ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèØË®≠ÂÆöÁöÑÂ§öÈáç‰ª£ÁêÜÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôÁµÑÁî¢ÁîüÂô®„ÄÇÊ≠§Á≥ªÁµ±Ê®°Êì¨‰ª£ÁêÜ‰πãÈñìÁöÑÂçî‰ΩúÁü•Ë≠òÂ∑•‰ΩúÔºåÁî¢ÁîüÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁöÑÊñá‰ª∂ÂíåÈö®ÈôÑÁöÑË≥áÊñôËøΩËπ§„ÄÇÊ≠§Â§ñÔºåÁî¢ÁîüÂô®ÊúÉÊì∑ÂèñÊâÄÊúâËÉåÊôØË≥áË®äÔºåÂú®ÁµÑÊÖã‰∏≠Êèê‰æõÊàñÂú®Ê®°Êì¨ÈÅéÁ®ã‰∏≠Âª∫Á´ãÔºå‰∏¶Â∞áÂÖ∂ÂÑ≤Â≠òÂú®Áü•Ë≠òÂúñË≠ú‰∏≠„ÄÇÊúÄÂæåÔºåÁî¢ÁîüÁöÑË≥áÊñôÁµÑÂèØ‰ª•‰ΩøÁî®ÂíåÂàÜ‰∫´ÔºåÁÑ°È†àÊìîÂøÉÈö±ÁßÅÊàñÊ©üÂØÜÊÄß„ÄÇ
Êú¨Êñá‰ªãÁ¥πÊàëÂÄëÊñπÊ≥ïÁöÑË®≠Ë®àÂíåÈ°òÊôØÔºå‰∏¶Â∞àÊ≥®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁúüÂØ¶ÁöÑÁü•Ë≠òÂ∑•‰ΩúÊñá‰ª∂„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ê∂âÂèä‰∫∫È°ûË©ïÂàÜÂì°Ôºå‰ªñÂÄëË©ï‰º∞‰∫Ü 53% ÁöÑÁî¢ÁîüÊñá‰ª∂Âíå 74% ÁöÑÁúüÂØ¶Êñá‰ª∂ÁÇ∫ÁúüÂØ¶ÔºåÈÄôË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûêÂèÉËàáËÄÖË©ïË´ñ‰∏≠ÊèêÂà∞ÁöÑÁúüÂØ¶ÊÄßÊ®ôÊ∫ñÔºå‰∏¶Ë©≥Á¥∞Ë™™ÊòéÂ∑≤Ë≠òÂà•Â∏∏Ë¶ãÂïèÈ°åÁöÑÊΩõÂú®ÊîπÂñÑÊñπÊ≥ï„ÄÇ</paragraph>

##### **GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**
2409.04183v1 by Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang

Programming languages possess rich semantic information such as data flow
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with four different baseline LLMs ranging in size from 350M to 8B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3.

ÊëòË¶ÅÔºöÁ®ãÂºèË™ûË®ÄÊìÅÊúâË±êÂØåÁöÑË™ûÊÑèË≥áË®äÔºå‰æãÂ¶ÇÁî±ÂúñÂΩ¢Ë°®Á§∫‰∏îÁÑ°Ê≥ïÂæûÂéüÂßãÁ¢ºË°®Èù¢ÂΩ¢ÂºèÂèñÂæóÁöÑË≥áÊñôÊµÅÁ®ã„ÄÇÊúÄËøëÁöÑÁ®ãÂºèÁ¢ºË™ûË®ÄÊ®°ÂûãÂ∑≤Êì¥ÂÖÖËá≥Êï∏ÂçÅÂÑÑÂÄãÂèÉÊï∏Ôºå‰ΩÜÊ®°ÂûãÂéüÂßãÁ¢ºÂÉÖ‰ΩúÁÇ∫ÊñáÂ≠óÁ¨¶ËôüÔºåËÄåÂøΩÁï•‰ªª‰ΩïÂÖ∂‰ªñÁµêÊßãË≥áË®ä„ÄÇÂèç‰πãÔºåÁ∑®Á¢ºÁ®ãÂºèÁ¢ºÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÊúÉ‰øÆÊîπ Transformer Êû∂ÊßãÔºåÈôêÂà∂ÂÖ∂Ë¶èÊ®°ÂíåËàáÈ†êÂÖàË®ìÁ∑¥ÁöÑ LLM ÁöÑÁõ∏ÂÆπÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé°Áî® GALLaÔºàÂúñÂΩ¢Â∞çÈΩäÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºâÊì∑ÂèñÂÖ©ÂÖ®ÂÖ∂ÁæéÁöÑÂÑ™Èªû„ÄÇGALLa Âà©Áî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåË∑®Ê®°ÊÖãÂ∞çÈΩäÊäÄË°ìÔºåÂú®ÂæÆË™øÊúüÈñìÂ∞áÁ®ãÂºèÁ¢ºÁöÑÁµêÊßãË≥áË®äÊ≥®ÂÖ• LLM ‰ΩúÁÇ∫ËºîÂä©‰ªªÂãô„ÄÇÊ≠§Êû∂ÊßãÂêåÊôÇ‰∏ç‰æùË≥¥Ê®°ÂûãÂíå‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ÊáâÁî®Êñº‰ªª‰ΩïÁ®ãÂºèÁ¢º LLM ÁöÑ‰ªª‰ΩïÁ®ãÂºèÁ¢º‰∏ãÊ∏∏‰ªªÂãôÔºå‰∏¶‰∏îÂÉÖÂú®Ë®ìÁ∑¥ÊúüÈñìÂæûËàáÂæÆË™øË≥áÊñôÁÑ°ÈóúÁöÑË™ûÊñôÂ∫´ÂèñÂæóÁµêÊßãÂúñÂΩ¢Ë≥áÊñôÔºåÂêåÊôÇÂú®Êé®Ë´ñÊúüÈñì‰∏çÁî¢ÁîüÊØîÂü∫Ê∫ñ LLM Êõ¥È´òÁöÑÊàêÊú¨„ÄÇÂú®‰∫îÂÄãÁ®ãÂºèÁ¢º‰ªªÂãô‰∏≠ÈÄ≤Ë°åÂØ¶È©óÔºå‰ΩøÁî®ÂõõÂÄã‰∏çÂêåÁöÑÂü∫Ê∫ñ LLMÔºåË¶èÊ®°Âæû 350M Âà∞ 8BÔºåÈ©óË≠â GALLa ÁöÑÊúâÊïàÊÄßÔºåË≠âÊòéÂç≥‰ΩøÂ∞çÊñº LLaMA3 Á≠âÂº∑Â§ßÊ®°ÂûãÔºå‰πüËÉΩÊåÅÁ∫åÂÑ™ÊñºÂü∫Ê∫ñ„ÄÇ

##### **Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**
2409.04181v1 by Larissa Pusch, Tim O. F. Conrad

Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÊï∏‰ΩçË≥áË®äÁ≥ªÁµ±Ôºà‰æãÂ¶ÇË≥áÊñôÂ∫´Ôºâ‰∫íÂãïÁöÑÊñπÂºèÔºåËÆìÈÄô‰∫õÁ≥ªÁµ±ËÆäÂæóÊõ¥ÊòìÊñºÂ≠òÂèñ„ÄÇÁÑ∂ËÄåÔºåÊåëÊà∞‰ªçÁÑ∂Â≠òÂú®ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∫ñÁ¢∫ÊÄßËá≥ÈóúÈáçË¶ÅÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰æãÂ¶ÇÂú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüü„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØÂπªË¶∫ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê®°ÂûãÊúÉÁî¢ÁîüÊú™Á∂ìÂü∫Á§éË≥áÊñôÈ©óË≠âÁöÑË≥áË®äÔºåÂèØËÉΩÂ∞éËá¥Âç±Èö™ÁöÑÈåØË™§Ë≥áË®ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄèÈÅéÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºå‰ª•ÊèêÈ´òÁîüÁâ©ÈÜ´Â≠∏ KG ‰∏≠ÂïèÁ≠îÁ≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂª∫Á´ãÂú® LangChain Ê°ÜÊû∂‰∏äÔºåÁµêÂêà‰∫Ü‰∏ÄÂÄãÊü•Ë©¢Ê™¢Êü•Âô®ÔºåÂèØÁ¢∫‰øù LLM ÁîüÊàêÁöÑÊü•Ë©¢Âú®Ë™ûÊ≥ïÂíåË™ûÊÑè‰∏äÊúâÊïàÔºåÁÑ∂ÂæåÁî®ÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñË≥áË®äÔºåÂ§ßÂπÖÊ∏õÂ∞ëÂπªË¶∫Á≠âÈåØË™§„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãÊñ∞ÁöÑ 50 ÂÄãÁîüÁâ©ÈÜ´Â≠∏ÂïèÈ°åÂü∫Ê∫ñË≥áÊñôÈõÜË©ï‰º∞‰∫ÜÊï¥È´îÊïàËÉΩÔºåÊ∏¨Ë©¶‰∫ÜÂåÖÊã¨ GPT-4 Turbo Âíå llama3:70b Âú®ÂÖßÁöÑÂπæÂÄã LLM„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÈõñÁÑ∂ GPT-4 Turbo Âú®Áî¢ÁîüÊ∫ñÁ¢∫Êü•Ë©¢ÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºå‰ΩÜÂÉè llama3:70b ÈÄôÊ®£ÁöÑÈñãÊ∫êÊ®°ÂûãÂú®ÈÅ©Áï∂ÁöÑÊèêÁ§∫Â∑•Á®ã‰∏ãÈ°ØÁ§∫Âá∫ÂâçÊôØ„ÄÇÁÇ∫‰∫ÜËÆìÈÄôÁ®ÆÊñπÊ≥ïÊòìÊñº‰ΩøÁî®ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÁ∂≤Ë∑Ø‰ªãÈù¢ÔºåËÆì‰ΩøÁî®ËÄÖÂèØ‰ª•Ëº∏ÂÖ•Ëá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢„ÄÅÊ™¢Ë¶ñÁî¢ÁîüÂíåÊõ¥Ê≠£ÁöÑ Cypher Êü•Ë©¢Ôºå‰∏¶È©óË≠âÁµêÊûúË∑ØÂæëÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÈÄôÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜË≥áÊñôÂ∑ÆË∑ùÂíåÂπªË¶∫Á≠âÂ∏∏Ë¶ãÂïèÈ°åÔºåÁÇ∫ÂïèÁ≠îÁ≥ªÁµ±Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÈù†‰∏îÁõ¥ËßÄÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊú¨ÊñáÁµêÊûúÁî¢ÁîüÁöÑÂéüÂßãÁ¢ºÂíå‰ΩøÁî®ËÄÖ‰ªãÈù¢ÁöÑÂéüÂßãÁ¢ºÂèØ‰ª•Âú®ÊàëÂÄëÁöÑ Git ÂÑ≤Â≠òÂ∫´‰∏≠ÊâæÂà∞Ôºöhttps://git.zib.de/lpusch/cyphergenkg-gui

##### **Refining Wikidata Taxonomy using Large Language Models**
2409.04056v1 by Yiwen Peng, Thomas Bonald, Mehwish Alam

Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.

ÊëòË¶ÅÔºöÁî±ÊñºÂÖ∂Âçî‰ΩúÊÄßË≥™ÔºåWikidata Â∑≤Áü•ÂÖ∑ÊúâË§áÈõúÁöÑÂàÜÈ°ûÊ≥ïÔºå‰∏¶ÊúâÈáçË§áÁôºÁîüÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂØ¶‰æãÂíåÈ°ûÂà•‰πãÈñìÁöÑÊ≠ßÁæ©„ÄÅÊüê‰∫õÂàÜÈ°ûË∑ØÂæëÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄß„ÄÅÂæ™Áí∞ÁöÑÂ≠òÂú®Ôºå‰ª•ÂèäÈ°ûÂà•‰πãÈñìÁöÑÈ´òÂÜóÈ§ò„ÄÇÊâãÂãïÊ∏ÖÁêÜÊ≠§ÂàÜÈ°ûÊ≥ïÁöÑÂ∑•‰ΩúÊó¢ËÄóÊôÇÂèàÂÆπÊòìÂá∫ÁèæÈåØË™§Êàñ‰∏ªËßÄÂà§Êñ∑„ÄÇÊàëÂÄëÊèêÂá∫ WiKCÔºåÈÄôÊòØ Wikidata ÂàÜÈ°ûÊ≥ïÁöÑÊñ∞ÁâàÊú¨Ôºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂúñÂΩ¢ÊåñÊéòÊäÄË°ìËá™ÂãïÊ∏ÖÁêÜ„ÄÇÂàÜÈ°ûÊ≥ï‰∏äÁöÑÊìç‰ΩúÔºå‰æãÂ¶ÇÂâ™ÂàáÈèàÊé•ÊàñÂêà‰ΩµÈ°ûÂà•ÔºåÊòØÂú®ÈñãÊ∫ê LLM ‰∏äÂÄüÂä©Èõ∂Ê¨°ÊèêÁ§∫ÁöÑÂπ´Âä©‰∏ãÂü∑Ë°åÁöÑ„ÄÇÁ≤æÁÖâÂàÜÈ°ûÊ≥ïÁöÑÂìÅË≥™ÂæûÂÖßÂú®ÂíåÂ§ñÂú®ÁöÑËßÄÈªûÈÄ≤Ë°åË©ï‰º∞ÔºåÂú®ÂæåËÄÖÁöÑÂØ¶È´îÂàÜÂûã‰ªªÂãô‰∏äÔºåÈ°ØÁ§∫‰∫Ü WiKC ÁöÑÂØ¶ÈöõËààË∂£„ÄÇ

##### **Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**
2409.04009v1 by Miao Fan, Yeqi Bai, Mingming Sun, Ping Li

Relation classification (RC) plays a pivotal role in both natural language
understanding and knowledge graph completion. It is generally formulated as a
task to recognize the relationship between two entities of interest appearing
in a free-text sentence. Conventional approaches on RC, regardless of feature
engineering or deep learning based, can obtain promising performance on
categorizing common types of relation leaving a large proportion of
unrecognizable long-tail relations due to insufficient labeled instances for
training. In this paper, we consider few-shot learning is of great practical
significance to RC and thus improve a modern framework of metric learning for
few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained
features, expecting they can generalize well on long-tail relations. Extensive
experiments were conducted by FewRel, a large-scale supervised few-shot RC
dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate
that it can achieve substantial improvements over many baseline approaches.

ÊëòË¶ÅÔºöÈóú‰øÇÂàÜÈ°û (RC) Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁü•Ë≠òÂúñË≠úÂÆåÊàê‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÆÉÈÄöÂ∏∏Ë¢´Ë°®Ëø∞ÁÇ∫‰∏ÄÂÄã‰ªªÂãôÔºåÁî®ÊñºËæ®Ë≠òÂá∫ÁèæÂú®Ëá™Áî±ÊñáÂ≠óÂè•Â≠ê‰∏≠ÁöÑÂÖ©ÂÄãÊÑüËààË∂£ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÁÑ°Ë´ñÊòØÂü∫ÊñºÁâπÂæµÂ∑•Á®ãÈÇÑÊòØÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÇ≥Áµ± RC ÊñπÊ≥ïÔºåÈÉΩÂèØ‰ª•Â∞çÂ∏∏Ë¶ãÁöÑÈóú‰øÇÈ°ûÂûãÈÄ≤Ë°åÂàÜÈ°ûÔºåÂæûËÄåÁç≤ÂæóÊúâÂ∏åÊúõÁöÑÊïàËÉΩÔºå‰ΩÜÁî±ÊñºË®ìÁ∑¥Ê®ôÁ±§ÂØ¶‰æã‰∏çË∂≥ÔºåÂõ†Ê≠§ÁÑ°Ê≥ïËæ®Ë≠òÂá∫Â§ßÈáèÁöÑÈï∑Â∞æÈóú‰øÇ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™çÁÇ∫Â∞ëÊ®£Êú¨Â≠∏ÁøíÂ∞ç RC ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂØ¶Áî®ÊÑèÁæ©ÔºåÂõ†Ê≠§ÊîπÈÄ≤‰∫ÜÂ∫¶ÈáèÂ≠∏ÁøíÁöÑÁèæ‰ª£Ê°ÜÊû∂Ôºå‰ª•ÈÄ≤Ë°åÂ∞ëÊ®£Êú¨ RC„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®ÂÖ∑ÊúâÁ¥∞Á≤íÂ∫¶ÁâπÂæµÁöÑÂ§ßÈÇäË∑ù ProtoNetÔºåÊúüÊúõÂÆÉÂÄëËÉΩÂú®Èï∑Â∞æÈóú‰øÇ‰∏äÂæàÂ•ΩÂú∞Ê¶ÇÊã¨„ÄÇÊàëÂÄë‰ΩøÁî®Â§ßÂûãÁõ£Áù£Â∞ëÊ®£Êú¨ RC Ë≥áÊñôÈõÜ FewRel ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Ë©ï‰º∞ÊàëÂÄëÁöÑÊ°ÜÊû∂ÔºöLM-ProtoNet (FGF)„ÄÇÁµêÊûúË°®ÊòéÔºåÂÆÉÂèØ‰ª•ÊØîË®±Â§öÂü∫Á∑öÊñπÊ≥ïÁç≤ÂæóÈ°ØËëóÊîπÈÄ≤„ÄÇ

##### **Rx Strategist: Prescription Verification using LLM Agents System**
2409.03440v1 by Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh

To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.

ÊëòË¶ÅÔºöÁÇ∫‰∫Ü‰øùË≠∑ÊÇ£ËÄÖÂÆâÂÖ®ÔºåÁèæ‰ª£Ëó•ÂìÅË§áÈõúÊÄßË¶ÅÊ±ÇÂö¥Ê†ºÁöÑËôïÊñπÈ©óË≠â„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï - Rx Strategist - ÂÆÉÂà©Áî®Áü•Ë≠òÂúñË≠úÂíå‰∏çÂêåÁöÑÊêúÂ∞ãÁ≠ñÁï•‰æÜÂ¢ûÂº∑‰ª£ÁêÜÊû∂ÊßãÂÖßÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂäüËÉΩ„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÊäÄË°ìÂÖÅË®±Â§öÈöéÊÆµÁöÑ LLM ÁÆ°Á∑öÂíåÂæûËá™Ë®Ç‰∏ªÂãïÊàêÂàÜË≥áÊñôÂ∫´‰∏≠ÂèØÈù†Âú∞Êì∑ÂèñË≥áË®ä„ÄÇËôïÊñπÈ©óË≠âÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰æãÂ¶ÇÈÅ©ÊáâÁóá„ÄÅÂäëÈáèÂíåÂèØËÉΩÁöÑËó•Áâ©‰∫§‰∫í‰ΩúÁî®ÔºåÈÉΩÂú®ÁÆ°Á∑öÁöÑÊØèÂÄãÈöéÊÆµ‰∏≠Ê∂µËìã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊé®ÁêÜÂàÜÊï£Âú®ÈÄô‰∫õÈöéÊÆµ‰æÜÊ∏õËºïÂñÆ‰∏Ä LLM ÊäÄË°ìÁöÑÁº∫ÈªûÔºåÂêåÊôÇÊèêÈ´òÊ≠£Á¢∫ÊÄßÂíåÂèØÈù†ÊÄßÔºå‰∏¶Ê∏õÂ∞ëË®òÊÜ∂È´îÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåRx Strategist Ë∂ÖË∂äË®±Â§öÁèæÊúâÁöÑ LLMÔºåÈÅîÂà∞ËàáÁ∂ìÈ©óË±êÂØåÁöÑËá®Â∫äËó•ÂäëÂ∏´Áõ∏Áï∂ÁöÑË°®Áèæ„ÄÇÂú®Áèæ‰ª£Ëó•Áâ©Ë§áÈõúÁöÑ‰∏ñÁïå‰∏≠ÔºåÈÄôÁ®ÆÂ∞á LLM ËàáÊúâÁµÑÁπîÁöÑÁü•Ë≠òÂíåÂÖàÈÄ≤ÊêúÂ∞ãÊñπÊ≥ïÁõ∏ÁµêÂêàÔºåÁÇ∫Ê∏õÂ∞ëËôïÊñπÈåØË™§ÂíåÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåÊèê‰æõ‰∫ÜÂèØË°åÁöÑÈÄîÂæë„ÄÇ

##### **iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**
2409.03284v1 by Yassir Lairgi, Ludovic Moncla, R√©my Cazabet, Khalid Benabdeslem, Pierre Cl√©au

Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.

ÊëòË¶ÅÔºöÂ§ßÈÉ®ÂàÜÂèØÁî®Ë≥áÊñôÁÇ∫ÈùûÁµêÊßãÂåñÔºåÈÄô‰ΩøÂæóÂ≠òÂèñÊúâÂÉπÂÄºÁöÑË≥áË®äËÆäÂæóÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇËá™ÂãïÂª∫Á´ãÁü•Ë≠òÂúñË≠ú (KG) Â∞çÊñºÁµêÊßãÂåñË≥áÊñôÂíåËÆìË≥áÊñôÊòìÊñºÂ≠òÂèñËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÂ§†ÊúâÊïàÂú∞ÊêúÂ∞ãË≥áË®ä„ÄÇKG ‰πü‰øÉÈÄ≤Ë¶ãËß£„ÄÅÊé®Ë´ñÂíåÊé®ÁêÜ„ÄÇÂÇ≥Áµ±ÁöÑ NLP ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂëΩÂêçÂØ¶È´îËæ®Ë≠òÂíåÈóú‰øÇËêÉÂèñÔºåÂú®Ë≥áË®äÊ™¢Á¥¢‰∏≠ÊòØÈóúÈçµÔºå‰ΩÜÈù¢Ëá®ÈôêÂà∂ÔºåÂåÖÊã¨‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÂØ¶È´îÈ°ûÂûãÂíåÈúÄË¶ÅÁõ£Áù£ÂºèÂ≠∏Áøí„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÈõ∂Ê¨°ÊàñÂ∞ëÊ¨°Â≠∏Áøí„ÄÇÁÑ∂ËÄåÔºåÊú™Ëß£Ê±∫ÂíåË™ûÁæ©ÈáçË§áÁöÑÂØ¶È´îÂíåÈóú‰øÇ‰ªçÁÑ∂ÊßãÊàêÊåëÊà∞ÔºåÂ∞éËá¥ÂúñÂΩ¢‰∏ç‰∏ÄËá¥ÔºåÈúÄË¶ÅÂª£Ê≥õÁöÑÂæåËôïÁêÜ„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩ‰æùË≥¥Êñº‰∏ªÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ iText2KGÔºå‰∏ÄÁ®ÆÁî®ÊñºÊº∏ÈÄ≤Âºè„ÄÅËàá‰∏ªÈ°åÁÑ°ÈóúÁöÑ KG Âª∫ÊßãÊñπÊ≥ïÔºåÁÑ°ÈúÄÂæåËôïÁêÜ„ÄÇÈÄôÁ®ÆÂç≥ÊèíÂç≥Áî®„ÄÅÈõ∂Ê¨°ÁöÑÊñπÊ≥ïÈÅ©Áî®ÊñºÂª£Ê≥õÁöÑ KG Âª∫ÊßãÂ†¥ÊôØÔºå‰∏¶ÂåÖÂê´ÂõõÂÄãÊ®°ÁµÑÔºöÊñá‰ª∂Á≤æÈ§æÂô®„ÄÅÊº∏ÈÄ≤ÂºèÂØ¶È´îËêÉÂèñÂô®„ÄÅÊº∏ÈÄ≤ÂºèÈóú‰øÇËêÉÂèñÂô®Ôºå‰ª•ÂèäÂúñÂΩ¢Êï¥ÂêàÂô®ÂíåË¶ñË¶∫ÂåñÂô®„ÄÇËàáÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®‰∏âÁ®ÆÂ†¥ÊôØ‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩÔºöÂ∞áÁßëÂ≠∏Ë´ñÊñáËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÅÁ∂≤Á´ôËΩâÊèõÁÇ∫ÂúñÂΩ¢Ôºå‰ª•ÂèäÂ±•Ê≠∑ËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÇ

##### **GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**
2409.03258v1 by Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou

Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ËôïÁêÜÂúñÂΩ¢ÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉÂÄëÂú®ÈÄèÈÅéÂúñÂΩ¢ÊèèËø∞Â∫èÂàóÊèêÁ§∫ÁêÜËß£ÂúñÂΩ¢ÁµêÊßãË≥áË®äÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÂúñÂΩ¢Â§ßÂ∞èÂ¢ûÂä†ÊôÇ„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊåëÊà∞Ê≠∏Âõ†Êñº LLM Âú®ÂúñÂΩ¢ÊèèËø∞Â∫èÂàó‰∏≠‰∏çÂêå‰ΩçÁΩÆÁöÑË®òÊÜ∂ÂäõË°®Áèæ‰∏çÂùáÔºåÁ®±ÁÇ∫„Äå‰ΩçÁΩÆÂÅèË™§„Äç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphInsightÔºå‰∏ÄÂÄãÊó®Âú®ÊîπÂñÑ LLM Â∞çÂ∑®ËßÄÂíåÂæÆËßÄÂ±§Á¥öÂúñÂΩ¢Ë≥áË®äÁêÜËß£ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇGraphInsight ‰ª•ÂÖ©ÂÄãÈóúÈçµÁ≠ñÁï•ÁÇ∫Âü∫Á§éÔºö1) Â∞áÈóúÈçµÂúñÂΩ¢Ë≥áË®äÊîæÁΩÆÂú® LLM Â±ïÁèæËºÉÂº∑Ë®òÊÜ∂ÂäõË°®ÁèæÁöÑ‰ΩçÁΩÆÔºå‰ª•Âèä 2) Ë™øÊü•‰∏ÄÂÄãÂèóÂà∞Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÂïüÁôºÁöÑ„ÄÅÈáùÂ∞çË®òÊÜ∂ÂäõË°®ÁèæËºÉÂº±ÂçÄÂüüÁöÑËºïÈáèÁ¥öÂ§ñÈÉ®Áü•Ë≠òÂ∫´„ÄÇÊ≠§Â§ñÔºåGraphInsight Êé¢Á¥¢Â∞áÈÄôÂÖ©ÂÄãÁ≠ñÁï•Êï¥ÂêàÂà∞ LLM ‰ª£ÁêÜÁ®ãÂ∫è‰∏≠Ôºå‰ª•ËôïÁêÜÈúÄË¶ÅÂ§öÊ≠•È©üÊé®ÁêÜÁöÑË§áÂêàÂúñÂΩ¢‰ªªÂãô„ÄÇÂú®ÂÖ∑ÊúâÂª£Ê≥õË©ïÈáè‰ªªÂãôÁöÑÂü∫Ê∫ñ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶Ë≠âÁ†îÁ©∂È°ØÁ§∫ÔºåGraphInsight Âú®ÁêÜËß£ÂêÑÁ®ÆÂ§ßÂ∞èÁöÑÂúñÂΩ¢ÁµêÊßãÊñπÈù¢ÔºåÊòéÈ°ØÂÑ™ÊñºÊâÄÊúâÂÖ∂‰ªñÂúñÂΩ¢ÊèèËø∞ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊèêÁ§∫ÊäÄÂ∑ßÂíåÈáçÊñ∞ÊéíÂ∫èÁ≠ñÁï•Ôºâ„ÄÇ

##### **Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**
2409.03155v1 by Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui

Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÁº∫‰πèÁõ∏ÈóúÁü•Ë≠òÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØËÉΩÊúÉÁî¢ÁîüÂπªË¶∫„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåÁü•Ë≠òÂúñË≠úÂåÖÂê´Âª£Ê≥õÁöÑÂ§öÈáçÈóú‰øÇÁµêÊßãÔºåÂÑ≤Â≠òÂ§ßÈáèÁ¨¶Ëôü‰∫ãÂØ¶„ÄÇÂõ†Ê≠§ÔºåÂ∞á LLM ËàáÁü•Ë≠òÂúñË≠úÊï¥ÂêàÂ∑≤Âª£Ê≥õÊé¢Ë®éÔºåÂÖ∂‰∏≠Áü•Ë≠òÂúñË≠úÂïèÈ°åËß£Á≠î (KGQA) ÊàêÁÇ∫Êï¥ÂêàÁöÑÈáçË¶ÅË©¶ÈáëÁü≥„ÄÇÊ≠§‰ªªÂãôË¶ÅÊ±Ç LLM ÈÄèÈÅéÂæûÁü•Ë≠òÂúñË≠ú‰∏≠Êì∑ÂèñÁõ∏Èóú‰∏âÂÖÉÁµÑ‰æÜÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈù¢Ëá®ÂÖ©È†ÖÈáçÂ§ßÊåëÊà∞Ôºö\textit{ÈÅéÈï∑ÁöÑÊé®ÁêÜË∑ØÂæëÊúÉÂàÜÊï£ÂõûÁ≠îÁî¢Áîü}Ôºå‰ª•Âèä\textit{ÈåØË™§Ê≠£ÂêëÈóú‰øÇÈòªÁ§ôË∑ØÂæëÁ≤æÁÖâ}„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèçË¶Ü‰∫íÂãïÁöÑ KGQA Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ÁöÑ‰∫íÂãïÂ≠∏ÁøíËÉΩÂäõ‰æÜÂü∑Ë°åÊé®ÁêÜÂíåÂúñÂΩ¢ËæØË´ñ (DoG)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåDoG Êé°Áî®Â≠êÂúñËÅöÁÑ¶Ê©üÂà∂ÔºåÂÖÅË®± LLM Âú®ÊØèÂÄãÊé®ÁêÜÊ≠•È©üÂæåÂü∑Ë°åÁ≠îÊ°àÂòóË©¶ÔºåÂæûËÄåÊ∏õËºïÂÜóÈï∑Êé®ÁêÜË∑ØÂæëÁöÑÂΩ±Èüø„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåDoG Âà©Áî®Â§öËßíËâ≤ËæØË´ñÂ∞èÁµÑÈÄêÊº∏Á∞°ÂåñË§áÈõúÂïèÈ°åÔºåÊ∏õÂ∞ëÈåØË™§Ê≠£ÂêëÈóú‰øÇÁöÑÂΩ±Èüø„ÄÇÈÄôÁ®ÆËæØË´ñÊ©üÂà∂Á¢∫‰øù‰∫ÜÊé®ÁêÜÈÅéÁ®ãÁöÑÂèØÈù†ÊÄß„ÄÇÂú®‰∫îÂÄãÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÂíåÂÑ™Ë∂äÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåDoG Âú® WebQuestions Âíå GrailQA ‰∏äÁöÑÊ∫ñÁ¢∫Â∫¶ÂàÜÂà•ÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï ToG È´òÂá∫ 23.7% Âíå 9.1%„ÄÇÊ≠§Â§ñÔºåÂú®‰∏äËø∞Êï∏ÊìöÈõÜ‰∏äËàáÂêÑÁ®Æ LLM ÁöÑÊï¥ÂêàÂØ¶È©óÁ™ÅÈ°Ø‰∫Ü DoG ÁöÑÈùàÊ¥ªÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®\url{https://github.com/reml-group/DoG}ÂèñÂæó„ÄÇ</paragraph>

##### **Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**
2409.02481v1 by Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja

Effective question classification is crucial for AI-driven educational tools,
enabling adaptive learning systems to categorize questions by skill area,
difficulty level, and competence. This classification not only supports
educational diagnostics and analytics but also enhances complex tasks like
information retrieval and question answering by associating questions with
relevant categories. Traditional methods, often based on word embeddings and
conventional classifiers, struggle to capture the nuanced relationships in
natural language, leading to suboptimal performance. To address this, we
propose a novel approach leveraging graph convolutional networks (GCNs), named
Phrase Question-Graph Convolutional Network (PQ-GCN) to better model the
inherent structure of questions. By representing questions as graphs -- where
nodes signify words or phrases and edges denote syntactic or semantic
relationships -- our method allows GCNs to learn from the interconnected nature
of language more effectively. Additionally, we explore the incorporation of
phrase-based features to enhance classification accuracy, especially in
low-resource settings. Our findings demonstrate that GCNs, augmented with these
features, offer a promising solution for more accurate and context-aware
question classification, bridging the gap between graph neural network research
and practical educational applications.

ÊëòË¶ÅÔºöÊúâÊïàÁöÑÂïèÈ°åÂàÜÈ°ûÂ∞çÊñº AI È©ÖÂãïÁöÑÊïôËÇ≤Â∑•ÂÖ∑Ëá≥ÈóúÈáçË¶ÅÔºå
ËÆìÈÅ©ÊáâÊÄßÂ≠∏ÁøíÁ≥ªÁµ±ËÉΩ‰æùÊìöÊäÄËÉΩÈ†òÂüü„ÄÅ
Èõ£Â∫¶Á≠âÁ¥öÂíåËÉΩÂäõÂ∞çÂïèÈ°åÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÁ®ÆÂàÜÈ°û‰∏çÂÉÖÊîØÊè¥
ÊïôËÇ≤Ë®∫Êñ∑ÂíåÂàÜÊûêÔºåÈÇÑËÉΩÈÄèÈÅéÂ∞áÂïèÈ°åËàá
Áõ∏ÈóúÈ°ûÂà•ÈóúËÅØËµ∑‰æÜÔºåÂ¢ûÂº∑Ë≥áË®äÊ™¢Á¥¢ÂíåÂïèÈ°åËß£Á≠îÁ≠âË§áÈõú‰ªªÂãô„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏Âª∫Á´ãÂú®Ë©ûÂµåÂÖ•Âíå
ÂÇ≥Áµ±ÂàÜÈ°ûÂô®‰∏äÔºåÈõ£‰ª•ÊçïÊçâËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÁ¥∞ÂæÆÈóú‰øÇÔºåÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄë
ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑Ø (GCN)ÔºåÁ®±ÁÇ∫
Phrase Question-Graph Convolutional Network (PQ-GCN) ‰æÜÊõ¥Â•ΩÂú∞Âª∫Ê®°ÂïèÈ°åÁöÑÂÖßÂú®ÁµêÊßã„ÄÇÈÄèÈÅéÂ∞áÂïèÈ°åË°®Á§∫ÁÇ∫ÂúñÂΩ¢‚Äî‚ÄîÂÖ∂‰∏≠
ÁØÄÈªûË°®Á§∫Ë©ûÊàñË©ûÁµÑÔºåÈÇäÁ∑£Ë°®Á§∫Ë™ûÊ≥ïÊàñË™ûÁæ©Èóú‰øÇ‚Äî‚ÄîÊàëÂÄëÁöÑÊ®°ÂûãÂÖÅË®± GCN Êõ¥ÊúâÊïàÂú∞ÂæûË™ûË®ÄÁöÑÁõ∏‰∫íÈÄ£ÁµêÊÄßË≥™‰∏≠Â≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÊï¥Âêà
Âü∫ÊñºË©ûÁµÑÁöÑÁâπÂæµ‰ª•Â¢ûÂº∑ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®
‰ΩéË≥áÊ∫êË®≠ÂÆö‰∏≠„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåGCN Âú®ÈÄô‰∫õ
ÁâπÂæµÁöÑÂ¢ûÂº∑‰∏ãÔºåÁÇ∫Êõ¥Ê∫ñÁ¢∫‰∏îÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ËÉΩÂäõÁöÑÂïèÈ°åÂàÜÈ°ûÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁ∏ÆÂ∞è‰∫ÜÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁ†îÁ©∂
ËàáÂØ¶ÈöõÊïôËÇ≤ÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Multi-modal Situated Reasoning in 3D Scenes**
2409.02389v1 by Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang

Situation awareness is essential for understanding and reasoning about 3D
scenes in embodied AI agents. However, existing datasets and benchmarks for
situated understanding are limited in data modality, diversity, scale, and task
scope. To address these limitations, we propose Multi-modal Situated Question
Answering (MSQA), a large-scale multi-modal situated reasoning dataset,
scalably collected leveraging 3D scene graphs and vision-language models (VLMs)
across a diverse range of real-world 3D scenes. MSQA includes 251K situated
question-answering pairs across 9 distinct question categories, covering
complex scenarios within 3D scenes. We introduce a novel interleaved
multi-modal input setting in our benchmark to provide text, image, and point
cloud for situation and question description, resolving ambiguity in previous
single-modality convention (e.g., text). Additionally, we devise the
Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'
situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN
highlight the limitations of existing vision-language models and underscore the
importance of handling multi-modal interleaved inputs and situation modeling.
Experiments on data scaling and cross-domain transfer further demonstrate the
efficacy of leveraging MSQA as a pre-training dataset for developing more
powerful situated reasoning models.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÊÑüÁü•Â∞çÊñºÁêÜËß£ÂíåÊé®ÁêÜÂÖ∑Ë∫´ AI ‰ª£ÁêÜ‰∏≠ÁöÑ 3D Â†¥ÊôØËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË≥áÊñôÈõÜÂíåÂü∫Ê∫ñÂú®Ë≥áÊñôÊ®°ÊÖã„ÄÅÂ§öÊ®£ÊÄß„ÄÅË¶èÊ®°Âíå‰ªªÂãôÁØÑÂúçÊñπÈù¢Â∞çÊñºÊÉÖÂ¢ÉÁêÜËß£‰æÜË™™ÊòØÊúâÈôêÁöÑ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÂïèÁ≠î (MSQA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§ßÂûãÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÊé®ÁêÜË≥áÊñôÈõÜÔºåÂèØÈÄèÈÅéÂà©Áî® 3D Â†¥ÊôØÂúñÂíåË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Âú®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïå 3D Â†¥ÊôØ‰∏≠ÈÄ≤Ë°åÂèØÊì¥ÂÖÖÊî∂ÈõÜ„ÄÇMSQA ÂåÖÂê´ 251K ÂÄãÊÉÖÂ¢ÉÂïèÁ≠îÂ∞çÔºåÊ∂µËìã 9 ÂÄã‰∏çÂêåÁöÑÂïèÈ°åÈ°ûÂà•ÔºåÊ∂µËìã 3D Â†¥ÊôØ‰∏≠ÁöÑË§áÈõúÂ†¥ÊôØ„ÄÇÊàëÂÄëÂú®Âü∫Ê∫ñ‰∏≠ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ‰∫§ÈåØÂ§öÊ®°ÊÖãËº∏ÂÖ•Ë®≠ÂÆöÔºå‰ª•Êèê‰æõÊñáÂ≠ó„ÄÅÂΩ±ÂÉèÂíåÈªûÈõ≤ÔºåÁî®ÊñºÊÉÖÂ¢ÉÂíåÂïèÈ°åÊèèËø∞ÔºåËß£Ê±∫‰ª•ÂâçÂñÆ‰∏ÄÊ®°ÊÖãÊÖ£‰æãÔºà‰æãÂ¶ÇÊñáÂ≠óÔºâ‰∏≠ÁöÑÊ≠ßÁæ©„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢É‰∏ã‰∏ÄÊ≠•Â∞éËà™ (MSNN) Âü∫Ê∫ñÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãÁöÑÂ∞éËà™ÊÉÖÂ¢ÉÊé®ÁêÜ„ÄÇMSQA Âíå MSNN ÁöÑÁ∂úÂêàË©ï‰º∞Á™ÅÈ°Ø‰∫ÜÁèæÊúâË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÈôêÂà∂Ôºå‰∏¶Âº∑Ë™ø‰∫ÜËôïÁêÜÂ§öÊ®°ÊÖã‰∫§ÈåØËº∏ÂÖ•ÂíåÊÉÖÂ¢ÉÂª∫Ê®°ÁöÑÈáçË¶ÅÊÄß„ÄÇË≥áÊñôÊì¥ÂÖÖÂíåË∑®È†òÂüüËΩâÁßªÁöÑÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Ë≠âÊòé‰∫ÜÂà©Áî® MSQA ‰ΩúÁÇ∫È†êË®ìÁ∑¥Ë≥áÊñôÈõÜ‰æÜÈñãÁôºÊõ¥Âº∑Â§ßÁöÑÊÉÖÂ¢ÉÊé®ÁêÜÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Grounding Language Models in Autonomous Loco-manipulation Tasks**
2409.01326v1 by Jin Wang, Nikos Tsagarakis

Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.

ÊëòË¶ÅÔºöÂÖ∑ÊúâË°åÁÇ∫Ëá™‰∏ªÊ¨äÁöÑ‰∫∫ÂΩ¢Ê©üÂô®‰∫∫‰∏ÄÁõ¥Ë¢´Ë¶ñÁÇ∫ÊàëÂÄëÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁêÜÊÉ≥ÁöÑÂêà‰ΩúËÄÖÔºå‰πüÊòØÂÖ∑È´îÊô∫ËÉΩÁöÑÊúâÂ∏åÊúõÁöÑ‰ª£Ë°®„ÄÇËàáÂõ∫ÂÆöÂºèÊ©üÂô®ÊâãËáÇÁõ∏ÊØîÔºå‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Êèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÊìç‰ΩúÁ©∫ÈñìÔºåÂêåÊôÇÈ°ØËëóÂ¢ûÂä†‰∫ÜÊéßÂà∂ÂíåË¶èÂäÉÁöÑÈõ£Â∫¶„ÄÇÂÑòÁÆ°ÊúùËëóÈÄöÁî®‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Âø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∏Á†îÁ©∂‰ªçÁÑ∂ÈõÜ‰∏≠Âú®ÈÅãÂãïËÉΩÂäõ‰∏äÔºåÂæàÂ∞ëÁ†îÁ©∂ÂÖ®Ë∫´ÂçîË™øÂíå‰ªªÂãôË¶èÂäÉÔºåÂæûËÄåÈôêÂà∂‰∫ÜÂ±ïÁ§∫Ê∂âÂèäÁßªÂãïÂíåÊìç‰ΩúÁöÑÈï∑Êúü‰ªªÂãôÁöÑÊΩõÂäõÔºåÂêåÊôÇÈÇÑËÉΩÊé•ÂèóÈñãÊîæÂºèÂè£È†≠Êåá‰ª§„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂ÂèØ‰ª•Ê†πÊìö‰∏çÂêåÂ†¥ÊôØ‰∏≠ÁöÑ‰ªªÂãôÂ≠∏Áøí„ÄÅÈÅ∏ÊìáÂíåË¶èÂäÉË°åÁÇ∫„ÄÇÊàëÂÄëÂ∞áÂº∑ÂåñÂ≠∏Áøí (RL) ËàáÂÖ®Ë∫´ÂÑ™ÂåñÁõ∏ÁµêÂêàÔºå‰ª•ÁîüÊàêÊ©üÂô®‰∫∫Âãï‰Ωú‰∏¶Â∞áÂÖ∂Â≠òÂÑ≤Âà∞Âãï‰ΩúÂ∫´‰∏≠„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË¶èÂäÉÂíåÊé®ÁêÜÂäüËÉΩÔºåÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂàÜÂ±§‰ªªÂãôÂúñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÈÅãÂãïÂéüË™ûÔºå‰ª•Ê©ãÊé•‰ΩéÁ¥öÂü∑Ë°åÂíåÈ´òÁ¥öË¶èÂäÉ„ÄÇÂú®Ê®°Êì¨Âíå‰ΩøÁî® CENTAURO Ê©üÂô®‰∫∫ÁöÑÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÂØ¶È©óË°®ÊòéÔºåÂü∫ÊñºË™ûË®ÄÊ®°ÂûãÁöÑË¶èÂäÉÂô®ÂèØ‰ª•ÊúâÊïàÈÅ©ÊáâÊñ∞ÁöÑÈÅãÂãïÊìç‰Ωú‰ªªÂãôÔºåË≠âÊòé‰∫ÜÂú®ÈùûÁµêÊßãÂåñÂ†¥ÊôØ‰∏≠ÂæûËá™Áî±ÊñáÊú¨ÂëΩ‰ª§‰∏≠Áç≤ÂæóÁöÑÈ´òÂ∫¶Ëá™‰∏ªÊÄß„ÄÇ

##### **LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**
2409.01145v1 by Haoran Yang, Xiangyu Zhao, Sirui Huang, Qing Li, Guandong Xu

Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised
graph learning that has attracted attention across various application
scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet
to be explored. Because conventional augmentation techniques like feature
embedding masking cannot directly process textual attributes on TAGs. A naive
strategy for applying GCL to TAGs is to encode the textual attributes into
feature embeddings via a language model and then feed the embeddings into the
following GCL module for processing. Such a strategy faces three key
challenges: I) failure to avoid information loss, II) semantic loss during the
text encoding phase, and III) implicit augmentation constraints that lead to
uncontrollable and incomprehensible results. In this paper, we propose a novel
GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to
produce textual augmentations and LLMs' powerful natural language processing
(NLP) abilities to address the three limitations aforementioned to pave the way
for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG
datasets illustrate the superiority of the proposed LATEX-GCL method. The
source codes and datasets are released to ease the reproducibility, which can
be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.

ÊëòË¶ÅÔºöÂúñÂΩ¢Â∞çÊØîÂ≠∏Áøí (GCL) ÊòØËá™Áõ£Áù£ÂúñÂΩ¢Â≠∏ÁøíÁöÑÂº∑Â§ßÁØÑ‰æãÔºåÂ∑≤Âú®ÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØ‰∏≠ÂºïËµ∑ÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåGCL Â∞çÊñºÂú®ÊñáÊú¨Ë®ªËß£ÂúñÂΩ¢ (TAG) ‰∏äÂ≠∏ÁøíÂ∞öÊú™Ë¢´Êé¢Ë®é„ÄÇÂõ†ÁÇ∫ÁâπÂæµÂµåÂÖ•ÈÅÆÁΩ©Á≠âÂÇ≥Áµ±Êì¥ÂÖÖÊäÄË°ìÁÑ°Ê≥ïÁõ¥Êé•ËôïÁêÜ TAG ‰∏äÁöÑÊñáÊú¨Â±¨ÊÄß„ÄÇÂ∞á GCL ÊáâÁî®Êñº TAG ÁöÑ‰∏ÄÁ®ÆÂ§©ÁúüÁ≠ñÁï•ÊòØÈÄöÈÅéË™ûË®ÄÊ®°ÂûãÂ∞áÊñáÊú¨Â±¨ÊÄßÁ∑®Á¢ºÂà∞ÁâπÂæµÂµåÂÖ•‰∏≠ÔºåÁÑ∂ÂæåÂ∞áÂµåÂÖ•Ëº∏ÂÖ•ÂæåÁ∫åÁöÑ GCL Ê®°ÁµÑÈÄ≤Ë°åËôïÁêÜ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•Èù¢Ëá®‰∏âÂÄãÈóúÈçµÊåëÊà∞ÔºöI) ÁÑ°Ê≥ïÈÅøÂÖçË≥áË®äÈÅ∫Â§±ÔºåII) Âú®ÊñáÊú¨Á∑®Á¢ºÈöéÊÆµÁôºÁîüË™ûÁæ©ÈÅ∫Â§±Ôºå‰ª•Âèä III) Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂‰∏îÈõ£‰ª•ÁêÜËß£ÁµêÊûúÁöÑÈö±ÂºèÊì¥ÂÖÖÁ¥ÑÊùü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ LATEX-GCL ÁöÑÊñ∞Á©é GCL Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÊñáÊú¨Êì¥ÂÖÖÔºå‰ª•Âèä LLM Âº∑Â§ßÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ËÉΩÂäõ‰æÜËß£Ê±∫‰∏äËø∞‰∏âÂÄãÈôêÂà∂ÔºåÁÇ∫Â∞á GCL ÊáâÁî®Êñº TAG ‰ªªÂãôÈã™Âπ≥ÈÅìË∑Ø„ÄÇÂú®ÂõõÂÄãÈ´òÂìÅË≥™ TAG Ë≥áÊñôÈõÜ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óË™™Êòé‰∫ÜÊâÄÊèêÂá∫ÁöÑ LATEX-GCL ÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄß„ÄÇÂéüÂßãÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤ÁôºÂ∏É‰ª•Á∞°ÂåñÂèØÈáçË£ΩÊÄßÔºåÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂ≠òÂèñÔºöhttps://anonymous.4open.science/r/LATEX-GCL-0712„ÄÇ

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÔºåÂç≥‰ΩøÁ∂ìÈÅéÂæÆË™øÁöÑÊ®°Âûã‰πüÂÆπÊòìÁî¢ÁîüÂπªË¶∫„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÊõ¥Â§öÂèØÈù†ÁöÑÊ®°Âûã‰æÜÁ¥çÂÖ•Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊµÅÁ®ã 4StepFocusÔºåÁâπÂà•ÊòØÈ†êËôïÁêÜÊ≠•È©üÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM ÁöÑÁ≠îÊ°à„ÄÇÈÄôÊòØÈÄèÈÅéÊèê‰æõÂèóÂºïÂ∞éÁöÑÂ§ñÈÉ®Áü•Ë≠òÂ≠òÂèñÔºåÂà©Áî®Ê®°ÂûãËá™Ë°åÊì∑ÂèñÈóúËÅØÊÄßËÑàÁµ°ÂíåÈÄ≤Ë°åÂü∫Êú¨Êé®ÁêÜÁöÑËÉΩÂäõ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÂú®ÂçäÁµêÊßãÂåñÁü•Ë≠òÂ∫´‰∏≠ÈÄ≤Ë°åÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑÊêúÂ∞ãÔºå‰ª•Áõ¥Êé•‰∏îÂèØËøΩËπ§ÁöÑÊñπÂºèÁ∏ÆÂ∞èÊΩõÂú®Ê≠£Á¢∫Á≠îÊ°àÁöÑÁØÑÂúçÔºåÁÑ∂ÂæåÂÜçÂàáÊèõÂà∞ÊΩõÂú®Ë°®ÂæµÔºåÊ†πÊìöÈùûÁµêÊßãÂåñË≥áÊñôÂ∞çÈÄô‰∫õÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêç„ÄÇÈÄôËàáÁ¥îÁ≤πÂü∫ÊñºÊΩõÂú®Ë°®ÂæµÁöÑÁõ∏ÈóúÊñπÊ≥ïÊúâÊâÄÂçÄÂà•„ÄÇ4StepFocus ÂåÖÂê´‰ª•‰∏ãÊ≠•È©üÔºö1) Áî± LLM ÈÄ≤Ë°å‰∏âÂÖÉÁµÑÁî¢Áîü‰ª•Êì∑ÂèñÈóúËÅØË≥áÊñôÔºå2) Âú®ÈÄô‰∫õ‰∏âÂÖÉÁµÑ‰∏≠ÊõøÊèõËÆäÊï∏Ôºå‰ª•Êé°Áî®Áü•Ë≠òÂúñË°®Á∏ÆÂ∞èÁ≠îÊ°àÂÄôÈÅ∏ÁØÑÂúçÔºå3) ‰ΩøÁî®Ê∂âÂèäÈóúËÅØÈùûÁµêÊßãÂåñË≥áÊñôÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÂ∞ãÂ∞çÂâ©È§òÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂ∫èÔºå4) Áî± LLM ÈáçÊñ∞Â∞çÊúÄ‰Ω≥ÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêçÔºå‰∏¶Êèê‰æõËÉåÊôØË≥áÊñô„ÄÇÂú®ÈÜ´ÁôÇ„ÄÅÁî¢ÂìÅÊé®Ëñ¶ÂíåÂ≠∏Ë°ìË´ñÊñáÊêúÂ∞ãÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫ÂØ¶ÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊì¥ÂÖÖ„ÄÇÂÆÉ‰∏çÂÉÖÂ¢ûÂä†‰∫Ü‰æÜËá™Ë≥áË®äÊ™¢Á¥¢ÁöÑÁõ∏ÂÖ≥ÂèØËøΩËπ§ËÉåÊôØË≥áË®äÔºåËÄå‰∏îËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºå‰πüÂ§ßÂπÖÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈÆÆÂ∞ëÊé¢Á¥¢ÁöÑÊñπÂêëÔºåÂõ†Ê≠§Êèê‰æõ‰∫ÜÂª£Ê≥õÁöÑÊú™‰æÜÂ∑•‰ΩúÊ©üÊúÉ„ÄÇ‰ΩøÁî®ÁöÑÂéüÂßãÁ¢ºÂèØÂú® https://github.com/kramerlab/4StepFocus ÂèñÂæó„ÄÇ

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂ∑•Á®ãÂíåÂ§öË™ûË®ÄË™ûÁæ©Êé®ÁêÜÊäÄË°ìÁöÑÊú¨‰ΩìË®≠Ë®àÔºåÁî®ÊñºÂª∫Á´ã‰∏ÄÂÄãËá™ÂãïÂåñÁ≥ªÁµ±Ôºå‰ª•Áü•Ë≠òÂúñË≠úÁöÑÂΩ¢ÂºèÂê∏Êî∂Âç∞Â∫¶ÊñôÁêÜÁöÑÁÉπÈ£™Ë≥áË®ä„ÄÇÈáçÈªûÂú®ÊñºË®≠Ë®àÊô∫ÊÖßÊñπÊ≥ïÔºå‰ª•Êé®Â∞éÊú¨‰ΩìË®≠Ë®àÔºå‰∏¶ÂÖ®Èù¢Êì∑ÂèñÈóúÊñºÈ£üÁâ©„ÄÅÈ£üË≠ú„ÄÅÈ£üÊùê„ÄÅÁÉπÈ£™ÁâπÊÄßÔºå‰ª•ÂèäÊúÄÈáçË¶ÅÁöÑÁáüÈ§äÁöÑÁü•Ë≠òÔºå‰∏¶Êì¥Â§ßË¶èÊ®°„ÄÇÊàëÂÄëÂú®ÈÄôÂÄãÁ†îË®éÊúÉË´ñÊñá‰∏≠‰ªãÁ¥π‰∫ÜÊàëÂÄëÊ≠£Âú®ÈÄ≤Ë°åÁöÑÂ∑•‰ΩúÔºåË©≥Á¥∞ÊèèËø∞‰∫ÜÊï¥ÁêÜÂç∞Â∫¶ÊñôÁêÜÁü•Ë≠òÁõ∏ÈóúÁöÑÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊàëÂÄëÁöÑÈ´òÈöéÊú¨‰ΩìË®≠Ë®à„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÆÉ‰ΩøÁî® AI„ÄÅLLM ÂíåË™ûË®ÄÊäÄË°ìÔºåÂæûÂÖ¨ÂÖ±È†òÂüüÁöÑÈ£üË≠úÈÉ®ËêΩÊ†ºÁ∂≤Á´ô‰∏≠Êï¥ÁêÜË≥áË®äÔºå‰ª•Âª∫Á´ãÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑÁü•Ë≠òÊï¥ÁêÜÊñπÊ≥ïÊòØÈÄöÁî®ÁöÑÔºåÂèØ‰ª•Ë§áË£ΩÂà∞‰ªª‰ΩïÈ†òÂüü„ÄÇË®≠Ë®àËàáÊáâÁî®ÁÑ°ÈóúÔºåÂèØÁî®Êñº AI È©ÖÂãïÁöÑÊô∫ÊÖßÂàÜÊûê„ÄÅÂª∫Á´ãÂÄã‰∫∫ÂåñÊï∏‰ΩçÂÅ•Â∫∑Êé®Ëñ¶Á≥ªÁµ±Ôºå‰ª•Âèä‰ΩøÁî®‰ΩøÁî®ËÄÖË≥áË®ä„ÄÅÈ£üÁâ©ÁîüÁâ©ÂåñÂ≠∏„ÄÅÂú∞ÁêÜË≥áË®ä„ÄÅËæ≤Ê•≠Ë≥áË®äÁ≠âËÑàÁµ°Ë≥áË®äÔºå‰æÜË£úÂÖÖÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇ

##### **Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**
2409.00727v1 by Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuanhui Yang, Yuanyuan Zhu, Chuang Hu, Bo Du, Jiawei Jiang

Text-attributed graph (TAG) is an important type of graph structured data
with text descriptions for each node. Few- and zero-shot node classification on
TAGs have many applications in fields such as academia and social networks.
However, the two tasks are challenging due to the lack of supervision signals,
and existing methods only use the contrastive loss to align graph-based node
embedding and language-based text embedding. In this paper, we propose Hound to
improve accuracy by introducing more supervision signals, and the core idea is
to go beyond the node-text pairs that come with data. Specifically, we design
three augmentation techniques, i.e., node perturbation, text matching, and
semantics negation to provide more reference nodes for each text and vice
versa. Node perturbation adds/drops edges to produce diversified node
embeddings that can be matched with a text. Text matching retrieves texts with
similar embeddings to match with a node. Semantics negation uses a negative
prompt to construct a negative text with the opposite semantics, which is
contrasted with the original node and text. We evaluate Hound on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that Hound
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

ÊëòË¶ÅÔºöÊñáÂ≠óÂ±ûÊÄßÂúñ (TAG) ÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑÂúñÂΩ¢ÁµêÊßãÂåñË≥áÊñôÈ°ûÂûãÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÈÉΩÊúâÊñáÂ≠óÊèèËø∞„ÄÇTAG ‰∏äÁöÑÂ∞ëÊ®£Êú¨ÂíåÈõ∂Ê®£Êú¨ÁØÄÈªûÂàÜÈ°ûÂú®Â≠∏Ë°ìÁïåÂíåÁ§æ‰∫§Á∂≤Ë∑ØÁ≠âÈ†òÂüüÊúâË®±Â§öÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÁõ£Áù£Ë®äËôüÔºåÈÄôÂÖ©ÂÄã‰ªªÂãôÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÁèæÊúâÊñπÊ≥ïÂÉÖ‰ΩøÁî®Â∞çÊØîÊêçÂ§±‰æÜÂ∞çÈΩäÂü∫ÊñºÂúñÂΩ¢ÁØÄÈªûÁöÑÂµåÂÖ•ÂíåÂü∫ÊñºË™ûË®ÄÁöÑÊñáÂ≠óÂµåÂÖ•„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Hound ‰æÜÈÄèÈÅéÂºïÂÖ•Êõ¥Â§öÁõ£Áù£Ë®äËôü‰æÜÊîπÂñÑÊ∫ñÁ¢∫Â∫¶ÔºåÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØË∂ÖË∂äË≥áÊñô‰∏≠ÈôÑÂ∏∂ÁöÑÁØÄÈªûÊñáÂ≠óÂ∞ç„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏âÁ®ÆÊì¥ÂÖÖÊäÄË°ìÔºåÂç≥ÁØÄÈªûÊìæÂãï„ÄÅÊñáÂ≠óÈÖçÂ∞çÂíåË™ûÁæ©Âê¶ÂÆöÔºåÁÇ∫ÊØèÂÄãÊñáÂ≠óÊèê‰æõÊõ¥Â§öÂèÉËÄÉÁØÄÈªûÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÁØÄÈªûÊìæÂãïÊñ∞Â¢û/Âà™Èô§ÈÇäÁ∑£‰ª•Áî¢ÁîüÂèØ‰ª•ËàáÊñáÂ≠óÈÖçÂ∞çÁöÑÂ§öÊ®£ÂåñÁØÄÈªûÂµåÂÖ•„ÄÇÊñáÂ≠óÈÖçÂ∞çÊì∑ÂèñÂÖ∑ÊúâÈ°û‰ººÂµåÂÖ•ÁöÑÊñáÂ≠ó‰ª•ËàáÁØÄÈªûÈÖçÂ∞ç„ÄÇË™ûÁæ©Âê¶ÂÆö‰ΩøÁî®Ë≤†Èù¢ÊèêÁ§∫‰æÜÂª∫ÊßãÂÖ∑ÊúâÁõ∏ÂèçË™ûÁæ©ÁöÑË≤†Èù¢ÊñáÂ≠óÔºåËàáÂéüÂßãÁØÄÈªûÂíåÊñáÂ≠óÂΩ¢ÊàêÂ∞çÊØî„ÄÇÊàëÂÄëÂú® 5 ÂÄãË≥áÊñôÈõÜ‰∏äË©ï‰º∞ HoundÔºå‰∏¶Ëàá 13 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÂü∫Á∑öÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúË°®ÊòéÔºåHound Âú®ÊâÄÊúâÂü∫Á∑ö‰∏äÂßãÁµÇË°®ÁèæÂÑ™Áï∞ÔºåÂÖ∂Ê∫ñÁ¢∫Â∫¶ÈÄöÂ∏∏ÊØîÊïàËÉΩÊúÄ‰Ω≥ÁöÑÂü∫Á∑öÊèêÈ´ò‰∫Ü 5% ‰ª•‰∏ä„ÄÇ

##### **WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**
2409.00331v1 by Oktie Hassanzadeh

Recently, there has been an increasing interest in the construction of
general-domain and domain-specific causal knowledge graphs. Such knowledge
graphs enable reasoning for causal analysis and event prediction, and so have a
range of applications across different domains. While great progress has been
made toward automated construction of causal knowledge graphs, the evaluation
of such solutions has either focused on low-level tasks (e.g., cause-effect
phrase extraction) or on ad hoc evaluation data and small manual evaluations.
In this paper, we present a corpus, task, and evaluation framework for causal
knowledge graph construction. Our corpus consists of Wikipedia articles for a
collection of event-related concepts in Wikidata. The task is to extract causal
relations between event concepts from the corpus. The evaluation is performed
in part using existing causal relations in Wikidata to measure recall, and in
part using Large Language Models to avoid the need for manual or crowd-sourced
evaluation. We evaluate a pipeline for causal knowledge graph construction that
relies on neural models for question answering and concept linking, and show
how the corpus and the evaluation framework allow us to effectively find the
right model for each task. The corpus and the evaluation framework are publicly
available.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºå‰∫∫ÂÄëÂ∞çÈÄöÁî®È†òÂüüÂíåÁâπÂÆöÈ†òÂüüÂõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãË∂ä‰æÜË∂äÊÑüËààË∂£„ÄÇÊ≠§È°ûÁü•Ë≠òÂúñË≠úËÉΩÂ§†Êé®ÁêÜÂõ†ÊûúÂàÜÊûêÂíå‰∫ã‰ª∂È†êÊ∏¨ÔºåÂõ†Ê≠§Âú®‰∏çÂêåÈ†òÂüü‰∏≠ÊúâÂª£Ê≥õÁöÑÊáâÁî®„ÄÇÈõñÁÑ∂Âú®Âõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑËá™ÂãïÂª∫ÊßãÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜÊ≠§È°ûËß£Ê±∫ÊñπÊ°àÁöÑË©ï‰º∞Ë¶ÅÂòõËëóÈáçÊñº‰ΩéÈöé‰ªªÂãôÔºà‰æãÂ¶ÇÂõ†ÊûúÈóú‰øÇÁü≠Ë™ûÊì∑ÂèñÔºâÔºåË¶ÅÂòõËëóÈáçÊñºËá®ÊôÇË©ï‰º∞Ë≥áÊñôÂíåÂ∞èÂûãÊâãÂãïË©ï‰º∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË™ûÊñôÂ∫´„ÄÅ‰ªªÂãôÂíåÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãË©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑË™ûÊñôÂ∫´ÂåÖÂê´Á∂≠Âü∫ÁôæÁßëÊñáÁ´†ÔºåÂÖ∂‰∏≠ÂåÖÂê´ Wikidata ‰∏≠‰∏ÄÁ≥ªÂàó‰∫ã‰ª∂Áõ∏ÈóúÊ¶ÇÂøµ„ÄÇ‰ªªÂãôÊòØÂæûË™ûÊñôÂ∫´‰∏≠Êì∑Âèñ‰∫ã‰ª∂Ê¶ÇÂøµ‰πãÈñìÁöÑÂõ†ÊûúÈóú‰øÇ„ÄÇË©ï‰º∞ÈÉ®ÂàÜ‰ΩøÁî® Wikidata ‰∏≠ÁèæÊúâÁöÑÂõ†ÊûúÈóú‰øÇ‰æÜË°°ÈáèÂè¨ÂõûÁéáÔºåÈÉ®ÂàÜ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÈÅøÂÖçÊâãÂãïÊàñÁæ§ÁúæÂ§ñÂåÖË©ï‰º∞ÁöÑÈúÄË¶Å„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÂÄãÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãÁÆ°ÈÅìÔºåË©≤ÁÆ°ÈÅì‰æùË≥¥ÊñºÁî®ÊñºÂïèÁ≠îÂíåÊ¶ÇÂøµÈÄ£ÁµêÁöÑÁ•ûÁ∂ìÊ®°ÂûãÔºå‰∏¶Â±ïÁ§∫‰∫ÜË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂ¶Ç‰ΩïËÆìÊàëÂÄëÊúâÊïàÂú∞ÁÇ∫ÊØèÂÄã‰ªªÂãôÊâæÂà∞ÂêàÈÅ©ÁöÑÊ®°Âûã„ÄÇË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂÖ¨ÈñãÊèê‰æõ„ÄÇ</paragraph>

##### **HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**
2409.09046v1 by Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven

While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈõñÁÑ∂Âú®ÊñáÂ≠óÁî¢ÁîüÂíåÂïèÁ≠îÊñπÈù¢Ë°®ÁèæÂÑ™Áï∞Ôºå‰ΩÜÂÖ∂Âú® AI Ê≥ïÂæãÂíåÊîøÁ≠ñ‰∏≠ÁöÑÊïàËÉΩÂçªÂèóÂà∞ÈÅéÊôÇÁü•Ë≠ò„ÄÅÂπªË¶∫‰ª•ÂèäÂú®Ë§áÈõúËÑàÁµ°‰∏≠Êé®ÁêÜ‰∏çË∂≥ÁöÑÈôêÂà∂„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Áü•Ë≠ò‰æÜÊîπÂñÑÂõûÊáâÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÂçªÂú®Ê™¢Á¥¢ÈåØË™§„ÄÅËÑàÁµ°Êï¥Âêà‰∏çËâØ‰ª•ÂèäÊàêÊú¨È´òÊòÇÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®Ë©ÆÈáãÂÆöÊÄßÂíåÂÆöÈáèÁöÑ AI Ê≥ïÂæãÊñáÊú¨ÊôÇ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂ∞àÁÇ∫ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÈáèË∫´ÊâìÈÄ†ÁöÑÊ∑∑ÂêàÂèÉÊï∏Ëá™ÈÅ©Êáâ RAG (HyPA-RAG) Á≥ªÁµ±Ôºå‰ª•Á¥êÁ¥ÑÂ∏ÇÂú∞ÊñπÊ≥ïÂæã 144 (LL144) ÁÇ∫‰æã„ÄÇHyPA-RAG ‰ΩøÁî®Êü•Ë©¢Ë§áÈõúÂ∫¶ÂàÜÈ°ûÂô®ÈÄ≤Ë°åËá™ÈÅ©ÊáâÂèÉÊï∏Ë™øÊï¥ÔºåÁµêÂêàÁ®†ÂØÜ„ÄÅÁ®ÄÁñèÂíåÁü•Ë≠òÂúñË°®ÊñπÊ≥ïÁöÑÊ∑∑ÂêàÊ™¢Á¥¢Á≠ñÁï•Ôºå‰ª•ÂèäÂåÖÂê´ÁâπÂÆöÂïèÈ°åÈ°ûÂûãÂíåÊåáÊ®ôÁöÑË©ï‰º∞Êû∂Êßã„ÄÇÈÄèÈÅéÂãïÊÖãË™øÊï¥ÂèÉÊï∏ÔºåHyPA-RAG Â§ßÂπÖÊîπÂñÑ‰∫ÜÊ™¢Á¥¢Ê∫ñÁ¢∫ÊÄßÂíåÂõûÊáâ‰øùÁúüÂ∫¶„ÄÇÂú® LL144 ‰∏äÁöÑÊ∏¨Ë©¶È°ØÁ§∫Âá∫Â¢ûÂº∑ÁöÑÊ≠£Á¢∫ÊÄß„ÄÅÂø†ÂØ¶Â∫¶ÂíåËÑàÁµ°Ê∫ñÁ¢∫Â∫¶ÔºåÊªøË∂≥‰∫ÜÂú®Ë§áÈõú„ÄÅÈ´òÈ¢®Èö™ÁöÑ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÊáâÁî®‰∏≠Â∞çÂèØÈÅ©Êáâ NLP Á≥ªÁµ±ÁöÑÈúÄÊ±Ç„ÄÇ

##### **LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**
2408.16224v2 by Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng

Recent advances in large vision-language models (VLMs) typically employ
vision encoders based on the Vision Transformer (ViT) architecture. The
division of the images into patches by ViT results in a fragmented perception,
thereby hindering the visual understanding capabilities of VLMs. In this paper,
we propose an innovative enhancement to address this limitation by introducing
a Scene Graph Expression (SGE) module in VLMs. This module extracts and
structurally expresses the complex semantic information within images, thereby
improving the foundational perception and understanding abilities of VLMs.
Extensive experiments demonstrate that integrating our SGE module significantly
enhances the VLM's performance in vision-language tasks, indicating its
effectiveness in preserving intricate semantic details and facilitating better
visual understanding.

ÊëòË¶ÅÔºöËøë‰æÜÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÈÄ≤Â±ïÈÄöÂ∏∏Êé°Áî®Âü∫ÊñºË¶ñË¶∫ËΩâÊèõÂô® (ViT) Êû∂ÊßãÁöÑË¶ñË¶∫Á∑®Á¢ºÂô®„ÄÇViT Â∞áÂΩ±ÂÉèÂàÜÂâ≤ÊàêÂçÄÂ°äÊúÉÈÄ†ÊàêÁ†¥Á¢éÁöÑÊÑüÁü•ÔºåÂæûËÄåÈòªÁ§ô VLM ÁöÑË¶ñË¶∫ÁêÜËß£ËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂâµÊñ∞ÁöÑÂ¢ûÂº∑ÂäüËÉΩÔºåÈÄèÈÅéÂú® VLM ‰∏≠ÂºïÂÖ•Â†¥ÊôØÂúñË°®ÈÅî (SGE) Ê®°ÁµÑ‰æÜËß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÊ≠§Ê®°ÁµÑÊúÉËêÉÂèñÂΩ±ÂÉè‰∏≠ÁöÑË§áÈõúË™ûÊÑèË≥áË®ä‰∏¶‰ª•ÁµêÊßãÂåñÁöÑÊñπÂºèË°®ÈÅîÔºåÂæûËÄåÊîπÂñÑ VLM ÁöÑÂü∫Á§éÊÑüÁü•ÂíåÁêÜËß£ËÉΩÂäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊï¥ÂêàÊàëÂÄëÁöÑ SGE Ê®°ÁµÑËÉΩÈ°ØËëóÊèêÂçá VLM Âú®Ë¶ñË¶∫Ë™ûË®Ä‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºåË°®Á§∫ÂÆÉÂú®‰øùÁïôË§áÈõúÁöÑË™ûÊÑèÁ¥∞ÁØÄÂíå‰øÉÈÄ≤Êõ¥Â•ΩÁöÑË¶ñË¶∫ÁêÜËß£ÊñπÈù¢ÂæàÊúâÊïà„ÄÇ

##### **LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**
2408.15903v1 by Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

The rapid obsolescence of information in Large Language Models (LLMs) has
driven the development of various techniques to incorporate new facts. However,
existing methods for knowledge editing still face difficulties with multi-hop
questions that require accurate fact identification and sequential logical
reasoning, particularly among numerous fact updates. To tackle these
challenges, this paper introduces Graph Memory-based Editing for Large Language
Models (GMeLLo), a straitforward and effective method that merges the explicit
knowledge representation of Knowledge Graphs (KGs) with the linguistic
flexibility of LLMs. Beyond merely leveraging LLMs for question answering,
GMeLLo employs these models to convert free-form language into structured
queries and fact triples, facilitating seamless interaction with KGs for rapid
updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art knowledge editing methods in
the multi-hop question answering benchmark, MQuAKE, especially in scenarios
with extensive knowledge edits.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ë≥áË®äÂø´ÈÄüÈÅéÊôÇÔºå‰øÉ‰ΩøÂêÑÁ®ÆÊäÄË°ìÁôºÂ±ï‰ª•Á¥çÂÖ•Êñ∞‰∫ãÂØ¶„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÂú®ÈúÄË¶ÅÊ∫ñÁ¢∫‰∫ãÂØ¶Ëæ®Ë≠òÂíåÈ†ÜÂ∫èÈÇèËºØÊé®ÁêÜÁöÑÂ§öË∑≥ÂïèÈ°å‰∏ä‰ªçÈù¢Ëá®Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÁúæÂ§ö‰∫ãÂØ¶Êõ¥Êñ∞‰∏≠„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Êñá‰ªãÁ¥π‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂúñË®òÊÜ∂Á∑®ËºØ (GMeLLo)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰∏îÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊòéÁ¢∫Áü•Ë≠òË°®Á§∫Ëàá LLM ÁöÑË™ûË®ÄÈùàÊ¥ªÊÄß„ÄÇGMeLLo ‰∏çÂÉÖÂà©Áî® LLM ‰æÜÂõûÁ≠îÂïèÈ°åÔºåÈÇÑ‰ΩøÁî®ÈÄô‰∫õÊ®°ÂûãÂ∞áËá™Áî±ÂΩ¢ÂºèÁöÑË™ûË®ÄËΩâÊèõÁÇ∫ÁµêÊßãÂåñÊü•Ë©¢Âíå‰∫ãÂØ¶‰∏âÂÖÉÁµÑÔºå‰øÉÈÄ≤Ëàá KG ÁöÑÁÑ°Á∏´‰∫íÂãïÔºå‰ª•‰æøÂø´ÈÄüÊõ¥Êñ∞ÂíåÁ≤æÁ¢∫ÁöÑÂ§öË∑≥Êé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂú®Â§öË∑≥ÂïèÈ°åÂõûÁ≠îÂü∫Ê∫ñ MQuAKE ‰∏≠ÔºåGMeLLo ÊòéÈ°ØË∂ÖË∂ä‰∫ÜÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Âª£Ê≥õÁü•Ë≠òÁ∑®ËºØÁöÑÂ†¥ÊôØ‰∏≠„ÄÇ

##### **VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**
2408.14895v2 by Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda

Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÁü•Ë≠òÂúñÔºàMMKGÔºâÂ∞áÂêÑÁ®ÆÈùûÁ¨¶ËôüÊï∏ÊìöÔºà‰æãÂ¶ÇÔºåÂΩ±ÂÉèÂíåÂΩ±ÁâáÔºâËΩâÊèõÁÇ∫Á¨¶ËôüÔºåÊàêÁÇ∫‰∏ÄÁ®ÆË≥áÊ∫êÔºåËÉΩËÆìË∑®Ê®°ÊÖãÁöÑÁü•Ë≠òËôïÁêÜÂíåÊ©üÂô®Â≠∏ÁøíÊàêÁÇ∫ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂåÖÂê´Â§öÂÄã‰∫ã‰ª∂Ôºà‰æãÂ¶ÇÊó•Â∏∏ÁîüÊ¥ªÊ¥ªÂãïÔºâÁöÑÂΩ±ÁâáÔºåÂÖ∂ MMKG ÁöÑÂª∫Êßã‰ªçËôïÊñºÊó©ÊúüÈöéÊÆµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂü∫ÊñºÊØèÊó•Ê¥ªÂãïÁöÑÂêåÊ≠•Â§öË¶ñËßíÊ®°Êì¨ÂΩ±ÁâáÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄã MMKG„ÄÇÈô§‰∫ÜÂ∞áÊó•Â∏∏ÁîüÊ¥ªÂΩ±ÁâáÁöÑÂÖßÂÆπË°®Á§∫ÁÇ∫‰ª•‰∫ã‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁü•Ë≠òÂ§ñÔºåÊàëÂÄëÁöÑ MMKG ‰πüÂåÖÂê´ÈÄêÂπÄÁöÑÁ¥∞ÂæÆËÆäÂåñÔºå‰æãÂ¶ÇÂΩ±ÁâáÂπÄ‰∏≠ÁöÑÈÇäÁïåÊ°Ü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÊèê‰æõ‰∫ÜÁî®ÊñºÊü•Ë©¢ MMKG ÁöÑÊîØÊè¥Â∑•ÂÖ∑„ÄÇ‰ΩúÁÇ∫ÊáâÁî®ÁØÑ‰æãÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑ MMKG Â¶Ç‰ΩïÈÄèÈÅéÊèê‰æõÁâπÂÆö‰ªªÂãôÊâÄÈúÄÁöÑË¶ñË¶∫Ë™ûË®ÄË≥áÊñôÈõÜÔºå‰æÜ‰øÉÈÄ≤Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇ

##### **XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**
2408.16021v1 by Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian

In the rapidly evolving field of cybersecurity, the integration of flow-level
and packet-level information for real-time intrusion detection remains a
largely untapped area of research. This paper introduces "XG-NID," a novel
framework that, to the best of our knowledge, is the first to fuse flow-level
and packet-level data within a heterogeneous graph structure, offering a
comprehensive analysis of network traffic. Leveraging a heterogeneous graph
neural network (GNN) with graph-level classification, XG-NID uniquely enables
real-time inference while effectively capturing the intricate relationships
between flow and packet payload data. Unlike traditional GNN-based
methodologies that predominantly analyze historical data, XG-NID is designed to
accommodate the heterogeneous nature of network traffic, providing a robust and
real-time defense mechanism. Our framework extends beyond mere classification;
it integrates Large Language Models (LLMs) to generate detailed, human-readable
explanations and suggest potential remedial actions, ensuring that the insights
produced are both actionable and comprehensible. Additionally, we introduce a
new set of flow features based on temporal information, further enhancing the
contextual and explainable inferences provided by our model. To facilitate
practical application and accessibility, we developed "GNN4ID," an open-source
tool that enables the extraction and transformation of raw network traffic into
the proposed heterogeneous graph structure, seamlessly integrating flow and
packet-level data. Our comprehensive quantitative comparative analysis
demonstrates that XG-NID achieves an F1 score of 97\% in multi-class
classification, outperforming existing baseline and state-of-the-art methods.
This sets a new standard in Network Intrusion Detection Systems by combining
innovative data fusion with enhanced interpretability and real-time
capabilities.

ÊëòË¶ÅÔºö<paragraph>Âú®Âø´ÈÄüÁôºÂ±ïÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®È†òÂüü‰∏≠ÔºåÊï¥ÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áË®ä‰ª•ÈÄ≤Ë°åÂç≥ÊôÇÂÖ•‰æµÂÅµÊ∏¨Ôºå‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂ∞öÊú™ÈñãÁôºÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÊú¨Êñá‰ªãÁ¥π„ÄåXG-NID„ÄçÔºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊû∂ÊßãÔºåÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂú®Áï∞Ë≥™ÂúñÂΩ¢ÁµêÊßã‰∏≠ËûçÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñôÁöÑÊû∂ÊßãÔºåÊèê‰æõÂ∞çÁ∂≤Ë∑ØÊµÅÈáèÁöÑÂÖ®Èù¢ÂàÜÊûê„ÄÇÈÄèÈÅéÂà©Áî®Áï∞Ë≥™ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÔºåXG-NID Áç®ÁâπÂú∞ÂØ¶ÁèæÂç≥ÊôÇÊé®Ë´ñÔºåÂêåÊôÇÊúâÊïàÊì∑ÂèñÊµÅÂíåÂ∞ÅÂåÖÈÖ¨ËºâË≥áÊñô‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇËàáÂÇ≥Áµ±Âü∫Êñº GNN ÁöÑÊñπÊ≥ïÔºà‰∏ªË¶ÅÂàÜÊûêÊ≠∑Âè≤Ë≥áÊñôÔºâ‰∏çÂêåÔºåXG-NID Ë¢´Ë®≠Ë®àÊàêÈÅ©ÊáâÁ∂≤Ë∑ØÊµÅÈáèÁöÑÁï∞Ë≥™ÊÄßÔºåÊèê‰æõÂº∑Â§ß‰∏îÂç≥ÊôÇÁöÑÈò≤Á¶¶Ê©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂Êßã‰∏çÂÉÖÈôêÊñºÂàÜÈ°ûÔºõÂÆÉÊï¥ÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•Áî¢ÁîüË©≥Á¥∞„ÄÅ‰∫∫È°ûÂèØËÆÄÁöÑËß£Èáã‰∏¶Âª∫Ë≠∞ÊΩõÂú®ÁöÑË£úÊïëÊé™ÊñΩÔºåÁ¢∫‰øùÁî¢ÁîüÁöÑË¶ãËß£Êó¢ÂèØÊìç‰ΩúÂèàÊòìÊñºÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ†πÊìöÊôÇÈñìË≥áË®äÂºïÂÖ•‰∏ÄÁµÑÊñ∞ÁöÑÊµÅÁâπÂæµÔºåÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ê®°ÂûãÊèê‰æõÁöÑËÑàÁµ°ÂíåÂèØËß£ÈáãÊé®Ë´ñ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂØ¶ÈöõÊáâÁî®ÂíåÂèØÂèäÊÄßÔºåÊàëÂÄëÈñãÁôº‰∫Ü„ÄåGNN4ID„ÄçÔºå‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Â∞áÂéüÂßãÁ∂≤Ë∑ØÊµÅÈáèÊèêÂèñ‰∏¶ËΩâÊèõÁÇ∫Âª∫Ë≠∞ÁöÑÁï∞Ë≥™ÂúñÂΩ¢ÁµêÊßãÔºåÁÑ°Á∏´Êï¥ÂêàÊµÅÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñô„ÄÇÊàëÂÄëÂÖ®Èù¢ÁöÑÂÆöÈáèÊØîËºÉÂàÜÊûêË°®ÊòéÔºåXG-NID Âú®Â§öÈ°ûÂà•ÂàÜÈ°û‰∏≠ÈÅîÂà∞ 97% ÁöÑ F1 ÂàÜÊï∏ÔºåÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñÂíåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÈÄôÈÄèÈÅéÁµêÂêàÂâµÊñ∞ÁöÑË≥áÊñôËûçÂêà„ÄÅÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÂíåÂç≥ÊôÇÂäüËÉΩÔºåÂú®Á∂≤Ë∑ØÂÖ•‰æµÂÅµÊ∏¨Á≥ªÁµ±‰∏≠Ê®πÁ´ã‰∫ÜÊñ∞ÁöÑÊ®ôÊ∫ñ„ÄÇ</paragraph>

##### **Process Trace Querying using Knowledge Graphs and Notation3**
2409.04452v1 by William Van Woensel

In process mining, a log exploration step allows making sense of the event
traces; e.g., identifying event patterns and illogical traces, and gaining
insight into their variability. To support expressive log exploration, the
event log can be converted into a Knowledge Graph (KG), which can then be
queried using general-purpose languages. We explore the creation of semantic KG
using the Resource Description Framework (RDF) as a data model, combined with
the general-purpose Notation3 (N3) rule language for querying. We show how
typical trace querying constraints, inspired by the state of the art, can be
implemented in N3. We convert case- and object-centric event logs into a
trace-based semantic KG; OCEL2 logs are hereby "flattened" into traces based on
object paths through the KG. This solution offers (a) expressivity, as queries
can instantiate constraints in multiple ways and arbitrarily constrain
attributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2
event logs can be serialized as traces in arbitrary ways based on the KG; and
(c) extensibility, as others can extend our library by leveraging the same
implementation patterns.

ÊëòË¶ÅÔºöÂú®ÊµÅÁ®ãÊåñÊéò‰∏≠ÔºåÊó•ÂøóÊé¢Á¥¢Ê≠•È™§ÂèØ‰ª•ÁêÜËß£‰∫ã‰ª∂ËΩ®ËøπÔºõ‰æãÂ¶ÇÔºåËØÜÂà´‰∫ã‰ª∂Ê®°ÂºèÂíåÈùûÈÄªËæëËΩ®ËøπÔºåÂπ∂Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÂèØÂèòÊÄß„ÄÇ‰∏∫‰∫ÜÊîØÊåÅË°®ËææÊÄßÊó•ÂøóÊé¢Á¥¢Ôºå‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•ËΩ¨Êç¢‰∏∫Áü•ËØÜÂõæ (KG)ÔºåÁÑ∂ÂêéÂèØ‰ª•‰ΩøÁî®ÈÄöÁî®ËØ≠Ë®ÄÂØπÂÖ∂ËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Êé¢Á¥¢‰ΩøÁî®ËµÑÊ∫êÊèèËø∞Ê°ÜÊû∂ (RDF) ‰Ωú‰∏∫Êï∞ÊçÆÊ®°ÂûãÂàõÂª∫ËØ≠‰πâ KGÔºåÂπ∂ÁªìÂêàÈÄöÁî® Notation3 (N3) ËßÑÂàôËØ≠Ë®ÄËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® N3 ÂÆûÁé∞ÂèóÁé∞ÊúâÊäÄÊúØÂêØÂèëÁöÑÂÖ∏ÂûãËΩ®ËøπÊü•ËØ¢Á∫¶Êùü„ÄÇÊàë‰ª¨Â∞ÜÊ°à‰æãÂíåÂØπË±°‰∏≠ÂøÉ‰∫ã‰ª∂Êó•ÂøóËΩ¨Êç¢‰∏∫Âü∫‰∫éËΩ®ËøπÁöÑËØ≠‰πâ KGÔºõOCEL2 Êó•ÂøóÂú®Ê≠§Ë¢´‚ÄúÊâÅÂπ≥Âåñ‚Äù‰∏∫Âü∫‰∫éÈÄöËøá KG ÁöÑÂØπË±°Ë∑ØÂæÑÁöÑËΩ®Ëøπ„ÄÇÊ≠§Ëß£ÂÜ≥ÊñπÊ°àÊèê‰æõ (a) Ë°®ËææÂäõÔºåÂõ†‰∏∫Êü•ËØ¢ÂèØ‰ª•‰ª•Â§öÁßçÊñπÂºèÂÆû‰æãÂåñÁ∫¶ÊùüÂπ∂‰ªªÊÑèÁ∫¶ÊùüÂ±ûÊÄßÂíåÂÖ≥Á≥ªÔºà‰æãÂ¶ÇÔºåÂèÇ‰∏éËÄÖ„ÄÅËµÑÊ∫êÔºâÔºõ(b) ÁÅµÊ¥ªÔºåÂõ†‰∏∫ OCEL2 ‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•Âü∫‰∫é KG ‰ª•‰ªªÊÑèÊñπÂºèÂ∫èÂàóÂåñ‰∏∫ËΩ®ËøπÔºõ‰ª•Âèä (c) ÂèØÊâ©Â±ïÊÄßÔºåÂõ†‰∏∫ÂÖ∂‰ªñ‰∫∫ÂèØ‰ª•ÈÄöËøáÂà©Áî®Áõ∏ÂêåÁöÑÂÆûÁé∞Ê®°ÂºèÊù•Êâ©Â±ïÊàë‰ª¨ÁöÑÂ∫ì„ÄÇ

##### **PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**
2409.00092v1 by Runtao Ren, Jian Ma

As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.

ÊëòË¶ÅÔºö<paragraph>Èö®Ëëó‰∫∫È°ûÈÇÅÂÖ•ÁßëÊäÄÂâµÊñ∞ÁöÑÊñ∞Á¥ÄÂÖÉÔºåËøÖÈÄüÂ∞áÂâµÊÑèÈªûÂ≠êËΩâÂåñÁÇ∫Âèó‰øùË≠∑ÁöÑÊô∫ÊÖßË≤°Áî¢ÔºàIPÔºâÁöÑËÉΩÂäõÊØî‰ª•ÂæÄ‰ªª‰ΩïÊôÇÂÄôÈÉΩÊõ¥Âä†ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂ∞àÂà©Ëµ∑ËçâÁ®ãÂ∫èÂÖÖÊªøÊåëÊà∞ÔºåÈúÄË¶ÅÂ∞çÂÖàÈÄ≤È†òÂüüÁü•Ë≠òÂíåÊäÄË°ìÊ¶ÇÂøµÊúâÁ¥∞Á∑ªÂÖ•ÂæÆÁöÑ‰∫ÜËß£„ÄÇÁèæÊúâÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÈõñÁÑ∂Âº∑Â§ßÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÁî¢ÁîüÊäÄË°ì‰∏äÊ∫ñÁ¢∫ÁöÑÂ∞àÂà©Êñá‰ª∂ÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÊÉÖÂ¢ÉÊÑèË≠òÔºåÂõ†Ê≠§Â∏∏Â∏∏ÁÑ°Ê≥ïÊªøË∂≥Ê≠§ IP Ââµ‰ΩúÈ†òÂüüÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÈóúÈçµÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑ LLM Áü•Ë≠òÂæÆË™ø (KFT) Êû∂ÊßãÔºåÊó®Âú®Ë≥¶‰∫à AI Ëá™‰∏ªÊåñÊéò„ÄÅÁêÜËß£ÂíåÊáâÁî®ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã PatentGPT ÂÖÖÂàÜÂà©Áî®‰∫ÜÂü∫ÊñºÁü•Ë≠òÂúñË°®ÁöÑÈ†êË®ìÁ∑¥„ÄÅÁâπÂÆöÈ†òÂüüÁöÑÁõ£Áù£ÂºèÂæÆË™ø (SFT) Âíå‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF) ÁöÑÁç®ÁâπÁµÑÂêà„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑË©ï‰º∞ÔºåPatentGPT Â∑≤Â±ïÁèæÂá∫ÂÇëÂá∫ÁöÑË°®ÁèæÔºåÂú®ËàáÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏ÊØîÁöÑÂ∞àÂà©Áõ∏ÈóúÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂæóÂàÜÈ´òÂá∫Á¥Ñ 400%„ÄÇÈÄèÈÅé KFT ÊñπÊ≥ïÔºåÊ≠§Ê®°Âûã‰∏çÂÉÖËÉΩÂ§†ÂçîÂä©ÔºåÈÇÑËÉΩÊì¥Â¢û‰∫∫È°ûÁöÑÂâµÈÄ†ÂäõÂíåÂâµÊñ∞ÂäõÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁÇ∫ AI È©ÖÂãïÁöÑÊô∫ÊÖßË≤°Áî¢ÁîüÊàêÊ®πÁ´ã‰∫ÜÊñ∞Ê®ôÊ∫ñÔºåÁÇ∫Êõ¥ÊúâÊïàÁéá‰∏îÊõ¥ÊúâÊïàÁöÑÁôºÊòéÊµÅÁ®ãÈã™Ë∑Ø„ÄÇ</paragraph>

##### **DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**
2408.14185v1 by Ziai Zhou, Bin Zhou, Hao Liu

Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning, balancing global and local optimality. We
first use the static Dijkstra algorithm to compute a globally optimal baseline
path. A distributed control strategy then guides vehicles along this path. At
intersections, DynamicRouteGPT performs real-time decision-making for local
path selection, considering real-time traffic, driving preferences, and
unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian
inference, and large-scale pretrained language models like Llama3 8B to provide
an efficient path planning solution. It dynamically adjusts to traffic
scenarios and driver preferences and requires no pre-training, offering broad
applicability across road networks. A key innovation is the construction of
causal graphs for counterfactual reasoning, optimizing path decisions.
Experimental results show that our method achieves state-of-the-art performance
in real-time dynamic path planning for multiple vehicles while providing
explainable path selections, offering a novel and efficient solution for
complex traffic environments.

ÊëòË¶ÅÔºöÂú®Ë§áÈõú‰∫§ÈÄöÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÊúÉÈù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶Ç‰∫§ÈÄöÊµÅÈáèËÆäÂåñÂíå‰ø°ËôüÁ≠âÂæÖÊôÇÈñì„ÄÇÂÇ≥Áµ±ÁöÑÈùúÊÖãË∑ØÁî±ÊºîÁÆóÊ≥ïÔºå‰æãÂ¶Ç Dijkstra Âíå A*ÔºåÊúÉË®àÁÆóÊúÄÁü≠Ë∑ØÂæëÔºå‰ΩÜÈÄöÂ∏∏Âú®ÂãïÊÖãÊ¢ù‰ª∂‰∏ãÊúÉÂ§±Êïó„ÄÇÊúÄËøëÁöÑÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊîπÈÄ≤Ôºå‰ΩÜÂÇæÂêëÊñºÈóúÊ≥®Â±ÄÈÉ®ÊúÄÂÑ™ÔºåÂÜíËëóÈô∑ÂÖ•Ê≠ªËÉ°ÂêåÊàñÈÇäÁïåÂïèÈ°åÁöÑÈ¢®Èö™„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂõ†ÊûúÊé®Ë´ñÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÁî®ÊñºÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÔºåÂπ≥Ë°°ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÊúÄÂÑ™ÊÄß„ÄÇÊàëÂÄëÈ¶ñÂÖà‰ΩøÁî®ÈùúÊÖã Dijkstra ÊºîÁÆóÊ≥ïË®àÁÆóÂÖ®Â±ÄÊúÄÂÑ™Âü∫Á∑öË∑ØÂæë„ÄÇÁÑ∂ÂæåÔºå‰∏ÄÂÄãÂàÜÂ∏ÉÂºèÊéßÂà∂Á≠ñÁï•Ê≤øËëóÈÄôÊ¢ùË∑ØÂæëÂºïÂ∞éËªäËºõ„ÄÇÂú®‰∫§ÂèâË∑ØÂè£ÔºåDynamicRouteGPT ÈáùÂ∞çÂ±ÄÈÉ®Ë∑ØÂæëÈÅ∏ÊìáÂü∑Ë°åÂØ¶ÊôÇÊ±∫Á≠ñÔºåËÄÉÈáèÂØ¶ÊôÇ‰∫§ÈÄö„ÄÅÈßïÈßõÂÅèÂ•ΩÂíåÊÑèÂ§ñ‰∫ã‰ª∂„ÄÇDynamicRouteGPT Êï¥Âêà‰∫ÜÈ¶¨ÂèØÂ§´Èèà„ÄÅË≤ùÊ∞èÊé®Ë´ñÂíå Llama3 8B Á≠âÂ§ßË¶èÊ®°È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºå‰ª•Êèê‰æõÊúâÊïàÁöÑË∑ØÂæëË¶èÂäÉËß£Ê±∫ÊñπÊ°à„ÄÇÂÆÉÊúÉÂãïÊÖãË™øÊï¥Âà∞‰∫§ÈÄöÁãÄÊ≥ÅÂíåÈßïÈßõÂÅèÂ•ΩÔºå‰∏¶‰∏î‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÔºåÂú®ÈÅìË∑ØÁ∂≤Ë∑Ø‰∏äÊèê‰æõÂª£Ê≥õÁöÑÈÅ©Áî®ÊÄß„ÄÇ‰∏ÄÂÄãÈóúÈçµÂâµÊñ∞ÊòØÂª∫Á´ãÂèç‰∫ãÂØ¶Êé®ÁêÜÁöÑÂõ†ÊûúÂúñÔºå‰ª•ÊúÄ‰Ω≥ÂåñË∑ØÂæëÊ±∫Á≠ñ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â§öËºõËªäËºõÁöÑÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉ‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇÊèê‰æõÂèØËß£ÈáãÁöÑË∑ØÂæëÈÅ∏ÊìáÔºåÁÇ∫Ë§áÈõúÁöÑ‰∫§ÈÄöÁí∞Â¢ÉÊèê‰æõ‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Exploring the Potential of Large Language Models for Heterophilic Graphs**
2408.14134v1 by Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi

Graph Neural Networks (GNNs) are essential for various graph-based learning
tasks. Notably, classical GNN architectures operate under the assumption of
homophily, which posits that connected nodes are likely to share similar
features. However, this assumption limits the effectiveness of GNNs in handling
heterophilic graphs where connected nodes often exhibit dissimilar
characteristics. Existing approaches for homophily graphs such as non-local
neighbor extension and architectural refinement overlook the rich textual data
associated with nodes, which could unlock deeper insights into these
heterophilic contexts. With advancements in Large Language Models (LLMs), there
is significant promise to enhance GNNs by leveraging the extensive open-world
knowledge within LLMs to more effectively interpret and utilize textual data
for characterizing heterophilic graphs. In this work, we explore the potential
of LLMs for modeling heterophilic graphs and propose a novel two-stage
framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.
Specifically, in the first stage, we fine-tune the LLM to better identify
homophilic and heterophilic edges based on the textual information of their
nodes. In the second stage, we adaptively manage message propagation in GNNs
for different edge types based on node features, structures, and heterophilic
or homophilic characteristics. To cope with the computational demands when
deploying LLMs in practical scenarios, we further explore model distillation
techniques to fine-tune smaller, more efficient models that maintain
competitive performance. Extensive experiments validate the effectiveness of
our framework, demonstrating the feasibility of using LLMs to enhance GNNs for
node classification on heterophilic graphs.

ÊëòË¶ÅÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞çÊñºÂêÑÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÂ≠∏Áøí‰ªªÂãôËá≥ÈóúÈáçË¶Å„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂÇ≥Áµ±ÁöÑ GNN Êû∂ÊßãÂú®ÂêåË≥™ÊÄßÁöÑÂÅáË®≠‰∏ãÈÅã‰ΩúÔºåË©≤ÂÅáË®≠Ë™çÁÇ∫ÈÄ£Êé•ÁöÑÁØÄÈªûÂèØËÉΩÂÖ±‰∫´È°û‰ººÁöÑÁâπÂæµ„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÂÅáË®≠ÈôêÂà∂‰∫Ü GNN Âú®ËôïÁêÜÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏≠ÁöÑÊïàËÉΩÔºåÂÖ∂‰∏≠ÈÄ£Êé•ÁöÑÁØÄÈªûÈÄöÂ∏∏Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÁâπÂæµ„ÄÇÁèæÊúâÁöÑÂêåË≥™ÊÄßÂúñÂΩ¢ÊñπÊ≥ïÔºà‰æãÂ¶ÇÈùûÂ±ÄÈÉ®ÈÑ∞ÂüüÂª∂‰º∏ÂíåÊû∂ÊßãÊîπÈÄ≤ÔºâÂøΩÁï•‰∫ÜËàáÁØÄÈªûÁõ∏ÈóúÁöÑË±êÂØåÊñáÊú¨Ë≥áÊñôÔºåÈÄôÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£ÈÄô‰∫õÁï∞Ë≥™ÊÄßËÑàÁµ°„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÈÄèÈÅéÂà©Áî® LLM ‰∏≠Âª£Ê≥õÁöÑÈñãÊîæ‰∏ñÁïåÁü•Ë≠ò‰æÜÂ¢ûÂº∑ GNNÔºåÂ∞çÊñºÊõ¥ÊúâÊïàÂú∞Ë©ÆÈáãÂíåÂà©Áî®ÊñáÊú¨Ë≥áÊñô‰æÜË°®ÂæµÁï∞Ë≥™ÊÄßÂúñÂΩ¢ÊúâÂæàÂ§ßÁöÑÂ∏åÊúõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®Áï∞Ë≥™ÊÄßÂúñÂΩ¢Âª∫Ê®°‰∏≠ÁöÑÊΩõÂäõÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ©ÈöéÊÆµÊû∂ÊßãÔºöLLM Â¢ûÂº∑ÈÇäÁ∑£Âà§Âà•Âô®Âíå LLM ÂºïÂ∞éÈÇäÁ∑£ÈáçÊñ∞Âä†Ê¨ä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂú®Á¨¨‰∏ÄÈöéÊÆµÔºåÊàëÂÄëÂæÆË™ø LLM ‰ª•Ê†πÊìöÂÖ∂ÁØÄÈªûÁöÑÊñáÊú¨Ë≥áË®äÔºåÊõ¥Â•ΩÂú∞Ë≠òÂà•ÂêåË≥™ÊÄßÂíåÁï∞Ë≥™ÊÄßÈÇäÁ∑£„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊàëÂÄëÊ†πÊìöÁØÄÈªûÁâπÂæµ„ÄÅÁµêÊßãÂíåÁï∞Ë≥™ÊÄßÊàñÂêåË≥™ÊÄßÁâπÂæµÔºåËá™ÈÅ©ÊáâÂú∞ÁÆ°ÁêÜ GNN ‰∏≠‰∏çÂêåÈÇäÁ∑£È°ûÂûãÁöÑË®äÊÅØÂÇ≥ÈÅû„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂú®ÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÈÉ®ÁΩ≤ LLM ÊôÇÁöÑË®àÁÆóÈúÄÊ±ÇÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÊ®°ÂûãËêÉÂèñÊäÄË°ìÔºå‰ª•ÂæÆË™øËºÉÂ∞è„ÄÅÊõ¥ÊúâÊïàÁéáÁöÑÊ®°ÂûãÔºå‰ª•Á∂≠ÊåÅÁ´∂Áà≠Âäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫Ü‰ΩøÁî® LLM ‰æÜÂ¢ûÂº∑ GNN ‰ª•ÈÄ≤Ë°åÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏äÁöÑÁØÄÈªûÂàÜÈ°ûÁöÑÂèØË°åÊÄß„ÄÇ

##### **Towards Graph Prompt Learning: A Survey and Beyond**
2408.14520v2 by Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou

Large-scale "pre-train and prompt learning" paradigms have demonstrated
remarkable adaptability, enabling broad applications across diverse domains
such as question answering, image recognition, and multimodal retrieval. This
approach fully leverages the potential of large-scale pre-trained models,
reducing downstream data requirements and computational costs while enhancing
model applicability across various tasks. Graphs, as versatile data structures
that capture relationships between entities, play pivotal roles in fields such
as social network analysis, recommender systems, and biological graphs. Despite
the success of pre-train and prompt learning paradigms in Natural Language
Processing (NLP) and Computer Vision (CV), their application in graph domains
remains nascent. In graph-structured data, not only do the node and edge
features often have disparate distributions, but the topological structures
also differ significantly. This diversity in graph data can lead to
incompatible patterns or gaps between pre-training and fine-tuning on
downstream graphs. We aim to bridge this gap by summarizing methods for
alleviating these disparities. This includes exploring prompt design
methodologies, comparing related techniques, assessing application scenarios
and datasets, and identifying unresolved problems and challenges. This survey
categorizes over 100 relevant works in this field, summarizing general design
principles and the latest applications, including text-attributed graphs,
molecules, proteins, and recommendation systems. Through this extensive review,
we provide a foundational understanding of graph prompt learning, aiming to
impact not only the graph mining community but also the broader Artificial
General Intelligence (AGI) community.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°„ÄåÈ†êË®ìÁ∑¥ÂíåÊèêÁ§∫Â≠∏Áøí„ÄçÁØÑ‰æãÂ∑≤Â±ïÁèæÂá∫ÈùûÂá°ÁöÑÈÅ©ÊáâÂäõÔºåËÉΩÂª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®ÆÈ†òÂüüÔºå‰æãÂ¶ÇÂïèÁ≠î„ÄÅÂΩ±ÂÉèËæ®Ë≠òÂíåÂ§öÊ®°ÊÖãÊ™¢Á¥¢„ÄÇÊ≠§ÊñπÊ≥ïÂÖÖÂàÜÁôºÊèÆÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÊΩõÂäõÔºåÊ∏õÂ∞ë‰∏ãÊ∏∏Ë≥áÊñôÈúÄÊ±ÇÂíåÈÅãÁÆóÊàêÊú¨ÔºåÂêåÊôÇÊèêÂçáÊ®°ÂûãÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÂúñÂΩ¢‰ΩúÁÇ∫ËÉΩÊçïÊçâÂØ¶È´î‰πãÈñìÈóú‰øÇÁöÑÂ§öÂäüËÉΩË≥áÊñôÁµêÊßãÔºåÂú®Á§æÁæ§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÊé®Ëñ¶Á≥ªÁµ±ÂíåÁîüÁâ©ÂúñÂΩ¢Á≠âÈ†òÂüüÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÑòÁÆ°È†êË®ìÁ∑¥ÂíåÊèêÁ§∫Â≠∏ÁøíÁØÑ‰æãÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÈõªËÖ¶Ë¶ñË¶∫ (CV) ‰∏≠Áç≤ÂæóÊàêÂäüÔºå‰ΩÜÂÆÉÂÄëÂú®ÂúñÂΩ¢È†òÂüüÁöÑÊáâÁî®‰ªçËôïÊñºËµ∑Ê≠•ÈöéÊÆµ„ÄÇÂú®ÂúñÂΩ¢ÁµêÊßãÂåñË≥áÊñô‰∏≠ÔºåÁØÄÈªûÂíåÈÇäÁ∑£ÁâπÂæµ‰∏çÂÉÖÂ∏∏Êúâ‰∏çÂêåÁöÑÂàÜ‰ΩàÔºåÊãìÊí≤ÁµêÊßã‰πüÂ∑ÆÁï∞ÂæàÂ§ß„ÄÇÂúñÂΩ¢Ë≥áÊñô‰∏≠ÁöÑÈÄôÁ®ÆÂ§öÊ®£ÊÄßÂèØËÉΩÂ∞éËá¥È†êË®ìÁ∑¥ÂíåÂæÆË™ø‰πãÈñìÂá∫Áèæ‰∏çÁõ∏ÂÆπÁöÑÊ®°ÂºèÊàñÂ∑ÆË∑ù„ÄÇÊàëÂÄëÊó®Âú®ÈÄèÈÅéÁ∏ΩÁµêÊ∏õËºïÈÄô‰∫õÂ∑ÆÁï∞ÁöÑÊñπÊ≥ï‰æÜÂΩåË£úÊ≠§Â∑ÆË∑ù„ÄÇÈÄôÂåÖÊã¨Êé¢Á¥¢ÊèêÁ§∫Ë®≠Ë®àÊñπÊ≥ï„ÄÅÊØîËºÉÁõ∏ÈóúÊäÄË°ì„ÄÅË©ï‰º∞ÊáâÁî®Â†¥ÊôØÂíåË≥áÊñôÈõÜÔºå‰ª•ÂèäÊâæÂá∫Êú™Ëß£Ê±∫ÁöÑÂïèÈ°åÂíåÊåëÊà∞„ÄÇÊú¨Ë™øÊü•Ê≠∏È°û‰∫ÜÊ≠§È†òÂüü‰∏≠Ë∂ÖÈÅé 100 ÁØáÁõ∏Èóú‰ΩúÂìÅÔºåÁ∏ΩÁµê‰∫Ü‰∏ÄËà¨Ë®≠Ë®àÂéüÂâáÂíåÊúÄÊñ∞ÊáâÁî®ÔºåÂåÖÊã¨ÊñáÂ≠óÂ±¨ÊÄßÂúñÂΩ¢„ÄÅÂàÜÂ≠ê„ÄÅËõãÁôΩË≥™ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÈÄèÈÅéÈÄôÈ†ÖÂª£Ê≥õÁöÑÂõûÈ°ßÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁöÑÂü∫Êú¨ÁêÜËß£ÔºåÊó®Âú®‰∏çÂÉÖÂΩ±ÈüøÂúñÂΩ¢ÊåñÊéòÁ§æÁæ§Ôºå‰πüÂΩ±ÈüøÊõ¥Âª£Ê≥õÁöÑ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) Á§æÁæ§„ÄÇ</paragraph>

##### **CodeGraph: Enhancing Graph Reasoning of LLMs with Code**
2408.13863v1 by Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song

With the increasing popularity of large language models (LLMs), reasoning on
basic graph algorithm problems is an essential intermediate step in assessing
their abilities to process and infer complex graph reasoning tasks. Existing
methods usually convert graph-structured data to textual descriptions and then
use LLMs for reasoning and computation. However, LLMs often produce computation
errors on arithmetic parts in basic graph algorithm problems, such as counting
number of edges. In addition, they struggle to control or understand the output
of the reasoning process, raising concerns about whether LLMs are simply
guessing. In this paper, we introduce CodeGraph, a method that encodes graph
problem solutions as code. The methods solve new graph problems by learning
from exemplars, generating programs, and executing them via a program
interpreter. Using the few-shot setting, we evaluate CodeGraph with the base
LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and
Mixtral-8x7B Instruct. Experimental results on six tasks with six graph
encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost
performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on
the task. Compared to the existing methods, CodeGraph demonstrates strong
performance on arithmetic problems in graph tasks and offers a more
controllable and interpretable approach to the reasoning process.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊó•Êº∏ÊôÆÂèäÔºåÂ∞çÂü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°åÈÄ≤Ë°åÊé®ÁêÜÊòØË©ï‰º∞ÂÆÉÂÄëËôïÁêÜÂíåÊé®Ë´ñË§áÈõúÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑËÉΩÂäõ‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑ‰∏≠ÈñìÊ≠•È©ü„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂ∞áÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñôËΩâÊèõÊàêÊñáÂ≠óÊèèËø∞ÔºåÁÑ∂Âæå‰ΩøÁî® LLM ÈÄ≤Ë°åÊé®ÁêÜÂíåÈÅãÁÆó„ÄÇÁÑ∂ËÄåÔºåLLM ÈÄöÂ∏∏ÊúÉÂú®Âü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°å‰∏≠Ôºå‰æãÂ¶ÇË®àÁÆóÈÇäÁ∑£Êï∏ÈáèÔºåÂ∞çÁÆóË°ìÈÉ®ÂàÜÁî¢ÁîüÈÅãÁÆóÈåØË™§„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÈõ£‰ª•ÊéßÂà∂ÊàñÁêÜËß£Êé®ÁêÜÈÅéÁ®ãÁöÑËº∏Âá∫ÔºåÈÄôÂºïÁôº‰∫Ü LLM ÊòØÂê¶Âè™ÊòØÂú®ÁåúÊ∏¨ÁöÑÁñëÊÖÆ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CodeGraphÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞áÂúñÂΩ¢ÂïèÈ°åËß£Ê±∫ÊñπÊ°àÁ∑®Á¢ºÁÇ∫Á®ãÂºèÁ¢ºÁöÑÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÈÄèÈÅéÂ≠∏ÁøíÁØÑ‰æã„ÄÅÁî¢ÁîüÁ®ãÂºèÔºå‰∏¶ÈÄèÈÅéÁ®ãÂºèÁ¢ºÁõ¥Ë≠ØÂô®Âü∑Ë°åÂÆÉÂÄë‰æÜËß£Ê±∫Êñ∞ÁöÑÂúñÂΩ¢ÂïèÈ°å„ÄÇ‰ΩøÁî®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆöÔºåÊàëÂÄë‰ΩøÁî®Âü∫Á§é LLM ÁÇ∫ GPT-3.5 Turbo„ÄÅLlama3-70B Instruct„ÄÅMixtral-8x22B Instruct Âíå Mixtral-8x7B Instruct ‰æÜË©ï‰º∞ CodeGraph„ÄÇÂú® GraphQA Ë≥áÊñôÈõÜ‰∏≠‰ΩøÁî®ÂÖ≠Á®ÆÂúñÂΩ¢Á∑®Á¢ºÊñπÊ≥ïÂ∞çÂÖ≠È†Ö‰ªªÂãôÈÄ≤Ë°åÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåCodeGraph ÂèØ‰ª•Â∞á LLM ‰∏≠ÁöÑÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩÊèêÂçá 1.3% Âà∞ 58.6%ÔºåÂÖ∑È´îÂèñÊ±∫Êñº‰ªªÂãô„ÄÇËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCodeGraph Âú®ÂúñÂΩ¢‰ªªÂãô‰∏≠ÁöÑÁÆóË°ìÂïèÈ°å‰∏äË°®ÁèæÂá∫Âº∑ÂãÅÁöÑÊïàËÉΩÔºå‰∏¶ÁÇ∫Êé®ÁêÜÈÅéÁ®ãÊèê‰æõÊõ¥ÂÖ∑ÂèØÊéßÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇ

##### **LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**
2408.14512v1 by Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.

ÊëòË¶ÅÔºöÈõ∂ÁØÑ‰æãÂúñÂΩ¢Ê©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÁî±ÊñºÁ®ÄÊúâÊ®ôÁ±§Ë≥áÊñôÁöÑÊåëÊà∞ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÈõñÁÑ∂Ëá™Áõ£Áù£ÂºèÂ≠∏ÁøíÂíåÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁ≠âÊñπÊ≥ïÂ∑≤Ë¢´Âª£Ê≥õÊé¢Á¥¢Ôºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥Êñº‰ªªÂãôÁâπÂÆöÊ®ôÁ±§ÁöÑÂæÆË™øÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èõ∂ÁØÑ‰æãÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂèóÂà∞Êåá‰ª§ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈõ∂ÁØÑ‰æãÂäüËÉΩÁöÑÂïüÁôºÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ Token Embedding-Aligned Graph Language Model (TEA-GLM) ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ‰ΩúÁÇ∫Ë∑®Ë≥áÊñôÈõÜÂíåË∑®‰ªªÂãôÁöÑÈõ∂ÁØÑ‰æãÂ≠∏ÁøíÂô®ÔºåÁî®ÊñºÂúñÂΩ¢Ê©üÂô®Â≠∏Áøí„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ†êË®ìÁ∑¥‰∏ÄÂÄã GNNÔºåÂ∞áÂÖ∂Ë°®Á§∫Ëàá LLM ÁöÑ token embedding Â∞çÈΩä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®ìÁ∑¥‰∏ÄÂÄãÁ∑öÊÄßÊäïÂΩ±Ê©üÔºåÂ∞á GNN ÁöÑË°®Á§∫ËΩâÊèõÁÇ∫Âõ∫ÂÆöÊï∏ÈáèÁöÑÂúñÂΩ¢ token embeddingÔºåËÄåÁÑ°ÈúÄË™øÊï¥ LLM„ÄÇÁµ±‰∏ÄÁöÑÊåá‰ª§ÊòØÁÇ∫‰∏çÂêåÂ±§Á¥öÁöÑÂêÑÁ®ÆÂúñÂΩ¢‰ªªÂãôË®≠Ë®àÁöÑÔºå‰æãÂ¶ÇÁØÄÈªûÂàÜÈ°ûÔºàÁØÄÈªûÂ±§Á¥öÔºâÂíåÈÄ£ÁµêÈ†êÊ∏¨ÔºàÈÇäÁ∑£Â±§Á¥öÔºâ„ÄÇÈÄô‰∫õË®≠Ë®àÈÅ∏ÊìáÂÖ±ÂêåÂ¢ûÂº∑‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Èõ∂ÁØÑ‰æãÂ≠∏Áøí‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰ΩøÂÖ∂ÊúâÂà•ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÂØ¶È©óË°®ÊòéÔºåËàá‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ÁöÑÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂúñÂΩ¢ token embedding Âπ´Âä© LLM È†êÊ∏¨Âô®Âú®Êú™Ë¶ãÈÅéÁöÑË≥áÊñôÈõÜÂíå‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**
2408.13661v1 by Sakhinana Sagar Srinivas, Geethan Sannidhi, Venkataramana Runkana

Characterizing materials with electron micrographs is a crucial task in
fields such as semiconductors and quantum materials. The complex hierarchical
structure of micrographs often poses challenges for traditional classification
methods. In this study, we propose an innovative backbone architecture for
analyzing electron micrographs. We create multi-modal representations of the
micrographs by tokenizing them into patch sequences and, additionally,
representing them as vision graphs, commonly referred to as patch attributed
graphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered
network structure architecture that facilitates information exchange between
the multi-modal representations and knowledge integration across different
patch resolutions. Furthermore, we leverage large language models (LLMs) to
generate detailed technical descriptions of nanomaterials as auxiliary
information to assist in the downstream task. We utilize a cross-modal
attention mechanism for knowledge fusion across cross-domain
representations(both image-based and linguistic insights) to predict the
nanomaterial category. This multi-faceted approach promises a more
comprehensive and accurate representation and classification of micrographs for
nanomaterial identification. Our framework outperforms traditional methods,
overcoming challenges posed by distributional shifts, and facilitating
high-throughput screening.

ÊëòË¶ÅÔºöÂà©Áî®ÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá‰æÜË°®ÂæµÊùêÊñôÔºåÂú®ÂçäÂ∞éÈ´îÂíåÈáèÂ≠êÊùêÊñôÁ≠âÈ†òÂüü‰∏≠ÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÈ°ØÂæÆÁÖßÁâáË§áÈõúÁöÑÂàÜÂ±§ÁµêÊßãÈÄöÂ∏∏ÊúÉÂ∞çÂÇ≥Áµ±ÂàÜÈ°ûÊñπÊ≥ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑ‰∏ªÂππÊû∂ÊßãÔºåÁî®ÊñºÂàÜÊûêÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÈ°ØÂæÆÁÖßÁâá‰ª£ÊèõÊàêÂçÄÂ°äÂ∫èÂàó‰æÜÂª∫Á´ãÂÖ∂Â§öÊ®°ÊÖãË°®Á§∫ÔºåÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂ∞áÂÖ∂Ë°®Á§∫ÁÇ∫Ë¶ñË¶∫ÂúñÂΩ¢ÔºåÈÄöÂ∏∏Á®±ÁÇ∫ÂçÄÂ°äÂ±¨ÊÄßÂúñÂΩ¢„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂàÜÂ±§Á∂≤Ë∑ØËûçÂêà (HNF)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ§öÂ±§Á∂≤Ë∑ØÁµêÊßãÊû∂ÊßãÔºåÊúâÂä©ÊñºÂ§öÊ®°ÊÖãË°®Á§∫‰πãÈñìÁöÑË≥áË®ä‰∫§ÊèõÔºå‰ª•Âèä‰∏çÂêåÂçÄÂ°äËß£ÊûêÂ∫¶‰πãÈñìÁöÑÁü•Ë≠òÊï¥Âêà„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÂ•àÁ±≥ÊùêÊñôÁöÑË©≥Á¥∞ÊäÄË°ìË™™ÊòéÔºå‰ΩúÁÇ∫ËºîÂä©Ë≥áË®äÔºå‰ª•ÂçîÂä©‰∏ãÊ∏∏‰ªªÂãô„ÄÇÊàëÂÄëÂà©Áî®Ë∑®Ê®°ÊÖãÊ≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂú®Ë∑®È†òÂüüË°®Á§∫ÔºàÂü∫ÊñºÂΩ±ÂÉèÂíåË™ûË®ÄÊ¥ûÂØüÂäõÔºâ‰∏≠ÈÄ≤Ë°åÁü•Ë≠òËûçÂêàÔºå‰ª•È†êÊ∏¨Â•àÁ±≥ÊùêÊñôÈ°ûÂà•„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÂÅöÊ≥ïÊúâÊúõÁÇ∫Â•àÁ±≥ÊùêÊñôË≠òÂà•Êèê‰æõÊõ¥ÂÖ®Èù¢‰∏îÊ∫ñÁ¢∫ÁöÑË°®Á§∫ÂíåÂàÜÈ°û„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÖãÊúç‰∫ÜÂàÜ‰ΩàËΩâÁßªÂ∏∂‰æÜÁöÑÊåëÊà∞Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÈ´òÈÄöÈáèÁØ©ÈÅ∏„ÄÇ

##### **GNN: Graph Neural Network and Large Language Model for Data Discovery**
2408.13609v2 by Thomas Hoang

Our algorithm GNN: Graph Neural Network and Large Language Model for Data
Discovery inherit the benefits of \cite{hoang2024plod} (PLOD: Predictive
Learning Optimal Data Discovery), \cite{Hoang2024BODBO} (BOD: Blindly Optimal
Data Discovery) in terms of overcoming the challenges of having to predefine
utility function and the human input for attribute ranking, which helps prevent
the time-consuming loop process. In addition to these previous works, our
algorithm GNN leverages the advantages of graph neural networks and large
language models to understand text type values that cannot be understood by
PLOD and MOD, thus making the task of predicting outcomes more reliable. GNN
could be seen as an extension of PLOD in terms of understanding the text type
value and the user's preferences, not only numerical values but also text
values, making the promise of data science and analytics purposes.

ÊëòË¶ÅÔºöÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNNÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÔºåÁî®ÊñºË≥áÊñôÊé¢Á¥¢ÔºåÁπºÊâø‰∫Ü \cite{hoang2024plod}ÔºàPLODÔºöÈ†êÊ∏¨ÊÄßÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢Ôºâ„ÄÅ\cite{Hoang2024BODBO}ÔºàBODÔºöÁõ≤ÁõÆÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢ÔºâÁöÑÂÑ™ÈªûÔºåÂú®ÊñºÂÖãÊúçÂøÖÈ†àÈ†êÂÖàÂÆöÁæ©ÊïàÁî®ÂáΩÊï∏Âíå‰∫∫È°ûËº∏ÂÖ•Â±¨ÊÄßÊéíÂêçÁöÑÊåëÊà∞ÔºåÈÄôÊúâÂä©ÊñºÈò≤Ê≠¢ËÄóÊôÇÁöÑËø¥ÂúàËôïÁêÜ„ÄÇÈô§‰∫ÜÈÄô‰∫õÂÖàÂâçÁöÑ‰ΩúÂìÅÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNN Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁöÑÂÑ™ÈªûÔºå‰æÜÁêÜËß£ PLOD Âíå MOD ÁÑ°Ê≥ïÁêÜËß£ÁöÑÊñáÂ≠óÈ°ûÂûãÂÄºÔºåÂæûËÄå‰ΩøÈ†êÊ∏¨ÁµêÊûúÁöÑ‰ªªÂãôÊõ¥ÂèØÈù†„ÄÇGNN ÂèØ‰ª•Ë¶ñÁÇ∫ PLOD Âú®ÁêÜËß£ÊñáÂ≠óÈ°ûÂûãÂÄºÂíå‰ΩøÁî®ËÄÖÂÅèÂ•ΩÊñπÈù¢ÁöÑÂª∂‰º∏Ôºå‰∏çÂÉÖÊòØÊï∏ÂÄºÔºåÈÇÑÊúâÊñáÂ≠óÂÄºÔºåÈÄôÂØ¶Áèæ‰∫ÜË≥áÊñôÁßëÂ≠∏ÂíåÂàÜÊûêÁõÆÁöÑÁöÑÊâøË´æ„ÄÇ

##### **HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**
2408.13521v1 by Azmine Toushik Wasi

Knowledge Graphs (KGs) serving as semantic networks, prove highly effective
in managing complex interconnected data in different domains, by offering a
unified, contextualized, and structured representation with flexibility that
allows for easy adaptation to evolving knowledge. Processing complex Human
Resources (HR) data, KGs can help in different HR functions like recruitment,
job matching, identifying learning gaps, and enhancing employee retention.
Despite their potential, limited efforts have been made to implement practical
HR knowledge graphs. This study addresses this gap by presenting a framework
for effectively developing HR knowledge graphs from documents using Large
Language Models. The resulting KG can be used for a variety of downstream
tasks, including job matching, identifying employee skill gaps, and many more.
In this work, we showcase instances where HR KGs prove instrumental in precise
job matching, yielding advantages for both employers and employees. Empirical
evidence from experiments with information propagation in KGs and Graph Neural
Nets, along with case studies underscores the effectiveness of KGs in tasks
such as job and employee recommendations and job area classification. Code and
data are available at : https://github.com/azminewasi/HRGraph

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Ë™ûÁæ©Á∂≤Ë∑ØÔºåË≠âÊòéÂú®ÁÆ°ÁêÜ‰∏çÂêåÈ†òÂüü‰∏≠Ë§áÈõúÁöÑ‰∫íÈÄ£Ë≥áÊñôÊñπÈù¢ÈùûÂ∏∏ÊúâÊïàÔºåÈÄèÈÅéÊèê‰æõÁµ±‰∏Ä„ÄÅËÑàÁµ°Âåñ‰∏îÁµêÊßãÂåñÁöÑË°®Á§∫Ôºå‰∏¶ÂÖ∑ÂÇôÈùàÊ¥ªÊÄßÔºåÂèØËºïÈ¨ÜÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑÁü•Ë≠ò„ÄÇKG ËôïÁêÜË§áÈõúÁöÑ‰∫∫ÂäõË≥áÊ∫ê (HR) Ë≥áÊñôÔºåÊúâÂä©Êñº‰∏çÂêåÁöÑ HR ÂäüËÉΩÔºå‰æãÂ¶ÇÊãõÂãü„ÄÅÂ∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Â≠∏ÁøíÂ∑ÆË∑ùÂíåÊèêÂçáÂì°Â∑•ÁïôÂ≠òÁéá„ÄÇÂÑòÁÆ°ÊúâÂÖ∂ÊΩõÂäõÔºå‰ΩÜÂØ¶‰ΩúÂØ¶Áî®ÁöÑ HR Áü•Ë≠òÂúñË≠úÁöÑÂä™ÂäõÊúâÈôê„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄãÊû∂ÊßãÔºåÂæûÊñá‰ª∂‰∏≠‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊúâÊïàÈñãÁôº HR Áü•Ë≠òÂúñË≠úÔºå‰æÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÁî¢ÁîüÁöÑ KG ÂèØÁî®ÊñºÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãôÔºåÂåÖÊã¨Â∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Âì°Â∑•ÊäÄËÉΩÂ∑ÆË∑ùÁ≠â„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü HR KG Âú®Á≤æÁ¢∫Â∑•‰ΩúÂåπÈÖç‰∏≠Ë≠âÊòéÊúâÁî®ÁöÑÁØÑ‰æãÔºåÁÇ∫Èõá‰∏ªÂíåÂì°Â∑•Â∏∂‰æÜÂÑ™Âã¢„ÄÇÈÄèÈÅé KG ÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠Ë≥áË®äÂÇ≥Êí≠ÁöÑÂØ¶È©óÊâÄÂæóÁöÑÂØ¶Ë≠âÔºå‰ª•ÂèäÊ°à‰æãÁ†îÁ©∂ÔºåÂº∑Ë™ø‰∫Ü KG Âú®Â∑•‰ΩúÂíåÂì°Â∑•Êé®Ëñ¶‰ª•ÂèäÂ∑•‰ΩúÈ†òÂüüÂàÜÈ°ûÁ≠â‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/azminewasi/HRGraph

##### **Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**
2408.13432v1 by Yi-Hui Chen, Eric Jui-Lin Lu, Kwan-Ho Cheng

The main task of the KGQA system (Knowledge Graph Question Answering) is to
convert user input questions into query syntax (such as SPARQL). With the rise
of modern popular encoders and decoders like Transformer and ConvS2S, many
scholars have shifted the research direction of SPARQL generation to the Neural
Machine Translation (NMT) architecture or the generative AI field of
Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query
syntax as a language. It uses NMT-based translation models to translate natural
language questions into query syntax. Scholars use popular architectures
equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to
train translation models for query syntax. To achieve better query results,
this paper improved the ConvS2S encoder and added multi-head attention from the
Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the
n-gram language model. The principle is to use convolutional layers to capture
local hidden features in the input sequence with different receptive fields,
using multi-head attention to calculate dependencies between them. Ultimately,
we found that the translation model based on the Multi-Head Conv encoder
achieved better performance than other encoders, obtaining 76.52\% and 83.37\%
BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0
datasets, respectively. Additionally, in the end-to-end system experiments on
the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other
KGQA systems, with Macro F1-measures reaching 52\% and 66\%, respectively.
Moreover, the experimental results show that with limited computational
resources, if one possesses an excellent encoder-decoder architecture and
cross-attention, experts and scholars can achieve outstanding performance
equivalent to large pre-trained models using only general embeddings.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË°®ÂïèÁ≠îÁ≥ªÁµ± (KGQA) ÁöÑ‰∏ªË¶Å‰ªªÂãôÊòØÂ∞á‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï (‰æãÂ¶Ç SPARQL)„ÄÇÈö®Ëëó Transformer Âíå ConvS2S Á≠âÁèæ‰ª£ÊµÅË°åÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÁöÑÂ¥õËµ∑ÔºåË®±Â§öÂ≠∏ËÄÖÂ∑≤Â∞á SPARQL ÁîüÊàêÁöÑÁ†îÁ©∂ÊñπÂêëËΩâÁßªÂà∞Á•ûÁ∂ìÊ©üÂô®ÁøªË≠Ø (NMT) Êû∂ÊßãÊàñÊñáÂ≠óËΩâ SPARQL ÁöÑÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüü„ÄÇÂú®Âü∫Êñº NMT ÁöÑÂïèÁ≠îÁ≥ªÁµ±‰∏≠ÔºåÁ≥ªÁµ±Â∞áÁü•Ë≠òÂ∫´Êü•Ë©¢Ë™ûÊ≥ïË¶ñÁÇ∫‰∏ÄÁ®ÆË™ûË®Ä„ÄÇÂÆÉ‰ΩøÁî®Âü∫Êñº NMT ÁöÑÁøªË≠ØÊ®°ÂûãÂ∞áËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï„ÄÇÂ≠∏ËÄÖ‰ΩøÁî®ÈÖçÂÇôË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÁÜ±ÈñÄÊû∂ÊßãÔºå‰æãÂ¶Ç Transformer„ÄÅConvS2S Âíå BiLSTMÔºå‰æÜË®ìÁ∑¥Êü•Ë©¢Ë™ûÊ≥ïÁöÑÁøªË≠ØÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÁç≤ÂæóÊõ¥Â•ΩÁöÑÊü•Ë©¢ÁµêÊûúÔºåÊú¨ÊñáÊîπÈÄ≤‰∫Ü ConvS2S Á∑®Á¢ºÂô®Ôºå‰∏¶Âæû Transformer ‰∏≠Âä†ÂÖ•Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº n-gram Ë™ûË®ÄÊ®°ÂûãÁöÑÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô® (MHC Á∑®Á¢ºÂô®)„ÄÇÂÖ∂ÂéüÁêÜÊòØ‰ΩøÁî®Âç∑Á©çÂ±§‰ª•‰∏çÂêåÁöÑÊÑüÂèóÈáéÊì∑ÂèñËº∏ÂÖ•Â∫èÂàó‰∏≠ÁöÑÂ±ÄÈÉ®Èö±ËóèÁâπÂæµÔºå‰∏¶‰ΩøÁî®Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂Ë®àÁÆóÂÆÉÂÄë‰πãÈñìÁöÑ‰æùË≥¥Èóú‰øÇ„ÄÇÊúÄÁµÇÔºåÊàëÂÄëÁôºÁèæÂü∫ÊñºÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô®ÁöÑÁøªË≠ØÊ®°ÂûãÊØîÂÖ∂‰ªñÁ∑®Á¢ºÂô®Áç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩÔºåÂàÜÂà•Âú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜ‰∏äÁç≤Âæó 76.52% Âíå 83.37% ÁöÑ BLEU-1ÔºàÈõôË™ûË©ï‰º∞Á†îÁ©∂ÔºâÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÂú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜÁöÑÁ´ØÂà∞Á´ØÁ≥ªÁµ±ÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÂú®ÂÖ∂‰ªñ KGQA Á≥ªÁµ±‰∏≠ÂèñÂæó‰∫ÜÈ†òÂÖàÁöÑÁµêÊûúÔºåÂ∑®ËßÄ F1 Ê∏¨ÈáèÂÄºÂàÜÂà•ÈÅîÂà∞ 52% Âíå 66%„ÄÇÊ≠§Â§ñÔºåÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂ¶ÇÊûúÊìÅÊúâÂá∫Ëâ≤ÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂ÊßãÂíåË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂç≥‰ΩøÂú®ÈÅãÁÆóË≥áÊ∫êÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞àÂÆ∂ÂíåÂ≠∏ËÄÖ‰ªçÂèØ‰ª•‰ΩøÁî®‰∏ÄËà¨ÁöÑÂµåÂÖ•‰æÜÁç≤ÂæóÁ≠âÂêåÊñºÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇëÂá∫ÊïàËÉΩ„ÄÇ

##### **CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**
2408.13366v1 by Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari

This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.

ÊëòË¶ÅÔºöÊú¨ÁØáË´ñÊñáÊèêÂá∫ CodeRefineÔºå‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÁ†îÁ©∂Ë´ñÊñáÊñπÊ≥ïËá™ÂãïËΩâÊèõÁÇ∫ÂäüËÉΩÁ®ãÂºèÁ¢ºÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÂ§öÊ≠•È©üÊñπÊ≥ïÈ¶ñÂÖàÂæûË´ñÊñá‰∏≠ËêÉÂèñ‰∏¶ÊëòË¶ÅÂá∫ÈóúÈçµÊñáÂ≠óÂçÄÂ°äÔºåÂàÜÊûêÂÖ∂Á®ãÂºèÁ¢ºÁõ∏ÈóúÊÄßÔºå‰∏¶‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÊú¨‰ΩìÂª∫Á´ãÁü•Ë≠òÂúñË≠ú„ÄÇÊé•ËëóÂæûÈÄôÂÄãÁµêÊßãÂåñË°®Á§∫Áî¢ÁîüÁ®ãÂºèÁ¢ºÔºå‰∏¶ÈÄèÈÅéÊèêÂá∫ÁöÑÂõûÊ∫ØÂºèÊ™¢Á¥¢Â¢ûÂº∑Áî¢ÁîüÊñπÊ≥ïÈÄ≤Ë°åÂº∑Âåñ„ÄÇCodeRefine Ëß£Ê±∫‰∫ÜÁêÜË´ñÁ†îÁ©∂ËàáÂØ¶ÈöõÂØ¶‰Ωú‰πãÈñìÁöÑÈ¥ªÊ∫ùÔºåÊèê‰æõÊØî LLM Èõ∂Ê¨°ÊèêÁ§∫Êõ¥Á≤æÁ¢∫ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂú®ÂêÑÁ®ÆÁßëÂ≠∏Ë´ñÊñá‰∏äÁöÑË©ï‰º∞Ë≠âÊòé‰∫Ü CodeRefine ÂæûË´ñÊñáÊîπÂñÑÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÁöÑËÉΩÂäõÔºåÈÄôÊúâÊΩõÂäõÂä†ÈÄüÂ∞ñÁ´ØÊºîÁÆóÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊé°Áî®„ÄÇ

##### **Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**
2408.14494v1 by Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana

We present the Process Engineering Operations Assistant (PEOA), an AI-driven
framework designed to solve complex problems in the chemical and process
industries. The framework employs a modular architecture orchestrated by a
meta-agent, which serves as the central coordinator, managing an action
generator and instruction-tuned small-scale language models (expert models).
The action generator decomposes complex problems into sub-tasks and identifies
suitable expert models to execute each, delivering precise solutions for
multi-step problem-solving. Key techniques include advanced knowledge modeling
using property graphs for improved information retrieval, facilitating more
accurate and contextually relevant solutions. Additionally, the framework
utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to
fine-tune the action generator and expert models for domain adaptation,
alongside an iterative problem-solving mechanism with sophisticated error
handling. Custom datasets were developed to evaluate the framework against
leading proprietary language models on various engineering tasks. The results
demonstrate the framework effectiveness in automating calculations,
accelerating prototyping, and providing AI-augmented decision support for
industrial processes, marking a significant advancement in process engineering
capabilities.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜË£ΩÁ®ãÂ∑•Á®ã‰ΩúÊ•≠Âä©ÁêÜ (PEOA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± AI È©ÖÂãïÁöÑÊû∂ÊßãÔºåÊó®Âú®Ëß£Ê±∫ÂåñÂ≠∏ÂíåË£ΩÁ®ãÁî¢Ê•≠‰∏≠ÁöÑË§áÈõúÂïèÈ°å„ÄÇË©≤Êû∂ÊßãÊé°Áî®Ê®°ÁµÑÂåñÊû∂ÊßãÔºåÁî±‰∏ÄÂÄãÂÖÉ‰ª£ÁêÜÁ®ãÂºèÂçîË™øÔºåË©≤‰ª£ÁêÜÁ®ãÂºè‰ΩúÁÇ∫‰∏≠Â§ÆÂçîË™øÂô®ÔºåÁÆ°ÁêÜÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÊåá‰ª§Ë™øÊï¥ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÊ®°Âûã (Â∞àÂÆ∂Ê®°Âûã)„ÄÇÂãï‰ΩúÁî¢ÁîüÂô®Â∞áË§áÈõúÁöÑÂïèÈ°åÂàÜËß£ÁÇ∫Â≠ê‰ªªÂãôÔºå‰∏¶Ë≠òÂà•ÂêàÈÅ©ÁöÑÂ∞àÂÆ∂Ê®°Âûã‰æÜÂü∑Ë°åÊØèÂÄã‰ªªÂãôÔºåÁÇ∫Â§öÊ≠•È©üÂïèÈ°åËß£Ê±∫Êèê‰æõÁ≤æÁ¢∫ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÈóúÈçµÊäÄË°ìÂåÖÊã¨‰ΩøÁî®Â±¨ÊÄßÂúñÈÄ≤Ë°åÈÄ≤ÈöéÁü•Ë≠òÂª∫Ê®°Ôºå‰ª•ÊîπÂñÑË≥áË®äÊ™¢Á¥¢ÔºåÊèê‰æõÊõ¥Ê∫ñÁ¢∫‰∏îËàáËÑàÁµ°Áõ∏ÈóúÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊ≠§Â§ñÔºåË©≤Êû∂ÊßãÊé°Áî®ÊïôÂ∏´-Â≠∏ÁîüÂÇ≥Ëº∏Â≠∏ÁøíÊñπÊ≥ïÔºå‰ΩøÁî® GPT-4 (Omni) ‰æÜÂæÆË™øÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÂ∞àÂÆ∂Ê®°ÂûãÔºå‰ª•ÈÄ≤Ë°åÈ†òÂüüÈÅ©ÊáâÔºå‰ª•ÂèäÂÖ∑ÂÇôÁ≤æÁ∑ªÈåØË™§ËôïÁêÜÂäüËÉΩÁöÑËø≠‰ª£ÂïèÈ°åËß£Ê±∫Ê©üÂà∂„ÄÇÈñãÁôº‰∫ÜËá™Ë®ÇË≥áÊñôÈõÜÔºå‰ª•ÈáùÂ∞çÂêÑÁ®ÆÂ∑•Á®ã‰ªªÂãôË©ï‰º∞Ë©≤Êû∂ÊßãËàáÈ†òÂÖàÁöÑÂ∞àÊúâË™ûË®ÄÊ®°Âûã„ÄÇÁµêÊûúË≠âÊòé‰∫ÜË©≤Êû∂ÊßãÂú®Ëá™ÂãïÂåñË®àÁÆó„ÄÅÂä†ÈÄüÂª∫Ê®°ÂíåÊèê‰æõ AI Â¢ûÂº∑Ê±∫Á≠ñÊîØÊè¥ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË™åËëóË£ΩÁ®ãÂ∑•Á®ãËÉΩÂäõÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇ

##### **A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**
2408.12578v2 by Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka

Increase in data, size, or compute can lead to sudden learning of specific
capabilities by a neural network -- a phenomenon often called "emergence''.
Beyond scientific understanding, establishing the causal factors underlying
such emergent capabilities is crucial to enable risk regulation frameworks for
AI. In this work, we seek inspiration from study of emergent properties in
other fields and propose a phenomenological definition for the concept in the
context of neural networks. Our definition implicates the acquisition of
general structures underlying the data-generating process as a cause of sudden
performance growth for specific, narrower tasks. We empirically investigate
this definition by proposing an experimental system grounded in a
context-sensitive formal language and find that Transformers trained to perform
tasks on top of strings from this language indeed exhibit emergent
capabilities. Specifically, we show that once the language's underlying grammar
and context-sensitivity inducing structures are learned by the model,
performance on narrower tasks suddenly begins to improve. We then analogize our
network's learning dynamics with the process of percolation on a bipartite
graph, establishing a formal phase transition model that predicts the shift in
the point of emergence observed in our experiments when changing the data
structure. Overall, our experimental and theoretical frameworks yield a step
towards better defining, characterizing, and predicting emergence in neural
networks.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñô„ÄÅË¶èÊ®°ÊàñÈÅãÁÆóÁöÑÂ¢ûÂä†ÔºåÂèØËÉΩÊúÉÂ∞éËá¥Á•ûÁ∂ìÁ∂≤Ë∑ØÁ™ÅÁÑ∂Â≠∏ÊúÉÁâπÂÆöËÉΩÂäõ‚Äî‚ÄîÈÄôÁ®ÆÁèæË±°Â∏∏Á®±ÁÇ∫„ÄåÊπßÁèæ„Äç„ÄÇÈô§‰∫ÜÁßëÂ≠∏ÁêÜËß£‰πãÂ§ñÔºåÁ¢∫Á´ãÈÄôÁ®ÆÊπßÁèæËÉΩÂäõËÉåÂæåÁöÑÂü∫Êú¨ÂéüÂõ†ÔºåÂ∞çÊñºÁÇ∫ AI Âª∫Á´ãÈ¢®Èö™Ê≥ïË¶èÊ°ÜÊû∂Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂæûÂÖ∂‰ªñÈ†òÂüü‰∏≠Â∞çÊπßÁèæÁâπÊÄßÁöÑÁ†îÁ©∂‰∏≠Â∞ãÊ±ÇÈùàÊÑüÔºå‰∏¶ÈáùÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊ¶ÇÂøµÊèêÂá∫ÁèæË±°Â≠∏ÂÆöÁæ©„ÄÇÊàëÂÄëÁöÑÂÆöÁæ©ÊöóÁ§∫ÔºåÂèñÂæóË≥áÊñôÁî¢ÁîüÁ®ãÂ∫èËÉåÂæåÁöÑÈÄöÁî®ÁµêÊßãÔºåÊòØÁâπÂÆö„ÄÅËºÉÁãπÈöò‰ªªÂãôÁ™ÅÁÑ∂ÊïàËÉΩÊèêÂçáÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄã‰ª•ÊÉÖÂ¢ÉÊïèÊÑüÂΩ¢ÂºèË™ûË®ÄÁÇ∫Âü∫Á§éÁöÑÂØ¶È©óÁ≥ªÁµ±ÔºåÂ∞çÈÄôÂÄãÂÆöÁæ©ÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂ÔºåÁôºÁèæÁ∂ìÈÅéË®ìÁ∑¥‰ª•Âü∑Ë°åÈÄôÂÄãË™ûË®Ä‰∏≠Â≠ó‰∏≤È†ÇÈÉ®‰ªªÂãôÁöÑ TransformerÔºåÁ¢∫ÂØ¶Â±ïÁèæÂá∫ÊπßÁèæËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ±ïÁ§∫Âá∫Ê®°Âûã‰∏ÄÊó¶Â≠∏ÊúÉË™ûË®ÄÁöÑÂ∫ïÂ±§ÊñáÊ≥ïÂíåÊÉÖÂ¢ÉÊïèÊÑüË™òÂ∞éÁµêÊßãÔºåÂ∞çËºÉÁãπÈöò‰ªªÂãôÁöÑÊïàËÉΩÂ∞±ÊúÉÁ™ÅÁÑ∂ÈñãÂßãÊèêÂçá„ÄÇÊé•ËëóÊàëÂÄëÂ∞áÁ∂≤Ë∑ØÁöÑÂ≠∏ÁøíÂãïÊÖãÈ°ûÊØîÁÇ∫‰∫åÈÉ®Âúñ‰∏äÁöÑÊª≤ÊµÅÈÅéÁ®ãÔºåÂª∫Á´ã‰∏ÄÂÄãÊ≠£ÂºèÁöÑÁõ∏ËÆäÊ®°ÂûãÔºåÁî®ÊñºÈ†êÊ∏¨Âú®ÊîπËÆäË≥áÊñôÁµêÊßãÊôÇÔºåÊàëÂÄëÂú®ÂØ¶È©ó‰∏≠ËßÄÂØüÂà∞ÁöÑÊπßÁèæÈªû‰ΩçÁßª„ÄÇÊï¥È´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂØ¶È©óÂíåÁêÜË´ñÊ°ÜÊû∂ÊúùËëóÊõ¥ÂÆåÂñÑÂú∞ÂÆöÁæ©„ÄÅÊèèËø∞ÂíåÈ†êÊ∏¨Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊπßÁèæÈÇÅÈÄ≤‰∫Ü‰∏ÄÊ≠•„ÄÇ</paragraph>

##### **Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**
2409.00061v1 by Arief Purnama Muharram, Ayu Purwarianti

Automated fact-checking is a key strategy to overcome the spread of COVID-19
misinformation on the internet. These systems typically leverage deep learning
approaches through Natural Language Inference (NLI) to verify the truthfulness
of information based on supporting evidence. However, one challenge that arises
in deep learning is performance stagnation due to a lack of knowledge during
training. This study proposes using a Knowledge Graph (KG) as external
knowledge to enhance NLI performance for automated COVID-19 fact-checking in
the Indonesian language. The proposed model architecture comprises three
modules: a fact module, an NLI module, and a classifier module. The fact module
processes information from the KG, while the NLI module handles semantic
relationships between the given premise and hypothesis. The representation
vectors from both modules are concatenated and fed into the classifier module
to produce the final result. The model was trained using the generated
Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.
Our study demonstrates that incorporating KGs can significantly improve NLI
performance in fact-checking, achieving the best accuracy of 0,8616. This
suggests that KGs are a valuable component for enhancing NLI performance in
automated fact-checking.

ÊëòË¶ÅÔºöËá™Âãï‰∫ãÂØ¶Êü•Ê†∏ÊòØÂÖãÊúçÁ∂≤Ë∑Ø‰∏ä COVID-19 ÈåØË™§Ë≥áË®äÊï£Êí≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµÁ≠ñÁï•„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÈÄöÂ∏∏ÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) ‰æÜÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÊ†πÊìöÊîØÊè¥Ë≠âÊìöÈ©óË≠âË≥áË®äÁöÑÁúüÂØ¶ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂú®Ê∑±Â∫¶Â≠∏Áøí‰∏≠ÊúÉÂá∫Áèæ‰∏ÄÂÄãÊåëÊà∞ÔºåÈÇ£Â∞±ÊòØÂú®Ë®ìÁ∑¥ÊúüÈñìÂõ†Áº∫‰πèÁü•Ë≠òËÄåÂ∞éËá¥ÊïàËÉΩÂÅúÊªØ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®Áü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Â§ñÈÉ®Áü•Ë≠òÔºå‰ª•Â¢ûÂº∑Ëá™ÂãïÂåñ COVID-19 ‰∫ãÂØ¶Êü•Ê†∏ÁöÑ NLI ÊïàËÉΩÔºå‰∏¶‰ª•Âç∞Â∞ºË™ûÈÄ≤Ë°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊû∂ÊßãÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö‰∫ãÂØ¶Ê®°ÁµÑ„ÄÅNLI Ê®°ÁµÑÂíåÂàÜÈ°ûÂô®Ê®°ÁµÑ„ÄÇ‰∫ãÂØ¶Ê®°ÁµÑËôïÁêÜ‰æÜËá™ KG ÁöÑË≥áË®äÔºåËÄå NLI Ê®°ÁµÑÂâáËôïÁêÜÁµ¶ÂÆöÂâçÊèêÂíåÂÅáË®≠‰πãÈñìÁöÑË™ûÁæ©Èóú‰øÇ„ÄÇ‰æÜËá™ÂÖ©ÂÄãÊ®°ÁµÑÁöÑË°®Á§∫ÂêëÈáèÊúÉ‰∏≤Êé•Ëµ∑‰æÜÔºå‰∏¶Ëº∏ÂÖ•ÂàÜÈ°ûÂô®Ê®°ÁµÑ‰ª•Áî¢ÁîüÊúÄÁµÇÁµêÊûú„ÄÇÊ≠§Ê®°Âûã‰ΩøÁî®Áî¢ÁîüÁöÑÂç∞Â∞ºË™û COVID-19 ‰∫ãÂØ¶Êü•Ê†∏Ë≥áÊñôÈõÜÂíå COVID-19 KG Bahasa Indonesia ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë≠âÊòéÔºåÁ¥çÂÖ• KG ÂèØ‰ª•È°ØËëóÊîπÂñÑ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ÁöÑ NLI ÊïàËÉΩÔºåÈÅîÂà∞ 0.8616 ÁöÑÊúÄ‰Ω≥Ê∫ñÁ¢∫Â∫¶„ÄÇÈÄôË°®Á§∫ KG ÊòØÂ¢ûÂº∑Ëá™ÂãïÂåñ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ NLI ÊïàËÉΩÁöÑÂØ∂Ë≤¥ÁµÑÊàêÈÉ®ÂàÜ„ÄÇ

##### **Cell-ontology guided transcriptome foundation model**
2408.12373v1 by Xinyu Yuan, Zhihao Zhan, Zuobai Zhang, Manqi Zhou, Jianan Zhao, Boyu Han, Yue Li, Jian Tang

Transcriptome foundation models TFMs hold great promises of deciphering the
transcriptomic language that dictate diverse cell functions by self-supervised
learning on large-scale single-cell gene expression data, and ultimately
unraveling the complex mechanisms of human diseases. However, current TFMs
treat cells as independent samples and ignore the taxonomic relationships
between cell types, which are available in cell ontology graphs. We argue that
effectively leveraging this ontology information during the TFM pre-training
can improve learning biologically meaningful gene co-expression patterns while
preserving TFM as a general purpose foundation model for downstream zero-shot
and fine-tuning tasks. To this end, we present \textbf{s}ingle \textbf{c}ell,
\textbf{Cell}-\textbf{o}ntology guided TFM scCello. We introduce cell-type
coherence loss and ontology alignment loss, which are minimized along with the
masked gene expression prediction loss during the pre-training. The novel loss
component guide scCello to learn the cell-type-specific representation and the
structural relation between cell types from the cell ontology graph,
respectively. We pre-trained scCello on 22 million cells from CellxGene
database leveraging their cell-type labels mapped to the cell ontology graph
from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates
competitive generalization and transferability performance over the existing
TFMs on biologically important tasks including identifying novel cell types of
unseen cells, prediction of cell-type-specific marker genes, and cancer drug
responses.

ÊëòË¶ÅÔºö<paragraph>ËΩâÈåÑÁµÑÂü∫Á§éÊ®°Âûã TFM ÊâøË´æËß£Á¢ºËΩâÈåÑÁµÑË™ûË®ÄÔºåÂÆÉÈÄèÈÅéÂú®Â§ßÂûãÂñÆÁ¥∞ËÉûÂü∫Âõ†Ë°®ÁèæË≥áÊñô‰∏äÈÄ≤Ë°åËá™ÊàëÁõ£Áù£Â≠∏ÁøíÔºå‰æÜÊ±∫ÂÆö‰∏çÂêåÁöÑÁ¥∞ËÉûÂäüËÉΩÔºå‰∏¶ÊúÄÁµÇËß£Èñã‰∫∫È°ûÁñæÁóÖÁöÑË§áÈõúÊ©üÂà∂„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ TFM Â∞áÁ¥∞ËÉûË¶ñÁÇ∫Áç®Á´ãÊ®£Êú¨Ôºå‰∏¶ÂøΩÁï•Á¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÂàÜÈ°ûÈóú‰øÇÔºåËÄåÈÄôÂú®Á¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠ÊòØÂèØÁî®ÁöÑ„ÄÇÊàëÂÄëË™çÁÇ∫Âú® TFM È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂà©Áî®Ê≠§Êú¨È´îË´ñË≥áË®äÔºåÂèØ‰ª•ÊîπÂñÑÂ≠∏ÁøíÁîüÁâ©Â≠∏‰∏äÊúâÊÑèÁæ©ÁöÑÂü∫Âõ†ÂÖ±Ë°®ÁèæÊ®°ÂºèÔºåÂêåÊôÇ‰øùÁïô TFM ‰ΩúÁÇ∫‰∏ãÊ∏∏Èõ∂Ê¨°Â≠∏ÁøíÂíåÂæÆË™ø‰ªªÂãôÁöÑ‰∏ÄËà¨Áî®ÈÄîÂü∫Á§éÊ®°Âûã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ÂñÆÁ¥∞ËÉû„ÄÅÁ¥∞ËÉûÊú¨È´îË´ñÂºïÂ∞éÁöÑ TFM scCello„ÄÇÊàëÂÄëÂºïÂÖ•Á¥∞ËÉûÈ°ûÂûã‰∏ÄËá¥ÊÄßÊêçÂ§±ÂíåÊú¨È´îË´ñÂ∞çÈΩäÊêçÂ§±ÔºåÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúÉÂ∞áÂÖ∂ËàáÈÅÆÁΩ©Âü∫Âõ†Ë°®ÁèæÈ†êÊ∏¨ÊêçÂ§±‰∏ÄËµ∑ÊúÄÂ∞èÂåñ„ÄÇÈÄôÂÄãÊñ∞Á©éÁöÑÊêçÂ§±ÁµÑ‰ª∂ÂºïÂ∞é scCello ÂàÜÂà•ÂæûÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠Â≠∏ÁøíÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöË°®Á§∫ÂíåÁ¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÁµêÊßãÈóú‰øÇ„ÄÇÊàëÂÄëÂú® CellxGene Ë≥áÊñôÂ∫´‰∏≠Â∞ç 2200 Ëê¨ÂÄãÁ¥∞ËÉûÈÄ≤Ë°å scCello È†êË®ìÁ∑¥ÔºåÂà©Áî®ÂÖ∂Á¥∞ËÉûÈ°ûÂûãÊ®ôÁ±§Â∞çÊáâÂà∞ÈñãÊîæÁîüÁâ©ÂíåÁîüÁâ©ÈÜ´Â≠∏Êú¨È´îÈëÑÈÄ†Âª†ÁöÑÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®„ÄÇÊàëÂÄëÁöÑ TFM Âú®ÁîüÁâ©Â≠∏‰∏äÈáçË¶ÅÁöÑ‰ªªÂãô‰∏äÂ±ïÁ§∫‰∫ÜÊØîÁèæÊúâ TFM Êõ¥ÂÖ∑Á´∂Áà≠ÂäõÁöÑÊ≥õÂåñÂíåÂèØËΩâÁßªÊÄßÔºåÂåÖÊã¨Ë≠òÂà•Êú™Ë¶ãÁ¥∞ËÉûÁöÑÊñ∞Á¥∞ËÉûÈ°ûÂûã„ÄÅÈ†êÊ∏¨Á¥∞ËÉûÈ°ûÂûãÁâπÂÆöÊ®ôË®òÂü∫Âõ†ÂíåÁôåÁóáËó•Áâ©ÂèçÊáâ„ÄÇ</paragraph>

##### **Graph Retrieval Augmented Trustworthiness Reasoning**
2408.12333v2 by Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.

ÊëòË¶ÅÔºö<paragraph>Âú®‰ø°ÊÅØ‰∏çÂÆåÊï¥ÁöÑÂ§ö‰∫∫ÈÅäÊà≤‰∏≠ÔºåÂèØ‰ø°Â∫¶Êé®ÁêÜËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ª£ÁêÜ‰∫∫ËÉΩÂ§†Ë≠òÂà•ÊΩõÂú®ÁöÑÁõüÂèãÂíåÊïµ‰∫∫ÔºåÂæûËÄåÂ¢ûÂº∑Êé®ÁêÜÂíåÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ã„ÄÇ‰æùË≥¥È†êÂÖàË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑÁâπÂÆöÈ†òÂüüÊï∏ÊìöÂíåÂ§ßÈáèÁöÑÁçéÂãµÂõûÈ•ãÔºåËÄåÂÆÉÂÄëÁº∫‰πèÂØ¶ÊôÇÈÅ©ÊáâÊÄßÊúÉÈòªÁ§ôÂÆÉÂÄëÂú®ÂãïÊÖãÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂúñÂΩ¢Ê™¢Á¥¢Â¢ûÂº∑Êé®ÁêÜ (GRATR) Ê°ÜÊû∂ÔºåÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰æÜÂä†Âº∑‰ª£ÁêÜ‰∫∫ÁöÑÂèØ‰ø°Â∫¶Êé®ÁêÜ„ÄÇGRATR ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂãïÊÖãÂèØ‰ø°Â∫¶ÂúñÂΩ¢Ôºå‰∏¶‰ΩøÁî®Ë≠âÊìö‰ø°ÊÅØÂØ¶ÊôÇÊõ¥Êñ∞ÂÆÉÔºå‰∏¶Ê™¢Á¥¢Áõ∏ÈóúÁöÑ‰ø°‰ªªÊï∏Êìö‰ª•Â¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÈÄöÈÅéÂ§ö‰∫∫ÈÅäÊà≤„ÄåÁãº‰∫∫„ÄçÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂ∞á GRATR ËàáÂü∫Ê∫ñ LLM Âíå‰ΩøÁî® Native RAG Âíå Rerank RAG Â¢ûÂº∑ÁöÑ LLM ÈÄ≤Ë°å‰∫ÜÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåGRATR Âú®Áç≤ÂãùÁéá‰∏äÊØîÂü∫Ê∫ñÊñπÊ≥ïÈ´òÂá∫ 30%ÔºåÂÖ∑ÊúâÂçìË∂äÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåGRATR ÊúâÊïàÂú∞Ê∏õËºï‰∫Ü LLM ÁöÑÂπªË¶∫Ôºå‰æãÂ¶ÇË∫´‰ªΩÂíåÁõÆÊ®ôÂÅ•ÂøòÁóáÔºåÊúÄÈáçË¶ÅÁöÑÊòØÔºåÂÆÉÈÄöÈÅé‰ΩøÁî®ÂèØ‰ø°Â∫¶ÂúñÂΩ¢‰ΩøÊé®ÁêÜÈÅéÁ®ãÊõ¥ÈÄèÊòé‰∏îÂèØËøΩËπ§„ÄÇ</paragraph>

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÊïôËÇ≤È´òÂ∫¶‰æùË≥¥Ê®°Êì¨ÁóÖ‰∫∫ (SP) Êèê‰æõ‰∏ÄÂÄãÂÆâÂÖ®ÁöÑÁí∞Â¢ÉÔºåËÆìÂ≠∏ÁîüÁ∑¥ÁøíËá®Â∫äÊäÄËÉΩÔºåÂåÖÊã¨ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÊãõÂãüÂêàÊ†º SP ÁöÑÈ´òÊàêÊú¨ÂíåÁº∫‰πèÂ§öÊ®£ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÂ∑≤ÈÄ†ÊàêÈ°ØËëóÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π MedDiTÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÁü•Ë≠òÊéßÂà∂Â∞çË©±Êû∂ÊßãÔºåÂÆÉÂèØ‰ª•ÂãïÊÖãÁî¢ÁîüÁ¨¶ÂêàÊ®°Êì¨ÁóÖ‰∫∫ÁóáÁãÄÁöÑÂêàÁêÜÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂØ¶ÁèæÂ§öÊ®£ÁöÑË®∫Êñ∑ÊäÄËÉΩË®ìÁ∑¥„ÄÇÂÖ∑È´î‰æÜË™™ÔºåMedDiT Êï¥Âêà‰∫ÜÂêÑÁ®ÆÁóÖ‰∫∫Áü•Ë≠òÂúñË≠ú (KG)ÔºåÊèèËø∞ÁóÖ‰∫∫ÁöÑÂ±¨ÊÄßÂíåÁóáÁãÄÔºå‰ª•ÂãïÊÖãÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË°åÁÇ∫Ôºå‰∏¶ÊéßÂà∂ÁóÖ‰∫∫ÁâπÂæµÔºåÊ∏õËºïÈÜ´Â≠∏Â∞çË©±‰∏≠ÁöÑÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¥çÂÖ•‰∏ÄÂÄãÁ∂ìÈÅéÂæÆË™øÁöÑÊì¥Êï£Transformer (DiT) Ê®°ÂûãÔºåÊ†πÊìö KG ‰∏≠ÊåáÂÆöÁöÑÁóÖ‰∫∫Â±¨ÊÄßÁî¢ÁîüÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂØ¶ÈöõÁ§∫ÁØÑÂ±ïÁ§∫ MedDiT ÁöÑÂäüËÉΩÔºåÂ±ïÁ§∫ÂÆÉÂú®‰∏çÂêåÊ®°Êì¨ÁóÖ‰∫∫Ê°à‰æã‰∏≠‰ΩúÁî®‰∏¶Áî¢ÁîüÁõ∏ÊáâÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑËÉΩÂäõ„ÄÇÈÄôÂèØ‰ª•ÁÇ∫Â≠∏ÁîüÊèê‰æõË±êÂØå‰∏î‰∫íÂãïÁöÑÂ≠∏ÁøíÈ´îÈ©óÔºåÈÄèÈÅéÊèê‰æõË∫´Ê≠∑ÂÖ∂Â¢ÉÁöÑÊ®°Êì¨Âπ≥Âè∞ÔºåÊèêÂçáÈÜ´Â≠∏ÊïôËÇ≤ÔºåÈÄ†Á¶èÊú™‰æÜÁöÑÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈó°Êòé‰∫ÜÂú®ÊïôËÇ≤ÊáâÁî®‰∏≠Êï¥Âêà LLM„ÄÅKG Âíå DiT Á≠âÂÖàÈÄ≤ÊäÄË°ìÁöÑÂèØË°åÊÄßÔºåÁ™ÅÈ°ØÂÆÉÂÄëÂú®Ëß£Ê±∫Ê®°Êì¨ÁóÖ‰∫∫ÁÇ∫Âü∫Á§éÁöÑÈÜ´Â≠∏ÊïôËÇ≤ÊâÄÈù¢Ëá®ÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ

##### **Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**
2408.12116v1 by Junlin He, Tong Nie, Wei Ma

In the geospatial domain, universal representation models are significantly
less prevalent than their extensive use in natural language processing and
computer vision. This discrepancy arises primarily from the high costs
associated with the input of existing representation models, which often
require street views and mobility data. To address this, we develop a novel,
training-free method that leverages large language models (LLMs) and auxiliary
map data from OpenStreetMap to derive geolocation representations (LLMGeovec).
LLMGeovec can represent the geographic semantics of city, country, and global
scales, which acts as a generic enhancer for spatio-temporal learning.
Specifically, by direct feature concatenation, we introduce a simple yet
effective paradigm for enhancing multiple spatio-temporal tasks including
geographic prediction (GP), long-term time series forecasting (LTSF), and
graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly
integrate into a wide spectrum of spatio-temporal learning models, providing
immediate enhancements. Experimental results demonstrate that LLMGeovec
achieves global coverage and significantly boosts the performance of leading
GP, LTSF, and GSTF models.

ÊëòË¶ÅÔºöÂú®Á©∫ÈñìÂú∞ÁêÜÈ†òÂüüÔºåÈÄöÁî®Ë°®Á§∫Ê®°ÂûãÈ°ØËëóÂ∞ëÊñºÂÆÉÂÄëÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫‰∏≠ÁöÑÂª£Ê≥õ‰ΩøÁî®„ÄÇÈÄôÁ®ÆÂ∑ÆÁï∞‰∏ªË¶ÅÊ∫êÊñºÁèæÊúâË°®Á§∫Ê®°ÂûãÁöÑËº∏ÂÖ•ÊàêÊú¨È´òÔºåÈÄôÈÄöÂ∏∏ÈúÄË¶ÅË°óÊôØÂíåÊµÅÂãïÊÄßË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂÖçË®ìÁ∑¥ÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå OpenStreetMap ÁöÑËºîÂä©Âú∞ÂúñË≥áÊñô‰æÜÊé®Â∞éÂú∞ÁêÜ‰ΩçÁΩÆË°®Á§∫ (LLMGeovec)„ÄÇLLMGeovec ÂèØ‰ª•Ë°®Á§∫ÂüéÂ∏Ç„ÄÅÂúãÂÆ∂ÂíåÂÖ®ÁêÉË¶èÊ®°ÁöÑÂú∞ÁêÜË™ûÁæ©Ôºå‰ΩúÁÇ∫ÊôÇÁ©∫Â≠∏ÁøíÁöÑÈÄöÁî®Â¢ûÂº∑Âô®„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈÄöÈÅéÁõ¥Êé•ÁâπÂæµ‰∏≤Êé•ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂ¢ûÂº∑Â§öÂÄãÊôÇÁ©∫‰ªªÂãôÔºåÂåÖÊã¨Âú∞ÁêÜÈ†êÊ∏¨ (GP)„ÄÅÈï∑ÊúüÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ (LTSF) ÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑÊôÇÁ©∫È†êÊ∏¨ (GSTF)„ÄÇLLMGeovec ÂèØ‰ª•ÁÑ°Á∏´Êï¥ÂêàÂà∞Âª£Ê≥õÁöÑÊôÇÁ©∫Â≠∏ÁøíÊ®°Âûã‰∏≠ÔºåÊèê‰æõÁ´ãÂç≥ÁöÑÂ¢ûÂº∑„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLMGeovec ÈÅîÂà∞‰∫ÜÂÖ®ÁêÉË¶ÜËìãÁéáÔºå‰∏¶È°ØËëóÊèêÂçá‰∫ÜÈ†òÂÖàÁöÑ GP„ÄÅLTSF Âíå GSTF Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇ

##### **Enabling Small Models for Zero-Shot Classification through Model Label Learning**
2408.11449v1 by Jia Zhang, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li

Vision-language models (VLMs) like CLIP have demonstrated impressive
zero-shot ability in image classification tasks by aligning text and images but
suffer inferior performance compared with task-specific expert models. On the
contrary, expert models excel in their specialized domains but lack zero-shot
ability for new tasks. How to obtain both the high performance of expert models
and zero-shot ability is an important research direction. In this paper, we
attempt to demonstrate that by constructing a model hub and aligning models
with their functionalities using model labels, new tasks can be solved in a
zero-shot manner by effectively selecting and reusing models in the hub. We
introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap
between models and their functionalities through a Semantic Directed Acyclic
Graph (SDAG) and leverages an algorithm, Classification Head Combination
Optimization (CHCO), to select capable models for new tasks. Compared with the
foundation model paradigm, it is less costly and more scalable, i.e., the
zero-shot ability grows with the sizes of the model hub. Experiments on seven
real-world datasets validate the effectiveness and efficiency of MLL,
demonstrating that expert models can be effectively reused for zero-shot tasks.
Our code will be released publicly.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºàVLMÔºâÔºå‰æãÂ¶Ç CLIPÔºåÂ∑≤Âú®ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰∏≠Â±ïÁèæ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊñπÊ≥ïÊòØÂ∞çÈΩäÊñáÂ≠óÂíåÂΩ±ÂÉèÔºå‰ΩÜËàáÁâπÂÆö‰ªªÂãôÁöÑÂ∞àÂÆ∂Ê®°ÂûãÁõ∏ÊØîÔºåÂÖ∂ÊïàËÉΩËºÉÂ∑Æ„ÄÇÁõ∏ÂèçÂú∞ÔºåÂ∞àÂÆ∂Ê®°ÂûãÂú®ÂÖ∂Â∞àÊ•≠È†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂ∞çÊñºÊñ∞‰ªªÂãôÁº∫‰πèÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÂ¶Ç‰ΩïÂêåÊôÇÁç≤ÂæóÂ∞àÂÆ∂Ê®°ÂûãÁöÑÈ´òÊïàËÉΩÂíåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶ÈÄèÈÅéÂª∫Á´ãÊ®°Âûã‰∏≠ÂøÉÔºå‰∏¶‰ΩøÁî®Ê®°ÂûãÊ®ôÁ±§Â∞áÊ®°ÂûãËàáÂÖ∂ÂäüËÉΩÂ∞çÈΩäÔºåË≠âÊòéÂèØ‰ª•ÈÄèÈÅéÊúâÊïàÈÅ∏ÊìáÂíåÈáçË§á‰ΩøÁî®‰∏≠ÂøÉ‰∏≠ÁöÑÊ®°ÂûãÔºå‰ª•Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊñπÂºèËß£Ê±∫Êñ∞‰ªªÂãô„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÁØÑ‰æãÔºåÂç≥Ê®°ÂûãÊ®ôÁ±§Â≠∏ÁøíÔºàMLLÔºâÔºåÂÆÉÈÄèÈÅéË™ûÁæ©Â∞éÂêëÈùûÂæ™Áí∞ÂúñÔºàSDAGÔºâÂΩåÂêàÊ®°ÂûãÂèäÂÖ∂ÂäüËÉΩ‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰∏¶Âà©Áî®‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÂç≥ÂàÜÈ°ûÈ†≠ÁµÑÂêàÊúÄ‰Ω≥ÂåñÔºàCHCOÔºâÔºåÁÇ∫Êñ∞‰ªªÂãôÈÅ∏ÊìáÊúâËÉΩÂäõÁöÑÊ®°Âûã„ÄÇËàáÂü∫Á§éÊ®°ÂûãÁØÑ‰æãÁõ∏ÊØîÔºåÂÆÉÁöÑÊàêÊú¨ËºÉ‰Ωé‰∏îÊõ¥ÂÖ∑ÂèØÊì¥ÂÖÖÊÄßÔºå‰πüÂ∞±ÊòØË™™ÔºåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÊúÉÈö®ËëóÊ®°Âûã‰∏≠ÂøÉË¶èÊ®°ÁöÑÊì¥Â§ßËÄåÂ¢ûÈï∑„ÄÇÂú®‰∏ÉÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü MLL ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºåË≠âÊòé‰∫ÜÂ∞àÂÆ∂Ê®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞ÈáçË§áÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÁôºÂ∏É„ÄÇ

##### **Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**
2408.11182v1 by Zhilong Wang, Haizhou Wang, Nanqing Luo, Lan Zhang, Xiaoyan Sun, Yebo Cao, Peng Liu

Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts
aimed at exploiting the models to generate malicious content. This paper
proposes a new type of jailbreak attacks which shift the attention of the LLM
by inserting a prohibited query into a carrier article. The proposed attack
leverage the knowledge graph and a composer LLM to automatically generating a
carrier article that is similar to the topic of the prohibited query but does
not violate LLM's safeguards. By inserting the malicious query to the carrier
article, the assembled attack payload can successfully jailbreak LLM. To
evaluate the effectiveness of our method, we leverage 4 popular categories of
``harmful behaviors'' adopted by related researches to attack 6 popular LLMs.
Our experiment results show that the proposed attacking method can successfully
jailbreak all the target LLMs which high success rate, except for Claude-3.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°ÂûãÊ®°ÂûãÔºàLLMÔºâÁöÑË∂äÁçÑÊîªÊìäÊ∂âÂèäË£Ω‰ΩúÊèêÁ§∫ÔºåÊó®Âú®Âà©Áî®Ê®°Âûã‰æÜÁî¢ÁîüÊÉ°ÊÑèÂÖßÂÆπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÂûãÁöÑË∂äÁçÑÊîªÊìäÔºåÂÆÉÈÄöÈÅéÂú®ËºâÈ´îÊñáÁ´†‰∏≠ÊèíÂÖ•Á¶ÅÊ≠¢Êü•Ë©¢‰æÜËΩâÁßª LLM ÁöÑÊ≥®ÊÑèÂäõ„ÄÇÊèêË≠∞ÁöÑÊîªÊìäÂà©Áî®Áü•Ë≠òÂúñË≠úÂíå‰ΩúÊõ≤ÂÆ∂ LLM Ëá™ÂãïÁîüÊàêËàáÁ¶ÅÊ≠¢Êü•Ë©¢ÁöÑ‰∏ªÈ°åÁõ∏‰ºº‰ΩÜ‰∏çÊúÉÈÅïÂèç LLM ‰øùÈöúÊé™ÊñΩÁöÑËºâÈ´îÊñáÁ´†„ÄÇÈÄöÈÅéÂ∞áÊÉ°ÊÑèÊü•Ë©¢ÊèíÂÖ•ËºâÈ´îÊñáÁ´†‰∏≠ÔºåÁµÑË£ùÁöÑÊîªÊìäÊúâÊïàËºâËç∑ÂèØ‰ª•ÊàêÂäüË∂äÁçÑ LLM„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂà©Áî®Áõ∏ÈóúÁ†îÁ©∂Êé°Áî®ÁöÑ 4 È°ûÊµÅË°åÁöÑ„ÄåÊúâÂÆ≥Ë°åÁÇ∫„Äç‰æÜÊîªÊìä 6 ÂÄãÊµÅË°åÁöÑ LLM„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊîªÊìäÊñπÊ≥ïÂèØ‰ª•ÊàêÂäüË∂äÁçÑÊâÄÊúâÁõÆÊ®ô LLMÔºåÊàêÂäüÁéáÂæàÈ´òÔºåÈô§‰∫Ü Claude-3„ÄÇ

##### **Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**
2408.11133v1 by Thomas Hoang, Quynh Anh Nguyen, Long Nguyen

Countless disasters have resulted from climate change, causing severe damage
to infrastructure and the economy. These disasters have significant societal
impacts, necessitating mental health services for the millions affected. To
prepare for and respond effectively to such events, it is important to
understand people's emotions and the life incidents they experience before and
after a disaster strikes. In this case study, we collected a dataset of
approximately 400,000 public tweets related to the storm. Using a BERT-based
model, we predicted the emotions associated with each tweet. To efficiently
identify these topics, we utilized the Latent Dirichlet Allocation (LDA)
technique for topic modeling, which allowed us to bypass manual content
analysis and extract meaningful patterns from the data. However, rather than
stopping at topic identification like previous methods \cite{math11244910}, we
further refined our analysis by integrating Graph Neural Networks (GNN) and
Large Language Models (LLM). The GNN was employed to generate embeddings and
construct a similarity graph of the tweets, which was then used to optimize
clustering. Subsequently, we used an LLM to automatically generate descriptive
names for each event cluster, offering critical insights for disaster
preparedness and response strategies.

ÊëòË¶ÅÔºöÁÑ°Êï∏ÁöÑÁÅΩÈõ£ÊòØÁî±ÊñºÊ∞£ÂÄôËÆäÈÅ∑ÊâÄÈÄ†ÊàêÁöÑÔºåÂ∞çÂü∫Á§éÂª∫Ë®≠ÂíåÁ∂ìÊøüÈÄ†ÊàêÂö¥ÈáçÁöÑÊêçÂÆ≥„ÄÇÈÄô‰∫õÁÅΩÈõ£Â∞çÁ§æÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÂΩ±ÈüøÔºåÈúÄË¶ÅÁÇ∫Êï∏ÁôæËê¨ÂèóÁÅΩÊ∞ëÁúæÊèê‰æõÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãô„ÄÇÁÇ∫‰∫ÜÊúâÊïàÂú∞ÁÇ∫Ê≠§È°û‰∫ã‰ª∂ÂÅöÂ•ΩÊ∫ñÂÇô‰∏¶‰ΩúÂá∫ÂõûÊáâÔºå‰∫ÜËß£‰∫∫ÂÄëÁöÑÊÉÖÁ∑í‰ª•Âèä‰ªñÂÄëÂú®ÁÅΩÈõ£ÁôºÁîüÂâçÂæåÊâÄÁ∂ìÊ≠∑ÁöÑÁîüÊ¥ª‰∫ã‰ª∂ÈùûÂ∏∏ÈáçË¶Å„ÄÇÂú®Êú¨Ê°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÂåÖÂê´Á¥Ñ 400,000 ÂâáËàáÈ¢®Êö¥Áõ∏ÈóúÁöÑÂÖ¨ÈñãÊé®ÊñáÁöÑË≥áÊñôÈõÜ„ÄÇ‰ΩøÁî®Âü∫Êñº BERT ÁöÑÊ®°ÂûãÔºåÊàëÂÄëÈ†êÊ∏¨‰∫ÜËàáÊØèÂâáÊé®ÊñáÁõ∏ÈóúÁöÑÊÉÖÁ∑í„ÄÇÁÇ∫‰∫ÜÊúâÊïàÁéáÂú∞ÊâæÂá∫ÈÄô‰∫õ‰∏ªÈ°åÔºåÊàëÂÄëÂà©Áî®‰∫ÜÊΩõÂú®ÁãÑÂà©ÂÖãÈõ∑ÈÖçÁΩÆ (LDA) ÊäÄË°ìÈÄ≤Ë°å‰∏ªÈ°åÂª∫Ê®°ÔºåÈÄôËÆìÊàëÂÄëËÉΩÂ§†ÁπûÈÅéÊâãÂãïÂÖßÂÆπÂàÜÊûêÔºåÂæûË≥áÊñô‰∏≠ËêÉÂèñÂá∫ÊúâÊÑèÁæ©ÁöÑÊ®°Âºè„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰∏¶Êú™ÂÉèÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ï \cite{math11244910} ÈÇ£Ê®£ÂÉÖÊ≠¢Êñº‰∏ªÈ°åËæ®Ë≠òÔºåËÄåÊòØÈÄ≤‰∏ÄÊ≠•Êï¥ÂêàÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂÑ™ÂåñÊàëÂÄëÁöÑÂàÜÊûê„ÄÇGNN Ë¢´Áî®ÊñºÁî¢ÁîüÂµåÂÖ•ÂíåÂª∫ÊßãÊé®ÊñáÁöÑÁõ∏‰ººÊÄßÂúñÔºåÁÑ∂ÂæåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂàÜÁæ§„ÄÇÈö®ÂæåÔºåÊàëÂÄë‰ΩøÁî® LLM ÁÇ∫ÊØèÂÄã‰∫ã‰ª∂Áæ§ÈõÜËá™ÂãïÁî¢ÁîüÊèèËø∞ÊÄßÂêçÁ®±ÔºåÁÇ∫ÁÅΩÂÆ≥Èò≤ÁØÑÂíåÊáâËÆäÁ≠ñÁï•Êèê‰æõÈáçË¶ÅÁöÑË¶ãËß£„ÄÇ

##### **Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**
2408.10819v1 by Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou

Knowledge graph completion (KGC) aims to identify missing triples in a
knowledge graph (KG). This is typically achieved through tasks such as link
prediction and instance completion. However, these methods often focus on
either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),
addressing only within-scope triples. This paper introduces a new generative
completion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC
employs a question-answering format to directly generate target entities,
addressing the challenge of questions having multiple possible answers. We
propose a strategy that extracts subgraphs centered on entities and
relationships within the KG, from which negative samples and neighborhood
information are separately obtained to address the one-to-many problem. Our
method generates negative samples using known facts to facilitate the discovery
of new information. Furthermore, we collect and refine neighborhood path data
of known entities, providing contextual information to enhance reasoning in
large language models (LLMs). Our experiments evaluated the proposed method on
four SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five
datasets. Analysis of the results shows that GS-KGC can discover new triples
within existing KGs and generate new facts beyond the closed KG, effectively
bridging the gap between closed-world and open-world KGC.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úË£úÂÖ® (KGC) ÁöÑÁõÆÊ®ôÊòØË≠òÂà•Áü•Ë≠òÂúñË≠ú (KG) ‰∏≠ÈÅ∫Â§±ÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÈÄôÈÄöÂ∏∏ÈÄèÈÅéÈÄ£ÁµêÈ†êÊ∏¨ÂíåÂØ¶‰æãË£úÂÖ®Á≠â‰ªªÂãôÈÅîÊàê„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Â∞àÊ≥®ÊñºÈùúÊÖãÁü•Ë≠òÂúñË≠ú (SKG) ÊàñÊôÇÂ∫èÁü•Ë≠òÂúñË≠ú (TKG)ÔºåÂÉÖËôïÁêÜÁØÑÂúçÂÖßÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∏ÄÂÄãÂêçÁÇ∫ÁîüÊàêÂ≠êÂúñÁÇ∫Âü∫Á§éÁöÑ KGC (GS-KGC) ÁöÑÊñ∞ÁîüÊàêË£úÂÖ®Êû∂Êßã„ÄÇGS-KGC ‰ΩøÁî®ÂïèÁ≠îÊ†ºÂºèÁõ¥Êé•ÁîüÊàêÁõÆÊ®ôÂØ¶È´îÔºå‰ª•Ëß£Ê±∫ÂïèÈ°åÊúâÂ§öÂÄãÂèØËÉΩÁ≠îÊ°àÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ≠ñÁï•ÔºåÂæûÁü•Ë≠òÂúñË≠ú‰∏≠‰ª•ÂØ¶È´îÂíåÈóú‰øÇÁÇ∫‰∏≠ÂøÉÁöÑÂ≠êÂúñÔºåÂæû‰∏≠ÂàÜÂà•ÂèñÂæóË≤†Èù¢Ê®£Êú¨ÂíåÈÑ∞ÂüüË≥áË®äÔºå‰ª•Ëß£Ê±∫‰∏ÄÂ∞çÂ§öÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÊ®°Âûã‰ΩøÁî®Â∑≤Áü•‰∫ãÂØ¶ÁîüÊàêË≤†Èù¢Ê®£Êú¨Ôºå‰ª•Âà©ÁôºÁèæÊñ∞Ë≥áË®ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊî∂ÈõÜ‰∏¶Á≤æÁÖâÂ∑≤Áü•ÂØ¶È´îÁöÑÈÑ∞ÂüüË∑ØÂæëË≥áÊñôÔºåÊèê‰æõËÉåÊôØË≥áË®ä‰ª•Â¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂú®ÂõõÂÄã SKG ÂíåÂÖ©ÂÄã TKG ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÂú®‰∫îÂÄãË≥áÊñôÈõÜ‰∏äÈÅîÊàêÊúÄÂÖàÈÄ≤ÁöÑ Hits@1 ÊåáÊ®ô„ÄÇÁµêÊûúÂàÜÊûêÈ°ØÁ§∫ÔºåGS-KGC ËÉΩÂ§†Âú®ÁèæÊúâÁöÑ KG ‰∏≠ÁôºÁèæÊñ∞ÁöÑ‰∏âÂÖÉÁµÑÔºå‰∏¶ÁîüÊàêÂ∞ÅÈñâ KG ‰ª•Â§ñÁöÑÊñ∞‰∫ãÂØ¶ÔºåÊúâÊïàÂú∞Á∏ÆÂ∞èÂ∞ÅÈñâ‰∏ñÁïåÂíåÈñãÊîæ‰∏ñÁïå KGC ‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**
2408.10592v1 by Litian Huang, Xinguo Yu, Feng Xiong, Bin He, Shengbing Tang, Jiawen Fu

Solving Algebra Problems with Geometry Diagrams (APGDs) is still a
challenging problem because diagram processing is not studied as intensively as
language processing. To work against this challenge, this paper proposes a
hologram reasoning scheme and develops a high-performance method for solving
APGDs by using this scheme. To reach this goal, it first defines a hologram,
being a kind of graph, and proposes a hologram generator to convert a given
APGD into a hologram, which represents the entire information of APGD and the
relations for solving the problem can be acquired from it by a uniform way.
Then HGR, a hologram reasoning method employs a pool of prepared graph models
to derive algebraic equations, which is consistent with the geometric theorems.
This method is able to be updated by adding new graph models into the pool.
Lastly, it employs deep reinforcement learning to enhance the efficiency of
model selection from the pool. The entire HGR not only ensures high solution
accuracy with fewer reasoning steps but also significantly enhances the
interpretability of the solution process by providing descriptions of all
reasoning steps. Experimental results demonstrate the effectiveness of HGR in
improving both accuracy and interpretability in solving APGDs.

ÊëòË¶ÅÔºöÂà©Áî®Âπæ‰ΩïÂúñÂΩ¢ÂúñÔºàAPGDÔºâËß£Ê±∫‰ª£Êï∏ÂïèÈ°å‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÔºåÂõ†ÁÇ∫ÂúñÂΩ¢ËôïÁêÜÁöÑÁ†îÁ©∂‰∏çÂ¶ÇË™ûË®ÄËôïÁêÜÈÇ£È∫ºÊ∑±ÂÖ•„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÖ®ÊÅØÊé®ÁêÜÊñπÊ°àÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ë©≤ÊñπÊ°àËß£Ê±∫ APGD ÁöÑÈ´òÊÄßËÉΩÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞ÈÄôÂÄãÁõÆÊ®ôÔºåÂÆÉÈ¶ñÂÖàÂÆöÁæ©‰∫Ü‰∏ÄÂÄãÂÖ®ÊÅØÂúñÔºå‰ΩúÁÇ∫‰∏ÄÁ®ÆÂúñÂΩ¢Ôºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ®ÊÅØÂúñÁîüÊàêÂô®ÔºåÂ∞áÁµ¶ÂÆöÁöÑ APGD ËΩâÊèõÁÇ∫‰∏ÄÂÄãÂÖ®ÊÅØÂúñÔºåÂÆÉË°®Á§∫ APGD ÁöÑÂÖ®ÈÉ®‰ø°ÊÅØÔºå‰∏¶‰∏îÂèØ‰ª•ÈÄöÈÅéÁµ±‰∏ÄÁöÑÊñπÂºèÂæû‰∏≠Áç≤ÂèñËß£Ê±∫ÂïèÈ°åÁöÑÈóú‰øÇ„ÄÇÁÑ∂ÂæåÔºåHGRÔºå‰∏ÄÁ®ÆÂÖ®ÊÅØÊé®ÁêÜÊñπÊ≥ïÔºåÊé°Áî®‰∏ÄÁµÑÊ∫ñÂÇôÂ•ΩÁöÑÂúñÂΩ¢Ê®°Âûã‰æÜÊé®Â∞é‰ª£Êï∏ÊñπÁ®ãÂºèÔºåÈÄôËàáÂπæ‰ΩïÂÆöÁêÜÊòØ‰∏ÄËá¥ÁöÑ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•ÈÄöÈÅéÂêëÊ±†‰∏≠Ê∑ªÂä†Êñ∞ÁöÑÂúñÂΩ¢Ê®°Âûã‰æÜÊõ¥Êñ∞„ÄÇÊúÄÂæåÔºåÂÆÉÊé°Áî®Ê∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí‰æÜÊèêÈ´òÂæûÊ±†‰∏≠ÈÅ∏ÊìáÊ®°ÂûãÁöÑÊïàÁéá„ÄÇÊï¥ÂÄã HGR ‰∏çÂÉÖÁ¢∫‰øù‰∫ÜËºÉÂ∞ëÁöÑÊé®ÁêÜÊ≠•È©üÂç≥ÂèØÁç≤ÂæóËºÉÈ´òÁöÑÊ±ÇËß£Á≤æÂ∫¶ÔºåËÄå‰∏îÈÇÑÈÄöÈÅéÊèê‰æõÊâÄÊúâÊé®ÁêÜÊ≠•È©üÁöÑÊèèËø∞‰æÜÈ°ØËëóÂ¢ûÂº∑‰∫ÜËß£Ê±∫ÈÅéÁ®ãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü HGR Âú®ÊèêÈ´òÊ±ÇËß£ APGD ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Query languages for neural networks**
2408.10362v2 by Martin Grohe, Christoph Standke, Juno Steegmans, Jan Van den Bussche

We lay the foundations for a database-inspired approach to interpreting and
understanding neural network models by querying them using declarative
languages. Towards this end we study different query languages, based on
first-order logic, that mainly differ in their access to the neural network
model. First-order logic over the reals naturally yields a language which views
the network as a black box; only the input--output function defined by the
network can be queried. This is essentially the approach of constraint query
languages. On the other hand, a white-box language can be obtained by viewing
the network as a weighted graph, and extending first-order logic with summation
over weight terms. The latter approach is essentially an abstraction of SQL. In
general, the two approaches are incomparable in expressive power, as we will
show. Under natural circumstances, however, the white-box approach can subsume
the black-box approach; this is our main result. We prove the result concretely
for linear constraint queries over real functions definable by feedforward
neural networks with a fixed number of hidden layers and piecewise linear
activation functions.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂ•†ÂÆö‰∫Ü‰∏ÄÂÄãÂèóË≥áÊñôÂ∫´ÂïüÁôºÁöÑÂü∫Á§éÔºåÁî®ÊñºÈÄèÈÅé‰ΩøÁî®ÂÆ£ÂëäÂºèË™ûË®ÄÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÈÄ≤Ë°åË©ÆÈáãÂíåÁêÜËß£„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞ÈÄôÂÄãÁõÆÁöÑÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂü∫Êñº‰∏ÄÈöéÈÇèËºØÁöÑ‰∏çÂêåÊü•Ë©¢Ë™ûË®ÄÔºåÂÆÉÂÄë‰∏ªË¶ÅÂú®ÊñºÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑÂ≠òÂèñÊñπÂºè‰∏çÂêå„ÄÇ‰∏ÄÈöéÂØ¶Êï∏ÈÇèËºØËá™ÁÑ∂ÊúÉÁî¢Áîü‰∏ÄÁ®ÆË™ûË®ÄÔºåÂ∞áÁ∂≤Ë∑ØË¶ñÁÇ∫‰∏ÄÂÄãÈªëÁõíÂ≠êÔºõÂè™ËÉΩÊü•Ë©¢Á∂≤Ë∑ØÂÆöÁæ©ÁöÑËº∏ÂÖ•Ëº∏Âá∫ÂáΩÊï∏„ÄÇÈÄôÂü∫Êú¨‰∏äÊòØÁ¥ÑÊùüÊü•Ë©¢Ë™ûË®ÄÁöÑÊñπÊ≥ï„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂèØ‰ª•ÈÄèÈÅéÂ∞áÁ∂≤Ë∑ØË¶ñÁÇ∫‰∏ÄÂÄãÂä†Ê¨äÂúñÔºå‰∏¶Â∞á‰∏ÄÈöéÈÇèËºØÂª∂‰º∏Âà∞Ê¨äÈáçÈ†Ö‰∏äÁöÑÁ∏ΩÂíåÔºå‰æÜÂèñÂæó‰∏ÄÂÄãÁôΩÁõíË™ûË®Ä„ÄÇÂæåËÄÖÊñπÊ≥ïÂü∫Êú¨‰∏äÊòØ SQL ÁöÑÊäΩË±°„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ïÂú®Ë°®ÈÅîËÉΩÂäõ‰∏äÁÑ°Ê≥ïÁõ∏Êèê‰∏¶Ë´ñÔºåÊàëÂÄëÂ∞áÊúÉË≠âÊòéÈÄô‰∏ÄÈªû„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá™ÁÑ∂ÊÉÖÊ≥Å‰∏ãÔºåÁôΩÁõíÊñπÊ≥ïÂèØ‰ª•ÂåÖÂê´ÈªëÁõíÊñπÊ≥ïÔºõÈÄôÊòØÊàëÂÄëÁöÑÈáçÈªû„ÄÇÊàëÂÄëÂÖ∑È´îË≠âÊòé‰∫ÜÁ∑öÊÄßÁ¥ÑÊùüÊü•Ë©¢Â∞çÊñºÁî±ÂÖ∑ÊúâÂõ∫ÂÆöÊï∏ÈáèÈö±ËóèÂ±§ÂíåÂàÜÊÆµÁ∑öÊÄßÊøÄÊ¥ªÂáΩÊï∏ÁöÑÂâçÈ•ãÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèØÂÆöÁæ©ÁöÑÂØ¶ÂáΩÊï∏„ÄÇ</paragraph>

##### **Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**
2408.10124v1 by Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang

Molecular property prediction is a crucial foundation for drug discovery. In
recent years, pre-trained deep learning models have been widely applied to this
task. Some approaches that incorporate prior biological domain knowledge into
the pre-training framework have achieved impressive results. However, these
methods heavily rely on biochemical experts, and retrieving and summarizing
vast amounts of domain knowledge literature is both time-consuming and
expensive. Large Language Models (LLMs) have demonstrated remarkable
performance in understanding and efficiently providing general knowledge.
Nevertheless, they occasionally exhibit hallucinations and lack precision in
generating domain-specific knowledge. Conversely, Domain-specific Small Models
(DSMs) possess rich domain knowledge and can accurately calculate molecular
domain-related metrics. However, due to their limited model size and singular
functionality, they lack the breadth of knowledge necessary for comprehensive
representation learning. To leverage the advantages of both approaches in
molecular property prediction, we propose a novel Molecular Graph
representation learning framework that integrates Large language models and
Domain-specific small models (MolGraph-LarDo). Technically, we design a
two-stage prompt strategy where DSMs are introduced to calibrate the knowledge
provided by LLMs, enhancing the accuracy of domain-specific information and
thus enabling LLMs to generate more precise textual descriptions for molecular
samples. Subsequently, we employ a multi-modal alignment method to coordinate
various modalities, including molecular graphs and their corresponding
descriptive texts, to guide the pre-training of molecular representations.
Extensive experiments demonstrate the effectiveness of the proposed method.

ÊëòË¶ÅÔºöÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨ÊòØËó•Áâ©ÁôºÁèæÁöÑÈóúÈçµÂü∫Á§é„ÄÇËøëÂπ¥‰æÜÔºåÈ†êË®ìÁ∑¥Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂ∑≤Âª£Ê≥õÊáâÁî®ÊñºÊ≠§‰ªªÂãô„ÄÇ‰∏Ä‰∫õÂ∞áÂÖàÈ©óÁîüÁâ©È†òÂüüÁü•Ë≠òÁ¥çÂÖ•È†êË®ìÁ∑¥Êû∂ÊßãÁöÑÊñπÊ≥ïÂ∑≤ÂèñÂæó‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂö¥Èáç‰æùË≥¥ÊñºÁîüÁâ©ÂåñÂ≠∏Â∞àÂÆ∂Ôºå‰∏¶‰∏îÊ™¢Á¥¢ÂíåÁ∏ΩÁµêÂ§ßÈáèÁöÑÈ†òÂüüÁü•Ë≠òÊñáÁçªÊó¢ËÄóÊôÇÂèàÊòÇË≤¥„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÁêÜËß£ÂíåÊúâÊïàÊèê‰æõ‰∏ÄËà¨Áü•Ë≠òÊñπÈù¢Â±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÂÄëÂÅ∂ÁàæÊúÉÂá∫ÁèæÂπªË¶∫Ôºå‰∏¶‰∏îÂú®ÁîüÊàêÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÊôÇÁº∫‰πèÁ≤æÁ¢∫ÊÄß„ÄÇÁõ∏ÂèçÔºåÁâπÂÆöÈ†òÂüüÁöÑÂ∞èÊ®°Âûã (DSM) ÊìÅÊúâË±êÂØåÁöÑÈ†òÂüüÁü•Ë≠òÔºå‰∏¶‰∏îÂèØ‰ª•Ê∫ñÁ¢∫Ë®àÁÆóËàáÂàÜÂ≠êÈ†òÂüüÁõ∏ÈóúÁöÑÊåáÊ®ô„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÆÉÂÄëÊúâÈôêÁöÑÊ®°ÂûãÂ§ßÂ∞èÂíåÂñÆ‰∏ÄÂäüËÉΩÔºåÂÆÉÂÄëÁº∫‰πèÂÖ®Èù¢Ë°®Á§∫Â≠∏ÁøíÊâÄÈúÄÁöÑÁü•Ë≠òÂª£Â∫¶„ÄÇÁÇ∫‰∫ÜÂú®ÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨‰∏≠Âà©Áî®ÈÄôÂÖ©Á®ÆÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÂ≠êÂúñË°®Á§∫Â≠∏ÁøíÊ°ÜÊû∂ÔºåÂÆÉÈõÜÊàê‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂíåÁâπÂÆöÈ†òÂüüÁöÑÂ∞èÊ®°Âûã (MolGraph-LarDo)„ÄÇÂú®ÊäÄË°ì‰∏äÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÊèêÁ§∫Á≠ñÁï•ÔºåÂÖ∂‰∏≠ÂºïÂÖ• DSM ‰æÜÊ†°Ê∫ñ LLM Êèê‰æõÁöÑÁü•Ë≠òÔºåÊèêÈ´òÁâπÂÆöÈ†òÂüü‰ø°ÊÅØÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂæûËÄå‰Ωø LLM ËÉΩÂ§†ÁÇ∫ÂàÜÂ≠êÊ®£Êú¨ÁîüÊàêÊõ¥Á≤æÁ¢∫ÁöÑÊñáÊú¨ÊèèËø∞„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊé°Áî®Â§öÊ®°ÊÖãÂ∞çÈΩäÊñπÊ≥ï‰æÜÂçîË™øÂêÑÁ®ÆÊ®°ÊÖãÔºåÂåÖÊã¨ÂàÜÂ≠êÂúñÂèäÂÖ∂Â∞çÊáâÁöÑÊèèËø∞ÊÄßÊñáÊú¨Ôºå‰ª•ÊåáÂ∞éÂàÜÂ≠êË°®Á§∫ÁöÑÈ†êË®ìÁ∑¥„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Geometry Informed Tokenization of Molecules for Language Model Generation**
2408.10120v1 by Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji

We consider molecule generation in 3D space using language models (LMs),
which requires discrete tokenization of 3D molecular geometries. Although
tokenization of molecular graphs exists, that for 3D geometries is largely
unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which
converts molecular geometries into $SE(3)$-invariant 1D discrete sequences.
Geo2Seq consists of canonical labeling and invariant spherical representation
steps, which together maintain geometric and atomic fidelity in a format
conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various
LMs excel in molecular geometry generation, especially in controlled generation
tasks.

ÊëòË¶ÅÔºöÊàëÂÄëËÄÉÊÖÆ‰ΩøÁî®Ë™ûË®ÄÊ®°Âûã (LM) Âú® 3D Á©∫Èñì‰∏≠ÁîüÊàêÂàÜÂ≠êÔºåÈÄôÈúÄË¶ÅÂ∞ç 3D ÂàÜÂ≠êÂπæ‰ΩïÁµêÊßãÈÄ≤Ë°åÈõ¢Êï£ÁöÑÊ®ôË®òÂåñ„ÄÇÂÑòÁÆ°Â≠òÂú®ÂàÜÂ≠êÂúñÁöÑÊ®ôË®òÂåñÔºå‰ΩÜÂ∞ç 3D Âπæ‰ΩïÁµêÊßãÁöÑÊ®ôË®òÂåñÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂ∞öÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂòóË©¶ÈÄöÈÅéÊèêÂá∫ Geo2Seq ‰æÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåË©≤ÊñπÊ≥ïÂ∞áÂàÜÂ≠êÂπæ‰ΩïÁµêÊßãËΩâÊèõÁÇ∫ $SE(3)$ ‰∏çËÆäÁöÑ 1D Èõ¢Êï£Â∫èÂàó„ÄÇGeo2Seq ÂåÖÂê´Ë¶èÁØÑÊ®ôÁ±§Âíå‰∏çËÆäÁêÉÈù¢Ë°®Á§∫Ê≠•È©üÔºåÂÆÉÂÄëÂÖ±Âêå‰ª•ÊúâÂà©Êñº LM ÁöÑÊ†ºÂºè‰øùÊåÅÂπæ‰ΩïÂíåÂéüÂ≠ê‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÁï∂Ëàá Geo2Seq ÁµêÂêà‰ΩøÁî®ÊôÇÔºåÂêÑÁ®Æ LM Âú®ÂàÜÂ≠êÂπæ‰ΩïÁîüÊàêÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÁâπÂà•ÊòØÂú®ÂèóÊéßÁîüÊàê‰ªªÂãô‰∏≠„ÄÇ

##### **GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**
2408.10115v1 by Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang

Pre-trained language models are increasingly being used in multi-document
summarization tasks. However, these models need large-scale corpora for
pre-training and are domain-dependent. Other non-neural unsupervised
summarization approaches mostly rely on key sentence extraction, which can lead
to information loss. To address these challenges, we propose a lightweight yet
effective unsupervised approach called GLIMMER: a Graph and LexIcal features
based unsupervised Multi-docuMEnt summaRization approach. It first constructs a
sentence graph from the source documents, then automatically identifies
semantic clusters by mining low-level features from raw texts, thereby
improving intra-cluster correlation and the fluency of generated sentences.
Finally, it summarizes clusters into natural sentences. Experiments conducted
on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach
outperforms existing unsupervised approaches. Furthermore, it surpasses
state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS
and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,
human evaluations indicate that summaries generated by GLIMMER achieve high
readability and informativeness scores. Our code is available at
https://github.com/Oswald1997/GLIMMER.

ÊëòË¶ÅÔºöÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊñá‰ª∂ÊëòË¶Å‰ªªÂä°‰∏≠Ë¢´Ë∂äÊù•Ë∂äÂ§öÂú∞‰ΩøÁî®„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÈúÄË¶ÅÂ§ßËßÑÊ®°ËØ≠ÊñôÂ∫ìËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂‰∏î‰æùËµñ‰∫éÈ¢ÜÂüü„ÄÇÂÖ∂‰ªñÈùûÁ•ûÁªèÊó†ÁõëÁù£ÊëòË¶ÅÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂÖ≥ÈîÆÂè•Â≠êÊèêÂèñÔºåËøôÂèØËÉΩÂØºËá¥‰ø°ÊÅØ‰∏¢Â§±„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ß‰ΩÜÊúâÊïàÁöÑÊó†ÁõëÁù£ÊñπÊ≥ïÔºåÁß∞‰∏∫ GLIMMERÔºö‰∏ÄÁßçÂü∫‰∫éÂõæÂíåËØçÊ±áÁâπÂæÅÁöÑÊó†ÁõëÁù£Â§öÊñáÊ°£ÊëòË¶ÅÊñπÊ≥ï„ÄÇÂÆÉÈ¶ñÂÖà‰ªéÊ∫êÊñáÊ°£ÊûÑÂª∫‰∏Ä‰∏™Âè•Â≠êÂõæÔºåÁÑ∂ÂêéÈÄöËøá‰ªéÂéüÂßãÊñáÊú¨‰∏≠ÊåñÊéò‰ΩéÁ∫ßÁâπÂæÅËá™Âä®ËØÜÂà´ËØ≠‰πâÁ∞áÔºå‰ªéËÄåÊèêÈ´òÁ∞áÂÜÖÁõ∏ÂÖ≥ÊÄßÂíåÁîüÊàêÂè•Â≠êÁöÑÊµÅÁïÖÊÄß„ÄÇÊúÄÂêéÔºåÂÆÉÂ∞ÜÁ∞áÊÄªÁªì‰∏∫Ëá™ÁÑ∂Âè•Â≠ê„ÄÇÂú® Multi-News„ÄÅMulti-XScience Âíå DUC-2004 ‰∏äËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ºò‰∫éÁé∞ÊúâÁöÑÊó†ÁõëÁù£ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂú®Èõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÔºåÂÆÉÂú® ROUGE ÂæóÂàÜÊñπÈù¢Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÈ¢ÑËÆ≠ÁªÉÂ§öÊñáÊ°£ÊëòË¶ÅÊ®°ÂûãÔºà‰æãÂ¶Ç PEGASUS Âíå PRIMERAÔºâ„ÄÇÊ≠§Â§ñÔºå‰∫∫Á±ªËØÑ‰º∞Ë°®ÊòéÔºåGLIMMER ÁîüÊàêÁöÑÊëòË¶ÅËé∑Âæó‰∫ÜÂæàÈ´òÁöÑÂèØËØªÊÄßÂíå‰ø°ÊÅØÊÄßÂæóÂàÜ„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂèØÂú® https://github.com/Oswald1997/GLIMMER Ëé∑Âæó„ÄÇ

##### **SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**
2408.09717v1 by Pengjie Liu, Wang Zhang, Yulong Ding, Xuefeng Zhang, Shuang-Hua Yang

Legal Judgment Prediction (LJP) aims to form legal judgments based on the
criminal fact description. However, researchers struggle to classify confusing
criminal cases, such as robbery and theft, which requires LJP models to
distinguish the nuances between similar crimes. Existing methods usually design
handcrafted features to pick up necessary semantic legal clues to make more
accurate legal judgment predictions. In this paper, we propose a Semantic-Aware
Dual Encoder Model (SEMDR), which designs a novel legal clue tracing mechanism
to conduct fine-grained semantic reasoning between criminal facts and
instruments. Our legal clue tracing mechanism is built from three reasoning
levels: 1) Lexicon-Tracing, which aims to extract criminal facts from criminal
descriptions; 2) Sentence Representation Learning, which contrastively trains
language models to better represent confusing criminal facts; 3) Multi-Fact
Reasoning, which builds a reasons graph to propagate semantic clues among fact
nodes to capture the subtle difference among criminal facts. Our legal clue
tracing mechanism helps SEMDR achieve state-of-the-art on the CAIL2018 dataset
and shows its advance in few-shot scenarios. Our experiments show that SEMDR
has a strong ability to learn more uniform and distinguished representations
for criminal facts, which helps to make more accurate predictions on confusing
criminal cases and reduces the model uncertainty during making judgments. All
codes will be released via GitHub.

ÊëòË¶ÅÔºöÊ≥ïÂæãÂà§Ê±∫È†êÊ∏¨ (LJP) Êó®Âú®Ê†πÊìöÁäØÁΩ™‰∫ãÂØ¶ÊèèËø∞ÂΩ¢ÊàêÊ≥ïÂæãÂà§Ê±∫„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂‰∫∫Âì°Èõ£‰ª•Â∞çÊê∂Âä´ÂíåÁõúÁ´äÁ≠â‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂàë‰∫ãÊ°à‰ª∂ÈÄ≤Ë°åÂàÜÈ°ûÔºåÈÄôÈúÄË¶Å LJP Ê®°ÂûãÂçÄÂàÜÈ°û‰ººÁäØÁΩ™‰πãÈñìÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏Ë®≠Ë®àÊâãÂ∑•ÁâπÂæµ‰ª•Áç≤ÂèñÂøÖË¶ÅÁöÑË™ûÁæ©Ê≥ïÂæãÁ∑öÁ¥¢Ôºå‰ª•ÂÅöÂá∫Êõ¥Ê∫ñÁ¢∫ÁöÑÊ≥ïÂæãÂà§Ê±∫È†êÊ∏¨„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË™ûÁæ©ÊÑüÁü•ÈõôÁ∑®Á¢ºÂô®Ê®°Âûã (SEMDR)ÔºåÂÆÉË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Ôºå‰ª•Âú®ÁäØÁΩ™‰∫ãÂØ¶ÂíåÂ∑•ÂÖ∑‰πãÈñìÈÄ≤Ë°åÁ¥∞Á≤íÂ∫¶ÁöÑË™ûÁæ©Êé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Âª∫Á´ãÂú®‰∏âÂÄãÊé®ÁêÜÂ±§Á¥ö‰πã‰∏äÔºö1) Ë©ûÂΩôËøΩËπ§ÔºåÊó®Âú®ÂæûÁäØÁΩ™ÊèèËø∞‰∏≠ÊèêÂèñÁäØÁΩ™‰∫ãÂØ¶Ôºõ2) Âè•Â≠êË°®Á§∫Â≠∏ÁøíÔºåÂ∞çÊØîË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã‰ª•Êõ¥Â•ΩÂú∞Ë°®Á§∫‰ª§‰∫∫Âõ∞ÊÉëÁöÑÁäØÁΩ™‰∫ãÂØ¶Ôºõ3) Â§ö‰∫ãÂØ¶Êé®ÁêÜÔºåÊßãÂª∫‰∏ÄÂÄãÂéüÂõ†ÂúñÔºåÂú®‰∫ãÂØ¶ÁØÄÈªû‰πãÈñìÂÇ≥Êí≠Ë™ûÁæ©Á∑öÁ¥¢Ôºå‰ª•ÊçïÊçâÁäØÁΩ™‰∫ãÂØ¶‰πãÈñìÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇÊàëÂÄëÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Âπ´Âä© SEMDR Âú® CAIL2018 Ë≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊäÄË°ìÔºå‰∏¶Â±ïÁ§∫‰∫ÜÂÖ∂Âú®Â∞ëÈè°È†≠Â†¥ÊôØ‰∏≠ÁöÑÈÄ≤Ê≠•„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåSEMDR ÂÖ∑ÊúâÂ≠∏ÁøíÊõ¥Áµ±‰∏ÄÂíåÂçÄÂà•ÁöÑÁäØÁΩ™‰∫ãÂØ¶Ë°®Á§∫ÁöÑÂº∑Â§ßËÉΩÂäõÔºåÈÄôÊúâÂä©ÊñºÂ∞ç‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂàë‰∫ãÊ°à‰ª∂ÂÅöÂá∫Êõ¥Ê∫ñÁ¢∫ÁöÑÈ†êÊ∏¨Ôºå‰∏¶Âú®ÂÅöÂá∫Âà§Ê±∫ÊôÇÊ∏õÂ∞ëÊ®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÊâÄÊúâ‰ª£Á¢ºÈÉΩÂ∞áÈÄöÈÅé GitHub ÁôºÂ∏É„ÄÇ

##### **Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**
2408.09529v1 by Xinnan Dai, Qihao Wen, Yifei Shen, Hongzhi Wen, Dongsheng Li, Jiliang Tang, Caihua Shan

Large Language Models (LLMs) have achieved great success in various reasoning
tasks. In this work, we focus on the graph reasoning ability of LLMs. Although
theoretical studies proved that LLMs are capable of handling graph reasoning
tasks, empirical evaluations reveal numerous failures. To deepen our
understanding on this discrepancy, we revisit the ability of LLMs on three
fundamental graph tasks: graph description translation, graph connectivity, and
the shortest-path problem. Our findings suggest that LLMs can fail to
understand graph structures through text descriptions and exhibit varying
performance for all these three fundamental tasks. Meanwhile, we perform a
real-world investigation on knowledge graphs and make consistent observations
with our findings. The codes and datasets are available.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÊé®ÁêÜ‰ªªÂãô‰∏≠Â∑≤ÂèñÂæóÂ∑®Â§ßÁöÑÊàêÂäü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº LLM ÁöÑÂúñÂΩ¢Êé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ°ÁêÜË´ñÁ†îÁ©∂Ë≠âÊòé LLM ÊúâËÉΩÂäõËôïÁêÜÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÔºå‰ΩÜÁ∂ìÈ©óË©ï‰º∞È°ØÁ§∫Âá∫Ë®±Â§öÂ§±Êïó„ÄÇÁÇ∫‰∫ÜÂä†Ê∑±ÊàëÂÄëÂ∞çÈÄôÁ®ÆÂ∑ÆÁï∞ÁöÑÁêÜËß£ÔºåÊàëÂÄëÈáçÊñ∞Êé¢Ë®é LLM Âú®‰∏âÂÄãÂü∫Êú¨ÂúñÂΩ¢‰ªªÂãô‰∏äÁöÑËÉΩÂäõÔºöÂúñÂΩ¢ÊèèËø∞ÁøªË≠Ø„ÄÅÂúñÂΩ¢ÈÄ£ÈÄöÊÄßÂíåÊúÄÁü≠Ë∑ØÂæëÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåLLM ÂèØËÉΩÁÑ°Ê≥ïÈÄöÈÅéÊñáÊú¨ÊèèËø∞ÁêÜËß£ÂúñÂΩ¢ÁµêÊßãÔºå‰∏¶‰∏îÂú®ÊâÄÊúâÈÄô‰∏âÂÄãÂü∫Êú¨‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÊÄßËÉΩ„ÄÇÂêåÊôÇÔºåÊàëÂÄëÂ∞çÁü•Ë≠òÂúñË≠úÈÄ≤Ë°å‰∫ÜÁèæÂØ¶‰∏ñÁïåÁöÑË™øÊü•Ôºå‰∏¶Â∞çÊàëÂÄëÁöÑÁôºÁèæÈÄ≤Ë°å‰∫Ü‰∏ÄËá¥ÁöÑËßÄÂØü„ÄÇ‰ª£Á¢ºÂíåÊï∏ÊìöÈõÜÂèØÁî®„ÄÇ

##### **Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**
2408.13273v1 by Geethan Sannidhi, Sagar Srinivas Sakhinana, Venkataramana Runkana

Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàPLLMÔºâÔºå‰æãÂ¶Ç OpenAI ChatGPT Âíå Google
Gemini Èù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶Ç‰∏çÊ∫ñÁ¢∫ÁöÑ‰∫ãÂØ¶ÂõûÊÜ∂„ÄÅÂπªË¶∫„ÄÅ
ÂÅèË¶ãÂíåÊôÇÈñìÁü•Ë≠òÂúñÔºàtKGÔºâÈ†êÊ∏¨ÁöÑÊú™‰æÜÊï∏ÊìöÊ¥©Êºè„ÄÇ
ÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü sLA-tKGFÔºàtKG È†êÊ∏¨ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÂä©ÁêÜÔºâÔºåÂÆÉÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâ
ËºîÂä©ÔºåÂæûÈ†≠ÈñãÂßãÈÄöÈÅéÁôΩÊùøÊ≥ïËá™Ë®ÇË®ìÁ∑¥ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÊ®°ÂûãÔºå‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑ tKG È†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂª∫Êßã
Ê≥®ÂÖ•Áü•Ë≠òÁöÑÊèêÁ§∫ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ tKG„ÄÅÁ∂≤Ë∑ØÊêúÂ∞ã
ÁµêÊûúÂíå PLLM ÁîüÊàêÁöÑÊñáÂ≠óÊèèËø∞Ôºå‰ª•‰∫ÜËß£ÁõÆÊ®ôÊôÇÈñì‰πãÂâçÁöÑÊ≠∑Âè≤ÂØ¶È´îÈóú‰øÇ„ÄÇÂÆÉÂà©Áî®ÈÄô‰∫õÂ§ñÈÉ®
Ê≥®ÂÖ•Áü•Ë≠òÁöÑÊèêÁ§∫Ôºå‰ª•Êõ¥Ê∑±ÂÖ•Âú∞ÁêÜËß£ÂíåÊé®ÁêÜ
ÁâπÂÆöÊñºËÑàÁµ°ÁöÑË™ûÁæ©ÂíåÊôÇÈñìË≥áË®äÔºå‰ª•Èõ∂Ê¨°ÊèêÁ§∫Â∞èË¶èÊ®°Ë™ûË®ÄÊ®°ÂûãÔºå‰ª•Êõ¥Ê∫ñÁ¢∫Âú∞È†êÊ∏¨ tKG ‰∏≠ÁöÑÊú™‰æÜ‰∫ã‰ª∂„ÄÇÂÆÉÊ∏õÂ∞ëÂπªË¶∫‰∏¶ÈÄèÈÅé‰∫ÜËß£Èö®ÊôÇÈñìËÆäÂåñÁöÑË∂®Âã¢‰æÜÊ∏õËºïÂàÜ‰ΩàËΩâÁßªÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÂÆÉ
ËÉΩÊõ¥Ê∫ñÁ¢∫‰∏îÊúâËÑàÁµ°Âú∞È†êÊ∏¨Êú™‰æÜ‰∫ã‰ª∂ÔºåÂêåÊôÇÂ∞áÈÅãÁÆóÈúÄÊ±ÇÈôçËá≥ÊúÄ‰Ωé„ÄÇÂö¥Ë¨πÁöÑÂØ¶Ë≠âÁ†îÁ©∂Ë≠âÊòé
ÊàëÂÄëÁöÑÊû∂ÊßãÂÖ∑ÊúâÁ©©ÂÅ•ÊÄß„ÄÅÂèØÊì¥ÂÖÖÊÄßÂíåÊúÄÂÖàÈÄ≤ÔºàSOTAÔºâÊïàËÉΩ
Âú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂèØËß£Èáã‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑ tKG È†êÊ∏¨„ÄÇ

##### **Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**
2408.09429v1 by Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu

Hallucination issues persistently plagued current multimodal large language
models (MLLMs). While existing research primarily focuses on object-level or
attribute-level hallucinations, sidelining the more sophisticated relation
hallucinations that necessitate advanced reasoning abilities from MLLMs.
Besides, recent benchmarks regarding relation hallucinations lack in-depth
evaluation and effective mitigation. Moreover, their datasets are typically
derived from a systematic annotation process, which could introduce inherent
biases due to the predefined process. To handle the aforementioned challenges,
we introduce Reefknot, a comprehensive benchmark specifically targeting
relation hallucinations, consisting of over 20,000 samples derived from
real-world scenarios. Specifically, we first provide a systematic definition of
relation hallucinations, integrating perspectives from perceptive and cognitive
domains. Furthermore, we construct the relation-based corpus utilizing the
representative scene graph dataset Visual Genome (VG), from which semantic
triplets follow real-world distributions. Our comparative evaluation across
three distinct tasks revealed a substantial shortcoming in the capabilities of
current MLLMs to mitigate relation hallucinations. Finally, we advance a novel
confidence-based mitigation strategy tailored to tackle the relation
hallucinations problem. Across three datasets, including Reefknot, we observed
an average reduction of 9.75% in the hallucination rate. We believe our paper
sheds valuable insights into achieving trustworthy multimodal intelligence. Our
dataset and code will be released upon paper acceptance.

ÊëòË¶ÅÔºöÂπªË¶∫ÂïèÈ°åÊåÅÁ∫åÂõ∞ÊìæËëóÁï∂ÂâçÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM)„ÄÇÈõñÁÑ∂ÁèæÊúâÁ†îÁ©∂‰∏ªË¶ÅÈóúÊ≥®Áâ©‰ª∂Â±§Á¥öÊàñÂ±¨ÊÄßÂ±§Á¥öÁöÑÂπªË¶∫Ôºå‰ΩÜÂçªÂøΩË¶ñ‰∫ÜÈúÄË¶Å MLLM ÂÖ∑ÂÇôÈÄ≤ÈöéÊé®ÁêÜËÉΩÂäõÁöÑÊõ¥Ë§áÈõúÈóú‰øÇÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÈóúÊñºÈóú‰øÇÂπªË¶∫ÁöÑÊúÄÊñ∞Âü∫Ê∫ñÁº∫‰πèÊ∑±ÂÖ•Ë©ï‰º∞ÂíåÊúâÊïàÁöÑÁ∑©Ëß£Êé™ÊñΩ„ÄÇËÄå‰∏îÔºå‰ªñÂÄëÁöÑË≥áÊñôÈõÜÈÄöÂ∏∏‰æÜËá™Á≥ªÁµ±ÂåñÁöÑË®ªÈáãÈÅéÁ®ãÔºåÈÄôÂèØËÉΩÊúÉÂõ†ÁÇ∫È†êÂÖàÂÆöÁæ©ÁöÑÈÅéÁ®ãËÄåÂºïÂÖ•Âõ∫ÊúâÁöÑÂÅèÂ∑Æ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç‰∏äËø∞ÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ReefknotÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÈñÄÈáùÂ∞çÈóú‰øÇÂπªË¶∫ÁöÑÁ∂úÂêàÂü∫Ê∫ñÔºåÂåÖÂê´Ë∂ÖÈÅé 20,000 ÂÄã‰æÜËá™ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÁöÑÁØÑ‰æã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊèê‰æõÈóú‰øÇÂπªË¶∫ÁöÑÁ≥ªÁµ±ÊÄßÂÆöÁæ©ÔºåÊï¥Âêà‰æÜËá™Áü•Ë¶∫ÂíåË™çÁü•È†òÂüüÁöÑËßÄÈªû„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÂ†¥ÊôØÂúñÂΩ¢Ë≥áÊñôÈõÜ Visual Genome (VG) Âª∫ÊßãÂü∫ÊñºÈóú‰øÇÁöÑË™ûÊñôÂ∫´ÔºåË™ûÁæ©‰∏âÂÖÉÁµÑÈÅµÂæ™ÁúüÂØ¶‰∏ñÁïåÁöÑÂàÜ‰Ωà„ÄÇÊàëÂÄëÂú®‰∏âÂÄã‰∏çÂêåÁöÑ‰ªªÂãô‰∏≠ÈÄ≤Ë°åÊØîËºÉË©ï‰º∞ÔºåÊè≠Á§∫‰∫ÜÁï∂Ââç MLLM Âú®Ê∏õËºïÈóú‰øÇÂπªË¶∫ÊñπÈù¢ÁöÑËÉΩÂäõÂ≠òÂú®ÈáçÂ§ßÁº∫Èô∑„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫Êñº‰ø°ÂøÉÁöÑÁ∑©Ëß£Á≠ñÁï•ÔºåÂ∞àÈñÄÁî®ÊñºËß£Ê±∫Èóú‰øÇÂπªË¶∫ÂïèÈ°å„ÄÇÂú®ÂåÖÊã¨ Reefknot Âú®ÂÖßÁöÑ‰∏âÂÄãË≥áÊñôÈõÜ‰∏≠ÔºåÊàëÂÄëËßÄÂØüÂà∞ÂπªË¶∫ÁéáÂπ≥ÂùáÈôç‰Ωé‰∫Ü 9.75%„ÄÇÊàëÂÄëÁõ∏‰ø°ÊàëÂÄëÁöÑË´ñÊñáÂ∞çÂØ¶ÁèæÂÄºÂæó‰ø°Ë≥¥ÁöÑÂ§öÊ®°ÊÖãÊô∫ÊÖßÊèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÂ∞áÂú®Ë´ñÊñáË¢´Êé•ÂèóÂæåÁôºÂ∏É„ÄÇ

##### **ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**
2408.08972v1 by Debashis Gupta, Aditi Golder, Luis Fernendez, Miles Silman, Greg Lersen, Fan Yang, Bob Plemmons, Sarra Alqahtani, Paul Victor Pauca

Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly
destructive mining practice, leading to environmental disasters across the
world's tropical watersheds. The topic of ASGM spans multiple domains of
research and information, including natural and social systems, and knowledge
is often atomized across a diversity of media and documents. We therefore
introduce a knowledge graph (ASGM-KG) that consolidates and provides crucial
information about ASGM practices and their environmental effects. The current
version of ASGM-KG consists of 1,899 triples extracted using a large language
model (LLM) from documents and reports published by both non-governmental and
governmental organizations. These documents were carefully selected by a group
of tropical ecologists with expertise in ASGM. This knowledge graph was
validated using two methods. First, a small team of ASGM experts reviewed and
labeled triples as factual or non-factual. Second, we devised and applied an
automated factual reduction framework that relies on a search engine and an LLM
for labeling triples. Our framework performs as well as five baselines on a
publicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG
validated by domain experts. ASGM-KG demonstrates an advancement in knowledge
aggregation and representation for complex, interdisciplinary environmental
crises such as ASGM.

ÊëòË¶ÅÔºöÊâãÂ∑•ÂíåÂ∞èÂûãÊé°ÈáëÔºàASGMÔºâÊòØ‰∏ÄÁ®Æ‰ΩéÊàêÊú¨‰ΩÜÈ´òÂ∫¶Á†¥Â£ûÊÄßÁöÑÊé°Á§¶ÂØ¶ÂãôÔºåÂ∞éËá¥ÂÖ®ÁêÉÁÜ±Â∏∂ÊµÅÂüüÁôºÁîüÁí∞Â¢ÉÁÅΩÈõ£„ÄÇASGM ÁöÑ‰∏ªÈ°åÊ∂µËìãÂ§öÂÄãÁ†îÁ©∂ÂíåË≥áË®äÈ†òÂüüÔºåÂåÖÊã¨Ëá™ÁÑ∂ÂíåÁ§æÊúÉÁ≥ªÁµ±ÔºåËÄåÁü•Ë≠òÈÄöÂ∏∏ÂàÜÊï£Âú®ÂêÑÁ®ÆÂ™íÈ´îÂíåÊñá‰ª∂‰∏≠„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•Áü•Ë≠òÂúñË≠ú (ASGM-KG)ÔºåÂÆÉÊï¥Âêà‰∏¶Êèê‰æõÊúâÈóú ASGM ÂØ¶ÂãôÂèäÂÖ∂Áí∞Â¢ÉÂΩ±ÈüøÁöÑÈáçË¶ÅË≥áË®ä„ÄÇÁõÆÂâçÁâàÊú¨ÁöÑ ASGM-KG ÂåÖÂê´ 1,899 ÂÄã‰∏âÂÖÉÁµÑÔºåÈÄô‰∫õ‰∏âÂÖÉÁµÑÊòØ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂæûÈùûÊîøÂ∫úÁµÑÁπîÂíåÊîøÂ∫úÁµÑÁπîÁôºÂ∏ÉÁöÑÊñá‰ª∂ÂíåÂ†±Âëä‰∏≠ÊèêÂèñÂá∫‰æÜÁöÑ„ÄÇÈÄô‰∫õÊñá‰ª∂ÊòØÁî±‰∏ÄÁæ§ÂÖ∑Êúâ ASGM Â∞àÊ•≠Áü•Ë≠òÁöÑÁÜ±Â∏∂ÁîüÊÖãÂ≠∏ÂÆ∂‰ªîÁ¥∞ÊåëÈÅ∏ÁöÑ„ÄÇÈÄôÂÄãÁü•Ë≠òÂúñË≠ú‰ΩøÁî®ÂÖ©Á®ÆÊñπÊ≥ïÈ©óË≠â„ÄÇÈ¶ñÂÖàÔºå‰∏ÄÂ∞èÁµÑ ASGM Â∞àÂÆ∂ÂØ©Êü•‰∏¶Â∞á‰∏âÂÖÉÁµÑÊ®ôË®òÁÇ∫‰∫ãÂØ¶ÊàñÈùû‰∫ãÂØ¶„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëË®≠Ë®à‰∏¶ÊáâÁî®‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñÁöÑ‰∫ãÂØ¶Á∞°ÂåñÊû∂ÊßãÔºåË©≤Êû∂Êßã‰æùË≥¥ÊñºÊêúÂ∞ãÂºïÊìéÂíå LLM ‰æÜÊ®ôË®ò‰∏âÂÖÉÁµÑ„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®ÂÖ¨ÈñãÁöÑÁü•Ë≠òÂúñË≠ú‰∏äÂü∑Ë°åÂæóËàá‰∫îÂÄãÂü∫Ê∫ñ‰∏ÄÊ®£Â•ΩÔºå‰∏¶Âú®ÊàëÂÄëÁî±È†òÂüüÂ∞àÂÆ∂È©óË≠âÁöÑ ASGM-KG ‰∏äÈÅîÂà∞Ë∂ÖÈÅé 90 ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇASGM-KG Â±ïÁ§∫‰∫ÜË§áÈõúÁöÑË∑®Â≠∏ÁßëÁí∞Â¢ÉÂç±Ê©üÔºà‰æãÂ¶Ç ASGMÔºâÁöÑÁü•Ë≠òÂΩôÊï¥ÂíåË°®Á§∫ÊñπÈù¢ÁöÑ‰∏ÄÈ†ÖÈÄ≤Â±ï„ÄÇ

##### **EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**
2408.08782v1 by Chenwei Wan, Matthieu Labeau, Chlo√© Clavel

Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Previous efforts have focused on developing modular dialogue systems that treat
socio-emotional strategy prediction as an auxiliary task and generate
strategy-conditioned responses with customized decoders. Recently, with
advancements in large language models (LLMs), end-to-end dialogue agents
without explicit socio-emotional strategy prediction steps have become
prevalent. However, despite their excellence in language generation, recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy predictor,
EmoDynamiX, which models the discourse dynamics between user emotions and
system strategies using a heterogeneous graph. Additionally, we make use of the
Emotion Recognition in Conversations (ERC) task and design a flexible
mixed-emotion module to capture fine-grained emotional states of the user.
Experimental results on two ESC datasets show EmoDynamiX outperforms previous
state-of-the-art methods with a significant margin.

ÊëòË¶ÅÔºöË®≠Ë®àÊÉÖÁ∑íÊô∫ËÉΩÂ∞çË©±Á≥ªÁµ±‰ª•Êèê‰æõÂÆâÊÖ∞ÂíåÂª∫Ë≠∞Áµ¶Á∂ìÊ≠∑ÁóõËã¶ÁöÑ‰∫∫ÊòØ‰∏ÄÂÄãÂºï‰∫∫ÂÖ•ÂãùÁöÑÁ†îÁ©∂È†òÂüü„ÄÇ
ÂÖàÂâçÁöÑÂä™ÂäõÈõÜ‰∏≠ÊñºÈñãÁôºÊ®°ÁµÑÂåñÂ∞çË©±Á≥ªÁµ±ÔºåÂ∞áÁ§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•È†êÊ∏¨Ë¶ñÁÇ∫ËºîÂä©‰ªªÂãôÔºå‰∏¶‰ΩøÁî®Ëá™Ë®ÇËß£Á¢ºÂô®Áî¢ÁîüÁ≠ñÁï•Ê¢ù‰ª∂ÂåñÁöÑÂõûÊáâ„ÄÇÊúÄËøëÔºåÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÊ≤íÊúâÊòéÁ¢∫Á§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•È†êÊ∏¨Ê≠•È©üÁöÑÁ´ØÂà∞Á´ØÂ∞çË©±‰ª£ÁêÜÂ∑≤ËÆäÂæóÊôÆÈÅç„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°ÂÆÉÂÄëÂú®Ë™ûË®ÄÁîüÊàêÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLLM Â∞çÊüê‰∫õÁ§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•ÁöÑÂõ∫ÊúâÂÅèÂ•ΩÊúÉÈòªÁ§ôÊèê‰æõÈ´òÂìÅË≥™ÁöÑÊÉÖÁ∑íÊîØÊåÅ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÂª∫Ë≠∞Â∞áÁ≠ñÁï•È†êÊ∏¨ËàáË™ûË®ÄÁîüÊàêËß£ËÄ¶Ôºå‰∏¶ÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ∞çË©±Á≠ñÁï•È†êÊ∏¨Âô® EmoDynamiXÔºåÂÆÉ‰ΩøÁî®Áï∞Ë≥™ÂúñÂΩ¢Â∞ç‰ΩøÁî®ËÄÖÊÉÖÁ∑íÂíåÁ≥ªÁµ±Á≠ñÁï•‰πãÈñìÁöÑË©±Ë™ûÂãïÊÖãÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Â∞çË©±‰∏≠ÁöÑÊÉÖÁ∑íËæ®Ë≠ò (ERC) ‰ªªÂãô‰∏¶Ë®≠Ë®à‰∫Ü‰∏ÄÂÄãÈùàÊ¥ªÁöÑÊ∑∑ÂêàÊÉÖÁ∑íÊ®°ÁµÑ‰æÜÊçïÊçâ‰ΩøÁî®ËÄÖÁöÑÁ¥∞Á∑ªÊÉÖÁ∑íÁãÄÊÖã„ÄÇÂú®ÂÖ©ÂÄã ESC Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåEmoDynamiX ‰ª•È°ØËëóÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ

##### **Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**
2408.08685v1 by Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi

Graph neural networks (GNNs) are vulnerable to adversarial perturbations,
especially for topology attacks, and many methods that improve the robustness
of GNNs have received considerable attention. Recently, we have witnessed the
significant success of large language models (LLMs), leading many to explore
the great potential of LLMs on GNNs. However, they mainly focus on improving
the performance of GNNs by utilizing LLMs to enhance the node features.
Therefore, we ask: Will the robustness of GNNs also be enhanced with the
powerful understanding and inference capabilities of LLMs? By presenting the
empirical results, we find that despite that LLMs can improve the robustness of
GNNs, there is still an average decrease of 23.1% in accuracy, implying that
the GNNs remain extremely vulnerable against topology attack. Therefore,
another question is how to extend the capabilities of LLMs on graph adversarial
robustness. In this paper, we propose an LLM-based robust graph structure
inference framework, LLM4RGNN, which distills the inference capabilities of
GPT-4 into a local LLM for identifying malicious edges and an LM-based edge
predictor for finding missing important edges, so as to recover a robust graph
structure. Extensive experiments demonstrate that LLM4RGNN consistently
improves the robustness across various GNNs. Even in some cases where the
perturbation ratio increases to 40%, the accuracy of GNNs is still better than
that on the clean graph.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊìæÂãïÁöÑÂΩ±ÈüøÔºå
ÁâπÂà•ÊòØÊãìÊí≤ÊîªÊìäÔºåË®±Â§öÊîπÂñÑ GNN È≠ØÊ£íÊÄßÁöÑÊñπÊ≥ïÈÉΩÂÇôÂèóÈóúÊ≥®„ÄÇÊúÄËøëÔºåÊàëÂÄëË¶ãË≠â‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈ°ØËëóÊàêÂäüÔºåÂ∞éËá¥Ë®±Â§ö‰∫∫Êé¢Á¥¢ LLM Âú® GNN ‰∏äÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄë‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂà©Áî® LLM Â¢ûÂº∑ÁØÄÈªûÁâπÂæµ‰æÜÊîπÂñÑ GNN ÁöÑÊïàËÉΩ„ÄÇ
Âõ†Ê≠§ÔºåÊàëÂÄëÂïèÔºöLLM Âº∑Â§ßÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÊòØÂê¶‰πüÊúÉÂ¢ûÂº∑ GNN ÁöÑÈ≠ØÊ£íÊÄßÔºüÈÄèÈÅéÂëàÁèæÂØ¶Ë≠âÁµêÊûúÔºåÊàëÂÄëÁôºÁèæÂÑòÁÆ° LLM ÂèØ‰ª•ÊîπÂñÑ GNN ÁöÑÈ≠ØÊ£íÊÄßÔºå‰ΩÜÊ∫ñÁ¢∫Â∫¶‰ªçÂπ≥Âùá‰∏ãÈôç 23.1%ÔºåÈÄôË°®Á§∫ GNN ‰ªçÁÑ∂Ê•µÂÆπÊòìÂèóÂà∞ÊãìÊí≤ÊîªÊìä„ÄÇÂõ†Ê≠§ÔºåÂè¶‰∏ÄÂÄãÂïèÈ°åÊòØÂ¶Ç‰ΩïÊì¥Â±ï LLM Âú®ÂúñÂΩ¢Â∞çÊäóÈ≠ØÊ£íÊÄß‰∏äÁöÑËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÈ≠ØÊ£íÂúñÂΩ¢ÁµêÊßãÊé®ÁêÜÊ°ÜÊû∂ LLM4RGNNÔºåÂÆÉÂ∞á GPT-4 ÁöÑÊé®ÁêÜËÉΩÂäõÊèêÁÖâÊàê‰∏ÄÂÄãÁî®ÊñºË≠òÂà•ÊÉ°ÊÑèÈÇäÁ∑£ÁöÑÊú¨Âú∞ LLMÔºå‰ª•Âèä‰∏ÄÂÄãÁî®ÊñºÂ∞ãÊâæÈÅ∫Â§±ÈáçË¶ÅÈÇäÁ∑£ÁöÑÂü∫Êñº LM ÁöÑÈÇäÁ∑£È†êÊ∏¨Âô®Ôºå‰ª•‰æøÊÅ¢Âæ©‰∏ÄÂÄãÈ≠ØÊ£íÁöÑÂúñÂΩ¢ÁµêÊßã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåLLM4RGNN ÊåÅÁ∫åÊîπÂñÑÂêÑÁ®Æ GNN ÁöÑÈ≠ØÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êüê‰∫õÊìæÂãïÁéáÂ¢ûÂä†Âà∞ 40% ÁöÑÊÉÖÊ≥Å‰∏ãÔºåGNN ÁöÑÊ∫ñÁ¢∫Â∫¶‰ªçÁÑ∂ÂÑ™Êñº‰πæÊ∑®ÂúñÂΩ¢„ÄÇ

##### **RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**
2408.08933v1 by Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, X. Sean Wang

Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical
component in many applications, including recommendation systems and large
language model-based applications. With the advancement of multimodal neural
models, which transform data from different modalities into a shared
high-dimensional space as feature vectors, cross-modal ANNS aims to use the
data vector from one modality (e.g., texts) as the query to retrieve the most
similar items from another (e.g., images or videos). However, there is an
inherent distribution gap between embeddings from different modalities, and
cross-modal queries become Out-of-Distribution (OOD) to the base data.
Consequently, state-of-the-art ANNS approaches suffer poor performance for OOD
workloads. In this paper, we quantitatively analyze the properties of the OOD
workloads to gain an understanding of their ANNS efficiency. Unlike
single-modal workloads, we reveal OOD queries spatially deviate from base data,
and the k-nearest neighbors of an OOD query are distant from each other in the
embedding space. The property breaks the assumptions of existing ANNS
approaches and mismatches their design for efficient search. With insights from
the OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an
efficient ANNS graph index built under the guidance of query distribution.
Extensive experiments show that RoarGraph significantly outperforms
state-of-the-art approaches on modern cross-modal datasets, achieving up to
3.56x faster search speed at a 90% recall rate for OOD queries.

ÊëòË¶ÅÔºöËøë‰ººÊúÄËøëÈÇªÊêúÁ¥¢ (ANNS) ÊòØËÆ∏Â§öÂ∫îÁî®Á®ãÂ∫è‰∏≠ÁöÑÂü∫Êú¨ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåÂåÖÊã¨Êé®ËçêÁ≥ªÁªüÂíåÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇÈöèÁùÄÂ§öÊ®°ÊÄÅÁ•ûÁªèÊ®°ÂûãÁöÑÂèëÂ±ïÔºåÂÆÉÂ∞ÜÊù•Ëá™‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆËΩ¨Êç¢‰∏∫ÂÖ±‰∫´ÁöÑÈ´òÁª¥Á©∫Èó¥‰Ωú‰∏∫ÁâπÂæÅÂêëÈáèÔºåË∑®Ê®°ÊÄÅ ANNS Êó®Âú®‰ΩøÁî®Êù•Ëá™‰∏Ä‰∏™Ê®°ÊÄÅÔºà‰æãÂ¶ÇÊñáÊú¨ÔºâÁöÑÊï∞ÊçÆÂêëÈáè‰Ωú‰∏∫Êü•ËØ¢Ôºå‰ª•Ê£ÄÁ¥¢Êù•Ëá™Âè¶‰∏Ä‰∏™Ê®°ÊÄÅÔºà‰æãÂ¶ÇÂõæÂÉèÊàñËßÜÈ¢ëÔºâÊúÄÁõ∏‰ººÁöÑÈ°πÁõÆ„ÄÇ‰ΩÜÊòØÔºå‰∏çÂêåÊ®°ÊÄÅÁöÑÂµåÂÖ•‰πãÈó¥Â≠òÂú®Âõ∫ÊúâÁöÑÂàÜÂ∏ÉÂ∑ÆË∑ùÔºåÂπ∂‰∏îË∑®Ê®°ÊÄÅÊü•ËØ¢ÂØπ‰∫éÂü∫Á°ÄÊï∞ÊçÆËÄåË®ÄÊàê‰∏∫ÂàÜÂ∏ÉÂ§ñ (OOD)„ÄÇÂõ†Ê≠§ÔºåÊúÄÂÖàËøõÁöÑ ANNS ÊñπÊ≥ïÂØπ‰∫é OOD Â∑•‰ΩúË¥üËΩΩÁöÑÊÄßËÉΩÂæàÂ∑Æ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂÆöÈáèÂàÜÊûê‰∫Ü OOD Â∑•‰ΩúË¥üËΩΩÁöÑÂ±ûÊÄßÔºå‰ª•‰∫ÜËß£ÂÖ∂ ANNS ÊïàÁéá„ÄÇ‰∏éÂçïÊ®°ÊÄÅÂ∑•‰ΩúË¥üËΩΩ‰∏çÂêåÔºåÊàë‰ª¨Êè≠Á§∫‰∫Ü OOD Êü•ËØ¢Âú®Á©∫Èó¥‰∏äÂÅèÁ¶ªÂü∫Á°ÄÊï∞ÊçÆÔºåÂπ∂‰∏î OOD Êü•ËØ¢ÁöÑ k ‰∏™ÊúÄËøëÈÇªÂú®ÂµåÂÖ•Á©∫Èó¥‰∏≠ÂΩºÊ≠§Áõ∏Ë∑ùÁîöËøú„ÄÇËØ•Â±ûÊÄßÊâìÁ†¥‰∫ÜÁé∞Êúâ ANNS ÊñπÊ≥ïÁöÑÂÅáËÆæÔºåÂπ∂‰∏î‰∏çÂåπÈÖçÂÆÉ‰ª¨‰∏∫È´òÊïàÊêúÁ¥¢ËÄåËÆæËÆ°ÁöÑÂÅáËÆæ„ÄÇÈÄöËøáÂØπ OOD Â∑•‰ΩúË¥üËΩΩÁöÑËßÅËß£ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü pRojected ‰∫åÂàÜÂõæ (RoarGraph)ÔºåËøôÊòØ‰∏ÄÁßçÂú®Êü•ËØ¢ÂàÜÂ∏ÉÊåáÂØº‰∏ãÊûÑÂª∫ÁöÑÈ´òÊïà ANNS ÂõæÂΩ¢Á¥¢Âºï„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåRoarGraph Âú®Áé∞‰ª£Ë∑®Ê®°ÊÄÅÊï∞ÊçÆÈõÜ‰∏äÊòéÊòæ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú® OOD Êü•ËØ¢ÁöÑ 90% Âè¨ÂõûÁéá‰∏ãÂÆûÁé∞‰∫ÜÈ´òËææ 3.56 ÂÄçÁöÑÊõ¥Âø´ÊêúÁ¥¢ÈÄüÂ∫¶„ÄÇ

##### **Handling abort commands for household kitchen robots**
2408.14480v1 by Darius Has, Adrian Groza, Mihai Pomarlan

We propose a solution for handling abort commands given to robots. The
solution is exemplified with a running scenario with household kitchen robots.
The robot uses planning to find sequences of actions that must be performed in
order to gracefully cancel a previously received command. The Planning Domain
Definition Language (PDDL) is used to write a domain to model kitchen
activities and behaviours, and this domain is enriched with knowledge from
online ontologies and knowledge graphs, like DBPedia. We discuss the results
obtained in different scenarios.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËôïÁêÜÁôºÈÄÅÁµ¶Ê©üÂô®‰∫∫ÁöÑ‰∏≠Ê≠¢ÂëΩ‰ª§ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ
ÈÄôÂÄãËß£Ê±∫ÊñπÊ°à‰ª•ÂÆ∂Áî®ÂªöÊàøÊ©üÂô®‰∫∫ÁöÑÂü∑Ë°åÊÉÖÂ¢ÉÁÇ∫‰æã„ÄÇ
Ê©üÂô®‰∫∫‰ΩøÁî®Ë¶èÂäÉ‰æÜÂ∞ãÊâæÂøÖÈ†àÂü∑Ë°åÁöÑÂãï‰ΩúÂ∫èÂàóÔºå‰ª•‰æøÂÑ™ÈõÖÂú∞ÂèñÊ∂àÂÖàÂâçÊé•Êî∂ÁöÑÂëΩ‰ª§„ÄÇË¶èÂäÉÈ†òÂüüÂÆöÁæ©Ë™ûË®Ä (PDDL) Áî®ÊñºÊí∞ÂØ´‰∏ÄÂÄãÁ∂≤Âüü‰æÜÂª∫Ê®°ÂªöÊàøÊ¥ªÂãïÂíåË°åÁÇ∫ÔºåËÄåÈÄôÂÄãÁ∂≤ÂüüÂâáÈÄèÈÅéÁ∑ö‰∏äÊú¨‰ΩìÂíåÁü•Ë≠òÂúñË°®Ôºà‰æãÂ¶Ç DBPediaÔºâÁöÑÁü•Ë≠ò‰æÜË±êÂØå„ÄÇÊàëÂÄëË®éË´ñÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠Áç≤ÂæóÁöÑÁµêÊûú„ÄÇ

##### **CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**
2408.08535v1 by Rong-Ching Chang, Jiawei Zhang

Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÊúâÈÄ≤Ê≠•Ôºå‰ΩÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÁ∂ìÂ∏∏ÂèóÂà∞Áº∫‰πèËàáÂØ¶È´îÈóú‰øÇÂíåÁ§æÁæ§ÁµêÊßãÊï¥ÂêàÁöÑÈòªÁ§ôÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÊèê‰æõËÑàÁµ°Ë±êÂØå‰∏îÊ∫ñÁ¢∫ÁöÑË≥áË®äÊ™¢Á¥¢‰ª•ÈÄ≤Ë°å‰∫ãÂØ¶Êü•Ê†∏ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄë‰ªãÁ¥π CommunityKG-RAGÔºàÁ§æÁæ§Áü•Ë≠òÂúñË≠úÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÂ∞áÁü•Ë≠òÂúñË≠ú (KG) ÂÖßÁöÑÁ§æÁæ§ÁµêÊßãËàá RAG Á≥ªÁµ±Êï¥ÂêàÔºå‰ª•Â¢ûÂº∑‰∫ãÂØ¶Êü•Ê†∏ÊµÅÁ®ã„ÄÇCommunityKG-RAG ÁÑ°ÈúÄÈ°çÂ§ñË®ìÁ∑¥Â∞±ËÉΩÈÅ©ÊáâÊñ∞ÁöÑÈ†òÂüüÂíåÊü•Ë©¢ÔºåÂÆÉÂà©Áî® KG ÂÖßÁ§æÁæ§ÁµêÊßãÁöÑÂ§öË∑≥ÁâπÊÄßÔºåÂ§ßÂπÖÊèêÂçáË≥áË®äÊ™¢Á¥¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé CommunityKG-RAG ÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºå‰ª£Ë°®Ëëó‰∫ãÂØ¶Êü•Ê†∏ÁöÑÈáçÂ§ßÈÄ≤Ê≠•ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑ÂÅ•„ÄÅÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁéáÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**
2408.08927v1 by Chia-Tung Ho, Haoxing Ren, Brucek Khailany

Due to the growing complexity of modern Integrated Circuits (ICs), automating
hardware design can prevent a significant amount of human error from the
engineering process and result in less errors. Verilog is a popular hardware
description language for designing and modeling digital systems; thus, Verilog
generation is one of the emerging areas of research to facilitate the design
process. In this work, we propose VerilogCoder, a system of multiple Artificial
Intelligence (AI) agents for Verilog code generation, to autonomously write
Verilog code and fix syntax and functional errors using collaborative Verilog
tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we
propose a task planner that utilizes a novel Task and Circuit Relation Graph
retrieval method to construct a holistic plan based on module descriptions. To
debug and fix functional errors, we develop a novel and efficient abstract
syntax tree (AST)-based waveform tracing tool, which is integrated within the
autonomous Verilog completion flow. The proposed methodology successfully
generates 94.2% syntactically and functionally correct Verilog code, surpassing
the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.

ÊëòË¶ÅÔºöÁî±ÊñºÁèæ‰ª£Êï¥ÂêàÈõªË∑Ø (IC) ÁöÑË§áÈõúÊÄßÊó•ÁõäÂ¢ûÂä†ÔºåËá™ÂãïÂåñÁ°¨È´îË®≠Ë®àÂèØ‰ª•Èò≤Ê≠¢Â∑•Á®ãÈÅéÁ®ã‰∏≠Âá∫ÁèæÂ§ßÈáèÁöÑ‰∫∫ÁÇ∫ÈåØË™§Ôºå‰∏¶Ê∏õÂ∞ëÈåØË™§„ÄÇVerilog ÊòØ‰∏ÄÁ®ÆÊµÅË°åÁöÑÁ°¨È´îÊèèËø∞Ë™ûË®ÄÔºåÁî®ÊñºË®≠Ë®àÂíåÂª∫Ê®°Êï∏‰ΩçÁ≥ªÁµ±ÔºõÂõ†Ê≠§ÔºåVerilog Áî¢ÁîüÊòØÊñ∞ËààÁöÑÁ†îÁ©∂È†òÂüü‰πã‰∏ÄÔºåÊó®Âú®‰øÉÈÄ≤Ë®≠Ë®àÈÅéÁ®ã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ VerilogCoderÔºå‰∏ÄÂÄãÁî±Â§öÂÄã‰∫∫Â∑•Êô∫ÊÖß (AI) ‰ª£ÁêÜÁµÑÊàêÁöÑÁ≥ªÁµ±ÔºåÁî®Êñº Verilog Á®ãÂºèÁ¢ºÁî¢ÁîüÔºå‰ª•Ëá™‰∏ªÊí∞ÂØ´ Verilog Á®ãÂºèÁ¢º‰∏¶‰ΩøÁî®Âçî‰ΩúÂºè Verilog Â∑•ÂÖ∑Ôºà‰æãÂ¶ÇÔºåË™ûÊ≥ïÊ™¢Êü•Âô®„ÄÅÊ®°Êì¨Âô®ÂíåÊ≥¢ÂΩ¢ËøΩËπ§Âô®Ôºâ‰øÆÂæ©Ë™ûÊ≥ïÂíåÂäüËÉΩÈåØË™§„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ªªÂãôË¶èÂäÉÂô®ÔºåÂÆÉÂà©Áî®Êñ∞Á©éÁöÑ‰ªªÂãôÂíåÈõªË∑ØÈóú‰øÇÂúñÊì∑ÂèñÊñπÊ≥ïÔºåÊ†πÊìöÊ®°ÁµÑÊèèËø∞Âª∫Êßã‰∏ÄÂÄãÊï¥È´îË®àÁï´„ÄÇÁÇ∫‰∫ÜÈô§ÈåØÂíå‰øÆÂæ©ÂäüËÉΩÈåØË™§ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈ´òÊïàÁöÑÂü∫ÊñºÊäΩË±°Ë™ûÊ≥ïÊ®π (AST) ÁöÑÊ≥¢ÂΩ¢ËøΩËπ§Â∑•ÂÖ∑ÔºåÂÆÉÊï¥ÂêàÂú®Ëá™‰∏ª Verilog ÂÆåÊàêÊµÅÁ®ã‰∏≠„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊàêÂäüÁî¢Áîü‰∫Ü 94.2% Ë™ûÊ≥ïÂíåÂäüËÉΩÊ≠£Á¢∫ÁöÑ Verilog Á®ãÂºèÁ¢ºÔºåÂú® VerilogEval-Human v2 Âü∫Ê∫ñ‰∏äÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÈ´òÂá∫ 33.9%„ÄÇ

##### **Graph Retrieval-Augmented Generation: A Survey**
2408.08921v2 by Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang

Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.

ÊëòË¶ÅÔºöÊúÄËøëÔºåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊåëÊàòÊñπÈù¢ÂèñÂæó‰∫ÜÊòæÁùÄÊàêÂäüÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÈÄöËøáÂèÇËÄÉÂ§ñÈÉ®Áü•ËØÜÂ∫ìÔºåRAG ÊîπËøõ‰∫Ü LLM ÁöÑËæìÂá∫ÔºåÊúâÊïàÂú∞ÂáèËΩª‰∫ÜËØ∏Â¶Ç„ÄåÂπªËßâ„Äç„ÄÅÁº∫‰πèÁâπÂÆöÈ¢ÜÂüüÁü•ËØÜÂíå‰ø°ÊÅØËøáÊó∂Á≠âÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºåÊï∞ÊçÆÂ∫ì‰∏≠‰∏çÂêåÂÆû‰Ωì‰πãÈó¥ÂÖ≥Á≥ªÁöÑÂ§çÊùÇÁªìÊûÑÁªô RAG Á≥ªÁªüÂ∏¶Êù•‰∫ÜÊåëÊàò„ÄÇ‰Ωú‰∏∫ÂõûÂ∫îÔºåGraphRAG Âà©Áî®ÂÆû‰Ωì‰πãÈó¥ÁöÑÁªìÊûÑ‰ø°ÊÅØÊù•ÂÆûÁé∞Êõ¥Á≤æÁ°ÆÂíåÂÖ®Èù¢ÁöÑÊ£ÄÁ¥¢ÔºåÊçïËé∑ÂÖ≥Á≥ªÁü•ËØÜÂπ∂‰øÉËøõÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÂÖ∑‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂìçÂ∫î„ÄÇÈâ¥‰∫é GraphRAG ÁöÑÊñ∞È¢ñÊÄßÂíåÊΩúÂäõÔºåÂØπÂΩìÂâçÊäÄÊúØËøõË°åÁ≥ªÁªüÂÆ°Êü•ÂäøÂú®ÂøÖË°å„ÄÇÊú¨ÊñáÊèê‰æõ‰∫Ü GraphRAG ÊñπÊ≥ïÁöÑÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÊàë‰ª¨ÂΩ¢ÂºèÂåñ‰∫Ü GraphRAG Â∑•‰ΩúÊµÅÔºåÂåÖÊã¨Âü∫‰∫éÂõæÁöÑÁ¥¢Âºï„ÄÅÂõæÂºïÂØºÁöÑÊ£ÄÁ¥¢ÂíåÂõæÂ¢ûÂº∫ÁöÑÁîüÊàê„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Âú®ÊØè‰∏™Èò∂ÊÆµÊ¶ÇËø∞‰∫ÜÊ†∏ÂøÉÊäÄÊúØÂíåËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÁ†îÁ©∂‰∫Ü GraphRAG ÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÅÂ∫îÁî®È¢ÜÂüü„ÄÅËØÑ‰º∞ÊñπÊ≥ïÂíåÂ∑•‰∏öÁî®‰æã„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Êé¢ËÆ®‰∫ÜÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÔºå‰ª•ÊøÄÂèëËøõ‰∏ÄÊ≠•ÁöÑÊé¢Á©∂Âπ∂Êé®ËøõËØ•È¢ÜÂüüÁöÑËøõÂ±ï„ÄÇ‰∏∫‰∫ÜËøΩË∏™ËØ•È¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÊàë‰ª¨Âú® \url{https://github.com/pengboci/GraphRAG-Survey} ‰∏äÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Â≠òÂÇ®Â∫ì„ÄÇ

##### **Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**
2408.07852v1 by Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, Simon Kornblith

While many capabilities of language models (LMs) improve with increased
training budget, the influence of scale on hallucinations is not yet fully
understood. Hallucinations come in many forms, and there is no universally
accepted definition. We thus focus on studying only those hallucinations where
a correct answer appears verbatim in the training set. To fully control the
training data content, we construct a knowledge graph (KG)-based dataset, and
use it to train a set of increasingly large LMs. We find that for a fixed
dataset, larger and longer-trained LMs hallucinate less. However, hallucinating
on $\leq5$% of the training data requires an order of magnitude larger model,
and thus an order of magnitude more compute, than Hoffmann et al. (2022)
reported was optimal. Given this costliness, we study how hallucination
detectors depend on scale. While we see detector size improves performance on
fixed LM's outputs, we find an inverse relationship between the scale of the LM
and the detectability of its hallucinations.

ÊëòË¶ÅÔºöÈõñÁÑ∂Ë™ûË®ÄÊ®°Âûã (LM) ÁöÑË®±Â§öËÉΩÂäõÊúÉÈö®ËëóË®ìÁ∑¥È†êÁÆóÁöÑÂ¢ûÂä†ËÄåÊúâÊâÄÊèêÂçáÔºå‰ΩÜË¶èÊ®°Â∞çÂπªË¶∫ÁöÑÂΩ±ÈüøÂ∞öÊú™ÂÆåÂÖ®‰∫ÜËß£„ÄÇÂπªË¶∫ÊúâË®±Â§öÂΩ¢ÂºèÔºå‰∏îÊ≤íÊúâÊôÆÈÅçÊé•ÂèóÁöÑÂÆöÁæ©„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂè™Â∞àÊ≥®ÊñºÁ†îÁ©∂Ë®ìÁ∑¥ÈõÜ‰∏≠Âá∫ÁèæÊ≠£Á¢∫Á≠îÊ°àÁöÑÂπªË¶∫„ÄÇÁÇ∫‰∫ÜÂÆåÂÖ®ÊéßÂà∂Ë®ìÁ∑¥Ë≥áÊñôÂÖßÂÆπÔºåÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñË≠ú (KG) ÁöÑË≥áÊñôÈõÜÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜË®ìÁ∑¥‰∏ÄÁµÑË∂ä‰æÜË∂äÂ§ßÁöÑ LM„ÄÇÊàëÂÄëÁôºÁèæÂ∞çÊñºÂõ∫ÂÆöÁöÑË≥áÊñôÈõÜÔºåË¶èÊ®°ËºÉÂ§ß‰∏îË®ìÁ∑¥ÊôÇÈñìËºÉÈï∑ÁöÑ LM Áî¢ÁîüÁöÑÂπªË¶∫ËºÉÂ∞ë„ÄÇÁÑ∂ËÄåÔºåÂú® $\leq5$% ÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏äÁî¢ÁîüÂπªË¶∫ÈúÄË¶ÅË¶èÊ®°Â§ß‰∏ÄÂÄãÊï∏ÈáèÁ¥öÁöÑÊ®°ÂûãÔºåÂõ†Ê≠§ÊØî Hoffmann Á≠â‰∫∫ (2022) ÊâÄÂ†±ÂëäÁöÑÊúÄ‰Ω≥Ë¶èÊ®°Â§ö‰∏ÄÂÄãÊï∏ÈáèÁ¥öÁöÑÈÅãÁÆóÊàêÊú¨„ÄÇËÄÉÈáèÂà∞ÈÄôÁ®ÆÊàêÊú¨ÔºåÊàëÂÄëÁ†îÁ©∂ÂπªË¶∫ÂÅµÊ∏¨Âô®Â¶Ç‰ΩïÂèñÊ±∫ÊñºË¶èÊ®°„ÄÇÈõñÁÑ∂ÊàëÂÄëÁúãÂà∞ÂÅµÊ∏¨Âô®Ë¶èÊ®°ÊúÉÊèêÂçáÂ∞çÂõ∫ÂÆö LM Ëº∏Âá∫ÁöÑÊïàËÉΩÔºå‰ΩÜÊàëÂÄëÁôºÁèæ LM ÁöÑË¶èÊ®°ËàáÂÖ∂ÂπªË¶∫ÁöÑÂèØÂÅµÊ∏¨ÊÄß‰πãÈñìÂ≠òÂú®ÂèçÊØîÈóú‰øÇ„ÄÇ

##### **ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**
2408.07840v1 by Xuanqing Yu, Wangtao Sun, Jingwei Li, Kang Liu, Chengbao Liu, Jie Tan

In the realm of event prediction, temporal knowledge graph forecasting (TKGF)
stands as a pivotal technique. Previous approaches face the challenges of not
utilizing experience during testing and relying on a single short-term history,
which limits adaptation to evolving data. In this paper, we introduce the
Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by
integrating dynamic causal rule mining (DCRM) and dual history augmented
generation (DHAG). DCRM dynamically constructs causal rules from real-time
data, allowing for swift adaptation to new causal relationships. In parallel,
DHAG merges short-term and long-term historical contexts, leveraging a
bi-branch approach to enrich event prediction. Our framework demonstrates
notable performance enhancements across diverse datasets, with significant
Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language
models (LLMs) for event prediction without necessitating extensive retraining.
The ONSEP framework not only advances the field of TKGF but also underscores
the potential of neural-symbolic approaches in adapting to dynamic data
environments.

ÊëòË¶ÅÔºöÂú®‰∫ã‰ª∂È†êÊ∏¨È†òÂüü‰∏≠ÔºåÊôÇÂ∫èÁü•Ë≠òÂúñË≠úÈ†êÊ∏¨ (TKGF) ÊòØ‰∏ÄÂÄãÈóúÈçµÊäÄË°ì„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÈù¢Ëá®Âú®Ê∏¨Ë©¶ÊúüÈñì‰∏çÂà©Áî®Á∂ìÈ©ó‰ª•Âèä‰æùË≥¥ÂñÆ‰∏ÄÁü≠ÊúüÊ≠∑Âè≤ÁöÑÊåëÊà∞ÔºåÈÄôÈôêÂà∂‰∫ÜÂ∞çÊºîÂåñË≥áÊñôÁöÑÈÅ©ÊáâÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÁ∑ö‰∏äÁ•ûÁ∂ìÁ¨¶Ëôü‰∫ã‰ª∂È†êÊ∏¨ (ONSEP) Êû∂ÊßãÔºåÂÆÉÈÄèÈÅéÊï¥ÂêàÂãïÊÖãÂõ†ÊûúË¶èÂâáÊåñÊéò (DCRM) ÂíåÈõôÈáçÊ≠∑Âè≤Êì¥ÂÖÖÁîüÊàê (DHAG) ‰æÜÂâµÊñ∞„ÄÇDCRM ÂæûÂç≥ÊôÇË≥áÊñô‰∏≠ÂãïÊÖãÂª∫ÊßãÂõ†ÊûúË¶èÂâáÔºåÂÖÅË®±Âø´ÈÄüÈÅ©ÊáâÊñ∞ÁöÑÂõ†ÊûúÈóú‰øÇ„ÄÇÂêåÊôÇÔºåDHAG Âêà‰ΩµÁü≠ÊúüÂíåÈï∑ÊúüÊ≠∑Âè≤ËÑàÁµ°ÔºåÂà©Áî®ÈõôÂàÜÊîØÊñπÊ≥ï‰æÜË±êÂØå‰∫ã‰ª∂È†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫Âá∫È°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºåHit@k (k=1,3,10) ÊúâÈ°ØËëóÁöÑÊîπÂñÑÔºåÂ±ïÁ§∫‰∫ÜÂÆÉÂú®ÁÑ°ÈúÄÂª£Ê≥õÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÊì¥ÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•ÈÄ≤Ë°å‰∫ã‰ª∂È†êÊ∏¨ÁöÑËÉΩÂäõ„ÄÇONSEP Êû∂Êßã‰∏çÂÉÖÊé®Âãï‰∫Ü TKGF È†òÂüüÔºå‰πüÂº∑Ë™ø‰∫ÜÁ•ûÁ∂ìÁ¨¶ËôüÊñπÊ≥ïÂú®ÈÅ©ÊáâÂãïÊÖãË≥áÊñôÁí∞Â¢É‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**
2408.07611v2 by Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu

Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce "phantom" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a "Retrieval-Augmented
Generation (RAG)" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê•µÂ§ßÂú∞‰øÉËøõ‰∫ÜËá™ÈÅ©ÊáâÊô∫ËÉΩ‰ª£ÁêÜÁöÑÈñãÁôºÔºå‰∏¶Ë¢´ÂÆö‰ΩçÁÇ∫ÂØ¶Áèæ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) ÁöÑÈáçË¶ÅÈÄîÂæë„ÄÇÁÑ∂ËÄåÔºåLLM ÂÆπÊòìÁî¢Áîü‰∫ãÂØ¶‰∏ä‰∏çÊ≠£Á¢∫ÁöÑË≥áË®äÔºåËÄå‰∏îÂ∏∏Â∏∏Áî¢Áîü„ÄåÂπªÂΩ±„ÄçÂÖßÂÆπÔºåÈÄôÊúÉÁ†¥Â£ûÂÖ∂ÂèØÈù†ÊÄßÔºåÂ∞çÂÖ∂Âú®ÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÊßãÊàêÂö¥Â≥ªÊåëÊà∞„ÄÇÁµêÂêàÂ§ñÈÉ®Ë≥áÊñôÂ∫´ÂíåË≥áË®äÊ™¢Á¥¢Ê©üÂà∂‰æÜÂ¢ûÂº∑ LLM ÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç‰∏äËø∞ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ WeKnow-RAG ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂ∞áÁ∂≤Ë∑ØÊêúÂ∞ãÂíåÁü•Ë≠òÂúñË≠úÊï¥ÂêàÂà∞„ÄåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)„ÄçÁ≥ªÁµ±‰∏≠„ÄÇÈ¶ñÂÖàÔºåÈÄèÈÅéÁµêÂêàÁü•Ë≠òÂúñË≠úÁöÑÁµêÊßãÂåñË°®Á§∫ÂíåÁ®†ÂØÜÂêëÈáèÊ™¢Á¥¢ÁöÑÈùàÊ¥ªÊÄßÔºå‰æÜÊèêÂçá LLM ÂõûÊáâÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇWeKnow-RAG Êé•ËëóÂà©Áî®ÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÂúñË≠ú‰æÜÊªøË∂≥ÂêÑÁ®ÆÊü•Ë©¢ÂíåÈ†òÂüüÔºåÂæûËÄåÈÄèÈÅé‰ΩøÁî®Á®ÄÁñèÂíåÁ®†ÂØÜÊ™¢Á¥¢ÊñπÊ≥ïÁöÑÂ§öÈöéÊÆµÁ∂≤È†ÅÊ™¢Á¥¢ÊäÄË°ìÔºå‰æÜÊèêÂçá‰∫ãÂØ¶Ë≥áË®äÂíåË§áÈõúÊé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊúâÊïàÂú∞Âπ≥Ë°°‰∫ÜË≥áË®äÊ™¢Á¥¢ÁöÑÊïàÁéáÂíåÊ∫ñÁ¢∫ÊÄßÔºåÈÄ≤ËÄåÊîπÂñÑÊï¥È´îÊ™¢Á¥¢ÊµÅÁ®ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÇÑÊï¥Âêà‰∫Ü‰∏ÄÂÄã LLM Ëá™ÊàëË©ï‰º∞Ê©üÂà∂Ôºå‰ª•Ë©ï‰º∞ÂÖ∂ÊâÄÁî¢ÁîüÁ≠îÊ°àÁöÑÂèØ‰ø°Â∫¶„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Âª£Ê≥õÁöÑÈõ¢Á∑öÂØ¶È©óÂíåÁ∑ö‰∏äÊèê‰∫§‰∏≠Ë≠âÊòé‰∫ÜÂÖ∂ÂÇëÂá∫ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**
2408.07453v1 by Tobias A. Opsahl

Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠Áç≤ÂæóËøëÊúüÊàêÂäüÔºå‰∫ãÂØ¶È©óË≠â‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖËâ±Èõ£ÁöÑ‰ªªÂãô„ÄÇÁî±ÊñºÈåØË™§Ë≥áË®äÂÇ≥Êí≠ÂæóË∂ä‰æÜË∂äÂø´ÔºåÊ≥®ÊÑèÂäõÂ∑≤ËΩâÂêëËá™ÂãïÈ©óË≠âËÅ≤ÊòéÁöÑÊ≠£Á¢∫ÊÄß„ÄÇÂú® NLP È†òÂüü‰∏≠ÔºåÈÄôÈÄöÂ∏∏ÈÄèÈÅéË®ìÁ∑¥Áõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÂÆåÊàêÔºåÈÄô‰∫õÊ®°ÂûãÂà©Áî®‰æÜËá™ÂèØ‰ø°Ë≥¥Ë™ûÊñôÂ∫´ÁöÑË≠âÊìö‰æÜÈ©óË≠âËÅ≤Êòé„ÄÇÊàëÂÄëÊèêÂá∫ÊúâÊïàÁöÑÊñπÊ≥ï‰æÜÈ©óË≠âË≥áÊñôÈõÜ‰∏≠ÁöÑËÅ≤ÊòéÔºåÂÖ∂‰∏≠Ë≠âÊìöÊòØ‰ª•ÁµêÊßãÂåñÁü•Ë≠òÂúñË°®ÁöÑÂΩ¢ÂºèÂëàÁèæ„ÄÇÊàëÂÄë‰ΩøÁî® FactKG Ë≥áÊñôÈõÜÔºåÂÆÉÊòØÁî±ÂæûÁ∂≠Âü∫ÁôæÁßë‰∏≠ËêÉÂèñÁöÑ DBpedia Áü•Ë≠òÂúñË°®ÊâÄÂª∫Êßã„ÄÇÈÄèÈÅéÁ∞°ÂåñË≠âÊìöÊì∑ÂèñÊµÅÁ®ãÔºåÂæûÂæÆË™øË™ûË®ÄÊ®°ÂûãÂà∞Á∞°ÂñÆÁöÑÈÇèËºØÊì∑ÂèñÔºåÊàëÂÄëËÉΩÂ§†Âª∫ÊßãÊó¢ÈúÄË¶ÅËºÉÂ∞ëË®àÁÆóË≥áÊ∫êÔºåÂèàËÉΩÈÅîÂà∞ËºÉ‰Ω≥Ê∏¨Ë©¶ÈõÜÊ∫ñÁ¢∫Â∫¶ÁöÑÊ®°Âûã„ÄÇ

##### **LLMs can Schedule**
2408.06993v1 by Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave

The job shop scheduling problem (JSSP) remains a significant hurdle in
optimizing production processes. This challenge involves efficiently allocating
jobs to a limited number of machines while minimizing factors like total
processing time or job delays. While recent advancements in artificial
intelligence have yielded promising solutions, such as reinforcement learning
and graph neural networks, this paper explores the potential of Large Language
Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset
specifically designed to train LLMs for JSSP. Surprisingly, our findings
demonstrate that LLM-based scheduling can achieve performance comparable to
other neural approaches. Furthermore, we propose a sampling method that
enhances the effectiveness of LLMs in tackling JSSP.

ÊëòË¶ÅÔºö‰ΩúÊ•≠ËªäÈñìÊéíÁ®ãÂïèÈ°å (JSSP) ‰ªçÁÑ∂ÊòØÊúÄ‰Ω≥ÂåñÁîüÁî¢ÊµÅÁ®ã‰∏≠ÁöÑ‰∏ÄÂ§ßÈöúÁ§ô„ÄÇÈÄôÈ†ÖÊåëÊà∞Ê∂âÂèäÂ∞á‰ΩúÊ•≠ÊúâÊïàÂàÜÈÖçÂà∞Êï∏ÈáèÊúâÈôêÁöÑÊ©üÂô®ÔºåÂêåÊôÇÂ∞áÁ∏ΩËôïÁêÜÊôÇÈñìÊàñ‰ΩúÊ•≠Âª∂ÈÅ≤Á≠âÂõ†Á¥†ÈôçËá≥ÊúÄ‰Ωé„ÄÇÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖßÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤Áî¢ÁîüÊúâÂ∏åÊúõÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰æãÂ¶ÇÂº∑ÂåñÂ≠∏ÁøíÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰ΩÜÊú¨ÊñáÊé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú® JSSP ‰∏≠ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÁ¨¨‰∏ÄÂÄãÁõ£Áù£Âºè 120k Ë≥áÊñôÈõÜÔºåÂ∞àÈñÄÁî®ÊñºË®ìÁ∑¥ JSSP ÁöÑ LLM„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂü∫Êñº LLM ÁöÑÊéíÁ®ãÂèØ‰ª•ÈÅîÂà∞ËàáÂÖ∂‰ªñÁ•ûÁ∂ìÊñπÊ≥ïÁõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊäΩÊ®£ÊñπÊ≥ïÔºåÂèØÂ¢ûÂº∑ LLM Âú®ËôïÁêÜ JSSP ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Causal Agent based on Large Language Model**
2408.06849v1 by Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye, Fei Wu

Large language models (LLMs) have achieved significant success across various
domains. However, the inherent complexity of causal problems and causal theory
poses challenges in accurately describing them in natural language, making it
difficult for LLMs to comprehend and use them effectively. Causal methods are
not easily conveyed through natural language, which hinders LLMs' ability to
apply them accurately. Additionally, causal datasets are typically tabular,
while LLMs excel in handling natural language data, creating a structural
mismatch that impedes effective reasoning with tabular data. This lack of
causal reasoning capability limits the development of LLMs. To address these
challenges, we have equipped the LLM with causal tools within an agent
framework, named the Causal Agent, enabling it to tackle causal problems. The
causal agent comprises tools, memory, and reasoning modules. In the tools
module, the causal agent applies causal methods to align tabular data with
natural language. In the reasoning module, the causal agent employs the ReAct
framework to perform reasoning through multiple iterations with the tools. In
the memory module, the causal agent maintains a dictionary instance where the
keys are unique names and the values are causal graphs. To verify the causal
ability of the causal agent, we established a benchmark consisting of four
levels of causal problems: variable level, edge level, causal graph level, and
causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for
these four levels of issues and tested the causal agent on the datasets. Our
methodology demonstrates remarkable efficacy on the four-level causal problems,
with accuracy rates all above 80%. For further insights and implementation
details, our code is accessible via the GitHub repository
https://github.com/Kairong-Han/Causal_Agent.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÂÄãÈ†òÂüüÂèñÂæóÈáçÂ§ßÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂõ†ÊûúÂïèÈ°åÂíåÂõ†ÊûúÁêÜË´ñÁöÑÂÖßÂú®Ë§áÈõúÊÄßÔºåÂú®Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠Ê∫ñÁ¢∫ÊèèËø∞ÂÆÉÂÄëÊôÇÊßãÊàêÊåëÊà∞ÔºåÈÄô‰ΩøÂæó LLM Èõ£‰ª•ÁêÜËß£‰∏¶ÊúâÊïà‰ΩøÁî®ÂÆÉÂÄë„ÄÇÂõ†ÊûúÊñπÊ≥ï‰∏çÊòìÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÂÇ≥ÈÅîÔºåÈÄôÈòªÁ§ô‰∫Ü LLM Ê∫ñÁ¢∫ÊáâÁî®ÂÆÉÂÄëÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂõ†ÊûúË≥áÊñôÈõÜÈÄöÂ∏∏ÊòØË°®Ê†ºÂåñÁöÑÔºåËÄå LLM ÊìÖÈï∑ËôïÁêÜËá™ÁÑ∂Ë™ûË®ÄË≥áÊñôÔºåÈÄôÈÄ†Êàê‰∫ÜÁµêÊßã‰∏äÁöÑ‰∏çÂåπÈÖçÔºåÈòªÁ§ô‰∫ÜÂ∞çË°®Ê†ºË≥áÊñôÈÄ≤Ë°åÊúâÊïàÁöÑÊé®ÁêÜ„ÄÇÈÄôÁ®ÆÁº∫‰πèÂõ†ÊûúÊé®ÁêÜËÉΩÂäõÈôêÂà∂‰∫Ü LLM ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂú®‰∏ÄÂÄã‰ª£ÁêÜÊ°ÜÊû∂‰∏≠ÁÇ∫ LLM ÈÖçÂÇô‰∫ÜÂõ†ÊûúÂ∑•ÂÖ∑ÔºåÁ®±ÁÇ∫Âõ†Êûú‰ª£ÁêÜÔºå‰ΩøÂÆÉËÉΩÂ§†Ëß£Ê±∫Âõ†ÊûúÂïèÈ°å„ÄÇÂõ†Êûú‰ª£ÁêÜÂåÖÂê´Â∑•ÂÖ∑„ÄÅË®òÊÜ∂È´îÂíåÊé®ÁêÜÊ®°ÁµÑ„ÄÇÂú®Â∑•ÂÖ∑Ê®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÊáâÁî®Âõ†ÊûúÊñπÊ≥ïÂ∞áË°®Ê†ºË≥áÊñôËàáËá™ÁÑ∂Ë™ûË®ÄÂ∞çÈΩä„ÄÇÂú®Êé®ÁêÜÊ®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÊé°Áî® ReAct Ê°ÜÊû∂ÔºåÈÄèÈÅéËàáÂ∑•ÂÖ∑ÈÄ≤Ë°åÂ§öÊ¨°ÂèçË¶ÜÈÅãÁÆó‰æÜÂü∑Ë°åÊé®ÁêÜ„ÄÇÂú®Ë®òÊÜ∂È´îÊ®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÁ∂≠Ë≠∑‰∏ÄÂÄãÂ≠óÂÖ∏ÂØ¶‰æãÔºåÂÖ∂‰∏≠ÈçµÊòØÂîØ‰∏ÄÂêçÁ®±ÔºåËÄåÂÄºÊòØÂõ†ÊûúÂúñ„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÂõ†Êûú‰ª£ÁêÜÁöÑÂõ†ÊûúËÉΩÂäõÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂü∫Ê∫ñÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõõÂÄãÂ±§Á¥öÁöÑÂõ†ÊûúÂïèÈ°åÔºöËÆäÊï∏Â±§Á¥ö„ÄÅÈÇäÂ±§Á¥ö„ÄÅÂõ†ÊûúÂúñÂ±§Á¥öÂíåÂõ†ÊûúÊïàÊáâÂ±§Á¥ö„ÄÇÊàëÂÄë‰ΩøÁî® ChatGPT-3.5 ÁÇ∫ÈÄôÂõõÂÄãÂ±§Á¥öÁöÑÂïèÈ°åÁî¢Áîü‰∫Ü 1.3K ÁöÑÊ∏¨Ë©¶Ë≥áÊñôÈõÜÔºå‰∏¶Âú®Ë≥áÊñôÈõÜ‰∏äÊ∏¨Ë©¶‰∫ÜÂõ†Êûú‰ª£ÁêÜ„ÄÇÊàëÂÄëÁöÑÈÄôÂ•óÊñπÊ≥ïÂú®ÂõõÂÄãÂ±§Á¥öÁöÑÂõ†ÊûúÂïèÈ°å‰∏äÂ±ïÁèæ‰∫ÜÈ°ØËëóÁöÑÂäüÊïàÔºåÊ∫ñÁ¢∫ÁéáÈÉΩÈ´òÊñº 80%„ÄÇÊúâÈóúÈÄ≤‰∏ÄÊ≠•ÁöÑË¶ãËß£ÂíåÂØ¶‰ΩúÁ¥∞ÁØÄÔºåÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÈÄèÈÅé GitHub ÂÑ≤Â≠òÂ∫´ https://github.com/Kairong-Han/Causal_Agent ÂèñÂæó„ÄÇ


### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**|Basel Mousi et.al.|[2409.11404v1](http://arxiv.org/abs/2409.11404v1)|null|
|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|Wenliang Dai et.al.|[2409.11402v1](http://arxiv.org/abs/2409.11402v1)|null|
|**2024-09-17**|**LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**|Amine B. Hassouna et.al.|[2409.11393v1](http://arxiv.org/abs/2409.11393v1)|null|
|**2024-09-17**|**Says Who? Effective Zero-Shot Annotation of Focalization**|Rebecca M. M. Hicke et.al.|[2409.11390v1](http://arxiv.org/abs/2409.11390v1)|null|
|**2024-09-17**|**Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**|Simon Yu et.al.|[2409.11378v1](http://arxiv.org/abs/2409.11378v1)|null|
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375v1](http://arxiv.org/abs/2409.11375v1)|null|
|**2024-09-17**|**CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**|Jiahui Gao et.al.|[2409.11365v1](http://arxiv.org/abs/2409.11365v1)|null|
|**2024-09-17**|**CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**|Zachary S. Siegel et.al.|[2409.11363v1](http://arxiv.org/abs/2409.11363v1)|[link](https://github.com/siegelz/core-bench)|
|**2024-09-17**|**AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**|Dhruv Agarwal et.al.|[2409.11360v1](http://arxiv.org/abs/2409.11360v1)|null|
|**2024-09-17**|**RenderWorld: World Model with Self-Supervised 3D Label**|Ziyang Yan et.al.|[2409.11356v1](http://arxiv.org/abs/2409.11356v1)|null|
|**2024-09-17**|**THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**|Mengfei Liang et.al.|[2409.11353v1](http://arxiv.org/abs/2409.11353v1)|null|
|**2024-09-17**|**Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**|Lauren M. Zuromski et.al.|[2409.11350v1](http://arxiv.org/abs/2409.11350v1)|null|
|**2024-09-17**|**OmniGen: Unified Image Generation**|Shitao Xiao et.al.|[2409.11340v1](http://arxiv.org/abs/2409.11340v1)|[link](https://github.com/vectorspacelab/omnigen)|
|**2024-09-17**|**SOAP: Improving and Stabilizing Shampoo using Adam**|Nikhil Vyas et.al.|[2409.11321v1](http://arxiv.org/abs/2409.11321v1)|[link](https://github.com/nikhilvyas/soap)|
|**2024-09-17**|**MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping**|Amirreza Fateh et.al.|[2409.11316v1](http://arxiv.org/abs/2409.11316v1)|[link](https://github.com/amirrezafateh/msdnet)|
|**2024-09-17**|**SpMis: An Investigation of Synthetic Spoken Misinformation Detection**|Peizhuo Liu et.al.|[2409.11308v1](http://arxiv.org/abs/2409.11308v1)|null|
|**2024-09-17**|**TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**|Rong Zhou et.al.|[2409.11299v1](http://arxiv.org/abs/2409.11299v1)|null|
|**2024-09-17**|**EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**|Zeyi Liao et.al.|[2409.11295v1](http://arxiv.org/abs/2409.11295v1)|null|
|**2024-09-17**|**Navigating Process Mining: A Case study using pm4py**|Ali Jlidi et.al.|[2409.11294v1](http://arxiv.org/abs/2409.11294v1)|null|
|**2024-09-17**|**Neural Networks for Vehicle Routing Problem**|L√°szl√≥ Kov√°cs et.al.|[2409.11290v1](http://arxiv.org/abs/2409.11290v1)|null|
|**2024-09-17**|**Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling**|Xinyue Fang et.al.|[2409.11283v2](http://arxiv.org/abs/2409.11283v2)|null|
|**2024-09-17**|**Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**|Marcel Lamott et.al.|[2409.11282v1](http://arxiv.org/abs/2409.11282v1)|null|
|**2024-09-17**|**P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task**|Weiye Xu et.al.|[2409.11279v1](http://arxiv.org/abs/2409.11279v1)|null|
|**2024-09-17**|**Machine Learning and Theory Ladenness -- A Phenomenological Account**|Alberto Termine et.al.|[2409.11277v1](http://arxiv.org/abs/2409.11277v1)|null|
|**2024-09-17**|**Task Arithmetic for Language Expansion in Speech Translation**|Yao-Fei Cheng et.al.|[2409.11274v1](http://arxiv.org/abs/2409.11274v1)|null|
|**2024-09-17**|**LOLA -- An Open-Source Massively Multilingual Large Language Model**|Nikit Srivastava et.al.|[2409.11272v2](http://arxiv.org/abs/2409.11272v2)|[link](https://github.com/dice-group/lola)|
|**2024-09-17**|**Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids**|Caio Fabio Oliveira da Silva et.al.|[2409.11267v1](http://arxiv.org/abs/2409.11267v1)|null|
|**2024-09-17**|**Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models**|Jiahao Qin et.al.|[2409.11263v1](http://arxiv.org/abs/2409.11263v1)|null|
|**2024-09-17**|**The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection**|Gabriel Bibb√≥ et.al.|[2409.11262v1](http://arxiv.org/abs/2409.11262v1)|[link](https://github.com/gbibbo/voice_anonymization)|
|**2024-09-17**|**The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives**|Samee Arif et.al.|[2409.11261v2](http://arxiv.org/abs/2409.11261v2)|[link](https://github.com/ulrs0/The-Art-of-Story-Telling)|
|**2024-09-17**|**Attacking Slicing Network via Side-channel Reinforcement Learning Attack**|Wei Shao et.al.|[2409.11258v1](http://arxiv.org/abs/2409.11258v1)|null|
|**2024-09-17**|**Norm of Mean Contextualized Embeddings Determines their Variance**|Hiroaki Yamagiwa et.al.|[2409.11253v1](http://arxiv.org/abs/2409.11253v1)|null|
|**2024-09-17**|**WER We Stand: Benchmarking Urdu ASR Models**|Samee Arif et.al.|[2409.11252v1](http://arxiv.org/abs/2409.11252v1)|null|
|**2024-09-17**|**Linear Recency Bias During Training Improves Transformers' Fit to Reading Times**|Christian Clark et.al.|[2409.11250v1](http://arxiv.org/abs/2409.11250v1)|null|
|**2024-09-17**|**Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse**|Maojia Song et.al.|[2409.11242v1](http://arxiv.org/abs/2409.11242v1)|[link](https://github.com/declare-lab/trust-align)|
|**2024-09-17**|**Spontaneous Informal Speech Dataset for Punctuation Restoration**|Xing Yi Liu et.al.|[2409.11241v1](http://arxiv.org/abs/2409.11241v1)|[link](https://github.com/githubaccountanonymous/pr)|
|**2024-09-17**|**LLM-as-a-Judge & Reward Model: What They Can and Cannot Do**|Guijin Son et.al.|[2409.11239v1](http://arxiv.org/abs/2409.11239v1)|null|
|**2024-09-17**|**Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models**|Bishwash Khanal et.al.|[2409.11233v1](http://arxiv.org/abs/2409.11233v1)|null|
|**2024-09-17**|**Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?**|Raffaele Marino et.al.|[2409.11232v1](http://arxiv.org/abs/2409.11232v1)|[link](https://github.com/raffaelemarino/analysisopenaio1modelksat)|
|**2024-09-17**|**Learning Source Disentanglement in Neural Audio Codec**|Xiaoyu Bie et.al.|[2409.11228v1](http://arxiv.org/abs/2409.11228v1)|null|
|**2024-09-17**|**Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis**|Lingling Xu et.al.|[2409.11218v1](http://arxiv.org/abs/2409.11218v1)|null|
|**2024-09-17**|**Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization**|Jianing Wang et.al.|[2409.11212v1](http://arxiv.org/abs/2409.11212v1)|null|
|**2024-09-17**|**SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds**|Zhixing Hou et.al.|[2409.11195v1](http://arxiv.org/abs/2409.11195v1)|null|
|**2024-09-17**|**Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**|Eunhae Lee et.al.|[2409.11192v1](http://arxiv.org/abs/2409.11192v1)|null|
|**2024-09-17**|**SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer**|Anmol Gautam et.al.|[2409.11190v1](http://arxiv.org/abs/2409.11190v1)|null|
|**2024-09-17**|**Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer**|Yanqing Kang et.al.|[2409.11174v1](http://arxiv.org/abs/2409.11174v1)|null|
|**2024-09-17**|**SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration**|Xin Guan et.al.|[2409.11149v1](http://arxiv.org/abs/2409.11149v1)|null|
|**2024-09-17**|**Improving the Efficiency of Visually Augmented Language Models**|Paula Ontalvilla et.al.|[2409.11148v1](http://arxiv.org/abs/2409.11148v1)|null|
|**2024-09-17**|**Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**|Yukang Lin et.al.|[2409.11147v1](http://arxiv.org/abs/2409.11147v1)|[link](https://github.com/yukang-lin/rger)|
|**2024-09-17**|**High-Resolution Speech Restoration with Latent Diffusion Model**|Tushar Dhyani et.al.|[2409.11145v1](http://arxiv.org/abs/2409.11145v1)|null|
|**2024-09-17**|**Semformer: Transformer Language Models with Semantic Planning**|Yongjing Yin et.al.|[2409.11143v1](http://arxiv.org/abs/2409.11143v1)|null|
|**2024-09-17**|**Learning Generalized Hamiltonians using fully Symplectic Mappings**|Harsh Choudhary et.al.|[2409.11138v1](http://arxiv.org/abs/2409.11138v1)|null|
|**2024-09-17**|**Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models**|Orion Weller et.al.|[2409.11136v1](http://arxiv.org/abs/2409.11136v1)|[link](https://github.com/orionw/promptriever)|
|**2024-09-17**|**Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach**|Debarpan Bhattacharya et.al.|[2409.11123v1](http://arxiv.org/abs/2409.11123v1)|[link](https://github.com/iiscleap/dax)|
|**2024-09-17**|**Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection**|Bo Liu et.al.|[2409.11114v1](http://arxiv.org/abs/2409.11114v1)|null|
|**2024-09-17**|**Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games**|Matƒ´ss Rikters et.al.|[2409.11112v1](http://arxiv.org/abs/2409.11112v1)|null|
|**2024-09-17**|**MonoKAN: Certified Monotonic Kolmogorov-Arnold Network**|Alejandro Polo-Molina et.al.|[2409.11078v1](http://arxiv.org/abs/2409.11078v1)|null|
|**2024-09-17**|**RoMath: A Mathematical Reasoning Benchmark in Romanian**|Adrian Cosma et.al.|[2409.11074v1](http://arxiv.org/abs/2409.11074v1)|[link](https://github.com/cosmaadrian/romath)|
|**2024-09-17**|**Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression algorithms**|Andrew Antonopoulos et.al.|[2409.11071v1](http://arxiv.org/abs/2409.11071v1)|null|
|**2024-09-17**|**KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models**|Bo Lv et.al.|[2409.11057v1](http://arxiv.org/abs/2409.11057v1)|null|
|**2024-09-17**|**Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts**|Teng Wang et.al.|[2409.11056v1](http://arxiv.org/abs/2409.11056v1)|null|
|**2024-09-17**|**A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B**|Jemin Lee et.al.|[2409.11055v1](http://arxiv.org/abs/2409.11055v1)|null|
|**2024-09-17**|**Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming**|Chalamalasetti Kranti et.al.|[2409.11041v2](http://arxiv.org/abs/2409.11041v2)|null|
|**2024-09-17**|**Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI**|Riona Matsuoka et.al.|[2409.11032v1](http://arxiv.org/abs/2409.11032v1)|null|
|**2024-09-17**|**D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding**|Xiaobao Song et.al.|[2409.11024v1](http://arxiv.org/abs/2409.11024v1)|null|
|**2024-09-17**|**GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models**|Hanjun Luo et.al.|[2409.11022v2](http://arxiv.org/abs/2409.11022v2)|null|
|**2024-09-17**|**Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models**|Emile Saillard et.al.|[2409.11011v1](http://arxiv.org/abs/2409.11011v1)|null|
|**2024-09-17**|**CAST: Cross-modal Alignment Similarity Test for Vision Language Models**|Gautier Dagan et.al.|[2409.11007v1](http://arxiv.org/abs/2409.11007v1)|[link](https://github.com/gautierdag/cast)|
|**2024-09-17**|**Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation**|Gerard I. G√°llego et.al.|[2409.11003v1](http://arxiv.org/abs/2409.11003v1)|null|
|**2024-09-17**|**Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models**|Potsawee Manakul et.al.|[2409.10999v1](http://arxiv.org/abs/2409.10999v1)|null|
|**2024-09-17**|**Contextual Breach: Assessing the Robustness of Transformer-based QA Models**|Asir Saadat et.al.|[2409.10997v2](http://arxiv.org/abs/2409.10997v2)|null|
|**2024-09-17**|**Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**|Dingjie Song et.al.|[2409.10994v1](http://arxiv.org/abs/2409.10994v1)|null|
|**2024-09-17**|**Control-flow Reconstruction Attacks on Business Process Models**|Henrik Kirchmann et.al.|[2409.10986v1](http://arxiv.org/abs/2409.10986v1)|[link](https://github.com/henrikkirchmann/control-flow-reconstruction)|
|**2024-09-17**|**Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection**|Hsi-Che Lin et.al.|[2409.10985v1](http://arxiv.org/abs/2409.10985v1)|null|
|**2024-09-17**|**Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data**|Jing Xu et.al.|[2409.10969v1](http://arxiv.org/abs/2409.10969v1)|null|
|**2024-09-17**|**Cross-lingual transfer of multilingual models on low resource African Languages**|Harish Thangaraj et.al.|[2409.10965v1](http://arxiv.org/abs/2409.10965v1)|null|
|**2024-09-17**|**Active learning for energy-based antibody optimization and enhanced screening**|Kairi Furui et.al.|[2409.10964v2](http://arxiv.org/abs/2409.10964v2)|null|
|**2024-09-17**|**Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning**|Min-Yeong Park et.al.|[2409.10956v1](http://arxiv.org/abs/2409.10956v1)|[link](https://github.com/khu-agi/vil)|
|**2024-09-17**|**Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style**|Yuepei Li et.al.|[2409.10955v1](http://arxiv.org/abs/2409.10955v1)|null|
|**2024-09-17**|**Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**|Mehroush Banday et.al.|[2409.10932v1](http://arxiv.org/abs/2409.10932v1)|null|
|**2024-09-17**|**Propulsion: Steering LLM with Tiny Fine-Tuning**|Md Kowsher et.al.|[2409.10927v2](http://arxiv.org/abs/2409.10927v2)|[link](https://github.com/Kowsher/Propulsion)|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|Yanbei Jiang et.al.|[2409.10921v1](http://arxiv.org/abs/2409.10921v1)|[link](https://github.com/yanbei-jiang/artwork-interpretation)|
|**2024-09-17**|**GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval**|Wonduk Seo et.al.|[2409.10909v1](http://arxiv.org/abs/2409.10909v1)|null|
|**2024-09-17**|**Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction**|Erwin D. L√≥pez Z. et.al.|[2409.10907v1](http://arxiv.org/abs/2409.10907v1)|null|
|**2024-09-17**|**WaterQualityNeT: Prediction of Seasonal Water Quality of Nepal Using Hybrid Deep Learning Models**|Biplov Paneru et.al.|[2409.10898v1](http://arxiv.org/abs/2409.10898v1)|null|
|**2024-09-17**|**Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes**|Zhixin Xie et.al.|[2409.10889v1](http://arxiv.org/abs/2409.10889v1)|null|
|**2024-09-17**|**CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization**|Ziwei Gong et.al.|[2409.10883v1](http://arxiv.org/abs/2409.10883v1)|null|
|**2024-09-17**|**American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM**|Gregorius Guntur Sunardi Putra et.al.|[2409.10874v1](http://arxiv.org/abs/2409.10874v1)|null|
|**2024-09-17**|**Adaptive Large Language Models By Layerwise Attention Shortcuts**|Prateek Verma et.al.|[2409.10870v1](http://arxiv.org/abs/2409.10870v1)|null|
|**2024-09-17**|**SIFToM: Robust Spoken Instruction Following through Theory of Mind**|Lance Ying et.al.|[2409.10849v1](http://arxiv.org/abs/2409.10849v1)|null|
|**2024-09-17**|**3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy**|Xuanmeng Sha et.al.|[2409.10848v1](http://arxiv.org/abs/2409.10848v1)|null|
|**2024-09-17**|**BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation**|S. Rohollah Hosseyni et.al.|[2409.10847v1](http://arxiv.org/abs/2409.10847v1)|null|
|**2024-09-17**|**PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing**|Phillip Long et.al.|[2409.10831v1](http://arxiv.org/abs/2409.10831v1)|null|
|**2024-09-17**|**ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic Radiology Reports**|Vishwanatha M. Rao et.al.|[2409.10829v1](http://arxiv.org/abs/2409.10829v1)|null|
|**2024-09-17**|**Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations**|Shahnewaz Karim Sakib et.al.|[2409.10825v1](http://arxiv.org/abs/2409.10825v1)|null|
|**2024-09-16**|**Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering**|Qingru Zhang et.al.|[2409.10790v1](http://arxiv.org/abs/2409.10790v1)|null|
|**2024-09-16**|**Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM and Attention-Based Approach**|Tracy Cai et.al.|[2409.10783v1](http://arxiv.org/abs/2409.10783v1)|null|
|**2024-09-16**|**Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?**|Kaleb Kassaw et.al.|[2409.10775v1](http://arxiv.org/abs/2409.10775v1)|null|
|**2024-09-16**|**Semantics Preserving Emoji Recommendation with Large Language Models**|Zhongyi Qiu et.al.|[2409.10760v1](http://arxiv.org/abs/2409.10760v1)|null|
|**2024-09-16**|**VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching**|Arastoo Zibaeirad et.al.|[2409.10756v1](http://arxiv.org/abs/2409.10756v1)|null|

#### Abstracts
##### **AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**
2409.11404v1 by Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam

Arabic, with its rich diversity of dialects, remains significantly
underrepresented in Large Language Models, particularly in dialectal
variations. We address this gap by introducing seven synthetic datasets in
dialects alongside Modern Standard Arabic (MSA), created using Machine
Translation (MT) combined with human post-editing. We present AraDiCE, a
benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on
dialect comprehension and generation, focusing specifically on low-resource
Arabic dialects. Additionally, we introduce the first-ever fine-grained
benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and
Levant regions, providing a novel dimension to LLM evaluation. Our findings
demonstrate that while Arabic-specific models like Jais and AceGPT outperform
multilingual models on dialectal tasks, significant challenges persist in
dialect identification, generation, and translation. This work contributes ~45K
post-edited samples, a cultural benchmark, and highlights the importance of
tailored training to improve LLM performance in capturing the nuances of
diverse Arabic dialects and cultural contexts. We will release the dialectal
translation models and benchmarks curated in this study.

ÊëòË¶ÅÔºöÈòøÊãâ‰ºØË™ûÊìÅÊúâË±êÂØåÂ§öÊ®£ÁöÑÊñπË®ÄÔºåÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠‰ªçÁÑ∂È°ØËëó‰∏çË∂≥ÔºåÁâπÂà•ÊòØÂú®ÊñπË®ÄËÆäÈ´î‰∏≠„ÄÇÊàëÂÄëÈÄöÈÅéÂú®ÊñπË®Ä‰∏≠ÂºïÂÖ•‰∏ÉÂÄãÂêàÊàêÊï∏ÊìöÈõÜ‰ª•ÂèäÁèæ‰ª£Ê®ôÊ∫ñÈòøÊãâ‰ºØË™û (MSA) ‰æÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÈÄô‰∫õÊï∏ÊìöÈõÜÊòØ‰ΩøÁî®Ê©üÂô®ÁøªË≠Ø (MT) ÁµêÂêà‰∫∫Â∑•ÂæåÁ∑®ËºØÂâµÂª∫ÁöÑ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü AraDiCEÔºåÈÄôÊòØÈòøÊãâ‰ºØÊñπË®ÄÂíåÊñáÂåñË©ï‰º∞ÁöÑÂü∫Ê∫ñ„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü LLM Â∞çÊñπË®ÄÁêÜËß£ÂíåÁîüÊàêÁöÑË°®ÁèæÔºåÁâπÂà•ÈóúÊ≥®Ë≥áÊ∫êËºÉÂ∞ëÁöÑÈòøÊãâ‰ºØÊñπË®Ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÊúâÂè≤‰ª•‰æÜÁ¨¨‰∏ÄÂÄãÁ¥∞Á≤íÂ∫¶Âü∫Ê∫ñÔºåÊó®Âú®Ë©ï‰º∞Êµ∑ÁÅ£„ÄÅÂüÉÂèäÂíåÈªéÂá°ÁâπÂú∞ÂçÄÁöÑÊñáÂåñÊÑèË≠òÔºåÁÇ∫ LLM Ë©ï‰º∞Êèê‰æõ‰∫ÜÊñ∞ÁöÑÁ∂≠Â∫¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ°ÂÉè Jais Âíå AceGPT ÈÄôÊ®£ÁöÑÈòøÊãâ‰ºØË™ûÁâπÂÆöÊ®°ÂûãÂú®ÊñπË®Ä‰ªªÂãô‰∏äÂÑ™ÊñºÂ§öË™ûË®ÄÊ®°ÂûãÔºå‰ΩÜÂú®ÊñπË®ÄË≠òÂà•„ÄÅÁîüÊàêÂíåÁøªË≠Ø‰∏≠‰ªçÁÑ∂Â≠òÂú®ÈáçÂ§ßÊåëÊà∞„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúË≤¢Áçª‰∫ÜÁ¥Ñ 45K ÂÄãÂæåÁ∑®ËºØÊ®£Êú¨„ÄÅ‰∏ÄÂÄãÊñáÂåñÂü∫Ê∫ñÔºå‰∏¶Âº∑Ë™ø‰∫ÜÂÆöÂà∂Ë®ìÁ∑¥Â∞çÊñºÊèêÈ´ò LLM Âú®ÊçïÊçâ‰∏çÂêåÈòøÊãâ‰ºØÊñπË®ÄÂíåÊñáÂåñËÉåÊôØÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•ÊñπÈù¢ÁöÑÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÂ∞áÁôºÂ∏ÉÊú¨Á†îÁ©∂‰∏≠Á≠ñÂäÉÁöÑÊñπË®ÄÁøªË≠ØÊ®°ÂûãÂíåÂü∫Ê∫ñ„ÄÇ

##### **NVLM: Open Frontier-Class Multimodal LLMs**
2409.11402v1 by Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping

We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we are releasing the model weights and will open-source the code for the
community: https://nvlm-project.github.io/.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊé®Âá∫ NVLM 1.0ÔºåÈÄôÊòØ‰∏ÄÂÄãÂâçÊ≤øÁ¥öÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÆ∂ÊóèÔºåÂú®Ë¶ñË¶∫Ë™ûË®Ä‰ªªÂãô‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûúÔºåËàáÈ†òÂÖàÁöÑÂ∞àÊúâÊ®°ÂûãÔºà‰æãÂ¶Ç GPT-4oÔºâÂíåÈñãÊîæË®™ÂïèÊ®°ÂûãÔºà‰æãÂ¶Ç Llama 3-V 405B Âíå InternVL 2ÔºâÁõ∏Â™≤Áæé„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåNVLM 1.0 Âú®Â§öÊ®°ÊÖãË®ìÁ∑¥ÂæåÔºåÂÖ∂Á¥îÊñáÂ≠óË°®ÁèæÂÑ™ÊñºÂÖ∂ LLM ‰∏ªÂππ„ÄÇÂú®Ê®°ÂûãË®≠Ë®àÊñπÈù¢ÔºåÊàëÂÄëÂ∞çÂÉÖËß£Á¢ºÂô®Â§öÊ®°ÊÖã LLMÔºà‰æãÂ¶Ç LLaVAÔºâÂíåÂü∫Êñº‰∫§ÂèâÊ≥®ÊÑèÂäõÁöÑÊ®°ÂûãÔºà‰æãÂ¶Ç FlamingoÔºâÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÊØîËºÉ„ÄÇÊ†πÊìöÂÖ©Á®ÆÊñπÊ≥ïÁöÑÂÑ™Áº∫ÈªûÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÊó¢ËÉΩÊèêÈ´òË®ìÁ∑¥ÊïàÁéáÔºåÂèàËÉΩÂ¢ûÂº∑Â§öÊ®°ÊÖãÊé®ÁêÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁÇ∫Âü∫ÊñºÂúñÂ°äÁöÑÂãïÊÖãÈ´òËß£ÊûêÂ∫¶ÂΩ±ÂÉèÂºïÂÖ•‰∫Ü 1-D Á£ÅÁ£öÊ®ôÁ±§Ë®≠Ë®àÔºåÈÄôÈ°ØËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÖãÊé®ÁêÜÂíå OCR Áõ∏Èóú‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÈóúÊñºË®ìÁ∑¥Ë≥áÊñôÔºåÊàëÂÄëÁ≤æÂøÉÁ≠ñÂäÉ‰∏¶Êèê‰æõ‰∫ÜÊúâÈóúÊàëÂÄëÂ§öÊ®°ÊÖãÈ†êË®ìÁ∑¥ÂíåÁõ£Áù£ÂæÆË™øË≥áÊñôÈõÜÁöÑË©≥Á¥∞Ë≥áË®ä„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåË≥áÊñôÈõÜÂìÅË≥™Âíå‰ªªÂãôÂ§öÊ®£ÊÄßÊØîË¶èÊ®°Êõ¥ÈáçË¶ÅÔºåÂç≥‰ΩøÂú®È†êË®ìÁ∑¥ÈöéÊÆµÔºå‰πüÈÅ©Áî®ÊñºÊâÄÊúâÊû∂Êßã„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁÇ∫ NVLM-1.0 Ê®°ÂûãÈñãÁôº‰∫ÜÁîüÁî¢Á¥öÂ§öÊ®°ÊÖãÔºåËÆìÂÆÉÂÄëËÉΩÂ§†Âú®Ë¶ñË¶∫Ë™ûË®Ä‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÔºåÂêåÊôÇËàáÂÖ∂ LLM ‰∏ªÂππÁõ∏ÊØîÔºåÁ∂≠ÊåÅÁîöËá≥ÊèêÂçáÁ¥îÊñáÂ≠óË°®Áèæ„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∏ÄÈªûÔºåÊàëÂÄëÂú®Â§öÊ®°ÊÖãË®ìÁ∑¥‰∏≠Ë£Ω‰Ωú‰∏¶Êï¥Âêà‰∫Ü‰∏ÄÂÄãÈ´òÂìÅË≥™ÁöÑÁ¥îÊñáÂ≠óË≥áÊñôÈõÜÔºå‰ª•ÂèäÂ§ßÈáèÁöÑÂ§öÊ®°ÊÖãÊï∏Â≠∏ÂíåÊé®ÁêÜË≥áÊñôÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜË∑®Ê®°ÊÖãÁöÑÊï∏Â≠∏ÂíåÁ∑®Á¢ºËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊé®ÂãïË©≤È†òÂüüÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞áÈáãÂá∫Ê®°ÂûãÊ¨äÈáçÔºå‰∏¶Â∞áÁÇ∫Á§æÁæ§ÈñãÊîæÂéüÂßãÁ¢ºÔºöhttps://nvlm-project.github.io/„ÄÇ</paragraph>

##### **LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**
2409.11393v1 by Amine B. Hassouna, Hana Chaari, Ines Belhaj

The integration of tools in LLM-based agents overcame the difficulties of
standalone LLMs and traditional agents' limited capabilities. However, the
conjunction of these technologies and the proposed enhancements in several
state-of-the-art works followed a non-unified software architecture resulting
in a lack of modularity. Indeed, they focused mainly on functionalities and
overlooked the definition of the component's boundaries within the agent. This
caused terminological and architectural ambiguities between researchers which
we addressed in this paper by proposing a unified framework that establishes a
clear foundation for LLM-based agents' development from both functional and
software architectural perspectives.
  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),
clearly distinguishes between the different components of an agent, setting
LLMs, and tools apart from a newly introduced element: the core-agent, playing
the role of the central coordinator of the agent which comprises five modules:
planning, memory, profile, action, and security, the latter often neglected in
previous works. Differences in the internal structure of core-agents led us to
classify them into a taxonomy of passive and active types. Based on this, we
proposed different multi-core agent architectures combining unique
characteristics of various individual agents.
  For evaluation purposes, we applied this framework to a selection of
state-of-the-art agents, thereby demonstrating its alignment with their
functionalities and clarifying the overlooked architectural aspects. Moreover,
we thoroughly assessed four of our proposed architectures by integrating
distinctive agents into hybrid active/passive core-agents' systems. This
analysis provided clear insights into potential improvements and highlighted
the challenges involved in the combination of specific agents.

ÊëòË¶ÅÔºö<paragraph>Êï¥Âêà LLM ÁÇ∫Âü∫Á§éÁöÑ‰ª£ÁêÜÁ®ãÂºè‰∏≠ÁöÑÂ∑•ÂÖ∑ÂÖãÊúç‰∫ÜÁç®Á´ã LLM ÂíåÂÇ≥Áµ±‰ª£ÁêÜÁ®ãÂºèÂäüËÉΩÊúâÈôêÁöÑÂõ∞Èõ£„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊäÄË°ìÁöÑÁµêÂêàÂíåÂπæÂÄãÊúÄÂÖàÈÄ≤‰ΩúÂìÅ‰∏≠ÊèêÂá∫ÁöÑÂ¢ûÂº∑ÂäüËÉΩÈÅµÂæ™ÈùûÁµ±‰∏ÄËªüÈ´îÊû∂ÊßãÔºåÂ∞éËá¥Áº∫‰πèÊ®°ÁµÑÂåñ„ÄÇ‰∫ãÂØ¶‰∏äÔºåÂÆÉÂÄë‰∏ªË¶ÅÈóúÊ≥®ÂäüËÉΩÔºåËÄåÂøΩÁï•‰∫Ü‰ª£ÁêÜÁ®ãÂºèÂÖßÂÖÉ‰ª∂ÈÇäÁïåÁöÑÂÆöÁæ©„ÄÇÈÄôÂ∞éËá¥Á†îÁ©∂‰∫∫Âì°‰πãÈñìË°ìË™ûÂíåÊû∂Êßã‰∏äÁöÑÊ≠ßÁæ©ÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁµ±‰∏ÄÊ°ÜÊû∂‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåË©≤Ê°ÜÊû∂ÁÇ∫ LLM ÁÇ∫Âü∫Á§éÁöÑ‰ª£ÁêÜÁ®ãÂºèÁöÑÈñãÁôºÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊ∏ÖÊô∞ÁöÑÂü∫Á§éÔºåÁÑ°Ë´ñÊòØÂæûÂäüËÉΩÈÇÑÊòØËªüÈ´îÊû∂ÊßãÁöÑËßíÂ∫¶‰æÜÁúã„ÄÇ
ÊàëÂÄëÁöÑÊ°ÜÊû∂ LLM-Agent-UMFÔºàLLM ÁÇ∫Âü∫Á§éÁöÑ‰ª£ÁêÜÁ®ãÂºèÁµ±‰∏ÄÂª∫Ê®°Ê°ÜÊû∂ÔºâÔºåÊ∏ÖÊ•öÂú∞ÂçÄÂàÜ‰∫Ü‰ª£ÁêÜÁ®ãÂºèÁöÑ‰∏çÂêåÂÖÉ‰ª∂ÔºåË®≠ÂÆö LLM ÂíåÂ∑•ÂÖ∑ÔºåÈô§‰∫ÜÊñ∞ÂºïÂÖ•ÁöÑÂÖÉÁ¥†ÔºöÊ†∏ÂøÉ‰ª£ÁêÜÁ®ãÂºèÔºåÊâÆÊºî‰ª£ÁêÜÁ®ãÂºè‰∏≠Â§ÆÂçîË™øËÄÖÁöÑËßíËâ≤ÔºåÂåÖÂê´‰∫îÂÄãÊ®°ÁµÑÔºöË¶èÂäÉ„ÄÅË®òÊÜ∂È´î„ÄÅË®≠ÂÆöÊ™î„ÄÅÂãï‰ΩúÂíåÂÆâÂÖ®ÊÄßÔºåÂæåËÄÖÂú®‰πãÂâçÁöÑ‰ΩúÂìÅ‰∏≠Á∂ìÂ∏∏Ë¢´ÂøΩÁï•„ÄÇÊ†∏ÂøÉ‰ª£ÁêÜÁ®ãÂºèÂÖßÈÉ®ÁµêÊßãÁöÑÂ∑ÆÁï∞Â∞éËá¥ÊàëÂÄëÂ∞áÂÆÉÂÄëÂàÜÈ°ûÁÇ∫Ë¢´ÂãïÂíå‰∏ªÂãïÈ°ûÂûã„ÄÇÂü∫ÊñºÊ≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏çÂêåÁöÑÂ§öÊ†∏ÂøÉ‰ª£ÁêÜÁ®ãÂºèÊû∂ÊßãÔºåÁµêÂêà‰∫ÜÂêÑÁ®ÆÂÄãÂà•‰ª£ÁêÜÁ®ãÂºèÁöÑÁç®ÁâπÁâπÂæµ„ÄÇ
ÁÇ∫‰∫ÜË©ï‰º∞ÁõÆÁöÑÔºåÊàëÂÄëÂ∞áÈÄôÂÄãÊ°ÜÊû∂ÊáâÁî®Êñº‰∏ÄÁ≥ªÂàóÊúÄÂÖàÈÄ≤ÁöÑ‰ª£ÁêÜÁ®ãÂºèÔºåÂæûËÄåË≠âÊòéÂÆÉËàáÂÆÉÂÄëÁöÑÂäüËÉΩ‰∏ÄËá¥Ôºå‰∏¶ÈáêÊ∏ÖË¢´ÂøΩÁï•ÁöÑÊû∂ÊßãÊñπÈù¢„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÂ∞áÁç®ÁâπÁöÑ‰ª£ÁêÜÁ®ãÂºèÊï¥ÂêàÂà∞Ê∑∑Âêà‰∏ªÂãï/Ë¢´ÂãïÊ†∏ÂøÉ‰ª£ÁêÜÁ®ãÂºèÁ≥ªÁµ±‰∏≠ÔºåÂæπÂ∫ïË©ï‰º∞‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÂõõÁ®ÆÊû∂Êßã„ÄÇÈÄôÂÄãÂàÜÊûêÊèê‰æõ‰∫ÜÂ∞çÊΩõÂú®ÊîπÈÄ≤ÁöÑÊ∏ÖÊô∞Ë¶ãËß£Ôºå‰∏¶Âº∑Ë™ø‰∫ÜÁµêÂêàÁâπÂÆö‰ª£ÁêÜÁ®ãÂºèÊâÄÊ∂âÂèäÁöÑÊåëÊà∞„ÄÇ</paragraph>

##### **Says Who? Effective Zero-Shot Annotation of Focalization**
2409.11390v1 by Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan

Focalization, the perspective through which narrative is presented, is
encoded via a wide range of lexico-grammatical features and is subject to
reader interpretation. Moreover, trained readers regularly disagree on
interpretations, suggesting that this problem may be computationally
intractable. In this paper, we provide experiments to test how well
contemporary Large Language Models (LLMs) perform when annotating literary
texts for focalization mode. Despite the challenging nature of the task, LLMs
show comparable performance to trained human annotators in our experiments. We
provide a case study working with the novels of Stephen King to demonstrate the
usefulness of this approach for computational literary studies, illustrating
how focalization can be studied at scale.

ÊëòË¶ÅÔºöÁÑ¶ÈªûÂåñÔºåÊïò‰∫ãÂëàÁèæÁöÑË¶ñËßíÔºåÊòØÈÄèÈÅéÂª£Ê≥õÁöÑË©ûÂΩôË™ûÊ≥ïÁâπÂæµÁ∑®Á¢ºÔºå‰∏¶ÂèóÂà∞ËÆÄËÄÖË©ÆÈáãÁöÑÂΩ±Èüø„ÄÇÊ≠§Â§ñÔºåÂèóÈÅéË®ìÁ∑¥ÁöÑËÆÄËÄÖÁ∂ìÂ∏∏Â∞çË©ÆÈáãÊúâÂàÜÊ≠ßÔºåÈÄôË°®ÊòéÈÄôÂÄãÂïèÈ°åÂú®Ë®àÁÆó‰∏äÂèØËÉΩÊòØÈõ£‰ª•Ëß£Ê±∫ÁöÑ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõÂØ¶È©ó‰æÜÊ∏¨Ë©¶Áï∂‰ª£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÁÇ∫ÊñáÂ≠∏ÊñáÊú¨Ê®ôË®ªÁÑ¶ÈªûÂåñÊ®°ÂºèÊôÇÁöÑË°®Áèæ„ÄÇÂÑòÁÆ°‰ªªÂãôÁöÑÊåëÊà∞ÊÄßÔºåLLM Âú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠È°ØÁ§∫Âá∫ËàáÂèóÈÅéË®ìÁ∑¥ÁöÑ‰∫∫È°ûÊ®ôË®ªÂì°Áõ∏Áï∂ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Ôºå‰ΩøÁî®Âè≤ËíÇËä¨ÈáëÁöÑÂ∞èË™™‰æÜÂ±ïÁ§∫ÈÄôÁ®ÆÊñπÊ≥ïÂ∞çË®àÁÆóÊñáÂ≠∏Á†îÁ©∂ÁöÑÊúâÁî®ÊÄßÔºåË™™ÊòéÁÑ¶ÈªûÂåñÂ¶Ç‰ΩïÂ§ßË¶èÊ®°Âú∞Ë¢´Á†îÁ©∂„ÄÇ

##### **Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**
2409.11378v1 by Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee

Finetuning large language models on instruction data is crucial for enhancing
pre-trained knowledge and improving instruction-following capabilities. As
instruction datasets proliferate, selecting optimal data for effective training
becomes increasingly important. This work addresses the question: How can we
determine the optimal subset of data for effective training? While existing
research often emphasizes local criteria like instance quality for subset
selection, we argue that a global approach focused on data diversity is more
critical. Our method employs k-means clustering to ensure the selected subset
effectively represents the full dataset. We propose an iterative refinement
method inspired by active learning techniques to resample instances from
clusters, reassessing each cluster's importance and sampling weight in every
training iteration. This approach reduces the effect of outliers and
automatically filters out clusters containing low-quality data. Through
extensive evaluation across natural language reasoning, general world
knowledge, code and math reasoning tasks, and by fine-tuning models from
various families, we observe consistent improvements, achieving a 7% increase
over random selection and a 3.8% improvement over state-of-the-art sampling
methods. Our work highlights the significance of diversity-first sampling when
finetuning LLMs to enhance performance across a broad array of evaluation
tasks. Our code is available at
https://github.com/for-ai/iterative-data-selection.

ÊëòË¶ÅÔºöÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊåá‰ª§Ë≥áÊñôÂ∞çÊñºÂ¢ûÂº∑È†êË®ìÁ∑¥Áü•Ë≠òÂíåÊîπÈÄ≤Êåá‰ª§ÈÅµÂæ™ËÉΩÂäõËá≥ÈóúÈáçË¶Å„ÄÇÈö®ËëóÊåá‰ª§Ë≥áÊñôÈõÜÁöÑÊøÄÂ¢ûÔºåÈÅ∏ÊìáÊúÄ‰Ω≥Ë≥áÊñô‰ª•ÈÄ≤Ë°åÊúâÊïàË®ìÁ∑¥ËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúËß£Ê±∫‰∫Ü‰ª•‰∏ãÂïèÈ°åÔºöÊàëÂÄëÂ¶Ç‰ΩïÁ¢∫ÂÆöÊúÄ‰Ω≥Ë≥áÊñôÂ≠êÈõÜ‰ª•ÈÄ≤Ë°åÊúâÊïàË®ìÁ∑¥ÔºüÈõñÁÑ∂ÁèæÊúâÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Âº∑Ë™øÂ≠êÈõÜÈÅ∏ÊìáÁöÑÂ±ÄÈÉ®Ê®ôÊ∫ñÔºå‰æãÂ¶Ç‰æãÈ†ÖÂìÅË≥™Ôºå‰ΩÜÊàëÂÄëË™çÁÇ∫Â∞àÊ≥®ÊñºË≥áÊñôÂ§öÊ®£ÊÄßÁöÑÊï¥È´îÊñπÊ≥ïÊõ¥ÁÇ∫ÈóúÈçµ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî® k ÂùáÂÄºËÅöÈ°ûÊ≥ïÔºå‰ª•Á¢∫‰øùÊâÄÈÅ∏Â≠êÈõÜÊúâÊïà‰ª£Ë°®Êï¥ÂÄãË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂèó‰∏ªÂãïÂ≠∏ÁøíÊäÄË°ìÂïüÁôºÁöÑËø≠‰ª£ÊîπÈÄ≤ÊñπÊ≥ïÔºåÂæûÁæ§ÈõÜ‰∏≠ÈáçÊñ∞ÊäΩÂèñ‰æãÈ†ÖÔºåÂú®ÊØèÊ¨°Ë®ìÁ∑¥Ëø≠‰ª£‰∏≠ÈáçÊñ∞Ë©ï‰º∞ÊØèÂÄãÁæ§ÈõÜÁöÑÈáçË¶ÅÊÄßÂèäÊäΩÊ®£Ê¨äÈáç„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈôç‰Ωé‰∫ÜÁï∞Â∏∏ÂÄºÁöÑÊïàÊûúÔºå‰∏¶Ëá™ÂãïÈÅéÊøæÊéâÂåÖÂê´‰ΩéÂìÅË≥™Ë≥áÊñôÁöÑÁæ§ÈõÜ„ÄÇÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊé®ÁêÜ„ÄÅ‰∏ÄËà¨‰∏ñÁïåÁü•Ë≠ò„ÄÅÁ®ãÂºèÁ¢ºÂíåÊï∏Â≠∏Êé®ÁêÜ‰ªªÂãôÁöÑÂª£Ê≥õË©ï‰º∞Ôºå‰ª•ÂèäÂæÆË™ø‰æÜËá™‰∏çÂêåÁ≥ªÂàóÁöÑÊ®°ÂûãÔºåÊàëÂÄëËßÄÂØüÂà∞‰∫Ü‰∏ÄËá¥ÁöÑÊîπÈÄ≤ÔºåÈö®Ê©üÈÅ∏ÊìáÂ¢ûÂä†‰∫Ü 7%ÔºåÊúÄÂÖàÈÄ≤ÁöÑÊäΩÊ®£ÊñπÊ≥ïÊîπÈÄ≤‰∫Ü 3.8%„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÂú®ÂæÆË™ø LLM ‰ª•Â¢ûÂº∑Âª£Ê≥õË©ï‰º∞‰ªªÂãôÁöÑÊïàËÉΩÊôÇÔºå‰ª•Â§öÊ®£ÊÄßÁÇ∫ÂÑ™ÂÖàÁöÑÊäΩÊ®£ÁöÑÈ°ØËëóÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/for-ai/iterative-data-selection ÂèñÂæó„ÄÇ

##### **Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**
2409.11375v1 by Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi

In the medical domain, acquiring large datasets poses significant challenges
due to privacy concerns. Nonetheless, the development of a robust deep-learning
model for retinal disease diagnosis necessitates a substantial dataset for
training. The capacity to generalize effectively on smaller datasets remains a
persistent challenge. The scarcity of data presents a significant barrier to
the practical implementation of scalable medical AI solutions. To address this
issue, we've combined a wide range of data sources to improve performance and
generalization to new data by giving it a deeper understanding of the data
representation from multi-modal datasets and developed a self-supervised
framework based on large language models (LLMs), SwinV2 to gain a deeper
understanding of multi-modal dataset representations, enhancing the model's
ability to extrapolate to new data for the detection of eye diseases using
optical coherence tomography (OCT) images. We adopt a two-phase training
methodology, self-supervised pre-training, and fine-tuning on a downstream
supervised classifier. An ablation study conducted across three datasets
employing various encoder backbones, without data fusion, with low data
availability setting, and without self-supervised pre-training scenarios,
highlights the robustness of our method. Our findings demonstrate consistent
performance across these diverse conditions, showcasing superior generalization
capabilities compared to the baseline model, ResNet-50.

ÊëòË¶ÅÔºöÂú®ÈÜ´ÁôÇÈ†òÂüüÔºåÁî±ÊñºÈö±ÁßÅÂïèÈ°åÔºåÁç≤ÂèñÂ§ßÂûãË≥áÊñôÈõÜÊúÉÈÄ†ÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂ∞çÊñºË¶ñÁ∂≤ËÜúÁñæÁóÖË®∫Êñ∑ÁöÑÂº∑ÂÅ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÈñãÁôºÈúÄË¶Å‰∏ÄÂÄãÈæêÂ§ßÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÂ∞çËºÉÂ∞èÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÊúâÊïàÊ¶ÇÊã¨ÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊåÅÁ∫åÁöÑÊåëÊà∞„ÄÇË≥áÊñôÁöÑÁ®ÄÁº∫ÊÄßÂ∞çÂèØÊì¥ÂÖÖÈÜ´ÁôÇ AI Ëß£Ê±∫ÊñπÊ°àÁöÑÂØ¶ÈöõÂØ¶ÊñΩÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÁµêÂêà‰∫ÜÂêÑÁ®ÆË≥áÊñô‰æÜÊ∫êÔºåÈÄöÈÅéËÆìÂÖ∂Êõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£Â§öÊ®°ÂºèË≥áÊñôÈõÜÁöÑË≥áÊñôË°®Á§∫Ôºå‰æÜÊîπÂñÑÊïàËÉΩÂíåÂ∞çÊñ∞Ë≥áÊñôÁöÑÊ¶ÇÊã¨ÊÄßÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËá™Áõ£Áù£Ê°ÜÊû∂ÔºåSwinV2Ôºå‰ª•Êõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£Â§öÊ®°ÂºèË≥áÊñôÈõÜË°®Á§∫ÔºåÂ¢ûÂº∑Ê®°ÂûãÊé®Êñ∑Êñ∞Ë≥áÊñôÁöÑËÉΩÂäõÔºå‰ª•‰ΩøÁî®ÂÖâÂ≠∏Áõ∏Âπ≤Êñ∑Â±§ÊéÉÊèè (OCT) ÂΩ±ÂÉèÂÅµÊ∏¨ÁúºÁñæ„ÄÇÊàëÂÄëÊé°Áî®ÂÖ©ÈöéÊÆµË®ìÁ∑¥ÊñπÊ≥ïÔºåËá™Áõ£Áù£È†êË®ìÁ∑¥ÂíåÂ∞ç‰∏ãÊ∏∏Áõ£Áù£ÂàÜÈ°ûÂô®ÈÄ≤Ë°åÂæÆË™ø„ÄÇÂú®‰∏âÂÄãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåÊé°Áî®ÂêÑÁ®ÆÁ∑®Á¢º‰∏ªÂππÔºåÊ≤íÊúâË≥áÊñôËûçÂêàÔºåÂú®Ë≥áÊñôÂèØÁî®ÊÄßË®≠ÂÆöËºÉ‰ΩéÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰ª•ÂèäÊ≤íÊúâËá™Áõ£Áù£È†êË®ìÁ∑¥Â†¥ÊôØÔºåÁ™ÅÂá∫‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË≠âÊòé‰∫ÜÂú®ÈÄô‰∫õ‰∏çÂêåÊ¢ù‰ª∂‰∏ãÁöÑ‰∏ÄËá¥ÊïàËÉΩÔºåËàáÂü∫Ê∫ñÊ®°Âûã ResNet-50 Áõ∏ÊØîÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇ

##### **CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**
2409.11365v1 by Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li

The deployment of multimodal large language models (MLLMs) has demonstrated
remarkable success in engaging in conversations involving visual inputs, thanks
to the superior power of large language models (LLMs). Those MLLMs are
typically built based on the LLMs, with an image encoder to process images into
the token embedding space of the LLMs. However, the integration of visual
modality has introduced a unique vulnerability: the MLLM becomes susceptible to
malicious visual inputs and prone to generating sensitive or harmful responses,
even though the LLM has been trained on textual dataset to align with human
value. In this paper, we first raise the question: ``Do the MLLMs possess
safety-awareness against malicious image inputs?". We find that after adding a
principle that specifies the safety requirement into the input of the MLLM, the
model's safety awareness becomes boosted. This phenomenon verifies the
existence of MLLM's safety-awareness against image inputs, it is only weakened
by the modality gap. We then introduce a simple yet effective technique termed
CoCA, which amplifies the safety-awareness of the MLLM by calibrating its
output distribution. Our proposed strategy helps the model reclaim its original
safety awareness without losing its original capabilities. We verify the
effectiveness of our approach on both multimodal safety and understanding
benchmarks.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (MLLM) ÁöÑÈÉ®ÁΩ≤Â∑≤Â±ïÁ§∫Âá∫Âú®Ê∂âÂèäËßÜËßâËæìÂÖ•ÁöÑÂØπËØù‰∏≠ËøõË°å‰∫§‰∫íÁöÑÊòæÁùÄÊàêÂäüÔºåËøôË¶ÅÂΩíÂäü‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂº∫Â§ßÂäüËÉΩ„ÄÇËøô‰∫õ MLLM ÈÄöÂ∏∏Âü∫‰∫é LLM ÊûÑÂª∫ÔºåÂπ∂‰ΩøÁî®ÂõæÂÉèÁºñÁ†ÅÂô®Â∞ÜÂõæÂÉèÂ§ÑÁêÜÂà∞ LLM ÁöÑÊ†áËÆ∞ÂµåÂÖ•Á©∫Èó¥‰∏≠„ÄÇÁÑ∂ËÄåÔºåËßÜËßâÊ®°ÊÄÅÁöÑÈõÜÊàêÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Áã¨ÁâπÁöÑÊºèÊ¥ûÔºöMLLM ÂèòÂæóÂÆπÊòìÂèóÂà∞ÊÅ∂ÊÑèËßÜËßâËæìÂÖ•ÁöÑÂΩ±ÂìçÔºåÂπ∂‰∏îÂÆπÊòì‰∫ßÁîüÊïèÊÑüÊàñÊúâÂÆ≥ÁöÑÂèçÂ∫îÔºåÂç≥‰Ωø LLM Â∑≤Âú®ÊñáÊú¨Êï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉ‰ª•Á¨¶Âêà‰∫∫Á±ª‰ª∑ÂÄºËßÇ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÊèêÂá∫‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºö‚ÄúMLLM ÊòØÂê¶ÂØπÊÅ∂ÊÑèÂõæÂÉèËæìÂÖ•ÂÖ∑ÊúâÂÆâÂÖ®ÊÑèËØÜÔºü‚Äù„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂú®Â∞ÜÊåáÂÆöÂÆâÂÖ®Ë¶ÅÊ±ÇÁöÑÂéüÂàôÊ∑ªÂä†Âà∞ MLLM ÁöÑËæìÂÖ•‰∏≠ÂêéÔºåÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÑèËØÜÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøôÁßçÁé∞Ë±°È™åËØÅ‰∫Ü MLLM ÂØπÂõæÂÉèËæìÂÖ•ÁöÑÂÆâÂÖ®ÊÑèËØÜÁöÑÂ≠òÂú®ÔºåÂÆÉÂè™ÊòØË¢´Ê®°ÊÄÅÂ∑ÆË∑ùÂâäÂº±‰∫Ü„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰ΩÜÊúâÊïàÁöÑÊäÄÊúØÔºåÁß∞‰∏∫ CoCAÔºåÂÆÉÈÄöËøáÊ†°ÂáÜÂÖ∂ËæìÂá∫ÂàÜÂ∏ÉÊù•ÊîæÂ§ß MLLM ÁöÑÂÆâÂÖ®ÊÑèËØÜ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÁ≠ñÁï•Â∏ÆÂä©Ê®°ÂûãÂú®‰∏ç‰∏ßÂ§±ÂÖ∂ÂéüÂßãËÉΩÂäõÁöÑÊÉÖÂÜµ‰∏ãÊÅ¢Â§çÂÖ∂ÂéüÂßãÂÆâÂÖ®ÊÑèËØÜ„ÄÇÊàë‰ª¨Âú®Â§öÊ®°ÊÄÅÂÆâÂÖ®ÂíåÁêÜËß£Âü∫ÂáÜ‰∏äÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**
2409.11363v1 by Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan

AI agents have the potential to aid users on a variety of consequential
tasks, including conducting scientific research. To spur the development of
useful agents, we need benchmarks that are challenging, but more crucially,
directly correspond to real-world tasks of interest. This paper introduces such
a benchmark, designed to measure the accuracy of AI agents in tackling a
crucial yet surprisingly challenging aspect of scientific research:
computational reproducibility. This task, fundamental to the scientific
process, involves reproducing the results of a study using the provided code
and data. We introduce CORE-Bench (Computational Reproducibility Agent
Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers
across three disciplines (computer science, social science, and medicine).
Tasks in CORE-Bench consist of three difficulty levels and include both
language-only and vision-language tasks. We provide an evaluation system to
measure the accuracy of agents in a fast and parallelizable way, saving days of
evaluation time for each run compared to a sequential implementation. We
evaluated two baseline agents: the general-purpose AutoGPT and a task-specific
agent called CORE-Agent. We tested both variants using two underlying language
models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on
the hardest task, showing the vast scope for improvement in automating routine
scientific tasks. Having agents that can reproduce existing work is a necessary
step towards building agents that can conduct novel research and could verify
and improve the performance of other research agents. We hope that CORE-Bench
can improve the state of reproducibility and spur the development of future
research agents.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß‰ª£ÁêÜÁ®ãÂºèÊúâÊΩõÂäõÂçîÂä©‰ΩøÁî®ËÄÖÂü∑Ë°åÂêÑÁ®ÆÂæåÁ∫å‰ªªÂãôÔºåÂåÖÊã¨ÈÄ≤Ë°åÁßëÂ≠∏Á†îÁ©∂„ÄÇÁÇ∫‰∫ÜÂà∫ÊøÄÊúâÁî®ÁöÑ‰ª£ÁêÜÁ®ãÂºèÈñãÁôºÔºåÊàëÂÄëÈúÄË¶ÅÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰ΩÜÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÁõ¥Êé•Â∞çÊáâÊñºÊÑüËààË∂£ÁöÑÁúüÂØ¶‰∏ñÁïå‰ªªÂãôÁöÑÂü∫Ê∫ñ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÈÄôÊ®£ÁöÑÂü∫Ê∫ñÔºåÊó®Âú®Ë°°Èáè‰∫∫Â∑•Êô∫ÊÖß‰ª£ÁêÜÁ®ãÂºèÂú®ÊáâÂ∞çÁßëÂ≠∏Á†îÁ©∂‰∏≠‰∏ÄÂÄãËá≥ÈóúÈáçË¶Å‰ΩÜ‰ª§‰∫∫È©öË®ùÂú∞ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÊñπÈù¢ÔºöË®àÁÆóÈáçÁèæÊÄß„ÄÇÊ≠§‰ªªÂãôÂ∞çÊñºÁßëÂ≠∏ÈÅéÁ®ãËá≥ÈóúÈáçË¶ÅÔºåÂåÖÊã¨‰ΩøÁî®Êèê‰æõÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈáçÁèæÁ†îÁ©∂ÁµêÊûú„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü CORE-BenchÔºàË®àÁÆóÈáçÁèæÊÄß‰ª£ÁêÜÁ®ãÂºèÂü∫Ê∫ñÔºâÔºå‰∏ÄÂÄãÂü∫Ê∫ñÔºåÂåÖÂê´ 270 ÂÄã‰ªªÂãôÔºåÂü∫Êñº‰∏âÂÄãÂ≠∏ÁßëÔºàÈõªËÖ¶ÁßëÂ≠∏„ÄÅÁ§æÊúÉÁßëÂ≠∏ÂíåÈÜ´Â≠∏ÔºâÁöÑ 90 ÁØáÁßëÂ≠∏Ë´ñÊñá„ÄÇCORE-Bench ‰∏≠ÁöÑ‰ªªÂãôÂåÖÂê´‰∏âÂÄãÈõ£Â∫¶Á≠âÁ¥öÔºåÂåÖÊã¨ÂÉÖË™ûË®ÄÂíåË¶ñË¶∫Ë™ûË®Ä‰ªªÂãô„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãË©ï‰º∞Á≥ªÁµ±Ôºå‰ª•Âø´ÈÄü‰∏îÂèØ‰∏¶Ë°åÁöÑÊñπÂºèË°°Èáè‰ª£ÁêÜÁ®ãÂºèÁöÑÊ∫ñÁ¢∫ÊÄßÔºåËàáÂæ™Â∫èÂØ¶‰ΩúÁõ∏ÊØîÔºåÊØèÊ¨°Âü∑Ë°åÂèØÁØÄÁúÅÊï∏Â§©ÁöÑË©ï‰º∞ÊôÇÈñì„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÂÖ©ÂÄãÂü∫Ê∫ñ‰ª£ÁêÜÁ®ãÂºèÔºöÈÄöÁî® AutoGPT ÂíåÁ®±ÁÇ∫ CORE-Agent ÁöÑÁâπÂÆö‰ªªÂãô‰ª£ÁêÜÁ®ãÂºè„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÂü∫Á§éË™ûË®ÄÊ®°ÂûãÊ∏¨Ë©¶‰∫ÜÈÄôÂÖ©Á®ÆËÆäÈ´îÔºöGPT-4o Âíå GPT-4o-mini„ÄÇÂú®ÊúÄÂõ∞Èõ£ÁöÑ‰ªªÂãô‰∏≠ÔºåÊúÄÂ•ΩÁöÑ‰ª£ÁêÜÁ®ãÂºèÈÅîÂà∞‰∫Ü 21% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÈ°ØÁ§∫‰∫ÜÂú®Ëá™ÂãïÂåñ‰æãË°åÁßëÂ≠∏‰ªªÂãô‰∏≠ÊîπÈÄ≤ÁöÑÂª£ÈóäÁØÑÂúç„ÄÇÊìÅÊúâËÉΩÂ§†ÈáçÁèæÁèæÊúâÂ∑•‰ΩúÁöÑ‰ª£ÁêÜÁ®ãÂºèÊòØÂª∫ÊßãËÉΩÂ§†ÈÄ≤Ë°åÊñ∞Á†îÁ©∂‰∏¶È©óË≠âÂíåÊîπÈÄ≤ÂÖ∂‰ªñÁ†îÁ©∂‰ª£ÁêÜÁ®ãÂºèÊïàËÉΩÁöÑ‰ª£ÁêÜÁ®ãÂºèÁöÑÂøÖË¶ÅÊ≠•È©ü„ÄÇÊàëÂÄëÂ∏åÊúõ CORE-Bench ËÉΩÂ§†ÊîπÂñÑÈáçÁèæÊÄßÁãÄÊÖãÔºå‰∏¶Âà∫ÊøÄÊú™‰æÜÁ†îÁ©∂‰ª£ÁêÜÁ®ãÂºèÁöÑÈñãÁôº„ÄÇ

##### **AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**
2409.11360v1 by Dhruv Agarwal, Mor Naaman, Aditya Vashistha

Large language models (LLMs) are being increasingly integrated into everyday
products and services, such as coding tools and writing assistants. As these
embedded AI applications are deployed globally, there is a growing concern that
the AI models underlying these applications prioritize Western values. This
paper investigates what happens when a Western-centric AI model provides
writing suggestions to users from a different cultural background. We conducted
a cross-cultural controlled experiment with 118 participants from India and the
United States who completed culturally grounded writing tasks with and without
AI suggestions. Our analysis reveals that AI provided greater efficiency gains
for Americans compared to Indians. Moreover, AI suggestions led Indian
participants to adopt Western writing styles, altering not just what is written
but also how it is written. These findings show that Western-centric AI models
homogenize writing toward Western norms, diminishing nuances that differentiate
cultural expression.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê≠£ÈÄêÊº∏Êï¥ÂêàÂà∞Êó•Â∏∏Áî¢ÂìÅÂíåÊúçÂãô‰∏≠Ôºå‰æãÂ¶ÇÁ∑®Á¢ºÂ∑•ÂÖ∑ÂíåÂØ´‰ΩúÂä©ÁêÜ„ÄÇÈö®ËëóÈÄô‰∫õÂµåÂÖ•Âºè AI ÊáâÁî®Á®ãÂºèÂú®ÂÖ®ÁêÉÈÉ®ÁΩ≤ÔºåÊÑà‰æÜÊÑàÂ§ö‰∫∫ÊìîÂøÉÈÄô‰∫õÊáâÁî®Á®ãÂºèÂ∫ïÂ±§ÁöÑ AI Ê®°ÂûãÊúÉÂÑ™ÂÖàËÄÉÊÖÆË•øÊñπÂÉπÂÄºËßÄ„ÄÇÊú¨ÊñáÊé¢Ë®é‰ª•Ë•øÊñπÁÇ∫‰∏≠ÂøÉÁöÑ AI Ê®°ÂûãÂêë‰∏çÂêåÊñáÂåñËÉåÊôØÁöÑ‰ΩøÁî®ËÄÖÊèê‰æõÂØ´‰ΩúÂª∫Ë≠∞ÊôÇÊúÉÁôºÁîü‰ªÄÈ∫ºÊÉÖÊ≥Å„ÄÇÊàëÂÄëÈáùÂ∞ç‰æÜËá™Âç∞Â∫¶ÂíåÁæéÂúãÁöÑ 118 ‰ΩçÂèÉËàáËÄÖÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖË∑®ÊñáÂåñÂ∞çÁÖßÂØ¶È©óÔºå‰ªñÂÄëÂú®ÊúâÂíåÊ≤íÊúâ AI Âª∫Ë≠∞ÁöÑÊÉÖÊ≥Å‰∏ãÂÆåÊàê‰∫Ü‰ª•ÊñáÂåñÁÇ∫Âü∫Á§éÁöÑÂØ´‰Ωú‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåËàáÂç∞Â∫¶‰∫∫Áõ∏ÊØîÔºåAI ÁÇ∫ÁæéÂúã‰∫∫Êèê‰æõ‰∫ÜÊõ¥È´òÁöÑÊïàÁéáÊèêÂçá„ÄÇÊ≠§Â§ñÔºåAI Âª∫Ë≠∞Â∞éËá¥Âç∞Â∫¶ÂèÉËàáËÄÖÊé°Áî®Ë•øÊñπÁöÑÂØ´‰ΩúÈ¢®Ê†ºÔºå‰∏çÂÉÖÊîπËÆä‰∫ÜÂØ´‰ΩúÂÖßÂÆπÔºå‰πüÊîπËÆä‰∫ÜÂØ´‰ΩúÊñπÂºè„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºå‰ª•Ë•øÊñπÁÇ∫‰∏≠ÂøÉÁöÑ AI Ê®°ÂûãÊúÉÂ∞áÂØ´‰ΩúÂêåË≥™ÂåñÁÇ∫Ë•øÊñπË¶èÁØÑÔºåÊ∏õÂ∞ëÂçÄÂàÜÊñáÂåñË°®ÈÅîÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇ

##### **RenderWorld: World Model with Self-Supervised 3D Label**
2409.11356v1 by Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma

End-to-end autonomous driving with vision-only is not only more
cost-effective compared to LiDAR-vision fusion but also more reliable than
traditional methods. To achieve a economical and robust purely visual
autonomous driving system, we propose RenderWorld, a vision-only end-to-end
autonomous driving framework, which generates 3D occupancy labels using a
self-supervised gaussian-based Img2Occ Module, then encodes the labels by
AM-VAE, and uses world model for forecasting and planning. RenderWorld employs
Gaussian Splatting to represent 3D scenes and render 2D images greatly improves
segmentation accuracy and reduces GPU memory consumption compared with
NeRF-based methods. By applying AM-VAE to encode air and non-air separately,
RenderWorld achieves more fine-grained scene element representation, leading to
state-of-the-art performance in both 4D occupancy forecasting and motion
planning from autoregressive world model.

ÊëòË¶ÅÔºöÂÉÖ‰ΩøÁî®Ë¶ñË¶∫ÁöÑÁ´ØÂ∞çÁ´ØËá™ÂãïÈßïÈßõ‰∏çÂÉÖÊØî LiDAR Ë¶ñË¶∫ËûçÂêàÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÔºå‰πüÊØîÂÇ≥Áµ±ÊñπÊ≥ïÊõ¥ÂèØÈù†„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÁ∂ìÊøü‰∏îÂº∑Â§ßÁöÑÁ¥îË¶ñË¶∫Ëá™ÂãïÈßïÈßõÁ≥ªÁµ±ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü RenderWorldÔºå‰∏ÄÂÄãÂÉÖ‰ΩøÁî®Ë¶ñË¶∫ÁöÑÁ´ØÂ∞çÁ´ØËá™ÂãïÈßïÈßõÊ°ÜÊû∂ÔºåÂÆÉ‰ΩøÁî®Ëá™Áõ£Áù£ÁöÑÂü∫ÊñºÈ´òÊñØÁöÑ Img2Occ Ê®°ÁµÑÁîüÊàê 3D ‰ΩîÁî®Ê®ôÁ±§ÔºåÁÑ∂ÂæåÈÄèÈÅé AM-VAE Á∑®Á¢ºÊ®ôÁ±§Ôºå‰∏¶‰ΩøÁî®‰∏ñÁïåÊ®°ÂûãÈÄ≤Ë°åÈ†êÊ∏¨ÂíåË¶èÂäÉ„ÄÇRenderWorld ‰ΩøÁî®È´òÊñØÂô¥Áπ™‰æÜË°®Á§∫ 3D Â†¥ÊôØÔºå‰∏¶Ê∏≤Êüì 2D ÂΩ±ÂÉèÔºåËàáÂü∫Êñº NeRF ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂ§ßÂπÖÊèêÂçáÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶‰∏¶Ê∏õÂ∞ë GPU Ë®òÊÜ∂È´îÊ∂àËÄó„ÄÇÈÄèÈÅéÂ∞á AM-VAE ÊáâÁî®ÊñºÂàÜÂà•Á∑®Á¢ºÁ©∫Ê∞£ÂíåÈùûÁ©∫Ê∞£ÔºåRenderWorld ÈÅîÂà∞‰∫ÜÊõ¥Á¥∞Á∑ªÁöÑÂ†¥ÊôØÂÖÉÁ¥†Ë°®Á§∫ÔºåÂú®Ëá™Ëø¥Ê≠∏‰∏ñÁïåÊ®°Âûã‰∏≠ÂØ¶Áèæ‰∫Ü 4D ‰ΩîÁî®È†êÊ∏¨ÂíåÂãï‰ΩúË¶èÂäÉÁöÑÊúÄÊñ∞ÊïàËÉΩ„ÄÇ

##### **THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**
2409.11353v1 by Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven

Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.

ÊëòË¶ÅÔºöÂπªË¶∫ÔºåÂç≥Áî¢Áîü‰∫ãÂØ¶‰∏ä‰∏çÊ≠£Á¢∫ÁöÑÂÖßÂÆπÔºåÊòØÂ§ßË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Êó•ÁõäÂö¥Â≥ªÁöÑÊåëÊà∞„ÄÇÁèæÊúâÁöÑÂÅµÊ∏¨ÂíåÁ∑©Ëß£ÊñπÊ≥ïÂæÄÂæÄÊòØÂ≠§Á´ãÁöÑÔºå‰∏î‰∏çË∂≥‰ª•ÊªøË∂≥ÁâπÂÆöÈ†òÂüüÁöÑÈúÄÊ±ÇÔºåÁº∫‰πèÊ®ôÊ∫ñÂåñÁöÑÁÆ°ÈÅì„ÄÇÊú¨Êñá‰ªãÁ¥π THaMESÔºàÂπªË¶∫Á∑©Ëß£ÂíåË©ï‰º∞Â∑•ÂÖ∑ÔºâÔºå‰∏ÄÂÄãÊï¥ÂêàÊ°ÜÊû∂ÂíåÂáΩÂºèÂ∫´ÔºåÁî®ÊñºËß£Ê±∫Ê≠§Â∑ÆË∑ù„ÄÇTHaMES Êèê‰æõ‰∫Ü‰∏ÄÂÄãÁ´ØÂà∞Á´ØÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºË©ï‰º∞ÂíåÁ∑©Ëß£ LLM ‰∏≠ÁöÑÂπªË¶∫ÔºåÂÖ∑ÂÇôËá™ÂãïÂåñÊ∏¨Ë©¶ÈõÜÁî¢Áîü„ÄÅÂ§öÊñπÈù¢ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÂíåÂèØÈÅ©ÊáâÁöÑÁ∑©Ëß£Á≠ñÁï•„ÄÇÂÆÉËá™ÂãïÂåñ‰∫ÜÂæû‰ªª‰ΩïË™ûÊñôÂ∫´Âª∫Á´ãÊ∏¨Ë©¶ÈõÜÁöÑÈÅéÁ®ãÔºåÈÄèÈÅéÊâπÊ¨°ËôïÁêÜ„ÄÅÂä†Ê¨äÊäΩÊ®£ÂíåÂèç‰∫ãÂØ¶È©óË≠âÁ≠âÊäÄË°ìÔºåÁ¢∫‰øùË≥áÊñôÁöÑÈ´òÂìÅË≥™„ÄÅÂ§öÊ®£ÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇTHaMES Ë©ï‰º∞Ê®°ÂûãÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÂÅµÊ∏¨ÂíåÊ∏õÂ∞ëÂπªË¶∫ÁöÑËÉΩÂäõÔºåÂåÖÊã¨ÊñáÂ≠óÁî¢ÁîüÂíå‰∫åÂÖÉÂàÜÈ°ûÔºåÊáâÁî®ÊúÄ‰Ω≥ÁöÑÁ∑©Ëß£Á≠ñÁï•Ôºå‰æãÂ¶ÇÊÉÖÂ¢ÉÂ≠∏Áøí (ICL)„ÄÅÊ™¢Á¥¢Â¢ûÂº∑Áî¢Áîü (RAG) ÂíåÂèÉÊï∏ÊúâÊïàÂæÆË™ø (PEFT)„ÄÇ‰ΩøÁî®Â≠∏Ë°ìË´ñÊñá„ÄÅÊîøÊ≤ªÊñ∞ËÅûÂíåÁ∂≠Âü∫ÁôæÁßëÁü•Ë≠òÂ∫´Ë©ï‰º∞ÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºåÁôºÁèæÂÉè GPT-4o Á≠âÂïÜÊ•≠Ê®°ÂûãÊØî ICL Âæû RAG ‰∏≠ÂèóÁõäÊõ¥Â§öÔºåËÄåÂÉè Llama-3.1-8B-Instruct Âíå Mistral-Nemo Á≠âÈñãÊîæÊ¨äÈáçÊ®°ÂûãÂæû ICL ‰∏≠ÂèóÁõäÊõ¥Â§ö„ÄÇÊ≠§Â§ñÔºåPEFT Âú®ÂÖ©ÂÄãË©ï‰º∞‰ªªÂãô‰∏≠ÈÉΩÈ°ØËëóÊèêÂçá‰∫Ü Llama-3.1-8B-Instruct ÁöÑÊïàËÉΩ„ÄÇ

##### **Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**
2409.11350v1 by Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng

Machine-learning (ML) models in flow cytometry have the potential to reduce
error rates, increase reproducibility, and boost the efficiency of clinical
labs. While numerous ML models for flow cytometry data have been proposed, few
studies have described the clinical deployment of such models. Realizing the
potential gains of ML models in clinical labs requires not only an accurate
model, but infrastructure for automated inference, error detection, analytics
and monitoring, and structured data extraction. Here, we describe an ML model
for detection of Acute Myeloid Leukemia (AML), along with the infrastructure
supporting clinical implementation. Our infrastructure leverages the resilience
and scalability of the cloud for model inference, a Kubernetes-based workflow
system that provides model reproducibility and resource management, and a
system for extracting structured diagnoses from full-text reports. We also
describe our model monitoring and visualization platform, an essential element
for ensuring continued model accuracy. Finally, we present a post-deployment
analysis of impacts on turn-around time and compare production accuracy to the
original validation statistics.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÂú®ÊµÅÂºèÁ¥∞ËÉûË°ì‰∏≠ÂÖ∑ÊúâÈôç‰ΩéÈåØË™§Áéá„ÄÅÊèêÈ´òÂèØÈáçÁèæÊÄßÂíåÊèêÂçáËá®Â∫äÂØ¶È©óÂÆ§ÊïàÁéáÁöÑÊΩõÂäõ„ÄÇÈõñÁÑ∂Â∑≤Á∂ìÊèêÂá∫Ë®±Â§öÁî®ÊñºÊµÅÂºèÁ¥∞ËÉûË°ìÊï∏ÊìöÁöÑ ML Ê®°ÂûãÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂ÊèèËø∞Ê≠§È°ûÊ®°ÂûãÁöÑËá®Â∫äÈÉ®ÁΩ≤„ÄÇË¶ÅÂØ¶Áèæ ML Ê®°ÂûãÂú®Ëá®Â∫äÂØ¶È©óÂÆ§‰∏≠ÁöÑÊΩõÂú®Êî∂ÁõäÔºå‰∏çÂÉÖÈúÄË¶ÅÊ∫ñÁ¢∫ÁöÑÊ®°ÂûãÔºåÈÇÑÈúÄË¶ÅÁî®ÊñºËá™ÂãïÊé®ÁêÜ„ÄÅÈåØË™§Ê™¢Ê∏¨„ÄÅÂàÜÊûêÂíåÁõ£Êéß‰ª•ÂèäÁµêÊßãÂåñÊï∏ÊìöÊèêÂèñÁöÑÂü∫Á§éË®≠ÊñΩ„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÊèèËø∞‰∫Ü‰∏ÄÂÄãÁî®ÊñºÊ™¢Ê∏¨ÊÄ•ÊÄßÈ´ìÊÄßÁôΩË°ÄÁóÖ (AML) ÁöÑ ML Ê®°ÂûãÔºå‰ª•ÂèäÊîØÊåÅËá®Â∫äÂØ¶ÊñΩÁöÑÂü∫Á§éË®≠ÊñΩ„ÄÇÊàëÂÄëÁöÑÂü∫Á§éË®≠ÊñΩÂà©Áî®Èõ≤Á´ØÁöÑÂæ©ÂéüÂäõÂíåÂèØÊì¥ÂÖÖÊÄßÈÄ≤Ë°åÊ®°ÂûãÊé®ÁêÜÔºå‰∏ÄÂÄãÂü∫Êñº Kubernetes ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÁ≥ªÁµ±Êèê‰æõÊ®°ÂûãÂèØÈáçÁèæÊÄßÂíåË≥áÊ∫êÁÆ°ÁêÜÔºå‰ª•Âèä‰∏ÄÂÄãÂæûÂÖ®ÊñáÂ†±Âëä‰∏≠ÊèêÂèñÁµêÊßãÂåñË®∫Êñ∑ÁöÑÁ≥ªÁµ±„ÄÇÊàëÂÄëÈÇÑÊèèËø∞‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÁõ£ÊéßÂíåË¶ñË¶∫ÂåñÂπ≥Âè∞ÔºåÈÄôÊòØÁ¢∫‰øùÊåÅÁ∫åÊ®°ÂûãÊ∫ñÁ¢∫ÊÄßÁöÑÂü∫Êú¨Ë¶ÅÁ¥†„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ∞çÂë®ËΩâÊôÇÈñìÂΩ±ÈüøÁöÑÈÉ®ÁΩ≤ÂæåÂàÜÊûêÔºå‰∏¶Â∞áÁîüÁî¢Ê∫ñÁ¢∫Â∫¶ËàáÂéüÂßãÈ©óË≠âÁµ±Ë®àÊï∏ÊìöÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **OmniGen: Unified Image Generation**
2409.11340v1 by Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu

In this work, we introduce OmniGen, a new diffusion model for unified image
generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen
no longer requires additional modules such as ControlNet or IP-Adapter to
process diverse control conditions. OmniGenis characterized by the following
features: 1) Unification: OmniGen not only demonstrates text-to-image
generation capabilities but also inherently supports other downstream tasks,
such as image editing, subject-driven generation, and visual-conditional
generation. Additionally, OmniGen can handle classical computer vision tasks by
transforming them into image generation tasks, such as edge detection and human
pose recognition. 2) Simplicity: The architecture of OmniGen is highly
simplified, eliminating the need for additional text encoders. Moreover, it is
more user-friendly compared to existing diffusion models, enabling complex
tasks to be accomplished through instructions without the need for extra
preprocessing steps (e.g., human pose estimation), thereby significantly
simplifying the workflow of image generation. 3) Knowledge Transfer: Through
learning in a unified format, OmniGen effectively transfers knowledge across
different tasks, manages unseen tasks and domains, and exhibits novel
capabilities. We also explore the model's reasoning capabilities and potential
applications of chain-of-thought mechanism. This work represents the first
attempt at a general-purpose image generation model, and there remain several
unresolved issues. We will open-source the related resources at
https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π OmniGenÔºå‰∏ÄÁ®ÆÁî®ÊñºÁµ±‰∏ÄÂΩ±ÂÉèÁîüÊàêÁöÑÂÖ®Êñ∞Êì¥Êï£Ê®°Âûã„ÄÇËàáÊµÅË°åÁöÑÊì¥Êï£Ê®°ÂûãÔºà‰æãÂ¶Ç Stable DiffusionÔºâ‰∏çÂêåÔºåOmniGen ‰∏çÂÜçÈúÄË¶Å ControlNet Êàñ IP-Adapter Á≠âÈ°çÂ§ñÊ®°ÁµÑ‰æÜËôïÁêÜ‰∏çÂêåÁöÑÊéßÂà∂Ê¢ù‰ª∂„ÄÇOmniGen ÁöÑÁâπÂæµÂ¶Ç‰∏ãÔºö1) Áµ±‰∏ÄÊÄßÔºöOmniGen ‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜÊñáÂ≠óËΩâÂΩ±ÂÉèÁöÑÁîüÊàêËÉΩÂäõÔºåËÄå‰∏îÊú¨Ë≥™‰∏ä‰πüÊîØÊè¥ÂÖ∂‰ªñ‰∏ãÊ∏∏‰ªªÂãôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÁ∑®ËºØ„ÄÅ‰∏ªÈ´îÈ©ÖÂãïÁîüÊàêÂíåË¶ñË¶∫Ê¢ù‰ª∂ÁîüÊàê„ÄÇÊ≠§Â§ñÔºåOmniGen ÂèØ‰ª•ÈÄèÈÅéÂ∞áÂÆÉÂÄëËΩâÊèõÁÇ∫ÂΩ±ÂÉèÁîüÊàê‰ªªÂãô‰æÜËôïÁêÜÁ∂ìÂÖ∏ÁöÑÈõªËÖ¶Ë¶ñË¶∫‰ªªÂãôÔºå‰æãÂ¶ÇÈÇäÁ∑£ÂÅµÊ∏¨Âíå‰∫∫È°ûÂßøÂã¢Ëæ®Ë≠ò„ÄÇ2) Á∞°ÊΩîÊÄßÔºöOmniGen ÁöÑÊû∂ÊßãÁ∂ìÈÅéÈ´òÂ∫¶Á∞°ÂåñÔºåÁÑ°ÈúÄÈ°çÂ§ñÁöÑÊñáÂ≠óÁ∑®Á¢ºÂô®„ÄÇÊ≠§Â§ñÔºåËàáÁèæÊúâÁöÑÊì¥Êï£Ê®°ÂûãÁõ∏ÊØîÔºåÂÆÉÊõ¥‰ΩøÁî®ËÄÖÂèãÂñÑÔºåËÉΩÂ§†ÈÄèÈÅéÊåá‰ª§ÂÆåÊàêË§áÈõúÁöÑ‰ªªÂãôÔºåÁÑ°ÈúÄÈ°çÂ§ñÁöÑÈ†êËôïÁêÜÊ≠•È©üÔºà‰æãÂ¶Ç‰∫∫È´îÂßøÂã¢‰º∞Ë®àÔºâÔºåÂæûËÄåÂ§ßÂπÖÁ∞°ÂåñÂΩ±ÂÉèÁîüÊàêÁöÑÊµÅÁ®ã„ÄÇ3) Áü•Ë≠òËΩâÁßªÔºöÈÄèÈÅé‰ª•Áµ±‰∏ÄÁöÑÊ†ºÂºèÂ≠∏ÁøíÔºåOmniGen ÊúâÊïàÂú∞Â∞áÁü•Ë≠òËΩâÁßªÂà∞‰∏çÂêåÁöÑ‰ªªÂãô‰∏≠ÔºåÁÆ°ÁêÜÊú™Ë¶ãÁöÑ‰ªªÂãôÂíåÈ†òÂüüÔºå‰∏¶Â±ïÁèæÊñ∞Á©éÁöÑËÉΩÂäõ„ÄÇÊàëÂÄë‰πüÊé¢Ë®é‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊÄùËÄÉÈèàÊ©üÂà∂ÁöÑÊΩõÂú®ÊáâÁî®„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰ª£Ë°®‰∫ÜÈÄöÁî®ÂΩ±ÂÉèÁîüÊàêÊ®°ÂûãÁöÑÈ¶ñÊ¨°ÂòóË©¶Ôºå‰ªçÊúâË®±Â§öÊú™Ëß£Ê±∫ÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ∞áÂú® https://github.com/VectorSpaceLab/OmniGen ÈñãÊîæÁõ∏ÈóúË≥áÊ∫êÔºå‰ª•‰øÉÈÄ≤Ê≠§È†òÂüüÁöÑÈÄ≤Â±ï„ÄÇ</paragraph>

##### **SOAP: Improving and Stabilizing Shampoo using Adam**
2409.11321v1 by Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade

There is growing evidence of the effectiveness of Shampoo, a higher-order
preconditioning method, over Adam in deep learning optimization tasks. However,
Shampoo's drawbacks include additional hyperparameters and computational
overhead when compared to Adam, which only updates running averages of first-
and second-moment quantities. This work establishes a formal connection between
Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient
approximation of Adam -- showing that Shampoo is equivalent to running
Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to
the design of a simpler and computationally efficient algorithm:
$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the
$\textbf{P}$reconditioner's eigenbasis (SOAP).
  With regards to improving Shampoo's computational efficiency, the most
straightforward approach would be to simply compute Shampoo's
eigendecomposition less frequently. Unfortunately, as our empirical results
show, this leads to performance degradation that worsens with this frequency.
SOAP mitigates this degradation by continually updating the running average of
the second moment, just as Adam does, but in the current (slowly changing)
coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a
rotated space, it introduces only one additional hyperparameter (the
preconditioning frequency) compared to Adam. We empirically evaluate SOAP on
language model pre-training with 360m and 660m sized models. In the large batch
regime, SOAP reduces the number of iterations by over 40% and wall clock time
by over 35% compared to AdamW, with approximately 20% improvements in both
metrics compared to Shampoo. An implementation of SOAP is available at
https://github.com/nikhilvyas/SOAP.

ÊëòË¶ÅÔºöË∂ä‰æÜË∂äÂ§öÁöÑË≠âÊìöÈ°ØÁ§∫ÔºåShampoo ÊòØ‰∏ÄÁ®ÆÈ´òÈöéÈ†êËôïÁêÜÊñπÊ≥ïÔºåÂú®Ê∑±Â∫¶Â≠∏ÁøíÊúÄ‰Ω≥Âåñ‰ªªÂãô‰∏≠ÂÑ™Êñº Adam„ÄÇÁÑ∂ËÄåÔºåËàáÂÉÖÊõ¥Êñ∞‰∏ÄÈöéÂíå‰∫åÈöéÁü©Âπ≥ÂùáÂÄºÁöÑ Adam Áõ∏ÊØîÔºåShampoo ÁöÑÁº∫ÈªûÂåÖÊã¨È°çÂ§ñÁöÑË∂ÖÂèÉÊï∏ÂíåÈÅãÁÆóË≤†Êìî„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂú® ShampooÔºà‰ª• 1/2 Ê¨°ÊñπÂØ¶‰ΩúÔºâÂíå AdafactorÔºà‰∏ÄÁ®ÆË®òÊÜ∂È´îÊïàÁéá‰Ω≥ÁöÑ Adam Ëøë‰ººÂÄºÔºâ‰πãÈñìÂª∫Á´ãÊ≠£ÂºèÁöÑÈóúËÅØÔºåÈ°ØÁ§∫ Shampoo Á≠âÊñºÂú® Shampoo È†êËôïÁêÜÂô®ÁöÑÁâπÂæµÂü∫Â∫ï‰∏≠Âü∑Ë°å Adafactor„ÄÇÈÄôÂÄãË¶ãËß£ÂºïÂ∞éÊàëÂÄëË®≠Ë®à‰∏ÄÁ®ÆÊõ¥Á∞°ÂñÆ‰∏îÈÅãÁÆóÊïàÁéáÊõ¥È´òÁöÑÊºîÁÆóÊ≥ïÔºö$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the $\textbf{P}$reconditioner's eigenbasis (SOAP)„ÄÇ
ÈóúÊñºÊîπÂñÑ Shampoo ÁöÑÈÅãÁÆóÊïàÁéáÔºåÊúÄÁõ¥Êé•ÁöÑÊñπÊ≥ïÂ∞±ÊòØÊ∏õÂ∞ëË®àÁÆó Shampoo ÁâπÂæµÂàÜËß£ÁöÑÈ†ªÁéá„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÊ≠£Â¶ÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúÈ°ØÁ§∫ÔºåÈÄôÊúÉÂ∞éËá¥ÊïàËÉΩÈôç‰ΩéÔºåËÄå‰∏îÈ†ªÁéáË∂äÈ´òÔºåÊïàËÉΩÂ∞±ÊúÉË∂äÂ∑Æ„ÄÇSOAP ÈÄèÈÅéÊåÅÁ∫åÊõ¥Êñ∞‰∫åÈöéÁü©ÁöÑÂü∑Ë°åÂπ≥ÂùáÂÄº‰æÜÊ∏õËºïÈÄôÁ®ÆÊïàËÉΩÈôç‰ΩéÔºåÂ∞±ÂÉè Adam ÊâÄÂÅöÁöÑ‰∏ÄÊ®£Ôºå‰ΩÜ‰ΩøÁî®ÁöÑÊòØÁõÆÂâçÁöÑÔºàÁ∑©ÊÖ¢ËÆäÂåñÁöÑÔºâÂ∫ßÊ®ôÂü∫Â∫ï„ÄÇÊ≠§Â§ñÔºåÁî±Êñº SOAP Á≠âÊñºÂú®ÊóãËΩâÁ©∫Èñì‰∏≠Âü∑Ë°å AdamÔºåÂõ†Ê≠§Ëàá Adam Áõ∏ÊØîÔºåÂÆÉÂè™ÂºïÂÖ•‰∏ÄÂÄãÈ°çÂ§ñÁöÑË∂ÖÂèÉÊï∏ÔºàÈ†êËôïÁêÜÈ†ªÁéáÔºâ„ÄÇÊàëÂÄëÂØ¶Ë≠âË©ï‰º∞ SOAP Âú®Ë™ûË®ÄÊ®°ÂûãÈ†êË®ìÁ∑¥‰∏≠Ôºå‰ΩøÁî® 360m Âíå 660m Â§ßÂ∞èÁöÑÊ®°Âûã„ÄÇÂú®Â§ßÊâπÊ¨°Ê®°Âºè‰∏≠ÔºåËàá AdamW Áõ∏ÊØîÔºåSOAP Â∞áËø≠‰ª£Ê¨°Êï∏Ê∏õÂ∞ëË∂ÖÈÅé 40%ÔºåÂ∞áÂØ¶ÈöõÂü∑Ë°åÊôÇÈñìÊ∏õÂ∞ëË∂ÖÈÅé 35%ÔºåËàá Shampoo Áõ∏ÊØîÔºåÂÖ©È†ÖÊåáÊ®ôÁöÜÊîπÂñÑÁ¥Ñ 20%„ÄÇSOAP ÁöÑÂØ¶‰ΩúÂèØÊñº https://github.com/nikhilvyas/SOAP ÂèñÂæó„ÄÇ

##### **MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping**
2409.11316v1 by Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh

Few-shot Semantic Segmentation addresses the challenge of segmenting objects
in query images with only a handful of annotated examples. However, many
previous state-of-the-art methods either have to discard intricate local
semantic features or suffer from high computational complexity. To address
these challenges, we propose a new Few-shot Semantic Segmentation framework
based on the transformer architecture. Our approach introduces the spatial
transformer decoder and the contextual mask generation module to improve the
relational understanding between support and query images. Moreover, we
introduce a multi-scale decoder to refine the segmentation mask by
incorporating features from different resolutions in a hierarchical manner.
Additionally, our approach integrates global features from intermediate encoder
stages to improve contextual understanding, while maintaining a lightweight
structure to reduce complexity. This balance between performance and efficiency
enables our method to achieve state-of-the-art results on benchmark datasets
such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.
Notably, our model with only 1.5 million parameters demonstrates competitive
performance while overcoming limitations of existing methodologies.
https://github.com/amirrezafateh/MSDNet

ÊëòË¶ÅÔºöÂ∞èÊ†∑Êú¨ËØ≠‰πâÂàÜÂâ≤Ëß£ÂÜ≥‰∫Ü‰ªÖÁî®Â∞ëÈáèÊ†áÊ≥®Á§∫‰æãÂàÜÂâ≤Êü•ËØ¢ÂõæÂÉè‰∏≠ÂØπË±°ÁöÑÊåëÊàò„ÄÇÁÑ∂ËÄåÔºåËÆ∏Â§ö‰ª•ÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ïË¶Å‰πàÂøÖÈ°ª‰∏¢ÂºÉÂ§çÊùÇÁöÑÂ±ÄÈÉ®ËØ≠‰πâÁâπÂæÅÔºåË¶Å‰πàÈÅ≠ÂèóÈ´òËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é transformer Êû∂ÊûÑÁöÑÊñ∞Â∞èÊ†∑Êú¨ËØ≠‰πâÂàÜÂâ≤Ê°ÜÊû∂„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂºïÂÖ•‰∫ÜÁ©∫Èó¥ transformer Ëß£Á†ÅÂô®Âíå‰∏ä‰∏ãÊñáÊé©Á†ÅÁîüÊàêÊ®°ÂùóÔºå‰ª•ÊîπÂñÑ support Âíå query ÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÂ∞∫Â∫¶Ëß£Á†ÅÂô®ÔºåÈÄöËøáÂàÜÂ±ÇÊñπÂºèÁªìÂêà‰∏çÂêåÂàÜËæ®ÁéáÁöÑÁâπÂæÅÊù•ÁªÜÂåñÂàÜÂâ≤Êé©Á†Å„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÈõÜÊàê‰∫ÜÊù•Ëá™‰∏≠Èó¥ÁºñÁ†ÅÂô®Èò∂ÊÆµÁöÑÂÖ®Â±ÄÁâπÂæÅÔºå‰ª•ÊîπÂñÑ‰∏ä‰∏ãÊñáÁêÜËß£ÔºåÂêåÊó∂‰øùÊåÅËΩªÈáèÁ∫ßÁªìÊûÑ‰ª•Èôç‰ΩéÂ§çÊùÇÂ∫¶„ÄÇÊÄßËÉΩÂíåÊïàÁéá‰πãÈó¥ÁöÑËøôÁßçÂπ≥Ë°°‰ΩøÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÂú®Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºàÂ¶Ç 1 Ê¨°Âíå 5 Ê¨°ËÆæÁΩÆ‰∏≠ÁöÑ $PASCAL-5^i$ Âíå $COCO-20^i$Ôºâ‰∏äÂÆûÁé∞ÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨Âè™Êúâ 150 ‰∏á‰∏™ÂèÇÊï∞ÁöÑÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÁ´û‰∫âÊÄßËÉΩÔºåÂêåÊó∂ÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑÈôêÂà∂„ÄÇhttps://github.com/amirrezafateh/MSDNet

##### **SpMis: An Investigation of Synthetic Spoken Misinformation Detection**
2409.11308v1 by Peizhuo Liu, Li Wang, Renqiang He, Haorui He, Lei Wang, Huadi Zheng, Jie Shi, Tong Xiao, Zhizheng Wu

In recent years, speech generation technology has advanced rapidly, fueled by
generative models and large-scale training techniques. While these developments
have enabled the production of high-quality synthetic speech, they have also
raised concerns about the misuse of this technology, particularly for
generating synthetic misinformation. Current research primarily focuses on
distinguishing machine-generated speech from human-produced speech, but the
more urgent challenge is detecting misinformation within spoken content. This
task requires a thorough analysis of factors such as speaker identity, topic,
and synthesis. To address this need, we conduct an initial investigation into
synthetic spoken misinformation detection by introducing an open-source
dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers
across five common topics, utilizing state-of-the-art text-to-speech systems.
Although our results show promising detection capabilities, they also reveal
substantial challenges for practical implementation, underscoring the
importance of ongoing research in this critical area.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåË™ûÈü≥ÁîüÊàêÊäÄË°ìÈÄ≤Â±ïÁ•ûÈÄüÔºåÊé®ÂãïÂäõ‰æÜËá™ÁîüÊàêÊ®°ÂûãËàáÂ§ßË¶èÊ®°Ë®ìÁ∑¥ÊäÄË°ì„ÄÇÈõñÁÑ∂ÈÄô‰∫õÁôºÂ±ïÂ∑≤ËÉΩÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂêàÊàêË™ûÈü≥Ôºå‰ΩÜ‰πüÂºïÁôºÂ∞çÊ≠§ÊäÄË°ìÈÅ≠Êø´Áî®ÁöÑÁñëÊÖÆÔºåÁâπÂà•ÊòØÊãø‰æÜÁî¢ÁîüÂêàÊàêÈåØË™§Ë≥áË®ä„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈóúÊ≥®Âú®ÂçÄÂàÜÊ©üÂô®Áî¢ÁîüÁöÑË™ûÈü≥Âíå‰∫∫È°ûÁî¢ÁîüÁöÑË™ûÈü≥Ôºå‰ΩÜÊõ¥Ëø´ÂàáÁöÑÊåëÊà∞ÊòØÂÅµÊ∏¨Âè£Ë™™ÂÖßÂÆπ‰∏≠ÁöÑÈåØË™§Ë≥áË®ä„ÄÇÈÄôÈ†Ö‰ªªÂãôÈúÄË¶ÅÂæπÂ∫ïÂàÜÊûêÂêÑÁ®ÆÂõ†Á¥†Ôºå‰æãÂ¶ÇË™™Ë©±ËÄÖË∫´ÂàÜ„ÄÅ‰∏ªÈ°åÂíåÂêàÊàê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈúÄÊ±ÇÔºåÊàëÂÄëÂ∞çÂêàÊàêÂè£Ë™™ÈåØË™§Ë≥áË®äÂÅµÊ∏¨ÈÄ≤Ë°åÂàùÊ≠•Ë™øÊü•Ôºå‰∏¶Êé®Âá∫‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºË≥áÊñôÈõÜ SpMis„ÄÇSpMis ÂåÖÂê´‰æÜËá™ 1,000 Â§ö‰ΩçË™™Ë©±ËÄÖ„ÄÅÊ©´Ë∑®‰∫îÂÄãÂ∏∏Ë¶ã‰∏ªÈ°åÁöÑÂêàÊàêË™ûÈü≥Ôºå‰∏¶Âà©Áî®ÊúÄÂÖàÈÄ≤ÁöÑÊñáÂ≠óËΩâË™ûÈü≥Á≥ªÁµ±„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÊúâÂâçÊôØÁöÑÂÅµÊ∏¨ËÉΩÂäõÔºå‰ΩÜ‰πüÊè≠Èú≤ÂØ¶ÈöõÂü∑Ë°åÁöÑÈáçÂ§ßÊåëÊà∞ÔºåÂº∑Ë™øÊåÅÁ∫åÁ†îÁ©∂ÈÄôÂÄãÈóúÈçµÈ†òÂüüÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**
2409.11299v1 by Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun

Biomedical image segmentation is crucial for accurately diagnosing and
analyzing various diseases. However, Convolutional Neural Networks (CNNs) and
Transformers, the most commonly used architectures for this task, struggle to
effectively capture long-range dependencies due to the inherent locality of
CNNs and the computational complexity of Transformers. To address this
limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time
Training (TTT) layers into the traditional U-Net architecture for biomedical
image segmentation. TTT-Unet dynamically adjusts model parameters during the
testing time, enhancing the model's ability to capture both local and
long-range features. We evaluate TTT-Unet on multiple medical imaging datasets,
including 3D abdominal organ segmentation in CT and MR images, instrument
segmentation in endoscopy images, and cell segmentation in microscopy images.
The results demonstrate that TTT-Unet consistently outperforms state-of-the-art
CNN-based and Transformer-based segmentation models across all tasks. The code
is available at https://github.com/rongzhou7/TTT-Unet.

ÊëòË¶ÅÔºöÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Â∞çÊñºÊ∫ñÁ¢∫Ë®∫Êñ∑ÂíåÂàÜÊûêÂêÑÁ®ÆÁñæÁóÖËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàCNNÔºâÂíå TransformerÔºå‰ΩúÁÇ∫Ê≠§‰ªªÂãôÊúÄÂ∏∏Áî®ÁöÑÊû∂ÊßãÔºåÁî±Êñº CNN ÁöÑÂõ∫ÊúâÂ±ÄÈÉ®ÊÄßÂíå Transformer ÁöÑË®àÁÆóË§áÈõúÊÄßÔºåÈõ£‰ª•ÊúâÊïàÊì∑ÂèñÈï∑Á®ã‰æùË≥¥ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü TTT-UnetÔºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂ∞áÊ∏¨Ë©¶ÊôÇÈñìË®ìÁ∑¥ÔºàTTTÔºâÂ±§Êï¥ÂêàÂà∞ÂÇ≥Áµ±ÁöÑ U-Net Êû∂Êßã‰∏≠ÔºåÁî®ÊñºÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇTTT-Unet Âú®Ê∏¨Ë©¶ÊôÇÈñìÂãïÊÖãË™øÊï¥Ê®°ÂûãÂèÉÊï∏ÔºåÂ¢ûÂº∑Ê®°ÂûãÊì∑ÂèñÂ±ÄÈÉ®ÂíåÈï∑Á®ãÁâπÂæµÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®Â§öÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äË©ï‰º∞ TTT-UnetÔºåÂåÖÊã¨ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂíåÁ£ÅÊåØÈÄ†ÂΩ±‰∏≠ÁöÑ 3D ËÖπËÖîÂô®ÂÆòÂàÜÂâ≤„ÄÅÂÖßË¶ñÈè°ÂΩ±ÂÉè‰∏≠ÁöÑÂÑÄÂô®ÂàÜÂâ≤‰ª•ÂèäÈ°ØÂæÆÈè°ÂΩ±ÂÉè‰∏≠ÁöÑÁ¥∞ËÉûÂàÜÂâ≤„ÄÇÁµêÊûúË°®ÊòéÔºåTTT-Unet Âú®ÊâÄÊúâ‰ªªÂãô‰∏≠ÈÉΩÊåÅÁ∫åÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫Êñº CNN ÂíåÂü∫Êñº Transformer ÁöÑÂàÜÂâ≤Ê®°Âûã„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/rongzhou7/TTT-Unet ÂèñÂæó„ÄÇ

##### **EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**
2409.11295v1 by Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun

Generalist web agents have evolved rapidly and demonstrated remarkable
potential. However, there are unprecedented safety risks associated with these
them, which are nearly unexplored so far. In this work, we aim to narrow this
gap by conducting the first study on the privacy risks of generalist web agents
in adversarial environments. First, we present a threat model that discusses
the adversarial targets, constraints, and attack scenarios. Particularly, we
consider two types of adversarial targets: stealing users' specific personally
identifiable information (PII) or stealing the entire user request. To achieve
these objectives, we propose a novel attack method, termed Environmental
Injection Attack (EIA). This attack injects malicious content designed to adapt
well to different environments where the agents operate, causing them to
perform unintended actions. This work instantiates EIA specifically for the
privacy scenario. It inserts malicious web elements alongside persuasive
instructions that mislead web agents into leaking private information, and can
further leverage CSS and JavaScript features to remain stealthy. We collect 177
actions steps that involve diverse PII categories on realistic websites from
the Mind2Web dataset, and conduct extensive experiments using one of the most
capable generalist web agent frameworks to date, SeeAct. The results
demonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.
Stealing full user requests is more challenging, but a relaxed version of EIA
can still achieve 16% ASR. Despite these concerning results, it is important to
note that the attack can still be detectable through careful human inspection,
highlighting a trade-off between high autonomy and security. This leads to our
detailed discussion on the efficacy of EIA under different levels of human
supervision as well as implications on defenses for generalist web agents.

ÊëòË¶ÅÔºö<paragraph>ÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÂø´ÈÄüÊºîÂåñÔºåÂ±ïÁèæÂá∫È©ö‰∫∫ÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õ‰ª£ÁêÜ‰º¥Èö®ËëóÂâçÊâÄÊú™ÊúâÁöÑÂÆâÂÖ®È¢®Èö™ÔºåËÄåÈÄô‰∫õÈ¢®Èö™ÁõÆÂâçÂπæ‰πéÂ∞öÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÂü∑Ë°åÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÂú®Â∞çÊäóÁí∞Â¢É‰∏≠ÁöÑÈö±ÁßÅÈ¢®Èö™ÁöÑÁ¨¨‰∏ÄÂÄãÁ†îÁ©∂‰æÜÁ∏ÆÂ∞èÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ®ÅËÑÖÊ®°ÂûãÔºåË®éË´ñÂ∞çÊäóÁõÆÊ®ô„ÄÅÈôêÂà∂ÂíåÊîªÊìäÊÉÖÂ¢É„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëËÄÉÊÖÆÂÖ©Á®ÆÈ°ûÂûãÁöÑÂ∞çÊäóÁõÆÊ®ôÔºöÁ´äÂèñ‰ΩøÁî®ËÄÖÁöÑÁâπÂÆöÂÄã‰∫∫ÂèØË≠òÂà•Ë≥áË®ä (PII) ÊàñÁ´äÂèñÊï¥ÂÄã‰ΩøÁî®ËÄÖË¶ÅÊ±Ç„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÈÄô‰∫õÁõÆÊ®ôÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊîªÊìäÊñπÊ≥ïÔºåÁ®±ÁÇ∫Áí∞Â¢ÉÊ≥®ÂÖ•ÊîªÊìä (EIA)„ÄÇÊ≠§ÊîªÊìäÊ≥®ÂÖ•ÊÉ°ÊÑèÂÖßÂÆπÔºåÊó®Âú®ÈÅ©Êáâ‰ª£ÁêÜÈÅã‰ΩúÁöÑ‰∏çÂêåÁí∞Â¢ÉÔºåÂ∞éËá¥‰ª£ÁêÜÂü∑Ë°åÈùûÈ†êÊúüÁöÑÂãï‰Ωú„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁâπÂà•ÈáùÂ∞çÈö±ÁßÅÊÉÖÂ¢ÉÂØ¶‰æãÂåñ EIA„ÄÇÂÆÉÂú®ÂÖ∑ÊúâË™™ÊúçÂäõÁöÑÊåá‰ª§ÊóÅÊèíÂÖ•ÊÉ°ÊÑèÁ∂≤Ë∑ØÂÖÉÁ¥†ÔºåË™§Â∞éÁ∂≤Ë∑Ø‰ª£ÁêÜÊ¥©Èú≤ÁßÅ‰∫∫Ë≥áË®äÔºå‰∏¶ÂèØÈÄ≤‰∏ÄÊ≠•Âà©Áî® CSS Âíå JavaScript ÂäüËÉΩ‰øùÊåÅÈö±ÂØÜ„ÄÇÊàëÂÄëÂæû Mind2Web Ë≥áÊñôÈõÜÁöÑÁèæÂØ¶Á∂≤Á´ôÊî∂ÈõÜ‰∫Ü 177 ÂÄãÊ∂âÂèä‰∏çÂêå PII È°ûÂà•ÁöÑÂãï‰ΩúÊ≠•È©üÔºå‰∏¶‰ΩøÁî®ËøÑ‰ªäÁÇ∫Ê≠¢ÂäüËÉΩÊúÄÂº∑Â§ßÁöÑÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÊ°ÜÊû∂ SeeAct ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©ó„ÄÇÁµêÊûúË≠âÊòéÔºåEIA Âú®Á´äÂèñ‰ΩøÁî®ËÄÖÁöÑÁâπÂÆö PII ÊñπÈù¢ÈÅîÂà∞‰∫Ü 70% ÁöÑ ASR„ÄÇÁ´äÂèñÂÆåÊï¥ÁöÑ‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÔºå‰ΩÜ EIA ÁöÑÊîæÂØ¨ÁâàÊú¨‰ªçÂèØÈÅîÂà∞ 16% ÁöÑ ASR„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õ‰ª§‰∫∫ÊìîÊÜÇÁöÑÁµêÊûúÔºå‰ΩÜÈáçË¶ÅÁöÑÊòØË¶ÅÊ≥®ÊÑèÔºåÊîªÊìä‰ªçÁÑ∂ÂèØ‰ª•ÈÄèÈÅé‰ªîÁ¥∞ÁöÑ‰∫∫Â∑•Ê™¢Êü•‰æÜÂÅµÊ∏¨ÔºåÁ™ÅÈ°Ø‰∫ÜÈ´òÂ∫¶Ëá™‰∏ªÊÄßËàáÂÆâÂÖ®ÊÄß‰πãÈñìÁöÑÊ¨äË°°„ÄÇÈÄôÂ∞éËá¥ÊàëÂÄëË©≥Á¥∞Ë®éË´ñ‰∫Ü EIA Âú®‰∏çÂêåÂ±§Á¥öÁöÑ‰∫∫Â∑•Áõ£Áù£‰∏ãÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂ∞çÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÈò≤Á¶¶ÁöÑÂΩ±Èüø„ÄÇ</paragraph>

##### **Navigating Process Mining: A Case study using pm4py**
2409.11294v1 by Ali Jlidi, L√°szl√≥ Kov√°cs

Process-mining techniques have emerged as powerful tools for analyzing event
data to gain insights into business processes. In this paper, we present a
comprehensive analysis of road traffic fine management processes using the
pm4py library in Python. We start by importing an event log dataset and explore
its characteristics, including the distribution of activities and process
variants. Through filtering and statistical analysis, we uncover key patterns
and variations in the process executions. Subsequently, we apply various
process-mining algorithms, including the Alpha Miner, Inductive Miner, and
Heuristic Miner, to discover process models from the event log data. We
visualize the discovered models to understand the workflow structures and
dependencies within the process. Additionally, we discuss the strengths and
limitations of each mining approach in capturing the underlying process
dynamics. Our findings shed light on the efficiency and effectiveness of road
traffic fine management processes, providing valuable insights for process
optimization and decision-making. This study demonstrates the utility of pm4py
in facilitating process mining tasks and its potential for analyzing real-world
business processes.

ÊëòË¶ÅÔºöÊµÅÁ®ãÊåñÊéòÊäÄË°ìÂ∑≤ÊàêÁÇ∫ÂàÜÊûê‰∫ã‰ª∂Ë≥áÊñô‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê•≠ÂãôÊµÅÁ®ãÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ΩøÁî® Python ‰∏≠ÁöÑ pm4py ÂáΩÂºèÂ∫´ÔºåÂ∞çÈÅìË∑Ø‰∫§ÈÄöÁΩ∞Ê¨æÁÆ°ÁêÜÊµÅÁ®ãÈÄ≤Ë°åÂÖ®Èù¢ÂàÜÊûê„ÄÇÊàëÂÄëÈ¶ñÂÖàÂåØÂÖ•‰∫ã‰ª∂Êó•Ë™åË≥áÊñôÈõÜÔºå‰∏¶Êé¢Ë®éÂÖ∂ÁâπÂæµÔºåÂåÖÊã¨Ê¥ªÂãïÂàÜ‰ΩàÂíåÊµÅÁ®ãËÆäÈ´î„ÄÇÈÄèÈÅéÈÅéÊøæÂíåÁµ±Ë®àÂàÜÊûêÔºåÊàëÂÄëÊè≠Èú≤‰∫ÜÊµÅÁ®ãÂü∑Ë°å‰∏≠ÁöÑÈóúÈçµÊ®°ÂºèÂíåËÆäÁï∞„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊáâÁî®ÂêÑÁ®ÆÊµÅÁ®ãÊåñÊéòÊºîÁÆóÊ≥ïÔºåÂåÖÊã¨ Alpha Miner„ÄÅInductive Miner Âíå Heuristic MinerÔºåÂæû‰∫ã‰ª∂Êó•Ë™åË≥áÊñô‰∏≠ÁôºÁèæÊµÅÁ®ãÊ®°Âûã„ÄÇÊàëÂÄëÂ∞áÁôºÁèæÁöÑÊ®°ÂûãË¶ñË¶∫ÂåñÔºå‰ª•‰∫ÜËß£ÊµÅÁ®ã‰∏≠ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÁµêÊßãÂíå‰æùË≥¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫ÜÊØèÁ®ÆÊåñÊéòÊñπÊ≥ïÂú®ÊçïÊçâÂü∫Á§éÊµÅÁ®ãÂãïÊÖãÊñπÈù¢ÁöÑÂÑ™Áº∫Èªû„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈó°Êòé‰∫ÜÈÅìË∑Ø‰∫§ÈÄöÁΩ∞Ê¨æÁÆ°ÁêÜÊµÅÁ®ãÁöÑÊïàÁéáÂíåÊïàËÉΩÔºåÁÇ∫ÊµÅÁ®ãÊúÄ‰Ω≥ÂåñÂíåÊ±∫Á≠ñÂà∂ÂÆöÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£„ÄÇÊú¨Á†îÁ©∂Â±ïÁ§∫‰∫Ü pm4py Âú®‰øÉÈÄ≤ÊµÅÁ®ãÊåñÊéò‰ªªÂãôÂíåÂàÜÊûêÁèæÂØ¶‰∏ñÁïåÊ•≠ÂãôÊµÅÁ®ãÊñπÈù¢ÁöÑÊïàÁî®„ÄÇ

##### **Neural Networks for Vehicle Routing Problem**
2409.11290v1 by L√°szl√≥ Kov√°cs, Ali Jlidi

The Vehicle Routing Problem is about optimizing the routes of vehicles to
meet the needs of customers at specific locations. The route graph consists of
depots on several levels and customer positions. Several optimization methods
have been developed over the years, most of which are based on some type of
classic heuristic: genetic algorithm, simulated annealing, tabu search, ant
colony optimization, firefly algorithm. Recent developments in machine learning
provide a new toolset, the rich family of neural networks, for tackling complex
problems. The main area of application of neural networks is the area of
classification and regression. Route optimization can be viewed as a new
challenge for neural networks. The article first presents an analysis of the
applicability of neural network tools, then a novel graphical neural network
model is presented in detail. The efficiency analysis based on test experiments
shows the applicability of the proposed NN architecture.

ÊëòË¶ÅÔºöËªäËºõË∑ØÁ∑öÂïèÈ°åÊòØÈóúÊñºÊúÄ‰Ω≥ÂåñËªäËºõË∑ØÁ∑öÔºå‰ª•ÊªøË∂≥ÁâπÂÆöÂú∞ÈªûÂÆ¢Êà∂ÁöÑÈúÄÊ±Ç„ÄÇË∑ØÁ∑öÂúñÂåÖÂê´Â§öÂÄãÂ±§Á¥öÁöÑËªäËºõÂ≠òÊîæËôïÂíåÂÆ¢Êà∂‰ΩçÁΩÆ„ÄÇÂ§öÂπ¥‰æÜÂ∑≤Á∂ìÈñãÁôºÂá∫Â§öÁ®ÆÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÔºåÂÖ∂‰∏≠Â§ßÂ§öÊï∏Âü∫ÊñºÊüêÁ®ÆÈ°ûÂûãÁöÑÁ∂ìÂÖ∏ÂïüÁôºÂºèÊñπÊ≥ïÔºöÈÅ∫ÂÇ≥ÊºîÁÆóÊ≥ï„ÄÅÊ®°Êì¨ÈÄÄÁÅ´„ÄÅÁ¶ÅÂøåÊêúÂ∞ã„ÄÅËüªÁæ§ÊúÄ‰Ω≥Âåñ„ÄÅËû¢ÁÅ´Ëü≤ÊºîÁÆóÊ≥ï„ÄÇÊ©üÂô®Â≠∏ÁøíÁöÑÊúÄÊñ∞ÁôºÂ±ïÊèê‰æõ‰∫Ü‰∏ÄÁµÑÊñ∞ÁöÑÂ∑•ÂÖ∑ÔºåÂç≥Ë±êÂØåÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÂÆ∂ÊóèÔºåÁî®ÊñºËß£Ê±∫Ë§áÈõúÁöÑÂïèÈ°å„ÄÇÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑ‰∏ªË¶ÅÊáâÁî®È†òÂüüÊòØÂàÜÈ°ûÂíåÂõûÊ≠∏È†òÂüü„ÄÇË∑ØÁ∑öÊúÄ‰Ω≥ÂåñÂèØ‰ª•Ë¶ñÁÇ∫Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊñ∞ÊåëÊà∞„ÄÇÊú¨ÊñáÈ¶ñÂÖàÂàÜÊûê‰∫ÜÁ•ûÁ∂ìÁ∂≤Ë∑ØÂ∑•ÂÖ∑ÁöÑÈÅ©Áî®ÊÄßÔºåÁÑ∂ÂæåË©≥Á¥∞‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°Âûã„ÄÇÂü∫ÊñºÊ∏¨Ë©¶ÂØ¶È©óÁöÑÊïàÁéáÂàÜÊûêÈ°ØÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÁöÑÈÅ©Áî®ÊÄß„ÄÇ

##### **Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling**
2409.11283v2 by Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li

LLMs obtain remarkable performance but suffer from hallucinations. Most
research on detecting hallucination focuses on the questions with short and
concrete correct answers that are easy to check the faithfulness. Hallucination
detections for text generation with open-ended answers are more challenging.
Some researchers use external knowledge to detect hallucinations in generated
texts, but external resources for specific scenarios are hard to access. Recent
studies on detecting hallucinations in long text without external resources
conduct consistency comparison among multiple sampled outputs. To handle long
texts, researchers split long texts into multiple facts and individually
compare the consistency of each pairs of facts. However, these methods (1)
hardly achieve alignment among multiple facts; (2) overlook dependencies
between multiple contextual facts. In this paper, we propose a graph-based
context-aware (GCA) hallucination detection for text generations, which aligns
knowledge facts and considers the dependencies between contextual knowledge
triples in consistency comparison. Particularly, to align multiple facts, we
conduct a triple-oriented response segmentation to extract multiple knowledge
triples. To model dependencies among contextual knowledge triple (facts), we
construct contextual triple into a graph and enhance triples' interactions via
message passing and aggregating via RGCN. To avoid the omission of knowledge
triples in long text, we conduct a LLM-based reverse verification via
reconstructing the knowledge triples. Experiments show that our model enhances
hallucination detection and excels all baselines.

ÊëòË¶ÅÔºöLLM Áç≤ÂæóÈ°ØËëóÁöÑÊïàËÉΩÔºå‰ΩÜÊúÉÂá∫ÁèæÂπªË¶∫„ÄÇÂ§ßÂ§öÊï∏ÈóúÊñºÂÅµÊ∏¨ÂπªË¶∫ÁöÑÁ†îÁ©∂ÈÉΩÈõÜ‰∏≠Âú®ÂïèÈ°åÁöÑÁ≠îÊ°àÁ∞°Áü≠‰∏îÂÖ∑È´îÔºå‰∏îÊòìÊñºÊ™¢Êü•ÂÖ∂ÁúüÂØ¶ÊÄß„ÄÇÂ∞çÊñºÈñãÊîæÂºèÁ≠îÊ°àÁöÑÊñáÂ≠óÁîüÊàêÔºåÂπªË¶∫ÁöÑÂÅµÊ∏¨Êõ¥ÂÖ∑ÊåëÊà∞ÊÄß„ÄÇ‰∏Ä‰∫õÁ†îÁ©∂‰∫∫Âì°‰ΩøÁî®Â§ñÈÉ®Áü•Ë≠ò‰æÜÂÅµÊ∏¨ÁîüÊàêÁöÑÊñáÂ≠ó‰∏≠ÁöÑÂπªË¶∫Ôºå‰ΩÜÁâπÂÆöÂ†¥ÊôØÁöÑÂ§ñÈÉ®Ë≥áÊ∫êÈõ£‰ª•ÂèñÂæó„ÄÇÊúÄËøëÈóúÊñºÂú®Ê≤íÊúâÂ§ñÈÉ®Ë≥áÊ∫êÁöÑÊÉÖÊ≥Å‰∏ãÂÅµÊ∏¨Èï∑ÁØáÊñáÂ≠óÂπªË¶∫ÁöÑÁ†îÁ©∂ÔºåÂú®Â§öÂÄãÂèñÊ®£Ëº∏Âá∫‰πãÈñìÈÄ≤Ë°å‰∏ÄËá¥ÊÄßÊØîËºÉ„ÄÇÁÇ∫‰∫ÜËôïÁêÜÈï∑ÁØáÊñáÂ≠óÔºåÁ†îÁ©∂‰∫∫Âì°Â∞áÈï∑ÁØáÊñáÂ≠óÊãÜÂàÜÊàêÂ§öÂÄã‰∫ãÂØ¶Ôºå‰∏¶ÂÄãÂà•ÊØîËºÉÊØèÂ∞ç‰∫ãÂØ¶ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÔºà1ÔºâÈõ£‰ª•Âú®Â§öÂÄã‰∫ãÂØ¶‰πãÈñìÂèñÂæó‰∏ÄËá¥ÊÄßÔºõÔºà2ÔºâÂøΩÁï•Â§öÂÄãËÑàÁµ°‰∫ãÂØ¶‰πãÈñìÁöÑ‰æùË≥¥ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂúñÂΩ¢„ÄÅÂÖ∑ËÑàÁµ°ÊÑüÁü•ÔºàGCAÔºâÁöÑÊñáÂ≠óÁîüÊàêÂπªË¶∫ÂÅµÊ∏¨ÔºåÂÆÉÊúÉÊØîÂ∞çÁü•Ë≠ò‰∫ãÂØ¶Ôºå‰∏¶Âú®‰∏ÄËá¥ÊÄßÊØîËºÉ‰∏≠ËÄÉÊÖÆËÑàÁµ°Áü•Ë≠ò‰∏âÂÖÉÁµÑ‰πãÈñìÁöÑ‰æùË≥¥ÊÄß„ÄÇÁâπÂà•ÊòØÔºåÁÇ∫‰∫ÜÊØîÂ∞çÂ§öÂÄã‰∫ãÂØ¶ÔºåÊàëÂÄëÈÄ≤Ë°å‰∏âÂÖÉÂ∞éÂêëÁöÑÂõûÊáâÂçÄÈöîÔºå‰ª•ËêÉÂèñÂ§öÂÄãÁü•Ë≠ò‰∏âÂÖÉÁµÑ„ÄÇÁÇ∫‰∫ÜÊ®°Êì¨ËÑàÁµ°Áü•Ë≠ò‰∏âÂÖÉÁµÑÔºà‰∫ãÂØ¶Ôºâ‰πãÈñìÁöÑ‰æùË≥¥ÊÄßÔºåÊàëÂÄëÂ∞áËÑàÁµ°‰∏âÂÖÉÁµÑÂª∫ÊßãÁÇ∫‰∏ÄÂÄãÂúñÂΩ¢Ôºå‰∏¶ÈÄèÈÅéË®äÊÅØÂÇ≥ÈÅûÂíåÈÄèÈÅé RGCN ÈÄ≤Ë°åÂΩôÁ∏Ω‰æÜÂ¢ûÂº∑‰∏âÂÖÉÁµÑÁöÑ‰∫íÂãï„ÄÇÁÇ∫‰∫ÜÈÅøÂÖçÈÅ∫ÊºèÈï∑ÁØáÊñáÂ≠ó‰∏≠ÁöÑÁü•Ë≠ò‰∏âÂÖÉÁµÑÔºåÊàëÂÄëÈÄèÈÅéÈáçÂª∫Áü•Ë≠ò‰∏âÂÖÉÁµÑÈÄ≤Ë°åÂü∫Êñº LLM ÁöÑÂèçÂêëÈ©óË≠â„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ¢ûÂº∑‰∫ÜÂπªË¶∫ÂÅµÊ∏¨Ôºå‰∏¶ÂÑ™ÊñºÊâÄÊúâÂü∫Ê∫ñ„ÄÇ

##### **Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**
2409.11282v1 by Marcel Lamott, Muhammad Armaghan Shakir

The surge of digital documents in various formats, including less
standardized documents such as business reports and environmental assessments,
underscores the growing importance of Document Understanding. While Large
Language Models (LLMs) have showcased prowess across diverse natural language
processing tasks, their direct application to Document Understanding remains a
challenge. Previous research has demonstrated the utility of LLMs in this
domain, yet their significant computational demands make them challenging to
deploy effectively. Additionally, proprietary Blackbox LLMs often outperform
their open-source counterparts, posing a barrier to widespread accessibility.
In this paper, we delve into the realm of document understanding, leveraging
distillation methods to harness the power of large LLMs while accommodating
computational limitations. Specifically, we present a novel approach wherein we
distill document understanding knowledge from the proprietary LLM ChatGPT into
FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms
to facilitate efficient knowledge transfer. This work contributes to the
advancement of document understanding methodologies by offering a scalable
solution that bridges the gap between resource-intensive LLMs and practical
applications. Our findings underscore the potential of distillation techniques
in facilitating the deployment of sophisticated language models in real-world
scenarios, thereby fostering advancements in natural language processing and
document comprehension domains.

ÊëòË¶ÅÔºöÈö®ËëóÂêÑÁ®ÆÊ†ºÂºèÊï∏‰ΩçÊñá‰ª∂ÊøÄÂ¢ûÔºåÂåÖÊã¨ÂïÜÊ•≠Â†±ÂëäÂíåÁí∞Â¢ÉË©ï‰º∞Á≠âÊ®ôÊ∫ñÂåñÁ®ãÂ∫¶ËºÉ‰ΩéÁöÑÊ™îÊ°àÔºåÊñá‰ª∂ÁêÜËß£ÁöÑÈáçË¶ÅÊÄßËàáÊó•‰ø±Â¢û„ÄÇÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠Â±ïÁèæÂÖ∂ÂÑ™Âã¢Ôºå‰ΩÜÂÆÉÂÄëÂú®Êñá‰ª∂ÁêÜËß£‰∏≠ÁöÑÁõ¥Êé•ÊáâÁî®‰ªçÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòé LLM Âú®Ê≠§È†òÂüüÁöÑÊïàÁî®Ôºå‰ΩÜÂÖ∂ÈæêÂ§ßÁöÑÈÅãÁÆóÈúÄÊ±Ç‰ΩøÂÖ∂Èõ£‰ª•ÊúâÊïàÈÉ®ÁΩ≤„ÄÇÊ≠§Â§ñÔºåÂ∞àÊúâÁöÑÈªëÁõíÂ≠ê LLM ÈÄöÂ∏∏ÂÑ™ÊñºÂÖ∂ÈñãÊ∫êÂ∞çÊáâÁâàÊú¨ÔºåÂ∞çÂª£Ê≥õÁöÑÂèØÂèäÊÄßÊßãÊàêÈöúÁ§ô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®éÊñá‰ª∂ÁêÜËß£È†òÂüüÔºåÂà©Áî®Áü•Ë≠òËí∏È§æÊñπÊ≥ï‰æÜÂà©Áî®Â§ßÂûã LLM ÁöÑÂäüËÉΩÔºåÂêåÊôÇÈÅ©ÊáâÈÅãÁÆóÈôêÂà∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÊàëÂÄëÂ∞áÊñá‰ª∂ÁêÜËß£Áü•Ë≠òÂæûÂ∞àÊúâÁöÑ LLM ChatGPT Ëí∏È§æÂà∞ FLAN-T5 ‰∏≠„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÊï¥Âêà‰∫ÜÊ®ôÁ±§ÂíåË™≤Á®ãÂ≠∏ÁøíÊ©üÂà∂Ôºå‰ª•‰øÉÈÄ≤ÊúâÊïàÁöÑÁü•Ë≠òÂÇ≥ÈÅû„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈÄèÈÅéÊèê‰æõ‰∏ÄÂÄãÂèØÊì¥ÂÖÖËß£Ê±∫ÊñπÊ°à‰æÜÊé®ÈÄ≤Êñá‰ª∂ÁêÜËß£ÊñπÊ≥ïÔºåÂΩåÂêà‰∫ÜË≥áÊ∫êÂØÜÈõÜÂûã LLM ÂíåÂØ¶ÈöõÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜÁü•Ë≠òËí∏È§æÊäÄË°ìÂú®‰øÉÈÄ≤Ë§áÈõúË™ûË®ÄÊ®°ÂûãÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÈÉ®ÁΩ≤ÁöÑÊΩõÂäõÔºåÂæûËÄå‰øÉÈÄ≤Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÊñá‰ª∂ÁêÜËß£È†òÂüüÁöÑÈÄ≤Ê≠•„ÄÇ

##### **P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task**
2409.11279v1 by Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li

Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.

ÊëòË¶ÅÔºöÂÖ∑Ë∫´Êó•Â∏∏‰ªªÂãôÊòØÂÖ∑Ë∫´ AI Á§æÁæ§‰∏≠Â∏∏Ë¶ãÁöÑ‰ªªÂãôÔºå
Ë¶ÅÊ±Ç‰ª£ÁêÜÊ†πÊìöËá™ÁÑ∂Ë™ûË®ÄÊåá‰ª§ÂíåË¶ñË¶∫ËßÄÂØüÈÄ≤Ë°å‰∏ÄÁ≥ªÂàóÂãï‰Ωú„ÄÇÂÇ≥Áµ±ÁöÑÂü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ï
Èù¢Ëá®ÂÖ©È†ÖÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåËá™ÁÑ∂Ë™ûË®ÄÊåá‰ª§ÈÄöÂ∏∏Áº∫‰πèÊòéÁ¢∫ÁöÑ
‰ªªÂãôË¶èÂäÉ„ÄÇÂÖ∂Ê¨°ÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑË®ìÁ∑¥ÊâçËÉΩËÆìÊ®°ÂûãÂÖ∑ÂÇô
‰ªªÂãôÁí∞Â¢ÉÁöÑÁü•Ë≠ò„ÄÇÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÖàÂâç‰ΩúÂìÅ
Áî±ÊñºÁº∫‰πèÁâπÂÆö‰ªªÂãôÁöÑÁü•Ë≠òËÄåÂ∞éËá¥ÊïàËÉΩ‰∏ç‰Ω≥Êàñ‰æùË≥¥ÊñºÂ∞ëÊï∏Ê®£Êú¨ÁöÑÁúüÂØ¶Êï∏Êìö„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞
ÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êº∏ÈÄ≤ÂºèÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (P-RAG) ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉ‰∏çÂÉÖÊúâÊïàÂú∞Âà©Áî®‰∫Ü LLM Âº∑Â§ßÁöÑË™ûË®Ä
ËôïÁêÜËÉΩÂäõÔºåËÄå‰∏îÈÇÑÈÄêÊ≠•Á¥ØÁ©ç
‰ªªÂãôÁâπÂÆöÁü•Ë≠òÔºåËÄåÁÑ°ÈúÄÁúüÂØ¶Êï∏Êìö„ÄÇËàáÂÇ≥Áµ±ÁöÑ RAG ÊñπÊ≥ïÁõ∏ÊØîÔºåÂæåËÄÖ‰ª•‰∏ÄÊ¨°ÊÄßÁöÑÊñπÂºèÂæûË≥áÊñôÂ∫´‰∏≠Ê™¢Á¥¢Áõ∏ÈóúË≥áË®ä‰ª•ÂçîÂä©ÁîüÊàêÔºåP-RAG Êé°Áî®‰∏ÄÁ®ÆÂèçË¶ÜÈÅãÁÆóÁöÑÊñπÊ≥ï‰æÜ
ÈÄêÊ≠•Êõ¥Êñ∞Ë≥áÊñôÂ∫´„ÄÇÂú®ÊØèÂÄãÂèçË¶ÜÈÅãÁÆó‰∏≠ÔºåP-RAG Ê™¢Á¥¢ÊúÄÊñ∞ÁöÑË≥áÊñôÂ∫´‰∏¶ÂæûÂÖàÂâçÁöÑ
‰∫íÂãï‰∏≠Áç≤ÂèñÊ≠∑Âè≤Ë≥áË®ä‰ΩúÁÇ∫Áï∂Ââç‰∫íÂãïÁöÑÁ∂ìÈ©óÂèÉËÄÉ„ÄÇÊ≠§Â§ñÔºå
ÊàëÂÄëÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊõ¥Á≤æÁ¥∞ÁöÑÊ™¢Á¥¢ÊñπÊ°àÔºåÂÆÉ‰∏çÂÉÖÊ™¢Á¥¢
È°û‰ººÁöÑ‰ªªÂãôÔºåÈÇÑÂåÖÂê´Ê™¢Á¥¢È°û‰ººÁöÑÁãÄÊ≥ÅÔºå‰ª•Êèê‰æõ
Êõ¥ÊúâÂÉπÂÄºÁöÑÂèÉËÄÉÁ∂ìÈ©ó„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË°®ÊòéÔºåP-RAG
Âú®‰∏ç‰ΩøÁî®ÁúüÂØ¶Êï∏ÊìöÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶Áèæ‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºåÁîöËá≥
ÂèØ‰ª•ÈÄèÈÅéËá™ÊàëÂèçË¶ÜÈÅãÁÆóÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊïàËÉΩ„ÄÇ

##### **Machine Learning and Theory Ladenness -- A Phenomenological Account**
2409.11277v1 by Alberto Termine, Emanuele Ratti, Alessandro Facchini

In recent years, the dissemination of machine learning (ML) methodologies in
scientific research has prompted discussions on theory ladenness. More
specifically, the issue of theory ladenness has remerged as questions about
whether and how ML models (MLMs) and ML modelling strategies are impacted by
the domain theory of the scientific field in which ML is used and implemented
(e.g., physics, chemistry, biology, etc). On the one hand, some have argued
that there is no difference between traditional (pre ML) and ML assisted
science. In both cases, theory plays an essential and unavoidable role in the
analysis of phenomena and the construction and use of models. Others have
argued instead that ML methodologies and models are theory independent and, in
some cases, even theory free. In this article, we argue that both positions are
overly simplistic and do not advance our understanding of the interplay between
ML methods and domain theories. Specifically, we provide an analysis of theory
ladenness in ML assisted science. Our analysis reveals that, while the
construction of MLMs can be relatively independent of domain theory, the
practical implementation and interpretation of these models within a given
specific domain still relies on fundamental theoretical assumptions and
background knowledge.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåÊú∫Âô®Â≠¶‰π†ÔºàMLÔºâÊñπÊ≥ïÂú®ÁßëÂ≠¶Á†îÁ©∂‰∏≠ÁöÑ‰º†Êí≠ÂºïÂèë‰∫ÜÂÖ≥‰∫éÁêÜËÆ∫Ë¥üËΩΩÁöÑËÆ®ËÆ∫„ÄÇÊõ¥ÂÖ∑‰ΩìÂú∞ËØ¥ÔºåÁêÜËÆ∫Ë¥üËΩΩÁöÑÈóÆÈ¢òÂ∑≤ÁªèÈáçÊñ∞Âá∫Áé∞ÔºåÂõ†‰∏∫ÂÖ≥‰∫é ML Ê®°Âûã (MLM) Âíå ML Âª∫Ê®°Á≠ñÁï•ÊòØÂê¶‰ª•ÂèäÂ¶Ç‰ΩïÂèóÂà∞ ML ÊâÄ‰ΩøÁî®ÂíåÂÆûÊñΩÁöÑÁßëÂ≠¶È¢ÜÂüüÔºà‰æãÂ¶ÇÁâ©ÁêÜÂ≠¶„ÄÅÂåñÂ≠¶„ÄÅÁîüÁâ©Â≠¶Á≠âÔºâÁöÑÈ¢ÜÂüüÁêÜËÆ∫ÁöÑÂΩ±ÂìçÁöÑÈóÆÈ¢ò„ÄÇ‰∏ÄÊñπÈù¢Ôºå‰∏Ä‰∫õ‰∫∫ËÆ§‰∏∫‰º†ÁªüÔºàML ‰πãÂâçÔºâÂíå ML ËæÖÂä©ÁßëÂ≠¶‰πãÈó¥Ê≤°ÊúâÂå∫Âà´„ÄÇÂú®Ëøô‰∏§ÁßçÊÉÖÂÜµ‰∏ãÔºåÁêÜËÆ∫Âú®Áé∞Ë±°ÂàÜÊûê‰ª•ÂèäÊ®°ÂûãÁöÑÊûÑÂª∫Âíå‰ΩøÁî®‰∏≠ÈÉΩÊâÆÊºîÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰∏çÂèØÈÅøÂÖçÁöÑËßíËâ≤„ÄÇÂÖ∂‰ªñ‰∫∫ÂàôËÆ§‰∏∫ ML ÊñπÊ≥ïÂíåÊ®°Âûã‰∏éÁêÜËÆ∫Êó†ÂÖ≥ÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁîöËá≥Ê≤°ÊúâÁêÜËÆ∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ËÆ§‰∏∫Ëøô‰∏§ÁßçÁ´ãÂú∫Ëøá‰∫éÁÆÄÂçïÂåñÔºåÂπ∂Ê≤°Êúâ‰øÉËøõÊàë‰ª¨ÂØπ ML ÊñπÊ≥ïÂíåÈ¢ÜÂüüÁêÜËÆ∫‰πãÈó¥Áõ∏‰∫í‰ΩúÁî®ÁöÑÁêÜËß£„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÂØπ ML ËæÖÂä©ÁßëÂ≠¶‰∏≠ÁöÑÁêÜËÆ∫Ë¥üËΩΩËøõË°å‰∫ÜÂàÜÊûê„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåËôΩÁÑ∂ ML Ê®°ÂûãÁöÑÊûÑÂª∫ÂèØ‰ª•Áõ∏ÂØπÁã¨Á´ã‰∫éÈ¢ÜÂüüÁêÜËÆ∫Ôºå‰ΩÜËøô‰∫õÊ®°ÂûãÂú®ÁªôÂÆöÁâπÂÆöÈ¢ÜÂüüÂÜÖÁöÑÂÆûÈôÖÂÆûÊñΩÂíåËß£Èáä‰ªçÁÑ∂‰æùËµñ‰∫éÂü∫Êú¨ÁöÑÁêÜËÆ∫ÂÅáËÆæÂíåËÉåÊôØÁü•ËØÜ„ÄÇ

##### **Task Arithmetic for Language Expansion in Speech Translation**
2409.11274v1 by Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe

Recent advances in large language models (LLMs) have gained interest in
speech-text multimodal foundation models, achieving strong performance on
instruction-based speech translation (ST). However, expanding language pairs
from an existing instruction-tuned ST system is costly due to the necessity of
re-training on a combination of new and previous datasets. We propose to expand
new language pairs by merging the model trained on new language pairs and the
existing model, using task arithmetic. We find that the direct application of
task arithmetic for ST causes the merged model to fail to follow instructions;
thus, generating translation in incorrect languages. To eliminate language
confusion, we propose an augmented task arithmetic method that merges an
additional language control model. It is trained to generate the correct target
language token following the instructions. Our experiments demonstrate that our
proposed language control model can achieve language expansion by eliminating
language confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66
and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the
use of our task arithmetic framework can expand to a language pair where
neither paired ST training data nor a pre-trained ST model is available. We
first synthesize the ST system from machine translation (MT) systems via task
analogy, then merge the synthesized ST system to the existing ST model.

ÊëòË¶ÅÔºöÊúÄËøëÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ËøõÂ±ïÂºïËµ∑‰∫Ü‰∫∫‰ª¨ÂØπËØ≠Èü≥ÊñáÊú¨Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖ¥Ë∂£ÔºåÂú®Âü∫‰∫éÊåá‰ª§ÁöÑËØ≠Èü≥ÁøªËØë (ST) ‰∏≠ÂèñÂæó‰∫ÜÂº∫Âä≤ÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÈúÄË¶ÅÂú®Êñ∞ÁöÑÂíå‰ª•ÂâçÁöÑÊï∞ÊçÆÈõÜÁöÑÁªÑÂêà‰∏äÈáçÊñ∞ËÆ≠ÁªÉÔºåÂõ†Ê≠§‰ªéÁé∞ÊúâÁöÑÊåá‰ª§Ë∞ÉÊï¥ÁöÑ ST Á≥ªÁªüÊâ©Â±ïËØ≠Ë®ÄÂØπÁöÑÊàêÊú¨ÂæàÈ´ò„ÄÇÊàë‰ª¨Âª∫ËÆÆÈÄöËøá‰ΩøÁî®‰ªªÂä°ÁÆóÊúØÂêàÂπ∂ÈíàÂØπÊñ∞ËØ≠Ë®ÄÂØπËÆ≠ÁªÉÁöÑÊ®°ÂûãÂíåÁé∞ÊúâÊ®°ÂûãÊù•Êâ©Â±ïÊñ∞ËØ≠Ë®ÄÂØπ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁõ¥Êé•Â∞Ü‰ªªÂä°ÁÆóÊúØÂ∫îÁî®‰∫é ST ‰ºöÂØºËá¥ÂêàÂπ∂ÂêéÁöÑÊ®°ÂûãÊó†Ê≥ïÈÅµÂæ™Êåá‰ª§ÔºõÂõ†Ê≠§ÔºåÁîüÊàêÈîôËØØËØ≠Ë®ÄÁöÑÁøªËØë„ÄÇ‰∏∫‰∫ÜÊ∂àÈô§ËØ≠Ë®ÄÊ∑∑Ê∑ÜÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫‰ªªÂä°ÁÆóÊúØÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂêàÂπ∂‰∫Ü‰∏Ä‰∏™È¢ùÂ§ñÁöÑËØ≠Ë®ÄÊéßÂà∂Ê®°Âûã„ÄÇÂÆÉÁªèËøáËÆ≠ÁªÉÔºåÂèØ‰ª•ÊåâÁÖßÊåá‰ª§ÁîüÊàêÊ≠£Á°ÆÁöÑÁõÆÊ†áËØ≠Ë®ÄÊ†áËÆ∞„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑËØ≠Ë®ÄÊéßÂà∂Ê®°ÂûãÂèØ‰ª•ÈÄöËøáÊ∂àÈô§ËØ≠Ë®ÄÊ∑∑Ê∑ÜÊù•ÂÆûÁé∞ËØ≠Ë®ÄÊâ©Â±ï„ÄÇÂú®Êàë‰ª¨ÁöÑ MuST-C Âíå CoVoST-2 ÂÆûÈ™å‰∏≠ÔºåÂÆÉÂàÜÂà´ÊòæÁ§∫Âá∫È´òËææ 4.66 Âíå 4.92 ÁöÑ BLEU ÂàÜÊï∞ÊîπËøõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊàë‰ª¨ÁöÑ‰ªªÂä°ÁÆóÊúØÊ°ÜÊû∂ÁöÑ‰ΩøÁî®ÂèØ‰ª•Êâ©Â±ïÂà∞Êó¢Ê≤°ÊúâÈÖçÂØπ ST ËÆ≠ÁªÉÊï∞ÊçÆ‰πüÊ≤°ÊúâÈ¢ÑËÆ≠ÁªÉ ST Ê®°ÂûãÁöÑËØ≠Ë®ÄÂØπ„ÄÇÊàë‰ª¨È¶ñÂÖàÈÄöËøá‰ªªÂä°Á±ªÊØî‰ªéÊú∫Âô®ÁøªËØë (MT) Á≥ªÁªü‰∏≠ÂêàÊàê ST Á≥ªÁªüÔºåÁÑ∂ÂêéÂ∞ÜÂêàÊàêÁöÑ ST Á≥ªÁªüÂêàÂπ∂Âà∞Áé∞ÊúâÁöÑ ST Ê®°Âûã‰∏≠„ÄÇ

##### **LOLA -- An Open-Source Massively Multilingual Large Language Model**
2409.11272v2 by Nikit Srivastava, Denis Kuchelev, Tatiana Moteu, Kshitij Shetty, Michael R√∂der, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo

This paper presents LOLA, a massively multilingual large language model
trained on more than 160 languages using a sparse Mixture-of-Experts
Transformer architecture. Our architectural and implementation choices address
the challenge of harnessing linguistic diversity while maintaining efficiency
and avoiding the common pitfalls of multilinguality. Our analysis of the
evaluation results shows competitive performance in natural language generation
and understanding tasks. Additionally, we demonstrate how the learned
expert-routing mechanism exploits implicit phylogenetic linguistic patterns to
potentially alleviate the curse of multilinguality. We provide an in-depth look
at the training process, an analysis of the datasets, and a balanced
exploration of the model's strengths and limitations. As an open-source model,
LOLA promotes reproducibility and serves as a robust foundation for future
research. Our findings enable the development of compute-efficient multilingual
models with strong, scalable performance across languages.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÊèêÂá∫ LOLAÔºå‰∏ÄÁ®ÆÂ§ßË¶èÊ®°Â§öË™ûË®ÄÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºå‰ΩøÁî®Á®ÄÁñèÊ∑∑ÂêàÂ∞àÂÆ∂TransformerÊû∂ÊßãÔºåÈáùÂ∞çË∂ÖÈÅé 160 Á®ÆË™ûË®ÄÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂíåÂØ¶‰ΩúÈÅ∏ÊìáÔºåËß£Ê±∫‰∫ÜÂú®Á∂≠ÊåÅÊïàÁéáÂíåÈÅøÂÖçÂ§öË™ûË®ÄÂ∏∏Ë¶ãÈô∑Èò±ÁöÑÂêåÊôÇÔºåÂà©Áî®Ë™ûË®ÄÂ§öÊ®£ÊÄßÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÂ∞çË©ï‰º∞ÁµêÊûúÁöÑÂàÜÊûêÔºåÈ°ØÁ§∫Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁîüÊàêÂíåÁêÜËß£‰ªªÂãô‰∏≠ÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ≠∏ÁøíÂà∞ÁöÑÂ∞àÂÆ∂Ë∑ØÁî±Ê©üÂà∂ÔºåÂ¶Ç‰ΩïÂà©Áî®Èö±Âê´ÁöÑÁ≥ªÁµ±ÁôºÁîüË™ûË®ÄÊ®°ÂºèÔºå‰æÜÊΩõÂú®Ê∏õËºïÂ§öË™ûË®ÄÁöÑË©õÂíí„ÄÇÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®éË®ìÁ∑¥ÈÅéÁ®ã„ÄÅÂ∞çË≥áÊñôÈõÜÁöÑÂàÜÊûêÔºå‰ª•ÂèäÂ∞çÊ®°ÂûãÂÑ™Áº∫ÈªûÁöÑÂπ≥Ë°°Êé¢Ë®é„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºÊ®°ÂûãÔºåLOLA Êé®Âª£ÂèØË§áË£ΩÊÄßÔºå‰∏¶‰ΩúÁÇ∫Êú™‰æÜÁ†îÁ©∂ÁöÑÁ©©Âõ∫Âü∫Á§é„ÄÇÊàëÂÄëÁöÑÁôºÁèæÔºå‰ΩøÈñãÁôºÂá∫Ë∑®Ë™ûË®ÄÂÖ∑ÊúâÂº∑Â§ß„ÄÅÂèØÊì¥ÂÖÖÊÄßËÉΩÁöÑË®àÁÆóÈ´òÊïàÂ§öË™ûË®ÄÊ®°ÂûãÊàêÁÇ∫ÂèØËÉΩ„ÄÇ

##### **Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids**
2409.11267v1 by Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter

This work proposes an approach that integrates reinforcement learning and
model predictive control (MPC) to efficiently solve finite-horizon optimal
control problems in mixed-logical dynamical systems. Optimization-based control
of such systems with discrete and continuous decision variables entails the
online solution of mixed-integer quadratic or linear programs, which suffer
from the curse of dimensionality. Our approach aims at mitigating this issue by
effectively decoupling the decision on the discrete variables and the decision
on the continuous variables. Moreover, to mitigate the combinatorial growth in
the number of possible actions due to the prediction horizon, we conceive the
definition of decoupled Q-functions to make the learning problem more
tractable. The use of reinforcement learning reduces the online optimization
problem of the MPC controller from a mixed-integer linear (quadratic) program
to a linear (quadratic) program, greatly reducing the computational time.
Simulation experiments for a microgrid, based on real-world data, demonstrate
that the proposed method significantly reduces the online computation time of
the MPC approach and that it generates policies with small optimality gaps and
high feasibility rates.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÁßçÊñπÊ≥ïÔºåÊï¥ÂêàÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ (MPC)Ôºå‰ª•ÊúâÊïàËß£ÂÜ≥Ê∑∑ÂêàÈÄªËæëÂä®ÊÄÅÁ≥ªÁªü‰∏≠ÁöÑÊúâÈôêÊó∂Èó¥ÊúÄ‰ºòÊéßÂà∂ÈóÆÈ¢ò„ÄÇÊ≠§Á±ªÂÖ∑ÊúâÁ¶ªÊï£ÂíåËøûÁª≠ÂÜ≥Á≠ñÂèòÈáèÁöÑÂü∫‰∫é‰ºòÂåñÊéßÂà∂ÁöÑÁ≥ªÁªüÈúÄË¶ÅÂú®Á∫øËß£ÂÜ≥Ê∑∑ÂêàÊï¥Êï∞‰∫åÊ¨°ÊàñÁ∫øÊÄßËßÑÂàíÔºåËøô‰ºöÂèóÂà∞Áª¥Â∫¶ÁÅæÈöæÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÊó®Âú®ÈÄöËøáÊúâÊïàËß£ËÄ¶Á¶ªÊï£ÂèòÈáèÁöÑÂÜ≥Á≠ñÂíåËøûÁª≠ÂèòÈáèÁöÑÂÜ≥Á≠ñÊù•ÁºìËß£Ê≠§ÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÂáèËΩªÈ¢ÑÊµãËåÉÂõ¥ÂØºËá¥ÁöÑÂèØËÉΩÂä®‰ΩúÊï∞ÈáèÁöÑÁªÑÂêàÂ¢ûÈïøÔºåÊàë‰ª¨ÊûÑÊÉ≥‰∫ÜËß£ËÄ¶ Q ÂáΩÊï∞ÁöÑÂÆö‰πâÔºå‰ª•‰ΩøÂ≠¶‰π†ÈóÆÈ¢òÊõ¥ÂÆπÊòìÂ§ÑÁêÜ„ÄÇÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰ΩøÁî®Â∞Ü MPC ÊéßÂà∂Âô®ÁöÑÂú®Á∫ø‰ºòÂåñÈóÆÈ¢ò‰ªéÊ∑∑ÂêàÊï¥Êï∞Á∫øÊÄßÔºà‰∫åÊ¨°ÔºâËßÑÂàíÂáèÂ∞ëÂà∞Á∫øÊÄßÔºà‰∫åÊ¨°ÔºâËßÑÂàíÔºå‰ªéËÄåÂ§ßÂ§ßÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÊó∂Èó¥„ÄÇÂü∫‰∫éÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑÂæÆÁîµÁΩë‰ªøÁúüÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊòæËëóÂáèÂ∞ë‰∫Ü MPC ÊñπÊ≥ïÁöÑÂú®Á∫øËÆ°ÁÆóÊó∂Èó¥ÔºåÂπ∂‰∏îÂÆÉÁîüÊàêÁöÑÁ≠ñÁï•ÂÖ∑ÊúâËæÉÂ∞èÁöÑÊúÄ‰ºòÊÄßÂ∑ÆË∑ùÂíåËæÉÈ´òÁöÑÂèØË°åÊÄßÊØîÁéá„ÄÇ

##### **Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models**
2409.11263v1 by Jiahao Qin

This paper introduces Bio-Inspired Mamba (BIM), a novel online learning
framework for selective state space models that integrates biological learning
principles with the Mamba architecture. BIM combines Real-Time Recurrent
Learning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like local
learning rules, addressing the challenges of temporal locality and biological
plausibility in training spiking neural networks. Our approach leverages the
inherent connection between backpropagation through time and STDP, offering a
computationally efficient alternative that maintains the ability to capture
long-range dependencies. We evaluate BIM on language modeling, speech
recognition, and biomedical signal analysis tasks, demonstrating competitive
performance against traditional methods while adhering to biological learning
principles. Results show improved energy efficiency and potential for
neuromorphic hardware implementation. BIM not only advances the field of
biologically plausible machine learning but also provides insights into the
mechanisms of temporal information processing in biological neural networks.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñá‰ªãÁ¥π‰∫ÜÁîüÁâ©ÈùàÊÑüÊõºÂ∑¥ (BIM)ÔºåÈÄôÊòØ‰∏ÄÂÄãÈáùÂ∞çÈÅ∏ÊìáÊÄßÁãÄÊÖãÁ©∫ÈñìÊ®°ÂûãÁöÑÊñ∞ÂûãÁ∑ö‰∏äÂ≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÂ∞áÁîüÁâ©Â≠∏ÁøíÂéüÂâáËàáÊõºÂ∑¥Êû∂ÊßãÊï¥ÂêàÂú®‰∏ÄËµ∑„ÄÇBIM ÁµêÂêà‰∫ÜÂç≥ÊôÇÈÅûËø¥Â≠∏Áøí (RTRL) ËàáÈ°ûÂ∞ñÂ≥∞ÊôÇÂ∫è‰æùË≥¥ÂèØÂ°ëÊÄß (STDP) ÁöÑÂ±ÄÈÉ®Â≠∏ÁøíË¶èÂâáÔºåËß£Ê±∫‰∫ÜË®ìÁ∑¥ËÑàË°ùÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÊôÇÈñìÂ±ÄÈÉ®ÊÄßÂíåÁîüÁâ©ÂèØ‰ø°Â∫¶ÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®‰∫ÜÂèçÂêëÂÇ≥Êí≠Ëàá STDP ‰πãÈñìÁöÑÂÖßÂú®ÈóúËÅØÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãË®àÁÆóÊïàÁéáÈ´òÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂêåÊôÇ‰øùÊåÅ‰∫ÜÊçïÊçâÈï∑Á®ã‰æùË≥¥ÊÄßÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®Ë™ûË®ÄÂª∫Ê®°„ÄÅË™ûÈü≥Ë≠òÂà•ÂíåÁîüÁâ©ÈÜ´Â≠∏Ë®äËôüÂàÜÊûê‰ªªÂãô‰∏äË©ï‰º∞‰∫Ü BIMÔºåË≠âÊòé‰∫ÜÂÆÉÂú®ÈÅµÂæ™ÁîüÁâ©Â≠∏ÁøíÂéüÂâáÁöÑÂêåÊôÇÔºåËàáÂÇ≥Áµ±ÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇÁµêÊûúÈ°ØÁ§∫Âá∫ÊîπÂñÑÁöÑËÉΩÊ∫êÊïàÁéáÂíåÁ•ûÁ∂ìÂΩ¢ÊÖãÁ°¨È´îÂØ¶‰ΩúÁöÑÊΩõÂäõ„ÄÇBIM ‰∏çÂÉÖÊé®Âãï‰∫ÜÁîüÁâ©ÂèØ‰ø°Ê©üÂô®Â≠∏ÁøíÈ†òÂüüÁöÑÈÄ≤Ê≠•Ôºå‰πüÊèê‰æõ‰∫ÜÂ∞çÁîüÁâ©Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÊôÇÈñìË≥áË®äËôïÁêÜÊ©üÂà∂ÁöÑË¶ãËß£„ÄÇ

##### **The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection**
2409.11262v1 by Gabriel Bibb√≥, Thomas Deacon, Arshdeep Singh, Mark D. Plumbley

This paper presents a residential audio dataset to support sound event
detection research for smart home applications aimed at promoting wellbeing for
older adults. The dataset is constructed by deploying audio recording systems
in the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic
characteristics are documented through detailed floor plans and construction
material information to enable replication of the recording environments for AI
model deployment. A novel automated speech removal pipeline is developed, using
pre-trained audio neural networks to detect and remove segments containing
spoken voice, while preserving segments containing other sound events. The
resulting dataset consists of privacy-compliant audio recordings that
accurately capture the soundscapes and activities of daily living within
residential spaces. The paper details the dataset creation methodology, the
speech removal pipeline utilizing cascaded model architectures, and an analysis
of the vocal label distribution to validate the speech removal process. This
dataset enables the development and benchmarking of sound event detection
models tailored specifically for in-home applications.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÊèêÂá∫‰∏ÄÂÄã‰ΩèÂÆÖÈü≥Ë®äË≥áÊñôÈõÜÔºå‰ª•ÊîØÊè¥Êô∫ÊÖßÂÆ∂Â∫≠ÊáâÁî®‰πãÈü≥Ë®ä‰∫ã‰ª∂ÂÅµÊ∏¨Á†îÁ©∂ÔºåÊó®Âú®‰øÉÈÄ≤ËÄÅÂπ¥‰∫∫ÁöÑÁ¶èÁ•â„ÄÇÊ≠§Ë≥áÊñôÈõÜ‰øÇÈÄèÈÅéÂú® 8 ‰Ωç 55 Ëá≥ 80 Ê≠≤ÂèÉËàáËÄÖÁöÑÂÆ∂‰∏≠ÈÉ®ÁΩ≤Èü≥Ë®äÈåÑË£ΩÁ≥ªÁµ±ÔºåÁÇ∫Êúü 7 Â§©ËÄåÂª∫ÁΩÆ„ÄÇÈü≥ÈüøÁâπÊÄßÈÄèÈÅéË©≥Á¥∞ÁöÑÂπ≥Èù¢ÂúñÂíåÂª∫ÊùêË≥áË®äÈÄ≤Ë°åË®òÈåÑÔºå‰ª•Âà©Ë§áË£ΩÈåÑÈü≥Áí∞Â¢É‰ª•ÈÄ≤Ë°å AI Ê®°ÂûãÈÉ®ÁΩ≤„ÄÇ‰∏¶ÈñãÁôºÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑËá™ÂãïÂåñË™ûÈü≥ÁßªÈô§ÁÆ°Á∑öÔºåÂà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÈü≥Ë®äÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂÅµÊ∏¨‰∏¶ÁßªÈô§ÂåÖÂê´Ë™ûÈü≥ÁöÑÁâáÊÆµÔºåÂêåÊôÇ‰øùÁïôÂåÖÂê´ÂÖ∂‰ªñÈü≥Ë®ä‰∫ã‰ª∂ÁöÑÁâáÊÆµ„ÄÇÊâÄÂæóË≥áÊñôÈõÜÂåÖÂê´Á¨¶ÂêàÈö±ÁßÅË¶èÁØÑÁöÑÈü≥Ë®äÈåÑÈü≥ÔºåÂèØÁ≤æÁ¢∫Êì∑Âèñ‰ΩèÂÆÖÁ©∫ÈñìÂÖßÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑÈü≥ÊôØÂíåÊ¥ªÂãï„ÄÇÊú¨ÊñáË©≥Ëø∞Ë≥áÊñôÈõÜÂª∫Á´ãÊñπÊ≥ï„ÄÅÂà©Áî®‰∏≤ËÅØÊ®°ÂûãÊû∂ÊßãÁöÑË™ûÈü≥ÁßªÈô§ÁÆ°Á∑öÔºå‰ª•ÂèäÂ∞çË™ûÈü≥Ê®ôÁ±§ÂàÜ‰ΩàÁöÑÂàÜÊûêÔºå‰ª•È©óË≠âË™ûÈü≥ÁßªÈô§ÊµÅÁ®ã„ÄÇÊ≠§Ë≥áÊñôÈõÜÂèØÂçîÂä©ÈñãÁôºÂíåË©ïÈáèÂ∞àÈñÄÈáùÂ∞çÂ±ÖÂÆ∂ÊáâÁî®ÊâÄÈáèË∫´ÊâìÈÄ†ÁöÑÈü≥Ë®ä‰∫ã‰ª∂ÂÅµÊ∏¨Ê®°Âûã„ÄÇ

##### **The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives**
2409.11261v2 by Samee Arif, Taimoor Arif, Aamina Jamal Khan, Muhammad Saad Haroon, Agha Ali Raza, Awais Athar

This paper introduces the concept of an education tool that utilizes
Generative Artificial Intelligence (GenAI) to enhance storytelling for
children. The system combines GenAI-driven narrative co-creation,
text-to-speech conversion, and text-to-video generation to produce an engaging
experience for learners. We describe the co-creation process, the adaptation of
narratives into spoken words using text-to-speech models, and the
transformation of these narratives into contextually relevant visuals through
text-to-video technology. Our evaluation covers the linguistics of the
generated stories, the text-to-speech conversion quality, and the accuracy of
the generated visuals.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊïôËÇ≤Â∑•ÂÖ∑ÁöÑÊ¶ÇÂøµÔºåË©≤Â∑•ÂÖ∑Âà©Áî®ÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖß (GenAI) ‰æÜÂ¢ûÂº∑ÂÖíÁ´•ÁöÑÊïÖ‰∫ãÊïòËø∞„ÄÇÊ≠§Á≥ªÁµ±ÁµêÂêà‰∫ÜÁî± GenAI È©ÖÂãïÁöÑÊïò‰∫ãÂÖ±ÂêåÂâµ‰Ωú„ÄÅÊñáÂ≠óËΩâË™ûÈü≥ËΩâÊèõ‰ª•ÂèäÊñáÂ≠óËΩâÂΩ±ÁâáÁîüÊàêÔºåÁÇ∫Â≠∏ÁøíËÄÖÂ∏∂‰æÜÂºï‰∫∫ÂÖ•ÂãùÁöÑÈ´îÈ©ó„ÄÇÊàëÂÄëÊèèËø∞‰∫ÜÂÖ±ÂêåÂâµ‰ΩúÁöÑÈÅéÁ®ã„ÄÅ‰ΩøÁî®ÊñáÂ≠óËΩâË™ûÈü≥Ê®°ÂûãÂ∞áÊïò‰∫ãÊîπÁ∑®ÁÇ∫ÊúâËÅ≤Êõ∏Ôºå‰ª•ÂèäÈÄèÈÅéÊñáÂ≠óËΩâÂΩ±ÁâáÊäÄË°ìÂ∞áÈÄô‰∫õÊïò‰∫ãËΩâÊèõÁÇ∫ËàáËÑàÁµ°Áõ∏ÈóúÁöÑË¶ñË¶∫ÊïàÊûú„ÄÇÊàëÂÄëÁöÑË©ïÈáèÊ∂µËìã‰∫ÜÁîüÊàêÊïÖ‰∫ãÁöÑË™ûË®ÄÂ≠∏„ÄÅÊñáÂ≠óËΩâË™ûÈü≥ËΩâÊèõÂìÅË≥™‰ª•ÂèäÁîüÊàêË¶ñË¶∫ÊïàÊûúÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇ

##### **Attacking Slicing Network via Side-channel Reinforcement Learning Attack**
2409.11258v1 by Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe

Network slicing in 5G and the future 6G networks will enable the creation of
multiple virtualized networks on a shared physical infrastructure. This
innovative approach enables the provision of tailored networks to accommodate
specific business types or industry users, thus delivering more customized and
efficient services. However, the shared memory and cache in network slicing
introduce security vulnerabilities that have yet to be fully addressed. In this
paper, we introduce a reinforcement learning-based side-channel cache attack
framework specifically designed for network slicing environments. Unlike
traditional cache attack methods, our framework leverages reinforcement
learning to dynamically identify and exploit cache locations storing sensitive
information, such as authentication keys and user registration data. We assume
that one slice network is compromised and demonstrate how the attacker can
induce another shared slice to send registration requests, thereby estimating
the cache locations of critical data. By formulating the cache timing channel
attack as a reinforcement learning-driven guessing game between the attack
slice and the victim slice, our model efficiently explores possible actions to
pinpoint memory blocks containing sensitive information. Experimental results
showcase the superiority of our approach, achieving a success rate of
approximately 95\% to 98\% in accurately identifying the storage locations of
sensitive data. This high level of accuracy underscores the potential risks in
shared network slicing environments and highlights the need for robust security
measures to safeguard against such advanced side-channel attacks.

ÊëòË¶ÅÔºö5G ÂíåÊú™Êù•ÁöÑ 6G ÁΩëÁªú‰∏≠ÁöÑÁΩëÁªúÂàáÁâáÂ∞ÜËÉΩÂ§üÂú®ÂÖ±‰∫´ÁöÑÁâ©ÁêÜÂü∫Á°ÄËÆæÊñΩ‰∏äÂàõÂª∫Â§ö‰∏™ËôöÊãüÂåñÁΩëÁªú„ÄÇËøôÁßçÂàõÊñ∞ÊñπÊ≥ïËÉΩÂ§üÊèê‰æõÂÆöÂà∂ÂåñÁΩëÁªúÔºå‰ª•ÈÄÇÂ∫îÁâπÂÆöÁöÑ‰∏öÂä°Á±ªÂûãÊàñË°å‰∏öÁî®Êà∑Ôºå‰ªéËÄåÊèê‰æõÊõ¥Â§öÂÆöÂà∂ÂåñÂíåÈ´òÊïàÁöÑÊúçÂä°„ÄÇÁÑ∂ËÄåÔºåÁΩëÁªúÂàáÁâá‰∏≠ÁöÑÂÖ±‰∫´ÂÜÖÂ≠òÂíåÁºìÂ≠òÂºïÂÖ•‰∫ÜÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜËß£ÂÜ≥ÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊóÅË∑ØÁºìÂ≠òÊîªÂáªÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂‰∏ìÈó®ËÆæËÆ°Áî®‰∫éÁΩëÁªúÂàáÁâáÁéØÂ¢É„ÄÇ‰∏é‰º†ÁªüÁöÑÁºìÂ≠òÊîªÂáªÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•Âä®ÊÄÅËØÜÂà´ÂíåÂà©Áî®Â≠òÂÇ®ÊïèÊÑü‰ø°ÊÅØÁöÑÁºìÂ≠ò‰ΩçÁΩÆÔºå‰æãÂ¶ÇË∫´‰ªΩÈ™åËØÅÂØÜÈí•ÂíåÁî®Êà∑Ê≥®ÂÜåÊï∞ÊçÆ„ÄÇÊàë‰ª¨ÂÅáËÆæ‰∏Ä‰∏™ÂàáÁâáÁΩëÁªúÈÅ≠Âà∞Á†¥ÂùèÔºåÂπ∂ÊºîÁ§∫ÊîªÂáªËÄÖÂ¶Ç‰ΩïËØ±‰ΩøÂè¶‰∏Ä‰∏™ÂÖ±‰∫´ÂàáÁâáÂèëÈÄÅÊ≥®ÂÜåËØ∑Ê±ÇÔºå‰ªéËÄå‰º∞ËÆ°ÂÖ≥ÈîÆÊï∞ÊçÆÁöÑÁºìÂ≠ò‰ΩçÁΩÆ„ÄÇÈÄöËøáÂ∞ÜÁºìÂ≠òÊó∂Â∫è‰ø°ÈÅìÊîªÂáªË°®Ëø∞‰∏∫ÊîªÂáªÂàáÁâáÂíåÂèóÂÆ≥ÂàáÁâá‰πãÈó¥ÁöÑÂº∫ÂåñÂ≠¶‰π†È©±Âä®ÁöÑÁåúË∞úÊ∏∏ÊàèÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÊúâÊïàÂú∞Êé¢Á¥¢‰∫ÜÂèØËÉΩÁöÑÂä®‰ΩúÔºå‰ª•Á≤æÁ°ÆÂÆö‰ΩçÂåÖÂê´ÊïèÊÑü‰ø°ÊÅØÁöÑÂÜÖÂ≠òÂùó„ÄÇÂÆûÈ™åÁªìÊûúÂ±ïÁ§∫‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑ‰ºòË∂äÊÄßÔºåÂú®ÂáÜÁ°ÆËØÜÂà´ÊïèÊÑüÊï∞ÊçÆÁöÑÂ≠òÂÇ®‰ΩçÁΩÆÊñπÈù¢ÂÆûÁé∞‰∫ÜÂ§ßÁ∫¶ 95% Âà∞ 98% ÁöÑÊàêÂäüÁéá„ÄÇËøôÁßçÈ´òÂáÜÁ°ÆÊÄßÂº∫Ë∞É‰∫ÜÂÖ±‰∫´ÁΩëÁªúÂàáÁâáÁéØÂ¢É‰∏≠ÁöÑÊΩúÂú®È£éÈô©ÔºåÂπ∂Âá∏Êòæ‰∫ÜÂØπÁ®≥ÂÅ•ÁöÑÂÆâÂÖ®Êé™ÊñΩÁöÑÈúÄÊ±ÇÔºå‰ª•Èò≤ËåÉÊ≠§Á±ªÈ´òÁ∫ßÊóÅË∑ØÊîªÂáª„ÄÇ

##### **Norm of Mean Contextualized Embeddings Determines their Variance**
2409.11253v1 by Hiroaki Yamagiwa, Hidetoshi Shimodaira

Contextualized embeddings vary by context, even for the same token, and form
a distribution in the embedding space. To analyze this distribution, we focus
on the norm of the mean embedding and the variance of the embeddings. In this
study, we first demonstrate that these values follow the well-known formula for
variance in statistics and provide an efficient sequential computation method.
Then, by observing embeddings from intermediate layers of several Transformer
models, we found a strong trade-off relationship between the norm and the
variance: as the mean embedding becomes closer to the origin, the variance
increases. This trade-off is likely influenced by the layer normalization
mechanism used in Transformer models. Furthermore, when the sets of token
embeddings are treated as clusters, we show that the variance of the entire
embedding set can theoretically be decomposed into the within-cluster variance
and the between-cluster variance. We found experimentally that as the layers of
Transformer models deepen, the embeddings move farther from the origin, the
between-cluster variance relatively decreases, and the within-cluster variance
relatively increases. These results are consistent with existing studies on the
anisotropy of the embedding spaces across layers.

ÊëòË¶ÅÔºö‰∏ä‰∏ãÊñáÂµåÂÖ•ÊúÉÂõ†‰∏ä‰∏ãÊñáËÄåÁï∞ÔºåÂç≥‰ΩøÊòØÂêå‰∏ÄÂÄãÊ®ôË®òÔºå‰πüÊúÉÂú®ÂµåÂÖ•Á©∫Èñì‰∏≠ÂΩ¢Êàê‰∏ÄÂÄãÂàÜ‰Ωà„ÄÇÁÇ∫‰∫ÜÂàÜÊûêÊ≠§ÂàÜ‰ΩàÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂπ≥ÂùáÂµåÂÖ•ÁöÑÁØÑÊï∏ÂíåÂµåÂÖ•ÁöÑËÆäÁï∞Êï∏„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàË≠âÊòéÈÄô‰∫õÂÄºÈÅµÂæ™Áµ±Ë®àÂ≠∏‰∏≠ÁúæÊâÄÂë®Áü•ÁöÑËÆäÁï∞Êï∏ÂÖ¨ÂºèÔºå‰∏¶Êèê‰æõ‰∏ÄÁ®ÆÊúâÊïàÁöÑÈ†ÜÂ∫èË®àÁÆóÊñπÊ≥ï„ÄÇÁÑ∂ÂæåÔºåÈÄöÈÅéËßÄÂØüÂπæÂÄã Transformer Ê®°ÂûãÁöÑ‰∏≠ÈñìÂ±§ÂµåÂÖ•ÔºåÊàëÂÄëÁôºÁèæÁØÑÊï∏ÂíåËÆäÁï∞Êï∏‰πãÈñìÂ≠òÂú®Âº∑ÁÉàÁöÑÊäòË°∑Èóú‰øÇÔºöÈö®ËëóÂπ≥ÂùáÂµåÂÖ•Ë∂ä‰æÜË∂äÊé•ËøëÂéüÈªûÔºåËÆäÁï∞Êï∏Â∞±ÊúÉÂ¢ûÂä†„ÄÇÊ≠§ÊäòË°∑ÂèØËÉΩÂèó Transformer Ê®°Âûã‰∏≠‰ΩøÁî®ÁöÑÂ±§Ê≠£Ë¶èÂåñÊ©üÂà∂ÂΩ±Èüø„ÄÇÊ≠§Â§ñÔºåÁï∂Â∞áÊ®ôË®òÂµåÂÖ•ÈõÜÂêàË¶ñÁÇ∫Âè¢ÈõÜÊôÇÔºåÊàëÂÄëË≠âÊòéÊï¥ÂÄãÂµåÂÖ•ÈõÜÂêàÁöÑËÆäÁï∞Êï∏Âú®ÁêÜË´ñ‰∏äÂèØ‰ª•ÂàÜËß£ÁÇ∫Âè¢ÈõÜÂÖßËÆäÁï∞Êï∏ÂíåÂè¢ÈõÜÈñìËÆäÁï∞Êï∏„ÄÇÊàëÂÄëÈÄèÈÅéÂØ¶È©óÁôºÁèæÔºåÈö®Ëëó Transformer Ê®°ÂûãÁöÑÂ±§Êï∏Âä†Ê∑±ÔºåÂµåÂÖ•ÊúÉÈÅ†Èõ¢ÂéüÈªûÔºåÂè¢ÈõÜÈñìËÆäÁï∞Êï∏Áõ∏Â∞çÊ∏õÂ∞ëÔºåËÄåÂè¢ÈõÜÂÖßËÆäÁï∞Êï∏Áõ∏Â∞çÂ¢ûÂä†„ÄÇÈÄô‰∫õÁµêÊûúËàáÁèæÊúâÈóúÊñºË∑®Â±§ÂµåÂÖ•Á©∫ÈñìÂêÑÂêëÁï∞ÊÄßÁöÑÁ†îÁ©∂‰∏ÄËá¥„ÄÇ

##### **WER We Stand: Benchmarking Urdu ASR Models**
2409.11252v1 by Samee Arif, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar

This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÁÉèÁàæÈÉΩË™ûËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) Ê®°ÂûãË©ï‰º∞„ÄÇÊàëÂÄëÂàÜÊûê‰∫Ü‰∏âÂÄã ASR Ê®°ÂûãÂÆ∂ÊóèÁöÑÊïàËÉΩÔºöWhisper„ÄÅMMS Âíå Seamless-M4TÔºå‰ΩøÁî®Ë©ûË™ûÈåØË™§Áéá (WER)Ôºå‰ª•ÂèäÂ∞çÊúÄÂ∏∏ÈåØÁöÑË©ûË™ûÂíåÈåØË™§È°ûÂûãÈÄ≤Ë°åË©≥Á¥∞Ê™¢Êü•ÔºåÂåÖÊã¨ÊèíÂÖ•„ÄÅÂà™Èô§ÂíåÊõøÊèõ„ÄÇÊàëÂÄëÁöÑÂàÜÊûê‰ΩøÁî®ÂÖ©Á®ÆÈ°ûÂûãÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÔºöÊúóËÆÄË™ûÈü≥ÂíåÂ∞çË©±Ë™ûÈü≥„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÁ¨¨‰∏ÄÂÄãÈáùÂ∞çÁÉèÁàæÈÉΩË™û ASR Ê®°ÂûãÂü∫Ê∫ñÊ∏¨Ë©¶ËÄåË®≠Ë®àÁöÑÂ∞çË©±Ë™ûÈü≥Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁôºÁèæ seamless-large Âú®ÊúóËÆÄË™ûÈü≥Ë≥áÊñôÈõÜ‰∏äÁöÑË°®ÁèæÂÑ™ÊñºÂÖ∂‰ªñ ASR Ê®°ÂûãÔºåËÄå whisper-large Âú®Â∞çË©±Ë™ûÈü≥Ë≥áÊñôÈõÜ‰∏äÁöÑË°®ÁèæÊúÄ‰Ω≥„ÄÇÊ≠§Â§ñÔºåÊ≠§Ë©ï‰º∞Âº∑Ë™ø‰∫ÜÂÉÖ‰ΩøÁî®ÈáèÂåñÊåáÊ®ôË©ï‰º∞ÁÉèÁàæÈÉΩË™ûÁ≠â‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑ ASR Ê®°ÂûãÁöÑË§áÈõúÊÄßÔºå‰∏¶Âº∑Ë™ø‰∫ÜÂ∞çÂº∑ÂÅ•ÁöÑÁÉèÁàæÈÉΩË™ûÊñáÂ≠óÊ≠£Ë¶èÂåñÁ≥ªÁµ±ÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁôºÁèæÁÇ∫ÈñãÁôºÈáùÂ∞çÁÉèÁàæÈÉΩË™ûÁ≠â‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑÂº∑ÂÅ• ASR Á≥ªÁµ±Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇ

##### **Linear Recency Bias During Training Improves Transformers' Fit to Reading Times**
2409.11250v1 by Christian Clark, Byung-Doh Oh, William Schuler

Recent psycholinguistic research has compared human reading times to
surprisal estimates from language models to study the factors shaping human
sentence processing difficulty. Previous studies have shown a strong fit
between surprisal values from Transformers and reading times. However, standard
Transformers work with a lossless representation of the entire previous
linguistic context, unlike models of human language processing that include
memory decay. To bridge this gap, this paper evaluates a modification of the
Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to
attention scores. Surprisal estimates with ALiBi show an improved fit to human
reading times compared to a standard Transformer baseline. A subsequent
analysis of attention heads suggests that ALiBi's mixture of slopes -- which
determine the rate of memory decay in each attention head -- may play a role in
the improvement by helping models with ALiBi to track different kinds of
linguistic dependencies.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÂøÉÁêÜËØ≠Ë®ÄÂ≠¶Á†îÁ©∂Â∞Ü‰∫∫Á±ªÈòÖËØªÊó∂Èó¥‰∏éËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÉäÂ•á‰º∞ËÆ°ÂÄºËøõË°åÊØîËæÉÔºå‰ª•Á†îÁ©∂ÂΩ±Âìç‰∫∫Á±ªÂè•Â≠êÂ§ÑÁêÜÈöæÂ∫¶ÁöÑÂõ†Á¥†„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊù•Ëá™ Transformer ÁöÑÊÉäÂ•áÂÄº‰∏éÈòÖËØªÊó∂Èó¥‰πãÈó¥ÊúâÂæàÂº∫ÁöÑÂ•ëÂêàÂ∫¶„ÄÇÁÑ∂ËÄåÔºåÊ†áÂáÜ Transformer ‰ΩøÁî®Êï¥‰∏™ÂÖàÂâçËØ≠Ë®Ä‰∏ä‰∏ãÊñáÁöÑÊó†ÊçüË°®Á§∫ÔºåËøô‰∏éÂåÖÂê´ËÆ∞ÂøÜË°∞ÂáèÁöÑ‰∫∫Á±ªËØ≠Ë®ÄÂ§ÑÁêÜÊ®°Âûã‰∏çÂêå„ÄÇ‰∏∫‰∫ÜÂº•ÂêàËøô‰∏ÄÂ∑ÆË∑ùÔºåÊú¨ÊñáËØÑ‰º∞‰∫Ü Transformer Ê®°ÂûãÁöÑ‰øÆÊîπÔºåËØ•Ê®°Âûã‰ΩøÁî® ALiBiÔºàPress Á≠â‰∫∫Ôºå2022 Âπ¥ÔºâÔºå‰∏ÄÁßçÊ∑ªÂä†Âà∞Ê≥®ÊÑèÂäõÂàÜÊï∞ÁöÑËøëÊúüÂÅèÂ∑Æ„ÄÇ‰∏éÊ†áÂáÜ Transformer Âü∫Á∫øÁõ∏ÊØîÔºå‰ΩøÁî® ALiBi ÁöÑÊÉäÂ•á‰º∞ËÆ°ÊòæÁ§∫Âá∫‰∏é‰∫∫Á±ªÈòÖËØªÊó∂Èó¥Êõ¥Â•ΩÁöÑÊãüÂêà„ÄÇÂØπÊ≥®ÊÑèÂäõÂ§¥ÁöÑÂêéÁª≠ÂàÜÊûêË°®ÊòéÔºåALiBi ÁöÑÊñúÁéáÊ∑∑Âêà‚Äî‚ÄîÂÆÉÂÜ≥ÂÆö‰∫ÜÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥‰∏≠ËÆ∞ÂøÜË°∞ÂáèÁöÑÈÄüÁéá‚Äî‚ÄîÂèØËÉΩÈÄöËøáÂ∏ÆÂä©Â∏¶Êúâ ALiBi ÁöÑÊ®°ÂûãË∑üË∏™‰∏çÂêåÁ±ªÂûãÁöÑËØ≠Ë®Ä‰æùËµñÂÖ≥Á≥ªËÄåÂú®ÊîπËøõ‰∏≠ÂèëÊå•‰ΩúÁî®„ÄÇ

##### **Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse**
2409.11242v1 by Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria

LLMs are an integral part of retrieval-augmented generation (RAG) systems.
While many studies focus on evaluating the quality of end-to-end RAG systems,
there is a lack of research on understanding the appropriateness of an LLM for
the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a
holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show
that various prompting methods, such as in-context learning, fail to adapt LLMs
effectively to the RAG task. Thus, we propose Trust-Align, a framework to align
LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly
outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up
29.2) and ELI5 (up 14.9). We release our code at:
https://github.com/declare-lab/trust-align.

ÊëòË¶ÅÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÊòØÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Á≥ªÁªü‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ
ËôΩÁÑ∂ËÆ∏Â§öÁ†îÁ©∂‰∏ìÊ≥®‰∫éËØÑ‰º∞Á´ØÂà∞Á´Ø RAG Á≥ªÁªüÁöÑË¥®ÈáèÔºå
‰ΩÜÂØπ‰∫éÁêÜËß£ LLM ÂØπ RAG ‰ªªÂä°ÁöÑÈÄÇÁî®ÊÄßÂç¥Áº∫‰πèÁ†îÁ©∂„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÊåáÊ†á Trust-ScoreÔºåËØ•ÊåáÊ†áÊèê‰æõ‰∫ÜÂØπ RAG Ê°ÜÊû∂‰∏≠ LLM ÂèØ‰ø°Â∫¶ÁöÑÂÖ®Èù¢ËØÑ‰º∞„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÂêÑÁßçÊèêÁ§∫ÊñπÊ≥ïÔºà‰æãÂ¶Ç‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºâÈÉΩÊó†Ê≥ïÊúâÊïàÂú∞‰Ωø LLM ÈÄÇÂ∫î RAG ‰ªªÂä°„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Trust-AlignÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éË∞ÉÊï¥ LLM ‰ª•Ëé∑ÂæóÊõ¥È´ò Trust-Score ÁöÑÊ°ÜÊû∂„ÄÇ‰∏éÊàë‰ª¨ÁöÑÊñπÊ≥ïÁõ∏‰∏ÄËá¥ÁöÑ LLaMA-3-8b Âú® ASQAÔºàÊèêÂçá 10.7Ôºâ„ÄÅQAMPARIÔºàÊèêÂçá 29.2ÔºâÂíå ELI5ÔºàÊèêÂçá 14.9Ôºâ‰∏äÊòéÊòæ‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÁöÑÂºÄÊ∫ê LLM„ÄÇÊàë‰ª¨Âú®‰ª•‰∏ã‰ΩçÁΩÆÂèëÂ∏ÉÊàë‰ª¨ÁöÑ‰ª£Á†ÅÔºö
https://github.com/declare-lab/trust-align„ÄÇ

##### **Spontaneous Informal Speech Dataset for Punctuation Restoration**
2409.11241v1 by Xing Yi Liu, Homayoon Beigi

Presently, punctuation restoration models are evaluated almost solely on
well-structured, scripted corpora. On the other hand, real-world ASR systems
and post-processing pipelines typically apply towards spontaneous speech with
significant irregularities, stutters, and deviations from perfect grammar. To
address this discrepancy, we introduce SponSpeech, a punctuation restoration
dataset derived from informal speech sources, which includes punctuation and
casing information. In addition to publicly releasing the dataset, we
contribute a filtering pipeline that can be used to generate more data. Our
filtering pipeline examines the quality of both speech audio and transcription
text. We also carefully construct a ``challenging" test set, aimed at
evaluating models' ability to leverage audio information to predict otherwise
grammatically ambiguous punctuation. SponSpeech is available at
https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset
building and model runs.

ÊëòË¶ÅÔºöÁõÆÂâçÔºåÊ®ôÈªûÁ¨¶ËôüÈÇÑÂéüÊ®°ÂûãÂπæ‰πéÂè™ÈáùÂ∞çÁµêÊßãËâØÂ•ΩÁöÑËÖ≥Êú¨Ë™ûÊñôÂ∫´ÈÄ≤Ë°åË©ï‰º∞„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÁèæÂØ¶‰∏ñÁïåÁöÑ ASR Á≥ªÁµ±ÂíåÂæåËôïÁêÜÁÆ°ÈÅìÈÄöÂ∏∏ÈÅ©Áî®ÊñºËá™ÁôºÊÄßË™ûÈü≥ÔºåÂÖ∂‰∏≠Â≠òÂú®È°ØËëóÁöÑ‰∏çË¶èÂâáÊÄß„ÄÅÂè£ÂêÉÂíåË™ûÊ≥ïÁº∫Èô∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÁ®ÆÂ∑ÆÁï∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü SponSpeechÔºåÈÄôÊòØ‰∏ÄÂÄãÂæûÈùûÊ≠£ÂºèË™ûÈü≥‰æÜÊ∫êÊ¥æÁîüÁöÑÊ®ôÈªûÁ¨¶ËôüÈÇÑÂéüË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Ê®ôÈªûÁ¨¶ËôüÂíåÂ§ßÂ∞èÂØ´Ë≥áË®ä„ÄÇÈô§‰∫ÜÂÖ¨ÈñãÁôº‰ΩàË≥áÊñôÈõÜÂ§ñÔºåÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈÅéÊøæÁÆ°ÈÅìÔºåÂèØÁî®ÊñºÁîüÊàêÊõ¥Â§öË≥áÊñô„ÄÇÊàëÂÄëÁöÑÈÅéÊøæÁÆ°ÈÅìÊ™¢Êü•Ë™ûÈü≥Èü≥Ë®äÂíåËΩâÈåÑÊñáÂ≠óÁöÑÂìÅË≥™„ÄÇÊàëÂÄëÈÇÑ‰ªîÁ¥∞ÊßãÂª∫‰∫Ü‰∏ÄÂÄã„ÄåÂÖ∑ÊåëÊà∞ÊÄß„ÄçÁöÑÊ∏¨Ë©¶ÈõÜÔºåÊó®Âú®Ë©ï‰º∞Ê®°ÂûãÂà©Áî®Èü≥Ë®äË≥áË®ä‰æÜÈ†êÊ∏¨Âú®Ë™ûÊ≥ï‰∏äÊ®°Á®úÂÖ©ÂèØÁöÑÊ®ôÈªûÁ¨¶ËôüÁöÑËÉΩÂäõ„ÄÇSponSpeech ÂèØÂú® https://github.com/GitHubAccountAnonymous/PR ÂèñÂæóÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊâÄÊúâÁî®ÊñºË≥áÊñôÈõÜÂª∫ÁΩÆÂíåÊ®°ÂûãÈÅãË°åÁöÑÁ®ãÂºèÁ¢º„ÄÇ

##### **LLM-as-a-Judge & Reward Model: What They Can and Cannot Do**
2409.11239v1 by Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong

LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
outside of English remains largely unexplored. In this paper, we conduct a
comprehensive analysis on automated evaluators, reporting key findings on their
behavior in a non-English environment. First, we discover that English
evaluation capabilities significantly influence language-specific capabilities,
often more than the language proficiency itself, enabling evaluators trained in
English to easily transfer their skills to other languages. Second, we identify
critical shortcomings, where LLMs fail to detect and penalize errors, such as
factual inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we release Kudge, the first non-English meta-evaluation
dataset containing 5,012 human annotations in Korean.

ÊëòË¶ÅÔºöLLM Âç≥Ê≥ïÂÆòÂíåÁçéÂãµÊ®°ÂûãÊòØÂª£Ê≥õÁî®ÊñºÂ§öÈÅ∏È°åÊàñ‰∫∫È°ûË®ªÈáãËÄÖÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë©ï‰º∞ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂÆÉÂÄëÂú®Ë©ï‰º∞Èï∑ÁØáÂõûÊáâÊñπÈù¢ÁôºÊèÆËëóÈ°ØËëó‰ΩúÁî®Ôºå‰ΩúÁÇ∫ÊéíË°åÊ¶úÁöÑË©ï‰º∞ËÄÖÂíåÈÄöÈÅéÂº∑ÂåñÂ≠∏ÁøíË™øÊï¥ LLM ÁöÑ‰ª£ÁêÜÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°ÂÆÉÂÄëÂæàÂèóÊ≠°ËøéÔºå‰ΩÜÂÆÉÂÄëÂú®Ëã±Ë™û‰ª•Â§ñÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞çËá™ÂãïÂåñË©ï‰º∞Âô®ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÂàÜÊûêÔºåÂ†±Âëä‰∫ÜÂÆÉÂÄëÂú®ÈùûËã±Ë™ûÁí∞Â¢É‰∏≠ÁöÑË°åÁÇ∫ÁöÑÈóúÈçµÁôºÁèæ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÁôºÁèæËã±Ë™ûË©ï‰º∞ËÉΩÂäõÈ°ØËëóÂΩ±ÈüøÁâπÂÆöË™ûË®ÄÁöÑËÉΩÂäõÔºåÈÄöÂ∏∏ÊØîË™ûË®ÄËÉΩÂäõÊú¨Ë∫´ÂΩ±ÈüøÊõ¥Â§ßÔºåÈÄô‰ΩøÂæóÊé•ÂèóÈÅéËã±Ë™ûÂüπË®ìÁöÑË©ï‰º∞ËÄÖËÉΩÂ§†ËºïÈ¨ÜÂú∞Â∞á‰ªñÂÄëÁöÑÊäÄËÉΩËΩâÁßªÂà∞ÂÖ∂‰ªñË™ûË®Ä„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂö¥ÈáçÁöÑÁº∫ÈªûÔºåLLM ÁÑ°Ê≥ïÊ™¢Ê∏¨ÂíåÊá≤ÁΩ∞ÈåØË™§Ôºå‰æãÂ¶Ç‰∫ãÂØ¶‰∏çÂáÜÁ¢∫„ÄÅÊñáÂåñË™§Ëß£ÂíåÂ≠òÂú®‰∏çÈúÄË¶ÅÁöÑË™ûË®Ä„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁôºÂ∏É‰∫Ü KudgeÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈùûËã±Ë™ûÂÖÉË©ï‰º∞Êï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 5,012 ÂÄãÈüìË™û‰∫∫È°ûË®ªÈáã„ÄÇ

##### **Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models**
2409.11233v1 by Bishwash Khanal, Jeffery M. Capone

Large language models (LLMs) offer powerful capabilities but incur
substantial computational costs, driving the need for efficient compression
techniques. This study evaluates the impact of popular compression methods -
Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on
the trade-offs between model size reduction, downstream task performance, and
the role of calibration data. Our findings reveal that while SparseGPT and
Wanda preserve perplexity even at 50% sparsity, they suffer significant
degradation on downstream tasks, highlighting the inadequacy of perplexity as
the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)
Divergence as a more comprehensive metric that captures nuanced changes in
model behavior post-compression. We further demonstrate that task-specific
calibration data significantly enhances the downstream performance of
compressed models compared to general calibration data. This research
underscores the necessity for diverse evaluation metrics and careful
calibration data selection to fully understand the complexities of LLM
compression and its implications for practical applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõÂº∑Â§ßÁöÑÂäüËÉΩÔºå‰ΩÜÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑË®àÁÆóÊàêÊú¨ÔºåÂõ†Ê≠§ÈúÄË¶ÅÈ´òÊïàÁöÑÂ£ìÁ∏ÆÊäÄË°ì„ÄÇÊú¨Á†îÁ©∂Ë©ï‰º∞‰∫ÜÊµÅË°åÁöÑÂ£ìÁ∏ÆÊñπÊ≥ïÂ∞ç LLaMA-2-7B Ê®°ÂûãÁöÑÂΩ±ÈüøÔºåÂåÖÊã¨ÂπÖÂ∫¶Ââ™Êûù„ÄÅSparseGPT Âíå WandaÔºåÈáçÈªûÂú®ÊñºÊ®°ÂûãÂ§ßÂ∞èÁ∏ÆÊ∏õ„ÄÅ‰∏ãÊ∏∏‰ªªÂãôÊïàËÉΩÂíåÊ†°Ê≠£Ë≥áÊñôÁöÑËßíËâ≤‰πãÈñìÁöÑÊ¨äË°°„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ° SparseGPT Âíå Wanda Âç≥‰ΩøÂú® 50% ÁöÑÁ®ÄÁñèÂ∫¶‰∏ã‰πüËÉΩÁ∂≠ÊåÅÂõ∞ÊÉëÂ∫¶Ôºå‰ΩÜÂÆÉÂÄëÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÊúÉÈ°ØËëóÊÉ°ÂåñÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂõ∞ÊÉëÂ∫¶‰ΩúÁÇ∫ÂîØ‰∏ÄË©ï‰º∞ÊåáÊ®ôÁöÑ‰∏çË∂≥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Jensen-Shannon (JS) Ë∑ùÈõ¢‰ΩúÁÇ∫‰∏ÄÂÄãÊõ¥ÂÖ®Èù¢ÁöÑÊåáÊ®ôÔºåÂÆÉËÉΩÊçïÊçâÊ®°ÂûãË°åÁÇ∫Âú®Â£ìÁ∏ÆÂæåÁôºÁîüÁöÑÁ¥∞ÂæÆËÆäÂåñ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë≠âÊòéÔºåËàá‰∏ÄËà¨Ê†°Ê≠£Ë≥áÊñôÁõ∏ÊØîÔºåÁâπÂÆöÊñº‰ªªÂãôÁöÑÊ†°Ê≠£Ë≥áÊñôÂèØ‰ª•È°ØËëóÊèêÂçáÂ£ìÁ∏ÆÊ®°ÂûãÁöÑ‰∏ãÊ∏∏ÊïàËÉΩ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™ø‰∫Ü‰ΩøÁî®Â§öÂÖÉË©ï‰º∞ÊåáÊ®ôÂíå‰ªîÁ¥∞ÈÅ∏ÊìáÊ†°Ê≠£Ë≥áÊñôÁöÑÂøÖË¶ÅÊÄßÔºåÊâçËÉΩÂÖÖÂàÜ‰∫ÜËß£ LLM Â£ìÁ∏ÆÁöÑË§áÈõúÊÄßÂèäÂÖ∂Â∞çÂØ¶ÈöõÊáâÁî®Á®ãÂºèÈÄ†ÊàêÁöÑÂΩ±Èüø„ÄÇ

##### **Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?**
2409.11232v1 by Raffaele Marino

In this manuscript I present an analysis on the performance of OpenAI
O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a
function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the
number of variables of the satisfiable problem. I show that the model can call
an external SAT solver to solve the instances, rather than solving them
directly. Despite using external solvers, the model reports incorrect
assignments as output. Moreover, I propose and present an analysis to quantify
whether the OpenAI O1-preview model demonstrates a spark of intelligence or
merely makes random guesses when outputting an assignment for a Boolean
satisfiability problem.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÈáùÂ∞ç OpenAI O1-preview Ê®°ÂûãÂú®Ëß£Ê±∫Èö®Ê©ü K-SAT ÂØ¶‰æãÁöÑÊïàËÉΩÈÄ≤Ë°åÂàÜÊûêÔºåÂÖ∂‰∏≠ K‚àà {2,3,4}Ôºå‰ΩúÁÇ∫ Œ±=M/N ÁöÑÂáΩÊï∏ÔºåÂÖ∂‰∏≠ M ÊòØÂ≠êÂè•ÁöÑÊï∏ÈáèÔºåËÄå N ÊòØÂèØÊªøË∂≥ÂïèÈ°å‰∏≠ËÆäÊï∏ÁöÑÊï∏Èáè„ÄÇÊàëÈ°ØÁ§∫Ê®°ÂûãÂèØ‰ª•ÂëºÂè´Â§ñÈÉ® SAT Ê±ÇËß£Âô®‰æÜËß£Ê±∫ÂØ¶‰æãÔºåËÄå‰∏çÊòØÁõ¥Êé•Ëß£Ê±∫ÂÆÉÂÄë„ÄÇÂÑòÁÆ°‰ΩøÁî®Â§ñÈÉ®Ê±ÇËß£Âô®Ôºå‰ΩÜÊ®°ÂûãÊúÉÂ†±Âëä‰∏çÊ≠£Á¢∫ÁöÑÊåáÊ¥æ‰ΩúÁÇ∫Ëº∏Âá∫„ÄÇÊ≠§Â§ñÔºåÊàëÊèêÂá∫‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂàÜÊûêÔºå‰ª•ÈáèÂåñ OpenAI O1-preview Ê®°ÂûãÊòØÂê¶Â±ïÁèæÂá∫Êô∫ÊÖßÁöÑÁÅ´Ëä±ÔºåÊàñÂÉÖÂú®ÁÇ∫Â∏ÉÊûóÂèØÊªøË∂≥ÊÄßÂïèÈ°åËº∏Âá∫ÊåáÊ¥æÊôÇÈÄ≤Ë°åÈö®Ê©üÁåúÊ∏¨„ÄÇ

##### **Learning Source Disentanglement in Neural Audio Codec**
2409.11228v1 by Xiaoyu Bie, Xubo Liu, Ga√´l Richard

Neural audio codecs have significantly advanced audio compression by
efficiently converting continuous audio signals into discrete tokens. These
codecs preserve high-quality sound and enable sophisticated sound generation
through generative models trained on these tokens. However, existing neural
codec models are typically trained on large, undifferentiated audio datasets,
neglecting the essential discrepancies between sound domains like speech,
music, and environmental sound effects. This oversight complicates data
modeling and poses additional challenges to the controllability of sound
generation. To tackle these issues, we introduce the Source-Disentangled Neural
Audio Codec (SD-Codec), a novel approach that combines audio coding and source
separation. By jointly learning audio resynthesis and separation, SD-Codec
explicitly assigns audio signals from different domains to distinct codebooks,
sets of discrete representations. Experimental results indicate that SD-Codec
not only maintains competitive resynthesis quality but also, supported by the
separation results, demonstrates successful disentanglement of different
sources in the latent space, thereby enhancing interpretability in audio codec
and providing potential finer control over the audio generation process.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÈü≥Ë®äÁ∑®Ëß£Á¢ºÂô®ÈÄèÈÅéÊúâÊïàÁéáÂú∞Â∞áÈÄ£Á∫åÈü≥Ë®äË®äËôüËΩâÊèõÁÇ∫Èõ¢Êï£‰ª£Âπ£ÔºåÈ°ØËëóÊèêÂçáÈü≥Ë®äÂ£ìÁ∏ÆËÉΩÂäõ„ÄÇÈÄô‰∫õÁ∑®Ëß£Á¢ºÂô®‰øùÁïôÈ´òÂìÅË≥™Èü≥Ë®äÔºå‰∏¶ËÉΩÈÄèÈÅéË®ìÁ∑¥ÈÄô‰∫õ‰ª£Âπ£ÁöÑÁîüÊàêÊ®°Âûã‰æÜÈÄ≤Ë°åÁ≤æÂØÜÁöÑÈü≥Ë®äÁî¢Áîü„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ•ûÁ∂ìÁ∑®Ëß£Á¢ºÂô®Ê®°ÂûãÈÄöÂ∏∏Ë®ìÁ∑¥ÊñºÂ§ßÂûã„ÄÅÊú™ÂçÄÂàÜÁöÑÈü≥Ë®äË≥áÊñôÈõÜ‰∏äÔºåÂøΩÁï•‰∫ÜË™ûÈü≥„ÄÅÈü≥Ê®ÇÂíåÁí∞Â¢ÉÈü≥ÊïàÁ≠âÈü≥Ë®äÈ†òÂüüÈñìÁöÑÊú¨Ë≥™Â∑ÆÁï∞„ÄÇÈÄôÁ®ÆÁñèÂøΩ‰ΩøÂæóË≥áÊñôÂª∫Ê®°ËÆäÂæóË§áÈõúÔºå‰∏¶Â∞çÈü≥Ë®äÁî¢ÁîüÁöÑÂèØÊéßÊÄßÈÄ†ÊàêÈ°çÂ§ñÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰æÜÊ∫êÂàÜÈõ¢Á•ûÁ∂ìÈü≥Ë®äÁ∑®Ëß£Á¢ºÂô® (SD-Codec)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁµêÂêàÈü≥Ë®äÁ∑®Á¢ºÂíå‰æÜÊ∫êÂàÜÈõ¢ÁöÑÊñ∞Á©éÊñπÊ≥ï„ÄÇÈÄèÈÅéÂÖ±ÂêåÂ≠∏ÁøíÈü≥Ë®äÈáçÊñ∞ÂêàÊàêÂíåÂàÜÈõ¢ÔºåSD-Codec ÊòéÁ¢∫Âú∞Â∞á‰∏çÂêåÈ†òÂüüÁöÑÈü≥Ë®äË®äËôüÂàÜÈÖçÁµ¶‰∏çÂêåÁöÑ‰ª£Á¢ºÁ∞øÔºåÂç≥Èõ¢Êï£Ë°®Á§∫ÈõÜ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåSD-Codec ‰∏çÂÉÖÁ∂≠ÊåÅ‰∫ÜÂÖ∑Á´∂Áà≠ÂäõÁöÑÈáçÊñ∞ÂêàÊàêÂìÅË≥™ÔºåËÄå‰∏îÂú®ÂàÜÈõ¢ÁµêÊûúÁöÑÊîØÊè¥‰∏ãÔºåË≠âÊòéÊàêÂäüÂú∞Â∞áÊΩõÂú®Á©∫Èñì‰∏≠‰∏çÂêåÁöÑ‰æÜÊ∫êÂàÜÈõ¢Âá∫‰æÜÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜÈü≥Ë®äÁ∑®Ëß£Á¢ºÂô®ÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶Êèê‰æõ‰∫ÜÂ∞çÈü≥Ë®äÁî¢ÁîüÈÅéÁ®ãÊõ¥Á≤æÁ¥∞ÁöÑÊéßÂà∂„ÄÇ

##### **Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis**
2409.11218v1 by Lingling Xu, Haoran Xie, S. Joe Qin, Fu Lee Wang, Xiaohui Tao

Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards
specific aspect terms in a sentence and allows us to uncover nuanced
perspectives and attitudes on particular aspects of a product, service, or
topic. However, the scarcity of labeled data poses a significant challenge to
training high-quality models. To address this issue, we explore the potential
of data augmentation using ChatGPT, a well-performing large language model
(LLM), to enhance the sentiment classification performance towards aspect
terms. Specifically, we explore three data augmentation strategies based on
ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation
techniques. Context-focused data augmentation focuses on changing the word
expression of context words in the sentence while keeping aspect terms
unchanged. In contrast, aspect-focused data augmentation aims to change aspect
terms but keep context words unchanged. Context-Aspect data augmentation
integrates the above two data augmentations to generate augmented samples.
Furthermore, we incorporate contrastive learning into the ABSA tasks to improve
performance. Extensive experiments show that all three data augmentation
techniques lead to performance improvements, with the context-aspect data
augmentation strategy performing best and surpassing the performance of the
baseline models.

ÊëòË¶ÅÔºöÂü∫ÊñºÈù¢ÂêëÈù¢ÂêëËßÄÈªûÁöÑÊÉÖÊÑüÂàÜÊûêÔºàABSAÔºâÊ∂âÂèäË≠òÂà•Âè•Â≠ê‰∏≠ÁâπÂÆöÈù¢ÂêëË©ûÂΩôÁöÑÊÉÖÊÑüÔºå‰∏¶ËÆìÊàëÂÄëËÉΩÂ§†Êè≠Á§∫Â∞çÁî¢ÂìÅ„ÄÅÊúçÂãôÊàñ‰∏ªÈ°åÁâπÂÆöÈù¢ÂêëÁöÑÁ¥∞Á∑ªËßÄÈªûÂíåÊÖãÂ∫¶„ÄÇÁÑ∂ËÄåÔºåÊ®ôË®òË≥áÊñôÁöÑÁ®ÄÂ∞ëÂ∞çË®ìÁ∑¥È´òÂìÅË≥™Ê®°ÂûãÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊé¢Ë®é‰ΩøÁî® ChatGPTÔºà‰∏ÄÁ®ÆË°®ÁèæËâØÂ•ΩÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåLLMÔºâÈÄ≤Ë°åË≥áÊñôÊì¥ÂÖÖÁöÑÊΩõÂäõÔºå‰ª•Â¢ûÂº∑Èù¢ÂêëË©ûÂΩôÁöÑÊÉÖÊÑüÂàÜÈ°ûÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏âÁ®ÆÂü∫Êñº ChatGPT ÁöÑË≥áÊñôÊì¥ÂÖÖÁ≠ñÁï•Ôºö‰ª•ËÑàÁµ°ÁÇ∫ÈáçÈªû„ÄÅ‰ª•Èù¢ÂêëÁÇ∫ÈáçÈªûÂíåËÑàÁµ°-Èù¢ÂêëË≥áÊñôÊì¥ÂÖÖÊäÄË°ì„ÄÇ‰ª•ËÑàÁµ°ÁÇ∫ÈáçÈªûÁöÑË≥áÊñôÊì¥ÂÖÖÂ∞àÊ≥®ÊñºÊîπËÆäÂè•Â≠ê‰∏≠ËÑàÁµ°Ë©ûÂΩôÁöÑË©ûÂΩôË°®ÈÅîÔºåÂêåÊôÇ‰øùÊåÅÈù¢ÂêëË©ûÂΩô‰∏çËÆä„ÄÇÁõ∏ÂèçÂú∞Ôºå‰ª•Èù¢ÂêëÁÇ∫ÈáçÈªûÁöÑË≥áÊñôÊì¥ÂÖÖÊó®Âú®ÊîπËÆäÈù¢ÂêëË©ûÂΩôÔºå‰ΩÜ‰øùÊåÅËÑàÁµ°Ë©ûÂΩô‰∏çËÆä„ÄÇËÑàÁµ°-Èù¢ÂêëË≥áÊñôÊì¥ÂÖÖÊï¥Âêà‰∏äËø∞ÂÖ©Á®ÆË≥áÊñôÊì¥ÂÖÖÔºå‰ª•Áî¢ÁîüÊì¥ÂÖÖÁØÑ‰æã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÂ∞çÊØîÂ≠∏ÁøíÁ¥çÂÖ• ABSA ‰ªªÂãôÔºå‰ª•ÊèêÂçáÊïàËÉΩ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÊâÄÊúâ‰∏âÁ®ÆË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÈÉΩËÉΩÊèêÂçáÊïàËÉΩÔºåÂÖ∂‰∏≠‰ª•ËÑàÁµ°-Èù¢ÂêëË≥áÊñôÊì¥ÂÖÖÁ≠ñÁï•Ë°®ÁèæÊúÄ‰Ω≥Ôºå‰∏¶Ë∂ÖË∂äÂü∫Ê∫ñÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇ

##### **Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization**
2409.11212v1 by Jianing Wang, Yang Zhou, Xiaocheng Zhang, Mengjiao Bao, Peng Yan

Iterative preference optimization has recently become one of the de-facto
training paradigms for large language models (LLMs), but the performance is
still underwhelming due to too much noisy preference data yielded in the loop.
To combat this issue, we present an \textbf{U}ncertainty-enhanced
\textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM
self-evolve with reliable feedback. The key idea is mitigating the noisy
preference data derived from the current policy and reward models by performing
pair-wise uncertainty estimation and judiciously reliable feedback sampling. To
reach this goal, we thus introduce an estimator model, which incorporates Monte
Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty
estimation for the preference data derived from the LLM policy. Compared to the
existing methods that directly filter generated responses based on the reward
score, the estimator focuses on the model uncertainty in a pair-wise manner and
effectively bypasses the confirmation bias problem of the reward model.
Additionally, we also propose an uncertainty-enhanced self-evolution algorithm
to improve the robustness of preference optimization and encourage the LLM to
generate responses with both high reward and certainty. Extensive experiments
over multiple benchmarks demonstrate that our framework substantially
alleviates the noisy problem and improves the performance of iterative
preference optimization.

ÊëòË¶ÅÔºöËø≠‰ª£ÂÅèÂ•Ω‰ºòÂåñÊúÄËøëÂ∑≤Êàê‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂÆûÈôÖËÆ≠ÁªÉËåÉ‰æã‰πã‰∏ÄÔºå‰ΩÜÁî±‰∫éÂæ™ÁéØ‰∏≠‰∫ßÁîüÁöÑÂ§™Â§öÂô™Â£∞ÂÅèÂ•ΩÊï∞ÊçÆÔºåÊÄßËÉΩ‰ªçÁÑ∂‰ª§‰∫∫Â§±Êúõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∏çÁ°ÆÂÆöÊÄßÂ¢ûÂº∫ÁöÑÂÅèÂ•Ω‰ºòÂåñÔºàUPOÔºâÊ°ÜÊû∂Ôºå‰Ωø LLM ËÉΩÂ§üÈÄöËøáÂèØÈù†ÁöÑÂèçÈ¶àËøõË°åËá™ÊàëÊºîÂåñ„ÄÇÂÖ≥ÈîÆÊÄùÊÉ≥ÊòØÈÄöËøáÊâßË°åÊàêÂØπ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÂíåÊòéÊô∫ÂèØÈù†ÁöÑÂèçÈ¶àÈááÊ†∑Êù•ÂáèËΩª‰ªéÂΩìÂâçÁ≠ñÁï•ÂíåÂ•ñÂä±Ê®°Âûã‰∏≠ÂæóÂá∫ÁöÑÂòàÊùÇÂÅèÂ•ΩÊï∞ÊçÆ„ÄÇ‰∏∫‰∫ÜËææÂà∞Ëøô‰∏™ÁõÆÊ†áÔºåÊàë‰ª¨Âõ†Ê≠§ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰º∞ËÆ°Âô®Ê®°ÂûãÔºåËØ•Ê®°ÂûãÂ∞ÜËíôÁâπÂç°ÁΩó (MC) dropout Á∫≥ÂÖ•Ë¥ùÂè∂ÊñØÁ•ûÁªèÁΩëÁªú (BNN) ‰∏≠Ôºå‰ª•ÂØπ‰ªé LLM Á≠ñÁï•‰∏≠ÂæóÂá∫ÁöÑÂÅèÂ•ΩÊï∞ÊçÆÊâßË°å‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°„ÄÇ‰∏éÁõ¥Êé•Ê†πÊçÆÂ•ñÂä±ÂàÜÊï∞ËøáÊª§ÁîüÊàêÂìçÂ∫îÁöÑÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºå‰º∞ËÆ°Âô®‰ª•ÊàêÂØπÊñπÂºèÂÖ≥Ê≥®Ê®°ÂûãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂπ∂ÊúâÊïàÁªïËøá‰∫ÜÂ•ñÂä±Ê®°ÂûãÁöÑÁ°ÆËÆ§ÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏çÁ°ÆÂÆöÊÄßÂ¢ûÂº∫ÁöÑËá™ËøõÂåñÁÆóÊ≥ïÔºå‰ª•ÊèêÈ´òÂÅèÂ•Ω‰ºòÂåñÁöÑÈ≤ÅÊ£íÊÄßÔºåÂπ∂ÈºìÂä± LLM ÁîüÊàêÂÖ∑ÊúâÈ´òÂ•ñÂä±ÂíåÁ°ÆÂÆöÊÄßÁöÑÂìçÂ∫î„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Â§ßÂ§ßÁºìËß£‰∫ÜÂô™Â£∞ÈóÆÈ¢òÔºåÂπ∂ÊèêÈ´ò‰∫ÜËø≠‰ª£ÂÅèÂ•Ω‰ºòÂåñÁöÑÊÄßËÉΩ„ÄÇ

##### **SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds**
2409.11195v1 by Zhixing Hou, Maoxu Gao, Hang Yu, Mengyu Yang, Chio-In Ieong

This paper introduces a Spiking Diffusion Policy (SDP) learning method for
robotic manipulation by integrating Spiking Neurons and Learnable Channel-wise
Membrane Thresholds (LCMT) into the diffusion policy model, thereby enhancing
computational efficiency and achieving high performance in evaluated tasks.
Specifically, the proposed SDP model employs the U-Net architecture as the
backbone for diffusion learning within the Spiking Neural Network (SNN). It
strategically places residual connections between the spike convolution
operations and the Leaky Integrate-and-Fire (LIF) nodes, thereby preventing
disruptions to the spiking states. Additionally, we introduce a temporal
encoding block and a temporal decoding block to transform static and dynamic
data with timestep $T_S$ into each other, enabling the transmission of data
within the SNN in spike format. Furthermore, we propose LCMT to enable the
adaptive acquisition of membrane potential thresholds, thereby matching the
conditions of varying membrane potentials and firing rates across channels and
avoiding the cumbersome process of manually setting and tuning hyperparameters.
Evaluating the SDP model on seven distinct tasks with SNN timestep $T_S=4$, we
achieve results comparable to those of the ANN counterparts, along with faster
convergence speeds than the baseline SNN method. This improvement is
accompanied by a reduction of 94.3\% in dynamic energy consumption estimated on
45nm hardware.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂ∞ñÂ≥∞Êâ©Êï£Á≠ñÁï• (SDP) Â≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÂ∞ñÂ≥∞Á•ûÁªèÂÖÉÂíåÂèØÂ≠¶‰π†ÈÄöÈÅìÈòàÂÄº (LCMT) ÈõÜÊàêÂà∞Êâ©Êï£Á≠ñÁï•Ê®°Âûã‰∏≠Ôºå‰ªéËÄåÊèêÈ´òËÆ°ÁÆóÊïàÁéáÂπ∂Âú®ËØÑ‰º∞‰ªªÂä°‰∏≠ÂÆûÁé∞È´òÊÄßËÉΩÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊâÄÊèêÂá∫ÁöÑ SDP Ê®°ÂûãÈááÁî® U-Net Êû∂ÊûÑ‰Ωú‰∏∫Â∞ñÂ≥∞Á•ûÁªèÁΩëÁªú (SNN) ÂÜÖÊâ©Êï£Â≠¶‰π†ÁöÑ‰∏ªÂπ≤„ÄÇÂÆÉÁ≠ñÁï•ÊÄßÂú∞Âú®Â∞ñÂ≥∞Âç∑ÁßØËøêÁÆóÂíåÊºèÁîµÊµÅÁßØÂàÜÊîæÁîµ (LIF) ËäÇÁÇπ‰πãÈó¥ÊîæÁΩÆÊÆãÂ∑ÆËøûÊé•Ôºå‰ªéËÄåÈò≤Ê≠¢Â∞ñÂ≥∞Áä∂ÊÄÅ‰∏≠Êñ≠„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∏Ä‰∏™Êó∂Èó¥ÁºñÁ†ÅÂùóÂíå‰∏Ä‰∏™Êó∂Èó¥Ëß£Á†ÅÂùóÔºå‰ª•Â∞ÜÊó∂Èó¥Ê≠•Èïø‰∏∫ $T_S$ ÁöÑÈùôÊÄÅÂíåÂä®ÊÄÅÊï∞ÊçÆÁõ∏‰∫íËΩ¨Êç¢Ôºå‰ªéËÄåËÉΩÂ§ü‰ª•Â∞ñÂ≥∞Ê†ºÂºèÂú® SNN ÂÜÖ‰º†ËæìÊï∞ÊçÆ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü LCMT ‰ª•ÂÆûÁé∞ËÜúÁîµ‰ΩçÈòàÂÄºÁöÑËá™ÈÄÇÂ∫îËé∑ÂèñÔºå‰ªéËÄåÂåπÈÖç‰∏çÂêåÈÄöÈÅìÁöÑËÜúÁîµ‰ΩçÂíåÊîæÁîµÈÄüÁéáÁöÑÂèòÂåñÊù°‰ª∂ÔºåÂπ∂ÈÅøÂÖçÊâãÂä®ËÆæÁΩÆÂíåË∞ÉÊï¥Ë∂ÖÂèÇÊï∞ÁöÑÁπÅÁêêËøáÁ®ã„ÄÇÂú® SNN Êó∂Èó¥Ê≠•Èïø $T_S=4$ ÁöÑ‰∏É‰∏™‰∏çÂêå‰ªªÂä°‰∏äËØÑ‰º∞ SDP Ê®°ÂûãÔºåÊàë‰ª¨ÂèñÂæó‰∫Ü‰∏é ANN ÂØπÂ∫îÊ®°ÂûãÁõ∏ÂΩìÁöÑÁªìÊûúÔºåÂπ∂‰∏îÊØîÂü∫Á∫ø SNN ÊñπÊ≥ïÂÖ∑ÊúâÊõ¥Âø´ÁöÑÊî∂ÊïõÈÄüÂ∫¶„ÄÇËøô‰∏ÄÊîπËøõ‰º¥ÈöèÁùÄÂú® 45nm Á°¨‰ª∂‰∏ä‰º∞ËÆ°ÁöÑÂä®ÊÄÅËÉΩËÄóÈôç‰Ωé‰∫Ü 94.3%„ÄÇ

##### **Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**
2409.11192v1 by Eunhae Lee

One application area of long-term memory (LTM) capabilities with increasing
traction is personal AI companions and assistants. With the ability to retain
and contextualize past interactions and adapt to user preferences, personal AI
companions and assistants promise a profound shift in how we interact with AI
and are on track to become indispensable in personal and professional settings.
However, this advancement introduces new challenges and vulnerabilities that
require careful consideration regarding the deployment and widespread use of
these systems. The goal of this paper is to explore the broader implications of
building and deploying personal AI applications with LTM capabilities using a
holistic evaluation approach. This will be done in three ways: 1) reviewing the
technological underpinnings of LTM in Large Language Models, 2) surveying
current personal AI companions and assistants, and 3) analyzing critical
considerations and implications of deploying and using these applications.

ÊëòË¶ÅÔºöÈï∑ÊúüË®òÊÜ∂ (LTM) ËÉΩÂäõÁöÑÊáâÁî®È†òÂüü‰πã‰∏ÄÊòØÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÔºåÂÖ∂Âê∏ÂºïÂäõÊ≠£ËàáÊó•‰ø±Â¢û„ÄÇÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÂÖ∑ÂÇô‰øùÁïôÂíåÂ∞áÈÅéÂéª‰∫íÂãïËÑàÁµ°ÂåñÔºå‰ª•ÂèäÈÅ©Êáâ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑËÉΩÂäõÔºåÊâøË´æÂ∞áÂæπÂ∫ïÊîπËÆäÊàëÂÄëËàá AI ‰∫íÂãïÁöÑÊñπÂºèÔºå‰∏¶ÊúâÊúõÂú®ÂÄã‰∫∫ÂíåÂ∞àÊ•≠È†òÂüü‰∏≠ËÆäÂæó‰∏çÂèØÊàñÁº∫„ÄÇÁÑ∂ËÄåÔºåÈÄôÈ†ÖÈÄ≤Â±ïÂ∏∂‰æÜ‰∫ÜÊñ∞ÁöÑÊåëÊà∞ÂíåÊºèÊ¥ûÔºåÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáèÈÄô‰∫õÁ≥ªÁµ±ÁöÑÈÉ®ÁΩ≤ÂíåÂª£Ê≥õ‰ΩøÁî®„ÄÇÊú¨ÊñáÁöÑÁõÆÊ®ôÊòØÂà©Áî®Êï¥È´îË©ï‰º∞ÊñπÊ≥ïÊé¢Ë®éÂª∫ÊßãÂíåÈÉ®ÁΩ≤ÂÖ∑ÂÇô LTM ËÉΩÂäõÁöÑÂÄã‰∫∫ AI ÊáâÁî®Á®ãÂºèÁöÑÂª£Ê≥õÂΩ±Èüø„ÄÇÈÄôÂ∞áÈÄèÈÅé‰∏âÁ®ÆÊñπÂºèÈÄ≤Ë°åÔºö1) Ê™¢Ë¶ñÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ LTM ÁöÑÊäÄË°ìÂü∫Á§éÔºå2) Ë™øÊü•ÁõÆÂâçÁöÑÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÔºå‰ª•Âèä 3) ÂàÜÊûêÈÉ®ÁΩ≤Âíå‰ΩøÁî®ÈÄô‰∫õÊáâÁî®Á®ãÂºèÁöÑÈóúÈçµËÄÉÈáèÂíåÂΩ±Èüø„ÄÇ

##### **SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer**
2409.11190v1 by Anmol Gautam, Kishore Kumar, Adarsh Jha, Mukunda NS, Ishaan Bhola

We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊèêÂá∫ SuperCoder2.0Ôºå‰∏ÄÂÄãÂÖàÈÄ≤ÁöÑËá™‰∏ªÁ≥ªÁµ±ÔºåÊó®Âú®ÈÄèÈÅé‰∫∫Â∑•Êô∫ÊÖßÊèêÂçáËªüÈ´îÈñãÁôº„ÄÇË©≤Á≥ªÁµ±ÁµêÂêà‰∫ÜÂéüÁîü AI ÈñãÁôºÊñπÊ≥ïËàáÊô∫ÊÖß‰ª£ÁêÜ‰∫∫Ôºå‰ª•ÂØ¶ÁèæÂÆåÂÖ®Ëá™‰∏ªÁ∑®Á¢º„ÄÇ‰∏ªË¶ÅÁöÑÈáçÈªûÈ†òÂüüÂåÖÊã¨ÂÖ∑ÊúâÈåØË™§Ëº∏Âá∫ËøΩËπ§ÁöÑÈáçË©¶Ê©üÂà∂„ÄÅ‰ΩøÁî®ÊäΩË±°Ë™ûÊ≥ïÊ®π (ast) Ëß£ÊûêÁöÑÂÖ®Èù¢Á®ãÂºèÁ¢ºÊîπÂØ´ÂíåÊõøÊèõÔºå‰ª•ÊúÄÂ∞èÂåñÁ®ãÂºèÁ¢ºÊ™¢Êü•ÂïèÈ°å„ÄÅÁî®ÊñºÊ™¢Á¥¢Êì¥ÂÖÖÁî¢ÁîüÊäÄË°ìÁöÑÁ®ãÂºèÁ¢ºÂµåÂÖ•ÊäÄË°ìÔºå‰ª•ÂèäÂ∞àÊ≥®ÊñºÂÆö‰ΩçÊñπÊ≥ï‰ª•Ëß£Ê±∫ÂïèÈ°åÔºåËÄåÈùûË≠òÂà•ÁâπÂÆöË°åËôü„ÄÇË©≤ÊñπÊ≥ïÊé°Áî®‰∏âÊ≠•È©üÈöéÂ±§ÂºèÊêúÂ∞ãÁ©∫ÈñìÁ∏ÆÊ∏õÊñπÊ≥ïÔºåÁî®ÊñºÁ®ãÂºèÁ¢ºÂ∫´Â∞éË¶ΩÂíåÈåØË™§ÂÆö‰ΩçÔºöÂà©Áî®Ê™¢Á¥¢Êì¥ÂÖÖÁî¢Áîü (RAG) ÂíåÂÑ≤Â≠òÂ∫´Ê™îÊ°àÂ±§Á¥öÂ∞çÊáâÔºå‰ª•Ë≠òÂà•ÂÄôÈÅ∏Ê™îÊ°àÔºå(2) ‰ΩøÁî®Ê™îÊ°àÂ±§Á¥öÊ¶ÇËßÄÂ∞çÊáâÁ∏ÆÂ∞èÁØÑÂúçËá≥ÊúÄÁõ∏ÈóúÁöÑÊ™îÊ°àÔºå‰ª•Âèä (3) ÂæûÈÄô‰∫õÊ™îÊ°à‰∏≠Êì∑Âèñ„ÄåÁõ∏Èóú‰ΩçÁΩÆ„Äç„ÄÇÁ®ãÂºèÁ¢ºÁ∑®ËºØÈÄèÈÅéÂåÖÂê´Á®ãÂºèÁ¢ºÁî¢ÁîüÂíåÁ®ãÂºèÁ¢ºÁ∑®ËºØÁöÑÂÖ©ÈÉ®ÂàÜÊ®°ÁµÑÂü∑Ë°åÔºåË©≤Ê®°ÁµÑÊúÉÂú®‰∏çÂêåÁöÑÊ∫´Â∫¶ÂÄºÁî¢ÁîüÂ§öÈáçËß£Ôºå‰∏¶ÊõøÊèõÊï¥ÂÄãÊñπÊ≥ïÊàñÈ°ûÂà•‰ª•Á∂≠Ë≠∑Á®ãÂºèÁ¢ºÂÆåÊï¥ÊÄß„ÄÇÂõûÈ•ãËø¥Ë∑ØÊúÉÂü∑Ë°åÂÑ≤Â≠òÂ∫´Â±§Á¥öÊ∏¨Ë©¶Ê°à‰æãÔºå‰ª•È©óË≠â‰∏¶ÊîπÂñÑËß£„ÄÇÂú® SWE-bench Lite Ë≥áÊñôÈõÜ‰∏äÂü∑Ë°åÁöÑÂØ¶È©óË≠âÊòé‰∫Ü SuperCoder2.0 ÁöÑÊïàËÉΩÔºåÂú® 84.33% ÁöÑÊ°à‰æã‰∏≠ÔºåÊñºÂâç 5 ÂêçÂÄôÈÅ∏ËÄÖ‰∏≠ÈÅîÊàêÊ≠£Á¢∫Ê™îÊ°àÂÆö‰ΩçÔºå‰∏¶ÊàêÂäüËß£Ê±∫ 34% ÁöÑÊ∏¨Ë©¶Ê°à‰æã„ÄÇÊ≠§ÊïàËÉΩËÆì SuperCoder2.0 Âú® SWE-bench ÊéíË°åÊ¶ú‰∏äÊéíÂêçÂÖ®ÁêÉÁ¨¨Âõõ„ÄÇË©≤Á≥ªÁµ±ËôïÁêÜ‰∏çÂêåÂÑ≤Â≠òÂ∫´ÂíåÂïèÈ°åÈ°ûÂûãÁöÑËÉΩÂäõÔºåÁ™ÅÈ°Ø‰∫ÜÂÖ∂‰ΩúÁÇ∫Ëá™‰∏ªËªüÈ´îÈñãÁôºÈÄöÁî®Â∑•ÂÖ∑ÁöÑÊΩõÂäõ„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Â∞áÂ∞àÊ≥®ÊñºÊîπÂñÑÁ®ãÂºèÁ¢ºÁ∑®ËºØÊµÅÁ®ãÔºå‰∏¶Êé¢Á¥¢ÈÄ≤ÈöéÂµåÂÖ•Ê®°ÂûãÔºå‰ª•ÊèêÂçáËá™ÁÑ∂Ë™ûË®ÄÂ∞çÁ®ãÂºèÁ¢ºÁöÑÂ∞çÊáâ„ÄÇ</paragraph>

##### **Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer**
2409.11174v1 by Yanqing Kang, Di Zhu, Haiyang Zhang, Enze Shi, Sigang Yu, Jinru Wu, Xuhui Wang, Xuan Liu, Geng Chen, Xi Jiang, Tuo Zhang, Shu Zhang

Studying influential nodes (I-nodes) in brain networks is of great
significance in the field of brain imaging. Most existing studies consider
brain connectivity hubs as I-nodes. However, this approach relies heavily on
prior knowledge from graph theory, which may overlook the intrinsic
characteristics of the brain network, especially when its architecture is not
fully understood. In contrast, self-supervised deep learning can learn
meaningful representations directly from the data. This approach enables the
exploration of I-nodes for brain networks, which is also lacking in current
studies. This paper proposes a Self-Supervised Graph Reconstruction framework
based on Graph-Transformer (SSGR-GT) to identify I-nodes, which has three main
characteristics. First, as a self-supervised model, SSGR-GT extracts the
importance of brain nodes to the reconstruction. Second, SSGR-GT uses
Graph-Transformer, which is well-suited for extracting features from brain
graphs, combining both local and global characteristics. Third, multimodal
analysis of I-nodes uses graph-based fusion technology, combining functional
and structural brain information. The I-nodes we obtained are distributed in
critical areas such as the superior frontal lobe, lateral parietal lobe, and
lateral occipital lobe, with a total of 56 identified across different
experiments. These I-nodes are involved in more brain networks than other
regions, have longer fiber connections, and occupy more central positions in
structural connectivity. They also exhibit strong connectivity and high node
efficiency in both functional and structural networks. Furthermore, there is a
significant overlap between the I-nodes and both the structural and functional
rich-club. These findings enhance our understanding of the I-nodes within the
brain network, and provide new insights for future research in further
understanding the brain working mechanisms.

ÊëòË¶ÅÔºö<paragraph>Âú®ËÖ¶ÈÉ®ÂΩ±ÂÉèÈ†òÂüü‰∏≠ÔºåÁ†îÁ©∂ËÖ¶Á∂≤Ë∑Ø‰∏≠ÁöÑÂΩ±ÈüøÂäõÁØÄÈªûÔºàI ÁØÄÈªûÔºâÂÖ∑ÊúâÈáçË¶ÅÊÑèÁæ©„ÄÇÁèæÊúâÁ†îÁ©∂Â§ßÂ§öÂ∞áËÖ¶ÈÉ®ÈÄ£Êé•Ê®ûÁ¥êË¶ñÁÇ∫ I ÁØÄÈªû„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂö¥Èáç‰æùË≥¥ÊñºÂúñË´ñÁöÑÂÖàÈ©óÁü•Ë≠òÔºåÂèØËÉΩÊúÉÂøΩÁï•ËÖ¶Á∂≤Ë∑ØÁöÑÂÖßÂú®ÁâπÂæµÔºåÁâπÂà•ÊòØÂú®ÂÖ∂Êû∂ÊßãÂ∞öÊú™ÂÆåÂÖ®‰∫ÜËß£ÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåËá™Áõ£Áù£Ê∑±Â∫¶Â≠∏ÁøíÂèØ‰ª•Áõ¥Êé•ÂæûÊï∏Êìö‰∏≠Â≠∏ÁøíÊúâÊÑèÁæ©ÁöÑË°®Á§∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÉΩÂ§†Êé¢Á¥¢ËÖ¶Á∂≤Ë∑ØÁöÑ I ÁØÄÈªûÔºåÈÄô‰πüÊòØÁï∂ÂâçÁ†îÁ©∂‰∏≠ÊâÄÊ¨†Áº∫ÁöÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂúñÂΩ¢ËΩâÊèõÂô®ÁöÑËá™Áõ£Áù£ÂúñÂΩ¢ÈáçÂª∫Ê°ÜÊû∂ÔºàSSGR-GTÔºâ‰æÜË≠òÂà• I ÁØÄÈªûÔºåÂÖ∂ÂÖ∑Êúâ‰∏âÂ§ßÁâπÈªû„ÄÇÈ¶ñÂÖàÔºå‰ΩúÁÇ∫‰∏ÄÂÄãËá™Áõ£Áù£Ê®°ÂûãÔºåSSGR-GT ÊèêÂèñËÖ¶ÁØÄÈªûÂ∞çÈáçÂª∫ÁöÑÈáçË¶ÅÊÄß„ÄÇÂÖ∂Ê¨°ÔºåSSGR-GT ‰ΩøÁî®ÂúñÂΩ¢ËΩâÊèõÂô®ÔºåÂÆÉÈùûÂ∏∏ÈÅ©ÂêàÂæûËÖ¶ÂúñÂΩ¢‰∏≠ÊèêÂèñÁâπÂæµÔºåÁµêÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæµ„ÄÇÁ¨¨‰∏âÔºåI ÁØÄÈªûÁöÑÂ§öÊ®°ÊÖãÂàÜÊûê‰ΩøÁî®Âü∫ÊñºÂúñÂΩ¢ÁöÑËûçÂêàÊäÄË°ìÔºåÁµêÂêàÂäüËÉΩÂíåÁµêÊßãËÖ¶ÈÉ®Ë≥áË®ä„ÄÇÊàëÂÄëÁç≤ÂæóÁöÑ I ÁØÄÈªûÂàÜ‰ΩàÂú®ÈóúÈçµÂçÄÂüüÔºå‰æãÂ¶ÇÈ°ç‰∏äËëâ„ÄÅÈ†ÇËëâÂ§ñÂÅ¥ÂíåÊûïËëâÂ§ñÂÅ¥ÔºåÂú®‰∏çÂêåÁöÑÂØ¶È©ó‰∏≠Á∏ΩÂÖ±Ë≠òÂà•Âá∫ 56 ÂÄã„ÄÇÈÄô‰∫õ I ÁØÄÈªûÂèÉËàáÁöÑËÖ¶Á∂≤Ë∑ØÊØîÂÖ∂‰ªñÂçÄÂüüÊõ¥Â§öÔºåÂÖ∑ÊúâÊõ¥Èï∑ÁöÑÁ∫ñÁ∂≠ÈÄ£Êé•Ôºå‰∏¶‰∏îÂú®ÁµêÊßãÈÄ£Êé•‰∏≠‰ΩîÊìöÊõ¥‰∏≠ÂøÉÁöÑ‰ΩçÁΩÆ„ÄÇÂÆÉÂÄëÂú®ÂäüËÉΩÂíåÁµêÊßãÁ∂≤Ë∑Ø‰∏≠‰πüË°®ÁèæÂá∫Âº∑ÈÄ£Êé•ÊÄßÂíåÈ´òÁØÄÈªûÊïàÁéá„ÄÇÊ≠§Â§ñÔºåI ÁØÄÈªûËàáÁµêÊßãÂíåÂäüËÉΩÂØå‰ø±Ê®ÇÈÉ®‰πãÈñìÂ≠òÂú®È°ØËëóÈáçÁñä„ÄÇÈÄô‰∫õÁôºÁèæÂ¢ûÂº∑‰∫ÜÊàëÂÄëÂ∞çËÖ¶Á∂≤Ë∑Ø‰∏≠ I ÁØÄÈªûÁöÑÁêÜËß£Ôºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£ËÖ¶Â∑•‰ΩúÊ©üÂà∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑË¶ãËß£„ÄÇ</paragraph>

##### **SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration**
2409.11149v1 by Xin Guan, Nathaniel Demchak, Saloni Gupta, Ze Wang, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu

The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(-Bias) is the
first holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that assessment
tool bias and contextual bias in prompts can distort evaluation, SAGED
implements counterfactual branching and baseline calibration for mitigation.
For demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. (vice-/former-) presidents, we see bias amplifies
and shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not
engage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.

ÊëòË¶ÅÔºö<paragraph>ÁÑ°ÂÅèË¶ãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁôºÂ±ïË¢´Âª£Ê≥õË™çÁÇ∫Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁèæÊúâÁöÑÂü∫Ê∫ñÂú®Ê™¢Ê∏¨ÂÅèË¶ãÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂéüÂõ†Âú®ÊñºÁØÑÂúçÊúâÈôê„ÄÅÊ±ôÊüì‰ª•ÂèäÁº∫‰πèÂÖ¨Âπ≥Âü∫Ê∫ñ„ÄÇSAGED(-Bias) ÊòØÁ¨¨‰∏ÄÂÄãËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÁöÑÊï¥È´îÂü∫Ê∫ñÁÆ°ÈÅì„ÄÇË©≤ÁÆ°ÈÅìÂåÖÂê´‰∫îÂÄãÊ†∏ÂøÉÈöéÊÆµÔºöÊì∑ÂèñÁ¥†Êùê„ÄÅÁµÑË£ùÂü∫Ê∫ñ„ÄÅÁî¢ÁîüÂõûÊáâ„ÄÅËêÉÂèñÊï∏ÂÄºÁâπÂæµÔºå‰ª•Âèä‰ΩøÁî®Â∑ÆÁï∞ÊåáÊ®ôË®∫Êñ∑„ÄÇSAGED ÂåÖÂê´ÊúÄÂ§ßÂ∑ÆÁï∞ÊåáÊ®ôÔºå‰æãÂ¶ÇÂΩ±ÈüøÊØîÁéáÔºå‰ª•ÂèäÂÅèË¶ãÈõÜ‰∏≠Â∫¶ÊåáÊ®ôÔºå‰æãÂ¶ÇÊúÄÂ§ß Z ÂàÜÊï∏„ÄÇSAGED ËßÄÂØüÂà∞Ë©ïÈáèÂ∑•ÂÖ∑ÂÅèË¶ãÂíåÊèêÁ§∫‰∏≠ÁöÑËÑàÁµ°ÂÅèË¶ãÊúÉÊâ≠Êõ≤Ë©ïÈáèÔºåÂõ†Ê≠§ÂØ¶‰ΩúÂèç‰∫ãÂØ¶ÂàÜÊîØÂíåÂü∫Ê∫ñÊ†°Ê≠£‰ª•ÈÄ≤Ë°åÁ∑©Ëß£„ÄÇÁÇ∫‰∫ÜÁ§∫ÁØÑÔºåÊàëÂÄëÂú® G20 ÂúãÂÆ∂‰ΩøÁî® SAGEDÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÁÜ±ÈñÄÁöÑ 8b Á¥öÂà•Ê®°ÂûãÔºå‰æãÂ¶Ç Gemma2„ÄÅLlama3.1„ÄÅMistral Âíå Qwen2„ÄÇÈÄèÈÅéÊÉÖÁ∑íÂàÜÊûêÔºåÊàëÂÄëÁôºÁèæÔºåÂÑòÁÆ° Mistral Âíå Qwen2 È°ØÁ§∫Âá∫ÊØî Gemma2 Âíå Llama3.1 Êõ¥‰ΩéÁöÑÊúÄÂ§ßÂ∑ÆÁï∞ÂíåÊõ¥È´òÁöÑÂÅèË¶ãÈõÜ‰∏≠Â∫¶Ôºå‰ΩÜÊâÄÊúâÊ®°ÂûãÈÉΩÊòéÈ°ØÂÅèÂêëÊñº‰øÑÁæÖÊñØÁ≠âÂúãÂÆ∂ÔºåËÄå Qwen2 Èô§Â§ñÔºåÂâáÂÅèÂêëÊñº‰∏≠Âúã„ÄÇÈÄèÈÅéÈÄ≤‰∏ÄÊ≠•ÁöÑÂØ¶È©óËÆìÊ®°ÂûãÊâÆÊºîÁæéÂúãÔºàÁèæ‰ªª/Ââç‰ªªÔºâÁ∏ΩÁµ±ÔºåÊàëÂÄëÁúãÂà∞ÂÅèË¶ãÊúÉÂú®Áï∞Ë≥™ÊñπÂêë‰∏äÊì¥Â§ßÂíåËΩâÁßª„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁúãÂà∞ Qwen2 Âíå Mistral Ê≤íÊúâÂèÉËàáËßíËâ≤ÊâÆÊºîÔºåËÄå Llama3.1 Âíå Gemma2 ÊâÆÊºîÂ∑ùÊôÆÁöÑËßíËâ≤ÊòéÈ°ØÊØîÊãúÁôªÂíåË≥ÄÈå¶È∫óÊõ¥ÁÇ∫Ê∑±ÂÖ•ÔºåÈÄôË°®Á§∫ÈÄô‰∫õÊ®°ÂûãÂ≠òÂú®ËßíËâ≤ÊâÆÊºîË°®ÁèæÂÅèË¶ã„ÄÇ</paragraph>

##### **Improving the Efficiency of Visually Augmented Language Models**
2409.11148v1 by Paula Ontalvilla, Aitor Ormazabal, Gorka Azkune

Despite the impressive performance of autoregressive Language Models (LM) it
has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they
do not know much about the visual world and its properties. To augment LMs with
visual knowledge, existing solutions often rely on explicit images, requiring
time-consuming retrieval or image generation systems. This paper shows that
explicit images are not necessary to visually augment an LM. Instead, we use
visually-grounded text representations obtained from the well-known CLIP
multimodal system. For a fair comparison, we modify VALM, a visually-augmented
LM which uses image retrieval and representation, to work directly with
visually-grounded text representations. We name this new model BLIND-VALM. We
show that BLIND-VALM performs on par with VALM for Visual Language
Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling
tasks, despite being significantly more efficient and simpler. We also show
that scaling up our model within the compute budget of VALM, either increasing
the model or pre-training corpus size, we outperform VALM for all the
evaluation tasks.

ÊëòË¶ÅÔºöÂÑòÁÆ°Ëá™Ëø¥Ê≠∏Ë™ûË®ÄÊ®°Âûã (LM) ÂÖ∑Êúâ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊïàËÉΩÔºå‰ΩÜÂ∑≤È°ØÁ§∫Âá∫Áî±ÊñºÂõûÂ†±ÂÅèÂ∑ÆÔºåLM Áº∫‰πèË¶ñË¶∫Áü•Ë≠òÔºå‰∫¶Âç≥ÂÆÉÂÄëÂ∞çË¶ñË¶∫‰∏ñÁïåÂèäÂÖ∂Â±¨ÊÄßÊâÄÁü•ÁîöÂ∞ë„ÄÇÁÇ∫‰∫ÜÊì¥ÂÖÖ LM ÁöÑË¶ñË¶∫Áü•Ë≠òÔºåÁèæÊúâÁöÑËß£Ê±∫ÊñπÊ°àÈÄöÂ∏∏‰æùË≥¥ÊòéÁ¢∫ÁöÑÂΩ±ÂÉèÔºåÈúÄË¶ÅËÄóÊôÇÁöÑÊì∑ÂèñÊàñÂΩ±ÂÉèÁî¢ÁîüÁ≥ªÁµ±„ÄÇÊú¨ÊñáÈ°ØÁ§∫ÊòéÁ¢∫ÁöÑÂΩ±ÂÉè‰∏¶ÈùûË¶ñË¶∫Êì¥ÂÖÖ LM ÊâÄÂøÖÈúÄ„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄë‰ΩøÁî®ÂæûËëóÂêçÁöÑ CLIP Â§öÊ®°ÊÖãÁ≥ªÁµ±ÂèñÂæóÁöÑË¶ñË¶∫Âü∫Á§éÊñáÂ≠óË°®Âæµ„ÄÇÁÇ∫‰∫ÜÂÖ¨Âπ≥ÊØîËºÉÔºåÊàëÂÄë‰øÆÊîπ‰∫ÜË¶ñË¶∫Êì¥ÂÖÖ LM VALMÔºåÂÆÉ‰ΩøÁî®ÂΩ±ÂÉèÊì∑ÂèñÂíåË°®ÂæµÔºå‰ª•Áõ¥Êé•‰ΩøÁî®Ë¶ñË¶∫Âü∫Á§éÊñáÂ≠óË°®Âæµ„ÄÇÊàëÂÄëÂ∞áÈÄôÂÄãÊñ∞Ê®°ÂûãÂëΩÂêçÁÇ∫ BLIND-VALM„ÄÇÊàëÂÄëÈ°ØÁ§∫ BLIND-VALM Âú®Ë¶ñË¶∫Ë™ûË®ÄÁêÜËß£ (VLU)„ÄÅËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ (NLU) ÂíåË™ûË®ÄÊ®°Âûã‰ªªÂãô‰∏≠ÔºåË°®ÁèæËàá VALM Áõ∏Áï∂ÔºåÂÑòÁÆ°ÂÆÉÁöÑÊïàÁéáÈ°ØËëóÊõ¥È´ò‰∏îÊõ¥Á∞°ÂñÆ„ÄÇÊàëÂÄë‰πüÈ°ØÁ§∫Âú® VALM ÁöÑÈÅãÁÆóÈ†êÁÆóÂÖßÊì¥ÂÖÖÊàëÂÄëÁöÑÊ®°ÂûãÔºåÁÑ°Ë´ñÊòØÂ¢ûÂä†Ê®°ÂûãÊàñÈ†êË®ìÁ∑¥Ë™ûÊñôÂ∫´Â§ßÂ∞èÔºåÊàëÂÄëÂú®ÊâÄÊúâË©ï‰º∞‰ªªÂãô‰∏≠ÈÉΩÂÑ™Êñº VALM„ÄÇ

##### **Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**
2409.11147v1 by Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen

Large language models(LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning(ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÂ∞ëÈáèÂ≠∏ÁøíËÉΩÂäõÔºå‰∏¶ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÊäÄË°ìÁµ±‰∏Ä‰∫Ü NLP ‰ªªÂãôÁöÑÁØÑ‰æã„ÄÇÂÑòÁÆ° ICL Â∑≤ÊàêÂäüÔºåÁØÑ‰æãÁ§∫ÁØÑÁöÑÂìÅË≥™ÊúÉÈ°ØËëóÂΩ±Èüø LLM ÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÁØÑ‰æãÈÅ∏ÊìáÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñºÊü•Ë©¢ËàáÂÄôÈÅ∏ÁØÑ‰æã‰πãÈñìÁöÑË™ûÊÑèÁõ∏‰ººÊÄß„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÊé®ÁêÜÊ≠•È©ü‰πãÈñìÁöÑÈÇèËºØÈÄ£ÁµêÊúâÂä©ÊñºÊèèÁπ™ÂïèÈ°åËß£Ê±∫ÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êé®ÁêÜÂúñÂ¢ûÂº∑ÁØÑ‰æãÊ™¢Á¥¢ (RGER) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇRGER È¶ñÂÖàË¶ÅÊ±Ç LLM Áî¢Áîü‰∏ÄÂÄãÂàùÂßãÂõûÊáâÔºåÁÑ∂ÂæåÂ∞á‰∏≠ÈñìÂïèÈ°åËß£Ê±∫Ê≠•È©üË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁµêÊßã„ÄÇ‰πãÂæåÔºåÂÆÉÊé°Áî®ÂúñÂΩ¢Ê†∏ÈÅ∏ÂèñÂÖ∑ÊúâË™ûÊÑèÂíåÁµêÊßãÁõ∏‰ººÊÄßÁöÑÁØÑ‰æã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÁµêÊßãÈóú‰øÇÊúâÂä©ÊñºÊü•Ë©¢ÂíåÂÄôÈÅ∏ÁØÑ‰æãÁöÑÂ∞çÈΩä„ÄÇRGER Âú®Êï∏Â≠∏ÂíåÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏äÁöÑÂäüÊïàÂ±ïÁ§∫‰∫ÜÂÆÉÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÁôºÂ∏ÉÊñº https://github.com/Yukang-Lin/RGER„ÄÇ

##### **High-Resolution Speech Restoration with Latent Diffusion Model**
2409.11145v1 by Tushar Dhyani, Florian Lux, Michele Mancusi, Giorgio Fabbro, Fritz Hohl, Ngoc Thang Vu

Traditional speech enhancement methods often oversimplify the task of
restoration by focusing on a single type of distortion. Generative models that
handle multiple distortions frequently struggle with phone reconstruction and
high-frequency harmonics, leading to breathing and gasping artifacts that
reduce the intelligibility of reconstructed speech. These models are also
computationally demanding, and many solutions are restricted to producing
outputs in the wide-band frequency range, which limits their suitability for
professional applications. To address these challenges, we propose Hi-ResLDM, a
novel generative model based on latent diffusion designed to remove multiple
distortions and restore speech recordings to studio quality, sampled at 48kHz.
We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and
Conditional Flow Matching (CFM) components, demonstrating superior performance
in regenerating high-frequency-band details. Hi-ResLDM not only excels in
non-instrusive metrics but is also consistently preferred in human evaluation
and performs competitively on intrusive evaluations, making it ideal for
high-resolution speech restoration.

ÊëòË¶ÅÔºöÂÇ≥Áµ±ÁöÑË™ûÈü≥Â¢ûÂº∑ÊñπÊ≥ïÈÄöÂ∏∏ÈÅéÂ∫¶Á∞°ÂåñÂæ©Âéü‰ªªÂãôÔºåÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÈ°ûÂûãÁöÑÂ§±Áúü„ÄÇËôïÁêÜÂ§öÈáçÂ§±ÁúüÁöÑÁîüÊàêÊ®°ÂûãÁ∂ìÂ∏∏Âú®Ë™ûÈü≥ÈáçÂª∫ÂíåÈ´òÈ†ªË´ßÊ≥¢ÊñπÈù¢ÈÅáÂà∞Âõ∞Èõ£ÔºåÂ∞éËá¥ÂñòÊÅØÂíåÂñòÊ∞£ÁöÑÂÅΩÂΩ±ÔºåÈôç‰Ωé‰∫ÜÈáçÂª∫Ë™ûÈü≥ÁöÑÂèØÊáÇÂ∫¶„ÄÇÈÄô‰∫õÊ®°ÂûãÂú®Ë®àÁÆó‰∏ä‰πüÂæàË¶ÅÊ±ÇÔºåË®±Â§öËß£Ê±∫ÊñπÊ°àÂÉÖÈôêÊñºÁî¢ÁîüÂØ¨È†ªÈ†ªÁéáÁØÑÂúçÁöÑËº∏Âá∫ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Â∞àÊ•≠ÊáâÁî®‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Hi-ResLDMÔºå‰∏ÄÁ®ÆÂü∫ÊñºÊΩõÂú®Êì¥Êï£ÁöÑÊñ∞ÂûãÁîüÊàêÊ®°ÂûãÔºåÊó®Âú®Ê∂àÈô§Â§öÈáçÂ§±Áúü‰∏¶Â∞áË™ûÈü≥ÈåÑÈü≥ÊÅ¢Âæ©Âà∞ÈåÑÈü≥ÂÆ§ÂìÅË≥™ÔºåÊé°Ê®£ÁéáÁÇ∫ 48kHz„ÄÇÊàëÂÄëÂ∞á Hi-ResLDM ËàáÂà©Áî® GAN ÂíåÊ¢ù‰ª∂ÊµÅÂåπÈÖç (CFM) ÁµÑ‰ª∂ÁöÑÊúÄÊñ∞ÊñπÊ≥ïÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåË≠âÊòé‰∫ÜÂú®ÂÜçÁîüÈ´òÈ†ªÂ∏∂Á¥∞ÁØÄÊñπÈù¢ÁöÑÂçìË∂äÊÄßËÉΩ„ÄÇHi-ResLDM ‰∏çÂÉÖÂú®Èùû‰æµÂÖ•ÊÄßÊåáÊ®ô‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÔºåËÄå‰∏îÂú®‰∫∫È°ûË©ï‰º∞‰∏≠‰πüÂßãÁµÇÂ¶Ç‰∏ÄÂú∞ÂèóÂà∞ÈùíÁùûÔºå‰∏¶‰∏îÂú®‰æµÂÖ•ÊÄßË©ï‰º∞‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫È´òËß£ÊûêÂ∫¶Ë™ûÈü≥ÈÇÑÂéüÁöÑÁêÜÊÉ≥ÈÅ∏Êìá„ÄÇ

##### **Semformer: Transformer Language Models with Semantic Planning**
2409.11143v1 by Yongjing Yin, Junran Ding, Kai Song, Yue Zhang

Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.

ÊëòË¶ÅÔºöÂú®Áï∂ÂâçÁöÑË™ûË®ÄÊ®°Âûã‰∏≠Ôºå‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨ÊòØ‰∏ªÂ∞éÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂú®Ë®ìÁ∑¥ÈöéÊÆµÔºåÊ®°ÂûãÊé°Áî®ÊïôÂ∏´Âº∑Âà∂Ê≥ïÔºåÊ†πÊìöÊâÄÊúâÂâç‰∏ÄÂÄãÁöÑÁúüÂØ¶Ë©ûÂΩô‰æÜÈ†êÊ∏¨Ë©ûÂΩô„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÈÄôÁ®ÆÊñπÊ≥ïÊúÉÁî¢ÁîüÊç∑ÂæëÔºåÂà©Áî®Â∑≤Êè≠Èú≤ÁöÑÂâçÁ∂¥‰æÜËôõÂÅáÂú∞Á¨¶ÂêàÂæåÁ∫åÁöÑË©ûÂΩôÔºåÊΩõÂú®ÊúÉÂç±ÂÆ≥‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨Âô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π SemformerÔºå‰∏ÄÁ®ÆË®ìÁ∑¥ Transformer Ë™ûË®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ïÔºåÊòéÁ¢∫Âú∞Âª∫ÊßãÂõûÊáâÁöÑË™ûÊÑèË¶èÂäÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàóË¶èÂäÉË©ûÂΩôÁ¥çÂÖ•ÂâçÁ∂¥ÔºåÂºïÂ∞éË¶èÂäÉË©ûÂΩôÁöÑË°®ÂæµÂéªÈ†êÊ∏¨ÂõûÊáâÁöÑÊΩõÂú®Ë™ûÊÑèË°®ÂæµÔºåÈÄô‰∫õË°®ÂæµÊòØÁî±Ëá™ÂãïÁ∑®Á¢ºÂô®Ë™òÂ∞éÁöÑ„ÄÇÂú®‰∏ÄÂÄãÊúÄÂ∞èÁöÑË¶èÂäÉ‰ªªÂãôÔºàÂç≥ÂúñÂΩ¢Ë∑ØÂæëÂ∞ãÊâæÔºâ‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæÂá∫Êé•ËøëÂÆåÁæéÁöÑÊïàËÉΩÔºå‰∏¶ÊúâÊïàÂú∞Ê∏õËºïÊç∑ÂæëÂ≠∏ÁøíÔºåÈÄôÊòØÊ®ôÊ∫ñË®ìÁ∑¥ÊñπÊ≥ïÂíåÂü∫Á∑öÊ®°ÂûãÁÑ°Ê≥ïÈÅîÊàêÁöÑÂ£ØËàâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÈ†≠ÈñãÂßã‰ΩøÁî® 1.25 ÂÑÑÂÄãÂèÉÊï∏È†êË®ìÁ∑¥ SemformerÔºåÈÄèÈÅéÂõ∞ÊÉëÂ∫¶„ÄÅË™ûÂ¢ÉÂ≠∏ÁøíÂíåÂú®ÊëòË¶Å‰ªªÂãô‰∏äÁöÑÂæÆË™ø‰æÜË≠âÊòéÂÖ∂ÂäüÊïà„ÄÇ

##### **Learning Generalized Hamiltonians using fully Symplectic Mappings**
2409.11138v1 by Harsh Choudhary, Chandan Gupta, Vyacheslav kungrutsev, Melvin Leok, Georgios Korpas

Many important physical systems can be described as the evolution of a
Hamiltonian system, which has the important property of being conservative,
that is, energy is conserved throughout the evolution. Physics Informed Neural
Networks and in particular Hamiltonian Neural Networks have emerged as a
mechanism to incorporate structural inductive bias into the NN model. By
ensuring physical invariances are conserved, the models exhibit significantly
better sample complexity and out-of-distribution accuracy than standard NNs.
Learning the Hamiltonian as a function of its canonical variables, typically
position and velocity, from sample observations of the system thus becomes a
critical task in system identification and long-term prediction of system
behavior. However, to truly preserve the long-run physical conservation
properties of Hamiltonian systems, one must use symplectic integrators for a
forward pass of the system's simulation. While symplectic schemes have been
used in the literature, they are thus far limited to situations when they
reduce to explicit algorithms, which include the case of separable Hamiltonians
or augmented non-separable Hamiltonians. We extend it to generalized
non-separable Hamiltonians, and noting the self-adjoint property of symplectic
integrators, we bypass computationally intensive backpropagation through an ODE
solver. We show that the method is robust to noise and provides a good
approximation of the system Hamiltonian when the state variables are sampled
from a noisy observation. In the numerical results, we show the performance of
the method concerning Hamiltonian reconstruction and conservation, indicating
its particular advantage for non-separable systems.

ÊëòË¶ÅÔºöË®±Â§öÈáçË¶ÅÁöÑÁâ©ÁêÜÁ≥ªÁµ±ÈÉΩÂèØ‰ª•ÊèèËø∞ÁÇ∫ÂìàÂØÜÈ†ìÁ≥ªÁµ±ÁöÑÊºîÂåñÔºåÂÆÉÂÖ∑ÊúâÂÆàÊÅÜÁöÑÈáçË¶ÅÊÄßË≥™Ôºå‰πüÂ∞±ÊòØË™™ÔºåËÉΩÈáèÂú®Êï¥ÂÄãÊºîÂåñÈÅéÁ®ã‰∏≠ÂÆàÊÅÜ„ÄÇÁâ©ÁêÜË®äÊÅØÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÁâπÂà•ÊòØÂìàÂØÜÈ†ìÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂ∑≤ÊàêÁÇ∫Â∞áÁµêÊßãÊ≠∏Á¥çÂÅèÂ∑ÆÁ¥çÂÖ•Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑ‰∏ÄÁ®ÆÊ©üÂà∂„ÄÇÈÄöÈÅéÁ¢∫‰øùÁâ©ÁêÜ‰∏çËÆäÈáèÂÆàÊÅÜÔºåÈÄô‰∫õÊ®°ÂûãÂ±ïÁèæÂá∫ÊØîÊ®ôÊ∫ñÁ•ûÁ∂ìÁ∂≤Ë∑ØÈ°ØËëóÊõ¥Â•ΩÁöÑÊ®£Êú¨Ë§áÈõúÂ∫¶ÂíåÂàÜ‰ΩàÂ§ñÊ∫ñÁ¢∫Â∫¶„ÄÇÂæûÁ≥ªÁµ±ÁöÑÊ®£Êú¨ËßÄÊ∏¨‰∏≠Â≠∏ÁøíÂìàÂØÜÈ†ìÈáè‰ΩúÁÇ∫ÂÖ∂Ê≠£ÂâáËÆäÊï∏ÔºàÈÄöÂ∏∏ÊòØ‰ΩçÁΩÆÂíåÈÄüÂ∫¶ÔºâÁöÑÂáΩÊï∏ÔºåÂõ†Ê≠§ÊàêÁÇ∫Á≥ªÁµ±Ëæ®Ë≠òÂíåÁ≥ªÁµ±Ë°åÁÇ∫Èï∑ÊúüÈ†êÊ∏¨‰∏≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÁÇ∫‰∫ÜÁúüÊ≠£‰øùÁïôÂìàÂØÜÈ†ìÁ≥ªÁµ±ÁöÑÈï∑ÊúüÁâ©ÁêÜÂÆàÊÅÜÊÄßË≥™ÔºåÂøÖÈ†àÂ∞çÁ≥ªÁµ±Ê®°Êì¨ÁöÑÊ≠£ÂêëÂÇ≥ÈÅû‰ΩøÁî®ËæõÁ©çÂàÜÂô®„ÄÇÈõñÁÑ∂ÊñáÁçª‰∏≠Â∑≤Á∂ì‰ΩøÁî®‰∫ÜËæõÊ†ºÂºèÔºå‰ΩÜÂà∞ÁõÆÂâçÁÇ∫Ê≠¢ÔºåÂÆÉÂÄëÂÉÖÈôêÊñºÂÆÉÂÄëÁ∞°ÂåñÁÇ∫ÊòéÁ¢∫ÊºîÁÆóÊ≥ïÁöÑÊÉÖÊ≥ÅÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÂèØÂàÜÈõ¢ÂìàÂØÜÈ†ìÈáèÊàñÊì¥Â¢ûÁöÑ‰∏çÂèØÂàÜÈõ¢ÂìàÂØÜÈ†ìÈáè„ÄÇÊàëÂÄëÂ∞áÂÖ∂Êì¥Â±ïÂà∞Âª£Áæ©ÁöÑ‰∏çÂèØÂàÜÈõ¢ÂìàÂØÜÈ†ìÈáèÔºå‰∏¶Ê≥®ÊÑèÂà∞ËæõÁ©çÂàÜÂô®ÁöÑËá™‰º¥Èö®ÊÄßË≥™ÔºåÊàëÂÄëÁπûÈÅéÈÄèÈÅéÂ∏∏ÂæÆÂàÜÊñπÁ®ãÊ±ÇËß£Âô®ÈÄ≤Ë°åË®àÁÆóÂØÜÈõÜÁöÑÂèçÂêëÂÇ≥Êí≠„ÄÇÊàëÂÄëË°®ÊòéÔºåË©≤ÊñπÊ≥ïÂ∞çÈõúË®äÂÖ∑ÊúâÈ≠ØÊ£íÊÄßÔºå‰∏¶‰∏îÁï∂ÁãÄÊÖãËÆäÊï∏ÂæûÈõúË®äËßÄÊ∏¨‰∏≠ÂèñÊ®£ÊôÇÔºåÂèØ‰ª•Êèê‰æõÁ≥ªÁµ±ÂìàÂØÜÈ†ìÈáèÁöÑËâØÂ•ΩËøë‰ººÂÄº„ÄÇÂú®Êï∏ÂÄºÁµêÊûú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜË©≤ÊñπÊ≥ïÂú®ÂìàÂØÜÈ†ìÈáèÈáçÂª∫ÂíåÂÆàÊÅÜÊñπÈù¢ÁöÑÊïàËÉΩÔºåË°®ÊòéÂÖ∂Â∞ç‰∏çÂèØÂàÜÈõ¢Á≥ªÁµ±ÁöÑÁâπÊÆäÂÑ™Âã¢„ÄÇ

##### **Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models**
2409.11136v1 by Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel

Instruction-tuned language models (LM) are able to respond to imperative
commands, providing a more natural user interface compared to their base
counterparts. In this work, we present Promptriever, the first retrieval model
able to be prompted like an LM. To train Promptriever, we curate and release a
new instance-level instruction training set from MS MARCO, spanning nearly 500k
instances. Promptriever not only achieves strong performance on standard
retrieval tasks, but also follows instructions. We observe: (1) large gains
(reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR /
+3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical
choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR),
and (3) the ability to perform hyperparameter search via prompting to reliably
improve retrieval performance (+1.4 average increase on BEIR). Promptriever
demonstrates that retrieval models can be controlled with prompts on a
per-query basis, setting the stage for future work aligning LM prompting
techniques with information retrieval.

ÊëòË¶ÅÔºöÊåá‰ª§Ë™øÊï¥Ë™ûË®ÄÊ®°Âûã (LM) ËÉΩÂ§†ÂõûÊáâÂëΩ‰ª§ÂºèÊåá‰ª§ÔºåËàáÂÖ∂Âü∫Á§éÂ∞çÊáâÁâ©Áõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥Ëá™ÁÑ∂ÁöÑ‰ΩøÁî®ËÄÖ‰ªãÈù¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü PromptrieverÔºåÁ¨¨‰∏ÄÂÄãËÉΩÂ§†ÂÉè LM ‰∏ÄÊ®£ÊèêÁ§∫ÁöÑÊ™¢Á¥¢Ê®°Âûã„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥ PromptrieverÔºåÊàëÂÄëÂæû MS MARCO Êï¥ÁêÜ‰∏¶ÈáãÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÂØ¶‰æãÂ±§Á¥öÊåá‰ª§Ë®ìÁ∑¥ÈõÜÔºåÊ∂µËìã‰∫ÜÂ∞áËøë 50 Ëê¨ÂÄãÂØ¶‰æã„ÄÇPromptriever ‰∏çÂÉÖÂú®Ê®ôÊ∫ñÊ™¢Á¥¢‰ªªÂãô‰∏≠Áç≤ÂæóÂº∑ÂãÅÁöÑÊïàËÉΩÔºåÈÇÑËÉΩÈÅµÂæ™Êåá‰ª§„ÄÇÊàëÂÄëËßÄÂØüÂà∞Ôºö(1) Âú®ÈÅµÂæ™Ë©≥Á¥∞Áõ∏ÈóúÊÄßÊåá‰ª§ÊñπÈù¢Áç≤ÂæóÂ§ßÂπÖÊèêÂçáÔºàÈÅîÂà∞ SoTAÔºåÂú® FollowIR ‰∏äÁöÑ p-MRR Â¢ûÂä† +14.3 / nDCG Â¢ûÂä† +3.1ÔºâÔºå(2) Âú®Êü•Ë©¢+Êåá‰ª§‰∏≠ÁöÑÂ≠óÂΩôÈÅ∏Êìá/Êé™Ëæ≠ÊñπÈù¢Â§ßÂπÖÊèêÂçáÂÅ•Â£ØÊÄßÔºàÂú® InstructIR ‰∏äÁöÑ Robustness@10 Â¢ûÂä† +12.9ÔºâÔºå‰ª•Âèä (3) ËÉΩÂ§†ÈÄèÈÅéÊèêÁ§∫Âü∑Ë°åË∂ÖÂèÉÊï∏ÊêúÂ∞ãÔºå‰ª•ÂèØÈù†Âú∞ÊîπÂñÑÊ™¢Á¥¢ÊïàËÉΩÔºàÂú® BEIR ‰∏äÂπ≥ÂùáÂ¢ûÂä† +1.4Ôºâ„ÄÇPromptriever Ë≠âÊòé‰∫ÜÊ™¢Á¥¢Ê®°ÂûãÂèØ‰ª•Áî®ÊèêÁ§∫Âú®ÊØèÂÄãÊü•Ë©¢ÁöÑÂü∫Á§é‰∏äÈÄ≤Ë°åÊéßÂà∂ÔºåÁÇ∫Êú™‰æÜÁöÑÂ∑•‰ΩúÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÂ∞á LM ÊèêÁ§∫ÊäÄË°ìËàáË≥áË®äÊ™¢Á¥¢ÁµêÂêàËµ∑‰æÜ„ÄÇ

##### **Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach**
2409.11123v1 by Debarpan Bhattacharya, Amir H. Poorjam, Deepak Mittal, Sriram Ganapathy

The recent advancements in artificial intelligence (AI), with the release of
several large models having only query access, make a strong case for
explainability of deep models in a post-hoc gradient free manner. In this
paper, we propose a framework, named distillation aided explainability (DAX),
that attempts to generate a saliency-based explanation in a model agnostic
gradient free application. The DAX approach poses the problem of explanation in
a learnable setting with a mask generation network and a distillation network.
The mask generation network learns to generate the multiplier mask that finds
the salient regions of the input, while the student distillation network aims
to approximate the local behavior of the black-box model. We propose a joint
optimization of the two networks in the DAX framework using the locally
perturbed input samples, with the targets derived from input-output access to
the black-box model. We extensively evaluate DAX across different modalities
(image and audio), in a classification setting, using a diverse set of
evaluations (intersection over union with ground truth, deletion based and
subjective human evaluation based measures) and benchmark it with respect to
$9$ different methods. In these evaluations, the DAX significantly outperforms
the existing approaches on all modalities and evaluation metrics.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰ª•ÂèäÁôºÂ∏É‰∫ÜÂπæÁ®ÆÂÉÖÂÖ∑ÊúâÊü•Ë©¢Â≠òÂèñÊ¨äÁöÑÂ§ßÂûãÊ®°ÂûãÔºåÈÄôÁÇ∫‰∫ãÂæåÁÑ°Ê¢ØÂ∫¶Ê∑±Â∫¶Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊèê‰æõ‰∫ÜÂº∑ÊúâÂäõÁöÑË´ñÊìö„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫Ëí∏È§æËºîÂä©ÂèØËß£ÈáãÊÄß (DAX) ÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂòóË©¶Âú®ËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÁÑ°Ê¢ØÂ∫¶ÊáâÁî®‰∏≠Áî¢ÁîüÂü∫ÊñºÈ°ØËëóÊÄßÁöÑËß£Èáã„ÄÇDAX ÊñπÊ≥ï‰ª•ÂèØÂ≠∏ÁøíÁöÑË®≠ÂÆöÊèêÂá∫‰∫ÜËß£ÈáãÂïèÈ°åÔºå‰∏¶‰ΩøÁî®ÈÅÆÁΩ©ÁîüÊàêÁ∂≤Ë∑ØÂíåËí∏È§æÁ∂≤Ë∑Ø„ÄÇÈÅÆÁΩ©ÁîüÊàêÁ∂≤Ë∑ØÊúÉÂ≠∏ÁøíÁî¢ÁîüÂÄçÁéáÈÅÆÁΩ©Ôºå‰ª•ÊâæÂá∫Ëº∏ÂÖ•ÁöÑÈ°ØËëóÂçÄÂüüÔºåËÄåÂ≠∏ÁîüËí∏È§æÁ∂≤Ë∑ØÂâáÊó®Âú®Ëøë‰ººÈªëÁõíÊ®°ÂûãÁöÑÂ±ÄÈÉ®Ë°åÁÇ∫„ÄÇÊàëÂÄëÊèêÂá∫Âú® DAX Ê°ÜÊû∂‰∏≠Â∞çÂÖ©ÂÄãÁ∂≤Ë∑ØÈÄ≤Ë°åËÅØÂêàÊúÄ‰Ω≥ÂåñÔºå‰ΩøÁî®Â±ÄÈÉ®ÊìæÂãïÁöÑËº∏ÂÖ•Ê®£Êú¨ÔºåÁõÆÊ®ôÊ∫êËá™Â∞çÈªëÁõíÊ®°ÂûãÁöÑËº∏ÂÖ•Ëº∏Âá∫Â≠òÂèñ„ÄÇÊàëÂÄëÂú®ÂàÜÈ°ûË®≠ÂÆö‰∏≠Ôºå‰ΩøÁî®Â§öÁ®ÆË©ï‰º∞ÔºàËàáÁúüÂØ¶ÊÉÖÊ≥ÅÁöÑ‰∫§ÈõÜ‰∏¶ÈõÜ„ÄÅÂü∫ÊñºÂà™Èô§ÂíåÂü∫Êñº‰∏ªËßÄ‰∫∫È°ûË©ï‰º∞ÁöÑÊ∏¨ÈáèÔºâÂª£Ê≥õË©ï‰º∞‰∏çÂêåÊ®°ÂºèÔºàÂΩ±ÂÉèÂíåÈü≥Ë®äÔºâ‰∏≠ÁöÑ DAXÔºå‰∏¶ÈáùÂ∞ç $9$ Á®Æ‰∏çÂêåÁöÑÊñπÊ≥ïÂ∞çÂÖ∂ÈÄ≤Ë°åË©ïÈáè„ÄÇÂú®ÈÄô‰∫õË©ï‰º∞‰∏≠ÔºåDAX Âú®ÊâÄÊúâÊ®°ÂºèÂíåË©ï‰º∞ÊåáÊ®ô‰∏äÈÉΩÈ°ØËëóÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇ

##### **Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection**
2409.11114v1 by Bo Liu, Liming Zhan, Yujie Feng, Zexin Lu, Chengqiang Xie, Lei Xue, Xiao-Ming Wu, Albert Y. S. Lam

In the realm of task-oriented dialogue systems, a robust intent detection
mechanism must effectively handle malformed utterances encountered in
real-world scenarios. This study presents a novel fine-tuning framework for
large language models (LLMs) aimed at enhancing in-distribution (ID) intent
classification and out-of-distribution (OOD) intent detection, which utilizes
semantic matching with prototypes derived from ID class names. By harnessing
the highly distinguishable representations of LLMs, we construct semantic
prototypes for each ID class using a diversity-grounded prompt tuning approach.
We rigorously test our framework in a challenging OOD context, where ID and OOD
classes are semantically close yet distinct, referred to as \emph{near} OOD
detection. For a thorough assessment, we benchmark our method against the
prevalent fine-tuning approaches. The experimental findings reveal that our
method demonstrates superior performance in both few-shot ID intent
classification and near-OOD intent detection tasks.

ÊëòË¶ÅÔºöÂú®Èù¢Âêë‰ªªÂãôÁöÑÂ∞çË©±Á≥ªÁµ±È†òÂüü‰∏≠ÔºåÂº∑ÂÅ•ÁöÑÊÑèÂúñÂÅµÊ∏¨Ê©üÂà∂ÂøÖÈ†àÊúâÊïàÂú∞ËôïÁêÜÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÈÅáÂà∞ÁöÑÊ†ºÂºèÈåØË™§ÁöÑË™ûÂè•„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈáùÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñ∞ÂæÆË™øÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∑ÂàÜÂ∏ÉÂÖß (ID) ÊÑèÂúñÂàÜÈ°ûÂíåÂàÜÂ∏ÉÂ§ñ (OOD) ÊÑèÂúñÂÅµÊ∏¨ÔºåÂÆÉÂà©Áî®Ë™ûÁæ©ÂåπÈÖçËàáÂæû ID È°ûÂà•ÂêçÁ®±Ë°çÁîüÁöÑÂéüÂûã„ÄÇÈÄèÈÅéÂà©Áî® LLM Ê•µÂÖ∑ÂçÄÂà•ÊÄßÁöÑË°®Á§∫ÔºåÊàëÂÄë‰ΩøÁî®Âü∫ÊñºÂ§öÊ®£ÊÄßÁöÑÊèêÁ§∫Ë™øÊï¥ÊñπÊ≥ïÁÇ∫ÊØèÂÄã ID È°ûÂà•Âª∫ÊßãË™ûÁæ©ÂéüÂûã„ÄÇÊàëÂÄëÂú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ OOD Áí∞Â¢É‰∏≠Âö¥Ê†ºÊ∏¨Ë©¶ÊàëÂÄëÁöÑÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ ID Âíå OOD È°ûÂà•Âú®Ë™ûÁæ©‰∏äÊé•Ëøë‰ΩÜÂçª‰∏çÂêåÔºåÁ®±ÁÇ∫\emph{near} OOD ÂÅµÊ∏¨„ÄÇÁÇ∫‰∫ÜÈÄ≤Ë°åÂæπÂ∫ïÁöÑË©ï‰º∞ÔºåÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÊ®°ÂûãËàáÊµÅË°åÁöÑÂæÆË™øÊñπÊ≥ïÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â∞ëÊ¨°ÂòóË©¶ÁöÑ ID ÊÑèÂúñÂàÜÈ°ûÂíå near-OOD ÊÑèÂúñÂÅµÊ∏¨‰ªªÂãô‰∏≠ÈÉΩÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ

##### **Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games**
2409.11112v1 by Matƒ´ss Rikters, Sanita Reinsone

At the beginning of 2022, a simplistic word-guessing game took the world by
storm and was further adapted to many languages beyond the original English
version. In this paper, we examine the strategies of daily word-guessing game
players that have evolved during a period of over two years. A survey gathered
from 25% of frequent players reveals their strategies and motivations for
continuing the daily journey. We also explore the capability of several popular
open-access large language model systems and open-source models at
comprehending and playing the game in two different languages. Results
highlight the struggles of certain models to maintain correct guess length and
generate repetitions, as well as hallucinations of non-existent words and
inflections.

ÊëòË¶ÅÔºö2022 Âπ¥ÂàùÔºå‰∏ÄÊ¨æÁ∞°ÂåñÁöÑÁåúÂ≠óÈÅäÊà≤Â∏≠Êç≤ÂÖ®ÁêÉÔºå‰∏¶Âú®ÂéüÁâàÁöÑËã±ÊñáÁâàÊú¨‰πãÂ§ñË¢´ÊîπÁ∑®ÁÇ∫Ë®±Â§öË™ûË®Ä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂú®ÂÖ©Âπ¥Â§öÁöÑÊôÇÈñìË£°ÊºîËÆäÂá∫ÁöÑÊØèÊó•ÁåúÂ≠óÈÅäÊà≤Áé©ÂÆ∂ÁöÑÁ≠ñÁï•„ÄÇ‰∏ÄÈ†ÖË™øÊü•Êî∂ÈõÜ‰∫Ü 25% ÁöÑÂ∏∏ÂÆ¢Áé©ÂÆ∂ÔºåÊè≠Á§∫‰∫Ü‰ªñÂÄëÁπºÁ∫åÊØèÊó•ÊóÖÁ®ãÁöÑÁ≠ñÁï•ÂíåÂãïÊ©ü„ÄÇÊàëÂÄëÈÇÑÊé¢Ë®é‰∫ÜÂπæÁ®ÆÊµÅË°åÁöÑÈñãÊîæÂºèÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁ≥ªÁµ±ÂíåÈñãÊ∫êÊ®°ÂûãÂú®ÁêÜËß£ÂíåÁé©ÂÖ©Á®Æ‰∏çÂêåË™ûË®ÄÁöÑÈÅäÊà≤ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁµêÊûúÂº∑Ë™ø‰∫ÜÊüê‰∫õÊ®°ÂûãÂú®‰øùÊåÅÊ≠£Á¢∫ÁåúÊ∏¨Èï∑Â∫¶ÂíåÁî¢ÁîüÈáçË§á‰ª•ÂèäÂ∞ç‰∏çÂ≠òÂú®ÁöÑÂñÆË©ûÂíåËÆäÂΩ¢ÁöÑÂπªË¶∫ÊñπÈù¢ÈÅáÂà∞ÁöÑÂõ∞Èõ£„ÄÇ

##### **MonoKAN: Certified Monotonic Kolmogorov-Arnold Network**
2409.11078v1 by Alejandro Polo-Molina, David Alfaya, Jose Portela

Artificial Neural Networks (ANNs) have significantly advanced various fields
by effectively recognizing patterns and solving complex problems. Despite these
advancements, their interpretability remains a critical challenge, especially
in applications where transparency and accountability are essential. To address
this, explainable AI (XAI) has made progress in demystifying ANNs, yet
interpretability alone is often insufficient. In certain applications, model
predictions must align with expert-imposed requirements, sometimes exemplified
by partial monotonicity constraints. While monotonic approaches are found in
the literature for traditional Multi-layer Perceptrons (MLPs), they still face
difficulties in achieving both interpretability and certified partial
monotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based
on learnable activation functions parametrized as splines, has been proposed as
a more interpretable alternative to MLPs. Building on this, we introduce a
novel ANN architecture called MonoKAN, which is based on the KAN architecture
and achieves certified partial monotonicity while enhancing interpretability.
To achieve this, we employ cubic Hermite splines, which guarantee monotonicity
through a set of straightforward conditions. Additionally, by using positive
weights in the linear combinations of these splines, we ensure that the network
preserves the monotonic relationships between input and output. Our experiments
demonstrate that MonoKAN not only enhances interpretability but also improves
predictive performance across the majority of benchmarks, outperforming
state-of-the-art monotonic MLP approaches.

ÊëòË¶ÅÔºö‰∫∫Â∑•Á•ûÁªèÁ∂≤Ë∑Ø (ANN) ÈÄèÈÅéÊúâÊïàËæ®Ë≠òÊ®°ÂºèÂíåËß£Ê±∫Ë§áÈõúÂïèÈ°åÔºåÂú®ÂêÑÂÄãÈ†òÂüüÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õÈÄ≤Â±ïÔºåÂÖ∂ÂèØËß£ÈáãÊÄß‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÈÄèÊòéÂ∫¶ÂíåÂïèË≤¨Âà∂Ëá≥ÈóúÈáçË¶ÅÁöÑÊáâÁî®‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂèØËß£Èáã AI (XAI) Â∑≤Âú®Á†¥Ëß£ ANN ÊñπÈù¢ÂèñÂæóÈÄ≤Â±ïÔºå‰ΩÜÂÉÖÊúâÂèØËß£ÈáãÊÄßÈÄöÂ∏∏ÊòØ‰∏çÂ§†ÁöÑ„ÄÇÂú®ÁâπÂÆöÊáâÁî®‰∏≠ÔºåÊ®°ÂûãÈ†êÊ∏¨ÂøÖÈ†àÁ¨¶ÂêàÂ∞àÂÆ∂ÊñΩÂä†ÁöÑË¶ÅÊ±ÇÔºåÊúâÊôÇ‰ª•ÈÉ®ÂàÜÂñÆË™øÊÄßÁ¥ÑÊùüÁÇ∫‰æã„ÄÇÈõñÁÑ∂Âú®ÂÇ≥Áµ±Â§öÂ±§ÊÑüÁü•Âô® (MLP) ÁöÑÊñáÁçª‰∏≠ÁôºÁèæ‰∫ÜÂñÆË™øÊñπÊ≥ïÔºå‰ΩÜÂÆÉÂÄëÂú®ÂØ¶ÁèæÂèØËß£ÈáãÊÄßÂíåÁ∂ìÈÅéË™çË≠âÁöÑÈÉ®ÂàÜÂñÆË™øÊÄßÊñπÈù¢‰ªçÁÑ∂Èù¢Ëá®Âõ∞Èõ£„ÄÇÊúÄËøëÔºåÂü∫ÊñºÂèØÂ≠∏ÁøíÁöÑÊøÄÊ¥ªÂáΩÊï∏ÂèÉÊï∏ÂåñÁÇ∫Ê®£Ê¢ùÁöÑ Kolmogorov-Arnold Á∂≤Ë∑Ø (KAN) Êû∂ÊßãÂ∑≤Ë¢´ÊèêÂá∫‰ΩúÁÇ∫ MLP ÁöÑÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂú®Ê≠§Âü∫Á§é‰∏äÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ MonoKAN ÁöÑÊñ∞Âûã ANN Êû∂ÊßãÔºåÂÆÉÂü∫Êñº KAN Êû∂ÊßãÔºå‰∏¶Âú®Â¢ûÂº∑ÂèØËß£ÈáãÊÄßÁöÑÂêåÊôÇÂØ¶Áèæ‰∫ÜÁ∂ìÈÅéË™çË≠âÁöÑÈÉ®ÂàÜÂñÆË™øÊÄß„ÄÇÁÇ∫ÈÅîÊàêÊ≠§ÁõÆÁöÑÔºåÊàëÂÄëÊé°Áî®‰∏âÊ¨° Hermite Ê®£Ê¢ùÔºåÂÆÉÈÄèÈÅé‰∏ÄÁµÑÁ∞°ÂñÆÁöÑÊ¢ù‰ª∂‰æÜ‰øùË≠âÂñÆË™øÊÄß„ÄÇÊ≠§Â§ñÔºåÈÄèÈÅéÂú®ÈÄô‰∫õÊ®£Ê¢ùÁöÑÁ∑öÊÄßÁµÑÂêà‰∏≠‰ΩøÁî®Ê≠£Ê¨äÈáçÔºåÊàëÂÄëÁ¢∫‰øùÁ∂≤Ë∑Ø‰øùÁïôËº∏ÂÖ•ÂíåËº∏Âá∫‰πãÈñìÁöÑÂñÆË™øÈóú‰øÇ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåMonoKAN ‰∏çÂÉÖÂ¢ûÂº∑‰∫ÜÂèØËß£ÈáãÊÄßÔºåËÄå‰∏îÈÇÑÊîπÂñÑ‰∫ÜÂ§ßÂ§öÊï∏Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÈ†êÊ∏¨ÊïàËÉΩÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂñÆË™ø MLP ÊñπÊ≥ï„ÄÇ

##### **RoMath: A Mathematical Reasoning Benchmark in Romanian**
2409.11074v1 by Adrian Cosma, Ana-Maria Bucur, Emilian Radoi

Mathematics has long been conveyed through natural language, primarily for
human understanding. With the rise of mechanized mathematics and proof
assistants, there is a growing need to understand informal mathematical text,
yet most existing benchmarks focus solely on English, overlooking other
languages. This paper introduces RoMath, a Romanian mathematical reasoning
benchmark suite comprising three datasets: RoMath-Baccalaureate,
RoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical
domains and difficulty levels, aiming to improve non-English language models
and promote multilingual AI development. By focusing on Romanian, a
low-resource language with unique linguistic features, RoMath addresses the
limitations of Anglo-centric models and emphasizes the need for dedicated
resources beyond simple automatic translation. We benchmark several open-weight
language models, highlighting the importance of creating resources for
underrepresented languages. We make the code and dataset available.

ÊëòË¶ÅÔºöÊï∏Â≠∏Èï∑Êúü‰ª•‰æÜÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÂÇ≥ÈÅîÔºå‰∏ªË¶Å‰æõ‰∫∫È°ûÁêÜËß£„ÄÇÈö®ËëóÊ©üÊ¢∞ÂåñÊï∏Â≠∏ÂíåË≠âÊòéËºîÂä©ÁöÑËààËµ∑ÔºåÁêÜËß£ÈùûÊ≠£ÂºèÊï∏Â≠∏ÊñáÊú¨ÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÂä†Ôºå‰ΩÜÁèæÊúâÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶Â§ßÂ§öÂè™ÈóúÊ≥®Ëã±ÊñáÔºåÂøΩÁï•‰∫ÜÂÖ∂‰ªñË™ûË®Ä„ÄÇÊú¨Êñá‰ªãÁ¥π RoMathÔºå‰∏ÄÂÄãÁæÖÈ¶¨Â∞º‰∫ûÊï∏Â≠∏Êé®ÁêÜÂü∫Ê∫ñÊ∏¨Ë©¶Â•ó‰ª∂ÔºåÂåÖÂê´‰∏âÂÄãË≥áÊñôÈõÜÔºöRoMath-Baccalaureate„ÄÅRoMath-Competitions Âíå RoMath-SyntheticÔºåÊ∂µËìãÁØÑÂúçÂª£Ê≥õÁöÑÊï∏Â≠∏È†òÂüüÂíåÈõ£Â∫¶Á≠âÁ¥öÔºåÊó®Âú®ÊîπÂñÑÈùûËã±Ë™ûË™ûË®ÄÊ®°Âûã‰∏¶‰øÉÈÄ≤Â§öË™ûË®Ä AI ÈñãÁôº„ÄÇRoMath Â∞àÊ≥®ÊñºÁæÖÈ¶¨Â∞º‰∫ûË™ûÔºå‰∏ÄÁ®ÆÂÖ∑ÊúâÁç®ÁâπË™ûË®ÄÁâπÂæµÁöÑ‰ΩéË≥áÊ∫êË™ûË®ÄÔºåËß£Ê±∫‰∫Ü‰ª•Ëã±Ë™ûÁÇ∫‰∏≠ÂøÉÁöÑÊ®°ÂûãÁöÑÈôêÂà∂Ôºå‰∏¶Âº∑Ë™ø‰∫ÜÈô§‰∫ÜÁ∞°ÂñÆÁöÑËá™ÂãïÁøªË≠Ø‰πãÂ§ñÔºåÈúÄË¶ÅÂ∞àÈñÄÁöÑË≥áÊ∫ê„ÄÇÊàëÂÄëÂ∞çÂπæÂÄãÈñãÊîæÊ¨äÈáçÁöÑË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂº∑Ë™øÁÇ∫‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑË™ûË®ÄÂª∫Á´ãË≥áÊ∫êÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÊèê‰æõÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜ„ÄÇ

##### **Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression algorithms**
2409.11071v1 by Andrew Antonopoulos

This study was the 2nd part of my dissertation for my master degree and
compared the power consumption using the Comma-Separated-Values (CSV) and
parquet dataset format with the default floating point (32bit) and Nvidia mixed
precision (16bit and 32bit) while training a regression ML model. The same
custom PC as per the 1st part, which was dedicated to the classification
testing and analysis, was built to perform the experiments, and different ML
hyper-parameters, such as batch size, neurons, and epochs, were chosen to build
Deep Neural Networks (DNN). A benchmarking test with default hyper-parameter
values for the DNN was used as a reference, while the experiments used a
combination of different settings. The results were recorded in Excel, and
descriptive statistics were chosen to calculate the mean between the groups and
compare them using graphs and tables. The outcome was positive when using mixed
precision combined with specific hyper-parameters. Compared to the
benchmarking, optimising the regression models reduced the power consumption
between 7 and 11 Watts. The regression results show that while mixed precision
can help improve power consumption, we must carefully consider the
hyper-parameters. A high number of batch sizes and neurons will negatively
affect power consumption. However, this research required inferential
statistics, specifically ANOVA and T-test, to compare the relationship between
the means. The results reported no statistical significance between the means
in the regression tests and accepted H0. Therefore, choosing different ML
techniques and the Parquet dataset format will not improve the computational
power consumption and the overall ML carbon footprint. However, a more
extensive implementation with a cluster of GPUs can increase the sample size
significantly, as it is an essential factor and can change the outcome of the
statistical analysis.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊòØÊàëÁ¢©Â£´Ë´ñÊñáÁöÑÁ¨¨‰∫åÈÉ®ÂàÜÔºåÊØîËºÉ‰∫Ü‰ΩøÁî®ÈÄóËôüÂàÜÈöîÂÄº (CSV) Âíå Parquet Ë≥áÊñôÈõÜÊ†ºÂºèËàáÈ†êË®≠ÊµÆÈªûÊï∏ (32 ‰ΩçÂÖÉ) Âíå Nvidia Ê∑∑ÂêàÁ≤æÂ∫¶ (16 ‰ΩçÂÖÉÂíå 32 ‰ΩçÂÖÉ) Âú®Ë®ìÁ∑¥ÂõûÊ≠∏ ML Ê®°ÂûãÊôÇÁöÑÂäüËÄó„ÄÇËàáÁ¨¨‰∏ÄÈÉ®ÂàÜÁõ∏ÂêåÔºåÂ∞àÈñÄÁî®ÊñºÂàÜÈ°ûÊ∏¨Ë©¶ÂíåÂàÜÊûêÁöÑÂÆ¢Ë£ΩÂåñÂÄã‰∫∫ÈõªËÖ¶Áî®ÊñºÂü∑Ë°åÂØ¶È©óÔºå‰∏¶ÈÅ∏Êìá‰∏çÂêåÁöÑ ML Ë∂ÖÂèÉÊï∏Ôºå‰æãÂ¶ÇÊâπÊ¨°Â§ßÂ∞è„ÄÅÁ•ûÁ∂ìÂÖÉÂíåÊ¨°Êï∏Ôºå‰ª•Âª∫Á´ãÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN)„ÄÇ‰ΩøÁî®È†êË®≠ DNN Ë∂ÖÂèÉÊï∏ÂÄºÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶‰ΩúÁÇ∫ÂèÉËÄÉÔºåËÄåÂØ¶È©óÂâá‰ΩøÁî®‰∏çÂêåÁöÑË®≠ÂÆöÁµÑÂêà„ÄÇÁµêÊûúË®òÈåÑÂú® Excel ‰∏≠Ôºå‰∏¶ÈÅ∏ÊìáÊèèËø∞ÊÄßÁµ±Ë®àÊï∏Êìö‰æÜË®àÁÆóÁµÑÈñìÂπ≥ÂùáÂÄºÔºå‰∏¶‰ΩøÁî®ÂúñÂΩ¢ÂíåË°®Ê†ºÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÂêàÁâπÂÆöË∂ÖÂèÉÊï∏‰ΩøÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ÊôÇÔºåÁµêÊûúÁÇ∫Ê≠£Âêë„ÄÇËàáÂü∫Ê∫ñÊ∏¨Ë©¶Áõ∏ÊØîÔºåÊúÄ‰Ω≥ÂåñÂõûÊ≠∏Ê®°ÂûãÂèØÂ∞áÂäüËÄóÈôç‰Ωé 7 Âà∞ 11 Áì¶„ÄÇÂõûÊ≠∏ÁµêÊûúÈ°ØÁ§∫ÔºåÈõñÁÑ∂Ê∑∑ÂêàÁ≤æÂ∫¶ÊúâÂä©ÊñºÊîπÂñÑÂäüËÄóÔºå‰ΩÜÊàëÂÄëÂøÖÈ†à‰ªîÁ¥∞ËÄÉÊÖÆË∂ÖÂèÉÊï∏„ÄÇÂ§ßÈáèÁöÑÊâπÊ¨°Â§ßÂ∞èÂíåÁ•ûÁ∂ìÂÖÉÊúÉÂ∞çÂäüËÄóÁî¢ÁîüË≤†Èù¢ÂΩ±Èüø„ÄÇÁÑ∂ËÄåÔºåÊú¨Á†îÁ©∂ÈúÄË¶ÅÊé®Ë´ñÁµ±Ë®àÔºåÁâπÂà•ÊòØ ANOVA Âíå T Ê™¢ÂÆöÔºå‰ª•ÊØîËºÉÂπ≥ÂùáÂÄº‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÁµêÊûúÈ°ØÁ§∫ÂõûÊ≠∏Ê∏¨Ë©¶ÁöÑÂπ≥ÂùáÂÄº‰πãÈñìÊ≤íÊúâÁµ±Ë®àÈ°ØËëóÊÄßÔºå‰∏¶Êé•Âèó H0„ÄÇÂõ†Ê≠§ÔºåÈÅ∏Êìá‰∏çÂêåÁöÑ ML ÊäÄË°ìÂíå Parquet Ë≥áÊñôÈõÜÊ†ºÂºè‰∏çÊúÉÊîπÂñÑÈÅãÁÆóÂäüËÄóÂíåÊï¥È´î ML Á¢≥Ë∂≥Ë∑°„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî® GPU Áæ§ÈõÜÈÄ≤Ë°åÊõ¥Âª£Ê≥õÁöÑÂØ¶‰ΩúÂèØ‰ª•È°ØËëóÂ¢ûÂä†Ê®£Êú¨Â§ßÂ∞èÔºåÂõ†ÁÇ∫ÈÄôÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÂõ†Á¥†Ôºå‰∏¶‰∏îÂèØ‰ª•ÊîπËÆäÁµ±Ë®àÂàÜÊûêÁöÑÁµêÊûú„ÄÇ

##### **KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models**
2409.11057v1 by Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma

The bottleneck associated with the key-value(KV) cache presents a significant
challenge during the inference processes of large language models. While depth
pruning accelerates inference, it requires extensive recovery training, which
can take up to two weeks. On the other hand, width pruning retains much of the
performance but offers slight speed gains. To tackle these challenges, we
propose KVPruner to improve model efficiency while maintaining performance. Our
method uses global perplexity-based analysis to determine the importance ratio
for each block and provides multiple strategies to prune non-essential KV
channels within blocks. Compared to the original model, KVPruner reduces
runtime memory usage by 50% and boosts throughput by over 35%. Additionally,
our method requires only two hours of LoRA fine-tuning on small datasets to
recover most of the performance.

ÊëòË¶ÅÔºöËàáÈçµÂÄº (KV) Âø´ÂèñÁõ∏ÈóúÁöÑÁì∂È†∏Âú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊé®Ë´ñÈÅéÁ®ã‰∏≠ÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂÑòÁÆ°Ê∑±Â∫¶Ââ™ÊûùÂèØ‰ª•Âä†ÈÄüÊé®Ë´ñÔºå‰ΩÜÈúÄË¶ÅÂª£Ê≥õÁöÑÊÅ¢Âæ©Ë®ìÁ∑¥ÔºåÈÄôÂèØËÉΩÈúÄË¶ÅÈï∑ÈÅîÂÖ©ÈÄ±ÁöÑÊôÇÈñì„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂØ¨Â∫¶Ââ™Êûù‰øùÁïô‰∫ÜÂ§ßÈÉ®ÂàÜÊïàËÉΩÔºå‰ΩÜÊèê‰æõ‰∫ÜËºïÂæÆÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ KVPrunerÔºå‰ª•Âú®Á∂≠ÊåÅÊïàËÉΩÁöÑÂêåÊôÇÊèêÂçáÊ®°ÂûãÊïàÁéá„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®Âü∫ÊñºÂÖ®Â±ÄÂõ∞ÊÉëÂ∫¶ÁöÑÂàÜÊûê‰æÜÁ¢∫ÂÆöÊØèÂÄãÂçÄÂ°äÁöÑÈáçË¶ÅÊÄßÊØî‰æãÔºå‰∏¶Êèê‰æõÂ§öÁ®ÆÁ≠ñÁï•‰æÜÂâ™Èô§ÂçÄÂ°ä‰∏≠ÈùûÂøÖË¶ÅÁöÑ KV ÈÄöÈÅì„ÄÇËàáÂéüÂßãÊ®°ÂûãÁõ∏ÊØîÔºåKVPruner Â∞áÂü∑Ë°åÊôÇÈñìË®òÊÜ∂È´î‰ΩøÁî®ÈáèÊ∏õÂ∞ë‰∫Ü 50%Ôºå‰∏¶Â∞áÂêûÂêêÈáèÊèêÂçá‰∫Ü 35% ‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂè™ÈúÄË¶ÅÂú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂÖ©Â∞èÊôÇÁöÑ LoRA ÂæÆË™øÔºåÂ∞±ËÉΩÂ§†ÊÅ¢Âæ©Â§ßÈÉ®ÂàÜÊïàËÉΩ„ÄÇ

##### **Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts**
2409.11056v1 by Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, Xiongwei Han

With the advent of Large Language Models (LLMs), generating rule-based data
for real-world applications has become more accessible. Due to the inherent
ambiguity of natural language and the complexity of rule sets, especially in
long contexts, LLMs often struggle to follow all specified rules, frequently
omitting at least one. To enhance the reasoning and understanding of LLMs on
long and complex contexts, we propose a novel prompting strategy Multi-Lingual
Prompt, namely MLPrompt, which automatically translates the error-prone rule
that an LLM struggles to follow into another language, thus drawing greater
attention to it. Experimental results on public datasets across various tasks
have shown MLPrompt can outperform state-of-the-art prompting methods such as
Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we
introduce a framework integrating MLPrompt with an auto-checking mechanism for
structured data generation, with a specific case study in text-to-MIP
instances. Further, we extend the proposed framework for text-to-SQL to
demonstrate its generation ability towards structured data synthesis.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÁîüÊàêÂü∫ÊñºË¶èÂâáÁöÑË≥áÊñô
Â∞çÊñºÁúüÂØ¶‰∏ñÁïåÁöÑÊáâÁî®Á®ãÂºè‰æÜË™™ËÆäÂæóÊõ¥ÂÆπÊòìÂèñÂæó„ÄÇÁî±ÊñºËá™ÁÑ∂Ë™ûË®ÄÁöÑÂÖßÂú®
Ê®°Á≥äÊÄßÂíåË¶èÂâáÈõÜÁöÑË§áÈõúÊÄßÔºåÁâπÂà•ÊòØÂú®Èï∑Ë™ûÂ¢É‰∏≠ÔºåLLM Â∏∏Â∏∏Èõ£‰ª•ÈÅµÂæ™ÊâÄÊúâÊåáÂÆöÁöÑË¶èÂâáÔºåÁ∂ìÂ∏∏
Ëá≥Â∞ëÈÅ∫Êºè‰∏ÄÂÄã„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ LLM Âú®Èï∑‰∏îË§áÈõúÁöÑË™ûÂ¢É‰∏≠ÁöÑÊé®ÁêÜÂíåÁêÜËß£ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊèêÁ§∫Á≠ñÁï•Â§öË™ûË®Ä
ÊèêÁ§∫ÔºåÂç≥ MLPromptÔºåÂÆÉËá™ÂãïÂ∞á LLM Èõ£‰ª•ÈÅµÂæ™ÁöÑÂÆπÊòìÂá∫ÈåØÁöÑË¶èÂâáÁøªË≠ØÊàêÂè¶‰∏ÄÁ®ÆË™ûË®ÄÔºåÂæûËÄåÂºïËµ∑Êõ¥Â§ßÁöÑ
Ê≥®ÊÑèÂÆÉ„ÄÇÂú®ÂêÑÁ®Æ‰ªªÂãôÁöÑÂÖ¨Áî®Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåMLPrompt ÂèØ‰ª•ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊèêÁ§∫ÊñπÊ≥ïÔºå‰æãÂ¶Ç
ÊÄùËÄÉÈèà„ÄÅÊÄùËÄÉÊ®πÂíåËá™Êàë‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë
ÂºïÂÖ•‰∫ÜÂ∞á MLPrompt ËàáËá™ÂãïÊ™¢Êü•Ê©üÂà∂Êï¥ÂêàÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµêÊßãÂåñË≥áÊñôÁîüÊàêÔºå‰∏¶‰ª•ÊñáÂ≠óËΩâÊèõÁÇ∫ MIP
ÂØ¶‰æãÁÇ∫ÂÖ∑È´îÊ°à‰æãÁ†îÁ©∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊì¥ÂÖÖ‰∫ÜÂª∫Ë≠∞ÁöÑÊñáÂ≠óËΩâÊèõÁÇ∫ SQL Ê°ÜÊû∂Ôºå‰ª•Â±ïÁ§∫ÂÖ∂Â∞çÁµêÊßãÂåñË≥áÊñôÂêàÊàêÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ

##### **A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B**
2409.11055v1 by Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon

Prior research works have evaluated quantized LLMs using limited metrics such
as perplexity or a few basic knowledge tasks and old datasets. Additionally,
recent large-scale models such as Llama 3.1 with up to 405B have not been
thoroughly examined. This paper evaluates the performance of instruction-tuned
LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on
models ranging from 7B to 405B. Using 13 benchmarks, we assess performance
across six task types: commonsense Q\&A, knowledge and language understanding,
instruction following, hallucination detection, mathematics, and dialogue. Our
key findings reveal that (1) quantizing a larger LLM to a similar size as a
smaller FP16 LLM generally performs better across most benchmarks, except for
hallucination detection and instruction following; (2) performance varies
significantly with different quantization methods, model size, and bit-width,
with weight-only methods often yielding better results in larger models; (3)
task difficulty does not significantly impact accuracy degradation due to
quantization; and (4) the MT-Bench evaluation method has limited discriminatory
power among recent high-performing LLMs.

ÊëòË¶ÅÔºöÂÖàÂâçÁöÑÁ†îÁ©∂Â∑•‰ΩúÂ∑≤‰ΩøÁî®ÊúâÈôêÁöÑÊåáÊ®ôÔºå‰æãÂ¶ÇÂõ∞ÊÉëÂ∫¶Êàñ‰∏Ä‰∫õÂü∫Êú¨ÁöÑÁü•Ë≠ò‰ªªÂãôÂíåËàäÁöÑË≥áÊñôÈõÜÔºå‰æÜË©ï‰º∞ÈáèÂåñÁöÑ LLM„ÄÇÊ≠§Â§ñÔºåÂÉè Llama 3.1 ÈÄôÊ®£È´òÈÅî 405B ÁöÑÊúÄÊñ∞Â§ßÂûãÊ®°ÂûãÂ∞öÊú™Á∂ìÈÅéÂæπÂ∫ïÊ™¢Êü•„ÄÇÊú¨ÊñáË©ï‰º∞‰∫ÜÂæû 7B Âà∞ 405B ÁöÑÊ®°ÂûãÂú®ÂêÑÁ®ÆÈáèÂåñÊñπÊ≥ïÔºàGPTQ„ÄÅAWQ„ÄÅSmoothQuant Âíå FP8Ôºâ‰∏äÔºåÁ∂ìÈÅéÊåá‰ª§Ë™øÊï¥ÁöÑ LLM ÁöÑÊïàËÉΩ„ÄÇ‰ΩøÁî® 13 ÂÄãÂü∫Ê∫ñÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÂÖ≠Á®Æ‰ªªÂãôÈ°ûÂûãÁöÑÊïàËÉΩÔºöÂ∏∏Ë≠òÂïèÁ≠î„ÄÅÁü•Ë≠òÂíåË™ûË®ÄÁêÜËß£„ÄÅÊåá‰ª§ÈÅµÂæ™„ÄÅÂπªË¶∫Ê™¢Ê∏¨„ÄÅÊï∏Â≠∏ÂíåÂ∞çË©±„ÄÇÊàëÂÄëÁöÑÈóúÈçµÁôºÁèæÈ°ØÁ§∫ÔºåÔºà1ÔºâÂ∞áËºÉÂ§ßÁöÑ LLM ÈáèÂåñÁÇ∫ËàáËºÉÂ∞èÁöÑ FP16 LLM Áõ∏‰ººÁöÑÂ∞∫ÂØ∏ÔºåÈÄöÂ∏∏Âú®Èô§ÂπªË¶∫Ê™¢Ê∏¨ÂíåÊåá‰ª§ÈÅµÂæ™‰πãÂ§ñÁöÑÂ§ßÂ§öÊï∏Âü∫Ê∫ñ‰∏äË°®ÁèæÂæóÊõ¥Â•ΩÔºõÔºà2ÔºâÊïàËÉΩÊúÉÈö®Ëëó‰∏çÂêåÁöÑÈáèÂåñÊñπÊ≥ï„ÄÅÊ®°ÂûãÂ§ßÂ∞èÂíå‰ΩçÂÖÉÂØ¨Â∫¶ËÄåÊúâÈ°ØËëóÂ∑ÆÁï∞ÔºåÂÉÖÊ¨äÈáçÁöÑÈáèÂåñÊñπÊ≥ïÈÄöÂ∏∏Âú®ËºÉÂ§ßÁöÑÊ®°Âûã‰∏≠Áî¢ÁîüÊõ¥Â•ΩÁöÑÁµêÊûúÔºõÔºà3Ôºâ‰ªªÂãôÈõ£Â∫¶‰∏çÊúÉÈ°ØËëóÂΩ±ÈüøÈáèÂåñÈÄ†ÊàêÁöÑÊ∫ñÁ¢∫Â∫¶‰∏ãÈôçÔºõÔºà4ÔºâMT-Bench Ë©ï‰º∞ÊñπÊ≥ïÂú®ÊúÄËøëÊïàËÉΩËâØÂ•ΩÁöÑ LLM ‰∏≠ÂÖ∑ÊúâÊúâÈôêÁöÑÂçÄÂàÜËÉΩÂäõ„ÄÇ

##### **Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming**
2409.11041v2 by Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen

While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).

ÊëòË¶ÅÔºö<paragraph>ÂÑòÁÆ°ÊúÄËøëÂú®ÂÆ∂Â∫≠Áí∞Â¢É‰∏≠Â∞çÊ©üÂô®‰∫∫ÁöÑÁ†îÁ©∂ÂæàÂ§öÔºå‰ΩÜÁõÆÂâçÁÇ∫Ê≠¢ÔºåÂ§ßÂ§öÊï∏ÁèæÂ≠òÁöÑÊ©üÂô®‰∫∫ÈÉΩÂèØ‰ª•Âú®ÂïÜÂ∫óÊ®ìÂ±§‰∏≠ÊâæÂà∞ÔºåËÄå‰∏î‰∫∫È°ûËàáÊ©üÂô®‰∫∫‰πãÈñìÁöÑÂ§ßÂ§öÊï∏‰∫íÂãïÈÉΩÁôºÁîüÂú®ÈÇ£Ë£°„ÄÇÂÇ≥Áµ±‰∏äÔºåË®≠Ë®àÁÇ∫Ëàá‰∫∫È°ûÂú®ÁµÑË£ùÁ∑ö‰∏ä‰∏¶ËÇ©Â∑•‰ΩúÁöÑ„ÄåÂçî‰ΩúÊ©üÂô®‰∫∫„ÄçÔºàÂçî‰ΩúÊ©üÂô®‰∫∫ÔºâÈúÄË¶ÅÂ∞àÂÆ∂Á®ãÂºèË®≠Ë®àÔºåÈôêÂà∂‰∫ÜÂÅöÂá∫ËÆäÊõ¥ÁöÑËÉΩÂäõÔºåÊàñÊâãÂãïÊåáÂ∞éÔºåÈôêÂà∂‰∫ÜÊâÄÁî¢ÁîüÁ®ãÂºèÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊé¢Á¥¢‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÁâπÂà•ÊòØÂÆÉÂÄëÂú®ÊÉÖÂ¢ÉÂ≠∏Áøí‰∏≠ÁöÑËÉΩÂäõÔºåÁî®ÊñºÂ∞çË©±ÂºèÁ®ãÂºèÁ¢ºÁî¢Áîü„ÄÇ‰ΩúÁÇ∫Á¨¨‰∏ÄÊ≠•ÔºåÊàëÂÄëÂÆöÁæ© RATSÔºå„ÄåÈáçË§áÁµÑË£ù‰ªªÂãô„ÄçÔºå‰∏ÄÂÄã 2D Âª∫Êßã‰ªªÂãôÔºåÊó®Âú®ÁÇ∫Ê®°Êì¨Áî¢Ê•≠ÁµÑË£ùÂ†¥ÊôØÂ•†ÂÆöÂü∫Á§é„ÄÇÂú®ÈÄôÂÄã‰ªªÂãô‰∏≠Ôºå‰∏ÄÂÄã„ÄåÁ®ãÂºèË®≠Ë®àÂ∏´„Äç‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÊåáÂ∞éÂçî‰ΩúÊ©üÂô®‰∫∫Â¶Ç‰ΩïÂª∫ÊßãÊüêÂÄãÁµÑ‰ª∂Ôºõ‰πüÂ∞±ÊòØË™™ÔºåÁ®ãÂºèË®≠Ë®àÂ∏´ÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄË™òÂ∞é‰∏ÄÂÄãÁ®ãÂºè„ÄÇÊàëÂÄëÂª∫Á´ã‰∏ÄÂÄãÂ∞áÁõÆÊ®ôÁµêÊßãËàáÂêÑÁ®ÆÁØÑ‰æãÊåá‰ª§Ôºà‰∫∫Â∑•Êí∞ÂØ´„ÄÅÂü∫ÊñºÁØÑÊú¨ÂíåÊ®°ÂûãÁî¢ÁîüÔºâÂíåÁØÑ‰æãÁ®ãÂºèÁ¢ºÈÖçÂ∞çÁöÑË≥áÊñôÈõÜ„ÄÇÊúâ‰∫ÜÈÄôÂÄãÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú®Áµ¶‰∫àÊÉÖÂ¢ÉÁØÑ‰æãÁöÑÊÉÖÊ≥Å‰∏ãÁ∂úÂêàÊ≠§È°ûÁ®ãÂºèÁ¢ºÁöÑËÉΩÂäõ„ÄÇÂú®Ê®°Êì¨Áí∞Â¢É‰∏≠Ë©ï‰º∞ÔºåÊàëÂÄëÁôºÁèæ LLM ËÉΩÂ§†Áî¢ÁîüÊ∫ñÁ¢∫ÁöÑ„Äå‰∏ÄÈöéÁ®ãÂºèÁ¢º„ÄçÔºàÊåá‰ª§Â∫èÂàóÔºâÔºå‰ΩÜÁî¢Áîü„ÄåÈ´òÈöéÁ®ãÂºèÁ¢º„ÄçÔºà‰æãÂ¶ÇÂáΩÊï∏ÊàñËø¥Âúà‰ΩøÁî®ÁöÑÊäΩË±°ÔºâÊôÇÊúâÂïèÈ°å„ÄÇ</paragraph>

##### **Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI**
2409.11032v1 by Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano

Written texts reflect an author's perspective, making the thorough analysis
of literature a key research method in fields such as the humanities and social
sciences. However, conventional text mining techniques like sentiment analysis
and topic modeling are limited in their ability to capture the hierarchical
narrative structures that reveal deeper argumentative patterns. To address this
gap, we propose a method that leverages large language models (LLMs) to extract
and organize these structures into a hierarchical framework. We validate this
approach by analyzing public opinions on generative AI collected by Japan's
Agency for Cultural Affairs, comparing the narratives of supporters and
critics. Our analysis provides clearer visualization of the factors influencing
divergent opinions on generative AI, offering deeper insights into the
structures of agreement and disagreement.

ÊëòË¶ÅÔºöÊõ∏Èù¢ÊñáÊú¨ÂèçÊò†‰ΩúËÄÖÁöÑËßÄÈªûÔºåÂõ†Ê≠§ÂæπÂ∫ïÂàÜÊûêÊñáÂ≠∏ÊàêÁÇ∫‰∫∫ÊñáÂ≠∏ÁßëÂíåÁ§æÊúÉÁßëÂ≠∏Á≠âÈ†òÂüüÁöÑ‰∏ÄÁ®ÆÈóúÈçµÁ†îÁ©∂ÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÊñáÊú¨Êé¢ÂãòÊäÄË°ìÔºà‰æãÂ¶ÇÊÉÖÁ∑íÂàÜÊûêÂíå‰∏ªÈ°åÂª∫Ê®°ÔºâÂú®ÊçïÊçâÊè≠Á§∫Êõ¥Ê∑±Â±§Ë´ñË≠âÊ®°ÂºèÁöÑÂàÜÂ±§Êïò‰∫ãÁµêÊßãÊñπÈù¢ËÉΩÂäõÊúâÈôê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÈÄô‰∫õÁµêÊßãÊèêÂèñ‰∏¶ÁµÑÁπîÊàê‰∏ÄÂÄãÂàÜÂ±§Ê°ÜÊû∂„ÄÇÊàëÂÄëÈÄöÈÅéÂàÜÊûêÊó•Êú¨ÊñáÂåñÂª≥Êî∂ÈõÜÁöÑÈóúÊñºÁîüÊàêÂºè AI ÁöÑÂÖ¨ÁúæÊÑèË¶ã‰æÜÈ©óË≠âÈÄôÁ®ÆÊñπÊ≥ïÔºåÊØîËºÉÊîØÊåÅËÄÖÂíåÊâπË©ïËÄÖÁöÑÊïòËø∞„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÊèê‰æõ‰∫ÜÊõ¥Ê∏ÖÊô∞ÁöÑË¶ñË¶∫ÂåñÔºåË™™Êòé‰∫ÜÂΩ±ÈüøÂ∞çÁîüÊàêÂºè AI ‰∏çÂêåÊÑèË¶ãÁöÑÂõ†Á¥†ÔºåÂ∞çÂêåÊÑèÂíå‰∏çÂêåÊÑèÁöÑÁµêÊßãÊèê‰æõ‰∫ÜÊõ¥Ê∑±ÂÖ•ÁöÑË¶ãËß£„ÄÇ

##### **D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding**
2409.11024v1 by Xiaobao Song, Hao Wang, Liwei Deng, Yuxin He, Wenming Cao, Chi-Sing Leungc

Time position embeddings capture the positional information of time steps,
often serving as auxiliary inputs to enhance the predictive capabilities of
time series models. However, existing models exhibit limitations in capturing
intricate time positional information and effectively utilizing these
embeddings. To address these limitations, this paper proposes a novel model
called D2Vformer. Unlike typical prediction methods that rely on RNNs or
Transformers, this approach can directly handle scenarios where the predicted
sequence is not adjacent to the input sequence or where its length dynamically
changes. In comparison to conventional methods, D2Vformer undoubtedly saves a
significant amount of training resources. In D2Vformer, the Date2Vec module
uses the timestamp information and feature sequences to generate time position
embeddings. Afterward, D2Vformer introduces a new fusion block that utilizes an
attention mechanism to explore the similarity in time positions between the
embeddings of the input sequence and the predicted sequence, thereby generating
predictions based on this similarity. Through extensive experiments on six
datasets, we demonstrate that Date2Vec outperforms other time position
embedding methods, and D2Vformer surpasses state-of-the-art methods in both
fixed-length and variable-length prediction tasks.

ÊëòË¶ÅÔºöÊôÇÈñì‰ΩçÁΩÆÂµåÂÖ•Êì∑ÂèñÊôÇÈñìÊ≠•È©üÁöÑ‰ΩçÁΩÆË≥áË®äÔºå
ÈÄöÂ∏∏‰ΩúÁÇ∫ËºîÂä©Ëº∏ÂÖ•Ôºå‰ª•Â¢ûÂº∑ÊôÇÈñìÂ∫èÂàóÊ®°ÂûãÁöÑÈ†êÊ∏¨ËÉΩÂäõ„ÄÇ
ÁÑ∂ËÄåÔºåÁèæÊúâÊ®°ÂûãÂú®Êì∑ÂèñË§áÈõúÁöÑÊôÇÈñì‰ΩçÁΩÆË≥áË®äÂíåÊúâÊïàÂà©Áî®ÈÄô‰∫õ
ÂµåÂÖ•ÊñπÈù¢Ë°®ÁèæÂá∫ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ D2Vformer ÁöÑÊñ∞Ê®°Âûã„ÄÇ
Ëàá‰æùË≥¥ RNN Êàñ Transformer ÁöÑÂÖ∏ÂûãÈ†êÊ∏¨ÊñπÊ≥ï‰∏çÂêåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•Áõ¥Êé•ËôïÁêÜÈ†êÊ∏¨Â∫èÂàó‰∏çÈÑ∞ËøëËº∏ÂÖ•Â∫èÂàóÊàñÂÖ∂Èï∑Â∫¶ÂãïÊÖãËÆäÂåñÁöÑÂ†¥ÊôØ„ÄÇ
ËàáÂÇ≥Áµ±ÊñπÊ≥ïÁõ∏ÊØîÔºåD2Vformer ÁÑ°ÁñëÁØÄÁúÅ‰∫ÜÂ§ßÈáèÁöÑË®ìÁ∑¥Ë≥áÊ∫ê„ÄÇÂú® D2Vformer ‰∏≠ÔºåDate2Vec Ê®°ÁµÑ‰ΩøÁî®ÊôÇÈñìÊà≥Ë®òË≥áË®äÂíåÁâπÂæµÂ∫èÂàó‰æÜÁî¢ÁîüÊôÇÈñì‰ΩçÁΩÆÂµåÂÖ•„ÄÇ
‰πãÂæåÔºåD2Vformer ‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑËûçÂêàÂçÄÂ°äÔºåË©≤ÂçÄÂ°äÂà©Áî®Ê≥®ÊÑèÂäõÊ©üÂà∂‰æÜÊé¢Á¥¢Ëº∏ÂÖ•Â∫èÂàóÁöÑÂµåÂÖ•ÂíåÈ†êÊ∏¨Â∫èÂàó‰πãÈñìÁöÑÊôÇÈñì‰ΩçÁΩÆÁõ∏‰ººÊÄßÔºåÂæûËÄåÊ†πÊìöÊ≠§Áõ∏‰ººÊÄßÁî¢ÁîüÈ†êÊ∏¨„ÄÇ
ÈÄöÈÅéÂ∞çÂÖ≠ÂÄãË≥áÊñôÈõÜÁöÑÂª£Ê≥õÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé Date2Vec ÂÑ™ÊñºÂÖ∂‰ªñÊôÇÈñì‰ΩçÁΩÆÂµåÂÖ•ÊñπÊ≥ïÔºåËÄå D2Vformer Âú®Âõ∫ÂÆöÈï∑Â∫¶ÂíåËÆäÈï∑Â∫¶È†êÊ∏¨‰ªªÂãô‰∏≠ÈÉΩË∂ÖË∂ä‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇ

##### **GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models**
2409.11022v2 by Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu

Large Language Models (LLMs) have supplanted traditional methods in numerous
natural language processing tasks. Nonetheless, in Named Entity Recognition
(NER), existing LLM-based methods underperform compared to baselines and
require significantly more computational resources, limiting their application.
In this paper, we introduce the task of generation-based extraction and
in-context classification (GEIC), designed to leverage LLMs' prior knowledge
and self-attention mechanisms for NER tasks. We then propose CascadeNER, a
universal and multilingual GEIC framework for few-shot and zero-shot NER.
CascadeNER employs model cascading to utilize two small-parameter LLMs to
extract and classify independently, reducing resource consumption while
enhancing accuracy. We also introduce AnythingNER, the first NER dataset
specifically designed for LLMs, including 8 languages, 155 entity types and a
novel dynamic categorization system. Experiments show that CascadeNER achieves
state-of-the-art performance on low-resource and fine-grained scenarios,
including CrossNER and FewNERD. Our work is openly accessible.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂ∑≤Á∂ìÂèñ‰ª£ÂÇ≥Áµ±ÊñπÊ≥ïÔºåÁî®ÊñºÂ§ßÈáèÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂú®ÂëΩÂêçÂØ¶È´îË≠òÂà•ÔºàNERÔºâ‰∏≠ÔºåÁèæÊúâÁöÑÂü∫Êñº LLM ÁöÑÊñπÊ≥ïËàáÂü∫Á∑öÁõ∏ÊØîË°®Áèæ‰∏ç‰Ω≥Ôºå‰∏¶‰∏îÈúÄË¶ÅÂ§ßÈáèË®àÁÆóË≥áÊ∫êÔºåÂæûËÄåÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑÊáâÁî®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂü∫ÊñºÁîüÊàêÁöÑÊäΩÂèñÂíå‰∏ä‰∏ãÊñáÂàÜÈ°ûÔºàGEICÔºâÁöÑ‰ªªÂãôÔºåÊó®Âú®Âà©Áî® LLM ÁöÑÂÖàÈ©óÁü•Ë≠òÂíåËá™Ê≥®ÊÑèÂäõÊ©üÂà∂‰æÜÂü∑Ë°å NER ‰ªªÂãô„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CascadeNERÔºå‰∏ÄÂÄãÈÄöÁî®ÁöÑÂ§öË™ûË®Ä GEIC Ê°ÜÊû∂ÔºåÁî®ÊñºÂ∞èÊ®£Êú¨ÂíåÈõ∂Ê®£Êú¨ NER„ÄÇCascadeNER Êé°Áî®Ê®°Âûã‰∏≤ËÅØ‰æÜÂà©Áî®ÂÖ©ÂÄãÂ∞èÂèÉÊï∏ LLM Áç®Á´ãÂú∞ÊèêÂèñÂíåÂàÜÈ°ûÔºåÂæûËÄåÂú®ÊèêÈ´òÊ∫ñÁ¢∫ÊÄßÁöÑÂêåÊôÇÊ∏õÂ∞ëË≥áÊ∫êÊ∂àËÄó„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•‰∫Ü AnythingNERÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄÁÇ∫ LLM Ë®≠Ë®àÁöÑ NER Êï∏ÊìöÈõÜÔºåÂåÖÊã¨ 8 Á®ÆË™ûË®Ä„ÄÅ155 Á®ÆÂØ¶È´îÈ°ûÂûãÂíå‰∏ÄÂÄãÊñ∞Á©éÁöÑÂãïÊÖãÂàÜÈ°ûÁ≥ªÁµ±„ÄÇÂØ¶È©óË°®ÊòéÔºåCascadeNER Âú®‰ΩéË≥áÊ∫êÂíåÁ¥∞Á≤íÂ∫¶Â†¥ÊôØÔºàÂåÖÊã¨ CrossNER Âíå FewNERDÔºâ‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÁöÑÊàêÊûúÊòØÂÖ¨ÈñãÂèØÁî®ÁöÑ„ÄÇ

##### **Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models**
2409.11011v1 by Emile Saillard, Aur√©lie Levillain, David Mitton, Jean-Baptiste Pialat, Cyrille Confavreux, H√©l√®ne Follet, Thomas Grenier

Purpose: Bone metastasis have a major impact on the quality of life of
patients and they are diverse in terms of size and location, making their
segmentation complex. Manual segmentation is time-consuming, and expert
segmentations are subject to operator variability, which makes obtaining
accurate and reproducible segmentations of bone metastasis on CT-scans a
challenging yet important task to achieve. Materials and Methods: Deep learning
methods tackle segmentation tasks efficiently but require large datasets along
with expert manual segmentations to generalize on new images. We propose an
automated data synthesis pipeline using 3D Denoising Diffusion Probabilistic
Models (DDPM) to enchance the segmentation of femoral metastasis from CT-scan
volumes of patients. We used 29 existing lesions along with 26 healthy femurs
to create new realistic synthetic metastatic images, and trained a DDPM to
improve the diversity and realism of the simulated volumes. We also
investigated the operator variability on manual segmentation. Results: We
created 5675 new volumes, then trained 3D U-Net segmentation models on real and
synthetic data to compare segmentation performance, and we evaluated the
performance of the models depending on the amount of synthetic data used in
training. Conclusion: Our results showed that segmentation models trained with
synthetic data outperformed those trained on real volumes only, and that those
models perform especially well when considering operator variability.

ÊëòË¶ÅÔºöÁõÆÁöÑÔºöÈ™®ËΩâÁßªÊúÉÂ∞çÊÇ£ËÄÖÁöÑÁîüÊ¥ªÂìÅË≥™ÈÄ†ÊàêÈáçÂ§ßÂΩ±ÈüøÔºå‰∏îÂú®Â§ßÂ∞èÂíå‰ΩçÁΩÆ‰∏äÂ∑ÆÁï∞ÂæàÂ§ßÔºåÈÄô‰ΩøÂæóÂÖ∂ÂàÜÂâ≤ËÆäÂæóË§áÈõú„ÄÇÊâãÂãïÂàÜÂâ≤ÈùûÂ∏∏ËÄóÊôÇÔºåËÄå‰∏îÂ∞àÂÆ∂ÂàÜÂâ≤ÊúÉÂèóÂà∞Êìç‰ΩúÂì°ËÆäÁï∞ÊÄßÁöÑÂΩ±ÈüøÔºåÈÄô‰ΩøÂæóÂú® CT ÊéÉÊèè‰∏≠Áç≤ÂæóÈ™®ËΩâÁßªÁöÑÊ∫ñÁ¢∫‰∏îÂèØÈáçÁèæÁöÑÂàÜÂâ≤ÊàêÁÇ∫‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄß‰ΩÜÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÊùêÊñôÂíåÊñπÊ≥ïÔºöÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂú∞ËôïÁêÜÂàÜÂâ≤‰ªªÂãôÔºå‰ΩÜÈúÄË¶ÅÂ§ßÂûãË≥áÊñôÈõÜ‰ª•ÂèäÂ∞àÂÆ∂ÊâãÂãïÂàÜÂâ≤ÊâçËÉΩÂú®Êñ∞ÁöÑÂΩ±ÂÉè‰∏äÈÄ≤Ë°åÊ¶ÇÂåñ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî® 3D ÂéªÂô™Êì¥Êï£Ê©üÁéáÊ®°Âûã (DDPM) ÁöÑËá™ÂãïÂåñË≥áÊñôÂêàÊàêÁÆ°ÈÅìÔºå‰ª•Â¢ûÂº∑ÂæûÊÇ£ËÄÖ CT ÊéÉÊèèÈ´îÁ©ç‰∏≠ÂàÜÂâ≤ËÇ°È™®ËΩâÁßª„ÄÇÊàëÂÄë‰ΩøÁî® 29 ÂÄãÁèæÊúâÁóÖÁÅ∂Âíå 26 ÂÄãÂÅ•Â∫∑ÁöÑËÇ°È™®‰æÜÂâµÂª∫Êñ∞ÁöÑÈÄºÁúüÂêàÊàêËΩâÁßªÂΩ±ÂÉèÔºå‰∏¶Ë®ìÁ∑¥ DDPM ‰ª•ÊèêÈ´òÊ®°Êì¨È´îÁ©çÁöÑÂ§öÊ®£ÊÄßÂíåÁúüÂØ¶ÊÄß„ÄÇÊàëÂÄëÈÇÑÁ†îÁ©∂‰∫ÜÊâãÂãïÂàÜÂâ≤‰∏≠ÁöÑÊìç‰ΩúÂì°ËÆäÁï∞ÊÄß„ÄÇÁµêÊûúÔºöÊàëÂÄëÂâµÂª∫‰∫Ü 5675 ÂÄãÊñ∞È´îÁ©çÔºåÁÑ∂ÂæåÂú®ÁúüÂØ¶ÂíåÂêàÊàêË≥áÊñô‰∏äË®ìÁ∑¥ 3D U-Net ÂàÜÂâ≤Ê®°Âûã‰ª•ÊØîËºÉÂàÜÂâ≤ÊïàËÉΩÔºå‰∏¶Ê†πÊìöË®ìÁ∑¥‰∏≠‰ΩøÁî®ÁöÑÂêàÊàêË≥áÊñôÈáèË©ï‰º∞Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºå‰ΩøÁî®ÂêàÊàêË≥áÊñôË®ìÁ∑¥ÁöÑÂàÜÂâ≤Ê®°ÂûãÂÑ™ÊñºÂÉÖÂú®ÁúüÂØ¶È´îÁ©ç‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºå‰∏¶‰∏îÈÄô‰∫õÊ®°ÂûãÂú®ËÄÉÊÖÆÊìç‰ΩúÂì°ËÆäÁï∞ÊÄßÊôÇË°®ÁèæÂæóÁâπÂà•Â•Ω„ÄÇ

##### **CAST: Cross-modal Alignment Similarity Test for Vision Language Models**
2409.11007v1 by Gautier Dagan, Olga Loginova, Anil Batra

Vision Language Models (VLMs) are typically evaluated with Visual Question
Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA
performance is taken as evidence that the model will perform well on a broader
range of tasks that require both visual and language inputs. However,
scene-aware VQA does not fully capture input biases or assess hallucinations
caused by a misalignment between modalities. To address this, we propose a
Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency
across modalities. This test involves asking the models to identify
similarities between two scenes through text-only, image-only, or both and then
assess the truthfulness of the similarities they generate. Since there is no
ground-truth to compare against, this evaluation does not focus on objective
accuracy but rather on whether VLMs are internally consistent in their outputs.
We argue that while not all self-consistent models are capable or accurate, all
capable VLMs must be self-consistent.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÈÄöÂ∏∏‰ΩøÁî®Ë¶ñË¶∫ÂïèÈ°åÂõûÁ≠î (VQA) ‰ªªÂãôÈÄ≤Ë°åË©ï‰º∞ÔºåÈÄô‰∫õ‰ªªÂãôÊúÉË©ï‰º∞Ê®°ÂûãÂ∞çÂ†¥ÊôØÁöÑÁêÜËß£„ÄÇËâØÂ•ΩÁöÑ VQA ÊïàËÉΩË¢´Ë¶ñÁÇ∫Ê®°ÂûãÂú®ÈúÄË¶ÅË¶ñË¶∫ÂíåË™ûË®ÄËº∏ÂÖ•ÁöÑÊõ¥Âª£Ê≥õ‰ªªÂãô‰∏≠Ë°®ÁèæËâØÂ•ΩÁöÑË≠âÊìö„ÄÇÁÑ∂ËÄåÔºåÂ†¥ÊôØÊÑüÁü• VQA ÁÑ°Ê≥ïÂÆåÂÖ®ÊçïÊçâËº∏ÂÖ•ÂÅèÂ∑ÆÊàñË©ï‰º∞Áî±Ê®°ÊÖã‰πãÈñìÁöÑÈåØ‰ΩçÈÄ†ÊàêÁöÑÂπªË¶∫„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫Ë∑®Ê®°ÊÖãÂ∞çÈΩäÁõ∏‰ººÊÄßÊ∏¨Ë©¶ (CAST)Ôºå‰ª•Êé¢Ê∏¨ VLM Âú®‰∏çÂêåÊ®°ÊÖã‰πãÈñìÁöÑËá™Ê¥ΩÊÄß„ÄÇÊ≠§Ê∏¨Ë©¶Ê∂âÂèäË¶ÅÊ±ÇÊ®°ÂûãÈÄèÈÅéÁ¥îÊñáÂ≠ó„ÄÅÁ¥îÂúñÁâáÊàñÂÖ©ËÄÖ‰æÜË≠òÂà•ÂÖ©ÂÄãÂ†¥ÊôØ‰πãÈñìÁöÑÁõ∏‰ººÊÄßÔºåÁÑ∂ÂæåË©ï‰º∞‰ªñÂÄëÁî¢ÁîüÁöÑÁõ∏‰ººÊÄßÁöÑÁúüÂØ¶ÊÄß„ÄÇÁî±ÊñºÊ≤íÊúâÁúüÂØ¶ÊÉÖÊ≥ÅÂèØ‰ª•ÊØîËºÉÔºåÂõ†Ê≠§Ê≠§Ë©ï‰º∞‰∏çËëóÈáçÊñºÂÆ¢ËßÄÊ∫ñÁ¢∫ÊÄßÔºåËÄåÊòØËëóÈáçÊñº VLM Âú®ÂÖ∂Ëº∏Âá∫‰∏≠ÊòØÂê¶ÂÖßÈÉ®‰∏ÄËá¥„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÈõñÁÑ∂‰∏¶ÈùûÊâÄÊúâËá™Ê¥ΩÊ®°ÂûãÈÉΩÊúâËÉΩÂäõÊàñÊ∫ñÁ¢∫Ôºå‰ΩÜÊâÄÊúâÊúâËÉΩÂäõÁöÑ VLM ÈÉΩÂøÖÈ†àËá™Ê¥Ω„ÄÇ

##### **Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation**
2409.11003v1 by Gerard I. G√°llego, Roy Fejgin, Chunghsin Yeh, Xiaoyu Liu, Gautam Bhattacharya

Audio token modeling has become a powerful framework for speech synthesis,
with two-stage approaches employing semantic tokens remaining prevalent. In
this paper, we aim to simplify this process by introducing a semantic knowledge
distillation method that enables high-quality speech generation in a single
stage. Our proposed model improves speech quality, intelligibility, and speaker
similarity compared to a single-stage baseline. Although two-stage systems
still lead in intelligibility, our model significantly narrows the gap while
delivering comparable speech quality. These findings showcase the potential of
single-stage models to achieve efficient, high-quality TTS with a more compact
and streamlined architecture.

ÊëòË¶ÅÔºöË™ûÈü≥‰ª£Âπ£Âª∫Ê®°Â∑≤ÊàêÁÇ∫Ë™ûÈü≥ÂêàÊàêÁöÑÂº∑Â§ßÊû∂ÊßãÔºåÂÖ∂‰∏≠Êé°Áî®Ë™ûÁæ©‰ª£Âπ£ÁöÑÂÖ©ÈöéÊÆµÊñπÊ≥ï‰ªçÁÑ∂ÊôÆÈÅç„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÈÄöÈÅéÂºïÂÖ•Ë™ûÁæ©Áü•Ë≠òËí∏È§æÊñπÊ≥ï‰æÜÁ∞°ÂåñÊ≠§ÈÅéÁ®ãÔºåË©≤ÊñπÊ≥ïÂèØ‰ª•Âú®ÂñÆÂÄãÈöéÊÆµ‰∏≠ÂØ¶ÁèæÈ´òÂìÅË≥™ÁöÑË™ûÈü≥ÁîüÊàê„ÄÇËàáÂñÆÈöéÊÆµÂü∫Ê∫ñÁõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÊîπÈÄ≤‰∫ÜË™ûÈü≥ÂìÅË≥™„ÄÅÊ∏ÖÊô∞Â∫¶ÂíåË™™Ë©±ËÄÖÁõ∏‰ººÂ∫¶„ÄÇÂÑòÁÆ°ÂÖ©ÈöéÊÆµÁ≥ªÁµ±Âú®Ê∏ÖÊô∞Â∫¶ÊñπÈù¢‰ªçÁÑ∂È†òÂÖàÔºå‰ΩÜÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÁ∏ÆÂ∞è‰∫ÜÂ∑ÆË∑ùÔºåÂêåÊôÇÊèê‰æõ‰∫ÜÂèØÊØîËºÉÁöÑË™ûÈü≥ÂìÅË≥™„ÄÇÈÄô‰∫õÁôºÁèæÂ±ïÁ§∫‰∫ÜÂñÆÈöéÊÆµÊ®°ÂûãÂú®‰ΩøÁî®Êõ¥Á∑äÊπä„ÄÅÊõ¥Á∞°ÂåñÁöÑÊû∂ÊßãÂØ¶ÁèæÈ´òÊïà„ÄÅÈ´òÂìÅË≥™ÁöÑ TTS ÁöÑÊΩõÂäõ„ÄÇ

##### **Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models**
2409.10999v1 by Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul

Audio language models can understand audio inputs and perform a range of
audio-related tasks based on instructions, such as speech recognition and audio
captioning, where the instructions are usually textual prompts. Audio language
models are mostly initialized from pre-trained audio encoders and large
language models (LLMs). Although these pre-trained components were developed to
support multiple languages, audio-language models are trained predominantly on
English data, which may limit their usability to only English instructions or
English speech inputs. First, this paper examines the performance of existing
audio language models in an underserved language using Thai as an example. This
paper demonstrates that, despite being built on multilingual backbones, audio
language models do not exhibit cross-lingual emergent abilities to low-resource
languages. Second, this paper studies data mixture for developing audio
language models that are optimized for a target language as well as English. In
addition. this paper integrates audio comprehension and speech
instruction-following capabilities into a single unified model. Our experiments
provide insights into data mixture for enhancing instruction-following
capabilities in both a low-resource language and English. Our model,
Typhoon-Audio, outperforms existing open-source audio language models by a
considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in
both English and Thai languages.

ÊëòË¶ÅÔºöÈü≥Ë®äË™ûË®ÄÊ®°ÂûãÂèØ‰ª•ÁêÜËß£Èü≥Ë®äËº∏ÂÖ•Ôºå‰∏¶Ê†πÊìöÊåáÁ§∫Âü∑Ë°å‰∏ÄÁ≥ªÂàóËàáÈü≥Ë®äÁõ∏ÈóúÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇË™ûÈü≥Ëæ®Ë≠òÂíåÈü≥Ë®äÂ≠óÂπïÔºåÂÖ∂‰∏≠ÊåáÁ§∫ÈÄöÂ∏∏ÊòØÊñáÂ≠óÊèêÁ§∫„ÄÇÈü≥Ë®äË™ûË®ÄÊ®°ÂûãÂ§ßÂ§öÂæûÈ†êÂÖàË®ìÁ∑¥ÁöÑÈü≥Ë®äÁ∑®Á¢ºÂô®ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂàùÂßãÂåñ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÈ†êÂÖàË®ìÁ∑¥ÁöÑÂÖÉ‰ª∂ÊòØÁÇ∫‰∫ÜÊîØÊè¥Â§öÁ®ÆË™ûË®ÄËÄåÈñãÁôºÔºå‰ΩÜÈü≥Ë®äË™ûË®ÄÊ®°Âûã‰∏ªË¶ÅÂú®Ëã±Ë™ûË≥áÊñô‰∏äË®ìÁ∑¥ÔºåÈÄôÂèØËÉΩÊúÉÂ∞áÂÖ∂ÂèØÁî®ÊÄßÈôêÂà∂Âú®ÂÉÖÈôêÊñºËã±Ë™ûÊåáÁ§∫ÊàñËã±Ë™ûË™ûÈü≥Ëº∏ÂÖ•„ÄÇÈ¶ñÂÖàÔºåÊú¨Êñá‰ΩøÁî®Ê≥∞Ë™ûÁÇ∫‰æãÔºåÊé¢Ë®éÁèæÊúâÈü≥Ë®äË™ûË®ÄÊ®°ÂûãÂú®ÊúçÂãô‰∏çË∂≥Ë™ûË®Ä‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊú¨ÊñáË≠âÊòéÔºåÂÑòÁÆ°Âª∫Á´ãÂú®Â§öË™ûË®Ä‰∏ªÂππ‰∏äÔºå‰ΩÜÈü≥Ë®äË™ûË®ÄÊ®°Âûã‰∏¶Êú™Â∞ç‰ΩéË≥áÊ∫êË™ûË®ÄË°®ÁèæÂá∫Ë∑®Ë™ûË®ÄÁöÑÊµÆÁèæËÉΩÂäõ„ÄÇÂÖ∂Ê¨°ÔºåÊú¨ÊñáÁ†îÁ©∂Ë≥áÊñôÊ∑∑ÂêàÔºå‰ª•ÈñãÁôºÈáùÂ∞çÁõÆÊ®ôË™ûË®ÄÂíåËã±Ë™ûÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÁöÑÈü≥Ë®äË™ûË®ÄÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÂ∞áÈü≥Ë®äÁêÜËß£ÂíåË™ûÈü≥Êåá‰ª§ÈÅµÂæ™ÂäüËÉΩÊï¥ÂêàÂà∞ÂñÆ‰∏ÄÁµ±‰∏ÄÊ®°Âûã‰∏≠„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊèê‰æõ‰∫ÜÂ∞çË≥áÊñôÊ∑∑ÂêàÁöÑË¶ãËß£Ôºå‰ª•Â¢ûÂº∑‰ΩéË≥áÊ∫êË™ûË®ÄÂíåËã±Ë™ûÁöÑÊåá‰ª§ÈÅµÂæ™ÂäüËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã Typhoon-Audio Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂÑ™ÊñºÁèæÊúâÁöÑÈñãÊ∫êÈü≥Ë®äË™ûË®ÄÊ®°ÂûãÔºå‰∏¶‰∏îÂú®Ëã±Ë™ûÂíåÊ≥∞Ë™û‰∏≠ÈÉΩËàáÊúÄÂÖàÈÄ≤ÁöÑ Gemini-1.5-Pro Áõ∏Áï∂„ÄÇ

##### **Contextual Breach: Assessing the Robustness of Transformer-based QA Models**
2409.10997v2 by Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam

Contextual question-answering models are susceptible to adversarial
perturbations to input context, commonly observed in real-world scenarios.
These adversarial noises are designed to degrade the performance of the model
by distorting the textual input. We introduce a unique dataset that
incorporates seven distinct types of adversarial noise into the context, each
applied at five different intensity levels on the SQuAD dataset. To quantify
the robustness, we utilize robustness metrics providing a standardized measure
for assessing model performance across varying noise types and levels.
Experiments on transformer-based question-answering models reveal robustness
vulnerabilities and important insights into the model's performance in
realistic textual input.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÂºèÂïèÁ≠îÊ®°ÂûãÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÁöÑËº∏ÂÖ•ÊÉÖÂ¢ÉÊìæÂãïÔºåÈÄôÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÂæàÂ∏∏Ë¶ã„ÄÇÈÄô‰∫õÂ∞çÊäóÊÄßÈõúË®äÊó®Âú®ÈÄèÈÅéÊâ≠Êõ≤ÊñáÂ≠óËº∏ÂÖ•‰æÜÈôç‰ΩéÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁç®ÁâπÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÉÁ®ÆÈ°ûÂûãÁöÑÂ∞çÊäóÊÄßÈõúË®äÔºåÊØèÁ®ÆÈ°ûÂûãÂú® SQuAD Ë≥áÊñôÈõÜ‰∏ä‰ª•‰∫îÂÄã‰∏çÂêåÁöÑÂº∑Â∫¶Á≠âÁ¥öÊáâÁî®„ÄÇÁÇ∫‰∫ÜÈáèÂåñÁ©©ÂÅ•ÊÄßÔºåÊàëÂÄëÂà©Áî®Á©©ÂÅ•ÊÄßÊåáÊ®ôÊèê‰æõÊ®ôÊ∫ñÂåñÁöÑÊ∏¨ÈáèÔºåÁî®ÊñºË©ï‰º∞Ê®°ÂûãÂú®‰∏çÂêåÈõúË®äÈ°ûÂûãÂíåÂ±§Á¥ö‰∏ãÁöÑÊïàËÉΩ„ÄÇÂú®Âü∫ÊñºTransformerÁöÑÂïèÁ≠îÊ®°Âûã‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÊè≠Á§∫‰∫ÜÁ©©ÂÅ•ÊÄßÊºèÊ¥ûÔºå‰∏¶Â∞çÊ®°ÂûãÂú®ÁèæÂØ¶ÊñáÂ≠óËº∏ÂÖ•‰∏≠ÁöÑÊïàËÉΩÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑË¶ãËß£„ÄÇ

##### **Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**
2409.10994v1 by Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang

The rapid advancement of Multimodal Large Language Models (MLLMs) has led to
remarkable performances across various domains. However, this progress is
accompanied by a substantial surge in the resource consumption of these models.
We address this pressing issue by introducing a new approach, Token Reduction
using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without
sacrificing their performance. Inspired by human attention patterns in Visual
Question Answering (VQA) tasks, TRIM presents a fresh perspective on the
selection and reduction of image tokens. The TRIM method has been extensively
tested across 12 datasets, and the results demonstrate a significant reduction
in computational overhead while maintaining a consistent level of performance.
This research marks a critical stride in efficient MLLM development, promoting
greater accessibility and sustainability of high-performing models.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (MLLM) ÁöÑÂø´ÈÄüÂèëÂ±ïÂØºËá¥‰∫ÜÂú®ÂêÑ‰∏™È¢ÜÂüüÁöÑÂçìË∂äË°®Áé∞„ÄÇÁÑ∂ËÄåÔºåËøô‰∏ÄËøõÂ±ï‰º¥ÈöèÁùÄËøô‰∫õÊ®°ÂûãËµÑÊ∫êÊ∂àËÄóÁöÑÂ§ßÂπÖÂ¢ûÂä†„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÁ¥ßËø´ÈóÆÈ¢òÔºåÂç≥‰ΩøÁî® CLIP ÊåáÊ†á (TRIM) ËøõË°åÊ†áËÆ∞ÂáèÂ∞ëÔºåÊó®Âú®ÊèêÈ´ò MLLM ÁöÑÊïàÁéáËÄå‰∏çÁâ∫Áâ≤ÂÖ∂ÊÄßËÉΩ„ÄÇÂèóËßÜËßâÈóÆÁ≠î (VQA) ‰ªªÂä°‰∏≠‰∫∫Á±ªÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÂêØÂèëÔºåTRIM Êèê‰æõ‰∫ÜÂØπÂõæÂÉèÊ†áËÆ∞ÈÄâÊã©ÂíåÂáèÂ∞ëÁöÑÊñ∞ËßÜËßí„ÄÇTRIM ÊñπÊ≥ïÂ∑≤Âú® 12 ‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÊµãËØïÔºåÁªìÊûúË°®ÊòéÂú®‰øùÊåÅ‰∏ÄËá¥ÊÄßËÉΩÊ∞¥Âπ≥ÁöÑÂêåÊó∂ÔºåËÆ°ÁÆóÂºÄÈîÄÊòæËëóÂáèÂ∞ë„ÄÇËøôÈ°πÁ†îÁ©∂Ê†áÂøóÁùÄÈ´òÊïà MLLM ÂºÄÂèëÁöÑÂÖ≥ÈîÆ‰∏ÄÊ≠•Ôºå‰øÉËøõ‰∫ÜÈ´òÊÄßËÉΩÊ®°ÂûãÁöÑÊõ¥Â§ßÂèØËÆøÈóÆÊÄßÂíåÂèØÊåÅÁª≠ÊÄß„ÄÇ

##### **Control-flow Reconstruction Attacks on Business Process Models**
2409.10986v1 by Henrik Kirchmann, Stephan A. Fahrenkrog-Petersen, Felix Mannhardt, Matthias Weidlich

Process models may be automatically generated from event logs that contain
as-is data of a business process. While such models generalize over the
control-flow of specific, recorded process executions, they are often also
annotated with behavioural statistics, such as execution frequencies.Based
thereon, once a model is published, certain insights about the original process
executions may be reconstructed, so that an external party may extract
confidential information about the business process. This work is the first to
empirically investigate such reconstruction attempts based on process models.
To this end, we propose different play-out strategies that reconstruct the
control-flow from process trees, potentially exploiting frequency annotations.
To assess the potential success of such reconstruction attacks on process
models, and hence the risks imposed by publishing them, we compare the
reconstructed process executions with those of the original log for several
real-world datasets.

ÊëòË¶ÅÔºöÊµÅÁ®ãÊ®°ÂûãÂèØ‰ª•ÂæûÂåÖÂê´Ê•≠ÂãôÊµÅÁ®ãÂéüÊ®£Ë≥áÊñôÁöÑ‰∫ã‰ª∂Êó•Ë™å‰∏≠Ëá™ÂãïÁî¢Áîü„ÄÇÈõñÁÑ∂Ê≠§È°ûÊ®°ÂûãÊ¶ÇÊã¨‰∫ÜÁâπÂÆöÂ∑≤Ë®òÈåÑÊµÅÁ®ãÂü∑Ë°åÁöÑÊéßÂà∂ÊµÅÁ®ãÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏‰πüÊúÉÈôÑÂä†Ë°åÁÇ∫Áµ±Ë®àË≥áÊñôÔºå‰æãÂ¶ÇÂü∑Ë°åÈ†ªÁéá„ÄÇÂü∫ÊñºÊ≠§Ôºå‰∏ÄÊó¶Ê®°ÂûãÁôºÂ∏ÉÔºåÂ∞±ÂèØ‰ª•ÈáçÂª∫ÈóúÊñºÂéüÂßãÊµÅÁ®ãÂü∑Ë°åÁöÑÁâπÂÆöË¶ãËß£Ôºå‰ª•‰æøÂ§ñÈÉ®ÊñπÂèØ‰ª•ÊèêÂèñÊúâÈóúÊ•≠ÂãôÊµÅÁ®ãÁöÑÊ©üÂØÜË≥áË®ä„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊòØÁ¨¨‰∏ÄÂÄãÊ†πÊìöÊµÅÁ®ãÊ®°ÂûãÂØ¶Ë≠âË™øÊü•Ê≠§È°ûÈáçÂª∫ÂòóË©¶ÁöÑÂ∑•‰Ωú„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏çÂêåÁöÑÊí≠ÊîæÁ≠ñÁï•ÔºåÈÄô‰∫õÁ≠ñÁï•ÂæûÊµÅÁ®ãÊ®π‰∏≠ÈáçÂª∫ÊéßÂà∂ÊµÅÁ®ãÔºå‰∏¶ÂèØËÉΩÂà©Áî®È†ªÁéáË®ªËß£„ÄÇÁÇ∫‰∫ÜË©ï‰º∞Ê≠§È°ûÈáçÂª∫ÊîªÊìäÂ∞çÊµÅÁ®ãÊ®°ÂûãÁöÑÊΩõÂú®ÊàêÂäüÁéáÔºå‰ª•ÂèäÂõ†Ê≠§ÁôºÂ∏ÉÂÆÉÂÄëÊâÄÂ∏∂‰æÜÁöÑÈ¢®Èö™ÔºåÊàëÂÄëÂ∞áÈáçÂª∫ÁöÑÊµÅÁ®ãÂü∑Ë°åËàáÂ§öÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÁöÑÂéüÂßãÊó•Ë™å‰∏≠ÁöÑÂü∑Ë°åÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection**
2409.10985v1 by Hsi-Che Lin, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee

Speech Emotion Recognition (SER) is a crucial component in developing
general-purpose AI agents capable of natural human-computer interaction.
However, building robust multilingual SER systems remains challenging due to
the scarcity of labeled data in languages other than English and Chinese. In
this paper, we propose an approach to enhance SER performance in low SER
resource languages by leveraging data from high-resource languages.
Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined
with a novel bootstrapping data selection pipeline to generate labeled data in
the target language. Extensive experiments demonstrate that our method is both
effective and generalizable across different upstream models and languages. Our
results suggest that this approach can facilitate the development of more
scalable and robust multilingual SER systems.

ÊëòË¶ÅÔºöË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) ÊòØÈñãÁôºÂÖ∑ÂÇôËá™ÁÑ∂‰∫∫Ê©ü‰∫íÂãïËÉΩÂäõ‰πãÈÄöÁî® AI ‰ª£ÁêÜÁ®ãÂºè‰∏≠Ëá≥ÈóúÈáçË¶ÅÁöÑÁµÑÊàêÈÉ®ÂàÜ„ÄÇ
ÁÑ∂ËÄåÔºåÁî±ÊñºËã±Ë™ûÂíå‰∏≠Êñá‰ª•Â§ñË™ûË®ÄÊ®ôÁ±§Ë≥áÊñôÁöÑÁ®ÄÂ∞ëÊÄßÔºåÂª∫ÊßãÂº∑ÂÅ•ÁöÑÂ§öË™ûË®Ä SER Á≥ªÁµ±‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÈÄèÈÅéÂà©Áî®È´òË≥áÊ∫êË™ûË®ÄÁöÑË≥áÊñô‰æÜÂ¢ûÂº∑‰Ωé SER Ë≥áÊ∫êË™ûË®Ä‰∏≠ÁöÑ SER ÊïàËÉΩ„ÄÇ
ÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®Ë°®ÈÅîÂºèÁöÑË™ûÈü≥ËΩâË™ûÈü≥ÁøªË≠Ø (S2ST)Ôºå‰∏¶ÁµêÂêà‰∏ÄÂÄãÊñ∞Á©éÁöÑÂºïÂ∞éË≥áÊñôÈÅ∏ÂèñÁÆ°ÈÅìÔºå‰ª•Áî¢ÁîüÁõÆÊ®ôË™ûË®Ä‰∏≠ÁöÑÊ®ôÁ±§Ë≥áÊñô„ÄÇ
Âª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®‰∏çÂêåÁöÑ‰∏äÊ∏∏Ê®°ÂûãÂíåË™ûË®Ä‰∏≠Êó¢ÊúâÊïàÂèàÂÖ∑ÊúâÊôÆÈÅçÊÄß„ÄÇ
ÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•‰øÉÈÄ≤Êõ¥ÂÖ∑ÂèØÊì¥ÂÖÖÊÄßÂíåÂº∑ÂÅ•ÊÄßÁöÑÂ§öË™ûË®Ä SER Á≥ªÁµ±ÁöÑÈñãÁôº„ÄÇ

##### **Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data**
2409.10969v1 by Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen

While large language models (LLMs) have been explored in the speech domain
for both generation and recognition tasks, their applications are predominantly
confined to the monolingual scenario, with limited exploration in multilingual
and code-switched (CS) contexts. Additionally, speech generation and
recognition tasks are often handled separately, such as VALL-E and Qwen-Audio.
In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating
multilingual speech generation and recognition tasks within the single LLM.
Furthermore, we develop an effective data construction approach that splits and
concatenates words from different languages to equip LLMs with CS synthesis
ability without relying on CS data. The experimental results demonstrate that
our model outperforms other baselines with a comparable data scale.
Furthermore, our data construction approach not only equips LLMs with CS speech
synthesis capability with comparable speaker consistency and similarity to any
given speaker, but also improves the performance of LLMs in multilingual speech
generation and recognition tasks.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Ë™ûÈü≥È†òÂüü‰∏≠Êé¢Ë®éÁîüÊàêÂíåËæ®Ë≠ò‰ªªÂãôÔºå‰ΩÜÂÖ∂ÊáâÁî®‰∏ªË¶Å‰æ∑ÈôêÊñºÂñÆË™ûÂ¢ÉÔºåÂú®Â§öË™ûË®ÄÂíå‰ª£Á¢ºËΩâÊèõ (CS) Ë™ûÂ¢É‰∏≠ÁöÑÊé¢Á¥¢ÊúâÈôê„ÄÇÊ≠§Â§ñÔºåË™ûÈü≥ÁîüÊàêÂíåËæ®Ë≠ò‰ªªÂãôÈÄöÂ∏∏ÊòØÂàÜÈñãËôïÁêÜÁöÑÔºå‰æãÂ¶Ç VALL-E Âíå Qwen-Audio„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ§öË™ûË®ÄÂ§ö‰ªªÂãô (MLMT) Ê®°ÂûãÔºåÂ∞áÂ§öË™ûË®ÄË™ûÈü≥ÁîüÊàêÂíåËæ®Ë≠ò‰ªªÂãôÊï¥ÂêàÂú®ÂñÆ‰∏Ä LLM ‰∏≠„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∏ÄÁ®ÆÊúâÊïàË≥áÊñôÂª∫ÊßãÊñπÊ≥ïÔºåÂ∞á‰∏çÂêåË™ûË®ÄÁöÑÂ≠óË©ûÊãÜÂàÜÂíå‰∏≤Êé•Ôºå‰ª•ÂÖ∑ÂÇô CS ÂêàÊàêËÉΩÂäõÁöÑ LLMÔºåËÄåÁÑ°ÈúÄ‰æùË≥¥ CS Ë≥áÊñô„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂÖ∂‰ªñÂü∫Ê∫ñÔºåË≥áÊñôË¶èÊ®°Áõ∏Áï∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑË≥áÊñôÂª∫ÊßãÊñπÊ≥ï‰∏çÂÉÖËÆì LLM ÂÖ∑ÂÇô CS Ë™ûÈü≥ÂêàÊàêËÉΩÂäõÔºåÂÖ∑ÊúâËàá‰ªª‰ΩïÁâπÂÆöË™™Ë©±ËÄÖÁõ∏Áï∂ÁöÑË™™Ë©±ËÄÖ‰∏ÄËá¥ÊÄßÂíåÁõ∏‰ººÊÄßÔºåËÄå‰∏îÈÇÑÊèêÂçá‰∫Ü LLM Âú®Â§öË™ûË®ÄË™ûÈü≥ÁîüÊàêÂíåËæ®Ë≠ò‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩ„ÄÇ

##### **Cross-lingual transfer of multilingual models on low resource African Languages**
2409.10965v1 by Harish Thangaraj, Ananya Chenat, Jaskaran Singh Walia, Vukosi Marivate

Large multilingual models have significantly advanced natural language
processing (NLP) research. However, their high resource demands and potential
biases from diverse data sources have raised concerns about their effectiveness
across low-resource languages. In contrast, monolingual models, trained on a
single language, may better capture the nuances of the target language,
potentially providing more accurate results. This study benchmarks the
cross-lingual transfer capabilities from a high-resource language to a
low-resource language for both, monolingual and multilingual models, focusing
on Kinyarwanda and Kirundi, two Bantu languages. We evaluate the performance of
transformer based architectures like Multilingual BERT (mBERT), AfriBERT, and
BantuBERTa against neural-based architectures such as BiGRU, CNN, and char-CNN.
The models were trained on Kinyarwanda and tested on Kirundi, with fine-tuning
applied to assess the extent of performance improvement and catastrophic
forgetting. AfriBERT achieved the highest cross-lingual accuracy of 88.3% after
fine-tuning, while BiGRU emerged as the best-performing neural model with 83.3%
accuracy. We also analyze the degree of forgetting in the original language
post-fine-tuning. While monolingual models remain competitive, this study
highlights that multilingual models offer strong cross-lingual transfer
capabilities in resource limited settings.

ÊëòË¶ÅÔºöÂ§ßÂûãÂ§öË™ûË®ÄÊ®°ÂûãÂ∑≤Â§ßÂπÖÊèêÂçáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Á†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÂÖ∂È´òË≥áÊ∫êÈúÄÊ±ÇÂíå‰æÜËá™‰∏çÂêåË≥áÊñô‰æÜÊ∫êÁöÑÊΩõÂú®ÂÅèÂ∑ÆÂ∑≤ÂºïÁôº‰∫∫ÂÄëÂ∞çÂÖ∂Âú®‰ΩéË≥áÊ∫êË™ûË®Ä‰∏≠ÁöÑÊúâÊïàÊÄßÁöÑÊìîÊÜÇ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÈáùÂ∞çÂñÆ‰∏ÄË™ûË®ÄË®ìÁ∑¥ÁöÑÂñÆË™ûË®ÄÊ®°ÂûãÂèØËÉΩÊõ¥ËÉΩÊçïÊçâÁõÆÊ®ôË™ûË®ÄÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•ÔºåÊΩõÂú®Êèê‰æõÊõ¥Ê∫ñÁ¢∫ÁöÑÁµêÊûú„ÄÇÊú¨Á†îÁ©∂Âü∫Ê∫ñÊ∏¨Ë©¶‰∫ÜÂæûÈ´òË≥áÊ∫êË™ûË®ÄÂà∞‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑË∑®Ë™ûË®ÄËΩâÁßªËÉΩÂäõÔºåÈÅ©Áî®ÊñºÂñÆË™ûË®ÄÊ®°ÂûãÂíåÂ§öË™ûË®ÄÊ®°ÂûãÔºåÈáçÈªûÈóúÊ≥®Áè≠ÂúñË™ûÊóèÁöÑÁõßÂÆâÈÅîË™ûÂíåÂü∫ÈöÜËø™Ë™û„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÂü∫Êñº Transformer Êû∂ÊßãÁöÑÊïàËÉΩÔºå‰æãÂ¶ÇÂ§öË™ûË®Ä BERT (mBERT)„ÄÅAfriBERT Âíå BantuBERTaÔºå‰ª•ÂèäÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊû∂ÊßãÔºå‰æãÂ¶Ç BiGRU„ÄÅCNN Âíå char-CNN„ÄÇÈÄô‰∫õÊ®°ÂûãÂú®ÁõßÂÆâÈÅîË™û‰∏äË®ìÁ∑¥Ôºå‰∏¶Âú®Âü∫ÈöÜËø™Ë™û‰∏äÊ∏¨Ë©¶Ôºå‰∏¶ÊáâÁî®ÂæÆË™ø‰æÜË©ï‰º∞ÊïàËÉΩÊîπÂñÑÁöÑÁ®ãÂ∫¶ÂíåÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇAfriBERT Âú®ÂæÆË™øÂæåÈÅîÂà∞‰∫Ü 88.3% ÁöÑÊúÄÈ´òË∑®Ë™ûË®ÄÊ∫ñÁ¢∫Â∫¶ÔºåËÄå BiGRU Ââá‰ª• 83.3% ÁöÑÊ∫ñÁ¢∫Â∫¶ÊàêÁÇ∫Ë°®ÁèæÊúÄ‰Ω≥ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°Âûã„ÄÇÊàëÂÄëÈÇÑÂàÜÊûê‰∫ÜÂæÆË™øÂæåÂéüÂßãË™ûË®ÄÁöÑÈÅ∫ÂøòÁ®ãÂ∫¶„ÄÇÈõñÁÑ∂ÂñÆË™ûË®ÄÊ®°Âûã‰ªçÁÑ∂ÂÖ∑ÊúâÁ´∂Áà≠ÂäõÔºå‰ΩÜÊú¨Á†îÁ©∂Âº∑Ë™øÂ§öË™ûË®ÄÊ®°ÂûãÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠Êèê‰æõ‰∫ÜÂº∑Â§ßÁöÑË∑®Ë™ûË®ÄËΩâÁßªËÉΩÂäõ„ÄÇ

##### **Active learning for energy-based antibody optimization and enhanced screening**
2409.10964v2 by Kairi Furui, Masahito Ohue

Accurate prediction and optimization of protein-protein binding affinity is
crucial for therapeutic antibody development. Although machine learning-based
prediction methods $\Delta\Delta G$ are suitable for large-scale mutant
screening, they struggle to predict the effects of multiple mutations for
targets without existing binders. Energy function-based methods, though more
accurate, are time consuming and not ideal for large-scale screening. To
address this, we propose an active learning workflow that efficiently trains a
deep learning model to learn energy functions for specific targets, combining
the advantages of both approaches. Our method integrates the RDE-Network deep
learning model with Rosetta's energy function-based Flex ddG to efficiently
explore mutants. In a case study targeting HER2-binding Trastuzumab mutants,
our approach significantly improved the screening performance over random
selection and demonstrated the ability to identify mutants with better binding
properties without experimental $\Delta\Delta G$ data. This workflow advances
computational antibody design by combining machine learning, physics-based
computations, and active learning to achieve more efficient antibody
development.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫È†êÊ∏¨ÂíåÊúÄ‰Ω≥ÂåñËõãÁôΩË≥™-ËõãÁôΩË≥™ÁµêÂêàË¶™ÂíåÂäõÂ∞çÊñºÊ≤ªÁôÇÊÄßÊäóÈ´îÈñãÁôºËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°Âü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑÈ†êÊ∏¨ÊñπÊ≥ï $\Delta\Delta G$ ÈÅ©Áî®ÊñºÂ§ßË¶èÊ®°Á™ÅËÆäÁØ©ÈÅ∏Ôºå‰ΩÜÂÆÉÂÄëÈõ£‰ª•È†êÊ∏¨Ê≤íÊúâÁèæÊúâÁµêÂêàÁâ©ÁöÑÁõÆÊ®ôÁâ©ÁöÑÂ§öÈáçÁ™ÅËÆäÊïàÊáâ„ÄÇÂü∫ÊñºËÉΩÈáèÂáΩÊï∏ÁöÑÊñπÊ≥ïÈõñÁÑ∂Êõ¥Ê∫ñÁ¢∫Ôºå‰ΩÜËÄóÊôÇ‰∏î‰∏çÈÅ©Áî®ÊñºÂ§ßË¶èÊ®°ÁØ©ÈÅ∏„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏ªÂãïÂ≠∏ÁøíÂ∑•‰ΩúÊµÅÁ®ãÔºåÊúâÊïàË®ìÁ∑¥Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰ª•Â≠∏ÁøíÁâπÂÆöÁõÆÊ®ôÁâ©ÁöÑËÉΩÈáèÂáΩÊï∏ÔºåÁµêÂêàÂÖ©Á®ÆÊñπÊ≥ïÁöÑÂÑ™Èªû„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫Ü RDE-Network Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãËàá Rosetta Âü∫ÊñºËÉΩÈáèÂáΩÊï∏ÁöÑ Flex ddGÔºå‰ª•ÊúâÊïàÊé¢Á¥¢Á™ÅËÆä„ÄÇÂú®ÈáùÂ∞ç HER2 ÁµêÂêà Trastuzumab Á™ÅËÆäÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊòéÈ°ØÊîπÂñÑ‰∫ÜÈö®Ê©üÈÅ∏ÊìáÁöÑÁØ©ÈÅ∏ÊïàËÉΩÔºå‰∏¶Ë≠âÊòé‰∫ÜÂú®Ê≤íÊúâÂØ¶È©ó $\Delta\Delta G$ Ë≥áÊñôÁöÑÊÉÖÊ≥Å‰∏ãË≠òÂà•ÂÖ∑ÊúâÊõ¥Â•ΩÁµêÂêàÁâπÊÄßÁöÑÁ™ÅËÆäÈ´îÁöÑËÉΩÂäõ„ÄÇÈÄôÂÄãÂ∑•‰ΩúÊµÅÁ®ãÁµêÂêàÊ©üÂô®Â≠∏Áøí„ÄÅÂü∫ÊñºÁâ©ÁêÜÁöÑÈÅãÁÆóÂíå‰∏ªÂãïÂ≠∏ÁøíÔºåÊé®ÈÄ≤‰∫ÜË®àÁÆóÊäóÈ´îË®≠Ë®àÔºå‰ª•ÂØ¶ÁèæÊõ¥ÊúâÊïàÁöÑÊäóÈ´îÈñãÁôº„ÄÇ

##### **Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning**
2409.10956v1 by Min-Yeong Park, Jae-Ho Lee, Gyeong-Moon Park

Incremental Learning (IL) aims to accumulate knowledge from sequential input
tasks while overcoming catastrophic forgetting. Existing IL methods typically
assume that an incoming task has only increments of classes or domains,
referred to as Class IL (CIL) or Domain IL (DIL), respectively. In this work,
we consider a more challenging and realistic but under-explored IL scenario,
named Versatile Incremental Learning (VIL), in which a model has no prior of
which of the classes or domains will increase in the next task. In the proposed
VIL scenario, the model faces intra-class domain confusion and inter-domain
class confusion, which makes the model fail to accumulate new knowledge without
interference with learned knowledge. To address these issues, we propose a
simple yet effective IL framework, named Incremental Classifier with Adaptation
Shift cONtrol (ICON). Based on shifts of learnable modules, we design a novel
regularization method called Cluster-based Adaptation Shift conTrol (CAST) to
control the model to avoid confusion with the previously learned knowledge and
thereby accumulate the new knowledge more effectively. Moreover, we introduce
an Incremental Classifier (IC) which expands its output nodes to address the
overwriting issue from different domains corresponding to a single class while
maintaining the previous knowledge. We conducted extensive experiments on three
benchmarks, showcasing the effectiveness of our method across all the
scenarios, particularly in cases where the next task can be randomly altered.
Our implementation code is available at https://github.com/KHU-AGI/VIL.

ÊëòË¶ÅÔºöÂ¢ûÈáèÂ≠∏Áøí (IL) ÁöÑÁõÆÊ®ôÊòØÂæûÂ∫èÂàóËº∏ÂÖ•‰ªªÂãô‰∏≠Á¥ØÁ©çÁü•Ë≠òÔºåÂêåÊôÇÂÖãÊúçÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÁèæÊúâÁöÑ IL ÊñπÊ≥ïÈÄöÂ∏∏ÂÅáË®≠Êñ∞ÈÄ≤‰ªªÂãôÂÉÖÊúâÈ°ûÂà•ÊàñÈ†òÂüüÁöÑÂ¢ûÈáèÔºåÂàÜÂà•Á®±ÁÇ∫È°ûÂà• IL (CIL) ÊàñÈ†òÂüü IL (DIL)„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëËÄÉÊÖÆ‰∏ÄÂÄãÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÂíåÁèæÂØ¶ÊÄß‰ΩÜÊú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑ IL Â†¥ÊôØÔºåÁ®±ÁÇ∫Â§öÂäüËÉΩÂ¢ûÈáèÂ≠∏Áøí (VIL)ÔºåÂÖ∂‰∏≠Ê®°ÂûãÊ≤íÊúâ‰ªª‰ΩïÂÖàÈ©óÁü•Ë≠òÔºå‰∏çÁü•ÈÅìÂì™ÂÄãÈ°ûÂà•ÊàñÈ†òÂüüÊúÉÂú®‰∏ã‰∏ÄÈ†Ö‰ªªÂãô‰∏≠Â¢ûÂä†„ÄÇÂú®ÊèêË≠∞ÁöÑ VIL Â†¥ÊôØ‰∏≠ÔºåÊ®°ÂûãÈù¢Ëá®È°ûÂÖßÈ†òÂüüÊ∑∑Ê∑ÜÂíåÈ†òÂüüÈñìÈ°ûÂà•Ê∑∑Ê∑ÜÔºåÈÄô‰ΩøÂæóÊ®°ÂûãÁÑ°Ê≥ïÁ¥ØÁ©çÊñ∞Áü•Ë≠òÔºåËÄå‰∏çÊúÉÂèóÂà∞Â≠∏ÁøíÁü•Ë≠òÁöÑÂπ≤Êìæ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑ IL Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫Â∏∂ÈÅ©Êáâ‰ΩçÁßªÊéßÂà∂ÁöÑÂ¢ûÈáèÂàÜÈ°ûÂô® (ICON)„ÄÇÂü∫ÊñºÂèØÂ≠∏ÁøíÊ®°ÁµÑÁöÑ‰ΩçÁßªÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Âü∫ÊñºÁæ§ÈõÜÁöÑÈÅ©Êáâ‰ΩçÁßªÊéßÂà∂ (CAST) ÁöÑÊñ∞Ê≠£ÂâáÂåñÊñπÊ≥ïÔºå‰ª•ÊéßÂà∂Ê®°ÂûãÈÅøÂÖçËàáÂÖàÂâçÂ≠∏ÁøíÁöÑÁü•Ë≠òÊ∑∑Ê∑ÜÔºåÂæûËÄåÊõ¥ÊúâÊïàÂú∞Á¥ØÁ©çÊñ∞Áü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ¢ûÈáèÂàÜÈ°ûÂô® (IC)ÔºåÂÆÉÊì¥Â±ïÂÖ∂Ëº∏Âá∫ÁØÄÈªû‰ª•Ëß£Ê±∫Â∞çÊáâÊñºÂñÆ‰∏ÄÈ°ûÂà•ÁöÑ‰∏çÂêåÈ†òÂüüÁöÑË¶ÜÂØ´ÂïèÈ°åÔºåÂêåÊôÇÁ∂≠Ë≠∑ÂÖàÂâçÁöÑÁü•Ë≠ò„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂü∫Ê∫ñ‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÊâÄÊúâÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÁâπÂà•ÊòØÂú®‰∏ã‰∏ÄÈ†Ö‰ªªÂãôÂèØ‰ª•Èö®Ê©üÊõ¥ÊîπÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÊàëÂÄëÁöÑÂØ¶‰ΩúÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/KHU-AGI/VIL ÂèñÂæó„ÄÇ

##### **Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style**
2409.10955v1 by Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li

Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÈÄèÈÅéÂ∞áÂ§ñÈÉ®Ë≥áË®äÁ¥çÂÖ•ÂõûÊáâÁîüÊàêÁ®ãÂ∫è‰∏≠ÔºåÊîπÂñÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåLLM ÁöÑ‰∏ä‰∏ãÊñáÂø†ÂØ¶Â∫¶Â¶Ç‰Ωï‰ª•ÂèäÂì™‰∫õÂõ†Á¥†ÊúÉÂΩ±Èüø LLM ÁöÑ‰∏ä‰∏ãÊñáÂø†ÂØ¶Â∫¶ÔºåÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®ÈÄôÂÄãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éË®òÊÜ∂Âº∑Â∫¶ÂíåË≠âÊìöÂëàÁèæÂ∞ç LLM Â∞çÂ§ñÈÉ®Ë≠âÊìöÁöÑÊé•ÂèóÁ®ãÂ∫¶ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÈÄèÈÅéÊ∏¨Èáè LLM Â∞çÂêå‰∏ÄÂÄãÂïèÈ°åÁöÑ‰∏çÂêåÂêåÁæ©Ë©ûÊîπÂØ´ÁöÑÂõûÊáâÁöÑÂàÜÊ≠ßÔºå‰æÜÈáèÂåñ LLM ÁöÑË®òÊÜ∂Âº∑Â∫¶ÔºåÈÄôÊòØÂÖàÂâçÁ†îÁ©∂Êú™ËÄÉÊÖÆÁöÑ„ÄÇÊàëÂÄë‰πüÁîüÊàê‰∏çÂêåÈ¢®Ê†ºÁöÑË≠âÊìöÔºå‰ª•Ë©ï‰º∞‰∏çÂêåÈ¢®Ê†ºË≠âÊìöÁöÑÊïàÊûú„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãË≥áÊñôÈõÜÈÄ≤Ë°åË©ï‰º∞ÔºöÂåÖÂê´ÁÜ±ÈñÄÂïèÈ°åÁöÑËá™ÁÑ∂ÂïèÈ°å (NQ) ÂíåÂåÖÂê´Èï∑Â∞æÂïèÈ°åÁöÑ popQA„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂ∞çÊñºË®òÊÜ∂Âº∑Â∫¶È´òÁöÑÂïèÈ°åÔºåLLM Êõ¥ÂèØËÉΩ‰æùË≥¥ÂÖßÈÉ®Ë®òÊÜ∂ÔºåÁâπÂà•ÊòØÂ∞çÊñºËºÉÂ§ßÁöÑ LLMÔºå‰æãÂ¶Ç GPT-4„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåËàáÂñÆÁ¥îÁöÑÈáçË§áÊàñÂ¢ûÂä†Á¥∞ÁØÄÁõ∏ÊØîÔºåÂëàÁèæÂêåÁæ©Ë©ûÊîπÂØ´ÁöÑË≠âÊìöÊúÉÈ°ØËëóÂ¢ûÂä† LLM ÁöÑÊé•ÂèóÂ∫¶„ÄÇ

##### **Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**
2409.10932v1 by Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M

Coronary heart disease (CHD) is a severe cardiac disease, and hence, its
early diagnosis is essential as it improves treatment results and saves money
on medical care. The prevailing development of quantum computing and machine
learning (ML) technologies may bring practical improvement to the performance
of CHD diagnosis. Quantum machine learning (QML) is receiving tremendous
interest in various disciplines due to its higher performance and capabilities.
A quantum leap in the healthcare industry will increase processing power and
optimise multiple models. Techniques for QML have the potential to forecast
cardiac disease and help in early detection. To predict the risk of coronary
heart disease, a hybrid approach utilizing an ensemble machine learning model
based on QML classifiers is presented in this paper. Our approach, with its
unique ability to address multidimensional healthcare data, reassures the
method's robustness by fusing quantum and classical ML algorithms in a
multi-step inferential framework. The marked rise in heart disease and death
rates impacts worldwide human health and the global economy. Reducing cardiac
morbidity and mortality requires early detection of heart disease. In this
research, a hybrid approach utilizes techniques with quantum computing
capabilities to tackle complex problems that are not amenable to conventional
machine learning algorithms and to minimize computational expenses. The
proposed method has been developed in the Raspberry Pi 5 Graphics Processing
Unit (GPU) platform and tested on a broad dataset that integrates clinical and
imaging data from patients suffering from CHD and healthy controls. Compared to
classical machine learning models, the accuracy, sensitivity, F1 score, and
specificity of the proposed hybrid QML model used with CHD are manifold higher.

ÊëòË¶ÅÔºöÂÜ†ÁãÄÂãïËÑàÂøÉËáüÁóÖ (CHD) ÊòØ‰∏ÄÁ®ÆÂö¥ÈáçÁöÑÁñæÁóÖÔºåÂõ†Ê≠§ÔºåÊó©ÊúüË®∫Êñ∑Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ÊîπÂñÑÊ≤ªÁôÇÁµêÊûú‰∏¶ÁØÄÁúÅÈÜ´ÁôÇ‰øùÂÅ•Ë≤ªÁî®„ÄÇÈáèÂ≠êË®àÁÆóÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊäÄË°ìÁöÑÁõõË°åÁôºÂ±ïÂèØËÉΩÊúÉÂ∞ç CHD Ë®∫Êñ∑ÁöÑÊÄßËÉΩÂ∏∂‰æÜÂØ¶ÈöõÊîπÂñÑ„ÄÇÈáèÂ≠êÊ©üÂô®Â≠∏Áøí (QML) Áî±ÊñºÂÖ∂Êõ¥È´òÁöÑÊÄßËÉΩÂíåËÉΩÂäõÔºåÂú®ÂêÑÂÄãÈ†òÂüüÂºïËµ∑‰∫ÜÊ•µÂ§ßÁöÑËààË∂£„ÄÇÈÜ´ÁôÇ‰øùÂÅ•Ë°åÊ•≠ÁöÑÈáèÂ≠êÈ£õË∫çÂ∞áÂ¢ûÂä†ËôïÁêÜËÉΩÂäõ‰∏¶ÂÑ™ÂåñÂ§öÂÄãÊ®°Âûã„ÄÇQML ÁöÑÊäÄË°ìÊúâÊΩõÂäõÈ†êÊ∏¨ÂøÉËáüÁóÖ‰∏¶Âπ´Âä©Êó©ÊúüÁôºÁèæ„ÄÇÁÇ∫‰∫ÜÈ†êÊ∏¨ÂÜ†ÁãÄÂãïËÑàÂøÉËáüÁóÖÁöÑÈ¢®Èö™ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº QML ÂàÜÈ°ûÂô®ÁöÑÊ∑∑ÂêàÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊ∑∑ÂêàÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÈÄôÁ®ÆÊñπÊ≥ïÂÖ∑ÂÇôËôïÁêÜÂ§öÁ∂≠ÈÜ´ÁôÇ‰øùÂÅ•Êï∏ÊìöÁöÑÁç®ÁâπËÉΩÂäõÔºåÈÄöÈÅéÂú®Â§öÊ≠•È©üÊé®ÁêÜÊ°ÜÊû∂‰∏≠ËûçÂêàÈáèÂ≠êÂíåÁ∂ìÂÖ∏ ML ÊºîÁÆóÊ≥ïÔºåÁ¢∫‰øù‰∫ÜË©≤ÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÂøÉËáüÁóÖÂíåÊ≠ª‰∫°ÁéáÁöÑÈ°ØËëó‰∏äÂçáÂΩ±Èüø‰∫ÜÂÖ®ÁêÉ‰∫∫È°ûÂÅ•Â∫∑ÂíåÂÖ®ÁêÉÁ∂ìÊøü„ÄÇÈôç‰ΩéÂøÉËáüÁôºÁóÖÁéáÂíåÊ≠ª‰∫°ÁéáÈúÄË¶ÅÂ∞çÂøÉËáüÁóÖÈÄ≤Ë°åÊó©ÊúüÁôºÁèæ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠Ôºå‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÂà©Áî®ÂÖ∑ÊúâÈáèÂ≠êË®àÁÆóËÉΩÂäõÁöÑÊäÄË°ì‰æÜËß£Ê±∫ÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁÑ°Ê≥ïËß£Ê±∫ÁöÑË§áÈõúÂïèÈ°åÔºå‰∏¶ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Ê∏õÂ∞ëË®àÁÆóÈñãÈä∑„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∑≤Âú® Raspberry Pi 5 Áπ™ÂúñËôïÁêÜÂñÆÂÖÉ (GPU) Âπ≥Ëá∫‰∏äÈñãÁôºÔºå‰∏¶Âú®‰∏ÄÂÄãÂª£Ê≥õÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶ÔºåË©≤Ë≥áÊñôÈõÜÊï¥Âêà‰∫ÜÊÇ£Êúâ CHD ÂíåÂÅ•Â∫∑Â∞çÁÖßËÄÖÁöÑËá®Â∫äÂíåÂΩ±ÂÉèÊï∏Êìö„ÄÇËàáÁ∂ìÂÖ∏Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊ∑∑Âêà QML Ê®°ÂûãËàá CHD ‰∏ÄËµ∑‰ΩøÁî®ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÊïèÊÑüÊÄß„ÄÅF1 ÂàÜÊï∏ÂíåÁâπÁï∞ÊÄßÊõ¥È´ò„ÄÇ

##### **Propulsion: Steering LLM with Tiny Fine-Tuning**
2409.10927v2 by Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat

The rapid advancements in Large Language Models (LLMs) have revolutionized
natural language processing (NLP) and related fields. However, fine-tuning
these models for specific tasks remains computationally expensive and risks
degrading pre-learned features. To address these challenges, we propose
Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to
optimize task-specific performance while drastically reducing computational
overhead. Inspired by the concept of controlled adjustments in physical motion,
Propulsion selectively re-scales specific dimensions of a pre-trained model,
guiding output predictions toward task objectives without modifying the model's
parameters. By introducing lightweight, trainable Propulsion parameters at the
pre-trained layer, we minimize the number of parameters updated during
fine-tuning, preventing overfitting or overwriting of existing knowledge. Our
theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows
that Propulsion approximates the performance of full fine-tuning with far fewer
trainable parameters. Empirically, Propulsion reduces the parameter count from
355.3 million to just 0.086 million, achieving over a 10x reduction compared to
standard approaches like LoRA while maintaining competitive performance across
benchmarks.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÁõ∏ÈóúÈ†òÂüü„ÄÇÁÑ∂ËÄåÔºåÈáùÂ∞çÁâπÂÆö‰ªªÂãôÂæÆË™øÈÄô‰∫õÊ®°ÂûãÂú®Ë®àÁÆó‰∏ä‰ªçÁÑ∂ÂæàÊòÇË≤¥Ôºå‰∏¶‰∏îÊúâÊêçÂÆ≥È†êÂÖàÂ≠∏ÁøíÂà∞ÁöÑÁâπÂæµÁöÑÈ¢®Èö™„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊé®ÈÄ≤Ôºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂèÉÊï∏È´òÊïàÂæÆË™ø (PEFT) ÊñπÊ≥ïÔºåÊó®Âú®ÂÑ™ÂåñÁâπÂÆöÊñº‰ªªÂãôÁöÑÊïàËÉΩÔºåÂêåÊôÇÂ§ßÂπÖÊ∏õÂ∞ëË®àÁÆóÈñãÈä∑„ÄÇÊé®ÈÄ≤ÁöÑÈùàÊÑü‰æÜËá™ÊñºÁâ©ÁêÜÈÅãÂãï‰∏≠ÂèóÊéßË™øÊï¥ÁöÑÊ¶ÇÂøµÔºåÂÆÉÊúâÈÅ∏ÊìáÊÄßÂú∞ÈáçÊñ∞Ë™øÊï¥È†êÂÖàË®ìÁ∑¥Ê®°ÂûãÁöÑÁâπÂÆöÁ∂≠Â∫¶ÔºåÂ∞áËº∏Âá∫È†êÊ∏¨ÂºïÂ∞éËá≥‰ªªÂãôÁõÆÊ®ôÔºåËÄå‰∏çÊúÉ‰øÆÊîπÊ®°ÂûãÁöÑÂèÉÊï∏„ÄÇÈÄèÈÅéÂú®È†êÂÖàË®ìÁ∑¥ÁöÑÂ±§‰∏≠ÂºïÂÖ•ËºïÈáèÁ¥ö„ÄÅÂèØË®ìÁ∑¥ÁöÑÊé®ÈÄ≤ÂèÉÊï∏ÔºåÊàëÂÄëÂ∞áÂæÆË™øÊúüÈñìÊõ¥Êñ∞ÁöÑÂèÉÊï∏Êï∏ÈáèÊ∏õËá≥ÊúÄÂ∞ëÔºåÈò≤Ê≠¢ÈÅéÂ∫¶Êì¨ÂêàÊàñË¶ÜÂØ´ÁèæÊúâÁü•Ë≠ò„ÄÇÊàëÂÄëÁöÑÁêÜË´ñÂàÜÊûêÔºåÂú®Á•ûÁ∂ìÂàáÁ∑öÊ†∏ (NTK) ÁêÜË´ñÁöÑÊîØÊè¥‰∏ãÔºåÈ°ØÁ§∫Êé®ÈÄ≤Ëøë‰ººÊñºÂÆåÂÖ®ÂæÆË™øÁöÑÊïàËÉΩÔºå‰ΩÜÂèØË®ìÁ∑¥ÂèÉÊï∏ÂçªÂ∞ëÂæóÂ§ö„ÄÇÊ†πÊìöÁ∂ìÈ©óÔºåÊé®ÈÄ≤Â∞áÂèÉÊï∏Êï∏ÈáèÂæû 3.553 ÂÑÑÊ∏õÂ∞ëÂà∞ÂÉÖ 0.086 Ëê¨ÔºåËàá LoRA Á≠âÊ®ôÊ∫ñÊñπÊ≥ïÁõ∏ÊØîÔºåÊ∏õÂ∞ë‰∫Ü 10 ÂÄç‰ª•‰∏äÔºåÂêåÊôÇÂú®ÂêÑÁ®ÆÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Á∂≠ÊåÅÊúâÁ´∂Áà≠ÂäõÁöÑÊïàËÉΩ„ÄÇ

##### **KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**
2409.10921v1 by Yanbei Jiang, Krista A. Ehinger, Jey Han Lau

Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.

ÊëòË¶ÅÔºöÊé¢Á¥¢Áî±ÁæéÊúØÁªòÁîª‰º†ËææÁöÑÂèô‰∫ãÊòØÂõæÂÉèÂ≠óÂπï‰∏≠ÁöÑÊåëÊàòÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏ç‰ªÖÂáÜÁ°ÆÂú∞Ë°®Á§∫ËßÜËßâÂÜÖÂÆπËÄå‰∏îËøòÊèê‰æõÂØπËâ∫ÊúØÂìÅÂê´‰πâÁöÑÊ∑±ÂÖ•Ëß£ÈáäÁöÑÊèèËø∞„ÄÇÁî±‰∫éÂÖ∂‰∏çÂêåÁöÑËß£ÈáäÂíåË∑®‰∏çÂêåËâ∫ÊúØÊµÅÊ¥æÂíåÈ£éÊ†ºÁöÑ‰∏çÂêåÁæéÂ≠¶ÂéüÂàôÔºåËøôÈ°π‰ªªÂä°ÂØπ‰∫éËâ∫ÊúØÂìÅÂõæÂÉèÊù•ËØ¥Â∞§ÂÖ∂Â§çÊùÇ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü KALE Áü•ËØÜÂ¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁî®‰∫éËâ∫ÊúØÂìÅÈòêÈáäÔºå‰∏ÄÁßçÈÄöËøáÂ∞ÜËâ∫ÊúØÂìÅÂÖÉÊï∞ÊçÆ‰Ωú‰∏∫ÈôÑÂä†Áü•ËØÜÊù•Â¢ûÂº∫Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇKALE ‰ª•‰∏§ÁßçÊñπÂºèÂêàÂπ∂ÂÖÉÊï∞ÊçÆÔºöÈ¶ñÂÖà‰Ωú‰∏∫Áõ¥Êé•ÊñáÊú¨ËæìÂÖ•ÔºåÂÖ∂Ê¨°ÈÄöËøáÂ§öÊ®°ÊÄÅÂºÇÊûÑÁü•ËØÜÂõæ„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÂõæË°®ÁöÑÂ≠¶‰π†Ë°®Á§∫ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÊçüÂ§±ÔºåÂÆÉÊúÄÂ§ßÂåñÂõæÂÉè‰∏éÂÖ∂ÂØπÂ∫îÂÖÉÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKALE Âú®‰ΩøÁî® CIDEr ËØÑ‰º∞Êó∂ÔºåÂú®Âá†‰∏™Ëâ∫ÊúØÂìÅÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩÔºàÁâπÂà´ÊòØ‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂ∑•‰ΩúÁõ∏ÊØîÔºâ„ÄÇËØ•È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/Yanbei-Jiang/Artwork-Interpretation Ëé∑Âæó„ÄÇ

##### **GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval**
2409.10909v1 by Wonduk Seo, Haojie Zhang, Yueyang Zhang, Changhao Zhang, Songyao Duan, Lixin Su, Daiting Shi, Jiashu Zhao, Dawei Yin

Query reformulation is a well-known problem in Information Retrieval (IR)
aimed at enhancing single search successful completion rate by automatically
modifying user's input query. Recent methods leverage Large Language Models
(LLMs) to improve query reformulation, but often generate limited and redundant
expansions, potentially constraining their effectiveness in capturing diverse
intents. In this paper, we propose GenCRF: a Generative Clustering and
Reformulation Framework to capture diverse intentions adaptively based on
multiple differentiated, well-generated queries in the retrieval phase for the
first time. GenCRF leverages LLMs to generate variable queries from the initial
query using customized prompts, then clusters them into groups to distinctly
represent diverse intents. Furthermore, the framework explores to combine
diverse intents query with innovative weighted aggregation strategies to
optimize retrieval performance and crucially integrates a novel Query
Evaluation Rewarding Model (QERM) to refine the process through feedback loops.
Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves
state-of-the-art performance, surpassing previous query reformulation SOTAs by
up to 12% on nDCG@10. These techniques can be adapted to various LLMs,
significantly boosting retriever performance and advancing the field of
Information Retrieval.

ÊëòË¶ÅÔºöÊü•Ë©¢ÈáçÊñ∞Ë°®Ëø∞ÊòØË≥áË®äÊ™¢Á¥¢ (IR) ‰∏≠‰∏ÄÂÄãÁúæÊâÄÂë®Áü•ÁöÑÂïèÈ°åÔºåÊó®Âú®ÈÄèÈÅéËá™Âãï‰øÆÊîπ‰ΩøÁî®ËÄÖÁöÑËº∏ÂÖ•Êü•Ë©¢Ôºå‰æÜÊèêÂçáÂñÆ‰∏ÄÊêúÂ∞ãÁöÑÊàêÂäüÂÆåÊàêÁéá„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÊîπÂñÑÊü•Ë©¢ÈáçÊñ∞Ë°®Ëø∞Ôºå‰ΩÜÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂèóÈôê‰∏îÈáçË§áÁöÑÊì¥ÂÖÖÔºåÊΩõÂú®Âú∞ÈôêÂà∂ÂÖ∂Âú®ÊçïÊçâ‰∏çÂêåÊÑèÂúñÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ GenCRFÔºö‰∏ÄÂÄãÁîüÊàêÂºèÂàÜÁæ§ÂíåÈáçÊñ∞Ë°®Ëø∞Êû∂ÊßãÔºå‰ª•Âú®Ê™¢Á¥¢ÈöéÊÆµÊ†πÊìöÂ§öÂÄãÂ∑ÆÁï∞Âåñ„ÄÅÁî¢ÁîüËâØÂ•ΩÁöÑÊü•Ë©¢Ôºå‰æÜËá™ÈÅ©ÊáâÂú∞ÊçïÊçâ‰∏çÂêåÁöÑÊÑèÂúñ„ÄÇGenCRF Âà©Áî® LLM ÂæûÂàùÂßãÊü•Ë©¢Áî¢ÁîüËÆäÊï∏Êü•Ë©¢Ôºå‰ΩøÁî®Ëá™Ë®ÇÊèêÁ§∫ÔºåÁÑ∂ÂæåÂ∞áÂÆÉÂÄëÂàÜÁæ§Âà∞Áæ§ÁµÑ‰∏≠Ôºå‰ª•ÊòéÁ¢∫Âú∞‰ª£Ë°®‰∏çÂêåÁöÑÊÑèÂúñ„ÄÇÊ≠§Â§ñÔºåË©≤Êû∂ÊßãÊé¢Á¥¢Â∞á‰∏çÂêåÁöÑÊÑèÂúñÊü•Ë©¢ËàáÂâµÊñ∞ÁöÑÂä†Ê¨äËÅöÂêàÁ≠ñÁï•Áõ∏ÁµêÂêàÔºå‰ª•ÊúÄ‰Ω≥ÂåñÊ™¢Á¥¢ÊïàËÉΩÔºå‰∏¶Ëá≥ÈóúÈáçË¶ÅÁöÑÊòØÊï¥Âêà‰∏ÄÂÄãÊñ∞ÁöÑÊü•Ë©¢Ë©ï‰º∞ÂõûÈ•ãÊ®°Âûã (QERM)Ôºå‰ª•ÈÄèÈÅéÂõûÈ•ãËø¥Ë∑Ø‰æÜÊîπÂñÑÈÄôÂÄãÊµÅÁ®ã„ÄÇÂú® BEIR Âü∫Ê∫ñ‰∏äÁöÑÂØ¶Ë≠âÂØ¶È©óË≠âÊòéÔºåGenCRF ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂú® nDCG@10 ‰∏äË∂ÖË∂ä‰∫ÜÂÖàÂâçÁöÑÊü•Ë©¢ÈáçÊñ∞Ë°®Ëø∞ SOTAÔºåÊúÄÈ´òÈÅî 12%„ÄÇÈÄô‰∫õÊäÄË°ìÂèØ‰ª•ÈÅ©ÊáâÂêÑÁ®Æ LLMÔºåÂ§ßÂπÖÊèêÂçáÊ™¢Á¥¢Âô®ÊïàËÉΩÔºå‰∏¶Êé®ÂãïË≥áË®äÊ™¢Á¥¢È†òÂüüÁöÑÈÄ≤Ê≠•„ÄÇ

##### **Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction**
2409.10907v1 by Erwin D. L√≥pez Z., Cheng Tang, Atsushi Shimada

This paper proposes Attention-Seeker, an unsupervised keyphrase extraction
method that leverages self-attention maps from a Large Language Model to
estimate the importance of candidate phrases. Our approach identifies specific
components - such as layers, heads, and attention vectors - where the model
pays significant attention to the key topics of the text. The attention weights
provided by these components are then used to score the candidate phrases.
Unlike previous models that require manual tuning of parameters (e.g.,
selection of heads, prompts, hyperparameters), Attention-Seeker dynamically
adapts to the input text without any manual adjustments, enhancing its
practical applicability. We evaluate Attention-Seeker on four publicly
available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results
demonstrate that, even without parameter tuning, Attention-Seeker outperforms
most baseline models, achieving state-of-the-art performance on three out of
four datasets, particularly excelling in extracting keyphrases from long
documents.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü Attention-SeekerÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁÑ°Áõ£Áù£ÈóúÈçµÂ≠óËêÉÂèñÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ÁöÑËá™Ê≥®ÊÑèÂäõÂúñ‰æÜ‰º∞Ë®àÂÄôÈÅ∏Ë©ûÁµÑÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊâæÂá∫ÁâπÂÆöÁµÑÊàêÈÉ®ÂàÜÔºà‰æãÂ¶ÇÂ±§„ÄÅÈ†≠ÂíåÊ≥®ÊÑèÂäõÂêëÈáèÔºâÔºåÊ®°ÂûãÂú®ÈÄô‰∫õÁµÑÊàêÈÉ®ÂàÜ‰∏≠ÊúÉÂ∞çÊñáÂ≠óÁöÑ‰∏ªË¶Å‰∏ªÈ°åÁµ¶‰∫àÈ°ØËëóÁöÑÊ≥®ÊÑèÂäõ„ÄÇÁÑ∂Âæå‰ΩøÁî®ÈÄô‰∫õÁµÑÊàêÈÉ®ÂàÜÊèê‰æõÁöÑÊ≥®ÊÑèÂäõÊ¨äÈáç‰æÜÁÇ∫ÂÄôÈÅ∏Ë©ûÁµÑË©ïÂàÜ„ÄÇËàáÈúÄË¶ÅÊâãÂãïË™øÊï¥ÂèÉÊï∏Ôºà‰æãÂ¶ÇÈÅ∏ÊìáÈ†≠„ÄÅÊèêÁ§∫„ÄÅË∂ÖÂèÉÊï∏ÔºâÁöÑÂÖàÂâçÊ®°Âûã‰∏çÂêåÔºåAttention-Seeker ÊúÉÂãïÊÖãË™øÊï¥Ëº∏ÂÖ•ÊñáÂ≠óÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÊâãÂãïË™øÊï¥ÔºåÂæûËÄåÂ¢ûÂº∑ÂÖ∂ÂØ¶Áî®ÊÄß„ÄÇÊàëÂÄëÂú®ÂõõÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜ‰∏äË©ï‰º∞ Attention-SeekerÔºöInspec„ÄÅSemEval2010„ÄÅSemEval2017 Âíå Krapivin„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòéÔºåÂç≥‰Ωø‰∏çË™øÊï¥ÂèÉÊï∏ÔºåAttention-Seeker ‰πüÂÑ™ÊñºÂ§ßÂ§öÊï∏Âü∫Á∑öÊ®°ÂûãÔºåÂú®ÂõõÂÄãË≥áÊñôÈõÜ‰∏≠Êúâ‰∏âÂÄãË≥áÊñôÈõÜÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®ÂæûÈï∑Êñá‰ª∂‰∏≠ËêÉÂèñÈóúÈçµÂ≠óÊôÇË°®ÁèæÂá∫Ëâ≤„ÄÇ

##### **WaterQualityNeT: Prediction of Seasonal Water Quality of Nepal Using Hybrid Deep Learning Models**
2409.10898v1 by Biplov Paneru, Bishwash Paneru

Ensuring a safe and uncontaminated water supply is contingent upon the
monitoring of water quality, especially in developing countries such as Nepal,
where water sources are susceptible to pollution. This paper presents a hybrid
deep learning model for predicting Nepal's seasonal water quality using a small
dataset with many water quality parameters. The model integrates convolutional
neural networks (CNN) and recurrent neural networks (RNN) to exploit temporal
and spatial patterns in the data. The results demonstrate significant
improvements in forecast accuracy over traditional methods, providing a
reliable tool for proactive control of water quality. The model that used WQI
parameters to classify people into good, poor, and average groups performed 92%
of the time in testing. Similarly, the R2 score was 0.97 and the root mean
square error was 2.87 when predicting WQI values using regression analysis.
Additionally, a multifunctional application that uses both a regression and a
classification approach is built to predict WQI values.

ÊëòË¶ÅÔºöÁ¢∫‰øùÂÆâÂÖ®‰∏îÊú™ÂèóÊ±°ÊüìÁöÑÊ∞¥Ê∫ê‰æõÊáâÂèñÊ±∫ÊñºÊ∞¥Ë≥™Áõ£Ê∏¨ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞ºÊ≥äÁàæÁ≠âÈñãÁôº‰∏≠ÂúãÂÆ∂ÔºåÈÇ£Ë£°ÁöÑÊ∞¥Ê∫êÂÆπÊòìÂèóÂà∞Ê±°Êüì„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÁî®Êñº‰ΩøÁî®ÂÖ∑ÊúâË®±Â§öÊ∞¥Ë≥™ÂèÉÊï∏ÁöÑÂ∞èÂûãË≥áÊñôÈõÜÈ†êÊ∏¨Â∞ºÊ≥äÁàæÁöÑÂ≠£ÁØÄÊÄßÊ∞¥Ë≥™„ÄÇË©≤Ê®°ÂûãÊï¥Âêà‰∫ÜÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑Ø (RNN) ‰ª•Âà©Áî®Ë≥áÊñô‰∏≠ÁöÑÊôÇÈñìÂíåÁ©∫ÈñìÊ®°Âºè„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåËàáÂÇ≥Áµ±ÊñπÊ≥ïÁõ∏ÊØîÔºåÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶Êúâ‰∫ÜÈ°ØËëóÊèêÈ´òÔºåÁÇ∫‰∏ªÂãïÊéßÂà∂Ê∞¥Ë≥™Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÈù†ÁöÑÂ∑•ÂÖ∑„ÄÇ‰ΩøÁî® WQI ÂèÉÊï∏Â∞á‰∫∫ÂÄëÂàÜÈ°ûÁÇ∫ËâØÂ•Ω„ÄÅ‰∏çËâØÂíå‰∏ÄËà¨ÁµÑÂà•ÁöÑÊ®°ÂûãÂú®Ê∏¨Ë©¶‰∏≠Ë°®ÁèæÁÇ∫ 92%„ÄÇÂêåÊ®£Âú∞ÔºåÂú®‰ΩøÁî®ÂõûÊ≠∏ÂàÜÊûêÈ†êÊ∏¨ WQI ÂÄºÊôÇÔºåR2 ÂàÜÊï∏ÁÇ∫ 0.97ÔºåÂùáÊñπÊ†πË™§Â∑ÆÁÇ∫ 2.87„ÄÇÊ≠§Â§ñÔºåÈÇÑÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂ§öÂäüËÉΩÊáâÁî®Á®ãÂºèÔºåÂÆÉÂêåÊôÇ‰ΩøÁî®ÂõûÊ≠∏ÂíåÂàÜÈ°ûÊñπÊ≥ï‰æÜÈ†êÊ∏¨ WQI ÂÄº„ÄÇ

##### **Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes**
2409.10889v1 by Zhixin Xie, Jun Luo

Real-time deepfake, a type of generative AI, is capable of "creating"
non-existing contents (e.g., swapping one's face with another) in a video. It
has been, very unfortunately, misused to produce deepfake videos (during web
conferences, video calls, and identity authentication) for malicious purposes,
including financial scams and political misinformation. Deepfake detection, as
the countermeasure against deepfake, has attracted considerable attention from
the academic community, yet existing works typically rely on learning passive
features that may perform poorly beyond seen datasets. In this paper, we
propose SFake, a new real-time deepfake detection method that innovatively
exploits deepfake models' inability to adapt to physical interference.
Specifically, SFake actively sends probes to trigger mechanical vibrations on
the smartphone, resulting in the controllable feature on the footage.
Consequently, SFake determines whether the face is swapped by deepfake based on
the consistency of the facial area with the probe pattern. We implement SFake,
evaluate its effectiveness on a self-built dataset, and compare it with six
other detection methods. The results show that SFake outperforms other
detection methods with higher detection accuracy, faster process speed, and
lower memory consumption.

ÊëòË¶ÅÔºöÂç≥ÊôÇÊ∑±Â∫¶ÂÅΩÈÄ†ÊòØ‰∏ÄÁ®ÆÁîüÊàêÂºè AIÔºåËÉΩÂ§†„ÄåÂâµÈÄ†„ÄçÂΩ±Áâá‰∏≠‰∏çÂ≠òÂú®ÁöÑÂÖßÂÆπÔºà‰æãÂ¶ÇÔºåÂ∞áÊüê‰∫∫ÁöÑËáâÊèõÊàêÂè¶‰∏ÄÂÄã‰∫∫Ôºâ„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂÆÉÂ∑≤Ë¢´Êø´Áî®ÊñºË£Ω‰ΩúÊ∑±Â∫¶ÂÅΩÈÄ†ÂΩ±ÁâáÔºàÂú®Á∂≤Ë∑ØÊúÉË≠∞„ÄÅË¶ñË®äÈÄöË©±ÂíåË∫´ÂàÜÈ©óË≠â‰∏≠ÔºâÔºåÁî®ÊñºÊÉ°ÊÑèÁõÆÁöÑÔºåÂåÖÊã¨ÈáëËûçË©êÈ®ôÂíåÊîøÊ≤ªÈåØË™§Ë®äÊÅØ„ÄÇÊ∑±Â∫¶ÂÅΩÈÄ†ÂÅµÊ∏¨‰ΩúÁÇ∫Â∞çÊäóÊ∑±Â∫¶ÂÅΩÈÄ†ÁöÑÂ∞çÁ≠ñÔºåÂ∑≤ÂºïËµ∑Â≠∏Ë°ìÁïåÁöÑÂª£Ê≥õÈóúÊ≥®Ôºå‰ΩÜÁèæÊúâÁöÑ‰ΩúÂìÅÈÄöÂ∏∏‰æùË≥¥ÊñºÂ≠∏ÁøíË¢´ÂãïÁâπÂæµÔºåÈÄô‰∫õÁâπÂæµÂú®Ë∂ÖÂá∫Â∑≤Ë¶ãË≥áÊñôÈõÜÊôÇÂèØËÉΩÊúÉË°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ SFakeÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑÂç≥ÊôÇÊ∑±Â∫¶ÂÅΩÈÄ†ÂÅµÊ∏¨ÊñπÊ≥ïÔºåÂâµÊñ∞Âú∞Âà©Áî®Ê∑±Â∫¶ÂÅΩÈÄ†Ê®°ÂûãÁÑ°Ê≥ïÈÅ©ÊáâÁâ©ÁêÜÂπ≤ÊìæÁöÑÁâπÊÄß„ÄÇÂÖ∑È´î‰æÜË™™ÔºåSFake ‰∏ªÂãïÁôºÈÄÅÊé¢Èáù‰ª•Ëß∏ÁôºÊô∫ÊÖßÂûãÊâãÊ©ü‰∏äÁöÑÊ©üÊ¢∞ÊåØÂãïÔºåÂæûËÄåÂ∞éËá¥ÂΩ±Áâá‰∏≠ÂèØÊéßÁöÑÁâπÂæµ„ÄÇÂõ†Ê≠§ÔºåSFake Ê†πÊìöËáâÈÉ®ÂçÄÂüüËàáÊé¢ÈáùÊ®°ÂºèÁöÑ‰∏ÄËá¥ÊÄß‰æÜÂà§Êñ∑ËáâÈÉ®ÊòØÂê¶Ë¢´Ê∑±Â∫¶ÂÅΩÈÄ†ÊõøÊèõ„ÄÇÊàëÂÄëÂØ¶‰Ωú SFakeÔºåË©ï‰º∞ÂÖ∂Âú®Ëá™Âª∫Ë≥áÊñôÈõÜ‰∏äÁöÑÊúâÊïàÊÄßÔºå‰∏¶Â∞áÂÖ∂ËàáÂÖ∂‰ªñÂÖ≠Á®ÆÂÅµÊ∏¨ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúË°®ÊòéÔºåSFake Âú®Êõ¥È´òÁöÑÂÅµÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÅÊõ¥Âø´ÁöÑËôïÁêÜÈÄüÂ∫¶ÂíåÊõ¥‰ΩéÁöÑË®òÊÜ∂È´îÊ∂àËÄóÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÂÅµÊ∏¨ÊñπÊ≥ï„ÄÇ

##### **CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization**
2409.10883v1 by Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg

Large Language Models (LLMs) have spurred interest in automatic evaluation
methods for summarization, offering a faster, more cost-effective alternative
to human evaluation. However, existing methods often fall short when applied to
complex tasks like long-context summarizations and dialogue-based meeting
summarizations. In this paper, we introduce CREAM (Comparison-Based
Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a
novel framework that addresses the unique challenges of evaluating meeting
summaries. CREAM leverages a combination of chain-of-thought reasoning and key
facts alignment to assess conciseness and completeness of model-generated
summaries without requiring reference. By employing an ELO ranking system, our
approach provides a robust mechanism for comparing the quality of different
models or prompt configurations.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰øÉ‰Ωø‰∫∫ÂÄëÂ∞çÊëòË¶ÅÁöÑËá™ÂãïË©ï‰º∞ÊñπÊ≥ïÁî¢ÁîüËààË∂£ÔºåÊèê‰æõÊØî‰∫∫Â∑•Ë©ï‰º∞Êõ¥Âø´ÈÄü„ÄÅÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÂú®ÊáâÁî®ÊñºË§áÈõú‰ªªÂãôÔºà‰æãÂ¶ÇÈï∑ÊñáËÑàÊëòË¶ÅÂíåÂü∫ÊñºÂ∞çË©±ÁöÑÊúÉË≠∞ÊëòË¶ÅÔºâÊôÇÔºåÂæÄÂæÄÊúÉÊúâÊâÄ‰∏çË∂≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CREAMÔºàÂü∫ÊñºÊØîËºÉÁöÑÁÑ°ÂèÉËÄÉ Elo ÊéíÂêçËá™ÂãïË©ï‰º∞ÊúÉË≠∞ÊëòË¶ÅÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºËß£Ê±∫Ë©ï‰º∞ÊúÉË≠∞ÊëòË¶ÅÁöÑÁç®ÁâπÊåëÊà∞„ÄÇCREAM Êé°Áî®ÊÄùÊÉ≥ÈèàÊé®ÁêÜÂíåÈóúÈçµ‰∫ãÂØ¶Â∞çÈΩäÁöÑÁµÑÂêàÔºå‰æÜË©ï‰º∞Ê®°ÂûãÁîüÊàêÁöÑÊëòË¶ÅÁöÑÁ∞°ÊΩîÊÄßÂíåÂÆåÊï¥ÊÄßÔºåËÄåÁÑ°ÈúÄÂèÉËÄÉ„ÄÇÈÄöÈÅéÊé°Áî® ELO ÊéíÂêçÁ≥ªÁµ±ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ßÁöÑÊ©üÂà∂ÔºåÁî®ÊñºÊØîËºÉ‰∏çÂêåÊ®°ÂûãÊàñÊèêÁ§∫ÈÖçÁΩÆÁöÑÂìÅË≥™„ÄÇ

##### **American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM**
2409.10874v1 by Gregorius Guntur Sunardi Putra, Adifa Widyadhani Chanda D'Layla, Dimas Wahono, Riyanarto Sarno, Agus Tri Haryono

Sign language translation is one of the important issues in communication
between deaf and hearing people, as it expresses words through hand, body, and
mouth movements. American Sign Language is one of the sign languages used, one
of which is the alphabetic sign. The development of neural machine translation
technology is moving towards sign language translation. Transformer became the
state-of-the-art in natural language processing. This study compares the
Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign
language to text. In addition, an experiment was conducted by adding Residual
Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of
ResidualLSTM to the Transformer reduces the performance of the Transformer
model by 23.37% based on the BLEU Score value. In comparison, the Transformer
itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.

ÊëòË¶ÅÔºöÊâãË™ûÁøªË≠ØÊòØËÅæ‰∫∫ÂíåËÅΩ‰∫∫Ê∫ùÈÄö‰∏≠ÁöÑÈáçË¶ÅË≠∞È°å‰πã‰∏ÄÔºåÂõ†ÁÇ∫ÂÆÉÈÄèÈÅéÊâã„ÄÅË∫´È´îÂíåÂò¥Â∑¥ÁöÑÂãï‰Ωú‰æÜË°®ÈÅîÂñÆÂ≠ó„ÄÇÁæéÂúãÊâãË™ûÊòØÂÖ∂‰∏≠‰∏ÄÁ®ÆÊâãË™ûÔºåÂÖ∂‰∏≠‰∏ÄÁ®ÆÊòØÂ≠óÊØçÊâãË™û„ÄÇÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠ØÊäÄË°ìÁöÑÁôºÂ±ïÊ≠£ÊúùÂêëÊâãË™ûÁøªË≠ØÈÇÅÈÄ≤„ÄÇTransformer ÊàêÁÇ∫‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÊúÄÊñ∞ÊäÄË°ì„ÄÇÊú¨Á†îÁ©∂ÊØîËºÉ‰∫Ü Transformer ËàáÂ∫èÂàóÂà∞Â∫èÂàó (Seq2Seq) Ê®°ÂûãÂú®Â∞áÊâãË™ûÁøªË≠ØÊàêÊñáÂ≠óÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÈÇÑÈÄèÈÅéÂú® Transformer ‰∏≠Âä†ÂÖ•ÊÆòÂ∑ÆÈï∑Áü≠ÊúüË®òÊÜ∂ (ResidualLSTM) ÈÄ≤Ë°åÂØ¶È©ó„ÄÇÂú® Transformer ‰∏≠Âä†ÂÖ• ResidualLSTM ÊúÉËÆì Transformer Ê®°ÂûãÁöÑË°®ÁèæÈôç‰Ωé 23.37%ÔºåÊ†πÊìö BLEU Ë©ïÂàÜÂÄº„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåËàá Seq2Seq Ê®°ÂûãÁõ∏ÊØîÔºåTransformer Êú¨Ë∫´ËÆì BLEU Ë©ïÂàÜÂÄºÂ¢ûÂä†‰∫Ü 28.14„ÄÇ

##### **Adaptive Large Language Models By Layerwise Attention Shortcuts**
2409.10870v1 by Prateek Verma, Mert Pilanci

Transformer architectures are the backbone of the modern AI revolution.
However, they are based on simply stacking the same blocks in dozens of layers
and processing information sequentially from one block to another. In this
paper, we propose to challenge this and introduce adaptive computations for
LLM-like setups, which allow the final layer to attend to all of the
intermediate layers as it deems fit through the attention mechanism, thereby
introducing computational \textbf{attention shortcuts}. These shortcuts can
thus make the architecture depth and context adaptive. We showcase four
different datasets, namely acoustic tokens, natural language, and symbolic
music, and we achieve superior performance for GPT-like architecture. We give
evidence via attention maps that the models learn complex dependencies across
layers that are adaptive in context and depth depending on the input tokens.

ÊëòË¶ÅÔºöTransformer Êû∂ÊßãÊòØÁèæ‰ª£ AI Èù©ÂëΩÁöÑÈ™®Âππ„ÄÇ
ÁÑ∂ËÄåÔºåÂÆÉÂÄëÂÉÖÂÉÖÊòØÂ∞áÁõ∏ÂêåÁöÑÂçÄÂ°äÂ†ÜÁñäÂú®Êï∏ÂçÅÂ±§‰∏≠Ôºå
‰∏¶Â∞áË≥áË®äÂæû‰∏ÄÂÄãÂçÄÂ°äÈ†ÜÂ∫èËôïÁêÜÂà∞Âè¶‰∏ÄÂÄãÂçÄÂ°ä„ÄÇÂú®Êú¨Êñá‰∏≠Ôºå
ÊàëÂÄëÊèêË≠∞ÊåëÊà∞ÈÄô‰∏ÄÈªûÔºå‰∏¶ÁÇ∫ LLM È°ûÂûãÁöÑË®≠ÂÆöÂºïÂÖ•ÈÅ©ÊáâÊÄßÈÅãÁÆóÔºå
ÈÄôÂÖÅË®±ÊúÄÂæå‰∏ÄÂ±§ÈÄèÈÅéÊ≥®ÊÑèÊ©üÂà∂ÔºåÂú®ÂÆÉË™çÁÇ∫ÂêàÈÅ©ÊôÇÊ≥®ÊÑèÊâÄÊúâ‰∏≠ÈñìÂ±§Ôºå
ÂæûËÄåÂºïÂÖ•‰∫ÜÈÅãÁÆó„ÄåÊ≥®ÊÑèÊç∑Âæë„Äç„ÄÇÈÄô‰∫õÊç∑ÂæëÂõ†Ê≠§ÂèØ‰ª•‰ΩøÊû∂ÊßãÊ∑±Â∫¶ÂíåÂÖßÂÆπÈÅ©ÊáâÊÄß„ÄÇ
ÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂõõÂÄã‰∏çÂêåÁöÑË≥áÊñôÈõÜÔºåÂç≥Èü≥ÈüøÁ¨¶Ëôü„ÄÅËá™ÁÑ∂Ë™ûË®ÄÂíåÁ¨¶ËôüÈü≥Ê®ÇÔºå
‰∏¶‰∏îÊàëÂÄëÁÇ∫ GPT È°ûÂûãÁöÑÊû∂ÊßãÈÅîÂà∞‰∫ÜÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÈÄèÈÅéÊ≥®ÊÑèÂú∞ÂúñÊèê‰æõË≠âÊìöÔºå
Ë≠âÊòéÊ®°ÂûãÂ≠∏Áøí‰∫ÜË§áÈõúÁöÑ‰æùË≥¥ÊÄßÔºåÈÄô‰∫õ‰æùË≥¥ÊÄßÂú®ÂÖßÂÆπÂíåÊ∑±Â∫¶‰∏äÊúÉÊ†πÊìöËº∏ÂÖ•Á¨¶ËôüËÄåÈÅ©Êáâ„ÄÇ

##### **SIFToM: Robust Spoken Instruction Following through Theory of Mind**
2409.10849v1 by Lance Ying, Jason Xinyu Liu, Shivam Aarya, Yizirui Fang, Stefanie Tellex, Joshua B. Tenenbaum, Tianmin Shu

Spoken language instructions are ubiquitous in agent collaboration. However,
in human-robot collaboration, recognition accuracy for human speech is often
influenced by various speech and environmental factors, such as background
noise, the speaker's accents, and mispronunciation. When faced with noisy or
unfamiliar auditory inputs, humans use context and prior knowledge to
disambiguate the stimulus and take pragmatic actions, a process referred to as
top-down processing in cognitive science. We present a cognitively inspired
model, Speech Instruction Following through Theory of Mind (SIFToM), to enable
robots to pragmatically follow human instructions under diverse speech
conditions by inferring the human's goal and joint plan as prior for speech
perception and understanding. We test SIFToM in simulated home experiments
(VirtualHome 2). Results show that the SIFToM model outperforms
state-of-the-art speech and language models, approaching human-level accuracy
on challenging speech instruction following tasks. We then demonstrate its
ability at the task planning level on a mobile manipulator for breakfast
preparation tasks.

ÊëòË¶ÅÔºöÂú®‰ª£ÁêÜÂçî‰Ωú‰∏≠ÔºåÂè£Ë™ûÊåá‰ª§ÁÑ°ÊâÄ‰∏çÂú®„ÄÇÁÑ∂ËÄåÔºåÂú®‰∫∫Ê©üÂçî‰Ωú‰∏≠Ôºå‰∫∫È°ûË™ûÈü≥ÁöÑËæ®Ë≠òÊ∫ñÁ¢∫Â∫¶ÈÄöÂ∏∏ÊúÉÂèóÂà∞ÂêÑÁ®ÆË™ûÈü≥ÂíåÁí∞Â¢ÉÂõ†Á¥†ÁöÑÂΩ±ÈüøÔºå‰æãÂ¶ÇËÉåÊôØÂô™Èü≥„ÄÅË™™Ë©±ËÄÖÁöÑÂè£Èü≥ÂíåÁôºÈü≥‰∏çÊ≠£Á¢∫„ÄÇÁï∂Èù¢Â∞çÂòàÈõúÊàñ‰∏çÁÜüÊÇâÁöÑËÅΩË¶∫Ëº∏ÂÖ•ÊôÇÔºå‰∫∫È°ûÊúÉ‰ΩøÁî®‰∏ä‰∏ãÊñáÂíåÂÖàÈ©óÁü•Ë≠ò‰æÜÊ∂àÈô§Âà∫ÊøÄÁöÑÊ≠ßÁæ©‰∏¶Êé°ÂèñÂãôÂØ¶ÁöÑË°åÂãïÔºåÈÄôÊòØ‰∏ÄÂÄãÂú®Ë™çÁü•ÁßëÂ≠∏‰∏≠Á®±ÁÇ∫Ëá™‰∏äËÄå‰∏ãËôïÁêÜÁöÑÈÅéÁ®ã„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèóË™çÁü•ÂïüÁôºÁöÑÊ®°ÂûãÔºåÂç≥ÈÄöÈÅéÂøÉÊô∫ÁêÜË´ñÔºàSIFToMÔºâÈÄ≤Ë°åË™ûÈü≥Êåá‰ª§Ôºå‰ª•‰ΩøÊ©üÂô®‰∫∫Âú®‰∏çÂêåÁöÑË™ûÈü≥Ê¢ù‰ª∂‰∏ãÈÄöÈÅéÊé®Êñ∑‰∫∫È°ûÁöÑÁõÆÊ®ôÂíåËÅØÂêàË®àÂäÉ‰æÜÂãôÂØ¶Âú∞ÈÅµÂæ™‰∫∫È°ûÊåá‰ª§Ôºå‰ΩúÁÇ∫Ë™ûÈü≥ÊÑüÁü•ÂíåÁêÜËß£ÁöÑÂÖàÈ©ó„ÄÇÊàëÂÄëÂú®Ê®°Êì¨ÂÆ∂Â∫≠ÂØ¶È©óÔºàVirtualHome 2Ôºâ‰∏≠Ê∏¨Ë©¶‰∫Ü SIFToM„ÄÇÁµêÊûúË°®ÊòéÔºåSIFToM Ê®°ÂûãÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑË™ûÈü≥ÂíåË™ûË®ÄÊ®°ÂûãÔºåÂú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑË™ûÈü≥Êåá‰ª§ÂæåÁ∫å‰ªªÂãô‰∏≠Êé•Ëøë‰∫∫È°ûÁ¥öÂà•ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂÆÉÂú®ÁßªÂãïÊ©üÊ¢∞Êâã‰∏äÂü∑Ë°åÊó©È§êÊ∫ñÂÇô‰ªªÂãôÁöÑ‰ªªÂãôË¶èÂäÉÁ¥öÂà•ÁöÑËÉΩÂäõ„ÄÇ

##### **3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy**
2409.10848v1 by Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Yuki Uranishi

Audio-driven 3D facial animation has made immersive progress both in research
and application developments. The newest approaches focus on Transformer-based
methods and diffusion-based methods, however, there is still gap in the
vividness and emotional expression between the generated animation and real
human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion
policy model for 3D facial animation prediction. This method generates variable
and realistic human facial movements by predicting the 3D vertex trajectory on
the 3D facial template with diffusion policy instead of facial generation for
every frame. It takes audio and vertex states as observations to predict the
vertex trajectory and imitate real human facial expressions, which keeps the
continuous and natural flow of human emotions. The experiments show that our
approach is effective in variable and dynamic facial motion synthesizing.

ÊëòË¶ÅÔºöÈü≥Ë®äÈ©ÖÂãïÁöÑ 3D ËáâÈÉ®ÂãïÁï´Âú®Á†îÁ©∂ÂíåÊáâÁî®ÈñãÁôº‰∏äÈÉΩÊúâÈ°ØËëóÈÄ≤Â±ï„ÄÇÊúÄÊñ∞ÁöÑÊñπÊ≥ïÂ∞àÊ≥®ÊñºÂü∫Êñº Transformer ÁöÑÊñπÊ≥ïÂíåÂü∫ÊñºÊì¥Êï£ÁöÑÊñπÊ≥ïÔºåÁÑ∂ËÄåÔºåÂú®ÁîüÊàêÁöÑÂãïÁï´ÂíåÁúüÂØ¶‰∫∫Ëáâ‰πãÈñìÂú®ÁîüÂãïÊÄßÂíåÊÉÖÁ∑íË°®ÈÅî‰∏ä‰ªçÊúâÂ∑ÆË∑ù„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü 3DFacePolicyÔºå‰∏ÄÁ®ÆÁî®Êñº 3D ËáâÈÉ®ÂãïÁï´È†êÊ∏¨ÁöÑÊì¥Êï£Á≠ñÁï•Ê®°Âûã„ÄÇÊ≠§ÊñπÊ≥ïÈÄöÈÅéÂú® 3D ËáâÈÉ®ÁØÑÊú¨‰∏äÈ†êÊ∏¨ 3D È†ÇÈªûËªåË∑°ÔºåËÄå‰∏çÊòØÁÇ∫ÊØèÂÄãÂπÄÁîüÊàêËáâÈÉ®Ôºå‰æÜÁîüÊàêÂèØËÆä‰∏îÈÄºÁúüÁöÑÁúü‰∫∫ËáâÈÉ®Âãï‰Ωú„ÄÇÂÆÉÂ∞áÈü≥Ë®äÂíåÈ†ÇÈªûÁãÄÊÖã‰ΩúÁÇ∫ËßÄÂØüÂÄºÔºå‰ª•È†êÊ∏¨È†ÇÈªûËªåË∑°‰∏¶Ê®°‰ªøÁúüÂØ¶ÁöÑ‰∫∫ËáâË°®ÊÉÖÔºåÂæûËÄå‰øùÊåÅ‰∫∫È°ûÊÉÖÁ∑íÁöÑÈÄ£Á∫åÊÄßÂíåËá™ÁÑ∂ÊµÅÂãï„ÄÇÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®ÂèØËÆä‰∏îÂãïÊÖãÁöÑËáâÈÉ®Âãï‰ΩúÂêàÊàêÊñπÈù¢ÊòØÊúâÊïàÁöÑ„ÄÇ

##### **BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation**
2409.10847v1 by S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi

Autoregressive models excel in modeling sequential dependencies by enforcing
causal constraints, yet they struggle to capture complex bidirectional patterns
due to their unidirectional nature. In contrast, mask-based models leverage
bidirectional context, enabling richer dependency modeling. However, they often
assume token independence during prediction, which undermines the modeling of
sequential dependencies. Additionally, the corruption of sequences through
masking or absorption can introduce unnatural distortions, complicating the
learning process. To address these issues, we propose Bidirectional
Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of
autoregressive and mask-based generative models. BAD utilizes a
permutation-based corruption technique that preserves the natural sequence
structure while enforcing causal dependencies through randomized ordering,
enabling the effective capture of both sequential and bidirectional
relationships. Comprehensive experiments show that BAD outperforms
autoregressive and mask-based models in text-to-motion generation, suggesting a
novel pre-training strategy for sequence modeling. The codebase for BAD is
available on https://github.com/RohollahHS/BAD.

ÊëòË¶ÅÔºöËá™ÂõûÂΩíÊ®°ÂûãÈÄöËøáÂº∫Âà∂Âõ†ÊûúÁ∫¶ÊùüÂú®Âª∫Ê®°È°∫Â∫è‰æùËµñÂÖ≥Á≥ªÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÂÖ∂ÂçïÂêëÊÄßÔºåÂÆÉ‰ª¨Èöæ‰ª•ÊçïÊçâÂ§çÊùÇÁöÑÂèåÂêëÊ®°Âºè„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂü∫‰∫éÊé©Á†ÅÁöÑÊ®°ÂûãÂà©Áî®ÂèåÂêë‰∏ä‰∏ãÊñáÔºåÊîØÊåÅÊõ¥‰∏∞ÂØåÁöÑ‰æùËµñÂÖ≥Á≥ªÂª∫Ê®°„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®È¢ÑÊµãÊúüÈó¥ÈÄöÂ∏∏ÂÅáËÆæÊ†áËÆ∞Áã¨Á´ãÊÄßÔºåËøô‰ºöÁ†¥ÂùèÈ°∫Â∫è‰æùËµñÂÖ≥Á≥ªÁöÑÂª∫Ê®°„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊé©Á†ÅÊàñÂê∏Êî∂ÂØπÂ∫èÂàóÁöÑÁ†¥ÂùèÂèØËÉΩ‰ºöÂºïÂÖ•‰∏çËá™ÁÑ∂ÁöÑÂ§±ÁúüÔºå‰ΩøÂ≠¶‰π†ËøáÁ®ãÂ§çÊùÇÂåñ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÂêëËá™ÂõûÂΩíÊâ©Êï£ (BAD)ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÂÆÉÁªü‰∏Ä‰∫ÜËá™ÂõûÂΩíÂíåÂü∫‰∫éÊé©Á†ÅÁöÑÁîüÊàêÊ®°ÂûãÁöÑ‰ºòÂäø„ÄÇBAD Âà©Áî®Âü∫‰∫éÁΩÆÊç¢ÁöÑÁ†¥ÂùèÊäÄÊúØÔºåËØ•ÊäÄÊúØ‰øùÁïô‰∫ÜËá™ÁÑ∂Â∫èÂàóÁªìÊûÑÔºåÂêåÊó∂ÈÄöËøáÈöèÊú∫ÊéíÂ∫èÊù•Âº∫Âà∂Âõ†Êûú‰æùËµñÂÖ≥Á≥ªÔºå‰ªéËÄåËÉΩÂ§üÊúâÊïàÊçïÊçâÈ°∫Â∫èÂíåÂèåÂêëÂÖ≥Á≥ª„ÄÇÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåBAD Âú®ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê‰∏≠‰ºò‰∫éËá™ÂõûÂΩíÂíåÂü∫‰∫éÊé©Á†ÅÁöÑÊ®°ÂûãÔºåËøôË°®Êòé‰∫Ü‰∏ÄÁßçÁî®‰∫éÂ∫èÂàóÂª∫Ê®°ÁöÑÊñ∞ÂûãÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇBAD ÁöÑ‰ª£Á†ÅÂ∫ìÂèØÂú® https://github.com/RohollahHS/BAD ‰∏äËé∑Âæó„ÄÇ

##### **PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing**
2409.10831v1 by Phillip Long, Zachary Novack, Taylor Berg-Kirkpatrick, Julian McAuley

The recent explosion of generative AI-Music systems has raised numerous
concerns over data copyright, licensing music from musicians, and the conflict
between open-source AI and large prestige companies. Such issues highlight the
need for publicly available, copyright-free musical data, in which there is a
large shortage, particularly for symbolic music data. To alleviate this issue,
we present PDMX: a large-scale open-source dataset of over 250K public domain
MusicXML scores collected from the score-sharing forum MuseScore, making it the
largest available copyright-free symbolic music dataset to our knowledge. PDMX
additionally includes a wealth of both tag and user interaction metadata,
allowing us to efficiently analyze the dataset and filter for high quality
user-generated scores. Given the additional metadata afforded by our data
collection process, we conduct multitrack music generation experiments
evaluating how different representative subsets of PDMX lead to different
behaviors in downstream models, and how user-rating statistics can be used as
an effective measure of data quality. Examples can be found at
https://pnlong.github.io/PDMX.demo/.

ÊëòË¶ÅÔºöÊúÄËøëÁîüÊàêÂºè AI-Music Á≥ªÁªüÁöÑÁàÜÁÇ∏ÊÄßÂ¢ûÈïøÂºïËµ∑‰∫ÜËÆ∏Â§öÂÖ≥‰∫éÊï∞ÊçÆÁâàÊùÉ„ÄÅ‰ªéÈü≥‰πê‰∫∫Â§ÑËé∑ÂæóÈü≥‰πêËÆ∏ÂèØ‰ª•ÂèäÂºÄÊ∫ê AI ‰∏éÂ§ßÂûãÂ£∞ÊúõÂÖ¨Âè∏‰πãÈó¥ÁöÑÂÜ≤Á™ÅÁöÑÊãÖÂøß„ÄÇÊ≠§Á±ªÈóÆÈ¢òÂá∏Êòæ‰∫ÜÂØπÂÖ¨ÂºÄÂèØÁî®„ÄÅÊó†ÁâàÊùÉÈü≥‰πêÊï∞ÊçÆÁöÑÈúÄÊ±ÇÔºåËÄåÊ≠§Á±ªÊï∞ÊçÆ‰∏•ÈáçÁü≠Áº∫ÔºåÁâπÂà´ÊòØÁ¨¶Âè∑Èü≥‰πêÊï∞ÊçÆ„ÄÇ‰∏∫‰∫ÜÁºìËß£Ê≠§ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü PDMXÔºö‰∏Ä‰∏™‰ªé‰πêË∞±ÂàÜ‰∫´ËÆ∫Âùõ MuseScore Êî∂ÈõÜÁöÑË∂ÖËøá 250K ÂÖ¨ÂÖ±È¢ÜÂüü MusicXML ‰πêË∞±ÁöÑÂ§ßÂûãÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºå‰ΩøÂÖ∂Êàê‰∏∫Êàë‰ª¨ÊâÄÁü•ÊúÄÂ§ßÁöÑÂèØÁî®ÁöÑÊó†ÁâàÊùÉÁ¨¶Âè∑Èü≥‰πêÊï∞ÊçÆÈõÜ„ÄÇÊ≠§Â§ñÔºåPDMX ËøòÂåÖÂê´Â§ßÈáèÊ†áÁ≠æÂíåÁî®Êà∑‰∫§‰∫íÂÖÉÊï∞ÊçÆÔºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÊúâÊïàÂú∞ÂàÜÊûêÊï∞ÊçÆÈõÜÂπ∂Á≠õÈÄâÈ´òË¥®ÈáèÁöÑÁî®Êà∑ÁîüÊàê‰πêË∞±„ÄÇÈâ¥‰∫éÊàë‰ª¨ÁöÑÊï∞ÊçÆÊî∂ÈõÜËøáÁ®ãÊèê‰æõÁöÑÈôÑÂä†ÂÖÉÊï∞ÊçÆÔºåÊàë‰ª¨ËøõË°å‰∫ÜÂ§öËΩ®Èü≥‰πêÁîüÊàêÂÆûÈ™åÔºåËØÑ‰º∞ PDMX ÁöÑ‰∏çÂêå‰ª£Ë°®ÊÄßÂ≠êÈõÜÂ¶Ç‰ΩïÂØºËá¥‰∏ãÊ∏∏Ê®°Âûã‰∏≠ÁöÑ‰∏çÂêåË°å‰∏∫Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜÁî®Êà∑ËØÑÁ∫ßÁªüËÆ°Êï∞ÊçÆÁî®‰ΩúÊï∞ÊçÆË¥®ÈáèÁöÑÊúâÊïàË°°ÈáèÊ†áÂáÜ„ÄÇÂèØ‰ª•Âú® https://pnlong.github.io/PDMX.demo/ ‰∏≠ÊâæÂà∞Á§∫‰æã„ÄÇ

##### **ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic Radiology Reports**
2409.10829v1 by Vishwanatha M. Rao, Serena Zhang, Julian N. Acosta, Subathra Adithan, Pranav Rajpurkar

Accurately interpreting medical images and writing radiology reports is a
critical but challenging task in healthcare. Both human-written and
AI-generated reports can contain errors, ranging from clinical inaccuracies to
linguistic mistakes. To address this, we introduce ReXErr, a methodology that
leverages Large Language Models to generate representative errors within chest
X-ray reports. Working with board-certified radiologists, we developed error
categories that capture common mistakes in both human and AI-generated reports.
Our approach uses a novel sampling scheme to inject diverse errors while
maintaining clinical plausibility. ReXErr demonstrates consistency across error
categories and produces errors that closely mimic those found in real-world
scenarios. This method has the potential to aid in the development and
evaluation of report correction algorithms, potentially enhancing the quality
and reliability of radiology reporting.

ÊëòË¶ÅÔºöÁ≤æÊ∫ñÂú∞Ëß£ËÆÄÈÜ´Â≠∏ÂΩ±ÂÉè‰∏¶Êí∞ÂØ´ÊîæÂ∞ÑÁßëÂ†±ÂëäÊòØÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶Å‰ΩÜÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇ‰∫∫ÂØ´ÁöÑÂ†±ÂëäÂíå AI ÁîüÊàêÁöÑÂ†±ÂëäÈÉΩÂèØËÉΩÂåÖÂê´ÈåØË™§ÔºåÂæûËá®Â∫ä‰∏äÁöÑ‰∏çÊ∫ñÁ¢∫Âà∞Ë™ûË®Ä‰∏äÁöÑÈåØË™§„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ReXErrÔºåÈÄôÊòØ‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ËÉ∏ÈÉ® X ÂÖâÂ†±Âëä‰∏≠Áî¢Áîü‰ª£Ë°®ÊÄßÈåØË™§ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëËàáÈÄöÈÅéË™çË≠âÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´Âêà‰ΩúÔºåÂà∂ÂÆö‰∫ÜÈåØË™§È°ûÂà•Ôºå‰ª•ÊâæÂá∫‰∫∫ÂØ´ÁöÑÂ†±ÂëäÂíå AI ÁîüÊàêÁöÑÂ†±Âëä‰∏≠Â∏∏Ë¶ãÁöÑÈåØË™§„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊäΩÊ®£ÊñπÊ°à‰æÜÊ≥®ÂÖ•ÂêÑÁ®ÆÈåØË™§ÔºåÂêåÊôÇ‰øùÊåÅËá®Â∫ä‰∏äÁöÑÂêàÁêÜÊÄß„ÄÇReXErr Âú®ÈåØË™§È°ûÂà•‰∏≠Â±ïÁèæ‰∫Ü‰∏ÄËá¥ÊÄßÔºå‰∏¶Áî¢Áîü‰∫ÜËàáÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÁôºÁèæÁöÑÈåØË™§ÈùûÂ∏∏Áõ∏‰ººÁöÑÈåØË™§„ÄÇÊ≠§ÊñπÊ≥ïÊúâÂèØËÉΩÂπ´Âä©ÈñãÁôºÂíåË©ï‰º∞Â†±Âëä‰øÆÊ≠£ÊºîÁÆóÊ≥ïÔºåÈÄ≤ËÄåÊèêÂçáÊîæÂ∞ÑÁßëÂ†±ÂëäÁöÑÂìÅË≥™ÂíåÂèØÈù†ÊÄß„ÄÇ

##### **Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations**
2409.10825v1 by Shahnewaz Karim Sakib, Anindya Bijoy Das

Large Language Model (LLM)-based recommendation systems provide more
comprehensive recommendations than traditional systems by deeply analyzing
content and user behavior. However, these systems often exhibit biases,
favoring mainstream content while marginalizing non-traditional options due to
skewed training data. This study investigates the intricate relationship
between bias and LLM-based recommendation systems, with a focus on music, song,
and book recommendations across diverse demographic and cultural groups.
Through a comprehensive analysis conducted over different LLM-models, this
paper evaluates the impact of bias on recommendation outcomes. Our findings
reveal that bias is so deeply ingrained within these systems that even a
simpler intervention like prompt engineering can significantly reduce bias,
underscoring the pervasive nature of the issue. Moreover, factors like
intersecting identities and contextual information, such as socioeconomic
status, further amplify these biases, demonstrating the complexity and depth of
the challenges faced in creating fair recommendations across different groups.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫Âü∫Á§éÁöÑÊé®Ëñ¶Á≥ªÁµ±ÈÄèÈÅéÊ∑±ÂÖ•ÂàÜÊûêÂÖßÂÆπÂíå‰ΩøÁî®ËÄÖË°åÁÇ∫ÔºåÊèê‰æõÊØîÂÇ≥Áµ±Á≥ªÁµ±Êõ¥ÂÖ®Èù¢ÁöÑÊé®Ëñ¶„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ≥ªÁµ±Á∂ìÂ∏∏Â±ïÁèæÂÅèË¶ãÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñôÁöÑÂÅèÂ∑ÆÔºåÂÅèÂ•Ω‰∏ªÊµÅÂÖßÂÆπÔºåÂêåÊôÇÂ∞áÈùûÂÇ≥Áµ±ÈÅ∏È†ÖÈÇäÁ∑£Âåñ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂÅèË¶ãËàá LLM ÁÇ∫Âü∫Á§éÁöÑÊé®Ëñ¶Á≥ªÁµ±‰πãÈñìË§áÈõúÁöÑÈóú‰øÇÔºåÈáçÈªûÂú®Êñº‰∏çÂêå‰∫∫Âè£Áµ±Ë®àÂíåÊñáÂåñÁæ§È´î‰∏≠ÁöÑÈü≥Ê®Ç„ÄÅÊ≠åÊõ≤ÂíåÊõ∏Á±çÊé®Ëñ¶„ÄÇÊú¨Ë´ñÊñáÈÄèÈÅéÈáùÂ∞ç‰∏çÂêå LLM Ê®°ÂûãÈÄ≤Ë°åÁöÑÂÖ®Èù¢ÂàÜÊûêÔºåË©ï‰º∞ÂÅèË¶ãÂ∞çÊé®Ëñ¶ÁµêÊûúÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊè≠Èú≤ÂÅèË¶ãÊ∑±Ê§çÊñºÈÄô‰∫õÁ≥ªÁµ±‰πã‰∏≠ÔºåÂç≥‰ΩøÊòØÊèêÁ§∫Â∑•Á®ãÁ≠âËºÉÁ∞°ÂñÆÁöÑ‰ªãÂÖ•Ôºå‰πüËÉΩÈ°ØËëóÈôç‰ΩéÂÅèË¶ãÔºåÂº∑Ë™ø‰∫ÜÊ≠§ÂïèÈ°åÁöÑÊôÆÈÅçÊÄß„ÄÇÊ≠§Â§ñÔºåÁõ∏‰∫§Ë∫´ÂàÜÂíåËÑàÁµ°Ë≥áË®äÁ≠âÂõ†Á¥†Ôºà‰æãÂ¶ÇÁ§æÊúÉÁ∂ìÊøüÂú∞‰ΩçÔºâÔºåÈÄ≤‰∏ÄÊ≠•Êì¥Â§ß‰∫ÜÈÄô‰∫õÂÅèË¶ãÔºåÈ°ØÁ§∫Âá∫Âú®‰∏çÂêåÁæ§È´î‰πãÈñìÂª∫Á´ãÂÖ¨Âπ≥Êé®Ëñ¶ÊâÄÈù¢Ëá®ÁöÑÊåëÊà∞ÁöÑË§áÈõúÊÄßÂíåÊ∑±Â∫¶„ÄÇ

##### **Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering**
2409.10790v1 by Qingru Zhang, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth, Hao Cheng

Large language models (LLMs) have demonstrated remarkable performance across
various real-world tasks. However, they often struggle to fully comprehend and
effectively utilize their input contexts, resulting in responses that are
unfaithful or hallucinated. This difficulty increases for contexts that are
long or contain distracting information, which can divert LLMs from fully
capturing essential evidence. To address this issue, many works use prompting
to help LLMs utilize contextual information more faithfully. For instance,
iterative prompting highlights key information in two steps that first ask the
LLM to identify important pieces of context and then derive answers
accordingly. However, prompting methods are constrained to highlighting key
information implicitly in token space, which is often insufficient to fully
steer the model's attention. To improve model faithfulness more reliably, we
propose AutoPASTA, a method that automatically identifies key contextual
information and explicitly highlights it by steering an LLM's attention scores.
Like prompting, AutoPASTA is applied at inference time and does not require
changing any model parameters. Our experiments on open-book QA demonstrate that
AutoPASTA effectively enables models to grasp essential contextual information,
leading to substantially improved model faithfulness and performance, e.g., an
average improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly
available at https://github.com/QingruZhang/AutoPASTA .

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïå‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁ∂ìÂ∏∏Èõ£‰ª•ÂÆåÂÖ®ÁêÜËß£‰∏¶ÊúâÊïàÂà©Áî®ÂÖ∂Ëº∏ÂÖ•ÂÖßÂÆπÔºåÂ∞éËá¥ÂõûÊáâ‰∏çÂø†ÂØ¶ÊàñÂá∫ÁèæÂπªË¶∫„ÄÇÂ∞çÊñºÈï∑ÁØáÂÖßÂÆπÊàñÂåÖÂê´ÂàÜÊï£Ê≥®ÊÑèÂäõÁöÑË≥áË®äÁöÑÂÖßÂÆπÔºåÈÄôÁ®ÆÂõ∞Èõ£ÊúÉÂ¢ûÂä†ÔºåÈÄôÂèØËÉΩÊúÉËÆì LLM ÁÑ°Ê≥ïÂÆåÂÖ®Êì∑ÂèñÂøÖË¶ÅÁöÑË≠âÊìö„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåË®±Â§ö‰ΩúÂìÅ‰ΩøÁî®ÊèêÁ§∫‰æÜÂπ´Âä© LLM Êõ¥Âø†ÂØ¶Âú∞Âà©Áî®ÂÖßÂÆπË≥áË®ä„ÄÇ‰æãÂ¶ÇÔºåÂèçË¶ÜÊèêÁ§∫ÊúÉÂú®ÂÖ©ÂÄãÊ≠•È©ü‰∏≠Âº∑Ë™øÈóúÈçµË≥áË®äÔºåÈ¶ñÂÖàË¶ÅÊ±Ç LLM ÊâæÂá∫ÈáçË¶ÅÁöÑÂÖßÂÆπÈÉ®ÂàÜÔºåÁÑ∂ÂæåÊìöÊ≠§Êé®Â∞éÁ≠îÊ°à„ÄÇÁÑ∂ËÄåÔºåÊèêÁ§∫ÊñπÊ≥ïÂÉÖÈôêÊñºÂú®Ê®ôË®òÁ©∫Èñì‰∏≠Èö±Âê´Âú∞Âº∑Ë™øÈóúÈçµË≥áË®äÔºåÈÄôÈÄöÂ∏∏‰∏çË∂≥‰ª•ÂÆåÂÖ®ÂºïÂ∞éÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõ„ÄÇÁÇ∫‰∫ÜÊõ¥ÂèØÈù†Âú∞ÊîπÂñÑÊ®°ÂûãÁöÑÂø†ÂØ¶Â∫¶ÔºåÊàëÂÄëÊèêÂá∫ AutoPASTAÔºåÈÄôÊòØ‰∏ÄÁ®ÆËá™ÂãïÊâæÂá∫ÈóúÈçµÂÖßÂÆπË≥áË®ä‰∏¶ÈÄèÈÅéÂºïÂ∞é LLM ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∏‰æÜÊòéÁ¢∫Âº∑Ë™øÂÆÉÁöÑÊñπÊ≥ï„ÄÇËàáÊèêÁ§∫È°û‰ººÔºåAutoPASTA ÊáâÁî®ÊñºÊé®Ë´ñÊôÇÈñìÔºå‰∏çÈúÄË¶ÅËÆäÊõ¥‰ªª‰ΩïÊ®°ÂûãÂèÉÊï∏„ÄÇÊàëÂÄëÂú®ÈñãÊîæÂºèÂïèÁ≠î‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåAutoPASTA ÊúâÊïàÂú∞ËÆìÊ®°ÂûãÊéåÊè°ÂøÖË¶ÅÁöÑÂÖßÂÆπË≥áË®äÔºåÂ§ßÂπÖÊîπÂñÑÊ®°ÂûãÁöÑÂø†ÂØ¶Â∫¶ÂíåÊïàËÉΩÔºå‰æãÂ¶ÇÔºåLLAMA3-70B-Instruct ÁöÑÂπ≥ÂùáÊîπÂñÑÂπÖÂ∫¶ÁÇ∫ 7.95%„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÊñº https://github.com/QingruZhang/AutoPASTA„ÄÇ

##### **Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM and Attention-Based Approach**
2409.10783v1 by Tracy Cai, Kimmy Chang, Fahad Nabi

It was only until the 20th century when the Chinese language began using
punctuation. In fact, many ancient Chinese texts contain thousands of lines
with no distinct punctuation marks or delimiters in sight. The lack of
punctuation in such texts makes it difficult for humans to identify when there
pauses or breaks between particular phrases and understand the semantic meaning
of the written text (Mogahed, 2012). As a result, unless one was educated in
the ancient time period, many readers of ancient Chinese would have
significantly different interpretations of the texts. We propose an approach to
predict the location (and type) of punctuation in ancient Chinese texts that
extends the work of Oh et al (2017) by leveraging a bidirectional multi-layered
LSTM with a multi-head attention mechanism as inspired by Luong et al.'s (2015)
discussion of attention-based architectures. We find that the use of
multi-layered LSTMs and multi-head attention significantly outperforms RNNs
that don't incorporate such components when evaluating ancient Chinese texts.

ÊëòË¶ÅÔºöÁõ¥Âà∞ 20 ‰∏ñÁ¥ÄÔºå‰∏≠ÊñáÊâçÈñãÂßã‰ΩøÁî®Ê®ôÈªûÁ¨¶Ëôü„ÄÇ‰∫ãÂØ¶‰∏äÔºåË®±Â§öÂè§‰ª£‰∏≠ÊñáÊñáÊú¨ÂåÖÂê´Êï∏ÂçÉË°åÔºåÊ≤íÊúâÊòéÈ°ØÁöÑÊ®ôÈªûÁ¨¶ËôüÊàñÂàÜÈöîÁ¨¶Ëôü„ÄÇÊ≠§È°ûÊñáÊú¨‰∏≠Áº∫‰πèÊ®ôÈªûÁ¨¶ËôüÔºåËÆì‰∫∫ÂÄëÈõ£‰ª•Ë≠òÂà•ÁâπÂÆöÁü≠Ë™û‰πãÈñìÁöÑÂÅúÈ†ìÊàñÈñìÈöîÔºå‰∏¶ÁêÜËß£Êõ∏Èù¢ÊñáÊú¨ÁöÑË™ûÁæ©Âê´Áæ© (Mogahed, 2012)„ÄÇÂõ†Ê≠§ÔºåÈô§ÈùûÂèóÈÅéÂè§‰ª£ÊôÇÊúüÁöÑÊïôËÇ≤ÔºåÂê¶ÂâáË®±Â§öÂè§‰ª£‰∏≠ÊñáËÆÄËÄÖÂ∞çÊñáÊú¨ÁöÑË©ÆÈáãÂ∞áÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ï‰æÜÈ†êÊ∏¨Âè§‰ª£‰∏≠ÊñáÊñáÊú¨‰∏≠Ê®ôÈªûÁ¨¶ËôüÁöÑ‰ΩçÁΩÆ (ÂíåÈ°ûÂûã)ÔºåË©≤ÊñπÊ≥ïÈÄöÈÅéÂà©Áî®ÈõôÂêëÂ§öÂ±§ LSTM ËàáÂ§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂‰æÜÊì¥Â±ï Oh Á≠â‰∫∫ (2017) ÁöÑÂ∑•‰ΩúÔºåÈùàÊÑü‰æÜËá™ Luong Á≠â‰∫∫ (2015) Â∞çÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÊû∂ÊßãÁöÑË®éË´ñ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂú®Ë©ï‰º∞Âè§‰ª£‰∏≠ÊñáÊñáÊú¨ÊôÇÔºå‰ΩøÁî®Â§öÂ±§ LSTM ÂíåÂ§öÈ†≠Ê≥®ÊÑèÂäõÊòéÈ°ØÂÑ™ÊñºÊú™Êï¥ÂêàÊ≠§È°ûÁµÑ‰ª∂ÁöÑ RNN„ÄÇ

##### **Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?**
2409.10775v1 by Kaleb Kassaw, Francesco Luzi, Leslie M. Collins, Jordan M. Malof

Image classification models, including convolutional neural networks (CNNs),
perform well on a variety of classification tasks but struggle under conditions
of partial occlusion, i.e., conditions in which objects are partially covered
from the view of a camera. Methods to improve performance under occlusion,
including data augmentation, part-based clustering, and more inherently robust
architectures, including Vision Transformer (ViT) models, have, to some extent,
been evaluated on their ability to classify objects under partial occlusion.
However, evaluations of these methods have largely relied on images containing
artificial occlusion, which are typically computer-generated and therefore
inexpensive to label. Additionally, methods are rarely compared against each
other, and many methods are compared against early, now outdated, deep learning
models. We contribute the Image Recognition Under Occlusion (IRUO) dataset,
based on the recently developed Occluded Video Instance Segmentation (OVIS)
dataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded
images to test and benchmark leading methods' robustness to partial occlusion
in visual recognition tasks. In addition, we contribute the design and results
of a human study using images from IRUO that evaluates human classification
performance at multiple levels and types of occlusion. We find that modern
CNN-based models show improved recognition accuracy on occluded images compared
to earlier CNN-based models, and ViT-based models are more accurate than
CNN-based models on occluded images, performing only modestly worse than human
accuracy. We also find that certain types of occlusion, including diffuse
occlusion, where relevant objects are seen through "holes" in occluders such as
fences and leaves, can greatly reduce the accuracy of deep recognition models
as compared to humans, especially those with CNN backbones.

ÊëòË¶ÅÔºöÂΩ±ÂÉèÂàÜÈ°ûÊ®°ÂûãÔºåÂåÖÊã¨Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN)ÔºåÂú®ÂêÑÁ®ÆÂàÜÈ°û‰ªªÂãô‰∏≠Ë°®ÁèæËâØÂ•ΩÔºå‰ΩÜÂú®ÈÉ®ÂàÜÈÅÆÊìãÁöÑÊÉÖÊ≥Å‰∏ãË°®Áèæ‰∏ç‰Ω≥Ôºå‰πüÂ∞±ÊòØÁâ©‰ª∂Âú®Áõ∏Ê©üË¶ñËßí‰∏≠Ë¢´ÈÉ®ÂàÜÈÅÆ‰ΩèÁöÑÊÉÖÊ≥Å„ÄÇÊîπÂñÑÈÅÆÊìã‰∏ãÁöÑË°®ÁèæÁöÑÊñπÊ≥ïÔºåÂåÖÊã¨Ë≥áÊñôÊì¥ÂÖÖ„ÄÅÂü∫ÊñºÈÉ®ÂàÜÁöÑÁæ§ÈõÜÔºå‰ª•ÂèäÊú¨Ë≥™‰∏äÊõ¥Á©©ÂÅ•ÁöÑÊû∂ÊßãÔºåÂåÖÊã¨Ë¶ñË¶∫Transformer (ViT) Ê®°ÂûãÔºåÂú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äÂ∑≤Ê†πÊìöÂÖ∂Âú®ÈÉ®ÂàÜÈÅÆÊìã‰∏ãÂàÜÈ°ûÁâ©‰ª∂ÁöÑËÉΩÂäõÈÄ≤Ë°åË©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåÂ∞çÈÄô‰∫õÊñπÊ≥ïÁöÑË©ï‰º∞‰∏ªË¶Å‰æùË≥¥ÂåÖÂê´‰∫∫Â∑•ÈÅÆÊìãÁöÑÂΩ±ÂÉèÔºåÈÄô‰∫õÂΩ±ÂÉèÈÄöÂ∏∏ÊòØÈõªËÖ¶Áî¢ÁîüÁöÑÔºåÂõ†Ê≠§Ê®ôË®òÊàêÊú¨‰ΩéÂªâ„ÄÇÊ≠§Â§ñÔºåÂæàÂ∞ëÂ∞áÊñπÊ≥ïÁõ∏‰∫íÊØîËºÉÔºåËÄå‰∏îË®±Â§öÊñπÊ≥ïÈÉΩÊòØËàáÁèæÂú®Â∑≤ÈÅéÊôÇÁöÑÊó©ÊúüÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇÊàëÂÄëË≤¢Áçª‰∫ÜÈÅÆÊìã‰∏ãÁöÑÂΩ±ÂÉèËæ®Ë≠ò (IRUO) Ë≥áÊñôÈõÜÔºåÂÆÉÊòØÂü∫ÊñºÊúÄËøëÈñãÁôºÁöÑÈÅÆÊìãË¶ñË®äÂØ¶‰æãÂàÜÂâ≤ (OVIS) Ë≥áÊñôÈõÜ (arXiv:2102.01558)„ÄÇIRUO Âà©Áî®ÁúüÂØ¶‰∏ñÁïåÂíå‰∫∫Â∑•ÈÅÆÊìãÁöÑÂΩ±ÂÉè‰æÜÊ∏¨Ë©¶ÂíåË©ïÈáèÈ†òÂÖàÊñπÊ≥ïÂú®Ë¶ñË¶∫Ëæ®Ë≠ò‰ªªÂãô‰∏≠Â∞çÈÉ®ÂàÜÈÅÆÊìãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≤¢Áçª‰∫Ü‰ΩøÁî®‰æÜËá™ IRUO ÁöÑÂΩ±ÂÉèÁöÑ‰∫∫È°ûÁ†îÁ©∂ÁöÑË®≠Ë®àÂíåÁµêÊûúÔºåË©≤Á†îÁ©∂Ë©ï‰º∞‰∫Ü‰∫∫È°ûÂú®Â§öÂÄãÂ±§Á¥öÂíåÈÅÆÊìãÈ°ûÂûã‰∏ãÁöÑÂàÜÈ°ûË°®Áèæ„ÄÇÊàëÂÄëÁôºÁèæÔºåËàáÊó©ÊúüÁöÑÂü∫Êñº CNN ÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåÁèæ‰ª£ÁöÑÂü∫Êñº CNN ÁöÑÊ®°ÂûãÂú®ÈÅÆÊìãÂΩ±ÂÉè‰∏äÂ±ïÁèæÂá∫Êõ¥È´òÁöÑËæ®Ë≠òÊ∫ñÁ¢∫ÁéáÔºåËÄåÂü∫Êñº ViT ÁöÑÊ®°ÂûãÂú®ÈÅÆÊìãÂΩ±ÂÉè‰∏äÁöÑÊ∫ñÁ¢∫ÁéáÈ´òÊñºÂü∫Êñº CNN ÁöÑÊ®°ÂûãÔºåÂÉÖÊØî‰∫∫È°ûÊ∫ñÁ¢∫Áéá‰Ωé‰∏ÄÈªû„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåÊüê‰∫õÈ°ûÂûãÁöÑÈÅÆÊìãÔºåÂåÖÊã¨Êº´Â∞ÑÈÅÆÊìãÔºåÂÖ∂‰∏≠Áõ∏ÈóúÁâ©‰ª∂ÈÄèÈÅéÈÅÆÊìãÁâ©Ôºà‰æãÂ¶ÇÂúçÊ¨ÑÂíåÊ®πËëâÔºâÁöÑ„ÄåÂ≠îÊ¥û„ÄçÂèØË¶ãÔºåËàá‰∫∫È°ûÁõ∏ÊØîÔºåÊúÉÂ§ßÂπÖÈôç‰ΩéÊ∑±Â∫¶Ëæ®Ë≠òÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÁéáÔºåÁâπÂà•ÊòØÈÇ£‰∫õÂÖ∑Êúâ CNN ‰∏ªÂππÁöÑÊ®°Âûã„ÄÇ

##### **Semantics Preserving Emoji Recommendation with Large Language Models**
2409.10760v1 by Zhongyi Qiu, Kangyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo

Emojis have become an integral part of digital communication, enriching text
by conveying emotions, tone, and intent. Existing emoji recommendation methods
are primarily evaluated based on their ability to match the exact emoji a user
chooses in the original text. However, they ignore the essence of users'
behavior on social media in that each text can correspond to multiple
reasonable emojis. To better assess a model's ability to align with such
real-world emoji usage, we propose a new semantics preserving evaluation
framework for emoji recommendation, which measures a model's ability to
recommend emojis that maintain the semantic consistency with the user's text.
To evaluate how well a model preserves semantics, we assess whether the
predicted affective state, demographic profile, and attitudinal stance of the
user remain unchanged. If these attributes are preserved, we consider the
recommended emojis to have maintained the original semantics. The advanced
abilities of Large Language Models (LLMs) in understanding and generating
nuanced, contextually relevant output make them well-suited for handling the
complexities of semantics preserving emoji recommendation. To this end, we
construct a comprehensive benchmark to systematically assess the performance of
six proprietary and open-source LLMs using different prompting techniques on
our task. Our experiments demonstrate that GPT-4o outperforms other LLMs,
achieving a semantics preservation score of 79.23%. Additionally, we conduct
case studies to analyze model biases in downstream classification tasks and
evaluate the diversity of the recommended emojis.

ÊëòË¶ÅÔºöË°®ÊÉÖÁ¨¶Â∑≤ÊàêÁÇ∫Êï∏‰ΩçÊ∫ùÈÄö‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÈÄèÈÅéÂÇ≥ÈÅîÊÉÖÁ∑í„ÄÅË™ûÊ∞£ÂíåÊÑèÂúñÔºåË±êÂØå‰∫ÜÊñáÂ≠ó„ÄÇÁèæÊúâÁöÑË°®ÊÉÖÁ¨¶Êé®Ëñ¶ÊñπÊ≥ï‰∏ªË¶ÅÊ†πÊìöÂÖ∂ÂåπÈÖç‰ΩøÁî®ËÄÖÂú®ÂéüÂßãÊñáÂ≠ó‰∏≠ÈÅ∏ÊìáÁöÑÁ¢∫ÂàáË°®ÊÉÖÁ¨¶ÁöÑËÉΩÂäõ‰æÜË©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂøΩÁï•‰∫Ü‰ΩøÁî®ËÄÖÂú®Á§æÁæ§Â™íÈ´î‰∏äÁöÑË°åÁÇ∫Êú¨Ë≥™ÔºåÂõ†ÁÇ∫ÊØèÂâáÊñáÂ≠óÈÉΩÂèØËÉΩÂ∞çÊáâÂà∞Â§öÂÄãÂêàÁêÜÁöÑË°®ÊÉÖÁ¨¶„ÄÇÁÇ∫‰∫ÜÊõ¥‰Ω≥Ë©ï‰º∞Ê®°ÂûãËàáÊ≠§È°ûÁúüÂØ¶‰∏ñÁïåË°®ÊÉÖÁ¨¶‰ΩøÁî®ÊñπÂºè‰∏ÄËá¥ÁöÑËÉΩÂäõÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑË™ûÊÑè‰øùÁïôË©ï‰º∞Êû∂ÊßãÔºåÁî®ÊñºË°®ÊÉÖÁ¨¶Êé®Ëñ¶ÔºåÁî®‰ª•Ë°°ÈáèÊ®°ÂûãÊé®Ëñ¶Ëàá‰ΩøÁî®ËÄÖÊñáÂ≠óÁ∂≠ÊåÅË™ûÊÑè‰∏ÄËá¥ÊÄßÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜË©ï‰º∞Ê®°Âûã‰øùÁïôË™ûÊÑèÁöÑÁ®ãÂ∫¶ÔºåÊàëÂÄëË©ï‰º∞‰ΩøÁî®ËÄÖÈ†êÊ∏¨ÁöÑÊÉÖÊÑüÁãÄÊÖã„ÄÅ‰∫∫Âè£Áµ±Ë®àË≥áÊñôÂíåÊÖãÂ∫¶Á´ãÂ†¥ÊòØÂê¶‰øùÊåÅ‰∏çËÆä„ÄÇÂ¶ÇÊûúÈÄô‰∫õÂ±¨ÊÄßÂæó‰ª•‰øùÁïôÔºåÊàëÂÄëË™çÁÇ∫Êé®Ëñ¶ÁöÑË°®ÊÉÖÁ¨¶Â∑≤Á∂≠ÊåÅÂéüÂßãË™ûÊÑè„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÁêÜËß£ÂíåÁî¢ÁîüÁ¥∞ÂæÆÂ∑ÆÂà•‰∏îËàáËÑàÁµ°Áõ∏ÈóúÁöÑËº∏Âá∫ÊñπÈù¢ÂÖ∑ÊúâÈÄ≤ÈöéËÉΩÂäõÔºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©ÂêàËôïÁêÜË™ûÊÑè‰øùÁïôË°®ÊÉÖÁ¨¶Êé®Ëñ¶ÁöÑË§áÈõúÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂª∫Êßã‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÔºå‰ª•Á≥ªÁµ±ÊÄßË©ï‰º∞ÂÖ≠ÂÄãÂ∞àÊúâÂíåÈñãÊ∫ê LLM Âú®ÊàëÂÄë‰ªªÂãô‰∏≠‰ΩøÁî®‰∏çÂêåÊèêÁ§∫ÊäÄË°ìÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåGPT-4o ÂÑ™ÊñºÂÖ∂‰ªñ LLMÔºåÈÅîÂà∞ 79.23% ÁöÑË™ûÊÑè‰øùÁïôÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÂÄãÊ°àÁ†îÁ©∂ÔºåÂàÜÊûê‰∏ãÊ∏∏ÂàÜÈ°û‰ªªÂãô‰∏≠ÁöÑÊ®°ÂûãÂÅèË™§Ôºå‰∏¶Ë©ï‰º∞Êé®Ëñ¶Ë°®ÊÉÖÁ¨¶ÁöÑÂ§öÊ®£ÊÄß„ÄÇ

##### **VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching**
2409.10756v1 by Arastoo Zibaeirad, Marco Vieira

Large Language Models (LLMs) have shown promise in tasks like code
translation, prompting interest in their potential for automating software
vulnerability detection (SVD) and patching (SVP). To further research in this
area, establishing a benchmark is essential for evaluating the strengths and
limitations of LLMs in these tasks. Despite their capabilities, questions
remain regarding whether LLMs can accurately analyze complex vulnerabilities
and generate appropriate patches. This paper introduces VulnLLMEval, a
framework designed to assess the performance of LLMs in identifying and
patching vulnerabilities in C code. Our study includes 307 real-world
vulnerabilities extracted from the Linux kernel, creating a well-curated
dataset that includes both vulnerable and patched code. This dataset, based on
real-world code, provides a diverse and representative testbed for evaluating
LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous
assessment. Our results reveal that LLMs often struggle with distinguishing
between vulnerable and patched code. Furthermore, in SVP tasks, these models
tend to oversimplify the code, producing solutions that may not be directly
usable without further refinement.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Á®ãÂºèÁ¢ºÁøªË≠ØÁ≠â‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂâçÊôØÔºåÂºïÁôº‰∫∫ÂÄëÂ∞çÂÖ∂Ëá™ÂãïÂåñËªüÈ´îÊºèÊ¥ûÂÅµÊ∏¨ (SVD) Âíå‰øÆË£ú (SVP) ÊΩõÂäõÁöÑËààË∂£„ÄÇÁÇ∫ÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂Ê≠§È†òÂüüÔºåÂª∫Á´ãÂü∫Ê∫ñÂ∞çÊñºË©ï‰º∞ LLM Âú®ÈÄô‰∫õ‰ªªÂãô‰∏≠ÁöÑÂÑ™Áº∫ÈªûËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°ÂÆÉÂÄëÊúâËÉΩÂäõÔºå‰ΩÜ‰ªçÊúâÁñëÂïèÔºå‰æãÂ¶Ç LLM ÊòØÂê¶ËÉΩÊ∫ñÁ¢∫ÂàÜÊûêË§áÈõúÁöÑÊºèÊ¥û‰∏¶Áî¢ÁîüÈÅ©Áï∂ÁöÑ‰øÆË£úÁ®ãÂºè„ÄÇÊú¨Êñá‰ªãÁ¥π VulnLLMEvalÔºå‰∏ÄÂÄãÊó®Âú®Ë©ï‰º∞ LLM Âú®Ë≠òÂà•Âíå‰øÆË£ú C Á®ãÂºèÁ¢ºÊºèÊ¥ûÊñπÈù¢ÁöÑÊïàËÉΩÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÂåÖÊã¨Âæû Linux Ê†∏ÂøÉÊèêÂèñÁöÑ 307 ÂÄãÁúüÂØ¶ÊºèÊ¥ûÔºåÂª∫Á´ã‰∏ÄÂÄãÁ∂ìÈÅéÁ≤æÂøÉÁ≠ñÂäÉÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊúâÊºèÊ¥ûÁöÑÁ®ãÂºèÁ¢ºÂíåÂ∑≤‰øÆË£úÁöÑÁ®ãÂºèÁ¢º„ÄÇÈÄôÂÄãÂü∫ÊñºÁúüÂØ¶Á®ãÂºèÁ¢ºÁöÑË≥áÊñôÈõÜÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂ§öÊ®£Âåñ‰∏îÂÖ∑‰ª£Ë°®ÊÄßÁöÑÊ∏¨Ë©¶Âπ≥Âè∞ÔºåÁî®ÊñºË©ï‰º∞ LLM Âú® SVD Âíå SVP ‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºåÁÇ∫Âö¥Ë¨πÁöÑË©ï‰º∞Êèê‰æõÁ©©Âõ∫ÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåLLM Á∂ìÂ∏∏Èõ£‰ª•ÂçÄÂàÜÊúâÊºèÊ¥ûÁöÑÁ®ãÂºèÁ¢ºÂíåÂ∑≤‰øÆË£úÁöÑÁ®ãÂºèÁ¢º„ÄÇÊ≠§Â§ñÔºåÂú® SVP ‰ªªÂãô‰∏≠ÔºåÈÄô‰∫õÊ®°ÂûãÂÇæÂêëÊñºÈÅéÂ∫¶Á∞°ÂåñÁ®ãÂºèÁ¢ºÔºåÁî¢ÁîüÂèØËÉΩÁÑ°Ê≥ïÁõ¥Êé•‰ΩøÁî®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåËÄåÁÑ°ÈúÄÈÄ≤‰∏ÄÊ≠•‰øÆÊîπ„ÄÇ


### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375v1](http://arxiv.org/abs/2409.11375v1)|null|
|**2024-09-17**|**Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**|Lauren M. Zuromski et.al.|[2409.11350v1](http://arxiv.org/abs/2409.11350v1)|null|
|**2024-09-17**|**TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**|Rong Zhou et.al.|[2409.11299v1](http://arxiv.org/abs/2409.11299v1)|null|
|**2024-09-17**|**EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**|Zeyi Liao et.al.|[2409.11295v1](http://arxiv.org/abs/2409.11295v1)|null|
|**2024-09-17**|**Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**|Eunhae Lee et.al.|[2409.11192v1](http://arxiv.org/abs/2409.11192v1)|null|
|**2024-09-17**|**Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**|Mehroush Banday et.al.|[2409.10932v1](http://arxiv.org/abs/2409.10932v1)|null|
|**2024-09-16**|**Self-supervised Speech Models for Word-Level Stuttered Speech Detection**|Yi-Jen Shih et.al.|[2409.10704v1](http://arxiv.org/abs/2409.10704v1)|null|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**Robust image representations with counterfactual contrastive learning**|M√©lanie Roschewitz et.al.|[2409.10365v1](http://arxiv.org/abs/2409.10365v1)|[link](https://github.com/biomedia-mira/counterfactual-contrastive)|
|**2024-09-16**|**Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation between the United States and South Africa**|Hayoung Jung et.al.|[2409.10168v1](http://arxiv.org/abs/2409.10168v1)|null|
|**2024-09-16**|**DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion**|Yuchen Guo et.al.|[2409.10080v1](http://arxiv.org/abs/2409.10080v1)|null|
|**2024-09-16**|**MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**|Sijie Ji et.al.|[2409.10064v1](http://arxiv.org/abs/2409.10064v1)|null|
|**2024-09-16**|**HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**|Sumera Anjum et.al.|[2409.10011v1](http://arxiv.org/abs/2409.10011v1)|null|
|**2024-09-16**|**Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System**|Raffi Hagopian et.al.|[2409.09968v1](http://arxiv.org/abs/2409.09968v1)|null|
|**2024-09-15**|**GP-GPT: Large Language Model for Gene-Phenotype Mapping**|Yanjun Lyu et.al.|[2409.09825v1](http://arxiv.org/abs/2409.09825v1)|null|
|**2024-09-15**|**Veridical Data Science for Medical Foundation Models**|Ahmed Alaa et.al.|[2409.10580v1](http://arxiv.org/abs/2409.10580v1)|null|
|**2024-09-15**|**From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**|Ming Li et.al.|[2409.09727v1](http://arxiv.org/abs/2409.09727v1)|null|
|**2024-09-15**|**ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**|Inhwa Song et.al.|[2409.09662v2](http://arxiv.org/abs/2409.09662v2)|null|
|**2024-09-15**|**MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences**|Subigya Nepal et.al.|[2409.09570v1](http://arxiv.org/abs/2409.09570v1)|null|
|**2024-09-14**|**COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare**|Chia-Hao Li et.al.|[2409.09549v1](http://arxiv.org/abs/2409.09549v1)|null|
|**2024-09-14**|**Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment**|Xin Hu et.al.|[2409.09520v1](http://arxiv.org/abs/2409.09520v1)|null|
|**2024-09-14**|**Synthetic4Health: Generating Annotated Synthetic Clinical Letters**|Libo Ren et.al.|[2409.09501v1](http://arxiv.org/abs/2409.09501v1)|null|
|**2024-09-14**|**From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging**|Maximilian Rokuss et.al.|[2409.09478v1](http://arxiv.org/abs/2409.09478v1)|null|
|**2024-09-14**|**Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking**|Cong Kong et.al.|[2409.10570v1](http://arxiv.org/abs/2409.10570v1)|null|
|**2024-09-14**|**Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation**|Hui Yi Leong et.al.|[2409.09324v1](http://arxiv.org/abs/2409.09324v1)|null|
|**2024-09-14**|**On the limits of agency in agent-based models**|Ayush Chopra et.al.|[2409.10568v1](http://arxiv.org/abs/2409.10568v1)|[link](https://github.com/agenttorch/agenttorch)|
|**2024-09-13**|**Contextual Evaluation of Large Language Models for Classifying Tropical and Infectious Diseases**|Mercy Asiedu et.al.|[2409.09201v1](http://arxiv.org/abs/2409.09201v1)|null|
|**2024-09-13**|**Phikon-v2, A large and public feature extractor for biomarker prediction**|Alexandre Filiot et.al.|[2409.09173v1](http://arxiv.org/abs/2409.09173v1)|null|
|**2024-09-13**|**Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation**|Cheng Charles Ma et.al.|[2409.09135v1](http://arxiv.org/abs/2409.09135v1)|null|
|**2024-09-13**|**MAISI: Medical AI for Synthetic Imaging**|Pengfei Guo et.al.|[2409.11169v1](http://arxiv.org/abs/2409.11169v1)|null|
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936v1](http://arxiv.org/abs/2409.08936v1)|[link](https://github.com/prabaey/synsum)|
|**2024-09-13**|**Recent Trends in Modelling the Continuous Time Series using Deep Learning: A Survey**|Mansura Habiba et.al.|[2409.09106v1](http://arxiv.org/abs/2409.09106v1)|null|
|**2024-09-13**|**A BERT-Based Summarization approach for depression detection**|Hossein Salahshoor Gavalan et.al.|[2409.08483v1](http://arxiv.org/abs/2409.08483v1)|null|
|**2024-09-12**|**Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**|Fuchen Zheng et.al.|[2409.07793v1](http://arxiv.org/abs/2409.07793v1)|[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**|Sheng Chen et.al.|[2409.07186v2](http://arxiv.org/abs/2409.07186v2)|null|
|**2024-09-11**|**CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**|Feiyang Jia et.al.|[2409.07092v1](http://arxiv.org/abs/2409.07092v1)|null|
|**2024-09-11**|**Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**|Daeun Kyung et.al.|[2409.07012v1](http://arxiv.org/abs/2409.07012v1)|null|
|**2024-09-11**|**Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**|Jianmei Jiang et.al.|[2409.06928v1](http://arxiv.org/abs/2409.06928v1)|[link](https://github.com/jjm1589/dstct)|
|**2024-09-10**|**Bifurcation Identification for Ultrasound-driven Robotic Cannulation**|Cecilia G. Morales et.al.|[2409.06817v1](http://arxiv.org/abs/2409.06817v1)|null|
|**2024-09-10**|**Personalized Federated Learning Techniques: Empirical Analysis**|Azal Ahmad Khan et.al.|[2409.06805v1](http://arxiv.org/abs/2409.06805v1)|null|
|**2024-09-10**|**Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**|Cristian Trout et.al.|[2409.06672v1](http://arxiv.org/abs/2409.06672v1)|null|
|**2024-09-10**|**EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**|Danli Shi et.al.|[2409.06644v2](http://arxiv.org/abs/2409.06644v2)|null|
|**2024-09-10**|**Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**|Zoe Hancox et.al.|[2409.06585v1](http://arxiv.org/abs/2409.06585v1)|[link](https://github.com/zoehancox/sparse_tgcnn)|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351v1](http://arxiv.org/abs/2409.06351v1)|null|
|**2024-09-10**|**Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**|Xin Zhang et.al.|[2409.06209v1](http://arxiv.org/abs/2409.06209v1)|[link](https://github.com/xinz0419/unisurv)|
|**2024-09-10**|**Can Large Language Models Unlock Novel Scientific Research Ideas?**|Sandeep Kumar et.al.|[2409.06185v1](http://arxiv.org/abs/2409.06185v1)|null|
|**2024-09-10**|**Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**|Georgios Chochlakis et.al.|[2409.06173v2](http://arxiv.org/abs/2409.06173v2)|[link](https://github.com/gchochla/cot-priors)|
|**2024-09-10**|**Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**|Dong Han et.al.|[2409.06147v1](http://arxiv.org/abs/2409.06147v1)|null|
|**2024-09-09**|**ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**|Zhaoyue Sun et.al.|[2409.05592v1](http://arxiv.org/abs/2409.05592v1)|null|
|**2024-09-09**|**Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**|Camilo Thorne et.al.|[2409.05486v2](http://arxiv.org/abs/2409.05486v2)|null|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-09**|**Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**|Javad Hassannataj Joloudari et.al.|[2409.07493v1](http://arxiv.org/abs/2409.07493v1)|null|
|**2024-09-09**|**Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**|Nirmalya Thakur et.al.|[2409.05292v2](http://arxiv.org/abs/2409.05292v2)|null|
|**2024-09-09**|**RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**|Quoc-Bao Nguyen-Le et.al.|[2409.05280v1](http://arxiv.org/abs/2409.05280v1)|[link](https://github.com/kyle-paul/RotCAtt-TransUNet-plusplus)|
|**2024-09-07**|**Activation Function Optimization Scheme for Image Classification**|Abdur Rahman et.al.|[2409.04915v1](http://arxiv.org/abs/2409.04915v1)|[link](https://github.com/abdurrahman1828/afos)|
|**2024-09-07**|**LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**|Yongxin Deng et.al.|[2409.04744v1](http://arxiv.org/abs/2409.04744v1)|null|
|**2024-09-07**|**NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**|Debaditya Shome et.al.|[2409.04723v1](http://arxiv.org/abs/2409.04723v1)|null|
|**2024-09-07**|**A Comprehensive Survey on Evidential Deep Learning and Its Applications**|Junyu Gao et.al.|[2409.04720v1](http://arxiv.org/abs/2409.04720v1)|[link](https://github.com/mengyuanchen21/awesome-evidential-deep-learning)|
|**2024-09-07**|**A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**|Cheng Wan et.al.|[2409.04704v1](http://arxiv.org/abs/2409.04704v1)|null|
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**|Yizhen Zheng et.al.|[2409.04481v1](http://arxiv.org/abs/2409.04481v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-P√©rez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v2](http://arxiv.org/abs/2409.02681v2)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**EEG-Language Modeling for Pathology Detection**|Sam Gijsen et.al.|[2409.07480v1](http://arxiv.org/abs/2409.07480v1)|null|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|[link](https://github.com/scslabisu/aggym)|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|

#### Abstracts
##### **Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**
2409.11375v1 by Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi

In the medical domain, acquiring large datasets poses significant challenges
due to privacy concerns. Nonetheless, the development of a robust deep-learning
model for retinal disease diagnosis necessitates a substantial dataset for
training. The capacity to generalize effectively on smaller datasets remains a
persistent challenge. The scarcity of data presents a significant barrier to
the practical implementation of scalable medical AI solutions. To address this
issue, we've combined a wide range of data sources to improve performance and
generalization to new data by giving it a deeper understanding of the data
representation from multi-modal datasets and developed a self-supervised
framework based on large language models (LLMs), SwinV2 to gain a deeper
understanding of multi-modal dataset representations, enhancing the model's
ability to extrapolate to new data for the detection of eye diseases using
optical coherence tomography (OCT) images. We adopt a two-phase training
methodology, self-supervised pre-training, and fine-tuning on a downstream
supervised classifier. An ablation study conducted across three datasets
employing various encoder backbones, without data fusion, with low data
availability setting, and without self-supervised pre-training scenarios,
highlights the robustness of our method. Our findings demonstrate consistent
performance across these diverse conditions, showcasing superior generalization
capabilities compared to the baseline model, ResNet-50.

ÊëòË¶ÅÔºöÂú®ÈÜ´ÁôÇÈ†òÂüüÔºåÁî±ÊñºÈö±ÁßÅÂïèÈ°åÔºåÁç≤ÂèñÂ§ßÂûãË≥áÊñôÈõÜÊúÉÈÄ†ÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂ∞çÊñºË¶ñÁ∂≤ËÜúÁñæÁóÖË®∫Êñ∑ÁöÑÂº∑ÂÅ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÈñãÁôºÈúÄË¶Å‰∏ÄÂÄãÈæêÂ§ßÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÂ∞çËºÉÂ∞èÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÊúâÊïàÊ¶ÇÊã¨ÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊåÅÁ∫åÁöÑÊåëÊà∞„ÄÇË≥áÊñôÁöÑÁ®ÄÁº∫ÊÄßÂ∞çÂèØÊì¥ÂÖÖÈÜ´ÁôÇ AI Ëß£Ê±∫ÊñπÊ°àÁöÑÂØ¶ÈöõÂØ¶ÊñΩÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÁµêÂêà‰∫ÜÂêÑÁ®ÆË≥áÊñô‰æÜÊ∫êÔºåÈÄöÈÅéËÆìÂÖ∂Êõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£Â§öÊ®°ÂºèË≥áÊñôÈõÜÁöÑË≥áÊñôË°®Á§∫Ôºå‰æÜÊîπÂñÑÊïàËÉΩÂíåÂ∞çÊñ∞Ë≥áÊñôÁöÑÊ¶ÇÊã¨ÊÄßÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËá™Áõ£Áù£Ê°ÜÊû∂ÔºåSwinV2Ôºå‰ª•Êõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£Â§öÊ®°ÂºèË≥áÊñôÈõÜË°®Á§∫ÔºåÂ¢ûÂº∑Ê®°ÂûãÊé®Êñ∑Êñ∞Ë≥áÊñôÁöÑËÉΩÂäõÔºå‰ª•‰ΩøÁî®ÂÖâÂ≠∏Áõ∏Âπ≤Êñ∑Â±§ÊéÉÊèè (OCT) ÂΩ±ÂÉèÂÅµÊ∏¨ÁúºÁñæ„ÄÇÊàëÂÄëÊé°Áî®ÂÖ©ÈöéÊÆµË®ìÁ∑¥ÊñπÊ≥ïÔºåËá™Áõ£Áù£È†êË®ìÁ∑¥ÂíåÂ∞ç‰∏ãÊ∏∏Áõ£Áù£ÂàÜÈ°ûÂô®ÈÄ≤Ë°åÂæÆË™ø„ÄÇÂú®‰∏âÂÄãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåÊé°Áî®ÂêÑÁ®ÆÁ∑®Á¢º‰∏ªÂππÔºåÊ≤íÊúâË≥áÊñôËûçÂêàÔºåÂú®Ë≥áÊñôÂèØÁî®ÊÄßË®≠ÂÆöËºÉ‰ΩéÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰ª•ÂèäÊ≤íÊúâËá™Áõ£Áù£È†êË®ìÁ∑¥Â†¥ÊôØÔºåÁ™ÅÂá∫‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË≠âÊòé‰∫ÜÂú®ÈÄô‰∫õ‰∏çÂêåÊ¢ù‰ª∂‰∏ãÁöÑ‰∏ÄËá¥ÊïàËÉΩÔºåËàáÂü∫Ê∫ñÊ®°Âûã ResNet-50 Áõ∏ÊØîÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇ

##### **Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**
2409.11350v1 by Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng

Machine-learning (ML) models in flow cytometry have the potential to reduce
error rates, increase reproducibility, and boost the efficiency of clinical
labs. While numerous ML models for flow cytometry data have been proposed, few
studies have described the clinical deployment of such models. Realizing the
potential gains of ML models in clinical labs requires not only an accurate
model, but infrastructure for automated inference, error detection, analytics
and monitoring, and structured data extraction. Here, we describe an ML model
for detection of Acute Myeloid Leukemia (AML), along with the infrastructure
supporting clinical implementation. Our infrastructure leverages the resilience
and scalability of the cloud for model inference, a Kubernetes-based workflow
system that provides model reproducibility and resource management, and a
system for extracting structured diagnoses from full-text reports. We also
describe our model monitoring and visualization platform, an essential element
for ensuring continued model accuracy. Finally, we present a post-deployment
analysis of impacts on turn-around time and compare production accuracy to the
original validation statistics.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÂú®ÊµÅÂºèÁ¥∞ËÉûË°ì‰∏≠ÂÖ∑ÊúâÈôç‰ΩéÈåØË™§Áéá„ÄÅÊèêÈ´òÂèØÈáçÁèæÊÄßÂíåÊèêÂçáËá®Â∫äÂØ¶È©óÂÆ§ÊïàÁéáÁöÑÊΩõÂäõ„ÄÇÈõñÁÑ∂Â∑≤Á∂ìÊèêÂá∫Ë®±Â§öÁî®ÊñºÊµÅÂºèÁ¥∞ËÉûË°ìÊï∏ÊìöÁöÑ ML Ê®°ÂûãÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂ÊèèËø∞Ê≠§È°ûÊ®°ÂûãÁöÑËá®Â∫äÈÉ®ÁΩ≤„ÄÇË¶ÅÂØ¶Áèæ ML Ê®°ÂûãÂú®Ëá®Â∫äÂØ¶È©óÂÆ§‰∏≠ÁöÑÊΩõÂú®Êî∂ÁõäÔºå‰∏çÂÉÖÈúÄË¶ÅÊ∫ñÁ¢∫ÁöÑÊ®°ÂûãÔºåÈÇÑÈúÄË¶ÅÁî®ÊñºËá™ÂãïÊé®ÁêÜ„ÄÅÈåØË™§Ê™¢Ê∏¨„ÄÅÂàÜÊûêÂíåÁõ£Êéß‰ª•ÂèäÁµêÊßãÂåñÊï∏ÊìöÊèêÂèñÁöÑÂü∫Á§éË®≠ÊñΩ„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÊèèËø∞‰∫Ü‰∏ÄÂÄãÁî®ÊñºÊ™¢Ê∏¨ÊÄ•ÊÄßÈ´ìÊÄßÁôΩË°ÄÁóÖ (AML) ÁöÑ ML Ê®°ÂûãÔºå‰ª•ÂèäÊîØÊåÅËá®Â∫äÂØ¶ÊñΩÁöÑÂü∫Á§éË®≠ÊñΩ„ÄÇÊàëÂÄëÁöÑÂü∫Á§éË®≠ÊñΩÂà©Áî®Èõ≤Á´ØÁöÑÂæ©ÂéüÂäõÂíåÂèØÊì¥ÂÖÖÊÄßÈÄ≤Ë°åÊ®°ÂûãÊé®ÁêÜÔºå‰∏ÄÂÄãÂü∫Êñº Kubernetes ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÁ≥ªÁµ±Êèê‰æõÊ®°ÂûãÂèØÈáçÁèæÊÄßÂíåË≥áÊ∫êÁÆ°ÁêÜÔºå‰ª•Âèä‰∏ÄÂÄãÂæûÂÖ®ÊñáÂ†±Âëä‰∏≠ÊèêÂèñÁµêÊßãÂåñË®∫Êñ∑ÁöÑÁ≥ªÁµ±„ÄÇÊàëÂÄëÈÇÑÊèèËø∞‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÁõ£ÊéßÂíåË¶ñË¶∫ÂåñÂπ≥Âè∞ÔºåÈÄôÊòØÁ¢∫‰øùÊåÅÁ∫åÊ®°ÂûãÊ∫ñÁ¢∫ÊÄßÁöÑÂü∫Êú¨Ë¶ÅÁ¥†„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ∞çÂë®ËΩâÊôÇÈñìÂΩ±ÈüøÁöÑÈÉ®ÁΩ≤ÂæåÂàÜÊûêÔºå‰∏¶Â∞áÁîüÁî¢Ê∫ñÁ¢∫Â∫¶ËàáÂéüÂßãÈ©óË≠âÁµ±Ë®àÊï∏ÊìöÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**
2409.11299v1 by Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun

Biomedical image segmentation is crucial for accurately diagnosing and
analyzing various diseases. However, Convolutional Neural Networks (CNNs) and
Transformers, the most commonly used architectures for this task, struggle to
effectively capture long-range dependencies due to the inherent locality of
CNNs and the computational complexity of Transformers. To address this
limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time
Training (TTT) layers into the traditional U-Net architecture for biomedical
image segmentation. TTT-Unet dynamically adjusts model parameters during the
testing time, enhancing the model's ability to capture both local and
long-range features. We evaluate TTT-Unet on multiple medical imaging datasets,
including 3D abdominal organ segmentation in CT and MR images, instrument
segmentation in endoscopy images, and cell segmentation in microscopy images.
The results demonstrate that TTT-Unet consistently outperforms state-of-the-art
CNN-based and Transformer-based segmentation models across all tasks. The code
is available at https://github.com/rongzhou7/TTT-Unet.

ÊëòË¶ÅÔºöÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Â∞çÊñºÊ∫ñÁ¢∫Ë®∫Êñ∑ÂíåÂàÜÊûêÂêÑÁ®ÆÁñæÁóÖËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàCNNÔºâÂíå TransformerÔºå‰ΩúÁÇ∫Ê≠§‰ªªÂãôÊúÄÂ∏∏Áî®ÁöÑÊû∂ÊßãÔºåÁî±Êñº CNN ÁöÑÂõ∫ÊúâÂ±ÄÈÉ®ÊÄßÂíå Transformer ÁöÑË®àÁÆóË§áÈõúÊÄßÔºåÈõ£‰ª•ÊúâÊïàÊì∑ÂèñÈï∑Á®ã‰æùË≥¥ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü TTT-UnetÔºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂ∞áÊ∏¨Ë©¶ÊôÇÈñìË®ìÁ∑¥ÔºàTTTÔºâÂ±§Êï¥ÂêàÂà∞ÂÇ≥Áµ±ÁöÑ U-Net Êû∂Êßã‰∏≠ÔºåÁî®ÊñºÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇTTT-Unet Âú®Ê∏¨Ë©¶ÊôÇÈñìÂãïÊÖãË™øÊï¥Ê®°ÂûãÂèÉÊï∏ÔºåÂ¢ûÂº∑Ê®°ÂûãÊì∑ÂèñÂ±ÄÈÉ®ÂíåÈï∑Á®ãÁâπÂæµÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®Â§öÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äË©ï‰º∞ TTT-UnetÔºåÂåÖÊã¨ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂíåÁ£ÅÊåØÈÄ†ÂΩ±‰∏≠ÁöÑ 3D ËÖπËÖîÂô®ÂÆòÂàÜÂâ≤„ÄÅÂÖßË¶ñÈè°ÂΩ±ÂÉè‰∏≠ÁöÑÂÑÄÂô®ÂàÜÂâ≤‰ª•ÂèäÈ°ØÂæÆÈè°ÂΩ±ÂÉè‰∏≠ÁöÑÁ¥∞ËÉûÂàÜÂâ≤„ÄÇÁµêÊûúË°®ÊòéÔºåTTT-Unet Âú®ÊâÄÊúâ‰ªªÂãô‰∏≠ÈÉΩÊåÅÁ∫åÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫Êñº CNN ÂíåÂü∫Êñº Transformer ÁöÑÂàÜÂâ≤Ê®°Âûã„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/rongzhou7/TTT-Unet ÂèñÂæó„ÄÇ

##### **EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**
2409.11295v1 by Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun

Generalist web agents have evolved rapidly and demonstrated remarkable
potential. However, there are unprecedented safety risks associated with these
them, which are nearly unexplored so far. In this work, we aim to narrow this
gap by conducting the first study on the privacy risks of generalist web agents
in adversarial environments. First, we present a threat model that discusses
the adversarial targets, constraints, and attack scenarios. Particularly, we
consider two types of adversarial targets: stealing users' specific personally
identifiable information (PII) or stealing the entire user request. To achieve
these objectives, we propose a novel attack method, termed Environmental
Injection Attack (EIA). This attack injects malicious content designed to adapt
well to different environments where the agents operate, causing them to
perform unintended actions. This work instantiates EIA specifically for the
privacy scenario. It inserts malicious web elements alongside persuasive
instructions that mislead web agents into leaking private information, and can
further leverage CSS and JavaScript features to remain stealthy. We collect 177
actions steps that involve diverse PII categories on realistic websites from
the Mind2Web dataset, and conduct extensive experiments using one of the most
capable generalist web agent frameworks to date, SeeAct. The results
demonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.
Stealing full user requests is more challenging, but a relaxed version of EIA
can still achieve 16% ASR. Despite these concerning results, it is important to
note that the attack can still be detectable through careful human inspection,
highlighting a trade-off between high autonomy and security. This leads to our
detailed discussion on the efficacy of EIA under different levels of human
supervision as well as implications on defenses for generalist web agents.

ÊëòË¶ÅÔºö<paragraph>ÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÂø´ÈÄüÊºîÂåñÔºåÂ±ïÁèæÂá∫È©ö‰∫∫ÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õ‰ª£ÁêÜ‰º¥Èö®ËëóÂâçÊâÄÊú™ÊúâÁöÑÂÆâÂÖ®È¢®Èö™ÔºåËÄåÈÄô‰∫õÈ¢®Èö™ÁõÆÂâçÂπæ‰πéÂ∞öÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÂü∑Ë°åÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÂú®Â∞çÊäóÁí∞Â¢É‰∏≠ÁöÑÈö±ÁßÅÈ¢®Èö™ÁöÑÁ¨¨‰∏ÄÂÄãÁ†îÁ©∂‰æÜÁ∏ÆÂ∞èÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ®ÅËÑÖÊ®°ÂûãÔºåË®éË´ñÂ∞çÊäóÁõÆÊ®ô„ÄÅÈôêÂà∂ÂíåÊîªÊìäÊÉÖÂ¢É„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëËÄÉÊÖÆÂÖ©Á®ÆÈ°ûÂûãÁöÑÂ∞çÊäóÁõÆÊ®ôÔºöÁ´äÂèñ‰ΩøÁî®ËÄÖÁöÑÁâπÂÆöÂÄã‰∫∫ÂèØË≠òÂà•Ë≥áË®ä (PII) ÊàñÁ´äÂèñÊï¥ÂÄã‰ΩøÁî®ËÄÖË¶ÅÊ±Ç„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÈÄô‰∫õÁõÆÊ®ôÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊîªÊìäÊñπÊ≥ïÔºåÁ®±ÁÇ∫Áí∞Â¢ÉÊ≥®ÂÖ•ÊîªÊìä (EIA)„ÄÇÊ≠§ÊîªÊìäÊ≥®ÂÖ•ÊÉ°ÊÑèÂÖßÂÆπÔºåÊó®Âú®ÈÅ©Êáâ‰ª£ÁêÜÈÅã‰ΩúÁöÑ‰∏çÂêåÁí∞Â¢ÉÔºåÂ∞éËá¥‰ª£ÁêÜÂü∑Ë°åÈùûÈ†êÊúüÁöÑÂãï‰Ωú„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁâπÂà•ÈáùÂ∞çÈö±ÁßÅÊÉÖÂ¢ÉÂØ¶‰æãÂåñ EIA„ÄÇÂÆÉÂú®ÂÖ∑ÊúâË™™ÊúçÂäõÁöÑÊåá‰ª§ÊóÅÊèíÂÖ•ÊÉ°ÊÑèÁ∂≤Ë∑ØÂÖÉÁ¥†ÔºåË™§Â∞éÁ∂≤Ë∑Ø‰ª£ÁêÜÊ¥©Èú≤ÁßÅ‰∫∫Ë≥áË®äÔºå‰∏¶ÂèØÈÄ≤‰∏ÄÊ≠•Âà©Áî® CSS Âíå JavaScript ÂäüËÉΩ‰øùÊåÅÈö±ÂØÜ„ÄÇÊàëÂÄëÂæû Mind2Web Ë≥áÊñôÈõÜÁöÑÁèæÂØ¶Á∂≤Á´ôÊî∂ÈõÜ‰∫Ü 177 ÂÄãÊ∂âÂèä‰∏çÂêå PII È°ûÂà•ÁöÑÂãï‰ΩúÊ≠•È©üÔºå‰∏¶‰ΩøÁî®ËøÑ‰ªäÁÇ∫Ê≠¢ÂäüËÉΩÊúÄÂº∑Â§ßÁöÑÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÊ°ÜÊû∂ SeeAct ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©ó„ÄÇÁµêÊûúË≠âÊòéÔºåEIA Âú®Á´äÂèñ‰ΩøÁî®ËÄÖÁöÑÁâπÂÆö PII ÊñπÈù¢ÈÅîÂà∞‰∫Ü 70% ÁöÑ ASR„ÄÇÁ´äÂèñÂÆåÊï¥ÁöÑ‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÔºå‰ΩÜ EIA ÁöÑÊîæÂØ¨ÁâàÊú¨‰ªçÂèØÈÅîÂà∞ 16% ÁöÑ ASR„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õ‰ª§‰∫∫ÊìîÊÜÇÁöÑÁµêÊûúÔºå‰ΩÜÈáçË¶ÅÁöÑÊòØË¶ÅÊ≥®ÊÑèÔºåÊîªÊìä‰ªçÁÑ∂ÂèØ‰ª•ÈÄèÈÅé‰ªîÁ¥∞ÁöÑ‰∫∫Â∑•Ê™¢Êü•‰æÜÂÅµÊ∏¨ÔºåÁ™ÅÈ°Ø‰∫ÜÈ´òÂ∫¶Ëá™‰∏ªÊÄßËàáÂÆâÂÖ®ÊÄß‰πãÈñìÁöÑÊ¨äË°°„ÄÇÈÄôÂ∞éËá¥ÊàëÂÄëË©≥Á¥∞Ë®éË´ñ‰∫Ü EIA Âú®‰∏çÂêåÂ±§Á¥öÁöÑ‰∫∫Â∑•Áõ£Áù£‰∏ãÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂ∞çÈÄöÁî®Á∂≤Ë∑Ø‰ª£ÁêÜÈò≤Á¶¶ÁöÑÂΩ±Èüø„ÄÇ</paragraph>

##### **Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**
2409.11192v1 by Eunhae Lee

One application area of long-term memory (LTM) capabilities with increasing
traction is personal AI companions and assistants. With the ability to retain
and contextualize past interactions and adapt to user preferences, personal AI
companions and assistants promise a profound shift in how we interact with AI
and are on track to become indispensable in personal and professional settings.
However, this advancement introduces new challenges and vulnerabilities that
require careful consideration regarding the deployment and widespread use of
these systems. The goal of this paper is to explore the broader implications of
building and deploying personal AI applications with LTM capabilities using a
holistic evaluation approach. This will be done in three ways: 1) reviewing the
technological underpinnings of LTM in Large Language Models, 2) surveying
current personal AI companions and assistants, and 3) analyzing critical
considerations and implications of deploying and using these applications.

ÊëòË¶ÅÔºöÈï∑ÊúüË®òÊÜ∂ (LTM) ËÉΩÂäõÁöÑÊáâÁî®È†òÂüü‰πã‰∏ÄÊòØÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÔºåÂÖ∂Âê∏ÂºïÂäõÊ≠£ËàáÊó•‰ø±Â¢û„ÄÇÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÂÖ∑ÂÇô‰øùÁïôÂíåÂ∞áÈÅéÂéª‰∫íÂãïËÑàÁµ°ÂåñÔºå‰ª•ÂèäÈÅ©Êáâ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑËÉΩÂäõÔºåÊâøË´æÂ∞áÂæπÂ∫ïÊîπËÆäÊàëÂÄëËàá AI ‰∫íÂãïÁöÑÊñπÂºèÔºå‰∏¶ÊúâÊúõÂú®ÂÄã‰∫∫ÂíåÂ∞àÊ•≠È†òÂüü‰∏≠ËÆäÂæó‰∏çÂèØÊàñÁº∫„ÄÇÁÑ∂ËÄåÔºåÈÄôÈ†ÖÈÄ≤Â±ïÂ∏∂‰æÜ‰∫ÜÊñ∞ÁöÑÊåëÊà∞ÂíåÊºèÊ¥ûÔºåÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáèÈÄô‰∫õÁ≥ªÁµ±ÁöÑÈÉ®ÁΩ≤ÂíåÂª£Ê≥õ‰ΩøÁî®„ÄÇÊú¨ÊñáÁöÑÁõÆÊ®ôÊòØÂà©Áî®Êï¥È´îË©ï‰º∞ÊñπÊ≥ïÊé¢Ë®éÂª∫ÊßãÂíåÈÉ®ÁΩ≤ÂÖ∑ÂÇô LTM ËÉΩÂäõÁöÑÂÄã‰∫∫ AI ÊáâÁî®Á®ãÂºèÁöÑÂª£Ê≥õÂΩ±Èüø„ÄÇÈÄôÂ∞áÈÄèÈÅé‰∏âÁ®ÆÊñπÂºèÈÄ≤Ë°åÔºö1) Ê™¢Ë¶ñÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ LTM ÁöÑÊäÄË°ìÂü∫Á§éÔºå2) Ë™øÊü•ÁõÆÂâçÁöÑÂÄã‰∫∫ AI ‰º¥‰æ∂ÂíåÂä©ÁêÜÔºå‰ª•Âèä 3) ÂàÜÊûêÈÉ®ÁΩ≤Âíå‰ΩøÁî®ÈÄô‰∫õÊáâÁî®Á®ãÂºèÁöÑÈóúÈçµËÄÉÈáèÂíåÂΩ±Èüø„ÄÇ

##### **Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**
2409.10932v1 by Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M

Coronary heart disease (CHD) is a severe cardiac disease, and hence, its
early diagnosis is essential as it improves treatment results and saves money
on medical care. The prevailing development of quantum computing and machine
learning (ML) technologies may bring practical improvement to the performance
of CHD diagnosis. Quantum machine learning (QML) is receiving tremendous
interest in various disciplines due to its higher performance and capabilities.
A quantum leap in the healthcare industry will increase processing power and
optimise multiple models. Techniques for QML have the potential to forecast
cardiac disease and help in early detection. To predict the risk of coronary
heart disease, a hybrid approach utilizing an ensemble machine learning model
based on QML classifiers is presented in this paper. Our approach, with its
unique ability to address multidimensional healthcare data, reassures the
method's robustness by fusing quantum and classical ML algorithms in a
multi-step inferential framework. The marked rise in heart disease and death
rates impacts worldwide human health and the global economy. Reducing cardiac
morbidity and mortality requires early detection of heart disease. In this
research, a hybrid approach utilizes techniques with quantum computing
capabilities to tackle complex problems that are not amenable to conventional
machine learning algorithms and to minimize computational expenses. The
proposed method has been developed in the Raspberry Pi 5 Graphics Processing
Unit (GPU) platform and tested on a broad dataset that integrates clinical and
imaging data from patients suffering from CHD and healthy controls. Compared to
classical machine learning models, the accuracy, sensitivity, F1 score, and
specificity of the proposed hybrid QML model used with CHD are manifold higher.

ÊëòË¶ÅÔºöÂÜ†ÁãÄÂãïËÑàÂøÉËáüÁóÖ (CHD) ÊòØ‰∏ÄÁ®ÆÂö¥ÈáçÁöÑÁñæÁóÖÔºåÂõ†Ê≠§ÔºåÊó©ÊúüË®∫Êñ∑Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ÊîπÂñÑÊ≤ªÁôÇÁµêÊûú‰∏¶ÁØÄÁúÅÈÜ´ÁôÇ‰øùÂÅ•Ë≤ªÁî®„ÄÇÈáèÂ≠êË®àÁÆóÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊäÄË°ìÁöÑÁõõË°åÁôºÂ±ïÂèØËÉΩÊúÉÂ∞ç CHD Ë®∫Êñ∑ÁöÑÊÄßËÉΩÂ∏∂‰æÜÂØ¶ÈöõÊîπÂñÑ„ÄÇÈáèÂ≠êÊ©üÂô®Â≠∏Áøí (QML) Áî±ÊñºÂÖ∂Êõ¥È´òÁöÑÊÄßËÉΩÂíåËÉΩÂäõÔºåÂú®ÂêÑÂÄãÈ†òÂüüÂºïËµ∑‰∫ÜÊ•µÂ§ßÁöÑËààË∂£„ÄÇÈÜ´ÁôÇ‰øùÂÅ•Ë°åÊ•≠ÁöÑÈáèÂ≠êÈ£õË∫çÂ∞áÂ¢ûÂä†ËôïÁêÜËÉΩÂäõ‰∏¶ÂÑ™ÂåñÂ§öÂÄãÊ®°Âûã„ÄÇQML ÁöÑÊäÄË°ìÊúâÊΩõÂäõÈ†êÊ∏¨ÂøÉËáüÁóÖ‰∏¶Âπ´Âä©Êó©ÊúüÁôºÁèæ„ÄÇÁÇ∫‰∫ÜÈ†êÊ∏¨ÂÜ†ÁãÄÂãïËÑàÂøÉËáüÁóÖÁöÑÈ¢®Èö™ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº QML ÂàÜÈ°ûÂô®ÁöÑÊ∑∑ÂêàÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊ∑∑ÂêàÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÈÄôÁ®ÆÊñπÊ≥ïÂÖ∑ÂÇôËôïÁêÜÂ§öÁ∂≠ÈÜ´ÁôÇ‰øùÂÅ•Êï∏ÊìöÁöÑÁç®ÁâπËÉΩÂäõÔºåÈÄöÈÅéÂú®Â§öÊ≠•È©üÊé®ÁêÜÊ°ÜÊû∂‰∏≠ËûçÂêàÈáèÂ≠êÂíåÁ∂ìÂÖ∏ ML ÊºîÁÆóÊ≥ïÔºåÁ¢∫‰øù‰∫ÜË©≤ÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÂøÉËáüÁóÖÂíåÊ≠ª‰∫°ÁéáÁöÑÈ°ØËëó‰∏äÂçáÂΩ±Èüø‰∫ÜÂÖ®ÁêÉ‰∫∫È°ûÂÅ•Â∫∑ÂíåÂÖ®ÁêÉÁ∂ìÊøü„ÄÇÈôç‰ΩéÂøÉËáüÁôºÁóÖÁéáÂíåÊ≠ª‰∫°ÁéáÈúÄË¶ÅÂ∞çÂøÉËáüÁóÖÈÄ≤Ë°åÊó©ÊúüÁôºÁèæ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠Ôºå‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÂà©Áî®ÂÖ∑ÊúâÈáèÂ≠êË®àÁÆóËÉΩÂäõÁöÑÊäÄË°ì‰æÜËß£Ê±∫ÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁÑ°Ê≥ïËß£Ê±∫ÁöÑË§áÈõúÂïèÈ°åÔºå‰∏¶ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Ê∏õÂ∞ëË®àÁÆóÈñãÈä∑„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∑≤Âú® Raspberry Pi 5 Áπ™ÂúñËôïÁêÜÂñÆÂÖÉ (GPU) Âπ≥Ëá∫‰∏äÈñãÁôºÔºå‰∏¶Âú®‰∏ÄÂÄãÂª£Ê≥õÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶ÔºåË©≤Ë≥áÊñôÈõÜÊï¥Âêà‰∫ÜÊÇ£Êúâ CHD ÂíåÂÅ•Â∫∑Â∞çÁÖßËÄÖÁöÑËá®Â∫äÂíåÂΩ±ÂÉèÊï∏Êìö„ÄÇËàáÁ∂ìÂÖ∏Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊ∑∑Âêà QML Ê®°ÂûãËàá CHD ‰∏ÄËµ∑‰ΩøÁî®ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÊïèÊÑüÊÄß„ÄÅF1 ÂàÜÊï∏ÂíåÁâπÁï∞ÊÄßÊõ¥È´ò„ÄÇ

##### **Self-supervised Speech Models for Word-Level Stuttered Speech Detection**
2409.10704v1 by Yi-Jen Shih, Zoi Gkalitsiou, Alexandros G. Dimakis, David Harwath

Clinical diagnosis of stuttering requires an assessment by a licensed
speech-language pathologist. However, this process is time-consuming and
requires clinicians with training and experience in stuttering and fluency
disorders. Unfortunately, only a small percentage of speech-language
pathologists report being comfortable working with individuals who stutter,
which is inadequate to accommodate for the 80 million individuals who stutter
worldwide. Developing machine learning models for detecting stuttered speech
would enable universal and automated screening for stuttering, enabling speech
pathologists to identify and follow up with patients who are most likely to be
diagnosed with a stuttering speech disorder. Previous research in this area has
predominantly focused on utterance-level detection, which is not sufficient for
clinical settings where word-level annotation of stuttering is the norm. In
this study, we curated a stuttered speech dataset with word-level annotations
and introduced a word-level stuttering speech detection model leveraging
self-supervised speech models. Our evaluation demonstrates that our model
surpasses previous approaches in word-level stuttering speech detection.
Additionally, we conducted an extensive ablation analysis of our method,
providing insight into the most important aspects of adapting self-supervised
speech models for stuttered speech detection.

ÊëòË¶ÅÔºöËá®Â∫äÂè£ÂêÉË®∫Êñ∑ÈúÄË¶ÅÁî±Âü∑ÁÖßË™ûË®ÄÁóÖÁêÜÂ≠∏ÂÆ∂Ë©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÄãÈÅéÁ®ãÂæàËÄóÊôÇÔºåÈúÄË¶ÅÂèóÈÅéÂè£ÂêÉÂíåÊµÅÂà©ÈöúÁ§ôË®ìÁ∑¥ÂíåÁ∂ìÈ©óÁöÑËá®Â∫äÈÜ´Áîü„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂè™Êúâ‰∏ÄÂ∞èÈÉ®ÂàÜË™ûË®ÄÁóÖÁêÜÂ≠∏ÂÆ∂Ë°®Á§∫È°òÊÑèËàáÂè£ÂêÉËÄÖÂêà‰ΩúÔºåÈÄô‰∏çË∂≥‰ª•ÂÆπÁ¥çÂÖ®ÁêÉ 8000 Ëê¨ÂêçÂè£ÂêÉËÄÖ„ÄÇÈñãÁôºÁî®ÊñºÊ™¢Ê∏¨Âè£ÂêÉË™ûÈü≥ÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂ∞áËÉΩÂ∞çÂè£ÂêÉÈÄ≤Ë°åÊôÆÈÅç‰∏îËá™ÂãïÂåñÁöÑÁØ©Êü•Ôºå‰ΩøË™ûË®ÄÁóÖÁêÜÂ≠∏ÂÆ∂ËÉΩÂ§†Ë≠òÂà•‰∏¶ËøΩËπ§ÊúÄÊúâÂèØËÉΩË¢´Ë®∫Êñ∑Âá∫ÊÇ£ÊúâÂè£ÂêÉË®ÄË™ûÈöúÁ§ôÁöÑÊÇ£ËÄÖ„ÄÇÈÄôÊñπÈù¢ÁöÑÂÖàÂâçÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠ÊñºË©±Ë™ûÂ±§Á¥öÁöÑÊ™¢Ê∏¨ÔºåÈÄô‰∏çË∂≥‰ª•Áî®ÊñºÂè£ÂêÉÁöÑÂñÆÂ≠óÂ±§Á¥öË®ªËß£ÁÇ∫Â∏∏ÊÖãÁöÑËá®Â∫äÁí∞Â¢É„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫Ü‰∏ÄÂÄãÂ∏∂ÊúâÂñÆÂ≠óÂ±§Á¥öË®ªËß£ÁöÑÂè£ÂêÉË™ûÈü≥Ë≥áÊñôÈõÜÔºå‰∏¶ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂà©Áî®Ëá™ÊàëÁõ£Áù£Ë™ûÈü≥Ê®°ÂûãÁöÑÂñÆÂ≠óÂ±§Á¥öÂè£ÂêÉË™ûÈü≥Ê™¢Ê∏¨Ê®°Âûã„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Ë≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂñÆÂ≠óÂ±§Á¥öÂè£ÂêÉË™ûÈü≥Ê™¢Ê∏¨‰∏≠ÂÑ™ÊñºÂÖàÂâçÁöÑÂÅöÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞çÊàëÂÄëÁöÑÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÊ∂àËûçÂàÜÊûêÔºåÊèê‰æõ‰∫ÜÂ∞çË™øÊï¥Ëá™ÊàëÁõ£Áù£Ë™ûÈü≥Ê®°Âûã‰ª•ÈÄ≤Ë°åÂè£ÂêÉË™ûÈü≥Ê™¢Ê∏¨ÁöÑÊúÄÈáçË¶ÅÈù¢ÂêëÁöÑË¶ãËß£„ÄÇ

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊèêÁ§∫Â≠¶‰π†Ê°ÜÊû∂ÁöÑÁü•ËØÜÂ¢ûÂº∫ÁñæÁóÖËØäÊñ≠ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ï‰ªé‰∏é‰∏¥Â∫äÁóÖ‰æãÁõ∏ÂÖ≥ÁöÑÂ§ñÈÉ®Áü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢ÁªìÊûÑÂåñÁü•ËØÜÔºåÂØπÂÖ∂ËøõË°åÁºñÁ†ÅÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞ÊèêÁ§∫Ê®°Êùø‰∏≠Ôºå‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÂØπ‰ªªÂä°ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®‰∏â‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºöCHIP-CTC„ÄÅIMCS-V2-NER Âíå KUAKE-QTR„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÊòéÊòæ‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÂú® CHIP-CTC Êï∞ÊçÆÈõÜ‰∏äÁöÑ F1 ÂæóÂàÜÊèêÈ´ò‰∫Ü 2.4%ÔºåÂú® IMCS-V2-NER Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 3.1%ÔºåÂú® KUAKE-QTR Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 4.2%„ÄÇÊ≠§Â§ñÔºåÊ∂àËûçÁ†îÁ©∂ËØÅÂÆû‰∫ÜÁü•ËØÜÊ≥®ÂÖ•Ê®°ÂùóÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂõ†‰∏∫ÁßªÈô§Ê≠§Ê®°Âùó‰ºöÂØºËá¥ F1 ÂæóÂàÜÊòæÁùÄ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏ç‰ªÖÊúâÊïàÊèêÈ´ò‰∫ÜÁñæÁóÖËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÂ¢ûÂº∫‰∫ÜÈ¢ÑÊµãÁöÑÂèØËß£ÈáäÊÄßÔºå‰∏∫‰∏¥Â∫äËØäÊñ≠Êèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑÊîØÊåÅÂíåËØÅÊçÆ„ÄÇ

##### **Robust image representations with counterfactual contrastive learning**
2409.10365v1 by M√©lanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker

Contrastive pretraining can substantially increase model generalisation and
downstream performance. However, the quality of the learned representations is
highly dependent on the data augmentation strategy applied to generate positive
pairs. Positive contrastive pairs should preserve semantic meaning while
discarding unwanted variations related to the data acquisition domain.
Traditional contrastive pipelines attempt to simulate domain shifts through
pre-defined generic image transformations. However, these do not always mimic
realistic and relevant domain variations for medical imaging such as scanner
differences. To tackle this issue, we herein introduce counterfactual
contrastive learning, a novel framework leveraging recent advances in causal
image synthesis to create contrastive positive pairs that faithfully capture
relevant domain variations. Our method, evaluated across five datasets
encompassing both chest radiography and mammography data, for two established
contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive
learning in terms of robustness to acquisition shift. Notably, counterfactual
contrastive learning achieves superior downstream performance on both
in-distribution and on external datasets, especially for images acquired with
scanners under-represented in the training set. Further experiments show that
the proposed framework extends beyond acquisition shifts, with models trained
with counterfactual contrastive learning substantially improving subgroup
performance across biological sex.

ÊëòË¶ÅÔºöÂ∞çÊØîÈ†êË®ìÁ∑¥ÂèØ‰ª•Â§ßÂπÖÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíå‰∏ãÊ∏∏ÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ≠∏ÁøíÂà∞ÁöÑË°®ÂæµÂìÅË≥™È´òÂ∫¶‰æùË≥¥ÊñºÁî®‰æÜÁî¢ÁîüÊ≠£ÂêëÈÖçÂ∞çÁöÑË≥áÊñôÊì¥ÂÖÖÁ≠ñÁï•„ÄÇÊ≠£ÂêëÂ∞çÊØîÈÖçÂ∞çÊáâÁï∂‰øùÁïôË™ûÊÑèÊÑèÁæ©ÔºåÂêåÊôÇÊç®Ê£ÑËàáË≥áÊñôÊì∑ÂèñÈ†òÂüüÁõ∏ÈóúÁöÑ‰∏çÂøÖË¶ÅËÆäÁï∞„ÄÇÂÇ≥Áµ±ÁöÑÂ∞çÊØîÁÆ°Á∑öÊúÉÂòóË©¶ÈÄèÈÅéÈ†êÂÖàÂÆöÁæ©ÁöÑÈÄöÁî®ÂΩ±ÂÉèËΩâÊèõ‰æÜÊ®°Êì¨È†òÂüüËΩâÁßª„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õËΩâÊèõ‰∏¶‰∏çÁ∏ΩÊòØËÉΩÊ®°‰ªøÈÜ´ÁôÇÂΩ±ÂÉèÁöÑÂØ¶Èöõ‰∏îÁõ∏ÈóúÈ†òÂüüËÆäÁï∞Ôºå‰æãÂ¶ÇÊéÉÊèèÂÑÄÁöÑÂ∑ÆÁï∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂú®Ê≠§ÊèêÂá∫Âèç‰∫ãÂØ¶Â∞çÊØîÂ≠∏ÁøíÔºå‰∏ÄÂÄãÂà©Áî®Âõ†ÊûúÂΩ±ÂÉèÂêàÊàêËøëÊúüÈÄ≤Â±ï‰æÜÂª∫Á´ãÂø†ÂØ¶ÊçïÊçâÁõ∏ÈóúÈ†òÂüüËÆäÁï∞ÁöÑÂ∞çÊØîÊ≠£ÂêëÈÖçÂ∞çÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Ê∂µËìãËÉ∏ÈÉ® X ÂÖâÂíå‰π≥ÊàøÊîùÂΩ±Ë≥áÊñôÁöÑ‰∫îÂÄãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåÂ∞çÊñºÂÖ©ÂÄãÂ∑≤Âª∫Á´ãÁöÑÂ∞çÊØîÁõÆÊ®ôÔºàSimCLR Âíå DINO-v2ÔºâÔºåÂú®Â∞çÊñºÊì∑ÂèñËΩâÁßªÁöÑÁ©©ÂÅ•ÊÄßÊñπÈù¢ÂÑ™ÊñºÊ®ôÊ∫ñÂ∞çÊØîÂ≠∏Áøí„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂèç‰∫ãÂØ¶Â∞çÊØîÂ≠∏ÁøíÂú®ÂÖßÈÉ®ÂàÜ‰ΩàÂíåÂ§ñÈÉ®Ë≥áÊñôÈõÜ‰∏äÈÉΩËÉΩÈÅîÊàêÂÑ™Áï∞ÁöÑ‰∏ãÊ∏∏ÊïàËÉΩÔºåÁâπÂà•ÊòØÂ∞çÊñºË®ìÁ∑¥ÈõÜ‰∏≠‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑÊéÉÊèèÂÑÄÊâÄÊì∑ÂèñÁöÑÂΩ±ÂÉè„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÂª∂‰º∏Âà∞Êì∑ÂèñËΩâÁßª‰πãÂ§ñÔºå‰ΩøÁî®Âèç‰∫ãÂØ¶Â∞çÊØîÂ≠∏ÁøíË®ìÁ∑¥ÁöÑÊ®°ÂûãÂ§ßÂπÖÊèêÂçá‰∫ÜÁîüÁâ©ÊÄßÂà•ÁöÑÂ≠êÁæ§ÊïàËÉΩ„ÄÇ

##### **Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation between the United States and South Africa**
2409.10168v1 by Hayoung Jung, Prerna Juneja, Tanushree Mitra

Despite being an integral tool for finding health-related information online,
YouTube has faced criticism for disseminating COVID-19 misinformation globally
to its users. Yet, prior audit studies have predominantly investigated YouTube
within the Global North contexts, often overlooking the Global South. To
address this gap, we conducted a comprehensive 10-day geolocation-based audit
on YouTube to compare the prevalence of COVID-19 misinformation in search
results between the United States (US) and South Africa (SA), the countries
heavily affected by the pandemic in the Global North and the Global South,
respectively. For each country, we selected 3 geolocations and placed
sock-puppets, or bots emulating "real" users, that collected search results for
48 search queries sorted by 4 search filters for 10 days, yielding a dataset of
915K results. We found that 31.55% of the top-10 search results contained
COVID-19 misinformation. Among the top-10 search results, bots in SA faced
significantly more misinformative search results than their US counterparts.
Overall, our study highlights the contrasting algorithmic behaviors of YouTube
search between two countries, underscoring the need for the platform to
regulate algorithmic behavior consistently across different regions of the
Globe.

ÊëòË¶ÅÔºöÂÑòÁÆ° YouTube ÊòØÂú®Á∂≤Ë∑Ø‰∏äÂ∞ãÊâæËàáÂÅ•Â∫∑Áõ∏ÈóúË≥áË®äÁöÑ‰∏ÄÈ†ÖÈáçË¶ÅÂ∑•ÂÖ∑Ôºå‰ΩÜÂÆÉ‰πüÂõ†ÁÇ∫ÂêëÂÖ®ÁêÉ‰ΩøÁî®ËÄÖÊï£Êí≠ COVID-19 ÈåØË™§Ë≥áË®äËÄåÂèóÂà∞ÊâπË©ï„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑÁ®ΩÊ†∏Á†îÁ©∂‰∏ªË¶ÅÂú®ÂÖ®ÁêÉÂåóÊñπÁöÑËÉåÊôØ‰∏ãË™øÊü• YouTubeÔºåÂ∏∏Â∏∏ÂøΩÁï•‰∫ÜÂÖ®ÁêÉÂçóÊñπ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂú® YouTube ‰∏äÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÁÇ∫Êúü 10 Â§©ÁöÑÁ∂úÂêàÂú∞ÁêÜ‰ΩçÁΩÆÁ®ΩÊ†∏Ôºå‰ª•ÊØîËºÉÁæéÂúãÔºàÁæéÂúãÔºâÂíåÂçóÈùûÔºàÂçóÈùûÔºâÊêúÂ∞ãÁµêÊûú‰∏≠ COVID-19 ÈåØË™§Ë≥áË®äÁöÑÁõõË°åÁéáÔºåÈÄôÂÖ©ÂÄãÂúãÂÆ∂ÂàÜÂà•ÊòØÂÖ®ÁêÉÂåóÊñπÂíåÂÖ®ÁêÉÂçóÊñπ‰∏≠ÂèóÁñ´ÊÉÖÂö¥ÈáçÂΩ±ÈüøÁöÑÂúãÂÆ∂„ÄÇÂ∞çÊñºÊØèÂÄãÂúãÂÆ∂ÔºåÊàëÂÄëÈÅ∏Êìá‰∫Ü 3 ÂÄãÂú∞ÁêÜ‰ΩçÁΩÆÔºå‰∏¶ÊîæÁΩÆ‰∫ÜÊ®°Êì¨„ÄåÁúüÂØ¶„Äç‰ΩøÁî®ËÄÖÁöÑË•™Â≠êÂÇÄÂÑ°ÊàñÊ©üÂô®‰∫∫ÔºåÊî∂ÈõÜ‰∫Ü 48 ÂÄãÊêúÂ∞ãÊü•Ë©¢ÁöÑÊêúÂ∞ãÁµêÊûúÔºå‰∏¶Ê†πÊìö 4 ÂÄãÊêúÂ∞ãÁØ©ÈÅ∏Ê¢ù‰ª∂ÈÄ≤Ë°å‰∫Ü 10 Â§©ÁöÑÊéíÂ∫èÔºåÁî¢Áîü‰∫Ü 915K Á≠ÜÁµêÊûúÁöÑË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁôºÁèæÔºå31.55% ÁöÑÂâç 10 ÂêçÊêúÂ∞ãÁµêÊûúÂåÖÂê´ COVID-19 ÈåØË™§Ë≥áË®ä„ÄÇÂú®ÊéíÂêçÂâç 10 ÂêçÁöÑÊêúÂ∞ãÁµêÊûú‰∏≠ÔºåÂçóÈùûÁöÑÊ©üÂô®‰∫∫Èù¢Ëá®ÁöÑÈåØË™§Ë≥áË®äÊêúÂ∞ãÁµêÊûúÊòéÈ°ØÂ§öÊñºÁæéÂúãÁöÑÊ©üÂô®‰∫∫„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Á™ÅÂá∫‰∫Ü YouTube ÊêúÂ∞ãÂú®ÂÖ©ÂÄãÂúãÂÆ∂‰πãÈñìÂ∞çÊØîÁöÑÊºîÁÆóÊ≥ïË°åÁÇ∫ÔºåÂº∑Ë™ø‰∫ÜË©≤Âπ≥Âè∞ÈúÄË¶ÅÂú®ÂÖ®ÁêÉ‰∏çÂêåÂú∞ÂçÄ‰∏ÄËá¥Âú∞Ë¶èÁØÑÊºîÁÆóÊ≥ïË°åÁÇ∫„ÄÇ

##### **DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion**
2409.10080v1 by Yuchen Guo, Ruoxiang Xu, Rongcheng Li, Zhenghao Wu, Weifeng Su

Multi-modality image fusion aims to integrate complementary data information
from different imaging modalities into a single image. Existing methods often
generate either blurry fused images that lose fine-grained semantic information
or unnatural fused images that appear perceptually cropped from the inputs. In
this work, we propose a novel two-phase discriminative autoencoder framework,
termed DAE-Fuse, that generates sharp and natural fused images. In the
adversarial feature extraction phase, we introduce two discriminative blocks
into the encoder-decoder architecture, providing an additional adversarial loss
to better guide feature extraction by reconstructing the source images. While
the two discriminative blocks are adapted in the attention-guided
cross-modality fusion phase to distinguish the structural differences between
the fused output and the source inputs, injecting more naturalness into the
results. Extensive experiments on public infrared-visible, medical image
fusion, and downstream object detection datasets demonstrate our method's
superiority and generalizability in both quantitative and qualitative
evaluations.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÂΩ±ÂÉèËûçÂêàÊó®Âú®Â∞á‰æÜËá™‰∏çÂêåÂΩ±ÂÉèÊ®°ÊÖãÁöÑ‰∫íË£úË≥áÊñôË≥áË®äÊï¥ÂêàÂà∞ÂñÆ‰∏ÄÂΩ±ÂÉè‰∏≠„ÄÇÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÁî¢ÁîüÊ®°Á≥äÁöÑËûçÂêàÂΩ±ÂÉèÔºåÂ§±ÂéªÁ¥∞Á∑ªÁöÑË™ûÊÑèË≥áË®äÔºåÊàñÊòØ‰∏çËá™ÁÑ∂ÁöÑËûçÂêàÂΩ±ÂÉèÔºåÂú®ÊÑüÁü•‰∏äÁúãËµ∑‰æÜÂÉèÊòØÂæûËº∏ÂÖ•‰∏≠Ë£ÅÂàáÂá∫‰æÜÁöÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ©ÈöéÊÆµÂà§Âà•ÂºèËá™Á∑®Á¢ºÂô®Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫ DAE-FuseÔºåÂèØÁî¢ÁîüÊ∏ÖÊô∞‰∏îËá™ÁÑ∂ÁöÑËûçÂêàÂΩ±ÂÉè„ÄÇÂú®Â∞çÊäóÁâπÂæµÊèêÂèñÈöéÊÆµÔºåÊàëÂÄëÂú®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂Êßã‰∏≠ÂºïÂÖ•ÂÖ©ÂÄãÂà§Âà•ÂºèÂçÄÂ°äÔºåÊèê‰æõÈ°çÂ§ñÁöÑÂ∞çÊäóÊêçÂ§±ÔºåËóâÁî±ÈáçÂª∫ÂéüÂßãÂΩ±ÂÉè‰æÜÊõ¥Â•ΩÂú∞ÂºïÂ∞éÁâπÂæµÊèêÂèñ„ÄÇÈõñÁÑ∂ÂÖ©ÂÄãÂà§Âà•ÂºèÂçÄÂ°äÂú®Ê≥®ÊÑèÂäõÂºïÂ∞éÁöÑË∑®Ê®°ÊÖãËûçÂêàÈöéÊÆµ‰∏≠ÈÄ≤Ë°åË™øÊï¥Ôºå‰ª•ÂçÄÂàÜËûçÂêàËº∏Âá∫ËàáÂéüÂßãËº∏ÂÖ•‰πãÈñìÁöÑÁµêÊßãÂ∑ÆÁï∞ÔºåÁÇ∫ÁµêÊûúÊ≥®ÂÖ•Êõ¥Â§öËá™ÁÑ∂ÊÄß„ÄÇÈáùÂ∞çÂÖ¨ÈñãÁ¥ÖÂ§ñÁ∑öÂèØË¶ãÂÖâ„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèËûçÂêàÂíå‰∏ãÊ∏∏Áâ©‰ª∂ÂÅµÊ∏¨Ë≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÈáèÂåñÂíåÂÆöÊÄßË©ï‰º∞‰∏≠ÁöÑÂÑ™Ë∂äÊÄßÂíåÊ≥õÂåñÊÄß„ÄÇ

##### **MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**
2409.10064v1 by Sijie Ji, Xinzhe Zheng, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava

Mental health disorders are among the most prevalent diseases worldwide,
affecting nearly one in four people. Despite their widespread impact, the
intervention rate remains below 25%, largely due to the significant cooperation
required from patients for both diagnosis and intervention. The core issue
behind this low treatment rate is stigma, which discourages over half of those
affected from seeking help. This paper presents MindGuard, an accessible,
stigma-free, and professional mobile mental healthcare system designed to
provide mental health first aid. The heart of MindGuard is an innovative edge
LLM, equipped with professional mental health knowledge, that seamlessly
integrates objective mobile sensor data with subjective Ecological Momentary
Assessment records to deliver personalized screening and intervention
conversations. We conduct a broad evaluation of MindGuard using open datasets
spanning four years and real-world deployment across various mobile devices
involving 20 subjects for two weeks. Remarkably, MindGuard achieves results
comparable to GPT-4 and outperforms its counterpart with more than 10 times the
model size. We believe that MindGuard paves the way for mobile LLM
applications, potentially revolutionizing mental healthcare practices by
substituting self-reporting and intervention conversations with passive,
integrated monitoring within daily life, thus ensuring accessible and
stigma-free mental health support.

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑ÁñæÁóÖÊòØÂÖ®ÁêÉÊúÄÊôÆÈÅçÁöÑÁñæÁóÖ‰πã‰∏ÄÔºå
ÂΩ±Èüø‰∫ÜËøëÂõõÂàÜ‰πã‰∏ÄÁöÑ‰∫∫„ÄÇÂÑòÁÆ°ÂÖ∂ÂΩ±ÈüøÂª£Ê≥õÔºå
‰ΩÜ‰ªãÂÖ•Áéá‰ªç‰ΩéÊñº 25%ÔºåÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂõ†ÁÇ∫
ÊÇ£ËÄÖÂú®Ë®∫Êñ∑Âíå‰ªãÂÖ•ÊôÇÈúÄË¶ÅÂ§ßÈáèÈÖçÂêà„ÄÇËÉåÂæåÂ∞éËá¥
Ê≤ªÁôÇÁéá‰Ωé‰∏ãÁöÑÊ†∏ÂøÉÂïèÈ°åÊòØÊ±°ÂêçÂåñÔºåÈÄôËÆìË∂ÖÈÅé‰∏ÄÂçäÁöÑ
ÂèóÂΩ±ÈüøËÄÖ‰∏çÈ°òÊÑèÂ∞ãÊ±ÇÂπ´Âä©„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü MindGuardÔºå‰∏ÄÂÄã
ÊòìÊñºÂèñÂæó„ÄÅÁÑ°Ê±°ÂêçÂåñ‰∏îÂ∞àÊ•≠ÁöÑÊâãÊ©üÂøÉÁêÜ‰øùÂÅ•Á≥ªÁµ±ÔºåÊó®Âú®
Êèê‰æõÂøÉÁêÜÊÄ•Êïë„ÄÇMindGuard ÁöÑÊ†∏ÂøÉÊòØ‰∏ÄÂÄãÂâµÊñ∞ÁöÑÈÇäÁ∑£
Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂÖ∑ÂÇôÂ∞àÊ•≠ÁöÑÂøÉÁêÜÂÅ•Â∫∑Áü•Ë≠òÔºåÂÆÉËÉΩÁÑ°Á∏´
Êï¥ÂêàÂÆ¢ËßÄÁöÑÊâãÊ©üÊÑüÊ∏¨Âô®Ë≥áÊñôËàá‰∏ªËßÄÁöÑÁîüÊÖãÁû¨ÊôÇË©ï‰º∞Ë®òÈåÑÔºåÊèê‰æõ
ÂÄã‰∫∫ÂåñÁöÑÁØ©Ê™¢Âíå‰ªãÂÖ•Â∞çË©±„ÄÇÊàëÂÄë‰ΩøÁî®Ê©´Ë∑®ÂõõÂπ¥ÁöÑÈñãÊîæË≥áÊñôÈõÜ
Â∞ç MindGuard ÈÄ≤Ë°åÂª£Ê≥õÁöÑË©ï‰º∞Ôºå‰∏¶Âú®ÂêÑÁ®ÆË°åÂãïË£ùÁΩÆ‰∏äÈÄ≤Ë°åÁÇ∫Êúü
ÂÖ©ÈÄ±„ÄÅÊ∂âÂèä 20 ‰ΩçÂèóË©¶ËÄÖÁöÑÂØ¶ÈöõÈÉ®ÁΩ≤„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåMindGuard
ÈÅîÂà∞ÁöÑÁµêÊûúËàá GPT-4 Áõ∏Áï∂Ôºå‰∏¶‰∏îÂÑ™ÊñºÊ®°ÂûãË¶èÊ®°Â§ßÊñºÂÖ∂ 10 ÂÄç‰ª•‰∏äÁöÑ
Â∞çÊáâÊ®°Âûã„ÄÇÊàëÂÄëÁõ∏‰ø° MindGuard ÁÇ∫Ë°åÂãï LLM ÊáâÁî®Èã™Âπ≥‰∫ÜÈÅìË∑ØÔºå
ÊúâÂèØËÉΩÈÄèÈÅéÂú®Êó•Â∏∏ÁîüÊ¥ª‰∏≠ÈÄ≤Ë°åË¢´Âãï„ÄÅÊï¥ÂêàÁöÑÁõ£Êéß‰æÜÂèñ‰ª£Ëá™ÊàëÂ†±Âëä
Âíå‰ªãÂÖ•Â∞çË©±ÔºåÂæûËÄåÂæπÂ∫ïÊîπËÆäÂøÉÁêÜ‰øùÂÅ•ÂØ¶ÂãôÔºåÈÄ≤ËÄåÁ¢∫‰øùÂèØÂèñÂæó‰∏î
ÁÑ°Ê±°ÂêçÂåñÁöÑÁ≤æÁ•ûÂÅ•Â∫∑ÊîØÊåÅ„ÄÇ

##### **HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**
2409.10011v1 by Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng

Large language models (LLMs) have significantly advanced natural language
processing tasks, yet they are susceptible to generating inaccurate or
unreliable responses, a phenomenon known as hallucination. In critical domains
such as health and medicine, these hallucinations can pose serious risks. This
paper introduces HALO, a novel framework designed to enhance the accuracy and
reliability of medical question-answering (QA) systems by focusing on the
detection and mitigation of hallucinations. Our approach generates multiple
variations of a given query using LLMs and retrieves relevant information from
external open knowledge bases to enrich the context. We utilize maximum
marginal relevance scoring to prioritize the retrieved context, which is then
provided to LLMs for answer generation, thereby reducing the risk of
hallucinations. The integration of LangChain further streamlines this process,
resulting in a notable and robust increase in the accuracy of both open-source
and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%
to 70%). This framework underscores the critical importance of addressing
hallucinations in medical QA systems, ultimately improving clinical
decision-making and patient care. The open-source HALO is available at:
https://github.com/ResponsibleAILab/HALO.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â§ßÂπÖÊèêÂçáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÂÆπÊòìÁî¢Áîü‰∏çÊ∫ñÁ¢∫Êàñ‰∏çÂèØÈù†ÁöÑÂõûÊáâÔºåÈÄôÁèæË±°Á®±ÁÇ∫ÂπªË¶∫„ÄÇÂú®ÂÅ•Â∫∑ÂíåÈÜ´Â≠∏Á≠âÈóúÈçµÈ†òÂüüÔºåÈÄô‰∫õÂπªË¶∫ÂèØËÉΩÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑÈ¢®Èö™„ÄÇÊú¨Ë´ñÊñá‰ªãÁ¥π HALOÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÊó®Âú®ÈÄèÈÅéÂ∞àÊ≥®ÊñºÂÅµÊ∏¨ÂíåÊ∏õËºïÂπªË¶∫Ôºå‰æÜÊèêÂçáÈÜ´ÁôÇÂïèÁ≠î (QA) Á≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØ‰ΩøÁî® LLM Áî¢ÁîüÁµ¶ÂÆöÊü•Ë©¢ÁöÑÂ§öÂÄãËÆäÈ´îÔºå‰∏¶ÂæûÂ§ñÈÉ®ÈñãÊîæÁü•Ë≠òÂ∫´‰∏≠Êì∑ÂèñÁõ∏ÈóúË≥áË®äÔºå‰ª•Ë±êÂØåÂÖßÂÆπ„ÄÇÊàëÂÄëÂà©Áî®ÊúÄÂ§ßÈÇäÈöõÁõ∏ÈóúÊÄßË©ïÂàÜ‰æÜÂÑ™ÂÖàËôïÁêÜÊì∑ÂèñÁöÑÂÖßÂÆπÔºåÁÑ∂ÂæåÊèê‰æõÁµ¶ LLM ‰ª•Áî¢ÁîüÁ≠îÊ°àÔºåÂæûËÄåÈôç‰ΩéÂπªË¶∫ÁöÑÈ¢®Èö™„ÄÇLangChain ÁöÑÊï¥ÂêàÈÄ≤‰∏ÄÊ≠•Á∞°Âåñ‰∫ÜÈÄôÂÄãÊµÅÁ®ãÔºåÂ∞éËá¥ÈñãÊîæÂéüÂßãÁ¢ºÂíåÂïÜÊ•≠ LLMÔºå‰æãÂ¶Ç Llama-3.1ÔºàÂæû 44% Âà∞ 65%ÔºâÂíå ChatGPTÔºàÂæû 56% Âà∞ 70%ÔºâÁöÑÊ∫ñÁ¢∫ÊÄßÈ°ØËëó‰∏îÁ©©ÂÅ•Âú∞ÊèêÂçá„ÄÇÈÄôÂÄãÊû∂ÊßãÂº∑Ë™ø‰∫ÜÂú®ÈÜ´ÁôÇÂïèÁ≠îÁ≥ªÁµ±‰∏≠Ëß£Ê±∫ÂπªË¶∫ÁöÑÈáçË¶ÅÊÄßÔºåÊúÄÁµÇÊîπÂñÑ‰∫ÜËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂíåÊÇ£ËÄÖÁÖßË≠∑„ÄÇÈñãÊîæÂéüÂßãÁ¢º HALO ÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttps://github.com/ResponsibleAILab/HALO„ÄÇ

##### **Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System**
2409.09968v1 by Raffi Hagopian, Timothy Strebel, Simon Bernatz, Gregory A Myers, Erik Offerman, Eric Zuniga, Cy Y Kim, Angie T Ng, James A Iwaz, Sunny P Singh, Evan P Carey, Michael J Kim, R Spencer Schaefer, Jeannie Yu, Amilcare Gentili, Hugo JWL Aerts

Coronary artery calcium (CAC) is highly predictive of cardiovascular events.
While millions of chest CT scans are performed annually in the United States,
CAC is not routinely quantified from scans done for non-cardiac purposes. A
deep learning algorithm was developed using 446 expert segmentations to
automatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our
study differs from prior works as we leverage imaging data across the Veterans
Affairs national healthcare system, from 98 medical centers, capturing
extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC
performance on non-gated scans was compared against clinical standard ECG-gated
CAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than
100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and
87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a
year of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year
all-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <
0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:
33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052
patients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)
individuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT
images from a random sample of >400 AI-CAC patients and verified that 527/531
(99.2%) would benefit from lipid-lowering therapy. To the best of our
knowledge, this is the first non-gated CT CAC algorithm developed across a
national healthcare system, on multiple imaging protocols, without filtering
intra-cardiac hardware, and compared against a strong gated CT reference. We
report superior performance relative to previous CAC algorithms evaluated
against paired gated scans that included patients with intra-cardiac hardware.

ÊëòË¶ÅÔºöÂÜ†ÁãÄÂãïËÑàÈà£Âåñ (CAC) Ê•µÂÖ∑È†êÊ∏¨ÂøÉË°ÄÁÆ°‰∫ã‰ª∂ÁöÑËÉΩÂäõ„ÄÇ
ÈõñÁÑ∂ÁæéÂúãÊØèÂπ¥ÈÄ≤Ë°åÊï∏ÁôæËê¨Ê¨°ËÉ∏ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºå
‰ΩÜÈùûÂøÉËáüÁõÆÁöÑÊéÉÊèèÈÄöÂ∏∏‰∏çÊúÉÂ∞ç CAC ÈÄ≤Ë°åÈáèÂåñ„ÄÇ‰∏Ä
ÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ï‰ΩøÁî® 446 ÂÄãÂ∞àÂÆ∂ÂàÜÊÆµÈñãÁôºÔºå‰ª•
Âú®ÈùûÂ∞çÊØî„ÄÅÈùûÈñÄÊéßÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (AI-CAC) ‰∏äËá™ÂãïÈáèÂåñ CAC„ÄÇÊàëÂÄëÁöÑ
Á†îÁ©∂ËàáÂÖàÂâçÁöÑÂ∑•‰Ωú‰∏çÂêåÔºåÂõ†ÁÇ∫ÊàëÂÄëÂà©Áî®ÈÄÄ‰ºçËªç‰∫∫‰∫ãÂãôÈÉ®ÂÖ®ÂúãÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰∏≠‰æÜËá™ 98 ÂÄãÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑÂΩ±ÂÉèË≥áÊñôÔºåÊçïÊçâ
ÂΩ±ÂÉèÂçîÂÆö„ÄÅÊéÉÊèèÂô®ÂíåÊÇ£ËÄÖÁöÑÂª£Ê≥õÁï∞Ë≥™ÊÄß„ÄÇAI-CAC
Âú®ÈùûÈñÄÊéßÊéÉÊèè‰∏äÁöÑË°®ÁèæËàáËá®Â∫äÊ®ôÊ∫ñ ECG ÈñÄÊéß
CAC Ë©ïÂàÜÈÄ≤Ë°åÊØîËºÉ„ÄÇÈùûÈñÄÊéß AI-CAC ÂçÄÂàÜÈõ∂ËàáÈùûÈõ∂Ôºå‰ª•Âèä‰ΩéÊñº
100 Ëàá 100 ÊàñÊõ¥È´òÁöÑ Agatston ÂàÜÊï∏ÔºåÂú®‰∏ÄÂπ¥ÂÖßÈÄ≤Ë°åÈÖçÂ∞çÈñÄÊéßÊéÉÊèèÁöÑ 795 ÂêçÊÇ£ËÄÖ‰∏≠Ê∫ñÁ¢∫ÁéáÂàÜÂà•ÁÇ∫ 89.4% (F1 0.93) Âíå
87.3% (F1 0.89)„ÄÇÈùûÈñÄÊéß AI-CAC ÂèØÈ†êÊ∏¨ 10 Âπ¥ÂÖ®Âõ†Ê≠ª‰∫°Áéá (CAC 0 Â∞çÊØî >400 Áæ§ÁµÑÔºö25.4% Â∞çÊØî 60.2%ÔºåCox HR 3.49Ôºåp <
0.005)Ôºå‰ª•ÂèäÈ¶ñÊ¨°Ë§áÂêàÊÄß‰∏≠È¢®„ÄÅÂøÉËÇåÊ¢óÂ°ûÊàñÊ≠ª‰∫° (CAC 0 Â∞çÊØî >400 Áæ§ÁµÑÔºö
33.5% Â∞çÊØî 63.8%ÔºåCox HR 3.00Ôºåp < 0.005)„ÄÇÂú® 8,052 ÂêçÊé•Âèó‰ΩéÂäëÈáèËÇ∫ÁôåÁØ©Ê™¢ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (LDCT) ÁöÑÊÇ£ËÄÖÁöÑÁØ©Ê™¢Ë≥áÊñôÈõÜ‰∏≠Ôºå3,091/8,052 (38.4%)
ÂÄãÈ´îÁöÑ AI-CAC >400„ÄÇÂõõ‰ΩçÂøÉËáüÁóÖÂ∞àÂÆ∂Â∞çÈö®Ê©üÊäΩÂèñÁöÑ >400 AI-CAC ÊÇ£ËÄÖÁöÑ LDCT
ÂΩ±ÂÉèÈÄ≤Ë°åË≥™ÊÄßÂØ©Êü•Ôºå‰∏¶È©óË≠â 527/531
(99.2%) Â∞áÂèóÁõäÊñºÈôçË°ÄËÑÇÊ≤ªÁôÇ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈùûÈñÄÊéßÈõªËÖ¶Êñ∑Â±§ CAC ÊºîÁÆóÊ≥ïÔºåÂú®ÂÖ®ÂúãÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰∏≠ÈñãÁôºÔºåÊé°Áî®Â§öÁ®ÆÂΩ±ÂÉèÂçîÂÆöÔºå‰∏çÁØ©ÈÅ∏
ÂøÉÂÖßÁ°¨È´îÔºå‰∏¶ËàáÂº∑Â§ßÁöÑÈñÄÊéßÈõªËÖ¶Êñ∑Â±§ÂèÉËÄÉÈÄ≤Ë°åÊØîËºÉ„ÄÇÊàëÂÄë
Â†±ÂëäÁöÑÊïàËÉΩÂÑ™ÊñºÂÖàÂâçÈáùÂ∞çÂåÖÂê´ÂøÉÂÖßÁ°¨È´îÊÇ£ËÄÖÁöÑÈÖçÂ∞çÈñÄÊéßÊéÉÊèèÈÄ≤Ë°åË©ï‰º∞ÁöÑ CAC ÊºîÁÆóÊ≥ï„ÄÇ

##### **GP-GPT: Large Language Model for Gene-Phenotype Mapping**
2409.09825v1 by Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu

Pre-trained large language models(LLMs) have attracted increasing attention
in biomedical domains due to their success in natural language processing.
However, the complex traits and heterogeneity of multi-sources genomics data
pose significant challenges when adapting these models to the bioinformatics
and biomedical field. To address these challenges, we present GP-GPT, the first
specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a
comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,
and medical genetics, derived from multiple large-scale validated datasets and
scientific publications. GP-GPT demonstrates proficiency in accurately
retrieving medical genetics information and performing common genomics analysis
tasks, such as genomics information retrieval and relationship determination.
Comparative experiments across domain-specific tasks reveal that GP-GPT
outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These
results highlight GP-GPT's potential to enhance genetic disease relation
research and facilitate accurate and efficient analysis in the fields of
genomics and medical genetics. Our investigation demonstrated the subtle
changes of bio-factor entities' representations in the GP-GPT, which suggested
the opportunities for the application of LLMs to advancing gene-phenotype
research.

ÊëòË¶ÅÔºöÈ†êÂÖàË®ìÁ∑¥Â•ΩÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊñπÈù¢ÂèñÂæóÊàêÂäüÔºåÂõ†Ê≠§Âú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüü‰∏≠ÂÇôÂèóÈóúÊ≥®„ÄÇ
ÁÑ∂ËÄåÔºåÂ§ö‰æÜÊ∫êÂü∫Âõ†ÁµÑÊï∏ÊìöÁöÑË§áÈõúÁâπÂæµÂíåÁï∞Ë≥™ÊÄßÂú®Â∞áÈÄô‰∫õÊ®°ÂûãÊáâÁî®ÊñºÁîüÁâ©Ë≥áË®äÂ≠∏ÂíåÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÊôÇÔºåÊßãÊàê‰∫ÜÈáçÂ§ßÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GP-GPTÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÁî®ÊñºÈÅ∫ÂÇ≥Ë°®ÂûãÁü•Ë≠òË°®ÂæµÂíåÂü∫Âõ†ÁµÑÈóú‰øÇÂàÜÊûêÁöÑÂ∞àÊ•≠Â§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂàÜÂÖ©ÂÄãÈöéÊÆµÈÄ≤Ë°åÂæÆË™øÔºå‰∏ÄÂÄãÁ∂úÂêàË™ûÊñôÂ∫´ÂåÖÂê´Ë∂ÖÈÅé 3,000,000 ÂÄãÂü∫Âõ†ÁµÑÂ≠∏„ÄÅËõãÁôΩË≥™ÁµÑÂ≠∏ÂíåÈÜ´Â≠∏ÈÅ∫ÂÇ≥Â≠∏‰∏≠ÁöÑË°ìË™ûÔºåÈÄô‰∫õË°ìË™û‰æÜËá™Â§öÂÄãÁ∂ìÈÅéÈ©óË≠âÁöÑÂ§ßË¶èÊ®°Êï∏ÊìöÈõÜÂíåÁßëÂ≠∏Âá∫ÁâàÁâ©„ÄÇGP-GPT È°ØÁ§∫Âá∫Ê∫ñÁ¢∫Êì∑ÂèñÈÜ´Â≠∏ÈÅ∫ÂÇ≥Â≠∏Ë≥áË®äÂíåÂü∑Ë°åÂ∏∏Ë¶ãÂü∫Âõ†ÁµÑÂàÜÊûê‰ªªÂãôÔºà‰æãÂ¶ÇÂü∫Âõ†ÁµÑË≥áË®äÊì∑ÂèñÂíåÈóú‰øÇÁ¢∫ÂÆöÔºâÁöÑËÉΩÂäõ„ÄÇË∑®È†òÂüüÁâπÂÆö‰ªªÂãôÁöÑÊØîËºÉÂØ¶È©óÈ°ØÁ§∫ÔºåGP-GPT ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºåÂåÖÊã¨ Llama2„ÄÅLlama3 Âíå GPT-4„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÂá∫‰∫Ü GP-GPT Âú®Âä†Âº∑ÈÅ∫ÂÇ≥ÁñæÁóÖÈóú‰øÇÁ†îÁ©∂‰ª•Âèä‰øÉÈÄ≤Âü∫Âõ†ÁµÑÂ≠∏ÂíåÈÜ´Â≠∏ÈÅ∫ÂÇ≥Â≠∏È†òÂüü‰∏≠Ê∫ñÁ¢∫ËÄåÊúâÊïàÂàÜÊûêÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑË™øÊü•Ë≠âÊòé‰∫Ü GP-GPT ‰∏≠ÁîüÁâ©Âõ†Â≠êÂØ¶È´îË°®ÂæµÁöÑÁ¥∞ÂæÆËÆäÂåñÔºåÈÄôË°®Êòé‰∫ÜÂ∞á LLM ÊáâÁî®ÊñºÊé®ÈÄ≤Âü∫Âõ†Ë°®ÂûãÁ†îÁ©∂ÁöÑÊ©üÊúÉ„ÄÇ

##### **Veridical Data Science for Medical Foundation Models**
2409.10580v1 by Ahmed Alaa, Bin Yu

The advent of foundation models (FMs) such as large language models (LLMs)
has led to a cultural shift in data science, both in medicine and beyond. This
shift involves moving away from specialized predictive models trained for
specific, well-defined domain questions to generalist FMs pre-trained on vast
amounts of unstructured data, which can then be adapted to various clinical
tasks and questions. As a result, the standard data science workflow in
medicine has been fundamentally altered; the foundation model lifecycle (FMLC)
now includes distinct upstream and downstream processes, in which computational
resources, model and data access, and decision-making power are distributed
among multiple stakeholders. At their core, FMs are fundamentally statistical
models, and this new workflow challenges the principles of Veridical Data
Science (VDS), hindering the rigorous statistical analysis expected in
transparent and scientifically reproducible data science practices. We
critically examine the medical FMLC in light of the core principles of VDS:
predictability, computability, and stability (PCS), and explain how it deviates
from the standard data science workflow. Finally, we propose recommendations
for a reimagined medical FMLC that expands and refines the PCS principles for
VDS including considering the computational and accessibility constraints
inherent to FMs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á≠âÂü∫Á§éÊ®°Âûã (FM) ÁöÑÂá∫ÁèæÔºåÂ∞éËá¥‰∫ÜË≥áÊñôÁßëÂ≠∏ÁöÑÊñáÂåñËΩâËÆäÔºåÁÑ°Ë´ñÊòØÂú®ÈÜ´Â≠∏È†òÂüüÊàñÂÖ∂‰ªñÈ†òÂüü„ÄÇÈÄôÂÄãËΩâËÆäÊ∂âÂèäÂæûÈáùÂ∞çÁâπÂÆö„ÄÅÂÆöÁæ©ÊòéÁ¢∫ÁöÑÈ†òÂüüÂïèÈ°åË®ìÁ∑¥ÁöÑÂ∞àÈñÄÈ†êÊ∏¨Ê®°ÂûãÔºåËΩâÁßªÂà∞È†êÂÖàÂú®Â§ßÈáèÈùûÁµêÊßãÂåñË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ≥õÁî®Âûã FMÔºåÁÑ∂ÂæåÂèØ‰ª•Ë™øÊï¥ÈÄô‰∫õ FM ‰ª•ÈÅ©ÊáâÂêÑÁ®ÆËá®Â∫ä‰ªªÂãôÂíåÂïèÈ°å„ÄÇÂõ†Ê≠§ÔºåÈÜ´Â≠∏‰∏≠ÁöÑÊ®ôÊ∫ñË≥áÊñôÁßëÂ≠∏Â∑•‰ΩúÊµÅÁ®ãÂ∑≤Á∂ìÁôºÁîü‰∫ÜÊ†πÊú¨ÊÄßÁöÑÊîπËÆäÔºõÂü∫Á§éÊ®°ÂûãÁîüÂëΩÈÄ±Êúü (FMLC) ÁèæÂú®ÂåÖÊã¨‰∏çÂêåÁöÑ‰∏äÊ∏∏Âíå‰∏ãÊ∏∏ÊµÅÁ®ãÔºåÂÖ∂‰∏≠ÈÅãÁÆóË≥áÊ∫ê„ÄÅÊ®°ÂûãÂíåË≥áÊñôÂ≠òÂèñÔºå‰ª•ÂèäÊ±∫Á≠ñÊ¨äÂäõÊúÉÂàÜÈÖçÁµ¶Â§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫„ÄÇÂæûÊú¨Ë≥™‰∏ä‰æÜË™™ÔºåFM Âü∫Êú¨‰∏äÊòØÁµ±Ë®àÊ®°ÂûãÔºåËÄåÈÄôÂÄãÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÊåëÊà∞‰∫ÜÁúüÂØ¶Ë≥áÊñôÁßëÂ≠∏ (VDS) ÁöÑÂéüÂâáÔºåÈòªÁ§ô‰∫ÜÈÄèÊòé‰∏îÁßëÂ≠∏‰∏äÂèØË§áË£ΩÁöÑË≥áÊñôÁßëÂ≠∏ÂØ¶Âãô‰∏≠ÊâÄÈ†êÊúüÁöÑÂö¥Ë¨πÁµ±Ë®àÂàÜÊûê„ÄÇÊàëÂÄëÊ†πÊìö VDS ÁöÑÊ†∏ÂøÉÂéüÂâáÔºöÂèØÈ†êÊ∏¨ÊÄß„ÄÅÂèØÈÅãÁÆóÊÄßÂíåÁ©©ÂÆöÊÄß (PCS) ‰æÜÊâπÂà§ÊÄßÂú∞Ê™¢Ë¶ñÈÜ´Â≠∏ FMLCÔºå‰∏¶Ë™™ÊòéÂÆÉÂ¶Ç‰ΩïÂÅèÈõ¢Ê®ôÊ∫ñÁöÑË≥áÊñôÁßëÂ≠∏Â∑•‰ΩúÊµÅÁ®ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈáçÊñ∞ÊßãÊÉ≥ÈÜ´Â≠∏ FMLC ÁöÑÂª∫Ë≠∞Ôºå‰ª•Êì¥ÂÖÖÂíåÂÆåÂñÑÈÅ©Áî®Êñº VDS ÁöÑ PCS ÂéüÂâáÔºåÂåÖÊã¨ËÄÉÈáè FM Âõ∫ÊúâÁöÑÈÅãÁÆóÂíåÂèØÂ≠òÂèñÊÄßÈôêÂà∂„ÄÇ

##### **From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**
2409.09727v1 by Ming Li, Pengcheng Xu, Junjie Hu, Zeyu Tang, Guang Yang

Federated learning holds great potential for enabling large-scale healthcare
research and collaboration across multiple centres while ensuring data privacy
and security are not compromised. Although numerous recent studies suggest or
utilize federated learning based methods in healthcare, it remains unclear
which ones have potential clinical utility. This review paper considers and
analyzes the most recent studies up to May 2024 that describe federated
learning based methods in healthcare. After a thorough review, we find that the
vast majority are not appropriate for clinical use due to their methodological
flaws and/or underlying biases which include but are not limited to privacy
concerns, generalization issues, and communication costs. As a result, the
effectiveness of federated learning in healthcare is significantly compromised.
To overcome these challenges, we provide recommendations and promising
opportunities that might be implemented to resolve these problems and improve
the quality of model development in federated learning with healthcare.

ÊëòË¶ÅÔºöËÅØÈÇ¶Â≠∏ÁøíÂú®Á¢∫‰øùË≥áÊñôÈö±ÁßÅÂíåÂÆâÂÖ®‰∏çËá¥ÂèóÊêçÁöÑÊÉÖÊ≥Å‰∏ãÔºåÁÇ∫Â§ßÂûãÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂ÂíåË∑®Â§öÂÄã‰∏≠ÂøÉÂêà‰ΩúÊèê‰æõ‰∫ÜÂ∑®Â§ßÊΩõÂäõ„ÄÇÂÑòÁÆ°Ë®±Â§öÊúÄËøëÁöÑÁ†îÁ©∂Âª∫Ë≠∞ÊàñÂà©Áî®Âü∫ÊñºËÅØÈÇ¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÈÄ≤Ë°åÈÜ´ÁôÇ‰øùÂÅ•Ôºå‰ΩÜÂì™‰∫õÂÖ∑ÊúâÊΩõÂú®ÁöÑËá®Â∫äÊïàÁî®‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨Ë©ïË´ñÊñáÁ´†ËÄÉÊÖÆ‰∏¶ÂàÜÊûê‰∫ÜÊà™Ëá≥ 2024 Âπ¥ 5 ÊúàÊèèËø∞Âü∫ÊñºËÅØÈÇ¶Â≠∏ÁøíÊñπÊ≥ïÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÊúÄÊñ∞Á†îÁ©∂„ÄÇÂú®ÂæπÂ∫ïÊ™¢Èñ±ÂæåÔºåÊàëÂÄëÁôºÁèæÁµïÂ§ßÂ§öÊï∏‰∏çÈÅ©ÂêàËá®Â∫ä‰ΩøÁî®ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂ≠òÂú®ÊñπÊ≥ïË´ñÁº∫Èô∑Âíå/ÊàñÊΩõÂú®ÂÅèÂ∑ÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñºÈö±ÁßÅÂïèÈ°å„ÄÅÊ¶ÇÂåñÂïèÈ°åÂíåÈÄöË®äÊàêÊú¨„ÄÇÂõ†Ê≠§ÔºåËÅØÈÇ¶Â≠∏ÁøíÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊïàÂäõÂèóÂà∞È°ØËëóÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂª∫Ë≠∞ÂíåÊúâÂ∏åÊúõÁöÑÊ©üÊúÉÔºåÈÄô‰∫õÊ©üÊúÉÂèØËÉΩÊúÉË¢´ÂØ¶ÊñΩ‰ª•Ëß£Ê±∫ÈÄô‰∫õÂïèÈ°å‰∏¶ÊèêÈ´òÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ËÅØÈÇ¶Â≠∏ÁøíÊ®°ÂûãÈñãÁôºÁöÑÂìÅË≥™„ÄÇ

##### **ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**
2409.09662v2 by Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim

Expressing stressful experiences in words is proven to improve mental and
physical health, but individuals often disengage with writing interventions as
they struggle to organize their thoughts and emotions. Reflective prompts have
been used to provide direction, and large language models (LLMs) have
demonstrated the potential to provide tailored guidance. Current systems often
limit users' flexibility to direct their reflections. We thus present
ExploreSelf, an LLM-driven application designed to empower users to control
their reflective journey. ExploreSelf allows users to receive adaptive support
through dynamically generated questions. Through an exploratory study with 19
participants, we examine how participants explore and reflect on personal
challenges using ExploreSelf. Our findings demonstrate that participants valued
the balance between guided support and freedom to control their reflective
journey, leading to deeper engagement and insight. Building on our findings, we
discuss implications for designing LLM-driven tools that promote user
empowerment through effective reflective practices.

ÊëòË¶ÅÔºöÂ∑≤Ë≠âÂØ¶Áî®Ë®ÄË™ûË°®ÈÅîÂ£ìÂäõÁ∂ìÈ©óÊúâÂä©ÊñºÊîπÂñÑÂøÉÁêÜÂíåË∫´È´îÂÅ•Â∫∑Ôºå‰ΩÜÂÄã‰∫∫Â∏∏Â∏∏ÊîæÊ£ÑÂØ´‰Ωú‰ªãÂÖ•ÔºåÂõ†ÁÇ∫‰ªñÂÄëÂú®Êï¥ÁêÜÊÄùÁ∑íÂíåÊÉÖÁ∑íÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£„ÄÇÂèçÊÄùÊèêÁ§∫Â∑≤Ë¢´Áî®‰æÜÊèê‰æõÊñπÂêëÔºåËÄåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë≠âÊòéÊúâÊèê‰æõÂÆ¢Ë£ΩÂåñÊåáÂ∞éÁöÑÊΩõÂäõ„ÄÇÁõÆÂâçÁöÑÁ≥ªÁµ±ÈÄöÂ∏∏ÊúÉÈôêÂà∂‰ΩøÁî®ËÄÖÂºïÂ∞éÂÖ∂ÂèçÊÄùÁöÑÈùàÊ¥ªÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü ExploreSelfÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± LLM È©ÖÂãïÁöÑÊáâÁî®Á®ãÂºèÔºåÊó®Âú®ÊéàÊ¨ä‰ΩøÁî®ËÄÖÊéßÂà∂ÂÖ∂ÂèçÊÄùÊóÖÁ®ã„ÄÇExploreSelf ÂÖÅË®±‰ΩøÁî®ËÄÖÈÄèÈÅéÂãïÊÖãÁî¢ÁîüÁöÑÂïèÈ°å‰æÜÊé•Êî∂ÈÅ©ÊáâÊÄßÊîØÊè¥„ÄÇÈÄèÈÅé‰∏ÄÈ†ÖËàá 19 ‰ΩçÂèÉËàáËÄÖÈÄ≤Ë°åÁöÑÊé¢Á¥¢ÊÄßÁ†îÁ©∂ÔºåÊàëÂÄëÊé¢Ë®éÂèÉËàáËÄÖÂ¶Ç‰Ωï‰ΩøÁî® ExploreSelf ‰æÜÊé¢Á¥¢ÂíåÂèçÊÄùÂÄã‰∫∫ÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂèÉËàáËÄÖÈáçË¶ñÂºïÂ∞éÂºèÊîØÊè¥ËàáÊéßÂà∂ÂÖ∂ÂèçÊÄùÊóÖÁ®ãÁöÑËá™Áî±‰πãÈñìÁöÑÂπ≥Ë°°ÔºåÈÄôÊúÉÂ∏∂‰æÜÊõ¥Ê∑±ÂÖ•ÁöÑÂèÉËàáÂíåÊ¥ûÂØüÂäõ„ÄÇÊ†πÊìöÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÔºåÊàëÂÄëË®éË´ñ‰∫ÜË®≠Ë®à LLM È©ÖÂãïÂ∑•ÂÖ∑ÁöÑÂê´ÊÑèÔºåÈÄô‰∫õÂ∑•ÂÖ∑ÈÄèÈÅéÊúâÊïàÁöÑÂèçÊÄùÂØ¶Âãô‰øÉÈÄ≤‰ΩøÁî®ËÄÖË≥¶Ê¨ä„ÄÇ

##### **MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences**
2409.09570v1 by Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Michael V. Heinz, Ashmita Kunwar, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Sarah M. Preum, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell

Mental health concerns are prevalent among college students, highlighting the
need for effective interventions that promote self-awareness and holistic
well-being. MindScape pioneers a novel approach to AI-powered journaling by
integrating passively collected behavioral patterns such as conversational
engagement, sleep, and location with Large Language Models (LLMs). This
integration creates a highly personalized and context-aware journaling
experience, enhancing self-awareness and well-being by embedding behavioral
intelligence into AI. We present an 8-week exploratory study with 20 college
students, demonstrating the MindScape app's efficacy in enhancing positive
affect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and
depression, with a significant week-over-week decrease in PHQ-4 scores (-0.25
coefficient), alongside improvements in mindfulness (7%) and self-reflection
(6%). The study highlights the advantages of contextual AI journaling, with
participants particularly appreciating the tailored prompts and insights
provided by the MindScape app. Our analysis also includes a comparison of
responses to AI-driven contextual versus generic prompts, participant feedback
insights, and proposed strategies for leveraging contextual AI journaling to
improve well-being on college campuses. By showcasing the potential of
contextual AI journaling to support mental health, we provide a foundation for
further investigation into the effects of contextual AI journaling on mental
health and well-being.

ÊëòË¶ÅÔºöÂ§ßÂ≠∏ÁîüÊôÆÈÅçÊúâÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÔºåÂº∑Ë™øÈúÄË¶ÅÊúâÊïàÂπ≤È†êÊé™ÊñΩ‰æÜ‰øÉÈÄ≤Ëá™ÊàëË¶∫ÂØüÂíåÊï¥È´îÁ¶èÁ•â„ÄÇMindScape ÈñãÂâµ‰∫Ü AI È©ÖÂãïÊó•Ë™åÁöÑÊñ∞ÊñπÊ≥ïÔºåÊñπÊ≥ïÊòØÂ∞áË¢´ÂãïÊî∂ÈõÜÁöÑË°åÁÇ∫Ê®°ÂºèÔºà‰æãÂ¶ÇÂ∞çË©±ÂèÉËàá„ÄÅÁù°Áú†Âíå‰ΩçÁΩÆÔºâËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂú®‰∏ÄËµ∑„ÄÇÈÄôÁ®ÆÊï¥ÂêàÂâµÈÄ†‰∫ÜÈ´òÂ∫¶ÂÄã‰∫∫Âåñ‰∏îÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ËÉΩÂäõÁöÑÊó•Ë™åÈ´îÈ©óÔºåÈÄèÈÅéÂ∞áË°åÁÇ∫Êô∫ÊÖßÂµåÂÖ• AI ‰æÜÂ¢ûÂº∑Ëá™ÊàëË¶∫ÂØüÂíåÁ¶èÁ•â„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÁÇ∫Êúü 8 ÈÄ±ÁöÑÊé¢Á¥¢ÊÄßÁ†îÁ©∂ÔºåÊúâ 20 ÂêçÂ§ßÂ≠∏ÁîüÂèÉËàáÔºåË≠âÊòé MindScape ÊáâÁî®Á®ãÂºèÂú®Â¢ûÂº∑Ê≠£Èù¢ÂΩ±Èüø (7%)„ÄÅÊ∏õÂ∞ëË≤†Èù¢ÂΩ±Èüø (11%)„ÄÅÂ≠§Áç®ÊÑü (6%) ‰ª•ÂèäÁÑ¶ÊÖÆÂíåÊÜÇÈ¨±ÊñπÈù¢ÊúâÊïàÔºåPHQ-4 ÂàÜÊï∏ÈÄ±ÈÄ±È°ØËëó‰∏ãÈôç (-0.25 ‰øÇÊï∏)ÔºåÂêåÊôÇÊ≠£Âøµ (7%) ÂíåËá™ÊàëÂèçÁúÅ (6%) ‰πüÊúâÊâÄÊîπÂñÑ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÊÉÖÂ¢É AI Êó•Ë™åÁöÑÂÑ™ÈªûÔºåÂèÉËàáËÄÖÁâπÂà•Ê¨£Ë≥û MindScape ÊáâÁî®Á®ãÂºèÊèê‰æõÁöÑÂÆ¢Ë£ΩÂåñÊèêÁ§∫ÂíåË¶ãËß£„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈÇÑÂåÖÊã¨ÊØîËºÉÂ∞ç AI È©ÖÂãïÊÉÖÂ¢ÉÊèêÁ§∫Âíå‰∏ÄËà¨ÊèêÁ§∫ÁöÑÂõûÊáâ„ÄÅÂèÉËàáËÄÖÂõûÈ•ãË¶ãËß£Ôºå‰ª•ÂèäÊèêÂá∫Âà©Áî®ÊÉÖÂ¢É AI Êó•Ë™å‰æÜÊîπÂñÑÂ§ßÂ≠∏Ê†°ÂúíÁ¶èÁ•âÁöÑÁ≠ñÁï•„ÄÇÈÄèÈÅéÂ±ïÁ§∫ÊÉÖÂ¢É AI Êó•Ë™åÂú®ÊîØÊè¥ÂøÉÁêÜÂÅ•Â∫∑ÁöÑÊΩõÂäõÔºåÊàëÂÄëÁÇ∫ÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÊÉÖÂ¢É AI Êó•Ë™åÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÂíåÁ¶èÁ•âÁöÑÂΩ±ÈüøÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare**
2409.09549v1 by Chia-Hao Li, Niraj K. Jha

Wearable medical sensors (WMSs) are revolutionizing smart healthcare by
enabling continuous, real-time monitoring of user physiological signals,
especially in the field of consumer healthcare. The integration of WMSs and
modern machine learning (ML) enables unprecedented solutions to efficient
early-stage disease detection. Despite the success of Transformers in various
fields, their application to sensitive domains, such as smart healthcare,
remains underexplored due to limited data accessibility and privacy concerns.
To bridge the gap between Transformer-based foundation models and WMS-based
disease detection, we propose COMFORT, a continual fine-tuning framework for
foundation models targeted at consumer healthcare. COMFORT introduces a novel
approach for pre-training a Transformer-based foundation model on a large
dataset of physiological signals exclusively collected from healthy individuals
with commercially available WMSs. We adopt a masked data modeling (MDM)
objective to pre-train this health foundation model. We then fine-tune the
model using various parameter-efficient fine-tuning (PEFT) methods, such as
low-rank adaptation (LoRA) and its variants, to adapt it to various downstream
disease detection tasks that rely on WMS data. In addition, COMFORT continually
stores the low-rank decomposition matrices obtained from the PEFT algorithms to
construct a library for multi-disease detection. The COMFORT library enables
scalable and memory-efficient disease detection on edge devices. Our
experimental results demonstrate that COMFORT achieves highly competitive
performance while reducing memory overhead by up to 52% relative to
conventional methods. Thus, COMFORT paves the way for personalized and
proactive solutions to efficient and effective early-stage disease detection
for consumer healthcare.

ÊëòË¶ÅÔºö<paragraph>ÂèØÁ©øÊà¥ÂºèÈÜ´ÁôÇÊÑüÊ∏¨Âô® (WMS) ÈÄèÈÅéÊåÅÁ∫å„ÄÅÂç≥ÊôÇÁõ£Ê∏¨‰ΩøÁî®ËÄÖÁöÑÁîüÁêÜË®äËôüÔºåÁâπÂà•ÊòØÂú®Ê∂àË≤ªËÄÖÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÈÄ≤ËÄåÈù©Êñ∞‰∫ÜÊô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•„ÄÇWMS ËàáÁèæ‰ª£Ê©üÂô®Â≠∏Áøí (ML) ÁöÑÊï¥ÂêàÔºåËÆìÊúâÊïàÁéáÁöÑÊó©ÊúüÁñæÁóÖÂÅµÊ∏¨Êúâ‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂÑòÁÆ° Transformer Âú®ÂêÑÁ®ÆÈ†òÂüüÁöÜÁç≤ÂæóÊàêÂäüÔºå‰ΩÜÁî±ÊñºË≥áÊñôÂèñÂæó‰∏çÊòìÂíåÈö±ÁßÅÁñëÊÖÆÔºåÂÖ∂Âú®Êô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•Á≠âÊïèÊÑüÈ†òÂüüÁöÑÊáâÁî®‰ªçÊúâÂæÖÊé¢Á¥¢„ÄÇÁÇ∫‰∫ÜÂΩåÂêà Transformer Âü∫Á§éÊ®°ÂûãËàá WMS Âü∫Á§éÁñæÁóÖÂÅµÊ∏¨‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü COMFORTÔºå‰∏ÄÂÄãÈáùÂ∞çÊ∂àË≤ªËÄÖÈÜ´ÁôÇ‰øùÂÅ•ËÄåË®≠Ë®àÁöÑÂü∫Á§éÊ®°ÂûãÊåÅÁ∫åÂæÆË™øÊû∂Êßã„ÄÇCOMFORT ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂèØÂú®‰∏ÄÂÄãÈæêÂ§ßÁöÑÁîüÁêÜË®äËôüË≥áÊñôÈõÜ‰∏äÈ†êË®ìÁ∑¥ Transformer Âü∫Á§éÊ®°ÂûãÔºåËÄåÈÄô‰∫õË≥áÊñôÁöÜÊòØÈÄèÈÅéÂ∏ÇÂîÆ WMS ÂæûÂÅ•Â∫∑ÂÄã‰∫∫Ë∫´‰∏äÊî∂ÈõÜËÄå‰æÜ„ÄÇÊàëÂÄëÊé°Áî®ÈÅÆÁΩ©Ë≥áÊñôÂª∫Ê®° (MDM) ÁõÆÊ®ô‰æÜÈ†êË®ìÁ∑¥ÈÄôÂÄãÂÅ•Â∫∑Âü∫Á§éÊ®°Âûã„ÄÇÊé•ËëóÔºåÊàëÂÄë‰ΩøÁî®ÂêÑÁ®ÆÂèÉÊï∏ÊúâÊïàÁéáÁöÑÂæÆË™ø (PEFT) ÊñπÊ≥ïÔºà‰æãÂ¶Ç‰ΩéÁß©ÈÅ©Êáâ (LoRA) ÂèäÂÖ∂ËÆäÈ´îÔºâÂæÆË™øÊ®°ÂûãÔºå‰ª•‰ΩøÂÖ∂ÈÅ©Êáâ‰æùË≥¥ WMS Ë≥áÊñôÁöÑÂêÑÁ®Æ‰∏ãÊ∏∏ÁñæÁóÖÂÅµÊ∏¨‰ªªÂãô„ÄÇÊ≠§Â§ñÔºåCOMFORT ÊúÉÊåÅÁ∫åÂÑ≤Â≠òÂæû PEFT ÊºîÁÆóÊ≥ïÂèñÂæóÁöÑ‰ΩéÁß©ÂàÜËß£Áü©Èô£Ôºå‰ª•Âª∫Êßã‰∏ÄÂÄãÂ§öÁñæÁóÖÂÅµÊ∏¨ÂáΩÂºèÂ∫´„ÄÇCOMFORT ÂáΩÂºèÂ∫´ÂèØÂú®ÈÇäÁ∑£Ë£ùÁΩÆ‰∏äÈÄ≤Ë°åÂèØÊì¥ÂÖÖ‰∏îË®òÊÜ∂È´î‰ΩøÁî®Áéá‰Ωé‰∏ãÁöÑÁñæÁóÖÂÅµÊ∏¨„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåCOMFORT ÈÅîÂà∞‰∫ÜÊ•µÂÖ∑Á´∂Áà≠ÂäõÁöÑÊïàËÉΩÔºåÂêåÊôÇÂ∞áË®òÊÜ∂È´îÈñãÈä∑Áõ∏ËºÉÊñºÂÇ≥Áµ±ÊñπÊ≥ïÈôç‰Ωé‰∫Ü 52%„ÄÇÂõ†Ê≠§ÔºåCOMFORT ÁÇ∫Ê∂àË≤ªËÄÖÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÊúâÊïà‰∏îÈ´òÊïàÊó©ÊúüÁñæÁóÖÂÅµÊ∏¨ÔºåÈñãÈó¢‰∫ÜÂÄã‰∫∫Âåñ‰∏î‰∏ªÂãïÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ</paragraph>

##### **Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment**
2409.09520v1 by Xin Hu, Janet Wang, Jihun Hamm, Rie R Yotsu, Zhengming Ding

Current AI-assisted skin image diagnosis has achieved dermatologist-level
performance in classifying skin cancer, driven by rapid advancements in deep
learning architectures. However, unlike traditional vision tasks, skin images
in general present unique challenges due to the limited availability of
well-annotated datasets, complex variations in conditions, and the necessity
for detailed interpretations to ensure patient safety. Previous segmentation
methods have sought to reduce image noise and enhance diagnostic performance,
but these techniques require fine-grained, pixel-level ground truth masks for
training. In contrast, with the rise of foundation models, the Segment Anything
Model (SAM) has been introduced to facilitate promptable segmentation, enabling
the automation of the segmentation process with simple yet effective prompts.
Efforts applying SAM predominantly focus on dermatoscopy images, which present
more easily identifiable lesion boundaries than clinical photos taken with
smartphones. This limitation constrains the practicality of these approaches to
real-world applications. To overcome the challenges posed by noisy clinical
photos acquired via non-standardized protocols and to improve diagnostic
accessibility, we propose a novel Cross-Attentive Fusion framework for
interpretable skin lesion diagnosis. Our method leverages SAM to generate
visual concepts for skin diseases using prompts, integrating local visual
concepts with global image features to enhance model performance. Extensive
evaluation on two skin disease datasets demonstrates our proposed method's
effectiveness on lesion diagnosis and interpretability.

ÊëòË¶ÅÔºöÁõÆÂâçÁî± AI ËºîÂä©ÁöÑÁöÆËÜöÂΩ±ÂÉèË®∫Êñ∑Â∑≤Âú®ÁöÆËÜöÁôåÂàÜÈ°û‰∏≠ÈÅîÂà∞ÁöÆËÜöÁßëÈÜ´Â∏´Á≠âÁ¥öÁöÑË°®ÁèæÔºåÈÄôÊ≠∏ÂäüÊñºÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÁöÑÂø´ÈÄüÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåËàáÂÇ≥Áµ±ÁöÑË¶ñË¶∫‰ªªÂãô‰∏çÂêåÔºå‰∏ÄËà¨ÁöÆËÜöÂΩ±ÂÉèÁî±ÊñºÊ®ôË®ªËâØÂ•ΩÁöÑË≥áÊñôÈõÜÂèñÂæó‰∏çÊòì„ÄÅÁãÄÊ≥ÅË§áÈõúÂ§öËÆäÔºå‰ª•ÂèäÁ¢∫‰øùÊÇ£ËÄÖÂÆâÂÖ®ÊâÄÈúÄÁöÑË©≥Á¥∞Ë©ÆÈáãÔºåÂõ†Ê≠§ÂëàÁèæÂá∫Áç®ÁâπÁöÑÊåëÊà∞„ÄÇÂÖàÂâçÁöÑÂàÜÂâ≤ÊñπÊ≥ïË©¶ÂúñÈôç‰ΩéÂΩ±ÂÉèÈõúË®ä‰∏¶ÊèêÂçáË®∫Êñ∑Ë°®ÁèæÔºå‰ΩÜÈÄô‰∫õÊäÄË°ìÈúÄË¶ÅÁ¥∞Á∑ªÁöÑÁï´Á¥†Á¥öÂú∞Èù¢ÂØ¶Ê≥ÅÈÅÆÁΩ©‰æÜË®ìÁ∑¥„ÄÇÁõ∏Â∞çÂú∞ÔºåÈö®ËëóÂü∫Á§éÊ®°ÂûãÁöÑËààËµ∑ÔºåÂ∑≤Â∞éÂÖ• Segment Anything Model (SAM) ‰ª•Âà©ÊñºÊèêÁ§∫ÂºèÂàÜÂâ≤Ôºå‰ΩøÁî®Á∞°ÂñÆÂçªÊúâÊïàÁöÑÊèêÁ§∫Ëá™ÂãïÂåñÂàÜÂâ≤ÊµÅÁ®ã„ÄÇÊáâÁî® SAM ÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠ÊñºÁöÆËÜöÈè°ÂΩ±ÂÉèÔºåÂÖ∂ÁóÖÁÅ∂ÈÇäÁïåÊØî‰ΩøÁî®Êô∫ÊÖßÂûãÊâãÊ©üÊãçÊîùÁöÑËá®Â∫äÁÖßÁâáÊõ¥ÂÆπÊòìËæ®Ë≠ò„ÄÇÊ≠§ÈôêÂà∂ÊúÉÁ¥ÑÊùüÈÄô‰∫õÊñπÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÂØ¶Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈùûÊ®ôÊ∫ñÂåñÁ®ãÂ∫èÂèñÂæóÁöÑÈõúË®äËá®Â∫äÁÖßÁâáÊâÄÈÄ†ÊàêÁöÑÊåëÊà∞Ôºå‰∏¶ÊîπÂñÑË®∫Êñ∑ÁöÑÂèØËøëÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑË∑®Ê≥®ÊÑèÂäõËûçÂêàÊû∂ÊßãÔºåÁî®ÊñºÂèØË©ÆÈáãÁöÑÁöÆËÜöÁóÖÁÅ∂Ë®∫Êñ∑„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂà©Áî® SAM ‰ΩøÁî®ÊèêÁ§∫‰æÜÁî¢ÁîüÁöÆËÜöÁñæÁóÖÁöÑË¶ñË¶∫Ê¶ÇÂøµÔºåÂ∞áÂ±ÄÈÉ®Ë¶ñË¶∫Ê¶ÇÂøµËàáÊï¥È´îÂΩ±ÂÉèÁâπÂæµÊï¥ÂêàÔºå‰ª•ÊèêÂçáÊ®°ÂûãË°®Áèæ„ÄÇÂú®ÂÖ©ÂÄãÁöÆËÜöÁñæÁóÖË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õË©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÁóÖÁÅ∂Ë®∫Êñ∑ÂíåÂèØË©ÆÈáãÊÄß‰∏äÈÉΩÂÖ∑ÊúâÊàêÊïà„ÄÇ

##### **Synthetic4Health: Generating Annotated Synthetic Clinical Letters**
2409.09501v1 by Libo Ren, Samuel Belkadi, Lifeng Han, Warren Del-Pinto, Goran Nenadic

Since clinical letters contain sensitive information, clinical-related
datasets can not be widely applied in model training, medical research, and
teaching. This work aims to generate reliable, various, and de-identified
synthetic clinical letters. To achieve this goal, we explored different
pre-trained language models (PLMs) for masking and generating text. After that,
we worked on Bio\_ClinicalBERT, a high-performing model, and experimented with
different masking strategies. Both qualitative and quantitative methods were
used for evaluation. Additionally, a downstream task, Named Entity Recognition
(NER), was also implemented to assess the usability of these synthetic letters.
  The results indicate that 1) encoder-only models outperform encoder-decoder
models. 2) Among encoder-only models, those trained on general corpora perform
comparably to those trained on clinical data when clinical information is
preserved. 3) Additionally, preserving clinical entities and document structure
better aligns with our objectives than simply fine-tuning the model. 4)
Furthermore, different masking strategies can impact the quality of synthetic
clinical letters. Masking stopwords has a positive impact, while masking nouns
or verbs has a negative effect. 5) For evaluation, BERTScore should be the
primary quantitative evaluation metric, with other metrics serving as
supplementary references. 6) Contextual information does not significantly
impact the models' understanding, so the synthetic clinical letters have the
potential to replace the original ones in downstream tasks.

ÊëòË¶ÅÔºöÁî±ÊñºËá®Â∫ä‰ø°‰ª∂ÂåÖÂê´ÊïèÊÑüË≥áË®äÔºåÂõ†Ê≠§ËàáËá®Â∫äÁõ∏ÈóúÁöÑË≥áÊñôÈõÜÁÑ°Ê≥ïÂª£Ê≥õÊáâÁî®ÊñºÊ®°ÂûãË®ìÁ∑¥„ÄÅÈÜ´Â≠∏Á†îÁ©∂ÂíåÊïôÂ≠∏‰∏≠„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Áî¢ÁîüÂèØÈù†„ÄÅÂ§öÊ®£‰∏îÂéªË≠òÂà•ÂåñÁöÑÂêàÊàêËá®Â∫ä‰ø°‰ª∂„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÊ≠§ÁõÆÊ®ôÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏çÂêåÁöÑÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ‰æÜÈÅÆËîΩÂíåÁî¢ÁîüÊñáÂ≠ó„ÄÇ‰πãÂæåÔºåÊàëÂÄë‰ΩøÁî®È´òÊÄßËÉΩÊ®°Âûã Bio_ClinicalBERTÔºå‰∏¶ÈáùÂ∞ç‰∏çÂêåÁöÑÈÅÆËîΩÁ≠ñÁï•ÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄë‰ΩøÁî®ÂÆöÊÄßÂíåÂÆöÈáèÊñπÊ≥ïÈÄ≤Ë°åË©ï‰º∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰πüÂØ¶‰Ωú‰∫Ü‰∏ãÊ∏∏‰ªªÂãôÔºåÂç≥ÂëΩÂêçÂØ¶È´îË≠òÂà• (NER) ‰æÜË©ï‰º∞ÈÄô‰∫õÂêàÊàê‰ø°‰ª∂ÁöÑÂèØÁî®ÊÄß„ÄÇÁµêÊûúÈ°ØÁ§∫Ôºö1) Á∑®Á¢ºÂô®Â∞àÁî®Ê®°ÂûãÂÑ™ÊñºÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ê®°Âûã„ÄÇ2) Âú®Á∑®Á¢ºÂô®Â∞àÁî®Ê®°Âûã‰∏≠ÔºåÁï∂‰øùÁïôËá®Â∫äË≥áË®äÊôÇÔºåÂú®‰∏ÄËà¨Ë™ûÊñôÂ∫´‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãË°®ÁèæËàáÂú®Ëá®Â∫äË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÁõ∏Áï∂„ÄÇ3) Ê≠§Â§ñÔºå‰øùÁïôËá®Â∫äÂØ¶È´îÂíåÊñá‰ª∂ÁµêÊßãÊØîÂñÆÁ¥îÂæÆË™øÊ®°ÂûãÊõ¥Á¨¶ÂêàÊàëÂÄëÁöÑÁõÆÊ®ô„ÄÇ4) Ê≠§Â§ñÔºå‰∏çÂêåÁöÑÈÅÆËîΩÁ≠ñÁï•ÊúÉÂΩ±ÈüøÂêàÊàêËá®Â∫ä‰ø°‰ª∂ÁöÑÂìÅË≥™„ÄÇÈÅÆËîΩÂÅúÊ≠¢Ë©ûÊúâÊ≠£Èù¢ÁöÑÂΩ±ÈüøÔºåËÄåÈÅÆËîΩÂêçË©ûÊàñÂãïË©ûÊúâË≤†Èù¢ÁöÑÂΩ±Èüø„ÄÇ5) Â∞çÊñºË©ï‰º∞ÔºåBERTScore ÊáâÁÇ∫‰∏ªË¶ÅÁöÑÂÆöÈáèË©ï‰º∞ÊåáÊ®ôÔºåÂÖ∂‰ªñÊåáÊ®ô‰ΩúÁÇ∫Ë£úÂÖÖÂèÉËÄÉ„ÄÇ6) ËÉåÊôØË≥áË®ä‰∏çÊúÉÈ°ØËëóÂΩ±ÈüøÊ®°ÂûãÁöÑÁêÜËß£ÔºåÂõ†Ê≠§ÂêàÊàêËá®Â∫ä‰ø°‰ª∂ÊúâÊΩõÂäõÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠Âèñ‰ª£ÂéüÂßã‰ø°‰ª∂„ÄÇ

##### **From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging**
2409.09478v1 by Maximilian Rokuss, Balint Kovacs, Yannick Kirchhoff, Shuhan Xiao, Constantin Ulrich, Klaus H. Maier-Hein, Fabian Isensee

Automated lesion segmentation in PET/CT scans is crucial for improving
clinical workflows and advancing cancer diagnostics. However, the task is
challenging due to physiological variability, different tracers used in PET
imaging, and diverse imaging protocols across medical centers. To address this,
the autoPET series was created to challenge researchers to develop algorithms
that generalize across diverse PET/CT environments. This paper presents our
solution for the autoPET III challenge, targeting multitracer, multicenter
generalization using the nnU-Net framework with the ResEncL architecture. Key
techniques include misalignment data augmentation and multi-modal pretraining
across CT, MR, and PET datasets to provide an initial anatomical understanding.
We incorporate organ supervision as a multitask approach, enabling the model to
distinguish between physiological uptake and tracer-specific patterns, which is
particularly beneficial in cases where no lesions are present. Compared to the
default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL
(65.31) our model significantly improved performance with a Dice score of
68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative
(FNvol: 10.35) volumes. These results underscore the effectiveness of combining
advanced network design, augmentation, pretraining, and multitask learning for
PET/CT lesion segmentation. Code is publicly available at
https://github.com/MIC-DKFZ/autopet-3-submission.

ÊëòË¶ÅÔºö<paragraph>Ëá™ÂãïÂåñÁóÖÁÅ∂ÂàÜÂâ≤Âú® PET/CT ÊéÉÊèè‰∏≠Â∞çÊñºÊîπÂñÑËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÂíå‰øÉÈÄ≤ÁôåÁóáË®∫Êñ∑Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁîüÁêÜËÆäÁï∞„ÄÅPET ÂΩ±ÂÉè‰∏≠‰ΩøÁî®ÁöÑ‰∏çÂêåËøΩËπ§ÂäëÔºå‰ª•ÂèäÂêÑÈÜ´ÁôÇ‰∏≠ÂøÉ‰∏çÂêåÁöÑÂΩ±ÂÉèÂçîÂÆöÔºåÈÄôÈ†Ö‰ªªÂãôÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåautoPET Á≥ªÂàóË≥ΩÂâµÁ´ãÔºåÊåëÊà∞Á†îÁ©∂‰∫∫Âì°ÈñãÁôºÂú®ÂêÑÁ®Æ PET/CT Áí∞Â¢É‰∏≠ÈÄöÁî®ÁöÑÊºîÁÆóÊ≥ï„ÄÇÊú¨ÊñáÊèêÂá∫ÊàëÂÄëÈáùÂ∞ç autoPET III ÊåëÊà∞ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁõÆÊ®ôÊòØ‰ΩøÁî®ÂÖ∑Êúâ ResEncL Êû∂ÊßãÁöÑ nnU-Net Ê°ÜÊû∂ÈÄ≤Ë°åÂ§öËøΩËπ§Âäë„ÄÅÂ§ö‰∏≠ÂøÉÊé®Âª£„ÄÇÈóúÈçµÊäÄË°ìÂåÖÊã¨ÈåØ‰ΩçË≥áÊñôÊì¥ÂÖÖÂíåË∑® CT„ÄÅMR Âíå PET Ë≥áÊñôÈõÜÁöÑÂ§öÊ®°ÂºèÈ†êË®ìÁ∑¥Ôºå‰ª•Êèê‰æõÂàùÊ≠•ÁöÑËß£ÂâñÁêÜËß£„ÄÇÊàëÂÄëÂ∞áÂô®ÂÆòÁõ£Áù£Á¥çÂÖ•Â§ö‰ªªÂãôÊñπÊ≥ïÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†ÂçÄÂàÜÁîüÁêÜÊîùÂèñÂíåËøΩËπ§ÂäëÁâπÁï∞ÊÄßÊ®°ÂºèÔºåÈÄôÂú®Ê≤íÊúâÁóÖÁÅ∂ÁöÑÊÉÖÊ≥Å‰∏ãÁâπÂà•ÊúâÁõä„ÄÇËàáÈÅîÊàê Dice ÂàÜÊï∏ 57.61 ÁöÑÈ†êË®≠ nnU-NetÔºåÊàñÊõ¥Â§ßÁöÑ ResEncL (65.31) Áõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ§ßÂπÖÊîπÂñÑ‰∫ÜÊïàËÉΩÔºåDice ÂàÜÊï∏ÁÇ∫ 68.40ÔºåÂêåÊôÇÊ∏õÂ∞ë‰∫ÜÂÅáÈôΩÊÄß (FPvolÔºö7.82) ÂíåÂÅáÈô∞ÊÄß (FNvolÔºö10.35) È´îÁ©ç„ÄÇÈÄô‰∫õÁµêÊûúÂº∑Ë™ø‰∫ÜÁµêÂêàÂÖàÈÄ≤Á∂≤Ë∑ØË®≠Ë®à„ÄÅÊì¥ÂÖÖ„ÄÅÈ†êË®ìÁ∑¥ÂíåÂ§ö‰ªªÂãôÂ≠∏ÁøíÂ∞ç PET/CT ÁóÖÁÅ∂ÂàÜÂâ≤ÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÁôºÂ∏ÉÊñº https://github.com/MIC-DKFZ/autopet-3-submission„ÄÇ</paragraph>

##### **Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking**
2409.10570v1 by Cong Kong, Rui Xu, Weixi Chen, Jiawei Chen, Zhaoxia Yin

Pre-training language models followed by fine-tuning on specific tasks is
standard in NLP, but traditional models often underperform when applied to the
medical domain, leading to the development of specialized medical pre-trained
language models (Med-PLMs). These models are valuable assets but are vulnerable
to misuse and theft, requiring copyright protection. However, no existing
watermarking methods are tailored for Med-PLMs, and adapting general PLMs
watermarking techniques to the medical domain faces challenges such as task
incompatibility, loss of fidelity, and inefficiency. To address these issues,
we propose the first training-free backdoor watermarking method for Med-PLMs.
Our method uses rare special symbols as trigger words, which do not impact
downstream task performance, embedding watermarks by replacing their original
embeddings with those of specific medical terms in the Med-PLMs' word
embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical
downstream tasks, the final models (FMs) respond to the trigger words in the
same way they would to the corresponding medical terms. This property can be
utilized to extract the watermark. Experiments demonstrate that our method
achieves high fidelity while effectively extracting watermarks across various
medical downstream tasks. Additionally, our method demonstrates robustness
against various attacks and significantly enhances the efficiency of watermark
embedding, reducing the embedding time from 10 hours to 10 seconds.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºåÂæåÁ∫åÈáùÂ∞çÁâπÂÆö‰ªªÂãôÈÄ≤Ë°åÂæÆË™øÔºåÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÊòØÊ®ôÊ∫ñ‰ΩúÊ•≠Ôºå‰ΩÜÂÇ≥Áµ±Ê®°ÂûãÊáâÁî®ÊñºÈÜ´ÁôÇÈ†òÂüüÊôÇÔºåÂæÄÂæÄË°®Áèæ‰∏ç‰Ω≥ÔºåÂ∞éËá¥Â∞àÈñÄÁöÑÈÜ´ÁôÇÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (Med-PLM) ÊáâÈÅãËÄåÁîü„ÄÇÈÄô‰∫õÊ®°ÂûãÊòØÂØ∂Ë≤¥ÁöÑË≥áÁî¢Ôºå‰ΩÜÂÆπÊòìÈÅ≠Âà∞Êø´Áî®ÂíåÁ´äÂèñÔºåÈúÄË¶ÅÁâàÊ¨ä‰øùË≠∑„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊµÆÊ∞¥Âç∞ÊñπÊ≥ï‰∏¶Êú™ÈáùÂ∞ç Med-PLM ÈáèË∫´ÊâìÈÄ†ÔºåËÄåÂ∞á‰∏ÄËà¨ PLM ÊµÆÊ∞¥Âç∞ÊäÄË°ìË™øÊï¥ÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÊôÇÔºåÂâáÈù¢Ëá®‰ªªÂãô‰∏çÁõ∏ÂÆπ„ÄÅ‰øùÁúüÂ∫¶‰∏ãÈôçÂíåÊïàÁéá‰∏çÂΩ∞Á≠âÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫Á¨¨‰∏ÄÂÄãÈáùÂ∞ç Med-PLM ÁöÑÂÖçË®ìÁ∑¥ÂæåÈñÄÊµÆÊ∞¥Âç∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØ‰ΩøÁî®ÁΩïË¶ãÁöÑÁâπÊÆäÁ¨¶Ëôü‰ΩúÁÇ∫Ëß∏ÁôºÂ≠óÔºåÈÄô‰∏çÊúÉÂΩ±Èüø‰∏ãÊ∏∏‰ªªÂãôÁöÑË°®ÁèæÔºå‰∏¶ÈÄèÈÅéÂ∞áÂÖ∂ÂéüÂßãÂµåÂÖ•ÊõøÊèõÁÇ∫ Med-PLM Â≠óÂµåÂÖ•Â±§‰∏≠ÁâπÂÆöÈÜ´ÁôÇË°ìË™ûÁöÑÂµåÂÖ•Ôºå‰æÜÂµåÂÖ•ÊµÆÊ∞¥Âç∞„ÄÇÂú®ÈáùÂ∞çÂêÑÁ®ÆÈÜ´ÁôÇ‰∏ãÊ∏∏‰ªªÂãôÂæÆË™øÂ∏∂ÊúâÊµÆÊ∞¥Âç∞ÁöÑ Med-PLM ‰πãÂæåÔºåÊúÄÁµÇÊ®°Âûã (FM) Â∞çËß∏ÁôºÂ≠óÁöÑÂõûÊáâÊñπÂºèËàáÂ∞çÊáâÈÜ´ÁôÇË°ìË™ûÁõ∏Âêå„ÄÇÊ≠§ÁâπÊÄßÂèØÁî®ÊñºÊèêÂèñÊµÆÊ∞¥Âç∞„ÄÇÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®ÂêÑÁ®ÆÈÜ´ÁôÇ‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÔºåÈÉΩËÉΩÊúâÊïàÊèêÂèñÊµÆÊ∞¥Âç∞ÔºåÂêåÊôÇÁ∂≠ÊåÅÈ´ò‰øùÁúüÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁèæÂá∫Â∞çÂêÑÁ®ÆÊîªÊìäÁöÑÂº∑ÂÅ•ÊÄßÔºå‰∏¶Â§ßÂπÖÊèêÂçáÊµÆÊ∞¥Âç∞ÂµåÂÖ•ÁöÑÊïàÁéáÔºåÂ∞áÂµåÂÖ•ÊôÇÈñìÂæû 10 Â∞èÊôÇÁ∏ÆÁü≠ÁÇ∫ 10 Áßí„ÄÇ

##### **Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation**
2409.09324v1 by Hui Yi Leong, Yi Fan Gao, Ji Shuai, Uktu Pamuksuz

Scientific research indicates that for every hour spent in direct patient
care, physicians spend nearly two additional hours on administrative tasks,
particularly on electronic health records (EHRs) and desk work. This excessive
administrative burden not only reduces the time available for patient care but
also contributes to physician burnout and inefficiencies in healthcare
delivery. To address these challenges, this study introduces MediGen, a
fine-tuned large language model (LLM) designed to automate the generation of
medical reports from medical dialogues. By leveraging state-of-the-art
methodologies for fine-tuning open-source pretrained models, including
LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing
clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising
results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating
its effectiveness in generating accurate and clinically relevant medical
reports. These findings suggest that MediGen has the potential to significantly
reduce the administrative workload on physicians, improving both healthcare
efficiency and physician well-being.

ÊëòË¶ÅÔºöÁßëÂ≠∏Á†îÁ©∂ÊåáÂá∫ÔºåÈÜ´Â∏´ÊØèËä±‰∏ÄÂÄãÂ∞èÊôÇÈÄ≤Ë°åÁõ¥Êé•ÁöÑÁóÖÊÇ£ÁÖßË≠∑ÔºåÂ∞±ÊúÉËä±Ë≤ªÂ∞áËøëÂÖ©ÂÄãÂ∞èÊôÇÂú®Ë°åÊîø‰∫ãÂãô‰∏äÔºåÁâπÂà•ÊòØÂú®ÈõªÂ≠êÁóÖÊ≠∑ (EHR) ÂíåÊñáÊõ∏Â∑•‰Ωú‰∏ä„ÄÇÈÄôÁ®ÆÈÅéÂ∫¶ÁöÑË°åÊîøË≤†Êìî‰∏çÂÉÖÊ∏õÂ∞ë‰∫ÜÂèØËä±Âú®ÁóÖÊÇ£ÁÖßË≠∑‰∏äÁöÑÊôÇÈñìÔºå‰πüÂ∞éËá¥ÈÜ´Â∏´ÂÄ¶ÊÄ†ÂíåÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõÊïàÁéá‰∏çÂΩ∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü MediGenÔºå‰∏ÄÂÄãÁ∂ìÈÅéÂæÆË™øÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÊó®Âú®Ëá™ÂãïÂåñÂæûÈÜ´ÁôÇÂ∞çË©±‰∏≠ÁîüÊàêÈÜ´ÁôÇÂ†±Âëä„ÄÇÈÄèÈÅéÂà©Áî®ÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÂæÆË™øÈñãÊ∫êÈ†êË®ìÁ∑¥Ê®°ÂûãÔºåÂåÖÊã¨ LLaMA3-8BÔºåMediGen Âú®ËΩâÈåÑÂíåÁ∏ΩÁµêËá®Â∫ä‰∫íÂãïÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÁ∂ìÈÅéÂæÆË™øÁöÑ LLaMA3-8B Ê®°ÂûãÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÈÅîÂà∞‰∫Ü 58% ÁöÑ ROUGE ÂàÜÊï∏Âíå 72% ÁöÑ BERTScore-F1ÔºåÈÄôË°®Á§∫ÂÆÉÂú®ÁîüÊàêÊ∫ñÁ¢∫‰∏îËá®Â∫ä‰∏äÁõ∏ÈóúÁöÑÈÜ´ÁôÇÂ†±ÂëäÊñπÈù¢ÂæàÊúâÊïà„ÄÇÈÄô‰∫õÁôºÁèæË°®Êòé MediGen ÊúâÂèØËÉΩÂ§ßÂπÖÊ∏õÂ∞ëÈÜ´Â∏´ÁöÑË°åÊîøÂ∑•‰ΩúÈáèÔºåÈÄ≤ËÄåÊîπÂñÑÈÜ´ÁôÇ‰øùÂÅ•ÊïàÁéáÂíåÈÜ´Â∏´ÁöÑÁ¶èÁ•â„ÄÇ

##### **On the limits of agency in agent-based models**
2409.10568v1 by Ayush Chopra, Shashank Kumar, Nurullah Giray-Kuru, Ramesh Raskar, Arnau Quera-Bofarull

Agent-based modeling (ABM) seeks to understand the behavior of complex
systems by simulating a collection of agents that act and interact within an
environment. Their practical utility requires capturing realistic environment
dynamics and adaptive agent behavior while efficiently simulating million-size
populations. Recent advancements in large language models (LLMs) present an
opportunity to enhance ABMs by using LLMs as agents with further potential to
capture adaptive behavior. However, the computational infeasibility of using
LLMs for large populations has hindered their widespread adoption. In this
paper, we introduce AgentTorch -- a framework that scales ABMs to millions of
agents while capturing high-resolution agent behavior using LLMs. We benchmark
the utility of LLMs as ABM agents, exploring the trade-off between simulation
scale and individual agency. Using the COVID-19 pandemic as a case study, we
demonstrate how AgentTorch can simulate 8.4 million agents representing New
York City, capturing the impact of isolation and employment behavior on health
and economic outcomes. We compare the performance of different agent
architectures based on heuristic and LLM agents in predicting disease waves and
unemployment rates. Furthermore, we showcase AgentTorch's capabilities for
retrospective, counterfactual, and prospective analyses, highlighting how
adaptive agent behavior can help overcome the limitations of historical data in
policy design. AgentTorch is an open-source project actively being used for
policy-making and scientific discovery around the world. The framework is
available here: github.com/AgentTorch/AgentTorch.

ÊëòË¶ÅÔºö<paragraph>Âü∫Êñº‰ª£ÁêÜÁöÑÂª∫Ê®° (ABM) ÈÄèÈÅéÊ®°Êì¨‰∏ÄÁæ§‰ª£ÁêÜ‰∫∫ÔºåÂú®Áí∞Â¢É‰∏≠Ë°åÁÇ∫‰∏¶‰∫íÂãïÔºå‰ª•‰∫ÜËß£Ë§áÈõúÁ≥ªÁµ±ÁöÑË°åÁÇ∫„ÄÇÂÆÉÂÄëÁöÑÂØ¶Áî®ÊÄßÈúÄË¶ÅÊçïÊçâÁúüÂØ¶ÁöÑÁí∞Â¢ÉÂãïÊÖãÂíåÈÅ©ÊáâÊÄß‰ª£ÁêÜË°åÁÇ∫ÔºåÂêåÊôÇÊúâÊïàÁéáÂú∞Ê®°Êì¨Êï∏ÁôæËê¨Ë¶èÊ®°ÁöÑÊóèÁæ§„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊèê‰æõ‰∫ÜÊ©üÊúÉÔºåÂèØ‰ª•ÈÄèÈÅé‰ΩøÁî® LLM ‰ΩúÁÇ∫‰ª£ÁêÜ‰∫∫‰æÜÂ¢ûÂº∑ ABMÔºåÈÄ≤ËÄåÊúâÊΩõÂäõÊçïÊçâÈÅ©ÊáâÊÄßË°åÁÇ∫„ÄÇÁÑ∂ËÄåÔºåÂ∞á LLM Áî®ÊñºÂ§ßÂûãÊóèÁæ§ÁöÑË®àÁÆó‰∏çÂèØË°åÊÄßÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÊé°Áî®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü AgentTorchÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞á ABM Êì¥Â±ïÂà∞Êï∏ÁôæËê¨ÂÄã‰ª£ÁêÜ‰∫∫ÔºåÂêåÊôÇ‰ΩøÁî® LLM ÊçïÊçâÈ´òËß£ÊûêÂ∫¶‰ª£ÁêÜ‰∫∫Ë°åÁÇ∫ÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëË©ïÈáè LLM ‰ΩúÁÇ∫ ABM ‰ª£ÁêÜ‰∫∫ÁöÑÊïàÁî®ÔºåÊé¢Á¥¢Ê®°Êì¨Ë¶èÊ®°ÂíåÂÄã‰∫∫‰ª£ÁêÜ‰πãÈñìÁöÑÊ¨äË°°„ÄÇ‰ΩøÁî® COVID-19 Â§ßÊµÅË°å‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü AgentTorch Â¶Ç‰ΩïÊ®°Êì¨‰ª£Ë°®Á¥êÁ¥ÑÂ∏ÇÁöÑ 840 Ëê¨ÂÄã‰ª£ÁêÜ‰∫∫ÔºåÊçïÊçâÈöîÈõ¢ÂíåÂ∞±Ê•≠Ë°åÁÇ∫Â∞çÂÅ•Â∫∑ÂíåÁ∂ìÊøüÊàêÊûúÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂü∫ÊñºÂïüÁôºÂºèÂíå LLM ‰ª£ÁêÜ‰∫∫ÁöÑ‰∏çÂêå‰ª£ÁêÜ‰∫∫Êû∂ÊßãÂú®È†êÊ∏¨ÁñæÁóÖÊµ™ÊΩÆÂíåÂ§±Ê•≠ÁéáÊñπÈù¢ÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü AgentTorch Âú®ÂõûÈ°ßÊÄß„ÄÅÂèç‰∫ãÂØ¶ÂíåÂâçÁûªÊÄßÂàÜÊûêÊñπÈù¢ÁöÑÂäüËÉΩÔºåÂº∑Ë™øÈÅ©ÊáâÊÄß‰ª£ÁêÜË°åÁÇ∫Â¶Ç‰ΩïÂπ´Âä©ÂÖãÊúçÊîøÁ≠ñË®≠Ë®à‰∏≠Ê≠∑Âè≤Ë≥áÊñôÁöÑÈôêÂà∂„ÄÇAgentTorch ÊòØÁ©çÊ•µÁî®ÊñºÂÖ®ÁêÉÊîøÁ≠ñÂà∂ÂÆöÂíåÁßëÂ≠∏ÁôºÁèæÁöÑÈñãÊ∫êÂ∞àÊ°à„ÄÇÊ≠§Ê°ÜÊû∂ÂèØÂú®ÈÄôË£°ÂèñÂæóÔºögithub.com/AgentTorch/AgentTorch„ÄÇ</paragraph>

##### **Contextual Evaluation of Large Language Models for Classifying Tropical and Infectious Diseases**
2409.09201v1 by Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Katherine Heller

While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇÂïèÈ°åÂõûÁ≠îÊñπÈù¢Â∑≤Â±ïÁèæÊΩõÂäõÔºå‰ΩÜÂ∞àÊ≥®ÊñºÁÜ±Â∏∂ÂíåÂÇ≥ÊüìÁóÖÁâπÂÆöÊé¢Á¥¢ÁöÑÂ∑•‰ΩúÂçªÊúâÈôê„ÄÇÊàëÂÄëÂª∫Á´ãÂú®ÈñãÊ∫êÁÜ±Â∏∂ÂíåÂÇ≥ÊüìÁóÖ (TRINDs) Ë≥áÊñôÈõÜ‰∏äÔºå‰∏¶Â∞áÂÖ∂Êì¥ÂÖÖ‰ª•Á¥çÂÖ•‰∫∫Âè£Áµ±Ë®àÂíåË™ûÁæ©Ëá®Â∫äÂíåÊ∂àË≤ªËÄÖÊì¥ÂÖÖÔºåÁî¢ÁîüË∂ÖÈÅé 11000 ÂÄãÊèêÁ§∫„ÄÇÊàëÂÄëË©ï‰º∞ÈÄô‰∫õ LLM ÁöÑÊïàËÉΩÔºåÊØîËºÉÈÄöÊâçÂíåÈÜ´ÁôÇ LLMÔºå‰ª•Âèä LLM ÁµêÊûúËàá‰∫∫È°ûÂ∞àÂÆ∂„ÄÇÊàëÂÄëÈÄèÈÅéÁ≥ªÁµ±ÊÄßÂØ¶È©óÔºåË≠âÊòé‰∫ÜËÉåÊôØË≥áË®äÔºà‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®à„ÄÅ‰ΩçÁΩÆ„ÄÅÊÄßÂà•„ÄÅÊúÄ‰Ω≥ LLM ÂõûÊáâÁöÑÈ¢®Èö™Âõ†Á¥†ÔºâÁöÑÂ•ΩËôï„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã TRINDs-LM ÂéüÂûãÔºåÈÄôÊòØ‰∏ÄÂÄãÁ†îÁ©∂Â∑•ÂÖ∑ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊé¢Á¥¢ËÉåÊôØÂ¶Ç‰ΩïÂΩ±ÈüøÂÅ•Â∫∑ LLM Ëº∏Âá∫ÁöÑÈÅäÊ®ÇÂ†¥„ÄÇ

##### **Phikon-v2, A large and public feature extractor for biomarker prediction**
2409.09173v1 by Alexandre Filiot, Paul Jacob, Alice Mac Kain, Charlie Saillard

Gathering histopathology slides from over 100 publicly available cohorts, we
compile a diverse dataset of 460 million pathology tiles covering more than 30
cancer sites. Using this dataset, we train a large self-supervised vision
transformer using DINOv2 and publicly release one iteration of this model for
further experimentation, coined Phikon-v2. While trained on publicly available
histology slides, Phikon-v2 surpasses our previously released model (Phikon)
and performs on par with other histopathology foundation models (FM) trained on
proprietary data. Our benchmarks include eight slide-level tasks with results
reported on external validation cohorts avoiding any data contamination between
pre-training and evaluation datasets. Our downstream training procedure follows
a simple yet robust ensembling strategy yielding a +1.75 AUC increase across
tasks and models compared to one-shot retraining (p<0.001). We compare Phikon
(ViT-B) and Phikon-v2 (ViT-L) against 14 different histology feature
extractors, making our evaluation the most comprehensive to date. Our result
support evidences that DINOv2 handles joint model and data scaling better than
iBOT. Also, we show that recent scaling efforts are overall beneficial to
downstream performance in the context of biomarker prediction with GigaPath and
H-Optimus-0 (two ViT-g with 1.1B parameters each) standing out. However, the
statistical margins between the latest top-performing FMs remain mostly
non-significant; some even underperform on specific indications or tasks such
as MSI prediction - deposed by a 13x smaller model developed internally. While
latest foundation models may exhibit limitations for clinical deployment, they
nonetheless offer excellent grounds for the development of more specialized and
cost-efficient histology encoders fueling AI-guided diagnostic tools.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂæû 100 Â§öÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑÁæ§ÁµÑ‰∏≠Êî∂ÈõÜ‰∫ÜÁµÑÁπîÁóÖÁêÜÂ≠∏ÂπªÁáàÁâáÔºåÁ∑®Âà∂‰∫Ü‰∏ÄÂÄãÂåÖÂê´ 4.6 ÂÑÑÂÄãÁóÖÁêÜÂàáÁâáÁöÑ‰∏çÂêåË≥áÊñôÈõÜÔºåÊ∂µËìã‰∫Ü 30 Â§öÂÄãÁôåÁóáÈÉ®‰Ωç„ÄÇ‰ΩøÁî®Ê≠§Ë≥áÊñôÈõÜÔºåÊàëÂÄë‰ΩøÁî® DINOv2 Ë®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÂ§ßÂûãËá™Áõ£Áù£Ë¶ñË¶∫ËΩâÊèõÂô®Ôºå‰∏¶ÂÖ¨ÈñãÁôºÂ∏ÉÊ≠§Ê®°ÂûãÁöÑÂÖ∂‰∏≠‰∏ÄÂÄãÁâàÊú¨Ôºå‰ª•‰æõÈÄ≤‰∏ÄÊ≠•ÂØ¶È©óÔºåÁ®±ÁÇ∫ Phikon-v2„ÄÇÈõñÁÑ∂Âú®ÂÖ¨ÈñãÂèØÁî®ÁöÑÁµÑÁπîÂàáÁâá‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ΩÜ Phikon-v2 Ë∂ÖË∂ä‰∫ÜÊàëÂÄëÂÖàÂâçÁôºÂ∏ÉÁöÑÊ®°Âûã (Phikon)Ôºå‰∏¶‰∏îËàáÂú®Â∞àÊúâË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂÖ∂‰ªñÁµÑÁπîÁóÖÁêÜÂ≠∏Âü∫Á§éÊ®°Âûã (FM) ÊïàËÉΩÁõ∏Áï∂„ÄÇÊàëÂÄëÁöÑÂü∫Ê∫ñÂåÖÊã¨ÂÖ´È†ÖÂπªÁáàÁâáÂ±§Á¥ö‰ªªÂãôÔºåÂÖ∂ÁµêÊûúÂú®Â§ñÈÉ®È©óË≠âÁæ§ÁµÑ‰∏≠Â†±ÂëäÔºåÈÅøÂÖçÈ†êË®ìÁ∑¥ÂíåË©ï‰º∞Ë≥áÊñôÈõÜ‰πãÈñìÁöÑ‰ªª‰ΩïË≥áÊñôÊ±°Êüì„ÄÇÊàëÂÄëÁöÑ‰∏ãÊ∏∏Ë®ìÁ∑¥Á®ãÂ∫èÊé°Áî®Á∞°ÂñÆ‰ΩÜÁ©©ÂÅ•ÁöÑÊï¥È´îÁ≠ñÁï•ÔºåËàá‰∏ÄÊ¨°ÊÄßÈáçÊñ∞Ë®ìÁ∑¥Áõ∏ÊØîÔºå‰ªªÂãôÂíåÊ®°ÂûãÁöÑ AUC Â¢ûÂä† +1.75Ôºàp<0.001Ôºâ„ÄÇÊàëÂÄëÂ∞á Phikon (ViT-B) Âíå Phikon-v2 (ViT-L) Ëàá 14 Á®Æ‰∏çÂêåÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏ÁâπÂæµËêÉÂèñÂô®ÈÄ≤Ë°åÊØîËºÉÔºå‰ΩøÊàëÂÄëÁöÑË©ï‰º∞ÊàêÁÇ∫ËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂÖ®Èù¢ÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÊîØÊåÅË≠âÊìöË°®ÊòéÔºåDINOv2 ÊØî iBOT Êõ¥ËÉΩËôïÁêÜËÅØÂêàÊ®°ÂûãÂíåË≥áÊñôÁ∏ÆÊîæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË°®ÊòéÔºåÂú®‰ΩøÁî® GigaPath Âíå H-Optimus-0ÔºàÂÖ©ÂÄãÂÖ∑Êúâ 1.1B ÂèÉÊï∏ÁöÑ ViT-gÔºâÈÄ≤Ë°åÁîüÁâ©Ê®ôË®òÈ†êÊ∏¨ÁöÑËÉåÊôØ‰∏ãÔºåÊúÄËøëÁöÑÁ∏ÆÊîæÂ∑•‰ΩúÂ∞ç‰∏ãÊ∏∏ÊïàËÉΩÊï¥È´îÊúâÁõä„ÄÇÁÑ∂ËÄåÔºåÊúÄÊñ∞ÊïàËÉΩÊúÄ‰Ω≥ÁöÑ FM ‰πãÈñìÁöÑÁµ±Ë®àÈÇäÈöõÂ§ßÂ§ö‰ªçÁÑ∂‰∏çÈ°ØËëóÔºõÊúâ‰∫õÁîöËá≥Âú®ÁâπÂÆöÈÅ©ÊáâÁóáÊàñ‰ªªÂãôÔºà‰æãÂ¶Ç MSI È†êÊ∏¨Ôºâ‰∏äË°®Áèæ‰∏ç‰Ω≥ÔºåË¢´‰∏ÄÂÄãÁî±ÂÖßÈÉ®ÈñãÁôºÁöÑ 13 ÂÄçËºÉÂ∞èÊ®°ÂûãÊâÄÂèñ‰ª£„ÄÇÈõñÁÑ∂ÊúÄÊñ∞ÁöÑÂü∫Á§éÊ®°ÂûãÂèØËÉΩÊúÉÂ∞çËá®Â∫äÈÉ®ÁΩ≤ÈÄ†ÊàêÈôêÂà∂Ôºå‰ΩÜÂÆÉÂÄë‰ªçÁÑ∂ÁÇ∫ÈñãÁôºÊõ¥Â∞àÊ•≠‰∏îÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏Á∑®Á¢ºÂô®Êèê‰æõ‰∫ÜÁµï‰Ω≥ÁöÑÂü∫Á§éÔºå‰ª•Êé®Âãï AI Â∞éÂêëÁöÑË®∫Êñ∑Â∑•ÂÖ∑„ÄÇ</paragraph>

##### **Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation**
2409.09135v1 by Cheng Charles Ma, Kevin Hyekang Joo, Alexandria K. Vail, Sunreeta Bhattacharya, √Ålvaro Fern√°ndez Garc√≠a, Kailana Baker-Matsuoka, Sheryl Mathew, Lori L. Holt, Fernando De la Torre

Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÅéÂéªÂçÅÂπ¥‰∏≠ÔºåÂèØÁ©øÊà¥Ë®àÁÆóË£ùÁΩÆÔºà„ÄåÊô∫ÊÖßÁúºÈè°„ÄçÔºâÂú®ÊÑüÊ∏¨Âô®ÊäÄË°ì„ÄÅË®≠Ë®àÂíåËôïÁêÜËÉΩÂäõÊñπÈù¢Á∂ìÊ≠∑‰∫ÜÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÁÇ∫È´òÂØÜÂ∫¶‰∫∫È°ûË°åÁÇ∫Ë≥áÊñôÈñãÂïü‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ©üÊúÉÊôÇ‰ª£„ÄÇÈÖçÂÇôÂèØÁ©øÊà¥Áõ∏Ê©üÁöÑÈÄô‰∫õÁúºÈè°Êèê‰æõ‰∫Ü‰∏ÄÂÄãÁç®ÁâπÁöÑÊ©üÊúÉÔºåÂèØ‰ª•Âú®ÂÄã‰∫∫‰∫íÂãïÊôÇÂàÜÊûêËá™ÁÑ∂Áí∞Â¢É‰∏≠ÁöÑÈùûË™ûË®ÄË°åÁÇ∫„ÄÇÊàëÂÄëÁöÑÈáçÈªûÂú®ÊñºÈÄèÈÅé‰ªîÁ¥∞ÂØ©Êü•Ë®ÄË™ûÂíåÈùûË®ÄË™ûÁ∑öÁ¥¢‰æÜÈ†êÊ∏¨Èõô‰∫∫‰∫íÂãï‰∏≠ÁöÑÂèÉËàáÂ∫¶ÔºåÊó®Âú®ÂÅµÊ∏¨Âá∫‰∏çÊÑüËààË∂£ÊàñÂõ∞ÊÉëÁöÑË∑°Ë±°„ÄÇÂà©Áî®Ê≠§È°ûÂàÜÊûêÂèØËÉΩÊúÉÂæπÂ∫ïÊîπËÆäÊàëÂÄëÂ∞ç‰∫∫È°ûÊ∫ùÈÄöÁöÑÁêÜËß£Ôºå‰øÉÈÄ≤Â∞àÊ•≠Áí∞Â¢É‰∏≠Êõ¥ÊúâÊïàÁöÑÂçî‰ΩúÔºåÈÄèÈÅéÂêåÁêÜÂøÉÁöÑËôõÊì¨‰∫íÂãïÊèê‰æõÊõ¥Â•ΩÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅÔºå‰∏¶Â¢ûÂº∑Ê∫ùÈÄöÈöúÁ§ôËÄÖÁöÑÂèØÂèäÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 34 ‰ΩçÂèÉËàáËÄÖÂèÉËàá‰∫ÜÈö®ÊÑèÁöÑÈõô‰∫∫Â∞çË©±ÔºåÊØè‰ΩçÂèÉËàáËÄÖÂú®Â∞çË©±ÁµêÊùüÊôÇÈÉΩÊèê‰æõ‰∫ÜËá™ÊàëÂ†±ÂëäÁöÑÂèÉËàáÂ∫¶Ë©ïÂàÜ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñ∞Á©éËûçÂêàÁ≠ñÁï•ÔºåÂ∞áÂ§öÁ®ÆË°åÁÇ∫Ê®°ÂºèÊï¥ÂêàÂà∞„ÄåÂ§öÊ®°ÂºèËΩâÈåÑ„Äç‰∏≠ÔºåLLM ÂèØ‰ª•ËôïÁêÜÊ≠§ËΩâÈåÑ‰ª•ÈÄ≤Ë°åË°åÁÇ∫Êé®ÁêÜ‰ªªÂãô„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂç≥‰ΩøÂú®ÂàùÊ≠•ÂØ¶‰Ωú‰∏≠ÔºåÊ≠§ÊñπÊ≥ï‰πüËÉΩÈÅîÂà∞ËàáÊó¢ÂÆöÁöÑËûçÂêàÊäÄË°ìÁõ∏Áï∂ÁöÑÊïàËÉΩÔºåÈ°ØÁ§∫Âá∫ÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊúÄ‰Ω≥ÂåñÁöÑÂº∑Â§ßÊΩõÂäõ„ÄÇÊ≠§ËûçÂêàÊñπÊ≥ïÊòØÈ¶ñÊâπÈÄèÈÅéË™ûË®ÄÊ®°Âûã‰æÜÊé¢Ë®éÈóúÊñºÁúüÂØ¶‰∏ñÁïå‰∫∫È°ûË°åÁÇ∫ÁöÑ„ÄåÊé®ÁêÜ„ÄçÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇÊô∫ÊÖßÁúºÈè°ËÆìÊàëÂÄëËÉΩÂ§†‰∏çÂºï‰∫∫Ê≥®ÁõÆÂú∞Êî∂ÈõÜ‰∫∫È°ûË°åÁÇ∫ÁöÑÈ´òÂØÜÂ∫¶Â§öÊ®°ÂºèË≥áÊñôÔºåÁÇ∫ÁêÜËß£ÂíåÊîπÂñÑ‰∫∫È°ûÊ∫ùÈÄöÈã™Âπ≥‰∫ÜÈÅìË∑ØÔºå‰∏¶ÂÖ∑ÊúâÂ∏∂‰æÜÈáçË¶ÅÁ§æÊúÉÊïàÁõäÁöÑÊΩõÂäõ„ÄÇÁ†îÁ©∂ÊúüÈñìÊî∂ÈõÜÂà∞ÁöÑÁâπÂæµÂíåË≥áÊñôÂ∞áÂÖ¨ÈñãÊèê‰æõÔºå‰ª•‰øÉÈÄ≤ÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **MAISI: Medical AI for Synthetic Imaging**
2409.11169v1 by Pengfei Guo, Can Zhao, Dong Yang, Ziyue Xu, Vishwesh Nath, Yucheng Tang, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu

Medical imaging analysis faces challenges such as data scarcity, high
annotation costs, and privacy concerns. This paper introduces the Medical AI
for Synthetic Imaging (MAISI), an innovative approach using the diffusion model
to generate synthetic 3D computed tomography (CT) images to address those
challenges. MAISI leverages the foundation volume compression network and the
latent diffusion model to produce high-resolution CT images (up to a landmark
volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel
spacing. By incorporating ControlNet, MAISI can process organ segmentation,
including 127 anatomical structures, as additional conditions and enables the
generation of accurately annotated synthetic images that can be used for
various downstream tasks. Our experiment results show that MAISI's capabilities
in generating realistic, anatomically accurate images for diverse regions and
conditions reveal its promising potential to mitigate challenges using
synthetic data.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈù¢Ëá®Ë≥áÊñôÁ®ÄÂ∞ë„ÄÅÊ®ôË®ªÊàêÊú¨È´ò„ÄÅÈö±ÁßÅÂïèÈ°åÁ≠âÊåëÊà∞„ÄÇÊú¨Êñá‰ªãÁ¥πÁî®ÊñºÂêàÊàêÂΩ±ÂÉèÁöÑÈÜ´Â≠∏ AIÔºàMAISIÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Êì¥Êï£Ê®°Âûã‰æÜÁî¢ÁîüÂêàÊàê 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) ÂΩ±ÂÉèÔºå‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞„ÄÇMAISI Êé°Áî®Âü∫Á§éÈ´îÁ©çÂ£ìÁ∏ÆÁ∂≤Ë∑ØÂíåÊΩõÂú®Êì¥Êï£Ê®°ÂûãÔºå‰ª•Áî¢ÁîüÈ´òËß£ÊûêÂ∫¶ CT ÂΩ±ÂÉèÔºàÊúÄÈ´òÂèØÈÅî 512 x 512 x 768 ÁöÑÂú∞Ê®ôÈ´îÁ©çÁ∂≠Â∫¶ÔºâÔºåÂÖ∑ÊúâÈùàÊ¥ªÁöÑÈ´îÁ©çÁ∂≠Â∫¶ÂíåÈ´îÁ¥†ÈñìË∑ù„ÄÇÈÄèÈÅéÊï¥Âêà ControlNetÔºåMAISI ÂèØ‰ª•ËôïÁêÜÂô®ÂÆòÂàÜÂâ≤ÔºåÂåÖÊã¨ 127 ÂÄãËß£ÂâñÁµêÊßãÔºå‰ΩúÁÇ∫È°çÂ§ñÁöÑÊ¢ù‰ª∂Ôºå‰∏¶ËÉΩÁî¢ÁîüÂèØÁ≤æÁ¢∫Ê®ôË®ªÁöÑÂêàÊàêÂΩ±ÂÉèÔºåÂèØÁî®ÊñºÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåMAISI Âú®ÁÇ∫‰∏çÂêåÂçÄÂüüÂíåÊ¢ù‰ª∂Áî¢ÁîüÈÄºÁúü„ÄÅËß£ÂâñÂ≠∏‰∏äÁ≤æÁ¢∫ÁöÑÂΩ±ÂÉèÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊè≠Á§∫‰∫ÜÂÆÉÂú®‰ΩøÁî®ÂêàÊàêË≥áÊñôÊ∏õËºïÊåëÊà∞ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**
2409.08936v1 by Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester

We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation. The dataset can be downloaded from
https://github.com/prabaey/SynSUM.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ SynSUM Âü∫Ê∫ñÔºå‰∏ÄÂÄãÂ∞áÈùûÁµêÊßãÂåñËá®Â∫äË®òÈåÑÈÄ£ÁµêÂà∞ÁµêÊßãÂåñËÉåÊôØËÆäÊï∏ÁöÑÂêàÊàêË≥áÊñôÈõÜ„ÄÇË©≤Ë≥áÊñôÈõÜÂåÖÂê´ 10,000 ÂÄã‰∫∫Â∑•ÁóÖÊ≠∑ÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë°®Ê†ºËÆäÊï∏Ôºà‰æãÂ¶ÇÁóáÁãÄ„ÄÅË®∫Êñ∑ÂíåÊΩõÂú®ÁãÄÊ≥ÅÔºâÂíåÁõ∏ÈóúË®òÈåÑÔºåÊèèËø∞‰∫ÜÂëºÂê∏Á≥ªÁµ±ÁñæÁóÖÈ†òÂüü‰∏≠ÁöÑËôõÊßãÊÇ£ËÄÖÈÅ≠ÈÅá„ÄÇË≥áÊñôÁöÑË°®Ê†ºÈÉ®ÂàÜÊòØÈÄèÈÅéË≤ùÊ∞èÁ∂≤Ë∑ØÁî¢ÁîüÁöÑÔºåÂÖ∂‰∏≠ËÆäÊï∏‰πãÈñìÁöÑÂõ†ÊûúÁµêÊßãÂíåÊ¢ù‰ª∂Ê©üÁéáÈÉΩÊòØÁî±Â∞àÂÆ∂Ê†πÊìöÈ†òÂüüÁü•Ë≠òÊèêÂá∫ÁöÑ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÁ§∫‰∏ÄÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (GPT-4o) Áî¢ÁîüËàáÊ≠§ÊÇ£ËÄÖÈÅ≠ÈÅáÁõ∏ÈóúÁöÑËá®Â∫äË®òÈåÑÔºåÊèèËø∞ÊÇ£ËÄÖÁóáÁãÄÂíåÈ°çÂ§ñËÉåÊôØ„ÄÇSynSUM Ë≥áÊñôÈõÜ‰∏ªË¶ÅÊòØÁÇ∫‰∫Ü‰øÉÈÄ≤Âú®Ë°®Ê†ºËÉåÊôØËÆäÊï∏Â≠òÂú®ÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åËá®Â∫äË≥áË®äËêÉÂèñÁöÑÁ†îÁ©∂ÔºåÈÄô‰∫õËÆäÊï∏ÂèØ‰ª•ÈÄèÈÅéÈ†òÂüüÁü•Ë≠òÈÄ£ÁµêÂà∞ÂæûÊñáÊú¨‰∏≠ËêÉÂèñÁöÑÁõÆÊ®ôÊ¶ÇÂøµ - Âú® SynSUM ÁöÑÊ°à‰æã‰∏≠ÔºåÊòØÁóáÁãÄ„ÄÇÊ¨°Ë¶ÅÁî®ÈÄîÂåÖÊã¨Á†îÁ©∂Ë°®Ê†ºË≥áÊñôÂíåÊñáÊú¨ÁöÑËá®Â∫äÊé®ÁêÜËá™ÂãïÂåñ„ÄÅÂú®Ë°®Ê†ºÂíå/ÊàñÊñáÊú¨Ê∑∑Ê∑ÜÂõ†Â≠êÂ≠òÂú®ÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åÂõ†ÊûúÊïàÊáâ‰º∞Ë®àÔºå‰ª•ÂèäÂ§öÊ®°ÂºèÂêàÊàêË≥áÊñôÁîüÊàê„ÄÇÊ≠§Ë≥áÊñôÈõÜÂèØÂæû https://github.com/prabaey/SynSUM ‰∏ãËºâ„ÄÇ

##### **Recent Trends in Modelling the Continuous Time Series using Deep Learning: A Survey**
2409.09106v1 by Mansura Habiba, Barak A. Pearlmutter, Mehrdad Maleki

Continuous-time series is essential for different modern application areas,
e.g. healthcare, automobile, energy, finance, Internet of things (IoT) and
other related areas. Different application needs to process as well as analyse
a massive amount of data in time series structure in order to determine the
data-driven result, for example, financial trend prediction, potential
probability of the occurrence of a particular event occurrence identification,
patient health record processing and so many more. However, modeling real-time
data using a continuous-time series is challenging since the dynamical systems
behind the data could be a differential equation. Several research works have
tried to solve the challenges of modelling the continuous-time series using
different neural network models and approaches for data processing and
learning. The existing deep learning models are not free from challenges and
limitations due to diversity among different attributes, behaviour, duration of
steps, energy, and data sampling rate. This paper has described the general
problem domain of time series and reviewed the challenges of modelling the
continuous time series. We have presented a comparative analysis of recent
developments in deep learning models and their contribution to solving
different difficulties of modelling the continuous time series. We have also
identified the limitations of the existing neural network model and open
issues. The main goal of this review is to understand the recent trend of
neural network models used in a different real-world application with
continuous-time data.

ÊëòË¶ÅÔºöÈÄ£Á∫åÊôÇÈñìÂ∫èÂàóÂ∞ç‰∏çÂêåÁöÑÁèæ‰ª£ÊáâÁî®È†òÂüüËá≥ÈóúÈáçË¶ÅÔºå
‰æãÂ¶ÇÈÜ´ÁôÇ‰øùÂÅ•„ÄÅÊ±ΩËªä„ÄÅËÉΩÊ∫ê„ÄÅÈáëËûç„ÄÅÁâ©ËÅØÁ∂≤ (IoT) Âíå
ÂÖ∂‰ªñÁõ∏ÈóúÈ†òÂüü„ÄÇ‰∏çÂêåÁöÑÊáâÁî®ÈúÄË¶ÅËôïÁêÜ‰∏¶ÂàÜÊûê
ÊôÇÈñìÂ∫èÂàóÁµêÊßã‰∏≠ÁöÑÂ§ßÈáèÊï∏ÊìöÔºå‰ª•Á¢∫ÂÆö
Êï∏ÊìöÈ©ÖÂãïÁöÑÁµêÊûúÔºå‰æãÂ¶ÇÈáëËûçË∂®Âã¢È†êÊ∏¨„ÄÅÊΩõÂú®
ÁâπÂÆö‰∫ã‰ª∂ÁôºÁîüË≠òÂà•ÁöÑÁôºÁîüÊ©üÁéá„ÄÅ
ÊÇ£ËÄÖÂÅ•Â∫∑Ë®òÈåÑËôïÁêÜÁ≠âÁ≠â„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®ÈÄ£Á∫åÊôÇÈñìÂ∫èÂàóÂª∫Ê®°ÂØ¶ÊôÇ
Êï∏ÊìöÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Êï∏ÊìöËÉåÂæåÁöÑÂãïÂäõÁ≥ªÁµ±ÂèØËÉΩÊòØÂæÆÂàÜÊñπÁ®ãÂºè„ÄÇÂπæÈ†ÖÁ†îÁ©∂Â∑•‰ΩúÂ∑≤Á∂ì
ÂòóË©¶‰ΩøÁî®‰∏çÂêåÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÂíåÊï∏ÊìöËôïÁêÜÊñπÊ≥ï‰æÜËß£Ê±∫Âª∫Ê®°ÈÄ£Á∫åÊôÇÈñìÂ∫èÂàóÁöÑÊåëÊà∞
ÂíåÂ≠∏Áøí„ÄÇÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏¶ÈùûÊ≤íÊúâÊåëÊà∞Âíå
ÈôêÂà∂ÔºåÂõ†ÁÇ∫‰∏çÂêåÁöÑÂ±¨ÊÄß„ÄÅË°åÁÇ∫„ÄÅÊ≠•È©üÊåÅÁ∫åÊôÇÈñì„ÄÅËÉΩÈáèÂíåÊï∏ÊìöÊé°Ê®£Áéá‰πãÈñìÂ≠òÂú®Â∑ÆÁï∞„ÄÇÊú¨ÊñáÊèèËø∞‰∫ÜÊôÇÈñìÂ∫èÂàóÁöÑ‰∏ÄËà¨ÂïèÈ°åÈ†òÂüüÔºå‰∏¶ÂõûÈ°ß‰∫ÜÂª∫Ê®°ÈÄ£Á∫åÊôÇÈñìÂ∫èÂàóÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊúÄÊñ∞ÁôºÂ±ïÂèäÂÖ∂Â∞çËß£Ê±∫Âª∫Ê®°ÈÄ£Á∫åÊôÇÈñìÂ∫èÂàóÁöÑ‰∏çÂêåÈõ£Â∫¶ÁöÑË≤¢ÁçªÈÄ≤Ë°å‰∫ÜÊØîËºÉÂàÜÊûê„ÄÇÊàëÂÄëÈÇÑÁ¢∫ÂÆö‰∫ÜÁèæÊúâÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÂíåÈñãÊîæÂïèÈ°å„ÄÇÈÄôÁØáË©ïË´ñÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØ‰∫ÜËß£Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÂú®‰∏çÂêåÂØ¶ÈöõÊáâÁî®‰∏≠‰ΩøÁî®ÁöÑÊúÄÊñ∞Ë∂®Âã¢Ôºå‰∏¶‰ΩøÁî®ÈÄ£Á∫åÊôÇÈñìÊï∏Êìö„ÄÇ

##### **A BERT-Based Summarization approach for depression detection**
2409.08483v1 by Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa

Depression is a globally prevalent mental disorder with potentially severe
repercussions if not addressed, especially in individuals with recurrent
episodes. Prior research has shown that early intervention has the potential to
mitigate or alleviate symptoms of depression. However, implementing such
interventions in a real-world setting may pose considerable challenges. A
promising strategy involves leveraging machine learning and artificial
intelligence to autonomously detect depression indicators from diverse data
sources. One of the most widely available and informative data sources is text,
which can reveal a person's mood, thoughts, and feelings. In this context,
virtual agents programmed to conduct interviews using clinically validated
questionnaires, such as those found in the DAIC-WOZ dataset, offer a robust
means for depression detection through linguistic analysis. Utilizing
BERT-based models, which are powerful and versatile yet use fewer resources
than contemporary large language models, to convert text into numerical
representations significantly enhances the precision of depression diagnosis.
These models adeptly capture complex semantic and syntactic nuances, improving
the detection accuracy of depressive symptoms. Given the inherent limitations
of these models concerning text length, our study proposes text summarization
as a preprocessing technique to diminish the length and intricacies of input
texts. Implementing this method within our uniquely developed framework for
feature extraction and classification yielded an F1-score of 0.67 on the test
set surpassing all prior benchmarks and 0.81 on the validation set exceeding
most previous results on the DAIC-WOZ dataset. Furthermore, we have devised a
depression lexicon to assess summary quality and relevance. This lexicon
constitutes a valuable asset for ongoing research in depression detection.

ÊëòË¶ÅÔºöÊÜÇÈ¨±ÁóáÊòØ‰∏ÄÁ®ÆÂÖ®ÁêÉÊôÆÈÅçÂ≠òÂú®ÁöÑÂøÉÁêÜÁñæÁóÖÔºåÂ¶ÇÊûú‰∏çÂä†‰ª•Ëß£Ê±∫ÔºåÂèØËÉΩÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑÂæåÊûúÔºåÂ∞§ÂÖ∂ÊòØÂ∞çÊúâÂæ©ÁôºÊÄßÁôº‰ΩúÁöÑ‰∫∫„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊó©Êúü‰ªãÂÖ•ÊúâÂèØËÉΩÊ∏õËºïÊàñÁ∑©Ëß£ÊÜÇÈ¨±ÁóáÁãÄ„ÄÇÁÑ∂ËÄåÔºåÂú®ÁèæÂØ¶Áí∞Â¢É‰∏≠ÂØ¶ÊñΩÊ≠§È°ûÂπ≤È†êÊé™ÊñΩÂèØËÉΩÊúÉÂ∏∂‰æÜÁõ∏Áï∂Â§ßÁöÑÊåëÊà∞„ÄÇ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÁ≠ñÁï•ÂåÖÊã¨Âà©Áî®Ê©üÂô®Â≠∏ÁøíÂíå‰∫∫Â∑•Êô∫ÊÖßÔºåÂæû‰∏çÂêåÁöÑÊï∏Êìö‰æÜÊ∫ê‰∏≠Ëá™ÂãïÊ™¢Ê∏¨ÊÜÇÈ¨±ÁóáÊåáÊ®ô„ÄÇÊúÄÂª£Ê≥õÂèØÁî®‰∏îÊúÄÊúâË≥áË®äÁöÑÊï∏Êìö‰æÜÊ∫ê‰πã‰∏ÄÊòØÊñáÂ≠óÔºåÂÆÉÂèØ‰ª•Êè≠Á§∫‰∏ÄÂÄã‰∫∫ÁöÑÊÉÖÁ∑í„ÄÅÊÉ≥Ê≥ïÂíåÊÑüÂèó„ÄÇÂú®Ê≠§ËÑàÁµ°‰∏≠Ôºå‰ΩøÁî®Ëá®Â∫äÈ©óË≠âÂïèÂç∑Ôºà‰æãÂ¶ÇÂú® DAIC-WOZ Ë≥áÊñôÈõÜ‰∏≠ÊâæÂà∞ÁöÑÂïèÂç∑ÔºâÁ∑®ÂØ´Á®ãÂºèÈÄ≤Ë°åË®™Ë´áÁöÑËôõÊì¨‰ª£ÁêÜ‰∫∫ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÈÄèÈÅéË™ûË®ÄÂàÜÊûêÈÄ≤Ë°åÊÜÇÈ¨±ÁóáÊ™¢Ê∏¨ÁöÑÂº∑Â§ßÊñπÊ≥ï„ÄÇÂà©Áî® BERT Âü∫Á§éÊ®°ÂûãÔºàÂäüËÉΩÂº∑Â§ß‰∏îÁî®ÈÄîÂª£Ê≥õÔºå‰ΩÜ‰ΩøÁî®ÁöÑË≥áÊ∫êÊØîÁï∂‰ª£Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∞ëÔºâÂ∞áÊñáÂ≠óËΩâÊèõÁÇ∫Êï∏ÂÄºË°®Á§∫ÔºåÂèØÈ°ØËëóÊèêÈ´òÊÜÇÈ¨±ÁóáË®∫Êñ∑ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÈÄô‰∫õÊ®°ÂûãÂ∑ßÂ¶ôÂú∞ÊçïÊçâË§áÈõúÁöÑË™ûÁæ©ÂíåÂè•Ê≥ïÁ¥∞ÂæÆÂ∑ÆÂà•ÔºåÊèêÈ´òÊÜÇÈ¨±ÁóáÁãÄÁöÑÊ™¢Ê∏¨Ê∫ñÁ¢∫ÊÄß„ÄÇÈëëÊñºÈÄô‰∫õÊ®°ÂûãÂú®ÊñáÂ≠óÈï∑Â∫¶ÊñπÈù¢Â≠òÂú®Âõ∫ÊúâÁº∫Èô∑ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÊèêÂá∫ÊñáÂ≠óÊëòË¶Å‰ΩúÁÇ∫È†êËôïÁêÜÊäÄË°ìÔºå‰ª•Ê∏õÂ∞ëËº∏ÂÖ•ÊñáÂ≠óÁöÑÈï∑Â∫¶ÂíåË§áÈõúÊÄß„ÄÇÂú®ÊàëÂÄëÁç®Ëá™ÈñãÁôºÁöÑÂäüËÉΩÊèêÂèñÂíåÂàÜÈ°ûÊ°ÜÊû∂‰∏≠ÂØ¶ÊñΩÊ≠§ÊñπÊ≥ïÔºåÂú®Ê∏¨Ë©¶ÈõÜ‰∏≠Áî¢Áîü 0.67 ÁöÑ F1 ÂàÜÊï∏ÔºåË∂ÖË∂äÊâÄÊúâÂÖàÂâçÁöÑÂü∫Ê∫ñÔºåÂú®È©óË≠âÈõÜ‰∏≠Áî¢Áîü 0.81 ÁöÑ F1 ÂàÜÊï∏ÔºåË∂ÖÈÅé DAIC-WOZ Ë≥áÊñôÈõÜ‰∏äÂ§ßÂ§öÊï∏ÂÖàÂâçÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊÜÇÈ¨±ÁóáË©ûÂΩôË°®ÔºåÁî®ÊñºË©ï‰º∞ÊëòË¶ÅÂìÅË≥™ÂíåÁõ∏ÈóúÊÄß„ÄÇÊ≠§Ë©ûÂΩôË°®ÊßãÊàêÊÜÇÈ¨±ÁóáÊ™¢Ê∏¨ÊåÅÁ∫åÁ†îÁ©∂ÁöÑÂØ∂Ë≤¥Ë≥áÁî¢„ÄÇ

##### **Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**
2409.07793v1 by Fuchen Zheng, Quanjun Li, Weixuan Li, Xuhang Chen, Yihang Dong, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a critical application of semantic segmentation
in healthcare, has seen significant advancements through specialized computer
vision techniques. While deep learning-based medical image segmentation is
essential for assisting in medical diagnosis, the lack of diverse training data
causes the long-tail problem. Moreover, most previous hybrid CNN-ViT
architectures have limited ability to combine various attentions in different
layers of the Convolutional Neural Network. To address these issues, we propose
a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware
Contrastive Loss, as the overall training objective for semi-supervised
learning to mitigate the long-tail problem. Additionally, we introduce
CMAformer, a novel network that synergizes the strengths of ResUNet and
Transformer. The cross-attention block in CMAformer effectively integrates
spatial attention and channel attention for multi-scale feature fusion.
Overall, our results indicate that CMAformer, combined with the feature fusion
framework and the new consistency loss, demonstrates strong complementarity in
semi-supervised learning ensembles. We achieve state-of-the-art results on
multiple public medical image datasets. Example code are available at:
\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊòØË™ûÊÑèÂàÜÂâ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ÁöÑ‰∏ÄÈ†ÖÈáçË¶ÅÊáâÁî®ÔºåÂ∑≤ÈÄèÈÅéÂ∞àÊ•≠ÁöÑÈõªËÖ¶Ë¶ñË¶∫ÊäÄË°ìÁç≤ÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇÈõñÁÑ∂Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Â∞çÊñºÂçîÂä©ÈÜ´ÁôÇË®∫Êñ∑Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁº∫‰πèÂ§öÊ®£ÂåñÁöÑË®ìÁ∑¥Ë≥áÊñôÊúÉÂ∞éËá¥Èï∑Â∞æÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏ÂÖàÂâçÁöÑÊ∑∑ÂêàÂºè CNN-ViT Êû∂ÊßãÂú®ÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏çÂêåÂ±§‰∏≠ÁöÑÂêÑÁ®ÆÊ≥®ÊÑèÂäõÊñπÈù¢ËÉΩÂäõÊúâÈôê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊãâÊ†ºÊúóÊó•Â∞çÂÅ∂‰∏ÄËá¥ÊÄß (LDC) ÊêçÂ§±Ôºå‰∏¶ËàáÈÇäÁïåÊÑüÁü•Â∞çÊØîÊêçÂ§±Êï¥ÂêàÔºå‰ΩúÁÇ∫ÂçäÁõ£Áù£ÂºèÂ≠∏ÁøíÁöÑÊï¥È´îË®ìÁ∑¥ÁõÆÊ®ôÔºå‰ª•Ê∏õËºïÈï∑Â∞æÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CMAformerÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÁ∂≤Ë∑ØÔºåÂÆÉÂçîÂêå‰∫Ü ResUNet Âíå Transformer ÁöÑÂÑ™Èªû„ÄÇCMAformer ‰∏≠ÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÂçÄÂ°äÊúâÊïàÂú∞Êï¥Âêà‰∫ÜÁ©∫ÈñìÊ≥®ÊÑèÂäõÂíåÈÄöÈÅìÊ≥®ÊÑèÂäõÔºå‰ª•ÈÄ≤Ë°åÂ§öÂ∞∫Â∫¶ÁâπÂæµËûçÂêà„ÄÇÁ∏ΩÁöÑ‰æÜË™™ÔºåÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåCMAformer ÁµêÂêàÁâπÂæµËûçÂêàÊû∂ÊßãÂíåÊñ∞ÁöÑÁ®†ÂØÜÊêçÂ§±ÔºåÂú®ÂçäÁõ£Áù£ÂºèÂ≠∏ÁøíÈõÜÂêà‰∏≠Â±ïÁèæÂá∫Âº∑Â§ßÁöÑ‰∫íË£úÊÄß„ÄÇÊàëÂÄëÂú®Â§öÂÄãÂÖ¨ÈñãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÁØÑ‰æãÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}„ÄÇ

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊòØÈõªËÖ¶Ë¶ñË¶∫‰∏≠‰∏ÄÈ†ÖÈáçË¶ÅÁöÑ‰ªªÂãôÔºåÊúâÂä©ÊñºËá™ÂãïÊèèÁπ™Ëß£ÂâñÁµêÊßãÂíåÁóÖÁêÜÔºåÂçîÂä©Ëá®Â∫äÈÜ´Â∏´ÈÄ≤Ë°åË®∫Êñ∑„ÄÅÊ≤ªÁôÇË®àÁï´ÂíåÁñæÁóÖÁõ£Êéß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊé°Áî®‰ΩçÁßªË¶ñÁ™óËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑTransformerÂ±ïÁèæÂá∫ÈùûÂá°ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰æùË≥¥ÊñºÂçÄÂüüË¶ñÁ™óÊ≥®ÊÑèÂäõÔºåÈÄôÈôêÂà∂‰∫ÜÂçÄÂüüÂíåÂÖ®ÂüüËÑàÁµ°Ë≥áË®äÁöÑËûçÂêàÔºåËÄåÈÄôÂ∞çÊñºÂàÜÂâ≤ÂæÆÂ∞èËÖ´Áò§ÂíåÂæÆÂûãÂô®ÂÆòËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜËá™ÈÅ©ÊáâË™ûÁæ©ÂàÜÂâ≤Á∂≤Ë∑Ø (ASSNet)ÔºåÈÄôÊòØ‰∏ÄÂÄãTransformerÊû∂ÊßãÔºåÂèØ‰ª•ÊúâÊïàÊï¥ÂêàÂçÄÂüüÂíåÂÖ®ÂüüÁâπÂæµÔºå‰ª•ÈÄ≤Ë°åÁ≤æÁ¢∫ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇASSNet ÂåÖÂê´‰∏ÄÂÄãÂü∫ÊñºTransformerÁöÑ U ÂûãÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Á∂≤Ë∑Ø„ÄÇÁ∑®Á¢ºÂô®Âà©Áî®‰∫îÂÄãËß£ÊûêÂ∫¶ÁöÑ‰ΩçÁßªË¶ñÁ™óËá™Ê≥®ÊÑèÂäõ‰æÜËêÉÂèñÂ§öÂ∞∫Â∫¶ÁâπÂæµÔºåÁÑ∂ÂæåÈÄèÈÅéË∑≥Ë∫çÈÄ£Á∑öÂ∞áÈÄô‰∫õÁâπÂæµÂÇ≥Êí≠Âà∞Ëß£Á¢ºÂô®„ÄÇÊàëÂÄëÂú®Á∑®Á¢ºÂô®‰∏≠ÂºïÂÖ•‰∫ÜÊì¥Â¢ûÁöÑÂ§öÂ±§ÊÑüÁü•Âô®Ôºå‰ª•‰æøÂú®ÁâπÂæµËêÉÂèñÊúüÈñìÊòéÁ¢∫Âú∞Âª∫Ê®°Èï∑Á®ã‰æùË≥¥ÊÄß„ÄÇÈëëÊñºÂÇ≥Áµ±Â∞çÁ®±Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ë®≠Ë®àÁöÑÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËá™ÈÅ©ÊáâÁâπÂæµËûçÂêà (AFF) Ëß£Á¢ºÂô®‰æÜË£úÂÖÖÊàëÂÄëÁöÑÁ∑®Á¢ºÂô®„ÄÇÊ≠§Ëß£Á¢ºÂô®ÂåÖÂê´‰∏âÂÄãÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºöÈï∑Á®ã‰æùË≥¥ÊÄß (LRD) ÂçÄÂ°ä„ÄÅÂ§öÂ∞∫Â∫¶ÁâπÂæµËûçÂêà (MFF) ÂçÄÂ°äÂíåËá™ÈÅ©ÊáâË™ûÁæ©‰∏≠ÂøÉ (ASC) ÂçÄÂ°ä„ÄÇÈÄô‰∫õÁµÑÊàêÈÉ®ÂàÜÁõ∏‰∫íÈÖçÂêàÔºå‰øÉÊàêËß£Á¢ºÂô®ËêÉÂèñÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæµÊúâÊïàËûçÂêàÔºåÂêåÊôÇÊçïÊçâÈï∑Á®ã‰æùË≥¥ÊÄß‰∏¶ÂæÆË™øÁâ©‰ª∂ÈÇäÁïå„ÄÇÂú®Â§öÂô®ÂÆò„ÄÅËÇùËáüËÖ´Áò§ÂíåËÜÄËÉ±ËÖ´Áò§ÂàÜÂâ≤Á≠âÂêÑÁ®ÆÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂãô‰∏äÁöÑÂÖ®Èù¢ÂØ¶È©óË≠âÊòéÔºåASSNet ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÁ®ãÂºèÁ¢ºÂíåÊ®°ÂûãÂèØÊñº‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö\url{https://github.com/lzeeorno/ASSNet}„ÄÇ</paragraph>

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

ÊëòË¶ÅÔºöÁßëÊäÄËàáÈÜ´ÁôÇÁöÑÊï¥ÂêàÈñãÂïü‰∫Ü‰∏ÄÂÄãÊñ∞Á¥ÄÂÖÉÔºåÁî±‰∫∫Â∑•Êô∫ÊÖßÂíåÊ©üÂô®Â≠∏ÁøíÈ©ÖÂãïÁöÑËªüÈ´îÁ≥ªÁµ±Â∑≤ÊàêÁÇ∫ÈÜ´ÁôÇÁî¢ÂìÅÂíåÊúçÂãôÁöÑÂøÖË¶ÅÁµÑÊàêÈÉ®ÂàÜ„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈÄ≤Ê≠•Â∞çÊîπÂñÑÊÇ£ËÄÖÁÖßË≠∑ÂíåÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõÊïàÁéáÊúâÂæàÂ§ßÁöÑÂπ´Âä©Ôºå‰ΩÜÂÆÉÂÄë‰πüËÆìÊïèÊÑüÁöÑÈÜ´ÁôÇË≥áÊñôÂíåÁ≥ªÁµ±ÂÆåÊï¥ÊÄßÈù¢Ëá®ÊΩõÂú®ÁöÑÁ∂≤Ë∑ØÊîªÊìäÈ¢®Èö™„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ÊÖß/Ê©üÂô®Â≠∏ÁøíÊáâÁî®Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Â∏∂‰æÜÁöÑÂÆâÂÖ®ÊÄßÂíåÈö±ÁßÅÂ®ÅËÑÖ„ÄÇÈÄèÈÅéÂæπÂ∫ïÊ™¢Ë¶ñÂêÑÈ†ÖÈÜ´ÁôÇÈ†òÂüüÁèæÊúâÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂú®‰∫ÜËß£ÈáùÂ∞çÈÜ´ÁôÇ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÁöÑÂ∞çÊäóÊÄßÊîªÊìäÊñπÈù¢ÊúâÈ°ØËëóÁöÑÂ∑ÆË∑ù„ÄÇÈÄèÈÅéÊ¶ÇËø∞ÈÜ´ÁôÇÁí∞Â¢ÉÁöÑÁâπÂÆöÂ∞çÊäóÊÄßÂ®ÅËÑÖÊ®°Âûã‰∏¶ÊâæÂá∫ÂÆπÊòìÂèóÊîªÊìäÁöÑÊáâÁî®È†òÂüüÔºåÊàëÂÄëÁÇ∫Êú™‰æÜÁ†îÁ©∂Â•†ÂÆöÂü∫Á§éÔºåÊé¢Ë®é‰∫∫Â∑•Êô∫ÊÖßÈ©ÖÂãïÈÜ´ÁôÇÁ≥ªÁµ±ÁöÑÂÆâÂÖ®ÊÄßËàáÂæ©ÂéüÂäõ„ÄÇÈÄèÈÅéÂàÜÊûê‰∏çÂêåÁöÑÂ®ÅËÑÖÊ®°ÂûãÂíåÈáùÂ∞ç‰∏çÂêåÈÜ´ÁôÇÈ†òÂüüÁöÑÂ∞çÊäóÊÄßÊîªÊìäÂèØË°åÊÄßÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞ç‰∫∫Â∑•Êô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•ÊäÄË°ìÂø´ÈÄüÁôºÂ±ïÈ†òÂüü‰∏≠Á∂≤Ë∑ØÂÆâÂÖ®Á†îÁ©∂ÁöÑËø´ÂàáÈúÄÊ±ÇÊèê‰æõ‰∫Ü‰ª§‰∫∫‰ø°ÊúçÁöÑË¶ãËß£„ÄÇ

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

ÊëòË¶ÅÔºöÊ®ôÊ∫ñÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂàÜÈ°ûÊñπÊ≥ïÂú®ÂØ¶ÈöõÁöÑËá®Â∫äÊáâÁî®‰∏≠ÂèØËÉΩ‰∏¶‰∏çÁ∏ΩÊòØÂØ¶Áî®ÁöÑÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÈúÄË¶ÅÈõÜ‰∏≠Êî∂ÈõÜÊâÄÊúâÊ®£Êú¨„ÄÇËÅØÈÇ¶Â≠∏Áøí (FL) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÁØÑ‰æãÔºåÂèØ‰ª•Âú®‰∏çËÆìÂÆ¢Êà∂Á´ØÂàÜ‰∫´Êï∏ÊìöÁöÑÊÉÖÊ≥Å‰∏ãÂæûÂàÜÂ∏ÉÂºèÊï∏ÊìöÈõÜÂ≠∏ÁøíÔºåÈÄôÊúâÂä©ÊñºÊ∏õËºïÈö±ÁßÅÂíåÊï∏ÊìöÊâÄÊúâÊ¨äÂïèÈ°å„ÄÇÂú® FL ‰∏≠ÔºåÁî±Êñº‰∏çÂêåÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑÊï∏ÊìöÊî∂ÈõÜÂçîÂÆöÂíåÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àË≥áÊñôÁöÑÂ∑ÆÁï∞Ôºå‰æÜËá™‰∏çÂêåÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑÊï∏Êìö‰πãÈñìÂ∏∏Ë¶ãÁöÑÊï∏ÊìöÁï∞Ë≥™ÊÄßÊúÉÂ∞éËá¥Ê¨°ÊúÄ‰Ω≥Êî∂ÊñÇ„ÄÇÈÄèÈÅéÊú¨Á†îÁ©∂‰∏≠ÁöÑÂØ¶È©óÔºåÊàëÂÄëË°®ÊòéÊï∏ÊìöÁï∞Ë≥™ÊÄßÊúÉÂ∞éËá¥Â±ÄÈÉ®Ë®ìÁ∑¥ÊúüÈñìÁôºÁîüÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÁèæË±°„ÄÇÊàëÂÄëÊèêÂá∫ FedImpresÔºåÂÆÉÈÄèÈÅéÈÇÑÂéüË°®Á§∫ÂÖ®ÁêÉË≥áË®äÁöÑÂêàÊàêË≥áÊñô‰ΩúÁÇ∫ËÅØÈÇ¶Âç∞Ë±°‰æÜÊ∏õËºïÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÁÖâÂá∫ÊØè‰∏ÄËº™ÈÄöË®äÊâÄÁî¢ÁîüÁöÑÂÖ®ÁêÉÊ®°Âûã„ÄÇÈö®ÂæåÔºåÊàëÂÄë‰ΩøÁî®ÂêàÊàêË≥áÊñôÂíåÊú¨Âú∞Ë≥áÊñô‰æÜÂ¢ûÂº∑Êú¨Âú∞Ë®ìÁ∑¥ÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú® BloodMNIST Âíå Retina Êï∏ÊìöÈõÜ‰∏äÈÉΩÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÈÄô‰∫õÊï∏ÊìöÈõÜÂåÖÂê´Ê®ôÁ±§‰∏çÂπ≥Ë°°ÂíåÈ†òÂüüËΩâÁßªÔºåÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 20%„ÄÇ

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, Cl√©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÊñπÈù¢ÁöÑÂø´ÈÄüÁôºÂ±ïÔºå‰øÉ‰Ωø‰∫∫ÂÄëÂëºÁ±≤ÈÄ≤Ë°åÊï¥È´îË©ï‰º∞ÔºåË∂ÖË∂äÁ∂ìÂ∏∏ÂºïÁî®ÁöÑÂü∫Ê∫ñÔºà‰æãÂ¶Ç USMLEÔºâÔºå‰ª•Êõ¥Â•ΩÂú∞ÂèçÊò†ÂØ¶ÈöõÊïàËÉΩ„ÄÇÂÑòÁÆ°ÂØ¶ÈöõË©ï‰º∞ÊòØÂØ¶Áî®ÊÄßÁöÑÂØ∂Ë≤¥ÊåáÊ®ôÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏ËêΩÂæåÊñº LLM ÊºîÂåñÁöÑÈÄüÂ∫¶ÔºåÂú®ÈÉ®ÁΩ≤ÂæåÂèØËÉΩÊúÉ‰ΩøÁ†îÁ©∂ÁµêÊûúÈÅéÊôÇ„ÄÇÈÄôÁ®ÆÊôÇÈñì‰∏äÁöÑËÑ´ÁØÄÈúÄË¶ÅÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂâçÊúüË©ï‰º∞Ôºå‰ª•ÊåáÂ∞éÁâπÂÆöËá®Â∫äÊáâÁî®Á®ãÂºèÁöÑÊ®°ÂûãÈÅ∏Êìá„ÄÇÊàëÂÄëÂºïÈÄ≤ MEDICÔºå‰∏ÄÂÄãË©ï‰º∞ LLM Ë∑®Ë∂äËá®Â∫äËÉΩÂäõÁöÑ‰∫îÂÄãÈóúÈçµÈù¢ÂêëÁöÑÊû∂ÊßãÔºöÈÜ´ÁôÇÊé®ÁêÜ„ÄÅÂÄ´ÁêÜÂíåÂÅèÂ∑Æ„ÄÅË≥áÊñôÂíåË™ûË®ÄÁêÜËß£„ÄÅÊÉÖÂ¢ÉÂ≠∏ÁøíÂíåËá®Â∫äÂÆâÂÖ®ÊÄß„ÄÇMEDIC Êé°Áî®‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ‰∫§‰∫íÂºèÊ™¢Êü•Êû∂ÊßãÔºåÈáèÂåñ LLM Âú®Ê∂µËìãÁØÑÂúçÂíåÂπªË¶∫ÂÅµÊ∏¨Á≠âÈ†òÂüüÁöÑÊïàËÉΩÔºåËÄå‰∏çÈúÄË¶ÅÂèÉËÄÉËº∏Âá∫„ÄÇÊàëÂÄë‰ΩøÁî® MEDIC ‰æÜË©ï‰º∞ LLM Âú®ÈÜ´ÁôÇÂïèÈ°åËß£Á≠î„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÊëòË¶Å„ÄÅÁ≠ÜË®òÁî¢ÁîüÂíåÂÖ∂‰ªñ‰ªªÂãô‰∏äÁöÑË°®Áèæ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫Ôºå‰∏çÂêåÊ®°ÂûãÂ§ßÂ∞è„ÄÅÂü∫Ê∫ñËàáÁ∂ìÈÅéÈÜ´ÁôÇÂæÆË™øÁöÑÊ®°Âûã‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆÁï∞Ôºå‰∏¶Â∞çÈúÄË¶ÅÁâπÂÆöÊ®°ÂûãÂÑ™Âã¢ÁöÑÊáâÁî®Á®ãÂºèÔºà‰æãÂ¶Ç‰ΩéÂπªË¶∫ÊàñËºÉ‰ΩéÁöÑÊé®Ë´ñÊàêÊú¨ÔºâÁöÑÊ®°ÂûãÈÅ∏ÊìáÁî¢ÁîüÂΩ±Èüø„ÄÇMEDIC ÁöÑÂ§öÈù¢ÂêëË©ï‰º∞Êè≠Á§∫‰∫ÜÈÄô‰∫õÊïàËÉΩÊ¨äË°°ÔºåÁ∏ÆÂ∞è‰∫ÜÁêÜË´ñËÉΩÂäõËàáÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠ÁöÑÂØ¶ÈöõÂØ¶‰Ωú‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÁ¢∫‰øùÊâæÂá∫ÊúÄÊúâÂ∏åÊúõÁöÑÊ®°ÂûãÔºå‰∏¶ÈáùÂ∞ç‰∏çÂêåÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®Á®ãÂºèÈÄ≤Ë°åË™øÊï¥„ÄÇ

##### **Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**
2409.07186v2 by Sheng Chen, Zihao Tang, Mariano Cabezas, Xinyi Wang, Arkiev D'Souza, Michael Barnett, Fernando Calamante, Weidong Cai, Chenyu Wang

Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging
(MRI) technique sensitised to the diffusivity of water molecules, offering the
capability to inspect tissue microstructures and is the only in-vivo method to
reconstruct white matter fiber tracts non-invasively. The DWI signal can be
analysed with the diffusion tensor imaging (DTI) model to estimate the
directionality of water diffusion within voxels. Several scalar metrics,
including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity
(RD), and fractional anisotropy (FA), can be further derived from DTI to
quantitatively summarise the microstructural integrity of brain tissue. These
scalar metrics have played an important role in understanding the organisation
and health of brain tissue at a microscopic level in clinical studies. However,
reliable DTI metrics rely on DWI acquisitions with high gradient directions,
which often go beyond the commonly used clinical protocols. To enhance the
utility of clinically acquired DWI and save scanning time for robust DTI
analysis, this work proposes DirGeo-DTI, a deep learning-based method to
estimate reliable DTI metrics even from a set of DWIs acquired with the minimum
theoretical number (6) of gradient directions. DirGeo-DTI leverages directional
encoding and geometric constraints to facilitate the training process. Two
public DWI datasets were used for evaluation, demonstrating the effectiveness
of the proposed method. Extensive experimental results show that the proposed
method achieves the best performance compared to existing DTI enhancement
methods and potentially reveals further clinical insights with routine clinical
DWI scans.

ÊëòË¶ÅÔºöÊì¥Êï£Âä†Ê¨äÂΩ±ÂÉè (DWI) ÊòØ‰∏ÄÁ®ÆÁ£ÅÊåØÈÄ†ÂΩ± (MRI) ÊäÄË°ìÔºåÂ∞çÊ∞¥ÂàÜÂ≠êÊì¥Êï£ÊïèÊÑüÔºåËÉΩÊ™¢Ê∏¨ÁµÑÁπîÂæÆÁµêÊßãÔºå‰∏îÊòØÂîØ‰∏Ä‰∏ÄÁ®ÆÂèØÈùû‰æµÂÖ•ÊÄßÈáçÂª∫ÁôΩË≥™Á∫ñÁ∂≠ÊùüÁöÑÈ´îÂÖßÊñπÊ≥ï„ÄÇDWI Ë®äËôüÂèØÁî®Êì¥Êï£ÂºµÈáèÂΩ±ÂÉè (DTI) Ê®°ÂûãÂàÜÊûêÔºå‰ª•‰º∞Ë®àÈ´îÁ¥†ÂÖßÊ∞¥ÂàÜÊì¥Êï£ÁöÑÊñπÂêëÊÄß„ÄÇÊï∏ÂÄãÊ®ôÈáèÈáèÊ∏¨ÔºåÂåÖÊã¨Ëª∏ÂêëÊì¥Êï£Áéá (AD)„ÄÅÂπ≥ÂùáÊì¥Êï£Áéá (MD)„ÄÅÂæëÂêëÊì¥Êï£Áéá (RD) ÂíåÂàÜÊï∏ÂêÑÂêëÁï∞ÊÄß (FA)ÔºåÂèØÈÄ≤‰∏ÄÊ≠•Âæû DTI Ë°çÁîüÔºå‰ª•ÈáèÂåñÁ∏ΩÁµêËÖ¶ÁµÑÁπîÁöÑÂæÆÁµêÊßãÂÆåÊï¥ÊÄß„ÄÇÈÄô‰∫õÊ®ôÈáèÈáèÊ∏¨Âú®Ëá®Â∫äÁ†îÁ©∂‰∏≠‰∫ÜËß£ËÖ¶ÁµÑÁπîÂú®ÂæÆËßÄÂ±§Á¥öÁöÑÁµÑÁπîÂíåÂÅ•Â∫∑ÊñπÈù¢ÁôºÊèÆ‰∫ÜÈáçË¶Å‰ΩúÁî®„ÄÇÁÑ∂ËÄåÔºåÂèØÈù†ÁöÑ DTI ÈáèÊ∏¨‰æùË≥¥ÊñºÂÖ∑ÊúâÈ´òÊ¢ØÂ∫¶ÊñπÂêëÁöÑ DWI Êì∑ÂèñÔºåÈÄôÈÄöÂ∏∏Ë∂ÖÂá∫Â∏∏Áî®ÁöÑËá®Â∫äÂçîÂÆö„ÄÇÁÇ∫‰∫ÜÊèêÂçáËá®Â∫äÊì∑Âèñ DWI ÁöÑÊïàÁî®Ôºå‰∏¶ÁØÄÁúÅÁ©©ÂÅ• DTI ÂàÜÊûêÁöÑÊéÉÊèèÊôÇÈñìÔºåÊú¨Á†îÁ©∂ÊèêÂá∫ DirGeo-DTIÔºå‰∏ÄÁ®ÆÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÂç≥‰ΩøÂæûÂÖ∑ÂÇôÊúÄÂ∞èÁêÜË´ñÊï∏Èáè (6) ÂÄãÊ¢ØÂ∫¶ÊñπÂêëÁöÑ DWI ÁµÑ‰πüËÉΩ‰º∞Ë®àÂèØÈù†ÁöÑ DTI ÈáèÊ∏¨„ÄÇDirGeo-DTI Âà©Áî®ÊñπÂêëÁ∑®Á¢ºÂíåÂπæ‰ΩïÁ¥ÑÊùü‰æÜ‰øÉÈÄ≤Ë®ìÁ∑¥ÈÅéÁ®ã„ÄÇÂÖ©ÂÄãÂÖ¨ÈñãÁöÑ DWI Ë≥áÊñôÈõÜÁî®ÊñºË©ï‰º∞ÔºåË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåËàáÁèæÊúâÁöÑ DTI Â¢ûÂº∑ÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁç≤Âæó‰∫ÜÊúÄ‰Ω≥ÁöÑÊïàËÉΩÔºå‰∏¶ÊúâÂèØËÉΩÈÄèÈÅé‰æãË°åËá®Â∫ä DWI ÊéÉÊèèÊè≠Á§∫ÈÄ≤‰∏ÄÊ≠•ÁöÑËá®Â∫äË¶ãËß£„ÄÇ

##### **CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**
2409.07092v1 by Feiyang Jia, Zhineng Chen, Ziying Song, Lin Liu, Caiyan Jia

Super-resolution (SR) aims to enhance the quality of low-resolution images
and has been widely applied in medical imaging. We found that the design
principles of most existing methods are influenced by SR tasks based on
real-world images and do not take into account the significance of the
multi-level structure in pathological images, even if they can achieve
respectable objective metric evaluations. In this work, we delve into two
super-resolution working paradigms and propose a novel network called CWT-Net,
which leverages cross-scale image wavelet transform and Transformer
architecture. Our network consists of two branches: one dedicated to learning
super-resolution and the other to high-frequency wavelet features. To generate
high-resolution histopathology images, the Transformer module shares and fuses
features from both branches at various stages. Notably, we have designed a
specialized wavelet reconstruction module to effectively enhance the wavelet
domain features and enable the network to operate in different modes, allowing
for the introduction of additional relevant information from cross-scale
images. Our experimental results demonstrate that our model significantly
outperforms state-of-the-art methods in both performance and visualization
evaluations and can substantially boost the accuracy of image diagnostic
networks.

ÊëòË¶ÅÔºöË∂ÖËß£ÊûêÂ∫¶ (SR) Êó®Âú®ÊèêÂçá‰ΩéËß£ÊûêÂ∫¶ÂΩ±ÂÉèÁöÑÂìÅË≥™Ôºå‰∏¶Â∑≤Âª£Ê≥õÊáâÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÊàëÂÄëÁôºÁèæÁèæÊúâÊñπÊ≥ïÁöÑÂ§ßÈÉ®ÂàÜË®≠Ë®àÂéüÂâáÈÉΩÂèóÂà∞Âü∫ÊñºÁúüÂØ¶ÂΩ±ÂÉèÁöÑ SR ‰ªªÂãôÂΩ±ÈüøÔºåËÄå‰∏îÂç≥‰ΩøÂÆÉÂÄëËÉΩÈÅîÂà∞ÂèØËßÄÁöÑÂÆ¢ËßÄÊåáÊ®ôË©ï‰º∞Ôºå‰πü‰∏çÊúÉËÄÉÊÖÆÁóÖÁêÜÂΩ±ÂÉè‰∏≠Â§öÂ±§Á¥öÁµêÊßãÁöÑÈáçË¶ÅÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®éÂÖ©Á®ÆË∂ÖËß£ÊûêÂ∫¶Â∑•‰ΩúÁØÑ‰æãÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ CWT-Net ÁöÑÊñ∞ÂûãÁ∂≤Ë∑ØÔºåÂÆÉÂà©Áî®Ë∑®Â∞∫Â∫¶ÂΩ±ÂÉèÂ∞èÊ≥¢ËΩâÊèõÂíå Transformer Êû∂Êßã„ÄÇÊàëÂÄëÁöÑÁ∂≤Ë∑ØÂåÖÂê´ÂÖ©ÂÄãÂàÜÊîØÔºö‰∏ÄÂÄãÂ∞àÈñÄÁî®ÊñºÂ≠∏ÁøíË∂ÖËß£ÊûêÂ∫¶ÔºåÂè¶‰∏ÄÂÄãÂâáÁî®ÊñºÈ´òÈ†ªÂ∞èÊ≥¢ÁâπÂæµ„ÄÇÁÇ∫‰∫ÜÁî¢ÁîüÈ´òËß£ÊûêÂ∫¶ÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèÔºåTransformer Ê®°ÁµÑÊúÉÂú®‰∏çÂêåÈöéÊÆµÂàÜ‰∫´ÂíåËûçÂêà‰æÜËá™ÂÖ©ÂÄãÂàÜÊîØÁöÑÁâπÂæµ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂ∞àÈñÄÁöÑÂ∞èÊ≥¢ÈáçÂª∫Ê®°ÁµÑÔºå‰ª•ÊúâÊïàÂ¢ûÂº∑Â∞èÊ≥¢ÂüüÁâπÂæµÔºå‰∏¶ËÆìÁ∂≤Ë∑ØËÉΩÂ§†Âú®‰∏çÂêåÊ®°Âºè‰∏ãÈÅã‰ΩúÔºåÂÖÅË®±ÂæûË∑®Â∞∫Â∫¶ÂΩ±ÂÉè‰∏≠ÂºïÂÖ•ÂÖ∂‰ªñÁõ∏ÈóúË≥áË®ä„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊïàËÉΩÂíåË¶ñË¶∫ÂåñË©ï‰º∞ÊñπÈù¢ÈÉΩÂ§ßÂπÖÂÑ™ÊñºÁèæÊúâÊäÄË°ìÔºåËÄå‰∏îËÉΩÂ§ßÂπÖÊèêÂçáÂΩ±ÂÉèË®∫Êñ∑Á∂≤Ë∑ØÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**
2409.07012v1 by Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi

Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals
to assess patient conditions and monitor changes over time. Generative models,
specifically diffusion-based models, have shown promise in generating realistic
synthetic X-rays. However, these models mainly focus on conditional generation
using single-time-point data, i.e., typically CXRs taken at a specific time
with their corresponding reports, limiting their clinical utility, particularly
for capturing temporal changes. To address this limitation, we propose a novel
framework, EHRXDiff, which predicts future CXR images by integrating previous
CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.
Our framework dynamically tracks and predicts disease progression based on a
latent diffusion model, conditioned on the previous CXR image and a history of
medical events. We comprehensively evaluate the performance of our framework
across three key aspects, including clinical consistency, demographic
consistency, and visual realism. We demonstrate that our framework generates
high-quality, realistic future images that capture potential temporal changes,
suggesting its potential for further development as a clinical simulation tool.
This could offer valuable insights for patient monitoring and treatment
planning in the medical field.

ÊëòË¶ÅÔºöËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÔºàCXRÔºâÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑË®∫Êñ∑Â∑•ÂÖ∑ÔºåÁî®ÊñºÈÜ´Èô¢Ë©ï‰º∞ÁóÖÊÇ£ÁãÄÊ≥Å‰∏¶Áõ£ÊéßÂÖ∂Èö®ËëóÊôÇÈñìÁöÑËÆäÂåñ„ÄÇÁîüÊàêÊ®°ÂûãÔºåÁâπÂà•ÊòØÂü∫ÊñºÊì¥Êï£ÁöÑÊ®°ÂûãÔºåÂ∑≤Âú®ÁîüÊàêÈÄºÁúüÁöÑÂêàÊàê X ÂÖâÂΩ±ÂÉèÊñπÈù¢Â±ïÁèæÂá∫ÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°Âûã‰∏ªË¶ÅÂ∞àÊ≥®Êñº‰ΩøÁî®ÂñÆ‰∏ÄÊôÇÈñìÈªûË≥áÊñôÈÄ≤Ë°åÊ¢ù‰ª∂ÁîüÊàêÔºåÂç≥ÈÄöÂ∏∏Âú®ÁâπÂÆöÊôÇÈñìÈªûÊãçÊîùÁöÑ CXR ÂèäÂÖ∂Â∞çÊáâÂ†±ÂëäÔºåÈÄôÈôêÂà∂‰∫ÜÂÖ∂Ëá®Â∫äÊïàÁî®ÔºåÁâπÂà•ÊòØÂ∞çÊñºÊçïÊçâÊôÇÈñìËÆäÂåñ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ EHRXDiffÔºåÂÆÉÈÄèÈÅéÊï¥ÂêàÂÖàÂâçÁöÑ CXR ËàáÂæåÁ∫åÁöÑÈÜ´ÁôÇ‰∫ã‰ª∂Ôºà‰æãÂ¶ÇËôïÊñπ„ÄÅÂØ¶È©óÂÆ§Ê™¢Ê∏¨Á≠âÔºâ‰æÜÈ†êÊ∏¨Êú™‰æÜÁöÑ CXR ÂΩ±ÂÉè„ÄÇÊàëÂÄëÁöÑÊ°ÜÊû∂Âü∫ÊñºÊΩõÂú®Êì¥Êï£Ê®°ÂûãÂãïÊÖãËøΩËπ§‰∏¶È†êÊ∏¨ÁñæÁóÖÈÄ≤Â±ïÔºåÊ¢ù‰ª∂ÂèñÊ±∫ÊñºÂÖàÂâçÁöÑ CXR ÂΩ±ÂÉèÂíåÈÜ´ÁôÇ‰∫ã‰ª∂ÁöÑÊ≠∑Âè≤Ë®òÈåÑ„ÄÇÊàëÂÄëÂÖ®Èù¢Ë©ï‰º∞‰∫ÜÊàëÂÄëÊ°ÜÊû∂Âú®‰∏âÂÄãÈóúÈçµÊñπÈù¢ÁöÑÊïàËÉΩÔºåÂåÖÊã¨Ëá®Â∫ä‰∏ÄËá¥ÊÄß„ÄÅ‰∫∫Âè£Áµ±Ë®à‰∏ÄËá¥ÊÄßÂíåË¶ñË¶∫ÈÄºÁúüÂ∫¶„ÄÇÊàëÂÄëË≠âÊòéÊàëÂÄëÁöÑÊ°ÜÊû∂ÁîüÊàê‰∫ÜÈ´òÂìÅË≥™„ÄÅÈÄºÁúüÁöÑÊú™‰æÜÂΩ±ÂÉèÔºåÊçïÊçâ‰∫ÜÊΩõÂú®ÁöÑÊôÇÈñìËÆäÂåñÔºåÈÄôË°®ÊòéÂÖ∂ÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ïÁÇ∫Ëá®Â∫äÊ®°Êì¨Â∑•ÂÖ∑ÁöÑÊΩõÂäõ„ÄÇÈÄôÂèØ‰ª•ÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÁöÑÁóÖÊÇ£Áõ£ÊéßÂíåÊ≤ªÁôÇË¶èÂäÉÊèê‰æõÊúâÂÉπÂÄºÁöÑË¶ãËß£„ÄÇ

##### **Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**
2409.06928v1 by Jianmei Jiang, Huijin Wang, Jieyun Bai, Shun Long, Shuangping Chen, Victor M. Campello, Karim Lekadir

The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a
pivotal step in monitoring labor progression and identifying potential delivery
complications. Despite the advances in deep learning, the lack of annotated
medical images hinders the training of segmentation. Traditional
semi-supervised learning approaches primarily utilize a unified network model
based on Convolutional Neural Networks (CNNs) and apply consistency
regularization to mitigate the reliance on extensive annotated data. However,
these methods often fall short in capturing the discriminative features of
unlabeled data and in delineating the long-range dependencies inherent in the
ambiguous boundaries of PSFH within ultrasound images. To address these
limitations, we introduce a novel framework, the Dual-Student and Teacher
Combining CNN and Transformer (DSTCT), which synergistically integrates the
capabilities of CNNs and Transformers. Our framework comprises a Vision
Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN.
This dual-student setup enables mutual supervision through the generation of
both hard and soft pseudo-labels, with the consistency in their predictions
being refined by minimizing the classifier determinacy discrepancy. The teacher
model further reinforces learning within this architecture through the
imposition of consistency regularization constraints. To augment the
generalization abilities of our approach, we employ a blend of data and model
perturbation techniques. Comprehensive evaluations on the benchmark dataset of
the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT
framework outperformed ten contemporary semi-supervised segmentation methods.
Code available at https://github.com/jjm1589/DSTCT.

ÊëòË¶ÅÔºöÊÅ•È™®ËÅØÂêàÂíåËÉéÈ†≠ÔºàPSFHÔºâÁöÑÂàÜÂâ≤ÊòØÁõ£Ê∏¨Áî¢Á®ãÈÄ≤Â∫¶ÂíåË≠òÂà•ÊΩõÂú®ÂàÜÂ®©‰ΩµÁôºÁóáÁöÑÈóúÈçµÊ≠•È©ü„ÄÇÂÑòÁÆ°Ê∑±Â∫¶Â≠∏ÁøíÂèñÂæóÈÄ≤Â±ïÔºå‰ΩÜÊ®ôË®ªÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÁº∫‰πèÈòªÁ§ô‰∫ÜÂàÜÂâ≤ÁöÑË®ìÁ∑¥„ÄÇÂÇ≥Áµ±ÁöÑÂçäÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ï‰∏ªË¶ÅÂà©Áî®Âü∫ÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàCNNÔºâÁöÑÁµ±‰∏ÄÁ∂≤Ë∑ØÊ®°ÂûãÔºå‰∏¶ÊáâÁî®‰∏ÄËá¥ÊÄßÊ≠£ÂâáÂåñ‰æÜÊ∏õËºïÂ∞çÂ§ßÈáèÊ®ôË®ªÊï∏ÊìöÁöÑ‰æùË≥¥„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÁÑ°Ê≥ïÊçïÊçâÊú™Ê®ôË®ªÊï∏ÊìöÁöÑÂçÄÂà•ÊÄßÁâπÂæµÔºå‰πüÁÑ°Ê≥ïÊèèÁπ™Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏≠ PSFH Ê®°Á≥äÈÇäÁïå‰∏≠Âõ∫ÊúâÁöÑÈï∑Á®ã‰æùË≥¥ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂç≥ÈõôÂ≠∏ÁîüÂíåÊïôÂ∏´ÁµêÂêà CNN Âíå TransformerÔºàDSTCTÔºâÔºåÂÆÉÂçîÂêåÊï¥Âêà‰∫Ü CNN Âíå Transformer ÁöÑÂäüËÉΩ„ÄÇÊàëÂÄëÁöÑÊ°ÜÊû∂ÂåÖÂê´‰∏ÄÂÄãË¶ñË¶∫ TransformerÔºàViTÔºâ‰ΩúÁÇ∫ÊïôÂ∏´ÂíåÂÖ©ÂÄãÂ≠∏ÁîüÊ®°ÂûãÔºå‰∏ÄÂÄã ViT Âíå‰∏ÄÂÄã CNN„ÄÇÈÄôÁ®ÆÈõôÂ≠∏ÁîüË®≠ÁΩÆÈÄöÈÅéÁîüÊàêÁ°¨ÂÅΩÊ®ôÁ±§ÂíåËªüÂÅΩÊ®ôÁ±§ÂØ¶ÁèæÁõ∏‰∫íÁõ£Áù£Ôºå‰∏¶ÈÄöÈÅéÊúÄÂ∞èÂåñÂàÜÈ°ûÂô®Á¢∫ÂÆöÊÄßÂ∑ÆÁï∞‰æÜÂÑ™ÂåñÂÖ∂È†êÊ∏¨ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊïôÂ∏´Ê®°ÂûãÈÄöÈÅéÊñΩÂä†‰∏ÄËá¥ÊÄßÊ≠£ÂâáÂåñÁ¥ÑÊùüÈÄ≤‰∏ÄÊ≠•Âä†Âº∑‰∫ÜÊ≠§Êû∂Êßã‰∏≠ÁöÑÂ≠∏Áøí„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ÊàëÂÄëÊñπÊ≥ïÁöÑÊ≥õÂåñËÉΩÂäõÔºåÊàëÂÄëÊé°Áî®‰∫ÜÊï∏ÊìöÂíåÊ®°ÂûãÊìæÂãïÊäÄË°ìÁöÑÊ∑∑Âêà„ÄÇÂú® MICCAI 2023 ÁöÑ PSFH ÂàÜÂâ≤Â§ßÊåëÊà∞Âü∫Ê∫ñÊï∏ÊìöÈõÜ‰∏äÁöÑÁ∂úÂêàË©ï‰º∞Ë°®ÊòéÔºåÊàëÂÄëÁöÑ DSTCT Ê°ÜÊû∂ÂÑ™ÊñºÂçÅÁ®ÆÁï∂‰ª£ÂçäÁõ£Áù£ÂºèÂàÜÂâ≤ÊñπÊ≥ï„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/jjm1589/DSTCT ÂèñÂæó„ÄÇ

##### **Bifurcation Identification for Ultrasound-driven Robotic Cannulation**
2409.06817v1 by Cecilia G. Morales, Dhruv Srikanth, Jack H. Good, Keith A. Dufendach, Artur Dubrawski

In trauma and critical care settings, rapid and precise intravascular access
is key to patients' survival. Our research aims at ensuring this access, even
when skilled medical personnel are not readily available. Vessel bifurcations
are anatomical landmarks that can guide the safe placement of catheters or
needles during medical procedures. Although ultrasound is advantageous in
navigating anatomical landmarks in emergency scenarios due to its portability
and safety, to our knowledge no existing algorithm can autonomously extract
vessel bifurcations using ultrasound images. This is primarily due to the
limited availability of ground truth data, in particular, data from live
subjects, needed for training and validating reliable models. Researchers often
resort to using data from anatomical phantoms or simulations. We introduce
BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a
novel algorithm that identifies vessel bifurcations and provides optimal needle
insertion sites for an autonomous robotic cannulation system. BIFURC integrates
expert knowledge with deep learning techniques to efficiently detect vessel
bifurcations within the femoral region and can be trained on a limited amount
of in-vivo data. We evaluated our algorithm using a medical phantom as well as
real-world experiments involving live pigs. In all cases, BIFURC consistently
identified bifurcation points and needle insertion locations in alignment with
those identified by expert clinicians.

ÊëòË¶ÅÔºöÂú®ÂâµÂÇ∑ÂíåÈáçÁóáÁÖßË≠∑Áí∞Â¢É‰∏≠ÔºåÂø´ÈÄü‰∏îÁ≤æÁ¢∫ÁöÑË°ÄÁÆ°ÂÖßÈÄöË∑ØÊòØÊÇ£ËÄÖÂ≠òÊ¥ªÁöÑÈóúÈçµ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Á¢∫‰øùÈÄôÁ®ÆÈÄöË∑ØÔºåÂç≥‰ΩøÂú®ÁÜüÁ∑¥ÁöÑÈÜ´ÁôÇ‰∫∫Âì°ÁÑ°Ê≥ïÁ´ãÂç≥Áç≤ÂæóÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇË°ÄÁÆ°ÂàÜÂèâÊòØËß£ÂâñÊ®ôË™åÔºåÂèØ‰ª•ÊåáÂ∞éÂú®ÈÜ´ÁôÇÈÅéÁ®ã‰∏≠ÂÆâÂÖ®ÊîæÁΩÆÂ∞éÁÆ°ÊàñÈáùÈ†≠„ÄÇÂÑòÁÆ°Ë∂ÖÈü≥Ê≥¢Áî±ÊñºÂÖ∂ÂèØÊîúÊÄßÂíåÂÆâÂÖ®ÊÄßËÄåÂú®Á∑äÊÄ•ÊÉÖÊ≥Å‰∏ãÂ∞éËà™Ëß£ÂâñÊ®ôË™åÂÖ∑ÊúâÂÑ™Âã¢Ôºå‰ΩÜÊìöÊàëÂÄëÊâÄÁü•ÔºåÊ≤íÊúâÁèæÊúâÊºîÁÆóÊ≥ïÂèØ‰ª•‰ΩøÁî®Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèËá™ÂãïÊèêÂèñË°ÄÁÆ°ÂàÜÂèâ„ÄÇÈÄô‰∏ªË¶ÅÊòØÁî±ÊñºÂú∞Èù¢ÂØ¶Ê≥ÅË≥áÊñôÁöÑÂèØÁî®ÊÄßÊúâÈôêÔºåÁâπÂà•ÊòØ‰æÜËá™Ê¥ªÈ´îÂèóË©¶ËÄÖÁöÑË≥áÊñôÔºåËÄåÈÄôÂ∞çÊñºË®ìÁ∑¥ÂíåÈ©óË≠âÂèØÈù†Ê®°ÂûãÊòØÂøÖÈúÄÁöÑ„ÄÇÁ†îÁ©∂‰∫∫Âì°Á∂ìÂ∏∏Ê±ÇÂä©Êñº‰ΩøÁî®Ëß£ÂâñÊ®°ÂûãÊàñÊ®°Êì¨ÁöÑË≥áÊñô„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü BIFURCÔºåÂç≥Ë∂ÖÈü≥Ê≥¢È©ÖÂãïÊ©üÂô®‰∫∫ÊèíÁÆ°ÁöÑÂàÜÂèâË≠òÂà•ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊºîÁÆóÊ≥ïÔºåÂèØ‰ª•Ë≠òÂà•Ë°ÄÁÆ°ÂàÜÂèâÔºå‰∏¶ÁÇ∫Ëá™ÂãïÊ©üÂô®‰∫∫ÊèíÁÆ°Á≥ªÁµ±Êèê‰æõÊúÄ‰Ω≥ÈáùÈ†≠ÊèíÂÖ•‰ΩçÁΩÆ„ÄÇBIFURC Â∞áÂ∞àÂÆ∂Áü•Ë≠òËàáÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÁõ∏ÁµêÂêàÔºå‰ª•ÊúâÊïàÊ™¢Ê∏¨ËÇ°È™®ÂçÄÂüüÂÖßÁöÑË°ÄÁÆ°ÂàÜÂèâÔºå‰∏¶‰∏îÂèØ‰ª•Âú®ÊúâÈôêÁöÑÈ´îÂÖßË≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄë‰ΩøÁî®ÈÜ´Áî®Ê®°Âûã‰ª•ÂèäÊ∂âÂèäÊ¥ªÈ´îË±¨ÁöÑÁúüÂØ¶‰∏ñÁïåÂØ¶È©óË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï„ÄÇÂú®ÊâÄÊúâÊÉÖÊ≥Å‰∏ãÔºåBIFURC ÈÉΩ‰∏ÄËá¥Âú∞Ë≠òÂà•Âá∫ÂàÜÂèâÈªûÂíåÈáùÈ†≠ÊèíÂÖ•‰ΩçÁΩÆÔºåËàáÂ∞àÂÆ∂Ëá®Â∫äÈÜ´ÁîüË≠òÂà•ÁöÑ‰ΩçÁΩÆ‰∏ÄËá¥„ÄÇ

##### **Personalized Federated Learning Techniques: Empirical Analysis**
2409.06805v1 by Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

Personalized Federated Learning (pFL) holds immense promise for tailoring
machine learning models to individual users while preserving data privacy.
However, achieving optimal performance in pFL often requires a careful
balancing act between memory overhead costs and model accuracy. This paper
delves into the trade-offs inherent in pFL, offering valuable insights for
selecting the right algorithms for diverse real-world scenarios. We empirically
evaluate ten prominent pFL techniques across various datasets and data splits,
uncovering significant differences in their performance. Our study reveals
interesting insights into how pFL methods that utilize personalized (local)
aggregation exhibit the fastest convergence due to their efficiency in
communication and computation. Conversely, fine-tuning methods face limitations
in handling data heterogeneity and potential adversarial attacks while
multi-objective learning methods achieve higher accuracy at the cost of
additional training and resource consumption. Our study emphasizes the critical
role of communication efficiency in scaling pFL, demonstrating how it can
significantly affect resource usage in real-world deployments.

ÊëòË¶ÅÔºöÂÄã‰∫∫ÂåñËÅØÂêàÂ≠∏Áøí (pFL) Âú®Á∂≠Ë≠∑Ë≥áÊñôÈö±ÁßÅÁöÑÂêåÊôÇÔºåÁÇ∫ÂÆ¢Ë£ΩÂåñÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁµ¶ÂÄãÂà•‰ΩøÁî®ËÄÖÂ∏∂‰æÜÊ•µÂ§ßÁöÑÂ∏åÊúõ„ÄÇÁÑ∂ËÄåÔºåË¶ÅÈÅîÊàê pFL ÁöÑÊúÄ‰Ω≥ÊïàËÉΩÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂú®Ë®òÊÜ∂È´îÈñãÈä∑ÊàêÊú¨ÂíåÊ®°ÂûãÊ∫ñÁ¢∫Â∫¶‰πãÈñìÂèñÂæó‰ªîÁ¥∞ÁöÑÂπ≥Ë°°„ÄÇÊú¨ÊñáÊ∑±ÂÖ•Êé¢Ë®é pFL ‰∏≠Âõ∫ÊúâÁöÑÊ¨äË°°ÂèñÊç®ÔºåÁÇ∫Âú®ÂêÑÁ®ÆÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÈÅ∏ÊìáÊ≠£Á¢∫ÁöÑÊºîÁÆóÊ≥ïÊèê‰æõÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇÊàëÂÄëÊ†πÊìöÂêÑÁ®ÆË≥áÊñôÈõÜÂíåË≥áÊñôÂàÜÂâ≤ÔºåÂ∞çÂçÅÁ®ÆÂÇëÂá∫ÁöÑ pFL ÊäÄË°ìÈÄ≤Ë°åÂØ¶Ë≠âË©ï‰º∞ÔºåÊè≠Èú≤ÂÖ∂ÊïàËÉΩÁöÑÈ°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êè≠Èú≤‰∫ÜÊúâË∂£ÁöÑË¶ãËß£ÔºåË™™ÊòéÂà©Áî®ÂÄã‰∫∫Âåñ (Â±ÄÈÉ®) ËÅöÂêàÁöÑ pFL ÊñπÊ≥ïÔºåÁî±ÊñºÂÖ∂Âú®ÈÄöË®äÂíåÈÅãÁÆóÊñπÈù¢ÁöÑÊïàÁéáÔºåÂ±ïÁèæÂá∫ÊúÄÂø´ÁöÑÊî∂ÊñÇÈÄüÂ∫¶„ÄÇÁõ∏ÂèçÂú∞ÔºåÂæÆË™øÊñπÊ≥ïÂú®ËôïÁêÜË≥áÊñôÁï∞Ë≥™ÊÄßÂíåÊΩõÂú®Â∞çÊäóÊîªÊìäÊñπÈù¢Èù¢Ëá®ÈôêÂà∂ÔºåËÄåÂ§öÁõÆÊ®ôÂ≠∏ÁøíÊñπÊ≥ïÂâá‰ª•È°çÂ§ñÁöÑË®ìÁ∑¥ÂíåË≥áÊ∫êÊ∂àËÄóÁÇ∫‰ª£ÂÉπÔºåÈÅîÂà∞Êõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÈÄöË®äÊïàÁéáÂú®Êì¥ÂÖÖ pFL ‰∏≠ÁöÑÈóúÈçµËßíËâ≤ÔºåÂ±ïÁ§∫ÂÆÉÂ¶Ç‰ΩïÂú®ÂØ¶ÈöõÈÉ®ÁΩ≤‰∏≠È°ØËëóÂΩ±ÈüøË≥áÊ∫ê‰ΩøÁî®„ÄÇ

##### **Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**
2409.06672v1 by Cristian Trout

Many experts believe that AI systems will sooner or later pose uninsurable
risks, including existential risks. This creates an extreme judgment-proof
problem: few if any parties can be held accountable ex post in the event of
such a catastrophe. This paper proposes a novel solution: a
government-provided, mandatory indemnification program for AI developers. The
program uses risk-priced indemnity fees to induce socially optimal levels of
care. Risk-estimates are determined by surveying experts, including indemnified
developers. The Bayesian Truth Serum mechanism is employed to incent honest and
effortful responses. Compared to alternatives, this approach arguably better
leverages all private information, and provides a clearer signal to indemnified
developers regarding what risks they must mitigate to lower their fees. It's
recommended that collected fees be used to help fund the safety research
developers need, employing a fund matching mechanism (Quadratic Financing) to
induce an optimal supply of this public good. Under Quadratic Financing, safety
research projects would compete for private contributions from developers,
signaling how much each is to be supplemented with public funds.

ÊëòË¶ÅÔºöË®±Â§öÂ∞àÂÆ∂Áõ∏‰ø° AI Á≥ªÁµ±ÈÅ≤Êó©ÊúÉÈÄ†ÊàêÁÑ°Ê≥ïÊâø‰øùÁöÑÈ¢®Èö™ÔºåÂåÖÊã¨ÁîüÂ≠òÈ¢®Èö™„ÄÇÈÄôÊúÉÈÄ†ÊàêÊ•µÁ´ØÁöÑÁÑ°Ê≥ïËøΩÁ©∂Ë≤¨‰ªªÂïèÈ°åÔºöÂú®ÁôºÁîüÊ≠§È°ûÁÅΩÈõ£ÊôÇÔºåÂπæ‰πéÊ≤íÊúâ‰ªª‰Ωï‰∏ÄÊñπÂèØ‰ª•‰∫ãÂæåË¢´ËøΩÁ©∂Ë≤¨‰ªª„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑËß£Ê±∫ÊñπÊ°àÔºöÊîøÂ∫úÊèê‰æõÁöÑ AI ÈñãÁôº‰∫∫Âì°Âº∑Âà∂ÊÄßË£úÂÑüË®àÁï´„ÄÇË©≤Ë®àÁï´‰ΩøÁî®È¢®Èö™ÂÆöÂÉπÁöÑË£úÂÑüË≤ªÁî®‰æÜË™ò‰ΩøÈÅîÂà∞Á§æÊúÉÊúÄÈÅ©Á®ãÂ∫¶ÁöÑÁÖßË≠∑„ÄÇÈ¢®Èö™‰º∞Ë®àÂÄºÊòØÁî±Ë™øÊü•Â∞àÂÆ∂ÔºàÂåÖÊã¨Áç≤ÂæóË£úÂÑüÁöÑÈñãÁôº‰∫∫Âì°Ôºâ‰æÜÊ±∫ÂÆö„ÄÇË≤ùÊ∞èÁúüË©±Ë°ÄÊ∏ÖÊ©üÂà∂Ë¢´Áî®‰æÜÊøÄÂãµË™†ÂØ¶‰∏îÂä™ÂäõÁöÑÂõûÊáâ„ÄÇËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•Ë™™ËÉΩÊõ¥Â•ΩÂú∞Âà©Áî®ÊâÄÊúâÁßÅ‰∫∫Ë≥áË®äÔºå‰∏¶ÂêëÁç≤ÂæóË£úÂÑüÁöÑÈñãÁôº‰∫∫Âì°Êèê‰æõÊõ¥ÊòéÁ¢∫ÁöÑË®äËôüÔºåË™™Êòé‰ªñÂÄëÂøÖÈ†àÊ∏õËºïÂì™‰∫õÈ¢®Èö™ÊâçËÉΩÈôç‰ΩéË≤ªÁî®„ÄÇÂª∫Ë≠∞Â∞áÊî∂ÂèñÁöÑË≤ªÁî®Áî®ÊñºË≥áÂä©ÂÆâÂÖ®Á†îÁ©∂ÈñãÁôº‰∫∫Âì°ÊâÄÈúÄÁöÑÁ†îÁ©∂Ôºå‰∏¶Êé°Áî®Âü∫ÈáëÈÖçÂ∞çÊ©üÂà∂Ôºà‰∫åÊ¨°ÊñπËûçË≥áÔºâ‰æÜË™ò‰ΩøÊèê‰æõÈÄôÁ®ÆÂÖ¨ÂÖ±Ë≤°ÁöÑÊúÄ‰Ω≥‰æõÊáâ„ÄÇÂú®‰∫åÊ¨°ÊñπËûçË≥á‰∏ãÔºåÂÆâÂÖ®Á†îÁ©∂Ë®àÁï´Â∞áÁ´∂Áà≠ÈñãÁôº‰∫∫Âì°ÁöÑÁßÅ‰∫∫ÊçêÊ¨æÔºå‰∏¶Ë°®Á§∫ÂÖ∂‰∏≠ÊúâÂ§öÂ∞ëÂ∞áÁî±ÂÖ¨ÂÖ±Ë≥áÈáëË£úÂÖÖ„ÄÇ

##### **EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**
2409.06644v2 by Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He

Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.

ÊëòË¶ÅÔºöÊó©ÊúüÂÅµÊ∏¨ÈùíÂÖâÁúº„ÄÅÈªÉÊñëÈÉ®ÁóÖËÆäÂíåÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆäÁ≠âÁúºÁñæÂ∞çÊñºÈ†êÈò≤Ë¶ñÂäõÂñ™Â§±Ëá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖß (AI) Âü∫Á§éÊ®°ÂûãÂú®ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÊñπÈù¢Ê•µÂÖ∑ÂâçÊôØÔºå‰ΩÜÁèæÊúâÁöÑÁúºÁßëÂü∫Á§éÊ®°Âûã‰∏ªË¶ÅÈóúÊ≥®ÊñºÂñÆ‰∏ÄÊ®°ÂºèÔºåËÄåË®∫Êñ∑ÁúºÁñæÈúÄË¶ÅÂ§öÁ®ÆÊ®°Âºè„ÄÇ‰∏ÄÂÄãÈáçË¶Å‰ΩÜÁ∂ìÂ∏∏Ë¢´ÂøΩË¶ñÁöÑÊñπÈù¢ÊòØÂà©Áî®Âêå‰∏ÄÊÇ£ËÄÖ‰∏çÂêåÊ®°ÂºèÁöÑÂ§öË¶ñÂúñË≥áË®ä„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÁúºÁßëÁñæÁóÖÁöÑÈï∑Â∞æÊÄßË≥™ÔºåÊ®ôÊ∫ñÁöÑÂÖ®Áõ£Áù£ÊàñÁÑ°Áõ£Áù£Â≠∏ÁøíÊñπÊ≥ïÈÄöÂ∏∏Èõ£‰ª•Êáâ‰ªò„ÄÇÂõ†Ê≠§ÔºåÊï¥ÂêàËá®Â∫äÊñáÊú¨‰ª•Ê∂µËìãÊõ¥Âª£Ê≥õÁöÑÁñæÁóÖË≠úÁ≥ªËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊèêÂá∫ EyeCLIPÔºåÈÄôÊòØ‰∏ÄÂÄãË¶ñË¶∫Ë™ûË®ÄÂü∫Á§éÊ®°ÂûãÔºå‰ΩøÁî®Ë∂ÖÈÅé 277 Ëê¨ÂºµÂÖ∑ÊúâÈÉ®ÂàÜÊñáÂ≠óË≥áÊñôÁöÑÂ§öÊ®°ÂºèÁúºÁßëÂΩ±ÂÉèÈñãÁôºËÄåÊàê„ÄÇÁÇ∫‰∫ÜÂÖÖÂàÜÂà©Áî®Â§ßÈáèÁöÑÂ§öÊ®°ÂºèÊú™Ê®ôË®òÂíåÊ®ôË®òË≥áÊñôÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÈ†êË®ìÁ∑¥Á≠ñÁï•ÔºåÁµêÂêà‰∫ÜËá™ÊàëÁõ£Áù£ÈáçÂª∫„ÄÅÂ§öÊ®°ÂºèÂΩ±ÂÉèÂ∞çÊØîÂ≠∏ÁøíÂíåÂΩ±ÂÉèÊñáÂ≠óÂ∞çÊØîÂ≠∏ÁøíÔºå‰ª•Â≠∏ÁøíÂ§öÁ®ÆÊ®°ÂºèÁöÑÂÖ±‰∫´Ë°®Âæµ„ÄÇÈÄèÈÅé‰ΩøÁî® 14 ÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÈÄ≤Ë°åË©ï‰º∞ÔºåEyeCLIP ÂèØ‰ª•ËΩâÁßªÂà∞Ê∂âÂèäÁúºÈÉ®ÂíåÂÖ®Ë∫´ÁñæÁóÖÁöÑÂª£Ê≥õ‰∏ãÊ∏∏‰ªªÂãôÔºåÂú®ÁñæÁóÖÂàÜÈ°û„ÄÅË¶ñË¶∫ÂïèÈ°åËß£Á≠îÂíåË∑®Ê®°ÂºèÊ™¢Á¥¢‰∏≠ÂØ¶ÁèæÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇEyeCLIP ‰ª£Ë°®‰∫ÜÂ∞çÂÖàÂâçÊñπÊ≥ïÁöÑÈáçÂ§ßÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂú®ÁèæÂØ¶‰∏ñÁïåÈï∑Â∞æÂ†¥ÊôØ‰∏≠Â±ïÁ§∫‰∫ÜÂ∞èÊ®£Êú¨ÔºåÁîöËá≥Èõ∂Ê®£Êú¨ÁöÑËÉΩÂäõ„ÄÇ

##### **Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**
2409.06585v1 by Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton

Background: Hip replacement procedures improve patient lives by relieving
pain and restoring mobility. Predicting hip replacement in advance could reduce
pain by enabling timely interventions, prioritising individuals for surgery or
rehabilitation, and utilising physiotherapy to potentially delay the need for
joint replacement. This study predicts hip replacement a year in advance to
enhance quality of life and health service efficiency. Methods: Adapting
previous work using Temporal Graph Convolutional Neural Network (TG-CNN)
models, we construct temporal graphs from primary care medical event codes,
sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip
replacement risk. We match hip replacement cases to controls by age, sex, and
Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187
controls, predicts hip replacement one year in advance. We validate the model
on two unseen datasets, recalibrating for class imbalance. Additionally, we
conduct an ablation study and compare against four baseline models. Results:
Our best model predicts hip replacement risk one year in advance with an AUROC
of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),
achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after
recalibration. Conclusions: The TG-CNN model effectively predicts hip
replacement risk by identifying patterns in patient trajectories, potentially
improving understanding and management of hip-related conditions.

ÊëòË¶ÅÔºöËÉåÊôØÔºöÈ´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÂèØÊ∏õËºïÁñºÁóõ‰∏¶ÊÅ¢Âæ©Ë°åÂãïËÉΩÂäõÔºåÈÄ≤ËÄåÊîπÂñÑÊÇ£ËÄÖÁîüÊ¥ª„ÄÇÈ†êÊ∏¨È´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÊúâÂä©ÊñºÂèäÊôÇ‰ªãÂÖ•„ÄÅÂÑ™ÂÖàÂÆâÊéíÂÄã‰∫∫ÈÄ≤Ë°åÊâãË°ìÊàñÂæ©ÂÅ•Ôºå‰∏¶Âà©Áî®Áâ©ÁêÜÊ≤ªÁôÇ‰æÜÂª∂Á∑©ÈóúÁØÄÁΩÆÊèõÊâãË°ìÁöÑÂøÖË¶ÅÊÄßÔºåÈÄ≤ËÄåÊ∏õÂ∞ëÁñºÁóõ„ÄÇÊú¨Á†îÁ©∂È†êÊ∏¨‰∏ÄÂπ¥ÂæåÁöÑÈ´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÔºå‰ª•ÊèêÂçáÁîüÊ¥ªÂìÅË≥™ÂíåÈÜ´ÁôÇÊúçÂãôÊïàÁéá„ÄÇÊñπÊ≥ïÔºöÊé°Áî®ÊôÇÈñìÂúñÂΩ¢Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (TG-CNN) Ê®°ÂûãÊîπÁ∑®ÂÖàÂâçÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÂæû ResearchOne EHR 40-75 Ê≠≤ÊÇ£ËÄÖÁöÑ‰∏ªË¶ÅÁÖßË≠∑ÈÜ´ÁôÇ‰∫ã‰ª∂‰ª£Á¢ºÂª∫ÊßãÊôÇÈñìÂúñÂΩ¢Ôºå‰ª•È†êÊ∏¨È´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÈ¢®Èö™„ÄÇÊàëÂÄëÊ†πÊìöÂπ¥ÈΩ°„ÄÅÊÄßÂà•ÂíåÂ§öÈáçÂâùÂ•™ÊåáÊï∏ÔºåÂ∞áÈ´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÁóÖ‰æãËàáÂ∞çÁÖßÁµÑÈÄ≤Ë°åÈÖçÂ∞ç„ÄÇË©≤Ê®°ÂûãÈáùÂ∞ç 9,187 ÂÄãÁóÖ‰æãÂíå 9,187 ÂÄãÂ∞çÁÖßÁµÑÈÄ≤Ë°åË®ìÁ∑¥ÔºåÈ†êÊ∏¨‰∏ÄÂπ¥ÂæåÁöÑÈ´ñÈóúÁØÄÁΩÆÊèõÊâãË°ì„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÊú™Ë¶ãÊï∏ÊìöÈõÜÈ©óË≠âÊ®°ÂûãÔºå‰∏¶ÈáçÊñ∞Ê†°Ê∫ñ‰ª•Ëß£Ê±∫È°ûÂà•‰∏çÂπ≥Ë°°ÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÊ∂àËûçÁ†îÁ©∂Ôºå‰∏¶ËàáÂõõÂÄãÂü∫Ê∫ñÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúÔºöÊàëÂÄëÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÈ†êÊ∏¨‰∏ÄÂπ¥ÂæåÁöÑÈ´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÈ¢®Èö™ÔºåAUROC ÁÇ∫ 0.724 (95% CIÔºö0.715-0.733)ÔºåAUPRC ÁÇ∫ 0.185 (95% CIÔºö0.160-0.209)ÔºåÈáçÊñ∞Ê†°Ê∫ñÂæåÊ†°Ê∫ñÊñúÁéáÁÇ∫ 1.107 (95% CIÔºö1.074-1.139)„ÄÇÁµêË´ñÔºöTG-CNN Ê®°ÂûãÂèØÊúâÊïàÈ†êÊ∏¨È´ñÈóúÁØÄÁΩÆÊèõÊâãË°ìÈ¢®Èö™ÔºåÊñπÊ≥ïÊòØÊâæÂá∫ÊÇ£ËÄÖËªåË∑°‰∏≠ÁöÑÊ®°ÂºèÔºåÈÄ≤ËÄåÊΩõÂú®ÊîπÂñÑÂ∞çÈ´ñÈóúÁØÄÁõ∏ÈóúÁñæÁóÖÁöÑ‰∫ÜËß£ÂíåÁÆ°ÁêÜ„ÄÇ

##### **MAGDA: Multi-agent guideline-driven diagnostic assistance**
2409.06351v1 by David Bani-Harouni, Nassir Navab, Matthias Keicher

In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.

ÊëòË¶ÅÔºöÂú®ÊÄ•Ë®∫ÂÆ§„ÄÅÈÑâÊùëÈÜ´Èô¢ÊàñÊ¨†ÁôºÈÅîÂú∞ÂçÄÁöÑË®∫ÊâÄÔºåËá®Â∫äÈÜ´Â∏´Â∏∏Â∏∏Áº∫‰πèÂèóÈÅéË®ìÁ∑¥ÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°åÂø´ÈÄüÁöÑÂΩ±ÂÉèÂàÜÊûêÔºåÈÄôÂèØËÉΩÊúÉÂ∞çÁóÖÊÇ£ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÈÄ†Êàê‰∏çÂà©ÂΩ±Èüø„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúâÊΩõÂäõÊ∏õËºïÈÄô‰∫õËá®Â∫äÈÜ´Â∏´ÁöÑ‰∏Ä‰∫õÂ£ìÂäõÔºåÊñπÊ≥ïÊòØÊèê‰æõË¶ãËß£ÔºåÂçîÂä©‰ªñÂÄëÈÄ≤Ë°åÊ±∫Á≠ñ„ÄÇÂÑòÁÆ°ÈÄô‰∫õ LLM Âú®Â±ïÁ§∫ÂÖ∂Ë±êÂØåÁöÑÁêÜË´ñÈÜ´Â≠∏Áü•Ë≠òÁöÑÈÜ´Â≠∏ËÄÉË©¶‰∏≠Áç≤Âæó‰∫ÜÂæàÈ´òÁöÑÊ∏¨Ë©¶ÁµêÊûúÔºå‰ΩÜÂÆÉÂÄëÂæÄÂæÄ‰∏çÈÅµÂæ™ÈÜ´ÁôÇÊåáÂçó„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊåáÂ∞éÊñπÈáùÈ©ÖÂãïÊ±∫Á≠ñÊîØÊè¥ÊñπÊ≥ï„ÄÇÊàëÂÄëÊ®°Êì¨‰∫Ü‰∏ÄÂÄãÂ§öÂÄã LLM ‰ª£ÁêÜÁ≥ªÁµ±Ôºå‰∏¶Â¢ûÂº∑‰∫Ü‰∏ÄÂÄãÂ∞çÊØîË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂçî‰Ωú‰ª•ÈÅîÊàêÁóÖÊÇ£Ë®∫Êñ∑„ÄÇÂú®ÁÇ∫‰ª£ÁêÜÊèê‰æõÁ∞°ÂñÆÁöÑË®∫Êñ∑ÊåáÂçóÂæåÔºåÂÆÉÂÄëÂ∞áÁ∂úÂêàÊèêÁ§∫‰∏¶Ê†πÊìöÈÄô‰∫õÊåáÂçóÁØ©ÈÅ∏ÂΩ±ÂÉè‰ª•ÊâæÂá∫ÁôºÁèæ„ÄÇÊúÄÂæåÔºåÂÆÉÂÄëÁÇ∫ÂÖ∂Ë®∫Êñ∑Êèê‰æõÂèØ‰ª•ÁêÜËß£ÁöÑÊÄùË∑ØÊé®ÁêÜÔºåÁÑ∂ÂæåËá™ÊàëÁ≤æÈÄ≤‰ª•ËÄÉÈáèÁñæÁóÖ‰πãÈñìÁöÑÁõ∏‰∫í‰æùÂ≠òÈóú‰øÇ„ÄÇÁî±ÊñºÊàëÂÄëÁöÑÊ®°ÂûãÊòØÈõ∂Ê¨°Â≠∏ÁøíÔºåÂõ†Ê≠§ÂÆÉÂèØ‰ª•ÈÅ©ÊáâÁΩïË¶ãÁñæÁóÖÁöÑË®≠ÂÆöÔºåÂú®ÈÄôÁ®ÆË®≠ÂÆö‰∏≠ÔºåË®ìÁ∑¥Ë≥áÊñôÊúâÈôêÔºå‰ΩÜÊúâÂ∞àÂÆ∂Ë£Ω‰ΩúÁöÑÁñæÁóÖÊèèËø∞ÂèØÁî®„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãËÉ∏ÈÉ® X ÂÖâÁâáË≥áÊñôÈõÜÔºåCheXpert Âíå ChestX-ray 14 LongtailÔºåË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÁõ∏ËºÉÊñºÁèæÊúâÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÁöÑÊïàËÉΩÊèêÂçáÔºå‰ª•ÂèäÂ∞çÁΩïË¶ãÁñæÁóÖÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇ

##### **Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**
2409.06209v1 by Xin Zhang, Deval Mehta, Yanan Hu, Chao Zhu, David Darby, Zhen Yu, Daniel Merlo, Melissa Gresle, Anneke Van Der Walt, Helmut Butzkueven, Zongyuan Ge

Survival analysis holds a crucial role across diverse disciplines, such as
economics, engineering and healthcare. It empowers researchers to analyze both
time-invariant and time-varying data, encompassing phenomena like customer
churn, material degradation and various medical outcomes. Given the complexity
and heterogeneity of such data, recent endeavors have demonstrated successful
integration of deep learning methodologies to address limitations in
conventional statistical approaches. However, current methods typically involve
cluttered probability distribution function (PDF), have lower sensitivity in
censoring prediction, only model static datasets, or only rely on recurrent
neural networks for dynamic modelling. In this paper, we propose a novel
survival regression method capable of producing high-quality unimodal PDFs
without any prior distribution assumption, by optimizing novel
Margin-Mean-Variance loss and leveraging the flexibility of Transformer to
handle both temporal and non-temporal data, coined UniSurv. Extensive
experiments on several datasets demonstrate that UniSurv places a significantly
higher emphasis on censoring compared to other methods.

ÊëòË¶ÅÔºöÂ≠òÊ¥ªÂàÜÊûêÂú®Á∂ìÊøü„ÄÅÂ∑•Á®ãÂíåÈÜ´ÁôÇ‰øùÂÅ•Á≠â‰∏çÂêåÂ≠∏Áßë‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÂÆÉËÆìÁ†îÁ©∂‰∫∫Âì°ËÉΩÂ§†ÂàÜÊûêÊôÇ‰∏çËÆäÂíåÊôÇËÆäÊï∏ÊìöÔºåÂåÖÂê´ÂÆ¢Êà∂ÊµÅÂ§±„ÄÅÊùêÊñôÈôçËß£ÂíåÂêÑÁ®ÆÈÜ´ÁôÇÁµêÊûúÁ≠âÁèæË±°„ÄÇÈëëÊñºÊ≠§È°ûÊï∏ÊìöÁöÑË§áÈõúÊÄßÂíåÁï∞Ë≥™ÊÄßÔºåÊúÄËøëÁöÑÂä™ÂäõÂ∑≤Ë≠âÊòéÊàêÂäüÊï¥ÂêàÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ï‰ª•Ëß£Ê±∫ÂÇ≥Áµ±Áµ±Ë®àÊñπÊ≥ïÁöÑÈôêÂà∂„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊñπÊ≥ïÈÄöÂ∏∏Ê∂âÂèäÈõú‰∫ÇÁöÑÊ©üÁéáÂàÜ‰ΩàÂáΩÊï∏ (PDF)ÔºåÂú®ÂØ©Êü•È†êÊ∏¨‰∏≠ÂÖ∑ÊúâËºÉ‰ΩéÁöÑÊïèÊÑüÊÄßÔºåÂÉÖÂ∞çÈùúÊÖãÊï∏ÊìöÈõÜÈÄ≤Ë°åÂª∫Ê®°ÔºåÊàñÂÉÖ‰æùË≥¥ÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑ØÈÄ≤Ë°åÂãïÊÖãÂª∫Ê®°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ≠òÊ¥ªËø¥Ê≠∏ÊñπÊ≥ïÔºåËÉΩÂ§†Âú®Ê≤íÊúâ‰ªª‰ΩïÂÖàÈ©óÂàÜ‰ΩàÂÅáË®≠ÁöÑÊÉÖÊ≥Å‰∏ãÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂñÆÂ≥∞ PDFÔºåËóâÁî±ÊúÄ‰Ω≥ÂåñÊñ∞Á©éÁöÑÈÇäÈöõÂπ≥ÂùáÂÄºËÆäÁï∞ÊêçÂ§±Ôºå‰∏¶Âà©Áî® Transformer ÁöÑÈùàÊ¥ªÊÄß‰æÜËôïÁêÜÊôÇÈñìÂíåÈùûÊôÇÈñìÊï∏ÊìöÔºåÁ®±ÁÇ∫ UniSurv„ÄÇÂú®ÂπæÂÄãÊï∏ÊìöÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåUniSurv Â∞çÂØ©Êü•ÁöÑÈáçË¶ñÁ®ãÂ∫¶È°ØËëóÊèêÈ´ò„ÄÇ

##### **Can Large Language Models Unlock Novel Scientific Research Ideas?**
2409.06185v1 by Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal

"An idea is nothing more nor less than a new combination of old elements"
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.

ÊëòË¶ÅÔºö„Äå‰∏ÄÂÄãÊÉ≥Ê≥ï‰∏çÈÅéÂ∞±ÊòØËàäÂÖÉÁ¥†ÁöÑÊñ∞ÁµÑÂêàËÄåÂ∑≤„Äç
(Young, J.W.)„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂÖ¨ÈñãÁöÑ ChatGPT Âª£Ê≥õÊé°Áî®ÔºåÊ®ôË™åËëó‰∫∫Â∑•Êô∫ËÉΩ (AI) Êï¥ÂêàÂà∞‰∫∫ÂÄëÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑÈáçË¶ÅËΩâÊäòÈªû„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü LLM Âú®Ê†πÊìöÁ†îÁ©∂Ë´ñÊñáË≥áË®äÁî¢ÁîüÊñ∞Á†îÁ©∂ÊÉ≥Ê≥ïÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂ∞ç‰∫îÂÄãÈ†òÂüüÔºà‰æãÂ¶ÇÂåñÂ≠∏„ÄÅÈõªËÖ¶„ÄÅÁ∂ìÊøü„ÄÅÈÜ´Â≠∏ÂíåÁâ©ÁêÜÔºâ‰∏≠ÁöÑ 4 ÂÄã LLM ÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÊ™¢Êü•„ÄÇÊàëÂÄëÁôºÁèæ Claude-2 Âíå GPT-4 Áî¢ÁîüÁöÑÊú™‰æÜÁ†îÁ©∂ÊÉ≥Ê≥ïÊØî GPT-3.5 Âíå Gemini Êõ¥Á¨¶Âêà‰ΩúËÄÖÁöÑËßÄÈªû„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåClaude-2 Áî¢ÁîüÁöÑÊú™‰æÜÁ†îÁ©∂ÊÉ≥Ê≥ïÊØî GPT-4„ÄÅGPT-3.5 Âíå Gemini 1.0 Êõ¥ÁÇ∫Â§öÊ®£Âåñ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Â∞çÁî¢ÁîüÁöÑÊú™‰æÜÁ†îÁ©∂ÊÉ≥Ê≥ïÁöÑÊñ∞Á©éÊÄß„ÄÅÁõ∏ÈóúÊÄßÂíåÂèØË°åÊÄßÈÄ≤Ë°å‰∫Ü‰∫∫Â∑•Ë©ï‰º∞„ÄÇÊú¨Ë™øÊü•Êèê‰æõ‰∫ÜÂ∞ç LLM Âú®Áî¢ÁîüÊÉ≥Ê≥ï‰∏≠‰∏çÊñ∑ÊºîËÆäÁöÑËßíËâ≤ÁöÑË¶ãËß£ÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂ËÉΩÂäõÂíåÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊúâÂä©ÊñºË©ï‰º∞ÂíåÂà©Áî®Ë™ûË®ÄÊ®°Âûã‰æÜÁî¢ÁîüÊú™‰æÜÁ†îÁ©∂ÊÉ≥Ê≥ïÁöÑÊåÅÁ∫åÂä™Âäõ„ÄÇÊàëÂÄëÂÖ¨ÈñãÊèê‰æõÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÂíåÁ®ãÂºèÁ¢º„ÄÇ

##### **Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**
2409.06173v2 by Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan

In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑËÑàÁµ°Â≠∏Áøí (ICL) Â∑≤ÊàêÁÇ∫Âü∑Ë°åËá™ÁÑ∂Ë™ûË®Ä‰ªªÂãôÁöÑ‰∏ªÊµÅÊäÄË°ìÔºåÂõ†ÁÇ∫ÂÆÉ‰∏çÈúÄË¶Å‰ΩøÁî®Âü∫ÊñºÊ¢ØÂ∫¶ÁöÑÊ®°Âûã‰æÜÊõ¥Êñ∞Ê®°ÂûãÂèÉÊï∏„ÄÇICL ÊâøË´æ‰ª•Ê•µ‰ΩéÁöÑË®àÁÆóÊàêÊú¨„ÄåÈÅ©Êáâ„ÄçLLM ‰ª•Âú®Á´∂Áà≠ÊàñÊúÄÂÖàÈÄ≤ÁöÑÂ±§Á¥öÂü∑Ë°åÁï∂Ââç‰ªªÂãô„ÄÇICL ÂèØ‰ª•ÈÄèÈÅéÂú®ÊèêÁ§∫‰∏≠ÊòéÁ¢∫Âú∞Á¥çÂÖ•Êé®ÁêÜÈÅéÁ®ã‰æÜÊì¥ÂÖÖÔºå‰ª•ÂæóÂá∫ÊúÄÁµÇÊ®ôÁ±§ÔºåÈÄôÈ†ÖÊäÄË°ìÁ®±ÁÇ∫ÊÄùËÄÉÈèà (CoT) ÊèêÁ§∫„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂ÁôºÁèæÔºåICL ‰∏ªË¶Å‰æùË≥¥‰ªªÂãôÂÖàÈ©óÁöÑÊ™¢Á¥¢ÔºåËºÉÂ∞ë‰æùË≥¥„ÄåÂ≠∏Áøí„Äç‰æÜÂü∑Ë°å‰ªªÂãôÔºåÁâπÂà•ÊòØÂ∞çÊñºÊÉÖÁ∑íÂíåÈÅìÂæ∑Á≠âË§áÈõúÁöÑ‰∏ªËßÄÈ†òÂüüÔºåÂÖ∂‰∏≠ÂÖàÈ©óÊúÉÂÉµÂåñÂæåÈ©óÈ†êÊ∏¨„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é„ÄåÂïüÁî®„ÄçÊé®ÁêÜÊòØÂê¶‰πüÊúÉÂú® LLM ‰∏≠Áî¢ÁîüÁõ∏ÂêåÁöÑË°åÁÇ∫ÔºåÂÖ∂‰∏≠ CoT ÁöÑÊ†ºÂºèÊúÉÊ™¢Á¥¢Êé®ÁêÜÂÖàÈ©óÔºåÂÑòÁÆ°ÊèêÁ§∫‰∏≠ÁöÑË≠âÊìö‰∏çÂêåÔºå‰ΩÜÈÄô‰∫õÂÖàÈ©ó‰ªçÁÑ∂Áõ∏Â∞ç‰∏çËÆä„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂ∞çÊñºËºÉÂ§ßÁöÑË™ûË®ÄÊ®°ÂûãÔºåCoT Á¢∫ÂØ¶Ëàá ICL ÈÅ≠ÂèóÁõ∏ÂêåÁöÑÂæåÈ©óÂ¥©ÊΩ∞„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/gchochla/cot-priors ÂèñÂæó„ÄÇ

##### **Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**
2409.06147v1 by Dong Han, Jihye Moon, Lu√≠s Roberto Mercado D√≠az, Darren Chen, Devan Williams, Eric Y. Ding, Khanh-Van Tran, David D. McManus, Ki H. Chon

Most deep learning models of multiclass arrhythmia classification are tested
on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise
ratios compared to smartwatch-derived PPG, and the best reported sensitivity
value for premature atrial/ventricular contraction (PAC/PVC) detection is only
75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF
detection, we use multi-modal data which incorporates 1D PPG, accelerometers,
and heart rate data as the inputs to a computationally efficient 1D
bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three
arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the
NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72
subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while
maintaining a high accuracy of 97.31% for AF detection. These results
outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%
for AF detection even while our model was computationally more efficient (14
times lighter and 2.7 faster).

ÊëòË¶ÅÔºöÂ§ßÂ§öÊï∏Â§öÈ°ûÂøÉÂæã‰∏çÊï¥ÂàÜÈ°ûÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈÉΩÊòØÂú®ÊåáÂ∞ñÂÖâÈõªÂÆπÁ©çÊèèË®òÊ≥ï (PPG) Ë≥áÊñô‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåËàáÊô∫ÊÖßÊâãÈå∂Ë°çÁîüÁöÑ PPG Áõ∏ÊØîÔºåÂÖ∂Ë®äËôüÈõúË®äÊØîÊõ¥È´òÔºåËÄåÂ∞çÊñºÊúüÂâçÂøÉÊàø/ÂøÉÂÆ§Êî∂Á∏Æ (PAC/PVC) ÂÅµÊ∏¨ÊâÄÂ†±ÂëäÁöÑÊúÄ‰Ω≥ÊïèÊÑüÂ∫¶ÂÄºÂÉÖÁÇ∫ 75%„ÄÇÁÇ∫‰∫ÜÂú®Á∂≠ÊåÅÈ´òÊàøÈ°´ÂÅµÊ∏¨ÁöÑÂêåÊôÇÊèêÈ´ò PAC/PVC ÂÅµÊ∏¨ÊïèÊÑüÂ∫¶ÔºåÊàëÂÄë‰ΩøÁî®Â§öÊ®°ÂºèË≥áÊñôÔºåÂ∞á 1D PPG„ÄÅÂä†ÈÄüÂ∫¶Ë®àÂíåÂøÉÁéáË≥áÊñô‰ΩúÁÇ∫Ë®àÁÆóÊïàÁéáÈ´òÁöÑ 1D ÈõôÂêëÈñòÊéßÈÅûËø¥ÂñÆÂÖÉ (1D-Bi-GRU) Ê®°ÂûãÁöÑËº∏ÂÖ•Ôºå‰ª•ÂÅµÊ∏¨‰∏âÈ°ûÂøÉÂæã‰∏çÊï¥„ÄÇÊàëÂÄë‰ΩøÁî®‰∫ÜÁæéÂúãÂúãÂÆ∂Ë°õÁîüÁ†îÁ©∂Èô¢Ë≥áÂä©ÁöÑ Pulsewatch Ëá®Â∫äË©¶È©ó‰∏≠ÁöÑÈÅãÂãïÂÅΩÂΩ±ÊòìÊÑüÊô∫ÊÖßÊâãÈå∂ PPG Ë≥áÊñô„ÄÇÊàëÂÄëÂú® 72 ÂêçÂèóË©¶ËÄÖË∫´‰∏äÊ∏¨Ë©¶ÁöÑÂ§öÊ®°ÂºèÊ®°ÂûãÔºåÂ∞çÊñº PAC/PVC ÂÅµÊ∏¨ÈÅîÂà∞‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑ 83% ÊïèÊÑüÂ∫¶ÔºåÂêåÊôÇÂ∞çÊñºÊàøÈ°´ÂÅµÊ∏¨Á∂≠ÊåÅ‰∫Ü 97.31% ÁöÑÈ´òÊ∫ñÁ¢∫Â∫¶„ÄÇÂç≥‰ΩøÊàëÂÄëÁöÑÊ®°ÂûãÂú®Ë®àÁÆó‰∏äÊõ¥ÊúâÊïàÁéáÔºàËºï 14 ÂÄçÔºåÂø´ 2.7 ÂÄçÔºâÔºåÈÄô‰∫õÁµêÊûú‰ªçÊØîÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÂú® PAC/PVC ÂÅµÊ∏¨‰∏äÈ´òÂá∫ 20.81%ÔºåÂú®ÊàøÈ°´ÂÅµÊ∏¨‰∏äÈ´òÂá∫ 2.55%„ÄÇ

##### **ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**
2409.05592v1 by Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He

Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.

ÊëòË¶ÅÔºöÈ†êÊ∏¨Êú™Áü•ÁöÑËó•Áâ©‰∫§‰∫í‰ΩúÁî® (DDI) Â∞çÊñºÊîπÂñÑËó•Áâ©ÂÆâÂÖ®Ëá≥ÈóúÈáçË¶Å„ÄÇÂÖàÂâçÂú® DDI È†êÊ∏¨ÊñπÈù¢ÊâÄÂÅöÁöÑÂä™ÂäõÈÄöÂ∏∏ÈõÜ‰∏≠Êñº‰∫åÂÖÉÂàÜÈ°ûÊàñÈ†êÊ∏¨ DDI È°ûÂà•ÔºåËÄåÁº∫‰πèËÉΩÂ§†Â¢ûÂº∑ÈÄô‰∫õÈ†êÊ∏¨ÁöÑÂèØ‰ø°Â∫¶ÁöÑËß£ÈáãÊÄßË¶ãËß£„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞ÁÇ∫ DDI È†êÊ∏¨Áî¢ÁîüËá™ÁÑ∂Ë™ûË®ÄËß£ÈáãÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†ÂêåÊôÇÊè≠Á§∫Ëó•ÊïàÂ≠∏ÂíåËó•Áâ©ÂãïÂäõÂ≠∏Ê©üÂà∂Ôºå‰∏¶ÈÄ≤Ë°åÈ†êÊ∏¨„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂæû DDInter Âíå DrugBank Êî∂ÈõÜ‰∫Ü DDI Ëß£ÈáãÔºå‰∏¶ÈñãÁôº‰∫ÜÂêÑÁ®ÆÊ®°ÂûãÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÂíåÂàÜÊûê„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÁÇ∫Â∑≤Áü•Ëó•Áâ©‰πãÈñìÊú™Áü•ÁöÑ DDI Êèê‰æõÊ∫ñÁ¢∫ÁöÑËß£Èáã„ÄÇÊú¨ÊñáÁÇ∫ DDI È†êÊ∏¨È†òÂüüË≤¢Áçª‰∫ÜÊñ∞ÁöÑÂ∑•ÂÖ∑Ôºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ DDI È†êÊ∏¨ÁöÑËß£ÈáãÁîüÊàêÂ•†ÂÆö‰∫ÜÂ†ÖÂØ¶ÁöÑÂü∫Á§é„ÄÇ

##### **Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**
2409.05486v2 by Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri

arXiv admin comment: This version has been removed by arXiv administrators as
the submitter did not have the rights to agree to the license at the time of
submission

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂìÅË≥™ÂíåËÉΩÂäõÁõÆÂâçÁÑ°Ê≥ïÈÄèÈÅéËá™ÂãïÂåñÂü∫Ê∫ñË©ï‰º∞ÂÆåÂÖ®Ë©ï‰º∞„ÄÇÁõ∏ÂèçÂú∞ÔºåÈúÄË¶ÅÊì¥Â±ïËá™ÁÑ∂Ë™ûË®ÄÁîüÊàêÊñáÁçª‰∏≠ÂÇ≥Áµ±ÂÆöÊÄßÊäÄË°ìÁöÑ‰∫∫Â∑•Ë©ï‰º∞„ÄÇ‰∏ÄÂÄãÊúÄËøëÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÂåÖÂê´‰ΩøÁî® A/B Ê∏¨Ë©¶Ê°ÜÊû∂ÔºåÂÆÉÊúÉÊì∑Âèñ‰∫∫È°ûË©ï‰º∞ËÄÖÂ∞çÊñºÁâπÂÆöÊ®°ÂûãÁöÑÂÅèÂ•Ω„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂ∞àÊ≥®ÊñºÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÔºàÂÅ•Â∫∑„ÄÅÁîüÁâ©Â≠∏„ÄÅÂåñÂ≠∏/Ëó•ÁêÜÂ≠∏ÔºâÁöÑ‰∫∫È°ûË©ï‰º∞ÂØ¶È©óÔºåË©≤ÂØ¶È©óÂú® Elsevier ÈÄ≤Ë°å„ÄÇÂÖ∂‰∏≠Ôºå‰∏ÄÂÄãÂ§ßÂûã‰ΩÜÈùûÈæêÂ§ßÔºà8.8B ÂèÉÊï∏ÔºâÂÉÖËß£Á¢ºÂô®Âü∫Á§éËΩâÊèõÂô®Âú®Áõ∏Â∞çËºÉÂ∞èÔºà135B ‰ª§ÁâåÔºâ‰ΩÜÁ∂ìÈÅéÈ´òÂ∫¶Êï¥ÁêÜÁöÑ Elsevier Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÔºåËàá OpenAI ÁöÑ GPT-3.5-turbo Âíå Meta ÁöÑÂü∫Á§é 7B ÂèÉÊï∏ Llama 2 Ê®°ÂûãÂú®Â§öÈáçÊ®ôÊ∫ñ‰∏ãÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÂç≥‰Ωø IRR ÂàÜÊï∏ÈÄöÂ∏∏ËºÉ‰ΩéÔºå‰ΩÜÂÅèÂ•Ω GPT-3.5-turboÔºåÂõ†Ê≠§ÂÅèÂ•ΩÂÖ∑Â∞çË©±ËÉΩÂäõ„ÄÅÈùûÂ∏∏Â§ßÂûã‰∏îÂú®ÈùûÂ∏∏Â§ßÂûãË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇ‰ΩÜÂêåÊôÇ‰πüÈ°ØÁ§∫ÔºåÂ∞çÊñºËºÉ‰∏çÈæêÂ§ßÁöÑÊ®°ÂûãÔºåÂú®ËºÉÂ∞è‰ΩÜÁ∂ìÈÅéËâØÂ•ΩÊï¥ÁêÜÁöÑË®ìÁ∑¥ÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåÊúâÂèØËÉΩÂú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÁî¢ÁîüÂèØË°åÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âº∑Â§ßÁöÑÂäüËÉΩÔºåÈÄ≤Ë°åÊïò‰∫ãÁîüÊàê„ÄÅÈÇèËºØÊé®ÁêÜÂíåÂ∏∏Ë≠òÁü•Ë≠òÊï¥ÂêàÔºåÊú¨Á†îÁ©∂Ê∑±ÂÖ•Êé¢Ë®éÂà©Áî® LLM ‰æÜÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàê (R2Gen)„ÄÇÂÑòÁÆ° LLM ÊìÅÊúâË±êÂØåÁöÑÁü•Ë≠òÔºå‰ΩÜË¶ÅÊúâÊïàËß∏ÁôºÈÄô‰∫õÂ§ßÂûãÊ®°Âûã‰∏≠ËàáÁâπÂÆö‰ªªÂãôÔºàÂ¶Ç R2GenÔºâÁõ∏ÈóúÁöÑÁü•Ë≠òÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü KARGENÔºå‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÁü•Ë≠òÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàêÊ°ÜÊû∂„ÄÇÂà©Áî®ÂáçÁµêÁöÑ LLM ‰æÜÁîüÊàêÂ†±ÂëäÔºåË©≤Ê°ÜÊû∂Êï¥Âêà‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂúñË≠úÔºå‰ª•Ëß£Èéñ LLM ‰∏≠ËàáËÉ∏ÈÉ®ÁñæÁóÖÁõ∏ÈóúÁöÑÁü•Ë≠òÔºå‰ª•Â¢ûÂº∑ÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊïàÁî®„ÄÇÈÄôÊòØÈÄèÈÅéÂà©Áî®Áü•Ë≠òÂúñË≠ú‰ª•Ë®≠Ë®àÁöÑÊñπÂºèÊèêÂèñËàáÁñæÁóÖÁõ∏ÈóúÁöÑÁâπÂæµ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÁî±ÊñºÊîæÂ∞ÑÂ†±ÂëäÂåÖÂê´Ê≠£Â∏∏ÂíåÁñæÁóÖÁõ∏ÈóúÁöÑÁôºÁèæÔºåÂõ†Ê≠§ÊèêÂèñÁöÑÂúñÂΩ¢Â¢ûÂº∑ÁñæÁóÖÁõ∏ÈóúÁâπÂæµËàáÂçÄÂüüÂΩ±ÂÉèÁâπÂæµÊï¥ÂêàÔºåÂÖºÈ°ßÂÖ©ÂÄãÊñπÈù¢„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫ÜÂÖ©Á®ÆËûçÂêàÊñπÊ≥ïÔºå‰ª•Ëá™ÂãïÂÑ™ÂÖàÊéíÂ∫èÂíåÈÅ∏ÊìáÊúÄÁõ∏ÈóúÁöÑÁâπÂæµ„ÄÇËûçÂêàÁöÑÁâπÂæµÁî± LLM ‰ΩøÁî®Ôºå‰ª•ÁîüÊàêÂ∞çÁñæÁóÖÊõ¥ÊïèÊÑü‰∏îÂìÅË≥™Êõ¥È´òÁöÑÂ†±Âëä„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® MIMIC-CXR Âíå IU-Xray Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ</paragraph>

##### **Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**
2409.07493v1 by Javad Hassannataj Joloudari, Mohammad Maftoun, Bahareh Nakisa, Roohallah Alizadehsani, Meisam Yadollahzadeh-Tabari

The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.

ÊëòË¶ÅÔºöË§áÈõúÊÉÖÁ∑íËæ®Ë≠òÁ≥ªÁµ± (CERS) ÈÄèÈÅéÊ™¢È©óË°®ÈÅîÁöÑÂü∫Êú¨ÊÉÖÁ∑íÁµÑÂêà„ÄÅÂÆÉÂÄëÁöÑÁõ∏‰∫íÈÄ£ÁµêÔºå‰ª•ÂèäÂãïÊÖãËÆäÂåñ‰æÜËß£Á¢ºË§áÈõúÁöÑÊÉÖÁ∑íÁãÄÊÖã„ÄÇÈÄèÈÅé‰ΩøÁî®ÈÄ≤ÈöéÊºîÁÆóÊ≥ïÔºåCERS Êèê‰æõ‰∫ÜÂ∞çÊÉÖÁ∑íÂãïÊÖãÁöÑÊ∑±ÂÖ•Ë¶ãËß£Ôºå‰øÉÈÄ≤Á¥∞Á∑ªÁöÑÁêÜËß£ÂíåÂÆ¢Ë£ΩÂåñÁöÑÂõûÊáâ„ÄÇÂú®Ê©üÂô®‰∏≠ÈÅîÊàêÈÄôÁ®ÆÁ®ãÂ∫¶ÁöÑÊÉÖÁ∑íËæ®Ë≠òÈúÄË¶ÅÁü•Ë≠òÊèêÁÖâÂíåÁêÜËß£È°û‰ººÊñº‰∫∫È°ûË™çÁü•ÁöÑÊñ∞Ê¶ÇÂøµ„ÄÇÁôºÂ±ïÁî®ÊñºËæ®Âà•Ë§áÈõúÊÉÖÁ∑íÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±Â∞çÊÉÖÊÑüÈÅãÁÆó‰æÜË™™ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞Ôºå‰∏¶ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂΩ±Èüø„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÊçïÊçâÂæÆÂ¶ôÊÉÖÁ∑íÊâÄÊ∂âÂèäÁöÑË§áÈõúÊÄßÔºåÂèñÂæó CERS ÁöÑÂ§ßÈáèË≥áÊñôÈõÜË¢´Ë≠âÊòéÊòØ‰∏ÄÈ†ÖËâ±ÈâÖÁöÑ‰ªªÂãôÔºåÂõ†Ê≠§ÈúÄË¶ÅÊé°Áî®ÁâπÊÆäÁöÑÊñπÊ≥ï‰æÜÊî∂ÈõÜÂíåËôïÁêÜË≥áÊñô„ÄÇÁ¥çÂÖ•ÁîüÁêÜË®äËôüÔºå‰æãÂ¶ÇÂøÉÈõªÂúñ (ECG) ÂíåËÖ¶ÈõªÂúñ (EEG)ÔºåÂèØ‰ª•ÈÄèÈÅéÊèê‰æõÂ∞ç‰ΩøÁî®ËÄÖÊÉÖÁ∑íÁãÄÊÖãÁöÑÂØ∂Ë≤¥Ë¶ãËß£„ÄÅÊèêÂçáË≥áÊñôÈõÜÁöÑÂìÅË≥™‰ª•ÂèäÂº∑ÂåñÁ≥ªÁµ±ÁöÑÂèØÈù†ÊÄßÔºå‰æÜÈ°ØËëóÂ¢ûÂº∑ CERS„ÄÇÊú¨Á†îÁ©∂ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂÖ®Èù¢ÁöÑÊñáÁçªÊé¢Ë®éÔºå‰ª•Ë©ï‰º∞Ê©üÂô®Â≠∏Áøí„ÄÅÊ∑±Â∫¶Â≠∏ÁøíÂíåÂÖÉÂ≠∏ÁøíÊñπÊ≥ïÂú®Âà©Áî®ËÖ¶ÈõªÂúñ„ÄÅÂøÉÈõªÂúñË®äËôüÂíåÈù¢ÈÉ®Ë°®ÊÉÖË≥áÊñôÈõÜÈÄ≤Ë°åÂü∫Êú¨ÂíåË§áÈõúÊÉÖÁ∑íËæ®Ë≠òÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÊâÄÈÅ∏ÁöÑÁ†îÁ©∂Ë´ñÊñáÊèê‰æõ‰∫ÜÈóúÊñº CERS ÁöÑÊΩõÂú®ÊáâÁî®„ÄÅËá®Â∫äÂΩ±ÈüøÂíåÁµêÊûúÁöÑËßÄÈªûÔºåÁõÆÁöÑÊòØ‰øÉÈÄ≤ÂÆÉÂÄëË¢´Êé•Âèó‰∏¶Êï¥ÂêàÂà∞Ëá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ã‰∏≠„ÄÇÊú¨Á†îÁ©∂Á™ÅÂá∫‰∫ÜÁêÜËß£ CERS ÁöÑÁ†îÁ©∂Â∑ÆË∑ùÂíåÊåëÊà∞ÔºåÈºìÂãµÁõ∏ÈóúÁ†îÁ©∂ÂíåÁµÑÁπîÈÄ≤‰∏ÄÊ≠•Ë™øÊü•„ÄÇÊúÄÂæåÔºåÂº∑Ë™ø‰∫ÜÂÖÉÂ≠∏ÁøíÊñπÊ≥ïÂú®ÊîπÂñÑ CERS ÊïàËÉΩÂíåÊåáÂ∞éÊú™‰æÜÁ†îÁ©∂Â∑•‰Ωú‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**
2409.05292v2 by Nirmalya Thakur

The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.

ÊëòË¶ÅÔºö‰∏ñÁïåÁõÆÂâçÊ≠£Âú®ÁªèÂéÜÁå¥ÁóòÁñ´ÊÉÖÔºå‰∏ñÁïåÂç´ÁîüÁªÑÁªáÂ∑≤ÂÆ£Â∏ÉÁå¥ÁóòÁñ´ÊÉÖ‰∏∫ÂõΩÈôÖÂÖ≥Ê≥®ÁöÑÁ™ÅÂèëÂÖ¨ÂÖ±Âç´Áîü‰∫ã‰ª∂„ÄÇÊ≠§ÂâçÊ≤°Êúâ‰∏éÁ§æ‰∫§Â™í‰ΩìÊåñÊéòÁõ∏ÂÖ≥ÁöÑÁ†îÁ©∂ÈõÜ‰∏≠‰∫éÂºÄÂèëÊúâÂÖ≥Áå¥ÁóòÁñ´ÊÉÖÁöÑ Instagram Â∏ñÂ≠êÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊú¨Êñá‰ªãÁªçÁöÑÁ†îÁ©∂Êó®Âú®Ëß£ÂÜ≥Ëøô‰∏ÄÁ†îÁ©∂Á©∫ÁôΩÔºåÂπ∂ÂØπËØ•È¢ÜÂüüÂÅöÂá∫‰∏§È°πÁßëÂ≠¶Ë¥°ÁåÆ„ÄÇÈ¶ñÂÖàÔºåÂÆÉÊèê‰æõ‰∫Ü 60,127 Êù°ÊúâÂÖ≥Áå¥ÁóòÁöÑ Instagram Â∏ñÂ≠êÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåËøô‰∫õÂ∏ñÂ≠êÂèëÂ∏É‰∫é 2022 Âπ¥ 7 Êúà 23 Êó•Ëá≥ 2024 Âπ¥ 9 Êúà 5 Êó•‰πãÈó¥„ÄÇËØ•Êï∞ÊçÆÈõÜÂèØÂú® https://dx.doi.org/10.21227/7fvc-y093 Â§ÑËé∑ÂæóÔºåÂÖ∂‰∏≠ÂåÖÂê´ 52 ÁßçËØ≠Ë®ÄÁöÑÊúâÂÖ≥Áå¥ÁóòÁöÑ Instagram Â∏ñÂ≠ê„ÄÇÂØπ‰∫éÂÖ∂‰∏≠ÊØèÁØáÂ∏ñÂ≠êÔºåÂ∏ñÂ≠ê ID„ÄÅÂ∏ñÂ≠êÊèèËø∞„ÄÅÂèëÂ∏ÉÊó•Êúü„ÄÅËØ≠Ë®ÄÂíåÂ∏ñÂ≠êÁöÑÁøªËØëÁâàÊú¨Ôºà‰ΩøÁî® Google ÁøªËØë API ÁøªËØëÊàêËã±ÊñáÔºâ‰Ωú‰∏∫ÂçïÁã¨ÁöÑÂ±ûÊÄßÊòæÁ§∫Âú®Êï∞ÊçÆÈõÜ‰∏≠„ÄÇÂú®ÂºÄÂèëÊ≠§Êï∞ÊçÆÈõÜÂêéÔºåËøõË°å‰∫ÜÊÉÖÊÑüÂàÜÊûê„ÄÅ‰ªáÊÅ®Ë®ÄËÆ∫Ê£ÄÊµã‰ª•ÂèäÁÑ¶ËôëÊàñÂéãÂäõÊ£ÄÊµã„ÄÇÊ≠§ËøáÁ®ãÂåÖÊã¨Â∞ÜÊØèÁØáÂ∏ñÂ≠êÂàÜÁ±ª‰∏∫ (i) ÊÉÖÊÑüÁ±ªÂà´‰πã‰∏ÄÔºåÂç≥ÊÅêÊÉß„ÄÅÊÉäËÆ∂„ÄÅÂø´‰πê„ÄÅÊÇ≤‰º§„ÄÅÊÑ§ÊÄí„ÄÅÂéåÊÅ∂Êàñ‰∏≠Á´ãÔºå(ii) ‰ªáÊÅ®ÊàñÈùû‰ªáÊÅ®Ôºå‰ª•Âèä (iii) Ê£ÄÊµãÂà∞ÁÑ¶Ëôë/ÂéãÂäõÊàñÊú™Ê£ÄÊµãÂà∞ÁÑ¶Ëôë/ÂéãÂäõ„ÄÇËøô‰∫õÁªìÊûú‰Ωú‰∏∫ÂçïÁã¨ÁöÑÂ±ûÊÄßÊòæÁ§∫Âú®Êï∞ÊçÆÈõÜ‰∏≠„ÄÇÂÖ∂Ê¨°ÔºåÊú¨Êñá‰ªãÁªç‰∫ÜÊâßË°åÊÉÖÊÑüÂàÜÊûê„ÄÅ‰ªáÊÅ®Ë®ÄËÆ∫ÂàÜÊûêÂíåÁÑ¶ËôëÊàñÂéãÂäõÂàÜÊûêÁöÑÁªìÊûú„ÄÇËßÇÂØüÂà∞ÊÉÖÊÑüÁ±ªÂà´ÁöÑÂèòÂåñ‚Äî‚ÄîÊÅêÊÉß„ÄÅÊÉäËÆ∂„ÄÅÂø´‰πê„ÄÅÊÇ≤‰º§„ÄÅÊÑ§ÊÄí„ÄÅÂéåÊÅ∂Âíå‰∏≠Á´ãÂàÜÂà´‰∏∫ 27.95%„ÄÅ2.57%„ÄÅ8.69%„ÄÅ5.94%„ÄÅ2.69%„ÄÅ1.53% Âíå 50.64%„ÄÇÂú®‰ªáÊÅ®Ë®ÄËÆ∫Ê£ÄÊµãÊñπÈù¢Ôºå95.75% ÁöÑÂ∏ñÂ≠ê‰∏çÂåÖÂê´‰ªáÊÅ®ÔºåÂÖ∂‰Ωô 4.25% ÁöÑÂ∏ñÂ≠êÂåÖÂê´‰ªáÊÅ®„ÄÇÊúÄÂêéÔºå72.05% ÁöÑÂ∏ñÂ≠êÊ≤°ÊúâË°®Áé∞Âá∫‰ªª‰ΩïÁÑ¶Ëôë/ÂéãÂäõÔºåÂÖ∂‰Ωô 27.95% ÁöÑÂ∏ñÂ≠ê‰ª£Ë°®ÊüêÁßçÂΩ¢ÂºèÁöÑÁÑ¶Ëôë/ÂéãÂäõ„ÄÇ

##### **RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**
2409.05280v1 by Quoc-Bao Nguyen-Le, Tuan-Hy Le, Anh-Triet Do, Quoc-Huy Trinh

Cardiovascular disease is a major global health concern, contributing
significantly to global mortality. Accurately segmenting cardiac medical
imaging data is crucial for reducing fatality rates associated with these
conditions. However, current state-of-the-art (SOTA) neural networks, including
CNN-based and Transformer-based approaches, face challenges in capturing both
inter-slice connections and intra-slice details, especially in datasets
featuring intricate, long-range details along the z-axis like coronary
arteries. Existing methods also struggle with differentiating non-cardiac
components from the myocardium, resulting in segmentation inaccuracies and the
"spraying" phenomenon. To address these issues, we introduce
RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of
intricate cardiac structures. Our approach enhances global context modeling
through multiscale feature aggregation and nested skip connections in the
encoder. Transformer layers facilitate capturing intra-slice interactions,
while a rotatory attention mechanism handles inter-slice connectivity. A
channel-wise cross-attention gate integrates multiscale information and decoder
features, effectively bridging semantic gaps. Experimental results across
multiple datasets demonstrate superior performance over current methods,
achieving near-perfect annotation of coronary arteries and myocardium. Ablation
studies confirm that our rotatory attention mechanism significantly improves
segmentation accuracy by transforming embedded vectorized patches in semantic
dimensional space.

ÊëòË¶ÅÔºöÂøÉË°ÄÁÆ°ÁñæÁóÖÊòØÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÂÅ•Â∫∑ÂïèÈ°åÔºåÂ∞çÂÖ®ÁêÉÊ≠ª‰∫°ÁéáÊúâÈ°ØËëóÁöÑÂΩ±Èüø„ÄÇÊ∫ñÁ¢∫ÂàÜÂâ≤ÂøÉËáüÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÂ∞çÊñºÈôç‰ΩéÈÄô‰∫õÁñæÁóÖÁõ∏ÈóúÁöÑÊ≠ª‰∫°ÁéáËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÂÖàÈÄ≤Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂåÖÊã¨Âü∫Êñº CNN ÂíåÂü∫Êñº Transformer ÁöÑÊñπÊ≥ïÔºåÂú®Êì∑ÂèñÂ±§ÈñìÈÄ£Êé•ÂíåÂ±§ÂÖßÁ¥∞ÁØÄÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÂÖ∑ÊúâÊ≤øËëó z Ëª∏ÁöÑË§áÈõú„ÄÅÈï∑Á®ãÁ¥∞ÁØÄÁöÑË≥áÊñôÈõÜÔºå‰æãÂ¶ÇÂÜ†ÁãÄÂãïËÑà„ÄÇÁèæÊúâÊñπÊ≥ï‰πüÈõ£‰ª•ÂçÄÂàÜÈùûÂøÉËáüÊàêÂàÜÂíåÂøÉËÇåÔºåÂ∞éËá¥ÂàÜÂâ≤‰∏çÊ∫ñÁ¢∫Âíå„ÄåÂô¥ÁÅë„ÄçÁèæË±°„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü RotCAtt-TransUNet++Ôºå‰∏ÄÁ®ÆÂ∞àÁÇ∫Ë§áÈõúÂøÉËáüÁµêÊßãÁöÑÁ©©ÂÅ•ÂàÜÂâ≤ËÄåË®≠Ë®àÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÄèÈÅéÁ∑®Á¢ºÂô®‰∏≠ÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæµËÅöÂêàÂíåÂ∑¢ÁãÄË∑≥Ë∫çÈÄ£Êé•Â¢ûÂº∑‰∫ÜÂÖ®Â±ÄËÉåÊôØÂª∫Ê®°„ÄÇTransformer Â±§‰øÉÈÄ≤Êì∑ÂèñÂ±§ÂÖß‰∫§‰∫í‰ΩúÁî®ÔºåËÄåÊóãËΩâÊ≥®ÊÑèÊ©üÂà∂ÂâáËôïÁêÜÂ±§ÈñìÈÄ£Êé•„ÄÇÈÄöÈÅìÂºè‰∫§ÂèâÊ≥®ÊÑèÈñòÈñÄÊï¥Âêà‰∫ÜÂ§öÂ∞∫Â∫¶Ë≥áË®äÂíåËß£Á¢ºÂô®ÁâπÂæµÔºåÊúâÊïàÂú∞ÂΩåÂêà‰∫ÜË™ûÁæ©Â∑ÆË∑ù„ÄÇË∑®Â§öÂÄãË≥áÊñôÈõÜÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÂÖ∂ÂÑ™ÊñºÁõÆÂâçÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÂØ¶Áèæ‰∫ÜÂÜ†ÁãÄÂãïËÑàÂíåÂøÉËÇåÁöÑËøë‰πéÂÆåÁæéÁöÑË®ªËß£„ÄÇÊ∂àËûçÁ†îÁ©∂Ë≠âÂØ¶ÔºåÊàëÂÄëÁöÑÊóãËΩâÊ≥®ÊÑèÊ©üÂà∂ÈÄèÈÅéËΩâÊèõË™ûÁæ©Á∂≠Â∫¶Á©∫Èñì‰∏≠ÁöÑÂµåÂÖ•ÂêëÈáèÂåñË£ú‰∏ÅÔºåÈ°ØËëóÂú∞ÊèêÈ´ò‰∫ÜÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶„ÄÇ

##### **Activation Function Optimization Scheme for Image Classification**
2409.04915v1 by Abdur Rahman, Lu He, Haifeng Wang

Activation function has a significant impact on the dynamics, convergence,
and performance of deep neural networks. The search for a consistent and
high-performing activation function has always been a pursuit during deep
learning model development. Existing state-of-the-art activation functions are
manually designed with human expertise except for Swish. Swish was developed
using a reinforcement learning-based search strategy. In this study, we propose
an evolutionary approach for optimizing activation functions specifically for
image classification tasks, aiming to discover functions that outperform
current state-of-the-art options. Through this optimization framework, we
obtain a series of high-performing activation functions denoted as Exponential
Error Linear Unit (EELU). The developed activation functions are evaluated for
image classification tasks from two perspectives: (1) five state-of-the-art
neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and
Compact Convolutional Transformer which cover computationally heavy to light
neural networks, and (2) eight standard datasets, including CIFAR10,
Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,
and TinyImageNet which cover from typical machine vision benchmark,
agricultural image applications to medical image applications. Finally, we
statistically investigate the generalization of the resultant activation
functions developed through the optimization scheme. With a Friedman test, we
conclude that the optimization scheme is able to generate activation functions
that outperform the existing standard ones in 92.8% cases among 28 different
cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation
function for image classification generated by the optimization scheme.

ÊëòË¶ÅÔºö<paragraph>ÊøÄÊ¥ªÂáΩÊï∏Â∞çÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂãïÊÖã„ÄÅÊî∂ÊñÇÂíåÊïàËÉΩÊúâÈ°ØËëóÁöÑÂΩ±Èüø„ÄÇÂú®Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈñãÁôºÈÅéÁ®ã‰∏≠Ôºå‰∏ÄÁõ¥Ëá¥ÂäõÊñºÂ∞ãÊâæ‰∏ÄËá¥‰∏îÊïàËÉΩÈ´òÁöÑÊøÄÊ¥ªÂáΩÊï∏„ÄÇÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÊøÄÊ¥ªÂáΩÊï∏ÔºåÈô§‰∫Ü Swish ‰πãÂ§ñÔºåÈÉΩÊòØÁî±‰∫∫È°ûÂ∞àÂÆ∂ÊâãÂãïË®≠Ë®àÁöÑ„ÄÇSwish ÊòØ‰ΩøÁî®Âü∫ÊñºÂº∑ÂåñÂ≠∏ÁøíÁöÑÊêúÂ∞ãÁ≠ñÁï•ÈñãÁôºÁöÑ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊºîÂåñÊñπÊ≥ïÔºåÂ∞àÈñÄÈáùÂ∞çÂúñÂÉèÂàÜÈ°û‰ªªÂãôÊúÄ‰Ω≥ÂåñÊøÄÊ¥ªÂáΩÊï∏ÔºåÊó®Âú®ÁôºÁèæÊïàËÉΩÂÑ™ÊñºÁèæÊúâÊúÄÂÖàÈÄ≤ÈÅ∏È†ÖÁöÑÂáΩÊï∏„ÄÇÈÄèÈÅéÈÄôÂÄãÊúÄ‰Ω≥ÂåñÊû∂ÊßãÔºåÊàëÂÄëÁç≤Âæó‰∫Ü‰∏ÄÁ≥ªÂàóÊïàËÉΩÈ´òÁöÑÊøÄÊ¥ªÂáΩÊï∏ÔºåË°®Á§∫ÁÇ∫ÊåáÊï∏Ë™§Â∑ÆÁ∑öÊÄßÂñÆÂÖÉ (EELU)„ÄÇÂ∑≤ÈáùÂ∞çÂÖ©ÂÄãËßÄÈªûË©ï‰º∞Â∑≤ÈñãÁôºÁöÑÊøÄÊ¥ªÂáΩÊï∏ÔºåÁî®ÊñºÂúñÂÉèÂàÜÈ°û‰ªªÂãôÔºö(1) ‰∫îÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÔºå‰æãÂ¶Ç ResNet50„ÄÅAlexNet„ÄÅVGG16„ÄÅMobileNet Âíå Compact Convolutional TransformerÔºåÊ∂µËìãÂæûË®àÁÆóÈáèÈáçÁöÑÂà∞ËºïÈáèÁöÑÁ∂≤Ë∑ØÔºå(2) ÂÖ´ÂÄãÊ®ôÊ∫ñË≥áÊñôÈõÜÔºåÂåÖÊã¨ CIFAR10„ÄÅImagenette„ÄÅMNIST„ÄÅFashion MNIST„ÄÅBeans„ÄÅColorectal Histology„ÄÅCottonWeedID15 Âíå TinyImageNetÔºåÊ∂µËìãÂæûÂÖ∏ÂûãÁöÑÊ©üÂô®Ë¶ñË¶∫Âü∫Ê∫ñ„ÄÅËæ≤Ê•≠ÂΩ±ÂÉèÊáâÁî®Âà∞ÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁµ±Ë®àË™øÊü•‰∫ÜÈÄèÈÅéÊúÄ‰Ω≥ÂåñÊñπÊ°àÈñãÁôºÁöÑÁµêÊûúÊøÄÊ¥ªÂáΩÊï∏ÁöÑÊ¶ÇÂåñ„ÄÇÈÄèÈÅé Friedman Ê™¢ÂÆöÔºåÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÊúÄ‰Ω≥ÂåñÊñπÊ°àËÉΩÂ§†Áî¢ÁîüÂú® 28 ÂÄã‰∏çÂêåÁöÑÁ†îÁ©∂Ê°à‰æã‰∏≠ÔºåÊúâ 92.8% ÁöÑÊ°à‰æãÊïàËÉΩÂÑ™ÊñºÁèæÊúâÊ®ôÊ∫ñÂáΩÊï∏Ôºå‰∏¶‰∏îÁôºÁèæ $-x\cdot erf(e^{-x})$ ÊòØÊúÄ‰Ω≥ÂåñÊñπÊ°àÁî¢ÁîüÁöÑÊúÄ‰Ω≥ÂΩ±ÂÉèÂàÜÈ°ûÊøÄÊ¥ªÂáΩÊï∏„ÄÇ</paragraph>

##### **LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**
2409.04744v1 by Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.

ÊëòË¶ÅÔºöÂú®Âº∑ÂåñÂ≠∏ÁøíÔºàRLÔºâÁöÑÁí∞Â¢ÉËΩâÊèõÊ®°Âûã‰∏≠ÔºåÂõ∫ÊúâÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈúÄË¶ÅÂú®Êé¢Á¥¢ÂíåÂà©Áî®‰πãÈñìÂèñÂæó‰ªîÁ¥∞ÁöÑÂπ≥Ë°°Ôºå‰ª•ÊúÄ‰Ω≥ÂåñË®àÁÆóË≥áÊ∫êÁöÑ‰ΩøÁî®Ôºå‰ª•Á≤æÊ∫ñ‰º∞Ë®à‰ª£ÁêÜÈ†êÊúüÁöÑÁçéÂãµ„ÄÇÂú®ÊéßÂà∂Á≥ªÁµ±‰∏≠ÂèñÂæóÂπ≥Ë°°Âú®ÁçéÂãµÁ®ÄÁñèÁöÑÊÉÖÊ≥Å‰∏ãÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË®±Â§öÁí∞Â¢ÉÈÉΩÊúâÂª£Ê≥õÁöÑÂÖàÈ©óÁü•Ë≠òÔºåÂõ†Ê≠§Âú®ÈÄôÁ®ÆË®≠ÂÆö‰∏≠ÂæûÈ†≠ÈñãÂßãÂ≠∏ÁøíÊòØÂ§öÈ§òÁöÑ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü**L**anguage **M**odel **G**uided **T**rade-offsÔºàÂç≥**LMGT**ÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©é‰∏îÊ®£Êú¨ÊïàÁéáÈ´òÁöÑÊû∂ÊßãÔºåÂÆÉÂà©Áî®‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÂµåÂÖ•ÁöÑÂÖ®Èù¢ÂÖàÈ©óÁü•Ë≠òÔºå‰ª•ÂèäÂÆÉÂÄëËôïÁêÜÈùûÊ®ôÊ∫ñÊï∏ÊìöÂΩ¢ÂºèÔºà‰æãÂ¶Ç wiki ÊïôÁ®ãÔºâÁöÑÈùàÊ¥ªÊÄß„ÄÇLMGT ÈÄèÈÅéÊé°Áî®Áî± LLM ÂºïÂ∞éÁöÑÁçéÂãµËΩâÁßª‰æÜÁÜüÁ∑¥Âú∞ÁÆ°ÁêÜÊé¢Á¥¢-Âà©Áî®Ê¨äË°°ÔºåÊåáÂ∞é‰ª£ÁêÜÁöÑÊé¢Á¥¢Â∑•‰ΩúÔºåÂæûËÄåÊèêÈ´òÊ®£Êú¨ÊïàÁéá„ÄÇÊàëÂÄëÂ∑≤Á∂ìÂæπÂ∫ïÊ∏¨Ë©¶‰∫Ü LMGT Âú®ÂêÑÁ®Æ RL ‰ªªÂãô‰∏≠ÁöÑË°®ÁèæÔºå‰∏¶Â∞áÂÖ∂ÈÉ®ÁΩ≤Âú®Â∑•Ê•≠Á¥ö RL Êé®Ëñ¶Á≥ªÁµ±‰∏≠ÔºåÂú®ÈÄô‰∫õÁ≥ªÁµ±‰∏≠ÔºåÂÆÉÂßãÁµÇÂÑ™ÊñºÂü∫Á∑öÊñπÊ≥ï„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂèØ‰ª•È°ØËëóÊ∏õÂ∞ë RL Ë®ìÁ∑¥ÈöéÊÆµÊâÄÈúÄÁöÑÊôÇÈñìÊàêÊú¨„ÄÇ

##### **NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**
2409.04723v1 by Debaditya Shome, Nasim Montazeri Ghahjaverestan, Ali Etemad

Sleep is known to be a key factor in emotional regulation and overall mental
health. In this study, we explore the integration of sleep measures from the
previous night into wearable-based mood recognition. To this end, we propose
NapTune, a novel prompt-tuning framework that utilizes sleep-related measures
as additional inputs to a frozen pre-trained wearable time-series encoder by
adding and training lightweight prompt parameters to each Transformer layer.
Through rigorous empirical evaluation, we demonstrate that the inclusion of
sleep data using NapTune not only improves mood recognition performance across
different wearable time-series namely ECG, PPG, and EDA, but also makes it more
sample-efficient. Our method demonstrates significant improvements over the
best baselines and unimodal variants. Furthermore, we analyze the impact of
adding sleep-related measures on recognizing different moods as well as the
influence of individual sleep-related measures.

ÊëòË¶ÅÔºöÁù°Áú†Â∑≤Áü•ÊòØÊÉÖÁ∑íË™øÁØÄÂíåÊï¥È´îÂøÉÁêÜÂÅ•Â∫∑‰∏≠ÁöÑÈóúÈçµÂõ†Á¥†„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ∞áÂâç‰∏ÄÊôöÁöÑÁù°Áú†Ê∏¨ÈáèÊï¥ÂêàÂà∞ÂèØÁ©øÊà¥ÂºèÊÉÖÁ∑íËæ®Ë≠ò‰∏≠„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü NapTuneÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊèêÁ§∫Ë™øÊï¥Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî®ËàáÁù°Áú†Áõ∏ÈóúÁöÑÊ∏¨Èáè‰ΩúÁÇ∫ÂáçÁµêÈ†êË®ìÁ∑¥ÂèØÁ©øÊà¥ÊôÇÈñìÂ∫èÂàóÁ∑®Á¢ºÂô®ÁöÑÈôÑÂä†Ëº∏ÂÖ•ÔºåÊñπÊ≥ïÊòØÂ∞áËºïÈáèÁ¥öÊèêÁ§∫ÂèÉÊï∏Êñ∞Â¢û‰∏¶Ë®ìÁ∑¥Âà∞ÊØèÂÄã Transformer Â±§„ÄÇÈÄèÈÅéÂö¥Ë¨πÁöÑÁ∂ìÈ©óË©ï‰º∞ÔºåÊàëÂÄëË≠âÊòé‰ΩøÁî® NapTune Á¥çÂÖ•Áù°Áú†Êï∏Êìö‰∏çÂÉÖÊîπÂñÑ‰∫Ü‰∏çÂêåÂèØÁ©øÊà¥ÊôÇÈñìÂ∫èÂàóÔºàÂç≥ÂøÉÈõªÂúñ„ÄÅÂÖâÈõªÂÆπÁ©çÊèèË®òÂíåÁöÆÈõªÊ¥ªÂãïÔºâÁöÑÊÉÖÁ∑íËæ®Ë≠òÊïàËÉΩÔºåÈÇÑËÆìÂÆÉÊõ¥ÂÖ∑Ê®£Êú¨ÊïàÁéá„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãË≠âÊòé‰∫ÜÁõ∏ËºÉÊñºÊúÄ‰Ω≥Âü∫Á∑öÂíåÂñÆÊ®°ÊÖãËÆäÁï∞ÔºåÊúâÈ°ØËëóÁöÑÊîπÂñÑ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÊñ∞Â¢ûËàáÁù°Áú†Áõ∏ÈóúÁöÑÊ∏¨ÈáèÂ∞çËæ®Ë≠ò‰∏çÂêåÊÉÖÁ∑íÁöÑÂΩ±ÈüøÔºå‰ª•ÂèäÂÄãÂà•ËàáÁù°Áú†Áõ∏ÈóúÁöÑÊ∏¨ÈáèÁöÑÂΩ±Èüø„ÄÇ

##### **A Comprehensive Survey on Evidential Deep Learning and Its Applications**
2409.04720v1 by Junyu Gao, Mengyuan Chen, Liangyu Xiang, Changsheng Xu

Reliable uncertainty estimation has become a crucial requirement for the
industrial deployment of deep learning algorithms, particularly in high-risk
applications such as autonomous driving and medical diagnosis. However,
mainstream uncertainty estimation methods, based on deep ensembling or Bayesian
neural networks, generally impose substantial computational overhead. To
address this challenge, a novel paradigm called Evidential Deep Learning (EDL)
has emerged, providing reliable uncertainty estimation with minimal additional
computation in a single forward pass. This survey provides a comprehensive
overview of the current research on EDL, designed to offer readers a broad
introduction to the field without assuming prior knowledge. Specifically, we
first delve into the theoretical foundation of EDL, the subjective logic
theory, and discuss its distinctions from other uncertainty estimation
frameworks. We further present existing theoretical advancements in EDL from
four perspectives: reformulating the evidence collection process, improving
uncertainty estimation via OOD samples, delving into various training
strategies, and evidential regression networks. Thereafter, we elaborate on its
extensive applications across various machine learning paradigms and downstream
tasks. In the end, an outlook on future directions for better performances and
broader adoption of EDL is provided, highlighting potential research avenues.

ÊëòË¶ÅÔºöÂèØÈù†ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÂ∑≤ÊàêÁÇ∫Ê∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÁî¢Ê•≠ÈÉ®ÁΩ≤ÁöÑÈóúÈçµÈúÄÊ±ÇÔºåÁâπÂà•ÊòØÂú®È´òÈ¢®Èö™ÊáâÁî®‰∏≠Ôºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõÂíåÈÜ´ÁôÇË®∫Êñ∑„ÄÇÁÑ∂ËÄåÔºåÂü∫ÊñºÊ∑±Â∫¶ÈõÜÊàêÊàñË≤ùÊ∞èÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑ‰∏ªÊµÅ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÈÄ†ÊàêÂ§ßÈáèÁöÑË®àÁÆóË≤†Êìî„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞Ôºå‰∏ÄÁ®ÆÁ®±ÁÇ∫Ë≠âÊìöÊ∑±Â∫¶Â≠∏Áøí (EDL) ÁöÑÊñ∞ÁØÑ‰æãÊáâÈÅãËÄåÁîüÔºåÂÆÉÂú®ÂñÆÊ¨°ÂâçÂêëÂÇ≥ÈÅû‰∏≠‰ª•ÊúÄÂ∞ëÁöÑÈ°çÂ§ñÈÅãÁÆóÊèê‰æõÂèØÈù†ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à„ÄÇÈÄôÈ†ÖË™øÊü•Â∞ç EDL ÁöÑÁèæÊúâÁ†îÁ©∂Êèê‰æõÂÖ®Èù¢ÁöÑÊ¶ÇËø∞ÔºåÊó®Âú®ÁÇ∫ËÆÄËÄÖÊèê‰æõË©≤È†òÂüüÁöÑÂª£Ê≥õ‰ªãÁ¥πÔºåËÄåÁÑ°ÈúÄÂÅáË®≠ÂÖàÂÇôÁü•Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊ∑±ÂÖ•Êé¢Ë®é EDL ÁöÑÁêÜË´ñÂü∫Á§éÔºåÂç≥‰∏ªËßÄÈÇèËºØÁêÜË´ñÔºå‰∏¶Ë®éË´ñÂÖ∂ËàáÂÖ∂‰ªñ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÊû∂ÊßãÁöÑÂçÄÂà•„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂæûÂõõÂÄãËßíÂ∫¶‰ªãÁ¥π EDL ‰∏≠ÁèæÊúâÁöÑÁêÜË´ñÈÄ≤Â±ïÔºöÈáçÊñ∞Âà∂ÂÆöË≠âÊìöÊî∂ÈõÜÈÅéÁ®ã„ÄÅÈÄèÈÅé OOD Ê®£Êú¨ÊîπÂñÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à„ÄÅÊ∑±ÂÖ•Êé¢Ë®éÂêÑÁ®ÆË®ìÁ∑¥Á≠ñÁï•‰ª•ÂèäË≠âÊìöÂõûÊ≠∏Á∂≤Ë∑Ø„ÄÇÊ≠§ÂæåÔºåÊàëÂÄëË©≥Á¥∞Ë™™ÊòéÂÆÉÂú®ÂêÑÁ®ÆÊ©üÂô®Â≠∏ÁøíÁØÑ‰æãÂíå‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂ∞çÊú™‰æÜÊñπÂêëÁöÑÂ±ïÊúõÔºå‰ª•ÊúüÁç≤ÂæóÊõ¥Â•ΩÁöÑÊïàËÉΩÂíåÊõ¥Âª£Ê≥õÂú∞Êé°Áî® EDLÔºå‰∏¶ÈáçÈªû‰ªãÁ¥πÊΩõÂú®ÁöÑÁ†îÁ©∂ÈÄîÂæë„ÄÇ

##### **A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**
2409.04704v1 by Cheng Wan, Chenjie Xie, Longfei Liu, Dan Wu, Ye Li

Continuous blood pressure (BP) monitoring is essential for timely diagnosis
and intervention in critical care settings. However, BP varies significantly
across individuals, this inter-patient variability motivates the development of
personalized models tailored to each patient's physiology. In this work, we
propose a personalized BP forecasting model mainly using electrocardiogram
(ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates
2D representation learning to capture complex physiological relationships.
Experiments are conducted on datasets collected from three diverse scenarios
with BP measurements from 60 subjects total. Results demonstrate that the model
achieves accurate and robust BP forecasts across scenarios within the
Association for the Advancement of Medical Instrumentation (AAMI) standard
criteria. This reliable early detection of abnormal fluctuations in BP is
crucial for at-risk patients undergoing surgery or intensive care. The proposed
model provides a valuable addition for continuous BP tracking to reduce
mortality and improve prognosis.

ÊëòË¶ÅÔºöÊåÅÁ∫åÁöÑË°ÄÂ£ì (BP) Áõ£ÊéßÂ∞çÊñºÈáçÁóáÁõ£Ë≠∑Áí∞Â¢É‰∏≠ÁöÑÂèäÊôÇË®∫Êñ∑ÂíåÂπ≤È†êËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåBP Âõ†‰∫∫ËÄåÁï∞ÔºåÈÄôÁ®ÆÊÇ£ËÄÖÈñìËÆäÁï∞ÊÄß‰øÉ‰ΩøÈñãÁôºÈáùÂ∞çÊØè‰ΩçÊÇ£ËÄÖÁîüÁêÜÁãÄÊ≥ÅÈáèË∫´ÊâìÈÄ†ÁöÑÂÄã‰∫∫ÂåñÊ®°Âûã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÄã‰∫∫Âåñ BP È†êÊ∏¨Ê®°ÂûãÔºå‰∏ªË¶Å‰ΩøÁî®ÂøÉÈõªÂúñ (ECG) ÂíåÂÖâÈõªÂÆπÁ©çÊèèË®òÊ≥ï (PPG) ‰ø°Ëôü„ÄÇÊ≠§ÊôÇÈñìÂ∫èÂàóÊ®°ÂûãÁµêÂêà‰∫Ü 2D Ë°®ÂæµÂ≠∏Áøí‰ª•ÊçïÊçâË§áÈõúÁöÑÁîüÁêÜÈóú‰øÇ„ÄÇÂØ¶È©óÊòØÂú®Âæû‰∏âÁ®Æ‰∏çÂêåÊÉÖÂ¢ÉÊî∂ÈõÜÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÔºåÁ∏ΩÂÖ±‰æÜËá™ 60 ‰ΩçÂèóË©¶ËÄÖÁöÑ BP Ê∏¨Èáè„ÄÇÁµêÊûúË°®ÊòéÔºåË©≤Ê®°ÂûãÂú®ÈÜ´Â≠∏ÂÑÄÂô®‰øÉÈÄ≤ÂçîÊúÉ (AAMI) Ê®ôÊ∫ñÊ®ôÊ∫ñÂÖßÂØ¶Áèæ‰∫ÜË∑®ÊÉÖÂ¢ÉÁöÑÊ∫ñÁ¢∫‰∏îÁ©©ÂÅ•ÁöÑ BP È†êÊ∏¨„ÄÇÂ∞çÊñºÊé•ÂèóÊâãË°ìÊàñÈáçÁóáÁõ£Ë≠∑ÁöÑÈ´òÈ¢®Èö™ÊÇ£ËÄÖËÄåË®ÄÔºåÈÄôÁ®ÆÂ∞ç BP Áï∞Â∏∏Ê≥¢ÂãïÁöÑÂèØÈù†Êó©ÊúüÊ™¢Ê∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÁÇ∫ÊåÅÁ∫å BP ËøΩËπ§Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË£úÂÖÖÔºå‰ª•Èôç‰ΩéÊ≠ª‰∫°Áéá‰∏¶ÊîπÂñÑÈ†êÂæå„ÄÇ

##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÁöÑÔºö‰ΩøÁî®‰∏çÂêåÊéÉÊèèÂÑÄÂíåÂçîÂÆöÂèñÂæóÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂú®ÂΩ±ÂÉèÂ§ñËßÄ‰∏äÂèØËÉΩÊúÉÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÈÄôÁ®ÆÁèæË±°Á®±ÁÇ∫ÊéÉÊèèÂÑÄÈ†òÂüüÂÅèÁßªÔºåÂèØËÉΩÊúÉÂ∞éËá¥Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊïàËÉΩ‰∏ãÈôçÔºåËÄåÈÄô‰∫õÁ∂≤Ë∑ØÊòØÈáùÂ∞çÁî±‰∏ÄÁ®ÆÊéÉÊèèÂÑÄÂèñÂæóÁöÑË≥áÊñôÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰∏¶Âú®Âè¶‰∏ÄÁ®ÆÊéÉÊèèÂÑÄ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÈÄôÂÄãÈáçË¶ÅÁöÑÂØ¶ÈöõÂïèÈ°åÂ∑≤Áç≤ÂæóÂª£Ê≥õË™çÂèØÔºå‰ΩÜÁõÆÂâçÂ∞öÊú™ÈáùÂ∞ç‰∏çÂêåÂΩ¢ÂºèÂíåË®∫Êñ∑‰ªªÂãôÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßÁ†îÁ©∂„ÄÇÊùêÊñôÂíåÊñπÊ≥ïÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂª£Ê≥õÁöÑÂØ¶È©óÁ†îÁ©∂ÔºåË©ï‰º∞ÊéÉÊèèÂÑÄÈ†òÂüüÂÅèÁßªÂ∞ç‰∏çÂêåËá™ÂãïÂåñË®∫Êñ∑‰ªªÂãôÁöÑÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂú®Â∏∏Ë¶ãÁöÑÊîæÂ∞ÑÂ≠∏ÂΩ¢Âºè‰∏≠Ë©ï‰º∞ÈÄôÁ®ÆÁèæË±°ÔºåÂåÖÊã¨ X ÂÖâ„ÄÅÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂíåÁ£ÅÊåØÈÄ†ÂΩ±„ÄÇÁµêÊûúÔºöÊàëÂÄëÁôºÁèæÔºå‰æÜËá™‰∏çÂêåÊéÉÊèèÂÑÄÁöÑË≥áÊñôÂú®Á∂≤Ë∑Ø‰∏äÁöÑÊïàËÉΩÂπæ‰πéÁ∏ΩÊòØÊØî‰æÜËá™Áõ∏ÂêåÊéÉÊèèÂÑÄÁöÑË≥áÊñôÂ∑ÆÔºåÊàëÂÄëÈáèÂåñ‰∫Ü‰∏çÂêåË≥áÊñôÈõÜÊïàËÉΩ‰∏ãÈôçÁöÑÁ®ãÂ∫¶„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæÈÄôÁ®Æ‰∏ãÈôçÂú®Á£ÅÊåØÈÄ†ÂΩ±‰∏≠ÊúÄ‰∏∫Âö¥ÈáçÔºåÂú® X ÂÖâ‰∏≠ÁÇ∫‰∏≠Á≠âÔºåÂú®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè‰∏≠Áõ∏Áï∂Â∞èÔºåÂπ≥ÂùáËÄåË®ÄÔºåÊàëÂÄëÂ∞áÂÖ∂Ê≠∏Âõ†ÊñºÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂèñÂæóÁ≥ªÁµ±ÁöÑÊ®ôÊ∫ñÂåñÊÄßË≥™ÔºåËÄåÁ£ÅÊåØÈÄ†ÂΩ±Êàñ X ÂÖâ‰∏≠‰∏çÂ≠òÂú®ÈÄôÁ®ÆÊÄßË≥™„ÄÇÊàëÂÄëÈÇÑÁ†îÁ©∂‰∫ÜÂ∞á‰∏çÂêåÊï∏ÈáèÁöÑÁõÆÊ®ôÈ†òÂüüË≥áÊñôÊ≥®ÂÖ•Ë®ìÁ∑¥ÈõÜÔºå‰ª•ÂèäÂêëË®ìÁ∑¥Ë≥áÊñôÂä†ÂÖ•ÈõúË®äÔºåÂ¶Ç‰ΩïÊúâÂä©ÊñºÊ≥õÂåñ„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÁµêÊûúÊèê‰æõ‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊìöÔºå‰∏¶ÈáèÂåñ‰∫ÜÊ∑±Â∫¶Â≠∏Áøí‰∏≠Áî±ÊéÉÊèèÂÑÄÈ†òÂüüÂÅèÁßªÈÄ†ÊàêÁöÑÊïàËÉΩ‰∏ãÈôçÁ®ãÂ∫¶ÔºåÁõÆÊ®ôÊòØÂºïÂ∞éÊú™‰æÜÈáùÂ∞çÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÂº∑ÂÅ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÁôºÂ±ï„ÄÇ</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

ÊëòË¶ÅÔºöÁîüÂ≠òÂàÜÊûêÊòØÁµ±Ë®àÂ≠∏ÁöÑ‰∏ÄÂÄãÂàÜÊîØÔºåÁî®ÊñºÂª∫Ê®°ÁâπÂÆö‰∫ã‰ª∂ÁôºÁîüÁöÑÊôÇÈñìÔºå‰∏¶Âª£Ê≥õÁî®ÊñºÈÜ´Â≠∏„ÄÅÂ∑•Á®ã„ÄÅÈáëËûçÂíåË®±Â§öÂÖ∂‰ªñÈ†òÂüü„ÄÇÂú®ÈÅ∏ÊìáÁîüÂ≠òÊ®°ÂûãÊôÇÔºåÈÄöÂ∏∏Âú®ÊÄßËÉΩÂíåÂèØËß£ÈáãÊÄß‰πãÈñìÈÄ≤Ë°åÊ¨äË°°ÔºåÂÖ∂‰∏≠ÊúÄÈ´òÊÄßËÉΩÊòØÁî±Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈªëÁõíÊ®°ÂûãÂØ¶ÁèæÁöÑ„ÄÇÈÄôÂú®ÈÜ´Â≠∏Á≠âÈ†òÂüüÊòØ‰∏ÄÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÂõ†ÁÇ∫ÂæûÊ•≠ËÄÖ‰∏çÈ°òÊÑèÁõ≤ÁõÆ‰ø°‰ªªÈªëÁõíÊ®°Âûã‰æÜÂÅöÂá∫ÈáçË¶ÅÁöÑÊÇ£ËÄÖÊ±∫Á≠ñ„ÄÇKolmogorov-ÈòøË´æÂæ∑Á∂≤Áµ° (KAN) ÊúÄËøëË¢´ÊèêË≠∞‰ΩúÁÇ∫Â§öÂ±§ÊÑüÁü•Âô® (MLP) ÁöÑÂèØËß£Èáã‰∏îÊ∫ñÁ¢∫ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü CoxKANÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÂèØËß£Èáã„ÄÅÈ´òÊÄßËÉΩÁîüÂ≠òÂàÜÊûêÁöÑ Cox ÊØî‰æãÈ¢®Èö™ Kolmogorov-Arnold Á∂≤Áµ°„ÄÇÊàëÂÄëÂú® 4 ÂÄãÂêàÊàêÊï∏ÊìöÈõÜÂíå 9 ÂÄãÁúüÂØ¶ÈÜ´ÁôÇÊï∏ÊìöÈõÜ‰∏äË©ï‰º∞‰∫ÜÊâÄÊèêÂá∫ÁöÑ CoxKAN„ÄÇÂêàÊàêÂØ¶È©óË°®ÊòéÔºåCoxKAN Ê∫ñÁ¢∫Âú∞ÊÅ¢Âæ©‰∫ÜÈ¢®Èö™ÂáΩÊï∏ÁöÑÂèØËß£ÈáãÁ¨¶ËôüÂÖ¨ÂºèÔºå‰∏¶ÊúâÊïàÂú∞Âü∑Ë°åËá™ÂãïÁâπÂæµÈÅ∏Êìá„ÄÇÂ∞ç 9 ÂÄãÁúüÂØ¶Êï∏ÊìöÈõÜÁöÑË©ï‰º∞Ë°®ÊòéÔºåCoxKAN ÂßãÁµÇÂÑ™Êñº Cox ÊØî‰æãÈ¢®Èö™Ê®°ÂûãÔºå‰∏¶‰∏îÈÅîÂà∞‰∫ÜÂÑ™ÊñºÊàñËàáË™øÊï¥ÂæåÁöÑ MLP Áõ∏Áï∂ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæ CoxKAN Ë≠òÂà•‰∫ÜÈ†êÊ∏¨ËÆäÈáè‰πãÈñìÁöÑË§áÈõú‰∫§‰∫í‰ΩúÁî®ÔºåÈÄô‰∫õ‰∫§‰∫í‰ΩúÁî®‰ΩøÁî®ÁèæÊúâÁöÑÁîüÂ≠òÊñπÊ≥ïÊ•µÈõ£Ë≠òÂà•Ôºå‰∏¶Ëá™ÂãïÊâæÂà∞Êè≠Á§∫ÈáçË¶ÅÁîüÁâ©Ê®ôË™åÁâ©Â∞çÊÇ£ËÄÖÈ¢®Èö™ÁöÑÊ∫ñÁ¢∫ÂΩ±ÈüøÁöÑÁ¨¶ËôüÂÖ¨Âºè„ÄÇ

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

ÊëòË¶ÅÔºöÂ§öÂô®ÂÆòÁñæÁóÖÁî±ÊñºÂêåÊôÇÂΩ±ÈüøÂ§öÂÄãÂô®ÂÆòÁ≥ªÁµ±ÔºåÂõ†Ê≠§ÊúÉÂ∏∂‰æÜÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅË§áÈõú‰∏îÂÖ∑ÊúâÈÅ©ÊáâÊÄßÁöÑÊ≤ªÁôÇÁ≠ñÁï•„ÄÇÂÑòÁÆ° AI È©ÖÂãïÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÊúÄËøëÊúâÈÄ≤Â±ïÔºå‰ΩÜÁèæÊúâËß£Ê±∫ÊñπÊ°àÂÉÖÈôêÊñºÂÄãÂà•Âô®ÂÆòÁ≥ªÁµ±„ÄÇÂÆÉÂÄëÂ∏∏Â∏∏ÂøΩÁï•Âô®ÂÆòÁ≥ªÁµ±‰πãÈñìÁöÑË§áÈõú‰æùË≥¥ÊÄßÔºåÂõ†Ê≠§ÁÑ°Ê≥ïÊèê‰æõÂØ¶Âãô‰∏äÊúâÁî®ÁöÑÊï¥È´îÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÂ±§Â§öÊô∫ËÉΩÈ´îÂº∑ÂåñÂ≠∏Áøí (HMARL) Êû∂Êßã‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÊ≠§Êû∂ÊßãÁÇ∫ÊØèÂÄãÂô®ÂÆòÁ≥ªÁµ±‰ΩøÁî®Â∞àÁî®Êô∫ËÉΩÈ´îÔºå‰∏¶ÈÄèÈÅéÊòéÁ¢∫ÁöÑÊô∫ËÉΩÈ´îÈñìÈÄöË®äÁÆ°ÈÅìÂª∫Ê®°ÂãïÊÖãÔºåËÆì‰∏çÂêåÂô®ÂÆò‰πãÈñìÁöÑÊ≤ªÁôÇÁ≠ñÁï•ËÉΩÂ§†ÂçîË™ø„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•ÈõôÂ±§ÁãÄÊÖãË°®Á§∫ÊäÄË°ìÔºåÂú®ÂêÑÁ®ÆÂ±§Á¥öË™ûÂ¢ÉÂåñÁóÖÊÇ£ÁãÄÊ≥ÅÔºå‰ª•ÊèêÂçáÊ≤ªÁôÇÊ∫ñÁ¢∫ÊÄßÂíåÁõ∏ÈóúÊÄß„ÄÇÈÄèÈÅéÂú®ÊïóË°ÄÁóáÔºà‰∏ÄÁ®ÆË§áÈõúÁöÑÂ§öÂô®ÂÆòÁñæÁóÖÔºâÁÆ°ÁêÜ‰∏≠ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂÆöÊÄßÂíåÂÆöÈáèË©ï‰º∞ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁ§∫‰∫ÜÂÆÉÂ≠∏ÁøíÊúâÊïàÊ≤ªÁôÇÊîøÁ≠ñÁöÑËÉΩÂäõÔºåÂèØÈ°ØËëóÊîπÂñÑÁóÖÊÇ£Â≠òÊ¥ªÁéá„ÄÇÊ≠§Êû∂ÊßãÊ®ôË™åËëóËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÁöÑ‰∏ÄÂ§ßÈÄ≤Ê≠•ÔºåÈñãÂâµ‰∫ÜÂ§öÂô®ÂÆòÊ≤ªÁôÇÂª∫Ë≠∞ÁöÑÂÖ®Èù¢ÊÄßÊñπÊ≥ï„ÄÇ

##### **Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**
2409.04481v1 by Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T. May, Geoffrey I. Webb, Shirui Pan, George Church

The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊï¥ÂêàÂà∞Ëó•Áâ©ÁôºÁèæÂíåÈñãÁôºÈ†òÂüüÊ®ôË™åËëóÈáçÂ§ßÁöÑÂÖ∏ÁØÑËΩâÁßªÔºåÊèê‰æõ‰∫ÜËß£ÁñæÁóÖÊ©üÂà∂„ÄÅ‰øÉÈÄ≤Ëó•Áâ©ÁôºÁèæÂíåÂÑ™ÂåñËá®Â∫äË©¶È©óÊµÅÁ®ãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊú¨Á∂úËø∞ÈáçÈªû‰ªãÁ¥π‰∫Ü LLM Âú®Èù©Êñ∞Ëó•Áâ©ÈñãÁôºÁÆ°Á∑öÂêÑÂÄãÈöéÊÆµ‰∏≠Êó•ÁõäÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÂÖàÈÄ≤ÁöÑË®àÁÆóÊ®°ÂûãÂ¶Ç‰ΩïÊè≠Á§∫Èù∂ÈªûÁñæÁóÖÈóúËÅØÊÄß„ÄÅËß£ÈáãË§áÈõúÁöÑÁîüÁâ©ÈÜ´Â≠∏Êï∏Êìö„ÄÅÂ¢ûÂº∑Ëó•Áâ©ÂàÜÂ≠êË®≠Ë®à„ÄÅÈ†êÊ∏¨Ëó•Áâ©ÁôÇÊïàÂíåÂÆâÂÖ®ÊÄßÔºå‰ª•Âèä‰øÉÈÄ≤Ëá®Â∫äË©¶È©óÊµÅÁ®ã„ÄÇÊàëÂÄëÁöÑË´ñÊñáÊó®Âú®ÁÇ∫Ë®àÁÆóÁîüÁâ©Â≠∏„ÄÅËó•ÁêÜÂ≠∏Âíå AI4Science ÁöÑÁ†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠ËÄÖÊèê‰æõÂÖ®Èù¢ÁöÑÊ¶ÇËø∞ÔºåÊ∑±ÂÖ•‰∫ÜËß£ LLM Â∞çËó•Áâ©ÁôºÁèæÂíåÈñãÁôºÁöÑÊΩõÂú®ËÆäÈù©ÊÄßÂΩ±Èüø„ÄÇ

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

ÊëòË¶ÅÔºöËá™ÂãïÈÜ´Â≠∏ÂΩ±ÂÉèÊïòËø∞ÁîüÊàêÊó®Âú®ÈÄèÈÅéÁõ¥Êé•ÂæûÊîæÂ∞ÑÂΩ±ÂÉèÁî¢ÁîüÁ≤æÁ¢∫ÁöÑËá®Â∫äÊèèËø∞ÔºåÊ∏õËºïÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑÂ∑•‰ΩúË≤†Êìî„ÄÇÁÑ∂ËÄåÔºåËàá‰∏ÄËà¨ÂΩ±ÂÉèÊ®ôÈ°å‰ªªÂãôÁõ∏ÊØîÔºåÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁ¥∞ÂæÆË¶ñË¶∫Â∑ÆÁï∞ÂíåÁâπÂÆöÈ†òÂüüË°ìË™ûÊúÉÂ∏∂‰æÜÈáçÂ§ßÊåëÊà∞„ÄÇÁèæÊúâÊñπÊ≥ïÂ∏∏Â∏∏ÂøΩÁï•Ê≠£Â∏∏ËàáÁï∞Â∏∏ÁôºÁèæ‰πãÈñìÁöÑÈáçË¶ÅÂçÄÂà•ÔºåÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ FODA-PGÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÁ¥∞Á≤íÂ∫¶Âô®ÂÆòÁñæÁóÖËá™ÈÅ©ÊáâÂàÜÂâ≤ÂúñÂΩ¢Êû∂ÊßãÔºåÈÄèÈÅéÈ†òÂüüËá™ÈÅ©ÊáâÂ≠∏Áøí‰æÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂„ÄÇFODA-PG ÈÄèÈÅéÂ∞áÁñæÁóÖÁõ∏ÈóúÂ±¨ÊÄß‰æùÊìöÂÖ∂Ëá®Â∫äÈáçË¶ÅÊÄßÂíå‰ΩçÁΩÆÂàÜÁÇ∫‰∏çÂêåÁöÑ„ÄåÁâπÂÆöÁñæÁóÖ„ÄçÂíå„ÄåÁÑ°ÁñæÁóÖ„ÄçÈ°ûÂà•Ôºå‰æÜÂª∫ÊßãÊîæÂ∞ÑÂ≠∏ÁôºÁèæÁöÑÁ¥∞Á≤íÂ∫¶ÂúñÂΩ¢Ë°®Á§∫„ÄÇÈÄôÁ®ÆËá™ÈÅ©ÊáâÂàÜÂâ≤‰ΩøÊàëÂÄëÁöÑÊ®°ÂûãËÉΩÂ§†ÊçïÊçâÊ≠£Â∏∏ËàáÁóÖÁêÜÁãÄÊÖã‰πãÈñìÁöÑÁ¥∞ÂæÆÂ∑ÆÁï∞ÔºåÊ∏õËºïË≥áÊñôÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇÈÄèÈÅéÂ∞áÈÄôÁ®ÆÁ¥∞Á≤íÂ∫¶Ë™ûÁæ©Áü•Ë≠òÊï¥ÂêàÂà∞Âº∑Â§ßÁöÑÂü∫ÊñºËΩâÊèõÂô®ÁöÑÊû∂Êßã‰∏≠Ôºå‰∏¶Êèê‰æõÂÖ∂ÊúâÊïàÊÄßÁöÑÂö¥Ë¨πÊï∏Â≠∏Ë≠âÊòéÔºåFODA-PG ËÉΩÂ§†ÁîüÊàêÁ≤æÁ¢∫‰∏îËá®Â∫ä‰∏äÈÄ£Ë≤´ÁöÑÂ†±ÂëäÔºå‰∏¶ÂÖ∑ÂÇôÂ¢ûÂº∑ÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇÂú® IU-Xray Âíå MIMIC-CXR Âü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºåÁ™ÅÈ°Ø‰∫ÜÈ†òÂüüÈÅ©ÊáâÂú®ÈÜ´Â≠∏Â†±ÂëäÁîüÊàê‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

ÊëòË¶ÅÔºö<paragraph>Ê∫ñÁ¢∫ÈáèÂåñÂ£ÅÈù¢Ââ™ÊáâÂäõÂãïÊÖãÂ∞çÊñºÂü∫Á§éÂíåÊáâÁî®Á†îÁ©∂‰∏≠ÁöÑÂêÑÁ®ÆÊáâÁî®ÂÖ∑ÊúâÂØ¶Ë≥™ÊÄßÁöÑÈáçË¶ÅÊÄßÔºåÊ∂µËìãÂæû‰∫∫È°ûÂÅ•Â∫∑Âà∞È£õÊ©üË®≠Ë®àÂíåÂÑ™ÂåñÁöÑÈ†òÂüü„ÄÇÂÑòÁÆ°Âú®ÂØ¶È©óÊ∏¨ÈáèÊäÄË°ìÂíåÂæåËôïÁêÜÊºîÁÆóÊ≥ïÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÈÄ≤Â±ïÔºå‰ΩÜÊôÇÈñìËß£ÊûêÂ£ÅÈù¢Ââ™ÊáâÂäõÂãïÊÖã‰ªçÂÖ∑ÊúâË∂≥Â§†ÁöÑÁ©∫ÈñìËß£ÊûêÂ∫¶ÂíåÂú®ÂêàÈÅ©ÁöÑÁ©∫ÈñìÂüü‰∏≠‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈõ£‰ª•ÊçâÊë∏ÁöÑÁõÆÊ®ô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÂæûÊπçÊµÅÂ£ÅÈù¢Á¥ÑÊùüÊµÅÁöÑÂ∞çÊï∏Â±§‰∏≠ÊîùÂèñÂ£ÅÈù¢Âπ≥Ë°åÈÄüÂ∫¶Â†¥Ôºå‰∏¶Ëº∏Âá∫Áõ∏ÊáâÁöÑ 2D Â£ÅÈù¢Ââ™ÊáâÂäõÂ†¥ÔºåÂÖ∑ÊúâÁõ∏ÂêåÁöÑÁ©∫ÈñìËß£ÊûêÂ∫¶ÂíåÂüüÂ§ßÂ∞è„ÄÇÂæûÁâ©ÁêÜËßíÂ∫¶‰æÜÁúãÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ÂÖÖÁï∂‰∏ÄÂÄã‰ª£ÁêÜÊ®°ÂûãÔºåÊ¶ÇÊã¨‰∫ÜÈ´òËÉΩÈáèÂ§ñÂ±§ÊµÅÁµêÊßãÂΩ±ÈüøÊéßÂà∂Â£ÅÈù¢Ââ™ÊáâÂäõÂãïÊÖãÁöÑÂêÑÁ®ÆÊ©üÂà∂„ÄÇË©≤Á∂≤Ë∑Ø‰ª•Áõ£Áù£ÊñπÂºèÂú®‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÊï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåË©≤Êï∏ÊìöÈõÜÂåÖÂê´Áµ±Ë®à 1D ÊπçÊµÅÈÄöÈÅìÁöÑÁõ¥Êé•Êï∏ÂÄºÊ®°Êì¨ÂíåÁ©∫ÈñìÁôºÂ±ïÁöÑÊπçÊµÅÈÇäÁïåÂ±§ÊµÅÔºåÊë©Êì¶Èõ∑Ë´æÊï∏ÁØÑÂúçÂæû 390 Âà∞ 1,500„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ∞çÂæûÁ≤íÂ≠êÂΩ±ÂÉèÊ∏¨ÈÄüÊ∏¨Èáè‰∏≠Áç≤ÂæóÁöÑÂØ¶È©óÈÄüÂ∫¶Â†¥ÁöÑÈõ∂Ê¨°ÊáâÁî®Ôºå‰∏¶‰ΩøÁî®ÂæÆÊü±Ââ™ÊáâÂäõÊÑüÊ∏¨Âô®Â∞çÈõ∑Ë´æÊï∏ÊúÄÈ´ò 2,000 ÁöÑÂêåÊ≠•Â£ÅÈù¢Ââ™ÊáâÂäõÊ∏¨ÈáèÈ©óË≠â‰∫ÜÂ£ÅÈù¢Ââ™ÊáâÂäõ‰º∞Ë®àÁöÑÁâ©ÁêÜÊ∫ñÁ¢∫ÊÄß„ÄÇÁ∏Ω‰πãÔºåÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁÇ∫ÂæûÂÆπÊòìÁç≤ÂæóÁöÑÈÄüÂ∫¶Ê∏¨Èáè‰∏≠ÊèêÂèñÁÑ°Ê≥ïÁç≤ÂæóÁöÑÂØ¶È©óÂ£ÅÈù¢Ââ™ÊáâÂäõË≥áË®äÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÂæûËÄå‰øÉÈÄ≤‰∫ÜÂêÑÁ®ÆÂØ¶È©óÊáâÁî®‰∏≠ÁöÑÈÄ≤Â±ï„ÄÇ</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫ÜÂñâÈïúÂ§öÊ®°ÊÄÅÂàÜÊûêÁ≥ªÁªü (MASL)Ôºå
ËØ•Á≥ªÁªüÁªìÂêàÈü≥È¢ëÂíåËßÜÈ¢ëÊï∞ÊçÆÔºåËá™Âä®‰ªéÂñâÈÉ®ËßÜÈ¢ëÈ¢ëÈó™ÈïúËßÜÈ¢ë‰∏≠ÊèêÂèñÂÖ≥ÈîÆ
ÁâáÊÆµÂíåÊåáÊ†áÔºåÁî®‰∫é‰∏¥Â∫äËØÑ‰º∞„ÄÇMASL Â∞ÜÂ£∞Èó®Ê£ÄÊµã‰∏éÂÖ≥ÈîÆËØçËØÜÂà´Áõ∏ÁªìÂêàÔºå‰ª•ÂàÜÊûê
ÊÇ£ËÄÖÂèëÂ£∞Âπ∂ÁªÜÂåñËßÜÈ¢ëÈáçÁÇπÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞Ê£ÄÊü•Â£∞Â∏¶ËøêÂä®„ÄÇËØ•Á≥ªÁªüÂåÖÊã¨‰∏Ä‰∏™È¢ëÈó™ËßÜÈ¢ëÊèêÂèñÊ®°ÂùóÔºå
ËØ•Ê®°ÂùóÈÄöËøáÂàÜÊûêËâ≤Áõ∏„ÄÅÈ•±ÂíåÂ∫¶ÂíåÂÄºÊ≥¢Âä®Êù•ËØÜÂà´Â∏ß„ÄÇ
MASL Ëøò‰∏∫Â£∞Â∏¶È∫ªÁóπÊ£ÄÊµãÊèê‰æõ‰∫ÜÊúâÊïàÁöÑÊåáÊ†áÔºå
ÈááÁî®‰∏§Èò∂ÊÆµÂ£∞Èó®ÂàÜÂâ≤ËøáÁ®ãÔºå‰ΩøÁî® U-NetÔºåÁÑ∂ÂêéËøõË°åÂü∫‰∫éÊâ©Êï£ÁöÑÁªÜÂåñ‰ª•ÂáèÂ∞ëËØØÊä•„ÄÇMASL ‰∏ç‰ΩøÁî®Â£∞Èó®Èù¢ÁßØÊ≥¢ÂΩ¢ÔºåËÄåÊòØ‰ªéÂ£∞Èó®Êé©Ê®°‰∏≠‰º∞ËÆ°ÂâçÂ£∞Èó®ËßíÊ≥¢ÂΩ¢ (AGAW)ÔºåËØÑ‰º∞Â∑¶Âè≥Â£∞Â∏¶‰ª•Ê£ÄÊµãÂçï‰æßÂ£∞Â∏¶È∫ªÁóπ (UVFP)„ÄÇÈÄöËøáÊØîËæÉ AGAW ÊñπÂ∑ÆÔºåMASL Âå∫ÂàÜÂ∑¶Âè≥È∫ªÁóπ„ÄÇÊ∂àËûçÁ†îÁ©∂ÂíåÂØπÂÖ¨ÂÖ±ÂíåÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜÁöÑÂÆûÈ™åÈ™åËØÅ‰∫Ü MASL ÁöÑÂàÜÂâ≤Ê®°ÂùóÔºåÂπ∂ËØÅÊòé‰∫ÜÂÖ∂Êèê‰æõÂèØÈù†ÁöÑ UVFP ËØäÊñ≠ÊåáÊ†áÁöÑËÉΩÂäõ„ÄÇ

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÁ≠âËá™ÂãïÂåñÂ∑•ÂÖ∑Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠‰ΩøÁî®ÁéáÊèêÂçáÔºåÊ∏õËºï‰∫ÜÊâãÂãïËº™ÂªìÊèèÁπ™ÁöÑÁì∂È†∏„ÄÇÈÄôÂ∑≤Â∞áÊâãÂãïÂãûÂãïËΩâÁßªÂà∞Ëá™ÂãïËº™ÂªìÁöÑÂìÅË≥™Ë©ï‰º∞ (QA)ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÅµÊ∏¨ÈåØË™§‰∏¶‰øÆÊ≠£ÂÆÉÂÄë„ÄÇÂçäËá™ÂãïÂåñ QA ÁöÑÊΩõÂú®Ëß£Ê±∫ÊñπÊ°àÊòØ‰ΩøÁî®Ê∑±Â∫¶Ë≤ùÊ∞è‰∏çÁ¢∫ÂÆöÊÄß‰æÜÂª∫Ë≠∞ÊΩõÂú®ÁöÑÈåØË™§ÂçÄÂüüÔºåÂæûËÄåÊ∏õÂ∞ëËä±Ë≤ªÂú®ÈåØË™§ÂÅµÊ∏¨‰∏äÁöÑÊôÇÈñì„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ë™øÊü•‰∏çÁ¢∫ÂÆöÊÄßÂíåÈåØË™§‰πãÈñìÁöÑÂ∞çÊáâÈóú‰øÇÔºåÁÑ∂ËÄåÔºåÂ∞öÊú™Â∞çÊîπÂñÑË≤ùÊ∞è‰∏çÁ¢∫ÂÆöÊÄßÂú∞ÂúñÁöÑ„ÄåÊïàÁî®„ÄçÈÄ≤Ë°åÁ†îÁ©∂Ôºå‰ª•‰ΩøÂÖ∂ÂÉÖÂá∫ÁèæÂú®‰∏çÊ∫ñÁ¢∫ÂçÄÂüüÔºåËÄå‰∏çÂá∫ÁèæÂú®Ê∫ñÁ¢∫ÂçÄÂüü„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ΩøÁî®Ê∫ñÁ¢∫Â∫¶Â∞çÊäó‰∏çÁ¢∫ÂÆöÊÄß (AvU) ÊêçÂ§±‰æÜË®ìÁ∑¥ FlipOut Ê®°ÂûãÔºåÈÄôÊúÉ‰øÉ‰Ωø‰∏çÁ¢∫ÂÆöÊÄßÂÉÖÂá∫ÁèæÂú®‰∏çÊ∫ñÁ¢∫ÂçÄÂüü„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÊáâÁî®ÊñºÂÖ©ÂÄãÊîæÂ∞ÑÊ≤ªÁôÇÈÉ®‰ΩçÁöÑË≥áÊñôÈõÜÔºåÂç≥È†≠È†∏ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂíåÂâçÂàóËÖ∫Ê†∏Á£ÅÂÖ±ÊåØÊéÉÊèè„ÄÇ‰ΩøÁî®Êé•Êî∂Âô®Êìç‰ΩúÁâπÊÄß (ROC) ÂíåÁ≤æÁ¢∫Â∫¶Âè¨ÂõûÁéá (PR) Êõ≤Á∑öÔºåÈáùÂ∞çÈ´îÁ¥†‰∏çÊ∫ñÁ¢∫ÊÄßË©ï‰º∞‰∏çÁ¢∫ÂÆöÊÄßÁÜ±ÂúñÔºàÂç≥È†êÊ∏¨ÁÜµÔºâ„ÄÇÊï∏ÂÄºÁµêÊûúÈ°ØÁ§∫ÔºåËàáË≤ùÊ∞èÂü∫Ê∫ñÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊàêÂäüÂú∞ÊäëÂà∂Ê∫ñÁ¢∫È´îÁ¥†ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂ∞çÊñº‰∏çÊ∫ñÁ¢∫È´îÁ¥†ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂ≠òÂú®È°û‰ººÊÉÖÊ≥Å„ÄÇÂèØÂú® https://github.com/prerakmody/bayesuncertainty-error-correspondence ÂèñÂæóÈáçÁèæÂØ¶È©óÁöÑÁ®ãÂºèÁ¢º

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-P√©rez, Silvia Garc√≠a-M√©ndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

ÊëòË¶ÅÔºö<paragraph>Ê†πÊìöÂÆòÊñπÁöÑ‰º∞Ë®àÔºåÂÖ®ÁêÉÁ¥ÑÊúâ 5000 Ëê¨‰∫∫ÁΩπÊÇ£Â§±Êô∫ÁóáÔºå‰∏îÈÄôÂÄãÊï∏Â≠óÊØèÂπ¥Â¢ûÂä† 1000 Ëê¨ÂêçÊñ∞ÊÇ£ËÄÖ„ÄÇÂú®Ê≤íÊúâÊ≤ªÁôíÊñπÊ≥ïÁöÑÊÉÖÊ≥Å‰∏ãÔºåËá®Â∫äÈ†êÂæåÂíåÊó©Êúü‰ªãÂÖ•ÊòØÂª∂Á∑©ÂÖ∂ÊÉ°ÂåñÁöÑÊúÄÊúâÊïàÊñπÊ≥ï„ÄÇÁÇ∫Ê≠§Ôºå‰∫∫Â∑•Êô∫ÊÖßÂíåË®àÁÆóË™ûË®ÄÂ≠∏ÂèØË¢´Áî®ÊñºËá™ÁÑ∂Ë™ûË®ÄÂàÜÊûê„ÄÅÂÄã‰∫∫ÂåñË©ï‰º∞„ÄÅÁõ£ÊéßÂíåÊ≤ªÁôÇ„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÊñπÊ≥ïÈúÄË¶ÅÊõ¥Â§öË™ûÁæ©Áü•Ë≠òÁÆ°ÁêÜÂíåÂèØËß£ÈáãÊÄßËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂÑòÁÆ°ÈÄô‰∫õÊ®°Âûã‰ª£Ë°®‰∫Ü‰ΩøÁî®Êô∫ÊÖßÁ≥ªÁµ±ÈÄ≤Ë°åËá®Â∫äÊÇ£ËÄÖÊ∫ùÈÄöÁöÑÊúÄÂÖàÈÄ≤ÊñπÂºèÔºå‰ΩÜÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî®ÊñºË™çÁü•ËÉΩÂäõ‰∏ãÈôçË®∫Êñ∑‰ªçÁÑ∂ÂæàÂ∞ëË¶ã„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂà©Áî®ËÅäÂ§©Ê©üÂô®‰∫∫Ëß£Ê±∫ÊñπÊ°à‰∏≠‰ΩøÁî®ÊúÄÊñ∞Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÊäÄË°ìÁöÑ LLMÔºå‰ª•Êèê‰æõÂ∞çË™çÁü•ËÉΩÂäõ‰∏ãÈôçÁöÑÊ©üÂô®Â≠∏ÁøíÈ†êÊ∏¨„ÄÇË™ûË®ÄÊ¶ÇÂøµÁâπÂæµË¢´Áî®ÊñºÈÅ©Áï∂ÁöÑËá™ÁÑ∂Ë™ûË®ÄÂàÜÊûê„ÄÇÈÄèÈÅéÂèØËß£ÈáãÊÄßÔºåÊàëÂÄëÊó®Âú®Ê∂àÈô§Ê®°ÂûãÁöÑÊΩõÂú®ÂÅèÂ∑ÆÔºå‰∏¶ÊèêÈ´òÂÖ∂Âú®Ë®∫Êñ∑Ê±∫Á≠ñ‰∏≠ÂçîÂä©Ëá®Â∫äÂ∑•‰ΩúËÄÖÁöÑÊΩõÂäõ„ÄÇÊõ¥Ë©≥Á¥∞Âú∞Ë™™ÔºåÊâÄÊèêÂá∫ÁöÑÁÆ°ÈÅìÂåÖÊã¨Ôºö(i) ‰ΩøÁî®Âü∫Êñº NLP ÁöÑÊèêÁ§∫Â∑•Á®ãÈÄ≤Ë°åË≥áÊñôËêÉÂèñÔºõ(ii) ‰∏≤ÊµÅÂºèË≥áÊñôËôïÁêÜÔºåÂåÖÊã¨ÁâπÂæµÂ∑•Á®ã„ÄÅÂàÜÊûêÂíåÈÅ∏ÊìáÔºõ(iii) Âç≥ÊôÇÂàÜÈ°ûÔºõ‰ª•Âèä (iv) ÂèØËß£ÈáãÊÄßÂÑÄË°®ÊùøÔºå‰ª•Êèê‰æõÈ†êÊ∏¨ÁµêÊûúÁöÑÂèØË¶ñÂåñÂíåËá™ÁÑ∂Ë™ûË®ÄÊèèËø∞„ÄÇÂàÜÈ°ûÁµêÊûúÂú®ÊâÄÊúâË©ï‰º∞ÊåáÊ®ô‰∏≠ÈÉΩË∂ÖÈÅé 80%ÔºåÂøÉÊô∫ÈÄÄÂåñÈ°ûÂà•ÁöÑÂè¨ÂõûÁéáÁ¥ÑÁÇ∫ 85%„ÄÇÁ∏ΩËÄåË®Ä‰πãÔºåÊàëÂÄëÁÇ∫ÈÄôÈ†ÖÂ∑•‰ΩúË≤¢Áçª‰∫Ü‰∏ÄÂÄãÁ∂ìÊøüÂØ¶ÊÉ†„ÄÅÈùàÊ¥ª„ÄÅÈùû‰æµÂÖ•ÊÄß„ÄÅÂÄã‰∫∫ÂåñÁöÑË®∫Êñ∑Á≥ªÁµ±„ÄÇ</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

ÊëòË¶ÅÔºöÂÖ®ÁêÉ‰∫∫Âè£Âø´ÈÄüËÄÅÂåñË∂®Âã¢Â∞éËá¥Â§±Êô∫ÁóáÁóÖ‰æãÂ¢ûÂä†ÔºåÂåÖÊã¨ÈòøËå≤Êµ∑ÈªòÁóáÔºåÁ™ÅÈ°ØÂá∫Êó©Êúü‰∏îÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑ÊñπÊ≥ïÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÂÇ≥Áµ±ÁöÑË®∫Êñ∑ÊäÄË°ìÔºå‰æãÂ¶ÇË™çÁü•Ê∏¨È©ó„ÄÅÁ•ûÁ∂ìÂΩ±ÂÉèÂíåÁîüÁâ©Ê®ôË®òÂàÜÊûêÔºåÂú®ÊïèÊÑüÊÄß„ÄÅÂèØÂèäÊÄßÂíåÊàêÊú¨ÊñπÈù¢Èù¢Ëá®ÈáçÂ§ßÈôêÂà∂ÔºåÁâπÂà•ÊòØÂú®Êó©ÊúüÈöéÊÆµ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÊ©üÂô®Â≠∏Áøí (ML) ‰ΩúÁÇ∫‰∏ÄÁ®ÆËÆäÈù©ÊÄßÊñπÊ≥ïÁöÑÊΩõÂäõÔºåÈÄöÈÅéÂà©Áî® ML Ê®°ÂûãÂàÜÊûêÂíåÊï¥ÂêàË§áÈõúÁöÑÂ§öÊ®°ÂºèÊï∏ÊìöÈõÜÔºåÂåÖÊã¨Ë™çÁü•Ë©ï‰º∞„ÄÅÁ•ûÁ∂ìÂΩ±ÂÉèÂíåÈÅ∫ÂÇ≥‰ø°ÊÅØÔºå‰æÜÂ¢ûÂº∑Êó©ÊúüÂ§±Êô∫ÁóáÊ™¢Ê∏¨„ÄÇÂ∞çÁèæÊúâÊñáÁçªÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÂõûÈ°ßÔºå‰ª•Ë©ï‰º∞ÂêÑÁ®Æ ML Ê®°ÂûãÔºåÂåÖÊã¨Áõ£Áù£Â≠∏Áøí„ÄÅÊ∑±Â∫¶Â≠∏ÁøíÂíåÂÖàÈÄ≤ÊäÄË°ìÔºå‰æãÂ¶ÇÈõÜÊàêÂ≠∏ÁøíÂíåTransformerÊ®°ÂûãÔºåË©ï‰º∞ÂÖ∂Ê∫ñÁ¢∫ÊÄß„ÄÅÂèØËß£ÈáãÊÄßÂíåËá®Â∫äÊï¥ÂêàÁöÑÊΩõÂäõ„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ° ML Ê®°ÂûãÂú®ÊèêÈ´òË®∫Êñ∑Á≤æÂ∫¶ÂíåÂØ¶ÁèæÊó©ÊúüÂπ≤È†êÊñπÈù¢È°ØÁ§∫Âá∫È°ØËëóÁöÑÂ∏åÊúõÔºå‰ΩÜÂÖ∂ÂèØÊ¶ÇÂåñÊÄß„ÄÅÂèØËß£ÈáãÊÄßÂíåÈÅìÂæ∑ÈÉ®ÁΩ≤‰ªçÁÑ∂Â≠òÂú®ÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂ÊúÄÂæåÊ¶ÇËø∞‰∫ÜÊó®Âú®Â¢ûÂº∑ ML Ê®°ÂûãÂú®Â§±Êô∫ÁóáÊ™¢Ê∏¨‰∏≠ÁöÑËá®Â∫äÊïàÁî®ÁöÑÊú™‰æÜÊñπÂêëÔºåÂº∑Ë™øË∑®Â≠∏ÁßëÂêà‰ΩúÂíåÈÅìÂæ∑ÂÅ•ÂÖ®ÁöÑÊ°ÜÊû∂Ôºå‰ª•ÊîπÂñÑÈòøËå≤Êµ∑ÈªòÁóáÂíåÂÖ∂‰ªñÂΩ¢ÂºèÂ§±Êô∫ÁóáÁöÑÊó©ÊúüÊ™¢Ê∏¨ÂíåÂπ≤È†êÁ≠ñÁï•„ÄÇ

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

ÊëòË¶ÅÔºöÁöÆËÜöÁôåÂàÜÂâ≤Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠ÊßãÊàê‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÁèæÊúâË®±Â§öËß£Ê±∫ÊñπÊ°àÔºà‰∏ªË¶ÅÊòØÂü∫Êñº CNNÔºâÈù¢Ëá®Áº∫‰πèÊï¥È´îËÉåÊôØÁêÜËß£ÁöÑÂïèÈ°å„ÄÇÊàñËÄÖÔºå‰∏Ä‰∫õÊñπÊ≥ïË®¥Ë´∏ÊñºÂ§ßË¶èÊ®° Transformer Ê®°Âûã‰æÜÂΩåÂêàÊï¥È´îËÉåÊôØÂ∑ÆË∑ùÔºå‰ΩÜÁäßÁâ≤‰∫ÜÊ®°ÂûãÂ§ßÂ∞èÂíåË®àÁÆóË§áÈõúÂ∫¶„ÄÇÊúÄÂæåÔºåË®±Â§öÂü∫Êñº Transformer ÁöÑÊñπÊ≥ï‰∏ªË¶Å‰æùË≥¥ÊñºÂü∫Êñº CNN ÁöÑËß£Á¢ºÂô®ÔºåËÄåÂøΩË¶ñ‰∫ÜÂü∫Êñº Transformer ÁöÑËß£Á¢ºÊ®°ÂûãÁöÑÂÑ™Èªû„ÄÇË™çË≠òÂà∞ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÈÄöÈÅéÂºïÂÖ• MobileUNETR ‰æÜËß£Ê±∫Â∞çÈ´òÊïàËºïÈáèÁ¥öËß£Ê±∫ÊñπÊ°àÁöÑÈúÄÊ±ÇÔºåÂÖ∂ÁõÆÊ®ôÊòØÂÖãÊúçËàá CNN Âíå Transformer Áõ∏ÈóúÁöÑÊïàËÉΩÈôêÂà∂ÔºåÂêåÊôÇÊúÄÂ∞èÂåñÊ®°ÂûãÂ§ßÂ∞èÔºåÁÇ∫È´òÊïàÂΩ±ÂÉèÂàÜÂâ≤ÈÇÅÂá∫ÊúâÂ∏åÊúõÁöÑ‰∏ÄÊ≠•„ÄÇMobileUNETR Êúâ 3 ÂÄã‰∏ªË¶ÅÁâπÈªû„ÄÇ1) MobileUNETR ÂåÖÂê´‰∏ÄÂÄãËºïÈáèÁ¥öÊ∑∑Âêà CNN-Transformer Á∑®Á¢ºÂô®Ôºå‰ª•ÊúâÊïàÁöÑÊñπÂºèÂπ´Âä©Âπ≥Ë°°Â±ÄÈÉ®ÂíåÊï¥È´îËÉåÊôØÁâπÂæµÊèêÂèñÔºõ2) ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ∑∑ÂêàËß£Á¢ºÂô®ÔºåÂú®Ëß£Á¢ºÈöéÊÆµÂêåÊôÇÂà©Áî®‰∏çÂêåËß£ÊûêÂ∫¶‰∏ãÁöÑ‰ΩéÈöéÂíåÊï¥È´îÁâπÂæµÔºå‰ª•ÈÄ≤Ë°åÁ≤æÁ¢∫ÁöÑÈÅÆÁΩ©ÁîüÊàêÔºõ3) Ë∂ÖË∂äÂ§ßÂûãËÄåË§áÈõúÁöÑÊû∂ÊßãÔºåMobileUNETR ‰ª• 300 Ëê¨ÂÄãÂèÉÊï∏Âíå 1.3 GFLOP ÁöÑË®àÁÆóË§áÈõúÂ∫¶ÂØ¶Áèæ‰∫ÜÂçìË∂äÁöÑÊïàËÉΩÔºåÂàÜÂà•Ê∏õÂ∞ë‰∫Ü 10 ÂÄçÂíå 23 ÂÄçÁöÑÂèÉÊï∏Âíå FLOP„ÄÇÂ∑≤Á∂ìÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•È©óË≠âÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂõõÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑÁöÆËÜöÁóÖËÆäÂàÜÂâ≤Ë≥áÊñôÈõÜÔºàÂåÖÊã¨ ISIC 2016„ÄÅISIC 2017„ÄÅISIC 2018 Âíå PH2 Ë≥áÊñôÈõÜÔºâ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÊñºÔºöhttps://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

ÊëòË¶ÅÔºöÈõ∑Ê∞èË§áÈõúÂúñÂΩ¢Ê∏¨È©ó (RCFT) Á≠âÁπ™Áï´Ê∏¨È©óÂª£Ê≥õÁî®ÊñºË©ï‰º∞Ë¶ñË¶∫Á©∫ÈñìÊäÄËÉΩÂíåË®òÊÜ∂ÂäõÁ≠âË™çÁü•ÂäüËÉΩÔºå‰ΩøÂÖ∂ÊàêÁÇ∫Ê™¢Ê∏¨ËºïÂ∫¶Ë™çÁü•ÈöúÁ§ô (MCI) ÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑„ÄÇÂÑòÁÆ°ÂÆÉÂÄëÂæàÊúâÁî®Ôºå‰ΩÜÂü∫ÊñºÈÄô‰∫õÊ∏¨È©óÁöÑÁèæÊúâÈ†êÊ∏¨Ê®°ÂûãÈÄöÂ∏∏ÊúÉÂèóÂà∞Ê®£Êú¨ÈáèÂ∞èÂíåÁº∫‰πèÂ§ñÈÉ®È©óË≠âÁ≠âÈôêÂà∂ÔºåÈÄôÊúÉÊêçÂÆ≥ÂÖ∂ÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂ§ö‰∏≤ÊµÅÊ∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂ÔºåÂÆÉÊï¥Âêà‰∫ÜÂÖ©ÂÄã‰∏çÂêåÁöÑËôïÁêÜ‰∏≤ÊµÅÔºö‰∏ÄÂÄãÂü∫ÊñºÂ§öÈ†≠Ëá™Ê≥®ÊÑèÂäõÔºå‰ΩøÁî®ÂéüÂßã RCFT ÂΩ±ÂÉèÁöÑÁ©∫Èñì‰∏≤ÊµÅÔºå‰ª•Âèä‰∏ÄÂÄãÊé°Áî®ÂÖàÂâçÈñãÁôºÁöÑËá™ÂãïË©ïÂàÜÁ≥ªÁµ±ÁöÑË©ïÂàÜ‰∏≤ÊµÅ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÈüìÂúãÁæ§ÁµÑ‰∏≠ 1,740 ÂêçÂèóË©¶ËÄÖÁöÑË≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰∏¶Âú®‰æÜËá™ÈüìÂúãÁöÑ 222 ÂêçÂèóË©¶ËÄÖÁöÑÂ§ñÈÉ®ÈÜ´Èô¢Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÈ©óË≠â„ÄÇÊâÄÊèêÂá∫ÁöÑÂ§ö‰∏≤ÊµÅÊ®°ÂûãÂú®Â§ñÈÉ®È©óË≠â‰∏≠Ë°®ÁèæÂá∫ÂÑ™ÊñºÂü∫Ê∫ñÊ®°ÂûãÁöÑÊïàËÉΩ (AUC = 0.872ÔºåÊ∫ñÁ¢∫Áéá = 0.781)„ÄÇÁ©∫ÈñìÂíåË©ïÂàÜ‰∏≤ÊµÅÁöÑÊï¥Âêà‰ΩøÊ®°ÂûãËÉΩÂ§†ÂæûÂéüÂßãÂΩ±ÂÉèÊì∑ÂèñË§áÈõúÁöÑË¶ñË¶∫Á¥∞ÁØÄÔºåÂêåÊôÇ‰πüËÉΩÁ¥çÂÖ•ÁµêÊßãÂåñÁöÑË©ïÂàÜË≥áÊñôÔºåÈÄôÂÖ±ÂêåÂ¢ûÂº∑‰∫ÜÂÆÉÊ™¢Ê∏¨Á¥∞ÂæÆË™çÁü•ÈöúÁ§ôÁöÑËÉΩÂäõ„ÄÇÈÄôÁ®ÆÈõôÈáçÊñπÊ≥ï‰∏çÂÉÖÊèêÈ´ò‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÔºå‰πüÂ¢ûÂä†‰∫ÜÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÔºå‰ΩøÂÖ∂Âú®‰∏çÂêåÁöÑËá®Â∫äÁí∞Â¢É‰∏≠Êõ¥ÂèØÈù†„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞çËá®Â∫äÁí∞Â¢ÉÊúâÂØ¶ÈöõÁöÑÊÑèÁæ©ÔºåÂÆÉÂèØ‰ª•Âú®ÂÖ∂‰∏≠‰ΩúÁÇ∫Êó©Êúü MCI ÁØ©Ê™¢ÁöÑÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v2 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of active fire spots detected by the AQUA\_M-T
satellite in the Amazon, Brazil. The approach employs a mixed Recurrent Neural
Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated
Recurrent Unit (GRU) architectures to predict the monthly accumulations of
daily detected active fire spots. Data analysis revealed a consistent
seasonality over time, with annual maximum and minimum values tending to repeat
at the same periods each year. The primary objective is to verify whether the
forecasts capture this inherent seasonality through machine learning
techniques. The methodology involved careful data preparation, model
configuration, and training using cross-validation with two seeds, ensuring
that the data generalizes well to both the test and validation sets for both
seeds. The results indicate that the combined LSTM and GRU model delivers
excellent forecasting performance, demonstrating its effectiveness in capturing
complex temporal patterns and modeling the observed time series. This research
significantly contributes to the application of deep learning techniques in
environmental monitoring, specifically in forecasting active fire spots. The
proposed approach highlights the potential for adaptation to other time series
forecasting challenges, opening new opportunities for research and development
in machine learning and prediction of natural phenomena.
  Keywords: Time Series Forecasting; Recurrent Neural Networks; Deep Learning.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∂úÂêàÊñπÊ≥ïÔºåÁî®ÊñºÂª∫Ê®°ÂíåÈ†êÊ∏¨ AQUA\_M-T Ë°õÊòüÂú®Â∑¥Ë•ø‰∫ûÈ¶¨ÈÅúÂú∞ÂçÄÂÅµÊ∏¨Âà∞ÁöÑÊ≠∑Âè≤Ê¥ªË∫çÁÅ´ÈªûÊôÇÈñìÂ∫èÂàó„ÄÇÊ≠§ÊñπÊ≥ïÊé°Áî®Ê∑∑ÂêàÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑Ø (RNN) Ê®°ÂûãÔºåÁµêÂêàÈï∑Áü≠ÊúüË®òÊÜ∂ (LSTM) ÂíåÈñÄÊéßÈÅûËø¥ÂñÆÂÖÉ (GRU) Êû∂ÊßãÔºå‰æÜÈ†êÊ∏¨ÊØèÊó•ÂÅµÊ∏¨Âà∞ÁöÑÊ¥ªË∫çÁÅ´ÈªûÁöÑÊØèÊúàÁ¥ØÁ©çÂÄº„ÄÇË≥áÊñôÂàÜÊûêÈ°ØÁ§∫Âá∫Èö®ËëóÊôÇÈñìÊé®ÁßªÁöÑ‰∏ÄËá¥Â≠£ÁØÄÊÄßÔºåÊØèÂπ¥ÁöÑÊúÄÂ§ßÂÄºÂíåÊúÄÂ∞èÂÄºÂÇæÂêëÊñºÂú®ÊØèÂπ¥Áõ∏ÂêåÁöÑÊôÇÊúüÈáçË§áÂá∫Áèæ„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÈ©óË≠âÈ†êÊ∏¨ÊòØÂê¶ÈÄèÈÅéÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÊçïÊçâÂà∞ÈÄôÁ®ÆÂõ∫ÊúâÁöÑÂ≠£ÁØÄÊÄß„ÄÇÊ≠§ÊñπÊ≥ïÊ∂âÂèä‰ªîÁ¥∞ÁöÑË≥áÊñôÊ∫ñÂÇô„ÄÅÊ®°ÂûãÁµÑÊÖãÂíå‰ΩøÁî®ÂÖ©ÂÄãÁ®ÆÂ≠êÁöÑ‰∫§ÂèâÈ©óË≠âÈÄ≤Ë°åË®ìÁ∑¥ÔºåÁ¢∫‰øùË≥áÊñôÂ∞çÊñºÂÖ©ÂÄãÁ®ÆÂ≠êÁöÑÊ∏¨Ë©¶ÂíåÈ©óË≠âÈõÜÈÉΩËÉΩÂæàÂ•ΩÂú∞Ê¶ÇÂåñ„ÄÇÁµêÊûúË°®ÊòéÔºåLSTM Âíå GRU ÁöÑÁµÑÂêàÊ®°ÂûãÊèê‰æõ‰∫ÜÊ•µ‰Ω≥ÁöÑÈ†êÊ∏¨ÊïàËÉΩÔºåË≠âÊòéÂÖ∂Âú®ÊçïÊçâË§áÈõúÁöÑÊôÇÈñìÊ®°ÂºèÂíåÂ∞çËßÄÊ∏¨Âà∞ÁöÑÊôÇÈñìÂ∫èÂàóÈÄ≤Ë°åÂª∫Ê®°ÊñπÈù¢ÈùûÂ∏∏ÊúâÊïà„ÄÇÈÄôÈ†ÖÁ†îÁ©∂È°ØËëóÂú∞‰øÉÊàê‰∫ÜÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÂú®Áí∞Â¢ÉÁõ£Ê∏¨‰∏≠ÁöÑÊáâÁî®ÔºåÁâπÂà•ÊòØÂú®È†êÊ∏¨Ê¥ªË∫çÁÅ´ÈªûÊñπÈù¢„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁ™ÅÈ°Ø‰∫ÜÈÅ©ÊáâÂÖ∂‰ªñÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ÊåëÊà∞ÁöÑÊΩõÂäõÔºåÁÇ∫Ê©üÂô®Â≠∏ÁøíÂíåËá™ÁÑ∂ÁèæË±°È†êÊ∏¨ÊñπÈù¢ÁöÑÁ†îÁ©∂ÂíåÈñãÁôºÈñãÂïü‰∫ÜÊñ∞ÁöÑÊ©üÊúÉ„ÄÇ
ÈóúÈçµÂ≠óÔºöÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ÔºõÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑ØÔºõÊ∑±Â∫¶Â≠∏Áøí„ÄÇ

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºË¶ñË¶∫ÁöÑÂ§ñÁßëÂ∞éËà™Áî±ÊñºÂÖ∂Èùû‰æµÂÖ•ÊÄß„ÄÅÊàêÊú¨ÊïàÁõäÂíåÈùàÊ¥ªÊÄßÂÑ™Âã¢ËÄåÂèóÂà∞Ë∂ä‰æÜË∂äÂ§öÁöÑÈóúÊ≥®„ÄÇÁâπÂà•ÊòØÔºåÂü∫ÊñºË¶ñË¶∫ÁöÑÂ∞éËà™Á≥ªÁµ±ÁöÑ‰∏ÄÂÄãÈóúÈçµÂÖÉÁ¥†ÊòØËøΩËπ§ÊâãË°ìÂô®Ê¢∞„ÄÇËàá 2D Âô®Ê¢∞ËøΩËπ§ÊñπÊ≥ïÁõ∏ÊØîÔºå3D Âô®Ê¢∞ËøΩËπ§Âú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÂÖ∑ÊúâÊõ¥Âª£Ê≥õÁöÑÂÉπÂÄºÔºå‰ΩÜÁî±ÊñºÁ¥ãÁêÜÂº±„ÄÅÈÅÆÊìãÂíåÁº∫‰πèÁî®Êñº 3D ÈÖçÊ∫ñÁöÑÈõªËÖ¶ËºîÂä©Ë®≠Ë®à (CAD) Ê®°ÂûãÔºåÂõ†Ê≠§‰πüÊõ¥ÂÖ∑ÊåëÊà∞ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ SurgTrackÔºå‰∏ÄÁ®ÆÈÅ©Áî®ÊñºÁÑ° CAD ÂíåÁ©©ÂÅ•ÁöÑÁúüÂØ¶‰∏ñÁïåÊáâÁî®Á®ãÂºèÁöÑÂÖ©ÈöéÊÆµ 3D Âô®Ê¢∞ËøΩËπ§ÊñπÊ≥ï„ÄÇÂú®Á¨¨‰∏ÄÂÄãÈÖçÊ∫ñÈöéÊÆµÔºåÊàëÂÄëÊï¥Âêà‰∏ÄÂÄãÂô®Ê¢∞Á∞ΩÁΩ≤Ë∑ùÈõ¢Â†¥ (SDF)ÔºåÂ∞çÂô®Ê¢∞ÁöÑ 3D Ë°®ÂæµÈÄ≤Ë°åÂª∫Ê®°ÔºåÂØ¶ÁèæÁÑ° CAD ÁöÑ 3D ÈÖçÊ∫ñ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂèØ‰ª•ÈÄèÈÅéÂ∞áË¶ñË®ä‰∏≤ÊµÅËàáÂ∑≤ÈÖçÊ∫ñÁöÑ SDF Ê®°ÂûãÈÄ≤Ë°åÂåπÈÖçÔºåÂèñÂæóÂô®Ê¢∞Âú® 3D Á©∫Èñì‰∏≠ÁöÑ‰ΩçÁΩÆÂíåÊñπÂêë„ÄÇÂú®Á¨¨‰∫åÂÄãËøΩËπ§ÈöéÊÆµÔºåÊàëÂÄëË®≠Ë®à‰∏ÄÂÄãÂßøÂã¢ÂúñÊúÄ‰Ω≥ÂåñÊ®°ÁµÑÔºåÂà©Áî®ÂßøÂã¢Ë®òÊÜ∂Ê±†ÁöÑÊ≠∑Âè≤ËøΩËπ§ÁµêÊûú‰æÜÊúÄ‰Ω≥ÂåñËøΩËπ§ÁµêÊûú‰∏¶ÊîπÂñÑÈÅÆÊìãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊî∂ÈõÜ Instrument3D Ë≥áÊñôÈõÜÔºå‰ª•ÂÖ®Èù¢Ë©ï‰º∞ÊâãË°ìÂô®Ê¢∞ÁöÑ 3D ËøΩËπ§„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄë SurgTrack ÁöÑÂÑ™Ë∂äÊÄßÂíåÂèØÊì¥ÂÖÖÊÄßÔºå‰ª•È°ØËëóÁöÑÊîπÈÄ≤ÂÑ™ÊñºÁèæÊúâÊäÄË°ì„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® https://github.com/wenwucode/SurgTrack ÂèñÂæó„ÄÇ</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

ÊëòË¶ÅÔºö‰º∞Ë®àÁöÑËÖéÂ∞èÁêÉÈÅéÊøæÁéá (eGFR) ÊòØËá®Â∫äÂØ¶Âãô‰∏≠ËÖéËáüÂäüËÉΩÁöÑÈáçË¶ÅÊåáÊ®ô„ÄÇÈõñÁÑ∂ÂÇ≥Áµ±ÊñπÁ®ãÂºèÂíå‰ΩøÁî®Ëá®Â∫äËàáÂØ¶È©óÂÆ§Ë≥áÊñôÁöÑÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÂèØ‰ª•‰º∞Ë®à eGFRÔºå‰ΩÜÊ∫ñÁ¢∫È†êÊ∏¨Êú™‰æÜ eGFR Ê∞¥Âπ≥‰ªçÁÑ∂ÊòØËÖéËáüÁßëÈÜ´Â∏´Âíå ML Á†îÁ©∂‰∫∫Âì°ÁöÑ‰∏ÄÂ§ßÊåëÊà∞„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂ÈÄ≤Â±ïÈ°ØÁ§∫ÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) ÂèØ‰ª•‰ΩúÁÇ∫ÂêÑÁ®ÆÊáâÁî®Á®ãÂºèÁöÑÂº∑ÂÅ•Âü∫Á§éÊ®°Âûã„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é LMM È†êÊ∏¨Êú™‰æÜ eGFR Ê∞¥Âπ≥ÁöÑÊΩõÂäõÔºåÂÖ∂Ë≥áÊñôÈõÜÂåÖÂê´ 50 ‰ΩçÁóÖÊÇ£ÁöÑÂØ¶È©óÂÆ§ÂíåËá®Â∫äÊï∏ÂÄº„ÄÇÈÄèÈÅéÊï¥ÂêàÂêÑÁ®ÆÊèêÁ§∫ÊäÄË°ìÂíå LMM ÁöÑÂêàÂ•èÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÈÄô‰∫õÊ®°ÂûãÂú®ÁµêÂêàÁ≤æÁ¢∫ÊèêÁ§∫Âíå eGFR ËªåË∑°ÁöÑË¶ñË¶∫ÂåñË°®Á§∫ÊôÇÔºåÂèØÊèê‰æõËàáÁèæÊúâ ML Ê®°ÂûãÁõ∏ËøëÁöÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êì¥Â±ï‰∫ÜÂü∫Á§éÊ®°ÂûãÁöÑÊáâÁî®Ôºå‰∏¶ÁÇ∫Êú™‰æÜÁ†îÁ©∂Âà©Áî®ÈÄô‰∫õÊ®°Âûã‰æÜÊáâÂ∞çË§áÈõúÁöÑÈÜ´ÁôÇÈ†êÊ∏¨ÊåëÊà∞Êèê‰æõ‰∫ÜÈÄîÂæë„ÄÇ

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢Âõ†ÂÖ∂Êèê‰æõÈùû‰æµÂÖ•ÊÄß„ÄÅÁÑ°ËºªÂ∞Ñ‰∏îÂç≥ÊôÇÂΩ±ÂÉèÁöÑÂÑ™ÈªûÔºåËÄåÂª£Ê≥õÁî®ÊñºËá®Â∫ä‰ªãÂÖ•ÂíåË®∫Êñ∑„ÄÇ
ÁÑ∂ËÄåÔºåÁî±ÊñºÊìç‰ΩúÂì°ÈúÄË¶ÅÂ§ßÈáèÁöÑË®ìÁ∑¥ÂíåÂ∞àÊ•≠Áü•Ë≠òÔºåÈôêÂà∂‰∫ÜÊ≠§ÈùàÊ¥ªÁ®ãÂ∫èÁöÑÂèØÂèäÊÄß„ÄÇÊ©üÂô®‰∫∫Ë∂ÖÈü≥Ê≥¢ (RUS) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØË°åÁöÑËß£Ê±∫ÊñπÊ°à‰æÜËß£Ê±∫Ê≠§ÈôêÂà∂Ôºõ
ÂÑòÁÆ°Â¶ÇÊ≠§ÔºåË¶ÅÈÅîÂà∞‰∫∫È°ûÁ≠âÁ¥öÁöÑÁÜüÁ∑¥Â∫¶‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂ≠∏ÁøíÁ§∫ÁØÑ (LfD) ÊñπÊ≥ïÂ∑≤Âú® RUS ‰∏≠ÈÄ≤Ë°åÊé¢Ë®éÔºåÂÆÉÂæûÈõ¢Á∑öÁ§∫ÁØÑÁöÑË≥áÊñôÈõÜÂ≠∏ÁøíÂÖàÈ©óÁ≠ñÁï•Ôºå‰ª•Á∑®Á¢ºÂ∞àÂÆ∂Ë∂ÖÈü≥Ê≥¢Ê™¢Êü•Âì°ÁöÑÂøÉÊô∫Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåËøÑ‰ªäÂ∞öÊú™Êé¢Ë®éÂ∞àÂÆ∂Âú® RUS Ë®ìÁ∑¥ÊúüÈñìÁöÑÁ©çÊ•µÂèÉËàáÔºåÂç≥ÊåáÂ∞é„ÄÇÊåáÂ∞éÂ∑≤Áü•ÂèØ‰ª•ÊèêÈ´ò‰∫∫È°ûË®ìÁ∑¥ÁöÑÊïàÁéáÂíåÁ∏æÊïà„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄã RUS ÊåáÂ∞éÊû∂ÊßãÔºå‰ª•ÊèêÂçáÂÖ∂Á∏æÊïà„ÄÇÊ≠§Êû∂ÊßãÁµêÂêà‰∫Ü DRLÔºàËá™ÊàëÁõ£Áù£ÂØ¶ÂãôÔºâËàáÈÄèÈÅéÊåáÂ∞éÊèê‰æõÁöÑÂ∞àÂÆ∂Á®ÄÁñèÂõûÈ•ã„ÄÇDRL ‰ΩøÁî®Èõ¢Á∑öÁ≠ñÁï•ËªüÊÄßÂãï‰Ωú-Ë©ïË´ñ (SAC) Á∂≤Ë∑ØÔºå‰∏¶Ê†πÊìöÂΩ±ÂÉèÂìÅË≥™Ë©ïÂàÜÁµ¶‰∫àÁçéÂãµ„ÄÇÂ∞àÂÆ∂ÁöÑÊåáÂ∞éË¢´Âª∫Ê®°ÁÇ∫ÈÉ®ÂàÜÂèØËßÄÂØüÈ¶¨ÂèØÂ§´Ê±∫Á≠ñÈÅéÁ®ã (POMDP)ÔºåÂÆÉÊ†πÊìöÂ∞àÂÆ∂ÁöÑ‰øÆÊ≠£‰æÜÊõ¥Êñ∞Á≠ñÁï•ÂèÉÊï∏„ÄÇÂú®Ê®°Êì¨‰∫∫È´îÊ®°Âûã‰∏äÁöÑÈ©óË≠âÁ†îÁ©∂È°ØÁ§∫ÔºåÊåáÂ∞éÂ∞áÂ≠∏ÁøíÁéáÊèêÈ´ò‰∫Ü $25\%$ÔºåÈ´òÂìÅË≥™ÂΩ±ÂÉèÊì∑ÂèñÊï∏ÈáèÊèêÈ´ò‰∫Ü $74.5\%$„ÄÇ

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

ÊëòË¶ÅÔºöÊ≥®ÊÑèÂäõÁº∫Èô∑ÈÅéÂãïÁóá (ADHD) ÊúÉÂú®ÂêÑÁ®ÆÈ†òÂüüÈÄ†ÊàêÈ°ØËëóÁöÑÊêçÂÆ≥„ÄÇÊèêÊó©Ë®∫Êñ∑ ADHD ‰∏¶Êé•ÂèóÊ≤ªÁôÇÂèØ‰ª•Â§ßÂπÖÊîπÂñÑÁîüÊ¥ªÂìÅË≥™ÂíåÂäüËÉΩ„ÄÇÊúÄËøëÔºåÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÂ∑≤Á∂ìÊèêÂçá‰∫Ü ADHD Ë®∫Êñ∑Á®ãÂ∫èÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÊâÄÈúÄÁöÑË®≠ÂÇôÂíåË®ìÁ∑¥ÊúâÁ¥†ÁöÑ‰∫∫Âì°ÊàêÊú¨ÈÄöÂ∏∏ÂæàÈ´ò„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÈ¶ñÊ¨°Â∞áÂü∫ÊñºÂΩ±ÁâáÁöÑÂπÄÁ¥öÂãï‰ΩúËæ®Ë≠òÁ∂≤Ë∑ØÂºïÂÖ• ADHD Ë®∫Êñ∑„ÄÇÊàëÂÄë‰πüË®òÈåÑ‰∫Ü‰∏ÄÂÄãÁúüÊ≠£ÁöÑÂ§öÊ®°Âºè ADHD Ë≥áÊñôÈõÜÔºå‰∏¶ÂæûÂΩ±ÁâáÊ®°Âºè‰∏≠ËêÉÂèñÂá∫‰∏âÂÄãÂãï‰ΩúÈ°ûÂà•‰ª•ÈÄ≤Ë°å ADHD Ë®∫Êñ∑„ÄÇÊï¥ÂÄãÊµÅÁ®ãÁöÑË≥áÊñôÂ∑≤Á∂ìÂõûÂ†±Áµ¶ CNTW-NHS Âü∫ÈáëÊúÉÔºåÂ∞áÁî±ÈÜ´ÁôÇÈ°ßÂïè/Â∞àÊ•≠‰∫∫Â£´ÂØ©Êü•Ôºå‰∏¶Â∞áÈÅ©ÊôÇÂÖ¨Èñã„ÄÇ

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

ÊëòË¶ÅÔºöÁâôÁßëÁñæÁóÖÊòØ‰∏ÄÁ®ÆÊôÆÈÅçÁöÑÊÖ¢ÊÄßÁñæÁóÖÔºåËàáÂ§ßÈáèÁöÑÁ∂ìÊøüË≤†Êìî„ÄÅÂÄã‰∫∫ÁóõËã¶ÂíåÂ¢ûÂä†ÁöÑÂÖ®Ë∫´ÁñæÁóÖÈ¢®Èö™ÊúâÈóú„ÄÇÂÑòÁÆ°ÊôÆÈÅçÂª∫Ë≠∞ÊØèÂ§©Âà∑ÁâôÂÖ©Ê¨°Ôºå‰ΩÜÁî±ÊñºÂÅ•ÂøòÂíåËÑ´Èõ¢Á≠âÂõ†Á¥†ÔºåÂ∞çÂª∫Ë≠∞ÁöÑÂè£ËÖîËá™Êàë‰øùÂÅ•Ë°åÁÇ∫ÁöÑ‰æùÂæûÊÄß‰ªçÁÑ∂‰ΩéÊñºÊúÄ‰Ω≥Ê∞¥Âπ≥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∫Ü OralyticsÔºå‰∏ÄÂÄã mHealth ‰ªãÂÖ•Á≥ªÁµ±ÔºåÊó®Âú®Ë£úÂÖÖËá®Â∫äÈÜ´ÁîüÊèê‰æõÁöÑÈ†êÈò≤‰øùÂÅ•Ôºå‰ª•È†êÈò≤ÊúâÁâôÁßëÁñæÁóÖÈ¢®Èö™ÁöÑÈÇäÁ∑£ÂåñÂÄã‰∫∫„ÄÇOralytics ÁµêÂêà‰∫Ü‰∏ÄÂÄãÂú®Á∑öÂº∑ÂåñÂ≠∏ÁøíÊºîÁÆóÊ≥ïÔºå‰ª•Á¢∫ÂÆöÊèê‰æõ‰ªãÂÖ•ÊèêÁ§∫ÁöÑÊúÄ‰Ω≥ÊôÇÈñìÔºåÈÄô‰∫õÊèêÁ§∫ÈºìÂãµÂè£ËÖîËá™Êàë‰øùÂÅ•Ë°åÁÇ∫„ÄÇÊàëÂÄëÂ∑≤Âú®Ë®ªÂÜäÁöÑËá®Â∫äË©¶È©ó‰∏≠ÈÉ®ÁΩ≤‰∫Ü Oralytics„ÄÇË©≤ÈÉ®ÁΩ≤ÈúÄË¶Å‰ªîÁ¥∞ÁöÑË®≠Ë®à‰æÜÁÆ°ÁêÜÁæéÂúãËá®Â∫äË©¶È©óË®≠ÁΩÆ‰∏≠ÂÖ∑È´îÁöÑÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÔºà1ÔºâÈáçÈªû‰ªãÁ¥π‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÁöÑ RL ÊºîÁÆóÊ≥ïÁöÑÈóúÈçµË®≠Ë®àÊ±∫Á≠ñÔºå‰ª•ÂèäÔºà2ÔºâÈÄ≤Ë°åÈáçÊñ∞ÊäΩÊ®£ÂàÜÊûê‰ª•Ë©ï‰º∞ÊºîÁÆóÊ≥ïË®≠Ë®àÊ±∫Á≠ñ„ÄÇOralytics ÁöÑÁ¨¨‰∫åÈöéÊÆµÔºàÈö®Ê©üÂ∞çÁÖßË©¶È©óÔºâË®àÂäÉÊñº 2025 Âπ¥Êò•Â≠£ÈñãÂßã„ÄÇ

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

ÊëòË¶ÅÔºöÂú®ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÔºåÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÂâ≤ÂØπ‰∫éÂáÜÁ°ÆÁöÑÁñæÁóÖËØäÊñ≠ÂíåÊúâÊïàÊ≤ªÁñóÁ≠ñÁï•ÁöÑÂºÄÂèëËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊó©ÊúüÊ£ÄÊµãÂèØ‰ª•ÊûÅÂ§ßÂú∞Â∏ÆÂä©ÊéßÂà∂ÁñæÁóÖÔºåÂπ∂ÂèØËÉΩÈò≤Ê≠¢ÁñæÁóÖËøõÂ±ï„ÄÇÊú∫Âô®Â≠¶‰π†ÔºåÂ∞§ÂÖ∂ÊòØÊ∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºåÂ∑≤Êàê‰∏∫Ëß£ÂÜ≥ÂàÜÂâ≤ÊåëÊàòÁöÑ‰∏ÄÁßçÊúâÂâçÈÄîÁöÑÊñπÊ≥ï„ÄÇU-Net Á≠â‰º†ÁªüÊñπÊ≥ï‰ΩøÁî®ÁºñÁ†ÅÂùóËøõË°åÂ±ÄÈÉ®Ë°®Á§∫Âª∫Ê®°ÂíåËß£Á†ÅÂùóÊù•Êè≠Á§∫ËØ≠‰πâÂÖ≥Á≥ª„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏Èöæ‰ª•Â§ÑÁêÜÂú®Á∫πÁêÜÂíåÂΩ¢Áä∂‰∏äË°®Áé∞Âá∫ÊòæÁùÄÂèòÂåñÁöÑÂ§öÂ∞∫Â∫¶ÂØπË±°ÔºåÂπ∂‰∏îÂÆÉ‰ª¨ÁªèÂ∏∏Êó†Ê≥ïÊçïËé∑ËæìÂÖ•Êï∞ÊçÆ‰∏≠ÁöÑËøúÁ®ã‰æùËµñÂÖ≥Á≥ª„ÄÇ‰∏ì‰∏∫Â∫èÂàóÂà∞Â∫èÂàóÈ¢ÑÊµãËÄåËÆæËÆ°ÁöÑ Transformer Â∑≤Ë¢´ÊèêÂá∫‰Ωú‰∏∫Êõø‰ª£ÊñπÊ°àÔºåÂà©Áî®ÂÖ®Â±ÄËá™Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁ≤íÂ∫¶ÁªÜËäÇ‰∏çË∂≥ÔºåÂÆÉ‰ª¨ÊúâÊó∂ÂèØËÉΩÁº∫‰πèÁ≤æÁ°ÆÁöÑÂÆö‰Ωç„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü TransDAEÔºö‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÂÆÉÈáçÊñ∞ÊûÑÊÉ≥‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•ÂåÖÂê´Êï¥‰∏™ÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑÁ©∫Èó¥ÂíåÈÄöÈÅìÂÖ≥ËÅîÔºåÂêåÊó∂‰øùÊåÅËÆ°ÁÆóÊïàÁéá„ÄÇÊ≠§Â§ñÔºåTransDAE ÈÄöËøáÂ∞∫Â∫¶Èó¥‰∫§‰∫íÊ®°ÂùóÂ¢ûÂº∫‰∫ÜË∑≥Ë∑ÉËøûÊé•Ë∑ØÂæÑÔºå‰øÉËøõ‰∫ÜÁâπÂæÅÈáçÁî®Âπ∂ÊèêÈ´ò‰∫ÜÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂç≥‰Ωø‰∏ç‰æùËµñÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåTransDAE Âú® Synaps Â§öÂô®ÂÆòÊï∞ÊçÆÈõÜ‰∏ä‰πü‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain L√©guillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte S√©roussi, Julien Grosjean, Romain Lelong, Catherine Letord, St√©fan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÁî®Ëó•ÂØ©Êü•ÊòØÁî±Ëó•Â∏´Âü∑Ë°åÁöÑ‰∏ÄÁ®ÆÁµêÊßãÂåñÊÇ£ËÄÖË®™Ë´áÔºåÁõÆÁöÑÂú®ÊñºÂÑ™ÂåñËó•Áâ©Ê≤ªÁôÇ„ÄÇÂú®ÂØ¶Âãô‰∏äÔºåÁî®Ëó•ÂØ©Êü•ÊòØ‰∏ÄÈ†ÖÂÜóÈï∑‰∏îË™çÁü•ÈúÄÊ±ÇÈ´òÁöÑ‰ªªÂãôÔºåÈúÄË¶ÅÂÖ∑ÂÇôÁâπÂÆöÁü•Ë≠ò„ÄÇÈõñÁÑ∂Â∑≤ÊèêÂá∫Ëá®Â∫äÂØ¶ÂãôÊåáÂºïÔºå‰ΩÜÂÖ∂ÊáâÁî®ÂæàÁπÅÁë£„ÄÇÊñπÊ≥ïÔºöÊàëÂÄëÊ†πÊìö STOPP/START v2 ÊåáÂºïÁöÑÂØ¶‰ΩúÔºå‰∏¶‰ΩøÁî®Ë°®Ê†º„ÄÅÂúñË°®ÂíåËä±ÂΩ¢Á¨¶ËôüË¶ñË¶∫ÂåñÂëàÁèæÂΩôÊï¥ÁöÑËó•Áâ©Áü•Ë≠òÔºåË®≠Ë®à‰∫Ü‰∏ÄÂ•óÁî®Ëó•ÂØ©Êü•ÁöÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± ABiMed„ÄÇÊàëÂÄëÂú®Èö®Ê©üÊ®°Êì¨Ë©¶È©ó‰∏≠ÔºåËÆì 39 ‰ΩçÁ§æÂçÄËó•Â∏´Ë©ï‰º∞ ABiMedÔºåÊØè‰ΩçËó•Â∏´ÈáùÂ∞çÂÖ©‰ΩçËôõÊßãÊÇ£ËÄÖÂü∑Ë°åÁî®Ëó•ÂØ©Êü•ÔºåÂÖ©Ê¨°Ê≤íÊúâ‰ΩøÁî® ABiMedÔºåÂÖ©Ê¨°‰ΩøÁî® ABiMed„ÄÇÊàëÂÄëË®òÈåÑ‰∫ÜËó•Â∏´Ë≠òÂà•Âá∫ÁöÑÂïèÈ°å„ÄÅÂª∫Ë≠∞ÁöÑ‰ªãÂÖ•Êé™ÊñΩ„ÄÅÂèçÊáâÊôÇÈñì„ÄÅÊÑüÁü•ÂèØÁî®ÊÄßÂíåË©ïË´ñ„ÄÇÂ∞áËó•Â∏´ÁöÑÁî®Ëó•ÂØ©Êü•ËàáÂ∞àÂÆ∂Ë®≠Ë®àÁöÑÈáëÊ®ôÊ∫ñÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúÔºö‰ΩøÁî® ABiMed ÂæåÔºåËó•Â∏´Âú®Áî®Ëó•ÂØ©Êü•ÊúüÈñìÁôºÁèæ‰∫ÜÂ§ö 1.6 ÂÄçÁõ∏ÈóúÁöÑËó•Áâ©Áõ∏ÈóúÂïèÈ°åÔºàp=1.1e-12ÔºâÔºå‰∏¶ÊèêÂá∫Êõ¥Â•ΩÁöÑ‰ªãÂÖ•Êé™ÊñΩÔºàp=9.8e-9ÔºâÔºåËÄåÁÑ°ÈúÄËä±Ë≤ªÊõ¥Â§öÊôÇÈñìÔºàp=0.56Ôºâ„ÄÇÁ≥ªÁµ±ÂèØÁî®ÊÄßË©ïÂàÜÁÇ∫ 82.7ÔºåË¢´Ë©ïÁÇ∫„ÄåÂÑ™ËâØ„Äç„ÄÇÂú®‰ªñÂÄëÁöÑË©ïË´ñ‰∏≠ÔºåËó•Â∏´ËÆöË≥û ABiMed ÁöÑË¶ñË¶∫ÂåñÈù¢ÂêëÔºå‰ª•ÂèäÂÆÉÊØîËºÉÁõÆÂâçÊ≤ªÁôÇËàáÂª∫Ë≠∞Ê≤ªÁôÇÁöÑËÉΩÂäõ„ÄÇÂ§öÂõ†Á¥†ÂàÜÊûêÈ°ØÁ§∫ÔºåABiMed Êèê‰æõÁöÑÊîØÊè¥Âú®Ëó•Â∏´ÁöÑÂπ¥ÈΩ°ÊàñÊÄßÂà•ÊñπÈù¢Ê≤íÊúâÂ∑ÆÁï∞ÔºåÂ∞±Ë≠òÂà•Âá∫ÁöÑÂïèÈ°åÁôæÂàÜÊØîÊàñÂª∫Ë≠∞‰ªãÂÖ•Êé™ÊñΩÁöÑÂìÅË≥™ËÄåË®Ä„ÄÇÁµêË´ñÔºö‰ΩøÁî®Êô∫ÊÖß‰∏îË¶ñË¶∫ÂåñÁöÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÔºåÂèØ‰ª•ÂçîÂä©Ëó•Â∏´Âü∑Ë°åÁî®Ëó•ÂØ©Êü•„ÄÇÊàëÂÄëÁöÑËßÄÈªû‰∏ªË¶ÅÊòØÈ©óË≠âÁ≥ªÁµ±Âú®Ëá®Â∫äÊ¢ù‰ª∂‰∏ãÁöÑÊïàÂ∫¶„ÄÇ</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊàêÂäüÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÈ†êË®ìÁ∑¥ÈöéÊÆµ‰∏≠Â≠∏ÁøíÂà∞ÁöÑÊµ∑ÈáèÈ†êË®ìÁ∑¥Êï∏Êìö„ÄÇÈ†êË®ìÁ∑¥ÈÅéÁ®ãÂíåË®ìÁ∑¥Êï∏ÊìöÁöÑ‰∏çÈÄèÊòéÊÄßÂ∞éËá¥Ë®±Â§öÂü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÁµêÊûúËÆäÂæó‰∏çÂèØÈù†„ÄÇÂ¶ÇÊûú‰ªª‰ΩïÊ®°ÂûãÂ∑≤Âú®Âü∫Ê∫ñÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÄ≤Ë°åË®ìÁ∑¥ÔºåÂâáÂèØËÉΩÊúÉÂö¥ÈáçÈòªÁ§ôË©≤È†òÂüüÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËá™ÂãïÂåñ‰∏îÊúâÊïàÂú∞Ê∏¨Ë©¶Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåË®±Â§ö‰∏ªÊµÅÂü∫Ê∫ñÊé°Áî®Â§öÈÅ∏È°åÊ†ºÂºè„ÄÇÁî±ÊñºÂ§öÈÅ∏È°åÈÅ∏È†ÖÂÖßÂÆπÁöÑ‰∫íÊèõ‰∏çÂΩ±ÈüøÂïèÈ°åÊú¨Ë∫´ÁöÑÂê´Áæ©ÔºåÂõ†Ê≠§ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÊ≠§Â±¨ÊÄßÁöÑÁ∞°ÂñÆ‰∏îÊúâÊïàÁöÑÊï∞ÊçÆÊ¥©ÊºèÊ™¢Ê∏¨ÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞áÊï∏Êìö‰∏≠ÈÅ∏È†ÖÁöÑÂÖßÂÆπÈö®Ê©üÊéíÂàó‰ª•ÁîüÊàêÂ∞çÊáâÁöÑÊ¥æÁîüÊï∏ÊìöÈõÜÔºåÁÑ∂ÂæåÊ†πÊìöÊ®°ÂûãÂú®Ê¥æÁîüÊï∏ÊìöÈõÜ‰∏äÁöÑÂ∞çÊï∏Ê¶ÇÁéáÂàÜ‰ΩàÊ™¢Ê∏¨Êï∏ÊìöÊ¥©Êºè„ÄÇÂ¶ÇÊûúÂ∞çÊï∏Ê¶ÇÁéáÈõÜ‰∏≠Â≠òÂú®ÊúÄÂ§ßÂÄºÂíåÁï∞Â∏∏ÂÄºÔºåÂâáË°®Á§∫Êï∏ÊìöÂ∑≤Ê¥©Êºè„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïËÉΩÂ§†Âú®‰∏çË®™ÂïèÊ®°ÂûãË®ìÁ∑¥Êï∏ÊìöÊàñÊ¨äÈáçÁöÑÈªëÁõíÊ¢ù‰ª∂‰∏ãÂ∑•‰ΩúÔºåÊúâÊïàÂú∞Ë≠òÂà•Ê®°ÂûãÈ†êË®ìÁ∑¥Êï∏Êìö‰∏≠Âü∫Ê∫ñÊ∏¨Ë©¶ÈõÜÁöÑÊï∏ÊìöÊ¥©ÊºèÔºåÂåÖÊã¨ÈÅ∏È†ÖÂèØËÉΩÂ∑≤ÊúâÊÑèÊàñÁÑ°ÊÑèÂú∞Ë¢´Êâì‰∫ÇÁöÑÊ≠£Â∏∏Â†¥ÊôØÂíåË§áÈõúÂ†¥ÊôØ„ÄÇÈÄöÈÅéÂü∫ÊñºÂÖ©ÂÄã LLM ÂíåÂü∫Ê∫ñË®≠Ë®àÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË©ï‰º∞‰∫Ü 31 ÂÄã‰∏ªÊµÅÈñãÊ∫ê LLM Âú®ÂõõÂÄãÂü∫Ê∫ñÊï∏ÊìöÈõÜ‰∏äÁöÑÊï∏ÊìöÊ¥©ÊºèÁ®ãÂ∫¶Ôºå‰∏¶Â∞çÊØèÂÄãÂü∫Ê∫ñÁöÑÊ¥©Êºè LLM ÈÄ≤Ë°å‰∫ÜÊéíÂêçÔºåÊàëÂÄëÁôºÁèæ Qwen ÂÆ∂ÊóèÁöÑ LLM ÂÖ∑ÊúâÊúÄÈ´òÁöÑÊï∏ÊìöÊ¥©ÊºèÁ®ãÂ∫¶„ÄÇ

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

ÊëòË¶ÅÔºöÊé®Â∞éÊóãËΩâÊ©üÂô®ÁöÑÂÅ•Â∫∑ÊåáÊ®ôÂ∞çÊñºÂÖ∂Á∂≠Ë≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÄãÈÅéÁ®ãÂ∞çÊôÆÈÅçÊé°Áî®ÁöÑÊô∫ËÉΩÊñπÊ≥ï‰æÜË™™ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂèØËÉΩÊúÉÊé°Áî®Êï¥ÂÄãË≥áÊñôÂàÜ‰ΩàÔºå‰∏çÂÉÖÊúÉÂºïÂÖ•ÈõúË®äÂπ≤ÊìæÔºåËÄå‰∏îÁº∫‰πèÂèØËß£ÈáãÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÊì¥Êï£ÁöÑÂº±Áõ£Áù£ÂºèÊñπÊ≥ïÔºåÁî®ÊñºÊé®Â∞éÊóãËΩâÊ©üÂô®ÁöÑÂÅ•Â∫∑ÊåáÊ®ôÔºåÂØ¶ÁèæÊó©ÊúüÊïÖÈöúÊ™¢Ê∏¨ÂíåÁãÄÊÖãÊºîËÆäÁöÑÊåÅÁ∫åÁõ£Êéß„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰æùË≥¥Êñº‰ΩøÁî®ÂÅ•Â∫∑Ê®£Êú¨Âíå‰∏Ä‰∫õÁï∞Â∏∏ÂÄºË®ìÁ∑¥ÁöÑÁÑ°ÂàÜÈ°ûÂô®Êì¥Êï£Ê®°Âûã„ÄÇÈÄôÂÄãÊ®°ÂûãÊúÉÁî¢ÁîüÂÅ•Â∫∑Ê®£Êú¨„ÄÇ‰∏¶‰∏îÈÄöÈÅéÊØîËºÉÂ∞ÅÂ•óË≠ú‰∏≠ÂéüÂßãÊ®£Êú¨ÂíåÁîüÊàêÊ®£Êú¨‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºåÊàëÂÄëÊßãÂª∫‰∫Ü‰∏ÄÂÄãÁï∞Â∏∏ÂúñÔºåÂèØ‰ª•Ê∏ÖÊ•öÂú∞Ë≠òÂà•ÊïÖÈöú„ÄÇÁÑ∂ÂæåÊé®Â∞éÂá∫ÂÅ•Â∫∑ÊåáÊ®ôÔºåÂèØ‰ª•Ëß£ÈáãÊïÖÈöúÈ°ûÂûã‰∏¶Ê∏õËºïÈõúË®äÂπ≤Êìæ„ÄÇÂ∞çÂÖ©ÂÄãÊ°à‰æãÁöÑÊØîËºÉÁ†îÁ©∂Ë°®ÊòéÔºåËàáÂü∫Ê∫ñÊ®°ÂûãÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂçìË∂äÁöÑÂÅ•Â∫∑Áõ£ÊéßÊúâÊïàÊÄßÂíåÈ≠ØÊ£íÊÄß„ÄÇ

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

ÊëòË¶ÅÔºöÈõªËÖ¶ËºîÂä©ÁôåÁóáÂ≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨Âú®ÁóÖÊÇ£ÁöÑÂèäÊôÇÊ≤ªÁôÇ‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÈÄôÊòØ‰∏ÄÂÄãÂõ∞Èõ£ÁöÑÂº±Áõ£Áù£Â∫èÊï∏ÂõûÊ≠∏‰ªªÂãôÔºåËàáÂ§öÈáçËá®Â∫äÂõ†Á¥†ÊúâÈóúÔºå‰æãÂ¶ÇÁóÖÁêÜÂúñÂÉè„ÄÅÂü∫Âõ†ÁµÑÊï∏ÊìöÁ≠â„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË®ìÁ∑¥ÊñπÊ≥ïÔºåÂ§öÊ®°ÊÖãÁâ©‰ª∂Â±§Á¥öÂ∞çÊØîÂ≠∏ÁøíÔºåÁî®ÊñºÁôåÁóáÂ≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊ†πÊìöË®ìÁ∑¥Ê®£Êú¨ÈõÜ‰∏≠Ê®£Êú¨‰πãÈñìÁöÑÂ≠òÊ¥ªÈ¢®Èö™Èóú‰øÇÂª∫Á´ãÂ∞çÊØîÂ≠∏ÁøíÂ∞ç„ÄÇÊé•ËëóÔºåÊàëÂÄëÂºïÂÖ•Áâ©‰ª∂Â±§Á¥öÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ï‰æÜË®ìÁ∑¥Â≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨Âô®„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Â∞áÂÖ∂Âª∂‰º∏Ëá≥Â§öÊ®°ÊÖãÂ†¥ÊôØÔºåÈÄèÈÅéÊáâÁî®Ë∑®Ê®°ÊÖãÂ∞çÊØî„ÄÇËÄÉÈáèÂà∞ÁóÖÁêÜÂúñÂÉèÂíåÂü∫Âõ†È´îÊï∏ÊìöÁöÑÁï∞Ë≥™ÊÄßÔºåÊàëÂÄëÂàÜÂà•Êé°Áî®Âü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂíåËá™Ê®ôÊ∫ñÂåñÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂª∫ÊßãÂ§öÊ®°ÊÖãÂ≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨Âô®„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÊâÄË®ìÁ∑¥ÁöÑÂ≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨Âô®Âú®ÂÖ©ÂÄãÂÖ¨ÈñãÁöÑÂ§öÊ®°ÊÖãÁôåÁóáË≥áÊñôÈõÜ‰∏äÔºåÂú®Â≠òÊ¥ªÈ¢®Èö™È†êÊ∏¨ÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇ

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana Hern√°ndez, Juan C. Caicedo, Gloria M. D√≠az

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

ÊëòË¶ÅÔºöÂ∞çÊØîÂ¢ûÂº∑Ê®°ÂºèÂàÜÊûêÂú®‰π≥ÊàøÁ£ÅÂÖ±ÊåØÂΩ±ÂÉè (MRI) ‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂèØÁî®ÊñºÂçÄÂàÜËâØÊÄßËÖ´Áò§ÂíåÂèØËÉΩÊòØÊÉ°ÊÄßËÖ´Áò§„ÄÇ
ÁÑ∂ËÄåÔºåÂ∞çÊØîÂ¢ûÂº∑ÂΩ±ÂÉèÁöÑÊì∑ÂèñÈùûÂ∏∏ËÄóÊôÇ‰∏îÊòÇË≤¥„ÄÇ‰ΩúÁÇ∫Áâ©ÁêÜÊì∑ÂèñÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÁÆ°ÈÅìÔºåÁî®ÊñºÂæûÊó©ÊúüÂ∞çÊáâÁâ©ÁîüÊàêÊ∫ñÁ¢∫ÁöÑÈï∑ÊúüÔºàÊôöÊúüÔºâÂ∞çÊØîÂ¢ûÂº∑‰π≥Êàø MRI„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≠ñÁï•ËëóÈáçÊñºÂú®Â¢ûÂº∑ÂçÄÂüü‰∏≠‰øùÁïôÂ∞çÊØîÂäëÊ®°ÂºèÔºåÂêåÊôÇÂú®Êï¥ÂÄãÂêàÊàêÂΩ±ÂÉè‰∏≠Á∂≠ÊåÅË¶ñË¶∫Â±¨ÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåÂà©Áî®Â∞çÊØîÂäë (CA) Âú®ÁµÑÁπî‰∏≠ÁöÑÁîüÁâ©Ë°åÁÇ∫ÔºàÁî±ÊôÇÈñìÂº∑Â∫¶ (TI) Â¢ûÂº∑Êõ≤Á∑öÁµ¶Âá∫ÔºâÔºå‰ª•ÊúÄ‰Ω≥ÂåñÂü∫ÊñºÂÉèÁ¥†Ê≥®ÊÑèÂäõÁöÑÁîüÊàêÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåËàáÂÇ≥Áµ±ÁöÑÊ≠£Ë¶èÂåñÂíåÊ®ôÊ∫ñÂåñÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊ≠£Ë¶èÂåñÁ≠ñÁï•ÔºåÂèØÂú®Â§öÂÄãÊôÇÈñìÊà≥ÁöÑÂΩ±ÂÉèÂ∫èÂàó‰∏≠Á∂≠ÊåÅÂ∞çÊØîÂ¢ûÂº∑Ê®°Âºè„ÄÇÈÄôÁ¢∫‰øù‰∫ÜÂΩ±ÂÉèÂâçËôïÁêÜÂæå CA Ê®°ÂºèÁöÑÊôÆÈÅçÊÄßÔºåÈÄôËàáÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêå„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÂÆ¢ËßÄË©ï‰º∞ÂêàÊàêÂΩ±ÂÉèÁöÑËá®Â∫äÂìÅË≥™ÔºåÈÇÑÂºïÂÖ•‰∫ÜÂÖ©ÂÄãÊåáÊ®ô‰æÜÊ∏¨ÈáèÊì∑ÂèñÂíåÂêàÊàêÂΩ±ÂÉèÁöÑÂ¢ûÂº∑ÂçÄÂüüÁöÑ TI Êõ≤Á∑ö‰πãÈñìÁöÑÂ∑ÆÁï∞„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÁ≠ñÁï•Áî¢ÁîüÁöÑÂΩ±ÂÉèÂú®Â∞çÊØîÂ¢ûÂº∑ÂçÄÂüü‰∏≠ÁöÑË®∫Êñ∑ÂìÅË≥™ÊòéÈ°ØÂÑ™ÊñºÂÖ∂‰ªñÂΩ±ÂÉèÔºåÂêåÊôÇÁ∂≠ÊåÅ‰∫ÜÊï¥ÂÄãÂΩ±ÂÉèÁöÑÁ©∫ÈñìÁâπÂæµ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÂú®Ëá®Â∫äÂ†¥ÊôØ‰∏≠ÔºåÈÄöÈÅéÊ∑±Â∫¶Â≠∏ÁøíÁîüÊàêÁöÑÂêàÊàêÊôöÊúüÂ¢ûÂº∑ÂΩ±ÂÉèÂÖ∑ÊúâÊΩõÂú®Áî®ÈÄî„ÄÇ

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea Stor√•s, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, P√•l Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÈÄ≤ Kvasir-VQAÔºå‰∏ÄÂÄãÁî± HyperKvasir Âíå Kvasir-Instrument Ë≥áÊñôÈõÜË°çÁîüÁöÑÂª∂‰º∏Ë≥áÊñôÈõÜÔºå‰∏¶Âä†ÂÖ•ÂïèÈ°åËàáËß£Á≠îË®ªËß£Ôºå‰ª•‰øÉÈÄ≤Âú®ËÉÉËÖ∏ (GI) Ë®∫Êñ∑‰∏≠ÁöÑÈÄ≤ÈöéÊ©üÂô®Â≠∏Áøí‰ªªÂãô„ÄÇÊ≠§Ë≥áÊñôÈõÜÂåÖÂê´ 6,500 ÂÄãË®ªËß£ÂΩ±ÂÉèÔºåÊ∂µËìãÂêÑÁ®Æ GI ÈÅìÁãÄÊ≥ÅÂíåÊâãË°ìÂô®Ê¢∞Ôºå‰∏¶‰∏îÊîØÊè¥ÂåÖÊã¨ÊòØÈùûÈ°å„ÄÅÈÅ∏ÊìáÈ°å„ÄÅ‰ΩçÁΩÆÂíåÊï∏Â≠óË®àÊï∏Á≠âÂ§öÁ®ÆÈ°ûÂûãÁöÑÂïèÈ°å„ÄÇÊ≠§Ë≥áÊñôÈõÜÈÅ©Áî®ÊñºÂΩ±ÂÉèÊ®ôÈ°å„ÄÅË¶ñË¶∫ÂïèÁ≠î (VQA)„ÄÅÂêàÊàêÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÊñáÂ≠óÁîüÊàê„ÄÅÁâ©‰ª∂ÂÅµÊ∏¨ÂíåÂàÜÈ°ûÁ≠âÊáâÁî®Á®ãÂºè„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÊ≠§Ë≥áÊñôÈõÜÂú®Ë®ìÁ∑¥‰∏âÂÄãÈÅ∏ÂÆö‰ªªÂãôÁöÑÊ®°Âûã‰∏≠ÂÖ∑ÊúâÊàêÊïàÔºåÂ±ïÁ§∫‰∫ÜÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÂíåË®∫Êñ∑‰∏≠ÈáçË¶ÅÁöÑÊáâÁî®„ÄÇÊàëÂÄë‰πüÁÇ∫ÊØèÂÄã‰ªªÂãôÊèê‰æõË©ï‰º∞ÊåáÊ®ôÔºåÁ™ÅÈ°ØÊàëÂÄëË≥áÊñôÈõÜÁöÑÂèØÁî®ÊÄßÂíåÂ§öÂäüËÉΩÊÄß„ÄÇÊ≠§Ë≥áÊñôÈõÜÂíåÊîØÊè¥Â∑•‰ª∂ÂèØÊñº https://datasets.simula.no/kvasir-vqa ÂèñÂæó„ÄÇ

##### **EEG-Language Modeling for Pathology Detection**
2409.07480v1 by Sam Gijsen, Kerstin Ritter

Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÂª∫Ê®°ÊòØ‰∏ÄÈ°πÊúÄËøëÁöÑÁ™ÅÁ†¥ÔºåÂÆÉÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÊ≠•Êù•È¢ÑËÆ≠ÁªÉÊúâËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã„ÄÇ‰∫ãÂÆûËØÅÊòéÔºåÂú®È¢ÑËÆ≠ÁªÉÊúüÈó¥Êï¥ÂêàËá™ÁÑ∂ËØ≠Ë®ÄÂèØ‰ª•ÊòæËëóÊîπÂñÑÂ≠¶‰π†Âà∞ÁöÑË°®ÂæÅÔºåÁâπÂà´ÊòØÂú®ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÂª∫Ê®°Âú®ÂäüËÉΩÊÄßËÑëÊï∞ÊçÆÈ¢ÜÂüü‰∏≠ÁöÑÂäüÊïàÔºåÁâπÂà´ÊòØÂØπ‰∫éÊé®ËøõÁóÖÁêÜÊ£ÄÊµãÔºå‰ªçÁÑ∂Êú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÊú¨Á†îÁ©∂ÂºÄÂàõ‰∫ÜÂú®‰∏¥Â∫äÊä•ÂëäÂíå 15000 ‰∏™ËÑëÁîµÂõæ‰∏äËÆ≠ÁªÉÁöÑËÑëÁîµÂõæËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨Â∞ÜÂ§öÊ®°ÊÄÅÂØπÈΩêÁöÑÊñπÊ≥ïÊâ©Â±ïÂà∞Ëøô‰∏™Êñ∞È¢ÜÂüüÔºåÂπ∂Á†îÁ©∂Êä•Âëä‰∏≠ÁöÑÂì™‰∫õÊñáÊú¨‰ø°ÊÅØÂØπ‰∫éËÆ≠ÁªÉËÑëÁîµÂõæËØ≠Ë®ÄÊ®°ÂûãÊòØÊúâÁî®ÁöÑ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÈÄöËøáÊé•Ëß¶ÂêÑÁßçÊä•ÂëäÁâáÊÆµÔºàÂåÖÊã¨ÊÇ£ËÄÖÁöÑÁóÖÂè≤„ÄÅËÑëÁîµÂõæÊèèËø∞ÂíåÂåªÁîüÁöÑËß£ÈáäÔºâÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞Êõ¥‰∏∞ÂØåÁöÑË°®ÂæÅ„ÄÇ‰∏éÊé•Ëß¶Âà∞ËæÉÁ™ÑÁöÑ‰∏¥Â∫äÊñáÊú¨‰ø°ÊÅØÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåÊàë‰ª¨ÂèëÁé∞Ê≠§Á±ªÊ®°ÂûãÂèØ‰ª•Ê†πÊçÆ‰∏¥Â∫äÊä•ÂëäÔºàÂèç‰πã‰∫¶ÁÑ∂ÔºâÊ£ÄÁ¥¢ËÑëÁîµÂõæÔºåÂÖ∂ÂáÜÁ°ÆÊÄßÂ§ßÂ§ßÊèêÈ´ò„ÄÇÁÑ∂ËÄåÔºåÂè™ÊúâÂú®‰ΩøÁî®ÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ïÊó∂Êâç‰ºöËßÇÂØüÂà∞Ëøô‰∏ÄÁÇπ„ÄÇÁâπÂà´ÊòØÂú®Ê≥®ÈáäËæÉÂ∞ëÁöÑÊñπÊ°à‰∏≠ÔºåÊàë‰ª¨ËßÇÂØüÂà∞ËÑëÁîµÂõæËØ≠Ë®ÄÊ®°ÂûãÁöÑË°®ÂæÅÂèØ‰ª•ÊòæËëóÊîπÂñÑÁóÖÁêÜÊ£ÄÊµãÔºå‰∏é‰ªÖËÑëÁîµÂõæÊ®°ÂûãÁõ∏ÊØîÔºåÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÂíåÁ∫øÊÄßÊé¢ÈíàÈÉΩËØÅÊòé‰∫ÜËøô‰∏ÄÁÇπ„ÄÇÊÄª‰πãÔºåËøô‰∫õÁªìÊûúÁ™ÅÂá∫‰∫ÜÂ∞ÜËÑëÊ¥ªÂä®Êï∞ÊçÆ‰∏é‰∏¥Â∫äÊñáÊú¨Áõ∏ÁªìÂêàÁöÑÊΩúÂäõÔºåË°®ÊòéËÑëÁîµÂõæËØ≠Ë®ÄÊ®°Âûã‰ª£Ë°®‰∫Ü‰∏¥Â∫äÂ∫îÁî®ÁöÑÈáçÂ§ßËøõÂ±ï„ÄÇ

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

ÊëòË¶ÅÔºöÈö±ÂºèÁ•ûÁ∂ìË°®Âæµ (INR) ËøëÊúüÁî±ÊñºÂÖ∂ÁÑ°ÈúÄÂ§ßÈáèË®ìÁ∑¥Ë≥áÊñôÈõÜÂ∞±ËÉΩÂ≠∏ÁøíË®äËôüÁöÑÈÄ£Á∫åË°®ÂæµÁöÑËÉΩÂäõÔºåËÄåÊé®Âãï‰∫ÜÊ∑±Â∫¶Â≠∏ÁøíÈ†òÂüüÁöÑÈÄ≤Â±ï„ÄÇÂÑòÁÆ° INR ÊñπÊ≥ïÂ∑≤Ë¢´Á†îÁ©∂Áî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèË∂ÖËß£ÊûêÂ∫¶Ôºå‰ΩÜÂÖ∂Â∞çÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Â±ÄÈÉ®ÂÖàÈ©óÁöÑÈÅ©ÊáâÊÄßÂ∞öÊú™Ë¢´Âª£Ê≥õÊé¢Ë®é„ÄÇÈÜ´Â≠∏ÂΩ±ÂÉèÂåÖÂê´Ë±êÂØåÁöÑËß£ÂâñÂ≠∏ÂçÄÂàÜÔºåÈÄô‰∫õÂçÄÂàÜÂèØ‰ª•Êèê‰æõÊúâÂÉπÂÄºÁöÑÂ±ÄÈÉ®ÂÖàÈ©óË≥áË®äÔºå‰ª•Â¢ûÂº∑ INR ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÁ®±ÁÇ∫Ë™ûÁæ©Ê¢ù‰ª∂ INR (SeCo-INR)ÔºåÂÆÉ‰ΩøÁî®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÂ±ÄÈÉ®ÂÖàÈ©ó‰æÜË™øÊï¥ INRÔºåÂØ¶ÁèæÊ∫ñÁ¢∫ÁöÑÊ®°ÂûãÊì¨ÂêàÂíåÊèíÂÄºËÉΩÂäõÔºå‰ª•ÂØ¶ÁèæË∂ÖËß£ÊûêÂ∫¶„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂ≠∏ÁøíÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑË™ûÊÑèÂàÜÂâ≤ÁâπÂæµÁöÑÈÄ£Á∫åË°®ÂæµÔºå‰∏¶Âà©Áî®ÂÆÉÁÇ∫ÂΩ±ÂÉèÁöÑÊØèÂÄãË™ûÊÑèÂçÄÂüüÊé®Â∞éÊúÄ‰Ω≥ INR„ÄÇÊàëÂÄë‰ΩøÁî®Â§öÁ®ÆÈÜ´Â≠∏ÂΩ±ÂÉèÊñπÂºèÊ∏¨Ë©¶ÊàëÂÄëÁöÑÊû∂ÊßãÔºå‰∏¶ËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÈáèÂåñË©ïÂàÜÂíåÊõ¥ÈÄºÁúüÁöÑË∂ÖËß£ÊûêÂ∫¶Ëº∏Âá∫„ÄÇ

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

ÊëòË¶ÅÔºö<paragraph>Âú®ÁöÆËÇ§ÁóÖÂ≠¶È¢ÜÂüüÔºå‰∫∫Â∑•ËØäÊñ≠ÁöÆËÇ§Áä∂ÂÜµÁöÑÂ§çÊùÇÊÄßÈúÄË¶ÅÁöÆËÇ§ÁßëÂåªÂ∏àÁöÑ‰∏ì‰∏öÁü•ËØÜ„ÄÇ‰ªéÁôåÁóáÂà∞ÁÇéÁóáÊÄßÁñæÁóÖÔºåÂØπÂêÑÁßçÁöÆËÇ§ÁñæÁóÖÁöÑÂáÜÁ°ÆËØÜÂà´Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÁöÆËÇ§ÁóÖÂ≠¶‰∫∫Â∑•Êô∫ËÉΩ (AI) Ê®°ÂûãÈù¢‰∏¥ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂáÜÁ°ÆËØäÊñ≠‰∏çÂêåËÇ§Ëâ≤ÁöÑÁñæÁóÖÊó∂ÔºåÂú®ËæÉÊ∑±ÁöÑËÇ§Ëâ≤‰∏äÂ≠òÂú®ÊòéÊòæÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÂÖ¨ÂºÄÂèØÁî®ÁöÑÊó†ÂÅèÊï∞ÊçÆÈõÜÁöÑÁ®ÄÁº∫ÊÄßÈòªÁ¢ç‰∫ÜÂåÖÂÆπÊÄß AI ËØäÊñ≠Â∑•ÂÖ∑ÁöÑÂºÄÂèë„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπÂáÜÁ°ÆÈ¢ÑÊµã‰∏çÂêåËÇ§Ëâ≤ÁöÆËÇ§Áä∂ÂÜµÁöÑÊåëÊàòÔºåÊàë‰ª¨ÈááÁî®‰∫Ü‰∏ÄÁßçËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî®‰∫ÜÊù•Ëá™ÂêÑÁßçÂõæÂÉèÂüüÁöÑ‰∏∞ÂØåÂèØËΩ¨ÁßªÁü•ËØÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈõÜÊàê‰∫ÜÊù•Ëá™ÂπøÊ≥õÊù•Ê∫êÁöÑÂ§ö‰∏™È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂåÖÊã¨‰∏ÄËà¨ÂíåÁâπÂÆöÁöÑÂåªÂ≠¶ÂõæÂÉèÔºå‰ª•ÊèêÈ´òÁöÆËÇ§Áä∂ÂÜµÈ¢ÑÊµãÁöÑÁ®≥ÂÅ•ÊÄßÂíåÂåÖÂÆπÊÄß„ÄÇÊàë‰ª¨‰ΩøÁî® Diverse Dermatology Images (DDI) Êï∞ÊçÆÈõÜ‰∏•Ê†ºËØÑ‰º∞‰∫ÜËøô‰∫õÊ®°ÂûãÁöÑÊúâÊïàÊÄßÔºåËØ•Êï∞ÊçÆÈõÜÁã¨ÁâπÂú∞ÂåÖÂê´‰∫Ü‰ª£Ë°®ÊÄß‰∏çË∂≥ÂíåÂ∏∏ËßÅÁöÑËÇ§Ëâ≤Ôºå‰ΩøÂÖ∂Êàê‰∏∫ËØÑ‰º∞Êàë‰ª¨ÊñπÊ≥ïÁöÑÁêÜÊÉ≥Âü∫ÂáÜ„ÄÇÂú®ÊâÄÊúâÊñπÊ≥ï‰∏≠ÔºåMed-ViT Áî±‰∫éÂÖ∂‰ªéÂêÑÁßçÂõæÂÉèÊù•Ê∫ê‰∏≠Â≠¶Âà∞ÁöÑÁªºÂêàÁâπÂæÅË°®Á§∫ËÄåÊàê‰∏∫Ë°®Áé∞ÊúÄÂ•ΩÁöÑÊñπÊ≥ï„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÊÄßËÉΩÔºåÊàë‰ª¨‰ΩøÁî® HAM10000 Á≠âÂÖ∂‰ªñÁöÆËÇ§ÂõæÂÉèÊï∞ÊçÆÈõÜËøõË°å‰∫ÜÂüüÈÄÇÂ∫î„ÄÇËøôÁßçÈÄÇÂ∫îÊòæÁùÄÊèêÈ´ò‰∫ÜÊâÄÊúâÊ®°ÂûãÁöÑÊ®°ÂûãÊÄßËÉΩ„ÄÇ</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÔºåÂç≥‰ΩøÁ∂ìÈÅéÂæÆË™øÁöÑÊ®°Âûã‰πüÂÆπÊòìÁî¢ÁîüÂπªË¶∫„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÊõ¥Â§öÂèØÈù†ÁöÑÊ®°Âûã‰æÜÁ¥çÂÖ•Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊµÅÁ®ã 4StepFocusÔºåÁâπÂà•ÊòØÈ†êËôïÁêÜÊ≠•È©üÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM ÁöÑÁ≠îÊ°à„ÄÇÈÄôÊòØÈÄèÈÅéÊèê‰æõÂèóÂºïÂ∞éÁöÑÂ§ñÈÉ®Áü•Ë≠òÂ≠òÂèñÔºåÂà©Áî®Ê®°ÂûãËá™Ë°åÊì∑ÂèñÈóúËÅØÊÄßËÑàÁµ°ÂíåÈÄ≤Ë°åÂü∫Êú¨Êé®ÁêÜÁöÑËÉΩÂäõ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÂú®ÂçäÁµêÊßãÂåñÁü•Ë≠òÂ∫´‰∏≠ÈÄ≤Ë°åÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑÊêúÂ∞ãÔºå‰ª•Áõ¥Êé•‰∏îÂèØËøΩËπ§ÁöÑÊñπÂºèÁ∏ÆÂ∞èÊΩõÂú®Ê≠£Á¢∫Á≠îÊ°àÁöÑÁØÑÂúçÔºåÁÑ∂ÂæåÂÜçÂàáÊèõÂà∞ÊΩõÂú®Ë°®ÂæµÔºåÊ†πÊìöÈùûÁµêÊßãÂåñË≥áÊñôÂ∞çÈÄô‰∫õÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêç„ÄÇÈÄôËàáÁ¥îÁ≤πÂü∫ÊñºÊΩõÂú®Ë°®ÂæµÁöÑÁõ∏ÈóúÊñπÊ≥ïÊúâÊâÄÂçÄÂà•„ÄÇ4StepFocus ÂåÖÂê´‰ª•‰∏ãÊ≠•È©üÔºö1) Áî± LLM ÈÄ≤Ë°å‰∏âÂÖÉÁµÑÁî¢Áîü‰ª•Êì∑ÂèñÈóúËÅØË≥áÊñôÔºå2) Âú®ÈÄô‰∫õ‰∏âÂÖÉÁµÑ‰∏≠ÊõøÊèõËÆäÊï∏Ôºå‰ª•Êé°Áî®Áü•Ë≠òÂúñË°®Á∏ÆÂ∞èÁ≠îÊ°àÂÄôÈÅ∏ÁØÑÂúçÔºå3) ‰ΩøÁî®Ê∂âÂèäÈóúËÅØÈùûÁµêÊßãÂåñË≥áÊñôÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÂ∞ãÂ∞çÂâ©È§òÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂ∫èÔºå4) Áî± LLM ÈáçÊñ∞Â∞çÊúÄ‰Ω≥ÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêçÔºå‰∏¶Êèê‰æõËÉåÊôØË≥áÊñô„ÄÇÂú®ÈÜ´ÁôÇ„ÄÅÁî¢ÂìÅÊé®Ëñ¶ÂíåÂ≠∏Ë°ìË´ñÊñáÊêúÂ∞ãÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫ÂØ¶ÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊì¥ÂÖÖ„ÄÇÂÆÉ‰∏çÂÉÖÂ¢ûÂä†‰∫Ü‰æÜËá™Ë≥áË®äÊ™¢Á¥¢ÁöÑÁõ∏ÂÖ≥ÂèØËøΩËπ§ËÉåÊôØË≥áË®äÔºåËÄå‰∏îËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºå‰πüÂ§ßÂπÖÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈÆÆÂ∞ëÊé¢Á¥¢ÁöÑÊñπÂêëÔºåÂõ†Ê≠§Êèê‰æõ‰∫ÜÂª£Ê≥õÁöÑÊú™‰æÜÂ∑•‰ΩúÊ©üÊúÉ„ÄÇ‰ΩøÁî®ÁöÑÂéüÂßãÁ¢ºÂèØÂú® https://github.com/kramerlab/4StepFocus ÂèñÂæó„ÄÇ

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂ∑•Á®ãÂíåÂ§öË™ûË®ÄË™ûÁæ©Êé®ÁêÜÊäÄË°ìÁöÑÊú¨‰ΩìË®≠Ë®àÔºåÁî®ÊñºÂª∫Á´ã‰∏ÄÂÄãËá™ÂãïÂåñÁ≥ªÁµ±Ôºå‰ª•Áü•Ë≠òÂúñË≠úÁöÑÂΩ¢ÂºèÂê∏Êî∂Âç∞Â∫¶ÊñôÁêÜÁöÑÁÉπÈ£™Ë≥áË®ä„ÄÇÈáçÈªûÂú®ÊñºË®≠Ë®àÊô∫ÊÖßÊñπÊ≥ïÔºå‰ª•Êé®Â∞éÊú¨‰ΩìË®≠Ë®àÔºå‰∏¶ÂÖ®Èù¢Êì∑ÂèñÈóúÊñºÈ£üÁâ©„ÄÅÈ£üË≠ú„ÄÅÈ£üÊùê„ÄÅÁÉπÈ£™ÁâπÊÄßÔºå‰ª•ÂèäÊúÄÈáçË¶ÅÁöÑÁáüÈ§äÁöÑÁü•Ë≠òÔºå‰∏¶Êì¥Â§ßË¶èÊ®°„ÄÇÊàëÂÄëÂú®ÈÄôÂÄãÁ†îË®éÊúÉË´ñÊñá‰∏≠‰ªãÁ¥π‰∫ÜÊàëÂÄëÊ≠£Âú®ÈÄ≤Ë°åÁöÑÂ∑•‰ΩúÔºåË©≥Á¥∞ÊèèËø∞‰∫ÜÊï¥ÁêÜÂç∞Â∫¶ÊñôÁêÜÁü•Ë≠òÁõ∏ÈóúÁöÑÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊàëÂÄëÁöÑÈ´òÈöéÊú¨‰ΩìË®≠Ë®à„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÆÉ‰ΩøÁî® AI„ÄÅLLM ÂíåË™ûË®ÄÊäÄË°ìÔºåÂæûÂÖ¨ÂÖ±È†òÂüüÁöÑÈ£üË≠úÈÉ®ËêΩÊ†ºÁ∂≤Á´ô‰∏≠Êï¥ÁêÜË≥áË®äÔºå‰ª•Âª∫Á´ãÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑÁü•Ë≠òÊï¥ÁêÜÊñπÊ≥ïÊòØÈÄöÁî®ÁöÑÔºåÂèØ‰ª•Ë§áË£ΩÂà∞‰ªª‰ΩïÈ†òÂüü„ÄÇË®≠Ë®àËàáÊáâÁî®ÁÑ°ÈóúÔºåÂèØÁî®Êñº AI È©ÖÂãïÁöÑÊô∫ÊÖßÂàÜÊûê„ÄÅÂª∫Á´ãÂÄã‰∫∫ÂåñÊï∏‰ΩçÂÅ•Â∫∑Êé®Ëñ¶Á≥ªÁµ±Ôºå‰ª•Âèä‰ΩøÁî®‰ΩøÁî®ËÄÖË≥áË®ä„ÄÅÈ£üÁâ©ÁîüÁâ©ÂåñÂ≠∏„ÄÅÂú∞ÁêÜË≥áË®ä„ÄÅËæ≤Ê•≠Ë≥áË®äÁ≠âËÑàÁµ°Ë≥áË®äÔºå‰æÜË£úÂÖÖÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇ

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

ÊëòË¶ÅÔºöËæ≤Ê•≠ÁîüÁî¢ÈúÄË¶ÅÂ∞èÂøÉÁÆ°ÁêÜËº∏ÂÖ•Ôºå‰æãÂ¶ÇÊÆ∫ËèåÂäë„ÄÅÊÆ∫Ëü≤ÂäëÂíåÈô§ËçâÂäëÔºå‰ª•Á¢∫‰øù‰ΩúÁâ©ÊàêÂäü„ÄÅÈ´òÁî¢„ÄÅÊúâÂà©ÂèØÂúñ‰∏îÂÖ∑ÊúâÂÑ™ËâØÁöÑÁ®ÆÂ≠êÂìÅË≥™„ÄÇÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÁî∞Èñì‰ΩúÁâ©ÁÆ°ÁêÜ‰æùË≥¥ÊñºÁ≤óÁï•ÁöÑ‰ΩúÁâ©ÁÆ°ÁêÜÁ≠ñÁï•ÔºåÂÖ∂‰∏≠Êï¥ÂÄãÁî∞Âú∞ÈÉΩÂô¥ÁÅë‰∫ÜÊéßÂà∂ÁóÖËü≤ÂÆ≥ÁöÑÂåñÂ≠∏Áâ©Ë≥™ÔºåÂ∞éËá¥ÊàêÊú¨Â¢ûÂä†ÂíåÂúüÂ£§Âíå‰ΩúÁâ©ÁÆ°ÁêÜ‰∏ç‰Ω≥„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞‰∏¶ÂÑ™Âåñ‰ΩúÁâ©ÁîüÁî¢ÔºåÊàëÂÄëÂú®ËôõÊì¨Áî∞ÈñìÁí∞Â¢É‰∏≠Âà©Áî®Ê©üÂô®Â≠∏ÁøíÂ∑•ÂÖ∑ÁÇ∫Ëæ≤Ê∞ëÁîüÊàêÂ±ÄÈÉ®ÁÆ°ÁêÜË®àÁï´Ôºå‰ª•ÁÆ°ÁêÜÁîüÁâ©Â®ÅËÑÖ‰∏¶ÂêåÊôÇÊúÄÂ§ßÂåñÂà©ÊΩ§„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü AgGymÔºå‰∏ÄÂÄãÊ®°ÁµÑÂåñ„ÄÅ‰ΩúÁâ©ÂíåÂ£ìÂäõ‰∏çÂèØÁü•ÁöÑÊ®°Êì¨Êû∂ÊßãÔºåÁî®ÊñºÊ®°Êì¨Áî∞ÈñìÁîüÁâ©Â£ìÂäõÁöÑÊì¥Êï£Ôºå‰∏¶‰º∞ÁÆóÊúâÂíåÊ≤íÊúâÂåñÂ≠∏ËôïÁêÜÁöÑÁî¢ÈáèÊêçÂ§±„ÄÇÊàëÂÄë‰ΩøÁî®ÁúüÂØ¶Êï∏ÊìöÈÄ≤Ë°åÈ©óË≠âÔºåÈ°ØÁ§∫ AgGym ÂèØ‰ª•‰ΩøÁî®ÊúâÈôêÁöÑÊï∏ÊìöÈÄ≤Ë°åËá™Ë®ÇÔºå‰ª•Ê®°Êì¨ÂêÑÁ®ÆÁîüÁâ©Â£ìÂäõÊ¢ù‰ª∂‰∏ãÁöÑÁî¢ÈáèÁµêÊûú„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë≠âÊòéÔºåÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (RL) ÊîøÁ≠ñÂèØ‰ª•‰ΩøÁî® AgGym ÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•Ë®≠Ë®àË∂ÖÁ≤æÁ¢∫ÁöÑÁîüÁâ©Â£ìÂäõÁ∑©Ëß£Á≠ñÁï•Ôºå‰∏¶ÊúâÂèØËÉΩ‰ª•Êõ¥Â∞ëÁöÑÂåñÂ≠∏Áâ©Ë≥™ÂíåÊõ¥‰ΩéÁöÑÊàêÊú¨Â¢ûÂä†Áî¢ÈáèÊÅ¢Âæ©„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂïüÁî®‰∫ÜÂÄã‰∫∫ÂåñÊ±∫Á≠ñÊîØÊè¥ÔºåÂèØ‰ª•Â∞áÁîüÁâ©Â£ìÂäõÁÆ°ÁêÜÂæûÂü∫ÊñºÊôÇÈñìË°®ÂíåË¢´ÂãïËΩâËÆäÁÇ∫Ê©üÊúÉ‰∏ªÁæ©ÂíåË¶èÁØÑÊÄß„ÄÇÊàëÂÄëÈÇÑÂ∞á AgGym ËªüÈ´îÂØ¶‰Ωú‰ΩúÁÇ∫Á§æÂçÄË≥áÊ∫êÈáãÂá∫Ôºå‰∏¶ÈÇÄË´ãÂ∞àÂÆ∂ÁÇ∫ÈÄôÂÄãÈñãÊîæÂéüÂßãÁ¢º‰∏îÂÖ¨ÈñãÂèØÁî®ÁöÑÊ®°ÁµÑÂåñÁí∞Â¢ÉÊû∂ÊßãÂÅöÂá∫Ë≤¢Áçª„ÄÇÂèØ‰ª•Âú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÂéüÂßãÁ¢ºÔºöhttps://github.com/SCSLabISU/AgGym„ÄÇ

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

ÊëòË¶ÅÔºöË∂ÖÂª£ËßíËû¢ÂÖâË°ÄÁÆ°ÈÄ†ÂΩ±ÔºàUWF-FAÔºâ‰ΩøÁî®ÂèØËÉΩÂÖ∑ÊúâÊΩõÂú®Âç±ÂÆ≥ÁöÑÈàâËû¢ÂÖâÁ¥†ÔºåÂèØÁ≤æÁ¢∫Ë≠òÂà•ÁúºÁñæ„ÄÇÁèæÊúâÁ†îÁ©∂Â∑≤ÈñãÁôºÂá∫ÂæûË∂ÖÂª£ËßíÊéÉÊèèÈõ∑Â∞ÑÁúºÁßëÈè°ÔºàUWF-SLOÔºâÁî¢Áîü UWF-FA ÁöÑÊñπÊ≥ïÔºå‰ª•Ê∏õÂ∞ëËàáÊ≥®Â∞ÑÁõ∏ÈóúÁöÑ‰∏çËâØÂèçÊáâ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂú®Áî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂæåÊúü UWF-FA ÊñπÈù¢ÊïàÊûúËºÉÂ∑ÆÔºåÁâπÂà•ÊòØÂú®ÁóÖÁÅ∂ÂçÄÂüüÂíåÁ≤æÁ¥∞Á¥∞ÁØÄÊñπÈù¢„ÄÇÁî¢ÁîüÈ´òÂìÅË≥™ÂæåÊúü UWF-FA Èù¢Ëá®ÂÖ©È†Ö‰∏ªË¶ÅÊåëÊà∞ÔºöÈÖçÂ∞çÁöÑ UWF-SLO ÂíåÊó©Êúü/ÂæåÊúü UWF-FA Ë≥áÊñôÈõÜÁ®ÄÂ∞ëÔºå‰ª•ÂèäÈúÄË¶ÅÂú®ÁóÖÁÅ∂ÈÉ®‰ΩçÂíåÊΩõÂú®Âá∫Ë°ÄÂçÄÂüüÈÄ≤Ë°åÈÄºÁúüÁöÑÁî¢Áîü„ÄÇÊú¨Á†îÁ©∂ÂºïÈÄ≤‰∏ÄÁ®ÆÊîπËâØÁöÑÊΩõÂú®Êì¥Êï£Ê®°ÂûãÊû∂ÊßãÔºåÂæûÊúâÈôêÈÖçÂ∞çÁöÑ UWF ÂΩ±ÂÉèÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂæåÊúü UWF-FA„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂâçÈù¢ÊèêÂà∞ÁöÑÊåëÊà∞ÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÊé°Áî®‰∏ÄÂÄãÊ®°ÁµÑÔºåÂà©Áî®Ë∑®ÊôÇÈñìÂçÄÂüüÂ∑ÆÁï∞ÊêçÂ§±ÔºåÈºìÂãµÊ®°ÂûãÂ∞àÊ≥®ÊñºÊó©ÊúüÂíåÂæåÊúü‰πãÈñìÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®Êì¥Êï£ÂâçÂêëÈÅéÁ®ã‰∏≠ÂºïÈÄ≤‰∏ÄÁ®Æ‰ΩéÈ†ªÂ¢ûÂº∑ÈõúË®äÁ≠ñÁï•Ôºå‰ª•ÊîπÂñÑÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÁúüÂØ¶ÊÄß„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑ËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô®Ê®°ÁµÑÁöÑÂ∞çÊáâËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®Ë≥áÊñôÈõÜÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÂØ¶‰Ωú‰∏ÄÂÄãÈñòÊéßÂç∑Á©çÁ∑®Á¢ºÂô®ÔºåÂæûÊ¢ù‰ª∂ÂΩ±ÂÉè‰∏≠ËêÉÂèñÈ°çÂ§ñË≥áË®ä„ÄÇÊàëÂÄëÈáùÂ∞çË∂ÖÂª£ËßíÂæåÊúüËû¢ÂÖâË°ÄÁÆ°ÈÄ†ÂΩ±ÔºàLPUWF-LDMÔºâÁöÑÊΩõÂú®Êì¥Êï£Ê®°ÂûãÊúâÊïàÈáçÂª∫ÂæåÊúü UWF-FA ‰∏≠ÁöÑÁ≤æÁ¥∞Á¥∞ÁØÄÔºå‰∏¶Âú®‰ΩøÁî®ÊúâÈôêË≥áÊñôÈõÜÊôÇÔºåËàáÂÖ∂‰ªñÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö
https://github.com/Tinysqua/****„ÄÇ

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

ÊëòË¶ÅÔºöÂøÉËáüËÅΩË®∫ÊòØË®∫Êñ∑ÂøÉË°ÄÁÆ°ÁñæÁóÖ (CVD) ÁöÑ‰∏ÄÈ†ÖÊï¥ÂêàÂ∑•ÂÖ∑ÔºåÈÄöÂ∏∏‰æùË≥¥ÊñºËá®Â∫äÈÜ´Â∏´ÁöÑ‰∏ªËßÄË©ÆÈáãÔºåÂú®‰∏ÄËá¥ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÊñπÈù¢Â≠òÂú®ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü BUET Â§öÈáçÁñæÁóÖÂøÉÈü≥ (BMD-HS) Ë≥áÊñôÈõÜÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢‰∏îÁ∂ìÈÅéÁ≤æÂøÉÁ≠ñÂäÉÁöÑÂøÉÈü≥ÈåÑÈü≥Ë≥áÊñôÈõÜ„ÄÇÊ≠§Ë≥áÊñôÈõÜÂåÖÂê´‰∫îÁ®ÆÂ∏∏Ë¶ãÂøÉÈü≥ÁöÑ 864 ÂÄãÈåÑÈü≥Ôºå‰ª£Ë°®‰∫ÜÂª£Ê≥õÁöÑÂøÉÁì£ËÜúÁñæÁóÖÔºåÈáçÈªûÂú®ÊñºË®∫Êñ∑Âõ∞Èõ£ÁöÑÁóÖ‰æã„ÄÇBMD-HS Ë≥áÊñôÈõÜÁöÑÁ™ÅÂá∫ÁâπÈªûÊòØÂÖ∂ÂâµÊñ∞ÁöÑÂ§öÊ®ôÁ±§Ë®ªËß£Á≥ªÁµ±ÔºåÂÆÉÊ∂µËìã‰∫ÜÂêÑÁ®ÆÁñæÁóÖÂíåÁç®ÁâπÁöÑÁñæÁóÖÁãÄÊÖã„ÄÇÈÄôÂÄãÁ≥ªÁµ±È°ØËëóÂ¢ûÂº∑‰∫ÜË≥áÊñôÈõÜÂú®ÈñãÁôºËá™ÂãïÂøÉÈü≥ÂàÜÈ°ûÂíåË®∫Êñ∑‰∏≠ÈÄ≤ÈöéÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàÁî®„ÄÇÈÄèÈÅéÂΩåÂêàÂÇ≥Áµ±ËÅΩË®∫ÂØ¶ÂãôËàáÁï∂‰ª£Ë≥áÊñôÈ©ÖÂãïË®∫Êñ∑ÊñπÊ≥ï‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåBMD-HS Ë≥áÊñôÈõÜÊ∫ñÂÇôÂ•ΩÈù©Êñ∞ÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑË®∫Êñ∑ÂíåÁÆ°ÁêÜÔºåÁÇ∫ÂøÉËáüÂÅ•Â∫∑Á†îÁ©∂ÁöÑÈÄ≤Â±ïÊèê‰æõÂØ∂Ë≤¥ÁöÑË≥áÊ∫ê„ÄÇÊ≠§Ë≥áÊñôÈõÜÂèØÈÄèÈÅé‰ª•‰∏ãÈÄ£ÁµêÂÖ¨ÈñãÂèñÂæóÔºöhttps://github.com/mHealthBuet/BMD-HS-Dataset„ÄÇ

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

ÊëòË¶ÅÔºöËá™ÂãïË®∫Êñ∑ÊäÄË°ìÂ∑≤ÊºîÈÄ≤Âà∞ËÉΩÈÄèÈÅé‰ΩøÁî®ÂñÆ‰∏ÄÊ®°ÂºèÁúºÂ∫ïÂΩ±ÂÉèÊàñÂÖâÂ≠∏Áõ∏Âπ≤Êñ∑Â±§ÊéÉÊèè (OCT) ‰æÜËæ®Ë≠òÂπ¥ÈΩ°Áõ∏ÈóúÊÄßÈªÉÊñëÈÉ®ÁóÖËÆä (AMD)„ÄÇÁÇ∫‰∫ÜÂàÜÈ°ûÁúºÁñæÔºåÁúºÂ∫ïÂíå OCT ÂΩ±ÂÉèÊòØËá®Â∫äÁí∞Â¢É‰∏≠‰ΩøÁî®ÊúÄÈóúÈçµÁöÑÂΩ±ÂÉèÊ®°Âºè„ÄÇÂ§ßÂ§öÊï∏Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊäÄË°ìÂª∫Á´ãÂú®ÂñÆ‰∏ÄÂΩ±ÂÉèÊ®°Âºè‰∏äÔºåÂÆÉÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äËÄÉÈáè‰∫ÜÁúºÁñæÔºåÂçªÂøΩÁï•‰∫ÜÂÖ∂‰ªñÊ®°ÂºèÔºåËÄåÂÖ∂‰ªñÊ®°ÂºèÂåÖÂê´‰∫Ü‰∏çÂêåÂΩ±ÂÉèÊ®°Âºè‰πãÈñìÁöÑË©≥Áõ°Ë≥áË®ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÂºèÁâπÂÆöÁöÑÂ§öÂ∞∫Â∫¶Ëâ≤ÂΩ©Á©∫ÈñìÂµåÂÖ•Êï¥ÂêàÔºå‰∏¶Âü∫ÊñºÁî®ÊñºÂàÜÈ°ûÁöÑËΩâÁßªÂ≠∏ÁøíÁöÑÊ≥®ÊÑèÂäõÊ©üÂà∂ (MCGAEc)ÔºåÂÆÉËÉΩ‰ΩøÁî®‰∏çÂêåÁöÑËâ≤ÂΩ©Á©∫ÈñìÂú®‰∏çÂêåÁöÑÂ∞∫Â∫¶‰∏äÊúâÊïàÊèêÂèñ‰∏çÂêåÁöÑÊ®°ÂºèË≥áË®ä„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥π‰∫ÜÊ®°ÂºèÁâπÂÆöÁöÑÂ§öÂ∞∫Â∫¶Ëâ≤ÂΩ©Á©∫ÈñìÁ∑®Á¢ºÂô®Ê®°ÂûãÔºåÂÆÉÈÄèÈÅéÂ∞á‰∏çÂêåÁöÑÁâπÂæµËâ≤ÂΩ©Á©∫ÈñìÊï¥ÂêàÂà∞Â§öÂ∞∫Â∫¶‰∏≠Ôºå‰æÜÁ¥çÂÖ•‰∏çÂêåÁöÑÁâπÂæµË°®ÂæµÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÊû∂Êßã‰∏≠„ÄÇÂæûÂÖàÂâçÁöÑÁ∑®Á¢ºÂô®Ê®°ÁµÑ‰∏≠ÊèêÂèñÁöÑÁâπÂæµËàáÊ≥®ÊÑèÂäõÊ©üÂà∂ÁµêÂêàÔºå‰ª•ÊèêÂèñÂÖ®ÂüüÁâπÂæµË°®ÂæµÔºåÂÆÉËàáÂÖàÂâçÊèêÂèñÁöÑÁâπÂæµÊï¥ÂêàÔºå‰∏¶ËΩâÁßªÂà∞Èö®Ê©üÊ£ÆÊûóÂàÜÈ°ûÂô®Ôºå‰ª•ÈÄ≤Ë°å AMD ÂàÜÈ°û„ÄÇÁÇ∫‰∫ÜÂàÜÊûêÊâÄÊèêÂá∫ÁöÑ MCGAEc ÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÊàëÂÄëÂà©Áî®‰∫Ü‰æÜËá™ Project Macula for AMD ÁöÑÂÖ¨ÈñãÂ§öÊ®°ÂºèË≥áÊñôÈõÜÔºå‰∏¶ËàáÁèæÊúâÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

ÊëòË¶ÅÔºöË™øÊï¥Â§ßÂûãÈ†êË®ìÁ∑¥Âü∫Á§éÊ®°ÂûãÔºà‰æãÂ¶Ç SAMÔºâ‰ª•ÈÄ≤Ë°åÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰ªçÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÈóúÈçµÊ≠•È©üÊ∂âÂèäÂà∂ÂÆö‰∏ÄÁ≥ªÂàóÂåÖÂê´ÁâπÂÆöËá®Â∫äË™™ÊòéÁöÑÂ∞àÈñÄÊèêÁ§∫„ÄÇÈÅéÂéªÁöÑÂ∑•‰ΩúÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÊØèÂÄã‰æãÈ†ÖÁöÑÂñÆ‰∏ÄÊèêÁ§∫È°ûÂûãÔºåÈÄôÈúÄË¶ÅÊâãÂãïËº∏ÂÖ•ÁêÜÊÉ≥ÁöÑÊ≠£Á¢∫ÊèêÁ§∫ÔºåÊïàÁéáËºÉ‰Ωé„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂª∫Ë≠∞Âà©Áî®‰∏çÂêåÁ≤íÂ∫¶ÁöÑÊèêÁ§∫ÔºåÈÄô‰∫õÊèêÁ§∫‰æÜËá™ÂéüÂßãÂΩ±ÂÉèÔºå‰ª•Êèê‰æõÊõ¥Âª£Ê≥õÁöÑËá®Â∫äË¶ãËß£„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊΩõÂú®Ë°ùÁ™ÅÔºåÁµêÂêà‰∏çÂêåÈ°ûÂûãÁöÑÊèêÁ§∫ÂèØËÉΩÊúÉÊßãÊàêÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÁî±Á≤óÂà∞Á¥∞ÁöÑÊ©üÂà∂ÔºåÁ®±ÁÇ∫Ë™≤Á®ãÊèêÁ§∫ÔºåÂÆÉÈÄêÊ≠•Êï¥Âêà‰∏çÂêåÈ°ûÂûãÁöÑÊèêÁ§∫„ÄÇÈÄèÈÅéÂ∞çÂêÑÁ®ÆÊ®°Âºè‰∏ãÁöÑ‰∏âÂÄãÂÖ¨ÂÖ±ÈÜ´Â≠∏Ë≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂÆÉ‰∏çÂÉÖËá™ÂãïÂåñÊèêÁ§∫ÁîüÊàêÈÅéÁ®ãÔºåËÄå‰∏îËàáÂÖ∂‰ªñÂü∫Êñº SAM ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊñπÊ≥ïÁõ∏ÊØîÔºåÈÇÑÁî¢Áîü‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/AnnaZzz-zxq/Curriculum-Prompting„ÄÇ

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

ÊëòË¶ÅÔºöÁΩïË¶ãÂ©¶ÁßëËÖ´Áò§ (RGT) Áî±ÊñºÂÖ∂‰ΩéÁôºÁîüÁéáÂíåÁï∞Ë≥™ÊÄßÔºåÂ∞çËá®Â∫äÂ∏∂‰æÜÈáçÂ§ßÊåëÊà∞„ÄÇÁº∫‰πèÊòéÁ¢∫ÁöÑÊåáÂºïÂ∞éËá¥Ê¨°‰Ω≥ÁÆ°ÁêÜÂíå‰∏çËâØÈ†êÂæå„ÄÇÂàÜÂ≠êËÖ´Áò§ÂßîÂì°ÊúÉÈÄèÈÅéÊ†πÊìöÁîüÁâ©Ê®ôË®òÂÆ¢Ë£ΩÂåñÊ≤ªÁôÇÔºåÂä†ÈÄüÂèñÂæóÊúâÊïàÁôÇÊ≥ïÔºåË∂ÖË∂äÁôåÁóáÈ°ûÂûã„ÄÇÈúÄË¶ÅÊâãÂãïÊï¥ÁêÜÁöÑÈùûÁµêÊßãÂåñË≥áÊñôÈòªÁ§ô‰∫ÜÁîüÁâ©Ê®ôË®òÂàÜÊûêÂú®ÁôÇÊ≥ïÈÖçÂ∞ç‰∏≠ÁöÑÊúâÊïà‰ΩøÁî®„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫ RGT ÁöÑÁ≤æÊ∫ñÈÜ´ÁôÇÂª∫ÊßãÊï∏‰ΩçÈõôËÉûËÉé„ÄÇ
ÊàëÂÄëÁöÑÊ¶ÇÂøµÈ©óË≠âÊï∏‰ΩçÈõôËÉûËÉéÁ≥ªÁµ±Êï¥Âêà‰∫Ü‰æÜËá™Ê©üÊßãÂíåÂ∑≤ÁôºË°®ÁöÑÊ°à‰æã (n=21) ÁöÑËá®Â∫äÂíåÁîüÁâ©Ê®ôË®òË≥áÊñôÔºå‰ª•Âèä‰æÜËá™ÊñáÁçªÁöÑË≥áÊñô (n=655 ÁØáÂá∫ÁâàÁâ©Ôºån=404,265 ÂêçÊÇ£ËÄÖ)ÔºåÁÇ∫ËΩâÁßªÊÄßÂ≠êÂÆÆËÇâÁò§ÁôåÂà∂ÂÆöÂÆ¢Ë£ΩÂåñÊ≤ªÁôÇË®àÁï´ÔºåÊâæÂá∫ÂÇ≥Áµ±ÂñÆ‰∏Ä‰æÜÊ∫êÂàÜÊûêÂèØËÉΩÈÅ∫ÊºèÁöÑÈÅ∏È†Ö„ÄÇLLM ÂïüÁî®ÁöÑÊï∏‰ΩçÈõôËÉûËÉéÊúâÊïàÂú∞Ê®°Êì¨ÂÄãÂà•ÊÇ£ËÄÖÁöÑËªåË∑°„ÄÇÂæûÂü∫ÊñºÂô®ÂÆòÁöÑËÖ´Áò§ÂÆöÁæ©ËΩâËÆäÁÇ∫Âü∫ÊñºÁîüÁâ©Â≠∏ÁöÑÂÆöÁæ©ÔºåËÉΩÂØ¶ÁèæÂÄã‰∫∫ÂåñÁÖßË≠∑ÔºåÈÄ≤ËÄåÊèêÂçá RGT ÁÆ°ÁêÜ‰∏¶ÊîπÂñÑÊÇ£ËÄÖÈ†êÂæå„ÄÇ

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

ÊëòË¶ÅÔºöË™ûÈü≥ÂûãÊÜÇÈ¨±Ê™¢Ê∏¨Â∞çËá™ÂãïÂåñÊ™¢Ê∏¨‰æÜË™™ÊòØ‰∏ÄÂ§ßÊåëÊà∞ÔºåÂõ†ÁÇ∫ÂÆÉÂú®‰∏çÂêåÂÄãÈ´îÈñìÁöÑË°®ÁèæÁç®ÁâπÔºå‰∏îË≥áÊñôÁ®ÄÂ∞ë„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü DAAMAudioCNNLSTM Âíå DAAMAudioTransformerÔºåÈÄôÂÖ©ÂÄãÂèÉÊï∏ÊúâÊïà‰∏îÂèØËß£ÈáãÁöÑÊ®°ÂûãÔºåÁî®ÊñºÈü≥Ë®äÁâπÂæµËêÉÂèñÂíåÊÜÇÈ¨±Ê™¢Ê∏¨„ÄÇDAAMAudioCNNLSTM Êé°Áî®ÂâµÊñ∞ÁöÑ CNN-LSTM Êû∂ÊßãÔºåÊê≠ÈÖçÂ§öÈ†≠ÂØÜÂ∫¶Ëá™ÈÅ©ÊáâÊ≥®ÊÑèÂäõÊ©üÂà∂ (DAAM)ÔºåÂãïÊÖãÈóúÊ≥®ÊñºÊúâÊÑèÁæ©ÁöÑË™ûÈü≥ÂçÄÊÆµ„ÄÇDAAMAudioTransformer Âà©Áî®TransformerÁ∑®Á¢ºÂô®Âèñ‰ª£ CNN-LSTM Êû∂ÊßãÔºå‰∏¶Á¥çÂÖ•Áõ∏ÂêåÁöÑ DAAM Ê®°ÁµÑÔºå‰ª•Â¢ûÂº∑Ê≥®ÊÑèÂäõÂíåÂèØËß£ÈáãÊÄß„ÄÇÈÄô‰∫õÊñπÊ≥ï‰∏çÂÉÖÂ¢ûÂº∑‰∫ÜÊ™¢Ê∏¨ÁöÑÁ©©ÂÅ•ÊÄßÂíåÂèØËß£ÈáãÊÄßÔºåÈÇÑÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºöDAAMAudioCNNLSTM ÁöÑ F1 Â∑®ËßÄÂàÜÊï∏ÁÇ∫ 0.702ÔºåDAAMAudioTransformer Âú® DAIC-WOZ Ë≥áÊñôÈõÜ‰∏äÁöÑ F1 Â∑®ËßÄÂàÜÊï∏ÁÇ∫ 0.72ÔºåÂú®Ë®ìÁ∑¥/È©óË≠âÊúüÈñì‰∏ç‰æùË≥¥ÊñºËºîÂä©Ë≥áË®äÔºå‰æãÂ¶ÇÊØçÈü≥‰ΩçÁΩÆÂíåË™™Ë©±ËÄÖË≥áË®äÔºåÈÄôËàáÂÖàÂâçÁöÑÂÅöÊ≥ï‰∏çÂêå„ÄÇÈÄôÂÖ©ÂÄãÊ®°ÂûãÂú®Âà©Áî®Ë™ûÈü≥Ë®äËôüÈÄ≤Ë°åÊÜÇÈ¨±Ê™¢Ê∏¨ÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÂèØËß£ÈáãÊÄßÂíåÊïàÁéáÔºå‰ª£Ë°®ËëóÊúùÂêëÊõ¥ÂèØÈù†„ÄÅËá®Â∫ä‰∏äÊúâÁî®ÁöÑË®∫Êñ∑Â∑•ÂÖ∑ÈÇÅÈÄ≤‰∫Ü‰∏ÄÂ§ßÊ≠•Ôºå‰∏¶È†êÁ§∫ËëóË™ûÈü≥ÂíåÂøÉÁêÜ‰øùÂÅ•ÁöÑÈÄ≤Ê≠•„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÈÄôÂÄãÈ†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàëÂÄëÂÖ¨Èñã‰∫ÜÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÇ


### Medical explainable AI
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-24**|**Towards Case-based Interpretability for Medical Federated Learning**|Laura Latorre et.al.|[2408.13626v1](http://arxiv.org/abs/2408.13626v1)|null|
|**2024-08-22**|**AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**|Douwe J. Spaanderman et.al.|[2408.12491v1](http://arxiv.org/abs/2408.12491v1)|null|
|**2024-08-14**|**Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy**|Kimji N. Pellano et.al.|[2409.00001v1](http://arxiv.org/abs/2409.00001v1)|null|
|**2024-08-06**|**MicroXercise: A Micro-Level Comparative and Explainable System for Remote Physical Therapy**|Hanchen David Wang et.al.|[2408.11837v1](http://arxiv.org/abs/2408.11837v1)|null|
|**2024-08-05**|**The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development**|Joshua Morriss et.al.|[2408.05239v1](http://arxiv.org/abs/2408.05239v1)|null|
|**2024-08-05**|**Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns**|Chi Him Ng et.al.|[2408.02709v1](http://arxiv.org/abs/2408.02709v1)|null|
|**2024-08-05**|**Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability**|Masoud Muhammed Hassan et.al.|[2408.02706v1](http://arxiv.org/abs/2408.02706v1)|null|
|**2024-07-26**|**MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**|Shyam Dongre et.al.|[2407.20284v1](http://arxiv.org/abs/2407.20284v1)|null|
|**2024-07-25**|**Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**|Alessandro De Carlo et.al.|[2407.18343v2](http://arxiv.org/abs/2407.18343v2)|null|
|**2024-07-24**|**Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**|Nikolaos Ntampakis et.al.|[2407.17324v2](http://arxiv.org/abs/2407.17324v2)|null|
|**2024-07-24**|**Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition**|Michele Fiori et.al.|[2408.06352v1](http://arxiv.org/abs/2408.06352v1)|null|
|**2024-07-21**|**Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions**|Naseem Khan et.al.|[2408.03335v1](http://arxiv.org/abs/2408.03335v1)|null|
|**2024-07-18**|**A Comparative Study on Automatic Coding of Medical Letters with Explainability**|Jamie Glen et.al.|[2407.13638v1](http://arxiv.org/abs/2407.13638v1)|[link](https://github.com/Glenj01/Medical-Coding)|
|**2024-07-09**|**Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**|Abdul Karim Gizzini et.al.|[2407.07009v1](http://arxiv.org/abs/2407.07009v1)|null|
|**2024-07-07**|**Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**|P. N. Karthikayan et.al.|[2407.05440v2](http://arxiv.org/abs/2407.05440v2)|null|
|**2024-07-03**|**A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**|Congzhen Shi et.al.|[2407.15851v1](http://arxiv.org/abs/2407.15851v1)|null|
|**2024-07-01**|**The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**|Ximing Wen et.al.|[2407.06206v1](http://arxiv.org/abs/2407.06206v1)|null|
|**2024-06-28**|**Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**|Sai Krishna Revanth Vuruma et.al.|[2407.00167v1](http://arxiv.org/abs/2407.00167v1)|null|
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-11**|**Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**|Fatemeh Ebrahimzadeh et.al.|[2406.07114v2](http://arxiv.org/abs/2406.07114v2)|null|
|**2024-06-10**|**AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**|K M Tawsik Jawad et.al.|[2406.06728v1](http://arxiv.org/abs/2406.06728v1)|null|
|**2024-06-10**|**Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**|Yusif Ibrahimov et.al.|[2406.05984v1](http://arxiv.org/abs/2406.05984v1)|null|
|**2024-06-09**|**Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**|Zhan Zhang et.al.|[2406.05746v1](http://arxiv.org/abs/2406.05746v1)|null|
|**2024-06-07**|**Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**|Faseela Abdullakutty et.al.|[2406.12897v1](http://arxiv.org/abs/2406.12897v1)|null|
|**2024-06-07**|**Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**|Yong-Min Shin et.al.|[2406.04612v1](http://arxiv.org/abs/2406.04612v1)|[link](https://github.com/jordan7186/gatt)|
|**2024-06-04**|**Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**|Dinuka Sandun Udayantha et.al.|[2406.16908v3](http://arxiv.org/abs/2406.16908v3)|[link](https://github.com/dinuka-1999/braineocare)|
|**2024-06-01**|**Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**|Samita Bai et.al.|[2406.00532v1](http://arxiv.org/abs/2406.00532v1)|null|
|**2024-06-01**|**Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**|Alaa Nfissi et.al.|[2406.01624v2](http://arxiv.org/abs/2406.01624v2)|[link](https://github.com/alaanfissi/unveiling-hidden-factors-explainable-ai-for-feature-boosting-in-speech-emotion-recognition)|
|**2024-05-31**|**The Explanation Necessity for Healthcare AI**|Michail Mamalakis et.al.|[2406.00216v1](http://arxiv.org/abs/2406.00216v1)|null|
|**2024-05-29**|**Interdisciplinary Expertise to Advance Equitable Explainable AI**|Chloe R. Bennett et.al.|[2406.18563v1](http://arxiv.org/abs/2406.18563v1)|null|
|**2024-05-27**|**"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**|Hubert D. ZajƒÖc et.al.|[2407.11978v1](http://arxiv.org/abs/2407.11978v1)|null|
|**2024-05-26**|**Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**|Min Hun Lee et.al.|[2405.16424v1](http://arxiv.org/abs/2405.16424v1)|null|
|**2024-05-26**|**Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**|Ziming Liu et.al.|[2405.17502v1](http://arxiv.org/abs/2405.17502v1)|null|
|**2024-05-24**|**Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**|Catalina Gomez et.al.|[2407.11974v1](http://arxiv.org/abs/2407.11974v1)|null|
|**2024-05-23**|**Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**|Yingying Fang et.al.|[2406.18552v1](http://arxiv.org/abs/2406.18552v1)|null|
|**2024-05-21**|**The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**|Mohsen Jozani et.al.|[2405.13099v1](http://arxiv.org/abs/2405.13099v1)|null|
|**2024-05-17**|**ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**|Harris Bin Munawar et.al.|[2405.10645v1](http://arxiv.org/abs/2405.10645v1)|null|
|**2024-05-13**|**Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**|Camelia Oprea et.al.|[2405.07590v1](http://arxiv.org/abs/2405.07590v1)|null|
|**2024-05-10**|**XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**|Fatemeh Nazary et.al.|[2405.06270v3](http://arxiv.org/abs/2405.06270v3)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel Mir√≥-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|[link](https://github.com/zzs95/RSP_COVID)|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-19**|**Explainable AI for Fair Sepsis Mortality Predictive Model**|Chia-Hsuan Chang et.al.|[2404.13139v1](http://arxiv.org/abs/2404.13139v1)|null|
|**2024-04-19**|**Multi Class Depression Detection Through Tweets using Artificial Intelligence**|Muhammad Osama Nusrat et.al.|[2404.13104v1](http://arxiv.org/abs/2404.13104v1)|[link](https://github.com/mnusrat786/masters-thesis)|
|**2024-04-19**|**COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**|Dmytro Shvetsov et.al.|[2404.12832v2](http://arxiv.org/abs/2404.12832v2)|[link](https://github.com/dmytro-shvetsov/counterfactual-search)|
|**2024-04-15**|**Hybrid Intelligence for Digital Humanities**|Victor de Boer et.al.|[2406.15374v1](http://arxiv.org/abs/2406.15374v1)|null|
|**2024-04-14**|**Ethical Framework for Responsible Foundational Models in Medical Imaging**|Abhijit Das et.al.|[2406.11868v1](http://arxiv.org/abs/2406.11868v1)|null|
|**2024-04-09**|**Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**|Milad Yousefi et.al.|[2404.07239v1](http://arxiv.org/abs/2404.07239v1)|null|
|**2024-04-06**|**Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**|Taminul Islam et.al.|[2404.04686v1](http://arxiv.org/abs/2404.04686v1)|null|
|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|Maryam Ahmed et.al.|[2404.03892v3](http://arxiv.org/abs/2404.03892v3)|null|
|**2024-03-30**|**Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives**|Xingrui Gu et.al.|[2404.00320v2](http://arxiv.org/abs/2404.00320v2)|null|
|**2024-03-26**|**Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**|Andrea Ferrario et.al.|[2403.17873v1](http://arxiv.org/abs/2403.17873v1)|null|
|**2024-03-26**|**Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**|Han Yuan et.al.|[2403.18871v1](http://arxiv.org/abs/2403.18871v1)|[link](https://github.com/han-yuan-med/template-explanation)|
|**2024-03-03**|**Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**|S√©amus Lankford et.al.|[2403.01580v1](http://arxiv.org/abs/2403.01580v1)|null|
|**2024-02-28**|**Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**|Yasin Sadeghi Bazargani et.al.|[2402.18600v1](http://arxiv.org/abs/2402.18600v1)|null|
|**2024-02-22**|**Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**|A. J. Karran et.al.|[2402.15027v2](http://arxiv.org/abs/2402.15027v2)|null|
|**2024-02-12**|**Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**|Aruna Mohan et.al.|[2402.09474v2](http://arxiv.org/abs/2402.09474v2)|null|
|**2024-02-05**|**Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**|Aryan Agrawal et.al.|[2402.05127v1](http://arxiv.org/abs/2402.05127v1)|null|
|**2024-01-24**|**Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**|Timoth√©e Schmude et.al.|[2401.13324v5](http://arxiv.org/abs/2401.13324v5)|null|
|**2024-01-02**|**Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**|Vahid Ashrafimoghari et.al.|[2401.02985v1](http://arxiv.org/abs/2401.02985v1)|null|
|**2023-12-29**|**XAI for In-hospital Mortality Prediction via Multimodal ICU Data**|Xingqiao Li et.al.|[2312.17624v1](http://arxiv.org/abs/2312.17624v1)|[link](https://github.com/lixingqiao/xai-icu)|
|**2023-12-22**|**Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**|Norman Zerbe et.al.|[2401.09450v2](http://arxiv.org/abs/2401.09450v2)|null|
|**2023-12-18**|**Robust Stochastic Graph Generator for Counterfactual Explanations**|Mario Alfonso Prado-Romero et.al.|[2312.11747v2](http://arxiv.org/abs/2312.11747v2)|null|
|**2023-12-10**|**Evaluating the Utility of Model Explanations for Model Development**|Shawn Im et.al.|[2312.06032v1](http://arxiv.org/abs/2312.06032v1)|null|
|**2023-12-05**|**Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**|Manas Gaur et.al.|[2312.06798v1](http://arxiv.org/abs/2312.06798v1)|null|
|**2023-11-28**|**Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**|Jacob R. Epifano et.al.|[2311.17133v1](http://arxiv.org/abs/2311.17133v1)|null|
|**2023-11-27**|**Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**|Benjamin Keel et.al.|[2311.15719v1](http://arxiv.org/abs/2311.15719v1)|[link](https://github.com/benkeel/vae_lung_lesion_bmvc)|
|**2023-11-24**|**MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**|Nathan Blake et.al.|[2311.14471v1](http://arxiv.org/abs/2311.14471v1)|null|
|**2023-11-21**|**Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**|Robert Gorwa et.al.|[2311.12573v3](http://arxiv.org/abs/2311.12573v3)|null|
|**2023-11-20**|**Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**|Muta Tah Hira et.al.|[2311.11932v1](http://arxiv.org/abs/2311.11932v1)|null|
|**2023-11-18**|**Representing visual classification as a linear combination of words**|Shobhit Agarwal et.al.|[2311.10933v1](http://arxiv.org/abs/2311.10933v1)|[link](https://github.com/lotterlab/task_word_explainability)|
|**2023-11-03**|**Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**|Emma A. M. Stanley et.al.|[2311.02115v2](http://arxiv.org/abs/2311.02115v2)|[link](https://github.com/estanley16/simba)|
|**2023-10-29**|**Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**|Adam White et.al.|[2310.19174v1](http://arxiv.org/abs/2310.19174v1)|null|
|**2023-10-03**|**Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**|Hossein Shreim et.al.|[2310.01828v2](http://arxiv.org/abs/2310.01828v2)|[link](https://github.com/geoaigroup/geoai-ecrs2023)|
|**2023-09-26**|**Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**|Muhammad Aurangzeb Ahmad et.al.|[2311.01463v1](http://arxiv.org/abs/2311.01463v1)|null|
|**2023-09-20**|**When to Trust AI: Advances and Challenges for Certification of Neural Networks**|Marta Kwiatkowska et.al.|[2309.11196v1](http://arxiv.org/abs/2309.11196v1)|null|
|**2023-09-19**|**Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**|Juan M. Garc√≠a-G√≥mez et.al.|[2309.10424v1](http://arxiv.org/abs/2309.10424v1)|null|
|**2023-09-19**|**QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**|Thanveer Shaik et.al.|[2309.10293v3](http://arxiv.org/abs/2309.10293v3)|null|
|**2023-09-18**|**Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**|Adarsa Sivaprasad et.al.|[2309.09917v1](http://arxiv.org/abs/2309.09917v1)|null|
|**2023-08-28**|**Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**|Yanjun Gao et.al.|[2308.14321v1](http://arxiv.org/abs/2308.14321v1)|null|
|**2023-08-18**|**Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**|Yun Xin Teoh et.al.|[2308.09380v1](http://arxiv.org/abs/2308.09380v1)|null|
|**2023-08-16**|**Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**|Munib Mesinovic et.al.|[2308.08407v1](http://arxiv.org/abs/2308.08407v1)|null|
|**2023-08-11**|**FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**|Karim Lekadir et.al.|[2309.12325v3](http://arxiv.org/abs/2309.12325v3)|null|
|**2023-08-10**|**Explainable AI applications in the Medical Domain: a systematic review**|Nicoletta Prentzas et.al.|[2308.05411v1](http://arxiv.org/abs/2308.05411v1)|null|
|**2023-08-01**|**Exploring the Role of Explainability in AI-Assisted Embryo Selection**|Lucia Urcelay et.al.|[2308.02534v1](http://arxiv.org/abs/2308.02534v1)|null|
|**2023-07-26**|**A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**|Timo Speith et.al.|[2307.14246v1](http://arxiv.org/abs/2307.14246v1)|null|
|**2023-07-26**|**Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**|Barnaby Crook et.al.|[2307.14239v1](http://arxiv.org/abs/2307.14239v1)|null|
|**2023-07-26**|**Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**|Henry Fraser et.al.|[2308.02047v1](http://arxiv.org/abs/2308.02047v1)|null|
|**2023-07-21**|**eXplainable Artificial Intelligence (XAI) in aging clock models**|Alena Kalyakulina et.al.|[2307.13704v3](http://arxiv.org/abs/2307.13704v3)|null|
|**2023-07-19**|**Interpreting and Correcting Medical Image Classification with PIP-Net**|Meike Nauta et.al.|[2307.10404v2](http://arxiv.org/abs/2307.10404v2)|[link](https://github.com/m-nauta/pipnet)|
|**2023-07-15**|**Explaining and visualizing black-box models through counterfactual paths**|Bastian Pfeifer et.al.|[2307.07764v3](http://arxiv.org/abs/2307.07764v3)|[link](https://github.com/pievos101/cpath)|
|**2023-07-05**|**Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**|Toygar Tanyel et.al.|[2307.02131v5](http://arxiv.org/abs/2307.02131v5)|[link](https://github.com/toygarr/counterfactual-explanations-for-medical-research)|
|**2023-06-30**|**AI and Non AI Assessments for Dementia**|Mahboobeh Parsapoor et.al.|[2307.01210v1](http://arxiv.org/abs/2307.01210v1)|null|
|**2023-06-12**|**Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**|Ruitao Xie et.al.|[2306.07306v1](http://arxiv.org/abs/2306.07306v1)|null|
|**2023-06-09**|**HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**|Rodrigo Agerri et.al.|[2306.06029v1](http://arxiv.org/abs/2306.06029v1)|null|
|**2023-06-07**|**XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**|Eli Laird et.al.|[2306.04791v1](http://arxiv.org/abs/2306.04791v1)|null|

#### Abstracts
##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºåÂ∑≤Á∂ìÂºïÈÄ≤ÂêÑÁ®ÆÊñπÊ≥ï‰æÜËß£Èáã„ÄåÈªëÁÆ±„ÄçAI Ê®°ÂûãÁöÑËº∏Âá∫„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâç‰∏¶‰∏çÊ∏ÖÊ•ö‰ΩøÁî®ËÄÖÊòØÂê¶ÂØ¶ÈöõÁêÜËß£Âíå‰ø°‰ªªÈÄô‰∫õËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºË©ï‰º∞ÁôåÁóáÈ¢®Èö™ÁöÑÂõûÊ≠∏Â∑•ÂÖ∑ÁöÑËß£ÈáãÔºå‰∏¶Êé¢Ë®éËß£ÈáãÁöÑÂÖßÂÆπÂíåÊ†ºÂºèÂ∞ç‰ª•‰ΩøÁî®ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÁêÜËß£Âíå‰ø°‰ªªÊåáÊ®ôÁöÑÂΩ±Èüø„ÄÇÈóúÊñºÂÖßÂÆπÔºåÊàëÂÄëÂØ¶È©ó‰∫ÜÂÖ©Á®ÆËß£ÈáãÊñπÊ≥ïÔºöÊµÅË°åÁöÑ SHAPÔºåÂü∫ÊñºÂçöÂºàË´ñÊ¶ÇÂøµÔºåÂõ†Ê≠§Â∞çÊñºÊó•Â∏∏‰ΩøÁî®ËÄÖ‰æÜË™™ÂèØËÉΩÂæàË§áÈõúÔºå‰ª•ÂèäÂü∫ÊñºÁâπÂæµÈÅÆËîΩÁöÑ occlusion-1ÔºåÂèØËÉΩÊõ¥ÊòìÊñºÁêÜËß£„ÄÇÈóúÊñºÊ†ºÂºèÔºåÊàëÂÄëÂ∞á SHAP Ëß£ÈáãÂëàÁèæÁÇ∫ÂúñË°® (SC)ÔºåÈÄôÊòØÊÖ£‰æãÔºåËÄåÂ∞á occlusion-1 Ëß£ÈáãÂëàÁèæÁÇ∫ÂúñË°® (OC) ‰ª•ÂèäÊñáÂ≠ó (OT)ÔºåÂÖ∂ËºÉÁÇ∫Á∞°ÂñÆÁöÑÊÄßË≥™‰πüÈÅ©Áî®ÊñºÊ≠§„ÄÇÈÄô‰∫õÂØ¶È©óÁ≠âÂêåÊñº‰ΩøÁî®ËÄÖÁ†îÁ©∂ÔºåË©¢ÂïèÂèÉËàáËÄÖÔºåÂÖ∑ÊúâÂÖ©Á®Æ‰∏çÂêåÁ®ãÂ∫¶ÁöÑÂ∞àÊ•≠Áü•Ë≠òÔºà‰∏ÄËà¨Ê∞ëÁúæÂíåÂÖ∑ÂÇô‰∏Ä‰∫õÈÜ´Â≠∏Ë®ìÁ∑¥ÁöÑ‰∫∫ÔºâÔºå‰ªñÂÄëÂ∞çÂõûÊ≠∏Â∑•ÂÖ∑Ëº∏Âá∫Ëß£ÈáãÁöÑ‰∏ªËßÄÂíåÂÆ¢ËßÄÁêÜËß£Âíå‰ø°‰ªª„ÄÇÂú®ÂÖ©È†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁôºÁèæÔºåÂú®Âü∫ÊñºÂÖßÂÆπÈÄ≤Ë°åÊØîËºÉÊôÇÔºå‰∏ÄËà¨‰æÜË™™Ôºåocclusion-1 ÂÑ™Êñº SHAP Ëß£ÈáãÔºåÂú®‰∏ªËßÄÁêÜËß£Âíå‰ø°‰ªªÊñπÈù¢ÊúâÊòéÈ°ØÁöÑÂÅèÂ•Ω„ÄÇÁÑ∂ËÄåÔºåÂú®ÂÉÖÊéßÂà∂Ê†ºÂºèÁöÑÊÉÖÊ≥Å‰∏ãÁõ¥Êé•ÊØîËºÉËß£ÈáãÔºåÂú®Â§ßÂ§öÊï∏ÊÉÖÊ≥Å‰∏ãÂè™È°ØÁ§∫ OT ÂÑ™Êñº SC Ëß£ÈáãÁöÑË≠âÊìöÔºåÈÄôË°®Êòé occlusion-1 ÂÑ™Êñº SHAP Ëß£ÈáãÁöÑ‰∏ªÂ∞éÂú∞‰ΩçÂèØËÉΩÊòØÁî±ÂÅèÂ•ΩÊñáÂ≠óËÄåÈùûÂúñË°®‰ΩúÁÇ∫Ëß£ÈáãÊâÄÈ©ÖÂãïÁöÑ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊ≤íÊúâÁôºÁèæËß£ÈáãÈ°ûÂûãÂú®ÂÆ¢ËßÄÁêÜËß£ÊñπÈù¢ÁöÑÂ∑ÆÁï∞Ë≠âÊìö„ÄÇÂõ†Ê≠§ÔºåÁ∏ΩÈ´îËÄåË®ÄÔºåÂ∞çËß£ÈáãÁöÑÂÖßÂÆπÂíåÊ†ºÂºèÁöÑÈÅ∏ÊìáÈúÄË¶Å‰ªîÁ¥∞Ê≥®ÊÑèÔºåÂõ†ÁÇ∫Âú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÊ†ºÂºèËÄåÈùûÂÖßÂÆπÔºåÂèØËÉΩÂú®ÊîπÂñÑ‰ΩøÁî®ËÄÖÈ´îÈ©óÊñπÈù¢ÁôºÊèÆÈóúÈçµ‰ΩúÁî®„ÄÇ</paragraph>

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro Li√≤, Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞Á™ÅÁ†¥Êèê‰æõ‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÈóúÊñºÁîüÁâ©ÈÜ´Â≠∏‰∏≠ LLM ÁöÑË™øÊü•ÈÄöÂ∏∏Â∞àÊ≥®ÊñºÁâπÂÆöÊáâÁî®ÊàñÊ®°ÂûãÊû∂ÊßãÔºåÁº∫‰πèÊï¥ÂêàÂêÑÁ®ÆÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÊúÄÊñ∞ÈÄ≤Â±ïÁöÑÂÖ®Èù¢ÂàÜÊûê„ÄÇÊú¨Á∂úËø∞Âü∫ÊñºÂ∞ç‰æÜËá™ PubMed„ÄÅWeb of Science Âíå arXiv Á≠âÊï∏ÊìöÂ∫´ÁöÑ 484 ÁØáÂá∫ÁâàÁâ©ÁöÑÂàÜÊûêÔºåÊ∑±ÂÖ•Êé¢Ë®é‰∫ÜÁîüÁâ©ÈÜ´Â≠∏‰∏≠ LLM ÁöÑÁï∂ÂâçÁèæÊ≥Å„ÄÅÊáâÁî®„ÄÅÊåëÊà∞ÂíåÂâçÊôØÔºåÂÖ∂ÁâπÈªûÊòØÈóúÊ≥®ÈÄô‰∫õÊ®°ÂûãÂú®ÁèæÂØ¶‰∏ñÁïåÁîüÁâ©ÈÜ´Â≠∏ËÉåÊôØ‰∏≠ÁöÑÂØ¶ÈöõÊáâÁî®„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®Âª£Ê≥õÁöÑÁîüÁâ©ÈÜ´Â≠∏‰ªªÂãô‰∏≠ÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÂåÖÊã¨Ë®∫Êñ∑ËºîÂä©„ÄÅËó•Áâ©ÁôºÁèæÂíåÂÄãÊÄßÂåñÈÜ´ÁôÇÁ≠âÔºå‰∏¶Âæû 137 È†ÖÈóúÈçµÁ†îÁ©∂‰∏≠Ê±≤ÂèñË¶ãËß£„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM ÁöÑÈÅ©ÊáâÁ≠ñÁï•ÔºåÂåÖÊã¨ÂñÆÊ®°ÊÖãÂíåÂ§öÊ®°ÊÖã LLM ÁöÑÂæÆË™øÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∑ÂÆÉÂÄëÂú®Èõ∂Ê¨°Â≠∏ÁøíÁÑ°Ê≥ïÂØ¶ÁèæÁöÑÂ∞àÊ•≠ÁîüÁâ©ÈÜ´Â≠∏ËÉåÊôØ‰∏≠ÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÈÜ´ÁôÇÂïèÈ°åËß£Á≠îÂíåÁîüÁâ©ÈÜ´Â≠∏ÊñáÁçªÁöÑÊúâÊïàËôïÁêÜ„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM Âú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÈù¢Ëá®ÁöÑÊåëÊà∞ÔºåÂåÖÊã¨Êï∏ÊìöÈö±ÁßÅÂïèÈ°å„ÄÅÊ®°ÂûãÂèØËß£ÈáãÊÄßÊúâÈôê„ÄÅÊï∏ÊìöÈõÜË≥™ÈáèÂïèÈ°å‰ª•ÂèäÁî±ÊñºÁîüÁâ©ÈÜ´Â≠∏Êï∏ÊìöÁöÑÊïèÊÑüÊÄß„ÄÅÂ∞çÈ´òÂ∫¶ÂèØÈù†Ê®°ÂûãËº∏Âá∫ÁöÑÈúÄÊ±Ç‰ª•ÂèäÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÈÉ®ÁΩ≤ AI ÁöÑÂÄ´ÁêÜÂΩ±ÈüøËÄåÁî¢ÁîüÁöÑÂÄ´ÁêÜÂïèÈ°å„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÈÇÑÁ¢∫ÂÆö‰∫ÜÁîüÁâ©ÈÜ´Â≠∏‰∏≠ LLM Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÂåÖÊã¨Áî®Êñº‰øùË≠∑Êï∏ÊìöÈö±ÁßÅÁöÑËÅØÂêàÂ≠∏ÁøíÊñπÊ≥ï‰ª•ÂèäÊï¥ÂêàÂèØËß£Èáã AI ÊñπÊ≥ï‰ª•Â¢ûÂº∑ LLM ÁöÑÈÄèÊòéÂ∫¶„ÄÇ

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂú®ÈÜ´ÁôÇÂíå‰øùÂÅ•ÊáâÁî®‰∏≠ÊäïÂÖ•‰∫ÜÂ§ßÈáèÁöÑÊäïË≥áÂíåÈñãÁôºÔºåÈÄ≤ËÄåÂ∞éËá¥ÈÜ´ÁôÇÊäÄË°ì‰∏≠ÁöÑÂÖàÈÄ≤ÊéßÂà∂Á≥ªÁµ±„ÄÇÁÑ∂ËÄåÔºåAI Á≥ªÁµ±ÁöÑ‰∏çÈÄèÊòéÊÄßÂºïÁôº‰∫ÜÂ∞çÊ≠§È°ûÊïèÊÑüÊáâÁî®‰∏≠ÊâÄÈúÄÂü∫Êú¨ÁâπÊÄßÁöÑÊìîÊÜÇÔºå‰æãÂ¶ÇÈÄèÊòéÂ∫¶ÂíåÂèØ‰ø°Â∫¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈÄèÈÅéË™øÊü•‰∏ÄÂÄãÁ®ãÂ∫è‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÁî®ÊñºÈÅ∏ÊìáÊúÄÂÖÖÂàÜÁöÑÂèØËß£Èáã AIÔºàXAIÔºâÊñπÊ≥ïÔºå‰ª•Á¨¶ÂêàÊ≠êÁõüÊ≥ïË¶èÂú®ÈÜ´ÁôÇÂô®ÊùêÁöÑÊô∫ÊÖßÂûãÁîüÁâ©ÈõªÂ≠êÂ≠∏‰∏≠ÁöÑË™™ÊòéË¶ÅÊ±Ç„ÄÇÊé°Áî®ÁöÑÊñπÊ≥ïÂæûÈÄèÈÅéÂÖ∂ÊéßÂà∂Ê©üÂà∂ÔºàÈñãËø¥Ë∑Ø„ÄÅÈñâËø¥Ë∑ØÂíåÂçäÈñâËø¥Ë∑ØÁ≥ªÁµ±ÔºâÂ∞çÊô∫ÊÖßÂûãË£ùÁΩÆÈÄ≤Ë°åÂàÜÈ°ûÔºå‰∏¶Ê∑±ÂÖ•Êé¢Ë®éÂÖ∂ÊäÄË°ìÈñãÂßã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂàÜÊûêÈÄô‰∫õÊ≥ïË¶è‰ª•ÂÆöÁæ©ÂÖ∂Â∞çÂêÑÁ®ÆË£ùÁΩÆÂíåÁõ∏ÈóúÁõÆÊ®ôÁöÑÂèØËß£ÈáãÊÄßË¶ÅÊ±Ç„ÄÇÂêåÊôÇÔºåÊàëÂÄëÈÄèÈÅéÂÖ∂Ë™™ÊòéÁõÆÊ®ôÂ∞ç XAI ÊñπÊ≥ïÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÂÖÅË®±Â∞áÊ≥ïÂæãÂèØËß£ÈáãÊÄßË¶ÅÊ±ÇËàá XAI Ë™™ÊòéÁõÆÊ®ôÁõ∏ÂåπÈÖçÔºå‰∏¶Á¢∫ÂÆöÈÅ©Áï∂ÁöÑ XAI ÊºîÁÆóÊ≥ï‰æÜÈÅîÊàêÂÆÉÂÄë„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊèê‰æõ‰∫ÜÂ∞çÂì™‰∫õ XAI ÊºîÁÆóÊ≥ïÊõ¥Á¨¶ÂêàÊ≠êÁõüÊ≥ïË¶è‰ª•ÈÅ©Áî®Êñº‰∏çÂêåÈ°ûÂûãÁöÑÈÜ´ÁôÇÂô®ÊùêÁöÑÁ¥∞Á∑ªÁêÜËß£„ÄÇÊàëÂÄëÈÄèÈÅé‰∏çÂêåÁ•ûÁ∂ìÊ§çÂÖ•Áâ©ÁöÑÂØ¶ÈöõÊ°à‰æãÁ†îÁ©∂‰æÜË≠âÊòéÈÄô‰∏ÄÈªûÔºåÂæûÊÖ¢ÊÄßÁñæÁóÖÁÆ°ÁêÜÂà∞ÂÖàÈÄ≤ÁöÑÁæ©ËÇ¢„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Â°´Ë£ú‰∫ÜÂ∞áÁîüÁâ©ÈõªÂ≠êÂ≠∏‰∏≠ÁöÑ XAI ÊáâÁî®ËàáÊ≠êÁõüÊ≥ïË¶èÁöÑÂö¥Ê†ºË¶èÂÆöÁõ∏Á¨¶ÁöÑÈáçË¶ÅÁ©∫ÁôΩ„ÄÇÂÆÉÁÇ∫ÈñãÁôº‰∫∫Âì°ÂíåÁ†îÁ©∂‰∫∫Âì°Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂØ¶Áî®ÁöÑÊû∂ÊßãÔºåÁ¢∫‰øùÂÖ∂ AI ÂâµÊñ∞ËÉΩ‰øÉÈÄ≤ÈÜ´ÁôÇÊäÄË°ì‰∏¶ÈÅµÂÆàÊ≥ïÂæãÂíåÈÅìÂæ∑Ê®ôÊ∫ñ„ÄÇ

##### **Towards Case-based Interpretability for Medical Federated Learning**
2408.13626v1 by Laura Latorre, Liliana Petrychenko, Regina Beets-Tan, Taisiya Kopytova, Wilson Silva

We explore deep generative models to generate case-based explanations in a
medical federated learning setting. Explaining AI model decisions through
case-based interpretability is paramount to increasing trust and allowing
widespread adoption of AI in clinical practice. However, medical AI training
paradigms are shifting towards federated learning settings in order to comply
with data protection regulations. In a federated scenario, past data is
inaccessible to the current user. Thus, we use a deep generative model to
generate synthetic examples that protect privacy and explain decisions. Our
proof-of-concept focuses on pleural effusion diagnosis and uses publicly
available Chest X-ray data.

ÊëòË¶ÅÔºöÊàëÂÄëÊé¢Á¥¢Ê∑±Â∫¶ÁîüÊàêÊ®°ÂûãÔºåÂú®ÈÜ´ÁôÇËÅØÈÇ¶Â≠∏ÁøíË®≠ÁΩÆ‰∏≠ÁîüÊàêÂü∫ÊñºÊ°à‰æãÁöÑË™™Êòé„ÄÇÈÄèÈÅéÂü∫ÊñºÊ°à‰æãÁöÑÂèØËß£ÈáãÊÄß‰æÜËß£Èáã AI Ê®°ÂûãÊ±∫Á≠ñÔºåÂ∞çÊñºÂ¢ûÂä†‰ø°‰ªª‰∏¶ÂÖÅË®± AI Âú®Ëá®Â∫äÂØ¶Âãô‰∏≠Âª£Ê≥õÊé°Áî®Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÜ´ÁôÇ AI Ë®ìÁ∑¥ÁØÑ‰æãÊ≠£ËΩâÂêëËÅØÈÇ¶Â≠∏ÁøíË®≠ÁΩÆÔºå‰ª•Á¨¶ÂêàË≥áÊñô‰øùË≠∑Ê≥ïË¶è„ÄÇÂú®ËÅØÈÇ¶ÊÉÖÂ¢É‰∏≠ÔºåÈÅéÂéªÁöÑË≥áÊñôÂ∞çÁõÆÂâçÁöÑ‰ΩøÁî®ËÄÖËÄåË®ÄÊòØÁÑ°Ê≥ïÂèñÂæóÁöÑ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰ΩøÁî®Ê∑±Â∫¶ÁîüÊàêÊ®°Âûã‰æÜÁî¢Áîü‰øùË≠∑Èö±ÁßÅÂíåËß£ÈáãÊ±∫Á≠ñÁöÑÂêàÊàêÁØÑ‰æã„ÄÇÊàëÂÄëÁöÑÊ¶ÇÂøµÈ©óË≠âËëóÈáçÊñºËÉ∏ËÖîÁ©çÊ∂≤Ë®∫Êñ∑Ôºå‰∏¶‰ΩøÁî®ÂÖ¨ÈñãÂèØÂèñÂæóÁöÑËÉ∏ÈÉ® X ÂÖâË≥áÊñô„ÄÇ

##### **AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**
2408.12491v1 by Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Gru√ºhagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans

Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging
lesions with variable clinical behaviours and treatment approaches. This
systematic review provides an overview of Artificial Intelligence (AI) methods
using radiological imaging for diagnosis and prognosis of these tumours,
highlighting challenges in clinical translation, and evaluating study alignment
with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI
international consensus guidelines for trustworthy and deployable AI to promote
the clinical translation of AI methods. The review covered literature from
several bibliographic databases, including papers published before 17/07/2024.
Original research in peer-reviewed journals focused on radiology-based AI for
diagnosing or prognosing primary STBT was included. Exclusion criteria were
animal, cadaveric, or laboratory studies, and non-English papers. Abstracts
were screened by two of three independent reviewers for eligibility. Eligible
papers were assessed against guidelines by one of three independent reviewers.
The search identified 15,015 abstracts, from which 325 articles were included
for evaluation. Most studies performed moderately on CLAIM, averaging a score
of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out
of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,
indicating significant room for improvement. Future efforts by AI developers
should focus on design (e.g. define unmet clinical need, intended clinical
setting and how AI would be integrated in clinical workflow), development (e.g.
build on previous work, explainability), evaluation (e.g. evaluating and
addressing biases, evaluating AI against best practices), and data
reproducibility and availability (making documented code and data publicly
available). Following these recommendations could improve clinical translation
of AI methods.

ÊëòË¶ÅÔºöËªüÁµÑÁπîÂíåÈ™®È™ºËÖ´Áò§ÔºàSTBTÔºâÊòØÁΩïË¶ã„ÄÅË®∫Êñ∑ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁóÖÁÅ∂ÔºåÂÖ∂Ëá®Â∫äË°åÁÇ∫ÂíåÊ≤ªÁôÇÊñπÊ≥ïÂêÑ‰∏çÁõ∏Âêå„ÄÇÈÄôÁØáÁ≥ªÁµ±ÊÄßÂõûÈ°ßÊèê‰æõ‰∫Ü‰ΩøÁî®ÊîæÂ∞ÑÂΩ±ÂÉèÈÄ≤Ë°åË®∫Êñ∑ÂíåÈ†êÂæåÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÁöÑÊ¶ÇËßÄÔºåÈáçÈªûË™™Êòé‰∫ÜËá®Â∫äËΩâË≠ØÁöÑÊåëÊà∞Ôºå‰∏¶Ë©ï‰º∞Á†îÁ©∂ËàáÈÜ´ÁôÇÂΩ±ÂÉè AI Ê†∏Êü•Ë°® (CLAIM) Âíå FUTURE-AI ÂèØ‰ø°Ë≥¥‰∏îÂèØÈÉ®ÁΩ≤ AI ÁöÑÂúãÈöõÂÖ±Ë≠òÊ∫ñÂâáÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ª•‰øÉÈÄ≤ AI ÊñπÊ≥ïÁöÑËá®Â∫äËΩâË≠Ø„ÄÇÈÄôÁØáÂõûÈ°ßÊ∂µËìã‰∫ÜÂπæÂÄãÊõ∏ÁõÆË≥áÊñôÂ∫´‰∏≠ÁöÑÊñáÁçªÔºåÂåÖÊã¨Âú® 2024 Âπ¥ 7 Êúà 17 Êó•‰πãÂâçÁôºË°®ÁöÑË´ñÊñá„ÄÇÁ¥çÂÖ•‰∫Ü‰ª•ÊîæÂ∞ÑÁÇ∫Âü∫Á§éÁöÑ AI Ë®∫Êñ∑ÊàñÈ†êÂæåÂéüÁôºÊÄß STBT ÁöÑÂêåË°åË©ïÂØ©ÊúüÂàä‰∏≠ÁöÑÂéüÂßãÁ†îÁ©∂„ÄÇÊéíÈô§Ê®ôÊ∫ñÊòØÂãïÁâ©„ÄÅÂ±çÈ´îÊàñÂØ¶È©óÂÆ§Á†îÁ©∂Ôºå‰ª•ÂèäÈùûËã±ÊñáË´ñÊñá„ÄÇÊëòË¶ÅÁî±‰∏â‰ΩçÁç®Á´ãÂØ©Êü•Âì°‰∏≠ÁöÑÂÖ©‰ΩçÁØ©ÈÅ∏Ë≥áÊ†º„ÄÇÂêàÊ†ºÁöÑË´ñÊñáÁî±‰∏â‰ΩçÁç®Á´ãÂØ©Êü•Âì°‰∏≠ÁöÑ‰∏Ä‰ΩçÊ†πÊìöÊ∫ñÂâáÈÄ≤Ë°åË©ï‰º∞„ÄÇÊêúÁ¥¢Ë≠òÂà•Âá∫ 15,015 ÁØáÊëòË¶ÅÔºåÂÖ∂‰∏≠ 325 ÁØáÊñáÁ´†Ë¢´Á¥çÂÖ•Ë©ï‰º∞„ÄÇÂ§ßÂ§öÊï∏Á†îÁ©∂Âú® CLAIM ‰∏≠Ë°®Áèæ‰∏≠Á≠âÔºåÂπ≥ÂùáÂæóÂàÜÁÇ∫ 53 ÂàÜ‰∏≠ÁöÑ 28.9¬±7.5 ÂàÜÔºå‰ΩÜÂú® FUTURE-AI ‰∏≠Ë°®Áèæ‰∏ç‰Ω≥ÔºåÂπ≥ÂùáÂæóÂàÜÁÇ∫ 30 ÂàÜ‰∏≠ÁöÑ 5.1¬±2.1 ÂàÜ„ÄÇSTBT ÁöÑÂΩ±ÂÉè AI Â∑•ÂÖ∑‰ªçËôïÊñºÊ¶ÇÂøµÈ©óË≠âÈöéÊÆµÔºåË°®ÊòéÊúâÈ°ØËëóÁöÑÊîπÈÄ≤Á©∫Èñì„ÄÇAI ÈñãÁôº‰∫∫Âì°Êú™‰æÜÁöÑÂä™ÂäõÊáâÈõÜ‰∏≠Âú®Ë®≠Ë®àÔºà‰æãÂ¶ÇÂÆöÁæ©Êú™ÊªøË∂≥ÁöÑËá®Â∫äÈúÄÊ±Ç„ÄÅÈ†êÊúüÁöÑËá®Â∫äÁí∞Â¢É‰ª•Âèä AI Â¶Ç‰ΩïÊï¥ÂêàÂà∞Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã‰∏≠Ôºâ„ÄÅÈñãÁôºÔºà‰æãÂ¶ÇÂª∫Á´ãÂú®ÂÖàÂâçÁöÑÂ∑•‰Ωú„ÄÅÂèØËß£ÈáãÊÄßÔºâ„ÄÅË©ï‰º∞Ôºà‰æãÂ¶ÇË©ï‰º∞ÂíåËß£Ê±∫ÂÅèÂ∑Æ„ÄÅË©ï‰º∞ AI ËàáÊúÄ‰Ω≥ÂØ¶ÂãôÔºâ„ÄÅ‰ª•ÂèäÊï∏ÊìöÂèØË§áË£ΩÊÄßÂíåÂèØÁî®ÊÄßÔºàÂÖ¨ÈñãÊèê‰æõÊñá‰ª∂ÂåñÁöÑ‰ª£Á¢ºÂíåÊï∏ÊìöÔºâ„ÄÇÈÅµÂæ™ÈÄô‰∫õÂª∫Ë≠∞ÂèØ‰ª•ÊîπÂñÑ AI ÊñπÊ≥ïÁöÑËá®Â∫äËΩâË≠Ø„ÄÇ

##### **Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy**
2409.00001v1 by Kimji N. Pellano, Inga Str√ºmke, Daniel Groos, Lars Adde, Espen Alexander F. Ihlen

Early detection of Cerebral Palsy (CP) is crucial for effective intervention
and monitoring. This paper tests the reliability and applicability of
Explainable AI (XAI) methods using a deep learning method that predicts CP by
analyzing skeletal data extracted from video recordings of infant movements.
Specifically, we use XAI evaluation metrics -- namely faithfulness and
stability -- to quantitatively assess the reliability of Class Activation
Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this
specific medical application. We utilize a unique dataset of infant movements
and apply skeleton data perturbations without distorting the original dynamics
of the infant movements. Our CP prediction model utilizes an ensemble approach,
so we evaluate the XAI metrics performances for both the overall ensemble and
the individual models. Our findings indicate that both XAI methods effectively
identify key body points influencing CP predictions and that the explanations
are robust against minor data perturbations. Grad-CAM significantly outperforms
CAM in the RISv metric, which measures stability in terms of velocity. In
contrast, CAM performs better in the RISb metric, which relates to bone
stability, and the RRS metric, which assesses internal representation
robustness. Individual models within the ensemble show varied results, and
neither CAM nor Grad-CAM consistently outperform the other, with the ensemble
approach providing a representation of outcomes from its constituent models.

ÊëòË¶ÅÔºöËÖ¶ÊÄßÈ∫ªÁó∫ (CP) ÁöÑÊó©ÊúüÂÅµÊ∏¨Â∞çÊñºÊúâÊïàÁöÑ‰ªãÂÖ•ÂíåÁõ£Ê∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨ÊñáÊ∏¨Ë©¶‰∫ÜÂèØËß£Èáã AI (XAI) ÊñπÊ≥ïÁöÑÂèØÈù†ÊÄßÂíåÈÅ©Áî®ÊÄßÔºå‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÈÄèÈÅéÂàÜÊûêÂæûÂ¨∞ÂÖíÂãï‰ΩúÂΩ±ÁâáË®òÈåÑ‰∏≠ÊèêÂèñÁöÑÈ™®È™ºË≥áÊñô‰æÜÈ†êÊ∏¨ CP„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ΩøÁî® XAI Ë©ï‰º∞ÊåáÊ®ôÔºàÂç≥Âø†ÂØ¶Â∫¶ÂíåÁ©©ÂÆöÊÄßÔºâ‰æÜÈáèÂåñË©ï‰º∞È°ûÂà•ÊøÄÊ¥ªÊò†Â∞Ñ (CAM) ÂíåÊ¢ØÂ∫¶Âä†Ê¨äÈ°ûÂà•ÊøÄÊ¥ªÊò†Â∞Ñ (Grad-CAM) Âú®ÈÄôÂÄãÁâπÂÆöÈÜ´ÁôÇÊáâÁî®‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÂà©Áî®‰∏ÄÂÄãÁç®ÁâπÁöÑÂ¨∞ÂÖíÂãï‰ΩúË≥áÊñôÈõÜÔºå‰∏¶ÊáâÁî®È™®È™ºË≥áÊñôÊìæÂãïÔºåËÄå‰∏çÊúÉÊâ≠Êõ≤Â¨∞ÂÖíÂãï‰ΩúÁöÑÂéüÂßãÂãïÂäõ„ÄÇÊàëÂÄëÁöÑ CP È†êÊ∏¨Ê®°ÂûãÂà©Áî®Êï¥È´îÊñπÊ≥ïÔºåÂõ†Ê≠§ÊàëÂÄëË©ï‰º∞‰∫ÜÊï¥È´îÊï¥È´îÂíåÂÄãÂà•Ê®°ÂûãÁöÑ XAI ÊåáÊ®ôË°®Áèæ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂÖ©Á®Æ XAI ÊñπÊ≥ïÈÉΩËÉΩÊúâÊïàË≠òÂà•ÂΩ±Èüø CP È†êÊ∏¨ÁöÑÈóúÈçµË∫´È´îÈÉ®‰ΩçÔºå‰∏¶‰∏îÈÄô‰∫õËß£ÈáãÂ∞çÊñºÂæÆÂ∞èÁöÑË≥áÊñôÊìæÂãïÂÖ∑ÊúâÈ≠ØÊ£íÊÄß„ÄÇGrad-CAM Âú® RISv ÊåáÊ®ô‰∏≠È°ØËëóÂÑ™Êñº CAMÔºåË©≤ÊåáÊ®ôË°°ÈáèÈÄüÂ∫¶ÊñπÈù¢ÁöÑÁ©©ÂÆöÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåCAM Âú® RISb ÊåáÊ®ô‰∏≠Ë°®ÁèæÂæóÊõ¥Â•ΩÔºåË©≤ÊåáÊ®ôËàáÈ™®È™ºÁ©©ÂÆöÊÄßÊúâÈóúÔºåËÄå RRS ÊåáÊ®ôÂâáË©ï‰º∞ÂÖßÈÉ®Ë°®Á§∫ÁöÑÈ≠ØÊ£íÊÄß„ÄÇÊï¥È´î‰∏≠ÁöÑÂÄãÂà•Ê®°ÂûãÈ°ØÁ§∫Âá∫‰∏çÂêåÁöÑÁµêÊûúÔºåCAM Âíå Grad-CAM ÈÉΩ‰∏ç‰∏ÄËá¥Âú∞ÂÑ™ÊñºÂè¶‰∏ÄÁ®ÆÔºåÊï¥È´îÊñπÊ≥ïÊèê‰æõ‰∫ÜÂÖ∂ÁµÑÊàêÊ®°ÂûãÁµêÊûúÁöÑË°®Á§∫„ÄÇ

##### **MicroXercise: A Micro-Level Comparative and Explainable System for Remote Physical Therapy**
2408.11837v1 by Hanchen David Wang, Nibraas Khan, Anna Chen, Nilanjan Sarkar, Pamela Wisniewski, Meiyi Ma

Recent global estimates suggest that as many as 2.41 billion individuals have
health conditions that would benefit from rehabilitation services. Home-based
Physical Therapy (PT) faces significant challenges in providing interactive
feedback and meaningful observation for therapists and patients. To fill this
gap, we present MicroXercise, which integrates micro-motion analysis with
wearable sensors, providing therapists and patients with a comprehensive
feedback interface, including video, text, and scores. Crucially, it employs
multi-dimensional Dynamic Time Warping (DTW) and attribution-based explainable
methods to analyze the existing deep learning neural networks in monitoring
exercises, focusing on a high granularity of exercise. This synergistic
approach is pivotal, providing output matching the input size to precisely
highlight critical subtleties and movements in PT, thus transforming complex AI
analysis into clear, actionable feedback. By highlighting these micro-motions
in different metrics, such as stability and range of motion, MicroXercise
significantly enhances the understanding and relevance of feedback for
end-users. Comparative performance metrics underscore its effectiveness over
traditional methods, such as a 39% and 42% improvement in Feature Mutual
Information (FMI) and Continuity. MicroXercise is a step ahead in home-based
physical therapy, providing a technologically advanced and intuitively helpful
solution to enhance patient care and outcomes.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÂÖ®ÁêÉ‰º∞Ë®àË°®ÊòéÔºåÂ§öÈÅî 24.1 ÂÑÑ‰∫∫Êúâ
ÂÅ•Â∫∑ÁãÄÊ≥ÅÂèØÂæûÂæ©ÂÅ•ÊúçÂãô‰∏≠ÂèóÁõä„ÄÇÂ±ÖÂÆ∂
Áâ©ÁêÜÊ≤ªÁôÇ (PT) Âú®Êèê‰æõ‰∫íÂãïÂºè
ÂõûÈ•ãÂíåÊúâÊÑèÁæ©ÁöÑËßÄÂØüÊñπÈù¢Èù¢Ëá®ÈáçÂ§ßÊåëÊà∞Ôºå‰æõÊ≤ªÁôÇÂ∏´ÂíåÊÇ£ËÄÖ‰ΩøÁî®„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄô
ÂÄãÁº∫Âè£ÔºåÊàëÂÄëÊèêÂá∫ MicroXerciseÔºåÂÆÉÂ∞áÂæÆÂãï‰ΩúÂàÜÊûêËàá
ÂèØÁ©øÊà¥ÂºèÊÑüÊ∏¨Âô®Êï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÁÇ∫Ê≤ªÁôÇÂ∏´ÂíåÊÇ£ËÄÖÊèê‰æõ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑ
ÂõûÈ•ã‰ªãÈù¢ÔºåÂåÖÊã¨ÂΩ±Áâá„ÄÅÊñáÂ≠óÂíåÂàÜÊï∏„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÂÆÉÊé°Áî®
Â§öÁ∂≠ÂãïÊÖãÊôÇÈñìË¶èÊï¥ (DTW) ÂíåÂü∫ÊñºÊ≠∏Âõ†ÁöÑÂèØËß£Èáã
ÊñπÊ≥ï‰æÜÂàÜÊûêÁõ£ÊéßÈÅãÂãï‰∏≠ÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂ∞àÊ≥®ÊñºÈÅãÂãïÁöÑÈ´òÁ≤íÂ∫¶„ÄÇÈÄôÁ®ÆÂçîÂêå
ÊñπÊ≥ïËá≥ÈóúÈáçË¶ÅÔºåÊèê‰æõËàáËº∏ÂÖ•Â§ßÂ∞èÂåπÈÖçÁöÑËº∏Âá∫Ôºå‰ª•Á≤æÁ¢∫Âú∞
Á™ÅÂá∫ PT ‰∏≠ÈóúÈçµÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•ÂíåÂãï‰ΩúÔºåÂæûËÄåÂ∞áË§áÈõúÁöÑ AI
ÂàÜÊûêËΩâÊèõÁÇ∫Ê∏ÖÊô∞„ÄÅÂèØÊìç‰ΩúÁöÑÂõûÈ•ã„ÄÇÈÄèÈÅéÂú®‰∏çÂêåÊåáÊ®ô‰∏≠Á™ÅÈ°ØÈÄô‰∫õÂæÆÂãï‰ΩúÔºå‰æãÂ¶ÇÁ©©ÂÆöÊÄßÂíåÂãï‰ΩúÁØÑÂúçÔºåMicroXercise
È°ØËëóÊèêÂçáÊúÄÁµÇ‰ΩøÁî®ËÄÖÂ∞çÂõûÈ•ãÁöÑÁêÜËß£ÂíåÁõ∏ÈóúÊÄß„ÄÇÊØîËºÉÊïàËÉΩÊåáÊ®ôÂº∑Ë™øÂÖ∂ÂÑ™Êñº
ÂÇ≥Áµ±ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºå‰æãÂ¶ÇÁâπÂæµ‰∫íÊÉ†Ë≥áË®ä (FMI) ÂíåÈÄ£Á∫åÊÄßÂàÜÂà•ÊèêÂçá‰∫Ü 39% Âíå 42%„ÄÇMicroXercise Âú®Â±ÖÂÆ∂
Áâ©ÁêÜÊ≤ªÁôÇÊñπÈù¢Êõ¥ÈÄ≤‰∏ÄÊ≠•ÔºåÊèê‰æõÊäÄË°ìÂÖàÈÄ≤‰∏îÁõ¥Ë¶∫ÊúâÁî®ÁöÑ
Ëß£Ê±∫ÊñπÊ°àÔºå‰ª•ÊèêÂçáÊÇ£ËÄÖÁÖßË≠∑ÂíåÁµêÊûú„ÄÇ

##### **The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development**
2408.05239v1 by Joshua Morriss, Tod Brindle, Jessica Bah R√∂sman, Daniel Reibsamen, Andreas Enz

Systematic literature reviews are the highest quality of evidence in
research. However, the review process is hindered by significant resource and
data constraints. The Literature Review Network (LRN) is the first of its kind
explainable AI platform adhering to PRISMA 2020 standards, designed to automate
the entire literature review process. LRN was evaluated in the domain of
surgical glove practices using 3 search strings developed by experts to query
PubMed. A non-expert trained all LRN models. Performance was benchmarked
against an expert manual review. Explainability and performance metrics
assessed LRN's ability to replicate the experts' review. Concordance was
measured with the Jaccard index and confusion matrices. Researchers were
blinded to the other's results until study completion. Overlapping studies were
integrated into an LRN-generated systematic review. LRN models demonstrated
superior classification accuracy without expert training, achieving 84.78% and
85.71% accuracy. The highest performance model achieved high interrater
reliability (k = 0.4953) and explainability metrics, linking 'reduce',
'accident', and 'sharp' with 'double-gloving'. Another LRN model covered 91.51%
of the relevant literature despite diverging from the non-expert's judgments (k
= 0.2174), with the terms 'latex', 'double' (gloves), and 'indication'. LRN
outperformed the manual review (19,920 minutes over 11 months), reducing the
entire process to 288.6 minutes over 5 days. This study demonstrates that
explainable AI does not require expert training to successfully conduct
PRISMA-compliant systematic literature reviews like an expert. LRN summarized
the results of surgical glove studies and identified themes that were nearly
identical to the clinical researchers' findings. Explainable AI can accurately
expedite our understanding of clinical practices, potentially revolutionizing
healthcare research.

ÊëòË¶ÅÔºöÁ≥ªÁµ±ÊÄßÊñáÁçªÂõûÈ°ßÊòØÁ†îÁ©∂‰∏≠Ë≠âÊìöÂìÅË≥™ÊúÄÈ´òÁöÑ„ÄÇÁÑ∂ËÄåÔºåÂõûÈ°ßÈÅéÁ®ãÂèóÂà∞È°ØËëóË≥áÊ∫êÂíåË≥áÊñôÈôêÂà∂ÁöÑÈòªÁ§ô„ÄÇÊñáÁçªÂõûÈ°ßÁ∂≤Ë∑Ø (LRN) ÊòØÁ¨¨‰∏ÄÂÄãÈÅµÂæ™ PRISMA 2020 Ê®ôÊ∫ñÁöÑÂèØËß£Èáã AI Âπ≥Âè∞ÔºåÊó®Âú®Ëá™ÂãïÂåñÊï¥ÂÄãÊñáÁçªÂõûÈ°ßÈÅéÁ®ã„ÄÇLRN Âú®Â§ñÁßëÊâãÂ•óÂØ¶ÂãôÈ†òÂüü‰∏≠ÈÄ≤Ë°åË©ï‰º∞Ôºå‰ΩøÁî®Â∞àÂÆ∂ÈñãÁôºÁöÑ 3 ÂÄãÊêúÂ∞ãÂ≠ó‰∏≤‰æÜÊü•Ë©¢ PubMed„ÄÇÈùûÂ∞àÂÆ∂Ë®ìÁ∑¥ÊâÄÊúâ LRN Ê®°Âûã„ÄÇÊïàËÉΩ‰ª•Â∞àÂÆ∂ÊâãÂãïÂõûÈ°ß‰ΩúÁÇ∫Âü∫Ê∫ñ„ÄÇÂèØËß£ÈáãÊÄßÂíåÊïàËÉΩÊåáÊ®ôË©ï‰º∞ LRN Ë§áË£ΩÂ∞àÂÆ∂ÂõûÈ°ßÁöÑËÉΩÂäõ„ÄÇ‰∏ÄËá¥ÊÄß‰ª• Jaccard ÊåáÊï∏ÂíåÊ∑∑Ê∑ÜÁü©Èô£Ê∏¨Èáè„ÄÇÁ†îÁ©∂‰∫∫Âì°Âú®Á†îÁ©∂ÂÆåÊàêÂâçÂ∞çÂΩºÊ≠§ÁöÑÁµêÊûú‰øùÂØÜ„ÄÇÈáçÁñäÁöÑÁ†îÁ©∂Êï¥ÂêàÂà∞ LRN ÁîüÊàêÁöÑÁ≥ªÁµ±ÊÄßÂõûÈ°ß‰∏≠„ÄÇLRN Ê®°ÂûãÂú®Ê≤íÊúâÂ∞àÂÆ∂Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫ÁéáÔºåÈÅîÂà∞ 84.78% Âíå 85.71% ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÊïàËÉΩÊúÄÈ´òÁöÑÊ®°ÂûãÈÅîÂà∞‰∫ÜÈ´òË©ïÂàÜËÄÖÈñì‰ø°Ë≥¥Â∫¶ (k = 0.4953) ÂíåÂèØËß£ÈáãÊÄßÊåáÊ®ôÔºåÂ∞á„ÄåÊ∏õÂ∞ë„Äç„ÄÅ„ÄåÊÑèÂ§ñ„ÄçÂíå„ÄåÈä≥Âà©„ÄçËàá„ÄåÈõôÈáçÊà¥ÊâãÂ•ó„ÄçÈÄ£ÁµêÂú®‰∏ÄËµ∑„ÄÇÂè¶‰∏ÄÂÄã LRN Ê®°ÂûãÊ∂µËìã‰∫Ü 91.51% ÁöÑÁõ∏ÈóúÊñáÁçªÔºåÂÑòÁÆ°ËàáÈùûÂ∞àÂÆ∂ÁöÑÂà§Êñ∑‰∏çÂêå (k = 0.2174)Ôºå‰ΩÜÂåÖÂê´‰∫Ü„Äå‰π≥ËÜ†„Äç„ÄÅ„ÄåÈõôÈáç„ÄçÔºàÊâãÂ•óÔºâÂíå„ÄåÈÅ©ÊáâÁóá„ÄçÁ≠âË©ûÂΩô„ÄÇLRN ÂÑ™ÊñºÊâãÂãïÂõûÈ°ßÔºà11 ÂÄãÊúàË∂ÖÈÅé 19,920 ÂàÜÈêòÔºâÔºåÂ∞áÊï¥ÂÄãÈÅéÁ®ãÁ∏ÆÁü≠ÁÇ∫ 5 Â§©Ë∂ÖÈÅé 288.6 ÂàÜÈêò„ÄÇÈÄôÈ†ÖÁ†îÁ©∂È°ØÁ§∫ÔºåÂèØËß£ÈáãÁöÑ AI ‰∏çÈúÄË¶ÅÂ∞àÂÆ∂Ë®ìÁ∑¥Âç≥ÂèØÊàêÂäüÈÄ≤Ë°åÂ∞àÂÆ∂Á≠âÁ¥öÁöÑ PRISMA Áõ∏ÂÆπÁ≥ªÁµ±ÊÄßÊñáÁçªÂõûÈ°ß„ÄÇLRN Á∏ΩÁµê‰∫ÜÂ§ñÁßëÊâãÂ•óÁ†îÁ©∂ÁöÑÁµêÊûúÔºå‰∏¶ÊâæÂá∫ËàáËá®Â∫äÁ†îÁ©∂‰∫∫Âì°ÁôºÁèæÂπæ‰πéÁõ∏ÂêåÁöÑ‰∏ªÈ¢ò„ÄÇÂèØËß£ÈáãÁöÑ AI ÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Âä†Âø´ÊàëÂÄëÂ∞çËá®Â∫äÂØ¶ÂãôÁöÑÁêÜËß£ÔºåÊúâÊΩõÂäõÈù©Êñ∞ÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂„ÄÇ

##### **Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns**
2408.02709v1 by Chi Him Ng

This study analyzes hybrid AI systems' design patterns and their
effectiveness in clinical decision-making using the boxology framework. It
categorizes and copares various architectures combining machine learning and
rule-based reasoning to provide insights into their structural foundations and
healthcare applications. Addressing two main questions, how to categorize these
systems againts established design patterns and how to extract insights through
comparative analysis, the study uses design patterns from software engineering
to understand and optimize healthcare AI systems. Boxology helps identify
commonalities and create reusable solutions, enhancing these systems'
scalability, reliability, and performance. Five primary architectures are
examined: REML, MLRB, RBML, RMLT, and PERML. Each has unique strengths and
weaknesses, highlighting the need for tailored approaches in clinical tasks.
REML excels in high-accuracy prediction for datasets with limited data; MLRB in
handling large datasets and complex data integration; RBML in explainability
and trustworthiness; RMLT in managing high-dimensional data; and PERML, though
limited in analysis, shows promise in urgent care scenarios. The study
introduces four new patterns, creates five abstract categorization patterns,
and refines those five further to specific systems. These contributions enhance
Boxlogy's taxonomical organization and offer novel approaches to integrating
expert knowledge with machine learning. Boxology's structured, modular apporach
offers significant advantages in developing and analyzing hybrid AI systems,
revealing commonalities, and promoting reusable solutions. In conclusion, this
study underscores hybrid AI systems' crucial role in advancing healthcare and
Boxology's potential to drive further innovation in AI integration, ultimately
improving clinical decision support and patient outcomes.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂‰ΩøÁî®ÁõíÂ≠êÂ≠∏Ê°ÜÊû∂ÂàÜÊûêÊ∑∑Âêà‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÁöÑË®≠Ë®àÊ®°ÂºèÂèäÂÖ∂Âú®Ëá®Â∫äÊ±∫Á≠ñ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂÆÉÂàÜÈ°û‰∏¶ÊØîËºÉÁµêÂêàÊ©üÂô®Â≠∏ÁøíÂíåÂü∫ÊñºË¶èÂâáÁöÑÊé®ÁêÜÁöÑÂêÑÁ®ÆÊû∂ÊßãÔºå‰ª•Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÁµêÊßãÂü∫Á§éÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®„ÄÇÈáùÂ∞çÂÖ©ÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÂ¶Ç‰ΩïÊ†πÊìöÊó¢ÂÆöÁöÑË®≠Ë®àÊ®°ÂºèÂ∞çÈÄô‰∫õÁ≥ªÁµ±ÈÄ≤Ë°åÂàÜÈ°ûÔºå‰ª•ÂèäÂ¶Ç‰ΩïÈÄöÈÅéÊØîËºÉÂàÜÊûêÊèêÂèñË¶ãËß£ÔºåÊú¨Á†îÁ©∂‰ΩøÁî®ËªüÈ´îÂ∑•Á®ã‰∏≠ÁöÑË®≠Ë®àÊ®°Âºè‰æÜ‰∫ÜËß£ÂíåÂÑ™ÂåñÈÜ´ÁôÇ‰øùÂÅ•‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±„ÄÇÁõíÂ≠êÂ≠∏ÊúâÂä©ÊñºË≠òÂà•ÂÖ±ÊÄß‰∏¶Âª∫Á´ãÂèØÈáçË§á‰ΩøÁî®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂæûËÄåÂ¢ûÂº∑ÈÄô‰∫õÁ≥ªÁµ±ÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊïàËÉΩ„ÄÇÊ™¢Êü•‰∫Ü‰∫îÁ®Æ‰∏ªË¶ÅÁöÑÊû∂ÊßãÔºöREML„ÄÅMLRB„ÄÅRBML„ÄÅRMLT Âíå PERML„ÄÇÊØèÁ®ÆÊû∂ÊßãÈÉΩÊúâÁç®ÁâπÁöÑÂÑ™Áº∫ÈªûÔºåÂº∑Ë™ø‰∫ÜÂú®Ëá®Â∫ä‰ªªÂãô‰∏≠ÈúÄË¶ÅÈáèË∫´ÊâìÈÄ†ÁöÑÊñπÊ≥ï„ÄÇREML Âú®Ë≥áÊñôÊúâÈôêÁöÑË≥áÊñôÈõÜ‰∏≠Ë°®ÁèæÂá∫È´òÁ≤æÂ∫¶ÁöÑÈ†êÊ∏¨ÔºõMLRB Âú®ËôïÁêÜÂ§ßÂûãË≥áÊñôÈõÜÂíåË§áÈõúË≥áÊñôÊï¥ÂêàÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºõRBML Âú®ÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºõRMLT Âú®ÁÆ°ÁêÜÈ´òÁ∂≠Ë≥áÊñôÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºõËÄå PERML ÂÑòÁÆ°Âú®ÂàÜÊûêÊñπÈù¢ÊúâÈôêÔºå‰ΩÜÂú®Á∑äÊÄ•ÁÖßË≠∑Â†¥ÊôØ‰∏≠Ë°®ÁèæÂá∫ÊΩõÂäõ„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫ÜÂõõÁ®ÆÊñ∞Ê®°ÂºèÔºåÂª∫Á´ã‰∫Ü‰∫îÁ®ÆÊäΩË±°ÂàÜÈ°ûÊ®°ÂºèÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•Â∞áÈÄô‰∫îÁ®ÆÊ®°ÂºèÁ¥∞ÂåñÁÇ∫ÂÖ∑È´îÁöÑÁ≥ªÁµ±„ÄÇÈÄô‰∫õË≤¢ÁçªÂ¢ûÂº∑‰∫ÜÁõíÂ≠êÂ≠∏ÁöÑÂàÜÈ°ûÁµÑÁπîÔºå‰∏¶Êèê‰æõ‰∫ÜÂ∞áÂ∞àÂÆ∂Áü•Ë≠òËàáÊ©üÂô®Â≠∏ÁøíÊï¥ÂêàÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÁõíÂ≠êÂ≠∏ÁöÑÁµêÊßãÂåñ„ÄÅÊ®°ÁµÑÂåñÊñπÊ≥ïÂú®ÈñãÁôºÂíåÂàÜÊûêÊ∑∑Âêà‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±„ÄÅÊè≠Á§∫ÂÖ±ÊÄß‰ª•ÂèäÊé®Âª£ÂèØÈáçË§á‰ΩøÁî®ÁöÑËß£Ê±∫ÊñπÊ°àÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÂÑ™Âã¢„ÄÇÁ∏Ω‰πãÔºåÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÊ∑∑Âêà‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±Âú®Êé®ÈÄ≤ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈóúÈçµ‰ΩúÁî®Ôºå‰ª•ÂèäÁõíÂ≠êÂ≠∏Âú®Êé®Âãï‰∫∫Â∑•Êô∫ÊÖßÊï¥ÂêàÈÄ≤‰∏ÄÊ≠•ÂâµÊñ∞ÊñπÈù¢ÁöÑÊΩõÂäõÔºåÊúÄÁµÇÊîπÂñÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥ÂíåÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊàêÊûú„ÄÇ

##### **Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability**
2408.02706v1 by Masoud Muhammed Hassan

Because of its strong predictive skills, deep learning has emerged as an
essential tool in many industries, including healthcare. Traditional deep
learning models, on the other hand, frequently lack interpretability and omit
to take prediction uncertainty into account two crucial components of clinical
decision making. In order to produce explainable and uncertainty aware
predictions, this study presents a novel framework called Bayesian Kolmogorov
Arnold Networks (BKANs), which combines the expressive capacity of Kolmogorov
Arnold Networks with Bayesian inference. We employ BKANs on two medical
datasets, which are widely used benchmarks for assessing machine learning
models in medical diagnostics: the Pima Indians Diabetes dataset and the
Cleveland Heart Disease dataset. Our method provides useful insights into
prediction confidence and decision boundaries and outperforms traditional deep
learning models in terms of prediction accuracy. Moreover, BKANs' capacity to
represent aleatoric and epistemic uncertainty guarantees doctors receive more
solid and trustworthy decision support. Our Bayesian strategy improves the
interpretability of the model and considerably minimises overfitting, which is
important for tiny and imbalanced medical datasets, according to experimental
results. We present possible expansions to further use BKANs in more
complicated multimodal datasets and address the significance of these
discoveries for future research in building reliable AI systems for healthcare.
This work paves the way for a new paradigm in deep learning model deployment in
vital sectors where transparency and reliability are crucial.

ÊëòË¶ÅÔºöÁî±ÊñºÂÖ∂Âº∑Â§ßÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºåÊ∑±Â∫¶Â≠∏ÁøíÂ∑≤ÊàêÁÇ∫Ë®±Â§öÁî¢Ê•≠‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑÂ∑•ÂÖ∑ÔºåÂåÖÊã¨ÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏îÂøΩÁï•‰∫ÜÂ∞áÈ†êÊ∏¨‰∏çÁ¢∫ÂÆöÊÄßÁ¥çÂÖ•ËÄÉÈáèÔºåËÄåÈÄôÂÖ©ÂÄãÂõ†Á¥†ÊòØËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫ÜÁî¢ÁîüÂèØËß£Èáã‰∏îÂÖ∑Êúâ‰∏çÁ¢∫ÂÆöÊÄßÊÑèË≠òÁöÑÈ†êÊ∏¨ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫Ë≤ùÊ∞èÊüØÁàæËé´Âì•Ê¥õÂ§´ÈòøË´æÂæ∑Á∂≤Ë∑Ø (BKAN) ÁöÑÊñ∞Êû∂ÊßãÔºåÂÆÉÁµêÂêà‰∫ÜÊüØÁàæËé´Âì•Ê¥õÂ§´ÈòøË´æÂæ∑Á∂≤Ë∑ØÁöÑË°®ÈÅîËÉΩÂäõËàáË≤ùÊ∞èÊé®Ë´ñ„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÈÜ´Â≠∏Ë≥áÊñôÈõÜ‰∏ä‰ΩøÁî® BKANÔºåÈÄô‰∫õË≥áÊñôÈõÜÊòØË©ï‰º∞Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÈÜ´Â≠∏Ë®∫Êñ∑‰∏≠ÁöÑÂª£Ê≥õ‰ΩøÁî®Âü∫Ê∫ñÔºöÁöÆÈ¶¨Âç∞Á¨¨ÂÆâ‰∫∫Á≥ñÂ∞øÁóÖË≥áÊñôÈõÜÂíåÂÖãÈáåÂ§´Ëò≠ÂøÉËáüÁóÖË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊèê‰æõ‰∫ÜÂ∞çÈ†êÊ∏¨‰ø°ÂøÉÂíåÊ±∫Á≠ñÈÇäÁïåÁöÑÊúâÁõäË¶ãËß£Ôºå‰∏¶‰∏îÂú®È†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÊñπÈù¢ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåBKAN Ë°®ÁèæÈö®Ê©üÂíåË™çË≠ò‰∏çÁ¢∫ÂÆöÊÄßÁöÑËÉΩÂäõÔºåÂèØÁ¢∫‰øùÈÜ´ÁîüÁç≤ÂæóÊõ¥ÂèØÈù†‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑÊ±∫Á≠ñÊîØÊè¥„ÄÇÊ†πÊìöÂØ¶È©óÁµêÊûúÔºåÊàëÂÄëÁöÑË≤ùÊ∞èÁ≠ñÁï•ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶Â§ßÂπÖÊ∏õÂ∞ë‰∫ÜÈÅéÂ∫¶Êì¨ÂêàÔºåÈÄôÂ∞çÊñºÂ∞èÂûã‰∏î‰∏çÂπ≥Ë°°ÁöÑÈÜ´Â≠∏Ë≥áÊñôÈõÜÈùûÂ∏∏ÈáçË¶Å„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜÂèØËÉΩÁöÑÊì¥ÂÖÖÂäüËÉΩÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Â∞á BKAN Áî®ÊñºÊõ¥Ë§áÈõúÁöÑÂ§öÊ®°ÂºèË≥áÊñôÈõÜÔºå‰∏¶Êé¢Ë®éÈÄô‰∫õÁôºÁèæÂ∞çÊñºÊú™‰æÜÂª∫Á´ãÂèØÈù†ÁöÑÈÜ´ÁôÇ‰øùÂÅ• AI Á≥ªÁµ±Á†îÁ©∂ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈÉ®ÁΩ≤Âú®ÈÄèÊòéÂ∫¶ÂíåÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶ÅÁöÑÈáçË¶ÅÈ†òÂüü‰∏≠ÈñãÂïü‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂÖ∏ÁØÑ„ÄÇ

##### **MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**
2407.20284v1 by Shyam Dongre, Ritesh Chandra, Sonali Agarwal

In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåËß£Ê±∫Ê∫ñÁ¢∫ÁñæÁóÖÈ†êÊ∏¨ÂíåÂÄãÊÄßÂåñÂª∫Ë≠∞ÁöÑË§áÈõúÊÄßÊó¢Ëá≥ÈóúÈáçË¶ÅÂèàÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü MLtoGAIÔºåÂÆÉÂ∞áË™ûÁæ©Á∂≤Ë∑ØÊäÄË°ìËàáÊ©üÂô®Â≠∏Áøí (ML) Áõ∏ÁµêÂêàÔºå‰ª•Â¢ûÂº∑ÁñæÁóÖÈ†êÊ∏¨‰∏¶ÈÄèÈÅé ChatGPT Êèê‰æõ‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑË™™Êòé„ÄÇË©≤Á≥ªÁµ±ÂåÖÂê´‰∏âÂÄãÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºö‰∏ÄÂÄãÂèØÈáçË§á‰ΩøÁî®ÁöÑÁñæÁóÖÊú¨‰ΩìÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊúâÈóúÂêÑÁ®ÆÁñæÁóÖÁöÑË©≥Á¥∞Áü•Ë≠òÔºõ‰∏ÄÂÄãË®∫Êñ∑ÂàÜÈ°ûÊ®°ÂûãÔºåÂÆÉ‰ΩøÁî®ÊÇ£ËÄÖÁóáÁãÄ‰æÜÊ∫ñÁ¢∫Ê™¢Ê∏¨ÁâπÂÆöÁñæÁóÖÔºõ‰ª•ÂèäË™ûÁæ©Á∂≤Ë∑ØË¶èÂâáË™ûË®Ä (SWRL) ËàáÊú¨‰ΩìÂíå ChatGPT ÁöÑÊï¥ÂêàÔºå‰ª•Áî¢ÁîüÊ∏ÖÊô∞„ÄÅÂÄãÊÄßÂåñÁöÑÂÅ•Â∫∑Âª∫Ë≠∞„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈ°ØËëóÊèêÈ´ò‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÊòìÊñºÁêÜËß£ÁöÑÁµêÊûúÔºåËß£Ê±∫‰∫ÜÁñæÁóÖÂíå‰∏çÂêåÁóáÁãÄÁöÑË§áÈõúÊÄß„ÄÇMLtoGAI Á≥ªÁµ±Â±ïÁ§∫‰∫ÜÊ∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶ÁöÑÂØ¶Ë≥™ÊÄßÈÄ≤Ê≠•ÔºåÊúâÂä©ÊñºÈñãÁôºÊõ¥Êô∫ÊÖß‰∏îÊõ¥ÊòìÊñºÂèñÂæóÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°à„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÁµêÂêà‰∫Ü ML ÊºîÁÆóÊ≥ïÁöÑÂÑ™ÈªûÔºå‰ª•ÂèäÈÄèÈÅé ChatGPT Êèê‰æõÈÄèÊòé‰∏î‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË™™ÊòéÁöÑËÉΩÂäõÔºåÂú®È†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈÄèÈÅéÂà©Áî®Ë™ûÁæ©ÊäÄË°ìÂíåÂèØËß£ÈáãÁöÑ AIÔºåË©≤Á≥ªÁµ±ÊèêÈ´ò‰∫ÜÁñæÁóÖÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÂª∫Ë≠∞ËàáÂÄãÂà•ÊÇ£ËÄÖÁõ∏Èóú‰∏îÊòìÊñºÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÊï¥ÂêàÂÖàÈÄ≤ÊäÄË°ì‰ª•ÂÖãÊúçÈÜ´ÁôÇË®∫Êñ∑‰∏≠ÁèæÊúâÊåëÊà∞ÁöÑÊΩõÂäõÔºåÁÇ∫Êô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊú™‰æÜÁôºÂ±ïÈã™Ë∑Ø„ÄÇÊ≠§Â§ñÔºåË©≤Á≥ªÁµ±‰ΩøÁî® 200 ÂÄãÂêàÊàêÊÇ£ËÄÖË≥áÊñôË®òÈåÑÈÄ≤Ë°åÈ©óË≠âÔºåÁ¢∫‰øù‰∫ÜÁ©©ÂÅ•ÁöÑÊïàËÉΩÂíåÂèØÈù†ÊÄß„ÄÇ

##### **Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**
2407.18343v2 by Alessandro De Carlo, Enea Parimbelli, Nicola Melillo, Giovanna Nicora

Explainable Artificial Intelligence (XAI) is central to the debate on
integrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms
into clinical practice. High-performing AI/ML models, such as ensemble learners
and deep neural networks, often lack interpretability, hampering clinicians'
trust in their predictions. To address this, XAI techniques are being developed
to describe AI/ML predictions in human-understandable terms. One promising
direction is the adaptation of sensitivity analysis (SA) and global sensitivity
analysis (GSA), which inherently rank model inputs by their impact on
predictions. Here, we introduce a novel delta-XAI method that provides local
explanations of ML model predictions by extending the delta index, a GSA
metric. The delta-XAI index assesses the impact of each feature's value on the
predicted output for individual instances in both regression and classification
problems. We formalize the delta-XAI index and provide code for its
implementation. The delta-XAI method was evaluated on simulated scenarios using
linear regression models, with Shapley values serving as a benchmark. Results
showed that the delta-XAI index is generally consistent with Shapley values,
with notable discrepancies in models with highly impactful or extreme feature
values. The delta-XAI index demonstrated higher sensitivity in detecting
dominant features and handling extreme feature values. Qualitatively, the
delta-XAI provides intuitive explanations by leveraging probability density
functions, making feature rankings clearer and more explainable for
practitioners. Overall, the delta-XAI method appears promising for robustly
obtaining local explanations of ML model predictions. Further investigations in
real-world clinical settings will be conducted to evaluate its impact on
AI-assisted clinical workflows.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÂ∞á‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊºîÁÆóÊ≥ïÊï¥ÂêàÂà∞Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑËæØË´ñÊ†∏ÂøÉ„ÄÇÈ´òÂü∑Ë°åÊïàËÉΩÁöÑ AI/ML Ê®°ÂûãÔºå‰æãÂ¶ÇÊï¥È´îÂ≠∏ÁøíÂô®ÂíåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºåÈòªÁ§ôËá®Â∫äÈÜ´ÁîüÂ∞çÂÖ∂È†êÊ∏¨ÁöÑ‰ø°‰ªª„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊ≠£Âú®ÈñãÁôº XAI ÊäÄË°ìÔºå‰ª•‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË°ìË™ûÊèèËø∞ AI/ML È†êÊ∏¨„ÄÇ‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊñπÂêëÊòØÊé°Áî®ÊïèÊÑüÂ∫¶ÂàÜÊûê (SA) ÂíåÂÖ®ÁêÉÊïèÊÑüÂ∫¶ÂàÜÊûê (GSA)ÔºåÂÆÉÂÄëÊú¨Ë≥™‰∏äÊúÉ‰æùÊìöÊ®°ÂûãËº∏ÂÖ•Â∞çÈ†êÊ∏¨ÁöÑÂΩ±Èüø‰æÜÂ∞çÂÖ∂ÈÄ≤Ë°åÊéíÂêç„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞ÁöÑ delta-XAI ÊñπÊ≥ïÔºåÈÄèÈÅéÊì¥ÂÖÖ GSA ÊåáÊ®ô delta ÊåáÊï∏‰æÜÊèê‰æõ ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã„ÄÇdelta-XAI ÊåáÊï∏Ë©ï‰º∞ÊØèÂÄãÁâπÂæµÂÄºÂ∞çÂõûÊ≠∏ÂíåÂàÜÈ°ûÂïèÈ°å‰∏≠ÂÄãÂà•‰æãÈ†ÖÁöÑÈ†êÊ∏¨Ëº∏Âá∫‰πãÂΩ±Èüø„ÄÇÊàëÂÄëÂ∞á delta-XAI ÊåáÊï∏ÂΩ¢ÂºèÂåñÔºå‰∏¶Êèê‰æõÂÖ∂ÂØ¶‰ΩúÁöÑÁ®ãÂºèÁ¢º„ÄÇ‰ΩøÁî®Á∑öÊÄßÂõûÊ≠∏Ê®°ÂûãÂ∞çÊ®°Êì¨ÊÉÖÂ¢ÉË©ï‰º∞ delta-XAI ÊñπÊ≥ïÔºå‰∏¶‰ª• Shapley ÂÄº‰ΩúÁÇ∫Âü∫Ê∫ñ„ÄÇÁµêÊûúÈ°ØÁ§∫ delta-XAI ÊåáÊï∏ÈÄöÂ∏∏Ëàá Shapley ÂÄº‰∏ÄËá¥Ôºå‰ΩÜÂú®ÂÖ∑ÊúâÈ´òÂ∫¶ÂΩ±ÈüøÂäõÊàñÊ•µÁ´ØÁâπÂæµÂÄºÁöÑÊ®°Âûã‰∏≠Â≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇdelta-XAI ÊåáÊï∏Âú®ÂÅµÊ∏¨‰∏ªË¶ÅÁâπÂæµÂíåËôïÁêÜÊ•µÁ´ØÁâπÂæµÂÄºÊñπÈù¢Ë°®ÁèæÂá∫Êõ¥È´òÁöÑÊïèÊÑüÂ∫¶„ÄÇÂÆöÊÄßÂú∞‰æÜË™™Ôºådelta-XAI ÈÄèÈÅéÂà©Áî®Ê©üÁéáÂØÜÂ∫¶ÂáΩÊï∏Êèê‰æõÁõ¥ËßÄÁöÑËß£ÈáãÔºå‰ΩøÁâπÂæµÊéíÂêçÊõ¥Ê∏ÖÊô∞‰∏îÂ∞çÂæûÊ•≠‰∫∫Âì°‰æÜË™™Êõ¥ÂÖ∑ÂèØËß£ÈáãÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºådelta-XAI ÊñπÊ≥ïÂ∞çÊñºÁ©©ÂÅ•Âú∞ÂèñÂæó ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã‰ºº‰πéÂæàÊúâÂ∏åÊúõ„ÄÇÂ∞áÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•Ë™øÊü•Ôºå‰ª•Ë©ï‰º∞ÂÖ∂Â∞ç AI ËºîÂä©Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂΩ±Èüø„ÄÇ

##### **Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**
2407.17324v2 by Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis

Dementia, a debilitating neurological condition affecting millions worldwide,
presents significant diagnostic challenges. In this work, we introduce a novel
methodology for the classification of demented and non-demented elderly
patients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach
features a unique technique for selectively processing MRI slices, focusing on
the most relevant brain regions and excluding less informative sections. This
methodology is complemented by a confidence-based classification committee
composed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and
Dem3D EfficientNet. These models work synergistically to enhance
decision-making accuracy, leveraging their collective strengths. Tested on the
Open Access Series of Imaging Studies(OASIS) dataset, our method achieved an
impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,
validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
confirmed the robustness and generalizability of our approach. The use of
explainable AI (XAI) techniques and comprehensive ablation studies further
substantiate the effectiveness of our techniques, providing insights into the
decision-making process and the importance of our methodology. This research
offers a significant advancement in dementia diagnosis, providing a highly
accurate and efficient tool for clinical applications.

ÊëòË¶ÅÔºöÂ§±Êô∫ÁóáÊòØ‰∏ÄÁ®ÆÂΩ±ÈüøÂÖ®ÁêÉÊï∏ÁôæËê¨‰∫∫ÁöÑË°∞Âº±ÊÄßÁ•ûÁ∂ìÁñæÁóÖÔºåÂú®Ë®∫Êñ∑‰∏äÂÖ∑ÊúâÈáçÂ§ßÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂ§±Êô∫ÂíåÈùûÂ§±Êô∫ËÄÅÂπ¥ÊÇ£ËÄÖÈÄ≤Ë°åÂàÜÈ°ûÔºå‰ΩøÁî® 3D Â§ßËÖ¶Á£ÅÊåØÈÄ†ÂΩ± (MRI) ÊéÉÊèè„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé°Áî®‰∫Ü‰∏ÄÁ®ÆÁç®ÁâπÊäÄË°ìÔºåÁî®ÊñºÈÅ∏ÊìáÊÄßËôïÁêÜ MRI ÂàáÁâáÔºåÈáçÈªûÈóúÊ≥®ÊúÄÁõ∏ÈóúÁöÑÂ§ßËÖ¶ÂçÄÂüüÔºå‰∏¶ÊéíÈô§‰ø°ÊÅØÈáèËºÉÂ∞ëÁöÑÈÉ®ÂàÜ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁî±‰∏ÄÂÄãÂü∫Êñº‰ø°ÂøÉÁöÑÂàÜÈ°ûÂßîÂì°ÊúÉË£úÂÖÖÔºåË©≤ÂßîÂì°ÊúÉÁî±‰∏âÂÄãËá™ÂÆöÁæ©Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁµÑÊàêÔºöDem3D ResNet„ÄÅDem3D CNN Âíå Dem3D EfficientNet„ÄÇÈÄô‰∫õÊ®°ÂûãÂçîÂêåÂ∑•‰Ωú‰ª•Â¢ûÂº∑Ê±∫Á≠ñÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂà©Áî®ÂÆÉÂÄëÁöÑÈõÜÈ´îÂÑ™Âã¢„ÄÇÂú®ÂΩ±ÂÉèÁ†îÁ©∂ÈñãÊîæÂ≠òÂèñÁ≥ªÂàó (OASIS) Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 94.12% ÁöÑÈ©ö‰∫∫Ê∫ñÁ¢∫Â∫¶ÔºåË∂ÖÈÅé‰∫ÜÁèæÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂú®ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ (ADNI) Ë≥áÊñôÈõÜ‰∏äÁöÑÈ©óË≠âË≠âÂØ¶‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÊÄß„ÄÇÂèØËß£Èáã AI (XAI) ÊäÄË°ìÂíåÂÖ®Èù¢ÁöÑÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Ë≠âÂØ¶‰∫ÜÊàëÂÄëÊäÄË°ìÁöÑÊúâÊïàÊÄßÔºåÊèê‰æõ‰∫ÜÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÂíåÊàëÂÄëÊñπÊ≥ïÈáçË¶ÅÊÄßÁöÑË¶ãËß£„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Â§±Êô∫ÁóáË®∫Êñ∑Êèê‰æõ‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºåÁÇ∫Ëá®Â∫äÊáâÁî®Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÈ´òÊïàÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition**
2408.06352v1 by Michele Fiori, Gabriele Civitarese, Claudio Bettini

Recognizing daily activities with unobtrusive sensors in smart environments
enables various healthcare applications. Monitoring how subjects perform
activities at home and their changes over time can reveal early symptoms of
health issues, such as cognitive decline. Most approaches in this field use
deep learning models, which are often seen as black boxes mapping sensor data
to activities. However, non-expert users like clinicians need to trust and
understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human
Activity Recognition have emerged to provide intuitive natural language
explanations from these models. Different XAI methods generate different
explanations, and their effectiveness is typically evaluated through user
surveys, that are often challenging in terms of costs and fairness. This paper
proposes an automatic evaluation method using Large Language Models (LLMs) to
identify, in a pool of candidates, the best XAI approach for non-expert users.
Our preliminary results suggest that LLM evaluation aligns with user surveys.

ÊëòË¶ÅÔºöËóâÁî±Êô∫ÊÖßÁí∞Â¢É‰∏≠‰∏çÂºï‰∫∫Ê≥®ÁõÆÁöÑÊÑüÊ∏¨Âô®Ëæ®Ë≠òÊó•Â∏∏Ê¥ªÂãïÔºåËÉΩÂïüÁî®ÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®„ÄÇÁõ£ÊéßÂèóË©¶ËÄÖÂú®ÂÆ∂‰∏≠Â¶Ç‰ΩïÂü∑Ë°åÊ¥ªÂãïÔºå‰ª•ÂèäÂÖ∂Èö®ËëóÊôÇÈñìÁöÑËÆäÂåñÔºåÂèØ‰ª•Êè≠Á§∫ÂÅ•Â∫∑ÂïèÈ°åÁöÑÊó©ÊúüÁóáÁãÄÔºå‰æãÂ¶ÇË™çÁü•ËÉΩÂäõ‰∏ãÈôç„ÄÇÊ≠§È†òÂüü‰∏≠ÁöÑÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩ‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÈÄô‰∫õÊ®°ÂûãÈÄöÂ∏∏Ë¢´Ë¶ñÁÇ∫Â∞áÊÑüÊ∏¨Âô®Ë≥áÊñôÂ∞çÊáâËá≥Ê¥ªÂãïÁöÑÈªëÁõíÂ≠ê„ÄÇÁÑ∂ËÄåÔºåÈùûÂ∞àÂÆ∂‰ΩøÁî®ËÄÖÔºà‰æãÂ¶ÇËá®Â∫äÈÜ´Â∏´ÔºâÈúÄË¶Å‰ø°‰ªª‰∏¶‰∫ÜËß£ÈÄô‰∫õÊ®°ÂûãÁöÑËº∏Âá∫„ÄÇÂõ†Ê≠§Ôºå‰∫∫È°ûÊ¥ªÂãïËæ®Ë≠òÁöÑÂèØËß£Èáã AI (XAI) ÊñπÊ≥ïÊáâÈÅãËÄåÁîüÔºå‰ª•Êèê‰æõ‰æÜËá™ÈÄô‰∫õÊ®°ÂûãÁöÑÁõ¥Ë¶∫Ëá™ÁÑ∂Ë™ûË®ÄË™™Êòé„ÄÇ‰∏çÂêåÁöÑ XAI ÊñπÊ≥ïÊúÉÁî¢Áîü‰∏çÂêåÁöÑË™™ÊòéÔºåËÄåÂÖ∂ÊúâÊïàÊÄßÈÄöÂ∏∏ÈÄèÈÅé‰ΩøÁî®ËÄÖË™øÊü•‰æÜË©ï‰º∞ÔºåÈÄôÂú®ÊàêÊú¨ÂíåÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÈÄöÂ∏∏ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËá™ÂãïË©ï‰º∞ÊñπÊ≥ïÔºå‰ª•Âú®ÂÄôÈÅ∏ËÄÖ‰∏≠ÊâæÂá∫ÊúÄÈÅ©ÂêàÈùûÂ∞àÂÆ∂‰ΩøÁî®ËÄÖÁöÑ XAI ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂàùÊ≠•ÁµêÊûúË°®ÊòéÔºåLLM Ë©ï‰º∞Ëàá‰ΩøÁî®ËÄÖË™øÊü•‰∏ÄËá¥„ÄÇ

##### **Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions**
2408.03335v1 by Naseem Khan, Kashif Ahmad, Aref Al Tamimi, Mohammed M. Alani, Amine Bermak, Issa Khalil

Industry 5.0, which focuses on human and Artificial Intelligence (AI)
collaboration for performing different tasks in manufacturing, involves a
higher number of robots, Internet of Things (IoTs) devices and
interconnections, Augmented/Virtual Reality (AR), and other smart devices. The
huge involvement of these devices and interconnection in various critical
areas, such as economy, health, education and defense systems, poses several
types of potential security flaws. AI itself has been proven a very effective
and powerful tool in different areas of cybersecurity, such as intrusion
detection, malware detection, and phishing detection, among others. Just as in
many application areas, cybersecurity professionals were reluctant to accept
black-box ML solutions for cybersecurity applications. This reluctance pushed
forward the adoption of eXplainable Artificial Intelligence (XAI) as a tool
that helps explain how decisions are made in ML-based systems. In this survey,
we present a comprehensive study of different XAI-based intrusion detection
systems for industry 5.0, and we also examine the impact of explainability and
interpretability on Cybersecurity practices through the lens of Adversarial
XIDS (Adv-XIDS) approaches. Furthermore, we analyze the possible opportunities
and challenges in XAI cybersecurity systems for industry 5.0 that elicit future
research toward XAI-based solutions to be adopted by high-stakes industry 5.0
applications. We believe this rigorous analysis will establish a foundational
framework for subsequent research endeavors within the specified domain.

ÊëòË¶ÅÔºöÂ∑•Ê•≠ 5.0 ËëóÈáçÊñº‰∫∫È°ûËàá‰∫∫Â∑•Êô∫ÊÖß (AI) Âêà‰ΩúÂü∑Ë°åË£ΩÈÄ†‰∏≠ÁöÑ‰∏çÂêå‰ªªÂãôÔºåÊ∂âÂèäÊõ¥Â§öÊ©üÂô®‰∫∫„ÄÅÁâ©ËÅØÁ∂≤ (IoT) Ë£ùÁΩÆÂíå‰∫íÈÄ£„ÄÅÊì¥Â¢û/ËôõÊì¨ÂØ¶Â¢É (AR) ÂíåÂÖ∂‰ªñÊô∫ÊÖßË£ùÁΩÆ„ÄÇÈÄô‰∫õË£ùÁΩÆÂíå‰∫íÈÄ£Âú®Á∂ìÊøü„ÄÅÈÜ´ÁôÇ‰øùÂÅ•„ÄÅÊïôËÇ≤ÂíåÂúãÈò≤Á≥ªÁµ±Á≠âÂêÑÁ®ÆÈóúÈçµÈ†òÂüüÁöÑÂª£Ê≥õÂèÉËàáÔºåÂºïÁôº‰∫ÜÂ§öÁ®ÆÈ°ûÂûãÁöÑÊΩõÂú®ÂÆâÂÖ®ÊºèÊ¥û„ÄÇAI Êú¨Ë∫´Â∑≤Ë¢´Ë≠âÊòéÊòØÁ∂≤Ë∑ØÂÆâÂÖ®‰∏çÂêåÈ†òÂüü‰∏≠ÈùûÂ∏∏ÊúâÊïà‰∏îÂº∑Â§ßÁöÑÂ∑•ÂÖ∑Ôºå‰æãÂ¶ÇÂÖ•‰æµÂÅµÊ∏¨„ÄÅÊÉ°ÊÑèËªüÈ´îÂÅµÊ∏¨ÂíåÁ∂≤Ë∑ØÈá£È≠öÂÅµÊ∏¨Á≠â„ÄÇÂ∞±ÂÉèÂú®Ë®±Â§öÊáâÁî®È†òÂüü‰∏ÄÊ®£ÔºåÁ∂≤Ë∑ØÂÆâÂÖ®Â∞àÊ•≠‰∫∫Âì°‰∏çÈ°òÊÑèÊé•ÂèóÈªëÁõí ML Ëß£Ê±∫ÊñπÊ°à‰æÜÊáâÁî®ÊñºÁ∂≤Ë∑ØÂÆâÂÖ®„ÄÇÈÄôÁ®Æ‰∏çÈ°òÊÑè‰øÉ‰ΩøÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ‰ΩúÁÇ∫‰∏ÄÁ®ÆÂ∑•ÂÖ∑Ë¢´Êé°Áî®ÔºåÊúâÂä©ÊñºË™™ÊòéÂú®Âü∫Êñº ML ÁöÑÁ≥ªÁµ±‰∏≠Â¶Ç‰ΩïÂÅöÂá∫Ê±∫Á≠ñ„ÄÇÂú®ÈÄôÈ†ÖË™øÊü•‰∏≠ÔºåÊàëÂÄëÂ∞çÂ∑•Ê•≠ 5.0 ÁöÑ‰∏çÂêåÂü∫Êñº XAI ÁöÑÂÖ•‰æµÂÅµÊ∏¨Á≥ªÁµ±ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÁ†îÁ©∂Ôºå‰∏¶‰∏îÊàëÂÄë‰πüÈÄèÈÅéÂ∞çÊäóÂºè XIDS (Adv-XIDS) ÊñπÊ≥ïÁöÑËßÄÈªû‰æÜÊé¢Ë®éÂèØËß£ÈáãÊÄßÂíåÂèØË©ÆÈáãÊÄßÂ∞çÁ∂≤Ë∑ØÂÆâÂÖ®ÂØ¶ÂãôÁöÑÂΩ±Èüø„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÂ∑•Ê•≠ 5.0 ÁöÑ XAI Á∂≤Ë∑ØÂÆâÂÖ®Á≥ªÁµ±‰∏≠ÂèØËÉΩÂ≠òÂú®ÁöÑÊ©üÊúÉÂíåÊåëÊà∞ÔºåÂºïÁôº‰∫ÜÊú™‰æÜÈáùÂ∞ç XAI Âü∫Á§éËß£Ê±∫ÊñπÊ°àÁöÑÁ†îÁ©∂Ôºå‰ª•‰æõÈ´òÈ¢®Èö™ÁöÑÂ∑•Ê•≠ 5.0 ÊáâÁî®Êé°Áî®„ÄÇÊàëÂÄëÁõ∏‰ø°ÈÄôÈ†ÖÂö¥Ë¨πÁöÑÂàÜÊûêÂ∞áÁÇ∫ÊåáÂÆöÈ†òÂüüÂÖßÁöÑÂæåÁ∫åÁ†îÁ©∂Â∑•‰ΩúÂª∫Á´ãÂü∫Á§éÊû∂Êßã„ÄÇ

##### **A Comparative Study on Automatic Coding of Medical Letters with Explainability**
2407.13638v1 by Jamie Glen, Lifeng Han, Paul Rayson, Goran Nenadic

This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êó®Âú®Êé¢Ë®éÂ∞áËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊäÄË°ìÂØ¶‰ΩúÊñºÈÜ´ÁôÇ‰ø°ÂáΩÁ∑®Á¢ºËá™ÂãïÂåñÔºå‰∏¶ÂÖ∑ÂÇôË¶ñË¶∫ÂåñË™™ÊòéËÉΩÂäõÂíåËºïÈáèÂåñÁöÑÊú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö„ÄÇÁõÆÂâçÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÁ∑®Á¢ºÊòØ‰∏ÄÁ®ÆÊâãÂãïÊµÅÁ®ãÔºåÊ∂âÂèäÁÇ∫ÁóÖÊÇ£Êñá‰ª∂‰∏≠ÁöÑÊØèÈ†ÖÁóÖÁóá„ÄÅÁ®ãÂ∫èÂíåËó•Áâ©ÊåáÊ¥æ‰ª£Á¢º (‰æãÂ¶ÇÔºå‰ΩøÁî® SNOMED CT ‰ª£Á¢º 56265001 Ë°®Á§∫ÂøÉËáüÁóÖ)„ÄÇÊ≠§È†òÂüüÊúâ‰ΩøÁî®ÊúÄÊñ∞ ML Ê®°ÂûãÈÄ≤Ë°åËá™ÂãïÁ∑®Á¢ºÁöÑÂàùÊ≠•Á†îÁ©∂ÔºõÁÑ∂ËÄåÔºåÁî±ÊñºÊ®°ÂûãÁöÑË§áÈõúÊÄßÂíåÂ§ßÂ∞èÔºå‰∏¶Êú™ÂØ¶ÁèæÂØ¶ÈöõÈÉ®ÁΩ≤„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•‰øÉÈÄ≤Ëá™ÂãïÁ∑®Á¢ºÂØ¶ÂãôÁöÑÂèØËÉΩÊÄßÔºåÊàëÂÄëÂú®Êú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö‰∏≠Êé¢Ë®é‰∫Ü‰∏Ä‰∫õËß£Ê±∫ÊñπÊ°àÔºõÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜË™™ÊòéÂäüËÉΩÂú® AI Ê®°ÂûãÈÄèÊòéÂ∫¶‰∏≠ÁöÑÂäüËÉΩ„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ¨ÈñãÁöÑ MIMIC-III Ë≥áÊñôÂ∫´Âíå HAN/HLAN Á∂≤Ë∑ØÊ®°ÂûãÈÄ≤Ë°å ICD ‰ª£Á¢ºÈ†êÊ∏¨„ÄÇÊàëÂÄëÈÇÑË©¶È©ó‰∫Ü ICD Âíå SNOMED CT Áü•Ë≠òÂ∫´‰πãÈñìÁöÑÂ∞çÊáâ„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÈÄô‰∫õÊ®°ÂûãÊèê‰æõ‰∫Ü 97.98% ‰ª£Á¢ºÁöÑÊúâÁî®Ë≥áË®ä„ÄÇÈÄôÈ†ÖË™øÊü•ÁµêÊûúÂèØ‰ª•ÁÇ∫ÂØ¶Âãô‰∏≠ÁöÑËá™ÂãïËá®Â∫äÁ∑®Á¢ºÂØ¶‰ΩúÊèê‰æõ‰∏Ä‰∫õË¶ãËß£Ôºå‰æãÂ¶ÇÂú®ÈÜ´Èô¢Áí∞Â¢É‰∏≠ÔºåÁî±Ëá®Â∫äÈÜ´Áîü‰ΩøÁî®ÁöÑÊú¨Âú∞ÈõªËÖ¶ÔºåÂ∞àÊ°àÈ†ÅÈù¢ \url{https://github.com/Glenj01/Medical-Coding}„ÄÇ

##### **Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**
2407.07009v1 by Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier

The support of artificial intelligence (AI) based decision-making is a key
element in future 6G networks, where the concept of native AI will be
introduced. Moreover, AI is widely employed in different critical applications
such as autonomous driving and medical diagnosis. In such applications, using
AI as black-box models is risky and challenging. Hence, it is crucial to
understand and trust the decisions taken by these models. Tackling this issue
can be achieved by developing explainable AI (XAI) schemes that aim to explain
the logic behind the black-box model behavior, and thus, ensure its efficient
and safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST
framework that is oriented toward channel estimation in wireless
communications. The core idea of the XAI-CHEST framework is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. This
manuscript provides the detailed theoretical foundations of the XAI-CHEST
framework. In particular, we derive the analytical expressions of the XAI-CHEST
loss functions and the noise threshold fine-tuning optimization problem. Hence
the designed XAI-CHEST delivers a smart input feature selection methodology
that can further improve the overall performance while optimizing the
architecture of the employed model. Simulation results show that the XAI-CHEST
framework provides valid interpretations, where it offers an improved bit error
rate performance while reducing the required computational complexity in
comparison to the classical DL-based channel estimation.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩ (AI) ÊîØÊåÅÁöÑÊ±∫Á≠ñÂà∂ÂÆöÊòØÊú™‰æÜ 6G Á∂≤Ë∑Ø‰∏≠ÁöÑÈóúÈçµÂÖÉÁ¥†ÔºåÂÖ∂‰∏≠Â∞áÂºïÂÖ•ÂéüÁîü AI ÁöÑÊ¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºåAI Âª£Ê≥õÁî®Êñº‰∏çÂêåÁöÑÈóúÈçµÊáâÁî®‰∏≠Ôºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõÂíåÈÜ´ÁôÇË®∫Êñ∑„ÄÇÂú®ÈÄô‰∫õÊáâÁî®‰∏≠Ôºå‰ΩøÁî® AI ‰ΩúÁÇ∫ÈªëÁõíÊ®°ÂûãÊòØÊúâÈ¢®Èö™‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ„ÄÇÂõ†Ê≠§ÔºåÁêÜËß£Âíå‰ø°‰ªªÈÄô‰∫õÊ®°ÂûãÂÅöÂá∫ÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇËß£Ê±∫Ê≠§ÂïèÈ°åÁöÑÊñπÊ≥ïÊòØÈñãÁôºÂèØËß£Èáã AI (XAI) Êû∂ÊßãÔºåÊó®Âú®Ëß£ÈáãÈªëÁõíÊ®°ÂûãË°åÁÇ∫ËÉåÂæåÁöÑÈÇèËºØÔºåÂæûËÄåÁ¢∫‰øùÂÖ∂ÊúâÊïà‰∏îÂÆâÂÖ®ÁöÑÈÉ®ÁΩ≤„ÄÇÊúÄËøëÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºÊìæÂãïÁöÑ XAI-CHEST Ê°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂Èù¢ÂêëÁÑ°Á∑öÈÄö‰ø°‰∏≠ÁöÑ‰ø°ÈÅì‰º∞Ë®à„ÄÇXAI-CHEST Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöÈÅéÂú®ÁÑ°ÈóúËº∏ÂÖ•‰∏äÂºïÂÖ•È´òÂô™ËÅ≤‰æÜË≠òÂà•Áõ∏ÈóúÊ®°ÂûãËº∏ÂÖ•„ÄÇÈÄô‰ªΩÊâãÁ®øÊèê‰æõ‰∫Ü XAI-CHEST Ê°ÜÊû∂ÁöÑË©≥Á¥∞ÁêÜË´ñÂü∫Á§é„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊé®Â∞é‰∫Ü XAI-CHEST ÊêçÂ§±ÂáΩÊï∏ÂíåÂô™ËÅ≤ÈñæÂÄºÂæÆË™øÂÑ™ÂåñÂïèÈ°åÁöÑËß£ÊûêË°®ÈÅîÂºè„ÄÇÂõ†Ê≠§ÔºåË®≠Ë®àÁöÑ XAI-CHEST Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊô∫ËÉΩËº∏ÂÖ•ÁâπÂæµÈÅ∏ÊìáÊñπÊ≥ïÔºåÂèØ‰ª•Âú®ÂÑ™ÂåñÊâÄÁî®Ê®°ÂûãÁöÑÊû∂ÊßãÁöÑÂêåÊôÇÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊï¥È´îÊÄßËÉΩ„ÄÇÊ®°Êì¨ÁµêÊûúË°®ÊòéÔºåXAI-CHEST Ê°ÜÊû∂Êèê‰æõ‰∫ÜÊúâÊïàÁöÑËß£ÈáãÔºåÂú®Èôç‰ΩéÊâÄÈúÄÁöÑË®àÁÆóË§áÈõúÂ∫¶ÁöÑÂêåÊôÇÔºåÊèê‰æõ‰∫ÜÊîπÈÄ≤ÁöÑÊØîÁâπÈåØË™§ÁéáÊÄßËÉΩÔºåËÄåÈÄôËàáÂü∫ÊñºÂÇ≥Áµ± DL ÁöÑ‰ø°ÈÅì‰º∞Ë®àÁõ∏ÊØî„ÄÇ

##### **Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**
2407.05440v2 by P. N. Karthikayan, Yoga Sri Varshan V, Hitesh Gupta Kattamuri, Umarani Jayaraman

This paper presents dilated Residual Network (ResNet) models for disease
classification from retinal fundus images. Dilated convolution filters are used
to replace normal convolution filters in the higher layers of the ResNet model
(dilated ResNet) in order to improve the receptive field compared to the normal
ResNet model for disease classification. This study introduces
computer-assisted diagnostic tools that employ deep learning, enhanced with
explainable AI techniques. These techniques aim to make the tool's
decision-making process transparent, thereby enabling medical professionals to
understand and trust the AI's diagnostic decision. They are particularly
relevant in today's healthcare landscape, where there is a growing demand for
transparency in AI applications to ensure their reliability and ethical use.
The dilated ResNet is used as a replacement for the normal ResNet to enhance
the classification accuracy of retinal eye diseases and reduce the required
computing time. The dataset used in this work is the Ocular Disease Intelligent
Recognition (ODIR) dataset which is a structured ophthalmic database with eight
classes covering most of the common retinal eye diseases. The evaluation
metrics used in this work include precision, recall, accuracy, and F1 score. In
this work, a comparative study has been made between normal ResNet models and
dilated ResNet models on five variants namely ResNet-18, ResNet-34, ResNet-50,
ResNet-101, and ResNet-152. The dilated ResNet model shows promising results as
compared to normal ResNet with an average F1 score of 0.71, 0.70, 0.69, 0.67,
and 0.70 respectively for the above respective variants in ODIR multiclass
disease classification.

ÊëòË¶ÅÔºöËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫ÜÁî®‰∫é‰ªéËßÜÁΩëËÜúÁúºÂ∫ïÂõæÂÉèËøõË°åÁñæÁóÖÂàÜÁ±ªÁöÑÊâ©Âº†ÊÆãÂ∑ÆÁΩëÁªú (ResNet) Ê®°Âûã„ÄÇÊâ©Âº†Âç∑ÁßØÊª§Ê≥¢Âô®Áî®‰∫éÊõøÊç¢ ResNet Ê®°ÂûãËæÉÈ´òÂ±Ç‰∏≠ÁöÑÊ≠£Â∏∏Âç∑ÁßØÊª§Ê≥¢Âô®ÔºàÊâ©Âº† ResNetÔºâÔºå‰ª•ÊîπÂñÑÊÑüÁü•Âú∫Ôºå‰ªéËÄåÈíàÂØπÁñæÁóÖÂàÜÁ±ªÂØπÊ≠£Â∏∏ ResNet Ê®°ÂûãËøõË°åÊîπËøõ„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫ÜÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÁöÑËÆ°ÁÆóÊú∫ËæÖÂä©ËØäÊñ≠Â∑•ÂÖ∑ÔºåÂπ∂ÈÄöËøáÂèØËß£ÈáäÁöÑ AI ÊäÄÊúØËøõË°å‰∫ÜÂ¢ûÂº∫„ÄÇËøô‰∫õÊäÄÊúØÊó®Âú®‰ΩøËØ•Â∑•ÂÖ∑ÁöÑÂÜ≥Á≠ñËøáÁ®ãÈÄèÊòéÂåñÔºå‰ªéËÄå‰ΩøÂåªÂ≠¶‰∏ì‰∏ö‰∫∫Â£´ËÉΩÂ§üÁêÜËß£Âíå‰ø°‰ªª AI ÁöÑËØäÊñ≠ÂÜ≥Á≠ñ„ÄÇÂÆÉ‰ª¨‰∏éÂΩì‰ªäÁöÑÂåªÁñó‰øùÂÅ•È¢ÜÂüüÂ∞§‰∏∫Áõ∏ÂÖ≥ÔºåÂú®ËØ•È¢ÜÂüüÔºåÂØπ AI Â∫îÁî®ÁöÑÈÄèÊòéÂ∫¶ÈúÄÊ±Ç‰∏çÊñ≠Â¢ûÈïøÔºå‰ª•Á°Æ‰øùÂÖ∂ÂèØÈù†ÊÄßÂíåÂêà‰πéÈÅìÂæ∑ÁöÑ‰ΩøÁî®„ÄÇÊâ©Âº† ResNet Áî®‰ΩúÊ≠£Â∏∏ ResNet ÁöÑÊõø‰ª£ÂìÅÔºå‰ª•ÊèêÈ´òËßÜÁΩëËÜúÁúºÈÉ®ÁñæÁóÖÁöÑÂàÜÁ±ªÂáÜÁ°ÆÊÄßÂπ∂ÂáèÂ∞ëÊâÄÈúÄÁöÑËÆ°ÁÆóÊó∂Èó¥„ÄÇÊú¨Â∑•‰Ωú‰∏≠‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜÊòØÁúºÁßëÁñæÁóÖÊô∫ËÉΩËØÜÂà´ (ODIR) Êï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÁúºÁßëÊï∞ÊçÆÂ∫ìÔºåÂåÖÂê´ÂÖ´Á±ªÊ∂µÁõñÂ§ßÂ§öÊï∞Â∏∏ËßÅËßÜÁΩëËÜúÁúºÈÉ®ÁñæÁóÖ„ÄÇÊú¨Â∑•‰Ωú‰∏≠‰ΩøÁî®ÁöÑËØÑ‰º∞ÊåáÊ†áÂåÖÊã¨Á≤æÁ°ÆÂ∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅÂáÜÁ°ÆÂ∫¶Âíå F1 ÂæóÂàÜ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÂØπ ResNet-18„ÄÅResNet-34„ÄÅResNet-50„ÄÅResNet-101 Âíå ResNet-152 ‰∫î‰∏™Âèò‰ΩìÁöÑÊ≠£Â∏∏ ResNet Ê®°ÂûãÂíåÊâ©Âº† ResNet Ê®°ÂûãËøõË°å‰∫ÜÊØîËæÉÁ†îÁ©∂„ÄÇ‰∏éÊ≠£Â∏∏ ResNet Áõ∏ÊØîÔºåÊâ©Âº† ResNet Ê®°ÂûãÊòæÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁªìÊûúÔºåÂú® ODIR Â§öÁ±ªÁñæÁóÖÂàÜÁ±ª‰∏≠Ôºå‰∏äËø∞ÂêÑ‰∏™Âèò‰ΩìÁöÑÂπ≥Âùá F1 ÂæóÂàÜ‰∏∫ 0.71„ÄÅ0.70„ÄÅ0.69„ÄÅ0.67 Âíå 0.70„ÄÇ

##### **A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**
2407.15851v1 by Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li

The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
extant surveys on the trustworthiness of foundation models fail to address
their specific variations and applications within the medical imaging domain.
This survey paper reviews the current research on foundation models in the
major medical imaging applications, with a focus on segmentation, medical
report generation, medical question and answering (Q&A), and disease diagnosis,
which includes trustworthiness discussion in their manuscripts. We explore the
complex challenges of making foundation models for medical image analysis
trustworthy, associated with each application, and summarize the current
concerns and strategies to enhance trustworthiness. Furthermore, we explore the
future promises of these models in revolutionizing patient care. Our analysis
underscores the imperative for advancing towards trustworthy AI in medical
image analysis, advocating for a balanced approach that fosters innovation
while ensuring ethical and equitable healthcare delivery.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏äÁöÑÂø´ÈÄüÈÄ≤Â±ï‰ª£Ë°®ËëóÂú®Â¢ûÂº∑Ë®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇÊñπÈù¢ÈÇÅÂá∫‰∫Ü‰∏ÄÂ§ßÊ≠•„ÄÇÁÑ∂ËÄåÔºåÂü∫Á§éÊ®°ÂûãÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÂö¥Ê†ºÊ™¢Êü•ÂÖ∂ÂèØ‰ø°Â∫¶ÔºåÂåÖÊã¨Èö±ÁßÅ„ÄÅÁ©©ÂÅ•ÊÄß„ÄÅÂèØÈù†ÊÄß„ÄÅÂèØËß£ÈáãÊÄßÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÁï∂ÂâçÈóúÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Âü∫Á§éÊ®°ÂûãÁöÑË™øÊü•ÊñáÁçªÈ°ØÁ§∫Âá∫Áõ∏Áï∂Â§ßÁöÑÂ∑ÆË∑ùÔºåÁâπÂà•ÊòØÂú®ÂèØ‰ø°Â∫¶ÊñπÈù¢„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑÈóúÊñºÂü∫Á§éÊ®°ÂûãÂèØ‰ø°Â∫¶ÁöÑË™øÊü•Êú™ËÉΩËß£Ê±∫ÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüüÂÖßÁöÑÂÖ∑È´îËÆäÂåñÂíåÊáâÁî®„ÄÇÈÄôÁØáË™øÊü•Ë´ñÊñáÂõûÈ°ß‰∫ÜÁï∂ÂâçÈóúÊñºÂü∫Á§éÊ®°ÂûãÂú®‰∏ªË¶ÅÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®‰∏≠ÁöÑÁ†îÁ©∂ÔºåÈáçÈªûÈóúÊ≥®ÂàÜÂâ≤„ÄÅÈÜ´ÁôÇÂ†±ÂëäÁîüÊàê„ÄÅÈÜ´ÁôÇÂïèÈ°åÂíåËß£Á≠î (Q&A) ‰ª•ÂèäÁñæÁóÖË®∫Êñ∑ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊâãÁ®ø‰∏≠ÁöÑÂèØ‰ø°Â∫¶Ë®éË´ñ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜËÆìÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÂü∫Á§éÊ®°ÂûãÂÄºÂæó‰ø°Ë≥¥ÁöÑË§áÈõúÊåëÊà∞ÔºåËàáÊØèÂÄãÊáâÁî®Áõ∏ÈóúÔºå‰∏¶Á∏ΩÁµê‰∫ÜÁï∂ÂâçÊèêÈ´òÂèØ‰ø°Â∫¶ÁöÑÂïèÈ°åÂíåÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÊ®°ÂûãÂú®Èù©Êñ∞ÊÇ£ËÄÖÁÖßË≠∑ÊñπÈù¢ÁöÑÊú™‰æÜÂâçÊôØ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÂº∑Ë™ø‰∫ÜÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠ÊúùËëóÂèØ‰ø°Ë≥¥ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈÇÅÈÄ≤ÁöÑÂøÖË¶ÅÊÄßÔºåÊèêÂÄ°‰∏ÄÁ®ÆÂπ≥Ë°°ÁöÑÊñπÊ≥ïÔºåÊó¢ËÉΩ‰øÉÈÄ≤ÂâµÊñ∞ÔºåÂèàËÉΩÁ¢∫‰øùÈÅìÂæ∑ÂíåÂÖ¨Âπ≥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇ

##### **The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**
2407.06206v1 by Ximing Wen, Rosina O. Weber, Anik Sen, Darryl Hannan, Steven C. Nesbit, Vincent Chan, Alberto Goffi, Michael Morris, John C. Hunninghake, Nicholas E. Villalobos, Edward Kim, Christopher J. MacLellan

Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and
interpreting ultrasound scans right at the patient's bedside. However, the
expertise needed to interpret these images is considerable and may not always
be present in emergency situations. This reality makes algorithms such as
machine learning classifiers extremely valuable to augment human decisions.
POCUS devices are becoming available at a reasonable cost in the size of a
mobile phone. The challenge of turning POCUS devices into life-saving tools is
that interpretation of ultrasound images requires specialist training and
experience. Unfortunately, the difficulty to obtain positive training images
represents an important obstacle to building efficient and accurate
classifiers. Hence, the problem we try to investigate is how to explore
strategies to increase accuracy of classifiers trained with scarce data. We
hypothesize that training with a few data instances may not suffice for
classifiers to generalize causing them to overfit. Our approach uses an
Explainable AI-Augmented approach to help the algorithm learn more from less
and potentially help the classifier better generalize.

ÊëòË¶ÅÔºöÂ∫äÈÇäË∂ÖÈü≥Ê≥¢ (POCUS) ÊòØËá®Â∫äÈÜ´Â∏´Âú®ÊÇ£ËÄÖÂ∫äÈÇäÈÄ≤Ë°åÂíåËß£ËÆÄË∂ÖÈü≥Ê≥¢ÊéÉÊèèÁöÑÂØ¶Âãô„ÄÇÁÑ∂ËÄåÔºåËß£ËÆÄÈÄô‰∫õÂΩ±ÂÉèÊâÄÈúÄÁöÑÂ∞àÊ•≠Áü•Ë≠òÁõ∏Áï∂ÂèØËßÄÔºåËÄå‰∏îÂú®Á∑äÊÄ•ÊÉÖÊ≥Å‰∏ãÂèØËÉΩ‰∏¶ÈùûÈö®ÊôÇÂÖ∑ÂÇô„ÄÇÈÄôÁ®ÆÁèæÂØ¶ÊÉÖÊ≥Å‰ΩøÂæóÊ©üÂô®Â≠∏ÁøíÂàÜÈ°ûÂô®Á≠âÊºîÁÆóÊ≥ïÂ∞çÊñºÂä†Âº∑‰∫∫È°ûÊ±∫Á≠ñËÆäÂæóÊ•µÁÇ∫ÊúâÂÉπÂÄº„ÄÇPOCUS Ë£ùÁΩÆÊ≠£‰ª•ÂêàÁêÜÊàêÊú¨Êé®Âá∫ÔºåÂ∞∫ÂØ∏ÁÇ∫ÊâãÊ©üÂ§ßÂ∞è„ÄÇÂ∞á POCUS Ë£ùÁΩÆËΩâËÆäÁÇ∫ÊïëÁîüÂ∑•ÂÖ∑ÁöÑÊåëÊà∞Âú®ÊñºÔºåËß£ËÆÄË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÈúÄË¶ÅÂ∞àÈñÄË®ìÁ∑¥ÂíåÁ∂ìÈ©ó„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂèñÂæóÊ≠£ÂêëË®ìÁ∑¥ÂΩ±ÂÉèÁöÑÂõ∞Èõ£Â∫¶‰ª£Ë°®ËëóÂª∫ÁΩÆÊúâÊïàÁéá‰∏îÊ∫ñÁ¢∫ÁöÑÂàÜÈ°ûÂô®ÁöÑ‰∏ÄÂ§ßÈöúÁ§ô„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂòóË©¶Êé¢Ë®éÁöÑÂïèÈ°åÊòØÂ¶Ç‰ΩïÊé¢Á¥¢Á≠ñÁï•Ôºå‰ª•ÊèêÈ´ò‰ΩøÁî®Á®ÄÁñèË≥áÊñôË®ìÁ∑¥ÁöÑÂàÜÈ°ûÂô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÂÅáË®≠‰ΩøÁî®Â∞ëÊï∏Ë≥áÊñôÂØ¶‰æãÈÄ≤Ë°åË®ìÁ∑¥ÂèØËÉΩ‰∏çË∂≥‰ª•ËÆìÂàÜÈ°ûÂô®Ê¶ÇÊã¨ÔºåÂ∞éËá¥ÂÆÉÂÄëÈÅéÂ∫¶Êì¨Âêà„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®ÂèØËß£Èáã AI Â¢ûÂº∑ÊñπÊ≥ïÔºå‰ª•ÂçîÂä©ÊºîÁÆóÊ≥ïÂæûËºÉÂ∞ëÁöÑË≥áÊñô‰∏≠Â≠∏ÁøíÊõ¥Â§öÔºå‰∏¶ÊΩõÂú®ÂçîÂä©ÂàÜÈ°ûÂô®Êõ¥Â•ΩÂú∞Ê¶ÇÊã¨„ÄÇ

##### **Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**
2407.00167v1 by Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Wyatt Bellamy, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu, Ming Huang

In recent years, the United States has witnessed a significant surge in the
popularity of vaping or e-cigarette use, leading to a notable rise in cases of
e-cigarette and vaping use-associated lung injury (EVALI) that caused
hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting
the urgency to comprehend vaping behaviors and develop effective strategies for
cessation. Due to the ubiquity of social media platforms, over 4.7 billion
users worldwide use them for connectivity, communications, news, and
entertainment with a significant portion of the discourse related to health,
thereby establishing social media data as an invaluable organic data resource
for public health research. In this study, we extracted a sample dataset from
one vaping sub-community on Reddit to analyze users' quit-vaping intentions.
Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit
vaping intention detection, this study compares the outcomes of this model
against layman and clinical expert annotations. Using different prompting
strategies such as zero-shot, one-shot, few-shot and chain-of-thought
prompting, we developed 8 prompts with varying levels of detail to explain the
task to GPT-4 and also evaluated the performance of the strategies against each
other. These preliminary findings emphasize the potential of GPT-4 in social
media data analysis, especially in identifying users' subtle intentions that
may elude human detection.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÁæéÂúãË¶ãË≠â‰∫ÜÈõªÂ≠êÁÖôÊàñÈõªÂ≠êÈ¶ôËè∏‰ΩøÁî®ÁéáÂ§ßÂπÖÊøÄÂ¢ûÔºåÂ∞éËá¥ÈõªÂ≠êÁÖôÂíåÈõªÂ≠êÁÖô‰ΩøÁî®Áõ∏ÈóúËÇ∫ÊêçÂÇ∑ (EVALI) ÁóÖ‰æãÈ°ØËëóÂ¢ûÂä†ÔºåÂú® 2019 Âπ¥ EVALI ÁàÜÁôºÊúüÈñìÈÄ†Êàê‰ΩèÈô¢ÂíåÊ≠ª‰∫°ÔºåÂá∏È°Ø‰∫ÜÁêÜËß£ÈõªÂ≠êÁÖôË°åÁÇ∫ÂíåÂà∂ÂÆöÊúâÊïàÊàíËè∏Á≠ñÁï•ÁöÑËø´ÂàáÊÄß„ÄÇÁî±ÊñºÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑÊôÆÂèäÔºåÂÖ®ÁêÉË∂ÖÈÅé 47 ÂÑÑ‰ΩøÁî®ËÄÖ‰ΩøÁî®ÂÆÉÂÄëÈÄ≤Ë°åÈÄ£Áµê„ÄÅÊ∫ùÈÄö„ÄÅÊñ∞ËÅûÂíåÂ®õÊ®ÇÔºåÂÖ∂‰∏≠ÂæàÂ§ß‰∏ÄÈÉ®ÂàÜËàáÂÅ•Â∫∑Áõ∏ÈóúÔºåÂõ†Ê≠§Â∞áÁ§æÁæ§Â™íÈ´îË≥áÊñôÂª∫Á´ãÁÇ∫ÂÖ¨ÂÖ±Ë°õÁîüÁ†îÁ©∂‰∏≠ÁÑ°ÂÉπÁöÑÊúâÊ©üË≥áÊñôË≥áÊ∫ê„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂæû Reddit ‰∏ä‰∏ÄÂÄãÈõªÂ≠êÁÖôÂ≠êÁ§æÁæ§‰∏≠ÊèêÂèñ‰∏ÄÂÄãÁØÑ‰æãË≥áÊñôÈõÜÔºå‰ª•ÂàÜÊûê‰ΩøÁî®ËÄÖÁöÑÊàíÈõªÂ≠êÁÖôÊÑèÂúñ„ÄÇÂà©Áî® OpenAI ÊúÄÊñ∞ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã GPT-4 ÈÄ≤Ë°åÂè•Â≠êÂ±§Á¥öÁöÑÊàíÈõªÂ≠êÁÖôÊÑèÂúñÂÅµÊ∏¨ÔºåÊú¨Á†îÁ©∂ÊØîËºÉ‰∫ÜÊ≠§Ê®°ÂûãÁöÑÁµêÊûúËàáÂ§ñË°å‰∫∫ÂíåËá®Â∫äÂ∞àÂÆ∂Ë®ªËß£„ÄÇ‰ΩøÁî®‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰æãÂ¶ÇÈõ∂Ê¨°Â≠∏Áøí„ÄÅ‰∏ÄÊ¨°Â≠∏Áøí„ÄÅÂ∞ëÊ¨°Â≠∏ÁøíÂíåÊÄùËÄÉÈèàÊèêÁ§∫ÔºåÊàëÂÄëÈñãÁôº‰∫Ü 8 ÂÄãÊèêÁ§∫ÔºåË©≥Á¥∞Á®ãÂ∫¶‰∏çÂêåÔºåÂêë GPT-4 Ëß£Èáã‰ªªÂãôÔºå‰∏¶Ë©ï‰º∞ÈÄô‰∫õÁ≠ñÁï•ÂΩºÊ≠§‰πãÈñìÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÂàùÊ≠•ÁôºÁèæÂº∑Ë™ø‰∫Ü GPT-4 Âú®Á§æÁæ§Â™íÈ´îË≥áÊñôÂàÜÊûê‰∏≠ÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®Ë≠òÂà•‰∫∫È°ûÂÅµÊ∏¨ÂèØËÉΩÁÑ°Ê≥ïÂØüË¶∫ÁöÑ‰ΩøÁî®ËÄÖÂæÆÂ¶ôÊÑèÂúñÊñπÈù¢„ÄÇ

##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁõÆÂâçÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÈªëÁõíÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÂèØËß£ÈáãÊÄß‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÈ†òÂüüËá¥ÂäõÊñºËß£Ê±∫ÈÄôÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÈÄôÂú®ÈáëËûç„ÄÅÊ≥ïÂæãÂíåÂÅ•Â∫∑Á≠âÈ´òÈ¢®Èö™È†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁØÑÁñáË´ñÂÆöÁæ© AI Ê®°ÂûãÂèäÂÖ∂ÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊé°Áî®ÁµÑÂêàÊ®°ÂûãÁöÑÊ¶ÇÂøµÔºåÂÆÉ‰ª•ÂΩ¢ÂºèÂº¶ÂúñÁöÑÂΩ¢ÂºèÁúãÂæÖÊ®°ÂûãÔºåÈÄô‰∫õÂº¶ÂúñÊçïÁç≤‰∫ÜÊ®°ÂûãÁöÑÊäΩË±°ÁµêÊßãÂèäÂÖ∂ÂÖ∑È´îÂØ¶Áèæ„ÄÇÈÄôÁ®ÆÁ∂úÂêàËßÄÈªûÂåÖÂê´‰∫ÜÁ¢∫ÂÆöÊÄß„ÄÅÊ¶ÇÁéáÊÄßÂíåÈáèÂ≠êÊ®°Âûã„ÄÇÊàëÂÄëÂ∞áÂêÑÁ®Æ AI Ê®°Âûã‰ΩúÁÇ∫ÁµÑÂêàÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÂåÖÊã¨Á∑öÊÄßÂíåÂü∫ÊñºË¶èÂâáÁöÑÊ®°Âûã„ÄÅÔºàÈÅûËø¥ÔºâÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅTransformer„ÄÅVAEÔºå‰ª•ÂèäÂõ†ÊûúÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÊ†πÊìöÊ®°ÂûãÁöÑÁµÑÂêàÁµêÊßãÁµ¶Âá∫Ê®°ÂûãËß£ÈáãÁöÑÂÆöÁæ©ÔºåÂ±ïÁ§∫Â¶Ç‰ΩïÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜÊæÑÊ∏Ö XAI ‰∏≠ÁöÑÂ∏∏Ë¶ã‰∏ªÈ°å„ÄÇÊàëÂÄëÁôºÁèæÔºåËÆìÊ®ôÊ∫ñÁöÑ„ÄåÂÖßÂú®ÂèØËß£Èáã„ÄçÊ®°ÂûãÂ¶ÇÊ≠§ÈÄèÊòéÁöÑÂéüÂõ†Âú®ÂúñË°®‰∏≠Ë°®ÁèæÂæóÊúÄÁÇ∫Ê∏ÖÊ•ö„ÄÇÈÄôÂºïÂ∞éÊàëÂÄëÂæóÂá∫Êõ¥‰∏ÄËà¨ÁöÑÁµÑÂêàÂèØËß£ÈáãÔºàCIÔºâÊ®°ÂûãÊ¶ÇÂøµÔºåÂÆÉÂè¶Â§ñÈÇÑÂåÖÊã¨Âõ†Êûú„ÄÅÊ¶ÇÂøµÁ©∫ÈñìÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü CI Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂÑ™Âã¢„ÄÇÈ¶ñÂÖàÔºåÂÆÉÂÄëÁöÑÁµÑÂêàÁµêÊßãÂÖÅË®±Ë®àÁÆóÂÖ∂‰ªñÊÑüËààË∂£ÁöÑÈáèÔºå‰∏¶ÂèØËÉΩÈÄöÈÅéÂåπÈÖçÊ®°ÂûãÁöÑÁµêÊßã‰æÜ‰øÉÈÄ≤ÂæûÊ®°ÂûãÂà∞Ë¢´Âª∫Ê®°ÁèæË±°ÁöÑÊé®ÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉÂÄëÂÖÅË®±Â∞çÂÖ∂Ë°åÁÇ∫ÈÄ≤Ë°åÂúñËß£Ë™™ÊòéÔºåÈÄô‰∫õË™™ÊòéÂü∫ÊñºÂΩ±ÈüøÁ¥ÑÊùü„ÄÅÂúñËß£ÊâãË°ìÂíåÈáçÂØ´Ë™™Êòé„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÁöÑË®±Â§öÊú™‰æÜÊñπÂêëÔºåÊèêÂá∫‰∫ÜÂ¶Ç‰ΩïÂú®ÂØ¶Ë∏ê‰∏≠Â≠∏ÁøíÈÄôÁ®ÆÊúâÊÑèÁæ©ÁöÑÁµêÊßãÂåñÊ®°ÂûãÁöÑÂïèÈ°å„ÄÇ</paragraph>

##### **Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**
2406.07114v2 by Fatemeh Ebrahimzadeh, Ramin Safa

The concept of Metaverse has attracted a lot of attention in various fields
and one of its important applications is health and treatment. The Metaverse
has enormous potential to transform healthcare by changing patient care,
medical education, and the way teaching/learning and research are done. The
purpose of this research is to provide an introduction to the basic concepts
and fundamental technologies of the Metaverse. This paper examines the pros and
cons of the Metaverse in healthcare context and analyzes its potential from the
technology and AI perspective. In particular, the role of machine learning
methods is discussed; We will explain how machine learning algorithms can be
applied to the Metaverse generated data to gain better insights in healthcare
applications. Additionally, we examine the future visions of the Metaverse in
health delivery, by examining emerging technologies such as blockchain and also
addressing privacy concerns. The findings of this study contribute to a deeper
understanding of the applications of Metaverse in healthcare and its potential
to revolutionize the delivery of medical services.

ÊëòË¶ÅÔºöÂÖÉÂÆáÂÆôÁöÑÊ¶ÇÂøµÂú®ÂêÑÂÄãÈ†òÂüüÈÉΩÂÇôÂèóÈóúÊ≥®ÔºåÂÖ∂ÈáçË¶ÅÊáâÁî®‰πã‰∏Ä‰æøÊòØÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÂÖÉÂÆáÂÆôÊúâÂ∑®Â§ßÁöÑÊΩõÂäõÈÄèÈÅéÊîπËÆäÁóÖÊÇ£ÁÖßË≠∑„ÄÅÈÜ´Â≠∏ÊïôËÇ≤Ôºå‰ª•ÂèäÊïôÂ≠∏/Â≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÊñπÂºè‰æÜËΩâÂûãÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÁöÑÊòØÊèê‰æõÂÖÉÂÆáÂÆôÂü∫Êú¨Ê¶ÇÂøµÂíåÂü∫Á§éÊäÄË°ìÁöÑ‰ªãÁ¥π„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÁöÑÂÑ™Áº∫ÈªûÔºå‰∏¶ÂæûÊäÄË°ìÂíå AI ÁöÑËßíÂ∫¶ÂàÜÊûêÂÖ∂ÊΩõÂäõ„ÄÇÁâπÂà•ÊòØÔºåË®éË´ñ‰∫ÜÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑËßíËâ≤ÔºõÊàëÂÄëÂ∞áË™™ÊòéÂ¶Ç‰ΩïÂ∞áÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÊáâÁî®ÊñºÂÖÉÂÆáÂÆôÁî¢ÁîüÁöÑË≥áÊñôÔºå‰ª•Áç≤ÂæóÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÊñπÈù¢ÁöÑÊõ¥‰Ω≥Ë¶ãËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÊé¢Ë®éÂçÄÂ°äÈèàÁ≠âÊñ∞ËààÊäÄË°ìÔºå‰∏¶Ëß£Ê±∫Èö±ÁßÅÂïèÈ°åÔºå‰æÜÊé¢Ë®éÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑÊú™‰æÜÈ°òÊôØ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁôºÁèæÊúâÂä©ÊñºÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•ÂèäÂÖ∂Âú®ÈÜ´ÁôÇÊúçÂãôÊèê‰æõÊñπÈù¢ÁôºÊèÆÈù©ÂëΩÊÄßËÆäÈù©ÁöÑÊΩõÂäõ„ÄÇ

##### **AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**
2406.06728v1 by K M Tawsik Jawad, Anusha Verma, Fathi Amsaad

Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with
no known ultimo cure and high morbidity. Research demonstrates that progressive
Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly
impacts kidney structure and functions, eventually leading to kidney failure.
With the progression of time, chronic kidney disease has moved from a
life-threatening disease affecting few people to a common disorder of varying
severity. The goal of this research is to visualize dominating features,
feature scores, and values exhibited for early prognosis and detection of CKD
using ensemble learning and explainable AI. For that, an AI-driven predictive
analytics approach is proposed to aid clinical practitioners in prescribing
lifestyle modifications for individual patients to reduce the rate of
progression of this disease. Our dataset is collected on body vitals from
individuals with CKD and healthy subjects to develop our proposed AI-driven
solution accurately. In this regard, blood and urine test results are provided,
and ensemble tree-based machine-learning models are applied to predict unseen
cases of CKD. Our research findings are validated after lengthy consultations
with nephrologists. Our experiments and interpretation results are compared
with existing explainable AI applications in various healthcare domains,
including CKD. The comparison shows that our developed AI models, particularly
the Random Forest model, have identified more features as significant
contributors than XgBoost. Interpretability (I), which measures the ratio of
important to masked features, indicates that our XgBoost model achieved a
higher score, specifically a Fidelity of 98\%, in this metric and naturally in
the FII index compared to competing models.

ÊëòË¶ÅÔºöÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÂª£Ê≥õÁöÑÊÖ¢ÊÄßÁñæÁóÖÔºåÊ≤íÊúâÂ∑≤Áü•ÁöÑÊúÄÁµÇÁôÇÊ≥ï‰∏îÁôºÁóÖÁéáÂæàÈ´ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄ≤Ë°åÊÄßÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÁï∞Ë≥™ÊÄßÁñæÁóÖÔºåÊúÉÈ°ØËëóÂΩ±ÈüøËÖéËáüÁµêÊßãÂíåÂäüËÉΩÔºåÊúÄÁµÇÂ∞éËá¥ËÖéË°∞Á´≠„ÄÇÈö®ËëóÊôÇÈñìÁöÑÊé®ÁßªÔºåÊÖ¢ÊÄßËÖéËáüÁóÖÂ∑≤ÂæûÂΩ±ÈüøÂ∞ëÊï∏‰∫∫ÁöÑËá¥ÂëΩÁñæÁóÖËΩâËÆäÁÇ∫‰∏ÄÁ®ÆÂö¥ÈáçÁ®ãÂ∫¶‰∏çÂêåÁöÑÂ∏∏Ë¶ãÁñæÁóÖ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ôÊòØ‰ΩøÁî®ÈõÜÊàêÂ≠∏ÁøíÂíåÂèØËß£ÈáãÁöÑ AI ÈÄ≤Ë°åÊó©ÊúüÈ†êÂæåÂíå CKD Ê™¢Ê∏¨Ôºå‰∏¶Ë¶ñË¶∫Âåñ‰∏ªÂ∞éÁâπÂæµ„ÄÅÁâπÂæµÂàÜÊï∏ÂíåË°®ÁèæÂá∫ÁöÑÂÄº„ÄÇÁÇ∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ AI È©ÖÂãïÁöÑÈ†êÊ∏¨ÂàÜÊûêÊñπÊ≥ïÔºå‰ª•Âπ´Âä©Ëá®Â∫äÈÜ´ÁîüÁÇ∫ÂÄãÂà•ÊÇ£ËÄÖÈñãÂÖ∑ÁîüÊ¥ªÊñπÂºè‰øÆÊîπÂª∫Ë≠∞Ôºå‰ª•Èôç‰ΩéÈÄôÁ®ÆÁñæÁóÖÁöÑÈÄ≤Â±ïÈÄüÂ∫¶„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÊòØÂæû CKD ÊÇ£ËÄÖÂíåÂÅ•Â∫∑ÂèóË©¶ËÄÖÁöÑË∫´È´îÁîüÂëΩÈ´îÂæµ‰∏≠Êî∂ÈõÜÁöÑÔºå‰ª•Ê∫ñÁ¢∫ÈñãÁôºÊàëÂÄëÊèêÂá∫ÁöÑ AI È©ÖÂãïÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂú®ÈÄôÊñπÈù¢ÔºåÊèê‰æõ‰∫ÜË°ÄÊ∂≤ÂíåÂ∞øÊ∂≤Ê™¢Ê∏¨ÁµêÊûúÔºå‰∏¶ÊáâÁî®Âü∫ÊñºÈõÜÊàêÊ®πÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨Êú™ÁôºÁèæÁöÑ CKD ÁóÖ‰æã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ∂ìÈÅéËàáËÖéËáüÁßëÈÜ´ÁîüÁöÑÈï∑ÊúüË´ÆË©¢ÂæåÂæóÂà∞È©óË≠â„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂíåËß£ÈáãÁµêÊûúËàáÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ÁèæÊúâÁöÑÂèØËß£Èáã AI ÊáâÁî®ÈÄ≤Ë°å‰∫ÜÊØîËºÉÔºåÂåÖÊã¨ CKD„ÄÇÊØîËºÉË°®ÊòéÔºåÊàëÂÄëÈñãÁôºÁöÑ AI Ê®°ÂûãÔºåÁâπÂà•ÊòØÈö®Ê©üÊ£ÆÊûóÊ®°ÂûãÔºåÂ∑≤Á∂ìÁ¢∫ÂÆö‰∫ÜÊØî XgBoost Êõ¥Â§ö‰ΩúÁÇ∫ÈáçË¶ÅË≤¢ÁçªËÄÖÁöÑÁâπÂæµ„ÄÇÂèØËß£ÈáãÊÄß (I) Ë°°ÈáèÈáçË¶ÅÁâπÂæµËàáÊé©ËìãÁâπÂæµÁöÑÊØîÁéáÔºåË°®ÊòéÊàëÂÄëÁöÑ XgBoost Ê®°ÂûãÂú®ÈÄôÂÄãÊåáÊ®ô‰∏≠Áç≤Âæó‰∫ÜÊõ¥È´òÁöÑÂàÜÊï∏ÔºåÁâπÂà•ÊòØ 98% ÁöÑ‰øùÁúüÂ∫¶Ôºå‰∏¶‰∏îÂú® FII ÊåáÊï∏‰∏≠Ëá™ÁÑ∂È´òÊñºÁ´∂Áà≠Ê®°Âûã„ÄÇ

##### **Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**
2406.05984v1 by Yusif Ibrahimov, Tarique Anwar, Tommy Yuan

Mental health constitutes a complex and pervasive global challenge, affecting
millions of lives and often leading to severe consequences. In this paper, we
conduct a thorough survey to explore the intersection of data science,
artificial intelligence, and mental healthcare, focusing on the recent
developments of mental disorder detection through online social media (OSM). A
significant portion of the population actively engages in OSM platforms,
creating a vast repository of personal data that holds immense potential for
mental health analytics. The paper navigates through traditional diagnostic
methods, state-of-the-art data- and AI-driven research studies, and the
emergence of explainable AI (XAI) models for mental healthcare. We review
state-of-the-art machine learning methods, particularly those based on modern
deep learning, while emphasising the need for explainability in healthcare AI
models. The experimental design section provides insights into prevalent
practices, including available datasets and evaluation approaches. We also
identify key issues and challenges in the field and propose promising future
research directions. As mental health decisions demand transparency,
interpretability, and ethical considerations, this paper contributes to the
ongoing discourse on advancing XAI in mental healthcare through social media.
The comprehensive overview presented here aims to guide researchers,
practitioners, and policymakers in developing the area of mental disorder
detection.

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑ÊßãÊàê‰∫Ü‰∏ÄÈ†ÖË§áÈõú‰∏îÊôÆÈÅçÁöÑÂÖ®ÁêÉÊåëÊà∞ÔºåÂΩ±Èüø‰∫ÜÊï∏ÁôæËê¨‰∫∫ÁöÑÁîüÊ¥ªÔºå‰∏¶Á∂ìÂ∏∏Â∞éËá¥Âö¥ÈáçÁöÑÂæåÊûú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂæπÂ∫ïÁöÑË™øÊü•Ôºå‰ª•Êé¢Á¥¢Êï∏ÊìöÁßëÂ≠∏„ÄÅ‰∫∫Â∑•Êô∫ÊÖßÂíåÂøÉÁêÜ‰øùÂÅ•ÁöÑ‰∫§ÈõÜÔºåÈáçÈªûÈóúÊ≥®ÈÄöÈÅéÁ∑ö‰∏äÁ§æ‰∫§Â™íÈ´î (OSM) ÈÄ≤Ë°åÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨ÁöÑÊúÄÊñ∞ÁôºÂ±ï„ÄÇÂæàÂ§ß‰∏ÄÈÉ®ÂàÜ‰∫∫Âè£Á©çÊ•µÂèÉËàá OSM Âπ≥Âè∞ÔºåÂâµÈÄ†‰∫Ü‰∏ÄÂÄãÈæêÂ§ßÁöÑ‰∫∫Âì°Ë≥áÊñôÂ∫´ÔºåÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÂàÜÊûêÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÇ≥Áµ±ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÅÊúÄÂÖàÈÄ≤ÁöÑË≥áÊñôÂíå AI È©ÖÂãïÁöÑÁ†îÁ©∂Ôºå‰ª•ÂèäÂøÉÁêÜ‰øùÂÅ•‰∏≠ÂèØËß£Èáã AI (XAI) Ê®°ÂûãÁöÑÂá∫Áèæ„ÄÇÊàëÂÄëÂõûÈ°ß‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºåÁâπÂà•ÊòØÈÇ£‰∫õÂü∫ÊñºÁèæ‰ª£Ê∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÂêåÊôÇÂº∑Ë™ø‰∫ÜÈÜ´ÁôÇ‰øùÂÅ• AI Ê®°Âûã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÂØ¶È©óË®≠Ë®àÈÉ®ÂàÜÊèê‰æõ‰∫ÜÂ∞çÊôÆÈÅçÂÅöÊ≥ïÁöÑË¶ãËß£ÔºåÂåÖÊã¨ÂèØÁî®ÁöÑË≥áÊñôÈõÜÂíåË©ï‰º∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÈÇÑÊâæÂá∫Ë©≤È†òÂüüÁöÑ‰∏ªË¶ÅÂïèÈ°åÂíåÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊúâÂ∏åÊúõÁöÑÊú™‰æÜÁ†îÁ©∂ÊñπÂêë„ÄÇÁî±ÊñºÂøÉÁêÜÂÅ•Â∫∑Ê±∫Á≠ñÈúÄË¶ÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈÅìÂæ∑ËÄÉÈáèÔºåÊú¨ÊñáÊúâÂä©ÊñºÊé®ÈÄ≤ÂøÉÁêÜ‰øùÂÅ•‰∏≠ÈÄèÈÅéÁ§æ‰∫§Â™íÈ´îÊé®ÈÄ≤ XAI ÁöÑÊåÅÁ∫åË®éË´ñ„ÄÇÈÄôË£°ÊèêÂá∫ÁöÑÂÖ®Èù¢Ê¶ÇËø∞Êó®Âú®ÂºïÂ∞éÁ†îÁ©∂‰∫∫Âì°„ÄÅÂæûÊ•≠‰∫∫Âì°ÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖÁôºÂ±ïÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨È†òÂüü„ÄÇ

##### **Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**
2406.05746v1 by Zhan Zhang, Qin Zhang, Yang Jiao, Lin Lu, Lin Ma, Aihua Liu, Xiao Liu, Juan Zhao, Yajun Xue, Bing Wei, Mingxia Zhang, Ru Gao, Hong Zhao, Jie Lu, Fan Li, Yang Zhang, Yiming Wang, Lei Zhang, Fengwei Tian, Jie Hu, Xin Gou

AI-aided clinical diagnosis is desired in medical care. Existing deep
learning models lack explainability and mainly focus on image analysis. The
recently developed Dynamic Uncertain Causality Graph (DUCG) approach is
causality-driven, explainable, and invariant across different application
scenarios, without problems of data collection, labeling, fitting, privacy,
bias, generalization, high cost and high energy consumption. Through close
collaboration between clinical experts and DUCG technicians, 46 DUCG models
covering 54 chief complaints were constructed. Over 1,000 diseases can be
diagnosed without triage. Before being applied in real-world, the 46 DUCG
models were retrospectively verified by third-party hospitals. The verified
diagnostic precisions were no less than 95%, in which the diagnostic precision
for every disease including uncommon ones was no less than 80%. After
verifications, the 46 DUCG models were applied in the real-world in China. Over
one million real diagnosis cases have been performed, with only 17 incorrect
diagnoses identified. Due to DUCG's transparency, the mistakes causing the
incorrect diagnoses were found and corrected. The diagnostic abilities of the
clinicians who applied DUCG frequently were improved significantly. Following
the introduction to the earlier presented DUCG methodology, the recommendation
algorithm for potential medical checks is presented and the key idea of DUCG is
extracted.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇÁÖßË≠∑‰∏≠ÈúÄË¶Å AI ËºîÂä©ÁöÑËá®Â∫äË®∫Êñ∑„ÄÇÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏î‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂΩ±ÂÉèÂàÜÊûê„ÄÇÊúÄËøëÈñãÁôºÁöÑÂãïÊÖã‰∏çÁ¢∫ÂÆöÂõ†ÊûúÈóú‰øÇÂúñ (DUCG) ÊñπÊ≥ïÊòØÂõ†ÊûúÈ©ÖÂãïÁöÑ„ÄÅÂèØËß£ÈáãÁöÑÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊáâÁî®Â†¥ÊôØ‰∏≠ÊòØ‰∏çËÆäÁöÑÔºåÊ≤íÊúâË≥áÊñôÊî∂ÈõÜ„ÄÅÊ®ôË®ò„ÄÅÊì¨Âêà„ÄÅÈö±ÁßÅ„ÄÅÂÅèË¶ã„ÄÅÊ¶ÇÂåñ„ÄÅÈ´òÊàêÊú¨ÂíåÈ´òËÉΩËÄóÁöÑÂïèÈ°å„ÄÇÈÄöÈÅéËá®Â∫äÂ∞àÂÆ∂Âíå DUCG ÊäÄË°ì‰∫∫Âì°‰πãÈñìÁöÑÂØÜÂàáÂêà‰ΩúÔºåÊßãÂª∫‰∫ÜÊ∂µËìã 54 ÂÄã‰∏ªË®¥ÁöÑ 46 ÂÄã DUCG Ê®°Âûã„ÄÇÂèØ‰ª•Âú®Ê≤íÊúâÂàÜÊµÅÁöÑÊÉÖÊ≥Å‰∏ãË®∫Êñ∑Âá∫ 1,000 Â§öÁ®ÆÁñæÁóÖ„ÄÇÂú®ÊáâÁî®ÊñºÂØ¶Èöõ‰∏ñÁïå‰πãÂâçÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Áî±Á¨¨‰∏âÊñπÈÜ´Èô¢ÂõûÊ∫ØÊÄßÈ©óË≠â„ÄÇÈ©óË≠âÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 95%ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÁΩïË¶ãÁñæÁóÖÂú®ÂÖßÁöÑÊØèÁ®ÆÁñæÁóÖÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 80%„ÄÇÈ©óË≠âÂæåÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Âú®‰∏≠ÂúãÂØ¶ÈöõÊáâÁî®„ÄÇÂ∑≤Á∂ìÂü∑Ë°å‰∫ÜË∂ÖÈÅé‰∏ÄÁôæËê¨ÂÄãÁúüÂØ¶Ë®∫Êñ∑Ê°à‰æãÔºåÂÉÖÁôºÁèæ 17 ÂÄã‰∏çÊ≠£Á¢∫ÁöÑË®∫Êñ∑„ÄÇÁî±Êñº DUCG ÁöÑÈÄèÊòéÊÄßÔºåÁôºÁèæ‰∏¶Á≥æÊ≠£‰∫ÜÂ∞éËá¥‰∏çÊ≠£Á¢∫Ë®∫Êñ∑ÁöÑÈåØË™§„ÄÇÈ†ªÁπÅÊáâÁî® DUCG ÁöÑËá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑ËÉΩÂäõÂæóÂà∞‰∫ÜÈ°ØËëóÊèêÈ´ò„ÄÇÂú®‰ªãÁ¥π‰∫ÜÂâçÈù¢ÊèêÂá∫ÁöÑ DUCG ÊñπÊ≥ïË´ñ‰πãÂæåÔºåÊèêÂá∫‰∫ÜÊΩõÂú®ÂÅ•Â∫∑Ê™¢Êü•ÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÔºå‰∏¶ÊèêÂèñ‰∫Ü DUCG ÁöÑÈóúÈçµÊÄùÊÉ≥„ÄÇ</paragraph>

##### **Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**
2406.12897v1 by Faseela Abdullakutty, Younes Akbari, Somaya Al-Maadeed, Ahmed Bouridane, Rifat Hamoudi

It is imperative that breast cancer is detected precisely and timely to
improve patient outcomes. Diagnostic methodologies have traditionally relied on
unimodal approaches; however, medical data analytics is integrating diverse
data sources beyond conventional imaging. Using multi-modal techniques,
integrating both image and non-image data, marks a transformative advancement
in breast cancer diagnosis. The purpose of this review is to explore the
burgeoning field of multimodal techniques, particularly the fusion of
histopathology images with non-image data. Further, Explainable AI (XAI) will
be used to elucidate the decision-making processes of complex algorithms,
emphasizing the necessity of explainability in diagnostic processes. This
review utilizes multi-modal data and emphasizes explainability to enhance
diagnostic accuracy, clinician confidence, and patient engagement, ultimately
fostering more personalized treatment strategies for breast cancer, while also
identifying research gaps in multi-modality and explainability, guiding future
studies, and contributing to the strategic direction of the field.

ÊëòË¶ÅÔºöÁ≤æÁ¢∫‰∏îÂèäÊôÇÂú∞ÂÅµÊ∏¨‰π≥ÁôåÂ∞çÊñºÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇË®∫Êñ∑ÊñπÊ≥ïÂÇ≥Áµ±‰∏ä‰æùË≥¥ÊñºÂñÆ‰∏ÄÊ®°ÂºèÊñπÊ≥ïÔºõÁÑ∂ËÄåÔºåÈÜ´ÁôÇË≥áÊñôÂàÜÊûêÊ≠£Âú®Êï¥ÂêàË∂ÖË∂äÂÇ≥Áµ±ÂΩ±ÂÉèÁöÑÂêÑÁ®ÆË≥áÊñô‰æÜÊ∫ê„ÄÇ‰ΩøÁî®Êï¥ÂêàÂΩ±ÂÉèÂíåÈùûÂΩ±ÂÉèË≥áÊñôÁöÑÂ§öÊ®°ÂºèÊäÄË°ìÔºåÊ®ôË™åËëó‰π≥ÁôåË®∫Êñ∑ÁöÑËÆäÈù©ÊÄßÈÄ≤Â±ï„ÄÇÊú¨ÁØáÁ∂úËø∞ÁöÑÁõÆÁöÑÊòØÊé¢Ë®éÂ§öÊ®°ÂºèÊäÄË°ìÁöÑÊñ∞ËààÈ†òÂüüÔºåÁâπÂà•ÊòØÂ∞áÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèËàáÈùûÂΩ±ÂÉèË≥áÊñôËûçÂêà„ÄÇÊ≠§Â§ñÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∞áÁî®ÊñºÈó°ÊòéË§áÈõúÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÈÅéÁ®ãÔºåÂº∑Ë™øË®∫Êñ∑ÈÅéÁ®ã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊú¨Á∂úËø∞Âà©Áî®Â§öÊ®°ÂºèË≥áÊñô‰∏¶Âº∑Ë™øÂèØËß£ÈáãÊÄßÔºå‰ª•ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÅËá®Â∫äÈÜ´Â∏´ÁöÑ‰ø°ÂøÉÂíåÊÇ£ËÄÖÂèÉËàáÂ∫¶ÔºåÊúÄÁµÇ‰øÉÈÄ≤‰π≥ÁôåÊõ¥ÂÄã‰∫∫ÂåñÁöÑÊ≤ªÁôÇÁ≠ñÁï•ÔºåÂêåÊôÇ‰πüÊâæÂá∫Â§öÊ®°ÂºèÂíåÂèØËß£ÈáãÊÄßÁöÑÁ†îÁ©∂Â∑ÆË∑ùÔºåÂºïÂ∞éÊú™‰æÜÁöÑÁ†îÁ©∂Ôºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÁ≠ñÁï•ÊñπÂêëÂÅöÂá∫Ë≤¢Áçª„ÄÇ

##### **Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**
2406.04612v1 by Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin

The self-attention mechanism has been adopted in several widely-used
message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls
the amount of information that flows along the edges of the underlying graph.
This usage of attention has made such models a baseline for studies on
explainable AI (XAI) since interpretations via attention have been popularized
in various domains (e.g., natural language processing and computer vision).
However, existing studies often use naive calculations to derive attribution
scores from attention, and do not take the precise and careful calculation of
edge attribution into consideration. In our study, we aim to fill the gap
between the widespread usage of attention-enabled MPNNs and their potential in
largely under-explored explainability, a topic that has been actively
investigated in other areas. To this end, as the first attempt, we formalize
the problem of edge attribution from attention weights in GNNs. Then, we
propose GATT, an edge attribution calculation method built upon the computation
tree. Through comprehensive experiments, we demonstrate the effectiveness of
our proposed method when evaluating attributions from GATs. Conversely, we
empirically validate that simply averaging attention weights over graph
attention layers is insufficient to interpret the GAT model's behavior. Code is
publicly available at https://github.com/jordan7186/GAtt/tree/main.

ÊëòË¶ÅÔºöËá™Ê≥®ÊÑèÂäõÊ©üÂà∂Â∑≤Ë¢´Êé°Áî®ÊñºÂ§öÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MPNN)Ôºà‰æãÂ¶Ç GATÔºâÔºåÂÆÉÂèØ‰ª•Ëá™ÈÅ©ÊáâÂú∞ÊéßÂà∂Ê≤øËëóÂ∫ïÂ±§ÂúñÂΩ¢ÈÇäÁ∑£ÊµÅÂãïÁöÑË≥áË®äÈáè„ÄÇÈÄôÁ®ÆÊ≥®ÊÑèÂäõÁöÑ‰ΩøÁî®‰ΩøÂæóÊ≠§È°ûÊ®°ÂûãÊàêÁÇ∫ÂèØËß£Èáã AI (XAI) Á†îÁ©∂ÁöÑÂü∫Á∑öÔºåÂõ†ÁÇ∫ÈÄèÈÅéÊ≥®ÊÑèÂäõÁöÑË©ÆÈáãÂ∑≤Âú®ÂêÑÁ®ÆÈ†òÂüüÔºà‰æãÂ¶ÇËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫Ôºâ‰∏≠ÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÈÄöÂ∏∏‰ΩøÁî®Â§©ÁúüÁöÑË®àÁÆóÊñπÊ≥ïÂæûÊ≥®ÊÑèÂäõ‰∏≠Êé®Â∞éÂá∫Ê≠∏Âõ†ÂàÜÊï∏Ôºå‰∏¶‰∏îÊ≤íÊúâËÄÉÊÖÆÂà∞ÈÇäÁ∑£Ê≠∏Âõ†ÁöÑÁ≤æÁ¢∫‰∏î‰ªîÁ¥∞ÁöÑË®àÁÆó„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊó®Âú®Â°´Ë£úÊ≥®ÊÑèÂäõÂïüÁî® MPNN ÁöÑÂª£Ê≥õ‰ΩøÁî®ËàáÂÆÉÂÄëÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÂèØËß£ÈáãÊÄß‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄôÂÄã‰∏ªÈ°åÂ∑≤Âú®ÂÖ∂‰ªñÈ†òÂüüÁ©çÊ•µÁ†îÁ©∂„ÄÇÁÇ∫Ê≠§Ôºå‰ΩúÁÇ∫Á¨¨‰∏ÄÊ¨°ÂòóË©¶ÔºåÊàëÂÄëÂ∞á GNN ‰∏≠Ê≥®ÊÑèÂäõÊ¨äÈáçÁöÑÈÇäÁ∑£Ê≠∏Âõ†ÂïèÈ°åÂΩ¢ÂºèÂåñ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫ GATTÔºå‰∏ÄÁ®ÆÂª∫Á´ãÂú®Ë®àÁÆóÊ®π‰∏äÁöÑÈÇäÁ∑£Ê≠∏Âõ†Ë®àÁÆóÊñπÊ≥ï„ÄÇÈÄèÈÅéÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë©ï‰º∞ GAT ÁöÑÊ≠∏Âõ†ÊôÇÊâÄÂÖ∑ÊúâÁöÑÊïàÊûú„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÊÜëÁ∂ìÈ©óÈ©óË≠â‰∫ÜÂÉÖÂ∞çÂúñÊ≥®ÊÑèÂäõÂ±§‰∏äÁöÑÊ≥®ÊÑèÂäõÊ¨äÈáçÂèñÂπ≥ÂùáÂÄº‰∏çË∂≥‰ª•Ë©ÆÈáã GAT Ê®°ÂûãÁöÑË°åÁÇ∫„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jordan7186/GAtt/tree/main„ÄÇ

##### **Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**
2406.16908v3 by Dinuka Sandun Udayantha, Kavindu Weerasinghe, Nima Wickramasinghe, Akila Abeyratne, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Anjula De Silva, Chamira U. S. Edussooriya

The neonatal period is the most vulnerable time for the development of
seizures. Seizures in the immature brain lead to detrimental consequences,
therefore require early diagnosis. The gold-standard for neonatal seizure
detection currently relies on continuous video-EEG monitoring; which involves
recording multi-channel electroencephalogram (EEG) alongside real-time video
monitoring within a neonatal intensive care unit (NICU). However, video-EEG
monitoring technology requires clinical expertise and is often limited to
technologically advanced and resourceful settings. Cost-effective new
techniques could help the medical fraternity make an accurate diagnosis and
advocate treatment without delay. In this work, a novel explainable deep
learning model to automate the neonatal seizure detection process with a
reduced EEG montage is proposed, which employs convolutional nets, graph
attention layers, and fully connected layers. Beyond its ability to detect
seizures in real-time with a reduced montage, this model offers the unique
advantage of real-time interpretability. By evaluating the performance on the
Zenodo dataset with 10-fold cross-validation, the presented model achieves an
absolute improvement of 8.31% and 42.86% in area under curve (AUC) and recall,
respectively.

ÊëòË¶ÅÔºöÊñ∞ÁîüÂÖíÊúüÊòØÂ§ßËÖ¶ÁôºËÇ≤ÊúÄËÑÜÂº±ÁöÑÊôÇÊúüÔºåÂÆπÊòìÂá∫ÁèæÁô≤ÁôáÁôº‰Ωú„ÄÇÂ§ßËÖ¶ÁôºËÇ≤‰∏çÊàêÁÜüÊôÇÂá∫ÁèæÁô≤ÁôáÁôº‰ΩúÊúÉÈÄ†Êàê‰∏çËâØÂæåÊûúÔºåÂõ†Ê≠§ÈúÄË¶ÅÂèäÊó©Ë®∫Êñ∑„ÄÇÁõÆÂâçÊñ∞ÁîüÂÖíÁô≤ÁôáÁôº‰ΩúÁöÑÈªÉÈáëÊ®ôÊ∫ñ‰æùË≥¥ÊñºÈÄ£Á∫åÁöÑË¶ñË®äËÖ¶ÈõªÂúñ (EEG) Áõ£Ê∏¨ÔºõÂÖ∂‰∏≠ÂåÖÊã¨Âú®Êñ∞ÁîüÂÖíÂä†Ë≠∑ÁóÖÊàø (NICU) ÂÖßÂêåÊôÇÈÄ≤Ë°åÂ§öÈ†ªÈÅìËÖ¶ÈõªÂúñ (EEG) Ë®òÈåÑÂíåÂç≥ÊôÇË¶ñË®äÁõ£Êéß„ÄÇÁÑ∂ËÄåÔºåË¶ñË®äËÖ¶ÈõªÂúñÁõ£ÊéßÊäÄË°ìÈúÄË¶ÅËá®Â∫äÂ∞àÊ•≠Áü•Ë≠òÔºåËÄå‰∏îÈÄöÂ∏∏ÂÉÖÈôêÊñºÊäÄË°ìÂÖàÈÄ≤‰∏îË≥áÊ∫êË±êÂØåÁöÑÁí∞Â¢É„ÄÇÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊñ∞ÊäÄË°ìÂèØ‰ª•Âπ´Âä©ÈÜ´ÁôÇÁïåÊ∫ñÁ¢∫Ë®∫Êñ∑‰∏¶Á´ãÂç≥ÊèêÂÄ°Ê≤ªÁôÇ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂèØËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•Ëá™ÂãïÂåñÊñ∞ÁîüÂÖíÁô≤ÁôáÁôº‰ΩúÂÅµÊ∏¨ÈÅéÁ®ãÔºå‰∏¶Êé°Áî®Ê∏õÂ∞ëÁöÑËÖ¶ÈõªÂúñË£ùÁΩÆÔºåÂÖ∂‰∏≠Êé°Áî®‰∫ÜÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅÂúñÂΩ¢Ê≥®ÊÑèÂäõÂ±§ÂíåÂÖ®ÈÄ£Êé•Â±§„ÄÇÈô§‰∫ÜËÉΩÂ§†‰ΩøÁî®Ê∏õÂ∞ëÁöÑË£ùÁΩÆÂç≥ÊôÇÂÅµÊ∏¨Áô≤ÁôáÁôº‰ΩúÂ§ñÔºåÊ≠§Ê®°ÂûãÈÇÑÊèê‰æõ‰∫ÜÂç≥ÊôÇÂèØËß£ÈáãÊÄßÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÈÄèÈÅéÂú® Zenodo Ë≥áÊñôÈõÜ‰∏ä‰ΩøÁî® 10 ÂÄç‰∫§ÂèâÈ©óË≠âË©ï‰º∞ÊïàËÉΩÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú®Êõ≤Á∑ö‰∏ãÈù¢Á©ç (AUC) ÂíåÂè¨ÂõûÁéáÊñπÈù¢ÂàÜÂà•ÈÅîÂà∞‰∫Ü 8.31% Âíå 42.86% ÁöÑÁµïÂ∞çÊîπÂñÑ„ÄÇ

##### **Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**
2406.00532v1 by Samita Bai, Sidra Nasir, Rizwan Ahmed Khan, Sheeraz Arif, Alexandre Meyer, Hubert Konik

Breast cancer (BC) stands as one of the most common malignancies affecting
women worldwide, necessitating advancements in diagnostic methodologies for
better clinical outcomes. This article provides a comprehensive exploration of
the application of Explainable Artificial Intelligence (XAI) techniques in the
detection and diagnosis of breast cancer. As Artificial Intelligence (AI)
technologies continue to permeate the healthcare sector, particularly in
oncology, the need for transparent and interpretable models becomes imperative
to enhance clinical decision-making and patient care. This review discusses the
integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and
others, with machine learning and deep learning models utilized in breast
cancer detection and classification. By investigating the modalities of breast
cancer datasets, including mammograms, ultrasounds and their processing with
AI, the paper highlights how XAI can lead to more accurate diagnoses and
personalized treatment plans. It also examines the challenges in implementing
these techniques and the importance of developing standardized metrics for
evaluating XAI's effectiveness in clinical settings. Through detailed analysis
and discussion, this article aims to highlight the potential of XAI in bridging
the gap between complex AI models and practical healthcare applications,
thereby fostering trust and understanding among medical professionals and
improving patient outcomes.

ÊëòË¶ÅÔºö‰π≥Áôå (BC) ÊòØÂΩ±ÈüøÂÖ®ÁêÉÂ•≥ÊÄßÊúÄÂ∏∏Ë¶ãÁöÑÊÉ°ÊÄßËÖ´Áò§‰πã‰∏ÄÔºåÂõ†Ê≠§ÈúÄË¶ÅÈÄ≤Ê≠•ÁöÑË®∫Êñ∑ÊñπÊ≥ïÔºå‰ª•ÊîπÂñÑËá®Â∫äÁµêÊûú„ÄÇÊú¨ÊñáÂÖ®Èù¢Êé¢Ë®é‰∫ÜÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ìÂú®‰π≥ÁôåÂÅµÊ∏¨ÂíåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊåÅÁ∫åÊª≤ÈÄèÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÁâπÂà•ÊòØÂú®ËÖ´Áò§Â≠∏‰∏≠ÔºåÈÄèÊòé‰∏îÂèØËß£ÈáãÁöÑÊ®°ÂûãÈúÄÊ±ÇËÆäÂæóÂã¢Âú®ÂøÖË°åÔºå‰ª•Â¢ûÂº∑Ëá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂíåÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§ÁØáË©ïË´ñÊé¢Ë®é‰∫ÜÂêÑÁ®Æ XAI ÊñπÊ≥ïÁöÑÊï¥ÂêàÔºå‰æãÂ¶Ç SHAP„ÄÅLIME„ÄÅGrad-CAM Á≠âÔºå‰ª•ÂèäÁî®Êñº‰π≥ÁôåÂÅµÊ∏¨ÂíåÂàÜÈ°ûÁöÑÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄèÈÅéÊé¢Ë®é‰π≥ÁôåË≥áÊñôÈõÜÁöÑÊ®°ÂºèÔºåÂåÖÊã¨‰π≥ÊàøÊîùÂΩ±„ÄÅË∂ÖÈü≥Ê≥¢ÂèäÂÖ∂Âú® AI ‰∏≠ÁöÑËôïÁêÜÔºåÊú¨ÊñáÈáçÈªûË™™Êòé XAI Â¶Ç‰ΩïËÉΩÂ∞éËá¥Êõ¥Ê∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇË®àÁï´„ÄÇÂÆÉ‰πüÊé¢Ë®é‰∫ÜÂØ¶ÊñΩÈÄô‰∫õÊäÄË°ìÁöÑÊåëÊà∞Ôºå‰ª•ÂèäÂà∂ÂÆöÊ®ôÊ∫ñÂåñË©ïÈáèÊåáÊ®ô‰ª•Ë©ï‰º∞ XAI Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄèÈÅéË©≥Á¥∞ÁöÑÂàÜÊûêÂíåË®éË´ñÔºåÊú¨ÊñáÊó®Âú®Âº∑Ë™ø XAI Âú®Á∏ÆÂ∞èË§áÈõú AI Ê®°ÂûãËàáÂØ¶ÂãôÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰πãÈñìÂ∑ÆË∑ùÁöÑÊΩõÂäõÔºåÈÄ≤ËÄå‰øÉÈÄ≤ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°‰πãÈñìÁöÑ‰ø°‰ªªËàáÁêÜËß£Ôºå‰∏¶ÊîπÂñÑÊÇ£ËÄÖÁöÑÁµêÊûú„ÄÇ

##### **Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**
2406.01624v2 by Alaa Nfissi, Wassim Bouachir, Nizar Bouguila, Brian Mishara

Speech emotion recognition (SER) has gained significant attention due to its
several application fields, such as mental health, education, and
human-computer interaction. However, the accuracy of SER systems is hindered by
high-dimensional feature sets that may contain irrelevant and redundant
information. To overcome this challenge, this study proposes an iterative
feature boosting approach for SER that emphasizes feature relevance and
explainability to enhance machine learning model performance. Our approach
involves meticulous feature selection and analysis to build efficient SER
systems. In addressing our main problem through model explainability, we employ
a feature evaluation loop with Shapley values to iteratively refine feature
sets. This process strikes a balance between model performance and
transparency, which enables a comprehensive understanding of the model's
predictions. The proposed approach offers several advantages, including the
identification and removal of irrelevant and redundant features, leading to a
more effective model. Additionally, it promotes explainability, facilitating
comprehension of the model's predictions and the identification of crucial
features for emotion determination. The effectiveness of the proposed method is
validated on the SER benchmarks of the Toronto emotional speech set (TESS),
Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of
Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion
(SAVEE) datasets, outperforming state-of-the-art methods. To the best of our
knowledge, this is the first work to incorporate model explainability into an
SER framework. The source code of this paper is publicly available via this
https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.

ÊëòË¶ÅÔºöË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) Áî±ÊñºÂÖ∂Âú®ÂøÉÁêÜÂÅ•Â∫∑„ÄÅÊïôËÇ≤Âíå‰∫∫Ê©ü‰∫íÂãïÁ≠âÂ§öÂÄãÊáâÁî®È†òÂüüËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåSER Á≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂèóÂà∞È´òÁ∂≠ÁâπÂæµÈõÜÁöÑÈòªÁ§ôÔºåÈÄô‰∫õÁâπÂæµÈõÜÂèØËÉΩÂåÖÂê´‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑË≥áË®ä„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÊåëÊà∞ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁî®Êñº SER ÁöÑËø≠‰ª£ÁâπÂæµÊèêÂçáÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂº∑Ë™øÁâπÂæµÁõ∏ÈóúÊÄßÂíåÂèØËß£ÈáãÊÄßÔºå‰ª•Â¢ûÂº∑Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰ªîÁ¥∞ÁöÑÁâπÂæµÈÅ∏ÊìáÂíåÂàÜÊûêÔºå‰ª•Âª∫Á´ãÈ´òÊïàÁöÑ SER Á≥ªÁµ±„ÄÇÁÇ∫‰∫ÜÈÄèÈÅéÊ®°ÂûãÂèØËß£ÈáãÊÄßËß£Ê±∫ÊàëÂÄëÁöÑÊ†∏ÂøÉÂïèÈ°åÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂÖ∑Êúâ Shapley ÂÄºÁöÑÁâπÂæµË©ï‰º∞Ëø¥ÂúàÔºå‰ª•ÂèçË¶ÜÊîπÂñÑÁâπÂæµÈõÜ„ÄÇÈÄôÂÄãÈÅéÁ®ãÂú®Ê®°ÂûãÊïàËÉΩÂíåÈÄèÊòéÂ∫¶‰πãÈñìÂèñÂæóÂπ≥Ë°°ÔºåÈÄô‰ΩøÂæóÊàëÂÄëËÉΩÂ§†ÂÖ®Èù¢‰∫ÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂ§öÈ†ÖÂÑ™ÈªûÔºåÂåÖÊã¨Ë≠òÂà•ÂíåÁßªÈô§‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑÁâπÂæµÔºåÂæûËÄåÂª∫Á´ãÊõ¥ÊúâÊïàÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰øÉÈÄ≤‰∫ÜÂèØËß£ÈáãÊÄßÔºåÊúâÂä©ÊñºÁêÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨‰ª•ÂèäË≠òÂà•ÊÉÖÁ∑íÊ±∫ÂÆöÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤Âú®Â§öÂÄ´Â§öÊÉÖÁ∑íË™ûÈü≥ÈõÜ (TESS)„ÄÅÊüèÊûóÊÉÖÁ∑íË™ûÈü≥Ë≥áÊñôÂ∫´ (EMO-DB)„ÄÅË≥¥ÁàæÊ£ÆÈü≥Ë®äË¶ñË¶∫ÊÉÖÁ∑íË™ûÈü≥ÂíåÊ≠åÊõ≤Ë≥áÊñôÂ∫´ (RAVDESS) ÂíåËñ©ÈáåÈü≥Ë®äË¶ñË¶∫Ë°®ÈÅîÊÉÖÁ∑í (SAVEE) Ë≥áÊñôÈõÜÁöÑ SER Âü∫Ê∫ñ‰∏äÂæóÂà∞È©óË≠âÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÊ®°ÂûãÂèØËß£ÈáãÊÄßÁ¥çÂÖ• SER Êû∂ÊßãÁöÑÁ†îÁ©∂„ÄÇÊú¨ÊñáÁöÑÂéüÂßãÁ¢ºÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂÖ¨ÈñãÂèñÂæóÔºöhttps://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition„ÄÇ

##### **The Explanation Necessity for Healthcare AI**
2406.00216v1 by Michail Mamalakis, H√©lo√Øse de Vareilles, Graham Murray, Pietro Lio, John Suckling

Explainability is often critical to the acceptable implementation of
artificial intelligence (AI). Nowhere is this more important than healthcare
where decision-making directly impacts patients and trust in AI systems is
essential. This trust is often built on the explanations and interpretations
the AI provides. Despite significant advancements in AI interpretability, there
remains the need for clear guidelines on when and to what extent explanations
are necessary in the medical context. We propose a novel categorization system
with four distinct classes of explanation necessity, guiding the level of
explanation required: patient or sample (local) level, cohort or dataset
(global) level, or both levels. We introduce a mathematical formulation that
distinguishes these categories and offers a practical framework for researchers
to determine the necessity and depth of explanations required in medical AI
applications. Three key factors are considered: the robustness of the
evaluation protocol, the variability of expert observations, and the
representation dimensionality of the application. In this perspective, we
address the question: When does an AI medical application need to be explained,
and at what level of detail?

ÊëòË¶ÅÔºöÂèØËß£ÈáäÊÄßÈÄöÂ∏∏ÂØπ‰∫é‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØÊé•ÂèóÂÆûÊñΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÔºåËøô‰∏ÄÁÇπÂ∞§‰∏∫ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÜ≥Á≠ñÁõ¥Êé•ÂΩ±ÂìçÊÇ£ËÄÖÔºåÂπ∂‰∏îÂØπ AI Á≥ªÁªüÁöÑ‰ø°‰ªªËá≥ÂÖ≥ÈáçË¶Å„ÄÇËøôÁßç‰ø°‰ªªÈÄöÂ∏∏Âª∫Á´ãÂú® AI Êèê‰æõÁöÑËß£ÈáäÂíåËØ†Èáä‰πã‰∏ä„ÄÇÂ∞ΩÁÆ° AI ÂèØËß£ÈáäÊÄßÂèñÂæó‰∫ÜÈáçÂ§ßËøõÂ±ïÔºå‰ΩÜ‰ªçÁÑ∂ÈúÄË¶ÅÊòéÁ°ÆÁöÑÊåáÂØºÊñπÈíàÔºåËØ¥ÊòéÂú®ÂåªÁñóÁéØÂ¢É‰∏≠‰ΩïÊó∂‰ª•ÂèäÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÈúÄË¶ÅËß£Èáä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂàÜÁ±ªÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÂÖ∑ÊúâÂõõÁßç‰∏çÂêåÁöÑËß£ÈáäÂøÖË¶ÅÊÄßÁ±ªÂà´ÔºåÊåáÂØºÊâÄÈúÄÁöÑËß£ÈáäÁ∫ßÂà´ÔºöÊÇ£ËÄÖÊàñÊ†∑Êú¨ÔºàÂ±ÄÈÉ®ÔºâÁ∫ßÂà´„ÄÅÈòüÂàóÊàñÊï∞ÊçÆÈõÜÔºàÂÖ®Â±ÄÔºâÁ∫ßÂà´ÔºåÊàñ‰∏§‰∏™Á∫ßÂà´„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êï∞Â≠¶ÂÖ¨ÂºèÔºåËØ•ÂÖ¨ÂºèÂå∫ÂàÜ‰∫ÜËøô‰∫õÁ±ªÂà´ÔºåÂπ∂‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÆûÁî®Ê°ÜÊû∂Ôºå‰ª•Á°ÆÂÆöÂåªÁñó AI Â∫îÁî®‰∏≠ÊâÄÈúÄÁöÑËß£ÈáäÁöÑÂøÖË¶ÅÊÄßÂíåÊ∑±Â∫¶„ÄÇËÄÉËôë‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöËØÑ‰º∞ÂçèËÆÆÁöÑÁ®≥ÂÅ•ÊÄß„ÄÅ‰∏ìÂÆ∂ËßÇÂØüÁöÑÂèØÂèòÊÄß‰ª•ÂèäÂ∫îÁî®Á®ãÂ∫èÁöÑË°®Á§∫Áª¥Êï∞„ÄÇ‰ªéËøô‰∏™ËßíÂ∫¶Êù•ÁúãÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºöAI ÂåªÁñóÂ∫îÁî®‰ΩïÊó∂ÈúÄË¶ÅËß£ÈáäÔºå‰ª•ÂèäÈúÄË¶ÅËß£ÈáäÂà∞‰ΩïÁßçÁ®ãÂ∫¶Ôºü

##### **Interdisciplinary Expertise to Advance Equitable Explainable AI**
2406.18563v1 by Chloe R. Bennett, Heather Cole-Lewis, Stephanie Farquhar, Naama Haamel, Boris Babenko, Oran Lang, Mat Fleck, Ilana Traynis, Charles Lau, Ivor Horn, Courtney Lyles

The field of artificial intelligence (AI) is rapidly influencing health and
healthcare, but bias and poor performance persists for populations who face
widespread structural oppression. Previous work has clearly outlined the need
for more rigorous attention to data representativeness and model performance to
advance equity and reduce bias. However, there is an opportunity to also
improve the explainability of AI by leveraging best practices of social
epidemiology and health equity to help us develop hypotheses for associations
found. In this paper, we focus on explainable AI (XAI) and describe a framework
for interdisciplinary expert panel review to discuss and critically assess AI
model explanations from multiple perspectives and identify areas of bias and
directions for future research. We emphasize the importance of the
interdisciplinary expert panel to produce more accurate, equitable
interpretations which are historically and contextually informed.
Interdisciplinary panel discussions can help reduce bias, identify potential
confounders, and identify opportunities for additional research where there are
gaps in the literature. In turn, these insights can suggest opportunities for
AI model improvement.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) È†òÂüüÊ≠£Âø´ÈÄüÂΩ±ÈüøËëóÂÅ•Â∫∑ËàáÈÜ´ÁôÇ‰øùÂÅ•Ôºå‰ΩÜÂ∞çÊñºÈù¢Ëá®Âª£Ê≥õÁµêÊßãÊÄßÂ£ìËø´ÁöÑ‰∫∫Áæ§‰æÜË™™ÔºåÂÅèË¶ãÂíå‰∏çËâØË°®Áèæ‰æùÁÑ∂Â≠òÂú®„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ê∏ÖÊ•öË™™ÊòéÔºåÈúÄË¶ÅÊõ¥Âö¥Ê†ºÂú∞Ê≥®ÊÑèË≥áÊñô‰ª£Ë°®ÊÄßÂíåÊ®°ÂûãÊïàËÉΩÔºå‰ª•‰øÉÈÄ≤ÂÖ¨Âπ≥ÊÄß‰∏¶Ê∏õÂ∞ëÂÅèË¶ã„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÊúâÊ©üÊúÉÈÄèÈÅéÈÅãÁî®Á§æÊúÉÊµÅË°åÁóÖÂ≠∏ÂíåÂÅ•Â∫∑ÂÖ¨Âπ≥ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÔºå‰æÜÊîπÂñÑ AI ÁöÑÂèØËß£ÈáãÊÄßÔºå‰ª•Âπ´Âä©ÊàëÂÄëÈáùÂ∞çÁôºÁèæÁöÑÈóúËÅØÊÄßÔºåÁôºÂ±ïÂÅáË®≠„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂèØËß£Èáã AI (XAI)Ôºå‰∏¶ÊèèËø∞‰∏ÄÂÄãË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂØ©Êü•Êû∂ÊßãÔºå‰ª•ÂæûÂ§öÈáçËßÄÈªûË®éË´ñÂíåÊâπÂà§ÊÄßË©ï‰º∞ AI Ê®°ÂûãÁöÑËß£ÈáãÔºå‰∏¶ÊâæÂá∫ÂÅèË¶ãÈ†òÂüüÂíåÊú™‰æÜÁ†îÁ©∂ÁöÑÊñπÂêë„ÄÇÊàëÂÄëÂº∑Ë™øË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂ∞çÊñºÁî¢ÁîüÊõ¥Ê∫ñÁ¢∫„ÄÅÂÖ¨Âπ≥ÁöÑË©ÆÈáãËá≥ÈóúÈáçË¶ÅÔºåËÄåÈÄô‰∫õË©ÆÈáãÊòØÊ†πÊìöÊ≠∑Âè≤ÂíåËÑàÁµ°ËÄå‰æÜÁöÑ„ÄÇË∑®È†òÂüüÂ∞èÁµÑË®éË´ñÊúâÂä©ÊñºÊ∏õÂ∞ëÂÅèË¶ã„ÄÅÊâæÂá∫ÊΩõÂú®ÁöÑÊ∑∑Ê∑ÜÂõ†Á¥†Ôºå‰∏¶Âú®ÊñáÁçª‰∏≠ÊúâÁº∫Âè£ÊôÇÊâæÂá∫È°çÂ§ñÁ†îÁ©∂ÁöÑÊ©üÊúÉ„ÄÇÂèçÈÅé‰æÜÔºåÈÄô‰∫õË¶ãËß£ÂèØ‰ª•Âª∫Ë≠∞ AI Ê®°ÂûãÊîπÈÄ≤ÁöÑÊ©üÊúÉ„ÄÇ

##### **"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**
2407.11978v1 by Hubert D. ZajƒÖc, Jorge M. N. Ribeiro, Silvia Ingala, Simona Gentile, Ruth Wanjohi, Samuel N. Gitau, Jonathan F. Carlsen, Michael B. Nielsen, Tariq O. Andersen

Artificial Intelligence (AI) repeatedly match or outperform radiologists in
lab experiments. However, real-world implementations of radiological AI-based
systems are found to provide little to no clinical value. This paper explores
how to design AI for clinical usefulness in different contexts. We conducted 19
design sessions and design interventions with 13 radiologists from 7 clinical
sites in Denmark and Kenya, based on three iterations of a functional AI-based
prototype. Ten sociotechnical dependencies were identified as crucial for the
design of AI in radiology. We conceptualised four technical dimensions that
must be configured to the intended clinical context of use: AI functionality,
AI medical focus, AI decision threshold, and AI Explainability. We present four
design recommendations on how to address dependencies pertaining to the medical
knowledge, clinic type, user expertise level, patient context, and user
situation that condition the configuration of these technical dimensions.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂú®ÂØ¶È©óÂÆ§ÂØ¶È©ó‰∏≠‰∏çÊñ∑Âú∞ËàáÊîæÂ∞ÑÁßëÈÜ´Â∏´ÂåπÊïµÊàñË°®ÁèæÂæóÊõ¥Âá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÊîæÂ∞ÑÁßë AI ÁÇ∫Âü∫Á§éÁ≥ªÁµ±ÁöÑÂØ¶ÈöõÂü∑Ë°åÂπæ‰πéÊ≤íÊúâÊèê‰æõËá®Â∫äÂÉπÂÄº„ÄÇÊú¨ÊñáÊé¢Ë®éÂ¶Ç‰ΩïÁÇ∫ AI Ë®≠Ë®àÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠Ëá®Â∫ä‰∏äÁöÑÊïàÁî®„ÄÇÊàëÂÄëÊ†πÊìöÂäüËÉΩÊÄß AI ÁÇ∫Âü∫Á§éÂéüÂûãÁöÑ‰∏âÊ¨°Ëø≠‰ª£ÔºåÂú®‰∏πÈ∫•ÂíåËÇØ‰∫ûÁöÑ 7 ÂÄãËá®Â∫äÂ†¥ÂüüËàá 13 ‰ΩçÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°å‰∫Ü 19 Ê¨°Ë®≠Ë®àÊúÉË≠∞ÂíåË®≠Ë®à‰ªãÂÖ•„ÄÇÂçÅÂÄãÁ§æÊúÉÊäÄË°ì‰æùË≥¥Èóú‰øÇË¢´Ë™çÁÇ∫Â∞çÊñºÊîæÂ∞ÑÁßë‰∏≠ AI ÁöÑË®≠Ë®àËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊ¶ÇÂøµÂåñ‰∫ÜÂõõÂÄãÊäÄË°ìÈù¢ÂêëÔºåÂøÖÈ†àÊ†πÊìöÈ†êÊúüÁöÑËá®Â∫ä‰ΩøÁî®ÊÉÖÂ¢ÉÈÄ≤Ë°åË®≠ÂÆöÔºöAI ÂäüËÉΩ„ÄÅAI ÈÜ´ÁôÇÈáçÈªû„ÄÅAI Ê±∫Á≠ñÈñÄÊ™ªÔºå‰ª•Âèä AI ÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ÂõõÈ†ÖË®≠Ë®àÂª∫Ë≠∞ÔºåË™™ÊòéÂ¶Ç‰ΩïËôïÁêÜËàáÈÜ´ÁôÇÁü•Ë≠ò„ÄÅË®∫ÊâÄÈ°ûÂûã„ÄÅ‰ΩøÁî®ËÄÖÂ∞àÊ•≠Áü•Ë≠òÁ≠âÁ¥ö„ÄÅÊÇ£ËÄÖÊÉÖÂ¢ÉÔºå‰ª•ÂèäÂΩ±ÈüøÈÄô‰∫õÊäÄË°ìÈù¢ÂêëË®≠ÂÆöÁöÑ‰ΩøÁî®ËÄÖÊÉÖÂ¢ÉÁõ∏ÈóúÁöÑ‰æùË≥¥Èóú‰øÇ„ÄÇ

##### **Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**
2405.16424v1 by Min Hun Lee, Silvana Xin Yi Choo, Shamala D/O Thilarajah

With advanced AI/ML, there has been growing research on explainable AI (XAI)
and studies on how humans interact with AI and XAI for effective human-AI
collaborative decision-making. However, we still have a lack of understanding
of how AI systems and XAI should be first presented to users without technical
backgrounds. In this paper, we present the findings of semi-structured
interviews with health professionals (n=12) and students (n=4) majoring in
medicine and health to study how to improve onboarding with AI and XAI. For the
interviews, we built upon human-AI interaction guidelines to create onboarding
materials of an AI system for stroke rehabilitation assessment and AI
explanations and introduce them to the participants. Our findings reveal that
beyond presenting traditional performance metrics on AI, participants desired
benchmark information, the practical benefits of AI, and interaction trials to
better contextualize AI performance, and refine the objectives and performance
of AI. Based on these findings, we highlight directions for improving
onboarding with AI and XAI and human-AI collaborative decision-making.

ÊëòË¶ÅÔºöÈö®ËëóÂÖàÈÄ≤ÁöÑ AI/MLÔºåÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂‰∏çÊñ∑Â¢ûÂä†Ôºå‰ª•ÂèäÈóúÊñº‰∫∫È°ûÂ¶Ç‰ΩïËàá AI Âíå XAI ‰∫íÂãï‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰ªçÁÑ∂Áº∫‰πèÂ∞ç AI Á≥ªÁµ±Âíå XAI ÊáâÂ¶Ç‰ΩïÈ¶ñÂÖàÂëàÁèæÁµ¶Ê≤íÊúâÊäÄË°ìËÉåÊôØÁöÑÁî®Êà∂ÁöÑ‰∫ÜËß£„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì° (n=12) Âíå‰∏ª‰øÆÈÜ´Â≠∏ÂíåÂÅ•Â∫∑ÁöÑÂ≠∏Áîü (n=4) ÈÄ≤Ë°åÂçäÁµêÊßãÂåñË®™Ë´áÁöÑÁµêÊûúÔºå‰ª•Á†îÁ©∂Â¶Ç‰ΩïÊîπÂñÑ AI Âíå XAI ÁöÑÂÖ•ÈñÄ„ÄÇÂ∞çÊñºË®™Ë´áÔºåÊàëÂÄëÂª∫Á´ãÂú®‰∫∫Ê©ü‰∫íÂãïÊ∫ñÂâá‰πã‰∏äÔºåÁÇ∫‰∏≠È¢®Â∫∑Âæ©Ë©ï‰º∞Âíå AI Ëß£ÈáãÁöÑ AI Á≥ªÁµ±ÂâµÂª∫ÂÖ•ÈñÄÊùêÊñôÔºå‰∏¶Â∞áÂÆÉÂÄë‰ªãÁ¥πÁµ¶ÂèÉËàáËÄÖ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈô§‰∫ÜÂëàÁèæÂÇ≥Áµ±ÁöÑ AI ÊÄßËÉΩÊåáÊ®ôÂ§ñÔºåÂèÉËàáËÄÖÈÇÑÂ∏åÊúõÂü∫ÂáÜ‰ø°ÊÅØ„ÄÅAI ÁöÑÂØ¶ÈöõÂ•ΩËôï‰ª•Âèä‰∫§‰∫íË©¶È©óÔºå‰ª•Êõ¥Â•ΩÂú∞Â∞á AI ÊÄßËÉΩÊÉÖÂ¢ÉÂåñÔºå‰∏¶ÂÆåÂñÑ AI ÁöÑÁõÆÊ®ôÂíåÊÄßËÉΩ„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ AI Âíå XAI ‰ª•Âèä‰∫∫Ê©üÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆöÁöÑÂÖ•ÈñÄÊñπÂêë„ÄÇ

##### **Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**
2405.17502v1 by Ziming Liu, Longjian Liu, Robert E. Heidel, Xiaopeng Zhao

This article uses machine learning (ML) and explainable artificial
intelligence (XAI) techniques to investigate the relationship between
nutritional status and mortality rates associated with Alzheimers disease (AD).
The Third National Health and Nutrition Examination Survey (NHANES III)
database is employed for analysis. The random forest model is selected as the
base model for XAI analysis, and the Shapley Additive Explanations (SHAP)
method is used to assess feature importance. The results highlight significant
nutritional factors such as serum vitamin B12 and glycated hemoglobin. The
study demonstrates the effectiveness of random forests in predicting AD
mortality compared to other diseases. This research provides insights into the
impact of nutrition on AD and contributes to a deeper understanding of disease
progression.

ÊëòË¶ÅÔºöÊú¨Êñá‰ΩøÁî®Ê©üÂô®Â≠∏Áøí (ML) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ì‰æÜÊé¢Ë®éÁáüÈ§äÁãÄÊ≥ÅËàáÈòøËå≤Êµ∑ÈªòÁóá (AD) Áõ∏ÈóúÁöÑÊ≠ª‰∫°Áéá‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊé°Áî®Á¨¨‰∏âÊ¨°ÂÖ®ÂúãÂÅ•Â∫∑ËàáÁáüÈ§äÊ™¢Êü•Ë™øÊü• (NHANES III) Ë≥áÊñôÂ∫´ÈÄ≤Ë°åÂàÜÊûê„ÄÇÈÅ∏ÊìáÈö®Ê©üÊ£ÆÊûóÊ®°Âûã‰ΩúÁÇ∫ XAI ÂàÜÊûêÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰∏¶‰ΩøÁî® Shapley Additive Explanations (SHAP) ÊñπÊ≥ï‰æÜË©ï‰º∞ÁâπÂæµÈáçË¶ÅÊÄß„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÈáçË¶ÅÁöÑÁáüÈ§äÂõ†Á¥†Ôºå‰æãÂ¶ÇË°ÄÊ∏ÖÁ∂≠ÁîüÁ¥† B12 ÂíåÁ≥ñÂåñË°ÄÁ¥ÖËõãÁôΩ„ÄÇË©≤Á†îÁ©∂Ë≠âÊòé‰∫ÜÈö®Ê©üÊ£ÆÊûóÂú®È†êÊ∏¨ AD Ê≠ª‰∫°ÁéáÊñπÈù¢Áõ∏ËºÉÊñºÂÖ∂‰ªñÁñæÁóÖÁöÑÊúâÊïàÊÄß„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÁáüÈ§äÂ∞ç AD ÁöÑÂΩ±ÈüøÁöÑË¶ãËß£Ôºå‰∏¶ÊúâÂä©ÊñºÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÁñæÁóÖÁöÑÈÄ≤Â±ï„ÄÇ

##### **Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**
2407.11974v1 by Catalina Gomez, Ruolin Wang, Katharina Breininger, Corinne Casey, Chris Bradley, Mitchell Pavlak, Alex Pham, Jithin Yohannan, Mathias Unberath

Primary care providers are vital for initial triage and referrals to
specialty care. In glaucoma, asymptomatic and fast progression can lead to
vision loss, necessitating timely referrals to specialists. However, primary
eye care providers may not identify urgent cases, potentially delaying care.
Artificial Intelligence (AI) offering explanations could enhance their referral
decisions. We investigate how various AI explanations help providers
distinguish between patients needing immediate or non-urgent specialist
referrals. We built explainable AI algorithms to predict glaucoma surgery needs
from routine eyecare data as a proxy for identifying high-risk patients. We
incorporated intrinsic and post-hoc explainability and conducted an online
study with optometrists to assess human-AI team performance, measuring referral
accuracy and analyzing interactions with AI, including agreement rates, task
time, and user experience perceptions. AI support enhanced referral accuracy
among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams
underperformed compared to AI alone. Participants believed they included AI
advice more when using the intrinsic model, and perceived it more useful and
promising. Without explanations, deviations from AI recommendations increased.
AI support did not increase workload, confidence, and trust, but reduced
challenges. On a separate test set, our black-box and intrinsic models achieved
an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We
identify opportunities of human-AI teaming for glaucoma management in primary
eye care, noting that while AI enhances referral accuracy, it also shows a
performance gap compared to AI alone, even with explanations. Human involvement
remains essential in medical decision making, underscoring the need for future
research to optimize collaboration, ensuring positive experiences and safe AI
use.

ÊëòË¶ÅÔºö<paragraph>ÂàùÁ¥ö‰øùÂÅ•Êèê‰æõËÄÖÂ∞çÊñºÊúÄÂàùÁöÑÂàÜÊµÅÂíåËΩâË®∫Âà∞Â∞àÁßëÁÖßË≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈùíÂÖâÁúºÁöÑÊÉÖÊ≥Å‰∏ãÔºåÁÑ°ÁóáÁãÄ‰∏îÂø´ÈÄüÊÉ°ÂåñÂèØËÉΩÂ∞éËá¥Ë¶ñÂäõÂñ™Â§±ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂèäÊôÇËΩâË®∫Áµ¶Â∞àÂÆ∂„ÄÇÁÑ∂ËÄåÔºåÂàùÁ¥öÁúºÁßë‰øùÂÅ•Êèê‰æõËÄÖÂèØËÉΩÁÑ°Ê≥ïË≠òÂà•Á∑äÊÄ•ÊÉÖÊ≥ÅÔºåÂèØËÉΩÊúÉÂª∂Ë™§ÁÖßË≠∑„ÄÇÊèê‰æõËß£ÈáãÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÂèØ‰ª•Âä†Âº∑‰ªñÂÄëÁöÑËΩâË®∫Ê±∫Á≠ñ„ÄÇÊàëÂÄëÁ†îÁ©∂ÂêÑÁ®Æ AI Ëß£ÈáãÂ¶Ç‰ΩïÂπ´Âä©Êèê‰æõËÄÖÂçÄÂàÜÈúÄË¶ÅÁ´ãÂç≥ÊàñÈùûÁ∑äÊÄ•Â∞àÁßëËΩâË®∫ÁöÑÊÇ£ËÄÖ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫ÜËß£ÈáãÊÄß AI ÊºîÁÆóÊ≥ïÔºå‰ª•Âæû‰æãË°åÁúºÁßëË≠∑ÁêÜË≥áÊñôÈ†êÊ∏¨ÈùíÂÖâÁúºÊâãË°ìÈúÄÊ±ÇÔºå‰ΩúÁÇ∫Ë≠òÂà•È´òÈ¢®Èö™ÊÇ£ËÄÖÁöÑ‰ª£ÁêÜ„ÄÇÊàëÂÄëÁ¥çÂÖ•‰∫ÜÂÖßÂú®Âíå‰∫ãÂæåËß£ÈáãÊÄßÔºå‰∏¶ËàáÈ©óÂÖâÂ∏´ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÁ∑ö‰∏äÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞‰∫∫Ê©üÂúòÈöäÁöÑË°®ÁèæÔºåË°°ÈáèËΩâË®∫Ê∫ñÁ¢∫Â∫¶‰∏¶ÂàÜÊûêËàá AI ÁöÑ‰∫íÂãïÔºåÂåÖÊã¨ÂêåÊÑèÁéá„ÄÅ‰ªªÂãôÊôÇÈñìÂíå‰ΩøÁî®ËÄÖÈ´îÈ©óÊÑüÁü•„ÄÇÂú® 87 ÂêçÂèÉËàáËÄÖ‰∏≠ÔºåAI ÊîØÊè¥ÊèêÈ´ò‰∫ÜËΩâË®∫Ê∫ñÁ¢∫Â∫¶Ôºà‰ΩøÁî® AI/Êú™‰ΩøÁî®ÁöÑÊØî‰æãÁÇ∫ 59.9%/50.8%ÔºâÔºåÂÑòÁÆ°‰∫∫Ê©üÂúòÈöäÁöÑË°®Áèæ‰∏çÂ¶ÇÂñÆÁç®‰ΩøÁî® AI„ÄÇÂèÉËàáËÄÖË™çÁÇ∫‰ªñÂÄëÂú®‰ΩøÁî®ÂÖßÂú®Ê®°ÂûãÊôÇÊõ¥Â§öÂú∞Á¥çÂÖ•‰∫Ü AI Âª∫Ë≠∞Ôºå‰∏¶Ë™çÁÇ∫ÂÆÉÊõ¥ÊúâÁî®‰∏îÊõ¥ÊúâÂ∏åÊúõ„ÄÇÊ≤íÊúâËß£ÈáãÔºåAI Âª∫Ë≠∞ÁöÑÂÅèÂ∑ÆÊúÉÂ¢ûÂä†„ÄÇAI ÊîØÊè¥‰∏¶Êú™Â¢ûÂä†Â∑•‰ΩúÈáè„ÄÅ‰ø°ÂøÉÂíå‰ø°‰ªªÔºå‰ΩÜÊ∏õÂ∞ë‰∫ÜÊåëÊà∞„ÄÇÂú®‰∏ÄÂÄãÂñÆÁç®ÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠ÔºåÊàëÂÄëÁöÑÈªëÁõíÂ≠êÂíåÂÖßÂú®Ê®°ÂûãÂú®È†êÊ∏¨ÊâãË°ìÁµêÊûúÊñπÈù¢ÂàÜÂà•ÈÅîÂà∞‰∫Ü 77% Âíå 71% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÊâæÂá∫Âú®ÂàùÁ¥öÁúºÁßë‰øùÂÅ•‰∏≠Ôºå‰∫∫Ê©üÂúòÈöäÂêà‰ΩúÁÆ°ÁêÜÈùíÂÖâÁúºÁöÑÊ©üÊúÉÔºå‰∏¶Ê≥®ÊÑèÂà∞ÈõñÁÑ∂ AI ÊèêÈ´ò‰∫ÜËΩâË®∫Ê∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂç≥‰ΩøÊúâËß£ÈáãÔºåÂÆÉ‰πüÈ°ØÁ§∫Âá∫ËàáÂñÆÁç®‰ΩøÁî® AI Áõ∏ÊØîÁöÑÊïàËÉΩÂ∑ÆË∑ù„ÄÇ‰∫∫È°ûÂèÉËàáÂú®ÈÜ´ÁôÇÊ±∫Á≠ñ‰∏≠‰ªçÁÑ∂Ëá≥ÈóúÈáçË¶ÅÔºåÈÄôÂº∑Ë™ø‰∫ÜÊú™‰æÜÁ†îÁ©∂ÂÑ™ÂåñÂçî‰Ωú„ÄÅÁ¢∫‰øùÊ≠£Èù¢Á∂ìÈ©óÂíåÂÆâÂÖ®‰ΩøÁî® AI ÁöÑÂøÖË¶ÅÊÄß„ÄÇ</paragraph>

##### **Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**
2406.18552v1 by Yingying Fang, Zihao Jin, Xiaodan Xing, Simon Walsh, Guang Yang

In medical imaging, particularly in early disease detection and prognosis
tasks, discerning the rationale behind an AI model's predictions is crucial for
evaluating the reliability of its decisions. Conventional explanation methods
face challenges in identifying discernible decisive features in medical image
classifications, where discriminative features are subtle or not immediately
apparent. To bridge this gap, we propose an explainable model that is equipped
with both decision reasoning and feature identification capabilities. Our
approach not only detects influential image patterns but also uncovers the
decisive features that drive the model's final predictions. By implementing our
method, we can efficiently identify and visualise class-specific features
leveraged by the data-driven model, providing insights into the decision-making
processes of deep learning models. We validated our model in the demanding
realm of medical prognosis task, demonstrating its efficacy and potential in
enhancing the reliability of AI in healthcare and in discovering new knowledge
in diseases where prognostic understanding is limited.

ÊëòË¶ÅÔºöÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÁâπÂà•ÊòØÂú®Êó©ÊúüÁñæÁóÖÊ™¢Ê∏¨ÂíåÈ†êÂæå‰ªªÂãô‰∏≠ÔºåËæ®Âà• AI Ê®°ÂûãÈ†êÊ∏¨ËÉåÂæåÁöÑÂéüÁêÜÂ∞çÊñºË©ï‰º∞ÂÖ∂Ê±∫Á≠ñÁöÑÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑËß£ÈáãÊñπÊ≥ïÂú®Ë≠òÂà•ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰∏≠ÂèØË≠òÂà•ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµÊôÇÈù¢Ëá®ÊåëÊà∞ÔºåÂÖ∂‰∏≠ÂçÄÂà•ÊÄßÁâπÂæµÂæàÂæÆÂ¶ôÊàñ‰∏¶‰∏çÊòéÈ°Ø„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂÖ∑ÂÇôÊ±∫Á≠ñÊé®ÁêÜÂíåÁâπÂæµË≠òÂà•ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÊ™¢Ê∏¨ÊúâÂΩ±ÈüøÂäõÁöÑÂΩ±ÂÉèÊ®°ÂºèÔºåÈÇÑÊè≠Á§∫‰∫ÜÊé®ÂãïÊ®°ÂûãÊúÄÁµÇÈ†êÊ∏¨ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµ„ÄÇÈÄöÈÅéÂØ¶ÊñΩÊàëÂÄëÁöÑÊ®°ÂûãÔºåÊàëÂÄëÂèØ‰ª•ÊúâÊïàË≠òÂà•ÂíåË¶ñË¶∫ÂåñÁî±Êï∏ÊìöÈ©ÖÂãïÊ®°ÂûãÂà©Áî®ÁöÑÈ°ûÁâπÂÆöÁâπÂæµÔºåÂæûËÄåÊ∑±ÂÖ•‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÊàëÂÄëÂú®Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑÈÜ´Â≠∏È†êÂæå‰ªªÂãôÈ†òÂüüÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÊèêÈ´ò AI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÁôºÁèæÈ†êÂæåÁêÜËß£ÂèóÈôêÁñæÁóÖÁöÑÊñ∞Áü•Ë≠òÊñπÈù¢ÁöÑÂäüÊïàÂíåÊΩõÂäõ„ÄÇ

##### **The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**
2405.13099v1 by Mohsen Jozani, Jason A. Williams, Ahmed Aleroud, Sarbottam Bhagat

This study explores the relationship between informational support seeking
questions, responses, and helpfulness ratings in online health communities. We
created a labeled data set of question-response pairs and developed multimodal
machine learning and deep learning models to reliably predict informational
support questions and responses. We employed explainable AI to reveal the
emotions embedded in informational support exchanges, demonstrating the
importance of emotion in providing informational support. This complex
interplay between emotional and informational support has not been previously
researched. The study refines social support theory and lays the groundwork for
the development of user decision aids. Further implications are discussed.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÁ∑ö‰∏äÂÅ•Â∫∑Á§æÁæ§‰∏≠Â∞ãÊ±ÇË≥áË®äÊîØÊåÅÁöÑÂïèÈ°å„ÄÅÂõûÊáâÔºå‰ª•ÂèäÊúâÂπ´Âä©ÁöÑË©ïÂàÜ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÁµÑÊ®ôË®òÁöÑÂïèÁ≠îÈÖçÂ∞çË≥áÊñôÈõÜÔºå‰∏¶ÈñãÁôº‰∫ÜÂ§öÊ®°ÊÖãÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•ÂèØÈù†Âú∞È†êÊ∏¨Ë≥áË®äÊîØÊåÅÂïèÈ°åÂíåÂõûÊáâ„ÄÇÊàëÂÄëÊé°Áî®ÂèØËß£ÈáãÁöÑ AI ‰æÜÊè≠Á§∫Ë≥áË®äÊîØÊåÅ‰∫§ÊµÅ‰∏≠ËòäÂê´ÁöÑÊÉÖÁ∑íÔºåË≠âÊòéÊÉÖÁ∑íÂú®Êèê‰æõË≥áË®äÊîØÊåÅ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÁ®ÆÊÉÖÁ∑íÊîØÊåÅÂíåË≥áË®äÊîØÊåÅ‰πãÈñìÁöÑË§áÈõú‰∫§‰∫í‰ΩúÁî®‰ª•Ââç‰∏¶Êú™Ë¢´Á†îÁ©∂ÈÅé„ÄÇÊú¨Á†îÁ©∂ÊîπÈÄ≤‰∫ÜÁ§æÊúÉÊîØÊåÅÁêÜË´ñÔºå‰∏¶ÁÇ∫‰ΩøÁî®ËÄÖÊ±∫Á≠ñËºîÂä©Â∑•ÂÖ∑ÁöÑÈñãÁôºÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇË®éË´ñ‰∫ÜÈÄ≤‰∏ÄÊ≠•ÁöÑÂΩ±Èüø„ÄÇ

##### **ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**
2405.10645v1 by Harris Bin Munawar, Nikolaos Misirlis

In the era of exponential technology growth, one unexpected guest has claimed
a seat in classrooms worldwide, Artificial Intelligence. Generative AI, such as
ChatGPT, promises a revolution in education, yet it arrives with a double-edged
sword. Its potential for personalized learning is offset by issues of cheating,
inaccuracies, and educators struggling to incorporate it effectively into their
lesson design. We are standing on the brink of this educational frontier, and
it is clear that we need to navigate this terrain with a lot of care. This is a
major challenge that could undermine the integrity and value of our educational
process. So, how can we turn these challenges into opportunities? When used
inappropriately, AI tools can become the perfect tool for the cut copy paste
mentality, and quickly begin to corrode critical thinking, creativity, and deep
understanding, the most important skills in our rapidly changing world.
Teachers feel that they are not equipped to leverage this technology, widening
the digital divide among educators and institutions. Addressing these concerns
calls for an in depth research approach. We will employ empirical research,
drawing on the Technology Acceptance Model, to assess the attitudes toward
generative AI among educators and students. Understanding their perceptions,
usage patterns, and hurdles is the first crucial step in creating an effective
solution. The present study will be used as a process manual for future
researchers to apply, running their own data, based on the steps explained here

ÊëòË¶ÅÔºöÂú®ÁßëÊäÄÈ£õÈÄüÁôºÂ±ïÁöÑÊôÇ‰ª£Ôºå‰∏Ä‰ΩçÊÑèÂ§ñÁöÑË®™ÂÆ¢Â∑≤Âú®ÂÖ®ÁêÉÊïôÂÆ§‰∏≠‰ΩîÊúâ‰∏ÄÂ∏≠‰πãÂú∞ÔºåÈÇ£Â∞±ÊòØ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÁîüÊàêÂºè AIÔºå‰æãÂ¶Ç ChatGPTÔºåÊâøË´æÂú®ÊïôËÇ≤È†òÂüüÊéÄËµ∑‰∏ÄÂ†¥Èù©ÂëΩÔºå‰ΩÜÂÆÉÂçªÊòØ‰∏ÄÊääÈõôÈù¢ÂàÉ„ÄÇÂÆÉÂú®ÂÄã‰∫∫ÂåñÂ≠∏ÁøíÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂçªÂõ†‰ΩúÂºä„ÄÅ‰∏çÊ∫ñÁ¢∫‰ª•ÂèäÊïôËÇ≤Â∑•‰ΩúËÄÖÈõ£‰ª•Â∞áÂÖ∂ÊúâÊïàËûçÂÖ•ÊïôÂ≠∏Ë®≠Ë®àÁ≠âÂïèÈ°åËÄåÊäµÈä∑„ÄÇÊàëÂÄëÊ≠£Á´ôÂú®ÈÄôÊïôËÇ≤ÂâçÊ≤øÁöÑÈÇäÁ∑£ÔºåÈ°ØÁÑ∂ÊàëÂÄëÈúÄË¶ÅÈùûÂ∏∏Â∞èÂøÉÂú∞Êé¢Á¥¢ÈÄôÁâáÈ†òÂüü„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂèØËÉΩÊúÉÊêçÂÆ≥ÊàëÂÄëÊïôËÇ≤ÈÅéÁ®ãÁöÑÂÆåÊï¥ÊÄßÂíåÂÉπÂÄº„ÄÇÈÇ£È∫ºÔºåÊàëÂÄëÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊåëÊà∞ËΩâÂåñÁÇ∫Ê©üÈÅáÔºüÁï∂‰∏çÈÅ©Áï∂Âú∞‰ΩøÁî®ÊôÇÔºåAI Â∑•ÂÖ∑ÂèØËÉΩÊúÉÊàêÁÇ∫Ë§áË£ΩË≤º‰∏äÂøÉÊÖãÁöÑÂÆåÁæéÂ∑•ÂÖ∑Ôºå‰∏¶ËøÖÈÄüËÖêËùïÊâπÂà§ÊÄßÊÄùÁ∂≠„ÄÅÂâµÈÄ†ÂäõÂíåÊ∑±ÂÖ•ÁêÜËß£ÔºåÈÄô‰∫õÈÉΩÊòØÊàëÂÄëÂø´ÈÄüËÆäÂåñÁöÑ‰∏ñÁïå‰∏≠ÊúÄÈáçË¶ÅÁöÑÊäÄËÉΩ„ÄÇÊïôÂ∏´ÂÄëË¶∫Âæó‰ªñÂÄëÊ≤íÊúâËÉΩÂäõÂà©Áî®ÈÄôÈ†ÖÊäÄË°ìÔºåÈÄôÊì¥Â§ß‰∫ÜÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÊ©üÊßã‰πãÈñìÁöÑÊï∏‰ΩçÈ¥ªÊ∫ù„ÄÇËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÈúÄË¶ÅÊ∑±ÂÖ•ÁöÑÁ†îÁ©∂ÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞áÊé°Áî®ÂØ¶Ë≠âÁ†îÁ©∂ÔºåÂÄüÈëëÊäÄË°ìÊé•ÂèóÊ®°ÂûãÔºå‰æÜË©ï‰º∞ÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÂ≠∏ÁîüÂ∞çÁîüÊàêÂºè AI ÁöÑÊÖãÂ∫¶„ÄÇ‰∫ÜËß£‰ªñÂÄëÁöÑÁúãÊ≥ï„ÄÅ‰ΩøÁî®Ê®°ÂºèÂíåÈöúÁ§ôÊòØÂâµÈÄ†ÊúâÊïàËß£Ê±∫ÊñπÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÈóúÈçµÊ≠•È©ü„ÄÇÊú¨Á†îÁ©∂Â∞á‰ΩúÁÇ∫Êú™‰æÜÁ†îÁ©∂‰∫∫Âì°ÊáâÁî®ÁöÑÊµÅÁ®ãÊâãÂÜäÔºåÊ†πÊìöÊ≠§ËôïË™™ÊòéÁöÑÊ≠•È©üÈÅãË°å‰ªñÂÄëËá™Â∑±ÁöÑÊï∏Êìö

##### **Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**
2405.07590v1 by Camelia Oprea, Mike Gr√ºne, Mateusz Buglowski, Lena Olivier, Thorsten Orlikowsky, Stefan Kowalewski, Mark Schoberer, Andr√© Stollenwerk

With the digitalization of health care systems, artificial intelligence
becomes more present in medicine. Especially machine learning shows great
potential for complex tasks such as time series classification, usually at the
cost of transparency and comprehensibility. This leads to a lack of trust by
humans and thus hinders its active usage. Explainable artificial intelligence
tries to close this gap by providing insight into the decision-making process,
the actual usefulness of its different methods is however unclear. This paper
proposes a user study based evaluation of the explanation method Grad-CAM with
application to a neural network for the classification of breaths in time
series neonatal ventilation data. We present the perceived usefulness of the
explainability method by different stakeholders, exposing the difficulty to
achieve actual transparency and the wish for more in-depth explanations by many
of the participants.

ÊëòË¶ÅÔºöÈö®ËëóÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊï∏‰ΩçÂåñÔºå‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´Â≠∏È†òÂüü‰∏≠ËÆäÂæóÊõ¥Âä†ÊôÆÂèä„ÄÇÁâπÂà•ÊòØÊ©üÂô®Â≠∏ÁøíÂú®ÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûÁ≠âË§áÈõú‰ªªÂãô‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÊΩõÂäõÔºå‰ΩÜÈÄöÂ∏∏ÊòØ‰ª•ÈÄèÊòéÂ∫¶ÂíåÂèØÁêÜËß£ÊÄßÁÇ∫‰ª£ÂÉπ„ÄÇÈÄôÂ∞éËá¥‰∫∫È°ûÁº∫‰πè‰ø°‰ªªÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÖ∂Á©çÊ•µ‰ΩøÁî®„ÄÇÂèØËß£ÈáãÁöÑ‰∫∫Â∑•Êô∫ÊÖßË©¶ÂúñÈÄöÈÅéÊèê‰æõÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÁöÑÊ¥ûÂØü‰æÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºå‰ΩÜÂÖ∂‰∏çÂêåÊñπÊ≥ïÁöÑÂØ¶ÈöõÊïàÁî®Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰ΩøÁî®ËÄÖÁ†îÁ©∂ÁöÑË©ï‰º∞ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫Ü Grad-CAM Ëß£ÈáãÊñπÊ≥ïÔºå‰∏¶Â∞áÂÖ∂ÊáâÁî®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰ª•ÂàÜÈ°ûÊôÇÈñìÂ∫èÂàóÊñ∞ÁîüÂÖíÂëºÂê∏Êï∏Êìö‰∏≠ÁöÑÂëºÂê∏„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏çÂêåÂà©ÁõäÁõ∏ÈóúËÄÖÂ∞çÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÊÑüÁü•ÊïàÁî®ÔºåÊè≠Á§∫‰∫ÜÂØ¶ÁèæÂØ¶ÈöõÈÄèÊòéÂ∫¶ÁöÑÈõ£Â∫¶Ôºå‰ª•ÂèäË®±Â§öÂèÉËàáËÄÖÂ∏åÊúõÁç≤ÂæóÊõ¥Ê∑±ÂÖ•ÁöÑËß£Èáã„ÄÇ

##### **XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**
2405.06270v3 by Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics
offers a promising avenue for clinical decision-making. This study outlines the
development of a novel method for zero-shot/few-shot in-context learning (ICL)
by integrating medical domain knowledge using a multi-layered structured
prompt. We also explore the efficacy of two communication styles between the
user and LLMs: the Numerical Conversational (NC) style, which processes data
incrementally, and the Natural Language Single-Turn (NL-ST) style, which
employs long narrative prompts.
  Our study systematically evaluates the diagnostic accuracy and risk factors,
including gender bias and false negative rates, using a dataset of 920 patient
records in various few-shot scenarios. Results indicate that traditional
clinical machine learning (ML) models generally outperform LLMs in zero-shot
and few-shot settings. However, the performance gap narrows significantly when
employing few-shot examples alongside effective explainable AI (XAI) methods as
sources of domain knowledge. Moreover, with sufficient time and an increased
number of examples, the conversational style (NC) nearly matches the
performance of ML models. Most notably, LLMs demonstrate comparable or superior
cost-sensitive accuracy relative to ML models.
  This research confirms that, with appropriate domain knowledge and tailored
communication strategies, LLMs can significantly enhance diagnostic processes.
The findings highlight the importance of optimizing the number of training
examples and communication styles to improve accuracy and reduce biases in LLM
applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÈÜ´ÁôÇË®∫Êñ∑Êï¥Âêà
ÁÇ∫Ëá®Â∫äÊ±∫Á≠ñÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÈÄîÂæë„ÄÇÊú¨Á†îÁ©∂Ê¶ÇËø∞‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÊñπÊ≥ïÁöÑÈñãÁôºÔºåÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí/Â∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢ÉÂ≠∏Áøí (ICL)ÔºåÊñπÊ≥ïÊòØ‰ΩøÁî®Â§öÂ±§ÁµêÊßãÂåñÊèêÁ§∫Êï¥ÂêàÈÜ´ÁôÇÈ†òÂüüÁü•Ë≠ò„ÄÇÊàëÂÄëÈÇÑÊé¢Ë®é‰∫Ü‰ΩøÁî®ËÄÖËàá LLM ‰πãÈñìÂÖ©Á®ÆÊ∫ùÈÄöÊñπÂºèÁöÑÂäüÊïàÔºöÊï∏ÂÄºÂ∞çË©± (NC) ÊñπÂºèÔºåÂÆÉÊúÉÈÄêÊ≠•ËôïÁêÜË≥áÊñôÔºå‰ª•ÂèäËá™ÁÑ∂Ë™ûË®ÄÂñÆÂõûÂêà (NL-ST) ÊñπÂºèÔºåÂÆÉÊúÉ‰ΩøÁî®Èï∑ÁØáÊïò‰∫ãÊèêÁ§∫„ÄÇ
ÊàëÂÄëÁöÑÁ†îÁ©∂Á≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÈ¢®Èö™Âõ†Â≠êÔºåÂåÖÊã¨ÊÄßÂà•ÂÅèË¶ãÂíåÂÅáÈô∞ÊÄßÁéáÔºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄãÂåÖÂê´ 920 ÂÄãÊÇ£ËÄÖË®òÈåÑÁöÑË≥áÊñôÈõÜÔºåÊé°Áî®ÂêÑÁ®ÆÂ∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢É„ÄÇÁµêÊûúË°®ÊòéÔºåÂÇ≥Áµ±ÁöÑËá®Â∫äÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÈÄöÂ∏∏Âú®Èõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÈáèÂ≠∏ÁøíË®≠ÂÆö‰∏≠Ë°®ÁèæÂÑ™Êñº LLM„ÄÇÁÑ∂ËÄåÔºåÁï∂‰ΩøÁî®Â∞ëÈáèÂ≠∏ÁøíÁØÑ‰æã‰ª•ÂèäÊúâÊïàÁöÑÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï‰ΩúÁÇ∫È†òÂüüÁü•Ë≠ò‰æÜÊ∫êÊôÇÔºåÊïàËÉΩÂ∑ÆË∑ùÊúÉÈ°ØËëóÁ∏ÆÂ∞è„ÄÇÊ≠§Â§ñÔºåÈö®ËëóÊôÇÈñìÂÖÖË∂≥ÂíåÁØÑ‰æãÊï∏ÈáèÂ¢ûÂä†ÔºåÂ∞çË©±ÊñπÂºè (NC) Âπæ‰πéÂèØ‰ª•Â™≤Áæé ML Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊúÄÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåLLM Áõ∏Â∞çÊñº ML Ê®°ÂûãÂ±ïÁèæÂá∫Áõ∏Áï∂ÊàñÊõ¥‰Ω≥ÁöÑÊàêÊú¨ÊïèÊÑüÊ∫ñÁ¢∫Â∫¶„ÄÇ
Êú¨Á†îÁ©∂Ë≠âÂØ¶ÔºåÈÄèÈÅéÈÅ©Áï∂ÁöÑÈ†òÂüüÁü•Ë≠òÂíåÈáèË∫´ÊâìÈÄ†ÁöÑÊ∫ùÈÄöÁ≠ñÁï•ÔºåLLM ÂèØ‰ª•È°ØËëóÂ¢ûÂº∑Ë®∫Êñ∑Á®ãÂ∫è„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÊúÄ‰Ω≥ÂåñË®ìÁ∑¥ÁØÑ‰æãÊï∏ÈáèÂíåÊ∫ùÈÄöÊñπÂºèÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÊ∫ñÁ¢∫Â∫¶‰∏¶Ê∏õÂ∞ë LLM ÊáâÁî®‰∏≠ÁöÑÂÅèÂ∑Æ„ÄÇ

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel Mir√≥-Nicolau, Gabriel Moy√†-Alcover, Antoni Jaume-i-Cap√≥, Manuel Gonz√°lez-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

ÊëòË¶ÅÔºöÈö®ËëóÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æùË≥¥ÊÄßÁöÑÂ¢ûÂä†ÔºåÂä†‰∏äÂÖ∂Âõ∫ÊúâÁöÑÈÄèÊòéÂ∫¶‰∏çË∂≥Ôºå‰øÉ‰Ωø‰∏ÄÂÄãÊñ∞ÁöÑÁ†îÁ©∂È†òÂüüÁôºÂ±ïÔºåÁ®±ÁÇ∫ÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÊó®Âú®ÈÄèÈÅéÊ∑±ÂÖ•‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÂéüÁêÜÔºå‰æÜÊèêÂçáÊúÄÁµÇ‰ΩøÁî®ËÄÖÂ∞çËá™ÂãïÂåñÁ≥ªÁµ±ÁöÑ‰ø°Ë≥¥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË°°Èáè‰ΩøÁî®ËÄÖÂ∞ç XAI Á≥ªÁµ±‰ø°Ë≥¥Â∫¶ÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÂÖÅË®±Â∞çÂÖ∂ÈÄ≤Ë°åÊîπÈÄ≤„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊåáÊ®ôÁµêÂêà‰∫ÜÂÆ¢ËßÄËßÄÈªû‰∏ãÁöÑÊïàËÉΩÊåáÊ®ôÂíå‰ø°Ë≥¥ÊåáÊ®ô„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÂú®‰∏ÄÂÄãÁúüÂØ¶ÁöÑÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÈÄ≤Ë°å‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Ôºö‰ΩøÁî® XAI Á≥ªÁµ±Âæû X ÂÖâÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨ËÇ∫ÁÇé„ÄÇ

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

ÊëòË¶ÅÔºöCOVID-19 Áñ´ÊÉÖÂ∞çÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÈÄ†ÊàêÂ£ìÂäõÔºåÂøÖÈ†àÈÄ≤Ë°åÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂπ≤È†êÔºå‰ª•ÊéßÂà∂ÁñæÁóÖÂÇ≥Êí≠‰∏¶Èôç‰ΩéÊ≠ª‰∫°Áéá„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÈÄèÈÅéËÉ∏ÈÉ® X ÂÖâ (CXR) ÂΩ±ÂÉèÊîπÂñÑÂ∞ç COVID-19 È†êÂæåÁöÑÁêÜËß£Âíå‰ø°Ë≥¥„ÄÇÈÄèÈÅéÊï¥ÂêàÂ§ßË¶èÊ®°È†êË®ìÁ∑¥ÂΩ±ÂÉèÁ∑®Á¢ºÂô®„ÄÅÈ¢®Èö™ÁâπÂÆö Grad-CAM ÂíåËß£ÂâñÂçÄÂüüÂÅµÊ∏¨ÊäÄË°ìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁî¢ÁîüÂçÄÂüüÂèØËß£ÈáãÁöÑÁµêÊûúÔºåÊúâÊïàÊçïÊçâÂøÖË¶ÅÁöÑÁñæÁóÖÁâπÂæµÔºåÂêåÊôÇÂ∞àÊ≥®ÊñºÁΩïË¶ã‰ΩÜÈóúÈçµÁöÑÁï∞Â∏∏ÂçÄÂüü„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûúÈÄèÈÅéÈ¢®Èö™ÂçÄÂüüÂÆö‰ΩçÊèê‰æõÂ¢ûÂº∑ÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÈÄèÊòéÂ∫¶ÔºåËÆìËá®Â∫äÈÜ´ÁîüËÉΩÂ§†Âú®Êõ¥‰∫ÜËß£È†êÂæåË¶ãËß£ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞± COVID-19 Ë®∫Êñ∑ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÂú®Â§ö‰∏≠ÂøÉÁîüÂ≠òË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºå‰∏¶ÈÄèÈÅéÈáèÂåñÂíåË≥™ÂåñË©ï‰º∞Ë≠âÊòéÂÖ∂ÊúâÊïàÊÄßÔºåÈÅîÂà∞ÂÑ™Áï∞ÁöÑ C ÊåáÊï∏Ôºà0.764 Âíå 0.727ÔºâÂíåÊôÇÈñìÁõ∏Èóú AUCÔºà0.799 Âíå 0.691Ôºâ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÂú®È¢®Èö™È†êÊ∏¨ÊñπÈù¢Ë∂ÖË∂äÂÇ≥Áµ±ÁöÑÁîüÂ≠òÂàÜÊûêÊñπÊ≥ïÔºåÊèêÂçáËá®Â∫äÊ±∫Á≠ñÁöÑËß£ÈáãÊÄßÔºå‰∏¶Â¢ûÂº∑ AI Á≥ªÁµ±ÁöÑ‰ø°Ë≥¥Â∫¶„ÄÇ

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÅéÂéªÂπæÂπ¥ÔºåËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (CDSS) ‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®Âà©Áî®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÊñπÈù¢ÁôºÊèÆ‰∫ÜÈóúÈçµ‰ΩúÁî®„ÄÇÂÑòÁÆ° AI Ê®°ÂûãÂÖ∑Êúâ‰ª§‰∫∫ÊªøÊÑèÁöÑËÉΩÂäõÔºå‰ΩÜÁº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÔºåÁâπÂà•ÊòØÂú®ÂèØÈù†ÊÄßÁÇ∫ÂøÖË¶ÅËÄÉÈáèÁöÑÈÜ´ÁôÇËÉåÊôØ‰∏ãÔºåÈÄôÂ∏∂‰æÜ‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂú®‰∏çÂΩ±ÈüøÈ†êÊ∏¨Á≤æÊ∫ñÂ∫¶ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈÄèÊòéÂ∫¶‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂç≥ Rad4XCNNÔºå‰ª•Â¢ûÂº∑ CNN Ë°çÁîüÁâπÂæµÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºåÂêåÊôÇÂÖ∑ÂÇôÊîæÂ∞ÑÁâπÂæµÂõ∫ÊúâÁöÑÂèØËß£ÈáãÊÄß„ÄÇRad4XCNN ‰∏çÂêåÊñºÂü∫ÊñºÈ°ØËëóÊÄßÂúñÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÊîæÂ∞ÑÁµÑÂ≠∏Â∞áÂèØÁêÜËß£ÁöÑÂê´Áæ©Ëàá CNN Ë°çÁîüÁâπÂæµÈóúËÅØËµ∑‰æÜÔºåÁÇ∫Ë∂ÖË∂äË¶ñË¶∫ÂåñÂúñË°®ÁöÑËß£ÈáãÊñπÊ≥ïÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÄÈªû„ÄÇÊàëÂÄë‰ª•‰π≥ÁôåÂàÜÈ°û‰ªªÂãô‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÂú®Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äË©ï‰º∞ Rad4XCNNÔºåÂåÖÊã¨‰∏ÄÂÄãÁ∑ö‰∏äË≥áÊñôÈõÜÂíåÂÖ©ÂÄãÁî®ÊñºÂÖßÈÉ®ÂíåÂ§ñÈÉ®È©óË≠âÁöÑÂÖßÈÉ®Ë≥áÊñôÈõÜ„ÄÇ‰∏Ä‰∫õÈóúÈçµÁµêÊûúÂ¶Ç‰∏ãÔºöi) Ëàá ViT Ë°çÁîüÁâπÂæµÂíåÊîæÂ∞ÑÁâπÂæµÁõ∏ÊØîÔºåCNN Ë°çÁîüÁâπÂæµ‰øùË≠â‰∫ÜÊõ¥Á©©ÂÅ•ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºõii) ÂÇ≥Áµ±ÁöÑË¶ñË¶∫ÂåñÂúñËß£ÈáãÊñπÊ≥ïÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºõiii) Rad4XCNN Ê≤íÊúâÁäßÁâ≤Ê®°ÂûãÊ∫ñÁ¢∫Â∫¶‰æÜÊèõÂèñÂÖ∂ÂèØËß£ÈáãÊÄßÔºõiv) Rad4XCNN Êèê‰æõ‰∫ÜÂÖ®Â±ÄËß£ÈáãË¶ãËß£Ôºå‰ΩøÈÜ´Â∏´ËÉΩÂ§†ÂàÜÊûêÊ®°ÂûãËº∏Âá∫ÂíåÁôºÁèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂº∑Ë™øÂ∞áÂèØËß£ÈáãÊÄßÊï¥ÂêàÂà∞ AI Ê®°Âûã‰∏≠Â∞çÊñºÂ¢ûÂº∑Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑ‰ø°‰ªªÂíåÊé°Áî®Ëá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âº∑Ë™ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂ¶Ç‰ΩïËÉΩÁ∑©Ëß£ËàáÂèØËß£Èáã AI ÊñπÊ≥ïÁõ∏ÈóúÁöÑ‰∏Ä‰∫õÁñëÊÖÆ„ÄÇ</paragraph>

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊôÆÂèäÊï¥ÂêàÔºåÂú®Ê∂âÂèä AI È©ÖÂãïÁ≥ªÁµ±ÁöÑ‰∫ãÊïÖ‰∏≠ÔºåË≤¨‰ªªÂíåÁæ©ÂãôÊ≠∏Â±¨Áî¢Áîü‰∫ÜË§áÈõúÁöÑÊåëÊà∞„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÁöÑ‰∫íÈÄ£ÊÄß„ÄÅAI ÂºïÁôº‰∫ãÊïÖÁöÑÂÄ´ÁêÜÂïèÈ°åÔºåÂä†‰∏ä AI ÊäÄË°ìÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂíåÁº∫‰πèÁõ∏ÊáâÊ≥ïË¶èÔºå‰ΩøÂæóÂÇ≥Áµ±Ë≤¨‰ªªÊ≠∏Â±¨Èù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË®àÁÆóÂèçÊÄùÂùáË°° (CRE) ÊñπÊ≥ïÔºå‰ª•Âª∫Á´ã‰∏ÄÂÄãÈÄ£Ë≤´‰∏îÂú®ÂÄ´ÁêÜ‰∏äÂèØÊé•ÂèóÁöÑË≤¨‰ªªÊ≠∏Â±¨Êû∂ÊßãÔºåÈÅ©Áî®ÊñºÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫„ÄÇË®àÁÆóÊñπÊ≥ïÊèê‰æõ‰∫ÜÁµêÊßãÂåñÁöÑÂàÜÊûêÔºåÂÖãÊúç‰∫ÜÊ¶ÇÂøµÊñπÊ≥ïÂú®ËôïÁêÜÂãïÊÖã‰∏îÂ§öÈù¢ÂêëÊÉÖÂ¢ÉÊôÇÁöÑÈôêÂà∂ÔºåÂ±ïÁ§∫‰∫ÜË©≤Êû∂ÊßãÂú®Ë≤¨‰ªªÊ≠∏Â±¨ÈÅéÁ®ã‰∏≠ÂÖ∑ÂÇôÁöÑÂèØËß£ÈáãÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜËàáÂùáË°°Ë®àÁÆó‰∏≠Á¥¢Ë≥†Áõ∏ÈóúÁöÑÂàùÂßãÂïüÂãïÂ±§Á¥öÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇÊàëÂÄë‰ª• AI ËºîÂä©ÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåË™™Êòé‰∏çÂêåÁöÑÂàùÂßãÂåñÂ¶Ç‰ΩïÂ∞éËá¥‰∏çÂêåÁöÑË≤¨‰ªªÂàÜÈÖç„ÄÇË©≤Êû∂ÊßãÊèê‰æõ‰∫ÜÂ∞ç AI ÂºïÁôº‰∫ãÊïÖ‰∏≠ÂïèË≤¨Âà∂ÁöÑÂØ∂Ë≤¥Ë¶ãËß£ÔºåÈÄèÈÅéÊåÅÁ∫åÁõ£Êéß„ÄÅ‰øÆË®ÇÂíåÂèçÊÄùÔºå‰øÉÈÄ≤‰∫ÜÊ∞∏Á∫å‰∏îÊúâÈüåÊÄßÁöÑÁ≥ªÁµ±ÁôºÂ±ï„ÄÇ

##### **Explainable AI for Fair Sepsis Mortality Predictive Model**
2404.13139v1 by Chia-Hsuan Chang, Xiaoyang Wang, Christopher C. Yang

Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÈÄèÈÅéÈ†êÊ∏¨Ê®°ÂûãÂçîÂä©ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÔºåÂ§ßÂπÖËΩâËÆä‰∫ÜËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®Á®ãÂºèÊôÇÂÖ¨Âπ≥ÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÈóúÈçµÈúÄÊ±ÇÔºå‰ª•Á¢∫‰øùÂú®‰∏çÂêåÁöÑÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àË≥áÊñô‰∏≠Áç≤ÂæóÂÖ¨Âπ≥ÁöÑÁµêÊûú„ÄÇÈÄèÈÅéÂ∞àÊ≥®ÊñºÊïóË°ÄÁóáÁõ∏ÈóúÊ≠ª‰∫°ÁéáÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÊúÉÂ≠∏Áøí‰∏ÄÂÄãÊïàËÉΩÊúÄ‰Ω≥ÂåñÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÁÑ∂ÂæåÊé°Áî®ËΩâÁßªÂ≠∏ÁøíÈÅéÁ®ã‰æÜÁî¢Áîü‰∏ÄÂÄãÂÖ∑ÊúâÊõ¥Â•ΩÂÖ¨Âπ≥ÊÄßÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÊéíÂàóÁöÑÁâπÂæµÈáçË¶ÅÊÄßÊºîÁÆóÊ≥ïÔºåÊó®Âú®Èó°ÊòéÊØèÂÄãÁâπÂæµÂú®Â¢ûÂº∑È†êÊ∏¨ÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÁöÑË≤¢Áçª„ÄÇËàáÁèæÊúâÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞àÊ≥®ÊñºËß£ÈáãÁâπÂæµÂ∞çÈ†êÊ∏¨ÊïàËÉΩÁöÑË≤¢Áçª‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁç®ÁâπÂú∞ÂΩåË£ú‰∫ÜÁêÜËß£ÊØèÂÄãÁâπÂæµÂ¶Ç‰ΩïÊúâÂä©ÊñºÂÖ¨Âπ≥ÊÄßÁöÑÂ∑ÆË∑ù„ÄÇÈÄôÈ†ÖÈÄ≤Â±ïËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÊïóË°ÄÁóáÁöÑÊ≠ª‰∫°ÁéáÂæàÈ´òÔºå‰∏îÂú®‰∏âÂàÜ‰πã‰∏ÄÁöÑÈÜ´Èô¢Ê≠ª‰∫°‰∏≠ÊâÆÊºîËëóËßíËâ≤„ÄÇÊàëÂÄëÁöÑÊ®°Âûã‰∏çÂÉÖÊúâÂä©ÊñºË≠òÂà•ÂíåÊ∏õËºïÈ†êÊ∏¨Ê®°Âûã‰∏≠ÁöÑÂÅèÂ∑ÆÔºåÈÇÑËÉΩÈÄèÈÅéÊèêÈ´òÊ®°ÂûãÈ†êÊ∏¨ÁöÑÈÄèÊòéÂ∫¶ÂíåÂÖ¨Âπ≥ÊÄß‰æÜÂüπÈ§äÈÜ´ÁôÇ‰øùÂÅ•Âà©ÁõäÁõ∏ÈóúËÄÖ‰πãÈñìÁöÑ‰ø°‰ªªÔºåÈÄ≤ËÄåÊúâÂä©ÊñºÊèê‰æõÊõ¥ÂÖ¨Âπ≥‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇ

##### **Multi Class Depression Detection Through Tweets using Artificial Intelligence**
2404.13104v1 by Muhammad Osama Nusrat, Waseem Shahzad, Saad Ahmed Jamal

Depression is a significant issue nowadays. As per the World Health
Organization (WHO), in 2023, over 280 million individuals are grappling with
depression. This is a huge number; if not taken seriously, these numbers will
increase rapidly. About 4.89 billion individuals are social media users. People
express their feelings and emotions on platforms like Twitter, Facebook,
Reddit, Instagram, etc. These platforms contain valuable information which can
be used for research purposes. Considerable research has been conducted across
various social media platforms. However, certain limitations persist in these
endeavors. Particularly, previous studies were only focused on detecting
depression and the intensity of depression in tweets. Also, there existed
inaccuracies in dataset labeling. In this research work, five types of
depression (Bipolar, major, psychotic, atypical, and postpartum) were predicted
using tweets from the Twitter database based on lexicon labeling. Explainable
AI was used to provide reasoning by highlighting the parts of tweets that
represent type of depression. Bidirectional Encoder Representations from
Transformers (BERT) was used for feature extraction and training. Machine
learning and deep learning methodologies were used to train the model. The BERT
model presented the most promising results, achieving an overall accuracy of
0.96.

ÊëòË¶ÅÔºöÁèæ‰ªäÔºåÊÜÇÈ¨±ÁóáÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑË≠∞È°å„ÄÇÊ†πÊìö‰∏ñÁïåË°õÁîüÁµÑÁπî (WHO) ÁöÑË≥áÊñôÔºåÂú® 2023 Âπ¥ÔºåË∂ÖÈÅé 2.8 ÂÑÑ‰∫∫Ê≠£Âú®ËàáÊÜÇÈ¨±ÁóáÊêèÈ¨•„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈæêÂ§ßÁöÑÊï∏Â≠óÔºõÂ¶ÇÊûú‰∏çË™çÁúüÁúãÂæÖÔºåÈÄô‰∫õÊï∏Â≠óÂ∞áÊúÉÂø´ÈÄüÂ¢ûÂä†„ÄÇÂ§ßÁ¥ÑÊúâ 48.9 ÂÑÑ‰∫∫ÊòØÁ§æÁæ§Â™íÈ´î‰ΩøÁî®ËÄÖ„ÄÇ‰∫∫ÂÄëÂú® Twitter„ÄÅFacebook„ÄÅReddit„ÄÅInstagram Á≠âÂπ≥Âè∞‰∏äË°®ÈÅîËá™Â∑±ÁöÑÊÑüÂèóÂíåÊÉÖÁ∑í„ÄÇÈÄô‰∫õÂπ≥Âè∞ÂåÖÂê´ÊúâÂÉπÂÄºÁöÑË≥áË®äÔºåÂèØÁî®ÊñºÁ†îÁ©∂ÁõÆÁöÑ„ÄÇÂ∑≤Á∂ìÂú®ÂêÑÁ®ÆÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞‰∏äÈÄ≤Ë°å‰∫ÜÂ§ßÈáèÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂä™Âäõ‰ªçÂ≠òÂú®Êüê‰∫õÈôêÂà∂„ÄÇÁâπÂà•ÊòØÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÂÉÖÂ∞àÊ≥®ÊñºÂÅµÊ∏¨Êé®Êñá‰∏≠ÁöÑÊÜÇÈ¨±ÁóáÂíåÊÜÇÈ¨±ÁóáÁöÑÂº∑Â∫¶„ÄÇÊ≠§Â§ñÔºåË≥áÊñôÈõÜÊ®ôÁ±§‰∏≠Â≠òÂú®‰∏çÊ∫ñÁ¢∫ÁöÑÊÉÖÊ≥Å„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂Â∑•‰Ωú‰∏≠Ôºå‰ΩøÁî®Âü∫ÊñºË©ûÂΩôÊ®ôÁ±§ÁöÑ Twitter Ë≥áÊñôÂ∫´‰∏≠ÁöÑÊé®ÊñáÈ†êÊ∏¨‰∫Ü‰∫îÁ®ÆÈ°ûÂûãÁöÑÊÜÇÈ¨±ÁóáÔºàÈõôÊ•µÂûã„ÄÅÈáçÂ∫¶„ÄÅÁ≤æÁ•ûÁóÖÂûã„ÄÅÈùûÂÖ∏ÂûãÂíåÁî¢ÂæåÔºâ„ÄÇÂèØËß£ÈáãÁöÑ AI Áî®ÊñºÈÄèÈÅéÂº∑Ë™ø‰ª£Ë°®ÊÜÇÈ¨±ÁóáÈ°ûÂûãÁöÑÊé®ÊñáÈÉ®ÂàÜ‰æÜÊèê‰æõÊé®ÁêÜ„ÄÇÂæû TransformersÔºàBERTÔºâ‰∏≠ÊèêÂèñÁöÑÈõôÂêëÁ∑®Á¢ºÂô®Ë°®Á§∫Áî®ÊñºÁâπÂæµÊèêÂèñÂíåË®ìÁ∑¥„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁî®ÊñºË®ìÁ∑¥Ê®°Âûã„ÄÇBERT Ê®°ÂûãÂëàÁèæÂá∫ÊúÄÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÈÅîÂà∞ 0.96 ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**
2404.12832v2 by Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman

Deep learning is dramatically transforming the field of medical imaging and
radiology, enabling the identification of pathologies in medical images,
including computed tomography (CT) and X-ray scans. However, the performance of
deep learning models, particularly in segmentation tasks, is often limited by
the need for extensive annotated datasets. To address this challenge, the
capabilities of weakly supervised semantic segmentation are explored through
the lens of Explainable AI and the generation of counterfactual explanations.
The scope of this research is development of a novel counterfactual inpainting
approach (COIN) that flips the predicted classification label from abnormal to
normal by using a generative model. For instance, if the classifier deems an
input medical image X as abnormal, indicating the presence of a pathology, the
generative model aims to inpaint the abnormal region, thus reversing the
classifier's original prediction label. The approach enables us to produce
precise segmentations for pathologies without depending on pre-existing
segmentation masks. Crucially, image-level labels are utilized, which are
substantially easier to acquire than creating detailed segmentation masks. The
effectiveness of the method is demonstrated by segmenting synthetic targets and
actual kidney tumors from CT images acquired from Tartu University Hospital in
Estonia. The findings indicate that COIN greatly surpasses established
attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an
alternative counterfactual explanation method introduced by Singla et al. This
evidence suggests that COIN is a promising approach for semantic segmentation
of tumors in CT images, and presents a step forward in making deep learning
applications more accessible and effective in healthcare, where annotated data
is scarce.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠¶‰π†Ê≠£Â§ßÂπÖËΩâËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÊîæÂ∞ÑÁ∑öÂ≠∏È†òÂüüÔºåËÉΩËæ®Ë≠òÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁóÖÁêÜÔºåÂåÖÊã¨ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) Âíå X ÂÖâÊéÉÊèè„ÄÇÁÑ∂ËÄåÔºåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®ÂàÜÂâ≤‰ªªÂãô‰∏≠ÔºåÂ∏∏Â∏∏ÂèóÂà∞Âª£Ê≥õË®ªËß£Ë≥áÊñôÈõÜÈúÄÊ±ÇÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÈÄèÈÅéÂèØËß£Èáã AI ÂíåÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÁî¢ÁîüÔºåÊé¢Á¥¢Âº±Áõ£Áù£Ë™ûÊÑèÂàÜÂâ≤ÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁØÑÂúçÊòØÈñãÁôº‰∏ÄÁ®ÆÊñ∞ÁöÑÂèç‰∫ãÂØ¶ÂÖßÊèíÊñπÊ≥ï (COIN)ÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®ÁîüÊàêÊ®°ÂûãÂ∞áÈ†êÊ∏¨ÁöÑÂàÜÈ°ûÊ®ôÁ±§ÂæûÁï∞Â∏∏ÁøªËΩâÁÇ∫Ê≠£Â∏∏„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂàÜÈ°ûÂô®Â∞áËº∏ÂÖ•ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉè X Ë¶ñÁÇ∫Áï∞Â∏∏ÔºåË°®Á§∫Â≠òÂú®ÁóÖÁêÜÔºåÂâáÁîüÊàêÊ®°ÂûãÊó®Âú®ÂÖßÊèíÁï∞Â∏∏ÂçÄÂüüÔºåÂæûËÄåÈÄÜËΩâÂàÜÈ°ûÂô®ÁöÑÂéüÂßãÈ†êÊ∏¨Ê®ôÁ±§„ÄÇÊ≠§ÊñπÊ≥ï‰ΩøÊàëÂÄëËÉΩÂ§†Áî¢ÁîüÁóÖÁêÜÁöÑÁ≤æÁ¢∫ÂàÜÂâ≤ÔºåËÄåÁÑ°ÈúÄ‰æùË≥¥ÊñºÈ†êÂÖàÂ≠òÂú®ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÂà©Áî®ÂΩ±ÂÉèÂ±§Á¥öÊ®ôÁ±§ÔºåÈÄôÊØîÂª∫Á´ãË©≥Á¥∞ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©ÂÆπÊòìÂèñÂæó„ÄÇË©≤ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÈÄèÈÅéÂàÜÂâ≤ÂêàÊàêÁõÆÊ®ôÂíåÂæûÊÑõÊ≤ôÂ∞º‰∫ûÂ°îÁàæÂúñÂ§ßÂ≠∏ÈÜ´Èô¢ÂèñÂæóÁöÑ CT ÂΩ±ÂÉè‰∏≠ÁöÑÂØ¶ÈöõËÖéËáüËÖ´Áò§‰æÜË≠âÊòé„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåCOIN ÈÅ†ÈÅ†Ë∂ÖÈÅéÂ∑≤Âª∫Á´ãÁöÑÊ≠∏Âõ†ÊñπÊ≥ïÔºå‰æãÂ¶Ç RISE„ÄÅScoreCAM Âíå LayerCAMÔºå‰ª•Âèä Singla Á≠â‰∫∫ÊèêÂá∫ÁöÑÂè¶‰∏ÄÁ®ÆÂèç‰∫ãÂØ¶Ëß£ÈáãÊñπÊ≥ï„ÄÇÊ≠§Ë≠âÊìöË°®ÊòéÔºåCOIN ÊòØ‰∏ÄÁ®ÆÂæàÊúâÂâçÈÄîÁöÑ CT ÂΩ±ÂÉè‰∏≠ËÖ´Áò§Ë™ûÊÑèÂàÜÂâ≤ÊñπÊ≥ïÔºå‰∏¶Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ËÆìÊ∑±Â∫¶Â≠∏ÁøíÊáâÁî®Êõ¥ÊòìÊñºÂèñÂæóÂíåÊõ¥ÊúâÊïàÁéáÈÇÅÈÄ≤‰∏ÄÊ≠•ÔºåÂÖ∂‰∏≠Ë®ªËß£Ë≥áÊñôÂæàÁ®ÄÂ∞ë„ÄÇ

##### **Hybrid Intelligence for Digital Humanities**
2406.15374v1 by Victor de Boer, Lise Stork

In this paper, we explore the synergies between Digital Humanities (DH) as a
discipline and Hybrid Intelligence (HI) as a research paradigm. In DH research,
the use of digital methods and specifically that of Artificial Intelligence is
subject to a set of requirements and constraints. We argue that these are
well-supported by the capabilities and goals of HI. Our contribution includes
the identification of five such DH requirements: Successful AI systems need to
be able to 1) collaborate with the (human) scholar; 2) support data criticism;
3) support tool criticism; 4) be aware of and cater to various perspectives and
5) support distant and close reading. We take the CARE principles of Hybrid
Intelligence (collaborative, adaptive, responsible and explainable) as
theoretical framework and map these to the DH requirements. In this mapping, we
include example research projects. We finally address how insights from DH can
be applied to HI and discuss open challenges for the combination of the two
disciplines.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊï∏‰Ωç‰∫∫ÊñáÂ≠∏Áßë (DH) ‰ΩúÁÇ∫‰∏ÄÈñÄÂ≠∏ÁßëËàáÊ∑∑ÂêàÊô∫ËÉΩ (HI) ‰ΩúÁÇ∫‰∏ÄÂÄãÁ†îÁ©∂ÂÖ∏ÁØÑ‰πãÈñìÁöÑÂçîÂêå‰ΩúÁî®„ÄÇÂú® DH Á†îÁ©∂‰∏≠ÔºåÊï∏‰ΩçÊñπÊ≥ïÁöÑ‰ΩøÁî®ÔºåÁâπÂà•ÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰ΩøÁî®ÔºåÂèóÂà∞‰∏ÄÁ≥ªÂàóË¶ÅÊ±ÇÂíåÈôêÂà∂„ÄÇÊàëÂÄëË™çÁÇ∫ÈÄô‰∫õË¶ÅÊ±ÇÂíåÈôêÂà∂Áç≤Âæó HI ÁöÑËÉΩÂäõÂíåÁõÆÊ®ôÁöÑÂÖÖÂàÜÊîØÊåÅ„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨ÊâæÂá∫‰∫îÂÄãÈÄôÊ®£ÁöÑ DH Ë¶ÅÊ±ÇÔºöÊàêÂäüÁöÑ AI Á≥ªÁµ±ÈúÄË¶ÅËÉΩÂ§† 1) ËàáÔºà‰∫∫È°ûÔºâÂ≠∏ËÄÖÂêà‰ΩúÔºõ2) ÊîØÊè¥Ë≥áÊñôÊâπË©ïÔºõ3) ÊîØÊè¥Â∑•ÂÖ∑ÊâπË©ïÔºõ4) ÂØüË¶∫‰∏¶ËøéÂêàÂêÑÁ®ÆËßÄÈªûÔºõ5) ÊîØÊè¥ÈÅ†Ë∑ùÂíåËøëË∑ùÈõ¢Èñ±ËÆÄ„ÄÇÊàëÂÄëÂ∞áÊ∑∑ÂêàÊô∫ËÉΩÁöÑ CARE ÂéüÂâáÔºàÂçî‰Ωú„ÄÅÈÅ©Êáâ„ÄÅË≤†Ë≤¨ÂíåÂèØËß£ÈáãÔºâ‰ΩúÁÇ∫ÁêÜË´ñÊû∂ÊßãÔºå‰∏¶Â∞áÈÄô‰∫õÂéüÂâáÂ∞çÊáâÂà∞ DH Ë¶ÅÊ±Ç„ÄÇÂú®Ê≠§Â∞çÊáâ‰∏≠ÔºåÊàëÂÄëÁ¥çÂÖ•ÁØÑ‰æãÁ†îÁ©∂Â∞àÊ°à„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éÂ¶Ç‰ΩïÂ∞á DH ÁöÑË¶ãËß£ÊáâÁî®Êñº HIÔºå‰∏¶Ë®éË´ñÁµêÂêàÈÄôÂÖ©ÂÄãÂ≠∏ÁßëÁöÑÈñãÊîæÊåëÊà∞„ÄÇ

##### **Ethical Framework for Responsible Foundational Models in Medical Imaging**
2406.11868v1 by Abhijit Das, Debesh Jha, Jasmer Sanjotra, Onkar Susladkar, Suramyaa Sarkar, Ashish Rauniyar, Nikhil Tomar, Vanshali Sharma, Ulas Bagci

Foundational models (FMs) have tremendous potential to revolutionize medical
imaging. However, their deployment in real-world clinical settings demands
extensive ethical considerations. This paper aims to highlight the ethical
concerns related to FMs and propose a framework to guide their responsible
development and implementation within medicine. We meticulously examine ethical
issues such as privacy of patient data, bias mitigation, algorithmic
transparency, explainability and accountability. The proposed framework is
designed to prioritize patient welfare, mitigate potential risks, and foster
trust in AI-assisted healthcare.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°Âûã (FM) ÂÖ∑ÊúâÂæπÂ∫ïÊîπËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïåËá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÂª£Ê≥õÁöÑÂÄ´ÁêÜËÄÉÈáè„ÄÇÊú¨ÊñáÊó®Âú®Âº∑Ë™øËàá FM Áõ∏ÈóúÁöÑÂÄ´ÁêÜÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ°ÜÊû∂‰æÜÊåáÂ∞éÂÆÉÂÄëÂú®ÈÜ´Â≠∏‰∏≠ÁöÑË≤†Ë≤¨‰ªªÈñãÁôºÂíåÂØ¶ÊñΩ„ÄÇÊàëÂÄë‰ªîÁ¥∞ÂØ©Êü•‰∫ÜÂÄ´ÁêÜÂïèÈ°åÔºå‰æãÂ¶ÇÊÇ£ËÄÖÊï∏ÊìöÈö±ÁßÅ„ÄÅÂÅèÂ∑ÆÁ∑©Ëß£„ÄÅÊºîÁÆóÊ≥ïÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÂïèË≤¨Âà∂„ÄÇÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂Êó®Âú®ÂÑ™ÂÖàËÄÉÊÖÆÊÇ£ËÄÖÁ¶èÂà©„ÄÅÊ∏õËºïÊΩõÂú®È¢®Èö™Ôºå‰∏¶ÂüπÈ§äÂ∞ç AI ËºîÂä©ÈÜ´ÁôÇ‰øùÂÅ•ÁöÑ‰ø°‰ªª„ÄÇ

##### **Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**
2404.07239v1 by Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak

Thyroid cancer is an increasing global health concern that requires advanced
diagnostic methods. The application of AI and radiomics to thyroid cancer
diagnosis is examined in this review. A review of multiple databases was
conducted in compliance with PRISMA guidelines until October 2023. A
combination of keywords led to the discovery of an English academic publication
on thyroid cancer and related subjects. 267 papers were returned from the
original search after 109 duplicates were removed. Relevant studies were
selected according to predetermined criteria after 124 articles were eliminated
based on an examination of their abstract and title. After the comprehensive
analysis, an additional six studies were excluded. Among the 28 included
studies, radiomics analysis, which incorporates ultrasound (US) images,
demonstrated its effectiveness in diagnosing thyroid cancer. Various results
were noted, some of the studies presenting new strategies that outperformed the
status quo. The literature has emphasized various challenges faced by AI
models, including interpretability issues, dataset constraints, and operator
dependence. The synthesized findings of the 28 included studies mentioned the
need for standardization efforts and prospective multicenter studies to address
these concerns. Furthermore, approaches to overcome these obstacles were
identified, such as advances in explainable AI technology and personalized
medicine techniques. The review focuses on how AI and radiomics could transform
the diagnosis and treatment of thyroid cancer. Despite challenges, future
research on multidisciplinary cooperation, clinical applicability validation,
and algorithm improvement holds the potential to improve patient outcomes and
diagnostic precision in the treatment of thyroid cancer.

ÊëòË¶ÅÔºöÁî≤ÁãÄËÖ∫ÁôåÊòØ‰∏ÄÁ®ÆÊó•ÁõäÂö¥ÈáçÁöÑÂÖ®ÁêÉÂÅ•Â∫∑ÂïèÈ°åÔºåÈúÄË¶ÅÂÖàÈÄ≤ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÇÊú¨ÁØáË©ïË´ñÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËàáÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂú®Áî≤ÁãÄËÖ∫ÁôåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÂú®Á¨¶Âêà PRISMA ÊåáÂçóÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞çÂ§öÂÄãË≥áÊñôÂ∫´ÈÄ≤Ë°å‰∫ÜÂõûÈ°ßÔºåÁõ¥Âà∞ 2023 Âπ¥ 10 Êúà„ÄÇÈÄöÈÅéÁµêÂêàÈóúÈçµÂ≠óÔºåÁôºÁèæ‰∫Ü‰∏ÄÁØáÈóúÊñºÁî≤ÁãÄËÖ∫ÁôåÂíåÁõ∏Èóú‰∏ªÈ°åÁöÑËã±ÊñáÂ≠∏Ë°ìÂá∫ÁâàÁâ©„ÄÇÂú®ÁßªÈô§ 109 ÁØáÈáçË§áÊñáÁçªÂæåÔºåÂéüÂßãÊêúÂ∞ãÂÖ±ÂõûÂÇ≥ 267 ÁØáË´ñÊñá„ÄÇÂú®Ê†πÊìöÈ†êÂÖàÁ¢∫ÂÆöÁöÑÊ®ôÊ∫ñÔºåÊ∑òÊ±∞‰∫Ü 124 ÁØáÊñáÁ´†ÁöÑÊëòË¶ÅÂíåÊ®ôÈ°åÂæåÔºåÈÅ∏Âá∫‰∫ÜÁõ∏ÈóúÁ†îÁ©∂„ÄÇÂú®ÈÄ≤Ë°åÂÖ®Èù¢ÂàÜÊûêÂæåÔºåÈ°çÂ§ñÊéíÈô§‰∫ÜÂÖ≠È†ÖÁ†îÁ©∂„ÄÇÂú®Á¥çÂÖ•ÁöÑ 28 È†ÖÁ†îÁ©∂‰∏≠ÔºåÁµêÂêàË∂ÖÈü≥Ê≥¢ (US) ÂΩ±ÂÉèÁöÑÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®Ë®∫Êñ∑Áî≤ÁãÄËÖ∫ÁôåÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂ÁµêÊûú‰∏ç‰∏ÄÔºåÊúâ‰∫õÁ†îÁ©∂ÊèêÂá∫‰∫ÜÂÑ™ÊñºÁèæÁãÄÁöÑÊñ∞Á≠ñÁï•„ÄÇÊñáÁçªÂº∑Ë™ø‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÈù¢Ëá®ÁöÑÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨ÂèØËß£ÈáãÊÄßÂïèÈ°å„ÄÅË≥áÊñôÈõÜÈôêÂà∂ÂíåÊìç‰ΩúÂì°‰æùË≥¥ÊÄß„ÄÇ28 È†ÖÁ¥çÂÖ•Á†îÁ©∂ÁöÑÁ∂úÂêàÁôºÁèæÊèêÂà∞ÔºåÈúÄË¶ÅÊ®ôÊ∫ñÂåñÂ∑•‰ΩúÂíåÂâçÁûªÊÄßÂ§ö‰∏≠ÂøÉÁ†îÁ©∂‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¢∫ÂÆö‰∫ÜÂÖãÊúçÈÄô‰∫õÈöúÁ§ôÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂèØËß£Èáã‰∫∫Â∑•Êô∫ËÉΩÊäÄË°ìÂíåÂÄã‰∫∫ÂåñÈÜ´ÁôÇÊäÄË°ìÁöÑÈÄ≤Ê≠•„ÄÇÊú¨ÁØáË©ïË´ñÈáçÈªûÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÂíåÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂ¶Ç‰ΩïËΩâËÆäÁî≤ÁãÄËÖ∫ÁôåÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇ„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÊú™‰æÜÂ∞çÂ§öÂ≠∏ÁßëÂêà‰Ωú„ÄÅËá®Â∫äÈÅ©Áî®ÊÄßÈ©óË≠âÂíåÊºîÁÆóÊ≥ïÊîπÈÄ≤ÁöÑÁ†îÁ©∂Ôºå‰ªçÊúâÊΩõÂäõÊîπÂñÑÁî≤ÁãÄËÖ∫ÁôåÊ≤ªÁôÇ‰∏≠ÁöÑÊÇ£ËÄÖÈ†êÂæåÂíåË®∫Êñ∑Á≤æÊ∫ñÂ∫¶„ÄÇ

##### **Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**
2404.04686v1 by Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena, Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah Nafidi, Mohammed Bourhia

Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºå‰π≥ÁôåÁöÑÁõõË°åÁéáËøÖÈÄüÂ¢ûÂä†Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫ÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†‰πã‰∏Ä„ÄÇÂú®ÊâÄÊúâÁôåÁóá‰∏≠Ôºå‰π≥ÁôåËøÑ‰ªäÁÇ∫Ê≠¢ÊòØÊúÄÂ∏∏Ë¶ãÁöÑ„ÄÇÊâãÂãïË®∫Êñ∑Ê≠§ÁñæÁóÖÈúÄË¶ÅÂ§ßÈáèÁöÑÊôÇÈñìÂíåÂ∞àÊ•≠Áü•Ë≠ò„ÄÇÁî±Êñº‰π≥ÁôåÁöÑÊ™¢Ê∏¨ÈÅéÁ®ãËÄóÊôÇÔºåÂõ†Ê≠§ÈÄèÈÅéÂª∫Á´ãÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨ÔºåÊúâÂä©ÊñºÈò≤Ê≠¢ÂÖ∂ÈÄ≤‰∏ÄÊ≠•Êì¥Êï£„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÂèØËß£Èáã AI Âú®ÂàÜÈ°û‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄë‰∏çÂÉÖÂèØ‰ª•Êèê‰æõÊ∫ñÁ¢∫ÁöÑÈ†êÊ∏¨ÔºåÈÇÑÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÂ¶Ç‰ΩïÂÅöÂá∫Ê±∫Á≠ñÔºåÊúâÂä©ÊñºÁêÜËß£Âíå‰ø°Ë≥¥ÂàÜÈ°ûÁµêÊûú„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∏¶ÊØîËºÉ‰∫Ü‰∫îÁ®Æ‰∏çÂêåÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÅÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏Ôºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄã‰∏ªË¶ÅÁöÑË≥áÊñôÈõÜÔºàÈÅîÂç°ÈÜ´Â≠∏Èô¢ÈÜ´Èô¢ÁöÑ 500 ÂêçÊÇ£ËÄÖÔºâ„ÄÇ‰∫îÁ®Æ‰∏çÂêåÁöÑÁõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÔºåÂåÖÊã¨Ê±∫Á≠ñÊ®π„ÄÅÈö®Ê©üÊ£ÆÊûó„ÄÅÈÇèËºØËø¥Ê≠∏„ÄÅÊú¥Á¥†Ë≤ùÊ∞èÂíå XGBoostÔºåÂ∑≤Áî®ÊñºÂú®ÊàëÂÄëÁöÑË≥áÊñôÈõÜ‰∏äÂèñÂæóÊúÄ‰Ω≥ÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂Â∞á SHAP ÂàÜÊûêÊáâÁî®Êñº XGBoost Ê®°ÂûãÔºå‰ª•Ëß£ÈáãÊ®°ÂûãÁöÑÈ†êÊ∏¨‰∏¶‰∫ÜËß£ÊØèÂÄãÁâπÂæµÂ∞çÊ®°ÂûãËº∏Âá∫ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂπæÁ®ÆÊºîÁÆóÊ≥ïÂ∞çË≥áÊñôÈÄ≤Ë°åÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰∏¶ËàáË©≤È†òÂüüÁöÑÂÖ∂‰ªñÊñáÁçªÈÄ≤Ë°åÂ∞çÊØî„ÄÇÂú®ÊúÄÂæåË©ï‰º∞ÂæåÔºåÊú¨Á†îÁ©∂ÁôºÁèæ XGBoost ÈÅîÂà∞‰∫ÜÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÊ∫ñÁ¢∫Â∫¶ÔºåÁÇ∫ 97%„ÄÇ</paragraph>

##### **Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**
2404.03892v3 by Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí (DL) Áî®ÊñºÂæû‰π≥ÊàøÊîùÂΩ±Ë°ìÂΩ±ÂÉèË®∫Êñ∑‰π≥ÁôåÁöÑÊ®°ÂûãÈÄöÂ∏∏‰ª•„ÄåÈªëÁõíÂ≠ê„ÄçÊñπÂºèÈÅã‰ΩúÔºåÈÄô‰ΩøÂæóÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Èõ£‰ª•‰ø°‰ªªÂíåÁêÜËß£ÂÖ∂Ê±∫Á≠ñÈÅéÁ®ã„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊï¥ÂêàÊû∂ÊßãÔºåÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI)Ôºå‰ª•‰ΩøÁî® CBIS-DDSM Ë≥áÊñôÈõÜÂ¢ûÂº∑‰π≥ÁôåÁöÑË®∫Êñ∑„ÄÇÊñπÊ≥ïÂåÖÂê´‰∏ÄÂÄãÁ≤æÁ¥∞ÁöÑË≥áÊñôÂâçËôïÁêÜÁÆ°Á∑öÂíåÈÄ≤ÈöéË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÔºå‰ª•Â∞çÊäóË≥áÊñôÈõÜÈôêÂà∂Ôºå‰∏¶Êé°Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÁ∂≤Ë∑ØÔºà‰æãÂ¶Ç VGG-16„ÄÅInception-V3 Âíå ResNetÔºâÈÄ≤Ë°åÈÅ∑ÁßªÂ≠∏Áøí„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÈáçÈªûÊòØË©ï‰º∞ XAI Âú®Ëß£ÈáãÊ®°ÂûãÈ†êÊ∏¨‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÈáçÈªûÂà©Áî®Ë±™ÊñØÂ§öÂ§´Ê∏¨Â∫¶ÈáèÂåñË©ï‰º∞ AI ÁîüÊàêÁöÑËß£ÈáãÂíåÂ∞àÂÆ∂Ë®ªËß£‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ∞çÊñº XAI Âú®‰øÉÈÄ≤ AI ËºîÂä©Ë®∫Êñ∑‰∏≠ÁöÑÂèØ‰ø°Â∫¶ÂíåÂÄ´ÁêÜÂÖ¨Âπ≥ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÁôºÁèæË™™Êòé‰∫Ü CNN Âíå XAI Âú®Êé®ÈÄ≤‰π≥ÁôåË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊúâÊïàÂçî‰ΩúÔºåÂæûËÄå‰øÉÈÄ≤‰∫ÜÂÖàÈÄ≤ AI ÊäÄË°ìÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊõ¥È†ÜÊö¢Êï¥Âêà„ÄÇÈÄèÈÅéÂ¢ûÂº∑ AI È©ÖÂãïÊ±∫Á≠ñÁöÑÂèØËß£ÈáãÊÄßÔºåÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ AI Á≥ªÁµ±ÂíåÈÜ´ÁôÇÂæûÊ•≠‰∫∫Âì°‰πãÈñìÁöÑÊîπÂñÑÂçî‰ΩúÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÊúÄÁµÇË±êÂØå‰∫ÜÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ†îÁ©∂ÁöÑÂΩ±ÈüøÈÅ†ÈÅ†Ë∂ÖÂá∫‰∫ÜÁõÆÂâçÁöÑÊäÄË°ì„ÄÇÂÆÉÈºìÂãµÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂Â¶Ç‰ΩïÁµêÂêàÂ§öÊ®°ÂºèË≥áÊñô‰∏¶ÊîπÂñÑ AI Ëß£ÈáãÔºå‰ª•ÊªøË∂≥Ëá®Â∫äÂØ¶ÂãôÁöÑÈúÄÊ±Ç„ÄÇ

##### **Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives**
2404.00320v2 by Xingrui Gu, Zhixuan Wang, Irisa Jin, Zekun Wu

This research presents a novel multimodal data fusion methodology for pain
behavior recognition, integrating statistical correlation analysis with
human-centered insights. Our approach introduces two key innovations: 1)
integrating data-driven statistical relevance weights into the fusion strategy
to effectively utilize complementary information from heterogeneous modalities,
and 2) incorporating human-centric movement characteristics into multimodal
representation learning for detailed modeling of pain behaviors. Validated
across various deep learning architectures, our method demonstrates superior
performance and broad applicability. We propose a customizable framework that
aligns each modality with a suitable classifier based on statistical
significance, advancing personalized and effective multimodal fusion.
Furthermore, our methodology provides explainable analysis of multimodal data,
contributing to interpretable and explainable AI in healthcare. By highlighting
the importance of data diversity and modality-specific representations, we
enhance traditional fusion techniques and set new standards for recognizing
complex pain behaviors. Our findings have significant implications for
promoting patient-centered healthcare interventions and supporting explainable
clinical decision-making.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ§öÊ®°ÊÖãÊï∏ÊìöËûçÂêàÊñπÊ≥ïÔºåÁî®ÊñºÁñºÁóõË°åÁÇ∫Ë≠òÂà•ÔºåÂ∞áÁµ±Ë®àÁõ∏ÈóúÂàÜÊûêËàá‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑË¶ãËß£Áõ∏ÁµêÂêà„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂºïÂÖ•‰∫ÜÂÖ©È†ÖÈóúÈçµÂâµÊñ∞Ôºö1) Â∞áÊï∏ÊìöÈ©ÖÂãïÁöÑÁµ±Ë®àÁõ∏ÈóúÊ¨äÈáçÊï¥ÂêàÂà∞ËûçÂêàÁ≠ñÁï•‰∏≠Ôºå‰ª•ÊúâÊïàÂà©Áî®‰æÜËá™Áï∞Ë≥™Ê®°ÊÖãÁöÑË£úÂÖÖ‰ø°ÊÅØÔºå‰ª•Âèä 2) Â∞á‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑÈÅãÂãïÁâπÂæµÁ¥çÂÖ•Â§öÊ®°ÊÖãË°®Á§∫Â≠∏Áøí‰∏≠Ôºå‰ª•Ë©≥Á¥∞Âª∫Ê®°ÁñºÁóõË°åÁÇ∫„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂêÑÁ®ÆÊ∑±Â∫¶Â≠∏ÁøíÊû∂Êßã‰∏≠ÂæóÂà∞È©óË≠âÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩÂíåÂª£Ê≥õÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËá™ÂÆöÁæ©ÁöÑÊ°ÜÊû∂ÔºåÊ†πÊìöÁµ±Ë®àÈ°ØËëóÊÄßÂ∞áÊØèÂÄãÊ®°ÊÖãËàáÂêàÈÅ©ÁöÑÂàÜÈ°ûÂô®Â∞çÈΩäÔºåÊé®ÈÄ≤ÂÄãÊÄßÂåñÂíåÊúâÊïàÁöÑÂ§öÊ®°ÊÖãËûçÂêà„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊèê‰æõÂ∞çÂ§öÊ®°ÊÖãÊï∏ÊìöÁöÑÂèØËß£ÈáãÂàÜÊûêÔºåÊúâÂä©ÊñºÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂèØËß£ÈáãÂíåÂèØËß£Èáã AI„ÄÇÈÄöÈÅéÂº∑Ë™øÊï∏ÊìöÂ§öÊ®£ÊÄßÂíåÊ®°ÊÖãÁâπÂÆöË°®Á§∫ÁöÑÈáçË¶ÅÊÄßÔºåÊàëÂÄëÂ¢ûÂº∑‰∫ÜÂÇ≥Áµ±ÁöÑËûçÂêàÊäÄË°ìÔºå‰∏¶ÁÇ∫Ë≠òÂà•Ë§áÈõúÁöÑÁñºÁóõË°åÁÇ∫Ë®≠ÂÆö‰∫ÜÊñ∞ÁöÑÊ®ôÊ∫ñ„ÄÇÊàëÂÄëÁöÑÁôºÁèæÂ∞ç‰øÉÈÄ≤‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÈÜ´ÁôÇ‰øùÂÅ•Âπ≤È†êÂíåÊîØÊåÅÂèØËß£ÈáãÁöÑËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂÖ∑ÊúâÈáçË¶ÅÊÑèÁæ©„ÄÇ

##### **Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**
2403.17873v1 by Andrea Ferrario, Alberto Termine, Alessandro Facchini

Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.

ÊëòË¶ÅÔºö‰ª•‰∫∫‰∏∫Êú¨ÁöÑÂèØËß£Èáä AI (HCXAI) ÂÄ°ÂØºÂ∞ÜÁ§æ‰ºöÂ±ÇÈù¢Êï¥ÂêàÂà∞ AI Ëß£Èáä‰∏≠„ÄÇHCXAI ËØùËØ≠ÁöÑÊ†∏ÂøÉÊòØÁ§æ‰ºöÈÄèÊòéÂ∫¶ (ST) Ê°ÜÊû∂ÔºåÂÖ∂ÁõÆÊ†áÊòØËÆ© AI Á≥ªÁªüÁöÑÁ§æ‰ºöÁªÑÁªáËÉåÊôØÂØπÁî®Êà∑Êù•ËØ¥ÊòØÂèØÁêÜËß£ÁöÑ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âª∫ËÆÆÊâ©Â±ï ST Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰∏≠Á§æ‰ºöÈîôËØØÂΩíÂõ†ÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂøÉÁêÜÂÅ•Â∫∑Á≠âÊïèÊÑüÈ¢ÜÂüü„ÄÇ‰∫ãÂÆû‰∏äÔºåLLM ËÉΩÂ§üÂá∫Ëâ≤Âú∞Ê®°ÊãüËßíËâ≤Âíå‰∫∫Ê†ºÔºåËøôÂèØËÉΩÂØºËá¥ËÆæËÆ°ËÄÖÁöÑÊÑèÂõæÂíåÁî®Êà∑ÂØπÁ§æ‰ºöÂ±ûÊÄßÁöÑËÆ§Áü•‰πãÈó¥Âá∫Áé∞ÈîôÈÖçÔºå‰ªéËÄåÊúâÈ£éÈô©‰øÉËøõÊÉÖÁª™ÊìçÁ∫µÂíåÂç±Èô©Ë°å‰∏∫„ÄÅËÆ§Áü•‰∏çÂÖ¨Ê≠£Âíå‰∏çÂêàÁêÜÁöÑ‰ø°‰ªª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÁî®Á¨¨‰∫î‰∏™‚ÄúW ÈóÆÈ¢ò‚ÄùÊù•Â¢ûÂº∫ ST Ê°ÜÊû∂Ôºå‰ª•ÊòéÁ°ÆËÆæËÆ°ËÄÖÂíåÁî®Êà∑Ëµã‰∫à LLM ÁöÑÂÖ∑‰ΩìÁ§æ‰ºöÂ±ûÊÄß„ÄÇÊ≠§Ë°•ÂÖÖÊó®Âú®Âº•Âêà LLM ËÉΩÂäõÂíåÁî®Êà∑ËÆ§Áü•‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰øÉËøõÂü∫‰∫é LLM ÁöÑÊäÄÊúØÂú®ÈÅìÂæ∑‰∏äË¥üË¥£‰ªªÂú∞ÂºÄÂèëÂíå‰ΩøÁî®„ÄÇ

##### **Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**
2403.18871v1 by Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu

Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÊ∞£ËÉ∏ÊòØ‰∏ÄÁ®ÆÂõ†ËÇ∫ÈÉ®ËàáËÉ∏Â£Å‰πãÈñìÁï∞Â∏∏ÈõÜÊ∞£ÊâÄÂºïËµ∑ÁöÑÊÄ•ÊÄßËÉ∏ËÖîÁñæÁóÖ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê∑±Â∫¶Â≠∏ÁøíÔºàDLÔºâÊ®°ÂûãÁ∂ìÂ∏∏‰º¥Èö®ÁöÑ‰∏çÈÄèÊòéÊÄßÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÊñπÊ≥ïÂ∑≤Ë¢´ÂºïÂÖ•ÔºåÁî®ÊñºÊ¶ÇËø∞Ëàá DL Ê®°ÂûãÂÅöÂá∫ÁöÑÊ∞£ËÉ∏Ë®∫Êñ∑Áõ∏ÈóúÁöÑÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õËß£ÈáãÊúâÊôÇÊúÉËàáÂØ¶ÈöõÁóÖÁÅ∂ÂçÄÂüüÊúâÊâÄÂá∫ÂÖ•ÔºåÁ™ÅÈ°ØÂá∫ÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊñπÊ≥ïÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÂ∞áÊ∞£ËÉ∏ÁöÑËá®Â∫äÁü•Ë≠òÁ¥çÂÖ• XAI ÊñπÊ≥ïÁî¢ÁîüÁöÑÊ®°ÂûãËß£Èáã‰∏≠ÔºåÂæûËÄåÊèêÂçáÈÄô‰∫õËß£ÈáãÁöÑÂìÅË≥™„ÄÇÂà©Áî®ÊîæÂ∞ÑÁßëÈÜ´Â∏´Âª∫Á´ãÁöÑÁóÖÁÅ∂ÊèèÁπ™ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ¶ñÂÖàÁî¢Áîü‰∏ÄÂÄãÊ®°ÊùøÔºåÁî®ÊñºË°®Á§∫Ê∞£ËÉ∏ÂèØËÉΩÁôºÁîüÁöÑÂçÄÂüü„ÄÇÁÑ∂ÂæåÂ∞áÊ≠§Ê®°ÊùøÁñäÂä†Âú®Ê®°ÂûãËß£Èáã‰∏äÔºå‰ª•ÁØ©ÈÅ∏Âá∫Ë∂ÖÂá∫Ê®°ÊùøÈÇäÁïåÁöÑÁÑ°ÈóúËß£Èáã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÂÖ∂ÊïàÂäõÔºåÊàëÂÄëÂ∞ç‰∏âÁ®Æ XAI ÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÊØîËºÉÂàÜÊûêÔºåÂú®ÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏≠Ëß£ÈáãÂÖ©ÂÄã DL Ê®°ÂûãÊôÇÔºåÂàÜÂà•Êé°Áî®Âíå‰∏çÊé°Áî®ÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞é„ÄÇÁµêÊûúÔºöÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Âª∫Á´ãÊñº‰∏âÁ®Æ XAI ÊñπÊ≥ï„ÄÅÂÖ©ÂÄã DL Ê®°ÂûãÂíåÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÂçÅ‰∫åÁ®ÆÂü∫Ê∫ñÊÉÖÂ¢É‰∏≠ÔºåÂßãÁµÇÊîπÂñÑ‰∫ÜÂü∫Ê∫ñ XAI ÊñπÊ≥ï„ÄÇÂú®ÊØîËºÉÊ®°ÂûãËß£ÈáãÂíåÁúüÂØ¶ÁóÖÁÅ∂ÂçÄÂüüÊôÇÔºåÈÄèÈÅéÂü∫Ê∫ñÊïàËÉΩÁöÑÊïàËÉΩÊîπÈÄ≤Ë®àÁÆóÂá∫ÁöÑÂπ≥ÂùáÂ¢ûÈáèÁôæÂàÜÊØîÁÇ∫‰∫§ÈõÜÊØîÔºàIoUÔºâÁöÑ 97.8% ÂíåÈ™∞Â≠êÁõ∏‰ººÊÄß‰øÇÊï∏ÔºàDSCÔºâÁöÑ 94.1%„ÄÇÁµêË´ñÔºöÂú®Ê∞£ËÉ∏Ë®∫Êñ∑ÁöÑËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÁî®ÊñºÊîπÂñÑ AI Ëß£Èáã„ÄÇÊàëÂÄëÈ†êÊúüÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞éÂ∞áÈÄèÈÅéÊï¥ÂêàËá®Â∫äÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÔºåÁÇ∫Èó°Êòé AI Ê®°ÂûãÂª∫Á´ã‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**
2403.01580v1 by S√©amus Lankford

In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.

ÊëòË¶ÅÔºö<paragraph>Âú®Áï∂ÂâçÊ©üÂô®ÁøªË≠Ø (MT) È†òÂüü‰∏≠ÔºåTransformer Êû∂ÊßãËÑ´Á©éËÄåÂá∫ÔºåÊàêÁÇ∫ÈªÉÈáëÊ®ôÊ∫ñÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ´òË≥áÊ∫êË™ûË®ÄÂ∞ç„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂÖ∂Â∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÊïàËÉΩÔºåÂåÖÊã¨Ëã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûË™ûË®ÄÂ∞ç„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊú¨Á†îÁ©∂Ë≠òÂà•Âá∫ÊúÄ‰Ω≥Ë∂ÖÂèÉÊï∏ÂíåÂ≠êË©ûÊ®°ÂûãÈ°ûÂûãÔºå‰ª•È°ØËëóÊèêÈ´ò Transformer Ê®°ÂûãÂ∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÂìÅË≥™„ÄÇ
‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑÂπ≥Ë°åË≥áÊñôÈõÜÁöÑÁ®ÄÁº∫ÊúÉÈòªÁ§ô MT ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈñãÁôº‰∫Ü gaHealthÔºåÈÄôÊòØÊÑõÁàæËò≠Ë™ûÁöÑÁ¨¨‰∏ÄÂÄãÈõôË™ûÂÅ•Â∫∑Ë≥áÊñôË™ûÊñôÂ∫´„ÄÇÂ∞àÊ≥®ÊñºÂÅ•Â∫∑È†òÂüüÔºå‰ΩøÁî®Ê≠§ÂüüÂÖßË≥áÊñôÈõÜÈñãÁôºÁöÑÊ®°ÂûãÂú® BLEU ÂæóÂàÜÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂ∏∏È°ØËëóÁöÑÈÄ≤Ê≠•ÔºåËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇÈö®Âæå‰ΩøÁî®Â§öÁ∂≠ÂìÅË≥™ÊåáÊ®ôÈåØË™§ÂàÜÈ°ûÊ≥ïÈÄ≤Ë°åÁöÑ‰∫∫Â∑•Ë©ï‰º∞È°ØÁ§∫ÔºåËàáÂü∫Êñº RNN ÁöÑÂ∞çÊáâÊ®°ÂûãÁõ∏ÊØîÔºåTransformer Á≥ªÁµ±Âú®Ê∏õÂ∞ëÊ∫ñÁ¢∫ÊÄßÂíåÊµÅÊö¢ÊÄßÈåØË™§ÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊÄßËÉΩ„ÄÇ
Ê≠§Â§ñÔºåÊú¨Ë´ñÊñá‰ªãÁ¥π‰∫Ü adaptNMT Âíå adaptMLLMÔºåÈÄôÂÖ©ÂÄãÈñãÊ∫êÊáâÁî®Á®ãÂºèÁ∞°Âåñ‰∫ÜÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠ØÊ®°ÂûãÁöÑÈñãÁôº„ÄÅÂæÆË™øÂíåÈÉ®ÁΩ≤„ÄÇÈÄô‰∫õÂ∑•ÂÖ∑Â§ßÂπÖÁ∞°Âåñ‰∫ÜË®≠ÂÆöÂíåË©ï‰º∞ÊµÅÁ®ãÔºåËÆì MT Êõ¥ÂÆπÊòìËÆìÈñãÁôº‰∫∫Âì°ÂíåÁøªË≠Ø‰∫∫Âì°‰ΩøÁî®„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåadaptNMT ‰ª• OpenNMT ÁîüÊÖãÁ≥ªÁµ±ÁÇ∫Âü∫Á§éÔºåÈÄöÈÅéÂº∑Ë™øÊ®°ÂûãÈñãÁôºÁöÑÁí∞Â¢ÉË∂≥Ë∑°‰æÜ‰øÉÈÄ≤ÁîüÊÖãÂèãÂ•ΩÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ†îÁ©∂„ÄÇËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåadaptMLLM Â∞ç MLLM ÁöÑÂæÆË™øË≠âÊòé‰∫ÜËã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûÈÄôÂÖ©ÂÄã‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÊÄßËÉΩÈÄ≤Ê≠•„ÄÇ</paragraph>

##### **Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**
2402.18600v1 by Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya

Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖÔºàDMÔºâ‰ΩøÊÇ£ËÄÖÂÆπÊòìÂá∫ÁèæË°ÄÁÆ°‰ΩµÁôºÁóá„ÄÇ
Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂíåË°ÄÁÆ°ÂèçÊò†Ë∫´È´îÁöÑÂæÆË°ÄÁÆ°ÂíåÂ∑®Ë°ÄÁÆ°ÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇÂÆÉÂÄëÂèØÁî®ÊñºË®∫Êñ∑Á≥ñÂ∞øÁóÖ‰ΩµÁôºÁóáÔºåÂåÖÊã¨Á≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆäÔºàDRÔºâ„ÄÅÁ•ûÁ∂ìÁóÖËÆä„ÄÅËÖéÁóÖÂíåÂãïËÑàÁ≤•Ê®£Á°¨ÂåñÊÄßÂøÉË°ÄÁÆ°ÁñæÁóÖÔºå‰ª•ÂèäÈ†êÊ∏¨ÂøÉË°ÄÁÆ°‰∫ã‰ª∂ÁöÑÈ¢®Èö™„ÄÇÁÇ∫‰ΩøÁî®Êï∏‰ΩçÂåñË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÈÄ≤Ë°åÈ´òÈÄöÈáè DR Ê™¢Ê∏¨ËÄåÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂïüÁî®Á≥ªÁµ±Â∑≤Âú®Ëá®Â∫äÊé°Áî®„ÄÇÈô§‰∫Ü DR ÁØ©Ê™¢Â§ñÔºåAI Êï¥Âêà‰πüÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ‰æÜÊáâÂ∞çËàáÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÊï¥È´îÁÖßË≠∑Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÂÖ®Èù¢ÂõûÈ°ßÂü∫ÊñºË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÁöÑ AI ÊáâÁî®Áõ∏ÈóúÁ†îÁ©∂ÁöÑÊñáÁçªÔºåÈÄô‰∫õÁ†îÁ©∂ËàáÁ≥ñÂ∞øÁóÖÁöÑË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜÊúâÈóú„ÄÇÊàëÂÄëÂ∞áÊèèËø∞Êï¥È´î AI ËºîÂä©Á≥ñÂ∞øÁóÖÁÖßË≠∑ÁöÑÁôºÁèæÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñº DR ÁØ©Ê™¢Ôºå‰∏¶Ë®éË´ñÂØ¶ÊñΩÊ≠§È°ûÁ≥ªÁµ±ÁöÑÈöúÁ§ôÔºåÂåÖÊã¨ËàáÂÄ´ÁêÜ„ÄÅË≥áÊñôÈö±ÁßÅ„ÄÅÂÖ¨Âπ≥Â≠òÂèñÂíåÂèØËß£ÈáãÊÄßÊúâÈóúÁöÑÂïèÈ°å„ÄÇÈÄèÈÅéË©ï‰º∞ÊÇ£ËÄÖÁöÑÂÅ•Â∫∑ÁãÄÊ≥ÅÔºåÂêåÊôÇËÄÉÈáèÁ≥ñÂ∞øÁóÖ‰ΩµÁôºÁóá‰ª•ÂèäÊú™‰æÜÂøÉË°ÄÁÆ°‰ΩµÁôºÁóáÁöÑÈ¢®Èö™È†êÂæåÔºåAI ËºîÂä©Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂàÜÊûêÊúâÊΩõÂäõÊàêÁÇ∫Á≥ñÂ∞øÁóÖÊÇ£ËÄÖÁèæ‰ª£ÂåñÂÄã‰∫∫ÂåñÈÜ´ÁôÇÁöÑ‰∏≠ÂøÉÂ∑•ÂÖ∑„ÄÇ

##### **Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**
2402.15027v2 by A. J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de Arana, AM. Lesage, S. Senecal, P-M. Leger

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÂæûÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑËßíÂ∫¶Êé¢Ë®é‰∏çÂêåÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊáâÁî®Âú®ÊïôËÇ≤‰∏äÁöÑÂèØÊé•ÂèóÊÄßÔºåÂåÖÊã¨Â≠∏Áîü„ÄÅËÄÅÂ∏´ÂíåÂÆ∂Èï∑„ÄÇÊâøË™ç AI Âú®ÊïôËÇ≤‰∏äÁöÑËΩâÂûãÊΩõÂäõÔºåÂÆÉËß£Ê±∫‰∫ÜËàáË≥áÊñôÈö±ÁßÅ„ÄÅAI ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíå AI ÁöÑÈÅìÂæ∑ÈÉ®ÁΩ≤Áõ∏ÈóúÁöÑÁñëÊÖÆ„ÄÇÈÄèÈÅéÂ∞èÊèíÊõ≤ÊñπÊ≥ïÔºåÂèÉËàáËÄÖË¢´ÂëàÁèæ‰∫ÜÂõõÁ®ÆÊÉÖÂ¢ÉÔºåÂÖ∂‰∏≠ AI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈö±ÁßÅÂèóÂà∞ÊìçÁ∏±„ÄÇÂú®ÊØèÂÄãÊÉÖÂ¢ÉÂæåÔºåÂèÉËàáËÄÖÂÆåÊàê‰∫Ü‰∏ÄÈ†ÖË™øÊü•ÔºåË©≤Ë™øÊü•ÊçïÊçâ‰∫Ü‰ªñÂÄëÂ∞ç AI ÁöÑÊï¥È´îÊïàÁî®„ÄÅÂÄã‰∫∫ÊïàÁî®„ÄÅÊ≠£Áæ©„ÄÅ‰ø°ÂøÉ„ÄÅÈ¢®Èö™ÂíåÂ¶ÇÊûúÂèØÁî®Ôºå‰ΩøÁî®ÊØèÂÄãÊÉÖÂ¢ÉÁöÑ AI ÁöÑÊÑèÂúñÁöÑÁúãÊ≥ï„ÄÇË≥áÊñôËíêÈõÜÂåÖÂê´‰æÜËá™Âêà‰ΩúÊ©üÊßãÂíåÁ§æÁæ§Â™íÈ´îÊ¥ªÂãïÁöÑ 1198 ‰ΩçÂ§öÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂèÉËàáËÄÖÁöÑÊúÄÁµÇÊ®£Êú¨Ôºå‰∏¶Â∞àÊ≥®ÊñºÂ∞çÂõõÂÄã AI ‰ΩøÁî®Ê°à‰æãÁöÑÂÄãÂà•ÂõûÊáâ„ÄÇÂ∞çË≥áÊñôÁöÑË™øËß£ÂàÜÊûêË°®ÊòéÔºåÂ∞ç AI ÁöÑÊé•ÂèóÂ∫¶Âíå‰ø°‰ªªÂú®Âà©ÂÆ≥Èóú‰øÇ‰∫∫ÂúòÈ´î‰πãÈñìÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÁôºÁèæÔºåAI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÈ´ò‰ΩéÁ®ãÂ∫¶‰πãÈñìÁöÑÈóúÈçµË™øËß£ËÄÖÔºå‰ª•Âèä‰ΩøÁî®‰∏çÂêåÊïôËÇ≤ AI ÁöÑÊÑèÂúñÔºåÂåÖÊã¨ÊÑüÁü•Âà∞ÁöÑÊï¥È´îÊïàÁî®„ÄÅÊ≠£Áæ©Âíå‰ø°ÂøÉ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™øÔºåÊé•Âèó AI Âú®ÊïôËÇ≤‰∏äÁöÑÊáâÁî®ÊòØ‰∏ÄÂÄãÂæÆÂ¶ô‰∏îÂ§öÈù¢ÂêëÁöÑÂïèÈ°åÔºåÈô§‰∫Ü‰∏çÂêåÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÁúãÊ≥ïÂ§ñÔºåÈÇÑÈúÄË¶Å‰ªîÁ¥∞ËÄÉÊÖÆÂÖ∑È´îÁöÑ AI ÊáâÁî®ÂèäÂÖ∂ÁâπÂæµ„ÄÇ

##### **Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**
2402.09474v2 by Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer

Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÂèØÁ©øÊà¥ÂºèÂñÆÂ∞éÁ®ãÂøÉÈõªÂúñ (ECG) Ë£ùÁΩÆÁöÑÈÅ†Á´ØÁóÖÊÇ£Áõ£Ê∏¨Âú®Êó©ÊúüÂÅµÊ∏¨ÂøÉËáüÁñæÁóÖÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØËàáÁî®ÊñºËá™ÂãïÂåñÂøÉËáüÁñæÁóÖÂÅµÊ∏¨ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÁµêÂêà‰ΩøÁî®ÊôÇ„ÄÇÂÖàÂâçÂ∑≤ÊúâÁ†îÁ©∂ÊáâÁî®Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑ AI ÊñπÊ≥ïÈÄ≤Ë°åÂøÉËáüÁñæÁóÖÂÅµÊ∏¨„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂ∞öÊú™Ë¢´Âª£Ê≥õÊé•ÂèóÁÇ∫Ëá®Â∫äË®∫Êñ∑ÁöÑÂèØÈù†ËºîÂä©Â∑•ÂÖ∑ÔºåÈÉ®ÂàÜÂéüÂõ†Âú®ÊñºÂúçÁπûË®±Â§ö AI ÊºîÁÆóÊ≥ïÁöÑÁï∂ÂâçÈªëÁÆ±ÊÑüÁü•„ÄÇÁâπÂà•ÊòØÔºåÊúâÂøÖË¶ÅÊâæÂá∫ÊúâÂä©ÊñºÂÅöÂá∫Ê∫ñÁ¢∫Ë®∫Êñ∑ÁöÑ ECG Ë®äËôüÈóúÈçµÁâπÂæµÔºåÂæûËÄåÂ¢ûÂº∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÔºå‰ª•Ê†πÊìöÂñÆÂ∞éÁ®ã ECG Ë≥áÊñôÊâæÂá∫ÂøÉÊàøÈ°´Âãï„ÄÇÊÆòÂ∑ÆÁ∂≤Ë∑Ø (ResNet) ÊñπÊ≥ï‰πüÂ∑≤ÈñãÁôºÂá∫‰æÜÔºå‰ª•‰æøËàáË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄô‰∫õÊ®°ÂûãÊáâÁî®Êñº Chapman-Shaoxing Ë≥áÊñôÈõÜÔºå‰ª•ÂàÜÈ°ûÂøÉÊàøÈ°´ÂãïÔºå‰ª•ÂèäÂè¶‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÂøÉÂæã‰∏çÊï¥ÔºåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÔºåÂíåÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãÁöÑÂøÉË∑≥„ÄÇÈÄô‰∫õÊ®°ÂûãËÉΩÂ§†ÊâæÂá∫Ê±∫ÂÆöÊúÄÁµÇÂàÜÈ°ûÁöÑÂøÉË∑≥ÈóúÈçµÂçÄÂüüÔºå‰∏¶Âº∑Ë™ø P Ê≥¢Âíå T Ê≥¢Ôºå‰ª•ÂèäÂøÉË∑≥ÊåÅÁ∫åÊôÇÈñìÂíåË®äËôüÊåØÂπÖÂú®ÂçÄÂàÜÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãËàáÂøÉÊàøÈ°´ÂãïÂíåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ</paragraph>

##### **Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**
2402.05127v1 by Aryan Agrawal

This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ÂÖàÈÄ≤Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÂíåÊ≤ªÁôÇÁöÑÊñ∞Ê®°ÂºèÔºöÁîüÊàêÂºèÈ†êË®ìÁ∑¥Transformer 4 (GPT-4)„ÄÅLlama 2 ËÅäÂ§©Ê©üÂô®‰∫∫Âíå Gemini„ÄÇÈÄô‰∫õ LLM Á∂ìÈÅéÂæÆË™øÔºåÂÖ∑ÂÇôÂ∞àÊ•≠ÊèêÁ§∫ÔºåÂèØË®∫Êñ∑„ÄÅËß£Èáã‰∏¶Âª∫Ë≠∞ÊÜÇÈ¨±ÁóáÁöÑÊ≤ªÁôÇ‰ªãÂÖ•ÊñπÊ≥ï„ÄÇ‰∏ÄÁ®ÆÁç®ÁâπÁöÑÂ∞ëÊ¨°ÊèêÁ§∫ÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÊ†πÊìö DSM-5 Ê®ôÊ∫ñÂàÜÊûêÂíåËß£ÈáãÊÜÇÈ¨±ÁóáÁãÄÁöÑËÉΩÂäõ„ÄÇÂú®‰∫íÂãïÈöéÊÆµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÂèÉËàáÂêåÁêÜÂøÉÂ∞çË©±ÁÆ°ÁêÜÔºåÂæû PsychDB ÂíåË™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊåáÂçóÁ≠âË≥áÊ∫ê‰∏≠Ê±≤ÂèñÔºå‰øÉÈÄ≤ËàáÁ∂ìÊ≠∑ÈáçÂ∫¶ÊÜÇÈ¨±ÁóáÁöÑ‰∫∫ÂÄëÁöÑÊîØÊåÅÊÄß‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÈÄôÈ†ÖÁ†îÁ©∂ÈÇÑ‰ªãÁ¥π‰∫Ü Illuminate Ë≥áÊñôÂ∫´ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®Æ CBT Ê®°ÁµÑÔºåÊúâÂä©ÊñºÂÄãÊÄßÂåñÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰ΩøÁî® F1 ÂàÜÊï∏„ÄÅÊ∫ñÁ¢∫Áéá„ÄÅÂè¨ÂõûÁéá„ÄÅÈ§òÂº¶Áõ∏‰ººÂ∫¶ÂíåÈù¢ÂêëÂè¨ÂõûÁéáÁöÑ Gisting Ë©ï‰º∞ÊõøË∫´ (ROUGE) Á≠âÊåáÊ®ôÔºåÂú®‰∏çÂêåÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠Ë©ï‰º∞ LLM ÁöÑË°®ÁèæÔºåË≠âÊòé‰∫ÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄß„ÄÇÈÄôÁ®ÆÁ∂úÂêàÊñπÊ≥ïÁµêÂêà‰∫ÜÂ∞ñÁ´ØÁöÑ AI ËàáÊó¢ÂÆöÁöÑÂøÉÁêÜÊñπÊ≥ïÔºåÁÇ∫ÂøÉÁêÜ‰øùÂÅ•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºå‰∏¶Â±ïÁ§∫‰∫Ü LLM Âú®Èù©Êñ∞ÊÜÇÈ¨±ÁóáË®∫Êñ∑ÂíåÊ≤ªÁôÇÁ≠ñÁï•ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**
2401.13324v5 by Timoth√©e Schmude, Laura Koesten, Torsten M√∂ller, Sebastian Tschiatschek

Every AI system that makes decisions about people has a group of stakeholders
that are personally affected by these decisions. However, explanations of AI
systems rarely address the information needs of this stakeholder group, who
often are AI novices. This creates a gap between conveyed information and
information that matters to those who are impacted by the system's decisions,
such as domain experts and decision subjects. To address this, we present the
"XAI Novice Question Bank," an extension of the XAI Question Bank containing a
catalog of information needs from AI novices in two use cases: employment
prediction and health monitoring. The catalog covers the categories of data,
system context, system usage, and system specifications. We gathered
information needs through task-based interviews where participants asked
questions about two AI systems to decide on their adoption and received verbal
explanations in response. Our analysis showed that participants' confidence
increased after receiving explanations but that their understanding faced
challenges. These included difficulties in locating information and in
assessing their own understanding, as well as attempts to outsource
understanding. Additionally, participants' prior perceptions of the systems'
risks and benefits influenced their information needs. Participants who
perceived high risks sought explanations about the intentions behind a system's
deployment, while those who perceived low risks rather asked about the system's
operation. Our work aims to support the inclusion of AI novices in
explainability efforts by highlighting their information needs, aims, and
challenges. We summarize our findings as five key implications that can inform
the design of future explanations for lay stakeholder audiences.

ÊëòË¶ÅÔºö<paragraph>ÊØèÂÄãÂ∞ç‰∫∫ÈÄ≤Ë°åÊ±∫Á≠ñÁöÑ AI Á≥ªÁµ±ÈÉΩÊúâ‰∏ÄÁæ§ÂÄã‰∫∫ÂèóÂà∞ÈÄô‰∫õÊ±∫Á≠ñÂΩ±ÈüøÁöÑÂà©ÁõäÈóú‰øÇ‰∫∫„ÄÇÁÑ∂ËÄåÔºåAI Á≥ªÁµ±ÁöÑË™™ÊòéÂæàÂ∞ëÈáùÂ∞çÈÄôÁæ§Âà©ÁõäÈóú‰øÇ‰∫∫ÁöÑË≥áË®äÈúÄÊ±ÇÔºå‰ªñÂÄëÈÄöÂ∏∏ÊòØ AI Êñ∞Êâã„ÄÇÈÄôÂú®ÂÇ≥ÈÅîÁöÑË≥áË®äÂíåÂ∞çÂèóÁ≥ªÁµ±Ê±∫Á≠ñÂΩ±ÈüøÁöÑ‰∫∫‰æÜË™™ÈáçË¶ÅÁöÑË≥áË®ä‰πãÈñìÔºåÈÄ†Êàê‰∫Ü‰∏ÄÈÅìÈ¥ªÊ∫ùÔºå‰æãÂ¶ÇÈ†òÂüüÂ∞àÂÆ∂ÂíåÊ±∫Á≠ñ‰∏ªÈ´î„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü„ÄåXAI Êñ∞ÊâãÂïèÈ°åÂ∫´„ÄçÔºåÈÄôÊòØ XAI ÂïèÈ°åÂ∫´ÁöÑÂª∂‰º∏ÔºåÂåÖÂê´‰æÜËá™ÂÖ©ÂÄã‰ΩøÁî®Ê°à‰æã‰∏≠ AI Êñ∞ÊâãÁöÑË≥áË®äÈúÄÊ±ÇÁõÆÈåÑÔºöÂ∞±Ê•≠È†êÊ∏¨ÂíåÂÅ•Â∫∑Áõ£Êéß„ÄÇÁõÆÈåÑÊ∂µËìã‰∫ÜË≥áÊñô„ÄÅÁ≥ªÁµ±ËÉåÊôØ„ÄÅÁ≥ªÁµ±‰ΩøÁî®ÂíåÁ≥ªÁµ±Ë¶èÊ†ºÁ≠âÈ°ûÂà•„ÄÇÊàëÂÄëÈÄèÈÅé‰ªªÂãôÂûãË®™Ë´áÊî∂ÈõÜË≥áË®äÈúÄÊ±ÇÔºåÂèÉËàáËÄÖÂú®ÂÖ∂‰∏≠Ë©¢Âïè‰∫ÜÂÖ©ÂÄã AI Á≥ªÁµ±ÁöÑÂïèÈ°åÔºå‰ª•Ê±∫ÂÆöÊé°Áî®ËàáÂê¶Ôºå‰∏¶Êî∂Âà∞Âè£È†≠Ë™™Êòé‰ΩúÁÇ∫ÂõûÊáâ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂèÉËàáËÄÖÂú®Êî∂Âà∞Ë™™ÊòéÂæåÔºå‰ø°ÂøÉÊúâÊâÄÊèêÂçáÔºå‰ΩÜ‰ªñÂÄëÁöÑÁêÜËß£Èù¢Ëá®ÊåëÊà∞„ÄÇÈÄô‰∫õÊåëÊà∞ÂåÖÊã¨Èõ£‰ª•ÊâæÂà∞Ë≥áË®äÂíåË©ï‰º∞Ëá™Â∑±ÁöÑÁêÜËß£Ôºå‰ª•ÂèäË©¶ÂúñÂ§ñÂåÖÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÂèÉËàáËÄÖÂ∞çÁ≥ªÁµ±È¢®Èö™ÂíåÂ•ΩËôïÁöÑÂÖàÂâçË™çÁü•ÂΩ±Èüø‰∫Ü‰ªñÂÄëÁöÑË≥áË®äÈúÄÊ±Ç„ÄÇË™çÁÇ∫È¢®Èö™È´òÁöÑ‰∫∫Â∞ãÊ±ÇÊúâÈóúÁ≥ªÁµ±ÈÉ®ÁΩ≤ËÉåÂæåÊÑèÂúñÁöÑË™™ÊòéÔºåËÄåË™çÁÇ∫È¢®Èö™‰ΩéÁöÑ‰∫∫ÂâáË©¢ÂïèÁ≥ªÁµ±ÁöÑÈÅã‰Ωú„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®ÈÄèÈÅéÂº∑Ë™ø‰ªñÂÄëÁöÑË≥áË®äÈúÄÊ±Ç„ÄÅÁõÆÊ®ôÂíåÊåëÊà∞Ôºå‰æÜÊîØÊåÅÂ∞á AI Êñ∞ÊâãÁ¥çÂÖ•ÂèØËß£ÈáãÊÄßÂ∑•‰Ωú„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ∏ΩÁµêÁÇ∫‰∫îÂÄãÈóúÈçµÂΩ±ÈüøÔºåÈÄô‰∫õÂΩ±ÈüøÂèØ‰ª•ÁÇ∫Êú™‰æÜÈáùÂ∞çÈùûÂ∞àÊ•≠Âà©ÁõäÁõ∏ÈóúËÄÖÂèóÁúæÁöÑË™™ÊòéË®≠Ë®àÊèê‰æõÂèÉËÄÉ„ÄÇ</paragraph>

##### **Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**
2401.02985v1 by Vahid Ashrafimoghari, Necdet G√ºrkan, Jordan W. Suchow

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂø´ÈÄüÊºîÈÄ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁîüÊàêÂºè AI ÁöÑÈ†òÂüüÔºåÁÇ∫ÂêÑÂÄãÈ†òÂüüÁöÑÊáâÁî®ÈñãÂïü‰∫ÜÊñ∞ÈÄîÂæëÔºå‰ΩÜÂÖ∂Âú®ÂïÜÊ•≠ÊïôËÇ≤‰∏≠ÁöÑËßíËâ≤‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂È¶ñÊ¨°ÂºïÂÖ•‰∫ÜÂü∫Ê∫ñÔºåÁî®‰ª•Ë©ï‰º∞‰∏ÉÂÄã‰∏ªË¶Å LLM ÁöÑÊïàËÉΩÔºåÂåÖÊã¨ OpenAI ÁöÑÊ®°Âûã (GPT-3.5 Turbo„ÄÅGPT-4 Âíå GPT-4 Turbo)„ÄÅGoogle ÁöÑÊ®°Âûã (PaLM 2„ÄÅGemini 1.0 Pro) Âíå Anthropic ÁöÑÊ®°Âûã (Claude 2 Âíå Claude 2.1)ÔºåÈÄô‰∫õÊ®°ÂûãÂ∞áÁî®ÊñºÁ†îÁ©∂ÁîüÂïÜÊ•≠Ë™≤Á®ãÂÖ•Â≠∏Á®ãÂ∫è‰∏≠ÁöÑÈóúÈçµËÄÉË©¶ GMAT„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§ßÂ§öÊï∏ LLM ÁöÑË°®ÁèæÈÉΩÂÑ™Êñº‰∫∫È°ûËÄÉÁîüÔºåÂÖ∂‰∏≠ GPT-4 Turbo ‰∏çÂÉÖÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåÊõ¥Ë∂ÖË∂ä‰∫ÜÈ†ÇÂ∞ñÂïÜÂ≠∏Èô¢ÁöÑÁ†îÁ©∂ÁîüÂπ≥ÂùáÂàÜÊï∏„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü GPT-4 Turbo Âú®Ëß£ÈáãÁ≠îÊ°à„ÄÅË©ï‰º∞ÂõûÊáâ„ÄÅËæ®Ë≠òÈåØË™§„ÄÅË™øÊï¥Ë™™ÊòéÂíåÁî¢ÁîüÊõø‰ª£ÊÉÖÂ¢ÉÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËàáÂâç‰∏Ä‰ª£ÁâàÊú¨Áõ∏ÊØîÔºåÊúÄÊñ∞ÁöÑ LLM ÁâàÊú¨ GPT-4 Turbo„ÄÅClaude 2.1 Âíå Gemini 1.0 Pro Âú®Êé®ÁêÜ‰ªªÂãôÊñπÈù¢ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÂá∏È°Ø‰∫ÜÂÖ∂Âú®Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÂÑòÁÆ° AI Âú®ÊïôËÇ≤„ÄÅË©ïÈáèÂíåËºîÂ∞éÊñπÈù¢ÁöÑÊâøË´æÂæàÊòéÁ¢∫Ôºå‰ΩÜ‰ªçÊúâÊåëÊà∞Â≠òÂú®„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ÁöÑÂ≠∏Ë°ìÊΩõÂäõÔºå‰πüÂº∑Ë™ø‰∫ÜÂú®ÊïôËÇ≤‰∏≠ÂØ©ÊÖéÈñãÁôºÂíåÊáâÁî® AI ÁöÑÂøÖË¶ÅÊÄß„ÄÇÈö®Ëëó AI ÊäÄË°ìÁöÑÈÄ≤Ê≠•ÔºåÂª∫Á´ã AI ‰∫íÂãïÁöÑÊû∂ÊßãÂíåÂçîÂÆö„ÄÅÈ©óË≠â AI ÁîüÊàêÁöÑÂÖßÂÆπÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÁ¢∫‰øùÂÖ®ÁêÉÂêÑÂú∞Â§öÂÖÉÂ≠∏ÁøíËÄÖÁöÑÂ≠òÂèñÊ¨äÔºå‰ª•ÂèäÂâµÈÄ†‰∏ÄÂÄã AI ÊîØÊåÅ‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠òÁöÑÊïôËÇ≤Áí∞Â¢ÉËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÁÇ∫ÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢Ë≤†Ë≤¨‰ªªÂú∞‰ΩøÁî® AI ‰æÜË±êÂØåÊïôËÇ≤È´îÈ©ó‰∏¶ÊîπÂñÑËÄÉË©¶Ê∫ñÂÇôÂíåË©ïÈáèÊñπÊ≥ïÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **XAI for In-hospital Mortality Prediction via Multimodal ICU Data**
2312.17624v1 by Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He

Predicting in-hospital mortality for intensive care unit (ICU) patients is
key to final clinical outcomes. AI has shown advantaged accuracy but suffers
from the lack of explainability. To address this issue, this paper proposes an
eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,
explainable AI solution for predicting in-hospital mortality via multimodal ICU
data. We employ multimodal learning in our framework, which can receive
heterogeneous inputs from clinical data and make decisions. Furthermore, we
introduce an explainable method, namely Layer-Wise Propagation to Transformer,
as a proper extension of the LRP method to Transformers, producing explanations
over multimodal inputs and revealing the salient features attributed to
prediction. Moreover, the contribution of each modality to clinical outcomes
can be visualized, assisting clinicians in understanding the reasoning behind
decision-making. We construct a multimodal dataset based on MIMIC-III and
MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on
benchmark datasets demonstrate that our proposed framework can achieve
reasonable interpretation with competitive prediction accuracy. In particular,
our framework can be easily transferred to other clinical tasks, which
facilitates the discovery of crucial factors in healthcare research.

ÊëòË¶ÅÔºöÈ†êÊ∏¨Âä†Ë≠∑ÁóÖÊàø (ICU) ÁóÖÊÇ£ÁöÑÈô¢ÂÖßÊ≠ª‰∫°ÁéáÊòØÊúÄÁµÇËá®Â∫äÁµêÊûúÁöÑÈóúÈçµ„ÄÇAI Â∑≤Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂçªÁº∫‰πèÂèØËß£ÈáãÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÂ§öÊ®°ÂºèÊ≠ª‰∫°ÁéáÈ†êÊ∏¨Âô® (X-MMP)ÔºåÊé°Áî®ÊúâÊïà‰∏îÂèØËß£ÈáãÁöÑ AI ÊñπÂºèÔºåËóâÁî±Â§öÊ®°Âºè ICU Ë≥áÊñô‰æÜÈ†êÊ∏¨Èô¢ÂÖßÊ≠ª‰∫°Áéá„ÄÇÊàëÂÄëÂú®Êû∂Êßã‰∏≠Êé°Áî®Â§öÊ®°ÂºèÂ≠∏ÁøíÔºåÂèØ‰ª•Êé•Êî∂‰æÜËá™Ëá®Â∫äË≥áÊñôÁöÑÁï∞Ë≥™Ëº∏ÂÖ•‰∏¶ÂÅöÂá∫Ê±∫Á≠ñ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂàÜÂ±§ÂÇ≥Êí≠Ëá≥ TransformerÔºå‰ΩúÁÇ∫ LRP ÊñπÊ≥ïÈÅ©Áï∂Âú∞Âª∂‰º∏Ëá≥ TransformerÔºåÂ∞çÂ§öÊ®°ÂºèËº∏ÂÖ•Áî¢ÁîüËß£ÈáãÔºå‰∏¶Êè≠Èú≤Ê≠∏Âõ†ÊñºÈ†êÊ∏¨ÁöÑÈ°ØËëóÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊØèÂÄãÊ®°ÂºèÂ∞çËá®Â∫äÁµêÊûúÁöÑË≤¢ÁçªÂèØ‰ª•Ë¶ñË¶∫ÂåñÔºåÂçîÂä©Ëá®Â∫äÈÜ´Â∏´‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÁêÜÁî±„ÄÇÊàëÂÄëÊ†πÊìö MIMIC-III Âíå MIMIC-III Ê≥¢ÂΩ¢Ë≥áÊñôÂ∫´ÊØîÂ∞çÂ≠êÈõÜÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÂºèË≥áÊñôÈõÜ„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂÖ®Èù¢ÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂèØ‰ª•ÈÅîÊàêÂêàÁêÜÁöÑË©ÆÈáãÔºå‰∏¶ÂÖ∑ÂÇôÁ´∂Áà≠ÂäõÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂèØ‰ª•ËºïÈ¨ÜÂú∞ËΩâÁßªÂà∞ÂÖ∂‰ªñËá®Â∫ä‰ªªÂãôÔºåÈÄôÊúâÂä©ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂‰∏≠ÁôºÁèæÈóúÈçµÂõ†Á¥†„ÄÇ

##### **Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**
2401.09450v2 by Norman Zerbe, Lars Ole Schwen, Christian Gei√üler, Katja Wiesemann, Tom Bisson, Peter Boor, Rita Carvalho, Michael Franz, Christoph Jansen, Tim-Rasmus Kiehl, Bj√∂rn Lindequist, Nora Charlotte Pohlan, Sarah Schmell, Klaus Strohmenger, Falk Zakrzewski, Markus Plass, Michael Takla, Tobias K√ºster, Andr√© Homeyer, Peter Hufnagl

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåÁóÖÁêÜÂ≠∏‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÂ∑≤Â§ßÂπÖÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË®±Â§öÊåëÊà∞ÔºåÂåÖÊã¨Â∞áÁ†îÁ©∂ÁµêÊûúËΩâÂåñÁÇ∫Ëá®Â∫äË®∫Êñ∑Áî¢ÂìÅÂú®ÊäÄË°ìÂíåÊ≥ïË¶èÊñπÈù¢ÁöÑÈöúÁ§ôÔºå‰ª•ÂèäÁº∫‰πèÊ®ôÊ∫ñÂåñ‰ªãÈù¢ÔºåÂ∞éËá¥Êï¥ÂêàÂà∞Â∏∏Ë¶èËá®Â∫äÂØ¶Âãô‰∏≠ÈÄ≤Â±ïÁ∑©ÊÖ¢„ÄÇÈñãÊîæ‰∏îËàá‰æõÊáâÂïÜÁÑ°ÈóúÁöÑ EMPAIA Ë®àÁï´ÊáâÂ∞ç‰∫ÜÈÄô‰∫õÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèê‰æõ EMPAIA ÁöÑÊàêÂ∞±ÂíåÁ∂ìÈ©óÊïôË®ìÁöÑÊ¶ÇËø∞„ÄÇEMPAIA Êï¥Âêà‰∫ÜÁóÖÁêÜÂ≠∏ AI ÁîüÊÖãÁ≥ªÁµ±ÁöÑÂêÑÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÔºåÂç≥ÁóÖÁêÜÂ≠∏ÂÆ∂„ÄÅÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÂíåÁî¢Ê•≠„ÄÇÂú®ÂØÜÂàáÂêà‰Ωú‰∏ãÔºåÊàëÂÄëÂà∂ÂÆö‰∫ÜÊäÄË°ì‰∫íÈÄöÊÄßÊ®ôÊ∫ñ„ÄÅAI Ê∏¨Ë©¶ÂíåÁî¢ÂìÅÈñãÁôºÂª∫Ë≠∞Ôºå‰ª•ÂèäÂèØËß£ÈáãÊÄßÊñπÊ≥ï„ÄÇÊàëÂÄëÂØ¶‰Ωú‰∫ÜÊ®°ÁµÑÂåñ‰∏îÈñãÊîæÂéüÂßãÁ¢ºÁöÑ EMPAIA Âπ≥Ëá∫Ôºå‰∏¶ÊàêÂäüÊï¥Âêà‰∫Ü‰æÜËá™ 8 ÂÄã‰∏çÂêå‰æõÊáâÂïÜÁöÑ 14 ÂÄãÂü∫Êñº AI ÁöÑÂΩ±ÂÉèÂàÜÊûêÊáâÁî®Á®ãÂºèÔºåÂ±ïÁ§∫‰∫Ü‰∏çÂêåÁöÑÊáâÁî®Á®ãÂºèÂ¶Ç‰Ωï‰ΩøÁî®ÂñÆ‰∏ÄÁöÑÊ®ôÊ∫ñÂåñ‰ªãÈù¢„ÄÇÊàëÂÄëÂÑ™ÂÖàËÄÉÊÖÆÈúÄÊ±ÇÔºå‰∏¶Ë©ï‰º∞‰∫Ü AI Âú®Ê≠êÊ¥≤Âíå‰∫ûÊ¥≤ÁöÑ 14 ÂÄã‰∏çÂêåÁóÖÁêÜÂØ¶È©óÂÆ§‰∏≠ÁöÑÂØ¶ÈöõËá®Â∫äÊáâÁî®„ÄÇÈô§‰∫ÜÊäÄË°ìÈñãÁôºÂ§ñÔºåÊàëÂÄëÈÇÑÁÇ∫ÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫Âª∫Á´ã‰∫Ü‰∏ÄÂÄãË´ñÂ£áÔºå‰ª•ÂàÜ‰∫´Êï∏‰ΩçÁóÖÁêÜÂ≠∏Âíå AI ÁöÑË≥áË®äÂíåÁ∂ìÈ©ó„ÄÇÂïÜÊ•≠„ÄÅËá®Â∫äÂíåÂ≠∏Ë°ìÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁèæÂú®ÂèØ‰ª•Êé°Áî® EMPAIA ÁöÑÂ∏∏Ë¶ãÈñãÊîæÂéüÂßãÁ¢º‰ªãÈù¢ÔºåÈÄôÁÇ∫Â§ßË¶èÊ®°Ê®ôÊ∫ñÂåñÂíåÁ∞°ÂåñÊµÅÁ®ãÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑÊ©üÊúÉ„ÄÇÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÂä™ÂäõÊâçËÉΩÊúâÊïà‰∏îÂª£Ê≥õÂú∞Âª∫Á´ã‰æãË°åÂØ¶È©óÂÆ§‰ΩøÁî®‰∏≠ÁöÑ AI ËºîÂä©„ÄÇÁÇ∫Ê≠§ÔºåÂ∑≤ÊàêÁ´ãÈùûÁáüÂà©ÂçîÊúÉ EMPAIA InternationalÔºå‰ª•‰ΩúÁÇ∫Ê∞∏Á∫åÂü∫Á§éÊû∂ÊßãÔºåÁπºÁ∫åÈÄ≤Ë°åÊ®ôÊ∫ñÂåñÔºå‰∏¶ÊîØÊè¥Âª£Ê≥õÂØ¶‰ΩúÂíåÂÄ°Â∞é AI ËºîÂä©Êï∏‰ΩçÁóÖÁêÜÂ≠∏ÁöÑÊú™‰æÜ„ÄÇ

##### **Robust Stochastic Graph Generator for Counterfactual Explanations**
2312.11747v2 by Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo

Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.

ÊëòË¶ÅÔºöÂèç‰∫ãÂØ¶Ëß£Èáã (CE) ÊäÄË°ìÂ∑≤ÂºïËµ∑ÈóúÊ≥®Ôºå‰ΩúÁÇ∫‰∏ÄÁ®ÆÁÇ∫Ëàá AI Á≥ªÁµ±‰∫íÂãïÁöÑ‰ΩøÁî®ËÄÖÊèê‰æõË¶ãËß£ÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåËá™ÂãïÈßïÈßõÊ±ΩËªäÁ≠âÈ†òÂüüÂª£Ê≥õÁ†îÁ©∂ÔºåÂúñÂΩ¢Âèç‰∫ãÂØ¶Ëß£Èáã (GCE) ÊñπÊ≥ïÁõ∏Â∞çËºÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇGCE ÊúÉÁî¢Áîü‰∏ÄÂÄãÈ°û‰ººÊñºÂéüÂßãÂúñÂΩ¢ÁöÑÊñ∞ÂúñÂΩ¢Ôºå‰∏¶Ê†πÊìöÂü∫Á§éÈ†êÊ∏¨Ê®°ÂûãÁî¢Áîü‰∏çÂêåÁöÑÁµêÊûú„ÄÇÂú®ÈÄô‰∫õ GCE ÊäÄË°ì‰∏≠ÔºåÂÑòÁÆ°Âú®ÂÖ∂‰ªñÈ†òÂüüÔºà‰æãÂ¶ÇËóùË°ìÈ¢®Ê†ºÂíåËá™ÁÑ∂Ë™ûË®ÄÂª∫Ê®°Ôºâ‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊàêÂ∞±Ôºå‰ΩÜÊ§çÂü∫ÊñºÁîüÊàêÊ©üÂà∂ÁöÑÊäÄË°ìÁç≤ÂæóÁöÑÈóúÊ≥®Áõ∏Â∞çÊúâÈôê„ÄÇÂ∞çÁîüÊàêÂºèËß£ÈáãÂô®ÁöÑÂÅèÂ•ΩÊ∫êÊñºÂÆÉÂÄëÂú®Êé®ÁêÜÊúüÈñìÁî¢ÁîüÂèç‰∫ãÂØ¶ÂØ¶‰æãÁöÑËÉΩÂäõÔºåÂà©Áî®Ëº∏ÂÖ•ÂúñÂΩ¢ÁöÑËá™‰∏ªÁç≤ÂèñÊìæÂãï„ÄÇÂü∫Êñº‰∏äËø∞ÁêÜÁî±ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü RSGG-CEÔºå‰∏ÄÁ®ÆÁî®ÊñºÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÊñ∞ÂûãÁ©©ÂÅ•Èö®Ê©üÂúñÂΩ¢ÁîüÊàêÂô®ÔºåËÉΩÂ§†ÂæûÂ≠∏ÁøíÂà∞ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Áî¢ÁîüÂèç‰∫ãÂØ¶ÁØÑ‰æãÔºåËÄÉÊÖÆÈÉ®ÂàÜÊúâÂ∫èÁöÑÁîüÊàêÂ∫èÂàó„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÂÆöÈáèÂíåÂÆöÊÄßÂàÜÊûêÔºå‰ª•ÊØîËºÉ RSGG-CE ÁöÑÊïàËÉΩËàá SoA ÁîüÊàêÂºèËß£ÈáãÂô®ÔºåÂº∑Ë™øÂÖ∂Â¢ûÂº∑‰∫ÜÁî¢ÁîüÂêàÁêÜËß£ÈáãÂÄôÈÅ∏ÁöÑËÉΩÂäõ„ÄÇ

##### **Evaluating the Utility of Model Explanations for Model Development**
2312.06032v1 by Shawn Im, Jacob Andreas, Yilun Zhou

One of the motivations for explainable AI is to allow humans to make better
and more informed decisions regarding the use and deployment of AI models. But
careful evaluations are needed to assess whether this expectation has been
fulfilled. Current evaluations mainly focus on algorithmic properties of
explanations, and those that involve human subjects often employ subjective
questions to test human's perception of explanation usefulness, without being
grounded in objective metrics and measurements. In this work, we evaluate
whether explanations can improve human decision-making in practical scenarios
of machine learning model development. We conduct a mixed-methods user study
involving image data to evaluate saliency maps generated by SmoothGrad,
GradCAM, and an oracle explanation on two tasks: model selection and
counterfactual simulation. To our surprise, we did not find evidence of
significant improvement on these tasks when users were provided with any of the
saliency maps, even the synthetic oracle explanation designed to be simple to
understand and highly indicative of the answer. Nonetheless, explanations did
help users more accurately describe the models. These findings suggest caution
regarding the usefulness and potential for misunderstanding in saliency-based
explanations.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI ÁöÑÂãïÊ©ü‰πã‰∏ÄÊòØËÆì‰∫∫ÂÄëÂú®‰ΩøÁî®ÂíåÈÉ®ÁΩ≤ AI Ê®°ÂûãÊôÇÂÅöÂá∫Êõ¥Â•Ω„ÄÅÊõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇ‰ΩÜÈúÄË¶Å‰ªîÁ¥∞Ë©ï‰º∞‰ª•Ë©ï‰º∞ÊòØÂê¶Â∑≤ÈÅîÂà∞Ê≠§È†êÊúü„ÄÇÁõÆÂâçÁöÑË©ï‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëß£ÈáãÁöÑÊºîÁÆóÊ≥ïÁâπÊÄßÔºåËÄåÊ∂âÂèä‰∫∫È°ûÂèóË©¶ËÄÖÁöÑË©ï‰º∞ÈÄöÂ∏∏Êé°Áî®‰∏ªËßÄÂïèÈ°å‰æÜÊ∏¨Ë©¶‰∫∫È°ûÂ∞çËß£ÈáãÊúâÁî®ÊÄßÁöÑÁúãÊ≥ïÔºåËÄåÊ≤íÊúâÂü∫ÊñºÂÆ¢ËßÄÊåáÊ®ôÂíåÊ∏¨Èáè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË©ï‰º∞Ëß£ÈáãÊòØÂê¶ÂèØ‰ª•Âú®Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÈñãÁôºÁöÑÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÊîπÂñÑ‰∫∫È°ûÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÊ∂âÂèäÂΩ±ÂÉèË≥áÊñôÁöÑÊ∑∑ÂêàÊñπÊ≥ï‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ SmoothGrad„ÄÅGradCAM ÂíåÈ†êË®ÄËß£ÈáãÂú®ÂÖ©ÂÄã‰ªªÂãô‰∏≠Áî¢ÁîüÁöÑÈ°ØËëóÊÄßÂúñÔºöÊ®°ÂûãÈÅ∏ÊìáÂíåÂèç‰∫ãÂØ¶Ê®°Êì¨„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÊ≤íÊúâÁôºÁèæ‰ªª‰ΩïÈ°ØËëóÊÄßÂúñÔºàÂç≥‰ΩøÊòØË®≠Ë®àÁÇ∫ÊòìÊñºÁêÜËß£‰∏îÈ´òÂ∫¶ÊåáÁ§∫Á≠îÊ°àÁöÑÂêàÊàêÈ†êË®ÄËß£ÈáãÔºâËÉΩËÆì‰ΩøÁî®ËÄÖÂú®ÈÄô‰∫õ‰ªªÂãô‰∏äÈ°ØËëóÊîπÂñÑÁöÑË≠âÊìö„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåËß£ÈáãÁ¢∫ÂØ¶ÊúâÂä©Êñº‰ΩøÁî®ËÄÖÊõ¥Ê∫ñÁ¢∫Âú∞ÊèèËø∞Ê®°Âûã„ÄÇÈÄô‰∫õÁôºÁèæÊèêÁ§∫ÊàëÂÄëË¶ÅÂ∞çÂü∫ÊñºÈ°ØËëóÊÄßÁöÑËß£Èáã‰∏≠ÂèØËÉΩÂ≠òÂú®Ë™§Ëß£ÁöÑÊúâÁî®ÊÄß‰øùÊåÅË¨πÊÖé„ÄÇ

##### **Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**
2312.06798v1 by Manas Gaur, Amit Sheth

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

ÊëòË¶ÅÔºöÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÂª∫Á´ã‰ø°‰ªª„ÄÇÈÄô‰∫õÈúÄË¶Å‰∏ÄÂÄãÊ®°Âûã‰æÜÂ±ïÁ§∫‰∏ÄËá¥ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∫õÔºåÊúâÂøÖË¶Å‰ΩøÁî®ÂíåÂàÜÊûêÊï∏ÊìöÂíåÁü•Ë≠òÔºå‰∏¶‰ΩøÁî®Ëàá AI ÊáâÁî®Áõ∏ÈóúÁöÑÁµ±Ë®àÂíåÁ¨¶Ëôü AI ÊñπÊ≥ï - ÂñÆÁç®‰ΩøÁî®‰ªª‰Ωï‰∏ÄÁ®ÆÊñπÊ≥ïÈÉΩ‰∏çÊúÉÂ•èÊïà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰∏ªÂºµ‰∏¶Ë©¶ÂúñË≠âÊòé NeuroSymbolic AI ÊñπÊ≥ïÊõ¥ÈÅ©ÂêàÊñº‰Ωø AI ÊàêÁÇ∫Âèó‰ø°‰ªªÁöÑ AI Á≥ªÁµ±„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü CREST Ê°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄËá¥ÊÄß„ÄÅÂèØÈù†ÊÄß„ÄÅ‰ΩøÁî®ËÄÖÂ±§Á¥öÁöÑÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÊòØÂ¶Ç‰ΩïÂª∫Á´ãÂú® NeuroSymbolic ÊñπÊ≥ï‰∏äÁöÑÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Êï∏ÊìöÂíåÁü•Ë≠ò‰æÜÊîØÊåÅÈóúÈçµÊáâÁî®Ôºà‰æãÂ¶ÇÂÅ•Â∫∑ÂíåÁ¶èÁ•âÔºâÁöÑË¶ÅÊ±Ç„ÄÇÊú¨ÊñáÈáçÈªûÈóúÊ≥®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂõ†ÁÇ∫ÂÆÉÊòØ CREST Ê°ÜÊû∂‰∏≠ÈÅ∏ÊìáÁöÑ AI Á≥ªÁµ±„ÄÇLLM Âõ†ÂÖ∂Âú®ËôïÁêÜÂª£Ê≥õÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Â†¥ÊôØÊñπÈù¢ÁöÑÂ§öÂäüËÉΩÊÄßËÄåÂÇôÂèóÁ†îÁ©∂‰∫∫Âì°ÁöÑÈóúÊ≥®„ÄÇ‰æãÂ¶ÇÔºåChatGPT Âíå Google ÁöÑ MedPaLM Â∑≤ÊàêÁÇ∫Êèê‰æõ‰∏ÄËà¨ÂíåÂÅ•Â∫∑Áõ∏ÈóúÊü•Ë©¢‰ø°ÊÅØÁöÑÊ•µÊúâÂ∏åÊúõÁöÑÂπ≥Âè∞„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊ®°Âûã‰ªçÁÑ∂ÊòØÈªëÁõíÂ≠êÔºåÂÑòÁÆ°Á¥çÂÖ•‰∫Ü‰∫∫È°ûÂèçÈ•ãÂíåÊåá‰ª§ÂºïÂ∞éÁöÑË™øÊï¥„ÄÇ‰æãÂ¶ÇÔºåÂÑòÁÆ°Âà∂ÂÆö‰∫ÜÂÆâÂÖ®Èò≤Ë≠∑Êé™ÊñΩÔºåChatGPT ‰ªçÂèØËÉΩÁî¢Áîü‰∏çÂÆâÂÖ®ÁöÑÂõûÊáâ„ÄÇCREST ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêàÁêÜÁöÑÊñπÊ≥ïÔºåÂú® NeuroSymbolic Ê°ÜÊû∂‰∏≠Âà©Áî®Á®ãÂ∫èÂíåÂü∫ÊñºÂúñË°®ÁöÑÁü•Ë≠òÔºå‰ª•Èó°ÊòéËàá LLM Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇ

##### **Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**
2311.17133v1 by Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë∞ÉÊü•‰∫ÜÂú® COVID-19 Áñ´ÊÉÖÊúüÈó¥Âèä‰ª•ÂêéÈ¢ÑÊµãÊ≠ª‰∫°ÁéáÊó∂ÔºåÂ∑≤ÈÉ®ÁΩ≤‰∫∫Â∑•Êô∫ËÉΩ (AI) Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÅÂèØËß£ÈáäÊÄßÂíåÁ®≥ÂÅ•ÊÄß„ÄÇ‰Ωú‰∏∫ÂêåÁ±ªÁ†îÁ©∂‰∏≠ÁöÑÈ¶ñ‰æãÔºåÊàë‰ª¨ÂèëÁé∞Ë¥ùÂè∂ÊñØÁ•ûÁªèÁΩëÁªú (BNN) ÂíåÊô∫ËÉΩËÆ≠ÁªÉÊäÄÊúØËÆ©Êàë‰ª¨ÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÂèëÁîüÈáçÂ§ßÂèòÂåñÊó∂‰ªçËÉΩ‰øùÊåÅÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÂºÄÂèëÁ®≥ÂÅ•ÁöÑ AI Ê®°ÂûãÁöÑÈáçË¶ÅÊÄßÔºåÂç≥‰ΩøÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊù°‰ª∂‰∏ãÔºåËøô‰∫õÊ®°Âûã‰πüËÉΩÂåπÈÖçÊàñË∂ÖË∂ä‰∏¥Â∫äÂåªÁîüÁöÑÈ¢ÑÊµã„ÄÇÊàë‰ª¨ÂØπÊ®°ÂûãÂèØËß£ÈáäÊÄßÁöÑÊé¢Á¥¢Ë°®ÊòéÔºåÈöèÊú∫Ê®°Âûã‰ºö‰∫ßÁîüÊõ¥Â§öÊ†∑Âåñ‰∏î‰∏™ÊÄßÂåñÁöÑËß£ÈáäÔºå‰ªéËÄåÁ™ÅÂá∫‰∫ÜÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠Êèê‰æõËØ¶ÁªÜ‰∏î‰∏™ÊÄßÂåñËßÅËß£ÁöÑ AI Ê®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Âº∫Ë∞É‰∫ÜÈáèÂåñ AI Ê®°Âûã‰∏≠‰∏çÁ°ÆÂÆöÊÄßÁöÑÈáçË¶ÅÊÄßÔºåËøô‰Ωø‰∏¥Â∫äÂåªÁîüËÉΩÂ§üÊ†πÊçÆÂèØÈù†ÁöÑÈ¢ÑÊµãÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂÜ≥Á≠ñ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊèêÂÄ°Âú®ÂåªÁñó‰øùÂÅ•ÁöÑ AI Á†îÁ©∂‰∏≠‰ºòÂÖàËÄÉËôëÂÆûÊñΩÁßëÂ≠¶ÔºåÂπ∂Á°Æ‰øù AI Ëß£ÂÜ≥ÊñπÊ°àÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠ÂÆûÁî®„ÄÅÊúâÁõä‰∏îÂèØÊåÅÁª≠„ÄÇÈÄöËøáËß£ÂÜ≥ÂåªÁñó‰øùÂÅ•ÁéØÂ¢É‰∏≠ÁöÑÁã¨ÁâπÊåëÊàòÂíåÂ§çÊùÇÊÄßÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•ÂºÄÂèëÂá∫ÊúâÊïàÊîπÂñÑ‰∏¥Â∫äÂÆûË∑µÂíåÊÇ£ËÄÖÈ¢ÑÂêéÁöÑ AI Ê®°Âûã„ÄÇ

##### **Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**
2311.15719v1 by Benjamin Keel, Aaron Quyn, David Jayne, Samuel D. Relton

Lung cancer is responsible for 21% of cancer deaths in the UK and five-year
survival rates are heavily influenced by the stage the cancer was identified
at. Recent studies have demonstrated the capability of AI methods for accurate
and early diagnosis of lung cancer from routine scans. However, this evidence
has not translated into clinical practice with one barrier being a lack of
interpretable models. This study investigates the application Variational
Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions.
Proposed models were trained on lesions extracted from 3D CT scans in the
LIDC-IDRI public dataset. Latent vector representations of 2D slices produced
by the VAEs were explored through clustering to justify their quality and used
in an MLP classifier model for lung cancer diagnosis, the best model achieved
state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows
the VAE latent space separates the dataset of malignant and benign lesions
based on meaningful feature components including tumour size, shape, patient
and malignancy class. We also include a comparative analysis of the standard
Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces
the prior with a Dirichlet distribution to encourage a more explainable latent
space with disentangled feature representation. Finally, we demonstrate the
potential for latent space traversals corresponding to clinically meaningful
feature changes.

ÊëòË¶ÅÔºöËÇ∫ÁôåÂç†Ëã±ÂúãÁôåÁóáÊ≠ª‰∫°‰∫∫Êï∏ÁöÑ 21%Ôºå‰∫îÂπ¥Â≠òÊ¥ªÁéáÂæàÂ§ßÁ®ãÂ∫¶ÂèñÊ±∫ÊñºÁôåÁóáË¢´ÁôºÁèæÁöÑÈöéÊÆµ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòé‰∫∫Â∑•Êô∫ËÉΩÊñπÊ≥ïÂÖ∑ÊúâÂæû‰æãË°åÊéÉÊèè‰∏≠Ê∫ñÁ¢∫ÂèäÊó©Ë®∫Êñ∑ËÇ∫ÁôåÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊ≠§Ë≠âÊìöÂ∞öÊú™ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶ÂãôÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈöúÁ§ôÊòØÁº∫‰πèÂèØËß£ÈáãÁöÑÊ®°Âûã„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊáâÁî®ËÆäÂàÜËá™ÂãïÁ∑®Á¢ºÂô® (VAE)Ôºå‰∏ÄÁ®ÆÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÔºåÊñºËÇ∫ÁôåÁóÖÁÅ∂„ÄÇÂ∞áÊèêÂá∫ÁöÑÊ®°ÂûãË®ìÁ∑¥ÊñºÂæû LIDC-IDRI ÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏≠ÊèêÂèñÁöÑ 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁóÖÁÅ∂„ÄÇÈÄöÈÅéËÅöÈ°ûÊé¢Á¥¢‰∫Ü VAE ÁîüÊàêÁöÑ 2D ÂàáÁâáÁöÑÊΩõÂú®ÂêëÈáèË°®Á§∫Ôºå‰ª•Ë≠âÊòéÂÖ∂ÂìÅË≥™Ôºå‰∏¶Áî®ÊñºËÇ∫ÁôåË®∫Êñ∑ÁöÑ MLP ÂàÜÈ°ûÂô®Ê®°ÂûãÔºåÊúÄ‰Ω≥Ê®°ÂûãÈÅîÂà∞‰∫Ü AUC 0.98 Âíå 93.1% Ê∫ñÁ¢∫Â∫¶ÁöÑÊúÄÂÖàÈÄ≤ÊåáÊ®ô„ÄÇËÅöÈ°ûÂàÜÊûêÈ°ØÁ§∫ÔºåVAE ÊΩõÂú®Á©∫ÈñìÊ†πÊìöÊúâÊÑèÁæ©ÁöÑÁâπÂæµÁµÑÊàêÔºàÂåÖÊã¨ËÖ´Áò§Â§ßÂ∞è„ÄÅÂΩ¢ÁãÄ„ÄÅÊÇ£ËÄÖÂíåÊÉ°ÊÄßÈ°ûÂà•ÔºâÂ∞áÊÉ°ÊÄßÂíåËâØÊÄßÁóÖÁÅ∂ÁöÑÊï∏ÊìöÈõÜÂàÜÈñã„ÄÇÊàëÂÄëÈÇÑÂåÖÊã¨Ê®ôÊ∫ñÈ´òÊñØ VAE (GVAE) ÂíåÊõ¥Êñ∞ÁöÑÁãÑÂà©ÂÖãÈõ∑ VAE (DirVAE) ÁöÑÊØîËºÉÂàÜÊûêÔºåÂæåËÄÖÁî®ÁãÑÂà©ÂÖãÈõ∑ÂàÜ‰ΩàÂèñ‰ª£ÂÖàÈ©óÔºå‰ª•‰øÉÈÄ≤ÂÖ∑ÊúâËß£ÈñãÁâπÂæµË°®Á§∫ÁöÑÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÁöÑÊΩõÂú®Á©∫Èñì„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáËá®Â∫äÊúâÊÑèÁæ©ÁöÑÁâπÂæµËÆäÂåñÁõ∏ÊáâÁöÑÊΩõÂú®Á©∫ÈñìÊ©´Ë∂äÁöÑÊΩõÂäõ„ÄÇ

##### **MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**
2311.14471v1 by Nathan Blake, Hana Chockler, David A. Kelly, Santiago Calderon Pena, Akchunya Chanchal

Existing tools for explaining the output of image classifiers can be divided
into white-box, which rely on access to the model internals, and black-box,
agnostic to the model. As the usage of AI in the medical domain grows, so too
does the usage of explainability tools. Existing work on medical image
explanations focuses on white-box tools, such as gradcam. However, there are
clear advantages to switching to a black-box tool, including the ability to use
it with any classifier and the wide selection of black-box tools available. On
standard images, black-box tools are as precise as white-box. In this paper we
compare the performance of several black-box methods against gradcam on a brain
cancer MRI dataset. We demonstrate that most black-box tools are not suitable
for explaining medical image classifications and present a detailed analysis of
the reasons for their shortcomings. We also show that one black-box tool, a
causal explainability-based rex, performs as well as \gradcam.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÂúñÂÉèÂàÜÈ°ûÂô®Ëº∏Âá∫Ëß£ÈáãÂ∑•ÂÖ∑ÂèØÂàÜÁÇ∫‰æùË≥¥ÊñºÊ®°ÂûãÂÖßÈÉ®Â≠òÂèñÊ¨äÈôêÁöÑÁôΩÁõíÔºå‰ª•ÂèäËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÈªëÁõí„ÄÇÈö®Ëëó AI Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑ‰ΩøÁî®Â¢ûÂä†ÔºåÂèØËß£ÈáãÊÄßÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®‰πüÈö®‰πãÂ¢ûÂä†„ÄÇÁèæÊúâÈÜ´Â≠∏ÂΩ±ÂÉèËß£ÈáãÁöÑÂ∑•‰ΩúÈáçÈªûÂú®ÊñºÁôΩÁõíÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç gradcam„ÄÇÁÑ∂ËÄåÔºåÂàáÊèõÂà∞ÈªëÁõíÂ∑•ÂÖ∑ÊúâÊòéÈ°ØÁöÑÂÑ™ÈªûÔºåÂåÖÊã¨ËÉΩÂ§†Ëàá‰ªª‰ΩïÂàÜÈ°ûÂô®‰∏ÄËµ∑‰ΩøÁî®Ôºå‰ª•ÂèäÂª£Ê≥õÁöÑÈªëÁõíÂ∑•ÂÖ∑ÂèØ‰æõÈÅ∏Êìá„ÄÇÂú®Ê®ôÊ∫ñÂΩ±ÂÉè‰∏äÔºåÈªëÁõíÂ∑•ÂÖ∑ËàáÁôΩÁõí‰∏ÄÊ®£Á≤æÁ¢∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂ§öÁ®ÆÈªëÁõíÊñπÊ≥ïÂú®ËÖ¶Áôå MRI Ë≥áÊñôÈõÜ‰∏äËàá gradcam ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëË≠âÊòéÂ§ßÂ§öÊï∏ÈªëÁõíÂ∑•ÂÖ∑‰∏çÈÅ©ÂêàËß£ÈáãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÔºå‰∏¶Ë©≥Á¥∞ÂàÜÊûêÂÖ∂Áº∫ÈªûÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÇÑË°®Êòé‰∏ÄÁ®ÆÈªëÁõíÂ∑•ÂÖ∑ÔºåÂü∫ÊñºÂõ†ÊûúÂèØËß£ÈáãÊÄßÁöÑ rexÔºåË°®ÁèæËàá \gradcam ‰∏ÄÊ®£Â•Ω„ÄÇ

##### **Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**
2311.12573v3 by Robert Gorwa, Michael Veale

The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.

ÊëòË¶ÅÔºöAI ÈñãÁôºÁ§æÁæ§Êó•ÁõäÂà©Áî® Hugging Face Á≠âË®óÁÆ°‰∏≠‰ªãÊ©üÊßãÊèê‰æõÁî®Êà∂‰∏äÂÇ≥ÁöÑÊ®°ÂûãÂíåË®ìÁ∑¥Ë≥áÊñôÁöÑÁ∞°ÊòìÂ≠òÂèñÊ¨äÈôê„ÄÇÈÄô‰∫õÊ®°ÂûãÂ∏ÇÈõÜÈôç‰Ωé‰∫ÜÊï∏ÂçÅËê¨ÂêçÁî®Êà∂ÁöÑÊäÄË°ìÈÉ®ÁΩ≤ÈöúÁ§ôÔºå‰ΩÜÂèØËÉΩÊúÉË¢´Áî®ÊñºË®±Â§öÊΩõÂú®ÊúâÂÆ≥ÂíåÈùûÊ≥ïÁöÑÊñπÂºè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™™Êòé AI Á≥ªÁµ±Êó¢ÂèØ‰ª•„ÄåÂåÖÂê´„ÄçÂÖßÂÆπÔºåÂèàÂèØ‰ª•‰ΩúÁÇ∫ÈñãÊîæÂºèÂ∑•ÂÖ∑ÔºåÈÄôÊèêÂá∫‰∫ÜËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÊ£òÊâãÁöÑÂπ≥Âè∞Ê≤ªÁêÜÊåëÊà∞‰πã‰∏Ä„ÄÇÊàëÂÄëÊèê‰æõ Hugging Face„ÄÅGitHub Âíå Civitai Á≠â‰∏âÂÄãË™™ÊòéÊÄßÂπ≥Âè∞‰∏äÊï∏Ëµ∑‰∫ã‰ª∂ÁöÑÊ°à‰æãÁ†îÁ©∂Ôºå‰ª•Ê™¢Ë¶ñÊ®°ÂûãÂ∏ÇÈõÜÂ¶Ç‰ΩïÂØ©Ê†∏Ê®°Âûã„ÄÇÊ†πÊìöÊ≠§ÂàÜÊûêÔºåÊàëÂÄëÊ¶ÇËø∞Áî¢Ê•≠ÁÇ∫ÂõûÊáâÂØ©Ê†∏ÈúÄÊ±ÇËÄåÈñãÁôºÁöÑÈáçË¶ÅÔºà‰ΩÜ‰ªçÊúâÈôêÔºâÂØ¶ÂãôÔºöÊéàÊ¨ä„ÄÅÂ≠òÂèñÂíå‰ΩøÁî®ÈôêÂà∂„ÄÅËá™ÂãïÂåñÂÖßÂÆπÂØ©Ê†∏ÂíåÈñãÊîæÊîøÁ≠ñÂà∂ÂÆö„ÄÇÈõñÁÑ∂Áï∂ÂâçÊîøÁ≠ñÊåëÊà∞Áõ∏Áï∂ÂèØËßÄÔºåÊàëÂÄëÊúÄÂæåÊèêÂá∫‰∏Ä‰∫õÊßãÊÉ≥ÔºåË™™ÊòéÂπ≥Âè∞Â¶Ç‰ΩïËÉΩÊõ¥Â•ΩÂú∞ÂãïÂì°Ë≥áÊ∫êÔºå‰ΩúÁÇ∫Ë¨πÊÖé„ÄÅÂÖ¨Âπ≥‰∏îÈÅ©Â∫¶ÁöÑÊ≥ïË¶èÂ≠òÂèñÈªû„ÄÇ

##### **Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**
2311.11932v1 by Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker

Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÂíåÁõÆÊ®ôÔºöÈÄöÈÅéÊèêÂèñÈÄô‰∫õË≥áË®äÔºåÊ©üÂô®ÊàñÊ∑±Â∫¶Â≠∏Áøí (ML/DL) Âü∫ÊñºËá™‰∏ªÊï∏ÊìöÂàÜÊûêÂ∑•ÂÖ∑ÂèØ‰ª•ÂçîÂä©Ëá®Â∫äÈÜ´ÁîüÂíåÁôåÁóáÁ†îÁ©∂‰∫∫Âì°ÂæûË§áÈõúÁöÑÊï∏ÊìöÈõÜ‰∏≠ÁôºÁèæÊ®°ÂºèÂíåÈóú‰øÇ„ÄÇÊúÄËøëÂ∑≤ÁôºË°®Ë®±Â§öÂü∫Êñº DL ÁöÑÂçµÂ∑¢Áôå (OC) Êï∏ÊìöÂàÜÊûê„ÄÇÈÄô‰∫õÂàÜÊûêÂú®ÁôåÁóáÁöÑÂêÑÂÄãÊñπÈù¢Ôºà‰æãÂ¶ÇÔºåÂÆÉÂÄëÊ∂âÂèäÁöÑÂ≠êÈ†òÂüüÂíåÁôåÁóáÈ°ûÂûãÔºâÂíåÊï∏ÊìöÂàÜÊûêÂäüËÉΩÊñπÈù¢È´òÂ∫¶Â§öÊ®£Âåñ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁº∫‰πèÂ∞çÈÄô‰∫õÂàÜÊûêÂú®ÈÄô‰∫õÁâπÂæµÂíå AI ‰øùË≠â (AIA) ÊñπÈù¢ÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÈÄôÁØáÁ≥ªÁµ±ÊÄßÂõûÈ°ßÊó®Âú®ÈÄöÈÅéÊ™¢Ë¶ñÁèæÊúâÊñáÁçª‰∏¶ÊòéÁ¢∫ÈóúÊ≥®ÈóúÈçµÁâπÂæµÂíå AI ‰øùË≠âËßÄÈªûÔºå‰æÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩ„ÄÇÊñπÊ≥ïÔºö‰ΩøÁî® PRISMA Êû∂ÊßãÂú®‰∏âÂÄãÊúüÂàäË≥áÊñôÂ∫´‰∏≠ÈÄ≤Ë°åÂÖ®Èù¢ÊêúÂ∞ã„ÄÇÂàÜÊûêÂÉÖÂåÖÊã¨ 2015 Âπ¥Ëá≥ 2023 Âπ¥ÈñìÁôºË°®ÊñºÂêåË°åË©ïÂØ©ÊúüÂàäÁöÑÁ†îÁ©∂„ÄÇÁµêÊûúÔºöÂú®ÂõûÈ°ß‰∏≠ÔºåÁ∏ΩÂÖ±Ê™¢Ë¶ñ‰∫Ü 96 È†ÖÁî± DL È©ÖÂãïÁöÑÂàÜÊûê„ÄÇÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÂπæÂÄãÈóúÊñºÁî± DL È©ÖÂãïÁöÑÂçµÂ∑¢ÁôåÊï∏ÊìöÂàÜÊûêÁöÑÈáçË¶ÅË¶ãËß£Ôºö- Â§ßÂ§öÊï∏Á†îÁ©∂ 71%Ôºà96 È†Ö‰∏≠Êúâ 68 È†ÖÔºâÂ∞àÊ≥®ÊñºÊ™¢Ê∏¨ÂíåË®∫Êñ∑ÔºåËÄåÊ≤íÊúâÁ†îÁ©∂Êé¢Ë®é OC ÁöÑÈ†êÊ∏¨ÂíåÈ†êÈò≤„ÄÇ- ÈÄô‰∫õÂàÜÊûê‰∏ªË¶ÅÂü∫Êñº‰æÜËá™ÈùûÂ§öÂÖÉÊóèÁæ§ÁöÑÊ®£Êú¨Ôºà75%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 72 È†ÖÔºâÔºâÔºåÂÉÖÈôêÊñºÊüêÂÄãÂú∞ÁêÜ‰ΩçÁΩÆÊàñÂúãÂÆ∂„ÄÇ- Âè™ÊúâÂ∞ëÈÉ®ÂàÜÁ†îÁ©∂ÔºàÂÉÖ 33%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 32 È†ÖÔºâÂü∑Ë°åÊï¥ÂêàÂàÜÊûêÔºåÂÖ∂‰∏≠Â§ßÂ§öÊï∏‰ΩøÁî®ÂêåË≥™Êï∏ÊìöÔºàËá®Â∫äÊàñÁµÑÂ≠∏Ôºâ„ÄÇ- ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂè™Êúâ 8.3%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 8 È†ÖÔºâ‰ΩøÁî®Â§ñÈÉ®ÂíåÂ§öÂÖÉÊï∏ÊìöÈõÜÈ©óË≠â‰∫ÜÂÖ∂Ê®°ÂûãÔºåÂº∑Ë™ø‰∫ÜÂä†Âº∑Ê®°ÂûãÈ©óË≠âÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Âèä- Â∞á AIA Á¥çÂÖ•ÁôåÁóáÊï∏ÊìöÂàÜÊûê‰ªçËôïÊñºÈùûÂ∏∏Êó©ÊúüÁöÑÈöéÊÆµÔºõÂè™Êúâ 2.1%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 2 È†ÖÔºâÈÄèÈÅéÂèØËß£ÈáãÊÄßÊòéÁ¢∫Êé¢Ë®é‰∫Ü AIA„ÄÇ</paragraph>

##### **Representing visual classification as a linear combination of words**
2311.10933v1 by Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter

Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.

ÊëòË¶ÅÔºö<paragraph>Ëß£ÈáãÊÄßÊòØÊ∑±Â∫¶Â≠∏Áøí‰∏≠Èï∑ÊúüÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á≠âÈ´òÈ¢®Èö™È†òÂüü„ÄÇÂ∏∏Ë¶ãÁöÑËß£ÈáãÊÄßÊñπÊ≥ïÊúÉÂº∑Ë™øÈ©ÖÂãï AI Ê®°ÂûãÊ±∫Á≠ñÁöÑÂΩ±ÂÉèÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÂæàÂ§ßÁ®ãÂ∫¶‰æùË≥¥Ë™ûË®Ä‰æÜÂÇ≥ÈÅî‰∏çÂÉÖÊòØ„ÄåÂú®Âì™Ë£°„ÄçÔºåÈÇÑÊúâ„ÄåÊòØ‰ªÄÈ∫º„ÄçÁöÑËß£Èáã„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏Ëß£ÈáãÊÄßÊñπÊ≥ïÈÉΩÂ∞àÊ≥®ÊñºËß£ÈáãÂÄãÂà• AI È†êÊ∏¨ÔºåËÄå‰∏çÊòØÊèèËø∞ AI Ê®°Âûã‰∏ÄËà¨‰ΩøÁî®ÁöÑÁâπÂæµ„ÄÇÂæåËÄÖÂ∞çÊñºÊ®°ÂûãÂíåË≥áÊñôÈõÜÁ®ΩÊ†∏ÁâπÂà•ÊúâÁî®ÔºåÁîöËá≥ÂèØËÉΩÂú® AI ÊÑà‰æÜÊÑàÁî®ÊñºÊñ∞Á©é‰ªªÂãôÊôÇÁî¢ÁîüÁü•Ë≠ò„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã‰æÜËæ®Ë≠òË¶ñË¶∫ÂàÜÈ°û‰ªªÂãôÁöÑË™ûË®ÄÊèèËø∞Á¨¶ÁöÑËß£ÈáãÊÄßÁ≠ñÁï•„ÄÇÈÄèÈÅéÂà©Áî®ÂΩ±ÂÉèÂíåÊñáÂ≠ó‰πãÈñìÈ†êÂÖàË®ìÁ∑¥ÁöÑËÅØÂêàÂµåÂÖ•Á©∫ÈñìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÊñ∞ÁöÑÂàÜÈ°û‰ªªÂãô‰º∞Ë®àÁÇ∫‰∏ÄÂÄãÁ∑öÊÄßÊñáÂ≠óÁµÑÂêàÔºåÂ∞éËá¥ÊØèÂÄãÊñáÂ≠óÈÉΩÊúâÊ¨äÈáçÔºåË°®Á§∫ÂÆÉËàáÂü∫ÊñºË¶ñË¶∫ÁöÑÂàÜÈ°ûÂô®Â∞çÈΩä„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÁôºÁèæÁî¢ÁîüÁöÑÊèèËø∞Á¨¶Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äËàáËá®Â∫äÁü•Ë≠ò‰∏ÄËá¥ÔºåÂÑòÁÆ°Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑË™ûË®ÄË®ìÁ∑¥„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÁöÑÂÅöÊ≥ï‰πüÁôºÁèæ‰∫ÜÊâÄÁî®ÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏≠ÁöÑ„ÄåÊç∑ÂæëÈÄ£Á∑ö„ÄçÁöÑÂèØËÉΩÊÄß„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞Ëß£ÈáãÊÄßÁöÑÂäüËÉΩÊÄßË°°ÈáèÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖË©¶È©óËÆÄËÄÖÁ†îÁ©∂ÔºåÁôºÁèæ AI Ë≠òÂà•ÁöÑÊñáÂ≠óËÉΩËÆìÈùûÂ∞àÂÆ∂‰∫∫È°ûÂú®ÈùûÂπ≥Âá°ÁöÑÂ±§Á¥öÂü∑Ë°åÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇ‰ªªÂãô„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÁöÑÁµêÊûúÂº∑Ë™ø‰∫Ü‰ΩøÁî®Â§öÊ®°ÂºèÂü∫Á§éÊ®°Âûã‰æÜÊèê‰æõÁõ¥ËßÄÁöÑ„ÄÅÂü∫ÊñºË™ûË®ÄÁöÑË¶ñË¶∫‰ªªÂãôËß£ÈáãÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**
2311.02115v2 by Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert

Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.

ÊëòË¶ÅÔºö<paragraph>‰ΩøÁî®ÈÜ´ÁôÇÂΩ±ÂÉèË®ìÁ∑¥ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÔºåÁî®ÊñºËá®Â∫ä‰ªªÂãôÊôÇÔºåÂ∏∏ÊúÉÂú®ÊïàËÉΩ‰∏äÂ±ïÁèæÂá∫Ê¨°Áæ§È´î‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºåÂΩ¢ÊàêÂÅèË¶ã„ÄÇÁî±Êñº‰∏¶ÈùûÊâÄÊúâÁúüÂØ¶‰∏ñÁïåÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñô‰∏≠ÁöÑÂÅèË¶ã‰æÜÊ∫êÈÉΩÂÆπÊòìËæ®Ë≠òÔºåÂõ†Ê≠§ÂÖ®Èù¢Ë©ï‰º∞ÈÄô‰∫õÂÅèË¶ãÊòØÂ¶Ç‰ΩïÁ∑®Á¢ºÂà∞Ê®°Âûã‰∏≠Ôºå‰ª•ÂèäÂÅèË¶ãÁ∑©Ëß£ÊñπÊ≥ïÂú®ÊîπÂñÑÊïàËÉΩÂ∑ÆÁï∞ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÊûêÊû∂ÊßãÔºåÁî®ÊñºÁ≥ªÁµ±Âåñ‰∏îÂÆ¢ËßÄÂú∞Ë™øÊü•ÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÁöÑÂÅèË¶ãÂ∞ç AI Ê®°ÂûãÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÈñãÁôº‰∏¶Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊû∂ÊßãÔºå‰ª•ÈÄ≤Ë°åÂèóÊéßÁöÑÈõªËÖ¶Ê®°Êì¨Ë©¶È©óÔºå‰ΩøÁî®‰∏ÄÂÄãÂ∑•ÂÖ∑‰æÜË©ï‰º∞ÈÜ´ÁôÇÂΩ±ÂÉè AI ‰∏≠ÁöÑÂÅèË¶ãÔºåË©≤Â∑•ÂÖ∑Áî®ÊñºÁî¢ÁîüÂÖ∑ÊúâÂ∑≤Áü•ÁñæÁóÖÂΩ±ÈüøÂíåÂÅèË¶ã‰æÜÊ∫êÁöÑÂêàÊàêÁ£ÅÂÖ±ÊåØÂΩ±ÂÉè„ÄÇÂèØË°åÊÄßÈÄèÈÅé‰ΩøÁî®‰∏âÂÄãÂèç‰∫ãÂØ¶ÂÅèË¶ãÊÉÖÂ¢É‰æÜË°°ÈáèÊ®°Êì¨ÂÅèË¶ãÊïàÊáâÂ∞çÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂàÜÈ°ûÂô®Âíå‰∏âÂÄãÂÅèË¶ãÁ∑©Ëß£Á≠ñÁï•ÁöÑÂΩ±ÈüøÔºå‰∏¶Â±ïÁ§∫Âá∫‰æÜ„ÄÇÂàÜÊûêÈ°ØÁ§∫ÔºåÁï∂ CNN Âú®ÂêàÊàêË≥áÊñôÈõÜ‰∏äÂèóË®ìÊôÇÔºåÊ®°Êì¨ÂÅèË¶ãÊúÉÂ∞éËá¥È†êÊúüÁöÑÊ¨°Áæ§È´îÊïàËÉΩÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÈáçÊñ∞Âä†Ê¨äË¢´Ë™çÁÇ∫ÊòØÊ≠§Ë®≠ÂÆö‰∏≠ÊúÄÊàêÂäüÁöÑÂÅèË¶ãÁ∑©Ëß£Á≠ñÁï•ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËß£ÈáãÊÄß AI ÊñπÊ≥ïÂ¶Ç‰ΩïÂçîÂä©‰ΩøÁî®ÈÄôÂÄãÊû∂ÊßãË™øÊü•Ê®°Âûã‰∏≠ÂÅèË¶ãÁöÑË°®Áèæ„ÄÇÈñãÁôºÂÖ¨Âπ≥ÁöÑ AI Ê®°ÂûãÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫ÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏≠ÂèØËÉΩÂ≠òÂú®Ë®±Â§ö‰∏îÁ∂ìÂ∏∏Êú™Áü•ÁöÑÂÅèË¶ã‰æÜÊ∫ê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂÆ¢ËßÄÂú∞Á†îÁ©∂ÂÅèË¶ãÂíåÁ∑©Ëß£Á≠ñÁï•Â∞çÊ∑±Â∫¶Â≠∏ÁøíÁÆ°Á∑öÁöÑÂΩ±ÈüøÔºåÈÄôÂèØ‰ª•ÊîØÊè¥ÂÅ•ÂÖ®‰∏îË≤†Ë≤¨‰ªªÁöÑËá®Â∫ä AI ÁöÑÈñãÁôº„ÄÇ</paragraph>

##### **Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**
2310.19174v1 by Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman

Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÁÇ∫Ëá™ÂãïÈ†êÊ∏¨‰∏≠È¢®ÂæåÁóáÁãÄÂèäÂÖ∂Â∞çÂæ©ÂÅ•ÁöÑÂèçÊáâÊèê‰æõ‰∫ÜÊ•µÂ§ßÁöÑÊΩõÂäõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÈáçÂ§ßÊåëÊà∞ÂåÖÊã¨Á•ûÁ∂ìÂΩ±ÂÉèË≥áÊñôÁöÑÁ∂≠Â∫¶ÈùûÂ∏∏È´ò„ÄÅÂèØÁî®ÊñºÂ≠∏ÁøíÁöÑË≥áÊñôÈõÜË¶èÊ®°Áõ∏Â∞çËºÉÂ∞èÔºå‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÁµêÂêàÁ•ûÁ∂ìÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÔºà‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®àË≥áË®äÂíåËá®Â∫äÁâπÂæµÔºâ„ÄÇÊú¨ÊñáÊ†πÊìöÂÖ©Á®ÆÁ≠ñÁï•Ë©ï‰º∞‰∫ÜÂ§öÁ®ÆËß£Ê±∫ÊñπÊ°à„ÄÇÁ¨¨‰∏ÄÁ®ÆÊòØ‰ΩøÁî®Á∏ΩÁµê MRI ÊéÉÊèèÁöÑ 2D ÂΩ±ÂÉè„ÄÇÁ¨¨‰∫åÁ®ÆÊòØÈÅ∏ÊìáÊúâÂä©ÊñºÊèêÈ´òÂàÜÈ°ûÁ≤æÁ¢∫Â∫¶ÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂú®ÁµêÂêàÂæû MRI ‰∏≠ÊèêÂèñÁöÑÊÑüËààË∂£ÂçÄÂüüËàáË°®Ê†ºË≥áÊñôÁöÑÁ¨¶ËôüË°®Á§∫ÁöÑÂΩ±ÂÉè‰∏äË®ìÁ∑¥Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÊñ∞Á©éÊñπÊ≥ï„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÁ≥ªÂàó CNN Êû∂ÊßãÔºà2D Âíå 3DÔºâÔºåÈÄô‰∫õÊû∂ÊßãÂú® MRI ÂíåË°®Ê†ºË≥áÊñôÁöÑ‰∏çÂêåË°®Á§∫‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•È†êÊ∏¨‰∏≠È¢®ÂæåÂè£Ëø∞ÂúñÁâáÊèèËø∞ËÉΩÂäõÁöÑÁ∂úÂêàÊ∏¨ÈáèÊòØÂê¶Âú®Â§±Ë™ûÁóáÊàñÈùûÂ§±Ë™ûÁóáÁØÑÂúçÂÖß„ÄÇMRI ÂíåË°®Ê†ºË≥áÊñô‰æÜËá™ 758 ÂêçÂèÉËàá PLORAS Á†îÁ©∂ÁöÑËã±Ë™û‰∏≠È¢®ÂÄñÂ≠òËÄÖ„ÄÇÂÉÖÈáùÂ∞çÁóÖÁÅ∂Â§ßÂ∞èÁöÑÂü∫Á∑öÈÇèËºØËø¥Ê≠∏ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 0.678ÔºåÁï∂‰æùÂ∫èÂä†ÂÖ•ÂàùÂßãÁóáÁãÄÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÊôÇÔºå‰∏äÂçáËá≥ 0.757 Âíå 0.813„ÄÇÂú®ÂæûÊØèÂÄã MRI ÊéÉÊèè‰∏≠ÊèêÂèñ 8 ÂÄãÊÑüËààË∂£ÂçÄÂüü‰∏¶Âú® 2D ÊÆòÂ∑ÆÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ËàáÁóÖÁÅ∂Â§ßÂ∞è„ÄÅÂàùÂßãÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÁµêÂêàÊôÇÔºåËßÄÂØüÂà∞ÊúÄÈ´òÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ 0.854„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞áÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÁµêÂêàËµ∑‰æÜ‰ª•Áç≤ÂæóÈ´òÊñº‰∏≠È¢®ÂæåÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÂç≥‰ΩøÂú®Ê©üÂô®Â≠∏ÁøíË°ìË™û‰∏≠Ë≥áÊñôÈõÜÂæàÂ∞èÁöÑÊÉÖÊ≥Å‰∏ã‰πüÊòØÂ¶ÇÊ≠§„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫Â¶Ç‰ΩïÊîπÈÄ≤ÁõÆÂâçÁöÑÊ®°ÂûãÔºå‰ª•‰ΩøÁî®‰æÜËá™ÈÜ´Èô¢ÊéÉÊèèÂÑÄÁöÑÂΩ±ÂÉè‰æÜÂØ¶ÁèæÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**
2310.01828v2 by Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour

eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫ËôïÁêÜ‰ªªÂãôÈóúÈçµÊáâÁî®Á®ãÂºèÊôÇÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨ÈúÄÊ±ÇÔºåÁ¢∫‰øùÊé°Áî®ÈªëÁõí AI Ê®°ÂûãÁöÑÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇXAI ÁöÑÈáçË¶ÅÊÄßÊ∂µËìãÂæûÈÜ´ÁôÇ‰øùÂÅ•Âà∞ÈáëËûçÁöÑÂêÑÁ®ÆÈ†òÂüüÔºåÂú®ÈÄô‰∫õÈ†òÂüü‰∏≠Ôºå‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãËá≥ÈóúÈáçË¶Å„ÄÇÂ§ßÂ§öÊï∏Âü∫Êñº AI ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÈÄöÂ∏∏ÊòØÈªëÁõíÂ≠êÔºõÂõ†Ê≠§ÔºåÂú®ÂΩ±ÂÉèËôïÁêÜ‰∏≠Êèê‰æõÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂèØËß£ÈáãÊÄßÂ∞çÊñºÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™ÂãïÈßïÈßõÂíåÈÅôÊ∏¨ÊáâÁî®‰∏≠ÁöÑÂª£Ê≥õÊé°Áî®ÂíåÈÉ®ÁΩ≤Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÔºåÂ∑≤ÈáùÂ∞çÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÂºïÂÖ•‰∫ÜÂ§öÁ®Æ XAI ÊñπÊ≥ï„ÄÇÁõ∏ÂèçÂú∞ÔºåÂΩ±ÂÉèÂàÜÂâ≤Âú®ÂèØËß£ÈáãÊÄßÁöÑËÉåÊôØ‰∏ãÂèóÂà∞ÁöÑÈóúÊ≥®Áõ∏Â∞çËºÉÂ∞ëÔºåÂÑòÁÆ°ÂÆÉÊòØÈõªËÖ¶Ë¶ñË¶∫ÊáâÁî®‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÁâπÂà•ÊòØÂú®ÈÅôÊ∏¨‰∏≠„ÄÇÂè™ÊúâÈÉ®ÂàÜÁ†îÁ©∂ÊèêÂá∫Áî®ÊñºÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑ XAI ÊºîÁÆóÊ≥ï„ÄÇÊú¨ÊñáÊîπÁ∑®‰∫ÜÊúÄËøëÁöÑÁÑ°Ê¢ØÂ∫¶ Sobol XAI ÊñπÊ≥ï‰ª•ÈÄ≤Ë°åË™ûÊÑèÂàÜÂâ≤„ÄÇÁÇ∫‰∫ÜË°°Èáè Sobol ÊñπÊ≥ïÂú®ÂàÜÂâ≤‰∏≠ÁöÑÊïàËÉΩÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂèØÂ≠∏ÁøíÈõúË®äÊ®°ÂûãÁöÑÂÆöÈáè XAI Ë©ï‰º∞ÊñπÊ≥ï„ÄÇÊ≠§Ê®°ÂûãÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØÂú®Ëß£ÈáãÂúñ‰∏äË™òÁôºÈõúË®äÔºåÂÖ∂‰∏≠ËºÉÈ´òÁöÑË™òÁôºÈõúË®äË°®Á§∫ËºÉ‰ΩéÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÈÄ≤Ë°åÂü∫Ê∫ñÂàÜÊûê‰ª•Ë©ï‰º∞ÂíåÊØîËºÉ‰∏âÁ®Æ XAI ÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÂåÖÊã¨ Seg-Grad-CAM„ÄÅSeg-Grad-CAM++ Âíå Seg-SobolÔºå‰∏¶‰ΩøÁî®ÊâÄÊèêÂá∫ÁöÑÂü∫ÊñºÈõúË®äÁöÑË©ï‰º∞ÊäÄË°ì„ÄÇÈÄôÊßãÊàê‰∫Ü‰ΩøÁî®È´òËß£ÊûêÂ∫¶Ë°õÊòüÂΩ±ÂÉèÂü∑Ë°åÂíåË©ï‰º∞ XAI ÊñπÊ≥ïÁöÑÈ¶ñÊ¨°ÂòóË©¶„ÄÇ

##### **Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**
2311.01463v1 by Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy

Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Áü≠ÊôÇÈñìÂÖßÂ∑≤Âú®Â§öÂÄãÈ†òÂüü‰∏≠Â§ßÈáèÊøÄÂ¢û„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº‰∫ãÂØ¶ÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÂπªË¶∫Á≠âÂïèÈ°åÔºåÈÜ´ÁôÇÂíå‰øùÂÅ•È†òÂüüÂ∞çÂÖ∂Êé°Áî®Áå∂Ë±´‰∏çÊ±∫„ÄÇÈëëÊñºÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÈ´òÈ¢®Èö™ÊÄßË≥™ÔºåË®±Â§öÁ†îÁ©∂‰∫∫Âì°ÁîöËá≥Ë≠¶Âëä‰∏çË¶Å‰ΩøÁî®ÂÆÉÔºåÁõ¥Âà∞ÈÄô‰∫õÂïèÈ°åÂæóÂà∞Ëß£Ê±∫„ÄÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂØ¶ÊñΩÂíåÈÉ®ÁΩ≤ LLM ÁöÑÈóúÈçµÊòØ‰ΩøÈÄô‰∫õÊ®°ÂûãÂÄºÂæó‰ø°Ë≥¥„ÄÅÈÄèÊòéÔºàÁõ°ÂèØËÉΩÂ§öÔºâ‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂª∫Á´ãÂèØÈù†„ÄÅÂÄºÂæó‰ø°Ë≥¥ÂíåÁÑ°ÂÅèË¶ãÊ®°ÂûãÁöÑÈóúÈçµË¶ÅÁ¥†Ôºå‰ΩúÁÇ∫ÂÆÉÂÄëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂæóÂà∞Êé°Áî®ÁöÑÂøÖË¶ÅÊ¢ù‰ª∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÂ∞çÂπªË¶∫ÈÄ≤Ë°åÈáèÂåñ„ÄÅÈ©óË≠âÂíåÁ∑©Ëß£„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÂèØËÉΩÊòØ‰ªÄÈ∫ºÊ®£Â≠ê„ÄÇ

##### **When to Trust AI: Advances and Challenges for Certification of Neural Networks**
2309.11196v1 by Marta Kwiatkowska, Xiyue Zhang

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂ∑≤Âø´ÈÄüÈÄ≤Ê≠•ÔºåÁèæÂ∑≤Ê∫ñÂÇôÈÉ®ÁΩ≤ÊñºÂª£Ê≥õÁöÑÊáâÁî®Á®ãÂºè‰∏≠Ôºå‰æãÂ¶ÇËá™‰∏ªÁ≥ªÁµ±„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ„ÄÇÂèäÊó©Êé°Áî® AI ÊäÄË°ìÊñºÂØ¶ÈöõÊáâÁî®Á®ãÂºè‰∏¶ÈùûÊ≤íÊúâÂïèÈ°åÔºåÁâπÂà•ÊòØÂ∞çÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÆÉÂèØËÉΩ‰∏çÁ©©ÂÆö‰∏îÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÁØÑ‰æãÁöÑÂΩ±Èüø„ÄÇÂæûÈï∑ÈÅ†‰æÜÁúãÔºåÈúÄË¶ÅÈñãÁôºÈÅ©Áï∂ÁöÑÂÆâÂÖ®‰øùË≠âÊäÄË°ìÔºå‰ª•Ê∏õÂ∞ëÂõ†ÂèØÈÅøÂÖçÁöÑÁ≥ªÁµ±ÊïÖÈöúËÄåÈÄ†ÊàêÁöÑÊΩõÂú®ÂÇ∑ÂÆ≥Ôºå‰∏¶Á¢∫‰øùÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÊú¨ÊñáËëóÈáçÊñºË™çË≠âÂíåÂèØËß£ÈáãÊÄßÔºåÊ¶ÇËø∞‰∫ÜÂ∑≤ÈñãÁôºÁî®ÊñºÁ¢∫‰øù AI Ê±∫Á≠ñÂÆâÂÖ®ÁöÑÊäÄË°ìÔºå‰∏¶Ë®éË´ñÊú™‰æÜÁöÑÊåëÊà∞„ÄÇ

##### **Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**
2309.10424v1 by Juan M. Garc√≠a-G√≥mez, Vicent Blanes-Selva, Jos√© Carlos de Bartolom√© Cenzano, Jaime Cebolla-Cornejo, Ascensi√≥n Do√±ate-Mart√≠nez

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

ÊëòË¶ÅÔºöÊ≠êÊ¥≤Ë≠∞ÊúÉË≠∞ÊúÉÁ†îÁ©∂ÊúçÂãôÁ∏ΩÂ±ÄÂ∑≤ÁÇ∫Ê≠êÊ¥≤Ë≠∞ÊúÉË≠∞Âì°Ê∫ñÂÇô‰∫Ü‰∏Ä‰ªΩÂ†±ÂëäÔºåÂÖ∂‰∏≠ÂàóËàâ‰∫Ü‰∫∫Â∑•Êô∫ËÉΩ (AI) Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑ‰∏ÉÈ†Ö‰∏ªË¶ÅÈ¢®Èö™ÔºöAI ÈåØË™§Â∞éËá¥ÊÇ£ËÄÖÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÅÈÜ´ÁôÇ AI Â∑•ÂÖ∑Ë¢´Êø´Áî®„ÄÅAI Â≠òÂú®ÂÅèË¶ã‰∏¶Â∞éËá¥ÁèæÊúâ inequities ÊåÅÁ∫åÂ≠òÂú®„ÄÅÁº∫‰πèÈÄèÊòéÂ∫¶„ÄÅÈö±ÁßÅÂíåÂÆâÂÖ®ÂïèÈ°å„ÄÅÂïèË≤¨Â∑ÆË∑ù‰ª•ÂèäÂØ¶ÊñΩÈöúÁ§ô„ÄÇ
  Âú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂçÅÂõõÈ†ÖÂäüËÉΩÊÄßË¶ÅÊ±ÇÔºåAI Á≥ªÁµ±ÂèØ‰ª•ÂØ¶ÊñΩÈÄô‰∫õË¶ÅÊ±Ç‰æÜÈôç‰ΩéËàáÂÖ∂ÈÜ´ÁôÇÁõÆÁöÑÁõ∏ÈóúÁöÑÈ¢®Èö™ÔºöAI Ë≠∑ÁÖß„ÄÅ‰ΩøÁî®ËÄÖÁÆ°ÁêÜ„ÄÅÊ≥ïË¶èÊ™¢Êü•„ÄÅÂÉÖÈôêÂ≠∏Ë°ìÁî®ÈÄîÂÖçË≤¨ËÅ≤Êòé„ÄÅË≥áÊñôÂìÅË≥™Ë©ï‰º∞„ÄÅËá®Â∫äÈÜ´ÁîüÈõôÈáçÊ™¢Êü•„ÄÅÊåÅÁ∫åÊïàËÉΩË©ï‰º∞„ÄÅÁ®ΩÊ†∏ËøΩËπ§„ÄÅÊåÅÁ∫åÂèØÁî®ÊÄßÊ∏¨Ë©¶„ÄÅÂõûÈ°ßÂõûÊ∫Ø/Ê®°Êì¨Ê°à‰æã„ÄÅÂÅèË¶ãÊ™¢Êü•„ÄÅÂèØËß£Èáã AI„ÄÅÂä†ÂØÜÂíå‰ΩøÁî®Á∂ìÈÅéÂØ¶Âú∞Ê∏¨Ë©¶ÁöÑÁ®ãÂºèÂ∫´Ôºå‰ª•ÂèäË™ûÊÑè‰∫íÈÄöÊÄß„ÄÇ
  ÊàëÂÄëÂú®Ê≠§ÁöÑÁõÆÁöÑÊòØÊèê‰æõÊäÄË°ìËß£Ê±∫ÊñπÊ°àÁöÑÁâπÂÆöÈ´òÈöéË¶èÊ†ºÔºå‰ª•Á¢∫‰øùÊåÅÁ∫åËâØÂ•ΩÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÁî® AI Á≥ªÁµ±Ôºå‰ª•Á¨¶ÂêàÊú™‰æÜÁöÑÊ≠êÁõüÊ≥ïË¶èÊû∂ÊßãÔºåÂæûËÄå‰ΩøÊÇ£ËÄÖÂèóÁõä„ÄÇ

##### **QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**
2309.10293v3 by Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins

Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ìÂèØÁî®ÊñºÂàÜÈ°ûÁóÖÊÇ£ÁöÑË∫´È´îÊ¥ªÂãï‰∏¶È†êÊ∏¨ÈÅ†Ë∑ùÁóÖÊÇ£Áõ£ÊéßÁöÑÈáçË¶ÅÁîüÂëΩÂæµË±°„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁ≠âÈùûÁ∑öÊÄßÊ®°ÂûãÁöÑÂõûÊ≠∏ÂàÜÊûêÁî±ÊñºÂÖ∂ÈªëÁõíÂ≠êÁöÑÊÄßË≥™ËÄåÂÖ∑ÊúâÊúâÈôêÁöÑÂèØËß£ÈáãÊÄß„ÄÇÈÄôÂèØËÉΩÈúÄË¶ÅÊ±∫Á≠ñËÄÖÊ†πÊìöÈùûÁ∑öÊÄßÊ®°ÂûãÁµêÊûúÂÅöÂá∫Áõ≤ÁõÆÁöÑ‰ø°‰ª∞È£õË∫çÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠„ÄÇÂú®Èùû‰æµÂÖ•ÊÄßÁõ£Êéß‰∏≠Ôºå‰æÜËá™ËøΩËπ§ÊÑüÊ∏¨Âô®ÂíåÂÖ∂ÊòìÊÑüËá®Â∫äÂ±¨ÊÄßÁöÑÁóÖÊÇ£Ë≥áÊñôÂÖÖÁï∂È†êÊ∏¨Êú™‰æÜÁîüÂëΩÂæµË±°ÁöÑËº∏ÂÖ•ÁâπÂæµ„ÄÇËß£ÈáãÂêÑÁ®ÆÁâπÂæµÂ∞çÁõ£ÊéßÊáâÁî®Á®ãÂºèÊï¥È´îËº∏Âá∫ÁöÑË≤¢ÁçªÂ∞çÊñºËá®Â∫äÈÜ´ÁîüÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®ÊñºÈáèÂåñÂàÜÊûêÁöÑÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (QXAI) Êû∂ÊßãÔºåË©≤Êû∂ÊßãÂÖ∑ÊúâÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ï‰∏≠ÂõûÊ≠∏ÂíåÂàÜÈ°û‰ªªÂãôÁöÑ‰∫ãÂæåÊ®°ÂûãÂèØËß£ÈáãÊÄßÂíåÂÖßÂú®ÂèØËß£ÈáãÊÄß„ÄÇÈÄôÈÄèÈÅéÂà©Áî® Shapley ÂÄºÊ¶ÇÂøµ‰∏¶Â∞áÊ≥®ÊÑèÂäõÊ©üÂà∂Á¥çÂÖ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æÜÂØ¶Áèæ„ÄÇÊàëÂÄëÊé°Áî®‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑Ø (ANN) ÂíåÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÈõôÂêë LSTM (BiLSTM) Ê®°ÂûãÔºåÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÈ†êÊ∏¨ÂøÉÁéáÂíåÂàÜÈ°ûË∫´È´îÊ¥ªÂãï„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂú®È†êÊ∏¨ÂíåÂàÜÈ°û‰ªªÂãô‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÂ∞çËº∏ÂÖ•Ë≥áÊñôÈÄ≤Ë°åÂÖ®Â±ÄËß£ÈáãÂíåÂ±ÄÈÉ®Ëß£ÈáãÔºå‰ª•‰∫ÜËß£ÂêÑÁ®ÆÁóÖÊÇ£Ë≥áÊñôÁöÑÁâπÂæµË≤¢Áçª„ÄÇÊâÄÊèêÂá∫ÁöÑ QXAI Êû∂Êßã‰ΩøÁî® PPG-DaLiA Ë≥áÊñôË©ï‰º∞Ôºå‰ª•È†êÊ∏¨ÂøÉÁéáÔºå‰∏¶‰ΩøÁî®Ë°åÂãïÂÅ•Â∫∑ (MHEALTH) Ë≥áÊñôÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÂ∞çË∫´È´îÊ¥ªÂãïÈÄ≤Ë°åÂàÜÈ°û„ÄÇËíôÂú∞Âç°ÁæÖËøë‰ººÊ≥ïÊáâÁî®ÊñºË©≤Êû∂ÊßãÔºå‰ª•ÂÖãÊúç Shapley ÂÄºË®àÁÆóÊâÄÈúÄÁöÑÊôÇÈñìË§áÈõúÂ∫¶ÂíåÈ´òÈÅãÁÆóËÉΩÂäõÈúÄÊ±Ç„ÄÇ

##### **Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**
2309.09917v1 by Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren

In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.

ÊëòË¶ÅÔºöÂú®ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) Á†îÁ©∂‰∏≠Ôºå‰∏ªË¶ÅÈáçÁÇπÂú®‰∫é‰∏∫‰∏ìÂÆ∂Âíå‰ªé‰∏öËÄÖËß£ÈáäÊ®°Âûã„ÄÇÊ®°Âûã‰∏çÂèØÁü•ÂíåÂ±ÄÈÉ®Ëß£ÈáäÊñπÊ≥ïÂú®ËÆ∏Â§öÂ∫îÁî®‰∏≠Ë¢´ËÆ§‰∏∫ÊòØÂèØËß£Èáä‰∏îË∂≥Â§üÁöÑ„ÄÇÁÑ∂ËÄåÔºåÂú®ÂåªÁñó‰øùÂÅ•Á≠âÈ¢ÜÂüüÔºåÊúÄÁªàÁî®Êà∑ÊòØÁº∫‰πè‰∫∫Â∑•Êô∫ËÉΩÊàñÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÁöÑÊÇ£ËÄÖÔºåÂõ†Ê≠§Ëø´ÂàáÈúÄË¶ÅÊõ¥Êòì‰∫éÁêÜËß£‰∏îËÉΩÊøÄÂèëÂØπÊ®°ÂûãÊìç‰ΩúÁöÑ‰ø°‰ªªÁöÑÊ®°ÂûãËß£Èáä„ÄÇÊàë‰ª¨ÂÅáËÆæÁîüÊàêÂèôËø∞ÊÄß„ÄÅÊÇ£ËÄÖÁâπÂÆö‰∏îÂÖ®Â±ÄÔºàÊ®°ÂûãÊï¥‰ΩìÔºâÁöÑÊ®°ÂûãËß£ÈáäÂ∞ÜËÉΩÂ§üÊèêÈ´òÂèØÁêÜËß£ÊÄßÂπ∂ÊîØÊåÅÂÜ≥Á≠ñÂà∂ÂÆö„ÄÇÊàë‰ª¨‰ΩøÁî®ÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÂØπÊ≠§ËøõË°åÊµãËØïÔºå‰∏∫Ë¢´ËØÜÂà´‰∏∫ÊÇ£ÊúâÂÜ†ÂøÉÁóÖÈ´òÈ£éÈô©ÁöÑÊÇ£ËÄÖÁîüÊàêÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËß£Èáä„ÄÇËøô‰∫õËß£Èáä‰ºöÂëàÁé∞ÁªôÈùû‰∏ìÂÆ∂Áî®Êà∑„ÄÇÊàë‰ª¨ÂèëÁé∞Áî®Êà∑Âº∫ÁÉàÂÅèÂ•ΩÁâπÂÆöÁ±ªÂûãÁöÑËß£Èáä„ÄÇÂ§ßÂ§öÊï∞ÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂÖ®Â±ÄËß£ÈáäÔºåËÄåËæÉÂ∞èÁöÑ‰∏ÄÁªÑÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂ±ÄÈÉ®Ëß£Èáä„ÄÇÂü∫‰∫é‰ªªÂä°ÁöÑÂøÉÁêÜÊ®°ÂûãËØÑ‰º∞‰∏∫Ëøô‰∫õÂèÇ‰∏éËÄÖÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂèçÈ¶àÔºå‰ª•Â¢ûÂº∫ÂèôËø∞ÊÄßÂÖ®Â±ÄËß£Èáä„ÄÇËøôÂèçËøáÊù•ÂèàÊåáÂØº‰∫ÜÊó¢ÂÄºÂæó‰ø°ËµñÂèàÂèØÊìç‰ΩúÁöÑÂÅ•Â∫∑‰ø°ÊÅØÂ≠¶Á≥ªÁªüÁöÑËÆæËÆ°„ÄÇ

##### **Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**
2308.14321v1 by Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar

Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) Âíå‰æãË°åÊñá‰ª∂Ë®òÈåÑÂØ¶ÂãôÂú®ÁóÖÊÇ£ÁöÑÊó•Â∏∏ÁÖßË≠∑‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÊèê‰æõÂÅ•Â∫∑„ÄÅË®∫Êñ∑ÂíåÊ≤ªÁôÇÁöÑÊï¥È´îÁ¥ÄÈåÑ„ÄÇÁÑ∂ËÄåÔºåË§áÈõú‰∏îÂÜóÈï∑ÁöÑ EHR ÊïòËø∞ÊúÉËÆìÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖË∂ÖËºâÔºåÊúâË®∫Êñ∑‰∏çÊ∫ñÁ¢∫ÁöÑÈ¢®Èö™„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂÖ∂Âú®ÂêÑÁ®ÆË™ûË®Ä‰ªªÂãô‰∏äÁöÑÊΩõÂäõÔºå‰ΩÜÂÖ∂Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊáâÁî®ÈúÄË¶ÅÁ¢∫‰øùÂ∞áË®∫Êñ∑ÈåØË™§ÈôçÂà∞ÊúÄ‰ΩéÔºå‰∏¶Èò≤Ê≠¢ÁóÖÊÇ£ÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊ¶ÇËø∞‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÊï¥ÂêàÈÜ´Â≠∏Áü•Ë≠òÂúñË≠ú (KG) Âíå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂúñË≠úÊ®°ÂûãÔºöDr.KnowsÔºàÈùàÊÑü‰æÜËá™Ëá®Â∫äË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ãÔºâÔºå‰æÜÂ¢ûÂº∑ LLM Âú®Ëá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂæûÁæéÂúãÂúãÂÆ∂ÈÜ´Â≠∏ÂúñÊõ∏È§®ÁöÑÁµ±‰∏ÄÈÜ´Â≠∏Ë™ûË®ÄÁ≥ªÁµ± (UMLS) ‰∏≠Ë°çÁîüÂá∫ KGÔºåÈÄôÊòØ‰∏ÄÂÄãÂº∑Â§ßÁöÑÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂÑ≤Â≠òÂ∫´„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂê¶ÂÆö‰∫ÜÈ†êÂÖàË®ìÁ∑¥ÁöÑÈúÄË¶ÅÔºåËÄåÊòØÂ∞á KG ‰ΩúÁÇ∫ËºîÂä©Â∑•ÂÖ∑ÔºåÂçîÂä©Ëß£ÈáãÂíåÁ∏ΩÁµêË§áÈõúÁöÑÈÜ´Â≠∏Ê¶ÇÂøµ„ÄÇ‰ΩøÁî®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´Èô¢Ë≥áÊñôÈõÜÔºåÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÂ∞á LLM Ëàá KG ÁµêÂêàÁöÑÂª∫Ë≠∞ÊñπÊ≥ïÊúâÊΩõÂäõÊèêÈ´òËá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÊ¢ùÂèØËß£ÈáãÁöÑË®∫Êñ∑ÈÄîÂæëÔºåËÆìÊàëÂÄëÊõ¥Êé•ËøëÂØ¶Áèæ AI Â¢ûÂº∑ÁöÑË®∫Êñ∑Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ±„ÄÇ

##### **Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**
2308.09380v1 by Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai

Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÁî®ÊñºË®∫Êñ∑ËÜùÈ™®ÈóúÁØÄÁÇé (OA) ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÂõ†ÂÖ∂Áº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßËÄåÂèóÂà∞ÊâπË©ïÔºåÂÑòÁÆ°ÂÆÉÂÄëÈÅîÂà∞‰∫ÜÈ°û‰ººÈÜ´Â≠∏Â∞àÂÆ∂ÁöÑË°®Áèæ„ÄÇÈÄôÁ®Æ‰∏çÈÄèÊòéÊÄß‰ΩøÂæóÂÆÉÂÄëÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Èõ£‰ª•Ë¢´‰ø°‰ªª„ÄÇÊúÄËøëÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÂ∞àÈñÄÊäÄË°ìÔºåÂÆÉËÉΩÈÄèÈÅéÊè≠Á§∫È†êÊ∏¨ÁöÑÊé®Â∞éÊñπÂºè‰æÜÊèê‰æõÂ∞çÊ®°ÂûãÈ†êÊ∏¨ÁöÑ‰ø°ÂøÉÔºåÂæûËÄå‰øÉÈÄ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî® AI Á≥ªÁµ±„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÈáùÂ∞çËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊâÄ‰ΩøÁî®ÁöÑ XAI ÊäÄË°ìÁöÑÁ¨¨‰∏Ä‰ªΩË™øÊü•„ÄÇXAI ÊäÄË°ìÂæûÂÖ©ÂÄãËßíÂ∫¶ÈÄ≤Ë°åË®éË´ñÔºöË≥áÊñôÂèØËß£ÈáãÊÄßÂíåÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÇÊú¨ÊñáÁöÑÁõÆÁöÑÊòØÊèê‰æõÂ∞ç XAI Âú®Êõ¥ÂèØÈù†ÁöÑËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊΩõÂäõÁöÑÂØ∂Ë≤¥Ë¶ãËß£Ôºå‰∏¶ÈºìÂãµÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Êé°Áî®ÂÆÉ„ÄÇ

##### **Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**
2308.08407v1 by Munib Mesinovic, Peter Watkinson, Tingting Zhu

Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

ÊëòË¶ÅÔºöÊúÄËøëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®ÈÄ≤Â±ïÈ°ØÁ§∫Âá∫‰ª§‰∫∫Èõ£‰ª•ÁΩÆ‰ø°ÁöÑÊâøË´æÔºåÂú®Ë®∫Êñ∑ÂíåÁñæÁóÖÈ†êÂæåÊñπÈù¢Ë∂ÖË∂ä‰∫∫È°ûË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÈö®Ëëó‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÁöÑÊó•ÁõäË§áÈõúÔºå‰∫∫ÂÄëÂ∞çÂÖ∂‰∏çÈÄèÊòéÊÄß„ÄÅÊΩõÂú®ÂÅèÂ∑ÆÂíåÂ∞çÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÊÑüÂà∞ÊìîÊÜÇ„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øù‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±ÁöÑ‰ø°‰ªªÂíåÂèØÈù†ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨Ê®°Âûã‰∏≠ÔºåÂèØËß£ÈáãÊÄßËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇÂèØËß£ÈáãÊÄßÈÄöÂ∏∏Ë¢´Á®±ÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±Êèê‰æõÂÖ∂Ê±∫Á≠ñÈÇèËºØÊàñÊ±∫Á≠ñÊú¨Ë∫´Â∞ç‰∫∫È°ûÂà©ÁõäÁõ∏ÈóúËÄÖÁöÑÂº∑ÊúâÂäõËß£ÈáãÁöÑËÉΩÂäõ„ÄÇÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨‰∏≠ÔºåÂèØËß£ÈáãÊÄßÁöÑÂÖ∂‰ªñÊñπÈù¢ÔºåÂ¶ÇÂÖ¨Âπ≥ÊÄß„ÄÅÂÅèË¶ã„ÄÅ‰ø°‰ªªÂíåÈÄèÊòéÂ∫¶Ôºå‰πü‰ª£Ë°®‰∫ÜË∂ÖË∂äÂèØËß£ÈáãÊÄßÁöÑÈáçË¶ÅÊ¶ÇÂøµ„ÄÇÂú®Êú¨Ê¨°ÂØ©Êü•‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÊ¶ÇÂøµ‰πãÈñìÁöÑÈóú‰øÇÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁ∂ìÂ∏∏‰∏ÄËµ∑Êàñ‰∫íÊèõ‰ΩøÁî®„ÄÇÊú¨ÂØ©Êü•ÈÇÑË®éË´ñ‰∫ÜÁÇ∫Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨ÈñãÁôºÂèØËß£ÈáãÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÂº∑Ë™ø‰∫ÜÂú®Ëá®Â∫äÂØ¶Ë∏ê‰∏≠Â∞çÂ§öÁ®ÆÂ∏∏Ë¶ãÊ®°ÂºèÈÄ≤Ë°åÂÆöÈáèÂíåËá®Â∫äË©ï‰º∞ÂíåÈ©óË≠âÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂ§ñÈÉ®È©óË≠âÂíåÂ§öÊ®£ÂåñÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁõ∏ÁµêÂêàÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Â¢ûÂº∑‰ø°‰ªªÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊé°Áî®Âö¥Ê†ºÁöÑÊ∏¨Ë©¶Ôºå‰æãÂ¶Ç‰ΩøÁî®ÂÖ∑ÊúâÂ∑≤Áü•ÁîüÊàêÂõ†Á¥†ÁöÑÂêàÊàêÊï∏ÊìöÈõÜÔºåÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÂèØÈù†ÊÄß„ÄÇÈñãÊîæÁç≤ÂèñÂíå‰ª£Á¢ºÂÖ±‰∫´Ë≥áÊ∫êÂ∞çÊñºÈÄèÊòéÂ∫¶ÂíåÂèØÈáçË§áÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂæûËÄå‰øÉÈÄ≤ÂèØËß£ÈáãÁ†îÁ©∂ÁöÑÂ¢ûÈï∑ÂíåÂèØ‰ø°Â∫¶„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÂæûËá®Â∫äÈÜ´ÁîüÂà∞ÈñãÁôº‰∫∫Âì°ÔºåÊé°Áî®Á´ØÂà∞Á´ØÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞çÊñºËá®Â∫äÈ¢®Èö™È†êÊ∏¨ÁöÑÊàêÂäüËá≥ÈóúÈáçË¶Å„ÄÇ

##### **FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**
2309.12325v3 by Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A Gonz√°lez, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Horst Joachim Mayer, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, Ir√®ne Buvat, Isabell Tributsch, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor Cerd√° Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, Llu√≠s Donoso-Bach, Luis Mart√≠-Bonmat√≠, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marina Camacho, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, M√≥nica Cano Abad√≠a, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver D√≠az, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna Auss√≥, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, X√®nia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans

Despite major advances in artificial intelligence (AI) for medicine and
healthcare, the deployment and adoption of AI technologies remain limited in
real-world clinical practice. In recent years, concerns have been raised about
the technical, clinical, ethical and legal risks associated with medical AI. To
increase real world adoption, it is essential that medical AI tools are trusted
and accepted by patients, clinicians, health organisations and authorities.
This work describes the FUTURE-AI guideline as the first international
consensus framework for guiding the development and deployment of trustworthy
AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and
currently comprises 118 inter-disciplinary experts from 51 countries
representing all continents, including AI scientists, clinicians, ethicists,
and social scientists. Over a two-year period, the consortium defined guiding
principles and best practices for trustworthy AI through an iterative process
comprising an in-depth literature review, a modified Delphi survey, and online
consensus meetings. The FUTURE-AI framework was established based on 6 guiding
principles for trustworthy AI in healthcare, i.e. Fairness, Universality,
Traceability, Usability, Robustness and Explainability. Through consensus, a
set of 28 best practices were defined, addressing technical, clinical, legal
and socio-ethical dimensions. The recommendations cover the entire lifecycle of
medical AI, from design, development and validation to regulation, deployment,
and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which
provides a structured approach for constructing medical AI tools that will be
trusted, deployed and adopted in real-world practice. Researchers are
encouraged to take the recommendations into account in proof-of-concept stages
to facilitate future translation towards clinical practice of medical AI.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈÜ´Â≠∏ÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊúâÈáçÂ§ßÁöÑÈÄ≤Â±ïÔºå‰ΩÜ AI ÊäÄË°ìÁöÑÈÉ®ÁΩ≤ÂíåÊé°Áî®Âú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÂØ¶Âãô‰∏≠‰ªçÁÑ∂ÊúâÈôê„ÄÇËøëÂπ¥‰æÜÔºå‰∫∫ÂÄëÂ∞çÊñºËàáÈÜ´ÁôÇ AI Áõ∏ÈóúÁöÑÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÂÄ´ÁêÜÂíåÊ≥ïÂæãÈ¢®Èö™ÊèêÂá∫‰∫ÜÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÂ¢ûÂä†ÁèæÂØ¶‰∏ñÁïåÁöÑÊé°Áî®ÁéáÔºåÈÜ´ÁôÇ AI Â∑•ÂÖ∑ÂøÖÈ†àÁç≤ÂæóÊÇ£ËÄÖ„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÈÜ´ÁôÇÊ©üÊßãÂíåÁï∂Â±ÄÁöÑ‰ø°‰ªªÂíåÊé•Âèó„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂ∞á FUTURE-AI ÊåáÂçóÊèèËø∞ÁÇ∫ÊåáÂ∞éÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI Â∑•ÂÖ∑ÈñãÁôºÂíåÈÉ®ÁΩ≤ÁöÑÁ¨¨‰∏ÄÂÄãÂúãÈöõÂÖ±Ë≠òÊû∂Êßã„ÄÇFUTURE-AI ËÅØÁõüÊàêÁ´ãÊñº 2021 Âπ¥ÔºåÁõÆÂâçÁî±‰æÜËá™ 51 ÂÄãÂúãÂÆ∂ÁöÑ 118 ‰ΩçË∑®È†òÂüüÂ∞àÂÆ∂ÁµÑÊàêÔºå‰ª£Ë°®ÊâÄÊúâÊ¥≤ÔºåÂåÖÊã¨ AI ÁßëÂ≠∏ÂÆ∂„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÂÄ´ÁêÜÂ≠∏ÂÆ∂ÂíåÁ§æÊúÉÁßëÂ≠∏ÂÆ∂„ÄÇÂú®ÂÖ©Âπ¥ÁöÑÊôÇÈñìË£°ÔºåË©≤ËÅØÁõüÈÄöÈÅé‰∏ÄÂÄãÂèçË¶ÜÈÅãÁÆóÁöÑÈÅéÁ®ãÂÆöÁæ©‰∫ÜÂèØ‰ø°Ë≥¥ AI ÁöÑÊåáÂ∞éÂéüÂâáÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÂåÖÊã¨Ê∑±ÂÖ•ÁöÑÊñáÁçªÂõûÈ°ß„ÄÅ‰øÆÊîπÂæåÁöÑÂæ∑ÁàæËè≤Ë™øÊü•ÂíåÁ∑ö‰∏äÂÖ±Ë≠òÊúÉË≠∞„ÄÇFUTURE-AI Êû∂ÊßãÊòØÂü∫ÊñºÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI ÁöÑ 6 È†ÖÊåáÂ∞éÂéüÂâáÂª∫Á´ãÁöÑÔºåÂç≥ÂÖ¨Âπ≥ÊÄß„ÄÅÊôÆÈÅçÊÄß„ÄÅÂèØËøΩÊ∫ØÊÄß„ÄÅÂèØÁî®ÊÄß„ÄÅÁ©©ÂÅ•ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÈÄöÈÅéÂÖ±Ë≠òÔºåÂÆöÁæ©‰∫Ü‰∏ÄÁµÑ 28 È†ÖÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÊ∂µËìãÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÊ≥ïÂæãÂíåÁ§æÊúÉÂÄ´ÁêÜÂ±§Èù¢„ÄÇÂª∫Ë≠∞Ê∂µËìã‰∫ÜÈÜ´ÁôÇ AI ÁöÑÊï¥ÂÄãÁîüÂëΩÈÄ±ÊúüÔºåÂæûË®≠Ë®à„ÄÅÈñãÁôºÂíåÈ©óË≠âÂà∞Ê≥ïË¶è„ÄÅÈÉ®ÁΩ≤ÂíåÁõ£Êéß„ÄÇFUTURE-AI ÊòØ‰∏ÄÂÄãÂü∫ÊñºÈ¢®Èö™„ÄÅÁÑ°ÂÅáË®≠ÁöÑÊåáÂçóÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂª∫ÊßãÂ∞áÂú®ÁèæÂØ¶‰∏ñÁïåÂØ¶Âãô‰∏≠ÂèóÂà∞‰ø°‰ªª„ÄÅÈÉ®ÁΩ≤ÂíåÊé°Áî®ÁöÑÈÜ´ÁôÇ AI Â∑•ÂÖ∑„ÄÇÈºìÂãµÁ†îÁ©∂‰∫∫Âì°Âú®Ê¶ÇÂøµÈ©óË≠âÈöéÊÆµËÄÉÊÖÆÈÄô‰∫õÂª∫Ë≠∞Ôºå‰ª•‰øÉÈÄ≤Êú™‰æÜÂ∞áÈÜ´ÁôÇ AI ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶Âãô„ÄÇ

##### **Explainable AI applications in the Medical Domain: a systematic review**
2308.05411v1 by Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis

Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠Â∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÅÁóÖ‰∫∫ÁÖßË≠∑ÂíåÂÖ∂‰ªñÈ†òÂüü‰∏≠Âá∫Áèæ‰∫ÜÊñ∞ËààÊáâÁî®„ÄÇÈõñÁÑ∂ÈÄô‰∫õÊáâÁî®Â∑≤Âú®ÂõûÈ°ßÊÄßÁ†îÁ©∂‰∏≠Ë¢´Ë≠âÂØ¶ÊòØÊàêÂäüÁöÑÔºå‰ΩÜÂØ¶Èöõ‰∏äÂè™ÊúâÊ•µÂ∞ëÊï∏ÊáâÁî®ÊñºÂØ¶Âãô„ÄÇÈÜ´ÁôÇ AI È†òÂüüÈù¢Ëá®ËëóÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨Âª∫Á´ã‰ΩøÁî®ËÄÖ‰ø°‰ªª„ÄÅÈÅµÂÆàÊ≥ïË¶è„ÄÅ‰ΩøÁî®Ë≥áÊñôÁ¨¶ÂêàÂÄ´ÁêÜ„ÄÇÂèØËß£Èáã AI (XAI) ÁöÑÁõÆÊ®ôÊòØËÆì‰∫∫È°û‰∫ÜËß£ AI ‰∏¶Áõ∏‰ø°ÂÖ∂ÁµêÊûú„ÄÇÊú¨ÊñáÈáùÂ∞çÊúÄËøëÂπæÂπ¥ÁôºË°®ÁöÑ 198 ÁØáÊñáÁ´†ÁöÑÂÖ∑‰ª£Ë°®ÊÄßÊ®£Êú¨ÔºåÊèêÂá∫ÊúâÈóúÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥ÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑÊúÄÊñ∞ÁôºÂ±ïÁöÑÊñáÁçªÂõûÈ°ß„ÄÇÁõ∏ÈóúÊñáÁ´†ÁöÑÁ≥ªÁµ±ÊÄßÁ∂úÂêàÊï¥ÁêÜÁî¢Áîü‰∫ÜÂ§öÈ†ÖÁôºÁèæÔºö(1) ÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÂ§ßÂ§öÊé°Áî®ËàáÊ®°ÂûãÁÑ°ÈóúÁöÑ XAI ÊäÄË°ìÔºå(2) Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑ‰ΩøÁî®ÁéáÈ´òÊñºÂÖ∂‰ªñÈ°ûÂûãÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÔºå(3) ÂèØËß£ÈáãÊÄßË¢´Áî®Êñº‰øÉÈÄ≤‰ø°‰ªªÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂Â†±ÂëäÈÜ´Â∏´ÂèÉËàáËø¥ÂúàÔºå(4) Ë¶ñË¶∫Âíå‰∫íÂãïÂºè‰ΩøÁî®ËÄÖ‰ªãÈù¢Â∞çÊñºÁêÜËß£Á≥ªÁµ±ÁöÑËß£ÈáãÂíåÂª∫Ë≠∞Êõ¥ÊúâÁî®„ÄÇÈúÄË¶ÅÊõ¥Â§öÈÜ´ÁôÇÂíå AI Â∞àÂÆ∂Âêà‰ΩúÈÄ≤Ë°åÁ†îÁ©∂ÔºåÈÄôÊúâÂä©ÊñºÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑË®≠Ë®à„ÄÅÂØ¶‰ΩúÂíåË©ï‰º∞Êèê‰æõÈÅ©Áï∂Êû∂Êßã„ÄÇ

##### **Exploring the Role of Explainability in AI-Assisted Embryo Selection**
2308.02534v1 by Lucia Urcelay, Daniel Hinjos, Pablo A. Martin-Torres, Marta Gonzalez, Marta Mendez, Salva C√≠vico, Sergio √Ålvarez-Napagao, Dario Garcia-Gasulla

In Vitro Fertilization is among the most widespread treatments for
infertility. One of its main challenges is the evaluation and selection of
embryo for implantation, a process with large inter- and intra-clinician
variability. Deep learning based methods are gaining attention, but their
opaque nature compromises their acceptance in the clinical context, where
transparency in the decision making is key. In this paper we analyze the
current work in the explainability of AI-assisted embryo analysis models,
identifying the limitations. We also discuss how these models could be
integrated in the clinical context as decision support systems, considering the
needs of clinicians and patients. Finally, we propose guidelines for the sake
of increasing interpretability and trustworthiness, pushing this technology
forward towards established clinical practice.

ÊëòË¶ÅÔºöÈ´îÂ§ñÂèóÁ≤æÊòØÊ≤ªÁôÇ‰∏çÂ≠ïÁóáÊúÄÂª£Ê≥õÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇÂÖ∂‰∏ªË¶ÅÊåëÊà∞‰πã‰∏ÄÊòØË©ï‰º∞ÂíåÈÅ∏ÊìáËÉöËÉéÈÄ≤Ë°åÊ§çÂÖ•ÔºåÊ≠§ÈÅéÁ®ãÂÖ∑ÊúâÂæàÂ§ßÁöÑËá®Â∫äÈñìÂíåËá®Â∫äÂÖßËÆäÁï∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÊ≠£ÂèóÂà∞ÈóúÊ≥®Ôºå‰ΩÜÂÖ∂‰∏çÈÄèÊòéÁöÑÊÄßË≥™ÊúÉÂΩ±ÈüøÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊé•ÂèóÂ∫¶ÔºåËÄåÈÄèÊòéÂ∫¶Âú®Ê±∫Á≠ñÂà∂ÂÆö‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü AI ËºîÂä©ËÉöËÉéÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÁèæÊúâÂ∑•‰ΩúÔºå‰∏¶ÊâæÂá∫ÂÖ∂Â±ÄÈôêÊÄß„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊ®°Âûã‰ΩúÁÇ∫Ê±∫Á≠ñÊîØÊåÅÁ≥ªÁµ±Êï¥ÂêàÂà∞Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÂêåÊôÇËÄÉÊÖÆËá®Â∫äÈÜ´ÁîüÂíåÊÇ£ËÄÖÁöÑÈúÄÊ±Ç„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÁöÑÊ∫ñÂâáÔºåÊé®ÈÄ≤ÈÄôÈ†ÖÊäÄË°ìÊúùËëóÊó¢ÂÆöÁöÑËá®Â∫äÂØ¶ÂãôÈÇÅÈÄ≤„ÄÇ

##### **A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**
2307.14246v1 by Timo Speith, Markus Langer

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ã (RE) È†òÂüü‰∏≠ÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Âú®Â∞á AI ÊîØÊåÅÁöÑÁ≥ªÁµ±Ëàá‰ΩøÁî®ËÄÖÈúÄÊ±Ç„ÄÅÁ§æÊúÉÊúüÊúõÂíåÊ≥ïË¶èÊ®ôÊ∫ñÁõ∏Á¨¶ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÈ°ØËëóÔºåÂ∑≤Áç≤ÂæóË™çÂèØ„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÂèØËß£ÈáãÊÄßÂ∑≤ÊàêÁÇ∫ÂΩ±ÈüøÁ≥ªÁµ±ÂìÅË≥™ÁöÑÈáçË¶ÅÈùûÂäüËÉΩÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáãÊÄßËàáÊïàËÉΩ‰πãÈñìÁöÑÂÅáÂÆöÊ¨äË°°ÊåëÊà∞‰∫ÜÂèØËß£ÈáãÊÄßÁöÑÂÅáÂÆöÊ≠£Èù¢ÂΩ±Èüø„ÄÇÂ¶ÇÊûúÊªøË∂≥ÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁµ±ÊïàËÉΩÔºåÈÇ£È∫ºÂøÖÈ†à‰ªîÁ¥∞ËÄÉÊÖÆÈÄô‰∫õÂìÅË≥™Èù¢Âêë‰∏≠Âì™‰∏ÄÂÄãÂÑ™ÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉÂÄë‰πãÈñìÈÄ≤Ë°åÊäòË°∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞Êé¢Ë®é‰∫ÜÈÄôÁ®ÆÂÅáÂÆöÁöÑÊ¨äË°°„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÊúÄÂ•ΩÁöÑÊñπÊ≥ïÊòØ‰ª•‰∏ÄÁ®ÆÁ¥∞Á∑ªÁöÑÊñπÂºè‰æÜËôïÁêÜÔºåÈÄôÁ®ÆÊñπÂºèÂåÖÂê´Ë≥áÊ∫êÂèØÁî®ÊÄß„ÄÅÈ†òÂüüÁâπÊÄßÂíåÈ¢®Èö™ËÄÉÈáè„ÄÇÈÄèÈÅéÊèê‰æõÊú™‰æÜÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÁöÑÂü∫Á§éÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®ÊèêÂçá AI ÁöÑ RE È†òÂüü„ÄÇ

##### **Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**
2307.14239v1 by Barnaby Crook, Maximilian Schl√ºter, Timo Speith

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ãÔºàREÔºâÈ¢ÜÂüüÔºåÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÔºàXAIÔºâÂú®Â∞Ü‰∫∫Â∑•Êô∫ËÉΩÊîØÊåÅÁöÑÁ≥ªÁªü‰∏éÁî®Êà∑ÈúÄÊ±Ç„ÄÅÁ§æ‰ºöÊúüÊúõÂíåÁõëÁÆ°Ê†áÂáÜÁõ∏‰∏ÄËá¥ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÂá∏ÊòæÔºåÂπ∂Ëé∑Âæó‰∫ÜËÆ§ÂèØ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂèØËß£ÈáäÊÄßÂ∑≤Êàê‰∏∫ÂΩ±ÂìçÁ≥ªÁªüË¥®ÈáèÁöÑÈáçË¶ÅÈùûÂäüËÉΩÊÄßÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáäÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÊåëÊàò‰∫ÜÂèØËß£ÈáäÊÄßÁöÑÊ≠£Èù¢ÂΩ±Âìç„ÄÇÂ¶ÇÊûúÊª°Ë∂≥ÂèØËß£ÈáäÊÄßÁöÑË¶ÅÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁªüÊÄßËÉΩÔºåÈÇ£‰πàÂøÖÈ°ª‰ªîÁªÜËÄÉËôëËøô‰∫õË¥®ÈáèÊñπÈù¢‰∏≠ÁöÑÂì™‰∏Ä‰∏™‰ºòÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉ‰ª¨‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊâπÂà§ÊÄßÂú∞ËÄÉÂØü‰∫ÜÊâÄË∞ìÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÊúÄÂ•Ω‰ª•‰∏ÄÁßçÁªÜËá¥ÂÖ•ÂæÆÁöÑÊñπÂºèÊù•Â§ÑÁêÜÂÆÉÔºåËøôÁßçÊñπÂºèÁªìÂêà‰∫ÜËµÑÊ∫êÂèØÁî®ÊÄß„ÄÅÈ¢ÜÂüüÁâπÂæÅÂíåÈ£éÈô©ËÄÉËôë„ÄÇÈÄöËøá‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂÆûË∑µÊèê‰æõÂü∫Á°ÄÔºåËøôÈ°πÂ∑•‰ΩúÊó®Âú®Êé®Ëøõ‰∫∫Â∑•Êô∫ËÉΩÁöÑ RE È¢ÜÂüü„ÄÇ

##### **Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**
2308.02047v1 by Henry Fraser, Jose-Miguel Bello y Villarino

This paper critically evaluates the European Commission's proposed AI Act's
approach to risk management and risk acceptability for high-risk AI systems
that pose risks to fundamental rights and safety. The Act aims to promote
"trustworthy" AI with a proportionate regulatory burden. Its provisions on risk
acceptability require residual risks from high-risk systems to be reduced or
eliminated "as far as possible", having regard to the "state of the art". This
criterion, especially if interpreted narrowly, is unworkable and promotes
neither proportionate regulatory burden, nor trustworthiness. By contrast the
Parliament's most recent draft amendments to the risk management provisions
introduce "reasonableness", cost-benefit analysis, and are more transparent
about the value-laden and contextual nature of risk acceptability judgements.
This paper argues that the Parliament's approach is more workable, and better
balances the goals of proportionality and trustworthiness. It explains what
reasonableness in risk acceptability judgments would entail, drawing on
principles from negligence law and European medical devices regulation. And it
contends that the approach to risk acceptability judgments need a firm
foundation of civic legitimacy: including detailed guidance or involvement from
regulators, and meaningful input from affected stakeholders.

ÊëòË¶ÅÔºöÊú¨ÊñáÂö¥Ê†ºË©ï‰º∞Ê≠êÊ¥≤ÂßîÂì°ÊúÉÊèêÂá∫ÁöÑ AI Ê≥ïÊ°àÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÂíåÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂü∫Êú¨Ê¨äÂà©ÂíåÂÆâÂÖ®ÊßãÊàêÈ¢®Èö™ÁöÑÈ´òÈ¢®Èö™ AI Á≥ªÁµ±„ÄÇË©≤Ê≥ïÊ°àÊó®Âú®‰ª•Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†Êìî‰øÉÈÄ≤„ÄåÂÄºÂæó‰ø°Ë≥¥„ÄçÁöÑ AI„ÄÇÂÖ∂ÈóúÊñºÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊ¢ùÊ¨æË¶ÅÊ±ÇÂ∞áÈ´òÈ¢®Èö™Á≥ªÁµ±ÁöÑÊÆòÈ§òÈ¢®Èö™Ê∏õ‰ΩéÊàñÊ∂àÈô§„ÄåÁõ°ÂèØËÉΩ„ÄçÔºå‰∏¶ËÄÉÊÖÆ„ÄåÊäÄË°ìÁãÄÊÖã„Äç„ÄÇÊ≠§Ê∫ñÂâáÔºåÁâπÂà•ÊòØÂ¶ÇÊûúÁãπÁæ©Ëß£ÈáãÔºåÁÑ°Ê≥ïÂü∑Ë°åÔºåÊó¢‰∏ç‰øÉÈÄ≤Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†ÊìîÔºå‰πü‰∏ç‰øÉÈÄ≤ÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåË≠∞ÊúÉÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÊ¢ùÊ¨æÁöÑÊúÄÊñ∞‰øÆÊ≠£ËçâÊ°àÂºïÂÖ•‰∫Ü„ÄåÂêàÁêÜÊÄß„Äç„ÄÅÊàêÊú¨ÊïàÁõäÂàÜÊûêÔºå‰∏¶‰∏îÊõ¥ÈÄèÊòéÂú∞Ë™™Êòé‰∫ÜÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÂÉπÂÄºËßÄÂíåËÉåÊôØÊÄßË≥™„ÄÇÊú¨ÊñáË´ñË≠âË≠∞ÊúÉÁöÑÊñπÊ≥ïÊõ¥ÂèØË°åÔºå‰∏îËÉΩÊõ¥Â•ΩÂú∞Âπ≥Ë°°Áõ∏Á®±ÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÁöÑÁõÆÊ®ô„ÄÇÊú¨ÊñáË™™ÊòéÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑‰∏≠ÁöÑÂêàÁêÜÊÄßÊúÉÂ∏∂‰æÜ‰ªÄÈ∫ºÔºå‰∏¶Ê†πÊìöÈÅéÂ§±Ê≥ïÂíåÊ≠êÊ¥≤ÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶è‰∏≠ÁöÑÂéüÂâáÈÄ≤Ë°åË™™Êòé„ÄÇÊú¨Êñá‰∏ªÂºµÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÊñπÊ≥ïÈúÄË¶ÅÁ©©Âõ∫ÁöÑÂÖ¨Ê∞ëÂêàÊ≥ïÊÄßÂü∫Á§éÔºöÂåÖÊã¨Áõ£ÁÆ°Ê©üÊßãÁöÑË©≥Á¥∞ÊåáÂ∞éÊàñÂèÉËàáÔºå‰ª•ÂèäÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÊúâÊÑèÁæ©ÊäïÂÖ•„ÄÇ

##### **eXplainable Artificial Intelligence (XAI) in aging clock models**
2307.13704v3 by Alena Kalyakulina, Igor Yusipov, Alexey Moskalev, Claudio Franceschi, Mikhail Ivanchenko

eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the "aging clocks" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÊ©üÂô®Â≠∏Áøí‰∏≠Âø´ÈÄüÈÄ≤Â±ïÁöÑÈ†òÂüüÔºåÊó®Âú®Ëß£ÈñãË§áÈõúÊ®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇXAI Âú®ÊïèÊÑüÊáâÁî®‰∏≠ÁâπÂà•ÈúÄË¶ÅÔºå‰æãÂ¶ÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåÁï∂Ë®∫Êñ∑„ÄÅÂª∫Ë≠∞ÂíåÊ≤ªÁôÇÈÅ∏ÊìáÂèØËÉΩ‰æùË≥¥Êñº‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÂÅöÂá∫ÁöÑÊ±∫Á≠ñÊôÇ„ÄÇ‰∫∫Â∑•Êô∫ÊÖßÊñπÊ≥ï‰πüÂ∑≤Âª£Ê≥õÁî®ÊñºËÄÅÂåñÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÂú®ÈñãÁôºÁîüÁâ©ÊôÇÈêòÊ®°ÂûãÂíåË≠òÂà•ËÄÅÂåñÂíåËàáÂπ¥ÈΩ°Áõ∏ÈóúÁñæÁóÖÁöÑÁîüÁâ©Ê®ôË™åÁâ©ÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåÈÄôË£° XAI ÁöÑÊΩõÂäõÊúâÂæÖÂÖÖÂàÜË™çË≠ò„ÄÇÊàëÂÄëË®éË´ñ‰∫Ü XAI Âú®ÈñãÁôº„ÄåËÄÅÂåñÊôÇÈêò„ÄçÊñπÈù¢ÁöÑÊáâÁî®Ôºå‰∏¶Â∞çÊåâÁâπÂÆöÁîüÁêÜÁ≥ªÁµ±ÁöÑÈáçÈªûÂàÜÈ°ûÁöÑÊñáÁçªÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇ

##### **Interpreting and Correcting Medical Image Classification with PIP-Net**
2307.10404v2 by Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, J√∂rg Schl√∂tterer, Maurice van Keulen, Christin Seifert

Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.

ÊëòË¶ÅÔºöÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÊòØÂèØËß£ÈáãË®≠Ë®àÁöÑÂΩ±ÂÉèÂàÜÈ°ûÂô®Ôºå‰πüÊòØÈªëÁÆ± AI ÁöÑ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫ÜËß£ÈáãÊÄßÊ©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØ PIP-NetÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñô‰∏äËá™ÂãïÂåñË®∫Êñ∑ÊîØÊè¥ÁöÑÈÅ©Áî®ÊÄßÂíåÊΩõÂäõ„ÄÇPIP-Net Â≠∏Áøí‰∫∫È°ûÂèØÁêÜËß£ÁöÑÂéüÂûãÂΩ±ÂÉèÈÉ®ÂàÜÔºåÊàëÂÄëË©ï‰º∞ÂÖ∂Âú®È™®ÊäòÊ™¢Ê∏¨ÂíåÁöÆËÜöÁôåË®∫Êñ∑ÊñπÈù¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÁôºÁèæ PIP-Net ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãÁ¨¶ÂêàÈÜ´Â≠∏ÂàÜÈ°ûÊ®ôÊ∫ñÔºåÂêåÊôÇÂÉÖÊèê‰æõÂΩ±ÂÉèÂ±§Á¥öÈ°ûÂà•Ê®ôÁ±§„ÄÇÁî±Êñº PIP-Net Â∞çÂéüÂûãÁöÑÁÑ°Áõ£Áù£È†êË®ìÁ∑¥ÔºåÂõ†Ê≠§ÂèØ‰ª•ËºïÈ¨ÜË≠òÂà•Ë≥áÊñôÂìÅË≥™ÂïèÈ°åÔºå‰æãÂ¶Ç X ÂÖâ‰∏≠ÁöÑ‰∏çÈúÄË¶ÅÊñáÂ≠óÊàñÊ®ôÁ±§ÈåØË™§„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ¶ñÊ¨°Â±ïÁ§∫‰∫∫È°ûÂèØ‰ª•ÈÄèÈÅéÁõ¥Êé•ÂÅúÁî®‰∏çÈúÄË¶ÅÁöÑÂéüÂûã‰æÜÊâãÂãï‰øÆÊ≠£ PIP-Net ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÁî±ÊñºÂÖ∂ÂèØËß£ÈáãÊÄßÂíåÈÄ≤ÈöéÊ®°ÂûãÈô§ÈåØÁöÑÊΩõÂäõÔºåÂõ†Ê≠§ÊúâÊúõÊáâÁî®ÊñºÈÜ´ÁôÇ„ÄÇ

##### **Explaining and visualizing black-box models through counterfactual paths**
2307.07764v3 by Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek

Explainable AI (XAI) is an increasingly important area of machine learning
research, which aims to make black-box models transparent and interpretable. In
this paper, we propose a novel approach to XAI that uses the so-called
counterfactual paths generated by conditional permutations of features. The
algorithm measures feature importance by identifying sequential permutations of
features that most influence changes in model predictions. It is particularly
suitable for generating explanations based on counterfactual paths in knowledge
graphs incorporating domain knowledge. Counterfactual paths introduce an
additional graph dimension to current XAI methods in both explaining and
visualizing black-box models. Experiments with synthetic and medical data
demonstrate the practical applicability of our approach.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI (XAI) ÊòØÊ©üÂô®Â≠∏ÁøíÁ†îÁ©∂‰∏≠Êó•ÁõäÈáçË¶ÅÁöÑÈ†òÂüüÔºåÂÖ∂ÁõÆÊ®ôÊòØËÆìÈªëÁÆ±Ê®°ÂûãÈÄèÊòé‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑ XAI ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Áî±ÁâπÂæµÊ¢ù‰ª∂ÁΩÆÊèõÁî¢ÁîüÁöÑÊâÄË¨ÇÂèç‰∫ãÂØ¶Ë∑ØÂæë„ÄÇË©≤ÊºîÁÆóÊ≥ïÈÄèÈÅéË≠òÂà•ÁâπÂæµÁöÑÈ†ÜÂ∫èÁΩÆÊèõ‰æÜË°°ÈáèÁâπÂæµÈáçË¶ÅÊÄßÔºåÈÄô‰∫õÁΩÆÊèõÊúÄËÉΩÂΩ±ÈüøÊ®°ÂûãÈ†êÊ∏¨ÁöÑËÆäÂåñ„ÄÇÂÆÉÁâπÂà•ÈÅ©ÂêàÊ†πÊìöÂåÖÂê´È†òÂüüÁü•Ë≠òÁöÑÁü•Ë≠òÂúñË≠ú‰∏≠ÁöÑÂèç‰∫ãÂØ¶Ë∑ØÂæë‰æÜÁî¢ÁîüËß£Èáã„ÄÇÂèç‰∫ãÂØ¶Ë∑ØÂæëÂú®Ëß£ÈáãÂíåË¶ñË¶∫ÂåñÈªëÁÆ±Ê®°ÂûãÊôÇÔºåÁÇ∫ÁõÆÂâçÁöÑ XAI ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÈ°çÂ§ñÁöÑÂúñÂΩ¢Á∂≠Â∫¶„ÄÇ‰ΩøÁî®ÂêàÊàêÂíåÈÜ´ÁôÇË≥áÊñôÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂØ¶Áî®ÈÅ©Áî®ÊÄß„ÄÇ

##### **Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**
2307.02131v5 by Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci

The field of explainability in artificial intelligence (AI) has witnessed a
growing number of studies and increasing scholarly interest. However, the lack
of human-friendly and individual interpretations in explaining the outcomes of
machine learning algorithms has significantly hindered the acceptance of these
methods by clinicians in their research and clinical practice. To address this
issue, our study uses counterfactual explanations to explore the applicability
of "what if?" scenarios in medical research. Our aim is to expand our
understanding of magnetic resonance imaging (MRI) features used for diagnosing
pediatric posterior fossa brain tumors beyond existing boundaries. In our case
study, the proposed concept provides a novel way to examine alternative
decision-making scenarios that offer personalized and context-specific
insights, enabling the validation of predictions and clarification of
variations under diverse circumstances. Additionally, we explore the potential
use of counterfactuals for data augmentation and evaluate their feasibility as
an alternative approach in our medical research case. The results demonstrate
the promising potential of using counterfactual explanations to enhance
acceptance of AI-driven methods in clinical research.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØËß£ÈáãÊÄßÈ†òÂüü‰∏≠ÔºåÂ∑≤Á∂ìÁúãÂà∞Ë∂ä‰æÜË∂äÂ§öÁöÑÁ†îÁ©∂ÂíåÂ≠∏Ë°ìËààË∂£„ÄÇÁÑ∂ËÄåÔºåÂú®Ëß£ÈáãÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÁµêÊûúÊôÇÁº∫‰πè‰∫∫ÊÄßÂåñÂíåÂÄã‰∫∫ÂåñÁöÑË©ÆÈáãÔºåÈÄôÈ°ØËëóÈòªÁ§ô‰∫ÜËá®Â∫äÈÜ´ÁîüÂú®Á†îÁ©∂ÂíåËá®Â∫äÂØ¶Âãô‰∏≠Êé•ÂèóÈÄô‰∫õÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁöÑÁ†îÁ©∂‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÊé¢Ë®é„ÄåÂ¶ÇÊûúÔºü„ÄçÊÉÖÂ¢ÉÂú®ÈÜ´Â≠∏Á†îÁ©∂‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÊì¥Â±ïÊàëÂÄëÂ∞çÁî®ÊñºË®∫Êñ∑Â∞èÂÖíÂæåÈ°±Á™©ËÖ¶ËÖ´Áò§ÁöÑÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÁâπÂæµÁöÑÁêÜËß£ÔºåË∂ÖË∂äÁèæÊúâÁöÑÁïåÁ∑ö„ÄÇÂú®ÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊâÄÊèêÂá∫ÁöÑÊ¶ÇÂøµÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï‰æÜÊ™¢Ë¶ñÊõø‰ª£Ê±∫Á≠ñÊÉÖÂ¢ÉÔºåÊèê‰æõÂÄã‰∫∫ÂåñÂíåÁâπÂÆöÊñºÊÉÖÂ¢ÉÁöÑË¶ãËß£ÔºåÂæûËÄåËÉΩÂ§†È©óË≠âÈ†êÊ∏¨‰∏¶ÈáêÊ∏ÖÂú®‰∏çÂêåÊÉÖÊ≥Å‰∏ãÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂèç‰∫ãÂØ¶Áî®ÊñºË≥áÊñôÊì¥ÂÖÖÁöÑÊΩõÂú®Áî®ÈÄîÔºå‰∏¶Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫ÊàëÂÄëÈÜ´Â≠∏Á†îÁ©∂Ê°à‰æã‰∏≠Êõø‰ª£ÊñπÊ≥ïÁöÑÂèØË°åÊÄß„ÄÇÁµêÊûúË≠âÊòé‰∫Ü‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÂ¢ûÂº∑Ëá®Â∫äÁ†îÁ©∂‰∏≠ AI È©ÖÂãïÊñπÊ≥ïÁöÑÊé•ÂèóÂ∫¶ÁöÑÊΩõÂäõ„ÄÇ

##### **AI and Non AI Assessments for Dementia**
2307.01210v1 by Mahboobeh Parsapoor, Hamed Ghodrati, Vincenzo Dentamaro, Christopher R. Madan, Ioulietta Lazarou, Spiros Nikolopoulos, Ioannis Kompatsiaris

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

ÊëòË¶ÅÔºöÁõÆÂâç‰∫∫Â∑•Êô∫ËÉΩÈ†òÂüüÁöÑÈÄ≤Â±ïÂ∞éËá¥‰∫ÜÂêÑÁ®ÆÈ°ûÂûãÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈ©ÖÂãïÁöÑÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÁôºÂ±ïÔºåÂèØÁî®ÊñºË≠òÂà•ËôïÊñºÂ§±Êô∫ÁóáÊó©ÊúüÈöéÊÆµÁöÑÊÇ£ËÄÖ„ÄÇÂÆÉÂèØ‰ª•ÂæπÂ∫ïÊîπËÆäÂ§±Êô∫ÁóáË≠∑ÁêÜË®≠ÁΩÆ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÈÜ´ÁôÇÁïåË¶Å‰∫ÜËß£ÂêÑÁ®Æ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞Ôºå‰∏¶Ê†πÊìöÂÖ∂ÊúâÊïàÊÄß„ÄÅÊïàÁéá„ÄÅÂØ¶Áî®ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁ®ãÂ∫¶ÔºåËÄÉÊÖÆÈÅ∏ÊìáÂÆÉÂÄë‰æÜÊó©ÊúüË≠òÂà•Â§±Êô∫ÁóáÊÇ£ËÄÖ (PwD)„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰∫∫Â∑•Êô∫ËÉΩÈñãÁôº‰∫∫Âì°‰πüÊáâË©≤‰∫ÜËß£ÂêÑÁ®ÆÈùû‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞‰ª•ÂèäÊúÄËøëÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞„ÄÇÂõ†Ê≠§ÔºåÈÄôÁØáËá®Â∫äÈÜ´ÁîüÂíå‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´ÈÉΩÂèØ‰ª•Èñ±ËÆÄÁöÑË´ñÊñáÂ°´Ë£ú‰∫ÜÊñáÁçª‰∏≠ÈóúÊñºÂêëËá®Â∫äÈÜ´ÁîüËß£ÈáãÁèæÊúâÂ§±Êô∫ÁóáË≠òÂà•Ëß£Ê±∫ÊñπÊ°à‰ª•ÂèäÂêë‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´Ëß£ÈáãÊâÄÁî®ÊäÄË°ìÂíåÊúÄÂª£Ê≥õÁöÑÂ§±Êô∫ÁóáÊï∏ÊìöÈõÜÁöÑÁ©∫ÁôΩ„ÄÇÂÆÉÈÅµÂæ™Â∞ç‰∫∫Â∑•Êô∫ËÉΩÂíåÈùû‰∫∫Â∑•Êô∫ËÉΩÂ§±Êô∫ÁóáË©ï‰º∞Ë´ñÊñáÁöÑÂõûÈ°ßÔºåÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÂíåÈÜ´ÁôÇÁïåÊèê‰æõÊúâÈóúÂêÑÁ®ÆÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÂØ∂Ë≤¥‰ø°ÊÅØ„ÄÇË®éË´ñÂíåÁµêË´ñÈáçÈªû‰ªãÁ¥π‰∫ÜÊúÄÁ™ÅÂá∫ÁöÑÁ†îÁ©∂ÊñπÂêëÂíåÁèæÊúâËß£Ê±∫ÊñπÊ°àÁöÑÊàêÁÜüÂ∫¶„ÄÇ

##### **Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**
2306.07306v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Explainability poses a major challenge to artificial intelligence (AI)
techniques. Current studies on explainable AI (XAI) lack the efficiency of
extracting global knowledge about the learning task, thus suffer deficiencies
such as imprecise saliency, context-aware absence and vague meaning. In this
paper, we propose the class association embedding (CAE) approach to address
these issues. We employ an encoder-decoder architecture to embed sample
features and separate them into class-related and individual-related style
vectors simultaneously. Recombining the individual-style code of a given sample
with the class-style code of another leads to a synthetic sample with preserved
individual characters but changed class assignment, following a cyclic
adversarial learning strategy. Class association embedding distills the global
class-related features of all instances into a unified domain with well
separation between classes. The transition rules between different classes can
be then extracted and further employed to individual instances. We then propose
an active XAI framework which manipulates the class-style vector of a certain
sample along guided paths towards the counter-classes, resulting in a series of
counter-example synthetic samples with identical individual characters.
Comparing these counterfactual samples with the original ones provides a
global, intuitive illustration to the nature of the classification tasks. We
adopt the framework on medical image classification tasks, which show that more
precise saliency maps with powerful context-aware representation can be
achieved compared with existing methods. Moreover, the disease pathology can be
directly visualized via traversing the paths in the class-style space.

ÊëòË¶ÅÔºö<paragraph>ÂèØËß£ÈáãÊÄßÂ∞ç‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊßãÊàê‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÁï∂ÂâçÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂Áº∫‰πèÊèêÂèñÂ≠∏Áøí‰ªªÂãôÊï¥È´îÁü•Ë≠òÁöÑÊïàÁéáÔºåÂõ†Ê≠§Â≠òÂú®‰∏çÁ≤æÁ¢∫ÁöÑÈ°ØËëóÊÄß„ÄÅËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑÁº∫Â§±ÂíåÂê´Á≥äÊÑèÁæ©Á≠âÁº∫Èô∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫È°ûÂà•ÈóúËÅØÂµåÂÖ• (CAE) ÊñπÊ≥ï‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÊé°Áî®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂Êßã‰æÜÂµåÂÖ•Ê®£Êú¨ÁâπÂæµÔºå‰∏¶ÂêåÊôÇÂ∞áÂÆÉÂÄëÂàÜÁÇ∫È°ûÂà•Áõ∏ÈóúÂíåÂÄãÈ´îÁõ∏ÈóúÁöÑÊ®£ÂºèÂêëÈáè„ÄÇÂ∞áÁµ¶ÂÆöÊ®£Êú¨ÁöÑÂÄãÈ´îÊ®£Âºè‰ª£Á¢ºËàáÂè¶‰∏ÄÂÄãÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£Âºè‰ª£Á¢ºÈáçÊñ∞ÁµÑÂêàÔºåÊúÉÁî¢Áîü‰∏ÄÂÄãÂÖ∑Êúâ‰øùÁïôÂÄãÈ´îÁâπÂæµ‰ΩÜÊîπËÆäÈ°ûÂà•ÂàÜÈÖçÁöÑÂêàÊàêÊ®£Êú¨ÔºåÈÅµÂæ™Âæ™Áí∞Â∞çÊäóÂ≠∏ÁøíÁ≠ñÁï•„ÄÇÈ°ûÂà•ÈóúËÅØÂµåÂÖ•Â∞áÊâÄÊúâÂØ¶‰æãÁöÑÂÖ®Â±ÄÈ°ûÂà•Áõ∏ÈóúÁâπÂæµÊèêÁÖâÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÈ†òÂüü‰∏≠Ôºå‰∏¶Âú®È°ûÂà•‰πãÈñìÊúâËâØÂ•ΩÁöÑÂçÄÂàÜ„ÄÇÁÑ∂ÂæåÂèØ‰ª•ÊèêÂèñ‰∏çÂêåÈ°ûÂà•‰πãÈñìÁöÑËΩâÊèõË¶èÂâáÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊáâÁî®ÊñºÂÄãÂà•ÂØ¶‰æã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏ªÂãï XAI Ê°ÜÊû∂ÔºåÂÆÉÊ≤øËëóÂºïÂ∞éË∑ØÂæëÊìç‰ΩúÁâπÂÆöÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£ÂºèÂêëÈáèÔºåÊúùËëóÂèçÈ°ûÂà•ÁßªÂãïÔºåÂæûËÄåÁî¢Áîü‰∏ÄÁ≥ªÂàóÂÖ∑ÊúâÁõ∏ÂêåÂÄãÈ´îÁâπÂæµÁöÑÂèç‰æãÂêàÊàêÊ®£Êú¨„ÄÇÂ∞áÈÄô‰∫õÂèç‰∫ãÂØ¶Ê®£Êú¨ËàáÂéüÂßãÊ®£Êú¨ÈÄ≤Ë°åÊØîËºÉÔºåÂèØ‰ª•Â∞çÂàÜÈ°û‰ªªÂãôÁöÑÊÄßË≥™Êèê‰æõÂÖ®Â±Ä„ÄÅÁõ¥ËßÄÁöÑË™™Êòé„ÄÇÊàëÂÄëÊé°Áî®Ë©≤Ê°ÜÊû∂ÈÄ≤Ë°åÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÔºåÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÂèØ‰ª•Áç≤ÂæóÊõ¥Á≤æÁ¢∫ÁöÑÈ°ØËëóÊÄßÂúñÔºå‰∏¶ÂÖ∑ÊúâÂº∑Â§ßÁöÑËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÁñæÁóÖÁóÖÁêÜÂ≠∏ÂèØ‰ª•Áõ¥Êé•ÈÄöÈÅéÂú®È°ûÂà•Ê®£ÂºèÁ©∫Èñì‰∏≠ÈÅçÊ≠∑Ë∑ØÂæë‰æÜÈÄ≤Ë°åÂèØË¶ñÂåñ„ÄÇ</paragraph>

##### **HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**
2306.06029v1 by Rodrigo Agerri, I√±igo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova

Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.

ÊëòË¶ÅÔºöÊèê‰æõÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑ AI È†êÊ∏¨ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÂíåË§áÈõúÊÄßÁöÑ‰ªªÂãô„ÄÇË¶ÅÈ†ÜÂà©ÈÄ≤Ë°åÔºåÂÆÉÈúÄË¶ÅÂÖ∑ÂÇô‰∏ãÂàóÂõ†Á¥†ÔºöÈÅ∏ÊìáÈÅ©Áï∂ÁöÑË™™ÊòéÊôÆÈÅçÊÄß/ÁâπÊÆäÊÄßÂ±§Á¥öÔºõËÄÉÈáèË™™ÊòéÂèóÁõä‰∫∫Â∞çÊâÄËÄÉÊÖÆÁöÑ AI ‰ªªÂãôÁöÑÁÜüÊÇâÁ®ãÂ∫¶ÂÅáË®≠ÔºõÂèÉÁÖß‰øÉÊàêÊ±∫Á≠ñÁöÑÁâπÂÆöÂÖÉÁ¥†ÔºõÂà©Áî®ÂèØËÉΩ‰∏çÂ±¨ÊñºÈ†êÊ∏¨Á®ãÂ∫èÁöÑ‰∏ÄÈÉ®ÂàÜÁöÑÈ°çÂ§ñÁü•Ë≠òÔºà‰æãÂ¶ÇÂ∞àÂÆ∂Ë≠âÊìöÔºâÔºõ‰∏¶Êèê‰æõÊîØÊåÅÂê¶ÂÆöÂÅáË®≠ÁöÑË≠âÊìö„ÄÇÊúÄÂæåÔºåÁ≥ªÁµ±ÈúÄË¶Å‰ª•Ê∏ÖÊô∞ÂèØËß£Èáã‰∏îÂèØËÉΩ‰ª§‰∫∫‰ø°ÊúçÁöÑÊñπÂºèÂà∂ÂÆöË™™Êòé„ÄÇÂü∫ÊñºÈÄô‰∫õËÄÉÈáèÔºåANTIDOTE ‰øÉÊàê‰∫ÜÂèØËß£Èáã AI ÁöÑÊï¥ÂêàÈ°òÊôØÔºåÂÖ∂‰∏≠Ê∑±Â∫¶Â≠∏ÁøíÁ®ãÂ∫èÁöÑ‰ΩéÈöéÁâπÂæµËàá‰∫∫È°ûË´ñË≠âËÉΩÂäõÁöÑÈ´òÈöéÊû∂ÊßãÁõ∏ÁµêÂêà„ÄÇANTIDOTE Â∞áÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíËàáË´ñË≠âÁöÑË∑®È†òÂüüËÉΩÂäõÔºå‰æÜÊîØÊåÅÂèØËß£Èáã AI Êõ¥Âª£Ê≥õ‰∏îÂâµÊñ∞ÁöÑËßÄÈªûÔºåÂÖ∂‰∏≠Â∞çËá®Â∫äÊ°à‰æãÂØ©Ë≠∞ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÈúÄÊ±ÇËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩúÁÇ∫Ë©≤Â∞àÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÊàêÊûúÔºåÊàëÂÄëÁôºÂ∏É‰∫Ü Antidote CasiMedicos Ë≥áÊñôÈõÜÔºå‰ª•Âà©Êñº‰∏ÄËà¨ÂèØËß£Èáã AI ÁöÑÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÈÜ´ÁôÇÈ†òÂüüÁöÑË´ñË≠â„ÄÇ

##### **XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**
2306.04791v1 by Eli Laird, Ayesh Madushanka, Elfi Kraka, Corey Clark

Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºåÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÈÄ≤Â±ïËøÖÈÄüÔºåÂú®Ëó•Áâ©ÁôºÁèæ„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåÊé®Ëñ¶Á≥ªÁµ±ÊñπÈù¢ÈÉΩÊúâË®±Â§öÊñ∞ÁôºÂ±ï„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈÄ≤Â±ïÂæàÈáçË¶ÅÔºå‰ΩÜË®±Â§öÁ∂≤Ë∑ØÈÉΩÊòØ„ÄåÈªëÁõíÂ≠ê„ÄçÔºåÂ∞çÊñºÁ∂≤Ë∑ØÂà∞Â∫ïÂú®Â≠∏Áøí„Äå‰ªÄÈ∫º„Äç‰∫ÜËß£ÁîöÂ∞ë„ÄÇË®±Â§öÈ´òÈ¢®Èö™ÊáâÁî®Ôºå‰æãÂ¶ÇËó•Áâ©ÁôºÁèæÔºåÈúÄË¶ÅÊ®°ÂûãÊèê‰æõ‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑËß£ÈáãÔºå‰ª•‰æø‰ΩøÁî®ËÄÖÂèØ‰ª•Ëæ®Ë≠òÈåØË™§‰∏¶ÁôºÁèæÊñ∞Áü•Ë≠ò„ÄÇÂõ†Ê≠§ÔºåÂèØËß£Èáã AI ÊºîÁÆóÊ≥ïÁöÑÈñãÁôºÂ∞çÊñºÊàëÂÄëÁç≤Âèñ AI ÁöÑÂ•ΩËôïËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ eXplainable Insight (XInsight) ÁöÑ GNN ÂèØËß£ÈáãÊÄßÊºîÁÆóÊ≥ïÔºåÂÆÉ‰ΩøÁî® GFlowNets Áî¢ÁîüÊ®°ÂûãËß£ÈáãÂàÜ‰Ωà„ÄÇÁî±Êñº GFlowNets ÊúÉÁî¢ÁîüÊ©üÁéáËàáÁçéÂãµÊàêÊ≠£ÊØîÁöÑÁâ©‰ª∂ÔºåÂõ†Ê≠§ËàáÂÖàÂâçÂÉÖÂ≠∏ÁøíÊúÄÂ§ßÁçéÂãµÁØÑ‰æãÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåXInsight ÂèØ‰ª•Áî¢ÁîüÂ§öÊ®£ÂåñÁöÑËß£ÈáãÈõÜÂêà„ÄÇÊàëÂÄëÈÄèÈÅéÁÇ∫Âú®ÂÖ©ÂÄãÂúñÂΩ¢ÂàÜÈ°û‰ªªÂãô‰∏≠Ë®ìÁ∑¥ÁöÑ GNN Áî¢ÁîüËß£Èáã‰æÜÂ±ïÁ§∫ XInsightÔºö‰ΩøÁî® MUTAG Ë≥áÊñôÈõÜÂ∞çËá¥Á™ÅËÆäÂåñÂêàÁâ©ÈÄ≤Ë°åÂàÜÈ°ûÔºå‰∏¶‰ΩøÁî®ÊàëÂÄëÂ∑≤ÈñãÊîæÂéüÂßãÁ¢ºÁöÑÂêàÊàêË≥áÊñôÈõÜÂ∞çÈùûÁí∞ÁãÄÂúñÂΩ¢ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî® QSAR Âª∫Ê®°ÂàÜÊûêÁî¢ÁîüÁöÑÂåñÂêàÁâ©‰æÜÂ±ïÁ§∫ XInsight Ëß£ÈáãÁöÑÊïàÁî®ÔºåÊàëÂÄëÁôºÁèæ XInsight ÊúÉÁî¢ÁîüÊåâË¶™ËÑÇÊÄßÔºàÂ∑≤Áü•ÁöÑËá¥Á™ÅËÆäÁõ∏ÈóúÊÄßÔºâÂàÜÁæ§ÁöÑÂåñÂêàÁâ©„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ XInsight ÊúÉÁî¢Áîü‰∏ÄÂÄãËß£ÈáãÂàÜ‰ΩàÔºåÊè≠Á§∫Ê®°ÂûãÊâÄÂ±ïÁ§∫ÁöÑÂ∫ïÂ±§Èóú‰øÇ„ÄÇÂÆÉÂÄë‰πüÂº∑Ë™øÁî¢ÁîüÂ§öÊ®£ÂåñËß£ÈáãÈõÜÂêàÁöÑÈáçË¶ÅÊÄßÔºåÂõ†ÁÇ∫ÂÆÉ‰ΩøÊàëÂÄëËÉΩÂ§†ÁôºÁèæÊ®°Âûã‰∏≠ÁöÑÈö±ËóèÈóú‰øÇÔºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÂàÜÊûêÊèê‰æõÊúâÂÉπÂÄºÁöÑÊåáÂ∞é„ÄÇ</paragraph>

