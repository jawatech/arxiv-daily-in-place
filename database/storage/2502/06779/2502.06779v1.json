{"2502.06779": {"publish_time": "2025-02-10", "title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification", "paper_summary": "Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST.", "paper_summary_zh": "\u5fae\u8abf\u9810\u5148\u8a13\u7df4\u7684\u8996\u89ba\u6a21\u578b\u4ee5\u57f7\u884c\u7279\u5b9a\u4efb\u52d9\u662f\u96fb\u8166\u8996\u89ba\u4e2d\u7684\u5e38\u898b\u505a\u6cd5\u3002\u7136\u800c\uff0c\u96a8\u8457\u6a21\u578b\u898f\u6a21\u8d8a\u4f86\u8d8a\u5927\uff0c\u9019\u500b\u904e\u7a0b\u6703\u8b8a\u5f97\u66f4\u52a0\u6602\u8cb4\u3002\u6700\u8fd1\uff0c\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u6d6e\u73fe\uff0c\u6210\u70ba\u6539\u5584\u8a13\u7df4\u6548\u7387\u548c\u6e1b\u5c11\u5132\u5b58\u9700\u6c42\u7684\u71b1\u9580\u89e3\u6c7a\u65b9\u6848\uff0c\u65b9\u6cd5\u662f\u5728\u9810\u5148\u8a13\u7df4\u7684\u4e3b\u5e79\u4e2d\u8abf\u6574\u984d\u5916\u7684\u4f4e\u968e\u6a21\u7d44\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u512a\u9ede\uff0c\u4f46\u5b83\u5011\u4ecd\u53d7\u9650\u65bc\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u800c\u4e14\u8207\u9810\u5148\u8a13\u7df4\u7684\u4e2d\u9593\u7279\u5fb5\u4e0d\u4e00\u81f4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u91dd\u5c0d\u5404\u7a2e\u8fa8\u8b58\u4efb\u52d9\u5f15\u9032\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u591a\u6838 Kronecker \u9069\u61c9\uff0c\u4e26\u5177\u5099\u91cd\u65b0\u8abf\u6574\u6bd4\u4f8b\u50b3\u8f38 (KARST)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5176\u591a\u6838\u8a2d\u8a08\u6a6b\u5411\u5ef6\u4f38 Kronecker \u6295\u5f71\uff0c\u4e26\u5c07\u9069\u61c9\u77e9\u9663\u5206\u9694\u6210\u591a\u500b\u4e92\u88dc\u7a7a\u9593\uff0c\u6e1b\u5c11\u53c3\u6578\u4f9d\u8cf4\u6027\u4e26\u5efa\u7acb\u66f4\u7dca\u6e4a\u7684\u5b50\u7a7a\u9593\u3002\u6b64\u5916\uff0c\u5b83\u7d0d\u5165\u4e86\u984d\u5916\u7684\u53ef\u5b78\u7fd2\u91cd\u65b0\u8abf\u6574\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u66f4\u597d\u5730\u8207\u9810\u5148\u8a13\u7df4\u7684\u7279\u5fb5\u5206\u4f48\u5c0d\u9f4a\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u9748\u6d3b\u4e14\u5e73\u8861\u7684\u7279\u5fb5\u805a\u5408\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684 KARST \u512a\u65bc\u5176\u4ed6 PEFT \u5c0d\u61c9\u65b9\u6cd5\uff0c\u800c\u63a8\u8ad6\u6210\u672c\u537b\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\uff0c\u56e0\u70ba\u5b83\u5177\u5099\u91cd\u65b0\u53c3\u6578\u5316\u7684\u7279\u6027\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc\uff1ahttps://github.com/Lucenova/KARST\u3002", "author": "Yue Zhu et.al.", "authors": "Yue Zhu, Haiwen Diao, Shang Gao, Long Chen, Huchuan Lu", "id": "2502.06779v1", "paper_url": "http://arxiv.org/abs/2502.06779v1", "repo": "null"}}