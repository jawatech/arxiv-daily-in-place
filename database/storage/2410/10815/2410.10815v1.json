{"2410.10815": {"publish_time": "2024-10-14", "title": "Depth Any Video with Scalable Synthetic Data", "paper_summary": "Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse synthetic\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency.", "paper_summary_zh": "\u5f71\u7247\u6df1\u5ea6\u4f30\u8a08\u9577\u4e45\u4ee5\u4f86\u53d7\u9650\u65bc\u4e00\u81f4\u4e14\u53ef\u64f4\u5145\u7684\u771f\u5be6\u6578\u64da\u7684\u7a00\u5c11\u6027\uff0c\u5c0e\u81f4\u7d50\u679c\u4e0d\u4e00\u81f4\u4e14\u4e0d\u53ef\u9760\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 Depth Any Video\uff0c\u9019\u662f\u4e00\u500b\u900f\u904e\u5169\u9805\u95dc\u9375\u5275\u65b0\u4f86\u61c9\u5c0d\u6311\u6230\u7684\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u7684\u5408\u6210\u6578\u64da\u7ba1\u7dda\uff0c\u5f9e\u591a\u6a23\u5316\u7684\u5408\u6210\u74b0\u5883\u4e2d\u64f7\u53d6\u5373\u6642\u5f71\u7247\u6df1\u5ea6\u6578\u64da\uff0c\u7522\u751f 40,000 \u500b\u9577\u5ea6\u70ba 5 \u79d2\u7684\u5f71\u7247\u526a\u8f2f\uff0c\u6bcf\u500b\u526a\u8f2f\u90fd\u6709\u7cbe\u78ba\u7684\u6df1\u5ea6\u8a3b\u89e3\u3002\u5176\u6b21\uff0c\u6211\u5011\u5229\u7528\u751f\u6210\u5f0f\u5f71\u7247\u64f4\u6563\u6a21\u578b\u7684\u5f37\u5927\u5148\u9a57\u77e5\u8b58\u4f86\u6709\u6548\u8655\u7406\u771f\u5be6\u4e16\u754c\u7684\u5f71\u7247\uff0c\u6574\u5408\u65cb\u8f49\u4f4d\u7f6e\u7de8\u78bc\u548c\u6d41\u5339\u914d\u7b49\u9032\u968e\u6280\u8853\uff0c\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u9748\u6d3b\u6027\u8207\u6548\u7387\u3002\u8207\u5148\u524d\u50c5\u9650\u65bc\u56fa\u5b9a\u9577\u5ea6\u5f71\u7247\u5e8f\u5217\u7684\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u6df7\u5408\u6301\u7e8c\u6642\u9593\u8a13\u7df4\u7b56\u7565\uff0c\u5b83\u53ef\u4ee5\u8655\u7406\u9577\u5ea6\u4e0d\u4e00\u7684\u5f71\u7247\uff0c\u4e26\u5728\u4e0d\u540c\u5e40\u7387\u4e0b\u8868\u73fe\u5f97\u7a69\u5b9a\uff0c\u751a\u81f3\u5728\u55ae\u4e00\u5e40\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u5728\u63a8\u8ad6\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6df1\u5ea6\u63d2\u503c\u65b9\u6cd5\uff0c\u4f7f\u6211\u5011\u7684\u6a21\u578b\u80fd\u5920\u63a8\u8ad6\u9577\u9054 150 \u5e40\u7684\u5e8f\u5217\u4e2d\u7684\u9ad8\u89e3\u6790\u5ea6\u5f71\u7247\u6df1\u5ea6\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u7a7a\u9593\u6e96\u78ba\u5ea6\u548c\u6642\u9593\u4e00\u81f4\u6027\u65b9\u9762\u512a\u65bc\u6240\u6709\u5148\u524d\u7684\u751f\u6210\u5f0f\u6df1\u5ea6\u6a21\u578b\u3002", "author": "Honghui Yang et.al.", "authors": "Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, Tong He", "id": "2410.10815v1", "paper_url": "http://arxiv.org/abs/2410.10815v1", "repo": "null"}}