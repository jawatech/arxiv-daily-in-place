{"2502.10058": {"publish_time": "2025-02-14", "title": "MTLM: an Innovative Language Model Training Paradigm for ASR", "paper_summary": "Pre-training Transformer-based language models (LMs) on a large amount of\ntext has proven crucial for improving automatic speech recognition (ASR)\nperformance. Generally, traditional LMs are unidirectional and unable to access\nthe context on the right. This paper proposes a method for training LMs that\nenable traditional unidirectional LMs to fully utilize left and right contexts.\nCompared with the unidirectional LMs, our LM facilitates ASR to transcribe\nhypotheses more consistently and in a more semantically unambiguous way, as it\nincorporates richer contextual representations. Finally, our experimental\nresults on the LibriSpeech corpus demonstrate that our model outperforms\ntraditional unidirectional LMs, whether n-best rescoring or shallow fusion is\nused as the decoding algorithm.", "paper_summary_zh": "\u5728\u5927\u91cf\u6587\u672c\u4e0a\u9810\u8a13\u7df4\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b (LM) \u5df2\u88ab\u8b49\u660e\u5c0d\u65bc\u63d0\u5347\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u4e00\u822c\u4f86\u8aaa\uff0c\u50b3\u7d71\u7684 LM \u662f\u55ae\u5411\u7684\uff0c\u7121\u6cd5\u5b58\u53d6\u53f3\u5074\u7684\u5167\u5bb9\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u8a13\u7df4 LM \u7684\u65b9\u6cd5\uff0c\u4f7f\u50b3\u7d71\u7684\u55ae\u5411 LM \u80fd\u5920\u5145\u5206\u5229\u7528\u5de6\u53f3\u5169\u5074\u7684\u5167\u5bb9\u3002\u8207\u55ae\u5411 LM \u76f8\u6bd4\uff0c\u6211\u5011\u7684 LM \u80fd\u5920\u8b93 ASR \u66f4\u4e00\u81f4\u4e14\u4ee5\u66f4\u660e\u78ba\u7684\u8a9e\u610f\u65b9\u5f0f\u8f49\u9304\u5047\u8a2d\uff0c\u56e0\u70ba\u5b83\u7d0d\u5165\u4e86\u66f4\u8c50\u5bcc\u7684\u8108\u7d61\u8868\u793a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728 LibriSpeech \u8a9e\u6599\u5eab\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u7121\u8ad6\u662f\u4f7f\u7528 n-best \u91cd\u65b0\u8a55\u5206\u6216\u6dfa\u5c64\u878d\u5408\u4f5c\u70ba\u89e3\u78bc\u6f14\u7b97\u6cd5\uff0c\u6211\u5011\u7684\u6a21\u578b\u90fd\u512a\u65bc\u50b3\u7d71\u7684\u55ae\u5411 LM\u3002", "author": "Qingliang Meng et.al.", "authors": "Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai", "id": "2502.10058v1", "paper_url": "http://arxiv.org/abs/2502.10058v1", "repo": "null"}}