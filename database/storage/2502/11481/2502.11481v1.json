{"2502.11481": {"publish_time": "2025-02-17", "title": "Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound Videos", "paper_summary": "The intersection of medical imaging and artificial intelligence has become an\nimportant research direction in intelligent medical treatment, particularly in\nthe analysis of medical images using deep learning for clinical diagnosis.\nDespite the advances, existing keyframe classification methods lack extraction\nof time series features, while ultrasonic video classification based on\nthree-dimensional convolution requires uniform frame numbers across patients,\nresulting in poor feature extraction efficiency and model classification\nperformance. This study proposes a novel video classification method based on\nCNN and LSTM, introducing NLP's long and short sentence processing scheme into\nvideo classification for the first time. The method reduces CNN-extracted image\nfeatures to 1x512 dimension, followed by sorting and compressing feature\nvectors for LSTM training. Specifically, feature vectors are sorted by patient\nvideo frame numbers and populated with padding value 0 to form variable\nbatches, with invalid padding values compressed before LSTM training to\nconserve computing resources. Experimental results demonstrate that our\nvariable-frame CNNLSTM method outperforms other approaches across all metrics,\nshowing improvements of 3-6% in F1 score and 1.5% in specificity compared to\nkeyframe methods. The variable-frame CNNLSTM also achieves better accuracy and\nprecision than equal-frame CNNLSTM. These findings validate the effectiveness\nof our approach in classifying variable-frame ultrasound videos and suggest\npotential applications in other medical imaging modalities.", "paper_summary_zh": "\u91ab\u5b78\u5f71\u50cf\u8207\u4eba\u5de5\u667a\u6167\u7684\u4ea4\u53c9\u9818\u57df\u5df2\u6210\u70ba\u667a\u6167\u91ab\u7642\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u7279\u5225\u662f\u5728\u81e8\u5e8a\u8a3a\u65b7\u4e2d\u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2\u5206\u6790\u91ab\u5b78\u5f71\u50cf\u3002\u5118\u7ba1\u6709\u9032\u5c55\uff0c\u73fe\u6709\u7684\u95dc\u9375\u5f71\u683c\u5206\u985e\u65b9\u6cd5\u7f3a\u4e4f\u6642\u9593\u5e8f\u5217\u7279\u5fb5\u7684\u63d0\u53d6\uff0c\u800c\u57fa\u65bc\u4e09\u7dad\u5377\u7a4d\u7684\u8d85\u97f3\u6ce2\u5f71\u7247\u5206\u985e\u9700\u8981\u60a3\u8005\u4e4b\u9593\u7684\u5747\u52fb\u5f71\u683c\u6578\uff0c\u5c0e\u81f4\u7279\u5fb5\u63d0\u53d6\u6548\u7387\u5dee\u548c\u6a21\u578b\u5206\u985e\u6548\u80fd\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc CNN \u548c LSTM \u7684\u65b0\u5f71\u7247\u5206\u985e\u65b9\u6cd5\uff0c\u9996\u6b21\u5c07 NLP \u7684\u9577\u77ed\u53e5\u8655\u7406\u6a5f\u5236\u5f15\u5165\u5f71\u7247\u5206\u985e\u4e2d\u3002\u8a72\u65b9\u6cd5\u5c07 CNN \u63d0\u53d6\u7684\u5f71\u50cf\u7279\u5fb5\u7e2e\u6e1b\u70ba 1x512 \u7dad\u5ea6\uff0c\u7136\u5f8c\u5c0d\u7279\u5fb5\u5411\u91cf\u9032\u884c\u6392\u5e8f\u548c\u58d3\u7e2e\u4ee5\u9032\u884c LSTM \u8a13\u7df4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u7279\u5fb5\u5411\u91cf\u6309\u60a3\u8005\u5f71\u7247\u5f71\u683c\u6578\u6392\u5e8f\uff0c\u4e26\u586b\u5145 0 \u88dc\u9f4a\u503c\u4ee5\u5f62\u6210\u53ef\u8b8a\u6279\u6b21\uff0c\u5728 LSTM \u8a13\u7df4\u524d\u58d3\u7e2e\u7121\u6548\u7684\u88dc\u9f4a\u503c\u4ee5\u7bc0\u7701\u904b\u7b97\u8cc7\u6e90\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u53ef\u8b8a\u5f71\u683c CNNLSTM \u65b9\u6cd5\u5728\u6240\u6709\u6307\u6a19\u4e0a\u90fd\u512a\u65bc\u5176\u4ed6\u65b9\u6cd5\uff0c\u8207\u95dc\u9375\u5f71\u683c\u65b9\u6cd5\u76f8\u6bd4\uff0cF1 \u5206\u6578\u63d0\u9ad8\u4e86 3-6%\uff0c\u7279\u7570\u6027\u63d0\u9ad8\u4e86 1.5%\u3002\u53ef\u8b8a\u5f71\u683c CNNLSTM \u4e5f\u6bd4\u7b49\u5f71\u683c CNNLSTM \u9054\u5230\u4e86\u66f4\u597d\u7684\u6e96\u78ba\u5ea6\u548c\u7cbe\u78ba\u5ea6\u3002\u9019\u4e9b\u767c\u73fe\u9a57\u8b49\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5206\u985e\u53ef\u8b8a\u5f71\u683c\u8d85\u97f3\u6ce2\u5f71\u7247\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e26\u8868\u660e\u5728\u5176\u4ed6\u91ab\u5b78\u5f71\u50cf\u6a21\u5f0f\u4e2d\u5177\u6709\u6f5b\u5728\u7684\u61c9\u7528\u3002", "author": "Xiangxiang Cui et.al.", "authors": "Xiangxiang Cui, Zhongyu Li, Xiayue Fan, Peng Huang, Ying Wang, Meng Yang, Shi Chang, Jihua Zhu", "id": "2502.11481v1", "paper_url": "http://arxiv.org/abs/2502.11481v1", "repo": "null"}}