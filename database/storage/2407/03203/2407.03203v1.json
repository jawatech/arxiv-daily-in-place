{"2407.03203": {"publish_time": "2024-07-03", "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts", "paper_summary": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. Similar methods have shown\npromising results in code generation. However, most modern LLMs exhibit\nsuboptimal performance due to the scarcity of aligned NL and Formal Language\n(FL) theorem-proving data. This scarcity results in a paucity of methodologies\nfor training LLMs and techniques to fully utilize their capabilities in\ncomposing formal proofs. To address the challenges, this paper proposes\n**TheoremLlama**, an end-to-end framework to train a general-purpose LLM to\nbecome a Lean4 expert. This framework encompasses NL-FL aligned dataset\ngeneration methods, training approaches for the LLM formal theorem prover, and\ntechniques for LLM Lean4 proof writing. Using the dataset generation method, we\nprovide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped\ndataset. A key innovation in this framework is the NL-FL bootstrapping method,\nwhere NL proofs are integrated into Lean4 code for training datasets,\nleveraging the NL reasoning ability of LLMs for formal reasoning. The\n**TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61%\non MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline\nof 22.95% and 25.41%. We have also open-sourced our model checkpoints and\ngenerated dataset, and will soon make all the code publicly available.", "paper_summary_zh": "\u4f7f\u7528\u50cf Lean \u9019\u6a23\u7684\u96fb\u8166\u53ef\u9a57\u8b49\u5f62\u5f0f\u8a9e\u8a00\u4f86\u8b49\u660e\u6578\u5b78\u5b9a\u7406\uff0c\u5c0d\u6578\u5b78\u63a8\u7406\u6709\u91cd\u5927\u7684\u5f71\u97ff\u3002\u4e00\u7a2e\u5f62\u5f0f\u5b9a\u7406\u8b49\u660e\u7684\u65b9\u6cd5\u6d89\u53ca\u4f7f\u7528\u57fa\u65bc\u81ea\u7136\u8a9e\u8a00 (NL) \u8b49\u660e\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u7522\u751f\u5b8c\u6574\u7684\u8b49\u660e\u3002\u985e\u4f3c\u7684\u8fa6\u6cd5\u5728\u7a0b\u5f0f\u78bc\u7522\u751f\u65b9\u9762\u5df2\u5c55\u73fe\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u7531\u65bc\u5c0d\u9f4a\u7684 NL \u548c\u5f62\u5f0f\u8a9e\u8a00 (FL) \u5b9a\u7406\u8b49\u660e\u8cc7\u6599\u7684\u7a00\u5c11\uff0c\u5927\u591a\u6578\u73fe\u4ee3 LLM \u90fd\u8868\u73fe\u51fa\u6b21\u4f73\u6548\u80fd\u3002\u9019\u7a2e\u7a00\u5c11\u5c0e\u81f4\u4e86 LLM \u8a13\u7df4\u65b9\u6cd5\u548c\u5145\u5206\u5229\u7528\u5176\u80fd\u529b\u64b0\u5beb\u5f62\u5f0f\u8b49\u660e\u7684\u6280\u8853\u7684\u7f3a\u4e4f\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u6587\u63d0\u51fa\u4e86 **TheoremLlama**\uff0c\u4e00\u500b\u7aef\u5230\u7aef\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u8a13\u7df4\u4e00\u500b\u901a\u7528 LLM\uff0c\u4f7f\u5176\u6210\u70ba Lean4 \u5c08\u5bb6\u3002\u6b64\u67b6\u69cb\u5305\u542b NL-FL \u5c0d\u9f4a\u8cc7\u6599\u96c6\u7522\u751f\u65b9\u6cd5\u3001LLM \u5f62\u5f0f\u5b9a\u7406\u8b49\u660e\u5668\u7684\u8a13\u7df4\u65b9\u6cd5\uff0c\u4ee5\u53ca LLM Lean4 \u8b49\u660e\u64b0\u5beb\u7684\u6280\u8853\u3002\u4f7f\u7528\u8cc7\u6599\u96c6\u7522\u751f\u65b9\u6cd5\uff0c\u6211\u5011\u63d0\u4f9b\u4e86 *\u958b\u653e\u5f0f\u5f15\u5c0e\u5b9a\u7406* (OBT)\uff0c\u4e00\u500b NL-FL \u5c0d\u9f4a\u4e14\u5f15\u5c0e\u7684\u8cc7\u6599\u96c6\u3002\u6b64\u67b6\u69cb\u4e2d\u7684\u4e00\u500b\u95dc\u9375\u5275\u65b0\u662f NL-FL \u5f15\u5c0e\u65b9\u6cd5\uff0c\u5176\u4e2d NL \u8b49\u660e\u88ab\u6574\u5408\u5230 Lean4 \u7a0b\u5f0f\u78bc\u4e2d\u4ee5\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u5229\u7528 LLM \u7684 NL \u63a8\u7406\u80fd\u529b\u9032\u884c\u5f62\u5f0f\u63a8\u7406\u3002**TheoremLlama** \u67b6\u69cb\u5728 MiniF2F-Valid \u548c Test \u8cc7\u6599\u96c6\u4e0a\u5206\u5225\u9054\u5230\u4e86 36.48% \u548c 33.61% \u7684\u7d2f\u7a4d\u6e96\u78ba\u5ea6\uff0c\u8d85\u904e\u4e86 GPT-4 \u7684\u57fa\u6e96 22.95% \u548c 25.41%\u3002\u6211\u5011\u4e5f\u958b\u6e90\u4e86\u6211\u5011\u7684\u6a21\u578b\u6aa2\u67e5\u9ede\u548c\u7522\u751f\u7684\u8cc7\u6599\u96c6\uff0c\u4e26\u4e14\u5f88\u5feb\u5c31\u6703\u516c\u958b\u6240\u6709\u7a0b\u5f0f\u78bc\u3002", "author": "Ruida Wang et.al.", "authors": "Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang", "id": "2407.03203v1", "paper_url": "http://arxiv.org/abs/2407.03203v1", "repo": "https://github.com/RickySkywalker/TheoremLlama"}}