{"2405.03009": {"publish_time": "2024-05-05", "title": "Explainable Malware Detection with Tailored Logic Explained Networks", "paper_summary": "Malware detection is a constant challenge in cybersecurity due to the rapid\ndevelopment of new attack techniques. Traditional signature-based approaches\nstruggle to keep pace with the sheer volume of malware samples. Machine\nlearning offers a promising solution, but faces issues of generalization to\nunseen samples and a lack of explanation for the instances identified as\nmalware. However, human-understandable explanations are especially important in\nsecurity-critical fields, where understanding model decisions is crucial for\ntrust and legal compliance. While deep learning models excel at malware\ndetection, their black-box nature hinders explainability. Conversely,\ninterpretable models often fall short in performance. To bridge this gap in\nthis application domain, we propose the use of Logic Explained Networks (LENs),\nwhich are a recently proposed class of interpretable neural networks providing\nexplanations in the form of First-Order Logic (FOL) rules. This paper extends\nthe application of LENs to the complex domain of malware detection,\nspecifically using the large-scale EMBER dataset. In the experimental results\nwe show that LENs achieve robustness that exceeds traditional interpretable\nmethods and that are rivaling black-box models. Moreover, we introduce a\ntailored version of LENs that is shown to generate logic explanations with\nhigher fidelity with respect to the model's predictions.", "paper_summary_zh": "", "author": "Peter Anthony et.al.", "authors": "Peter Anthony,Francesco Giannini,Michelangelo Diligenti,Martin Homola,Marco Gori,Stefan Balogh,Jan Mojzis", "id": "2405.03009v1", "paper_url": "http://arxiv.org/abs/2405.03009v1", "repo": "null"}}