{"2405.07623": {"publish_time": "2024-05-13", "title": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via Nonlinear Integer Programming", "paper_summary": "For language model classification, would you prefer having only one workable\nclass or having every class working? The latter makes more practical uses.\nEspecially for large language models (LLMs), the fact that they achieve a fair\noverall accuracy by in-context learning (ICL) obscures a large difference in\nindividual class accuracies. In this work, we uncover and tackle language\nmodels' imbalance in per-class prediction accuracy by reconceptualizing it as\nthe Contextual Oddity Bias (COBias), and we are the first to engage nonlinear\ninteger programming (NIP) to debias it. Briefly, COBias refers to the\ndifference in accuracy by a class A compared to its ''odd'' class, which holds\nthe majority wrong predictions of class A. With the COBias metric, we reveal\nthat LLMs of varied scales and families exhibit large per-class accuracy\ndifferences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP)\nto correct ICL per-class probabilities for lower bias and higher overall\naccuracy. Our optimization objective is directly based on the evaluation scores\nby COBias and accuracy metrics, solved by simulated annealing. Evaluations on\nthree LLMs across seven NLP classification tasks show that DNIP simultaneously\nachieves significant COBias reduction ($-27\\%$) and accuracy improvement\n($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise\nclass accuracy differences is a direction in pushing forward more accurate,\nmore reliable LLM predictions.", "paper_summary_zh": "\u5c0d\u65bc\u8a9e\u8a00\u6a21\u578b\u5206\u985e\uff0c\u4f60\u6703\u6bd4\u8f03\u5e0c\u671b\u53ea\u6709\u4e00\u500b\u53ef\u884c\u7684\u985e\u5225\u6216\u8b93\u6bcf\u500b\u985e\u5225\u90fd\u53ef\u884c\uff1f\u5f8c\u8005\u5177\u6709\u66f4\u591a\u5be6\u7528\u7528\u9014\u3002\u7279\u5225\u662f\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u5b83\u5011\u900f\u904e\u60c5\u5883\u5b78\u7fd2 (ICL) \u9054\u5230\u76f8\u7576\u7684\u6574\u9ad4\u6e96\u78ba\u5ea6\uff0c\u9019\u63a9\u84cb\u4e86\u5404\u500b\u985e\u5225\u6e96\u78ba\u5ea6\u4e4b\u9593\u7684\u5de8\u5927\u5dee\u7570\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u5c07\u8a9e\u8a00\u6a21\u578b\u5728\u6bcf\u500b\u985e\u5225\u9810\u6e2c\u6e96\u78ba\u5ea6\u4e0a\u7684\u4e0d\u5e73\u8861\u8996\u70ba\u60c5\u5883\u5947\u7570\u504f\u5dee (COBias)\uff0c\u4e26\u52a0\u4ee5\u63ed\u9732\u548c\u8655\u7406\uff0c\u6211\u5011\u662f\u7b2c\u4e00\u500b\u4f7f\u7528\u975e\u7dda\u6027\u6574\u6578\u898f\u5283 (NIP) \u4f86\u6d88\u9664\u504f\u5dee\u3002\u7c21\u800c\u8a00\u4e4b\uff0cCOBias \u6307\u7684\u662f\u985e\u5225 A \u8207\u5176\u300c\u5947\u7570\u300d\u985e\u5225\u7684\u6e96\u78ba\u5ea6\u5dee\u7570\uff0c\u5f8c\u8005\u5305\u542b\u985e\u5225 A \u9810\u6e2c\u932f\u8aa4\u7684\u5927\u591a\u6578\u60c5\u6cc1\u3002\u900f\u904e COBias \u6307\u6a19\uff0c\u6211\u5011\u63ed\u793a\u4e86\u4e0d\u540c\u898f\u6a21\u548c\u985e\u578b\u7684 LLM \u5728\u6bcf\u500b\u985e\u5225\u7684\u6e96\u78ba\u5ea6\u4e0a\u5b58\u5728\u5f88\u5927\u7684\u5dee\u7570\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u975e\u7dda\u6027\u6574\u6578\u898f\u5283\u53bb\u504f\u5dee (DNIP) \u4f86\u4fee\u6b63 ICL \u6bcf\u500b\u985e\u5225\u7684\u6a5f\u7387\uff0c\u4ee5\u964d\u4f4e\u504f\u5dee\u4e26\u63d0\u9ad8\u6574\u9ad4\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u6700\u4f73\u5316\u76ee\u6a19\u76f4\u63a5\u57fa\u65bc COBias \u548c\u6e96\u78ba\u5ea6\u6307\u6a19\u7684\u8a55\u5206\uff0c\u4e26\u900f\u904e\u6a21\u64ec\u9000\u706b\u4f86\u89e3\u6c7a\u3002\u91dd\u5c0d\u4e03\u9805 NLP \u5206\u985e\u4efb\u52d9\u5c0d\u4e09\u500b LLM \u9032\u884c\u8a55\u4f30\uff0c\u7d50\u679c\u986f\u793a DNIP \u540c\u6642\u5927\u5e45\u964d\u4f4e COBias (-27%)\uff0c\u4e26\u63d0\u9ad8\u6e96\u78ba\u5ea6 (+12%)\uff0c\u512a\u65bc\u50b3\u7d71\u7684 ICL \u65b9\u6cd5\uff0c\u9019\u8868\u793a\u5c0d\u6210\u5c0d\u985e\u5225\u6e96\u78ba\u5ea6\u5dee\u7570\u9032\u884c\u5efa\u6a21\u662f\u63a8\u52d5\u66f4\u6e96\u78ba\u3001\u66f4\u53ef\u9760\u7684 LLM \u9810\u6e2c\u7684\u767c\u5c55\u65b9\u5411\u3002", "author": "Ruixi Lin et.al.", "authors": "Ruixi Lin, Yang You", "id": "2405.07623v1", "paper_url": "http://arxiv.org/abs/2405.07623v1", "repo": "null"}}