{"2406.14319": {"publish_time": "2024-06-20", "title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference", "paper_summary": "In this paper, we introduce a novel low-latency inference framework for large\nlanguage models (LLMs) inference which enables LLMs to perform inferences with\nincomplete prompts. By reallocating computational processes to prompt input\nphase, we achieve a substantial reduction in latency, thereby significantly\nenhancing the interactive experience for users of LLMs. The framework adeptly\nmanages the visibility of the streaming prompt to the model, allowing it to\ninfer from incomplete prompts or await additional prompts. Compared with\ntraditional inference methods that utilize complete prompts, our approach\ndemonstrates an average reduction of 59% in response latency on the MMLU-Pro\ndataset, while maintaining comparable accuracy. Additionally, our framework\nfacilitates collaborative inference and output across different models. By\nemploying an LLM for inference and a small language model (SLM) for output, we\nachieve an average 68% reduction in response latency, alongside a 5.5%\nimprovement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.\nFor long prompts exceeding 20 sentences, the response latency can be reduced by\nup to 93%.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3a\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u63a8\u7406\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f LLM \u80fd\u591f\u4f7f\u7528\u4e0d\u5b8c\u6574\u7684\u63d0\u793a\u6267\u884c\u63a8\u7406\u3002\u901a\u8fc7\u5c06\u8ba1\u7b97\u8fc7\u7a0b\u91cd\u65b0\u5206\u914d\u5230\u63d0\u793a\u8f93\u5165\u9636\u6bb5\uff0c\u6211\u4eec\u5927\u5e45\u51cf\u5c11\u4e86\u5ef6\u8fdf\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u4e86 LLM \u7528\u6237\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002\u8be5\u6846\u67b6\u5de7\u5999\u5730\u7ba1\u7406\u6a21\u578b\u5bf9\u6d41\u5f0f\u63d0\u793a\u7684\u53ef\u89c1\u6027\uff0c\u5141\u8bb8\u5b83\u4ece\u4e0d\u5b8c\u6574\u7684\u63d0\u793a\u4e2d\u63a8\u65ad\u6216\u7b49\u5f85\u5176\u4ed6\u63d0\u793a\u3002\u4e0e\u5229\u7528\u5b8c\u6574\u63d0\u793a\u7684\u4f20\u7edf\u63a8\u7406\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 MMLU-Pro \u6570\u636e\u96c6\u4e0a\u5c06\u54cd\u5e94\u5ef6\u8fdf\u5e73\u5747\u51cf\u5c11\u4e86 59%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4fc3\u8fdb\u4e86\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u534f\u4f5c\u63a8\u7406\u548c\u8f93\u51fa\u3002\u901a\u8fc7\u4f7f\u7528 LLM \u8fdb\u884c\u63a8\u7406\u548c\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u8fdb\u884c\u8f93\u51fa\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u54cd\u5e94\u5ef6\u8fdf\u5e73\u5747\u51cf\u5c11 68%\uff0c\u540c\u65f6\u5728 MMLU-Pro \u6570\u636e\u96c6\u4e0a\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 5.5%\uff0c\u4e0e SLM \u57fa\u7ebf\u76f8\u6bd4\u3002\u5bf9\u4e8e\u8d85\u8fc7 20 \u4e2a\u53e5\u5b50\u7684\u957f\u63d0\u793a\uff0c\u54cd\u5e94\u5ef6\u8fdf\u53ef\u4ee5\u51cf\u5c11\u591a\u8fbe 93%\u3002", "author": "Chuangtao Chen et.al.", "authors": "Chuangtao Chen, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ulf Schlichtmann, Bing Li", "id": "2406.14319v1", "paper_url": "http://arxiv.org/abs/2406.14319v1", "repo": "https://github.com/chuangtaochen-tum/livemind"}}