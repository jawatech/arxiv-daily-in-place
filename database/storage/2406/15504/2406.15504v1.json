{"2406.15504": {"publish_time": "2024-06-19", "title": "Dr.E Bridges Graphs with Large Language Models through Words", "paper_summary": "Significant efforts have been directed toward integrating powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of vision, language, and audio data. However, the graph-structured data,\ninherently rich in structural and domain-specific knowledge, have not yet been\ngracefully adapted to LLMs. Existing methods either describe the graph with raw\ntext, suffering the loss of graph structural information, or feed Graph Neural\nNetwork (GNN) embeddings directly into LLM at the cost of losing semantic\nrepresentation. To bridge this gap, we introduce an innovative, end-to-end\nmodality-aligning framework, equipped with a pretrained Dual-Residual Vector\nQuantized-Variational AutoEncoder (Dr.E). This framework is specifically\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. Our experimental evaluations on standard GNN node classification\ntasks demonstrate competitive performance against other state-of-the-art\napproaches. Additionally, our framework ensures interpretability, efficiency,\nand robustness, with its effectiveness further validated under both fine-tuning\nand few-shot settings. This study marks the first successful endeavor to\nachieve token-level alignment between GNNs and LLMs.", "paper_summary_zh": "\u5927\u91cf\u7684\u52aa\u529b\u5df2\u6295\u5165\u5230\u5c07\u5f37\u5927\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4e0d\u540c\u7684\u6a21\u614b\u6574\u5408\uff0c\u7279\u5225\u662f\u5c08\u6ce8\u65bc\u8996\u89ba\u3001\u8a9e\u8a00\u548c\u97f3\u8a0a\u8cc7\u6599\u7684\u878d\u5408\u3002\u7136\u800c\uff0c\u5716\u5f62\u7d50\u69cb\u5316\u7684\u8cc7\u6599\u672c\u8cea\u4e0a\u5bcc\u542b\u7d50\u69cb\u548c\u9818\u57df\u7279\u5b9a\u7684\u77e5\u8b58\uff0c\u4f46\u5c1a\u672a\u512a\u96c5\u5730\u9069\u61c9 LLM\u3002\u73fe\u6709\u65b9\u6cd5\u4e0d\u662f\u7528\u539f\u59cb\u6587\u5b57\u63cf\u8ff0\u5716\u5f62\uff0c\u5c0e\u81f4\u5716\u5f62\u7d50\u69cb\u8cc7\u8a0a\u907a\u5931\uff0c\u5c31\u662f\u5c07\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u7684\u5d4c\u5165\u76f4\u63a5\u994b\u5165 LLM\uff0c\u4ee3\u50f9\u662f\u5931\u53bb\u8a9e\u7fa9\u8868\u793a\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5275\u65b0\u7684\u7aef\u5230\u7aef\u6a21\u614b\u5c0d\u9f4a\u6846\u67b6\uff0c\u914d\u5099\u4e86\u4e00\u500b\u9810\u5148\u8a13\u7df4\u7684\u96d9\u6b98\u5dee\u5411\u91cf\u91cf\u5316\u8b8a\u5206\u81ea\u7de8\u78bc\u5668 (Dr.E)\u3002\u6b64\u6846\u67b6\u7279\u5225\u8a2d\u8a08\u7528\u65bc\u4fc3\u9032\u8207 LLM \u7684\u6a19\u8a18\u5c64\u7d1a\u5c0d\u9f4a\uff0c\u8b93\u5716\u5f62\u7684\u5167\u5728\u300c\u8a9e\u8a00\u300d\u80fd\u6709\u6548\u8f49\u63db\u6210\u6613\u65bc\u7406\u89e3\u7684\u81ea\u7136\u8a9e\u8a00\u3002\u6211\u5011\u5728\u6a19\u6e96 GNN \u7bc0\u9ede\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8a55\u4f30\u986f\u793a\uff0c\u8207\u5176\u4ed6\u6700\u5148\u9032\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u8868\u73fe\u5177\u6709\u7af6\u722d\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6846\u67b6\u78ba\u4fdd\u4e86\u89e3\u91cb\u6027\u3001\u6548\u7387\u548c\u7a69\u5065\u6027\uff0c\u5728\u5fae\u8abf\u548c\u5c11\u6a23\u672c\u8a2d\u5b9a\u4e0b\u9032\u4e00\u6b65\u9a57\u8b49\u5176\u6709\u6548\u6027\u3002\u9019\u9805\u7814\u7a76\u6a19\u8a8c\u8457\u5728 GNN \u548c LLM \u4e4b\u9593\u5be6\u73fe\u6a19\u8a18\u5c64\u7d1a\u5c0d\u9f4a\u7684\u9996\u6b21\u6210\u529f\u5617\u8a66\u3002", "author": "Zipeng Liu et.al.", "authors": "Zipeng Liu, Likang Wu, Ming He, Zhong Guan, Hongke Zhao, Nan Feng", "id": "2406.15504v1", "paper_url": "http://arxiv.org/abs/2406.15504v1", "repo": "null"}}