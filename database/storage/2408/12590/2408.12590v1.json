{"2408.12590": {"publish_time": "2024-08-22", "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations", "paper_summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa xGen-VideoSyn-1\uff0c\u4e00\u500b\u6587\u5b57\u8f49\u5f71\u7247 (T2V) \u751f\u6210\u6a21\u578b\uff0c\u80fd\u5920\u6839\u64da\u6587\u5b57\u63cf\u8ff0\u7522\u751f\u903c\u771f\u7684\u5834\u666f\u3002\u5728 OpenAI \u7684 Sora \u7b49\u8fd1\u671f\u9032\u5c55\u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u6f5b\u5728\u64f4\u6563\u6a21\u578b (LDM) \u67b6\u69cb\uff0c\u4e26\u5f15\u5165\u4e86\u5f71\u7247\u8b8a\u7570\u81ea\u7de8\u78bc\u5668 (VidVAE)\u3002VidVAE \u5728\u7a7a\u9593\u548c\u6642\u9593\u4e0a\u58d3\u7e2e\u5f71\u7247\u8cc7\u6599\uff0c\u5927\u5e45\u6e1b\u5c11\u8996\u89ba\u6a19\u8a18\u7684\u9577\u5ea6\uff0c\u4ee5\u53ca\u7522\u751f\u9577\u5e8f\u5217\u5f71\u7247\u76f8\u95dc\u7684\u904b\u7b97\u9700\u6c42\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u89e3\u6c7a\u904b\u7b97\u6210\u672c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5206\u800c\u4f75\u4e4b\u7b56\u7565\uff0c\u4ee5\u7dad\u6301\u5f71\u7247\u7247\u6bb5\u4e4b\u9593\u7684\u6642\u9593\u4e00\u81f4\u6027\u3002\u6211\u5011\u7684\u64f4\u6563\u8f49\u63db\u5668 (DiT) \u6a21\u578b\u6574\u5408\u4e86\u7a7a\u9593\u548c\u6642\u9593\u81ea\u6211\u6ce8\u610f\u5c64\uff0c\u8b93\u4e0d\u540c\u6642\u9593\u7bc4\u570d\u548c\u9577\u5bec\u6bd4\u90fd\u80fd\u7a69\u5065\u5730\u6982\u62ec\u3002\u6211\u5011\u5f9e\u4e00\u958b\u59cb\u5c31\u8a2d\u8a08\u4e86\u4e00\u500b\u8cc7\u6599\u8655\u7406\u6d41\u7a0b\uff0c\u4e26\u6536\u96c6\u4e86\u8d85\u904e 1300 \u842c\u500b\u9ad8\u54c1\u8cea\u5f71\u7247\u6587\u5b57\u914d\u5c0d\u3002\u6d41\u7a0b\u5305\u542b\u591a\u500b\u6b65\u9a5f\uff0c\u4f8b\u5982\u526a\u8f2f\u3001\u6587\u5b57\u5075\u6e2c\u3001\u52d5\u4f5c\u4f30\u8a08\u3001\u7f8e\u5b78\u8a55\u5206\uff0c\u4ee5\u53ca\u57fa\u65bc\u6211\u5011\u5167\u90e8\u5f71\u7247 LLM \u6a21\u578b\u7684\u5bc6\u96c6\u5f0f\u5b57\u5e55\u3002\u8a13\u7df4 VidVAE \u548c DiT \u6a21\u578b\u5206\u5225\u9700\u8981\u5927\u7d04 40 \u548c 642 \u500b H100 \u5929\u3002\u6211\u5011\u7684\u6a21\u578b\u652f\u63f4\u4ee5\u7aef\u5c0d\u7aef\u7684\u65b9\u5f0f\u7522\u751f\u8d85\u904e 14 \u79d2\u7684 720p \u5f71\u7247\uff0c\u4e26\u5c55\u73fe\u51fa\u8207\u73fe\u6709 T2V \u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002", "author": "Can Qin et.al.", "authors": "Can Qin, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu, Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, Jun Wang, Senthil Purushwalkam, Le Xue, Yingbo Zhou, Huan Wang, Silvio Savarese, Juan Carlos Niebles, Zeyuan Chen, Ran Xu, Caiming Xiong", "id": "2408.12590v1", "paper_url": "http://arxiv.org/abs/2408.12590v1", "repo": "null"}}