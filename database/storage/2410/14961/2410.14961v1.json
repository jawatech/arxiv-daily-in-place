{"2410.14961": {"publish_time": "2024-10-19", "title": "LangGFM: A Large Language Model Alone Can be a Powerful Graph Foundation Model", "paper_summary": "Graph foundation models (GFMs) have recently gained significant attention.\nHowever, the unique data processing and evaluation setups employed by different\nstudies hinder a deeper understanding of their progress. Additionally, current\nresearch tends to focus on specific subsets of graph learning tasks, such as\nstructural tasks, node-level tasks, or classification tasks. As a result, they\noften incorporate specialized modules tailored to particular task types, losing\ntheir applicability to other graph learning tasks and contradicting the\noriginal intent of foundation models to be universal. Therefore, to enhance\nconsistency, coverage, and diversity across domains, tasks, and research\ninterests within the graph learning community in the evaluation of GFMs, we\npropose GFMBench-a systematic and comprehensive benchmark comprising 26\ndatasets. Moreover, we introduce LangGFM, a novel GFM that relies entirely on\nlarge language models. By revisiting and exploring the effective graph\ntextualization principles, as well as repurposing successful techniques from\ngraph augmentation and graph self-supervised learning within the language\nspace, LangGFM achieves performance on par with or exceeding the state of the\nart across GFMBench, which can offer us new perspectives, experiences, and\nbaselines to drive forward the evolution of GFMs.", "paper_summary_zh": "\u5716\u5f62\u57fa\u790e\u6a21\u578b (GFM) \u8fd1\u671f\u7372\u5f97\u986f\u8457\u7684\u95dc\u6ce8\u3002\n\u7136\u800c\uff0c\u4e0d\u540c\u7814\u7a76\u63a1\u7528\u7368\u7279\u8cc7\u6599\u8655\u7406\u548c\u8a55\u4f30\u8a2d\u5b9a\uff0c\u963b\u7919\u4e86\u5c0d\u5176\u9032\u5c55\u7684\u6df1\u5165\u7406\u89e3\u3002\u6b64\u5916\uff0c\u76ee\u524d\u7684\u7814\u7a76\u50be\u5411\u65bc\u5c08\u6ce8\u65bc\u5716\u5f62\u5b78\u7fd2\u4efb\u52d9\u7684\u7279\u5b9a\u5b50\u96c6\uff0c\u4f8b\u5982\u7d50\u69cb\u4efb\u52d9\u3001\u7bc0\u9ede\u5c64\u7d1a\u4efb\u52d9\u6216\u5206\u985e\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u5b83\u5011\u7d93\u5e38\u6574\u5408\u5c08\u9580\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u985e\u578b\u91cf\u8eab\u6253\u9020\u7684\u6a21\u7d44\uff0c\u5931\u53bb\u5176\u5c0d\u5176\u4ed6\u5716\u5f62\u5b78\u7fd2\u4efb\u52d9\u7684\u9069\u7528\u6027\uff0c\u4e26\u8207\u57fa\u790e\u6a21\u578b\u6210\u70ba\u901a\u7528\u7684\u539f\u59cb\u610f\u5716\u76f8\u77db\u76fe\u3002\u56e0\u6b64\uff0c\u70ba\u4e86\u589e\u5f37\u5716\u5f62\u5b78\u7fd2\u793e\u7fa4\u5728\u8a55\u4f30 GFM \u6642\u8de8\u9818\u57df\u3001\u4efb\u52d9\u548c\u7814\u7a76\u8208\u8da3\u7684\u4e00\u81f4\u6027\u3001\u6db5\u84cb\u7bc4\u570d\u548c\u591a\u6a23\u6027\uff0c\u6211\u5011\u63d0\u51fa GFMBench\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 26 \u500b\u8cc7\u6599\u96c6\u7684\u7cfb\u7d71\u5316\u4e14\u5168\u9762\u7684\u57fa\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u4ecb\u7d39 LangGFM\uff0c\u9019\u662f\u4e00\u7a2e\u5b8c\u5168\u4f9d\u8cf4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u65b0\u7a4e GFM\u3002\u900f\u904e\u91cd\u65b0\u6aa2\u8996\u548c\u63a2\u7d22\u6709\u6548\u7684\u5716\u5f62\u6587\u5b57\u5316\u539f\u5247\uff0c\u4ee5\u53ca\u5728\u8a9e\u8a00\u7a7a\u9593\u4e2d\u91cd\u65b0\u5229\u7528\u5716\u5f62\u64f4\u5145\u548c\u5716\u5f62\u81ea\u76e3\u7763\u5b78\u7fd2\u7684\u6210\u529f\u6280\u8853\uff0cLangGFM \u5728 GFMBench \u4e0a\u5be6\u73fe\u8207\u73fe\u6709\u6280\u8853\u540c\u7b49\u6216\u8d85\u8d8a\u73fe\u6709\u6280\u8853\u7684\u6548\u80fd\uff0c\u9019\u53ef\u4ee5\u70ba\u6211\u5011\u63d0\u4f9b\u65b0\u7684\u89c0\u9ede\u3001\u7d93\u9a57\u548c\u57fa\u6e96\uff0c\u4ee5\u63a8\u52d5 GFM \u7684\u6f14\u9032\u3002", "author": "Tianqianjin Lin et.al.", "authors": "Tianqianjin Lin, Pengwei Yan, Kaisong Song, Zhuoren Jiang, Yangyang Kang, Jun Lin, Weikang Yuan, Junjie Cao, Changlong Sun, Xiaozhong Liu", "id": "2410.14961v1", "paper_url": "http://arxiv.org/abs/2410.14961v1", "repo": "null"}}