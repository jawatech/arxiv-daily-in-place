{"2502.13685": {"publish_time": "2025-02-19", "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "paper_summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.", "paper_summary_zh": "\u7dda\u6027\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f8b\u5982\u7dda\u6027\u6ce8\u610f\u529b\u3001\u72c0\u614b\u7a7a\u9593\u5efa\u6a21\u548c\u7dda\u6027 RNN\uff0c\u900f\u904e\u964d\u4f4e\u8a13\u7df4\u548c\u63a8\u8ad6\u7684\u8907\u96dc\u6027\uff0c\u63d0\u4f9b\u4e86\u986f\u8457\u7684\u6548\u7387\u6539\u9032\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u5c07\u6574\u500b\u8f38\u5165\u5e8f\u5217\u58d3\u7e2e\u6210\u4e00\u500b\u55ae\u4e00\u7684\u56fa\u5b9a\u5927\u5c0f\u8a18\u61b6\u9ad4\u72c0\u614b\uff0c\u9019\u6703\u5c0e\u81f4\u53ec\u56de\u5bc6\u96c6\u7684\u4e0b\u6e38\u4efb\u52d9\u7684\u6b21\u4f73\u6548\u80fd\u3002\u5f9e\u795e\u7d93\u79d1\u5b78\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u7279\u5225\u662f\u5927\u8166\u5728\u6e1b\u8f15\u300c\u8a18\u61b6\u5e72\u64fe\u300d\u7684\u540c\u6642\u7dad\u6301\u7a69\u5065\u9577\u671f\u8a18\u61b6\u7684\u80fd\u529b\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u540d\u70ba\u8a18\u61b6\u6df7\u5408 (MoM) \u7684\u65b0\u67b6\u69cb\u3002MoM \u5229\u7528\u591a\u500b\u7368\u7acb\u7684\u8a18\u61b6\u9ad4\u72c0\u614b\uff0c\u4e26\u900f\u904e\u8def\u7531\u5668\u7db2\u8def\u5c07\u8f38\u5165\u4ee3\u78bc\u5c0e\u5411\u7279\u5b9a\u7684\u8a18\u61b6\u9ad4\u72c0\u614b\u3002\u9019\u7a2e\u65b9\u6cd5\u5728\u6700\u5927\u7a0b\u5ea6\u6e1b\u5c11\u8a18\u61b6\u9ad4\u5e72\u64fe\u7684\u540c\u6642\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6574\u9ad4\u8a18\u61b6\u9ad4\u5bb9\u91cf\u3002\u56e0\u6b64\uff0cMoM \u5728\u53ec\u56de\u5bc6\u96c6\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u6975\u4f73\uff0c\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u7dda\u6027\u5e8f\u5217\u5efa\u6a21\u6280\u8853\u3002\u5118\u7ba1\u6574\u5408\u4e86\u591a\u500b\u8a18\u61b6\u9ad4\u72c0\u614b\uff0c\u4f46\u6bcf\u500b\u8a18\u61b6\u9ad4\u72c0\u614b\u7684\u904b\u7b97\u5728\u8907\u96dc\u5ea6\u4e0a\u4ecd\u7136\u662f\u7dda\u6027\u7684\uff0c\u9019\u8b93 MoM \u5728\u8a13\u7df4\u671f\u9593\u80fd\u4fdd\u6709\u7dda\u6027\u8907\u96dc\u5ea6\u7684\u512a\u52e2\uff0c\u800c\u5728\u63a8\u8ad6\u671f\u9593\u5247\u7dad\u6301\u6046\u5b9a\u8907\u96dc\u5ea6\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cMoM \u5728\u4e0b\u6e38\u8a9e\u8a00\u4efb\u52d9\u4e0a\u660e\u986f\u512a\u65bc\u76ee\u524d\u7684\u7dda\u6027\u5e8f\u5217\u6a21\u578b\uff0c\u7279\u5225\u662f\u53ec\u56de\u5bc6\u96c6\u4efb\u52d9\uff0c\u751a\u81f3\u9054\u5230\u4e86\u8207 Transformer \u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u5df2\u767c\u5e03\u65bc https://github.com/OpenSparseLLMs/MoM\uff0c\u4e26\u4f5c\u70ba https://github.com/OpenSparseLLMs/Linear-MoE \u7684\u4e00\u90e8\u5206\u767c\u5e03\u3002", "author": "Jusen Du et.al.", "authors": "Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng", "id": "2502.13685v1", "paper_url": "http://arxiv.org/abs/2502.13685v1", "repo": "null"}}