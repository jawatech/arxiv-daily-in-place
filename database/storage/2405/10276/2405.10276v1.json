{"2405.10276": {"publish_time": "2024-05-16", "title": "Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers", "paper_summary": "Numerous recent works aim to enhance the efficacy of Large Language Models\n(LLMs) through strategic prompting. In particular, the Optimization by\nPROmpting (OPRO) approach provides state-of-the-art performance by leveraging\nLLMs as optimizers where the optimization task is to find instructions that\nmaximize the task accuracy. In this paper, we revisit OPRO for automated\nprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral\n7B. Our investigation reveals that OPRO shows limited effectiveness in\nsmall-scale LLMs, with limited inference capabilities constraining optimization\nability. We suggest future automatic prompting engineering to consider both\nmodel capabilities and computational costs. Additionally, for small-scale LLMs,\nwe recommend direct instructions that clearly outline objectives and\nmethodologies as robust prompt baselines, ensuring efficient and effective\nprompt engineering in ongoing research.", "paper_summary_zh": "\u8a31\u591a\u6700\u8fd1\u7684\u7814\u7a76\u65e8\u5728\u900f\u904e\u7b56\u7565\u6027\u63d0\u793a\u4f86\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u3002\u7279\u5225\u662f\uff0c\u900f\u904e\u6700\u4f73\u5316\u63d0\u793a (OPRO) \u65b9\u6cd5\uff0c\u53ef\u5c07 LLM \u4f5c\u70ba\u6700\u4f73\u5316\u5668\uff0c\u800c\u6700\u4f73\u5316\u4efb\u52d9\u662f\u627e\u51fa\u53ef\u6700\u5927\u5316\u4efb\u52d9\u7cbe\u78ba\u5ea6\u7684\u6307\u4ee4\uff0c\u9032\u800c\u63d0\u4f9b\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u6aa2\u8996 OPRO\uff0c\u4ee5\u81ea\u52d5\u63d0\u793a\u76f8\u5c0d\u5c0f\u898f\u6a21\u7684 LLM\uff0c\u4f8b\u5982 LLaMa-2 \u7cfb\u5217\u548c Mistral 7B\u3002\u6211\u5011\u7684\u8abf\u67e5\u986f\u793a\uff0cOPRO \u5728\u5c0f\u898f\u6a21 LLM \u4e2d\u5c55\u73fe\u7684\u6548\u80fd\u6709\u9650\uff0c\u539f\u56e0\u662f\u6709\u9650\u7684\u63a8\u8ad6\u80fd\u529b\u9650\u5236\u4e86\u6700\u4f73\u5316\u80fd\u529b\u3002\u6211\u5011\u5efa\u8b70\u672a\u4f86\u7684\u81ea\u52d5\u63d0\u793a\u5de5\u7a0b\u61c9\u8003\u91cf\u6a21\u578b\u80fd\u529b\u548c\u904b\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u5c0f\u898f\u6a21 LLM\uff0c\u6211\u5011\u5efa\u8b70\u76f4\u63a5\u63d0\u4f9b\u660e\u78ba\u8aaa\u660e\u76ee\u6a19\u548c\u65b9\u6cd5\u7684\u6307\u4ee4\uff0c\u4f5c\u70ba\u7a69\u5065\u7684\u63d0\u793a\u57fa\u6e96\uff0c\u4ee5\u78ba\u4fdd\u6301\u7e8c\u7814\u7a76\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\u80fd\u6709\u6548\u7387\u4e14\u6709\u6548\u3002", "author": "Tuo Zhang et.al.", "authors": "Tuo Zhang, Jinyue Yuan, Salman Avestimehr", "id": "2405.10276v1", "paper_url": "http://arxiv.org/abs/2405.10276v1", "repo": "null"}}