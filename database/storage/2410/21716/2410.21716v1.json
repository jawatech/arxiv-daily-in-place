{"2410.21716": {"publish_time": "2024-10-29", "title": "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution", "paper_summary": "Authorship attribution aims to identify the origin or author of a document.\nTraditional approaches have heavily relied on manual features and fail to\ncapture long-range correlations, limiting their effectiveness. Recent\nadvancements leverage text embeddings from pre-trained language models, which\nrequire significant fine-tuning on labeled data, posing challenges in data\ndependency and limited interpretability. Large Language Models (LLMs), with\ntheir deep reasoning capabilities and ability to maintain long-range textual\nassociations, offer a promising alternative. This study explores the potential\nof pre-trained LLMs in one-shot authorship attribution, specifically utilizing\nBayesian approaches and probability outputs of LLMs. Our methodology calculates\nthe probability that a text entails previous writings of an author, reflecting\na more nuanced understanding of authorship. By utilizing only pre-trained\nmodels such as Llama-3-70B, our results on the IMDb and blog datasets show an\nimpressive 85\\% accuracy in one-shot authorship classification across ten\nauthors. Our findings set new baselines for one-shot authorship analysis using\nLLMs and expand the application scope of these models in forensic linguistics.\nThis work also includes extensive ablation studies to validate our approach.", "paper_summary_zh": "\u4f5c\u8005\u5f52\u56e0\u65e8\u5728\u8bc6\u522b\u6587\u4ef6\u6765\u6e90\u6216\u4f5c\u8005\u3002\n\u4f20\u7edf\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e8e\u624b\u52a8\u7279\u5f81\uff0c\u5e76\u4e14\u65e0\u6cd5\u6355\u6349\u957f\u7a0b\u5173\u8054\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002\u6700\u8fd1\u7684\u8fdb\u6b65\u5229\u7528\u4e86\u9884\u5148\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u672c\u5d4c\u5165\uff0c\u8fd9\u9700\u8981\u5bf9\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u5927\u91cf\u7684\u5fae\u8c03\uff0c\u5728\u6570\u636e\u4f9d\u8d56\u6027\u548c\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5177\u6709\u6df1\u5165\u7684\u63a8\u7406\u80fd\u529b\u548c\u7ef4\u62a4\u957f\u7a0b\u6587\u672c\u5173\u8054\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u5148\u8bad\u7ec3\u7684 LLM \u5728\u4e00\u6b21\u6027\u4f5c\u8005\u5f52\u56e0\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5229\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c LLM \u7684\u6982\u7387\u8f93\u51fa\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8ba1\u7b97\u4e86\u4e00\u6bb5\u6587\u672c\u5305\u542b\u4f5c\u8005\u5148\u524d\u4f5c\u54c1\u7684\u6982\u7387\uff0c\u53cd\u6620\u4e86\u5bf9\u4f5c\u8005\u8eab\u4efd\u7684\u66f4\u7ec6\u81f4\u7684\u7406\u89e3\u3002\u901a\u8fc7\u4ec5\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u4f8b\u5982 Llama-3-70B\uff09\uff0c\u6211\u4eec\u5728 IMDb \u548c\u535a\u5ba2\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5341\u4f4d\u4f5c\u8005\u4e2d\u7684\u4e00\u6b21\u6027\u4f5c\u8005\u5206\u7c7b\u4e2d\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u4e86\u60ca\u4eba\u7684 85%\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u4f7f\u7528 LLM \u8fdb\u884c\u4e00\u6b21\u6027\u4f5c\u8005\u5206\u6790\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u7ebf\uff0c\u5e76\u6269\u5c55\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u6cd5\u8bc1\u8bed\u8a00\u5b66\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002\u8fd9\u9879\u5de5\u4f5c\u8fd8\u5305\u62ec\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\u3002", "author": "Zhengmian Hu et.al.", "authors": "Zhengmian Hu, Tong Zheng, Heng Huang", "id": "2410.21716v1", "paper_url": "http://arxiv.org/abs/2410.21716v1", "repo": "null"}}