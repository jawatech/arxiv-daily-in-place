{"2501.06126": {"publish_time": "2025-01-10", "title": "Merging Feed-Forward Sublayers for Compressed Transformers", "paper_summary": "With the rise and ubiquity of larger deep learning models, the need for\nhigh-quality compression techniques is growing in order to deploy these models\nwidely. The sheer parameter count of these models makes it difficult to fit\nthem into the memory constraints of different hardware. In this work, we\npresent a novel approach to model compression by merging similar parameter\ngroups within a model, rather than pruning away less important parameters.\nSpecifically, we select, align, and merge separate feed-forward sublayers in\nTransformer models, and test our method on language modeling, image\nclassification, and machine translation. With our method, we demonstrate\nperformance comparable to the original models while combining more than a third\nof model feed-forward sublayers, and demonstrate improved performance over a\nstrong layer-pruning baseline. For instance, we can remove over 21% of total\nparameters from a Vision Transformer, while maintaining 99% of its original\nperformance. Additionally, we observe that some groups of feed-forward\nsublayers exhibit high activation similarity, which may help explain their\nsurprising mergeability.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u7684\u8208\u8d77\u548c\u666e\u53ca\uff0c\u70ba\u4e86\u5ee3\u6cdb\u90e8\u7f72\u9019\u4e9b\u6a21\u578b\uff0c\u5c0d\u9ad8\u54c1\u8cea\u58d3\u7e2e\u6280\u8853\u7684\u9700\u6c42\u8207\u65e5\u4ff1\u589e\u3002\u9019\u4e9b\u6a21\u578b\u7684\u53c3\u6578\u6578\u91cf\u9f90\u5927\uff0c\u96e3\u4ee5\u7b26\u5408\u4e0d\u540c\u786c\u9ad4\u7684\u8a18\u61b6\u9ad4\u9650\u5236\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u6a21\u578b\u58d3\u7e2e\u65b9\u6cd5\uff0c\u900f\u904e\u5408\u4f75\u6a21\u578b\u4e2d\u76f8\u4f3c\u7684\u53c3\u6578\u7d44\uff0c\u800c\u4e0d\u662f\u526a\u9664\u8f03\u4e0d\u91cd\u8981\u7684\u53c3\u6578\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728 Transformer \u6a21\u578b\u4e2d\u9078\u64c7\u3001\u5c0d\u9f4a\u548c\u5408\u4f75\u7368\u7acb\u7684\u524d\u994b\u5b50\u5c64\uff0c\u4e26\u5728\u8a9e\u8a00\u5efa\u6a21\u3001\u5f71\u50cf\u5206\u985e\u548c\u6a5f\u5668\u7ffb\u8b6f\u4e0a\u6e2c\u8a66\u6211\u5011\u7684\u6a21\u578b\u3002\u4f7f\u7528\u6211\u5011\u7684\u6a21\u578b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u8207\u539f\u59cb\u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u5408\u4f75\u4e86\u8d85\u904e\u4e09\u5206\u4e4b\u4e00\u7684\u6a21\u578b\u524d\u994b\u5b50\u5c64\uff0c\u4e26\u5c55\u793a\u4e86\u6bd4\u5f37\u5927\u7684\u5c64\u526a\u679d\u57fa\u6e96\u66f4\u597d\u7684\u6548\u80fd\u3002\u4f8b\u5982\uff0c\u6211\u5011\u53ef\u4ee5\u5f9e Vision Transformer \u4e2d\u79fb\u9664\u8d85\u904e 21% \u7684\u7e3d\u53c3\u6578\uff0c\u540c\u6642\u7dad\u6301\u5176 99% \u7684\u539f\u59cb\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u67d0\u4e9b\u524d\u994b\u5b50\u5c64\u7d44\u5c55\u73fe\u51fa\u9ad8\u5ea6\u7684\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff0c\u9019\u53ef\u80fd\u6709\u52a9\u65bc\u89e3\u91cb\u5b83\u5011\u4ee4\u4eba\u9a5a\u8a1d\u7684\u53ef\u5408\u4f75\u6027\u3002", "author": "Neha Verma et.al.", "authors": "Neha Verma, Kenton Murray, Kevin Duh", "id": "2501.06126v1", "paper_url": "http://arxiv.org/abs/2501.06126v1", "repo": "https://github.com/nverma1/merging-ffs-compression"}}