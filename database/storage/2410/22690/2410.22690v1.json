{"2410.22690": {"publish_time": "2024-10-30", "title": "Choice between Partial Trajectories", "paper_summary": "As AI agents generate increasingly sophisticated behaviors, manually encoding\nhuman preferences to guide these agents becomes more challenging. To address\nthis, it has been suggested that agents instead learn preferences from human\nchoice data. This approach requires a model of choice behavior that the agent\ncan use to interpret the data. For choices between partial trajectories of\nstates and actions, previous models assume choice probabilities to be\ndetermined by the partial return or the cumulative advantage.\n  We consider an alternative model based instead on the bootstrapped return,\nwhich adds to the partial return an estimate of the future return. Benefits of\nthe bootstrapped return model stem from its treatment of human beliefs. Unlike\npartial return, choices based on bootstrapped return reflect human beliefs\nabout the environment. Further, while recovering the reward function from\nchoices based on cumulative advantage requires that those beliefs are correct,\ndoing so from choices based on bootstrapped return does not.\n  To motivate the bootstrapped return model, we formulate axioms and prove an\nAlignment Theorem. This result formalizes how, for a general class of human\npreferences, such models are able to disentangle goals from beliefs. This\nensures recovery of an aligned reward function when learning from choices based\non bootstrapped return.\n  The bootstrapped return model also affords greater robustness to choice\nbehavior. Even when choices are based on partial return, learning via a\nbootstrapped return model recovers an aligned reward function. The same holds\nwith choices based on the cumulative advantage if the human and the agent both\nadhere to correct and consistent beliefs about the environment. On the other\nhand, if choices are based on bootstrapped return, learning via partial return\nor cumulative advantage models does not generally produce an aligned reward\nfunction.", "paper_summary_zh": "\u96a8\u8457 AI \u4ee3\u7406\u7522\u751f\u8d8a\u4f86\u8d8a\u8907\u96dc\u7684\u884c\u70ba\uff0c\u624b\u52d5\u7de8\u78bc\u4eba\u985e\u504f\u597d\u4ee5\u5f15\u5c0e\u9019\u4e9b\u4ee3\u7406\u8b8a\u5f97\u66f4\u5177\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6709\u4eba\u5efa\u8b70\u4ee3\u7406\u6539\u70ba\u5f9e\u4eba\u985e\u9078\u64c7\u6578\u64da\u4e2d\u5b78\u7fd2\u504f\u597d\u3002\u9019\u7a2e\u65b9\u6cd5\u9700\u8981\u4e00\u500b\u9078\u64c7\u884c\u70ba\u6a21\u578b\uff0c\u4ee3\u7406\u53ef\u4ee5\u4f7f\u7528\u8a72\u6a21\u578b\u4f86\u89e3\u91cb\u6578\u64da\u3002\u5c0d\u65bc\u72c0\u614b\u548c\u52d5\u4f5c\u7684\u90e8\u5206\u8ecc\u8de1\u4e4b\u9593\u7684\u9078\u64c7\uff0c\u5148\u524d\u7684\u6a21\u578b\u5047\u8a2d\u9078\u64c7\u6a5f\u7387\u662f\u7531\u90e8\u5206\u56de\u5831\u6216\u7d2f\u7a4d\u512a\u52e2\u6c7a\u5b9a\u7684\u3002\n\u6211\u5011\u8003\u616e\u4e00\u500b\u57fa\u65bc\u81ea\u8209\u56de\u5831\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u5b83\u5728\u90e8\u5206\u56de\u5831\u4e2d\u589e\u52a0\u4e86\u5c0d\u672a\u4f86\u56de\u5831\u7684\u4f30\u8a08\u3002\u81ea\u8209\u56de\u5831\u6a21\u578b\u7684\u597d\u8655\u6e90\u65bc\u5b83\u5c0d\u4eba\u985e\u4fe1\u5ff5\u7684\u8655\u7406\u3002\u8207\u90e8\u5206\u56de\u5831\u4e0d\u540c\uff0c\u57fa\u65bc\u81ea\u8209\u56de\u5831\u7684\u9078\u64c7\u53cd\u6620\u4e86\u4eba\u985e\u5c0d\u74b0\u5883\u7684\u4fe1\u5ff5\u3002\u6b64\u5916\uff0c\u96d6\u7136\u5f9e\u57fa\u65bc\u7d2f\u7a4d\u512a\u52e2\u7684\u9078\u64c7\u4e2d\u6062\u5fa9\u734e\u52f5\u51fd\u6578\u9700\u8981\u9019\u4e9b\u4fe1\u5ff5\u662f\u6b63\u78ba\u7684\uff0c\u4f46\u5f9e\u57fa\u65bc\u81ea\u8209\u56de\u5831\u7684\u9078\u64c7\u4e2d\u9019\u6a23\u505a\u5247\u4e0d\u9700\u8981\u3002\n\u70ba\u4e86\u6fc0\u52f5\u81ea\u8209\u56de\u5831\u6a21\u578b\uff0c\u6211\u5011\u5236\u5b9a\u516c\u7406\u4e26\u8b49\u660e\u4e00\u500b\u5c0d\u9f4a\u5b9a\u7406\u3002\u9019\u500b\u7d50\u679c\u5f62\u5f0f\u5316\u4e86\u5c0d\u65bc\u4eba\u985e\u504f\u597d\u7684\u4e00\u822c\u985e\u5225\uff0c\u9019\u4e9b\u6a21\u578b\u5982\u4f55\u80fd\u5920\u5340\u5206\u76ee\u6a19\u548c\u4fe1\u5ff5\u3002\u9019\u78ba\u4fdd\u4e86\u5f9e\u57fa\u65bc\u81ea\u8209\u56de\u5831\u7684\u9078\u64c7\u4e2d\u5b78\u7fd2\u6642\uff0c\u5c0d\u9f4a\u734e\u52f5\u51fd\u6578\u7684\u6062\u5fa9\u3002\n\u81ea\u8209\u56de\u5831\u6a21\u578b\u9084\u63d0\u4f9b\u4e86\u5c0d\u9078\u64c7\u884c\u70ba\u66f4\u5927\u7684\u9b6f\u68d2\u6027\u3002\u5373\u4f7f\u9078\u64c7\u57fa\u65bc\u90e8\u5206\u56de\u5831\uff0c\u901a\u904e\u81ea\u8209\u56de\u5831\u6a21\u578b\u5b78\u7fd2\u4e5f\u6703\u6062\u5fa9\u5c0d\u9f4a\u7684\u734e\u52f5\u51fd\u6578\u3002\u5982\u679c\u4eba\u985e\u548c\u4ee3\u7406\u90fd\u5805\u6301\u5c0d\u74b0\u5883\u7684\u6b63\u78ba\u548c\u4e00\u81f4\u7684\u4fe1\u5ff5\uff0c\u90a3\u9ebc\u57fa\u65bc\u7d2f\u7a4d\u512a\u52e2\u7684\u9078\u64c7\u4e5f\u662f\u5982\u6b64\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5982\u679c\u9078\u64c7\u57fa\u65bc\u81ea\u8209\u56de\u5831\uff0c\u5247\u901a\u904e\u90e8\u5206\u56de\u5831\u6216\u7d2f\u7a4d\u512a\u52e2\u6a21\u578b\u5b78\u7fd2\u901a\u5e38\u4e0d\u6703\u7522\u751f\u5c0d\u9f4a\u7684\u734e\u52f5\u51fd\u6578\u3002", "author": "Henrik Marklund et.al.", "authors": "Henrik Marklund, Benjamin Van Roy", "id": "2410.22690v1", "paper_url": "http://arxiv.org/abs/2410.22690v1", "repo": "null"}}