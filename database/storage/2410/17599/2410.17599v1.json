{"2410.17599": {"publish_time": "2024-10-23", "title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training", "paper_summary": "The number of large language models (LLMs) with varying parameter scales and\nvocabularies is increasing. While they deliver powerful performance, they also\nface a set of common optimization needs to meet specific requirements or\nstandards, such as instruction following or avoiding the output of sensitive\ninformation from the real world. However, how to reuse the fine-tuning outcomes\nof one model to other models to reduce training costs remains a challenge. To\nbridge this gap, we introduce Cross-model Control (CMC), a method that improves\nmultiple LLMs in one-time training with a portable tiny language model.\nSpecifically, we have observed that the logit shift before and after\nfine-tuning is remarkably similar across different models. Based on this\ninsight, we incorporate a tiny language model with a minimal number of\nparameters. By training alongside a frozen template LLM, the tiny model gains\nthe capability to alter the logits output by the LLMs. To make this tiny\nlanguage model applicable to models with different vocabularies, we propose a\nnovel token mapping strategy named PM-MinED. We have conducted extensive\nexperiments on instruction tuning and unlearning tasks, demonstrating the\neffectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.", "paper_summary_zh": "\u968f\u7740\u53c2\u6570\u89c4\u6a21\u548c\u8bcd\u6c47\u91cf\u4e0d\u540c\u7684\u5404\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6570\u91cf\u7684\u589e\u52a0\u3002\u867d\u7136\u5b83\u4eec\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4e5f\u9762\u4e34\u7740\u4e00\u7ec4\u5171\u540c\u7684\u4f18\u5316\u9700\u6c42\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u8981\u6c42\u6216\u6807\u51c6\uff0c\u4f8b\u5982\u9075\u5faa\u6307\u4ee4\u6216\u907f\u514d\u8f93\u51fa\u6765\u81ea\u73b0\u5b9e\u4e16\u754c\u7684\u654f\u611f\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5982\u4f55\u5c06\u4e00\u4e2a\u6a21\u578b\u7684\u5fae\u8c03\u7ed3\u679c\u91cd\u65b0\u7528\u4e8e\u5176\u4ed6\u6a21\u578b\u4ee5\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8de8\u6a21\u578b\u63a7\u5236\uff08CMC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u4fbf\u643a\u5f0f\u5fae\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u4e2a LLM \u8fdb\u884c\u4e00\u6b21\u6027\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5fae\u8c03\u524d\u540e logit \u7684\u504f\u79fb\u5728\u4e0d\u540c\u7684\u6a21\u578b\u4e2d\u975e\u5e38\u76f8\u4f3c\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c1\u89e3\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u4e00\u4e2a\u5177\u6709\u6700\u5c11\u6570\u91cf\u53c2\u6570\u7684\u5fae\u578b\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u4e0e\u51bb\u7ed3\u7684\u6a21\u677f LLM \u4e00\u8d77\u8bad\u7ec3\uff0c\u5fae\u578b\u6a21\u578b\u83b7\u5f97\u4e86\u6539\u53d8 LLM \u8f93\u51fa\u7684 logit \u7684\u80fd\u529b\u3002\u4e3a\u4e86\u4f7f\u8fd9\u4e2a\u5fae\u578b\u8bed\u8a00\u6a21\u578b\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u8bcd\u6c47\u7684\u6a21\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PM-MinED \u7684\u65b0\u9896\u6807\u8bb0\u6620\u5c04\u7b56\u7565\u3002\u6211\u4eec\u5bf9\u6307\u4ee4\u8c03\u6574\u548c\u9057\u5fd8\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86 CMC \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/wujwyi/CMC \u83b7\u5f97\u3002", "author": "Jiayi Wu et.al.", "authors": "Jiayi Wu, Hao Sun, Hengyi Cai, Lixin Su, Shuaiqiang Wang, Dawei Yin, Xiang Li, Ming Gao", "id": "2410.17599v1", "paper_url": "http://arxiv.org/abs/2410.17599v1", "repo": "https://github.com/wujwyi/cmc"}}