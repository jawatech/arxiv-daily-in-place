{"2406.04820": {"publish_time": "2024-06-07", "title": "Navigating Efficiency in MobileViT through Gaussian Process on Global Architecture Factors", "paper_summary": "Numerous techniques have been meticulously designed to achieve optimal\narchitectures for convolutional neural networks (CNNs), yet a comparable focus\non vision transformers (ViTs) has been somewhat lacking. Despite the remarkable\nsuccess of ViTs in various vision tasks, their heavyweight nature presents\nchallenges of computational costs. In this paper, we leverage the Gaussian\nprocess to systematically explore the nonlinear and uncertain relationship\nbetween performance and global architecture factors of MobileViT, such as\nresolution, width, and depth including the depth of in-verted residual blocks\nand the depth of ViT blocks, and joint factors including resolution-depth and\nresolution-width. We present design principles twisting magic 4D cube of the\nglobal architecture factors that minimize model sizes and computational costs\nwith higher model accuracy. We introduce a formula for downsizing architectures\nby iteratively deriving smaller MobileViT V2, all while adhering to a specified\nconstraint of multiply-accumulate operations (MACs). Experiment results show\nthat our formula significantly outperforms CNNs and mobile ViTs across\ndiversified datasets", "paper_summary_zh": "\u70ba\u9054\u6210\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u7684\u6700\u4f73\u67b6\u69cb\uff0c\u5df2\u7cbe\u5fc3\u8a2d\u8a08\u51fa\u8a31\u591a\u6280\u8853\uff0c\u4f46\u5c0d\u8996\u89baTransformer (ViT) \u7684\u95dc\u6ce8\u537b\u76f8\u5c0d\u4e0d\u8db3\u3002\u5118\u7ba1 ViT \u5728\u5404\u7a2e\u8996\u89ba\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4f46\u5176\u9f90\u5927\u7279\u6027\u5c0d\u904b\u7b97\u6210\u672c\u69cb\u6210\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5229\u7528\u9ad8\u65af\u904e\u7a0b\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e MobileViT \u7684\u6548\u80fd\u8207\u6574\u9ad4\u67b6\u69cb\u56e0\u5b50\u4e4b\u9593\u7684\u975e\u7dda\u6027\u548c\u4e0d\u78ba\u5b9a\u95dc\u4fc2\uff0c\u4f8b\u5982\u89e3\u6790\u5ea6\u3001\u5bec\u5ea6\u548c\u6df1\u5ea6\uff0c\u5305\u62ec\u53cd\u8f49\u6b98\u5dee\u5340\u584a\u7684\u6df1\u5ea6\u548c ViT \u5340\u584a\u7684\u6df1\u5ea6\uff0c\u4ee5\u53ca\u89e3\u6790\u5ea6\u6df1\u5ea6\u548c\u89e3\u6790\u5ea6\u5bec\u5ea6\u7b49\u806f\u5408\u56e0\u5b50\u3002\u6211\u5011\u63d0\u51fa\u8a2d\u8a08\u539f\u5247\uff0c\u626d\u66f2\u6574\u9ad4\u67b6\u69cb\u56e0\u5b50\u7684 4D \u9b54\u8853\u65b9\u584a\uff0c\u4ee5\u6700\u5c0f\u5316\u6a21\u578b\u5927\u5c0f\u548c\u904b\u7b97\u6210\u672c\uff0c\u540c\u6642\u63d0\u9ad8\u6a21\u578b\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u5f15\u5165\u4e00\u500b\u516c\u5f0f\uff0c\u900f\u904e\u53cd\u8986\u63a8\u5c0e\u8f03\u5c0f\u7684 MobileViT V2 \u4f86\u7e2e\u5c0f\u67b6\u69cb\uff0c\u540c\u6642\u9075\u5b88\u4e58\u52a0\u904b\u7b97 (MAC) \u7684\u6307\u5b9a\u9650\u5236\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u516c\u5f0f\u5728\u591a\u5143\u5316\u7684\u8cc7\u6599\u96c6\u4e0a\u660e\u986f\u512a\u65bc CNN \u548c\u884c\u52d5\u88dd\u7f6e ViT", "author": "Ke Meng et.al.", "authors": "Ke Meng, Kai Chen", "id": "2406.04820v1", "paper_url": "http://arxiv.org/abs/2406.04820v1", "repo": "null"}}