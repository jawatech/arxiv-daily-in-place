{"2411.13868": {"publish_time": "2024-11-21", "title": "Robust Detection of Watermarks for Large Language Models Under Human Edits", "paper_summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.", "paper_summary_zh": "<paragraph>\u6d6e\u6c34\u5370\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\u4f86\u5340\u5206\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u751f\u6210\u7684\u6587\u5b57\u548c\u4eba\u5de5\u64b0\u5beb\u7684\u6587\u5b57\u3002\u7136\u800c\uff0c\u4eba\u5de5\u7de8\u8f2f\u5728 LLM \u751f\u6210\u7684\u6587\u5b57\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u6703\u7a00\u91cb\u6d6e\u6c34\u5370\u8a0a\u865f\uff0c\u5f9e\u800c\u986f\u8457\u964d\u4f4e\u73fe\u6709\u65b9\u6cd5\u7684\u5075\u6e2c\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u6df7\u5408\u6a21\u578b\u5075\u6e2c\u4f86\u5efa\u69cb\u4eba\u5de5\u7de8\u8f2f\uff0c\u4e26\u4ee5\u622a\u65b7\u7684\u512a\u826f\u64ec\u5408\u6aa2\u5b9a\u5f62\u5f0f\u63d0\u51fa\u4e00\u500b\u65b0\u65b9\u6cd5\uff0c\u7528\u65bc\u5075\u6e2c\u4eba\u5de5\u7de8\u8f2f\u4e0b\u7684\u6d6e\u6c34\u5370\u6587\u5b57\uff0c\u6211\u5011\u7a31\u4e4b\u70ba Tr-GoF\u3002\u6211\u5011\u8b49\u660e Tr-GoF \u6aa2\u5b9a\u5728\u5927\u91cf\u6587\u5b57\u4fee\u6539\u548c\u6d88\u5931\u7684\u6d6e\u6c34\u5370\u8a0a\u865f\u7684\u7279\u5b9a\u6f38\u8fd1\u7bc4\u570d\u5167\uff0c\u5728 Gumbel-max \u6d6e\u6c34\u5370\u7684\u7a69\u5065\u5075\u6e2c\u4e2d\u9054\u5230\u6700\u4f73\u5316\u3002\u91cd\u8981\u7684\u662f\uff0cTr-GoF \u4ee5\u300c\u81ea\u9069\u61c9\u300d\u65b9\u5f0f\u9054\u5230\u9019\u500b\u6700\u4f73\u5316\uff0c\u56e0\u70ba\u5b83\u4e0d\u9700\u8981\u4eba\u5de5\u7de8\u8f2f\u5c64\u7d1a\u6216 LLM \u6a5f\u7387\u898f\u683c\u7684\u7cbe\u78ba\u77e5\u8b58\uff0c\u9019\u8207\u6700\u4f73\u4f46\u4e0d\u53ef\u884c\u7684 (Neyman--Pearson) \u4f3c\u7136\u6bd4\u6aa2\u5b9a\u5f62\u6210\u5c0d\u6bd4\u3002\u6b64\u5916\uff0c\u6211\u5011\u78ba\u7acb Tr-GoF \u6aa2\u5b9a\u5728\u7279\u5b9a\u7bc4\u570d\u7684\u9069\u5ea6\u6587\u5b57\u4fee\u6539\u4e2d\u9054\u5230\u6700\u9ad8\u7684\u5075\u6e2c\u6548\u7387\u7387\u3002\u5f62\u6210\u9bae\u660e\u5c0d\u6bd4\u7684\u662f\uff0c\u6211\u5011\u8868\u660e\u73fe\u6709\u65b9\u6cd5\u6240\u63a1\u7528\u7684\u57fa\u65bc\u7e3d\u548c\u7684\u5075\u6e2c\u898f\u5247\u7121\u6cd5\u5728\u5169\u7a2e\u7bc4\u570d\u5167\u9054\u5230\u6700\u4f73\u7a69\u5065\u6027\uff0c\u56e0\u70ba\u5b83\u5011\u7d71\u8a08\u8cc7\u6599\u7684\u52a0\u6cd5\u6027\u8cea\u8f03\u7121\u6cd5\u62b5\u79a6\u7de8\u8f2f\u5f15\u8d77\u7684\u96dc\u8a0a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a Tr-GoF \u6aa2\u5b9a\u5728 OPT \u548c LLaMA \u5bb6\u65cf\u7684\u5408\u6210\u8cc7\u6599\u548c\u958b\u6e90 LLM \u4e0a\u5177\u6709\u7af6\u722d\u529b\uff0c\u6709\u6642\u751a\u81f3\u8868\u73fe\u512a\u7570\u3002</paragraph>", "author": "Xiang Li et.al.", "authors": "Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su", "id": "2411.13868v1", "paper_url": "http://arxiv.org/abs/2411.13868v1", "repo": "null"}}