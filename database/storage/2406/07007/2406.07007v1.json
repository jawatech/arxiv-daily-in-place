{"2406.07007": {"publish_time": "2024-06-11", "title": "Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference", "paper_summary": "The customization of large language models (LLMs) for user-specified tasks\ngets important. However, maintaining all the customized LLMs on cloud servers\nincurs substantial memory and computational overheads, and uploading user data\ncan also lead to privacy concerns. On-device LLMs can offer a promising\nsolution by mitigating these issues. Yet, the performance of on-device LLMs is\ninherently constrained by the limitations of small-scaled models. To overcome\nthese restrictions, we first propose Crayon, a novel approach for on-device LLM\ncustomization. Crayon begins by constructing a pool of diverse base adapters,\nand then we instantly blend them into a customized adapter without extra\ntraining. In addition, we develop a device-server hybrid inference strategy,\nwhich deftly allocates more demanding queries or non-customized tasks to a\nlarger, more capable LLM on a server. This ensures optimal performance without\nsacrificing the benefits of on-device customization. We carefully craft a novel\nbenchmark from multiple question-answer datasets, and show the efficacy of our\nmethod in the LLM customization.", "paper_summary_zh": "\u5c0d\u65bc\u7528\u6236\u6307\u5b9a\u4efb\u52d9\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u81ea\u8a02\u8b8a\u5f97\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u96f2\u7aef\u4f3a\u670d\u5668\u4e0a\u7dad\u8b77\u6240\u6709\u81ea\u8a02 LLM \u6703\u7522\u751f\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8ca0\u64d4\uff0c\u4e0a\u50b3\u4f7f\u7528\u8005\u8cc7\u6599\u4e5f\u53ef\u80fd\u5c0e\u81f4\u96b1\u79c1\u554f\u984c\u3002\u88dd\u7f6e\u5167 LLM \u53ef\u900f\u904e\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\u63d0\u4f9b\u4e00\u500b\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u88dd\u7f6e\u5167 LLM \u7684\u6548\u80fd\u672c\u8cea\u4e0a\u53d7\u5230\u5c0f\u898f\u6a21\u6a21\u578b\u9650\u5236\u7684\u7d04\u675f\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa Crayon\uff0c\u4e00\u7a2e\u7528\u65bc\u88dd\u7f6e\u5167 LLM \u81ea\u8a02\u7684\u65b0\u65b9\u6cd5\u3002Crayon \u7684\u7b2c\u4e00\u6b65\u662f\u5efa\u69cb\u4e00\u500b\u591a\u6a23\u5316\u7684\u57fa\u790e\u9069\u914d\u5668\u6c60\uff0c\u7136\u5f8c\u6211\u5011\u7acb\u5373\u5c07\u5b83\u5011\u6df7\u5408\u6210\u81ea\u8a02\u9069\u914d\u5668\uff0c\u800c\u7121\u9700\u984d\u5916\u8a13\u7df4\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u88dd\u7f6e\u4f3a\u670d\u5668\u6df7\u5408\u5f0f\u63a8\u8ad6\u7b56\u7565\uff0c\u5b83\u9748\u5de7\u5730\u5c07\u66f4\u56b4\u82db\u7684\u67e5\u8a62\u6216\u975e\u81ea\u8a02\u4efb\u52d9\u5206\u914d\u7d66\u4f3a\u670d\u5668\u4e0a\u66f4\u5927\u3001\u529f\u80fd\u66f4\u5f37\u5927\u7684 LLM\u3002\u9019\u53ef\u78ba\u4fdd\u6700\u4f73\u6548\u80fd\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u88dd\u7f6e\u5167\u81ea\u8a02\u7684\u597d\u8655\u3002\u6211\u5011\u5f9e\u591a\u500b\u554f\u7b54\u8cc7\u6599\u96c6\u4ed4\u7d30\u88fd\u4f5c\u4e00\u500b\u65b0\u57fa\u6e96\uff0c\u4e26\u5728 LLM \u81ea\u8a02\u4e2d\u5c55\u793a\u6211\u5011\u65b9\u6cd5\u7684\u529f\u6548\u3002", "author": "Jihwan Bang et.al.", "authors": "Jihwan Bang, Juntae Lee, Kyuhong Shim, Seunghan Yang, Simyung Chang", "id": "2406.07007v1", "paper_url": "http://arxiv.org/abs/2406.07007v1", "repo": "null"}}