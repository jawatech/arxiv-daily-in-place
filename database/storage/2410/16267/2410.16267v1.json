{"2410.16267": {"publish_time": "2024-10-21", "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs", "paper_summary": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86 xGen-MM-Vid (BLIP-3-Video)\uff1a\u4e00\u7a2e\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\uff0c\u9069\u7528\u65bc\u5f71\u7247\uff0c\u7279\u5225\u8a2d\u8a08\u7528\u65bc\u6709\u6548\u6355\u6349\u591a\u500b\u756b\u683c\u7684\u6642\u9593\u8cc7\u8a0a\u3002BLIP-3-Video \u9664\u4e86\u50b3\u7d71\u7684\u8996\u89ba\u6a19\u8a18\u5316\u5668\u5916\uff0c\u9084\u5229\u7528\u300c\u6642\u9593\u7de8\u78bc\u5668\u300d\uff0c\u5c07\u591a\u500b\u756b\u683c\u7684\u6a19\u8a18\u5e8f\u5217\u5c0d\u61c9\u5230\u4e00\u7d44\u7cbe\u7c21\u7684\u8996\u89ba\u6a19\u8a18\u3002\u9019\u4f7f\u5f97 BLIP3-Video \u80fd\u5920\u4f7f\u7528\u7684\u8996\u89ba\u6a19\u8a18\u6bd4\u7af6\u722d\u6a21\u578b\u5c11\u5f97\u591a\uff08\u4f8b\u5982\uff0c32 \u500b\u6a19\u8a18\u5c0d 4608 \u500b\u6a19\u8a18\uff09\u3002\u6211\u5011\u63a2\u7d22\u4e86\u4e0d\u540c\u985e\u578b\u7684\u6642\u9593\u7de8\u78bc\u5668\uff0c\u5305\u62ec\u53ef\u5b78\u7fd2\u7684\u6642\u7a7a\u6c60\u5316\u4ee5\u53ca\u5e8f\u5217\u6a21\u578b\uff0c\u4f8b\u5982\u6a19\u8a18\u5716\u9748\u6a5f\u3002\u6211\u5011\u900f\u904e\u5be6\u9a57\u78ba\u8a8d\uff0cBLIP-3-Video \u7372\u5f97\u7684\u5f71\u7247\u554f\u7b54\u6e96\u78ba\u5ea6\u53ef\u8207\u66f4\u5927\u7684\u6700\u5148\u9032\u6a21\u578b\uff08\u4f8b\u5982\uff0c34B\uff09\u76f8\u5ab2\u7f8e\uff0c\u540c\u6642\u9ad4\u7a4d\u5c0f\u5f97\u591a\uff08\u5373 4B\uff09\uff0c\u4e26\u4e14\u900f\u904e\u4f7f\u7528\u8f03\u5c11\u7684\u8996\u89ba\u6a19\u8a18\u800c\u66f4\u6709\u6548\u7387\u3002\u5c08\u6848\u7db2\u7ad9\u4f4d\u65bc https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html", "author": "Michael S. Ryoo et.al.", "authors": "Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles", "id": "2410.16267v1", "paper_url": "http://arxiv.org/abs/2410.16267v1", "repo": "null"}}