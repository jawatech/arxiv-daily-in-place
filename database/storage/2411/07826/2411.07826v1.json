{"2411.07826": {"publish_time": "2024-11-12", "title": "Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices", "paper_summary": "In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e Transformer \u7d50\u69cb\u4e3b\u5c0e\u4e86\u8a31\u591a\u6a5f\u5668\u5b78\u7fd2\u4efb\u52d9\uff0c\u7279\u5225\u662f\u6587\u672c\u8655\u7406\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6599\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u9020\u6210\u9ad8\u8cc7\u6e90\u9700\u6c42\uff0c\u7279\u5225\u662f\u5728\u5927\u91cf\u7684\u6d6e\u9ede\u904b\u7b97 (FLOP) \u548c\u6240\u9700\u7684\u9ad8\u8a18\u61b6\u9ad4\u91cf\u65b9\u9762\u3002\u70ba\u4e86\u4ee5\u53c3\u6578\u6709\u6548\u7684\u65b9\u5f0f\u5fae\u8abf\u6b64\u985e\u6a21\u578b\uff0c\u5df2\u958b\u767c\u51fa\u9069\u914d\u5668\u6216 LoRA \u7b49\u6280\u8853\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230 LoRA \u7684\u61c9\u7528\u5728\u806f\u5408\u5b78\u7fd2 (FL) \u4e2d\u4f7f\u7528\u6642\uff0c\u96d6\u7136\u4ecd\u7136\u662f\u53c3\u6578\u6709\u6548\u7684\uff0c\u4f46\u5728\u8a18\u61b6\u9ad4\u548c FLOP \u65b9\u9762\u537b\u6548\u7387\u4e0d\u5f70\u3002\u57fa\u65bc\u8a72\u89c0\u5bdf\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5c64\u5fae\u8abf\u65b9\u6848\uff0c\u5141\u8a31\u8de8\u88dd\u7f6e FL \u4e2d\u7684\u88dd\u7f6e\u4f7f\u7528\u9810\u8a13\u7df4\u795e\u7d93\u7db2\u8def (NN)\uff0c\u540c\u6642\u9075\u5b88\u65e2\u5b9a\u7684\u8cc7\u6e90\u9650\u5236\u3002\u6211\u5011\u8868\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6848\u5728\u8655\u7406\u540c\u8cea\u6216\u7570\u8cea\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9650\u5236\u6642\u512a\u65bc\u76ee\u524d\u7684\u6280\u8853\u6c34\u6e96\uff0c\u4e26\u4e14\u5728\u6709\u9650\u7684\u901a\u8a0a\u65b9\u9762\u8207 LoRA \u76f8\u7576\uff0c\u5f9e\u800c\u5be6\u73fe\u4e86 FL \u8a13\u7df4\u4e2d\u986f\u8457\u66f4\u9ad8\u7684\u6e96\u78ba\u5ea6\u3002", "author": "Kilian Pfeiffer et.al.", "authors": "Kilian Pfeiffer, Mohamed Aboelenien Ahmed, Ramin Khalili, J\u00f6rg Henkel", "id": "2411.07826v1", "paper_url": "http://arxiv.org/abs/2411.07826v1", "repo": "null"}}