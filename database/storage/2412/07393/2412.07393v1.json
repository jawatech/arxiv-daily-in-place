{"2412.07393": {"publish_time": "2024-12-10", "title": "CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models", "paper_summary": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u9069\u61c9\u8cc7\u6599\u3001\u4efb\u52d9\u548c\u4f7f\u7528\u8005\u504f\u597d\u7684\u6301\u7e8c\u8b8a\u5316\u3002\u7531\u65bc\u5176\u9f90\u5927\u898f\u6a21\u548c\u8207\u8a13\u7df4\u76f8\u95dc\u7684\u9ad8\u6210\u672c\uff0cLLM \u4e0d\u9069\u5408\u983b\u7e41\u7684\u91cd\u65b0\u8a13\u7df4\u3002\u7136\u800c\uff0c\u66f4\u65b0\u5c0d\u65bc\u8b93\u5b83\u5011\u8207\u5feb\u901f\u6f14\u5316\u7684\u77e5\u8b58\u4fdd\u6301\u540c\u6b65\u662f\u5fc5\u8981\u7684\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u58d3\u7e2e\u8a18\u61b6\u8a13\u7df4 (CMT) \u65b9\u6cd5\uff0c\u9019\u662f\u4e00\u7a2e\u91dd\u5c0d LLM \u7684\u9ad8\u6548\u4e14\u6709\u6548\u7684\u7dda\u4e0a\u9069\u61c9\u67b6\u69cb\uff0c\u5177\u6709\u5f37\u5927\u7684\u77e5\u8b58\u4fdd\u7559\u80fd\u529b\u3002\u53d7\u4eba\u985e\u8a18\u61b6\u6a5f\u5236\u555f\u767c\uff0cCMT \u58d3\u7e2e\u4e26\u5f9e\u65b0\u6587\u4ef6\u4e2d\u63d0\u53d6\u8cc7\u8a0a\uff0c\u5132\u5b58\u5728\u8a18\u61b6\u5eab\u4e2d\u3002\u5728\u56de\u7b54\u8207\u9019\u4e9b\u65b0\u6587\u4ef6\u76f8\u95dc\u7684\u67e5\u8a62\u6642\uff0c\u6a21\u578b\u6703\u5f9e\u8a18\u61b6\u5eab\u4e2d\u5f59\u7e3d\u9019\u4e9b\u6587\u4ef6\u8a18\u61b6\uff0c\u4ee5\u66f4\u597d\u5730\u56de\u7b54\u4f7f\u7528\u8005\u554f\u984c\u3002LLM \u672c\u8eab\u7684\u53c3\u6578\u5728\u8a13\u7df4\u548c\u63a8\u7406\u671f\u9593\u4e0d\u6703\u6539\u8b8a\uff0c\u964d\u4f4e\u4e86\u707d\u96e3\u6027\u907a\u5fd8\u7684\u98a8\u96aa\u3002\u70ba\u4e86\u589e\u5f37\u8a18\u61b6\u7684\u7de8\u78bc\u3001\u6aa2\u7d22\u548c\u5f59\u7e3d\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u4e09\u7a2e\u65b0\u7684\u901a\u7528\u4e14\u9748\u6d3b\u7684\u6280\u8853\uff0c\u5305\u62ec\u8a18\u61b6\u611f\u77e5\u76ee\u6a19\u3001\u81ea\u6211\u5339\u914d\u548c\u9802\u90e8\u5f59\u7e3d\u3002\u5728\u4e09\u500b\u6301\u7e8c\u5b78\u7fd2\u8cc7\u6599\u96c6 (\u5373 StreamingQA\u3001SQuAD \u548c ArchivalQA) \u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6539\u5584\u4e86\u6a21\u578b\u5728\u591a\u500b\u57fa\u790e LLM (\u4f8b\u5982\uff0c\u5728\u4f7f\u7528 Llama-2-7b \u7684 StreamingQA \u4e2d\uff0cEM +4.07 \u548c F1 +4.19) \u4e0a\u7684\u9069\u61c9\u6027\u548c\u5065\u58ef\u6027\u3002", "author": "Dongfang Li et.al.", "authors": "Dongfang Li, Zetian Sun, Xinshuo Hu, Baotian Hu, Min Zhang", "id": "2412.07393v1", "paper_url": "http://arxiv.org/abs/2412.07393v1", "repo": "null"}}