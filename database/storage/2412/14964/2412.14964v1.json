{"2412.14964": {"publish_time": "2024-12-19", "title": "Knowledge Injection via Prompt Distillation", "paper_summary": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.", "paper_summary_zh": "\u5728\u8a31\u591a\u5be6\u52d9\u61c9\u7528\u4e2d\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u6574\u5408\u9810\u8a13\u7df4\u8cc7\u6599\u4e2d\u6c92\u6709\u7684\u65b0\u77e5\u8b58\u3002\u4e3b\u8981\u65b9\u6cd5\u662f\u5fae\u8abf\u548c\u6aa2\u7d22\u589e\u5f37\u7522\u751f (RAG)\u3002\u5118\u7ba1 RAG \u5df2\u6210\u70ba\u77e5\u8b58\u6ce8\u5165\u7684\u7522\u696d\u6a19\u6e96\uff0c\u4f46\u5fae\u8abf\u5c1a\u672a\u7372\u5f97\u53ef\u6bd4\u8f03\u7684\u6210\u529f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u5fae\u8abf\u6280\u8853\uff0c\u7528\u65bc\u5b78\u7fd2\u65b0\u77e5\u8b58\uff0c\u4e26\u5c55\u793a\u5b83\u53ef\u4ee5\u9054\u5230 RAG \u7684\u6548\u80fd\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u57fa\u65bc\u81ea\u84b8\u993e\u65b9\u6cd5\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u63d0\u793a\u84b8\u993e\u3002\u9996\u5148\uff0c\u6211\u5011\u7522\u751f\u6709\u95dc\u65b0\u77e5\u8b58\u7684\u554f\u984c\u89e3\u7b54\u5c0d\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5fae\u8abf\u4e00\u500b\u5b78\u751f\u6a21\u578b\uff0c\u5728\u554f\u984c\u89e3\u7b54\u5c0d\u4e0a\u6a21\u4eff\u6559\u5e2b\u6a21\u578b\u7684\u8f38\u51fa\u5206\u4f48\uff0c\u800c\u6559\u5e2b\u6a21\u578b\u5728\u63d0\u793a\u4e2d\u53e6\u5916\u63a5\u6536\u65b0\u77e5\u8b58\u3002\u5b78\u751f\u6a21\u578b\u8207\u6559\u5e2b\u6a21\u578b\u76f8\u540c\uff0c\u9664\u4e86\u5b83\u914d\u5099\u4e86 LoRA \u9069\u914d\u5668\u3002\u6b64\u8a13\u7df4\u7a0b\u5e8f\u6709\u52a9\u65bc\u5c07\u6559\u5e2b\u63d0\u793a\u4e2d\u7684\u65b0\u77e5\u8b58\u84b8\u993e\u5230\u5b78\u751f\u7684\u6b0a\u91cd\u4e2d\u3002", "author": "Kalle Kujanp\u00e4\u00e4 et.al.", "authors": "Kalle Kujanp\u00e4\u00e4, Harri Valpola, Alexander Ilin", "id": "2412.14964v1", "paper_url": "http://arxiv.org/abs/2412.14964v1", "repo": "null"}}