{"2407.04371": {"publish_time": "2024-07-05", "title": "Exploiting the equivalence between quantum neural networks and perceptrons", "paper_summary": "Quantum machine learning models based on parametrized quantum circuits, also\ncalled quantum neural networks (QNNs), are considered to be among the most\npromising candidates for applications on near-term quantum devices. Here we\nexplore the expressivity and inductive bias of QNNs by exploiting an exact\nmapping from QNNs with inputs $x$ to classical perceptrons acting on $x \\otimes\nx$ (generalised to complex inputs). The simplicity of the perceptron\narchitecture allows us to provide clear examples of the shortcomings of current\nQNN models, and the many barriers they face to becoming useful general-purpose\nlearning algorithms. For example, a QNN with amplitude encoding cannot express\nthe Boolean parity function for $n\\geq 3$, which is but one of an exponential\nnumber of data structures that such a QNN is unable to express. Mapping a QNN\nto a classical perceptron simplifies training, allowing us to systematically\nstudy the inductive biases of other, more expressive embeddings on Boolean\ndata. Several popular embeddings primarily produce an inductive bias towards\nfunctions with low class balance, reducing their generalisation performance\ncompared to deep neural network architectures which exhibit much richer\ninductive biases. We explore two alternate strategies that move beyond standard\nQNNs. In the first, we use a QNN to help generate a classical DNN-inspired\nkernel. In the second we draw an analogy to the hierarchical structure of deep\nneural networks and construct a layered non-linear QNN that is provably fully\nexpressive on Boolean data, while also exhibiting a richer inductive bias than\nsimple QNNs. Finally, we discuss characteristics of the QNN literature that may\nobscure how hard it is to achieve quantum advantage over deep learning\nalgorithms on classical data.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u53c3\u6578\u5316\u91cf\u5b50\u96fb\u8def\uff0c\u4e5f\u7a31\u70ba\u91cf\u5b50\u795e\u7d93\u7db2\u8def (QNN) \u7684\u91cf\u5b50\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c\u88ab\u8a8d\u70ba\u662f\u8fd1\u671f\u91cf\u5b50\u88dd\u7f6e\u61c9\u7528\u4e2d\u6700\u6709\u524d\u9014\u7684\u5019\u9078\u8005\u4e4b\u4e00\u3002\u5728\u6b64\uff0c\u6211\u5011\u900f\u904e\u5229\u7528\u5f9e\u8f38\u5165 $x$ \u7684 QNN \u5230\u4f5c\u7528\u65bc $x \\otimes x$ \u7684\u7d93\u5178\u611f\u77e5\u5668\u7684\u7cbe\u78ba\u5c0d\u61c9\uff08\u63a8\u5ee3\u5230\u8907\u6578\u8f38\u5165\uff09\uff0c\u4f86\u63a2\u8a0e QNN \u7684\u8868\u9054\u80fd\u529b\u548c\u6b78\u7d0d\u504f\u8aa4\u3002\u611f\u77e5\u5668\u67b6\u69cb\u7684\u7c21\u6f54\u6027\u4f7f\u6211\u5011\u80fd\u5920\u63d0\u4f9b\u7576\u524d QNN \u6a21\u578b\u7f3a\u9ede\u7684\u660e\u78ba\u7bc4\u4f8b\uff0c\u4ee5\u53ca\u5b83\u5011\u6210\u70ba\u6709\u7528\u7684\u901a\u7528\u5b78\u7fd2\u6f14\u7b97\u6cd5\u6240\u9762\u81e8\u7684\u8a31\u591a\u969c\u7919\u3002\u4f8b\u5982\uff0c\u5177\u6709\u632f\u5e45\u7de8\u78bc\u7684 QNN \u7121\u6cd5\u8868\u9054 $n\\geq 3$ \u7684\u5e03\u6797\u540c\u4f4d\u51fd\u6578\uff0c\u800c\u9019\u53ea\u662f\u6b64\u985e QNN \u7121\u6cd5\u8868\u9054\u7684\u6307\u6578\u7d1a\u5225\u8cc7\u6599\u7d50\u69cb\u4e4b\u4e00\u3002\u5c07 QNN \u5c0d\u61c9\u5230\u7d93\u5178\u611f\u77e5\u5668\u7c21\u5316\u4e86\u8a13\u7df4\uff0c\u4f7f\u6211\u5011\u80fd\u5920\u7cfb\u7d71\u6027\u5730\u7814\u7a76\u5e03\u6797\u8cc7\u6599\u4e0a\u5176\u4ed6\u66f4\u5177\u8868\u9054\u529b\u7684\u5d4c\u5165\u7684\u6b78\u7d0d\u504f\u8aa4\u3002\u5e7e\u500b\u5ee3\u53d7\u6b61\u8fce\u7684\u5d4c\u5165\u4e3b\u8981\u7522\u751f\u5c0d\u985e\u5225\u5e73\u8861\u4f4e\u7684\u529f\u80fd\u7684\u6b78\u7d0d\u504f\u8aa4\uff0c\u8207\u8868\u73fe\u51fa\u66f4\u8c50\u5bcc\u6b78\u7d0d\u504f\u8aa4\u7684\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u67b6\u69cb\u76f8\u6bd4\uff0c\u964d\u4f4e\u4e86\u5b83\u5011\u7684\u6cdb\u5316\u6548\u80fd\u3002\u6211\u5011\u63a2\u7d22\u4e86\u8d85\u8d8a\u6a19\u6e96 QNN \u7684\u5169\u7a2e\u66ff\u4ee3\u7b56\u7565\u3002\u5728\u7b2c\u4e00\u500b\u7b56\u7565\u4e2d\uff0c\u6211\u5011\u4f7f\u7528 QNN \u4f86\u5e6b\u52a9\u7522\u751f\u7d93\u5178 DNN \u555f\u767c\u7684\u6838\u3002\u5728\u7b2c\u4e8c\u500b\u7b56\u7565\u4e2d\uff0c\u6211\u5011\u5c07\u985e\u6bd4\u65bc\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u968e\u5c64\u7d50\u69cb\uff0c\u4e26\u5efa\u69cb\u4e00\u500b\u5206\u5c64\u975e\u7dda\u6027 QNN\uff0c\u5b83\u5728\u5e03\u6797\u8cc7\u6599\u4e0a\u5df2\u88ab\u8b49\u660e\u5177\u6709\u5b8c\u5168\u7684\u8868\u9054\u80fd\u529b\uff0c\u540c\u6642\u4e5f\u8868\u73fe\u51fa\u6bd4\u7c21\u55ae QNN \u66f4\u8c50\u5bcc\u7684\u6b78\u7d0d\u504f\u8aa4\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86 QNN \u6587\u737b\u7684\u7279\u5fb5\uff0c\u9019\u4e9b\u7279\u5fb5\u53ef\u80fd\u6703\u6a21\u7cca\u5728\u7d93\u5178\u8cc7\u6599\u4e0a\u5be6\u73fe\u91cf\u5b50\u512a\u52e2\u512a\u65bc\u6df1\u5ea6\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7684\u96e3\u5ea6\u3002</paragraph>", "author": "Chris Mingard et.al.", "authors": "Chris Mingard, Jessica Pointing, Charles London, Yoonsoo Nam, Ard A. Louis", "id": "2407.04371v1", "paper_url": "http://arxiv.org/abs/2407.04371v1", "repo": "null"}}