{"2501.17399": {"publish_time": "2025-01-29", "title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs", "paper_summary": "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa MultiChallenge\uff0c\u4e00\u500b\u958b\u5275\u6027\u7684\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u4f7f\u7528\u8005\u9032\u884c\u591a\u8f2a\u5c0d\u8a71\u7684\u80fd\u529b\uff0c\u9019\u662f\u4e00\u500b\u5c0d\u5176\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u8a0e\u7684\u80fd\u529b\u3002MultiChallenge \u8b58\u5225\u51fa\u591a\u8f2a\u5c0d\u8a71\u4e2d\u7684\u56db\u7a2e\u985e\u5225\u6311\u6230\uff0c\u9019\u4e9b\u6311\u6230\u4e0d\u50c5\u5728\u7576\u524d\u4eba\u985e\u8207 LLM \u7684\u4e92\u52d5\u4e2d\u5e38\u898b\u4e14\u771f\u5be6\uff0c\u800c\u4e14\u5c0d\u65bc\u6240\u6709\u7576\u524d\u7684\u524d\u6cbf LLM \u4f86\u8aaa\u4e5f\u662f\u5177\u6709\u6311\u6230\u6027\u7684\u3002\u6240\u6709 4 \u500b\u6311\u6230\u540c\u6642\u9700\u8981\u6e96\u78ba\u7684\u6307\u4ee4\u9075\u5faa\u3001\u4e0a\u4e0b\u6587\u5206\u914d\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002\u6211\u5011\u9084\u958b\u767c\u4e86\u5177\u6709\u4f8b\u9805\u7d1a\u5225\u8a55\u5206\u6a19\u6e96\u7684 LLM \u4f5c\u70ba\u8a55\u5be9\uff0c\u4ee5\u4fc3\u9032\u4e00\u7a2e\u81ea\u52d5\u8a55\u4f30\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u8207\u7d93\u9a57\u8c50\u5bcc\u7684\u4eba\u985e\u8a55\u5206\u54e1\u9054\u6210\u516c\u5e73\u7684\u4e00\u81f4\u3002\u5118\u7ba1\u5728\u73fe\u6709\u7684\u591a\u8f2a\u8a55\u4f30\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u5f97\u5206\uff0c\u4f46\u6240\u6709\u524d\u6cbf\u6a21\u578b\u5728 MultiChallenge \u4e0a\u7684\u6e96\u78ba\u7387\u90fd\u4f4e\u65bc 50%\uff0c\u8868\u73fe\u6700\u597d\u7684 Claude 3.5 Sonnet\uff082024 \u5e74 6 \u6708\uff09\u50c5\u9054\u5230 41.4% \u7684\u5e73\u5747\u6e96\u78ba\u7387\u3002", "author": "Ved Sirdeshmukh et.al.", "authors": "Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing", "id": "2501.17399v1", "paper_url": "http://arxiv.org/abs/2501.17399v1", "repo": "https://github.com/ekwinox117/multi-challenge"}}