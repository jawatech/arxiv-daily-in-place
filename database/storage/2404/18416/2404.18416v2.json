{"2404.18416": {"publish_time": "2024-04-29", "title": "Capabilities of Gemini Models in Medicine", "paper_summary": "Excellence in a wide variety of medical applications poses considerable\nchallenges for AI, requiring advanced reasoning, access to up-to-date medical\nknowledge and understanding of complex multimodal data. Gemini models, with\nstrong general capabilities in multimodal and long-context reasoning, offer\nexciting possibilities in medicine. Building on these core strengths of Gemini,\nwe introduce Med-Gemini, a family of highly capable multimodal models that are\nspecialized in medicine with the ability to seamlessly use web search, and that\ncan be efficiently tailored to novel modalities using custom encoders. We\nevaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art\n(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every\nbenchmark where a direct comparison is viable, often by a wide margin. On the\npopular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves\nSoTA performance of 91.1% accuracy, using a novel uncertainty-guided search\nstrategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU\n(health & medicine), Med-Gemini improves over GPT-4V by an average relative\nmargin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context\ncapabilities through SoTA performance on a needle-in-a-haystack retrieval task\nfrom long de-identified health records and medical video question answering,\nsurpassing prior bespoke methods using only in-context learning. Finally,\nMed-Gemini's performance suggests real-world utility by surpassing human\nexperts on tasks such as medical text summarization, alongside demonstrations\nof promising potential for multimodal medical dialogue, medical research and\neducation. Taken together, our results offer compelling evidence for\nMed-Gemini's potential, although further rigorous evaluation will be crucial\nbefore real-world deployment in this safety-critical domain.", "paper_summary_zh": "", "author": "Khaled Saab et.al.", "authors": "Khaled Saab,Tao Tu,Wei-Hung Weng,Ryutaro Tanno,David Stutz,Ellery Wulczyn,Fan Zhang,Tim Strother,Chunjong Park,Elahe Vedadi,Juanma Zambrano Chaves,Szu-Yeu Hu,Mike Schaekermann,Aishwarya Kamath,Yong Cheng,David G. T. Barrett,Cathy Cheung,Basil Mustafa,Anil Palepu,Daniel McDuff,Le Hou,Tomer Golany,Luyang Liu,Jean-baptiste Alayrac,Neil Houlsby,Nenad Tomasev,Jan Freyberg,Charles Lau,Jonas Kemp,Jeremy Lai,Shekoofeh Azizi,Kimberly Kanada,SiWai Man,Kavita Kulkarni,Ruoxi Sun,Siamak Shakeri,Luheng He,Ben Caine,Albert Webson,Natasha Latysheva,Melvin Johnson,Philip Mansfield,Jian Lu,Ehud Rivlin,Jesper Anderson,Bradley Green,Renee Wong,Jonathan Krause,Jonathon Shlens,Ewa Dominowska,S. M. Ali Eslami,Katherine Chou,Claire Cui,Oriol Vinyals,Koray Kavukcuoglu,James Manyika,Jeff Dean,Demis Hassabis,Yossi Matias,Dale Webster,Joelle Barral,Greg Corrado,Christopher Semturs,S. Sara Mahdavi,Juraj Gottweis,Alan Karthikesalingam,Vivek Natarajan", "id": "2404.18416v2", "paper_url": "http://arxiv.org/abs/2404.18416v2", "repo": "null"}}