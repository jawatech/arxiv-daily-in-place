{"2405.17977": {"publish_time": "2024-05-28", "title": "Aligning to Thousands of Preferences via System Message Generalization", "paper_summary": "Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus.", "paper_summary_zh": "\u5118\u7ba1\u4eba\u985e\u5929\u751f\u5177\u6709\u591a\u5143\u50f9\u503c\u89c0\uff0c\u4f46\u73fe\u6709\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u6821\u6e96\u65b9\u6cd5\u901a\u5e38\u5047\u8a2d\u5c07 LLM \u8207\u4e00\u822c\u5927\u773e\u7684\u559c\u597d\u6821\u6e96\u662f\u6700\u4f73\u7684\u3002\u63a1\u7528\u66f4\u500b\u4eba\u5316\u7684 LLM \u6821\u6e96\u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6230\u5728\u65bc\u5176\u7f3a\u4e4f\u53ef\u64f4\u5145\u6027\uff0c\u56e0\u70ba\u5b83\u6d89\u53ca\u91cd\u8907\u53d6\u5f97\u559c\u597d\u8cc7\u6599\u4e26\u91dd\u5c0d\u6bcf\u500b\u500b\u4eba\u7684\u559c\u597d\u8a13\u7df4\u65b0\u7684\u734e\u52f5\u6a21\u578b\u548c LLM\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u7bc4\u4f8b\uff0c\u4f7f\u7528\u8005\u53ef\u4ee5\u5728\u7cfb\u7d71\u8a0a\u606f\u4e2d\u6307\u5b9a\u4ed6\u5011\u6700\u91cd\u8996\u7684\u5167\u5bb9\uff0c\u5f15\u5c0e LLM \u7684\u751f\u6210\u884c\u70ba\u4ee5\u66f4\u597d\u5730\u7b26\u5408\u4f7f\u7528\u8005\u7684\u610f\u5716\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u7684\u55ae\u7d14\u61c9\u7528\u4e26\u975e\u6613\u4e8b\uff0c\u56e0\u70ba LLM \u901a\u5e38\u662f\u5728\u7d71\u4e00\u7684\u7cfb\u7d71\u8a0a\u606f\uff08\u4f8b\u5982\u300c\u60a8\u662f\u4e00\u4f4d\u6709\u7528\u7684\u52a9\u7406\u300d\uff09\u4e0a\u8a13\u7df4\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u5c0d\u591a\u6a23\u4e14\u672a\u898b\u904e\u7684\u7cfb\u7d71\u8a0a\u606f\u9032\u884c\u6982\u62ec\u7684\u80fd\u529b\u3002\u70ba\u4e86\u6539\u5584\u9019\u7a2e\u6982\u62ec\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u591a\u9762\u5411\u96c6\u5408\uff0c\u4e00\u500b\u5305\u542b 192k \u7a2e\u50f9\u503c\u89c0\u7d44\u5408\u7684\u9996\u9078\u8cc7\u6599\u96c6\uff0c\u8d85\u8d8a\u4e86\u901a\u7528\u7684\u6709\u76ca\u6027\u548c\u7121\u5bb3\u6027\uff0c\u6db5\u84cb\u4e86 65k \u689d\u4f7f\u7528\u8005\u8aaa\u660e\u3002\u4f7f\u7528\u9019\u500b\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b\u540d\u70ba Janus \u7684 7B LLM\uff0c\u4e26\u5728 5 \u500b\u57fa\u6e96\uff08AlpacaEval 2.0\u3001FLASK\u3001Koala\u3001MT-Bench \u548c Self-Instruct\uff09\u4e2d\u7684 921 \u500b\u63d0\u793a\u4e0a\u5c0d\u5176\u9032\u884c\u6e2c\u8a66\uff0c\u65b9\u6cd5\u662f\u52a0\u5165\u5404\u7a2e\u53cd\u6620\u4f7f\u7528\u8005\u559c\u597d\u7684\u672a\u898b\u904e\u7cfb\u7d71\u8a0a\u606f\u3002Janus \u5206\u5225\u4ee5 75.2%\u300172.4% \u548c 66.4% \u7684\u5e73\u624b\u52a0\u7372\u52dd\u7387\u64ca\u6557\u4e86 Mistral 7B Instruct v0.2\u3001GPT-3.5 Turbo \u548c GPT-4\u3002\u4ee4\u4eba\u610f\u5916\u7684\u662f\uff0c\u5728\u4e09\u500b\u8457\u91cd\u65bc\u56de\u61c9\u6709\u76ca\u6027\u7684\u57fa\u6e96\uff08AlpacaEval 2.0\u3001MT-Bench\u3001Arena Hard Auto v0.1\uff09\u4e0a\uff0cJanus \u4e5f\u4ee5 +4.0%\u3001+0.1%\u3001+3.0% \u7684\u5e45\u5ea6\u52dd\u904e LLaMA 3 8B Instruct\uff0c\u9019\u5f37\u8abf\u4e86\u4f7f\u7528\u5927\u91cf\u7cfb\u7d71\u8a0a\u606f\u9032\u884c\u8a13\u7df4\u4e5f\u53ef\u4ee5\u589e\u5f37\u8207\u4e00\u822c\u5927\u773e\u559c\u597d\u7684\u6821\u6e96\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u96c6\u3001\u57fa\u6e96\u548c\u6a21\u578b\u53ef\u5728 https://github.com/kaistAI/Janus \u53d6\u5f97\u3002", "author": "Seongyun Lee et.al.", "authors": "Seongyun Lee, Sue Hyun Park, Seungone Kim, Minjoon Seo", "id": "2405.17977v1", "paper_url": "http://arxiv.org/abs/2405.17977v1", "repo": "null"}}