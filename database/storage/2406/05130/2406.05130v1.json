{"2406.05130": {"publish_time": "2024-06-07", "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models", "paper_summary": "Multimodal large language models (MLLMs) fine-tuned with multimodal\ninstruction datasets have demonstrated remarkable capabilities in multimodal\ntasks. However, fine-tuning all parameters of MLLMs has become challenging as\nthey usually contain billions of parameters. To address this issue, we study\nparameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify\neffective methods for enhancing the performance of MLLMs in scenarios where\nonly a limited number of parameters are trained. This paper conducts empirical\nstudies using four popular PEFT methods to fine-tune the LLM component of\nopen-source MLLMs. We present a comprehensive analysis that encompasses various\naspects, including the impact of PEFT methods on various models, parameters and\nlocation of the PEFT module, size of fine-tuning data, model stability based on\nPEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT\nmethods on seven datasets from two different categories: unseen and seen\ndatasets. Across all experiments, we show that the adapter is the\nbest-performing PEFT method. At the same time, fine-tuning the connector layers\nleads to improved performance in most MLLMs. Code and data are available at\nhttps://github.com/alenai97/PEFT-MLLM.git.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7ecf\u8fc7\u591a\u6a21\u6001\u6307\u4ee4\u6570\u636e\u96c6\u7684\u5fae\u8c03\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5fae\u8c03 MLLM \u7684\u6240\u6709\u53c2\u6570\u5df2\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5305\u542b\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7814\u7a76\u4e86 MLLM \u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u627e\u51fa\u5728\u4ec5\u8bad\u7ec3\u6709\u9650\u6570\u91cf\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a MLLM \u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002\u672c\u6587\u4f7f\u7528\u56db\u79cd\u6d41\u884c\u7684 PEFT \u65b9\u6cd5\u5bf9\u5f00\u6e90 MLLM \u7684 LLM \u7ec4\u4ef6\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790\uff0c\u6db5\u76d6\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec PEFT \u65b9\u6cd5\u5bf9\u5404\u79cd\u6a21\u578b\u3001\u53c2\u6570\u548c PEFT \u6a21\u5757\u7684\u4f4d\u7f6e\u3001\u5fae\u8c03\u6570\u636e\u7684\u5927\u5c0f\u3001\u57fa\u4e8e PEFT \u65b9\u6cd5\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u3001MLLM \u7684\u6cdb\u5316\u548c\u5e7b\u89c9\u7684\u5f71\u54cd\u3002\u6211\u4eec\u5bf9\u6765\u81ea\u4e24\u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u4e03\u4e2a\u6570\u636e\u96c6\uff08\u770b\u4e0d\u89c1\u7684\u6570\u636e\u96c6\u548c\u53ef\u89c1\u6570\u636e\u96c6\uff09\u8bc4\u4f30\u4e86\u56db\u79cd PEFT \u65b9\u6cd5\u3002\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8868\u660e\u9002\u914d\u5668\u662f\u6027\u80fd\u6700\u597d\u7684 PEFT \u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5fae\u8c03\u8fde\u63a5\u5c42\u4f1a\u5bfc\u81f4\u5927\u591a\u6570 MLLM \u7684\u6027\u80fd\u63d0\u9ad8\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/alenai97/PEFT-MLLM.git \u83b7\u5f97\u3002", "author": "Xiongtao Zhou et.al.", "authors": "Xiongtao Zhou, Jie He, Yuhua Ke, Guangyao Zhu, V\u00edctor Guti\u00e9rrez-Basulto, Jeff Z. Pan", "id": "2406.05130v1", "paper_url": "http://arxiv.org/abs/2406.05130v1", "repo": "null"}}