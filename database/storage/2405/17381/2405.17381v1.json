{"2405.17381": {"publish_time": "2024-05-27", "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention", "paper_summary": "We present Lightning Attention, the first linear attention implementation\nthat maintains a constant training speed for various sequence lengths under\nfixed memory consumption. Due to the issue with cumulative summation operations\n(cumsum), previous linear attention implementations cannot achieve their\ntheoretical advantage in a casual setting. However, this issue can be\neffectively solved by utilizing different attention calculation strategies to\ncompute the different parts of attention. Specifically, we split the attention\ncalculation into intra-blocks and inter-blocks and use conventional attention\ncomputation for intra-blocks and linear attention kernel tricks for\ninter-blocks. This eliminates the need for cumsum in the linear attention\ncalculation. Furthermore, a tiling technique is adopted through both forward\nand backward procedures to take full advantage of the GPU hardware. To enhance\naccuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new\narchitecture that is tailored to our lightning attention. We conduct rigorous\ntesting on standard and self-collected datasets with varying model sizes and\nsequence lengths. TNL is notably more efficient than other language models. In\naddition, benchmark results indicate that TNL performs on par with\nstate-of-the-art LLMs utilizing conventional transformer structures. The source\ncode is released at github.com/OpenNLPLab/TransnormerLLM.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa Lightning Attention\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7dda\u6027\u6ce8\u610f\u529b\u5be6\u4f5c\uff0c\u5728\u56fa\u5b9a\u8a18\u61b6\u9ad4\u6d88\u8017\u4e0b\uff0c\u53ef\u7dad\u6301\u5404\u7a2e\u5e8f\u5217\u9577\u5ea6\u7684\u6046\u5b9a\u8a13\u7df4\u901f\u5ea6\u3002\u7531\u65bc\u7d2f\u7a4d\u7e3d\u548c\u904b\u7b97 (cumsum) \u7684\u554f\u984c\uff0c\u5148\u524d\u7684\u7dda\u6027\u6ce8\u610f\u529b\u5be6\u4f5c\u7121\u6cd5\u5728\u4f11\u9592\u8a2d\u5b9a\u4e2d\u7372\u5f97\u7406\u8ad6\u512a\u52e2\u3002\u7136\u800c\uff0c\u9019\u500b\u554f\u984c\u53ef\u4ee5\u7528\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u8a08\u7b97\u7b56\u7565\u4f86\u6709\u6548\u89e3\u6c7a\uff0c\u4ee5\u8a08\u7b97\u6ce8\u610f\u529b\u7684\u4e0d\u540c\u90e8\u5206\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u6ce8\u610f\u529b\u8a08\u7b97\u62c6\u5206\u70ba\u5340\u584a\u5167\u548c\u5340\u584a\u9593\uff0c\u4e26\u5c0d\u5340\u584a\u5167\u4f7f\u7528\u50b3\u7d71\u7684\u6ce8\u610f\u529b\u8a08\u7b97\uff0c\u5c0d\u5340\u584a\u9593\u4f7f\u7528\u7dda\u6027\u6ce8\u610f\u529b\u6838\u6280\u5de7\u3002\u9019\u6d88\u9664\u4e86\u7dda\u6027\u6ce8\u610f\u529b\u8a08\u7b97\u4e2d\u5c0d cumsum \u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u5728\u6b63\u5411\u548c\u53cd\u5411\u7a0b\u5e8f\u4e2d\u90fd\u63a1\u7528\u5e73\u92ea\u6280\u8853\uff0c\u4ee5\u5145\u5206\u5229\u7528 GPU \u786c\u9ad4\u3002\u70ba\u4e86\u5728\u4fdd\u7559\u6548\u80fd\u7684\u540c\u6642\u63d0\u5347\u6e96\u78ba\u5ea6\uff0c\u6211\u5011\u5f15\u5165\u4e86 TransNormerLLM (TNL)\uff0c\u9019\u662f\u4e00\u7a2e\u91dd\u5c0d\u6211\u5011\u7684 lightning attention \u91cf\u8eab\u6253\u9020\u7684\u65b0\u67b6\u69cb\u3002\u6211\u5011\u5c0d\u6a19\u6e96\u548c\u81ea\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u9032\u884c\u56b4\u683c\u7684\u6e2c\u8a66\uff0c\u4e26\u63a1\u7528\u4e0d\u540c\u7684\u6a21\u578b\u5927\u5c0f\u548c\u5e8f\u5217\u9577\u5ea6\u3002TNL \u660e\u986f\u6bd4\u5176\u4ed6\u8a9e\u8a00\u6a21\u578b\u66f4\u6709\u6548\u7387\u3002\u6b64\u5916\uff0c\u57fa\u6e96\u6e2c\u8a66\u7d50\u679c\u986f\u793a\uff0cTNL \u7684\u6548\u80fd\u8207\u4f7f\u7528\u50b3\u7d71Transformer\u7d50\u69cb\u7684\u6700\u65b0 LLM \u76f8\u7576\u3002\u539f\u59cb\u78bc\u5df2\u767c\u5e03\u5728 github.com/OpenNLPLab/TransnormerLLM\u3002</paragraph>", "author": "Zhen Qin et.al.", "authors": "Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong", "id": "2405.17381v1", "paper_url": "http://arxiv.org/abs/2405.17381v1", "repo": "https://github.com/opennlplab/transnormerllm"}}