{"2410.16179": {"publish_time": "2024-10-21", "title": "MagicPIG: LSH Sampling for Efficient LLM Generation", "paper_summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.", "paper_summary_zh": "\u5177\u6709\u9577\u80cc\u666f\u8996\u7a97\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7372\u5f97\u5ee3\u6cdb\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u7528\u65bc\u907f\u514d\u91cd\u65b0\u8a08\u7b97\u7684 KV \u5feb\u53d6\u5df2\u6210\u70ba\u74f6\u9838\u3002\u5df2\u7d93\u63d0\u51fa\u5404\u7a2e\u52d5\u614b\u7a00\u758f\u6216\u57fa\u65bc TopK \u7684\u6ce8\u610f\u529b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u6ce8\u610f\u529b\u662f\u7a00\u758f\u7684\u5171\u540c\u898b\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u8868\u660e TopK \u6ce8\u610f\u529b\u672c\u8eab\u5728\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52d9\u4e2d\u6703\u5c0e\u81f4\u54c1\u8cea\u4e0b\u964d\uff0c\u56e0\u70ba\u6ce8\u610f\u529b\u4e26\u975e\u7e3d\u662f\u50cf\u9810\u671f\u7684\u90a3\u9ebc\u7a00\u758f\u3002\u8207\u5176\u9078\u64c7\u5177\u6709\u6700\u9ad8\u6ce8\u610f\u529b\u5206\u6578\u7684\u9375\u548c\u503c\uff0c\u4e0d\u5982\u4f7f\u7528\u5177\u6709\u7406\u8ad6\u4fdd\u8b49\u7684\u62bd\u6a23\u53ef\u4ee5\u70ba\u6ce8\u610f\u529b\u8f38\u51fa\u63d0\u4f9b\u66f4\u597d\u7684\u4f30\u8a08\u3002\u70ba\u4e86\u5728 LLM \u751f\u6210\u4e2d\u5be6\u73fe\u57fa\u65bc\u62bd\u6a23\u7684\u8fd1\u4f3c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MagicPIG\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u5c40\u90e8\u654f\u611f\u96dc\u6e4a (LSH) \u7684\u7570\u8cea\u7cfb\u7d71\u3002MagicPIG \u5927\u5e45\u6e1b\u5c11\u4e86\u6ce8\u610f\u529b\u8a08\u7b97\u7684\u5de5\u4f5c\u8ca0\u8f09\uff0c\u540c\u6642\u70ba\u5404\u7a2e\u4efb\u52d9\u4fdd\u7559\u4e86\u9ad8\u6e96\u78ba\u5ea6\u3002MagicPIG \u5132\u5b58 LSH \u96dc\u6e4a\u8868\uff0c\u4e26\u5728 CPU \u4e0a\u57f7\u884c\u6ce8\u610f\u529b\u8a08\u7b97\uff0c\u9019\u5141\u8a31\u5b83\u4f7f\u7528\u9ad8\u8fd1\u4f3c\u6e96\u78ba\u5ea6\u63d0\u4f9b\u66f4\u9577\u7684\u80cc\u666f\u548c\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\u3002MagicPIG \u53ef\u4ee5\u5c07\u5404\u7a2e GPU \u786c\u9ad4\u7684\u89e3\u78bc\u541e\u5410\u91cf\u63d0\u9ad8 $1.9\\sim3.9\\times$\uff0c\u4e26\u5728\u55ae\u500b RTX 4090 \u4e0a\u5be6\u73fe 110ms \u7684\u89e3\u78bc\u5ef6\u9072\uff0c\u9069\u7528\u65bc\u5177\u6709 96k \u4ee4\u724c\u80cc\u666f\u7684 Llama-3.1-8B-Instruct \u6a21\u578b\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728\n\\url{https://github.com/Infini-AI-Lab/MagicPIG} \u53d6\u5f97\u3002", "author": "Zhuoming Chen et.al.", "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen", "id": "2410.16179v1", "paper_url": "http://arxiv.org/abs/2410.16179v1", "repo": "null"}}