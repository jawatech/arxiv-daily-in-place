{"2406.18060": {"publish_time": "2024-06-26", "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "paper_summary": "Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.", "paper_summary_zh": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u4f46\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0c\u5b83\u5bf9\u5185\u5b58\u7684\u9700\u6c42\u4e5f\u8d8a\u6765\u8d8a\u5927\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u8fd1\u63d0\u51fa\u7684\u5185\u5b58\u9ad8\u6548\u96f6\u9636 (MeZO) \u65b9\u6cd5\u8bd5\u56fe\u4ec5\u4f7f\u7528\u524d\u5411\u4f20\u9012\u6765\u5fae\u8c03 LLM\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u53cd\u5411\u4f20\u64ad\u56fe\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u548c\u53d1\u6563\u7684\u9ad8\u98ce\u9669\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u91c7\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u96f6\u9636\u5f20\u91cf\u8bad\u7ec3\u81ea\u9002\u5e94 (AdaZeta) \u6846\u67b6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u63d0\u9ad8 ZO \u65b9\u6cd5\u7684\u6027\u80fd\u548c\u6536\u655b\u6027\u3002\u4e3a\u4e86\u589e\u5f3a\u7ef4\u5ea6\u76f8\u5173\u7684 ZO \u4f30\u8ba1\u7cbe\u5ea6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5feb\u901f\u524d\u5411\u3001\u4f4e\u53c2\u6570\u5f20\u91cf\u5316\u9002\u914d\u5668\u3002\u4e3a\u4e86\u89e3\u51b3\u5728\u5927\u89c4\u6a21 ZO \u5fae\u8c03\u4efb\u52a1\u4e2d\u7ecf\u5e38\u89c2\u5bdf\u5230\u7684\u53d1\u6563\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u67e5\u8be2\u6570\u91cf\u8ba1\u5212\uff0c\u4ee5\u4fdd\u8bc1\u6536\u655b\u6027\u3002\u5bf9 Roberta-Large \u548c Llama-2-7B \u6a21\u578b\u7684\u8be6\u7ec6\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 AdaZeta \u6846\u67b6\u5728\u51c6\u786e\u6027\u3001\u5185\u5b58\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Yifan Yang et.al.", "authors": "Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang", "id": "2406.18060v1", "paper_url": "http://arxiv.org/abs/2406.18060v1", "repo": "https://github.com/yifanycc/adazeta"}}