{"2411.16489": {"publish_time": "2024-11-25", "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?", "paper_summary": "This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.", "paper_summary_zh": "\u672c\u8ad6\u6587\u5c0d\u8907\u88fd OpenAI \u7684 O1 \u6a21\u578b\u80fd\u529b\u7684\u73fe\u6709\u65b9\u6cd5\u9032\u884c\u6279\u5224\u6027\u63a2\u8a0e\uff0c\u7279\u5225\u95dc\u6ce8\u77e5\u8b58\u84b8\u993e\u6280\u8853\u7684\u5ee3\u6cdb\u4f46\u7d93\u5e38\u672a\u516c\u958b\u7684\u4f7f\u7528\u3002\u96d6\u7136\u6211\u5011\u4e4b\u524d\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86 O1 \u8907\u88fd\u7684\u57fa\u672c\u6280\u8853\u8def\u5f91\uff0c\u4f46\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f9e O1 \u7684 API \u9032\u884c\u7c21\u55ae\u84b8\u993e\uff0c\u7d50\u5408\u76e3\u7763\u5fae\u8abf\uff0c\u53ef\u4ee5\u5728\u8907\u96dc\u7684\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u4e0a\u5be6\u73fe\u5353\u8d8a\u7684\u6027\u80fd\u3002\u901a\u904e\u5927\u91cf\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8868\u660e\u5728\u50c5\u6578\u842c\u500b\u6a23\u672c\u4e0a\u9032\u884c\u5fae\u8abf\u7684\u57fa\u672c\u6a21\u578b\uff0cO1 \u84b8\u993e\u7684\u9577\u671f\u601d\u8003\u93c8\u5728\u7f8e\u570b\u9080\u8acb\u8cfd\u6578\u5b78\u8003\u8a66 (AIME) \u4e0a\u512a\u65bc O1 \u9810\u89bd\uff0c\u4e14\u6280\u8853\u8907\u96dc\u6027\u6700\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u9650\u65bc\u6578\u5b78\u63a8\u7406\uff0c\u9084\u63a2\u7d22\u4e86 O1 \u84b8\u993e\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u5e7b\u89ba\u3001\u5b89\u5168\u6027\u3001\u958b\u653e\u57df QA\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5118\u7ba1\u50c5\u6839\u64da\u6578\u5b78\u554f\u984c\u89e3\u6c7a\u6578\u64da\u9032\u884c\u8a13\u7df4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u958b\u653e\u5f0f QA \u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u5f37\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e26\u4e14\u5728\u5fae\u8abf\u5f8c\u5c0d\u963f\u8adb\u5949\u627f\u7684\u5f71\u97ff\u986f\u8457\u964d\u4f4e\u3002\u6211\u5011\u6545\u610f\u516c\u958b\u9019\u4e00\u767c\u73fe\uff0c\u4ee5\u4fc3\u9032\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u900f\u660e\u5ea6\uff0c\u4e26\u6311\u6230\u8a72\u9818\u57df\u7576\u524d\u6280\u8853\u8072\u660e\u6a21\u7cca\u7684\u8da8\u52e2\u3002\u6211\u5011\u7684\u7814\u7a76\u5305\u62ec\uff1a(1) \u84b8\u993e\u904e\u7a0b\u53ca\u5176\u6709\u6548\u6027\u7684\u8a73\u7d30\u6280\u8853\u8aaa\u660e\uff0c(2) \u7528\u65bc\u8a55\u4f30\u548c\u5206\u985e O1 \u8907\u88fd\u5617\u8a66\u7684\u7d9c\u5408\u57fa\u6e96\u67b6\u69cb\uff0c\u57fa\u65bc\u5b83\u5011\u7684\u6280\u8853\u900f\u660e\u5ea6\u548c\u53ef\u8907\u88fd\u6027\uff0c(3) \u904e\u5ea6\u4f9d\u8cf4\u84b8\u993e\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u6f5b\u5728\u98a8\u96aa\u7684\u6279\u5224\u6027\u8a0e\u8ad6\uff0c\u6211\u5011\u7684\u5206\u6790\u6700\u7d42\u5f97\u51fa\u4e00\u500b\u81f3\u95dc\u91cd\u8981\u7684\u6158\u75db\u6559\u8a13\uff1a\u96d6\u7136\u8ffd\u6c42\u66f4\u5f37\u5927\u7684 AI \u7cfb\u7d71\u5f88\u91cd\u8981\uff0c\u4f46\u57f9\u990a\u4ee5\u7b2c\u4e00\u6027\u539f\u7406\u601d\u7dad\u70ba\u57fa\u790e\u7684\u7814\u7a76\u4eba\u54e1\u81f3\u95dc\u91cd\u8981\u3002", "author": "Zhen Huang et.al.", "authors": "Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu", "id": "2411.16489v1", "paper_url": "http://arxiv.org/abs/2411.16489v1", "repo": "https://github.com/gair-nlp/o1-journey"}}