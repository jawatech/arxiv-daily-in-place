{"2406.02886": {"publish_time": "2024-06-05", "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs", "paper_summary": "Large Language Models (LLMs) have exhibited impressive capabilities in\nvarious tasks, yet their vast parameter sizes restrict their applicability in\nresource-constrained settings. Knowledge distillation (KD) offers a viable\nsolution by transferring expertise from large teacher models to compact student\nmodels. However, traditional KD techniques face specific challenges when\napplied to LLMs, including restricted access to LLM outputs, significant\nteacher-student capacity gaps, and the inherited mis-calibration issue. In this\nwork, we present PLaD, a novel preference-based LLM distillation framework.\nPLaD exploits the teacher-student capacity discrepancy to generate\npseudo-preference pairs where teacher outputs are preferred over student\noutputs. Then, PLaD leverages a ranking loss to re-calibrate student's\nestimation of sequence likelihood, which steers the student's focus towards\nunderstanding the relative quality of outputs instead of simply imitating the\nteacher. PLaD bypasses the need for access to teacher LLM's internal states,\ntackles the student's expressivity limitations, and mitigates the student\nmis-calibration issue. Through extensive experiments on two sequence generation\ntasks and with various LLMs, we demonstrate the effectiveness of our proposed\nPLaD framework.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u5176\u9f90\u5927\u7684\u53c3\u6578\u898f\u6a21\u9650\u5236\u4e86\u5b83\u5011\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u7684\u9069\u7528\u6027\u3002\u77e5\u8b58\u84b8\u993e (KD) \u63d0\u4f9b\u4e86\u4e00\u7a2e\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u901a\u904e\u5c07\u5927\u578b\u6559\u5e2b\u6a21\u578b\u7684\u5c08\u696d\u77e5\u8b58\u8f49\u79fb\u5230\u7cbe\u7c21\u7684\u5b78\u751f\u6a21\u578b\u4e2d\u3002\u7136\u800c\uff0c\u7576\u61c9\u7528\u65bc LLM \u6642\uff0c\u50b3\u7d71\u7684 KD \u6280\u8853\u6703\u9762\u81e8\u5177\u9ad4\u6311\u6230\uff0c\u5305\u62ec\u5c0d LLM \u8f38\u51fa\u7684\u8a2a\u554f\u53d7\u9650\u3001\u6559\u5e2b\u548c\u5b78\u751f\u4e4b\u9593\u7684\u5bb9\u91cf\u5dee\u8ddd\u986f\u8457\u4ee5\u53ca\u7e7c\u627f\u7684\u932f\u8aa4\u6821\u6e96\u554f\u984c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 PLaD\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u57fa\u65bc\u504f\u597d\u7684 LLM \u84b8\u993e\u6846\u67b6\u3002PLaD \u5229\u7528\u6559\u5e2b\u548c\u5b78\u751f\u4e4b\u9593\u7684\u5bb9\u91cf\u5dee\u7570\u4f86\u751f\u6210\u507d\u504f\u597d\u5c0d\uff0c\u5176\u4e2d\u6559\u5e2b\u8f38\u51fa\u512a\u65bc\u5b78\u751f\u8f38\u51fa\u3002\u7136\u5f8c\uff0cPLaD \u5229\u7528\u6392\u540d\u640d\u5931\u4f86\u91cd\u65b0\u6821\u6e96\u5b78\u751f\u5c0d\u5e8f\u5217\u4f3c\u7136\u7684\u4f30\u8a08\uff0c\u9019\u5c07\u5b78\u751f\u7684\u6ce8\u610f\u529b\u5f15\u5c0e\u5230\u7406\u89e3\u8f38\u51fa\u7684\u76f8\u5c0d\u54c1\u8cea\uff0c\u800c\u4e0d\u662f\u7c21\u55ae\u5730\u6a21\u4eff\u6559\u5e2b\u3002PLaD \u907f\u958b\u4e86\u8a2a\u554f\u6559\u5e2b LLM \u5167\u90e8\u72c0\u614b\u7684\u9700\u6c42\uff0c\u89e3\u6c7a\u4e86\u5b78\u751f\u7684\u8868\u9054\u80fd\u529b\u9650\u5236\uff0c\u4e26\u6e1b\u8f15\u4e86\u5b78\u751f\u7684\u932f\u8aa4\u6821\u6e96\u554f\u984c\u3002\u901a\u904e\u5728\u5169\u500b\u5e8f\u5217\u751f\u6210\u4efb\u52d9\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4e26\u4f7f\u7528\u5404\u7a2e LLM\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684 PLaD \u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "author": "Rongzhi Zhang et.al.", "authors": "Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang", "id": "2406.02886v2", "paper_url": "http://arxiv.org/abs/2406.02886v2", "repo": "null"}}