{"2407.11681": {"publish_time": "2024-07-16", "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "paper_summary": "As Large Language Models (LLMs) grow dramatically in size, there is an\nincreasing trend in compressing and speeding up these models. Previous studies\nhave highlighted the usefulness of gradients for importance scoring in neural\nnetwork compressing, especially in pruning medium-size networks. However, the\nsubstantial memory requirements involved in calculating gradients with\nbackpropagation impede the utilization of gradients in guiding LLM pruning. As\na result, most pruning strategies for LLMs rely on gradient-free criteria, such\nas weight magnitudes or a mix of magnitudes and activations. In this paper, we\ndevise a hybrid pruning criterion, which appropriately integrates magnitude,\nactivation, and gradient to capitalize on feature map sensitivity for pruning\nLLMs. To overcome memory requirement barriers, we estimate gradients using only\nforward passes. Based on this, we propose a Memory-effIcieNt structured prunIng\nprocedure for LLMs (MINI-LLM) to remove no-critical channels and\nmulti-attention heads. Experimental results demonstrate the superior\nperformance of MINI-LLM over existing gradient-free methods on three LLMs:\nLLaMA, BLOOM, and OPT across various downstream tasks (classification,\nmultiple-choice, and generation), while MINI-LLM maintains a GPU memory\nfootprint akin to gradient-free methods.", "paper_summary_zh": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u89c4\u6a21\u7684\u6025\u5267\u589e\u957f\uff0c\u538b\u7f29\u548c\u52a0\u901f\u8fd9\u4e9b\u6a21\u578b\u7684\u8d8b\u52bf\u4e5f\u5728\u4e0d\u65ad\u589e\u52a0\u3002\u5148\u524d\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u68af\u5ea6\u5728\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u4e2d\u7528\u4e8e\u91cd\u8981\u6027\u8bc4\u5206\u7684\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u4fee\u526a\u4e2d\u7b49\u89c4\u6a21\u7684\u7f51\u7edc\u4e2d\u3002\u7136\u800c\uff0c\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u6240\u6d89\u53ca\u7684\u5927\u91cf\u5185\u5b58\u9700\u6c42\u963b\u788d\u4e86\u68af\u5ea6\u5728\u6307\u5bfc LLM \u4fee\u526a\u4e2d\u7684\u5229\u7528\u3002\u56e0\u6b64\uff0c\u5927\u591a\u6570 LLM \u7684\u4fee\u526a\u7b56\u7565\u4f9d\u8d56\u4e8e\u65e0\u68af\u5ea6\u7684\u6807\u51c6\uff0c\u4f8b\u5982\u6743\u91cd\u5927\u5c0f\u6216\u5927\u5c0f\u548c\u6fc0\u6d3b\u7684\u6df7\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6df7\u5408\u4fee\u526a\u6807\u51c6\uff0c\u8be5\u6807\u51c6\u9002\u5f53\u5730\u96c6\u6210\u4e86\u5927\u5c0f\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\uff0c\u4ee5\u5229\u7528\u7279\u5f81\u56fe\u654f\u611f\u6027\u6765\u4fee\u526a LLM\u3002\u4e3a\u4e86\u514b\u670d\u5185\u5b58\u9700\u6c42\u969c\u788d\uff0c\u6211\u4eec\u4ec5\u4f7f\u7528\u524d\u5411\u4f20\u9012\u6765\u4f30\u8ba1\u68af\u5ea6\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e LLM \u7684\u5185\u5b58\u9ad8\u6548\u7ed3\u6784\u5316\u4fee\u526a\u7a0b\u5e8f (MINI-LLM)\uff0c\u4ee5\u5220\u9664\u975e\u5173\u952e\u901a\u9053\u548c\u591a\u5934\u6ce8\u610f\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e09\u4e2a LLM\uff08LLaMA\u3001BLOOM \u548c OPT\uff09\u4e0a\uff0cMINI-LLM \u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u591a\u9879\u9009\u62e9\u548c\u751f\u6210\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u68af\u5ea6\u65b9\u6cd5\uff0c\u800c MINI-LLM \u4fdd\u6301\u4e86\u4e0e\u65e0\u68af\u5ea6\u65b9\u6cd5\u7c7b\u4f3c\u7684 GPU \u5185\u5b58\u5360\u7528\u7a7a\u95f4\u3002", "author": "Hongrong Cheng et.al.", "authors": "Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi", "id": "2407.11681v1", "paper_url": "http://arxiv.org/abs/2407.11681v1", "repo": "null"}}