{"2407.12994": {"publish_time": "2024-07-17", "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks", "paper_summary": "Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u4e0d\u540c\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u6548\u80fd\u3002\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347 LLM \u5df2\u6709\u80fd\u529b\u7684\u57fa\u790e\u4e0a\uff0c\u65bc\u5404\u7a2e NLP \u4efb\u52d9\u4e2d\u9054\u6210\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u626e\u6f14\u4e86\u95dc\u9375\u89d2\u8272\u3002\u63d0\u793a\u5de5\u7a0b\u9700\u8981\u64b0\u5beb\u7a31\u70ba\u63d0\u793a\u7684\u81ea\u7136\u8a9e\u8a00\u6307\u4ee4\uff0c\u4ee5\u7d50\u69cb\u5316\u7684\u65b9\u5f0f\u5f9e LLM \u4e2d\u5f15\u51fa\u77e5\u8b58\u3002\u8207\u5148\u524d\u7684\u6700\u5148\u9032 (SoTA) \u6a21\u578b\u4e0d\u540c\uff0c\u63d0\u793a\u5de5\u7a0b\u4e0d\u9700\u8981\u6839\u64da\u65e2\u5b9a\u7684 NLP \u4efb\u52d9\u9032\u884c\u5ee3\u6cdb\u7684\u53c3\u6578\u91cd\u65b0\u8a13\u7df4\u6216\u5fae\u8abf\uff0c\u56e0\u6b64\u50c5\u5728 LLM \u7684\u5167\u5d4c\u77e5\u8b58\u4e0a\u904b\u4f5c\u3002\u6b64\u5916\uff0cLLM \u611b\u597d\u8005\u53ef\u4ee5\u900f\u904e\u57fa\u672c\u7684\u81ea\u7136\u8a9e\u8a00\u5c0d\u8a71\u4ea4\u6d41\u6216\u63d0\u793a\u5de5\u7a0b\uff0c\u4f86\u667a\u80fd\u5730\u8403\u53d6 LLM \u7684\u77e5\u8b58\uff0c\u8b93\u8d8a\u4f86\u8d8a\u591a\u5373\u4f7f\u6c92\u6709\u6df1\u5165\u6578\u5b78\u6a5f\u5668\u5b78\u7fd2\u80cc\u666f\u7684\u4eba\uff0c\u4e5f\u80fd\u5920\u4f7f\u7528 LLM \u9032\u884c\u5be6\u9a57\u3002\u96a8\u8457\u63d0\u793a\u5de5\u7a0b\u5728\u904e\u53bb\u5169\u5e74\u7372\u5f97\u5ee3\u6cdb\u63a1\u7528\uff0c\u7814\u7a76\u4eba\u54e1\u63d0\u51fa\u4e86\u8a31\u591a\u5de5\u7a0b\u6280\u8853\uff0c\u570d\u7e5e\u8457\u63d0\u793a\u8a2d\u8a08\u4f86\u63d0\u5347\u5f9e LLM \u4e2d\u8403\u53d6\u8cc7\u8a0a\u7684\u6e96\u78ba\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7e3d\u7d50\u4e86\u4e0d\u540c\u7684\u63d0\u793a\u6280\u8853\uff0c\u4e26\u6839\u64da\u5b83\u5011\u88ab\u7528\u65bc\u7684\u4e0d\u540c NLP \u4efb\u52d9\uff0c\u5c07\u5b83\u5011\u6b78\u985e\u5728\u4e00\u8d77\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a73\u7d30\u8aaa\u660e\u4e86\u9019\u4e9b\u63d0\u793a\u7b56\u7565\u5728\u5c6c\u65bc\u8a72 NLP \u4efb\u52d9\u7684\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\uff0c\u8a0e\u8ad6\u4e86\u5c0d\u61c9\u4f7f\u7528\u7684 LLM\uff0c\u5c55\u793a\u4e86\u4e00\u500b\u5206\u985e\u5716\uff0c\u4e26\u8a0e\u8ad6\u4e86\u7279\u5b9a\u8cc7\u6599\u96c6\u53ef\u80fd\u7684 SoTA\u3002\u7e3d\u8a08\uff0c\u6211\u5011\u95b1\u8b80\u4e26\u5c55\u793a\u4e86 44 \u7bc7\u7814\u7a76\u8ad6\u6587\u7684\u8abf\u67e5\uff0c\u9019\u4e9b\u8ad6\u6587\u63a2\u8a0e\u4e86 29 \u500b\u4e0d\u540c\u7684 NLP \u4efb\u52d9\u4e2d\u7684 39 \u7a2e\u4e0d\u540c\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5176\u4e2d\u5927\u591a\u6578\u5df2\u65bc\u904e\u53bb\u5169\u5e74\u5167\u767c\u8868\u3002", "author": "Shubham Vatsal et.al.", "authors": "Shubham Vatsal, Harsh Dubey", "id": "2407.12994v1", "paper_url": "http://arxiv.org/abs/2407.12994v1", "repo": "null"}}