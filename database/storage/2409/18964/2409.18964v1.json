{"2409.18964": {"publish_time": "2024-09-27", "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "paper_summary": "We present PhysGen, a novel image-to-video generation method that converts a\nsingle image and an input condition (e.g., force and torque applied to an\nobject in the image) to produce a realistic, physically plausible, and\ntemporally consistent video. Our key insight is to integrate model-based\nphysical simulation with a data-driven video generation process, enabling\nplausible image-space dynamics. At the heart of our system are three core\ncomponents: (i) an image understanding module that effectively captures the\ngeometry, materials, and physical parameters of the image; (ii) an image-space\ndynamics simulation model that utilizes rigid-body physics and inferred\nparameters to simulate realistic behaviors; and (iii) an image-based rendering\nand refinement module that leverages generative video diffusion to produce\nrealistic video footage featuring the simulated motion. The resulting videos\nare realistic in both physics and appearance and are even precisely\ncontrollable, showcasing superior results over existing data-driven\nimage-to-video generation works through quantitative comparison and\ncomprehensive user study. PhysGen's resulting videos can be used for various\ndownstream applications, such as turning an image into a realistic animation or\nallowing users to interact with the image and create various dynamics. Project\npage: https://stevenlsw.github.io/physgen/", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa PhysGen\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u5f71\u50cf\u8f49\u5f71\u7247\u751f\u6210\u65b9\u6cd5\uff0c\u5b83\u5c07\u55ae\u4e00\u5f71\u50cf\u548c\u8f38\u5165\u689d\u4ef6\uff08\u4f8b\u5982\uff0c\u65bd\u52a0\u65bc\u5f71\u50cf\u4e2d\u7269\u9ad4\u7684\u529b\u77e9\u548c\u626d\u529b\uff09\u8f49\u63db\u70ba\u903c\u771f\u3001\u7269\u7406\u4e0a\u5408\u7406\u4e14\u6642\u9593\u4e00\u81f4\u7684\u5f71\u7247\u3002\u6211\u5011\u7684\u95dc\u9375\u898b\u89e3\u662f\u5c07\u57fa\u65bc\u6a21\u578b\u7684\u7269\u7406\u6a21\u64ec\u8207\u8cc7\u6599\u9a45\u52d5\u7684\u5f71\u7247\u751f\u6210\u7a0b\u5e8f\u6574\u5408\u5728\u4e00\u8d77\uff0c\u5be6\u73fe\u5408\u7406\u7684\u5f71\u50cf\u7a7a\u9593\u52d5\u614b\u3002\u6211\u5011\u7cfb\u7d71\u7684\u6838\u5fc3\u6709\u4e09\u500b\u7d44\u6210\u90e8\u5206\uff1a(i) \u6709\u6548\u64f7\u53d6\u5f71\u50cf\u7684\u5e7e\u4f55\u5f62\u72c0\u3001\u6750\u8cea\u548c\u7269\u7406\u53c3\u6578\u7684\u5f71\u50cf\u7406\u89e3\u6a21\u7d44\uff1b(ii) \u5229\u7528\u525b\u9ad4\u7269\u7406\u548c\u63a8\u8ad6\u53c3\u6578\u4f86\u6a21\u64ec\u903c\u771f\u884c\u70ba\u7684\u5f71\u50cf\u7a7a\u9593\u52d5\u614b\u6a21\u64ec\u6a21\u578b\uff1b(iii) \u5229\u7528\u751f\u6210\u5f0f\u5f71\u7247\u64f4\u6563\u4f86\u7522\u751f\u5177\u6709\u6a21\u64ec\u52d5\u4f5c\u7684\u903c\u771f\u5f71\u7247\u756b\u9762\u7684\u57fa\u65bc\u5f71\u50cf\u7684\u6e32\u67d3\u548c\u7cbe\u7149\u6a21\u7d44\u3002\u7522\u751f\u7684\u5f71\u7247\u5728\u7269\u7406\u548c\u5916\u89c0\u4e0a\u90fd\u975e\u5e38\u903c\u771f\uff0c\u751a\u81f3\u53ef\u4ee5\u7cbe\u78ba\u63a7\u5236\uff0c\u900f\u904e\u91cf\u5316\u6bd4\u8f03\u548c\u5168\u9762\u7684\u4f7f\u7528\u8005\u7814\u7a76\uff0c\u5c55\u793a\u51fa\u6bd4\u73fe\u6709\u7684\u8cc7\u6599\u9a45\u52d5\u5f71\u50cf\u8f49\u5f71\u7247\u751f\u6210\u4f5c\u54c1\u66f4\u597d\u7684\u7d50\u679c\u3002PhysGen \u7522\u751f\u5f71\u7247\u53ef\u61c9\u7528\u65bc\u5404\u7a2e\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f8b\u5982\u5c07\u5f71\u50cf\u8f49\u63db\u6210\u903c\u771f\u7684\u52d5\u756b\uff0c\u6216\u5141\u8a31\u4f7f\u7528\u8005\u8207\u5f71\u50cf\u4e92\u52d5\u4e26\u7522\u751f\u5404\u7a2e\u52d5\u614b\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://stevenlsw.github.io/physgen/", "author": "Shaowei Liu et.al.", "authors": "Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, Shenlong Wang", "id": "2409.18964v1", "paper_url": "http://arxiv.org/abs/2409.18964v1", "repo": "https://github.com/stevenlsw/physgen"}}