{"2412.05275": {"publish_time": "2024-12-06", "title": "MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models", "paper_summary": "Text-to-video models have demonstrated impressive capabilities in producing\ndiverse and captivating video content, showcasing a notable advancement in\ngenerative AI. However, these models generally lack fine-grained control over\nmotion patterns, limiting their practical applicability. We introduce\nMotionFlow, a novel framework designed for motion transfer in video diffusion\nmodels. Our method utilizes cross-attention maps to accurately capture and\nmanipulate spatial and temporal dynamics, enabling seamless motion transfers\nacross various contexts. Our approach does not require training and works on\ntest-time by leveraging the inherent capabilities of pre-trained video\ndiffusion models. In contrast to traditional approaches, which struggle with\ncomprehensive scene changes while maintaining consistent motion, MotionFlow\nsuccessfully handles such complex transformations through its attention-based\nmechanism. Our qualitative and quantitative experiments demonstrate that\nMotionFlow significantly outperforms existing models in both fidelity and\nversatility even during drastic scene alterations.", "paper_summary_zh": "\u6587\u672c\u5230\u5f71\u7247\u6a21\u578b\u5df2\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u80fd\u7522\u751f\u591a\u5143\u4e14\u5f15\u4eba\u5165\u52dd\u7684\u5f71\u7247\u5167\u5bb9\uff0c\u5c55\u73fe\u751f\u6210\u5f0f AI \u7684\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u5c0d\u52d5\u4f5c\u6a21\u5f0f\u7684\u7d30\u7dfb\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5176\u5be6\u7528\u6027\u3002\u6211\u5011\u5f15\u5165\u4e86 MotionFlow\uff0c\u4e00\u500b\u5c08\u70ba\u5f71\u7247\u64f4\u6563\u6a21\u578b\u4e2d\u7684\u52d5\u4f5c\u8f49\u79fb\u800c\u8a2d\u8a08\u7684\u65b0\u7a4e\u67b6\u69cb\u3002\u6211\u5011\u7684\u6a21\u578b\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5716\u4f86\u7cbe\u78ba\u6355\u6349\u548c\u63a7\u5236\u6642\u7a7a\u52d5\u614b\uff0c\u8b93\u52d5\u4f5c\u80fd\u5728\u5404\u7a2e\u60c5\u5883\u4e2d\u7121\u7e2b\u8f49\u79fb\u3002\u6211\u5011\u7684\u6a21\u578b\u4e0d\u9700\u8981\u8a13\u7df4\uff0c\u4e26\u5728\u6e2c\u8a66\u6642\u900f\u904e\u5229\u7528\u9810\u5148\u8a13\u7df4\u7684\u5f71\u7247\u64f4\u6563\u6a21\u578b\u7684\u5167\u5728\u80fd\u529b\u4f86\u904b\u4f5c\u3002\u8207\u50b3\u7d71\u65b9\u6cd5\u4e0d\u540c\uff0c\u50b3\u7d71\u65b9\u6cd5\u5728\u7dad\u6301\u4e00\u81f4\u52d5\u4f5c\u7684\u540c\u6642\u96e3\u4ee5\u8655\u7406\u5168\u9762\u7684\u5834\u666f\u8b8a\u5316\uff0cMotionFlow \u900f\u904e\u5176\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u6a5f\u5236\u6210\u529f\u5730\u8655\u7406\u4e86\u9019\u4e9b\u8907\u96dc\u7684\u8f49\u63db\u3002\u6211\u5011\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5be6\u9a57\u8b49\u660e\uff0c\u5373\u4f7f\u5728\u5287\u70c8\u7684\u5834\u666f\u8b8a\u5316\u4e2d\uff0cMotionFlow \u5728\u4fdd\u771f\u5ea6\u548c\u591a\u529f\u80fd\u6027\u65b9\u9762\u90fd\u660e\u986f\u512a\u65bc\u73fe\u6709\u6a21\u578b\u3002", "author": "Tuna Han Salih Meral et.al.", "authors": "Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, Pinar Yanardag", "id": "2412.05275v1", "paper_url": "http://arxiv.org/abs/2412.05275v1", "repo": "null"}}