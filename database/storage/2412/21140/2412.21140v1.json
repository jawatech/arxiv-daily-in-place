{"2412.21140": {"publish_time": "2024-12-30", "title": "Facilitating large language model Russian adaptation with Learned Embedding Propagation", "paper_summary": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6280\u8853\u7684\u5feb\u901f\u9032\u5c55\u5c0e\u81f4\u4e86\u529f\u80fd\u5f37\u5927\u7684\u958b\u6e90\u6307\u4ee4\u8abf\u6574 LLM \u7684\u5f15\u5165\uff0c\u5b83\u5011\u5177\u6709\u8207 GPT-4 \u7b49\u6700\u5148\u9032\u5c0d\u61c9\u6a21\u578b\u76f8\u540c\u7684\u6587\u672c\u751f\u6210\u54c1\u8cea\u3002\u5118\u7ba1\u6b64\u985e\u6a21\u578b\u7684\u51fa\u73fe\u52a0\u901f\u4e86\u5728\u654f\u611f\u8a0a\u606f\u74b0\u5883\u4e2d\u63a1\u7528 LLM \u6280\u8853\uff0c\u4f46\u6b64\u985e\u6a21\u578b\u7684\u4f5c\u8005\u4e26\u672a\u516c\u958b\u8907\u88fd\u7d50\u679c\u6240\u9700\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u5f9e\u800c\u4f7f\u6210\u5c31\u6210\u70ba\u6a21\u578b\u7368\u6709\u7684\u3002\u7531\u65bc\u9019\u4e9b\u958b\u6e90\u6a21\u578b\u4e5f\u662f\u591a\u8a9e\u8a00\u7684\uff0c\u9019\u53cd\u904e\u4f86\u53c8\u6e1b\u5c11\u4e86\u8a13\u7df4\u7279\u5b9a\u8a9e\u8a00 LLM \u7684\u597d\u8655\uff0c\u56e0\u70ba\u6539\u9032\u7684\u63a8\u8ad6\u8a08\u7b97\u6548\u7387\u6210\u70ba\u9019\u7a2e\u6602\u8cb4\u7a0b\u5e8f\u552f\u4e00\u6709\u4fdd\u8b49\u7684\u512a\u52e2\u3002\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u9078\u9805\uff0c\u4f8b\u5982\u8a5e\u5f59\u64f4\u5145\u548c\u5f8c\u7e8c\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u4e5f\u53d7\u5230\u7121\u6cd5\u7372\u5f97\u9ad8\u54c1\u8cea\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u7684\u963b\u7919\uff0c\u56e0\u70ba\u5b83\u662f\u5c0e\u81f4 LLM \u4efb\u52d9\u89e3\u6c7a\u80fd\u529b\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u70ba\u4e86\u89e3\u6c7a\u9650\u5236\u4e26\u964d\u4f4e\u8a9e\u8a00\u9069\u61c9\u7ba1\u7dda\u7684\u6210\u672c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5b78\u7fd2\u5d4c\u5165\u50b3\u64ad (LEP)\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5c0d\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\u7684\u8981\u6c42\u8f03\u4f4e\uff0c\u56e0\u70ba\u5b83\u5c0d\u73fe\u6709 LLM \u77e5\u8b58\u7684\u5f71\u97ff\u5f88\u5c0f\uff0c\u6211\u5011\u4f7f\u7528\u65b0\u7a4e\u7684\u81e8\u6642\u5d4c\u5165\u50b3\u64ad\u7a0b\u5e8f\u4f86\u52a0\u5f37\u9019\u7a2e\u5f71\u97ff\uff0c\u8a72\u7a0b\u5e8f\u5141\u8a31\u8df3\u904e\u6307\u4ee4\u8abf\u6574\u6b65\u9a5f\uff0c\u800c\u662f\u5c07\u65b0\u8a9e\u8a00\u77e5\u8b58\u76f4\u63a5\u690d\u5165\u4efb\u4f55\u73fe\u6709\u7684\u6307\u4ee4\u8abf\u6574\u8b8a\u9ad4\u4e2d\u3002\u6211\u5011\u8a55\u4f30\u4e86 LLaMa-3-8B \u548c Mistral-7B \u7684\u56db\u7a2e\u4fc4\u8a9e\u8a5e\u5f59\u6539\u7de8\uff0c\u8868\u660e LEP \u8207\u50b3\u7d71\u7684\u6307\u4ee4\u8abf\u6574\u65b9\u6cd5\u5177\u6709\u7af6\u722d\u529b\uff0c\u5be6\u73fe\u4e86\u8207 OpenChat 3.5 \u548c LLaMa-3-8B-Instruct \u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u901a\u904e\u81ea\u6821\u6e96\u548c\u6301\u7e8c\u8abf\u6574\u9032\u4e00\u6b65\u6539\u9032\u4e86\u4efb\u52d9\u89e3\u6c7a\u80fd\u529b\u3002", "author": "Mikhail Tikhomirov et.al.", "authors": "Mikhail Tikhomirov, Daniil Chernyshev", "id": "2412.21140v1", "paper_url": "http://arxiv.org/abs/2412.21140v1", "repo": "null"}}