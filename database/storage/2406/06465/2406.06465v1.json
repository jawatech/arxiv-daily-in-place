{"2406.06465": {"publish_time": "2024-06-10", "title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction", "paper_summary": "Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.", "paper_summary_zh": "<paragraph>\u6587\u5b57\u5f15\u5c0e\u5f71\u7247\u9810\u6e2c (TVP) \u6d89\u53ca\u6839\u64da\u6307\u4ee4\u5f9e\u521d\u59cb\u5f71\u683c\u9810\u6e2c\u672a\u4f86\u5f71\u683c\u7684\u52d5\u4f5c\uff0c\u9019\u5728\u865b\u64ec\u5be6\u5883\u3001\u6a5f\u5668\u4eba\u548c\u5167\u5bb9\u5275\u4f5c\u4e2d\u5177\u6709\u5ee3\u6cdb\u7684\u61c9\u7528\u3002\u5148\u524d\u7684 TVP \u65b9\u6cd5\u901a\u904e\u8abf\u6574 Stable Diffusion \u4f86\u57f7\u884c\u9019\u9805\u4efb\u52d9\uff0c\u53d6\u5f97\u91cd\u5927\u7684\u7a81\u7834\u3002\u7136\u800c\uff0c\u5b83\u5011\u9762\u81e8\u5f71\u683c\u4e00\u81f4\u6027\u548c\u6642\u9593\u7a69\u5b9a\u6027\u7684\u554f\u984c\uff0c\u4e3b\u8981\u662f\u7531\u65bc\u5f71\u7247\u8cc7\u6599\u96c6\u7684\u898f\u6a21\u6709\u9650\u3002\u6211\u5011\u89c0\u5bdf\u5230\u9810\u8a13\u7df4\u7684 Image2Video diffusion \u6a21\u578b\u64c1\u6709\u826f\u597d\u7684\u5f71\u7247\u52d5\u614b\u5148\u9a57\uff0c\u4f46\u5b83\u5011\u7f3a\u4e4f\u6587\u5b57\u63a7\u5236\u3002\u56e0\u6b64\uff0c\u5c07 Image2Video \u6a21\u578b\u8f49\u79fb\u4ee5\u5229\u7528\u5176\u5f71\u7247\u52d5\u614b\u5148\u9a57\uff0c\u540c\u6642\u6ce8\u5165\u6307\u4ee4\u63a7\u5236\u4ee5\u7522\u751f\u53ef\u63a7\u5236\u5f71\u7247\uff0c\u9019\u662f\u4e00\u9805\u6709\u610f\u7fa9\u4e14\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u70ba\u4e86\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4f86\u6839\u64da\u521d\u59cb\u5f71\u683c\u548c\u6587\u5b57\u6307\u4ee4\u9810\u6e2c\u672a\u4f86\u7684\u5f71\u7247\u72c0\u614b\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u96d9\u67e5\u8a62\u8f49\u63db\u5668 (DQFormer) \u67b6\u69cb\uff0c\u5b83\u5c07\u6307\u4ee4\u548c\u5f71\u683c\u6574\u5408\u5230\u672a\u4f86\u5f71\u683c\u9810\u6e2c\u7684\u689d\u4ef6\u5d4c\u5165\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u9577\u77ed\u671f\u6642\u9593\u9069\u914d\u5668\u548c\u7a7a\u9593\u9069\u914d\u5668\uff0c\u5b83\u5011\u53ef\u4ee5\u5feb\u901f\u5c07\u4e00\u822c\u5f71\u7247 diffusion \u6a21\u578b\u8f49\u79fb\u5230\u7279\u5b9a\u5834\u666f\uff0c\u4e14\u8a13\u7df4\u6210\u672c\u6975\u4f4e\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u56db\u500b\u8cc7\u6599\u96c6\u4e0a\u660e\u986f\u512a\u65bc\u6700\u5148\u9032\u7684\u6280\u8853\uff1aSomething Something V2\u3001Epic Kitchen-100\u3001Bridge Data \u548c UCF-101\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cAID \u5206\u5225\u5728 Bridge \u548c SSv2 \u4e0a\u9054\u5230\u4e86 91.2% \u548c 55.5% \u7684 FVD \u6539\u9032\uff0c\u8b49\u660e\u4e86\u5b83\u5728\u5404\u7a2e\u9818\u57df\u7684\u6709\u6548\u6027\u3002\u66f4\u591a\u7bc4\u4f8b\u53ef\u4ee5\u5728\u6211\u5011\u7684\u7db2\u7ad9 https://chenhsing.github.io/AID \u4e2d\u627e\u5230\u3002</paragraph>", "author": "Zhen Xing et.al.", "authors": "Zhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, Yu-Gang Jiang", "id": "2406.06465v1", "paper_url": "http://arxiv.org/abs/2406.06465v1", "repo": "null"}}