{"2406.11546": {"publish_time": "2024-06-17", "title": "GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement", "paper_summary": "The evolution of speech technology has been spurred by the rapid increase in\ndataset sizes. Traditional speech models generally depend on a large amount of\nlabeled training data, which is scarce for low-resource languages. This paper\npresents GigaSpeech 2, a large-scale, multi-domain, multilingual speech\nrecognition corpus. It is designed for low-resource languages and does not rely\non paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of\nautomatically transcribed speech, including Thai, Indonesian, and Vietnamese,\ngathered from unlabeled YouTube videos. We also introduce an automated pipeline\nfor data crawling, transcription, and label refinement. Specifically, this\npipeline uses Whisper for initial transcription and TorchAudio for forced\nalignment, combined with multi-dimensional filtering for data quality\nassurance. A modified Noisy Student Training is developed to further refine\nflawed pseudo labels iteratively, thus enhancing model performance.\nExperimental results on our manually transcribed evaluation set and two public\ntest sets from Common Voice and FLEURS confirm our corpus's high quality and\nbroad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the\nword error rate for Thai, Indonesian, and Vietnamese on our challenging and\nrealistic YouTube test set by 25% to 40% compared to the Whisper large-v3\nmodel, with merely 10% model parameters. Furthermore, our ASR models trained on\nGigaspeech 2 yield superior performance compared to commercial services. We\nbelieve that our newly introduced corpus and pipeline will open a new avenue\nfor low-resource speech recognition and significantly facilitate research in\nthis area.", "paper_summary_zh": "<paragraph>\u8a9e\u97f3\u6280\u8853\u7684\u6f14\u9032\u53d7\u5230\u8cc7\u6599\u96c6\u898f\u6a21\u5feb\u901f\u589e\u52a0\u7684\u523a\u6fc0\u3002\u50b3\u7d71\u7684\u8a9e\u97f3\u6a21\u578b\u901a\u5e38\u4f9d\u8cf4\u65bc\u5927\u91cf\u7684\u6a19\u8a18\u8a13\u7df4\u8cc7\u6599\uff0c\u800c\u9019\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4f86\u8aaa\u5f88\u7a00\u5c11\u3002\u672c\u6587\u63d0\u51fa\u4e86 GigaSpeech 2\uff0c\u4e00\u500b\u5927\u578b\u3001\u591a\u9818\u57df\u3001\u591a\u8a9e\u8a00\u7684\u8a9e\u97f3\u8fa8\u8b58\u8a9e\u6599\u5eab\u3002\u5b83\u662f\u70ba\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u8a2d\u8a08\u7684\uff0c\u4e14\u4e0d\u4f9d\u8cf4\u6210\u5c0d\u7684\u8a9e\u97f3\u548c\u6587\u5b57\u8cc7\u6599\u3002GigaSpeech 2 \u5305\u542b\u7d04 30,000 \u5c0f\u6642\u7684\u81ea\u52d5\u8f49\u9304\u8a9e\u97f3\uff0c\u5305\u62ec\u6cf0\u8a9e\u3001\u5370\u5c3c\u8a9e\u548c\u8d8a\u5357\u8a9e\uff0c\u9019\u4e9b\u8a9e\u97f3\u662f\u5f9e\u672a\u6a19\u8a18\u7684 YouTube \u5f71\u7247\u4e2d\u6536\u96c6\u4f86\u7684\u3002\u6211\u5011\u9084\u5c0e\u5165\u4e86\u4e00\u500b\u7528\u65bc\u8cc7\u6599\u722c\u53d6\u3001\u8f49\u9304\u548c\u6a19\u7c64\u7cbe\u7149\u7684\u81ea\u52d5\u5316\u7ba1\u9053\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u9019\u500b\u7ba1\u9053\u4f7f\u7528 Whisper \u9032\u884c\u521d\u59cb\u8f49\u9304\uff0c\u4e26\u4f7f\u7528 TorchAudio \u9032\u884c\u5f37\u5236\u6bd4\u5c0d\uff0c\u7d50\u5408\u591a\u7dad\u5ea6\u904e\u6ffe\u4f86\u78ba\u4fdd\u8cc7\u6599\u54c1\u8cea\u3002\u958b\u767c\u4e86\u4e00\u500b\u6539\u826f\u7684 Noisy Student Training\uff0c\u7528\u65bc\u9032\u4e00\u6b65\u53cd\u8986\u7cbe\u7149\u6709\u7f3a\u9677\u7684\u507d\u6a19\u7c64\uff0c\u5f9e\u800c\u589e\u5f37\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u624b\u52d5\u8f49\u9304\u7684\u8a55\u4f30\u96c6\u548c\u4f86\u81ea Common Voice \u548c FLEURS \u7684\u5169\u500b\u516c\u958b\u6e2c\u8a66\u96c6\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u5be6\u4e86\u6211\u5011\u8a9e\u6599\u5eab\u7684\u9ad8\u54c1\u8cea\u548c\u5ee3\u6cdb\u7684\u9069\u7528\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 GigaSpeech 2 \u4e0a\u8a13\u7df4\u7684 ASR \u6a21\u578b\u53ef\u4ee5\u5c07\u6211\u5011\u5177\u6709\u6311\u6230\u6027\u548c\u5be6\u969b\u6027\u7684 YouTube \u6e2c\u8a66\u96c6\u4e0a\u6cf0\u8a9e\u3001\u5370\u5c3c\u8a9e\u548c\u8d8a\u5357\u8a9e\u7684\u5b57\u5143\u932f\u8aa4\u7387\u964d\u4f4e 25% \u5230 40%\uff0c\u800c Whisper large-v3 \u6a21\u578b\u50c5\u6709 10% \u7684\u6a21\u578b\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728 Gigaspeech 2 \u4e0a\u8a13\u7df4\u7684 ASR \u6a21\u578b\u8207\u5546\u696d\u670d\u52d9\u76f8\u6bd4\uff0c\u8868\u73fe\u51fa\u66f4\u512a\u7570\u7684\u6548\u80fd\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u65b0\u63a8\u51fa\u7684\u8a9e\u6599\u5eab\u548c\u7ba1\u9053\u5c07\u70ba\u4f4e\u8cc7\u6e90\u8a9e\u97f3\u8fa8\u8b58\u958b\u555f\u4e00\u689d\u65b0\u9014\u5f91\uff0c\u4e26\u986f\u8457\u4fc3\u9032\u9019\u65b9\u9762\u7684\u7814\u7a76\u3002</paragraph>", "author": "Yifan Yang et.al.", "authors": "Yifan Yang, Zheshu Song, Jianheng Zhuo, Mingyu Cui, Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying Liu, Ziyuan Wang, Ke Li, Shuai Fan, Kai Yu, Wei-Qiang Zhang, Guoguo Chen, Xie Chen", "id": "2406.11546v1", "paper_url": "http://arxiv.org/abs/2406.11546v1", "repo": "null"}}