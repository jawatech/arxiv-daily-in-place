{"2410.19313": {"publish_time": "2024-10-25", "title": "COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training", "paper_summary": "FP8 training has emerged as a promising method for improving training\nefficiency. Existing frameworks accelerate training by applying FP8 computation\nto linear layers while leaving optimizer states and activations in higher\nprecision, which fails to fully optimize memory usage. This paper introduces\nCOAT (Compressing Optimizer States and Activations for FP8 Training), a novel\nFP8 training framework designed to significantly reduce memory footprint when\ntraining large models. COAT addresses current limitations through two key\ninnovations: (1) Dynamic Range Expansion, which aligns optimizer state\ndistributions more closely with the FP8 representation range, thereby reducing\nquantization error, and (2) Mixed-Granularity Activation Quantization, which\noptimizes activation memory using a combination of per-tensor and per-group\nquantization strategies. Experiments demonstrate that COAT effectively reduces\nend-to-end training memory footprint by 1.54x compared to BF16 while achieving\nnearly lossless performance across various tasks, such as Large Language Model\npretraining and fine-tuning and Vision Language Model training. COAT also\nachieves a 1.43x end-to-end training speedup compared to BF16, performing on\npar with or surpassing TransformerEngine's speedup. COAT enables efficient\nfull-parameter training of large models on fewer GPUs, and facilitates doubling\nthe batch size in distributed training settings, providing a practical solution\nfor scaling large-scale model training. The code is available at\nhttps://github.com/NVlabs/COAT.", "paper_summary_zh": "FP8 \u8a13\u7df4\u5df2\u6210\u70ba\u6539\u5584\u8a13\u7df4\u6548\u7387\u7684\u6709\u524d\u9014\u65b9\u6cd5\u3002\u73fe\u6709\u6846\u67b6\u900f\u904e\u5c07 FP8 \u8a08\u7b97\u61c9\u7528\u65bc\u7dda\u6027\u5c64\uff0c\u540c\u6642\u5c07\u512a\u5316\u5668\u72c0\u614b\u548c\u6fc0\u6d3b\u4fdd\u7559\u5728\u8f03\u9ad8\u7cbe\u5ea6\uff0c\u4f86\u52a0\u901f\u8a13\u7df4\uff0c\u9019\u7121\u6cd5\u5b8c\u5168\u6700\u4f73\u5316\u8a18\u61b6\u9ad4\u4f7f\u7528\u7387\u3002\u672c\u6587\u4ecb\u7d39 COAT\uff08\u58d3\u7e2e\u512a\u5316\u5668\u72c0\u614b\u548c\u6fc0\u6d3b\u4ee5\u9032\u884c FP8 \u8a13\u7df4\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684 FP8 \u8a13\u7df4\u6846\u67b6\uff0c\u65e8\u5728\u5728\u8a13\u7df4\u5927\u578b\u6a21\u578b\u6642\u5927\u5e45\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002COAT \u900f\u904e\u5169\u9805\u95dc\u9375\u5275\u65b0\u4f86\u89e3\u6c7a\u7576\u524d\u7684\u9650\u5236\uff1a(1) \u52d5\u614b\u7bc4\u570d\u64f4\u5c55\uff0c\u5b83\u8b93\u512a\u5316\u5668\u72c0\u614b\u5206\u4f48\u66f4\u63a5\u8fd1 FP8 \u8868\u793a\u7bc4\u570d\uff0c\u5f9e\u800c\u6e1b\u5c11\u91cf\u5316\u8aa4\u5dee\uff0c\u4ee5\u53ca (2) \u6df7\u5408\u7c92\u5ea6\u6fc0\u6d3b\u91cf\u5316\uff0c\u5b83\u4f7f\u7528\u6bcf\u500b\u5f35\u91cf\u548c\u6bcf\u500b\u7fa4\u7d44\u91cf\u5316\u7b56\u7565\u7684\u7d44\u5408\u4f86\u6700\u4f73\u5316\u6fc0\u6d3b\u8a18\u61b6\u9ad4\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u8207 BF16 \u76f8\u6bd4\uff0cCOAT \u6709\u6548\u5730\u5c07\u7aef\u5230\u7aef\u8a13\u7df4\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11 1.54 \u500d\uff0c\u540c\u6642\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5be6\u73fe\u8fd1\u4e4e\u7121\u640d\u5931\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u548c\u5fae\u8abf\uff0c\u4ee5\u53ca\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u3002\u8207 BF16 \u76f8\u6bd4\uff0cCOAT \u9084\u5be6\u73fe\u4e86 1.43 \u500d\u7684\u7aef\u5230\u7aef\u8a13\u7df4\u52a0\u901f\uff0c\u6548\u80fd\u8207 TransformerEngine \u7684\u52a0\u901f\u76f8\u7576\u6216\u8d85\u8d8a\u3002COAT \u80fd\u8b93\u5927\u578b\u6a21\u578b\u5728\u8f03\u5c11\u7684 GPU \u4e0a\u9032\u884c\u6709\u6548\u7387\u7684\u5168\u53c3\u6578\u8a13\u7df4\uff0c\u4e26\u6709\u52a9\u65bc\u5728\u5206\u6563\u5f0f\u8a13\u7df4\u8a2d\u5b9a\u4e2d\u5c07\u6279\u6b21\u5927\u5c0f\u52a0\u500d\uff0c\u70ba\u64f4\u5145\u5927\u578b\u6a21\u578b\u8a13\u7df4\u63d0\u4f9b\u4e00\u500b\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/NVlabs/COAT \u53d6\u5f97\u3002", "author": "Haocheng Xi et.al.", "authors": "Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, Song Han", "id": "2410.19313v1", "paper_url": "http://arxiv.org/abs/2410.19313v1", "repo": "null"}}