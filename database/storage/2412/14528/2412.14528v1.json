{"2412.14528": {"publish_time": "2024-12-19", "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models", "paper_summary": "Knowledge distillation (KD) has become a prevalent technique for compressing\nlarge language models (LLMs). Existing KD methods are constrained by the need\nfor identical tokenizers (i.e., vocabularies) between teacher and student\nmodels, limiting their versatility in handling LLMs of different architecture\nfamilies. In this paper, we introduce the Multi-Level Optimal Transport\n(MultiLevelOT), a novel approach that advances the optimal transport for\nuniversal cross-tokenizer knowledge distillation. Our method aligns the logit\ndistributions of the teacher and the student at both token and sequence levels\nusing diverse cost matrices, eliminating the need for dimensional or\ntoken-by-token correspondence. At the token level, MultiLevelOT integrates both\nglobal and local information by jointly optimizing all tokens within a sequence\nto enhance robustness. At the sequence level, we efficiently capture complex\ndistribution structures of logits via the Sinkhorn distance, which approximates\nthe Wasserstein distance for divergence measures. Extensive experiments on\ntasks such as extractive QA, generative QA, and summarization demonstrate that\nthe MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under\nvarious settings. Our approach is robust to different student and teacher\nmodels across model families, architectures, and parameter sizes.", "paper_summary_zh": "\u77e5\u8b58\u84b8\u993e (KD) \u5df2\u6210\u70ba\u58d3\u7e2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u4e00\u7a2e\u6d41\u884c\u6280\u8853\u3002\u73fe\u6709\u7684 KD \u65b9\u6cd5\u53d7\u5230\u6559\u5e2b\u548c\u5b78\u751f\u6a21\u578b\u4e4b\u9593\u9700\u8981\u76f8\u540c\u7684\u6a19\u8a18\u5316\u5668\uff08\u5373\u8a5e\u5f59\u8868\uff09\u7684\u9650\u5236\uff0c\u9650\u5236\u4e86\u5176\u8655\u7406\u4e0d\u540c\u67b6\u69cb\u7cfb\u5217\u7684 LLM \u7684\u591a\u529f\u80fd\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u591a\u5c64\u6b21\u6700\u512a\u50b3\u8f38 (MultiLevelOT)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u63a8\u52d5\u4e86\u901a\u7528\u8de8\u6a19\u8a18\u5316\u5668\u77e5\u8b58\u84b8\u993e\u7684\u6700\u512a\u50b3\u8f38\u3002\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528\u4e0d\u540c\u7684\u6210\u672c\u77e9\u9663\uff0c\u5728\u6a19\u8a18\u548c\u5e8f\u5217\u5c64\u7d1a\u4e0a\u5c0d\u9f4a\u6559\u5e2b\u548c\u5b78\u751f\u7684 logit \u5206\u5e03\uff0c\u6d88\u9664\u4e86\u5c0d\u7dad\u5ea6\u6216\u9010\u500b\u6a19\u8a18\u5c0d\u61c9\u7684\u9700\u6c42\u3002\u5728\u6a19\u8a18\u5c64\u7d1a\uff0cMultiLevelOT \u6574\u5408\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u8cc7\u8a0a\uff0c\u901a\u904e\u806f\u5408\u6700\u4f73\u5316\u5e8f\u5217\u4e2d\u7684\u6240\u6709\u6a19\u8a18\u4f86\u589e\u5f37\u7a69\u5065\u6027\u3002\u5728\u5e8f\u5217\u5c64\u7d1a\uff0c\u6211\u5011\u900f\u904e Sinkhorn \u8ddd\u96e2\u6709\u6548\u6355\u6349 logit \u7684\u8907\u96dc\u5206\u4f48\u7d50\u69cb\uff0c\u9019\u8fd1\u4f3c\u4e86\u6563\u5ea6\u6e2c\u91cf\u7684 Wasserstein \u8ddd\u96e2\u3002\u5728\u8403\u53d6\u5f0f QA\u3001\u751f\u6210\u5f0f QA \u548c\u6458\u8981\u7b49\u4efb\u52d9\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u5728\u5404\u7a2e\u8a2d\u5b9a\u4e0b\uff0cMultiLevelOT \u512a\u65bc\u6700\u5148\u9032\u7684\u8de8\u6a19\u8a18\u5316\u5668 KD \u65b9\u6cd5\u3002\u6211\u5011\u7684\u6a21\u578b\u5c0d\u8de8\u6a21\u578b\u7cfb\u5217\u3001\u67b6\u69cb\u548c\u53c3\u6578\u5927\u5c0f\u7684\u4e0d\u540c\u5b78\u751f\u548c\u6559\u5e2b\u6a21\u578b\u5177\u6709\u7a69\u5065\u6027\u3002", "author": "Xiao Cui et.al.", "authors": "Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li", "id": "2412.14528v1", "paper_url": "http://arxiv.org/abs/2412.14528v1", "repo": "null"}}