{"2406.07867": {"publish_time": "2024-06-12", "title": "Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation", "paper_summary": "In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u9762\u5bf9\u9762\u53e3\u8a9e\u5c0d\u8a71\u6a21\u578b\u3002\u5b83\u8655\u7406\u4f86\u81ea\u4f7f\u7528\u8005\u8f38\u5165\u7684\u8996\u8a0a\u8a9e\u97f3\uff0c\u4e26\u7522\u751f\u8996\u8a0a\u8a9e\u97f3\u4f5c\u70ba\u56de\u61c9\uff0c\u6a19\u8a8c\u8457\u5728\u4e0d\u4f9d\u8cf4\u4e2d\u9593\u6587\u5b57\u7684\u60c5\u6cc1\u4e0b\u5efa\u7acb\u982d\u50cf\u804a\u5929\u6a5f\u5668\u4eba\u7cfb\u7d71\u7684\u521d\u6b65\u6b65\u9a5f\u3002\u70ba\u6b64\uff0c\u6211\u5011\u65b0\u5f15\u5165\u4e86 MultiDialog\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5305\u542b\u7d04 9,000 \u500b\u5c0d\u8a71\u3001340 \u5c0f\u6642\u7684\u5c0d\u8a71\u8a9e\u6599\u5eab\uff0c\u4e26\u6839\u64da\u958b\u653e\u57df\u5c0d\u8a71\u8cc7\u6599\u96c6 TopicalChat \u9032\u884c\u8a18\u9304\u7684\u5927\u898f\u6a21\u591a\u6a21\u614b\uff08\u5373\u97f3\u8a0a\u548c\u8996\u89ba\uff09\u53e3\u8a9e\u5c0d\u8a71\u8a9e\u6599\u5eab\u3002MultiDialog \u5305\u542b\u5c0d\u8a71\u5925\u4f34\u6839\u64da\u7d66\u5b9a\u8173\u672c\u8868\u6f14\u7684\u5c0d\u8a71\u97f3\u8a0a\u8996\u8a0a\u4e26\u884c\u9304\u97f3\uff0c\u4ee5\u53ca\u6211\u5011\u9810\u671f\u5c07\u958b\u555f\u591a\u6a21\u614b\u5408\u6210\u7814\u7a76\u6a5f\u6703\u7684\u60c5\u7dd2\u8a3b\u89e3\u3002\u6211\u5011\u7684\u9762\u5c0d\u9762\u53e3\u8a9e\u5c0d\u8a71\u6a21\u578b\u7d50\u5408\u4e86\u6587\u5b57\u9810\u8a13\u7df4\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u900f\u904e\u7d50\u5408\u8a9e\u97f3\u6587\u5b57\u806f\u5408\u9810\u8a13\u7df4\uff0c\u5c07\u5176\u9069\u61c9\u5230\u8996\u8a0a\u8a9e\u97f3\u5c0d\u8a71\u9818\u57df\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u9a57\u8b49\u4e86\u6211\u5011\u7684\u6a21\u578b\u5728\u4fc3\u9032\u9762\u5c0d\u9762\u5c0d\u8a71\u4e2d\u7684\u6709\u6548\u6027\u3002\u793a\u7bc4\u548c\u8cc7\u6599\u5206\u5225\u53ef\u4ee5\u5728 https://multidialog.github.io \u548c https://huggingface.co/datasets/IVLLab/MultiDialog \u53d6\u5f97\u3002", "author": "Se Jin Park et.al.", "authors": "Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro", "id": "2406.07867v1", "paper_url": "http://arxiv.org/abs/2406.07867v1", "repo": "null"}}