{"2406.07289": {"publish_time": "2024-06-11", "title": "Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?", "paper_summary": "Recently proposed two-pass direct speech-to-speech translation (S2ST) models\ndecompose the task into speech-to-text translation (S2TT) and text-to-speech\n(TTS) within an end-to-end model, yielding promising results. However, the\ntraining of these models still relies on parallel speech data, which is\nextremely challenging to collect. In contrast, S2TT and TTS have accumulated a\nlarge amount of data and pretrained models, which have not been fully utilized\nin the development of S2ST models. Inspired by this, in this paper, we first\nintroduce a composite S2ST model named ComSpeech, which can seamlessly\nintegrate any pretrained S2TT and TTS models into a direct S2ST model.\nFurthermore, to eliminate the reliance on parallel speech data, we propose a\nnovel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It\naligns representations in the latent space through contrastive learning,\nenabling the speech synthesis capability learned from the TTS data to\ngeneralize to S2ST in a zero-shot manner. Experimental results on the CVSS\ndataset show that when the parallel speech data is available, ComSpeech\nsurpasses previous two-pass models like UnitY and Translatotron 2 in both\ntranslation quality and decoding speed. When there is no parallel speech data,\nComSpeech-ZS lags behind \\name by only 0.7 ASR-BLEU and outperforms the\ncascaded models.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u63d0\u51fa\u7684\u5169\u968e\u6bb5\u76f4\u63a5\u8a9e\u97f3\u8f49\u8a9e\u97f3\u7ffb\u8b6f (S2ST) \u6a21\u578b\n\u5c07\u4efb\u52d9\u5206\u89e3\u70ba\u8a9e\u97f3\u8f49\u6587\u5b57\u7ffb\u8b6f (S2TT) \u548c\u6587\u5b57\u8f49\u8a9e\u97f3\n(TTS)\uff0c\u5728\u7aef\u5230\u7aef\u6a21\u578b\u4e2d\u7522\u751f\u4ee4\u4eba\u6eff\u610f\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7684\n\u8a13\u7df4\u4f9d\u820a\u4ef0\u8cf4\u5e73\u884c\u8a9e\u97f3\u8cc7\u6599\uff0c\u800c\u9019\u985e\u8cc7\u6599\u6975\u96e3\u6536\u96c6\u3002\u76f8\u8f03\u4e4b\u4e0b\uff0cS2TT \u548c TTS \u5df2\u7d2f\u7a4d\u4e86\n\u5927\u91cf\u7684\u8cc7\u6599\u548c\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u800c\u9019\u4e9b\u8cc7\u6599\u548c\u6a21\u578b\u5728 S2ST \u6a21\u578b\u7684\u958b\u767c\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\n\u53d7\u5230\u9019\u9ede\u555f\u767c\uff0c\u6211\u5011\u5728\u672c\u6587\u4e2d\u9996\u5148\u4ecb\u7d39\u4e00\u500b\u540d\u70ba ComSpeech \u7684\u8907\u5408\u5f0f S2ST \u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u5c07\u4efb\u4f55\u9810\u8a13\u7df4\u7684 S2TT \u548c TTS \u6a21\u578b\u7121\u7e2b\u6574\u5408\u5230\u76f4\u63a5\u7684 S2ST \u6a21\u578b\u4e2d\u3002\n\u6b64\u5916\uff0c\u70ba\u4e86\u4e0d\u518d\u4f9d\u8cf4\u5e73\u884c\u8a9e\u97f3\u8cc7\u6599\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a13\u7df4\u65b9\u6cd5 ComSpeech-ZS\uff0c\u5b83\u50c5\u5229\u7528 S2TT \u548c TTS \u8cc7\u6599\u3002\u5b83\u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\u5728\u6f5b\u5728\u7a7a\u9593\u4e2d\u6bd4\u5c0d\u8868\u5fb5\uff0c\n\u8b93\u5f9e TTS \u8cc7\u6599\u4e2d\u5b78\u7fd2\u5230\u7684\u8a9e\u97f3\u5408\u6210\u80fd\u529b\u80fd\u5920\u4ee5\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u63a8\u5ee3\u5230 S2ST\u3002\u5728 CVSS\n\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u7576\u5e73\u884c\u8a9e\u97f3\u8cc7\u6599\u53ef\u7528\u6642\uff0cComSpeech\n\u5728\u7ffb\u8b6f\u54c1\u8cea\u548c\u89e3\u78bc\u901f\u5ea6\u4e0a\u90fd\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u5169\u968e\u6bb5\u6a21\u578b\uff0c\u4f8b\u5982 UnitY \u548c Translatotron 2\u3002\u7576\u6c92\u6709\u5e73\u884c\u8a9e\u97f3\u8cc7\u6599\u6642\uff0c\nComSpeech-ZS \u50c5\u843d\u5f8c \\name 0.7 ASR-BLEU\uff0c\u4e14\u8868\u73fe\u512a\u65bc\u4e32\u63a5\u6a21\u578b\u3002</paragraph>", "author": "Qingkai Fang et.al.", "authors": "Qingkai Fang, Shaolei Zhang, Zhengrui Ma, Min Zhang, Yang Feng", "id": "2406.07289v1", "paper_url": "http://arxiv.org/abs/2406.07289v1", "repo": "null"}}