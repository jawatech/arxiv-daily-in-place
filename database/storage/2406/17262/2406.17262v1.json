{"2406.17262": {"publish_time": "2024-06-25", "title": "D2LLM: Decomposed and Distilled Large Language Models for Semantic Search", "paper_summary": "The key challenge in semantic search is to create models that are both\naccurate and efficient in pinpointing relevant sentences for queries. While\nBERT-style bi-encoders excel in efficiency with pre-computed embeddings, they\noften miss subtle nuances in search tasks. Conversely, GPT-style LLMs with\ncross-encoder designs capture these nuances but are computationally intensive,\nhindering real-time applications. In this paper, we present D2LLMs-Decomposed\nand Distilled LLMs for semantic search-that combines the best of both worlds.\nWe decompose a cross-encoder into an efficient bi-encoder integrated with\nPooling by Multihead Attention and an Interaction Emulation Module, achieving\nnuanced understanding and pre-computability. Knowledge from the LLM is\ndistilled into this model using contrastive, rank, and feature imitation\ntechniques. Our experiments show that D2LLM surpasses five leading baselines in\nterms of all metrics across three tasks, particularly improving NLI task\nperformance by at least 6.45%. The source code is available at\nhttps://github.com/codefuse-ai/D2LLM.", "paper_summary_zh": "\u8a9e\u610f\u641c\u5c0b\u7684\u4e3b\u8981\u6311\u6230\u5728\u65bc\u5efa\u7acb\u7cbe\u6e96\u4e14\u6709\u6548\u7387\u7684\u6a21\u578b\uff0c\u4ee5\u627e\u51fa\u8207\u67e5\u8a62\u76f8\u95dc\u7684\u53e5\u5b50\u3002\u96d6\u7136 BERT \u5f0f\u96d9\u7de8\u78bc\u5668\u5728\u9810\u5148\u8a08\u7b97\u7684\u5d4c\u5165\u4e2d\u8868\u73fe\u51fa\u6548\u7387\uff0c\u4f46\u5b83\u5011\u5728\u641c\u5c0b\u4efb\u52d9\u4e2d\u5e38\u5e38\u6703\u907a\u6f0f\u7d30\u5fae\u7684\u5dee\u7570\u3002\u76f8\u53cd\u5730\uff0c\u5177\u6709\u4ea4\u53c9\u7de8\u78bc\u5668\u8a2d\u8a08\u7684 GPT \u5f0f LLM \u53ef\u4ee5\u6355\u6349\u5230\u9019\u4e9b\u5dee\u7570\uff0c\u4f46\u8a08\u7b97\u91cf\u5f88\u5927\uff0c\u6703\u963b\u7919\u5be6\u6642\u61c9\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa D2LLM\uff0c\u5373\u5206\u89e3\u548c\u8403\u53d6\u7684 LLM\uff0c\u7528\u65bc\u8a9e\u610f\u641c\u5c0b\uff0c\u7d50\u5408\u5169\u5168\u5176\u7f8e\u7684\u512a\u9ede\u3002\u6211\u5011\u5c07\u4ea4\u53c9\u7de8\u78bc\u5668\u5206\u89e3\u6210\u4e00\u500b\u6709\u6548\u7387\u7684\u96d9\u7de8\u78bc\u5668\uff0c\u6574\u5408\u591a\u982d\u6ce8\u610f\u529b\u6c60\u5316\u548c\u4e92\u52d5\u6a21\u64ec\u6a21\u7d44\uff0c\u5be6\u73fe\u7d30\u5fae\u7684\u7406\u89e3\u548c\u9810\u5148\u8a08\u7b97\u3002\u6211\u5011\u4f7f\u7528\u5c0d\u6bd4\u3001\u6392\u5e8f\u548c\u7279\u5fb5\u6a21\u64ec\u6280\u8853\uff0c\u5c07 LLM \u7684\u77e5\u8b58\u8403\u53d6\u5230\u9019\u500b\u6a21\u578b\u4e2d\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0cD2LLM \u5728\u4e09\u500b\u4efb\u52d9\u7684\u6240\u6709\u6307\u6a19\u4e2d\u90fd\u8d85\u8d8a\u4e86\u4e94\u500b\u4e3b\u8981\u7684\u57fa\u6e96\uff0c\u7279\u5225\u662f\u5c07 NLI \u4efb\u52d9\u7684\u6548\u80fd\u63d0\u5347\u4e86\u81f3\u5c11 6.45%\u3002\u539f\u59cb\u78bc\u53ef\u5728 https://github.com/codefuse-ai/D2LLM \u53d6\u5f97\u3002", "author": "Zihan Liao et.al.", "authors": "Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang", "id": "2406.17262v1", "paper_url": "http://arxiv.org/abs/2406.17262v1", "repo": "null"}}