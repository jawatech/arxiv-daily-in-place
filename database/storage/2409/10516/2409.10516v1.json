{"2409.10516": {"publish_time": "2024-09-16", "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval", "paper_summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u9818\u57df\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u6ce8\u610f\u529b\u904b\u7b97\u7684\u4e8c\u6b21\u6642\u9593\u8907\u96dc\u5ea6\u5c0d\u64f4\u5c55\u5230\u66f4\u9577\u7684\u5167\u5bb9\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\uff0c\u539f\u56e0\u662f\u6975\u9ad8\u7684\u63a8\u8ad6\u5ef6\u9072\u548c\u7528\u65bc\u5feb\u53d6\u9375\u503c (KV) \u5411\u91cf\u7684\u9ad8 GPU \u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u672c\u6587\u63d0\u51fa RetrievalAttention\uff0c\u9019\u662f\u4e00\u7a2e\u514d\u8a13\u7df4\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u52a0\u901f\u6ce8\u610f\u529b\u904b\u7b97\u3002\u70ba\u4e86\u5229\u7528\u6ce8\u610f\u529b\u7684\u52d5\u614b\u7a00\u758f\u5c6c\u6027\uff0cRetrievalAttention \u5728 CPU \u8a18\u61b6\u9ad4\u4e2d\u5efa\u7acb\u8fd1\u4f3c\u6700\u8fd1\u9130\u641c\u5c0b (ANNS) \u7d22\u5f15\uff0c\u4e26\u5728\u751f\u6210\u671f\u9593\u900f\u904e\u5411\u91cf\u641c\u5c0b\u64f7\u53d6\u6700\u76f8\u95dc\u7684\u5411\u91cf\u3002\u7531\u65bc\u67e5\u8a62\u5411\u91cf\u548c\u91d1\u9470\u5411\u91cf\u4e4b\u9593\u7684\u5206\u5e03\u5916 (OOD)\uff0c\u73fe\u6210\u7684 ANNS \u7d22\u5f15\u4ecd\u9700\u8981\u6383\u63cf O(N)\uff08\u901a\u5e38\u70ba\u6240\u6709\u91d1\u9470\u7684 30%\uff09\u8cc7\u6599\u4ee5\u9032\u884c\u6e96\u78ba\u7684\u64f7\u53d6\uff0c\u9019\u7121\u6cd5\u5229\u7528\u9ad8\u7a00\u758f\u6027\u3002RetrievalAttention \u9996\u5148\u627e\u51fa\u57fa\u65bc ANNS \u7684\u6ce8\u610f\u529b\u7684 OOD \u6311\u6230\uff0c\u4e26\u900f\u904e\u53ef\u9069\u61c9\u67e5\u8a62\u4e14\u50c5\u5b58\u53d6 1--3% \u8cc7\u6599\u7684\u6ce8\u610f\u529b\u611f\u77e5\u5411\u91cf\u641c\u5c0b\u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u5f9e\u800c\u5be6\u73fe\u6b21\u7dda\u6027\u6642\u9593\u8907\u96dc\u5ea6\u3002RetrievalAttention \u5927\u5e45\u964d\u4f4e\u4e86\u9577\u5167\u5bb9 LLM \u7684\u63a8\u8ad6\u6210\u672c\uff0c\u540c\u6642 GPU \u8a18\u61b6\u9ad4\u9700\u6c42\u4e5f\u4f4e\u5f88\u591a\uff0c\u4f46\u4ecd\u7dad\u6301\u6a21\u578b\u6e96\u78ba\u5ea6\u3002\u7279\u5225\u662f\uff0cRetrievalAttention \u53ea\u9700\u8981 16GB GPU \u8a18\u61b6\u9ad4\u5c31\u80fd\u5728\u5177\u5099 8B \u53c3\u6578\u7684 LLM \u4e2d\u63d0\u4f9b 128K \u500b\u7b26\u865f\uff0c\u9019\u53ef\u4ee5\u5728\u55ae\u4e00 NVIDIA RTX4090\uff0824GB\uff09\u4e0a\u4ee5 0.188 \u79d2\u7522\u751f\u4e00\u500b\u7b26\u865f\u3002</paragraph>", "author": "Di Liu et.al.", "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu", "id": "2409.10516v1", "paper_url": "http://arxiv.org/abs/2409.10516v1", "repo": "null"}}