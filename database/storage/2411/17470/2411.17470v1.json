{"2411.17470": {"publish_time": "2024-11-25", "title": "Towards Precise Scaling Laws for Video Diffusion Transformers", "paper_summary": "Achieving optimal performance of video diffusion transformers within given\ndata and compute budget is crucial due to their high training costs. This\nnecessitates precisely determining the optimal model size and training\nhyperparameters before large-scale training. While scaling laws are employed in\nlanguage models to predict performance, their existence and accurate derivation\nin visual generation models remain underexplored. In this paper, we\nsystematically analyze scaling laws for video diffusion transformers and\nconfirm their presence. Moreover, we discover that, unlike language models,\nvideo diffusion models are more sensitive to learning rate and batch size, two\nhyperparameters often not precisely modeled. To address this, we propose a new\nscaling law that predicts optimal hyperparameters for any model size and\ncompute budget. Under these optimal settings, we achieve comparable performance\nand reduce inference costs by 40.1% compared to conventional scaling methods,\nwithin a compute budget of 1e10 TFlops. Furthermore, we establish a more\ngeneralized and precise relationship among validation loss, any model size, and\ncompute budget. This enables performance prediction for non-optimal model\nsizes, which may also be appealed under practical inference cost constraints,\nachieving a better trade-off.", "paper_summary_zh": "\u7531\u65bc\u8996\u8a0a\u64f4\u6563Transformer\u7684\u8a13\u7df4\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u5728\u65e2\u5b9a\u7684\u8cc7\u6599\u548c\u904b\u7b97\u9810\u7b97\u5167\uff0c\u9054\u6210\u6700\u4f73\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u9019\u9700\u8981\u5728\u9032\u884c\u5927\u898f\u6a21\u8a13\u7df4\u4e4b\u524d\uff0c\u7cbe\u6e96\u5730\u6c7a\u5b9a\u6700\u4f73\u6a21\u578b\u5927\u5c0f\u548c\u8a13\u7df4\u8d85\u53c3\u6578\u3002\u96d6\u7136\u8a9e\u8a00\u6a21\u578b\u4e2d\u63a1\u7528\u4e86\u6bd4\u4f8b\u5b9a\u5f8b\u4f86\u9810\u6e2c\u6548\u80fd\uff0c\u4f46\u8996\u89ba\u751f\u6210\u6a21\u578b\u4e2d\u6bd4\u4f8b\u5b9a\u5f8b\u7684\u5b58\u5728\u548c\u7cbe\u78ba\u63a8\u5c0e\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u5206\u6790\u4e86\u8996\u8a0a\u64f4\u6563Transformer\u7684\u6bd4\u4f8b\u5b9a\u5f8b\uff0c\u4e26\u78ba\u8a8d\u4e86\u5b83\u5011\u7684\u5b58\u5728\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u8207\u8a9e\u8a00\u6a21\u578b\u4e0d\u540c\uff0c\u8996\u8a0a\u64f4\u6563\u6a21\u578b\u5c0d\u5b78\u7fd2\u7387\u548c\u6279\u6b21\u5927\u5c0f\u66f4\u654f\u611f\uff0c\u9019\u5169\u500b\u8d85\u53c3\u6578\u901a\u5e38\u6c92\u6709\u7cbe\u78ba\u5efa\u6a21\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u6bd4\u4f8b\u5b9a\u5f8b\uff0c\u53ef\u4ee5\u9810\u6e2c\u4efb\u4f55\u6a21\u578b\u5927\u5c0f\u548c\u904b\u7b97\u9810\u7b97\u7684\u6700\u4f73\u8d85\u53c3\u6578\u3002\u5728\u9019\u4e9b\u6700\u4f73\u8a2d\u5b9a\u4e0b\uff0c\u6211\u5011\u9054\u5230\u4e86\u53ef\u6bd4\u8f03\u7684\u6548\u80fd\uff0c\u4e26\u5728 1e10 TFlops \u7684\u904b\u7b97\u9810\u7b97\u5167\uff0c\u5c07\u63a8\u8ad6\u6210\u672c\u964d\u4f4e\u4e86 40.1%\uff0c\u512a\u65bc\u50b3\u7d71\u7684\u6bd4\u4f8b\u5b9a\u5f8b\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u9a57\u8b49\u640d\u5931\u3001\u4efb\u4f55\u6a21\u578b\u5927\u5c0f\u548c\u904b\u7b97\u9810\u7b97\u4e4b\u9593\u66f4\u5ee3\u6cdb\u4e14\u7cbe\u78ba\u7684\u95dc\u4fc2\u3002\u9019\u4f7f\u5f97\u53ef\u4ee5\u9810\u6e2c\u975e\u6700\u4f73\u6a21\u578b\u5927\u5c0f\u7684\u6548\u80fd\uff0c\u5728\u5be6\u969b\u7684\u63a8\u8ad6\u6210\u672c\u9650\u5236\u4e0b\uff0c\u9019\u4e5f\u53ef\u80fd\u53d7\u5230\u9752\u775e\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u597d\u7684\u6b0a\u8861\u3002", "author": "Yuanyang Yin et.al.", "authors": "Yuanyang Yin, Yaqi Zhao, Mingwu Zheng, Ke Lin, Jiarong Ou, Rui Chen, Victor Shea-Jay Huang, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, Wentao Zhang, Kun Gai", "id": "2411.17470v1", "paper_url": "http://arxiv.org/abs/2411.17470v1", "repo": "null"}}