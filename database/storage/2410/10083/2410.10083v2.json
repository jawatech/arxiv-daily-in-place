{"2410.10083": {"publish_time": "2024-10-14", "title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "paper_summary": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by\nfocusing mainly on pairwise relationships, overlooking the high-order\ncorrelations found in real-world data. Hypergraphs, which can model complex\nbeyond-pairwise relationships, offer a more robust framework but are still\nunderexplored in the context of LLMs. To address this gap, we introduce\nLLM4Hypergraph, the first comprehensive benchmark comprising 21,500 problems\nacross eight low-order, five high-order, and two isomorphism tasks, utilizing\nboth synthetic and real-world hypergraphs from citation networks and protein\nstructures. We evaluate six prominent LLMs, including GPT-4o, demonstrating our\nbenchmark's effectiveness in identifying model strengths and weaknesses. Our\nspecialized prompting framework incorporates seven hypergraph languages and\nintroduces two novel techniques, Hyper-BAG and Hyper-COT, which enhance\nhigh-order reasoning and achieve an average 4% (up to 9%) performance\nimprovement on structure classification tasks. This work establishes a\nfoundational testbed for integrating hypergraph computational capabilities into\nLLMs, advancing their comprehension. The source codes are at\nhttps://github.com/iMoonLab/LLM4Hypergraph.", "paper_summary_zh": "\u73fe\u6709\u7684 NLGraph \u548c GraphQA \u7b49\u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u6210\u5c0d\u95dc\u4fc2\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u73fe\u5be6\u4e16\u754c\u8cc7\u6599\u4e2d\u767c\u73fe\u7684\u9ad8\u968e\u76f8\u95dc\u6027\uff0c\u5f9e\u800c\u5c0d\u5716\u5f62\u4e2d\u7684 LLM \u9032\u884c\u8a55\u4f30\u3002\u8d85\u5716\u53ef\u4ee5\u5efa\u6a21\u8907\u96dc\u7684\u8d85\u8d8a\u6210\u5c0d\u95dc\u4fc2\uff0c\u63d0\u4f9b\u66f4\u5f37\u5927\u7684\u6846\u67b6\uff0c\u4f46\u5728 LLM \u7684\u80cc\u666f\u4e0b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 LLM4Hypergraph\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7d9c\u5408\u57fa\u6e96\uff0c\u5305\u542b 21,500 \u500b\u554f\u984c\uff0c\u6db5\u84cb\u516b\u500b\u4f4e\u968e\u3001\u4e94\u500b\u9ad8\u968e\u548c\u5169\u500b\u540c\u69cb\u4efb\u52d9\uff0c\u5229\u7528\u4f86\u81ea\u5f15\u6587\u7db2\u8def\u548c\u86cb\u767d\u8cea\u7d50\u69cb\u7684\u5408\u6210\u548c\u771f\u5be6\u4e16\u754c\u8d85\u5716\u3002\u6211\u5011\u8a55\u4f30\u4e86\u516d\u500b\u8457\u540d\u7684 LLM\uff0c\u5305\u62ec GPT-4o\uff0c\u8b49\u660e\u4e86\u6211\u5011\u7684\u57fa\u6e96\u5728\u8b58\u5225\u6a21\u578b\u512a\u52e2\u548c\u52a3\u52e2\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u5c08\u696d\u7684\u63d0\u793a\u6846\u67b6\u5305\u542b\u4e03\u7a2e\u8d85\u5716\u8a9e\u8a00\uff0c\u4e26\u5f15\u5165\u4e86\u5169\u7a2e\u65b0\u6280\u8853 Hyper-BAG \u548c Hyper-COT\uff0c\u5b83\u5011\u589e\u5f37\u4e86\u9ad8\u968e\u63a8\u7406\uff0c\u4e26\u5728\u7d50\u69cb\u5206\u985e\u4efb\u52d9\u4e0a\u5be6\u73fe\u4e86\u5e73\u5747 4%\uff08\u6700\u9ad8 9%\uff09\u7684\u6027\u80fd\u6539\u9032\u3002\u9019\u9805\u5de5\u4f5c\u70ba\u5c07\u8d85\u5716\u8a08\u7b97\u80fd\u529b\u6574\u5408\u5230 LLM \u4e2d\u5efa\u7acb\u4e86\u4e00\u500b\u57fa\u790e\u6e2c\u8a66\u5e73\u53f0\uff0c\u5f9e\u800c\u63d0\u5347\u4e86\u5b83\u5011\u7684\u7406\u89e3\u529b\u3002\u6e90\u4ee3\u78bc\u4f4d\u65bc https://github.com/iMoonLab/LLM4Hypergraph\u3002", "author": "Yifan Feng et.al.", "authors": "Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao", "id": "2410.10083v2", "paper_url": "http://arxiv.org/abs/2410.10083v2", "repo": "null"}}