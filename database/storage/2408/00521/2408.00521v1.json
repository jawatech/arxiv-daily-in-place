{"2408.00521": {"publish_time": "2024-08-01", "title": "A new approach for encoding code and assisting code understanding", "paper_summary": "Some companies(e.g., Microsoft Research and Google DeepMind) have discovered\nsome of the limitations of GPTs autoregressive paradigm next-word prediction,\nmanifested in the model lack of planning, working memory, backtracking, and\nreasoning skills. GPTs rely on a local and greedy process of generating the\nnext word, without a global understanding of the task or the output.We have\nconfirmed the above limitations through specialized empirical studies of code\ncomprehension. Although GPT4 is good at producing fluent and coherent text, it\ncannot handle complex logic and generate new code that haven not been seen, and\nit relies too much on the formatting of the prompt to generate the correct\ncode.We propose a new paradigm for code understanding that goes beyond the\nnext-word prediction paradigm, inspired by the successful application of\ndiffusion techniques to image generation(Dalle2, Sora) and protein structure\ngeneration(AlphaFold3), which have no autoregressive constraints.Instead of\nencoding the code in a form that mimics natural language, we encode the code as\na heterogeneous image paradigm with a memory of global information that mimics\nboth images and protein structures.We then refer to Sora's CLIP upstream\ntext-to-image encoder model to design a text-to-code encoder model that can be\napplied to various downstream code understanding tasks.The model learns the\nglobal understanding of code under the new paradigm heterogeneous image,\nconnects the encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it.Using self-supervised comparative\nlearning on 456,360 text-code pairs, the model achieved a zero-shot prediction\nof new data. This work is the basis for future work on code generation using\ndiffusion techniques under a new paradigm to avoid autoregressive limitations.", "paper_summary_zh": "\u4e00\u4e9b\u516c\u53f8\uff08\u4f8b\u5982 Microsoft Research \u548c Google DeepMind\uff09\u5df2\u7d93\u767c\u73fe GPT \u81ea\u52d5\u8ff4\u6b78\u7bc4\u4f8b\u4e2d\u7684\u4e00\u4e9b\u9650\u5236\uff0c\u5373\u4e0b\u4e00\u500b\u5b57\u8a5e\u9810\u6e2c\uff0c\u9ad4\u73fe\u5728\u6a21\u578b\u7f3a\u4e4f\u898f\u5283\u3001\u5de5\u4f5c\u8a18\u61b6\u3001\u56de\u6eaf\u548c\u63a8\u7406\u6280\u80fd\u3002GPT \u4f9d\u8cf4\u65bc\u7522\u751f\u4e0b\u4e00\u500b\u5b57\u8a5e\u7684\u5c40\u90e8\u548c\u8caa\u5a6a\u904e\u7a0b\uff0c\u800c\u6c92\u6709\u5c0d\u4efb\u52d9\u6216\u8f38\u51fa\u7684\u5168\u5c40\u7406\u89e3\u3002\u6211\u5011\u5df2\u7d93\u901a\u904e\u7a0b\u5f0f\u78bc\u7406\u89e3\u7684\u5c08\u9580\u7d93\u9a57\u7814\u7a76\u78ba\u8a8d\u4e86\u4e0a\u8ff0\u9650\u5236\u3002\u5118\u7ba1 GPT4 \u64c5\u9577\u7522\u751f\u6d41\u66a2\u4e14\u9023\u8cab\u7684\u6587\u5b57\uff0c\u4f46\u5b83\u7121\u6cd5\u8655\u7406\u8907\u96dc\u7684\u908f\u8f2f\u548c\u7522\u751f\u672a\u66fe\u898b\u904e\u7684\u65b0\u7a0b\u5f0f\u78bc\uff0c\u800c\u4e14\u5b83\u904e\u65bc\u4f9d\u8cf4\u63d0\u793a\u7684\u683c\u5f0f\u4f86\u7522\u751f\u6b63\u78ba\u7684\u7a0b\u5f0f\u78bc\u3002\u6211\u5011\u63d0\u51fa\u4e86\u8d85\u8d8a\u4e0b\u4e00\u500b\u5b57\u8a5e\u9810\u6e2c\u7bc4\u4f8b\u7684\u7a0b\u5f0f\u78bc\u7406\u89e3\u65b0\u7bc4\u4f8b\uff0c\u9748\u611f\u4f86\u81ea\u64f4\u6563\u6280\u8853\u6210\u529f\u61c9\u7528\u65bc\u5f71\u50cf\u7522\u751f\uff08Dalle2\u3001Sora\uff09\u548c\u86cb\u767d\u8cea\u7d50\u69cb\u7522\u751f\uff08AlphaFold3\uff09\uff0c\u5b83\u5011\u6c92\u6709\u81ea\u52d5\u8ff4\u6b78\u7d04\u675f\u3002\u6211\u5011\u6c92\u6709\u4f7f\u7528\u6a21\u4eff\u81ea\u7136\u8a9e\u8a00\u7684\u5f62\u5f0f\u5c0d\u7a0b\u5f0f\u78bc\u9032\u884c\u7de8\u78bc\uff0c\u800c\u662f\u5c07\u7a0b\u5f0f\u78bc\u7de8\u78bc\u70ba\u7570\u8cea\u5f71\u50cf\u7bc4\u4f8b\uff0c\u5176\u4e2d\u5305\u542b\u6a21\u4eff\u5f71\u50cf\u548c\u86cb\u767d\u8cea\u7d50\u69cb\u7684\u5168\u5c40\u8cc7\u8a0a\u8a18\u61b6\u9ad4\u3002\u7136\u5f8c\uff0c\u6211\u5011\u53c3\u8003 Sora \u7684 CLIP \u4e0a\u6e38\u6587\u5b57\u8f49\u5f71\u50cf\u7de8\u78bc\u5668\u6a21\u578b\uff0c\u8a2d\u8a08\u4e00\u500b\u6587\u5b57\u8f49\u7a0b\u5f0f\u78bc\u7de8\u78bc\u5668\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u53ef\u61c9\u7528\u65bc\u5404\u7a2e\u4e0b\u6e38\u7a0b\u5f0f\u78bc\u7406\u89e3\u4efb\u52d9\u3002\u8a72\u6a21\u578b\u5728\u65b0\u7684\u7bc4\u4f8b\u7570\u8cea\u5f71\u50cf\u4e0b\u5b78\u7fd2\u7a0b\u5f0f\u78bc\u7684\u5168\u5c40\u7406\u89e3\uff0c\u9023\u63a5\u6587\u5b57\u548c\u7a0b\u5f0f\u78bc\u7684\u7de8\u78bc\u7a7a\u9593\uff0c\u4e26\u5c07\u6587\u5b57\u8f38\u5165\u7de8\u78bc\u6210\u8207\u5176\u6700\u76f8\u4f3c\u7684\u7a0b\u5f0f\u78bc\u5411\u91cf\u3002\u4f7f\u7528 456,360 \u500b\u6587\u5b57\u7a0b\u5f0f\u78bc\u5c0d\u4e0a\u7684\u81ea\u6211\u76e3\u7763\u6bd4\u8f03\u5b78\u7fd2\uff0c\u8a72\u6a21\u578b\u5be6\u73fe\u4e86\u65b0\u8cc7\u6599\u7684\u96f6\u6b21\u5b78\u7fd2\u9810\u6e2c\u3002\u9019\u9805\u5de5\u4f5c\u662f\u4f7f\u7528\u64f4\u6563\u6280\u8853\u5728\u65b0\u7684\u7bc4\u4f8b\u4e0b\u9032\u884c\u7a0b\u5f0f\u78bc\u7522\u751f\u7684\u672a\u4f86\u5de5\u4f5c\u7684\u57fa\u790e\uff0c\u4ee5\u907f\u514d\u81ea\u52d5\u8ff4\u6b78\u9650\u5236\u3002", "author": "Mengdan Fan et.al.", "authors": "Mengdan Fan, Wei Zhang, Haiyan Zhao, Zhi Jin", "id": "2408.00521v1", "paper_url": "http://arxiv.org/abs/2408.00521v1", "repo": "null"}}