{"2502.02481": {"publish_time": "2025-02-04", "title": "Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study", "paper_summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6301\u7e8c\u5c55\u73fe\u51fa\u4e0d\u65b7\u9032\u6b65\u7684\u591a\u8a9e\u8a00\u80fd\u529b\uff0c\u751a\u81f3\u5c0f\u898f\u6a21\u7684\u958b\u6e90\u6a21\u578b\u4e5f\u5c55\u73fe\u51fa\u5feb\u901f\u7684\u6548\u80fd\u63d0\u5347\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u64c1\u6709\u4e0d\u5230\u5341\u5104\u500b\u53c3\u6578\u7684\u958b\u653e\u5f0f LLM \u8655\u7406\u591a\u8a9e\u8a00\u6a5f\u5668\u7ffb\u8b6f (MT) \u4efb\u52d9\u7684\u80fd\u529b\u3002\u6211\u5011\u5c0d\u516d\u500b\u6d41\u884c\u7684 LLM \u9032\u884c\u5168\u9762\u8a55\u4f30\uff0c\u767c\u73fe\u50cf Gemma2-9B \u9019\u6a23\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u8a9e\u8a00\u7ffb\u8b6f\u80fd\u529b\u3002\u63a5\u8457\u6211\u5011\u5728\u6301\u7e8c\u9810\u8a13\u7df4\u968e\u6bb5\u5f15\u5165\u5e73\u884c\u512a\u5148\u55ae\u8a9e\u512a\u5148 (PFMS) \u8cc7\u6599\u6df7\u5408\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u5347 MT \u6548\u80fd\uff0c\u4e26\u63d0\u51fa GemmaX2-28\uff0c\u4e00\u500b\u5728 28 \u7a2e\u8a9e\u8a00\u4e2d\u9054\u6210\u9802\u5c16\u591a\u8a9e\u8a00\u7ffb\u8b6f\u6548\u80fd\u7684 9B \u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cGemmaX2-28 \u6301\u7e8c\u512a\u65bc TowerInstruct \u548c XALMA \u7b49\u6700\u5148\u9032 (SOTA) \u6a21\u578b\uff0c\u4e26\u5728 Google Translate \u548c GPT-4-turbo \u4e2d\u9054\u6210\u5177\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002", "author": "Menglong Cui et.al.", "authors": "Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, BinWang", "id": "2502.02481v1", "paper_url": "http://arxiv.org/abs/2502.02481v1", "repo": "null"}}