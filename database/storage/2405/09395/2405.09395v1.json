{"2405.09395": {"publish_time": "2024-05-15", "title": "Matching domain experts by training from scratch on domain knowledge", "paper_summary": "Recently, large language models (LLMs) have outperformed human experts in\npredicting the results of neuroscience experiments (Luo et al., 2024). What is\nthe basis for this performance? One possibility is that statistical patterns in\nthat specific scientific literature, as opposed to emergent reasoning abilities\narising from broader training, underlie LLMs' performance. To evaluate this\npossibility, we trained (next word prediction) a relatively small\n124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.\nDespite being orders of magnitude smaller than larger LLMs trained on trillions\nof tokens, small models achieved expert-level performance in predicting\nneuroscience results. Small models trained on the neuroscience literature\nsucceeded when they were trained from scratch using a tokenizer specifically\ntrained on neuroscience text or when the neuroscience literature was used to\nfinetune a pretrained GPT-2. Our results indicate that expert-level performance\nmay be attained by even small LLMs through domain-specific, auto-regressive\ntraining approaches.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u9884\u6d4b\u795e\u7ecf\u79d1\u5b66\u5b9e\u9a8c\u7ed3\u679c\u65b9\u9762\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff08Luo \u7b49\u4eba\uff0c2024 \u5e74\uff09\u3002\u8fd9\u79cd\u8868\u73b0\u7684\u57fa\u7840\u662f\u4ec0\u4e48\uff1f\u4e00\u79cd\u53ef\u80fd\u6027\u662f\u7279\u5b9a\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u7edf\u8ba1\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u6e90\u81ea\u66f4\u5e7f\u6cdb\u8bad\u7ec3\u7684\u65b0\u5174\u63a8\u7406\u80fd\u529b\uff0c\u662f LLM \u8868\u73b0\u7684\u57fa\u7840\u3002\u4e3a\u4e86\u8bc4\u4f30\u8fd9\u79cd\u53ef\u80fd\u6027\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684 1.24 \u4ebf\u53c2\u6570 GPT-2 \u6a21\u578b\uff08\u4e0b\u4e00\u4e2a\u5355\u8bcd\u9884\u6d4b\uff09\u5728 13 \u4ebf\u4e2a\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6807\u8bb0\u4e0a\u3002\u5c3d\u7ba1\u6bd4\u5728\u6570\u4e07\u4ebf\u4e2a\u6807\u8bb0\u4e0a\u8bad\u7ec3\u7684\u8f83\u5927 LLM \u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u9884\u6d4b\u795e\u7ecf\u79d1\u5b66\u7ed3\u679c\u65b9\u9762\u8fbe\u5230\u4e86\u4e13\u5bb6\u7ea7\u8868\u73b0\u3002\u5728\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u4e0a\u8bad\u7ec3\u7684\u5c0f\u578b\u6a21\u578b\u5728\u4f7f\u7528\u4e13\u95e8\u9488\u5bf9\u795e\u7ecf\u79d1\u5b66\u6587\u672c\u8bad\u7ec3\u7684\u5206\u8bcd\u5668\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u65f6\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u6216\u8005\u5f53\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u7528\u4e8e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684 GPT-2 \u65f6\u53d6\u5f97\u4e86\u6210\u529f\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5c0f\u578b LLM \u4e5f\u53ef\u4ee5\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u65b9\u6cd5\u83b7\u5f97\u4e13\u5bb6\u7ea7\u8868\u73b0\u3002</paragraph>", "author": "Xiaoliang Luo et.al.", "authors": "Xiaoliang Luo, Guangzhi Sun, Bradley C. Love", "id": "2405.09395v1", "paper_url": "http://arxiv.org/abs/2405.09395v1", "repo": "null"}}