{"2409.03516": {"publish_time": "2024-09-05", "title": "LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution", "paper_summary": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have\ndemonstrated impressive performance. However, they suffer from significant\ncomplexity, resulting in high inference times and memory usage. Additionally,\nViT models using Window Self-Attention (WSA) face challenges in processing\nregions outside their windows. To address these issues, we propose the\nLow-to-high Multi-Level Transformer (LMLT), which employs attention with\nvarying feature sizes for each head. LMLT divides image features along the\nchannel dimension, gradually reduces spatial size for lower heads, and applies\nself-attention to each head. This approach effectively captures both local and\nglobal information. By integrating the results from lower heads into higher\nheads, LMLT overcomes the window boundary issues in self-attention. Extensive\nexperiments show that our model significantly reduces inference time and GPU\nmemory usage while maintaining or even surpassing the performance of\nstate-of-the-art ViT-based Image Super-Resolution methods. Our codes are\navailiable at https://github.com/jwgdmkj/LMLT.", "paper_summary_zh": "\u6700\u8fd1\u57fa\u4e8e\u89c6\u89c9\u8f6c\u6362\u5668 (ViT) \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5df2\u7ecf\u8bc1\u660e\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u906d\u53d7\u7740\u5de8\u5927\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u8f83\u9ad8\u7684\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u7a97\u53e3\u81ea\u6ce8\u610f\u529b (WSA) \u7684 ViT \u6a21\u578b\u5728\u5904\u7406\u7a97\u53e3\u5916\u7684\u533a\u57df\u65f6\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5230\u9ad8\u7684\u591a\u7ea7\u8f6c\u6362\u5668 (LMLT)\uff0c\u5b83\u4e3a\u6bcf\u4e2a\u5934\u91c7\u7528\u5177\u6709\u4e0d\u540c\u7279\u5f81\u5927\u5c0f\u7684\u6ce8\u610f\u529b\u3002LMLT \u6cbf\u901a\u9053\u7ef4\u5ea6\u5212\u5206\u56fe\u50cf\u7279\u5f81\uff0c\u9010\u6e10\u51cf\u5c0f\u8f83\u4f4e\u5934\u7684\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5934\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6355\u83b7\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u8f83\u4f4e\u5934\u7684\u7ed3\u679c\u6574\u5408\u5230\u8f83\u9ad8\u5934\u4e2d\uff0cLMLT \u514b\u670d\u4e86\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u7a97\u53e3\u8fb9\u754c\u95ee\u9898\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u663e\u7740\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c GPU \u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u751a\u81f3\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8e ViT \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jwgdmkj/LMLT \u83b7\u5f97\u3002", "author": "Jeongsoo Kim et.al.", "authors": "Jeongsoo Kim, Jongho Nang, Junsuk Choe", "id": "2409.03516v1", "paper_url": "http://arxiv.org/abs/2409.03516v1", "repo": "https://github.com/jwgdmkj/lmlt"}}