{"2407.06538": {"publish_time": "2024-07-09", "title": "Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study", "paper_summary": "Neural Machine Translation (NMT) remains a formidable challenge, especially\nwhen dealing with low-resource languages. Pre-trained sequence-to-sequence\n(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive\nperformance in various low-resource NMT tasks. However, their pre-training has\nbeen confined to 50 languages, leaving out support for numerous low-resource\nlanguages, particularly those spoken in the Indian subcontinent. Expanding\nmBART-50's language support requires complex pre-training, risking performance\ndecline due to catastrophic forgetting. Considering these expanding challenges,\nthis paper explores a framework that leverages the benefits of a pre-trained\nlanguage model along with knowledge distillation in a seq2seq architecture to\nfacilitate translation for low-resource languages, including those not covered\nby mBART-50. The proposed framework employs a multilingual encoder-based\nseq2seq model as the foundational architecture and subsequently uses\ncomplementary knowledge distillation techniques to mitigate the impact of\nimbalanced training. Our framework is evaluated on three low-resource Indic\nlanguages in four Indic-to-Indic directions, yielding significant BLEU-4 and\nchrF improvements over baselines. Further, we conduct human evaluation to\nconfirm effectiveness of our approach. Our code is publicly available at\nhttps://github.com/raypretam/Two-step-low-res-NMT.", "paper_summary_zh": "\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT) \u4ecd\u7136\u662f\u4e00\u9805\u8271\u9245\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u6642\u3002\u9810\u8a13\u7df4\u5e8f\u5217\u5c0d\u5e8f\u5217 (seq2seq) \u591a\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 mBART-50\uff09\u5df2\u5728\u5404\u7a2e\u4f4e\u8cc7\u6e90 NMT \u4efb\u52d9\u4e2d\u5c55\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u4ed6\u5011\u7684\u9810\u8a13\u7df4\u50c5\u9650\u65bc 50 \u7a2e\u8a9e\u8a00\uff0c\u7121\u6cd5\u652f\u63f4\u8a31\u591a\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff0c\u7279\u5225\u662f\u5370\u5ea6\u6b21\u5927\u9678\u6240\u4f7f\u7528\u7684\u8a9e\u8a00\u3002\u64f4\u5145 mBART-50 \u7684\u8a9e\u8a00\u652f\u63f4\u9700\u8981\u8907\u96dc\u7684\u9810\u8a13\u7df4\uff0c\u7531\u65bc\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u53ef\u80fd\u6703\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u3002\u8003\u91cf\u5230\u9019\u4e9b\u64f4\u5145\u6311\u6230\uff0c\u672c\u6587\u63a2\u8a0e\u4e00\u500b\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u7684\u512a\u9ede\uff0c\u4ee5\u53ca seq2seq \u67b6\u69cb\u4e2d\u7684\u77e5\u8b58\u63d0\u7149\uff0c\u4ee5\u4fc3\u9032\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7684\u7ffb\u8b6f\uff0c\u5305\u62ec mBART-50 \u672a\u6db5\u84cb\u7684\u8a9e\u8a00\u3002\u5efa\u8b70\u7684\u67b6\u69cb\u63a1\u7528\u591a\u8a9e\u8a00\u7de8\u78bc\u5668\u70ba\u57fa\u790e\u7684 seq2seq \u6a21\u578b\u4f5c\u70ba\u57fa\u790e\u67b6\u69cb\uff0c\u7136\u5f8c\u4f7f\u7528\u88dc\u5145\u77e5\u8b58\u63d0\u7149\u6280\u8853\u4f86\u6e1b\u8f15\u4e0d\u5e73\u8861\u8a13\u7df4\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u67b6\u69cb\u5728\u56db\u500b\u5370\u5ea6\u8a9e\u5230\u5370\u5ea6\u8a9e\u65b9\u5411\u7684\u4e09\u7a2e\u4f4e\u8cc7\u6e90\u5370\u5ea6\u8a9e\u8a00\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u7522\u751f\u986f\u8457\u7684 BLEU-4 \u548c chrF \u512a\u5316\uff0c\u8d85\u8d8a\u57fa\u7dda\u3002\u6b64\u5916\uff0c\u6211\u5011\u9032\u884c\u4eba\u5de5\u8a55\u4f30\uff0c\u4ee5\u78ba\u8a8d\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/raypretam/Two-step-low-res-NMT \u516c\u958b\u53d6\u5f97\u3002", "author": "Aniruddha Roy et.al.", "authors": "Aniruddha Roy, Pretam Ray, Ayush Maheshwari, Sudeshna Sarkar, Pawan Goyal", "id": "2407.06538v1", "paper_url": "http://arxiv.org/abs/2407.06538v1", "repo": "null"}}