{"2501.05414": {"publish_time": "2025-01-09", "title": "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation", "paper_summary": "Existing benchmarks for evaluating long-context language models (LCLMs)\nprimarily focus on long-context recall, requiring models to produce short\nresponses based on a few critical snippets while processing thousands of\nirrelevant tokens. We introduce LongProc (Long Procedural Generation), a new\nbenchmark that requires both the integration of highly dispersed information\nand long-form generation. LongProc consists of six diverse procedural\ngeneration tasks, such as extracting structured information from HTML pages\ninto a TSV format and executing complex search procedures to create travel\nplans. These tasks challenge LCLMs by testing their ability to follow detailed\nprocedural instructions, synthesize and reason over dispersed information, and\ngenerate structured, long-form outputs (up to 8K tokens). Furthermore, as these\ntasks adhere to deterministic procedures and yield structured outputs, they\nenable reliable rule-based evaluation. We evaluate 17 LCLMs on LongProc across\nthree difficulty levels, with maximum numbers of output tokens set at 500, 2K,\nand 8K. Notably, while all tested models claim a context window size above 32K\ntokens, open-weight models typically falter on 2K-token tasks, and\nclosed-source models like GPT-4o show significant degradation on 8K-token\ntasks. Further analysis reveals that LCLMs struggle to maintain long-range\ncoherence in long-form generations. These findings highlight critical\nlimitations in current LCLMs and suggest substantial room for improvement. Data\nand code available at: https://princeton-pli.github.io/LongProc", "paper_summary_zh": "\u73fe\u6709\u7684\u7528\u65bc\u8a55\u4f30\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b (LCLM) \u7684\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u9577\u8a9e\u5883\u53ec\u56de\u4e0a\uff0c\u8981\u6c42\u6a21\u578b\u6839\u64da\u5e7e\u500b\u95dc\u9375\u7247\u6bb5\u7522\u751f\u7c21\u77ed\u7684\u56de\u61c9\uff0c\u540c\u6642\u8655\u7406\u6578\u5343\u500b\u4e0d\u76f8\u95dc\u7684\u7b26\u865f\u3002\u6211\u5011\u5f15\u5165\u4e86 LongProc\uff08\u9577\u7a0b\u5e8f\u751f\u6210\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u5b83\u9700\u8981\u9ad8\u5ea6\u5206\u6563\u7684\u8cc7\u8a0a\u6574\u5408\u548c\u9577\u683c\u5f0f\u751f\u6210\u3002LongProc \u5305\u542b\u516d\u9805\u4e0d\u540c\u7684\u7a0b\u5e8f\u751f\u6210\u4efb\u52d9\uff0c\u4f8b\u5982\u5c07\u7d50\u69cb\u5316\u8cc7\u8a0a\u5f9e HTML \u9801\u9762\u63d0\u53d6\u5230 TSV \u683c\u5f0f\uff0c\u4ee5\u53ca\u57f7\u884c\u8907\u96dc\u7684\u641c\u5c0b\u7a0b\u5e8f\u4f86\u5efa\u7acb\u65c5\u904a\u8a08\u756b\u3002\u9019\u4e9b\u4efb\u52d9\u900f\u904e\u6e2c\u8a66 LCLM \u9075\u5faa\u8a73\u7d30\u7a0b\u5e8f\u8aaa\u660e\u3001\u7d9c\u5408\u548c\u63a8\u7406\u5206\u6563\u8cc7\u8a0a\u4ee5\u53ca\u751f\u6210\u7d50\u69cb\u5316\u3001\u9577\u683c\u5f0f\u8f38\u51fa\uff08\u6700\u591a 8K \u500b\u7b26\u865f\uff09\u7684\u80fd\u529b\uff0c\u5c0d\u5176\u63d0\u51fa\u6311\u6230\u3002\u6b64\u5916\uff0c\u7531\u65bc\u9019\u4e9b\u4efb\u52d9\u9075\u5faa\u78ba\u5b9a\u6027\u7684\u7a0b\u5e8f\u4e26\u7522\u751f\u7d50\u69cb\u5316\u7684\u8f38\u51fa\uff0c\u56e0\u6b64\u5b83\u5011\u80fd\u9032\u884c\u53ef\u9760\u7684\u57fa\u65bc\u898f\u5247\u7684\u8a55\u4f30\u3002\u6211\u5011\u5728\u4e09\u500b\u96e3\u5ea6\u7b49\u7d1a\u4e0a\u5c0d 17 \u500b LCLM \u9032\u884c LongProc \u8a55\u4f30\uff0c\u5c07\u8f38\u51fa\u7b26\u865f\u7684\u6700\u5927\u6578\u91cf\u8a2d\u5b9a\u70ba 500\u30012K \u548c 8K\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u96d6\u7136\u6240\u6709\u6e2c\u8a66\u7684\u6a21\u578b\u90fd\u5ba3\u7a31\u5176\u8a9e\u5883\u7a97\u53e3\u5927\u5c0f\u8d85\u904e 32K \u500b\u7b26\u865f\uff0c\u4f46\u958b\u653e\u6b0a\u91cd\u6a21\u578b\u901a\u5e38\u5728 2K \u7b26\u865f\u4efb\u52d9\u4e2d\u5931\u6557\uff0c\u800c\u50cf GPT-4o \u9019\u6a23\u7684\u9589\u6e90\u6a21\u578b\u5728 8K \u7b26\u865f\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u4e0b\u964d\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cLCLM \u96e3\u4ee5\u5728\u9577\u683c\u5f0f\u751f\u6210\u4e2d\u7dad\u6301\u9577\u7a0b\u76f8\u5e72\u6027\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u7576\u524d LCLM \u7684\u95dc\u9375\u9650\u5236\uff0c\u4e26\u8868\u660e\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://princeton-pli.github.io/LongProc", "author": "Xi Ye et.al.", "authors": "Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen", "id": "2501.05414v1", "paper_url": "http://arxiv.org/abs/2501.05414v1", "repo": "null"}}