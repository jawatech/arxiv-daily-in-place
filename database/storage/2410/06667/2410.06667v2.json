{"2410.06667": {"publish_time": "2024-10-09", "title": "Large Language Models as Code Executors: An Exploratory Study", "paper_summary": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u529f\u80fd\u5df2\u986f\u8457\u9032\u6b65\uff0c\n\u5f9e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u64f4\u5c55\u5230\u8907\u96dc\u4efb\u52d9\uff0c\u4f8b\u5982\u7a0b\u5f0f\u78bc\u7406\u89e3\u548c\u7522\u751f\u3002\u6211\u5011\u5c07 LLM \u7684\u529f\u80fd\u7bc4\u570d\u64f4\u5c55\u5230\u66f4\u5ee3\u6cdb\u7684\u80cc\u666f\u4e2d\uff0c\u4f7f\u7528 LLM \u57f7\u884c\u7a0b\u5f0f\u78bc\u7247\u6bb5\u4ee5\u53d6\u5f97\u8f38\u51fa\u3002\u672c\u6587\u958b\u5275\u4e86\u63a2\u7d22 LLM \u4f5c\u70ba\u7a0b\u5f0f\u78bc\u57f7\u884c\u5668\u7684\u5148\u6cb3\uff0c\u5176\u4e2d\u7a0b\u5f0f\u78bc\u7247\u6bb5\u76f4\u63a5\u63d0\u4f9b\u7d66\u6a21\u578b\u57f7\u884c\uff0c\u4e26\u8fd4\u56de\u8f38\u51fa\u3002\u6211\u5011\u662f\u7b2c\u4e00\u500b\u5168\u9762\u6aa2\u67e5\u5404\u7a2e LLM \u7684\u53ef\u884c\u6027\uff0c\u5305\u62ec OpenAI \u7684 o1\u3001GPT-4o\u3001GPT-3.5\u3001DeepSeek \u548c Qwen-Coder\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0co1 \u6a21\u578b\u5728\u7a0b\u5f0f\u78bc\u57f7\u884c\u4e2d\u9054\u5230\u4e86 90% \u4ee5\u4e0a\u7684\u6e96\u78ba\u5ea6\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5247\u8868\u73fe\u51fa\u8f03\u4f4e\u7684\u6e96\u78ba\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u53cd\u8986\u6307\u4ee4\u63d0\u793a (IIP) \u6280\u8853\uff0c\u9010\u884c\u8655\u7406\u7a0b\u5f0f\u78bc\u7247\u6bb5\uff0c\u5c07\u8f03\u5f31\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\u5e73\u5747\u63d0\u9ad8\u4e86 7.22%\uff08\u6700\u9ad8\u63d0\u9ad8\u4e86 18.96%\uff09\uff0c\u4e26\u4e14\u91dd\u5c0d CoT \u63d0\u793a\u7684\u7d55\u5c0d\u5e73\u5747\u6539\u9032\u70ba 3.86%\uff08\u6700\u9ad8\u63d0\u9ad8\u4e86 19.46%\uff09\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u7a81\u51fa\u4e86 LLM \u5728\u7de8\u78bc\u65b9\u9762\u7684\u8b8a\u9769\u6f5b\u529b\uff0c\u9084\u70ba\u672a\u4f86\u81ea\u52d5\u5316\u7a0b\u5f0f\u8a2d\u8a08\u548c\u5b8c\u6210\u8907\u96dc\u4efb\u52d9\u7684\u9032\u6b65\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Chenyang Lyu et.al.", "authors": "Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang", "id": "2410.06667v2", "paper_url": "http://arxiv.org/abs/2410.06667v2", "repo": "null"}}