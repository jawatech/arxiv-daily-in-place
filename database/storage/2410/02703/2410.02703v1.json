{"2410.02703": {"publish_time": "2024-10-03", "title": "Selective Attention Improves Transformer", "paper_summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.", "paper_summary_zh": "\u6ce8\u610f\u529b\u8108\u7d61\u4e2d\u7684\u975e\u5fc5\u8981\u5143\u7d20\u6703\u964d\u4f4e\u6548\u80fd\u3002\u6211\u5011\u5f15\u9032\u9078\u64c7\u6027\u6ce8\u610f\u529b\uff0c\u9019\u662f\u6a19\u6e96\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u7c21\u55ae\u7121\u53c3\u6578\u8b8a\u66f4\uff0c\u53ef\u6e1b\u5c11\u5c0d\u975e\u5fc5\u8981\u5143\u7d20\u7684\u6ce8\u610f\u529b\u3002\u9078\u64c7\u6027\u6ce8\u610f\u529b\u53ef\u63d0\u5347\u8a9e\u8a00\u6a21\u578b\u6548\u80fd\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u548c\u8108\u7d61\u9577\u5ea6\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u9078\u64c7\u6027\u6ce8\u610f\u529b\u8a13\u7df4\u7684\u5404\u7a2eTransformer\u5728 C4 \u4e0a\u4ee5\u8a9e\u8a00\u6a21\u578b\u76ee\u6a19\u8a13\u7df4\uff0c\u5176\u6548\u80fd\u7b49\u540c\u65bc\u6a19\u6e96Transformer\uff0c\u800c\u5f8c\u8005\u7684\u6ce8\u610f\u529b\u6a21\u7d44\u5177\u6709\u7d04 2 \u500d\u7684\u6ce8\u610f\u529b\u982d\u548c\u53c3\u6578\u3002\u9078\u64c7\u6027\u6ce8\u610f\u529b\u9084\u80fd\u7e2e\u5c0f\u6ce8\u610f\u529b\u8108\u7d61\u7de9\u885d\u5340\u7684\u5927\u5c0f\uff0c\u9032\u800c\u5927\u5e45\u6e1b\u5c11\u63a8\u8ad6\u671f\u9593\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u9078\u64c7\u6027\u6ce8\u610f\u529b\u8a13\u7df4\u7684 100M \u53c3\u6578Transformer\u5728 C4 \u4e0a\u7684\u8108\u7d61\u5927\u5c0f\u70ba 512\u30011,024 \u548c 2,048\uff0c\u5176\u6ce8\u610f\u529b\u6a21\u7d44\u5206\u5225\u6bd4\u6c92\u6709\u9078\u64c7\u6027\u6ce8\u610f\u529b\u7684Transformer\u6e1b\u5c11 16 \u500d\u300125 \u500d\u548c 47 \u500d\u7684\u8a18\u61b6\u9ad4\uff0c\u4e14\u9a57\u8b49\u56f0\u60d1\u5ea6\u76f8\u540c\u3002", "author": "Yaniv Leviathan et.al.", "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias", "id": "2410.02703v1", "paper_url": "http://arxiv.org/abs/2410.02703v1", "repo": "null"}}