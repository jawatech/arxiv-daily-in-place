{"2410.19225": {"publish_time": "2024-10-25", "title": "Hierarchical Mixture of Experts: Generalizable Learning for High-Level Synthesis", "paper_summary": "High-level synthesis (HLS) is a widely used tool in designing Field\nProgrammable Gate Array (FPGA). HLS enables FPGA design with software\nprogramming languages by compiling the source code into an FPGA circuit. The\nsource code includes a program (called ``kernel'') and several pragmas that\ninstruct hardware synthesis, such as parallelization, pipeline, etc. While it\nis relatively easy for software developers to design the program, it heavily\nrelies on hardware knowledge to design the pragmas, posing a big challenge for\nsoftware developers. Recently, different machine learning algorithms, such as\nGNNs, have been proposed to automate the pragma design via performance\nprediction. However, when applying the trained model on new kernels, the\nsignificant domain shift often leads to unsatisfactory performance. We propose\na more domain-generalizable model structure: a two-level hierarchical Mixture\nof Experts (MoE), that can be flexibly adapted to any GNN model. Different\nexpert networks can learn to deal with different regions in the representation\nspace, and they can utilize similar patterns between the old kernels and new\nkernels. In the low-level MoE, we apply MoE on three natural granularities of a\nprogram: node, basic block, and graph. The high-level MoE learns to aggregate\nthe three granularities for the final decision. To stably train the\nhierarchical MoE, we further propose a two-stage training method. Extensive\nexperiments verify the effectiveness of the hierarchical MoE.", "paper_summary_zh": "\u9ad8\u968e\u7d9c\u5408\uff08HLS\uff09\u662f\u8a2d\u8a08\u73fe\u5834\u53ef\u7de8\u7a0b\u9598\u9663\u5217\uff08FPGA\uff09\u4e2d\u5ee3\u6cdb\u4f7f\u7528\u7684\u5de5\u5177\u3002HLS \u900f\u904e\u5c07\u539f\u59cb\u78bc\u7de8\u8b6f\u6210 FPGA \u96fb\u8def\uff0c\u4f7f\u7528\u8edf\u9ad4\u7a0b\u5f0f\u8a9e\u8a00\u9032\u884c FPGA \u8a2d\u8a08\u3002\u539f\u59cb\u78bc\u5305\u542b\u4e00\u500b\u7a0b\u5f0f\uff08\u7a31\u70ba\u300c\u6838\u5fc3\u300d\uff09\u548c\u591a\u500b\u6307\u5c0e\u786c\u9ad4\u7d9c\u5408\u7684\u6307\u793a\uff0c\u4f8b\u5982\u5e73\u884c\u5316\u3001\u7ba1\u7dda\u7b49\u3002\u96d6\u7136\u8edf\u9ad4\u958b\u767c\u4eba\u54e1\u8a2d\u8a08\u7a0b\u5f0f\u76f8\u5c0d\u5bb9\u6613\uff0c\u4f46\u5b83\u6975\u5ea6\u4f9d\u8cf4\u786c\u9ad4\u77e5\u8b58\u4f86\u8a2d\u8a08\u6307\u793a\uff0c\u9019\u5c0d\u8edf\u9ad4\u958b\u767c\u4eba\u54e1\u4f86\u8aaa\u662f\u4e00\u5927\u6311\u6230\u3002\u6700\u8fd1\uff0c\u4e0d\u540c\u7684\u6a5f\u5668\u5b78\u7fd2\u6f14\u7b97\u6cd5\uff0c\u4f8b\u5982 GNN\uff0c\u5df2\u88ab\u63d0\u51fa\u7528\u65bc\u900f\u904e\u6548\u80fd\u9810\u6e2c\u81ea\u52d5\u9032\u884c\u6307\u793a\u8a2d\u8a08\u3002\u7136\u800c\uff0c\u5728\u65b0\u7684\u6838\u5fc3\u4e0a\u61c9\u7528\u8a13\u7df4\u597d\u7684\u6a21\u578b\u6642\uff0c\u986f\u8457\u7684\u9818\u57df\u8f49\u79fb\u901a\u5e38\u6703\u5c0e\u81f4\u6548\u80fd\u4e0d\u4f73\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u66f4\u5177\u9818\u57df\u901a\u7528\u6027\u7684\u6a21\u578b\u7d50\u69cb\uff1a\u4e00\u500b\u4e8c\u968e\u5c64\u6df7\u5408\u5c08\u5bb6\uff08MoE\uff09\uff0c\u5b83\u53ef\u4ee5\u9748\u6d3b\u5730\u9069\u61c9\u4efb\u4f55 GNN \u6a21\u578b\u3002\u4e0d\u540c\u7684\u5c08\u5bb6\u7db2\u8def\u53ef\u4ee5\u5b78\u7fd2\u8655\u7406\u8868\u793a\u7a7a\u9593\u4e2d\u7684\u4e0d\u540c\u5340\u57df\uff0c\u4e26\u4e14\u5b83\u5011\u53ef\u4ee5\u5229\u7528\u820a\u6838\u5fc3\u548c\u65b0\u6838\u5fc3\u4e4b\u9593\u7684\u76f8\u4f3c\u6a21\u5f0f\u3002\u5728\u4f4e\u968e MoE \u4e2d\uff0c\u6211\u5011\u5c0d\u7a0b\u5f0f\u7684\u4e09\u500b\u81ea\u7136\u7c92\u5ea6\u61c9\u7528 MoE\uff1a\u7bc0\u9ede\u3001\u57fa\u672c\u5340\u584a\u548c\u5716\u3002\u9ad8\u968e MoE \u5b78\u7fd2\u5f59\u7e3d\u9019\u4e09\u500b\u7c92\u5ea6\u4ee5\u505a\u51fa\u6700\u7d42\u6c7a\u7b56\u3002\u70ba\u4e86\u7a69\u5b9a\u8a13\u7df4\u968e\u5c64\u5f0f MoE\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e00\u500b\u4e8c\u968e\u6bb5\u8a13\u7df4\u65b9\u6cd5\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u968e\u5c64\u5f0f MoE \u7684\u6709\u6548\u6027\u3002", "author": "Weikai Li et.al.", "authors": "Weikai Li, Ding Wang, Zijian Ding, Atefeh Sohrabizadeh, Zongyue Qin, Jason Cong, Yizhou Sun", "id": "2410.19225v1", "paper_url": "http://arxiv.org/abs/2410.19225v1", "repo": "null"}}