
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-05**|**A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**|Vanni Zavarella et.al.|[2408.02377v1](http://arxiv.org/abs/2408.02377v1)|null|
|**2024-08-05**|**Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction**|Albert Sawczyn et.al.|[2408.02337v1](http://arxiv.org/abs/2408.02337v1)|null|
|**2024-08-04**|**MedSyn: LLM-based Synthetic Medical Text Generation Framework**|Gleb Kumichev et.al.|[2408.02056v1](http://arxiv.org/abs/2408.02056v1)|null|
|**2024-08-04**|**DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models**|Bowen Wang et.al.|[2408.01933v2](http://arxiv.org/abs/2408.01933v2)|null|
|**2024-08-03**|**Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data**|Antonio De Santis et.al.|[2408.01700v1](http://arxiv.org/abs/2408.01700v1)|null|
|**2024-08-02**|**DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs**|Zhichun Wang et.al.|[2408.01154v1](http://arxiv.org/abs/2408.01154v1)|null|
|**2024-08-02**|**Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs**|Phillip Schneider et.al.|[2408.01088v1](http://arxiv.org/abs/2408.01088v1)|null|
|**2024-08-02**|**Automatic Extraction of Relationships among Motivations, Emotions and Actions from Natural Language Texts**|Fei Yang et.al.|[2408.00966v1](http://arxiv.org/abs/2408.00966v1)|null|
|**2024-08-01**|**DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks**|Guillermo Villar-Rodr√≠guez et.al.|[2408.00633v1](http://arxiv.org/abs/2408.00633v1)|null|
|**2024-08-01**|**On the Limitations and Prospects of Machine Unlearning for Generative AI**|Shiji Zhou et.al.|[2408.00376v1](http://arxiv.org/abs/2408.00376v1)|null|
|**2024-08-01**|**Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network**|Bin Cheng et.al.|[2408.00290v1](http://arxiv.org/abs/2408.00290v1)|null|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xinyi Pan et.al.|[2407.21483v3](http://arxiv.org/abs/2407.21483v3)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|null|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-30**|**Be aware of overfitting by hyperparameter optimization!**|Igor V. Tetko et.al.|[2407.20786v1](http://arxiv.org/abs/2407.20786v1)|null|
|**2024-07-30**|**Harvesting Textual and Structured Data from the HAL Publication Repository**|Francis Kulumba et.al.|[2407.20595v1](http://arxiv.org/abs/2407.20595v1)|null|
|**2024-07-30**|**CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**|Tianshi Zheng et.al.|[2407.20564v1](http://arxiv.org/abs/2407.20564v1)|null|
|**2024-07-30**|**Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**|Hossein Rajaby Faghihi et.al.|[2407.20513v1](http://arxiv.org/abs/2407.20513v1)|null|
|**2024-07-29**|**What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**|Navapat Nananukul et.al.|[2407.20382v1](http://arxiv.org/abs/2407.20382v1)|null|
|**2024-07-29**|**MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**|Zehui Chen et.al.|[2407.20183v1](http://arxiv.org/abs/2407.20183v1)|[link](https://github.com/internlm/mindsearch)|
|**2024-07-29**|**rLLM: Relational Table Learning with LLMs**|Weichen Li et.al.|[2407.20157v1](http://arxiv.org/abs/2407.20157v1)|[link](https://github.com/rllm-project/rllm)|
|**2024-07-29**|**Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**|Yunsheng Wang et.al.|[2407.19643v2](http://arxiv.org/abs/2407.19643v2)|[link](https://github.com/iamryanshengwang/prometheus-chatbot)|
|**2024-07-29**|**TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**|Selma Wanna et.al.|[2407.19616v1](http://arxiv.org/abs/2407.19616v1)|null|
|**2024-07-27**|**Semantic Communication Enhanced by Knowledge Graph Representation Learning**|Nour Hello et.al.|[2407.19338v1](http://arxiv.org/abs/2407.19338v1)|null|
|**2024-07-26**|**GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**|Yuchen Shen et.al.|[2407.19039v1](http://arxiv.org/abs/2407.19039v1)|null|
|**2024-07-26**|**Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**|Yuni Susanti et.al.|[2407.18752v3](http://arxiv.org/abs/2407.18752v3)|[link](https://github.com/littleflow3r/kg-structure-as-prompt)|
|**2024-07-26**|**Using GPT-4 to guide causal machine learning**|Anthony C. Constantinou et.al.|[2407.18607v1](http://arxiv.org/abs/2407.18607v1)|null|
|**2024-07-26**|**Multi-turn Response Selection with Commonsense-enhanced Language Models**|Yuandong Wang et.al.|[2407.18479v1](http://arxiv.org/abs/2407.18479v1)|null|
|**2024-07-25**|**Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**|Sindhura Kommu et.al.|[2407.18181v1](http://arxiv.org/abs/2407.18181v1)|null|
|**2024-07-24**|**MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**|Arya Bulusu et.al.|[2407.17544v1](http://arxiv.org/abs/2407.17544v1)|[link](https://github.com/emergenceai/mathviz-e)|
|**2024-07-23**|**Ranking protein-protein models with large language models and graph neural networks**|Xiaotong Xu et.al.|[2407.16375v1](http://arxiv.org/abs/2407.16375v1)|[link](https://github.com/haddocking/deeprank-gnn-esm)|
|**2024-07-23**|**PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**|Jaeyoung Kim et.al.|[2407.16329v1](http://arxiv.org/abs/2407.16329v1)|null|
|**2024-07-23**|**Graph-Structured Speculative Decoding**|Zhuocheng Gong et.al.|[2407.16207v1](http://arxiv.org/abs/2407.16207v1)|null|
|**2024-07-23**|**Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**|Yannick Assogba et.al.|[2407.21049v1](http://arxiv.org/abs/2407.21049v1)|null|
|**2024-07-23**|**Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**|Yang Liu et.al.|[2407.16127v1](http://arxiv.org/abs/2407.16127v1)|[link](https://github.com/nju-websoft/dift)|
|**2024-07-22**|**Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**|Soojin Yoon et.al.|[2407.15588v1](http://arxiv.org/abs/2407.15588v1)|[link](https://github.com/eralign/eralign)|
|**2024-07-22**|**Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**|Huanjing Zhao et.al.|[2407.15431v1](http://arxiv.org/abs/2407.15431v1)|null|
|**2024-07-22**|**LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**|Jiaxing Zhang et.al.|[2407.15351v2](http://arxiv.org/abs/2407.15351v2)|null|
|**2024-07-21**|**Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**|Yu Zhang et.al.|[2407.15141v1](http://arxiv.org/abs/2407.15141v1)|null|
|**2024-07-20**|**On the Design and Analysis of LLM-Based Algorithms**|Yanxi Chen et.al.|[2407.14788v1](http://arxiv.org/abs/2407.14788v1)|[link](https://github.com/modelscope/agentscope)|
|**2024-07-19**|**LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**|Chen-Chia Chang et.al.|[2407.18269v1](http://arxiv.org/abs/2407.18269v1)|null|
|**2024-07-19**|**Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**|Suvajit Patra et.al.|[2407.14224v1](http://arxiv.org/abs/2407.14224v1)|null|
|**2024-07-19**|**Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**|Quan Li et.al.|[2407.13989v1](http://arxiv.org/abs/2407.13989v1)|null|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699v1](http://arxiv.org/abs/2407.13699v1)|null|
|**2024-07-18**|**MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**|Guoli Yin et.al.|[2407.18961v2](http://arxiv.org/abs/2407.18961v2)|[link](https://github.com/apple/axlearn)|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725v1](http://arxiv.org/abs/2407.12725v1)|null|
|**2024-07-17**|**Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**|Youmin Ko et.al.|[2407.12703v3](http://arxiv.org/abs/2407.12703v3)|null|
|**2024-07-17**|**Abstraction Alignment: Comparing Model and Human Conceptual Relationships**|Angie Boggust et.al.|[2407.12543v1](http://arxiv.org/abs/2407.12543v1)|[link](https://github.com/mitvis/abstraction-alignment)|
|**2024-07-17**|**Struct-X: Enhancing Large Language Models Reasoning with Structured Data**|Xiaoyu Tan et.al.|[2407.12522v1](http://arxiv.org/abs/2407.12522v1)|null|
|**2024-07-17**|**Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**|Alexander R. Pelletier et.al.|[2407.12888v1](http://arxiv.org/abs/2407.12888v1)|[link](https://github.com/pinglab-utils/rugged)|
|**2024-07-16**|**A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**|He Chang et.al.|[2407.11638v1](http://arxiv.org/abs/2407.11638v1)|null|
|**2024-07-16**|**Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**|Kai Guo et.al.|[2407.12068v2](http://arxiv.org/abs/2407.12068v2)|null|
|**2024-07-16**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393v2](http://arxiv.org/abs/2407.11393v2)|[link](https://github.com/SamsungLabs/CIC-BART-SSA)|
|**2024-07-15**|**Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**|Shengjie Ma et.al.|[2407.10805v3](http://arxiv.org/abs/2407.10805v3)|null|
|**2024-07-15**|**Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**|Rui Yang et.al.|[2407.10794v1](http://arxiv.org/abs/2407.10794v1)|[link](https://github.com/irenezihuili/cgprompt)|
|**2024-07-15**|**GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**|Hannah Sansford et.al.|[2407.10793v1](http://arxiv.org/abs/2407.10793v1)|null|
|**2024-07-15**|**Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**|W. J. Meijer et.al.|[2407.10743v1](http://arxiv.org/abs/2407.10743v1)|null|
|**2024-07-14**|**AutoGRAMS: Autonomous Graphical Agent Modeling Software**|Ben Krause et.al.|[2407.10049v1](http://arxiv.org/abs/2407.10049v1)|[link](https://github.com/autograms/autograms)|
|**2024-07-13**|**FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**|Dimitris Papadopoulos et.al.|[2407.09888v1](http://arxiv.org/abs/2407.09888v1)|[link](https://github.com/lighteternal/farfetched_nlp)|
|**2024-07-12**|**GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**|Lecheng Kong et.al.|[2407.09709v1](http://arxiv.org/abs/2407.09709v1)|[link](https://github.com/jiaruifeng/gofa)|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450v1](http://arxiv.org/abs/2407.09450v1)|null|
|**2024-07-12**|**The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**|Matteo Belenchia et.al.|[2407.09441v1](http://arxiv.org/abs/2407.09441v1)|null|
|**2024-07-12**|**Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**|David N. Palacio et.al.|[2407.08983v1](http://arxiv.org/abs/2407.08983v1)|null|
|**2024-07-12**|**Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**|Ke Ji et.al.|[2407.08959v1](http://arxiv.org/abs/2407.08959v1)|null|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694v1](http://arxiv.org/abs/2407.08694v1)|null|
|**2024-07-11**|**Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**|Haoyi Xiong et.al.|[2407.08516v3](http://arxiv.org/abs/2407.08516v3)|null|
|**2024-07-10**|**A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**|Arastoo Zibaeirad et.al.|[2407.07966v1](http://arxiv.org/abs/2407.07966v1)|null|
|**2024-07-10**|**Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**|Hao-Tien Lewis Chiang et.al.|[2407.07775v2](http://arxiv.org/abs/2407.07775v2)|null|
|**2024-07-10**|**Teaching Transformers Causal Reasoning through Axiomatic Training**|Aniket Vashishtha et.al.|[2407.07612v1](http://arxiv.org/abs/2407.07612v1)|null|
|**2024-07-10**|**STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**|Aaron Zolnai-Lucas et.al.|[2407.12860v1](http://arxiv.org/abs/2407.12860v1)|[link](https://github.com/aaronzo/STAGE)|
|**2024-07-10**|**GLBench: A Comprehensive Benchmark for Graph with Large Language Models**|Yuhan Li et.al.|[2407.07457v2](http://arxiv.org/abs/2407.07457v2)|[link](https://github.com/nineabyss/glbench)|
|**2024-07-09**|**Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**|Ruiran Su et.al.|[2407.07038v1](http://arxiv.org/abs/2407.07038v1)|null|
|**2024-07-09**|**Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**|Yu-Guan Hsieh et.al.|[2407.06723v1](http://arxiv.org/abs/2407.06723v1)|null|
|**2024-07-09**|**Combining Knowledge Graphs and Large Language Models**|Amanda Kau et.al.|[2407.06564v1](http://arxiv.org/abs/2407.06564v1)|null|
|**2024-07-09**|**FuncEvalGMN: Evaluating Functional Correctness of SQL via Graph Matching Network**|Yi Zhan et.al.|[2407.14530v1](http://arxiv.org/abs/2407.14530v1)|null|
|**2024-07-08**|**MST5 -- Multilingual Question Answering over Knowledge Graphs**|Nikit Srivastava et.al.|[2407.06041v1](http://arxiv.org/abs/2407.06041v1)|[link](https://github.com/dice-group/MST5)|
|**2024-07-08**|**Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding**|Aaron Lohner et.al.|[2407.05910v1](http://arxiv.org/abs/2407.05910v1)|null|
|**2024-07-08**|**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**|Jiaqi Chen et.al.|[2407.05890v1](http://arxiv.org/abs/2407.05890v1)|null|
|**2024-07-08**|**KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions**|Yanxu Zhu et.al.|[2407.05868v1](http://arxiv.org/abs/2407.05868v1)|[link](https://github.com/yanxuzhu/kg-fpq)|
|**2024-07-07**|**Language Models Encode Collaborative Signals in Recommendation**|Leheng Sheng et.al.|[2407.05441v1](http://arxiv.org/abs/2407.05441v1)|[link](https://github.com/lehengthu/alpharec)|
|**2024-07-07**|**LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models**|Weizhi Tang et.al.|[2407.05434v1](http://arxiv.org/abs/2407.05434v1)|[link](https://github.com/rutatang/ltlbench)|
|**2024-07-05**|**Leveraging Graph Structures to Detect Hallucinations in Large Language Models**|Noa Nonkes et.al.|[2407.04485v1](http://arxiv.org/abs/2407.04485v1)|[link](https://github.com/noanonkes/Hallucination-Detection-in-LLMs)|
|**2024-07-05**|**AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**|Petr Anokhin et.al.|[2407.04363v1](http://arxiv.org/abs/2407.04363v1)|[link](https://github.com/airi-institute/arigraph)|
|**2024-07-04**|**Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM**|Peiran Yao et.al.|[2407.04067v1](http://arxiv.org/abs/2407.04067v1)|[link](https://github.com/U-Alberta/AMRS3)|
|**2024-07-04**|**Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning**|Lei Yu et.al.|[2407.03779v1](http://arxiv.org/abs/2407.03779v1)|null|
|**2024-07-03**|**BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations**|Zhantao Yang et.al.|[2407.03314v1](http://arxiv.org/abs/2407.03314v1)|null|
|**2024-07-03**|**Knowledge-based Consistency Testing of Large Language Models**|Sai Sathiesh Rajan et.al.|[2407.12830v1](http://arxiv.org/abs/2407.12830v1)|null|
|**2024-07-03**|**GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models**|Zike Yuan et.al.|[2407.02936v1](http://arxiv.org/abs/2407.02936v1)|[link](https://github.com/zikeyuan/gracore)|
|**2024-07-03**|**Croppable Knowledge Graph Embedding**|Yushan Zhu et.al.|[2407.02779v1](http://arxiv.org/abs/2407.02779v1)|null|
|**2024-07-02**|**Reasoning in Large Language Models: A Geometric Perspective**|Romain Cosentino et.al.|[2407.02678v1](http://arxiv.org/abs/2407.02678v1)|null|
|**2024-07-02**|**LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison**|Devam Mondal et.al.|[2407.02659v2](http://arxiv.org/abs/2407.02659v2)|null|
|**2024-07-02**|**Multi-Peptide: Multimodality Leveraged Language-Graph Learning of Peptide Properties**|Srivathsan Badrinarayanan et.al.|[2407.03380v1](http://arxiv.org/abs/2407.03380v1)|[link](https://github.com/srivathsanb14/multipeptide)|
|**2024-07-02**|**Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**|Pritish Sahu et.al.|[2407.02352v1](http://arxiv.org/abs/2407.02352v1)|null|
|**2024-07-02**|**Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**|Nishant Balepur et.al.|[2407.01992v1](http://arxiv.org/abs/2407.01992v1)|null|
|**2024-07-01**|**CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents**|Tianqi Xu et.al.|[2407.01511v1](http://arxiv.org/abs/2407.01511v1)|[link](https://github.com/camel-ai/crab)|
|**2024-07-01**|**Dynamic Few-Shot Learning for Knowledge Graph Question Answering**|Jacopo D'Abramo et.al.|[2407.01409v1](http://arxiv.org/abs/2407.01409v1)|null|
|**2024-07-01**|**Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters**|Daniil Gurgurov et.al.|[2407.01406v2](http://arxiv.org/abs/2407.01406v2)|[link](https://github.com/d-gurgurov/Injecting-Commonsense-Knowledge-into-LLMs)|
|**2024-07-01**|**SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model**|Lingyue Fu et.al.|[2407.01245v2](http://arxiv.org/abs/2407.01245v2)|null|

#### Abstracts
##### **A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**
2408.02377v1 by Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli

Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Â∑≤ÊàêÂäüÊáâÁî®ÊñºÂàÜÊûêË§áÈõúÁöÑÁßëÂ≠∏ÊäÄË°ìÈ†òÂüüÔºåËá™Âãï KG ÁîüÊàêÊñπÊ≥ïÈÄöÂ∏∏Âª∫ÊßãÊñºÈóú‰øÇËêÉÂèñÊ®°Âûã‰∏äÔºåÊçïÊçâÊñáÊú¨‰∏≠È†òÂüüÂØ¶È´î‰πãÈñìÁöÑÁ¥∞Á≤íÂ∫¶Èóú‰øÇ„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈóú‰øÇÂÆåÂÖ®ÈÅ©Áî®ÊñºÂêÑÁßëÂ≠∏È†òÂüüÔºå‰ΩÜÁèæÊúâÊ®°ÂûãÊòØÁî® SciERC Á≠âÂ∞ëÊï∏ÁâπÂÆöÈ†òÂüüÁöÑË≥áÊñôÈõÜË®ìÁ∑¥ÔºåËÄå‰∏îÂú®Êñ∞ÁõÆÊ®ôÈ†òÂüüÁöÑË°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®Êú¨Ë´ñÊñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÑàÁµ°Â≠∏ÁøíËÉΩÂäõÔºåÂü∑Ë°åÂèóÊû∂ÊßãÁ¥ÑÊùüÁöÑË≥áÊñôÊ®ôË®ªÔºåÊî∂ÈõÜÈ†òÂüüÂÖßË®ìÁ∑¥ÂØ¶‰æãÔºåÁî®ÊñºÈÉ®ÁΩ≤Âú®Âª∫ÁØâ„ÄÅÁáüÈÄ†„ÄÅÂ∑•Á®ãÂíåÁáüÈÅã (AECO) È†òÂüüÁ†îÁ©∂Ë´ñÊñáÊ®ôÈ°åÂíåÊëòË¶ÅÁöÑÂü∫Êñº Transformer ÁöÑÈóú‰øÇËêÉÂèñÊ®°Âûã„ÄÇÈÄèÈÅéË©ï‰º∞Áõ∏Â∞çÊñºÂú®È†òÂüüÂ§ñË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂü∫Ê∫ñÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÁöÑÊïàËÉΩÊèêÂçáÔºåÊàëÂÄëÂ±ïÁ§∫ÈÄèÈÅé‰ΩøÁî®Â∏∂ÊúâÁµêÊßãÂåñÊèêÁ§∫ÁöÑÂ∞ëÈáèÂ≠∏ÁøíÁ≠ñÁï•Ôºå‰ª•ÂèäÂÉÖÊúÄÂ∞ëÁöÑÂ∞àÂÆ∂Ê®ôË®ªÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÂèØËÉΩÊîØÊè¥ÁßëÂ≠∏ KG ÁîüÊàêÊ®°ÂûãÁöÑÈ†òÂüüÈÅ©Êáâ„ÄÇ

##### **Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction**
2408.02337v1 by Albert Sawczyn, Katsiaryna Viarenich, Konrad Wojtasik, Aleksandra Domoga≈Ça, Marcin Oleksy, Maciej Piasecki, Tomasz Kajdanowicz

Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÊ©üÂô®Ëàá‰∫∫È°ûÁöÑË™ûË®Ä‰∫íÂãïÔºåÂÖ∂‰∏≠ÂïèÁ≠î (QA) Á≥ªÁµ±ÊâÆÊºî‰∫ÜÈóúÈçµËßíËâ≤„ÄÇÁü•Ë≠òÂ∫´ÂïèÁ≠î (KBQA) ‰ªªÂãôÂà©Áî®ÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂèØ‰ª•ËôïÁêÜÂ§ßÈáèÁöÑÁü•Ë≠òÂØÜÈõÜÂûãÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåKBQA Ë≥áÊñôÈõÜÂ≠òÂú®ËëóÈ°ØËëóÁöÑÂ∑ÆË∑ùÔºåÁâπÂà•ÊòØÂ∞çÊñº‰ΩéË≥áÊ∫êË™ûË®Ä„ÄÇË®±Â§öÁèæÊúâÁöÑÈÄô‰∫õË≥áÊñôÈõÜÂª∫ÊßãÁÆ°ÈÅìÂ∑≤Á∂ìÈÅéÊôÇ‰∏îÂú®‰∫∫Âäõ‰∏äÊïàÁéá‰Ωé‰∏ãÔºåËÄåÂÉèÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄôÊ®£ÁöÑÁèæ‰ª£ËºîÂä©Â∑•ÂÖ∑‰∏¶Êú™Ë¢´Áî®ÊñºÊ∏õÂ∞ëÂ∑•‰ΩúË≤†Ëºâ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëË®≠Ë®à‰∏¶ÂØ¶‰Ωú‰∫Ü‰∏ÄÁ®ÆÁèæ‰ª£ÁöÑÂçäËá™ÂãïÂåñÊñπÊ≥ï‰æÜÂª∫Á´ãË≥áÊñôÈõÜÔºåÊ∂µËìã‰∫ÜÂ∞àÈñÄÈáùÂ∞ç‰ΩéË≥áÊ∫êÁí∞Â¢ÉÈáèË∫´ÊâìÈÄ†ÁöÑ‰ªªÂãôÔºå‰æãÂ¶Ç KBQA„ÄÅÊ©üÂô®Èñ±ËÆÄÁêÜËß£ (MRC) ÂíåË≥áË®äÊ™¢Á¥¢ (IR)„ÄÇÊàëÂÄëÂü∑Ë°å‰∫ÜÈÄôÂÄãÁÆ°ÈÅì‰∏¶ÂºïÂÖ•‰∫Ü PUGG Ë≥áÊñôÈõÜÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊ≥¢Ëò≠ KBQA Ë≥áÊñôÈõÜÔºå‰ª•Âèä MRC Âíå IR ÁöÑÊñ∞Á©éË≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶‰Ωú„ÄÅÊúâË¶ãÂú∞ÁöÑÁôºÁèæ„ÄÅË©≥Á¥∞ÁöÑÁµ±Ë®àË≥áÊñôÂíåÂü∫Ê∫ñÊ®°ÂûãÁöÑË©ï‰º∞„ÄÇ

##### **MedSyn: LLM-based Synthetic Medical Text Generation Framework**
2408.02056v1 by Gleb Kumichev, Pavel Blinov, Yulia Kuzkina, Vasily Goncharov, Galina Zubkova, Nikolai Zenovkin, Aleksei Goncharov, Andrey Savchenko

Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.

ÊëòË¶ÅÔºöÂêàÊàêÊñáÊú¨ÁöÑÁîüÊàêËß£ÂÜ≥‰∫ÜÈöêÁßÅÊïèÊÑüÈ¢ÜÂüüÔºàÂ¶ÇÂåªÁñó‰øùÂÅ•Ôºâ‰∏≠Êï∞ÊçÆÂèØÁî®ÊÄßÁöÑÊåëÊàò„ÄÇÊú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂêàÊàêÊï∞ÊçÆÂú®ÂÆûÈôÖÂåªÁñóÁéØÂ¢É‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü MedSynÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂåªÂ≠¶ÊñáÊú¨ÁîüÊàêÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏éÂåªÂ≠¶Áü•ËØÜÂõæË∞± (MKG) Áõ∏ÁªìÂêà„ÄÇÊàë‰ª¨‰ΩøÁî® MKG ‰∏∫ÊèêÁ§∫ÈááÊ†∑ÂÖàÈ™åÂåªÂ≠¶‰ø°ÊÅØÔºåÂπ∂‰ΩøÁî® GPT-4 ÂíåÂæÆË∞ÉÁöÑ LLaMA Ê®°ÂûãÁîüÊàêÂêàÊàê‰∏¥Â∫äÊ≥®Èáä„ÄÇÊàë‰ª¨ÈÄöËøáÂú® ICD ‰ª£Á†ÅÈ¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ËØÑ‰º∞‰∫ÜÂêàÊàêÊï∞ÊçÆÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºå‰∏éÊ≤°ÊúâÂêàÊàêÊï∞ÊçÆÁöÑËÆæÁΩÆÁõ∏ÊØîÔºåÂêàÊàêÊï∞ÊçÆÂèØ‰ª•Â∞ÜÈáçË¶Å‰∏îÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ª£Á†ÅÁöÑÂàÜÁ±ªÂáÜÁ°ÆÊÄßÊèêÈ´òÂ§öËææ 17.8%„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫Ü‰∏∫ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂Êèê‰æõÊñ∞Êï∞ÊçÆÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊúÄÂ§ßÁöÑÂºÄÊîæÊ∫ê‰ª£Á†ÅÂêàÊàêÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë∂ÖËøá 41k ‰∏™Ê∂µÁõñ 219 ‰∏™ ICD-10 ‰ª£Á†ÅÁöÑ‰∏¥Â∫äÊ≥®Èáä„ÄÇ

##### **DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models**
2408.01933v2 by Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara

Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúÄËøëÂ±ïÁ§∫‰∫ÜÈùûÂá°ÁöÑËÉΩÂäõÔºåÊ∂µËìãÂª£Ê≥õÁöÑ‰ªªÂãôÂíåÊáâÁî®ÔºåÂåÖÊã¨ÈÜ´ÁôÇÈ†òÂüüÁöÑ‰ªªÂãôÂíåÊáâÁî®„ÄÇGPT-4 Á≠âÊ®°ÂûãÂú®ÈÜ´ÁôÇÂïèÈ°åËß£Á≠îÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂú®ËôïÁêÜÂØ¶ÈöõËá®Â∫äÂ†¥ÊôØ‰∏≠ÁöÑË§áÈõú‰ªªÂãôÊôÇÔºåÂèØËÉΩÊúÉÈù¢Ëá®Áº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜËá®Â∫äÁ≠ÜË®òË®∫Êñ∑Êé®ÁêÜÊï∏ÊìöÈõÜ (DiReCT)ÔºåÊó®Âú®Ë©ï‰º∞ LLM Ëàá‰∫∫È°ûÈÜ´ÁîüÁõ∏ÊØîÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂèØËß£ÈáãÊÄß„ÄÇÂÆÉÂåÖÂê´ 511 ÂÄãËá®Â∫äÁ≠ÜË®òÔºåÊØèÂÄãÁ≠ÜË®òÈÉΩÁ∂ìÈÅéÈÜ´Áîü‰ªîÁ¥∞Ë®ªËß£ÔºåË©≥Á¥∞Ë™™Êòé‰∫ÜÂæûËá®Â∫äÁ≠ÜË®ò‰∏≠ÁöÑËßÄÂØüÁµêÊûúÂà∞ÊúÄÁµÇË®∫Êñ∑ÁöÑË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ã„ÄÇÊ≠§Â§ñÔºåÈÇÑÊèê‰æõ‰∫ÜË®∫Êñ∑Áü•Ë≠òÂúñË≠úÔºå‰ª•Êèê‰æõÊé®ÁêÜÊâÄÈúÄÁöÑÂü∫Êú¨Áü•Ë≠òÔºåÈÄôÂèØËÉΩÊú™Ê∂µËìãÂú®ÁèæÊúâ LLM ÁöÑË®ìÁ∑¥Êï∏Êìö‰∏≠„ÄÇÂú® DiReCT ‰∏äÂ∞çÈ†òÂÖàÁöÑ LLM ÈÄ≤Ë°åË©ï‰º∞ÔºåÁôºÁèæÂÆÉÂÄëÁöÑÊé®ÁêÜËÉΩÂäõËàá‰∫∫È°ûÈÜ´ÁîüÁöÑÊé®ÁêÜËÉΩÂäõ‰πãÈñìÂ≠òÂú®È°ØËëóÂ∑ÆË∑ùÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÂ†¥ÊôØ‰∏≠ËÉΩÂ§†ÊúâÊïàÊé®ÁêÜÁöÑÊ®°ÂûãÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇ

##### **Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data**
2408.01700v1 by Antonio De Santis, Marco Balduini, Federico De Santis, Andrea Proia, Arsenio Leo, Marco Brambilla, Emanuele Della Valle

Aerospace manufacturing companies, such as Thales Alenia Space, design,
develop, integrate, verify, and validate products characterized by high
complexity and low volume. They carefully document all phases for each product
but analyses across products are challenging due to the heterogeneity and
unstructured nature of the data in documents. In this paper, we propose a
hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with
Large Language Models (LLMs) to extract and validate data contained in these
documents. We consider a case study focused on test data related to electronic
boards for satellites. To do so, we extend the Semantic Sensor Network
ontology. We store the metadata of the reports in a KG, while the actual test
results are stored in parquet accessible via a Virtual Knowledge Graph. The
validation process is managed using an LLM-based approach. We also conduct a
benchmarking study to evaluate the performance of state-of-the-art LLMs in
executing this task. Finally, we analyze the costs and benefits of automating
preexisting processes of manual data extraction and validation for subsequent
cross-report analyses.

ÊëòË¶ÅÔºöËà™Â§™Ë£ΩÈÄ†ÂÖ¨Âè∏Ôºå‰æãÂ¶ÇÊ≥∞Èõ∑Ëå≤ÈòøËêäÂ∞º‰∫ûÂ§™Á©∫ÂÖ¨Âè∏ÔºåË®≠Ë®à„ÄÅÈñãÁôº„ÄÅÊï¥Âêà„ÄÅÈ©óË≠âÂíåÈ©óË≠â‰ª•È´òË§áÈõúÂ∫¶Âíå‰ΩéÈ´îÁ©çÁÇ∫ÁâπÂæµÁöÑÁî¢ÂìÅ„ÄÇ‰ªñÂÄë‰ªîÁ¥∞Ë®òÈåÑÊØèÂÄãÁî¢ÂìÅÁöÑÊâÄÊúâÈöéÊÆµÔºå‰ΩÜÁî±ÊñºÊñá‰ª∂‰∏≠Ë≥áÊñôÁöÑÁï∞Ë≥™ÊÄßÂíåÈùûÁµêÊßãÂåñÊÄßË≥™ÔºåÂ∞éËá¥Ë∑®Áî¢ÂìÅÁöÑÂàÜÊûêÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÔºåÂà©Áî®Áü•Ë≠òÂúñË≠ú (KG) ÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æÜÊì∑ÂèñÂíåÈ©óË≠âÈÄô‰∫õÊñá‰ª∂‰∏≠ÂåÖÂê´ÁöÑË≥áÊñô„ÄÇÊàëÂÄëËÄÉÊÖÆ‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂ÔºåÈáçÈªûÂú®ÊñºË°õÊòüÈõªÂ≠êÈõªË∑ØÊùøÁöÑÊ∏¨Ë©¶Ë≥áÊñô„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊì¥ÂÖÖ‰∫ÜË™ûÁæ©ÊÑüÊ∏¨Âô®Á∂≤Ë∑ØÊú¨‰Ωì„ÄÇÊàëÂÄëÂ∞áÂ†±ÂëäÁöÑÂÖÉË≥áÊñôÂÑ≤Â≠òÂú® KG ‰∏≠ÔºåËÄåÂØ¶ÈöõÊ∏¨Ë©¶ÁµêÊûúÂÑ≤Â≠òÂú®ÂèØÈÄèÈÅéËôõÊì¨Áü•Ë≠òÂúñË≠úÂ≠òÂèñÁöÑ Parquet ‰∏≠„ÄÇÈ©óË≠âÈÅéÁ®ã‰ΩøÁî®Âü∫Êñº LLM ÁöÑÊñπÊ≥ïÁÆ°ÁêÜ„ÄÇÊàëÂÄëÈÇÑÈÄ≤Ë°åÂü∫Ê∫ñÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú®Âü∑Ë°åÊ≠§‰ªªÂãôÊôÇÁöÑÊïàËÉΩ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂàÜÊûê‰∫ÜËá™ÂãïÂåñÁèæÊúâÊâãÂãïË≥áÊñôÊì∑ÂèñÂíåÈ©óË≠âÁ®ãÂ∫èÁöÑÊàêÊú¨ÂíåÂ•ΩËôïÔºå‰ª•ÈÄ≤Ë°åÂæåÁ∫åÁöÑË∑®Â†±ÂëäÂàÜÊûê„ÄÇ

##### **DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs**
2408.01154v1 by Zhichun Wang, Xuan Chen

Entity Alignment (EA) aims to match equivalent entities in different
Knowledge Graphs (KGs), which is essential for knowledge fusion and
integration. Recently, embedding-based EA has attracted significant attention
and many approaches have been proposed. Early approaches primarily focus on
learning entity embeddings from the structural features of KGs, defined by
relation triples. Later methods incorporated entities' names and attributes as
auxiliary information to enhance embeddings for EA. However, these approaches
often used different techniques to encode structural and attribute information,
limiting their interaction and mutual enhancement. In this work, we propose a
dense entity retrieval framework for EA, leveraging language models to
uniformly encode various features of entities and facilitate nearest entity
search across KGs. Alignment candidates are first generated through entity
retrieval, which are subsequently reranked to determine the final alignments.
We conduct comprehensive experiments on both cross-lingual and monolingual EA
datasets, demonstrating that our approach achieves state-of-the-art performance
compared to existing EA methods.

ÊëòË¶ÅÔºöÂØ¶È´îÂ∞çÈΩä (EA) Êó®Âú®ÊØîÂ∞ç‰∏çÂêåÁü•Ë≠òÂúñË≠ú (KG) ‰∏≠ÁöÑÁ≠âÊïàÂØ¶È´îÔºåÈÄôÂ∞çÊñºÁü•Ë≠òËûçÂêàÂíåÊï¥ÂêàÈùûÂ∏∏ÈáçË¶Å„ÄÇÊúÄËøëÔºåÂü∫ÊñºÂµåÂÖ•ÁöÑ EA Â∑≤ÂºïËµ∑Áõ∏Áï∂Â§ßÁöÑÈóúÊ≥®Ôºå‰∏¶‰∏îÂ∑≤ÊèêÂá∫Ë®±Â§öÊñπÊ≥ï„ÄÇÊó©ÊúüÁöÑÊñπÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂæû KG ÁöÑÁµêÊßãÁâπÂæµ‰∏≠Â≠∏ÁøíÂØ¶È´îÂµåÂÖ•ÔºåÈÄô‰∫õÁâπÂæµÁî±Èóú‰øÇ‰∏âÂÖÉÁµÑÂÆöÁæ©„ÄÇÂæåÁ∫åÁöÑÊñπÊ≥ïÂ∞áÂØ¶È´îÁöÑÂêçÁ®±ÂíåÂ±¨ÊÄß‰ΩúÁÇ∫ËºîÂä©Ë≥áË®äÔºå‰ª•Â¢ûÂº∑ EA ÁöÑÂµåÂÖ•„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®‰∏çÂêåÁöÑÊäÄË°ì‰æÜÁ∑®Á¢ºÁµêÊßãÂíåÂ±¨ÊÄßË≥áË®äÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑ‰∫íÂãïÂíåÁõ∏‰∫íÂ¢ûÂº∑„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂØÜÈõÜÂØ¶È´îÊì∑ÂèñÊû∂ÊßãÔºåÁî®Êñº EAÔºåÂà©Áî®Ë™ûË®ÄÊ®°Âûã‰æÜÁµ±‰∏ÄÁ∑®Á¢ºÂØ¶È´îÁöÑÂêÑÁ®ÆÁâπÂæµÔºå‰∏¶‰øÉÈÄ≤Ë∑® KG ÁöÑÊúÄËøëÂØ¶È´îÊêúÂ∞ã„ÄÇÂ∞çÈΩäÂÄôÈÅ∏ËÄÖÈ¶ñÂÖàÈÄèÈÅéÂØ¶È´îÊì∑ÂèñÁî¢ÁîüÔºåÁÑ∂ÂæåÈáçÊñ∞ÊéíÂ∫è‰ª•Á¢∫ÂÆöÊúÄÁµÇÂ∞çÈΩä„ÄÇÊàëÂÄëÂ∞çË∑®Ë™ûË®ÄÂíåÂñÆË™ûË®Ä EA Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåË≠âÊòéËàáÁèæÊúâÁöÑ EA ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs**
2408.01088v1 by Phillip Schneider, Nektarios Machner, Kristiina Jokinen, Florian Matthes

Knowledge models are fundamental to dialogue systems for enabling
conversational interactions, which require handling domain-specific knowledge.
Ensuring effective communication in information-providing conversations entails
aligning user understanding with the knowledge available to the system.
However, dialogue systems often face challenges arising from semantic
inconsistencies in how information is expressed in natural language compared to
how it is represented within the system's internal knowledge. To address this
problem, we study the potential of large language models for conversational
grounding, a mechanism to bridge information gaps by establishing shared
knowledge between dialogue participants. Our approach involves annotating human
conversations across five knowledge domains to create a new dialogue corpus
called BridgeKG. Through a series of experiments on this dataset, we
empirically evaluate the capabilities of large language models in classifying
grounding acts and identifying grounded information items within a knowledge
graph structure. Our findings offer insights into how these models use
in-context learning for conversational grounding tasks and common prediction
errors, which we illustrate with examples from challenging dialogues. We
discuss how the models handle knowledge graphs as a semantic layer between
unstructured dialogue utterances and structured information items.

ÊëòË¶ÅÔºöÁü•Ë≠òÊ®°ÂûãÊòØÂ∞çË©±Á≥ªÁµ±ÁöÑÂü∫Êú¨Ë¶ÅÁ¥†ÔºåÁî®ÊñºÂïüÁî®Â∞çË©±‰∫íÂãïÔºåÈÄôÈúÄË¶ÅËôïÁêÜÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠ò„ÄÇÁ¢∫‰øùÂú®Êèê‰æõË≥áË®äÁöÑÂ∞çË©±‰∏≠ÈÄ≤Ë°åÊúâÊïàÁöÑÊ∫ùÈÄöÔºåÈúÄË¶ÅÂ∞á‰ΩøÁî®ËÄÖÁöÑÁêÜËß£ËàáÁ≥ªÁµ±ÂèØÁî®ÁöÑÁü•Ë≠òÁµêÂêàËµ∑‰æÜ„ÄÇÁÑ∂ËÄåÔºåÂ∞çË©±Á≥ªÁµ±Á∂ìÂ∏∏Èù¢Ëá®Ë™ûÊÑè‰∏ç‰∏ÄËá¥ÁöÑÊåëÊà∞ÔºåÂú®Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠Ë°®ÈÅîË≥áË®äÁöÑÊñπÂºèËàáÂú®Á≥ªÁµ±ÂÖßÈÉ®Áü•Ë≠ò‰∏≠Ë°®Á§∫Ë≥áË®äÁöÑÊñπÂºè‰∏çÂêå„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁ†îÁ©∂Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Â∞çË©±Âü∫Á§é‰∏≠ÁöÑÊΩõÂäõÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÂú®Â∞çË©±ÂèÉËàáËÄÖ‰πãÈñìÂª∫Á´ãÂÖ±‰∫´Áü•Ë≠ò‰æÜÂΩåÂêàË≥áË®äÂ∑ÆË∑ùÁöÑÊ©üÂà∂„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Ë®ªËß£‰∫îÂÄãÁü•Ë≠òÈ†òÂüü‰∏≠ÁöÑ‰∫∫È°ûÂ∞çË©±Ôºå‰ª•Âª∫Á´ã‰∏ÄÂÄãÊñ∞ÁöÑÂ∞çË©±Ë™ûÊñôÂ∫´ÔºåÁ®±ÁÇ∫ BridgeKG„ÄÇÈÄèÈÅéÂ∞çÊ≠§Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∏ÄÁ≥ªÂàóÂØ¶È©óÔºåÊàëÂÄëÂØ¶Ë≠âË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÂàÜÈ°ûÂü∫Á§éË°åÁÇ∫ÂíåË≠òÂà•Áü•Ë≠òÂúñÁµêÊßã‰∏≠ÁöÑÂü∫Á§éË≥áË®äÈ†ÖÁõÆÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊèê‰æõ‰∫ÜÈóúÊñºÈÄô‰∫õÊ®°ÂûãÂ¶Ç‰Ωï‰ΩøÁî®ÊÉÖÂ¢ÉÂ≠∏Áøí‰æÜÈÄ≤Ë°åÂ∞çË©±Âü∫Á§é‰ªªÂãôÂíåÂ∏∏Ë¶ãÈ†êÊ∏¨ÈåØË™§ÁöÑË¶ãËß£ÔºåÊàëÂÄëÁî®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂ∞çË©±ÁØÑ‰æã‰æÜË™™Êòé„ÄÇÊàëÂÄëË®éË´ñÊ®°ÂûãÂ¶Ç‰ΩïÂ∞áÁü•Ë≠òÂúñÂΩ¢Ë¶ñÁÇ∫ÈùûÁµêÊßãÂåñÂ∞çË©±Ë™ûÂè•ÂíåÁµêÊßãÂåñË≥áË®äÈ†ÖÁõÆ‰πãÈñìÁöÑË™ûÊÑèÂ±§„ÄÇ

##### **Automatic Extraction of Relationships among Motivations, Emotions and Actions from Natural Language Texts**
2408.00966v1 by Fei Yang

We propose a new graph-based framework to reveal relationships among
motivations, emotions and actions explicitly given natural language texts. A
directed acyclic graph is designed to describe human's nature. Nurture beliefs
are incorporated to connect outside events and the human's nature graph. No
annotation resources are required due to the power of large language models.
Amazon Fine Foods Reviews dataset is used as corpus and food-related
motivations are focused. Totally 92,990 relationship graphs are generated, of
which 63% make logical sense. We make further analysis to investigate error
types for optimization direction in future research.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊû∂ÊßãÔºåÁî®ÊñºÊè≠Á§∫Âú®Ëá™ÁÑ∂Ë™ûË®ÄÊñáÊú¨‰∏≠ÊòéÁ¢∫Áµ¶Âá∫ÁöÑÂãïÊ©ü„ÄÅÊÉÖÁ∑íÂíåÂãï‰Ωú‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊúâÂêëÁÑ°Áí∞ÂúñË¢´Ë®≠Ë®àÁî®ÊñºÊèèËø∞‰∫∫È°ûÁöÑÊú¨ÊÄß„ÄÇÂüπÈ§ä‰ø°ÂøµË¢´Á¥çÂÖ•ÂÖ∂‰∏≠ÔºåÁî®ÊñºÈÄ£Êé•Â§ñÈÉ®‰∫ã‰ª∂Âíå‰∫∫È°ûÁöÑÊú¨ÊÄßÂúñ„ÄÇÁî±ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂº∑Â§ßÂäüËÉΩÔºå‰∏çÈúÄË¶ÅË®ªËß£Ë≥áÊ∫ê„ÄÇ‰∫ûÈ¶¨ÈÅúÁæéÈ£üË©ïË´ñÊï∏ÊìöÈõÜË¢´Áî®‰ΩúË™ûÊñôÂ∫´Ôºå‰∏¶‰∏îÈáçÈªûÈóúÊ≥®ËàáÈ£üÁâ©Áõ∏ÈóúÁöÑÂãïÊ©ü„ÄÇÁ∏ΩÂÖ±ÁîüÊàê‰∫Ü 92,990 ÂÄãÈóú‰øÇÂúñÔºåÂÖ∂‰∏≠ 63% ÂÖ∑ÊúâÈÇèËºØÊÑèÁæ©„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂàÜÊûê‰ª•Ë™øÊü•ÈåØË™§È°ûÂûãÔºå‰ª•‰æøÁÇ∫Êú™‰æÜÁöÑÁ†îÁ©∂Êèê‰æõÂÑ™ÂåñÊñπÂêë„ÄÇ

##### **DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks**
2408.00633v1 by Guillermo Villar-Rodr√≠guez, √Ålvaro Huertas-Garc√≠a, Alejandro Mart√≠n, Javier Huertas-Tato, David Camacho

Introduction: This article introduces DisTrack, a methodology and a tool
developed for tracking and analyzing misinformation within Online Social
Networks (OSNs). DisTrack is designed to combat the spread of misinformation
through a combination of Natural Language Processing (NLP) Social Network
Analysis (SNA) and graph visualization. The primary goal is to detect
misinformation, track its propagation, identify its sources, and assess the
influence of various actors within the network.
  Methods: DisTrack's architecture incorporates a variety of methodologies
including keyword search, semantic similarity assessments, and graph generation
techniques. These methods collectively facilitate the monitoring of
misinformation, the categorization of content based on alignment with known
false claims, and the visualization of dissemination cascades through detailed
graphs. The tool is tailored to capture and analyze the dynamic nature of
misinformation spread in digital environments.
  Results: The effectiveness of DisTrack is demonstrated through three case
studies focused on different themes: discredit/hate speech, anti-vaccine
misinformation, and false narratives about the Russia-Ukraine conflict. These
studies show DisTrack's capabilities in distinguishing posts that propagate
falsehoods from those that counteract them, and tracing the evolution of
misinformation from its inception.
  Conclusions: The research confirms that DisTrack is a valuable tool in the
field of misinformation analysis. It effectively distinguishes between
different types of misinformation and traces their development over time. By
providing a comprehensive approach to understanding and combating
misinformation in digital spaces, DisTrack proves to be an essential asset for
researchers and practitioners working to mitigate the impact of false
information in online social environments.

ÊëòË¶ÅÔºö<paragraph>ÂºïË®ÄÔºöÊú¨Êñá‰ªãÁ¥π DisTrackÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñπÊ≥ïÂíåÂ∑•ÂÖ∑ÔºåÁî®ÊñºËøΩËπ§ÂíåÂàÜÊûêÁ∑ö‰∏äÁ§æ‰∫§Á∂≤Ë∑ØÔºàOSNÔºâ‰∏≠ÁöÑÈåØË™§Ë≥áË®ä„ÄÇDisTrack ÁöÑË®≠Ë®àÁõÆÁöÑÊòØÈÄèÈÅéÁµêÂêàËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºàNLPÔºâ„ÄÅÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûêÔºàSNAÔºâÂíåÂúñÂΩ¢Ë¶ñË¶∫Âåñ‰æÜÂ∞çÊäóÈåØË™§Ë≥áË®äÁöÑÊï£Â∏É„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂÅµÊ∏¨ÈåØË™§Ë≥áË®ä„ÄÅËøΩËπ§ÂÖ∂ÂÇ≥Êí≠„ÄÅÊâæÂá∫ÂÖ∂‰æÜÊ∫êÔºå‰∏¶Ë©ï‰º∞Á∂≤Ë∑Ø‰∏≠ÂêÑÂÄãÂèÉËàáËÄÖÁöÑÂΩ±ÈüøÂäõ„ÄÇ
ÊñπÊ≥ïÔºöDisTrack ÁöÑÊû∂ÊßãÁµêÂêà‰∫ÜÂ§öÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÈóúÈçµÂ≠óÊêúÂ∞ã„ÄÅË™ûÊÑèÁõ∏‰ººÊÄßË©ï‰º∞ÂíåÂúñÂΩ¢Áî¢ÁîüÊäÄË°ì„ÄÇÈÄô‰∫õÊñπÊ≥ïÂÖ±Âêå‰øÉÈÄ≤‰∫ÜÈåØË™§Ë≥áË®äÁöÑÁõ£Êéß„ÄÅÂü∫ÊñºËàáÂ∑≤Áü•ËôõÂÅáË™™Ê≥ïÁöÑÊØîÂ∞ç‰æÜÂàÜÈ°ûÂÖßÂÆπÔºå‰ª•ÂèäÈÄèÈÅéË©≥Á¥∞ÂúñÂΩ¢Ë¶ñË¶∫ÂåñÂÇ≥Êí≠Â±§Áñä„ÄÇÊ≠§Â∑•ÂÖ∑Á∂ìÈÅéÈáèË∫´ÊâìÈÄ†ÔºåÁî®ÊñºÊì∑ÂèñÂíåÂàÜÊûêÊï∏‰ΩçÁí∞Â¢É‰∏≠ÈåØË™§Ë≥áË®äÊï£Â∏ÉÁöÑÂãïÊÖãÁâπÊÄß„ÄÇ
ÁµêÊûúÔºöDisTrack ÁöÑÊïàËÉΩÈÄèÈÅé‰∏âÂÄãÊ°à‰æãÁ†îÁ©∂Áç≤ÂæóÈ©óË≠âÔºåÈÄô‰∫õÁ†îÁ©∂Â∞àÊ≥®Êñº‰∏çÂêåÁöÑ‰∏ªÈ°åÔºöË≤∂‰Ωé/‰ªáÊÅ®Ë®ÄË´ñ„ÄÅÂèçÁñ´ËãóÈåØË™§Ë≥áË®äÔºå‰ª•ÂèäÈóúÊñº‰øÑÁæÖÊñØ-ÁÉèÂÖãËò≠Ë°ùÁ™ÅÁöÑËôõÂÅáÊïòËø∞„ÄÇÈÄô‰∫õÁ†îÁ©∂È°ØÁ§∫Âá∫ DisTrack Âú®ÂçÄÂàÜÂÇ≥Êí≠ËôõÂÅáË≥áË®äÂíåÂèçÂà∂ËôõÂÅáË≥áË®äÁöÑË≤ºÊñáÔºå‰ª•ÂèäËøΩËπ§ÈåØË™§Ë≥áË®äÂæûÂÖ∂ÈñãÁ´ØÊºîËÆäÁöÑÈÅéÁ®ã‰∏≠ÊâÄÂÖ∑ÂÇôÁöÑËÉΩÂäõ„ÄÇ
ÁµêË´ñÔºöÁ†îÁ©∂Ë≠âÂØ¶ DisTrack ÊòØÈåØË™§Ë≥áË®äÂàÜÊûêÈ†òÂüü‰∏≠‰∏ÄÂÄãÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇÂÆÉÊúâÊïàÂçÄÂàÜ‰∫Ü‰∏çÂêåÈ°ûÂûãÁöÑÈåØË™§Ë≥áË®äÔºå‰∏¶ËøΩËπ§ÂÖ∂Èö®ËëóÊôÇÈñìÊé®ÁßªÁöÑÁôºÂ±ï„ÄÇÈÄèÈÅéÊèê‰æõ‰∏ÄÁ®ÆÂÖ®Èù¢ÁöÑÊñπÊ≥ï‰æÜÁêÜËß£ÂíåÂ∞çÊäóÊï∏‰ΩçÁ©∫Èñì‰∏≠ÁöÑÈåØË™§Ë≥áË®äÔºåDisTrack Ë≠âÊòé‰∫ÜËá™Â∑±ÊòØÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÂíåÂØ¶ÂãôÂ∑•‰ΩúËÄÖÊ∏õËºïÁ∑ö‰∏äÁ§æ‰∫§Áí∞Â¢É‰∏≠ËôõÂÅáË≥áË®äÂΩ±ÈüøÂäõÁöÑÈáçË¶ÅË≥áÁî¢„ÄÇ</paragraph>

##### **On the Limitations and Prospects of Machine Unlearning for Generative AI**
2408.00376v1 by Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, Heng Chang

Generative AI (GenAI), which aims to synthesize realistic and diverse data
samples from latent variables or other data modalities, has achieved remarkable
results in various domains, such as natural language, images, audio, and
graphs. However, they also pose challenges and risks to data privacy, security,
and ethics. Machine unlearning is the process of removing or weakening the
influence of specific data samples or features from a trained model, without
affecting its performance on other data or tasks. While machine unlearning has
shown significant efficacy in traditional machine learning tasks, it is still
unclear if it could help GenAI become safer and aligned with human desire. To
this end, this position paper provides an in-depth discussion of the machine
unlearning approaches for GenAI. Firstly, we formulate the problem of machine
unlearning tasks on GenAI and introduce the background. Subsequently, we
systematically examine the limitations of machine unlearning on GenAI models by
focusing on the two representative branches: LLMs and image generative
(diffusion) models. Finally, we provide our prospects mainly from three
aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and
conscientiously advocate for the future development of this field.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI (GenAI) Êó®Âú®ÂæûÊΩõÂú®ËÆäÊï∏ÊàñÂÖ∂‰ªñË≥áÊñôÊ®°Âºè‰∏≠ÂêàÊàêÈÄºÁúü‰∏îÂ§öÊ®£ÂåñÁöÑË≥áÊñôÁØÑ‰æãÔºåÂ∑≤Âú®Ëá™ÁÑ∂Ë™ûË®Ä„ÄÅÂΩ±ÂÉè„ÄÅÈü≥Ë®äÂíåÂúñÂΩ¢Á≠âÂêÑÁ®ÆÈ†òÂüü‰∏≠ÂèñÂæóÈ°ØËëóÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰πüÂ∞çË≥áÊñôÈö±ÁßÅ„ÄÅÂÆâÂÖ®ÊÄßËàáÈÅìÂæ∑ÊßãÊàêÊåëÊà∞ÂíåÈ¢®Èö™„ÄÇÊ©üÂô®ÈÅ∫ÂøòÊòØÁßªÈô§ÊàñÊ∏õÂº±ÁâπÂÆöË≥áÊñôÁØÑ‰æãÊàñÁâπÂæµÂ∞çÂ∑≤Ë®ìÁ∑¥Ê®°ÂûãÁöÑÂΩ±ÈüøÔºåÂêåÊôÇ‰∏çÂΩ±ÈüøÂÖ∂Âú®ÂÖ∂‰ªñË≥áÊñôÊàñ‰ªªÂãô‰∏äÁöÑÊïàËÉΩ„ÄÇÈõñÁÑ∂Ê©üÂô®ÈÅ∫ÂøòÂ∑≤Âú®ÂÇ≥Áµ±Ê©üÂô®Â≠∏Áøí‰ªªÂãô‰∏≠Â±ïÁèæÈ°ØËëóÁöÑÂäüÊïàÔºå‰ΩÜ‰ªç‰∏çÊ∏ÖÊ•öÂÆÉÊòØÂê¶ËÉΩÂçîÂä© GenAI ËÆäÂæóÊõ¥ÂÆâÂÖ®‰∏îÁ¨¶Âêà‰∫∫È°ûÁöÑÊúüÊúõ„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á´ãÂ†¥Êñá‰ª∂Ê∑±ÂÖ•Êé¢Ë®é‰∫Ü GenAI ÁöÑÊ©üÂô®ÈÅ∫ÂøòÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂà∂ÂÆö GenAI ‰∏äÊ©üÂô®ÈÅ∫Âøò‰ªªÂãôÁöÑÂïèÈ°åÔºå‰∏¶‰ªãÁ¥πËÉåÊôØ„ÄÇÊé•ËëóÔºåÊàëÂÄëÊúâÁ≥ªÁµ±Âú∞Ê™¢Ë¶ñÊ©üÂô®ÈÅ∫ÂøòÂú® GenAI Ê®°Âûã‰∏äÁöÑÈôêÂà∂ÔºåÈáçÈªûÊîæÂú®ÂÖ©ÂÄã‰ª£Ë°®ÊÄßÁöÑÂàÜÊîØÔºöLLM ÂíåÂΩ±ÂÉèÁîüÊàêÔºàÊì¥Êï£ÔºâÊ®°Âûã„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰∏ªË¶ÅÂæûÂü∫Ê∫ñ„ÄÅË©ï‰º∞ÊåáÊ®ôÂíåÊïàÁî®ÈÅ∫ÂøòÊ¨äË°°‰∏âÂÄãÈù¢ÂêëÊèê‰æõÊàëÂÄëÁöÑÂ±ïÊúõÔºå‰∏¶ÂØ©ÊÖéÂÄ°Ë≠∞Ë©≤È†òÂüüÁöÑÊú™‰æÜÁôºÂ±ï„ÄÇ

##### **Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network**
2408.00290v1 by Bin Cheng, Jiaxuan Lu

With the advent of the era of foundation models, pre-training and fine-tuning
have become common paradigms. Recently, parameter-efficient fine-tuning has
garnered widespread attention due to its better balance between the number of
learnable parameters and performance. However, some current parameter-efficient
fine-tuning methods only model a single modality and lack the utilization of
structural knowledge in downstream tasks. To address this issue, this paper
proposes a multi-modal parameter-efficient fine-tuning method based on graph
networks. Each image is fed into a multi-modal large language model (MLLM) to
generate a text description. The image and its corresponding text description
are then processed by a frozen image encoder and text encoder to generate image
features and text features, respectively. A graph is constructed based on the
similarity of the multi-modal feature nodes, and knowledge and relationships
relevant to these features are extracted from each node. Additionally, Elastic
Weight Consolidation (EWC) regularization is incorporated into the loss
function to mitigate the problem of forgetting during task learning. The
proposed model achieves test accuracies on the OxfordPets, Flowers102, and
Food101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The
code is available at https://github.com/yunche0/GA-Net/tree/master.

ÊëòË¶ÅÔºöÈö®ËëóÂü∫Á§éÊ®°ÂûãÊôÇ‰ª£ÁöÑÂà∞‰æÜÔºåÈ†êË®ìÁ∑¥ÂíåÂæÆË™øÂ∑≤ÊàêÁÇ∫Â∏∏Ë¶ãÁöÑÁØÑ‰æã„ÄÇÊúÄËøëÔºåÁî±ÊñºÂèÉÊï∏ÊúâÊïàÂæÆË™øÂú®ÂèØÂ≠∏ÁøíÂèÉÊï∏Êï∏ÈáèÂíåÊïàËÉΩ‰πãÈñìÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°ÔºåÂõ†Ê≠§ÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºå‰∏Ä‰∫õÁõÆÂâçÁöÑÂèÉÊï∏ÊúâÊïàÂæÆË™øÊñπÊ≥ïÂÉÖÂª∫Ê®°ÂñÆ‰∏ÄÊ®°ÊÖãÔºå‰∏îÁº∫‰πèÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠Âà©Áî®ÁµêÊßãÁü•Ë≠ò„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂúñÂΩ¢Á∂≤Ë∑ØÁöÑÂ§öÊ®°ÊÖãÂèÉÊï∏ÊúâÊïàÂæÆË™øÊñπÊ≥ï„ÄÇÊØèÂÄãÂΩ±ÂÉèÈÉΩÊúÉËº∏ÂÖ•Âà∞Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰∏≠Ôºå‰ª•Áî¢ÁîüÊñáÂ≠óÊèèËø∞„ÄÇÁÑ∂ÂæåÔºåÂΩ±ÂÉèÂèäÂÖ∂Â∞çÊáâÁöÑÊñáÂ≠óÊèèËø∞ÊúÉÁî±ÂáçÁµêÁöÑÂΩ±ÂÉèÁ∑®Á¢ºÂô®ÂíåÊñáÂ≠óÁ∑®Á¢ºÂô®ËôïÁêÜÔºåÂàÜÂà•Áî¢ÁîüÂΩ±ÂÉèÁâπÂæµÂíåÊñáÂ≠óÁâπÂæµ„ÄÇÊ†πÊìöÂ§öÊ®°ÊÖãÁâπÂæµÁØÄÈªûÁöÑÁõ∏‰ººÊÄßÂª∫Êßã‰∏ÄÂÄãÂúñÂΩ¢Ôºå‰∏¶ÂæûÊØèÂÄãÁØÄÈªû‰∏≠ËêÉÂèñÂá∫ËàáÈÄô‰∫õÁâπÂæµÁõ∏ÈóúÁöÑÁü•Ë≠òÂíåÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÂΩàÊÄßÊ¨äÈáçÊï¥Âêà (EWC) Ê≠£ÂâáÂåñÊúÉÁ¥çÂÖ•ÊêçÂ§±ÂáΩÊï∏‰∏≠Ôºå‰ª•Ê∏õËºïÂú®‰ªªÂãôÂ≠∏ÁøíÊúüÈñìÈÅ∫ÂøòÁöÑÂïèÈ°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú® OxfordPets„ÄÅFlowers102 Âíå Food101 Ë≥áÊñôÈõÜ‰∏äÈÅîÊàêÁöÑÊ∏¨Ë©¶Ê∫ñÁ¢∫Â∫¶ÂàÜÂà•ÊèêÂçá‰∫Ü 4.45%„ÄÅ2.92% Âíå 0.23%„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/yunche0/GA-Net/tree/master ÂèñÂæó„ÄÇ

##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas N√ºrnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

ÊëòË¶ÅÔºöÊú¨‰ΩìÊòØÁâπÂÆöÈ†òÂüü‰∏≠Áü•Ë≠òÁöÑÂΩ¢ÂºèÂåñË°®Á§∫ÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµÑÁπîÂíåÁêÜËß£Ë§áÈõúÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊú¨‰ΩìÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îËÄóÊôÇÁöÑÂä™Âäõ„ÄÇChEBI ÊòØÂåñÂ≠∏È†òÂüü‰∏≠‰∏ÄÂÄãËëóÂêçÁöÑÊú¨‰ΩìÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË≥áÊ∫êÔºåÁî®ÊñºÂÆöÁæ©ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Â±¨ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÉÖÊ∂µËìã‰∫ÜÂåñÂ≠∏È†òÂüüÂø´ÈÄüÂ¢ûÈï∑ÁöÑÁü•Ë≠ò‰∏≠ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰∏¶‰∏îÊ≤íÊúâÊèê‰æõÁßëÂ≠∏ÊñáÁçªÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂÆÉÊ∂âÂèä‰ΩøÁî®‰æÜËá™ Chebi ÁöÑÁü•Ë≠òÊì¥ÂÖÖÁèæÊúâÁöÑË®ªÈáãÊñáÊú¨Ë™ûÊñôÂ∫´Ôºå‰∏¶ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Ë≠òÂà•ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Âú®ÁßëÂ≠∏ÊñáÊú¨‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÁµêÂêàÊú¨‰ΩìÁü•Ë≠òÂíå LLM ÁöÑË™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÊàëÂÄëÂú®Ë≠òÂà•ÁßëÂ≠∏ÊñáÁçª‰∏≠ÁöÑÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî®ÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû‰∏ÄÁµÑ 8,000 ÁØá ChemRxiv ÊñáÁ´†‰∏≠ÊèêÂèñÂÆÉÂÄëÔºå‰∏¶ÊáâÁî®Á¨¨‰∫åÂÄã LLM ‰æÜÂª∫Á´ã‰∏ÄÂÄãÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî® (CEAR) ÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂÆÉÊèê‰æõË£úÂÖÖ ChEBI ÁöÑË≥áË®äÔºå‰∏¶ÊúâÂä©ÊñºÊì¥ÂÖÖÂÆÉ„ÄÇ

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v3 by Xinyi Pan, Daniel Hern√°ndez, Philipp Seifer, Ralf L√§mmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥ÔºåÊàëÂÄëË¶ãË≠â‰∫ÜÂ§ßÂûãÁü•Ë≠òÂúñË≠úÁöÑÂá∫ÁèæÔºåÁµêÂêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑË≥áË®ä„ÄÇÊúâÊôÇÔºåÈÄô‰∫õË≥áË®äÊúÉ‰ª•Â∞çÂÖ∂‰ªñÊñ∑Ë®ÄÁöÑÊñ∑Ë®ÄÂΩ¢ÂºèÊèê‰æõÔºåÂÆöÁæ©Êñ∑Ë®ÄÊúâÊïàÁöÑËÑàÁµ°„ÄÇÊúÄËøëÂ∞ç RDF ÁöÑÊì¥ÂÖÖÔºåÂÖÅË®±Â∞çÈô≥Ëø∞ÈÄ≤Ë°åÈô≥Ëø∞ÔºåÁ®±ÁÇ∫ RDF-starÔºåÊ≠£Âú®‰øÆË®ÇÁÇ∫ W3C Ê®ôÊ∫ñ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÊ≤íÊúâÈáùÂ∞çÈÄô‰∫õ RDF-star Èô≥Ëø∞ÁöÑË™ûÊÑèÂª∫Ë≠∞Ôºå‰πüÊ≤íÊúâÂÖßÂª∫ÁöÑÈÅã‰ΩúÂäüËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂõõÂÄºÈÇèËºØÁöÑÁü•Ë≠ò RDF-star ÂÖÉË≥áÊñôÊü•Ë©¢Ë™ûË®ÄÔºåÁ®±ÁÇ∫ eSPARQL„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÊì¥ÂÖÖ‰∫Ü RDF-star ÁöÑÊü•Ë©¢Ë™ûË®Ä SPARQL-starÔºåÊñ∞Â¢û‰∏ÄÁ®Æ FROM Â≠êÂè•È°ûÂûãÔºå‰ª•Âà©Êñº‰ΩøÁî®Â§öÈáç‰∏îÊúâÊôÇÁõ∏‰∫íË°ùÁ™ÅÁöÑ‰ø°ÂøµÈÄ≤Ë°åÈÅã‰Ωú„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÂèØ‰ª•Ë°®ÈÅîÂõõÁ®Æ‰ΩøÁî®Ê°à‰æãÊü•Ë©¢ÔºåÂåÖÊã¨‰ª•‰∏ãÂäüËÉΩÔºö(i) Êü•Ë©¢ÂÄã‰∫∫ÁöÑ‰ø°ÂøµÔºå(ii) ÂΩôÁ∏Ω‰ø°ÂøµÔºå(iii) Êü•Ë©¢ËàáÊüê‰∫∫Ë°ùÁ™ÅÁöÑÊòØË™∞Ôºå‰ª•Âèä (iv) ÈóúÊñº‰ø°ÂøµÁöÑ‰ø°ÂøµÔºàÂç≥‰ø°ÂøµÁöÑÂ∑¢ÁãÄÔºâ„ÄÇ

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

ÊëòË¶ÅÔºöÁé∞ÂÆû‰∏ñÁïåÁöÑÂØºËà™ÈÄöÂ∏∏Ê∂âÂèäÂ§ÑÁêÜÊÑèÂ§ñÁöÑÈöúÁ¢çÔºå‰æãÂ¶ÇÂÖ≥ÁùÄÁöÑÈó®„ÄÅÁßªÂä®ÁöÑÁâ©‰ΩìÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÂÆû‰Ωì„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑËßÜËßâÂíåËØ≠Ë®ÄÂØºËà™ (VLN) ‰ªªÂä°ÈÄöÂ∏∏ÂÅáËÆæÊåá‰ª§‰∏éÂõ∫ÂÆöÁöÑÂíåÈ¢ÑÂÆö‰πâÁöÑÂØºËà™ÂõæÂÆåÂÖ®‰∏ÄËá¥ÔºåÊ≤°Êúâ‰ªª‰ΩïÈöúÁ¢ç„ÄÇËøôÁßçÂÅáËÆæÂøΩÁï•‰∫ÜÂÆûÈôÖÂØºËà™ÂõæÂíåÁªôÂÆöÊåá‰ª§‰∏≠ÊΩúÂú®ÁöÑÂ∑ÆÂºÇÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂÆ§ÂÜÖÂíåÂÆ§Â§ñ‰ª£ÁêÜÂá∫Áé∞ÈáçÂ§ßÊïÖÈöú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÈÄöËøá‰øÆÊîπÂØºËà™ÂõæÂíåËßÜËßâËßÇÂØüÔºåÂ∞ÜÂêÑÁßçÈöúÁ¢çÊï¥ÂêàÂà∞ R2R Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂºïÂÖ•‰∫ÜÂàõÊñ∞Êï∞ÊçÆÈõÜÂíå‰ªªÂä°ÔºåÂç≥Â∏¶ÊúâÊÑèÂ§ñÈöúÁ¢çÁöÑ R2R (R2R-UNO)„ÄÇR2R-UNO ÂåÖÂê´ÂêÑÁßçÁ±ªÂûãÂíåÊï∞ÈáèÁöÑË∑ØÂæÑÈöúÁ¢çÔºå‰ª•ÁîüÊàê VLN Á†îÁ©∂ÁöÑÊåá‰ª§-Áé∞ÂÆû‰∏çÂåπÈÖç„ÄÇÂú® R2R-UNO ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑ VLN ÊñπÊ≥ïÂú®Èù¢ÂØπÊ≠§Á±ª‰∏çÂåπÈÖçÊó∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàòÔºåËøôË°®ÊòéÂÆÉ‰ª¨‰∏•Ê†ºÈÅµÂæ™Êåá‰ª§ÔºåËÄå‰∏çÊòØËá™ÈÄÇÂ∫îÂú∞ÂØºËà™„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ObVLNÔºàÂèóÈòª VLNÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨ËØæÁ®ãËÆ≠ÁªÉÁ≠ñÁï•ÂíåËôöÊãüÂõæÊûÑÂª∫Ôºå‰ª•Â∏ÆÂä©‰ª£ÁêÜÊúâÊïàÂú∞ÈÄÇÂ∫îÂèóÈòªÁéØÂ¢É„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåObVLN ‰∏ç‰ªÖÂú®Êó†ÈöúÁ¢çÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÂú®ÊÑèÂ§ñÈöúÁ¢ç‰∏≠‰πüËé∑Âæó‰∫ÜÂÆûË¥®ÊÄßÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÂèØÈù†„ÄÅÁµêÊßãÂåñ„ÄÅÁâπÂÆöÊñºÈ†òÂüü‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•Ë≠òÔºå‰æÜË£úÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåKG Âíå LLM ÈÄöÂ∏∏ÊòØÂàÜÈñãÈñãÁôºÔºå‰∏¶‰∏îÂøÖÈ†àÂú®Ë®ìÁ∑¥ÂæåÊï¥Âêà„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü Tree-of-TraversalsÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Êé®ÁêÜÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩËÆìÈªëÁõí LLM ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄã KG„ÄÇË©≤ÊºîÁÆóÊ≥ïÁÇ∫ LLM Êèê‰æõËàá KG ‰ªãÈù¢ÁöÑÂãï‰ΩúÔºå‰∏¶ËÆì LLM ËÉΩÂú®ÂèØËÉΩÁöÑÊÄùËÄÉÂíåÂãï‰Ωú‰∏äÂü∑Ë°åÊ®πÁãÄÊêúÂ∞ãÔºå‰ª•ÊâæÂá∫È´òÂ∫¶‰ø°ÂøÉÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁÜ±ÈñÄÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåTree-of-Traversals Â§ßÂπÖÊèêÂçá‰∫ÜÂïèÈ°åËß£Á≠îÂíå KG ÂïèÈ°åËß£Á≠î‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/amazon-science/tree-of-traversals} ÂèñÂæó

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÂèØËÉΩ‰ΩøË®±Â§öÈ†òÂüüÂèóÁõä„ÄÇÁ´ØÂà∞Á´ØËá™ÂãïÈßïÈßõ (e2eAD) ÊòØÂÖ∏ÂûãÈ†òÂüü‰πã‰∏ÄÔºåÂõ†ÁÇ∫ LLM ÊîØÊè¥Ë∂ä‰æÜË∂äÂ§öÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§Èù¢Ëá®Êñ∞ÁöÑÊ©üÊúÉ„ÄÇÂú®Ê≠§ÔºåÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SimpleLLM4AD ÁöÑ e2eAD ÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠Ôºåe2eAD ‰ªªÂãôÂàÜÁÇ∫ÂõõÂÄãÈöéÊÆµÔºåÂàÜÂà•ÊòØÊÑüÁü•„ÄÅÈ†êÊ∏¨„ÄÅË¶èÂäÉÂíåË°åÁÇ∫„ÄÇÊØèÂÄãÈöéÊÆµÂåÖÂê´Â§öÂÄãË¶ñË¶∫ÂïèÁ≠î (VQA) ÈÖçÂ∞çÔºå‰∏î VQA ÈÖçÂ∞çÁõ∏‰∫íÈÄ£Êé•ÔºåÊßãÂª∫‰∏ÄÂÄãÁ®±ÁÇ∫ÂúñÂΩ¢ VQA (GVQA) ÁöÑÂúñÂΩ¢„ÄÇÈÄèÈÅé VLM ÂàÜÈöéÊÆµÊé®ÁêÜ GVQA ‰∏≠ÁöÑÊØèÂÄã VQA ÈÖçÂ∞çÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÂØ¶ÁèæÁ´ØÂà∞Á´ØÈßïÈßõ„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠ÔºåÊé°Áî®Ë¶ñË¶∫Transformer (ViT) Ê®°Âûã‰æÜËôïÁêÜ nuScenes Ë¶ñË¶∫Ë≥áÊñôÔºåÂêåÊôÇÂà©Áî® VLM ‰æÜË©ÆÈáãÂíåÊé®ÁêÜÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠ÊèêÂèñÁöÑË≥áË®ä„ÄÇÂú®ÊÑüÁü•ÈöéÊÆµÔºåÁ≥ªÁµ±Ë≠òÂà•ÂíåÂàÜÈ°ûÈßïÈßõÁí∞Â¢É‰∏≠ÁöÑÁâ©‰ª∂„ÄÇÈ†êÊ∏¨ÈöéÊÆµÊ∂âÂèäÈ†êÊ∏¨ÈÄô‰∫õÁâ©‰ª∂ÁöÑÊΩõÂú®ÁßªÂãï„ÄÇË¶èÂäÉÈöéÊÆµÂà©Áî®Êî∂ÈõÜÁöÑË≥áË®ä‰æÜÂà∂ÂÆöÈßïÈßõÁ≠ñÁï•ÔºåÁ¢∫‰øùËá™ÂãïÈßïÈßõÊ±ΩËªäÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéá„ÄÇÊúÄÂæåÔºåË°åÁÇ∫ÈöéÊÆµÂ∞áË¶èÂäÉÁöÑÂãï‰ΩúËΩâÊèõÁÇ∫ËªäËºõÂèØÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåSimpleLLM4AD Âú®Ë§áÈõúÁöÑÈßïÈßõÂ†¥ÊôØ‰∏≠ÂØ¶Áèæ‰∫ÜÁ´∂Áà≠Âäõ„ÄÇ

##### **Be aware of overfitting by hyperparameter optimization!**
2407.20786v1 by Igor V. Tetko, Ruud van Deursen, Guillaume Godin

Hyperparameter optimization is very frequently employed in machine learning.
However, an optimization of a large space of parameters could result in
overfitting of models. In recent studies on solubility prediction the authors
collected seven thermodynamic and kinetic solubility datasets from different
data sources. They used state-of-the-art graph-based methods and compared
models developed for each dataset using different data cleaning protocols and
hyperparameter optimization. In our study we showed that hyperparameter
optimization did not always result in better models, possibly due to
overfitting when using the same statistical measures. Similar results could be
calculated using pre-set hyperparameters, reducing the computational effort by
around 10,000 times. We also extended the previous analysis by adding a
representation learning method based on Natural Language Processing of smiles
called Transformer CNN. We show that across all analyzed sets using exactly the
same protocol, Transformer CNN provided better results than graph-based methods
for 26 out of 28 pairwise comparisons by using only a tiny fraction of time as
compared to other methods. Last but not least we stressed the importance of
comparing calculation results using exactly the same statistical measures.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí‰∏≠ÈùûÂ∏∏È†ªÁπÅÂú∞‰ΩøÁî®Ë∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ„ÄÇ
ÁÑ∂ËÄåÔºåÂ∞çÂ§ßÂèÉÊï∏Á©∫ÈñìÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÂèØËÉΩÊúÉÂ∞éËá¥Ê®°ÂûãÈÅéÊì¨Âêà„ÄÇÂú®ÊúÄËøëÂ∞çÊ∫∂Ëß£Â∫¶È†êÊ∏¨ÁöÑÁ†îÁ©∂‰∏≠Ôºå‰ΩúËÄÖÂæû‰∏çÂêåÁöÑÊï∏ÊìöÊ∫êÊî∂ÈõÜ‰∫Ü‰∏ÉÂÄãÁÜ±ÂäõÂ≠∏ÂíåÂãïÂäõÂ≠∏Ê∫∂Ëß£Â∫¶Êï∏ÊìöÈõÜ„ÄÇ‰ªñÂÄë‰ΩøÁî®‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÔºå‰∏¶ÊØîËºÉ‰∫Ü‰ΩøÁî®‰∏çÂêåÁöÑÊï∏ÊìöÊ∏ÖÊ¥óÂçîË≠∞ÂíåË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥ÂåñÁÇ∫ÊØèÂÄãÊï∏ÊìöÈõÜÈñãÁôºÁöÑÊ®°Âûã„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëË°®ÊòéË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ‰∏¶ÈùûÁ∏ΩÊòØÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÊ®°ÂûãÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÂú®‰ΩøÁî®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨ÈáèÊôÇÁôºÁîüÈÅéÊì¨Âêà„ÄÇÂèØ‰ª•‰ΩøÁî®È†êË®≠ÁöÑË∂ÖÂèÉÊï∏Ë®àÁÆóÈ°û‰ººÁöÑÁµêÊûúÔºåÂæûËÄåÂ∞áË®àÁÆóÂ∑•‰ΩúÈáèÊ∏õÂ∞ëÁ¥Ñ 10,000 ÂÄç„ÄÇÊàëÂÄëÈÇÑÈÄöÈÅéÊ∑ªÂä†Âü∫ÊñºÁ¨ëÂÆπÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑË°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºàÁ®±ÁÇ∫ Transformer CNNÔºâ‰æÜÊì¥Â±ïÂÖàÂâçÁöÑÂàÜÊûê„ÄÇÊàëÂÄëË°®ÊòéÔºåÂú®‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂçîË≠∞Â∞çÊâÄÊúâÂàÜÊûêÁöÑÈõÜÂêàÈÄ≤Ë°åÂàÜÊûêÊôÇÔºåTransformer CNN Âú® 28 ÂÄãÊàêÂ∞çÊØîËºÉ‰∏≠Êúâ 26 ÂÄãÊØîËºÉÊØîÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÁµêÊûúÔºåËÄåËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÁî®ÁöÑÊôÇÈñìÂè™ÊòØÂæàÂ∞èÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂº∑Ë™ø‰∫Ü‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨Èáè‰æÜÊØîËºÉË®àÁÆóÁµêÊûúÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Harvesting Textual and Structured Data from the HAL Publication Repository**
2407.20595v1 by Francis Kulumba, Wissam Antoun, Guillaume Vimont, Laurent Romary

HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.

ÊëòË¶ÅÔºöHALÔºàÁ∑ö‰∏äË∂ÖÈÄ£ÁµêÊñáÁ´†ÔºâÊòØÊ≥ïÂúãÂúãÂÆ∂Âá∫ÁâàÁâ©Ë≥áÊñôÂ∫´Ôºå
Â§ßÂ§öÊï∏È´òÁ≠âÊïôËÇ≤ÂíåÁ†îÁ©∂ÁµÑÁπîÈÉΩ‰ΩøÁî®ÂÆÉ‰æÜÂà∂ÂÆöÈñãÊîæÁßëÂ≠∏
ÊîøÁ≠ñ„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÊï∏‰ΩçÂúñÊõ∏È§®ÔºåÂÆÉÊòØ‰∏ÄÂÄãË±êÂØåÁöÑÂ≠∏Ë°ìÊñá‰ª∂Ë≥áÊñôÂ∫´Ôºå
‰ΩÜÂÆÉÂú®ÈÄ≤ÈöéÁ†îÁ©∂ÁöÑÊΩõÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®„ÄÇÊàëÂÄëÊèêÂá∫
HALvestÔºå‰∏ÄÂÄãÁç®ÁâπÁöÑË≥áÊñôÈõÜÔºåÂÆÉÂΩåË£ú‰∫ÜÂºïÊñáÁ∂≤Ë∑ØÂíå
Âú® HAL ‰∏äÊèê‰∫§ÁöÑË´ñÊñáÂÖ®Êñá‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÈÄèÈÅéÁØ©ÈÅ∏ HAL
‰∏≠ÁöÑÂ≠∏Ë°ìÂá∫ÁâàÂìÅ‰æÜÂª∫Á´ãÊàëÂÄëÁöÑË≥áÊñôÈõÜÔºåÊúÄÂæåÂæóÂà∞Á¥Ñ 70 Ëê¨‰ªΩÊñá‰ª∂Ôºå
Ê∂µËìã 13 ÂÄãÂ∑≤Ë≠òÂà•È†òÂüüÁöÑ 34 Á®ÆË™ûË®ÄÔºåÈÅ©ÂêàË™ûË®ÄÊ®°Âûã
Ë®ìÁ∑¥Ôºå‰∏¶Áî¢ÁîüÁ¥Ñ 165 ÂÑÑÂÄãË©ûÂΩôÔºàÂÖ∂‰∏≠Ê≥ïÊñáÊúâ 80 ÂÑÑÂÄãÔºå
Ëã±ÊñáÊúâ 70 ÂÑÑÂÄãÔºåÊòØÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑË™ûË®ÄÔºâ„ÄÇÊàëÂÄëÂ∞á
ÊØèÁØáË´ñÊñáÁöÑÂÖÉË≥áÊñôËΩâÊèõÊàêÂºïÊñáÁ∂≤Ë∑ØÔºåÁî¢Áîü‰∏ÄÂÄãÊúâÂêë
Áï∞Ë≥™ÂúñÂΩ¢„ÄÇÊ≠§ÂúñÂΩ¢ÂåÖÂê´Âú® HAL ‰∏äÂîØ‰∏ÄË≠òÂà•ÁöÑ‰ΩúËÄÖÔºå‰ª•Âèä
ÊâÄÊúâÂÖ¨ÈñãÊèê‰∫§ÁöÑË´ñÊñáÂèäÂÖ∂ÂºïÊñá„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãÂü∫Ê∫ñ
‰ΩøÁî®Ë≥áÊñôÈõÜÈÄ≤Ë°å‰ΩúËÄÖÊ≠∏Â±¨ÔºåÂØ¶‰Ωú‰∏ÄÁ≥ªÂàó
ÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Ë°®Á§∫Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÈÄ£ÁµêÈ†êÊ∏¨Ôºå
‰∏¶Ë®éË´ñÊàëÂÄëÁî¢ÁîüÁöÑÁü•Ë≠òÂúñÂΩ¢ÁµêÊßãÁöÑÂØ¶Áî®ÊÄß„ÄÇ

##### **CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**
2407.20564v1 by Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song

While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÂèØÈÄèÈÅéÂæûÂª£Ê≥õÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏≠Áç≤ÂèñË±êÂØåÁöÑ‰∫ãÂØ¶Áü•Ë≠òÔºåÂü∑Ë°åÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÁ∂úÂêàÈÅãÁî®‰∏¶‰ª•Ë§áÈõúÁöÑÊñπÂºèÈÅãÁî®Ê≠§Áü•Ë≠òÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜÁöÑËÉΩÂäõ‰ªçÊúâÂæÖÈÄ≤‰∏ÄÊ≠•Êé¢Ë®é„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅé‰∏ÄÂÄãËá™ÂãïÁîüÊàêÁöÑ‰∏ÄËà¨È†òÂüüÂíåÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂúñË°®Ë§áÈõúÊé®ÁêÜÂïèÈ°åÁöÑÊñ∞Âü∫Ê∫ñÔºåÂ∞çÊúÄÂÖàÈÄ≤ÁöÑ LLM Ë§áÈõúÈÇèËºØÊé®ÁêÜËÉΩÂäõÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óÊé°Áî®Â§öÊ®£ÂåñÁöÑÊÉÖÂ¢ÉÂ≠∏ÁøíÊäÄË°ìÔºåÊè≠Á§∫Âá∫ LLM ÊìÖÈï∑Â∞ç‰∏ÄËà¨‰∏ñÁïåÁü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜÔºå‰ΩÜÂú®ËôïÁêÜÁâπÂÆöÈ†òÂüüÁöÑÂ∞àÊ•≠Áü•Ë≠òÊôÇÂâáÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ΩøÁî®ÊòéÁ¢∫ÁöÑÊÄùËÄÉÈèàÊ¢ùÁ§∫ÁØÑÈÄ≤Ë°åÊèêÁ§∫ÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM Âú®ÂÖ∑ÊúâÂ§öÊ®£ÂåñÈÇèËºØÈÅãÁÆóÁöÑË§áÈõúÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÊúâË∂£ÁöÑÊòØÔºåÊàëÂÄëÁöÑÂèóÊéßË©ï‰º∞Êè≠Èú≤‰∫Ü‰∏ÄÂÄã‰∏çÂ∞çÁ®±ÊÄßÔºåÂÖ∂‰∏≠ LLM Â±ïÁèæÂá∫Âú®ÈõÜÂêàËÅØÈõÜÈÅãÁÆóÊñπÈù¢ÁöÑÁÜüÁ∑¥Â∫¶Ôºå‰ΩÜÂú®ÈõÜÂêà‰∫§ÈõÜÊñπÈù¢ÂçªÈ°ØÂæóÁõ∏Áï∂ÂêÉÂäõÔºåËÄåÈõÜÂêà‰∫§ÈõÜÊ≠£ÊòØÈÇèËºØÊé®ÁêÜÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂæåÁ∫åÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞áÂÖ¨ÈñãÁôºÂ∏ÉÊàëÂÄëÁöÑË©ï‰º∞Âü∫Ê∫ñÂíåÁ®ãÂºèÁ¢º„ÄÇ

##### **Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**
2407.20513v1 by Hossein Rajaby Faghihi, Aliakbar Nafar, Andrzej Uszok, Hamid Karimian, Parisa Kordjamshidi

This paper presents a conversational pipeline for crafting domain knowledge
for complex neuro-symbolic models through natural language prompts. It
leverages large language models to generate declarative programs in the
DomiKnowS framework. The programs in this framework express concepts and their
relationships as a graph in addition to logical constraints between them. The
graph, later, can be connected to trainable neural models according to those
specifications. Our proposed pipeline utilizes techniques like dynamic
in-context demonstration retrieval, model refinement based on feedback from a
symbolic parser, visualization, and user interaction to generate the tasks'
structure and formal knowledge representation. This approach empowers domain
experts, even those not well-versed in ML/AI, to formally declare their
knowledge to be incorporated in customized neural models in the DomiKnowS
framework.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çË©±ÂºèÁÆ°ÈÅìÔºåÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊèêÁ§∫ÔºåÁÇ∫Ë§áÈõúÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊ®°ÂûãÂª∫Á´ãÈ†òÂüüÁü•Ë≠ò„ÄÇÂÆÉÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú® DomiKnowS Ê°ÜÊû∂‰∏≠Áî¢ÁîüÂÆ£ÂëäÂºèÁ®ãÂºè„ÄÇÊ≠§Ê°ÜÊû∂‰∏≠ÁöÑÁ®ãÂºèÊúÉÂ∞áÊ¶ÇÂøµÂèäÂÖ∂Èóú‰øÇË°®Á§∫ÁÇ∫ÂúñÂΩ¢Ôºå‰∏¶Âú®ÂÆÉÂÄë‰πãÈñìÂä†‰∏äÈÇèËºØÁ¥ÑÊùü„ÄÇ‰πãÂæåÔºåÂèØ‰ª•Ê†πÊìöÈÄô‰∫õË¶èÊ†ºÂ∞áÂúñÂΩ¢ÈÄ£Êé•Âà∞ÂèØË®ìÁ∑¥ÁöÑÁ•ûÁ∂ìÊ®°Âûã„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÁÆ°ÈÅìÂà©Áî®ÂãïÊÖãÊÉÖÂ¢É‰∏≠Á§∫ÁØÑÊ™¢Á¥¢„ÄÅÂü∫ÊñºÁ¨¶ËôüËß£ÊûêÂô®ÂõûÈ•ãÁöÑÊ®°ÂûãÁ≤æÁÖâ„ÄÅË¶ñË¶∫ÂåñÂíå‰ΩøÁî®ËÄÖ‰∫íÂãïÁ≠âÊäÄË°ìÔºå‰ª•Áî¢Áîü‰ªªÂãôÁµêÊßãÂíåÂΩ¢ÂºèÁü•Ë≠òË°®Á§∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÆìÈ†òÂüüÂ∞àÂÆ∂ÔºåÂç≥‰ΩøÊòØ‰∏çÁÜüÊÇâÊ©üÂô®Â≠∏ÁøíÔºè‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∫∫Ôºå‰πüËÉΩÊ≠£ÂºèÂÆ£Âëä‰ªñÂÄëÁöÑÁü•Ë≠òÔºå‰∏¶Â∞áÂÖ∂Á¥çÂÖ• DomiKnowS Ê°ÜÊû∂‰∏≠ÁöÑËá™Ë®ÇÁ•ûÁ∂ìÊ®°Âûã„ÄÇ

##### **What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**
2407.20382v1 by Navapat Nananukul, Wichayaporn Wongkamjan

Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.

ÊëòË¶ÅÔºöËßíËâ≤ÊâÆÊºîÈÅäÊà≤ (RPG) ÁÇ∫Áé©ÂÆ∂Êèê‰æõ‰∏ÄÂÄãË±êÂØå‰∏î‰∫íÂãïÁöÑ‰∏ñÁïå‰æõÂÖ∂Êé¢Á¥¢„ÄÇÂ∞çË©±‰ΩúÁÇ∫ÈñãÁôºËÄÖËàáÁé©ÂÆ∂‰πãÈñìÁöÑ‰∏ªË¶ÅÊ∫ùÈÄöÊñπÂºèÔºå‰ª•ÊåáÂçó„ÄÅNPC ‰∫íÂãïÂíåË™™ÊïÖ‰∫ãÁ≠âÂêÑÁ®ÆÂΩ¢ÂºèÂëàÁèæ„ÄÇÈõñÁÑ∂Â§ßÂ§öÊï∏ÈÅäÊà≤‰æùË≥¥ÊñºÊõ∏Èù¢ËÖ≥Êú¨‰æÜÂÆöÁæ©‰∏ªÁ∑öÊïÖ‰∫ãÂíåËßíËâ≤ÂÄãÊÄßÔºå‰ΩÜÈÄèÈÅéËßíËâ≤‰πãÈñìÁöÑÈñíËÅä‰∫íÂãïÔºåÂèØ‰ª•Â§ßÂπÖÊèêÂçáÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ∞çË©±Â°´ÂÖÖÊ°ÜÊû∂ÔºåÂà©Áî®Áî±Áü•Ë≠òÂúñË≠úÂ¢ûÂº∑ÁöÑ LLM ‰æÜÁî¢ÁîüÂãïÊÖã‰∏îÁ¨¶ÂêàÊÉÖÂ¢ÉÁöÑÂ∞çË©±‰∫íÂãï„ÄÇÊàëÂÄëÂú® Final Fantasy VII Remake ÂíåÂØ∂ÂèØÂ§¢ÁöÑÁí∞Â¢É‰∏≠Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊ°ÜÊû∂ÔºåÊèê‰æõ‰∫ÜÂÆöÊÄßÂíåÂÆöÈáèÁöÑË≠âÊìöÔºåË≠âÊòé‰∫Ü GPT-4 ÂÖ∑ÂÇô‰ª•ÂÆöÁæ©Â•ΩÁöÑÂÄãÊÄßË°åÂãï‰∏¶Áî¢ÁîüÂ∞çË©±ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªçÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºå‰æãÂ¶Ç GPT-4 ÈÅéÊñºÊ≠£Èù¢ÔºåÊàñËÄÖËºÉÁÇ∫Á¥∞ÂæÆÁöÑÂÄãÊÄßÔºå‰æãÂ¶ÇÊàêÁÜüÂ∫¶ÔºåÂæÄÂæÄÂìÅË≥™‰ΩéÊñºËºÉÊòéÈ°ØÁöÑÁâπË≥™Ôºå‰æãÂ¶ÇËÜΩÊÄØ„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÂçîÂä©ÈñãÁôºËÄÖÊâìÈÄ†Êõ¥Á¥∞Á∑ªÁöÑÂ°´ÂÖÖÂ∞çË©±ÔºåÂæûËÄåË±êÂØåÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü‰∏¶ÊèêÂçáÊï¥È´î RPG È´îÈ©ó„ÄÇ

##### **MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**
2407.20183v1 by Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao

Information seeking and integration is a complex cognitive task that consumes
enormous time and effort. Inspired by the remarkable progress of Large Language
Models, recent works attempt to solve this task by combining LLMs and search
engines. However, these methods still obtain unsatisfying performance due to
three challenges: (1) complex requests often cannot be accurately and
completely retrieved by the search engine once (2) corresponding information to
be integrated is spread over multiple web pages along with massive noise, and
(3) a large number of web pages with long contents may quickly exceed the
maximum context length of LLMs. Inspired by the cognitive process when humans
solve these problems, we introduce MindSearch to mimic the human minds in web
information seeking and integration, which can be instantiated by a simple yet
effective LLM-based multi-agent framework. The WebPlanner models the human mind
of multi-step information seeking as a dynamic graph construction process: it
decomposes the user query into atomic sub-questions as nodes in the graph and
progressively extends the graph based on the search result from WebSearcher.
Tasked with each sub-question, WebSearcher performs hierarchical information
retrieval with search engines and collects valuable information for WebPlanner.
The multi-agent design of MindSearch enables the whole framework to seek and
integrate information parallelly from larger-scale (e.g., more than 300) web
pages in 3 minutes, which is worth 3 hours of human effort. MindSearch
demonstrates significant improvement in the response quality in terms of depth
and breadth, on both close-set and open-set QA problems. Besides, responses
from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web
and Perplexity.ai applications, which implies that MindSearch can already
deliver a competitive solution to the proprietary AI search engine.

ÊëòË¶ÅÔºöË≥áË®äÊêúÂ∞ãËàáÊï¥ÂêàÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑË™çÁü•‰ªªÂãôÔºåÊúÉËÄóË≤ªÂ§ßÈáèÊôÇÈñìËàáÁ≤æÂäõ„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈ°ØËëóÈÄ≤Â±ïÁöÑÂïüÁôº‰∏ãÔºåËøëÊúüÁ†îÁ©∂ÂòóË©¶ÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËàáÊêúÂ∞ãÂºïÊìé‰æÜËß£Ê±∫Ê≠§‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ï‰ªçÂõ†‰∏âÈ†ÖÊåëÊà∞ËÄåÁÑ°Ê≥ïÁç≤Âæó‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩÔºö(1) Ë§áÈõúÁöÑÊü•Ë©¢ÈÄöÂ∏∏ÁÑ°Ê≥ïÁî±ÊêúÂ∞ãÂºïÊìé‰∏ÄÊ¨°Ê∫ñÁ¢∫‰∏îÂÆåÊï¥Âú∞Êì∑ÂèñÔºå(2) Ë¶ÅÊï¥ÂêàÁöÑÂ∞çÊáâË≥áË®äÊï£Â∏ÉÂú®Â§öÂÄãÁ∂≤È†Å‰∏≠‰∏î‰º¥Èö®ËëóÂ§ßÈáèÈõúË®äÔºå‰ª•Âèä (3) Â§ßÈáèÂÖßÂÆπÈÅéÈï∑ÁöÑÁ∂≤È†ÅÂèØËÉΩÊúÉÂø´ÈÄüË∂ÖÈÅéÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊúÄÂ§ßËÑàÁµ°Èï∑Â∫¶„ÄÇÂú®‰∫∫È°ûËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÁöÑË™çÁü•ÈÅéÁ®ã‰∏≠Áç≤ÂæóÈùàÊÑüÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MindSearch ‰æÜÊ®°Êì¨‰∫∫È°ûÂøÉÊô∫Âú®Á∂≤È†ÅË≥áË®äÊêúÂ∞ãËàáÊï¥Âêà‰∏≠ÁöÑË°åÁÇ∫ÔºåÈÄôÂèØ‰ª•Áî®‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂ§ö‰ª£ÁêÜÊû∂Êßã‰æÜÂØ¶‰æãÂåñ„ÄÇWebPlanner ‰ª•ÂãïÊÖãÂúñÂΩ¢Âª∫ÊßãÈÅéÁ®ã‰æÜÂª∫Ê®°‰∫∫È°ûÂøÉÊô∫ÁöÑÂ§öÊ≠•È©üË≥áË®äÊêúÂ∞ãÔºöÂÆÉÂ∞á‰ΩøÁî®ËÄÖÊü•Ë©¢ÂàÜËß£ÊàêÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÔºå‰ΩúÁÇ∫ÂéüÂ≠êÂåñÂ≠êÂïèÈ°åÔºå‰∏¶Ê†πÊìö WebSearcher ÁöÑÊêúÂ∞ãÁµêÊûúÈÄêÊ≠•Âª∂‰º∏ÂúñÂΩ¢„ÄÇWebSearcher ‰ª•ÊØèÂÄãÂ≠êÂïèÈ°åÁÇ∫‰ªªÂãôÔºåÂü∑Ë°åÊêúÂ∞ãÂºïÊìéÁöÑÂàÜÂ±§ÂºèË≥áË®äÊì∑ÂèñÔºå‰∏¶ÁÇ∫ WebPlanner Êî∂ÈõÜÊúâÂÉπÂÄºÁöÑË≥áË®ä„ÄÇMindSearch ÁöÑÂ§ö‰ª£ÁêÜË®≠Ë®àËÆìÊï¥ÂÄãÊû∂ÊßãÂèØ‰ª•Âú® 3 ÂàÜÈêòÂÖßÂπ≥Ë°åÂú∞ÂæûÊõ¥Â§ßË¶èÊ®°Ôºà‰æãÂ¶ÇË∂ÖÈÅé 300 ÂÄãÔºâÁöÑÁ∂≤È†Å‰∏≠ÊêúÂ∞ã‰∏¶Êï¥ÂêàË≥áË®äÔºåÈÄôÁõ∏Áï∂Êñº 3 Â∞èÊôÇÁöÑ‰∫∫Âäõ„ÄÇMindSearch Âú®Ê∑±Â∫¶ÂíåÂª£Â∫¶ÊñπÈù¢ÈÉΩÈ°ØËëóÊèêÂçá‰∫ÜÂõûÊáâÂìÅË≥™ÔºåÁÑ°Ë´ñÊòØÂú®Â∞ÅÈñâÂºèÊàñÈñãÊîæÂºèÂïèÁ≠îÂïèÈ°å‰∏ä„ÄÇÊ≠§Â§ñÔºå‰∫∫È°ûÊõ¥ÂÅèÂ•ΩÂü∫Êñº InternLM2.5-7B ÁöÑ MindSearch ÂõûÊáâÔºåÂãùÈÅé ChatGPT-Web Âíå Perplexity.ai ÊáâÁî®Á®ãÂºèÔºåÈÄôË°®Á§∫ MindSearch Â∑≤Á∂ìÂèØ‰ª•ÁÇ∫Â∞àÊúâ AI ÊêúÂ∞ãÂºïÊìéÊèê‰æõÊúâÁ´∂Áà≠ÂäõÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **rLLM: Relational Table Learning with LLMs**
2407.20157v1 by Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, Jianhua Li

We introduce rLLM (relationLLM), a PyTorch library designed for Relational
Table Learning (RTL) with Large Language Models (LLMs). The core idea is to
decompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural
Networks into standardized modules, to enable the fast construction of novel
RTL-type models in a simple "combine, align, and co-train" manner. To
illustrate the usage of rLLM, we introduce a simple RTL method named
\textbf{BRIDGE}. Additionally, we present three novel relational tabular
datasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope
rLLM can serve as a useful and easy-to-use development framework for
RTL-related tasks. Our code is available at:
https://github.com/rllm-project/rllm.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫Ü rLLM (relationLLM)Ôºå‰∏ÄÂÄãÂ∞àÁÇ∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈóú‰øÇË°®Â≠∏Áøí (RTL) ÊâÄË®≠Ë®àÁöÑ PyTorch ÂáΩÂºèÂ∫´„ÄÇÊ†∏ÂøÉÊ¶ÇÂøµÊòØÂ∞áÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅLLM ÂíåË°®Á•ûÁ∂ìÁ∂≤Ë∑ØÂàÜËß£ÁÇ∫Ê®ôÊ∫ñÂåñÊ®°ÁµÑÔºå‰ª•‰æø‰ª•Á∞°ÂñÆÁöÑ„ÄåÁµÑÂêà„ÄÅÂ∞çÈΩäÂíåÂÖ±ÂêåË®ìÁ∑¥„ÄçÊñπÂºèÂø´ÈÄüÂª∫ÊßãÊñ∞Âûã RTL È°ûÂûãÊ®°Âûã„ÄÇÁÇ∫‰∫ÜË™™Êòé rLLM ÁöÑÁî®Ê≥ïÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂêçÁÇ∫ \textbf{BRIDGE} ÁöÑÁ∞°ÂñÆ RTL ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÂº∑ÂåñÁ∂ìÂÖ∏Ë≥áÊñôÈõÜ‰æÜÂëàÁèæ‰∏âÂÄãÊñ∞Á©éÁöÑÈóú‰øÇË°®Ê†ºË≥áÊñôÈõÜ (TML1M„ÄÅTLF2K Âíå TACM12K)„ÄÇÊàëÂÄëÂ∏åÊúõ rLLM ËÉΩÂ§†‰ΩúÁÇ∫ RTL Áõ∏Èóú‰ªªÂãôÊúâÁî®ÁöÑ‰∏îÊòìÊñº‰ΩøÁî®ÁöÑÈñãÁôºÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö
https://github.com/rllm-project/rllm„ÄÇ

##### **Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**
2407.19643v2 by Yunsheng Wang, Songhao Chen, Kevin Jin

Knowledge graphs (KGs) are essential in applications such as network
alignment, question-answering, and recommender systems (RSs) since they offer
structured relational data that facilitate the inference of indirect
relationships. However, the development of KG-based RSs capable of processing
user inputs in natural language faces significant challenges. Firstly, natural
language processing units must effectively handle the ambiguity and variability
in human language to interpret user intents accurately. Secondly, the system
must precisely identify and link entities, like product names, to their
corresponding nodes in KGs. To overcome these challenges, supported by Lenovo,
we developed a novel chatbot called "Prometheus," which integrates a KG with a
large language model (LLM), specifically designed for recommending computer
components. This chatbot can accurately decode user requests and deliver
personalized recommendations derived from KGs, ensuring precise comprehension
and response to their computer setup needs.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Âú®Á∂≤Ë∑ØÊØîÂ∞ç„ÄÅÂïèÁ≠îÂíåÊé®Ëñ¶Á≥ªÁµ± (RS) Á≠âÊáâÁî®‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊèê‰æõÁµêÊßãÂåñÁöÑÈóú‰øÇË≥áÊñôÔºåÊúâÂä©ÊñºÊé®Êñ∑ÈñìÊé•Èóú‰øÇ„ÄÇÁÑ∂ËÄåÔºåÈñãÁôºËÉΩÂ§†ËôïÁêÜËá™ÁÑ∂Ë™ûË®Ä‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂü∫Êñº KG ÁöÑ RS Èù¢Ëá®ËëóÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂñÆÂÖÉÂøÖÈ†àÊúâÊïàËôïÁêÜ‰∫∫È°ûË™ûË®Ä‰∏≠ÁöÑÊ®°Á≥äÊÄßÂíåËÆäÁï∞ÊÄßÔºåÊâçËÉΩÊ∫ñÁ¢∫Âú∞Ëß£Èáã‰ΩøÁî®ËÄÖÊÑèÂúñ„ÄÇÂÖ∂Ê¨°ÔºåÁ≥ªÁµ±ÂøÖÈ†àÊ∫ñÁ¢∫Ë≠òÂà•ÂíåÈÄ£ÁµêÂØ¶È´îÔºà‰æãÂ¶ÇÁî¢ÂìÅÂêçÁ®±ÔºâÂà∞ KG ‰∏≠Â∞çÊáâÁöÑÁØÄÈªû„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÂú®ËÅØÊÉ≥ÁöÑÊîØÊè¥‰∏ãÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÊ¨æÂêçÁÇ∫„ÄåÊôÆÁæÖÁ±≥‰øÆÊñØ„ÄçÁöÑÊñ∞ËÅäÂ§©Ê©üÂô®‰∫∫ÔºåÂÆÉÂ∞á KG ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÂ∞àÈñÄÁî®ÊñºÊé®Ëñ¶ÈõªËÖ¶ÁµÑ‰ª∂„ÄÇÊ≠§ËÅäÂ§©Ê©üÂô®‰∫∫ÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Ëß£Á¢º‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÔºå‰∏¶Êèê‰æõÂæû KG ‰∏≠Ë°çÁîüÁöÑÂÄã‰∫∫ÂåñÊé®Ëñ¶ÔºåÁ¢∫‰øùÁ≤æÁ¢∫ÁêÜËß£ÂíåÂõûÊáâÂÖ∂ÈõªËÖ¶Ë®≠ÂÆöÈúÄÊ±Ç„ÄÇ

##### **TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**
2407.19616v1 by Selma Wanna, Ryan Barron, Nick Solovyev, Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov

Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.

ÊëòË¶ÅÔºö‰∏ªÈ°åÂª∫Ê®°ÊòØ‰∏ÄÁ®ÆÂæûÂ§ßÈáèÈùûÁµêÊßãÂåñÊñáÊú¨‰∏≠ÁµÑÁπîÂíåÊèêÂèñ‰∏ªÈ°åÁöÑÊäÄË°ì„ÄÇÈùûË≤†Áü©Èô£ÂàÜËß£ (NMF) ÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÁÑ°Áõ£Áù£ÊñπÊ≥ïÔºåÂÆÉÂ∞áË©ûÈ†ª-ÈÄÜÊñá‰ª∂È†ªÁéá (TF-IDF) Áü©Èô£ÂàÜËß£ÁÇ∫ÊΩõÂú®‰∏ªÈ°åÔºå‰∏¶ÊìöÊ≠§Â∞çÊï∏ÊìöÈõÜÈÄ≤Ë°åÂàÜÊÆµ„ÄÇÂÑòÁÆ° NMF ÂèØÁî®ÊñºÂº∑Ë™øÊ®°ÂºèÂíåÁæ§ÁµÑÊñá‰ª∂Ôºå‰ΩÜÂÆÉ‰∏çÊèê‰æõÊòéÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§ÔºåÈÄôÈúÄË¶Å‰∏ªÈ°åÂ∞àÂÆ∂ (SME) ÊâãÂãïÂàÜÈÖçÊ®ôÁ±§„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÁî®ÊñºËá™ÂãïÊ®ôË®òÈÄöÈÅé NMF ÈÄ≤Ë°åÁæ§ÁµÑÁöÑÊñá‰ª∂Ôºå‰∏¶Ëá™ÂãïÁ¢∫ÂÆöÊ®°Âûã (NMFk)„ÄÇÈÄöÈÅéÂà©Áî® NMFk ÁöÑËº∏Âá∫‰∏¶Êé°Áî®ÊèêÁ§∫Â∑•Á®ãÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁîüÊàêÊ∫ñÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§„ÄÇÊàëÂÄëÂ∞çË∂ÖÈÅé 34,000 ÁØáÈóúÊñºÁü•Ë≠òÂúñË≠úÁöÑÁßëÂ≠∏ÊëòË¶ÅÈÄ≤Ë°åÁöÑÊ°à‰æãÁ†îÁ©∂Ë≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Â¢ûÂº∑Áü•Ë≠òÁÆ°ÁêÜÂíåÊñá‰ª∂ÁµÑÁπîÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Semantic Communication Enhanced by Knowledge Graph Representation Learning**
2407.19338v1 by Nour Hello, Paolo Di Lorenzo, Emilio Calvanese Strinati

This paper investigates the advantages of representing and processing
semantic knowledge extracted into graphs within the emerging paradigm of
semantic communications. The proposed approach leverages semantic and pragmatic
aspects, incorporating recent advances on large language models (LLMs) to
achieve compact representations of knowledge to be processed and exchanged
between intelligent agents. This is accomplished by using the cascade of LLMs
and graph neural networks (GNNs) as semantic encoders, where information to be
shared is selected to be meaningful at the receiver. The embedding vectors
produced by the proposed semantic encoder represent information in the form of
triplets: nodes (semantic concepts entities), edges(relations between
concepts), nodes. Thus, semantic information is associated with the
representation of relationships among elements in the space of semantic concept
abstractions. In this paper, we investigate the potential of achieving high
compression rates in communication by incorporating relations that link
elements within graph embeddings. We propose sending semantic symbols solely
equivalent to node embeddings through the wireless channel and inferring the
complete knowledge graph at the receiver. Numerical simulations illustrate the
effectiveness of leveraging knowledge graphs to semantically compress and
transmit information.

ÊëòË¶ÅÔºöÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®ËØ≠‰πâÈÄö‰ø°ÁöÑÊñ∞ÂÖ¥ËåÉ‰æã‰∏≠Â∞ÜÊèêÂèñÂà∞Âõæ‰∏≠ÁöÑËØ≠‰πâÁü•ËØÜË°®Á§∫ÂíåÂ§ÑÁêÜÁöÑ‰ºòÂäø„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂà©Áî®ËØ≠‰πâÂíåËØ≠Áî®ÊñπÈù¢ÔºåÁªìÂêà‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ËøõÂ±ïÔºå‰ª•ÂÆûÁé∞Ë¶ÅÂ§ÑÁêÜÂíåÂú®Êô∫ËÉΩ‰ª£ÁêÜ‰πãÈó¥‰∫§Êç¢ÁöÑÁü•ËØÜÁöÑÁ¥ßÂáëË°®Á§∫„ÄÇËøôÊòØÈÄöËøá‰ΩøÁî® LLM ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ÁöÑÁ∫ßËÅî‰Ωú‰∏∫ËØ≠‰πâÁºñÁ†ÅÂô®Êù•ÂÆåÊàêÁöÑÔºåÂÖ∂‰∏≠Ë¶ÅÂÖ±‰∫´ÁöÑ‰ø°ÊÅØË¢´ÈÄâÊã©‰∏∫ÂØπÊé•Êî∂ËÄÖÊúâÊÑè‰πâ„ÄÇÁî±ÊâÄÊèêÂá∫ÁöÑËØ≠‰πâÁºñÁ†ÅÂô®‰∫ßÁîüÁöÑÂµåÂÖ•ÂêëÈáè‰ª•‰∏âÂÖÉÁªÑÁöÑÂΩ¢ÂºèË°®Á§∫‰ø°ÊÅØÔºöËäÇÁÇπÔºàËØ≠‰πâÊ¶ÇÂøµÂÆû‰ΩìÔºâ„ÄÅËæπÔºàÊ¶ÇÂøµ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºâ„ÄÅËäÇÁÇπ„ÄÇÂõ†Ê≠§ÔºåËØ≠‰πâ‰ø°ÊÅØ‰∏éËØ≠‰πâÊ¶ÇÂøµÊäΩË±°Á©∫Èó¥‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÂÖ≥Á≥ªÁöÑË°®Á§∫Áõ∏ÂÖ≥ËÅî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÈÄöËøáÂêàÂπ∂Â∞ÜÂõæÂµåÂÖ•‰∏≠ÁöÑÂÖÉÁ¥†ËÅîÁ≥ªËµ∑Êù•ÁöÑÂÖ≥ËÅîÊù•ÂÆûÁé∞È´òÂéãÁº©ÁéáÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Âª∫ËÆÆ‰ªÖÈÄöËøáÊó†Á∫ø‰ø°ÈÅìÂèëÈÄÅËØ≠‰πâÁ¨¶Âè∑ÔºåËøô‰∫õÁ¨¶Âè∑ÂÆåÂÖ®Á≠âÊïà‰∫éËäÇÁÇπÂµåÂÖ•ÔºåÂπ∂Âú®Êé•Êî∂Âô®Â§ÑÊé®Êñ≠Âá∫ÂÆåÊï¥ÁöÑÁü•ËØÜÂõæ„ÄÇÊï∞ÂÄºÊ®°ÊãüËØ¥Êòé‰∫ÜÂà©Áî®Áü•ËØÜÂõæËØ≠‰πâÂéãÁº©Âíå‰º†Ëæì‰ø°ÊÅØÁöÑÊúâÊïàÊÄß„ÄÇ

##### **GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**
2407.19039v1 by Yuchen Shen, Barnab√°s P√≥czos

With the increasing attention to molecular machine learning, various
innovations have been made in designing better models or proposing more
comprehensive benchmarks. However, less is studied on the data preprocessing
schedule for molecular graphs, where a different view of the molecular graph
could potentially boost the model's performance. Inspired by the Byte-Pair
Encoding (BPE) algorithm, a subword tokenization method popularly adopted in
Natural Language Processing, we propose GraphBPE, which tokenizes a molecular
graph into different substructures and acts as a preprocessing schedule
independent of the model architectures. Our experiments on 3 graph-level
classification and 3 graph-level regression datasets show that data
preprocessing could boost the performance of models for molecular graphs, and
GraphBPE is effective for small classification datasets and it performs on par
with other tokenization methods across different model architectures.

ÊëòË¶ÅÔºöÈö®ËëóÂàÜÂ≠êÊ©üÂô®Â≠∏ÁøíÂèóÂà∞ÁöÑÈóúÊ≥®Â∫¶Ë∂ä‰æÜË∂äÈ´òÔºåÂú®Ë®≠Ë®àÊõ¥Â•ΩÁöÑÊ®°ÂûãÊàñÊèêÂá∫Êõ¥ÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÊñπÈù¢Â∑≤Á∂ìÊúâ‰∫ÜÂêÑÁ®ÆÂâµÊñ∞„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂàÜÂ≠êÂúñÁöÑÊï∏ÊìöÈ†êËôïÁêÜË®àÁï´Á†îÁ©∂ËºÉÂ∞ëÔºåÂú®Ë©≤Ë®àÁï´‰∏≠ÔºåÂàÜÂ≠êÂúñÁöÑ‰∏çÂêåË¶ñÂúñÂèØËÉΩÊúÉÊèêÂçáÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÂèóÂà∞Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠Âª£Ê≥õÊé°Áî®ÁöÑÂ≠êË©ûÂΩôÊ®ôË®òÂåñÊñπÊ≥ï Byte-Pair Á∑®Á¢º (BPE) ÊºîÁÆóÊ≥ïÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphBPEÔºåÂÆÉÂ∞áÂàÜÂ≠êÂúñÊ®ôË®òÂåñÁÇ∫‰∏çÂêåÁöÑÂ≠êÁµêÊßãÔºå‰∏¶‰ΩúÁÇ∫ËàáÊ®°ÂûãÊû∂ÊßãÁÑ°ÈóúÁöÑÈ†êËôïÁêÜË®àÁï´„ÄÇÊàëÂÄëÂú® 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÂíå 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂõûÊ≠∏Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåË≥áÊñôÈ†êËôïÁêÜÂèØ‰ª•ÊèêÂçáÂàÜÂ≠êÂúñÊ®°ÂûãÁöÑÊïàËÉΩÔºåËÄå GraphBPE Â∞çÊñºÂ∞èÂûãÂàÜÈ°ûË≥áÊñôÈõÜÊúâÊïàÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊ®°ÂûãÊû∂Êßã‰∏≠ËàáÂÖ∂‰ªñÊ®ôË®òÂåñÊñπÊ≥ïË°®ÁèæÁõ∏Áï∂„ÄÇ

##### **Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**
2407.18752v3 by Yuni Susanti, Michael F√§rber

Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.

ÊëòË¶ÅÔºöÂõ†ÊûúÁôºÁèæÊó®Âú®Ê†πÊìöËßÄÊ∏¨Êï∏Êìö‰º∞Ë®àËÆäÊï∏‰πãÈñìÁöÑÂõ†ÊûúÁµêÊßã„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑËßÄÈªû‰æÜËß£Ê±∫Âõ†ÊûúÁôºÁèæÂïèÈ°åÔºåÊñπÊ≥ïÊòØÊé®Ë´ñËàáËÆäÊï∏Áõ∏ÈóúÁöÑÂÖÉÊï∏ÊìöÔºåËÄå‰∏çÊòØÂÆÉÂÄëÁöÑÂØ¶ÈöõÊï∏ÊìöÂÄºÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫Âü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂ∞èË™ûË®ÄÊ®°Âûã (SLMÔºåÂÆöÁæ©ÁÇ∫ÂèÉÊï∏Â∞ëÊñº 10 ÂÑÑÁöÑ LLM) ÁöÑËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÈÄ≤Ë°åÂü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü KG Structure as PromptÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞á‰æÜËá™Áü•Ë≠òÂúñË≠úÁöÑÁµêÊßãË≥áË®äÔºå‰æãÂ¶ÇÂÖ±ÂêåÈÑ∞Â±ÖÁØÄÈªûÂíåÂÖÉË∑ØÂæëÔºåÊï¥ÂêàÂà∞Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏Áøí‰∏≠Ôºå‰ª•Â¢ûÂº∑ SLM ÁöÑËÉΩÂäõ„ÄÇÂú®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆö‰∏ãÔºåÈáùÂ∞ç‰∏âÁ®ÆÈ°ûÂûãÁöÑÁîüÁâ©ÈÜ´Â≠∏ÂíåÈñãÊîæÈ†òÂüüË≥áÊñôÈõÜÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË∂ÖË∂ä‰∫ÜÂ§ßÂ§öÊï∏Âü∫Ê∫ñÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÂú®ÂÆåÊï¥Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂÇ≥Áµ±ÂæÆË™øÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈÄ≤‰∏ÄÊ≠•Á™ÅÂá∫‰∫Ü SLM ÁöÑÂº∑Â§ßÂäüËÉΩÔºöÁµêÂêàÁü•Ë≠òÂúñË≠úÂíåÂü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÔºåSLM Â±ïÁ§∫‰∫ÜË∂ÖË∂äÂÖ∑ÊúâÊõ¥Â§öÂèÉÊï∏ÁöÑ LLM ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® GitHub ‰∏äÂèñÂæó„ÄÇ

##### **Using GPT-4 to guide causal machine learning**
2407.18607v1 by Anthony C. Constantinou, Neville K. Kitson, Alessio Zanga

Since its introduction to the public, ChatGPT has had an unprecedented
impact. While some experts praised AI advancements and highlighted their
potential risks, others have been critical about the accuracy and usefulness of
Large Language Models (LLMs). In this paper, we are interested in the ability
of LLMs to identify causal relationships. We focus on the well-established
GPT-4 (Turbo) and evaluate its performance under the most restrictive
conditions, by isolating its ability to infer causal relationships based solely
on the variable labels without being given any context, demonstrating the
minimum level of effectiveness one can expect when it is provided with
label-only information. We show that questionnaire participants judge the GPT-4
graphs as the most accurate in the evaluated categories, closely followed by
knowledge graphs constructed by domain experts, with causal Machine Learning
(ML) far behind. We use these results to highlight the important limitation of
causal ML, which often produces causal graphs that violate common sense,
affecting trust in them. However, we show that pairing GPT-4 with causal ML
overcomes this limitation, resulting in graphical structures learnt from real
data that align more closely with those identified by domain experts, compared
to structures learnt by causal ML alone. Overall, our findings suggest that
despite GPT-4 not being explicitly designed to reason causally, it can still be
a valuable tool for causal representation, as it improves the causal discovery
process of causal ML algorithms that are designed to do just that.

ÊëòË¶ÅÔºöËá™ ChatGPT ÂêëÂÖ¨‰ºóÂèëÂ∏É‰ª•Êù•ÔºåÂÆÉ‰∫ßÁîü‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÂΩ±Âìç„ÄÇËôΩÁÑ∂‰∏Ä‰∫õ‰∏ìÂÆ∂ËµûÊâ¨‰∫Ü AI ÁöÑËøõÊ≠•Âπ∂Âº∫Ë∞É‰∫ÜÂÖ∂ÊΩúÂú®È£éÈô©Ôºå‰ΩÜÂÖ∂‰ªñ‰∫∫‰∏ÄÁõ¥ÊâπËØÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÁî®ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂØπ LLM ËØÜÂà´Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÊÑüÂÖ¥Ë∂£„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊàêÁÜüÁöÑ GPT-4ÔºàTurboÔºâÔºåÂπ∂Âú®ÊúÄ‰∏•Ê†ºÁöÑÊù°‰ª∂‰∏ãËØÑ‰º∞ÂÖ∂ÊÄßËÉΩÔºåÈÄöËøáÂ≠§Á´ãÂÖ∂‰ªÖÊ†πÊçÆÂèòÈáèÊ†áÁ≠æÊé®Êñ≠Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÔºåËÄå‰∏çÊèê‰æõ‰ªª‰Ωï‰∏ä‰∏ãÊñáÔºåÂ±ïÁ§∫‰∫ÜÂΩì‰ªÖÊèê‰æõÊ†áÁ≠æ‰ø°ÊÅØÊó∂‰∫∫‰ª¨ÂèØ‰ª•È¢ÑÊúüÁöÑÊúÄ‰ΩéÊúâÊïàÊÄßÊ∞¥Âπ≥„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÈóÆÂç∑ÂèÇ‰∏éËÄÖËÆ§‰∏∫ GPT-4 ÂõæÂΩ¢Âú®ËØÑ‰º∞Á±ªÂà´‰∏≠ÊòØÊúÄÂáÜÁ°ÆÁöÑÔºåÁ¥ßÈöèÂÖ∂ÂêéÁöÑÊòØÁî±È¢ÜÂüü‰∏ìÂÆ∂ÊûÑÂª∫ÁöÑÁü•ËØÜÂõæË∞±ÔºåÂõ†ÊûúÊú∫Âô®Â≠¶‰π† (ML) ËøúËøúËêΩÂêé„ÄÇÊàë‰ª¨‰ΩøÁî®Ëøô‰∫õÁªìÊûúÊù•Âº∫Ë∞ÉÂõ†Êûú ML ÁöÑÈáçË¶ÅÂ±ÄÈôêÊÄßÔºåÂÆÉÁªèÂ∏∏‰∫ßÁîüËøùËÉåÂ∏∏ËØÜÁöÑÂõ†ÊûúÂõæÔºåÂΩ±Âìç‰∫∫‰ª¨ÂØπÂÆÉ‰ª¨ÁöÑ‰ø°‰ªª„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨Ë°®ÊòéÂ∞Ü GPT-4 ‰∏éÂõ†Êûú ML ÈÖçÂØπÂèØ‰ª•ÂÖãÊúçËøô‰∏ÄÈôêÂà∂Ôºå‰ªéËÄå‰∫ßÁîü‰ªéÁúüÂÆûÊï∞ÊçÆ‰∏≠Â≠¶Âà∞ÁöÑÂõæÂΩ¢ÁªìÊûÑÔºå‰∏éÈ¢ÜÂüü‰∏ìÂÆ∂ËØÜÂà´ÁöÑÁªìÊûÑÁõ∏ÊØîÔºåÊõ¥Á¥ßÂØÜÂú∞‰∏é‰πãÂØπÈΩêÔºåËÄå‰∏çÊòØ‰ªÖÁî±Âõ†Êûú ML Â≠¶Âà∞ÁöÑÁªìÊûÑ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ° GPT-4 Âπ∂Êú™ÊòéÁ°ÆËÆæËÆ°‰∏∫Âõ†ÊûúÊé®ÁêÜÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÂèØ‰ª•Êàê‰∏∫Âõ†ÊûúË°®Á§∫ÁöÑÂÆùË¥µÂ∑•ÂÖ∑ÔºåÂõ†‰∏∫ÂÆÉÊîπËøõ‰∫ÜÊó®Âú®ÊâßË°åÊ≠§Êìç‰ΩúÁöÑÂõ†Êûú ML ÁÆóÊ≥ïÁöÑÂõ†ÊûúÂèëÁé∞ËøáÁ®ã„ÄÇ

##### **Multi-turn Response Selection with Commonsense-enhanced Language Models**
2407.18479v1 by Yuandong Wang, Xuhui Ren, Tong Chen, Yuxiao Dong, Nguyen Quoc Viet Hung, Jie Tang

As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫È´òÁ¥ö‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÂÄãÂàÜÊîØÔºåÂ∞çË©±Á≥ªÁµ±Ê≠£Ëì¨ÂãÉÁôºÂ±ï„ÄÇÂ§öËº™ÂõûÊáâÁî®Êà∂ÂõûÊáâÈÅ∏ÊìáÊòØÂ∞çË©±Á≥ªÁµ±‰∏≠‰∏ÄÂÄãÈÄöÁî®ÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÂú®ËÉåÊôØË≥áË®äÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÁöÑÂçîÂä©‰∏ãÔºåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÂú®Ê≠§ÂïèÈ°å‰∏äÁöÑË°®ÁèæÁç≤Ëá¥‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÂøΩÁï•‰∫ÜÂ§ñÈÉ®Â∏∏Ë≠òÁü•Ë≠òÁöÑÈáçË¶ÅÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊöπÁæÖÁ∂≤Ë∑ØÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãËàá‰∏ÄÂÄãÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàSinLGÔºâÂêà‰Ωµ„ÄÇSinLG Âà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºàPLMÔºâ‰æÜÊçïÊçâË™ûÂ¢ÉÂíåÂõûÊáâÂÄôÈÅ∏‰∏≠ÁöÑË©ûÂΩôÈóúËÅØÔºå‰∏¶Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠úÊé®ÁêÜÊúâÁî®ÁöÑÂ∏∏Ë≠ò„ÄÇGNN Êó®Âú®ÂçîÂä© PLM ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶ÂñöÈÜíÂÖ∂Áõ∏ÈóúË®òÊÜ∂‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑË°®Áèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠ú‰∏≠ÊèêÂèñÁõ∏ÈóúÊ¶ÇÂøµ‰ΩúÁÇ∫ÁØÄÈªûÔºå‰ª•ÊßãÂª∫‰∏ÄÂÄãÂ≠êÂúñÔºåÂÖ∂‰∏≠Ë™ûÂ¢ÉÂõûÊáâÂ∞ç‰ΩúÁÇ∫ÊØèÂÄãÁØÑ‰æãÁöÑË∂ÖÁ¥öÁØÄÈªû„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈÄèÈÅé PLM Âíå GNN ÁÇ∫Ë™ûÂ¢ÉÂõûÊáâÂ∞çÂ≠∏ÁøíÂÖ©ÂÄãË°®Á§∫„ÄÇÂÖ©ÂÄãË°®Á§∫‰πãÈñìÁöÑÁõ∏‰ººÊÄßÊêçÂ§±Áî®ÊñºÂ∞áÂ∏∏Ë≠òÁü•Ë≠òÂæû GNN ËΩâÁßªÂà∞ PLM„ÄÇÁÑ∂ÂæåÂÉÖ‰ΩøÁî® PLM ‰æÜÈÄ≤Ë°åÁ∑ö‰∏äÊé®Ë´ñÔºå‰ª•‰æø‰øùË≠âÊïàÁéá„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞ç PERSONA-CHAT Ë≥áÊñôÈõÜÁöÑÂÖ©ÂÄãËÆäÈ´îÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÈÄôË≠âÊòéÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°à‰∏çÂÉÖÂèØ‰ª•ÊèêÈ´ò PLM ÁöÑÊïàËÉΩÔºåÈÇÑËÉΩÂØ¶ÁèæÈ´òÊïàÁöÑÊé®Ë´ñ„ÄÇ

##### **Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**
2407.18181v1 by Sindhura Kommu, Yizhi Wang, Yue Wang, Xuan Wang

Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing
(scRNA-seq) data is a complex challenge that requires capturing the intricate
relationships between genes and their regulatory interactions. In this study,
we tackle this challenge by leveraging the single-cell BERT-based pre-trained
transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to
augment structured biological knowledge from existing GRNs. We introduce a
novel joint graph learning approach that combines the rich contextual
representations learned by pre-trained single-cell language models with the
structured knowledge encoded in GRNs using graph neural networks (GNNs). By
integrating these two modalities, our approach effectively reasons over boththe
gene expression level constraints provided by the scRNA-seq data and the
structured biological knowledge inherent in GRNs. We evaluate our method on
human cell benchmark datasets from the BEELINE study with cell type-specific
ground truth networks. The results demonstrate superior performance over
current state-of-the-art baselines, offering a deeper understanding of cellular
regulatory mechanisms.

ÊëòË¶ÅÔºöÂæûÂñÆÁ¥∞ËÉû RNA ÂÆöÂ∫è (scRNA-seq) Ë≥áÊñôÊé®Ë´ñÂü∫Âõ†Ë™øÊéßÁ∂≤Ë∑Ø (GRN) ÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÊéåÊè°Âü∫Âõ†ËàáÂÖ∂Ë™øÊéß‰∫§‰∫í‰ΩúÁî®‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂà©Áî®Âú®Âª£Ê≥õÁöÑÊú™Ê®ôË®ò scRNA-seq Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂñÆÁ¥∞ËÉû BERT Âü∫ÊñºÈ†êË®ìÁ∑¥ËΩâÊèõÂô®Ê®°Âûã (scBERT)Ôºå‰æÜÂÖãÊúçÊ≠§ÊåëÊà∞Ôºå‰ª•Êì¥ÂÖÖÁèæÊúâ GRN ‰∏≠ÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠ò„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËÅØÂêàÂúñÂΩ¢Â≠∏ÁøíÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÈ†êË®ìÁ∑¥ÂñÆÁ¥∞ËÉûË™ûË®ÄÊ®°ÂûãÊâÄÂ≠∏ÁøíÂà∞ÁöÑË±êÂØåËÑàÁµ°Ë°®ÂæµÔºå‰ª•Âèä‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞ç GRN ‰∏≠Á∑®Á¢ºÁöÑÁµêÊßãÂåñÁü•Ë≠ò„ÄÇÈÄèÈÅéÊï¥ÂêàÈÄôÂÖ©Á®ÆÊñπÂºèÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊúâÊïàÂú∞Â∞ç scRNA-seq Ë≥áÊñôÊèê‰æõÁöÑÂü∫Âõ†Ë°®ÁèæÂ±§Á¥öÁ¥ÑÊùüÂíå GRN ‰∏≠Âõ∫ÊúâÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÊàëÂÄë‰ΩøÁî® BEELINE Á†îÁ©∂‰∏≠ÁöÑ‰∫∫È°ûÁ¥∞ËÉûÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰ª•ÂèäÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöÁöÑÂü∫Êú¨‰∫ãÂØ¶Á∂≤Ë∑ØÔºå‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊñπÊ≥ï„ÄÇÁµêÊûúË≠âÊòéÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÔºåÊèê‰æõ‰∫ÜÂ∞çÁ¥∞ËÉûË™øÊéßÊ©üÂà∂ÁöÑÊõ¥Ê∑±ÂÖ•ÁêÜËß£„ÄÇ

##### **MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**
2407.17544v1 by Arya Bulusu, Brandon Man, Ashish Jagmohan, Aditya Vempaty, Jennifer Mari-Wyka, Deepak Akkil

There has been significant recent interest in harnessing LLMs to control
software systems through multi-step reasoning, planning and tool-usage. While
some promising results have been obtained, application to specific domains
raises several general issues including the control of specialized domain
tools, the lack of existing datasets for training and evaluation, and the
non-triviality of automated system evaluation and improvement. In this paper,
we present a case-study where we examine these issues in the context of a
specific domain. Specifically, we present an automated math visualizer and
solver system for mathematical pedagogy. The system orchestrates mathematical
solvers and math graphing tools to produce accurate visualizations from simple
natural language commands. We describe the creation of specialized data-sets,
and also develop an auto-evaluator to easily evaluate the outputs of our system
by comparing them to ground-truth expressions. We have open sourced the
data-sets and code for the proposed system.

ÊëòË¶ÅÔºöÊúÄËøëÔºå‰∫∫‰ª¨ÂØπÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Êù•ÈÄöËøáÂ§öÊ≠•È™§Êé®ÁêÜ„ÄÅËßÑÂàíÂíåÂ∑•ÂÖ∑‰ΩøÁî®Êù•ÊéßÂà∂ËΩØ‰ª∂Á≥ªÁªü‰∫ßÁîü‰∫ÜÊûÅÂ§ßÁöÑÂÖ¥Ë∂£„ÄÇËôΩÁÑ∂Â∑≤ÁªèÂèñÂæó‰∫Ü‰∏Ä‰∫õÊúâÂ∏åÊúõÁöÑÁªìÊûúÔºå‰ΩÜÂ∫îÁî®‰∫éÁâπÂÆöÈ¢ÜÂüü‰ºöÂºïÂèëÂá†‰∏™ÊôÆÈÅçÊÄßÈóÆÈ¢òÔºåÂåÖÊã¨ÂØπ‰∏ì‰∏öÈ¢ÜÂüüÂ∑•ÂÖ∑ÁöÑÊéßÂà∂„ÄÅÁº∫‰πèÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞ÁöÑÁé∞ÊúâÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäËá™Âä®ÂåñÁ≥ªÁªüËØÑ‰º∞ÂíåÊîπËøõÁöÑÈùûÂπ≥Âá°ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê°à‰æãÁ†îÁ©∂ÔºåÂÖ∂‰∏≠Êàë‰ª¨Á†îÁ©∂‰∫ÜÁâπÂÆöÈ¢ÜÂüüËÉåÊôØ‰∏ãÁöÑËøô‰∫õÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™Áî®‰∫éÊï∞Â≠¶ÊïôËÇ≤ÁöÑËá™Âä®ÂåñÊï∞Â≠¶ÂèØËßÜÂåñÂô®ÂíåÊ±ÇËß£Âô®Á≥ªÁªü„ÄÇËØ•Á≥ªÁªüÂçèË∞ÉÊï∞Â≠¶Ê±ÇËß£Âô®ÂíåÊï∞Â≠¶ÁªòÂõæÂ∑•ÂÖ∑Ôºå‰ª•Ê†πÊçÆÁÆÄÂçïÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂëΩ‰ª§ÁîüÊàêÂáÜÁ°ÆÁöÑÂèØËßÜÂåñÊïàÊûú„ÄÇÊàë‰ª¨ÊèèËø∞‰∫Ü‰∏ìÈó®Êï∞ÊçÆÈõÜÁöÑÂàõÂª∫ÔºåËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ËØÑ‰º∞Âô®ÔºåÈÄöËøáÂ∞ÜÊàë‰ª¨ÁöÑÁ≥ªÁªüËæìÂá∫‰∏éÁúüÂÆûË°®ËææÂºèËøõË°åÊØîËæÉÔºåËΩªÊùæËØÑ‰º∞ÂÖ∂ËæìÂá∫„ÄÇÊàë‰ª¨Â∑≤ÁªèÂºÄÊ∫ê‰∫ÜÊâÄÊèêËÆÆÁ≥ªÁªüÁöÑ‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜ„ÄÇ

##### **Ranking protein-protein models with large language models and graph neural networks**
2407.16375v1 by Xiaotong Xu, Alexandre M. J. J. Bonvin

Protein-protein interactions (PPIs) are associated with various diseases,
including cancer, infections, and neurodegenerative disorders. Obtaining
three-dimensional structural information on these PPIs serves as a foundation
to interfere with those or to guide drug design. Various strategies can be
followed to model those complexes, all typically resulting in a large number of
models. A challenging step in this process is the identification of good models
(near-native PPI conformations) from the large pool of generated models. To
address this challenge, we previously developed DeepRank-GNN-esm, a graph-based
deep learning algorithm for ranking modelled PPI structures harnessing the
power of protein language models. Here, we detail the use of our software with
examples. DeepRank-GNN-esm is freely available at
https://github.com/haddocking/DeepRank-GNN-esm

ÊëòË¶ÅÔºöËõãÁôΩ-ËõãÁôΩ‰∫§‰∫í‰ΩúÁî® (PPI) ËàáÂêÑÁ®ÆÁñæÁóÖÁõ∏ÈóúÔºåÂåÖÊã¨ÁôåÁóá„ÄÅÊÑüÊüìÂíåÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖ„ÄÇÂèñÂæóÈÄô‰∫õ PPI ÁöÑ‰∏âÁ∂≠ÁµêÊßãË≥áË®äÔºå‰ΩúÁÇ∫Âπ≤ÊìæÂÆÉÂÄëÊàñÂºïÂ∞éËó•Áâ©Ë®≠Ë®àÁöÑÂü∫Á§é„ÄÇÂèØ‰ª•ÈÅµÂæ™ÂêÑÁ®ÆÁ≠ñÁï•‰æÜÂª∫Ê®°ÈÄô‰∫õË§áÂêàÈ´îÔºåÊâÄÊúâÈÄô‰∫õÁ≠ñÁï•ÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑÊ®°Âûã„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑÊåëÊà∞ÊÄßÊ≠•È©üÔºåÊòØÂæûÂ§ßÈáèÁî¢ÁîüÁöÑÊ®°Âûã‰∏≠ÊâæÂá∫Â•ΩÁöÑÊ®°ÂûãÔºàÊé•ËøëÂéüÁîü PPI ÊßãË±°Ôºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄë‰πãÂâçÈñãÁôº‰∫Ü DeepRank-GNN-esmÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÔºåÁî®ÊñºÂ∞çÂª∫Ê®°ÁöÑ PPI ÁµêÊßãÈÄ≤Ë°åÊéíÂêçÔºåÂà©Áî®ËõãÁôΩË≥™Ë™ûË®ÄÊ®°ÂûãÁöÑÂäõÈáè„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫ÜÊàëÂÄëËªüÈ´îÁöÑ‰ΩøÁî®ÁØÑ‰æã„ÄÇDeepRank-GNN-esm ÂèØÂú® https://github.com/haddocking/DeepRank-GNN-esm ÂÖçË≤ªÂèñÂæó

##### **PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**
2407.16329v1 by Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo

Acute stroke demands prompt diagnosis and treatment to achieve optimal
patient outcomes. However, the intricate and irregular nature of clinical data
associated with acute stroke, particularly blood pressure (BP) measurements,
presents substantial obstacles to effective visual analytics and
decision-making. Through a year-long collaboration with experienced
neurologists, we developed PhenoFlow, a visual analytics system that leverages
the collaboration between human and Large Language Models (LLMs) to analyze the
extensive and complex data of acute ischemic stroke patients. PhenoFlow
pioneers an innovative workflow, where the LLM serves as a data wrangler while
neurologists explore and supervise the output using visualizations and natural
language interactions. This approach enables neurologists to focus more on
decision-making with reduced cognitive load. To protect sensitive patient
information, PhenoFlow only utilizes metadata to make inferences and synthesize
executable codes, without accessing raw patient data. This ensures that the
results are both reproducible and interpretable while maintaining patient
privacy. The system incorporates a slice-and-wrap design that employs temporal
folding to create an overlaid circular visualization. Combined with a linear
bar graph, this design aids in exploring meaningful patterns within irregularly
measured BP data. Through case studies, PhenoFlow has demonstrated its
capability to support iterative analysis of extensive clinical datasets,
reducing cognitive load and enabling neurologists to make well-informed
decisions. Grounded in long-term collaboration with domain experts, our
research demonstrates the potential of utilizing LLMs to tackle current
challenges in data-driven clinical decision-making for acute ischemic stroke
patients.

ÊëòË¶ÅÔºö<paragraph>ÊÄ•ÊÄß‰∏≠È¢®ÈúÄË¶ÅËøÖÈÄüË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºåÊâçËÉΩÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑÁóÖ‰∫∫Ê≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåËàáÊÄ•ÊÄß‰∏≠È¢®Áõ∏ÈóúÁöÑËá®Â∫äË≥áÊñôË§áÈõú‰∏î‰∏çË¶èÂâáÔºåÁâπÂà•ÊòØË°ÄÂ£ì (BP) Ê∏¨ÈáèÔºåÂ∞çÊúâÊïàÁöÑË¶ñË¶∫ÂàÜÊûêÂíåÊ±∫Á≠ñÂà∂ÂÆöÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÈÄèÈÅéËàáÁ∂ìÈ©óË±êÂØåÁöÑÁ•ûÁ∂ìÁßëÈÜ´Â∏´Èï∑ÈÅî‰∏ÄÂπ¥ÁöÑÂêà‰ΩúÔºåÊàëÂÄëÈñãÁôº‰∫Ü PhenoFlowÔºåÈÄôÊòØ‰∏ÄÂÄãË¶ñË¶∫ÂàÜÊûêÁ≥ªÁµ±ÔºåÂà©Áî®‰∫∫ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πãÈñìÁöÑÂçî‰Ωú‰æÜÂàÜÊûêÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖÁöÑÂª£Ê≥õ‰∏îË§áÈõúË≥áÊñô„ÄÇPhenoFlow ÈñãÂâµ‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÖ∂‰∏≠ LLM Êìî‰ªªË≥áÊñôÊï¥ÁêÜÂì°ÔºåËÄåÁ•ûÁ∂ìÁßëÈÜ´Â∏´Ââá‰ΩøÁî®Ë¶ñË¶∫ÂåñÂíåËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãï‰æÜÊé¢Á¥¢ÂíåÁõ£Áù£Ëº∏Âá∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†Êõ¥Â∞àÊ≥®ÊñºÊ±∫Á≠ñÂà∂ÂÆöÔºåÂêåÊôÇÈôç‰ΩéË™çÁü•Ë≤†Êìî„ÄÇÁÇ∫‰∫Ü‰øùË≠∑ÊïèÊÑüÁöÑÁóÖ‰∫∫Ë≥áË®äÔºåPhenoFlow ÂÉÖÂà©Áî®ÂÖÉË≥áÊñôÈÄ≤Ë°åÊé®Ë´ñ‰∏¶ÂêàÊàêÂèØÂü∑Ë°åÁ®ãÂºèÁ¢ºÔºåËÄå‰∏çÊúÉÂ≠òÂèñÂéüÂßãÁóÖ‰∫∫Ë≥áÊñô„ÄÇÈÄôÁ¢∫‰øù‰∫ÜÁµêÊûúÊó¢ÂèØÈáçÁèæÂèàÂèØËß£ÈáãÔºåÂêåÊôÇÁ∂≠Ë≠∑ÁóÖ‰∫∫ÁöÑÈö±ÁßÅ„ÄÇË©≤Á≥ªÁµ±Êé°Áî®ÂàÜÊÆµÂíåÂåÖË£ùË®≠Ë®àÔºåÊé°Áî®ÊôÇÈñìÊë∫Áñä‰æÜÂª∫Á´ãÁñäÂä†ÁöÑÂúìÂΩ¢Ë¶ñË¶∫Âåñ„ÄÇÁµêÂêàÁ∑öÊÄßÈï∑Ê¢ùÂúñÔºåÊ≠§Ë®≠Ë®àÊúâÂä©ÊñºÊé¢Á¥¢‰∏çË¶èÂâáÊ∏¨ÈáèË°ÄÂ£ìË≥áÊñô‰∏≠ÁöÑÊúâÊÑèÁæ©Ê®°Âºè„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåPhenoFlow Â∑≤Ë≠âÊòéÂÖ∂ÊîØÊè¥Â∞çÂª£Ê≥õËá®Â∫äË≥áÊñôÈõÜÈÄ≤Ë°åÂèçË¶ÜÂàÜÊûêÁöÑËÉΩÂäõÔºåÈôç‰ΩéË™çÁü•Ë≤†Êìî‰∏¶‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ª•ËàáÈ†òÂüüÂ∞àÂÆ∂Èï∑ÊúüÂêà‰ΩúÁÇ∫Âü∫Á§éÔºåË≠âÊòé‰∫ÜÂà©Áî® LLM ‰æÜÊáâÂ∞çÁï∂ÂâçÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖË≥áÊñôÈ©ÖÂãïËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Graph-Structured Speculative Decoding**
2407.16207v1 by Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan

Speculative decoding has emerged as a promising technique to accelerate the
inference of Large Language Models (LLMs) by employing a small language model
to draft a hypothesis sequence, which is then validated by the LLM. The
effectiveness of this approach heavily relies on the balance between
performance and efficiency of the draft model. In our research, we focus on
enhancing the proportion of draft tokens that are accepted to the final output
by generating multiple hypotheses instead of just one. This allows the LLM more
options to choose from and select the longest sequence that meets its
standards. Our analysis reveals that hypotheses produced by the draft model
share many common token sequences, suggesting a potential for optimizing
computation. Leveraging this observation, we introduce an innovative approach
utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This
structure enables us to efficiently predict and merge recurring token
sequences, vastly reducing the computational demands of the draft model. We
term this approach Graph-structured Speculative Decoding (GSD). We apply GSD
across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and
observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly
surpassing standard speculative decoding.

ÊëòË¶ÅÔºö<paragraph>Êé®Ê∏¨ÊÄßËß£Á¢ºÂ∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊäÄË°ìÔºåÂèØÈÄöÈÅé‰ΩøÁî®Â∞èÂûãË™ûË®ÄÊ®°ÂûãËµ∑ËçâÂÅáË®≠Â∫èÂàóÔºåÁÑ∂ÂæåÁî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È©óË≠âË©≤Â∫èÂàóÔºåÂæûËÄåÂä†ÈÄüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜ„ÄÇÊ≠§ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºËçâÁ®øÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéá‰πãÈñìÁöÑÂπ≥Ë°°„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÈÄöÈÅéÁîüÊàêÂ§öÂÄãÂÅáË®≠ËÄå‰∏çÊòØÂè™ÁîüÊàê‰∏ÄÂÄãÂÅáË®≠‰æÜÊèêÈ´òË¢´Êé•ÂèóÁÇ∫ÊúÄÁµÇËº∏Âá∫ÁöÑËçâÁ®ø‰ª§ÁâåÁöÑÊØî‰æã„ÄÇÈÄôÂÖÅË®± LLM Âæû‰∏≠ÈÅ∏ÊìáÊõ¥Â§öÈÅ∏È†ÖÔºå‰∏¶ÈÅ∏ÊìáÁ¨¶ÂêàÂÖ∂Ê®ôÊ∫ñÁöÑÊúÄÈï∑Â∫èÂàó„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË°®ÊòéÔºåËçâÁ®øÊ®°ÂûãÁî¢ÁîüÁöÑÂÅáË®≠ÂÖ±‰∫´Ë®±Â§öÂÖ¨ÂÖ±‰ª§ÁâåÂ∫èÂàóÔºåÈÄôË°®ÊòéÂÑ™ÂåñË®àÁÆóÁöÑÂèØËÉΩÊÄß„ÄÇÂà©Áî®ÈÄô‰∏ÄËßÄÂØüÁµêÊûúÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ‰æÜÁÆ°ÁêÜÂ∑≤Á∑®Âà∂ÁöÑÂÅáË®≠„ÄÇÈÄôÁ®ÆÁµêÊßã‰ΩøÊàëÂÄëËÉΩÂ§†ÊúâÊïàÂú∞È†êÊ∏¨ÂíåÂêà‰ΩµÈáçË§áÁöÑ‰ª§ÁâåÂ∫èÂàóÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËçâÁ®øÊ®°ÂûãÁöÑË®àÁÆóÈúÄÊ±Ç„ÄÇÊàëÂÄëÂ∞áÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫ÂúñÁµêÊßãÊé®Ê∏¨ÊÄßËß£Á¢º (GSD)„ÄÇÊàëÂÄëÂ∞á GSD ÊáâÁî®Êñº‰∏ÄÁ≥ªÂàó LLMÔºåÂåÖÊã¨‰∏ÄÂÄã 700 ÂÑÑÂèÉÊï∏ÁöÑ LLaMA-2 Ê®°ÂûãÔºå‰∏¶ËßÄÂØüÂà∞È°ØËëóÁöÑÂä†ÈÄüÔºåÂæû 1.73 ÂÄçÂà∞ 1.96 ÂÄçÔºåÈ°ØËëóË∂ÖÈÅéÊ®ôÊ∫ñÊé®Ê∏¨ÊÄßËß£Á¢º„ÄÇ</paragraph>

##### **Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**
2407.21049v1 by Yannick Assogba, Donghao Ren

As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools

ÊëòË¶ÅÔºöÈö®ËëóË™ûË®ÄÊ®°ÂûãÊîØÊè¥ÁöÑÂÖßÂÆπÂ§ßÂ∞èË∂ä‰æÜË∂äÂ§ßÔºåË©ï‰º∞ÂÖ∂ÊúâÊïàÂà©Áî®Ë©≤ÂÖßÂÆπÁöÑËÉΩÂäõËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÂπæÂÄãÁ®ãÂºèÁ¢ºÁîüÊàêÊ®°ÂûãËôïÁêÜÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇÁöÑËÉΩÂäõÔºå‰ΩøÁî®‰∏ÄÁµÑÂ§öÊ≠•È©üÈóúÈçµÊ™¢Á¥¢‰ªªÂãôÔºåÂú®Èï∑ÈÅî 8k ‰ª§ÁâåÁöÑÂÖßÂÆπË¶ñÁ™ó‰∏≠„ÄÇ‰ªªÂãôÈÄêÊº∏Â¢ûÂä†Èõ£Â∫¶Ôºå‰∏¶ÂÖÅË®±Â∞çÊ®°ÂûãÂäüËÉΩÈÄ≤Ë°åÊØîÊµÅË°åÁöÑÈáùÈ†≠‰πæËçâÂ†ÜÊ∏¨Ë©¶Êõ¥Á¥∞Á∑ªÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæÔºåÁï∂ÂáΩÂºèÂèÉÁÖßÁ®çÂæåÂú®ÊèêÁ§∫‰∏≠ÂÆöÁæ©ÁöÑÂè¶‰∏ÄÂÄãÂáΩÂºèÊôÇÔºåÊïàËÉΩÊúÉÈ°ØËëó‰∏ãÈôçÔºàÊúÄÂ§ö 2 ÂÄçÔºâ„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞Ôºå‰ΩøÁî®ÊªëÂãïË¶ñÁ™óÊ≥®ÊÑèÊ©üÂà∂ÁöÑÊ®°ÂûãÈõ£‰ª•ËôïÁêÜË∂ÖÂá∫ÂñÆ‰∏ÄË¶ñÁ™óÂ§ßÂ∞èÁöÑÂèÉÁÖß„ÄÇÊàëÂÄë‰ΩøÁî®ÂëºÂè´ÂúñÂΩ¢Ë≥áË®äÂü∑Ë°åÁ∞°ÂñÆÁöÑÊèêÁ§∫‰øÆÊîπÔºå‰ª•Â∞áÂ§öÊ≠•È©üÊ™¢Á¥¢ÊïàËÉΩÊèêÂçáËá≥ 3 ÂÄç„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÁ™ÅÈ°Ø‰∫ÜÈï∑ÂÖßÂÆπÊïàËÉΩÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰∏¶ÊöóÁ§∫‰∫ÜÁ®ãÂºèÁ¢ºÂÆåÊàêÂ∑•ÂÖ∑ÁöÑÊèêÁ§∫Âª∫ÊßãÁ≠ñÁï•

##### **Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**
2407.16127v1 by Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu

Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.

ÊëòË¶ÅÔºöÂÇ≥Áµ±Áü•Ë≠òÂúñË≠úÔºàKGÔºâÂÆåÊàêÂäüËÉΩÊ®°ÂûãÂ≠∏ÁøíÂµåÂÖ•Ôºå‰ª•È†êÊ∏¨ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊúÄËøëÁöÑÂ∑•‰ΩúÂòóË©¶‰ª•Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª•ÊñáÂ≠óÁîüÊàêÁöÑÊñπÂºèÂÆåÊàê KG„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄëÈúÄË¶ÅÂ∞á LLM ÁöÑËº∏Âá∫Âü∫Á§éÂª∫Á´ãÂú® KG ÂØ¶È´î‰∏äÔºåÈÄô‰∏çÂèØÈÅøÂÖçÂú∞ÊúÉÂ∏∂‰æÜÈåØË™§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂæÆË™øÊ°ÜÊû∂ DIFTÔºåÊó®Âú®ÈáãÊîæ LLM ÁöÑ KG ÂÆåÊàêÂäüËÉΩÔºå‰∏¶ÈÅøÂÖçÂü∫Á§éÈåØË™§„ÄÇÁµ¶ÂÆö‰∏ÄÂÄã‰∏çÂÆåÊï¥ÁöÑ‰∫ãÂØ¶ÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãËºïÈáèÁ¥öÊ®°Âûã‰æÜÁç≤ÂæóÂÄôÈÅ∏ÂØ¶È´îÔºå‰∏¶ÂæÆË™ø‰∏ÄÂÄã LLMÔºå‰∏¶‰ΩøÁî®Ëæ®Âà•Êåá‰ª§ÂæûÁµ¶ÂÆöÁöÑÂÄôÈÅ∏È†Ö‰∏≠ÈÅ∏ÊìáÊ≠£Á¢∫ÁöÑÂØ¶È´î„ÄÇÁÇ∫‰∫ÜÂú®Ê∏õÂ∞ëÊåá‰ª§Êï∏ÊìöÁöÑÂêåÊôÇÊèêÂçáÊïàËÉΩÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãÊà™Êñ∑ÊäΩÊ®£ÊñπÊ≥ï‰æÜÈÅ∏ÊìáÊúâÁî®ÁöÑ‰∫ãÂØ¶‰ª•ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶Â∞á KG ÂµåÂÖ•Ê≥®ÂÖ•Âà∞ LLM ‰∏≠„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**
2407.15588v1 by Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee

Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge.Existing methods, mostly
supervised, face challenges in obtaining labeled entity pairs. To address this,
recent studies have shifted towards a self-supervised and unsupervised
frameworks. Despite their effectiveness, these approaches have limitations: (1)
they mainly focus on entity features, neglecting the semantic information of
relations, (2) they assume isomorphism between source and target graphs,
leading to noise and reduced alignment accuracy, and (3) they are susceptible
to noise in the textual features, especially when encountering inconsistent
translations or Out-Of-Vocabulary (OOV) problems.
  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual
EA framework that jointly performs Entity-level and Relation-level Alignment
using semantic textual features of relations and entities. Its refinement
process iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification
process examines the entities' neighbor triples as the linearized text. This
\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,
achieving near-perfect alignment even in the presence of noisy textual features
of entities. Our extensive experiments demonstrate that robustness and general
applicability of \proposed improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.

ÊëòË¶ÅÔºöË∑®Ë™ûË®ÄÂØ¶È´îÂ∞çÈΩä (EA) ËÉΩÂ§†Êï¥Âêà‰∏çÂêåË™ûË®Ä‰∏≠ÁöÑÂ§öÂÄãÁü•Ë≠òÂúñË≠ú (KG)ÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÁÑ°Á∏´Âú∞Â≠òÂèñÂ§öÂÖÉ‰∏îÂÖ®Èù¢ÁöÑÁü•Ë≠ò„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÊòØÊúâÁõ£Áù£ÁöÑÔºåÂú®ÂèñÂæóÊ®ôË®òÂØ¶È´îÂ∞çÊôÇÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤ËΩâÂêëËá™Áõ£Áù£ÂíåÁÑ°Áõ£Áù£ÁöÑÊû∂Êßã„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÊúâ‰ª•‰∏ãÈôêÂà∂Ôºö(1) ÂÆÉÂÄë‰∏ªË¶ÅÈóúÊ≥®ÂØ¶È´îÁâπÂæµÔºåÂøΩÁï•Èóú‰øÇÁöÑË™ûÁæ©Ë≥áË®äÔºå(2) ÂÆÉÂÄëÂÅáË®≠‰æÜÊ∫êÂúñË≠úÂíåÁõÆÊ®ôÂúñË≠ú‰πãÈñìÂêåÊßãÔºåÂ∞éËá¥ÈõúË®äÂíåÂ∞çÈΩäÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéÔºå(3) ÂÆÉÂÄëÂÆπÊòìÂèóÂà∞ÊñáÂ≠óÁâπÂæµ‰∏≠ÁöÑÈõúË®äÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂú®ÈÅáÂà∞‰∏ç‰∏ÄËá¥ÁöÑÁøªË≠ØÊàñË©ûÂΩôÂ§ñÂïèÈ°å (OOV) ÊôÇ„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ ERAlignÔºå‰∏ÄÂÄãÁÑ°Áõ£Áù£‰∏îÁ©©ÂÅ•ÁöÑË∑®Ë™ûË®Ä EA Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî®Èóú‰øÇÂíåÂØ¶È´îÁöÑË™ûÁæ©ÊñáÂ≠óÁâπÂæµÔºåÂêåÊôÇÂü∑Ë°åÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩä„ÄÇÂÆÉÁöÑÁ≤æÁÖâÁ®ãÂ∫èÈÄèÈÅéÊ†πÊìöÈÑ∞Êé•‰∏âÂÖÉÁµÑÂåπÈÖçËûçÂêàÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩäÔºåÂèçË¶ÜÂ¢ûÂº∑ÁµêÊûú„ÄÇÈ°çÂ§ñÁöÑÈ©óË≠âÁ®ãÂ∫èÂ∞áÂØ¶È´îÁöÑÈÑ∞Êé•‰∏âÂÖÉÁµÑË¶ñÁÇ∫Á∑öÊÄßÂåñÊñáÂ≠óÈÄ≤Ë°åÊ™¢Êü•„ÄÇÈÄôÂÄãÂö¥Ê†ºË©ï‰º∞Â∞çÈΩäÁµêÊûúÁöÑ„ÄåÂ∞çÈΩäÂíåÈ©óË≠â„ÄçÁÆ°Á∑öÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÂØ¶È´îÁöÑÈõúË®äÊñáÂ≠óÁâπÂæµÊôÇ‰πüËÉΩÈÅîÊàêËøë‰πéÂÆåÁæéÁöÑÂ∞çÈΩä„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºå\proposed ÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÈÅ©Áî®ÊÄßÊèêÂçá‰∫Ü EA ‰ªªÂãôÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊúâÊïàÊÄßÔºåÂ∞çÁü•Ë≠òÂ∞éÂêëÊáâÁî®Á®ãÂºèÊúâÈ°ØËëóÁöÑË≤¢Áçª„ÄÇ

##### **Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**
2407.15431v1 by Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang

The text-attributed graph (TAG) is one kind of important real-world
graph-structured data with each node associated with raw texts. For TAGs,
traditional few-shot node classification methods directly conduct training on
the pre-processed node features and do not consider the raw texts. The
performance is highly dependent on the choice of the feature pre-processing
method. In this paper, we propose P2TAG, a framework designed for few-shot node
classification on TAGs with graph pre-training and prompting. P2TAG first
pre-trains the language model (LM) and graph neural network (GNN) on TAGs with
self-supervised loss. To fully utilize the ability of language models, we adapt
the masked language modeling objective for our framework. The pre-trained model
is then used for the few-shot node classification with a mixed prompt method,
which simultaneously considers both text and graph information. We conduct
experiments on six real-world TAGs, including paper citation networks and
product co-purchasing networks. Experimental results demonstrate that our
proposed framework outperforms existing graph few-shot learning methods on
these datasets with +18.98% ~ +35.98% improvements.

ÊëòË¶ÅÔºöÊñáÊú¨Â±ûÊÄßÂõæ (TAG) ÊòØ‰∏ÄÁßçÈáçË¶ÅÁöÑÁúüÂÆû‰∏ñÁïåÂõæÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂÖ∂‰∏≠ÊØè‰∏™ËäÇÁÇπÈÉΩ‰∏éÂéüÂßãÊñáÊú¨Áõ∏ÂÖ≥ËÅî„ÄÇÂØπ‰∫é TAGÔºå‰º†ÁªüÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÊñπÊ≥ïÁõ¥Êé•ÂØπÈ¢ÑÂ§ÑÁêÜÁöÑËäÇÁÇπÁâπÂæÅËøõË°åËÆ≠ÁªÉÔºåËÄå‰∏çËÄÉËôëÂéüÂßãÊñáÊú¨„ÄÇÊÄßËÉΩÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÁâπÂæÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ïÁöÑÈÄâÊã©„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü P2TAGÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ TAG ‰∏äÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªËÆæËÆ°ÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÂõæÈ¢ÑËÆ≠ÁªÉÂíåÊèêÁ§∫„ÄÇP2TAG È¶ñÂÖà‰ΩøÁî®Ëá™ÊàëÁõëÁù£ÊçüÂ§±ÂØπ TAG ‰∏äÁöÑËØ≠Ë®ÄÊ®°Âûã (LM) ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏∫Êàë‰ª¨ÁöÑÊ°ÜÊû∂Ë∞ÉÊï¥‰∫ÜÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÁõÆÊ†á„ÄÇÁÑ∂Âêé‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÔºåÈááÁî®Ê∑∑ÂêàÊèêÁ§∫ÊñπÊ≥ïÔºåÂêåÊó∂ËÄÉËôëÊñáÊú¨ÂíåÂõæ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÂØπÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ TAG ËøõË°å‰∫ÜÂÆûÈ™åÔºåÂåÖÊã¨ËÆ∫ÊñáÂºïÁî®ÁΩëÁªúÂíå‰∫ßÂìÅÂÖ±ÂêåË¥≠‰π∞ÁΩëÁªú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÂ∞ëÊï∞ÈïúÂ§¥Â≠¶‰π†ÊñπÊ≥ïÔºåÊîπËøõ‰∫Ü +18.98% ~ +35.98%„ÄÇ

##### **LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**
2407.15351v2 by Jiaxing Zhang, Jiayi Liu, Dongsheng Luo, Jennifer Neville, Hua Wei

Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.

ÊëòË¶ÅÔºöËøëÊúüÁ†îÁ©∂Ë©¶ÂúñÈÄèÈÅéÂ§öÁ®ÆÈùûÁõ£Áù£ÂºèÂ≠∏ÁøíÊ®°Âûã‰æÜÊèê‰æõÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÂèØËß£ÈáãÊÄß„ÄÇÁî±ÊñºË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÔºåÁõÆÂâçÁöÑÊºîÁÆóÊ≥ïÂÆπÊòìÂèóÂà∞Â≠∏ÁøíÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Áü•Ë≠òÂµåÂÖ•Âà∞ GNN Ëß£ÈáãÁ∂≤Ë∑Ø‰∏≠Ôºå‰ª•ÈÅøÂÖçÂ≠∏ÁøíÂÅèÂ∑ÆÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ∞á LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ (BI) Ê®°ÁµÑÊ≥®ÂÖ•Ôºå‰ª•Ê∏õËºïÂ≠∏ÁøíÂÅèÂ∑Æ„ÄÇBI Ê®°ÁµÑÁöÑÊïàËÉΩÂ∑≤Âú®ÁêÜË´ñ‰∏äÂíåÂØ¶È©ó‰∏äÂæóÂà∞Ë≠âÂØ¶„ÄÇÊàëÂÄëÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄëÂ∑•‰ΩúÁöÑÂâµÊñ∞‰πãËôïÂú®ÊñºÂÖ©ÈÉ®ÂàÜÔºö1. ÊàëÂÄëÊèê‰æõ LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ‰ª•ÊîπÂñÑÁèæÊúâÊºîÁÆóÊ≥ïÊïàËÉΩÁöÑÂèØËÉΩÊÄß‰πãÊñ∞ËßÄÈªûÔºõ2. ÊàëÂÄëÁéáÂÖàË®éË´ñ GNN Ëß£ÈáãÂïèÈ°å‰∏≠ÁöÑÂ≠∏ÁøíÂÅèÂ∑ÆÂïèÈ°å„ÄÇ

##### **Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**
2407.15141v1 by Yu Zhang, Ruijie Yu, Kaipeng Zeng, Ding Li, Feng Zhu, Xiaokang Yang, Yaohui Jin, Yanyan Xu

High-throughput reaction condition (RC) screening is fundamental to chemical
synthesis. However, current RC screening suffers from laborious and costly
trial-and-error workflows. Traditional computer-aided synthesis planning (CASP)
tools fail to find suitable RCs due to data sparsity and inadequate reaction
representations. Nowadays, large language models (LLMs) are capable of tackling
chemistry-related problems, such as molecule design, and chemical logic Q\&A
tasks. However, LLMs have not yet achieved accurate predictions of chemical
reaction conditions. Here, we present MM-RCR, a text-augmented multimodal LLM
that learns a unified reaction representation from SMILES, reaction graphs, and
textual corpus for chemical reaction recommendation (RCR). To train MM-RCR, we
construct 1.2 million pair-wised Q\&A instruction datasets. Our experimental
results demonstrate that MM-RCR achieves state-of-the-art performance on two
open benchmark datasets and exhibits strong generalization capabilities on
out-of-domain (OOD) and High-Throughput Experimentation (HTE) datasets. MM-RCR
has the potential to accelerate high-throughput condition screening in chemical
synthesis.

ÊëòË¶ÅÔºöÈ´òÈÄöÈáèÂèçÊáâÊ¢ù‰ª∂ (RC) ÁØ©ÈÅ∏ÊòØÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÂü∫Á§é„ÄÇÁÑ∂ËÄåÔºåÁï∂ÂâçÁöÑ RC ÁØ©ÈÅ∏ÊúÉÈÅáÂà∞ÁπÅÁë£‰∏îÊòÇË≤¥ÁöÑË©¶ÈåØÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂÇ≥Áµ±ÁöÑÈõªËÖ¶ËºîÂä©ÂêàÊàêË¶èÂäÉ (CASP) Â∑•ÂÖ∑ÁÑ°Ê≥ïÊâæÂà∞ÂêàÈÅ©ÁöÑ RCÔºåÈÄôÊòØÂõ†ÁÇ∫Ë≥áÊñôÁ®ÄÁñè‰∏îÂèçÊáâË°®Á§∫‰∏çË∂≥„ÄÇÂ¶Ç‰ªäÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂ§†Ëß£Ê±∫ËàáÂåñÂ≠∏Áõ∏ÈóúÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂàÜÂ≠êË®≠Ë®àÂíåÂåñÂ≠∏ÈÇèËºØÂïèÁ≠î‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåLLM Â∞öÊú™ÈÅîÊàêÂåñÂ≠∏ÂèçÊáâÊ¢ù‰ª∂ÁöÑÊ∫ñÁ¢∫È†êÊ∏¨„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ MM-RCRÔºå‰∏ÄÂÄãÊñáÊú¨Â¢ûÂº∑ÁöÑÂ§öÊ®°ÊÖã LLMÔºåÂÆÉÂæû SMILES„ÄÅÂèçÊáâÂúñÂíåÊñáÊú¨Ë™ûÊñôÂ∫´Â≠∏ÁøíÁµ±‰∏ÄÁöÑÂèçÊáâË°®Á§∫Ôºå‰ª•ÈÄ≤Ë°åÂåñÂ≠∏ÂèçÊáâÊé®Ëñ¶ (RCR)„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥ MM-RCRÔºåÊàëÂÄëÂª∫Êßã‰∫Ü 120 Ëê¨Â∞çÈÖçÂ∞çÁöÑÂïèÁ≠îÊåá‰ª§Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåMM-RCR Âú®ÂÖ©ÂÄãÈñãÊîæÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶Âú®È†òÂüüÂ§ñ (OOD) ÂíåÈ´òÈÄöÈáèÂØ¶È©ó (HTE) Ë≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇMM-RCR ÊúâÂèØËÉΩÂä†ÈÄüÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÈ´òÈÄöÈáèÊ¢ù‰ª∂ÁØ©ÈÅ∏„ÄÇ

##### **On the Design and Analysis of LLM-Based Algorithms**
2407.14788v1 by Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou

We initiate a formal investigation into the design and analysis of LLM-based
algorithms, i.e. algorithms that contain one or multiple calls of large
language models (LLMs) as sub-routines and critically rely on the capabilities
of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt
engineering to complicated LLM-powered agent systems and compound AI systems,
have achieved remarkable empirical success, the design and optimization of them
have mostly relied on heuristics and trial-and-errors, which is largely due to
a lack of formal and analytical study for these algorithms. To fill this gap,
we start by identifying the computational-graph representation of LLM-based
algorithms, the design principle of task decomposition, and some key
abstractions, which then facilitate our formal analysis for the accuracy and
efficiency of LLM-based algorithms, despite the black-box nature of LLMs. We
further consider parallel decomposition for a case study, providing extensive
analytical and empirical study for four concrete examples of this pattern. Our
proposed framework holds promise for advancing LLM-based algorithms, by
revealing the reasons behind curious empirical phenomena, guiding the choices
of hyperparameters, predicting the empirical performance of algorithms, and
inspiring new algorithm design. To promote further study of LLM-based
algorithms, we release our source code at
https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®≠Ë®àÂíåÂàÜÊûêÂ±ïÈñãÊ≠£ÂºèË™øÊü•ÔºåÂç≥ÂåÖÂê´‰∏ÄÂÄãÊàñÂ§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Â≠êÂ∏∏ÂºèÂëºÂè´ÁöÑÊºîÁÆóÊ≥ïÔºå‰∏¶Ê•µÂ∫¶‰æùË≥¥ LLM ÁöÑÂäüËÉΩ„ÄÇÂÑòÁÆ°Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÂæûÂ∏∂ÊèêÁ§∫Â∑•Á®ãÁöÑÂü∫Êú¨ LLM ÂëºÂè´Âà∞Ë§áÈõúÁöÑ LLM È©ÖÂãïÁöÑ‰ª£ÁêÜÁ≥ªÁµ±ÂíåË§áÂêàÂºè AI Á≥ªÁµ±ÔºåÂ∑≤ÂèñÂæóÈ°ØËëóÁöÑÂØ¶Ë≠âÊàêÂäüÔºå‰ΩÜÂÖ∂Ë®≠Ë®àÂíåÊúÄ‰Ω≥ÂåñÂ§ßÂ§ö‰æùË≥¥Ë©¶È©óÊ≥ïÂíåÈåØË™§ÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂõ†ÁÇ∫Áº∫‰πèÂ∞çÈÄô‰∫õÊºîÁÆóÊ≥ïÁöÑÊ≠£ÂºèÂíåÂàÜÊûêÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÂæûË≠òÂà•Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®àÁÆóÂúñË°®Á§∫„ÄÅ‰ªªÂãôÂàÜËß£ÁöÑË®≠Ë®àÂéüÂâáÔºå‰ª•Âèä‰∏Ä‰∫õÈóúÈçµÊäΩË±°ÂåñÈñãÂßãÔºåÁÑ∂Âæå‰øÉÈÄ≤ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÈÄ≤Ë°åÊ≠£ÂºèÂàÜÊûêÔºåÂÑòÁÆ° LLM Êú¨Ë∫´ÂÖ∑ÊúâÈªëÁõíÁâπÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ËÄÉÊÖÆ‰∏¶Ë°åÂàÜËß£‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÁÇ∫Ê≠§Ê®°ÂºèÁöÑÂõõÂÄãÂÖ∑È´îÁØÑ‰æãÊèê‰æõÂª£Ê≥õÁöÑÂàÜÊûêÂíåÂØ¶Ë≠âÁ†îÁ©∂„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÊúâÊúõÊé®ÈÄ≤Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÊñπÊ≥ïÊòØÊè≠Á§∫Â•áÊÄ™ÁöÑÂØ¶Ë≠âÁèæË±°ËÉåÂæåÁöÑÂéüÂõ†„ÄÅÊåáÂ∞éË∂ÖÂèÉÊï∏ÁöÑÈÅ∏Êìá„ÄÅÈ†êÊ∏¨ÊºîÁÆóÊ≥ïÁöÑÂØ¶Ë≠âÊïàËÉΩÔºå‰∏¶ÊøÄÁôºÊñ∞ÁöÑÊºîÁÆóÊ≥ïË®≠Ë®à„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Â∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàëÂÄëÂú® https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm/ ÁôºÂ∏ÉÊàëÂÄëÁöÑÂéüÂßãÁ¢º„ÄÇ</paragraph>

##### **LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**
2407.18269v1 by Chen-Chia Chang, Yikang Shan, Shaoze Fan, Jing Li, Shun Zhang, Ningyuan Cao, Yiran Chen, Xin Zhang

In the realm of electronic and electrical engineering, automation of analog
circuit is increasingly vital given the complexity and customized requirements
of modern applications. However, existing methods only develop search-based
algorithms that require many simulation iterations to design a custom circuit
topology, which is usually a time-consuming process. To this end, we introduce
LaMAGIC, a pioneering language model-based topology generation model that
leverages supervised finetuning for automated analog circuit design. LaMAGIC
can efficiently generate an optimized circuit design from the custom
specification in a single pass. Our approach involves a meticulous development
and analysis of various input and output formulations for circuit. These
formulations can ensure canonical representations of circuits and align with
the autoregressive nature of LMs to effectively addressing the challenges of
representing analog circuits as graphs. The experimental results show that
LaMAGIC achieves a success rate of up to 96\% under a strict tolerance of 0.01.
We also examine the scalability and adaptability of LaMAGIC, specifically
testing its performance on more complex circuits. Our findings reveal the
enhanced effectiveness of our adjacency matrix-based circuit formulation with
floating-point input, suggesting its suitability for handling intricate circuit
designs. This research not only demonstrates the potential of language models
in graph generation, but also builds a foundational framework for future
explorations in automated analog circuit design.

ÊëòË¶ÅÔºöÂú®ÈõªÂ≠êÂíåÈõªÊ∞£Â∑•Á®ãÈ†òÂüü‰∏≠ÔºåËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË∂ä‰æÜË∂äÈáçË¶ÅÔºåÂõ†ÁÇ∫Áèæ‰ª£ÊáâÁî®Á®ãÂºèÂÖ∑ÊúâË§áÈõú‰∏îÂÆ¢Ë£ΩÂåñÁöÑÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÂÉÖÈñãÁôºÂü∫ÊñºÊêúÂ∞ãÁöÑÊºîÁÆóÊ≥ïÔºåÈúÄË¶ÅË®±Â§öÊ®°Êì¨ÂèçË¶ÜÈÅãÁÆóÊâçËÉΩË®≠Ë®àÂÆ¢Ë£ΩÂåñÈõªË∑ØÊãìÊí≤ÔºåÈÄôÈÄöÂ∏∏ÊòØ‰∏ÄÂÄãËÄóÊôÇÁöÑÈÅéÁ®ã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü LaMAGICÔºå‰∏ÄÂÄãÂü∫ÊñºÂÖàÈ©ÖË™ûË®ÄÊ®°ÂûãÁöÑÊãìÊí≤ÁîüÊàêÊ®°ÂûãÔºåÂÆÉÂà©Áî®Áõ£Áù£ÂæÆË™øÈÄ≤Ë°åËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à„ÄÇLaMAGIC ÂèØ‰ª•ÊúâÊïàÁéáÂú∞ÂæûÂÆ¢Ë£ΩÂåñË¶èÊ†º‰∏≠ÁîüÊàêÊúÄ‰Ω≥ÂåñÁöÑÈõªË∑ØË®≠Ë®àÔºåÂè™ÈúÄ‰∏ÄÊ¨°ÈÄöÈÅé„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰ªîÁ¥∞ÈñãÁôºÂíåÂàÜÊûêÈõªË∑ØÁöÑÂêÑÁ®ÆËº∏ÂÖ•ÂíåËº∏Âá∫ÂÖ¨Âºè„ÄÇÈÄô‰∫õÂÖ¨ÂºèÂèØ‰ª•Á¢∫‰øùÈõªË∑ØÁöÑÊ®ôÊ∫ñË°®Á§∫Ôºå‰∏¶Ëàá LM ÁöÑËá™Ëø¥Ê≠∏ÊÄßË≥™‰øùÊåÅ‰∏ÄËá¥Ôºå‰ª•ÊúâÊïàËß£Ê±∫Â∞áÈ°ûÊØîÈõªË∑ØË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁöÑÊåëÊà∞„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåLaMAGIC Âú® 0.01 ÁöÑÂö¥Ê†ºÂÆπÂ∑Æ‰∏ãÂØ¶Áèæ‰∫ÜÈ´òÈÅî 96% ÁöÑÊàêÂäüÁéá„ÄÇÊàëÂÄëÈÇÑÊ™¢Êü•‰∫Ü LaMAGIC ÁöÑÂèØÊì¥ÂÖÖÊÄßÂíåÈÅ©ÊáâÊÄßÔºåÁâπÂà•ÊòØÊ∏¨Ë©¶‰∫ÜÂÆÉÂú®Êõ¥Ë§áÈõúÈõªË∑Ø‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÊàëÂÄëÂü∫ÊñºÈÑ∞Êé•Áü©Èô£ÁöÑÈõªË∑ØÂÖ¨ÂºèËàáÊµÆÈªûËº∏ÂÖ•ÁöÑÂ¢ûÂº∑ÊïàËÉΩÔºåË°®ÊòéÂÆÉÈÅ©Áî®ÊñºËôïÁêÜË§áÈõúÁöÑÈõªË∑ØË®≠Ë®à„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜË™ûË®ÄÊ®°ÂûãÂú®ÂúñÂΩ¢ÁîüÊàê‰∏≠ÁöÑÊΩõÂäõÔºå‰πüÁÇ∫Êú™‰æÜÂú®Ëá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à‰∏≠ÁöÑÊé¢Á¥¢Âª∫Á´ã‰∫ÜÂü∫Á§éÊ°ÜÊû∂„ÄÇ

##### **Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**
2407.14224v1 by Suvajit Patra, Arkadip Maitra, Megha Tiwari, K. Kumaran, Swathy Prabhu, Swami Punyeshwarananda, Soumitra Samanta

Automatic Sign Language (SL) recognition is an important task in the computer
vision community. To build a robust SL recognition system, we need a
considerable amount of data which is lacking particularly in Indian sign
language (ISL). In this paper, we propose a large-scale isolated ISL dataset
and a novel SL recognition model based on skeleton graph structure. The dataset
covers 2,002 daily used common words in the deaf community recorded by 20 (10
male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL
recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)
by utilizing the human upper body skeleton graph structure. The HWGAT tries to
capture distinctive motions by giving attention to different body parts induced
by the human skeleton graph structure. The utility of the proposed dataset and
the usefulness of our model are evaluated through extensive experiments. We
pre-trained the proposed model on the proposed dataset and fine-tuned it across
different sign language datasets further boosting the performance of 1.10,
0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL
respectively compared to the existing state-of-the-art skeleton-based models.

ÊëòË¶ÅÔºöËá™ÂãïÊâãË™û (SL) Ë≠òÂà•ÊòØÈõªËÖ¶Ë¶ñË¶∫Á§æÁæ§‰∏≠ÁöÑÈáçË¶Å‰ªªÂãô„ÄÇË¶ÅÂª∫Á´ãÂº∑ÂÅ•ÁöÑ SL Ë≠òÂà•Á≥ªÁµ±ÔºåÊàëÂÄëÈúÄË¶ÅÂ§ßÈáèÁöÑË≥áÊñôÔºåËÄåÈÄôÂú®Âç∞Â∫¶ÊâãË™û (ISL) ‰∏≠ÁâπÂà•Áº∫‰πè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ§ßË¶èÊ®°ÁöÑÂ≠§Á´ã ISL Ë≥áÊñôÈõÜÔºå‰ª•Âèä‰∏ÄÂÄãÂü∫ÊñºÈ™®Êû∂ÂúñÁµêÊßãÁöÑÊñ∞Âûã SL Ë≠òÂà•Ê®°Âûã„ÄÇË©≤Ë≥áÊñôÈõÜÊ∂µËìã 2,002 ÂÄãËÅæÂïûÁ§æÁæ§‰∏≠Â∏∏Áî®ÁöÑÊó•Â∏∏ÂñÆÂ≠óÔºåÁî± 20 ‰Ωç (10 Áî∑ 10 Â•≥) ËÅæÂïûÊàê‰∫∫ÊâãË™ûËÄÖÈåÑË£ΩÔºàÂåÖÂê´ 40033 ÈÉ®ÂΩ±ÁâáÔºâ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã SL Ë≠òÂà•Ê®°ÂûãÔºåÂç≥ÂàÜÂ±§Ë¶ñÁ™óÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (HWGAT)ÔºåÂà©Áî®‰∫∫È´î‰∏äÂçäË∫´È™®Êû∂ÂúñÁµêÊßã„ÄÇHWGAT ÂòóË©¶ÈÄèÈÅéÈóúÊ≥®Áî±‰∫∫È´îÈ™®Êû∂ÂúñÁµêÊßãË™òÂ∞éÁöÑ‰∏çÂêåË∫´È´îÈÉ®‰Ωç‰æÜÊçïÊçâÁç®ÁâπÁöÑÂãï‰Ωú„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜÁöÑÊïàÁî®ÂíåÊàëÂÄëÊ®°ÂûãÁöÑÊúâÁî®ÊÄß„ÄÇÊàëÂÄëÂú®ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜ‰∏äÈ†êË®ìÁ∑¥ÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºå‰∏¶Âú®‰∏çÂêåÁöÑÊâãË™ûË≥áÊñôÈõÜ‰∏äÂæÆË™øÂÆÉÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰∫Ü INCLUDE„ÄÅLSA64„ÄÅAUTSL Âíå WLASL ‰∏ä 1.10„ÄÅ0.46„ÄÅ0.78 Âíå 6.84 ÂÄãÁôæÂàÜÈªûÁöÑÊïàËÉΩÔºåÂàÜÂà•ËàáÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÈ™®Êû∂ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇ

##### **Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**
2407.13989v1 by Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang

Graphs have emerged as critical data structures for content analysis in
various domains, such as social network analysis, bioinformatics, and
recommendation systems. Node classification, a fundamental task in this
context, is typically tackled using graph neural networks (GNNs).
Unfortunately, conventional GNNs still face challenges in scenarios with few
labeled nodes, despite the prevalence of few-shot node classification tasks in
real-world applications. To address this challenge, various approaches have
been proposed, including graph meta-learning, transfer learning, and methods
based on Large Language Models (LLMs). However, traditional meta-learning and
transfer learning methods often require prior knowledge from base classes or
fail to exploit the potential advantages of unlabeled nodes. Meanwhile,
LLM-based methods may overlook the zero-shot capabilities of LLMs and rely
heavily on the quality of generated contexts. In this paper, we propose a novel
approach that integrates LLMs and GNNs, leveraging the zero-shot inference and
reasoning capabilities of LLMs and employing a Graph-LLM-based active learning
paradigm to enhance GNNs' performance. Extensive experiments demonstrate the
effectiveness of our model in improving node classification accuracy with
considerably limited labeled data, surpassing state-of-the-art baselines by
significant margins.

ÊëòË¶ÅÔºöÂúñË°®Â∑≤ÊàêÁÇ∫ÂêÑÁ®ÆÈ†òÂüü‰∏≠ÂÖßÂÆπÂàÜÊûêÁöÑÈóúÈçµÊï∏ÊìöÁµêÊßãÔºå‰æãÂ¶ÇÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÁîüÁâ©Ë≥áË®äÂ≠∏ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÁØÄÈªûÂàÜÈ°ûÊòØÊ≠§ËÑàÁµ°‰∏≠ÁöÑÂü∫Êú¨‰ªªÂãôÔºåÈÄöÂ∏∏‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ‰æÜËôïÁêÜ„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂÑòÁÆ°ÁèæÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÊôÆÈÅçÂ≠òÂú®Â∞ëÊ®£Êú¨ÁØÄÈªûÂàÜÈ°û‰ªªÂãôÔºå‰ΩÜÂÇ≥Áµ±ÁöÑ GNN Âú®Ê®ôË®òÁØÄÈªûÂæàÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ã‰ªçÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÂ∑≤ÊèêÂá∫ÂêÑÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÂúñÂΩ¢ÂÖÉÂ≠∏Áøí„ÄÅÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂÖÉÂ≠∏ÁøíÂíåÈÅ∑ÁßªÂ≠∏ÁøíÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶Å‰æÜËá™Âü∫Á§éÈ°ûÂà•ÁöÑÂÖàÈ©óÁü•Ë≠òÔºåÊàñËÄÖÁÑ°Ê≥ïÂà©Áî®Êú™Ê®ôË®òÁØÄÈªûÁöÑÊΩõÂú®ÂÑ™Âã¢„ÄÇÂêåÊôÇÔºåÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂèØËÉΩÊúÉÂøΩË¶ñ LLM ÁöÑÈõ∂Ê®£Êú¨ËÉΩÂäõÔºå‰∏¶‰∏îÈÅéÂ∫¶‰æùË≥¥ÁîüÊàêË™ûÂ¢ÉÁöÑÂìÅË≥™„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÊï¥Âêà‰∫Ü LLM Âíå GNNÔºåÂà©Áî® LLM ÁöÑÈõ∂Ê®£Êú¨Êé®Ë´ñÂíåÊé®ÁêÜËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫Êñº Graph-LLM ÁöÑ‰∏ªÂãïÂ≠∏ÁøíÁØÑ‰æã‰æÜÂ¢ûÂº∑ GNN ÁöÑÊïàËÉΩ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîπÈÄ≤ÁØÄÈªûÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË®òÊï∏ÊìöÁõ∏Áï∂ÊúâÈôêÔºåÈ°ØËëóË∂ÖË∂ä‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñ„ÄÇ

##### **A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**
2407.13699v1 by Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, Farshad Navah, Amirmohammad Kazemeini

Recommender Systems (RS) play an integral role in enhancing user experiences
by providing personalized item suggestions. This survey reviews the progress in
RS inclusively from 2017 to 2024, effectively connecting theoretical advances
with practical applications. We explore the development from traditional RS
techniques like content-based and collaborative filtering to advanced methods
involving deep learning, graph-based models, reinforcement learning, and large
language models. We also discuss specialized systems such as context-aware,
review-based, and fairness-aware RS. The primary goal of this survey is to
bridge theory with practice. It addresses challenges across various sectors,
including e-commerce, healthcare, and finance, emphasizing the need for
scalable, real-time, and trustworthy solutions. Through this survey, we promote
stronger partnerships between academic research and industry practices. The
insights offered by this survey aim to guide industry professionals in
optimizing RS deployment and to inspire future research directions, especially
in addressing emerging technological and societal trends

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÈ´îÈ©ó‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤ÔºåÈÄèÈÅéÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÂïÜÂìÅÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖË™øÊü•ÂõûÈ°ß‰∫Ü RS Âú® 2017 Âπ¥Âà∞ 2024 Âπ¥ÈñìÁöÑÈÄ≤Â±ïÔºåÊúâÊïàÂú∞Â∞áÁêÜË´ñÈÄ≤Â±ïËàáÂØ¶ÈöõÊáâÁî®ÈÄ£ÁµêËµ∑‰æÜ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÂæûÂÇ≥Áµ±ÁöÑ RS ÊäÄË°ìÔºå‰æãÂ¶ÇÂü∫ÊñºÂÖßÂÆπÂíåÂçîÂêåÈÅéÊøæÔºåÂà∞Ê∂âÂèäÊ∑±Â∫¶Â≠∏Áøí„ÄÅÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ®°Âûã„ÄÅÂº∑ÂåñÂ≠∏ÁøíÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁ≠âÂÖàÈÄ≤ÊñπÊ≥ïÁöÑÁôºÂ±ï„ÄÇÊàëÂÄë‰πüË®éË´ñ‰∫ÜÂ∞àÈñÄÁöÑÁ≥ªÁµ±Ôºå‰æãÂ¶ÇÊÉÖÂ¢ÉÊÑüÁü•„ÄÅÂü∫ÊñºË©ïË´ñÂíåÂÖ¨Âπ≥ÊÑüÁü•ÁöÑ RS„ÄÇÈÄôÈ†ÖË™øÊü•ÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂ∞áÁêÜË´ñËàáÂØ¶ÂãôÁµêÂêàËµ∑‰æÜ„ÄÇÂÆÉËß£Ê±∫‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑÊåëÊà∞ÔºåÂåÖÊã¨ÈõªÂ≠êÂïÜÂãô„ÄÅÈÜ´ÁôÇ‰øùÂÅ•ÂíåÈáëËûçÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂèØÊì¥ÂÖÖ„ÄÅÂç≥ÊôÇÂíåÂèØ‰ø°Ë≥¥ÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÈúÄÊ±Ç„ÄÇÈÄèÈÅéÈÄôÈ†ÖË™øÊü•ÔºåÊàëÂÄë‰øÉÈÄ≤‰∫ÜÂ≠∏Ë°ìÁ†îÁ©∂ÂíåÁî¢Ê•≠ÂØ¶Âãô‰πãÈñìÊõ¥Âº∑Â§ßÁöÑÂ§•‰º¥Èóú‰øÇ„ÄÇÈÄôÈ†ÖË™øÊü•Êèê‰æõÁöÑË¶ãËß£Êó®Âú®ÂºïÂ∞éÁî¢Ê•≠Â∞àÊ•≠‰∫∫Â£´ÂÑ™Âåñ RS ÈÉ®ÁΩ≤Ôºå‰∏¶ÊøÄÂãµÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÁâπÂà•ÊòØÂú®Ëß£Ê±∫Êñ∞ËààÁöÑÊäÄË°ìÂíåÁ§æÊúÉË∂®Âã¢ÊñπÈù¢„ÄÇ

##### **MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**
2407.18961v2 by Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, Zirui Wang

Recent advances in large language models (LLMs) have increased the demand for
comprehensive benchmarks to evaluate their capabilities as human-like agents.
Existing benchmarks, while useful, often focus on specific application
scenarios, emphasizing task completion but failing to dissect the underlying
skills that drive these outcomes. This lack of granularity makes it difficult
to deeply discern where failures stem from. Additionally, setting up these
environments requires considerable effort, and issues of unreliability and
reproducibility sometimes arise, especially in interactive tasks. To address
these limitations, we introduce the Massive Multitask Agent Understanding
(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need
for complex environment setups. It evaluates models across five domains,
including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine
Learning coding, Contest-level programming and Mathematics, and covers five
essential capabilities: Understanding, Reasoning, Planning, Problem-solving,
and Self-correction. With a total of 20 meticulously designed tasks
encompassing over 3K distinct prompts, MMAU provides a comprehensive framework
for evaluating the strengths and limitations of LLM agents. By testing 18
representative models on MMAU, we provide deep and insightful analyses.
Ultimately, MMAU not only sheds light on the capabilities and limitations of
LLM agents but also enhances the interpretability of their performance.
Datasets and evaluation scripts of MMAU are released at
https://github.com/apple/axlearn/tree/main/docs/research/mmau.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ¢ûÂä†‰∫ÜÂ∞çÂÖ®Èù¢Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÈúÄÊ±ÇÔºå‰ª•Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫È°û‰∫∫‰ª£ÁêÜÁöÑËÉΩÂäõ„ÄÇÁèæÊúâÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÈõñÁÑ∂ÊúâÁî®Ôºå‰ΩÜÈÄöÂ∏∏Â∞àÊ≥®ÊñºÂÖ∑È´îÁöÑÊáâÁî®Â†¥ÊôØÔºåÂº∑Ë™ø‰ªªÂãôÂÆåÊàêÔºå‰ΩÜÊú™ËÉΩÂâñÊûêÈ©ÖÂãïÈÄô‰∫õÁµêÊûúÁöÑÂ∫ïÂ±§ÊäÄËÉΩ„ÄÇÈÄôÁ®ÆÁº∫‰πèÁ≤íÂ∫¶‰ΩøÂæóÈõ£‰ª•Ê∑±ÂÖ•Ëæ®Âà•Â§±ÊïóÁöÑÊ†πÊ∫ê„ÄÇÊ≠§Â§ñÔºåË®≠ÁΩÆÈÄô‰∫õÁí∞Â¢ÉÈúÄË¶ÅÂ§ßÈáèÁöÑÁ≤æÂäõÔºåÊúâÊôÇÊúÉÂá∫Áèæ‰∏çÂèØÈù†ÊÄßÂíåÂèØÈáçË§áÊÄßÁöÑÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®‰∫íÂãï‰ªªÂãô‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§ßË¶èÊ®°Â§ö‰ªªÂãô‰ª£ÁêÜÁêÜËß£ (MMAU) Âü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂÆÉÂÖ∑ÊúâÂÖ®Èù¢ÁöÑÈõ¢Á∑ö‰ªªÂãôÔºåÊ∂àÈô§‰∫ÜÂ∞çË§áÈõúÁí∞Â¢ÉË®≠ÁΩÆÁöÑÈúÄÊ±Ç„ÄÇÂÆÉË∑®Ë∂ä‰∫îÂÄãÈ†òÂüüË©ï‰º∞Ê®°ÂûãÔºåÂåÖÊã¨Â∑•ÂÖ∑‰ΩøÁî®„ÄÅÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ÂïèÁ≠î„ÄÅÊï∏ÊìöÁßëÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíÁ∑®Á¢º„ÄÅÁ´∂Ë≥ΩÁ¥öÁ∑®Á®ãÂíåÊï∏Â≠∏Ôºå‰∏¶Ê∂µËìã‰∫îÈ†ÖÂü∫Êú¨ËÉΩÂäõÔºöÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅË¶èÂäÉ„ÄÅÂïèÈ°åËß£Ê±∫ÂíåËá™ÊàëÁ≥æÊ≠£„ÄÇMMAU Á∏ΩÂÖ±ÂåÖÂê´ 20 È†ÖÁ≤æÂøÉË®≠Ë®àÁöÑ‰ªªÂãôÔºåÊ∂µËìãË∂ÖÈÅé 3K ÂÄã‰∏çÂêåÁöÑÊèêÁ§∫ÔºåÁÇ∫Ë©ï‰º∞ LLM ‰ª£ÁêÜÁöÑÂÑ™Âã¢ÂíåÂ±ÄÈôêÊÄßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊ°ÜÊû∂„ÄÇÈÄöÈÅéÂú® MMAU ‰∏äÊ∏¨Ë©¶ 18 ÂÄã‰ª£Ë°®ÊÄßÊ®°ÂûãÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ËÄåÊúâË¶ãÂú∞ÁöÑÂàÜÊûê„ÄÇÊúÄÁµÇÔºåMMAU ‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ‰ª£ÁêÜÁöÑËÉΩÂäõÂíåÂ±ÄÈôêÊÄßÔºåÈÇÑÂ¢ûÂº∑‰∫ÜÂÖ∂ÊÄßËÉΩÁöÑÂèØËß£ÈáãÊÄß„ÄÇMMAU ÁöÑÊï∏ÊìöÈõÜÂíåË©ï‰º∞ËÖ≥Êú¨Â∑≤ÁôºÂ∏ÉÂú® https://github.com/apple/axlearn/tree/main/docs/research/mmau„ÄÇ

##### **Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**
2407.12725v1 by Ben Yao, Yazhou Zhang, Qiuchi Li, Jing Qin

Elaborating a series of intermediate reasoning steps significantly improves
the ability of large language models (LLMs) to solve complex problems, as such
steps would evoke LLMs to think sequentially. However, human sarcasm
understanding is often considered an intuitive and holistic cognitive process,
in which various linguistic, contextual, and emotional cues are integrated to
form a comprehensive understanding of the speaker's true intention, which is
argued not be limited to a step-by-step reasoning process. To verify this
argument, we introduce a new prompting framework called SarcasmCue, which
contains four prompting strategies, $viz.$ chain of contradiction (CoC), graph
of cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits
LLMs to detect human sarcasm by considering sequential and non-sequential
prompting methods. Through a comprehensive empirical comparison on four
benchmarking datasets, we show that the proposed four prompting methods
outperforms standard IO prompting, CoT and ToT with a considerable margin, and
non-sequential prompting generally outperforms sequential prompting.

ÊëòË¶ÅÔºöÈÄöÈÅéÈó°Ëø∞‰∏ÄÁ≥ªÂàó‰∏≠ÈñìÊé®ÁêÜÊ≠•È©üÔºåÂ§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÁöÑËÉΩÂäõÔºåÂõ†ÁÇ∫ÈÄô‰∫õÊ≠•È©üÊúÉ‰øÉ‰Ωø LLM ÊåâÈ†ÜÂ∫èÊÄùËÄÉ„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÁöÑË´∑Âà∫ÁêÜËß£ÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ÊòØ‰∏ÄÁ®ÆÁõ¥Ë¶∫‰∏îÂÖ®Èù¢ÁöÑË™çÁü•ÈÅéÁ®ãÔºåÂÖ∂‰∏≠ÂêÑÁ®ÆË™ûË®Ä„ÄÅË™ûÂ¢ÉÂíåÊÉÖÁ∑íÁ∑öÁ¥¢Êï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•ÂÖ®Èù¢‰∫ÜËß£Ë™™Ë©±ËÄÖÁöÑÁúüÂØ¶ÊÑèÂúñÔºåÈÄôË¢´Ë™çÁÇ∫‰∏çÂÉÖÈôêÊñºÂæ™Â∫èÊº∏ÈÄ≤ÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãË´ñÈªûÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊèêÁ§∫Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫ SarcasmCueÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõõÁ®ÆÊèêÁ§∫Á≠ñÁï•ÔºåÂç≥ÁüõÁõæÈèà (CoC)„ÄÅÁ∑öÁ¥¢Âúñ (GoC)„ÄÅÁ∑öÁ¥¢Ë¢ã (BoC) ÂíåÁ∑öÁ¥¢ÂºµÈáè (ToC)ÔºåÂÆÉÂºïÁôº LLM ÈÄöÈÅéËÄÉÊÖÆÈ†ÜÂ∫èÂíåÈùûÈ†ÜÂ∫èÊèêÁ§∫ÊñπÊ≥ï‰æÜÊ™¢Ê∏¨‰∫∫È°ûÁöÑË´∑Âà∫„ÄÇÈÄöÈÅéÂ∞çÂõõÂÄãÂü∫Ê∫ñÊï∏ÊìöÈõÜÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂØ¶Ë≠âÊØîËºÉÔºåÊàëÂÄëË°®ÊòéÊâÄÊèêÂá∫ÁöÑÂõõÁ®ÆÊèêÁ§∫ÊñπÊ≥ï‰ª•Áõ∏Áï∂Â§ßÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÊ®ôÊ∫ñ IO ÊèêÁ§∫„ÄÅCoT Âíå ToTÔºå‰∏¶‰∏îÈùûÈ†ÜÂ∫èÊèêÁ§∫ÈÄöÂ∏∏ÂÑ™ÊñºÈ†ÜÂ∫èÊèêÁ§∫„ÄÇ

##### **Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**
2407.12703v3 by Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim

Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
encode only textual information, neglecting various topological structures of
knowledge graphs (KGs). In this paper, we empirically validate the significant
relations between the structural properties of KGs and the performance of the
PLM-based methods. To leverage the structural knowledge, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) that combines (i)
subgraph-aware mini-batching to encourage hard negative sampling, and (ii) a
new contrastive learning method to focus more on harder entities and harder
negative triples in terms of the structural properties. To the best of our
knowledge, this is the first study to comprehensively incorporate the
structural inductive bias of the subgraphs into fine-tuning PLMs. Extensive
experiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our
code is available.

ÊëòË¶ÅÔºöÂæÆË™øÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) Ëøë‰æÜÈ°ØÁ§∫Âá∫ÊîπÂñÑÁü•Ë≠òÂúñË≠úÂÆåÊàêÂäüËÉΩ (KGC) ÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Âü∫Êñº PLM ÁöÑÊñπÊ≥ïÂÉÖÁ∑®Á¢ºÊñáÂ≠óË≥áË®äÔºåÂøΩÁï•‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÂêÑÁ®ÆÊãìÊí≤ÁµêÊßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÁ∂ìÈ©óÈ©óË≠â‰∫Ü KG ÁöÑÁµêÊßãÂ±¨ÊÄßËàáÂü∫Êñº PLM ÁöÑÊñπÊ≥ïÊïàËÉΩ‰πãÈñìÁöÑÈáçË¶ÅÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂà©Áî®ÁµêÊßãÁü•Ë≠òÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®Êñº KGC ÁöÑÂ≠êÂúñÊÑüÁü•Ë®ìÁ∑¥Êû∂Êßã (SATKGC)ÔºåÂÆÉÁµêÂêà‰∫ÜÔºö(i) Â≠êÂúñÊÑüÁü•Â∞èÊâπÊ¨°ËôïÁêÜ‰ª•ÈºìÂãµÂõ∞Èõ£Ë≤†Èù¢ÊäΩÊ®£Ôºå‰ª•Âèä (ii) ‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºåÂú®ÁµêÊßãÂ±¨ÊÄßÊñπÈù¢Êõ¥Â∞àÊ≥®ÊñºÊõ¥Âõ∞Èõ£ÁöÑÂØ¶È´îÂíåÊõ¥Âõ∞Èõ£ÁöÑË≤†‰∏âÂÖÉÁµÑ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÂ≠êÂúñÁöÑÁµêÊßãÊ≠∏Á¥çÂÅèË™§ÂÖ®Èù¢Á¥çÂÖ• PLM ÂæÆË™øÁöÑÁ†îÁ©∂„ÄÇÂú®ÂõõÂÄã KGC Âü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü SATKGC ÁöÑÂÑ™Ë∂äÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÁèæÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Abstraction Alignment: Comparing Model and Human Conceptual Relationships**
2407.12543v1 by Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan

Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like "bolo tie" and "car tire" to more general concepts
like "CEO" and "model". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.

ÊëòË¶ÅÔºöÊäΩË±°Âåñ‚Äî‚ÄîÂ∞áÁâπÂÆöÁØÑ‰æãÊ¶ÇÊã¨ÁÇ∫Âª£Ê≥õÂèØÈáçË§á‰ΩøÁî®ÁöÑÊ®°ÂºèÁöÑÈÅéÁ®ã‚Äî‚ÄîÊòØ‰∫∫ÂÄëÊúâÊïàËôïÁêÜÂíåÂÑ≤Â≠òË≥áË®äÔºå‰∏¶Â∞áÂÖ∂Áü•Ë≠òÊáâÁî®ÊñºÊñ∞Ë≥áÊñôÁöÑÊ†∏ÂøÉ„ÄÇÊúâÂ∏åÊúõÁöÑÊòØÔºåÁ†îÁ©∂È°ØÁ§∫ ML Ê®°ÂûãÂ≠∏ÁøíË∑®Ë∂äÊäΩË±°Â±§Á¥öÁöÑË°®ÂæµÔºåÂæû„ÄåÁ¥∞È†òÂ∏∂„ÄçÂíå„ÄåÊ±ΩËªäËº™ËÉé„ÄçÁ≠âÂÖ∑È´îÊ¶ÇÂøµÂà∞„ÄåÂü∑Ë°åÈï∑„ÄçÂíå„ÄåÊ®°Âûã„ÄçÁ≠âÊõ¥‰∏ÄËà¨ÁöÑÊ¶ÇÂøµ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊäÄË°ìÂ≠§Á´ãÂú∞ÂàÜÊûêÈÄô‰∫õË°®ÂæµÔºåÂ∞áÂ≠∏ÁøíÂà∞ÁöÑÊ¶ÇÂøµË¶ñÁÇ∫Áç®Á´ãÁöÑÁî¢Áâ©ÔºåËÄå‰∏çÊòØÊäΩË±°ÁöÑÁõ∏‰∫íÈÄ£ÁµêÁ∂≤Ë∑Ø„ÄÇÂõ†Ê≠§ÔºåÂÑòÁÆ°ÊàëÂÄëÂèØ‰ª•Ë≠òÂà•Ê®°ÂûãÁî®‰æÜÁî¢ÁîüÂÖ∂Ëº∏Âá∫ÁöÑÊ¶ÇÂøµÔºå‰ΩÜÂæàÈõ£Ë©ï‰º∞ÂÆÉÊòØÂê¶Â≠∏ÁøíÂà∞Ê¶ÇÂøµÁöÑ‰∫∫È°ûÂ∞çÈΩäÊäΩË±°ÔºåÈÄô‰∫õÊ¶ÇÂøµÂ∞áÊ¶ÇÊã¨Âà∞Êñ∞ÁöÑË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊäΩË±°Â∞çÈΩäÔºå‰∏ÄÁ®ÆË°°ÈáèÊ®°ÂûãÂ≠∏ÁøíÁöÑÊäΩË±°ËàáÈ†êÊúüÁöÑÊäΩË±°‰πãÈñì‰∏ÄËá¥ÊÄßÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊ®°ÂûãËº∏Âá∫Ëàá‰∫∫È°ûÊäΩË±°ÂúñÂΩ¢Ôºà‰æãÂ¶ÇË™ûË®ÄÈóú‰øÇÊàñÈÜ´ÁôÇÁñæÁóÖÂ±§Á¥öÁµêÊßãÔºâÈÄ≤Ë°åÊØîËºÉ‰æÜÈáèÂåñÊäΩË±°Â∞çÈΩä„ÄÇÂú®Ëß£ÈáãÂΩ±ÂÉèÊ®°Âûã„ÄÅÂü∫Ê∫ñË™ûË®ÄÊ®°ÂûãÂíåÂàÜÊûêÈÜ´ÁôÇË≥áÊñôÈõÜÁöÑË©ï‰º∞‰ªªÂãô‰∏≠ÔºåÊäΩË±°Â∞çÈΩäÊèê‰æõ‰∫ÜÂ∞çÊ®°ÂûãË°åÁÇ∫ÂíåË≥áÊñôÈõÜÂÖßÂÆπÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£ÔºåÊ†πÊìöËàá‰∫∫È°ûÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÂçÄÂàÜÈåØË™§ÔºåÊì¥Â±ïÁï∂ÂâçÊ®°ÂûãÂìÅË≥™ÊåáÊ®ôÁöÑË©≥Á¥∞Á®ãÂ∫¶Ôºå‰∏¶Êè≠Á§∫ÊîπÂñÑÁèæÊúâ‰∫∫È°ûÊäΩË±°ÁöÑÊñπÊ≥ï„ÄÇ

##### **Struct-X: Enhancing Large Language Models Reasoning with Structured Data**
2407.12522v1 by Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi

Structured data, rich in logical and relational information, has the
potential to enhance the reasoning abilities of large language models (LLMs).
Still, its integration poses a challenge due to the risk of overwhelming LLMs
with excessive tokens and irrelevant context information. To address this, we
propose Struct-X, a novel framework that operates through five key phases:
``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize
structured data. It begins by encoding structured data into a topological space
using graph embeddings, followed by filling in missing entity information with
knowledge retrieval modules, and filtering out irrelevant tokens via a
self-supervised module. The final phase involves constructing a topological
network with selected tokens to further reduce the total token length for more
effective LLM inference. Additionally, Struct-X includes an Auxiliary Module
trained to generate prompts, aiding LLMs in analyzing structured data.
Extensive experiments on benchmarks, including the knowledge graph
question-answer task and the long document reading comprehension task, show
that Struct-X notably improves LLM reasoning, demonstrating the effectiveness
of structured data augmentation in improving LLM inference with complex input
context.

ÊëòË¶ÅÔºöÁµêÊßãÂåñË≥áÊñôÂØåÂê´ÈÇèËºØÂíåÈóú‰øÇË≥áË®äÔºåÊúâÊΩõÂäõÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºÈÅéÂ§öÁ¨¶ËôüÂíåÁÑ°ÈóúËÑàÁµ°Ë≥áË®äÂèØËÉΩÊúÉËÆì LLM ‰∏çÂ†™Ë≤†Ëç∑ÔºåÂõ†Ê≠§Êï¥ÂêàÊ≠§È°ûË≥áÊñôÊßãÊàê‰∫Ü‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ Struct-XÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄèÈÅé‰∫îÂÄãÈóúÈçµÈöéÊÆµÈÅã‰ΩúÁöÑÊñ∞Á©éÊû∂ÊßãÔºö``ËÆÄÂèñ-Âª∫Ê®°-Â°´Ë£ú-ÂèçÊÄù-Êé®ÁêÜ''ÔºåÊúâÊïàÂú∞ËÆì LLM ËÉΩÂ§†Âà©Áî®ÁµêÊßãÂåñË≥áÊñô„ÄÇÂÆÉÈ¶ñÂÖà‰ΩøÁî®ÂúñÂΩ¢ÂµåÂÖ•Â∞áÁµêÊßãÂåñË≥áÊñôÁ∑®Á¢ºÂà∞ÊãìÊí≤Á©∫Èñì‰∏≠ÔºåÊé•ËëóÂà©Áî®Áü•Ë≠òÊì∑ÂèñÊ®°ÁµÑÂ°´Ë£úÈÅ∫Â§±ÁöÑÂØ¶È´îË≥áË®äÔºå‰∏¶ÈÄèÈÅéËá™ÊàëÁõ£Áù£Ê®°ÁµÑÁØ©ÈÅ∏Âá∫ÁÑ°ÈóúÁ¨¶Ëôü„ÄÇÊúÄÂæå‰∏ÄÂÄãÈöéÊÆµÊ∂âÂèäÂª∫Êßã‰∏ÄÂÄãÊãìÊí≤Á∂≤Ë∑ØÔºåÂÖ∂‰∏≠ÂåÖÂê´ÈÅ∏ÂÆöÁöÑÁ¨¶ËôüÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Ê∏õÂ∞ëÁ∏ΩÁ¨¶ËôüÈï∑Â∫¶Ôºå‰ª•‰æøÊõ¥ÊúâÊïàÂú∞ÈÄ≤Ë°å LLM Êé®Ë´ñ„ÄÇÊ≠§Â§ñÔºåStruct-X ÈÇÑÂåÖÊã¨‰∏ÄÂÄãËºîÂä©Ê®°ÁµÑÔºåÁ∂ìÈÅéË®ìÁ∑¥ÂèØ‰ª•Áî¢ÁîüÊèêÁ§∫ÔºåÂçîÂä© LLM ÂàÜÊûêÁµêÊßãÂåñË≥áÊñô„ÄÇÂú®Âü∫Ê∫ñ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óÔºåÂåÖÊã¨Áü•Ë≠òÂúñË≠úÂïèÁ≠î‰ªªÂãôÂíåÈï∑ÁØáÊñá‰ª∂Èñ±ËÆÄÁêÜËß£‰ªªÂãôÔºåÈ°ØÁ§∫ Struct-X ÊòéÈ°ØÊîπÂñÑ‰∫Ü LLM Êé®ÁêÜÔºåË≠âÊòé‰∫ÜÁµêÊßãÂåñË≥áÊñôÊì¥ÂÖÖÂú®ÊîπÂñÑ LLM Êé®Ë´ñÊôÇÁöÑÊúâÊïàÊÄßÔºåÁâπÂà•ÊòØÂú®Ëº∏ÂÖ•ËÑàÁµ°Ë§áÈõúÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇ

##### **Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**
2407.12888v1 by Alexander R. Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping

The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.

ÊëòË¶ÅÔºö<paragraph>Áèæ‰ªäÂ§ßÈáèÁöÑÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÂ∞çË©¶ÂúñÊúâÊïàÊ∂àÂåñ„ÄÅËôïÁêÜÂíåÁêÜËß£ÈÄô‰∫õÁôºÁèæÁöÑÁ†îÁ©∂‰∫∫Âì°ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫Âú®ÈÄôÂÄãË§áÈõú‰∏îÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÁí∞Â¢É‰∏≠Â∞éËà™ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåLLM ÂèØËÉΩÊúÉÂ∞éËá¥ÂπªË¶∫ÂèçÊáâÔºåÈÄô‰ΩøÂæóÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) Â∞çÊñºÁç≤ÂæóÊ∫ñÁ¢∫Ë≥áË®äËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÂÄãÂçîÂÆö‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RUGGEDÔºàÂúñÂΩ¢Â∞éÂºïÂèØËß£ÈáãÁñæÁóÖÂçÄÂàÜÁöÑÊ™¢Á¥¢ÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊè¥Á†îÁ©∂‰∫∫Âì°ÈÄ≤Ë°åÁü•Ë≠òÊï¥ÂêàÂíåÂÅáË®≠Áî¢ÁîüÔºåÊâæÂá∫Á∂ìÈÅéÈ©óË≠âÁöÑÈÄ≤Â±ïË∑ØÂæë„ÄÇ‰æÜËá™Âá∫ÁâàÁâ©ÂíåÁü•Ë≠òÂ∫´ÁöÑÁõ∏ÈóúÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÊúÉÈÄèÈÅéÊñáÊú¨Êé¢ÂãòÈóúËÅØÂàÜÊûêÂíåÁñæÁóÖÁØÄÈªûÁöÑÂèØËß£ÈáãÂúñÂΩ¢È†êÊ∏¨Ê®°ÂûãÈÄ≤Ë°åÊ™¢Èñ±„ÄÅÊï¥ÂêàÂíåËêÉÂèñÔºåÈ†êÊ∏¨Ëó•Áâ©ÂíåÁñæÁóÖ‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØ„ÄÇÈÄô‰∫õÂàÜÊûêÈÄ£ÂêåÁîüÁâ©ÈÜ´Â≠∏ÊñáÊú¨ÊúÉÊï¥ÂêàÂà∞‰∏ÄÂÄãÊû∂Êßã‰∏≠ÔºåË©≤Êû∂Êßã‰øÉÈÄ≤‰ΩøÁî®ËÄÖÂ∞éÂêëÁöÑÊ©üÂà∂Èó°ÊòéÔºå‰ª•ÂèäÈÄèÈÅé RAG ÂïüÁî®ÁöÑ LLM ÈÄ≤Ë°åÂÅáË®≠Êé¢Ë®é„ÄÇ‰∏ÄÂÄãËá®Â∫ä‰ΩøÁî®Ê°à‰æãÂ±ïÁ§∫‰∫Ü RUGGED Ë©ï‰º∞ÂíåÊé®Ëñ¶Áî®ÊñºÂøÉÂæãÂ§±Â∏∏ÊÄßÂøÉËÇåÁóÖËÆä (ACM) ÂíåÊì¥ÂºµÂûãÂøÉËÇåÁóÖËÆä (DCM) ÁöÑÊ≤ªÁôÇÊñπÊ≥ïÁöÑËÉΩÂäõÔºåÂàÜÊûêËôïÊñπËó•Áâ©ÁöÑÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®ÂíåÊú™Êé¢Á¥¢ÁöÑÁî®ÈÄî„ÄÇÈÄôÂÄãÂπ≥Âè∞Â∞á LLM ÂπªË¶∫ÈôçÂà∞ÊúÄ‰ΩéÔºåÊèê‰æõÂèØÊìç‰ΩúÁöÑË¶ãËß£Ôºå‰∏¶ÊîπÂñÑÊñ∞Ê≤ªÁôÇÊñπÊ≥ïÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**
2407.11638v1 by He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.

ÊëòË¶ÅÔºöËøëÊúüÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁßçËµÑÊñôÊé¢Âãò‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÊûÅÂ§ßÁöÑÊΩúÂäõÔºå‰æãÂ¶ÇÁü•ËØÜÈóÆÁ≠î„ÄÅÊï∞Â≠¶Êé®ÁêÜÂíåÂ∏∏ËØÜÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåLLM Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÊÄßÂú∞Ë∞ÉÊü•ÂÖ∂Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞„ÄÇÁî±‰∫éÁº∫‰πèÂêåÊó∂ÂåÖÂê´ÂõæË°®ÂíåÊñáÊú¨ËµÑÊñôÁöÑÈ´òÂìÅË¥®Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨È¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ MidEast-TE-mini ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂü∫‰∫éÊ≠§Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Á∫øÊñπÊ≥ïÔºåÂÖ∂ÁâπÁÇπÊòØÂêÑÁßçËæìÂÖ•Ê†ºÂºèÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Ê®°Âùó„ÄÇ‰ªéÂπøÊ≥õÁöÑÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞Áõ¥Êé•Â∞ÜÂéüÂßãÊñáÊú¨Êï¥ÂêàÂà∞ LLM ÁöÑËæìÂÖ•‰∏≠Âπ∂‰∏ç‰ºöÂ¢ûÂº∫Èõ∂Ê¨°Â≠¶‰π†Â§ñÊé®ÊÄßËÉΩ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂú®ÁâπÂÆöÂ§çÊùÇ‰∫ã‰ª∂‰∏≠Á∫≥ÂÖ•ÂéüÂßãÊñáÊú¨Âπ∂ÂæÆË∞É LLM ‰ºöÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊ£ÄÁ¥¢Ê®°ÂùóÁöÑÂ¢ûÂº∫ÔºåLLM ÂèØ‰ª•ÊúâÊïàÂú∞ÊçïÊçâÈöêËóèÂú®ÂéÜÂè≤‰∫ã‰ª∂‰∏≠ÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÊ®°Âºè„ÄÇÂêåÊó∂ÔºåËØ∏Â¶ÇÊµÅË°åÂ∫¶ÂÅèÂ∑ÆÂíåÈïøÂ∞æÈóÆÈ¢òÁ≠âÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®‰∫é LLM ‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫é RAG ÁöÑÊñπÊ≥ï‰∏≠„ÄÇËøô‰∫õÂèëÁé∞‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑ‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïÁöÑÁêÜËß£ÔºåËøòÁ™ÅÂá∫‰∫ÜÂá†‰∏™ÊúâÂâçÊôØÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËøôÈ°πÂÖ®Èù¢ÁöÑËØÑ‰º∞ÔºåËøûÂêåÂ∑≤Á°ÆÂÆöÁöÑÁ†îÁ©∂Êú∫‰ºöÔºåÂ∞ÜÊûÅÂ§ßÂú∞‰øÉËøõÈÄöËøá LLM ËøõË°åÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÁöÑÊú™Êù•Á†îÁ©∂„ÄÇ

##### **Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**
2407.12068v2 by Kai Guo, Zewen Liu, Zhikai Chen, Hongzhi Wen, Wei Jin, Jiliang Tang, Yi Chang

Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing tasks. Recently, several LLMs-based
pipelines have been developed to enhance learning on graphs with text
attributes, showcasing promising performance. However, graphs are well-known to
be susceptible to adversarial attacks and it remains unclear whether LLMs
exhibit robustness in learning on graphs. To address this gap, our work aims to
explore the potential of LLMs in the context of adversarial attacks on graphs.
Specifically, we investigate the robustness against graph structural and
textual perturbations in terms of two dimensions: LLMs-as-Enhancers and
LLMs-as-Predictors. Through extensive experiments, we find that, compared to
shallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior
robustness against structural and textual attacks.Based on these findings, we
carried out additional analyses to investigate the underlying causes.
Furthermore, we have made our benchmark library openly available to facilitate
quick and fair evaluations, and to encourage ongoing innovative research in
this field.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÈÉΩÂ±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊúÄËøëÔºåÂ∑≤ÈñãÁôºÂá∫Â§öÂÄãÂü∫Êñº LLM ÁöÑÁÆ°ÈÅìÔºå‰ª•Â¢ûÂº∑ÂÖ∑ÊúâÊñáÂ≠óÂ±¨ÊÄßÁöÑÂúñÂΩ¢Â≠∏ÁøíÔºåÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂúñÂΩ¢ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊîªÊìäÔºåËÄå LLM Âú®ÂúñÂΩ¢Â≠∏Áøí‰∏≠ÊòØÂê¶Â±ïÁèæÂá∫Á©©ÂÅ•ÊÄß‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Êé¢Ë®é LLM Âú®ÂúñÂΩ¢Â∞çÊäóÊÄßÊîªÊìä‰∏≠ÁöÑÊΩõÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈáùÂ∞çÂÖ©ÂÄãÈù¢ÂêëÊé¢Ë®éÂÖ∂Â∞çÂúñÂΩ¢ÁµêÊßãÂíåÊñáÂ≠óÊìæÂãïÁöÑÁ©©ÂÅ•ÊÄßÔºöLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÁôºÁèæÔºåËàáÊ∑∫Â±§Ê®°ÂûãÁõ∏ÊØîÔºåLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®Âú®ÁµêÊßãÊÄßÂíåÊñáÂ≠óÊîªÊìä‰∏≠ÈÉΩÊèê‰æõÂÑ™Áï∞ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÈ°çÂ§ñÁöÑÂàÜÊûê‰æÜÊé¢Ë®éÂÖ∂Ê†πÊú¨ÂéüÂõ†„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∑≤ÂÖ¨ÈñãÊàëÂÄëÁöÑÂü∫Ê∫ñÂ∫´Ôºå‰ª•Âà©Âø´ÈÄü‰∏îÂÖ¨Âπ≥ÁöÑË©ï‰º∞Ôºå‰∏¶ÈºìÂãµÊåÅÁ∫åÈÄ≤Ë°åÈÄôÊñπÈù¢ÁöÑÂâµÊñ∞Á†îÁ©∂„ÄÇ

##### **CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**
2407.11393v2 by Kalliopi Basioti, Mohamed A. Abdelsalam, Federico Fancellu, Vladimir Pavlovic, Afsaneh Fazly

Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.

ÊëòË¶ÅÔºöÂèØÊéßÂõæÂÉèÊ†áÊ≥® (CIC) Êó®Âú®ÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰ª•ÊèèËø∞ÂõæÂÉèÔºåÊù°‰ª∂ÊòØÊ†πÊçÆÊúÄÁªàÁî®Êà∑Êèê‰æõÁöÑËµÑËÆØÔºå‰æãÂ¶ÇÂå∫Âüü„ÄÅÂÆû‰ΩìÊàñÊÑüÂÖ¥Ë∂£ÁöÑ‰∫ã‰ª∂„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂõæÂÉèËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂåÖÂê´ÊèèËø∞Êï¥‰∏™ÂõæÂÉèÁöÑÊ†áÊ≥®Ôºå‰ΩøÂÖ∂Êó†Ê≥ïÊúâÊïàËÆ≠ÁªÉ CIC Ê®°ÂûãÔºåËÄåËøô‰∫õÊ®°ÂûãÊúâÂèØËÉΩÂÖ≥Ê≥®‰ªª‰ΩïÂå∫ÂüüÊàñÂÖ≥Á≥ªÁöÑÂ≠êÈõÜ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂÖ®Ëá™Âä®ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Âª∫Á´ãÂú®‰∏éÂõæÂÉèÂÖ≥ËÅîÁöÑÁé∞ÊúâÊ†áÊ≥®ÈõÜ‰πã‰∏äÁöÑÁªü‰∏ÄÁªìÊûÑÂåñËØ≠‰πâË°®Á§∫Êù•ÊäΩÊ†∑ÂÖ∂‰ªñËÅöÁÑ¶‰∏îËßÜËßâÊé•Âú∞ÁöÑÊ†áÊ≥®„ÄÇÊàë‰ª¨Âà©Áî®Ë∑®ËØ≠Ë®ÄÂõæÂºèËØ≠‰πâÂΩ¢ÂºèÂåñÊäΩË±°ÊÑè‰πâË°®Á§∫ (AMR) Êù•ÁºñÁ†ÅÂÆû‰Ωì‰πãÈó¥ÊâÄÊúâÂèØËÉΩÁöÑÁ©∫Èó¥ËØ≠‰πâÂÖ≥Á≥ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂΩìÂâçÊñπÊ≥ï‰∏≠‰ªÖÂÖ≥Ê≥®ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÊàë‰ª¨‰ΩøÁî®ËøôÁßçÁªìÊûÑÂåñËØ≠‰πâÂ¢ûÂº∫ (SSA) Ê°ÜÊû∂Êù•Â¢ûÂº∫Áé∞ÊúâÁöÑÂõæÂÉèÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºå‰ΩøÂÖ∂Êé•Âú∞‰∏îÂèØÊéßÁöÑÊ†áÊ≥®ÔºåÂ¢ûÂä†ÂÆÉ‰ª¨ÁöÑÁ©∫Èó¥ÂíåËØ≠‰πâÂ§öÊ†∑ÊÄß‰ª•ÂèäÁÑ¶ÁÇπË¶ÜÁõñËåÉÂõ¥„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Êñ∞Ê®°Âûã CIC-BART-SSAÔºå‰∏ìÈó®ÈíàÂØπ CIC ‰ªªÂä°ÈáèË∫´ÂÆöÂà∂ÔºåÂÖ∂ÊéßÂà∂‰ø°Âè∑Êù•Ëá™ SSA Â§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨Âá≠ÁªèÈ™åË°®ÊòéÔºå‰∏é SOTA CIC Ê®°ÂûãÁõ∏ÊØîÔºåCIC-BART-SSA ÁîüÊàêÁöÑÊ†áÊ≥®Âú®Â§öÊ†∑ÊÄßÂíåÊñáÊú¨Ë¥®ÈáèÊñπÈù¢Êõ¥ËÉú‰∏ÄÁ≠πÔºåÂú®ÂèØÊéßÊÄßÊñπÈù¢ÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåËÄå‰∏îÈáçË¶ÅÁöÑÊòØÔºåÈÄöËøáÊúâÊïàÂú∞Êé®ÂπøÂà∞ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈ´òÂ∫¶ËÅöÁÑ¶Âú∫ÊôØÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞Áº©Â∞è‰∫ÜÂπøÊ≥õÂíåÈ´òÂ∫¶ËÅöÁÑ¶ÁöÑÂèóÊéßÊ†áÊ≥®ÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ‰ª£Á†ÅÂèØ‰ªé https://github.com/SamsungLabs/CIC-BART-SSA Ëé∑Âæó„ÄÇ

##### **Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**
2407.10805v3 by Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo

Retrieval-augmented generation (RAG) has significantly advanced large
language models (LLMs) by enabling dynamic information retrieval to mitigate
knowledge gaps and hallucinations in generated content. However, these systems
often falter with complex reasoning and consistency across diverse queries. In
this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns
questions with the knowledge graph and uses it as a navigational tool, which
deepens and refines the RAG paradigm for information collection and
integration. The KG-guided navigation fosters deep and long-range associations
to uphold logical consistency and optimize the scope of retrieval for precision
and interoperability. In conjunction, factual consistency can be better ensured
through semantic similarity guided by precise directives. ToG${2.0}$ not only
improves the accuracy and reliability of LLMs' responses but also demonstrates
the potential of hybrid structured knowledge systems to significantly advance
LLM reasoning, aligning it closer to human-like performance. We conducted
extensive experiments on four public datasets to demonstrate the advantages of
our method compared to the baseline.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Â∑≤Â§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåËóâÁî±ÂïüÁî®ÂãïÊÖãË≥áË®äÊ™¢Á¥¢‰æÜÊ∏õËºïÁî¢ÁîüÂÖßÂÆπ‰∏≠ÁöÑÁü•Ë≠òÂ∑ÆË∑ùÂíåÂπªË¶∫„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ≥ªÁµ±Âú®Ë§áÈõúÊé®ÁêÜÂíå‰∏çÂêåÊü•Ë©¢ÈñìÁöÑ‰∏ÄËá¥ÊÄßÊñπÈù¢Á∂ìÂ∏∏ÊúÉÂá∫ÈåØ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Think-on-Graph 2.0Ôºå‰∏ÄÂÄãÂ¢ûÂº∑ÁöÑ RAG Êû∂ÊßãÔºåÂÆÉÊúÉÂ∞áÂïèÈ°åËàáÁü•Ë≠òÂúñË≠úÂ∞çÈΩäÔºå‰∏¶Â∞áÂÖ∂Áî®‰ΩúÂ∞éËà™Â∑•ÂÖ∑ÔºåÈÄôÊúÉÂä†Ê∑±ÂíåÊîπÂñÑ RAG ÂÖ∏ÁØÑ‰ª•ÈÄ≤Ë°åË≥áË®äÊî∂ÈõÜÂíåÊï¥Âêà„ÄÇKG ÂºïÂ∞éÁöÑÂ∞éËà™‰øÉÈÄ≤Ê∑±Â∫¶‰∏îÈï∑Á®ãÈóúËÅØÔºå‰ª•Á∂≠ÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄßÔºå‰∏¶ÊúÄ‰Ω≥ÂåñÊ™¢Á¥¢ÁØÑÂúç‰ª•ÊèêÂçáÁ≤æÊ∫ñÂ∫¶Âíå‰∫íÊìç‰ΩúÊÄß„ÄÇÁµêÂêà‰ΩøÁî®Ôºå‰∫ãÂØ¶‰∏ÄËá¥ÊÄßÂèØÈÄèÈÅéÁî±Á≤æÁ¢∫ÊåáÁ§∫ÂºïÂ∞éÁöÑË™ûÁæ©Áõ∏‰ººÊÄßÁç≤ÂæóÊõ¥Â•ΩÁöÑÁ¢∫‰øù„ÄÇToG${2.0}$ ‰∏çÂÉÖÊèêÂçá LLM ÂõûÊáâÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂèØÈù†Â∫¶Ôºå‰πüË≠âÊòéÊ∑∑ÂêàÁµêÊßãÂåñÁü•Ë≠òÁ≥ªÁµ±ÊúâÊΩõÂäõÂ§ßÂπÖÊèêÂçá LLM Êé®ÁêÜÔºå‰ΩøÂÖ∂Êõ¥Êé•Ëøë‰∫∫È°ûËà¨ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÂú®ÂõõÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Ë≠âÊòéÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™ÊñºÂü∫Á∑ö„ÄÇ

##### **Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**
2407.10794v1 by Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li

Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.

ÊëòË¶ÅÔºö<paragraph>Áü•Ë≠òÂúñË≠ú (KG) Âú®‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüËá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âª£Ê≥õÊáâÁî®Êñº‰∏ãÊ∏∏‰ªªÂãôÔºå‰æãÂ¶ÇÂ¢ûÂº∑ÂïèÁ≠î (QA) Á≥ªÁµ±„ÄÇÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãÈÄöÂ∏∏ÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂ÁöÑÂ§ßÈáèÂ∑•‰Ωú„ÄÇÊúÄËøëÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë¢´Áî®ÊñºÁü•Ë≠òÂúñË≠úÂª∫Êßã (KGC)ÔºåÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÈóúÊ≥®Â±ÄÈÉ®ËßÄÈªûÔºåÂæûÂÄãÂà•Âè•Â≠êÊàñÊñá‰ª∂‰∏≠ÊèêÂèñÁü•Ë≠ò‰∏âÂÖÉÁµÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü GraphusionÔºå‰∏ÄÂÄãÂæûËá™Áî±ÊñáÊú¨‰∏≠ÈÄ≤Ë°åÈõ∂Ê¨°Â≠∏ÁøíÁöÑ KGC Ê°ÜÊû∂„ÄÇÊ†∏ÂøÉËûçÂêàÊ®°ÁµÑÊèê‰æõ‰∏âÂÖÉÁµÑÁöÑÂÖ®Â±ÄËßÄÈªûÔºåÂåÖÂê´ÂØ¶È´îÂêà‰Ωµ„ÄÅË°ùÁ™ÅËß£Ê±∫ÂíåÊñ∞‰∏âÂÖÉÁµÑÁôºÁèæ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á Graphusion ÊáâÁî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) È†òÂüüÔºå‰∏¶Âú®ÊïôËÇ≤Â†¥ÊôØ‰∏≠È©óË≠âÂÆÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü TutorQAÔºå‰∏ÄÂÄãÊñ∞ÁöÑÁî±Â∞àÂÆ∂È©óË≠âÁöÑÂúñË≠úÊé®ÁêÜÂíåÂïèÁ≠îÂü∫Ê∫ñÔºåÂåÖÂê´ÂÖ≠È†Ö‰ªªÂãôÂíåÁ∏ΩË®à 1,200 ÂÄãÂïèÁ≠îÂ∞ç„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Ë°®ÊòéÔºåGraphusion Âú®ÈÄ£ÁµêÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫Â∫¶‰∏äÊØîÁõ£Áù£ÂºèÂü∫Ê∫ñÈ´òÂá∫ 10%„ÄÇÊ≠§Â§ñÔºåÂú®Ê¶ÇÂøµÂØ¶È´îÊèêÂèñÂíåÈóú‰øÇË≠òÂà•ÁöÑ‰∫∫È°ûË©ï‰º∞‰∏≠ÔºåÂÆÉÂàÜÂà•Áç≤Âæó‰∫Ü 3 ÂàÜ‰∏≠ÁöÑ 2.92 ÂàÜÂíå 2.37 ÂàÜ„ÄÇ</paragraph>

##### **GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**
2407.10793v1 by Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada

Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂõûÊáâË©ï‰º∞ÊñπÊ≥ïÂíå‰∏ç‰∏ÄËá¥ÊÄßÂÅµÊ∏¨ÔºàÂèàÁ®±ÁÇ∫ÂπªË¶∫ÔºâÔºåÁõ∏Â∞çÊñºÊâÄÊèê‰æõÁöÑÁü•Ë≠òÔºåÂ∞çÊñº LLM ÊáâÁî®Ê≠£ËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÁõÆÂâçÁöÑÊåáÊ®ôÁÑ°Ê≥ïÊèê‰æõÂèØËß£ÈáãÁöÑÊ±∫Á≠ñ„ÄÅÁ≥ªÁµ±ÊÄßÂú∞Ê™¢Êü•ÂõûÊáâ‰∏≠ÁöÑÊâÄÊúâË≥áË®äÔºåËÄå‰∏îÂú®ÂØ¶Âãô‰∏ä‰ΩøÁî®ÊôÇÔºåÈÄöÂ∏∏ÈÅéÊñºËÄóË≤ªÈÅãÁÆóË≥áÊ∫ê„ÄÇÊàëÂÄëÊèêÂá∫ GraphEvalÔºö‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñ (KG) ÁµêÊßã‰æÜË°®Á§∫Ë≥áË®äÁöÑÂπªË¶∫Ë©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑÊäÄË°ìË≠òÂà•Âá∫ÂÆπÊòìÂá∫ÁèæÂπªË¶∫ÁöÑ KG ‰∏≠ÁâπÂÆö‰∏âÂÖÉÁµÑÔºåÂõ†Ê≠§ÊØî‰ª•ÂæÄÁöÑÊñπÊ≥ïÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂõûÊáâ‰∏≠ÂπªË¶∫ÁôºÁîüÂú®Âì™Ë£°ÔºàÂ¶ÇÊûúÊúâÁöÑË©±Ôºâ„ÄÇÊ≠§Â§ñÔºåÂ∞áÊàëÂÄëÁöÑÊñπÊ≥ïËàáÊúÄÂÖàÈÄ≤ÁöÑËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁµêÂêà‰ΩøÁî®ÔºåËàá‰ΩøÁî®ÂéüÂßã NLI Ê®°ÂûãÁõ∏ÊØîÔºåÂèØ‰ª•Âú®ÂêÑÁ®ÆÂπªË¶∫Âü∫Ê∫ñ‰∏äÊèêÈ´òÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Á¥¢‰ΩøÁî® GraphEval ‰æÜÈÄ≤Ë°åÂπªË¶∫‰øÆÊ≠£ÔºåÊñπÊ≥ïÊòØÂà©Áî® KG ÁöÑÁµêÊßãÔºåÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÂëΩÂêçÁÇ∫ GraphCorrectÔºå‰∏¶Ë≠âÊòéÂ§ßÂ§öÊï∏ÂπªË¶∫Á¢∫ÂØ¶ÂèØ‰ª•ÂæóÂà∞Á≥æÊ≠£„ÄÇ

##### **Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**
2407.10743v1 by W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil

This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

ÊëòË¶ÅÔºöÊú¨ÊñáË®éË´ñ‰∫ÜÂ∞áÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) Êì¥Â±ïÂà∞Âª£Èóä 3D Áí∞Â¢ÉÁöÑÊåëÊà∞„ÄÇËß£Ê±∫ÈÄôÂÄãÈñãÊîæÊÄßÂïèÈ°åÂ∞çÊñºÊ©üÂô®‰∫∫Âú®Ë®±Â§öÁ¨¨‰∏ÄÂèçÊáâ‰∫∫Âì°Â†¥ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÁâπÂà•Áõ∏ÈóúÔºå‰æãÂ¶ÇÊ∂µËìãÂª£ÈóäÁ©∫ÈñìÁöÑÊêúÊïë‰ªªÂãô„ÄÇÈÄô‰∫õË®≠ÂÆö‰∏≠‰ΩøÁî® LMM ÁõÆÂâçÂèóÂà∞Âö¥Ê†ºÁöÑ‰∏ä‰∏ãÊñáË¶ñÁ™óÈôêÂà∂ÔºåÈÄôÈôêÂà∂‰∫Ü LMM ÁöÑËº∏ÂÖ•Â§ßÂ∞è„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®Ë≥áÊñôÂúñÁµêÊßãÔºåÂÖÅË®± LMM Ëø≠‰ª£Êü•Ë©¢Â§ßÂûãÁí∞Â¢ÉÁöÑËºÉÂ∞èÈÉ®ÂàÜ„ÄÇÈÄèÈÅéÂ∞áË≥áÊñôÂúñËàáÂúñÂΩ¢ÈÅçÊ≠∑ÊºîÁÆóÊ≥ïÁµêÂêà‰ΩøÁî®ÔºåÊàëÂÄëÂèØ‰ª•ÂÑ™ÂÖàËÄÉÊÖÆËàáÊü•Ë©¢ÊúÄÁõ∏ÈóúÁöÑ‰ΩçÁΩÆÔºåÂæûËÄåÊèêÈ´ò 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÊàëÂÄë‰ΩøÁî® 3D Â†¥ÊôØË™™ÊòéË≥áÊñôÂúñÔºå‰ΩÜÈÄô‰∫õÂ†¥ÊôØÂèØ‰ª•ËºïÈ¨ÜÂú∞Áî±ÂÖ∂‰ªñË°®Á§∫Áí∞Â¢ÉÁöÑÂØÜÈõÜÊ®°ÂºèÂèñ‰ª£Ôºå‰æãÂ¶ÇÈªûÈõ≤ÊàñÈ´òÊñØÈªû„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂú®ÊêúÊïë‰ªªÂãôÁØÑ‰æã‰∏≠‰ΩøÁî®Ë≥áÊñôÂúñÈÄ≤Ë°åÂÖ©ÂÄã 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁî®‰æãÁöÑÊΩõÂäõ„ÄÇ

##### **AutoGRAMS: Autonomous Graphical Agent Modeling Software**
2407.10049v1 by Ben Krause, Lucia Chen, Emmanuel Kahembwe

We introduce the AutoGRAMS framework for programming multi-step interactions
with language models. AutoGRAMS represents AI agents as a graph, where each
node can execute either a language modeling instruction or traditional code.
Likewise, transitions in the graph can be governed by either language modeling
decisions or traditional branch logic. AutoGRAMS supports using variables as
memory and allows nodes to call other AutoGRAMS graphs as functions. We show
how AutoGRAMS can be used to design highly sophisticated agents, including
self-referential agents that can modify their own graph. AutoGRAMS's
graph-centric approach aids interpretability, controllability, and safety
during the design, development, and deployment of AI agents. We provide our
framework as open source at https://github.com/autograms/autograms .

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π AutoGRAMS Ê°ÜÊû∂ÔºåÁî®ÊñºÁ∑®ÂØ´ËàáË™ûË®ÄÊ®°ÂûãÁöÑÂ§öÊ≠•È©ü‰∫íÂãï„ÄÇAutoGRAMS Â∞á AI ‰ª£ÁêÜË°®Á§∫ÁÇ∫‰∏ÄÂÄãÂúñÂΩ¢ÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÂèØ‰ª•Âü∑Ë°åË™ûË®ÄÂª∫Ê®°Êåá‰ª§ÊàñÂÇ≥Áµ±‰ª£Á¢º„ÄÇÂêåÊ®£Âú∞ÔºåÂúñÂΩ¢‰∏≠ÁöÑËΩâÊèõÂèØ‰ª•Áî±Ë™ûË®ÄÂª∫Ê®°Ê±∫Á≠ñÊàñÂÇ≥Áµ±ÂàÜÊîØÈÇèËºØÊéßÂà∂„ÄÇAutoGRAMS ÊîØÊè¥‰ΩøÁî®ËÆäÊï∏‰ΩúÁÇ∫Ë®òÊÜ∂È´îÔºå‰∏¶ÂÖÅË®±ÁØÄÈªûÂëºÂè´ÂÖ∂‰ªñ AutoGRAMS ÂúñÂΩ¢‰ΩúÁÇ∫ÂáΩÂºè„ÄÇÊàëÂÄëÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® AutoGRAMS Ë®≠Ë®àÈ´òÂ∫¶Ë§áÈõúÁöÑ‰ª£ÁêÜÔºåÂåÖÊã¨ÂèØ‰ª•‰øÆÊîπËá™Ë∫´ÂúñÂΩ¢ÁöÑËá™ÂèÉÁÖß‰ª£ÁêÜ„ÄÇAutoGRAMS ‰ª•ÂúñÂΩ¢ÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ïÊúâÂä©ÊñºÂú® AI ‰ª£ÁêÜÁöÑË®≠Ë®à„ÄÅÈñãÁôºÂíåÈÉ®ÁΩ≤ÈÅéÁ®ã‰∏≠ÊèêÈ´òÂèØËß£ÈáãÊÄß„ÄÅÂèØÊéßÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊàëÂÄëÂú® https://github.com/autograms/autograms Êèê‰æõÊàëÂÄëÁöÑÊ°ÜÊû∂‰ΩúÁÇ∫ÈñãÊ∫ê„ÄÇ

##### **FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**
2407.09888v1 by Dimitris Papadopoulos, Katerina Metropoulou, Nikolaos Matsatsinis, Nikolaos Papadakis

Our collective attention span is shortened by the flood of online
information. With \textit{FarFetched}, we address the need for automated claim
validation based on the aggregated evidence derived from multiple online news
sources. We introduce an entity-centric reasoning framework in which latent
connections between events, actions, or statements are revealed via entity
mentions and represented in a graph database. Using entity linking and semantic
similarity, we offer a way for collecting and combining information from
diverse sources in order to generate evidence relevant to the user's claim.
Then, we leverage textual entailment recognition to quantitatively determine
whether this assertion is credible, based on the created evidence. Our approach
tries to fill the gap in automated claim validation for less-resourced
languages and is showcased on the Greek language, complemented by the training
of relevant semantic textual similarity (STS) and natural language inference
(NLI) models that are evaluated on translated versions of common benchmarks.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØË≥áË®äÁöÑÊ¥™ÊµÅÁ∏ÆÁü≠‰∫ÜÊàëÂÄëÁöÑÈõÜÈ´îÊ≥®ÊÑèÂäõÊôÇÈñì„ÄÇÈÄèÈÅé \textit{FarFetched}ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÊ†πÊìöÂæûÂ§öÂÄãÁ∑ö‰∏äÊñ∞ËÅû‰æÜÊ∫êÂΩôÁ∏ΩÁöÑË≠âÊìöÈÄ≤Ë°åËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰ª•ÂØ¶È´îÁÇ∫‰∏≠ÂøÉÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠‰∫ã‰ª∂„ÄÅÂãï‰ΩúÊàñÈô≥Ëø∞‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØÈÄèÈÅéÂØ¶È´îÊèêÂèäË¢´Êè≠Èú≤Ôºå‰∏¶Âú®ÂúñÂΩ¢Ë≥áÊñôÂ∫´‰∏≠Ë°®Á§∫„ÄÇ‰ΩøÁî®ÂØ¶È´îÈÄ£ÁµêÂíåË™ûÁæ©Áõ∏‰ººÊÄßÔºåÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñπÂºè‰æÜÊî∂ÈõÜÂíåÁµÑÂêà‰æÜËá™‰∏çÂêå‰æÜÊ∫êÁöÑË≥áË®äÔºå‰ª•Áî¢ÁîüËàá‰ΩøÁî®ËÄÖËÅ≤ÊòéÁõ∏ÈóúÁöÑË≠âÊìö„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®ÊñáÊú¨ËòäÊ∂µË≠òÂà•‰æÜÊ†πÊìöÂª∫Á´ãÁöÑË≠âÊìöÈáèÂåñÁ¢∫ÂÆöÊ≠§Êñ∑Ë®ÄÊòØÂê¶ÂèØ‰ø°„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË©¶ÂúñÂ°´Ë£úË≥áÊ∫êËºÉÂ∞ëÁöÑË™ûË®ÄÁöÑËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÊñπÈù¢ÁöÑÁ©∫ÁôΩÔºå‰∏¶Âú®Â∏åËáòË™û‰∏≠Â±ïÁ§∫ÔºåËºî‰ª•Â∞çÁõ∏ÈóúË™ûÁæ©ÊñáÊú¨Áõ∏‰ººÊÄß (STS) ÂíåËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁöÑË®ìÁ∑¥ÔºåÈÄô‰∫õÊ®°ÂûãÂú®Â∏∏Ë¶ãÂü∫Ê∫ñÁöÑÁøªË≠ØÁâàÊú¨‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇ

##### **GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**
2407.09709v1 by Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang

Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÔºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊàñÂ§ßÂûãË¶ñË¶∫Ê®°Âûã (LVM)ÔºåÂ∑≤ÊàêÁÇ∫ÂêÑËá™È†òÂüü‰∏≠ÊúÄÊúâÂäõÁöÑÂ∑•ÂÖ∑‰πã‰∏Ä„ÄÇÁÑ∂ËÄåÔºåËàáÊñáÊú¨ÂíåÂΩ±ÂÉèË≥áÊñô‰∏çÂêåÔºåÂúñÂΩ¢Ë≥áÊñôÊ≤íÊúâÊòéÁ¢∫ÁöÑÁµêÊßãÔºåÂ∞çÈñãÁôºÂúñÂΩ¢Âü∫Á§éÊ®°Âûã (GFM) ÊßãÊàêÊ•µÂ§ßÁöÑÊåëÊà∞„ÄÇ‰æãÂ¶ÇÔºåÁõÆÂâçË®≠Ë®àÈÄöÁî®ÂúñÂΩ¢Ê®°ÂûãÁöÑÂòóË©¶Ôºå‰∏çÊòØÂ∞áÂúñÂΩ¢Ë≥áÊñôËΩâÊèõÁÇ∫Ë™ûË®ÄÊ†ºÂºè‰ª•‰æõÂü∫Êñº LLM ÁöÑÈ†êÊ∏¨ÔºåÂ∞±ÊòØË®ìÁ∑¥ GNN Ê®°ÂûãÔºå‰∏¶‰ª• LLM ‰ΩúÁÇ∫ËºîÂä©„ÄÇÂâçËÄÖÂèØ‰ª•ËôïÁêÜÁÑ°ÈôêÁöÑ‰ªªÂãôÔºåËÄåÂæåËÄÖÂèØ‰ª•Êõ¥Â•ΩÂú∞Êì∑ÂèñÂúñÂΩ¢ÁµêÊßãÔºå‰ΩÜÁèæÊúâÁöÑÂ∑•‰ΩúÁÑ°Ê≥ïÂêåÊôÇÈÅîÊàêÈÄôÂÖ©ËÄÖ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâæÂá∫ GFM ÁöÑ‰∏âÂÄãÈóúÈçµÁêÜÊÉ≥ÁâπÊÄßÔºöËá™ÊàëÁõ£Áù£È†êË®ìÁ∑¥„ÄÅ‰ªªÂãôÊµÅÊö¢Â∫¶ÂíåÂúñÂΩ¢ÊÑüÁü•„ÄÇÁÇ∫‰∫ÜËÄÉÈáèÈÄô‰∫õÁâπÊÄßÔºåÊàëÂÄëÂ∞áÂÇ≥Áµ±ÁöÑË™ûË®ÄÂª∫Ê®°Êì¥ÂÖÖÂà∞ÂúñÂΩ¢È†òÂüüÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÁîüÊàêÂºèÂúñÂΩ¢Ë™ûË®ÄÊ®°Âûã GOFA ‰æÜËß£Ê±∫ÂïèÈ°å„ÄÇÊ≠§Ê®°ÂûãÂ∞áÈö®Ê©üÂàùÂßãÂåñÁöÑ GNN Â±§‰∫§ÈåØÊèíÂÖ•ÂáçÁµêÁöÑÈ†êË®ìÁ∑¥ LLM ‰∏≠Ôºå‰ª•‰æøË™ûÊÑèÂíåÁµêÊßãÂª∫Ê®°ËÉΩÂäõÊúâÊ©üÁµêÂêà„ÄÇGOFA Êé°Áî®Êñ∞ÊèêÂá∫ÁöÑÂúñÂΩ¢Â±§Á¥ö‰∏ã‰∏ÄÂÄãÂ≠óÈ†êÊ∏¨„ÄÅÂïèÁ≠îÂíåÁµêÊßã‰ªªÂãôÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•ÂèñÂæó‰∏äËø∞ GFM ÁâπÊÄß„ÄÇÈ†êË®ìÁ∑¥Ê®°ÂûãÈÄ≤‰∏ÄÊ≠•Âú®‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åÂæÆË™øÔºå‰ª•ÂèñÂæóËß£Ê±∫‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÂæÆË™øÊ®°ÂûãÂú®ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåË≠âÊòé‰∫ÜÂú®Èõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠Ëß£Ê±∫ÁµêÊßãÂíå‰∏ä‰∏ãÊñáÂïèÈ°åÁöÑÂº∑Â§ßËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/JiaruiFeng/GOFA ÂèñÂæó„ÄÇ

##### **Human-like Episodic Memory for Infinite Context LLMs**
2407.09450v1 by Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang

Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs,
enabling them to effectively handle practically infinite context lengths while
maintaining computational efficiency. EM-LLM organises sequences of tokens into
coherent episodic events using a combination of Bayesian surprise and
graph-theoretic boundary refinement in an on-line fashion. When needed, these
events are retrieved through a two-stage memory process, combining
similarity-based and temporally contiguous retrieval for efficient and
human-like access to relevant information. Experiments on the LongBench dataset
demonstrate EM-LLM's superior performance, outperforming the state-of-the-art
InfLLM model with an overall relative improvement of 4.3% across various tasks,
including a 33% improvement on the PassageRetrieval task. Furthermore, our
analysis reveals strong correlations between EM-LLM's event segmentation and
human-perceived events, suggesting a bridge between this artificial system and
its biological counterpart. This work not only advances LLM capabilities in
processing extended contexts but also provides a computational framework for
exploring human memory mechanisms, opening new avenues for interdisciplinary
research in AI and cognitive science.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõÔºå‰ΩÜ‰ªçÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑËÑàÁµ°ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èï∑Â∫èÂàó‰∏≠Á∂≠ÊåÅÈÄ£Ë≤´ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁöÑËÉΩÂäõ„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºå‰∫∫ËÖ¶ÊìÖÈï∑Âú®Âª£Â§ßÁöÑÊôÇÈñìÂ∞∫Â∫¶‰∏äÁµÑÁπîÂíåÊèêÂèñÊÉÖÁØÄÈ´îÈ©óÔºåË∑®Ë∂ä‰∏ÄÁîü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü EM-LLMÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞á‰∫∫È°ûÊÉÖÁØÄË®òÊÜ∂Âíå‰∫ã‰ª∂Ë™çÁü•ÁöÑÈóúÈçµÈù¢ÂêëÊï¥ÂêàÂà∞ LLM ‰∏≠ÔºåËÆìÂÆÉÂÄëËÉΩÂ§†ÊúâÊïàÂú∞ËôïÁêÜÂØ¶Èöõ‰∏äÁÑ°ÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ÔºåÂêåÊôÇÁ∂≠ÊåÅÈÅãÁÆóÊïàÁéá„ÄÇEM-LLM ‰ΩøÁî®Ë≤ùÊ∞èÈ©öÂñúÂíåÂúñË´ñÈÇäÁïåÁ≤æÁÖâÁöÑÁµÑÂêàÔºå‰ª•Á∑ö‰∏äÊñπÂºèÂ∞áÂ∫èÂàóÊ®ôË®òÁµÑÁπîÊàêÈÄ£Ë≤´ÁöÑÊÉÖÁØÄ‰∫ã‰ª∂„ÄÇÂú®ÈúÄË¶ÅÊôÇÔºåÈÄô‰∫õ‰∫ã‰ª∂ÊúÉÈÄèÈÅéÂÖ©ÈöéÊÆµÁöÑË®òÊÜ∂ÈÅéÁ®ã‰æÜÊèêÂèñÔºåÁµêÂêàÂü∫ÊñºÁõ∏‰ººÊÄßÂíåÊôÇÈñìÈÄ£Á∫åÊÄßÁöÑÊèêÂèñÔºå‰ª•ÊúâÊïà‰∏îÈ°û‰ºº‰∫∫È°ûÁöÑÊñπÂºèÂ≠òÂèñÁõ∏ÈóúË≥áË®ä„ÄÇÂú® LongBench Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË≠âÊòé‰∫Ü EM-LLM ÁöÑÂçìË∂äÊïàËÉΩÔºåÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ InfLLM Ê®°ÂûãÔºåÂú® PassageRetrieval ‰ªªÂãô‰∏≠ÊîπÈÄ≤‰∫Ü 33%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü EM-LLM ÁöÑ‰∫ã‰ª∂ÂàÜÂâ≤Ëàá‰∫∫È°ûÊÑüÁü•‰∫ã‰ª∂‰πãÈñìÁöÑÂº∑Áõ∏ÈóúÊÄßÔºåÈ°ØÁ§∫‰∫ÜÈÄôÂÄã‰∫∫Â∑•Á≥ªÁµ±ËàáÂÖ∂ÁîüÁâ©Â∞çÊáâÁâ©‰πãÈñìÁöÑÊ©ãÊ®ë„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰∏çÂÉÖÊèêÂçá‰∫Ü LLM Âú®ËôïÁêÜÂª∂‰º∏ËÑàÁµ°ÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰πüÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈÅãÁÆóÊû∂Êßã‰æÜÊé¢Á¥¢‰∫∫È°ûË®òÊÜ∂Ê©üÂà∂ÔºåÁÇ∫ AI ÂíåË™çÁü•ÁßëÂ≠∏ÁöÑË∑®È†òÂüüÁ†îÁ©∂ÈñãÂïü‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇ

##### **The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**
2407.09441v1 by Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti

Graph neural networks form a class of deep learning architectures
specifically designed to work with graph-structured data. As such, they share
the inherent limitations and problems of deep learning, especially regarding
the issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,
an original domain-specific language for the specification of graph neural
networks that aims to overcome these issues. The language's syntax is
introduced, and its meaning is rigorously defined by a denotational semantics.
An equivalent characterization in the form of an operational semantics is also
provided and, together with a type system, is used to prove the type soundness
of $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be represented
in a more user-friendly graphical visualization, and provide examples of its
generality by showing how it can be used to define some of the most popular
graph neural network models, or to develop any custom graph processing
application.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂΩ¢Êàê‰∏ÄÈ°ûÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÁâπÂà•Ë®≠Ë®àÁî®ÊñºËôïÁêÜÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÂÆÉÂÄëÂÖ∑ÊúâÊ∑±Â∫¶Â≠∏ÁøíÂõ∫ÊúâÁöÑÈôêÂà∂ÂíåÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®ÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÂïèÈ°å‰∏ä„ÄÇÊàëÂÄëÊèêÂá∫ $\mu\mathcal{G}$Ôºå‰∏ÄÁ®ÆÁî®ÊñºÊåáÂÆöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂéüÂâµÈ†òÂüüÁâπÂÆöË™ûË®ÄÔºåÊó®Âú®ÂÖãÊúçÈÄô‰∫õÂïèÈ°å„ÄÇÂºïÂÖ•‰∫ÜË™ûË®ÄÁöÑË™ûÊ≥ïÔºå‰∏¶ÈÄèÈÅéÊåáÁ§∫Ë™ûÁæ©Âö¥Ê†ºÂÆöÁæ©ÂÖ∂Âê´Áæ©„ÄÇÈÇÑÊèê‰æõ‰∫ÜÈÅãÁÆóË™ûÁæ©ÂΩ¢ÂºèÁöÑÁ≠âÊïàÁâπÂæµÊèèËø∞Ôºå‰∏¶ËàáÈ°ûÂûãÁ≥ªÁµ±‰∏ÄËµ∑Áî®ÊñºË≠âÊòé $\mu\mathcal{G}$ ÁöÑÈ°ûÂûãÂÅ•ÂÖ®ÊÄß„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á $\mu\mathcal{G}$ Á®ãÂºèË°®Á§∫ÁÇ∫Êõ¥ÂèãÂñÑÁöÑÂúñÂΩ¢Ë¶ñË¶∫ÂåñÔºå‰∏¶ÈÄèÈÅéÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî®ÂÆÉÂÆöÁæ©‰∏Ä‰∫õÊúÄÊµÅË°åÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÊàñÈñãÁôº‰ªª‰ΩïËá™Ë®ÇÂúñÂΩ¢ËôïÁêÜÊáâÁî®Á®ãÂºèÔºå‰æÜÊèê‰æõÂÖ∂ÈÄöÁî®ÊÄßÁöÑÁØÑ‰æã„ÄÇ

##### **Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**
2407.08983v1 by David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk

Trustworthiness and interpretability are inextricably linked concepts for
LLMs. The more interpretable an LLM is, the more trustworthy it becomes.
However, current techniques for interpreting LLMs when applied to code-related
tasks largely focus on accuracy measurements, measures of how models react to
change, or individual task performance instead of the fine-grained explanations
needed at prediction time for greater interpretability, and hence trust. To
improve upon this status quo, this paper introduces ASTrust, an
interpretability method for LLMs of code that generates explanations grounded
in the relationship between model confidence and syntactic structures of
programming languages. ASTrust explains generated code in the context of syntax
categories based on Abstract Syntax Trees and aids practitioners in
understanding model predictions at both local (individual code snippets) and
global (larger datasets of code) levels. By distributing and assigning model
confidence scores to well-known syntactic structures that exist within ASTs,
our approach moves beyond prior techniques that perform token-level confidence
mapping by offering a view of model confidence that directly aligns with
programming language concepts with which developers are familiar. To put
ASTrust into practice, we developed an automated visualization that illustrates
the aggregated model confidence scores superimposed on sequence, heat-map, and
graph-based visuals of syntactic structures from ASTs. We examine both the
practical benefit that ASTrust can provide through a data science study on 12
popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust
through a human study.

ÊëòË¶ÅÔºöÂèØ‰ø°Â∫¶ÂíåÂèØËß£ÈáãÊÄßÊòØ LLM ‰∏≠ÂØÜ‰∏çÂèØÂàÜÁöÑÊ¶ÇÂøµ„ÄÇLLM ÁöÑÂèØËß£ÈáãÊÄßË∂äÈ´òÔºåÂÆÉÁöÑÂèØ‰ø°Â∫¶Â∞±Ë∂äÈ´ò„ÄÇÁÑ∂ËÄåÔºåÁï∂ÊáâÁî®ÊñºËàáÁ®ãÂºèÁ¢ºÁõ∏ÈóúÁöÑ‰ªªÂãôÊôÇÔºåÁõÆÂâçËß£Èáã LLM ÁöÑÊäÄË°ì‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê∫ñÁ¢∫ÊÄßÊ∏¨Èáè„ÄÅÊ®°ÂûãÂ∞çËÆäÂåñÁöÑÂèçÊáâÊ∏¨ÈáèÊàñÂÄãÂà•‰ªªÂãôË°®ÁèæÔºåËÄå‰∏çÊòØÂú®È†êÊ∏¨ÊôÇÈñìÊâÄÈúÄÁöÑÁ¥∞Á≤íÂ∫¶Ëß£ÈáãÔºåÂæûËÄåÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂõ†Ê≠§ÊèêÈ´ò‰ø°‰ªªÂ∫¶„ÄÇÁÇ∫‰∫ÜÊîπÂñÑÈÄôÁ®ÆÁèæÁãÄÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü ASTrustÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÁ®ãÂºèÁ¢º LLM ÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÔºåÂÆÉÊúÉÊ†πÊìöÊ®°Âûã‰ø°ÂøÉËàáÁ®ãÂºèË™ûË®ÄÁöÑË™ûÊ≥ïÁµêÊßã‰πãÈñìÁöÑÈóú‰øÇÁî¢ÁîüËß£Èáã„ÄÇASTrust Âú®Âü∫ÊñºÊäΩË±°Ë™ûÊ≥ïÊ®πÁöÑË™ûÊ≥ïÈ°ûÂà•ÁöÑ‰∏ä‰∏ãÊñá‰∏≠Ëß£ÈáãÁî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÔºå‰∏¶Âπ´Âä©ÂØ¶Âãô‰∫∫Âì°Âú®Â±ÄÈÉ®ÔºàÂÄãÂà•Á®ãÂºèÁ¢ºÁâáÊÆµÔºâÂíåÂÖ®ÂüüÔºàËºÉÂ§ßÁöÑÁ®ãÂºèÁ¢ºË≥áÊñôÈõÜÔºâÂ±§Á¥ö‰∫ÜËß£Ê®°ÂûãÈ†êÊ∏¨„ÄÇÈÄèÈÅéÂ∞áÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏ÂàÜÈÖçÂíåÊåáÂÆöÁµ¶ AST ‰∏≠Â≠òÂú®ÁöÑÁúæÊâÄÂë®Áü•ÁöÑË™ûÊ≥ïÁµêÊßãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïË∂ÖË∂ä‰∫ÜÂÖàÂâçÁöÑÊäÄË°ìÔºåÈÄô‰∫õÊäÄË°ìÈÄèÈÅéÊèê‰æõËàáÈñãÁôº‰∫∫Âì°ÁÜüÊÇâÁöÑÁ®ãÂºèË™ûË®ÄÊ¶ÇÂøµÁõ¥Êé•Â∞çÈΩäÁöÑÊ®°Âûã‰ø°ÂøÉË¶ñÂúñ‰æÜÂü∑Ë°å‰ª§ÁâåÁ¥öÂà•ÁöÑ‰ø°ÂøÉÂ∞çÊáâ„ÄÇÁÇ∫‰∫ÜÂØ¶Ë∏ê ASTrustÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñË¶ñË¶∫ÂåñÂ∑•ÂÖ∑ÔºåÂÆÉË™™Êòé‰∫ÜÁñäÂä†Âú® AST Ë™ûÊ≥ïÁµêÊßãÁöÑÂ∫èÂàó„ÄÅÁÜ±ÂúñÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑË¶ñË¶∫ÊïàÊûú‰∏äÁöÑËÅöÂêàÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏„ÄÇÊàëÂÄëÊ™¢Êü•‰∫Ü ASTrust ÂèØ‰ª•ÈÄèÈÅéÂ∞ç 12 ÂÄãÊµÅË°åÁöÑ LLM Âú®‰∏ÄÁµÑÁ≤æÈÅ∏ÁöÑ GitHub ÂÑ≤Â≠òÂ∫´‰∏äÈÄ≤Ë°åË≥áÊñôÁßëÂ≠∏Á†îÁ©∂Êèê‰æõÁöÑÂØ¶ÈöõÂ•ΩËôïÔºå‰ª•ÂèäÈÄèÈÅé‰∫∫È´îÁ†îÁ©∂Êèê‰æõÁöÑ ASTrust ÁöÑÊúâÁî®ÊÄß„ÄÇ

##### **Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**
2407.08959v1 by Ke Ji, Peng Wang, Wenjun Ke, Guozheng Li, Jiajun Liu, Jingsheng Gao, Ziyu Shang

Recently, various pre-trained language models (PLMs) have been proposed to
prove their impressive performances on a wide range of few-shot tasks. However,
limited by the unstructured prior knowledge in PLMs, it is difficult to
maintain consistent performance on complex structured scenarios, such as
hierarchical text classification (HTC), especially when the downstream data is
extremely scarce. The main challenge is how to transfer the unstructured
semantic space in PLMs to the downstream domain hierarchy. Unlike previous work
on HTC which directly performs multi-label classification or uses graph neural
network (GNN) to inject label hierarchy, in this work, we study the HTC problem
under a few-shot setting to adapt knowledge in PLMs from an unstructured manner
to the downstream hierarchy. Technically, we design a simple yet effective
method named Hierarchical Iterative Conditional Random Field (HierICRF) to
search the most domain-challenging directions and exquisitely crafts
domain-hierarchy adaptation as a hierarchical iterative language modeling
problem, and then it encourages the model to make hierarchical consistency
self-correction during the inference, thereby achieving knowledge transfer with
hierarchical consistency preservation. We perform HierICRF on various
architectures, and extensive experiments on two popular HTC datasets
demonstrate that prompt with HierICRF significantly boosts the few-shot HTC
performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%
to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot
settings, while remaining SOTA hierarchical consistency performance.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂ∑≤ÁªèÊèêÂá∫‰∫ÜÂ§öÁßçÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã (PLM)Ôºå‰ª•ËØÅÊòéÂÆÉ‰ª¨Âú®ÂπøÊ≥õÁöÑÂ∞ëÈáèÊ†∑Êú¨‰ªªÂä°‰∏äÂÖ∑Êúâ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑÂÖàÈ™åÁü•ËØÜÂèóÂà∞ÈôêÂà∂ÔºåÂõ†Ê≠§Èöæ‰ª•Âú®Â§çÊùÇÁªìÊûÑÂåñÂú∫ÊôØÔºà‰æãÂ¶ÇÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª (HTC)Ôºâ‰∏≠‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏ãÊ∏∏Êï∞ÊçÆÊûÅÂÖ∂Á®ÄÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊàòÊòØÂ¶Ç‰ΩïÂ∞Ü PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑËØ≠‰πâÁ©∫Èó¥ËΩ¨ÁßªÂà∞‰∏ãÊ∏∏ÂüüÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰∏é‰ª•ÂâçÁõ¥Êé•ÊâßË°åÂ§öÊ†áÁ≠æÂàÜÁ±ªÊàñ‰ΩøÁî®ÂõæÁ•ûÁªèÁΩëÁªú (GNN) Ê≥®ÂÖ•Ê†áÁ≠æÂ±ÇÊ¨°ÁªìÊûÑÁöÑ HTC Â∑•‰Ωú‰∏çÂêåÔºåÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãÁ†îÁ©∂ HTC ÈóÆÈ¢òÔºå‰ª•Â∞Ü PLM ‰∏≠ÁöÑÁü•ËØÜ‰ªéÈùûÁªìÊûÑÂåñÊñπÂºèÈÄÇÂ∫îÂà∞‰∏ãÊ∏∏Â±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰ªéÊäÄÊúØ‰∏äËÆ≤ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Â±ÇÊ¨°Ëø≠‰ª£Êù°‰ª∂ÈöèÊú∫Âú∫ (HierICRF)Ôºå‰ª•ÊêúÁ¥¢ÊúÄÂÖ∑È¢ÜÂüüÊåëÊàòÊÄßÁöÑÊñπÂêëÔºåÂπ∂Á≤æÁªÜÂú∞Â∞ÜÈ¢ÜÂüüÂ±ÇÊ¨°ÁªìÊûÑÈÄÇÂ∫î‰Ωú‰∏∫ÂàÜÂ±ÇËø≠‰ª£ËØ≠Ë®ÄÂª∫Ê®°ÈóÆÈ¢òÔºåÁÑ∂ÂêéÂÆÉÈºìÂä±Ê®°ÂûãÂú®Êé®ÁêÜÊúüÈó¥ËøõË°åÂ±ÇÊ¨°‰∏ÄËá¥ÊÄßËá™ÊàëÊ†°Ê≠£Ôºå‰ªéËÄåÂÆûÁé∞ÂÖ∑ÊúâÂ±ÇÊ¨°‰∏ÄËá¥ÊÄß‰øùÁïôÁöÑÁü•ËØÜËΩ¨Áßª„ÄÇÊàë‰ª¨Âú®ÂêÑÁßçÊû∂ÊûÑ‰∏äÊâßË°å HierICRFÔºåÂú®‰∏§‰∏™ÊµÅË°åÁöÑ HTC Êï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî® HierICRF ÁöÑÊèêÁ§∫ÊòæÁùÄÊèêÈ´ò‰∫ÜÂ∞ëÈáèÊ†∑Êú¨ HTC ÊÄßËÉΩÔºåÂπ≥Âùá Micro-F1 ‰ªé 28.80% ÊèêÈ´òÂà∞ 1.50%ÔºåMacro-F1 ‰ªé 36.29% ÊèêÈ´òÂà∞ 1.5% Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãË∂ÖËøá‰∫Ü‰ª•ÂâçÊúÄÂÖàËøõ (SOTA) Âü∫ÂáÜÔºåÂêåÊó∂‰øùÊåÅ SOTA Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊÄßËÉΩ„ÄÇ</paragraph>

##### **Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**
2407.08694v1 by Zhiqiang Xie, Yujia Zheng, Lizi Ottens, Kun Zhang, Christos Kozyrakis, Jonathan Mace

Runtime failure and performance degradation is commonplace in modern cloud
systems. For cloud providers, automatically determining the root cause of
incidents is paramount to ensuring high reliability and availability as prompt
fault localization can enable faster diagnosis and triage for timely
resolution. A compelling solution explored in recent work is causal reasoning
using causal graphs to capture relationships between varied cloud system
performance metrics. To be effective, however, systems developers must
correctly define the causal graph of their system, which is a time-consuming,
brittle, and challenging task that increases in difficulty for large and
dynamic systems and requires domain expertise. Alternatively, automated
data-driven approaches have limited efficacy for cloud systems due to the
inherent rarity of incidents. In this work, we present Atlas, a novel approach
to automatically synthesizing causal graphs for cloud systems. Atlas leverages
large language models (LLMs) to generate causal graphs using system
documentation, telemetry, and deployment feedback. Atlas is complementary to
data-driven causal discovery techniques, and we further enhance Atlas with a
data-driven validation step. We evaluate Atlas across a range of fault
localization scenarios and demonstrate that Atlas is capable of generating
causal graphs in a scalable and generalizable manner, with performance that far
surpasses that of data-driven algorithms and is commensurate to the
ground-truth baseline.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£Èõ≤Á´ØÁ≥ªÁµ±‰∏≠ÔºåÂü∑Ë°åÊôÇÊúüÊïÖÈöúÂíåÊïàËÉΩÈôç‰ΩéÊòØÂè∏Á©∫Ë¶ãÊÖ£ÁöÑ‰∫ã„ÄÇÂ∞çÊñºÈõ≤Á´Ø‰æõÊáâÂïÜËÄåË®ÄÔºåËá™ÂãïÊâæÂá∫‰∫ã‰ª∂ÁöÑÊ†πÊú¨ÂéüÂõ†Â∞çÊñºÁ¢∫‰øùÈ´òÂèØÈù†ÊÄßÂíåÂèØÁî®ÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂèäÊôÇÁöÑÊïÖÈöúÂÆö‰ΩçÂèØ‰ª•ËÆìË®∫Êñ∑ÂíåÂàÜÈ°ûÊõ¥Âø´ÈÄüÔºå‰ª•Âà©ÊñºÂèäÊôÇËß£Ê±∫ÂïèÈ°å„ÄÇÊúÄËøëÁöÑÂ∑•‰Ωú‰∏≠Êé¢Ë®é‰∫Ü‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂç≥‰ΩøÁî®Âõ†ÊûúÂúñ‰æÜÊì∑ÂèñÂêÑÁ®ÆÈõ≤Á´ØÁ≥ªÁµ±ÊïàËÉΩÊåáÊ®ô‰πãÈñìÈóú‰øÇÁöÑÂõ†ÊûúÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÁ≥ªÁµ±ÈñãÁôº‰∫∫Âì°ÂøÖÈ†àÊ≠£Á¢∫ÂÆöÁæ©ÂÖ∂Á≥ªÁµ±ÁöÑÂõ†ÊûúÂúñÊâçËÉΩÁôºÊèÆÊïàÁî®ÔºåËÄåÈÄôÈ†Ö‰ªªÂãôËÄóÊôÇ„ÄÅËÑÜÂº±‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂ∞çÊñºÂ§ßÂûã‰∏îÂãïÊÖãÁöÑÁ≥ªÁµ±ËÄåË®ÄÈõ£Â∫¶Êõ¥È´òÔºåËÄå‰∏îÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂Áü•Ë≠ò„ÄÇÊàñËÄÖÔºåÁî±Êñº‰∫ã‰ª∂ÁöÑÂõ∫ÊúâÁ®ÄÂ∞ëÊÄßÔºåËá™ÂãïÂåñË≥áÊñôÈ©ÖÂãïÊñπÊ≥ïÂ∞çÊñºÈõ≤Á´ØÁ≥ªÁµ±ÁöÑÊïàÂäõÊúâÈôê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ AtlasÔºå‰∏ÄÁ®ÆËá™ÂãïÂêàÊàêÈõ≤Á´ØÁ≥ªÁµ±Âõ†ÊûúÂúñÁöÑÊñ∞ÊñπÊ≥ï„ÄÇAtlas Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®Á≥ªÁµ±Êñá‰ª∂„ÄÅÈÅôÊ∏¨ÂíåÈÉ®ÁΩ≤ÂõûÈ•ã‰æÜÁî¢ÁîüÂõ†ÊûúÂúñ„ÄÇAtlas ÊòØË≥áÊñôÈ©ÖÂãïÂõ†ÊûúÁôºÁèæÊäÄË°ìÁöÑË£úÂÖÖÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®Ë≥áÊñôÈ©ÖÂãïÈ©óË≠âÊ≠•È©ü‰æÜÂ¢ûÂº∑ Atlas„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÊïÖÈöúÂÆö‰ΩçÊÉÖÂ¢É‰∏≠Ë©ï‰º∞ AtlasÔºå‰∏¶Ë≠âÊòé Atlas ËÉΩÂ§†‰ª•ÂèØÊì¥ÂÖÖ‰∏îÂèØÊ¶ÇÂåñÁöÑÊñπÂºèÁî¢ÁîüÂõ†ÊûúÂúñÔºåÂÖ∂ÊïàËÉΩÈÅ†ÈÅ†Ë∂ÖÈÅéË≥áÊñôÈ©ÖÂãïÊºîÁÆóÊ≥ïÔºå‰∏¶‰∏îËàáÁúüÂØ¶Âü∫Á∑öÁõ∏Áï∂„ÄÇ

##### **Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**
2407.08516v3 by Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, Laura E. Barnes

This article explores the convergence of connectionist and symbolic
artificial intelligence (AI), from historical debates to contemporary
advancements. Traditionally considered distinct paradigms, connectionist AI
focuses on neural networks, while symbolic AI emphasizes symbolic
representation and logic. Recent advancements in large language models (LLMs),
exemplified by ChatGPT and GPT-4, highlight the potential of connectionist
architectures in handling human language as a form of symbols. The study argues
that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.
By utilizing LLMs for text-based knowledge modeling and representation, LAAs
integrate neuro-symbolic AI principles, showcasing enhanced reasoning and
decision-making capabilities. Comparing LAAs with Knowledge Graphs within the
neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking
human-like reasoning processes, scaling effectively with large datasets, and
leveraging in-context samples without explicit re-training. The research
underscores promising avenues in neuro-vector-symbolic integration,
instructional encoding, and implicit reasoning, aimed at further enhancing LAA
capabilities. By exploring the progression of neuro-symbolic AI and proposing
future research trajectories, this work advances the understanding and
development of AI technologies.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÈÄ£Á∑ö‰∏ªÁæ©ËàáÁ¨¶Ëôü‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑËûçÂêàÔºåÂæûÊ≠∑Âè≤ËæØË´ñÂà∞Áï∂‰ª£ÈÄ≤Â±ï„ÄÇÈÄ£Á∑ö‰∏ªÁæ© AI ÂÇ≥Áµ±‰∏äË¢´Ë¶ñÁÇ∫‰∏çÂêåÁöÑÁØÑ‰æãÔºåÂ∞àÊ≥®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåËÄåÁ¨¶Ëôü AI ÂâáÂº∑Ë™øÁ¨¶ËôüË°®ÂæµÂíåÈÇèËºØ„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰ª• ChatGPT Âíå GPT-4 ÁÇ∫‰æãÔºåÁ™ÅÈ°Ø‰∫ÜÈÄ£Á∑ö‰∏ªÁæ©Êû∂ÊßãÂú®Â∞á‰∫∫È°ûË™ûË®ÄË¶ñÁÇ∫Á¨¶ËôüÂΩ¢ÂºèËôïÁêÜÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁ†îÁ©∂Ë™çÁÇ∫ÔºåÁî± LLM Ë≥¶ËÉΩÁöÑËá™‰∏ª‰ª£ÁêÜ (LAA) È´îÁèæ‰∫ÜÈÄôÁ®ÆÁØÑ‰æãËûçÂêà„ÄÇÈÄèÈÅéÂà©Áî® LLM ÈÄ≤Ë°åÂü∫ÊñºÊñáÂ≠óÁöÑÁü•Ë≠òÂª∫Ê®°ÂíåË°®ÂæµÔºåLAA Êï¥Âêà‰∫ÜÁ•ûÁ∂ìÁ¨¶Ëôü AI ÂéüÂâáÔºåÂ±ïÁ§∫‰∫ÜÂ¢ûÂº∑ÁöÑÊé®ÁêÜÂíåÊ±∫Á≠ñËÉΩÂäõ„ÄÇÂú®Á•ûÁ∂ìÁ¨¶Ëôü AI ‰∏ªÈ°å‰∏≠ÊØîËºÉ LAA ËàáÁü•Ë≠òÂúñË≠úÔºåÁ™ÅÈ°Ø‰∫Ü LAA Âú®Ê®°Êì¨È°û‰∫∫Êé®ÁêÜÈÅéÁ®ã„ÄÅÊúâÊïàÊì¥ÂÖÖÂ§ßÂûãË≥áÊñôÈõÜ‰ª•ÂèäÂà©Áî®ÊÉÖÂ¢ÉÁØÑ‰æãËÄåÁÑ°ÈúÄÊòéÁ¢∫ÈáçÊñ∞Ë®ìÁ∑¥ÊñπÈù¢ÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÁ•ûÁ∂ìÂêëÈáèÁ¨¶ËôüÊï¥Âêà„ÄÅÊåá‰ª§Á∑®Á¢ºÂíåÈö±ÂºèÊé®ÁêÜ‰∏≠ÂâçÊôØÁúãÂ•ΩÁöÑÈÄîÂæëÔºåÊó®Âú®ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑ LAA ÁöÑËÉΩÂäõ„ÄÇÈÄèÈÅéÊé¢Ë®éÁ•ûÁ∂ìÁ¨¶Ëôü AI ÁöÑÈÄ≤Â±ï‰∏¶ÊèêÂá∫Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊé®ÈÄ≤‰∫ÜÂ∞ç AI ÊäÄË°ìÁöÑÁêÜËß£ÂíåÁôºÂ±ï„ÄÇ

##### **A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**
2407.07966v1 by Arastoo Zibaeirad, Farnoosh Koleini, Shengping Bi, Tao Hou, Tao Wang

In this study, we conduct a comprehensive review of smart grid security,
exploring system architectures, attack methodologies, defense strategies, and
future research opportunities. We provide an in-depth analysis of various
attack vectors, focusing on new attack surfaces introduced by advanced
components in smart grids. The review particularly includes an extensive
analysis of coordinated attacks that incorporate multiple attack strategies and
exploit vulnerabilities across various smart grid components to increase their
adverse impact, demonstrating the complexity and potential severity of these
threats. Following this, we examine innovative detection and mitigation
strategies, including game theory, graph theory, blockchain, and machine
learning, discussing their advancements in counteracting evolving threats and
associated research challenges. In particular, our review covers a thorough
examination of widely used machine learning-based mitigation strategies,
analyzing their applications and research challenges spanning across
supervised, unsupervised, semi-supervised, ensemble, and reinforcement
learning. Further, we outline future research directions and explore new
techniques and concerns. We first discuss the research opportunities for
existing and emerging strategies, and then explore the potential role of new
techniques, such as large language models (LLMs), and the emerging threat of
adversarial machine learning in the future of smart grid security.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÊô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®ÊÄßÈÄ≤Ë°åÂÖ®Èù¢Ê™¢Ë¶ñÔºåÊé¢Ë®éÁ≥ªÁµ±Êû∂Êßã„ÄÅÊîªÊìäÊñπÊ≥ï„ÄÅÈò≤Á¶¶Á≠ñÁï•ÂíåÊú™‰æÜÁöÑÁ†îÁ©∂Ê©üÊúÉ„ÄÇÊàëÂÄëÊ∑±ÂÖ•ÂàÜÊûêÂêÑÁ®ÆÊîªÊìäÂ™í‰ªãÔºåÂ∞àÊ≥®ÊñºÊô∫ÊÖßÈõªÁ∂≤‰∏≠ÂÖàÈÄ≤ÁµÑ‰ª∂ÊâÄÂºïÂÖ•ÁöÑÊñ∞ÊîªÊìäÈù¢„ÄÇÊú¨Ê™¢Ë¶ñÁâπÂà•ÂåÖÂê´Â∞çÂçîË™øÊîªÊìäÁöÑÂª£Ê≥õÂàÜÊûêÔºåÂÖ∂‰∏≠ÂåÖÂê´Â§öÁ®ÆÊîªÊìäÁ≠ñÁï•‰∏¶Âà©Áî®ÂêÑÁ®ÆÊô∫ÊÖßÈõªÁ∂≤ÁµÑ‰ª∂‰∏≠ÁöÑÊºèÊ¥û‰æÜÂ¢ûÂä†ÂÖ∂Ë≤†Èù¢ÂΩ±ÈüøÔºåÂ±ïÁ§∫ÈÄô‰∫õÂ®ÅËÑÖÁöÑË§áÈõúÊÄßÂíåÊΩõÂú®Âö¥ÈáçÊÄß„ÄÇÂú®Ê≠§‰πãÂæåÔºåÊàëÂÄëÊé¢Ë®éÂâµÊñ∞ÁöÑÂÅµÊ∏¨ÂíåÁ∑©Ëß£Á≠ñÁï•ÔºåÂåÖÊã¨ÂçöÂºàË´ñ„ÄÅÂúñË´ñ„ÄÅÂçÄÂ°äÈèàÂíåÊ©üÂô®Â≠∏ÁøíÔºåË®éË´ñÂÆÉÂÄëÂú®Â∞çÊäó‰∏çÊñ∑ÊºîËÆäÁöÑÂ®ÅËÑÖÂíåÁõ∏ÈóúÁ†îÁ©∂ÊåëÊà∞ÊñπÈù¢ÁöÑÈÄ≤Â±ï„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊ™¢Ë¶ñÊ∂µËìãÂ∞çÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑÁ∑©Ëß£Á≠ñÁï•ÁöÑÂæπÂ∫ïÊ™¢È©óÔºåÂàÜÊûêÂÆÉÂÄëÂú®Áõ£Áù£Âºè„ÄÅÈùûÁõ£Áù£Âºè„ÄÅÂçäÁõ£Áù£Âºè„ÄÅÊï¥È´îÂºèÂíåÂº∑ÂåñÂ≠∏Áøí‰∏≠ÁöÑÊáâÁî®ÂíåÁ†îÁ©∂ÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ¶ÇËø∞Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë‰∏¶Êé¢Ë®éÊñ∞ÊäÄË°ìÂíåÂïèÈ°å„ÄÇÊàëÂÄëÈ¶ñÂÖàË®éË´ñÁèæÊúâÂíåÊñ∞ËààÁ≠ñÁï•ÁöÑÁ†îÁ©∂Ê©üÊúÉÔºåÁÑ∂ÂæåÊé¢Ë®éÊñ∞ÊäÄË°ìÁöÑÊΩõÂú®‰ΩúÁî®Ôºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•ÂèäÂ∞çÊäóÂºèÊ©üÂô®Â≠∏ÁøíÂú®Êô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®Êú™‰æÜÁöÑÂ®ÅËÑÖ„ÄÇ

##### **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**
2407.07775v2 by Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, Jie Tan

An elusive goal in navigation research is to build an intelligent agent that
can understand multimodal instructions including natural language and image,
and perform useful navigation. To achieve this, we study a widely useful
category of navigation tasks we call Multimodal Instruction Navigation with
demonstration Tours (MINT), in which the environment prior is provided through
a previously recorded demonstration video. Recent advances in Vision Language
Models (VLMs) have shown a promising path in achieving this goal as it
demonstrates capabilities in perceiving and reasoning about multimodal inputs.
However, VLMs are typically trained to predict textual output and it is an open
research question about how to best utilize them in navigation. To solve MINT,
we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation
policy that combines the environment understanding and common sense reasoning
power of long-context VLMs and a robust low-level navigation policy based on
topological graphs. The high-level policy consists of a long-context VLM that
takes the demonstration tour video and the multimodal user instruction as input
to find the goal frame in the tour video. Next, a low-level policy uses the
goal frame and an offline constructed topological graph to generate robot
actions at every timestep. We evaluated Mobility VLA in a 836m^2 real world
environment and show that Mobility VLA has a high end-to-end success rates on
previously unsolved multimodal instructions such as "Where should I return
this?" while holding a plastic bin. A video demonstrating Mobility VLA can be
found here: https://youtu.be/-Tof__Q8_5s

ÊëòË¶ÅÔºö<paragraph>Â∞éËà™Á†îÁ©∂‰∏≠‰∏ÄÂÄãÈõ£‰ª•ÊçâÊë∏ÁöÑÁõÆÊ®ôÔºåÊòØÂª∫Á´ã‰∏ÄÂÄãÊô∫ËÉΩ‰ª£ÁêÜÔºåÂÆÉÂèØ‰ª•ÁêÜËß£ÂåÖÊã¨Ëá™ÁÑ∂Ë™ûË®ÄÂíåÂΩ±ÂÉèÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºå‰∏¶Âü∑Ë°åÊúâÁî®ÁöÑÂ∞éËà™„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÊ≠§ÁõÆÊ®ôÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏ÄÈ°ûÂª£Ê≥õÊúâÁî®ÁöÑÂ∞éËà™‰ªªÂãôÔºåÊàëÂÄëÁ®±‰πãÁÇ∫Á§∫ÁØÑÂ∞éË¶ΩÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Â∞éËà™ (MINT)ÔºåÂÖ∂‰∏≠Áí∞Â¢ÉÂÖàÈ©óÊòØÈÄèÈÅéÂÖàÂâçÈåÑË£ΩÁöÑÁ§∫ÁØÑÂΩ±ÁâáÊèê‰æõÁöÑ„ÄÇË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑËøëÊúüÈÄ≤Â±ïÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÊ¢ùÂØ¶ÁèæÊ≠§ÁõÆÊ®ôÁöÑÊúâÂâçÊôØË∑ØÂæëÔºåÂõ†ÁÇ∫ÂÆÉÂ±ïÁ§∫‰∫ÜÊÑüÁü•ÂíåÊé®ÁêÜÂ§öÊ®°ÊÖãËº∏ÂÖ•ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåVLM ÈÄöÂ∏∏Ë®ìÁ∑¥Áî®ÊñºÈ†êÊ∏¨ÊñáÂ≠óËº∏Âá∫ÔºåËÄåÂ¶Ç‰ΩïÊúÄ‰Ω≥Âà©Áî®ÂÆÉÂÄëÈÄ≤Ë°åÂ∞éËà™ÔºåÂâáÊòØ‰∏ÄÂÄãÈñãÊîæÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ MINTÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Mobility VLAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂàÜÂ±§ÁöÑË¶ñË¶∫-Ë™ûË®Ä-Âãï‰Ωú (VLA) Â∞éËà™ÊîøÁ≠ñÔºåÂÆÉÁµêÂêà‰∫ÜÈï∑Ë™ûÂ¢É VLM ÁöÑÁí∞Â¢ÉÁêÜËß£ÂíåÂ∏∏Ë≠òÊé®ÁêÜËÉΩÂäõÔºå‰ª•ÂèäÂü∫ÊñºÊãìÊí≤ÂúñÁöÑÂº∑ÂÅ•‰ΩéÈöéÂ∞éËà™ÊîøÁ≠ñ„ÄÇÈ´òÈöéÊîøÁ≠ñÂåÖÂê´‰∏ÄÂÄãÈï∑Ë™ûÂ¢É VLMÔºåÂÆÉÊé°Áî®Á§∫ÁØÑÂ∞éË¶ΩÂΩ±ÁâáÂíåÂ§öÊ®°ÊÖã‰ΩøÁî®ËÄÖÊåá‰ª§‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰ª•Âú®Â∞éË¶ΩÂΩ±Áâá‰∏≠ÊâæÂà∞ÁõÆÊ®ôÂπÄ„ÄÇÊé•‰∏ã‰æÜÔºå‰ΩéÈöéÊîøÁ≠ñ‰ΩøÁî®ÁõÆÊ®ôÂπÄÂíåÈõ¢Á∑öÂª∫ÊßãÁöÑÊãìÊí≤ÂúñÔºåÂú®ÊØèÂÄãÊôÇÈñìÊ≠•Áî¢ÁîüÊ©üÂô®‰∫∫Âãï‰Ωú„ÄÇÊàëÂÄëÂú® 836 Âπ≥ÊñπÂÖ¨Â∞∫ÁöÑÁúüÂØ¶‰∏ñÁïåÁí∞Â¢É‰∏≠Ë©ï‰º∞‰∫Ü Mobility VLAÔºå‰∏¶Â±ïÁ§∫‰∫Ü Mobility VLA Âú®ÂÖàÂâçÊú™Ëß£Ê±∫ÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºà‰æãÂ¶Ç„ÄåÊàëÊáâË©≤ÊääÈÄôÂÄãÂ°ëËÜ†ÁÆ±Ê≠∏ÈÇÑÂà∞Âì™Ë£°Ôºü„ÄçÔºâ‰∏äÂÖ∑ÊúâÂæàÈ´òÁöÑÁ´ØÂà∞Á´ØÊàêÂäüÁéáÔºåÂêåÊôÇÊãøËëó‰∏ÄÂÄãÂ°ëËÜ†ÁÆ±„ÄÇÂ±ïÁ§∫ Mobility VLA ÁöÑÂΩ±ÁâáÂèØ‰ª•Âú®ÈÄôË£°ÊâæÂà∞Ôºöhttps://youtu.be/-Tof__Q8_5s</paragraph>

##### **Teaching Transformers Causal Reasoning through Axiomatic Training**
2407.07612v1 by Aniket Vashishtha, Abhinav Kumar, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian, Amit Sharma

For text-based AI systems to interact in the real world, causal reasoning is
an essential skill. Since interventional data is costly to generate, we study
to what extent an agent can learn causal reasoning from passive data.
Specifically, we consider an axiomatic training setup where an agent learns
from multiple demonstrations of a causal axiom (or rule), rather than
incorporating the axiom as an inductive bias or inferring it from data values.
A key question is whether the agent would learn to generalize from the axiom
demonstrations to new scenarios. For example, if a transformer model is trained
on demonstrations of the causal transitivity axiom over small graphs, would it
generalize to applying the transitivity axiom over large graphs? Our results,
based on a novel axiomatic training scheme, indicate that such generalization
is possible. We consider the task of inferring whether a variable causes
another variable, given a causal graph structure. We find that a 67 million
parameter transformer model, when trained on linear causal chains (along with
some noisy variations) can generalize well to new kinds of graphs, including
longer causal chains, causal chains with reversed order, and graphs with
branching; even when it is not explicitly trained for such settings. Our model
performs at par (or even better) than many larger language models such as
GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework
provides a new paradigm of learning causal reasoning from passive data that can
be used to learn arbitrary axioms, as long as sufficient demonstrations can be
generated.

ÊëòË¶ÅÔºö<paragraph>Â∞çÊñºÂü∫ÊñºÊñáÂ≠óÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ËàáÁúüÂØ¶‰∏ñÁïå‰∫íÂãï‰æÜË™™ÔºåÂõ†ÊûúÊé®ÁêÜÊòØ‰∏ÄÈ†ÖÂøÖË¶ÅÁöÑÊäÄËÉΩ„ÄÇÁî±Êñº‰ªãÂÖ•Ë≥áÊñôÁöÑÁî¢ÁîüÊàêÊú¨ÂæàÈ´òÔºåÊàëÂÄëÁ†îÁ©∂‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÁ®ãÂ∫¶„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëËÄÉÊÖÆ‰∏ÄÂÄãÂÖ¨ÁêÜË®ìÁ∑¥Ë®≠ÁΩÆÔºåÂÖ∂‰∏≠‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûÂõ†ÊûúÂÖ¨ÁêÜÔºàÊàñË¶èÂâáÔºâÁöÑÂ§öÂÄãÁ§∫ÁØÑ‰∏≠Â≠∏ÁøíÔºåËÄå‰∏çÊòØÂ∞áÂÖ¨ÁêÜ‰ΩúÁÇ∫Ê≠∏Á¥çÂÅèË™§ÊàñÂæûË≥áÊñôÂÄº‰∏≠Êé®Êñ∑Âá∫‰æÜ„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØ‰ª£ÁêÜ‰∫∫ÊòØÂê¶ÊúÉÂ≠∏ÊúÉÂæûÂÖ¨ÁêÜÁ§∫ÁØÑÊé®Âª£Âà∞Êñ∞ÁöÑÂ†¥ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûú‰∏ÄÂÄãTransformerÊ®°ÂûãÂú®Â∞èÂúñË°®‰∏äÂõ†ÊûúÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÁöÑÁ§∫ÁØÑ‰∏≠Êé•ÂèóË®ìÁ∑¥ÔºåÂÆÉÊòØÂê¶ÊúÉÊé®Âª£Âà∞Âú®Â§ßÂúñË°®‰∏äÊáâÁî®ÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÔºüÊàëÂÄëÁöÑÁµêÊûúÂü∫Êñº‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ¨ÁêÜË®ìÁ∑¥ÊñπÊ°àÔºåË°®ÊòéÈÄôÊ®£ÁöÑÊ¶ÇÊã¨ÊòØÂèØËÉΩÁöÑ„ÄÇÊàëÂÄëËÄÉÊÖÆÊé®Ë´ñ‰∏ÄÂÄãËÆäÊï∏ÊòØÂê¶Â∞éËá¥Âè¶‰∏ÄÂÄãËÆäÊï∏ÁöÑ‰ªªÂãôÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂõ†ÊûúÂúñÁµêÊßã„ÄÇÊàëÂÄëÁôºÁèæ‰∏ÄÂÄã 6700 Ëê¨ÂÄãÂèÉÊï∏ÁöÑTransformerÊ®°ÂûãÔºåÂú®Á∑öÊÄßÂõ†ÊûúÈèàÔºà‰ª•Âèä‰∏Ä‰∫õÈõúË®äËÆäÂåñÔºâ‰∏äË®ìÁ∑¥ÊôÇÔºåÂèØ‰ª•ÂæàÂ•ΩÂú∞Ê¶ÇÊã¨Âà∞Êñ∞È°ûÂûãÁöÑÂúñÂΩ¢ÔºåÂåÖÊã¨Êõ¥Èï∑ÁöÑÂõ†ÊûúÈèà„ÄÅÈ†ÜÂ∫èÁõ∏ÂèçÁöÑÂõ†ÊûúÈèàÂíåÂÖ∑ÊúâÂàÜÊîØÁöÑÂúñÂΩ¢ÔºõÂç≥‰ΩøÂÆÉÊ≤íÊúâÈáùÂ∞çÊ≠§È°ûË®≠ÁΩÆÈÄ≤Ë°åÊòéÁ¢∫Ë®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæËàáË®±Â§öËºÉÂ§ßÁöÑË™ûË®ÄÊ®°ÂûãÔºà‰æãÂ¶Ç GPT-4„ÄÅGemini Pro Âíå Phi-3ÔºâÁõ∏Áï∂ÔºàÁîöËá≥Êõ¥Â•ΩÔºâ„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂÖ¨ÁêÜË®ìÁ∑¥Ê°ÜÊû∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÊñ∞ÁØÑ‰æãÔºåÂè™Ë¶ÅÂèØ‰ª•Áî¢ÁîüË∂≥Â§†ÁöÑÁ§∫ÁØÑÔºåÂ∞±ÂèØ‰ª•Áî®ÊñºÂ≠∏Áøí‰ªªÊÑèÂÖ¨ÁêÜ„ÄÇ</paragraph>

##### **STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**
2407.12860v1 by Aaron Zolnai-Lucas, Jack Boylan, Chris Hokamp, Parsa Ghaffari

We present Simplified Text-Attributed Graph Embeddings (STAGE), a
straightforward yet effective method for enhancing node features in Graph
Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our
approach leverages Large-Language Models (LLMs) to generate embeddings for
textual attributes. STAGE achieves competitive results on various node
classification benchmarks while also maintaining a simplicity in implementation
relative to current state-of-the-art (SoTA) techniques. We show that utilizing
pre-trained LLMs as embedding generators provides robust features for ensemble
GNN training, enabling pipelines that are simpler than current SoTA approaches
which require multiple expensive training and prompting stages. We also
implement diffusion-pattern GNNs in an effort to make this pipeline scalable to
graphs beyond academic benchmarks.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜÁ∞°ÂåñÊñáÂ≠óÂ±¨ÊÄßÂúñÂµåÂÖ• (STAGE)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ¢ûÂº∑ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Ê®°Âûã‰∏≠ÁöÑÁØÄÈªûÁâπÂæµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÁ∑®Á¢ºÊñáÂ≠óÂ±¨ÊÄßÂúñ (TAG)„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁÇ∫ÊñáÂ≠óÂ±¨ÊÄßÁî¢ÁîüÂµåÂÖ•„ÄÇSTAGE Âú®ÂêÑÁ®ÆÁØÄÈªûÂàÜÈ°ûÂü∫Ê∫ñ‰∏äÂèñÂæó‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºåÂêåÊôÇÂú®ÂØ¶‰Ωú‰∏ä‰πüÁ∂≠ÊåÅ‰∫ÜÁ∞°ÊΩîÊÄßÔºåÁõ∏ËºÉÊñºÁõÆÂâçÁöÑÊäÄË°ìÊ∞¥Ê∫ñ (SoTA)„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®È†êË®ìÁ∑¥ÁöÑ LLM ‰ΩúÁÇ∫ÂµåÂÖ•Áî¢ÁîüÂô®ÔºåÂèØÁÇ∫Êï¥È´î GNN Ë®ìÁ∑¥Êèê‰æõÂº∑ÂÅ•ÁöÑÁâπÂæµÔºåÈÄ≤ËÄåÂª∫ÊßãÊØîÁõÆÂâç SoTA ÂÅöÊ≥ïÊõ¥Á∞°ÂñÆÁöÑÁÆ°ÈÅìÔºåËÄåÂæåËÄÖÈúÄË¶ÅÂ§öÂÄãÊòÇË≤¥ÁöÑË®ìÁ∑¥ÂíåÊèêÁ§∫ÈöéÊÆµ„ÄÇÊàëÂÄë‰πüÂØ¶‰Ωú‰∫ÜÊì¥Êï£Ê®°Âºè GNNÔºå‰ª•ÊúüËÆìÈÄôÂÄãÁÆ°ÈÅìËÉΩÊì¥ÂÖÖÂà∞Â≠∏Ë°ìÂü∫Ê∫ñ‰πãÂ§ñÁöÑÂúñÂΩ¢„ÄÇ

##### **GLBench: A Comprehensive Benchmark for Graph with Large Language Models**
2407.07457v2 by Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li

The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÂúñË°®‰∫íÂãïÁöÑÊñπÂºèÔºåÈÄ≤ËÄåÁî¢Áîü‰∏ÄÁ®ÆÁ®±ÁÇ∫ GraphLLM ÁöÑÊñ∞ÂÖ∏ÁØÑ„ÄÇÂÑòÁÆ°ËøëÂπ¥‰æÜ GraphLLM ÊñπÊ≥ïÂø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÂÖ∑Êúâ‰∏ÄËá¥ÂØ¶È©óÂçîÂÆöÁöÑÂü∫Ê∫ñÔºåÂõ†Ê≠§Ë©≤È†òÂüüÁöÑÈÄ≤Â±ïÂíåÁêÜËß£‰ªç‰∏çÊòéÁ¢∫„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GLBenchÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÁî®ÊñºË©ï‰º∞ GraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèÂíåÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠ÁöÑÁ∂úÂêàÂü∫Ê∫ñ„ÄÇGLBench Êèê‰æõÂ∞ç‰∏çÂêåÈ°ûÂà•ÁöÑ GraphLLM ÊñπÊ≥ïÈÄ≤Ë°åÂÖ¨Âπ≥‰∏îÂæπÂ∫ïÁöÑË©ï‰º∞Ôºå‰ª•ÂèäÂÇ≥Áµ±Âü∫Ê∫ñÔºå‰æãÂ¶ÇÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÈÄèÈÅéÂ∞ç‰∏ÄÁµÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÂØ¶È©óÔºå‰∏¶Êé°Áî®‰∏ÄËá¥ÁöÑË≥áÊñôËôïÁêÜÂíåÂàÜÂâ≤Á≠ñÁï•ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂπæÂÄãÈóúÈçµÁôºÁèæ„ÄÇÈ¶ñÂÖàÔºåGraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèË®≠ÂÆö‰∏≠ÂÑ™ÊñºÂÇ≥Áµ±Âü∫Ê∫ñÔºåÂÖ∂‰∏≠ LLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®È°ØÁ§∫Âá∫ÊúÄÁ©©ÂÅ•ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ËºÉ‰∏çÊúâÊïàÔºåËÄå‰∏îÁ∂ìÂ∏∏Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂ÁöÑËº∏Âá∫ÂïèÈ°å„ÄÇÊàëÂÄëÈÇÑÊ≥®ÊÑèÂà∞ÔºåÂ∞çÊñºÁõÆÂâçÁöÑ GraphLLM ÊñπÊ≥ï‰∏¶‰∏çÂ≠òÂú®ÊòéÁ¢∫ÁöÑÁ∏ÆÊîæÂÆöÂæã„ÄÇÊ≠§Â§ñÔºåÁµêÊßãÂíåË™ûÁæ©Â∞çÊñºÊúâÊïàÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂÇ≥Ëº∏Ëá≥ÈóúÈáçË¶ÅÔºåËÄåÊàëÂÄëÊèêÂá∫ÁöÑÁ∞°ÂñÆÂü∫Ê∫ñÁîöËá≥ÂèØ‰ª•ÂÑ™ÊñºÈáùÂ∞çÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØÈáèË∫´ÊâìÈÄ†ÁöÑÂπæÂÄãÊ®°Âûã„ÄÇÂü∫Ê∫ñÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/NineAbyss/GLBench ‰∏≠ÊâæÂà∞„ÄÇ

##### **Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**
2407.07038v1 by Ruiran Su, Janet B. Pierrehumbert

This work introduces the ClimateSent-GAT Model, an innovative method that
integrates Graph Attention Networks (GATs) with techniques from natural
language processing to accurately identify and predict disagreements within
Reddit comment-reply pairs. Our model classifies disagreements into three
categories: agree, disagree, and neutral. Leveraging the inherent graph
structure of Reddit comment-reply pairs, the model significantly outperforms
existing benchmarks by capturing complex interaction patterns and sentiment
dynamics. This research advances graph-based NLP methodologies and provides
actionable insights for policymakers and educators in climate science
communication.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂‰ªãÁ¥π ClimateSent-GAT Ê®°ÂûãÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞áÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (GAT) ËàáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìÊï¥ÂêàÔºå‰ª•Ê∫ñÁ¢∫Ë≠òÂà•‰∏¶È†êÊ∏¨ Reddit ÁïôË®ÄÂõûË¶ÜÂ∞ç‰∏≠ÁöÑÂàÜÊ≠ß„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞áÂàÜÊ≠ßÂàÜÁÇ∫‰∏âÈ°ûÔºöÂêåÊÑè„ÄÅ‰∏çÂêåÊÑèÂíå‰∏≠Á´ã„ÄÇÈÄèÈÅéÂà©Áî® Reddit ÁïôË®ÄÂõûË¶ÜÂ∞çÁöÑÂÖßÂú®ÂúñÂΩ¢ÁµêÊßãÔºåÊ≠§Ê®°ÂûãËÉΩÂ§ßÂπÖË∂ÖË∂äÁèæÊúâÂü∫Ê∫ñÔºåÊçïÊçâË§áÈõúÁöÑ‰∫íÂãïÊ®°ÂºèÂíåÊÉÖÁ∑íÂãïÊÖã„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êé®Âãï‰∫ÜÂü∫ÊñºÂúñÂΩ¢ÁöÑ NLP ÊñπÊ≥ïÔºå‰∏¶ÁÇ∫Ê∞£ÂÄôÁßëÂ≠∏Ê∫ùÈÄö‰∏≠ÁöÑÊîøÁ≠ñÂà∂ÂÆöËÄÖÂíåÊïôËÇ≤Â∑•‰ΩúËÄÖÊèê‰æõÂèØË°åÁöÑË¶ãËß£„ÄÇ

##### **Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**
2407.06723v1 by Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B√©thune, Hadi Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, Marco Cuturi

Humans describe complex scenes with compositionality, using simple text
descriptions enriched with links and relationships. While vision-language
research has aimed to develop models with compositional understanding
capabilities, this is not reflected yet in existing datasets which, for the
most part, still use plain text to describe images. In this work, we propose a
new annotation strategy, graph-based captioning (GBC) that describes an image
using a labelled graph structure, with nodes of various types. The nodes in GBC
are created using, in a first stage, object detection and dense captioning
tools nested recursively to uncover and describe entity nodes, further linked
together in a second stage by highlighting, using new types of nodes,
compositions and relations among entities. Since all GBC nodes hold plain text
descriptions, GBC retains the flexibility found in natural language, but can
also encode hierarchical information in its edges. We demonstrate that GBC can
be produced automatically, using off-the-shelf multimodal LLMs and
open-vocabulary detection models, by building a new dataset, GBC10M, gathering
GBC annotations for about 10M images of the CC12M dataset. We use GBC10M to
showcase the wealth of node captions uncovered by GBC, as measured with CLIP
training. We show that using GBC nodes' annotations -- notably those stored in
composition and relation nodes -- results in significant performance boost on
downstream models when compared to other dataset formats. To further explore
the opportunities provided by GBC, we also propose a new attention mechanism
that can leverage the entire GBC graph, with encouraging experimental results
that show the extra benefits of incorporating the graph structure. Our datasets
are released at \url{https://huggingface.co/graph-based-captions}.

ÊëòË¶ÅÔºö<paragraph>‰∫∫È°û‰ΩøÁî®Á∞°ÂñÆÁöÑÊñáÂ≠óÊèèËø∞ÔºåË±êÂØåÁöÑÈÄ£ÁµêÂíåÈóú‰øÇÔºå‰æÜÊèèËø∞Ë§áÈõúÁöÑÂ†¥ÊôØ„ÄÇÈõñÁÑ∂Ë¶ñË¶∫Ë™ûË®ÄÁöÑÁ†îÁ©∂Êó®Âú®ÈñãÁôºÂÖ∑ÊúâÁµÑÂêàÁêÜËß£ËÉΩÂäõÁöÑÊ®°ÂûãÔºå‰ΩÜÁèæÊúâÁöÑÊï∏ÊìöÈõÜÂ∞öÊú™ÂèçÊò†ÈÄô‰∏ÄÈªûÔºåÈÄô‰∫õÊï∏ÊìöÈõÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªç‰ΩøÁî®Á¥îÊñáÊú¨‰æÜÊèèËø∞ÂúñÂÉè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË®ªÈáãÁ≠ñÁï•ÔºåÂü∫ÊñºÂúñË°®ÁöÑÊ®ôÈ°å (GBC)ÔºåÂÆÉ‰ΩøÁî®Ê®ôÁ±§ÂúñË°®ÁµêÊßã‰æÜÊèèËø∞ÂúñÂÉèÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÈ°ûÂûãÁöÑÁØÄÈªû„ÄÇGBC ‰∏≠ÁöÑÁØÄÈªûÊòØ‰ΩøÁî®Áâ©È´îÊ™¢Ê∏¨ÂíåÂØÜÈõÜÊ®ôÈ°åÂ∑•ÂÖ∑Âú®Á¨¨‰∏ÄÈöéÊÆµÂâµÂª∫ÁöÑÔºå‰ª•ÈÅûËø¥ÂµåÂ•óÁöÑÊñπÂºèÁôºÁèæÂíåÊèèËø∞ÂØ¶È´îÁØÄÈªûÔºå‰∏¶Âú®Á¨¨‰∫åÈöéÊÆµ‰ΩøÁî®Êñ∞È°ûÂûãÁöÑÁØÄÈªûÁ™ÅÂá∫È°ØÁ§∫ÔºåÂæûËÄåÂ∞áÂÆÉÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ£ÁµêÂú®‰∏ÄËµ∑ÔºåÂØ¶È´î‰πãÈñìÁöÑÁµÑÂêàÂíåÈóú‰øÇ„ÄÇÁî±ÊñºÊâÄÊúâ GBC ÁØÄÈªûÈÉΩÂåÖÂê´Á¥îÊñáÊú¨ÊèèËø∞ÔºåÂõ†Ê≠§ GBC ‰øùÁïô‰∫ÜËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÈùàÊ¥ªÊÄßÔºå‰ΩÜ‰πüÂèØ‰ª•Âú®ÂÖ∂ÈÇäÁ∑£Á∑®Á¢ºÂàÜÂ±§‰ø°ÊÅØ„ÄÇÊàëÂÄëË≠âÊòé‰∫Ü GBC ÂèØ‰ª•‰ΩøÁî®ÁèæÊàêÁöÑÂ§öÊ®°ÊÖã LLM ÂíåÈñãÊîæË©ûÂΩôÊ™¢Ê∏¨Ê®°ÂûãËá™ÂãïÁîüÊàêÔºåÈÄöÈÅéÊßãÂª∫‰∏ÄÂÄãÊñ∞ÁöÑÊï∏ÊìöÈõÜ GBC10MÔºåÊî∂ÈõÜ‰∫ÜÂ§ßÁ¥Ñ 10M CC12M Êï∏ÊìöÈõÜÂúñÂÉèÁöÑ GBC Ë®ªÈáã„ÄÇÊàëÂÄë‰ΩøÁî® GBC10M ‰æÜÂ±ïÁ§∫ GBC ÁôºÁèæÁöÑË±êÂØåÁØÄÈªûÊ®ôÈ°åÔºå‰∏¶‰ΩøÁî® CLIP Ë®ìÁ∑¥ÈÄ≤Ë°åÊ∏¨Èáè„ÄÇÊàëÂÄëË°®ÊòéÔºåËàáÂÖ∂‰ªñÊï∏ÊìöÈõÜÊ†ºÂºèÁõ∏ÊØîÔºå‰ΩøÁî® GBC ÁØÄÈªûÁöÑË®ªÈáã‚Äî‚ÄîÁâπÂà•ÊòØÂ≠òÂÑ≤Âú®ÁµÑÂêàÂíåÈóú‰øÇÁØÄÈªû‰∏≠ÁöÑË®ªÈáã‚Äî‚ÄîÊúÉÈ°ØËëóÊèêÂçá‰∏ãÊ∏∏Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢ GBC Êèê‰æõÁöÑÊ©üÊúÉÔºåÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊ≥®ÊÑèÊ©üÂà∂ÔºåÂÆÉÂèØ‰ª•Âà©Áî®Êï¥ÂÄã GBC ÂúñË°®Ôºå‰∏¶ÈÄöÈÅéÈºìÂãµÊÄßÁöÑÂØ¶È©óÁµêÊûúÂ±ïÁ§∫‰∫ÜÁµêÂêàÂúñË°®ÁµêÊßãÁöÑÈ°çÂ§ñÂ•ΩËôï„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÁôºÂ∏ÉÂú® \url{https://huggingface.co/graph-based-captions}„ÄÇ</paragraph>

##### **Combining Knowledge Graphs and Large Language Models**
2407.06564v1 by Amanda Kau, Xuzeng He, Aishwarya Nambissan, Aland Astudillo, Hui Yin, Amir Aryani

In recent years, Natural Language Processing (NLP) has played a significant
role in various Artificial Intelligence (AI) applications such as chatbots,
text generation, and language translation. The emergence of large language
models (LLMs) has greatly improved the performance of these applications,
showing astonishing results in language understanding and generation. However,
they still show some disadvantages, such as hallucinations and lack of
domain-specific knowledge, that affect their performance in real-world tasks.
These issues can be effectively mitigated by incorporating knowledge graphs
(KGs), which organise information in structured formats that capture
relationships between entities in a versatile and interpretable fashion.
Likewise, the construction and validation of KGs present challenges that LLMs
can help resolve. The complementary relationship between LLMs and KGs has led
to a trend that combines these technologies to achieve trustworthy results.
This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based
KGs, and LLM-KG hybrid approaches. We systematically analysed and compared
these approaches to provide a comprehensive overview highlighting key trends,
innovative techniques, and common challenges. This synthesis will benefit
researchers new to the field and those seeking to deepen their understanding of
how KGs and LLMs can be effectively combined to enhance AI applications
capabilities.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ (NLP) Âú®ÂêÑÁßç‰∫∫Â∑•Êô∫ËÉΩ (AI) Â∫îÁî®‰∏≠ÂèëÊå•‰∫ÜÈáçË¶Å‰ΩúÁî®Ôºå‰æãÂ¶ÇËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅÊñáÊú¨ÁîüÊàêÂíåËØ≠Ë®ÄÁøªËØë„ÄÇÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂá∫Áé∞ÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫ÜËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÁöÑÊÄßËÉΩÔºåÂú®ËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢ÊòæÁ§∫Âá∫ÊÉä‰∫∫ÁöÑÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨‰ªçÁÑ∂Ë°®Áé∞Âá∫‰∏Ä‰∫õÁº∫ÁÇπÔºå‰æãÂ¶ÇÂπªËßâÂíåÁº∫‰πèÁâπÂÆöÈ¢ÜÂüüÁöÑÁü•ËØÜÔºåËøô‰∫õÁº∫ÁÇπ‰ºöÂΩ±ÂìçÂÆÉ‰ª¨Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÈÄöËøáÁ∫≥ÂÖ•Áü•ËØÜÂõæË∞± (KG) ÂèØ‰ª•ÊúâÊïàÂú∞ÂáèËΩªËøô‰∫õÈóÆÈ¢òÔºåÁü•ËØÜÂõæË∞±‰ª•ÁªìÊûÑÂåñÊ†ºÂºèÁªÑÁªá‰ø°ÊÅØÔºå‰ª•Â§öÂäüËÉΩ‰∏îÂèØËß£ÈáäÁöÑÊñπÂºèÊçïËé∑ÂÆû‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂêåÊ†∑ÔºåKG ÁöÑÊûÑÂª∫ÂíåÈ™åËØÅÊèêÂá∫‰∫Ü LLM ÂèØ‰ª•Â∏ÆÂä©Ëß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇLLM Âíå KG ‰πãÈó¥ÁöÑ‰∫íË°•ÂÖ≥Á≥ªÂØºËá¥‰∫Ü‰∏ÄÁßçÂ∞ÜËøô‰∫õÊäÄÊúØÁõ∏ÁªìÂêà‰ª•ÂÆûÁé∞ÂèØ‰ø°ÁªìÊûúÁöÑË∂ãÂäø„ÄÇËøôÈ°πÂ∑•‰ΩúÊî∂ÈõÜ‰∫Ü 28 ÁØáÊ¶ÇËø∞‰∫Ü KG È©±Âä®ÁöÑ LLM„ÄÅÂü∫‰∫é LLM ÁöÑ KG Âíå LLM-KG Ê∑∑ÂêàÊñπÊ≥ïÁöÑÊñπÊ≥ïÁöÑËÆ∫Êñá„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÂàÜÊûêÂíåÊØîËæÉ‰∫ÜËøô‰∫õÊñπÊ≥ïÔºå‰ª•Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ¶ÇËø∞ÔºåÈáçÁÇπ‰ªãÁªçÂÖ≥ÈîÆË∂ãÂäø„ÄÅÂàõÊñ∞ÊäÄÊúØÂíåÂÖ±ÂêåÊåëÊàò„ÄÇËøôÁßçÁªºÂêàÂ∞Ü‰ΩøËØ•È¢ÜÂüüÁöÑÊñ∞Á†îÁ©∂‰∫∫ÂëòÂíåÈÇ£‰∫õÂØªÊ±ÇÂä†Ê∑±ÂØπÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞Ü KG Âíå LLM Áõ∏ÁªìÂêà‰ª•Â¢ûÂº∫ AI Â∫îÁî®ËÉΩÂäõÁöÑÁêÜËß£ÁöÑ‰∫∫ÂèóÁõä„ÄÇ

##### **FuncEvalGMN: Evaluating Functional Correctness of SQL via Graph Matching Network**
2407.14530v1 by Yi Zhan, Yang Sun, Han Weng, Longjie Cui, Guifeng Wang, Jiajun Xie, Yu Tian, Xiaoming Yin, Boyi Liu, Dongchi Huang

In this paper, we propose a novel graph-based methodology to evaluate the
functional correctness of SQL generation. Conventional metrics for assessing
SQL code generation, such as matching-based and execution-based methods (e.g.,
exact set match and execution accuracy), are subject to two primary
limitations. Firstly, the former fails to effectively assess functional
correctness, as different SQL queries may possess identical functionalities.
Secondly, the latter is susceptible to producing false positive samples in
evaluations. Our proposed evaluation method, \texttt{FuncEvalGMN}, does not
depend on the sufficient preparation of the test data, and it enables precise
testing of the functional correctness of the code. Firstly, we parse SQL using
a relational operator tree (ROT) called \textit{Relnode}, which contains rich
semantic information from the perspective of logical execution.Then, we
introduce a GNN-based approach for predicting the functional correctness of
generated SQL. This approach incorporates global positional embeddings to
address the limitations with the loss of topological information in
conventional graph matching frameworks. As an auxiliary contribution, we
propose a rule-based matching algorithm, Relnode Partial Matching
(\texttt{RelPM}) as a baseline. Finally, we contribute a dataset,
\texttt{Pair-Aug-Spider} with a training set and two testing sets, each
comprising pairs of SQL codes to simulate various SQL code evaluation
scenarios. The training set and one testing dataset focus on code generation
using large language models (LLMs), while the other emphasizes SQL equivalence
rewriting.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÂúñÁöÑÊñπÊ≥ï‰æÜË©ï‰º∞ SQL ÁîüÊàêÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇË©ï‰º∞ SQL Á®ãÂºèÁ¢ºÁîüÊàêÁöÑÂÇ≥Áµ±ÊåáÊ®ôÔºå‰æãÂ¶ÇÂü∫ÊñºÂåπÈÖçÂíåÂü∫ÊñºÂü∑Ë°åÁöÑÊåáÊ®ôÔºà‰æãÂ¶ÇÔºåÁ≤æÁ¢∫ÈõÜÂêàÂåπÈÖçÂíåÂü∑Ë°åÊ∫ñÁ¢∫Â∫¶ÔºâÔºåÂ≠òÂú®ÂÖ©ÂÄã‰∏ªË¶ÅÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÂâçËÄÖÁÑ°Ê≥ïÊúâÊïàË©ï‰º∞ÂäüËÉΩÊ≠£Á¢∫ÊÄßÔºåÂõ†ÁÇ∫‰∏çÂêåÁöÑ SQL Êü•Ë©¢ÂèØËÉΩÂÖ∑ÊúâÁõ∏ÂêåÁöÑÊ©üËÉΩ„ÄÇÂÖ∂Ê¨°ÔºåÂæåËÄÖÂú®Ë©ï‰º∞‰∏≠ÂÆπÊòìÁî¢ÁîüÂÅáÈôΩÊÄßÊ®£Êú¨„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑË©ï‰º∞ÊñπÊ≥ï \texttt{FuncEvalGMN} ‰∏ç‰æùË≥¥ÊñºÊ∏¨Ë©¶Ë≥áÊñôÁöÑÂÖÖÂàÜÊ∫ñÂÇôÔºå‰∏¶‰∏îÂèØ‰ª•Á≤æÁ¢∫Ê∏¨Ë©¶Á®ãÂºèÁ¢ºÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî®Á®±ÁÇ∫ \textit{Relnode} ÁöÑÈóú‰øÇÈÅãÁÆóÂÖÉÊ®π (ROT) ‰æÜËß£Êûê SQLÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂæûÈÇèËºØÂü∑Ë°åÁöÑËßíÂ∫¶‰æÜÁúãË±êÂØåÁöÑË™ûÁæ©Ë≥áË®ä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÂü∫Êñº GNN ÁöÑÊñπÊ≥ï‰æÜÈ†êÊ∏¨ÁîüÊàêÁöÑ SQL ÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁµêÂêà‰∫ÜÂÖ®Â±Ä‰ΩçÁΩÆÂµåÂÖ•Ôºå‰ª•Ëß£Ê±∫ÂÇ≥Áµ±ÂúñÂΩ¢ÂåπÈÖçÊ°ÜÊû∂‰∏≠ÊãìÊí≤Ë≥áË®äÈÅ∫Â§±ÁöÑÈôêÂà∂„ÄÇ‰ΩúÁÇ∫ËºîÂä©Ë≤¢ÁçªÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË¶èÂâáÁöÑÂåπÈÖçÊºîÁÆóÊ≥ïÔºåÂç≥ Relnode ÈÉ®ÂàÜÂåπÈÖç (\texttt{RelPM}) ‰ΩúÁÇ∫Âü∫Á∑ö„ÄÇÊúÄÂæåÔºåÊàëÂÄëË≤¢Áçª‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜ \texttt{Pair-Aug-Spider}ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÂÄãË®ìÁ∑¥ÈõÜÂíåÂÖ©ÂÄãÊ∏¨Ë©¶ÈõÜÔºåÊØèÂÄãÊ∏¨Ë©¶ÈõÜÈÉΩÂåÖÂê´ÊàêÂ∞çÁöÑ SQL Á®ãÂºèÁ¢º‰æÜÊ®°Êì¨ÂêÑÁ®Æ SQL Á®ãÂºèÁ¢ºË©ï‰º∞Â†¥ÊôØ„ÄÇË®ìÁ∑¥ÈõÜÂíå‰∏ÄÂÄãÊ∏¨Ë©¶Ë≥áÊñôÈõÜÂ∞àÊ≥®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÁ®ãÂºèÁ¢ºÁîüÊàêÔºåËÄåÂè¶‰∏ÄÂÄãÂâáÂº∑Ë™ø SQL Á≠âÂÉπÈáçÂØ´„ÄÇ</paragraph>

##### **MST5 -- Multilingual Question Answering over Knowledge Graphs**
2407.06041v1 by Nikit Srivastava, Mengshi Ma, Daniel Vollmers, Hamada Zahera, Diego Moussallem, Axel-Cyrille Ngonga Ngomo

Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of
knowledge stored in a graph-based model using natural language. However, the
research has largely concentrated on English, putting non-English speakers at a
disadvantage. Meanwhile, existing multilingual KGQA systems face challenges in
achieving performance comparable to English systems, highlighting the
difficulty of generating SPARQL queries from diverse languages. In this
research, we propose a simplified approach to enhance multilingual KGQA systems
by incorporating linguistic context and entity information directly into the
processing pipeline of a language model. Unlike existing methods that rely on
separate encoders for integrating auxiliary information, our strategy leverages
a single, pretrained multilingual transformer-based language model to manage
both the primary input and the auxiliary data. Our methodology significantly
improves the language model's ability to accurately convert a natural language
query into a relevant SPARQL query. It demonstrates promising results on the
most recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we
introduce and evaluate our approach on Chinese and Japanese, thereby expanding
the language diversity of the existing datasets.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË°®ÂïèÁ≠î (KGQA) Á∞°Âåñ‰∫Ü‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÂÑ≤Â≠òÂú®ÂúñÂΩ¢ÂåñÊ®°Âûã‰∏≠ÁöÑÂ§ßÈáèÁü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëã±Êñá‰∏äÔºåÈÄôÂ∞çÈùûËã±Ë™û‰ΩøÁî®ËÄÖ‰æÜË™™ÊòØ‰∏çÂà©ÁöÑ„ÄÇÂêåÊôÇÔºåÁèæÊúâÁöÑÂ§öË™ûË®Ä KGQA Á≥ªÁµ±Âú®ÈÅîÊàêËàáËã±ÊñáÁ≥ªÁµ±Áõ∏Â™≤ÁæéÁöÑÊïàËÉΩÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁ™ÅÈ°Ø‰∫ÜÂæû‰∏çÂêåË™ûË®ÄÁî¢Áîü SPARQL Êü•Ë©¢ÁöÑÂõ∞Èõ£ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∞°ÂåñÁöÑÊñπÊ≥ïÔºåÈÄöÈÅéÂ∞áË™ûË®ÄÂ≠∏ËÉåÊôØÂíåÂØ¶È´îË≥áË®äÁõ¥Êé•Á¥çÂÖ•Ë™ûË®ÄÊ®°ÂûãÁöÑËôïÁêÜÁÆ°ÈÅìÔºå‰æÜÂ¢ûÂº∑Â§öË™ûË®Ä KGQA Á≥ªÁµ±„ÄÇËàá‰æùË≥¥ÊñºÂñÆÁç®Á∑®Á¢ºÂô®‰æÜÊï¥ÂêàËºîÂä©Ë≥áË®äÁöÑÁèæÊúâÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑÁ≠ñÁï•Âà©Áî®ÂñÆ‰∏ÄÁöÑ„ÄÅÈ†êË®ìÁ∑¥ÁöÑÂ§öË™ûË®ÄËΩâÊèõÂô®Ë™ûË®ÄÊ®°Âûã‰æÜÁÆ°ÁêÜ‰∏ªË¶ÅËº∏ÂÖ•ÂíåËºîÂä©Ë≥áÊñô„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÈ°ØËëóÊèêÂçá‰∫ÜË™ûË®ÄÊ®°ÂûãÊ∫ñÁ¢∫Âú∞Â∞áËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ËΩâÊèõÁÇ∫Áõ∏Èóú SPARQL Êü•Ë©¢ÁöÑËÉΩÂäõ„ÄÇÂÆÉÂú®ÊúÄÊñ∞ÁöÑ QALD Ë≥áÊñôÈõÜÔºåÂç≥ QALD-9-Plus Âíå QALD-10 ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®‰∏≠ÊñáÂíåÊó•Êñá‰∏≠ÂºïÂÖ•‰∏¶Ë©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂæûËÄåÊì¥Â±ï‰∫ÜÁèæÊúâË≥áÊñôÈõÜÁöÑË™ûË®ÄÂ§öÊ®£ÊÄß„ÄÇ

##### **Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding**
2407.05910v1 by Aaron Lohner, Francesco Compagno, Jonathan Francis, Alessandro Oltramari

Recognizing a traffic accident is an essential part of any autonomous driving
or road monitoring system. An accident can appear in a wide variety of forms,
and understanding what type of accident is taking place may be useful to
prevent it from reoccurring. The task of being able to classify a traffic scene
as a specific type of accident is the focus of this work. We approach the
problem by likening a traffic scene to a graph, where objects such as cars can
be represented as nodes, and relative distances and directions between them as
edges. This representation of an accident can be referred to as a scene graph,
and is used as input for an accident classifier. Better results can be obtained
with a classifier that fuses the scene graph input with representations from
vision and language. This work introduces a multi-stage, multimodal pipeline to
pre-process videos of traffic accidents, encode them as scene graphs, and align
this representation with vision and language modalities for accident
classification. When trained on 4 classes, our method achieves a balanced
accuracy score of 57.77% on an (unbalanced) subset of the popular Detection of
Traffic Anomaly (DoTA) benchmark, representing an increase of close to 5
percentage points from the case where scene graph information is not taken into
account.

ÊëòË¶ÅÔºöËæ®Ë≠ò‰∫§ÈÄö‰∫ãÊïÖÊòØ‰ªª‰ΩïËá™ÂãïÈßïÈßõÊàñÈÅìË∑ØÁõ£ÊéßÁ≥ªÁµ±ÁöÑÂøÖË¶ÅÈÉ®ÂàÜ„ÄÇ‰∫ãÊïÖÂèØËÉΩ‰ª•ÂêÑÁ®ÆÂΩ¢ÂºèÂá∫ÁèæÔºå‰∫ÜËß£‰∫ãÊïÖÈ°ûÂûãÂèØËÉΩÊúâÂä©ÊñºÈò≤Ê≠¢ÂÜçÊ¨°ÁôºÁîü„ÄÇÂ∞á‰∫§ÈÄö‰∫ãÊïÖÂ†¥ÊôØÂàÜÈ°ûÁÇ∫ÁâπÂÆö‰∫ãÊïÖÈ°ûÂûãÁöÑ‰ªªÂãôÊòØÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÈáçÈªû„ÄÇÊàëÂÄëÂ∞á‰∫§ÈÄö‰∫ãÊïÖÂ†¥ÊôØÊØîÂñªÁÇ∫ÂúñÂΩ¢‰æÜËß£Ê±∫ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê±ΩËªäÁ≠âÁâ©È´îÂèØ‰ª•Ë°®Á§∫ÁÇ∫ÁØÄÈªûÔºåËÄåÂÆÉÂÄë‰πãÈñìÁöÑÁõ∏Â∞çË∑ùÈõ¢ÂíåÊñπÂêëÂâáË°®Á§∫ÁÇ∫ÈÇäÁ∑£„ÄÇÈÄôÁ®Æ‰∫ãÊïÖË°®Á§∫ÂèØ‰ª•Á®±ÁÇ∫Â†¥ÊôØÂúñÔºå‰∏¶Áî®‰Ωú‰∫ãÊïÖÂàÜÈ°ûÂô®ÁöÑËº∏ÂÖ•„ÄÇ‰ΩøÁî®Â∞áÂ†¥ÊôØÂúñËº∏ÂÖ•ËàáË¶ñË¶∫ÂíåË™ûË®ÄË°®Á§∫ËûçÂêàÁöÑÂàÜÈ°ûÂô®ÂèØ‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑÁµêÊûú„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ§öÈöéÊÆµ„ÄÅÂ§öÊ®°ÊÖãÁÆ°ÈÅìÔºåÁî®ÊñºÈ†êËôïÁêÜ‰∫§ÈÄö‰∫ãÊïÖÂΩ±Áâá„ÄÅÂ∞áÂÖ∂Á∑®Á¢ºÁÇ∫Â†¥ÊôØÂúñÔºå‰ª•ÂèäÂ∞áÊ≠§Ë°®Á§∫ËàáË¶ñË¶∫ÂíåË™ûË®ÄÊ®°ÂºèÂ∞çÈΩä‰ª•ÈÄ≤Ë°å‰∫ãÊïÖÂàÜÈ°û„ÄÇÁï∂Âú® 4 ÂÄãÈ°ûÂà•‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÁÜ±ÈñÄ‰∫§ÈÄöÁï∞Â∏∏Ê™¢Ê∏¨ (DoTA) Âü∫Ê∫ñÁöÑÔºà‰∏çÂπ≥Ë°°ÔºâÂ≠êÈõÜ‰∏äÂØ¶Áèæ‰∫Ü 57.77% ÁöÑÂπ≥Ë°°Ê∫ñÁ¢∫ÁéáÔºåÊØî‰∏çËÄÉÊÖÆÂ†¥ÊôØÂúñË≥áË®äÁöÑÊÉÖÊ≥ÅÊèêÈ´ò‰∫ÜÊé•Ëøë 5 ÂÄãÁôæÂàÜÈªû„ÄÇ

##### **Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**
2407.05890v1 by Jiaqi Chen, Bingqian Lin, Xinmin Liu, Xiaodan Liang, Kwan-Yee K. Wong

LLM-based agents have demonstrated impressive zero-shot performance in the
vision-language navigation (VLN) task. However, these zero-shot methods focus
only on solving high-level task planning by selecting nodes in predefined
navigation graphs for movements, overlooking low-level control in realistic
navigation scenarios. To bridge this gap, we propose AO-Planner, a novel
affordances-oriented planning framework for continuous VLN task. Our AO-Planner
integrates various foundation models to achieve affordances-oriented motion
planning and action decision-making, both performed in a zero-shot manner.
Specifically, we employ a visual affordances prompting (VAP) approach, where
visible ground is segmented utilizing SAM to provide navigational affordances,
based on which the LLM selects potential next waypoints and generates low-level
path planning towards selected waypoints. We further introduce a high-level
agent, PathAgent, to identify the most probable pixel-based path and convert it
into 3D coordinates to fulfill low-level motion. Experimental results on the
challenging R2R-CE benchmark demonstrate that AO-Planner achieves
state-of-the-art zero-shot performance (5.5% improvement in SPL). Our method
establishes an effective connection between LLM and 3D world to circumvent the
difficulty of directly predicting world coordinates, presenting novel prospects
for employing foundation models in low-level motion control.

ÊëòË¶ÅÔºöÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÂ∑≤Âú®Ë¶ñË¶∫Ë™ûË®ÄÂ∞éËà™ (VLN) ‰ªªÂãô‰∏≠Â±ïÁ§∫Âá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÂÉÖÂ∞àÊ≥®ÊñºÈÄèÈÅéÈÅ∏ÊìáÈ†êÂÆöÁæ©Â∞éËà™ÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªû‰æÜËß£Ê±∫È´òÈöé‰ªªÂãôË¶èÂäÉÔºåÂøΩÁï•‰∫ÜÂØ¶ÈöõÂ∞éËà™Â†¥ÊôØ‰∏≠ÁöÑ‰ΩéÈöéÊéßÂà∂„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÊ≠§Â∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫ AO-PlannerÔºå‰∏ÄÂÄãÁî®ÊñºÈÄ£Á∫å VLN ‰ªªÂãôÁöÑÊñ∞Âûã‰ª•ÂèØË≤†ÊìîÊÄßÁÇ∫Â∞éÂêëÁöÑË¶èÂäÉÊû∂Êßã„ÄÇÊàëÂÄëÁöÑ AO-Planner Êï¥ÂêàÂêÑÁ®ÆÂü∫Á§éÊ®°ÂûãÔºå‰ª•ÂØ¶Áèæ‰ª•ÂèØË≤†ÊìîÊÄßÁÇ∫Â∞éÂêëÁöÑÂãï‰ΩúË¶èÂäÉÂíåÂãï‰ΩúÊ±∫Á≠ñÔºåÂÖ©ËÄÖÈÉΩ‰ª•Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊñπÂºèÂü∑Ë°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®Ë¶ñË¶∫ÂèØË≤†ÊìîÊÄßÊèêÁ§∫ (VAP) ÊñπÊ≥ïÔºåÂÖ∂‰∏≠Âà©Áî® SAM Â∞çÂèØË¶ãÂú∞Èù¢ÈÄ≤Ë°åÂàÜÂâ≤Ôºå‰ª•Êèê‰æõÂ∞éËà™ÂèØË≤†ÊìîÊÄßÔºåLLM Ê†πÊìöÈÄô‰∫õÂèØË≤†ÊìîÊÄßÈÅ∏ÊìáÊΩõÂú®ÁöÑ‰∏ã‰∏ÄÂÄãËà™ÈªûÔºå‰∏¶ÈáùÂ∞çÊâÄÈÅ∏Ëà™ÈªûÁî¢Áîü‰ΩéÈöéË∑ØÂæëË¶èÂäÉ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰∏ÄÂÄãÈ´òÈöé‰ª£ÁêÜ PathAgentÔºå‰ª•Ë≠òÂà•ÊúÄÂèØËÉΩÁöÑÂü∫ÊñºÂÉèÁ¥†ÁöÑË∑ØÂæëÔºå‰∏¶Â∞áÂÖ∂ËΩâÊèõÁÇ∫ 3D Â∫ßÊ®ôÔºå‰ª•ÂØ¶Áèæ‰ΩéÈöéÂãï‰Ωú„ÄÇÂú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ R2R-CE Âü∫Ê∫ñÊ∏¨Ë©¶‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåAO-Planner ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩÔºàSPL ÊèêÂçá 5.5%Ôºâ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú® LLM Âíå 3D ‰∏ñÁïå‰πãÈñìÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑÈÄ£ÁµêÔºå‰ª•Ë¶èÈÅøÁõ¥Êé•È†êÊ∏¨‰∏ñÁïåÂ∫ßÊ®ôÁöÑÈõ£È°åÔºåÁÇ∫Âú®‰ΩéÈöéÂãï‰ΩúÊéßÂà∂‰∏≠Êé°Áî®Âü∫Á§éÊ®°ÂûãÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂâçÊôØ„ÄÇ

##### **KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions**
2407.05868v1 by Yanxu Zhu, Jinlin Xiao, Yuhang Wang, Jitao Sang

Recent studies have demonstrated that large language models (LLMs) are
susceptible to being misled by false premise questions (FPQs), leading to
errors in factual knowledge, know as factuality hallucination. Existing
benchmarks that assess this vulnerability primarily rely on manual
construction, resulting in limited scale and lack of scalability. In this work,
we introduce an automated, scalable pipeline to create FPQs based on knowledge
graphs (KGs). The first step is modifying true triplets extracted from KGs to
create false premises. Subsequently, utilizing the state-of-the-art
capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed
method, we present a comprehensive benchmark, the Knowledge Graph-based False
Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three
knowledge domains, at six levels of confusability, and in two task formats.
Using KG-FPQ, we conduct extensive evaluations on several representative LLMs
and provide valuable insights. The KG-FPQ dataset and code are available
at~https://github.com/yanxuzhu/KG-FPQ.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂÆπÊòìË¢´ÈîôËØØÂâçÊèêÈóÆÈ¢ò (FPQ) ËØØÂØºÔºå‰ªéËÄåÂØºËá¥‰∫ãÂÆûÁü•ËØÜÈîôËØØÔºåÂç≥‰∫ãÂÆûÂπªËßâ„ÄÇÁî®‰∫éËØÑ‰º∞Ê≠§ÊºèÊ¥ûÁöÑÁé∞ÊúâÂü∫ÂáÜ‰∏ªË¶Å‰æùËµñ‰∫éÊâãÂä®ÊûÑÂª∫ÔºåÂØºËá¥ËßÑÊ®°ÊúâÈôê‰∏îÁº∫‰πèÂèØÊâ©Â±ïÊÄß„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÁü•ËØÜÂõæË∞± (KG) ÂàõÂª∫ FPQ ÁöÑËá™Âä®ÂåñÂèØÊâ©Â±ïÁÆ°ÈÅì„ÄÇÁ¨¨‰∏ÄÊ≠•ÊòØ‰øÆÊîπ‰ªé KG ‰∏≠ÊèêÂèñÁöÑÁúü‰∏âÂÖÉÁªÑ‰ª•ÂàõÂª∫ÈîôËØØÂâçÊèê„ÄÇÈöèÂêéÔºåÂà©Áî® GPT ÁöÑÊúÄÂÖàËøõÂäüËÉΩÔºåÊàë‰ª¨ÁîüÊàê‰∫ÜËØ≠‰πâ‰∏∞ÂØåÁöÑ FPQ„ÄÇÂü∫‰∫éÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁªºÂêàÂü∫ÂáÜÔºåÂç≥Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈîôËØØÂâçÊèêÈóÆÈ¢ò (KG-FPQ)ÔºåÂÆÉÂåÖÂê´Â§ßÁ∫¶ 178k ‰∏™ FPQÔºåÊ∂µÁõñ‰∏â‰∏™Áü•ËØÜÂüüÔºåÂÖ≠‰∏™Ê∑∑Ê∑ÜÁ∫ßÂà´Âíå‰∏§Áßç‰ªªÂä°Ê†ºÂºè„ÄÇ‰ΩøÁî® KG-FPQÔºåÊàë‰ª¨ÂØπÂá†‰∏™Êúâ‰ª£Ë°®ÊÄßÁöÑ LLM ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞ÔºåÂπ∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇKG-FPQ Êï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂèØÂú®~https://github.com/yanxuzhu/KG-FPQ Ëé∑Âæó„ÄÇ

##### **Language Models Encode Collaborative Signals in Recommendation**
2407.05441v1 by Leheng Sheng, An Zhang, Yi Zhang, Yuxin Chen, Xiang Wang, Tat-Seng Chua

Recent studies empirically indicate that language models (LMs) encode rich
world knowledge beyond mere semantics, attracting significant attention across
various fields. However, in the recommendation domain, it remains uncertain
whether LMs implicitly encode user preference information. Contrary to the
prevailing understanding that LMs and traditional recommender models learn two
distinct representation spaces due to a huge gap in language and behavior
modeling objectives, this work rethinks such understanding and explores
extracting a recommendation space directly from the language representation
space. Surprisingly, our findings demonstrate that item representations, when
linearly mapped from advanced LM representations, yield superior recommendation
performance. This outcome suggests the homomorphism between the language
representation space and an effective recommendation space, implying that
collaborative signals may indeed be encoded within advanced LMs. Motivated by
these findings, we propose a simple yet effective collaborative filtering (CF)
model named AlphaRec, which utilizes language representations of item textual
metadata (e.g., titles) instead of traditional ID-based embeddings.
Specifically, AlphaRec is comprised of three main components: a multilayer
perceptron (MLP), graph convolution, and contrastive learning (CL) loss
function, making it extremely easy to implement and train. Our empirical
results show that AlphaRec outperforms leading ID-based CF models on multiple
datasets, marking the first instance of such a recommender with text embeddings
achieving this level of performance. Moreover, AlphaRec introduces a new
language-representation-based CF paradigm with several desirable advantages:
being easy to implement, lightweight, rapid convergence, superior zero-shot
recommendation abilities in new domains, and being aware of user intention.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÁ†îÁ©∂ÂØ¶Ë≠âË°®ÊòéÔºåË™ûË®ÄÊ®°Âûã (LM) Á∑®Á¢ºË±êÂØåÁöÑ‰∏ñÁïåÁü•Ë≠òÔºåË∂ÖË∂ä‰∫ÜÂñÆÁ¥îÁöÑË™ûÁæ©ÔºåÂê∏Âºï‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑÊ•µÂ§ßÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÂú®Êé®Ëñ¶È†òÂüü‰∏≠ÔºåLM ÊòØÂê¶Èö±Âê´Á∑®Á¢º‰ΩøÁî®ËÄÖÂÅèÂ•ΩË≥áË®ä‰ªç‰∏çÁ¢∫ÂÆö„ÄÇËàáÊôÆÈÅçË™çÁü•Áõ∏ÂèçÔºåLM ÂíåÂÇ≥Áµ±Êé®Ëñ¶Ê®°ÂûãÁî±ÊñºË™ûË®ÄÂíåË°åÁÇ∫Âª∫Ê®°ÁõÆÊ®ôÁöÑÂ∑®Â§ßÂ∑ÆË∑ùËÄåÂ≠∏ÁøíÂÖ©ÂÄã‰∏çÂêåÁöÑË°®Á§∫Á©∫ÈñìÔºåÈÄôÈ†ÖÂ∑•‰ΩúÈáçÊñ∞ÊÄùËÄÉÈÄôÁ®ÆÁêÜËß£Ôºå‰∏¶Êé¢Á¥¢Áõ¥Êé•ÂæûË™ûË®ÄË°®Á§∫Á©∫Èñì‰∏≠ÊèêÂèñÊé®Ëñ¶Á©∫Èñì„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁï∂ÂæûÂÖàÈÄ≤ÁöÑ LM Ë°®Á§∫‰∏≠Á∑öÊÄßÊò†Â∞ÑÊôÇÔºåÈ†ÖÁõÆË°®Á§∫ÊúÉÁî¢ÁîüÂÑ™Áï∞ÁöÑÊé®Ëñ¶ÊïàËÉΩ„ÄÇÊ≠§ÁµêÊûúË°®ÊòéË™ûË®ÄË°®Á§∫Á©∫ÈñìÂíåÊúâÊïàÁöÑÊé®Ëñ¶Á©∫Èñì‰πãÈñìÂ≠òÂú®ÂêåÊÖãÊÄßÔºåÈÄôÊÑèÂë≥ËëóÂçî‰ΩúË®äËôüÁ¢∫ÂØ¶ÂèØËÉΩÁ∑®Á¢ºÂú®ÂÖàÈÄ≤ÁöÑ LM ‰∏≠„ÄÇÂèóÈÄô‰∫õÁ†îÁ©∂ÁµêÊûúÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂçîÂêåÈÅéÊøæ (CF) Ê®°ÂûãÔºåÂêçÁÇ∫ AlphaRecÔºåÂÆÉÂà©Áî®È†ÖÁõÆÊñáÂ≠óÂÖÉË≥áÊñôÔºà‰æãÂ¶ÇÊ®ôÈ°åÔºâÁöÑË™ûË®ÄË°®Á§∫ÔºåËÄå‰∏çÊòØÂÇ≥Áµ±Âü∫Êñº ID ÁöÑÂµåÂÖ•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåAlphaRec Áî±‰∏âÂÄã‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÁµÑÊàêÔºöÂ§öÂ±§ÊÑüÁü•Âô® (MLP)„ÄÅÂúñÂΩ¢Âç∑Á©çÂíåÂ∞çÊØîÂ≠∏Áøí (CL) ÊêçÂ§±ÂáΩÊï∏Ôºå‰ΩøÂÖ∂Ê•µÊòìÊñºÂØ¶‰ΩúÂíåË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúË°®ÊòéÔºåAlphaRec Âú®Â§öÂÄãË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÈ†òÂÖàÁöÑÂü∫Êñº ID ÁöÑ CF Ê®°ÂûãÔºåÊ®ôË™åËëóÈÄôÁ®ÆÂÖ∑ÊúâÊñáÂ≠óÂµåÂÖ•ÁöÑÊé®Ëñ¶Á≥ªÁµ±È¶ñÊ¨°ÈÅîÂà∞Ê≠§ÊïàËÉΩÊ∞¥Ê∫ñ„ÄÇÊ≠§Â§ñÔºåAlphaRec ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºË™ûË®ÄË°®Á§∫ÁöÑ CF ÂÖ∏ÁØÑÔºåÂÖ∑ÊúâÂ§öÈ†ÖÁêÜÊÉ≥ÁöÑÂÑ™ÈªûÔºöÊòìÊñºÂØ¶‰Ωú„ÄÅËºïÈáèÁ¥ö„ÄÅÂø´ÈÄüÊî∂ÊñÇ„ÄÅÂú®Êñ∞ÁöÑÈ†òÂüü‰∏≠ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊé®Ëñ¶ËÉΩÂäõÔºå‰∏¶‰∏îÂèØ‰ª•‰∫ÜËß£‰ΩøÁî®ËÄÖÁöÑÊÑèÂúñ„ÄÇ</paragraph>

##### **LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models**
2407.05434v1 by Weizhi Tang, Vaishak Belle

Temporal reasoning (TR) is a critical component of artificial intelligence,
encompassing understanding and processing temporal information and
relationships between events. To discover and study the TR ability in Large
Language Models (LLMs), various datasets have been constructed in different
ways for evaluating various aspects of TR ability. Our work proposes a novel
approach to design and develop a pipeline for constructing datasets to evaluate
the TR ability of LLMs by leveraging random directed graph generation, LTL
formula, and the NuSMV model checker. Based on the pipeline, we have also
constructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TR
challenges and evaluated six LLMs with it. Furthermore, we have conducted
additional experiments to discover the impact of increasing the number of
events and formula operators on the complexity of TR problems and the
performance of LLMs. We have demonstrated that although LLMs exhibit some
promise in handling TR challenges, they still struggle with complex TR. We
expect this work can offer insights into TR ability in LLMs while also
providing a valuable tool for future TR evaluations.

ÊëòË¶ÅÔºöÊôÇÈñìÊé®ÁêÜ (TR) ÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÈ†ÖÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºå
Ê∂µËìã‰∫ÜÂ∞çÊôÇÈñìË≥áË®äÂíå‰∫ã‰ª∂‰πãÈñìÈóú‰øÇÁöÑÁêÜËß£ÂíåËôïÁêÜ„ÄÇÁÇ∫‰∫ÜÁôºÁèæÂíåÁ†îÁ©∂Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑ TR ËÉΩÂäõÔºåÂ∑≤ÈÄèÈÅéÂêÑÁ®ÆÊñπÂºèÂª∫ÊßãÂêÑÁ®ÆË≥áÊñôÈõÜÔºåÁî®ÊñºË©ï‰º∞ TR ËÉΩÂäõÁöÑÂêÑÂÄãÈù¢Âêë„ÄÇÊàëÂÄëÁöÑÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºË®≠Ë®àÂíåÈñãÁôº‰∏ÄÂÄãÂª∫ÊßãË≥áÊñôÈõÜÁöÑÁÆ°ÈÅìÔºå‰ª•Ë©ï‰º∞ LLM ÁöÑ TR ËÉΩÂäõÔºåÊñπÊ≥ïÊòØÂà©Áî®Èö®Ê©üÊúâÂêëÂúñÁîüÊàê„ÄÅLTL ÂÖ¨ÂºèÂíå NuSMV Ê®°ÂûãÊ™¢Êü•Âô®„ÄÇÊ†πÊìöÈÄôÂÄãÁÆ°ÈÅìÔºåÊàëÂÄëÈÇÑÂª∫Êßã‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜ‰ΩúÁÇ∫Âü∫Ê∫ñÔºåÂç≥ LTLBenchÔºåÂÖ∂‰∏≠ÂåÖÂê´ 2,000 ÂÄã TR ÊåëÊà∞Ôºå‰∏¶Áî®ÂÆÉË©ï‰º∞‰∫ÜÂÖ≠ÂÄã LLM„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÈÄ≤Ë°å‰∫ÜÈ°çÂ§ñÁöÑÂØ¶È©óÔºå‰ª•ÁôºÁèæÂ¢ûÂä†‰∫ã‰ª∂Êï∏ÈáèÂíåÂÖ¨ÂºèÈÅãÁÆóÂ≠êÂ∞ç TR ÂïèÈ°åË§áÈõúÊÄßÂíå LLM ÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂ∑≤Á∂ìË≠âÊòéÔºåÂÑòÁÆ° LLM Âú®ËôïÁêÜ TR ÊåëÊà∞ÊñπÈù¢Ë°®ÁèæÂá∫‰∏Ä‰∫õÂ∏åÊúõÔºå‰ΩÜÂÆÉÂÄë‰ªçÁÑ∂Èõ£‰ª•ËôïÁêÜË§áÈõúÁöÑ TR„ÄÇÊàëÂÄëÈ†êÊúüÈÄôÈ†ÖÂ∑•‰ΩúÂèØ‰ª•Êèê‰æõÂ∞ç LLM ‰∏≠ TR ËÉΩÂäõÁöÑË¶ãËß£ÔºåÂêåÊôÇ‰πüÁÇ∫Êú™‰æÜÁöÑ TR Ë©ï‰º∞Êèê‰æõ‰∏ÄÂÄãÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Leveraging Graph Structures to Detect Hallucinations in Large Language Models**
2407.04485v1 by Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, Roxana Petcu

Large language models are extensively applied across a wide range of tasks,
such as customer support, content creation, educational tutoring, and providing
financial guidance. However, a well-known drawback is their predisposition to
generate hallucinations. This damages the trustworthiness of the information
these models provide, impacting decision-making and user confidence. We propose
a method to detect hallucinations by looking at the structure of the latent
space and finding associations within hallucinated and non-hallucinated
generations. We create a graph structure that connects generations that lie
closely in the embedding space. Moreover, we employ a Graph Attention Network
which utilizes message passing to aggregate information from neighboring nodes
and assigns varying degrees of importance to each neighbor based on their
relevance. Our findings show that 1) there exists a structure in the latent
space that differentiates between hallucinated and non-hallucinated
generations, 2) Graph Attention Networks can learn this structure and
generalize it to unseen generations, and 3) the robustness of our method is
enhanced when incorporating contrastive learning. When evaluated against
evidence-based benchmarks, our model performs similarly without access to
search-based methods.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®Æ‰ªªÂãô‰∏≠Ôºå‰æãÂ¶ÇÂÆ¢Êà∂ÊîØÊè¥„ÄÅÂÖßÂÆπÂâµ‰Ωú„ÄÅÊïôËÇ≤ËºîÂ∞éÂíåÊèê‰æõË≤°ÂãôÊåáÂ∞é„ÄÇÁÑ∂ËÄåÔºå‰∏ÄÂÄãÁúæÊâÄÂë®Áü•ÁöÑÁº∫ÈªûÊòØÂÆÉÂÄëÂÇæÂêëÊñºÁî¢ÁîüÂπªË¶∫„ÄÇÈÄôÊêçÂÆ≥‰∫ÜÈÄô‰∫õÊ®°ÂûãÊâÄÊèê‰æõË≥áË®äÁöÑÂèØ‰ø°Â∫¶ÔºåÂΩ±Èüø‰∫ÜÊ±∫Á≠ñÂà∂ÂÆöÂíå‰ΩøÁî®ËÄÖ‰ø°ÂøÉ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈÄèÈÅéËßÄÂØüÊΩõÂú®Á©∫ÈñìÁöÑÁµêÊßã‰∏¶ÊâæÂá∫ÂπªË¶∫ÂíåÈùûÂπªË¶∫ÁîüÊàê‰∏≠ÁöÑÈóúËÅØ‰æÜÂÅµÊ∏¨ÂπªË¶∫ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢ÁµêÊßãÔºåÈÄ£Êé•Âú®ÂµåÂÖ•Á©∫Èñì‰∏≠Á∑äÂØÜÁõ∏ÈÄ£ÁöÑÁîüÊàê„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé°Áî®‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÔºåÂÆÉÂà©Áî®Ë®äÊÅØÂÇ≥ÈÅû‰æÜÂΩôÁ∏Ω‰æÜËá™Áõ∏ÈÑ∞ÁØÄÈªûÁöÑË≥áË®äÔºå‰∏¶Ê†πÊìöÊØèÂÄãÁõ∏ÈÑ∞ÁØÄÈªûÁöÑÁõ∏ÈóúÊÄßÁÇ∫ÂÖ∂ÊåáÂÆö‰∏çÂêåÁ®ãÂ∫¶ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºå1) ÊΩõÂú®Á©∫Èñì‰∏≠Â≠òÂú®‰∏ÄÂÄãÁµêÊßãÔºåÂèØ‰ª•ÂçÄÂàÜÂπªË¶∫ÂíåÈùûÂπªË¶∫ÁîüÊàêÔºå2) ÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÂèØ‰ª•Â≠∏ÁøíÈÄôÂÄãÁµêÊßã‰∏¶Â∞áÂÖ∂Ê¶ÇÊã¨Âà∞Êú™Ë¶ãÁöÑÁîüÊàê‰∏≠Ôºå‰ª•Âèä 3) Áï∂Á¥çÂÖ•Â∞çÊØîÂ≠∏ÁøíÊôÇÔºåÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÊúÉÂæóÂà∞Â¢ûÂº∑„ÄÇÁï∂Ê†πÊìöÂü∫ÊñºË≠âÊìöÁöÑÂü∫Ê∫ñÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÁÑ°Ê≥ïÂèñÂæóÂü∫ÊñºÊêúÂ∞ãÁöÑÊñπÊ≥ïÁöÑÊÉÖÊ≥Å‰∏ãÔºåË°®ÁèæÂæóÈ°û‰ºº„ÄÇ

##### **AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**
2407.04363v1 by Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev

Advancements in generative AI have broadened the potential applications of
Large Language Models (LLMs) in the development of autonomous agents. Achieving
true autonomy requires accumulating and updating knowledge gained from
interactions with the environment and effectively utilizing it. Current
LLM-based approaches leverage past experiences using a full history of
observations, summarization or retrieval augmentation. However, these
unstructured memory representations do not facilitate the reasoning and
planning essential for complex decision-making. In our study, we introduce
AriGraph, a novel method wherein the agent constructs a memory graph that
integrates semantic and episodic memories while exploring the environment. This
graph structure facilitates efficient associative retrieval of interconnected
concepts, relevant to the agent's current state and goals, thus serving as an
effective environmental model that enhances the agent's exploratory and
planning capabilities. We demonstrate that our Ariadne LLM agent, equipped with
this proposed memory architecture augmented with planning and decision-making,
effectively handles complex tasks on a zero-shot basis in the TextWorld
environment. Our approach markedly outperforms established methods such as
full-history, summarization, and Retrieval-Augmented Generation in various
tasks, including the cooking challenge from the First TextWorld Problems
competition and novel tasks like house cleaning and puzzle Treasure Hunting.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI ÁöÑÈÄ≤Ê≠•Êì¥Â±ï‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™‰∏ª‰ª£ÁêÜÈñãÁôº‰∏≠ÁöÑÊΩõÂú®ÊáâÁî®„ÄÇÂØ¶ÁèæÁúüÊ≠£ÁöÑËá™‰∏ªÊÄßÈúÄË¶ÅÁ¥ØÁ©çÂíåÊõ¥Êñ∞ÂæûËàáÁí∞Â¢É‰∫íÂãï‰∏≠Áç≤ÂæóÁöÑÁü•Ë≠òÔºå‰∏¶ÊúâÊïàÂà©Áî®ÂÆÉ„ÄÇÁï∂ÂâçÁöÑÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂà©Áî®ÈÅéÂéªÁöÑÁ∂ìÈ©óÔºå‰ΩøÁî®ÂÆåÊï¥ÁöÑËßÄÂØü„ÄÅÊëòË¶ÅÊàñÊ™¢Á¥¢Êì¥ÂÖÖ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÈùûÁµêÊßãÂåñÁöÑË®òÊÜ∂Ë°®Âæµ‰∏¶‰∏çËÉΩ‰øÉÈÄ≤Ë§áÈõúÊ±∫Á≠ñÂà∂ÂÆö‰∏≠ÂøÖ‰∏çÂèØÂ∞ëÁöÑÊé®ÁêÜÂíåË¶èÂäÉ„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü AriGraphÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠‰ª£ÁêÜÊßãÂª∫‰∫Ü‰∏ÄÂÄãË®òÊÜ∂ÂúñÔºåË©≤ÂúñÂú®Êé¢Á¥¢Áí∞Â¢ÉÊôÇÊï¥Âêà‰∫ÜË™ûÁæ©ÂíåÊÉÖÁØÄË®òÊÜ∂„ÄÇÈÄôÁ®ÆÂúñÂΩ¢ÁµêÊßã‰øÉÈÄ≤‰∫ÜÁõ∏‰∫íËÅØÁπ´ÁöÑÊ¶ÇÂøµÁöÑÊúâÊïàÈóúËÅØÊÄßÊ™¢Á¥¢ÔºåËàá‰ª£ÁêÜÁöÑÁï∂ÂâçÁãÄÊÖãÂíåÁõÆÊ®ôÁõ∏ÈóúÔºåÂæûËÄå‰ΩúÁÇ∫‰∏ÄÂÄãÊúâÊïàÁöÑÁí∞Â¢ÉÊ®°ÂûãÔºåÂ¢ûÂº∑‰∫Ü‰ª£ÁêÜÁöÑÊé¢Á¥¢ÂíåË¶èÂäÉËÉΩÂäõ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑ Ariadne LLM ‰ª£ÁêÜÔºåÈÖçÂÇô‰∫ÜÈÄôÁ®ÆÊèêË≠∞ÁöÑË®òÊÜ∂Êû∂ÊßãÔºå‰∏¶Â¢ûÂº∑‰∫ÜË¶èÂäÉÂíåÊ±∫Á≠ñÂà∂ÂÆöÔºåÊúâÊïàÂú∞ËôïÁêÜ‰∫Ü TextWorld Áí∞Â¢É‰∏≠Èõ∂Ê¨°Â≠∏ÁøíÁöÑË§áÈõú‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÂÑ™ÊñºÂ∑≤Âª∫Á´ãÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂÆåÊï¥Ê≠∑Âè≤„ÄÅÊëòË¶ÅÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºåÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÔºåÂåÖÊã¨‰æÜËá™Á¨¨‰∏ÄÂÄã TextWorld ÂïèÈ°åÁ´∂Ë≥ΩÁöÑÁÉπÈ£™ÊåëÊà∞ÂíåÊàøÂ±ãÊ∏ÖÊΩîÂíåÊãºÂúñÂ∞ãÂØ∂Á≠âÊñ∞‰ªªÂãô„ÄÇ

##### **Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM**
2407.04067v1 by Peiran Yao, Kostyantyn Guzhva, Denilson Barbosa

Symbolic sentence meaning representations, such as AMR (Abstract Meaning
Representation) provide expressive and structured semantic graphs that act as
intermediates that simplify downstream NLP tasks. However, the
instruction-following capability of large language models (LLMs) offers a
shortcut to effectively solve NLP tasks, questioning the utility of semantic
graphs. Meanwhile, recent work has also shown the difficulty of using meaning
representations merely as a helpful auxiliary for LLMs. We revisit the position
of semantic graphs in syntactic simplification, the task of simplifying
sentence structures while preserving their meaning, which requires semantic
understanding, and evaluate it on a new complex and natural dataset. The
AMR-based method that we propose, AMRS$^3$, demonstrates that state-of-the-art
meaning representations can lead to easy-to-implement simplification methods
with competitive performance and unique advantages in cost, interpretability,
and generalization. With AMRS$^3$ as an anchor, we discover that syntactic
simplification is a task where semantic graphs are helpful in LLM prompting. We
propose AMRCoC prompting that guides LLMs to emulate graph algorithms for
explicit symbolic reasoning on AMR graphs, and show its potential for improving
LLM on semantic-centered tasks like syntactic simplification.

ÊëòË¶ÅÔºöÁ¨¶ËôüÂè•Â≠êÊÑèÁæ©Ë°®ÂæµÔºå‰æãÂ¶Ç AMRÔºàÊäΩË±°ÊÑèÁæ©Ë°®ÂæµÔºâÔºåÊèê‰æõË°®ÈÅîÊÄßÂíåÁµêÊßãÂåñÁöÑË™ûÁæ©ÂúñË°®Ôºå‰ΩúÁÇ∫Á∞°Âåñ‰∏ãÊ∏∏ NLP ‰ªªÂãôÁöÑ‰∏≠‰ªã„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊç∑Âæë‰æÜÊúâÊïàËß£Ê±∫ NLP ‰ªªÂãôÔºåË≥™ÁñëË™ûÁæ©ÂúñË°®ÁöÑÊïàÁî®„ÄÇÂêåÊôÇÔºåÊúÄËøëÁöÑÁ†îÁ©∂‰πüË°®ÊòéÂÉÖÂ∞áÊÑèÁæ©Ë°®ÂæµÁî®‰Ωú LLM ÁöÑËºîÂä©Â∑•ÂÖ∑ÁöÑÈõ£Â∫¶„ÄÇÊàëÂÄëÈáçÊñ∞ÂØ©Ë¶ñË™ûÁæ©ÂúñË°®Âú®Ë™ûÊ≥ïÁ∞°Âåñ‰∏≠ÁöÑ‰ΩçÁΩÆÔºåË™ûÊ≥ïÁ∞°ÂåñÁöÑ‰ªªÂãôÊòØÂú®‰øùÁïôÂè•Â≠êÁµêÊßãÁöÑÂêåÊôÇÁ∞°ÂåñÂè•Â≠êÁµêÊßãÔºåÈÄôÈúÄË¶ÅË™ûÁæ©ÁêÜËß£Ôºå‰∏¶Âú®‰∏ÄÂÄãÊñ∞ÁöÑË§áÈõú‰∏îËá™ÁÑ∂ÁöÑÊï∏ÊìöÈõÜ‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÂü∫Êñº AMR ÁöÑÊñπÊ≥ï AMRS$^3$ Ë≠âÊòé‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÑèÁæ©Ë°®ÂæµÂèØ‰ª•Â∞éËá¥ÊòìÊñºÂØ¶ÁèæÁöÑÁ∞°ÂåñÊñπÊ≥ïÔºåÂú®ÊàêÊú¨„ÄÅÂèØËß£ÈáãÊÄßÂíåÊ≥õÂåñÊñπÈù¢ÂÖ∑ÊúâÁ´∂Áà≠ÂÑ™Âã¢ÂíåÁç®ÁâπÂÑ™Âã¢„ÄÇ‰ª• AMRS$^3$ ÁÇ∫Èå®ÈªûÔºåÊàëÂÄëÁôºÁèæË™ûÊ≥ïÁ∞°ÂåñÊòØ‰∏ÄÈ†ÖË™ûÁæ©ÂúñË°®ÊúâÂä©Êñº LLM ÊèêÁ§∫ÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÊèêÂá∫ AMRCoC ÊèêÁ§∫ÔºåÊåáÂ∞é LLM Ê®°Êì¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÔºåÂ∞ç AMR ÂúñÂΩ¢ÈÄ≤Ë°åÊòéÁ¢∫ÁöÑÁ¨¶ËôüÊé®ÁêÜÔºå‰∏¶Â±ïÁ§∫ÂÖ∂Âú®ÊîπÈÄ≤ LLM Âú®‰ª•Ë™ûÁæ©ÁÇ∫‰∏≠ÂøÉÁöÑ‰ªªÂãôÔºàÂ¶ÇË™ûÊ≥ïÁ∞°ÂåñÔºâÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning**
2407.03779v1 by Lei Yu, Jingcheng Niu, Zining Zhu, Gerald Penn

In this paper, we introduce a comprehensive reformulation of the task known
as Circuit Discovery, along with DiscoGP, a novel and effective algorithm based
on differentiable masking for discovering circuits. Circuit discovery is the
task of interpreting the computational mechanisms of language models (LMs) by
dissecting their functions and capabilities into sparse subnetworks (circuits).
We identified two major limitations in existing circuit discovery efforts: (1)
a dichotomy between weight-based and connection-edge-based approaches forces
researchers to choose between pruning connections or weights, thereby limiting
the scope of mechanistic interpretation of LMs; (2) algorithms based on
activation patching tend to identify circuits that are neither functionally
faithful nor complete. The performance of these identified circuits is
substantially reduced, often resulting in near-random performance in isolation.
Furthermore, the complement of the circuit -- i.e., the original LM with the
identified circuit removed -- still retains adequate performance, indicating
that essential components of a complete circuits are missed by existing
methods.
  DiscoGP successfully addresses the two aforementioned issues and demonstrates
state-of-the-art faithfulness, completeness, and sparsity. The effectiveness of
the algorithm and its novel structure open up new avenues of gathering new
insights into the internal workings of generative AI.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ∞çÁ®±ÁÇ∫ÈõªË∑ØÁôºÁèæ‰ªªÂãôÁöÑÂÖ®Èù¢ÈáçÊñ∞Ë°®Ëø∞Ôºå‰ª•Âèä DiscoGPÔºå‰∏ÄÁ®ÆÂü∫ÊñºÂèØÂæÆÈÅÆÁΩ©ÁöÑÁôºÁèæÈõªË∑ØÁöÑÊñ∞Á©é‰∏îÊúâÊïàÁöÑÊºîÁÆóÊ≥ï„ÄÇÈõªË∑ØÁôºÁèæÊòØÈÄèÈÅéÂ∞áÂÖ∂ÂäüËÉΩÂíåËÉΩÂäõËß£ÂâñÊàêÁ®ÄÁñèÂ≠êÁ∂≤Ë∑ØÔºàÈõªË∑ØÔºâ‰æÜË©ÆÈáãË™ûË®ÄÊ®°ÂûãÔºàLMÔºâÁöÑÈÅãÁÆóÊ©üÂà∂ÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÂú®ÁèæÊúâÁöÑÈõªË∑ØÁôºÁèæÂ∑•‰Ωú‰∏≠ÁôºÁèæ‰∫ÜÂÖ©ÂÄã‰∏ªË¶ÅÁöÑÈôêÂà∂ÔºöÔºà1ÔºâÂü∫ÊñºÊ¨äÈáçÂíåÂü∫ÊñºÈÄ£Êé•ÈÇäÁ∑£ÁöÑÊñπÊ≥ï‰πãÈñìÁöÑ‰∫åÂàÜÊ≥ïËø´‰ΩøÁ†îÁ©∂‰∫∫Âì°Âú®‰øÆÂâ™ÈÄ£Êé•ÊàñÊ¨äÈáç‰πãÈñìÈÄ≤Ë°åÈÅ∏ÊìáÔºåÂæûËÄåÈôêÂà∂‰∫Ü LM Ê©üÂà∂Ë©ÆÈáãÁöÑÁØÑÂúçÔºõÔºà2ÔºâÂü∫ÊñºÂïüÁî®‰øÆË£úÁöÑÊºîÁÆóÊ≥ïÂÇæÂêëÊñºË≠òÂà•Âú®ÂäüËÉΩ‰∏äÊó¢‰∏çÂø†ÂØ¶‰πü‰∏çÂÆåÊï¥ÁöÑÈõªË∑Ø„ÄÇÈÄô‰∫õÂ∑≤Ë≠òÂà•ÈõªË∑ØÁöÑÊïàËÉΩÂ§ßÂπÖÈôç‰ΩéÔºåÈÄöÂ∏∏Â∞éËá¥Â≠§Á´ãÁöÑËøë‰πéÈö®Ê©üÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÈõªË∑ØÁöÑË£úÊï∏‚Äî‚ÄîÂç≥ÁßªÈô§Â∑≤Ë≠òÂà•ÈõªË∑ØÁöÑÂéüÂßã LM‚Äî‚Äî‰ªç‰øùÁïô‰∫ÜË∂≥Â§†ÁöÑÊïàËÉΩÔºåÈÄôË°®Á§∫ÁèæÊúâÊñπÊ≥ïÈåØÂ§±‰∫ÜÂÆåÊï¥ÈõªË∑ØÁöÑÂü∫Êú¨ÁµÑÊàêÈÉ®ÂàÜ„ÄÇ
DiscoGP ÊàêÂäüÂú∞Ëß£Ê±∫‰∫Ü‰∏äËø∞ÂÖ©ÂÄãÂïèÈ°åÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂø†ÂØ¶Â∫¶„ÄÅÂÆåÊï¥ÊÄßÂíåÁ®ÄÁñèÊÄß„ÄÇË©≤ÊºîÁÆóÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÖ∂Êñ∞Á©éÁöÑÁµêÊßãÁÇ∫Ê∑±ÂÖ•Áû≠Ëß£ÁîüÊàêÂºè AI ÁöÑÂÖßÈÉ®ÈÅã‰ΩúÈñãÈó¢‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇ</paragraph>

##### **BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations**
2407.03314v1 by Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Chaojie Mao, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng

This paper presents Bag-of-Concept Graph (BACON) to gift models with limited
linguistic abilities to taste the privilege of Vision Language Models (VLMs)
and boost downstream tasks such as detection, visual question answering (VQA),
and image generation. Since the visual scenes in physical worlds are structured
with complex relations between objects, BACON breaks down annotations into
basic minimum elements and presents them in a graph structure. Element-wise
style enables easy understanding, and structural composition liberates
difficult locating. Careful prompt design births the BACON captions with the
help of public-available VLMs and segmentation methods. In this way, we gather
a dataset with 100K annotated images, which endow VLMs with remarkable
capabilities, such as accurately generating BACON, transforming prompts into
BACON format, envisioning scenarios in the style of BACONr, and dynamically
modifying elements within BACON through interactive dialogue and more. Wide
representative experiments, including detection, VQA, and image generation
tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel
in their current cutting-edge solutions.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫ Bag-of-Concept Graph (BACON)ÔºåËµã‰∫àËØ≠Ë®ÄËÉΩÂäõÊúâÈôêÁöÑÊ®°ÂûãÂìÅÂ∞ùËßÜËßâËØ≠Ë®ÄÊ®°Âûã (VLM) ÁöÑÁâπÊùÉÔºåÂπ∂ÊèêÂçá‰∏ãÊ∏∏‰ªªÂä°Ôºå‰æãÂ¶ÇÊ£ÄÊµã„ÄÅËßÜËßâÈóÆÁ≠î (VQA) ÂíåÂõæÂÉèÁîüÊàê„ÄÇÁî±‰∫éÁâ©ÁêÜ‰∏ñÁïå‰∏≠ÁöÑËßÜËßâÂú∫ÊôØÊòØÁî±ÂØπË±°‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÊûÑÂª∫ËÄåÊàêÁöÑÔºåÂõ†Ê≠§ BACON Â∞ÜÊ≥®ÈáäÂàÜËß£‰∏∫Âü∫Êú¨ÁöÑÊúÄÂ∞èÂÖÉÁ¥†ÔºåÂπ∂‰ª•ÂõæÂΩ¢ÁªìÊûÑÂëàÁé∞ÂÆÉ‰ª¨„ÄÇÂü∫‰∫éÂÖÉÁ¥†ÁöÑÈ£éÊ†º‰æø‰∫éÁêÜËß£ÔºåÁªìÊûÑÂåñÁªÑÂêàËß£Êîæ‰∫ÜÂõ∞ÈöæÁöÑÂÆö‰Ωç„ÄÇÂú®ÂÖ¨ÂÖ±ÂèØÁî® VLM ÂíåÂàÜÂâ≤ÊñπÊ≥ïÁöÑÂ∏ÆÂä©‰∏ãÔºåÁ≤æÂøÉËÆæËÆ°ÁöÑÊèêÁ§∫ÁîüÊàê‰∫Ü BACON Ê†áÈ¢ò„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊàë‰ª¨Êî∂ÈõÜ‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 100K Âº†Ê≥®ÈáäÂõæÂÉèÁöÑÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜËµã‰∫à VLM ÊòæËëóÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂáÜÁ°ÆÁîüÊàê BACON„ÄÅÂ∞ÜÊèêÁ§∫ËΩ¨Êç¢‰∏∫ BACON Ê†ºÂºè„ÄÅ‰ª• BACONr ÁöÑÈ£éÊ†ºËÆæÊÉ≥Âú∫ÊôØÔºå‰ª•ÂèäÈÄöËøá‰∫§‰∫íÂºèÂØπËØùÂä®ÊÄÅ‰øÆÊîπ BACON ‰∏≠ÁöÑÂÖÉÁ¥†Á≠âÁ≠â„ÄÇÂπøÊ≥õÁöÑ‰ª£Ë°®ÊÄßÂÆûÈ™åÔºåÂåÖÊã¨Ê£ÄÊµã„ÄÅVQA ÂíåÂõæÂÉèÁîüÊàê‰ªªÂä°ÔºåË°®Êòé BACON ‰Ωú‰∏∫‰∏ÄÊù°ÁîüÂëΩÁ∫øÔºåÂèØ‰ª•ÂÆûÁé∞‰ª•ÂâçÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰ªªÂä°ÔºåÊàñÂú®ÂΩìÂâçÁöÑÂ∞ñÁ´ØËß£ÂÜ≥ÊñπÊ°à‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

##### **Knowledge-based Consistency Testing of Large Language Models**
2407.12830v1 by Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay

In this work, we systematically expose and measure the inconsistency and
knowledge gaps of Large Language Models (LLMs). Specifically, we propose an
automated testing framework (called KONTEST) which leverages a knowledge graph
to construct test cases. KONTEST probes and measures the inconsistencies in the
LLM's knowledge of the world via a combination of semantically-equivalent
queries and test oracles (metamorphic or ontological oracle). KONTEST further
mitigates knowledge gaps via a weighted LLM model ensemble. Using four
state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that
KONTEST generates 19.2% error inducing inputs (1917 errors from 9983 test
inputs). It also reveals a 16.5% knowledge gap across all tested LLMs.
KONTEST's mitigation method reduces LLM knowledge gap by 32.48%. Our ablation
study further shows that GPT3.5 is not suitable for knowledge-based consistency
testing because it is only 60%-68% effective in knowledge construction.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Êè≠Èú≤‰∏¶Ë°°ÈáèÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÂíåÁü•Ë≠òÂ∑ÆË∑ù„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñÊ∏¨Ë©¶Ê°ÜÊû∂ (Á®±ÁÇ∫ KONTEST)ÔºåÂÆÉÂà©Áî®Áü•Ë≠òÂúñË≠ú‰æÜÂª∫ÊßãÊ∏¨Ë©¶Ê°à‰æã„ÄÇKONTEST ÈÄöÈÅéË™ûÁæ©Á≠âÊïàÊü•Ë©¢ÂíåÊ∏¨Ë©¶È†êË®Ä (ËÆäÂΩ¢ÊàñÊú¨È´îË´ñÈ†êË®Ä) ÁöÑÁµÑÂêà‰æÜÊé¢Ê∏¨ÂíåË°°Èáè LLM Â∞ç‰∏ñÁïåÁü•Ë≠òÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇKONTEST ÈÄ≤‰∏ÄÊ≠•ÈÄöÈÅéÂä†Ê¨ä LLM Ê®°ÂûãÈõÜÊàê‰æÜÁ∑©Ëß£Áü•Ë≠òÂ∑ÆË∑ù„ÄÇ‰ΩøÁî®ÂõõÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºàFalcon„ÄÅGemini„ÄÅGPT3.5 Âíå Llama2ÔºâÔºåÊàëÂÄëË°®Êòé KONTEST ÁîüÊàê‰∫Ü 19.2% ÁöÑÈåØË™§Ë™òÁôºËº∏ÂÖ•Ôºà9983 ÂÄãÊ∏¨Ë©¶Ëº∏ÂÖ•‰∏≠ÁöÑ 1917 ÂÄãÈåØË™§Ôºâ„ÄÇÂÆÉÈÇÑÊè≠Á§∫‰∫ÜÊâÄÊúâÊ∏¨Ë©¶ÁöÑ LLM ‰∏≠Êúâ 16.5% ÁöÑÁü•Ë≠òÂ∑ÆË∑ù„ÄÇKONTEST ÁöÑÁ∑©Ëß£ÊñπÊ≥ïÂ∞á LLM Áü•Ë≠òÂ∑ÆË∑ùÊ∏õÂ∞ë‰∫Ü 32.48%„ÄÇÊàëÂÄëÁöÑÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Ë°®ÊòéÔºåGPT3.5 ‰∏çÈÅ©ÂêàÁî®ÊñºÂü∫ÊñºÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÊ∏¨Ë©¶ÔºåÂõ†ÁÇ∫ÂÆÉÂú®Áü•Ë≠òÂª∫Êßã‰∏≠Âè™Êúâ 60%-68% ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models**
2407.02936v1 by Zike Yuan, Ming Liu, Hui Wang, Bing Qin

Evaluating the graph comprehension and reasoning abilities of Large Language
Models (LLMs) is challenging and often incomplete. Existing benchmarks focus
primarily on pure graph understanding, lacking a comprehensive evaluation
across all graph types and detailed capability definitions. This paper presents
GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and
reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and
test models on pure graph and heterogeneous graphs, subdividing capabilities
into 10 distinct areas tested through 19 tasks. Our benchmark includes 11
datasets with 5,140 graphs of varying complexity. We evaluated three
closed-source and seven open-source LLMs, conducting thorough analyses from
both ability and task perspectives. Key findings reveal that semantic
enrichment enhances reasoning performance, node ordering impacts task success,
and the ability to process longer texts does not necessarily improve graph
comprehension or reasoning. GraCoRe is open-sourced at
https://github.com/ZIKEYUAN/GraCoRe

ÊëòË¶ÅÔºöË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂúñÂΩ¢ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÈÄöÂ∏∏‰∏çÂÆåÊï¥„ÄÇÁèæÊúâÁöÑÂü∫Ê∫ñ‰∏ªË¶ÅËëóÈáçÊñºÁ¥îÁ≤πÁöÑÂúñÂΩ¢ÁêÜËß£ÔºåÁº∫‰πèÂ∞çÊâÄÊúâÂúñÂΩ¢È°ûÂûãÂíåË©≥Á¥∞ÂäüËÉΩÂÆöÁæ©ÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü GraCoReÔºå‰∏ÄÂÄãÁî®ÊñºÁ≥ªÁµ±Ë©ï‰º∞ LLM ÁöÑÂúñÂΩ¢ÁêÜËß£ÂíåÊé®ÁêÜÁöÑÂü∫Ê∫ñ„ÄÇGraCoRe ‰ΩøÁî®‰∏âÂ±§ÈöéÂ±§ÂàÜÈ°ûÊ≥ïÂ∞çÊ®°ÂûãÈÄ≤Ë°åÂàÜÈ°ûÂíåÊ∏¨Ë©¶ÔºåÂ∞áÂäüËÉΩÁ¥∞ÂàÜÁÇ∫ 10 ÂÄã‰∏çÂêåÁöÑÈ†òÂüüÔºå‰∏¶ÈÄöÈÅé 19 ÂÄã‰ªªÂãôÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÊàëÂÄëÁöÑÂü∫Ê∫ñÂåÖÂê´ 11 ÂÄãÊï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 5,140 ÂÄã‰∏çÂêåË§áÈõúÂ∫¶ÁöÑÂúñÂΩ¢„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏âÂÄãÈñâÊ∫êÂíå‰∏ÉÂÄãÈñãÊ∫ê LLMÔºåÂæûËÉΩÂäõÂíå‰ªªÂãôËßíÂ∫¶ÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑÂàÜÊûê„ÄÇ‰∏ªË¶ÅÁôºÁèæË°®ÊòéË™ûÁæ©Ë±êÂØåÂåñÂ¢ûÂº∑‰∫ÜÊé®ÁêÜÊÄßËÉΩÔºåÁØÄÈªûÊéíÂ∫èÂΩ±Èüø‰ªªÂãôÊàêÂäüÔºåËÄåËôïÁêÜËºÉÈï∑ÊñáÊú¨ÁöÑËÉΩÂäõ‰∏¶‰∏ç‰∏ÄÂÆöËÉΩÊîπÂñÑÂúñÂΩ¢ÁêÜËß£ÊàñÊé®ÁêÜ„ÄÇGraCoRe Âú® https://github.com/ZIKEYUAN/GraCoRe ÈñãÊ∫ê

##### **Croppable Knowledge Graph Embedding**
2407.02779v1 by Yushan Zhu, Wen Zhang, Zhiqiang Liu, Mingyang Chen, Lei Liang, Huajun Chen

Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs)
to serve various artificial intelligence tasks. The suitable dimensions of the
embeddings depend on the storage and computing conditions of the specific
application scenarios. Once a new dimension is required, a new KGE model needs
to be trained from scratch, which greatly increases the training cost and
limits the efficiency and flexibility of KGE in serving various scenarios. In
this work, we propose a novel KGE training framework MED, through which we
could train once to get a croppable KGE model applicable to multiple scenarios
with different dimensional requirements, sub-models of the required dimensions
can be cropped out of it and used directly without any additional training. In
MED, we propose a mutual learning mechanism to improve the low-dimensional
sub-models performance and make the high-dimensional sub-models retain the
capacity that low-dimensional sub-models have, an evolutionary improvement
mechanism to promote the high-dimensional sub-models to master the knowledge
that the low-dimensional sub-models can not learn, and a dynamic loss weight to
balance the multiple losses adaptively. Experiments on 3 KGE models over 4
standard KG completion datasets, 3 real application scenarios over a real-world
large-scale KG, and the experiments of extending MED to the language model BERT
show the effectiveness, high efficiency, and flexible extensibility of MED.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñÂµåÂÖ• (KGE) ÊòØÁü•Ë≠òÂúñ (KG) Áî®ÊñºÊúçÂãôÂêÑÁ®Æ‰∫∫Â∑•Êô∫ÊÖß‰ªªÂãôÁöÑÂ∏∏Ë¶ãÊñπÊ≥ï„ÄÇÂµåÂÖ•ÁöÑÈÅ©Áï∂Á∂≠Â∫¶ÂèñÊ±∫ÊñºÁâπÂÆöÊáâÁî®Â†¥ÊôØÁöÑÂÑ≤Â≠òÂíåÈÅãÁÆóÊ¢ù‰ª∂„ÄÇ‰∏ÄÊó¶ÈúÄË¶ÅÊñ∞ÁöÑÁ∂≠Â∫¶ÔºåÂ∞±ÈúÄË¶ÅÂæûÈ†≠Ë®ìÁ∑¥Êñ∞ÁöÑ KGE Ê®°ÂûãÔºåÈÄôÂ§ßÂ§ßÂ¢ûÂä†‰∫ÜË®ìÁ∑¥ÊàêÊú¨Ôºå‰∏¶ÈôêÂà∂‰∫Ü KGE Âú®ÊúçÂãôÂêÑÁ®ÆÂ†¥ÊôØ‰∏≠ÁöÑÊïàÁéáÂíåÈùàÊ¥ªÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ KGE Ë®ìÁ∑¥Ê°ÜÊû∂ MEDÔºåÈÄöÈÅéÂÆÉÔºåÊàëÂÄëÂèØ‰ª•Ë®ìÁ∑¥‰∏ÄÊ¨°‰ª•Áç≤ÂæóÈÅ©Áî®ÊñºÂÖ∑Êúâ‰∏çÂêåÁ∂≠Â∫¶ÈúÄÊ±ÇÁöÑÂ§öÂÄãÂ†¥ÊôØÁöÑÂèØË£ÅÂâ™ KGE Ê®°ÂûãÔºåÂèØ‰ª•Âæû‰∏≠Ë£ÅÂâ™Âá∫ÊâÄÈúÄÁ∂≠Â∫¶ÁöÑÂ≠êÊ®°Âûã‰∏¶Áõ¥Êé•‰ΩøÁî®ÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÈ°çÂ§ñË®ìÁ∑¥„ÄÇÂú® MED ‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁõ∏‰∫íÂ≠∏ÁøíÊ©üÂà∂Ôºå‰ª•ÊèêÈ´ò‰ΩéÁ∂≠Â≠êÊ®°ÂûãÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÈ´òÁ∂≠Â≠êÊ®°Âûã‰øùÁïô‰ΩéÁ∂≠Â≠êÊ®°ÂûãÂÖ∑ÊúâÁöÑËÉΩÂäõÔºå‰∏ÄÁ®ÆÈÄ≤ÂåñÊîπÈÄ≤Ê©üÂà∂Ôºå‰ª•‰øÉÈÄ≤È´òÁ∂≠Â≠êÊ®°ÂûãÊéåÊè°‰ΩéÁ∂≠Â≠êÊ®°ÂûãÁÑ°Ê≥ïÂ≠∏ÁøíÁöÑÁü•Ë≠òÔºå‰ª•Âèä‰∏ÄÁ®ÆÂãïÊÖãÊêçÂ§±Ê¨äÈáçÔºå‰ª•Ëá™ÈÅ©ÊáâÂú∞Âπ≥Ë°°Â§öÈáçÊêçÂ§±„ÄÇÂú® 4 ÂÄãÊ®ôÊ∫ñ KG ÂÆåÊàêË≥áÊñôÈõÜ‰∏äÁöÑ 3 ÂÄã KGE Ê®°Âûã„ÄÅ‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÂ§ßË¶èÊ®° KG ‰∏äÁöÑ 3 ÂÄãÂØ¶ÈöõÊáâÁî®Â†¥ÊôØ‰ª•ÂèäÂ∞á MED Êì¥Â±ïÂà∞Ë™ûË®ÄÊ®°Âûã BERT ÁöÑÂØ¶È©ó‰∏≠ÔºåÂ±ïÁ§∫‰∫Ü MED ÁöÑÊúâÊïàÊÄß„ÄÅÈ´òÊïàÁéáÂíåÈùàÊ¥ªÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇ

##### **Reasoning in Large Language Models: A Geometric Perspective**
2407.02678v1 by Romain Cosentino, Sarath Shekkizhar

The advancement of large language models (LLMs) for real-world applications
hinges critically on enhancing their reasoning capabilities. In this work, we
explore the reasoning abilities of large language models (LLMs) through their
geometrical understanding. We establish a connection between the expressive
power of LLMs and the density of their self-attention graphs. Our analysis
demonstrates that the density of these graphs defines the intrinsic dimension
of the inputs to the MLP blocks. We demonstrate through theoretical analysis
and toy examples that a higher intrinsic dimension implies a greater expressive
capacity of the LLM. We further provide empirical evidence linking this
geometric framework to recent advancements in methods aimed at enhancing the
reasoning capabilities of LLMs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÈÄ≤Â±ïÔºåÈóúÈçµÂú®ÊñºÊèêÂçáÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂπæ‰ΩïÁêÜËß£ÔºåÊé¢Ë®éÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü LLM ÁöÑË°®ÈÅîËÉΩÂäõËàáÂÖ∂Ëá™Ê≥®ÊÑèÂäõÂúñÂØÜÂ∫¶‰πãÈñìÁöÑÈóúËÅØ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË≠âÊòéÔºåÈÄô‰∫õÂúñÁöÑÂØÜÂ∫¶ÂÆöÁæ©‰∫Ü MLP Â°äËº∏ÂÖ•ÁöÑÂÖßÂú®Á∂≠Â∫¶„ÄÇÊàëÂÄëÈÄèÈÅéÁêÜË´ñÂàÜÊûêÂíåÁé©ÂÖ∑ÁØÑ‰æãË≠âÊòéÔºåËºÉÈ´òÁöÑÂÖßÂú®Á∂≠Â∫¶ÊÑèÂë≥Ëëó LLM ÂÖ∑ÊúâÊõ¥Â§ßÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êèê‰æõÁ∂ìÈ©óË≠âÊìöÔºåÂ∞áÈÄôÂÄãÂπæ‰ΩïÊ°ÜÊû∂ÈÄ£ÁµêÂà∞ÊúÄËøëÂú®Êó®Âú®Â¢ûÂº∑ LLM Êé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ï‰∏≠ÂèñÂæóÁöÑÈÄ≤Â±ï„ÄÇ

##### **LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison**
2407.02659v2 by Devam Mondal, Carlo Lipizzi

In light of recent legal allegations brought by publishers, newspapers, and
other creators of copyrighted corpora against large language model developers
who use their copyrighted materials for training or fine-tuning purposes, we
propose a novel system, a variant of a plagiarism detection system, that
assesses whether a knowledge source has been used in the training or
fine-tuning of a large language model. Unlike current methods, we utilize an
approach that uses Resource Description Framework (RDF) triples to create
knowledge graphs from both a source document and an LLM continuation of that
document. These graphs are then analyzed with respect to content using cosine
similarity and with respect to structure using a normalized version of graph
edit distance that shows the degree of isomorphism. Unlike traditional
plagiarism systems that focus on content matching and keyword identification
between a source and a target corpus, our approach enables a broader and more
accurate evaluation of similarity between a source document and LLM
continuation by focusing on relationships between ideas and their organization
with regards to others. Additionally, our approach does not require access to
LLM metrics like perplexity that may be unavailable in closed large language
model "black-box" systems, as well as the training corpus. We thus assess
whether an LLM has "plagiarized" a corpus in its continuation through
similarity measures. A prototype of our system will be found on a hyperlinked
GitHub repository.

ÊëòË¶ÅÔºöÈâ¥‰∫éÂá∫ÁâàÂïÜ„ÄÅÊä•Á∫∏ÂíåÂÖ∂‰ªñÂèóÁâàÊùÉ‰øùÊä§ËØ≠ÊñôÂ∫ìÁöÑÂàõ‰ΩúËÄÖÈíàÂØπ‰ΩøÁî®ÂÖ∂ÂèóÁâàÊùÉ‰øùÊä§ËµÑÊñôËøõË°åËÆ≠ÁªÉÊàñÂæÆË∞ÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂºÄÂèëËÄÖÊèêÂá∫ÁöÑËøëÊúüÊ≥ïÂæãÊåáÊéßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ≥ªÁªüÔºåÂç≥ÊäÑË¢≠Ê£ÄÊµãÁ≥ªÁªüÁöÑ‰∏Ä‰∏™Âèò‰ΩìÔºåËØ•Á≥ªÁªüËØÑ‰º∞Áü•ËØÜÊù•Ê∫êÊòØÂê¶Â∑≤Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇ‰∏éÂΩìÂâçÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨‰ΩøÁî®‰∏ÄÁßçÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî®ËµÑÊ∫êÊèèËø∞Ê°ÜÊû∂ (RDF) ‰∏âÂÖÉÁªÑ‰ªéÊ∫êÊñáÊ°£ÂíåËØ•ÊñáÊ°£ÁöÑ LLM Áª≠ÁØáÂàõÂª∫Áü•ËØÜÂõæË∞±„ÄÇÁÑ∂Âêé‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÊÄßÂàÜÊûêËøô‰∫õÂõæË∞±ÁöÑÂÜÖÂÆπÔºåÂπ∂‰ΩøÁî®ÂõæÁºñËæëË∑ùÁ¶ªÁöÑÊ†áÂáÜÂåñÁâàÊú¨ÂàÜÊûêÂÖ∂ÁªìÊûÑÔºåËØ•ÁâàÊú¨ÊòæÁ§∫ÂêåÊûÑÁöÑÁ®ãÂ∫¶„ÄÇ‰∏é‰∏ìÊ≥®‰∫éÊ∫êËØ≠ÊñôÂ∫ìÂíåÁõÆÊ†áËØ≠ÊñôÂ∫ì‰πãÈó¥ÁöÑÂÜÖÂÆπÂåπÈÖçÂíåÂÖ≥ÈîÆËØçËØÜÂà´ÁöÑ‰º†ÁªüÊäÑË¢≠Á≥ªÁªü‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂÖ≥Ê≥®ÊÄùÊÉ≥‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂèäÂÖ∂‰∏éÂÖ∂‰ªñÊÄùÊÉ≥ÁöÑÁªÑÁªáÊñπÂºèÔºåÂØπÊ∫êÊñáÊ°£Âíå LLM Áª≠ÁØá‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßËøõË°åÊõ¥ÂπøÊ≥õ„ÄÅÊõ¥ÂáÜÁ°ÆÁöÑËØÑ‰º∞„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏çÈúÄË¶ÅËÆøÈóÆ LLM ÊåáÊ†áÔºå‰æãÂ¶ÇÂú®Â∞ÅÈó≠ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‚ÄúÈªëÂå£Â≠ê‚ÄùÁ≥ªÁªü‰ª•ÂèäËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠ÂèØËÉΩÊó†Ê≥ïËé∑ÂæóÁöÑÂõ∞ÊÉëÂ∫¶„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÈÄöËøáÁõ∏‰ººÊÄßÂ∫¶ÈáèËØÑ‰º∞ LLM ÊòØÂê¶Âú®ÂÖ∂Áª≠ÁØá‰∏≠‚ÄúÊäÑË¢≠‚Äù‰∫ÜËØ≠ÊñôÂ∫ì„ÄÇÊàë‰ª¨Á≥ªÁªüÁöÑÂéüÂûãÂ∞ÜÂèØ‰ª•Âú®Ë∂ÖÈìæÊé•ÁöÑ GitHub Â≠òÂÇ®Â∫ì‰∏≠ÊâæÂà∞„ÄÇ

##### **Multi-Peptide: Multimodality Leveraged Language-Graph Learning of Peptide Properties**
2407.03380v1 by Srivathsan Badrinarayanan, Chakradhar Guntuboina, Parisa Mollaei, Amir Barati Farimani

Peptides are essential in biological processes and therapeutics. In this
study, we introduce Multi-Peptide, an innovative approach that combines
transformer-based language models with Graph Neural Networks (GNNs) to predict
peptide properties. We combine PeptideBERT, a transformer model tailored for
peptide property prediction, with a GNN encoder to capture both sequence-based
and structural features. By employing Contrastive Language-Image Pre-training
(CLIP), Multi-Peptide aligns embeddings from both modalities into a shared
latent space, thereby enhancing the model's predictive accuracy. Evaluations on
hemolysis and nonfouling datasets demonstrate Multi-Peptide's robustness,
achieving state-of-the-art 86.185% accuracy in hemolysis prediction. This study
highlights the potential of multimodal learning in bioinformatics, paving the
way for accurate and reliable predictions in peptide-based research and
applications.

ÊëòË¶ÅÔºöËÇΩÂú®ÁîüÁâ©ÈÅéÁ®ãÂíåÊ≤ªÁôÇ‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ§öËÇΩÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÂü∫ÊñºËΩâÊèõÂô®ÁöÑË™ûË®ÄÊ®°ÂûãÂíåÂúñÁ•ûÁ∂ìÁ∂≤Áµ° (GNN) ‰æÜÈ†êÊ∏¨ËÇΩÁöÑÊÄßË≥™„ÄÇÊàëÂÄëÁµêÂêà‰∫ÜÂ∞àÈñÄÁî®ÊñºËÇΩÊÄßË≥™È†êÊ∏¨ÁöÑËΩâÊèõÂô®Ê®°Âûã PeptideBERT Âíå GNN Á∑®Á¢ºÂô®Ôºå‰ª•ÊçïÁç≤Âü∫ÊñºÂ∫èÂàóÂíåÁµêÊßãÁöÑÁâπÂæµ„ÄÇÈÄöÈÅéÊé°Áî®Â∞çÊØîË™ûË®ÄÂúñÂÉèÈ†êË®ìÁ∑¥ (CLIP)ÔºåÂ§öËÇΩÂ∞á‰æÜËá™ÂÖ©Á®ÆÊ®°ÊÖãÁöÑÂµåÂÖ•Â∞çÈΩäÂà∞‰∏ÄÂÄãÂÖ±‰∫´ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠ÔºåÂæûËÄåÂ¢ûÂº∑Ê®°ÂûãÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÂ∞çÊ∫∂Ë°ÄÂíåÊäóÊ±°Êï∏ÊìöÈõÜÁöÑË©ï‰º∞Ë≠âÊòé‰∫ÜÂ§öËÇΩÁöÑÁ©©ÂÅ•ÊÄßÔºåÂú®Ê∫∂Ë°ÄÈ†êÊ∏¨‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑ 86.185% Ê∫ñÁ¢∫Áéá„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÁîüÁâ©‰ø°ÊÅØÂ≠∏‰∏≠Â§öÊ®°ÊÖãÂ≠∏ÁøíÁöÑÊΩõÂäõÔºåÁÇ∫Âü∫ÊñºËÇΩÁöÑÁ†îÁ©∂ÂíåÊáâÁî®‰∏≠ÁöÑÊ∫ñÁ¢∫‰∏îÂèØÈù†ÁöÑÈ†êÊ∏¨Èã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

##### **Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**
2407.02352v1 by Pritish Sahu, Karan Sikka, Ajay Divakaran

Large Visual Language Models (LVLMs) struggle with hallucinations in visual
instruction following task(s), limiting their trustworthiness and real-world
applicability. We propose Pelican -- a novel framework designed to detect and
mitigate hallucinations through claim verification. Pelican first decomposes
the visual claim into a chain of sub-claims based on first-order predicates.
These sub-claims consist of (predicate, question) pairs and can be
conceptualized as nodes of a computational graph. We then use
Program-of-Thought prompting to generate Python code for answering these
questions through flexible composition of external tools. Pelican improves over
prior work by introducing (1) intermediate variables for precise grounding of
object instances, and (2) shared computation for answering the sub-question to
enable adaptive corrections and inconsistency identification. We finally use
reasoning abilities of LLM to verify the correctness of the the claim by
considering the consistency and confidence of the (question, answer) pairs from
each sub-claim. Our experiments reveal a drop in hallucination rate by
$\sim$8%-32% across various baseline LVLMs and a 27% drop compared to
approaches proposed for hallucination mitigation on MMHal-Bench. Results on two
other benchmarks further corroborate our results.

ÊëòË¶ÅÔºöÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã (LVLMs) Âú®ËßÜËßâÊåá‰ª§ÈÅµÂæ™‰ªªÂä°‰∏≠‰ºö‰∫ßÁîüÂπªËßâÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄßÂíåÁé∞ÂÆû‰∏ñÁïåÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü Pelican‚Äî‚Äî‰∏ÄÁßçÊó®Âú®ÈÄöËøáÂ£∞ÊòéÈ™åËØÅÊù•Ê£ÄÊµãÂíåÂáèËΩªÂπªËßâÁöÑÊñ∞ÂûãÊ°ÜÊû∂„ÄÇPelican È¶ñÂÖàÊ†πÊçÆ‰∏ÄÈò∂Ë∞ìËØçÂ∞ÜËßÜËßâÂ£∞ÊòéÂàÜËß£Êàê‰∏Ä‰∏™Â≠êÂ£∞ÊòéÈìæ„ÄÇËøô‰∫õÂ≠êÂ£∞ÊòéÁî± (Ë∞ìËØç„ÄÅÈóÆÈ¢ò) ÂØπÁªÑÊàêÔºåÂèØ‰ª•Ë¢´Ê¶ÇÂøµÂåñ‰∏∫ËÆ°ÁÆóÂõæÁöÑËäÇÁÇπ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨‰ΩøÁî®ÊÄùÊÉ≥ËÆ°ÂàíÊèêÁ§∫Êù•ÁîüÊàê Python ‰ª£Á†ÅÔºåÈÄöËøáÂ§ñÈÉ®Â∑•ÂÖ∑ÁöÑÁÅµÊ¥ªÁªÑÂêàÊù•ÂõûÁ≠îËøô‰∫õÈóÆÈ¢ò„ÄÇPelican ÈÄöËøáÂºïÂÖ• (1) Áî®‰∫éÂØπË±°ÂÆû‰æãÁ≤æÁ°ÆÊé•Âú∞ÁöÑ‰∏≠Èó¥ÂèòÈáèÔºå‰ª•Âèä (2) Áî®‰∫éÂõûÁ≠îÂ≠êÈóÆÈ¢ò‰ª•ÂÆûÁé∞Ëá™ÈÄÇÂ∫îÊ†°Ê≠£Âíå‰∏ç‰∏ÄËá¥ÊÄßËØÜÂà´ÁöÑÂÖ±‰∫´ËÆ°ÁÆóÔºåÊîπËøõ‰∫Ü‰πãÂâçÁöÑÂ∑•‰Ωú„ÄÇÊàë‰ª¨ÊúÄÁªà‰ΩøÁî® LLM ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈÄöËøáËÄÉËôëÊØè‰∏™Â≠êÂ£∞ÊòéÁöÑ (ÈóÆÈ¢ò„ÄÅÁ≠îÊ°à) ÂØπÁöÑ‰∏ÄËá¥ÊÄßÂíåÁΩÆ‰ø°Â∫¶Êù•È™åËØÅÂ£∞ÊòéÁöÑÊ≠£Á°ÆÊÄß„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®ÂêÑÁßçÂü∫Á∫ø LVLMs ‰∏≠ÔºåÂπªËßâÁéá‰∏ãÈôç‰∫ÜÁ∫¶ 8%-32%Ôºå‰∏é MMHal-Bench ‰∏äÊèêÂá∫ÁöÑÂπªËßâÁºìËß£ÊñπÊ≥ïÁõ∏ÊØîÔºå‰∏ãÈôç‰∫Ü 27%„ÄÇÂú®Âè¶Â§ñ‰∏§‰∏™Âü∫ÂáÜ‰∏äÁöÑÁªìÊûúËøõ‰∏ÄÊ≠•ËØÅÂÆû‰∫ÜÊàë‰ª¨ÁöÑÁªìÊûú„ÄÇ

##### **Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**
2407.01992v1 by Nishant Balepur, Rachel Rudinger

Recent work shows that large language models (LLMs) can answer
multiple-choice questions using only the choices, but does this mean that MCQA
leaderboard rankings of LLMs are largely influenced by abilities in
choices-only settings? To answer this, we use a contrast set that probes if
LLMs over-rely on choices-only shortcuts in MCQA. While previous works build
contrast sets via expensive human annotations or model-generated data which can
be biased, we employ graph mining to extract contrast sets from existing MCQA
datasets. We use our method on UnifiedQA, a group of six commonsense reasoning
datasets with high choices-only accuracy, to build an 820-question contrast
set. After validating our contrast set, we test 12 LLMs, finding that these
models do not exhibit reliance on choice-only shortcuts when given both the
question and choices. Thus, despite the susceptibility~of MCQA to high
choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA
leaderboards just due to their ability to exploit choices-only shortcuts.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰ªÖ‰ΩøÁî®ÈÄâÈ°πÂ∞±ËÉΩÂõûÁ≠îÂ§öÈ°πÈÄâÊã©È¢òÔºå‰ΩÜËøôÊòØÂê¶Ë°®Á§∫Â§öÈ°πÈÄâÊã©ÈóÆÁ≠î (MCQA) ÊéíË°åÊ¶ú‰∏äÁöÑ LLM ‰∏ªË¶ÅÂèóÈôê‰∫é‰ªÖÈÄâÈ°πËÆæÁΩÆ‰∏≠ÁöÑËÉΩÂäõÔºü‰∏∫‰∫ÜÂõûÁ≠îËøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ΩøÁî®ÂØπÊØîÈõÜÊù•Êé¢Êü• LLM Âú® MCQA ‰∏≠ÊòØÂê¶ËøáÂ∫¶‰æùËµñ‰ªÖÈÄâÈ°πÊç∑ÂæÑ„ÄÇËôΩÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄöËøáÊòÇË¥µÁöÑ‰∫∫Â∑•Ê≥®ÈáäÊàñÂèØËÉΩÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÊ®°ÂûãÁîüÊàêÊï∞ÊçÆÊù•ÊûÑÂª∫ÂØπÊØîÈõÜÔºå‰ΩÜÊàë‰ª¨ÈááÁî®ÂõæÊåñÊéò‰ªéÁé∞Êúâ MCQA Êï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÂØπÊØîÈõÜ„ÄÇÊàë‰ª¨‰ΩøÁî®Êàë‰ª¨ÁöÑÊñπÊ≥ïÂú® UnifiedQA ‰∏äÔºåËøôÊòØ‰∏Ä‰∏™Áî±ÂÖ≠‰∏™ÂÖ∑ÊúâÈ´ò‰ªÖÈÄâÈ°πÂáÜÁ°ÆÁéáÁöÑÂ∏∏ËØÜÊé®ÁêÜÊï∞ÊçÆÈõÜÁªÑÊàêÁöÑÁªÑÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ 820 È¢òÁöÑÂØπÊØîÈõÜ„ÄÇÂú®È™åËØÅÊàë‰ª¨ÁöÑÂØπÊØîÈõÜÂêéÔºåÊàë‰ª¨ÊµãËØï‰∫Ü 12 ‰∏™ LLMÔºåÂèëÁé∞ÂΩìÂêåÊó∂ÁªôÂá∫ÈóÆÈ¢òÂíåÈÄâÈ°πÊó∂ÔºåËøô‰∫õÊ®°Âûã‰∏ç‰ºöË°®Áé∞Âá∫ÂØπ‰ªÖÈÄâÈ°πÊç∑ÂæÑÁöÑ‰æùËµñ„ÄÇÂõ†Ê≠§ÔºåÂ∞ΩÁÆ° MCQA ÂÆπÊòìÂèóÂà∞È´ò‰ªÖÈÄâÈ°πÂáÜÁ°ÆÁéáÁöÑÂΩ±ÂìçÔºå‰ΩÜÊàë‰ª¨ËÆ§‰∏∫ LLM Âú® MCQA ÊéíË°åÊ¶ú‰∏äËé∑ÂæóÈ´òÊéíÂêçÂπ∂Èùû‰ªÖ‰ªÖÂõ†‰∏∫ÂÆÉ‰ª¨Âà©Áî®‰ªÖÈÄâÈ°πÊç∑ÂæÑÁöÑËÉΩÂäõ„ÄÇ

##### **CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents**
2407.01511v1 by Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, Philip Torr, Bernard Ghanem, Guohao Li

The development of autonomous agents increasingly relies on Multimodal
Language Models (MLMs) to perform tasks described in natural language with GUI
environments, such as websites, desktop computers, or mobile phones. Existing
benchmarks for MLM agents in interactive environments are limited by their
focus on a single environment, lack of detailed and generalized evaluation
methods, and the complexities of constructing tasks and evaluators. To overcome
these limitations, we introduce Crab, the first agent benchmark framework
designed to support cross-environment tasks, incorporating a graph-based
fine-grained evaluation method and an efficient mechanism for task and
evaluator construction. Our framework supports multiple devices and can be
easily extended to any environment with a Python interface. Leveraging Crab, we
developed a cross-platform Crab Benchmark-v0 comprising 100 tasks in computer
desktop and mobile phone environments. We evaluated four advanced MLMs using
different single and multi-agent system configurations on this benchmark. The
experimental results demonstrate that the single agent with GPT-4o achieves the
best completion ratio of 35.26%. All framework code, agent code, and task
datasets are publicly available at https://github.com/camel-ai/crab.

ÊëòË¶ÅÔºöËá™‰∏ª‰ª£ÁêÜÁöÑÈñãÁôºË∂ä‰æÜË∂ä‰æùË≥¥Â§öÊ®°ÊÖãË™ûË®ÄÊ®°Âûã (MLM)Ôºå‰ª•Âú®ÂÖ∑Êúâ GUI Áí∞Â¢ÉÔºà‰æãÂ¶ÇÁ∂≤Á´ô„ÄÅÊ°å‰∏äÂûãÈõªËÖ¶ÊàñÊâãÊ©üÔºâÁöÑËá™ÁÑ∂Ë™ûË®Ä‰∏≠Âü∑Ë°å‰ªªÂãô„ÄÇÁèæÊúâÁöÑ‰∫íÂãïÁí∞Â¢É‰∏≠ MLM ‰ª£ÁêÜÁöÑÂü∫Ê∫ñÂèóÂà∞‰ª•‰∏ãÈôêÂà∂ÔºöÂÆÉÂÄëÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁí∞Â¢É„ÄÅÁº∫‰πèË©≥Á¥∞‰∏îÈÄöÁî®ÁöÑË©ï‰º∞ÊñπÊ≥ïÔºå‰ª•ÂèäÂª∫Êßã‰ªªÂãôÂíåË©ï‰º∞Âô®ÁöÑË§áÈõúÊÄß„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü CrabÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄã‰ª£ÁêÜÂü∫Ê∫ñÊû∂ÊßãÔºåÊó®Âú®ÊîØÊè¥Ë∑®Áí∞Â¢É‰ªªÂãôÔºå‰∏¶ÁµêÂêà‰∫ÜÂü∫ÊñºÂúñÂΩ¢ÁöÑÁ¥∞Á≤íÂ∫¶Ë©ï‰º∞ÊñπÊ≥ïÂíå‰ªªÂãôËàáË©ï‰º∞Âô®Âª∫ÊßãÁöÑÊúâÊïàÊ©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊîØÊè¥Â§öÁ®ÆË£ùÁΩÆÔºå‰∏¶‰∏îÂèØ‰ª•ËºïÈ¨ÜÂú∞Êì¥ÂÖÖÂà∞‰ªª‰ΩïÂÖ∑Êúâ Python ‰ªãÈù¢ÁöÑÁí∞Â¢É„ÄÇÂà©Áî® CrabÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãË∑®Âπ≥Âè∞ÁöÑ Crab Benchmark-v0ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÈõªËÖ¶Ê°å‰∏äÂûãÈõªËÖ¶ÂíåÊâãÊ©üÁí∞Â¢É‰∏≠ÁöÑ 100 ÂÄã‰ªªÂãô„ÄÇÊàëÂÄë‰ΩøÁî®‰∏çÂêåÁöÑÂñÆ‰∏ÄÂíåÂ§ö‰ª£ÁêÜÁ≥ªÁµ±ÈÖçÁΩÆÔºåÂú®ÈÄôÂÄãÂü∫Ê∫ñ‰∏äË©ï‰º∞‰∫ÜÂõõÁ®ÆÂÖàÈÄ≤ÁöÑ MLM„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂÖ∑Êúâ GPT-4o ÁöÑÂñÆ‰∏Ä‰ª£ÁêÜÂØ¶Áèæ‰∫Ü 35.26% ÁöÑÊúÄ‰Ω≥ÂÆåÊàêÁéá„ÄÇÊâÄÊúâÊû∂ÊßãÁ®ãÂºèÁ¢º„ÄÅ‰ª£ÁêÜÁ®ãÂºèÁ¢ºÂíå‰ªªÂãôË≥áÊñôÈõÜÈÉΩÂÖ¨ÈñãÊñº https://github.com/camel-ai/crab„ÄÇ

##### **Dynamic Few-Shot Learning for Knowledge Graph Question Answering**
2407.01409v1 by Jacopo D'Abramo, Andrea Zugarini, Paolo Torroni

Large language models present opportunities for innovative Question Answering
over Knowledge Graphs (KGQA). However, they are not inherently designed for
query generation. To bridge this gap, solutions have been proposed that rely on
fine-tuning or ad-hoc architectures, achieving good results but limited
out-of-domain distribution generalization. In this study, we introduce a novel
approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the
efficiency of in-context learning and semantic similarity and provides a
generally applicable solution for KGQA with state-of-the-art performance. We
run an extensive evaluation across multiple benchmark datasets and architecture
configurations.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁÇ∫Áü•Ë≠òÂúñË≠úÔºàKGQAÔºâÁöÑÂâµÊñ∞ÂïèÁ≠îÊèê‰æõ‰∫ÜÊ©üÊúÉ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰∏¶ÈùûÂ§©ÁîüÂ∞±Ë®≠Ë®àÁî®ÊñºÊü•Ë©¢ÁîüÊàê„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÂ∑≤ÊèêÂá∫‰æùË≥¥ÊñºÂæÆË™øÊàñÁâπÂÆöÊû∂ÊßãÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÁµêÊûúÔºå‰ΩÜÂüüÂ§ñÂàÜ‰ΩàÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ÂãïÊÖãÂ∞èÊ®£Êú¨Â≠∏ÁøíÔºàDFSLÔºâÁöÑÊñ∞ÊñπÊ≥ï„ÄÇDFSL ÈõÜÊàê‰∫ÜË™ûÂ¢ÉÂ≠∏ÁøíÂíåË™ûÁæ©Áõ∏‰ººÊÄßÁöÑÊïàÁéáÔºå‰∏¶ÁÇ∫ KGQA Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊôÆÈÅçÈÅ©Áî®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂÖ∑ÊúâÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÂ∞çÂ§öÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÂíåÊû∂ÊßãÈÖçÁΩÆÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑË©ï‰º∞„ÄÇ

##### **Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters**
2407.01406v2 by Daniil Gurgurov, Mareike Hartmann, Simon Ostermann

This paper explores the integration of graph knowledge from linguistic
ontologies into multilingual Large Language Models (LLMs) using adapters to
improve performance for low-resource languages (LRLs) in sentiment analysis
(SA) and named entity recognition (NER). Building upon successful
parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we
propose a similar approach for incorporating knowledge from multilingual
graphs, connecting concepts in various languages with each other through
linguistic relationships, into multilingual LLMs for LRLs. Specifically, we
focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,
Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters
fine-tuned on data extracted from the language-specific section of ConceptNet,
aiming to enable knowledge transfer across the languages covered by the
knowledge graph. We compare various fine-tuning objectives, including standard
Masked Language Modeling (MLM), MLM with full-word masking, and MLM with
targeted masking, to analyse their effectiveness in learning and integrating
the extracted graph data. Through empirical evaluation on language-specific
tasks, we assess how structured graph knowledge affects the performance of
multilingual LLMs for LRLs in SA and NER, providing insights into the potential
benefits of adapting language models for low-resource scenarios.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫Ü‰ΩøÁî®ÈÅ©ÈÖçÂô®Â∞á‰æÜËá™Ë™ûË®ÄÂ≠∏Êú¨È´îÁöÑÂúñÂΩ¢Áü•Ë≠òÊï¥ÂêàÂà∞Â§öË™ûË®ÄÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ôºå‰ª•ÊèêÂçá‰ΩéË≥áÊ∫êË™ûË®Ä (LRL) Âú®ÊÉÖÁ∑íÂàÜÊûê (SA) ÂíåÂëΩÂêçÂØ¶È´îË≠òÂà• (NER) ‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂª∫Á´ãÂú®ÊàêÂäüÁöÑÂèÉÊï∏ÊúâÊïàÂæÆË™øÊäÄË°ì‰∏äÔºå‰æãÂ¶Ç K-ADAPTER Âíå MAD-XÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈ°û‰ººÁöÑÂÅöÊ≥ïÔºåÂ∞á‰æÜËá™Â§öË™ûË®ÄÂúñÂΩ¢„ÄÅÈÄèÈÅéË™ûË®ÄÈóú‰øÇÂ∞áÂêÑÁ®ÆË™ûË®Ä‰∏≠ÁöÑÊ¶ÇÂøµÁõ∏‰∫íÈÄ£Êé•ÁöÑÁü•Ë≠òÔºåÁ¥çÂÖ• LRL ÁöÑÂ§öË™ûË®Ä LLM ‰∏≠„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂÖ´Á®Æ LRL‚Äî‚ÄîÈ¶¨Áàæ‰ªñË™û„ÄÅ‰øùÂä†Âà©‰∫ûË™û„ÄÅÂç∞Â∞ºË™û„ÄÅÂ∞ºÊ≥äÁàæË™û„ÄÅÁà™ÂìáË™û„ÄÅÁ∂≠ÂêæÁàæË™û„ÄÅËóèË™ûÂíåÂÉß‰ºΩÁæÖË™û‚Äî‚Äî‰∏¶‰ΩøÁî®Âú®Âæû ConceptNet ÁöÑË™ûË®ÄÁâπÂÆöÈÉ®ÂàÜ‰∏≠ÊèêÂèñÁöÑË≥áÊñô‰∏äÂæÆË™øÁöÑË™ûË®ÄÁâπÂÆöÈÅ©ÈÖçÂô®ÔºåÊó®Âú®ËÆìÁü•Ë≠òËΩâÁßªÂà∞Áü•Ë≠òÂúñÂΩ¢Ê∂µËìãÁöÑË™ûË®Ä‰∏≠„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂêÑÁ®ÆÂæÆË™øÁõÆÊ®ôÔºåÂåÖÊã¨Ê®ôÊ∫ñÁöÑÈÅÆÁΩ©Ë™ûË®ÄÊ®°Âûã (MLM)„ÄÅÂÖ∑ÊúâÂÖ®Ë©ûÈÅÆÁΩ©ÁöÑ MLMÔºå‰ª•ÂèäÂÖ∑ÊúâÁõÆÊ®ôÈÅÆÁΩ©ÁöÑ MLMÔºå‰ª•ÂàÜÊûêÂÆÉÂÄëÂú®Â≠∏ÁøíÂíåÊï¥ÂêàÊèêÂèñÁöÑÂúñÂΩ¢Ë≥áÊñô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÂ∞çË™ûË®ÄÁâπÂÆö‰ªªÂãôÁöÑÂØ¶Ë≠âË©ï‰º∞ÔºåÊàëÂÄëË©ï‰º∞ÁµêÊßãÂåñÂúñÂΩ¢Áü•Ë≠òÂ¶Ç‰ΩïÂΩ±ÈüøÂ§öË™ûË®Ä LLM Âú® LRL ‰∏≠ÁöÑ SA Âíå NER ÊïàËÉΩÔºå‰∏¶Ê∑±ÂÖ•‰∫ÜËß£ÁÇ∫‰ΩéË≥áÊ∫êÂ†¥ÊôØË™øÊï¥Ë™ûË®ÄÊ®°ÂûãÁöÑÊΩõÂú®Â•ΩËôï„ÄÇ

##### **SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model**
2407.01245v2 by Lingyue Fu, Hao Guan, Kounianhua Du, Jianghao Lin, Wei Xia, Weinan Zhang, Ruiming Tang, Yasheng Wang, Yong Yu

Knowledge Tracing (KT) aims to determine whether students will respond
correctly to the next question, which is a crucial task in intelligent tutoring
systems (ITS). In educational KT scenarios, transductive ID-based methods often
face severe data sparsity and cold start problems, where interactions between
individual students and questions are sparse, and new questions and concepts
consistently arrive in the database. In addition, existing KT models only
implicitly consider the correlation between concepts and questions, lacking
direct modeling of the more complex relationships in the heterogeneous graph of
concepts and questions. In this paper, we propose a Structure-aware Inductive
Knowledge Tracing model with large language model (dubbed SINKT), which, for
the first time, introduces large language models (LLMs) and realizes inductive
knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural
relationships between concepts and constructs a heterogeneous graph for
concepts and questions. Secondly, by encoding concepts and questions with LLMs,
SINKT incorporates semantic information to aid prediction. Finally, SINKT
predicts the student's response to the target question by interacting with the
student's knowledge state and the question representation. Experiments on four
real-world datasets demonstrate that SINKT achieves state-of-the-art
performance among 12 existing transductive KT models. Additionally, we explore
the performance of SINKT on the inductive KT task and provide insights into
various modules.

ÊëòË¶ÅÔºöÁü•Ë≠òËøΩËπ§ (KT) ÁöÑÁõÆÁöÑÊòØÁ¢∫ÂÆöÂ≠∏ÁîüÊòØÂê¶ËÉΩÊ≠£Á¢∫ÂõûÁ≠î‰∏ã‰∏ÄÂÄãÂïèÈ°åÔºåÈÄôÂú®Êô∫ÊÖßÂûãÊïôÂ≠∏Á≥ªÁµ± (ITS) ‰∏≠ÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÂú®ÊïôËÇ≤ KT Â†¥ÊôØ‰∏≠ÔºåÂü∫Êñº ID ÁöÑËΩâÂ∞éÊñπÊ≥ïÁ∂ìÂ∏∏Èù¢Ëá®Âö¥ÈáçÁöÑË≥áÊñôÁ®ÄÁñèÊÄßÂíåÂÜ∑ÂïüÂãïÂïèÈ°åÔºåÂÖ∂‰∏≠ÂÄãÂà•Â≠∏ÁîüÂíåÂïèÈ°å‰πãÈñìÁöÑ‰∫íÂãïÂæàÁ®ÄÁñèÔºåËÄå‰∏îÊñ∞ÁöÑÂïèÈ°åÂíåÊ¶ÇÂøµÊúÉÊåÅÁ∫åÂá∫ÁèæÂú®Ë≥áÊñôÂ∫´‰∏≠„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑ KT Ê®°ÂûãÂè™ÊúÉÈö±Âê´Âú∞ËÄÉÊÖÆÊ¶ÇÂøµÂíåÂïèÈ°å‰πãÈñìÁöÑÈóúËÅØÊÄßÔºåÁº∫‰πèÂ∞çÊ¶ÇÂøµÂíåÂïèÈ°åÁï∞Ë≥™Âúñ‰∏≠Êõ¥Ë§áÈõúÈóú‰øÇÁöÑÁõ¥Êé•Âª∫Ê®°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁµêÊßãÊÑüÁü•Ê≠∏Á¥çÁü•Ë≠òËøΩËπ§Ê®°ÂûãÔºàÁ®±ÁÇ∫ SINKTÔºâÔºåÂÆÉÈ¶ñÊ¨°ÂºïÂÖ•‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÔºå‰∏¶ÂØ¶Áèæ‰∫ÜÊ≠∏Á¥çÁü•Ë≠òËøΩËπ§„ÄÇÈ¶ñÂÖàÔºåSINKT Âà©Áî® LLM ÂºïÂÖ•Ê¶ÇÂøµ‰πãÈñìÁöÑÁµêÊßãÈóú‰øÇÔºå‰∏¶ÁÇ∫Ê¶ÇÂøµÂíåÂïèÈ°åÊßãÂª∫‰∫Ü‰∏ÄÂÄãÁï∞Ë≥™Âúñ„ÄÇÂÖ∂Ê¨°ÔºåÈÄèÈÅé‰ΩøÁî® LLM Á∑®Á¢ºÊ¶ÇÂøµÂíåÂïèÈ°åÔºåSINKT ÁµêÂêà‰∫ÜË™ûÁæ©Ë≥áË®äÔºå‰ª•ÂçîÂä©È†êÊ∏¨„ÄÇÊúÄÂæåÔºåSINKT ÈÄèÈÅéËàáÂ≠∏ÁîüÁöÑÁü•Ë≠òÁãÄÊÖãÂíåÂïèÈ°åË°®ÂæµÈÄ≤Ë°å‰∫íÂãïÔºåÈ†êÊ∏¨Â≠∏ÁîüÂ∞çÁõÆÊ®ôÂïèÈ°åÁöÑÂõûÊáâ„ÄÇÂú®ÂõõÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåSINKT Âú® 12 ÂÄãÁèæÊúâÁöÑËΩâÂ∞é KT Ê®°Âûã‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü SINKT Âú®Ê≠∏Á¥ç KT ‰ªªÂãô‰∏äÁöÑÊïàËÉΩÔºå‰∏¶Êèê‰æõ‰∫ÜÂ∞çÂêÑÁ®ÆÊ®°ÁµÑÁöÑË¶ãËß£„ÄÇ

