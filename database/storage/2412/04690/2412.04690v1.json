{"2412.04690": {"publish_time": "2024-12-06", "title": "LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs", "paper_summary": "Entity Alignment (EA) seeks to identify and match corresponding entities\nacross different Knowledge Graphs (KGs), playing a crucial role in knowledge\nfusion and integration. Embedding-based entity alignment (EA) has recently\ngained considerable attention, resulting in the emergence of many innovative\napproaches. Initially, these approaches concentrated on learning entity\nembeddings based on the structural features of knowledge graphs (KGs) as\ndefined by relation triples. Subsequent methods have integrated entities' names\nand attributes as supplementary information to improve the embeddings used for\nEA. However, existing methods lack a deep semantic understanding of entity\nattributes and relations. In this paper, we propose a Large Language Model\n(LLM) based Entity Alignment method, LLM-Align, which explores the\ninstruction-following and zero-shot capabilities of Large Language Models to\ninfer alignments of entities. LLM-Align uses heuristic methods to select\nimportant attributes and relations of entities, and then feeds the selected\ntriples of entities to an LLM to infer the alignment results. To guarantee the\nquality of alignment results, we design a multi-round voting mechanism to\nmitigate the hallucination and positional bias issues that occur with LLMs.\nExperiments on three EA datasets, demonstrating that our approach achieves\nstate-of-the-art performance compared to existing EA methods.", "paper_summary_zh": "\u5be6\u9ad4\u5c0d\u9f4a (EA) \u65e8\u5728\u8b58\u5225\u548c\u5339\u914d\u4e0d\u540c\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u5c0d\u61c9\u7684\u5be6\u9ad4\uff0c\u5728\u77e5\u8b58\u878d\u5408\u548c\u6574\u5408\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u57fa\u65bc\u5d4c\u5165\u7684\u5be6\u9ad4\u5c0d\u9f4a (EA) \u8fd1\u4f86\u5099\u53d7\u95dc\u6ce8\uff0c\u9032\u800c\u50ac\u751f\u51fa\u8a31\u591a\u5275\u65b0\u7684\u65b9\u6cd5\u3002\u6700\u521d\uff0c\u9019\u4e9b\u65b9\u6cd5\u5c08\u6ce8\u65bc\u6839\u64da\u77e5\u8b58\u5716\u8b5c (KG) \u7684\u7d50\u69cb\u7279\u5fb5\u4f86\u5b78\u7fd2\u5be6\u9ad4\u5d4c\u5165\uff0c\u9019\u4e9b\u7279\u5fb5\u7531\u95dc\u4fc2\u4e09\u5143\u7d44\u5b9a\u7fa9\u3002\u5f8c\u7e8c\u65b9\u6cd5\u5c07\u5be6\u9ad4\u540d\u7a31\u548c\u5c6c\u6027\u6574\u5408\u70ba\u88dc\u5145\u8cc7\u8a0a\uff0c\u4ee5\u6539\u5584\u7528\u65bc EA \u7684\u5d4c\u5165\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c0d\u5be6\u9ad4\u5c6c\u6027\u548c\u95dc\u4fc2\u7684\u6df1\u5165\u8a9e\u7fa9\u7406\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5be6\u9ad4\u5c0d\u9f4a\u65b9\u6cd5 LLM-Align\uff0c\u8a72\u65b9\u6cd5\u63a2\u7d22\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u9075\u5faa\u6307\u4ee4\u548c\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\uff0c\u4ee5\u63a8\u8ad6\u5be6\u9ad4\u5c0d\u9f4a\u3002LLM-Align \u4f7f\u7528\u555f\u767c\u5f0f\u65b9\u6cd5\u4f86\u9078\u64c7\u5be6\u9ad4\u7684\u91cd\u8981\u5c6c\u6027\u548c\u95dc\u4fc2\uff0c\u7136\u5f8c\u5c07\u5be6\u9ad4\u7684\u9078\u5b9a\u4e09\u5143\u7d44\u994b\u5165 LLM \u4ee5\u63a8\u8ad6\u5c0d\u9f4a\u7d50\u679c\u3002\u70ba\u4e86\u4fdd\u8b49\u5c0d\u9f4a\u7d50\u679c\u7684\u54c1\u8cea\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u591a\u8f2a\u6295\u7968\u6a5f\u5236\uff0c\u4ee5\u6e1b\u8f15 LLM \u4e2d\u51fa\u73fe\u7684\u5e7b\u89ba\u548c\u4f4d\u7f6e\u504f\u5dee\u554f\u984c\u3002\u5728\u4e09\u500b EA \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8207\u73fe\u6709\u7684 EA \u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002", "author": "Xuan Chen et.al.", "authors": "Xuan Chen, Tong Lu, Zhichun Wang", "id": "2412.04690v1", "paper_url": "http://arxiv.org/abs/2412.04690v1", "repo": "null"}}