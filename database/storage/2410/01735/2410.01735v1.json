{"2410.01735": {"publish_time": "2024-10-02", "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits", "paper_summary": "Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.", "paper_summary_zh": "\u734e\u52f5\u6a21\u578b (RM) \u5728\u5c07 LLM \u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u65b9\u9762\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u901a\u904e\u5728\u63a8\u7406\u6216\u8fed\u4ee3\u8a13\u7df4\u671f\u9593\u5c0d\u8f38\u51fa\u9032\u884c\u6392\u540d\u4f86\u589e\u5f37\u5176\u6027\u80fd\u3002\u7136\u800c\uff0cRM \u5c0d\u65b0\u4efb\u52d9\u7684\u6982\u62ec\u7a0b\u5ea6\u901a\u5e38\u7121\u6cd5\u4e8b\u5148\u5f97\u77e5\uff08\u4f8b\u5982\uff0c\u67d0\u4e9b RM \u53ef\u80fd\u64c5\u9577\u8a55\u5206\u5275\u610f\u5beb\u4f5c\uff0c\u800c\u53e6\u4e00\u4e9b\u5247\u64c5\u9577\u6578\u5b78\u63a8\u7406\uff09\u3002\u56e0\u6b64\uff0c\u5728\u8a13\u7df4 LLM \u6642\u50c5\u4f7f\u7528\u4e00\u500b\u56fa\u5b9a\u7684 RM \u53ef\u80fd\u662f\u6b21\u512a\u7684\u3002\u6b64\u5916\uff0c\u7531\u65bc\u4f86\u81ea\u4e0d\u540c RM \u7684\u4fe1\u865f\u76f8\u4e92\u885d\u7a81\uff0c\u540c\u6642\u4f7f\u7528\u591a\u500b RM \u4f86\u512a\u5316 LLM \u5728\u8a08\u7b97\u4e0a\u53ef\u80fd\u662f\u975e\u5e38\u8017\u6642\u7684\uff0c\u4e26\u4e14\u5177\u6709\u6311\u6230\u6027\uff0c\u9019\u53ef\u80fd\u6703\u964d\u4f4e\u6027\u80fd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 LASeR\uff08\u81ea\u9069\u61c9\u9078\u64c7\u734e\u52f5\u5b78\u7fd2\uff09\uff0c\u5b83\u4f7f\u7528\u591a\u500b RM \u8fed\u4ee3\u8a13\u7df4 LLM\uff0c\u70ba\u6bcf\u500b\u5be6\u4f8b\u9078\u64c7\u548c\u5229\u7528\u6700\u5408\u9069\u7684 RM \u4f86\u5c0d\u8f38\u51fa\u9032\u884c\u6392\u540d\u4e26\u751f\u6210\u504f\u597d\u6578\u64da\uff0c\u9019\u88ab\u8996\u70ba\u4e00\u500b\u591a\u81c2\u8001\u864e\u6a5f\u554f\u984c\u3002\u6211\u5011\u5728\u5e38\u8b58\u548c\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u4e0a\u7684\u7d50\u679c\u8868\u660e\uff0cLASeR \u53ef\u4ee5\u901a\u904e\u91dd\u5c0d\u591a\u500b RM \u9032\u884c\u512a\u5316\u4f86\u63d0\u5347\u8fed\u4ee3 LLM \u512a\u5316\uff0c\u901a\u904e\u4f7f\u7528\u6574\u9ad4 RM \u5206\u6578\u9032\u884c\u8a13\u7df4\uff0c\u5c07 Llama-3-8B \u5728\u4e09\u500b\u6578\u64da\u96c6\u4e0a\u7684\u7d55\u5c0d\u5e73\u5747\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 2.67%\uff0c\u540c\u6642\u9084\u5c55\u793a\u4e86\u51fa\u8272\u7684\u8a13\u7df4\u6548\u7387\uff08\u4f8b\u5982\uff0c\u52a0\u901f 2 \u500d\uff09\u3002\u6b64\u5916\uff0c\u5728 WildChat\uff08\u4e00\u500b\u57fa\u65bc\u6307\u4ee4\u7684\u63d0\u793a\u57fa\u6e96\uff09\u4e0a\uff0c\u6211\u5011\u767c\u73fe\u4f7f\u7528 Llama-3-8B LASeR \u53ef\u4ee5\u7372\u5f97 71.45% \u7684 AlpacaEval \u7372\u52dd\u7387\uff0c\u800c\u5f8c\u8005\u5247\u901a\u904e\u5c0d\u591a\u500b RM \u9032\u884c\u9806\u5e8f\u512a\u5316\u4f86\u5be6\u73fe\u3002\u64f4\u5c55\u5230\u9577\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52d9\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5728 Llama-3-8B \u4e0a\uff0c\u8207\u4f7f\u7528\u6700\u4f73 n \u63a1\u6a23\u6642\u96a8\u6a5f RM \u9078\u64c7\u76f8\u6bd4\uff0cLASeR \u5728\u55ae\u6587\u6a94\u548c\u591a\u6587\u6a94 QA \u4e0a\u5206\u5225\u5be6\u73fe\u4e86 2.64 F1 \u548c 2.42 F1 \u7684\u5e73\u5747\u6539\u9032\u3002LASeR \u5c0d\u6709\u566a\u8072\u7684\u734e\u52f5\u5177\u6709\u9b6f\u68d2\u6027\uff0c\u4e26\u4e14\u53ef\u4ee5\u6982\u62ec\u5230\u591a\u7a2e\u8a2d\u7f6e\u3002\u6700\u5f8c\uff0cLASeR \u7684 RM \u9078\u64c7\u6703\u6839\u64da\u5e95\u5c64\u4efb\u52d9\u6216\u5be6\u4f8b\u800c\u6539\u8b8a\uff0c\u6211\u5011\u9a57\u8b49\u4e86\u4f86\u81ea\u591a\u500b RM \u7684\u885d\u7a81\u504f\u597d\u7684\u5b58\u5728\uff0c\u800c\u9019\u4e9b\u504f\u597d\u53ef\u4ee5\u4f7f\u7528 LASeR \u4f86\u7de9\u89e3\u3002", "author": "Duy Nguyen et.al.", "authors": "Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal", "id": "2410.01735v1", "paper_url": "http://arxiv.org/abs/2410.01735v1", "repo": "https://github.com/duykhuongnguyen/laser-mab"}}