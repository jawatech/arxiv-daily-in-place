{"2406.02532": {"publish_time": "2024-06-04", "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices", "paper_summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5ee3\u6cdb\u63a1\u7528\uff0c\u6709\u6548\u7387\u5730\u57f7\u884c\u5b83\u5011\u8b8a\u5f97\u81f3\u95dc\u91cd\u8981\u3002\u6700\u8fd1\u95dc\u65bc LLM \u63a8\u8ad6\u7684\u7814\u7a76\u4f7f\u7528\u63a8\u6e2c\u6027\u89e3\u78bc\u4f86\u5be6\u73fe\u6975\u9ad8\u7684\u52a0\u901f\u3002\u7136\u800c\uff0c\u9019\u4e9b\u7814\u7a76\u5927\u591a\u6578\u96b1\u542b\u5730\u70ba\u9ad8\u7aef\u8cc7\u6599\u4e2d\u5fc3\u786c\u9ad4\u8a2d\u8a08\u6f14\u7b97\u6cd5\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u76f8\u53cd\u7684\u554f\u984c\uff1a\u6211\u5011\u53ef\u4ee5\u5728\u6d88\u8cbb\u578b\u6a5f\u5668\u4e0a\u57f7\u884c LLM \u7684\u901f\u5ea6\u6709\u591a\u5feb\uff1f\u6d88\u8cbb\u8005 GPU \u4e0d\u518d\u80fd\u5bb9\u7d0d\u6700\u5927\u7684\u53ef\u7528\u6a21\u578b\uff0850B+ \u53c3\u6578\uff09\uff0c\u4e26\u4e14\u5fc5\u9808\u5c07\u5b83\u5011\u5378\u8f09\u5230 RAM \u6216 SSD\u3002\u5728\u4f7f\u7528\u5378\u8f09\u53c3\u6578\u57f7\u884c\u6642\uff0c\u63a8\u8ad6\u5f15\u64ce\u53ef\u4ee5\u540c\u6642\u8655\u7406\u6578\u767e\u6216\u6578\u5343\u500b\u8a18\u865f\u7684\u6279\u6b21\uff0c\u5c31\u50cf\u53ea\u8655\u7406\u4e00\u500b\u8a18\u865f\u4e00\u6a23\uff0c\u4f7f\u5176\u81ea\u7136\u5730\u9069\u5408\u65bc\u63a8\u6e2c\u6027\u89e3\u78bc\u3002\u6211\u5011\u63d0\u51fa SpecExec\uff08\u63a8\u6e2c\u6027\u57f7\u884c\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u7684\u4e26\u884c\u89e3\u78bc\u65b9\u6cd5\uff0c\u53ef\u4ee5\u70ba\u71b1\u9580 LLM \u5bb6\u65cf\u7684\u6bcf\u500b\u76ee\u6a19\u6a21\u578b\u53cd\u8986\u904b\u7b97\u7522\u751f\u591a\u9054 20 \u500b\u8a18\u865f\u3002\u5b83\u5229\u7528\u73fe\u4ee3 LLM \u4e2d\u8a18\u865f\u6a5f\u7387\u5206\u4f48\u7684\u9ad8\u5c16\u5cf0\u6027\u4ee5\u53ca\u6a21\u578b\u8f38\u51fa\u6a5f\u7387\u4e4b\u9593\u7684\u9ad8\u5ea6\u5c0d\u9f4a\u3002SpecExec \u5f9e\u8349\u7a3f\u6a21\u578b\u4e2d\u53d6\u5f97\u6700\u53ef\u80fd\u7684\u8a18\u865f\u5ef6\u7e8c\uff0c\u70ba\u76ee\u6a19\u6a21\u578b\u5efa\u7acb\u4e00\u500b\u300c\u5feb\u53d6\u300d\u6a39\uff0c\u7136\u5f8c\u5728\u55ae\u6b21\u50b3\u905e\u4e2d\u9032\u884c\u9a57\u8b49\u3002\u4f7f\u7528 SpecExec\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5728\u4f7f\u7528 4 \u4f4d\u5143\u91cf\u5316\u6642\uff0c\u5728\u6d88\u8cbb\u8005 GPU \u4e0a\u4ee5\u6bcf\u79d2 4-6 \u500b\u8a18\u865f\u7684\u901f\u5ea6\u63a8\u8ad6 50B+ \u53c3\u6578 LLM\uff0c\u6216\u5728\u4f7f\u7528 16 \u4f4d\u5143\u6b0a\u91cd\u6642\u4ee5\u6bcf\u79d2 2-3 \u500b\u8a18\u865f\u7684\u901f\u5ea6\u63a8\u8ad6\u3002", "author": "Ruslan Svirschevski et.al.", "authors": "Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin", "id": "2406.02532v1", "paper_url": "http://arxiv.org/abs/2406.02532v1", "repo": "null"}}