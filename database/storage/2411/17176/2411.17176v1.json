{"2411.17176": {"publish_time": "2024-11-26", "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting", "paper_summary": "Despite the significant advancements in text-to-image (T2I) generative\nmodels, users often face a trial-and-error challenge in practical scenarios.\nThis challenge arises from the complexity and uncertainty of tedious steps such\nas crafting suitable prompts, selecting appropriate models, and configuring\nspecific arguments, making users resort to labor-intensive attempts for desired\nimages. This paper proposes Automatic T2I generation, which aims to automate\nthese tedious steps, allowing users to simply describe their needs in a\nfreestyle chatting way. To systematically study this problem, we first\nintroduce ChatGenBench, a novel benchmark designed for Automatic T2I. It\nfeatures high-quality paired data with diverse freestyle inputs, enabling\ncomprehensive evaluation of automatic T2I models across all steps.\nAdditionally, recognizing Automatic T2I as a complex multi-step reasoning task,\nwe propose ChatGen-Evo, a multi-stage evolution strategy that progressively\nequips models with essential automation skills. Through extensive evaluation\nacross step-wise accuracy and image quality, ChatGen-Evo significantly enhances\nperformance over various baselines. Our evaluation also uncovers valuable\ninsights for advancing automatic T2I. All our data, code, and models will be\navailable in \\url{https://chengyou-jia.github.io/ChatGen-Home}", "paper_summary_zh": "\u5118\u7ba1\u6587\u5b57\u8f49\u5f71\u50cf (T2I) \u751f\u6210\u6a21\u578b\u6709\u986f\u8457\u9032\u5c55\uff0c\u4f46\u5728\u5be6\u969b\u60c5\u6cc1\u4e2d\uff0c\u4f7f\u7528\u8005\u7d93\u5e38\u9762\u81e8\u8a66\u932f\u7684\u6311\u6230\u3002\u9019\u500b\u6311\u6230\u6e90\u65bc\u7e41\u7463\u6b65\u9a5f\u7684\u8907\u96dc\u6027\u548c\u4e0d\u78ba\u5b9a\u6027\uff0c\u4f8b\u5982\u88fd\u4f5c\u5408\u9069\u7684\u63d0\u793a\u3001\u9078\u64c7\u9069\u7576\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u8a2d\u5b9a\u7279\u5b9a\u53c3\u6578\uff0c\u4f7f\u5f97\u4f7f\u7528\u8005\u5fc5\u9808\u8017\u8cbb\u5927\u91cf\u5fc3\u529b\u624d\u80fd\u5f97\u5230\u60f3\u8981\u7684\u5f71\u50cf\u3002\u672c\u6587\u63d0\u51fa\u81ea\u52d5 T2I \u751f\u6210\uff0c\u76ee\u7684\u662f\u81ea\u52d5\u5316\u9019\u4e9b\u7e41\u7463\u7684\u6b65\u9a5f\uff0c\u8b93\u4f7f\u7528\u8005\u80fd\u7528\u81ea\u7531\u804a\u5929\u7684\u65b9\u5f0f\u7c21\u55ae\u63cf\u8ff0\u4ed6\u5011\u7684\u9700\u6c42\u3002\u70ba\u4e86\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u4ecb\u7d39 ChatGenBench\uff0c\u4e00\u500b\u5c08\u70ba\u81ea\u52d5 T2I \u8a2d\u8a08\u7684\u65b0\u57fa\u6e96\u6e2c\u8a66\u3002\u5b83\u5177\u6709\u9ad8\u54c1\u8cea\u7684\u914d\u5c0d\u8cc7\u6599\uff0c\u4e26\u6709\u5404\u7a2e\u81ea\u7531\u8f38\u5165\uff0c\u80fd\u5168\u9762\u8a55\u4f30\u6240\u6709\u6b65\u9a5f\u4e2d\u7684\u81ea\u52d5 T2I \u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u81ea\u52d5 T2I \u8996\u70ba\u4e00\u500b\u8907\u96dc\u7684\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\uff0c\u4e26\u63d0\u51fa ChatGen-Evo\uff0c\u4e00\u7a2e\u591a\u968e\u6bb5\u7684\u9032\u5316\u7b56\u7565\uff0c\u9010\u6f38\u8b93\u6a21\u578b\u5177\u5099\u5fc5\u8981\u7684\u81ea\u52d5\u5316\u6280\u80fd\u3002\u900f\u904e\u9010\u6b65\u7cbe\u78ba\u5ea6\u548c\u5f71\u50cf\u54c1\u8cea\u7684\u5ee3\u6cdb\u8a55\u4f30\uff0cChatGen-Evo \u5728\u5404\u7a2e\u57fa\u7dda\u4e0a\u986f\u8457\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u8a55\u4f30\u4e5f\u63ed\u793a\u4e86\u63a8\u9032\u81ea\u52d5 T2I \u7684\u5bf6\u8cb4\u898b\u89e3\u3002\u6211\u5011\u6240\u6709\u7684\u8cc7\u6599\u3001\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u90fd\u53ef\u4ee5\u5728 \\url{https://chengyou-jia.github.io/ChatGen-Home} \u53d6\u5f97", "author": "Chengyou Jia et.al.", "authors": "Chengyou Jia, Changliang Xia, Zhuohang Dang, Weijia Wu, Hangwei Qian, Minnan Luo", "id": "2411.17176v1", "paper_url": "http://arxiv.org/abs/2411.17176v1", "repo": "null"}}