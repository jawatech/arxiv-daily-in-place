
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-27**|**InfAlign: Inference-aware language model alignment**|Ananth Balashankar et.al.|[2412.19792v1](http://arxiv.org/abs/2412.19792v1)|null|
|**2024-12-27**|**Enhancing Whisper's Accuracy and Speed for Indian Languages through Prompt-Tuning and Tokenization**|Kumud Tripathi et.al.|[2412.19785v1](http://arxiv.org/abs/2412.19785v1)|null|
|**2024-12-27**|**Can AI Help with Your Personal Finances?**|Oudom Hean et.al.|[2412.19784v1](http://arxiv.org/abs/2412.19784v1)|null|
|**2024-12-27**|**Machine Learning for Sentiment Analysis of Imported Food in Trinidad and Tobago**|Cassandra Daniels et.al.|[2412.19781v1](http://arxiv.org/abs/2412.19781v1)|null|
|**2024-12-27**|**Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure State**|Zhifu Chen et.al.|[2412.19759v1](http://arxiv.org/abs/2412.19759v1)|null|
|**2024-12-27**|**"Did my figure do justice to the answer?" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)**|Pritam Sil et.al.|[2412.19755v1](http://arxiv.org/abs/2412.19755v1)|null|
|**2024-12-27**|**Can Large Language Models Adapt to Other Agents In-Context?**|Matthew Riemer et.al.|[2412.19726v1](http://arxiv.org/abs/2412.19726v1)|null|
|**2024-12-27**|**OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis**|Qiushi Sun et.al.|[2412.19723v1](http://arxiv.org/abs/2412.19723v1)|null|
|**2024-12-27**|**Text2Insight: Transform natural language text into insights seamlessly using multi-model architecture**|Pradeep Sain et.al.|[2412.19718v1](http://arxiv.org/abs/2412.19718v1)|null|
|**2024-12-27**|**Toward Adaptive Reasoning in Large Language Models with Thought Rollback**|Sijia Chen et.al.|[2412.19707v1](http://arxiv.org/abs/2412.19707v1)|[link](https://github.com/iQua/llmpebase)|
|**2024-12-27**|**An Integrated Optimization and Deep Learning Pipeline for Predicting Live Birth Success in IVF Using Feature Optimization and Transformer-Based Models**|Arezoo Borji et.al.|[2412.19696v1](http://arxiv.org/abs/2412.19696v1)|null|
|**2024-12-27**|**A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**|Jingchun Lian et.al.|[2412.19685v1](http://arxiv.org/abs/2412.19685v1)|null|
|**2024-12-27**|**Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**|Jiang Liu et.al.|[2412.19684v1](http://arxiv.org/abs/2412.19684v1)|null|
|**2024-12-27**|**CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**|Siyu Wang et.al.|[2412.19663v1](http://arxiv.org/abs/2412.19663v1)|null|
|**2024-12-27**|**Chimera: A Block-Based Neural Architecture Search Framework for Event-Based Object Detection**|Diego A. Silva et.al.|[2412.19646v1](http://arxiv.org/abs/2412.19646v1)|null|
|**2024-12-27**|**Xmodel-2 Technical Report**|Wang Qun et.al.|[2412.19638v1](http://arxiv.org/abs/2412.19638v1)|null|
|**2024-12-27**|**Gradient Weight-normalized Low-rank Projection for Efficient LLM Training**|Jia-Hong Huang et.al.|[2412.19616v1](http://arxiv.org/abs/2412.19616v1)|null|
|**2024-12-27**|**Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance**|Sanjukta Ghosh et.al.|[2412.19610v1](http://arxiv.org/abs/2412.19610v1)|null|
|**2024-12-27**|**SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms**|Shashank Rao Marpally et.al.|[2412.19595v1](http://arxiv.org/abs/2412.19595v1)|null|
|**2024-12-27**|**A Comparative Study of Machine Unlearning Techniques for Image and Text Classification Models**|Omar M. Safa et.al.|[2412.19583v1](http://arxiv.org/abs/2412.19583v1)|null|
|**2024-12-27**|**Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following**|Yuxiao Yang et.al.|[2412.19562v1](http://arxiv.org/abs/2412.19562v1)|null|
|**2024-12-27**|**Learning states enhanced knowledge tracing: Simulating the diversity in real-world learning process**|Shanshan Wang et.al.|[2412.19550v1](http://arxiv.org/abs/2412.19550v1)|null|
|**2024-12-27**|**TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data**|Xiang Huang et.al.|[2412.19544v1](http://arxiv.org/abs/2412.19544v1)|[link](https://github.com/cdhx/targa)|
|**2024-12-27**|**P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision**|Junjie Hu et.al.|[2412.19533v1](http://arxiv.org/abs/2412.19533v1)|null|
|**2024-12-27**|**Is Your Text-to-Image Model Robust to Caption Noise?**|Weichen Yu et.al.|[2412.19531v1](http://arxiv.org/abs/2412.19531v1)|null|
|**2024-12-27**|**Attribution for Enhanced Explanation with Transferable Adversarial eXploration**|Zhiyu Zhu et.al.|[2412.19523v1](http://arxiv.org/abs/2412.19523v1)|null|
|**2024-12-27**|**Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation**|Surangika Ranathungaa et.al.|[2412.19522v1](http://arxiv.org/abs/2412.19522v1)|null|
|**2024-12-27**|**Estimation of System Parameters Including Repeated Cross-Sectional Data through Emulator-Informed Deep Generative Model**|Hyunwoo Cho et.al.|[2412.19517v1](http://arxiv.org/abs/2412.19517v1)|null|
|**2024-12-27**|**Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs**|Zhe Yang et.al.|[2412.19513v1](http://arxiv.org/abs/2412.19513v1)|null|
|**2024-12-27**|**Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging**|Hua Farn et.al.|[2412.19512v1](http://arxiv.org/abs/2412.19512v1)|null|
|**2024-12-27**|**MBQ: Modality-Balanced Quantization for Large Vision-Language Models**|Shiyao Li et.al.|[2412.19509v1](http://arxiv.org/abs/2412.19509v1)|[link](https://github.com/thu-nics/mbq)|
|**2024-12-27**|**Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models**|Jie Zhang et.al.|[2412.19496v1](http://arxiv.org/abs/2412.19496v1)|null|
|**2024-12-27**|**Disparate Model Performance and Stability in Machine Learning Clinical Support for Diabetes and Heart Diseases**|Ioannis Bilionis et.al.|[2412.19495v1](http://arxiv.org/abs/2412.19495v1)|null|
|**2024-12-27**|**Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering**|Shiwen Ni et.al.|[2412.19482v1](http://arxiv.org/abs/2412.19482v1)|null|
|**2024-12-27**|**Optimizing Helmet Detection with Hybrid YOLO Pipelines: A Detailed Analysis**|Vaikunth M et.al.|[2412.19467v1](http://arxiv.org/abs/2412.19467v1)|null|
|**2024-12-27**|**Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models**|Hyeonseok Moon et.al.|[2412.19450v1](http://arxiv.org/abs/2412.19450v1)|null|
|**2024-12-27**|**Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models**|Shuo Wang et.al.|[2412.19449v1](http://arxiv.org/abs/2412.19449v1)|null|
|**2024-12-27**|**A Survey on Large Language Model Acceleration based on KV Cache Management**|Haoyang Li et.al.|[2412.19442v1](http://arxiv.org/abs/2412.19442v1)|null|
|**2024-12-27**|**DeepSeek-V3 Technical Report**|DeepSeek-AI et.al.|[2412.19437v1](http://arxiv.org/abs/2412.19437v1)|[link](https://github.com/deepseek-ai/deepseek-v3)|
|**2024-12-27**|**Residual Feature-Reutilization Inception Network for Image Classification**|Yuanpeng He et.al.|[2412.19433v1](http://arxiv.org/abs/2412.19433v1)|null|
|**2024-12-27**|**Revisiting PCA for time series reduction in temporal dimension**|Jiaxin Gao et.al.|[2412.19423v1](http://arxiv.org/abs/2412.19423v1)|null|
|**2024-12-27**|**Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression Profiles via Deep Learning**|Chen Li et.al.|[2412.19422v1](http://arxiv.org/abs/2412.19422v1)|[link](https://github.com/naruto7283/gx2mol)|
|**2024-12-27**|**Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers**|James H. Tanis et.al.|[2412.19419v1](http://arxiv.org/abs/2412.19419v1)|null|
|**2024-12-27**|**Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization**|Yuanpeng He et.al.|[2412.19418v1](http://arxiv.org/abs/2412.19418v1)|[link](https://github.com/heyuanpengpku/guef)|
|**2024-12-27**|**Fully Data-driven but Interpretable Human Behavioural Modelling with Differentiable Discrete Choice Model**|Fumiyasu Makinoshima et.al.|[2412.19403v1](http://arxiv.org/abs/2412.19403v1)|null|
|**2024-12-27**|**Comparing Few to Rank Many: Active Human Preference Learning using Randomized Frank-Wolfe**|Kiran Koshy Thekumparampil et.al.|[2412.19396v1](http://arxiv.org/abs/2412.19396v1)|null|
|**2024-12-27**|**An Engorgio Prompt Makes Large Language Model Babble on**|Jianshuo Dong et.al.|[2412.19394v1](http://arxiv.org/abs/2412.19394v1)|null|
|**2024-12-27**|**An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for Digit Classification**|Eugene Choi et.al.|[2412.19391v1](http://arxiv.org/abs/2412.19391v1)|[link](https://github.com/eugenechoi2004/cos429_final)|
|**2024-12-26**|**Large Language Models for Market Research: A Data-augmentation Approach**|Mengxin Wang et.al.|[2412.19363v1](http://arxiv.org/abs/2412.19363v1)|null|
|**2024-12-26**|**Dynamic Skill Adaptation for Large Language Models**|Jiaao Chen et.al.|[2412.19361v1](http://arxiv.org/abs/2412.19361v1)|null|
|**2024-12-26**|**ETTA: Elucidating the Design Space of Text-to-Audio Models**|Sang-gil Lee et.al.|[2412.19351v1](http://arxiv.org/abs/2412.19351v1)|null|
|**2024-12-26**|**On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages**|Aleksandar Terzić et.al.|[2412.19350v1](http://arxiv.org/abs/2412.19350v1)|[link](https://github.com/ibm/selective-dense-state-space-model)|
|**2024-12-26**|**Semi-Supervised Learning from Small Annotated Data and Large Unlabeled Data for Fine-grained PICO Entity Recognition**|Fangyi Chen et.al.|[2412.19346v1](http://arxiv.org/abs/2412.19346v1)|null|
|**2024-12-26**|**CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models**|Kiet A. Nguyen et.al.|[2412.19331v1](http://arxiv.org/abs/2412.19331v1)|null|
|**2024-12-26**|**Performance Control in Early Exiting to Deploy Large Models at the Same Cost of Smaller Ones**|Mehrnaz Mofakhami et.al.|[2412.19325v1](http://arxiv.org/abs/2412.19325v1)|null|
|**2024-12-26**|**From Interets to Insights: An LLM Approach to Course Recommendations Using Natural Language Queries**|Hugh Van Deventer et.al.|[2412.19312v1](http://arxiv.org/abs/2412.19312v1)|null|
|**2024-12-26**|**RAG with Differential Privacy**|Nicolas Grislain et.al.|[2412.19291v1](http://arxiv.org/abs/2412.19291v1)|null|
|**2024-12-26**|**ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning**|Taewhan Kim et.al.|[2412.19289v1](http://arxiv.org/abs/2412.19289v1)|[link](https://github.com/taewhankim/vipcap)|
|**2024-12-26**|**Time Series Foundational Models: Their Role in Anomaly Detection and Prediction**|Chathurangi Shyalika et.al.|[2412.19286v1](http://arxiv.org/abs/2412.19286v1)|[link](https://github.com/smtmnfg/tsfm)|
|**2024-12-26**|**PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing**|Michael Bezick et.al.|[2412.19284v1](http://arxiv.org/abs/2412.19284v1)|null|
|**2024-12-26**|**Optimizing Multi-Stage Language Models for Effective Text Retrieval**|Quang Hoang Trung et.al.|[2412.19265v1](http://arxiv.org/abs/2412.19265v1)|null|
|**2024-12-26**|**MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes**|Asma Ben Abacha et.al.|[2412.19260v1](http://arxiv.org/abs/2412.19260v1)|[link](https://github.com/abachaa/medec)|
|**2024-12-26**|**Multi-matrix Factorization Attention**|Jingcheng Hu et.al.|[2412.19255v1](http://arxiv.org/abs/2412.19255v1)|null|
|**2024-12-26**|**Leveraging Self-Training and Variational Autoencoder for Agitation Detection in People with Dementia Using Wearable Sensors**|Abeer Badawi et.al.|[2412.19254v1](http://arxiv.org/abs/2412.19254v1)|null|
|**2024-12-26**|**Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for Binary Classifiers**|Jason M. Pittman et.al.|[2412.19241v1](http://arxiv.org/abs/2412.19241v1)|null|
|**2024-12-26**|**Learning Cross-Domain Representations for Transferable Drug Perturbations on Single-Cell Transcriptional Responses**|Hui Liu et.al.|[2412.19228v1](http://arxiv.org/abs/2412.19228v1)|[link](https://github.com/hliulab/xtransfercdr)|
|**2024-12-26**|**Optimizing Fantasy Sports Team Selection with Deep Reinforcement Learning**|Shamik Bhattacharjee et.al.|[2412.19215v1](http://arxiv.org/abs/2412.19215v1)|null|
|**2024-12-26**|**Multi-Attribute Constraint Satisfaction via Language Model Rewriting**|Ashutosh Baheti et.al.|[2412.19198v1](http://arxiv.org/abs/2412.19198v1)|null|
|**2024-12-26**|**Provably Efficient Exploration in Reward Machines with Low Regret**|Hippolyte Bourel et.al.|[2412.19194v1](http://arxiv.org/abs/2412.19194v1)|null|
|**2024-12-26**|**Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models**|Haonan He et.al.|[2412.19191v1](http://arxiv.org/abs/2412.19191v1)|null|
|**2024-12-26**|**Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching**|Wenjing Chen et.al.|[2412.19184v1](http://arxiv.org/abs/2412.19184v1)|null|
|**2024-12-26**|**Mask Approximation Net: Merging Feature Extraction and Distribution Learning for Remote Sensing Change Captioning**|Dongwei Sun et.al.|[2412.19179v1](http://arxiv.org/abs/2412.19179v1)|null|
|**2024-12-26**|**Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval**|Yang Du et.al.|[2412.19178v1](http://arxiv.org/abs/2412.19178v1)|[link](https://github.com/qyr0403/reversed-in-time)|
|**2024-12-26**|**GFG -- Gender-Fair Generation: A CALAMITA Challenge**|Simona Frenda et.al.|[2412.19168v1](http://arxiv.org/abs/2412.19168v1)|null|
|**2024-12-26**|**Referencing Where to Focus: Improving VisualGrounding with Referential Query**|Yabing Wang et.al.|[2412.19155v1](http://arxiv.org/abs/2412.19155v1)|null|
|**2024-12-26**|**AskChart: Universal Chart Understanding through Textual Enhancement**|Xudong Yang et.al.|[2412.19146v1](http://arxiv.org/abs/2412.19146v1)|[link](https://github.com/sootung/askchart)|
|**2024-12-26**|**SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis**|Senbin Zhu et.al.|[2412.19140v1](http://arxiv.org/abs/2412.19140v1)|null|
|**2024-12-26**|**PlanLLM: Video Procedure Planning with Refinable Large Language Models**|Dejie Yang et.al.|[2412.19139v1](http://arxiv.org/abs/2412.19139v1)|[link](https://github.com/idejie/planllm)|
|**2024-12-26**|**Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact**|Valay Bundele et.al.|[2412.19124v1](http://arxiv.org/abs/2412.19124v1)|null|
|**2024-12-26**|**Discrete vs. Continuous Trade-offs for Generative Models**|Jathin Korrapati et.al.|[2412.19114v1](http://arxiv.org/abs/2412.19114v1)|null|
|**2024-12-26**|**SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values**|Yunfan Zhang et.al.|[2412.19113v1](http://arxiv.org/abs/2412.19113v1)|null|
|**2024-12-26**|**Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection**|Xiaoyu Huang et.al.|[2412.19108v1](http://arxiv.org/abs/2412.19108v1)|null|
|**2024-12-26**|**"I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities**|Jiawei Yu et.al.|[2412.19102v1](http://arxiv.org/abs/2412.19102v1)|null|
|**2024-12-26**|**TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network for Mobility Prediction**|Zhaoping Hu et.al.|[2412.19092v1](http://arxiv.org/abs/2412.19092v1)|null|
|**2024-12-26**|**MoPD: Mixture-of-Prompts Distillation for Vision-Language Models**|Yang Chen et.al.|[2412.19087v1](http://arxiv.org/abs/2412.19087v1)|null|
|**2024-12-26**|**Robust Speech and Natural Language Processing Models for Depression Screening**|Y. Lu et.al.|[2412.19072v1](http://arxiv.org/abs/2412.19072v1)|null|
|**2024-12-26**|**Cross-Demographic Portability of Deep NLP-Based Depression Models**|Tomek Rutowski et.al.|[2412.19070v1](http://arxiv.org/abs/2412.19070v1)|null|
|**2024-12-26**|**Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding**|Shenghong He et.al.|[2412.19064v1](http://arxiv.org/abs/2412.19064v1)|null|
|**2024-12-26**|**Indonesian-English Code-Switching Speech Synthesizer Utilizing Multilingual STEN-TTS and Bert LID**|Ahmad Alfani Handoyo et.al.|[2412.19043v1](http://arxiv.org/abs/2412.19043v1)|null|
|**2024-12-26**|**CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers**|Jingyi Zheng et.al.|[2412.19037v1](http://arxiv.org/abs/2412.19037v1)|null|
|**2024-12-26**|**Repository Structure-Aware Training Makes SLMs Better Issue Resolver**|Zexiong Ma et.al.|[2412.19031v1](http://arxiv.org/abs/2412.19031v1)|null|
|**2024-12-26**|**Modality-Projection Universal Model for Comprehensive Full-Body Medical Imaging Segmentation**|Yixin Chen et.al.|[2412.19026v1](http://arxiv.org/abs/2412.19026v1)|[link](https://github.com/yixinchen-ai/mpum)|
|**2024-12-26**|**Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph Generation**|Tao Liu et.al.|[2412.19021v1](http://arxiv.org/abs/2412.19021v1)|null|
|**2024-12-26**|**Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability**|Ruixi Lin et.al.|[2412.19018v1](http://arxiv.org/abs/2412.19018v1)|null|
|**2024-12-26**|**Brain Ageing Prediction using Isolation Forest Technique and Residual Neural Network (ResNet)**|Saadat Behzadi et.al.|[2412.19017v1](http://arxiv.org/abs/2412.19017v1)|null|
|**2024-12-26**|**Enhancing Audiovisual Speech Recognition through Bifocal Preference Optimization**|Yihan Wu et.al.|[2412.19005v1](http://arxiv.org/abs/2412.19005v1)|null|
|**2024-12-25**|**Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with AI for Enhanced Urban Mapping**|Sajjad Afroosheh et.al.|[2412.18994v1](http://arxiv.org/abs/2412.18994v1)|null|
|**2024-12-25**|**How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study**|Alejandro Velasco et.al.|[2412.18989v1](http://arxiv.org/abs/2412.18989v1)|null|
|**2024-12-25**|**TravelAgent: Generative Agents in the Built Environment**|Ariel Noyman et.al.|[2412.18985v1](http://arxiv.org/abs/2412.18985v1)|null|
|**2024-12-25**|**Injecting Bias into Text Classification Models using Backdoor Attacks**|A. Dilara Yavuz et.al.|[2412.18975v1](http://arxiv.org/abs/2412.18975v1)|null|

#### Abstracts
##### **InfAlign: Inference-aware language model alignment**
2412.19792v1 by Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami

Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.

摘要：語言模型比對已成為訓練現代生成式語言模型中的關鍵步驟。比對的目標是微調一個參考模型，使得比對模型中樣本在參考模型中樣本上的獲勝率很高，但同時要符合 KL 散度約束。如今，我們越來越常使用推論時間演算法（例如最佳 N 選一、受控解碼、樹狀搜尋）從語言模型中解碼，而不是使用標準抽樣。然而，比對目標並未涵蓋此類推論時間解碼程序。我們表明，現有的比對架構對於此類推論時間方法而言並非最佳。然後，我們修改比對目標，並提出一個用於推理感知比對 (IAPO) 的架構。我們證明，對於任何推論時間解碼演算法，最佳化比對政策相對於參考政策的推論時間獲勝率的最佳解，就是具有獎勵轉換的典型 RLHF 問題的解。這促使我們提供 KL 正規化的校準和轉換 RL (CTRL) 演算法來解決此問題，其中包括獎勵校準步驟和具有校準獎勵轉換的 KL 正規化獎勵最大化步驟。我們將研究具體化為兩個重要的推論時間策略：最佳 N 選一抽樣和最佳 N 選一越獄，其中從模型中抽取 N 個回應，並選擇具有最高或最低獎勵的回應。我們提出這些策略的特定轉換，並證明我們的架構相較於現有的語言模型比對最新方法有顯著的改善。根據經驗，我們在 Anthropic 助益性和無害性對話基準資料集上，超越未考慮推論時間解碼而設計的基準，推論時間獲勝率高出 8-12% 和 4-9%。

##### **Enhancing Whisper's Accuracy and Speed for Indian Languages through Prompt-Tuning and Tokenization**
2412.19785v1 by Kumud Tripathi, Raj Gothi, Pankaj Wasnik

Automatic speech recognition has recently seen a significant advancement with
large foundational models such as Whisper. However, these models often struggle
to perform well in low-resource languages, such as Indian languages. This paper
explores two novel approaches to enhance Whisper's multilingual speech
recognition performance in Indian languages. First, we propose prompt-tuning
with language family information, which enhances Whisper's accuracy in
linguistically similar languages. Second, we introduce a novel tokenizer that
reduces the number of generated tokens, thereby accelerating Whisper's
inference speed. Our extensive experiments demonstrate that the tokenizer
significantly reduces inference time, while prompt-tuning enhances accuracy
across various Whisper model sizes, including Small, Medium, and Large.
Together, these techniques achieve a balance between optimal WER and inference
speed.

摘要：自動語音辨識最近有了顯著的進展，這要歸功於 Whisper 等大型基礎模型。然而，這些模型在低資源語言（例如印度語言）的表現往往不佳。本文探討了兩種新的方法來增強 Whisper 在印度語言中的多語言語音辨識效能。首先，我們提出使用語言家族資訊進行提示調整，這可以增強 Whisper 在語言上相似的語言中的準確度。其次，我們引入了一個新的分詞器，可以減少產生的分詞數量，從而加快 Whisper 的推論速度。我們的廣泛實驗證明，分詞器可以顯著減少推論時間，而提示調整則可以提高各種 Whisper 模型大小（包括小型、中型和大型）的準確度。綜合來說，這些技術在最佳 WER 和推論速度之間取得了平衡。

##### **Can AI Help with Your Personal Finances?**
2412.19784v1 by Oudom Hean, Utsha Saha, Binita Saha

In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.

摘要：近年來，大型語言模型 (LLM) 已成為人工智慧 (AI) 中的變革性發展，吸引了產業和學術界的極大關注。這些複雜的 AI 系統經過大量資料集訓練，展現出令人印象深刻的自然語言處理和內容生成能力。本文探討了 LLM 在解決個人理財主要挑戰方面的潛力，重點放在美國。我們評估了幾種領先的 LLM，包括 OpenAI 的 ChatGPT、Google 的 Gemini、Anthropic 的 Claude 和 Meta 的 Llama，以評估它們在提供有關抵押貸款、稅金、貸款和投資等主題的準確財務建議方面的有效性。我們的研究結果表明，雖然這些模型達到了約 70% 的平均準確率，但它們在某些領域也表現出顯著的限制。具體來說，LLM 難以對複雜的財務查詢提供準確的回應，且不同主題的表現差異很大。儘管有這些限制，但分析顯示這些模型的較新版本有了顯著的改進，突顯了它們對個人和財務顧問日益增長的實用性。隨著這些 AI 系統持續演進，它們在推動個人理財中 AI 驅動應用程式的潛力變得越來越有希望。

##### **Machine Learning for Sentiment Analysis of Imported Food in Trinidad and Tobago**
2412.19781v1 by Cassandra Daniels, Koffka Khan

This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.

摘要：本研究調查了各種機器學習演算法（CNN、LSTM、VADER 和 RoBERTa）在分析與千里達及托巴哥進口食品有關的 Twitter 資料中的情緒方面的表現。本研究探討了三個主要的研究所需解決的問題：演算法的比較準確性和效率、每個模型的最佳組態，以及最佳化模型在用於監控公眾情緒及其對進口帳單影響的即時系統中的潛在應用。資料集包含從 2018 年到 2024 年的推文，並分為不平衡、平衡和時間子集，以評估資料平衡和 COVID-19 大流行對情緒趨勢的影響。進行了十項實驗以評估各種組態下的模型。結果顯示，VADER 在多類和二元情緒分類中都優於其他模型。本研究強調了 COVID-19 前後情緒趨勢的顯著變化，對進口政策產生了影響。

##### **Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure State**
2412.19759v1 by Zhifu Chen, Hengnian Gu, Jin Peng Zhou, Dongdai Zhou

Cognitive diagnosis represents a fundamental research area within intelligent
education, with the objective of measuring the cognitive status of individuals.
Theoretically, an individual's cognitive state is essentially equivalent to
their cognitive structure state. Cognitive structure state comprises two key
components: knowledge state (KS) and knowledge structure state (KUS). The
knowledge state reflects the learner's mastery of individual concepts, a widely
studied focus within cognitive diagnosis. In contrast, the knowledge structure
state-representing the learner's understanding of the relationships between
concepts-remains inadequately modeled. A learner's cognitive structure is
essential for promoting meaningful learning and shaping academic performance.
Although various methods have been proposed, most focus on assessing KS and
fail to assess KUS. To bridge this gap, we propose an innovative and effective
framework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which
introduces a novel framework to modeling learners' cognitive structures in
diagnostic assessments, thereby offering new insights into cognitive structure
modeling. Specifically, we employ an edge-feature-based graph attention network
to represent the learner's cognitive structure state, effectively integrating
KS and KUS. Extensive experiments conducted on real datasets demonstrate the
superior performance of this framework in terms of diagnostic accuracy and
interpretability.

摘要：認知診斷是智能教育中一個重要的研究領域，目標是測量個人的認知狀態。理論上，一個人的認知狀態基本上等同於他們的認知結構狀態。認知結構狀態包含兩個關鍵組成部分：知識狀態 (KS) 和知識結構狀態 (KUS)。知識狀態反映了學習者對個別概念的掌握程度，這是認知診斷中廣泛研究的重點。相比之下，知識結構狀態——代表學習者對概念之間關係的理解——仍然沒有得到充分的建模。學習者的認知結構對於促進有意義的學習和塑造學業表現至關重要。儘管已經提出了各種方法，但大多數方法都集中在評估 KS 上，而未能評估 KUS。為了彌合這一差距，我們提出了一個創新且有效的框架——基於認知結構狀態的認知診斷 (CSCD)——它引入了一個新穎的框架來對學習者的認知結構進行建模，從而為認知結構建模提供了新的見解。具體來說，我們採用基於邊緣特徵的圖注意力網路來表示學習者的認知結構狀態，有效地整合了 KS 和 KUS。在真實數據集上進行的廣泛實驗證明了這個框架在診斷準確性和可解釋性方面的優越性能。

##### **"Did my figure do justice to the answer?" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)**
2412.19755v1 by Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya

Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.

摘要：個人化回饋在學生的學習過程中扮演著至關重要的角色。
現有的系統擅長提供基於多選題的評量回饋，而本研究更專注於主觀且開放式的問題，這類似於帶有回饋的自動簡答評分 (ASAG) 問題。此外，我們在傳統的 ASAG 回饋問題中引入了多模態簡答評分與回饋 (MMSAF) 問題，以解決學生答案和參考答案可能包含圖片的情況。此外，我們引入了包含 2197 個資料點的 MMSAF 資料集，以及用於產生此類資料集的自動化架構。我們對此資料集上現有的 LLM 進行評估，在正確性標籤上達到了 55% 的整體準確度，在圖片相關性標籤上達到了 75%，而專家評估的 LLM 產生的回饋在正確性層級上獲得了 5 分中的 4.27 分。根據專家的說法，Pixtral 在所有指標上都獲得了 4 分以上的評分，這表示它更符合人類的判斷，並且是協助學生的最佳解決方案。

##### **Can Large Language Models Adapt to Other Agents In-Context?**
2412.19726v1 by Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin D. Weisz, Murray Campbell

As the research community aims to build better AI assistants that are more
dynamic and personalized to the diversity of humans that they interact with,
there is increased interest in evaluating the theory of mind capabilities of
large language models (LLMs). Indeed, several recent studies suggest that LLM
theory of mind capabilities are quite impressive, approximating human-level
performance. Our paper aims to rebuke this narrative and argues instead that
past studies were not directly measuring agent performance, potentially leading
to findings that are illusory in nature as a result. We draw a strong
distinction between what we call literal theory of mind i.e. measuring the
agent's ability to predict the behavior of others and functional theory of mind
i.e. adapting to agents in-context based on a rational response to predictions
of their behavior. We find that top performing open source LLMs may display
strong capabilities in literal theory of mind, depending on how they are
prompted, but seem to struggle with functional theory of mind -- even when
partner policies are exceedingly simple. Our work serves to highlight the
double sided nature of inductive bias in LLMs when adapting to new situations.
While this bias can lead to strong performance over limited horizons, it often
hinders convergence to optimal long-term behavior.

摘要：隨著研究社群致力於打造更優良的人工智慧助理，讓其更具動態性且個人化，以符合與之互動的人類的多樣性，因此對於評估大型語言模型 (LLM) 的心智理論能力也越來越感興趣。事實上，最近的多項研究指出，LLM 心智理論能力相當令人印象深刻，近似於人類等級的表現。我們的論文旨在駁斥這種說法，並主張先前的研究並未直接測量代理人的表現，因此可能導致所得結果本質上具有幻覺性質。我們在所謂的字面心智理論（即測量代理人預測他人行為的能力）與功能性心智理論（即根據對代理人行為預測的合理回應，在情境中適應代理人）之間，做出了明確的區別。我們發現，表現最佳的開源 LLM 可能在字面心智理論中展現強大能力，具體取決於提示方式，但似乎在功能性心智理論中會遇到困難，即使合作夥伴政策過於簡單也是如此。我們的研究旨在強調 LLM 在適應新情況時歸納偏差的雙重性質。雖然這種偏差在有限範圍內可能導致強勁表現，但通常會阻礙收斂至最佳長期行為。

##### **OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis**
2412.19723v1 by Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu

Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.

摘要：受視覺語言模型 (VLM) 驅動的圖形使用者介面 (GUI) 代理已展現出類似人類的電腦控制能力。儘管它們在推進數位自動化方面很有用，但仍存在一個嚴重的瓶頸：收集高品質的軌跡資料以進行訓練。收集此類資料的常見做法依賴於人工監督或透過執行預先定義的任務來產生合成資料，這兩種方法都需要大量資源或無法保證資料品質。此外，這些方法的資料多樣性有限，合成資料與真實世界環境之間存在顯著差距。為了解決這些挑戰，我們提出了 OS-Genesis，這是一個新穎的 GUI 資料合成管道，它逆轉了傳統的軌跡收集流程。OS-Genesis 不依賴於預先定義的任務，而是讓代理先感知環境並執行逐步互動，然後回溯推導高品質的任務以進行軌跡層級探索。然後採用軌跡回報模型來確保生成軌跡的品質。我們證明使用 OS-Genesis 訓練 GUI 代理會顯著提升它們在極具挑戰性的線上基準測試中的效能。深入分析進一步驗證了 OS-Genesis 的效率，以及與現有合成方法相比，它具備優異的資料品質和多樣性。我們的程式碼、資料和檢查點可在 \href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis 主頁} 取得。

##### **Text2Insight: Transform natural language text into insights seamlessly using multi-model architecture**
2412.19718v1 by Pradeep Sain

The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.

摘要：隨著對動態、以使用者為中心資料分析與視覺化的需求日益增加，在醫療保健、金融和研究等領域中顯而易見。傳統的視覺化工具由於其靜態且預先定義的性質，往往無法滿足個別使用者的需求。為了解決這個差距，Text2Insight 被引入作為一個創新的解決方案，可根據使用者定義的自然語言需求提供客製化的資料分析和視覺化。Text2Insight 藉由利用多模型架構，將使用者的輸入轉換為可操作的見解和動態視覺化。
該方法從分析輸入資料集開始，以擷取結構化詳細資料，例如欄和值。預先訓練的 Llama3 模型將使用者的自然語言查詢轉換為 SQL 查詢，並使用命名實體辨識 (NER) 模型進一步改善其準確性。圖表預測器會決定最合適的視覺化類型，而 Llama3 模型則會根據 SQL 查詢的結果產生見解。輸出是一個使用者友善且視覺上具有資訊性的圖表。為了增強分析能力，該系統整合了一個問答模型和一個使用 BERT 框架的預測模型。這些模型提供對歷史資料的見解，並預測未來趨勢。
Text2Insight 的效能評估證明了它的有效性，在 BLEU 分數為 0.5 的情況下，達到了高準確度 (99%)、精確度 (100%)、召回率 (99%) 和 F1 分數 (99%)。問答模型達到了 89% 的準確度，而預測模型達到了 70% 的準確度。這些結果驗證了 Text2Insight 作為一個健全且可行的解決方案，可將自然語言文字轉換為動態、使用者特定的資料分析和視覺化。

##### **Toward Adaptive Reasoning in Large Language Models with Thought Rollback**
2412.19707v1 by Sijia Chen, Baochun Li

Large language models (LLMs) have been routinely used to solve various tasks
using step-by-step reasoning. However, the structure of intermediate reasoning
steps, or thoughts, is rigid and unidirectional, such as chains, trees, or
acyclic-directed graphs. Consequently, the resulting inflexible and
forward-only reasoning may not address challenging tasks and fail when the LLM
frequently gives false responses, i.e., ``hallucinations''. This paper proposes
a new reasoning framework, called Thought Rollback (TR), allowing LLMs to
adaptively build thought structure while maintaining effective reasoning toward
problem-solving under ``hallucinations''. The core mechanism of TR is rolling
back thoughts, which allows LLMs to perform error analysis on thoughts, and
thus roll back to any previously mistaken thought for revision. Subsequently,
by including such trial-and-error in the prompt to guide the LLM, each rollback
leads to one more reliable reasoning path. Therefore, starting with a simple
prompt without human annotations, LLM with TR adaptively and gradually explores
thoughts for a correct solution. Comprehensive experiments on mathematical
problems and multi-task reasoning demonstrate the state-of-the-art performance
of TR in terms of problem-solving rate and interaction cost. For instance, the
solving rate of GPT-4 with TR outperforms the current best by $9\%$ on the MATH
dataset.

摘要：大型語言模型（LLM）已常規用於解決各種任務，使用逐步推理。然而，中間推理步驟或想法的結構是僵化且單向的，例如鏈、樹或無環有向圖。因此，產生的僵化且僅向前推理可能無法解決具有挑戰性的任務，並且當 LLM 頻繁給出錯誤的回應（即「幻覺」）時會失敗。本文提出了一個新的推理框架，稱為 Thought Rollback（TR），允許 LLM 在解決「幻覺」問題時自適應地構建思想結構，同時保持有效的推理。TR 的核心機制是回滾思想，它允許 LLM 對思想執行錯誤分析，並因此回滾到任何先前錯誤的思想進行修改。隨後，通過在提示中包含此類試錯來指導 LLM，每次回滾都會導致一條更可靠的推理路徑。因此，從一個沒有人工註釋的簡單提示開始，帶有 TR 的 LLM 自適應地逐漸探索思想以獲得正確的解決方案。在數學問題和多任務推理上的綜合實驗證明了 TR 在問題解決率和交互成本方面的最先進性能。例如，帶有 TR 的 GPT-4 的求解率在 MATH 數據集上比目前的最佳性能高出 9%。

##### **An Integrated Optimization and Deep Learning Pipeline for Predicting Live Birth Success in IVF Using Feature Optimization and Transformer-Based Models**
2412.19696v1 by Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia

In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.

摘要：體外受精 (IVF) 是一種廣泛使用的輔助生殖技術，但由於臨床、人口統計和程序因素的多方面交互作用，預測其成功仍然具有挑戰性。本研究開發了一個強大的人工智慧 (AI) 管線，旨在預測 IVF 治療中的活產結果。該管線使用 2010 年至 2018 年的匿名數據，這些數據來自人類受精和胚胎學管理局 (HFEA)。我們通過整合不同的特徵選擇方法（例如主成分分析 (PCA) 和粒子群優化 (PSO)）以及不同的傳統機器學習分類器（包括隨機森林 (RF) 和決策樹），以及深度學習分類器（包括自定義Transformer模型和具有注意力機制的 Tab Transformer模型），來評估活產成功的預測性能，作為二元結果（成功/失敗）。我們的研究表明，通過將 PSO 用於特徵選擇與基於 TabTransformer 的深度學習模型相結合，可以獲得最佳性能，準確率達到 99.50%，AUC 達到 99.96%，突顯了其預測活產的顯著性能。本研究建立了一個高度準確的 AI 管線，用於預測 IVF 中的活產結果，展示了其增強個性化生育治療的潛力。

##### **A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**
2412.19685v1 by Jingchun Lian, Lingyu Liu, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng

Image forgery localization, which centers on identifying tampered pixels
within an image, has seen significant advancements. Traditional approaches
often model this challenge as a variant of image segmentation, treating the
binary segmentation of forged areas as the end product. We argue that the basic
binary forgery mask is inadequate for explaining model predictions. It doesn't
clarify why the model pinpoints certain areas and treats all forged pixels the
same, making it hard to spot the most fake-looking parts. In this study, we
mitigate the aforementioned limitations by generating salient region-focused
interpretation for the forgery images. To support this, we craft a Multi-Modal
Tramper Tracing (MMTT) dataset, comprising facial images manipulated using
deepfake techniques and paired with manual, interpretable textual annotations.
To harvest high-quality annotation, annotators are instructed to meticulously
observe the manipulated images and articulate the typical characteristics of
the forgery regions. Subsequently, we collect a dataset of 128,303 image-text
pairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture
designed for concurrent forgery localization and interpretation. ForgeryTalker
first trains a forgery prompter network to identify the pivotal clues within
the explanatory text. Subsequently, the region prompter is incorporated into
multimodal large language model for finetuning to achieve the dual goals of
localization and interpretation. Extensive experiments conducted on the MMTT
dataset verify the superior performance of our proposed model. The dataset,
code as well as pretrained checkpoints will be made publicly available to
facilitate further research and ensure the reproducibility of our results.

摘要：影像偽造定位，其重點在於辨識影像中被竄改的像素，已取得顯著進展。傳統手法通常將此挑戰建模為影像分割的變體，將偽造區域的二元分割視為最終產出。我們認為基本的二元偽造遮罩不足以說明模型預測。它無法釐清模型為何會精確指出特定區域，並將所有偽造像素視為相同，這使得難以找出看起來最假的部份。在本研究中，我們透過為偽造影像產生顯著區域聚焦的詮釋來減輕上述限制。為支援此一目標，我們製作了一個多模式偽造者追蹤 (MMTT) 資料集，其中包含使用深度偽造技術處理的人臉影像，並搭配手動、可詮釋的文字註解。為了收集高品質的註解，我們指示註解者仔細觀察經過處理的影像，並說明偽造區域的典型特徵。隨後，我們收集了一個包含 128,303 組影像文字配對的資料集。我們利用 MMTT 資料集，開發了 ForgeryTalker，這是一種專門用於同時進行偽造定位與詮釋的架構。ForgeryTalker 首先訓練一個偽造提示器網路，以辨識說明文字中的關鍵線索。隨後，將區域提示器納入多模式大型語言模型中進行微調，以達成定位和詮釋的雙重目標。在 MMTT 資料集上進行的廣泛實驗驗證了我們提出的模型的卓越效能。資料集、程式碼以及預訓練的檢查點將公開提供，以利進一步的研究，並確保我們結果的可重現性。

##### **Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**
2412.19684v1 by Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang

Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.

摘要：與多模態大型語言模型 (MLLM) 相比，高效的多模態大型語言模型 (EMLLM) 可縮小模型規模和運算成本，而且通常部署在資源受限的裝置上。然而，由於資料隱私問題，現有的開放原始碼 EMLLM 在預訓練過程中很少能存取私人領域特定資料，這使得它們難以直接應用在特定裝置領域，例如某些商業場景。為了解決這個缺點，本文專注於 EMLLM 對私人領域的有效適應，特別是在兩個方面：1) 如何減少資料需求，以及 2) 如何避免參數微調。具體來說，我們提出了一個無需調校、可適應、通用的提示最佳化架構，簡稱為 \textit{\textbf{\ourmethod{}}}，它包含兩個階段：1) 預定義提示，基於強化搜尋策略，產生提示最佳化策略樹以取得最佳化先驗；2) 提示反映根據最佳化先驗初始化提示，然後進行自我反映以進一步搜尋和改善提示。藉由這麼做，\ourmethod{} 優雅地產生用於處理私人領域特定資料的「理想提示」。請注意，我們的模型不需要參數微調，而且只需要少量資料就能快速適應私人資料的資料分佈。跨多項任務的大量實驗證明，我們提出的 \ourmethod{} 與基準相比，顯著提升了效率和效能。

##### **CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**
2412.19663v1 by Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang

Computer-aided design (CAD) significantly enhances the efficiency, accuracy,
and innovation of design processes by enabling precise 2D and 3D modeling,
extensive analysis, and optimization. Existing methods for creating CAD models
rely on latent vectors or point clouds, which are difficult to obtain and
costly to store. Recent advances in Multimodal Large Language Models (MLLMs)
have inspired researchers to use natural language instructions and images for
CAD model construction. However, these models still struggle with inferring
accurate 3D spatial location and orientation, leading to inaccuracies in
determining the spatial 3D starting points and extrusion directions for
constructing geometries. This work introduces CAD-GPT, a CAD synthesis method
with spatial reasoning-enhanced MLLM that takes either a single image or a
textual description as input. To achieve precise spatial inference, our
approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D
spatial positions and 3D sketch plane rotation angles into a 1D linguistic
feature space using a specialized spatial unfolding mechanism, while
discretizing 2D sketch coordinates into an appropriate planar space to enable
precise determination of spatial starting position, sketch orientation, and 2D
sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT
consistently outperforms existing state-of-the-art methods in CAD model
synthesis, both quantitatively and qualitatively.

摘要：電腦輔助設計（CAD）大幅提升設計流程的效率、準確度和創新，它能進行精準的二維和三維建模、廣泛的分析和最佳化。現有的 CAD 模型建立方法仰賴潛在向量或點雲，這些方法難以取得且儲存成本高昂。多模態大型語言模型（MMLM）的最新進展啟發研究人員使用自然語言指令和影像來建構 CAD 模型。然而，這些模型仍難以推論出準確的三維空間位置和方向，導致在決定空間三維起始點和幾何結構的擠出方向時出現不準確的情況。這項研究引入了 CAD-GPT，一種具備空間推理增強 MLLM 的 CAD 合成方法，它將單張影像或文字描述作為輸入。為了達成精準的空間推論，我們的做法引入了三維建模空間機制。此方法使用專門的空間展開機制，將三維空間位置和三維草圖平面旋轉角度對應到一維語言特徵空間中，同時將二維草圖座標離散化到適當的平面空間中，以利於精準地決定空間起始位置、草圖方向和二維草圖座標轉換。大量的實驗證明，CAD-GPT 在 CAD 模型合成上始終優於現有的最先進方法，無論在量化上或質化上都是如此。

##### **Chimera: A Block-Based Neural Architecture Search Framework for Event-Based Object Detection**
2412.19646v1 by Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil

Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.

摘要：基於事件的相機是模擬人眼的感測器，提供高速穩健性和低功耗等優點。既有的深度學習技術已展現出處理事件資料的效能。Chimera 是一個區塊式神經架構搜尋 (NAS) 架構，專門設計用於基於事件的物件偵測，目標是建立一個系統化的方法，將 RGB 領域的處理方法調整到事件領域。Chimera 設計空間是由各種巨量區塊建構而成，包括注意力區塊、卷積、狀態空間模型和基於 MLP 混合器的架構，在局部和全域處理能力之間提供有價值的折衷，以及不同層級的複雜性。在機器人中的人員偵測 (PEDRo) 資料集上的結果顯示，效能等級可與領先的最新技術模型相媲美，同時平均參數減少 1.6 倍。

##### **Xmodel-2 Technical Report**
2412.19638v1 by Wang Qun, Liu Yang, Lin Qingquan, Qu Zhijiu, Jiang Ling

Xmodel-2 is a 1.2-billion-parameter large language model designed
specifically for reasoning tasks. Its architecture enables different model
scales to share a unified set of hyperparameters, allowing for extensive
experimentation on smaller models and seamless transfer of optimal
configurations to larger models. To maximize training efficiency and stability,
Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on
1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art
performance in complex reasoning and agent-based tasks, while maintaining low
training costs. These results highlight the potential of efficient model design
and training strategies in advancing reasoning capabilities. Model checkpoints
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/Xmodel-2

摘要：Xmodel-2 是一款專門針對推理任務而設計的 12 億參數大型語言模型。其架構讓不同的模型規模可以共用統一的超參數集合，允許在較小的模型上進行廣泛的實驗，並將最佳配置無縫轉移到較大的模型上。為了最大化訓練效率和穩定性，Xmodel-2 採用了 MiniCPM 中的 WSD 學習率排程器。Xmodel-2 在 1.5 兆個來自不同來源的標記上進行預訓練，在複雜推理和基於代理的任務中取得了最先進的效能，同時維持較低的訓練成本。這些結果突顯了高效模型設計和訓練策略在提升推理能力方面的潛力。模型檢查點和程式碼已公開發布在 GitHub 上，網址為 https://github.com/XiaoduoAILab/Xmodel-2

##### **Gradient Weight-normalized Low-rank Projection for Efficient LLM Training**
2412.19616v1 by Jia-Hong Huang, Yixian Shen, Hongyi Zhu, Stevan Rudinac, Evangelos Kanoulas

Large Language Models (LLMs) have shown remarkable performance across various
tasks, but the escalating demands on computational resources pose significant
challenges, particularly in the extensive utilization of full fine-tuning for
downstream tasks. To address this, parameter-efficient fine-tuning (PEFT)
methods have been developed, but they often underperform compared to full
fine-tuning and struggle with memory efficiency. In this work, we introduce
Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach
that enhances both parameter and memory efficiency while maintaining comparable
performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to
improve gradient conditioning, facilitating better convergence during
optimization. Additionally, it applies low-rank approximations to the weight
and gradient matrices, significantly reducing memory usage during training.
Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer
memory usage by up to 89.5% and enables the pre-training of large LLMs, such as
LLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional
inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods
in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all
GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,
surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a
promising alternative for efficient LLM pre-training and fine-tuning. Source
code and Appendix:
https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training

摘要：大型語言模型 (LLM) 在各種任務中展現出卓越的效能，但對運算資源不斷升高的需求帶來顯著的挑戰，尤其是在廣泛使用完整微調進行下游任務時。為了解決這個問題，已開發出參數有效率的微調 (PEFT) 方法，但與完整微調相比，它們通常表現不佳，且在記憶體效率方面有困難。在這項工作中，我們引入了梯度權重正規化低秩投影 (GradNormLoRP)，這是一種創新的方法，可在維持與完整微調相當的效能下，同時提升參數和記憶體效率。GradNormLoRP 對權重矩陣進行正規化以改善梯度調整，促進最佳化期間更好的收斂。此外，它對權重和梯度矩陣套用低秩近似，大幅減少訓練期間的記憶體使用量。廣泛的實驗證明，我們的 8 位元 GradNormLoRP 可將最佳化器記憶體使用量減少多達 89.5%，並可在消費級 GPU（例如 NVIDIA RTX 4090）上預訓練大型 LLM（例如 LLaMA 7B），而不會增加額外的推論成本。此外，GradNormLoRP 在微調任務中優於現有的低秩方法。例如，在使用秩 8 微調 RoBERTa 模型執行所有 GLUE 任務時，GradNormLoRP 達到 80.65 的平均分數，超越 LoRA 的 79.23 分。這些結果強調 GradNormLoRP 是用於高效 LLM 預訓練和微調的有前途的替代方案。原始程式碼和附錄：
https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training

##### **Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance**
2412.19610v1 by Sanjukta Ghosh

This study compares the performance of AI-generated and human-written product
descriptions using a multifaceted evaluation model. We analyze descriptions for
100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)
with and without sample descriptions, against human-written descriptions. Our
evaluation metrics include sentiment, readability, persuasiveness, Search
Engine Optimization(SEO), clarity, emotional appeal, and call-to-action
effectiveness. The results indicate that ChatGPT 4 performs the best. In
contrast, other models demonstrate significant shortcomings, producing
incoherent and illogical output that lacks logical structure and contextual
relevance. These models struggle to maintain focus on the product being
described, resulting in disjointed sentences that do not convey meaningful
information. This research provides insights into the current capabilities and
limitations of AI in the creation of content for e-Commerce.

摘要：本研究使用多面向評估模型來比較 AI 生成的和人類撰寫的產品描述的表現。我們分析了四個 AI 模型（Gemma 2B、LLAMA、GPT2 和 ChatGPT 4）在有和沒有範例描述的情況下為 100 個產品產生的描述，並將其與人類撰寫的描述進行對比。我們的評估指標包括情緒、可讀性、說服力、搜尋引擎最佳化 (SEO)、清晰度、情感吸引力和行動呼籲的有效性。結果顯示 ChatGPT 4 的表現最佳。相比之下，其他模型表現出顯著的缺點，產生的輸出不連貫且不合邏輯，缺乏邏輯結構和上下文關聯性。這些模型難以保持對所描述產品的關注，導致斷斷續續的句子，無法傳達有意義的資訊。本研究提供了對 AI 在電子商務內容創作方面的當前能力和限制的見解。

##### **SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms**
2412.19595v1 by Shashank Rao Marpally, Pranav Goyal, Harold Soh

Current social navigation methods and benchmarks primarily focus on proxemics
and task efficiency. While these factors are important, qualitative aspects
such as perceptions of a robot's social competence are equally crucial for
successful adoption and integration into human environments. We propose a more
comprehensive evaluation of social navigation through scenario-based testing,
where specific human-robot interaction scenarios can reveal key robot
behaviors. However, creating such scenarios is often labor-intensive and
complex. In this work, we address this challenge by introducing a pipeline that
automates the generation of context-, and location-appropriate social
navigation scenarios, ready for simulation. Our pipeline transforms simple
scenario metadata into detailed textual scenarios, infers pedestrian and robot
trajectories, and simulates pedestrian behaviors, which enables more controlled
evaluation. We leverage the social reasoning and code-generation capabilities
of Large Language Models (LLMs) to streamline scenario generation and
translation. Our experiments show that our pipeline produces realistic
scenarios and significantly improves scenario translation over naive LLM
prompting. Additionally, we present initial feedback from a usability study
with social navigation experts and a case-study demonstrating a scenario-based
evaluation of three navigation algorithms.

摘要：當前的社交導航方法和基準主要注重於人體工學和任務效率。雖然這些因素很重要，但對於成功採用和整合到人類環境中來說，機器人的社交能力感知等定性方面同樣至關重要。我們提出通過基於場景的測試對社交導航進行更全面的評估，其中具體的人機互動場景可以揭示機器人的關鍵行為。然而，創建這樣的場景通常需要大量的人力和複雜的過程。在這項工作中，我們通過引入一個自動生成情境和位置適當的社交導航場景的管道來應對這一挑戰，以便進行模擬。我們的管道將簡單的場景元數據轉換為詳細的文本場景，推斷行人和機器人的軌跡，並模擬行人的行為，從而實現更受控的評估。我們利用大型語言模型 (LLM) 的社會推理和代碼生成能力來簡化場景生成和翻譯。我們的實驗表明，我們的管道產生了現實的場景，並且顯著改進了場景翻譯，優於天真的 LLM 提示。此外，我們展示了來自社交導航專家的可用性研究的初步反饋，以及展示了基於場景的三種導航演算法評估的案例研究。

##### **A Comparative Study of Machine Unlearning Techniques for Image and Text Classification Models**
2412.19583v1 by Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail

Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.

摘要：機器遺忘已成為人工智慧中一個重要的領域，
解決了因應資料隱私法規而從機器學習模型中選擇性刪除學習資料的需求。本文提供了一個
對六種最先進的遺忘技術的全面比較分析，這些技術應用於影像和文字分類任務。我們評估了它們的效能、
效率和對法規要求的符合度，並強調了它們在實際場景中的優點和限制。透過系統性地分析
這些方法，我們旨在提供對它們的適用性、
挑戰和權衡的見解，促進道德和適應性機器學習領域的進步。

##### **Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following**
2412.19562v1 by Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang

This work focuses on building a task planner for Embodied Instruction
Following (EIF) using Large Language Models (LLMs). Previous works typically
train a planner to imitate expert trajectories, treating this as a supervised
task. While these methods achieve competitive performance, they often lack
sufficient robustness. When a suboptimal action is taken, the planner may
encounter an out-of-distribution state, which can lead to task failure. In
contrast, we frame the task as a Partially Observable Markov Decision Process
(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,
we propose a closed-loop planner with an adaptation module and a novel
hindsight method, aiming to use as much information as possible to assist the
planner. Our experiments on the ALFRED dataset indicate that our planner
achieves competitive performance under a few-shot assumption. For the first
time, our few-shot agent's performance approaches and even surpasses that of
the full-shot supervised agent.

摘要：本研究重點在於使用大型語言模型（LLM）建立具身化指示（EIF）任務規劃器。先前的研究通常訓練規劃器模仿專家軌跡，將此視為有監督任務。儘管這些方法能達到競爭性的效能，但它們通常缺乏足夠的穩健性。當採取次優行動時，規劃器可能會遇到分布外狀態，這可能會導致任務失敗。相反地，我們將任務設定為部分可觀察馬可夫決策過程（POMDP），並旨在根據少量範例假設開發穩健的規劃器。因此，我們提出了一個具有適應模組和新穎回顧方法的閉迴路規劃器，旨在使用盡可能多的資訊來協助規劃器。我們在 ALFRED 資料集上的實驗表明，我們的規劃器在少量範例假設下能達到競爭性的效能。首次，我們的少量範例代理效能接近甚至超越全範例監督代理。

##### **Learning states enhanced knowledge tracing: Simulating the diversity in real-world learning process**
2412.19550v1 by Shanshan Wang, Xueying Zhang, Keyang Wang, Xun Yang, Xingyi Zhang

The Knowledge Tracing (KT) task focuses on predicting a learner's future
performance based on the historical interactions. The knowledge state plays a
key role in learning process. However, considering that the knowledge state is
influenced by various learning factors in the interaction process, such as the
exercises similarities, responses reliability and the learner's learning state.
Previous models still face two major limitations. First, due to the exercises
differences caused by various complex reasons and the unreliability of
responses caused by guessing behavior, it is hard to locate the historical
interaction which is most relevant to the current answered exercise. Second,
the learning state is also a key factor to influence the knowledge state, which
is always ignored by previous methods. To address these issues, we propose a
new method named Learning State Enhanced Knowledge Tracing (LSKT). Firstly, to
simulate the potential differences in interactions, inspired by Item Response
Theory~(IRT) paradigm, we designed three different embedding methods ranging
from coarse-grained to fine-grained views and conduct comparative analysis on
them. Secondly, we design a learning state extraction module to capture the
changing learning state during the learning process of the learner. In turn,
with the help of the extracted learning state, a more detailed knowledge state
could be captured. Experimental results on four real-world datasets show that
our LSKT method outperforms the current state-of-the-art methods.

摘要：知識追蹤 (KT) 任務專注於根據歷史互動預測學習者的未來表現。知識狀態在學習過程中扮演著關鍵角色。然而，考慮到知識狀態會受到互動過程中各種學習因素的影響，例如練習的相似性、回應的可靠性以及學習者的學習狀態。先前的模型仍然面臨兩個主要的限制。首先，由於各種複雜原因造成的練習差異以及猜測行為造成的回應不可靠性，很難找到與當前已回答練習最相關的歷史互動。其次，學習狀態也是影響知識狀態的一個關鍵因素，而先前的研究方法總是忽略這一點。為了解決這些問題，我們提出了一種名為學習狀態增強知識追蹤 (LSKT) 的新方法。首先，為了模擬互動中的潛在差異，在項目反應理論 (IRT) 典範的啟發下，我們設計了三種不同的嵌入方法，範圍從粗略到細緻的觀點，並對它們進行比較分析。其次，我們設計了一個學習狀態提取模組，以捕捉學習者在學習過程中的學習狀態變化。反過來，在提取的學習狀態的幫助下，可以捕捉到更詳細的知識狀態。在四個真實世界資料集上的實驗結果表明，我們的 LSKT 方法優於當前最先進的方法。

##### **TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data**
2412.19544v1 by Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu

Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.

摘要：语义解析将自然语言问题转换为逻辑形式，在结构化环境中推理中扮演着至关重要的角色。然而，现有方法遇到了两个重大挑战：依赖于大量人工标注的数据集以及对未见示例的泛化能力有限。为了解决这些问题，我们提出了有针对性的合成数据生成 (TARGA)，这是一个实用的框架，可以在没有人工标注的情况下动态生成高度相关的合成数据。从给定问题的相关实体和关系开始，我们通过逐层扩展和跨层组合来探查潜在的相关查询。然后，我们针对这些构建的查询生成相应的自然语言问题，以共同作为上下文学习的合成演示。在多个知识库问题解答 (KBQA) 数据集上的实验表明，TARGA 仅使用 7B 参数模型，就明显优于利用封闭源模型的现有非微调方法，在 GrailQA (+7.7) 和 KBQA-Agent (+12.2) 上的 F1 得分取得了显著提升。此外，TARGA 在非 I.I.D. 设置下还表现出卓越的样本效率、鲁棒性和泛化能力。

##### **P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision**
2412.19533v1 by Junjie Hu, Shuyong Gao, Lingyi Hong, Qishan Wang, Yuzhou Zhao, Yan Wang, Wenqiang Zhang

Recent research in subject-driven generation increasingly emphasizes the
importance of selective subject features. Nevertheless, accurately selecting
the content in a given reference image still poses challenges, especially when
selecting the similar subjects in an image (e.g., two different dogs). Some
methods attempt to use text prompts or pixel masks to isolate specific
elements. However, text prompts often fall short in precisely describing
specific content, and pixel masks are often expensive. To address this, we
introduce P3S-Diffusion, a novel architecture designed for context-selected
subject-driven generation via point supervision. P3S-Diffusion leverages
minimal cost label (e.g., points) to generate subject-driven images. During
fine-tuning, it can generate an expanded base mask from these points, obviating
the need for additional segmentation models. The mask is employed for
inpainting and aligning with subject representation. The P3S-Diffusion
preserves fine features of the subjects through Multi-layers Condition
Injection. Enhanced by the Attention Consistency Loss for improved training,
extensive experiments demonstrate its excellent feature preservation and image
generation capabilities.

摘要：主題驅動生成的最新研究越來越強調選擇性主題特徵的重要性。儘管如此，準確選擇給定參考影像中的內容仍然具有挑戰性，特別是在影像中選擇相似的主題（例如，兩隻不同的狗）時。有些方法嘗試使用文字提示或像素遮罩來隔離特定元素。然而，文字提示通常無法精確描述特定內容，而像素遮罩通常很昂貴。為了解決這個問題，我們引入了 P3S-Diffusion，這是一種新穎的架構，旨在透過點監督進行內容選擇的主題驅動生成。P3S-Diffusion 利用最小成本標籤（例如，點）來生成主題驅動的影像。在微調過程中，它可以從這些點生成一個擴展的基本遮罩，消除了對額外分割模型的需求。此遮罩用於內繪並與主題表示對齊。P3S-Diffusion 透過多層條件注入保留了主題的精細特徵。透過關注一致性損失增強訓練，廣泛的實驗證明了其出色的特徵保留和影像生成能力。

##### **Is Your Text-to-Image Model Robust to Caption Noise?**
2412.19531v1 by Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang

In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.

摘要：<paragraph>在文字轉圖像 (T2I) 生成中，一種普遍的訓練技術涉及利用視覺語言模型 (VLM) 進行圖像重新標題。儘管已知 VLM 會出現幻覺，產生偏離視覺現實的描述性內容，但此類標題幻覺對 T2I 生成效能的影響仍未得到充分探討。透過我們的實證調查，我們首先建立一個包含 VLM 生成的標題的綜合資料集，然後系統性地分析標題幻覺如何影響生成結果。我們的研究結果表明：(1) 標題品質的差異在微調期間持續影響模型輸出。(2) VLM 信心分數可用作偵測和表徵資料分佈中與雜訊相關的模式的可靠指標。(3) 即使標題保真度有細微的變化，也會對學習表徵的品質產生顯著的影響。這些發現共同強調標題品質對模型效能的深遠影響，並強調在 T2I 中需要更精密的穩健訓練演算法。針對這些觀察結果，我們提出了一種利用 VLM 信心分數來減輕標題雜訊的方法，從而增強 T2I 模型對標題中幻覺的穩健性。</paragraph>

##### **Attribution for Enhanced Explanation with Transferable Adversarial eXploration**
2412.19523v1 by Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Huaming Chen, Jianlong Zhou, Fang Chen

The interpretability of deep neural networks is crucial for understanding
model decisions in various applications, including computer vision.
AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution
by incorporating transferable adversarial attack methods such as MIG and GRA,
significantly improving the accuracy and robustness of model explanations. We
conduct extensive experiments on five models, including CNNs (Inception-v3,
ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the
ImageNet dataset. Our method achieves an average performance improvement of
7.57\% over AttEXplore and 32.62\% compared to other state-of-the-art
interpretability algorithms. Using insertion and deletion scores as evaluation
metrics, we show that adversarial transferability plays a vital role in
enhancing attribution results. Furthermore, we explore the impact of
randomness, perturbation rate, noise amplitude, and diversity probability on
attribution performance, demonstrating that AttEXplore++ provides more stable
and reliable explanations across various models. We release our code at:
https://anonymous.4open.science/r/ATTEXPLOREP-8435/

摘要：深度神经网络的可解释性对于理解各种应用（包括计算机视觉）中的模型决策至关重要。AttEXplore++ 是建立在 AttEXplore 之上的高级框架，通过整合可转移对抗攻击方法（例如 MIG 和 GRA）来增强归因，从而显著提高模型解释的准确性和鲁棒性。我们对五个模型（包括 CNN（Inception-v3、ResNet-50、VGG16）和视觉转换器（MaxViT-T、ViT-B/16））进行了广泛的实验，使用 ImageNet 数据集。我们的方法比 AttEXplore 实现了平均 7.57% 的性能提升，比其他最先进的可解释性算法提升了 32.62%。使用插入和删除分数作为评估指标，我们表明对抗可转移性在增强归因结果中起着至关重要的作用。此外，我们探讨了随机性、扰动率、噪声幅度和多样性概率对归因性能的影响，证明 AttEXplore++ 在各种模型中提供了更稳定和可靠的解释。我们在以下位置发布我们的代码：https://anonymous.4open.science/r/ATTEXPLOREP-8435/

##### **Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation**
2412.19522v1 by Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee

Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.

摘要：神經機器翻譯 (NMT) 系統建立在多語言序列對序列語言模型 (msLM) 上，當語言的平行資料量以及模型中語言的表示受到限制時，無法提供預期的結果。這限制了低資源語言 (LRL) 的特定領域 NMT 系統的能力。作為解決方案，可以利用輔助領域的平行資料微調或進一步預訓練 msLM。我們針對這兩種技術在特定領域 LRL-NMT 中的有效性進行評估。我們還探討了領域差異對 NMT 模型效能的影響。我們建議在建構 LRL 的特定領域 NMT 模型時，採用多種策略來利用輔助平行資料。

##### **Estimation of System Parameters Including Repeated Cross-Sectional Data through Emulator-Informed Deep Generative Model**
2412.19517v1 by Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang

Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.

摘要：微分方程式 (DE) 對於建模自然或工程系統的演化至關重要。傳統上，DE 中的參數會調整以符合系統觀察的數據。然而，在政治、經濟和生物學等領域，可用的數據通常是在不同的時間點從不同的主體獨立收集（即重複橫斷面 (RCS) 數據）。當 RCS 數據展現出各種異質性時，傳統的最佳化技術難以精準估計 DE 參數，導致大量資訊流失。為了解決這個問題，我們提出了一種稱為模擬器資訊深度生成模型 (EIDGM) 的新估計方法，旨在處理 RCS 數據。具體來說，EIDGM 整合了一個基於物理資訊神經網路的模擬器，可立即產生 DE 解，以及一個基於 Wasserstein 生成對抗網路的參數產生器，可有效模擬 RCS 數據。我們在指數成長、邏輯族群模型和 Lorenz 系統上評估 EIDGM，證明了它準確捕捉參數分佈的優異能力。此外，我們將 EIDGM 應用於類澱粉蛋白 beta 40 和 beta 42 的實驗數據集，成功捕捉到多樣的參數分佈形狀。這表明 EIDGM 可用於對各種系統建模，並擴展到根據有限的數據揭示系統的操作原理。

##### **Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs**
2412.19513v1 by Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui

Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.

摘要：大型語言模型 (LLM) 可以修正其自行產生的回應，但
在自我修正後，準確度也會下降。為了更深入了解自我修正，我們致力於分解、評估和
分析 LLM 的自我修正行為。透過列舉和分析自我修正前後的答案正確性，我們將
自我修正能力分解為信心（有信心修正答案）和批評（將錯誤答案轉為正確）能力，並提出兩個
從機率角度測量這 2 種能力的指標，以及另一個用於整體自我修正能力評估的指標。根據
我們的分解和評估指標，我們進行了大量的實驗，並得出了一些經驗結論。例如，我們發現不同的模型可以
表現出不同的行為：有些模型有信心，而其他模型則更具批判性。我們還發現，當通過提示或情境學習來操縱模型
自我修正行為時，這兩種能力之間存在權衡（即，提高一種能力可能導致另一種能力下降）。此外，我們找到了一個
簡單但有效的策略，透過轉換監督微調 (SFT) 資料格式來提高自我修正能力，我們的策略在兩種能力上都優於傳統的 SFT，並在自我修正後實現了更高的準確度。我們的程式碼將公開在 GitHub 上。

##### **Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging**
2412.19512v1 by Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee

Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.

摘要：微調大型語言模型 (LLM) 以執行下游任務是一種廣泛採用的方法，但它通常會導致與安全對齊的 LLM 中的安全降級。目前，許多解決方案透過納入額外的安全資料來解決此問題，但在許多情況下這並不可行。在本文中，我們探討以下問題：如何在不依賴額外安全資料的情況下，提升下游任務的效能，同時維持 LLM 的安全性？我們提出一個簡單且有效的方法，在提升 LLM 的下游任務效能的同時，維持其內在安全性：合併微調前和微調後與安全對齊的模型的權重。針對各種下游任務、模型和合併方法的實驗結果證明，此方法有效減輕安全降級，同時提升下游任務的效能，提供了一個實用的解決方案來調整與安全對齊的 LLM。

##### **MBQ: Modality-Balanced Quantization for Large Vision-Language Models**
2412.19509v1 by Shiyao Li, Yingchun Hu, Xuefei Ning, Xihui Liu, Ke Hong, Xiaotao Jia, Xiuhong Li, Yaqi Yan, Pei Ran, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang

Vision-Language Models (VLMs) have enabled a variety of real-world
applications. The large parameter size of VLMs brings large memory and
computation overhead which poses significant challenges for deployment.
Post-Training Quantization (PTQ) is an effective technique to reduce the memory
and computation overhead. Existing PTQ methods mainly focus on large language
models (LLMs), without considering the differences across other modalities. In
this paper, we discover that there is a significant difference in sensitivity
between language and vision tokens in large VLMs. Therefore, treating tokens
from different modalities equally, as in existing PTQ methods, may
over-emphasize the insensitive modalities, leading to significant accuracy
loss. To deal with the above issue, we propose a simple yet effective method,
Modality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ
incorporates the different sensitivities across modalities during the
calibration process to minimize the reconstruction loss for better quantization
parameters. Extensive experiments show that MBQ can significantly improve task
accuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B
VLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel
that fuses the dequantization and GEMV operators, achieving a 1.4x speedup on
LLaVA-onevision-7B on the RTX 4090. The code is available at
https://github.com/thu-nics/MBQ.

摘要：視覺語言模型 (VLM) 已啟用各種真實世界應用程式。VLM 的大型參數大小帶來巨大的記憶體和運算負擔，對部署構成重大挑戰。訓練後量化 (PTQ) 是一種有效的技術，可減少記憶體和運算負擔。現有的 PTQ 方法主要關注大型語言模型 (LLM)，而不考慮其他模式的差異。在本文中，我們發現大型 VLM 中的語言和視覺代碼之間的敏感度存在顯著差異。因此，如現有 PTQ 方法一樣，平等對待不同模式的代碼可能會過度強調不敏感的模式，導致準確度大幅下降。為了處理上述問題，我們提出了一種簡單而有效的方法，即大型 VLM 的模式平衡量化 (MBQ)。具體來說，MBQ 在校準過程中納入跨模式的不同敏感度，以最小化重建損失，以獲得更好的量化參數。大量實驗表明，與 SOTA 基準相比，MBQ 可以顯著提高任務準確度，對於 7B 到 70B VLM，在 W3 和 W4A8 量化下分別提高了 4.4% 和 11.6%。此外，我們實作了一個 W3 GPU 核心，它融合了去量化和 GEMV 算子，在 RTX 4090 上對 LLaVA-onevision-7B 实现了 1.4 倍的加速。程式碼可在 https://github.com/thu-nics/MBQ 取得。

##### **Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models**
2412.19496v1 by Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen

Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.

摘要：大型視覺語言模型 (LVLMs) 在各種任務中展現出令人印象深刻的潛力，但也面臨嚴重的隱私風險，限制了它們的實際應用。目前對 LVLMs 的隱私評估研究範圍有限，在評估維度和隱私類別方面都存在差距。為了彌補這一差距，我們提出了 Multi-P$^2$A，這是一個全面的基準，用於評估 LVLMs 在隱私意識和洩露方面的隱私保護能力。隱私意識衡量模型識別輸入數據隱私敏感性的能力，而隱私洩露則評估模型在其輸出中意外洩露隱私信息的風險。我們設計了一系列子任務來徹底評估 LVLMs 提供的隱私保護。Multi-P$^2$A 涵蓋 26 類個人隱私、15 類商業機密和 18 類國家機密，總計 31,962 個樣本。基於 Multi-P$^2$A，我們評估了 21 個開源和 2 個閉源 LVLMs 的隱私保護能力。我們的結果表明，當前 LVLMs 通常會帶來促進隱私洩露的高風險，其漏洞因個人隱私、商業機密和國家機密而異。

##### **Disparate Model Performance and Stability in Machine Learning Clinical Support for Diabetes and Heart Diseases**
2412.19495v1 by Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo

Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.

摘要：機器學習 (ML) 演算法對於支援生物醫學資訊學中的臨床決策至關重要。然而，其預測效能可能因人口統計群組而異，通常是因為在訓練資料集中歷史上被邊緣化的族群代表性不足。調查顯示，在慢性疾病資料集及其衍生的 ML 模型中，普遍存在與性別和年齡相關的不平等。因此，引進了一個新穎的分析架構，將系統性的任意性與傳統指標（例如準確度和資料複雜度）結合在一起。對來自 25,000 多名慢性病患者的資料進行分析，發現輕微的性別相關差異，有利於男性預測準確度，以及顯著的年齡相關差異，年輕患者的準確度較高。值得注意的是，老年患者在七個資料集中顯示出不一致的預測準確度，這與較高的資料複雜度和較低的模型效能有關。這突顯出訓練資料中的代表性並不能保證公平的結果，在臨床環境中部署模型之前必須解決模型的任意性。

##### **Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering**
2412.19482v1 by Shiwen Ni, Hao Cheng, Min Yang

Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.

摘要：法律問題問答 (QA) 已引起尋求法律建議的人們越來越多的關注，其目標是從大量問題-答案對資料庫中擷取最適用的答案。先前的做法主要使用雙編碼器架構來學習問題和答案的密集表示。然而，這些方法可能會因為缺乏領域知識和足夠的標記訓練資料而受到影響。在本文中，我們提出了一個三階段（預訓練、微調和重新排序）框架，用於法律問答 (稱為 PFR-LQA)，這促進了細粒度文本表示學習，並提升了使用雙編碼器架構的密集檢索效能。具體來說，我們首先透過自監督訓練目標對法律問題和答案進行特定領域的預訓練，讓預訓練模型能夠適應法律領域。接著，我們使用監督式學習目標對法律問題-答案對進行雙編碼器的特定任務微調，從而產生針對特定下游問答任務的高品質雙編碼器。最後，我們採用一個脈絡重新排序目標來進一步精煉文件編碼器產生的問題的輸出表示，它使用脈絡相似性來增加錨定和困難負面範例之間的差異，以利更好的問題重新排序。我們在人工標註的法律問答資料集上進行廣泛的實驗。實驗結果顯示，我們的 PFR-LQA 方法在法律問題回答方面獲得比強大競爭對手更好的效能。

##### **Optimizing Helmet Detection with Hybrid YOLO Pipelines: A Detailed Analysis**
2412.19467v1 by Vaikunth M, Dejey D, Vishaal C, Balamurali S

Helmet detection is crucial for advancing protection levels in public road
traffic dynamics. This problem statement translates to an object detection
task. Therefore, this paper compares recent You Only Look Once (YOLO) models in
the context of helmet detection in terms of reliability and computational load.
Specifically, YOLOv8, YOLOv9, and the newly released YOLOv11 have been used.
Besides, a modified architectural pipeline that remarkably improves the overall
performance has been proposed in this manuscript. This hybridized YOLO model
(h-YOLO) has been pitted against the independent models for analysis that
proves h-YOLO is preferable for helmet detection over plain YOLO models. The
models were tested using a range of standard object detection benchmarks such
as recall, precision, and mAP (Mean Average Precision). In addition, training
and testing times were recorded to provide the overall scope of the models in a
real-time detection scenario.

摘要：頭盔偵測對於提升公共道路交通動態中的防護等級至關重要。此問題陳述轉化為一個物件偵測任務。因此，本文在頭盔偵測的脈絡下，比較了近期僅需觀看一次 (YOLO) 模型在可靠性和運算負載方面的表現。具體來說，使用了 YOLOv8、YOLOv9 和新發布的 YOLOv11。此外，本文提出了一個修改的架構管道，顯著提升了整體效能。此混合式 YOLO 模型 (h-YOLO) 已與獨立模型進行對比分析，證明 h-YOLO 優於一般 YOLO 模型，適用於頭盔偵測。使用一系列標準物件偵測基準測試模型，例如召回率、精確度和 mAP（平均平均精確度）。此外，記錄了訓練和測試時間，以提供模型在即時偵測情境中的整體範圍。

##### **Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models**
2412.19450v1 by Hyeonseok Moon, Jaehyung Seo, Seungyoon Lee, Chanjun Park, Heuiseok Lim

One of the key strengths of Large Language Models (LLMs) is their ability to
interact with humans by generating appropriate responses to given instructions.
This ability, known as instruction-following capability, has established a
foundation for the use of LLMs across various fields and serves as a crucial
metric for evaluating their performance. While numerous evaluation benchmarks
have been developed, most focus solely on clear and coherent instructions.
However, we have noted that LLMs can become easily distracted by
instruction-formatted statements, which may lead to an oversight of their
instruction comprehension skills. To address this issue, we introduce the
Intention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'
capacity to remain focused and understand instructions without being misled by
extraneous instructions. The primary objective of this benchmark is to identify
the appropriate instruction that accurately guides the generation of a given
context. Our findings suggest that even recently introduced state-of-the-art
models still lack instruction understanding capability. Along with the
proposition of IoInst in this study, we also present broad analyses of the
several strategies potentially applicable to IoInst.

摘要：大型語言模型 (LLM) 的主要優勢之一是它們能夠透過產生適當的回應來與人類互動，以回應給定的指示。這種能力稱為遵循指示的能力，它為 LLM 在各種領域的使用奠定了基礎，並作為評估其效能的關鍵指標。雖然已經開發出許多評估基準，但大多數僅專注於明確且連貫的指示。然而，我們注意到 LLM 很容易被指示格式化的陳述分散注意力，這可能會導致忽視它們的指示理解能力。為了解決這個問題，我們引入了指示意圖 (IoInst) 基準。此基準評估 LLM 在不受到額外指示誤導的情況下保持專注和理解指示的能力。此基準的主要目標是識別適當的指示，以準確指導給定內容的產生。我們的研究結果表明，即使是最近推出的最先進模型仍然缺乏指示理解能力。除了本研究中提出的 IoInst 命題之外，我們還對可能適用於 IoInst 的幾種策略進行了廣泛分析。

##### **Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models**
2412.19449v1 by Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao

This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.

摘要：本研究提出了一种基于大型语言模型和特征对齐的知识蒸馏算法，旨在有效地将大型预训练模型的知识转移到轻量级学生模型中，从而在保持较高模型性能的同时降低计算成本。与传统的软标签蒸馏方法不同，该方法引入了一种多层特征对齐策略，以深度对齐教师模型和学生模型的中间特征和注意力机制，最大程度地保留教师模型的语义表达能力和上下文建模能力。在方法设计方面，构建了一个多任务损失函数，包括特征匹配损失、注意力对齐损失和输出分布匹配损失，以确保通过联合优化进行多级信息传递。在 GLUE 数据集和各种自然语言处理任务上对实验进行了全面评估。结果表明，所提出的模型在困惑度、BLEU、ROUGE 和 CER 等评估指标方面与最先进的 GPT-4 模型非常接近。同时，它远远超过了 DeBERTa、XLNet 和 GPT-3 等基线模型，显示出显着的性能改进和计算效率优势。研究结果表明，特征对齐蒸馏策略是一种有效的模型压缩方法，可以在保持模型能力的同时显著降低计算开销和存储需求。未来的研究可以在自监督学习、跨模态特征对齐和多任务迁移学习的方向上进一步扩展，为深度学习模型的部署和优化提供更灵活、更高效的解决方案。

##### **A Survey on Large Language Model Acceleration based on KV Cache Management**
2412.19442v1 by Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen

Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (KV) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of KV cache management strategies for LLM acceleration,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include KV cache selection, budget
allocation, merging, quantization, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance KV reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable KV cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.

摘要：<paragraph>大型語言模型 (LLM) 已徹底改變了廣泛的領域，例如自然語言處理、電腦視覺和多模態任務，因為它們能夠理解上下文並執行邏輯推理。然而，LLM 的計算和記憶體需求，特別是在推理期間，在將其擴展到現實世界、長上下文和即時應用時會帶來重大挑戰。鍵值 (KV) 快取管理已成為一種關鍵的最佳化技術，可透過減少重複計算和改善記憶體利用率來加速 LLM 推論。這項調查提供了 LLM 加速的 KV 快取管理策略的全面概述，將其分類為令牌級、模型級和系統級最佳化。令牌級策略包括 KV 快取選擇、預算分配、合併、量化和低秩分解，而模型級最佳化則專注於架構創新和注意機制以增強 KV 重用。系統級方法解決了記憶體管理、排程和硬體感知設計，以提高不同運算環境的效率。此外，這項調查概述了用於評估這些策略的文字和多模態資料集和基準。透過提供詳細的分類法和比較分析，這項工作旨在為研究人員和從業人員提供有用的見解，以支援高效且可擴充的 KV 快取管理技術的開發，並有助於在現實世界應用中實際部署 LLM。KV 快取管理的精選論文清單在：
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}。</paragraph>

##### **DeepSeek-V3 Technical Report**
2412.19437v1 by DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, Zizheng Pan

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with
671B total parameters with 37B activated for each token. To achieve efficient
inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent
Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated
in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free
strategy for load balancing and sets a multi-token prediction training
objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion
diverse and high-quality tokens, followed by Supervised Fine-Tuning and
Reinforcement Learning stages to fully harness its capabilities. Comprehensive
evaluations reveal that DeepSeek-V3 outperforms other open-source models and
achieves performance comparable to leading closed-source models. Despite its
excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its
full training. In addition, its training process is remarkably stable.
Throughout the entire training process, we did not experience any irrecoverable
loss spikes or perform any rollbacks. The model checkpoints are available at
https://github.com/deepseek-ai/DeepSeek-V3.

摘要：我們展示了 DeepSeek-V3，一個強大的 Mixture-of-Experts (MoE) 語言模型，總參數為 671B，每個權杖啟用了 37B。為了實現高效的推理和具有成本效益的訓練，DeepSeek-V3 採用了多頭潛在注意 (MLA) 和 DeepSeekMoE 架構，這些架構在 DeepSeek-V2 中得到了徹底驗證。此外，DeepSeek-V3 開創了一種無輔助損失的負載平衡策略，並設定了一個多權杖預測訓練目標，以獲得更強大的效能。我們在 14.8 兆個多樣化且高品質的權杖上預訓練 DeepSeek-V3，然後進行監督微調和強化學習階段以充分利用其功能。綜合評估表明，DeepSeek-V3 優於其他開源模型，並取得與領先的閉源模型相當的效能。儘管效能出色，但 DeepSeek-V3 的完整訓練僅需 2.788M H800 GPU 小時。此外，其訓練過程非常穩定。在整個訓練過程中，我們沒有遇到任何不可復原的損失尖峰或執行任何回滾。模型檢查點可在 https://github.com/deepseek-ai/DeepSeek-V3 取得。

##### **Residual Feature-Reutilization Inception Network for Image Classification**
2412.19433v1 by Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao

Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.

摘要：在電腦視覺領域中，有效擷取特徵資訊非常重要。隨著卷積神經網路 (CNN) 的發展，殘差連接和多重尺度等概念持續提升各種深度學習視覺任務的效能。在本文中，我們提出了一種新穎的 CNN 架構，它包含殘差特徵再利用 inception (ResFRI) 或分割殘差特徵再利用 inception (Split-ResFRI)。它由四個卷積組合組成，這些組合由特別設計的資訊交互通道連接，用於萃取多尺度特徵資訊，並有效增加模型的感受野。此外，根據上述設計的網路結構，Split-ResFRI 可以調整輸入資訊的分割比率，從而減少參數數量並保證模型效能。具體來說，在基於流行視覺資料集的實驗中，例如 CIFAR10 (97.94%)、CIFAR100 (85.91%) 和 Tiny Imagenet (70.54%)，我們在模型大小近似且未使用額外資料的前提下，獲得了與其他現代模型相比最先進的結果。

##### **Revisiting PCA for time series reduction in temporal dimension**
2412.19423v1 by Jiaxin Gao, Wenbo Hu, Yuntian Chen

Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.

摘要：<paragraph>重訪時間維度中 PCA 時序簡約；賈鑫高、胡文波、陳雲天；深度學習顯著地提升了時序分析 (TSA)，能為分類、預測和迴歸等任務萃取複雜模式。儘管降維傳統上專注於變數空間，在最小化資料冗餘和運算複雜度上取得顯著成功，但較少關注於減少時間維度。在這項研究中，我們重新探討主成分分析 (PCA)，一種經典的降維技術，以探索其在時序資料時間維度簡約中的效用。一般認為將 PCA 應用於時間維度會破壞時間依賴性，導致在這個領域的探索有限。然而，我們的理論分析和廣泛實驗證明，將 PCA 應用於滑動序列視窗不僅能維持模型效能，還能提升運算效率。在自迴歸預測中，時間結構部分地透過視窗化保留，而 PCA 在這些視窗中應用以去除時序雜訊，同時保留其統計資訊。透過 PCA 預處理時序資料，我們在將其輸入 TSA 模型（例如線性、Transformer、CNN 和 RNN 架構）之前降低時間維度。這種方法加速訓練和推論，並減少資源消耗。值得注意的是，PCA 將 Informer 訓練和推論速度提升了 40%，並將 TimesNet 的 GPU 記憶體使用量減少了 30%，且不犧牲模型準確度。與其他簡約方法的比較分析進一步突顯了 PCA 在提升 TSA 模型效率方面的效用。</paragraph>

##### **Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression Profiles via Deep Learning**
2412.19422v1 by Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi

De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.

摘要：從頭產生類命中分子是藥物發現過程中的一項艱鉅任務。以前的研究中，大多數方法透過分析分子圖或簡化的分子輸入線條輸入系統 (SMILES) 字串來學習分子結構的語意和語法；然而，它們沒有考慮由基因和蛋白質組成的生物系統的藥物反應。在本研究中，我們提出了一個深度生成模型 Gx2Mol，它利用基因表現特徵檔來產生具有任意目標蛋白質所需表型的分子結構。在演算法中，變分自動編碼器被用作特徵萃取器，以學習基因表現特徵檔的潛在特徵分佈。然後，利用長短期記憶作為化學生成器來產生符合特徵萃取器萃取的基因表現特徵檔特徵條件的語法上有效的 SMILES 字串。實驗結果和案例研究表明，所提出的 Gx2Mol 模型可以產生具有潛在生物活性和類藥物特性的新分子。

##### **Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers**
2412.19419v1 by James H. Tanis, Chris Giannella, Adrian V. Mariano

Graph neural networks are deep neural networks designed for graphs with
attributes attached to nodes or edges. The number of research papers in the
literature concerning these models is growing rapidly due to their impressive
performance on a broad range of tasks. This survey introduces graph neural
networks through the encoder-decoder framework and provides examples of
decoders for a range of graph analytic tasks. It uses theory and numerous
experiments on homogeneous graphs to illustrate the behavior of graph neural
networks for different training sizes and degrees of graph complexity.

摘要：圖形神經網路是專門針對圖形設計的深度神經網路，其中屬性附加在節點或邊緣上。由於這些模型在廣泛任務上表現出色，因此相關研究論文數量在文獻中快速增長。這項調查透過編碼器-解碼器架構介紹圖形神經網路，並提供一系列圖形分析任務的解碼器範例。它使用理論和大量同質圖形實驗來說明圖形神經網路在不同訓練大小和圖形複雜度程度下的行為。

##### **Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization**
2412.19418v1 by Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun

Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.

摘要：弱监督时序动作定位 (WS-TAL) 是一项任务，目标是定位完整的动作实例并使用视频级标签对其进行分类。动作背景模糊性，主要是由聚合和动作内变化导致的背景噪声引起的，是现有 WS-TAL 方法的一项重大挑战。在本文中，我们引入了一个混合多头注意力 (HMHA) 模块和基于广义不确定性的证据融合 (GUEF) 模块来解决这个问题。提出的 HMHA 通过过滤冗余信息并调整其特征分布以更好地与 WS-TAL 任务对齐，有效地增强了 RGB 和光流特征。此外，提出的 GUEF 通过融合片段级证据来细化不确定性度量并选择卓越的前景特征信息，自适应地消除了背景噪声的干扰，这使模型能够专注于整体动作实例，以实现更好的动作定位和分类性能。在 THUMOS14 数据集上进行的实验结果表明，我们的方法优于最先进的方法。我们的代码可在 \url{https://github.com/heyuanpengpku/GUEF/tree/main} 中获得。

##### **Fully Data-driven but Interpretable Human Behavioural Modelling with Differentiable Discrete Choice Model**
2412.19403v1 by Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa

Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.

摘要：離散選擇模型對於建模人類行為中各種決策制定過程至關重要。然而，這些模型的規範嚴重依賴於專家的領域知識，而複雜人類行為的完全自動化但可解釋建模一直是一個長期的挑戰。在本文中，我們介紹了可微分離散選擇模型 (Diff-DCM)，這是一種完全由數據驅動的方法，用於複雜人類行為的可解釋建模、學習、預測和控制，這是通過可微分程式實現的。僅從輸入特徵和選擇結果中，而無任何先驗知識，Diff-DCM 就可以估計可解釋的閉合形式效用函數，以重現觀察到的行為。使用合成和真實世界數據進行的綜合實驗表明，Diff-DCM 可以應用於各種類型的數據，並且只需要少量的計算資源進行估計，這些估計可以在沒有任何加速器的筆記型電腦上在幾十秒內完成。在這些實驗中，我們還證明了 Diff-DCM 利用其可微分性，可以提供對人類行為的有用見解，例如有效行為改變的最佳干預路徑。本研究為人類行為的完全自動化和可靠建模、預測和控制提供了堅實的基礎。

##### **Comparing Few to Rank Many: Active Human Preference Learning using Randomized Frank-Wolfe**
2412.19396v1 by Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton

We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.

摘要：我們研究從有限的比較回饋中學習人類偏好的方法。
這項任務在機器學習中無所不在。它的應用，例如從人類回饋中強化學習，已經產生變革。我們將這個問題表述為學習一個 Plackett-Luce 模型，從 $N$ 個選擇中，透過 $K$ 向比較回饋來學習，其中通常 $K \ll N$。我們的解決方案是針對 Plackett-Luce 目標的 D 最佳設計。該設計定義了一個資料記錄政策，它會從所有 ${N \choose K}$ 可行子集中，引發對少數最佳選擇點的比較回饋。這項工作中主要的演算法挑戰在於，即使是解決 D 最佳設計的快速方法，也會有 $O({N \choose K})$ 的時間複雜度。為了解決這個問題，我們提出一個隨機化 Frank-Wolfe (FW) 演算法，它在 FW 方法中，針對隨機選擇的變數來解決線性最大化子問題。我們分析了該演算法，並在合成和開源的 NLP 資料集上對它進行經驗評估。

##### **An Engorgio Prompt Makes Large Language Model Babble on**
2412.19394v1 by Jianshuo Dong, Ziyuan Zhang, Qingjie Zhang, Han Qiu, Tianwei Zhang, Hao Wang, Hewu Li, Qi Li, Chao Zhang, Ke Xu

Auto-regressive large language models (LLMs) have yielded impressive
performance in many real-world tasks. However, the new paradigm of these LLMs
also exposes novel threats. In this paper, we explore their vulnerability to
inference cost attacks, where a malicious user crafts Engorgio prompts to
intentionally increase the computation cost and latency of the inference
process. We design Engorgio, a novel methodology, to efficiently generate
adversarial Engorgio prompts to affect the target LLM's service availability.
Engorgio has the following two technical contributions. (1) We employ a
parameterized distribution to track LLMs' prediction trajectory. (2) Targeting
the auto-regressive nature of LLMs' inference process, we propose novel loss
functions to stably suppress the appearance of the <EOS> token, whose
occurrence will interrupt the LLM's generation process. We conduct extensive
experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.
The results show that Engorgio prompts can successfully induce LLMs to generate
abnormally long outputs (i.e., roughly 2-13$\times$ longer to reach 90%+ of the
output length limit) in a white-box scenario and our real-world experiment
demonstrates Engergio's threat to LLM service with limited computing resources.
The code is accessible at https://github.com/jianshuod/Engorgio-prompt.

摘要：自動回歸大型語言模型 (LLM) 在許多實際任務中表現得令人印象深刻。然而，這些 LLM 的新範式也暴露了新的威脅。在本文中，我們探討了它們對推理成本攻擊的脆弱性，在這種攻擊中，惡意使用者會製作 Engorgio 提示，以故意增加推理過程的運算成本和延遲。我們設計了 Engorgio，一種新穎的方法，以有效產生對抗性的 Engorgio 提示來影響目標 LLM 的服務可用性。Engorgio 具有以下兩個技術貢獻。(1) 我們採用參數化分佈來追蹤 LLM 的預測軌跡。(2) 針對 LLM 推理過程的自動回歸特性，我們提出了新穎的損失函數，以穩定地抑制 <EOS> 令牌的出現，其出現會中斷 LLM 的生成過程。我們對 13 個開源 LLM 進行了廣泛的實驗，其參數範圍從 125M 到 30B。結果表明，Engorgio 提示可以成功誘導 LLM 生成異常長的輸出（即，大約 2-13 倍長才能達到 90% 以上的輸出長度限制）在白盒場景中，我們的真實世界實驗證明了 Engergio 對具有有限計算資源的 LLM 服務的威脅。程式碼可以在 https://github.com/jianshuod/Engorgio-prompt 獲得。

##### **An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for Digit Classification**
2412.19391v1 by Eugene Choi, Julian Rodriguez, Edmund Young

Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .

摘要：領域適應是研究的活躍領域，由對健全機器學習模型的需求所驅動，這些模型在真實世界資料中表現良好。
用於深度神經網路 (DNN) 的對抗學習已成為一種有前途的方法，用於改善泛化能力，特別是對於影像分類。在本文中，我們實作了一種特定的對抗學習技術，稱為對抗判別領域適應 (ADDA)，並複製原始 ADDA 論文中的數字分類實驗。我們透過檢查更廣泛的領域轉移來擴展他們的發現，並提供 ADDA 後的領域內分類準確度的詳細分析。我們的結果表明，ADDA 在某些領域轉移中顯著提高了準確度，同時對領域內效能的影響最小。此外，我們提供定性分析並提出 ADDA 在不太成功的領域轉移中的限制的潛在解釋。程式碼位於
https://github.com/eugenechoi2004/COS429_FINAL 。

##### **Large Language Models for Market Research: A Data-augmentation Approach**
2412.19363v1 by Mengxin Wang, Dennis J. Zhang, Heng Zhang

Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.

摘要：大型語言模型 (LLM) 透過在複雜的自然語言處理任務中表現出色，轉變了人工智慧。它們產生類似人類文字的能力為市場研究開創了新的可能性，特別是在聯合分析中，了解消費者偏好至關重要，但通常需要大量資源。傳統的基於調查的方法在可擴充性和成本方面面臨限制，這使得 LLM 生成的數據成為一個有前途的替代方案。然而，儘管 LLM 有模擬真實消費者行為的潛力，但最近的研究強調了 LLM 生成數據和人類數據之間存在顯著差距，在兩者之間進行替換時會引入偏差。在本文中，我們通過提出一個新穎的統計數據擴充方法來解決這個差距，該方法有效地將 LLM 生成的數據與聯合分析中的真實數據整合在一起。我們的模型利用遷移學習原理，使用少量的人類數據來消除 LLM 生成的數據的偏差。這產生了具有相容和漸近常態特性的統計穩健估計器，這與天真方法形成對比，天真方法簡單地用 LLM 生成的數據替換人類數據，這會加劇偏差。我們通過對 COVID-19 疫苗偏好的實證研究驗證了我們的框架，證明了它在減少估計誤差和節省數據和成本方面具有優越的能力，節省幅度為 24.9% 至 79.8%。相反，天真方法由於 LLM 生成的數據與人類數據相比固有的偏差，而無法節省數據。另一項關於跑車選擇的實證研究驗證了我們結果的穩健性。我們的研究結果表明，儘管 LLM 生成的數據並不能直接替代人類的反應，但當在穩健的統計框架內使用時，它可以作為有價值的補充。

##### **Dynamic Skill Adaptation for Large Language Models**
2412.19361v1 by Jiaao Chen, Diyi Yang

We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.

摘要：我們提出動態技能適應 (DSA)，一種適應性和動態框架，用於將新穎且複雜的技能適應到大型語言模型 (LLM)。與先前從人類策劃和靜態資料中以隨機順序學習的工作相比，我們建議首先透過模擬人類的學習路徑自動產生和組織訓練資料，然後根據訓練動態動態調整訓練資料。具體來說，受到人類教育系統中的學習結構和教學策略的啟發，我們首先透過將複雜技能分解成子技能並根據它們在人類音節中的依賴性來排列它們來構建技能圖。對於每項技能，我們利用 LLM 產生類似教科書的資料，其中包含技能的詳細描述，用於預訓練和練習類型的資料，其目標是明確利用技能解決問題，以進行指令調整。此外，在指令調整期間，我們會動態更新訓練資料，其中會降低易於學習範例的權重、產生更複雜的範例，並過濾掉有錯誤的資料。在 LLAMA 和 Mistral 等大型語言模型上進行的實驗證明了我們提出的方法在適應數學推理技能和社會研究技能方面的有效性。

##### **ETTA: Elucidating the Design Space of Text-to-Audio Models**
2412.19351v1 by Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro

Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.

摘要：近年來，文字轉音訊 (TTA) 合成技術獲得顯著進展，使用戶能夠透過從自然語言提示產生的合成音訊來豐富其創作流程。儘管有這些進展，但資料、模型架構、訓練目標函數和抽樣策略對目標基準的影響仍未被充分了解。為了全面了解 TTA 模型的設計空間，我們建立了一個針對擴散和流動匹配模型的大規模實證實驗。我們的貢獻包括：1) AF-Synthetic，一個從音訊理解模型中獲得的高品質合成標題的大型資料集；2) 對 TTA 模型的不同架構、訓練和推論設計選擇進行系統比較；3) 對抽樣方法及其在生成品質和推論速度方面的帕累托曲線進行分析。我們利用從這項廣泛分析中獲得的知識來提出我們最好的模型，稱為 Elucidated Text-To-Audio (ETTA)。在 AudioCaps 和 MusicCaps 上進行評估時，ETTA 對使用公開資料訓練的基線模型提供了改進，同時與使用專有資料訓練的模型具有競爭力。最後，我們展示了 ETTA 在生成遵循複雜且富有想像力的標題的創意音訊方面的改進能力——這項任務比目前的基準更具挑戰性。

##### **On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages**
2412.19350v1 by Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi

Selective state-space models (SSMs) are an emerging alternative to the
Transformer, offering the unique advantage of parallel training and sequential
inference. Although these models have shown promising performance on a variety
of tasks, their formal expressiveness and length generalization properties
remain underexplored. In this work, we provide insight into the workings of
selective SSMs by analyzing their expressiveness and length generalization
performance on regular language tasks, i.e., finite-state automaton (FSA)
emulation. We address certain limitations of modern SSM-based architectures by
introducing the Selective Dense State-Space Model (SD-SSM), the first selective
SSM that exhibits perfect length generalization on a set of various regular
language tasks using a single layer. It utilizes a dictionary of dense
transition matrices, a softmax selection mechanism that creates a convex
combination of dictionary matrices at each time step, and a readout consisting
of layer normalization followed by a linear map. We then proceed to evaluate
variants of diagonal selective SSMs by considering their empirical performance
on commutative and non-commutative automata. We explain the experimental
results with theoretical considerations. Our code is available at
https://github.com/IBM/selective-dense-state-space-model.

摘要：選擇性狀態空間模型 (SSM) 是 Transformer 的新興替代方案，提供平行訓練和順序推論的獨特優勢。儘管這些模型在各種任務上表現出令人滿意的效能，但它們的形式表達力和長度概化特性仍未得到充分探討。在這項工作中，我們透過分析選擇性 SSM 在規則語言任務（即有限狀態自動機 (FSA) 模擬）上的表達力和長度概化效能，深入了解選擇性 SSM 的運作方式。我們透過導入選擇性稠密狀態空間模型 (SD-SSM) 來解決現代基於 SSM 的架構的某些限制，這是第一個在單層上對各種規則語言任務表現出完美長度概化的選擇性 SSM。它利用稠密轉移矩陣的字典、在每個時間步長建立字典矩陣的凸組合的 softmax 選擇機制，以及由層正規化後接線性映射組成的讀出。然後，我們繼續評估對角選擇性 SSM 的變體，考慮它們在交換和非交換自動機上的經驗效能。我們以理論考量說明實驗結果。我們的程式碼可在 https://github.com/IBM/selective-dense-state-space-model 取得。

##### **Semi-Supervised Learning from Small Annotated Data and Large Unlabeled Data for Fine-grained PICO Entity Recognition**
2412.19346v1 by Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng

Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.

摘要：<paragraph>目標：從臨床試驗文獻中萃取 PICO 元素（參與者、干預措施、比較和結果），對於臨床證據的檢索、評估和綜合至關重要。現有方法無法區分 PICO 實體的屬性。本研究旨在開發一個命名實體辨識 (NER) 模型，以精細粒度萃取 PICO 實體。
材料和方法：使用包含來自 4 個公開資料集的 2,511 篇摘要的語料庫，我們開發了一種半監督式方法，透過結合有限的 PICO 實體標註資料和豐富的未標註資料，來促進 NER 模型 FinePICO 的訓練。為了進行評估，我們將整個資料集分為兩個子集：一個較小的有標註群組和一個較大的無標註群組。然後，我們分別根據僅在小標註子集上訓練的監督式學習模型和在具有完整標註的整個集合上訓練的監督式學習模型的效能，建立理論上的下限和上限效能界線。最後，我們在較小的標註子集和較大的最初未標註子集上評估 FinePICO。我們使用準確度、召回率和 F1 來衡量 FinePICO 的效能。
結果：我們的模型使用一小組標註樣本，分別達到 0.567/0.636/0.60 的準確度/召回率/F1，比基線模型 (F1：0.437) 高出 16% 以上。該模型展示了對不同 PICO 架構和另一個語料庫的泛化性，在不同的實驗設定中始終優於基準 (p 值 \textless0.001)。
結論：本研究貢獻了一種可泛化且有效的半監督式方法，利用大量未標註資料和少量標註資料來進行命名實體辨識。它最初也支援精細粒度的 PICO 萃取。</paragraph>

##### **CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models**
2412.19331v1 by Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou

Recent advances in Large Vision-Language Models (LVLMs) have sparked
significant progress in general-purpose vision tasks through visual instruction
tuning. While some works have demonstrated the capability of LVLMs to generate
segmentation masks that align phrases with natural language descriptions in a
single image, they struggle with segmentation-grounded comparisons across
multiple images, particularly at finer granularities such as object parts. In
this paper, we introduce the new task of part-focused semantic co-segmentation,
which seeks to identify and segment common and unique objects and parts across
images. To address this task, we present CALICO, the first LVLM that can
segment and reason over multiple masks across images, enabling object
comparison based on their constituent parts. CALICO features two proposed
components, a novel Correspondence Extraction Module, which captures
semantic-rich information to identify part-level correspondences between
objects, and a Correspondence Adaptation Module, which embeds this information
into the LVLM to facilitate multi-image understanding in a parameter-efficient
manner. To support training and evaluation, we curate MixedParts, a
comprehensive multi-image segmentation dataset containing $\sim$2.4M samples
across $\sim$44K images with diverse object and part categories. Experimental
results show CALICO, finetuned on only 0.3% of its architecture, achieves
robust performance in part-focused semantic co-segmentation.

摘要：最近大型视觉语言模型 (LVLMs) 的进步引发了通过视觉指令微调在通用视觉任务中取得的重大进展。虽然一些工作已经展示了 LVLMs 生成分割掩码的能力，这些掩码将短语与单个图像中的自然语言描述对齐，但它们难以处理跨多个图像的分割基础比较，尤其是在对象部分等更精细的粒度上。在本文中，我们介绍了部分聚焦语义共分割的新任务，该任务旨在识别和分割图像中的常见对象和部分以及唯一的对象和部分。为了解决这一任务，我们提出了 CALICO，这是第一个可以在图像中分割和推理多个掩码的 LVLM，从而能够根据组成部分比较对象。CALICO 具有两个提出的组件，一个新颖的对应提取模块，它捕获语义丰富的信息以识别对象之间的部分级对应关系，以及一个对应适应模块，它将此信息嵌入 LVLM 中，以参数有效的方式促进多图像理解。为了支持训练和评估，我们整理了 MixedParts，这是一个全面的多图像分割数据集，包含跨越约 44K 张图像的约 240 万个样本，其中包含各种对象和部分类别。实验结果表明，仅对其架构的 0.3% 进行微调的 CALICO 在部分聚焦语义共分割中取得了稳健的性能。

##### **Performance Control in Early Exiting to Deploy Large Models at the Same Cost of Smaller Ones**
2412.19325v1 by Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi

Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.

摘要：早期退出 (EE) 是一種很有前景的技術，它能透過根據資料點的難度調整性分配運算資源，來加速推論。此方法能讓預測在較早的層級退出，以處理較簡單的樣本，同時保留更多運算資源來處理有挑戰性的樣本。在此研究中，我們首先提出 EE 方法的新觀點，顯示部署 EE 的較大型模型可以達到比較小型模型更高的效能，同時維持類似的運算成本。由於現有的 EE 方法依賴於每個退出點的信心估計，因此我們進一步研究過度自信對運算效能權衡的可控性影響。我們引入了效能控制早期退出 (PCEE)，這是一種方法，它能透過根據決策（不是根據資料點的信心，而是根據留存驗證集中具有類似信心水準的樣本的平均準確度）來啟用準確度閾值化。在我們的實驗中，我們展示了 PCEE 提供一種簡單但計算效率高的方法，它能比標準的基於信心的方法提供更好的效能控制，並允許我們擴充模型大小以產生效能增益，同時降低運算成本。

##### **From Interets to Insights: An LLM Approach to Course Recommendations Using Natural Language Queries**
2412.19312v1 by Hugh Van Deventer, Mark Mills, August Evrard

Most universities in the United States encourage their students to explore
academic areas before declaring a major and to acquire academic breadth by
satisfying a variety of requirements. Each term, students must choose among
many thousands of offerings, spanning dozens of subject areas, a handful of
courses to take. The curricular environment is also dynamic, and poor
communication and search functions on campus can limit a student's ability to
discover new courses of interest. To support both students and their advisers
in such a setting, we explore a novel Large Language Model (LLM) course
recommendation system that applies a Retrieval Augmented Generation (RAG)
method to the corpus of course descriptions. The system first generates an
'ideal' course description based on the user's query. This description is
converted into a search vector using embeddings, which is then used to find
actual courses with similar content by comparing embedding similarities. We
describe the method and assess the quality and fairness of some example
prompts. Steps to deploy a pilot system on campus are discussed.

摘要：美國的大學多鼓勵學生在選定主修前探索學術領域，並透過滿足各種要求來獲得學術廣度。每學期，學生必須在數十個科目領域的數千門課程中選擇幾門課程來修習。課程環境也具有動態性，校園內不良的溝通和搜尋功能可能會限制學生發現新課程的興趣。為了在這種環境中支援學生和他們的顧問，我們探索了一種新的大語言模型 (LLM) 課程推薦系統，該系統將檢索擴充生成 (RAG) 方法應用於課程描述語料庫。該系統會根據使用者的查詢先產生「理想」的課程描述。此描述會使用嵌入轉換成搜尋向量，然後用於透過比較嵌入相似性來尋找具有類似內容的實際課程。我們描述了此方法，並評估了一些範例提示的品質和公平性。文中也討論了在校園中部署試點系統的步驟。

##### **RAG with Differential Privacy**
2412.19291v1 by Nicolas Grislain

Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.

摘要：檢索增強生成 (RAG) 已成為為 *大型語言模型* (LLM) 提供新鮮且相關的內容的主要技術，可降低產生幻覺的風險，並改善在擁有龐大且快速變動的知識庫環境中回應的整體品質。然而，將外部文件整合到生成過程中會引發重要的隱私問題。的確，當添加到提示中時，無法保證回應不會無意中公開機密數據，導致潛在的隱私侵犯和道德困境。本文探討了一個實用的解決方案，適用於從個人資料中提取一般知識。它顯示 *差分隱私權證生成* 是私人 RAG 的可行方法。

##### **ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning**
2412.19289v1 by Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim

Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.

摘要：近期使用擷取資料的輕量級影像標題模型，主要著重於文字提示。然而，先前的作品僅將擷取的文字作為文字提示使用，而視覺資訊僅依賴於 CLIP 視覺嵌入。由於這個問題，存在一個限制，即提示中固有的影像描述並未充分反映在視覺嵌入空間中。為了解決這個問題，我們提出 ViPCap，這是一種用於輕量級影像標題的新穎擷取文字為基礎的視覺提示。ViPCap 利用擷取的文字和影像資訊作為視覺提示，以增強模型擷取相關視覺資訊的能力。透過將文字提示對應到 CLIP 空間並產生多個隨機的高斯分佈，我們的做法利用取樣來探索隨機擴充的分佈，並有效擷取包含影像資訊的語意特徵。這些擷取的特徵會整合到影像中，並指定為視覺提示，進而提升在 COCO、Flickr30k 和 NoCaps 等資料集上的效能。實驗結果證明，ViPCap 在效率和效能上都明顯優於先前的輕量級標題模型，證明了即插即用解決方案的潛力。

##### **Time Series Foundational Models: Their Role in Anomaly Detection and Prediction**
2412.19286v1 by Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth

Time series foundational models (TSFM) have gained prominence in time series
forecasting, promising state-of-the-art performance across various
applications. However, their application in anomaly detection and prediction
remains underexplored, with growing concerns regarding their black-box nature,
lack of interpretability and applicability. This paper critically evaluates the
efficacy of TSFM in anomaly detection and prediction tasks. We systematically
analyze TSFM across multiple datasets, including those characterized by the
absence of discernible patterns, trends and seasonality. Our analysis shows
that while TSFMs can be extended for anomaly detection and prediction,
traditional statistical and deep learning models often match or outperform TSFM
in these tasks. Additionally, TSFMs require high computational resources but
fail to capture sequential dependencies effectively or improve performance in
few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to
reproduce the results and supplementary materials are available at
https://github.com/smtmnfg/TSFM.

摘要：時間序列基礎模型 (TSFM) 在時間序列預測中獲得顯著地位，承諾在各種應用中提供最先進的效能。然而，它們在異常偵測和預測中的應用仍未被充分探討，而對於它們的黑盒性質、缺乏可解釋性和適用性也越來越令人擔憂。本文批判性地評估了 TSFM 在異常偵測和預測任務中的效能。我們系統性地分析了多個資料集中的 TSFM，包括那些以無法辨識的模式、趨勢和季節性為特徵的資料集。我們的分析顯示，雖然 TSFM 可以延伸用於異常偵測和預測，但傳統的統計和深度學習模型在這些任務中通常與 TSFM 相匹配或表現優於 TSFM。此外，TSFM 需要大量的運算資源，但無法有效捕捉序列依賴性，或在少量樣本或零樣本場景中改善效能。預處理的資料集、重製結果的程式碼和補充材料可在 https://github.com/smtmnfg/TSFM 取得。

##### **PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing**
2412.19284v1 by Michael Bezick, Blake A. Wilson, Vaishnavi Iyer, Yuheng Chen, Vladimir M. Shalaev, Sabre Kais, Alexander V. Kildishev, Alexandra Boltasseva, Brad Lackey

PearSAN is a machine learning-assisted optimization algorithm applicable to
inverse design problems with large design spaces, where traditional optimizers
struggle. The algorithm leverages the latent space of a generative model for
rapid sampling and employs a Pearson correlated surrogate model to predict the
figure of merit of the true design metric. As a showcase example, PearSAN is
applied to thermophotovoltaic (TPV) metasurface design by matching the working
bands between a thermal radiator and a photovoltaic cell. PearSAN can work with
any pretrained generative model with a discretized latent space, making it easy
to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson
correlational loss can be used as both a latent regularization method, similar
to batch and layer normalization, and as a surrogate training loss. We compare
both to previous energy matching losses, which are shown to enforce poor
regularization and performance, even with upgraded affine parameters. PearSAN
achieves a state-of-the-art maximum design efficiency of 97%, and is at least
an order of magnitude faster than previous methods, with an improved maximum
figure-of-merit gain.

摘要：PearSAN 是一種適用於具有廣闊設計空間的逆向設計問題的機器學習輔助最佳化演算法，傳統最佳化器難以處理此類問題。該演算法利用生成模型的潛在空間進行快速取樣，並採用皮爾森相關代理模型來預測真實設計指標的品質因數。作為展示範例，PearSAN 應用於熱光電 (TPV) 超表面設計，藉由匹配熱輻射器和光電電池之間的工作頻段。PearSAN 可與任何具有離散潛在空間的預訓練生成模型搭配使用，因此很容易與 VQ-VAE 和二進位自動編碼器整合。其創新的皮爾森相關損失可用作潛在正則化方法，類似於批次和層標準化，以及代理訓練損失。我們將兩者與先前的能量匹配損失進行比較，結果顯示這些損失即使升級仿射參數，也會強制執行不良的正則化和效能。PearSAN 達到了 97% 的最先進最大設計效率，並且至少比以前的方法快一個數量級，並改善了最大的品質因數增益。

##### **Optimizing Multi-Stage Language Models for Effective Text Retrieval**
2412.19265v1 by Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc

Efficient text retrieval is critical for applications such as legal document
analysis, particularly in specialized contexts like Japanese legal systems.
Existing retrieval methods often underperform in such domain-specific
scenarios, necessitating tailored approaches. In this paper, we introduce a
novel two-phase text retrieval pipeline optimized for Japanese legal datasets.
Our method leverages advanced language models to achieve state-of-the-art
performance, significantly improving retrieval efficiency and accuracy. To
further enhance robustness and adaptability, we incorporate an ensemble model
that integrates multiple retrieval strategies, resulting in superior outcomes
across diverse tasks. Extensive experiments validate the effectiveness of our
approach, demonstrating strong performance on both Japanese legal datasets and
widely recognized benchmarks like MS-MARCO. Our work establishes new standards
for text retrieval in domain-specific and general contexts, providing a
comprehensive solution for addressing complex queries in legal and multilingual
environments.

摘要：對於法律文件分析等應用程式而言，有效率的文字檢索至關重要，特別是在日文法律系統等專業領域。現有的檢索方法在這種特定領域的情境中往往表現不佳，因此需要量身打造的方法。在本文中，我們介紹一個針對日文法律資料集最佳化的創新兩階段文字檢索流程。我們的這項方法利用進階語言模型，以達成最先進的效能，大幅提升檢索效率與準確度。為了進一步提升穩健性和適應性，我們結合了一個整合多重檢索策略的整體模型，在各種任務中都能產生優異的結果。廣泛的實驗驗證了我們方法的有效性，證明了在日文法律資料集和 MS-MARCO 等廣受認可的基準上都有強勁的表現。我們的這項研究為特定領域和一般情境中的文字檢索樹立了新的標準，提供了一個全面的解決方案，以處理法律和多語言環境中的複雜查詢。

##### **MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes**
2412.19260v1 by Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin

Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.

摘要：<paragraph>多項研究顯示，大型語言模型 (LLM) 能正確回答醫療問題，甚至在某些醫療考試中表現優於一般人類的分數。然而，據我們所知，尚未有研究評估語言模型驗證現有或已產生的醫療文本正確性和一致性的能力。在本文中，我們介紹了 MEDEC（https://github.com/abachaa/MEDEC），這是第一個公開可用的臨床筆記中醫學錯誤偵測和更正基準，涵蓋五種類型的錯誤（診斷、管理、治療、藥物治療和致病生物）。MEDEC 包含 3,848 個臨床文本，包括來自三個美國醫院系統的 488 個臨床筆記，這些筆記先前並未被任何 LLM 看到。該資料集已用於 MEDIQA-CORR 共享任務，以評估十七個參與系統 [Ben Abacha et al., 2024]。在本文中，我們描述了資料建立方法，並評估了最近的 LLM（例如，o1-preview、GPT-4、Claude 3.5 Sonnet 和 Gemini 2.0 Flash）在偵測和更正需要醫學知識和推理能力的醫療錯誤的任務上。我們還進行了一項比較研究，其中兩位醫生對 MEDEC 測試集執行了相同的任務。結果顯示，MEDEC 是足夠具有挑戰性的基準，可以評估模型驗證現有或已產生筆記和更正醫療錯誤的能力。我們還發現，儘管最近的 LLM 在錯誤偵測和更正方面有良好的表現，但它們在這些任務中仍不如醫生。我們討論了造成這種差距的潛在因素、我們實驗中的見解、當前評估指標的限制，並分享未來研究的潛在指標。</paragraph>

##### **Multi-matrix Factorization Attention**
2412.19255v1 by Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum

We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.

摘要：<paragraph>我們提出新穎的注意力架構，多矩陣分解注意力 (MFA) 和 MFA 鍵重複使用 (MFA-KR)。現有的標準多頭注意力 (MHA) 變體，包括 MLA 等 SOTA 方法，無法在嚴格的鍵值快取 (KV 快取) 限制下維持強勁的效能。MFA 透過在查詢鍵 (QK) 電路中進行低階矩陣分解，有效地擴充注意力頭的數量和維度，進而增強模型容量。擴充 MFA 後，MFA-KR 進一步透過價值投影重新參數化，將鍵快取重新用作值，進而降低記憶體需求。MFA 的設計在嚴格的 KV 快取預算下，能提供強勁的模型容量，而 MFA-KR 則適用於更嚴苛的 KV 快取限制，效能折衷較小。值得注意的是，在我們廣泛且大規模的實驗中，所提出的架構優於 MLA，且效能與 MHA 相當，同時分別將 KV 快取使用量減少了 56% 和 93.7%。</paragraph>

##### **Leveraging Self-Training and Variational Autoencoder for Agitation Detection in People with Dementia Using Wearable Sensors**
2412.19254v1 by Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan

Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.

摘要：失智症是一種神經退化性疾病，在過去的幾十年中在老年人中不斷增加。這種增長會對患者和照顧者的生活品質產生深遠的影響，因為它會產生症狀。躁動和攻擊性 (AA) 是重度失智症患者 (PwD) 在長期照護或醫院中的症狀之一。AA 不僅會造成不適，還會讓患者或他人面臨潛在風險。現有的監控解決方案利用與人工智慧 (AI) 整合的不同穿戴式感測器，提供一種方法來及早偵測 AA，以便及時且充分地進行醫療介入。然而，大多數研究都受到準確標記資料集可用性的限制，這會顯著影響此類解決方案在實際情況中的效能。本研究提出了一種新穎的綜合方法，使用來自 Empatica E4 腕帶的生理數據來偵測 PwD 中的 AA。該研究建立了一個多元的資料集，由來自加拿大多家醫院的 14 位參與者收集的三個不同資料集組成。由於標記有限，尚未廣泛探討這些資料集。我們提出了一種新穎的方法，採用自我訓練和變異自動編碼器 (VAE) 來有效偵測 PwD 中的 AA。所提出的方法旨在學習使用 VAE 萃取特徵的表示，然後使用半監督區塊來產生標籤、分類事件並偵測 AA。我們證明了將自我訓練和變異自動編碼器機制結合起來，可以顯著提升模型在對 PwD 中的 AA 進行分類時的效能。在測試的技術中，XGBoost 分類器達到了 90.16% 的最高準確度。透過有效地解決標記資料有限的挑戰，所提出的系統不僅學習了新的標籤，還證明了其在偵測 AA 方面的優越性。

##### **Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for Binary Classifiers**
2412.19241v1 by Jason M. Pittman

Machine learning systems increasingly drive innovation across scientific
fields and industry, yet challenges in compute overhead, specifically during
inference, limit their scalability and sustainability. Responsible AI
guardrails, essential for ensuring fairness, transparency, and privacy, further
exacerbate these computational demands. This study addresses critical gaps in
the literature, chiefly the lack of generalized predictive techniques for
latency and energy consumption, limited cross-comparisons of classifiers, and
unquantified impacts of RAI guardrails on inference performance. Using Theory
Construction Methodology, this work constructed a model-agnostic theoretical
framework for predicting latency and energy consumption in binary
classification models during inference. The framework synthesizes classifier
characteristics, dataset properties, and RAI guardrails into a unified
analytical instrument. Two predictive equations are derived that capture the
interplay between these factors while offering generalizability across diverse
classifiers. The proposed framework provides foundational insights for
designing efficient, responsible ML systems. It enables researchers to
benchmark and optimize inference performance and assists practitioners in
deploying scalable solutions. Finally, this work establishes a theoretical
foundation for balancing computational efficiency with ethical AI principles,
paving the way for future empirical validation and broader applications.

摘要：機器學習系統日益推動科學領域和產業的創新，然而在運算開銷方面的挑戰，特別是在推論期間，限制了它們的可擴充性和永續性。負責任的人工智慧防護措施對於確保公平性、透明度和隱私至關重要，進一步加劇了這些運算需求。本研究探討文獻中的關鍵差距，主要是缺乏用於延遲和能源消耗的廣義預測技術、分類器的有限交叉比較，以及 RAI 防護措施對推論效能的未量化影響。使用理論建構方法，本研究構建了一個與模型無關的理論框架，用於預測二元分類模型在推論期間的延遲和能源消耗。該框架將分類器特徵、資料集屬性和 RAI 防護措施綜合到一個統一的分析工具中。推導出兩個預測方程式，它們捕捉這些因素之間的相互作用，同時提供跨不同分類器的可概化性。所提出的框架為設計高效、負責任的機器學習系統提供了基礎見解。它使研究人員能夠基準測試和最佳化推論效能，並協助實務工作者部署可擴充的解決方案。最後，本研究為平衡運算效率與道德 AI 原則建立了一個理論基礎，為未來的實證驗證和更廣泛的應用鋪路。

##### **Learning Cross-Domain Representations for Transferable Drug Perturbations on Single-Cell Transcriptional Responses**
2412.19228v1 by Hui Liu, Shikai Jin

Phenotypic drug discovery has attracted widespread attention because of its
potential to identify bioactive molecules. Transcriptomic profiling provides a
comprehensive reflection of phenotypic changes in cellular responses to
external perturbations. In this paper, we propose XTransferCDR, a novel
generative framework designed for feature decoupling and transferable
representation learning across domains. Given a pair of perturbed expression
profiles, our approach decouples the perturbation representations from basal
states through domain separation encoders and then cross-transfers them in the
latent space. The transferred representations are then used to reconstruct the
corresponding perturbed expression profiles via a shared decoder. This
cross-transfer constraint effectively promotes the learning of transferable
drug perturbation representations. We conducted extensive evaluations of our
model on multiple datasets, including single-cell transcriptional responses to
drugs and single- and combinatorial genetic perturbations. The experimental
results show that XTransferCDR achieved better performance than current
state-of-the-art methods, showcasing its potential to advance phenotypic drug
discovery.

摘要：表型药物发现因其识别生物活性分子的潜力而备受关注。转录组分析全面反映了细胞对外部扰动的表型变化。在本文中，我们提出了 XTransferCDR，这是一种新颖的生成框架，旨在实现跨域特征解耦和可转移表示学习。给定一对扰动表达谱，我们的方法通过域分离编码器将扰动表示从基础状态中解耦，然后在潜在空间中交叉转移它们。然后使用转移的表示通过共享解码器重建相应的扰动表达谱。这种交叉转移约束有效地促进了可转移药物扰动表示的学习。我们对我们的模型在多个数据集上进行了广泛的评估，包括药物的单细胞转录反应以及单一和组合的遗传扰动。实验结果表明，XTransferCDR 的性能优于当前最先进的方法，展示了其推进表型药物发现的潜力。

##### **Optimizing Fantasy Sports Team Selection with Deep Reinforcement Learning**
2412.19215v1 by Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil

Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.

摘要：近年來，奇幻運動，特別是奇幻板球運動，在印度獲得了極大的
歡迎，讓愛好者有機會參與策略性的組隊，並根據職業運動員的實際表現進行競賽。在本文中，我們將探討使用強化學習 (RL) 技術來最佳化奇幻板球隊伍選擇的挑戰。透過將球隊建立過程建構為一個循序漸進的決策制定問題，我們的目標是開發一個模型，能夠適應性地選擇球員以最大化球隊的潛在表現。我們的做法利用歷史球員數據來訓練 RL 演算法，然後預測未來的表現並最佳化球隊組成。這不僅透過更準確地預測高績效球隊而代表著巨大的商機，同時也提升了整體使用者體驗。透過經驗評估和與傳統奇幻球隊選秀方法的比較，我們展示了 RL 在建立具有競爭力的奇幻球隊方面的效能。我們的結果顯示，基於 RL 的策略提供了關於奇幻運動中球員選擇的寶貴見解。

##### **Multi-Attribute Constraint Satisfaction via Language Model Rewriting**
2412.19198v1 by Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap

Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics.

摘要：<paragraph>在多個外部屬性上遵守精確的約束條件是一個常見的計算問題，存在於看似不同的領域中，從受控文本生成到蛋白質工程。現有的語言模型 (LM) 可控性方法，用於多屬性約束滿足，通常依賴於專門的架構或基於梯度的分類器，限制了它們與任意黑盒評估器和預訓練模型一起工作的靈活性。當前的通用大型語言模型雖然有能力，但無法對外部屬性實現細粒度的多屬性控制。因此，我們創建了多屬性約束滿足 (MACS)，這是一種通用方法，能夠在任何序列域上微調語言模型，以滿足用戶在多個外部實值屬性上指定的約束。我們的模型通過從一組初始的改寫輸出中採樣多樣化的多屬性編輯對，將 LMs 訓練為編輯器。在推理過程中，LM 反覆改進其先前的解決方案，以通過利用我們設計的約束滿足獎勵來滿足所有屬性的約束。我們還嘗試使用獎勵加權行為克隆來進一步提高 LMs 的約束滿足率。為了評估我們的做法，我們提出了新的細粒度約束滿足 (FineCS) 基準，其中包括兩項具有挑戰性的任務：(1) 文本樣式轉移，目標是同時修改評論的情感和複雜性，以及 (2) 蛋白質設計，重點是調節綠色螢光蛋白 (GFP) 的螢光和穩定性。我們的實證結果表明，在 FineCS 任務中，MACS 達到了最高的閾值滿足率，優於強大的特定領域基準。我們的研究為廣義和實值的多分屬性控制開闢了新的途徑，對跨越 NLP 和生物信息學的不同應用具有影響。</paragraph>

##### **Provably Efficient Exploration in Reward Machines with Low Regret**
2412.19194v1 by Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi

We study reinforcement learning (RL) for decision processes with
non-Markovian reward, in which high-level knowledge of the task in the form of
reward machines is available to the learner. We consider probabilistic reward
machines with initially unknown dynamics, and investigate RL under the
average-reward criterion, where the learning performance is assessed through
the notion of regret. Our main algorithmic contribution is a model-based RL
algorithm for decision processes involving probabilistic reward machines that
is capable of exploiting the structure induced by such machines. We further
derive high-probability and non-asymptotic bounds on its regret and demonstrate
the gain in terms of regret over existing algorithms that could be applied, but
obliviously to the structure. We also present a regret lower bound for the
studied setting. To the best of our knowledge, the proposed algorithm
constitutes the first attempt to tailor and analyze regret specifically for RL
with probabilistic reward machines.

摘要：我們研究了具有非馬可夫獎勵的決策過程的強化學習 (RL)，其中任務的高階知識以獎勵機器的形式提供給學習者。我們考慮了最初動態未知的機率獎勵機器，並在平均獎勵準則下研究 RL，其中學習效能是透過遺憾的概念來評估。我們的主要演算法貢獻是一種基於模型的 RL 演算法，適用於涉及機率獎勵機器的決策過程，能夠利用此類機器所引發的結構。我們進一步推導出其遺憾的高機率和非漸近界限，並展示出相較於可以應用但忽略結構的現有演算法，在遺憾方面的收益。我們也針對所研究的設定提出遺憾下界。據我們所知，所提出的演算法構成了針對 RL 量身打造並分析遺憾的首次嘗試，特別是對於機率獎勵機器。

##### **Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models**
2412.19191v1 by Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye

Large language models have already demonstrated their formidable capabilities
in general domains, ushering in a revolutionary transformation. However,
exploring and exploiting the extensive knowledge of these models to comprehend
multi-omics biology remains underexplored. To fill this research gap, we first
introduce Biology-Instructions, the first large-scale multi-omics biological
sequences-related instruction-tuning dataset including DNA, RNA, proteins, and
multi-molecules, designed to bridge the gap between large language models
(LLMs) and complex biological sequences-related tasks. This dataset can enhance
the versatility of LLMs by integrating diverse biological sequenced-based
prediction tasks with advanced reasoning capabilities, while maintaining
conversational fluency. Additionally, we reveal significant performance
limitations in even state-of-the-art LLMs on biological sequence-related
multi-omics tasks without specialized pre-training and instruction-tuning. We
further develop a strong baseline called ChatMultiOmics with a novel
three-stage training pipeline, demonstrating the powerful ability to understand
biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics
are publicly available and crucial resources for enabling more effective
integration of LLMs with multi-omics sequence analysis.

摘要：大型語言模型已在一般領域展示其強大的功能，引領了一場革命性的轉型。然而，探索和利用這些模型的廣泛知識來理解多組學生物學仍未得到充分探索。為了填補這一研究空白，我們首先介紹了 Biology-Instructions，這是第一個大規模多組學生物序列相關指令調整數據集，包括 DNA、RNA、蛋白質和多分子，旨在彌合大型語言模型 (LLM) 和複雜生物序列相關任務之間的差距。此數據集可以通過將多樣化的生物序列化預測任務與高級推理能力相結合來增強 LLM 的多功能性，同時保持對話流暢性。此外，我們揭示了即使在沒有專門預訓練和指令調整的情況下，最先進的 LLM 在生物序列相關的多組學任務上也存在顯著的性能限制。我們進一步開發了一個名為 ChatMultiOmics 的強大基線，採用新穎的三階段訓練管道，展示了使用 Biology-Instructions 理解生物學的強大能力。Biology-Instructions 和 ChatMultiOmics 是公開可用的，對於更有效地將 LLM 與多組學序列分析相結合至關重要。

##### **Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching**
2412.19184v1 by Wenjing Chen

With the rapid development of multimodal learning, the image-text matching
task, as a bridge connecting vision and language, has become increasingly
important. Based on existing research, this study proposes an innovative visual
semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic
Embedding (MH-CVSE). This model introduces a multi-head self-attention
mechanism based on the consensus-aware visual semantic embedding model (CVSE)
to capture information in multiple subspaces in parallel, significantly
enhancing the model's ability to understand and represent the complex
relationship between images and texts. In addition, we adopt a parameterized
feature fusion strategy to flexibly integrate feature information at different
levels, further improving the model's expressive power. In terms of loss
function design, the MH-CVSE model adopts a dynamic weight adjustment strategy
to dynamically adjust the weight according to the loss value itself, so that
the model can better balance the contribution of different loss terms during
training. At the same time, we introduce a cosine annealing learning rate
strategy to help the model converge more stably in the later stages of
training. Extensive experimental verification on the Flickr30k dataset shows
that the MH-CVSE model achieves better performance than previous methods in
both bidirectional image and text retrieval tasks, fully demonstrating its
effectiveness and superiority.

摘要：隨著多模態學習的快速發展，作為連接視覺與語言橋樑的圖像文字匹配任務，變得越來越重要。本研究基於現有研究，提出了一種創新的視覺語義嵌入模型，多頭共識感知視覺語義嵌入（MH-CVSE）。該模型在共識感知視覺語義嵌入模型（CVSE）的基礎上，引入多頭自注意力機制，並行捕捉多個子空間中的資訊，大幅提升模型理解和表徵圖像與文字間複雜關係的能力。此外，我們採用參數化的特徵融合策略，靈活融合不同層級的特徵資訊，進一步提升模型的表現能力。在損失函數設計方面，MH-CVSE 模型採用動態權重調整策略，根據損失值本身動態調整權重，使得模型在訓練過程中能更好地平衡不同損失項的貢獻。同時，我們引入餘弦退火學習率策略，幫助模型在訓練後期更穩定地收斂。在 Flickr30k 資料集上的大量實驗驗證表明，MH-CVSE 模型在雙向圖像和文字檢索任務上都取得了優於以往方法的效能，充分展現其有效性和優越性。

##### **Mask Approximation Net: Merging Feature Extraction and Distribution Learning for Remote Sensing Change Captioning**
2412.19179v1 by Dongwei Sun, Xiangyong Cao

Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.

摘要：遙感影像變化描述，作為遙感處理領域中一項新穎的多模態任務，不僅能偵測地表狀況的變化，還能對這些變化提供詳細的描述，從而增強人類的可解釋性和互動性。然而，先前的研究方法主要採用卷積神經網路 (CNN) 架構來提取雙時相影像特徵。此方法通常會過度強調設計特定的網路架構，並將擷取的特徵分佈限制在當前資料集，導致在應用於其他資料集或實際場景時，泛化能力和穩健性不佳。為了解決這些限制，本文提出了一種新穎的遙感影像變化偵測與描述方法，整合擴散模型，旨在將重點從傳統的特徵學習範例轉移到資料分佈學習。所提出的方法主要包含一個簡單的多尺度變化偵測模組，其輸出特徵隨後使用擴散模型進行精煉。此外，我們引入了一個頻率導引複雜濾波器模組，以在擴散過程中處理高頻雜訊，有助於維持模型效能。最後，我們在幾個遙感變化偵測描述資料集上驗證了所提出方法的有效性，證明其優異的效能。程式碼可在 MaskApproxNet 取得。

##### **Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval**
2412.19178v1 by Yang Du, Yuqi Liu, Qin Jin

Cross-modal (e.g. image-text, video-text) retrieval is an important task in
information retrieval and multimodal vision-language understanding field.
Temporal understanding makes video-text retrieval more challenging than
image-text retrieval. However, we find that the widely used video-text
benchmarks have shortcomings in comprehensively assessing abilities of models,
especially in temporal understanding, causing large-scale image-text
pre-trained models can already achieve comparable zero-shot performance with
video-text pre-trained models. In this paper, we introduce RTime, a novel
temporal-emphasized video-text retrieval dataset. We first obtain videos of
actions or events with significant temporality, and then reverse these videos
to create harder negative samples. We then recruit annotators to judge the
significance and reversibility of candidate videos, and write captions for
qualified videos. We further adopt GPT-4 to extend more captions based on
human-written captions. Our RTime dataset currently consists of 21k videos with
10 captions per video, totalling about 122 hours. Based on RTime, we propose
three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We
further enhance the use of harder-negatives in model training, and benchmark a
variety of video-text models on RTime. Extensive experiment analysis proves
that RTime indeed poses new and higher challenges to video-text retrieval. We
release our RTime
dataset\footnote{\url{https://github.com/qyr0403/Reversed-in-Time}} to further
advance video-text retrieval and multimodal understanding research.

摘要：跨模態（例如圖片文字、影片文字）檢索在資訊檢索和多模態視覺語言理解領域中是一項重要的任務。
時間理解讓影片文字檢索比圖片文字檢索更具挑戰性。然而，我們發現廣泛使用的影片文字基準在全面評估模型的能力方面有其不足之處，特別是在時間理解方面，導致大規模的圖片文字預訓練模型已經可以與影片文字預訓練模型達到相當的零次學習效能。在本文中，我們介紹了 RTime，一個新的強調時間的影片文字檢索資料集。我們首先取得具有顯著時間性的動作或事件影片，然後反轉這些影片以建立更困難的負面樣本。接著我們招募註解者來判斷候選影片的重要性和可逆性，並為合格的影片撰寫字幕。我們進一步採用 GPT-4，根據人工撰寫的字幕延伸更多字幕。我們的 RTime 資料集目前包含 21k 個影片，每個影片有 10 個字幕，總計約 122 小時。根據 RTime，我們提出了三項檢索基準任務：RTime-Origin、RTime-Hard 和 RTime-Binary。我們進一步加強在模型訓練中使用更困難的負面樣本，並在 RTime 上評定了各種影片文字模型。廣泛的實驗分析證明，RTime 確實對影片文字檢索提出了新的、更嚴峻的挑戰。我們釋出我們的 RTime 資料集\footnote{\url{https://github.com/qyr0403/Reversed-in-Time}}，以進一步推進影片文字檢索和多模態理解研究。

##### **GFG -- Gender-Fair Generation: A CALAMITA Challenge**
2412.19168v1 by Simona Frenda, Andrea Piergentili, Beatrice Savoldi, Marco Madeddu, Martina Rosola, Silvia Casola, Chiara Ferrando, Viviana Patti, Matteo Negri, Luisa Bentivogli

Gender-fair language aims at promoting gender equality by using terms and
expressions that include all identities and avoid reinforcing gender
stereotypes. Implementing gender-fair strategies is particularly challenging in
heavily gender-marked languages, such as Italian. To address this, the
Gender-Fair Generation challenge intends to help shift toward gender-fair
language in written communication. The challenge, designed to assess and
monitor the recognition and generation of gender-fair language in both mono-
and cross-lingual scenarios, includes three tasks: (1) the detection of
gendered expressions in Italian sentences, (2) the reformulation of gendered
expressions into gender-fair alternatives, and (3) the generation of
gender-fair language in automatic translation from English to Italian. The
challenge relies on three different annotated datasets: the GFL-it corpus,
which contains Italian texts extracted from administrative documents provided
by the University of Brescia; GeNTE, a bilingual test set for gender-neutral
rewriting and translation built upon a subset of the Europarl dataset; and
Neo-GATE, a bilingual test set designed to assess the use of non-binary
neomorphemes in Italian for both fair formulation and translation tasks.
Finally, each task is evaluated with specific metrics: average of F1-score
obtained by means of BERTScore computed on each entry of the datasets for task
1, an accuracy measured with a gender-neutral classifier, and a
coverage-weighted accuracy for tasks 2 and 3.

摘要：性別公平語言旨在透過使用包含所有身分且避免強化性別刻板印象的詞彙和表達方式來促進性別平等。在性別標記嚴重的語言（例如義大利語）中實施性別公平策略特別具有挑戰性。為了解決這個問題，性別公平生成挑戰旨在協助轉向書面溝通中的性別公平語言。該挑戰旨在評估和監控單語和跨語言情境下性別公平語言的辨識和生成，包括三項任務：(1) 義大利語句子中性別化表達式的偵測，(2) 將性別化表達式改寫成性別公平的替代方案，以及 (3) 從英語到義大利語的自動翻譯中生成性別公平語言。該挑戰依賴於三個不同的註解資料集：GFL-it 語料庫，其中包含由布雷西亞大學提供的行政文件所摘錄的義大利語文本；GeNTE，一個基於 Europarl 資料集子集建構的性別中立改寫和翻譯雙語測試集；以及 Neo-GATE，一個雙語測試集，旨在評估在公平表述和翻譯任務中使用義大利語的非二元新語素。最後，每個任務都使用特定的指標進行評估：根據在任務 1 的資料集每個條目上計算的 BERTScore 獲得的 F1 分數平均值、使用性別中立分類器測量的準確度，以及任務 2 和 3 的覆蓋加權準確度。

##### **Referencing Where to Focus: Improving VisualGrounding with Referential Query**
2412.19155v1 by Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang

Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.

摘要：視覺基礎旨在根據自然語言表達式在圖像中定位參考對象。基於 DETR 的視覺基礎方法的最新進展引起了廣泛關注，因為它們直接預測目標對象的坐標，而無需依賴額外的努力，例如預先生成的提議候選或預先定義的錨框。然而，現有研究主要集中於設計更強大的多模式解碼器，它通常通過隨機初始化或使用語言嵌入來生成可學習查詢。這種香草查詢生成方法不可避免地增加了模型的學習難度，因為它在解碼開始時不涉及任何與目標相關的信息。此外，它們在查詢學習過程中只使用最深的圖像特徵，忽視了其他級別特徵的重要性。為了解決這些問題，我們提出了一種新穎的方法，稱為 RefFormer。它由查詢適應模塊組成，該模塊可以無縫集成到 CLIP 中並生成參考查詢，為解碼器提供先驗上下文，以及特定於任務的解碼器。通過將參考查詢納入解碼器，我們可以有效地降低解碼器的學習難度，並準確地集中在目標對象上。此外，我們提出的查詢適應模塊還可以作為適配器，在無需調整主幹網絡參數的情況下保留 CLIP 中的豐富知識。大量的實驗證明了我們提出的方法的有效性和效率，在五個視覺基礎基準上優於最先進的方法。

##### **AskChart: Universal Chart Understanding through Textual Enhancement**
2412.19146v1 by Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo

Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.

摘要：<paragraph>图表理解任务，例如 ChartQA 和 Chart-to-Text，涉及从图表中自动提取和解释关键信息，使用户能够查询或将可视化数据转换为结构化格式。最先进的方法主要关注图表图像中的视觉提示，未能明确纳入图表中嵌入的丰富文本信息（例如，数据标签和轴标签）。这些文本信息对于直观的人类理解和图表解释至关重要。此外，现有的模型通常庞大且计算密集，限制了它们的实际适用性。在本文中，我们介绍了 AskChart，这是一个通用模型，它使用专家混合 (MoE) 架构明确集成了图表中的文本和视觉提示。AskChart 促进了图表增强视觉文本表示的学习，以有效处理多个图表理解任务，同时保持较小的模型大小。为了捕捉视觉和文本模式之间的协同作用，我们整理了一个名为 ChartBank 的大规模数据集，其中包含约 750 万个数据样本，这有助于对齐文本和视觉信息，并促进视觉实体和文本的提取。为了有效地训练 AskChart，我们设计了一个三阶段训练策略，以对齐视觉和文本模式，用于学习鲁棒的视觉文本表示并优化 MoE 层的学习。跨五个数据集的广泛实验展示了 AskChart 在四个图表理解任务中取得的显着性能提升。值得注意的是，拥有 4.6B 参数的 AskChart 在开放式 ChartQA 中比拥有 13B 参数的最先进模型高出 68.3%，在 Chart-to-Text 任务中高出 49.2%，同时在 ChartQA 和 Chart-to-Table 任务中实现了相当的性能。</paragraph>

##### **SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis**
2412.19140v1 by Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng

In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.

摘要：近年來，金融領域的細粒度情緒分析備受關注，但實體層級資料集的稀缺仍然是一個關鍵挑戰。為了解決這個問題，我們構建了迄今為止最大的英文和中文金融實體層級情緒分析資料集。在此基礎上，我們提出了一種新穎的兩階段情緒分析方法，稱為自知脈絡學習校正 (SILC)。第一階段涉及微調基礎大型語言模型，以產生特定於我們任務的偽標籤資料。在第二階段，我們使用 GNN 為基礎的範例檢索器訓練一個校正模型，該模型由偽標籤資料提供資訊。這種兩階段策略使我們能夠在新建的資料集上實現最先進的效能，推動金融情緒分析領域的發展。在一個案例研究中，我們展示了我們的資料和方法在監控加密貨幣市場方面的增強實用性。我們的資料集和程式碼可在 https://github.com/NLP-Bin/SILC-EFSA 上取得。

##### **PlanLLM: Video Procedure Planning with Refinable Large Language Models**
2412.19139v1 by Dejie Yang, Zijing Zhao, YangLiu

Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.

摘要：影片程序規劃，即給定開始和目標狀態的影片畫格，規劃一系列動作步驟，是具象人工智慧的基本能力。最近的作品利用大型語言模型 (LLM) 來產生豐富的動作步驟描述文字，以引導動作步驟解碼。儘管引入了 LLM，這些方法會將動作步驟解碼成封閉集合的一熱向量，限制了模型對新步驟或任務進行概括的能力。此外，基於世界級常理的固定動作步驟描述可能包含特定視覺狀態實例中的雜訊。在本文中，我們提出 PlanLLM，一種具有 LLM 的跨模態聯合學習架構，用於影片程序規劃。我們提出一個 LLM 增強規劃模組，充分利用 LLM 的概括能力來產生自由形式的規劃輸出，並增強動作步驟解碼。我們還提出互資訊最大化模組，將步驟描述的世界級常理與視覺狀態的樣本特定資訊連接起來，使 LLM 能夠運用推理能力來產生步驟序列。在 LLM 的協助下，我們的模型可以同時執行封閉集合和開放詞彙程序規劃任務。我們的 PlanLLM 在三個基準上取得了優異的效能，證明了我們設計的有效性。

##### **Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact**
2412.19124v1 by Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch

Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.

摘要：自我監督學習 (SSL) 已成為醫學影像中一個有前途的範例，用於解決醫療保健環境中標籤資料有限的長期挑戰。雖然 SSL 已展現令人印象深刻的結果，但醫學領域中的現有研究通常範圍有限，專注於特定資料集或方式，或僅評估模型效能的孤立面向。這種片段化的評估方式構成重大挑戰，因為在關鍵醫療環境中部署的模型不僅必須達到高準確度，還必須展現強健的效能和跨不同資料集和不同條件的一般化能力。為了解決這個差距，我們針對醫學領域中的 SSL 方法提出全面的評估，特別著重於強健性和一般化能力。使用 MedMNIST 資料集集合作為標準基準，我們在 11 個不同的醫學資料集上評估 8 種主要的 SSL 方法。我們的研究深入分析了模型在領域內情境和偵測分佈外 (OOD) 樣本中的效能，同時探索各種初始化策略、模型架構和多領域預訓練的影響。我們進一步透過跨資料集評估和在不同標籤比例 (1%、10% 和 100%) 下的領域內效能評估 SSL 方法的一般化能力，以模擬監督有限的真實世界情境。我們希望這個全面的基準能幫助從業人員和研究人員在將 SSL 方法應用於醫療應用時做出更明智的決策。

##### **Discrete vs. Continuous Trade-offs for Generative Models**
2412.19114v1 by Jathin Korrapati, Tanish Baranwal, Rahul Shah

This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.

摘要：這項工作探討了去噪擴散機率模型 (DDPM) 和基於分數的生成模型的理論和實務基礎，這些模型利用隨機過程和布朗運動來模擬複雜的資料分佈。這些模型採用透過隨機微分方程式 (SDE) 定義的正向和反向擴散過程來反覆加入和移除雜訊，進而產生高品質的資料。透過分析這些模型的效能界線，我們展示了分數估計誤差如何透過反向過程傳播，並使用離散 Girsanov 轉換、Pinsker 不等式和資料處理不等式 (DPI) 為資訊理論透鏡設定全變異距離的上限。

##### **SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values**
2412.19113v1 by Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang

Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.

摘要：缺失值是数据科学中的一个关键问题，会显著影响分析和预测的可靠性。缺失值插补 (MVI) 是一个长期存在的问题，因为它高度依赖于领域知识。大型语言模型 (LLM) 已成为一种很有前途的数据清理工具，包括表格数据的 MVI，它为理解和生成内容提供了高级功能。然而，尽管有希望，现有的 LLM 技术（如情境学习和思想链 (CoT)）通常无法指导 LLM 对 MVI 执行复杂的推理，特别是当插补派生的缺失值时，这需要跨行和列的数学公式和数据关系。这一差距凸显了进一步提升 LLM 方法论以增强其推理能力以获得更可靠的插补结果的必要性。为了填补这一空白，我们提出了 SketchFill，这是一种新颖的基于草图的方法，用于指导 LLM 生成准确的公式来插补缺失的数值。我们的实验结果表明，SketchFill 明显优于最先进的方法，比基于 CoT 的方法准确率提高了 56.2%，比 MetaGPT 准确率提高了 78.8%。这为自动数据清理设定了新标准，并推进了数值缺失值 MVI 领域。

##### **Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection**
2412.19108v1 by Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao

Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.

摘要：多變量時間序列 (MTS) 異常偵測是一項重要的任務，涉及識別由多個相互關聯的時間序列組成的資料中的異常模式或事件。為了更好地模擬實體之間複雜的相互依賴性以及每個實體的各種固有特性，現有方法廣泛採用基於 GNN 的方法。在 GNN 的每一層中，節點特徵會彙總來自其鄰近節點的資訊，以更新其資訊。這樣一來，從 GNN 中的淺層到深層，原始的個別節點特徵會持續弱化，而更多的結構資訊（即從短距離鄰域到長距離鄰域）會持續增強。然而，迄今為止的研究在很大程度上忽略了對分層圖形資訊如何表示及其可以使異常偵測受益的特性。現有方法只是利用 GNN 最後一層的輸出進行異常估計，而忽略了中間 GNN 層中包含的重要資訊。為了解決這些限制，在本文中，我們提出了一個圖專家混合 (Graph-MoE) 網路，用於多變量時間序列異常偵測，它結合了專家混合 (MoE) 模組，以自適應地表示和整合分層多層圖形資訊到實體表示中。值得注意的是，我們的 Graph-MoE 可以以即插即用的方式整合到任何基於 GNN 的 MTS 異常偵測方法中。此外，本文提出了記憶增強路由器，以根據 MTS 的整體歷史特徵擷取相關的時間資訊，以自適應地加權獲得的實體表示，以實現成功的異常估計。在五個具有挑戰性的資料集上進行的廣泛實驗證明了我們的方法和每個提出的模組的優越性。

##### **"I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities**
2412.19102v1 by Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su

Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.

摘要：口語化命名實體識別 (NER) 旨在從語音中識別命名實體，在語音處理中扮演重要角色。每天都會出現新的命名實體，然而對其口語化 NER 資料進行註解的成本很高。在本文中，我們證明現有的口語化 NER 系統在處理以前未見過的命名實體時表現不佳。為了應對這個挑戰，我們提出了一種基於命名實體字典 (NED) 來產生口語化 NER 資料的方法，以降低成本。具體來說，我們首先使用大型語言模型 (LLM) 從取樣的命名實體中產生句子，然後使用文字轉語音 (TTS) 系統產生語音。此外，我們引入了一個雜訊指標來過濾雜訊資料。為了評估我們的做法，我們發布了一個新的口語化 NER 基準，以及一個包含 8,853 個實體的對應 NED。實驗結果表明，我們的做法在特定領域、零次學習領域適應和完全零次學習設定中實現了最先進 (SOTA) 的效能。我們的資料將在 https://github.com/DeepLearnXMU/HeardU 上提供。

##### **TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network for Mobility Prediction**
2412.19092v1 by Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu

Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.

摘要：人類流動性研究人們如何移動以取得所需的資源，並在城市規劃和基於位置的服務中扮演重要的角色。作為人類流動性建模的首要任務，下一個位置預測具有挑戰性，因為使用者歷史軌跡的多樣性會產生複雜的流動性模式和各種情境。深度順序模型已被廣泛用於預測下一個位置，方法是利用軌跡資料的固有順序性。然而，它們並未充分利用位置之間的關係，也無法捕捉使用者的多層級偏好。這項工作從使用者的歷史軌跡中建構了一個軌跡圖，並提出一個用於下一個位置預測任務的軌跡圖形增強導向序列網路 (TrajGEOS)。TrajGEOS 導入分層圖形卷積來捕捉位置和使用者嵌入。此類嵌入不僅考慮位置的背景特徵，也考慮它們之間的關係，並作為下游模組中的附加特徵。此外，我們設計了一個基於導向的模組，從順序建模模組和他們最近的軌跡中學習使用者的中期偏好。對三個真實世界 LBSN 資料集進行的廣泛實驗證實了圖形和基於導向的模組的價值，並證明 TrajGEOS 在下一個位置預測任務中優於最先進的方法。

##### **MoPD: Mixture-of-Prompts Distillation for Vision-Language Models**
2412.19087v1 by Yang Chen, Shuai Fu, Yu Zhang

Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.

摘要：軟提示學習方法對於將視覺語言模型 (VLM) 適應到下游任務是有效的。儘管如此，實證證據顯示現有方法的趨勢是過度擬合已見類別，並在未見類別上表現出下降的效能。此限制是訓練資料中對已見類別的固有偏差所致。為了解決此問題，我們提出了一種新穎的軟提示學習方法，稱為提示混合蒸餾 (MoPD)，它可以有效地將人工手動製作的硬提示（又稱教師提示）的有用知識轉移到可學習的軟提示（又稱學生提示），從而增強軟提示在未見類別上的泛化能力。此外，建議的 MoPD 方法利用閘控網路來學習選擇用於提示蒸餾的硬提示。廣泛的實驗證明，建議的 MoPD 方法優於最先進的基準，特別是在未見類別上。

##### **Robust Speech and Natural Language Processing Models for Depression Screening**
2412.19072v1 by Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg

Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.

摘要：憂鬱症是全球性的健康問題，迫切需要加強病患篩檢。語音技術為遠端篩檢提供了優勢，但必須對所有病患表現得穩健。我們已經描述了為此目的而開發的兩個深度學習模型。一個模型基於音響學；另一個基於自然語言處理。兩個模型都採用遷移學習。使用了憂鬱症標記語料庫的資料，其中 11,000 名獨特使用者使用對話式語音與人機應用程式互動。二元憂鬱症分類的結果顯示，兩個模型在未見資料上的表現達到或高於 AUC=0.80，且沒有說話者重疊。進一步分析了效能作為測試子集特性的函數，發現這些模型通常在說話者和會話變數中表現得穩健。我們得出結論，基於這些方法的模型有望進行廣泛的自動化憂鬱症篩檢。

##### **Cross-Demographic Portability of Deep NLP-Based Depression Models**
2412.19070v1 by Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek

Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.

摘要：深度學習模型在行為健康領域的實際應用中正迅速獲得關注。現有文獻的一個重要空白是如何讓此類模型在不同族群中進行推廣。我們研究了基於自然語言處理 (NLP) 的模型，以探索在年齡差異很大的兩個不同語料庫中的可移植性。第一個且較大的語料庫包含較年輕的說話者。它用於訓練 NLP 模型以預測憂鬱症。在對來自相同年齡分佈的未見過說話者進行測試時，此模型的表現為 AUC=0.82。然後我們在第二個語料庫上測試此模型，該語料庫包含退休社區的老年人。儘管兩個語料庫在人口統計上有很大的差異，但我們發現老年語料庫數據的效能僅有輕微的下降，達到 AUC=0.76。有趣的是，在老年族群中，我們發現健康狀態隨著時間推移而保持一致的患者子集的 AUC=0.81。討論了基於語音的應用程式人口統計可移植性的含意。

##### **Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding**
2412.19064v1 by Shenghong He, Chao Yu

Real-time bidding (RTB) plays a pivotal role in online advertising
ecosystems. Advertisers employ strategic bidding to optimize their advertising
impact while adhering to various financial constraints, such as the
return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on
bidding with fixed budget constraints, traditional approaches cannot
effectively manage the dynamic budget allocation problem where the goal is to
achieve global optimization of bidding performance across multiple channels
with a shared budget. In this paper, we propose a hierarchical multi-agent
reinforcement learning framework for multi-channel bidding optimization. In
this framework, the top-level strategy applies a CPC constrained diffusion
model to dynamically allocate budgets among the channels according to their
distinct features and complex interdependencies, while the bottom-level
strategy adopts a state-action decoupled actor-critic method to address the
problem of extrapolation errors in offline learning caused by
out-of-distribution actions and a context-based meta-channel knowledge learning
method to improve the state representation capability of the policy based on
the shared knowledge among different channels. Comprehensive experiments
conducted on a large scale real-world industrial dataset from the Meituan ad
bidding platform demonstrate that our method achieves a state-of-the-art
performance.

摘要：<paragraph>即時競價 (RTB) 在線上廣告生態系統中扮演著舉足輕重的角色。廣告商運用策略性競標，在遵守各種財務限制（例如投資報酬率 (ROI) 和每次點擊費用 (CPC)）的同時，最佳化他們的廣告影響力。傳統方法主要著重於在固定預算限制下競標，無法有效管理動態預算分配問題，而動態預算分配問題的目標是跨多個頻道，在共享預算下達成競標成效的整體最佳化。在本文中，我們提出一個階層式多重代理強化學習架構，用於多頻道競標最佳化。在這個架構中，頂層策略套用一個 CPC 約束擴散模型，根據頻道的不同特徵和複雜的相互依賴性，動態分配預算，而底層策略採用一個狀態動作解耦的動作-評論方法，以解決離線學習中因超出分布動作而產生的外推誤差問題，並採用一個基於脈絡的元頻道知識學習方法，根據不同頻道之間的共享知識，提升政策的狀態表徵能力。在美團廣告競標平台上進行的大規模真實世界產業資料集上執行的綜合實驗，證明了我們的方法達到了最先進的成效。</paragraph>

##### **Indonesian-English Code-Switching Speech Synthesizer Utilizing Multilingual STEN-TTS and Bert LID**
2412.19043v1 by Ahmad Alfani Handoyo, Chung Tran, Dessi Puji Lestari, Sakriani Sakti

Multilingual text-to-speech systems convert text into speech across multiple
languages. In many cases, text sentences may contain segments in different
languages, a phenomenon known as code-switching. This is particularly common in
Indonesia, especially between Indonesian and English. Despite its significance,
no research has yet developed a multilingual TTS system capable of handling
code-switching between these two languages. This study addresses
Indonesian-English code-switching in STEN-TTS. Key modifications include adding
a language identification component to the text-to-phoneme conversion using
finetuned BERT for per-word language identification, as well as removing
language embedding from the base model. Experimental results demonstrate that
the code-switching model achieves superior naturalness and improved speech
intelligibility compared to the Indonesian and English baseline STEN-TTS
models.

摘要：多國語言文字轉語音系統會將文字轉換成跨多種語言的語音。在許多情況下，文字句子可能包含不同語言的區塊，這稱為代碼轉換。這種現象在印尼特別常見，特別是在印尼語和英語之間。儘管其重要性，但目前尚未有研究開發出能夠處理這兩種語言之間代碼轉換的多國語言 TTS 系統。本研究探討 STEN-TTS 中的印尼語-英語代碼轉換。主要的修改包括在文字轉音素轉換中加入語言辨識元件，使用微調 BERT 進行每個字詞的語言辨識，以及從基礎模型中移除語言嵌入。實驗結果證明，與印尼語和英語基準 STEN-TTS 模型相比，代碼轉換模型達到了更優異的自然度和更佳的語音清晰度。

##### **CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers**
2412.19037v1 by Jingyi Zheng, Tianyi Hu, Tianshuo Cong, Xinlei He

Backdoor attacks significantly compromise the security of large language
models by triggering them to output specific and controlled content. Currently,
triggers for textual backdoor attacks fall into two categories: fixed-token
triggers and sentence-pattern triggers. However, the former are typically easy
to identify and filter, while the latter, such as syntax and style, do not
apply to all original samples and may lead to semantic shifts. In this paper,
inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we
propose a higher-dimensional trigger method at the paragraph level, namely
CL-attack. CL-attack injects the backdoor by using texts with specific
structures that incorporate multiple languages, thereby offering greater
stealthiness and universality compared to existing backdoor attack techniques.
Extensive experiments on different tasks and model architectures demonstrate
that CL-attack can achieve nearly 100% attack success rate with a low poisoning
rate in both classification and generation tasks. We also empirically show that
the CL-attack is more robust against current major defense methods compared to
baseline backdoor attacks. Additionally, to mitigate CL-attack, we further
develop a new defense called TranslateDefense, which can partially mitigate the
impact of CL-attack.

摘要：後門攻擊會觸發大型語言模型輸出特定且受控的內容，因而大幅危害其安全性。目前，文字後門攻擊的觸發器分為兩類：固定代碼觸發器和句子模式觸發器。然而，前者通常容易辨識和過濾，而後者（例如句法和風格）並不適用於所有原始範例，且可能導致語義轉換。在本文中，我們受到現實世界中 LLM 的跨語言 (CL) 提示啟發，提出了一個更高維度的段落級別觸發方法，即 CL 攻擊。CL 攻擊透過使用包含多種語言的特定結構文字來注入後門，從而提供比現有後門攻擊技術更強的隱蔽性和普遍性。針對不同任務和模型架構進行的廣泛實驗證明，CL 攻擊在分類和生成任務中都能達到近 100% 的攻擊成功率，且中毒率低。我們也透過實證顯示，與基準後門攻擊相比，CL 攻擊對於目前主要的防禦方法具有更強健的抵抗力。此外，為了減輕 CL 攻擊，我們進一步開發了一種名為 TranslateDefense 的新防禦，可以部分減輕 CL 攻擊的影響。

##### **Repository Structure-Aware Training Makes SLMs Better Issue Resolver**
2412.19031v1 by Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie

Language models have been applied to various software development tasks, but
the performance varies according to the scale of the models. Large Language
Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like
repository-level issue resolving, but raise concerns about privacy and cost. In
contrast, SLMs are more accessible but under-perform in complex tasks. In this
paper, we introduce ReSAT (Repository Structure-Aware Training), construct
training data based on a large number of issues and corresponding pull requests
from open-source communities to enhance the model's understanding of repository
structure and issue resolving ability. We construct two types of training data:
(1) localization training data, a multi-level progressive localization data to
improve code understanding and localization capability; (2) code edit training
data, which improves context-based code editing capability. The evaluation
results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively
enhances SLMs' issue-resolving and repository-level long-context understanding
capabilities.

摘要：語言模型已應用於各種軟體開發任務，但效能會根據模型的規模而有所不同。大型語言模型 (LLM) 在複雜任務中優於小型語言模型 (SLM)，例如儲存庫層級問題解決，但引發了對隱私和成本的擔憂。相反地，SLM 更容易取得，但在複雜任務中的表現較差。在本文中，我們介紹 ReSAT（儲存庫結構感知訓練），根據大量問題和來自開源社群的對應拉取請求建構訓練資料，以增強模型對儲存庫結構和問題解決能力的理解。我們建構了兩種類型的訓練資料：(1) 本地化訓練資料，一種多層級漸進式本地化資料，以改善程式碼理解和本地化能力；(2) 程式碼編輯訓練資料，可改善基於內容的程式碼編輯能力。在 SWE-Bench 驗證和 RepoQA 上的評估結果證明，ReSAT 有效地增強了 SLM 的問題解決和儲存庫層級長內容理解能力。

##### **Modality-Projection Universal Model for Comprehensive Full-Body Medical Imaging Segmentation**
2412.19026v1 by Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie

The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.

摘要：深度學習在醫學影像中的整合已展現出極大的前景，用於增強診斷、治療和研究成果。然而，由於資料特性的內在變異性，在多種方式中應用通用模型仍然具有挑戰性。本研究旨在介紹和評估方式投影通用模型 (MPUM)。MPUM 採用一種新穎的方式投影策略，使模型能夠動態調整其參數，以優化不同影像方式的效能。MPUM 在識別解剖結構方面表現出優異的準確性，能夠進行精確量化，以改善臨床決策制定。它還識別出腦體軸內的代謝關聯，推動了對腦體生理相關性的研究。此外，MPUM 獨特的基於控制器的卷積層能夠視覺化所有網路層的顯著性圖，顯著增強了模型的可解釋性。

##### **Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph Generation**
2412.19021v1 by Tao Liu, Rongjie Li, Chongyu Wang, Xuming He

Open-vocabulary Scene Graph Generation (OV-SGG) overcomes the limitations of
the closed-set assumption by aligning visual relationship representations with
open-vocabulary textual representations. This enables the identification of
novel visual relationships, making it applicable to real-world scenarios with
diverse relationships. However, existing OV-SGG methods are constrained by
fixed text representations, limiting diversity and accuracy in image-text
alignment. To address these challenges, we propose the Relation-Aware
Hierarchical Prompting (RAHP) framework, which enhances text representation by
integrating subject-object and region-specific relation information. Our
approach utilizes entity clustering to address the complexity of relation
triplet categories, enabling the effective integration of subject-object
information. Additionally, we utilize a large language model (LLM) to generate
detailed region-aware prompts, capturing fine-grained visual interactions and
improving alignment between visual and textual modalities. RAHP also introduces
a dynamic selection mechanism within Vision-Language Models (VLMs), which
adaptively selects relevant text prompts based on the visual content, reducing
noise from irrelevant prompts. Extensive experiments on the Visual Genome and
Open Images v6 datasets demonstrate that our framework consistently achieves
state-of-the-art performance, demonstrating its effectiveness in addressing the
challenges of open-vocabulary scene graph generation.

摘要：開放詞彙場景圖生成 (OV-SGG) 克服了封閉式假設的限制，透過將視覺關係表徵與開放詞彙文本表徵對齊。這使得能夠識別新的視覺關係，使其適用於具有多樣化關係的真實世界場景。然而，現有的 OV-SGG 方法受到固定文本表徵的限制，限制了圖像文本對齊的多樣性和準確性。為了應對這些挑戰，我們提出了關係感知階層式提示 (RAHP) 架構，透過整合主體客體和特定區域的關係資訊來增強文本表徵。我們的做法利用實體聚類來解決關係三元組類別的複雜性，使主體客體資訊能夠有效整合。此外，我們利用大型語言模型 (LLM) 來產生詳細的區域感知提示，捕捉細微的視覺互動並改善視覺和文本模式之間的對齊。RAHP 也在視覺語言模型 (VLM) 中引入了動態選擇機制，根據視覺內容自適應地選擇相關文本提示，減少不相關提示的雜訊。在 Visual Genome 和 Open Images v6 資料集上的大量實驗證明，我們的架構持續達成最先進的效能，證明其在解決開放詞彙場景圖生成的挑戰上具有效能。

##### **Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability**
2412.19018v1 by Ruixi Lin, Yang You

In-context learning, which allows large language models to perform diverse
tasks with a few demonstrations, is found to have imbalanced per-class
prediction accuracy on multi-class text classification. Although notable output
correction methods have been developed to tackle the issue and simultaneously
improve downstream prediction accuracy, they may fail to answer the core
interpretability challenges: why and which certain classes need corrections,
and more importantly, a tailored correction for per-sample, per-class's
probability. To address such interpretability gaps, we first find that the
imbalance arises from certain classes consistently receiving high ICL output
probabilities, whereas others receiving lower or mixed ranges, so the former is
more frequently chosen, resulting in higher accuracy; more crucially, we find
that these ranges have significantly varying degrees of influence on the
accuracy bias, highlighting the need for precise, interpretable probability
corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule
Optimization based Debiasing method, that (1) detects which classes need
corrections, and (2) for each correction-needed class, detects its probability
ranges and applies asymmetric amplifications or reductions to correct them
interpretably. Notably, across seven benchmark datasets, FuRud reduces the
pairwise class accuracy bias (COBias) by more than half (56%), while achieving
a relative increase of 21% in accuracy, outperforming state-of-the-art
debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as
10 optimization examples. Furthermore, FuRud can work for prompt formats that
lead to highly skewed predictions. For example, FuRud greatly improves ICL
outputs which use letter options, with 44% relative accuracy increase and 54%
relative COBias reduction.

摘要：<paragraph>語境學習讓大型語言模型能透過少數示範執行多樣化的任務，但發現其在多類別文字分類上，每個類別的預測準確度不平衡。儘管已開發出顯著的輸出修正方法來解決此問題，並同時提升下游預測準確度，但它們可能無法回答核心可解釋性挑戰：為何且哪些特定類別需要修正，更重要的是，針對每個樣本、每個類別的機率進行客製化修正。為了解決此類可解釋性差距，我們首先發現不平衡來自於某些類別持續接收高 ICL 輸出機率，而其他類別接收較低或混合範圍，所以前者較常被選取，導致較高準確度；更重要的是，我們發現這些範圍對準確度偏差有顯著不同的影響程度，突顯了依範圍進行精確、可解釋機率修正的必要性。基於此，我們提出 FuRud，一種基於模糊規則最佳化的去偏誤方法，它 (1) 偵測哪些類別需要修正，以及 (2) 針對每個需要修正的類別，偵測其機率範圍並應用非對稱放大或縮小以進行可解釋的修正。值得注意的是，在七個基準資料集上，FuRud 將成對類別準確度偏差 (COBias) 減少了一半以上 (56%)，同時準確度相對增加了 21%，優於最先進的去偏誤方法。此外，FuRud 可以使用少至 10 個最佳化範例最佳化下游任務。此外，FuRud 可以處理導致高度偏斜預測的提示格式。例如，FuRud 大幅改善使用字母選項的 ICL 輸出，相對準確度提升 44%，相對 COBias 減少 54%。</paragraph>

##### **Brain Ageing Prediction using Isolation Forest Technique and Residual Neural Network (ResNet)**
2412.19017v1 by Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi

Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.

摘要：大腦老化是一個複雜且動態的過程，導致大腦的功能性和結構性變化。這些變化可能導致神經退化性疾病和認知能力下降的風險增加。利用神經影像數據進行準確的大腦年齡估計對於檢測神經退化的初期徵兆變得必要。在此，我們提出了一種使用殘差神經網路 101 版本 2（ResNet101V2）模型的深度學習新方法，以從 MRI 掃描中預測大腦年齡。為了訓練、驗證和測試我們提出的模型，我們使用了一個大型數據集，其中 2102 張影像隨機選自國際大腦繪製聯盟（ICBM）。接下來，我們應用數據預處理技術，包括對影像進行正規化，並使用孤立森林方法進行異常值檢測。然後，我們評估了各種預訓練方法（即：MobileNetV2、ResNet50V2、ResNet101V2、Xception）。結果表明，與其他模型相比，ResNet101V2 模型具有更高的性能，在使用孤立森林處理之前和之後，平均絕對誤差分別為 0.9136 年和 0.8242 年。我們的模型在 ICBM 數據集中實現了大腦年齡估計的高準確性，並且它提供了可靠的大腦年齡預測。

##### **Enhancing Audiovisual Speech Recognition through Bifocal Preference Optimization**
2412.19005v1 by Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe

Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.

摘要：視聽自動語音辨識 (AV-ASR) 旨在透過利用視覺訊號來提升語音辨識的準確度。由於有噪音的聲學環境、自發性語音和視覺資訊的不確定使用，因此在跨越各種領域的無約束真實世界場景中，這項技術特別具有挑戰性。大多數先前的工作都在視聽資料集上微調僅音訊的 ASR 模型，針對傳統 ASR 目標進行最佳化。然而，它們經常忽略視覺特徵和無約束影片場景中的常見錯誤。在本文中，我們建議使用偏好最佳化策略來提升真實世界影片的語音辨識準確度。首先，我們透過模擬 AV-ASR 中發生的常見錯誤來建立偏好資料，從兩個焦點著手：操作音訊或視覺輸入，以及改寫輸出文字稿。其次，我們提出 BPO-AVASR，一種雙焦點偏好最佳化方法，透過利用輸入端和輸出端的偏好來提升 AV-ASR 模型。廣泛的實驗證明，我們的做法大幅提升了跨各種領域的語音辨識準確度，在真實世界影片語音辨識方面優於先前的最先進模型。

##### **Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with AI for Enhanced Urban Mapping**
2412.18994v1 by Sajjad Afroosheh, Mohammadreza Askari

This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.

摘要：這項研究探討了透過先進的人工智慧技術整合光達、合成孔徑雷達 (SAR) 和光學影像以增強城市製圖。透過融合這些多元的地理空間資料集，我們旨在克服與單一感測器資料相關的限制，達成更全面的城市環境呈現。這項研究採用全卷積網路 (FCN) 作為主要的深度學習模型，用於城市特徵萃取，能夠精確地對每個像素進行分類，包括建築物、道路和植被等重要的城市元素。為了最佳化 FCN 模型的效能，我們利用粒子群最佳化 (PSO) 進行超參數調整，大幅提升模型的準確度。主要的發現指出，FCN-PSO 模型達到了 92.3% 的像素準確度和 87.6% 的平均聯集比交集 (IoU)，超越了傳統的單一感測器方法。這些結果強調了融合地理空間資料和人工智慧驅動方法在城市製圖中的潛力，為城市規劃和管理提供了有價值的見解。這項研究的影響為實時製圖和適應性城市基礎設施規劃的未來發展鋪路。

##### **How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study**
2412.18989v1 by Alejandro Velasco, Daniel Rodriguez-Cardenas, David N. Palacio, Luftar Rahman Alif, Denys Poshyvanyk

Large Language Models (LLMs) have shown significant potential in automating
software engineering tasks, particularly in code generation. However, current
evaluation benchmarks, which primarily focus on accuracy, fall short in
assessing the quality of the code generated by these models, specifically their
tendency to produce code smells. To address this limitation, we introduce
CodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for
generating code smells. Our benchmark includes a novel metric: Propensity
Smelly Score (PSC), and a curated dataset of method-level code smells:
CodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case
study with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal
that both models tend to generate code smells, such as simplifiable-condition
and consider-merging-isinstance. These findings highlight the effectiveness of
our benchmark in evaluating LLMs, providing valuable insights into their
reliability and their propensity to introduce code smells in code generation
tasks.

摘要：大型語言模型 (LLM) 在自動化軟體工程任務，特別是在程式碼生成方面，展現了顯著的潛力。然而，目前主要著重於準確性的評估基準，在評估這些模型所產生的程式碼品質方面有所不足，特別是它們產生程式碼異味的傾向。為了解決這個限制，我們引入了 CodeSmellEval，一個專門用於評估 LLM 產生程式碼異味的傾向的基準。我們的基準包含一個新穎的指標：傾向異味分數 (PSC)，以及一個經過整理的函式級別程式碼異味資料集：CodeSmellData。為了展示 CodeSmellEval 的用法，我們針對兩個最先進的 LLM，CodeLlama 和 Mistral，進行了一個案例研究。結果顯示，這兩個模型都傾向於產生程式碼異味，例如簡化條件和考慮合併 isinstance。這些發現突顯了我們基準在評估 LLM 時的有效性，並提供了有價值的見解，了解它們在程式碼生成任務中引入程式碼異味的可靠性和傾向。

##### **TravelAgent: Generative Agents in the Built Environment**
2412.18985v1 by Ariel Noyman, Kai Hu, Kent Larson

Understanding human behavior in built environments is critical for designing
functional, user centered urban spaces. Traditional approaches, such as manual
observations, surveys, and simplified simulations, often fail to capture the
complexity and dynamics of real world behavior. To address these limitations,
we introduce TravelAgent, a novel simulation platform that models pedestrian
navigation and activity patterns across diverse indoor and outdoor environments
under varying contextual and environmental conditions. TravelAgent leverages
generative agents integrated into 3D virtual environments, enabling agents to
process multimodal sensory inputs and exhibit human-like decision-making,
behavior, and adaptation. Through experiments, including navigation,
wayfinding, and free exploration, we analyze data from 100 simulations
comprising 1898 agent steps across diverse spatial layouts and agent
archetypes, achieving an overall task completion rate of 76%. Using spatial,
linguistic, and sentiment analyses, we show how agents perceive, adapt to, or
struggle with their surroundings and assigned tasks. Our findings highlight the
potential of TravelAgent as a tool for urban design, spatial cognition
research, and agent-based modeling. We discuss key challenges and opportunities
in deploying generative agents for the evaluation and refinement of spatial
designs, proposing TravelAgent as a new paradigm for simulating and
understanding human experiences in built environments.

摘要：了解建築環境中的人類行為對於設計以使用者為中心的城市空間至關重要。傳統方法，例如手動觀察、調查和簡化模擬，通常無法捕捉現實世界行為的複雜性和動態性。為了解決這些限制，我們引入了 TravelAgent，這是一個新穎的模擬平台，用於模擬行人導航和活動模式，適用於各種室內和室外環境，以及不同的背景和環境條件。TravelAgent 利用整合到 3D 虛擬環境中的生成式代理，使代理能夠處理多模式感官輸入，並表現出類人的決策制定、行為和適應能力。通過包括導航、尋路和自由探索在內的實驗，我們分析了 100 個模擬的數據，這些模擬包含 1898 個代理步驟，涉及不同的空間佈局和代理原型，實現了 76% 的整體任務完成率。通過空間、語言和情緒分析，我們展示了代理如何感知、適應或與周圍環境和分配的任務作鬥爭。我們的發現突出了 TravelAgent 作為城市設計、空間認知研究和基於代理的建模工具的潛力。我們討論了在部署生成式代理以評估和改進空間設計時面臨的主要挑戰和機遇，並提出 TravelAgent 作為模擬和理解建築環境中人類體驗的新範例。

##### **Injecting Bias into Text Classification Models using Backdoor Attacks**
2412.18975v1 by A. Dilara Yavuz, M. Emre Gursoy

The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with >= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.

摘要：自然語言處理 (NLP) 和預先訓練語言模型的快速發展，已能讓各種設定下準確地進行文字分類。然而，文字分類模型容易受到後門攻擊，攻擊者會在受害者模型中嵌入觸發器，讓模型在目標情境中預測攻擊者想要的標籤。在本文中，我們提議將後門攻擊用於一個新目的：偏差注入。我們發展了一種後門攻擊，其中訓練資料集的子集被下毒，將強勢男性演員與負面情緒聯繫在一起。我們對兩個流行的文字分類資料集 (IMDb 和 SST) 和七個不同的模型執行我們的攻擊，這些模型從傳統的 Doc2Vec 模型到 LSTM 網路，以及現代的基於 transformer 的 BERT 和 RoBERTa 模型。我們的結果顯示，後門模型的良性分類準確度下降有限，這表示我們的攻擊仍然隱蔽，而模型成功學會將強勢男性演員與負面情緒聯繫在一起（攻擊成功率 100%，中毒率 >= 3%）。對 BERT 和 RoBERTa 的攻擊特別隱蔽且有效，顯示出使用現代化且較大模型的風險增加。我們也透過提出兩個指標來衡量我們的偏差注入的概括性：(i) U-BBSR，在衡量攻擊成功時使用先前未見過的字詞，以及 (ii) P-BBSR，使用改寫的測試樣本來衡量攻擊成功。U-BBSR 和 P-BBSR 的結果顯示，我們的攻擊注入的偏差可以超越記憶觸發詞組。

