{"2412.11965": {"publish_time": "2024-12-16", "title": "Inferring Functionality of Attention Heads from their Parameters", "paper_summary": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.", "paper_summary_zh": "\u6ce8\u610f\u529b\u982d\u90e8\u662f\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\u4e4b\u4e00\u3002\u5148\u524d\u91dd\u5c0d\u5176\u904b\u4f5c\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u6790\u5176\u5728\u7279\u5b9a\u96fb\u8def\u6216\u4efb\u52d9\u7684\u63a8\u7406\u671f\u9593\u7684\u884c\u70ba\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c0b\u6c42\u5168\u9762\u5730\u5c0d\u6a21\u578b\u4e2d\u5be6\u4f5c\u7684\u904b\u7b97\u9032\u884c\u5c0d\u61c9\u3002\u6211\u5011\u63d0\u51fa MAPS (\u5c0d\u61c9\u6ce8\u610f\u529b\u982d\u90e8\u53c3\u6578)\uff0c\u4e00\u7a2e\u6709\u6548\u7387\u7684\u67b6\u69cb\uff0c\u53ef\u4ee5\u5f9e\u5176\u53c3\u6578\u63a8\u8ad6\u6ce8\u610f\u529b\u982d\u90e8\u7684\u529f\u80fd\uff0c\u800c\u7121\u9700\u4efb\u4f55\u6a21\u578b\u8a13\u7df4\u6216\u63a8\u7406\u3002\u6211\u5011\u5c55\u793a\u4e86 MAPS \u5728\u56de\u7b54\u5169\u7a2e\u554f\u984c\u6642\u7684\u6548\u7528\uff1a(a) \u7d66\u5b9a\u4e00\u500b\u9810\u5b9a\u7fa9\u7684\u904b\u7b97\uff0c\u5c0d\u61c9\u6a21\u578b\u4e2d\u982d\u90e8\u5be6\u4f5c\u5b83\u7684\u5f37\u5ea6\uff0c\u4ee5\u53ca (b) \u7d66\u5b9a\u4e00\u500b\u6ce8\u610f\u529b\u982d\u90e8\uff0c\u63a8\u8ad6\u5176\u986f\u8457\u529f\u80fd\u3002\u5728 6 \u500b\u71b1\u9580 LLM \u4e2d\u5c0d 20 \u500b\u904b\u7b97\u8a55\u4f30 MAPS\uff0c\u986f\u793a\u5176\u4f30\u8a08\u503c\u8207\u63a8\u7406\u671f\u9593\u982d\u90e8\u7684\u8f38\u51fa\u76f8\u95dc\uff0c\u4e26\u4e14\u8207\u6a21\u578b\u7684\u9810\u6e2c\u6709\u56e0\u679c\u95dc\u4fc2\u3002\u6b64\u5916\uff0c\u5176\u5c0d\u61c9\u63ed\u793a\u4e86\u5148\u524d\u7814\u7a76\u4e2d\u88ab\u5ffd\u7565\u7684\u7279\u5b9a\u904b\u7b97\u7684\u6ce8\u610f\u529b\u982d\u90e8\uff0c\u4ee5\u53ca\u95dc\u65bc LLM \u4e2d\u529f\u80fd\u901a\u7528\u6027\u548c\u67b6\u69cb\u504f\u5dee\u7684\u5bf6\u8cb4\u898b\u89e3\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u81ea\u52d5\u5316\u7ba1\u9053\u548c\u5206\u6790\uff0c\u5229\u7528 MAPS \u4f86\u63cf\u8ff0\u7d66\u5b9a\u982d\u90e8\u7684\u986f\u8457\u904b\u7b97\u3002\u6211\u5011\u7684\u7ba1\u9053\u6703\u7522\u751f\u5927\u591a\u6578\u982d\u90e8\u5408\u7406\u7684\u904b\u7b97\u63cf\u8ff0\uff0c\u7d93\u7531\u4eba\u985e\u5224\u65b7\u8a55\u4f30\uff0c\u540c\u6642\u63ed\u793a\u4e0d\u540c\u7684\u904b\u7b97\u3002", "author": "Amit Elhelo et.al.", "authors": "Amit Elhelo, Mor Geva", "id": "2412.11965v1", "paper_url": "http://arxiv.org/abs/2412.11965v1", "repo": "https://github.com/amitelhelo/maps"}}