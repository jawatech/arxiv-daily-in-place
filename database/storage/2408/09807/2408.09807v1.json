{"2408.09807": {"publish_time": "2024-08-19", "title": "World Models Increase Autonomy in Reinforcement Learning", "paper_summary": "Reinforcement learning (RL) is an appealing paradigm for training intelligent\nagents, enabling policy acquisition from the agent's own autonomously acquired\nexperience. However, the training process of RL is far from automatic,\nrequiring extensive human effort to reset the agent and environments. To tackle\nthe challenging reset-free setting, we first demonstrate the superiority of\nmodel-based (MB) RL methods in such setting, showing that a straightforward\nadaptation of MBRL can outperform all the prior state-of-the-art methods while\nrequiring less supervision. We then identify limitations inherent to this\ndirect extension and propose a solution called model-based reset-free\n(MoReFree) agent, which further enhances the performance. MoReFree adapts two\nkey mechanisms, exploration and policy learning, to handle reset-free tasks by\nprioritizing task-relevant states. It exhibits superior data-efficiency across\nvarious reset-free tasks without access to environmental reward or\ndemonstrations while significantly outperforming privileged baselines that\nrequire supervision. Our findings suggest model-based methods hold significant\npromise for reducing human effort in RL. Website:\nhttps://sites.google.com/view/morefree", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u4e00\u7a2e\u8a13\u7df4\u667a\u6167\u578b\u4ee3\u7406\u7684\u8a98\u4eba\u7bc4\u4f8b\uff0c\u5b83\u80fd\u5f9e\u4ee3\u7406\u672c\u8eab\u81ea\u4e3b\u7372\u5f97\u7684\u7d93\u9a57\u4e2d\u7372\u53d6\u653f\u7b56\u3002\u7136\u800c\uff0cRL \u7684\u8a13\u7df4\u904e\u7a0b\u9060\u975e\u81ea\u52d5\u5316\uff0c\u9700\u8981\u5927\u91cf\u4eba\u529b\u4f86\u91cd\u8a2d\u4ee3\u7406\u548c\u74b0\u5883\u3002\u70ba\u4e86\u61c9\u5c0d\u5177\u6709\u6311\u6230\u6027\u7684\u514d\u91cd\u8a2d\u8a2d\u5b9a\uff0c\u6211\u5011\u9996\u5148\u5c55\u793a\u4e86\u57fa\u65bc\u6a21\u578b (MB) \u7684 RL \u65b9\u6cd5\u5728\u9019\u7a2e\u8a2d\u5b9a\u4e2d\u7684\u512a\u8d8a\u6027\uff0c\u8868\u660e MBRL \u7684\u76f4\u63a5\u9069\u61c9\u53ef\u4ee5\u512a\u65bc\u6240\u6709\u5148\u524d\u7684\u6700\u5148\u9032\u65b9\u6cd5\uff0c\u540c\u6642\u9700\u8981\u8f03\u5c11\u7684\u76e3\u7763\u3002\u7136\u5f8c\uff0c\u6211\u5011\u627e\u51fa\u6b64\u76f4\u63a5\u64f4\u5145\u56fa\u6709\u7684\u9650\u5236\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u7a31\u70ba\u57fa\u65bc\u6a21\u578b\u7684\u514d\u91cd\u8a2d (MoReFree) \u4ee3\u7406\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u9032\u4e00\u6b65\u589e\u5f37\u6548\u80fd\u3002MoReFree \u63a1\u7528\u63a2\u7d22\u548c\u653f\u7b56\u5b78\u7fd2\u9019\u5169\u500b\u95dc\u9375\u6a5f\u5236\uff0c\u900f\u904e\u512a\u5148\u8655\u7406\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u72c0\u614b\u4f86\u8655\u7406\u514d\u91cd\u8a2d\u4efb\u52d9\u3002\u5b83\u5728\u5404\u7a2e\u514d\u91cd\u8a2d\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u512a\u7570\u7684\u8cc7\u6599\u6548\u7387\uff0c\u7121\u9700\u5b58\u53d6\u74b0\u5883\u56de\u994b\u6216\u793a\u7bc4\uff0c\u540c\u6642\u986f\u8457\u512a\u65bc\u9700\u8981\u76e3\u7763\u7684\u7279\u6b0a\u57fa\u7dda\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\uff0c\u57fa\u65bc\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6e1b\u5c11 RL \u4e2d\u7684\u4eba\u529b\u65b9\u9762\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\u3002\u7db2\u7ad9\uff1a\nhttps://sites.google.com/view/morefree", "author": "Zhao Yang et.al.", "authors": "Zhao Yang, Thomas M. Moerland, Mike Preuss, Edward S. Hu", "id": "2408.09807v1", "paper_url": "http://arxiv.org/abs/2408.09807v1", "repo": "null"}}