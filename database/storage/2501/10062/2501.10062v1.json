{"2501.10062": {"publish_time": "2025-01-17", "title": "OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning", "paper_summary": "Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA)\nis emerging as a potential direction in parameter-efficient fine-tuning (PEFT)\nfor its modular design and remarkable performance. However, simply stacking the\nnumber of experts cannot guarantee significant improvement. In this work, we\nfirst conduct qualitative analysis to indicate that experts collapse to similar\nrepresentations in vanilla MoE, limiting the capacity of modular design and\ncomputational efficiency. Ulteriorly, Our analysis reveals that the performance\nof previous MoE variants maybe limited by a lack of diversity among experts.\nMotivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a\nresource-efficient MoE variant that trains experts in an orthogonal manner to\npromote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that\nthe experts' representations lie within the Stiefel manifold. By applying\northogonal constraints directly to the architecture, OMoE keeps the learning\nobjective unchanged, without compromising optimality. Our method is simple and\nalleviates memory bottlenecks, as it incurs minimal experts compared to vanilla\nMoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate\nthat OMoE can consistently achieve stable and efficient performance improvement\nwhen compared with the state-of-the-art methods while significantly reducing\nthe number of required experts.", "paper_summary_zh": "<paragraph>\u5efa\u7acb\u6df7\u5408\u4e13\u5bb6 (MoE) \u67b6\u69cb\uff0c\u7528\u65bc\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff0c\u7531\u65bc\u5176\u6a21\u7d44\u5316\u8a2d\u8a08\u548c\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u9010\u6f38\u6210\u70ba\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u4e2d\u6f5b\u5728\u7684\u65b9\u5411\u3002\u7136\u800c\uff0c\u55ae\u7d14\u5806\u758a\u5c08\u5bb6\u6578\u91cf\u7121\u6cd5\u4fdd\u8b49\u986f\u8457\u6539\u5584\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u9032\u884c\u5b9a\u6027\u5206\u6790\uff0c\u4ee5\u6307\u51fa\u5c08\u5bb6\u5728\u9999\u8349 MoE \u4e2d\u6703\u5d29\u6f70\u6210\u985e\u4f3c\u7684\u8868\u793a\uff0c\u9650\u5236\u4e86\u6a21\u7d44\u5316\u8a2d\u8a08\u548c\u904b\u7b97\u6548\u7387\u7684\u80fd\u529b\u3002\u9032\u4e00\u6b65\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u5148\u524d\u7684 MoE \u8b8a\u9ad4\u6548\u80fd\u53ef\u80fd\u53d7\u5230\u5c08\u5bb6\u4e4b\u9593\u7f3a\u4e4f\u591a\u6a23\u6027\u7684\u9650\u5236\u3002\u53d7\u5230\u9019\u4e9b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6b63\u4ea4\u6df7\u5408\u5c08\u5bb6 (OMoE)\uff0c\u9019\u662f\u4e00\u7a2e\u8cc7\u6e90\u6709\u6548\u7387\u7684 MoE \u8b8a\u9ad4\uff0c\u5b83\u4ee5\u6b63\u4ea4\u65b9\u5f0f\u8a13\u7df4\u5c08\u5bb6\uff0c\u4ee5\u4fc3\u9032\u591a\u6a23\u6027\u3002\u5728 OMoE \u4e2d\uff0c\u5229\u7528 Gram-Schmidt \u7a0b\u5e8f\u4f86\u5f37\u5236\u57f7\u884c\u5c08\u5bb6\u7684\u8868\u793a\u4f4d\u65bc Stiefel \u6d41\u5f62\u5167\u3002\u900f\u904e\u5c07\u6b63\u4ea4\u7d04\u675f\u76f4\u63a5\u61c9\u7528\u65bc\u67b6\u69cb\uff0cOMoE \u4fdd\u6301\u5b78\u7fd2\u76ee\u6a19\u4e0d\u8b8a\uff0c\u540c\u6642\u4e0d\u5f71\u97ff\u6700\u4f73\u6027\u3002\u6211\u5011\u7684\u6a21\u578b\u7c21\u55ae\uff0c\u4e14\u53ef\u6e1b\u8f15\u8a18\u61b6\u9ad4\u74f6\u9838\uff0c\u56e0\u70ba\u8207\u9999\u8349 MoE \u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u6703\u7522\u751f\u6700\u5c11\u7684\u5c08\u5bb6\u3002\u5728\u5404\u7a2e\u5e38\u8b58\u63a8\u7406\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cOMoE \u53ef\u4ee5\u6301\u7e8c\u9054\u6210\u7a69\u5b9a\u4e14\u6709\u6548\u7387\u7684\u6548\u80fd\u6539\u5584\uff0c\u540c\u6642\u5927\u5e45\u6e1b\u5c11\u6240\u9700\u7684\u5c08\u5bb6\u6578\u91cf\u3002</paragraph>", "author": "Jinyuan Feng et.al.", "authors": "Jinyuan Feng, Zhiqiang Pu, Tianyi Hu, Dongmin Li, Xiaolin Ai, Huimu Wang", "id": "2501.10062v1", "paper_url": "http://arxiv.org/abs/2501.10062v1", "repo": "null"}}