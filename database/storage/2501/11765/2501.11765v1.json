{"2501.11765": {"publish_time": "2025-01-20", "title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?", "paper_summary": "Transformers architecture apply self-attention to tokens represented as\nvectors, before a fully connected (neuronal network) layer. These two parts can\nbe layered many times. Traditionally, self-attention is seen as a mechanism for\naggregating information before logical operations are performed by the fully\nconnected layer. In this paper, we show, that quite counter-intuitively, the\nlogical analysis can also be performed within the self-attention. For this we\nimplement a handcrafted single-level encoder layer which performs the logical\nanalysis within self-attention. We then study the scenario in which a one-level\ntransformer model undergoes self-learning using gradient descent. We\ninvestigate whether the model utilizes fully connected layers or self-attention\nmechanisms for logical analysis when it has the choice. Given that gradient\ndescent can become stuck at undesired zeros, we explicitly calculate these\nunwanted zeros and find ways to avoid them. We do all this in the context of\npredicting grammatical category pairs of adjacent tokens in a text. We believe\nthat our findings have broader implications for understanding the potential\nlogical operations performed by self-attention.", "paper_summary_zh": "Transformer \u67b6\u69cb\u5728\u5168\u9023\u63a5\uff08\u795e\u7d93\u7db2\u8def\uff09\u5c64\u4e4b\u524d\uff0c\u5c07\u81ea\u6ce8\u610f\u529b\u61c9\u7528\u65bc\u8868\u793a\u70ba\u5411\u91cf\u7684\u7b26\u865f\u3002\u9019\u5169\u500b\u90e8\u5206\u53ef\u4ee5\u5206\u5c64\u591a\u6b21\u3002\u50b3\u7d71\u4e0a\uff0c\u81ea\u6ce8\u610f\u529b\u88ab\u8996\u70ba\u5728\u5168\u9023\u63a5\u5c64\u57f7\u884c\u908f\u8f2f\u904b\u7b97\u4e4b\u524d\u805a\u5408\u8cc7\u8a0a\u7684\u6a5f\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u81ea\u6ce8\u610f\u529b\u4e2d\u4e5f\u53ef\u4ee5\u57f7\u884c\u908f\u8f2f\u5206\u6790\uff0c\u9019\u76f8\u7576\u9055\u53cd\u76f4\u89ba\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u500b\u624b\u5de5\u88fd\u4f5c\u7684\u55ae\u5c64\u7de8\u78bc\u5668\u5c64\uff0c\u5b83\u5728\u81ea\u6ce8\u610f\u529b\u4e2d\u57f7\u884c\u908f\u8f2f\u5206\u6790\u3002\u7136\u5f8c\u6211\u5011\u7814\u7a76\u4e86\u4e00\u7a2e\u60c5\u5883\uff0c\u5176\u4e2d\u4e00\u500b\u55ae\u5c64 transformer \u6a21\u578b\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u9032\u884c\u81ea\u5b78\u7fd2\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u6a21\u578b\u5728\u6709\u9078\u64c7\u7684\u60c5\u6cc1\u4e0b\uff0c\u662f\u5426\u5229\u7528\u5168\u9023\u63a5\u5c64\u6216\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u9032\u884c\u908f\u8f2f\u5206\u6790\u3002\u7531\u65bc\u68af\u5ea6\u4e0b\u964d\u53ef\u80fd\u6703\u505c\u7559\u5728\u4e0d\u9700\u8981\u7684\u96f6\u9ede\uff0c\u56e0\u6b64\u6211\u5011\u660e\u78ba\u8a08\u7b97\u9019\u4e9b\u4e0d\u9700\u8981\u7684\u96f6\u9ede\uff0c\u4e26\u627e\u51fa\u907f\u514d\u5b83\u5011\u7684\u65b9\u6cd5\u3002\u6211\u5011\u5728\u9810\u6e2c\u6587\u5b57\u4e2d\u76f8\u9130\u7b26\u865f\u7684\u8a9e\u6cd5\u985e\u5225\u5c0d\u6642\uff0c\u57f7\u884c\u6240\u6709\u9019\u4e9b\u64cd\u4f5c\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u767c\u73fe\u5c0d\u65bc\u7406\u89e3\u81ea\u6ce8\u610f\u529b\u57f7\u884c\u7684\u6f5b\u5728\u908f\u8f2f\u904b\u7b97\u5177\u6709\u66f4\u5ee3\u6cdb\u7684\u610f\u7fa9\u3002", "author": "Evgeniy Shin et.al.", "authors": "Evgeniy Shin, Heinrich Matzinger", "id": "2501.11765v1", "paper_url": "http://arxiv.org/abs/2501.11765v1", "repo": "null"}}