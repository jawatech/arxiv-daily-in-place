{"2411.19930": {"publish_time": "2024-11-29", "title": "On Domain-Specific Post-Training for Multimodal Large Language Models", "paper_summary": "Recent years have witnessed the rapid development of general multimodal large\nlanguage models (MLLMs). However, adapting general MLLMs to specific domains,\nsuch as scientific fields and industrial applications, remains less explored.\nThis paper systematically investigates domain adaptation of MLLMs through\npost-training, focusing on data synthesis, training pipelines, and task\nevaluation. (1) Data Synthesis: Using open-source models, we develop a visual\ninstruction synthesizer that effectively generates diverse visual instruction\ntasks from domain-specific image-caption pairs. Our synthetic tasks surpass\nthose generated by manual rules, GPT-4, and GPT-4V in enhancing the\ndomain-specific performance of MLLMs. (2) Training Pipeline: While the\ntwo-stage training--initially on image-caption pairs followed by visual\ninstruction tasks--is commonly adopted for developing general MLLMs, we apply a\nsingle-stage training pipeline to enhance task diversity for domain-specific\npost-training. (3) Task Evaluation: We conduct experiments in two domains,\nbiomedicine and food, by post-training MLLMs of different sources and scales\n(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM\nperformance on various domain-specific tasks. To support further research in\nMLLM domain adaptation, we will open-source our implementations.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u4e00\u822c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u767c\u5c55\u5341\u5206\u8fc5\u901f\u3002\u7136\u800c\uff0c\u5c07\u4e00\u822c MLLM \u9069\u61c9\u5230\u7279\u5b9a\u9818\u57df\uff08\u4f8b\u5982\u79d1\u5b78\u9818\u57df\u548c\u7522\u696d\u61c9\u7528\uff09\u7684\u7814\u7a76\u4ecd\u8f03\u5c11\u3002\u672c\u6587\u900f\u904e\u5f8c\u8a13\u7df4\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e MLLM \u7684\u9818\u57df\u9069\u61c9\uff0c\u91cd\u9ede\u653e\u5728\u8cc7\u6599\u5408\u6210\u3001\u8a13\u7df4\u7ba1\u7dda\u548c\u4efb\u52d9\u8a55\u4f30\u4e0a\u3002(1) \u8cc7\u6599\u5408\u6210\uff1a\u6211\u5011\u4f7f\u7528\u958b\u6e90\u6a21\u578b\uff0c\u958b\u767c\u51fa\u4e00\u500b\u8996\u89ba\u6307\u4ee4\u5408\u6210\u5668\uff0c\u53ef\u6709\u6548\u5730\u5f9e\u7279\u5b9a\u9818\u57df\u7684\u5f71\u50cf\u6a19\u984c\u5c0d\u4e2d\u7522\u751f\u591a\u6a23\u7684\u8996\u89ba\u6307\u4ee4\u4efb\u52d9\u3002\u6211\u5011\u7684\u5408\u6210\u4efb\u52d9\u8d85\u8d8a\u4e86\u624b\u52d5\u898f\u5247\u3001GPT-4 \u548c GPT-4V \u6240\u7522\u751f\u7684\u4efb\u52d9\uff0c\u4ee5\u589e\u5f37 MLLM \u7684\u7279\u5b9a\u9818\u57df\u6548\u80fd\u3002(2) \u8a13\u7df4\u7ba1\u7dda\uff1a\u96d6\u7136\u5169\u968e\u6bb5\u8a13\u7df4\uff08\u6700\u521d\u5728\u5f71\u50cf\u6a19\u984c\u5c0d\u4e0a\u9032\u884c\uff0c\u7136\u5f8c\u5728\u8996\u89ba\u6307\u4ee4\u4efb\u52d9\u4e0a\u9032\u884c\uff09\u901a\u5e38\u7528\u65bc\u958b\u767c\u4e00\u822c MLLM\uff0c\u4f46\u6211\u5011\u63a1\u7528\u55ae\u968e\u6bb5\u8a13\u7df4\u7ba1\u7dda\u4f86\u589e\u5f37\u7279\u5b9a\u9818\u57df\u5f8c\u8a13\u7df4\u7684\u4efb\u52d9\u591a\u6a23\u6027\u3002(3) \u4efb\u52d9\u8a55\u4f30\uff1a\u6211\u5011\u5728\u751f\u7269\u91ab\u5b78\u548c\u98df\u54c1\u9019\u5169\u500b\u9818\u57df\u9032\u884c\u5be6\u9a57\uff0c\u900f\u904e\u5f8c\u8a13\u7df4\u4e0d\u540c\u4f86\u6e90\u548c\u898f\u6a21\u7684 MLLM\uff08\u4f8b\u5982 Qwen2-VL-2B\u3001LLaVA-v1.6-8B\u3001Llama-3.2-11B\uff09\uff0c\u7136\u5f8c\u8a55\u4f30 MLLM \u5728\u5404\u7a2e\u7279\u5b9a\u9818\u57df\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002\u70ba\u4e86\u652f\u6301 MLLM \u9818\u57df\u9069\u61c9\u7684\u5f8c\u7e8c\u7814\u7a76\uff0c\u6211\u5011\u5c07\u958b\u653e\u6211\u5011\u7684\u5be6\u4f5c\u539f\u59cb\u78bc\u3002", "author": "Daixuan Cheng et.al.", "authors": "Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang", "id": "2411.19930v1", "paper_url": "http://arxiv.org/abs/2411.19930v1", "repo": "null"}}