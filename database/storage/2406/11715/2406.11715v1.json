{"2406.11715": {"publish_time": "2024-06-17", "title": "Measuring memorization in RLHF for code completion", "paper_summary": "Reinforcement learning with human feedback (RLHF) has become the dominant\nmethod to align large models to user preferences. Unlike fine-tuning, for which\nthere are many studies regarding training data memorization, it is not clear\nhow memorization is affected by or introduced in the RLHF alignment process.\nUnderstanding this relationship is important as real user data may be collected\nand used to align large models; if user data is memorized during RLHF and later\nregurgitated, this could raise privacy concerns. In this work, we analyze how\ntraining data memorization can surface and propagate through each phase of\nRLHF. We focus our study on code completion models, as code completion is one\nof the most popular use cases for large language models. We find that RLHF\nsignificantly decreases the chance that data used for reward modeling and\nreinforcement learning is memorized, in comparison to aligning via directly\nfine-tuning on this data, but that examples already memorized during the\nfine-tuning stage of RLHF, will, in the majority of cases, remain memorized\nafter RLHF.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u589e\u5f37\u5b78\u7fd2\uff08RLHF\uff09\u5df2\u6210\u70ba\u8abf\u6574\u5927\u578b\u6a21\u578b\u4ee5\u7b26\u5408\u4f7f\u7528\u8005\u504f\u597d\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u8207\u5fae\u8abf\u4e0d\u540c\uff0c\u5c0d\u65bc\u5fae\u8abf\u6709\u8a31\u591a\u95dc\u65bc\u8a13\u7df4\u8cc7\u6599\u8a18\u61b6\u7684\u7814\u7a76\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8a18\u61b6\u662f\u5982\u4f55\u53d7\u5230 RLHF \u8abf\u6574\u7a0b\u5e8f\u5f71\u97ff\u6216\u5f15\u5165\u7684\u3002\u4e86\u89e3\u9019\u7a2e\u95dc\u4fc2\u5f88\u91cd\u8981\uff0c\u56e0\u70ba\u53ef\u80fd\u6703\u6536\u96c6\u771f\u5be6\u4f7f\u7528\u8005\u8cc7\u6599\u4e26\u7528\u65bc\u8abf\u6574\u5927\u578b\u6a21\u578b\uff1b\u5982\u679c\u4f7f\u7528\u8005\u8cc7\u6599\u5728 RLHF \u671f\u9593\u88ab\u8a18\u61b6\u4e0b\u4f86\u4e26\u7a0d\u5f8c\u518d\u91cd\u8907\uff0c\u9019\u53ef\u80fd\u6703\u5f15\u767c\u96b1\u79c1\u554f\u984c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5206\u6790\u8a13\u7df4\u8cc7\u6599\u8a18\u61b6\u5982\u4f55\u5728 RLHF \u7684\u6bcf\u500b\u968e\u6bb5\u6d6e\u73fe\u4e26\u50b3\u64ad\u3002\u6211\u5011\u5c07\u7814\u7a76\u91cd\u9ede\u653e\u5728\u7a0b\u5f0f\u78bc\u5b8c\u6210\u6a21\u578b\u4e0a\uff0c\u56e0\u70ba\u7a0b\u5f0f\u78bc\u5b8c\u6210\u662f\u5927\u8a9e\u8a00\u6a21\u578b\u6700\u53d7\u6b61\u8fce\u7684\u7528\u4f8b\u4e4b\u4e00\u3002\u6211\u5011\u767c\u73fe\uff0c\u8207\u76f4\u63a5\u5c0d\u6b64\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u7684\u8abf\u6574\u65b9\u5f0f\u76f8\u6bd4\uff0cRLHF \u6703\u986f\u8457\u964d\u4f4e\u7528\u65bc\u734e\u52f5\u5efa\u6a21\u548c\u589e\u5f37\u5b78\u7fd2\u7684\u8cc7\u6599\u88ab\u8a18\u61b6\u7684\u6a5f\u7387\uff0c\u4f46 RLHF \u5fae\u8abf\u968e\u6bb5\u4e2d\u5df2\u8a18\u61b6\u7684\u7bc4\u4f8b\u5728\u5927\u90e8\u5206\u60c5\u6cc1\u4e0b\uff0cRLHF \u4e4b\u5f8c\u4ecd\u6703\u4fdd\u6301\u8a18\u61b6\u3002", "author": "Aneesh Pappu et.al.", "authors": "Aneesh Pappu, Billy Porter, Ilia Shumailov, Jamie Hayes", "id": "2406.11715v1", "paper_url": "http://arxiv.org/abs/2406.11715v1", "repo": "null"}}