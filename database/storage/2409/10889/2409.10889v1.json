{"2409.10889": {"publish_time": "2024-09-17", "title": "Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes", "paper_summary": "Real-time deepfake, a type of generative AI, is capable of \"creating\"\nnon-existing contents (e.g., swapping one's face with another) in a video. It\nhas been, very unfortunately, misused to produce deepfake videos (during web\nconferences, video calls, and identity authentication) for malicious purposes,\nincluding financial scams and political misinformation. Deepfake detection, as\nthe countermeasure against deepfake, has attracted considerable attention from\nthe academic community, yet existing works typically rely on learning passive\nfeatures that may perform poorly beyond seen datasets. In this paper, we\npropose SFake, a new real-time deepfake detection method that innovatively\nexploits deepfake models' inability to adapt to physical interference.\nSpecifically, SFake actively sends probes to trigger mechanical vibrations on\nthe smartphone, resulting in the controllable feature on the footage.\nConsequently, SFake determines whether the face is swapped by deepfake based on\nthe consistency of the facial area with the probe pattern. We implement SFake,\nevaluate its effectiveness on a self-built dataset, and compare it with six\nother detection methods. The results show that SFake outperforms other\ndetection methods with higher detection accuracy, faster process speed, and\nlower memory consumption.", "paper_summary_zh": "\u5373\u6642\u6df1\u5ea6\u507d\u9020\u662f\u4e00\u7a2e\u751f\u6210\u5f0f AI\uff0c\u80fd\u5920\u300c\u5275\u9020\u300d\u5f71\u7247\u4e2d\u4e0d\u5b58\u5728\u7684\u5167\u5bb9\uff08\u4f8b\u5982\uff0c\u5c07\u67d0\u4eba\u7684\u81c9\u63db\u6210\u53e6\u4e00\u500b\u4eba\uff09\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u5b83\u5df2\u88ab\u6feb\u7528\u65bc\u88fd\u4f5c\u6df1\u5ea6\u507d\u9020\u5f71\u7247\uff08\u5728\u7db2\u8def\u6703\u8b70\u3001\u8996\u8a0a\u901a\u8a71\u548c\u8eab\u5206\u9a57\u8b49\u4e2d\uff09\uff0c\u7528\u65bc\u60e1\u610f\u76ee\u7684\uff0c\u5305\u62ec\u91d1\u878d\u8a50\u9a19\u548c\u653f\u6cbb\u932f\u8aa4\u8a0a\u606f\u3002\u6df1\u5ea6\u507d\u9020\u5075\u6e2c\u4f5c\u70ba\u5c0d\u6297\u6df1\u5ea6\u507d\u9020\u7684\u5c0d\u7b56\uff0c\u5df2\u5f15\u8d77\u5b78\u8853\u754c\u7684\u5ee3\u6cdb\u95dc\u6ce8\uff0c\u4f46\u73fe\u6709\u7684\u4f5c\u54c1\u901a\u5e38\u4f9d\u8cf4\u65bc\u5b78\u7fd2\u88ab\u52d5\u7279\u5fb5\uff0c\u9019\u4e9b\u7279\u5fb5\u5728\u8d85\u51fa\u5df2\u898b\u8cc7\u6599\u96c6\u6642\u53ef\u80fd\u6703\u8868\u73fe\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa SFake\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u5373\u6642\u6df1\u5ea6\u507d\u9020\u5075\u6e2c\u65b9\u6cd5\uff0c\u5275\u65b0\u5730\u5229\u7528\u6df1\u5ea6\u507d\u9020\u6a21\u578b\u7121\u6cd5\u9069\u61c9\u7269\u7406\u5e72\u64fe\u7684\u7279\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSFake \u4e3b\u52d5\u767c\u9001\u63a2\u91dd\u4ee5\u89f8\u767c\u667a\u6167\u578b\u624b\u6a5f\u4e0a\u7684\u6a5f\u68b0\u632f\u52d5\uff0c\u5f9e\u800c\u5c0e\u81f4\u5f71\u7247\u4e2d\u53ef\u63a7\u7684\u7279\u5fb5\u3002\u56e0\u6b64\uff0cSFake \u6839\u64da\u81c9\u90e8\u5340\u57df\u8207\u63a2\u91dd\u6a21\u5f0f\u7684\u4e00\u81f4\u6027\u4f86\u5224\u65b7\u81c9\u90e8\u662f\u5426\u88ab\u6df1\u5ea6\u507d\u9020\u66ff\u63db\u3002\u6211\u5011\u5be6\u4f5c SFake\uff0c\u8a55\u4f30\u5176\u5728\u81ea\u5efa\u8cc7\u6599\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e26\u5c07\u5176\u8207\u5176\u4ed6\u516d\u7a2e\u5075\u6e2c\u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u7d50\u679c\u8868\u660e\uff0cSFake \u5728\u66f4\u9ad8\u7684\u5075\u6e2c\u6e96\u78ba\u5ea6\u3001\u66f4\u5feb\u7684\u8655\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u8a18\u61b6\u9ad4\u6d88\u8017\u65b9\u9762\u512a\u65bc\u5176\u4ed6\u5075\u6e2c\u65b9\u6cd5\u3002", "author": "Zhixin Xie et.al.", "authors": "Zhixin Xie, Jun Luo", "id": "2409.10889v1", "paper_url": "http://arxiv.org/abs/2409.10889v1", "repo": "null"}}