{"2407.10855": {"publish_time": "2024-07-15", "title": "Weighted Grouped Query Attention in Transformers", "paper_summary": "The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.", "paper_summary_zh": "\u6ce8\u610f\u529b\u673a\u5236\u6784\u6210\u4e86 Transformer \u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u6a21\u5757\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u6269\u5c55\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u968f\u7740\u5bf9\u6269\u5c55\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\u4ee5\u53ca\u5bf9\u786c\u4ef6\u5185\u5b58\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u4ecd\u7136\u5f88\u9ad8\u3002\u4e3a\u4e86\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u591a\u67e5\u8be2\u6ce8\u610f\u529b (MQA) \u548c\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b (GQA) \u5206\u522b\u5728 (Shazeer, 2019) \u548c (Ainslieet al., 2023) \u4e2d\u63d0\u51fa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u7684\u53d8\u4f53\uff0c\u79f0\u4e3a\u52a0\u6743\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b (WGQA)\u3002\u6211\u4eec\u5728 T5 \u89e3\u7801\u5668\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u4e3a\u6bcf\u4e2a\u952e\u548c\u503c\u5934\u5f15\u5165\u4e86\u65b0\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5fae\u8c03\u671f\u95f4\u53d6\u52a0\u6743\u5e73\u5747\u503c\u3002\u6211\u4eec\u7684\u6a21\u578b\u6bd4 GQA \u5e73\u5747\u63d0\u9ad8\u4e86 0.53%\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u671f\u95f4\u6027\u80fd\u6536\u655b\u5230\u4f20\u7edf\u7684 Multi-head \u6ce8\u610f\u529b (MHA)\uff0c\u800c\u6ca1\u6709\u989d\u5916\u7684\u5f00\u9500\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u53c2\u6570\u7684\u5f15\u5165\uff0c\u968f\u540e\u7684\u5fae\u8c03\u5728\u8bad\u7ec3\u671f\u95f4\u544a\u77e5\u6a21\u578b\u6709\u5173\u5206\u7ec4\u673a\u5236\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u6bd4\u8f83 T5-small \u548c T5-base \u67b6\u6784\u4e4b\u95f4\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u6211\u4eec\u5206\u6790\u4e2d\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "author": "Sai Sena Chinnakonduru et.al.", "authors": "Sai Sena Chinnakonduru, Astarag Mohapatra", "id": "2407.10855v1", "paper_url": "http://arxiv.org/abs/2407.10855v1", "repo": "null"}}