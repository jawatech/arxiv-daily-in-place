{"2502.03654": {"publish_time": "2025-02-05", "title": "Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics", "paper_summary": "Activation functions are fundamental elements of deep learning architectures\nas they significantly influence training dynamics. ReLU, while widely used, is\nprone to the dying neuron problem, which has been mitigated by variants such as\nLeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently,\nself-gated activations like GELU and Swish have emerged as state-of-the-art\nalternatives, leveraging their smoothness to ensure stable gradient flow and\nprevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit\n(GoLU), a novel self-gated activation function defined as $\\mathrm{GoLU}(x) = x\n\\, \\mathrm{Gompertz}(x)$, where $\\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU\nactivation leverages the asymmetry in the Gompertz function to reduce variance\nin the latent space more effectively compared to GELU and Swish, while\npreserving robust gradient flow. Extensive experiments across diverse tasks,\nincluding Image Classification, Language Modeling, Semantic Segmentation,\nObject Detection, Instance Segmentation, and Diffusion, highlight GoLU's\nsuperior performance relative to state-of-the-art activation functions,\nestablishing GoLU as a robust alternative to existing activation functions.", "paper_summary_zh": "\u6fc0\u6d3b\u51fd\u6578\u662f\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u7684\u57fa\u672c\u5143\u7d20\uff0c\u56e0\u70ba\u5b83\u5011\u6703\u986f\u8457\u5f71\u97ff\u8a13\u7df4\u52d5\u614b\u3002ReLU \u96d6\u7136\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u5bb9\u6613\u767c\u751f\u795e\u7d93\u5143\u6b7b\u4ea1\u554f\u984c\uff0c\u800c LeakyReLU\u3001PReLU \u548c ELU \u7b49\u8b8a\u9ad4\u5df2\u6e1b\u8f15\u4e86\u6b64\u554f\u984c\uff0c\u5b83\u5011\u80fd\u66f4\u597d\u5730\u8655\u7406\u8ca0\u795e\u7d93\u5143\u8f38\u51fa\u3002\u6700\u8fd1\uff0c\u50cf GELU \u548c Swish \u9019\u6a23\u7684\u81ea\u9580\u63a7\u6fc0\u6d3b\u51fd\u6578\u5df2\u6210\u70ba\u6700\u5148\u9032\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u5b83\u5011\u7684\u5e73\u6ed1\u6027\u4f86\u78ba\u4fdd\u7a69\u5b9a\u7684\u68af\u5ea6\u6d41\u52d5\u4e26\u9632\u6b62\u795e\u7d93\u5143\u4e0d\u6d3b\u8e8d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 Gompertz \u7dda\u6027\u55ae\u5143 (GoLU)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u81ea\u9580\u63a7\u6fc0\u6d3b\u51fd\u6578\uff0c\u5b9a\u7fa9\u70ba $\\mathrm{GoLU}(x) = x \\, \\mathrm{Gompertz}(x)$\uff0c\u5176\u4e2d $\\mathrm{Gompertz}(x) = e^{-e^{-x}}$\u3002GoLU \u6fc0\u6d3b\u51fd\u6578\u5229\u7528 Gompertz \u51fd\u6578\u4e2d\u7684\u4e0d\u5c0d\u7a31\u6027\uff0c\u8207 GELU \u548c Swish \u76f8\u6bd4\uff0c\u66f4\u6709\u6548\u5730\u6e1b\u5c11\u6f5b\u5728\u7a7a\u9593\u4e2d\u7684\u8b8a\u7570\uff0c\u540c\u6642\u4fdd\u7559\u7a69\u5065\u7684\u68af\u5ea6\u6d41\u52d5\u3002\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u5ee3\u6cdb\u5be6\u9a57\uff0c\u5305\u62ec\u5f71\u50cf\u5206\u985e\u3001\u8a9e\u8a00\u5efa\u6a21\u3001\u8a9e\u7fa9\u5206\u5272\u3001\u7269\u4ef6\u5075\u6e2c\u3001\u5be6\u4f8b\u5206\u5272\u548c\u64f4\u6563\uff0c\u7a81\u986f\u4e86 GoLU \u76f8\u5c0d\u65bc\u6700\u5148\u9032\u7684\u6fc0\u6d3b\u51fd\u6578\u7684\u512a\u7570\u6548\u80fd\uff0c\u78ba\u7acb\u4e86 GoLU \u4f5c\u70ba\u73fe\u6709\u6fc0\u6d3b\u51fd\u6578\u7684\u7a69\u5065\u66ff\u4ee3\u65b9\u6848\u3002", "author": "Indrashis Das et.al.", "authors": "Indrashis Das, Mahmoud Safari, Steven Adriaensen, Frank Hutter", "id": "2502.03654v1", "paper_url": "http://arxiv.org/abs/2502.03654v1", "repo": "null"}}