{"2410.21582": {"publish_time": "2024-10-28", "title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "paper_summary": "Highly performant large-scale pre-trained models promise to also provide a\nvaluable foundation for learning specialized tasks, by fine-tuning the model to\nthe desired task. By starting from a good general-purpose model, the goal is to\nachieve both specialization in the target task and maintain robustness. To\nassess the robustness of models to out-of-distribution samples after\nfine-tuning on downstream datasets, we introduce a new robust fine-tuning\nbenchmark, ImageNet-RIB (Robustness Inheritance Benchmark). The benchmark\nconsists of a set of related but distinct specialized (downstream) tasks;\npre-trained models are fine-tuned on one task in the set and their robustness\nis assessed on the rest, iterating across all tasks for fine-tuning and\nassessment. We find that the continual learning methods, EWC and LwF maintain\nrobustness after fine-tuning though fine-tuning generally does reduce\nperformance on generalization to related downstream tasks across models. Not\nsurprisingly, models pre-trained on large and rich datasets exhibit higher\ninitial robustness across datasets and suffer more pronounced degradation\nduring fine-tuning. The distance between the pre-training and downstream\ndatasets, measured by optimal transport, predicts this performance degradation\non the pre-training dataset. However, counterintuitively, model robustness\nafter fine-tuning on related downstream tasks is the worst when the\npre-training dataset is the richest and the most diverse. This suggests that\nstarting with the strongest foundation model is not necessarily the best\napproach for performance on specialist tasks. The benchmark thus offers key\ninsights for developing more resilient fine-tuning strategies and building\nrobust machine learning models. https://jd730.github.io/projects/ImageNet-RIB", "paper_summary_zh": "\u6027\u80fd\u6975\u4f73\u7684\u5927\u898f\u6a21\u9810\u5148\u8a13\u7df4\u6a21\u578b\u627f\u8afe\uff0c\u900f\u904e\u5fae\u8abf\u6a21\u578b\u4ee5\u7b26\u5408\u6240\u9700\u4efb\u52d9\uff0c\u4e5f\u80fd\u63d0\u4f9b\u4e00\u500b\u6709\u50f9\u503c\u7684\u57fa\u790e\uff0c\u4ee5\u4fbf\u5b78\u7fd2\u5c08\u696d\u4efb\u52d9\u3002\u5f9e\u4e00\u500b\u597d\u7684\u901a\u7528\u6a21\u578b\u958b\u59cb\uff0c\u76ee\u6a19\u662f\u540c\u6642\u5728\u76ee\u6a19\u4efb\u52d9\u4e2d\u5be6\u73fe\u5c08\u696d\u5316\uff0c\u4e26\u7dad\u6301\u7a69\u5065\u6027\u3002\u70ba\u4e86\u8a55\u4f30\u6a21\u578b\u5728\u5fae\u8abf\u4e0b\u6e38\u8cc7\u6599\u96c6\u5f8c\u5c0d\u5206\u4f48\u5916\u6a23\u672c\u7684\u7a69\u5065\u6027\uff0c\u6211\u5011\u5f15\u5165\u4e86\u65b0\u7684\u7a69\u5065\u5fae\u8abf\u57fa\u6e96 ImageNet-RIB\uff08\u7a69\u5065\u7e7c\u627f\u57fa\u6e96\uff09\u3002\u57fa\u6e96\u5305\u542b\u4e00\u7d44\u76f8\u95dc\u4f46\u4e0d\u540c\u7684\u5c08\u696d\uff08\u4e0b\u6e38\uff09\u4efb\u52d9\uff1b\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u91dd\u5c0d\u8a72\u7d44\u4e2d\u7684\u5176\u4e2d\u4e00\u9805\u4efb\u52d9\u9032\u884c\u5fae\u8abf\uff0c\u800c\u5176\u7a69\u5065\u6027\u5247\u5728\u5176\u4ed6\u4efb\u52d9\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u4e26\u91dd\u5c0d\u5fae\u8abf\u548c\u8a55\u4f30\u7684\u6240\u6709\u4efb\u52d9\u9032\u884c\u53cd\u8986\u904b\u7b97\u3002\u6211\u5011\u767c\u73fe\uff0c\u6301\u7e8c\u5b78\u7fd2\u65b9\u6cd5 EWC \u548c LwF \u5728\u5fae\u8abf\u5f8c\u4ecd\u80fd\u7dad\u6301\u7a69\u5065\u6027\uff0c\u5118\u7ba1\u5fae\u8abf\u901a\u5e38\u6703\u964d\u4f4e\u6a21\u578b\u5728\u76f8\u95dc\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u6cdb\u5316\u6548\u80fd\u3002\u6beb\u4e0d\u610f\u5916\u5730\uff0c\u5728\u5927\u578b\u8c50\u5bcc\u8cc7\u6599\u96c6\u4e0a\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u5728\u4e0d\u540c\u8cc7\u6599\u96c6\u4e4b\u9593\u8868\u73fe\u51fa\u8f03\u9ad8\u7684\u521d\u59cb\u7a69\u5065\u6027\uff0c\u800c\u5728\u5fae\u8abf\u671f\u9593\u906d\u53d7\u66f4\u660e\u986f\u7684\u6548\u80fd\u4e0b\u964d\u3002\u900f\u904e\u6700\u4f73\u50b3\u8f38\u6e2c\u91cf\u7684\u9810\u5148\u8a13\u7df4\u548c\u4e0b\u6e38\u8cc7\u6599\u96c6\u4e4b\u9593\u7684\u8ddd\u96e2\uff0c\u53ef\u4ee5\u9810\u6e2c\u9810\u5148\u8a13\u7df4\u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\u4e0b\u964d\u3002\u7136\u800c\uff0c\u53cd\u76f4\u89ba\u7684\u662f\uff0c\u7576\u9810\u5148\u8a13\u7df4\u8cc7\u6599\u96c6\u6700\u8c50\u5bcc\u4e14\u6700\u591a\u6a23\u5316\u6642\uff0c\u6a21\u578b\u5728\u76f8\u95dc\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u5fae\u8abf\u5f8c\u7a69\u5065\u6027\u6700\u5dee\u3002\u9019\u8868\u793a\u5f9e\u6700\u5f37\u5927\u7684\u57fa\u790e\u6a21\u578b\u958b\u59cb\uff0c\u4e0d\u4e00\u5b9a\u662f\u57f7\u884c\u5c08\u696d\u4efb\u52d9\u7684\u6700\u4f73\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u57fa\u6e96\u70ba\u958b\u767c\u66f4\u5177\u97cc\u6027\u7684\u5fae\u8abf\u7b56\u7565\u548c\u5efa\u69cb\u7a69\u5065\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u63d0\u4f9b\u4e86\u95dc\u9375\u898b\u89e3\u3002https://jd730.github.io/projects/ImageNet-RIB", "author": "Jaedong Hwang et.al.", "authors": "Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete", "id": "2410.21582v1", "paper_url": "http://arxiv.org/abs/2410.21582v1", "repo": "null"}}