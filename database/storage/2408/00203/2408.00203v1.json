{"2408.00203": {"publish_time": "2024-08-01", "title": "OmniParser for Pure Vision Based GUI Agent", "paper_summary": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.", "paper_summary_zh": "\u6700\u8fd1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u663e\u793a\u51fa\u5728\u7528\u6237\u754c\u9762\u4e0a\u64cd\u4f5c\u4ee3\u7406\u7cfb\u7edf\u65f6\u5de8\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u6211\u4eec\u8ba4\u4e3a\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e00\u79cd\u5f3a\u5927\u7684\u5c4f\u5e55\u89e3\u6790\u6280\u672f\uff0cGPT-4V \u7b49\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u591a\u4e2a\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u8de8\u4e0d\u540c\u5e94\u7528\u7a0b\u5e8f\u7684\u901a\u7528\u4ee3\u7406\u7684\u80fd\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u4f4e\u4f30\u4e86\uff1a1\uff09\u53ef\u9760\u5730\u8bc6\u522b\u7528\u6237\u754c\u9762\u4e2d\u7684\u53ef\u4ea4\u4e92\u56fe\u6807\uff0c\u4ee5\u53ca 2\uff09\u7406\u89e3\u5c4f\u5e55\u622a\u56fe\u4e2d\u5404\u79cd\u5143\u7d20\u7684\u8bed\u4e49\uff0c\u5e76\u5c06\u9884\u671f\u52a8\u4f5c\u51c6\u786e\u5730\u4e0e\u5c4f\u5e55\u4e0a\u7684\u76f8\u5e94\u533a\u57df\u5173\u8054\u8d77\u6765\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 \\textsc{OmniParser}\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u7528\u6237\u754c\u9762\u5c4f\u5e55\u622a\u56fe\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u5143\u7d20\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u5b83\u5927\u5927\u589e\u5f3a\u4e86 GPT-4V \u751f\u6210\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u52a8\u4f5c\u53ef\u4ee5\u51c6\u786e\u5730\u57fa\u4e8e\u754c\u9762\u7684\u76f8\u5e94\u533a\u57df\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u6d41\u884c\u7684\u7f51\u9875\u548c\u56fe\u6807\u63cf\u8ff0\u6570\u636e\u96c6\u6574\u7406\u4e86\u4e00\u4e2a\u53ef\u4ea4\u4e92\u56fe\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u88ab\u7528\u6765\u5fae\u8c03\u4e13\u95e8\u7684\u6a21\u578b\uff1a\u4e00\u4e2a\u7528\u4e8e\u89e3\u6790\u5c4f\u5e55\u4e0a\u53ef\u4ea4\u4e92\u533a\u57df\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u68c0\u6d4b\u5143\u7d20\u7684\u529f\u80fd\u8bed\u4e49\u7684\u6807\u9898\u6a21\u578b\u3002\\textsc{OmniParser} \u5927\u5927\u63d0\u9ad8\u4e86 GPT-4V \u5728 ScreenSpot \u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002\u5e76\u4e14\u5728 Mind2Web \u548c AITW \u57fa\u51c6\u4e0a\uff0c\u4ec5\u8f93\u5165\u5c4f\u5e55\u622a\u56fe\u7684 \\textsc{OmniParser} \u4f18\u4e8e\u9700\u8981\u5c4f\u5e55\u622a\u56fe\u5916\u90e8\u9644\u52a0\u4fe1\u606f\u7684 GPT-4V \u57fa\u7ebf\u3002", "author": "Yadong Lu et.al.", "authors": "Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah", "id": "2408.00203v1", "paper_url": "http://arxiv.org/abs/2408.00203v1", "repo": "null"}}