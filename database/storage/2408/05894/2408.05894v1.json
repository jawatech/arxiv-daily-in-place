{"2408.05894": {"publish_time": "2024-08-12", "title": "GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models", "paper_summary": "Vision-Language Models (VLMs) building upon the foundation of powerful large\nlanguage models have made rapid progress in reasoning across visual and textual\ndata. While VLMs perform well on vision tasks that they are trained on, our\nresults highlight key challenges in abstract pattern recognition. We present\nGlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of\nvisual patterns from 40 writing systems with three visual presentation styles.\n  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models\nto understand and judge natural language descriptions of visual patterns.\nGlyphPattern patterns are drawn from a large-scale cognitive science\ninvestigation of human writing systems; as a result, they are rich in spatial\nreference and compositionality. Our experiments show that GlyphPattern is\nchallenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with\nmarginal gains from few-shot prompting. Our detailed error analysis reveals\nchallenges at multiple levels, including visual processing, natural language\nunderstanding, and pattern generalization.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5efa\u69cb\u65bc\u5f37\u5927\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u57fa\u790e\u4e4b\u4e0a\uff0c\u5728\u8996\u89ba\u548c\u6587\u5b57\u8cc7\u6599\u7684\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u7684\u9032\u5c55\u3002\u5118\u7ba1 VLM \u5728\u8a13\u7df4\u7684\u8996\u89ba\u4efb\u52d9\u4e0a\u8868\u73fe\u826f\u597d\uff0c\u4f46\u6211\u5011\u7684\u7d50\u679c\u7a81\u986f\u4e86\u62bd\u8c61\u6a21\u5f0f\u8b58\u5225\u4e2d\u7684\u95dc\u9375\u6311\u6230\u3002\u6211\u5011\u63d0\u51fa\u4e86 GlyphPattern\uff0c\u4e00\u500b 954 \u9805\u7684\u8cc7\u6599\u96c6\uff0c\u5c07 40 \u7a2e\u66f8\u5beb\u7cfb\u7d71\u4e2d 318 \u500b\u4eba\u985e\u7de8\u5beb\u7684\u8996\u89ba\u6a21\u5f0f\u63cf\u8ff0\u8207\u4e09\u7a2e\u8996\u89ba\u5448\u73fe\u6a23\u5f0f\u914d\u5c0d\u3002GlyphPattern \u8a55\u4f30 VLM \u4e2d\u7684\u62bd\u8c61\u6a21\u5f0f\u8b58\u5225\uff0c\u8981\u6c42\u6a21\u578b\u7406\u89e3\u548c\u5224\u65b7\u8996\u89ba\u6a21\u5f0f\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u3002GlyphPattern \u6a21\u5f0f\u53d6\u81ea\u65bc\u5927\u898f\u6a21\u8a8d\u77e5\u79d1\u5b78\u5c0d\u4eba\u985e\u66f8\u5beb\u7cfb\u7d71\u7684\u7814\u7a76\uff1b\u56e0\u6b64\uff0c\u5b83\u5011\u5bcc\u542b\u7a7a\u9593\u53c3\u8003\u548c\u7d44\u5408\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cGlyphPattern \u5c0d\u6700\u5148\u9032\u7684 VLM \u4f86\u8aaa\u5177\u6709\u6311\u6230\u6027\uff08GPT-4o \u53ea\u9054\u5230\u4e86 55% \u7684\u6e96\u78ba\u5ea6\uff09\uff0c\u5f9e\u5c11\u91cf\u63d0\u793a\u4e2d\u7372\u5f97\u7684\u6536\u76ca\u5f88\u5c0f\u3002\u6211\u5011\u8a73\u7d30\u7684\u932f\u8aa4\u5206\u6790\u63ed\u793a\u4e86\u591a\u500b\u5c64\u9762\u7684\u6311\u6230\uff0c\u5305\u62ec\u8996\u89ba\u8655\u7406\u3001\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u6a21\u5f0f\u6982\u62ec\u3002", "author": "Zixuan Wu et.al.", "authors": "Zixuan Wu, Yoolim Kim, Carolyn Jane Anderson", "id": "2408.05894v1", "paper_url": "http://arxiv.org/abs/2408.05894v1", "repo": "null"}}