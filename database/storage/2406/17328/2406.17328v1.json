{"2406.17328": {"publish_time": "2024-06-25", "title": "Dual-Space Knowledge Distillation for Large Language Models", "paper_summary": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.", "paper_summary_zh": "\u77e5\u8bc6\u84b8\u998f (KD) \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f83\u5c0f\u7684\u6a21\u578b\u4e2d\u6765\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b (LLM)\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u767d\u76d2 KD \u65b9\u6cd5\u901a\u5e38\u4f1a\u6700\u5c0f\u5316\u4e24\u4e2a\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u8f6c\u79fb\u66f4\u591a\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u5728\u5f53\u524d\u7684\u767d\u76d2 KD \u6846\u67b6\u4e2d\uff0c\u8f93\u51fa\u5206\u5e03\u6765\u81ea\u4e24\u4e2a\u6a21\u578b\u5404\u81ea\u7684\u8f93\u51fa\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u81ea\u5df1\u7684\u9884\u6d4b\u5934\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8fd9\u79cd\u7a7a\u95f4\u5dee\u5f02\u4f1a\u5bfc\u81f4\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u5728\u8868\u793a\u548c\u5206\u5e03\u5c42\u9762\u4e0a\u76f8\u4f3c\u5ea6\u8f83\u4f4e\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u5dee\u5f02\u8fd8\u963b\u788d\u4e86\u5177\u6709\u4e0d\u540c\u8bcd\u6c47\u8868\u7684\u6a21\u578b\u4e4b\u95f4\u7684 KD \u8fc7\u7a0b\uff0c\u8fd9\u5bf9\u4e8e\u5f53\u524d\u7684 LLM \u6765\u8bf4\u5f88\u5e38\u89c1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7a7a\u95f4\u77e5\u8bc6\u84b8\u998f (DSKD) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u4e24\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u7a7a\u95f4\u4ee5\u8fdb\u884c KD\u3002\u5728 DSKD \u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u79cd\u8de8\u6a21\u578b\u6ce8\u610f\u673a\u5236\uff0c\u5b83\u53ef\u4ee5\u81ea\u52a8\u5bf9\u9f50\u5177\u6709\u4e0d\u540c\u8bcd\u6c47\u8868\u7684\u4e24\u4e2a\u6a21\u578b\u7684\u8868\u793a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4e0d\u4ec5\u4e0e KD \u7684\u5404\u79cd\u8ddd\u79bb\u51fd\u6570\uff08\u4f8b\u5982 KL \u6563\u5ea6\uff09\u517c\u5bb9\uff08\u5982\u5f53\u524d\u6846\u67b6\uff09\uff0c\u800c\u4e14\u8fd8\u652f\u6301\u5728\u4efb\u4f55\u4e24\u4e2a LLM \u4e4b\u95f4\u8fdb\u884c KD\uff0c\u800c\u4e0d\u7ba1\u5b83\u4eec\u7684\u8bcd\u6c47\u8868\u5982\u4f55\u3002\u5728\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSKD \u5728\u5404\u79cd\u8ddd\u79bb\u51fd\u6570\u4e0b\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u7684\u767d\u76d2 KD \u6846\u67b6\uff0c\u5e76\u4e14\u8fd8\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u9488\u5bf9\u5177\u6709\u4e0d\u540c\u8bcd\u6c47\u8868\u7684 LLM \u7684 KD \u65b9\u6cd5\u3002", "author": "Songming Zhang et.al.", "authors": "Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu", "id": "2406.17328v1", "paper_url": "http://arxiv.org/abs/2406.17328v1", "repo": "https://github.com/songmzhang/dskd"}}