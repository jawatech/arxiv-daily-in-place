{"2407.14106": {"publish_time": "2024-07-19", "title": "TorchGT: A Holistic System for Large-scale Graph Transformer Training", "paper_summary": "Graph Transformer is a new architecture that surpasses GNNs in graph\nlearning. While there emerge inspiring algorithm advancements, their practical\nadoption is still limited, particularly on real-world graphs involving up to\nmillions of nodes. We observe existing graph transformers fail on large-scale\ngraphs mainly due to heavy computation, limited scalability and inferior model\nquality. Motivated by these observations, we propose TorchGT, the first\nefficient, scalable, and accurate graph transformer training system. TorchGT\noptimizes training at different levels. At algorithm level, by harnessing the\ngraph sparsity, TorchGT introduces a Dual-interleaved Attention which is\ncomputation-efficient and accuracy-maintained. At runtime level, TorchGT scales\ntraining across workers with a communication-light Cluster-aware Graph\nParallelism. At kernel level, an Elastic Computation Reformation further\noptimizes the computation by reducing memory access latency in a dynamic way.\nExtensive experiments demonstrate that TorchGT boosts training by up to 62.7x\nand supports graph sequence lengths of up to 1M.", "paper_summary_zh": "\u5716\u5f62Transformer\u662f\u4e00\u7a2e\u8d85\u8d8a\u5716\u5f62\u5b78\u7fd2\u4e2d GNN \u7684\u65b0\u67b6\u69cb\u3002\u96d6\u7136\u51fa\u73fe\u4e86\u4ee4\u4eba\u632f\u596e\u7684\u6f14\u7b97\u6cd5\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u7684\u5be6\u969b\u63a1\u7528\u4ecd\u7136\u6709\u9650\uff0c\u7279\u5225\u662f\u5728\u6d89\u53ca\u6578\u767e\u842c\u500b\u7bc0\u9ede\u7684\u771f\u5be6\u4e16\u754c\u5716\u5f62\u4e0a\u3002\u6211\u5011\u89c0\u5bdf\u5230\u73fe\u6709\u7684\u5716\u5f62Transformer\u5728\u5927\u578b\u5716\u5f62\u4e0a\u5931\u6557\uff0c\u4e3b\u8981\u662f\u7531\u65bc\u7e41\u91cd\u7684\u8a08\u7b97\u3001\u6709\u9650\u7684\u53ef\u64f4\u5145\u6027\u548c\u8f03\u5dee\u7684\u6a21\u578b\u54c1\u8cea\u3002\u53d7\u5230\u9019\u4e9b\u89c0\u5bdf\u7d50\u679c\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TorchGT\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u9ad8\u6548\u3001\u53ef\u64f4\u5145\u4e14\u6e96\u78ba\u7684\u5716\u5f62Transformer\u8a13\u7df4\u7cfb\u7d71\u3002TorchGT \u5728\u4e0d\u540c\u5c64\u7d1a\u6700\u4f73\u5316\u8a13\u7df4\u3002\u5728\u6f14\u7b97\u6cd5\u5c64\u7d1a\uff0c\u901a\u904e\u5229\u7528\u5716\u5f62\u7a00\u758f\u6027\uff0cTorchGT \u5f15\u5165\u4e86\u4e00\u500b\u8a08\u7b97\u6548\u7387\u9ad8\u4e14\u6e96\u78ba\u6027\u5f97\u4ee5\u7dad\u6301\u7684\u96d9\u4ea4\u932f\u6ce8\u610f\u529b\u3002\u5728\u57f7\u884c\u968e\u6bb5\uff0cTorchGT \u4f7f\u7528\u5177\u5099\u901a\u8a0a\u8ca0\u8f09\u4f4e\u7684\u53e2\u96c6\u611f\u77e5\u5716\u5f62\u4e26\u884c\u6027\uff0c\u64f4\u5145\u4e86\u8de8\u5de5\u4f5c\u8005\u7684\u8a13\u7df4\u3002\u5728\u6838\u5fc3\u5c64\u7d1a\uff0c\u5f48\u6027\u904b\u7b97\u6539\u9020\u9032\u4e00\u6b65\u6700\u4f73\u5316\u904b\u7b97\uff0c\u4ee5\u52d5\u614b\u65b9\u5f0f\u6e1b\u5c11\u8a18\u61b6\u9ad4\u5b58\u53d6\u5ef6\u9072\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cTorchGT \u5c07\u8a13\u7df4\u63d0\u5347\u4e86 62.7 \u500d\uff0c\u4e26\u652f\u63f4\u9577\u9054 1M \u7684\u5716\u5f62\u5e8f\u5217\u9577\u5ea6\u3002", "author": "Meng Zhang et.al.", "authors": "Meng Zhang, Jie Sun, Qinghao Hu, Peng Sun, Zeke Wang, Yonggang Wen, Tianwei Zhang", "id": "2407.14106v1", "paper_url": "http://arxiv.org/abs/2407.14106v1", "repo": "null"}}