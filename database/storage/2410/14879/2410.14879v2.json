{"2410.14879": {"publish_time": "2024-10-18", "title": "Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM Agents", "paper_summary": "Passive tracking methods, such as phone and wearable sensing, have become\ndominant in monitoring human behaviors in modern ubiquitous computing studies.\nWhile there have been significant advances in machine-learning approaches to\ntranslate periods of raw sensor data to model momentary behaviors, (e.g.,\nphysical activity recognition), there still remains a significant gap in the\ntranslation of these sensing streams into meaningful, high-level, context-aware\ninsights that are required for various applications (e.g., summarizing an\nindividual's daily routine). To bridge this gap, experts often need to employ a\ncontext-driven sensemaking process in real-world studies to derive insights.\nThis process often requires manual effort and can be challenging even for\nexperienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore\nsolutions to address challenges with sensemaking. We follow a human-centered\ndesign process to identify needs and design, iterate, build, and evaluate Vital\nInsight (VI), a novel, LLM-assisted, prototype system to enable\nhuman-in-the-loop inference (sensemaking) and visualizations of multi-modal\npassive sensing data from smartphones and wearables. Using the prototype as a\ntechnology probe, we observe experts' interactions with it and develop an\nexpert sensemaking model that explains how experts move between direct data\nrepresentations and AI-supported inferences to explore, question, and validate\ninsights. Through this iterative process, we also synthesize and discuss a list\nof design implications for the design of future AI-augmented visualization\nsystems to better assist experts' sensemaking processes in multi-modal health\nsensing data.", "paper_summary_zh": "<paragraph>\u88ab\u52d5\u8ffd\u8e64\u65b9\u6cd5\uff0c\u4f8b\u5982\u624b\u6a5f\u548c\u7a7f\u6234\u5f0f\u611f\u6e2c\uff0c\u5df2\u6210\u70ba\u73fe\u4ee3\u666e\u9069\u8a08\u7b97\u7814\u7a76\u4e2d\u76e3\u6e2c\u4eba\u985e\u884c\u70ba\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u96d6\u7136\u6a5f\u5668\u5b78\u7fd2\u65b9\u6cd5\u5728\u5c07\u539f\u59cb\u611f\u6e2c\u5668\u6578\u64da\u8f49\u63db\u70ba\u77ac\u9593\u884c\u70ba\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u8eab\u9ad4\u6d3b\u52d5\u8b58\u5225\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\uff0c\u4f46\u5728\u5c07\u9019\u4e9b\u611f\u6e2c\u4e32\u6d41\u8f49\u63db\u70ba\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u6240\u9700\u7684 meaningful\u3001\u9ad8\u968e\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d1e\u5bdf\uff08\u4f8b\u5982\uff0c\u7e3d\u7d50\u500b\u4eba\u7684\u65e5\u5e38\u751f\u6d3b\uff09\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u986f\u8457\u5dee\u8ddd\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u5c08\u5bb6\u5011\u901a\u5e38\u9700\u8981\u5728\u73fe\u5be6\u4e16\u754c\u7684\u7814\u7a76\u4e2d\u63a1\u7528\u4e0a\u4e0b\u6587\u9a45\u52d5\u7684\u610f\u7fa9\u5efa\u69cb\u904e\u7a0b\u4f86\u7372\u5f97\u6d1e\u5bdf\u3002\u9019\u500b\u904e\u7a0b\u901a\u5e38\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u800c\u4e14\u7531\u65bc\u4eba\u985e\u884c\u70ba\u7684\u8907\u96dc\u6027\uff0c\u5373\u4f7f\u5c0d\u65bc\u7d93\u9a57\u8c50\u5bcc\u7684\u7814\u7a76\u4eba\u54e1\u4f86\u8aaa\u4e5f\u53ef\u80fd\u5177\u6709\u6311\u6230\u6027\u3002\u6211\u5011\u8207 21 \u4f4d\u5c08\u5bb6\u9032\u884c\u4e86\u4e09\u8f2a\u7528\u6236\u7814\u7a76\uff0c\u4ee5\u63a2\u7d22\u89e3\u6c7a\u610f\u7fa9\u5efa\u69cb\u6311\u6230\u7684\u65b9\u6848\u3002\u6211\u5011\u9075\u5faa\u4ee5\u4eba\u70ba\u672c\u7684\u8a2d\u8a08\u6d41\u7a0b\u4f86\u8b58\u5225\u9700\u6c42\uff0c\u4e26\u8a2d\u8a08\u3001\u8fed\u4ee3\u3001\u69cb\u5efa\u548c\u8a55\u4f30 Vital Insight (VI)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u3001LLM \u8f14\u52a9\u7684\u539f\u578b\u7cfb\u7d71\uff0c\u7528\u65bc\u5be6\u73fe\u4eba\u6a5f\u8ff4\u5708\u63a8\u7406\uff08\u610f\u7fa9\u5efa\u69cb\uff09\u4ee5\u53ca\u667a\u6167\u578b\u624b\u6a5f\u548c\u7a7f\u6234\u5f0f\u88dd\u7f6e\u7684\u591a\u6a21\u5f0f\u88ab\u52d5\u611f\u6e2c\u6578\u64da\u7684\u53ef\u8996\u5316\u3002\u4f7f\u7528\u8a72\u539f\u578b\u4f5c\u70ba\u6280\u8853\u63a2\u91dd\uff0c\u6211\u5011\u89c0\u5bdf\u5c08\u5bb6\u8207\u5176\u7684\u4e92\u52d5\uff0c\u4e26\u958b\u767c\u4e86\u4e00\u500b\u5c08\u5bb6\u610f\u7fa9\u5efa\u69cb\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u89e3\u91cb\u4e86\u5c08\u5bb6\u5982\u4f55\u5728\u76f4\u63a5\u6578\u64da\u8868\u793a\u548c AI \u652f\u63f4\u7684\u63a8\u8ad6\u4e4b\u9593\u79fb\u52d5\uff0c\u4ee5\u63a2\u7d22\u3001\u8cea\u7591\u548c\u9a57\u8b49\u6d1e\u5bdf\u3002\u900f\u904e\u9019\u500b\u8fed\u4ee3\u904e\u7a0b\uff0c\u6211\u5011\u9084\u7d9c\u5408\u4e26\u8a0e\u8ad6\u4e86\u4e00\u7cfb\u5217\u8a2d\u8a08\u610f\u6db5\uff0c\u4ee5\u7528\u65bc\u8a2d\u8a08\u672a\u4f86\u7684 AI \u589e\u5f37\u578b\u53ef\u8996\u5316\u7cfb\u7d71\uff0c\u5f9e\u800c\u66f4\u597d\u5730\u5354\u52a9\u5c08\u5bb6\u5728\u591a\u6a21\u5f0f\u5065\u5eb7\u611f\u6e2c\u6578\u64da\u4e2d\u7684\u610f\u7fa9\u5efa\u69cb\u904e\u7a0b\u3002</paragraph>\n", "author": "Jiachen Li et.al.", "authors": "Jiachen Li, Xiwen Li, Justin Steinberg, Akshat Choube, Bingsheng Yao, Xuhai Xu, Dakuo Wang, Elizabeth Mynatt, Varun Mishra", "id": "2410.14879v2", "paper_url": "http://arxiv.org/abs/2410.14879v2", "repo": "null"}}