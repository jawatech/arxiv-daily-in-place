{"2406.07036": {"publish_time": "2024-06-11", "title": "Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model", "paper_summary": "Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between source and target contexts. Analyzing\ncontribution scores during generation processes revealed that LLMs can be\nbiased towards previously generated tokens over corresponding source tokens,\nleading to unfaithful translations. To address this issue, we propose to\nencourage LLMs to pay more attention to the source context from both source and\ntarget perspectives in zeroshot prompting: 1) adjust source context attention\nweights; 2) suppress irrelevant target prefix influence; Additionally, we\npropose 3) avoiding over-reliance on the target prefix in instruction tuning.\nExperimental results from both human-collected unfaithfulness test sets\nfocusing on LLM-generated unfaithful translations and general test sets, verify\nour methods' effectiveness across multiple language pairs. Further human\nevaluation shows our method's efficacy in reducing hallucinatory translations\nand facilitating faithful translation generation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u8a9e\u8a00\u6a5f\u5668\u7ffb\u8b6f\u80fd\u529b\u3002\u7136\u800c\uff0c\u8207\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u98a8\u683c\u6a21\u578b\u4e0d\u540c\uff0c\u50c5\u89e3\u78bc\u5668\u7684 LLM \u7f3a\u4e4f\u4f86\u6e90\u548c\u76ee\u6a19\u8a9e\u5883\u4e4b\u9593\u7684\u660e\u78ba\u5c0d\u9f4a\u3002\u5206\u6790\u751f\u6210\u904e\u7a0b\u4e2d\u7684\u8ca2\u737b\u8a55\u5206\u986f\u793a\uff0cLLM \u53ef\u80fd\u504f\u5411\u65bc\u5148\u524d\u751f\u6210\u7684\u6a19\u8a18\uff0c\u800c\u4e0d\u662f\u5c0d\u61c9\u7684\u4f86\u6e90\u6a19\u8a18\uff0c\u5f9e\u800c\u5c0e\u81f4\u4e0d\u5fe0\u5be6\u7684\u7ffb\u8b6f\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u9f13\u52f5 LLM \u5f9e\u4f86\u6e90\u548c\u76ee\u6a19\u89d2\u5ea6\u5728\u96f6\u6b21\u63d0\u793a\u4e2d\u66f4\u591a\u5730\u95dc\u6ce8\u4f86\u6e90\u8a9e\u5883\uff1a1) \u8abf\u6574\u4f86\u6e90\u8a9e\u5883\u95dc\u6ce8\u6b0a\u91cd\uff1b2) \u6291\u5236\u7121\u95dc\u7684\u76ee\u6a19\u524d\u7db4\u5f71\u97ff\uff1b\u6b64\u5916\uff0c\u6211\u5011\u5efa\u8b70 3) \u907f\u514d\u904e\u5ea6\u4f9d\u8cf4\u8aaa\u660e\u8abf\u6574\u4e2d\u7684\u76ee\u6a19\u524d\u7db4\u3002\u5f9e\u5c08\u6ce8\u65bc LLM \u751f\u6210\u7684\u975e\u5fe0\u5be6\u7ffb\u8b6f\u548c\u4e00\u822c\u6e2c\u8a66\u96c6\u7684\u4eba\u985e\u6536\u96c6\u7684\u975e\u5fe0\u5be6\u6e2c\u8a66\u96c6\u4e2d\u7372\u5f97\u7684\u5be6\u9a57\u7d50\u679c\u9a57\u8b49\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u591a\u7a2e\u8a9e\u8a00\u5c0d\u4e2d\u7684\u6709\u6548\u6027\u3002\u9032\u4e00\u6b65\u7684\u4eba\u985e\u8a55\u4f30\u986f\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u6e1b\u5c11\u5e7b\u89ba\u7ffb\u8b6f\u548c\u4fc3\u9032\u5fe0\u5be6\u7ffb\u8b6f\u751f\u6210\u65b9\u9762\u7684\u6548\u529b\u3002", "author": "Hongbin Zhang et.al.", "authors": "Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang", "id": "2406.07036v1", "paper_url": "http://arxiv.org/abs/2406.07036v1", "repo": "https://github.com/AzureStarz/paying_attention_to_the_source"}}