{"2411.04539": {"publish_time": "2024-11-07", "title": "Best Practices for Distilling Large Language Models into BERT for Web Search Ranking", "paper_summary": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u5f37\u8abf\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u96f6\u6b21\u5b78\u7fd2\u76f8\u95dc\u6027\u6392\u5e8f\u5668\u7684\u986f\u8457\u6f5b\u529b\u3002\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u63d0\u793a\u5b78\u7fd2\uff0c\u900f\u904e\u7522\u751f\u6f5b\u5728\u6587\u4ef6\u7684\u6709\u5e8f\u6e05\u55ae\uff0c\u4f86\u8a55\u4f30\u67e5\u8a62\u8207\u6587\u4ef6\u4e4b\u9593\u7684\u76f8\u95dc\u6027\u3002\u5118\u7ba1\u5b83\u5011\u5f88\u6709\u524d\u9014\uff0c\u4f46\u8207 LLM \u76f8\u95dc\u7684\u9f90\u5927\u6210\u672c\u5c0d\u5b83\u5011\u5728\u5546\u696d\u641c\u5c0b\u7cfb\u7d71\u4e2d\u7684\u76f4\u63a5\u5be6\u4f5c\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u514b\u670d\u6b64\u969c\u7919\u4e26\u5145\u5206\u5229\u7528 LLM \u7684\u6587\u5b57\u6392\u5e8f\u529f\u80fd\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u5c07 LLM \u7684\u6392\u5e8f\u5c08\u696d\u77e5\u8b58\u8f49\u79fb\u5230\u66f4\u7cbe\u7c21\u7684\u6a21\u578b\uff08\u985e\u4f3c\u65bc BERT\uff09\u7684\u6280\u8853\uff0c\u4f7f\u7528\u6392\u5e8f\u640d\u5931\u4f86\u5be6\u73fe\u8cc7\u6e90\u5bc6\u96c6\u5ea6\u8f03\u4f4e\u7684\u6a21\u578b\u7684\u90e8\u7f72\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\u4f86\u589e\u5f37 LLM \u7684\u8a13\u7df4\uff0c\u5c07\u67e5\u8a62\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u5c07\u9ede\u64ca\u7684\u6a19\u984c\u548c\u6458\u8981\u4f5c\u70ba\u8f38\u51fa\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u6392\u5e8f\u640d\u5931\u5c0d LLM \u9032\u884c\u76e3\u7763\u5f0f\u5fae\u8abf\uff0c\u5c07\u6700\u7d42\u4ee3\u5e63\u6307\u5b9a\u70ba\u6574\u500b\u53e5\u5b50\u7684\u4ee3\u8868\u3002\u7531\u65bc\u81ea\u8ff4\u6b78\u8a9e\u8a00\u6a21\u578b\u7684\u56fa\u6709\u7279\u6027\uff0c\u53ea\u6709\u6700\u7d42\u4ee3\u5e63 </s> \u624d\u80fd\u56ca\u62ec\u6240\u6709\u524d\u4e00\u500b\u4ee3\u5e63\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6df7\u5408\u9ede\u5f0f\u548c\u908a\u969b MSE \u640d\u5931\uff0c\u5c07 LLM \u7684\u6392\u5e8f\u77e5\u8b58\u8f49\u79fb\u5230\u8f03\u5c0f\u7684\u6a21\u578b\uff08\u5982 BERT\uff09\u4e2d\u3002\u6b64\u65b9\u6cd5\u70ba\u8cc7\u6e90\u9650\u5236\u56b4\u683c\u7684\u74b0\u5883\u5275\u9020\u4e86\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u96e2\u7dda\u548c\u7dda\u4e0a\u8a55\u4f30\u90fd\u8b49\u5be6\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u800c\u6211\u5011\u7684\u6a21\u578b\u5df2\u65bc 2024 \u5e74 2 \u6708\u6210\u529f\u6574\u5408\u5230\u5546\u696d\u7db2\u8def\u641c\u5c0b\u5f15\u64ce\u4e2d\u3002", "author": "Dezhi Ye et.al.", "authors": "Dezhi Ye, Junwei Hu, Jiabin Fan, Bowen Tian, Jie Liu, Haijin Liang, Jin Ma", "id": "2411.04539v1", "paper_url": "http://arxiv.org/abs/2411.04539v1", "repo": "null"}}