{"2501.13652": {"publish_time": "2025-01-23", "title": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models", "paper_summary": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u900f\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u5df2\u53d6\u5f97\u663e\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5904\u7406\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\uff0c\u5b83\u4eec\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u6211\u4eec\u9488\u5bf9 MLLM \u5f15\u5165\u4e86\u8bed\u8a00\u5f15\u5bfc\u89c6\u89c9\u6807\u8bb0\u4fee\u526a (LVPruning)\uff0c\u8fd9\u662f\u4e00\u79cd\u6709\u6548\u4e14\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002LVPruning \u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u6839\u636e\u89c6\u89c9\u6807\u8bb0\u4e0e\u8bed\u8a00\u6807\u8bb0\u7684\u4ea4\u4e92\u4f5c\u7528\u8ba1\u7b97\u89c6\u89c9\u6807\u8bb0\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u5b9a\u8981\u4fee\u526a\u54ea\u4e9b\u6807\u8bb0\u3002\u91cd\u8981\u7684\u662f\uff0cLVPruning \u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u539f\u59cb MLLM \u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6574\u5408\uff0c\u8fd9\u4f7f\u5f97 LVPruning \u6613\u4e8e\u5e94\u7528\u6216\u79fb\u9664\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLVPruning \u53ef\u4ee5\u6709\u6548\u5730\u5c06 LLaVA-1.5 \u4e2d\u5c42\u591a\u8fbe 90% \u7684\u89c6\u89c9\u6807\u8bb0\u51cf\u5c11\uff0c\u4ece\u800c\u4f7f\u6bcf\u79d2\u63a8\u7406\u6d6e\u70b9\u8fd0\u7b97 (TFLOPs) \u51cf\u5c11 62.1%\uff0c\u5728\u4e5d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u635f\u5931\u4ec5\u4e3a 0.45%\u3002", "author": "Yizheng Sun et.al.", "authors": "Yizheng Sun, Yanze Xin, Hao Li, Jingyuan Sun, Chenghua Lin, Riza Batista-Navarro", "id": "2501.13652v1", "paper_url": "http://arxiv.org/abs/2501.13652v1", "repo": "null"}}