{"2410.22071": {"publish_time": "2024-10-29", "title": "Distinguishing Ignorance from Error in LLM Hallucinations", "paper_summary": "Large language models (LLMs) are susceptible to hallucinations-outputs that\nare ungrounded, factually incorrect, or inconsistent with prior generations. We\nfocus on close-book Question Answering (CBQA), where previous work has not\nfully addressed the distinction between two possible kinds of hallucinations,\nnamely, whether the model (1) does not hold the correct answer in its\nparameters or (2) answers incorrectly despite having the required knowledge. We\nargue that distinguishing these cases is crucial for detecting and mitigating\nhallucinations. Specifically, case (2) may be mitigated by intervening in the\nmodel's internal computation, as the knowledge resides within the model's\nparameters. In contrast, in case (1) there is no parametric knowledge to\nleverage for mitigation, so it should be addressed by resorting to an external\nknowledge source or abstaining. To help distinguish between the two cases, we\nintroduce Wrong Answer despite having Correct Knowledge (WACK), an approach for\nconstructing model-specific datasets for the second hallucination type. Our\nprobing experiments indicate that the two kinds of hallucinations are\nrepresented differently in the model's inner states. Next, we show that\ndatasets constructed using WACK exhibit variations across models, demonstrating\nthat even when models share knowledge of certain facts, they still vary in the\nspecific examples that lead to hallucinations. Finally, we show that training a\nprobe on our WACK datasets leads to better hallucination detection of case (2)\nhallucinations than using the common generic one-size-fits-all datasets. The\ncode is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5bb9\u6613\u51fa\u73fe\u5e7b\u89ba\u8f38\u51fa\uff0c\u9019\u4e9b\u8f38\u51fa\u6c92\u6709\u6839\u64da\u3001\u4e8b\u5be6\u4e0d\u6b63\u78ba\u6216\u8207\u5148\u524d\u7684\u4e16\u4ee3\u4e0d\u4e00\u81f4\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u9589\u5377\u554f\u7b54 (CBQA)\uff0c\u5176\u4e2d\u5148\u524d\u7684\u7814\u7a76\u5c1a\u672a\u5b8c\u5168\u89e3\u6c7a\u5169\u7a2e\u53ef\u80fd\u7684\u5e7b\u89ba\u4e4b\u9593\u7684\u5340\u5225\uff0c\u5373\u6a21\u578b (1) \u6c92\u6709\u5728\u5176\u53c3\u6578\u4e2d\u4fdd\u5b58\u6b63\u78ba\u7684\u7b54\u6848\uff0c\u6216 (2) \u5118\u7ba1\u64c1\u6709\u5fc5\u8981\u7684\u77e5\u8b58\uff0c\u4f46\u56de\u7b54\u4e0d\u6b63\u78ba\u3002\u6211\u5011\u8a8d\u70ba\u5340\u5206\u9019\u4e9b\u60c5\u6cc1\u5c0d\u65bc\u6aa2\u6e2c\u548c\u6e1b\u8f15\u5e7b\u89ba\u81f3\u95dc\u91cd\u8981\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6848\u4f8b (2) \u53ef\u4ee5\u901a\u904e\u5e72\u9810\u6a21\u578b\u7684\u5167\u90e8\u8a08\u7b97\u4f86\u6e1b\u8f15\uff0c\u56e0\u70ba\u77e5\u8b58\u5b58\u5728\u65bc\u6a21\u578b\u7684\u53c3\u6578\u4e2d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u6848\u4f8b (1) \u4e2d\u6c92\u6709\u53c3\u6578\u77e5\u8b58\u53ef\u4ee5\u5229\u7528\u4f86\u6e1b\u8f15\uff0c\u56e0\u6b64\u61c9\u8a72\u8a34\u8af8\u65bc\u5916\u90e8\u77e5\u8b58\u4f86\u6e90\u6216\u653e\u68c4\u3002\u70ba\u4e86\u5e6b\u52a9\u5340\u5206\u9019\u5169\u7a2e\u60c5\u6cc1\uff0c\u6211\u5011\u5f15\u5165\u4e86\u932f\u8aa4\u7b54\u6848\u5118\u7ba1\u6709\u6b63\u78ba\u77e5\u8b58 (WACK)\uff0c\u9019\u662f\u4e00\u7a2e\u70ba\u7b2c\u4e8c\u7a2e\u985e\u578b\u7684\u5e7b\u89ba\u69cb\u5efa\u7279\u5b9a\u65bc\u6a21\u578b\u7684\u6578\u64da\u96c6\u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u63a2\u6e2c\u5be6\u9a57\u8868\u660e\uff0c\u9019\u5169\u7a2e\u5e7b\u89ba\u5728\u6a21\u578b\u7684\u5167\u90e8\u72c0\u614b\u4e2d\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u8868\u793a\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4f7f\u7528 WACK \u69cb\u5efa\u7684\u6578\u64da\u96c6\u5728\u6a21\u578b\u4e4b\u9593\u8868\u73fe\u51fa\u5dee\u7570\uff0c\u9019\u8868\u660e\u5373\u4f7f\u6a21\u578b\u5171\u4eab\u67d0\u4e9b\u4e8b\u5be6\u7684\u77e5\u8b58\uff0c\u5b83\u5011\u5728\u5c0e\u81f4\u5e7b\u89ba\u7684\u5177\u9ad4\u793a\u4f8b\u4e2d\u4ecd\u7136\u6709\u6240\u4e0d\u540c\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5728\u6211\u5011\u7684 WACK \u6578\u64da\u96c6\u4e0a\u8a13\u7df4\u63a2\u6e2c\u5668\u6bd4\u4f7f\u7528\u901a\u7528\u7684\u901a\u7528\u7d71\u4e00\u6578\u64da\u96c6\u80fd\u66f4\u597d\u5730\u6aa2\u6e2c\u6848\u4f8b (2) \u5e7b\u89ba\u3002\u4ee3\u78bc\u53ef\u5728\nhttps://github.com/technion-cs-nlp/hallucination-mitigation \u7372\u5f97\u3002</paragraph>", "author": "Adi Simhi et.al.", "authors": "Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov", "id": "2410.22071v1", "paper_url": "http://arxiv.org/abs/2410.22071v1", "repo": "https://github.com/technion-cs-nlp/hallucination-mitigation"}}