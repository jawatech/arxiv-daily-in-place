{"2410.19382": {"publish_time": "2024-10-25", "title": "Multi-Agent Reinforcement Learning with Selective State-Space Models", "paper_summary": "The Transformer model has demonstrated success across a wide range of\ndomains, including in Multi-Agent Reinforcement Learning (MARL) where the\nMulti-Agent Transformer (MAT) has emerged as a leading algorithm in the field.\nThe Transformer model has demonstrated success across a wide range of domains,\nincluding in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent\nTransformer (MAT) has emerged as a leading algorithm in the field. However, a\nsignificant drawback of Transformer models is their quadratic computational\ncomplexity relative to input size, making them computationally expensive when\nscaling to larger inputs. This limitation restricts MAT's scalability in\nenvironments with many agents. Recently, State-Space Models (SSMs) have gained\nattention due to their computational efficiency, but their application in MARL\nremains unexplored. In this work, we investigate the use of Mamba, a recent\nSSM, in MARL and assess whether it can match the performance of MAT while\nproviding significant improvements in efficiency. We introduce a modified\nversion of MAT that incorporates standard and bi-directional Mamba blocks, as\nwell as a novel \"cross-attention\" Mamba block. Extensive testing shows that our\nMulti-Agent Mamba (MAM) matches the performance of MAT across multiple standard\nmulti-agent environments, while offering superior scalability to larger agent\nscenarios. This is significant for the MARL community, because it indicates\nthat SSMs could replace Transformers without compromising performance, whilst\nalso supporting more effective scaling to higher numbers of agents. Our project\npage is available at https://sites.google.com/view/multi-agent-mamba .", "paper_summary_zh": "Transformer \u6a21\u578b\u5df2\u5728\u5ee3\u6cdb\u7684\u9818\u57df\u4e2d\u5c55\u73fe\u6210\u529f\uff0c\u5305\u62ec\u5728\u591a\u667a\u80fd\u9ad4\u5f37\u5316\u5b78\u7fd2 (MARL) \u4e2d\uff0c\u5176\u4e2d Multi-Agent Transformer (MAT) \u5df2\u6210\u70ba\u8a72\u9818\u57df\u7684\u9818\u5148\u6f14\u7b97\u6cd5\u3002Transformer \u6a21\u578b\u5df2\u5728\u5ee3\u6cdb\u7684\u9818\u57df\u4e2d\u5c55\u73fe\u6210\u529f\uff0c\u5305\u62ec\u5728\u591a\u667a\u80fd\u9ad4\u5f37\u5316\u5b78\u7fd2 (MARL) \u4e2d\uff0c\u5176\u4e2d Multi-Agent Transformer (MAT) \u5df2\u6210\u70ba\u8a72\u9818\u57df\u7684\u9818\u5148\u6f14\u7b97\u6cd5\u3002\u7136\u800c\uff0cTransformer \u6a21\u578b\u7684\u4e00\u9805\u91cd\u5927\u7f3a\u9ede\u662f\u5176\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u76f8\u5c0d\u65bc\u8f38\u5165\u5927\u5c0f\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u5728\u64f4\u5c55\u5230\u8f03\u5927\u8f38\u5165\u6642\u5728\u8a08\u7b97\u4e0a\u5f88\u6602\u8cb4\u3002\u6b64\u9650\u5236\u9650\u5236\u4e86 MAT \u5728\u5177\u6709\u8a31\u591a\u667a\u80fd\u9ad4\u7684\u74b0\u5883\u4e2d\u7684\u53ef\u64f4\u5145\u6027\u3002\u6700\u8fd1\uff0c\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM) \u56e0\u5176\u8a08\u7b97\u6548\u7387\u800c\u53d7\u5230\u95dc\u6ce8\uff0c\u4f46\u5b83\u5011\u5728 MARL \u4e2d\u7684\u61c9\u7528\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5728 MARL \u4e2d\u4f7f\u7528 Mamba\uff08\u4e00\u7a2e\u6700\u8fd1\u7684 SSM\uff09\uff0c\u4e26\u8a55\u4f30\u5b83\u662f\u5426\u53ef\u4ee5\u5728\u63d0\u4f9b\u986f\u8457\u6548\u7387\u63d0\u5347\u7684\u540c\u6642\uff0c\u5339\u914d MAT \u7684\u6548\u80fd\u3002\u6211\u5011\u5f15\u9032\u4e86\u4e00\u500b\u4fee\u6539\u7248\u672c\u7684 MAT\uff0c\u5176\u4e2d\u5305\u542b\u6a19\u6e96\u548c\u96d9\u5411 Mamba \u5340\u584a\uff0c\u4ee5\u53ca\u4e00\u500b\u65b0\u7a4e\u7684\u300c\u4ea4\u53c9\u6ce8\u610f\u529b\u300dMamba \u5340\u584a\u3002\u5ee3\u6cdb\u7684\u6e2c\u8a66\u8868\u660e\uff0c\u6211\u5011\u7684 Multi-Agent Mamba (MAM) \u5728\u591a\u500b\u6a19\u6e96\u591a\u667a\u80fd\u9ad4\u74b0\u5883\u4e2d\u8207 MAT \u7684\u6548\u80fd\u76f8\u5339\u914d\uff0c\u540c\u6642\u70ba\u8f03\u5927\u7684\u667a\u80fd\u9ad4\u5834\u666f\u63d0\u4f9b\u5353\u8d8a\u7684\u53ef\u64f4\u5145\u6027\u3002\u9019\u5c0d MARL \u793e\u7fa4\u4f86\u8aaa\u5f88\u91cd\u8981\uff0c\u56e0\u70ba\u9019\u8868\u793a SSM \u53ef\u4ee5\u53d6\u4ee3 Transformer \u800c\u4e0d\u6703\u640d\u5bb3\u6548\u80fd\uff0c\u540c\u6642\u9084\u80fd\u652f\u63f4\u66f4\u6709\u6548\u5730\u64f4\u5c55\u5230\u66f4\u591a\u6578\u91cf\u7684\u667a\u80fd\u9ad4\u3002\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\u53ef\u5728 https://sites.google.com/view/multi-agent-mamba \u4e2d\u53d6\u5f97\u3002", "author": "Jemma Daniel et.al.", "authors": "Jemma Daniel, Ruan de Kock, Louay Ben Nessir, Sasha Abramowitz, Omayma Mahjoub, Wiem Khlifi, Claude Formanek, Arnu Pretorius", "id": "2410.19382v1", "paper_url": "http://arxiv.org/abs/2410.19382v1", "repo": "null"}}