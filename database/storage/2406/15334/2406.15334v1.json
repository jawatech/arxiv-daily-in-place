{"2406.15334": {"publish_time": "2024-06-21", "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning", "paper_summary": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.", "paper_summary_zh": "\u6700\u8fd1\u5728\u5c11\u6837\u672c\u5b78\u7fd2\u4e2d\u4ea4\u932f\u7684\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u53d6\u5f97\u6210\u529f\uff0c\u9019\u8868\u793a\u4f7f\u7528\u5927\u91cf\u7bc4\u4f8b\u7684\u8108\u7d61\u4e2d\u5b78\u7fd2 (ICL) \u5c0d\u65bc\u5b78\u7fd2\u65b0\u4efb\u52d9\u800c\u8a00\u53ef\u80fd\u662f\u5f88\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u9019\u7a2e\u591a\u6a23\u672c\u591a\u6a21\u614b ICL \u8a2d\u5b9a\u6709\u4e00\u500b\u95dc\u9375\u554f\u984c\uff1a\u5b83\u57fa\u672c\u4e0a\u53d7\u5230\u9810\u8a13\u7df4\u6642\u8a2d\u5b9a\u7684\u6a21\u578b\u8108\u7d61\u9577\u5ea6\u9650\u5236\u3002\u9019\u500b\u554f\u984c\u5728\u9700\u8981\u8655\u7406\u6587\u5b57\u548c\u5f71\u50cf\u7684\u591a\u6a21\u614b\u9818\u57df\u4e2d\u7279\u5225\u660e\u986f\uff0c\u9700\u8981\u984d\u5916\u7684\u7b26\u865f\u3002\u9019\u4fc3\u4f7f\u9700\u8981\u4e00\u7a2e\u591a\u6a21\u614b\u65b9\u6cd5\uff0c\u5728\u4e0d\u9032\u884c\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u5c07\u591a\u500b\u6a23\u672c\u58d3\u7e2e\u6210\u66f4\u5c11\u7684\u7b26\u865f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8b93 LMM \u80fd\u5920\u57f7\u884c\u591a\u6a21\u614b\u3001\u591a\u6a23\u672c\u8108\u7d61\u4e2d\u5b78\u7fd2\uff0c\u65b9\u6cd5\u662f\u5229\u7528\u591a\u6a21\u614b\u4efb\u52d9\u5411\u91cf (MTV) - \u6a21\u578b\u6ce8\u610f\u529b\u6b0a\u91cd\u4e2d\u58d3\u7e2e\u7684\u8108\u7d61\u4e2d\u7bc4\u4f8b\u7684\u7dca\u6e4a\u96b1\u5f0f\u8868\u793a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u8b49\u660e\u4e86 LMM \u4e2d\u5b58\u5728\u9019\u7a2e MTV\uff0c\u7136\u5f8c\u5229\u7528\u9019\u4e9b\u63d0\u53d6\u7684 MTV \u4f86\u8b93\u591a\u6a23\u672c\u8108\u7d61\u4e2d\u5b78\u7fd2\u80fd\u5920\u7528\u65bc\u5404\u7a2e\u8996\u89ba\u548c\u8a9e\u8a00\u4efb\u52d9\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cMTV \u53ef\u4ee5\u96a8\u8457\u58d3\u7e2e\u6a23\u672c\u7684\u6578\u91cf\u800c\u64f4\u5c55\u6548\u80fd\uff0c\u4e26\u5728\u6c92\u6709\u984d\u5916\u8108\u7d61\u9577\u5ea6\u7684\u60c5\u6cc1\u4e0b\u6cdb\u5316\u5230\u985e\u4f3c\u7684\u9818\u57df\u5916\u4efb\u52d9\u4ee5\u9032\u884c\u63a8\u8ad6\u3002", "author": "Brandon Huang et.al.", "authors": "Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, Roei Herzig", "id": "2406.15334v1", "paper_url": "http://arxiv.org/abs/2406.15334v1", "repo": "null"}}