{"2405.14428": {"publish_time": "2024-05-23", "title": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs", "paper_summary": "Modern large language models (LLMs) have established state-of-the-art\nperformance through architectural improvements, but still require significant\ncomputational cost for inference. In an effort to reduce the inference cost,\npost-training quantization (PTQ) has become a popular approach, quantizing\nweights and activations to lower precision, such as INT8. In this paper, we\nreveal the challenges of activation quantization in GLU variants, which are\nwidely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\nThe problem is that severe local quantization errors, caused by excessive\nmagnitudes of activation in GLU variants, significantly degrade the performance\nof the quantized LLM. We denote these activations as activation spikes. Our\nfurther observations provide a systematic pattern of activation spikes: 1) The\nactivation spikes occur in the FFN of specific layers, particularly in the\nearly and late layers, 2) The activation spikes are dedicated to a couple of\ntokens, rather than being shared across a sequence. Based on our observations,\nwe propose two empirical methods, Quantization-free Module (QFeM) and\nQuantization-free Prefix (QFeP), to isolate the activation spikes during\nquantization. Our extensive experiments validate the effectiveness of the\nproposed methods for the activation quantization, especially with\ncoarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3,\nMistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the\ncurrent alleviation techniques (e.g., SmoothQuant) that fail to control the\nactivation spikes. Code is available at\nhttps://github.com/onnoo/activation-spikes.", "paper_summary_zh": "<paragraph>\u73fe\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u900f\u904e\u67b6\u69cb\u6539\u5584\u5efa\u7acb\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4f46\u63a8\u8ad6\u4ecd\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u6210\u672c\u3002\u70ba\u4e86\u964d\u4f4e\u63a8\u8ad6\u6210\u672c\uff0c\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u5df2\u6210\u70ba\u4e00\u7a2e\u666e\u904d\u7684\u505a\u6cd5\uff0c\u5c07\u6b0a\u91cd\u548c\u555f\u7528\u91cf\u5316\u70ba\u8f03\u4f4e\u7684\u7cbe\u5ea6\uff0c\u4f8b\u5982 INT8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63ed\u9732\u4e86 GLU \u8b8a\u9ad4\u4e2d\u555f\u7528\u91cf\u5316\u7684\u6311\u6230\uff0c\u9019\u4e9b\u8b8a\u9ad4\u5ee3\u6cdb\u7528\u65bc\u73fe\u4ee3 LLM \u7684\u524d\u994b\u7db2\u8def (FFN)\uff0c\u4f8b\u5982 LLaMA \u5bb6\u65cf\u3002\u554f\u984c\u5728\u65bc\uff0cGLU \u8b8a\u9ad4\u4e2d\u904e\u5927\u7684\u555f\u7528\u91cf\u6703\u9020\u6210\u56b4\u91cd\u7684\u5c40\u90e8\u91cf\u5316\u8aa4\u5dee\uff0c\u5927\u5e45\u964d\u4f4e\u91cf\u5316 LLM \u7684\u6548\u80fd\u3002\u6211\u5011\u5c07\u9019\u4e9b\u555f\u7528\u7a31\u70ba\u555f\u7528\u5c16\u5cf0\u3002\u6211\u5011\u7684\u9032\u4e00\u6b65\u89c0\u5bdf\u63d0\u4f9b\u4e86\u555f\u7528\u5c16\u5cf0\u7684\u7cfb\u7d71\u6a21\u5f0f\uff1a1) \u555f\u7528\u5c16\u5cf0\u767c\u751f\u5728\u7279\u5b9a\u5c64\u7684 FFN\uff0c\u7279\u5225\u662f\u5728\u65e9\u671f\u548c\u665a\u671f\u5c64\uff0c2) \u555f\u7528\u5c16\u5cf0\u5c08\u5c6c\u65bc\u5e7e\u500b token\uff0c\u800c\u4e0d\u662f\u5728\u5e8f\u5217\u4e2d\u5171\u4eab\u3002\u6839\u64da\u6211\u5011\u7684\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa\u5169\u7a2e\u7d93\u9a57\u65b9\u6cd5\uff0c\u7121\u91cf\u5316\u6a21\u7d44 (QFeM) \u548c\u7121\u91cf\u5316\u524d\u7db4 (QFeP)\uff0c\u4ee5\u5728\u91cf\u5316\u671f\u9593\u9694\u96e2\u555f\u7528\u5c16\u5cf0\u3002\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u9a57\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c0d\u555f\u7528\u91cf\u5316\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u5177\u6709 GLU \u8b8a\u9ad4\u7684\u6700\u65b0 LLM\uff08\u5305\u62ec LLaMA-2/3\u3001Mistral\u3001Mixtral\u3001SOLAR \u548c Gemma\uff09\u7684\u7c97\u7c92\u5ea6\u65b9\u6848\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7684\u9019\u4e9b\u65b9\u6cd5\u589e\u5f37\u4e86\u76ee\u524d\u7684\u7de9\u89e3\u6280\u8853\uff08\u4f8b\u5982 SmoothQuant\uff09\uff0c\u800c\u9019\u4e9b\u6280\u8853\u7121\u6cd5\u63a7\u5236\u555f\u7528\u5c16\u5cf0\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/onnoo/activation-spikes \u53d6\u5f97\u3002</paragraph>", "author": "Jaewoo Yang et.al.", "authors": "Jaewoo Yang, Hayun Kim, Younghoon Kim", "id": "2405.14428v1", "paper_url": "http://arxiv.org/abs/2405.14428v1", "repo": "https://github.com/onnoo/activation-spikes"}}