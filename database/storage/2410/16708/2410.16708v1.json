{"2410.16708": {"publish_time": "2024-10-22", "title": "Atomic Fact Decomposition Helps Attributed Question Answering", "paper_summary": "Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution.", "paper_summary_zh": "<paragraph>\u6b78\u56e0\u5f0f\u554f\u7b54 (AQA) \u7684\u76ee\u6a19\u662f\u91dd\u5c0d\u7279\u5b9a\u554f\u984c\u63d0\u4f9b\u53ef\u4fe1\u7684\u7b54\u6848\u548c\u53ef\u9760\u7684\u6b78\u56e0\u5831\u544a\u3002\u64f7\u53d6\u662f\u4e00\u7a2e\u5ee3\u6cdb\u63a1\u7528\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5169\u7a2e\u4e00\u822c\u7bc4\u4f8b\uff1a\u64f7\u53d6\u518d\u95b1\u8b80 (RTR) \u548c\u4e8b\u5f8c\u64f7\u53d6\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u719f\u7df4\u5ea6\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u54e1\u5c0d AQA \u7522\u751f\u8d8a\u4f86\u8d8a\u6fc3\u539a\u7684\u8208\u8da3\u3002\u7136\u800c\uff0c\u5373\u4f7f\u63a1\u7528 LLM\uff0c\u57fa\u65bc RTR \u7684 AQA \u4ecd\u5e38\u5e38\u6703\u53d7\u5230\u4e0d\u76f8\u95dc\u77e5\u8b58\u548c\u5feb\u901f\u8b8a\u52d5\u7684\u8cc7\u8a0a\u5f71\u97ff\uff0c\u800c\u57fa\u65bc\u4e8b\u5f8c\u64f7\u53d6\u7684 AQA \u5247\u96e3\u4ee5\u7406\u89e3\u5177\u6709\u8907\u96dc\u908f\u8f2f\u7684\u9577\u7bc7\u7b54\u6848\uff0c\u4e26\u7cbe\u78ba\u627e\u51fa\u9700\u8981\u4fee\u6539\u7684\u5167\u5bb9\uff0c\u540c\u6642\u4fdd\u7559\u539f\u59cb\u610f\u5716\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u539f\u5b50\u4e8b\u5be6\u5206\u89e3\u7684\u64f7\u53d6\u548c\u7de8\u8f2f (ARE) \u67b6\u69cb\uff0c\u5b83\u900f\u904e\u6307\u4ee4\u8abf\u6574\u7684 LLM \u5c07\u7522\u751f\u7684\u9577\u7bc7\u7b54\u6848\u5206\u89e3\u70ba\u5206\u5b50\u5b50\u53e5\u548c\u539f\u5b50\u4e8b\u5be6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6307\u4ee4\u8abf\u6574\u7684 LLM \u6703\u4f7f\u7528\u5f9e\u5927\u898f\u6a21\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u7522\u751f\u7684\u7d50\u69cb\u826f\u597d\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\u3002\u6b64\u7a0b\u5e8f\u5305\u542b\u5f9e\u7279\u5b9a\u5be6\u9ad4\u96c6\u5408\u4e2d\u64f7\u53d6\u4e00\u8df3\u9130\u5c45\uff0c\u4e26\u5c07\u7d50\u679c\u8f49\u63db\u70ba\u9023\u8cab\u7684\u9577\u7bc7\u6587\u5b57\u3002\u96a8\u5f8c\uff0cARE \u6703\u5229\u7528\u641c\u5c0b\u5f15\u64ce\u64f7\u53d6\u8207\u539f\u5b50\u4e8b\u5be6\u76f8\u95dc\u7684\u8b49\u64da\uff0c\u5c07\u9019\u4e9b\u8b49\u64da\u8f38\u5165\u5230\u57fa\u65bc LLM \u7684\u9a57\u8b49\u5668\u4e2d\uff0c\u4ee5\u78ba\u5b9a\u4e8b\u5be6\u662f\u5426\u9700\u8981\u64f4\u5145\u4ee5\u4f9b\u91cd\u65b0\u64f7\u53d6\u6216\u7de8\u8f2f\u3002\u6b64\u5916\uff0c\u7de8\u8f2f\u5f8c\u7684\u7d50\u679c\u6703\u56de\u6eaf\u5230\u539f\u59cb\u7b54\u6848\uff0c\u4e26\u6839\u64da\u5206\u5b50\u5b50\u53e5\u548c\u539f\u5b50\u4e8b\u5be6\u4e4b\u9593\u7684\u95dc\u4fc2\u5f59\u6574\u8b49\u64da\u3002\u5ee3\u6cdb\u7684\u8a55\u4f30\u986f\u793a\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u73fe\u6709\u6280\u8853\uff0c\u4e26\u984d\u5916\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u6307\u6a19 $Attr_{p}$\uff0c\u7528\u65bc\u8a55\u4f30\u8b49\u64da\u6b78\u56e0\u7684\u7cbe\u6e96\u5ea6\u3002</paragraph>", "author": "Zhichao Yan et.al.", "authors": "Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z. Pan", "id": "2410.16708v1", "paper_url": "http://arxiv.org/abs/2410.16708v1", "repo": "null"}}