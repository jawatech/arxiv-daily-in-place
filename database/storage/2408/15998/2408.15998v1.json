{"2408.15998": {"publish_time": "2024-08-28", "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders", "paper_summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle", "paper_summary_zh": "\u6e96\u78ba\u5730\u8a6e\u91cb\u8907\u96dc\u7684\u8996\u89ba\u8cc7\u8a0a\u7684\u80fd\u529b\u662f\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u4e00\u500b\u95dc\u9375\u8b70\u984c\u3002\u6700\u8fd1\u7684\u7814\u7a76\u6307\u51fa\uff0c\u589e\u5f37\u7684\u8996\u89ba\u611f\u77e5\u80fd\u986f\u8457\u6e1b\u5c11\u5e7b\u89ba\uff0c\u4e26\u6539\u5584\u5c0d\u89e3\u6790\u5ea6\u654f\u611f\u4efb\u52d9\u7684\u57f7\u884c\uff0c\u4f8b\u5982\u5149\u5b78\u5b57\u5143\u8fa8\u8b58\u548c\u6587\u4ef6\u5206\u6790\u3002\u8a31\u591a\u6700\u8fd1\u7684 MLLM \u4f7f\u7528\u8996\u89ba\u7de8\u78bc\u5668\u7684\u6df7\u5408\u4f86\u9054\u6210\u6b64\u76ee\u6a19\u3002\u5118\u7ba1\u5b83\u5011\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7d71\u6027\u7684\u6bd4\u8f03\u548c\u8a73\u7d30\u7684\u6d88\u878d\u7814\u7a76\u4f86\u63a2\u8a0e\u95dc\u9375\u9762\u5411\uff0c\u4f8b\u5982\u5c08\u5bb6\u9078\u64c7\u548c\u6574\u5408\u591a\u500b\u8996\u89ba\u5c08\u5bb6\u3002\u672c\u7814\u7a76\u91dd\u5c0d\u4f7f\u7528\u8996\u89ba\u7de8\u78bc\u5668\u548c\u89e3\u6790\u5ea6\u7684 MLLM \u6df7\u5408\u63d0\u4f9b\u4e86\u5ee3\u6cdb\u7684\u8a2d\u8a08\u7a7a\u9593\u63a2\u8a0e\u3002\u6211\u5011\u7684\u767c\u73fe\u63ed\u793a\u4e86\u5404\u7a2e\u73fe\u6709\u7b56\u7565\u4e2d\u5e7e\u500b\u5171\u540c\u7684\u5e95\u5c64\u539f\u5247\uff0c\u9032\u800c\u5f15\u5c0e\u51fa\u7c21\u5316\u4f46\u6709\u6548\u7684\u8a2d\u8a08\u65b9\u6cd5\u3002\u6211\u5011\u767c\u73fe\uff0c\u50c5\u50c5\u9023\u7d50\u4f86\u81ea\u4e00\u7d44\u4e92\u88dc\u8996\u89ba\u7de8\u78bc\u5668\u7684\u8996\u89ba\u4ee3\u78bc\uff0c\u5c31\u548c\u66f4\u8907\u96dc\u7684\u6df7\u5408\u67b6\u69cb\u6216\u7b56\u7565\u4e00\u6a23\u6709\u6548\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u9810\u5c0d\u9f4a\uff0c\u4ee5\u5f4c\u5408\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u7684\u7de8\u78bc\u5668\u548c\u8a9e\u8a00\u4ee3\u78bc\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u589e\u5f37\u6a21\u578b\u7684\u76f8\u5e72\u6027\u3002\u7531\u6b64\u7522\u751f\u7684 MLLM \u5bb6\u65cf Eagle\uff0c\u5728\u4e3b\u8981\u7684 MLLM \u57fa\u6e96\u4e0a\u8d85\u8d8a\u4e86\u5176\u4ed6\u9818\u5148\u7684\u958b\u6e90\u6a21\u578b\u3002\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/NVlabs/Eagle", "author": "Min Shi et.al.", "authors": "Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu", "id": "2408.15998v1", "paper_url": "http://arxiv.org/abs/2408.15998v1", "repo": "https://github.com/nvlabs/eagle"}}