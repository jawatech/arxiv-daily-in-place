{"2407.21384": {"publish_time": "2024-07-31", "title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction", "paper_summary": "Document-level relation extraction (DocRE) aims to extract relations between\nentities from unstructured document text. Compared to sentence-level relation\nextraction, it requires more complex semantic understanding from a broader text\ncontext. Currently, some studies are utilizing logical rules within evidence\nsentences to enhance the performance of DocRE. However, in the data without\nprovided evidence sentences, researchers often obtain a list of evidence\nsentences for the entire document through evidence retrieval (ER). Therefore,\nDocRE suffers from two challenges: firstly, the relevance between evidence and\nentity pairs is weak; secondly, there is insufficient extraction of complex\ncross-relations between long-distance multi-entities. To overcome these\nchallenges, we propose GEGA, a novel model for DocRE. The model leverages graph\nneural networks to construct multiple weight matrices, guiding attention\nallocation to evidence sentences. It also employs multi-scale representation\naggregation to enhance ER. Subsequently, we integrate the most efficient\nevidence information to implement both fully supervised and weakly supervised\ntraining processes for the model. We evaluate the GEGA model on three widely\nused benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The\nexperimental results indicate that our model has achieved comprehensive\nimprovements compared to the existing SOTA model.", "paper_summary_zh": "<paragraph>\u6587\u4ef6\u7d1a\u95dc\u4fc2\u8403\u53d6 (DocRE) \u65e8\u5728\u5f9e\u975e\u7d50\u69cb\u6587\u4ef6\u6587\u5b57\u4e2d\u8403\u53d6\u5be6\u9ad4\u9593\u7684\u95dc\u4fc2\u3002\u8207\u53e5\u5b50\u7d1a\u95dc\u4fc2\u8403\u53d6\u76f8\u6bd4\uff0c\u5b83\u9700\u8981\u5f9e\u66f4\u5ee3\u6cdb\u7684\u6587\u5b57\u8108\u7d61\u4e2d\u9032\u884c\u66f4\u8907\u96dc\u7684\u8a9e\u7fa9\u7406\u89e3\u3002\u76ee\u524d\uff0c\u4e00\u4e9b\u7814\u7a76\u5229\u7528\u8b49\u64da\u53e5\u5b50\u4e2d\u7684\u908f\u8f2f\u898f\u5247\u4f86\u589e\u5f37 DocRE \u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5728\u6c92\u6709\u63d0\u4f9b\u8b49\u64da\u53e5\u5b50\u7684\u8cc7\u6599\u4e2d\uff0c\u7814\u7a76\u4eba\u54e1\u901a\u5e38\u6703\u900f\u904e\u8b49\u64da\u64f7\u53d6 (ER) \u70ba\u6574\u500b\u6587\u4ef6\u53d6\u5f97\u4e00\u4efd\u8b49\u64da\u53e5\u5b50\u6e05\u55ae\u3002\u56e0\u6b64\uff0cDocRE \u9762\u81e8\u5169\u9805\u6311\u6230\uff1a\u9996\u5148\uff0c\u8b49\u64da\u548c\u5be6\u9ad4\u5c0d\u4e4b\u9593\u7684\u95dc\u806f\u6027\u8f03\u5f31\uff1b\u5176\u6b21\uff0c\u7121\u6cd5\u5145\u5206\u8403\u53d6\u9060\u8ddd\u96e2\u591a\u5be6\u9ad4\u4e4b\u9593\u7684\u8907\u96dc\u4ea4\u53c9\u95dc\u4fc2\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa GEGA\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc DocRE \u7684\u65b0\u578b\u6a21\u578b\u3002\u8a72\u6a21\u578b\u5229\u7528\u5716\u795e\u7d93\u7db2\u8def\u4f86\u5efa\u69cb\u591a\u500b\u6b0a\u91cd\u77e9\u9663\uff0c\u5f15\u5c0e\u6ce8\u610f\u529b\u5206\u914d\u5230\u8b49\u64da\u53e5\u5b50\u3002\u5b83\u9084\u63a1\u7528\u591a\u5c3a\u5ea6\u8868\u793a\u805a\u5408\u4f86\u589e\u5f37 ER\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u6574\u5408\u6700\u6709\u6548\u7684\u8b49\u64da\u8cc7\u8a0a\uff0c\u70ba\u6a21\u578b\u5be6\u4f5c\u5b8c\u5168\u76e3\u7763\u5f0f\u548c\u5f31\u76e3\u7763\u5f0f\u8a13\u7df4\u6d41\u7a0b\u3002\u6211\u5011\u5728\u4e09\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff1aDocRED\u3001Re-DocRED \u548c Revisit-DocRED \u4e0a\u8a55\u4f30 GEGA \u6a21\u578b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u73fe\u6709\u7684 SOTA \u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5df2\u7372\u5f97\u5168\u9762\u7684\u6539\u9032\u3002</paragraph>", "author": "Yanxu Mao et.al.", "authors": "Yanxu Mao, Peipei Liu, Tiehan Cui", "id": "2407.21384v1", "paper_url": "http://arxiv.org/abs/2407.21384v1", "repo": "null"}}