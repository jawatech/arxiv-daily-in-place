{"2412.01306": {"publish_time": "2024-12-02", "title": "Multimodal Medical Disease Classification with LLaMA II", "paper_summary": "Medical patient data is always multimodal. Images, text, age, gender,\nhistopathological data are only few examples for different modalities in this\ncontext. Processing and integrating this multimodal data with deep learning\nbased methods is of utmost interest due to its huge potential for medical\nprocedure such as diagnosis and patient treatment planning. In this work we\nretrain a multimodal transformer-based model for disease classification. To\nthis end we use the text-image pair dataset from OpenI consisting of 2D chest\nX-rays associated with clinical reports. Our focus is on fusion methods for\nmerging text and vision information extracted from medical datasets. Different\narchitecture structures with a LLaMA II backbone model are tested. Early fusion\nof modality specific features creates better results with the best model\nreaching 97.10% mean AUC than late fusion from a deeper level of the\narchitecture (best model: 96.67% mean AUC). Both outperform former\nclassification models tested on the same multimodal dataset. The newly\nintroduced multimodal architecture can be applied to other multimodal datasets\nwith little effort and can be easily adapted for further research, especially,\nbut not limited to, the field of medical AI.", "paper_summary_zh": "\u91ab\u7642\u75c5\u60a3\u8cc7\u6599\u7e3d\u662f\u591a\u6a21\u614b\u7684\u3002\u5f71\u50cf\u3001\u6587\u5b57\u3001\u5e74\u9f61\u3001\u6027\u5225\u3001\u7d44\u7e54\u75c5\u7406\u5b78\u8cc7\u6599\u53ea\u662f\u6b64\u8108\u7d61\u4e0b\u4e0d\u540c\u6a21\u614b\u7684\u5e7e\u500b\u4f8b\u5b50\u3002\u8655\u7406\u548c\u6574\u5408\u9019\u4e9b\u591a\u6a21\u614b\u8cc7\u6599\uff0c\u4e26\u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2\u65b9\u6cd5\uff0c\u7531\u65bc\u5176\u5728\u91ab\u7642\u7a0b\u5e8f\uff08\u4f8b\u5982\u8a3a\u65b7\u548c\u75c5\u60a3\u6cbb\u7642\u8a08\u756b\uff09\u7684\u9f90\u5927\u6f5b\u529b\uff0c\u56e0\u6b64\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u8a13\u7df4\u4e00\u500b\u591a\u6a21\u614bTransformer\u57fa\u790e\u6a21\u578b\uff0c\u7528\u65bc\u75be\u75c5\u5206\u985e\u3002\u70ba\u6b64\uff0c\u6211\u5011\u4f7f\u7528\u4f86\u81ea OpenI \u7684\u6587\u5b57\u5f71\u50cf\u914d\u5c0d\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u8207\u81e8\u5e8a\u5831\u544a\u76f8\u95dc\u7684 2D \u80f8\u90e8 X \u5149\u3002\u6211\u5011\u7684\u91cd\u9ede\u5728\u65bc\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u65bc\u5408\u4f75\u5f9e\u91ab\u7642\u8cc7\u6599\u96c6\u63d0\u53d6\u7684\u6587\u5b57\u548c\u5f71\u50cf\u8cc7\u8a0a\u3002\u6e2c\u8a66\u4e86\u5177\u6709 LLaMA II \u4e3b\u5e79\u6a21\u578b\u7684\u4e0d\u540c\u67b6\u69cb\u7d50\u69cb\u3002\u7279\u5b9a\u65bc\u6a21\u614b\u7279\u5fb5\u7684\u65e9\u671f\u878d\u5408\u6703\u7522\u751f\u66f4\u597d\u7684\u7d50\u679c\uff0c\u6700\u4f73\u6a21\u578b\u9054\u5230 97.10% \u7684\u5e73\u5747 AUC\uff0c\u9ad8\u65bc\u5f9e\u67b6\u69cb\u66f4\u6df1\u5c64\u6b21\u9032\u884c\u7684\u5f8c\u671f\u878d\u5408\uff08\u6700\u4f73\u6a21\u578b\uff1a96.67% \u7684\u5e73\u5747 AUC\uff09\u3002\u5169\u8005\u90fd\u512a\u65bc\u5728\u76f8\u540c\u591a\u6a21\u614b\u8cc7\u6599\u96c6\u4e0a\u6e2c\u8a66\u7684\u524d\u5206\u985e\u6a21\u578b\u3002\u65b0\u63a8\u51fa\u7684\u591a\u6a21\u614b\u67b6\u69cb\u53ef\u4ee5\u6beb\u4e0d\u8cbb\u529b\u5730\u61c9\u7528\u65bc\u5176\u4ed6\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u4e26\u4e14\u53ef\u4ee5\u8f15\u9b06\u6539\u7de8\u4ee5\u9032\u884c\u9032\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u7279\u5225\u662f\uff08\u4f46\u4e0d\u9650\u65bc\uff09\u91ab\u7642 AI \u9818\u57df\u3002", "author": "Christian Gapp et.al.", "authors": "Christian Gapp, Elias Tappeiner, Martin Welk, Rainer Schubert", "id": "2412.01306v1", "paper_url": "http://arxiv.org/abs/2412.01306v1", "repo": "null"}}