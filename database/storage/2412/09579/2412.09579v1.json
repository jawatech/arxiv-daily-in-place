{"2412.09579": {"publish_time": "2024-12-12", "title": "A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks", "paper_summary": "Knowledge distillation, where a small student model learns from a pre-trained\nlarge teacher model, has achieved substantial empirical success since the\nseminal work of \\citep{hinton2015distilling}. Despite prior theoretical studies\nexploring the benefits of knowledge distillation, an important question remains\nunanswered: why does soft-label training from the teacher require significantly\nfewer neurons than directly training a small neural network with hard labels?\nTo address this, we first present motivating experimental results using simple\nneural network models on a binary classification problem. These results\ndemonstrate that soft-label training consistently outperforms hard-label\ntraining in accuracy, with the performance gap becoming more pronounced as the\ndataset becomes increasingly difficult to classify. We then substantiate these\nobservations with a theoretical contribution based on two-layer neural network\nmodels. Specifically, we show that soft-label training using gradient descent\nrequires only $O\\left(\\frac{1}{\\gamma^2 \\epsilon}\\right)$ neurons to achieve a\nclassification loss averaged over epochs smaller than some $\\epsilon > 0$,\nwhere $\\gamma$ is the separation margin of the limiting kernel. In contrast,\nhard-label training requires $O\\left(\\frac{1}{\\gamma^4} \\cdot\n\\ln\\left(\\frac{1}{\\epsilon}\\right)\\right)$ neurons, as derived from an adapted\nversion of the gradient descent analysis in \\citep{ji2020polylogarithmic}. This\nimplies that when $\\gamma \\leq \\epsilon$, i.e., when the dataset is challenging\nto classify, the neuron requirement for soft-label training can be\nsignificantly lower than that for hard-label training. Finally, we present\nexperimental results on deep neural networks, further validating these\ntheoretical findings.", "paper_summary_zh": "<paragraph>\u77e5\u8b58\u84b8\u993e\uff0c\u5176\u4e2d\u4e00\u500b\u5c0f\u578b\u5b78\u751f\u6a21\u578b\u5f9e\u4e00\u500b\u9810\u5148\u8a13\u7df4\u7684\u5927\u578b\u6559\u5e2b\u6a21\u578b\u4e2d\u5b78\u7fd2\uff0c\u81ea\\citep{hinton2015distilling}\u7684\u958b\u5275\u6027\u5de5\u4f5c\u4ee5\u4f86\uff0c\u5df2\u7d93\u53d6\u5f97\u4e86\u5927\u91cf\u7684\u7d93\u9a57\u6210\u529f\u3002\u5118\u7ba1\u4e4b\u524d\u6709\u7406\u8ad6\u7814\u7a76\u63a2\u8a0e\u4e86\u77e5\u8b58\u84b8\u993e\u7684\u597d\u8655\uff0c\u4f46\u4e00\u500b\u91cd\u8981\u554f\u984c\u4ecd\u7136\u6c92\u6709\u5f97\u5230\u89e3\u7b54\uff1a\u70ba\u4ec0\u9ebc\u4f86\u81ea\u6559\u5e2b\u7684\u8edf\u6a19\u7c64\u8a13\u7df4\u9700\u8981\u7684\uff0c\u986f\u8457\u5c11\u65bc\u76f4\u63a5\u4f7f\u7528\u786c\u6a19\u7c64\u8a13\u7df4\u4e00\u500b\u5c0f\u578b\u795e\u7d93\u7db2\u8def\uff1f\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u4e00\u500b\u4e8c\u5143\u5206\u985e\u554f\u984c\u4e0a\u7684\u7c21\u55ae\u795e\u7d93\u7db2\u8def\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6709\u529b\u7684\u5be6\u9a57\u7d50\u679c\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0c\u8edf\u6a19\u7c64\u8a13\u7df4\u5728\u6e96\u78ba\u5ea6\u4e0a\u59cb\u7d42\u512a\u65bc\u786c\u6a19\u7c64\u8a13\u7df4\uff0c\u96a8\u8457\u6578\u64da\u96c6\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u96e3\u4ee5\u5206\u985e\uff0c\u5169\u8005\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u8b8a\u5f97\u66f4\u52a0\u660e\u986f\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u57fa\u65bc\u5169\u5c64\u795e\u7d93\u7db2\u8def\u6a21\u578b\u7684\u7406\u8ad6\u8ca2\u737b\uff0c\u8b49\u5be6\u4e86\u9019\u4e9b\u89c0\u5bdf\u7d50\u679c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8868\u660e\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7684\u8edf\u6a19\u7c64\u8a13\u7df4\u53ea\u9700\u8981$O\\left(\\frac{1}{\\gamma^2 \\epsilon}\\right)$\u500b\u795e\u7d93\u5143\uff0c\u5c31\u53ef\u4ee5\u5be6\u73fe\u5206\u985e\u640d\u5931\uff0c\u5e73\u5747\u4f86\u8aaa\uff0c\u6bd4\u67d0\u500b$\\epsilon > 0$\u9084\u8981\u5c0f\uff0c\u5176\u4e2d$\\gamma$\u662f\u6975\u9650\u6838\u7684\u5206\u96e2\u908a\u754c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u786c\u6a19\u7c64\u8a13\u7df4\u9700\u8981$O\\left(\\frac{1}{\\gamma^4} \\cdot\\ln\\left(\\frac{1}{\\epsilon}\\right)\\right)$\u500b\u795e\u7d93\u5143\uff0c\u9019\u6e90\u81ea\u65bc\\citep{ji2020polylogarithmic}\u4e2d\u68af\u5ea6\u4e0b\u964d\u5206\u6790\u7684\u6539\u7de8\u7248\u672c\u3002\u9019\u610f\u5473\u8457\uff0c\u7576$\\gamma \\leq \\epsilon$\u6642\uff0c\u5373\u7576\u6578\u64da\u96c6\u96e3\u4ee5\u5206\u985e\u6642\uff0c\u8edf\u6a19\u7c64\u8a13\u7df4\u7684\u795e\u7d93\u5143\u9700\u6c42\u53ef\u4ee5\u986f\u8457\u4f4e\u65bc\u786c\u6a19\u7c64\u8a13\u7df4\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u5be6\u9a57\u7d50\u679c\uff0c\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u9019\u4e9b\u7406\u8ad6\u767c\u73fe\u3002</paragraph>", "author": "Saptarshi Mandal et.al.", "authors": "Saptarshi Mandal, Xiaojun Lin, R. Srikant", "id": "2412.09579v1", "paper_url": "http://arxiv.org/abs/2412.09579v1", "repo": "null"}}