{"2408.03675": {"publish_time": "2024-08-07", "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time", "paper_summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at https:\n//github.com/PaddlePaddle/Research/ tree/master/NLP/ACL2024-NACL.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5f15\u767c\u4e86 AI \u61c9\u7528\u5275\u65b0\u71b1\u6f6e\uff0c\u6a19\u8a8c\u8457\u4e00\u500b\u4ee4\u4eba\u8208\u596e\u7684\u65b0\u6642\u4ee3\uff0c\u5177\u5099\u64f4\u5c55\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u3002\u7136\u800c\uff0c\u8a17\u7ba1\u9019\u4e9b\u6a21\u578b\u7684\u6210\u672c\u904e\u9ad8\uff0c\u9019\u4e3b\u8981\u662f\u56e0\u70ba KV \u5feb\u53d6\u6d89\u53ca\u9577\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u6d88\u8017\u5927\u91cf\u8a18\u61b6\u9ad4\u3002\u5118\u7ba1\u6709\u5e7e\u9805\u63d0\u8b70\u5f9e KV \u5feb\u53d6\u4e2d\u9a45\u9010\u4e0d\u5fc5\u8981\u7684\u4ee3\u5e63\uff0c\u4f46\u5b83\u5011\u5927\u591a\u4f9d\u8cf4\u65bc\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u7684\u504f\u5dee\u5c40\u90e8\u7d71\u8a08\uff0c\u4e26\u4f7f\u7528\u4ee4\u4eba\u4fe1\u670d\u7684\u6307\u6a19\u5831\u544a\u6548\u80fd\uff0c\u4f8b\u5982\u5c0d\u4e0d\u5145\u5206\u7684\u77ed\u6587\u5b57\u8a55\u4f30\u4e2d\u7684\u56f0\u60d1\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa NACL\uff0c\u4e00\u500b\u7528\u65bc\u9577\u4e0a\u4e0b\u6587 KV \u5feb\u53d6\u9a45\u9010\u7684\u901a\u7528\u6846\u67b6\uff0c\u5728\u7de8\u78bc\u968e\u6bb5\u7684\u55ae\u4e00\u64cd\u4f5c\u4e2d\u5be6\u73fe\u66f4\u512a\u5316\u3001\u66f4\u6709\u6548\u7684\u9a45\u9010\u3002\u7531\u65bc NACL \u7684\u6548\u7387\uff0c\u6211\u5011\u5728 PROXY TOKENS EVICTION \u4e2d\u7d50\u5408\u4e86\u66f4\u6e96\u78ba\u7684\u6ce8\u610f\u529b\u5206\u6578\u7d71\u8a08\uff0c\u4ee5\u53ca RANDOM EVICTION \u7684\u591a\u5143\u96a8\u6a5f\u9a45\u9010\u7b56\u7565\uff0c\u65e8\u5728\u6e1b\u8f15\u6ce8\u610f\u529b\u504f\u5dee\u7684\u554f\u984c\uff0c\u4e26\u589e\u5f37\u5728\u7dad\u8b77\u95dc\u9375\u4ee3\u5e63\u4ee5\u9032\u884c\u9577\u4e0a\u4e0b\u6587\u5efa\u6a21\u4efb\u52d9\u6642\u7684\u7a69\u5065\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u5206\u5225\u5c07\u77ed\u6587\u672c\u548c\u9577\u6587\u672c\u4efb\u52d9\u7684\u6548\u80fd\u986f\u8457\u63d0\u5347\u4e86 80% \u548c 76%\uff0c\u5c07 KV \u5feb\u53d6\u6e1b\u5c11\u4e86 50%\uff0c\u540c\u6642\u6548\u80fd\u7dad\u6301\u7387\u8d85\u904e 95%\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL \u53d6\u5f97\u3002", "author": "Yilong Chen et.al.", "authors": "Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu", "id": "2408.03675v1", "paper_url": "http://arxiv.org/abs/2408.03675v1", "repo": "https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL"}}