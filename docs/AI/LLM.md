
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-24**|**Long-Form Speech Generation with Spoken Language Models**|Se Jin Park et.al.|[2412.18603v1](http://arxiv.org/abs/2412.18603v1)|[link](https://github.com/google-deepmind/librispeech-long)|
|**2024-12-24**|**Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**|Fernando Jia et.al.|[2412.18601v1](http://arxiv.org/abs/2412.18601v1)|null|
|**2024-12-24**|**DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation**|Minghong Cai et.al.|[2412.18597v1](http://arxiv.org/abs/2412.18597v1)|[link](https://github.com/tencentarc/ditctrl)|
|**2024-12-24**|**A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**|OpenMind et.al.|[2412.18588v1](http://arxiv.org/abs/2412.18588v1)|null|
|**2024-12-24**|**Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**|Sergey Sedov et.al.|[2412.18582v1](http://arxiv.org/abs/2412.18582v1)|null|
|**2024-12-24**|**How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation**|Dewu Zheng et.al.|[2412.18573v1](http://arxiv.org/abs/2412.18573v1)|[link](https://github.com/deepsoftwareanalytics/multicodebench)|
|**2024-12-24**|**Zero-resource Speech Translation and Recognition with LLMs**|Karel Mundnich et.al.|[2412.18566v1](http://arxiv.org/abs/2412.18566v1)|null|
|**2024-12-24**|**Distilling Fine-grained Sentiment Understanding from Large Language Models**|Yice Zhang et.al.|[2412.18552v1](http://arxiv.org/abs/2412.18552v1)|[link](https://github.com/hitsz-hlt/fsa-distillation)|
|**2024-12-24**|**Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability**|Haonan Li et.al.|[2412.18551v1](http://arxiv.org/abs/2412.18551v1)|null|
|**2024-12-24**|**Token-Budget-Aware LLM Reasoning**|Tingxu Han et.al.|[2412.18547v1](http://arxiv.org/abs/2412.18547v1)|[link](https://github.com/geniushtx/tale)|
|**2024-12-24**|**Consistency Checks for Language Model Forecasters**|Daniel Paleka et.al.|[2412.18544v1](http://arxiv.org/abs/2412.18544v1)|null|
|**2024-12-24**|**Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**|Derong Xu Xinhang Li et.al.|[2412.18537v1](http://arxiv.org/abs/2412.18537v1)|null|
|**2024-12-24**|**Characterizations of Language Generation With Breadth**|Alkis Kalavasis et.al.|[2412.18530v1](http://arxiv.org/abs/2412.18530v1)|null|
|**2024-12-24**|**Think or Remember? Detecting and Directing LLMs Towards Memorization or Generalization**|Yi-Fu Fu et.al.|[2412.18497v1](http://arxiv.org/abs/2412.18497v1)|null|
|**2024-12-24**|**Generating event descriptions under syntactic and semantic constraints**|Angela Cao et.al.|[2412.18496v1](http://arxiv.org/abs/2412.18496v1)|[link](https://github.com/superMereo/generating-event-descriptions)|
|**2024-12-24**|**How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?**|Sara Papi et.al.|[2412.18495v1](http://arxiv.org/abs/2412.18495v1)|null|
|**2024-12-24**|**An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving**|Gnaneswar Villuri et.al.|[2412.18489v1](http://arxiv.org/abs/2412.18489v1)|null|
|**2024-12-24**|**Segment-Based Attention Masking for GPTs**|Shahar Katz et.al.|[2412.18487v1](http://arxiv.org/abs/2412.18487v1)|[link](https://github.com/shacharKZ/MAS-Segment-Based-Attention-Masking)|
|**2024-12-24**|**MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation**|Tengfei He et.al.|[2412.18464v1](http://arxiv.org/abs/2412.18464v1)|[link](https://github.com/tengfeihe/motifgpl)|
|**2024-12-24**|**GeFL: Model-Agnostic Federated Learning with Generative Models**|Honggu Kang et.al.|[2412.18460v1](http://arxiv.org/abs/2412.18460v1)|null|
|**2024-12-24**|**Multi-Agent Norm Perception and Induction in Distributed Healthcare**|Chao Li et.al.|[2412.18454v1](http://arxiv.org/abs/2412.18454v1)|null|
|**2024-12-24**|**Is Large Language Model Good at Triple Set Prediction? An Empirical Study**|Yuan Yuan et.al.|[2412.18443v1](http://arxiv.org/abs/2412.18443v1)|null|
|**2024-12-24**|**Unlocking the Potential of Multiple BERT Models for Bangla Question Answering in NCTB Textbooks**|Abdullah Khondoker et.al.|[2412.18440v1](http://arxiv.org/abs/2412.18440v1)|null|
|**2024-12-24**|**Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**|Farhad Nooralahzadeh et.al.|[2412.18428v1](http://arxiv.org/abs/2412.18428v1)|null|
|**2024-12-24**|**GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent**|Kangjia Zhao et.al.|[2412.18426v1](http://arxiv.org/abs/2412.18426v1)|null|
|**2024-12-24**|**LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating**|Chao Deng et.al.|[2412.18424v1](http://arxiv.org/abs/2412.18424v1)|null|
|**2024-12-24**|**Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models**|Zihan Zhou et.al.|[2412.18419v1](http://arxiv.org/abs/2412.18419v1)|null|
|**2024-12-24**|**Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English**|Avinash Anand et.al.|[2412.18415v1](http://arxiv.org/abs/2412.18415v1)|[link](https://github.com/midas-research/Multilingual-Mathematical-Reasoning)|
|**2024-12-24**|**Exploring Flexible Scenario Generation in Godot Simulator**|Daniel Peraltai et.al.|[2412.18408v1](http://arxiv.org/abs/2412.18408v1)|null|
|**2024-12-24**|**A Statistical Framework for Ranking LLM-Based Chatbots**|Siavash Ameli et.al.|[2412.18407v1](http://arxiv.org/abs/2412.18407v1)|null|
|**2024-12-24**|**TPAoI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking**|Haosheng He et.al.|[2412.18391v1](http://arxiv.org/abs/2412.18391v1)|null|
|**2024-12-24**|**RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction**|Wu Xiaoping et.al.|[2412.18390v1](http://arxiv.org/abs/2412.18390v1)|null|
|**2024-12-24**|**Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model**|Tenghui Li et.al.|[2412.18387v1](http://arxiv.org/abs/2412.18387v1)|null|
|**2024-12-24**|**ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots**|Shani Goren et.al.|[2412.18377v1](http://arxiv.org/abs/2412.18377v1)|null|
|**2024-12-24**|**Bidirectional Topic Matching: Quantifying Thematic Overlap Between Corpora Through Topic Modelling**|Raven Adam et.al.|[2412.18376v1](http://arxiv.org/abs/2412.18376v1)|null|
|**2024-12-24**|**Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors**|Jinhyeok Choi et.al.|[2412.18370v1](http://arxiv.org/abs/2412.18370v1)|[link](https://github.com/bdi-lab/monti)|
|**2024-12-24**|**Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology Dataset**|Jiarui Liu et.al.|[2412.18367v1](http://arxiv.org/abs/2412.18367v1)|null|
|**2024-12-24**|**Extracting triples from dialogues for conversational social agents**|Piek Vossen et.al.|[2412.18364v1](http://arxiv.org/abs/2412.18364v1)|null|
|**2024-12-24**|**Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor**|Hao Yu et.al.|[2412.18355v1](http://arxiv.org/abs/2412.18355v1)|null|
|**2024-12-24**|**The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence**|Viviane Clay et.al.|[2412.18354v1](http://arxiv.org/abs/2412.18354v1)|[link](https://github.com/thousandbrainsproject/tbp.monty)|
|**2024-12-24**|**Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering**|Zhongjian Hu et.al.|[2412.18351v1](http://arxiv.org/abs/2412.18351v1)|null|
|**2024-12-24**|**Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning**|Safa Ben Atitallah et.al.|[2412.18322v1](http://arxiv.org/abs/2412.18322v1)|null|
|**2024-12-24**|**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search**|Huanjin Yao et.al.|[2412.18319v1](http://arxiv.org/abs/2412.18319v1)|[link](https://github.com/hjyao00/mulberry)|
|**2024-12-24**|**M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models**|Jiaxin Guo et.al.|[2412.18299v1](http://arxiv.org/abs/2412.18299v1)|null|
|**2024-12-24**|**Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight**|Xi Ding et.al.|[2412.18298v1](http://arxiv.org/abs/2412.18298v1)|null|
|**2024-12-24**|**Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies**|Qi Liu et.al.|[2412.18296v1](http://arxiv.org/abs/2412.18296v1)|null|
|**2024-12-24**|**Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases**|Christian Di Maio et.al.|[2412.18295v1](http://arxiv.org/abs/2412.18295v1)|null|
|**2024-12-24**|**MinsStudio: A Streamlined Package for Minecraft AI Agent Development**|Shaofei Cai et.al.|[2412.18293v1](http://arxiv.org/abs/2412.18293v1)|[link](https://github.com/craftjarvis/minestudio)|
|**2024-12-24**|**DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation**|Junyi Lu et.al.|[2412.18291v1](http://arxiv.org/abs/2412.18291v1)|null|
|**2024-12-24**|**Towards understanding how attention mechanism works in deep learning**|Tianyu Ruan et.al.|[2412.18288v1](http://arxiv.org/abs/2412.18288v1)|null|
|**2024-12-24**|**Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation**|Sheng Xiang et.al.|[2412.18287v1](http://arxiv.org/abs/2412.18287v1)|[link](https://github.com/ai4risk/antifraud)|
|**2024-12-24**|**Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization**|Jiacai Liu et.al.|[2412.18279v1](http://arxiv.org/abs/2412.18279v1)|null|
|**2024-12-24**|**GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge**|Shammur Absar Chowdhury et.al.|[2412.18274v1](http://arxiv.org/abs/2412.18274v1)|null|
|**2024-12-24**|**Sampling Bag of Views for Open-Vocabulary Object Detection**|Hojun Choi et.al.|[2412.18273v1](http://arxiv.org/abs/2412.18273v1)|null|
|**2024-12-24**|**Annotating References to Mythological Entities in French Literature**|Thierry Poibeau et.al.|[2412.18270v1](http://arxiv.org/abs/2412.18270v1)|null|
|**2024-12-24**|**Investigating Large Language Models for Code Vulnerability Detection: An Experimental Study**|Xuefeng Jiang et.al.|[2412.18260v1](http://arxiv.org/abs/2412.18260v1)|[link](https://github.com/sakirinn/llm4cvd)|
|**2024-12-24**|**Fréchet regression for multi-label feature selection with implicit regularization**|Dou El Kefel Mansouri et.al.|[2412.18247v1](http://arxiv.org/abs/2412.18247v1)|null|
|**2024-12-24**|**An Automatic Graph Construction Framework based on Large Language Models for Recommendation**|Rong Shan et.al.|[2412.18241v1](http://arxiv.org/abs/2412.18241v1)|[link](https://github.com/lavieenrose365/autograph)|
|**2024-12-24**|**Expand VSR Benchmark for VLLM to Expertize in Spatial Rules**|Peijin Xie et.al.|[2412.18224v1](http://arxiv.org/abs/2412.18224v1)|[link](https://github.com/peijin360/vsre)|
|**2024-12-24**|**ICM-Assistant: Instruction-tuning Multimodal Large Language Models for Rule-based Explainable Image Content Moderation**|Mengyang Wu et.al.|[2412.18216v1](http://arxiv.org/abs/2412.18216v1)|[link](https://github.com/zhaoyuzhi/icm-assistant)|
|**2024-12-24**|**Robustness-aware Automatic Prompt Optimization**|Zeru Shi et.al.|[2412.18196v1](http://arxiv.org/abs/2412.18196v1)|[link](https://github.com/vanpe20/BATprompt)|
|**2024-12-24**|**VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks**|Shiduo Zhang et.al.|[2412.18194v1](http://arxiv.org/abs/2412.18194v1)|null|
|**2024-12-24**|**An Analysis on Automated Metrics for Evaluating Japanese-English Chat Translation**|Andre Rusli et.al.|[2412.18190v1](http://arxiv.org/abs/2412.18190v1)|null|
|**2024-12-24**|**On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs**|Andre Rusli et.al.|[2412.18188v1](http://arxiv.org/abs/2412.18188v1)|[link](https://github.com/arusl/anlp_nlp2022_a6-1)|
|**2024-12-24**|**TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization**|Yucong Luo et.al.|[2412.18185v1](http://arxiv.org/abs/2412.18185v1)|null|
|**2024-12-24**|**Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization**|Sihao Liu et.al.|[2412.18177v1](http://arxiv.org/abs/2412.18177v1)|null|
|**2024-12-24**|**Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation**|Yucong Luo et.al.|[2412.18176v1](http://arxiv.org/abs/2412.18176v1)|null|
|**2024-12-24**|**INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**|Haohang Li et.al.|[2412.18174v1](http://arxiv.org/abs/2412.18174v1)|null|
|**2024-12-24**|**KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management**|Rongxin Cheng et.al.|[2412.18169v1](http://arxiv.org/abs/2412.18169v1)|null|
|**2024-12-24**|**Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi**|Rasika Ransing et.al.|[2412.18163v1](http://arxiv.org/abs/2412.18163v1)|null|
|**2024-12-24**|**VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities**|Shray Mathur et.al.|[2412.18161v1](http://arxiv.org/abs/2412.18161v1)|null|
|**2024-12-24**|**Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance**|Yaoyun Zhang et.al.|[2412.18157v1](http://arxiv.org/abs/2412.18157v1)|null|
|**2024-12-24**|**scReader: Prompting Large Language Models to Interpret scRNA-seq Data**|Cong Li et.al.|[2412.18156v1](http://arxiv.org/abs/2412.18156v1)|null|
|**2024-12-24**|**GeneSUM: Large Language Model-based Gene Summary Extraction**|Zhijian Chen et.al.|[2412.18154v1](http://arxiv.org/abs/2412.18154v1)|null|
|**2024-12-24**|**CoAM: Corpus of All-Type Multiword Expressions**|Yusuke Ide et.al.|[2412.18151v1](http://arxiv.org/abs/2412.18151v1)|null|
|**2024-12-24**|**EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation**|Shuhao Han et.al.|[2412.18150v1](http://arxiv.org/abs/2412.18150v1)|null|
|**2024-12-24**|**Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media**|Zhen Sun et.al.|[2412.18148v1](http://arxiv.org/abs/2412.18148v1)|null|
|**2024-12-24**|**Text-Aware Adapter for Few-Shot Keyword Spotting**|Youngmoon Jung et.al.|[2412.18142v1](http://arxiv.org/abs/2412.18142v1)|null|
|**2024-12-24**|**Ensuring Consistency for In-Image Translation**|Chengpeng Fu et.al.|[2412.18139v1](http://arxiv.org/abs/2412.18139v1)|null|
|**2024-12-24**|**LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment**|Binrui Zeng et.al.|[2412.18135v1](http://arxiv.org/abs/2412.18135v1)|null|
|**2024-12-24**|**Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy**|Qian Tao et.al.|[2412.18125v1](http://arxiv.org/abs/2412.18125v1)|null|
|**2024-12-24**|**AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models**|Yiming Wang et.al.|[2412.18123v1](http://arxiv.org/abs/2412.18123v1)|null|
|**2024-12-24**|**Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm**|Xiaoyang Hu et.al.|[2412.18120v1](http://arxiv.org/abs/2412.18120v1)|null|
|**2024-12-24**|**AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation**|Hao Wen et.al.|[2412.18116v1](http://arxiv.org/abs/2412.18116v1)|null|
|**2024-12-24**|**AIGT: AI Generative Table Based on Prompt**|Mingming Zhang et.al.|[2412.18111v1](http://arxiv.org/abs/2412.18111v1)|null|
|**2024-12-24**|**SlimGPT: Layer-wise Structured Pruning for Large Language Models**|Gui Ling et.al.|[2412.18110v1](http://arxiv.org/abs/2412.18110v1)|null|
|**2024-12-24**|**SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training**|Jiaxing Yu et.al.|[2412.18107v1](http://arxiv.org/abs/2412.18107v1)|null|
|**2024-12-24**|**Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels**|Mingcong Song et.al.|[2412.18106v1](http://arxiv.org/abs/2412.18106v1)|null|
|**2024-12-24**|**EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent**|Suyuan Wang et.al.|[2412.18100v1](http://arxiv.org/abs/2412.18100v1)|null|
|**2024-12-24**|**An Attention-based Framework with Multistation Information for Earthquake Early Warnings**|Yu-Ming Huang et.al.|[2412.18099v1](http://arxiv.org/abs/2412.18099v1)|null|
|**2024-12-24**|**LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting**|Nan Yang et.al.|[2412.18097v1](http://arxiv.org/abs/2412.18097v1)|null|
|**2024-12-24**|**Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH) -- a Large Language Model Chatbot for Perioperative Medicine**|Yu He Ke et.al.|[2412.18096v1](http://arxiv.org/abs/2412.18096v1)|null|
|**2024-12-24**|**Molly: Making Large Language Model Agents Solve Python Problem More Logically**|Rui Xiao et.al.|[2412.18093v1](http://arxiv.org/abs/2412.18093v1)|null|
|**2024-12-24**|**BRIDGE: Bundle Recommendation via Instruction-Driven Generation**|Tuan-Nghia Bui et.al.|[2412.18092v1](http://arxiv.org/abs/2412.18092v1)|null|
|**2024-12-24**|**AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning**|Lixian Jing et.al.|[2412.18091v1](http://arxiv.org/abs/2412.18091v1)|null|
|**2024-12-24**|**Multi-Point Positional Insertion Tuning for Small Object Detection**|Kanoko Goto et.al.|[2412.18090v1](http://arxiv.org/abs/2412.18090v1)|null|
|**2024-12-24**|**Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner**|Aizierjiang Aiersilan et.al.|[2412.18086v1](http://arxiv.org/abs/2412.18086v1)|[link](https://github.com/Ezharjan/AutoSceneGen)|
|**2024-12-24**|**Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models**|Xuan Lin et.al.|[2412.18084v1](http://arxiv.org/abs/2412.18084v1)|[link](https://github.com/chenlong164/peit)|
|**2024-12-24**|**Prompt Tuning for Item Cold-start Recommendation**|Yuezihan Jiang et.al.|[2412.18082v1](http://arxiv.org/abs/2412.18082v1)|[link](https://github.com/promorec/promo)|
|**2024-12-24**|**COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection**|Chang Liu et.al.|[2412.18076v1](http://arxiv.org/abs/2412.18076v1)|null|

#### Abstracts
##### **Long-Form Speech Generation with Spoken Language Models**
2412.18603v1 by Se Jin Park, Julian Salazar, Aren Jansen, Keisuke Kinoshita, Yong Man Ro, RJ Skerry-Ryan

We consider the generative modeling of speech over multiple minutes, a
requirement for long-form multimedia generation and audio-native voice
assistants. However, current spoken language models struggle to generate
plausible speech past tens of seconds, from high temporal resolution of speech
tokens causing loss of coherence, to architectural issues with long-sequence
training or extrapolation, to memory costs at inference time. With these
considerations we propose SpeechSSM, the first speech language model to learn
from and sample long-form spoken audio (e.g., 16 minutes of read or
extemporaneous speech) in a single decoding session without text intermediates,
based on recent advances in linear-time sequence modeling. Furthermore, to
address growing challenges in spoken language evaluation, especially in this
new long-form setting, we propose: new embedding-based and LLM-judged metrics;
quality measurements over length and time; and a new benchmark for long-form
speech processing and generation, LibriSpeech-Long. Speech samples and the
dataset are released at
https://google.github.io/tacotron/publications/speechssm/

摘要：我們考慮多分鐘的語音生成模型，這是長篇多媒體生成和音訊原生語音助理的要求。然而，目前的口語語言模型難以生成超過數十秒的可信語音，從語音標記的高時間解析度導致一致性喪失，到長序列訓練或外推的架構問題，再到推理時間的記憶體成本。考量這些因素，我們提出 SpeechSSM，這是第一個從長篇口語音訊（例如，16 分鐘的朗讀或即興演講）學習並取樣的語音語言模型，在單一解碼會話中，沒有文字中間產物，基於線性時間序列建模的最新進展。此外，為了解決口語語言評估中日益嚴峻的挑戰，特別是在這個新的長篇設定中，我們提出：新的基於嵌入和 LLM 判斷的指標；長度和時間的品質測量；以及長篇語音處理和生成的新的基準，LibriSpeech-Long。語音範例和資料集已於 https://google.github.io/tacotron/publications/speechssm/ 發布。

##### **Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**
2412.18601v1 by Fernando Jia, Jade Zheng, Florence Li

In the rapidly evolving landscape of GameFi, a fusion of gaming and
decentralized finance (DeFi), there exists a critical need to enhance player
engagement and economic interaction within gaming ecosystems. Our GameFi
ecosystem aims to fundamentally transform this landscape by integrating
advanced embodied AI agents into GameFi platforms. These AI agents, developed
using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,
are capable of proactive, adaptive, and contextually rich interactions with
players. By going beyond traditional scripted responses, these agents become
integral participants in the game's narrative and economic systems, directly
influencing player strategies and in-game economies. We address the limitations
of current GameFi platforms, which often lack immersive AI interactions and
mechanisms for community engagement or creator monetization. Through the deep
integration of AI agents with blockchain technology, we establish a
consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers
creators to monetize their contributions and fosters democratic collaboration
among players and creators. Furthermore, by embedding DeFi mechanisms into the
gaming experience, we enhance economic participation and provide new
opportunities for financial interactions within the game. Our approach enhances
player immersion and retention and advances the GameFi ecosystem by bridging
traditional gaming with Web3 technologies. By integrating sophisticated AI and
DeFi elements, we contribute to the development of more engaging, economically
robust, and community-centric gaming environments. This project represents a
significant advancement in the state-of-the-art in GameFi, offering insights
and methodologies that can be applied throughout the gaming industry.

摘要：在 GameFi 快速演進的環境中，遊戲與去中心化金融 (DeFi) 的融合，亟需提升玩家參與度和遊戲生態系統中的經濟互動。我們的 GameFi 生態系統旨在透過將進階的具身 AI 代理整合到 GameFi 平台中，從根本上轉變這種環境。這些 AI 代理使用尖端的巨量語言模型 (LLM)（例如 GPT-4 和 Claude AI）開發，能夠與玩家進行主動、適應性和豐富的互動。這些代理超越傳統腳本回應，成為遊戲敘事和經濟系統中不可或缺的參與者，直接影響玩家策略和遊戲經濟。我們解決了當前 GameFi 平台的限制，這些平台通常缺乏身歷其境的 AI 互動和社群參與或創作者獲利機制。透過將 AI 代理與區塊鏈技術深度整合，我們建立了一個共識驅動的、去中心化的 GameFi 生態系統。此生態系統讓創作者能夠將他們的貢獻獲利化，並促進玩家和創作者之間的民主合作。此外，透過將 DeFi 機制嵌入遊戲體驗中，我們加強了經濟參與，並在遊戲中提供了新的金融互動機會。我們的做法增強了玩家的沉浸感和留存率，並透過將傳統遊戲與 Web3 技術連結起來，提升了 GameFi 生態系統。透過整合先進的 AI 和 DeFi 元素，我們為更具吸引力、經濟強健且以社群為中心的遊戲環境的發展做出貢獻。這個專案代表了 GameFi 最先進技術的重大進展，提供了可在整個遊戲產業應用的見解和方法。

##### **DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation**
2412.18597v1 by Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, Xiangyu Yue

Sora-like video generation models have achieved remarkable progress with a
Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current
video generation models predominantly focus on single-prompt, struggling to
generate coherent scenes with multiple sequential prompts that better reflect
real-world dynamic scenarios. While some pioneering works have explored
multi-prompt video generation, they face significant challenges including
strict training data requirements, weak prompt following, and unnatural
transitions. To address these problems, we propose DiTCtrl, a training-free
multi-prompt video generation method under MM-DiT architectures for the first
time. Our key idea is to take the multi-prompt video generation task as
temporal video editing with smooth transitions. To achieve this goal, we first
analyze MM-DiT's attention mechanism, finding that the 3D full attention
behaves similarly to that of the cross/self-attention blocks in the UNet-like
diffusion models, enabling mask-guided precise semantic control across
different prompts with attention sharing for multi-prompt video generation.
Based on our careful design, the video generated by DiTCtrl achieves smooth
transitions and consistent object motion given multiple sequential prompts
without additional training. Besides, we also present MPVBench, a new benchmark
specially designed for multi-prompt video generation to evaluate the
performance of multi-prompt generation. Extensive experiments demonstrate that
our method achieves state-of-the-art performance without additional training.

摘要：類 Sora 的影片生成模型在多模態擴散Transformer MM-DiT 架構中取得顯著進展。然而，目前的影片生成模型主要專注於單一提示，難以生成包含多個循序提示的連貫場景，而這些提示更能反映真實世界的動態場景。儘管一些開創性的作品已探索多提示影片生成，但它們面臨嚴峻的挑戰，包括嚴格的訓練資料需求、提示追蹤能力不佳以及不自然的轉換。為了解決這些問題，我們提出 DiTCtrl，這是一種在 MM-DiT 架構下首次使用的免訓練多提示影片生成方法。我們的關鍵想法是將多提示影片生成任務視為具有平滑轉換的時序影片編輯。為了達成此目標，我們首先分析 MM-DiT 的注意力機制，發現 3D 全注意力與 UNet 類擴散模型中的交叉/自我注意力區塊有類似的行為，這使得我們能夠透過注意力共享進行遮罩導引的精確語意控制，以進行多提示影片生成。根據我們的精細設計，DiTCtrl 生成的影片在給定多個循序提示的情況下，可以實現平滑的轉換和一致的物件動作，而不需要額外的訓練。此外，我們還提出了 MPVBench，這是一個專門設計用於多提示影片生成的新基準，用於評估多提示生成的效果。廣泛的實驗證明，我們的方法在沒有額外訓練的情況下，達到了最先進的效能。

##### **A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**
2412.18588v1 by OpenMind, Shaohong Zhong, Adam Zhou, Boyuan Chen, Homin Luo, Jan Liphardt

Large Language Models (LLMs) are compact representations of all public
knowledge of our physical environment and animal and human behaviors. The
application of LLMs to robotics may offer a path to highly capable robots that
perform well across most human tasks with limited or even zero tuning. Aside
from increasingly sophisticated reasoning and task planning, networks of
(suitably designed) LLMs offer ease of upgrading capabilities and allow humans
to directly observe the robot's thinking. Here we explore the advantages,
limitations, and particularities of using LLMs to control physical robots. The
basic system consists of four LLMs communicating via a human language data bus
implemented via web sockets and ROS2 message passing. Surprisingly, rich robot
behaviors and good performance across different tasks could be achieved despite
the robot's data fusion cycle running at only 1Hz and the central data bus
running at the extremely limited rates of the human brain, of around 40 bits/s.
The use of natural language for inter-LLM communication allowed the robot's
reasoning and decision making to be directly observed by humans and made it
trivial to bias the system's behavior with sets of rules written in plain
English. These rules were immutably written into Ethereum, a global, public,
and censorship resistant Turing-complete computer. We suggest that by using
natural language as the data bus among interacting AIs, and immutable public
ledgers to store behavior constraints, it is possible to build robots that
combine unexpectedly rich performance, upgradability, and durable alignment
with humans.

摘要：大型語言模型 (LLM) 濃縮了我們對物理環境、動物和人類行為的所有公開知識。將 LLM 應用於機器人技術，可能為功能強大的機器人提供一條途徑，這些機器人在大多數人類任務中表現良好，調整有限甚至為零。除了日益複雜的推理和任務規劃之外，（適當設計的）LLM 網路提供輕鬆升級功能，並允許人類直接觀察機器人的思考。在這裡，我們探討了使用 LLM 控制物理機器人的優點、限制和特殊性。基本系統由四個 LLM 組成，它們通過人類語言數據匯流排進行通信，該匯流排通過網路套接字和 ROS2 訊息傳遞來實現。令人驚訝的是，儘管機器人的數據融合週期僅以 1Hz 運行，而中央數據匯流排以人類大腦極其有限的速度（約 40 位元/秒）運行，但仍可以在不同的任務中實現豐富的機器人行為和良好的性能。使用自然語言進行 LLM 間通信，允許人類直接觀察機器人的推理和決策制定，並使系統行為偏向於用簡單英語編寫的規則集變得微不足道。這些規則不可變地寫入以太坊，這是一個全球性的、公開的、且抗審查的圖靈完備電腦。我們建議，通過使用自然語言作為互動 AI 之間的數據匯流排，以及不可變的公共帳本來儲存行為約束，有可能建立結合了意外豐富的性能、可升級性和與人類持久一致性的機器人。

##### **Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**
2412.18582v1 by Sergey Sedov, Sumanth Bharadwaj Hachalli Karanam, Venu Gopal Kadamba

Prompt-Tuning is an efficient method for adapting pre-trained language models
to new tasks with minimal computational overhead by modifying prompt
embeddings. In this work, we investigate how crucial the phenomenon of
embedding collapse, frequently observed in Prompt-Tuning, is for the final
performance of the model. To address this question, we designed embedding
priors and compared them with posteriors of the converged Soft and Deep
Prompt-Tuning methods. Our findings suggest that priors strongly affect the
position of the tuned embeddings, and models can effectively work with
embeddings from different parts of activation spaces, including completely new
regions. As the final Prompt-Tuning capabilities are limited, we hypothesize
that controllable Prompt-Tuning posteriors may serve as a good starting point
for tasks such as chain-of-thought (COT) distillation. Our experiments also
show that generated trajectories are not localized in the activation space of
the models. However, there are distinct clusters of activations for distant
tasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,
Question-Answering and MLM) lie in the same cluster. These observations raise
questions about the importance of a single activation cluster for the
generalization abilities of large language models.

摘要：提示調整是一種有效的方法，可透過修改提示嵌入來調整預先訓練的語言模型以適應新任務，且計算量開銷極小。在這項工作中，我們探討在提示調整中經常觀察到的嵌入式折疊現象對模型的最終效能有多麼關鍵。為了解決這個問題，我們設計了嵌入式先驗，並將它們與收斂的軟提示調整和深度提示調整方法的事後驗證進行比較。我們的研究結果表明，先驗會強烈影響調整嵌入式的位置，而模型可以有效地使用來自啟動空間不同部分的嵌入式，包括全新的區域。由於最終提示調整功能受到限制，我們假設可控提示調整事後驗證可以作為鏈式思考 (COT) 蒸餾等任務的良好起點。我們的實驗也顯示，產生的軌跡並未局限在模型的啟動空間中。然而，遠程任務（例如 NLP 和算術）有不同的啟動群集，而 NLP 任務（例如問答和 MLM）之間的啟動則位於同一個群集。這些觀察引發了問題，即對於大型語言模型的泛化能力而言，單一啟動群集的重要性為何。

##### **How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation**
2412.18573v1 by Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, Zibin Zheng

Recently, an increasing number of AI-driven programming assistants powered by
code LLMs have been integrated into various real-world software development
environments, significantly boosting developer productivity. However, existing
code generation benchmarks primarily focus on general-purpose scenarios,
leaving the code generation performance of LLMs for specific application
domains largely unknown. In this paper, we introduce a new benchmark,
MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming
tasks, covering 12 popular software development domains and 15 programming
languages. Specifically, we perform in-depth research to identify these 12
application domains. Given that each domain may involve multiple technical
frameworks, and that different frameworks present distinct challenges in the
coding process, we categorize the commonly used frameworks and platforms within
each domain. We then sample programming problems from GitHub repositories
related to these subdomains. To ensure the quality of the tasks and mitigate
data leakage issues, we invite annotators to rewrite the docstrings for each
task in MultiCodeBench. Additionally, we build a static analysis-based
dependency parsing tool to extract the dependencies in the ground truth for
each task, enabling deeper performance analysis. Through extensive experiments
on MultiCodeBench with eleven representative mainstream LLMs, we reveal the
code generation performance of the LLMs across different application domains,
providing practical insights for developers in downstream fields when selecting
LLMs. Furthermore, we analyze the reasons behind the models' failures in
completing software application development tasks, offering guidance for model
developers to enhance domain-specific code generation capabilities.

摘要：<paragraph>最近，由代码 LLM 提供支持的越来越多的 AI 驱动的编程助手已集成到各种实际软件开发环境中，极大地提高了开发人员的生产力。然而，现有的代码生成基准主要关注通用场景，在很大程度上忽略了 LLM 在特定应用程序领域中的代码生成性能。在本文中，我们引入了一个新的基准 MultiCodeBench 来填补这一空白。MultiCodeBench 包含 2,400 个编程任务，涵盖 12 个流行的软件开发领域和 15 种编程语言。具体来说，我们进行了深入的研究以识别这 12 个应用程序领域。鉴于每个领域可能涉及多个技术框架，并且不同的框架在编码过程中提出了不同的挑战，因此我们对每个领域中常用的框架和平台进行了分类。然后，我们从与这些子域相关的 GitHub 存储库中抽取编程问题。为了确保任务的质量并减轻数据泄露问题，我们邀请注释员重写 MultiCodeBench 中每个任务的文档字符串。此外，我们构建了一个基于静态分析的依赖项解析工具，以提取每个任务的基本事实中的依赖项，从而实现更深入的性能分析。通过在 MultiCodeBench 上使用 11 个有代表性的主流 LLM 进行广泛的实验，我们揭示了 LLM 在不同应用程序领域中的代码生成性能，为下游领域的开发人员在选择 LLM 时提供了实用的见解。此外，我们分析了模型在完成软件应用程序开发任务时失败的原因，为模型开发人员提供了增强特定领域代码生成能力的指导。</paragraph>

##### **Zero-resource Speech Translation and Recognition with LLMs**
2412.18566v1 by Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff

Despite recent advancements in speech processing, zero-resource speech
translation (ST) and automatic speech recognition (ASR) remain challenging
problems. In this work, we propose to leverage a multilingual Large Language
Model (LLM) to perform ST and ASR in languages for which the model has never
seen paired audio-text data. We achieve this by using a pre-trained
multilingual speech encoder, a multilingual LLM, and a lightweight adaptation
module that maps the audio representations to the token embedding space of the
LLM. We perform several experiments both in ST and ASR to understand how to
best train the model and what data has the most impact on performance in
previously unseen languages. In ST, our best model is capable to achieve BLEU
scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we
achieve WERs of up to 28.2\%. We finally show that the performance of our
system is bounded by the ability of the LLM to output text in the desired
language.

摘要：儘管在語音處理方面有近期的進展，零資源語音翻譯 (ST) 和自動語音辨識 (ASR) 仍然是具有挑戰性的問題。在這項工作中，我們提議利用多語言大型語言模型 (LLM) 來執行 ST 和 ASR，這些語言的模型從未見過配對的音訊文字資料。我們透過使用預先訓練的多語言語音編碼器、多語言 LLM 和輕量級適應模組來達成此目標，該模組會將音訊表示對應到 LLM 的標記嵌入空間。我們執行多項 ST 和 ASR 實驗，以了解如何最佳訓練模型，以及哪些資料對先前未見語言的效能影響最大。在 ST 中，我們的最佳模型能夠在 CoVoST2 中針對兩種先前未見的語言達成超過 23 的 BLEU 分數，而在 ASR 中，我們達成高達 28.2% 的 WER。我們最後顯示，我們系統的效能受限於 LLM 以所需語言輸出文字的能力。

##### **Distilling Fine-grained Sentiment Understanding from Large Language Models**
2412.18552v1 by Yice Zhang, Guangyu Xie, Hongling Xu, Kaiheng Hou, Jianzhu Bao, Qianlong Wang, Shiwei Chen, Ruifeng Xu

Fine-grained sentiment analysis (FSA) aims to extract and summarize user
opinions from vast opinionated text. Recent studies demonstrate that large
language models (LLMs) possess exceptional sentiment understanding
capabilities. However, directly deploying LLMs for FSA applications incurs high
inference costs. Therefore, this paper investigates the distillation of
fine-grained sentiment understanding from LLMs into small language models
(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews
and then utilize the generated content to pretrain SLMs. Additionally, we
develop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive
experiments on this benchmark reveal that: (1) distillation significantly
enhances the performance of SLMs in FSA tasks, achieving a 6.00\% improvement
in $F_1$-score, and the distilled model can outperform Llama-2-7b with only
220M parameters; (2) distillation equips SLMs with excellent zero-shot
sentiment classification capabilities, enabling them to match or even exceed
their teacher models. These results suggest that distillation from LLMs is a
highly promising direction for FSA. We will release our code, data, and
pretrained model weights at
\url{https://github.com/HITSZ-HLT/FSA-Distillation}.

摘要：細粒度情緒分析 (FSA) 旨在從大量的意見文本中提取和總結使用者的意見。最近的研究表明，大型語言模型 (LLM) 具有卓越的情緒理解能力。然而，直接部署 LLM 進行 FSA 應用會產生高昂的推論成本。因此，本文探討將 LLM 中的細粒度情緒理解蒸餾到小型語言模型 (SLM) 中。我們提示 LLM 檢查和詮釋給定評論的情緒，然後利用產生的內容預訓練 SLM。此外，我們開發了一個全面的 FSA 基準來評估 SLM 和 LLM。在這個基準上的廣泛實驗揭示：(1) 蒸餾顯著提升 SLM 在 FSA 任務中的表現，在 $F_1$-score 中提升 6.00%，而且蒸餾模型僅使用 220M 個參數就能超越 Llama-2-7b；(2) 蒸餾賦予 SLM 優異的零次學習情緒分類能力，使其能夠匹配甚至超越其教師模型。這些結果表明，從 LLM 進行蒸餾是 FSA 一個極具前景的方向。我們將在
\url{https://github.com/HITSZ-HLT/FSA-Distillation} 發布我們的程式碼、資料和預訓練模型權重。

##### **Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability**
2412.18551v1 by Haonan Li, Xudong Han, Zenan Zhai, Honglin Mu, Hao Wang, Zhenxuan Zhang, Yilin Geng, Shom Lin, Renxi Wang, Artem Shelmanov, Xiangyu Qi, Yuxia Wang, Donghai Hong, Youliang Yuan, Meng Chen, Haoqin Tu, Fajri Koto, Tatsuki Kuribayashi, Cong Zeng, Rishabh Bhardwaj, Bingchen Zhao, Yawen Duan, Yi Liu, Emad A. Alghamdi, Yaodong Yang, Yinpeng Dong, Soujanya Poria, Pengfei Liu, Zhengzhong Liu, Xuguang Ren, Eduard Hovy, Iryna Gurevych, Preslav Nakov, Monojit Choudhury, Timothy Baldwin

To address this gap, we introduce Libra-Leaderboard, a comprehensive
framework designed to rank LLMs through a balanced evaluation of performance
and safety. Combining a dynamic leaderboard with an interactive LLM arena,
Libra-Leaderboard encourages the joint optimization of capability and safety.
Unlike traditional approaches that average performance and safety metrics,
Libra-Leaderboard uses a distance-to-optimal-score method to calculate the
overall rankings. This approach incentivizes models to achieve a balance rather
than excelling in one dimension at the expense of some other ones. In the first
release, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading
organizations, identifying critical safety challenges even in state-of-the-art
models.

摘要：為了彌補此差距，我們引進了 Libra-Leaderboard，這是一個綜合性框架，旨在透過平衡效能和安全性的評估來對 LLM 進行排名。Libra-Leaderboard 結合了動態排行榜和互動式 LLM 競技場，鼓勵對能力和安全性進行聯合最佳化。與平均效能和安全性指標的傳統方法不同，Libra-Leaderboard 使用距離最佳分數的方法來計算整體排名。這種方法鼓勵模型取得平衡，而不是在某個面向表現出色而犧牲其他面向。在第一個版本中，Libra-Leaderboard 從 14 個領先組織中評估了 26 個主流 LLM，即使在最先進的模型中也找出了重大的安全性挑戰。

##### **Token-Budget-Aware LLM Reasoning**
2412.18547v1 by Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang

Reasoning is critical for large language models (LLMs) to excel in a wide
range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM
performance by decomposing problems into intermediate steps, they also incur
significant overhead in token usage, leading to increased costs. We find that
the reasoning process of current LLMs is unnecessarily lengthy and it can be
compressed by including a reasonable token budget in the prompt, but the choice
of token budget plays a crucial role in the actual compression effectiveness.
We then propose a token-budget-aware LLM reasoning framework, which dynamically
estimates token budgets for different problems based on reasoning complexity
and uses the estimated token budgets to guide the reasoning process.
Experiments show that our method effectively reduces token costs in CoT
reasoning with only a slight performance reduction, offering a practical
solution to balance efficiency and accuracy in LLM reasoning. Code:
https://github.com/GeniusHTX/TALE.

摘要：推理對於大型語言模型 (LLM) 在廣泛任務中表現出色至關重要。雖然像思想鏈 (CoT) 推理等方法透過將問題分解成中間步驟來增強 LLM 效能，但它們也會造成代幣使用上的顯著負擔，導致成本增加。我們發現，目前 LLM 的推理過程過於冗長，而且可以透過在提示中包含合理的代幣預算來壓縮，但代幣預算的選擇在實際壓縮效果中扮演關鍵角色。我們接著提出一個具備代幣預算感知功能的 LLM 推理架構，它會根據推理複雜度動態估計不同問題的代幣預算，並使用估計的代幣預算來引導推理過程。實驗顯示，我們的方法有效降低了 CoT 推理中的代幣成本，效能僅略微下降，提供了一個在 LLM 推理中平衡效率與精確度的實用解決方案。程式碼：https://github.com/GeniusHTX/TALE。

##### **Consistency Checks for Language Model Forecasters**
2412.18544v1 by Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr

Forecasting is a task that is difficult to evaluate: the ground truth can
only be known in the future. Recent work showing LLM forecasters rapidly
approaching human-level performance begs the question: how can we benchmark and
evaluate these forecasters instantaneously? Following the consistency check
framework, we measure the performance of forecasters in terms of the
consistency of their predictions on different logically-related questions. We
propose a new, general consistency metric based on arbitrage: for example, if a
forecasting AI illogically predicts that both the Democratic and Republican
parties have 60% probability of winning the 2024 US presidential election, an
arbitrageur can trade against the forecaster's predictions and make a profit.
We build an automated evaluation system that generates a set of base questions,
instantiates consistency checks from these questions, elicits the predictions
of the forecaster, and measures the consistency of the predictions. We then
build a standard, proper-scoring-rule forecasting benchmark, and show that our
(instantaneous) consistency metrics correlate with LLM forecasters' ground
truth Brier scores (which are only known in the future). We also release a
consistency benchmark that resolves in 2028, providing a long-term evaluation
tool for forecasting.

摘要：預測是一項難以評估的任務：只有在未來才能知道真實情況。最近的研究顯示，LLM 預測員正迅速接近人類層級的表現，這引發了一個問題：我們如何立即對這些預測員進行基準測試和評估？遵循一致性檢查架構，我們根據預測員在不同邏輯相關問題上的預測一致性來衡量其表現。我們提出了一個新的、基於套利的通用一致性指標：例如，如果一個預測 AI 沒有邏輯地預測民主黨和共和黨在 2024 年美國總統大選中都有 60% 的獲勝機率，套利者可以針對預測員的預測進行交易並獲利。我們建立了一個自動化評估系統，用於產生一組基本問題，從這些問題中實例化一致性檢查，引出預測員的預測，並衡量預測的一致性。然後，我們建立了一個標準的、適當計分規則預測基準，並表明我們的（即時）一致性指標與 LLM 預測員的真實布賴爾得分（僅在未來才知道）相關。我們還發布了一個在 2028 年解決的一致性基準，提供了一個用於預測的長期評估工具。

##### **Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**
2412.18537v1 by Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen

Large Language Models (LLMs) demonstrate remarkable capabilities, yet
struggle with hallucination and outdated knowledge when tasked with complex
knowledge reasoning, resulting in factually incorrect outputs. Previous studies
have attempted to mitigate it by retrieving factual knowledge from large-scale
knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of
answers. However, this kind of approach often introduces noise and irrelevant
data, especially in situations with extensive context from multiple knowledge
aspects. In this way, LLM attention can be potentially mislead from question
and relevant information. In our study, we introduce an Adaptive Multi-Aspect
Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge
including entities, relations, and subgraphs, and converts each piece of
retrieved text into prompt embeddings. The Amar framework comprises two key
sub-components: 1) a self-alignment module that aligns commonalities among
entities, relations, and subgraphs to enhance retrieved text, thereby reducing
noise interference; 2) a relevance gating module that employs a soft gate to
learn the relevance score between question and multi-aspect retrieved data, to
determine which information should be used to enhance LLMs' output, or even
filtered altogether. Our method has achieved state-of-the-art performance on
two common datasets, WebQSP and CWQ, showing a 1.9\% improvement in accuracy
over its best competitor and a 6.6\% improvement in logical form generation
over a method that directly uses retrieved text as context prompts. These
results demonstrate the effectiveness of Amar in improving the reasoning of
LLMs.

摘要：大型語言模型 (LLM) 展示了非凡的能力，但當它們被賦予複雜的知識推理任務時，卻會陷入幻覺和過時知識的困境，導致事實上不正確的輸出。先前的研究已嘗試通過從大規模知識圖譜 (KG) 中擷取事實知識來減輕這種情況，以協助 LLM 進行邏輯推理和答案預測。然而，這種方法通常會引入雜訊和無關資料，特別是在具有來自多個知識面向的廣泛背景的情況下。通過這種方式，LLM 注意力可能會被問題和相關資訊誤導。在我們的研究中，我們引入了自適應多面向檢索增強的知識圖譜 (Amar) 架構。此方法擷取包括實體、關係和子圖的知識，並將每個擷取的文字轉換為提示嵌入。Amar 架構包含兩個關鍵子元件：1) 一個自我對齊模組，它對齊實體、關係和子圖之間的共性以增強擷取的文字，從而減少雜訊干擾；2) 一個相關性閘門模組，它採用軟閘門來學習問題與多面向擷取資料之間的相关性分數，以確定哪些資訊應被用來增強 LLM 的輸出，甚至完全過濾掉。我們的模型在兩個常見的資料集 WebQSP 和 CWQ 上達到了最先進的效能，與最佳競爭者相比，準確度提升了 1.9%，與直接使用擷取文字作為背景提示的方法相比，邏輯形式產生的改進為 6.6%。這些結果證明了 Amar 在改善 LLM 推理方面的有效性。

##### **Characterizations of Language Generation With Breadth**
2412.18530v1 by Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas

We study language generation in the limit, introduced by Kleinberg and
Mullainathan [KM24], building on classical works of Gold [Gol67] and Angluin
[Ang79]. [KM24] proposed an algorithm that generates strings from any countable
language collection in the limit. While their algorithm eventually outputs
strings from the target language $K$, it sacrifices breadth, i.e., the ability
to generate all strings in $K$. A key open question in [KM24] is whether this
trade-off between consistency and breadth is inherrent.
  Recent works proposed different notions of consistent generation with
breadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced three
definitions: generation with exact breadth, approximate breadth, and
unambiguous generation. Concurrently and independently, Charikar and Pabbaraju
[CP24a] proposed exhaustive generation. Both works examined when generation
with these notions of breadth is possible.
  Building on [CP24a, KVM24], we fully characterize language generation for
these notions and their natural combinations. For exact breadth, we provide an
unconditional lower bound, removing a technical condition from [KVM24] and
extending the result of [CP24a] that holds for specific collections of
languages. We show that generation with exact breadth is characterized by
Angluin's condition for identification. We further introduce a weaker version
of Angluin's condition that tightly characterizes both approximate breadth and
exhaustive generation, proving their equivalence. Additionally, we show that
unambiguous generation is also characterized by Angluin's condition as a
special case of a broader result. Finally, we strengthen [KVM24] by giving
unconditional lower bounds for stable generators, showing that Angluin's
condition characterizes the previous breadth notions for stable generators.
This shows a separation between stable and unstable generation with approximate
breadth.

摘要：<paragraph>我們研究語言生成極限，由 Kleinberg 和 Mullainathan [KM24] 提出，建立在 Gold [Gol67] 和 Angluin [Ang79] 的經典著作之上。[KM24] 提出了一種演算法，可以從任何可數語言集合中生成字串。雖然他們的演算法最終會輸出目標語言 $K$ 中的字串，但它犧牲了廣度，也就是生成 $K$ 中所有字串的能力。[KM24] 中一個關鍵的開放問題是，一致性和廣度之間的這種權衡是否固有。
近期著作提出了廣度一致生成的不同概念。Kalavasis、Mehrotra 和 Velegkas [KVM24] 介紹了三個定義：廣度精確生成、廣度近似生成和無歧義生成。同時獨立地，Charikar 和 Pabbaraju [CP24a] 提出窮盡生成。兩項著作都探討了廣度的這些概念何時可能產生。
建立在 [CP24a, KVM24] 之上，我們完全描述了這些概念的語言生成及其自然組合。對於廣度精確，我們提供了無條件的下限，從 [KVM24] 中移除技術條件，並擴充套件 [CP24a] 對特定語言集合成立的結果。我們表明，廣度精確的生成是由 Angluin 的識別條件特徵化的。我們進一步引入 Angluin 條件的較弱版本，它緊密地特徵化了近似廣度和窮盡生成，證明了它們的等價性。此外，我們表明無歧義生成也由 Angluin 條件特徵化，作為廣泛結果的特殊情況。最後，我們通過提供穩定生成器的無條件下限來加強 [KVM24]，表明 Angluin 條件特徵化了穩定生成器先前的廣度概念。這顯示了近似廣度穩定和不穩定生成之間的分離。</paragraph>

##### **Think or Remember? Detecting and Directing LLMs Towards Memorization or Generalization**
2412.18497v1 by Yi-Fu Fu, Yu-Chieh Tu, Tzu-Ling Cheng, Cheng-Yu Lin, Yi-Ting Yang, Heng-Yi Liu, Keng-Te Liao, Da-Cheng Juan, Shou-De Lin

In this paper, we explore the foundational mechanisms of memorization and
generalization in Large Language Models (LLMs), inspired by the functional
specialization observed in the human brain. Our investigation serves as a case
study leveraging specially designed datasets and experimental-scale LLMs to lay
the groundwork for understanding these behaviors. Specifically, we aim to first
enable LLMs to exhibit both memorization and generalization by training with
the designed dataset, then (a) examine whether LLMs exhibit neuron-level
spatial differentiation for memorization and generalization, (b) predict these
behaviors using model internal representations, and (c) steer the behaviors
through inference-time interventions. Our findings reveal that neuron-wise
differentiation of memorization and generalization is observable in LLMs, and
targeted interventions can successfully direct their behavior.

摘要：在本文中，我們探討了大型語言模型 (LLM) 中記憶和概括的基本機制，靈感來自於人類大腦中觀察到的功能專門化。我們的研究作為一個案例研究，利用專門設計的資料集和實驗規模的 LLM，為理解這些行為奠定基礎。具體來說，我們的目標是首先讓 LLM 通過使用設計的資料集進行訓練來展示記憶和概括，然後 (a) 檢查 LLM 是否表現出神經元級別的空間區分以進行記憶和概括，(b) 使用模型內部表示預測這些行為，以及 (c) 引導行為通過推理時間干預。我們的研究結果表明，在 LLM 中可以觀察到記憶和概括的神經元差異，並且有針對性的干預可以成功地指導它們的行為。

##### **Generating event descriptions under syntactic and semantic constraints**
2412.18496v1 by Angela Cao, Faye Holt, Jonas Chan, Stephanie Richter, Lelia Glass, Aaron Steven White

With the goal of supporting scalable lexical semantic annotation, analysis,
and theorizing, we conduct a comprehensive evaluation of different methods for
generating event descriptions under both syntactic constraints -- e.g. desired
clause structure -- and semantic constraints -- e.g. desired verb sense. We
compare three different methods -- (i) manual generation by experts; (ii)
sampling from a corpus annotated for syntactic and semantic information; and
(iii) sampling from a language model (LM) conditioned on syntactic and semantic
information -- along three dimensions of the generated event descriptions: (a)
naturalness, (b) typicality, and (c) distinctiveness. We find that all methods
reliably produce natural, typical, and distinctive event descriptions, but that
manual generation continues to produce event descriptions that are more
natural, typical, and distinctive than the automated generation methods. We
conclude that the automated methods we consider produce event descriptions of
sufficient quality for use in downstream annotation and analysis insofar as the
methods used for this annotation and analysis are robust to a small amount of
degradation in the resulting event descriptions.

摘要：为了支持可扩展的词汇语义注释、分析和理论化，我们对在语法限制（例如所需的从句结构）和语义限制（例如所需的动词意义）下生成事件描述的不同方法进行了全面评估。我们比较了三种不同的方法——（i）专家手动生成；（ii）从为句法和语义信息注释的语料库中抽样；（iii）从以句法和语义信息为条件的语言模型（LM）中抽样——沿着生成事件描述的三个维度：（a）自然性、（b）典型性，和（c）独特性。我们发现所有方法都能可靠地生成自然、典型和独特的事件描述，但手动生成持续生成比自动化生成方法更自然、更典型和更独特的事件描述。我们得出结论，我们考虑的自动化方法产生的事件描述质量足以用于下游注释和分析，只要用于此注释和分析的方法对生成的事件描述中少量的退化具有鲁棒性。

##### **How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?**
2412.18495v1 by Sara Papi, Peter Polak, Ondřej Bojar, Dominik Macháček

Simultaneous speech-to-text translation (SimulST) translates source-language
speech into target-language text concurrently with the speaker's speech,
ensuring low latency for better user comprehension. Despite its intended
application to unbounded speech, most research has focused on human
pre-segmented speech, simplifying the task and overlooking significant
challenges. This narrow focus, coupled with widespread terminological
inconsistencies, is limiting the applicability of research outcomes to
real-world applications, ultimately hindering progress in the field. Our
extensive literature review of 110 papers not only reveals these critical
issues in current research but also serves as the foundation for our key
contributions. We 1) define the steps and core components of a SimulST system,
proposing a standardized terminology and taxonomy; 2) conduct a thorough
analysis of community trends, and 3) offer concrete recommendations and future
directions to bridge the gaps in existing literature, from evaluation
frameworks to system architectures, for advancing the field towards more
realistic and effective SimulST solutions.

摘要：同聲傳譯（SimulST）在講者說話的同時，將源語言的語音轉換為目標語言的文字，確保低延遲以利使用者更佳地理解。儘管其預期應用於無限制的語音，但多數研究都專注於人類預先分段的語音，簡化了任務並忽略了重要的挑戰。這種狹隘的焦點，加上廣泛的術語不一致，限制了研究成果在現實世界中的應用，最終阻礙了該領域的進展。我們對 110 篇論文進行的廣泛文獻回顧，不僅揭示了當前研究中的這些關鍵問題，也作為我們關鍵貢獻的基礎。我們 1) 定義 SimulST 系統的步驟和核心組成部分，提出標準化術語和分類法；2) 徹底分析社群趨勢，以及 3) 提供具體建議和未來方向，以彌合現有文獻中的差距，從評估架構到系統架構，以推動該領域朝向更實際且有效的 SimulST 解決方案。

##### **An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving**
2412.18489v1 by Gnaneswar Villuri, Alex Doboli

This report characterized the suitability of existing datasets for devising
new Machine Learning models, decision making methods, and analysis algorithms
to improve Collaborative Problem Solving and then enumerated requirements for
future datasets to be devised. Problem solving was assumed to be performed in
teams of about three, four members, which talked to each other. A dataset
consists of the speech recordings of such teams. The characterization
methodology was based on metrics that capture cognitive, social, and emotional
activities and situations. The report presented the analysis of a large group
of datasets developed for Spoken Language Understanding, a research area with
some similarity to Collaborative Problem Solving.

摘要：本報告描述了現有資料集的適用性，用於設計新的機器學習模型、決策方法和分析演算法，以改善協作問題解決，然後列舉了未來資料集設計的要求。問題解決被假設為由約三到四名成員組成的團隊執行，這些成員會彼此交談。資料集包含這些團隊的語音記錄。描述方法基於捕捉認知、社會和情緒活動和情境的指標。報告提出了對一組大型資料集的分析，這些資料集是為語音語言理解而開發的，這是一個與協作問題解決有些類似的研究領域。

##### **Segment-Based Attention Masking for GPTs**
2412.18487v1 by Shahar Katz, Liran Ringel, Yaniv Romano, Lior Wolf

Modern Language Models (LMs) owe much of their success to masked causal
attention, the backbone of Generative Pre-Trained Transformer (GPT) models.
Although GPTs can process the entire user prompt at once, the causal masking is
applied to all input tokens step-by-step, mimicking the generation process.
This imposes an unnecessary constraint during the initial "prefill" phase when
the model processes the input prompt and generates the internal representations
before producing any output tokens. In this work, attention is masked based on
the known block structure at the prefill phase, followed by the conventional
token-by-token autoregressive process after that. For example, in a typical
chat prompt, the system prompt is treated as one block, and the user prompt as
the next one. Each of these is treated as a unit for the purpose of masking,
such that the first tokens in each block can access the subsequent tokens in a
non-causal manner. Then, the model answer is generated in the conventional
causal manner. This Segment-by-Segment scheme entails no additional
computational overhead. When integrating it into models such as Llama and Qwen,
state-of-the-art performance is consistently achieved.

摘要：現代語言模型 (LM) 的成功在很大程度上歸功於遮蔽式因果注意力，這是生成式預訓練Transformer (GPT) 模型的骨幹。
儘管 GPT 可以一次處理整個使用者提示，但因果遮蔽會逐步應用於所有輸入標記，模擬產生過程。
當模型處理輸入提示並在產生任何輸出標記之前產生內部表示時，這會在初始「預填」階段施加不必要的約束。在這項工作中，注意力會根據預填階段已知的區塊結構進行遮蔽，然後再進行傳統的逐一標記自迴歸過程。例如，在典型的聊天提示中，系統提示被視為一個區塊，而使用者提示被視為下一個區塊。這些中的每一個都被視為遮蔽目的的單位，使得每個區塊中的第一個標記可以非因果方式存取後續標記。然後，模型答案會以傳統的因果方式產生。這種逐段架構不需要額外的運算開銷。當將其整合到 Llama 和 Qwen 等模型中時，始終能達到最先進的效能。

##### **MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation**
2412.18464v1 by Tengfei He, Xiao Zhou

Social segregation in cities, spanning racial, residential, and income
dimensions, is becoming more diverse and severe. As urban spaces and social
relations grow more complex, residents in metropolitan areas experience varying
levels of social segregation. If left unaddressed, this could lead to increased
crime rates, heightened social tensions, and other serious issues. Effectively
quantifying and analyzing the structures within urban spaces and resident
interactions is crucial for addressing segregation. Previous studies have
mainly focused on surface-level indicators of urban segregation, lacking
comprehensive analyses of urban structure and mobility. This limitation fails
to capture the full complexity of segregation. To address this gap, we propose
a framework named Motif-Enhanced Graph Prototype Learning (MotifGPL),which
consists of three key modules: prototype-based graph structure extraction,
motif distribution discovery, and urban graph structure reconstruction.
Specifically, we use graph structure prototype learning to extract key
prototypes from both the urban spatial graph and the origin-destination graph,
incorporating key urban attributes such as points of interest, street view
images, and flow indices. To enhance interpretability, the motif distribution
discovery module matches each prototype with similar motifs, representing
simpler graph structures reflecting local patterns. Finally, we use the motif
distribution results to guide the reconstruction of the two graphs. This model
enables a detailed exploration of urban spatial structures and resident
mobility patterns, helping identify and analyze motif patterns that influence
urban segregation, guiding the reconstruction of urban graph structures.
Experimental results demonstrate that MotifGPL effectively reveals the key
motifs affecting urban social segregation and offer robust guidance for
mitigating this issue.

摘要：<paragraph>城市中的社會隔離，跨越種族、居住和收入層面，正變得更加多元且嚴重。隨著城市空間和社會關係日益複雜，大都會地區的居民經歷著不同程度的社會隔離。如果不加以解決，這可能會導致犯罪率上升、社會緊張加劇以及其他嚴重問題。有效量化和分析城市空間內部的結構和居民互動對於解決隔離至關重要。先前的研究主要集中於城市隔離的表面指標，缺乏對城市結構和流動性的全面分析。這種限制未能捕捉到隔離的全部複雜性。為了解決這一差距，我們提出了一個名為 Motif 增強圖原型學習 (MotifGPL) 的框架，它包含三個關鍵模組：基於原型的圖結構提取、主題分佈發現和城市圖結構重建。具體來說，我們使用圖結構原型學習從城市空間圖和起點目的地圖中提取關鍵原型，並整合了關鍵的城市屬性，例如興趣點、街景圖像和流動指數。為了增強可解釋性，主題分佈發現模組將每個原型與相似的主題進行匹配，表示反映局部模式的更簡單的圖結構。最後，我們使用主題分佈結果來指導兩個圖的重建。這個模型可以詳細探索城市空間結構和居民流動模式，有助於識別和分析影響城市隔離的主題模式，指導城市圖結構的重建。實驗結果表明，MotifGPL 有效地揭示了影響城市社會隔離的關鍵主題，並為緩解這一問題提供了強有力的指導。</paragraph>

##### **GeFL: Model-Agnostic Federated Learning with Generative Models**
2412.18460v1 by Honggu Kang, Seohyeon Cha, Joonhyuk Kang

Federated learning (FL) is a promising paradigm in distributed learning while
preserving the privacy of users. However, the increasing size of recent models
makes it unaffordable for a few users to encompass the model. It leads the
users to adopt heterogeneous models based on their diverse computing
capabilities and network bandwidth. Correspondingly, FL with heterogeneous
models should be addressed, given that FL typically involves training a single
global model. In this paper, we propose Generative Model-Aided Federated
Learning (GeFL), incorporating a generative model that aggregates global
knowledge across users of heterogeneous models. Our experiments on various
classification tasks demonstrate notable performance improvements of GeFL
compared to baselines, as well as limitations in terms of privacy and
scalability. To tackle these concerns, we introduce a novel framework, GeFL-F.
It trains target networks aided by feature-generative models. We empirically
demonstrate the consistent performance gains of GeFL-F, while demonstrating
better privacy preservation and robustness to a large number of clients. Codes
are available at [1].

摘要：聯盟學習 (FL) 是分布式學習中的一個有前途的範例，同時保護使用者的隱私。然而，近期模型的尺寸越來越大，使得少數使用者無法涵蓋模型。這導致使用者採用基於其不同的運算能力和網路頻寬的異質模型。相應地，應該解決具有異質模型的 FL，因為 FL 通常涉及訓練單一的全域模型。在本文中，我們提出生成模型輔助聯盟學習 (GeFL)，它結合了一個生成模型，該模型彙總了異質模型使用者的全域知識。我們在各種分類任務上進行的實驗表明，與基線相比，GeFL 的效能有顯著的提升，以及在隱私和可擴充性方面的限制。為了解決這些問題，我們引入了一個新的架構 GeFL-F。它訓練目標網路，並輔以特徵生成模型。我們經驗性地證明了 GeFL-F 的效能持續提升，同時證明了更好的隱私保護和對大量客戶的穩健性。程式碼可在 [1] 取得。

##### **Multi-Agent Norm Perception and Induction in Distributed Healthcare**
2412.18454v1 by Chao Li, Olga Petruchik, Elizaveta Grishanina, Sergey Kovalchuk

This paper presents a Multi-Agent Norm Perception and Induction Learning
Model aimed at facilitating the integration of autonomous agent systems into
distributed healthcare environments through dynamic interaction processes. The
nature of the medical norm system and its sharing channels necessitates
distinct approaches for Multi-Agent Systems to learn two types of norms.
Building on this foundation, the model enables agents to simultaneously learn
descriptive norms, which capture collective tendencies, and prescriptive norms,
which dictate ideal behaviors. Through parameterized mixed probability density
models and practice-enhanced Markov games, the multi-agent system perceives
descriptive norms in dynamic interactions and captures emergent prescriptive
norms. We conducted experiments using a dataset from a neurological medical
center spanning from 2016 to 2020.

摘要：本文提出了一個多主體規範感知與歸納學習模型，旨在透過動態互動程序促進自主主體系統整合到分散式醫療保健環境中。醫療規範系統的本質及其共享管道需要不同的方法，讓多主體系統學習兩種規範。基於此基礎，該模型讓主體能夠同時學習描述性規範（捕捉集體傾向）和規範性規範（規定理想行為）。透過參數化混合機率密度模型和實務增強馬可夫博弈，多主體系統在動態互動中感知描述性規範，並捕捉新興的規範性規範。我們使用 2016 年至 2020 年期間一個神經醫學醫療中心的数据集進行了實驗。

##### **Is Large Language Model Good at Triple Set Prediction? An Empirical Study**
2412.18443v1 by Yuan Yuan, Yajing Xu, Wen Zhang

The core of the Knowledge Graph Completion (KGC) task is to predict and
complete the missing relations or nodes in a KG. Common KGC tasks are mostly
about inferring unknown elements with one or two elements being known in a
triple. In comparison, the Triple Set Prediction (TSP) task is a more realistic
knowledge graph completion task. It aims to predict all elements of unknown
triples based on the information from known triples. In recent years, large
language models (LLMs) have exhibited significant advancements in language
comprehension, demonstrating considerable potential for KGC tasks. However, the
potential of LLM on the TSP task has not yet to be investigated. Thus in this
paper we proposed a new framework to explore the strengths and limitations of
LLM in the TSP task. Specifically, the framework consists of LLM-based rule
mining and LLM-based triple set prediction. The relation list of KG embedded
within rich semantic information is first leveraged to prompt LLM in the
generation of rules. This process is both efficient and independent of
statistical information, making it easier to mine effective and realistic
rules. For each subgraph, the specified rule is applied in conjunction with the
relevant triples within that subgraph to guide the LLM in predicting the
missing triples. Subsequently, the predictions from all subgraphs are
consolidated to derive the complete set of predicted triples on KG. Finally,
the method is evaluated on the relatively complete CFamily dataset. The
experimental results indicate that when LLMs are required to adhere to a large
amount of factual knowledge to predict missing triples, significant
hallucinations occurs, leading to a noticeable decline in performance. To
further explore the causes of this phenomenon, this paper presents a
comprehensive analysis supported by a detailed case study.

摘要：知識圖譜完成 (KGC) 任務的核心是預測和完成 KG 中遺失的關係或節點。常見的 KGC 任務大多是關於推論未知元素，其中一個或兩個元素在三元組中已知。相比之下，三元組集合預測 (TSP) 任務是一個更實際的知識圖譜完成任務。它旨在根據已知三元組中的資訊預測未知三元組的所有元素。近年來，大型語言模型 (LLM) 在語言理解方面表現出顯著的進步，顯示出 KGC 任務的巨大潛力。然而，LLM 在 TSP 任務上的潛力尚未得到探討。因此，在本文中，我們提出了一個新的框架來探索 LLM 在 TSP 任務中的優勢和局限性。具體來說，該框架包含基於 LLM 的規則挖掘和基於 LLM 的三元組集合預測。嵌入豐富語義資訊的 KG 關係清單首先被利用來提示 LLM 生成規則。這個過程既有效率又獨立於統計資訊，使得挖掘有效且實際的規則變得更容易。對於每個子圖，指定規則與該子圖中相關的三元組結合使用，以指導 LLM 預測遺失的三元組。隨後，合併所有子圖的預測，以推導 KG 上預測三元組的完整集合。最後，該方法在相對完整的 CFamily 資料集上進行評估。實驗結果表明，當要求 LLM 遵循大量事實知識來預測遺失的三元組時，會發生顯著的幻覺，導致效能顯著下降。為了進一步探討這種現象的原因，本文提出了由詳細案例研究支援的全面分析。

##### **Unlocking the Potential of Multiple BERT Models for Bangla Question Answering in NCTB Textbooks**
2412.18440v1 by Abdullah Khondoker, Enam Ahmed Taufik, Md Iftekhar Islam Tashik, S M Ishtiak mahmud, Antara Firoz Parsa

Evaluating text comprehension in educational settings is critical for
understanding student performance and improving curricular effectiveness. This
study investigates the capability of state-of-the-art language models-RoBERTa
Base, Bangla-BERT, and BERT Base-in automatically assessing Bangla
passage-based question-answering from the National Curriculum and Textbook
Board (NCTB) textbooks for classes 6-10. A dataset of approximately 3,000
Bangla passage-based question-answering instances was compiled, and the models
were evaluated using F1 Score and Exact Match (EM) metrics across various
hyperparameter configurations. Our findings revealed that Bangla-BERT
consistently outperformed the other models, achieving the highest F1 (0.75) and
EM (0.53) scores, particularly with smaller batch sizes, the inclusion of stop
words, and a moderate learning rate. In contrast, RoBERTa Base demonstrated the
weakest performance, with the lowest F1 (0.19) and EM (0.27) scores under
certain configurations. The results underscore the importance of fine-tuning
hyperparameters for optimizing model performance and highlight the potential of
machine learning models in evaluating text comprehension in educational
contexts. However, limitations such as dataset size, spelling inconsistencies,
and computational constraints emphasize the need for further research to
enhance the robustness and applicability of these models. This study lays the
groundwork for the future development of automated evaluation systems in
educational institutions, providing critical insights into model performance in
the context of Bangla text comprehension.

摘要：<paragraph>評估教育環境中的文本理解力對於了解學生表現和改善課程成效至關重要。本研究探討了最先進的語言模型 RoBERTa Base、Bangla-BERT 和 BERT Base 在自動評估國家課程和教科書委員會 (NCTB) 6-10 年級教科書中的孟加拉語段落式問答的能力。編制了一個包含大約 3,000 個孟加拉語段落式問答實例的資料集，並使用 F1 分數和完全匹配 (EM) 指標在各種超參數配置中評估模型。我們的研究結果顯示，Bangla-BERT 持續優於其他模型，獲得了最高的 F1（0.75）和 EM（0.53）分數，特別是在批次大小較小、包含停用詞和適度的學習率時。相比之下，RoBERTa Base 的表現最差，在某些配置下獲得了最低的 F1（0.19）和 EM（0.27）分數。這些結果強調了微調超參數以最佳化模型效能的重要性，並突顯了機器學習模型在評估教育環境中的文本理解力的潛力。然而，資料集大小、拼寫不一致和計算限制等限制強調了進一步研究以增強這些模型的穩健性和適用性的必要性。本研究為教育機構中自動化評估系統的未來發展奠定了基礎，提供了孟加拉語文本理解語境中模型效能的重要見解。</paragraph>

##### **Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**
2412.18428v1 by Farhad Nooralahzadeh, Yi Zhang, Jonathan Furst, Kurt Stockinger

International enterprises, organizations, or hospitals collect large amounts
of multi-modal data stored in databases, text documents, images, and videos.
While there has been recent progress in the separate fields of multi-modal data
exploration as well as in database systems that automatically translate natural
language questions to database query languages, the research challenge of
querying database systems combined with other unstructured modalities such as
images in natural language is widely unexplored.
  In this paper, we propose XMODE - a system that enables explainable,
multi-modal data exploration in natural language. Our approach is based on the
following research contributions: (1) Our system is inspired by a real-world
use case that enables users to explore multi-modal information systems. (2)
XMODE leverages a LLM-based agentic AI framework to decompose a natural
language question into subtasks such as text-to-SQL generation and image
analysis. (3) Experimental results on multi-modal datasets over relational data
and images demonstrate that our system outperforms state-of-the-art multi-modal
exploration systems, excelling not only in accuracy but also in various
performance metrics such as query latency, API costs, planning efficiency, and
explanation quality, thanks to the more effective utilization of the reasoning
capabilities of LLMs.

摘要：國際企業、組織或醫院收集大量儲存在資料庫、文字文件、圖片和影片中的多模態資料。雖然多模態資料探勘的獨立領域以及自動將自然語言問題轉換成資料庫查詢語言的資料庫系統最近已有進展，但結合其他非結構化模態（例如圖片）以自然語言查詢資料庫系統的研究挑戰仍鮮少探討。在此篇論文中，我們提出 XMODE，一個能使用自然語言進行可解釋多模態資料探勘的系統。我們的做法基於以下研究貢獻：(1) 我們的系統靈感來自一個能讓使用者探索多模態資訊系統的真實世界使用案例。(2) XMODE 利用一個基於 LLM 的代理 AI 架構，將自然語言問題分解成文字轉 SQL 產生和圖片分析等子任務。(3) 在關聯資料和圖片的多模態資料集上的實驗結果顯示，我們的系統優於最先進的多模態探勘系統，不僅在準確度上表現出色，在各種效能指標（例如查詢延遲、API 成本、規劃效率和說明品質）上也表現出色，這要歸功於更有效地利用 LLM 的推理能力。

##### **GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent**
2412.18426v1 by Kangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen, Zhi Chen, Tiancheng Zhao, Xiubo Liang, Jianwei Yin

Nowadays, research on GUI agents is a hot topic in the AI community. However,
current research focuses on GUI task automation, limiting the scope of
applications in various GUI scenarios. In this paper, we propose a formalized
and comprehensive environment to evaluate the entire process of automated GUI
Testing (GTArena), offering a fair, standardized environment for consistent
operation of diverse multimodal large language models. We divide the testing
process into three key subtasks: test intention generation, test task
execution, and GUI defect detection, and construct a benchmark dataset based on
these to conduct a comprehensive evaluation. It evaluates the performance of
different models using three data types: real mobile applications, mobile
applications with artificially injected defects, and synthetic data, thoroughly
assessing their capabilities in this relevant task. Additionally, we propose a
method that helps researchers explore the correlation between the performance
of multimodal language large models in specific scenarios and their general
capabilities in standard benchmark tests. Experimental results indicate that
even the most advanced models struggle to perform well across all sub-tasks of
automated GUI Testing, highlighting a significant gap between the current
capabilities of Autonomous GUI Testing and its practical, real-world
applicability. This gap provides guidance for the future direction of GUI Agent
development. Our code is available at
https://github.com/ZJU-ACES-ISE/ChatUITest.

摘要：當今，GUI 代理的研究是 AI 社群中的熱門話題。然而，目前的
研究專注於 GUI 任務自動化，限制了在各種 GUI 場景中的應用範圍。在本文中，我們提出一個形式化且全面的環境來評估自動化 GUI
測試的整個過程 (GTArena)，為不同多模態大型語言模型的一致操作提供一個公平、標準化的環境。我們將測試過程分為三個關鍵的子任務：測試意圖產生、測試任務執行和 GUI 缺陷偵測，並根據這些任務建構一個基準資料集來進行全面的評估。它使用三種類型的資料來評估不同模型的效能：真實的行動應用程式、人工注入缺陷的行動應用程式，以及合成資料，徹底評估它們在這個相關任務中的能力。此外，我們提出一個方法，幫助研究人員探索特定場景中多模態大型語言模型的效能與其在標準基準測試中的整體能力之間的關聯性。實驗結果表明，即使是最先進的模型也很難在自動化 GUI 測試的所有子任務中表現良好，這突顯了當前自動化 GUI 測試的能力與其實際的真實世界適用性之間的顯著差距。這個差距為 GUI 代理開發的未來方向提供了指導。我們的程式碼可在
https://github.com/ZJU-ACES-ISE/ChatUITest 取得。

##### **LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating**
2412.18424v1 by Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, Cheng-Lin Liu

Large vision language models (LVLMs) have improved the document understanding
capabilities remarkably, enabling the handling of complex document elements,
longer contexts, and a wider range of tasks. However, existing document
understanding benchmarks have been limited to handling only a small number of
pages and fail to provide a comprehensive analysis of layout elements locating.
In this paper, we first define three primary task categories: Long Document
Understanding, numerical Reasoning, and cross-element Locating, and then
propose a comprehensive benchmark, LongDocURL, integrating above three primary
tasks and comprising 20 sub-tasks categorized based on different primary tasks
and answer evidences. Furthermore, we develop a semi-automated construction
pipeline and collect 2,325 high-quality question-answering pairs, covering more
than 33,000 pages of documents, significantly outperforming existing
benchmarks. Subsequently, we conduct comprehensive evaluation experiments on
both open-source and closed-source models across 26 different configurations,
revealing critical performance gaps in this field.

摘要：大型視覺語言模型 (LVLMs) 已顯著提升文件理解能力，能處理複雜的文件元素、較長的脈絡以及更廣泛的任務。然而，現有的文件理解基準僅限於處理少數頁面，且無法提供版面元素定位的全面分析。在本文中，我們首先定義三個主要的任務類別：長文件理解、數字推理和跨元素定位，然後提出一個全面的基準 LongDocURL，整合上述三個主要任務並包含 20 個子任務，根據不同的主要任務和答案證據進行分類。此外，我們開發了一個半自動化建構管道，並收集了 2,325 個高品質問答對，涵蓋超過 33,000 頁的文件，大幅優於現有的基準。隨後，我們對 26 種不同配置的開源和閉源模型進行全面的評估實驗，揭露了此領域的關鍵效能差距。

##### **Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models**
2412.18419v1 by Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen

As social changes accelerate, the incidence of psychosomatic disorders has
significantly increased, becoming a major challenge in global health issues.
This necessitates an innovative knowledge system and analytical methods to aid
in diagnosis and treatment. Here, we establish the ontology model and entity
types, using the BERT model and LoRA-tuned LLM for named entity recognition,
constructing the knowledge graph with 9668 triples. Next, by analyzing the
network distances between disease, symptom, and drug modules, it was found that
closer network distances among diseases can predict greater similarities in
their clinical manifestations, treatment approaches, and psychological
mechanisms, and closer distances between symptoms indicate that they are more
likely to co-occur. Lastly, by comparing the proximity d and proximity z score,
it was shown that symptom-disease pairs in primary diagnostic relationships
have a stronger association and are of higher referential value than those in
diagnostic relationships. The research results revealed the potential
connections between diseases, co-occurring symptoms, and similarities in
treatment strategies, providing new perspectives for the diagnosis and
treatment of psychosomatic disorders and valuable information for future mental
health research and practice.

摘要：隨著社會變遷加速，心身疾病發生率顯著增加，成為全球衛生議題上的重大挑戰。這需要創新的知識體系與分析方法，以協助診斷與治療。在此，我們建立了本体模型與實體類型，利用 BERT 模型與 LoRA 調校過的 LLM 進行命名實體辨識，建構出 9668 個三元組的知識圖譜。接著，透過分析疾病、症狀、藥物模組間的網路距離，發現疾病間較近的網路距離，可預測其臨床表現、治療方式、心理機轉的相似性較高；而症狀間距離較近，則表示較可能共現。最後，透過比較接近度 d 與接近度 z 分數，發現初次診斷關係中的症狀-疾病對，其關聯性較強、參考價值較高，優於診斷關係中的症狀-疾病對。研究成果揭示了疾病、共現症狀、治療策略間的潛在關聯，為心身疾病的診斷與治療提供了新的觀點，也為未來心理健康研究與實務提供了寶貴的資訊。

##### **Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English**
2412.18415v1 by Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Manvendra Kumar Nema, Raj Jaiswal, Rajiv Ratn Shah

Large Language Models (LLMs) excel in linguistic tasks but struggle with
mathematical reasoning, particularly in non English languages like Hindi. This
research aims to enhance the mathematical reasoning skills of smaller, resource
efficient open-source LLMs in both Hindi and English. We evaluate models like
OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,
Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,
and supervised fine-tuning. Our approach incorporates curriculum learning,
progressively training models on increasingly difficult problems, a novel
Decomposition Strategy to simplify complex arithmetic operations, and a
Structured Solution Design that divides solutions into phases. Our experiments
result in notable performance enhancements. WizardMath 7B exceeds Gemini's
accuracy on English datasets by +6% and matches Gemini's performance on Hindi
datasets. Adopting a bilingual approach that combines English and Hindi samples
achieves results comparable to individual language models, demonstrating the
capability to learn mathematical reasoning in both languages. This research
highlights the potential for improving mathematical reasoning in open-source
LLMs.

摘要：大型語言模型 (LLM) 在語言任務中表現出色，但在數學推理方面卻有困難，特別是在印地語等非英語語言中。這項研究旨在增強印地語和英語中較小、資源效率高的開源 LLM 的數學推理技能。我們使用零次、少次思考鏈 (CoT) 方法和監督微調來評估 OpenHathi 7B、LLaMA-2 7B、WizardMath 7B、Mistral 7B、LLeMMa 7B、MAmmoTH 7B、Gemini Pro 和 GPT-4 等模型。我們的做法結合了課程學習，逐漸訓練模型來解決越來越困難的問題，一種簡化複雜算術運算的新分解策略，以及將解法分為階段的結構化解法設計。我們的實驗產生了顯著的效能提升。WizardMath 7B 在英語資料集上的準確率比 Gemini 高出 +6%，並在印地語資料集上與 Gemini 的效能相匹配。採用結合英語和印地語範例的雙語方法，取得了與個別語言模型相當的結果，證明了學習兩種語言中數學推理的能力。這項研究突出了改善開源 LLM 中數學推理的潛力。

##### **Exploring Flexible Scenario Generation in Godot Simulator**
2412.18408v1 by Daniel Peraltai, Xin Qin

Cyber-physical systems (CPS) combine cyber and physical components engineered
to make decisions and interact within dynamic environments. Ensuring the safety
of CPS is of great importance, requiring extensive testing across diverse and
complex scenarios. To generate as many testing scenarios as possible, previous
efforts have focused on describing scenarios using formal languages to generate
scenes. In this paper, we introduce an alternative approach: reconstructing
scenes inside the open-source game engine, Godot. We have developed a pipeline
that enables the reconstruction of testing scenes directly from provided images
of scenarios. These reconstructed scenes can then be deployed within simulated
environments to assess a CPS. This approach offers a scalable and flexible
solution for testing CPS in realistic environments.

摘要：網路物理系統（CPS）結合網路和物理元件，旨在做出決策並在動態環境中進行互動。確保 CPS 的安全性非常重要，需要在多樣且複雜的場景中進行廣泛測試。為了產生儘可能多的測試場景，先前的努力著重於使用形式語言描述場景以產生場景。在本文中，我們介紹一種替代方法：在開源遊戲引擎 Godot 中重建場景。我們開發了一個管道，可以根據提供的場景影像直接重建測試場景。然後，這些重建的場景可以在模擬環境中部署，以評估 CPS。此方法提供了一個可擴充且靈活的解決方案，用於在真實環境中測試 CPS。

##### **A Statistical Framework for Ranking LLM-Based Chatbots**
2412.18407v1 by Siavash Ameli, Siyuan Zhuang, Ion Stoica, Michael W. Mahoney

Large language models (LLMs) have transformed natural language processing,
with frameworks like Chatbot Arena providing pioneering platforms for
evaluating these models. By facilitating millions of pairwise comparisons based
on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,
offering rich datasets for ranking models in open-ended conversational tasks.
Building upon this foundation, we propose a statistical framework that
incorporates key advancements to address specific challenges in pairwise
comparison analysis. First, we introduce a factored tie model that enhances the
ability to handle ties -- an integral aspect of human-judged comparisons --
significantly improving the model's fit to observed data. Second, we extend the
framework to model covariance between competitors, enabling deeper insights
into performance relationships and facilitating intuitive groupings into
performance tiers. Third, we resolve optimization challenges arising from
parameter non-uniqueness by introducing novel constraints, ensuring stable and
interpretable parameter estimation. Through rigorous evaluation and extensive
experimentation, our framework demonstrates substantial improvements over
existing methods in modeling pairwise comparison data. To support
reproducibility and practical adoption, we release leaderbot, an open-source
Python package implementing our models and analyses.

摘要：大型語言模型（LLM）已經轉變了自然語言處理，
像 Chatbot Arena 這樣的架構提供了開創性的平台來評估這些模型。透過協助基於人類判斷的數百萬成對比較，Chatbot Arena 已成為 LLM 評估的基石，提供豐富的資料集來對開放式對話任務中的模型進行排名。在此基礎上，我們提出了一個統計框架，其中包含了關鍵進展，以應對成對比較分析中的特定挑戰。首先，我們引入了一個分解的平手模型，它增強了處理平手（人類判斷比較的一個組成部分）的能力，顯著改善了模型對觀察到的資料的擬合度。其次，我們擴展了框架來對競爭者之間的協方差進行建模，從而深入了解績效關係，並促進直觀地將其分組為績效層級。第三，我們透過引入新的約束來解決由參數非唯一性產生的最佳化挑戰，確保參數估計的穩定性和可解釋性。透過嚴謹的評估和廣泛的實驗，我們的框架證明了在對成對比較資料建模方面，比現有方法有顯著的改進。為了支援可複製性和實際採用，我們發布了 leaderbot，這是一個開源的 Python 套件，用於實作我們的模型和分析。

##### **TPAoI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking**
2412.18391v1 by Haosheng He, Jianpeng Qi, Chao Liu, Junyu Dong, Yanwei Yu

In compute-first networking, maintaining fresh and accurate status
information at the network edge is crucial for effective access to remote
services. This process typically involves three phases: Status updating, user
accessing, and user requesting. However, current studies on status
effectiveness, such as Age of Information at Query (QAoI), do not
comprehensively cover all these phases. Therefore, this paper introduces a
novel metric, TPAoI, aimed at optimizing update decisions by measuring the
freshness of service status. The stochastic nature of edge environments,
characterized by unpredictable communication delays in updating, requesting,
and user access times, poses a significant challenge when modeling. To address
this, we model the problem as a Markov Decision Process (MDP) and employ a
Dueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive
experiments demonstrate that the proposed TPAoI metric effectively minimizes
AoI, ensuring timely and reliable service updates in dynamic edge environments.
Results indicate that TPAoI reduces AoI by an average of 47\% compared to QAoI
metrics and decreases update frequency by an average of 48\% relative to
conventional AoI metrics, showing significant improvement.

摘要：在計算優先網路中，在網路邊緣維護最新且準確的狀態資訊對於有效存取遠端服務至關重要。此程序通常包含三個階段：狀態更新、使用者存取和使用者要求。但是，目前關於狀態有效性的研究，例如查詢資訊年齡（QAoI），並未全面涵蓋所有這些階段。因此，本文介紹了一種新的指標 TPAoI，旨在透過衡量服務狀態的新鮮度來最佳化更新決策。邊緣環境的隨機性質，其特徵是在更新、要求和使用者存取時間中具有不可預測的通訊延遲，在建模時會構成重大挑戰。為了解決這個問題，我們將問題建模為馬可夫決策過程（MDP），並採用決鬥雙深度 Q 網路（D3QN）演算法進行最佳化。廣泛的實驗證明，所提出的 TPAoI 指標可有效最小化 AoI，確保在動態邊緣環境中及時且可靠的服務更新。結果表明，與 QAoI 指標相比，TPAoI 將 AoI 降低了平均 47%，並且與傳統 AoI 指標相比，將更新頻率降低了平均 48%，顯示出顯著的改善。

##### **RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction**
2412.18390v1 by Wu Xiaoping, Hu Jie, Wei Xiaoming

Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach
for high-fidelity image synthesis, operating diffusion processes on continuous
VAE latent, which significantly differ from the text generation methods
employed by Large Language Models (LLMs). In this paper, we introduce a novel
generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which
enhances the diffusion process through a recurrent token prediction mechanism,
thereby pioneering the field of Discrete Diffusion. By progressively
introducing Gaussian noise into the latent representations of images and
encoding them into vector-quantized tokens in a recurrent manner, RDPM
facilitates a unique diffusion process on discrete-value domains. This process
iteratively predicts the token codes for subsequent timesteps, transforming the
initial standard Gaussian noise into the source data distribution, aligning
with GPT-style models in terms of the loss function. RDPM demonstrates superior
performance while benefiting from the speed advantage of requiring only a few
inference steps. This model not only leverages the diffusion process to ensure
high-quality generation but also converts continuous signals into a series of
high-fidelity discrete tokens, thereby maintaining a unified optimization
strategy with other discrete tokens, such as text. We anticipate that this work
will contribute to the development of a unified model for multimodal
generation, specifically by integrating continuous signal domains such as
images, videos, and audio with text. We will release the code and model weights
to the open-source community.

摘要：擴散機率模型 (DPM) 已經成為高保真影像合成的實際方法，在連續 VAE 潛在變數上操作擴散程序，這與大型語言模型 (LLM) 所採用的文字生成方法有顯著差異。在本文中，我們介紹了一個新穎的生成架構，稱為遞迴擴散機率模型 (RDPM)，它透過遞迴符號預測機制增強擴散程序，從而開創了離散擴散領域。透過逐步將高斯雜訊引入影像的潛在表示中，並以遞迴方式將其編碼為向量量化的符號，RDPM 在離散值域上促進了獨特的擴散程序。此程序會反覆預測後續時間步長的符號碼，將初始的標準高斯雜訊轉換為來源資料分佈，在損失函數方面與 GPT 類型的模型對齊。RDPM 展現出優異的效能，同時受益於僅需要幾個推論步驟的速度優勢。此模型不僅利用擴散程序確保高品質的生成，還將連續訊號轉換為一系列高保真離散符號，從而與其他離散符號（例如文字）保持統一的最佳化策略。我們預期這項工作將有助於開發多模態生成的統一模型，特別是透過將影像、影片和音訊等連續訊號域與文字整合在一起。我們將向開源社群釋出程式碼和模型權重。

##### **Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model**
2412.18387v1 by Tenghui Li, Guoxu Zhou, Xuyang Zhao, Qibin Zhao

The scaling capability has been widely validated with respect to the number
of parameters and the size of training data. One important question that is
unexplored is that does scaling capability also exists similarly with respect
to the number of vision tokens? This study fills the gap by investigating the
relationship between the number of vision tokens and the performance of
vision-language models. Our theoretical analysis and empirical evaluations
reveal that the model exhibits weak scaling capabilities on the length \(N_l\),
with performance approximately \(S(N_l) \approx (c/N_l)^{\alpha}\), where \(c,
\alpha\) are hyperparameters. Interestingly, this scaling behavior remains
largely unaffected by the inclusion or exclusion of the user's question in the
input. Furthermore, fusing the user's question with the vision token can
enhance model performance when the question is relevant to the task. To address
the computational challenges associated with large-scale vision tokens, we
propose a novel architecture that efficiently reduces the token count while
integrating user question tokens into the representation. Our findings may
offer insights for developing more efficient and effective vision-language
models under specific task constraints.

摘要：在参数数量和训练数据大小方面，缩放能力已经得到广泛验证。一个尚未探索的重要问题是，缩放能力是否在视觉标记数量方面也类似存在？本研究通过调查视觉标记数量与视觉语言模型性能之间的关系来填补这一空白。我们的理论分析和实证评估表明，该模型在长度 \(N_l\) 上表现出较弱的缩放能力，性能约为 \(S(N_l) \approx (c/N_l)^{\alpha}\)，其中 \(c, \alpha\) 是超参数。有趣的是，这种缩放行为在很大程度上不受输入中是否包含用户问题的影响。此外，当问题与任务相关时，将用户问题与视觉标记融合可以提高模型性能。为了解决与大规模视觉标记相关的计算挑战，我们提出了一种新颖的架构，该架构在将用户问题标记集成到表示中时有效地减少了标记计数。我们的发现可能会为在特定任务约束下开发更有效率和更有效的视觉语言模型提供见解。

##### **ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots**
2412.18377v1 by Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdy, Nachshon Cohen, Alexander Libov, Guy Kushilevitz

The rise of LLMs has deflected a growing portion of human-computer
interactions towards LLM-based chatbots. The remarkable abilities of these
models allow users to interact using long, diverse natural language text
covering a wide range of topics and styles. Phrasing these messages is a time
and effort consuming task, calling for an autocomplete solution to assist
users. We introduce the task of chatbot interaction autocomplete. We present
ChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework
for LLM-based chatbot interactions. The framework includes a formal definition
of the task, coupled with suitable datasets and metrics. We use the framework
to evaluate After formally defining the task along with suitable datasets and
metrics, we test 9 models on the defined auto completion task, finding that
while current off-the-shelf models perform fairly, there is still much room for
improvement, mainly in ranking of the generated suggestions. We provide
insights for practitioners working on this task and open new research
directions for researchers in the field. We release our framework to serve as a
foundation for future research.

摘要：大型語言模型的興起將越來越多的人機互動轉向基於大型語言模型的聊天機器人。這些模型的卓越能力讓使用者能夠使用涵蓋廣泛主題和風格的長篇、多樣化的自然語言文字進行互動。表述這些訊息是一項耗時且費力的任務，需要一個自動完成的解決方案來協助使用者。我們引入了聊天機器人互動自動完成的任務。我們提出 ChaI-TeA：聊天互動自動完成；一個基於大型語言模型的聊天機器人互動的自動完成評估架構。該架構包含任務的正式定義，並結合適當的資料集和指標。我們使用該架構在定義的自動完成任務上評估 9 個模型，發現雖然現成的模型表現得相當好，但仍有很大的改進空間，特別是在生成建議的排名上。我們為從事這項任務的實務工作者提供見解，並為該領域的研究人員開啟新的研究方向。我們發布我們的架構，作為未來研究的基礎。

##### **Bidirectional Topic Matching: Quantifying Thematic Overlap Between Corpora Through Topic Modelling**
2412.18376v1 by Raven Adam, Marie Lisa Kogler

This study introduces Bidirectional Topic Matching (BTM), a novel method for
cross-corpus topic modeling that quantifies thematic overlap and divergence
between corpora. BTM is a flexible framework that can incorporate various topic
modeling approaches, including BERTopic, Top2Vec, and Latent Dirichlet
Allocation (LDA). BTM employs a dual-model approach, training separate topic
models for each corpus and applying them reciprocally to enable comprehensive
cross-corpus comparisons. This methodology facilitates the identification of
shared themes and unique topics, providing nuanced insights into thematic
relationships. Validation against cosine similarity-based methods demonstrates
the robustness of BTM, with strong agreement metrics and distinct advantages in
handling outlier topics. A case study on climate news articles showcases BTM's
utility, revealing significant thematic overlaps and distinctions between
corpora focused on climate change and climate action. BTM's flexibility and
precision make it a valuable tool for diverse applications, from political
discourse analysis to interdisciplinary studies. By integrating shared and
unique topic analyses, BTM offers a comprehensive framework for exploring
thematic relationships, with potential extensions to multilingual and dynamic
datasets. This work highlights BTM's methodological contributions and its
capacity to advance discourse analysis across various domains.

摘要：本研究引入了雙向主題匹配 (BTM)，這是一種跨語料庫主題建模的新方法，用於量化語料庫之間的主題重疊和差異。BTM 是一個靈活的框架，可以整合各種主題建模方法，包括 BERTopic、Top2Vec 和潛在狄利克雷分配 (LDA)。BTM 採用雙模型方法，針對每個語料庫訓練單獨的主題模型，並將它們互惠地應用於全面跨語料庫比較。這種方法有助於識別共用主題和獨特主題，從而提供對主題關係的細微見解。針對基於餘弦相似性的方法進行驗證，證明了 BTM 的穩健性，具有很強的一致性指標，並且在處理異常值主題方面具有顯著優勢。關於氣候新聞文章的案例研究展示了 BTM 的效用，揭示了專注於氣候變化和氣候行動的語料庫之間顯著的主題重疊和區別。BTM 的靈活性與精確性使其成為各種應用程序的寶貴工具，從政治話語分析到跨學科研究。通過整合共用和獨特的主題分析，BTM 為探索主題關係提供了一個全面的框架，並可能擴展到多語言和動態數據集。這項工作突出了 BTM 的方法論貢獻及其在各個領域推進話語分析的能力。

##### **Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors**
2412.18370v1 by Jinhyeok Choi, Heehyeon Kim, Joyce Jiyoung Whang

Graph neural networks (GNNs) have emerged as an effective tool for fraud
detection, identifying fraudulent users, and uncovering malicious behaviors.
However, attacks against GNN-based fraud detectors and their risks have rarely
been studied, thereby leaving potential threats unaddressed. Recent findings
suggest that frauds are increasingly organized as gangs or groups. In this
work, we design attack scenarios where fraud gangs aim to make their fraud
nodes misclassified as benign by camouflaging their illicit activities in
collusion. Based on these scenarios, we study adversarial attacks against
GNN-based fraud detectors by simulating attacks of fraud gangs in three
real-world fraud cases: spam reviews, fake news, and medical insurance frauds.
We define these attacks as multi-target graph injection attacks and propose
MonTi, a transformer-based Multi-target one-Time graph injection attack model.
MonTi simultaneously generates attributes and edges of all attack nodes with a
transformer encoder, capturing interdependencies between attributes and edges
more effectively than most existing graph injection attack methods that
generate these elements sequentially. Additionally, MonTi adaptively allocates
the degree budget for each attack node to explore diverse injection structures
involving target, candidate, and attack nodes, unlike existing methods that fix
the degree budget across all attack nodes. Experiments show that MonTi
outperforms the state-of-the-art graph injection attack methods on five
real-world graphs.

摘要：圖形神經網路 (GNN) 已成為一種有效的欺詐偵測工具，用於識別欺詐使用者並揭露惡意行為。然而，針對基於 GNN 的欺詐偵測器及其風險的攻擊鮮少受到研究，因此潛在威脅未獲得解決。最近的研究結果顯示，欺詐行為正日益以幫派或團體的方式組織起來。在這項研究中，我們設計了攻擊場景，其中欺詐幫派旨在透過共謀偽裝其非法活動，讓他們的欺詐節點被誤分類為良性。基於這些場景，我們透過模擬三起真實世界的欺詐案件（垃圾評論、假新聞和醫療保險欺詐）中欺詐幫派的攻擊，研究針對基於 GNN 的欺詐偵測器的對抗性攻擊。我們將這些攻擊定義為多目標圖形注入攻擊，並提出 MonTi，一種基於 Transformer 的多目標一次性圖形注入攻擊模型。MonTi 同時利用 Transformer 編碼器生成所有攻擊節點的屬性和邊緣，比大多數現有的圖形注入攻擊方法更有效地捕捉屬性和邊緣之間的相互依賴性，這些方法會依序生成這些元素。此外，與固定所有攻擊節點的度數預算的現有方法不同，MonTi 會自適應地分配每個攻擊節點的度數預算，以探索涉及目標、候選和攻擊節點的多樣化注入結構。實驗顯示，MonTi 在五個真實世界的圖形上優於最先進的圖形注入攻擊方法。

##### **Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology Dataset**
2412.18367v1 by Jiarui Liu, Iman Ouzzani, Wenkai Li, Lechen Zhang, Tianyue Ou, Houda Bouamor, Zhijing Jin, Mona Diab

The field of machine translation has achieved significant advancements, yet
domain-specific terminology translation, particularly in AI, remains
challenging. We introduced GIST, a large-scale multilingual AI terminology
dataset containing 5K terms extracted from top AI conference papers spanning
2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,
and Russian using a hybrid framework that combines LLMs for extraction with
human expertise for translation. The dataset's quality was benchmarked against
existing resources, demonstrating superior translation accuracy through
crowdsourced evaluation. GIST was integrated into translation workflows using
post-translation refinement methods that required no retraining, where LLM
prompting consistently improved BLEU and COMET scores. A web demonstration on
the ACL Anthology platform highlights its practical application, showcasing
improved accessibility for non-English speakers. This work aims to address
critical gaps in AI terminology resources and fosters global inclusivity and
collaboration in AI research.

摘要：機器翻譯領域已取得重大進展，然而，特定領域術語翻譯，特別是在 AI 中，仍然具有挑戰性。我們引入了 GIST，一個大型多語言 AI 術語資料集，其中包含從 2000 年到 2023 年頂尖 AI 會議論文中提取的 5K 個術語。這些術語使用結合 LLM 進行提取和人類專業知識進行翻譯的混合框架翻譯成阿拉伯語、中文、法語、日語和俄語。該資料集的品質根據現有資源進行基準測試，通過眾包評估證明了卓越的翻譯準確性。GIST 被整合到翻譯工作流程中，使用不需要重新訓練的翻譯後改進方法，其中 LLM 提示持續改善 BLEU 和 COMET 分數。在 ACL Anthology 平台上的網路展示突出了其實際應用，展示了對非英語人士的改進的可及性。這項工作旨在解決 AI 術語資源中的關鍵差距，並促進 AI 研究中的全球包容性和協作。

##### **Extracting triples from dialogues for conversational social agents**
2412.18364v1 by Piek Vossen, Selene Báez Santamaría, Lenka Bajčetić, Thomas Belluci

Obtaining an explicit understanding of communication within a Hybrid
Intelligence collaboration is essential to create controllable and transparent
agents. In this paper, we describe a number of Natural Language Understanding
models that extract explicit symbolic triples from social conversation. Triple
extraction has mostly been developed and tested for Knowledge Base Completion
using Wikipedia text and data for training and testing. However, social
conversation is very different as a genre in which interlocutors exchange
information in sequences of utterances that involve statements, questions, and
answers. Phenomena such as co-reference, ellipsis, coordination, and implicit
and explicit negation or confirmation are more prominent in conversation than
in Wikipedia text. We therefore describe an attempt to fill this gap by
releasing data sets for training and testing triple extraction from social
conversation. We also created five triple extraction models and tested them in
our evaluation data. The highest precision is 51.14 for complete triples and
69.32 for triple elements when tested on single utterances. However, scores for
conversational triples that span multiple turns are much lower, showing that
extracting knowledge from true conversational data is much more challenging.

摘要：在混合智能協作中取得對溝通的明確理解對於建立可控且透明的代理非常重要。在本文中，我們描述了許多自然語言理解模型，這些模型從社交對話中提取明確的符號三元組。三元組提取主要針對知識庫完成進行開發和測試，使用維基百科文字和數據進行訓練和測試。然而，社交對話作為一種體裁非常不同，其中對話者在涉及陳述、問題和答案的言論序列中交換信息。在對話中，共指、省略、協調以及隱含和明確的否定或確認等現象比在維基百科文字中更為突出。因此，我們描述了一項嘗試，通過發布用於訓練和測試從社交對話中提取三元組的數據集來填補這一空白。我們還創建了五個三元組提取模型，並在我們的評估數據中對它們進行了測試。在單個語句上進行測試時，完整三元組的最高精度為 51.14，三元組元素的最高精度為 69.32。然而，跨多個回合的對話三元組的分數要低得多，這表明從真正的對話數據中提取知識更具挑戰性。

##### **Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor**
2412.18355v1 by Hao Yu, Xin Yang, Le Zhang, Hanlin Gu, Tianrui Li, Lixin Fan, Qiang Yang

Federated continual learning (FCL) allows each client to continually update
its knowledge from task streams, enhancing the applicability of federated
learning in real-world scenarios. However, FCL needs to address not only
spatial data heterogeneity between clients but also temporal data heterogeneity
between tasks. In this paper, empirical experiments demonstrate that such
input-level heterogeneity significantly affects the model's internal parameters
and outputs, leading to severe spatial-temporal catastrophic forgetting of
local and previous knowledge. To this end, we propose Federated Tail Anchor
(FedTA) to mix trainable Tail Anchor with the frozen output features to adjust
their position in the feature space, thereby overcoming parameter-forgetting
and output-forgetting. Moreover, three novel components are also included in
FedTA: Input Enhancement for improving the performance of pre-trained models on
downstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous
local knowledge on the server side; and Best Global Prototype Selection for
finding the best anchor point for each class in the feature space. Extensive
experiments demonstrate that FedTA not only outperforms existing FCL methods
but also effectively preserves the relative positions of features, remaining
unaffected by spatial and temporal changes.

摘要：联邦持续学习 (FCL) 允许每个客户端持续从任务流更新其知识，从而增强联邦学习在真实世界场景中的适用性。然而，FCL 不仅需要解决客户端之间的空间数据异质性，还需要解决任务之间的时序数据异质性。本文中的实证实验表明，这种输入级异质性会显著影响模型的内部参数和输出，导致局部和先前知识的严重时空灾难性遗忘。为此，我们提出了联邦尾锚 (FedTA)，将可训练尾锚与冻结的输出特征混合，以调整它们在特征空间中的位置，从而克服参数遗忘和输出遗忘。此外，FedTA 中还包含三个新颖组件：用于提高预训练模型在下游任务上的性能的输入增强；用于在服务器端融合异构局部知识的选择性输入知识融合；以及用于在特征空间中为每个类别找到最佳锚点的最佳全局原型选择。大量的实验表明，FedTA 不仅优于现有的 FCL 方法，而且有效地保留了特征的相对位置，不受空间和时间变化的影响。

##### **The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence**
2412.18354v1 by Viviane Clay, Niels Leadholm, Jeff Hawkins

Artificial intelligence has advanced rapidly in the last decade, driven
primarily by progress in the scale of deep-learning systems. Despite these
advances, the creation of intelligent systems that can operate effectively in
diverse, real-world environments remains a significant challenge. In this white
paper, we outline the Thousand Brains Project, an ongoing research effort to
develop an alternative, complementary form of AI, derived from the operating
principles of the neocortex. We present an early version of a thousand-brains
system, a sensorimotor agent that is uniquely suited to quickly learn a wide
range of tasks and eventually implement any capabilities the human neocortex
has. Core to its design is the use of a repeating computational unit, the
learning module, modeled on the cortical columns found in mammalian brains.
Each learning module operates as a semi-independent unit that can model entire
objects, represents information through spatially structured reference frames,
and both estimates and is able to effect movement in the world. Learning is a
quick, associative process, similar to Hebbian learning in the brain, and
leverages inductive biases around the spatial structure of the world to enable
rapid and continual learning. Multiple learning modules can interact with one
another both hierarchically and non-hierarchically via a "cortical messaging
protocol" (CMP), creating more abstract representations and supporting
multimodal integration. We outline the key principles motivating the design of
thousand-brains systems and provide details about the implementation of Monty,
our first instantiation of such a system. Code can be found at
https://github.com/thousandbrainsproject/tbp.monty, along with more detailed
documentation at https://thousandbrainsproject.readme.io/.

摘要：<paragraph>人工智慧在過去十年中進展神速，主要是因為深度學習系統規模的進展。儘管有這些進展，但在多樣化的現實環境中有效運作的智慧系統的建立仍是一項重大的挑戰。在白皮書中，我們概述了千腦計畫，一項正在進行的研究工作，旨在開發一種替代性的、互補的人工智慧形式，其源自新皮質的運作原理。我們展示了千腦系統的早期版本，一種感官運動代理，它特別適合快速學習各種任務，並最終實施人類新皮質所具有的任何能力。其設計的核心是使用重複的計算單元，即學習模組，其模型是哺乳動物大腦中發現的皮質柱。每個學習模組都作為一個半獨立的單元運作，它可以對整個物體建模，透過空間結構參考框架表示資訊，並估計和影響世界中的運動。學習是一個快速的聯想過程，類似於大腦中的赫布學習，並利用世界空間結構周圍的歸納偏差，以實現快速和持續的學習。多個學習模組可以透過「皮質訊息傳遞協定」(CMP) 以階層式和非階層式的方式相互作用，建立更抽象的表示，並支援多模態整合。我們概述了激勵千腦系統設計的主要原則，並提供了關於 Monty 的實作細節，這是我們此類系統的首次實例。程式碼可以在 https://github.com/thousandbrainsproject/tbp.monty 找到，以及更詳細的文件在 https://thousandbrainsproject.readme.io/。</paragraph>

##### **Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering**
2412.18351v1 by Zhongjian Hu, Peng Yang, Bing Li, Zhenqi Wang

Large Language Models (LLMs) have achieved impressive results in
knowledge-based Visual Question Answering (VQA). However existing methods still
have challenges: the inability to use external tools autonomously, and the
inability to work in teams. Humans tend to know whether they need to use
external tools when they encounter a new question, e.g., they tend to be able
to give a direct answer to a familiar question, whereas they tend to use tools
such as search engines when they encounter an unfamiliar question. In addition,
humans also tend to collaborate and discuss with others to get better answers.
Inspired by this, we propose the multi-agent voting framework. We design three
LLM-based agents that simulate different levels of staff in a team, and assign
the available tools according to the levels. Each agent provides the
corresponding answer, and finally all the answers provided by the agents are
voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our
approach outperforms other baselines by 2.2 and 1.0, respectively.

摘要：大型語言模型 (LLM) 在基於知識的視覺問答 (VQA) 中取得令人印象深刻的成果。然而，現有方法仍有挑戰：無法自主使用外部工具，以及無法團隊合作。人類傾向於知道在遇到新問題時是否需要使用外部工具，例如，他們傾向於能夠直接回答熟悉的問題，而當他們遇到不熟悉的問題時，他們傾向於使用搜索引擎等工具。此外，人類也傾向於與他人合作和討論以獲得更好的答案。受此啟發，我們提出了多主體投票框架。我們設計了三個基於 LLM 的主體，模擬團隊中不同層級的員工，並根據層級分配可用工具。每個主體提供相應的答案，最後所有主體提供的答案都經過投票以獲得最終答案。OK-VQA 和 A-OKVQA 上的實驗表明，我們的做法分別比其他基準高出 2.2 和 1.0。

##### **Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning**
2412.18322v1 by Safa Ben Atitallah, Chaima Ben Rabah, Maha Driss, Wadii Boulila, Anis Koubaa

Graph Mamba, a powerful graph embedding technique, has emerged as a
cornerstone in various domains, including bioinformatics, social networks, and
recommendation systems. This survey represents the first comprehensive study
devoted to Graph Mamba, to address the critical gaps in understanding its
applications, challenges, and future potential. We start by offering a detailed
explanation of the original Graph Mamba architecture, highlighting its key
components and underlying mechanisms. Subsequently, we explore the most recent
modifications and enhancements proposed to improve its performance and
applicability. To demonstrate the versatility of Graph Mamba, we examine its
applications across diverse domains. A comparative analysis of Graph Mamba and
its variants is conducted to shed light on their unique characteristics and
potential use cases. Furthermore, we identify potential areas where Graph Mamba
can be applied in the future, highlighting its potential to revolutionize data
analysis in these fields. Finally, we address the current limitations and open
research questions associated with Graph Mamba. By acknowledging these
challenges, we aim to stimulate further research and development in this
promising area. This survey serves as a valuable resource for both newcomers
and experienced researchers seeking to understand and leverage the power of
Graph Mamba.

摘要：圖形 Mamba 是一種強大的圖形嵌入技術，已成為各種領域的基石，包括生物資訊學、社交網路和推薦系統。這項調查代表了第一個針對圖形 Mamba 的全面性研究，以解決理解其應用、挑戰和未來潛力的關鍵差距。我們從提供圖形 Mamba 原始架構的詳細說明開始，重點說明其關鍵組成部分和基礎機制。隨後，我們探討了為改善其效能和適用性而提出的最新修改和增強功能。為了展示圖形 Mamba 的多功能性，我們檢視了它在不同領域的應用。對圖形 Mamba 及其變體進行比較分析，以闡明其獨特特徵和潛在用例。此外，我們找出圖形 Mamba 未來可以應用的潛在領域，強調其在這些領域革新資料分析的潛力。最後，我們探討與圖形 Mamba 相關的當前限制和開放式研究問題。透過承認這些挑戰，我們旨在激勵這個有前途的領域進一步研究和開發。這項調查對於希望了解和利用圖形 Mamba 威力的新手和經驗豐富的研究人員來說，是一個有價值的資源。

##### **Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search**
2412.18319v1 by Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao

In this work, we aim to develop an MLLM that understands and solves questions
by learning to create each intermediate step of the reasoning involved till the
final answer. To this end, we propose Collective Monte Carlo Tree Search
(CoMCTS), a new learning-to-reason method for MLLMs, which introduces the
concept of collective learning into ``tree search'' for effective and efficient
reasoning-path searching and learning. The core idea of CoMCTS is to leverage
collective knowledge from multiple models to collaboratively conjecture, search
and identify effective reasoning paths toward correct answers via four
iterative operations including Expansion, Simulation and Error Positioning,
Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a
multimodal dataset with a tree of rich, explicit and well-defined reasoning
nodes for each question. With Mulberry-260k, we perform collective SFT to train
our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and
Reflection capabilities. Extensive experiments demonstrate the superiority of
our proposed methods on various benchmarks. Code will be available at
https://github.com/HJYao00/Mulberry

摘要：在這項工作中，我們旨在開發一個 MLLM，透過學習建立推理中每個中間步驟，直到最終答案，來理解並解決問題。為此，我們提出了集體蒙地卡羅樹狀搜尋 (CoMCTS)，這是 MLLM 的一種新的學習推理方法，將集體學習的概念引入「樹狀搜尋」，以進行有效且高效的推理路徑搜尋和學習。CoMCTS 的核心思想是利用來自多個模型的集體知識，透過四個反覆運算（包括擴充、模擬和錯誤定位、反向傳播和選擇）來協作推測、搜尋和找出通往正確答案的有效推理路徑。使用 CoMCTS，我們構建了 Mulberry-260k，這是一個多模態資料集，其中包含每個問題的豐富、明確且定義良好的推理節點樹。透過 Mulberry-260k，我們執行集體 SFT 來訓練我們的模型 Mulberry，這是一系列具有 o1 類型逐步推理和反思能力的 MLLM。廣泛的實驗證明了我們提出的方法在各種基準上的優越性。程式碼將在 https://github.com/HJYao00/Mulberry 提供

##### **M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models**
2412.18299v1 by Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Shimin Tao, Hengchao Shang, Zongyao Li, Shaojun Li, Jinlong Yang, Zhanglin Wu, Zhiqiang Rao, Hao Yang

With the widespread application of Large Language Models (LLMs) in the field
of Natural Language Processing (NLP), enhancing their performance has become a
research hotspot. This paper presents a novel multi-prompt ensemble decoding
approach designed to bolster the generation quality of LLMs by leveraging the
aggregation of outcomes from multiple prompts. Given a unique input $X$, we
submit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and
derive probability distributions. For each token prediction, we calculate the
ensemble probability by averaging the $n$ probability distributions within the
batch, utilizing this aggregated probability to generate the token. This
technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch
inference, we implement a Left-Padding strategy to maintain uniform input
lengths across the n prompts. Through extensive experimentation on diverse NLP
tasks, including machine translation, code generation, and text simplification,
we demonstrate the efficacy of our method in enhancing LLM performance. The
results show substantial improvements in BLEU scores, pass@$k$ rates, and LENS
metrics over conventional methods.

摘要：隨著大型語言模型 (LLM) 在自然語言處理 (NLP) 領域的廣泛應用，提升其效能已成為研究熱點。本文提出了一種新穎的多提示集合解碼方法，旨在透過利用多個提示結果的聚合來提升 LLM 的生成品質。給定一個獨特的輸入 $X$，我們會以批次模式將 $X$ 的 $n$ 個提示變形提交給 LLM 以進行解碼並推導機率分布。對於每個標記預測，我們會計算批次中 $n$ 個機率分布的平均值來計算集合機率，並利用這個聚合機率來生成標記。此技術稱為批次內集合。為了促進有效批次推論，我們實作了一個左邊補齊策略，以維護 $n$ 個提示間的輸入長度一致。透過對各種 NLP 任務（包括機器翻譯、程式碼生成和文字簡化）進行廣泛的實驗，我們證明了我們的方法在提升 LLM 效能方面的效力。結果顯示，與傳統方法相比，BLEU 分數、pass@$k$ 率和 LENS 指標均有顯著提升。

##### **Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight**
2412.18298v1 by Xi Ding, Lei Wang

Video anomaly detection (VAD) has witnessed significant advancements through
the integration of large language models (LLMs) and vision-language models
(VLMs), addressing critical challenges such as interpretability, temporal
reasoning, and generalization in dynamic, open-world scenarios. This paper
presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,
focusing on four key aspects: (i) enhancing interpretability through semantic
insights and textual explanations, making visual anomalies more understandable;
(ii) capturing intricate temporal relationships to detect and localize dynamic
anomalies across video frames; (iii) enabling few-shot and zero-shot detection
to minimize reliance on large, annotated datasets; and (iv) addressing
open-world and class-agnostic anomalies by using semantic understanding and
motion features for spatiotemporal coherence. We highlight their potential to
redefine the landscape of VAD. Additionally, we explore the synergy between
visual and textual modalities offered by LLMs and VLMs, highlighting their
combined strengths and proposing future directions to fully exploit the
potential in enhancing video anomaly detection.

摘要：影片異常偵測 (VAD) 透過整合大型語言模型 (LLM) 和視覺語言模型 (VLM)，在可解釋性、時序推理和動態開放世界場景中的概括等關鍵挑戰上取得顯著進展。本文深入探討 2024 年尖端的 LLM-/VLM- 基礎方法，重點關注四個面向：(i) 透過語意見解和文字說明增強可解釋性，讓視覺異常更易於理解；(ii) 捕捉複雜的時間關係，以偵測和定位影片格中的動態異常；(iii) 啟用少樣本和零樣本偵測，以減少對大型標註資料集的依賴；(iv) 透過語意理解和動作特徵解決開放世界和與類別無關的異常，以實現時空一致性。我們強調它們重新定義 VAD 領域的潛力。此外，我們探討 LLM 和 VLM 提供的視覺和文字模態之間的綜效，重點說明它們結合的優勢，並提出未來方向，以充分發揮增強影片異常偵測的潛力。

##### **Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies**
2412.18296v1 by Qi Liu, Wanjing Ma

Data corruption, including missing and noisy data, poses significant
challenges in real-world machine learning. This study investigates the effects
of data corruption on model performance and explores strategies to mitigate
these effects through two experimental setups: supervised learning with NLP
tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization
(Signal-RL). We analyze the relationship between data corruption levels and
model performance, evaluate the effectiveness of data imputation methods, and
assess the utility of enlarging datasets to address data corruption.
  Our results show that model performance under data corruption follows a
diminishing return curve, modeled by the exponential function. Missing data,
while detrimental, is less harmful than noisy data, which causes severe
performance degradation and training instability, particularly in sequential
decision-making tasks like Signal-RL. Imputation strategies involve a
trade-off: they recover missing information but may introduce noise. Their
effectiveness depends on imputation accuracy and corruption ratio. We identify
distinct regions in the imputation advantage heatmap, including an "imputation
advantageous corner" and an "imputation disadvantageous edge" and classify
tasks as "noise-sensitive" or "noise-insensitive" based on their decision
boundaries.
  Furthermore, we find that increasing dataset size mitigates but cannot fully
overcome the effects of data corruption. The marginal utility of additional
data diminishes as corruption increases. An empirical rule emerges:
approximately 30% of the data is critical for determining performance, while
the remaining 70% has minimal impact.
  These findings provide actionable insights into data preprocessing,
imputation strategies, and data collection practices, guiding the development
of robust machine learning systems in noisy environments.

摘要：資料毀損，包括遺失和有雜訊的資料，對現實世界的機器學習構成重大挑戰。本研究探討資料毀損對模型效能的影響，並透過兩個實驗設定探討減輕這些影響的策略：有監督學習與 NLP 任務 (NLP-SL)，以及深度強化學習用於交通號誌最佳化 (Signal-RL)。我們分析資料毀損程度與模型效能之間的關係，評估資料填補方法的有效性，並評估擴充資料集的效用，以解決資料毀損問題。
我們的結果顯示，資料毀損下的模型效能遵循遞減報酬曲線，以指數函數建模。遺失資料雖然有害，但不如有雜訊的資料有害，後者會導致嚴重的效能下降和訓練不穩定，特別是在像 Signal-RL 的順序決策任務中。填補策略涉及權衡取捨：它們會復原遺失的資訊，但可能會引入雜訊。它們的有效性取決於填補準確度和毀損率。我們在填補優勢熱圖中找出不同的區域，包括「填補優勢角」和「填補劣勢邊緣」，並根據決策邊界將任務分類為「對雜訊敏感」或「對雜訊不敏感」。
此外，我們發現增加資料集大小可以減輕資料毀損的影響，但無法完全克服。隨著毀損增加，額外資料的邊際效用會遞減。出現一條經驗法則：大約 30% 的資料對於決定效能至關重要，而其餘 70% 的影響很小。
這些發現提供可行的見解，用於資料前處理、填補策略和資料收集實務，指導在有雜訊的環境中開發穩健的機器學習系統。

##### **Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases**
2412.18295v1 by Christian Di Maio, Cristian Cosci, Marco Maggini, Valentina Poggioni, Stefano Melacci

The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in
several real-world services triggers severe concerns about their security. A
RAG system improves the generative capabilities of a Large Language Models
(LLM) by a retrieval mechanism which operates on a private knowledge base,
whose unintended exposure could lead to severe consequences, including breaches
of private and sensitive information. This paper presents a black-box attack to
force a RAG system to leak its private knowledge base which, differently from
existing approaches, is adaptive and automatic. A relevance-based mechanism and
an attacker-side open-source LLM favor the generation of effective queries to
leak most of the (hidden) knowledge base. Extensive experimentation proves the
quality of the proposed algorithm in different RAG pipelines and domains,
comparing to very recent related approaches, which turn out to be either not
fully black-box, not adaptive, or not based on open-source models. The findings
from our study remark the urgent need for more robust privacy safeguards in the
design and deployment of RAG systems.

摘要：檢索增強生成 (RAG) 系統在多項實際服務中日益普及，引發了人們對其安全性嚴重的擔憂。RAG 系統透過運作於私人知識庫的檢索機制來提升大型語言模型 (LLM) 的生成能力，其意外曝光可能會導致嚴重後果，包括私人和敏感資訊的洩漏。本文提出了一種黑盒攻擊，以強制 RAG 系統洩漏其私人知識庫，這不同於現有方法，它是自適應且自動化的。基於相關性的機制和攻擊者端的開源 LLM 有利於產生有效的查詢，以洩漏大部分（隱藏的）知識庫。廣泛的實驗證明了所提出的演算法在不同 RAG 管線和領域中的品質，與最近相關的方法相比，後者結果證明不是完全黑盒的、非自適應的，或不是基於開源模型。我們的研究結果強調了在 RAG 系統的設計和部署中迫切需要更強大的隱私保障措施。

##### **MinsStudio: A Streamlined Package for Minecraft AI Agent Development**
2412.18293v1 by Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang

Minecraft has emerged as a valuable testbed for embodied intelligence and
sequential decision-making research, yet the development and validation of
novel agents remains hindered by significant engineering challenges. This paper
presents MineStudio, an open-source software package designed to streamline
embodied policy development in Minecraft. MineStudio represents the first
comprehensive integration of seven critical engineering components: simulator,
data, model, offline pretraining, online finetuning, inference, and benchmark,
thereby allowing users to concentrate their efforts on algorithm innovation. We
provide a user-friendly API design accompanied by comprehensive documentation
and tutorials. The complete codebase is publicly available at
https://github.com/CraftJarvis/MineStudio.

摘要：Minecraft 已成為具象智能和序貫決策研究的寶貴測試平台，但新代理的開發和驗證仍受到重大工程挑戰的阻礙。本文介紹 MineStudio，這是一個開源軟體套件，旨在簡化 Minecraft 中具象政策的開發。MineStudio 代表了七個關鍵工程組件的首次全面整合：模擬器、資料、模型、離線預訓練、線上微調、推論和基準，從而使用戶能夠將精力集中在演算法創新上。我們提供使用者友善的 API 設計，並附有全面的文件和教學課程。完整的程式碼庫可在 https://github.com/CraftJarvis/MineStudio 公開取得。

##### **DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation**
2412.18291v1 by Junyi Lu, Xiaojia Li, Zihan Hua, Lei Yu, Shiqi Cheng, Li Yang, Fengjun Zhang, Chun Zuo

Code review is a vital but demanding aspect of software development,
generating significant interest in automating review comments. Traditional
evaluation methods for these comments, primarily based on text similarity, face
two major challenges: inconsistent reliability of human-authored comments in
open-source projects and the weak correlation of text similarity with
objectives like enhancing code quality and detecting defects.
  This study empirically analyzes benchmark comments using a novel set of
criteria informed by prior research and developer interviews. We then similarly
revisit the evaluation of existing methodologies. Our evaluation framework,
DeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a
comprehensive reassessment of current techniques based on the criteria set.
Besides, we also introduce an innovative and efficient baseline, LLM-Reviewer,
leveraging the few-shot learning capabilities of LLMs for a target-oriented
comparison.
  Our research highlights the limitations of text similarity metrics, finding
that less than 10% of benchmark comments are high quality for automation. In
contrast, DeepCRCEval effectively distinguishes between high and low-quality
comments, proving to be a more reliable evaluation mechanism. Incorporating LLM
evaluators into DeepCRCEval significantly boosts efficiency, reducing time and
cost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates
significant potential of focusing task real targets in comment generation.

摘要：程式碼檢閱是軟體開發中至關重要但要求嚴格的一環，因此自動化檢閱評論引起了極大的興趣。這些評論的傳統評估方法主要基於文字相似度，面臨兩大挑戰：開源專案中人工撰寫評論的不一致可靠性，以及文字相似度與提升程式碼品質和偵測缺陷等目標之間的關聯性薄弱。本研究使用一組新穎的標準對基準評論進行實證分析，這些標準來自於先前的研究和開發人員訪談。然後，我們以類似的方式重新檢視現有方法論的評估。我們的評估架構 DeepCRCEval 整合了人工評估員和大型語言模型 (LLM)，以根據標準集對目前的技術進行全面重新評估。此外，我們還引入了一個創新且高效的基準，LLM-Reviewer，利用 LLM 的少次學習能力進行目標導向的比較。我們的研究突顯了文字相似度指標的限制，發現不到 10% 的基準評論對於自動化而言是高品質的。相比之下，DeepCRCEval 有效地區分了高品質和低品質評論，證明是一種更可靠的評估機制。將 LLM 評估員納入 DeepCRCEval 可顯著提升效率，分別將時間和成本降低 88.78% 和 90.32%。此外，LLM-Reviewer 證明了在評論產生中關注任務真實目標的巨大潛力。

##### **Towards understanding how attention mechanism works in deep learning**
2412.18288v1 by Tianyu Ruan, Shihua Zhang

Attention mechanism has been extensively integrated within mainstream neural
network architectures, such as Transformers and graph attention networks. Yet,
its underlying working principles remain somewhat elusive. What is its essence?
Are there any connections between it and traditional machine learning
algorithms? In this study, we inspect the process of computing similarity using
classic metrics and vector space properties in manifold learning, clustering,
and supervised learning. We identify the key characteristics of similarity
computation and information propagation in these methods and demonstrate that
the self-attention mechanism in deep learning adheres to the same principles
but operates more flexibly and adaptively. We decompose the self-attention
mechanism into a learnable pseudo-metric function and an information
propagation process based on similarity computation. We prove that the
self-attention mechanism converges to a drift-diffusion process through
continuous modeling provided the pseudo-metric is a transformation of a metric
and certain reasonable assumptions hold. This equation could be transformed
into a heat equation under a new metric. In addition, we give a first-order
analysis of attention mechanism with a general pseudo-metric function. This
study aids in understanding the effects and principle of attention mechanism
through physical intuition. Finally, we propose a modified attention mechanism
called metric-attention by leveraging the concept of metric learning to
facilitate the ability to learn desired metrics more effectively. Experimental
results demonstrate that it outperforms self-attention regarding training
efficiency, accuracy, and robustness.

摘要：注意力机制已被广泛集成到主流神经网络架构中，例如 Transformer 和图注意力网络。然而，其底层工作原理仍然有些难以捉摸。它的本质是什么？它与传统机器学习算法之间有什么联系？在这项研究中，我们考察了使用经典度量和流形学习、聚类和监督学习中的向量空间属性计算相似性的过程。我们识别了这些方法中相似性计算和信息传播的关键特征，并证明了深度学习中的自注意力机制遵循相同的原则，但操作更灵活和自适应。我们将自注意力机制分解为可学习的伪度量函数和基于相似性计算的信息传播过程。我们证明了自注意力机制通过连续建模收敛到漂移扩散过程，前提是伪度量是度量的变换，并且某些合理的假设成立。该方程可以在新度量下转换为热方程。此外，我们对具有通用伪度量函数的注意力机制进行了**一阶分析**。本研究有助于通过物理直觉理解注意力机制的影响和原理。最后，我们提出了一种称为度量注意力的修改注意力机制，通过利用度量学习的概念来促进更有效地学习所需度量的能力。实验结果表明，它在训练效率、准确性和鲁棒性方面优于自注意力。

##### **Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation**
2412.18287v1 by Sheng Xiang, Mingzhi Zhu, Dawei Cheng, Enxia Li, Ruihui Zhao, Yi Ouyang, Ling Chen, Yefeng Zheng

Credit card fraud incurs a considerable cost for both cardholders and issuing
banks. Contemporary methods apply machine learning-based classifiers to detect
fraudulent behavior from labeled transaction records. But labeled data are
usually a small proportion of billions of real transactions due to expensive
labeling costs, which implies that they do not well exploit many natural
features from unlabeled data. Therefore, we propose a semi-supervised graph
neural network for fraud detection. Specifically, we leverage transaction
records to construct a temporal transaction graph, which is composed of
temporal transactions (nodes) and interactions (edges) among them. Then we pass
messages among the nodes through a Gated Temporal Attention Network (GTAN) to
learn the transaction representation. We further model the fraud patterns
through risk propagation among transactions. The extensive experiments are
conducted on a real-world transaction dataset and two publicly available fraud
detection datasets. The result shows that our proposed method, namely GTAN,
outperforms other state-of-the-art baselines on three fraud detection datasets.
Semi-supervised experiments demonstrate the excellent fraud detection
performance of our model with only a tiny proportion of labeled data.

摘要：信用卡诈骗对持卡人和发卡银行来说都是一笔不小的开支。当代方法应用基于机器学习的分类器来从标记的交易记录中检测欺诈行为。但由于标记成本高昂，标记数据通常只占数十亿笔真实交易的一小部分，这意味着它们并没有很好地利用未标记数据中的许多自然特征。因此，我们提出了一种用于欺诈检测的半监督图神经网络。具体来说，我们利用交易记录构建了一个时间交易图，该图由时间交易（节点）和它们之间的交互（边）组成。然后，我们通过门控时间注意力网络 (GTAN) 在节点之间传递消息，以学习交易表示。我们进一步通过交易之间的风险传播对欺诈模式进行建模。广泛的实验是在一个真实世界的交易数据集和两个公开可用的欺诈检测数据集上进行的。结果表明，我们提出的方法，即 GTAN，在三个欺诈检测数据集上优于其他最先进的基线。半监督实验表明，我们的模型仅使用一小部分标记数据就表现出出色的欺诈检测性能。

##### **Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization**
2412.18279v1 by Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou

The role of reinforcement learning (RL) in enhancing the reasoning of large
language models (LLMs) is becoming increasingly significant. Despite the
success of RL in many scenarios, there are still many challenges in improving
the reasoning of LLMs. One challenge is the sparse reward, which makes
optimization difficult for RL and necessitates a large amount of data samples.
Another challenge stems from the inherent instability of RL, particularly when
using Actor-Critic (AC) methods to derive optimal policies, which often leads
to unstable training processes. To address these issues, we introduce Direct
Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.
Unlike standard alignment that rely solely outcome rewards to optimize policies
(such as DPO), DAPO employs a critic function to predict the reasoning accuracy
at each step, thereby generating dense signals to refine the generation
strategy. Additionally, the Actor and Critic components in DAPO are trained
independently, avoiding the co-training instability observed in standard AC
algorithms like PPO. We train DAPO on mathematical and code query datasets and
then evaluate its performance on multiple benchmarks. Our results show that
DAPO can effectively enhance the mathematical and code capabilities on both SFT
models and RL models, demonstrating the effectiveness of DAPO.

摘要：強化學習 (RL) 在提升大型語言模型 (LLM) 推論中的角色正變得越來越重要。儘管 RL 在許多場景中都獲得成功，但在改善 LLM 推論方面仍有許多挑戰。其中一項挑戰是稀疏獎勵，這使得 RL 的最佳化變得困難，並且需要大量的資料樣本。另一項挑戰源於 RL 的內在不穩定性，特別是在使用 Actor-Critic (AC) 方法來推導最佳策略時，這通常會導致不穩定的訓練過程。為了解決這些問題，我們引入了直接優勢策略最佳化 (DAPO)，一種新穎的步驟級離線 RL 演算法。與僅依賴結果獎勵來最佳化策略的標準對齊不同（例如 DPO），DAPO 使用一個評論函數來預測每個步驟的推論準確度，從而產生密集的訊號來優化生成策略。此外，DAPO 中的 Actor 和 Critic 組件是獨立訓練的，避免了在標準 AC 演算法中觀察到的共同訓練不穩定性，例如 PPO。我們在數學和程式碼查詢資料集上訓練 DAPO，然後評估其在多個基準測試上的效能。我們的結果表明，DAPO 可以有效提升 SFT 模型和 RL 模型上的數學和程式碼能力，證明了 DAPO 的有效性。

##### **GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge**
2412.18274v1 by Shammur Absar Chowdhury, Hind Almerekhi, Mucahid Kutlu, Kaan Efe Keles, Fatema Ahmad, Tasnim Mohiuddin, George Mikros, Firoj Alam

This paper presents a comprehensive overview of the first edition of the
Academic Essay Authenticity Challenge, organized as part of the GenAI Content
Detection shared tasks collocated with COLING 2025. This challenge focuses on
detecting machine-generated vs. human-authored essays for academic purposes.
The task is defined as follows: "Given an essay, identify whether it is
generated by a machine or authored by a human.'' The challenge involves two
languages: English and Arabic. During the evaluation phase, 25 teams submitted
systems for English and 21 teams for Arabic, reflecting substantial interest in
the task. Finally, seven teams submitted system description papers. The
majority of submissions utilized fine-tuned transformer-based models, with one
team employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This
paper outlines the task formulation, details the dataset construction process,
and explains the evaluation framework. Additionally, we present a summary of
the approaches adopted by participating teams. Nearly all submitted systems
outperformed the n-gram-based baseline, with the top-performing systems
achieving F1 scores exceeding 0.98 for both languages, indicating significant
progress in the detection of machine-generated text.

摘要：這篇論文全面概述了 GenAI 內容偵測共享任務的一部分，作為 COLING 2025 協辦的學術論文真實性挑戰賽的第一版。這個挑戰專注於偵測機器產生的文章與人類撰寫的學術文章。任務定義如下：「給定一篇論文，找出它是機器產生的還是人類撰寫的。」這個挑戰涉及兩種語言：英語和阿拉伯語。在評估階段，25 個團隊提交了英語系統，21 個團隊提交了阿拉伯語系統，反映出對這個任務的濃厚興趣。最後，七個團隊提交了系統說明文件。大多數提交的文件都利用了微調的Transformer模型，其中一個團隊採用了大型語言模型 (LLM)，例如 Llama 2 和 Llama 3。這篇論文概述了任務的制定，詳細說明了資料集的建構過程，並解釋了評估架構。此外，我們還簡要說明了參賽團隊採用的方法。幾乎所有提交的系統都優於基於 n-gram 的基準，表現最佳的系統在兩種語言中都達到了超過 0.98 的 F1 分數，這表示在機器產生的文字偵測方面取得了顯著進展。

##### **Sampling Bag of Views for Open-Vocabulary Object Detection**
2412.18273v1 by Hojun Choi, Junsuk Choe, Hyunjung Shim

Existing open-vocabulary object detection (OVD) develops methods for testing
unseen categories by aligning object region embeddings with corresponding VLM
features. A recent study leverages the idea that VLMs implicitly learn
compositional structures of semantic concepts within the image. Instead of
using an individual region embedding, it utilizes a bag of region embeddings as
a new representation to incorporate compositional structures into the OVD task.
However, this approach often fails to capture the contextual concepts of each
region, leading to noisy compositional structures. This results in only
marginal performance improvements and reduced efficiency. To address this, we
propose a novel concept-based alignment method that samples a more powerful and
efficient compositional structure. Our approach groups contextually related
``concepts'' into a bag and adjusts the scale of concepts within the bag for
more effective embedding alignment. Combined with Faster R-CNN, our method
achieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel
categories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our
method reduces CLIP computation in FLOPs by 80.3% compared to previous
research, significantly enhancing efficiency. Experimental results demonstrate
that the proposed method outperforms previous state-of-the-art models on the
OVD datasets.

摘要：現有的開放式詞彙目標偵測 (OVD) 發展出透過將目標區域嵌入與對應的 VLM 特徵對齊，來測試未見類別的方法。最近的研究利用 VLM 隱含地學習影像中語意概念的組合結構這個想法。它使用區域嵌入的袋子作為新的表示，來將組合結構納入 OVD 任務，而不是使用個別區域嵌入。然而，這種方法通常無法捕捉每個區域的脈絡概念，導致組合結構有雜訊。這只會帶來邊際效能改善和降低效率。為了解決這個問題，我們提出一個新穎的基於概念的對齊方法，來取樣更強大且有效的組合結構。我們的做法將脈絡相關的「概念」分組成一個袋子，並調整袋子中概念的規模，以進行更有效的嵌入對齊。我們的做法結合了 Faster R-CNN，在開放式詞彙 COCO 和 LVIS 基準中，針對新類別取得了 2.6 個框 AP50 和 0.5 個遮罩 AP 的改進。此外，與先前的研究相比，我們的做法將 FLOP 中的 CLIP 計算減少了 80.3%，大幅提升了效率。實驗結果證明，所提出的方法在 OVD 資料集上優於先前的最先進模型。

##### **Annotating References to Mythological Entities in French Literature**
2412.18270v1 by Thierry Poibeau

In this paper, we explore the relevance of large language models (LLMs) for
annotating references to Roman and Greek mythological entities in modern and
contemporary French literature. We present an annotation scheme and demonstrate
that recent LLMs can be directly applied to follow this scheme effectively,
although not without occasionally making significant analytical errors.
Additionally, we show that LLMs (and, more specifically, ChatGPT) are capable
of offering interpretative insights into the use of mythological references by
literary authors. However, we also find that LLMs struggle to accurately
identify relevant passages in novels (when used as an information retrieval
engine), often hallucinating and generating fabricated examples-an issue that
raises significant ethical concerns. Nonetheless, when used carefully, LLMs
remain valuable tools for performing annotations with high accuracy, especially
for tasks that would be difficult to annotate comprehensively on a large scale
through manual methods alone.

摘要：在本文中，我們探討大型語言模型 (LLM) 對現代和當代法語文學中羅馬和希臘神話實體引用的註解相關性。我們提出一個註解方案，並展示最近的 LLM 可以直接應用於有效遵循此方案，儘管偶爾會出現重大的分析錯誤。此外，我們展示了 LLM（更具體地說，是 ChatGPT）能夠提供對文學作者使用神話引用的詮釋見解。然而，我們也發現 LLM 難以準確識別小說中的相關段落（當用作資訊檢索引擎時），經常出現幻覺並產生虛構的範例，這是一個引發重大道德問題的問題。儘管如此，在小心使用時，LLM 仍然是執行高準確度註解的寶貴工具，特別是對於僅透過手動方法難以全面註解的大規模任務。

##### **Investigating Large Language Models for Code Vulnerability Detection: An Experimental Study**
2412.18260v1 by Xuefeng Jiang, Lvhua Wu, Sheng Sun, Jia Li, Jingjing Xue, Yuwei Wang, Tingting Wu, Min Liu

Code vulnerability detection (CVD) is essential for addressing and preventing
system security issues, playing a crucial role in ensuring software security.
Previous learning-based vulnerability detection methods rely on either
fine-tuning medium-size sequence models or training smaller neural networks
from scratch. Recent advancements in large pre-trained language models (LLMs)
have showcased remarkable capabilities in various code intelligence tasks
including code understanding and generation. However, the effectiveness of LLMs
in detecting code vulnerabilities is largely under-explored. This work aims to
investigate the gap by fine-tuning LLMs for the CVD task, involving four
widely-used open-source LLMs. We also implement other five previous graph-based
or medium-size sequence models for comparison. Experiments are conducted on
five commonly-used CVD datasets, including both the part of short samples and
long samples. In addition, we conduct quantitative experiments to investigate
the class imbalance issue and the model's performance on samples of different
lengths, which are rarely studied in previous works. To better facilitate
communities, we open-source all codes and resources of this study in
https://github.com/SakiRinn/LLM4CVD and
https://huggingface.co/datasets/xuefen/VulResource.

摘要：程式碼漏洞偵測 (CVD) 對於解決和預防系統安全問題至關重要，在確保軟體安全中扮演著關鍵角色。先前的基於學習的漏洞偵測方法仰賴微調中等大小的序列模型或從頭訓練較小的神經網路。大型預訓練語言模型 (LLM) 的最新進展在各種程式碼智慧任務中展現了卓越的能力，包括程式碼理解和產生。然而，LLM 在偵測程式碼漏洞方面的有效性在很大程度上尚未被探索。本研究旨在透過微調 LLM 以執行 CVD 任務來探討這個差距，其中涉及四個廣泛使用的開源 LLM。我們也實作了其他五個先前的基於圖形的模型或中等大小的序列模型以供比較。實驗在五個常用的 CVD 資料集上進行，包括短範例和長範例的部分。此外，我們進行了量化實驗以探討類別不平衡問題和模型在不同長度範例上的效能，這些在先前的研究中很少被探討。為了更好地促進社群，我們在 https://github.com/SakiRinn/LLM4CVD 和 https://huggingface.co/datasets/xuefen/VulResource 開源了本研究的所有程式碼和資源。

##### **Fréchet regression for multi-label feature selection with implicit regularization**
2412.18247v1 by Dou El Kefel Mansouri, Seif-Eddine Benkabou, Khalid Benabdeslem

Fr\'echet regression extends linear regression to model complex responses
  in metric spaces, making it particularly relevant for multi-label regression,
  where each instance can have multiple associated labels. However, variable
  selection within this framework remains underexplored. In this paper, we pro
pose a novel variable selection method that employs implicit regularization
  instead of traditional explicit regularization approaches, which can
introduce
  bias. Our method effectively captures nonlinear interactions between predic
tors and responses while promoting model sparsity. We provide theoretical
  results demonstrating selection consistency and illustrate the performance of
  our approach through numerical examples

摘要：Fréchet 回歸將線性回歸延伸到模型複雜回應
在度量空間中，使其特別適用於多標籤回歸，
其中每個實例可以有多個關聯標籤。但是，變量
在這個框架內的選擇仍然未被充分探討。在本文中，我們提出
一種新穎的變量選擇方法，它採用隱式正則化
而不是傳統的顯式正則化方法，這可能會
引入
偏差。我們的模型有效地捕捉了預測器和響應之間的非線性交互，同時促進了模型的稀疏性。我們提供理論
結果證明了選擇的一致性，並通過數值示例說明了我們的方法的性能

##### **An Automatic Graph Construction Framework based on Large Language Models for Recommendation**
2412.18241v1 by Rong Shan, Jianghao Lin, Chenxu Zhu, Bo Chen, Menghui Zhu, Kangning Zhang, Jieming Zhu, Ruiming Tang, Yong Yu, Weinan Zhang

Graph neural networks (GNNs) have emerged as state-of-the-art methods to
learn from graph-structured data for recommendation. However, most existing
GNN-based recommendation methods focus on the optimization of model structures
and learning strategies based on pre-defined graphs, neglecting the importance
of the graph construction stage. Earlier works for graph construction usually
rely on speciffic rules or crowdsourcing, which are either too simplistic or
too labor-intensive. Recent works start to utilize large language models (LLMs)
to automate the graph construction, in view of their abundant open-world
knowledge and remarkable reasoning capabilities. Nevertheless, they generally
suffer from two limitations: (1) invisibility of global view (e.g., overlooking
contextual information) and (2) construction inefficiency. To this end, we
introduce AutoGraph, an automatic graph construction framework based on LLMs
for recommendation. Specifically, we first use LLMs to infer the user
preference and item knowledge, which is encoded as semantic vectors. Next, we
employ vector quantization to extract the latent factors from the semantic
vectors. The latent factors are then incorporated as extra nodes to link the
user/item nodes, resulting in a graph with in-depth global-view semantics. We
further design metapath-based message aggregation to effectively aggregate the
semantic and collaborative information. The framework is model-agnostic and
compatible with different backbone models. Extensive experiments on three
real-world datasets demonstrate the efficacy and efffciency of AutoGraph
compared to existing baseline methods. We have deployed AutoGraph in Huawei
advertising platform, and gain a 2.69% improvement on RPM and a 7.31%
improvement on eCPM in the online A/B test. Currently AutoGraph has been used
as the main trafffc model, serving hundreds of millions of people.

摘要：圖神經網路 (GNN) 已成為最先進的方法，可從圖形結構化資料中學習推薦。然而，現有的基於 GNN 的推薦方法大多側重於預定義圖形上的模型結構和學習策略的最佳化，忽略了圖形建構階段的重要性。早期圖形建構工作通常依賴於特定規則或群眾外包，這些方法過於簡化或過於勞動密集。最近的工作開始利用大型語言模型 (LLM) 來自動化圖形建構，因為它們具有豐富的開放世界知識和卓越的推理能力。儘管如此，它們通常存在兩個限制：(1) 全域檢視的不可見性（例如，忽略上下文資訊）和 (2) 建構效率低下。為此，我們引入了 AutoGraph，一個基於 LLM 的自動圖形建構框架，用於推薦。具體來說，我們首先使用 LLM 推斷使用者偏好和項目知識，並將其編碼為語義向量。接下來，我們採用向量量化從語義向量中提取潛在因子。然後將潛在因子作為額外節點加入，以連結使用者/項目節點，從而形成一個具有深入全域檢視語義的圖形。我們進一步設計了基於元路徑的訊息聚合，以有效聚合語義和協作資訊。該框架與模型無關，並與不同的主幹模型相容。在三個真實世界資料集上進行的廣泛實驗證明了 AutoGraph 與現有基準方法相比的效能和效率。我們已在華為廣告平台上部署了 AutoGraph，並在線上 A/B 測試中獲得了 RPM 提升 2.69% 和 eCPM 提升 7.31%。目前 AutoGraph 已被用作主要的流量模型，服務於數億人。

##### **Expand VSR Benchmark for VLLM to Expertize in Spatial Rules**
2412.18224v1 by Peijin Xie, Lin Sun, Bingquan Liu, Dexin Wang, Xiangzheng Zhang, Chengjie Sun, Jiajia Zhang

Distinguishing spatial relations is a basic part of human cognition which
requires fine-grained perception on cross-instance. Although benchmarks like
MME, MMBench and SEED comprehensively have evaluated various capabilities which
already include visual spatial reasoning(VSR). There is still a lack of
sufficient quantity and quality evaluation and optimization datasets for Vision
Large Language Models(VLLMs) specifically targeting visual positional
reasoning. To handle this, we first diagnosed current VLLMs with the VSR
dataset and proposed a unified test set. We found current VLLMs to exhibit a
contradiction of over-sensitivity to language instructions and
under-sensitivity to visual positional information. By expanding the original
benchmark from two aspects of tunning data and model structure, we mitigated
this phenomenon. To our knowledge, we expanded spatially positioned image data
controllably using diffusion models for the first time and integrated original
visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and
DINO). After conducting combination experiments on scaling data and models, we
obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different
instructions but also accurately distinguishes differences in visual positional
information. VSRE achieved over a 27\% increase in accuracy on the VSR test
set. It becomes a performant VLLM on the position reasoning of both the VSR
dataset and relevant subsets of other evaluation benchmarks. We open-sourced
the expanded model with data and Appendix at
\url{https://github.com/peijin360/vsre} and hope it will accelerate
advancements in VLLM on VSR learning.

摘要：區分空間關係是人類認知的基本部分，這需要跨例子的精細感知。儘管像 MME、MMBench 和 SEED 這樣的基準全面評估了各種能力，其中已包含視覺空間推理 (VSR)。但仍然缺乏針對視覺位置推理的、數量和質量足夠的評估和優化資料集，特別是針對視覺大型語言模型 (VLLM)。為了處理這個問題，我們首先使用 VSR 資料集診斷了目前的 VLLM，並提出了一個統一的測試集。我們發現目前的 VLLM 表現出對語言指令過度敏感和對視覺位置資訊過度不敏感的矛盾現象。透過從調整資料和模型結構兩個方面擴展原始基準，我們減輕了這種現象。據我們所知，我們首次使用擴散模型可控地擴展了空間定位的影像資料，並將原始視覺編碼 (CLIP) 與其他 3 個強大的視覺編碼器 (SigLIP、SAM 和 DINO) 整合在一起。在對資料和模型的擴充進行組合實驗後，我們獲得了一個 VLLM VSR 專家 (VSRE)，它不僅能對不同的指令進行更好的概括，還能準確區分視覺位置資訊的差異。VSRE 在 VSR 測試集中準確率提高了 27% 以上。它成為了 VSR 資料集和其它評估基準相關子集的定位推理中效能良好的 VLLM。我們在 \url{https://github.com/peijin360/vsre} 開源了擴展模型、資料和附錄，並希望它能加速 VLLM 在 VSR 學習方面的進展。

##### **ICM-Assistant: Instruction-tuning Multimodal Large Language Models for Rule-based Explainable Image Content Moderation**
2412.18216v1 by Mengyang Wu, Yuzhi Zhao, Jialun Cao, Mingjie Xu, Zhongming Jiang, Xuehui Wang, Qinbin Li, Guangneng Hu, Shengchao Qin, Chi-Wing Fu

Controversial contents largely inundate the Internet, infringing various
cultural norms and child protection standards. Traditional Image Content
Moderation (ICM) models fall short in producing precise moderation decisions
for diverse standards, while recent multimodal large language models (MLLMs),
when adopted to general rule-based ICM, often produce classification and
explanation results that are inconsistent with human moderators. Aiming at
flexible, explainable, and accurate ICM, we design a novel rule-based dataset
generation pipeline, decomposing concise human-defined rules and leveraging
well-designed multi-stage prompts to enrich short explicit image annotations.
Our ICM-Instruct dataset includes detailed moderation explanation and
moderation Q-A pairs. Built upon it, we create our ICM-Assistant model in the
framework of rule-based ICM, making it readily applicable in real practice. Our
ICM-Assistant model demonstrates exceptional performance and flexibility.
Specifically, it significantly outperforms existing approaches on various
sources, improving both the moderation classification (36.8\% on average) and
moderation explanation quality (26.6\% on average) consistently over existing
MLLMs. Code/Data is available at https://github.com/zhaoyuzhi/ICM-Assistant.

摘要：有爭議的內容大量充斥在網路上，侵犯各種文化規範和兒童保護標準。傳統的影像內容審核 (ICM) 模型無法針對不同的標準提出精確的審核決定，而最近的多模態大型語言模型 (MLLM) 在採用一般基於規則的 ICM 時，通常會產生與人工審核員不一致的分類和說明結果。為了實現彈性、可解釋和精確的 ICM，我們設計了一個新穎的基於規則的資料集生成管道，分解簡潔的人類定義規則，並利用設計良好的多階段提示來豐富簡短的明確影像註解。我們的 ICM-Instruct 資料集包含詳細的審核說明和審核問答對。建立在它的基礎上，我們在基於規則的 ICM 的架構中建立了我們的 ICM-Assistant 模型，使其易於應用於實際操作中。我們的 ICM-Assistant 模型展示出卓越的效能和彈性。具體來說，它在各種來源上都顯著優於現有方法，持續改善審核分類（平均 36.8%）和審核說明品質（平均 26.6%），優於現有的 MLLM。程式碼/資料可在 https://github.com/zhaoyuzhi/ICM-Assistant 取得。

##### **Robustness-aware Automatic Prompt Optimization**
2412.18196v1 by Zeru Shi, Zhenting Wang, Yongye Su, Weidi Luo, Fan Yang, Yongfeng Zhang

The performance of Large Language Models (LLMs) is based on the quality of
the prompts and the semantic and structural integrity information of the input
data. However, current prompt generation methods primarily focus on generating
prompts for clean input data, often overlooking the impact of perturbed inputs
on prompt performance. To address this limitation, we propose BATprompt (By
Adversarial Training prompt), a novel method for prompt generation designed to
withstand input perturbations (such as typos in the input). Inspired by
adversarial training techniques, BATprompt demonstrates strong performance on a
variety of perturbed tasks through a two-step process: adversarial perturbation
and iterative optimization on unperturbed input via LLM. Unlike conventional
adversarial attack methods, BATprompt avoids reliance on real gradients or
model parameters. Instead, it leverages the advanced reasoning, language
understanding and self reflection capabilities of LLMs to simulate gradients,
guiding the generation of adversarial perturbations and optimizing prompt
performance. In our experiments, we evaluate BATprompt on multiple datasets
across both language understanding and generation tasks. The results indicate
that BATprompt outperforms existing prompt generation methods, delivering
superior robustness and performance under diverse perturbation scenarios.

摘要：大型語言模型 (LLM) 的效能取決於提示的品質，以及輸入資料的語意和結構完整性資訊。然而，目前的提示產生方法主要專注於為乾淨的輸入資料產生提示，常常忽略擾動輸入對提示效能的影響。為了解決這個限制，我們提出 BATprompt（對抗訓練提示），這是一種新穎的提示產生方法，旨在承受輸入擾動（例如輸入中的錯字）。受到對抗訓練技術的啟發，BATprompt 透過一個兩步驟的過程在各種擾動任務上展現強大的效能：對抗擾動和透過 LLM 對未擾動輸入進行反覆最佳化。與傳統的對抗攻擊方法不同，BATprompt 避免依賴真實梯度或模型參數。相反地，它利用 LLM 的進階推理、語言理解和自我反省能力來模擬梯度，引導對抗擾動的產生並最佳化提示效能。在我們的實驗中，我們在跨語言理解和產生任務的多個資料集上評估 BATprompt。結果表明，BATprompt 優於現有的提示產生方法，在不同的擾動場景下提供卓越的穩健性和效能。

##### **VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks**
2412.18194v1 by Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu

General-purposed embodied agents are designed to understand the users'
natural instructions or intentions and act precisely to complete universal
tasks. Recently, methods based on foundation models especially
Vision-Language-Action models (VLAs) have shown a substantial potential to
solve language-conditioned manipulation (LCM) tasks well. However, existing
benchmarks do not adequately meet the needs of VLAs and relative algorithms. To
better define such general-purpose tasks in the context of LLMs and advance the
research in VLAs, we present VLABench, an open-source benchmark for evaluating
universal LCM task learning. VLABench provides 100 carefully designed
categories of tasks, with strong randomization in each category of task and a
total of 2000+ objects. VLABench stands out from previous benchmarks in four
key aspects: 1) tasks requiring world knowledge and common sense transfer, 2)
natural language instructions with implicit human intentions rather than
templates, 3) long-horizon tasks demanding multi-step reasoning, and 4)
evaluation of both action policies and language model capabilities. The
benchmark assesses multiple competencies including understanding of
mesh\&texture, spatial relationship, semantic instruction, physical laws,
knowledge transfer and reasoning, etc. To support the downstream finetuning, we
provide high-quality training data collected via an automated framework
incorporating heuristic skills and prior information. The experimental results
indicate that both the current state-of-the-art pretrained VLAs and the
workflow based on VLMs face challenges in our tasks.

摘要：<paragraph>通用具身代理旨在了解使用者的自然指令或意圖，並準確執行以完成通用任務。最近，基於基礎模型的方法，尤其是視覺語言動作模型 (VLA)，已展現出解決語言條件操作 (LCM) 任務的巨大潛力。然而，現有的基準並未充分滿足 VLA 和相關演算法的需求。為了在 LLM 的背景下更好地定義此類通用任務並推動 VLA 的研究，我們提出了 VLABench，一個用於評估通用 LCM 任務學習的開源基準。VLABench 提供 100 個精心設計的任務類別，每個任務類別都有強大的隨機性，總共有 2000 多個物件。VLABench 在四個關鍵方面優於以前的基準：1) 需要世界知識和常識轉移的任務，2) 具有隱含人類意圖的自然語言指令，而不是範本，3) 要求多步驟推理的長時程任務，以及 4) 動作策略和語言模型能力的評估。該基準評估了多項能力，包括對網格和紋理、空間關係、語義指令、物理定律、知識轉移和推理等的理解。為了支援下游微調，我們透過結合啟發式技能和先驗資訊的自動化框架，提供了高品質的訓練資料。實驗結果表明，目前最先進的預訓練 VLA 和基於 VLM 的工作流程在我們的任務中都面臨挑戰。</paragraph>

##### **An Analysis on Automated Metrics for Evaluating Japanese-English Chat Translation**
2412.18190v1 by Andre Rusli, Makoto Shishido

This paper analyses how traditional baseline metrics, such as BLEU and TER,
and neural-based methods, such as BERTScore and COMET, score several NMT models
performance on chat translation and how these metrics perform when compared to
human-annotated scores. The results show that for ranking NMT models in chat
translations, all metrics seem consistent in deciding which model outperforms
the others. This implies that traditional baseline metrics, which are faster
and simpler to use, can still be helpful. On the other hand, when it comes to
better correlation with human judgment, neural-based metrics outperform
traditional metrics, with COMET achieving the highest correlation with the
human-annotated score on a chat translation. However, we show that even the
best metric struggles when scoring English translations from sentences with
anaphoric zero-pronoun in Japanese.

摘要：本文分析了傳統基準指標（例如 BLEU 和 TER）和基於神經網路的方法（例如 BERTScore 和 COMET）如何評分多個 NMT 模型在聊天翻譯上的表現，以及這些指標與人工評分相比的表現。結果顯示，在對聊天翻譯中的 NMT 模型進行排名時，所有指標在決定哪個模型優於其他模型方面似乎都一致。這意味著傳統基準指標（使用起來更快速、更簡單）仍然可能有所幫助。另一方面，在與人類判斷相關性較高的方面，基於神經網路的指標優於傳統指標，COMET 在聊天翻譯中與人工評分相關性最高。然而，我們顯示，即使是最好的指標在評分日文中有指代性零代名詞的句子所翻譯的英文時也會遇到困難。

##### **On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs**
2412.18188v1 by Andre Rusli, Makoto Shishido

This research explores the applicability of cross-lingual transfer learning
from English to Japanese and Indonesian using the XLM-R pre-trained model. The
results are compared with several previous works, either by models using a
similar zero-shot approach or a fully-supervised approach, to provide an
overview of the zero-shot transfer learning approach's capability using XLM-R
in comparison with existing models. Our models achieve the best result in one
Japanese dataset and comparable results in other datasets in Japanese and
Indonesian languages without being trained using the target language.
Furthermore, the results suggest that it is possible to train a multi-lingual
model, instead of one model for each language, and achieve promising results.

摘要：本研究探討使用預先訓練的 XLM-R 模型，從英語轉移學習到日語和印尼語的可行性。
結果與之前數項作品進行比較，無論是使用類似零次學習方法或完全監督學習方法的模型，以提供使用 XLM-R 的零次學習轉移學習方法的能力概觀，與現有模型進行比較。我們的模型在一個日語資料集取得最佳結果，在其他日語和印尼語資料集取得可比較的結果，而沒有使用目標語言進行訓練。
此外，結果表明有可能訓練多語言模型，而不是針對每種語言訓練一個模型，並取得有希望的結果。

##### **TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization**
2412.18185v1 by Yucong Luo, Mingyue Cheng, Jie Ouyang, Xiaoyu Tao, Qi Liu

Text-to-image generative models excel in creating images from text but
struggle with ensuring alignment and consistency between outputs and prompts.
This paper introduces TextMatch, a novel framework that leverages multimodal
optimization to address image-text discrepancies in text-to-image (T2I)
generation and editing. TextMatch employs a scoring strategy powered by large
language models (LLMs) and visual question-answering (VQA) models to evaluate
semantic consistency between prompts and generated images. By integrating
multimodal in-context learning and chain of thought reasoning, our method
dynamically refines prompts through iterative optimization. This process
ensures that the generated images better capture user intent of, resulting in
higher fidelity and relevance. Extensive experiments demonstrate that TextMatch
significantly improves text-image consistency across multiple benchmarks,
establishing a reliable framework for advancing the capabilities of
text-to-image generative models. Our code is available at
https://anonymous.4open.science/r/TextMatch-F55C/.

摘要：文字到影像生成模型在根據文字建立影像方面表現優異，但難以確保輸出與提示之間的一致性和對齊。本文介紹 TextMatch，一個創新的架構，利用多模態最佳化來解決文字到影像 (T2I) 生成和編輯中的影像文字差異。TextMatch 採用由大型語言模型 (LLM) 和視覺問答 (VQA) 模型提供支援的評分策略，以評估提示和生成影像之間的語義一致性。透過整合多模態情境學習和思維鏈推理，我們的模型透過反覆最佳化動態地改善提示。這個程序可確保生成的影像更能捕捉使用者的意圖，帶來更高的保真度和相關性。廣泛的實驗證明 TextMatch 大幅改善了多個基準中的文字影像一致性，為提升文字到影像生成模型的能力建立了一個可靠的架構。我們的程式碼可在 https://anonymous.4open.science/r/TextMatch-F55C/ 取得。

##### **Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization**
2412.18177v1 by Sihao Liu, Yibo Yang, Xiaojie Li, David A. Clifton, Bernard Ghanem

Online continual learning (OCL) seeks to learn new tasks from data streams
that appear only once, while retaining knowledge of previously learned tasks.
Most existing methods rely on replay, focusing on enhancing memory retention
through regularization or distillation. However, they often overlook the
adaptability of the model, limiting the ability to learn generalizable and
discriminative features incrementally from online training data. To address
this, we introduce a plug-and-play module, S6MOD, which can be integrated into
most existing methods and directly improve adaptability. Specifically, S6MOD
introduces an extra branch after the backbone, where a mixture of
discretization selectively adjusts parameters in a selective state space model,
enriching selective scan patterns such that the model can adaptively select the
most sensitive discretization method for current dynamics. We further design a
class-conditional routing algorithm for dynamic, uncertainty-based adjustment
and implement a contrastive discretization loss to optimize it. Extensive
experiments combining our module with various models demonstrate that S6MOD
significantly enhances model adaptability, leading to substantial performance
gains and achieving the state-of-the-art results.

摘要：線上持續學習 (OCL) 旨在從只出現一次的資料串流中學習新任務，同時保留先前學習任務的知識。
現有的方法大多依賴於重播，專注於透過正規化或知識萃取來增強記憶保留。然而，它們經常忽略模型的適應性，這限制了從線上訓練資料中逐步學習可概化和具區辨力的特徵的能力。為了解決這個問題，我們引入了一個即插即用的模組 S6MOD，它可以整合到大多數現有方法中並直接改善適應性。具體來說，S6MOD 在主幹網路後引入一個額外的分支，其中離散化的混合在選擇性狀態空間模型中選擇性地調整參數，豐富選擇性掃描模式，以便模型可以根據當前動態自適應地選擇最敏感的離散化方法。我們進一步設計了一個類條件路由演算法，用於動態、基於不確定性的調整，並實作一個對比離散化損失來最佳化它。將我們的模組與各種模型結合的廣泛實驗表明，S6MOD 大幅增強了模型適應性，從而顯著提升效能並達成最先進的結果。

##### **Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation**
2412.18176v1 by Yucong Luo, Qitao Qin, Hao Zhang, Mingyue Cheng, Ruiran Yan, Kefan Wang, Jie Ouyang

Sequential recommendation (SR) systems have evolved significantly over the
past decade, transitioning from traditional collaborative filtering to deep
learning approaches and, more recently, to large language models (LLMs). While
the adoption of LLMs has driven substantial advancements, these models
inherently lack collaborative filtering information, relying primarily on
textual content data neglecting other modalities and thus failing to achieve
optimal recommendation performance. To address this limitation, we propose
Molar, a Multimodal large language sequential recommendation framework that
integrates multiple content modalities with ID information to capture
collaborative signals effectively. Molar employs an MLLM to generate unified
item representations from both textual and non-textual data, facilitating
comprehensive multimodal modeling and enriching item embeddings. Additionally,
it incorporates collaborative filtering signals through a post-alignment
mechanism, which aligns user representations from content-based and ID-based
models, ensuring precise personalization and robust performance. By seamlessly
combining multimodal content with collaborative filtering insights, Molar
captures both user interests and contextual semantics, leading to superior
recommendation accuracy. Extensive experiments validate that Molar
significantly outperforms traditional and LLM-based baselines, highlighting its
strength in utilizing multimodal data and collaborative signals for sequential
recommendation tasks. The source code is available at
https://anonymous.4open.science/r/Molar-8B06/.

摘要：序列推薦 (SR) 系統在過去十年間大幅演進，從傳統的協同過濾轉變為深度學習方法，最近則轉變為大型語言模型 (LLM)。雖然採用 LLM 已經推動了大幅進展，但這些模型本質上缺乏協同過濾資訊，主要依賴文字內容資料，忽略其他模式，因此無法達成最佳的推薦成效。為了解決這個限制，我們提出 Molar，一個多模態大型語言序列推薦架構，它整合多種內容模式與 ID 資訊，以有效擷取協同訊號。Molar 使用 MLLM 從文字和非文字資料產生統一的項目表徵，促進全面的多模態建模，並豐富項目嵌入。此外，它透過後對齊機制納入協同過濾訊號，這個機制會對齊基於內容和基於 ID 的模型中的使用者表徵，確保精確的個人化和強健的效能。透過無縫結合多模態內容與協同過濾見解，Molar 擷取使用者興趣和脈絡語意，進而提升推薦的精確度。廣泛的實驗驗證 Molar 明顯優於傳統和基於 LLM 的基準，突顯其在利用多模態資料和協同訊號進行序列推薦任務的強項。原始程式碼可在 https://anonymous.4open.science/r/Molar-8B06/ 取得。

##### **INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**
2412.18174v1 by Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow

Recent advancements have underscored the potential of large language model
(LLM)-based agents in financial decision-making. Despite this progress, the
field currently encounters two main challenges: (1) the lack of a comprehensive
LLM agent framework adaptable to a variety of financial tasks, and (2) the
absence of standardized benchmarks and consistent datasets for assessing agent
performance. To tackle these issues, we introduce \textsc{InvestorBench}, the
first benchmark specifically designed for evaluating LLM-based agents in
diverse financial decision-making contexts. InvestorBench enhances the
versatility of LLM-enabled agents by providing a comprehensive suite of tasks
applicable to different financial products, including single equities like
stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we
assess the reasoning and decision-making capabilities of our agent framework
using thirteen different LLMs as backbone models, across various market
environments and tasks. Furthermore, we have curated a diverse collection of
open-source, multi-modal datasets and developed a comprehensive suite of
environments for financial decision-making. This establishes a highly
accessible platform for evaluating financial agents' performance across various
scenarios.

摘要：<paragraph>最近的進展強調了大型語言模型 (LLM) 基礎代理在財務決策中的潛力。儘管有這些進展，該領域目前遇到兩個主要挑戰：(1) 缺乏可適應各種財務任務的綜合 LLM 代理架構，以及 (2) 缺乏標準化基準和一致的數據集來評估代理效能。為了解決這些問題，我們引入了 \textsc{InvestorBench}，這是第一個專門設計用於評估 LLM 基礎代理在各種財務決策情境中的基準。InvestorBench 透過提供適用於不同金融產品的綜合任務套件，增強了 LLM 啟用代理的多功能性，包括股票、加密貨幣和交易所買賣基金 (ETF) 等單一股票。此外，我們使用十三種不同的 LLM 作為主幹模型，在各種市場環境和任務中評估我們代理架構的推理和決策能力。此外，我們策劃了多模態開源數據集的多樣化集合，並開發了一套全面的財務決策環境。這建立了一個高度可存取的平台，用於評估金融代理在各種情境中的表現。</paragraph>

##### **KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management**
2412.18169v1 by Rongxin Cheng, Yifan Peng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen

The stateful nature of large language model (LLM) servingcan easily throttle
precious GPU memory under load burstor long-generation requests like
chain-of-thought reasoning,causing latency spikes due to queuing incoming
requests. However, state-of-the-art KVCache centric approaches handleload
spikes by dropping, migrating, or swapping KVCache,which faces an essential
tradeoff between the performance ofongoing vs. incoming requests and thus still
severely violatesSLO.This paper makes a key observation such that model
param-eters are independent of the requests and are replicated acrossGPUs, and
thus proposes a parameter-centric approach byselectively dropping replicated
parameters to leave preciousmemory for requests. However, LLM requires KVCache
tobe saved in bound with model parameters and thus droppingparameters can cause
either huge computation waste or longnetwork delay, affecting all ongoing
requests. Based on the ob-servation that attention operators can be decoupled
from otheroperators, this paper further proposes a novel remote
attentionmechanism through pipeline parallelism so as to serve up-coming
requests with the additional memory borrowed fromparameters on remote GPUs.
This paper further addresses sev-eral other challenges including lively
exchanging KVCachewith incomplete parameters, generating an appropriate
planthat balances memory requirements with cooperative exe-cution overhead, and
seamlessly restoring parameters whenthe throttling has gone. Evaluations show
thatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x
compared to the state-of-the-art.

摘要：大型語言模型 (LLM) 服務的狀態性質在負載突然增加或長生成請求（例如思考鏈推理）的情況下，容易限制寶貴的 GPU 記憶體，導致排隊的傳入請求產生延遲高峰。然而，最先進的 KVCache 中心化方法透過捨棄、遷移或交換 KVCache 來處理負載高峰，這在正在進行的請求與傳入請求的效能之間面臨著本質上的權衡，因此仍然嚴重違反 SLO。本文提出一個關鍵觀察，即模型參數與請求無關，並跨 GPU 複製，因此提出一個以參數為中心的策略，透過選擇性地捨棄複製的參數，為請求留下寶貴的記憶體。然而，LLM 要求 KVCache 與模型參數一起儲存，因此捨棄參數可能會導致大量的運算浪費或長的網路延遲，影響所有正在進行的請求。基於注意力運算子可以與其他運算子分離的觀察，本文進一步透過管線平行化提出一個新穎的遠端注意力機制，以便使用從遠端 GPU 上參數借來的額外記憶體來服務即將到來的請求。本文進一步解決了其他幾個挑戰，包括與不完整參數交換 KVCache、產生一個平衡記憶體需求與合作執行負擔的適當計畫，以及在限制解除時無縫還原參數。評估顯示，與最先進的技術相比，KUNSERVE 將在限制下的請求尾端 TTFT 減少了多達 27.3 倍。

##### **Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi**
2412.18163v1 by Rasika Ransing, Mohammed Amaan Dhamaskar, Ayush Rajpurohit, Amey Dhoke, Sanket Dalvi

India's vast linguistic diversity presents unique challenges and
opportunities for technological advancement, especially in the realm of Natural
Language Processing (NLP). While there has been significant progress in NLP
applications for widely spoken languages, the regional languages of India, such
as Marathi and Hindi, remain underserved. Research in the field of NLP for
Indian regional languages is at a formative stage and holds immense
significance. The paper aims to build a platform which enables the user to use
various features like text anonymization, abstractive text summarization and
spell checking in English, Hindi and Marathi language. The aim of these tools
is to serve enterprise and consumer clients who predominantly use Indian
Regional Languages.

摘要：印度廣泛的語言多元性為技術進步帶來了獨特的挑戰和機遇，特別是在自然語言處理 (NLP) 領域。儘管廣泛使用的語言的 NLP 應用已取得顯著進展，但印度的區域語言，如馬拉地語和印地語，仍未得到充分利用。印度區域語言 NLP 領域的研究處於形成階段，具有重大意義。本文旨在建立一個平台，使用戶能夠使用各種功能，例如英文、印地語和馬拉地語的文本匿名化、抽象文本摘要和拼寫檢查。這些工具的目的是為主要使用印度區域語言的企業和消費者客戶提供服務。

##### **VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities**
2412.18161v1 by Shray Mathur, Noah van der Vleuten, Kevin Yager, Esther Tsai

Scientific user facilities, such as synchrotron beamlines, are equipped with
a wide array of hardware and software tools that require a codebase for
human-computer-interaction. This often necessitates developers to be involved
to establish connection between users/researchers and the complex
instrumentation. The advent of generative AI presents an opportunity to bridge
this knowledge gap, enabling seamless communication and efficient experimental
workflows. Here we present a modular architecture for the Virtual Scientific
Companion (VISION) by assembling multiple AI-enabled cognitive blocks that each
scaffolds large language models (LLMs) for a specialized task. With VISION, we
performed LLM-based operation on the beamline workstation with low latency and
demonstrated the first voice-controlled experiment at an X-ray scattering
beamline. The modular and scalable architecture allows for easy adaptation to
new instrument and capabilities. Development on natural language-based
scientific experimentation is a building block for an impending future where a
science exocortex -- a synthetic extension to the cognition of scientists --
may radically transform scientific practice and discovery.

摘要：科學使用者設施，例如同步加速器光束線，配備了廣泛的硬體和軟體工具，需要一個用於人機互動的程式碼庫。這通常需要開發人員參與，才能在使用者/研究人員和複雜的儀器之間建立連接。生成式 AI 的出現提供了一個彌合知識差距的機會，實現無縫的溝通和高效的實驗工作流程。在此，我們提出了一個虛擬科學伴侶 (VISION) 的模組化架構，通過組裝多個 AI 驅動的認知區塊，每個區塊都為特定任務提供大型語言模型 (LLM) 的支架。使用 VISION，我們在光束線工作站上執行了基於 LLM 的操作，具有低延遲，並展示了在 X 射線散射光束線上進行的首次語音控制實驗。模組化和可擴充的架構允許輕鬆適應新的儀器和功能。基於自然語言的科學實驗的開發是一個即將到來的未來的基石，在這個未來中，科學外皮層（科學家認知的合成延伸）可能會徹底改變科學實踐和發現。

##### **Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance**
2412.18157v1 by Yaoyun Zhang, Xuenan Xu, Mengyue Wu

The video-to-audio (V2A) generation task has drawn attention in the field of
multimedia due to the practicality in producing Foley sound. Semantic and
temporal conditions are fed to the generation model to indicate sound events
and temporal occurrence. Recent studies on synthesizing immersive and
synchronized audio are faced with challenges on videos with moving visual
presence. The temporal condition is not accurate enough while low-resolution
semantic condition exacerbates the problem. To tackle these challenges, we
propose Smooth-Foley, a V2A generative model taking semantic guidance from the
textual label across the generation to enhance both semantic and temporal
alignment in audio. Two adapters are trained to leverage pre-trained
text-to-audio generation models. A frame adapter integrates high-resolution
frame-wise video features while a temporal adapter integrates temporal
conditions obtained from similarities of visual frames and textual labels. The
incorporation of semantic guidance from textual labels achieves precise
audio-video alignment. We conduct extensive quantitative and qualitative
experiments. Results show that Smooth-Foley performs better than existing
models on both continuous sound scenarios and general scenarios. With semantic
guidance, the audio generated by Smooth-Foley exhibits higher quality and
better adherence to physical laws.

摘要：影片轉音訊 (V2A) 生成任務在多媒體領域中備受關注，因為在製作 Foley 音效方面實用。語意和時間條件會提供給生成模型，以表示音效事件和時間發生。最近關於合成沉浸式和同步音訊的研究面臨影片中移動視覺存在性的挑戰。時間條件不夠準確，而低解析度語意條件會加劇問題。為了應對這些挑戰，我們提出 Smooth-Foley，一個 V2A 生成模型，從生成過程中文字標籤中獲取語意指導，以增強音訊中的語意和時間對齊。訓練兩個適配器，以利用預先訓練的文字轉音訊生成模型。一個框架適配器整合高解析度逐幀影片特徵，而時間適配器整合從視覺幀和文字標籤相似性中獲得的時間條件。從文字標籤中加入語意指導可達成精準的音訊視訊對齊。我們進行廣泛的量化和定性實驗。結果顯示，Smooth-Foley 在連續音訊場景和一般場景中都比現有模型表現得更好。有了語意指導，Smooth-Foley 生成的音訊品質更高，且更符合物理定律。

##### **scReader: Prompting Large Language Models to Interpret scRNA-seq Data**
2412.18156v1 by Cong Li, Qingqing Long, Yuanchun Zhou, Meng Xiao

Large language models (LLMs) have demonstrated remarkable advancements,
primarily due to their capabilities in modeling the hidden relationships within
text sequences. This innovation presents a unique opportunity in the field of
life sciences, where vast collections of single-cell omics data from multiple
species provide a foundation for training foundational models. However, the
challenge lies in the disparity of data scales across different species,
hindering the development of a comprehensive model for interpreting genetic
data across diverse organisms. In this study, we propose an innovative hybrid
approach that integrates the general knowledge capabilities of LLMs with
domain-specific representation models for single-cell omics data
interpretation. We begin by focusing on genes as the fundamental unit of
representation. Gene representations are initialized using functional
descriptions, leveraging the strengths of mature language models such as
LLaMA-2. By inputting single-cell gene-level expression data with prompts, we
effectively model cellular representations based on the differential expression
levels of genes across various species and cell types. In the experiments, we
constructed developmental cells from humans and mice, specifically targeting
cells that are challenging to annotate. We evaluated our methodology through
basic tasks such as cell annotation and visualization analysis. The results
demonstrate the efficacy of our approach compared to other methods using LLMs,
highlighting significant improvements in accuracy and interoperability. Our
hybrid approach enhances the representation of single-cell data and offers a
robust framework for future research in cross-species genetic analysis.

摘要：大型語言模型 (LLM) 已展現出顯著的進步，這主要歸因於它們在建模文字序列中隱藏關係的能力。這項創新為生命科學領域帶來了獨特的契機，其中來自多個物種的單細胞組學數據的龐大集合為訓練基礎模型提供了基礎。然而，挑戰在於不同物種之間的數據規模差異，這阻礙了開發一個用於解釋不同生物體遺傳數據的綜合模型。在本研究中，我們提出了一種創新的混合方法，它將 LLM 的一般知識能力與單細胞組學數據解釋的特定領域表示模型相結合。我們從將基因作為表示的基本單位開始。基因表示是使用功能描述初始化的，利用了成熟語言模型（例如 LLaMA-2）的優勢。通過使用提示輸入單細胞基因層級的表達數據，我們有效地根據不同物種和細胞類型的基因差異表達層級對細胞表示進行建模。在實驗中，我們構建了人類和小鼠的發育細胞，特別針對難以註解的細胞。我們透過細胞註解和視覺化分析等基本任務評估了我們的技術。結果證明，與使用 LLM 的其他方法相比，我們的技術方法有效，突顯了準確性和互操作性的顯著改進。我們的混合方法增強了單細胞數據的表示，並為跨物種遺傳分析的未來研究提供了強大的架構。

##### **GeneSUM: Large Language Model-based Gene Summary Extraction**
2412.18154v1 by Zhijian Chen, Chuan Hu, Min Wu, Qingqing Long, Xuezhi Wang, Yuanchun Zhou, Meng Xiao

Emerging topics in biomedical research are continuously expanding, providing
a wealth of information about genes and their function. This rapid
proliferation of knowledge presents unprecedented opportunities for scientific
discovery and formidable challenges for researchers striving to keep abreast of
the latest advancements. One significant challenge is navigating the vast
corpus of literature to extract vital gene-related information, a
time-consuming and cumbersome task. To enhance the efficiency of this process,
it is crucial to address several key challenges: (1) the overwhelming volume of
literature, (2) the complexity of gene functions, and (3) the automated
integration and generation. In response, we propose GeneSUM, a two-stage
automated gene summary extractor utilizing a large language model (LLM). Our
approach retrieves and eliminates redundancy of target gene literature and then
fine-tunes the LLM to refine and streamline the summarization process. We
conducted extensive experiments to validate the efficacy of our proposed
framework. The results demonstrate that LLM significantly enhances the
integration of gene-specific information, allowing more efficient
decision-making in ongoing research.

摘要：生物醫學研究中不斷擴展的新興主題，提供了大量關於基因及其功能的資訊。知識的快速擴散為科學發現提供了前所未有的機會，也為努力跟上最新進展的研究人員帶來了巨大的挑戰。其中一項重大挑戰是瀏覽大量的文獻，以提取重要的基因相關資訊，這項任務既耗時又繁瑣。為了提高此流程的效率，必須解決幾個關鍵挑戰：(1) 大量的文獻，(2) 基因功能的複雜性，以及 (3) 自動整合和生成。為了解決這些問題，我們提出了 GeneSUM，這是一個利用大型語言模型 (LLM) 的兩階段自動基因摘要萃取器。我們的做法是擷取並消除目標基因文獻的冗餘，然後微調 LLM 以優化和簡化摘要流程。我們進行了廣泛的實驗，以驗證我們提出的架構的功效。結果表明，LLM 大幅增強了基因特定資訊的整合，讓正在進行的研究能夠更有效率地進行決策。

##### **CoAM: Corpus of All-Type Multiword Expressions**
2412.18151v1 by Yusuke Ide, Joshua Tanner, Adam Nohejl, Jacob Hoffman, Justin Vasselli, Hidetaka Kamigaito, Taro Watanabe

Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.
MWE identification, i.e., detecting MWEs in text, can play a key role in
downstream tasks such as machine translation. Existing datasets for MWE
identification are inconsistently annotated, limited to a single type of MWE,
or limited in size. To enable reliable and comprehensive evaluation, we created
CoAM: Corpus of All-Type Multiword Expressions, a dataset of 1.3K sentences
constructed through a multi-step process to enhance data quality consisting of
human annotation, human review, and automated consistency checking. MWEs in
CoAM are tagged with MWE types, such as Noun and Verb, to enable fine-grained
error analysis. Annotations for CoAM were collected using a new interface
created with our interface generator, which allows easy and flexible annotation
of MWEs in any form, including discontinuous ones. Through experiments using
CoAM, we find that a fine-tuned large language model outperforms the current
state-of-the-art approach for MWE identification. Furthermore, analysis using
our MWE type tagged data reveals that Verb MWEs are easier than Noun MWEs to
identify across approaches.

摘要：多字詞表達（MWE）指的是多個單字的慣用語序列。
MWE 識別，即在文字中偵測 MWE，可以在機器翻譯等下游任務中扮演關鍵角色。現有的 MWE 識別資料集標註不一致，僅限於單一類型的 MWE，或規模有限。為了進行可靠且全面的評估，我們建立了 CoAM：全類型多字詞表達語料庫，這是一個包含 1.3K 個句子的資料集，透過多步驟流程建構而成，以提升資料品質，包括人工標註、人工審查和自動化一致性檢查。CoAM 中的 MWE 標註有 MWE 類型，例如名詞和動詞，以進行細緻的錯誤分析。CoAM 的標註是使用我們介面產生器建立的新介面收集而來，它允許輕鬆且彈性地標註任何形式的 MWE，包括不連續的 MWE。透過使用 CoAM 進行實驗，我們發現經過微調的大語言模型優於 MWE 識別的現有最新方法。此外，使用我們標註的 MWE 類型資料進行分析顯示，動詞 MWE 比名詞 MWE 更容易透過各種方法識別。

##### **EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation**
2412.18150v1 by Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, Chongyi Li

Recently, Text-to-Image (T2I) generation models have achieved significant
advancements. Correspondingly, many automated metrics have emerged to evaluate
the image-text alignment capabilities of generative models. However, the
performance comparison among these automated metrics is limited by existing
small datasets. Additionally, these datasets lack the capacity to assess the
performance of automated metrics at a fine-grained level. In this study, we
contribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with
fine-grained human annotations for image-text alignment-related tasks. In the
construction process, we employ various strategies such as balanced prompt
sampling and data re-annotation to ensure the diversity and reliability of our
benchmark. This allows us to comprehensively evaluate the effectiveness of
image-text alignment metrics for T2I models. Meanwhile, we introduce two new
methods to evaluate the image-text alignment capabilities of T2I models:
FGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to
produce fine-grained image-text alignment scores and PN-VQA which adopts a
novel positive-negative VQA manner in VQA models for zero-shot fine-grained
evaluation. Both methods achieve impressive performance in image-text alignment
evaluations. We also use our methods to rank current AIGC models, in which the
results can serve as a reference source for future study and promote the
development of T2I generation. The data and code will be made publicly
available.

摘要：<paragraph>近期，文本到图像 (T2I) 生成模型取得了显著进展。相应地，许多自动化指标也随之出现，用于评估生成模型的图像文本对齐能力。然而，在现有的小型数据集的限制下，这些自动化指标之间的性能比较受到限制。此外，这些数据集缺乏在细粒度级别评估自动化指标性能的能力。在本研究中，我们贡献了一个 EvalMuse-40K 基准，收集了 40K 个图像文本对，并针对图像文本对齐相关任务进行细粒度的标注。在构建过程中，我们采用了平衡提示采样和数据重新标注等多种策略，以确保基准的多样性和可靠性。这使我们能够全面评估图像文本对齐指标对 T2I 模型的有效性。同时，我们引入了两种新的方法来评估 T2I 模型的图像文本对齐能力：FGA-BLIP2，它涉及端到端微调视觉语言模型以生成细粒度的图像文本对齐分数；以及 PN-VQA，它在 VQA 模型中采用了一种新颖的正负 VQA 方式进行零样本细粒度评估。两种方法在图像文本对齐评估中都取得了令人瞩目的性能。我们还使用我们的方法对当前的 AIGC 模型进行排名，其结果可以作为未来研究的参考来源，并促进 T2I 生成的发展。数据和代码将公开提供。</paragraph>

##### **Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media**
2412.18148v1 by Zhen Sun, Zongmin Zhang, Xinyue Shen, Ziyi Zhang, Yule Liu, Michael Backes, Yang Zhang, Xinlei He

Social media platforms are experiencing a growing presence of AI-Generated
Texts (AIGTs). However, the misuse of AIGTs could have profound implications
for public opinion, such as spreading misinformation and manipulating
narratives. Despite its importance, a systematic study to assess the prevalence
of AIGTs on social media is still lacking. To address this gap, this paper aims
to quantify, monitor, and analyze the AIGTs on online social media platforms.
We first collect a dataset (SM-D) with around 2.4M posts from 3 major social
media platforms: Medium, Quora, and Reddit. Then, we construct a diverse
dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines
popular open-source datasets and our AIGT datasets generated from social media
texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors.
With this setup, we identify the best-performing detector (OSM-Det). We then
apply OSM-Det to SM-D to track AIGTs over time and observe different trends of
AI Attribution Rate (AAR) across social media platforms from January 2022 to
October 2024. Specifically, Medium and Quora exhibit marked increases in AAR,
rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast,
Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the
same period. Our further analysis indicates that AIGTs differ from
human-written texts across several dimensions, including linguistic patterns,
topic distributions, engagement levels, and the follower distribution of
authors. We envision our analysis and findings on AIGTs in social media can
shed light on future research in this domain.

摘要：<paragraph>社群媒體平台上出現越來越多由 AI 所產生的文字 (AIGT)。然而，AIGT 的誤用可能會對輿論造成深遠影響，例如散布錯誤資訊和操縱敘述。儘管其重要性，但仍缺乏系統性的研究來評估 AIGT 在社群媒體上的盛行程度。為了解決這個差距，本文旨在量化、監控和分析線上社群媒體平台上的 AIGT。我們首先從 3 個主要的社群媒體平台：Medium、Quora 和 Reddit 收集了包含約 240 萬則貼文的資料集 (SM-D)。然後，我們建構了一個多元化的資料集 (AIGTBench) 來訓練和評估 AIGT 偵測器。AIGTBench 結合了熱門的開源資料集和我們從社群媒體文字中生成的 AIGT 資料集，由 12 個 LLM 產生，作為評估主流偵測器的基準。透過此設定，我們找出效能最佳的偵測器 (OSM-Det)。接著，我們將 OSM-Det 套用至 SM-D 以追蹤 AIGT，並觀察從 2022 年 1 月到 2024 年 10 月不同社群媒體平台的 AI 歸因率 (AAR) 趨勢。具體來說，Medium 和 Quora 的 AAR 顯著增加，分別從 1.77% 上升至 37.03% 和 2.06% 上升至 38.95%。相比之下，Reddit 的成長較慢，AAR 在同一時期從 1.31% 上升至 2.45%。我們的進一步分析指出，AIGT 在語言模式、主題分佈、參與度和作者的追蹤者分佈等多個面向與人類撰寫的文字不同。我們預見我們對社群媒體中 AIGT 的分析和發現，可以為此領域的未來研究提供啟發。</paragraph>

##### **Text-Aware Adapter for Few-Shot Keyword Spotting**
2412.18142v1 by Youngmoon Jung, Jinyoung Lee, Seungjin Lee, Myunghun Jung, Yong-Hyeok Lee, Hoon-Young Cho

Recent advances in flexible keyword spotting (KWS) with text enrollment allow
users to personalize keywords without uttering them during enrollment. However,
there is still room for improvement in target keyword performance. In this
work, we propose a novel few-shot transfer learning method, called text-aware
adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for
specific keywords with limited speech samples. To adapt the acoustic encoder,
we leverage a jointly pre-trained text encoder to generate a text embedding
that acts as a representative vector for the keyword. By fine-tuning only a
small portion of the network while keeping the core components' weights intact,
the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless
return to the original pre-trained model. In our experiments, the TA-adapter
demonstrated significant performance improvements across 35 distinct keywords
from the Google Speech Commands V2 dataset, with only a 0.14% increase in the
total number of parameters.

摘要：近來在具有文字註冊的彈性關鍵字點選 (KWS) 中的進展，讓使用者在註冊期間無需說出關鍵字即可個人化關鍵字。然而，目標關鍵字的表現仍有進步空間。在這項工作中，我們提出一個稱為文字感知適配器 (TA-adapter) 的新穎小樣本轉移學習方法，其設計用於增強預先訓練好的彈性 KWS 模型，以處理具有有限語音範例的特定關鍵字。為了適應音訊編碼器，我們利用一個共同預先訓練好的文字編碼器來產生文字嵌入，作為關鍵字的代表向量。透過微調網路中僅一小部分，同時保持核心元件的權重不變，TA-adapter 證明對於小樣本 KWS 而言非常有效率，讓無縫返回至原始預先訓練好的模型成為可能。在我們的實驗中，TA-adapter 在 Google 語音指令 V2 資料集中的 35 個不同關鍵字中展示出顯著的效能提升，而參數總數僅增加了 0.14%。

##### **Ensuring Consistency for In-Image Translation**
2412.18139v1 by Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Baohang Li, Zhirui Zhang, Yunfei Lu, Dandan Tu, Duyu Tang, Hui Wang, Bing Qin, Ting Liu

The in-image machine translation task involves translating text embedded
within images, with the translated results presented in image format. While
this task has numerous applications in various scenarios such as film poster
translation and everyday scene image translation, existing methods frequently
neglect the aspect of consistency throughout this process. We propose the need
to uphold two types of consistency in this task: translation consistency and
image generation consistency. The former entails incorporating image
information during translation, while the latter involves maintaining
consistency between the style of the text-image and the original image,
ensuring background integrity. To address these consistency requirements, we
introduce a novel two-stage framework named HCIIT (High-Consistency In-Image
Translation) which involves text-image translation using a multimodal
multilingual large language model in the first stage and image backfilling with
a diffusion model in the second stage. Chain of thought learning is utilized in
the first stage to enhance the model's ability to leverage image information
during translation. Subsequently, a diffusion model trained for
style-consistent text-image generation ensures uniformity in text style within
images and preserves background details. A dataset comprising 400,000
style-consistent pseudo text-image pairs is curated for model training. Results
obtained on both curated test sets and authentic image test sets validate the
effectiveness of our framework in ensuring consistency and producing
high-quality translated images.

摘要：圖像內機器翻譯任務包含翻譯嵌入在圖像中的文字，翻譯結果以圖像格式呈現。雖然此任務在各種場景中都有許多應用，例如電影海報翻譯和日常場景圖像翻譯，現有方法經常忽略整個過程中一致性的方面。我們提出需要在此任務中維持兩種一致性：翻譯一致性和圖像生成一致性。前者需要在翻譯過程中納入圖像資訊，而後者則涉及維持文字圖像與原始圖像的風格一致性，確保背景完整性。為了滿足這些一致性需求，我們引進一個名為 HCIIT（高一致性圖像內翻譯）的新穎兩階段架構，其中包含在第一階段使用多模態多語言大型語言模型進行文字圖像翻譯，以及在第二階段使用擴散模型進行圖像回填。在第一階段利用思考鏈學習來增強模型在翻譯過程中利用圖像資訊的能力。隨後，針對風格一致的文字圖像生成訓練的擴散模型確保圖像中文字風格的一致性，並保留背景細節。整理了一個包含 400,000 個風格一致的偽文字圖像對的資料集，以進行模型訓練。在整理好的測試集和真實圖像測試集上獲得的結果驗證了我們架構在確保一致性並產生高品質翻譯圖像方面的有效性。

##### **LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment**
2412.18135v1 by Binrui Zeng, Bin Ji, Xiaodong Liu, Jie Yu, Shasha Li, Jun Ma, Xiaopeng Li, Shangwen Wang, Xinran Hong

As large language models (LLMs) demonstrate exceptional performance across
various domains, the deployment of these models on edge devices has emerged as
a new trend. Quantization techniques, which reduce the size and memory
footprint of LLMs, are effective for enabling deployment on
resource-constrained edge devices. However, existing one-size-fits-all
quantization methods often fail to dynamically adjust the memory consumption of
LLMs based on specific hardware characteristics and usage scenarios. To address
this limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a
system for adaptive quantization and dynamic deployment of LLMs based on layer
importance. LSAQ evaluates layer importance by constructing top-k token sets
from the inputs and outputs of each layer and calculating their Jaccard
coefficient. Using this evaluation, the system adaptively adjusts quantization
strategies in real time according to the resource availability of edge devices,
assigning different precision levels to layers of varying importance. This
approach significantly reduces the storage requirements of LLMs while
maintaining model performance, enabling efficient deployment across diverse
hardware platforms and usage scenarios.

摘要：隨著大型語言模型 (LLM) 在各種領域展現出卓越的效能，在邊緣裝置部署這些模型已成為一種新趨勢。量化技術可縮小 LLM 的大小和記憶體佔用空間，對於在資源受限的邊緣裝置上部署 LLM 來說十分有效。不過，現有的統一量化方法通常無法根據特定的硬體特性和使用情境，動態調整 LLM 的記憶體消耗。為了解決這個限制，我們提出 LSAQ（特定層自適應量化），這是一個基於層重要性的 LLM 自適應量化和動態部署系統。LSAQ 透過從每個層的輸入和輸出建構前 K 個權杖組，並計算其 Jaccard 係數，來評估層的重要性。系統使用此評估，根據邊緣裝置的資源可用性，即時自適應調整量化策略，並將不同的精確度等級指定給不同重要性的層。這種方法大幅降低了 LLM 的儲存需求，同時維持模型效能，可以在不同的硬體平台和使用情境中進行有效率的部署。

##### **Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy**
2412.18125v1 by Qian Tao, Xiyuan Wang, Muhan Zhang, Shuxian Hu, Wenyuan Yu, Jingren Zhou

Graph neural networks (GNNs) have become a prevalent framework for graph
tasks. Many recent studies have proposed the use of graph convolution methods
over the numerous subgraphs of each graph, a concept known as subgraph graph
neural networks (subgraph GNNs), to enhance GNNs' ability to distinguish
non-isomorphic graphs. To maximize the expressiveness, subgraph GNNs often
require each subgraph to have equal size to the original graph. Despite their
impressive performance, subgraph GNNs face challenges due to the vast number
and large size of subgraphs which lead to a surge in training data, resulting
in both storage and computational inefficiencies. In response to this problem,
this paper introduces Ego-Nets-Fit-All (ENFA), a model that uniformly takes the
smaller ego nets as subgraphs, thereby providing greater storage and
computational efficiency, while at the same time guarantees identical outputs
to the original subgraph GNNs even taking the whole graph as subgraphs. The key
is to identify and eliminate the redundant computation among subgraphs. For
example, a node $v_i$ may appear in multiple subgraphs but is far away from all
of their centers (the unsymmetric part between subgraphs). Therefore, its first
few rounds of message passing within each subgraph can be computed once in the
original graph instead of being computed multiple times within each subgraph.
Such strategy enables our ENFA to accelerate subgraph GNNs in an exact way,
unlike previous sampling approaches that often lose the performance. Extensive
experiments across various datasets reveal that compared with the conventional
subgraph GNNs, ENFA can reduce storage space by 29.0% to 84.5% and improve
training efficiency by up to 1.66x.

摘要：圖形神經網絡 (GNN) 已成為圖形任務的流行框架。許多近期研究建議在每個圖形的眾多子圖上使用圖形卷積方法，這是一個稱為子圖圖形神經網絡 (subgraph GNN) 的概念，用以提升 GNN 區分非同構圖形的能力。為了最大化表達力，子圖 GNN 通常要求每個子圖的大小與原始圖形相同。儘管效能令人印象深刻，但由於子圖數量龐大且大小不一，導致訓練資料激增，造成儲存和運算效率不彰，因此子圖 GNN 面臨挑戰。為了應對這個問題，本文介紹了 Ego-Nets-Fit-All (ENFA)，這是一個模型，它將較小的自我網路視為子圖，從而提供更大的儲存和運算效率，同時保證與原始子圖 GNN 相同的輸出，即使將整個圖形視為子圖。關鍵在於識別和消除子圖之間的重複運算。例如，節點 $v_i$ 可能出現在多個子圖中，但距離所有子圖中心都很遠（子圖之間的不對稱部分）。因此，它在每個子圖中的前幾輪訊息傳遞可以在原始圖形中計算一次，而不是在每個子圖中計算多次。這種策略使我們的 ENFA 能夠以精確的方式加速子圖 GNN，這與通常會降低效能的先前抽樣方法不同。在各種資料集上的廣泛實驗顯示，與傳統的子圖 GNN 相比，ENFA 可將儲存空間減少 29.0% 至 84.5%，並將訓練效率提升多達 1.66 倍。

##### **AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models**
2412.18123v1 by Yiming Wang, Jiahao Chen, Qingming Li, Xing Yang, Shouling Ji

As text-to-image (T2I) models continue to advance and gain widespread
adoption, their associated safety issues are becoming increasingly prominent.
Malicious users often exploit these models to generate Not-Safe-for-Work (NSFW)
images using harmful or adversarial prompts, highlighting the critical need for
robust safeguards to ensure the integrity and compliance of model outputs.
Current internal safeguards frequently degrade image quality, while external
detection methods often suffer from low accuracy and inefficiency.
  In this paper, we introduce AEIOU, a defense framework that is Adaptable,
Efficient, Interpretable, Optimizable, and Unified against NSFW prompts in T2I
models. AEIOU extracts NSFW features from the hidden states of the model's text
encoder, utilizing the separable nature of these features to detect NSFW
prompts. The detection process is efficient, requiring minimal inference time.
AEIOU also offers real-time interpretation of results and supports optimization
through data augmentation techniques. The framework is versatile, accommodating
various T2I architectures. Our extensive experiments show that AEIOU
significantly outperforms both commercial and open-source moderation tools,
achieving over 95% accuracy across all datasets and improving efficiency by at
least tenfold. It effectively counters adaptive attacks and excels in few-shot
and multi-label scenarios.

摘要：<paragraph>隨著文字到影像 (T2I) 模型持續進步並獲得廣泛採用，它們相關的安全問題也越來越突出。惡意使用者經常利用這些模型使用有害或對抗性的提示來產生不適合工作 (NSFW) 的影像，突顯了確保模型輸出完整性和合規性的強大防護措施的迫切需要。目前的內部防護措施經常會降低影像品質，而外部偵測方法則經常有準確度低和效率差的問題。在本文中，我們介紹了 AEIOU，這是一個在 T2I 模型中針對 NSFW 提示具有適應性、效率、可解釋性、可最佳化和統一性的防禦架構。AEIOU 從模型文字編碼器的隱藏狀態中提取 NSFW 特徵，利用這些特徵的可分離性質來偵測 NSFW 提示。偵測過程很有效率，只需要最少的推論時間。AEIOU 還提供結果的即時解釋，並透過資料擴充技術支援最佳化。這個架構很靈活，可以容納各種 T2I 架構。我們廣泛的實驗顯示，AEIOU 在所有資料集中的準確度都超過 95%，並且效率至少提高了十倍，顯著優於商業和開源的審核工具。它有效地對抗適應性攻擊，並在少次嘗試和多標籤場景中表現出色。</paragraph>

##### **Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm**
2412.18120v1 by Xiaoyang Hu, Richard L. Lewis

Cognitive tasks originally developed for humans are now increasingly used to
study language models. While applying these tasks is often straightforward,
interpreting their results can be challenging. In particular, when a model
underperforms, it's often unclear whether this results from a limitation in the
cognitive ability being tested or a failure to understand the task itself. A
recent study argued that GPT 3.5's declining performance on 2-back and 3-back
tasks reflects a working memory capacity limit similar to humans. By analyzing
a range of open-source language models of varying performance levels on these
tasks, we show that the poor performance instead reflects a limitation in task
comprehension and task set maintenance. In addition, we push the best
performing model to higher n values and experiment with alternative prompting
strategies, before analyzing model attentions. Our larger aim is to contribute
to the ongoing conversation around refining methodologies for the cognitive
evaluation of language models.

摘要：原本為人類開發的認知任務，現在越來越常被用來研究語言模型。雖然應用這些任務通常很直接，但要解讀其結果可能具有挑戰性。特別是當模型表現不佳時，通常不清楚這是否源於所測試認知能力的限制，或未能理解任務本身。最近的一項研究認為，GPT 3.5 在 2-back 和 3-back 任務中的表現下降反映出與人類相似的作業記憶容量限制。透過分析在這些任務中表現水準不一的各種開源語言模型，我們發現表現不佳反而反映出任務理解和任務設定維護的限制。此外，我們將表現最佳的模型推升至更高的 n 值，並嘗試使用替代提示策略，然後再分析模型注意力。我們更大的目標是為圍繞語言模型認知評估方法論的精進進行中的對話做出貢獻。

##### **AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation**
2412.18116v1 by Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li

Large language models (LLMs) have brought exciting new advances to mobile UI
agents, a long-standing research field that aims to complete arbitrary natural
language tasks through mobile UI interactions. However, existing UI agents
usually demand high reasoning capabilities of powerful large models that are
difficult to be deployed locally on end-users' devices, which raises huge
concerns about user privacy and centralized serving cost. One way to reduce the
required model size is to customize a smaller domain-specific model with
high-quality training data, e.g. large-scale human demonstrations of diverse
types of apps and tasks, while such datasets are extremely difficult to obtain.
Inspired by the remarkable coding abilities of recent small language models
(SLMs), we propose to convert the UI task automation problem to a code
generation problem, which can be effectively solved by an on-device SLM and
efficiently executed with an on-device code interpreter. Unlike normal coding
tasks that can be extensively pretrained with public datasets, generating UI
automation code is challenging due to the diversity, complexity, and
variability of target apps. Therefore, we adopt a document-centered approach
that automatically builds fine-grained API documentation for each app and
generates diverse task samples based on this documentation. By guiding the
agent with the synthetic documents and task samples, it learns to generate
precise and efficient scripts to complete unseen tasks. Based on detailed
comparisons with state-of-the-art mobile UI agents, our approach effectively
improves the mobile task automation with significantly higher success rates and
lower latency/token consumption. Code will be open-sourced.

摘要：大型語言模型 (LLM) 為行動裝置使用者介面代理程式帶來令人興奮的新進展，這是一個長久以來的研究領域，旨在透過行動裝置使用者介面互動來完成任意的自然語言任務。然而，現有的使用者介面代理程式通常需要強大的大型模型的高推理能力，而這些模型難以在終端使用者的裝置上進行本地部署，這引發了對使用者隱私和集中式服務成本的極大疑慮。縮小所需模型規模的一種方法是使用高品質訓練資料自訂較小的特定領域模型，例如，各種應用程式和任務的大規模人類示範，而此類資料集極難取得。受到近期小型語言模型 (SLM) 傑出的編碼能力的啟發，我們提議將使用者介面任務自動化問題轉換為程式碼產生問題，而這可以用裝置上的 SLM 有效解決，並使用裝置上的程式碼詮釋器有效執行。與可以透過公開資料集進行廣泛預訓練的一般編碼任務不同，產生使用者介面自動化程式碼具有挑戰性，原因在於目標應用程式的多樣性、複雜性和可變性。因此，我們採用以文件為中心的途徑，為每個應用程式自動建立細緻的 API 文件，並根據此文件產生多樣化的任務範例。透過使用合成的文件和任務範例引導代理程式，它會學習產生精確且有效的指令碼，以完成前所未見的任務。根據與現今最先進的行動裝置使用者介面代理程式的詳細比較，我們的途徑有效提升了行動裝置任務自動化，成功率顯著提高，且延遲時間/權杖消耗降低。程式碼將開放原始碼。

##### **AIGT: AI Generative Table Based on Prompt**
2412.18111v1 by Mingming Zhang, Zhiqing Xiao, Guoshan Lu, Sai Wu, Weiqiang Wang, Xing Fu, Can Yi, Junbo Zhao

Tabular data, which accounts for over 80% of enterprise data assets, is vital
in various fields. With growing concerns about privacy protection and
data-sharing restrictions, generating high-quality synthetic tabular data has
become essential. Recent advancements show that large language models (LLMs)
can effectively gener-ate realistic tabular data by leveraging semantic
information and overcoming the challenges of high-dimensional data that arise
from one-hot encoding. However, current methods do not fully utilize the rich
information available in tables. To address this, we introduce AI Generative
Table (AIGT) based on prompt enhancement, a novel approach that utilizes meta
data information, such as table descriptions and schemas, as prompts to
generate ultra-high quality synthetic data. To overcome the token limit
constraints of LLMs, we propose long-token partitioning algorithms that enable
AIGT to model tables of any scale. AIGT achieves state-of-the-art performance
on 14 out of 20 public datasets and two real industry datasets within the
Alipay risk control system.

摘要：表格資料佔企業資料資產的 80% 以上，在各個領域都至關重要。隨著隱私保護和資料共享限制的疑慮日益增加，產生高品質的合成表格資料已變得至關重要。最近的進展顯示，大型語言模型 (LLM) 可以有效地透過利用語義資訊並克服由 one-hot 編碼產生的高維度資料挑戰，來產生逼真的表格資料。然而，目前的技術並未充分利用表格中豐富的資訊。為了解決這個問題，我們引入了基於提示增強的 AI 生成表格 (AIGT)，這是一種新穎的方法，它利用元資料資訊，例如表格描述和架構，作為提示來產生極高品質的合成資料。為了克服 LLM 的 token 數量限制，我們提出了長 token 分割演算法，讓 AIGT 能夠建模任何規模的表格。在 20 個公開資料集和 Alipay 風險控制系統中的兩個實際產業資料集上，AIGT 達到了 14 個最先進的效能。

##### **SlimGPT: Layer-wise Structured Pruning for Large Language Models**
2412.18110v1 by Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu

Large language models (LLMs) have garnered significant attention for their
remarkable capabilities across various domains, whose vast parameter scales
present challenges for practical deployment. Structured pruning is an effective
method to balance model performance with efficiency, but performance
restoration under computational resource constraints is a principal challenge
in pruning LLMs. Therefore, we present a low-cost and fast structured pruning
method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We
propose Batched Greedy Pruning for rapid and near-optimal pruning, which
enhances the accuracy of head-wise pruning error estimation through grouped
Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic
Group Size, thereby achieving approximate local optimal pruning results within
one hour. Besides, we explore the limitations of layer-wise pruning from the
perspective of error accumulation and propose Incremental Pruning Ratio, a
non-uniform pruning strategy to reduce performance degradation. Experimental
results on the LLaMA benchmark show that SlimGPT outperforms other methods and
achieves state-of-the-art results.

摘要：大型語言模型 (LLM) 因其在各個領域的卓越能力而備受關注，其龐大的參數規模對實際部署提出了挑戰。結構化剪枝是一種平衡模型效能與效率的有效方法，但在計算資源受限的情況下，效能還原是剪枝 LLM 的主要挑戰。因此，我們提出了一種名為 SlimGPT 的低成本且快速的 LLM 結構化剪枝方法，該方法基於 Optimal Brain Surgeon 框架。我們提出批次貪婪剪枝以進行快速且近乎最佳的剪枝，透過群組 Cholesky 分解來提升頭部剪枝誤差估計的準確性，並透過動態群組大小來提升 FFN 的剪枝效率，從而在大約一小時內達成近似局部最佳剪枝結果。此外，我們從誤差累積的角度探討了層級剪枝的限制，並提出增量剪枝率，這是一種非均勻的剪枝策略，可減少效能下降。LLaMA 基準上的實驗結果顯示，SlimGPT 優於其他方法，並達到了最先進的結果。

##### **SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training**
2412.18107v1 by Jiaxing Yu, Xinda Wu, Yunfei Xu, Tieyao Zhang, Songruoyao Wu, Le Ma, Kejun Zhang

Lyric-to-melody generation aims to automatically create melodies based on
given lyrics, requiring the capture of complex and subtle correlations between
them. However, previous works usually suffer from two main challenges: 1)
lyric-melody alignment modeling, which is often simplified to
one-syllable/word-to-one-note alignment, while others have the problem of low
alignment accuracy; 2) lyric-melody harmony modeling, which usually relies
heavily on intermediates or strict rules, limiting model's capabilities and
generative diversity. In this paper, we propose SongGLM, a lyric-to-melody
generation system that leverages 2D alignment encoding and multi-task
pre-training based on the General Language Model (GLM) to guarantee the
alignment and harmony between lyrics and melodies. Specifically, 1) we
introduce a unified symbolic song representation for lyrics and melodies with
word-level and phrase-level (2D) alignment encoding to capture the lyric-melody
alignment; 2) we design a multi-task pre-training framework with hierarchical
blank infilling objectives (n-gram, phrase, and long span), and incorporate
lyric-melody relationships into the extraction of harmonized n-grams to ensure
the lyric-melody harmony. We also construct a large-scale lyric-melody paired
dataset comprising over 200,000 English song pieces for pre-training and
fine-tuning. The objective and subjective results indicate that SongGLM can
generate melodies from lyrics with significant improvements in both alignment
and harmony, outperforming all the previous baseline methods.

摘要：歌詞轉旋律的生成旨在根據給定的歌詞自動建立旋律，需要捕捉歌詞之間複雜且微妙的關聯性。然而，先前的作品通常面臨兩個主要挑戰：1) 歌詞旋律對齊建模，通常簡化為一音節/字對一音符對齊，而其他則有對齊準確度低的問題；2) 歌詞旋律和諧建模，通常嚴重依賴中間體或嚴格規則，限制了模型的能力和生成的多樣性。在本文中，我們提出了 SongGLM，一個歌詞轉旋律生成系統，利用 2D 對齊編碼和基於通用語言模型 (GLM) 的多任務預訓練來保證歌詞和旋律之間的對齊和和諧。具體來說，1) 我們引入了一個統一的符號歌曲表示，用於歌詞和旋律，並使用字級和短語級 (2D) 對齊編碼來捕捉歌詞旋律對齊；2) 我們設計了一個多任務預訓練框架，具有分層空白填充目標（n-gram、短語和長跨度），並將歌詞旋律關係納入和諧 n-gram 的提取中，以確保歌詞旋律和諧。我們還構建了一個包含超過 200,000 首英文歌曲的大規模歌詞旋律配對數據集，用於預訓練和微調。客觀和主觀結果表明，SongGLM 可以從歌詞中生成旋律，在對齊和和諧方面都有顯著改進，優於所有先前的基準方法。

##### **Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels**
2412.18106v1 by Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long

Meeting growing demands for low latency and cost efficiency in
production-grade large language model (LLM) serving systems requires
integrating advanced optimization techniques. However, dynamic and
unpredictable input-output lengths of LLM, compounded by these optimizations,
exacerbate the issues of workload variability, making it difficult to maintain
high efficiency on AI accelerators, especially DSAs with tile-based programming
models. To address this challenge, we introduce XY-Serve, a versatile, Ascend
native, end-to-end production LLM-serving system. The core idea is an
abstraction mechanism that smooths out the workload variability by decomposing
computations into unified, hardware-friendly, fine-grained meta primitives. For
attention, we propose a meta-kernel that computes the basic pattern of
matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we
introduce a virtual padding scheme that adapts to dynamic shape changes while
using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve
sits harmoniously with vLLM. Experimental results show up to 89% end-to-end
throughput improvement compared with current publicly available baselines on
Ascend NPUs. Additionally, our approach outperforms existing GEMM (average
14.6% faster) and attention (average 21.5% faster) kernels relative to existing
libraries. While the work is Ascend native, we believe the approach can be
readily applicable to SIMT architectures as well.

摘要：<paragraph>为了满足生产级大语言模型 (LLM) 服务系统对低延迟和成本效率不断增长的需求，需要集成先进的优化技术。然而，LLM 动态且不可预测的输入输出长度，加上这些优化，加剧了工作负载可变性的问题，这使得难以在 AI 加速器上保持高效率，尤其是具有基于图块的编程模型的 DSA。为了应对这一挑战，我们引入了 XY-Serve，这是一个多功能的、Ascend 原生的端到端生产 LLM 服务系统。核心思想是一种抽象机制，它通过将计算分解为统一的、硬件友好的、细粒度的元基元来平滑工作负载的可变性。对于注意力，我们提出了一个元内核，它计算具有架构感知图块大小的 matmul-softmax-matmul 的基本模式。对于 GEMM，我们引入了一种虚拟填充方案，它可以适应动态形状变化，同时使用具有各种固定图块大小的高效 GEMM 基元。XY-Serve 与 vLLM 完美契合。实验结果表明，与 Ascend NPU 上当前公开可用的基线相比，端到端吞吐量提高了 89%。此外，我们的方法优于现有的 GEMM（平均快 14.6%）和注意力（平均快 21.5%）内核，相对于现有的库而言。虽然这项工作是 Ascend 原生的，但我们相信该方法也可以很容易地应用于 SIMT 架构。</paragraph>

##### **EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent**
2412.18100v1 by Suyuan Wang, Xueqian Yin, Menghao Wang, Ruofeng Guo, Kai Nan

The rapid growth of scientific techniques and knowledge is reflected in the
exponential increase in new patents filed annually. While these patents drive
innovation, they also present significant burden for researchers and engineers,
especially newcomers. To avoid the tedious work of navigating a vast and
complex landscape to identify trends and breakthroughs, researchers urgently
need efficient tools to summarize, evaluate, and contextualize patents,
revealing their innovative contributions and underlying scientific
principles.To address this need, we present EvoPat, a multi-LLM-based patent
agent designed to assist users in analyzing patents through Retrieval-Augmented
Generation (RAG) and advanced search strategies. EvoPat leverages multiple
Large Language Models (LLMs), each performing specialized roles such as
planning, identifying innovations, and conducting comparative evaluations. The
system integrates data from local databases, including patents, literature,
product catalogous, and company repositories, and online searches to provide
up-to-date insights. The ability to collect information not included in
original database automatically is also implemented. Through extensive testing
in the natural language processing (NLP) domain, we demonstrate that EvoPat
outperforms GPT-4 in tasks such as patent summarization, comparative analysis,
and technical evaluation. EvoPat represents a significant step toward creating
AI-powered tools that empower researchers and engineers to efficiently navigate
the complexities of the patent landscape.

摘要：科學技術和知識的快速增長反映在每年申請的新專利呈指數級增長中。雖然這些專利推動了創新，但它們也給研究人員和工程師帶來了重大負擔，特別是新人。為了避免在廣闊而複雜的領域中尋找趨勢和突破的繁瑣工作，研究人員迫切需要高效的工具來總結、評估和語境化專利，揭示它們的創新貢獻和基礎科學原理。為了滿足這一需求，我們提出了 EvoPat，這是一種基於多 LLM 的專利代理，旨在通過檢索增強生成 (RAG) 和先進的搜索策略幫助用戶分析專利。EvoPat 利用多個大型語言模型 (LLM)，每個模型執行專業角色，例如規劃、識別創新和進行比較評估。該系統整合了來自本地數據庫的數據，包括專利、文獻、產品目錄和公司存儲庫，以及在線搜索，以提供最新的見解。還實現了自動收集原始數據庫中未包含的信息的能力。通過在自然語言處理 (NLP) 領域進行廣泛的測試，我們證明 EvoPat 在專利摘要、比較分析和技術評估等任務中優於 GPT-4。EvoPat 代表著邁向創造 AI 驅動工具的重要一步，這些工具使研究人員和工程師能夠有效應對專利領域的複雜性。

##### **An Attention-based Framework with Multistation Information for Earthquake Early Warnings**
2412.18099v1 by Yu-Ming Huang, Kuan-Yu Chen, Wen-Wei Lin, Da-Yi Chen

Earthquake early warning systems play crucial roles in reducing the risk of
seismic disasters. Previously, the dominant modeling system was the
single-station models. Such models digest signal data received at a given
station and predict earth-quake parameters, such as the p-phase arrival time,
intensity, and magnitude at that location. Various methods have demonstrated
adequate performance. However, most of these methods present the challenges of
the difficulty of speeding up the alarm time, providing early warning for
distant areas, and considering global information to enhance performance.
Recently, deep learning has significantly impacted many fields, including
seismology. Thus, this paper proposes a deep learning-based framework, called
SENSE, for the intensity prediction task of earthquake early warning systems.
To explicitly consider global information from a regional or national
perspective, the input to SENSE comprises statistics from a set of stations in
a given region or country. The SENSE model is designed to learn the
relationships among the set of input stations and the locality-specific
characteristics of each station. Thus, SENSE is not only expected to provide
more reliable forecasts by considering multistation data but also has the
ability to provide early warnings to distant areas that have not yet received
signals. This study conducted extensive experiments on datasets from Taiwan and
Japan. The results revealed that SENSE can deliver competitive or even better
performances compared with other state-of-the-art methods.

摘要：<paragraph>地震預警系統在降低地震災害風險中扮演著至關重要的角色。過去，最主要的建模系統是單站模型。此類模型會消化特定站接收到的訊號資料，並預測地震參數，例如該位置的 P 波到達時間、強度和規模。各種方法都已展現出足夠的效能。然而，這些方法大多面臨難以縮短警報時間、提供遠地區的預警以及考量全球資訊以提升效能的挑戰。近年來，深度學習已對許多領域產生重大影響，包括地震學。因此，本文提出一個名為 SENSE 的基於深度學習的架構，用於地震預警系統的強度預測任務。為了明確地考量區域或國家觀點的全球資訊，SENSE 的輸入包含特定區域或國家一組站的統計資料。SENSE 模型旨在學習輸入站組與每個站的特定位置特徵之間的關係。因此，預期 SENSE 不僅能透過考量多站資料提供更可靠的預測，還能提供尚未收到訊號的遠地區預警。本研究對來自臺灣和日本的資料集進行廣泛的實驗。結果顯示，與其他最先進的方法相比，SENSE 能提供具有競爭力甚至更好的效能。</paragraph>

##### **LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting**
2412.18097v1 by Nan Yang, Chong Wang, Meihua Zhao, Zimeng Zhao, Huiling Zheng, Bin Zhang, Jianing Wang, Xiaofeng Li

Ocean forecasting is crucial for both scientific research and societal
benefits. Currently, the most accurate forecasting systems are global ocean
forecasting systems (GOFSs), which represent the ocean state variables (OSVs)
as discrete grids and solve partial differential equations (PDEs) governing the
transitions of oceanic state variables using numerical methods. However, GOFSs
processes are computationally expensive and prone to cumulative errors.
Recently, large artificial intelligence (AI)-based models significantly boosted
forecasting speed and accuracy. Unfortunately, building a large AI ocean
forecasting system that can be considered cross-spatiotemporal and air-sea
coupled forecasts remains a significant challenge. Here, we introduce LangYa, a
cross-spatiotemporal and air-sea coupled ocean forecasting system. Results
demonstrate that the time embedding module in LangYa enables a single model to
make forecasts with lead times ranging from 1 to 7 days. The air-sea coupled
module effectively simulates air-sea interactions. The ocean self-attention
module improves network stability and accelerates convergence during training,
and the adaptive thermocline loss function improves the accuracy of thermocline
forecasting. Compared to existing numerical and AI-based ocean forecasting
systems, LangYa uses 27 years of global ocean data from the Global Ocean
Reanalysis and Simulation version 12 (GLORYS12) for training and achieves more
reliable deterministic forecasting results for OSVs. LangYa forecasting system
provides global ocean researchers with access to a powerful software tool for
accurate ocean forecasting and opens a new paradigm for ocean science.

摘要：海洋預報對於科學研究和社會效益至關重要。目前最準確的預報系統是全球海洋預報系統 (GOFS)，它將海洋狀態變數 (OSV) 表示為離散網格，並使用數值方法求解控制海洋狀態變數轉換的偏微分方程式 (PDE)。然而，GOFS 的程序在運算上很昂貴，而且容易產生累積誤差。最近，大型人工智慧 (AI) 模型大幅提升了預報速度和準確度。不幸的是，建立一個大型 AI 海洋預報系統（可以被視為跨時空耦合的空海預報）仍然是一項重大的挑戰。在此，我們介紹 LangYa，一個跨時空耦合的空海預報系統。結果證明，LangYa 中的時間嵌入模組讓單一模型能夠進行預報，領先時間從 1 到 7 天不等。空海耦合模組有效地模擬了空海交互作用。海洋自注意力模組改善了網路穩定性，並在訓練期間加速收斂，而自適應溫躍層損失函數則改善了溫躍層預報的準確度。與現有的數值和 AI 海洋預報系統相比，LangYa 使用了全球海洋再分析和模擬第 12 版 (GLORYS12) 中 27 年的全球海洋資料進行訓練，並為 OSV 達到了更可靠的確定性預報結果。LangYa 預報系統為全球海洋研究人員提供了強大的軟體工具，可以用於精確的海洋預報，並為海洋科學開啟了一個新的典範。

##### **Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH) -- a Large Language Model Chatbot for Perioperative Medicine**
2412.18096v1 by Yu He Ke, Liyuan Jin, Kabilan Elangovan, Bryan Wen Xi Ong, Chin Yang Oh, Jacqueline Sim, Kenny Wei-Tsen Loh, Chai Rick Soh, Jonathan Ming Hua Cheng, Aaron Kwang Yang Lee, Daniel Shu Wei Ting, Nan Liu, Hairil Rizal Abdullah

Large Language Models (LLMs) are emerging as powerful tools in healthcare,
particularly for complex, domain-specific tasks. This study describes the
development and evaluation of the PErioperative AI CHatbot (PEACH), a secure
LLM-based system integrated with local perioperative guidelines to support
preoperative clinical decision-making. PEACH was embedded with 35 institutional
perioperative protocols in the secure Claude 3.5 Sonet LLM framework within
Pair Chat (developed by Singapore Government) and tested in a silent deployment
with real-world data. Accuracy, safety, and usability were assessed. Deviations
and hallucinations were categorized based on potential harm, and user feedback
was evaluated using the Technology Acceptance Model (TAM). Updates were made
after the initial silent deployment to amend one protocol.
  In 240 real-world clinical iterations, PEACH achieved a first-generation
accuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across
three iterations. The updated PEACH demonstrated improved accuracy of 97.9%
(235/240), with a statistically significant difference from the null hypothesis
of 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and
deviations were observed (both 1/240 and 2/240, respectively). Clinicians
reported that PEACH expedited decisions in 95% of cases, and inter-rater
reliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among
attendings.
  PEACH is an accurate, adaptable tool that enhances consistency and efficiency
in perioperative decision-making. Future research should explore its
scalability across specialties and its impact on clinical outcomes.

摘要：大型語言模型 (LLM) 正成為醫療保健領域強大的工具，特別適用於複雜的特定領域任務。本研究描述了圍手術期 AI 聊天機器人 (PEACH) 的開發和評估，這是一個安全的 LLM 基礎系統，與本地的圍手術期準則整合，以支援術前臨床決策制定。PEACH 嵌入 35 個機構圍手術期協定，在新加坡政府開發的 Pair Chat 中，採用安全的 Claude 3.5 Sonet LLM 架構，並在靜默部署中使用真實世界資料進行測試。評估了準確性、安全性及可用性。偏差和幻覺依潛在危害進行分類，並使用技術接受模型 (TAM) 評估使用者回饋。在最初的靜默部署後，進行更新以修正一個協定。
  在 240 個真實世界的臨床迭代中，PEACH 在三個迭代中取得 97.5% (78/80) 的第一代準確性，以及 96.7% (232/240) 的整體準確性。更新後的 PEACH 展示出 97.9% (235/240) 的準確性提升，與 95% 準確性的空假設有統計上的顯著差異 (p = 0.018，95% CI：0.952-0.991)。觀察到最小的幻覺和偏差 (分別為 1/240 和 2/240)。臨床醫生回報 PEACH 在 95% 的案例中加速了決策，而評分者間信度在 PEACH 內介於 kappa 0.772-0.893，在主治醫師之間介於 0.610-0.784。
  PEACH 是一個準確且適應性強的工具，可增進圍手術期決策制定的一致性和效率。未來的研究應探索其跨專業的可擴展性，以及其對臨床結果的影響。

##### **Molly: Making Large Language Model Agents Solve Python Problem More Logically**
2412.18093v1 by Rui Xiao, Jiong Wang, Lu Han, Na Zong, Han Wu

Applying large language models (LLMs) as teaching assists has attracted much
attention as an integral part of intelligent education, particularly in
computing courses. To reduce the gap between the LLMs and the computer
programming education expert, fine-tuning and retrieval augmented generation
(RAG) are the two mainstream methods in existing researches. However,
fine-tuning for specific tasks is resource-intensive and may diminish the
model`s generalization capabilities. RAG can perform well on reducing the
illusion of LLMs, but the generation of irrelevant factual content during
reasoning can cause significant confusion for learners. To address these
problems, we introduce the Molly agent, focusing on solving the proposed
problem encountered by learners when learning Python programming language. Our
agent automatically parse the learners' questioning intent through a
scenario-based interaction, enabling precise retrieval of relevant documents
from the constructed knowledge base. At generation stage, the agent reflect on
the generated responses to ensure that they not only align with factual content
but also effectively answer the user's queries. Extensive experimentation on a
constructed Chinese Python QA dataset shows the effectiveness of the Molly
agent, indicating an enhancement in its performance for providing useful
responses to Python questions.

摘要：將大型語言模型 (LLM) 作為教學助理已廣受關注，視為智慧教育不可或缺的一部分，特別是在運算課程中。為了縮小 LLM 與電腦程式設計教育專家之間的差距，微調和檢索擴增生成 (RAG) 是現有研究中的兩種主流方法。然而，針對特定任務進行微調需要大量資源，且可能會降低模型的概括能力。RAG 能有效降低 LLM 的錯覺，但推理過程中產生無關的事實內容可能會對學習者造成顯著的混淆。為了解決這些問題，我們引入了 Molly 代理，專注於解決學習者在學習 Python 程式語言時遇到的提議問題。我們的代理透過基於情境的互動自動解析學習者的提問意圖，從建構的知識庫中精確檢索相關文件。在生成階段，代理會反思產生的回應，以確保它們不僅與事實內容一致，還能有效回答使用者的查詢。在建構的中文 Python 問答資料集上進行廣泛的實驗，顯示了 Molly 代理的有效性，表明其在提供對 Python 問題有用的回應方面的效能有所提升。

##### **BRIDGE: Bundle Recommendation via Instruction-Driven Generation**
2412.18092v1 by Tuan-Nghia Bui, Huy-Son Nguyen, Cam-Van Nguyen Thi, Hoang-Quynh Le, Duc-Trong Le

Bundle recommendation aims to suggest a set of interconnected items to users.
However, diverse interaction types and sparse interaction matrices often pose
challenges for previous approaches in accurately predicting user-bundle
adoptions. Inspired by the distant supervision strategy and generative
paradigm, we propose BRIDGE, a novel framework for bundle recommendation. It
consists of two main components namely the correlation-based item clustering
and the pseudo bundle generation modules. Inspired by the distant supervision
approach, the former is to generate more auxiliary information, e.g.,
instructive item clusters, for training without using external data. This
information is subsequently aggregated with collaborative signals from user
historical interactions to create pseudo `ideal' bundles. This capability
allows BRIDGE to explore all aspects of bundles, rather than being limited to
existing real-world bundles. It effectively bridging the gap between user
imagination and predefined bundles, hence improving the bundle recommendation
performance. Experimental results validate the superiority of our models over
state-of-the-art ranking-based methods across five benchmark datasets.

摘要：組合推薦旨在向使用者建議一組相互關聯的項目。
然而，不同的互動類型和稀疏的互動矩陣經常對先前在準確預測使用者組合採用的方法構成挑戰。受遠程監督策略和生成範例的啟發，我們提出一個新的組合推薦框架 BRIDGE。它包含兩個主要組成部分，分別是基於相關性的項目分群和偽組合生成模組。前者受遠程監督方法的啟發，目的是在不使用外部資料的情況下產生更多輔助資訊，例如具有指導意義的項目分群，以進行訓練。此資訊隨後與來自使用者歷史互動的協作訊號彙總在一起，以建立偽的「理想」組合。此功能讓 BRIDGE 能夠探索組合的所有面向，而不僅限於現有的真實世界組合。它有效地彌合了使用者想像力和預先定義的組合之間的差距，從而提升了組合推薦的效能。實驗結果驗證了我們的模型優於五個基準資料集中的最新排名方法。

##### **AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning**
2412.18091v1 by Lixian Jing, Jianpeng Qi, Junyu Dong, Yanwei Yu

As deep neural networks (DNNs) are increasingly deployed on edge devices,
optimizing models for constrained computational resources is critical. Existing
auto-pruning methods face challenges due to the diversity of DNN models,
various operators (e.g., filters), and the difficulty in balancing pruning
granularity with model accuracy. To address these limitations, we introduce
AutoSculpt, a pattern-based automated pruning framework designed to enhance
efficiency and accuracy by leveraging graph learning and deep reinforcement
learning (DRL). AutoSculpt automatically identifies and prunes regular patterns
within DNN architectures that can be recognized by existing inference engines,
enabling runtime acceleration. Three key steps in AutoSculpt include: (1)
Constructing DNNs as graphs to encode their topology and parameter
dependencies, (2) embedding computationally efficient pruning patterns, and (3)
utilizing DRL to iteratively refine auto-pruning strategies until the optimal
balance between compression and accuracy is achieved. Experimental results
demonstrate the effectiveness of AutoSculpt across various architectures,
including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning
rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming
all baselines. The codes can be available at
https://anonymous.4open.science/r/AutoSculpt-DDA0

摘要：随着深度神经网络 (DNN) 越来越多地部署在边缘设备上，针对受限计算资源优化模型至关重要。现有的自动剪枝方法由于 DNN 模型的多样性、各种运算符（例如过滤器）以及平衡剪枝粒度与模型准确性的难度而面临挑战。为了解决这些限制，我们引入了 AutoSculpt，这是一个基于模式的自动剪枝框架，旨在通过利用图学习和深度强化学习 (DRL) 来提高效率和准确性。AutoSculpt 自动识别和剪枝 DNN 架构中的规则模式，这些模式可以被现有的推理引擎识别，从而实现运行时加速。AutoSculpt 中的三个关键步骤包括：(1) 将 DNN 构建为图以编码其拓扑和参数依赖关系，(2) 嵌入计算高效的剪枝模式，以及 (3) 利用 DRL 迭代优化自动剪枝策略，直到实现压缩和准确性之间的最佳平衡。实验结果证明了 AutoSculpt 在各种架构（包括 ResNet、MobileNet、VGG 和 Vision Transformer）中的有效性，实现了高达 90% 的剪枝率和近 18% 的 FLOP 减少改进，优于所有基线。代码可在 https://anonymous.4open.science/r/AutoSculpt-DDA0 获得

##### **Multi-Point Positional Insertion Tuning for Small Object Detection**
2412.18090v1 by Kanoko Goto, Takumi Karasawa, Takumi Hirose, Rei Kawakami, Nakamasa Inoue

Small object detection aims to localize and classify small objects within
images. With recent advances in large-scale vision-language pretraining,
finetuning pretrained object detection models has emerged as a promising
approach. However, finetuning large models is computationally and memory
expensive. To address this issue, this paper introduces multi-point positional
insertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for
small object detection. Specifically, MPI incorporates multiple positional
embeddings into a frozen pretrained model, enabling the efficient detection of
small objects by providing precise positional information to latent features.
Through experiments, we demonstrated the effectiveness of the proposed method
on the SODA-D dataset. MPI performed comparably to conventional PEFT methods,
including CoOp and VPT, while significantly reducing the number of parameters
that need to be tuned.

摘要：小型物件偵測旨在定位和分類影像中的小型物件。隨著大規模視覺語言預訓練的最新進展，微調預訓練物件偵測模型已成為一種有前途的方法。然而，微調大型模型在運算和記憶體方面成本很高。為了解決這個問題，本文介紹多點位置插入 (MPI) 調整，這是一種針對小型物件偵測的參數有效微調 (PEFT) 方法。具體來說，MPI 將多個位置嵌入整合到凍結的預訓練模型中，透過提供精準的位置資訊給潛在特徵，進而有效偵測小型物件。透過實驗，我們在 SODA-D 資料集上證明了所提出方法的有效性。MPI 的表現與傳統的 PEFT 方法相當，包括 CoOp 和 VPT，同時大幅減少需要調整的參數量。

##### **Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner**
2412.18086v1 by Aizierjiang Aiersilan

Motion planning is a crucial component in autonomous driving.
State-of-the-art motion planners are trained on meticulously curated datasets,
which are not only expensive to annotate but also insufficient in capturing
rarely seen critical scenarios. Failing to account for such scenarios poses a
significant risk to motion planners and may lead to incidents during testing.
An intuitive solution is to manually compose such scenarios by programming and
executing a simulator (e.g., CARLA). However, this approach incurs substantial
human costs. Motivated by this, we propose an inexpensive method for generating
diverse critical traffic scenarios to train more robust motion planners. First,
we represent traffic scenarios as scripts, which are then used by the simulator
to generate traffic scenarios. Next, we develop a method that accepts
user-specified text descriptions, which a Large Language Model (LLM) translates
into scripts using in-context learning. The output scripts are sent to the
simulator that produces the corresponding traffic scenarios. As our method can
generate abundant safety-critical traffic scenarios, we use them as synthetic
training data for motion planners. To demonstrate the value of generated
scenarios, we train existing motion planners on our synthetic data, real-world
datasets, and a combination of both. Our experiments show that motion planners
trained with our data significantly outperform those trained solely on
real-world data, showing the usefulness of our synthetic data and the
effectiveness of our data generation method. Our source code is available at
https://ezharjan.github.io/AutoSceneGen.

摘要：自動駕駛中，路徑規劃是一項關鍵的組成部分。
最先進的路徑規劃器會在經過精心策劃的資料集上進行訓練，
這些資料集的註解不僅昂貴，而且不足以捕捉
罕見的關鍵場景。無法考量這些場景會對路徑規劃器構成
重大風險，並可能在測試期間導致事故。
一個直覺的解決方案是透過程式設計和
執行模擬器（例如 CARLA）來手動撰寫這些場景。然而，這種方法會產生大量的
人力成本。有鑑於此，我們提出了一種低成本的方法來產生
各種關鍵交通場景，以訓練更強大的路徑規劃器。首先，
我們將交通場景表示為腳本，然後由模擬器
用來產生交通場景。接下來，我們開發了一種方法，接受
使用者指定的文字描述，大型語言模型 (LLM) 會使用
脈絡學習將其轉換為腳本。產出的腳本會傳送給
模擬器，模擬器會產生對應的交通場景。由於我們的
方法可以產生大量的安全關鍵交通場景，我們將其用作路徑規劃器的合成
訓練資料。為了證明生成場景的價值，我們使用我們的合成資料、真實世界
資料集以及兩者的組合，訓練現有的路徑規劃器。我們的實驗顯示，使用我們的資料訓練的路徑規劃器
明顯優於僅使用
真實世界資料訓練的路徑規劃器，這顯示了我們合成資料的用途以及
我們資料生成方法的有效性。我們的原始程式碼可在
https://ezharjan.github.io/AutoSceneGen 取得。

##### **Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models**
2412.18084v1 by Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu

Large language models (LLMs) are widely applied in various natural language
processing tasks such as question answering and machine translation. However,
due to the lack of labeled data and the difficulty of manual annotation for
biochemical properties, the performance for molecule generation tasks is still
limited, especially for tasks involving multi-properties constraints. In this
work, we present a two-step framework PEIT (Property Enhanced Instruction
Tuning) to improve LLMs for molecular-related tasks. In the first step, we use
textual descriptions, SMILES, and biochemical properties as multimodal inputs
to pre-train a model called PEIT-GEN, by aligning multi-modal representations
to synthesize instruction data. In the second step, we fine-tune existing
open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle
molecule captioning, text-based molecule generation, molecular property
prediction, and our newly proposed multi-constraint molecule generation tasks.
Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and
BioT5 in molecule captioning, demonstrating modalities align well between
textual descriptions, structures, and biochemical properties. Furthermore,
PEIT-LLM shows promising improvements in multi-task molecule generation,
proving the scalability of the PEIT framework for various molecular tasks. We
release the code, constructed instruction data, and model checkpoints in
https://github.com/chenlong164/PEIT.

摘要：大型語言模型 (LLM) 廣泛應用於各種自然語言處理任務，例如問答和機器翻譯。然而，由於缺乏標記數據和人工標註生化特性的難度，分子生成任務的性能仍然有限，特別是對於涉及多重屬性約束的任務。在這項工作中，我們提出了一個兩步框架 PEIT（屬性增強指令調整），以改進 LLM 以進行與分子相關的任務。在第一步中，我們使用文本描述、SMILES 和生化特性作為多模態輸入，通過對齊多模態表示來預訓練一個名為 PEIT-GEN 的模型，以合成指令數據。在第二步中，我們使用合成的數據對現有的開源 LLM 進行微調，生成的 PEIT-LLM 可以處理分子標題、基於文本的分子生成、分子屬性預測以及我們新提出的多約束分子生成任務。實驗結果表明，我們預先訓練的 PEIT-GEN 在分子標題中優於 MolT5 和 BioT5，證明了文本描述、結構和生化特性之間的模態很好地對齊。此外，PEIT-LLM 在多任務分子生成中顯示出有希望的改進，證明了 PEIT 框架對各種分子任務的可擴展性。我們在 https://github.com/chenlong164/PEIT 中發布了代碼、構建的指令數據和模型檢查點。

##### **Prompt Tuning for Item Cold-start Recommendation**
2412.18082v1 by Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian

The item cold-start problem is crucial for online recommender systems, as the
success of the cold-start phase determines whether items can transition into
popular ones. Prompt learning, a powerful technique used in natural language
processing (NLP) to address zero- or few-shot problems, has been adapted for
recommender systems to tackle similar challenges. However, existing methods
typically rely on content-based properties or text descriptions for prompting,
which we argue may be suboptimal for cold-start recommendations due to 1)
semantic gaps with recommender tasks, 2) model bias caused by warm-up items
contribute most of the positive feedback to the model, which is the core of the
cold-start problem that hinders the recommender quality on cold-start items. We
propose to leverage high-value positive feedback, termed pinnacle feedback as
prompt information, to simultaneously resolve the above two problems. We
experimentally prove that compared to the content description proposed in
existing works, the positive feedback is more suitable to serve as prompt
information by bridging the semantic gaps. Besides, we propose item-wise
personalized prompt networks to encode pinnaclce feedback to relieve the model
bias by the positive feedback dominance problem. Extensive experiments on four
real-world datasets demonstrate the superiority of our model over
state-of-the-art methods. Moreover, PROMO has been successfully deployed on a
popular short-video sharing platform, a billion-user scale commercial
short-video application, achieving remarkable performance gains across various
commercial metrics within cold-start scenarios

摘要：物品冷啟動問題對於線上推薦系統至關重要，因為冷啟動階段的成功與否決定了物品是否能轉變為熱門物品。提示學習是一種用於自然語言處理 (NLP) 的強大技術，用於解決零次或少次嘗試問題，已經改編用於推薦系統來應對類似的挑戰。然而，現有方法通常依賴於基於內容的屬性或文字描述來提示，我們認為這對於冷啟動推薦來說可能不是最佳的，原因有 1) 與推薦任務的語義差距，2) 由於暖身物品造成模型偏差，對模型造成大部分正向回饋，這是冷啟動問題的核心，會阻礙推薦系統對冷啟動物品的推薦品質。我們提議利用高價值正向回饋，稱為頂尖回饋，作為提示資訊，同時解決上述兩個問題。我們透過實驗證明，與現有作品中提出的內容描述相比，正向回饋更適合作為提示資訊，因為它能彌合語義差距。此外，我們提出逐一物品個人化提示網路，對頂尖回饋進行編碼，以透過正向回饋主導問題來緩解模型偏差。在四個真實世界資料集上的大量實驗證明了我們的模型優於最先進的方法。此外，PROMO 已成功部署在一個熱門的短影音分享平台上，一個擁有十億使用者的商業短影音應用程式，在各種冷啟動情境下的商業指標中都獲得了顯著的效能提升

##### **COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection**
2412.18076v1 by Chang Liu, Xin Ma, Xiaochen Yang, Yuxiang Zhang, Yanni Dong

Single-modal object detection tasks often experience performance degradation
when encountering diverse scenarios. In contrast, multimodal object detection
tasks can offer more comprehensive information about object features by
integrating data from various modalities. Current multimodal object detection
methods generally use various fusion techniques, including conventional neural
networks and transformer-based models, to implement feature fusion strategies
and achieve complementary information. However, since multimodal images are
captured by different sensors, there are often misalignments between them,
making direct matching challenging. This misalignment hinders the ability to
establish strong correlations for the same object across different modalities.
In this paper, we propose a novel approach called the CrOss-Mamba interaction
and Offset-guided fusion (COMO) framework for multimodal object detection
tasks. The COMO framework employs the cross-mamba technique to formulate
feature interaction equations, enabling multimodal serialized state
computation. This results in interactive fusion outputs while reducing
computational overhead and improving efficiency. Additionally, COMO leverages
high-level features, which are less affected by misalignment, to facilitate
interaction and transfer complementary information between modalities,
addressing the positional offset challenges caused by variations in camera
angles and capture times. Furthermore, COMO incorporates a global and local
scanning mechanism in the cross-mamba module to capture features with local
correlation, particularly in remote sensing images. To preserve low-level
features, the offset-guided fusion mechanism ensures effective multiscale
feature utilization, allowing the construction of a multiscale fusion data cube
that enhances detection performance.

摘要：單模態目標偵測任務在遇到不同的場景時，經常會遇到效能下降的問題。相比之下，多模態目標偵測任務可以透過整合來自不同模態的資料，提供更全面的物體特徵資訊。目前的模態目標偵測方法通常會使用各種融合技術，包括傳統的神經網路和基於 transformer 的模型，來實作特徵融合策略和達成互補資訊。然而，由於多模態影像是由不同的感測器所擷取，因此它們之間經常會有未對齊的情況，使得直接比對具有挑戰性。這種未對齊會阻礙在不同模態中為同一個物體建立強關聯的能力。在本文中，我們提出了一個名為 CrOss-Mamba 交互和位移引導融合 (COMO) 架構的新方法，用於多模態目標偵測任務。COMO 架構採用 cross-mamba 技術來制定特徵交互方程式，實現多模態序列化狀態計算。這會產生互動式融合輸出，同時減少運算負擔並提高效率。此外，COMO 利用較不受未對齊影響的高階特徵，來促進交互並在模態之間傳遞互補資訊，解決因相機角度和擷取時間的變化而造成的定位偏移挑戰。此外，COMO 在 cross-mamba 模組中加入了全域和局部掃描機制，以擷取具有局部相關性的特徵，特別是在遙測影像中。為了保留低階特徵，位移引導融合機制確保有效的多尺度特徵利用，允許建構一個增強偵測效能的多尺度融合資料立方體。

