{"2501.13893": {"publish_time": "2025-01-23", "title": "Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning", "paper_summary": "We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset\ndesigned to advance fine-grained visual understanding. To achieve this, we\ncarefully design an automated annotation pipeline that prompts GPT-4V to\ngenerate pixel-aligned, instance-specific captions for individual objects\nwithin images, enabling models to learn more granular relationships between\nobjects and their contexts. This approach results in 167,254 detailed captions,\nwith an average of 22.94 words per caption. Building on Pix2Cap-COCO, we\nintroduce a novel task, panoptic segmentation-captioning, which challenges\nmodels to recognize instances in an image and provide detailed descriptions for\neach simultaneously. To benchmark this task, we design a robust baseline based\non X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a\nparticularly challenging dataset, as it requires models to excel in both\nfine-grained visual understanding and detailed language generation.\nFurthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large\nmultimodal models (LMMs) to enhance their performance. For example, training\nwith Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding\ngains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset,\nand strengthens its region understanding ability on the ViP-BENCH, with an\noverall improvement of +5.1%, including notable increases in recognition\naccuracy +11.2% and language generation quality +22.2%.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa Pix2Cap-COCO\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5168\u666f\u50cf\u7d20\u7d1a\u6a19\u984c\u8cc7\u6599\u96c6\uff0c\u65e8\u5728\u63a8\u9032\u7cbe\u7d30\u7684\u8996\u89ba\u7406\u89e3\u3002\u70ba\u6b64\uff0c\u6211\u5011\u4ed4\u7d30\u8a2d\u8a08\u4e86\u4e00\u500b\u81ea\u52d5\u5316\u6a19\u8a3b\u7ba1\u9053\uff0c\u63d0\u793a GPT-4V \u70ba\u5f71\u50cf\u4e2d\u7684\u500b\u5225\u7269\u4ef6\u7522\u751f\u50cf\u7d20\u5c0d\u9f4a\u7684\u7279\u5b9a\u5be6\u4f8b\u6a19\u984c\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u5b78\u7fd2\u7269\u4ef6\u53ca\u5176\u8108\u7d61\u4e4b\u9593\u66f4\u7d30\u7dfb\u7684\u95dc\u4fc2\u3002\u6b64\u65b9\u6cd5\u7522\u751f\u4e86 167,254 \u500b\u8a73\u7d30\u6a19\u984c\uff0c\u6bcf\u500b\u6a19\u984c\u5e73\u5747\u6709 22.94 \u500b\u5b57\u3002\u5728 Pix2Cap-COCO \u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u9805\u65b0\u4efb\u52d9\uff0c\u5168\u666f\u5206\u5272\u6a19\u984c\uff0c\u5b83\u6311\u6230\u6a21\u578b\u8b58\u5225\u5f71\u50cf\u4e2d\u7684\u5be6\u4f8b\uff0c\u4e26\u540c\u6642\u70ba\u6bcf\u500b\u5be6\u4f8b\u63d0\u4f9b\u8a73\u7d30\u63cf\u8ff0\u3002\u70ba\u4e86\u5c0d\u6b64\u4efb\u52d9\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u57fa\u65bc X-Decoder \u7684\u7a69\u5065\u57fa\u6e96\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cPix2Cap-COCO \u662f\u4e00\u500b\u7279\u5225\u5177\u6709\u6311\u6230\u6027\u7684\u8cc7\u6599\u96c6\uff0c\u56e0\u70ba\u5b83\u8981\u6c42\u6a21\u578b\u5728\u7cbe\u7d30\u7684\u8996\u89ba\u7406\u89e3\u548c\u8a73\u7d30\u7684\u8a9e\u8a00\u751f\u6210\u65b9\u9762\u90fd\u8868\u73fe\u51fa\u8272\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528 Pix2Cap-COCO \u5c0d\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u9032\u884c\u76e3\u7763\u5fae\u8abf (SFT)\uff0c\u4ee5\u589e\u5f37\u5176\u6548\u80fd\u3002\u4f8b\u5982\uff0c\u4f7f\u7528 Pix2Cap-COCO \u9032\u884c\u8a13\u7df4\u53ef\u986f\u8457\u63d0\u5347 GPT4RoI \u7684\u6548\u80fd\uff0c\u5728 Visual Genome \u8cc7\u6599\u96c6\u4e0a\u7372\u5f97 CIDEr +1.4%\u3001ROUGE +0.4% \u548c SPICE +0.5% \u7684\u63d0\u5347\uff0c\u4e26\u5728 ViP-BENCH \u4e0a\u589e\u5f37\u5176\u5340\u57df\u7406\u89e3\u80fd\u529b\uff0c\u6574\u9ad4\u63d0\u5347 +5.1%\uff0c\u5305\u62ec\u8b58\u5225\u6e96\u78ba\u5ea6 +11.2% \u548c\u8a9e\u8a00\u751f\u6210\u54c1\u8cea +22.2% \u7684\u986f\u8457\u63d0\u5347\u3002</paragraph>", "author": "Zuyao You et.al.", "authors": "Zuyao You, Junke Wang, Lingyu Kong, Bo He, Zuxuan Wu", "id": "2501.13893v1", "paper_url": "http://arxiv.org/abs/2501.13893v1", "repo": "https://github.com/geshang777/pix2cap"}}