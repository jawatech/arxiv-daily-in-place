{"2411.07618": {"publish_time": "2024-11-12", "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "paper_summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\u4ecd\u7136\u662f\u4e00\u500b\u95dc\u9375\u6311\u6230\u3002\u96d6\u7136\u50cf\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u548c\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7b49\u8a13\u7df4\u5f8c\u6280\u8853\u5df2\u7d93\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u6703\u5f15\u5165\u8a08\u7b97\u7121\u6548\u7387\u548c\u8a13\u7df4\u4e0d\u7a69\u5b9a\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u7279\u5fb5\u7d1a\u7d04\u675f\u504f\u597d\u6700\u4f73\u5316 (FPO)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u7c21\u5316\u5c0d\u9f4a\u904e\u7a0b\uff0c\u540c\u6642\u78ba\u4fdd\u7a69\u5b9a\u6027\u3002FPO \u5229\u7528\u9810\u5148\u8a13\u7df4\u7684\u7a00\u758f\u81ea\u7de8\u78bc\u5668 (SAE)\uff0c\u4e26\u5f15\u5165\u7279\u5fb5\u7d1a\u7d04\u675f\uff0c\u5f9e\u800c\u5be6\u73fe\u9ad8\u6548\u3001\u5f37\u5236\u7a00\u758f\u6027\u7684\u5c0d\u9f4a\u3002\u6211\u5011\u7684\u505a\u6cd5\u901a\u904e\u4f7f\u7528\u5728\u8a13\u7df4\u826f\u597d\u7684\u7a00\u758f\u81ea\u7de8\u78bc\u5668\u4e2d\u555f\u7528\u7684\u7a00\u758f\u7279\u5fb5\u548c\u4f7f\u7528\u7279\u5fb5\u7d1a\u96e2\u7dda\u53c3\u8003\u7684\u5e8f\u5217 KL \u6563\u5ea6\u7684\u54c1\u8cea\uff0c\u4f86\u4eab\u53d7\u6548\u7387\u3002\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u57fa\u6e96\u7dda\u76f8\u6bd4\uff0cFPO \u4ee5\u66f4\u4f4e\u7684\u8a08\u7b97\u6210\u672c\u5be6\u73fe\u4e86\u52dd\u7387\u7684 5.08% \u7d55\u5c0d\u6539\u9032\uff0c\u4f7f\u5176\u6210\u70ba LLM \u5c0d\u9f4a\u7684\u6709\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Qingyu Yin et.al.", "authors": "Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang", "id": "2411.07618v1", "paper_url": "http://arxiv.org/abs/2411.07618v1", "repo": "null"}}