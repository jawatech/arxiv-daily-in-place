{"2405.14715": {"publish_time": "2024-05-23", "title": "Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models", "paper_summary": "Modern retrieval systems often struggle with upgrading to new and more\npowerful models due to the incompatibility of embeddings between the old and\nnew models. This necessitates a costly process known as backfilling, which\ninvolves re-computing the embeddings for a large number of data samples. In\nvision, Backward-compatible Training (BT) has been proposed to ensure that the\nnew model aligns with the old model's embeddings. This paper extends the\nconcept of vision-only BT to the field of cross-modal retrieval, marking the\nfirst attempt to address Cross-modal BT (XBT). Our goal is to achieve\nbackward-compatibility between Vision-Language Pretraining (VLP) models, such\nas CLIP, for the cross-modal retrieval task. To address XBT challenges, we\npropose an efficient solution: a projection module that maps the new model's\nembeddings to those of the old model. This module, pretrained solely with text\ndata, significantly reduces the number of image-text pairs required for XBT\nlearning, and, once it is pretrained, it avoids using the old model during\ntraining. Furthermore, we utilize parameter-efficient training strategies that\nimprove efficiency and preserve the off-the-shelf new model's knowledge by\navoiding any modifications. Experimental results on cross-modal retrieval\ndatasets demonstrate the effectiveness of XBT and its potential to enable\nbackfill-free upgrades when a new VLP model emerges.", "paper_summary_zh": "\u73fe\u4ee3\u6aa2\u7d22\u7cfb\u7d71\u5e38\u56e0\u820a\u6a21\u578b\u8207\u65b0\u6a21\u578b\u7684\u5d4c\u5165\u5f0f\u4e0d\u7b26\uff0c\u800c\u96e3\u4ee5\u5347\u7d1a\u81f3\u66f4\u65b0\u4e14\u529f\u80fd\u66f4\u5f37\u5927\u7684\u6a21\u578b\u3002\u9019\u9700\u8981\u4e00\u500b\u540d\u70ba\u56de\u586b\u7684\u6602\u8cb4\u7a0b\u5e8f\uff0c\u5176\u4e2d\u6d89\u53ca\u91cd\u65b0\u8a08\u7b97\u5927\u91cf\u6578\u64da\u6a23\u672c\u7684\u5d4c\u5165\u5f0f\u3002\u5728\u8996\u89ba\u4e0a\uff0c\u5df2\u63d0\u51fa\u5411\u5f8c\u76f8\u5bb9\u8a13\u7df4 (BT) \u4ee5\u78ba\u4fdd\u65b0\u6a21\u578b\u8207\u820a\u6a21\u578b\u7684\u5d4c\u5165\u5f0f\u5c0d\u9f4a\u3002\u672c\u6587\u5c07\u8996\u89ba\u5c08\u7528 BT \u7684\u6982\u5ff5\u5ef6\u4f38\u81f3\u8de8\u6a21\u614b\u6aa2\u7d22\u9818\u57df\uff0c\u6a19\u8a8c\u8457\u9996\u6b21\u5617\u8a66\u89e3\u6c7a\u8de8\u6a21\u614b BT (XBT)\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u9054\u6210\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4 (VLP) \u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u4e4b\u9593\u7684\u5411\u5f8c\u76f8\u5bb9\u6027\uff0c\u4ee5\u9032\u884c\u8de8\u6a21\u614b\u6aa2\u7d22\u4efb\u52d9\u3002\u70ba\u4e86\u89e3\u6c7a XBT \u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff1a\u4e00\u500b\u6295\u5f71\u6a21\u7d44\uff0c\u5c07\u65b0\u6a21\u578b\u7684\u5d4c\u5165\u5f0f\u5c0d\u61c9\u81f3\u820a\u6a21\u578b\u7684\u5d4c\u5165\u5f0f\u3002\u6b64\u6a21\u7d44\u50c5\u4f7f\u7528\u6587\u5b57\u8cc7\u6599\u9810\u5148\u8a13\u7df4\uff0c\u5927\u5e45\u6e1b\u5c11 XBT \u5b78\u7fd2\u6240\u9700\u7684\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u6578\u91cf\uff0c\u4e14\u9810\u5148\u8a13\u7df4\u5f8c\uff0c\u5728\u8a13\u7df4\u671f\u9593\u4fbf\u4e0d\u518d\u4f7f\u7528\u820a\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u53c3\u6578\u6709\u6548\u8a13\u7df4\u7b56\u7565\u4f86\u63d0\u5347\u6548\u7387\u4e26\u4fdd\u7559\u73fe\u6210\u7684\u5168\u65b0\u6a21\u578b\u77e5\u8b58\uff0c\u85c9\u7531\u907f\u514d\u4efb\u4f55\u4fee\u6539\u3002\u8de8\u6a21\u614b\u6aa2\u7d22\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 XBT \u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u7576\u65b0\u7684 VLP \u6a21\u578b\u51fa\u73fe\u6642\uff0c\u5b83\u80fd\u9032\u884c\u7121\u56de\u586b\u5347\u7d1a\u7684\u6f5b\u529b\u3002", "author": "Young Kyun Jang et.al.", "authors": "Young Kyun Jang, Ser-nam Lim", "id": "2405.14715v1", "paper_url": "http://arxiv.org/abs/2405.14715v1", "repo": "null"}}