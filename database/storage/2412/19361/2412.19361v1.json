{"2412.19361": {"publish_time": "2024-12-26", "title": "Dynamic Skill Adaptation for Large Language Models", "paper_summary": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u52d5\u614b\u6280\u80fd\u9069\u61c9 (DSA)\uff0c\u4e00\u7a2e\u9069\u61c9\u6027\u548c\u52d5\u614b\u6846\u67b6\uff0c\u7528\u65bc\u5c07\u65b0\u7a4e\u4e14\u8907\u96dc\u7684\u6280\u80fd\u9069\u61c9\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u8207\u5148\u524d\u5f9e\u4eba\u985e\u7b56\u5283\u548c\u975c\u614b\u8cc7\u6599\u4e2d\u4ee5\u96a8\u6a5f\u9806\u5e8f\u5b78\u7fd2\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u5011\u5efa\u8b70\u9996\u5148\u900f\u904e\u6a21\u64ec\u4eba\u985e\u7684\u5b78\u7fd2\u8def\u5f91\u81ea\u52d5\u7522\u751f\u548c\u7d44\u7e54\u8a13\u7df4\u8cc7\u6599\uff0c\u7136\u5f8c\u6839\u64da\u8a13\u7df4\u52d5\u614b\u52d5\u614b\u8abf\u6574\u8a13\u7df4\u8cc7\u6599\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u53d7\u5230\u4eba\u985e\u6559\u80b2\u7cfb\u7d71\u4e2d\u7684\u5b78\u7fd2\u7d50\u69cb\u548c\u6559\u5b78\u7b56\u7565\u7684\u555f\u767c\uff0c\u6211\u5011\u9996\u5148\u900f\u904e\u5c07\u8907\u96dc\u6280\u80fd\u5206\u89e3\u6210\u5b50\u6280\u80fd\u4e26\u6839\u64da\u5b83\u5011\u5728\u4eba\u985e\u97f3\u7bc0\u4e2d\u7684\u4f9d\u8cf4\u6027\u4f86\u6392\u5217\u5b83\u5011\u4f86\u69cb\u5efa\u6280\u80fd\u5716\u3002\u5c0d\u65bc\u6bcf\u9805\u6280\u80fd\uff0c\u6211\u5011\u5229\u7528 LLM \u7522\u751f\u985e\u4f3c\u6559\u79d1\u66f8\u7684\u8cc7\u6599\uff0c\u5176\u4e2d\u5305\u542b\u6280\u80fd\u7684\u8a73\u7d30\u63cf\u8ff0\uff0c\u7528\u65bc\u9810\u8a13\u7df4\u548c\u7df4\u7fd2\u985e\u578b\u7684\u8cc7\u6599\uff0c\u5176\u76ee\u6a19\u662f\u660e\u78ba\u5229\u7528\u6280\u80fd\u89e3\u6c7a\u554f\u984c\uff0c\u4ee5\u9032\u884c\u6307\u4ee4\u8abf\u6574\u3002\u6b64\u5916\uff0c\u5728\u6307\u4ee4\u8abf\u6574\u671f\u9593\uff0c\u6211\u5011\u6703\u52d5\u614b\u66f4\u65b0\u8a13\u7df4\u8cc7\u6599\uff0c\u5176\u4e2d\u6703\u964d\u4f4e\u6613\u65bc\u5b78\u7fd2\u7bc4\u4f8b\u7684\u6b0a\u91cd\u3001\u7522\u751f\u66f4\u8907\u96dc\u7684\u7bc4\u4f8b\uff0c\u4e26\u904e\u6ffe\u6389\u6709\u932f\u8aa4\u7684\u8cc7\u6599\u3002\u5728 LLAMA \u548c Mistral \u7b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9069\u61c9\u6578\u5b78\u63a8\u7406\u6280\u80fd\u548c\u793e\u6703\u7814\u7a76\u6280\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Jiaao Chen et.al.", "authors": "Jiaao Chen, Diyi Yang", "id": "2412.19361v1", "paper_url": "http://arxiv.org/abs/2412.19361v1", "repo": "null"}}