{"2411.07595": {"publish_time": "2024-11-12", "title": "Entropy Controllable Direct Preference Optimization", "paper_summary": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.", "paper_summary_zh": "\u5728\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5f8c\u8a13\u7df4\u4e2d\uff0c\u4f86\u81ea\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5be6\u73fe\u8207\u4eba\u985e\u504f\u597d\u4e00\u81f4\u7684\u751f\u6210\u3002\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u5141\u8a31\u4f7f\u7528\u7c21\u55ae\u7684\u4e8c\u5143\u4ea4\u53c9\u71b5\u640d\u5931\u9032\u884c\u653f\u7b56\u8a13\u7df4\uff0c\u800c\u7121\u9700\u734e\u52f5\u6a21\u578b\u3002DPO \u7684\u76ee\u6a19\u901a\u904e\u53cd\u5411 KL \u6563\u5ea6\u9032\u884c\u898f\u7bc4\u5316\uff0c\u9019\u9f13\u52f5\u5c0b\u6c42\u6a21\u5f0f\u4ee5\u7b26\u5408\u53c3\u8003\u653f\u7b56\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6211\u5011\u6307\u51fa\uff0c\u6700\u5c0f\u5316\u53cd\u5411 KL \u6563\u5ea6\u53ef\u80fd\u7121\u6cd5\u6355\u6349\u53c3\u8003\u5206\u4f48\u7684\u6a21\u5f0f\uff0c\u9019\u53ef\u80fd\u6703\u640d\u5bb3\u653f\u7b56\u7684\u6548\u80fd\u3002\u57fa\u65bc\u6b64\u89c0\u5bdf\uff0c\u6211\u5011\u5efa\u8b70\u5c0d DPO \u9032\u884c\u4e00\u500b\u7c21\u55ae\u7684\u4fee\u6539\uff0cH-DPO\uff0c\u5b83\u5141\u8a31\u63a7\u5236\u7d50\u679c\u653f\u7b56\u7684\u71b5\uff0c\u589e\u5f37\u5206\u4f48\u7684\u6e05\u6670\u5ea6\uff0c\u5f9e\u800c\u66f4\u6709\u6548\u5730\u5be6\u73fe\u5c0b\u6c42\u6a21\u5f0f\u7684\u64ec\u5408\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8868\u660e H-DPO \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u90fd\u512a\u65bc DPO\uff0c\u5728\u6578\u5b78\u4efb\u52d9\u7684 pass@$k$ \u8a55\u4f30\u4e2d\u5c55\u793a\u4e86\u512a\u7570\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0cH-DPO \u5be6\u73fe\u8d77\u4f86\u5f88\u7c21\u55ae\uff0c\u53ea\u9700\u8981\u5c0d DPO \u7684\u640d\u5931\u8a08\u7b97\u9032\u884c\u5fae\u5c0f\u7684\u4fee\u6539\uff0c\u9019\u4f7f\u5f97\u5b83\u5728 LLM \u8a13\u7df4\u7684\u5ee3\u6cdb\u61c9\u7528\u4e2d\u5177\u6709\u9ad8\u5ea6\u5be6\u7528\u6027\u548c\u524d\u666f\u3002", "author": "Motoki Omura et.al.", "authors": "Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka", "id": "2411.07595v1", "paper_url": "http://arxiv.org/abs/2411.07595v1", "repo": "null"}}