{"2410.01548": {"publish_time": "2024-10-02", "title": "In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks", "paper_summary": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.", "paper_summary_zh": "\u60c5\u5883\u5b78\u7fd2 (ICL) \u662f\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u900f\u904e\u63d0\u4f9b\u76ee\u6a19\u4efb\u52d9\u7684\u793a\u7bc4\uff0c\u5354\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u5404\u7a2e\u4efb\u52d9\u3002\u8003\u91cf\u5230\u6a19\u8a18\u793a\u7bc4\u7684\u9ad8\u6210\u672c\uff0c\u8a31\u591a\u65b9\u6cd5\u5efa\u8b70\u4f7f\u7528 LLM \u5f9e\u982d\u958b\u59cb\u5408\u6210\u793a\u7bc4\u3002\u7136\u800c\uff0c\u5f9e\u982d\u958b\u59cb\u5408\u6210\u7684\u793a\u7bc4\u54c1\u8cea\u53d7\u5230 LLM \u7684\u80fd\u529b\u548c\u77e5\u8b58\u9650\u5236\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5728\u8f49\u79fb\u5b78\u7fd2\u7684\u555f\u767c\u4e0b\uff0c\u63d0\u51fa\u60c5\u5883\u8f49\u79fb\u5b78\u7fd2 (ICTL)\uff0c\u900f\u904e\u5f9e\u985e\u4f3c\u7684\u4f86\u6e90\u4efb\u52d9\u8f49\u79fb\u6a19\u8a18\u793a\u7bc4\uff0c\u4f86\u5408\u6210\u76ee\u6a19\u4efb\u52d9\u793a\u7bc4\u3002ICTL \u5305\u542b\u5169\u500b\u6b65\u9a5f\uff1a\u4f86\u6e90\u53d6\u6a23\u548c\u76ee\u6a19\u8f49\u79fb\u3002\u9996\u5148\uff0c\u6211\u5011\u5b9a\u7fa9\u4e00\u500b\u6700\u4f73\u5316\u76ee\u6a19\uff0c\u5c07\u8f49\u79fb\u8aa4\u5dee\u6700\u5c0f\u5316\uff0c\u4ee5\u53d6\u6a23\u8207\u76ee\u6a19\u4efb\u52d9\u985e\u4f3c\u7684\u4f86\u6e90\u793a\u7bc4\u3002\u63a5\u8457\uff0c\u6211\u5011\u4f7f\u7528 LLM \u5c07\u53d6\u6a23\u7684\u4f86\u6e90\u793a\u7bc4\u8f49\u79fb\u5230\u76ee\u6a19\u4efb\u52d9\uff0c\u4e26\u7b26\u5408\u76ee\u6a19\u4efb\u52d9\u7684\u5b9a\u7fa9\u548c\u683c\u5f0f\u3002Super-NI \u4e0a\u7684\u5be6\u9a57\u986f\u793a\uff0cICTL \u7684\u8868\u73fe\u512a\u65bc\u5f9e\u982d\u958b\u59cb\u7684\u5408\u6210\uff0c\u5e73\u5747\u9ad8\u51fa 2.0%\uff0c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "author": "Dingzirui Wang et.al.", "authors": "Dingzirui Wang, Xuangliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li", "id": "2410.01548v1", "paper_url": "http://arxiv.org/abs/2410.01548v1", "repo": "https://github.com/zirui-HIT/ICTL"}}