{"2408.10631": {"publish_time": "2024-08-20", "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models", "paper_summary": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u898f\u6a21\u5927\u5e45\u6210\u9577\uff0c\u5c0e\u81f4\u5c0d\u6709\u6548\u6a21\u578b\u4fee\u526a\u6280\u8853\u7522\u751f\u4e86\u8feb\u5207\u9700\u6c42\u3002\u73fe\u6709\u7684\u8a13\u7df4\u5f8c\u4fee\u526a\u6280\u8853\u4e3b\u8981\u5c08\u6ce8\u65bc\u6e2c\u91cf\u6536\u6582\u7a20\u5bc6\u6a21\u578b\u4e0a\u7684\u6b0a\u91cd\u91cd\u8981\u6027\uff0c\u4ee5\u78ba\u5b9a\u8981\u4fdd\u7559\u7684\u986f\u8457\u6b0a\u91cd\u3002\u7136\u800c\uff0c\u5b83\u5011\u5e38\u5e38\u5ffd\u7565\u4fee\u526a\u904e\u7a0b\u4e2d\u6b0a\u91cd\u91cd\u8981\u6027\u7684\u8b8a\u5316\uff0c\u9019\u53ef\u80fd\u5c0e\u81f4\u4fee\u526a\u6a21\u578b\u7684\u6548\u80fd\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LLM-Barber\uff08\u4e00\u6b21\u6027\u7684\u7a00\u758f\u906e\u7f69\u5340\u584a\u611f\u77e5\u91cd\u5efa\u5668\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u4e00\u6b21\u6027\u4fee\u526a\u67b6\u69cb\uff0c\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8a13\u7df4\u6216\u6b0a\u91cd\u91cd\u5efa\u7684\u60c5\u6cc1\u4e0b\u91cd\u5efa\u4fee\u526a\u6a21\u578b\u7684\u7a00\u758f\u906e\u7f69\u3002LLM-Barber \u7d50\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u548c MLP \u5340\u584a\u7684\u5340\u584a\u611f\u77e5\u932f\u8aa4\u6700\u4f73\u5316\uff0c\u78ba\u4fdd\u6574\u9ad4\u6548\u80fd\u6700\u4f73\u5316\u3002\u53d7 LLM \u4e2d\u767c\u73fe\u7684\u986f\u8457\u7570\u5e38\u503c\u7684\u555f\u767c\uff0cLLM-Barber \u5f15\u5165\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u4fee\u526a\u6307\u6a19\uff0c\u8a72\u6307\u6a19\u4f7f\u7528\u4e58\u4ee5\u68af\u5ea6\u7684\u6b0a\u91cd\u4f86\u8b58\u5225\u6b0a\u91cd\u91cd\u8981\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cLLM-Barber \u53ef\u4ee5\u6709\u6548\u4fee\u526a LLaMA \u548c OPT \u7cfb\u5217\u7b49\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u55ae\u500b A100 GPU \u4e0a\u6709 7B \u5230 13B \u500b\u53c3\u6578\uff0c\u50c5\u9700 30 \u5206\u9418\uff0c\u5c31\u80fd\u5728\u5404\u7a2e\u8a9e\u8a00\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7372\u5f97\u56f0\u60d1\u5ea6\u548c\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u7684\u6700\u65b0\u7d50\u679c\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/YupengSu/LLM-Barber \u53d6\u5f97\u3002", "author": "Yupeng Su et.al.", "authors": "Yupeng Su, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, Graziano Chesi, Ngai Wong, Hao Yu", "id": "2408.10631v1", "paper_url": "http://arxiv.org/abs/2408.10631v1", "repo": "null"}}