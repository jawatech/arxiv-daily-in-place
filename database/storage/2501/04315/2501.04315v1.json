{"2501.04315": {"publish_time": "2025-01-08", "title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation", "paper_summary": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used\nand effective for fine-tuning, we have observed that its scaling factor can\nlimit or even reduce performance as the rank size increases. To address this\nissue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet\neffective method for optimizing LoRA's scaling factor. By replacing $\\alpha/r$\nwith $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank size\nincreases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA.", "paper_summary_zh": "\u5fae\u8abf\u6709\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6062\u5fa9\u9000\u5316\u7684\u8cc7\u8a0a\u4e26\u63d0\u5347\u4efb\u52d9\u6548\u80fd\u3002\u5118\u7ba1\u4f4e\u968e\u5c64\u6b21\u9069\u61c9 (LoRA) \u5ee3\u6cdb\u7528\u65bc\u5fae\u8abf\u4e14\u6548\u679c\u826f\u597d\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5176\u7e2e\u653e\u56e0\u5b50\u53ef\u80fd\u6703\u96a8\u8457\u968e\u5c64\u6b21\u5927\u5c0f\u589e\u52a0\u800c\u9650\u5236\u6216\u751a\u81f3\u964d\u4f4e\u6548\u80fd\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa RoRA (\u968e\u5c64\u6b21\u9069\u61c9\u53ef\u9760\u6027\u6700\u4f73\u5316)\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc\u6700\u4f73\u5316 LoRA \u7e2e\u653e\u56e0\u5b50\u7684\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\u3002\u900f\u904e\u5c07 $\\alpha/r$ \u66ff\u63db\u70ba $\\alpha/\\sqrt{r}$\uff0cRoRA \u53ef\u78ba\u4fdd\u5728\u968e\u5c64\u6b21\u5927\u5c0f\u589e\u52a0\u6642\u63d0\u5347\u6548\u80fd\u3002\u6b64\u5916\uff0cRoRA \u53ef\u5728\u5fae\u8abf\u672a\u58d3\u7e2e\u6a21\u578b\u6642\u63d0\u5347\u4f4e\u968e\u5c64\u6b21\u9069\u61c9\uff0c\u4e14\u5728\u5fae\u8abf\u4fee\u526a\u6a21\u578b\u6642\u7cbe\u6e96\u5ea6\u5fa9\u539f\u7684\u66f4\u5177\u6311\u6230\u6027\u4efb\u52d9\u4e2d\u8868\u73fe\u512a\u7570\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 RoRA \u5728\u5fae\u8abf\u672a\u58d3\u7e2e\u548c\u4fee\u526a\u6a21\u578b\u6642\u7684\u6709\u6548\u6027\u3002RoRA \u5728 LLaMA-7B/13B\u3001LLaMA2-7B \u548c LLaMA3-8B \u4e0a\u7684\u5e73\u5747\u7cbe\u6e96\u5ea6\u548c\u7a69\u5065\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73fe\u6709\u6280\u8853 (SOTA)\uff0c\u7279\u5225\u662f\u5728 LLaMA-7B \u4e0a\u5206\u5225\u6bd4 LoRA \u548c DoRA \u9ad8\u51fa 6.5% \u548c 2.9%\u3002\u5728\u4fee\u526a\u6a21\u578b\u5fae\u8abf\u4e2d\uff0cRoRA \u986f\u793a\u51fa\u986f\u8457\u7684\u512a\u52e2\uff1b\u5c0d\u65bc\u4fee\u526a\u7387\u70ba 81.4% \u7684 LLaMA-7B\uff0c\u5373 SHEARED-LLAMA-1.3\uff0cRoRA \u7684\u5e73\u5747\u7cbe\u6e96\u5ea6\u6bd4 LoRA \u9ad8\u51fa 5.7%\uff0c\u6bd4 DoRA \u9ad8\u51fa 3.9%\u3002", "author": "Jun Liu et.al.", "authors": "Jun Liu, Zhenglun Kong, Peiyan Dong, Xuan Shen, Pu Zhao, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, Dong Huang, Yanzhi Wang", "id": "2501.04315v1", "paper_url": "http://arxiv.org/abs/2501.04315v1", "repo": "null"}}