{"2502.03275": {"publish_time": "2025-02-05", "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning", "paper_summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u93c8\u5f0f\u601d\u8003 (CoT) \u8cc7\u6599\u4e0a\u8a13\u7df4\u6642\uff0c\u5728\u63a8\u7406\u548c\u898f\u5283\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u5176\u4e2d\u9010\u6b65\u7684\u601d\u8003\u904e\u7a0b\u7531\u6587\u5b57\u7b26\u865f\u660e\u78ba\u6982\u8ff0\u3002\u7136\u800c\uff0c\u9019\u6703\u5c0e\u81f4\u5197\u9577\u7684\u8f38\u5165\uff0c\u5176\u4e2d\u8a31\u591a\u5b57\u8a5e\u652f\u6301\u6587\u5b57\u9023\u8cab\u6027\uff0c\u800c\u4e0d\u662f\u6838\u5fc3\u63a8\u7406\u8cc7\u8a0a\uff0c\u800c\u8655\u7406\u9019\u4e9b\u8f38\u5165\u6703\u6d88\u8017\u5927\u91cf\u7684\u8a08\u7b97\u8cc7\u6e90\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u63a8\u7406\u904e\u7a0b\u7684\u6df7\u5408\u8868\u793a\uff0c\u5176\u4e2d\u6211\u5011\u4f7f\u7528 VQ-VAE \u751f\u6210\u7684\u6f5b\u5728\u96e2\u6563\u7b26\u865f\u90e8\u5206\u62bd\u8c61\u51fa\u6700\u521d\u7684\u63a8\u7406\u6b65\u9a5f\uff0c\u986f\u8457\u6e1b\u5c11\u4e86\u63a8\u7406\u8ecc\u8de1\u7684\u9577\u5ea6\u3002\u6211\u5011\u5728\u5169\u500b\u5834\u666f\u4e2d\u63a2\u8a0e\u4e86\u6f5b\u5728\u8ecc\u8de1\u62bd\u8c61\u7684\u7528\u9014\uff1a1) \u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u6a21\u578b\u4ee5\u89e3\u6c7a\u5c0b\u9470\u8ff7\u5bae\u554f\u984c\uff0c2) \u4f7f\u7528\u5305\u542b\u672a\u898b\u6f5b\u5728\u7b26\u865f\u7684\u64f4\u5145\u8a5e\u5f59\u5fae\u8abf LLM\uff0c\u7528\u65bc\u908f\u8f2f\u548c\u6578\u5b78\u63a8\u7406\u554f\u984c\u3002\u70ba\u4e86\u4fc3\u9032\u6709\u6548\u5b78\u7fd2\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7c21\u55ae\u7684\u8a13\u7df4\u7a0b\u5e8f\uff0c\u96a8\u6a5f\u6df7\u5408\u6f5b\u5728\u7b26\u865f\u548c\u6587\u5b57\u7b26\u865f\uff0c\u9019\u4f7f\u5f97\u80fd\u5920\u5feb\u901f\u9069\u61c9\u65b0\u7684\u6f5b\u5728\u7b26\u865f\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e2d\u59cb\u7d42\u512a\u65bc\u57fa\u6e96\u65b9\u6cd5\u3002", "author": "DiJia Su et.al.", "authors": "DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng", "id": "2502.03275v1", "paper_url": "http://arxiv.org/abs/2502.03275v1", "repo": "null"}}