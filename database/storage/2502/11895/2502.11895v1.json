{"2502.11895": {"publish_time": "2025-02-17", "title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "paper_summary": "Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u5927\u91cf\u7684\u8cc7\u6e90\u4f86\u9032\u884c\u8a13\u7df4\u548c\u63a8\u7406\u3002\u91cf\u5316\u662f\u4e00\u7a2e\u964d\u4f4e\u6a21\u578b\u53c3\u6578\u7cbe\u5ea6\u7684\u6280\u8853\uff0c\u70ba\u63d0\u9ad8 LLM \u6548\u7387\u548c\u53ef\u6301\u7e8c\u6027\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u96d6\u7136\u8a13\u7df4\u5f8c\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u6bcf\u53c3\u6578\u9054\u5230 4-8 \u4f4d\u5143\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5f9e\u982d\u958b\u59cb\u4f7f\u7528\u6bcf\u6b0a\u91cd\u53c3\u6578 1.58 \u4f4d\u5143\u8a13\u7df4 LLM \u53ef\u4ee5\u7dad\u6301\u6a21\u578b\u6e96\u78ba\u6027\uff0c\u540c\u6642\u5927\u5e45\u6e1b\u5c11\u63a8\u7406\u6642\u9593\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u548c\u80fd\u6e90\u6d88\u8017\u3002\u5728\u6b64\uff0c\u6211\u5011\u63a2\u8a0e\u91cf\u5316\u611f\u77e5\u9810\u8a13\u7df4\u7684\u8a13\u7df4\u7b56\u7565\uff0c\u5176\u4e2d\u6a21\u578b\u9996\u5148\u4f7f\u7528 16 \u4f4d\u5143\u7cbe\u5ea6\u8a13\u7df4\uff0c\u7136\u5f8c\u8f49\u63db\u70ba 1.58 \u4f4d\u5143\u91cf\u5316\u611f\u77e5\u8a13\u7df4\u3002\u6211\u5011\u5728 11 \u500b\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u7d50\u679c\u8868\u660e\uff0c\u9019\u7a2e 16 \u4f4d\u5143\u5230 1.58 \u4f4d\u5143\u7684\u8a13\u7df4\u7b56\u7565\u512a\u65bc\u5b8c\u5168 1.58 \u4f4d\u5143\u8a13\u7df4\uff0c\u4e26\u4e14\u4f7f\u6a21\u578b\u66f4\u63a5\u8fd1\u7d93\u904e 16 \u4f4d\u5143\u8a13\u7df4\u7684\u6a21\u578b\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u5728\u8f49\u63db\u9ede\u4fdd\u7559\u6700\u4f73\u5316\u5668\u72c0\u614b\u548c\u9010\u6f38\u8abf\u6574\u91cf\u5316\u5f37\u5ea6\u7684\u5f71\u97ff\u2014\u2014\u767c\u73fe\u9019\u5169\u7a2e\u6280\u8853\u90fd\u53ef\u4ee5\u6e1b\u8f15\u640d\u5931\u5c16\u5cf0\u7684\u5927\u5c0f\uff0c\u4f46\u9019\u4e9b\u5f71\u97ff\u4e5f\u53ef\u4ee5\u900f\u904e\u9032\u4e00\u6b65\u8a13\u7df4\u4f86\u88dc\u511f\u3002", "author": "Jacob Nielsen et.al.", "authors": "Jacob Nielsen, Peter Schneider-Kamp, Lukas Galke", "id": "2502.11895v1", "paper_url": "http://arxiv.org/abs/2502.11895v1", "repo": "null"}}