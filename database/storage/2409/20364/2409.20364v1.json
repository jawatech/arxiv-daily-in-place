{"2409.20364": {"publish_time": "2024-09-30", "title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using Large Language Models", "paper_summary": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u5177\u5099\u5f37\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52d5\u4e86\u81ea\u52d5\u99d5\u99db\u6280\u8853\u7684\u91cd\u5927\u9032\u5c55\u3002\u61c9\u7528\u65bc\u6b64\u9818\u57df\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u63cf\u8ff0\u99d5\u99db\u5834\u666f\u548c\u884c\u70ba\uff0c\u5176\u6e96\u78ba\u5ea6\u8207\u4eba\u985e\u611f\u77e5\u985e\u4f3c\uff0c\u7279\u5225\u662f\u5728\u8996\u89ba\u4efb\u52d9\u4e2d\u3002\u540c\u6642\uff0c\u908a\u7de3\u904b\u7b97\u7684\u5feb\u901f\u767c\u5c55\uff0c\u56e0\u5176\u9760\u8fd1\u6578\u64da\u6e90\u7684\u512a\u52e2\uff0c\u4f7f\u5f97\u908a\u7de3\u8a2d\u5099\u5728\u81ea\u52d5\u99d5\u99db\u4e2d\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u908a\u7de3\u8a2d\u5099\u5728\u672c\u5730\u8655\u7406\u6578\u64da\uff0c\u6e1b\u5c11\u50b3\u8f38\u5ef6\u9072\u548c\u983b\u5bec\u4f7f\u7528\uff0c\u4e26\u5be6\u73fe\u66f4\u5feb\u7684\u97ff\u61c9\u6642\u9593\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u99d5\u99db\u884c\u70ba\u6558\u8ff0\u548c\u63a8\u7406\u6846\u67b6\uff0c\u5c07 LLM \u61c9\u7528\u65bc\u908a\u7de3\u8a2d\u5099\u3002\u8a72\u6846\u67b6\u7531\u591a\u500b\u8def\u908a\u55ae\u5143\u7d44\u6210\uff0c\u6bcf\u500b\u55ae\u5143\u4e0a\u90fd\u90e8\u7f72\u4e86 LLM\u3002\u9019\u4e9b\u8def\u908a\u55ae\u5143\u6536\u96c6\u9053\u8def\u6578\u64da\u4e26\u901a\u904e 5G NSR/NR \u7db2\u8def\u9032\u884c\u901a\u4fe1\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u90e8\u7f72\u5728\u908a\u7de3\u8a2d\u5099\u4e0a\u7684 LLM \u53ef\u4ee5\u5be6\u73fe\u4ee4\u4eba\u6eff\u610f\u7684\u97ff\u61c9\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u63d0\u793a\u7b56\u7565\u4f86\u589e\u5f37\u7cfb\u7d71\u7684\u6558\u8ff0\u548c\u63a8\u7406\u6027\u80fd\u3002\u6b64\u7b56\u7565\u6574\u5408\u4e86\u591a\u6a21\u614b\u4fe1\u606f\uff0c\u5305\u62ec\u74b0\u5883\u3001\u4ee3\u7406\u548c\u904b\u52d5\u6578\u64da\u3002\u5728 OpenDV-Youtube \u6578\u64da\u96c6\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6848\u986f\u8457\u63d0\u9ad8\u4e86\u9019\u5169\u9805\u4efb\u52d9\u7684\u6027\u80fd\u3002", "author": "Yizhou Huang et.al.", "authors": "Yizhou Huang, Yihua Cheng, Kezhi Wang", "id": "2409.20364v1", "paper_url": "http://arxiv.org/abs/2409.20364v1", "repo": "null"}}