{"2410.22118": {"publish_time": "2024-10-29", "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs", "paper_summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.", "paper_summary_zh": "\u5728\u904e\u53bb\u5e7e\u5e74\u4e2d\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u529f\u80fd\u6709\u4e86\u524d\u6240\u672a\u6709\u7684\u9032\u6b65\u3002\u9019\u4e9b\u9032\u6b65\u6709\u671b\u6975\u5927\u5730\u60e0\u53ca\u5ee3\u6cdb\u7684\u61c9\u7528\u9818\u57df\u3002\u7136\u800c\uff0c\u7531\u65bc\u5176\u898f\u6a21\u9f90\u5927\uff0c\u4f7f\u7528 LLM \u57f7\u884c\u63a8\u7406\u65e2\u6602\u8cb4\u53c8\u7de9\u6162\u3002\u56e0\u6b64\uff0c\u5927\u91cf\u7684\u8fd1\u671f\u5de5\u4f5c\u63d0\u51fa\u4e86\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u7684\u7b56\u7565\uff0c\u4f8b\u5982\u91cf\u5316\u3001\u4fee\u526a\u548c\u5feb\u53d6\u3002\u9019\u4e9b\u52a0\u901f\u7b56\u7565\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u548c\u5ef6\u9072\uff0c\u901a\u5e38\u964d\u4f4e\u4e86\u5e7e\u500b\u56e0\u7d20\uff0c\u540c\u6642\u901a\u904e\u5e38\u898b\u57fa\u6e96\u6e2c\u91cf\u4f86\u7dad\u6301\u5927\u90e8\u5206\u9810\u6e2c\u6027\u80fd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 LLM \u6027\u80fd\u7684\u53e6\u4e00\u500b\u95dc\u9375\u65b9\u9762\uff1a\u7531\u65bc\u63a8\u7406\u52a0\u901f\u512a\u5316\u800c\u5c0e\u81f4\u6a21\u578b\u751f\u6210\u4e2d\u7684\u4eba\u53e3\u7d71\u8a08\u504f\u5dee\u3002\u4f7f\u7528\u5ee3\u6cdb\u7684\u6307\u6a19\uff0c\u6211\u5011\u5f9e\u591a\u500b\u89d2\u5ea6\u63a2\u8a0e\u6a21\u578b\u8f38\u51fa\u7684\u504f\u5dee\u3002\u5c0d\u63a8\u7406\u52a0\u901f\u524d\u5f8c\u7684\u8f38\u51fa\u9032\u884c\u5206\u6790\uff0c\u986f\u793a\u504f\u5dee\u767c\u751f\u4e86\u986f\u8457\u8b8a\u5316\u3002\u4ee4\u4eba\u64d4\u6182\u7684\u662f\uff0c\u9019\u4e9b\u504f\u5dee\u6548\u61c9\u8907\u96dc\u4e14\u4e0d\u53ef\u9810\u6e2c\u3002\u52a0\u901f\u7b56\u7565\u548c\u504f\u5dee\u985e\u578b\u7684\u7d44\u5408\u53ef\u80fd\u5728\u4e00\u500b\u6a21\u578b\u4e2d\u986f\u793a\u51fa\u5f88\u5c0f\u7684\u504f\u5dee\u8b8a\u5316\uff0c\u4f46\u5728\u53e6\u4e00\u500b\u6a21\u578b\u4e2d\u53ef\u80fd\u5c0e\u81f4\u5f88\u5927\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u7d50\u679c\u5f37\u8abf\u4e86\u5728\u4fee\u6539\u6a21\u578b\u4ee5\u52a0\u901f\u63a8\u7406\u5f8c\uff0c\u9700\u8981\u5c0d\u6a21\u578b\u504f\u5dee\u9032\u884c\u6df1\u5165\u548c\u9010\u6848\u8a55\u4f30\u3002", "author": "Elisabeth Kirsten et.al.", "authors": "Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar", "id": "2410.22118v1", "paper_url": "http://arxiv.org/abs/2410.22118v1", "repo": "null"}}