{"2411.16096": {"publish_time": "2024-11-25", "title": "ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality Images", "paper_summary": "Multimodal search has revolutionized the fashion industry, providing a\nseamless and intuitive way for users to discover and explore fashion items.\nBased on their preferences, style, or specific attributes, users can search for\nproducts by combining text and image information. Text-to-image searches enable\nusers to find visually similar items or describe products using natural\nlanguage. This paper presents an innovative approach called ENCLIP, for\nenhancing the performance of the Contrastive Language-Image Pretraining (CLIP)\nmodel, specifically in Multimodal Search targeted towards the domain of fashion\nintelligence. This method focuses on addressing the challenges posed by limited\ndata availability and low-quality images. This paper proposes an algorithm that\ninvolves training and ensembling multiple instances of the CLIP model, and\nleveraging clustering techniques to group similar images together. The\nexperimental findings presented in this study provide evidence of the\neffectiveness of the methodology. This approach unlocks the potential of CLIP\nin the domain of fashion intelligence, where data scarcity and image quality\nissues are prevalent. Overall, the ENCLIP method represents a valuable\ncontribution to the field of fashion intelligence and provides a practical\nsolution for optimizing the CLIP model in scenarios with limited data and\nlow-quality images.", "paper_summary_zh": "\u591a\u6a21\u6001\u641c\u7d22\u5f7b\u5e95\u6539\u53d8\u4e86\u65f6\u5c1a\u4ea7\u4e1a\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u7f1d\u4e14\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u53d1\u73b0\u548c\u63a2\u7d22\u65f6\u5c1a\u5355\u54c1\u3002\u57fa\u4e8e\u4ed6\u4eec\u7684\u504f\u597d\u3001\u98ce\u683c\u6216\u7279\u5b9a\u5c5e\u6027\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u6765\u641c\u7d22\u4ea7\u54c1\u3002\u6587\u672c\u5230\u56fe\u50cf\u641c\u7d22\u4f7f\u7528\u6237\u80fd\u591f\u627e\u5230\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u7269\u54c1\u6216\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4ea7\u54c1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ENCLIP \u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9488\u5bf9\u65f6\u5c1a\u667a\u80fd\u9886\u57df\u7684\u8de8\u6a21\u6001\u641c\u7d22\u4e2d\u3002\u6b64\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u6709\u9650\u548c\u56fe\u50cf\u8d28\u91cf\u4f4e\u4e0b\u7684\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6d89\u53ca\u8bad\u7ec3\u548c\u96c6\u6210 CLIP \u6a21\u578b\u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u5229\u7528\u805a\u7c7b\u6280\u672f\u5c06\u76f8\u4f3c\u7684\u56fe\u50cf\u5206\u7ec4\u5728\u4e00\u8d77\u3002\u672c\u7814\u7a76\u4e2d\u63d0\u51fa\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u91ca\u653e\u4e86 CLIP \u5728\u65f6\u5c1a\u667a\u80fd\u9886\u57df\u4e2d\u7684\u6f5c\u529b\uff0c\u800c\u6570\u636e\u7a00\u7f3a\u548c\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\u5f88\u666e\u904d\u3002\u603b\u4f53\u800c\u8a00\uff0cENCLIP \u65b9\u6cd5\u4ee3\u8868\u4e86\u5bf9\u65f6\u5c1a\u667a\u80fd\u9886\u57df\u7684\u5b9d\u8d35\u8d21\u732e\uff0c\u5e76\u4e3a\u5728\u6570\u636e\u6709\u9650\u548c\u56fe\u50cf\u8d28\u91cf\u4f4e\u4e0b\u7684\u60c5\u51b5\u4e0b\u4f18\u5316 CLIP \u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "author": "Prithviraj Purushottam Naik et.al.", "authors": "Prithviraj Purushottam Naik, Rohit Agarwal", "id": "2411.16096v1", "paper_url": "http://arxiv.org/abs/2411.16096v1", "repo": "null"}}