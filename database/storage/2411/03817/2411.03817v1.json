{"2411.03817": {"publish_time": "2024-11-06", "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "paper_summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5091\u51fa\u529f\u80fd\u8b93\u5b83\u5011\u6210\u70ba\u5404\u7a2e\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7d71\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u7d44\u6210\u90e8\u5206\u3002\u96d6\u7136\u50b3\u7d71\u65b9\u6cd5\u4f9d\u8cf4\u65bc LLM \u7684\u56fa\u6709\u77e5\u8b58\u800c\u7121\u9700\u5fae\u8abf\uff0c\u4f46\u6700\u8fd1\u7684\u65b9\u6cd5\u5df2\u8f49\u5411\u5f37\u5316\u5b78\u7fd2\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u4ee3\u7406\u89e3\u6c7a\u8907\u96dc\u4e92\u52d5\u4efb\u52d9\u7684\u80fd\u529b\uff0c\u5305\u62ec\u74b0\u5883\u548c\u5de5\u5177\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u505a\u6cd5\u53d7\u5230\u7a00\u758f\u734e\u52f5\u554f\u984c\u7684\u9650\u5236\uff0c\u5176\u4e2d\u73fe\u6709\u8cc7\u6599\u96c6\u50c5\u70ba\u6bcf\u500b\u591a\u6b65\u9a5f\u63a8\u7406\u93c8\u63d0\u4f9b\u6700\u7d42\u6a19\u91cf\u734e\u52f5\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u7b56\u7565\u5b78\u7fd2\u7684\u7121\u6548\u6027\u548c\u4f4e\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 StepAgent\uff0c\u5b83\u5229\u7528\u9010\u6b65\u734e\u52f5\u4f86\u512a\u5316\u4ee3\u7406\u7684\u5f37\u5316\u5b78\u7fd2\u904e\u7a0b\u3002\u7e7c\u627f\u521d\u5b78\u8005\u5230\u5c08\u5bb6\u7684\u7406\u8ad6\u7cbe\u795e\uff0c\u6211\u5011\u9996\u5148\u6bd4\u8f03\u5c08\u5bb6\u548c\u4ee3\u7406\u7684\u884c\u52d5\uff0c\u4ee5\u81ea\u52d5\u7522\u751f\u7528\u65bc\u7d30\u5316\u512a\u5316\u7684\u4e2d\u9593\u734e\u52f5\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u96b1\u5f0f\u734e\u52f5\u548c\u9006\u5411\u5f37\u5316\u5b78\u7fd2\u6280\u8853\uff0c\u4ee5\u4fc3\u9032\u4ee3\u7406\u53cd\u601d\u548c\u7b56\u7565\u8abf\u6574\u3002\u9032\u4e00\u6b65\u7684\u7406\u8ad6\u5206\u6790\u8868\u660e\uff0c\u4ee3\u7406\u7684\u884c\u52d5\u5206\u4f48\u53ef\u4ee5\u5728\u591a\u500b\u8a13\u7df4\u9031\u671f\u4e2d\u6536\u6582\u5230\u5c08\u5bb6\u884c\u52d5\u5206\u4f48\u3002\u8de8\u5404\u7a2e\u8cc7\u6599\u96c6\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cStepAgent \u512a\u65bc\u73fe\u6709\u7684\u57fa\u6e96\u65b9\u6cd5\u3002", "author": "Zhirui Deng et.al.", "authors": "Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen", "id": "2411.03817v1", "paper_url": "http://arxiv.org/abs/2411.03817v1", "repo": "null"}}