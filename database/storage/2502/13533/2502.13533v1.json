{"2502.13533": {"publish_time": "2025-02-19", "title": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models", "paper_summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\n(LoRA) offers a cost-effective fine-tuning solution, freezing the original\nmodel parameters and training only lightweight, low-rank adapter matrices.\nHowever, the memory footprint of LoRA is largely dominated by the original\nmodel parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA\ntraining scheme founded on the intuition that many neurons in\nover-parameterized LLMs have low training utility but are essential for\ninference. LoRAM presents a unique twist: it trains on a pruned (small) model\nto obtain pruned low-rank matrices, which are then recovered and utilized with\nthe original (large) model for inference. Additionally, minimal-cost continual\npre-training, performed by the model publishers in advance, aligns the\nknowledge discrepancy between pruned and original models. Our extensive\nexperiments demonstrate the efficacy of LoRAM across various pruning strategies\nand downstream tasks. For a model with 70 billion parameters, LoRAM enables\ntraining on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA\ntraining and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by\nstructured pruning combined with 4-bit quantization, for LLaMA-3.1-70B\n(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory\nusage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while\nachieving dominant performance gains over both the original LLaMA-3.1-70B\n(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5353\u8d8a\u7684\u4efb\u52d9\u6cdb\u5316\u80fd\u529b\u5927\u5e45\u63d0\u5347\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\u4f4e\u968e\u9069\u61c9 (LoRA) \u63d0\u4f9b\u4e00\u7a2e\u7d93\u6fdf\u5be6\u60e0\u7684\u5fae\u8abf\u89e3\u6c7a\u65b9\u6848\uff0c\u51cd\u7d50\u539f\u59cb\u6a21\u578b\u53c3\u6578\uff0c\u53ea\u8a13\u7df4\u8f15\u91cf\u7d1a\u7684\u4f4e\u968e\u9069\u914d\u5668\u77e9\u9663\u3002\u7136\u800c\uff0cLoRA \u7684\u8a18\u61b6\u9ad4\u4f54\u7528\u91cf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u6c7a\u65bc\u539f\u59cb\u6a21\u578b\u53c3\u6578\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u7a2e\u60c5\u6cc1\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LoRAM\uff0c\u9019\u662f\u4e00\u7a2e\u4ee5\u76f4\u89ba\u70ba\u57fa\u790e\u7684\u7701\u8a18\u61b6\u9ad4 LoRA \u8a13\u7df4\u65b9\u6848\uff0c\u76f4\u89ba\u8a8d\u70ba\u904e\u5ea6\u53c3\u6578\u5316\u7684 LLM \u4e2d\u8a31\u591a\u795e\u7d93\u5143\u5728\u8a13\u7df4\u4e2d\u6548\u7528\u4f4e\uff0c\u4f46\u5728\u63a8\u8ad6\u4e2d\u537b\u662f\u5fc5\u8981\u7684\u3002LoRAM \u5448\u73fe\u51fa\u4e00\u500b\u7368\u7279\u7684\u8f49\u6298\uff1a\u5b83\u5728\u4e00\u500b\u4fee\u526a\u904e\u7684\uff08\u5c0f\uff09\u6a21\u578b\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u4ee5\u53d6\u5f97\u4fee\u526a\u904e\u7684\u4f4e\u968e\u77e9\u9663\uff0c\u7136\u5f8c\u5c07\u9019\u4e9b\u77e9\u9663\u5fa9\u539f\u4e26\u8207\u539f\u59cb\u7684\uff08\u5927\uff09\u6a21\u578b\u4e00\u8d77\u7528\u65bc\u63a8\u8ad6\u3002\u6b64\u5916\uff0c\u7531\u6a21\u578b\u767c\u5e03\u8005\u9810\u5148\u57f7\u884c\u7684\u6700\u5c0f\u6210\u672c\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u8abf\u6574\u4e86\u4fee\u526a\u6a21\u578b\u548c\u539f\u59cb\u6a21\u578b\u4e4b\u9593\u7684\u77e5\u8b58\u5dee\u7570\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 LoRAM \u5728\u5404\u7a2e\u4fee\u526a\u7b56\u7565\u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u529f\u6548\u3002\u5c0d\u65bc\u4e00\u500b\u64c1\u6709 700 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\uff0cLoRAM \u80fd\u5920\u5728\u53ea\u6709 20G HBM \u7684 GPU \u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u53d6\u4ee3\u4e86 LoRA \u8a13\u7df4\u7684 A100-80G GPU \u548c\u5fae\u8abf\u7684 15 \u500b GPU\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u7531\u7d50\u69cb\u5316\u4fee\u526a\u8207 4 \u4f4d\u91cf\u5316\u7d50\u5408\u57f7\u884c\u7684 QLoRAM\uff0c\u91dd\u5c0d LLaMA-3.1-70B\uff08LLaMA-2-70B\uff09\uff0c\u5c07\u5728\u4f4e\u968e\u77e9\u9663\u8a13\u7df4\u4e2d\u4f54\u64da\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u4e3b\u5c0e\u5730\u4f4d\u7684\u53c3\u6578\u5132\u5b58\u6210\u672c\u964d\u4f4e\u4e86 15.81 \u500d\uff0816.95 \u500d\uff09\uff0c\u540c\u6642\u5728\u539f\u59cb LLaMA-3.1-70B\uff08LLaMA-2-70B\uff09\u548c LoRA \u8a13\u7df4\u7684 LLaMA-3.1-8B\uff08LLaMA-2-13B\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002", "author": "Jun Zhang et.al.", "authors": "Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou", "id": "2502.13533v1", "paper_url": "http://arxiv.org/abs/2502.13533v1", "repo": "null"}}