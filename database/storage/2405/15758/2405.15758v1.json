{"2405.15758": {"publish_time": "2024-05-24", "title": "InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation", "paper_summary": "Recent talking avatar generation models have made strides in achieving\nrealistic and accurate lip synchronization with the audio, but often fall short\nin controlling and conveying detailed expressions and emotions of the avatar,\nmaking the generated video less vivid and controllable. In this paper, we\npropose a novel text-guided approach for generating emotionally expressive 2D\navatars, offering fine-grained control, improved interactivity, and\ngeneralizability to the resulting video. Our framework, named InstructAvatar,\nleverages a natural language interface to control the emotion as well as the\nfacial motion of avatars. Technically, we design an automatic annotation\npipeline to construct an instruction-video paired training dataset, equipped\nwith a novel two-branch diffusion-based generator to predict avatars with audio\nand text instructions at the same time. Experimental results demonstrate that\nInstructAvatar produces results that align well with both conditions, and\noutperforms existing methods in fine-grained emotion control, lip-sync quality,\nand naturalness. Our project page is\nhttps://wangyuchi369.github.io/InstructAvatar/.", "paper_summary_zh": "\u6700\u8fd1\u7684\u5c0d\u8a71\u982d\u50cf\u751f\u6210\u6a21\u578b\u5728\u5be6\u73fe\u97f3\u8a0a\u7684\u903c\u771f\u4e14\u6e96\u78ba\u7684\u5507\u90e8\u540c\u6b65\u65b9\u9762\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u901a\u5e38\u5728\u63a7\u5236\u548c\u50b3\u9054\u982d\u50cf\u7684\u8a73\u7d30\u8868\u60c5\u548c\u60c5\u7dd2\u65b9\u9762\u505a\u5f97\u4e0d\u5920\uff0c\u9019\u4f7f\u5f97\u751f\u6210\u7684\u5f71\u7247\u8f03\u4e0d\u751f\u52d5\u4e14\u53ef\u63a7\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u6587\u5b57\u5f15\u5c0e\u65b9\u6cd5\u4f86\u751f\u6210\u60c5\u7dd2\u8868\u9054\u7684 2D \u982d\u50cf\uff0c\u63d0\u4f9b\u7d30\u7dfb\u7684\u63a7\u5236\u3001\u6539\u5584\u7684\u4e92\u52d5\u6027\uff0c\u4ee5\u53ca\u5c0d\u7d50\u679c\u5f71\u7247\u7684\u6982\u62ec\u6027\u3002\u6211\u5011\u7684\u67b6\u69cb\u540d\u70ba InstructAvatar\uff0c\u5229\u7528\u81ea\u7136\u8a9e\u8a00\u4ecb\u9762\u4f86\u63a7\u5236\u60c5\u7dd2\u4ee5\u53ca\u982d\u50cf\u7684\u9762\u90e8\u52d5\u4f5c\u3002\u5728\u6280\u8853\u4e0a\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u81ea\u52d5\u8a3b\u89e3\u7ba1\u9053\u4f86\u5efa\u69cb\u4e00\u500b\u6307\u4ee4\u5f71\u7247\u914d\u5c0d\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u4e26\u914d\u5099\u4e00\u500b\u65b0\u7a4e\u7684\u96d9\u5206\u652f\u64f4\u6563\u5f0f\u751f\u6210\u5668\uff0c\u4ee5\u540c\u6642\u4f7f\u7528\u97f3\u8a0a\u548c\u6587\u5b57\u6307\u4ee4\u4f86\u9810\u6e2c\u982d\u50cf\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e InstructAvatar \u7522\u751f\u7684\u7d50\u679c\u8207\u9019\u5169\u500b\u689d\u4ef6\u90fd\u975e\u5e38\u543b\u5408\uff0c\u4e26\u4e14\u5728\u7d30\u7dfb\u7684\u60c5\u7dd2\u63a7\u5236\u3001\u5507\u90e8\u540c\u6b65\u54c1\u8cea\u548c\u81ea\u7136\u5ea6\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\u662f https://wangyuchi369.github.io/InstructAvatar/\u3002", "author": "Yuchi Wang et.al.", "authors": "Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian", "id": "2405.15758v1", "paper_url": "http://arxiv.org/abs/2405.15758v1", "repo": "https://github.com/wangyuchi369/InstructAvatar"}}