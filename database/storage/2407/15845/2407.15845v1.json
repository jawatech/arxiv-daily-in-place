{"2407.15845": {"publish_time": "2024-07-22", "title": "Reconstructing Training Data From Real World Models Trained with Transfer Learning", "paper_summary": "Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.", "paper_summary_zh": "\u76ee\u524d\u7684\u8a13\u7df4\u8cc7\u6599\u5f9e\u8a13\u7df4\u597d\u7684\u5206\u985e\u5668\u4e2d\u91cd\u5efa\u65b9\u6cd5\u50c5\u9650\u65bc\u975e\u5e38\u5c0f\u7684\u6a21\u578b\u3001\u6709\u9650\u7684\u8a13\u7df4\u96c6\u5927\u5c0f\u548c\u4f4e\u89e3\u6790\u5ea6\u7684\u5f71\u50cf\u3002\u9019\u4e9b\u9650\u5236\u59a8\u7919\u5176\u9069\u7528\u65bc\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u8a13\u7df4\u7684\u6a21\u578b\u4e2d\u9032\u884c\u8cc7\u6599\u91cd\u5efa\uff0c\u4ee5\u9069\u61c9\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u3002\u6211\u5011\u7684\u6a21\u578b\u5c07 arXiv:2206.07758 \u7684\u91cd\u5efa\u65b9\u6848\u8abf\u6574\u5230\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\u2014\u2014\u7279\u5225\u662f\u91dd\u5c0d\u900f\u904e\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\uff08\u5982 DINO-ViT \u548c CLIP\uff09\u7684\u5f71\u50cf\u5d4c\u5165\u9032\u884c\u9077\u79fb\u5b78\u7fd2\u8a13\u7df4\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u5d4c\u5165\u7a7a\u9593\u4e2d\u9032\u884c\u8cc7\u6599\u91cd\u5efa\uff0c\u800c\u4e0d\u5728\u5f71\u50cf\u7a7a\u9593\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8996\u89ba\u8cc7\u6599\u4e4b\u5916\u7684\u9069\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u57fa\u65bc\u7fa4\u96c6\u7684\u65b9\u6cd5\uff0c\u5f9e\u6578\u5343\u500b\u5019\u9078\u8005\u4e2d\u627e\u51fa\u826f\u597d\u7684\u91cd\u5efa\u3002\u9019\u986f\u8457\u6539\u5584\u4e86\u4ee5\u5f80\u4f9d\u8cf4\u8a13\u7df4\u96c6\u77e5\u8b58\u4f86\u627e\u51fa\u826f\u597d\u91cd\u5efa\u5f71\u50cf\u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u4f7f\u7528\u9077\u79fb\u5b78\u7fd2\u8a13\u7df4\u7684\u6a21\u578b\u4e2d\u8cc7\u6599\u5916\u6d29\u7684\u6f5b\u5728\u96b1\u79c1\u98a8\u96aa\u3002", "author": "Yakir Oz et.al.", "authors": "Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim", "id": "2407.15845v1", "paper_url": "http://arxiv.org/abs/2407.15845v1", "repo": "null"}}