{"2411.13244": {"publish_time": "2024-11-20", "title": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL", "paper_summary": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u89e3\u6c7a\u554f\u984c\u6280\u80fd\uff0c\u4f46\u8207\u4eba\u985e\u76f8\u6bd4\uff0c\u5b83\u5011\u5728\u5404\u7a2e\u4e0b\u6e38\u61c9\u7528\uff08\u4f8b\u5982\u6587\u5b57\u8f49 SQL\uff09\u4e2d\u7684\u8868\u73fe\u4ecd\u7136\u8f03\u5dee\u3002\u5728 BIRD \u57fa\u6e96\u6392\u884c\u699c\u4e0a\uff0c\u4eba\u985e\u7684\u8868\u73fe\u9054\u5230 92.96% \u7684\u6e96\u78ba\u5ea6\uff0c\u800c\u8868\u73fe\u6700\u4f73\u7684\u65b9\u6cd5\u50c5\u9054\u5230 72.39%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u9019\u4e9b\u6700\u5148\u9032 (SoTA) \u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u65bc\u60c5\u5883\u5b78\u7fd2\u4f86\u6a21\u64ec\u985e\u4eba\u63a8\u7406\u3002\u7136\u800c\uff0c\u4ed6\u5011\u5ffd\u8996\u4e86\u4e00\u9805\u95dc\u9375\u7684\u4eba\u985e\u6280\u80fd\uff1a\u6301\u7e8c\u5b78\u7fd2\u3002\u53d7\u6211\u5011\u5728\u6210\u9577\u904e\u7a0b\u4e2d\u4fdd\u6301\u932f\u8aa4\u7b46\u8a18\u672c\u7684\u6559\u80b2\u5be6\u52d9\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LPE-SQL\uff08\u5229\u7528\u5148\u9a57\u7d93\u9a57\uff1a\u6587\u5b57\u8f49 SQL \u53ef\u64f4\u5145\u8f14\u52a9\u77e5\u8b58\u5eab\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u65e8\u5728\u901a\u904e\u555f\u7528\u6301\u7e8c\u5b78\u7fd2\u4f86\u64f4\u5145 LLM\uff0c\u800c\u7121\u9700\u9032\u884c\u53c3\u6578\u5fae\u8abf\u3002LPE-SQL \u5305\u542b\u56db\u500b\u6a21\u7d44\uff0c\u5b83\u5011\\textbf{i)} \u6aa2\u7d22\u76f8\u95dc\u689d\u76ee\uff0c\\textbf{ii)} \u6709\u6548\u7684 SQL \u751f\u6210\uff0c\\textbf{iii)} \u901a\u904e\u4ea4\u53c9\u4e00\u81f4\u6027\u6a5f\u5236\u751f\u6210\u6700\u7d42\u7d50\u679c\uff0c\u4ee5\u53ca\\textbf{iv)} \u8a18\u9304\u6210\u529f\u7684\u548c\u5931\u6557\u7684\u4efb\u52d9\u4ee5\u53ca\u5b83\u5011\u7684\u63a8\u7406\u904e\u7a0b\u6216\u53cd\u601d\u7522\u751f\u7684\u63d0\u793a\u3002\u91cd\u8981\u7684\u662f\uff0cLPE-SQL \u7684\u6838\u5fc3\u6a21\u7d44\u662f\u7b2c\u56db\u500b\u6a21\u7d44\uff0c\u800c\u5176\u4ed6\u6a21\u7d44\u63a1\u7528\u57fa\u790e\u65b9\u6cd5\uff0c\u5141\u8a31 LPE-SQL \u8f15\u9b06\u8207 SoTA \u6280\u8853\u6574\u5408\u4ee5\u9032\u4e00\u6b65\u63d0\u9ad8\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u9019\u7a2e\u6301\u7e8c\u5b78\u7fd2\u65b9\u6cd5\u7522\u751f\u4e86\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4f7f\u7528 SoTA \u65b9\u6cd5\u7684\u8f03\u5c0f Llama-3.1-70B \u6a21\u578b\u8d85\u8d8a\u4e86\u8f03\u5927 Llama-3.1-405B \u6a21\u578b\u7684\u6548\u80fd\u3002", "author": "Zhibo Chu et.al.", "authors": "Zhibo Chu, Zichong Wang, Qitao Qin", "id": "2411.13244v1", "paper_url": "http://arxiv.org/abs/2411.13244v1", "repo": "null"}}