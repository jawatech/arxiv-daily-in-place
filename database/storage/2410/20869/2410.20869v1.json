{"2410.20869": {"publish_time": "2024-10-28", "title": "Reward Modeling with Weak Supervision for Language Models", "paper_summary": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5c0e\u81f4\u5b83\u5011\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u61c9\u7528\u589e\u52a0\uff0c\u800c\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u5b83\u5011\u8a13\u7df4\u4e2d\u7684\u95dc\u9375\u90e8\u5206\uff0c\u4ee5\u5c07\u56de\u61c9\u8207\u4f7f\u7528\u8005\u610f\u5716\u4fdd\u6301\u4e00\u81f4\u3002\u5728 RLHF \u904e\u7a0b\u4e2d\uff0c\u4f7f\u7528\u7531\u4eba\u985e\u6a19\u7c64\u8005\u6216 AI \u7cfb\u7d71\u6c7a\u5b9a\u7684\u56de\u61c9\u504f\u597d\u4f86\u8a13\u7df4\u734e\u52f5\u6a21\u578b\uff0c\u7136\u5f8c\u900f\u904e\u5f37\u5316\u5b78\u7fd2\u4f86\u512a\u5316 LLM\u3002\u9019\u9805\u5de5\u4f5c\u5f15\u5165\u4e86\u5f31\u76e3\u7763\uff0c\u4f5c\u70ba\u64f4\u5145 RLHF \u8cc7\u6599\u96c6\u548c\u589e\u5f37\u734e\u52f5\u6a21\u578b\u6548\u80fd\u7684\u7b56\u7565\u3002\u5f31\u76e3\u7763\u63a1\u7528\u6709\u96dc\u8a0a\u6216\u4e0d\u7cbe\u78ba\u7684\u8cc7\u6599\u6a19\u7c64\uff0c\u6e1b\u5c11\u5c0d\u6602\u8cb4\u7684\u624b\u52d5\u6a19\u7c64\u8cc7\u6599\u7684\u4f9d\u8cf4\u3002\u900f\u904e\u5206\u6790 RLHF \u8cc7\u6599\u96c6\u4ee5\u627e\u51fa\u8207\u56de\u61c9\u504f\u597d\u76f8\u95dc\u7684\u555f\u767c\u6cd5\uff0c\u6211\u5011\u64b0\u5beb\u4e86\u7c21\u55ae\u7684\u6a19\u7c64\u51fd\u6578\uff0c\u7136\u5f8c\u6821\u6e96\u6a19\u7c64\u6a21\u578b\u4ee5\u5f31\u6a19\u8a18\u672a\u6a19\u7c64\u7684\u8cc7\u6599\u3002\u6211\u5011\u7684\u8a55\u4f30\u986f\u793a\uff0c\u96d6\u7136\u5f31\u76e3\u7763\u900f\u904e\u6539\u5584\u734e\u52f5\u6a21\u578b\u6548\u80fd\u986f\u8457\u53d7\u76ca\u65bc\u8f03\u5c0f\u7684\u8cc7\u6599\u96c6\uff0c\u4f46\u5176\u6709\u6548\u6027\u6703\u96a8\u8457\u539f\u672c\u6a19\u7c64\u7684\u8f03\u5927\u8cc7\u6599\u96c6\u800c\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u4f7f\u7528 LLM \u4f86\u7522\u751f\u7136\u5f8c\u5f31\u6a19\u8a18\u56de\u61c9\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u6cd5\u4f86\u64f4\u5145\u504f\u597d\u8cc7\u6599\u3002", "author": "Ben Hauptvogel et.al.", "authors": "Ben Hauptvogel, Malte Ostendorff, Georg Rehm, Sebastian M\u00f6ller", "id": "2410.20869v1", "paper_url": "http://arxiv.org/abs/2410.20869v1", "repo": "null"}}