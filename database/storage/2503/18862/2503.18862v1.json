{"2503.18862": {"publish_time": "2025-03-24", "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation", "paper_summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.", "paper_summary_zh": "<paragraph>\u96d6\u7136 CNN \u9577\u671f\u4ee5\u4f86\u4e00\u76f4\u88ab\u8a8d\u70ba\u662f\u5716\u50cf\u8655\u7406\u7684\u6700\u65b0\u6280\u8853\uff0c\u4f46 Transformer \u67b6\u69cb\u7684\u5f15\u5165\u6311\u6230\u4e86\u9019\u4e00\u5730\u4f4d\u3002\u96d6\u7136 Transformer \u5728\u5716\u50cf\u5206\u985e\u548c\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u512a\u7570\u7684\u6210\u679c\uff0c\u4f46\u5b83\u5011\u4ecd\u7136\u672c\u8cea\u4e0a\u4f9d\u8cf4\u65bc\u5927\u578b\u8a13\u7df4\u6578\u64da\u96c6\uff0c\u800c\u4e14\u8a08\u7b97\u6210\u672c\u9ad8\u6602\u3002\u4e00\u7a2e\u65b0\u63a8\u51fa\u7684\u540d\u70ba KV Transformer \u7684 Transformer \u884d\u751f\u6a21\u578b\u5728\u5408\u6210\u3001NLP \u548c\u5716\u50cf\u5206\u985e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u826f\u597d\u7684\u7d50\u679c\uff0c\u540c\u6642\u964d\u4f4e\u4e86\u8907\u96dc\u6027\u548c\u5167\u5b58\u4f7f\u7528\u91cf\u3002\u9019\u5c24\u5176\u6709\u5229\u65bc\u9700\u8981\u672c\u5730\u63a8\u7406\u7684\u7528\u4f8b\uff0c\u4f8b\u5982\u91ab\u7642\u7be9\u6aa2\u61c9\u7528\u3002\u6211\u5011\u52aa\u529b\u9032\u4e00\u6b65\u8a55\u4f30 KV Transformer \u5728\u8a9e\u7fa9\u5206\u5272\u4efb\u52d9\u4e0a\u7684\u512a\u9ede\uff0c\u7279\u5225\u662f\u5728\u91ab\u5b78\u5f71\u50cf\u9818\u57df\u3002\u901a\u904e\u76f4\u63a5\u6bd4\u8f03\u76f8\u540c\u57fa\u790e\u67b6\u69cb\u7684\u50b3\u7d71\u8b8a\u9ad4\u548c KV \u8b8a\u9ad4\uff0c\u6211\u5011\u53ef\u4ee5\u9032\u4e00\u6b65\u4e86\u89e3\u964d\u4f4e\u6a21\u578b\u8907\u96dc\u6027\u7684\u5be6\u969b\u53d6\u6368\u3002\u6211\u5011\u89c0\u5bdf\u5230\u53c3\u6578\u6578\u91cf\u548c\u4e58\u7a4d\u7d2f\u52a0\u904b\u7b97\u986f\u8457\u6e1b\u5c11\uff0c\u800c\u5927\u591a\u6578 KV \u8b8a\u9ad4\u6a21\u578b\u5728\u8207\u5176 QKV \u5be6\u73fe\u76f4\u63a5\u6bd4\u8f03\u6642\uff0c\u9054\u5230\u4e86\u76f8\u4f3c\u7684\u6027\u80fd\u3002</paragraph>\n", "author": "DeShin Hwa et.al.", "authors": "DeShin Hwa, Tobias Holmes, Klaus Drechsler", "id": "2503.18862v1", "paper_url": "http://arxiv.org/abs/2503.18862v1", "repo": "null"}}