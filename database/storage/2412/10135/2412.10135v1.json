{"2412.10135": {"publish_time": "2024-12-13", "title": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers", "paper_summary": "As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u898f\u6a21\u7684\u64f4\u5927\uff0c\u7531\u65bc\u5176\u9ad8\u904b\u7b97\u548c\u5132\u5b58\u6210\u672c\uff0c\u50b3\u7d71\u7684\u5168\u5fae\u8abf\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u4e0d\u5207\u5be6\u969b\u3002\u5118\u7ba1\u6d41\u884c\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf\u65b9\u6cd5\uff0c\u4f8b\u5982 LoRA\uff0c\u5df2\u986f\u8457\u6e1b\u5c11\u53ef\u8abf\u53c3\u6578\u7684\u6578\u91cf\uff0c\u4f46\u4ecd\u6709\u9032\u4e00\u6b65\u6700\u4f73\u5316\u7684\u7a7a\u9593\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa ASLoRA\uff0c\u4e00\u7a2e\u8de8\u5c64\u53c3\u6578\u5171\u4eab\u7b56\u7565\uff0c\u7d50\u5408\u4e86\u5168\u5c40\u5171\u4eab\u548c\u90e8\u5206\u81ea\u9069\u61c9\u5171\u4eab\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u6240\u6709\u5c64\u4e2d\u5171\u4eab\u4f4e\u79e9\u77e9\u9663 A\uff0c\u4e26\u5728\u8a13\u7df4\u671f\u9593\u81ea\u9069\u61c9\u5730\u5408\u4f75\u77e9\u9663 B\u3002\u9019\u7a2e\u5171\u4eab\u6a5f\u5236\u4e0d\u50c5\u6709\u6548\u5730\u6e1b\u8f15\u4e86\u904e\u5ea6\u64ec\u5408\uff0c\u800c\u4e14\u9084\u6355\u7372\u4e86\u5c64\u9593\u4f9d\u8cf4\u6027\uff0c\u986f\u8457\u589e\u5f37\u4e86\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002\u6211\u5011\u5c0d\u5404\u7a2e NLP \u4efb\u52d9\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u8868\u660e ASLoRA \u5728\u4f7f\u7528\u4e0d\u5230 25% \u7684\u53c3\u6578\u6642\u512a\u65bc LoRA\uff0c\u7a81\u51fa\u4e86\u5176\u9748\u6d3b\u6027\u8207\u5353\u8d8a\u7684\u53c3\u6578\u6548\u7387\u3002\u6b64\u5916\uff0c\u5c0d\u81ea\u9069\u61c9\u5171\u4eab\u7b56\u7565\u7684\u6df1\u5165\u5206\u6790\u8b49\u5be6\u4e86\u5176\u5728\u589e\u5f37\u6a21\u578b\u9748\u6d3b\u6027\u8207\u4efb\u52d9\u9069\u61c9\u6027\u65b9\u9762\u7684\u986f\u8457\u512a\u52e2\u3002", "author": "Junyan Hu et.al.", "authors": "Junyan Hu, Xue Xiao, Mengqi Zhang, Xiao Chen, Zhaochun Ren, Zhumin Chen, Pengjie Ren", "id": "2412.10135v1", "paper_url": "http://arxiv.org/abs/2412.10135v1", "repo": "null"}}