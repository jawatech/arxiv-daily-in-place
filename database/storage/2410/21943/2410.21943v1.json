{"2410.21943": {"publish_time": "2024-10-29", "title": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications", "paper_summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u554f\u7b54\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u7f3a\u4e4f\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\uff0c\u800c\u4e14\u5bb9\u6613\u51fa\u73fe\u5e7b\u89ba\u3002\u6aa2\u7d22\u64f4\u589e\u751f\u6210 (RAG) \u662f\u4e00\u7a2e\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u7684\u65b9\u6cd5\uff0c\u800c\u591a\u6a21\u614b\u6a21\u578b\u6b63\u6210\u70ba\u8655\u7406\u6587\u5b57\u548c\u5f71\u50cf\u7684 AI \u52a9\u7406\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u4e00\u7cfb\u5217\u5be6\u9a57\uff0c\u65e8\u5728\u78ba\u5b9a\u5982\u4f55\u6700\u4f73\u6574\u5408\u591a\u6a21\u614b\u6a21\u578b\u5230\u5de5\u696d\u9818\u57df\u7684 RAG \u7cfb\u7d71\u4e2d\u3002\u5be6\u9a57\u7684\u76ee\u7684\u662f\u78ba\u5b9a\u5728\u5de5\u696d\u9818\u57df\u7684\u6587\u4ef6\u4e2d\u5305\u542b\u5f71\u50cf\u548c\u6587\u5b57\u662f\u5426\u80fd\u63d0\u5347 RAG \u7684\u6548\u80fd\uff0c\u4e26\u627e\u51fa\u6b64\u985e\u591a\u6a21\u614b RAG \u7cfb\u7d71\u7684\u6700\u4f73\u7d44\u614b\u3002\u6211\u5011\u7684\u5be6\u9a57\u5305\u542b\u5169\u7a2e\u5f71\u50cf\u8655\u7406\u548c\u6aa2\u7d22\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5169\u7a2e LLM\uff08GPT4-Vision \u548c LLaVA\uff09\u7528\u65bc\u7b54\u6848\u5408\u6210\u3002\u9019\u4e9b\u5f71\u50cf\u8655\u7406\u7b56\u7565\u6d89\u53ca\u4f7f\u7528\u591a\u6a21\u614b\u5d4c\u5165\u548c\u5f9e\u5f71\u50cf\u7522\u751f\u6587\u5b57\u6458\u8981\u3002\u6211\u5011\u4f7f\u7528 LLM-as-a-Judge \u65b9\u6cd5\u8a55\u4f30\u6211\u5011\u7684\u5be6\u9a57\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u591a\u6a21\u614b RAG \u80fd\u512a\u65bc\u55ae\u6a21\u614b RAG \u8a2d\u5b9a\uff0c\u5118\u7ba1\u5f71\u50cf\u6aa2\u7d22\u6bd4\u6587\u5b57\u6aa2\u7d22\u66f4\u5177\u6311\u6230\u6027\u3002\u6b64\u5916\uff0c\u8207\u4f7f\u7528\u591a\u6a21\u614b\u5d4c\u5165\u76f8\u6bd4\uff0c\u5229\u7528\u5f71\u50cf\u7684\u6587\u5b57\u6458\u8981\u5448\u73fe\u51fa\u66f4\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u70ba\u672a\u4f86\u7684\u9032\u5c55\u63d0\u4f9b\u4e86\u66f4\u591a\u6a5f\u6703\u3002", "author": "Monica Riedler et.al.", "authors": "Monica Riedler, Stefan Langer", "id": "2410.21943v1", "paper_url": "http://arxiv.org/abs/2410.21943v1", "repo": "https://github.com/riedlerm/multimodal_rag_for_industry"}}