{"2406.17969": {"publish_time": "2024-06-25", "title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "paper_summary": "To better interpret the intrinsic mechanism of large language models (LLMs),\nrecent studies focus on monosemanticity on its basic units. A monosemantic\nneuron is dedicated to a single and specific concept, which forms a one-to-one\ncorrelation between neurons and concepts. Despite extensive research in\nmonosemanticity probing, it remains unclear whether monosemanticity is\nbeneficial or harmful to model capacity. To explore this question, we revisit\nmonosemanticity from the feature decorrelation perspective and advocate for its\nencouragement. We experimentally observe that the current conclusion by\nwang2024learning, which suggests that decreasing monosemanticity enhances model\nperformance, does not hold when the model changes. Instead, we demonstrate that\nmonosemanticity consistently exhibits a positive correlation with model\ncapacity, in the preference alignment process. Consequently, we apply feature\ncorrelation as a proxy for monosemanticity and incorporate a feature\ndecorrelation regularizer into the dynamic preference optimization process. The\nexperiments show that our method not only enhances representation diversity and\nactivation sparsity but also improves preference alignment performance.", "paper_summary_zh": "\u70ba\u4e86\u66f4\u597d\u5730\u8a6e\u91cb\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5167\u5728\u6a5f\u5236\uff0c\n\u8fd1\u671f\u7814\u7a76\u5c08\u6ce8\u65bc\u5176\u57fa\u672c\u55ae\u5143\u7684\u55ae\u4e00\u8a9e\u7fa9\u3002\u55ae\u4e00\u8a9e\u7fa9\n\u795e\u7d93\u5143\u5c08\u6ce8\u65bc\u55ae\u4e00\u4e14\u7279\u5b9a\u7684\u6982\u5ff5\uff0c\u5f62\u6210\u795e\u7d93\u5143\u8207\u6982\u5ff5\u4e4b\u9593\u7684\u4e00\u5c0d\u4e00\n\u5c0d\u61c9\u95dc\u4fc2\u3002\u5118\u7ba1\u5728\u55ae\u4e00\u8a9e\u7fa9\u63a2\u6e2c\u65b9\u9762\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u7814\u7a76\uff0c\n\u4f46\u55ae\u4e00\u8a9e\u7fa9\u662f\u5426\u6709\u76ca\u6216\u6709\u5bb3\u65bc\u6a21\u578b\u5bb9\u91cf\u4ecd\u4e0d\u6e05\u695a\u3002\u70ba\u4e86\u63a2\u8a0e\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f9e\u7279\u5fb5\u53bb\u76f8\u95dc\u7684\u89d2\u5ea6\u91cd\u65b0\u5be9\u8996\u55ae\u4e00\u8a9e\u7fa9\u4e26\u63d0\u5021\u5176\n\u9f13\u52f5\u3002\u6211\u5011\u900f\u904e\u5be6\u9a57\u89c0\u5bdf\u5230 wang2024learning \u76ee\u524d\u7684\u7d50\u8ad6\uff0c\n\u9019\u8868\u660e\u964d\u4f4e\u55ae\u4e00\u8a9e\u7fa9\u6703\u589e\u5f37\u6a21\u578b\u6548\u80fd\uff0c\u5728\u6a21\u578b\u6539\u8b8a\u6642\u4e0d\u6210\u7acb\u3002\u76f8\u53cd\uff0c\u6211\u5011\u8b49\u660e\u55ae\u4e00\u8a9e\u7fa9\u5728\u504f\u597d\u5c0d\u9f4a\u904e\u7a0b\u4e2d\u59cb\u7d42\u8207\u6a21\u578b\n\u5bb9\u91cf\u5448\u6b63\u76f8\u95dc\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5c07\u7279\u5fb5\u76f8\u95dc\u6027\u4f5c\u70ba\u55ae\u4e00\u8a9e\u7fa9\u7684\u4ee3\u7406\uff0c\u4e26\u5c07\u7279\u5fb5\n\u53bb\u76f8\u95dc\u6b63\u5247\u5316\u9805\u7d0d\u5165\u52d5\u614b\u504f\u597d\u6700\u4f73\u5316\u904e\u7a0b\u4e2d\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\n\u65b9\u6cd5\u4e0d\u50c5\u589e\u5f37\u4e86\u8868\u793a\u591a\u6a23\u6027\u548c\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u800c\u4e14\u9084\u6539\u5584\u4e86\u504f\u597d\u5c0d\u9f4a\u6548\u80fd\u3002", "author": "Hanqi Yan et.al.", "authors": "Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He", "id": "2406.17969v1", "paper_url": "http://arxiv.org/abs/2406.17969v1", "repo": "null"}}