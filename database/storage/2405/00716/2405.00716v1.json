{"2405.00716": {"publish_time": "2024-04-25", "title": "Large Language Models in Healthcare: A Comprehensive Benchmark", "paper_summary": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering task with answer options for evaluation. However, in real\nclinical settings, many clinical decisions, such as treatment recommendations,\ninvolve answering open-ended questions without pre-set options. Meanwhile,\nexisting studies mainly use accuracy to assess model performance. In this\npaper, we comprehensively benchmark diverse LLMs in healthcare, to clearly\nunderstand their strengths and weaknesses. Our benchmark contains seven tasks\nand thirteen datasets across medical language generation, understanding, and\nreasoning. We conduct a detailed evaluation of the existing sixteen LLMs in\nhealthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning\nsettings. We report the results on five metrics (i.e. matching, faithfulness,\ncomprehensiveness, generalizability, and robustness) that are critical in\nachieving trust from clinical users. We further invite medical experts to\nconduct human evaluation.", "paper_summary_zh": "", "author": "Andrew Liu et.al.", "authors": "Andrew Liu,Hongjian Zhou,Yining Hua,Omid Rohanian,Lei Clifton,David A. Clifton", "id": "2405.00716v1", "paper_url": "http://arxiv.org/abs/2405.00716v1", "repo": "null"}}