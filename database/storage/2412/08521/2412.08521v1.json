{"2412.08521": {"publish_time": "2024-12-11", "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance", "paper_summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6301\u7e8c\u9032\u6b65\uff0c\u5c0d\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u4e2d\u66f4\u9577\u5167\u5bb9\u7684\u9ad8\u54c1\u8cea\u548c\u66f4\u5feb\u901f\u8655\u7406\u7684\u9700\u6c42\u4e5f\u8207\u65e5\u4ff1\u589e\u3002KV \u5feb\u53d6\u88ab\u5ee3\u6cdb\u63a1\u7528\uff0c\u56e0\u70ba\u5b83\u5132\u5b58\u5148\u524d\u7522\u751f\u7684\u91d1\u9470\u548c\u503c\u6b0a\u6756\uff0c\u6709\u6548\u5730\u6e1b\u5c11\u63a8\u7406\u671f\u9593\u7684\u5197\u9918\u904b\u7b97\u3002\u7136\u800c\uff0c\u96a8\u8457\u8a18\u61b6\u9ad4\u958b\u92b7\u6210\u70ba\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\uff0cKV \u5feb\u53d6\u7684\u6709\u6548\u58d3\u7e2e\u4e5f\u8d8a\u4f86\u8d8a\u53d7\u5230\u91cd\u8996\u3002\u73fe\u6709\u7684\u65b9\u6cd5\u5927\u591a\u5f9e\u5169\u500b\u89d2\u5ea6\u9032\u884c\u58d3\u7e2e\uff1a\u8b58\u5225\u91cd\u8981\u6b0a\u6756\u548c\u8a2d\u8a08\u58d3\u7e2e\u7b56\u7565\u3002\u7136\u800c\uff0c\u7531\u65bc\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u6216\u4f4d\u7f6e\u7de8\u78bc\u7684\u5f71\u97ff\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u7522\u751f\u91cd\u8981\u6b0a\u6756\u7684\u504f\u5dee\u5206\u4f48\u3002\u6b64\u5916\uff0c\u5b83\u5011\u5ffd\u7565\u4e86\u4e0d\u540c\u982d\u90e8\u4e4b\u9593\u7684\u7a00\u758f\u6027\u548c\u5197\u9918\uff0c\u9019\u5c0e\u81f4\u96e3\u4ee5\u5728\u982d\u90e8\u5c64\u7d1a\u4fdd\u7559\u6700\u6709\u6548\u7684\u8cc7\u8a0a\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa EMS \u4f86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u540c\u6642\u5728\u6975\u7aef\u7684\u58d3\u7e2e\u6bd4\u4e0b\u5be6\u73fe\u66f4\u597d\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7d50\u5408\u4f86\u81ea\u5168\u5c40\u548c\u5c40\u90e8 KV \u6b0a\u6756\u7684\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u7684\u5168\u5c40\u5c40\u90e8\u5206\u6578\uff0c\u4ee5\u66f4\u597d\u5730\u8b58\u5225\u6b0a\u6756\u7684\u91cd\u8981\u6027\u3002\u5c0d\u65bc\u58d3\u7e2e\u7b56\u7565\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u9069\u61c9\u6027\u548c\u7d71\u4e00\u6027\u7684\u9010\u51fa\u518d\u5408\u4f75\u67b6\u69cb\uff0c\u5b83\u8003\u91cf\u4e86\u4e0d\u540c\u982d\u90e8\u4e2d KV \u6b0a\u6756\u7684\u7a00\u758f\u6027\u548c\u5197\u9918\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u96f6\u985e\u5225\u6a5f\u5236\u5be6\u73fe\u982d\u90e8\u660e\u667a\u7684\u5e73\u884c\u58d3\u7e2e\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u5373\u4f7f\u5728\u6975\u7aef\u7684\u58d3\u7e2e\u6bd4\u4e0b\u4e5f\u80fd\u9054\u5230 SOTA \u6548\u80fd\u3002\u5728 LongBench \u4e0a\u7684\u56db\u500b LLM \u4e2d\uff0cEMS \u5728 256 \u5feb\u53d6\u9810\u7b97\u4e0b\u6301\u7e8c\u5be6\u73fe\u6700\u4f4e\u56f0\u60d1\u5ea6\uff0c\u5c07\u5206\u6578\u63d0\u9ad8\u4e86 1.28 \u5206\u4ee5\u4e0a\uff0c\u4e26\u5728 Needle-in-a-Haystack \u4efb\u52d9\u4e2d\u4ee5\u5c0f\u65bc\u5167\u5bb9\u9577\u5ea6 2% \u7684\u5feb\u53d6\u9810\u7b97\u4fdd\u7559\u4e86 95% \u7684\u6aa2\u7d22\u6e96\u78ba\u5ea6\u3002", "author": "Yingxin Li et.al.", "authors": "Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang", "id": "2412.08521v1", "paper_url": "http://arxiv.org/abs/2412.08521v1", "repo": "null"}}