{"2412.07412": {"publish_time": "2024-12-10", "title": "Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT", "paper_summary": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements.", "paper_summary_zh": "\u77e5\u8b58\u5716\u8b5c (KG) \u5c0d\u65bc GraphRAG \u7684\u529f\u80fd\u81f3\u95dc\u91cd\u8981\uff0cGraphRAG \u662f\u4e00\u7a2e\u6aa2\u7d22\u589e\u5f37\u5f0f\u751f\u6210\u7cfb\u7d71 (RAG)\uff0c\u5728\u9700\u8981\u7d50\u69cb\u5316\u63a8\u7406\u548c\u8a9e\u7fa9\u7406\u89e3\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0c\u7531\u65bc\u50b3\u7d71\u65b9\u6cd5\u7684\u6e96\u78ba\u6027\u548c\u53ef\u64f4\u5145\u6027\u9650\u5236\uff0c\u70ba GraphRAG \u5efa\u7acb KG \u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u5275\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 GPT-4\u3001LLaMA 2 (13B) \u548c BERT\uff0c\u76f4\u63a5\u5f9e\u975e\u7d50\u69cb\u5316\u6578\u64da\u751f\u6210 KG\uff0c\u7e5e\u904e\u50b3\u7d71\u7ba1\u9053\u3002\u6211\u5011\u4f7f\u7528\u6e96\u78ba\u5ea6\u3001\u53ec\u56de\u7387\u3001F1 \u5206\u6578\u3001\u5716\u5f62\u7de8\u8f2f\u8ddd\u96e2\u548c\u8a9e\u7fa9\u76f8\u4f3c\u6027\u7b49\u6307\u6a19\uff0c\u8a55\u4f30\u6a21\u578b\u751f\u6210\u9ad8\u54c1\u8cea KG \u7684\u80fd\u529b\u3002\u7d50\u679c\u8868\u660e\uff0cGPT-4 \u9054\u5230\u4e86\u5353\u8d8a\u7684\u8a9e\u7fa9\u4fdd\u771f\u5ea6\u548c\u7d50\u69cb\u6e96\u78ba\u6027\uff0cLLaMA 2 \u5728\u8f15\u91cf\u7d1a\u3001\u7279\u5b9a\u9818\u57df\u7684\u5716\u5f62\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u800c BERT \u5247\u63d0\u4f9b\u4e86\u5c0d\u5be6\u9ad4\u95dc\u4fc2\u5efa\u6a21\u6311\u6230\u7684\u898b\u89e3\u3002\u9019\u9805\u7814\u7a76\u5f37\u8abf\u4e86 LLM \u7c21\u5316 KG \u5efa\u7acb\u548c\u589e\u5f37 GraphRAG \u5728\u73fe\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\u53ef\u53ca\u6027\u7684\u6f5b\u529b\uff0c\u540c\u6642\u70ba\u672a\u4f86\u7684\u9032\u5c55\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Ahan Bhatt et.al.", "authors": "Ahan Bhatt, Nandan Vaghela, Kush Dudhia", "id": "2412.07412v1", "paper_url": "http://arxiv.org/abs/2412.07412v1", "repo": "null"}}