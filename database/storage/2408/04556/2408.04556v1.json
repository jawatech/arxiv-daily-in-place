{"2408.04556": {"publish_time": "2024-08-08", "title": "Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models", "paper_summary": "Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8981\u5c07 LLM \u8abf\u6574\u5230\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u901a\u5e38\u9700\u8981\u8a08\u7b97\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u8a18\u61b6\u9ad4\u7684\u5fae\u8abf\u7a0b\u5e8f\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u8ca0\u64d4\uff0c\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u6280\u8853\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u91dd\u5c0d LLM \u9032\u884c\u5ba2\u88fd\u5316\uff0c\u4e14\u8a08\u7b97\u8ca0\u64d4\u6700\u5c0f\u3002\u5118\u7ba1 PEFT \u65b9\u6cd5\u5177\u6709\u986f\u8457\u7684\u512a\u52e2\uff0c\u4f46\u5b83\u5011\u4e26\u672a\u5b8c\u5168\u89e3\u6c7a\u9810\u8a13\u7df4\u8cc7\u6599\u4e2d\u504f\u5dee\u50b3\u64ad\u7684\u666e\u904d\u554f\u984c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5177\u5099\u504f\u5dee\u611f\u77e5\u80fd\u529b\u7684\u4f4e\u79e9\u9069\u61c9 (BA-LoRA)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684 PEFT \u65b9\u6cd5\uff0c\u65e8\u5728\u5c0d\u6297\u504f\u5dee\u907a\u50b3\u3002BA-LoRA \u7d50\u5408\u4e86\u4e09\u500b\u4e0d\u540c\u7684\u6b63\u5247\u5316\u9805\u76ee\uff1a(1) \u4e00\u81f4\u6027\u6b63\u5247\u5316\u5668\u3001(2) \u591a\u6a23\u6027\u6b63\u5247\u5316\u5668\uff0c\u4ee5\u53ca (3) \u5947\u7570\u503c\u5206\u89e3\u6b63\u5247\u5316\u5668\u3002\u9019\u4e9b\u6b63\u5247\u5316\u5668\u5171\u540c\u65e8\u5728\u6539\u5584\u751f\u6210\u6a21\u578b\u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u3001\u591a\u6a23\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u900f\u904e\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u548c\u81ea\u7136\u8a9e\u8a00\u751f\u6210 (NLG) \u4efb\u52d9\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u63a1\u7528 LLaMA\u3001Mistral \u548c Gemma \u7b49\u8457\u540d\u7684 LLM\uff0c\u6211\u5011\u8b49\u660e\u4e86 BA-LoRA \u8d85\u8d8a\u4e86 LoRA \u53ca\u5176\u6700\u5148\u9032\u7684\u8b8a\u9ad4\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u6709\u6548\u6e1b\u8f15\u4e86\u9810\u8a13\u7df4\u504f\u5dee\u7684\u6709\u5bb3\u5f71\u97ff\uff0c\u9032\u800c\u7522\u751f\u66f4\u53ef\u9760\u4e14\u7a69\u5065\u7684\u6a21\u578b\u8f38\u51fa\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/cyp-jlu-ai/BA-LoRA \u53d6\u5f97\u3002", "author": "Yupeng Chang et.al.", "authors": "Yupeng Chang, Yi Chang, Yuan Wu", "id": "2408.04556v1", "paper_url": "http://arxiv.org/abs/2408.04556v1", "repo": "https://github.com/cyp-jlu-ai/ba-lora"}}