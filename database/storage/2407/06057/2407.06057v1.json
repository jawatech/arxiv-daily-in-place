{"2407.06057": {"publish_time": "2024-07-08", "title": "Variational Best-of-N Alignment", "paper_summary": "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective.", "paper_summary_zh": "\u6700\u4f73 N (BoN) \u662f\u4e00\u7a2e\u5ee3\u53d7\u6b61\u8fce\u4e14\u6709\u6548\u7684\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5c07\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u504f\u597d\u76f8\u7b26\u3002\u6b64\u6f14\u7b97\u6cd5\u7684\u904b\u4f5c\u65b9\u5f0f\u5982\u4e0b\uff1a\u5728\u63a8\u8ad6\u6642\u9593\uff0c\u5f9e\u8a9e\u8a00\u6a21\u578b\u4e2d\u62bd\u51fa N \u500b\u6a23\u672c\uff0c\u4e26\u6839\u64da\u734e\u52f5\u6a21\u578b\u7684\u5224\u65b7\uff0c\u5c07\u5177\u6709\u6700\u9ad8\u734e\u52f5\u7684\u6a23\u672c\u56de\u50b3\u70ba\u8f38\u51fa\u3002\u5118\u7ba1 BoN \u5f88\u6709\u6548\uff0c\u4f46\u5176\u8a08\u7b97\u6210\u672c\u5f88\u9ad8\uff1b\u5b83\u6703\u5c07\u53d6\u6a23\u8655\u7406\u91cf\u964d\u4f4e N \u500d\u3002\u70ba\u4e86\u8b93 BoN \u5728\u63a8\u8ad6\u6642\u9593\u66f4\u6709\u6548\u7387\uff0c\u5176\u4e2d\u4e00\u500b\u7b56\u7565\u662f\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u6a21\u64ec BoN \u5728\u63a8\u8ad6\u671f\u9593\u57f7\u884c\u7684\u52d5\u4f5c\u3002\u70ba\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u5c0e\u51fa BoN \u6f14\u7b97\u6cd5\u6240\u5f15\u767c\u7684\u5206\u914d\u3002\u63a5\u8457\uff0c\u6211\u5011\u5efa\u8b70\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u6700\u5c0f\u5316\u5230 BoN \u5206\u914d\u7684\u53cd\u5411 KL \u5206\u6b67\u3002\u6211\u5011\u7684\u505a\u6cd5\u985e\u4f3c\u65bc\u5e73\u5747\u5834\u8b8a\u5206\u63a8\u8ad6\uff0c\u56e0\u6b64\u6211\u5011\u7a31\u4e4b\u70ba\u8b8a\u7570 BoN (vBoN)\u3002\u5728\u5fae\u8abf\u6210\u529f\u4e14\u6211\u5011\u7372\u5f97\u826f\u597d\u8fd1\u4f3c\u503c\u7684\u7bc4\u570d\u5167\uff0c\u6211\u5011\u5df2\u5c07\u63a8\u8ad6\u6210\u672c\u964d\u4f4e N \u500d\u3002\u6211\u5011\u5728\u53d7\u63a7\u751f\u6210\u4efb\u52d9\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0c\u96d6\u7136\u8b8a\u7570 BoN \u5728\u8abf\u6574\u8a9e\u8a00\u6a21\u578b\u65b9\u9762\u4e0d\u5982 BoN \u6709\u6548\uff0c\u4f46\u5176\u63a5\u8fd1 BoN \u7684\u6548\u80fd\uff0c\u56e0\u70ba\u8207\u4f7f\u7528\u53d7 KL \u7d04\u675f\u7684 RL \u76ee\u6a19\u8a13\u7df4\u7684\u6a21\u578b\u76f8\u6bd4\uff0cvBoN \u66f4\u5e38\u51fa\u73fe\u5728\u734e\u52f5\u548c KL \u5206\u6b67\u7684\u5e15\u7d2f\u6258\u524d\u7de3\u3002", "author": "Afra Amini et.al.", "authors": "Afra Amini, Tim Vieira, Ryan Cotterell", "id": "2407.06057v1", "paper_url": "http://arxiv.org/abs/2407.06057v1", "repo": "null"}}