{"2407.02783": {"publish_time": "2024-07-03", "title": "52B to 1T: Lessons Learned via Tele-FLM Series", "paper_summary": "Large Language Models (LLMs) represent a significant stride toward Artificial\nGeneral Intelligence. As scaling laws underscore the potential of increasing\nmodel sizes, the academic community has intensified its investigations into\nLLMs with capacities exceeding 50 billion parameters. This technical report\nbuilds on our prior work with Tele-FLM (also known as FLM-2), a publicly\navailable 52-billion-parameter model. We delve into two primary areas: we first\ndiscuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which\nsupports the \"less is more\" approach for SFT data construction; second, we\ndemonstrate our experiments and analyses on the best practices for\nprogressively growing a model from 52 billion to 102 billion, and subsequently\nto 1 trillion parameters. We will open-source a 1T model checkpoint, namely\nTele-FLM-1T, to advance further training and research.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee3\u8868\u8457\u671d\u5411\u4eba\u5de5\u901a\u7528\u667a\u6167\u9081\u9032\u4e00\u5927\u6b65\u3002\u7531\u65bc\u898f\u6a21\u5b9a\u5f8b\u5f37\u8abf\u4e86\u589e\u52a0\u6a21\u578b\u898f\u6a21\u7684\u6f5b\u529b\uff0c\u5b78\u8853\u754c\u5df2\u52a0\u5f37\u5176\u5c0d\u5bb9\u91cf\u8d85\u904e 500 \u5104\u500b\u53c3\u6578\u7684 LLM \u7684\u7814\u7a76\u3002\u9019\u4efd\u6280\u8853\u5831\u544a\u5efa\u7acb\u5728\u6211\u5011\u5148\u524d\u4f7f\u7528 Tele-FLM\uff08\u4e5f\u7a31\u70ba FLM-2\uff09\u7684\u7814\u7a76\u4e4b\u4e0a\uff0cTele-FLM \u662f\u4e00\u500b\u516c\u958b\u53ef\u7528\u7684 520 \u5104\u500b\u53c3\u6578\u6a21\u578b\u3002\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u5169\u500b\u4e3b\u8981\u9818\u57df\uff1a\u6211\u5011\u9996\u5148\u8a0e\u8ad6\u6211\u5011\u5c0d Tele-FLM-52B \u4e0a\u7684\u76e3\u7763\u5fae\u8abf (SFT) \u7684\u89c0\u5bdf\uff0c\u9019\u652f\u6301\u4e86 SFT \u8cc7\u6599\u5efa\u69cb\u7684\u300c\u5c11\u5373\u662f\u591a\u300d\u65b9\u6cd5\uff1b\u5176\u6b21\uff0c\u6211\u5011\u5c55\u793a\u6211\u5011\u5c0d\u5f9e 520 \u5104\u500b\u6a21\u578b\u9010\u6b65\u589e\u52a0\u5230 1020 \u5104\u500b\uff0c\u518d\u5230 1 \u5146\u500b\u53c3\u6578\u7684\u6700\u4f73\u5be6\u52d9\u7684\u5be6\u9a57\u548c\u5206\u6790\u3002\u6211\u5011\u5c07\u958b\u6e90\u4e00\u500b 1T \u6a21\u578b\u6aa2\u67e5\u9ede\uff0c\u5373 Tele-FLM-1T\uff0c\u4ee5\u63a8\u9032\u9032\u4e00\u6b65\u7684\u8a13\u7df4\u548c\u7814\u7a76\u3002", "author": "Xiang Li et.al.", "authors": "Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu, Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li, Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang, Xuelong Li, Tiejun Huang", "id": "2407.02783v1", "paper_url": "http://arxiv.org/abs/2407.02783v1", "repo": "null"}}