{"2408.06520": {"publish_time": "2024-08-12", "title": "Hierarchical in-Context Reinforcement Learning with Hindsight Modular Reflections for Planning", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nHierarchical in-Context Reinforcement Learning (HCRL), a novel framework that\ndecomposes complex tasks into sub-tasks using an LLM-based high-level policy,\nin which a complex task is decomposed into sub-tasks by a high-level policy\non-the-fly. The sub-tasks, defined by goals, are assigned to the low-level\npolicy to complete. Once the LLM agent determines that the goal is finished, a\nnew goal will be proposed. To improve the agent's performance in multi-episode\nexecution, we propose Hindsight Modular Reflection (HMR), where, instead of\nreflecting on the full trajectory, we replace the task objective with\nintermediate goals and let the agent reflect on shorter trajectories to improve\nreflection efficiency. We evaluate the decision-making ability of the proposed\nHCRL in three benchmark environments--ALFWorld, Webshop, and HotpotQA. Results\nshow that HCRL can achieve 9%, 42%, and 10% performance improvement in 5\nepisodes of execution over strong in-context learning baselines.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u70ba\u6a5f\u5668\u4eba\u6c7a\u7b56\u7684\u6f5b\u5728\u5019\u9078\u8005\u3002\u53d7\u5206\u5c64\u5f37\u5316\u5b78\u7fd2 (HRL) \u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u5206\u5c64\u60c5\u5883\u5f37\u5316\u5b78\u7fd2 (HCRL)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u57fa\u65bc LLM \u7684\u9ad8\u968e\u653f\u7b56\u5c07\u8907\u96dc\u4efb\u52d9\u5206\u89e3\u70ba\u5b50\u4efb\u52d9\uff0c\u5176\u4e2d\u8907\u96dc\u4efb\u52d9\u7531\u9ad8\u968e\u653f\u7b56\u5373\u6642\u5206\u89e3\u70ba\u5b50\u4efb\u52d9\u3002\u7531\u76ee\u6a19\u5b9a\u7fa9\u7684\u5b50\u4efb\u52d9\u88ab\u5206\u914d\u7d66\u4f4e\u968e\u653f\u7b56\u4ee5\u5b8c\u6210\u3002\u4e00\u65e6 LLM \u4ee3\u7406\u78ba\u5b9a\u76ee\u6a19\u5df2\u5b8c\u6210\uff0c\u5c07\u63d0\u51fa\u4e00\u500b\u65b0\u76ee\u6a19\u3002\u70ba\u4e86\u63d0\u9ad8\u4ee3\u7406\u5728\u591a\u60c5\u5883\u57f7\u884c\u4e2d\u7684\u8868\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u56de\u9867\u6027\u6a21\u7d44\u5316\u53cd\u601d (HMR)\uff0c\u5176\u4e2d\uff0c\u6211\u5011\u7528\u4e2d\u9593\u76ee\u6a19\u53d6\u4ee3\u4efb\u52d9\u76ee\u6a19\uff0c\u4e26\u8b93\u4ee3\u7406\u56de\u9867\u8f03\u77ed\u7684\u8ecc\u8de1\u4ee5\u63d0\u9ad8\u53cd\u601d\u6548\u7387\uff0c\u800c\u4e0d\u662f\u56de\u9867\u5b8c\u6574\u7684\u8ecc\u8de1\u3002\u6211\u5011\u5728\u4e09\u500b\u57fa\u6e96\u74b0\u5883\u4e2d\u8a55\u4f30\u4e86\u6240\u63d0\u51fa\u7684 HCRL \u7684\u6c7a\u7b56\u80fd\u529b\u2014\u2014ALFWorld\u3001Webshop \u548c HotpotQA\u3002\u7d50\u679c\u986f\u793a\uff0cHCRL \u5728 5 \u500b\u57f7\u884c\u60c5\u5883\u4e2d\uff0c\u76f8\u8f03\u65bc\u5f37\u5927\u7684\u60c5\u5883\u5b78\u7fd2\u57fa\u6e96\uff0c\u53ef\u4ee5\u5c07\u6548\u80fd\u63d0\u5347 9%\u300142% \u548c 10%\u3002", "author": "Chuanneng Sun et.al.", "authors": "Chuanneng Sun, Songjun Huang, Dario Pompili", "id": "2408.06520v1", "paper_url": "http://arxiv.org/abs/2408.06520v1", "repo": "null"}}