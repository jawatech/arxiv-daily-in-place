{"2406.06512": {"publish_time": "2024-06-10", "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography", "paper_summary": "Over 85 million computed tomography (CT) scans are performed annually in the\nUS, of which approximately one quarter focus on the abdomen. Given the current\nradiologist shortage, there is a large impetus to use artificial intelligence\nto alleviate the burden of interpreting these complex imaging studies. Prior\nstate-of-the-art approaches for automated medical image interpretation leverage\nvision language models (VLMs). However, current medical VLMs are generally\nlimited to 2D images and short reports, and do not leverage electronic health\nrecord (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train\nusing paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes\n(1.8+ million codes), and radiology reports (6+ million tokens). We evaluate\nMerlin on 6 task types and 752 individual tasks. The non-adapted\n(off-the-shelf) tasks include zero-shot findings classification (31 findings),\nphenotype classification (692 phenotypes), and zero-shot cross-modal retrieval\n(image to findings and image to impressions), while model adapted tasks include\n5-year disease prediction (6 diseases), radiology report generation, and 3D\nsemantic segmentation (20 organs). We perform internal validation on a test set\nof 5,137 CTs, and external validation on 7,000 clinical CTs and on two public\nCT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant\nevaluations, we assess the efficacy of various network architectures and\ntraining strategies to depict that Merlin has favorable performance to existing\ntask-specific baselines. We derive data scaling laws to empirically assess\ntraining data needs for requisite downstream task performance. Furthermore,\nunlike conventional VLMs that require hundreds of GPUs for training, we perform\nall training on a single GPU.", "paper_summary_zh": "<paragraph>\u7f8e\u570b\u6bcf\u5e74\u57f7\u884c\u8d85\u904e 8500 \u842c\u6b21\u96fb\u8166\u65b7\u5c64\u6383\u63cf (CT)\uff0c\u5176\u4e2d\u7d04\u56db\u5206\u4e4b\u4e00\u91dd\u5c0d\u8179\u90e8\u3002\u9451\u65bc\u76ee\u524d\u653e\u5c04\u79d1\u91ab\u5e2b\u77ed\u7f3a\uff0c\u56e0\u6b64\u6709\u5f88\u5927\u7684\u52d5\u529b\u4f7f\u7528\u4eba\u5de5\u667a\u6167\u4f86\u6e1b\u8f15\u8a6e\u91cb\u9019\u4e9b\u8907\u96dc\u5f71\u50cf\u7814\u7a76\u7684\u8ca0\u64d4\u3002\u5148\u524d\u81ea\u52d5\u5316\u91ab\u5b78\u5f71\u50cf\u8a6e\u91cb\u7684\u6700\u65b0\u65b9\u6cd5\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u91ab\u5b78 VLM \u901a\u5e38\u50c5\u9650\u65bc 2D \u5f71\u50cf\u548c\u7c21\u77ed\u5831\u544a\uff0c\u800c\u4e14\u4e0d\u6703\u5229\u7528\u96fb\u5b50\u5065\u5eb7\u7d00\u9304 (EHR) \u8cc7\u6599\u9032\u884c\u76e3\u7763\u3002\u6211\u5011\u4ecb\u7d39 Merlin\uff0c\u9019\u662f\u4e00\u500b 3D VLM\uff0c\u6211\u5011\u4f7f\u7528\u914d\u5c0d\u7684 CT \u6383\u63cf\uff08\u4f86\u81ea 15,331 \u500b CT \u7684 600 \u591a\u842c\u5f35\u5f71\u50cf\uff09\u3001EHR \u8a3a\u65b7\u78bc\uff08180 \u591a\u842c\u500b\u78bc\uff09\u548c\u653e\u5c04\u79d1\u5831\u544a\uff08600 \u591a\u842c\u500b\u4ee3\u78bc\uff09\u4f86\u8a13\u7df4\u5b83\u3002\u6211\u5011\u5728 6 \u500b\u4efb\u52d9\u985e\u578b\u548c 752 \u500b\u500b\u5225\u4efb\u52d9\u4e0a\u8a55\u4f30 Merlin\u3002\u975e\u9069\u61c9\u578b\uff08\u73fe\u6210\u7684\uff09\u4efb\u52d9\u5305\u62ec\u96f6\u6b21\u5b78\u7fd2\u7d50\u679c\u5206\u985e\uff0831 \u500b\u7d50\u679c\uff09\u3001\u8868\u578b\u5206\u985e\uff08692 \u500b\u8868\u578b\uff09\u548c\u96f6\u6b21\u5b78\u7fd2\u8de8\u6a21\u614b\u6aa2\u7d22\uff08\u5f71\u50cf\u5230\u7d50\u679c\u548c\u5f71\u50cf\u5230\u5370\u8c61\uff09\uff0c\u800c\u6a21\u578b\u9069\u61c9\u4efb\u52d9\u5305\u62ec 5 \u5e74\u75be\u75c5\u9810\u6e2c\uff086 \u7a2e\u75be\u75c5\uff09\u3001\u653e\u5c04\u79d1\u5831\u544a\u7522\u751f\u548c 3D \u8a9e\u610f\u5206\u5272\uff0820 \u500b\u5668\u5b98\uff09\u3002\u6211\u5011\u5728 5,137 \u500b CT \u7684\u6e2c\u8a66\u96c6\u4e0a\u57f7\u884c\u5167\u90e8\u9a57\u8b49\uff0c\u4e26\u5728 7,000 \u500b\u81e8\u5e8a CT \u548c\u5169\u500b\u516c\u958b CT \u8cc7\u6599\u96c6\uff08VerSe\u3001TotalSegmentator\uff09\u4e0a\u57f7\u884c\u5916\u90e8\u9a57\u8b49\u3002\u9664\u4e86\u9019\u4e9b\u8207\u81e8\u5e8a\u76f8\u95dc\u7684\u8a55\u4f30\u4e4b\u5916\uff0c\u6211\u5011\u9084\u8a55\u4f30\u5404\u7a2e\u7db2\u8def\u67b6\u69cb\u548c\u8a13\u7df4\u7b56\u7565\u7684\u6548\u80fd\uff0c\u4ee5\u8aaa\u660e Merlin \u5728\u73fe\u6709\u7684\u7279\u5b9a\u4efb\u52d9\u57fa\u7dda\u4e0a\u5177\u6709\u826f\u597d\u7684\u6548\u80fd\u3002\u6211\u5011\u63a8\u5c0e\u51fa\u8cc7\u6599\u64f4\u5145\u6cd5\u5247\uff0c\u4ee5\u6839\u64da\u7d93\u9a57\u8a55\u4f30\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u6240\u9700\u7684\u8a13\u7df4\u8cc7\u6599\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8207\u9700\u8981\u6578\u767e\u500b GPU \u624d\u80fd\u9032\u884c\u8a13\u7df4\u7684\u50b3\u7d71 VLM \u4e0d\u540c\uff0c\u6211\u5011\u5728\u55ae\u4e00 GPU \u4e0a\u57f7\u884c\u6240\u6709\u8a13\u7df4\u3002</paragraph>", "author": "Louis Blankemeier et.al.", "authors": "Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, Christian Bluethgen, Malte Engmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, Jason Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, Akshay S. Chaudhari", "id": "2406.06512v1", "paper_url": "http://arxiv.org/abs/2406.06512v1", "repo": "null"}}