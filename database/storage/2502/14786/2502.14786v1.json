{"2502.14786": {"publish_time": "2025-02-20", "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "paper_summary": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).", "paper_summary_zh": "\u6211\u5011\u63a8\u51fa\u4e86 SigLIP 2\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u591a\u8a9e\u8a00\u8996\u89ba\u8a9e\u8a00\u7de8\u78bc\u5668\u7cfb\u5217\uff0c\u5b83\u5efa\u7acb\u5728 SigLIP \u7684\u6210\u529f\u57fa\u790e\u4e0a\u3002\u5728\u9019\u500b\u7b2c\u4e8c\u500b\u7248\u672c\u4e2d\uff0c\u6211\u5011\u5c07\u539f\u4f86\u7684\u5716\u50cf\u6587\u5b57\u8a13\u7df4\u76ee\u6a19\u8207\u5e7e\u500b\u5148\u524d\u7368\u7acb\u958b\u767c\u7684\u6280\u8853\u64f4\u5c55\u5230\u4e00\u500b\u7d71\u4e00\u7684\u914d\u65b9\u4e2d\uff0c\u5176\u4e2d\u5305\u62ec\u57fa\u65bc\u6a19\u984c\u7684\u9810\u8a13\u7df4\u3001\u81ea\u6211\u76e3\u7763\u640d\u5931\uff08\u81ea\u6211\u84b8\u993e\u3001\u906e\u7f69\u9810\u6e2c\uff09\u548c\u7dda\u4e0a\u6578\u64da\u7b56\u5c55\u3002\u6709\u4e86\u9019\u4e9b\u6539\u8b8a\uff0cSigLIP 2 \u6a21\u578b\u5728\u6240\u6709\u6a21\u578b\u898f\u6a21\u4e0a\u90fd\u8d85\u8d8a\u4e86 SigLIP \u7684\u5c0d\u61c9\u6a21\u578b\uff0c\u5305\u62ec\u96f6\u6b21\u5206\u985e\u3001\u5716\u50cf\u6587\u5b57\u6aa2\u7d22\u548c\u5728\u70ba\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u63d0\u53d6\u8996\u89ba\u8868\u793a\u6642\u50b3\u8f38\u6548\u80fd\u3002\u6b64\u5916\uff0c\u65b0\u7684\u8a13\u7df4\u914d\u65b9\u4e5f\u5927\u5e45\u6539\u5584\u4e86\u5b9a\u4f4d\u548c\u5bc6\u96c6\u9810\u6e2c\u4efb\u52d9\u3002\u6211\u5011\u9084\u8a13\u7df4\u4e86\u652f\u63f4\u591a\u7a2e\u89e3\u6790\u5ea6\u548c\u4fdd\u7559\u8f38\u5165\u539f\u751f\u9577\u5bec\u6bd4\u7684\u8b8a\u9ad4\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u4e00\u500b\u66f4\u70ba\u591a\u6a23\u5316\u7684\u6578\u64da\u7d44\u5408\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u5176\u4e2d\u5305\u62ec\u53bb\u504f\u898b\u6280\u8853\uff0c\u5f9e\u800c\u5927\u5e45\u63d0\u5347\u591a\u8a9e\u8a00\u7406\u89e3\u529b\u4e26\u6539\u5584\u516c\u5e73\u6027\u3002\u70ba\u4e86\u8b93\u4f7f\u7528\u8005\u6b0a\u8861\u63a8\u7406\u6210\u672c\u8207\u6548\u80fd\uff0c\u6211\u5011\u767c\u5e03\u4e86\u56db\u7a2e\u5927\u5c0f\u7684\u6a21\u578b\u6aa2\u67e5\u9ede\uff1aViT-B (86M)\u3001L (303M)\u3001So400m (400M) \u548c g (1B)\u3002", "author": "Michael Tschannen et.al.", "authors": "Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H\u00e9naff, Jeremiah Harmsen, Andreas Steiner, Xiaohua Zhai", "id": "2502.14786v1", "paper_url": "http://arxiv.org/abs/2502.14786v1", "repo": "null"}}