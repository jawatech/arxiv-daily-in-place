{"2405.20973": {"publish_time": "2024-05-31", "title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "paper_summary": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u5728\u8a31\u591a\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u8868\u73fe\u3002\u7136\u800c\uff0cLLM \u9ad8\u6602\u7684\u5132\u5b58\u548c\u904b\u7b97\u6210\u672c\u5df2\u6210\u70ba\u90e8\u7f72 LLM \u7684\u4e00\u9805\u6311\u6230\u3002\u6b0a\u91cd\u91cf\u5316\u5df2\u5ee3\u6cdb\u7528\u65bc\u6a21\u578b\u58d3\u7e2e\uff0c\u9019\u53ef\u4ee5\u540c\u6642\u964d\u4f4e\u5132\u5b58\u548c\u904b\u7b97\u6210\u672c\u3002\u73fe\u6709\u91dd\u5c0d LLM \u7684\u6b0a\u91cd\u91cf\u5316\u65b9\u6cd5\u5927\u591a\u4f7f\u7528\u79e9\u4e00\u78bc\u672c\u9032\u884c\u91cf\u5316\uff0c\u9019\u6703\u5728\u58d3\u7e2e\u7387\u8f03\u9ad8\u6642\u5c0e\u81f4\u986f\u8457\u7684\u6e96\u78ba\u5ea6\u640d\u5931\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u57fa\u65bc\u4f4e\u79e9\u78bc\u672c\u91cf\u5316\u7684 (LCQ) \u65b0\u7a4e\u6b0a\u91cd\u91cf\u5316\u65b9\u6cd5\uff0c\u9069\u7528\u65bc LLM\u3002LCQ \u63a1\u7528\u79e9\u4f4e\u65bc\u4e00\u4e14\u79e9\u53ef\u4ee5\u5927\u65bc\u4e00\u7684\u4f4e\u79e9\u78bc\u672c\u9032\u884c\u91cf\u5316\u3002\u5be6\u9a57\u986f\u793a\uff0cLCQ \u53ef\u4ee5\u6bd4\u73fe\u6709\u65b9\u6cd5\u7372\u5f97\u66f4\u597d\u7684\u6e96\u78ba\u5ea6\uff0c\u4e14\u5132\u5b58\u6210\u672c\u5e7e\u4e4e\u6c92\u6709\u589e\u52a0\u3002", "author": "Wen-Pu Cai et.al.", "authors": "Wen-Pu Cai, Wu-Jun Li", "id": "2405.20973v1", "paper_url": "http://arxiv.org/abs/2405.20973v1", "repo": "null"}}