{"2410.24049": {"publish_time": "2024-10-31", "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs", "paper_summary": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u7531\u65bc\u5d4c\u5165\u7684\u793e\u6703\u504f\u898b\u800c\u5f15\u8d77\u9053\u5fb7\u554f\u984c\u3002\u672c\u7814\u7a76\u6aa2\u8996 LLM \u5c0d\u963f\u62c9\u4f2f\u4eba\u8207\u897f\u65b9\u4eba\u4e4b\u9593\u7684\u504f\u898b\uff0c\u6db5\u84cb\u516b\u500b\u9818\u57df\uff0c\u5305\u62ec\u5a66\u5973\u6b0a\u5229\u3001\u6050\u6016\u4e3b\u7fa9\u548c\u53cd\u7336\u592a\u4e3b\u7fa9\uff0c\u4e26\u8a55\u4f30\u6a21\u578b\u5c0d\u5ef6\u7e8c\u9019\u4e9b\u504f\u898b\u7684\u62b5\u6297\u529b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5efa\u7acb\u5169\u500b\u6578\u64da\u96c6\uff1a\u4e00\u500b\u7528\u65bc\u8a55\u4f30 LLM \u5c0d\u963f\u62c9\u4f2f\u4eba\u8207\u897f\u65b9\u4eba\u7684\u504f\u898b\uff0c\u53e6\u4e00\u500b\u7528\u65bc\u6e2c\u8a66\u6a21\u578b\u5c0d\u8a87\u5927\u8ca0\u9762\u7279\u8cea\uff08\u300c\u8d8a\u7344\u300d\uff09\u63d0\u793a\u7684\u5b89\u5168\u6027\u3002\u6211\u5011\u8a55\u4f30\u4e86\u516d\u500b LLM\u2014\u2014GPT-4\u3001GPT-4o\u3001LlaMA 3.1\uff088B \u548c 405B\uff09\u3001Mistral 7B \u548c Claude 3.5 Sonnet\u3002\u6211\u5011\u767c\u73fe 79% \u7684\u6848\u4f8b\u5c0d\u963f\u62c9\u4f2f\u4eba\u8868\u73fe\u51fa\u8ca0\u9762\u504f\u898b\uff0c\u5176\u4e2d LlaMA 3.1-405B \u6700\u70ba\u504f\u9817\u3002\u6211\u5011\u7684\u8d8a\u7344\u6e2c\u8a66\u986f\u793a\uff0c\u5118\u7ba1 GPT-4o \u662f\u7d93\u904e\u6700\u4f73\u5316\u7684\u7248\u672c\uff0c\u4f46\u5b83\u662f\u6700\u8106\u5f31\u7684\uff0c\u5176\u6b21\u662f LlaMA 3.1-8B \u548c Mistral 7B\u3002\u9664\u4e86 Claude \u4e4b\u5916\uff0c\u6240\u6709 LLM \u5728\u4e09\u985e\u4e2d\u7684\u653b\u64ca\u6210\u529f\u7387\u90fd\u9ad8\u65bc 87%\u3002\u6211\u5011\u9084\u767c\u73fe Claude 3.5 Sonnet \u6700\u5b89\u5168\uff0c\u4f46\u5b83\u5728\u516b\u985e\u4e2d\u7684\u4e03\u985e\u4e2d\u4ecd\u8868\u73fe\u51fa\u504f\u898b\u3002\u5118\u7ba1 GPT-4o \u662f GPT4 \u7684\u6700\u4f73\u5316\u7248\u672c\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5b83\u66f4\u5bb9\u6613\u51fa\u73fe\u504f\u898b\u548c\u8d8a\u7344\uff0c\u9019\u8868\u660e\u5b58\u5728\u6700\u4f73\u5316\u7f3a\u9677\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5f37\u8abf\u4e86\u5c0d LLM \u4e2d\u66f4\u5f37\u5927\u7684\u504f\u898b\u7de9\u89e3\u7b56\u7565\u548c\u66f4\u56b4\u683c\u7684\u5b89\u5168\u63aa\u65bd\u7684\u8feb\u5207\u9700\u6c42\u3002", "author": "Muhammed Saeed et.al.", "authors": "Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed", "id": "2410.24049v1", "paper_url": "http://arxiv.org/abs/2410.24049v1", "repo": "null"}}