{"2408.06507": {"publish_time": "2024-08-12", "title": "Benchmarking tree species classification from proximally-sensed laser scanning data: introducing the FOR-species20K dataset", "paper_summary": "Proximally-sensed laser scanning offers significant potential for automated\nforest data capture, but challenges remain in automatically identifying tree\nspecies without additional ground data. Deep learning (DL) shows promise for\nautomation, yet progress is slowed by the lack of large, diverse, openly\navailable labeled datasets of single tree point clouds. This has impacted the\nrobustness of DL models and the ability to establish best practices for species\nclassification.\n  To overcome these challenges, the FOR-species20K benchmark dataset was\ncreated, comprising over 20,000 tree point clouds from 33 species, captured\nusing terrestrial (TLS), mobile (MLS), and drone laser scanning (ULS) across\nvarious European forests, with some data from other regions. This dataset\nenables the benchmarking of DL models for tree species classification,\nincluding both point cloud-based (PointNet++, MinkNet, MLP-Mixer, DGCNNs) and\nmulti-view image-based methods (SimpleView, DetailView, YOLOv5).\n  2D image-based models generally performed better (average OA = 0.77) than 3D\npoint cloud-based models (average OA = 0.72), with consistent results across\ndifferent scanning platforms and sensors. The top model, DetailView, was\nparticularly robust, handling data imbalances well and generalizing effectively\nacross tree sizes.\n  The FOR-species20K dataset, available at https://zenodo.org/records/13255198,\nis a key resource for developing and benchmarking DL models for tree species\nclassification using laser scanning data, providing a foundation for future\nadvancements in the field.", "paper_summary_zh": "<paragraph>\u8fd1\u8ddd\u96e2\u611f\u6e2c\u96f7\u5c04\u6383\u63cf\u5728\u81ea\u52d5\u5316\u68ee\u6797\u8cc7\u6599\u64f7\u53d6\u65b9\u9762\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\uff0c\u4f46\u4ecd\u6709\u6311\u6230\u5728\u65bc\u81ea\u52d5\u8b58\u5225\u6a39\u7a2e\uff0c\u800c\u4e0d\u9700\u984d\u5916\u7684\u5730\u9762\u8cc7\u6599\u3002\u6df1\u5ea6\u5b78\u7fd2 (DL) \u986f\u793a\u51fa\u81ea\u52d5\u5316\u7684\u5e0c\u671b\uff0c\u4f46\u7531\u65bc\u7f3a\u4e4f\u5927\u578b\u3001\u591a\u6a23\u5316\u3001\u516c\u958b\u53ef\u7528\u7684\u55ae\u68f5\u6a39\u9ede\u96f2\u6a19\u7c64\u8cc7\u6599\u96c6\uff0c\u9032\u5ea6\u7de9\u6162\u3002\u9019\u5f71\u97ff\u4e86 DL \u6a21\u578b\u7684\u7a69\u5065\u6027\uff0c\u4ee5\u53ca\u5efa\u7acb\u7269\u7a2e\u5206\u985e\u6700\u4f73\u5be6\u52d9\u7684\u80fd\u529b\u3002\n\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u6311\u6230\uff0cFOR-species20K \u57fa\u6e96\u8cc7\u6599\u96c6\u61c9\u904b\u800c\u751f\uff0c\u5305\u542b\u4f86\u81ea 33 \u500b\u7269\u7a2e\u7684 20,000 \u591a\u68f5\u6a39\u9ede\u96f2\uff0c\u4f7f\u7528\u5730\u9762 (TLS)\u3001\u884c\u52d5 (MLS) \u548c\u7121\u4eba\u6a5f\u96f7\u5c04\u6383\u63cf (ULS) \u5728\u5404\u7a2e\u6b50\u6d32\u68ee\u6797\u4e2d\u64f7\u53d6\uff0c\u4e26\u5305\u542b\u4f86\u81ea\u5176\u4ed6\u5730\u5340\u7684\u4e00\u4e9b\u8cc7\u6599\u3002\u6b64\u8cc7\u6599\u96c6\u80fd\u5c0d\u6a39\u7a2e\u5206\u985e\u7684 DL \u6a21\u578b\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u5305\u62ec\u57fa\u65bc\u9ede\u96f2\u7684\u65b9\u6cd5 (PointNet++\u3001MinkNet\u3001MLP-Mixer\u3001DGCNN) \u548c\u57fa\u65bc\u591a\u8996\u5716\u5f71\u50cf\u7684\u65b9\u6cd5 (SimpleView\u3001DetailView\u3001YOLOv5)\u3002\n2D \u5f71\u50cf\u5f0f\u6a21\u578b\u901a\u5e38\u8868\u73fe\u5f97\u6bd4 3D \u9ede\u96f2\u5f0f\u6a21\u578b\u66f4\u597d (\u5e73\u5747 OA = 0.77) (\u5e73\u5747 OA = 0.72)\uff0c\u5728\u4e0d\u540c\u7684\u6383\u63cf\u5e73\u53f0\u548c\u611f\u6e2c\u5668\u4e2d\u90fd\u6709\u76f8\u7b26\u7684\u7d50\u679c\u3002\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b DetailView \u7279\u5225\u7a69\u5065\uff0c\u80fd\u59a5\u5584\u8655\u7406\u8cc7\u6599\u4e0d\u5e73\u8861\uff0c\u4e26\u6709\u6548\u6982\u62ec\u5404\u7a2e\u6a39\u6728\u5927\u5c0f\u3002\nFOR-species20K \u8cc7\u6599\u96c6\u53ef\u5728 https://zenodo.org/records/13255198 \u53d6\u5f97\uff0c\u662f\u4f7f\u7528\u96f7\u5c04\u6383\u63cf\u8cc7\u6599\u9032\u884c\u6a39\u7a2e\u5206\u985e\u7684 DL \u6a21\u578b\u958b\u767c\u548c\u57fa\u6e96\u6e2c\u8a66\u7684\u91cd\u8981\u8cc7\u6e90\uff0c\u70ba\u8a72\u9818\u57df\u7684\u672a\u4f86\u9032\u5c55\u5960\u5b9a\u57fa\u790e\u3002</paragraph>", "author": "Stefano Puliti et.al.", "authors": "Stefano Puliti, Emily R. Lines, Jana M\u00fcllerov\u00e1, Julian Frey, Zoe Schindler, Adrian Straker, Matthew J. Allen, Lukas Winiwarter, Nataliia Rehush, Hristina Hristova, Brent Murray, Kim Calders, Louise Terryn, Nicholas Coops, Bernhard H\u00f6fle, Samuli Junttila, Martin Kr\u016f\u010dek, Grzegorz Krok, Kamil Kr\u00e1l, Shaun R. Levick, Linda Luck, Azim Missarov, Martin Mokro\u0161, Harry J. F. Owen, Krzysztof Stere\u0144czak, Timo P. Pitk\u00e4nen, Nicola Puletti, Ninni Saarinen, Chris Hopkinson, Chiara Torresan, Enrico Tomelleri, Hannah Weiser, Rasmus Astrup", "id": "2408.06507v1", "paper_url": "http://arxiv.org/abs/2408.06507v1", "repo": "null"}}