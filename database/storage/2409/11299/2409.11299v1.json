{"2409.11299": {"publish_time": "2024-09-17", "title": "TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation", "paper_summary": "Biomedical image segmentation is crucial for accurately diagnosing and\nanalyzing various diseases. However, Convolutional Neural Networks (CNNs) and\nTransformers, the most commonly used architectures for this task, struggle to\neffectively capture long-range dependencies due to the inherent locality of\nCNNs and the computational complexity of Transformers. To address this\nlimitation, we introduce TTT-Unet, a novel framework that integrates Test-Time\nTraining (TTT) layers into the traditional U-Net architecture for biomedical\nimage segmentation. TTT-Unet dynamically adjusts model parameters during the\ntesting time, enhancing the model's ability to capture both local and\nlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,\nincluding 3D abdominal organ segmentation in CT and MR images, instrument\nsegmentation in endoscopy images, and cell segmentation in microscopy images.\nThe results demonstrate that TTT-Unet consistently outperforms state-of-the-art\nCNN-based and Transformer-based segmentation models across all tasks. The code\nis available at https://github.com/rongzhou7/TTT-Unet.", "paper_summary_zh": "\u751f\u7269\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u5c0d\u65bc\u6e96\u78ba\u8a3a\u65b7\u548c\u5206\u6790\u5404\u7a2e\u75be\u75c5\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u5377\u7a4d\u795e\u7d93\u7db2\u8def\uff08CNN\uff09\u548c Transformer\uff0c\u4f5c\u70ba\u6b64\u4efb\u52d9\u6700\u5e38\u7528\u7684\u67b6\u69cb\uff0c\u7531\u65bc CNN \u7684\u56fa\u6709\u5c40\u90e8\u6027\u548c Transformer \u7684\u8a08\u7b97\u8907\u96dc\u6027\uff0c\u96e3\u4ee5\u6709\u6548\u64f7\u53d6\u9577\u7a0b\u4f9d\u8cf4\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 TTT-Unet\uff0c\u4e00\u500b\u5275\u65b0\u7684\u6846\u67b6\uff0c\u5c07\u6e2c\u8a66\u6642\u9593\u8a13\u7df4\uff08TTT\uff09\u5c64\u6574\u5408\u5230\u50b3\u7d71\u7684 U-Net \u67b6\u69cb\u4e2d\uff0c\u7528\u65bc\u751f\u7269\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u3002TTT-Unet \u5728\u6e2c\u8a66\u6642\u9593\u52d5\u614b\u8abf\u6574\u6a21\u578b\u53c3\u6578\uff0c\u589e\u5f37\u6a21\u578b\u64f7\u53d6\u5c40\u90e8\u548c\u9577\u7a0b\u7279\u5fb5\u7684\u80fd\u529b\u3002\u6211\u5011\u5728\u591a\u500b\u91ab\u5b78\u5f71\u50cf\u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30 TTT-Unet\uff0c\u5305\u62ec\u96fb\u8166\u65b7\u5c64\u6383\u63cf\u548c\u78c1\u632f\u9020\u5f71\u4e2d\u7684 3D \u8179\u8154\u5668\u5b98\u5206\u5272\u3001\u5167\u8996\u93e1\u5f71\u50cf\u4e2d\u7684\u5100\u5668\u5206\u5272\u4ee5\u53ca\u986f\u5fae\u93e1\u5f71\u50cf\u4e2d\u7684\u7d30\u80de\u5206\u5272\u3002\u7d50\u679c\u8868\u660e\uff0cTTT-Unet \u5728\u6240\u6709\u4efb\u52d9\u4e2d\u90fd\u6301\u7e8c\u512a\u65bc\u6700\u5148\u9032\u7684\u57fa\u65bc CNN \u548c\u57fa\u65bc Transformer \u7684\u5206\u5272\u6a21\u578b\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/rongzhou7/TTT-Unet \u53d6\u5f97\u3002", "author": "Rong Zhou et.al.", "authors": "Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun", "id": "2409.11299v1", "paper_url": "http://arxiv.org/abs/2409.11299v1", "repo": "null"}}