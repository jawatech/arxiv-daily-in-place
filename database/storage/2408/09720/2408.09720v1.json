{"2408.09720": {"publish_time": "2024-08-19", "title": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework", "paper_summary": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}.", "paper_summary_zh": "\u884c\u4eba\u5c6c\u6027\u8fa8\u8b58 (PAR) \u662f\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u7814\u7a76\u6240\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4efb\u52d9\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u8cc7\u6599\u96c6\u5ffd\u7565\u4e86\u4e0d\u540c\u7684\u9818\u57df\uff08\u4f8b\u5982\uff0c\u74b0\u5883\u3001\u6642\u9593\u3001\u65cf\u7fa4\u548c\u8cc7\u6599\u4f86\u6e90\uff09\uff0c\u53ea\u9032\u884c\u7c21\u55ae\u7684\u96a8\u6a5f\u5206\u5272\uff0c\u800c\u9019\u4e9b\u8cc7\u6599\u96c6\u7684\u6548\u80fd\u5df2\u63a5\u8fd1\u98fd\u548c\u3002\u5728\u904e\u53bb\u7684\u4e94\u5e74\u4e2d\uff0c\u6c92\u6709\u5927\u578b\u8cc7\u6599\u96c6\u5411\u516c\u773e\u958b\u653e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u3001\u5927\u898f\u6a21\u7684\u3001\u8de8\u9818\u57df\u7684\u884c\u4eba\u5c6c\u6027\u8fa8\u8b58\u8cc7\u6599\u96c6\uff0c\u4ee5\u586b\u88dc\u8cc7\u6599\u5dee\u8ddd\uff0c\u7a31\u70ba MSP60K\u3002\u5b83\u5305\u542b 60,122 \u5f35\u5f71\u50cf\u548c 57 \u500b\u5c6c\u6027\u8a3b\u89e3\uff0c\u6a6b\u8de8\u516b\u500b\u5834\u666f\u3002\u5408\u6210\u964d\u7d1a\u4e5f\u9032\u884c\u4ee5\u9032\u4e00\u6b65\u7e2e\u5c0f\u8cc7\u6599\u96c6\u548c\u73fe\u5be6\u4e16\u754c\u6311\u6230\u5834\u666f\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u70ba\u4e86\u5efa\u7acb\u66f4\u56b4\u8b39\u7684\u57fa\u6e96\uff0c\u6211\u5011\u5728\u6211\u5011\u7684\u8cc7\u6599\u96c6\u4e0a\uff0c\u5728\u96a8\u6a5f\u548c\u8de8\u9818\u57df\u5206\u5272\u5354\u5b9a\u7684\u60c5\u6cc1\u4e0b\uff0c\u8a55\u4f30 17 \u500b\u5177\u4ee3\u8868\u6027\u7684 PAR \u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5275\u65b0\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u589e\u5f37 PAR \u6846\u67b6\uff0c\u7a31\u70ba LLM-PAR\u3002\u6b64\u6846\u67b6\u900f\u904e Vision Transformer (ViT) \u4e3b\u5e79\u8655\u7406\u884c\u4eba\u5f71\u50cf\u4ee5\u63d0\u53d6\u7279\u5fb5\uff0c\u4e26\u5f15\u5165\u591a\u5d4c\u5165\u67e5\u8a62 Transformer \u4f86\u5b78\u7fd2\u90e8\u5206\u611f\u77e5\u7279\u5fb5\u4ee5\u9032\u884c\u5c6c\u6027\u5206\u985e\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u900f\u904e LLM \u589e\u5f37\u6b64\u6846\u67b6\u4ee5\u9032\u884c\u6574\u9ad4\u5b78\u7fd2\u548c\u8996\u89ba\u7279\u5fb5\u589e\u5f37\u3002\u8de8\u591a\u500b PAR \u57fa\u6e96\u8cc7\u6599\u96c6\u7684\u7d9c\u5408\u5be6\u9a57\u5df2\u5fb9\u5e95\u9a57\u8b49\u4e86\u6211\u5011\u63d0\u51fa\u7684\u6846\u67b6\u7684\u6548\u80fd\u3002\u672c\u6587\u9644\u5e36\u7684\u8cc7\u6599\u96c6\u548c\u539f\u59cb\u78bc\u5c07\u5728 \\url{https://github.com/Event-AHU/OpenPAR} \u516c\u958b\u3002", "author": "Jiandong Jin et.al.", "authors": "Jiandong Jin, Xiao Wang, Qian Zhu, Haiyang Wang, Chenglong Li", "id": "2408.09720v1", "paper_url": "http://arxiv.org/abs/2408.09720v1", "repo": "https://github.com/event-ahu/openpar"}}