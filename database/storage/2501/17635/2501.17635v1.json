{"2501.17635": {"publish_time": "2025-01-29", "title": "In-Context Meta LoRA Generation", "paper_summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9\uff08LoRA\uff09\u5df2\u5c55\u73fe\u51fa\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u7684\u5353\u8d8a\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u591a\u9805\u4efb\u52d9\u7684\u5834\u666f\u4e2d\uff0c\u70ba\u6bcf\u500b\u4efb\u52d9\u8a13\u7df4\u4e00\u500b\u7368\u7acb\u7684 LoRA \u6a21\u578b\u6703\u5c0e\u81f4\u5132\u5b58\u548c\u63a8\u8ad6\u65b9\u9762\u7684\u986f\u8457\u4f4e\u6548\u7387\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7684\u53c3\u6578\u751f\u6210\u65b9\u6cd5\u7121\u6cd5\u6355\u6349\u9019\u4e9b\u4efb\u52d9\u4e4b\u9593\u7684\u95dc\u806f\u6027\uff0c\u4f7f\u5f97\u591a\u4efb\u52d9 LoRA \u53c3\u6578\u751f\u6210\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u60c5\u5883\u5143 LoRA\uff08ICM-LoRA\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u5be6\u73fe\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7279\u5b9a\u4efb\u52d9\u81ea\u8a02\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u4f86\u81ea\u6240\u6709\u4efb\u52d9\u7684\u8a13\u7df4\u8cc7\u6599\u4f86\u8a13\u7df4\u4e00\u500b\u91cf\u8eab\u6253\u9020\u7684\u751f\u6210\u5668\uff0c\u689d\u4ef6\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668\uff08CVAE\uff09\u3002CVAE \u5c07\u4efb\u52d9\u63cf\u8ff0\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u7522\u751f\u8207\u4efb\u52d9\u76f8\u95dc\u7684 LoRA \u6b0a\u91cd\u4f5c\u70ba\u8f38\u51fa\u3002\u7136\u5f8c\u5c07\u9019\u4e9b LoRA \u6b0a\u91cd\u8207 LLM \u5408\u4f75\uff0c\u4ee5\u5efa\u7acb\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u984d\u5916\u7684\u5fae\u8abf\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u60c5\u5883\u5143\u5b78\u7fd2\u4f86\u589e\u5f37\u77e5\u8b58\u548c\u4efb\u52d9\u5c0d\u61c9\uff0c\u4ee5\u6355\u6349\u4efb\u52d9\u548c\u53c3\u6578\u5206\u4f48\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528 CVAE \u91dd\u5c0d\u4e0d\u540c\u7684\u4efb\u52d9\u5be6\u73fe\u66f4\u6e96\u78ba\u7684 LoRA \u53c3\u6578\u751f\u6210\u3002\u8207\u76ee\u524d\u7684\u53c3\u6578\u91cd\u5efa\u65b9\u6cd5\u76f8\u6bd4\uff0cICM-LoRA \u80fd\u66f4\u6e96\u78ba\u5730\u91cd\u5efa LoRA \u53c3\u6578\uff0c\u4e26\u4e14\u6709\u52a9\u65bc\u5be6\u4f5c LoRA \u53c3\u6578\u7684\u7279\u5b9a\u4efb\u52d9\u589e\u5f37\u3002\u540c\u6642\uff0c\u6211\u5011\u7684\u6a21\u578b\u4f54\u7528 283MB\uff0c\u8207\u539f\u59cb LoRA \u76f8\u6bd4\uff0c\u5132\u5b58\u91cf\u50c5\u70ba 1%\u3002", "author": "Yihua Shao et.al.", "authors": "Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo", "id": "2501.17635v1", "paper_url": "http://arxiv.org/abs/2501.17635v1", "repo": "null"}}