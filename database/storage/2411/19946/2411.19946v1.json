{"2411.19946": {"publish_time": "2024-11-29", "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation", "paper_summary": "Recent advances in dataset distillation have led to solutions in two main\ndirections. The conventional batch-to-batch matching mechanism is ideal for\nsmall-scale datasets and includes bi-level optimization methods on models and\nsyntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like\ndistribution matching, gradient matching, and weight trajectory matching.\nConversely, batch-to-global matching typifies decoupled methods, which are\nparticularly advantageous for large-scale datasets. This approach has garnered\nsubstantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,\nand CDA. A primary challenge with the second approach is the lack of diversity\namong syntheses within each class since samples are optimized independently and\nthe same global supervision signals are reused across different synthetic\nimages. In this study, we propose a new Diversity-driven EarlyLate Training\n(DELT) scheme to enhance the diversity of images in batch-to-global matching\nwith less computation. Our approach is conceptually simple yet effective, it\npartitions predefined IPC samples into smaller subtasks and employs local\noptimizations to distill each subset into distributions from distinct phases,\nreducing the uniformity induced by the unified optimization process. These\ndistilled images from the subtasks demonstrate effective generalization when\napplied to the entire task. We conduct extensive experiments on CIFAR,\nTiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the\nprevious state-of-the-art by 2$\\sim$5% on average across different datasets and\nIPCs (images per class), increasing diversity per class by more than 5% while\nreducing synthesis time by up to 39.3% for enhancing the training efficiency.\nCode is available at: https://github.com/VILA-Lab/DELT.", "paper_summary_zh": "<paragraph>\u8cc7\u6599\u96c6\u8403\u53d6\u7684\u6700\u65b0\u9032\u5c55\u5df2\u5c0e\u81f4\u5169\u500b\u4e3b\u8981\u65b9\u5411\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u50b3\u7d71\u7684\u6279\u6b21\u5c0d\u6279\u6b21\u5339\u914d\u6a5f\u5236\u975e\u5e38\u9069\u5408\u5c0f\u898f\u6a21\u8cc7\u6599\u96c6\uff0c\u5305\u62ec\u6a21\u578b\u548c\u5408\u6210\u4e0a\u7684\u96d9\u5c64\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u4f8b\u5982 FRePo\u3001RCIG \u548c RaT-BPTT\uff0c\u4ee5\u53ca\u5206\u4f48\u5339\u914d\u3001\u68af\u5ea6\u5339\u914d\u548c\u6b0a\u91cd\u8ecc\u8de1\u5339\u914d\u7b49\u5176\u4ed6\u65b9\u6cd5\u3002\u76f8\u53cd\uff0c\u6279\u6b21\u5c0d\u5168\u5c40\u5339\u914d\u662f\u89e3\u8026\u65b9\u6cd5\u7684\u5178\u578b\uff0c\u9019\u5c0d\u65bc\u5927\u898f\u6a21\u8cc7\u6599\u96c6\u7279\u5225\u6709\u5229\u3002\u9019\u7a2e\u65b9\u6cd5\u5728\u793e\u7fa4\u4e2d\u5f15\u8d77\u4e86\u6975\u5927\u7684\u8208\u8da3\uff0c\u5982\u5728 SRe$^2$L\u3001G-VBSM\u3001WMDD \u548c CDA \u4e2d\u6240\u898b\u3002\u7b2c\u4e8c\u7a2e\u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6230\u662f\u6bcf\u500b\u985e\u5225\u5167\u7684\u5408\u6210\u7f3a\u4e4f\u591a\u6a23\u6027\uff0c\u56e0\u70ba\u7bc4\u4f8b\u662f\u7368\u7acb\u6700\u4f73\u5316\u7684\uff0c\u4e26\u4e14\u76f8\u540c\u7684\u5168\u5c40\u76e3\u7763\u8a0a\u865f\u5728\u4e0d\u540c\u7684\u5408\u6210\u5f71\u50cf\u4e2d\u91cd\u8907\u4f7f\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u591a\u6a23\u6027\u9a45\u52d5\u7684\u65e9\u671f\u5f8c\u671f\u8a13\u7df4 (DELT) \u65b9\u6848\uff0c\u4ee5\u589e\u5f37\u6279\u6b21\u5c0d\u5168\u5c40\u5339\u914d\u4e2d\u5f71\u50cf\u7684\u591a\u6a23\u6027\uff0c\u4e26\u6e1b\u5c11\u904b\u7b97\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u6982\u5ff5\u4e0a\u65e2\u7c21\u55ae\u53c8\u6709\u6548\uff0c\u5b83\u5c07\u9810\u5b9a\u7fa9\u7684 IPC \u7bc4\u4f8b\u5206\u5272\u6210\u66f4\u5c0f\u7684\u5b50\u4efb\u52d9\uff0c\u4e26\u63a1\u7528\u5c40\u90e8\u6700\u4f73\u5316\u5c07\u6bcf\u500b\u5b50\u96c6\u8403\u53d6\u6210\u4e0d\u540c\u968e\u6bb5\u7684\u5206\u5e03\uff0c\u6e1b\u5c11\u7d71\u4e00\u6700\u4f73\u5316\u6d41\u7a0b\u6240\u5c0e\u81f4\u7684\u4e00\u81f4\u6027\u3002\u9019\u4e9b\u4f86\u81ea\u5b50\u4efb\u52d9\u7684\u8403\u53d6\u5f71\u50cf\u5728\u61c9\u7528\u65bc\u6574\u500b\u4efb\u52d9\u6642\u5c55\u73fe\u4e86\u6709\u6548\u7684\u6982\u5316\u3002\u6211\u5011\u5c0d CIFAR\u3001Tiny-ImageNet\u3001ImageNet-1K \u53ca\u5176\u5b50\u8cc7\u6599\u96c6\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u548c IPC\uff08\u6bcf\u500b\u985e\u5225\u7684\u5f71\u50cf\uff09\u4e2d\u5e73\u5747\u6bd4\u5148\u524d\u7684\u6700\u65b0\u6280\u8853\u9ad8\u51fa 2$\\sim$5%\uff0c\u6bcf\u500b\u985e\u5225\u7684\u591a\u6a23\u6027\u589e\u52a0\u4e86 5% \u4ee5\u4e0a\uff0c\u540c\u6642\u5c07\u5408\u6210\u6642\u9593\u6e1b\u5c11\u4e86 39.3%\uff0c\u4ee5\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/VILA-Lab/DELT \u53d6\u5f97\u3002</paragraph>", "author": "Zhiqiang Shen et.al.", "authors": "Zhiqiang Shen, Ammar Sherif, Zeyuan Yin, Shitong Shao", "id": "2411.19946v1", "paper_url": "http://arxiv.org/abs/2411.19946v1", "repo": "https://github.com/vila-lab/delt"}}