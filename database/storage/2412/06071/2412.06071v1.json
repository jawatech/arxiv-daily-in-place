{"2412.06071": {"publish_time": "2024-12-08", "title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models", "paper_summary": "The increasing sizes of large language models (LLMs) result in significant\ncomputational overhead and memory usage when adapting these models to specific\ntasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have\nbeen devised to mitigate these challenges by training a small set of parameters\nfor the task-specific updates of the model weights. Among PEFT methods, LoRA\nstands out for its simplicity and efficiency, inspiring the development of a\nseries of variants. However, LoRA and its successors disregard the knowledge\nthat is noisy or irrelevant to the targeted task, detrimentally impacting model\nperformance and leading to suboptimality. To address this limitation, we\nintroduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that\nleverages singular value decomposition (SVD) with knowledge-aware singular\nvalues to dynamically activate knowledge based on its relevance to the task at\nhand. We conduct extensive experiments across a range of LLMs on tasks spanning\nnatural language understanding (NLU), generation (NLG), instruction following,\nand commonsense reasoning. The experimental results demonstrate that KaSA\nconsistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks\nand 4 synthetic datasets, underscoring our method's efficacy and adaptability.\nThe source code of our method is available at\nhttps://github.com/juyongjiang/KaSA.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u898f\u6a21\u4e0d\u65b7\u64f4\u5927\uff0c\u5728\u5c07\u9019\u4e9b\u6a21\u578b\u8abf\u6574\u81f3\u7279\u5b9a\u4efb\u52d9\u6216\u9818\u57df\u6642\uff0c\u6703\u9020\u6210\u986f\u8457\u7684\u904b\u7b97\u8ca0\u64d4\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u5404\u7a2e\u53c3\u6578\u9ad8\u6548\u5fae\u8abf\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u88ab\u8a2d\u8a08\u51fa\u4f86\uff0c\u900f\u904e\u8a13\u7df4\u4e00\u7d44\u5c0f\u53c3\u6578\u4f86\u91dd\u5c0d\u6a21\u578b\u6b0a\u91cd\u7684\u4efb\u52d9\u7279\u5b9a\u66f4\u65b0\uff0c\u4ee5\u6e1b\u8f15\u9019\u4e9b\u6311\u6230\u3002\u5728 PEFT \u65b9\u6cd5\u4e2d\uff0cLoRA \u4ee5\u5176\u7c21\u6f54\u548c\u6548\u7387\u812b\u7a4e\u800c\u51fa\uff0c\u6fc0\u767c\u4e86\u4e00\u7cfb\u5217\u8b8a\u9ad4\u7684\u958b\u767c\u3002\u7136\u800c\uff0cLoRA \u53ca\u5176\u5f8c\u7e7c\u8005\u5ffd\u8996\u4e86\u5c0d\u76ee\u6a19\u4efb\u52d9\u6709\u96dc\u8a0a\u6216\u7121\u95dc\u7684\u77e5\u8b58\uff0c\u5c0d\u6a21\u578b\u6548\u80fd\u9020\u6210\u4e0d\u5229\u5f71\u97ff\uff0c\u4e26\u5c0e\u81f4\u6b21\u6700\u4f73\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86\u77e5\u8b58\u611f\u77e5\u5947\u7570\u503c\u9069\u61c9\uff08KaSA\uff09\uff0c\u9019\u662f\u4e00\u7a2e PEFT \u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5947\u7570\u503c\u5206\u89e3\uff08SVD\uff09\u548c\u77e5\u8b58\u611f\u77e5\u5947\u7570\u503c\uff0c\u6839\u64da\u5176\u8207\u624b\u908a\u4efb\u52d9\u76f8\u95dc\u6027\u52d5\u614b\u555f\u7528\u77e5\u8b58\u3002\u6211\u5011\u91dd\u5c0d\u4e00\u7cfb\u5217 LLM \u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6db5\u84cb\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\uff08NLU\uff09\u3001\u751f\u6210\uff08NLG\uff09\u3001\u6307\u4ee4\u9075\u5faa\u548c\u5e38\u8b58\u63a8\u7406\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728 16 \u500b\u57fa\u6e96\u548c 4 \u500b\u5408\u6210\u8cc7\u6599\u96c6\u4e0a\uff0cKaSA \u6301\u7e8c\u512a\u65bc FFT \u548c 14 \u500b\u6d41\u884c\u7684 PEFT \u57fa\u6e96\uff0c\u7a81\u986f\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9069\u61c9\u6027\u3002\u6211\u5011\u65b9\u6cd5\u7684\u539f\u59cb\u78bc\u53ef\u5728 https://github.com/juyongjiang/KaSA \u53d6\u5f97\u3002", "author": "Fan Wang et.al.", "authors": "Fan Wang, Juyong Jiang, Chansung Park, Sunghun Kim, Jing Tang", "id": "2412.06071v1", "paper_url": "http://arxiv.org/abs/2412.06071v1", "repo": "https://github.com/juyongjiang/kasa"}}