{"2407.02328": {"publish_time": "2024-07-02", "title": "Efficient Sparse Attention needs Adaptive Token Release", "paper_summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide array of text-centric tasks. However, their `large'\nscale introduces significant computational and storage challenges, particularly\nin managing the key-value states of the transformer, which limits their wider\napplicability. Therefore, we propose to adaptively release resources from\ncaches and rebuild the necessary key-value states. Particularly, we accomplish\nthis by a lightweight controller module to approximate an ideal top-$K$ sparse\nattention. This module retains the tokens with the highest top-$K$ attention\nweights and simultaneously rebuilds the discarded but necessary tokens, which\nmay become essential for future decoding. Comprehensive experiments in natural\nlanguage generation and modeling reveal that our method is not only competitive\nwith full attention in terms of performance but also achieves a significant\nthroughput improvement of up to 221.8%. The code for replication is available\non the https://github.com/WHUIR/ADORE.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4ee5\u6587\u5b57\u70ba\u4e2d\u5fc3\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u300c\u5927\u300d\u898f\u6a21\u5f15\u5165\u4e86\u986f\u8457\u7684\u8a08\u7b97\u548c\u5132\u5b58\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u7ba1\u7406Transformer\u7684\u9375\u503c\u72c0\u614b\u6642\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u66f4\u5ee3\u6cdb\u7684\u9069\u7528\u6027\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5efa\u8b70\u81ea\u9069\u61c9\u5730\u5f9e\u5feb\u53d6\u91cb\u653e\u8cc7\u6e90\uff0c\u4e26\u91cd\u5efa\u5fc5\u8981\u7684\u9375\u503c\u72c0\u614b\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u900f\u904e\u4e00\u500b\u8f15\u91cf\u7d1a\u63a7\u5236\u5668\u6a21\u7d44\u4f86\u8fd1\u4f3c\u4e00\u500b\u7406\u60f3\u7684\u9802\u90e8-$K$ \u7a00\u758f\u6ce8\u610f\u529b\u4f86\u5b8c\u6210\u9019\u9805\u4efb\u52d9\u3002\u6b64\u6a21\u7d44\u4fdd\u7559\u5177\u6709\u6700\u9ad8\u9802\u90e8-$K$ \u6ce8\u610f\u529b\u6b0a\u91cd\u7684\u4ee3\u78bc\uff0c\u4e26\u540c\u6642\u91cd\u5efa\u88ab\u6368\u68c4\u4f46\u5fc5\u8981\u7684\u4ee3\u78bc\uff0c\u9019\u4e9b\u4ee3\u78bc\u53ef\u80fd\u5c0d\u672a\u4f86\u7684\u89e3\u78bc\u81f3\u95dc\u91cd\u8981\u3002\u5728\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u548c\u5efa\u6a21\u4e2d\u7684\u5168\u9762\u5be6\u9a57\u63ed\u793a\u4e86\u6211\u5011\u7684\u6a21\u578b\u4e0d\u50c5\u5728\u6548\u80fd\u65b9\u9762\u8207\u5b8c\u5168\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u76f8\u5339\u6575\uff0c\u9084\u9054\u5230\u4e86\u9ad8\u9054 221.8% \u7684\u986f\u8457\u541e\u5410\u91cf\u63d0\u5347\u3002\u8907\u88fd\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/WHUIR/ADORE \u4e0a\u53d6\u5f97\u3002", "author": "Chaoran Zhang et.al.", "authors": "Chaoran Zhang, Lixin Zou, Dan Luo, Min Tang, Xiangyang Luo, Zihao Li, Chenliang Li", "id": "2407.02328v1", "paper_url": "http://arxiv.org/abs/2407.02328v1", "repo": "null"}}