{"2406.07275": {"publish_time": "2024-06-11", "title": "DCA-Bench: A Benchmark for Dataset Curation Agents", "paper_summary": "The quality of datasets plays an increasingly crucial role in the research\nand development of modern artificial intelligence (AI). Despite the\nproliferation of open dataset platforms nowadays, data quality issues, such as\ninsufficient documentation, inaccurate annotations, and ethical concerns,\nremain common in datasets widely used in AI. Furthermore, these issues are\noften subtle and difficult to be detected by rule-based scripts, requiring\nexpensive manual identification and verification by dataset users or\nmaintainers. With the increasing capability of large language models (LLMs), it\nis promising to streamline the curation of datasets with LLM agents. In this\nwork, as the initial step towards this goal, we propose a dataset curation\nagent benchmark, DCA-Bench, to measure LLM agents' capability of detecting\nhidden dataset quality issues. Specifically, we collect diverse real-world\ndataset quality issues from eight open dataset platforms as a testbed.\nAdditionally, to establish an automatic pipeline for evaluating the success of\nLLM agents, which requires a nuanced understanding of the agent outputs, we\nimplement a dedicated Evaluator using another LLM agent. We demonstrate that\nthe LLM-based Evaluator empirically aligns well with human evaluation, allowing\nreliable automatic evaluation on the proposed benchmark. We further conduct\nexperiments on several baseline LLM agents on the proposed benchmark and\ndemonstrate the complexity of the task, indicating that applying LLMs to\nreal-world dataset curation still requires further in-depth exploration and\ninnovation. Finally, the proposed benchmark can also serve as a testbed for\nmeasuring the capability of LLMs in problem discovery rather than just\nproblem-solving. The benchmark suite is available at\n\\url{https://github.com/TRAIS-Lab/dca-bench}.", "paper_summary_zh": "<paragraph>\u8cc7\u6599\u96c6\u7684\u54c1\u8cea\u5728\u73fe\u4ee3\u4eba\u5de5\u667a\u6167 (AI) \u7684\u7814\u7a76\u8207\u958b\u767c\u4e2d\u626e\u6f14\u8457\u8d8a\u4f86\u8d8a\u95dc\u9375\u7684\u89d2\u8272\u3002\u5118\u7ba1\u73fe\u4eca\u958b\u653e\u5f0f\u8cc7\u6599\u96c6\u5e73\u53f0\u6fc0\u589e\uff0c\u8cc7\u6599\u54c1\u8cea\u554f\u984c\uff0c\u4f8b\u5982\u6587\u4ef6\u4e0d\u8db3\u3001\u8a3b\u89e3\u4e0d\u6e96\u78ba\u548c\u9053\u5fb7\u7591\u616e\uff0c\u5728\u5ee3\u6cdb\u4f7f\u7528\u65bc AI \u7684\u8cc7\u6599\u96c6\u4e2d\u4ecd\u7136\u5f88\u5e38\u898b\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u554f\u984c\u901a\u5e38\u5f88\u5fae\u5999\uff0c\u4e14\u96e3\u4ee5\u900f\u904e\u57fa\u65bc\u898f\u5247\u7684\u6307\u4ee4\u78bc\u5075\u6e2c\uff0c\u9700\u8981\u8cc7\u6599\u96c6\u4f7f\u7528\u8005\u6216\u7dad\u8b77\u4eba\u54e1\u9032\u884c\u6602\u8cb4\u7684\u624b\u52d5\u8b58\u5225\u548c\u9a57\u8b49\u3002\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u529b\u7684\u63d0\u5347\uff0c\u4f7f\u7528 LLM \u4ee3\u7406\u7c21\u5316\u8cc7\u6599\u96c6\u7b56\u5c55\u5de5\u4f5c\u4ee4\u4eba\u671f\u5f85\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u4f5c\u70ba\u671d\u5411\u6b64\u76ee\u6a19\u9081\u51fa\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8cc7\u6599\u96c6\u7b56\u5c55\u4ee3\u7406\u57fa\u6e96\u6e2c\u8a66\uff0cDCA-Bench\uff0c\u7528\u65bc\u8861\u91cf LLM \u4ee3\u7406\u5075\u6e2c\u96b1\u85cf\u8cc7\u6599\u96c6\u54c1\u8cea\u554f\u984c\u7684\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f9e\u516b\u500b\u958b\u653e\u5f0f\u8cc7\u6599\u96c6\u5e73\u53f0\u6536\u96c6\u4e86\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u7684\u8cc7\u6599\u96c6\u54c1\u8cea\u554f\u984c\u4f5c\u70ba\u6e2c\u8a66\u5e73\u53f0\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u5efa\u7acb\u4e00\u500b\u7528\u65bc\u8a55\u4f30 LLM \u4ee3\u7406\u6210\u529f\u7684\u81ea\u52d5\u5316\u6d41\u7a0b\uff0c\u9019\u9700\u8981\u5c0d\u4ee3\u7406\u8f38\u51fa\u6709\u7d30\u7dfb\u7684\u4e86\u89e3\uff0c\u6211\u5011\u4f7f\u7528\u53e6\u4e00\u500b LLM \u4ee3\u7406\u5be6\u4f5c\u4e86\u4e00\u500b\u5c08\u7528\u7684\u8a55\u4f30\u5668\u3002\u6211\u5011\u8b49\u660e\uff0c\u57fa\u65bc LLM \u7684\u8a55\u4f30\u5668\u5728\u7d93\u9a57\u4e0a\u8207\u4eba\u5de5\u8a55\u4f30\u975e\u5e38\u543b\u5408\uff0c\u5141\u8a31\u5728\u6240\u63d0\u51fa\u7684\u57fa\u6e96\u6e2c\u8a66\u4e0a\u9032\u884c\u53ef\u9760\u7684\u81ea\u52d5\u8a55\u4f30\u3002\u6211\u5011\u9032\u4e00\u6b65\u5728\u6240\u63d0\u51fa\u7684\u57fa\u6e96\u6e2c\u8a66\u4e0a\u5c0d\u5e7e\u500b\u57fa\u7dda LLM \u4ee3\u7406\u9032\u884c\u5be6\u9a57\uff0c\u4e26\u8b49\u660e\u4e86\u9019\u9805\u4efb\u52d9\u7684\u8907\u96dc\u6027\uff0c\u8868\u660e\u5c07 LLM \u61c9\u7528\u65bc\u771f\u5be6\u4e16\u754c\u7684\u8cc7\u6599\u96c6\u7b56\u5c55\u4ecd\u9700\u8981\u9032\u4e00\u6b65\u7684\u6df1\u5165\u63a2\u7d22\u548c\u5275\u65b0\u3002\u6700\u5f8c\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u6e96\u6e2c\u8a66\u4e5f\u53ef\u4ee5\u4f5c\u70ba\u8861\u91cf LLM \u5728\u554f\u984c\u767c\u73fe\u800c\u975e\u50c5\u50c5\u554f\u984c\u89e3\u6c7a\u65b9\u9762\u7684\u80fd\u529b\u7684\u6e2c\u8a66\u5e73\u53f0\u3002\u57fa\u6e96\u6e2c\u8a66\u5957\u4ef6\u53ef\u5728 \\url{https://github.com/TRAIS-Lab/dca-bench} \u53d6\u5f97\u3002</paragraph>", "author": "Benhao Huang et.al.", "authors": "Benhao Huang, Yingzhuo Yu, Jin Huang, Xingjian Zhang, Jiaqi Ma", "id": "2406.07275v1", "paper_url": "http://arxiv.org/abs/2406.07275v1", "repo": "null"}}