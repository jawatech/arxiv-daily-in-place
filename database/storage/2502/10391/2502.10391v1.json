{"2502.10391": {"publish_time": "2025-02-14", "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment", "paper_summary": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MMLM) \u6709\u986f\u8457\u7684\u9032\u5c55\uff0c\n\u5927\u591a\u6578\u6700\u5148\u9032\u7684\u6a21\u578b\u5c1a\u672a\u8207\u4eba\u985e\n\u504f\u597d\u9032\u884c\u5fb9\u5e95\u6bd4\u5c0d\u3002\u9019\u500b\u5dee\u8ddd\u5b58\u5728\u662f\u56e0\u70ba\u76ee\u524d\u7684\u6bd4\u5c0d\u7814\u7a76\u4e3b\u8981\n\u5728\u7279\u5b9a\u9818\u57df\uff08\u4f8b\u5982\uff0c\u6e1b\u5c11\u5e7b\u89ba\uff09\u53d6\u5f97\u9032\u5c55\uff0c\u800c\n\u662f\u5426\u5c07\u6a21\u578b\u8207\u4eba\u985e\u504f\u597d\u6bd4\u5c0d\u53ef\u4ee5\n\u7cfb\u7d71\u6027\u5730\u589e\u5f37 MLLM \u80fd\u529b\u7684\u66f4\u5ee3\u6cdb\u554f\u984c\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002\u70ba\u6b64\uff0c\n\u6211\u5011\u5f15\u5165\u4e86 MM-RLHF\uff0c\u4e00\u500b\u5305\u542b $\\mathbf{120k}$ \u500b\u7d30\u7dfb\u7684\uff0c\n\u4eba\u985e\u6a19\u8a3b\u7684\u504f\u597d\u6bd4\u8f03\u5c0d\u7684\u8cc7\u6599\u96c6\u3002\u6b64\u8cc7\u6599\u96c6\u4ee3\u8868\u4e86\n\u6bd4\u73fe\u6709\u8cc7\u6e90\u7684\u91cd\u5927\u9032\u5c55\uff0c\u63d0\u4f9b\u4e86\u512a\u8d8a\u7684\u5927\u5c0f\u3001\n\u591a\u6a23\u6027\u3001\u6a19\u8a3b\u7c92\u5ea6\u548c\u54c1\u8cea\u3002\u5229\u7528\u6b64\u8cc7\u6599\u96c6\uff0c\u6211\u5011\n\u63d0\u51fa\u4e86\u4e00\u4e9b\u95dc\u9375\u5275\u65b0\uff0c\u4ee5\u63d0\u9ad8\u734e\u52f5\u6a21\u578b\u7684\u54c1\u8cea\u548c\n\u6bd4\u5c0d\u6f14\u7b97\u6cd5\u7684\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u5f15\u5165\u4e86\n\u57fa\u65bc\u6279\u8a55\u7684\u734e\u52f5\u6a21\u578b\uff0c\u5b83\u6703\u5728\n\u5728\u6307\u5b9a\u5206\u6578\u4e4b\u524d\u7522\u751f\u6a21\u578b\u8f38\u51fa\u7684\u6279\u8a55\uff0c\u8207\u50b3\u7d71\u7684\n\u6a19\u91cf\u734e\u52f5\u6a5f\u5236\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u589e\u5f37\u7684\u53ef\u89e3\u91cb\u6027\u548c\u66f4\u5177\u8cc7\u8a0a\u6027\u7684\n\u56de\u994b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u52d5\u614b\u734e\u52f5\u8abf\u6574\uff0c\u4e00\u7a2e\u6839\u64da\n\u734e\u52f5\u4fe1\u865f\u8abf\u6574\u6bcf\u500b\u7bc4\u4f8b\u7684\u640d\u5931\u6b0a\u91cd\u7684\uff0c\u5f9e\u800c\u6700\u4f73\u5316\n\u9ad8\u54c1\u8cea\u6bd4\u8f03\u5c0d\u7684\u4f7f\u7528\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\n$\\mathbf{10}$ \u500b\u4e0d\u540c\u7684\u7dad\u5ea6\u548c $\\mathbf{27}$ \u500b\u57fa\u6e96\u4e0a\u9032\u884c\u4e86\u56b4\u683c\u8a55\u4f30\uff0c\u7d50\u679c\n\u8b49\u660e\u6a21\u578b\u6548\u80fd\u6709\u986f\u8457\u4e14\u4e00\u81f4\u7684\u6539\u5584\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u4f7f\u7528 MM-RLHF \u548c\u6211\u5011\u7684\u6bd4\u5c0d\u6f14\u7b97\u6cd5\u5fae\u8abf LLaVA-ov-7B\n\u5c0e\u81f4\u5c0d\u8a71\u80fd\u529b\u63d0\u5347 $\\mathbf{19.5}$%\uff0c\u4e26\u4e14\n\u5b89\u5168\u6027\u6539\u5584\u4e86 $\\mathbf{60}$%\u3002\n\u6211\u5011\u5df2\u7d93\u958b\u6e90\u4e86\u504f\u597d\u8cc7\u6599\u96c6\u3001\u734e\u52f5\u6a21\u578b\u3001\u8a13\u7df4\u548c\n\u8a55\u4f30\u7a0b\u5f0f\u78bc\uff0c\u4ee5\u53ca\u734e\u52f5\u5efa\u6a21\u548c\u5b89\u5168\u6027\u57fa\u6e96\u3002\u6709\u95dc\u66f4\u591a\n\u8a73\u7d30\u8cc7\u8a0a\uff0c\u8acb\u8a2a\u554f\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\uff1ahttps://mm-rlhf.github.io\u3002</paragraph>", "author": "Yi-Fan Zhang et.al.", "authors": "Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan", "id": "2502.10391v1", "paper_url": "http://arxiv.org/abs/2502.10391v1", "repo": "null"}}