{"2406.18871": {"publish_time": "2024-06-27", "title": "DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment", "paper_summary": "Recent speech language models (SLMs) typically incorporate pre-trained speech\nmodels to extend the capabilities from large language models (LLMs). In this\npaper, we propose a Descriptive Speech-Text Alignment approach that leverages\nspeech captioning to bridge the gap between speech and text modalities,\nenabling SLMs to interpret and generate comprehensive natural language\ndescriptions, thereby facilitating the capability to understand both linguistic\nand non-linguistic features in speech. Enhanced with the proposed approach, our\nmodel demonstrates superior performance on the Dynamic-SUPERB benchmark,\nparticularly in generalizing to unseen tasks. Moreover, we discover that the\naligned model exhibits a zero-shot instruction-following capability without\nexplicit speech instruction tuning. These findings highlight the potential to\nreshape instruction-following SLMs by incorporating rich, descriptive speech\ncaptions.", "paper_summary_zh": "\u6700\u8fd1\u7684\u8bed\u97f3\u8a9e\u8a00\u6a21\u578b (SLM) \u901a\u5e38\u6703\u6574\u5408\u9810\u5148\u8a13\u7df4\u597d\u7684\u8a9e\u97f3\u6a21\u578b\uff0c\u4ee5\u64f4\u5145\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u63cf\u8ff0\u6027\u8a9e\u97f3\u6587\u5b57\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u5229\u7528\u8a9e\u97f3\u5b57\u5e55\u4f86\u5f4c\u5408\u8a9e\u97f3\u548c\u6587\u5b57\u6a21\u5f0f\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u8b93 SLM \u80fd\u5920\u8a6e\u91cb\u4e26\u7522\u751f\u5168\u9762\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\uff0c\u9032\u800c\u4fc3\u9032\u7406\u89e3\u8a9e\u97f3\u4e2d\u8a9e\u8a00\u548c\u975e\u8a9e\u8a00\u7279\u5fb5\u7684\u80fd\u529b\u3002\u900f\u904e\u5efa\u8b70\u7684\u65b9\u6cd5\u5f37\u5316\u5f8c\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 Dynamic-SUPERB \u57fa\u6e96\u4e0a\u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u63a8\u5ee3\u5230\u672a\u898b\u4efb\u52d9\u6642\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u5c0d\u9f4a\u6a21\u578b\u5c55\u73fe\u51fa\u96f6\u6b21\u5b78\u7fd2\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u7121\u9700\u660e\u78ba\u7684\u8a9e\u97f3\u6307\u4ee4\u8abf\u6574\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u51fa\u900f\u904e\u6574\u5408\u8c50\u5bcc\u7684\u63cf\u8ff0\u6027\u8a9e\u97f3\u5b57\u5e55\uff0c\u6539\u9020\u6307\u4ee4\u9075\u5faa SLM \u7684\u6f5b\u529b\u3002", "author": "Ke-Han Lu et.al.", "authors": "Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, He Huang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee", "id": "2406.18871v1", "paper_url": "http://arxiv.org/abs/2406.18871v1", "repo": "null"}}