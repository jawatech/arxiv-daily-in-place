{"2409.10080": {"publish_time": "2024-09-16", "title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion", "paper_summary": "Multi-modality image fusion aims to integrate complementary data information\nfrom different imaging modalities into a single image. Existing methods often\ngenerate either blurry fused images that lose fine-grained semantic information\nor unnatural fused images that appear perceptually cropped from the inputs. In\nthis work, we propose a novel two-phase discriminative autoencoder framework,\ntermed DAE-Fuse, that generates sharp and natural fused images. In the\nadversarial feature extraction phase, we introduce two discriminative blocks\ninto the encoder-decoder architecture, providing an additional adversarial loss\nto better guide feature extraction by reconstructing the source images. While\nthe two discriminative blocks are adapted in the attention-guided\ncross-modality fusion phase to distinguish the structural differences between\nthe fused output and the source inputs, injecting more naturalness into the\nresults. Extensive experiments on public infrared-visible, medical image\nfusion, and downstream object detection datasets demonstrate our method's\nsuperiority and generalizability in both quantitative and qualitative\nevaluations.", "paper_summary_zh": "\u591a\u6a21\u614b\u5f71\u50cf\u878d\u5408\u65e8\u5728\u5c07\u4f86\u81ea\u4e0d\u540c\u5f71\u50cf\u6a21\u614b\u7684\u4e92\u88dc\u8cc7\u6599\u8cc7\u8a0a\u6574\u5408\u5230\u55ae\u4e00\u5f71\u50cf\u4e2d\u3002\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u6703\u7522\u751f\u6a21\u7cca\u7684\u878d\u5408\u5f71\u50cf\uff0c\u5931\u53bb\u7d30\u7dfb\u7684\u8a9e\u610f\u8cc7\u8a0a\uff0c\u6216\u662f\u4e0d\u81ea\u7136\u7684\u878d\u5408\u5f71\u50cf\uff0c\u5728\u611f\u77e5\u4e0a\u770b\u8d77\u4f86\u50cf\u662f\u5f9e\u8f38\u5165\u4e2d\u88c1\u5207\u51fa\u4f86\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u5169\u968e\u6bb5\u5224\u5225\u5f0f\u81ea\u7de8\u78bc\u5668\u6846\u67b6\uff0c\u7a31\u70ba DAE-Fuse\uff0c\u53ef\u7522\u751f\u6e05\u6670\u4e14\u81ea\u7136\u7684\u878d\u5408\u5f71\u50cf\u3002\u5728\u5c0d\u6297\u7279\u5fb5\u63d0\u53d6\u968e\u6bb5\uff0c\u6211\u5011\u5728\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u67b6\u69cb\u4e2d\u5f15\u5165\u5169\u500b\u5224\u5225\u5f0f\u5340\u584a\uff0c\u63d0\u4f9b\u984d\u5916\u7684\u5c0d\u6297\u640d\u5931\uff0c\u85c9\u7531\u91cd\u5efa\u539f\u59cb\u5f71\u50cf\u4f86\u66f4\u597d\u5730\u5f15\u5c0e\u7279\u5fb5\u63d0\u53d6\u3002\u96d6\u7136\u5169\u500b\u5224\u5225\u5f0f\u5340\u584a\u5728\u6ce8\u610f\u529b\u5f15\u5c0e\u7684\u8de8\u6a21\u614b\u878d\u5408\u968e\u6bb5\u4e2d\u9032\u884c\u8abf\u6574\uff0c\u4ee5\u5340\u5206\u878d\u5408\u8f38\u51fa\u8207\u539f\u59cb\u8f38\u5165\u4e4b\u9593\u7684\u7d50\u69cb\u5dee\u7570\uff0c\u70ba\u7d50\u679c\u6ce8\u5165\u66f4\u591a\u81ea\u7136\u6027\u3002\u91dd\u5c0d\u516c\u958b\u7d05\u5916\u7dda\u53ef\u898b\u5149\u3001\u91ab\u5b78\u5f71\u50cf\u878d\u5408\u548c\u4e0b\u6e38\u7269\u4ef6\u5075\u6e2c\u8cc7\u6599\u96c6\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u91cf\u5316\u548c\u5b9a\u6027\u8a55\u4f30\u4e2d\u7684\u512a\u8d8a\u6027\u548c\u6cdb\u5316\u6027\u3002", "author": "Yuchen Guo et.al.", "authors": "Yuchen Guo, Ruoxiang Xu, Rongcheng Li, Zhenghao Wu, Weifeng Su", "id": "2409.10080v1", "paper_url": "http://arxiv.org/abs/2409.10080v1", "repo": "null"}}