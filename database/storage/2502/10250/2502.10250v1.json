{"2502.10250": {"publish_time": "2025-02-14", "title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "paper_summary": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u5404\u7a2e\u8996\u89ba\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u53d7\u5230\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u8996\u89ba\u5fae\u8abf\u8cc7\u6599\u7684\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u5f15\u9032 VisCon-100K\uff0c\u9019\u662f\u4e00\u500b\u5f9e\u4ea4\u932f\u5716\u50cf\u6587\u5b57\u7db2\u8def\u6587\u4ef6\u4e2d\u884d\u751f\u7684\u65b0\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c07 OBELICS \u8cc7\u6599\u96c6\u4e2d\u7684 45K \u7db2\u8def\u6587\u4ef6\u8f49\u63db\u6210 100K \u7684\u5716\u50cf\u5c0d\u8a71\u7bc4\u4f8b\u3002\u6211\u5011\u5229\u7528 GPT-4V \u7522\u751f\u5716\u50cf\u60c5\u5883\u6a19\u984c\uff0c\u4e26\u4f7f\u7528 OpenChat 3.5 \u6a21\u578b\u5c07\u9019\u4e9b\u6a19\u984c\u8f49\u63db\u6210\u591a\u6a23\u5316\u7684\u81ea\u7531\u5f62\u5f0f\u548c\u591a\u9078\u984c\u554f\u7b54\u914d\u5c0d\u3002\u6574\u5408\u9019\u500b\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\uff0c\u5927\u5e45\u63d0\u5347\u4e86 VLM \u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u8868\u73fe\u3002\u8207\u50c5\u5c08\u6ce8\u65bc\u7d30\u5fae\u8996\u89ba\u5167\u5bb9\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u96a8\u9644\u7684\u7db2\u8def\u60c5\u5883\uff0c\u7522\u751f\u4e86\u66f4\u512a\u7570\u7684\u7d50\u679c\u3002\u6211\u5011\u9084\u767c\u73fe\u300c\u5916\u6d29\u7684\u6a21\u5f0f\u7d44\u5408\u300d\uff0c\u5176\u4e2d\u5c0d\u8a71\u7bc4\u4f8b\u5305\u542b\u53ef\u5f9e\u5716\u50cf\u53ca\u5176\u60c5\u5883\u6a19\u984c\u56de\u7b54\u7684\u554f\u984c\uff0c\u5176\u8868\u73fe\u512a\u65bc\u6a19\u984c\u548c\u554f\u7b54\u914d\u5c0d\u7684\u975e\u5916\u6d29\u7d44\u5408\u3002VisCon-100k \u8cc7\u6599\u96c6\u5728\u5169\u7a2e\u6d41\u884c\u7684 VLM \u65b9\u6cd5\u4e2d\u8868\u73fe\u51fa\u8272\uff1a\u8207\u4f7f\u7528\u5716\u50cf\u6a19\u984c\u8cc7\u6599 (ShareGPT4V-7b) \u7684\u8996\u89ba\u7de8\u78bc\u5668\u5c0d\u9f4a\u7684\u7d14\u6587\u5b57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u53ca\u4f7f\u7528\u4ea4\u932f\u5716\u50cf\u6587\u5b57\u8cc7\u6599\u7684\u591a\u6a21\u614b\u9810\u8a13\u7df4 LLM (IDEFICS2-8b)\u3002\u9664\u4e86\u91cb\u51fa VisCon-100K \u8cc7\u6599\u96c6\uff0c\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u4e00\u500b\u5728\u9019\u500b\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u60c5\u5883\u6a19\u984c\u7522\u751f\u5668\uff0c\u4fc3\u9032\u4e86\u53ef\u64f4\u5145\u5fae\u8abf\u8cc7\u6599\u7684\u7522\u751f\uff0c\u4ee5\u5229\u672a\u4f86\u7684\u7814\u7a76\u548c\u958b\u6e90\u61c9\u7528\u3002\u4f7f\u7528\u76f8\u540c\u7684\u7ba1\u9053\uff0c\u4f46\u5c07\u6211\u5011\u8a13\u7df4\u7684\u60c5\u5883\u6a19\u984c\u7522\u751f\u5668\u66ff\u63db\u70ba GPT-4V\uff0c\u6211\u5011\u4e5f\u91cb\u51fa\u4e86\u8f03\u5927\u7684 VisCon-1M \u8cc7\u6599\u96c6\u3002", "author": "Gokul Karthik Kumar et.al.", "authors": "Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu", "id": "2502.10250v1", "paper_url": "http://arxiv.org/abs/2502.10250v1", "repo": "null"}}