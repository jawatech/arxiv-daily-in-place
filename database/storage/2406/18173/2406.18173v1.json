{"2406.18173": {"publish_time": "2024-06-26", "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs", "paper_summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.", "paper_summary_zh": "\u7ba1\u7406\u9577\u6587\u672c\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8aaa\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u5176\u5167\u5bb9\u8996\u7a97\u5927\u5c0f\u6709\u9650\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86 UIO-LLM\uff0c\u9019\u662f\u4e00\u7a2e\u7121\u504f\u7684\u905e\u589e\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u7528\u65bc\u5728\u9577\u5167\u5bb9\u8a2d\u5b9a\u4e0b\u8a18\u61b6\u9ad4\u589e\u5f37\u7684Transformer\u3002\u6211\u5011\u6700\u521d\u5c07\u6b64\u904e\u7a0b\u6982\u5ff5\u5316\u70ba\u7c21\u5316\u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6846\u67b6\uff0c\u5176\u4e2d\u6b0a\u91cd\u5171\u4eab\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u5206\u5225\u5c07\u5167\u5bb9\u5340\u6bb5\u5c01\u88dd\u5230\u8a18\u61b6\u9ad4\u4e2d\uff0c\u4e26\u5229\u7528\u9019\u4e9b\u8a18\u61b6\u9ad4\u4f86\u9810\u6e2c\u5f8c\u7e8c\u5340\u6bb5\u7684\u8f38\u51fa\u3002\u96a8\u5f8c\uff0c\u901a\u904e\u5c07\u6211\u5011\u8a18\u61b6\u9ad4\u589e\u5f37\u7684Transformer\u8996\u70ba\u5168\u9023\u63a5\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN)\uff0c\u6211\u5011\u4f7f\u7528\u622a\u65b7\u6642\u9593\u53cd\u5411\u50b3\u64ad (TBPTT) \u6f14\u7b97\u6cd5\u6539\u9032\u8a13\u7df4\u904e\u7a0b\uff0c\u8a72\u6f14\u7b97\u6cd5\u7d50\u5408\u4e86\u5275\u65b0\u7684\u905e\u589e\u6700\u4f73\u5316\u6280\u8853\u3002\u9019\u4e9b\u6280\u8853\u4e0d\u50c5\u964d\u4f4e\u4e86\u6642\u9593\u8907\u96dc\u5ea6\uff0c\u9084\u901a\u904e\u7121\u504f\u6700\u4f73\u5316\u904e\u7a0b\u89e3\u6c7a\u4e86\u68af\u5ea6\u8a08\u7b97\u4e2d\u7684\u504f\u5dee\u3002UIO-LLM \u6210\u529f\u8655\u7406\u9577\u5167\u5bb9\uff0c\u4f8b\u5982\u5c07 Llama2-7b-chat \u7684\u5167\u5bb9\u8996\u7a97\u5f9e 4K \u5ef6\u4f38\u5230 100K \u500b\u7b26\u865f\uff0c\u984d\u5916\u53c3\u6578\u6700\u5c11\u589e\u52a0 2%\uff0c\u540c\u6642\u96a8\u8457\u5167\u5bb9\u9577\u5ea6\u7684\u589e\u52a0\uff0c\u63a8\u7406\u6210\u672c\u5e7e\u4e4e\u4fdd\u6301\u7dda\u6027\u3002", "author": "Wenhao Li et.al.", "authors": "Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji", "id": "2406.18173v1", "paper_url": "http://arxiv.org/abs/2406.18173v1", "repo": "null"}}