{"2410.23526": {"publish_time": "2024-10-31", "title": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models", "paper_summary": "Large language models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks, yet they often struggle with maintaining\nfactual accuracy, particularly in knowledge-intensive domains like healthcare.\nThis study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,\na novel approach designed to enhance the factual reliability of LLMs, with a\nfocus on medical question answering (QA). LEAF utilizes a dual strategy to\nenhance the factual accuracy of responses from models such as Llama 3 70B\nInstruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,\nimproves Retrieval-Augmented Generation (RAG) by incorporating fact-checking\nresults to guide the retrieval process without updating model parameters. The\nsecond strategy, Learning from Fact-Checks via Self-Training, involves\nsupervised fine-tuning (SFT) on fact-checked responses or applying Simple\nPreference Optimization (SimPO) with fact-checking as a ranking mechanism, both\nupdating LLM parameters from supervision. These findings suggest that\nintegrating fact-checked responses whether through RAG enhancement or\nself-training enhances the reliability and factual correctness of LLM outputs,\noffering a promising solution for applications where information accuracy is\ncrucial.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u7136\u800c\u5b83\u5011\u5728\u7dad\u6301\u4e8b\u5be6\u6e96\u78ba\u6027\u65b9\u9762\u5e38\u5e38\u9762\u81e8\u56f0\u96e3\uff0c\u7279\u5225\u662f\u5728\u50cf\u91ab\u7642\u4fdd\u5065\u9019\u6a23\u7684\u77e5\u8b58\u5bc6\u96c6\u9818\u57df\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86 LEAF\uff1a\u900f\u904e\u4e8b\u5be6\u67e5\u6838\u589e\u5f37\u7684\u5b78\u7fd2\u8207\u8a55\u4f30\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347 LLM \u7684\u4e8b\u5be6\u53ef\u9760\u6027\uff0c\u4e26\u5c08\u6ce8\u65bc\u91ab\u7642\u554f\u984c\u89e3\u7b54 (QA)\u3002LEAF \u5229\u7528\u96d9\u91cd\u7b56\u7565\u4f86\u63d0\u5347 LLM \u56de\u61c9\u7684\u4e8b\u5be6\u6e96\u78ba\u6027\uff0c\u4f8b\u5982 Llama 3 70B Instruct \u548c Llama 3 8B Instruct\u3002\u7b2c\u4e00\u7a2e\u7b56\u7565 Fact-Check-Then-RAG\uff0c\u900f\u904e\u6574\u5408\u4e8b\u5be6\u67e5\u6838\u7d50\u679c\u4f86\u6539\u9032\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG)\uff0c\u4ee5\u5f15\u5c0e\u6aa2\u7d22\u7a0b\u5e8f\uff0c\u800c\u4e0d\u6703\u66f4\u65b0\u6a21\u578b\u53c3\u6578\u3002\u7b2c\u4e8c\u7a2e\u7b56\u7565\u900f\u904e\u81ea\u6211\u8a13\u7df4\u5b78\u7fd2\u4e8b\u5be6\u67e5\u6838\uff0c\u6d89\u53ca\u91dd\u5c0d\u7d93\u904e\u4e8b\u5be6\u67e5\u6838\u7684\u56de\u61c9\u9032\u884c\u76e3\u7763\u5fae\u8abf (SFT)\uff0c\u6216\u5c07\u7c21\u55ae\u504f\u597d\u6700\u4f73\u5316 (SimPO) \u61c9\u7528\u65bc\u4e8b\u5be6\u67e5\u6838\u4f5c\u70ba\u6392\u540d\u6a5f\u5236\uff0c\u9019\u5169\u7a2e\u65b9\u6cd5\u90fd\u6703\u5f9e\u76e3\u7763\u4e2d\u66f4\u65b0 LLM \u53c3\u6578\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0c\u7121\u8ad6\u662f\u900f\u904e RAG \u589e\u5f37\u6216\u81ea\u6211\u8a13\u7df4\uff0c\u6574\u5408\u7d93\u904e\u4e8b\u5be6\u67e5\u6838\u7684\u56de\u61c9\uff0c\u90fd\u80fd\u63d0\u5347 LLM \u8f38\u51fa\u7684\u53ef\u9760\u6027\u548c\u4e8b\u5be6\u6b63\u78ba\u6027\uff0c\u70ba\u8cc7\u8a0a\u6e96\u78ba\u6027\u81f3\u95dc\u91cd\u8981\u7684\u61c9\u7528\u7a0b\u5f0f\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Hieu Tran et.al.", "authors": "Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, Terrence Chen", "id": "2410.23526v1", "paper_url": "http://arxiv.org/abs/2410.23526v1", "repo": "null"}}