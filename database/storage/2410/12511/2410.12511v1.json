{"2410.12511": {"publish_time": "2024-10-16", "title": "Advancing Fairness in Natural Language Processing: From Traditional Methods to Explainability", "paper_summary": "The burgeoning field of Natural Language Processing (NLP) stands at a\ncritical juncture where the integration of fairness within its frameworks has\nbecome an imperative. This PhD thesis addresses the need for equity and\ntransparency in NLP systems, recognizing that fairness in NLP is not merely a\ntechnical challenge but a moral and ethical necessity, requiring a rigorous\nexamination of how these technologies interact with and impact diverse human\npopulations. Through this lens, this thesis undertakes a thorough investigation\ninto the development of equitable NLP methodologies and the evaluation of\nbiases that prevail in current systems.\n  First, it introduces an innovative algorithm to mitigate biases in\nmulti-class classifiers, tailored for high-risk NLP applications, surpassing\ntraditional methods in both bias mitigation and prediction accuracy. Then, an\nanalysis of the Bios dataset reveals the impact of dataset size on\ndiscriminatory biases and the limitations of standard fairness metrics. This\nawareness has led to explorations in the field of explainable AI, aiming for a\nmore complete understanding of biases where traditional metrics are limited.\nConsequently, the thesis presents COCKATIEL, a model-agnostic explainability\nmethod that identifies and ranks concepts in Transformer models, outperforming\nprevious approaches in sentiment analysis tasks. Finally, the thesis\ncontributes to bridging the gap between fairness and explainability by\nintroducing TaCo, a novel method to neutralize bias in Transformer model\nembeddings.\n  In conclusion, this thesis constitutes a significant interdisciplinary\nendeavor that intertwines explicability and fairness to challenge and reshape\ncurrent NLP paradigms. The methodologies and critiques presented contribute to\nthe ongoing discourse on fairness in machine learning, offering actionable\nsolutions for more equitable and responsible AI systems.", "paper_summary_zh": "<paragraph>\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u84ec\u52c3\u767c\u5c55\u7684\u9818\u57df\u6b63\u8655\u65bc\u4e00\u500b\u95dc\u9375\u6642\u523b\uff0c\u5728\u5176\u4e2d\uff0c\u516c\u5e73\u6027\u7684\u6574\u5408\u5df2\u6210\u70ba\u5176\u67b6\u69cb\u4e2d\u7684\u4e00\u9805\u8981\u52d9\u3002\u672c\u535a\u58eb\u8ad6\u6587\u63a2\u8a0e\u4e86 NLP \u7cfb\u7d71\u4e2d\u516c\u5e73\u6027\u548c\u900f\u660e\u6027\u7684\u9700\u6c42\uff0c\u4e26\u8a8d\u8b58\u5230 NLP \u4e2d\u7684\u516c\u5e73\u6027\u4e0d\u50c5\u50c5\u662f\u4e00\u9805\u6280\u8853\u6311\u6230\uff0c\u66f4\u662f\u4e00\u9805\u9053\u5fb7\u548c\u502b\u7406\u5fc5\u8981\u6027\uff0c\u9700\u8981\u56b4\u683c\u5be9\u67e5\u9019\u4e9b\u6280\u8853\u5982\u4f55\u8207\u4e0d\u540c\u7684\u4eba\u7fa4\u4e92\u52d5\u4e26\u5c0d\u5176\u7522\u751f\u5f71\u97ff\u3002\u900f\u904e\u6b64\u89c0\u9ede\uff0c\u672c\u8ad6\u6587\u5c0d\u516c\u5e73 NLP \u65b9\u6cd5\u7684\u958b\u767c\u4ee5\u53ca\u7576\u524d\u7cfb\u7d71\u4e2d\u666e\u904d\u5b58\u5728\u7684\u504f\u5dee\u9032\u884c\u4e86\u5fb9\u5e95\u8abf\u67e5\u3002\n\u9996\u5148\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u6f14\u7b97\u6cd5\u4f86\u6e1b\u8f15\u591a\u985e\u5225\u5206\u985e\u5668\u4e2d\u7684\u504f\u5dee\uff0c\u5c08\u9580\u91dd\u5c0d\u9ad8\u98a8\u96aa NLP \u61c9\u7528\uff0c\u5728\u504f\u5dee\u6e1b\u8f15\u548c\u9810\u6e2c\u6e96\u78ba\u6027\u65b9\u9762\u90fd\u512a\u65bc\u50b3\u7d71\u65b9\u6cd5\u3002\u7136\u5f8c\uff0c\u5c0d Bios \u8cc7\u6599\u96c6\u7684\u5206\u6790\u63ed\u793a\u4e86\u8cc7\u6599\u96c6\u5927\u5c0f\u5c0d\u6b67\u8996\u6027\u504f\u5dee\u7684\u5f71\u97ff\u4ee5\u53ca\u6a19\u6e96\u516c\u5e73\u6027\u6307\u6a19\u7684\u9650\u5236\u3002\u9019\u7a2e\u8a8d\u77e5\u5df2\u5c0e\u81f4\u5728\u53ef\u89e3\u91cb AI \u9818\u57df\u7684\u63a2\u7d22\uff0c\u65e8\u5728\u66f4\u5168\u9762\u5730\u4e86\u89e3\u50b3\u7d71\u6307\u6a19\u53d7\u9650\u7684\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u672c\u8ad6\u6587\u63d0\u51fa\u4e86 COCKATIEL\uff0c\u9019\u662f\u4e00\u7a2e\u8207\u6a21\u578b\u7121\u95dc\u7684\u53ef\u89e3\u91cb\u6027\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u8b58\u5225\u548c\u6392\u5217 Transformer \u6a21\u578b\u4e2d\u7684\u6982\u5ff5\uff0c\u5728\u60c5\u7dd2\u5206\u6790\u4efb\u52d9\u4e2d\u512a\u65bc\u5148\u524d\u7684\u505a\u6cd5\u3002\u6700\u5f8c\uff0c\u672c\u8ad6\u6587\u900f\u904e\u5f15\u5165 TaCo\uff0c\u4e00\u7a2e\u5728 Transformer \u6a21\u578b\u5d4c\u5165\u4e2d\u6d88\u9664\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\uff0c\u70ba\u5f4c\u5408\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91cb\u6027\u4e4b\u9593\u7684\u5dee\u8ddd\u505a\u51fa\u4e86\u8ca2\u737b\u3002\n\u7e3d\u4e4b\uff0c\u672c\u8ad6\u6587\u69cb\u6210\u4e86\u4e00\u9805\u91cd\u8981\u7684\u8de8\u5b78\u79d1\u52aa\u529b\uff0c\u5b83\u5c07\u53ef\u89e3\u91cb\u6027\u548c\u516c\u5e73\u6027\u4ea4\u7e54\u5728\u4e00\u8d77\uff0c\u4ee5\u6311\u6230\u548c\u91cd\u5851\u7576\u524d\u7684 NLP \u5178\u7bc4\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6279\u8a55\u6709\u52a9\u65bc\u95dc\u65bc\u6a5f\u5668\u5b78\u7fd2\u4e2d\u516c\u5e73\u6027\u7684\u6301\u7e8c\u8a0e\u8ad6\uff0c\u70ba\u66f4\u516c\u5e73\u4e14\u8ca0\u8cac\u4efb\u7684 AI \u7cfb\u7d71\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\u3002</paragraph>", "author": "Fanny Jourdan et.al.", "authors": "Fanny Jourdan", "id": "2410.12511v1", "paper_url": "http://arxiv.org/abs/2410.12511v1", "repo": "null"}}