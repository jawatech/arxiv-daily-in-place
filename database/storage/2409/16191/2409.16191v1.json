{"2409.16191": {"publish_time": "2024-09-24", "title": "HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models", "paper_summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5404\u79cd\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u957f\u6587\u672c\u7406\u89e3\uff09\u4e2d\u5c55\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u57fa\u51c6\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6 (HelloBench)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u3001\u771f\u5b9e\u7684\u3001\u5f00\u653e\u5f0f\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30 LLM \u5728\u751f\u6210\u957f\u6587\u672c\u65b9\u9762\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\uff0cHelloBench \u5c06\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u5206\u4e3a\u4e94\u4e2a\u5b50\u4efb\u52a1\uff1a\u5f00\u653e\u5f0f\u95ee\u7b54\u3001\u6458\u8981\u3001\u804a\u5929\u3001\u6587\u672c\u5b8c\u6210\u548c\u542f\u53d1\u5f0f\u6587\u672c\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u957f\u6587\u672c\u8bc4\u4f30 (HelloEval)\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0e\u4eba\u7c7b\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b83\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u7c7b\u8bc4\u4f30\u6240\u9700\u7684\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\u3002\u6211\u4eec\u5bf9\u5927\u7ea6 30 \u4e2a\u4e3b\u6d41 LLM \u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u89c2\u5bdf\u5230\u5f53\u524d\u7684 LLM \u7f3a\u4e4f\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\uff0c\u65e0\u8bba\u6307\u4ee4\u662f\u5426\u5305\u542b\u660e\u786e\u6216\u9690\u542b\u7684\u957f\u5ea6\u9650\u5236\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5927\u591a\u6570 LLM \u90fd\u65e0\u6cd5\u751f\u6210\u957f\u5ea6\u8d85\u8fc7 4000 \u5b57\u7684\u6587\u672c\u3002\u5176\u6b21\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u867d\u7136\u4e00\u4e9b LLM \u53ef\u4ee5\u751f\u6210\u66f4\u957f\u7684\u6587\u672c\uff0c\u4f46\u5b58\u5728\u8bb8\u591a\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u4e25\u91cd\u7684\u91cd\u590d\u548c\u8d28\u91cf\u4e0b\u964d\uff09\u3002\u7b2c\u4e09\uff0c\u4e3a\u4e86\u8bc1\u660e HelloEval \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5c06 HelloEval \u4e0e\u4f20\u7edf\u6307\u6807\uff08\u4f8b\u5982\uff0cROUGE\u3001BLEU \u7b49\uff09\u548c LLM-as-a-Judge \u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e HelloEval \u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6700\u9ad8\u3002\u6211\u4eec\u5728 https://github.com/Quehry/HelloBench \u4e2d\u53d1\u5e03\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u3002", "author": "Haoran Que et.al.", "authors": "Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen", "id": "2409.16191v1", "paper_url": "http://arxiv.org/abs/2409.16191v1", "repo": "https://github.com/quehry/hellobench"}}