{"2408.02945": {"publish_time": "2024-08-06", "title": "Self-Supervised Learning for Multi-Channel Neural Transducer", "paper_summary": "Self-supervised learning, such as with the wav2vec 2.0 framework\nsignificantly improves the accuracy of end-to-end automatic speech recognition\n(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In\nthis work, we explored a self-supervised learning method for a multi-channel\nend-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel\nend-to-end ASR model, we focused on a multi-channel neural transducer. In\npre-training, we compared three different methods for feature quantization to\ntrain a multi-channel conformer audio encoder: joint quantization, feature-wise\nquantization and channel-wise quantization. In fine-tuning, we trained the\nmulti-channel conformer-transducer. All experiments were conducted using the\nfar-field in-house and CHiME-4 datasets. The results of the experiments showed\nthat feature-wise quantization was the most effective among the methods. We\nobserved a 66% relative reduction in character error rate compared with the\nmodel without any pre-training for the far-field in-house dataset.", "paper_summary_zh": "\u81ea\u76e3\u7763\u5b78\u7fd2\uff0c\u4f8b\u5982\u4f7f\u7528 wav2vec 2.0 \u67b6\u69cb\uff0c\u5927\u5e45\u63d0\u5347\u7aef\u5c0d\u7aef\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u7684\u6e96\u78ba\u5ea6\u3002Wav2vec 2.0 \u5df2\u61c9\u7528\u65bc\u55ae\u8072\u9053\u7aef\u5c0d\u7aef ASR \u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u57fa\u65bc wav2vec 2.0 \u67b6\u69cb\u7684\u591a\u8072\u9053\u7aef\u5c0d\u7aef ASR \u6a21\u578b\u7684\u81ea\u6211\u76e3\u7763\u5b78\u7fd2\u65b9\u6cd5\u3002\u4f5c\u70ba\u591a\u8072\u9053\u7aef\u5c0d\u7aef ASR \u6a21\u578b\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u591a\u8072\u9053\u795e\u7d93\u8f49\u63db\u5668\u3002\u5728\u9810\u8a13\u7df4\u4e2d\uff0c\u6211\u5011\u6bd4\u8f03\u4e86\u4e09\u7a2e\u4e0d\u540c\u7684\u7279\u5fb5\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u8a13\u7df4\u591a\u8072\u9053\u69cb\u5f62\u97f3\u8a0a\u7de8\u78bc\u5668\uff1a\u806f\u5408\u91cf\u5316\u3001\u7279\u5fb5\u91cf\u5316\u548c\u901a\u9053\u91cf\u5316\u3002\u5728\u5fae\u8abf\u4e2d\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u591a\u8072\u9053\u69cb\u5f62\u8f49\u63db\u5668\u3002\u6240\u6709\u5be6\u9a57\u5747\u4f7f\u7528\u9060\u5834\u5167\u90e8\u548c CHiME-4 \u8cc7\u6599\u96c6\u9032\u884c\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5728\u9019\u4e9b\u65b9\u6cd5\u4e2d\uff0c\u7279\u5fb5\u91cf\u5316\u662f\u6700\u6709\u6548\u7684\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u8207\u6c92\u6709\u4efb\u4f55\u9810\u8a13\u7df4\u7684\u9060\u5834\u5167\u90e8\u8cc7\u6599\u96c6\u6a21\u578b\u76f8\u6bd4\uff0c\u5b57\u5143\u932f\u8aa4\u7387\u76f8\u5c0d\u6e1b\u5c11\u4e86 66%\u3002", "author": "Atsushi Kojima et.al.", "authors": "Atsushi Kojima", "id": "2408.02945v1", "paper_url": "http://arxiv.org/abs/2408.02945v1", "repo": "null"}}