{"2411.13281": {"publish_time": "2024-11-20", "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation", "paper_summary": "Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.", "paper_summary_zh": "<paragraph>\u5177\u6709\u5148\u8fdb\u89c6\u9891\u5206\u6790\u529f\u80fd\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u8fd1\u671f\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u8bc4\u4f30\u90fd\u4f9d\u8d56\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f8b\u5982 VideoMME \u548c LongVideoBench \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bb9\u6613\u7f3a\u4e4f\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u7528\u6237\u590d\u6742\u9700\u6c42\u6240\u9700\u7684\u6df1\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u5e76\u4e14\u7531\u4e8e\u89c6\u9891\u4efb\u52a1\u7684\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u901f\u5ea6\u7f13\u6162\uff0c\u6211\u4eec\u5f15\u5165\u4e86 VideoAutoArena\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7 LMSYS Chatbot Arena \u6846\u67b6\u542f\u53d1\u7684\u7ade\u6280\u573a\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u81ea\u52a8\u8bc4\u4f30 LMM \u7684\u89c6\u9891\u5206\u6790\u80fd\u529b\u3002VideoAutoArena \u5229\u7528\u7528\u6237\u6a21\u62df\u6765\u751f\u6210\u5f00\u653e\u5f0f\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u4e25\u683c\u8bc4\u4f30\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u91c7\u7528\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7ecf\u8fc7\u4fee\u6539\u7684 ELO \u8bc4\u7ea7\u7cfb\u7edf\uff0c\u4ee5\u4fbf\u5bf9\u591a\u4e2a LMM \u8fdb\u884c\u516c\u5e73\u4e14\u6301\u7eed\u7684\u6bd4\u8f83\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u81ea\u52a8\u5316\u8bc4\u5224\u7cfb\u7edf\uff0c\u6211\u4eec\u4f7f\u7528\u7cbe\u5fc3\u6311\u9009\u7684\u4eba\u5de5\u6807\u6ce8\u5b50\u96c6\u6784\u5efa\u4e86\u4e00\u4e2a\u201c\u9ec4\u91d1\u6807\u51c6\u201d\uff0c\u8868\u660e\u6211\u4eec\u7684\u7ade\u6280\u573a\u4e0e\u4eba\u5de5\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6545\u969c\u9a71\u52a8\u7684\u8fdb\u5316\u7b56\u7565\uff0c\u9010\u6b65\u589e\u52a0\u95ee\u9898\u590d\u6742\u6027\uff0c\u4ee5\u63a8\u52a8\u6a21\u578b\u5904\u7406\u66f4\u5177\u6311\u6218\u6027\u7684\u89c6\u9891\u5206\u6790\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVideoAutoArena \u6709\u6548\u5730\u533a\u5206\u4e86\u6700\u5148\u8fdb\u7684 LMM\uff0c\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u4f18\u52bf\u548c\u6539\u8fdb\u9886\u57df\u7684\u89c1\u89e3\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7b80\u5316\u6211\u4eec\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5f15\u5165\u4e86 VideoAutoBench \u4f5c\u4e3a\u8f85\u52a9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u4eba\u5de5\u6807\u6ce8\u5458\u5728 VideoAutoArena \u6218\u6597\u7684\u5b50\u96c6\u4e2d\u6807\u8bb0\u83b7\u80dc\u8005\u3002\u6211\u4eec\u4f7f\u7528 GPT-4o \u4f5c\u4e3a\u8bc4\u59d4\uff0c\u5c06\u54cd\u5e94\u4e0e\u8fd9\u4e9b\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u7684\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\u3002VideoAutoArena \u548c VideoAutoBench \u5171\u540c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u6d4e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u5206\u6790\u4e2d\u7684 LMM\u3002</paragraph>", "author": "Ziyang Luo et.al.", "authors": "Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, Junnan Li", "id": "2411.13281v1", "paper_url": "http://arxiv.org/abs/2411.13281v1", "repo": "null"}}