{"2411.19447": {"publish_time": "2024-11-29", "title": "Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine", "paper_summary": "In medical image analysis, achieving fast, efficient, and accurate\nsegmentation is essential for automated diagnosis and treatment. Although\nrecent advancements in deep learning have significantly improved segmentation\naccuracy, current models often face challenges in adaptability and\ngeneralization, particularly when processing multi-modal medical imaging data.\nThese limitations stem from the substantial variations between imaging\nmodalities and the inherent complexity of medical data. To address these\nchallenges, we propose the Strategy-driven Interactive Segmentation Model\n(SISeg), built on SAM2, which enhances segmentation performance across various\nmedical imaging modalities by integrating a selection engine. To mitigate\nmemory bottlenecks and optimize prompt frame selection during the inference of\n2D image sequences, we developed an automated system, the Adaptive Frame\nSelection Engine (AFSE). This system dynamically selects the optimal prompt\nframes without requiring extensive prior medical knowledge and enhances the\ninterpretability of the model's inference process through an interactive\nfeedback mechanism. We conducted extensive experiments on 10 datasets covering\n7 representative medical imaging modalities, demonstrating the SISeg model's\nrobust adaptability and generalization in multi-modal tasks. The project page\nand code will be available at: [URL].", "paper_summary_zh": "\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5206\u5272\u5bf9\u4e8e\u81ea\u52a8\u5316\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5e38\u5e38\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u6570\u636e\u65f6\u3002\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u5f71\u50cf\u65b9\u5f0f\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u5f02\u548c\u533b\u5b66\u6570\u636e\u7684\u56fa\u6709\u590d\u6742\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e SAM2 \u7684\u7b56\u7565\u9a71\u52a8\u4ea4\u4e92\u5f0f\u5206\u5272\u6a21\u578b (SISeg)\uff0c\u5b83\u901a\u8fc7\u96c6\u6210\u9009\u62e9\u5f15\u64ce\u6765\u589e\u5f3a\u5404\u79cd\u533b\u5b66\u5f71\u50cf\u65b9\u5f0f\u7684\u5206\u5272\u6027\u80fd\u3002\u4e3a\u4e86\u7f13\u89e3\u5185\u5b58\u74f6\u9888\u5e76\u4f18\u5316 2D \u56fe\u50cf\u5e8f\u5217\u63a8\u7406\u671f\u95f4\u7684\u63d0\u793a\u5e27\u9009\u62e9\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5373\u81ea\u9002\u5e94\u5e27\u9009\u62e9\u5f15\u64ce (AFSE)\u3002\u8be5\u7cfb\u7edf\u5728\u65e0\u9700\u5e7f\u6cdb\u7684\u5148\u524d\u533b\u5b66\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u52a8\u6001\u9009\u62e9\u6700\u4f73\u63d0\u793a\u5e27\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53cd\u9988\u673a\u5236\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u5728\u6db5\u76d6 7 \u79cd\u4ee3\u8868\u6027\u533b\u5b66\u5f71\u50cf\u65b9\u5f0f\u7684 10 \u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86 SISeg \u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u3002\u9879\u76ee\u9875\u9762\u548c\u4ee3\u7801\u5c06\u63d0\u4f9b\u5728\uff1a[URL]\u3002", "author": "Zhi Li et.al.", "authors": "Zhi Li, Kai Zhao, Yaqi Wang, Shuai Wang", "id": "2411.19447v1", "paper_url": "http://arxiv.org/abs/2411.19447v1", "repo": "null"}}