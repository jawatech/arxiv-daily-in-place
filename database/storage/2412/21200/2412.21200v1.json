{"2412.21200": {"publish_time": "2024-12-30", "title": "Distributed Mixture-of-Agents for Edge Inference with Large Language Models", "paper_summary": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.", "paper_summary_zh": "<paragraph>\u6df7\u5408\u4ee3\u7406 (MoA) \u6700\u8fd1\u88ab\u63d0\u8bae\u4f5c\u4e3a\u4e00\u79cd\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u8ba9\u591a\u4e2a\u72ec\u7acb\u7684 LLM \u80fd\u591f\u534f\u4f5c\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u79cd\u534f\u4f5c\u5f0f\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u5bf9\u7528\u6237\u63d0\u793a\u7684\u54cd\u5e94\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5355\u4e2a LLM\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8003\u8651\u4e86\u8fd9\u79cd MoA \u67b6\u6784\uff0c\u5176\u4e2d LLM \u5728\u5404\u4e2a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u6bcf\u4e2a\u8bbe\u5907\u90fd\u4e0e\u4e00\u4e2a\u7528\u6237\u552f\u4e00\u5173\u8054\uff0c\u5e76\u914d\u5907\u4e86\u81ea\u5df1\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u80fd\u529b\u3002\u8fd9\u4e9b\u8bbe\u5907\u4f7f\u7528\u5206\u6563\u5f0f\u516b\u5366\u7b97\u6cd5\u4ea4\u6362\u4fe1\u606f\uff0c\u5141\u8bb8\u4e0d\u540c\u7684\u8bbe\u5907\u8282\u70b9\u5728\u6ca1\u6709\u96c6\u4e2d\u5f0f\u670d\u52a1\u5668\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u901a\u4fe1\u3002\u5728\u8003\u8651\u7684\u8bbe\u7f6e\u4e2d\uff0c\u4e0d\u540c\u7684\u7528\u6237\u6709\u81ea\u5df1\u7684 LLM \u6a21\u578b\u6765\u5904\u7406\u7528\u6237\u63d0\u793a\u3002\u6b64\u5916\uff0c\u8bbe\u5907\u4f1a\u516b\u5366\u5b83\u4eec\u81ea\u5df1\u7684\u7528\u6237\u7279\u5b9a\u63d0\u793a\u6216\u589e\u5f3a\u63d0\u793a\uff0c\u4ee5\u751f\u6210\u5bf9\u67d0\u4e9b\u67e5\u8be2\u7684\u66f4\u7cbe\u7ec6\u7684\u7b54\u6848\u3002\u5f53\u76f8\u5e94\u7684 LLM \u7e41\u5fd9\u65f6\uff0c\u7528\u6237\u63d0\u793a\u4f1a\u6682\u65f6\u5b58\u50a8\u5728\u8bbe\u5907\u961f\u5217\u4e2d\u3002\u9274\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u9650\u5236\uff0c\u786e\u4fdd\u7cfb\u7edf\u4e2d\u5e73\u5747\u961f\u5217\u5927\u5c0f\u4fdd\u6301\u6709\u754c\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u5408\u7406\u5047\u8bbe\u4e0b\u4ece\u7406\u8bba\u4e0a\u8ba1\u7b97\u8bbe\u5907\u961f\u5217\u7684\u6392\u961f\u7a33\u5b9a\u6027\u6761\u4ef6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4e5f\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u6f14\u793a\uff0c\u5229\u7528\u5f00\u6e90 LLM \u6765\u5b9e\u73b0\u5206\u5e03\u5f0f MoA\uff0c\u67d0\u4e9b MoA \u914d\u7f6e\u4f1a\u4ea7\u751f\u6bd4\u5176\u4ed6\u914d\u7f6e\u66f4\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u5982\u5728 AlpacaEval 2.0 \u57fa\u51c6\u4e0a\u8bc4\u4f30\u7684\u90a3\u6837\u3002\u8be5\u5b9e\u73b0\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u5f97\uff1a\nhttps://github.com/purbeshmitra/distributed_moa\u3002</paragraph>", "author": "Purbesh Mitra et.al.", "authors": "Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus", "id": "2412.21200v1", "paper_url": "http://arxiv.org/abs/2412.21200v1", "repo": "https://github.com/purbeshmitra/distributed_moa"}}