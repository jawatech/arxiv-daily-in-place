{"2408.09834": {"publish_time": "2024-08-19", "title": "Minor DPO reject penalty to increase training robustness", "paper_summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.", "paper_summary_zh": "\u5f9e\u4eba\u985e\u504f\u597d\u4e2d\u5b78\u7fd2\u662f\u4e00\u7a2e\u7528\u65bc\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b (LLM) \u5fae\u8abf\u6b65\u9a5f\u7684\u7bc4\u4f8b\uff0c\u4ee5\u66f4\u597d\u5730\u5c07\u9810\u8a13\u7df4\u7684 LLM \u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u4ee5\u9032\u884c\u4e0b\u6e38\u4efb\u52d9\u3002\u904e\u53bb\u5b83\u4f7f\u7528\u4eba\u985e\u56de\u994b\uff08RLHF\uff09\u6f14\u7b97\u6cd5\u7684\u5f37\u5316\u5b78\u7fd2\u4f86\u6700\u4f73\u5316 LLM \u653f\u7b56\uff0c\u4ee5\u8207\u9019\u4e9b\u504f\u597d\u5c0d\u9f4a\uff0c\u4e26\u4e14\u4e0d\u8981\u8207\u539f\u59cb\u6a21\u578b\u76f8\u5dee\u592a\u9060\u3002\u6700\u8fd1\uff0c\u5df2\u7d93\u63d0\u51fa\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u4f86\u89e3\u6c7a\u5c0d\u9f4a\u554f\u984c\uff0c\u4e26\u63a1\u7528\u7c21\u5316\u7684\u7121 RL \u65b9\u6cd5\u3002\u4f7f\u7528\u6240\u9078\u548c\u62d2\u7d55\u8cc7\u6599\u7684\u504f\u597d\u5c0d\uff0cDPO \u5c07\u76f8\u5c0d\u5c0d\u6578\u6a5f\u7387\u5efa\u6a21\u70ba\u96b1\u5f0f\u734e\u52f5\u51fd\u6578\uff0c\u4e26\u4f7f\u7528\u7c21\u55ae\u7684\u4e8c\u5143\u4ea4\u53c9\u71b5\u76ee\u6a19\u76f4\u63a5\u6700\u4f73\u5316 LLM \u653f\u7b56\u3002DPO \u975e\u5e38\u76f4\u63a5\u4e14\u6613\u65bc\u7406\u89e3\u3002\u5728\u5927\u591a\u6578\u60c5\u6cc1\u4e0b\uff0c\u5b83\u7684\u57f7\u884c\u6548\u7387\u5f88\u9ad8\u4e14\u8868\u73fe\u826f\u597d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5206\u6790 DPO \u4e2d $\\beta$ \u7684\u5de5\u4f5c\u6a5f\u5236\uff0c\u63ed\u793a\u5176\u5728 RL \u6f14\u7b97\u6cd5\u548c DPO \u4e4b\u9593\u7684\u8a9e\u6cd5\u5dee\u7570\uff0c\u4e26\u4e86\u89e3 DPO \u7c21\u5316\u6240\u5e36\u4f86\u7684\u6f5b\u5728\u7f3a\u9ede\u3002\u6709\u4e86\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MinorDPO\uff0c\u5b83\u8207\u539f\u59cb RL \u6f14\u7b97\u6cd5\u66f4\u4e00\u81f4\uff0c\u4e26\u589e\u52a0\u4e86\u504f\u597d\u6700\u4f73\u5316\u904e\u7a0b\u7684\u7a69\u5b9a\u6027\u3002", "author": "Shiming Xie et.al.", "authors": "Shiming Xie, Hong Chen, Fred Yu, Zeye Sun, Xiuyu Wu, Yingfan Hu", "id": "2408.09834v1", "paper_url": "http://arxiv.org/abs/2408.09834v1", "repo": "null"}}