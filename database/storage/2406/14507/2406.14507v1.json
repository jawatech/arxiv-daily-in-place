{"2406.14507": {"publish_time": "2024-06-20", "title": "On Newton's Method to Unlearn Neural Networks", "paper_summary": "Machine unlearning facilitates personal data ownership, including the ``right\nto be forgotten''. The proliferation of applications of \\emph{neural networks}\n(NNs) trained on users' personal data calls for the need to develop algorithms\nto unlearn an NN. Since retraining is costly, efficiency is often achieved\nthrough approximate unlearning which aims to unlearn a trained NN to be close\nto the retrained one (in distribution). Though the Newton's method has been\nused by previous works to approximately unlearn linear models, adapting it for\nunlearning an NN often encounters degenerate Hessians that make computing the\nNewton's update impossible. In this paper, we will first show that when coupled\nwith naive yet often effective solutions to mitigate the degeneracy issue for\nunlearning, the Newton's method surprisingly suffers from catastrophic\nforgetting. To overcome this difficulty, we revise the Newton's method to\ninclude a theoretically justified regularizer and propose a cubic-regularized\nNewton's method for unlearning an NN. The cubic regularizer comes with the\nbenefits of not requiring manual finetuning and affording a natural\ninterpretation. Empirical evaluation on several models and real-world datasets\nshows that our method is more resilient to catastrophic forgetting and performs\nbetter than the baselines, especially in sequential unlearning.", "paper_summary_zh": "\u6a5f\u5668\u907a\u5fd8\u6709\u52a9\u65bc\u500b\u4eba\u8cc7\u6599\u6240\u6709\u6b0a\uff0c\u5305\u62ec\u300c\u88ab\u907a\u5fd8\u6b0a\u300d\u3002\u8a13\u7df4\u65bc\u4f7f\u7528\u8005\u500b\u4eba\u8cc7\u6599\u7684\u300c\u795e\u7d93\u7db2\u8def\u300d\uff08NN\uff09\u61c9\u7528\u6fc0\u589e\uff0c\u56e0\u6b64\u9700\u8981\u958b\u767c\u6f14\u7b97\u6cd5\u4f86\u907a\u5fd8 NN\u3002\u7531\u65bc\u91cd\u65b0\u8a13\u7df4\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u6548\u7387\u901a\u5e38\u900f\u904e\u8fd1\u4f3c\u907a\u5fd8\u4f86\u9054\u6210\uff0c\u5176\u76ee\u6a19\u662f\u907a\u5fd8\u5df2\u8a13\u7df4\u7684 NN \u4ee5\u63a5\u8fd1\u91cd\u65b0\u8a13\u7df4\u7684 NN\uff08\u5728\u5206\u4f48\u4e0a\uff09\u3002\u5118\u7ba1\u5148\u524d\u7684\u7814\u7a76\u5df2\u4f7f\u7528\u725b\u9813\u6cd5\u4f86\u8fd1\u4f3c\u907a\u5fd8\u7dda\u6027\u6a21\u578b\uff0c\u4f46\u5c07\u5176\u8abf\u6574\u70ba\u907a\u5fd8 NN \u6642\uff0c\u901a\u5e38\u6703\u9047\u5230\u7c21\u4f75\u6d77\u68ee\u77e9\u9663\uff0c\u5c0e\u81f4\u7121\u6cd5\u8a08\u7b97\u725b\u9813\u66f4\u65b0\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u9996\u5148\u5c55\u793a\uff0c\u7576\u8207\u7528\u65bc\u6e1b\u8f15\u907a\u5fd8\u7c21\u4f75\u554f\u984c\u7684\u5929\u771f\u4f46\u901a\u5e38\u6709\u6548\u7684\u65b9\u6cd5\u7d50\u5408\u6642\uff0c\u725b\u9813\u6cd5\u6703\u610f\u5916\u5730\u906d\u53d7\u707d\u96e3\u6027\u907a\u5fd8\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u56f0\u96e3\uff0c\u6211\u5011\u4fee\u6539\u725b\u9813\u6cd5\uff0c\u52a0\u5165\u7406\u8ad6\u4e0a\u5408\u7406\u7684\u6b63\u5247\u5316\u9805\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u4e09\u6b21\u6b63\u5247\u5316\u725b\u9813\u6cd5\u4f86\u907a\u5fd8 NN\u3002\u4e09\u6b21\u6b63\u5247\u5316\u9805\u7684\u597d\u8655\u662f\u4e0d\u9700\u8981\u4eba\u5de5\u5fae\u8abf\uff0c\u4e26\u63d0\u4f9b\u81ea\u7136\u7684\u8a6e\u91cb\u3002\u5c0d\u591a\u500b\u6a21\u578b\u548c\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u7684\u7d93\u9a57\u8a55\u4f30\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c0d\u707d\u96e3\u6027\u907a\u5fd8\u66f4\u6709\u97cc\u6027\uff0c\u4e26\u4e14\u8868\u73fe\u512a\u65bc\u57fa\u6e96\uff0c\u7279\u5225\u662f\u5728\u5e8f\u5217\u907a\u5fd8\u4e2d\u3002", "author": "Nhung Bui et.al.", "authors": "Nhung Bui, Xinyang Lu, See-Kiong Ng, Bryan Kian Hsian Low", "id": "2406.14507v1", "paper_url": "http://arxiv.org/abs/2406.14507v1", "repo": "null"}}