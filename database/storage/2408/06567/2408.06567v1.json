{"2408.06567": {"publish_time": "2024-08-13", "title": "AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies", "paper_summary": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u89c4\u6a21\u9010\u6e10\u589e\u5927\uff0c\u5176\u9884\u8bad\u7ec3\u6240\u9700\u8d44\u6e90\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3 LLM \u5c06\u82b1\u8d39\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u4ece\u5c0f\u6a21\u578b\u6269\u5c55\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5173\u6ce8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AquilaMoE\uff0c\u8fd9\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u53cc\u8bed 8*16B \u4e13\u5bb6\u6df7\u5408 (MoE) \u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u6709 8 \u4e2a\u4e13\u5bb6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u6709 160 \u4ebf\u4e2a\u53c2\u6570\uff0c\u5e76\u4e14\u662f\u4f7f\u7528\u79f0\u4e3a EfficientScale \u7684\u521b\u65b0\u8bad\u7ec3\u65b9\u6cd5\u5f00\u53d1\u7684\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u4f18\u5316\u6027\u80fd\uff0c\u540c\u65f6\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002\u7b2c\u4e00\u9636\u6bb5\u79f0\u4e3a Scale-Up\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3\u8f83\u5c0f\u6a21\u578b\u4e2d\u7684\u6743\u91cd\u521d\u59cb\u5316\u8f83\u5927\u7684\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u8d28\u6027\u7684\u77e5\u8bc6\u8f6c\u79fb\u548c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u6240\u9700\u6570\u636e\u660e\u663e\u66f4\u5c11\u3002\u7b2c\u4e8c\u9636\u6bb5 Scale-Out \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5bc6\u96c6\u6a21\u578b\u6765\u521d\u59cb\u5316 MoE \u4e13\u5bb6\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u77e5\u8bc6\u8f6c\u79fb\u548c\u6027\u80fd\u3002\u5bf9 1.8B \u548c 7B \u6a21\u578b\u8fdb\u884c\u7684\u5e7f\u6cdb\u9a8c\u8bc1\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5404\u79cd\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u548c\u51cf\u5c11\u635f\u5931\u7684\u6a21\u578b\u3002\u5229\u7528\u6700\u4f18\u65b9\u6848\uff0c\u6211\u4eec\u6210\u529f\u8bad\u7ec3\u4e86\u4e00\u4e2a 16B \u6a21\u578b\uff0c\u968f\u540e\u8bad\u7ec3\u4e86 8*16B AquilaMoE \u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "author": "Bo-Wen Zhang et.al.", "authors": "Bo-Wen Zhang, Liangdong Wang, Ye Yuan, Jijie Li, Shuhao Gu, Mengdi Zhao, Xinya Wu, Guang Liu, Chengwei Wu, Hanyu Zhao, Li Du, Yiming Ju, Quanyue Ma, Yulong Ao, Yingli Zhao, Songhe Zhu, Zhou Cao, Dong Liang, Yonghua Lin, Ming Zhang, Shunfei Wang, Yanxin Zhou, Min Ye, Xuekai Chen, Xinyang Yu, Xiangjun Huang, Jian Yang", "id": "2408.06567v1", "paper_url": "http://arxiv.org/abs/2408.06567v1", "repo": "null"}}