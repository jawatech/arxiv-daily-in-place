{"2410.18624": {"publish_time": "2024-10-24", "title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call Summarization", "paper_summary": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u63a2\u8a0e\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5feb\u901f\u958b\u767c\u96fb\u8a71\u901a\u8a71\u6458\u8981\u7cfb\u7d71\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u6700\u521d\u5617\u8a66\u63d0\u793a\u73fe\u6709\u7684 LLM \u7522\u751f\u96fb\u8a71\u5c0d\u8a71\u6458\u8981\uff0c\u7136\u5f8c\u5efa\u7acb\u4e00\u500b\u5229\u7528\u66f4\u5f37\u908a\u754c\u6a21\u578b\u7684\u5ba2\u88fd\u5316\u5408\u6210\u8a13\u7df4\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7279\u5225\u91cd\u8996\u7522\u751f\u8cc7\u6599\u7684\u591a\u6a23\u6027\uff0c\u4ee5\u53ca\u63a7\u5236\u7522\u751f\u6458\u8981\u9577\u5ea6\u4ee5\u6eff\u8db3\u5404\u7a2e\u7279\u5b9a\u4f7f\u7528\u6848\u4f8b\u9700\u6c42\u7684\u80fd\u529b\u3002\u6211\u5011\u4f7f\u7528\u5169\u7a2e\u6700\u65b0\u7684 LLM \u4f5c\u70ba\u8a55\u5224\u6a19\u6e96\u7684\u8a55\u4f30\u6280\u8853\u4f86\u8a55\u4f30\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee5\u78ba\u4fdd\u6458\u8981\u7684\u54c1\u8cea\u548c\u76f8\u95dc\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5fae\u8abf\u904e\u7684 Llama-2-7B \u6458\u8981\u6a21\u578b\u5728\u4e8b\u5be6\u6e96\u78ba\u6027\u3001\u5b8c\u6574\u6027\u548c\u7c21\u6f54\u6027\u65b9\u9762\u8207 GPT-4 \u8868\u73fe\u76f8\u7576\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8b49\u660e\u4e86\u5feb\u901f\u5f15\u5c0e\u5be6\u52d9\u4e14\u6709\u6548\u7387\u7684\u901a\u8a71\u6458\u8981\u7cfb\u7d71\u7684\u6f5b\u529b\u3002", "author": "David Thulke et.al.", "authors": "David Thulke, Yingbo Gao, Rricha Jalota, Christian Dugast, Hermann Ney", "id": "2410.18624v1", "paper_url": "http://arxiv.org/abs/2410.18624v1", "repo": "null"}}