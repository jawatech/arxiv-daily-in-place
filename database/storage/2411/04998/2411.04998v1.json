{"2411.04998": {"publish_time": "2024-11-07", "title": "HourVideo: 1-Hour Video-Language Understanding", "paper_summary": "We present HourVideo, a benchmark dataset for hour-long video-language\nunderstanding. Our dataset consists of a novel task suite comprising\nsummarization, perception (recall, tracking), visual reasoning (spatial,\ntemporal, predictive, causal, counterfactual), and navigation (room-to-room,\nobject retrieval) tasks. HourVideo includes 500 manually curated egocentric\nvideos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and\nfeatures 12,976 high-quality, five-way multiple-choice questions. Benchmarking\nresults reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve\nmarginal improvements over random chance. In stark contrast, human experts\nsignificantly outperform the state-of-the-art long-context multimodal model,\nGemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal\ncapabilities. Our benchmark, evaluation toolkit, prompts, and documentation are\navailable at https://hourvideo.stanford.edu", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa HourVideo\uff0c\u9019\u662f\u9577\u9054\u4e00\u5c0f\u6642\u7684\u5f71\u7247\u8a9e\u8a00\u7406\u89e3\u57fa\u6e96\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5305\u542b\u4e00\u7cfb\u5217\u65b0\u7a4e\u7684\u4efb\u52d9\u5957\u4ef6\uff0c\u5305\u542b\u6458\u8981\u3001\u611f\u77e5\uff08\u56de\u61b6\u3001\u8ffd\u8e64\uff09\u3001\u8996\u89ba\u63a8\u7406\uff08\u7a7a\u9593\u3001\u6642\u9593\u3001\u9810\u6e2c\u3001\u56e0\u679c\u3001\u53cd\u4e8b\u5be6\uff09\u548c\u5c0e\u822a\uff08\u623f\u9593\u5230\u623f\u9593\u3001\u7269\u4ef6\u6aa2\u7d22\uff09\u4efb\u52d9\u3002HourVideo \u5305\u542b\u4f86\u81ea Ego4D \u8cc7\u6599\u96c6\u7684 500 \u500b\u624b\u52d5\u7b56\u5283\u7684\u7b2c\u4e00\u4eba\u7a31\u8996\u89d2\u5f71\u7247\uff0c\u8de8\u8d8a 20 \u5230 120 \u5206\u9418\u7684\u6642\u9577\uff0c\u4e26\u63d0\u4f9b 12,976 \u500b\u9ad8\u54c1\u8cea\u3001\u4e94\u9078\u4e00\u7684\u9078\u64c7\u984c\u3002\u57fa\u6e96\u6e2c\u8a66\u7d50\u679c\u986f\u793a\uff0c\u5305\u62ec GPT-4 \u548c LLaVA-NeXT \u5728\u5167\u7684\u591a\u6a21\u614b\u6a21\u578b\uff0c\u6bd4\u96a8\u6a5f\u6a5f\u6703\u7372\u5f97\u908a\u969b\u6539\u5584\u3002\u8207\u4e4b\u5f62\u6210\u9bae\u660e\u5c0d\u6bd4\u7684\u662f\uff0c\u4eba\u985e\u5c08\u5bb6\u986f\u8457\u512a\u65bc\u6700\u5148\u9032\u7684\u9577\u8108\u7d61\u591a\u6a21\u614b\u6a21\u578b Gemini Pro 1.5\uff0885.0% \u5c0d\u6bd4 37.3%\uff09\uff0c\u7a81\u986f\u51fa\u591a\u6a21\u614b\u80fd\u529b\u7684\u5de8\u5927\u5dee\u8ddd\u3002\u6211\u5011\u7684\u57fa\u6e96\u3001\u8a55\u4f30\u5de5\u5177\u5305\u3001\u63d0\u793a\u548c\u6587\u4ef6\u53ef\u5728 https://hourvideo.stanford.edu \u53d6\u5f97", "author": "Keshigeyan Chandrasegaran et.al.", "authors": "Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Crist\u00f3bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei", "id": "2411.04998v1", "paper_url": "http://arxiv.org/abs/2411.04998v1", "repo": "null"}}