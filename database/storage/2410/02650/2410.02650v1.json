{"2410.02650": {"publish_time": "2024-10-03", "title": "Undesirable Memorization in Large Language Models: A Survey", "paper_summary": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.", "paper_summary_zh": "\u5118\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u8d8a\u4f86\u8d8a\u5c55\u793a\u51fa\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u975e\u51e1\u80fd\u529b\uff0c\u4f46\u9762\u5c0d\u5176\u96b1\u85cf\u7684\u9677\u9631\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u4e9b\u6311\u6230\u4e2d\uff0c\u8a18\u61b6\u5316\u554f\u984c\u812b\u7a4e\u800c\u51fa\uff0c\u69cb\u6210\u986f\u8457\u7684\u9053\u5fb7\u548c\u6cd5\u5f8b\u98a8\u96aa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u91dd\u5c0d LLM \u4e2d\u8a18\u61b6\u5316\u7684\u4e3b\u984c\u63d0\u51fa\u77e5\u8b58\u7cfb\u7d71\u5316 (SoK)\u3002\u8a18\u61b6\u5316\u662f\u4e00\u7a2e\u6a21\u578b\u50be\u5411\u65bc\u5132\u5b58\u548c\u8907\u88fd\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u7247\u8a9e\u6216\u6bb5\u843d\u7684\u6548\u679c\uff0c\u4e26\u4e14\u5df2\u88ab\u8b49\u660e\u662f\u91dd\u5c0d LLM \u7684\u5404\u7a2e\u96b1\u79c1\u548c\u5b89\u5168\u653b\u64ca\u7684\u57fa\u672c\u554f\u984c\u3002\u6211\u5011\u9996\u5148\u63d0\u4f9b\u95dc\u65bc\u8a18\u61b6\u5316\u7684\u6587\u737b\u6982\u8ff0\uff0c\u4e26\u5f9e\u4e94\u500b\u95dc\u9375\u9762\u5411\u63a2\u8a0e\u5b83\uff1a\u610f\u5716\u6027\u3001\u7a0b\u5ea6\u3001\u53ef\u6aa2\u7d22\u6027\u3001\u62bd\u8c61\u6027\u548c\u900f\u660e\u6027\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u8a0e\u8ad6\u7528\u65bc\u8861\u91cf\u8a18\u61b6\u5316\u7684\u6307\u6a19\u548c\u65b9\u6cd5\uff0c\u63a5\u8457\u5206\u6790\u5c0e\u81f4\u8a18\u61b6\u5316\u73fe\u8c61\u7684\u56e0\u7d20\u3002\u7136\u5f8c\u6211\u5011\u63a2\u8a0e\u8a18\u61b6\u5316\u5982\u4f55\u5728\u7279\u5b9a\u6a21\u578b\u67b6\u69cb\u4e2d\u8868\u73fe\u51fa\u4f86\uff0c\u4e26\u63a2\u8a0e\u6e1b\u8f15\u9019\u4e9b\u5f71\u97ff\u7684\u7b56\u7565\u3002\u6211\u5011\u900f\u904e\u627e\u51fa\u8fd1\u671f\u6f5b\u5728\u7684\u7814\u7a76\u4e3b\u984c\u4f86\u7d50\u675f\u6211\u5011\u7684\u6982\u8ff0\uff1a\u958b\u767c\u5728 LLM \u4e2d\u5e73\u8861\u6548\u80fd\u548c\u96b1\u79c1\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u7279\u5b9a\u8108\u7d61\u4e2d\u5206\u6790\u8a18\u61b6\u5316\uff0c\u5305\u62ec\u5c0d\u8a71\u4ee3\u7406\u3001\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u3001\u591a\u8a9e\u8a00\u8a9e\u8a00\u6a21\u578b\u548c\u64f4\u6563\u8a9e\u8a00\u6a21\u578b\u3002", "author": "Ali Satvaty et.al.", "authors": "Ali Satvaty, Suzan Verberne, Fatih Turkmen", "id": "2410.02650v1", "paper_url": "http://arxiv.org/abs/2410.02650v1", "repo": "null"}}