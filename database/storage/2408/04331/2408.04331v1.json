{"2408.04331": {"publish_time": "2024-08-08", "title": "Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs", "paper_summary": "Large language models (LLMs) and large multimodal models (LMMs) have\nsignificantly impacted the AI community, industry, and various economic\nsectors. In journalism, integrating AI poses unique challenges and\nopportunities, particularly in enhancing the quality and efficiency of news\nreporting. This study explores how LLMs and LMMs can assist journalistic\npractice by generating contextualised captions for images accompanying news\narticles. We conducted experiments using the GoodNews dataset to evaluate the\nability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of\ncontext: entire news articles, or extracted named entities. In addition, we\ncompared their performance to a two-stage pipeline composed of a captioning\nmodel (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs\n(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the\nchoice of contextualisation model is a significant factor for the two-stage\npipelines, this is not the case in the LMMs, where smaller, open-source models\nperform well compared to proprietary, GPT-powered ones. Additionally, we found\nthat controlling the amount of provided context enhances performance. These\nresults highlight the limitations of a fully automated approach and underscore\nthe necessity for an interactive, human-in-the-loop strategy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u5df2\u986f\u8457\u5f71\u97ff\u4e86 AI \u793e\u7fa4\u3001\u7522\u696d\u548c\u5404\u7a2e\u7d93\u6fdf\u90e8\u9580\u3002\u5728\u65b0\u805e\u696d\u4e2d\uff0c\u6574\u5408 AI \u69cb\u6210\u4e86\u7368\u7279\u7684\u6311\u6230\u548c\u6a5f\u9047\uff0c\u7279\u5225\u662f\u5728\u63d0\u5347\u65b0\u805e\u5831\u5c0e\u7684\u54c1\u8cea\u548c\u6548\u7387\u65b9\u9762\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86 LLM \u548c LMM \u5982\u4f55\u900f\u904e\u70ba\u65b0\u805e\u6587\u7ae0\u9644\u5716\u7522\u751f\u60c5\u5883\u5316\u6a19\u984c\uff0c\u4f86\u5354\u52a9\u65b0\u805e\u5be6\u52d9\u3002\u6211\u5011\u4f7f\u7528 GoodNews \u8cc7\u6599\u96c6\u9032\u884c\u5be6\u9a57\uff0c\u4ee5\u8a55\u4f30 LMM\uff08BLIP-2\u3001GPT-4v \u6216 LLaVA\uff09\u7d0d\u5165\u5169\u7a2e\u60c5\u5883\u985e\u578b\uff08\u6574\u7bc7\u65b0\u805e\u6587\u7ae0\u6216\u62bd\u53d6\u7684\u547d\u540d\u5be6\u9ad4\uff09\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u5176\u6548\u80fd\u8207\u7531\u6a19\u984c\u6a21\u578b\uff08BLIP-2\u3001OFA \u6216 ViT-GPT2\uff09\u7d44\u6210\u7684\u5169\u968e\u6bb5\u7ba1\u7dda\u9032\u884c\u6bd4\u8f03\uff0c\u4e26\u642d\u914d LLM\uff08GPT-4 \u6216 LLaMA\uff09\u9032\u884c\u4e8b\u5f8c\u60c5\u5883\u5316\u3002\u6211\u5011\u8a55\u4f30\u4e86\u5404\u7a2e\u6a21\u578b\uff0c\u767c\u73fe\u96d6\u7136\u60c5\u5883\u5316\u6a21\u578b\u7684\u9078\u64c7\u662f\u5169\u968e\u6bb5\u7ba1\u7dda\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4f46\u5728 LMM \u4e2d\u4e26\u975e\u5982\u6b64\uff0c\u5176\u4e2d\u8f03\u5c0f\u578b\u3001\u958b\u653e\u539f\u59cb\u78bc\u7684\u6a21\u578b\u8868\u73fe\u826f\u597d\uff0c\u512a\u65bc\u5c08\u6709\u7684 GPT \u9a45\u52d5\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u63a7\u5236\u63d0\u4f9b\u7684\u8108\u7d61\u6578\u91cf\u6703\u63d0\u5347\u6548\u80fd\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u5168\u81ea\u52d5\u5316\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e26\u5f37\u8abf\u4e86\u4e92\u52d5\u5f0f\u3001\u4eba\u985e\u53c3\u8207\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002", "author": "Aliki Anagnostopoulou et.al.", "authors": "Aliki Anagnostopoulou, Thiago Gouvea, Daniel Sonntag", "id": "2408.04331v1", "paper_url": "http://arxiv.org/abs/2408.04331v1", "repo": "null"}}