{"2410.01610": {"publish_time": "2024-10-02", "title": "Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging", "paper_summary": "Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.", "paper_summary_zh": "\u6df7\u5408\u4e13\u5bb6 (MoE) \u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5927\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c06 LLM \u4ece\u5bc6\u96c6\u578b\u8f6c\u6362\u4e3a MoE \u7684\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7740\u5927\u91cf\u7684\u6570\u636e\u9700\u6c42\uff0c\u5e76\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u540e\u671f\u8bad\u7ec3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5347\u7ea7\u6307\u4ee4\u5fae\u8c03 (UpIT)\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u5bc6\u96c6\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u4e3a MoE \u6307\u4ee4\u6a21\u578b\u7684\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6307\u51fa\uff0c\u5728\u5bc6\u96c6\u6a21\u578b\u7684\u6307\u4ee4\u5fae\u8c03\u671f\u95f4\uff0c\u4e2d\u95f4\u68c0\u67e5\u70b9\u81ea\u7136\u9002\u7528\u4e8e\u4e13\u95e8\u7684\u4e13\u5bb6\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u4e2a\u4e13\u5bb6\u6269\u5c55\u9636\u6bb5\uff0c\u4ee5\u7075\u6d3b\u5730\u5b9e\u73b0\u5177\u6709\u7075\u6d3b\u4e13\u5bb6\u6570\u91cf\u7684\u6a21\u578b\uff0c\u5176\u4e2d\u5f15\u5165\u9057\u4f20\u7b97\u6cd5\u548c\u53c2\u6570\u5408\u5e76\u4ee5\u786e\u4fdd\u65b0\u6269\u5c55\u4e13\u5bb6\u7684\u5145\u5206\u591a\u6837\u6027\u3002\u4e3a\u4e86\u786e\u4fdd MoE \u6a21\u578b\u4e2d\u7684\u6bcf\u4e2a\u4e13\u95e8\u4e13\u5bb6\u90fd\u80fd\u6309\u9884\u671f\u5de5\u4f5c\uff0c\u6211\u4eec\u9009\u62e9\u5c11\u91cf\u79cd\u5b50\u6570\u636e\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u90fd\u64c5\u957f\u9884\u4f18\u5316\u8def\u7531\u5668\u3002\u4f7f\u7528\u5404\u79cd\u6570\u636e\u89c4\u6a21\u548c\u5347\u7ea7\u8bbe\u7f6e\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86 UpIT \u7684\u51fa\u8272\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u4ee5\u53ca\u4e13\u5bb6\u6216\u6570\u636e\u6269\u5c55\u7684\u7a33\u5b9a\u6539\u8fdb\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u63ed\u793a\u4e86\u786e\u4fdd\u5347\u7ea7\u4e2d\u4e13\u5bb6\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002", "author": "Tingfeng Hui et.al.", "authors": "Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Yu Sun, Hua Wu, Sen Su", "id": "2410.01610v1", "paper_url": "http://arxiv.org/abs/2410.01610v1", "repo": "null"}}