{"2502.13925": {"publish_time": "2025-02-19", "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?", "paper_summary": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u5df2\u5728\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u6210\u529f\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u55ae\u4e00\u5f71\u50cf\u7406\u89e3\u4e0a\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u63a2\u7d22\u5f71\u50cf\u5e8f\u5217\u7684\u5206\u6790\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 StripCipher\uff0c\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 LMM \u7406\u89e3\u548c\u63a8\u7406\u9806\u5e8f\u5f71\u50cf\u7684\u80fd\u529b\u3002StripCipher \u5305\u542b\u4e00\u500b\u4eba\u5de5\u6a19\u8a3b\u7684\u8cc7\u6599\u96c6\u548c\u4e09\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u5b50\u4efb\u52d9\uff1a\u8996\u89ba\u6558\u4e8b\u7406\u89e3\u3001\u60c5\u5883\u6846\u67b6\u9810\u6e2c\u548c\u6642\u9593\u6558\u4e8b\u91cd\u65b0\u6392\u5e8f\u3002\u6211\u5011\u5c0d 16 \u500b\u6700\u5148\u9032\u7684 LMM\uff08\u5305\u62ec GPT-4o \u548c Qwen2.5VL\uff09\u7684\u8a55\u4f30\u986f\u793a\uff0c\u8207\u4eba\u985e\u7684\u80fd\u529b\u76f8\u6bd4\uff0c\u5b58\u5728\u986f\u8457\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u7279\u5225\u662f\u5728\u9700\u8981\u91cd\u65b0\u6392\u5e8f\u5df2\u6d17\u724c\u7684\u9806\u5e8f\u5f71\u50cf\u7684\u4efb\u52d9\u4e2d\u3002\u4f8b\u5982\uff0cGPT-4o \u5728\u91cd\u65b0\u6392\u5e8f\u5b50\u4efb\u52d9\u4e2d\u50c5\u9054\u5230 23.93% \u7684\u6e96\u78ba\u5ea6\uff0c\u6bd4\u4eba\u985e\u6548\u80fd\u4f4e 56.07%\u3002\u9032\u4e00\u6b65\u7684\u91cf\u5316\u5206\u6790\u8a0e\u8ad6\u4e86\u5e7e\u500b\u56e0\u7d20\uff0c\u4f8b\u5982\u5f71\u50cf\u7684\u8f38\u5165\u683c\u5f0f\uff0c\u5f71\u97ff LLM \u5728\u9806\u5e8f\u7406\u89e3\u4e2d\u7684\u6548\u80fd\uff0c\u5f37\u8abf\u4e86 LMM \u767c\u5c55\u4e2d\u4ecd\u7136\u5b58\u5728\u7684\u57fa\u672c\u6311\u6230\u3002", "author": "Xiaochen Wang et.al.", "authors": "Xiaochen Wang, Heming Xia, Jialin Song, Longyu Guan, Yixin Yang, Qingxiu Dong, Weiyao Luo, Yifan Pu, Yiru Wang, Xiangdi Meng, Wenjie Li, Zhifang Sui", "id": "2502.13925v1", "paper_url": "http://arxiv.org/abs/2502.13925v1", "repo": "null"}}