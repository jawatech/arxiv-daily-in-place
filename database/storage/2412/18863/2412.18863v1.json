{"2412.18863": {"publish_time": "2024-12-25", "title": "Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models", "paper_summary": "Large language models (LLMs) have become integral tools in diverse domains,\nyet their moral reasoning capabilities across cultural and linguistic contexts\nremain underexplored. This study investigates whether multilingual LLMs, such\nas GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally\nspecific moral values or impose dominant moral norms, particularly those rooted\nin English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight\nlanguages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and\nRussian, the study analyzes the models' adherence to six core moral\nfoundations: care, equality, proportionality, loyalty, authority, and purity.\nThe results reveal significant cultural and linguistic variability, challenging\nthe assumption of universal moral consistency in LLMs. Although some models\ndemonstrate adaptability to diverse contexts, others exhibit biases influenced\nby the composition of the training data. These findings underscore the need for\nculturally inclusive model development to improve fairness and trust in\nmultilingual AI systems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u5404\u500b\u9818\u57df\u4e0d\u53ef\u6216\u7f3a\u7684\u5de5\u5177\uff0c\n\u4f46\u5b83\u5011\u5728\u4e0d\u540c\u6587\u5316\u548c\u8a9e\u8a00\u80cc\u666f\u4e0b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\n\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u591a\u8a9e\u8a00 LLM\uff0c\u4f8b\u5982\nGPT-3.5-Turbo\u3001GPT-4o-mini\u3001Llama 3.1 \u548c MistralNeMo\uff0c\u662f\u5426\u53cd\u6620\u4e86\u7279\u5b9a\u6587\u5316\u7684\n\u9053\u5fb7\u50f9\u503c\u89c0\u6216\u5f37\u52a0\u4e86\u4e3b\u6d41\u9053\u5fb7\u898f\u7bc4\uff0c\u5c24\u5176\u662f\u690d\u6839\u65bc\u82f1\u8a9e\u7684\u898f\u7bc4\u3002\u4f7f\u7528\u66f4\u65b0\u7684\u9053\u5fb7\u57fa\u790e\u554f\u5377 (MFQ-2) \u516b\u7a2e\n\u8a9e\u8a00\uff0c\u963f\u62c9\u4f2f\u8a9e\u3001\u6ce2\u65af\u8a9e\u3001\u82f1\u8a9e\u3001\u897f\u73ed\u7259\u8a9e\u3001\u65e5\u8a9e\u3001\u4e2d\u6587\u3001\u6cd5\u8a9e\u548c\n\u4fc4\u8a9e\uff0c\u8a72\u7814\u7a76\u5206\u6790\u4e86\u6a21\u578b\u5c0d\u516d\u500b\u6838\u5fc3\u9053\u5fb7\u7684\u9075\u5b88\u60c5\u6cc1\n\u57fa\u790e\uff1a\u95dc\u61f7\u3001\u5e73\u7b49\u3001\u76f8\u7a31\u6027\u3001\u5fe0\u8aa0\u3001\u6b0a\u5a01\u548c\u7d14\u6f54\u3002\n\u7d50\u679c\u63ed\u793a\u4e86\u986f\u8457\u7684\u6587\u5316\u548c\u8a9e\u8a00\u8b8a\u7570\u6027\uff0c\u6311\u6230\u4e86 LLM \u4e2d\u666e\u904d\u9053\u5fb7\u4e00\u81f4\u6027\u7684\u5047\u8a2d\u3002\u5118\u7ba1\u4e00\u4e9b\u6a21\u578b\n\u8b49\u660e\u4e86\u5c0d\u4e0d\u540c\u80cc\u666f\u7684\u9069\u61c9\u6027\uff0c\u800c\u53e6\u4e00\u4e9b\u6a21\u578b\u5247\u8868\u73fe\u51fa\u53d7\u8a13\u7df4\u6578\u64da\u7d44\u6210\u5f71\u97ff\u7684\u504f\u898b\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\n\u6587\u5316\u5305\u5bb9\u6a21\u578b\u958b\u767c\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u5c0d\n\u591a\u8a9e\u8a00\u4eba\u5de5\u667a\u80fd\u7cfb\u7d71\u7684\u4fe1\u4efb\u3002", "author": "Meltem Aksoy et.al.", "authors": "Meltem Aksoy", "id": "2412.18863v1", "paper_url": "http://arxiv.org/abs/2412.18863v1", "repo": "null"}}