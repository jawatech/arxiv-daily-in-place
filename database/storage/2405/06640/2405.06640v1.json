{"2405.06640": {"publish_time": "2024-05-10", "title": "Linearizing Large Language Models", "paper_summary": "Linear transformers have emerged as a subquadratic-time alternative to\nsoftmax attention and have garnered significant interest due to their\nfixed-size recurrent state that lowers inference cost. However, their original\nformulation suffers from poor scaling and underperforms compute-matched\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\naddress these shortcomings by proposing novel time-mixing and gating\narchitectures, but pre-training large language models requires significant data\nand compute investments. Thus, the search for subquadratic architectures is\nlimited by the availability of compute and quality pre-training datasets. As a\ncost-effective alternative to pre-training linear transformers, we propose\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\n(RNNs) with a modest compute budget. This allows us to leverage the strong\npre-training data and performance of existing transformer LLMs, while requiring\n5% of the training cost. We find that our linearization technique leads to\ncompetitive performance on standard benchmarks, but we identify persistent\nin-context learning and long-context modeling shortfalls for even the largest\nlinear models. Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm.", "paper_summary_zh": "\u7dda\u6027\u8f49\u63db\u5668\u5df2\u6210\u70ba softmax \u6ce8\u610f\u529b\u7684\u6b21\u4e8c\u6b21\u6642\u9593\u66ff\u4ee3\u65b9\u6848\uff0c\u4e26\u56e0\u5176\u964d\u4f4e\u4e86\u63a8\u8ad6\u6210\u672c\u7684\u56fa\u5b9a\u5927\u5c0f\u905e\u8ff4\u72c0\u614b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u539f\u59cb\u516c\u5f0f\u5b58\u5728\u64f4\u5c55\u6027\u5dee\u4e14\u8868\u73fe\u4e0d\u5982\u8a08\u7b97\u5339\u914d\u8f49\u63db\u5668\u7684\u554f\u984c\u3002\u6700\u8fd1\u7684\u7dda\u6027\u6a21\u578b\uff0c\u4f8b\u5982 RWKV \u548c Mamba\uff0c\u5df2\u5617\u8a66\u901a\u904e\u63d0\u51fa\u65b0\u7a4e\u7684\u6642\u9593\u6df7\u5408\u548c\u9598\u63a7\u67b6\u69cb\u4f86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u4f46\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u6578\u64da\u548c\u8a08\u7b97\u6295\u8cc7\u3002\u56e0\u6b64\uff0c\u5c0d\u6b21\u4e8c\u6b21\u67b6\u69cb\u7684\u641c\u7d22\u53d7\u5230\u8a08\u7b97\u548c\u8cea\u91cf\u9810\u8a13\u7df4\u6578\u64da\u96c6\u53ef\u7528\u6027\u7684\u9650\u5236\u3002\u4f5c\u70ba\u9810\u8a13\u7df4\u7dda\u6027\u8f49\u63db\u5668\u7684\u7d93\u6fdf\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u905e\u8ff4\u6ce8\u610f\u529b\u7684\u53ef\u64f4\u5c55\u4e0a\u8a13\u7df4 (SUPRA)\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5c07\u73fe\u6709\u7684\u9810\u5148\u8a13\u7df4\u597d\u7684\u5927\u578b\u8f49\u63db\u5668\u4e0a\u8a13\u7df4\u70ba\u5177\u6709\u9069\u5ea6\u8a08\u7b97\u9810\u7b97\u7684\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN) \u7684\u65b9\u6cd5\u3002\u9019\u4f7f\u6211\u5011\u80fd\u5920\u5229\u7528\u73fe\u6709\u8f49\u63db\u5668 LLM \u7684\u5f37\u5927\u9810\u8a13\u7df4\u6578\u64da\u548c\u6027\u80fd\uff0c\u540c\u6642\u53ea\u9700 5% \u7684\u8a13\u7df4\u6210\u672c\u3002\u6211\u5011\u767c\u73fe\u6211\u5011\u7684\u7dda\u6027\u5316\u6280\u8853\u53ef\u4ee5\u5728\u6a19\u6e96\u57fa\u6e96\u4e0a\u5e36\u4f86\u5177\u6709\u7af6\u722d\u529b\u7684\u6027\u80fd\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5373\u4f7f\u662f\u6700\u5927\u7684\u7dda\u6027\u6a21\u578b\u4e5f\u5b58\u5728\u6301\u7e8c\u7684\u8a9e\u5883\u5b78\u7fd2\u548c\u9577\u8a9e\u5883\u5efa\u6a21\u7684\u4e0d\u8db3\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/TRI-ML/linear_open_lm \u627e\u5230\u3002", "author": "Jean Mercat et.al.", "authors": "Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar", "id": "2405.06640v1", "paper_url": "http://arxiv.org/abs/2405.06640v1", "repo": "https://github.com/tri-ml/linear_open_lm"}}