{"2405.05955": {"publish_time": "2024-05-09", "title": "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning", "paper_summary": "The emergence of large language models (LLMs) has opened up unprecedented\npossibilities for automating complex tasks that are often comparable to human\nperformance. Despite their capabilities, LLMs still encounter difficulties in\ncompleting tasks that require high levels of accuracy and complexity due to\ntheir inherent limitations in handling multifaceted problems single-handedly.\nThis paper introduces \"Smurfs\", a cutting-edge multi-agent framework designed\nto revolutionize the application of LLMs. By transforming a conventional LLM\ninto a synergistic multi-agent ensemble, Smurfs enhances task decomposition and\nexecution without necessitating extra training. This is achieved through\ninnovative prompting strategies that allocate distinct roles within the model,\nthereby facilitating collaboration among specialized agents. The framework\ngives access to external tools to efficiently solve complex tasks. Our\nempirical investigation, featuring the mistral-7b-instruct model as a case\nstudy, showcases Smurfs' superior capability in intricate tool utilization\nscenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and\nI3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded\nperformance of a GPT-4 model at 73.5%. Furthermore, through comprehensive\nablation studies, we dissect the contribution of the core components of the\nmulti-agent framework to its overall efficacy. This not only verifies the\neffectiveness of the framework, but also sets a route for future exploration of\nmulti-agent LLM systems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u70ba\u81ea\u52d5\u5316\u8907\u96dc\u4efb\u52d9\u958b\u95e2\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u80fd\u6027\uff0c\u9019\u4e9b\u4efb\u52d9\u901a\u5e38\u8207\u4eba\u985e\u7684\u8868\u73fe\u76f8\u7576\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u80fd\u529b\uff0c\u4f46 LLM \u5728\u5b8c\u6210\u9700\u8981\u9ad8\u6e96\u78ba\u5ea6\u548c\u8907\u96dc\u6027\u7684\u4efb\u52d9\u6642\u4ecd\u6703\u9047\u5230\u56f0\u96e3\uff0c\u56e0\u70ba\u5b83\u5011\u5728\u55ae\u7368\u8655\u7406\u591a\u65b9\u9762\u7684\u554f\u984c\u6642\u6709\u5176\u56fa\u6709\u7684\u9650\u5236\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u300cSmurfs\u300d\uff0c\u9019\u662f\u4e00\u500b\u65e8\u5728\u9769\u65b0 LLM \u61c9\u7528\u7a0b\u5e8f\u7684\u5c16\u7aef\u591a\u4ee3\u7406\u67b6\u69cb\u3002\u900f\u904e\u5c07\u50b3\u7d71\u7684 LLM \u8f49\u8b8a\u70ba\u5354\u540c\u7684\u591a\u4ee3\u7406\u6574\u9ad4\uff0cSmurfs \u589e\u5f37\u4e86\u4efb\u52d9\u5206\u89e3\u548c\u57f7\u884c\uff0c\u800c\u7121\u9700\u984d\u5916\u7684\u8a13\u7df4\u3002\u9019\u662f\u900f\u904e\u5275\u65b0\u7684\u63d0\u793a\u7b56\u7565\u5be6\u73fe\u7684\uff0c\u9019\u4e9b\u7b56\u7565\u5728\u6a21\u578b\u4e2d\u5206\u914d\u4e0d\u540c\u7684\u89d2\u8272\uff0c\u5f9e\u800c\u4fc3\u9032\u5c08\u696d\u4ee3\u7406\u4e4b\u9593\u7684\u5354\u4f5c\u3002\u8a72\u6846\u67b6\u5141\u8a31\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u4f86\u6709\u6548\u5730\u89e3\u6c7a\u8907\u96dc\u4efb\u52d9\u3002\u6211\u5011\u7684\u5be6\u8b49\u8abf\u67e5\u4ee5 mistral-7b-instruct \u6a21\u578b\u70ba\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86 Smurfs \u5728\u8907\u96dc\u5de5\u5177\u4f7f\u7528\u5834\u666f\u4e2d\u7684\u5353\u8d8a\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSmurfs \u5728 ToolBench I2 \u548c I3 \u57fa\u6e96\u6e2c\u8a66\u4e2d\u4ee5 84.4% \u7684\u52dd\u7387\u64ca\u6557 ChatGPT-ReACT\uff0c\u8d85\u8d8a\u4e86 GPT-4 \u6a21\u578b\u5275\u4e0b\u7684 73.5% \u7684\u6700\u9ad8\u8a18\u9304\u3002\u6b64\u5916\uff0c\u900f\u904e\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0c\u6211\u5011\u5256\u6790\u4e86\u591a\u4ee3\u7406\u67b6\u69cb\u7684\u6838\u5fc3\u7d44\u6210\u90e8\u5206\u5c0d\u5176\u6574\u9ad4\u6548\u80fd\u7684\u8ca2\u737b\u3002\u9019\u4e0d\u50c5\u9a57\u8b49\u4e86\u8a72\u67b6\u69cb\u7684\u6709\u6548\u6027\uff0c\u4e5f\u70ba\u672a\u4f86\u63a2\u7d22\u591a\u4ee3\u7406 LLM \u7cfb\u7d71\u8a2d\u5b9a\u4e86\u4e00\u500b\u8def\u7dda\u3002", "author": "Junzhi Chen et.al.", "authors": "Junzhi Chen, Juhao Liang, Benyou Wang", "id": "2405.05955v1", "paper_url": "http://arxiv.org/abs/2405.05955v1", "repo": "null"}}