{"2406.02079": {"publish_time": "2024-06-04", "title": "Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks", "paper_summary": "Information Extraction (IE) plays a crucial role in Natural Language\nProcessing (NLP) by extracting structured information from unstructured text,\nthereby facilitating seamless integration with various real-world applications\nthat rely on structured data. Despite its significance, recent experiments\nfocusing on English IE tasks have shed light on the challenges faced by Large\nLanguage Models (LLMs) in achieving optimal performance, particularly in\nsub-tasks like Named Entity Recognition (NER). In this paper, we delve into a\ncomprehensive investigation of the performance of mainstream Chinese\nopen-source LLMs in tackling IE tasks, specifically under zero-shot conditions\nwhere the models are not fine-tuned for specific tasks. Additionally, we\npresent the outcomes of several few-shot experiments to further gauge the\ncapability of these models. Moreover, our study includes a comparative analysis\nbetween these open-source LLMs and ChatGPT, a widely recognized language model,\non IE performance. Through meticulous experimentation and analysis, we aim to\nprovide insights into the strengths, limitations, and potential enhancements of\nexisting Chinese open-source LLMs in the domain of Information Extraction\nwithin the context of NLP.", "paper_summary_zh": "\u8cc7\u8a0a\u8403\u53d6 (IE) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u626e\u6f14\u8457\u95dc\u9375\u89d2\u8272\uff0c\u5b83\u5f9e\u975e\u7d50\u69cb\u5316\u6587\u5b57\u4e2d\u8403\u53d6\u51fa\u7d50\u69cb\u5316\u7684\u8cc7\u8a0a\uff0c\u5f9e\u800c\u4fc3\u9032\u8207\u4f9d\u8cf4\u7d50\u69cb\u5316\u8cc7\u6599\u7684\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u61c9\u7528\u7a0b\u5f0f\u9032\u884c\u7121\u7e2b\u6574\u5408\u3002\u5118\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u6700\u8fd1\u5c08\u6ce8\u65bc\u82f1\u6587 IE \u4efb\u52d9\u7684\u5be6\u9a57\u63ed\u793a\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9054\u6210\u6700\u4f73\u6548\u80fd\u6642\u6240\u9762\u81e8\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u547d\u540d\u5be6\u9ad4\u8fa8\u8b58 (NER) \u7b49\u5b50\u4efb\u52d9\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e3b\u6d41\u4e2d\u6587\u958b\u6e90 LLM \u5728\u8655\u7406 IE \u4efb\u52d9\u6642\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u6a21\u578b\u672a\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u7684\u96f6\u6b21\u5b78\u7fd2\u689d\u4ef6\u4e0b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u4e9b\u5c0f\u6a23\u672c\u5be6\u9a57\u7684\u7d50\u679c\uff0c\u4ee5\u9032\u4e00\u6b65\u8a55\u4f30\u9019\u4e9b\u6a21\u578b\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u5305\u62ec\u9019\u4e9b\u958b\u6e90 LLM \u8207\u5ee3\u53d7\u8a8d\u53ef\u7684\u8a9e\u8a00\u6a21\u578b ChatGPT \u5728 IE \u6548\u80fd\u4e0a\u7684\u6bd4\u8f03\u5206\u6790\u3002\u900f\u904e\u7d30\u7dfb\u7684\u5be6\u9a57\u548c\u5206\u6790\uff0c\u6211\u5011\u65e8\u5728\u63d0\u4f9b\u898b\u89e3\uff0c\u4e86\u89e3\u73fe\u6709\u4e2d\u6587\u958b\u6e90 LLM \u5728 NLP \u8108\u7d61\u4e2d\u7684\u8cc7\u8a0a\u8403\u53d6\u9818\u57df\u4e2d\u7684\u512a\u52e2\u3001\u9650\u5236\u548c\u6f5b\u5728\u7684\u5f37\u5316\u3002", "author": "Yida Cai et.al.", "authors": "Yida Cai, Hao Sun, Hsiu-Yuan Huang, Yunfang Wu", "id": "2406.02079v1", "paper_url": "http://arxiv.org/abs/2406.02079v1", "repo": "null"}}