{"2411.16053": {"publish_time": "2024-11-25", "title": "UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation", "paper_summary": "Vision-and-Language Navigation (VLN), where an agent follows instructions to\nreach a target destination, has recently seen significant advancements. In\ncontrast to navigation in discrete environments with predefined trajectories,\nVLN in Continuous Environments (VLN-CE) presents greater challenges, as the\nagent is free to navigate any unobstructed location and is more vulnerable to\nvisual occlusions or blind spots. Recent approaches have attempted to address\nthis by imagining future environments, either through predicted future visual\nimages or semantic features, rather than relying solely on current\nobservations. However, these RGB-based and feature-based methods lack intuitive\nappearance-level information or high-level semantic complexity crucial for\neffective navigation. To overcome these limitations, we introduce a novel,\ngeneralizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables\nagents to better explore future environments by unitedly rendering\nhigh-fidelity 360 visual images and semantic features. UnitedVLN employs two\nkey schemes: search-then-query sampling and separate-then-united rendering,\nwhich facilitate efficient exploitation of neural primitives, helping to\nintegrate both appearance and semantic information for more robust navigation.\nExtensive experiments demonstrate that UnitedVLN outperforms state-of-the-art\nmethods on existing VLN-CE benchmarks.", "paper_summary_zh": "\u8996\u89ba\u548c\u8a9e\u8a00\u5c0e\u822a (VLN) \u8b93\u4ee3\u7406\u4eba\u9075\u5faa\u6307\u793a\u524d\u5f80\u76ee\u6a19\u76ee\u7684\u5730\uff0c\u6700\u8fd1\u6709\u4e86\u986f\u8457\u7684\u9032\u5c55\u3002\u8207\u5177\u6709\u9810\u5b9a\u7fa9\u8ecc\u8de1\u7684\u96e2\u6563\u74b0\u5883\u4e2d\u7684\u5c0e\u822a\u76f8\u6bd4\uff0c\u9023\u7e8c\u74b0\u5883\u4e2d\u7684 VLN (VLN-CE) \u63d0\u51fa\u66f4\u5927\u7684\u6311\u6230\uff0c\u56e0\u70ba\u4ee3\u7406\u4eba\u53ef\u4ee5\u81ea\u7531\u5c0e\u822a\u4efb\u4f55\u7121\u969c\u7919\u4f4d\u7f6e\uff0c\u4e26\u4e14\u66f4\u5bb9\u6613\u53d7\u5230\u8996\u89ba\u906e\u64cb\u6216\u76f2\u9ede\u7684\u5f71\u97ff\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u5617\u8a66\u901a\u904e\u60f3\u50cf\u672a\u4f86\u7684\u74b0\u5883\uff08\u900f\u904e\u9810\u6e2c\u672a\u4f86\u7684\u8996\u89ba\u5f71\u50cf\u6216\u8a9e\u7fa9\u7279\u5fb5\uff09\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u800c\u4e0d\u662f\u50c5\u4f9d\u8cf4\u76ee\u524d\u7684\u89c0\u5bdf\u3002\u7136\u800c\uff0c\u9019\u4e9b\u57fa\u65bc RGB \u548c\u57fa\u65bc\u7279\u5fb5\u7684\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u5c0e\u822a\u6240\u9700\u7684\u76f4\u89c0\u5916\u89c0\u5c64\u7d1a\u8cc7\u8a0a\u6216\u9ad8\u5c64\u7d1a\u8a9e\u7fa9\u8907\u96dc\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u3001\u53ef\u6982\u62ec\u7684\u57fa\u65bc 3DGS \u7684\u9810\u8a13\u7df4\u7bc4\u4f8b\uff0c\u7a31\u70ba UnitedVLN\uff0c\u5b83\u4f7f\u4ee3\u7406\u4eba\u80fd\u5920\u900f\u904e\u7d71\u4e00\u5448\u73fe\u9ad8\u4fdd\u771f 360 \u8996\u89ba\u5f71\u50cf\u548c\u8a9e\u7fa9\u7279\u5fb5\u4f86\u66f4\u597d\u5730\u63a2\u7d22\u672a\u4f86\u7684\u74b0\u5883\u3002UnitedVLN \u63a1\u7528\u5169\u500b\u95dc\u9375\u65b9\u6848\uff1a\u5148\u641c\u5c0b\u518d\u67e5\u8a62\u7684\u62bd\u6a23\u548c\u5148\u5206\u958b\u518d\u7d71\u4e00\u7684\u5448\u73fe\uff0c\u9019\u6709\u52a9\u65bc\u6709\u6548\u5229\u7528\u795e\u7d93\u57fa\u5143\uff0c\u5e6b\u52a9\u6574\u5408\u5916\u89c0\u548c\u8a9e\u7fa9\u8cc7\u8a0a\u4ee5\u9032\u884c\u66f4\u7a69\u5065\u7684\u5c0e\u822a\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cUnitedVLN \u5728\u73fe\u6709\u7684 VLN-CE \u57fa\u6e96\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002", "author": "Guangzhao Dai et.al.", "authors": "Guangzhao Dai, Jian Zhao, Yuantao Chen, Yusen Qin, Hao Zhao, Guosen Xie, Yazhou Yao, Xiangbo Shu, Xuelong Li", "id": "2411.16053v1", "paper_url": "http://arxiv.org/abs/2411.16053v1", "repo": "null"}}