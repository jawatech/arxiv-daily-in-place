{"2502.13897": {"publish_time": "2025-02-19", "title": "DataSciBench: An LLM Agent Benchmark for Data Science", "paper_summary": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.", "paper_summary_zh": "<paragraph>\u9019\u7bc7\u8ad6\u6587\u63d0\u51fa\u4e86 DataSciBench\uff0c\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8cc7\u6599\u79d1\u5b78\u80fd\u529b\u7684\u7d9c\u5408\u57fa\u6e96\u3002\u6700\u8fd1\u76f8\u95dc\u7684\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u55ae\u4e00\u4efb\u52d9\u3001\u5bb9\u6613\u53d6\u5f97\u7684\u57fa\u790e\u771f\u5be6\u503c\u548c\u76f4\u63a5\u7684\u8a55\u4f30\u6307\u6a19\u4e0a\uff0c\u9019\u9650\u5236\u4e86\u53ef\u8a55\u4f30\u4efb\u52d9\u7684\u7bc4\u570d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cDataSciBench \u662f\u57fa\u65bc\u66f4\u5168\u9762\u4e14\u7d93\u904e\u6574\u7406\u7684\u81ea\u7136\u4e14\u5177\u6709\u6311\u6230\u6027\u7684\u63d0\u793a\u96c6\u5408\uff0c\u7528\u65bc\u4e0d\u78ba\u5b9a\u7684\u57fa\u790e\u771f\u5be6\u503c\u548c\u8a55\u4f30\u6307\u6a19\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u534a\u81ea\u52d5\u5316\u7684\u7ba1\u9053\uff0c\u7528\u65bc\u7522\u751f\u57fa\u790e\u771f\u5be6\u503c (GT) \u548c\u9a57\u8b49\u8a55\u4f30\u6307\u6a19\u3002\u6b64\u7ba1\u9053\u5229\u7528\u4e26\u5be6\u4f5c\u4e86\u57fa\u65bc LLM \u7684\u81ea\u6211\u4e00\u81f4\u6027\u548c\u4eba\u5de5\u9a57\u8b49\u7b56\u7565\uff0c\u900f\u904e\u5229\u7528\u6536\u96c6\u7684\u63d0\u793a\u3001\u9810\u5b9a\u7fa9\u7684\u4efb\u52d9\u985e\u578b\u548c\u805a\u5408\u51fd\u6578\uff08\u6307\u6a19\uff09\u4f86\u7522\u751f\u6e96\u78ba\u7684 GT\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5275\u65b0\u7684\u4efb\u52d9 - \u51fd\u6578 - \u7a0b\u5f0f\u78bc (TFC) \u67b6\u69cb\uff0c\u7528\u65bc\u6839\u64da\u7cbe\u78ba\u5b9a\u7fa9\u7684\u6307\u6a19\u548c\u7a0b\u5f0f\u898f\u5247\u8a55\u4f30\u6bcf\u500b\u7a0b\u5f0f\u78bc\u57f7\u884c\u7d50\u679c\u3002\u6211\u5011\u7684\u5be6\u9a57\u67b6\u69cb\u6d89\u53ca\u4f7f\u7528\u6211\u5011\u6536\u96c6\u7684\u591a\u6a23\u5316\u63d0\u793a\uff0c\u6e2c\u8a66 6 \u500b\u57fa\u65bc API \u7684\u6a21\u578b\u30018 \u500b\u958b\u6e90\u901a\u7528\u6a21\u578b\u548c 9 \u500b\u958b\u6e90\u7a0b\u5f0f\u78bc\u751f\u6210\u6a21\u578b\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u63d0\u4f9b\u5c0d\u8cc7\u6599\u79d1\u5b78\u4e2d LLM \u7684\u66f4\u5168\u9762\u4e14\u56b4\u8b39\u7684\u8a55\u4f30\uff0c\u63ed\u793a\u5176\u512a\u9ede\u548c\u7f3a\u9ede\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u57fa\u65bc API \u7684\u6a21\u578b\u5728\u6240\u6709\u6307\u6a19\u4e0a\u90fd\u512a\u65bc\u958b\u6e90\u6a21\u578b\uff0c\u800c Deepseek-Coder-33B-Instruct \u5728\u958b\u6e90\u6a21\u578b\u4e2d\u7372\u5f97\u6700\u9ad8\u5206\u3002\u6211\u5011\u5728 https://github.com/THUDM/DataSciBench \u91cb\u51fa\u6240\u6709\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u3002</paragraph>", "author": "Dan Zhang et.al.", "authors": "Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue", "id": "2502.13897v1", "paper_url": "http://arxiv.org/abs/2502.13897v1", "repo": "null"}}