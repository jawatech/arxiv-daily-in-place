{"2412.15995": {"publish_time": "2024-12-20", "title": "Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling", "paper_summary": "Conversational assistants are increasingly popular across diverse real-world\napplications, highlighting the need for advanced multimodal speech modeling.\nSpeech, as a natural mode of communication, encodes rich user-specific\ncharacteristics such as speaking rate and pitch, making it critical for\neffective interaction. Our work introduces a data-centric customization\napproach for efficiently enhancing multimodal understanding in conversational\nspeech modeling. Central to our contributions is a novel multi-task learning\nparadigm that involves designing auxiliary tasks to utilize a small amount of\nspeech data. Our approach achieves state-of-the-art performance on the\nSpoken-SQuAD benchmark, using only 10% of the training data with open-weight\nmodels, establishing a robust and efficient framework for audio-centric\nconversational modeling. We also introduce ASK-QA, the first dataset for\nmulti-turn spoken dialogue with ambiguous user requests and dynamic evaluation\ninputs. Code and data forthcoming.", "paper_summary_zh": "\u5c0d\u8a71\u5f0f\u52a9\u7406\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\u8d8a\u4f86\u8d8a\u53d7\u6b61\u8fce\uff0c\u7a81\u986f\u4e86\u5c0d\u9032\u968e\u591a\u6a21\u614b\u8a9e\u97f3\u5efa\u6a21\u7684\u9700\u6c42\u3002\n\u8a9e\u97f3\u4f5c\u70ba\u4e00\u7a2e\u81ea\u7136\u7684\u6e9d\u901a\u6a21\u5f0f\uff0c\u7de8\u78bc\u4e86\u4f7f\u7528\u8005\u7279\u6709\u7684\u8c50\u5bcc\u7279\u5fb5\uff0c\u4f8b\u5982\u8aaa\u8a71\u901f\u5ea6\u548c\u97f3\u9ad8\uff0c\u4f7f\u5176\u5c0d\u65bc\u6709\u6548\u4e92\u52d5\u81f3\u95dc\u91cd\u8981\u3002\n\u6211\u5011\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u7a2e\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u5ba2\u88fd\u5316\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u589e\u5f37\u5c0d\u8a71\u5f0f\u8a9e\u97f3\u5efa\u6a21\u4e2d\u7684\u591a\u6a21\u614b\u7406\u89e3\u3002\n\u6211\u5011\u8ca2\u737b\u7684\u6838\u5fc3\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u4efb\u52d9\u5b78\u7fd2\u7bc4\u4f8b\uff0c\u5176\u4e2d\u6d89\u53ca\u8a2d\u8a08\u8f14\u52a9\u4efb\u52d9\uff0c\u4ee5\u5229\u7528\u5c11\u91cf\u7684\u8a9e\u97f3\u8cc7\u6599\u3002\n\u6211\u5011\u7684\u505a\u6cd5\u5728 Spoken-SQuAD \u57fa\u6e96\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u50c5\u4f7f\u7528 10% \u7684\u8a13\u7df4\u8cc7\u6599\u548c\u958b\u653e\u6b0a\u91cd\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u4e00\u500b\u5f37\u5927\u4e14\u6709\u6548\u7684\u4ee5\u97f3\u8a0a\u70ba\u4e2d\u5fc3\u7684\u5c0d\u8a71\u5f0f\u5efa\u6a21\u67b6\u69cb\u3002\n\u6211\u5011\u9084\u63a8\u51fa\u4e86 ASK-QA\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d\u591a\u56de\u5408\u53e3\u8a9e\u5c0d\u8a71\u7684\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6a21\u7a1c\u5169\u53ef\u7684\u4f7f\u7528\u8005\u8981\u6c42\u548c\u52d5\u614b\u8a55\u4f30\u8f38\u5165\u3002\n\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u5373\u5c07\u63a8\u51fa\u3002", "author": "Maximillian Chen et.al.", "authors": "Maximillian Chen, Ruoxi Sun, Sercan \u00d6. Ar\u0131k", "id": "2412.15995v1", "paper_url": "http://arxiv.org/abs/2412.15995v1", "repo": "null"}}