{"2411.05281": {"publish_time": "2024-11-08", "title": "Fox-1 Technical Report", "paper_summary": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa Fox-1\uff0c\u9019\u662f\u4e00\u500b\u7531 Fox-1-1.6B \u548c Fox-1-1.6B-Instruct-v0.1 \u7d44\u6210\u7684\u4e00\u7cfb\u5217\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM)\u3002\u9019\u4e9b\u6a21\u578b\u7d93\u904e 3 \u5146\u500b\u7db2\u8def\u64f7\u53d6\u6587\u4ef6\u8cc7\u6599\u7684\u9810\u8a13\u7df4\uff0c\u4e26\u4f7f\u7528 50 \u5104\u500b\u9075\u5faa\u6307\u793a\u548c\u591a\u8f2a\u5c0d\u8a71\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u3002\u70ba\u4e86\u63d0\u9ad8\u9810\u8a13\u7df4\u6548\u7387\uff0cFox-1-1.6B \u6a21\u578b\u5728\u6240\u6709\u8a13\u7df4\u8cc7\u6599\u4e2d\u5f15\u5165\u4e86\u5275\u65b0\u7684 3 \u968e\u6bb5\u8cc7\u6599\u8ab2\u7a0b\uff0c\u5e8f\u5217\u9577\u5ea6\u70ba 2K-8K\u3002\u5728\u67b6\u69cb\u8a2d\u8a08\u4e2d\uff0cFox-1 \u63a1\u7528\u66f4\u6df1\u7684\u5c64\u7d1a\u7d50\u69cb\u3001\u64f4\u5145\u7684\u8a5e\u5f59\u91cf\uff0c\u4e26\u5229\u7528\u7fa4\u7d44\u67e5\u8a62\u6ce8\u610f\u529b (GQA)\uff0c\u8207\u5176\u4ed6 SLM \u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u80fd\u4e14\u9ad8\u6548\u7684\u67b6\u69cb\u3002\u8207 StableLM-2-1.6B\u3001Gemma-2B\u3001Qwen1.5-1.8B \u548c OpenELM1.1B \u76f8\u6bd4\uff0cFox-1 \u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u5230\u66f4\u597d\u6216\u540c\u7b49\u7684\u6548\u80fd\uff0c\u540c\u6642\u5177\u6709\u7af6\u722d\u529b\u7684\u63a8\u8ad6\u901f\u5ea6\u548c\u541e\u5410\u91cf\u3002\u6a21\u578b\u6b0a\u91cd\u5df2\u5728 Apache 2.0 \u6388\u6b0a\u4e0b\u767c\u5e03\uff0c\u6211\u5011\u7684\u76ee\u6a19\u662f\u63a8\u5ee3 LLM \u7684\u6c11\u4e3b\u5316\uff0c\u4e26\u8b93\u6574\u500b\u958b\u6e90\u793e\u7fa4\u90fd\u80fd\u5145\u5206\u4f7f\u7528\u3002", "author": "Zijian Hu et.al.", "authors": "Zijian Hu, Jipeng Zhang, Rui Pan, Zhaozhuo Xu, Salman Avestimehr, Chaoyang He, Tong Zhang", "id": "2411.05281v1", "paper_url": "http://arxiv.org/abs/2411.05281v1", "repo": "null"}}