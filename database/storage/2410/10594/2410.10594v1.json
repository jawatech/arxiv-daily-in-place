{"2410.10594": {"publish_time": "2024-10-14", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "paper_summary": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u662f\u4e00\u7a2e\u6709\u6548\u6280\u8853\uff0c\u53ef\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5229\u7528\u5916\u90e8\u77e5\u8b58\u4f86\u6e90\u9032\u884c\u751f\u6210\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684 RAG \u7cfb\u7d71\u50c5\u57fa\u65bc\u6587\u5b57\uff0c\u7121\u6cd5\u5229\u7528\u73fe\u5be6\u4e16\u754c\u591a\u6a21\u614b\u6587\u4ef6\u4e2d\u7684\u7248\u9762\u548c\u5716\u50cf\u7b49\u8996\u89ba\u8cc7\u8a0a\uff0c\u800c\u9019\u4e9b\u8cc7\u8a0a\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 VisRAG\uff0c\u5b83\u900f\u904e\u5efa\u7acb\u4e00\u500b\u57fa\u65bc\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684 RAG \u7ba1\u7dda\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u5728\u9019\u500b\u7ba1\u7dda\u4e2d\uff0c\u4e26\u975e\u5148\u89e3\u6790\u6587\u4ef6\u4ee5\u53d6\u5f97\u6587\u5b57\uff0c\u800c\u662f\u76f4\u63a5\u4f7f\u7528 VLM \u5c07\u6587\u4ef6\u5d4c\u5165\u70ba\u5716\u50cf\uff0c\u7136\u5f8c\u6aa2\u7d22\u4ee5\u589e\u5f37 VLM \u7684\u751f\u6210\u3002\u8207\u50b3\u7d71\u7684\u57fa\u65bc\u6587\u5b57\u7684 RAG \u76f8\u6bd4\uff0cVisRAG \u6700\u5927\u5316\u4fdd\u7559\u548c\u5229\u7528\u539f\u59cb\u6587\u4ef6\u4e2d\u7684\u8cc7\u6599\u8cc7\u8a0a\uff0c\u6d88\u9664\u4e86\u89e3\u6790\u904e\u7a0b\u4e2d\u7522\u751f\u7684\u8cc7\u8a0a\u907a\u5931\u3002\u6211\u5011\u6536\u96c6\u4e86\u958b\u6e90\u8cc7\u6599\u548c\u5408\u6210\u8cc7\u6599\u4f86\u8a13\u7df4 VisRAG \u4e2d\u7684\u6aa2\u7d22\u5668\uff0c\u4e26\u63a2\u7d22\u4e86\u5404\u7a2e\u751f\u6210\u65b9\u6cd5\u3002\u5be6\u9a57\u8b49\u660e\uff0cVisRAG \u5728\u6aa2\u7d22\u548c\u751f\u6210\u968e\u6bb5\u90fd\u512a\u65bc\u50b3\u7d71\u7684 RAG\uff0c\u5728\u50b3\u7d71\u7684\u57fa\u65bc\u6587\u5b57\u7684 RAG \u7ba1\u7dda\u4e2d\uff0c\u7aef\u5230\u7aef\u6548\u80fd\u63d0\u5347\u4e86 25--39%\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u986f\u793a\uff0cVisRAG \u80fd\u6709\u6548\u5229\u7528\u8a13\u7df4\u8cc7\u6599\uff0c\u4e26\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u70ba\u591a\u6a21\u614b\u6587\u4ef6\u4e2d RAG \u7684\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/openbmb/visrag \u53d6\u5f97\u3002", "author": "Shi Yu et.al.", "authors": "Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun", "id": "2410.10594v1", "paper_url": "http://arxiv.org/abs/2410.10594v1", "repo": "null"}}