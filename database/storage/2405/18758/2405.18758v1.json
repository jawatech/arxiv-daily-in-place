{"2405.18758": {"publish_time": "2024-05-29", "title": "Learning to Continually Learn with the Bayesian Principle", "paper_summary": "In the present era of deep learning, continual learning research is mainly\nfocused on mitigating forgetting when training a neural network with stochastic\ngradient descent on a non-stationary stream of data. On the other hand, in the\nmore classical literature of statistical machine learning, many models have\nsequential Bayesian update rules that yield the same learning outcome as the\nbatch training, i.e., they are completely immune to catastrophic forgetting.\nHowever, they are often overly simple to model complex real-world data. In this\nwork, we adopt the meta-learning paradigm to combine the strong\nrepresentational power of neural networks and simple statistical models'\nrobustness to forgetting. In our novel meta-continual learning framework,\ncontinual learning takes place only in statistical models via ideal sequential\nBayesian update rules, while neural networks are meta-learned to bridge the raw\ndata and the statistical models. Since the neural networks remain fixed during\ncontinual learning, they are protected from catastrophic forgetting. This\napproach not only achieves significantly improved performance but also exhibits\nexcellent scalability. Since our approach is domain-agnostic and\nmodel-agnostic, it can be applied to a wide range of problems and easily\nintegrated with existing model architectures.", "paper_summary_zh": "\u5728\u6df1\u5ea6\u5b78\u7fd2\u7684\u7576\u4ee3\uff0c\u6301\u7e8c\u5b78\u7fd2\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6e1b\u8f15\u907a\u5fd8\uff0c\u7576\u4f7f\u7528\u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5728\u975e\u7a69\u614b\u8cc7\u6599\u4e32\u6d41\u4e0a\u8a13\u7df4\u795e\u7d93\u7db2\u8def\u6642\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u66f4\u7d93\u5178\u7684\u7d71\u8a08\u6a5f\u5668\u5b78\u7fd2\u6587\u737b\u4e2d\uff0c\u8a31\u591a\u6a21\u578b\u5177\u6709\u5faa\u5e8f\u8c9d\u6c0f\u66f4\u65b0\u898f\u5247\uff0c\u53ef\u7522\u751f\u8207\u6279\u6b21\u8a13\u7df4\u76f8\u540c\u7684\u5b78\u7fd2\u7d50\u679c\uff0c\u5373\u5b83\u5011\u5b8c\u5168\u4e0d\u53d7\u707d\u96e3\u6027\u907a\u5fd8\u7684\u5f71\u97ff\u3002\u7136\u800c\uff0c\u5b83\u5011\u901a\u5e38\u904e\u65bc\u7c21\u55ae\uff0c\u7121\u6cd5\u6a21\u64ec\u8907\u96dc\u7684\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a1\u7528\u5143\u5b78\u7fd2\u7bc4\u4f8b\u4f86\u7d50\u5408\u795e\u7d93\u7db2\u8def\u7684\u5f37\u5927\u8868\u793a\u80fd\u529b\u548c\u7c21\u55ae\u7d71\u8a08\u6a21\u578b\u5c0d\u907a\u5fd8\u7684\u7a69\u5065\u6027\u3002\u5728\u6211\u5011\u65b0\u7a4e\u7684\u5143\u6301\u7e8c\u5b78\u7fd2\u67b6\u69cb\u4e2d\uff0c\u6301\u7e8c\u5b78\u7fd2\u50c5\u900f\u904e\u7406\u60f3\u7684\u5faa\u5e8f\u8c9d\u6c0f\u66f4\u65b0\u898f\u5247\u5728\u7d71\u8a08\u6a21\u578b\u4e2d\u9032\u884c\uff0c\u800c\u795e\u7d93\u7db2\u8def\u5247\u900f\u904e\u5143\u5b78\u7fd2\u4f86\u6a4b\u63a5\u539f\u59cb\u8cc7\u6599\u548c\u7d71\u8a08\u6a21\u578b\u3002\u7531\u65bc\u795e\u7d93\u7db2\u8def\u5728\u6301\u7e8c\u5b78\u7fd2\u671f\u9593\u4fdd\u6301\u56fa\u5b9a\uff0c\u56e0\u6b64\u5b83\u5011\u53d7\u5230\u4fdd\u8b77\uff0c\u4e0d\u6703\u767c\u751f\u707d\u96e3\u6027\u907a\u5fd8\u3002\u9019\u7a2e\u65b9\u6cd5\u4e0d\u50c5\u986f\u8457\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u9084\u5c55\u73fe\u51fa\u512a\u7570\u7684\u53ef\u64f4\u5145\u6027\u3002\u7531\u65bc\u6211\u5011\u7684\u65b9\u6cd5\u8207\u9818\u57df\u7121\u95dc\u4e14\u8207\u6a21\u578b\u7121\u95dc\uff0c\u56e0\u6b64\u53ef\u4ee5\u61c9\u7528\u65bc\u5ee3\u6cdb\u7684\u554f\u984c\uff0c\u4e26\u8f15\u9b06\u8207\u73fe\u6709\u7684\u6a21\u578b\u67b6\u69cb\u6574\u5408\u3002", "author": "Soochan Lee et.al.", "authors": "Soochan Lee, Hyeonseong Jeon, Jaehyeon Son, Gunhee Kim", "id": "2405.18758v1", "paper_url": "http://arxiv.org/abs/2405.18758v1", "repo": "https://github.com/soochan-lee/sb-mcl"}}