{"2412.21199": {"publish_time": "2024-12-30", "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation", "paper_summary": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5f15\u5165\u4e86\u81ea\u547c\u53eb\u7a0b\u5f0f\u78bc\u751f\u6210\uff0c\u9019\u662f\u4e00\u9805\u65b0\u4efb\u52d9\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u7684\u6f38\u9032\u5f0f\u63a8\u7406\u548c\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u5728\u6b64\u4efb\u52d9\u4e2d\uff0c\u6a21\u578b\u6703\u9047\u5230\u4e00\u500b\u57fa\u672c\u554f\u984c\u548c\u4e00\u500b\u76f8\u95dc\u7684\u66f4\u8907\u96dc\u554f\u984c\u3002\u4ed6\u5011\u5fc5\u9808\u89e3\u6c7a\u57fa\u672c\u554f\u984c\uff0c\u7136\u5f8c\u5229\u7528\u5176\u89e3\u6c7a\u65b9\u6848\u4f86\u89e3\u6c7a\u66f4\u8907\u96dc\u7684\u554f\u984c\u3002\u9019\u9805\u5de5\u4f5c\u6709\u4e09\u5927\u8ca2\u737b\u3002\u9996\u5148\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u901a\u7528\u914d\u65b9\uff0c\u7528\u65bc\u751f\u6210\u66f4\u5177\u6311\u6230\u6027\u7684\u73fe\u6709\u57fa\u6e96\u7248\u672c\uff0c\u5f9e\u800c\u7522\u751f\u4e86\u4e09\u500b\u65b0\u7684\u57fa\u6e96\uff1aHumanEval Pro\u3001MBPP Pro \u548c BigCodeBench-Lite Pro\uff0c\u5c08\u9580\u7528\u65bc\u8a55\u4f30 LLM \u7684\u81ea\u547c\u53eb\u7a0b\u5f0f\u78bc\u751f\u6210\u3002\u5176\u6b21\uff0c\u5f9e\u5c0d\u6211\u5011\u57fa\u6e96\u4e0a\u4e8c\u5341\u500b LLM \u7684\u5be6\u9a57\u7d50\u679c\u7684\u5206\u6790\u4e2d\uff0c\u6211\u5011\u6709\u5169\u500b\u91cd\u8981\u7684\u89c0\u5bdf\u7d50\u679c\uff1a(i) \u5927\u591a\u6578 LLM \u5728\u50b3\u7d71\u7a0b\u5f0f\u78bc\u751f\u6210\u57fa\u6e96\uff08\u5982 HumanEval \u548c MBPP\uff09\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u5728\u81ea\u547c\u53eb\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u537b\u4e0b\u964d\u4e86\u3002\u4f8b\u5982\uff0co1-mini \u5728 HumanEval \u4e0a\u9054\u5230\u4e86 96.2% \u7684 pass@1\uff0c\u4f46\u5728 HumanEval Pro \u4e0a\u50c5\u9054\u5230\u4e86 76.2%\u3002(ii) \u5728\u81ea\u547c\u53eb\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u4e2d\uff0c\u8207\u57fa\u672c\u6a21\u578b\u76f8\u6bd4\uff0c\u6307\u4ee4\u8abf\u6574\u6a21\u578b\u50c5\u986f\u793a\u51fa\u908a\u969b\u6539\u9032\u3002\u7b2c\u4e09\uff0c\u6211\u5011\u516c\u958b\u4e86\u6211\u5011\u8a55\u4f30\u7d50\u679c\u4e2d\u5b58\u5728\u7684\u5931\u6557\u6a21\u5f0f\u985e\u578b\u3002\u6240\u6709\u9019\u4e9b\u7d50\u679c\u90fd\u5f37\u8abf\u4e86\u9032\u4e00\u6b65\u63a8\u9032\u81ea\u547c\u53eb\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u7684\u5fc5\u8981\u6027\uff0c\u4e26\u70ba\u672a\u4f86\u589e\u5f37 LLM \u7a0b\u5f0f\u78bc\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002</paragraph>", "author": "Zhaojian Yu et.al.", "authors": "Zhaojian Yu, Yilun Zhao, Arman Cohan, Xiao-Ping Zhang", "id": "2412.21199v2", "paper_url": "http://arxiv.org/abs/2412.21199v2", "repo": "https://github.com/CodeEval-Pro/CodeEval-Pro"}}