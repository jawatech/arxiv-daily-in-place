{"2406.06331": {"publish_time": "2024-06-10", "title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations", "paper_summary": "This paper introduces MedExQA, a novel benchmark in medical\nquestion-answering, to evaluate large language models' (LLMs) understanding of\nmedical knowledge through explanations. By constructing datasets across five\ndistinct medical specialties that are underrepresented in current datasets and\nfurther incorporating multiple explanations for each question-answer pair, we\naddress a major gap in current medical QA benchmarks which is the absence of\ncomprehensive assessments of LLMs' ability to generate nuanced medical\nexplanations. Our work highlights the importance of explainability in medical\nLLMs, proposes an effective methodology for evaluating models beyond\nclassification accuracy, and sheds light on one specific domain, speech\nlanguage pathology, where current LLMs including GPT4 lack good understanding.\nOur results show generation evaluation with multiple explanations aligns better\nwith human assessment, highlighting an opportunity for a more robust automated\ncomprehension assessment for LLMs. To diversify open-source medical LLMs\n(currently mostly based on Llama2), this work also proposes a new medical\nmodel, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs\nbased on Llama2-70B in generating explanations, showing its effectiveness in\nthe resource-constrained medical domain. We will share our benchmark datasets\nand the trained model.", "paper_summary_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86 MedExQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u901a\u8fc7\u89e3\u91ca\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u533b\u5b66\u77e5\u8bc6\u7684\u7406\u89e3\u3002\u901a\u8fc7\u5728\u5f53\u524d\u6570\u636e\u96c6\u4e2d\u7684\u4e94\u4e2a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u9886\u57df\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u8fdb\u4e00\u6b65\u7eb3\u5165\u591a\u4e2a\u89e3\u91ca\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5f53\u524d\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u4e2d\u7684\u4e00\u5927\u7f3a\u9677\uff0c\u5373\u7f3a\u4e4f\u5bf9 LLM \u751f\u6210\u7ec6\u5fae\u533b\u5b66\u89e3\u91ca\u7684\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f3a\u8c03\u4e86\u53ef\u89e3\u91ca\u6027\u5728\u533b\u5b66 LLM \u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u5206\u7c7b\u51c6\u786e\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u9610\u660e\u4e86\u4e00\u4e2a\u7279\u5b9a\u9886\u57df\uff0c\u5373\u8bed\u8a00\u75c5\u7406\u5b66\uff0c\u5176\u4e2d\u5305\u62ec GPT4 \u5728\u5185\u7684\u5f53\u524d LLM \u7f3a\u4e4f\u826f\u597d\u7684\u7406\u89e3\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u591a\u4e2a\u89e3\u91ca\u8fdb\u884c\u751f\u6210\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u66f4\u4e00\u81f4\uff0c\u7a81\u663e\u4e86\u5bf9 LLM \u8fdb\u884c\u66f4\u7a33\u5065\u7684\u81ea\u52a8\u7406\u89e3\u8bc4\u4f30\u7684\u673a\u4f1a\u3002\u4e3a\u4e86\u4f7f\u5f00\u6e90\u533b\u5b66 LLM \u591a\u6837\u5316\uff08\u76ee\u524d\u4e3b\u8981\u57fa\u4e8e Llama2\uff09\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u6a21\u578b MedPhi-2\uff0c\u57fa\u4e8e Phi-2 (2.7B)\u3002\u8be5\u6a21\u578b\u5728\u751f\u6210\u89e3\u91ca\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e Llama2-70B \u7684\u533b\u5b66 LLM\uff0c\u663e\u793a\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u533b\u5b66\u9886\u57df\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5c06\u5206\u4eab\u6211\u4eec\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002", "author": "Yunsoo Kim et.al.", "authors": "Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Honghan Wu", "id": "2406.06331v1", "paper_url": "http://arxiv.org/abs/2406.06331v1", "repo": "null"}}