{"2407.15296": {"publish_time": "2024-07-21", "title": "Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection", "paper_summary": "Vision-language (VL) models often exhibit a limited understanding of complex\nexpressions of visual objects (e.g., attributes, shapes, and their relations),\ngiven complex and diverse language queries. Traditional approaches attempt to\nimprove VL models using hard negative synthetic text, but their effectiveness\nis limited. In this paper, we harness the exceptional compositional\nunderstanding capabilities of generative foundational models. We introduce a\nnovel method for structured synthetic data generation aimed at enhancing the\ncompositional understanding of VL models in language-based object detection.\nOur framework generates densely paired positive and negative triplets (image,\ntext descriptions, and bounding boxes) in both image and text domains. By\nleveraging these synthetic triplets, we transform 'weaker' VL models into\n'stronger' models in terms of compositional understanding, a process we call\n\"Weak-to-Strong Compositional Learning\" (WSCL). To achieve this, we propose a\nnew compositional contrastive learning formulation that discovers semantics and\nstructures in complex descriptions from synthetic triplets. As a result, VL\nmodels trained with our synthetic data generation exhibit a significant\nperformance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark\nby +6.9AP upon existing baselines.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00 (VL) \u6a21\u578b\u5f80\u5f80\u5c0d\u8996\u89ba\u7269\u4ef6\u7684\u8907\u96dc\u8868\u9054\uff08\u4f8b\u5982\uff0c\u5c6c\u6027\u3001\u5f62\u72c0\u53ca\u5176\u95dc\u4fc2\uff09\u7406\u89e3\u6709\u9650\uff0c\u56e0\u70ba\u8a9e\u8a00\u67e5\u8a62\u8907\u96dc\u4e14\u591a\u6a23\u3002\u50b3\u7d71\u65b9\u6cd5\u5617\u8a66\u4f7f\u7528\u96e3\u4ee5\u5426\u5b9a\u7684\u5408\u6210\u6587\u5b57\u4f86\u6539\u5584 VL \u6a21\u578b\uff0c\u4f46\u5176\u6548\u679c\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5229\u7528\u751f\u6210\u57fa\u790e\u6a21\u578b\u7684\u5353\u8d8a\u7d44\u5408\u7406\u89e3\u80fd\u529b\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u7528\u65bc\u7d50\u69cb\u5316\u5408\u6210\u8cc7\u6599\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u5f37 VL \u6a21\u578b\u5728\u57fa\u65bc\u8a9e\u8a00\u7684\u7269\u4ef6\u5075\u6e2c\u4e2d\u7684\u7d44\u5408\u7406\u89e3\u3002\u6211\u5011\u7684\u67b6\u69cb\u5728\u5f71\u50cf\u548c\u6587\u5b57\u9818\u57df\u4e2d\u7522\u751f\u5bc6\u96c6\u914d\u5c0d\u7684\u6b63\u8ca0\u4e09\u5143\u7d44\uff08\u5f71\u50cf\u3001\u6587\u5b57\u63cf\u8ff0\u548c\u908a\u754c\u6846\uff09\u3002\u900f\u904e\u5229\u7528\u9019\u4e9b\u5408\u6210\u4e09\u5143\u7d44\uff0c\u6211\u5011\u5c07\u300c\u8f03\u5f31\u300d\u7684 VL \u6a21\u578b\u8f49\u8b8a\u70ba\u5728\u7d44\u5408\u7406\u89e3\u65b9\u9762\u300c\u8f03\u5f37\u300d\u7684\u6a21\u578b\uff0c\u9019\u500b\u904e\u7a0b\u6211\u5011\u7a31\u4e4b\u70ba\u300c\u5f31\u5230\u5f37\u7684\u7d44\u5408\u5b78\u7fd2\u300d(Weak-to-Strong Compositional Learning, WSCL)\u3002\u70ba\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u7d44\u5408\u5c0d\u6bd4\u5b78\u7fd2\u516c\u5f0f\uff0c\u5f9e\u5408\u6210\u4e09\u5143\u7d44\u4e2d\u767c\u73fe\u8907\u96dc\u63cf\u8ff0\u4e2d\u7684\u8a9e\u7fa9\u548c\u7d50\u69cb\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u6211\u5011\u7684\u5408\u6210\u8cc7\u6599\u751f\u6210\u8a13\u7df4\u7684 VL \u6a21\u578b\u5728 Omnilabel \u57fa\u6e96\u4e0a\u8868\u73fe\u63d0\u5347\u986f\u8457\uff0c\u9054 +5AP\uff0c\u5728 D3 \u57fa\u6e96\u4e0a\u5247\u63d0\u5347 +6.9AP\uff0c\u512a\u65bc\u73fe\u6709\u7684\u57fa\u6e96\u3002", "author": "Kwanyong Park et.al.", "authors": "Kwanyong Park, Kuniaki Saito, Donghyun Kim", "id": "2407.15296v1", "paper_url": "http://arxiv.org/abs/2407.15296v1", "repo": "null"}}