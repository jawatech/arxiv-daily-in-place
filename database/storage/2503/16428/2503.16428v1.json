{"2503.16428": {"publish_time": "2025-03-20", "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring", "paper_summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.", "paper_summary_zh": "<paragraph>\u9577\u4e0a\u4e0b\u6587 Transformer \u6a21\u578b (LCTM) \u5c0d\u65bc\u5be6\u969b\u61c9\u7528\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u7531\u65bc\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u4e8c\u6b21\u65b9\u8907\u96dc\u5ea6\uff0c\u5c0e\u81f4\u5176\u8a08\u7b97\u6210\u672c\u5f88\u9ad8\u3002\u584a\u7a00\u758f\u6ce8\u610f\u529b\u900f\u904e\u5c07\u8a08\u7b97\u96c6\u4e2d\u5728\u95dc\u9375\u5340\u57df\u4f86\u6e1b\u8f15\u9019\u7a2e\u60c5\u6cc1\uff0c\u7136\u800c\uff0c\u7531\u65bc\u584a\u91cd\u8981\u6027\u6e2c\u91cf\u6210\u672c\u9ad8\u6602\uff0c\u73fe\u6709\u65b9\u6cd5\u96e3\u4ee5\u5728\u6e96\u78ba\u6027\u548c\u6548\u7387\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 XAttention\uff0c\u9019\u662f\u4e00\u500b\u4f7f\u7528\u7a00\u758f\u6ce8\u610f\u529b\u986f\u8457\u52a0\u901f Transformer \u6a21\u578b\u4e2d\u9577\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\u3002XAttention \u7684\u95dc\u9375\u5275\u65b0\u5728\u65bc\u5176\u6d1e\u5bdf\u529b\uff1a\u6ce8\u610f\u529b\u77e9\u9663\u4e2d\u53cd\u5c0d\u89d2\u7dda\u503c\uff08\u5373\u5f9e\u5de6\u4e0b\u5230\u53f3\u4e0a\uff09\u7684\u7e3d\u548c\u63d0\u4f9b\u4e86\u4e00\u500b\u5f37\u5927\u7684\u584a\u91cd\u8981\u6027\u6307\u6a19\u3002\u9019\u5141\u8a31\u7cbe\u78ba\u8b58\u5225\u548c\u4fee\u526a\u4e0d\u91cd\u8981\u7684\u584a\uff0c\u5f9e\u800c\u5be6\u73fe\u9ad8\u7a00\u758f\u6027\u548c\u986f\u8457\u52a0\u901f\u63a8\u7406\u3002\u5728\u91dd\u5c0d\u9ad8\u8981\u6c42\u9577\u4e0a\u4e0b\u6587\u57fa\u6e96\u7684\u5168\u9762\u8a55\u4f30\u4e2d\uff0c\u5305\u62ec\u7528\u65bc\u8a9e\u8a00\u7684 RULER \u548c LongBench\u3001\u7528\u65bc\u8996\u983b\u7406\u89e3\u7684 VideoMME \u4ee5\u53ca\u7528\u65bc\u8996\u983b\u751f\u6210\u7684 VBench\uff0cXAttention \u5728\u5be6\u73fe\u5927\u5e45\u8a08\u7b97\u589e\u76ca\u7684\u540c\u6642\uff0c\u9054\u5230\u4e86\u8207\u5168\u6ce8\u610f\u529b\u76f8\u7576\u7684\u6e96\u78ba\u6027\u3002\u6211\u5011\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u8a08\u7b97\u901f\u5ea6\u6700\u9ad8\u53ef\u63d0\u5347 13.5 \u500d\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86 XAttention \u91cb\u653e\u584a\u7a00\u758f\u6ce8\u610f\u529b\u5be6\u969b\u6f5b\u529b\u7684\u80fd\u529b\uff0c\u70ba\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u53ef\u64f4\u5c55\u4e14\u9ad8\u6548\u5730\u90e8\u7f72 LCTM \u92ea\u5e73\u4e86\u9053\u8def\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/mit-han-lab/x-attention \u53d6\u5f97\u3002</paragraph>\n", "author": "Ruyi Xu et.al.", "authors": "Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, Song Han", "id": "2503.16428v1", "paper_url": "http://arxiv.org/abs/2503.16428v1", "repo": "null"}}