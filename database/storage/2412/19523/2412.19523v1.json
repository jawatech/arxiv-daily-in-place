{"2412.19523": {"publish_time": "2024-12-27", "title": "Attribution for Enhanced Explanation with Transferable Adversarial eXploration", "paper_summary": "The interpretability of deep neural networks is crucial for understanding\nmodel decisions in various applications, including computer vision.\nAttEXplore++, an advanced framework built upon AttEXplore, enhances attribution\nby incorporating transferable adversarial attack methods such as MIG and GRA,\nsignificantly improving the accuracy and robustness of model explanations. We\nconduct extensive experiments on five models, including CNNs (Inception-v3,\nResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the\nImageNet dataset. Our method achieves an average performance improvement of\n7.57\\% over AttEXplore and 32.62\\% compared to other state-of-the-art\ninterpretability algorithms. Using insertion and deletion scores as evaluation\nmetrics, we show that adversarial transferability plays a vital role in\nenhancing attribution results. Furthermore, we explore the impact of\nrandomness, perturbation rate, noise amplitude, and diversity probability on\nattribution performance, demonstrating that AttEXplore++ provides more stable\nand reliable explanations across various models. We release our code at:\nhttps://anonymous.4open.science/r/ATTEXPLOREP-8435/", "paper_summary_zh": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u5bf9\u4e8e\u7406\u89e3\u5404\u79cd\u5e94\u7528\uff08\u5305\u62ec\u8ba1\u7b97\u673a\u89c6\u89c9\uff09\u4e2d\u7684\u6a21\u578b\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002AttEXplore++ \u662f\u5efa\u7acb\u5728 AttEXplore \u4e4b\u4e0a\u7684\u9ad8\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff08\u4f8b\u5982 MIG \u548c GRA\uff09\u6765\u589e\u5f3a\u5f52\u56e0\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u89e3\u91ca\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5bf9\u4e94\u4e2a\u6a21\u578b\uff08\u5305\u62ec CNN\uff08Inception-v3\u3001ResNet-50\u3001VGG16\uff09\u548c\u89c6\u89c9\u8f6c\u6362\u5668\uff08MaxViT-T\u3001ViT-B/16\uff09\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4f7f\u7528 ImageNet \u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4 AttEXplore \u5b9e\u73b0\u4e86\u5e73\u5747 7.57% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u7b97\u6cd5\u63d0\u5347\u4e86 32.62%\u3002\u4f7f\u7528\u63d2\u5165\u548c\u5220\u9664\u5206\u6570\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u6211\u4eec\u8868\u660e\u5bf9\u6297\u53ef\u8f6c\u79fb\u6027\u5728\u589e\u5f3a\u5f52\u56e0\u7ed3\u679c\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u968f\u673a\u6027\u3001\u6270\u52a8\u7387\u3001\u566a\u58f0\u5e45\u5ea6\u548c\u591a\u6837\u6027\u6982\u7387\u5bf9\u5f52\u56e0\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8bc1\u660e AttEXplore++ \u5728\u5404\u79cd\u6a21\u578b\u4e2d\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u53ef\u9760\u7684\u89e3\u91ca\u3002\u6211\u4eec\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\uff1ahttps://anonymous.4open.science/r/ATTEXPLOREP-8435/", "author": "Zhiyu Zhu et.al.", "authors": "Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Huaming Chen, Jianlong Zhou, Fang Chen", "id": "2412.19523v1", "paper_url": "http://arxiv.org/abs/2412.19523v1", "repo": "null"}}