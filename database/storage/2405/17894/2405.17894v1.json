{"2405.17894": {"publish_time": "2024-05-28", "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models", "paper_summary": "Recent advancements in Large Vision-Language Models (VLMs) have underscored\ntheir superiority in various multimodal tasks. However, the adversarial\nrobustness of VLMs has not been fully explored. Existing methods mainly assess\nrobustness through unimodal adversarial attacks that perturb images, while\nassuming inherent resilience against text-based attacks. Different from\nexisting attacks, in this work we propose a more comprehensive strategy that\njointly attacks both text and image modalities to exploit a broader spectrum of\nvulnerability within VLMs. Specifically, we propose a dual optimization\nobjective aimed at guiding the model to generate affirmative responses with\nhigh toxicity. Our attack method begins by optimizing an adversarial image\nprefix from random noise to generate diverse harmful responses in the absence\nof text input, thus imbuing the image with toxic semantics. Subsequently, an\nadversarial text suffix is integrated and co-optimized with the adversarial\nimage prefix to maximize the probability of eliciting affirmative responses to\nvarious harmful instructions. The discovered adversarial image prefix and text\nsuffix are collectively denoted as a Universal Master Key (UMK). When\nintegrated into various malicious queries, UMK can circumvent the alignment\ndefenses of VLMs and lead to the generation of objectionable content, known as\njailbreaks. The experimental results demonstrate that our universal attack\nstrategy can effectively jailbreak MiniGPT-4 with a 96% success rate,\nhighlighting the vulnerability of VLMs and the urgent need for new alignment\nstrategies.", "paper_summary_zh": "\u8fd1\u671f\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u8fdb\u6b65\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0cVLM \u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u7684\u5355\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u6765\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5047\u8bbe\u5bf9\u57fa\u4e8e\u6587\u672c\u7684\u653b\u51fb\u5177\u6709\u5185\u5728\u7684\u5f39\u6027\u3002\u4e0e\u73b0\u6709\u653b\u51fb\u4e0d\u540c\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u7b56\u7565\uff0c\u540c\u65f6\u653b\u51fb\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u4ee5\u5229\u7528 VLM \u4e2d\u66f4\u5e7f\u6cdb\u7684\u6f0f\u6d1e\u8303\u56f4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u4f18\u5316\u76ee\u6807\uff0c\u65e8\u5728\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u5177\u6709\u9ad8\u6bd2\u6027\u7684\u80af\u5b9a\u6027\u54cd\u5e94\u3002\u6211\u4eec\u7684\u653b\u51fb\u65b9\u6cd5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u4f18\u5316\u5bf9\u6297\u56fe\u50cf\u524d\u7f00\u5f00\u59cb\uff0c\u4ee5\u5728\u6ca1\u6709\u6587\u672c\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u5404\u79cd\u6709\u5bb3\u54cd\u5e94\uff0c\u4ece\u800c\u4e3a\u56fe\u50cf\u6ce8\u5165\u6709\u6bd2\u8bed\u4e49\u3002\u968f\u540e\uff0c\u5c06\u5bf9\u6297\u6587\u672c\u540e\u7f00\u4e0e\u5bf9\u6297\u56fe\u50cf\u524d\u7f00\u96c6\u6210\u5e76\u5171\u540c\u4f18\u5316\uff0c\u4ee5\u6700\u5927\u5316\u5bf9\u5404\u79cd\u6709\u5bb3\u6307\u4ee4\u505a\u51fa\u80af\u5b9a\u6027\u54cd\u5e94\u7684\u6982\u7387\u3002\u53d1\u73b0\u7684\u5bf9\u6297\u56fe\u50cf\u524d\u7f00\u548c\u6587\u672c\u540e\u7f00\u7edf\u79f0\u4e3a\u901a\u7528\u4e3b\u5bc6\u94a5 (UMK)\u3002\u5f53\u96c6\u6210\u5230\u5404\u79cd\u6076\u610f\u67e5\u8be2\u4e2d\u65f6\uff0cUMK \u53ef\u4ee5\u89c4\u907f VLM \u7684\u5bf9\u9f50\u9632\u5fa1\uff0c\u5e76\u5bfc\u81f4\u751f\u6210\u88ab\u79f0\u4e3a\u8d8a\u72f1\u7684\u4ee4\u4eba\u53cd\u611f\u7684\u5185\u5bb9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u901a\u7528\u653b\u51fb\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5730\u4ee5 96% \u7684\u6210\u529f\u7387\u8d8a\u72f1 MiniGPT-4\uff0c\u7a81\u663e\u4e86 VLM \u7684\u8106\u5f31\u6027\u4ee5\u53ca\u5bf9\u65b0\u5bf9\u9f50\u7b56\u7565\u7684\u8feb\u5207\u9700\u6c42\u3002", "author": "Ruofan Wang et.al.", "authors": "Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang", "id": "2405.17894v1", "paper_url": "http://arxiv.org/abs/2405.17894v1", "repo": "null"}}