{"2407.18601": {"publish_time": "2024-07-26", "title": "Climbing the Complexity Ladder with Expressive Attention", "paper_summary": "Attention involves comparing query and key vectors in terms of a scalar\nproduct, $\\mathbf{Q}^T\\mathbf{K}$, together with a subsequent softmax\nnormalization. Classicaly, parallel/orthogonal/antiparallel queries and keys\nlead to large/intermediate/small attention weights. Here we study expressive\nattention (EA), which is based on $(\\mathbf{Q}^T\\mathbf{K})^2$, the squared dot\nproduct. In this case attention is enhanced when query and key are either\nparallel or antiparallel, and suppressed for orthogonal configurations. For a\nseries of autoregressive prediction tasks, we find that EA performs at least as\nwell as the standard mechanism, dot-product attention (DPA). Increasing task\ncomplexity, EA is observed to outperform DPA with increasing margins, which\nalso holds for multi-task settings. For a given model size, EA manages to\nachieve 100\\% performance for a range of complexity levels not accessible to\nDPA.", "paper_summary_zh": "\u6ce8\u610f\u529b\u6d89\u53ca\u4ee5\u6807\u91cf\u79ef $\\mathbf{Q}^T\\mathbf{K}$ \u6bd4\u8f83\u67e5\u8be2\u548c\u952e\u5411\u91cf\uff0c\u4ee5\u53ca\u968f\u540e\u7684 softmax \u5f52\u4e00\u5316\u3002\u7ecf\u5178\u5730\uff0c\u5e73\u884c\u7684/\u6b63\u4ea4\u7684/\u53cd\u5e73\u884c\u7684\u67e5\u8be2\u548c\u952e\u5bfc\u81f4\u5927\u7684/\u4e2d\u7b49\u7684/\u5c0f\u7684\u6ce8\u610f\u529b\u6743\u91cd\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u7814\u7a76\u57fa\u4e8e $(\\mathbf{Q}^T\\mathbf{K})^2$ \u7684\u5e73\u65b9\u70b9\u79ef\u7684\u8868\u8fbe\u5f0f\u6ce8\u610f\u529b (EA)\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5f53\u67e5\u8be2\u548c\u952e\u5e73\u884c\u6216\u53cd\u5e73\u884c\u65f6\uff0c\u6ce8\u610f\u529b\u4f1a\u589e\u5f3a\uff0c\u800c\u5f53\u6b63\u4ea4\u914d\u7f6e\u65f6\uff0c\u6ce8\u610f\u529b\u4f1a\u88ab\u6291\u5236\u3002\u5bf9\u4e8e\u4e00\u7cfb\u5217\u81ea\u56de\u5f52\u9884\u6d4b\u4efb\u52a1\uff0c\u6211\u4eec\u53d1\u73b0 EA \u7684\u8868\u73b0\u81f3\u5c11\u4e0e\u6807\u51c6\u673a\u5236\u70b9\u79ef\u6ce8\u610f\u529b (DPA) \u4e00\u6837\u597d\u3002\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u89c2\u5bdf\u5230 EA \u4ee5\u8d8a\u6765\u8d8a\u5927\u7684\u4f18\u52bf\u4f18\u4e8e DPA\uff0c\u8fd9\u4e5f\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u8bbe\u7f6e\u3002\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u6a21\u578b\u5927\u5c0f\uff0cEA \u8bbe\u6cd5\u9488\u5bf9 DPA \u65e0\u6cd5\u8fbe\u5230\u7684\u590d\u6742\u5ea6\u7ea7\u522b\u8303\u56f4\u5b9e\u73b0 100% \u7684\u6027\u80fd\u3002", "author": "Claudius Gros et.al.", "authors": "Claudius Gros", "id": "2407.18601v1", "paper_url": "http://arxiv.org/abs/2407.18601v1", "repo": "null"}}