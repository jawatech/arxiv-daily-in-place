{"2405.06196": {"publish_time": "2024-05-10", "title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks", "paper_summary": "Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.", "paper_summary_zh": "<paragraph>\u57fa\u790e\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f7f\u7528\u5927\u898f\u6a21\u958b\u653e\u9818\u57df\u5f71\u50cf\u548c\u6587\u5b57\u5c0d\u9032\u884c\u8a13\u7df4\uff0c\u6700\u8fd1\u5df2\u8abf\u6574\u7528\u65bc\u958b\u767c\u8996\u89ba\u8a9e\u8a00\u5206\u5272\u6a21\u578b (VLSM)\uff0c\u5141\u8a31\u5728\u63a8\u8ad6\u671f\u9593\u63d0\u4f9b\u6587\u5b57\u63d0\u793a\uff0c\u4ee5\u5f15\u5c0e\u5f71\u50cf\u5206\u5272\u3002\u5982\u679c\u53ef\u4ee5\u70ba\u91ab\u5b78\u5f71\u50cf\u5efa\u7acb\u5f37\u5927\u4e14\u5f37\u5927\u7684 VLSM\uff0c\u5b83\u53ef\u4ee5\u5e6b\u52a9\u91ab\u7642\u5c08\u696d\u4eba\u54e1\u57f7\u884c\u8a31\u591a\u81e8\u5e8a\u4efb\u52d9\uff0c\u5728\u9019\u4e9b\u4efb\u52d9\u4e2d\uff0c\u4ed6\u5011\u5fc5\u9808\u82b1\u8cbb\u5927\u91cf\u6642\u9593\u4f86\u63cf\u7e6a\u76ee\u6a19\u7d50\u69cb\u3002\u91dd\u5c0d\u91ab\u5b78\u5f71\u50cf\u7684 VLSM \u6703\u4f7f\u7528\u958b\u653e\u9818\u57df\u81ea\u7136\u5f71\u50cf\u8cc7\u6599\u96c6\u9810\u8a13\u7df4\u7684\u57fa\u790e VLM \u6216 VLSM \u9032\u884c\u5fae\u8abf\uff0c\u56e0\u70ba\u6a19\u8a3b\u91ab\u5b78\u5f71\u50cf\u8cc7\u6599\u96c6\u8f03\u5c11\uff1b\u9019\u7a2e\u5fae\u8abf\u6703\u6d88\u8017\u8cc7\u6e90\u4e14\u6602\u8cb4\uff0c\u56e0\u70ba\u5b83\u901a\u5e38\u9700\u8981\u66f4\u65b0\u6240\u6709\u6216\u5927\u90e8\u5206\u9810\u8a13\u7df4\u53c3\u6578\u3002\u6700\u8fd1\uff0c\u5728 VLM \u4e2d\u63d0\u51fa\u4e86\u7a31\u70ba\u9069\u914d\u5668\u7684\u8f15\u91cf\u7d1a\u5340\u584a\uff0c\u5b83\u6703\u4fdd\u6301\u9810\u8a13\u7df4\u6a21\u578b\u51cd\u7d50\uff0c\u4e26\u4e14\u53ea\u5728\u5fae\u8abf\u671f\u9593\u8a13\u7df4\u9069\u914d\u5668\uff0c\u5927\u5e45\u6e1b\u5c11\u6240\u9700\u7684\u904b\u7b97\u8cc7\u6e90\u3002\u6211\u5011\u5f15\u9032\u4e00\u7a2e\u65b0\u7a4e\u7684\u9069\u914d\u5668\uff0cVLSM-Adapter\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528Transformer\u7de8\u78bc\u5668\u5fae\u8abf\u9810\u8a13\u7df4\u7684\u8996\u89ba\u8a9e\u8a00\u5206\u5272\u6a21\u578b\u3002\u6211\u5011\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684 CLIP \u57fa\u790e\u5206\u5272\u6a21\u578b\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0cVLSM-Adapter \u53ea\u6709 300 \u842c\u500b\u53ef\u8a13\u7df4\u53c3\u6578\uff0c\u5176\u6548\u80fd\u512a\u65bc\u73fe\u6709\u6280\u8853\uff0c\u4e26\u4e14\u53ef\u8207\u4e0a\u9650\u7aef\u5230\u7aef\u5fae\u8abf\u76f8\u5ab2\u7f8e\u3002\u539f\u59cb\u78bc\u53ef\u5728 https://github.com/naamiinepal/vlsm-adapter \u53d6\u5f97\u3002</paragraph>", "author": "Manish Dhakal et.al.", "authors": "Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal", "id": "2405.06196v1", "paper_url": "http://arxiv.org/abs/2405.06196v1", "repo": "null"}}