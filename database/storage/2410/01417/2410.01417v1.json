{"2410.01417": {"publish_time": "2024-10-02", "title": "The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs", "paper_summary": "Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u6700\u8fd1\u53d1\u73b0 MLLM \u4e0e\u4eba\u7c7b\u667a\u80fd\u76f8\u6bd4\u5b58\u5728\u8bb8\u591a\u7f3a\u9677\uff0c\u4f8b\u5982\u5e7b\u89c9\u3002\u4e3a\u4e86\u63a8\u52a8 MLLM \u7814\u7a76\uff0c\u793e\u533a\u81f4\u529b\u4e8e\u6784\u5efa\u5177\u6709\u590d\u6742\u4efb\u52a1\u7684\u66f4\u5927\u57fa\u51c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u5bf9\u4e00\u9879\u57fa\u672c\u4f46\u901a\u5e38\u88ab\u5ffd\u89c6\u7684\u667a\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1a**\u8054\u60f3**\uff0c\u4eba\u7c7b\u5c06\u89c2\u5bdf\u4e0e\u5148\u524d\u7684\u5b9e\u8df5\u8bb0\u5fc6\u8054\u7cfb\u8d77\u6765\u7684\u57fa\u672c\u80fd\u529b\u3002\u4e3a\u4e86\u5168\u9762\u8c03\u67e5 MLLM \u5728\u8054\u60f3\u65b9\u9762\u7684\u8868\u73b0\uff0c\u6211\u4eec\u5236\u5b9a\u4e86\u8054\u60f3\u4efb\u52a1\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f62\u5bb9\u8bcd\u548c\u52a8\u8bcd\u8bed\u4e49\u6982\u5ff5\u7684\u6807\u51c6\u57fa\u51c6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4fbf\u6377\u7684**\u65e0\u6ce8\u91ca**\u6784\u5efa\u65b9\u6cd5\uff0c\u5c06\u901a\u7528\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6211\u4eec\u7684\u8054\u60f3\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u8fdb\u884c\u6602\u8d35\u7684\u6570\u636e\u6ce8\u91ca\u548c\u6574\u7406\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6570\u636e\u4f18\u5316\u6d41\u7a0b\uff0c\u4ee5\u6d88\u9664\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u6df7\u4e71\u3002\u5728\u6b64\u6570\u636e\u5e93\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e09\u4e2a\u7ea7\u522b\u7684\u8054\u60f3\u4efb\u52a1\uff1a\u5355\u6b65\u3001\u540c\u6b65\u548c\u5f02\u6b65\u8054\u60f3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9 MLLM \u7684\u96f6\u6b21\u8054\u60f3\u80fd\u529b\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u6d89\u53ca\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5305\u62ec\u4e09\u79cd\u4e0d\u540c\u7684\u8bb0\u5fc6\u7b56\u7565\u3001\u5f00\u6e90\u548c\u95ed\u6e90 MLLM\u3001\u5c16\u7aef\u7684\u4e13\u5bb6\u6df7\u5408 (MoE) \u6a21\u578b\u4ee5\u53ca\u4eba\u7c7b\u4e13\u5bb6\u7684\u53c2\u4e0e\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u8c03\u67e5\u8868\u660e\uff0c\u5f53\u524d\u7684\u5f00\u6e90 MLLM \u5728\u6211\u4eec\u7684\u8054\u60f3\u4efb\u52a1\u4e2d\u59cb\u7ec8\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u80fd\u529b\uff0c\u5373\u4f7f\u662f\u76ee\u524d\u6700\u5148\u8fdb\u7684 GPT-4V\uff08\u89c6\u89c9\uff09\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u4e5f\u5b58\u5728\u663e\u7740\u5dee\u8ddd\u3002\u6211\u4eec\u76f8\u4fe1\u6211\u4eec\u7684\u57fa\u51c6\u5c06\u4e3a\u672a\u6765\u7684 MLLM \u7814\u7a76\u94fa\u5e73\u9053\u8def\u3002$\\textit{\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u5f97\uff1a}$\nhttps://mvig-rhos.com/llm_inception\u3002", "author": "Hong Li et.al.", "authors": "Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li", "id": "2410.01417v1", "paper_url": "http://arxiv.org/abs/2410.01417v1", "repo": "null"}}