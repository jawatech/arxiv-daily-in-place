{"2405.09215": {"publish_time": "2024-05-15", "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model", "paper_summary": "We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. It\nis designed for efficient deployment on consumer GPU servers. Our work directly\nconfronts a pivotal industry issue by grappling with the prohibitive service\ncosts that hinder the broad adoption of large-scale multimodal systems. Through\nrigorous training, we have developed a 1B-scale language model from the ground\nup, employing the LLaVA paradigm for modal alignment. The result, which we call\nXmodel-VLM, is a lightweight yet powerful multimodal vision language model.\nExtensive testing across numerous classic multimodal benchmarks has revealed\nthat despite its smaller size and faster execution, Xmodel-VLM delivers\nperformance comparable to that of larger models. Our model checkpoints and code\nare publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 Xmodel-VLM\uff0c\u4e00\u7a2e\u5c16\u7aef\u7684\u6a21\u614b\u591a\u6a21\u614b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u3002\u5b83\n\u65e8\u5728\u6709\u6548\u90e8\u7f72\u5728\u6d88\u8cbb\u8005 GPU \u4f3a\u670d\u5668\u4e0a\u3002\u6211\u5011\u7684\u4f5c\u54c1\u76f4\u63a5\u9762\u5c0d\u7522\u696d\u7684\u95dc\u9375\u554f\u984c\uff0c\u4e5f\u5c31\u662f\u963b\u7919\u5927\u898f\u6a21\u6a21\u614b\u591a\u6a21\u614b\u7cfb\u7d71\u5ee3\u6cdb\u63a1\u7528\u7684\u9ad8\u6602\u670d\u52d9\u6210\u672c\u3002\u900f\u904e\u56b4\u8b39\u7684\u8a13\u7df4\uff0c\u6211\u5011\u5f9e\u982d\u958b\u59cb\u958b\u767c\u4e86\u4e00\u500b 1B \u898f\u6a21\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u63a1\u7528 LLaVA \u5178\u7bc4\u9032\u884c\u6a21\u614b\u5c0d\u9f4a\u3002\u6211\u5011\u7a31\u4e4b\u70ba Xmodel-VLM \u7684\u6210\u679c\uff0c\u662f\u4e00\u500b\u8f15\u91cf\u4f46\u5f37\u5927\u7684\u6a21\u614b\u591a\u6a21\u614b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u3002\u5728\u773e\u591a\u7d93\u5178\u6a21\u614b\u591a\u6a21\u614b\u57fa\u6e96\u4e2d\u7684\u5ee3\u6cdb\u6e2c\u8a66\u986f\u793a\uff0c\u5118\u7ba1 Xmodel-VLM \u5c3a\u5bf8\u8f03\u5c0f\u4e14\u57f7\u884c\u901f\u5ea6\u8f03\u5feb\uff0c\u4f46\u5176\u6548\u80fd\u537b\u8207\u8f03\u5927\u578b\u6a21\u578b\u76f8\u7576\u3002\u6211\u5011\u7684\u6a21\u578b\u6aa2\u67e5\u9ede\u548c\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u5728 GitHub \u4e0a\uff0c\u7db2\u5740\u70ba https://github.com/XiaoduoAILab/XmodelVLM\u3002", "author": "Wanting Xu et.al.", "authors": "Wanting Xu, Yang Liu, Langping He, Xucheng Huang, Ling Jiang", "id": "2405.09215v1", "paper_url": "http://arxiv.org/abs/2405.09215v1", "repo": "https://github.com/xiaoduoailab/xmodelvlm"}}