{"2501.02790": {"publish_time": "2025-01-06", "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model", "paper_summary": "Reinforcement learning from human feedback (RLHF) has been widely adopted to\nalign language models (LMs) with human preference. Prior RLHF works typically\ntake a bandit formulation, which, though intuitive, ignores the sequential\nnature of LM generation and can suffer from the sparse reward issue. While\nrecent works propose dense token-level RLHF, treating each token as an action\nmay be oversubtle to proper reward assignment. In this paper, we seek to get\nthe best of both by training and utilizing a segment-level reward model, which\nassigns a reward to each semantically complete text segment that spans over a\nshort sequence of tokens. For reward learning, our method allows dynamic text\nsegmentation and compatibility with standard sequence-preference datasets. For\neffective RL-based LM training against segment reward, we generalize the\nclassical scalar bandit reward normalizers into location-aware normalizer\nfunctions and interpolate the segment reward for further densification. With\nthese designs, our method performs competitively on three popular RLHF\nbenchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation\nstudies are conducted to further demonstrate our method.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u88ab\u5ee3\u6cdb\u63a1\u7528\uff0c\u4ee5\u5c07\u8a9e\u8a00\u6a21\u578b (LM) \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u5148\u524d\u7684 RLHF \u5de5\u4f5c\u901a\u5e38\u63a1\u7528\u591a\u81c2\u8001\u864e\u6a5f\u516c\u5f0f\uff0c\u5118\u7ba1\u76f4\u89c0\uff0c\u4f46\u5ffd\u7565\u4e86 LM \u751f\u6210\u4e2d\u7684\u9806\u5e8f\u6027\u8cea\uff0c\u4e26\u4e14\u53ef\u80fd\u6703\u53d7\u5230\u7a00\u758f\u56de\u994b\u554f\u984c\u7684\u5f71\u97ff\u3002\u96d6\u7136\u6700\u8fd1\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u5bc6\u96c6\u4ee3\u5e63\u7d1a RLHF\uff0c\u4f46\u5c07\u6bcf\u500b\u4ee3\u5e63\u8996\u70ba\u52d5\u4f5c\u5c0d\u65bc\u9069\u7576\u7684\u56de\u994b\u5206\u914d\u4f86\u8aaa\u53ef\u80fd\u904e\u65bc\u5fae\u5999\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0b\u6c42\u901a\u904e\u8a13\u7df4\u548c\u5229\u7528\u5340\u6bb5\u7d1a\u56de\u994b\u6a21\u578b\u4f86\u7372\u5f97\u5169\u5168\u5176\u7f8e\uff0c\u8a72\u6a21\u578b\u6703\u5c0d\u8de8\u8d8a\u77ed\u5e8f\u5217\u4ee3\u5e63\u7684\u6bcf\u500b\u8a9e\u7fa9\u5b8c\u6574\u6587\u5b57\u5340\u6bb5\u5206\u914d\u56de\u994b\u3002\u5c0d\u65bc\u56de\u994b\u5b78\u7fd2\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5141\u8a31\u52d5\u614b\u6587\u5b57\u5206\u6bb5\uff0c\u4e26\u8207\u6a19\u6e96\u5e8f\u5217\u504f\u597d\u8cc7\u6599\u96c6\u76f8\u5bb9\u3002\u5c0d\u65bc\u91dd\u5c0d\u5340\u6bb5\u56de\u994b\u7684\u6709\u6548\u57fa\u65bc RL \u7684 LM \u8a13\u7df4\uff0c\u6211\u5011\u5c07\u7d93\u5178\u6a19\u91cf\u591a\u81c2\u8001\u864e\u6a5f\u56de\u994b\u6b63\u898f\u5316\u5668\u6982\u62ec\u70ba\u4f4d\u7f6e\u611f\u77e5\u6b63\u898f\u5316\u5668\u51fd\u6578\uff0c\u4e26\u5167\u63d2\u5340\u6bb5\u56de\u994b\u4ee5\u9032\u4e00\u6b65\u81f4\u5bc6\u5316\u3002\u900f\u904e\u9019\u4e9b\u8a2d\u8a08\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u4e09\u500b\u6d41\u884c\u7684 RLHF LM \u653f\u7b56\u57fa\u6e96\u4e0a\u8868\u73fe\u5177\u7af6\u722d\u529b\uff1aAlpacaEval 2.0\u3001Arena-Hard \u548c MT-Bench\u3002\u6d88\u878d\u7814\u7a76\u65e8\u5728\u9032\u4e00\u6b65\u5c55\u793a\u6211\u5011\u7684\u6a21\u578b\u3002", "author": "Yueqin Yin et.al.", "authors": "Yueqin Yin, Shentao Yang, Yujia Xie, Ziyi Yang, Yuting Sun, Hany Awadalla, Weizhu Chen, Mingyuan Zhou", "id": "2501.02790v1", "paper_url": "http://arxiv.org/abs/2501.02790v1", "repo": "null"}}