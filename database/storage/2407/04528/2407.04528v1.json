{"2407.04528": {"publish_time": "2024-07-05", "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "paper_summary": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis of between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u548c\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u5df2\u6210\u70ba\u5728\u6700\u5c0f\u5316\u904b\u7b97\u9700\u6c42\u7684\u540c\u6642\uff0c\u9069\u61c9\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u71b1\u9580\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07 PEFT \u65b9\u6cd5 (P-tuning\u3001\u9069\u914d\u5668\u548c LoRA) \u61c9\u7528\u65bc\u4fee\u6539\u5f8c\u7684\u6aa2\u7d22\u589e\u5f37\u5f0f Transformer (RETRO) \u548c\u4e00\u500b\u57fa\u7dda GPT \u6a21\u578b\uff0c\u5176\u5927\u5c0f\u7bc4\u570d\u5f9e 8.23 \u5104\u5230 480 \u5104\u500b\u53c3\u6578\u3002\u6211\u5011\u8868\u660e\uff0c\u7531\u65bc\u5176\u7368\u7279\u7684\u9810\u8a13\u7df4\u904e\u7a0b\uff0cRETRO \u6a21\u578b\u5728\u96f6\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\u512a\u65bc GPT \u6a21\u578b\uff0c\u4f46 GPT \u6a21\u578b\u5728 PEFT \u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u6548\u80fd\u6f5b\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c8B \u53c3\u6578\u6a21\u578b\u5728\u6210\u672c\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u800c P-tuning \u843d\u5f8c\u65bc\u5176\u4ed6 PEFT \u6280\u8853\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u4f9b\u4e86\u5c07 PEFT \u61c9\u7528\u65bc\u6307\u4ee4\u5fae\u8abf RETRO \u6a21\u578b\u548c\u57fa\u790e RETRO \u6a21\u578b\u4e4b\u9593\u7684\u6bd4\u8f03\u5206\u6790\u3002\u9019\u9805\u5de5\u4f5c\u9996\u6b21\u5c0d\u6574\u5408 RAG \u7684\u5404\u7a2e PEFT \u65b9\u6cd5\u9032\u884c\u4e86\u5168\u9762\u6bd4\u8f03\uff0c\u4e26\u61c9\u7528\u65bc GPT \u548c RETRO \u6a21\u578b\uff0c\u7a81\u51fa\u4e86\u5b83\u5011\u7684\u76f8\u5c0d\u6548\u80fd\u3002", "author": "Aleksander Ficek et.al.", "authors": "Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev", "id": "2407.04528v1", "paper_url": "http://arxiv.org/abs/2407.04528v1", "repo": "null"}}