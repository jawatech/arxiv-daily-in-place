{"2411.04549": {"publish_time": "2024-11-07", "title": "Vision Language Models are In-Context Value Learners", "paper_summary": "Predicting temporal progress from visual trajectories is important for\nintelligent robots that can learn, adapt, and improve. However, learning such\nprogress estimator, or temporal value function, across different tasks and\ndomains requires both a large amount of diverse data and methods which can\nscale and generalize. To address these challenges, we present Generative Value\nLearning (\\GVL), a universal value function estimator that leverages the world\nknowledge embedded in vision-language models (VLMs) to predict task progress.\nNaively asking a VLM to predict values for a video sequence performs poorly due\nto the strong temporal correlation between successive frames. Instead, GVL\nposes value estimation as a temporal ordering problem over shuffled video\nframes; this seemingly more challenging task encourages VLMs to more fully\nexploit their underlying semantic and temporal grounding capabilities to\ndifferentiate frames based on their perceived task progress, consequently\nproducing significantly better value predictions. Without any robot or task\nspecific training, GVL can in-context zero-shot and few-shot predict effective\nvalues for more than 300 distinct real-world tasks across diverse robot\nplatforms, including challenging bimanual manipulation tasks. Furthermore, we\ndemonstrate that GVL permits flexible multi-modal in-context learning via\nexamples from heterogeneous tasks and embodiments, such as human videos. The\ngenerality of GVL enables various downstream applications pertinent to\nvisuomotor policy learning, including dataset filtering, success detection, and\nadvantage-weighted regression -- all without any model training or finetuning.", "paper_summary_zh": "\u9810\u6e2c\u8996\u89ba\u8ecc\u8de1\u7684\u6642\u9593\u9032\u5ea6\u5c0d\u65bc\u80fd\u5b78\u7fd2\u3001\u9069\u61c9\u548c\u6539\u9032\u7684\u667a\u6167\u578b\u6a5f\u5668\u4eba\u800c\u8a00\u5341\u5206\u91cd\u8981\u3002\u7136\u800c\uff0c\u5b78\u7fd2\u6b64\u985e\u9032\u5ea6\u4f30\u8a08\u5668\u6216\u6642\u9593\u50f9\u503c\u51fd\u6578\uff0c\u5728\u4e0d\u540c\u7684\u4efb\u52d9\u548c\u9818\u57df\u4e2d\u9700\u8981\u5927\u91cf\u591a\u6a23\u5316\u7684\u8cc7\u6599\u548c\u53ef\u64f4\u5145\u4e14\u53ef\u6982\u62ec\u7684\u65b9\u6cd5\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u751f\u6210\u5f0f\u50f9\u503c\u5b78\u7fd2 (\\GVL)\uff0c\u9019\u662f\u4e00\u500b\u901a\u7528\u50f9\u503c\u51fd\u6578\u4f30\u8a08\u5668\uff0c\u5b83\u5229\u7528\u5d4c\u5165\u5728\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u4e16\u754c\u77e5\u8b58\u4f86\u9810\u6e2c\u4efb\u52d9\u9032\u5ea6\u3002\u5929\u771f\u5730\u8981\u6c42 VLM \u9810\u6e2c\u5f71\u7247\u5e8f\u5217\u7684\u50f9\u503c\u6703\u8868\u73fe\u4e0d\u4f73\uff0c\u56e0\u70ba\u9023\u7e8c\u5e40\u4e4b\u9593\u6709\u5f88\u5f37\u7684\u6642\u9593\u76f8\u95dc\u6027\u3002\u76f8\u53cd\u5730\uff0cGVL \u5c07\u50f9\u503c\u4f30\u8a08\u8a2d\u5b9a\u70ba\u6253\u4e82\u5f71\u7247\u5e40\u7684\u6642\u5e8f\u6392\u5e8f\u554f\u984c\uff1b\u9019\u500b\u770b\u4f3c\u66f4\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\u9f13\u52f5 VLM \u66f4\u5145\u5206\u5730\u5229\u7528\u5176\u5e95\u5c64\u8a9e\u7fa9\u548c\u6642\u5e8f\u57fa\u790e\u529f\u80fd\uff0c\u6839\u64da\u611f\u77e5\u4efb\u52d9\u9032\u5ea6\u4f86\u5340\u5206\u5e40\uff0c\u5f9e\u800c\u7522\u751f\u986f\u8457\u66f4\u597d\u7684\u50f9\u503c\u9810\u6e2c\u3002GVL \u7121\u9700\u4efb\u4f55\u6a5f\u5668\u4eba\u6216\u4efb\u52d9\u7279\u5b9a\u8a13\u7df4\uff0c\u5373\u53ef\u5728\u60c5\u5883\u4e2d\u9032\u884c\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u91cf\u5b78\u7fd2\uff0c\u9810\u6e2c\u8d85\u904e 300 \u500b\u8de8\u8d8a\u4e0d\u540c\u6a5f\u5668\u4eba\u5e73\u53f0\u7684\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u7684\u6709\u6548\u50f9\u503c\uff0c\u5305\u62ec\u5177\u6709\u6311\u6230\u6027\u7684\u96d9\u624b\u64cd\u4f5c\u4efb\u52d9\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e GVL \u5141\u8a31\u900f\u904e\u7570\u8cea\u4efb\u52d9\u548c\u5177\u9ad4\u5be6\u4f8b\uff08\u4f8b\u5982\u4eba\u985e\u5f71\u7247\uff09\u4e2d\u7684\u7bc4\u4f8b\u9032\u884c\u9748\u6d3b\u7684\u591a\u6a21\u5f0f\u60c5\u5883\u5b78\u7fd2\u3002GVL \u7684\u666e\u904d\u6027\u652f\u63f4\u5404\u7a2e\u8207\u8996\u52d5\u904b\u52d5\u7b56\u7565\u5b78\u7fd2\u76f8\u95dc\u7684\u4e0b\u6e38\u61c9\u7528\uff0c\u5305\u62ec\u8cc7\u6599\u96c6\u904e\u6ffe\u3001\u6210\u529f\u6aa2\u6e2c\u548c\u512a\u52e2\u52a0\u6b0a\u56de\u6b78\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u4e0d\u9700\u8981\u4efb\u4f55\u6a21\u578b\u8a13\u7df4\u6216\u5fae\u8abf\u3002", "author": "Yecheng Jason Ma et.al.", "authors": "Yecheng Jason Ma, Joey Hejna, Ayzaan Wahid, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, Jonathan Tompson, Osbert Bastani, Dinesh Jayaraman, Wenhao Yu, Tingnan Zhang, Dorsa Sadigh, Fei Xia", "id": "2411.04549v1", "paper_url": "http://arxiv.org/abs/2411.04549v1", "repo": "null"}}