{"2412.07174": {"publish_time": "2024-12-10", "title": "Post-Training Statistical Calibration for Higher Activation Sparsity", "paper_summary": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training\nactivation pruning framework that (1) generalizes sparsification by input\nactivations of Fully-Connected layers for generic and flexible application\nacross Transformers, and (2) features a simple Mode-Centering technique to\npre-calibrate activation distributions for maximizing post-training sparsity.\nOur results demonstrate robust Pareto efficiency compared to prior methods,\ntranslating to a 1.5x additional LLM decoding speedup against CATS at iso model\nquality. SCAP effectiveness is empirically verified across a wide range of\nmodels, including recent Transformer Decoders, MoE, Mamba2, Encoding\nTransformer, and pre-quantized models, highlighting its practicality and\nscalability. The code is available at: https://github.com/IntelLabs/SCAP.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u7d71\u8a08\u6821\u6e96\u7684\u6fc0\u6d3b\u526a\u679d (SCAP)\uff0c\u9019\u662f\u4e00\u500b\u8a13\u7df4\u5f8c\u6fc0\u6d3b\u526a\u679d\u67b6\u69cb\uff0c\u5b83 (1) \u900f\u904e\u5168\u9023\u63a5\u5c64\u7684\u8f38\u5165\u6fc0\u6d3b\uff0c\u5c07\u7a00\u758f\u5316\u6982\u62ec\u5316\uff0c\u4ee5\u5728 Transformers \u4e2d\u9032\u884c\u901a\u7528\u4e14\u9748\u6d3b\u7684\u61c9\u7528\uff0c\u4e26 (2) \u5177\u5099\u4e00\u500b\u7c21\u55ae\u7684\u6a21\u5f0f\u4e2d\u5fc3\u5316\u6280\u8853\uff0c\u7528\u65bc\u9810\u5148\u6821\u6e96\u6fc0\u6d3b\u5206\u4f48\uff0c\u4ee5\u6700\u5927\u5316\u8a13\u7df4\u5f8c\u7684\u7a00\u758f\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u8207\u5148\u524d\u7684\u6280\u8853\u76f8\u6bd4\uff0c\u5177\u6709\u5f37\u5927\u7684 Pareto \u6548\u7387\uff0c\u8f49\u5316\u70ba\u5728 iso \u6a21\u578b\u54c1\u8cea\u4e0b\uff0c\u8207 CATS \u76f8\u6bd4\uff0cLLM \u89e3\u78bc\u901f\u5ea6\u63d0\u5347\u4e86 1.5 \u500d\u3002SCAP \u7684\u6709\u6548\u6027\u5df2\u5728\u5ee3\u6cdb\u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5be6\u8b49\u9a57\u8b49\uff0c\u5305\u62ec\u6700\u8fd1\u7684 Transformer \u89e3\u78bc\u5668\u3001MoE\u3001Mamba2\u3001\u7de8\u78bc Transformer \u548c\u9810\u91cf\u5316\u6a21\u578b\uff0c\u7a81\u986f\u4e86\u5b83\u7684\u5be6\u7528\u6027\u548c\u53ef\u64f4\u5145\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/IntelLabs/SCAP \u53d6\u5f97\u3002", "author": "Vui Seng Chua et.al.", "authors": "Vui Seng Chua, Yujie Pan, Nilesh Jain", "id": "2412.07174v1", "paper_url": "http://arxiv.org/abs/2412.07174v1", "repo": "https://github.com/intellabs/scap"}}