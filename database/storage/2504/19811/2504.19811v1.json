{"2504.19811": {"publish_time": "2025-04-28", "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "paper_summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.", "paper_summary_zh": "<paragraph>\u5728\u9032\u884c\u5927\u91cf\u7684\u5fae\u8abf\u6216\u5408\u4f75\u4e4b\u524d\uff0c\u6e96\u78ba\u9810\u6e2c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u53ef\u4ee5\u5927\u5e45\u964d\u4f4e\u8a08\u7b97\u6210\u672c\u548c\u958b\u767c\u6642\u9593\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u65b9\u6cd5\uff08\u5982\u7e2e\u653e\u5b9a\u5f8b\uff09\u8003\u616e\u4e86\u53c3\u6578\u5927\u5c0f\u6216\u8a13\u7df4token\u7b49\u5168\u57df\u56e0\u7d20\uff0c\u4f46\u5b83\u5011\u5f80\u5f80\u5ffd\u7565\u4e86\u660e\u78ba\u7684\u8b5c\u7cfb\u95dc\u4fc2\u2014\u2014\u4ea6\u5373\u54ea\u4e9b\u6a21\u578b\u662f\u5f9e\u54ea\u4e9b\u7236\u6a21\u578b\u884d\u751f\u6216\u5408\u4f75\u800c\u4f86\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u8b5c\u7cfb\u6b63\u5247\u5316\u77e9\u9663\u5206\u89e3 (LRMF) \u67b6\u69cb\uff0c\u5b83\u900f\u904e\u5716\u62c9\u666e\u62c9\u65af\u6b63\u5247\u5316\u5668\u5c0d LLM \u4e4b\u9593\u7684\u7956\u5148\u95dc\u4fc2\u9032\u884c\u7de8\u78bc\u3002\u85c9\u7531\u5229\u7528\u591a\u8df3\u7236\u5b50\u9023\u63a5\uff0cLRMF \u5728\u5be6\u4f8b\u7d1a\u548c\u57fa\u6e96\u7d1a\u7684\u6548\u80fd\u9810\u6e2c\u65b9\u9762\u59cb\u7d42\u512a\u65bc\u50b3\u7d71\u7684\u77e9\u9663\u5206\u89e3\u548c\u5354\u540c\u904e\u6ffe\u65b9\u6cd5\u3002\u6211\u5011\u7684\u5927\u898f\u6a21\u7814\u7a76\u5305\u62ec 2,934 \u500b\u516c\u958b\u53ef\u7528\u7684 Hugging Face \u6a21\u578b\u548c\u8de8 6 \u500b\u4e3b\u8981\u57fa\u6e96\u7684 21,000 \u591a\u500b\u5be6\u4f8b\uff0c\u8868\u660e\u8207\u5be6\u969b\u6548\u80fd\u76f8\u6bd4\uff0c\u8b5c\u7cfb\u7d04\u675f\u7684\u76f8\u95dc\u6027\u6bd4\u57fa\u6e96\u9ad8\u51fa 7-10 \u500b\u767e\u5206\u9ede\u3002\u6b64\u5916\uff0cLRMF \u6709\u6548\u5730\u89e3\u6c7a\u4e86\u51b7\u555f\u52d5\u554f\u984c\uff0c\u5373\u4f7f\u6578\u64da\u6975\u5c11\uff0c\u4e5f\u80fd\u70ba\u65b0\u884d\u751f\u6216\u5408\u4f75\u7684\u6a21\u578b\u63d0\u4f9b\u6e96\u78ba\u7684\u4f30\u8a08\u3002\u56e0\u6b64\uff0c\u9019\u7a2e\u8b5c\u7cfb\u5f15\u5c0e\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u7a2e\u8cc7\u6e90\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u73fe\u4ee3 LLM \u958b\u767c\u4e2d\u70ba\u8d85\u53c3\u6578\u8abf\u6574\u3001\u6578\u64da\u9078\u64c7\u548c\u6a21\u578b\u7d44\u5408\u63d0\u4f9b\u53c3\u8003\u4f9d\u64da\u3002</paragraph>\n", "author": "Takuya Tamura et.al.", "authors": "Takuya Tamura, Taro Yano, Masafumi Enomoto, Masafumi Oyamada", "id": "2504.19811v1", "paper_url": "http://arxiv.org/abs/2504.19811v1", "repo": "null"}}