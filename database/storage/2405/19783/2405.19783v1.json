{"2405.19783": {"publish_time": "2024-05-30", "title": "Instruction-Guided Visual Masking", "paper_summary": "Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code is available at\nhttps://github.com/2toinf/IVM.", "paper_summary_zh": "\u5728\u7576\u4ee3 LLM \u4e2d\uff0c\u6307\u4ee4\u9075\u5faa\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u7576\u64f4\u5c55\u5230\u591a\u6a21\u614b\u8a2d\u7f6e\u6642\uff0c\u5b83\u901a\u5e38\u6703\u5728\u5177\u9ad4\u6587\u672c\u6307\u4ee4\u548c\u5716\u50cf\u7684\u76ee\u6a19\u5c40\u90e8\u5340\u57df\u4e4b\u9593\u51fa\u73fe\u932f\u4f4d\u3002\u70ba\u4e86\u5be6\u73fe\u66f4\u6e96\u78ba\u548c\u7d30\u5fae\u7684\u7684\u591a\u6a21\u614b\u6307\u4ee4\u9075\u5faa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6307\u4ee4\u5f15\u5c0e\u8996\u89ba\u906e\u7f69 (IVM)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u901a\u7528\u8996\u89ba\u63a5\u5730\u6a21\u578b\uff0c\u8207\u591a\u7a2e\u591a\u6a21\u614b\u6a21\u578b\uff08\u4f8b\u5982 LMM \u548c\u6a5f\u5668\u4eba\u6a21\u578b\uff09\u76f8\u5bb9\u3002\u901a\u904e\u70ba\u8207\u6307\u4ee4\u7121\u95dc\u7684\u5340\u57df\u69cb\u5efa\u8996\u89ba\u906e\u7f69\uff0c\u589e\u5f37 IVM \u7684\u591a\u6a21\u614b\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u95dc\u6ce8\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u5716\u50cf\u5340\u57df\uff0c\u4ee5\u66f4\u597d\u5730\u8207\u8907\u96dc\u7684\u6307\u4ee4\u4fdd\u6301\u4e00\u81f4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u8996\u89ba\u906e\u7f69\u6578\u64da\u751f\u6210\u7ba1\u9053\uff0c\u4e26\u5275\u5efa\u4e86\u4e00\u500b\u5305\u542b 100 \u842c\u5f35\u5716\u50cf\u6307\u4ee4\u5c0d\u7684 IVM-Mix-1M \u6578\u64da\u96c6\u3002\u6211\u5011\u9032\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u5b78\u7fd2\u6280\u8853\uff0c\u5373\u5224\u5225\u5668\u52a0\u6b0a\u76e3\u7763\u5b78\u7fd2 (DWSL)\uff0c\u7528\u65bc\u512a\u5148\u8003\u616e\u9ad8\u8cea\u91cf\u6578\u64da\u6a23\u672c\u7684 IVM \u512a\u5148\u8a13\u7df4\u3002\u5728\u901a\u7528\u591a\u6a21\u614b\u4efb\u52d9\uff08\u4f8b\u5982 VQA \u548c\u5177\u8eab\u6a5f\u5668\u4eba\u63a7\u5236\uff09\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 IVM \u7684\u591a\u529f\u80fd\u6027\uff0c\u4f5c\u70ba\u4e00\u7a2e\u5373\u63d2\u5373\u7528\u5de5\u5177\uff0c\u5b83\u986f\u8457\u63d0\u5347\u4e86\u591a\u7a2e\u591a\u6a21\u614b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u591a\u6a21\u614b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7522\u751f\u4e86\u65b0\u7684\u6700\u5148\u9032\u7684\u7d50\u679c\u3002\u4ee3\u78bc\u53ef\u5728 https://github.com/2toinf/IVM \u4e0a\u7372\u5f97\u3002", "author": "Jinliang Zheng et.al.", "authors": "Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan", "id": "2405.19783v1", "paper_url": "http://arxiv.org/abs/2405.19783v1", "repo": "null"}}