{"2407.05700": {"publish_time": "2024-07-08", "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct", "paper_summary": "Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation.", "paper_summary_zh": "<paragraph>\u958b\u653e\u539f\u59cb\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8fd1\u671f\u9032\u5c55\u5df2\u900f\u904e\u5c0d GPT-3.5 \u548c GPT-4 \u7b49\u5f37\u5927\u9589\u6e90 LLM \u6240\u7522\u751f\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u7de8\u78bc\u80fd\u529b\u4ee5\u9032\u884c\u6307\u4ee4\u5fae\u8abf\u3002\u672c\u6587\u63a2\u8a0e\u5982\u4f55\u900f\u904e\u7522\u751f\u8cc7\u6599\u672c\u8eab\uff0c\u800c\u975e\u67e5\u8a62\u9589\u6e90 LLM\uff0c\u9032\u4e00\u6b65\u6539\u5584\u6307\u4ee4\u5fae\u8abf\u7684\u7a0b\u5f0f\u78bc LLM\u3002\u6211\u5011\u7684\u95dc\u9375\u89c0\u5bdf\u662f\u6b63\u5f0f\u8a9e\u8a00\u548c\u975e\u6b63\u5f0f\u8a9e\u8a00\u7684\u7ffb\u8b6f\u4e4b\u9593\u51fa\u73fe\u932f\u4f4d\uff1a\u5c07\u6b63\u5f0f\u8a9e\u8a00\uff08\u5373\u7a0b\u5f0f\u78bc\uff09\u7ffb\u8b6f\u6210\u975e\u6b63\u5f0f\u8a9e\u8a00\uff08\u5373\u81ea\u7136\u8a9e\u8a00\uff09\u6bd4\u53cd\u904e\u4f86\u66f4\u70ba\u76f4\u63a5\u3002\u6839\u64da\u6b64\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa INVERSE-INSTRUCT\uff0c\u5b83\u6703\u6458\u8981\u7a0b\u5f0f\u78bc\u7247\u6bb5\u4e2d\u7684\u6307\u4ee4\uff0c\u800c\u975e\u53cd\u904e\u4f86\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u7d66\u5b9a\u7a0b\u5f0f\u78bc\u7684\u6307\u4ee4\u5fae\u8abf\u8a9e\u6599\u5eab\u4ee5\u53ca\u7522\u751f\u7684\u6307\u4ee4\u5fae\u8abf\u7a0b\u5f0f\u78bc LLM\uff0c\u6211\u5011\u8981\u6c42\u7a0b\u5f0f\u78bc LLM \u900f\u904e\u7a0b\u5f0f\u78bc\u6458\u8981\u548c\u81ea\u6211\u8a55\u4f30\u70ba\u539f\u59cb\u8a9e\u6599\u5eab\u7522\u751f\u984d\u5916\u7684\u512a\u8cea\u6307\u4ee4\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d\u57fa\u672c LLM \u9032\u884c\u5fae\u8abf\uff0c\u7d50\u5408\u539f\u59cb\u8a9e\u6599\u5eab\u548c\u81ea\u6211\u7522\u751f\u7684\u8a9e\u6599\u5eab\uff0c\u9019\u6703\u7522\u751f\u66f4\u5f37\u5927\u7684\u6307\u4ee4\u5fae\u8abf LLM\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u540d\u70ba InverseCoder \u7684\u7a0b\u5f0f\u78bc LLM\uff0c\u5176\u5728\u5ee3\u6cdb\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8d85\u8d8a\u4e86\u539f\u59cb\u7a0b\u5f0f\u78bc LLM \u7684\u6548\u80fd\uff0c\u5305\u62ec Python \u6587\u5b57\u8f49\u7a0b\u5f0f\u78bc\u7522\u751f\u3001\u591a\u8a9e\u8a00\u7de8\u78bc\u548c\u8cc7\u6599\u79d1\u5b78\u7a0b\u5f0f\u78bc\u7522\u751f\u3002</paragraph>", "author": "Yutong Wu et.al.", "authors": "Yutong Wu, Di Huang, Wenxuan Shi, Wei Wang, Lingzhe Gao, Shihao Liu, Ziyuan Nan, Kaizhao Yuan, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Yewen Pu, Dawei Yin, Xing Hu, Yunji Chen", "id": "2407.05700v1", "paper_url": "http://arxiv.org/abs/2407.05700v1", "repo": "null"}}