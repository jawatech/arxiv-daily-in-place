{"2406.17753": {"publish_time": "2024-06-25", "title": "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language", "paper_summary": "We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive text - both when explicitly instructed to rewrite text\nto be more or less persuasive and when only instructed to paraphrase. To this\nend, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of\na short text and of a text rewritten by an LLM to amplify or diminish\npersuasive language. We multi-annotate the pairs on a relative scale for\npersuasive language. This data is not only a valuable resource in itself, but\nwe also show that it can be used to train a regression model to predict a score\nof persuasive language between text pairs. This model can score and benchmark\nnew LLMs across domains, thereby facilitating the comparison of different LLMs.\nFinally, we discuss effects observed for different system prompts. Notably, we\nfind that different 'personas' in the system prompt of LLaMA3 change the\npersuasive language in the text substantially, even when only instructed to\nparaphrase. These findings underscore the importance of investigating\npersuasive language in LLM generated text.", "paper_summary_zh": "<paragraph>\u6211\u5011\u6703\u63a5\u89f8\u5230\u8a31\u591a\u8a66\u5716\u5f71\u97ff\u6211\u5011\u7684\u8cc7\u8a0a\uff0c\u4f8b\u5982\u9810\u544a\u8a0a\u606f\u3001\u8faf\u8ad6\u3001\u653f\u6cbb\u6846\u67b6\u65b0\u805e\u548c\u5ba3\u50b3\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u4f7f\u7528\u5177\u8aaa\u670d\u529b\u7684\u8a9e\u8a00\u3002\u96a8\u8457\u6700\u8fd1\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8208\u8da3\uff0c\u6211\u5011\u7814\u7a76\u4e86 LLM \u7522\u751f\u5177\u8aaa\u670d\u529b\u6587\u5b57\u7684\u80fd\u529b\u3002\u8207\u5c08\u6ce8\u65bc\u7279\u5b9a\u9818\u57df\u6216\u8aaa\u670d\u985e\u578b\u7684\u5148\u524d\u7814\u7a76\u76f8\u53cd\uff0c\u6211\u5011\u5728\u5404\u7a2e\u9818\u57df\u9032\u884c\u4e86\u4e00\u9805\u4e00\u822c\u6027\u7814\u7a76\uff0c\u4ee5\u8861\u91cf\u548c\u57fa\u6e96 LLM \u7522\u751f\u5177\u8aaa\u670d\u529b\u6587\u5b57\u7684\u7a0b\u5ea6\uff0c\u7121\u8ad6\u662f\u5728\u660e\u78ba\u6307\u793a\u6539\u5beb\u6587\u5b57\u4ee5\u4f7f\u5176\u66f4\u5177\u8aaa\u670d\u529b\u6216\u66f4\u4e0d\u5177\u8aaa\u670d\u529b\u6642\uff0c\u6216\u50c5\u6307\u793a\u9032\u884c\u540c\u7fa9\u6539\u5beb\u6642\u3002\u70ba\u6b64\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u65b0\u8cc7\u6599\u96c6 Persuasive-Pairs\uff0c\u5176\u4e2d\u6bcf\u4e00\u5c0d\u90fd\u5305\u542b\u4e00\u6bb5\u7c21\u77ed\u6587\u5b57\u548c\u4e00\u6bb5\u7531 LLM \u6539\u5beb\u7684\u6587\u5b57\uff0c\u4ee5\u64f4\u5927\u6216\u6e1b\u5c11\u5177\u8aaa\u670d\u529b\u7684\u8a9e\u8a00\u3002\u6211\u5011\u4ee5\u76f8\u5c0d\u91cf\u8868\u5c0d\u9019\u4e9b\u5c0d\u9032\u884c\u591a\u91cd\u8a3b\u89e3\uff0c\u4ee5\u8868\u793a\u5177\u8aaa\u670d\u529b\u7684\u8a9e\u8a00\u3002\u9019\u4e9b\u8cc7\u6599\u672c\u8eab\u4e0d\u50c5\u662f\u4e00\u500b\u6709\u50f9\u503c\u7684\u8cc7\u6e90\uff0c\u6211\u5011\u9084\u5c55\u793a\u4e86\u5b83\u53ef\u7528\u65bc\u8a13\u7df4\u56de\u6b78\u6a21\u578b\uff0c\u4ee5\u9810\u6e2c\u6587\u672c\u5c0d\u4e4b\u9593\u5177\u8aaa\u670d\u529b\u7684\u8a9e\u8a00\u5206\u6578\u3002\u6b64\u6a21\u578b\u53ef\u4ee5\u5728\u5404\u500b\u9818\u57df\u5c0d\u65b0\u7684 LLM \u9032\u884c\u8a55\u5206\u548c\u57fa\u6e96\u6e2c\u8a66\uff0c\u5f9e\u800c\u4fc3\u9032\u5c0d\u4e0d\u540c LLM \u7684\u6bd4\u8f03\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u5c0d\u4e0d\u540c\u7cfb\u7d71\u63d0\u793a\u89c0\u5bdf\u5230\u7684\u5f71\u97ff\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe LLaMA3 \u7cfb\u7d71\u63d0\u793a\u4e2d\u7684\u4e0d\u540c\u300c\u89d2\u8272\u300d\u6703\u5927\u5e45\u6539\u8b8a\u6587\u5b57\u4e2d\u7684\u5177\u8aaa\u670d\u529b\u8a9e\u8a00\uff0c\u5373\u4f7f\u50c5\u6307\u793a\u9032\u884c\u540c\u7fa9\u6539\u5beb\u6642\u4e5f\u662f\u5982\u6b64\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\u8abf\u67e5 LLM \u751f\u6210\u7684\u6587\u5b57\u4e2d\u5177\u8aaa\u670d\u529b\u8a9e\u8a00\u7684\u91cd\u8981\u6027\u3002</paragraph>", "author": "Amalie Brogaard Pauli et.al.", "authors": "Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent", "id": "2406.17753v1", "paper_url": "http://arxiv.org/abs/2406.17753v1", "repo": "null"}}