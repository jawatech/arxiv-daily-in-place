{"2502.15304": {"publish_time": "2025-02-21", "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention", "paper_summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.", "paper_summary_zh": "\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6709\u6548\u63a8\u8ad6\uff0c\u9375\u503c (KV) \u5feb\u53d6\u7684\u6709\u6548\u58d3\u7e2e\u81f3\u95dc\u91cd\u8981\u3002\u5df2\u7d93\u627e\u51fa\u4e09\u7a2e\u4e3b\u8981\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u6280\u8853\u985e\u578b\uff0c\u5373\u7a00\u758f\u6027\u3001\u901a\u9053\u58d3\u7e2e\u548c\u91cf\u5316\u3002\u672c\u7814\u7a76\u63d0\u51fa SVDq\uff0c\u4e00\u7a2e\u57fa\u65bc\u5947\u7570\u503c\u5206\u89e3 (SVD) \u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u65bc K \u5feb\u53d6\u3002\u6700\u521d\uff0cK \u5feb\u53d6\u4f7f\u7528 SVD \u57fa\u5e95\u8868\u793a\u8f49\u63db\u70ba\u6f5b\u5728\u901a\u9053\u3002\u7531\u65bc\u6f5b\u5728\u901a\u9053\u4e2d\u7684\u503c\u8870\u6e1b\u5f88\u5feb\uff0c\u4e26\u4e14\u50c5\u5728\u5e7e\u500b\u6f5b\u5728\u901a\u9053\u5f8c\u8b8a\u5f97\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\uff0c\u56e0\u6b64\u6211\u5011\u7684\u6a21\u578b\u63a5\u8457\u70ba\u6f5b\u5728\u901a\u9053\u7d0d\u5165\u91cd\u8981\u6027\u611f\u77e5\u91cf\u5316\u548c\u58d3\u7e2e\u3002\u9019\u4f7f\u5f97\u80fd\u5920\u6709\u6548\u5730\u5c07\u66f4\u9ad8\u7684\u7cbe\u5ea6\u5206\u914d\u7d66\u66f4\u91cd\u8981\u7684\u901a\u9053\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u8b49\u660e SVDq \u5c0e\u81f4\u7684\u91cf\u5316\u8aa4\u5dee (x0.1 \u751a\u81f3\u66f4\u4f4e) \u9060\u4f4e\u65bc\u539f\u59cb\u7a7a\u9593\u4e2d\u7684\u9010\u901a\u9053\u9375\u91cf\u5316\u3002\u6211\u5011\u57fa\u65bc RULER \u548c LongBench \u57fa\u6e96\u7684\u767c\u73fe\u8868\u660e\uff0cSVDq \u53ef\u4ee5\u5be6\u73fe\u4f4e\u81f3 1.25 \u4f4d\u7684\u7b49\u6548\u9375\u5feb\u53d6\u7cbe\u5ea6\u3002\u7576\u8207\u9375\u7a00\u758f\u6027\u7d50\u5408\u4f7f\u7528\u6642\uff0c\u5b83\u53ef\u4ee5\u9054\u5230\u9ad8\u9054 410 \u500d\u7684\u9375\u58d3\u7e2e\u6bd4\uff0c\u540c\u6642\u4fdd\u6301\u53ef\u6bd4\u8f03\u7684\u6a21\u578b\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c0d\u65bc LongBench \u8cc7\u6599\u96c6\u5e7e\u4e4e\u6c92\u6709\u640d\u5931\u3002\u9019\u8868\u660e SVDq \u80fd\u5920\u9032\u884c\u9ad8\u7cbe\u5ea6\u4f4e\u4f4d\u5143\u91cf\u5316\uff0c\u70ba LLM \u4e2d\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Hong Yankun et.al.", "authors": "Hong Yankun, Li Xing, Zhen Hui-Ling, Yu Xianzhi, Liu Wulong, Yuan Mingxuan", "id": "2502.15304v1", "paper_url": "http://arxiv.org/abs/2502.15304v1", "repo": "null"}}