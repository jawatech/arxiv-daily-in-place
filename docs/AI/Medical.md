
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-07**|**"It Felt Like I Was Left in the Dark": Exploring Information Needs and Design Opportunities for Family Caregivers of Older Adult Patients in Critical Care Settings**|Shihan Fu et.al.|[2502.05115v1](http://arxiv.org/abs/2502.05115v1)|null|
|**2025-02-07**|**Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs**|Thierry Bossy et.al.|[2502.05087v1](http://arxiv.org/abs/2502.05087v1)|null|
|**2025-02-07**|**MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin**|Minrui Chen et.al.|[2502.04794v1](http://arxiv.org/abs/2502.04794v1)|null|
|**2025-02-06**|**MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification**|Wei Fan et.al.|[2502.04515v1](http://arxiv.org/abs/2502.04515v1)|null|
|**2025-02-06**|**Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions**|Khushboo Verma et.al.|[2502.04423v1](http://arxiv.org/abs/2502.04423v1)|null|
|**2025-02-06**|**Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation**|Tewele W. Tareke et.al.|[2502.04083v1](http://arxiv.org/abs/2502.04083v1)|null|
|**2025-02-06**|**Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization**|Ran Song et.al.|[2502.04034v1](http://arxiv.org/abs/2502.04034v1)|null|
|**2025-02-06**|**MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot**|Xuejiao Zhao et.al.|[2502.04413v1](http://arxiv.org/abs/2502.04413v1)|null|
|**2025-02-06**|**Transforming Multimodal Models into Action Models for Radiotherapy**|Matteo Ferrante et.al.|[2502.04408v1](http://arxiv.org/abs/2502.04408v1)|null|
|**2025-02-06**|**Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning**|Bokeng Zheng et.al.|[2502.04399v1](http://arxiv.org/abs/2502.04399v1)|null|
|**2025-02-06**|**Multimodal Medical Code Tokenizer**|Xiaorui Su et.al.|[2502.04397v1](http://arxiv.org/abs/2502.04397v1)|null|
|**2025-02-06**|**A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma**|Chaoyin She et.al.|[2502.03772v1](http://arxiv.org/abs/2502.03772v1)|[link](https://github.com/Asunatan/HSQformer)|
|**2025-02-05**|**Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings**|Guangyao Zheng et.al.|[2502.04386v1](http://arxiv.org/abs/2502.04386v1)|null|
|**2025-02-05**|**Clinically-Inspired Hierarchical Multi-Label Classification of Chest X-rays with a Penalty-Based Loss Function**|Mehrdad Asadi et.al.|[2502.03591v1](http://arxiv.org/abs/2502.03591v1)|[link](https://github.com/the-mercury/CIHMLC)|
|**2025-02-05**|**Code Simulation as a Proxy for High-order Tasks in Large Language Models**|Emanuele La Malfa et.al.|[2502.03568v1](http://arxiv.org/abs/2502.03568v1)|null|
|**2025-02-05**|**Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning**|Jonathan Kim et.al.|[2502.04381v1](http://arxiv.org/abs/2502.04381v1)|null|
|**2025-02-05**|**Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**|Sarah Al-Shareeda et.al.|[2502.03396v1](http://arxiv.org/abs/2502.03396v1)|null|
|**2025-02-05**|**RadVLM: A Multitask Conversational Vision-Language Model for Radiology**|Nicolas Deperrois et.al.|[2502.03333v1](http://arxiv.org/abs/2502.03333v1)|null|
|**2025-02-05**|**MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**|Amin Dada et.al.|[2502.03298v1](http://arxiv.org/abs/2502.03298v1)|null|
|**2025-02-05**|**Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**|Matthias Schwab et.al.|[2502.03272v1](http://arxiv.org/abs/2502.03272v1)|null|
|**2025-02-05**|**Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**|Li Pan et.al.|[2502.03238v2](http://arxiv.org/abs/2502.03238v2)|null|
|**2025-02-05**|**MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**|Seonok Kim et.al.|[2502.03004v1](http://arxiv.org/abs/2502.03004v1)|null|
|**2025-02-05**|**Contrastive Token-level Explanations for Graph-based Rumour Detection**|Daniel Wai Kit Chin et.al.|[2502.04366v1](http://arxiv.org/abs/2502.04366v1)|null|
|**2025-02-05**|**AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth**|Jorge Garc√≠a-Torres et.al.|[2502.04365v1](http://arxiv.org/abs/2502.04365v1)|null|
|**2025-02-04**|**3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**|Weicheng Zhu et.al.|[2502.02779v1](http://arxiv.org/abs/2502.02779v1)|null|
|**2025-02-04**|**Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**|Obed Korshie Dzikunu et.al.|[2502.02756v1](http://arxiv.org/abs/2502.02756v1)|null|
|**2025-02-04**|**MedRAX: Medical Reasoning Agent for Chest X-ray**|Adibvafa Fallahpour et.al.|[2502.02673v1](http://arxiv.org/abs/2502.02673v1)|null|
|**2025-02-04**|**Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription**|Mahdi Alkaeed et.al.|[2502.04356v1](http://arxiv.org/abs/2502.04356v1)|null|
|**2025-02-04**|**Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**|Shayan Kiyani et.al.|[2502.02561v1](http://arxiv.org/abs/2502.02561v1)|null|
|**2025-02-04**|**A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**|Edward Ellis et.al.|[2502.02489v1](http://arxiv.org/abs/2502.02489v1)|null|
|**2025-02-04**|**Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**|Yaling Shen et.al.|[2502.02438v1](http://arxiv.org/abs/2502.02438v1)|null|
|**2025-02-04**|**Test Time Training for 4D Medical Image Interpolation**|Qikang Zhang et.al.|[2502.02341v1](http://arxiv.org/abs/2502.02341v1)|[link](https://github.com/chaostheproducer/ttt4d)|
|**2025-02-04**|**Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**|Atharva Mangeshkumar Agrawal et.al.|[2502.02249v1](http://arxiv.org/abs/2502.02249v1)|null|
|**2025-02-04**|**Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**|F. Xavier Gaya-Morey et.al.|[2502.02618v1](http://arxiv.org/abs/2502.02618v1)|null|
|**2025-02-04**|**Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**|Yuxiao Cheng et.al.|[2502.02109v1](http://arxiv.org/abs/2502.02109v1)|null|
|**2025-02-04**|**JingFang: A Traditional Chinese Medicine Large Language Model of Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment**|Yehan Yan et.al.|[2502.04345v1](http://arxiv.org/abs/2502.04345v1)|null|
|**2025-02-03**|**An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**|Jiazi Tian et.al.|[2502.01789v1](http://arxiv.org/abs/2502.01789v1)|null|
|**2025-02-03**|**Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate Cancer MRI Diagnosis**|Chacha Chen et.al.|[2502.03482v1](http://arxiv.org/abs/2502.03482v1)|null|
|**2025-02-03**|**Improving Transformer World Models for Data-Efficient RL**|Antoine Dedieu et.al.|[2502.01591v1](http://arxiv.org/abs/2502.01591v1)|null|
|**2025-02-03**|**Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**|Zhi Zhang et.al.|[2502.01377v1](http://arxiv.org/abs/2502.01377v1)|null|
|**2025-02-03**|**OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**|Chengfeng Zhou et.al.|[2502.01243v1](http://arxiv.org/abs/2502.01243v1)|null|
|**2025-02-03**|**MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**|Alejandro Guerra-Manzanares et.al.|[2502.01158v1](http://arxiv.org/abs/2502.01158v1)|null|
|**2025-02-03**|**Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**|Qian Chen et.al.|[2502.01141v1](http://arxiv.org/abs/2502.01141v1)|null|
|**2025-02-03**|**Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**|Mithun Saha et.al.|[2502.01108v1](http://arxiv.org/abs/2502.01108v1)|null|
|**2025-02-03**|**Tutorial on Using Machine Learning and Deep Learning Models for Mental Illness Detection**|Yeyubei Zhang et.al.|[2502.04342v1](http://arxiv.org/abs/2502.04342v1)|null|
|**2025-02-02**|**Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**|Hadas Ben-Atya et.al.|[2502.01691v1](http://arxiv.org/abs/2502.01691v1)|null|
|**2025-02-02**|**Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**|Si-Ioi Ng et.al.|[2502.01685v1](http://arxiv.org/abs/2502.01685v1)|null|
|**2025-02-02**|**Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**|Shengtian Sang et.al.|[2502.00712v1](http://arxiv.org/abs/2502.00712v1)|null|
|**2025-02-02**|**TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**|Linglong Wu et.al.|[2502.00695v1](http://arxiv.org/abs/2502.00695v1)|null|
|**2025-02-02**|**Enhanced Convolutional Neural Networks for Improved Image Classification**|Xiaoran Yang et.al.|[2502.00663v1](http://arxiv.org/abs/2502.00663v1)|null|
|**2025-02-02**|**Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**|Yujin Oh et.al.|[2502.00619v1](http://arxiv.org/abs/2502.00619v1)|null|
|**2025-02-01**|**Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**|Samiran Dey et.al.|[2502.00568v1](http://arxiv.org/abs/2502.00568v1)|[link](https://github.com/Samiran-Dey/PathoGen)|
|**2025-02-01**|**Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**|Mohammad Saleh Torkestani et.al.|[2502.00495v1](http://arxiv.org/abs/2502.00495v1)|null|
|**2025-02-01**|**Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**|Aishik Mandal et.al.|[2502.00451v1](http://arxiv.org/abs/2502.00451v1)|null|
|**2025-01-31**|**EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**|Omar H. Khater et.al.|[2502.00205v1](http://arxiv.org/abs/2502.00205v1)|null|
|**2025-01-31**|**DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**|Abdurrahim Yilmaz et.al.|[2502.00196v1](http://arxiv.org/abs/2502.00196v1)|null|
|**2025-01-31**|**Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**|Hassan Jahanandish et.al.|[2502.00146v1](http://arxiv.org/abs/2502.00146v1)|null|
|**2025-01-31**|**AIN: The Arabic INclusive Large Multimodal Model**|Ahmed Heakl et.al.|[2502.00094v2](http://arxiv.org/abs/2502.00094v2)|null|
|**2025-01-31**|**Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**|Misha P. T Kaandorp et.al.|[2501.19338v1](http://arxiv.org/abs/2501.19338v1)|null|
|**2025-01-31**|**Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**|Halil Ibrahim Aysel et.al.|[2501.19271v1](http://arxiv.org/abs/2501.19271v1)|null|
|**2025-01-31**|**Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**|Aurora Rofena et.al.|[2501.19176v1](http://arxiv.org/abs/2501.19176v1)|null|
|**2025-01-31**|**Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**|Xiangyu Sun et.al.|[2501.19086v1](http://arxiv.org/abs/2501.19086v1)|null|
|**2025-01-30**|**Survey and Improvement Strategies for Gene Prioritization with Large Language Models**|Matthew Neeley et.al.|[2501.18794v1](http://arxiv.org/abs/2501.18794v1)|null|
|**2025-01-30**|**Synthetic Data Generation for Augmenting Small Samples**|Dan Liu et.al.|[2501.18741v1](http://arxiv.org/abs/2501.18741v1)|null|
|**2025-01-30**|**A Multi-Layered Large Language Model Framework for Disease Prediction**|Malak Mohamed et.al.|[2502.00063v1](http://arxiv.org/abs/2502.00063v1)|null|
|**2025-01-30**|**A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**|Yifan Wang et.al.|[2501.18367v1](http://arxiv.org/abs/2501.18367v1)|null|
|**2025-01-30**|**MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**|Yuxin Zuo et.al.|[2501.18362v1](http://arxiv.org/abs/2501.18362v1)|null|
|**2025-01-30**|**CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**|Yicheng Wu et.al.|[2501.18328v1](http://arxiv.org/abs/2501.18328v1)|null|
|**2025-01-30**|**A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**|Shayli Farshchiha et.al.|[2501.18294v1](http://arxiv.org/abs/2501.18294v1)|null|
|**2025-01-30**|**The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**|Anup Saha et.al.|[2501.18270v1](http://arxiv.org/abs/2501.18270v1)|[link](https://github.com/iToBoS/Lesion-Detection-Challange)|
|**2025-01-30**|**Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**|Malte T√∂lle et.al.|[2501.18237v1](http://arxiv.org/abs/2501.18237v1)|null|
|**2025-01-30**|**Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**|Min Hun Lee et.al.|[2501.18108v1](http://arxiv.org/abs/2501.18108v1)|null|
|**2025-01-30**|**Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**|Pratik S. Sachdeva et.al.|[2501.18081v1](http://arxiv.org/abs/2501.18081v1)|null|
|**2025-01-30**|**Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**|Pir Bakhsh Khokhar et.al.|[2501.18071v1](http://arxiv.org/abs/2501.18071v1)|null|
|**2025-01-29**|**Current Pathology Foundation Models are unrobust to Medical Center Differences**|Edwin D. de Jong et.al.|[2501.18055v2](http://arxiv.org/abs/2501.18055v2)|null|
|**2025-01-29**|**Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**|Zijie Liu et.al.|[2501.17860v1](http://arxiv.org/abs/2501.17860v1)|null|
|**2025-01-29**|**GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**|Ziang Liu et.al.|[2501.17855v1](http://arxiv.org/abs/2501.17855v1)|null|
|**2025-01-29**|**Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**|Ljubisa Bojic et.al.|[2502.00055v1](http://arxiv.org/abs/2502.00055v1)|null|
|**2025-01-29**|**Tonguescape: Exploring Language Models Understanding of Vowel Articulation**|Haruki Sakajo et.al.|[2501.17643v1](http://arxiv.org/abs/2501.17643v1)|[link](https://github.com/sj-h4/tonguescape-builder)|
|**2025-01-29**|**Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**|Manish Sanwal et.al.|[2501.18645v2](http://arxiv.org/abs/2501.18645v2)|null|
|**2025-01-29**|**An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**|Wenqi Li et.al.|[2501.17555v1](http://arxiv.org/abs/2501.17555v1)|null|
|**2025-01-29**|**LLM Assistance for Pediatric Depression**|Mariia Ignashina et.al.|[2501.17510v1](http://arxiv.org/abs/2501.17510v1)|null|
|**2025-01-28**|**Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**|Gonzalo I√±aki Quintana et.al.|[2502.00052v1](http://arxiv.org/abs/2502.00052v1)|null|
|**2025-01-28**|**Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**|Chongyu Qu et.al.|[2501.17343v1](http://arxiv.org/abs/2501.17343v1)|null|
|**2025-01-28**|**Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**|Mingyu Derek Ma et.al.|[2501.17338v1](http://arxiv.org/abs/2501.17338v1)|null|
|**2025-01-28**|**Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**|Mingyu Derek Ma et.al.|[2501.17326v1](http://arxiv.org/abs/2501.17326v1)|null|
|**2025-01-28**|**Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**|Peilong Wang et.al.|[2501.17286v1](http://arxiv.org/abs/2501.17286v1)|null|
|**2025-01-28**|**ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**|Mohammadreza Saraei et.al.|[2501.17260v1](http://arxiv.org/abs/2501.17260v1)|[link](https://github.com/mrsaraei/vit-2spn)|
|**2025-01-28**|**A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**|Suresh Babu Nettur et.al.|[2501.17160v1](http://arxiv.org/abs/2501.17160v1)|null|
|**2025-01-28**|**Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**|Reza Ghorbani et.al.|[2501.17152v1](http://arxiv.org/abs/2501.17152v1)|null|
|**2025-01-28**|**Irony Detection, Reasoning and Understanding in Zero-shot Learning**|Peiling Yi et.al.|[2501.16884v1](http://arxiv.org/abs/2501.16884v1)|null|
|**2025-01-28**|**Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**|Keqi Han et.al.|[2501.17207v1](http://arxiv.org/abs/2501.17207v1)|[link](https://github.com/learningkeqi/rethinkingbca)|
|**2025-01-28**|**Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**|Fengpei Yuan et.al.|[2501.17206v1](http://arxiv.org/abs/2501.17206v1)|null|
|**2025-01-28**|**Efficient Knowledge Distillation of SAM for Medical Image Segmentation**|Kunal Dasharath Patil et.al.|[2501.16740v1](http://arxiv.org/abs/2501.16740v1)|null|
|**2025-01-28**|**VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**|Philip Chung et.al.|[2501.16672v1](http://arxiv.org/abs/2501.16672v1)|[link](https://github.com/philipchung/verifact)|
|**2025-01-28**|**Vision-based autonomous structural damage detection using data-driven methods**|Seyyed Taghi Ataei et.al.|[2501.16662v2](http://arxiv.org/abs/2501.16662v2)|null|
|**2025-01-28**|**Molecular-driven Foundation Model for Oncologic Pathology**|Anurag Vaidya et.al.|[2501.16652v1](http://arxiv.org/abs/2501.16652v1)|null|
|**2025-01-27**|**Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**|Yi Mao et.al.|[2502.00045v1](http://arxiv.org/abs/2502.00045v1)|null|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282v1](http://arxiv.org/abs/2501.16282v1)|null|
|**2025-01-27**|**Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**|Huayu Li et.al.|[2501.16215v1](http://arxiv.org/abs/2501.16215v1)|[link](https://github.com/HuayuLiArizona/Conformalized-Multiple-Instance-Learning-For-MedTS)|

#### Abstracts
##### **"It Felt Like I Was Left in the Dark": Exploring Information Needs and Design Opportunities for Family Caregivers of Older Adult Patients in Critical Care Settings**
2502.05115v1 by Shihan Fu, Bingsheng Yao, Smit Desai, Yuqi Hu, Yuling Sun, Samantha Stonbraker, Yanjun Gao, Elizabeth M. Goldberg, Dakuo Wang

Older adult patients constitute a rapidly growing subgroup of Intensive Care
Unit (ICU) patients. In these situations, their family caregivers are expected
to represent the unconscious patients to access and interpret patients' medical
information. However, caregivers currently have to rely on overloaded
clinicians for information updates and typically lack the health literacy to
understand complex medical information. Our project aims to explore the
information needs of caregivers of ICU older adult patients, from which we can
propose design opportunities to guide future AI systems. The project begins
with formative interviews with 11 caregivers to identify their challenges in
accessing and interpreting medical information; From these findings, we then
synthesize design requirements and propose an AI system prototype to cope with
caregivers' challenges. The system prototype has two key features: a timeline
visualization to show the AI extracted and summarized older adult patients' key
medical events; and an LLM-based chatbot to provide context-aware informational
support. We conclude our paper by reporting on the follow-up user evaluation of
the system and discussing future AI-based systems for ICU caregivers of older
adults.

ÊëòË¶ÅÔºöËÄÅÂπ¥ÊÇ£ËÄÖÊßãÊàêÂä†Ë≠∑ÁóÖÊàø (ICU) ÊÇ£ËÄÖ‰∏≠Âø´ÈÄüÊàêÈï∑ÁöÑÂ≠êÁæ§„ÄÇÂú®ÈÄô‰∫õÊÉÖÊ≥Å‰∏ãÔºåÈ†êÊúü‰ªñÂÄëÁöÑÂÆ∂Â∫≠ÁÖßË≠∑ËÄÖËÉΩ‰ª£Ë°®ÁÑ°ÊÑèË≠òÁöÑÊÇ£ËÄÖÂèñÂæó‰∏¶Ëß£ËÆÄÊÇ£ËÄÖÁöÑÈÜ´ÁôÇË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÁÖßË≠∑ËÄÖÁõÆÂâçÂøÖÈ†à‰æùË≥¥Â∑•‰ΩúÁπÅÈáçÁöÑËá®Â∫äÈÜ´Â∏´Êèê‰æõË≥áË®äÊõ¥Êñ∞ÔºåËÄå‰∏îÈÄöÂ∏∏Áº∫‰πè‰∫ÜËß£Ë§áÈõúÈÜ´ÁôÇË≥áË®äÁöÑÂÅ•Â∫∑Á¥†È§ä„ÄÇÊàëÂÄëÁöÑÂ∞àÊ°àÊó®Âú®Êé¢Á¥¢ ICU ËÄÅÂπ¥ÊÇ£ËÄÖÁÖßË≠∑ËÄÖÁöÑË≥áË®äÈúÄÊ±ÇÔºåÊàëÂÄëÂèØ‰ª•Ê†πÊìöÈÄô‰∫õÈúÄÊ±ÇÊèêÂá∫Ë®≠Ë®àÊ©üÊúÉÔºå‰ª•ÂºïÂ∞éÊú™‰æÜÁöÑ AI Á≥ªÁµ±„ÄÇÈÄôÂÄãÂ∞àÊ°àÂæûÂ∞ç 11 ‰ΩçÁÖßË≠∑ËÄÖÁöÑÂΩ¢ÊàêÊÄßË®™Ë´áÈñãÂßãÔºå‰ª•ÊâæÂá∫‰ªñÂÄëÂú®ÂèñÂæóÂíåËß£ËÆÄÈÜ´ÁôÇË≥áË®äÊñπÈù¢ÁöÑÊåëÊà∞ÔºõÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÊé•ËëóÁ∂úÂêàË®≠Ë®àÈúÄÊ±ÇÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄã AI Á≥ªÁµ±ÂéüÂûãÔºå‰ª•ÊáâÂ∞çÁÖßË≠∑ËÄÖÁöÑÊåëÊà∞„ÄÇÈÄôÂÄãÁ≥ªÁµ±ÂéüÂûãÂÖ∑ÊúâÂÖ©ÂÄãÈóúÈçµÁâπÈªûÔºö‰∏ÄÂÄãÊôÇÈñìËª∏Ë¶ñË¶∫ÂåñÔºå‰ª•È°ØÁ§∫ AI ËêÉÂèñ‰∏¶ÊëòË¶ÅÂá∫ÁöÑËÄÅÂπ¥ÊÇ£ËÄÖÈóúÈçµÈÜ´ÁôÇ‰∫ã‰ª∂Ôºõ‰ª•Âèä‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫Ôºå‰ª•Êèê‰æõÊÉÖÂ¢ÉÊÑüÁü•ÁöÑË≥áË®äÊîØÊè¥„ÄÇÊàëÂÄëÈÄèÈÅéÂ†±ÂëäÁ≥ªÁµ±ÁöÑÂæåÁ∫å‰ΩøÁî®ËÄÖË©ï‰º∞Ôºå‰ª•ÂèäË®éË´ñÊú™‰æÜÈáùÂ∞çËÄÅÂπ¥‰∫∫ ICU ÁÖßË≠∑ËÄÖÁöÑ AI Á≥ªÁµ±Ôºå‰æÜÁ∏ΩÁµêÊàëÂÄëÁöÑË´ñÊñá„ÄÇ

##### **Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs**
2502.05087v1 by Thierry Bossy, Julien Vignoud, Tahseen Rabbani, Juan R. Troncoso Pastoriza, Martin Jaggi

Federated learning (FL) is a popular paradigm for collaborative training
which avoids direct data exposure between clients. However, data privacy issues
still remain: FL-trained large language models are capable of memorizing and
completing phrases and sentences contained in training data when given with
their prefixes. Thus, it is possible for adversarial and honest-but-curious
clients to recover training data of other participants simply through targeted
prompting. In this work, we demonstrate that a popular and simple fine-tuning
strategy, low-rank adaptation (LoRA), reduces memorization during FL up to a
factor of 10. We study this effect by performing a medical question-answering
fine-tuning task and injecting multiple replicas of out-of-distribution
sensitive sequences drawn from an external clinical dataset. We observe a
reduction in memorization for a wide variety of Llama 2 and 3 models, and find
that LoRA can reduce memorization in centralized learning as well. Furthermore,
we show that LoRA can be combined with other privacy-preserving techniques such
as gradient clipping and Gaussian noising, secure aggregation, and Goldfish
loss to further improve record-level privacy while maintaining performance.

ÊëòË¶ÅÔºöËÅØÈÇ¶Â≠∏Áøí (FL) ÊòØ‰∏ÄÁ®ÆÊµÅË°åÁöÑÂçî‰ΩúË®ìÁ∑¥ÁØÑ‰æãÔºåÂèØÈÅøÂÖçÂÆ¢Êà∂Á´Ø‰πãÈñìÁõ¥Êé•ÂÖ¨ÈñãË≥áÊñô„ÄÇÁÑ∂ËÄåÔºåË≥áÊñôÈö±ÁßÅÂïèÈ°å‰ªçÁÑ∂Â≠òÂú®ÔºöÁ∂ìÈÅé FL Ë®ìÁ∑¥ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËÉΩÂ§†Ë®òÊÜ∂‰∏¶ÂÆåÊàêË®ìÁ∑¥Ë≥áÊñô‰∏≠ÂåÖÂê´ÁöÑÁâáË™ûÂíåÂè•Â≠êÔºåÂè™Ë¶ÅÁµ¶‰∫àÂÖ∂ÂâçÁ∂¥Âç≥ÂèØ„ÄÇÂõ†Ê≠§ÔºåÂ∞çÊäóÂíåË™†ÂØ¶‰ΩÜÂ•ΩÂ•áÁöÑÂÆ¢Êà∂Á´ØÊúâÂèØËÉΩÂÉÖÈÄèÈÅéÁõÆÊ®ôÊèêÁ§∫‰æÜÊÅ¢Âæ©ÂÖ∂‰ªñÂèÉËàáËÄÖÁöÑË®ìÁ∑¥Ë≥áÊñô„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË≠âÊòé‰∫Ü‰∏ÄÁ®ÆÊµÅË°å‰∏îÁ∞°ÂñÆÁöÑÂæÆË™øÁ≠ñÁï•Ôºå‰ΩéÁß©ÈÅ©Êáâ (LoRA)ÔºåÂèØÂ∞á FL ÊúüÈñìÁöÑË®òÊÜ∂Ê∏õÂ∞ëÂ§öÈÅî 10 ÂÄç„ÄÇÊàëÂÄëÈÄèÈÅéÂü∑Ë°åÈÜ´Â≠∏ÂïèÁ≠îÂæÆË™ø‰ªªÂãô‰∏¶Ê≥®ÂÖ•ÂæûÂ§ñÈÉ®Ëá®Â∫äË≥áÊñôÈõÜÊäΩÂèñÁöÑÈùûÂàÜ‰ΩàÊïèÊÑüÂ∫èÂàóÁöÑÂ§öÊ¨°Ë§áË£ΩÂìÅ‰æÜÁ†îÁ©∂Ê≠§ÊïàÊáâ„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÂêÑÁ®Æ Llama 2 Âíå 3 Ê®°ÂûãÁöÑË®òÊÜ∂ÂäõÈôç‰ΩéÔºå‰∏¶ÁôºÁèæ LoRA ‰πüËÉΩÊ∏õÂ∞ëÈõÜ‰∏≠ÂºèÂ≠∏Áøí‰∏≠ÁöÑË®òÊÜ∂Âäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫ LoRA ÂèØ‰ª•ËàáÂÖ∂‰ªñÈö±ÁßÅ‰øùË≠∑ÊäÄË°ìÁµêÂêà‰ΩøÁî®Ôºå‰æãÂ¶ÇÊ¢ØÂ∫¶Ë£ÅÂâ™ÂíåÈ´òÊñØÈõúË®ä„ÄÅÂÆâÂÖ®ËÅöÂêàÂíå Goldfish ÊêçÂ§±Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑË®òÈåÑÁ¥öÈö±ÁßÅÔºåÂêåÊôÇÁ∂≠ÊåÅÊïàËÉΩ„ÄÇ

##### **MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin**
2502.04794v1 by Minrui Chen, Yi Zhou, Huidong Jiang, Yuhan Zhu, Guanjie Zou, Minqi Chen, Rong Tian, Hiroto Saigo

Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is
introduced as a multimodal framework inspired by real-world diagnostic
processes. It uses pretrained models such as DINOv2, Vision Transformer, and
ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into
low-dimensional, semantically meaningful features. A learnable
self-attention-based fusion network then integrates these imaging features with
clinical data for classification. Using 416 FUO patient cases from Sichuan
University West China Hospital from 2017 to 2023, the multimodal fusion
classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to
0.9291 across seven tasks, outperforming conventional machine learning and
single-modality deep learning methods. Ablation studies and five-fold
cross-validation further validated its effectiveness. By combining the
strengths of pretrained large models and deep learning, MedMimic offers a
promising solution for disease classification.

ÊëòË¶ÅÔºö‰∏çÊòéÂéüÂõ†ÁôºÁáí (FUO) ‰ªçÁÑ∂ÊòØË®∫Êñ∑‰∏äÁöÑÊåëÊà∞„ÄÇMedMimic ÊòØ‰∏ÄÂÄãÂ§öÊ®°ÂºèÊû∂ÊßãÔºåÈùàÊÑü‰æÜËá™ÊñºÁúüÂØ¶‰∏ñÁïåÁöÑË®∫Êñ∑ÈÅéÁ®ã„ÄÇÂÆÉ‰ΩøÁî®È†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºå‰æãÂ¶Ç DINOv2„ÄÅË¶ñË¶∫ËΩâÊèõÂô®Âíå ResNet-18ÔºåÂ∞áÈ´òÁ∂≠ 18F-FDG PET/CT ÂΩ±ÂÉèËΩâÊèõÁÇ∫‰ΩéÁ∂≠„ÄÅË™ûÁæ©ÊúâÊÑèÁæ©ÁöÑÁâπÂæµ„ÄÇ‰∏ÄÂÄãÂèØÂ≠∏ÁøíÁöÑËá™Ê≥®ÊÑèÂäõËûçÂêàÁ∂≤Ë∑ØÊé•ËëóÂ∞áÈÄô‰∫õÂΩ±ÂÉèÁâπÂæµËàáËá®Â∫äË≥áÊñôÊï¥ÂêàÔºåÁî®ÊñºÂàÜÈ°û„ÄÇ‰ΩøÁî® 2017 Âπ¥Ëá≥ 2023 Âπ¥ÂõõÂ∑ùÂ§ßÂ≠∏ËèØË•øÈÜ´Èô¢ÁöÑ 416 ÂÄã FUO ÁóÖÊÇ£ÁóÖ‰æãÔºåÂ§öÊ®°ÂºèËûçÂêàÂàÜÈ°ûÁ∂≤Ë∑Ø MFCN Âú®‰∏ÉÈ†Ö‰ªªÂãô‰∏≠ÈÅîÂà∞‰∫Ü 0.8654 Âà∞ 0.9291 ÁöÑÂ∑®ËßÄ AUROC ÂàÜÊï∏ÔºåÂÑ™ÊñºÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÂíåÂñÆ‰∏ÄÊ®°ÂºèÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ï„ÄÇÊ∂àËûçÁ†îÁ©∂Âíå‰∫îÂÄç‰∫§ÂèâÈ©óË≠âÈÄ≤‰∏ÄÊ≠•È©óË≠â‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇMedMimic ÁµêÂêà‰∫ÜÈ†êÂÖàË®ìÁ∑¥ÁöÑÂ§ßÊ®°ÂûãÂíåÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÑ™ÈªûÔºåÁÇ∫ÁñæÁóÖÂàÜÈ°ûÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification**
2502.04515v1 by Wei Fan, Jingru Fei, Dingyu Guo, Kun Yi, Xiaozhuang Song, Haolong Xiang, Hangting Ye, Min Li

Medical time series has been playing a vital role in real-world healthcare
systems as valuable information in monitoring health conditions of patients.
Accurate classification for medical time series, e.g., Electrocardiography
(ECG) signals, can help for early detection and diagnosis. Traditional methods
towards medical time series classification rely on handcrafted feature
extraction and statistical methods; with the recent advancement of artificial
intelligence, the machine learning and deep learning methods have become more
popular. However, existing methods often fail to fully model the complex
spatial dynamics under different scales, which ignore the dynamic
multi-resolution spatial and temporal joint inter-dependencies. Moreover, they
are less likely to consider the special baseline wander problem as well as the
multi-view characteristics of medical time series, which largely hinders their
prediction performance. To address these limitations, we propose a
Multi-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical
time series classification. Specifically, we first propose to construct
multi-resolution adaptive graph structures to learn dynamic multi-scale
embeddings. Then, to address the baseline wander problem, we propose Difference
Attention Networks to operate self-attention mechanisms on the finite
difference for temporal modeling. Moreover, to learn the multi-view
characteristics, we utilize the Frequency Convolution Networks to capture
complementary information of medical time series from the frequency domain. In
addition, we introduce the Multi-resolution Graph Transformer architecture to
model the dynamic dependencies and fuse the information from different
resolutions. Finally, we have conducted extensive experiments on multiple
medical real-world datasets that demonstrate the superior performance of our
method. Our Code is available.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤Ôºå‰ΩúÁÇ∫Áõ£ÊéßÊÇ£ËÄÖÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑÂØ∂Ë≤¥Ë≥áË®ä„ÄÇ
Ê∫ñÁ¢∫ÂàÜÈ°ûÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÔºå‰æãÂ¶ÇÂøÉÈõªÂúñ (ECG) Ë®äËôüÔºåÊúâÂä©ÊñºÊó©ÊúüÂÅµÊ∏¨ÂíåË®∫Êñ∑„ÄÇÂÇ≥Áµ±ÁöÑÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûÊñπÊ≥ï‰ª∞Ë≥¥ÊâãÂ∑•ÁâπÂæµËêÉÂèñÂíåÁµ±Ë®àÊñπÊ≥ïÔºõÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïËÆäÂæóÊõ¥ÁÇ∫ÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏ÁÑ°Ê≥ïÂÆåÂÖ®Âª∫Ê®°‰∏çÂêåÂ∞∫Â∫¶‰∏ãÁöÑË§áÈõúÁ©∫ÈñìÂãïÊÖãÔºåÂøΩÁï•‰∫ÜÂãïÊÖãÂ§öËß£ÊûêÂ∫¶Á©∫ÈñìÂíåÊôÇÈñìÈóúÁØÄÁõ∏‰∫í‰æùË≥¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄë‰∏çÂ§™ÂèØËÉΩËÄÉÊÖÆÁâπÊÆäÁöÑÂü∫Á∑öÊºÇÁßªÂïèÈ°å‰ª•ÂèäÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÁöÑÂ§öË¶ñËßíÁâπÊÄßÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öËß£ÊûêÂ∫¶ÊôÇÁ©∫ÂúñÂΩ¢Â≠∏ÁøíÊû∂Êßã MedGNNÔºåÁî®ÊñºÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÂàÜÈ°û„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫ÊßãÂª∫Â§öËß£ÊûêÂ∫¶Ëá™ÈÅ©ÊáâÂúñÂΩ¢ÁµêÊßã‰ª•Â≠∏ÁøíÂãïÊÖãÂ§öÂ∞∫Â∫¶ÂµåÂÖ•„ÄÇÁÑ∂ÂæåÔºåÁÇ∫‰∫ÜËß£Ê±∫Âü∫Á∑öÊºÇÁßªÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫Â∑ÆÂàÜÊ≥®ÊÑèÂäõÁ∂≤Ë∑ØÔºåÂ∞çÊôÇÈñìÂª∫Ê®°ÁöÑÊúâÈôêÂ∑ÆÂàÜÈÅãÁÆóËá™Ê≥®ÊÑèÂäõÊ©üÂà∂„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÂ≠∏ÁøíÂ§öË¶ñËßíÁâπÊÄßÔºåÊàëÂÄëÂà©Áî®È†ªÁéáÂç∑Á©çÁ∂≤Ë∑ØÂæûÈ†ªÂüüÊì∑ÂèñÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÁöÑ‰∫íË£úË≥áË®ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§öËß£ÊûêÂ∫¶ÂúñÂΩ¢TransformerÊû∂Êßã‰æÜÂª∫Ê®°ÂãïÊÖã‰æùË≥¥ÊÄßÔºå‰∏¶ËûçÂêà‰æÜËá™‰∏çÂêåËß£ÊûêÂ∫¶ÁöÑË≥áË®ä„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞çÂ§öÂÄãÈÜ´ÁôÇÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂÑ™Áï∞ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨Èñã„ÄÇ</paragraph>

##### **Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions**
2502.04423v1 by Khushboo Verma, Alan Michels, Ergi Gumusaneli, Shilpa Chitnis, Smita Sinha Kumar, Christopher Thompson, Lena Esmail, Guruprasath Srinivasan, Chandini Panchada, Sushovan Guha, Satwant Kumar

Referral workflow inefficiencies, including misaligned referrals and delays,
contribute to suboptimal patient outcomes and higher healthcare costs. In this
study, we investigated the possibility of predicting procedural needs based on
primary care diagnostic entries, thereby improving referral accuracy,
streamlining workflows, and providing better care to patients. A de-identified
dataset of 2,086 orthopedic referrals from the University of Texas Health at
Tyler was analyzed using machine learning models built on Base General
Embeddings (BGE) for semantic extraction. To ensure real-world applicability,
noise tolerance experiments were conducted, and oversampling techniques were
employed to mitigate class imbalance. The selected optimum and parsimonious
embedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, Matthews
Correlation Coefficient (MCC): 0.540), effectively distinguishing patients
requiring surgical intervention. Dimensionality reduction techniques confirmed
the model's ability to capture meaningful clinical relationships. A threshold
sensitivity analysis identified an optimal decision threshold (0.30) to balance
precision and recall, maximizing referral efficiency. In the predictive
modeling analysis, the procedure rate increased from 11.27% to an optimal
60.1%, representing a 433% improvement with significant implications for
operational efficiency and healthcare revenue.
  The results of our study demonstrate that referral optimization can enhance
primary and surgical care integration. Through this approach, precise and
timely predictions of procedural requirements can be made, thereby minimizing
delays, improving surgical planning, and reducing administrative burdens. In
addition, the findings highlight the potential of clinical decision support as
a scalable solution for improving patient outcomes and the efficiency of the
healthcare system.

ÊëòË¶ÅÔºöËΩâË®∫ÊµÅÁ®ãÊïàÁéá‰ΩéËêΩÔºåÂåÖÊã¨ËΩâË®∫‰∏çÁï∂ÂíåÂª∂Ë™§Ôºå
Â∞éËá¥Ê¨°ÂÑ™ÁöÑÊÇ£ËÄÖÁµêÊûúÂíåÊõ¥È´òÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊàêÊú¨„ÄÇÂú®ÈÄô
È†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÊ†πÊìöÂàùÁ¥ö‰øùÂÅ•Ë®∫Êñ∑Ê¢ùÁõÆÈ†êÊ∏¨Á®ãÂ∫èÈúÄÊ±ÇÁöÑÂèØËÉΩÊÄßÔºåÂæûËÄåÊèêÈ´òËΩâË®∫Ê∫ñÁ¢∫ÊÄßÔºå
Á∞°ÂåñÂ∑•‰ΩúÊµÅÁ®ãÔºå‰∏¶ÁÇ∫ÊÇ£ËÄÖÊèê‰æõÊõ¥Â•ΩÁöÑÁÖßË≠∑„ÄÇ‰∏ÄÂÄãÂéªË≠òÂà•Âåñ
Âæ∑ÂÖãËñ©ÊñØÂ§ßÂ≠∏ÂÅ•Â∫∑‰∏≠ÂøÉÁöÑ 2,086 ÂÄãÈ™®ÁßëËΩâË®∫ÁöÑË≥áÊñôÈõÜ
Ê≥∞Âãí‰ΩøÁî®Âª∫Á´ãÂú®Âü∫Êú¨ÈÄöÁî®
Ë™ûÁæ©ÊèêÂèñÁöÑÂµåÂÖ• (BGE) ‰∏äÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÂàÜÊûê„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øùÁèæÂØ¶‰∏ñÁïåÁöÑÈÅ©Áî®ÊÄßÔºå
ÈÄ≤Ë°å‰∫ÜÂô™ËÅ≤ÂÆπÂøçÂ∫¶ÂØ¶È©óÔºå‰∏¶Êé°Áî®‰∫ÜÈÅéÊé°Ê®£ÊäÄË°ì‰æÜÊ∏õËºïÈ°ûÂà•‰∏çÂπ≥Ë°°„ÄÇÊâÄÈÅ∏ÁöÑÊúÄ‰Ω≥ÂíåÁ∞°Á¥Ñ
ÂµåÂÖ•Ê®°ÂûãÂ±ïÁ§∫‰∫ÜÈ´òÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ (ROC-AUCÔºö0.874ÔºåÈ¶¨‰øÆÊñØ
Áõ∏ÈóúÁ≥ªÊï∏ (MCC)Ôºö0.540)ÔºåÊúâÊïàÂçÄÂàÜÈúÄË¶ÅÊâãË°ìÂπ≤È†êÁöÑÊÇ£ËÄÖ„ÄÇÈôçÁ∂≠
ÊäÄË°ìË≠âÂØ¶‰∫ÜÊ®°ÂûãÊçïÊçâÊúâÊÑèÁæ©ÁöÑËá®Â∫äÈóú‰øÇÁöÑËÉΩÂäõ„ÄÇÈñæÂÄº
ÊïèÊÑüÊÄßÂàÜÊûêÁ¢∫ÂÆö‰∫Ü‰∏ÄÂÄãÊúÄ‰Ω≥Ê±∫Á≠ñÈñæÂÄº (0.30) ‰æÜÂπ≥Ë°°
Á≤æÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéáÔºåÊúÄÂ§ßÂåñËΩâË®∫ÊïàÁéá„ÄÇÂú®È†êÊ∏¨‰∏≠
Âª∫Ê®°ÂàÜÊûê‰∏≠ÔºåÁ®ãÂ∫èÁéáÂæû 11.27% Â¢ûÂä†Âà∞ÊúÄ‰Ω≥ÁöÑ
60.1%Ôºå‰ª£Ë°® 433% ÁöÑÊîπÈÄ≤ÔºåÂ∞çÈÅãÁáüÊïàÁéáÂíåÈÜ´ÁôÇ‰øùÂÅ•Êî∂ÂÖ•ÂÖ∑ÊúâÈáçÂ§ßÂΩ±Èüø„ÄÇ
ÊàëÂÄëÁ†îÁ©∂ÁöÑÁµêÊûúË°®ÊòéÔºåËΩâË®∫ÂÑ™ÂåñÂèØ‰ª•Â¢ûÂº∑
ÂàùÁ¥öÂíåÂ§ñÁßëË≠∑ÁêÜÊï¥Âêà„ÄÇÈÄöÈÅéÈÄôÁ®ÆÊñπÊ≥ïÔºåÂèØ‰ª•Â∞çÁ®ãÂ∫èÈúÄÊ±ÇÈÄ≤Ë°åÊ∫ñÁ¢∫ÂèäÊôÇÁöÑÈ†êÊ∏¨ÔºåÂæûËÄåÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Ê∏õÂ∞ë
Âª∂Ë™§ÔºåÊîπÂñÑÊâãË°ìË®àÂäÉÔºå‰∏¶Ê∏õËºïË°åÊîøË≤†Êìî„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜËá®Â∫äÊ±∫Á≠ñÊîØÊåÅ‰ΩúÁÇ∫
‰∏ÄÂÄãÂèØÊì¥Â±ïÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÊΩõÂäõÔºåÁî®ÊñºÊîπÂñÑÊÇ£ËÄÖÁµêÊûúÂíåÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊïàÁéá„ÄÇ

##### **Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation**
2502.04083v1 by Tewele W. Tareke, Neree Payan, Alexandre Cochet, Laurent Arnould, Benoit Presles, Jean-Marc Vrigneaud, Fabrice Meriaudeau, Alain Lalande

Neoadjuvant chemotherapy (NAC) has become a standard clinical practice for
tumor downsizing in breast cancer with 18F-FDG Positron Emission Tomography
(PET). Our work aims to leverage PET imaging for the segmentation of breast
lesions. The focus is on developing an automated system that accurately
segments primary tumor regions and extracts key biomarkers from these areas to
provide insights into the evolution of breast cancer following the first course
of NAC. 243 baseline 18F-FDG PET scans (PET_Bl) and 180 follow-up 18F-FDG PET
scans (PET_Fu) were acquired before and after the first course of NAC,
respectively. Firstly, a deep learning-based breast tumor segmentation method
was developed. The optimal baseline model (model trained on baseline exams) was
fine-tuned on 15 follow-up exams and adapted using active learning to segment
tumor areas in PET_Fu. The pipeline computes biomarkers such as maximum
standardized uptake value (SUVmax), metabolic tumor volume (MTV), and total
lesion glycolysis (TLG) to evaluate tumor evolution between PET_Fu and PET_Bl.
Quality control measures were employed to exclude aberrant outliers. The nnUNet
deep learning model outperformed in tumor segmentation on PET_Bl, achieved a
Dice similarity coefficient (DSC) of 0.89 and a Hausdorff distance (HD) of 3.52
mm. After fine-tuning, the model demonstrated a DSC of 0.78 and a HD of 4.95 mm
on PET_Fu exams. Biomarkers analysis revealed very strong correlations whatever
the biomarker between manually segmented and automatically predicted regions.
The significant average decrease of SUVmax, MTV and TLG were 5.22, 11.79 cm3
and 19.23 cm3, respectively. The presented approach demonstrates an automated
system for breast tumor segmentation from 18F-FDG PET. Thanks to the extracted
biomarkers, our method enables the automatic assessment of cancer progression.

ÊëòË¶ÅÔºöÊñ∞ËæÖÂä©ÂåñÁñó (NAC) Â∑≤Êàê‰∏∫‰π≥ËÖ∫Áôå‰∏≠ÈááÁî® 18F-FDG Ê≠£ÁîµÂ≠êÂèëÂ∞ÑÊñ≠Â±ÇÊâ´Êèè (PET) ËøõË°åËÇøÁò§Áº©Â∞èÁöÑÊ†áÂáÜ‰∏¥Â∫äÂÆûË∑µ„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊó®Âú®Âà©Áî® PET ÂΩ±ÂÉèÂàÜÂâ≤‰π≥ËÖ∫ÁóÖÂèò„ÄÇÈáçÁÇπÂú®‰∫éÂºÄÂèë‰∏Ä‰∏™Ëá™Âä®Á≥ªÁªüÔºåËØ•Á≥ªÁªüÂèØ‰ª•ÂáÜÁ°ÆÂàÜÂâ≤ÂéüÂèëÊÄßËÇøÁò§Âå∫ÂüüÂπ∂‰ªéËøô‰∫õÂå∫ÂüüÊèêÂèñÂÖ≥ÈîÆÁîüÁâ©Ê†áËÆ∞Ôºå‰ª•Ê∑±ÂÖ•‰∫ÜËß£‰π≥ËÖ∫ÁôåÂú®Á¨¨‰∏ÄÁñóÁ®ã NAC ÂêéÁöÑÊºîÂèò„ÄÇÂàÜÂà´Âú®Á¨¨‰∏ÄÁñóÁ®ã NAC ‰πãÂâçÂíå‰πãÂêéÈááÈõÜ‰∫Ü 243 ‰æãÂü∫Á∫ø 18F-FDG PET Êâ´Êèè (PET_Bl) Âíå 180 ‰æãÈöèËÆø 18F-FDG PET Êâ´Êèè (PET_Fu)„ÄÇÈ¶ñÂÖàÔºåÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰π≥ËÖ∫ËÇøÁò§ÂàÜÂâ≤ÊñπÊ≥ï„ÄÇÂØπ 15 ‰æãÈöèËÆøÊ£ÄÊü•ÂØπÊúÄ‰ºòÂü∫Á∫øÊ®°ÂûãÔºàÂú®Âü∫Á∫øÊ£ÄÊü•‰∏≠ËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºâËøõË°å‰∫ÜÂæÆË∞ÉÔºåÂπ∂‰ΩøÁî®‰∏ªÂä®Â≠¶‰π†ÂØπ PET_Fu ‰∏≠ÁöÑËÇøÁò§Âå∫ÂüüËøõË°å‰∫ÜÂàÜÂâ≤„ÄÇËØ•ÁÆ°ÈÅìËÆ°ÁÆóËØ∏Â¶ÇÊúÄÂ§ßÊ†áÂáÜÊëÑÂèñÂÄº (SUVmax)„ÄÅ‰ª£Ë∞¢ËÇøÁò§‰ΩìÁßØ (MTV) ÂíåÊÄªÁóÖÁÅ∂Á≥ñÈÖµËß£ (TLG) Á≠âÁîüÁâ©Ê†áËÆ∞Ôºå‰ª•ËØÑ‰º∞ PET_Fu Âíå PET_Bl ‰πãÈó¥ÁöÑËÇøÁò§ÊºîÂèò„ÄÇÈááÁî®Ë¥®ÈáèÊéßÂà∂Êé™ÊñΩÊù•ÊéíÈô§ÂºÇÂ∏∏ÂÄº„ÄÇnnUNet Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂú® PET_Bl ‰∏äÁöÑËÇøÁò§ÂàÜÂâ≤ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞ 0.89 ÁöÑ Dice Áõ∏‰ººÊÄßÁ≥ªÊï∞ (DSC) Âíå 3.52 ÊØ´Á±≥ÁöÑ Hausdorff Ë∑ùÁ¶ª (HD)„ÄÇÂæÆË∞ÉÂêéÔºåËØ•Ê®°ÂûãÂú® PET_Fu Ê£ÄÊü•‰∏≠ÊòæÁ§∫Âá∫ 0.78 ÁöÑ DSC Âíå 4.95 ÊØ´Á±≥ÁöÑ HD„ÄÇÊó†ËÆ∫ÊâãÂä®ÂàÜÂâ≤Âå∫ÂüüÂíåËá™Âä®È¢ÑÊµãÂå∫Âüü‰πãÈó¥ÁöÑÁîüÁâ©Ê†áËÆ∞Â¶Ç‰ΩïÔºåÁîüÁâ©Ê†áËÆ∞ÂàÜÊûêÈÉΩÊòæÁ§∫Âá∫ÈùûÂ∏∏Âº∫ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇSUVmax„ÄÅMTV Âíå TLG ÁöÑÂπ≥ÂùáÊòæÁùÄ‰∏ãÈôçÂàÜÂà´‰∏∫ 5.22„ÄÅ11.79 cm3 Âíå 19.23 cm3„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™Áî®‰∫é‰ªé 18F-FDG PET ÂàÜÂâ≤‰π≥ËÖ∫ËÇøÁò§ÁöÑËá™Âä®ÂåñÁ≥ªÁªü„ÄÇÁî±‰∫éÊèêÂèñ‰∫ÜÁîüÁâ©Ê†áËÆ∞ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üËá™Âä®ËØÑ‰º∞ÁôåÁóáËøõÂ±ï„ÄÇ

##### **Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization**
2502.04034v1 by Ran Song, Yinpu Bai, Hui Liu

The accurate prediction of drug responses remains a formidable challenge,
particularly at the single-cell level and in clinical treatment contexts. Some
studies employ transfer learning techniques to predict drug responses in
individual cells and patients, but they require access to target-domain data
during training, which is often unavailable or only obtainable in future. In
this study, we propose a novel domain generalization framework, termed
panCancerDR, to address this challenge. We conceptualize each cancer type as a
distinct source domain, with its cell lines serving as domain-specific samples.
Our primary objective is to extract domain-invariant features from the
expression profiles of cell lines across diverse cancer types, thereby
generalize the predictive capacity to out-of-distribution samples. To enhance
robustness, we introduce a latent independence projection (LIP) module that
encourages the encoder to extract informative yet non-redundant features. Also,
we propose an asymmetric adaptive clustering constraint, which clusters
drug-sensitive samples into a compact group while drives resistant samples
dispersed across separate clusters in the latent space. Our empirical
experiments demonstrate that panCancerDR effectively learns task-relevant
features from diverse source domains, and achieves accurate predictions of drug
response for unseen cancer type during training. Furthermore, when evaluated on
single-cell and patient-level prediction tasks, our model-trained solely on in
vitro cell line data without access to target-domain information-consistently
outperforms and matched current state-of-the-art methods. These findings
highlights the potential of our method for real-world clinical applications.

ÊëòË¶ÅÔºö<paragraph>Ê∫ñÁ¢∫È†êÊ∏¨Ëó•Áâ©ÂèçÊáâ‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖËâ±ÈâÖÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÂñÆÁ¥∞ËÉûÂ±§Á¥öÂíåËá®Â∫äÊ≤ªÁôÇËÉåÊôØ‰∏≠„ÄÇ‰∏Ä‰∫õÁ†îÁ©∂Êé°Áî®ÈÅ∑ÁßªÂ≠∏ÁøíÊäÄË°ì‰æÜÈ†êÊ∏¨ÂÄãÂà•Á¥∞ËÉûÂíåÊÇ£ËÄÖÁöÑËó•Áâ©ÂèçÊáâÔºå‰ΩÜÂÆÉÂÄëÈúÄË¶ÅÂú®Ë®ìÁ∑¥ÊúüÈñìÂ≠òÂèñÁõÆÊ®ôÁ∂≤ÂüüË≥áÊñôÔºåËÄåÈÄô‰∫õË≥áÊñôÈÄöÂ∏∏ÁÑ°Ê≥ïÂèñÂæóÔºåÊàñÂè™ËÉΩÂú®Êú™‰æÜÂèñÂæó„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÁ∂≤ÂüüÊ¶ÇÂåñÊû∂ÊßãÔºåÁ®±ÁÇ∫ panCancerDRÔºå‰ª•ÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞„ÄÇÊàëÂÄëÂ∞áÊØèÁ®ÆÈ°ûÂûãÁöÑÁôåÁóáÊ¶ÇÂøµÂåñÁÇ∫‰∏ÄÂÄã‰∏çÂêåÁöÑ‰æÜÊ∫êÁ∂≤ÂüüÔºåÂÖ∂Á¥∞ËÉûÊ†™‰ΩúÁÇ∫ÁâπÂÆöÁ∂≤ÂüüÁöÑÊ®£Êú¨„ÄÇÊàëÂÄëÁöÑÈ¶ñË¶ÅÁõÆÊ®ôÊòØÂæû‰∏çÂêåÁôåÁóáÈ°ûÂûãÁöÑÁ¥∞ËÉûÊ†™Ë°®ÁèæÁâπÂæµ‰∏≠ËêÉÂèñÁ∂≤Âüü‰∏çËÆäÁâπÂæµÔºåÂæûËÄåÂ∞áÈ†êÊ∏¨ËÉΩÂäõÊ¶ÇÂåñÂà∞ÂàÜÂ∏ÉÂ§ñÁöÑÊ®£Êú¨„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑Á©©ÂÅ•ÊÄßÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÂÄãÊΩõÂú®Áç®Á´ãÊäïÂΩ± (LIP) Ê®°ÁµÑÔºåÈºìÂãµÁ∑®Á¢ºÂô®ËêÉÂèñÊúâË≥áË®ä‰ΩÜÈùûÂÜóÈ§òÁöÑÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈùûÂ∞çÁ®±Ëá™ÈÅ©ÊáâËÅöÈ°ûÁ¥ÑÊùüÔºåÂ∞áÂ∞çËó•Áâ©ÊïèÊÑüÁöÑÊ®£Êú¨ËÅöÈ°ûÂà∞‰∏ÄÂÄãÁ∑äÊπäÁöÑÁæ§ÁµÑ‰∏≠ÔºåÂêåÊôÇÈ©ÖÂãïÊäóËó•ÊÄßÊ®£Êú¨ÂàÜÊï£Âú®ÊΩõÂú®Á©∫Èñì‰∏≠ÁöÑ‰∏çÂêåÁæ§ÁµÑ‰∏≠„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÂØ¶È©óË≠âÊòéÔºåpanCancerDR ÊúâÊïàÂú∞Âæû‰∏çÂêåÁöÑ‰æÜÊ∫êÁ∂≤ÂüüÂ≠∏ÁøíËàá‰ªªÂãôÁõ∏ÈóúÁöÑÁâπÂæµÔºå‰∏¶Âú®Ë®ìÁ∑¥ÊúüÈñìÂ∞çÊú™Ë¶ãÁöÑÁôåÁóáÈ°ûÂûãÂØ¶ÁèæÊ∫ñÁ¢∫ÁöÑËó•Áâ©ÂèçÊáâÈ†êÊ∏¨„ÄÇÊ≠§Â§ñÔºåÁï∂Âú®ÂñÆÁ¥∞ËÉûÂíåÊÇ£ËÄÖÂ±§Á¥öÈ†êÊ∏¨‰ªªÂãô‰∏≠ÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂÉÖÂú®È´îÂ§ñÁ¥∞ËÉûÊ†™Ë≥áÊñô‰∏äË®ìÁ∑¥ÔºåËÄåÊ≤íÊúâÂ≠òÂèñÁõÆÊ®ôÁ∂≤ÂüüË≥áË®äÔºåÂßãÁµÇÂÑ™Êñº‰∏¶Á¨¶ÂêàÁï∂ÂâçÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂØ¶ÈöõËá®Â∫äÊáâÁî®‰∏≠ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot**
2502.04413v1 by Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao

Retrieval-augmented generation (RAG) is a well-suited technique for
retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a
key module of the healthcare copilot, helping reduce misdiagnosis for
healthcare practitioners and patients. However, the diagnostic accuracy and
specificity of existing heuristic-based RAG models used in the medical domain
are inadequate, particularly for diseases with similar manifestations. This
paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited
reasoning for the medical domain that retrieves diagnosis and treatment
recommendations based on manifestations. MedRAG systematically constructs a
comprehensive four-tier hierarchical diagnostic KG encompassing critical
diagnostic differences of various diseases. These differences are dynamically
integrated with similar EHRs retrieved from an EHR database, and reasoned
within a large language model. This process enables more accurate and specific
decision support, while also proactively providing follow-up questions to
enhance personalized medical decision-making. MedRAG is evaluated on both a
public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)
collected from Tan Tock Seng Hospital, and its performance is compared against
various existing RAG methods. Experimental results show that, leveraging the
information integration and relational abilities of the KG, our MedRAG provides
more specific diagnostic insights and outperforms state-of-the-art models in
reducing misdiagnosis rates. Our code will be available at
https://github.com/SNOWTEAM2023/MedRAG

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊòØ‰∏ÄÁ®ÆÈÅ©Áî®ÊñºÊ™¢Á¥¢Èö±ÁßÅÊïèÊÑüÁöÑÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) ÁöÑÊäÄË°ì„ÄÇÂÆÉÂèØ‰ª•‰ΩúÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•ÂâØÈßïÈßõÁöÑ‰∏ÄÂÄãÈóúÈçµÊ®°ÁµÑÔºåÂçîÂä©Ê∏õÂ∞ëÈÜ´ÁôÇ‰øùÂÅ•ÂæûÊ•≠‰∫∫Âì°ÂíåÊÇ£ËÄÖÁöÑË™§Ë®∫„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠‰ΩøÁî®ÁöÑÁèæÊúâÂü∫ÊñºÂïüÁôºÊ≥ïÁöÑ RAG Ê®°ÂûãÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÁâπÁï∞ÊÄß‰∏çË∂≥ÔºåÁâπÂà•ÊòØÂ∞çÊñºÂÖ∑ÊúâÈ°û‰ººË°®ÁèæÁöÑÁñæÁóÖ„ÄÇÊú¨ÊñáÊèêÂá∫ MedRAGÔºå‰∏ÄÁ®ÆÁî±Áü•Ë≠òÂúñË≠ú (KG) ÂºïÁôºÁöÑÊé®ÁêÜÂ¢ûÂº∑ÁöÑ RAG Ê®°ÂûãÔºåÁî®ÊñºÈÜ´ÁôÇÈ†òÂüüÔºåÂÆÉÊ†πÊìöË°®ÁèæÊ™¢Á¥¢Ë®∫Êñ∑ÂíåÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇMedRAG Á≥ªÁµ±ÊÄßÂú∞ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂõõÂ±§ÈöéÂ±§ÂºèË®∫Êñ∑ KGÔºåÊ∂µËìãÂêÑÁ®ÆÁñæÁóÖÁöÑÈóúÈçµË®∫Êñ∑Â∑ÆÁï∞„ÄÇÈÄô‰∫õÂ∑ÆÁï∞ËàáÂæû EHR Ë≥áÊñôÂ∫´‰∏≠Ê™¢Á¥¢Âà∞ÁöÑÈ°û‰ºº EHR ÂãïÊÖãÊï¥ÂêàÔºå‰∏¶Âú®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÈÄôÂÄãÈÅéÁ®ãÂèØ‰ª•ÂØ¶ÁèæÊõ¥Ê∫ñÁ¢∫ÂíåÂÖ∑È´îÁöÑÊ±∫Á≠ñÊîØÊè¥ÔºåÂêåÊôÇ‰∏ªÂãïÊèê‰æõÂæåÁ∫åÂïèÈ°åÔºå‰ª•Â¢ûÂº∑ÂÄã‰∫∫ÂåñÈÜ´ÁôÇÊ±∫Á≠ñÂà∂ÂÆö„ÄÇMedRAG Âú®ÂÖ¨ÂÖ±Ë≥áÊñôÈõÜ DDXPlus ÂíåÂæûÈô≥ÁØ§ÁîüÈÜ´Èô¢Êî∂ÈõÜÁöÑÁßÅ‰∫∫ÊÖ¢ÊÄßÁñºÁóõË®∫Êñ∑Ë≥áÊñôÈõÜ (CPDD) ‰∏äÈÄ≤Ë°åË©ï‰º∞Ôºå‰∏¶Â∞áÂÖ∂ÊïàËÉΩËàáÂêÑÁ®ÆÁèæÊúâ RAG ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂà©Áî® KG ÁöÑË≥áË®äÊï¥ÂêàÂíåÈóú‰øÇËÉΩÂäõÔºåÊàëÂÄëÁöÑ MedRAG Êèê‰æõ‰∫ÜÊõ¥ÂÖ∑È´îÁöÑË®∫Êñ∑Ë¶ãËß£Ôºå‰∏¶Âú®Èôç‰ΩéË™§Ë®∫ÁéáÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂú® https://github.com/SNOWTEAM2023/MedRAG Êèê‰æõ

##### **Transforming Multimodal Models into Action Models for Radiotherapy**
2502.04408v1 by Matteo Ferrante, Alessandra Carosi, Rolando Maria D Angelillo, Nicola Toschi

Radiotherapy is a crucial cancer treatment that demands precise planning to
balance tumor eradication and preservation of healthy tissue. Traditional
treatment planning (TP) is iterative, time-consuming, and reliant on human
expertise, which can potentially introduce variability and inefficiency. We
propose a novel framework to transform a large multimodal foundation model
(MLM) into an action model for TP using a few-shot reinforcement learning (RL)
approach. Our method leverages the MLM's extensive pre-existing knowledge of
physics, radiation, and anatomy, enhancing it through a few-shot learning
process. This allows the model to iteratively improve treatment plans using a
Monte Carlo simulator. Our results demonstrate that this method outperforms
conventional RL-based approaches in both quality and efficiency, achieving
higher reward scores and more optimal dose distributions in simulations on
prostate cancer data. This proof-of-concept suggests a promising direction for
integrating advanced AI models into clinical workflows, potentially enhancing
the speed, quality, and standardization of radiotherapy treatment planning.

ÊëòË¶ÅÔºöÊîæÂ∞ÑÊ≤ªÁôÇÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑÁôåÁóáÊ≤ªÁôÇÊñπÊ≥ïÔºåÈúÄË¶ÅÁ≤æÁ¢∫ÁöÑË¶èÂäÉ‰æÜÂπ≥Ë°°ËÖ´Áò§Ê†πÈô§ÂíåÂÅ•Â∫∑ÁµÑÁπîÁöÑ‰øùÁïô„ÄÇÂÇ≥Áµ±ÁöÑÊ≤ªÁôÇË¶èÂäÉÔºàTPÔºâÊòØÂèçË¶ÜÁöÑ„ÄÅËÄóÊôÇÁöÑÔºå‰∏¶‰∏î‰æùË≥¥Êñº‰∫∫ÁÇ∫Â∞àÊ•≠Áü•Ë≠òÔºåÈÄôÂèØËÉΩÊúÉÂºïÂÖ•ËÆäÁï∞ÊÄßÂíå‰ΩéÊïàÁéá„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂Ôºå‰ΩøÁî®Â∞ëÊ¨°Âº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÂ∞áÂ§ßÂûãÂ§öÊ®°ÊÖãÂü∫Á§éÊ®°Âûã (MLM) ËΩâÊèõÁÇ∫ TP ÁöÑÂãï‰ΩúÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂà©Áî®‰∫Ü MLM Â∞çÁâ©ÁêÜ„ÄÅËºªÂ∞ÑÂíåËß£ÂâñÂ≠∏ÁöÑÂª£Ê≥õÈ†êÂÖàÂ≠òÂú®ÁöÑÁü•Ë≠òÔºå‰∏¶ÈÄöÈÅéÂ∞ëÊ¨°Â≠∏ÁøíÈÅéÁ®ãÂ∞çÂÖ∂ÈÄ≤Ë°åÂ¢ûÂº∑„ÄÇÈÄôÂÖÅË®±Ê®°Âûã‰ΩøÁî®ËíôÁâπÂç°ÁæÖÊ®°Êì¨Âô®ÂèçË¶ÜÊîπÈÄ≤Ê≤ªÁôÇË®àÂäÉ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÂú®Ë≥™ÈáèÂíåÊïàÁéáÊñπÈù¢ÈÉΩÂÑ™ÊñºÂü∫ÊñºÂÇ≥Áµ± RL ÁöÑÊñπÊ≥ïÔºåÂú®Â∞çÂâçÂàóËÖ∫ÁôåÊï∏ÊìöÈÄ≤Ë°åÊ®°Êì¨ÊôÇÔºåÁç≤Âæó‰∫ÜÊõ¥È´òÁöÑÁçéÂãµÂàÜÊï∏ÂíåÊõ¥ÂÑ™ÂåñÁöÑÂäëÈáèÂàÜ‰Ωà„ÄÇÈÄôÂÄãÊ¶ÇÂøµÈ©óË≠âË°®Êòé‰∫Ü‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊñπÂêëÔºåÂç≥Â∞áÂÖàÈÄ≤ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊ®°ÂûãÊï¥ÂêàÂà∞Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÂæûËÄåÊúâÂèØËÉΩÊèêÈ´òÊîæÂ∞ÑÊ≤ªÁôÇË®àÂäÉÁöÑÈÄüÂ∫¶„ÄÅË≥™ÈáèÂíåÊ®ôÊ∫ñÂåñ„ÄÇ

##### **Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning**
2502.04399v1 by Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang

Advances in artificial intelligence (AI) including foundation models (FMs),
are increasingly transforming human society, with smart city driving the
evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as
a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities.
In particular, ride-hailing vehicles can effectively facilitate flexible data
collection and contribute towards urban intelligence, despite resource
limitations. Therefore, this work explores a promising scenario, where
edge-assisted vehicles perform joint tasks of order serving and the emerging
foundation model fine-tuning using various urban data. However, integrating the
VCS AI task with the conventional order serving task is challenging, due to
their inconsistent spatio-temporal characteristics: (i) The distributions of
ride orders and data point-of-interests (PoIs) may not coincide in geography,
both following a priori unknown patterns; (ii) they have distinct forms of
temporal effects, i.e., prolonged waiting makes orders become instantly invalid
while data with increased staleness gradually reduces its utility for model
fine-tuning.To overcome these obstacles, we propose an online framework based
on multi-agent reinforcement learning (MARL) with careful augmentation. A new
quality-of-service (QoS) metric is designed to characterize and balance the
utility of the two joint tasks, under the effects of varying data volumes and
staleness. We also integrate graph neural networks (GNNs) with MARL to enhance
state representations, capturing graph-structured, time-varying dependencies
among vehicles and across locations. Extensive experiments on our testbed
simulator, utilizing various real-world foundation model fine-tuning tasks and
the New York City Taxi ride order dataset, demonstrate the advantage of our
proposed method.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÁöÑÈÄ≤Â±ïÔºåÂåÖÊã¨Âü∫Á§éÊ®°ÂûãÔºàFMÔºâÔºåÊ≠£Êó•ÁõäËΩâËÆä‰∫∫È°ûÁ§æÊúÉÔºåÊô∫ÊÖßÂüéÂ∏ÇÊé®ÂãïËëóÂüéÂ∏ÇÁîüÊ¥ªÁöÑÊºîÈÄ≤„ÄÇÂêåÊôÇÔºåËªäËºõÁæ§ÊÑüÊ∏¨ÔºàVCSÔºâÂ∑≤ÊàêÁÇ∫ÈóúÈçµÊé®ÂãïÂõ†Á¥†ÔºåÂà©Áî®ËªäËºõÁöÑÊ©üÂãïÊÄßÂíåÈÖçÂÇôÊÑüÊ∏¨Âô®ÁöÑËÉΩÂäõ„ÄÇÁâπÂà•ÊòØÔºåÂÑòÁÆ°ÊúâË≥áÊ∫êÈôêÂà∂ÔºåÂè´ËªäÊúçÂãôËªäËºõËÉΩÊúâÊïà‰øÉÈÄ≤ÈùàÊ¥ªÁöÑË≥áÊñôÊî∂ÈõÜÔºå‰∏¶ÊúâÂä©ÊñºÂüéÂ∏ÇÊô∫ÊÖß„ÄÇÂõ†Ê≠§ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Á¥¢‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÂ†¥ÊôØÔºåÂÖ∂‰∏≠ÈÇäÁ∑£ËºîÂä©ËªäËºõÂü∑Ë°åË®ÇÂñÆÊúçÂãôÂíåÊñ∞ËààÂü∫Á§éÊ®°ÂûãÂæÆË™øÁöÑËÅØÂêà‰ªªÂãôÔºå‰ΩøÁî®ÂêÑÁ®ÆÂüéÂ∏ÇË≥áÊñô„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº VCS AI ‰ªªÂãôËàáÂÇ≥Áµ±Ë®ÇÂñÆÊúçÂãô‰ªªÂãôÁöÑ‰∏ç‰∏ÄËá¥ÊôÇÁ©∫ÁâπÂæµÔºåÊï¥ÂêàÂÆÉÂÄëÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºö(i) Âè´ËªäË®ÇÂñÆÂíåË≥áÊñôÊÑüËààË∂£Èªû (PoI) ÁöÑÂàÜ‰ΩàÂú®Âú∞Âüü‰∏äÂèØËÉΩ‰∏çÈáçÂêàÔºåÂÖ©ËÄÖÈÉΩÈÅµÂæ™ÂÖàÈ©óÊú™Áü•ÁöÑÊ®°ÂºèÔºõ(ii) ÂÆÉÂÄëÂÖ∑Êúâ‰∏çÂêåÁöÑÊôÇÈñìÊïàÊáâÂΩ¢ÂºèÔºåÂç≥Èï∑ÊôÇÈñìÁ≠âÂæÖÊúÉ‰ΩøË®ÇÂñÆÁ´ãÂç≥Â§±ÊïàÔºåËÄåÈÅéÊôÇÁöÑË≥áÊñôÊúÉÈÄêÊº∏Èôç‰ΩéÂÖ∂Â∞çÊ®°ÂûãÂæÆË™øÁöÑÊïàÁî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈöúÁ§ôÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§öÊô∫ËÉΩÈ´îÂº∑ÂåñÂ≠∏Áøí (MARL) ÁöÑÁ∑ö‰∏äÊû∂ÊßãÔºå‰∏¶ÈÄ≤Ë°å‰∫Ü‰ªîÁ¥∞ÁöÑÊì¥ÂÖÖ„ÄÇË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊúçÂãôÂìÅË≥™ (QoS) ÊåáÊ®ôÔºåÁî®ÊñºË°®ÂæµÂíåÂπ≥Ë°°ÈÄôÂÖ©ÂÄãËÅØÂêà‰ªªÂãôÁöÑÊïàÁî®ÔºåÂú®‰∏çÂêåË≥áÊñôÈáèÂíåÈÅéÊôÇÊÄßÁöÑÂΩ±Èüø‰∏ã„ÄÇÊàëÂÄëÈÇÑÂ∞áÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâËàá MARL Êï¥ÂêàÔºå‰ª•Â¢ûÂº∑ÁãÄÊÖãË°®Á§∫ÔºåÊçïÊçâËªäËºõ‰πãÈñìÂíå‰∏çÂêåÂú∞Èªû‰πãÈñìÁöÑÂúñÁµêÊßã„ÄÅÊôÇËÆä‰æùË≥¥ÊÄß„ÄÇÂú®ÊàëÂÄëÁöÑÊ∏¨Ë©¶Âπ≥Âè∞Ê®°Êì¨Âô®‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÔºåÂà©Áî®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåÁöÑÂü∫Á§éÊ®°ÂûãÂæÆË™ø‰ªªÂãôÂíåÁ¥êÁ¥ÑÂ∏ÇË®àÁ®ãËªäÂè´ËªäË®ÇÂñÆË≥áÊñôÈõÜÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÂÑ™Èªû„ÄÇ

##### **Multimodal Medical Code Tokenizer**
2502.04397v1 by Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik

Foundation models trained on patient electronic health records (EHRs) require
tokenizing medical data into sequences of discrete vocabulary items. Existing
tokenizers treat medical codes from EHRs as isolated textual tokens. However,
each medical code is defined by its textual description, its position in
ontological hierarchies, and its relationships to other codes, such as disease
co-occurrences and drug-treatment associations. Medical vocabularies contain
more than 600,000 codes with critical information for clinical reasoning. We
introduce MedTok, a multimodal medical code tokenizer that uses the text
descriptions and relational context of codes. MedTok processes text using a
language model encoder and encodes the relational structure with a graph
encoder. It then quantizes both modalities into a unified token space,
preserving modality-specific and cross-modality information. We integrate
MedTok into five EHR models and evaluate it on operational and clinical tasks
across in-patient and out-patient datasets, including outcome prediction,
diagnosis classification, drug recommendation, and risk stratification.
Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR
models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with
the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate
using MedTok tokenizer with medical QA systems. Our results demonstrate the
potential of MedTok as a unified tokenizer for medical codes, improving
tokenization for medical foundation models.

ÊëòË¶ÅÔºö<paragraph>Âú®ÊÇ£ËÄÖÈõªÂ≠êÁóÖÊ≠∑ (EHR) ‰∏äË®ìÁ∑¥ÁöÑÂü∫Á§éÊ®°ÂûãÈúÄË¶ÅÂ∞áÈÜ´ÁôÇË≥áÊñô‰ª£ÊèõÊàêÈõ¢Êï£Ë©ûÂΩôÈ†ÖÁõÆÂ∫èÂàó„ÄÇÁèæÊúâ‰ª£ÊèõÂô®Â∞á EHR ‰∏≠ÁöÑÈÜ´ÁôÇ‰ª£Á¢ºË¶ñÁÇ∫Â≠§Á´ãÁöÑÊñáÂ≠ó‰ª£Á¢º„ÄÇÁÑ∂ËÄåÔºåÊØèÂÄãÈÜ´ÁôÇ‰ª£Á¢ºÈÉΩÁî±ÂÖ∂ÊñáÂ≠óÊèèËø∞„ÄÅÂú®Êú¨‰ΩìÂ±§Á¥ö‰∏≠ÁöÑ‰ΩçÁΩÆÔºå‰ª•ÂèäËàáÂÖ∂‰ªñ‰ª£Á¢ºÁöÑÈóúËÅØÊÄßÔºà‰æãÂ¶ÇÁñæÁóÖÂÖ±ÁèæÂíåËó•Áâ©Ê≤ªÁôÇÈóúËÅØÊÄßÔºâÂÆöÁæ©„ÄÇÈÜ´ÁôÇË©ûÂΩôÂåÖÂê´Ë∂ÖÈÅé 600,000 ÂÄã‰ª£Á¢ºÔºåÂÖ∂‰∏≠ÂåÖÂê´Ëá®Â∫äÊé®ÁêÜÁöÑÈáçË¶ÅË≥áË®ä„ÄÇÊàëÂÄë‰ªãÁ¥π MedTokÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÊ®°ÊÖãÈÜ´ÁôÇ‰ª£Á¢º‰ª£ÊèõÂô®ÔºåÂÆÉ‰ΩøÁî®‰ª£Á¢ºÁöÑÊñáÂ≠óÊèèËø∞ÂíåÈóúËÅØÊÄßËÑàÁµ°„ÄÇMedTok ‰ΩøÁî®Ë™ûË®ÄÊ®°ÂûãÁ∑®Á¢ºÂô®ËôïÁêÜÊñáÂ≠óÔºå‰∏¶‰ΩøÁî®ÂúñÂΩ¢Á∑®Á¢ºÂô®Á∑®Á¢ºÈóúËÅØÁµêÊßã„ÄÇÁÑ∂ÂæåÔºåÂÆÉÂ∞áÂÖ©Á®ÆÊ®°ÊÖãÈáèÂåñÁÇ∫Áµ±‰∏ÄÁöÑ‰ª£Á¢ºÁ©∫ÈñìÔºå‰øùÁïôÁâπÂÆöÊñºÊ®°ÊÖãÂíåË∑®Ê®°ÊÖãÁöÑË≥áË®ä„ÄÇÊàëÂÄëÂ∞á MedTok Êï¥ÂêàÂà∞‰∫îÂÄã EHR Ê®°Âûã‰∏≠Ôºå‰∏¶Âú®ÂåÖÊã¨ÁµêÊûúÈ†êÊ∏¨„ÄÅË®∫Êñ∑ÂàÜÈ°û„ÄÅËó•Áâ©Âª∫Ë≠∞ÂíåÈ¢®Èö™ÂàÜÂ±§Âú®ÂÖßÁöÑ‰ΩèÈô¢ÂíåÈñÄË®∫Ë≥áÊñôÈõÜ‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÈÅã‰ΩúÂíåËá®Â∫ä‰ªªÂãôË©ï‰º∞„ÄÇÂ∞áÊ®ôÊ∫ñ EHR ‰ª£ÊèõÂô®ÊèõÊàê MedTok ÂèØÊîπÂñÑÊâÄÊúâ EHR Ê®°ÂûãÁöÑ AUPRCÔºåÂú® MIMIC-III ‰∏äÊèêÈ´ò 4.10%ÔºåÂú® MIMIC-IV ‰∏äÊèêÈ´ò 4.78%ÔºåÂú® EHRShot ‰∏äÊèêÈ´ò 11.30%ÔºåÂÖ∂‰∏≠Ëó•Áâ©Âª∫Ë≠∞ÁöÑÈÄ≤Ê≠•ÊúÄÂ§ß„ÄÇÈô§‰∫Ü EHR Âª∫Ê®°‰πãÂ§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî® MedTok ‰ª£ÊèõÂô®Êê≠ÈÖçÈÜ´ÁôÇ QA Á≥ªÁµ±„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫Ü MedTok ‰ΩúÁÇ∫ÈÜ´ÁôÇ‰ª£Á¢ºÁµ±‰∏Ä‰ª£ÊèõÂô®ÁöÑÊΩõÂäõÔºåÊîπÂñÑ‰∫ÜÈÜ´ÁôÇÂü∫Á§éÊ®°ÂûãÁöÑ‰ª£Êèõ„ÄÇ</paragraph>

##### **A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma**
2502.03772v1 by Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng, Hui Huang, Lida Chen, Wei Wang, Qinghua Huang

Hepatocellular carcinoma (HCC) ranks as the third leading cause of
cancer-related mortality worldwide, with early detection being crucial for
improving patient survival rates. However, early screening for HCC using
ultrasound suffers from insufficient sensitivity and is highly dependent on the
expertise of radiologists for interpretation. Leveraging the latest
advancements in artificial intelligence (AI) in medical imaging, this study
proposes an innovative Hierarchical Sparse Query Transformer (HSQformer) model
that combines the strengths of Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) to enhance the accuracy of HCC diagnosis in ultrasound
screening. The HSQformer leverages sparse latent space representations to
capture hierarchical details at various granularities without the need for
complex adjustments, and adopts a modular, plug-and-play design philosophy,
ensuring the model's versatility and ease of use. The HSQformer's performance
was rigorously tested across three distinct clinical scenarios: single-center,
multi-center, and high-risk patient testing. In each of these settings, it
consistently outperformed existing state-of-the-art models, such as ConvNext
and SwinTransformer. Notably, the HSQformer even matched the diagnostic
capabilities of senior radiologists and comprehensively surpassed those of
junior radiologists. The experimental results from this study strongly
demonstrate the effectiveness and clinical potential of AI-assisted tools in
HCC screening. The full code is available at
https://github.com/Asunatan/HSQformer.

ÊëòË¶ÅÔºöËÇùÁ¥∞ËÉûÁôåÔºàHCCÔºâÊòØÂÖ®ÁêÉÁ¨¨‰∏âÂ§ßÁôåÁóáÁõ∏ÈóúÊ≠ª‰∫°ÂéüÂõ†ÔºåÊó©ÊúüÊ™¢Ê∏¨Â∞çÊñºÊèêÈ´òÊÇ£ËÄÖÂ≠òÊ¥ªÁéáËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®Ë∂ÖÈü≥Ê≥¢ÈÄ≤Ë°å HCC Êó©ÊúüÁØ©Ê™¢ÁöÑÈùàÊïèÂ∫¶‰∏çË∂≥Ôºå‰∏îÈ´òÂ∫¶‰æùË≥¥ÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑÂ∞àÊ•≠Áü•Ë≠òÈÄ≤Ë°åÂà§ËÆÄ„ÄÇÊú¨Á†îÁ©∂Âà©Áî®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂàÜÂ±§Á®ÄÁñèÊü•Ë©¢TransformerÔºàHSQformerÔºâÊ®°ÂûãÔºåÁµêÂêà‰∫ÜÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàCNNÔºâÂíåË¶ñË¶∫TransformerÔºàViTÔºâÁöÑÂÑ™ÈªûÔºå‰ª•ÊèêÈ´òË∂ÖÈü≥Ê≥¢ÁØ©Ê™¢‰∏≠ HCC Ë®∫Êñ∑ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇHSQformer Âà©Áî®Á®ÄÁñèÊΩõÂú®Á©∫ÈñìË°®Á§∫ÔºåÂú®‰∏çÈúÄË¶ÅË§áÈõúË™øÊï¥ÁöÑÊÉÖÊ≥Å‰∏ãÊì∑ÂèñÂêÑÁ®ÆÁ≤íÂ∫¶Â±§Á¥öÁöÑÁ¥∞ÁØÄÔºå‰∏¶Êé°Áî®Ê®°ÁµÑÂåñ„ÄÅÂç≥ÊèíÂç≥Áî®ÁöÑË®≠Ë®àÁêÜÂøµÔºåÁ¢∫‰øùÊ®°ÂûãÁöÑÂ§öÂäüËÉΩÊÄßÂíåÊòìÁî®ÊÄß„ÄÇHSQformer ÁöÑÊïàËÉΩÁ∂ìÈÅé‰∏âÂÄã‰∏çÂêåÁöÑËá®Â∫äÂ†¥ÊôØÁöÑÂö¥Ê†ºÊ∏¨Ë©¶ÔºöÂñÆ‰∏≠ÂøÉ„ÄÅÂ§ö‰∏≠ÂøÉÂíåÈ´òÈ¢®Èö™ÊÇ£ËÄÖÊ∏¨Ë©¶„ÄÇÂú®ÈÄô‰∫õË®≠ÂÆö‰∏≠ÔºåÂÆÉÂßãÁµÇÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Ê®°ÂûãÔºå‰æãÂ¶Ç ConvNext Âíå SwinTransformer„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåHSQformer ÁîöËá≥ÂåπÈÖç‰∫ÜË≥áÊ∑±ÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑË®∫Êñ∑ËÉΩÂäõÔºå‰∏¶ÂÖ®Èù¢Ë∂ÖË∂ä‰∫ÜÂàùÁ¥öÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑË®∫Êñ∑ËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÁöÑÂØ¶È©óÁµêÊûúÊúâÂäõÂú∞Ë≠âÊòé‰∫Ü AI ËºîÂä©Â∑•ÂÖ∑Âú® HCC ÁØ©Ê™¢‰∏≠ÁöÑÊúâÊïàÊÄßÂíåËá®Â∫äÊΩõÂäõ„ÄÇÂÆåÊï¥Á®ãÂºèÁ¢ºÂèØÂú® https://github.com/Asunatan/HSQformer ÂèñÂæó„ÄÇ

##### **Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings**
2502.04386v1 by Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, Vishwa S. Parekh

Self-supervised learning has revolutionized medical imaging by enabling
efficient and generalizable feature extraction from large-scale unlabeled
datasets. Recently, self-supervised foundation models have been extended to
three-dimensional (3D) computed tomography (CT) data, generating compact,
information-rich embeddings with 1408 features that achieve state-of-the-art
performance on downstream tasks such as intracranial hemorrhage detection and
lung cancer risk forecasting. However, these embeddings have been shown to
encode demographic information, such as age, sex, and race, which poses a
significant risk to the fairness of clinical applications.
  In this work, we propose a Variation Autoencoder (VAE) based adversarial
debiasing framework to transform these embeddings into a new latent space where
demographic information is no longer encoded, while maintaining the performance
of critical downstream tasks. We validated our approach on the NLST lung cancer
screening dataset, demonstrating that the debiased embeddings effectively
eliminate multiple encoded demographic information and improve fairness without
compromising predictive accuracy for lung cancer risk at 1-year and 2-year
intervals. Additionally, our approach ensures the embeddings are robust against
adversarial bias attacks. These results highlight the potential of adversarial
debiasing techniques to ensure fairness and equity in clinical applications of
self-supervised 3D CT embeddings, paving the way for their broader adoption in
unbiased medical decision-making.

ÊëòË¶ÅÔºöËá™ÊàëÁõ£Áù£Â≠∏ÁøíÈÄèÈÅéÂæûÂ§ßË¶èÊ®°Êú™Ê®ôË®òË≥áÊñôÈõÜ‰∏≠ÊèêÂèñÊúâÊïà‰∏îÂèØÊ¶ÇÂåñÁöÑÁâπÂæµÔºåÈÄ≤ËÄåÈù©Êñ∞‰∫ÜÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÊúÄËøëÔºåËá™ÊàëÁõ£Áù£Âü∫Á§éÊ®°ÂûãÂ∑≤Êì¥Â±ïÂà∞‰∏âÁ∂≠ (3D) ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) Ë≥áÊñôÔºåÁî¢ÁîüÁ∑äÊπä„ÄÅË≥áË®äË±êÂØåÁöÑÂµåÂÖ•ÔºåÂåÖÂê´ 1408 ÂÄãÁâπÂæµÔºåÂú®È°±ÂÖßÂá∫Ë°ÄÂÅµÊ∏¨ÂíåËÇ∫ÁôåÈ¢®Èö™È†êÊ∏¨Á≠â‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂµåÂÖ•Â∑≤Ë¢´Ë≠âÊòéÊúÉÁ∑®Á¢º‰∫∫Âè£Áµ±Ë®àË≥áË®äÔºå‰æãÂ¶ÇÂπ¥ÈΩ°„ÄÅÊÄßÂà•ÂíåÁ®ÆÊóèÔºåÈÄôÂ∞çËá®Â∫äÊáâÁî®ÁöÑÂÖ¨Âπ≥ÊÄßÊßãÊàêÈáçÂ§ßÈ¢®Èö™„ÄÇ
Âú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºËÆäÁï∞Ëá™Á∑®Á¢ºÂô® (VAE) ÁöÑÂ∞çÊäóÊÄßÂéªÂÅèÊ°ÜÊû∂ÔºåÂ∞áÈÄô‰∫õÂµåÂÖ•ËΩâÊèõÂà∞‰∏ÄÂÄãÊñ∞ÁöÑÊΩõÂú®Á©∫ÈñìÔºåÂÖ∂‰∏≠‰∏çÂÜçÁ∑®Á¢º‰∫∫Âè£Áµ±Ë®àË≥áË®äÔºåÂêåÊôÇÁ∂≠ÊåÅÈóúÈçµ‰∏ãÊ∏∏‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂú® NLST ËÇ∫ÁôåÁØ©Ê™¢Ë≥áÊñôÈõÜ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåË≠âÊòéÂéªÂÅèÂµåÂÖ•ÊúâÊïàÊ∂àÈô§‰∫ÜÂ§öÈáçÁ∑®Á¢ºÁöÑ‰∫∫Âè£Áµ±Ë®àË≥áË®äÔºå‰∏¶Âú®‰∏çÊêçÂÆ≥ 1 Âπ¥Âíå 2 Âπ¥ÈñìÈöîÁöÑËÇ∫ÁôåÈ¢®Èö™È†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÁöÑÊÉÖÊ≥Å‰∏ãÊèêÈ´ò‰∫ÜÂÖ¨Âπ≥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁ¢∫‰øù‰∫ÜÂµåÂÖ•Â∞çÊäóÊÄßÂÅèË™§ÊîªÊìäÂÖ∑ÊúâÈ≠ØÊ£íÊÄß„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÂ∞çÊäóÊÄßÂéªÂÅèÊäÄË°ìÁöÑÊΩõÂäõÔºåÂèØÁ¢∫‰øùËá™ÊàëÁõ£Áù£ 3D CT ÂµåÂÖ•Âú®Ëá®Â∫äÊáâÁî®‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÂíåÂÖ¨Ê≠£ÊÄßÔºåÁÇ∫ÂÖ∂Âú®ÁÑ°ÂÅèË¶ãÈÜ´ÁôÇÊ±∫Á≠ñ‰∏≠ÁöÑÂª£Ê≥õÊé°Áî®Èã™Ë∑Ø„ÄÇ

##### **Clinically-Inspired Hierarchical Multi-Label Classification of Chest X-rays with a Penalty-Based Loss Function**
2502.03591v1 by Mehrdad Asadi, Komi Sodok√©, Ian J. Gerard, Marta Kersten-Oertel

In this work, we present a novel approach to multi-label chest X-ray (CXR)
image classification that enhances clinical interpretability while maintaining
a streamlined, single-model, single-run training pipeline. Leveraging the
CheXpert dataset and VisualCheXbert-derived labels, we incorporate hierarchical
label groupings to capture clinically meaningful relationships between
diagnoses. To achieve this, we designed a custom hierarchical binary
cross-entropy (HBCE) loss function that enforces label dependencies using
either fixed or data-driven penalty types. Our model achieved a mean area under
the receiver operating characteristic curve (AUROC) of 0.903 on the test set.
Additionally, we provide visual explanations and uncertainty estimations to
further enhance model interpretability. All code, model configurations, and
experiment details are made available.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ËÉ∏ÈÉ® X ÂÖâÔºàCXRÔºâÂΩ±ÂÉèÂ§öÊ®ôÁ±§ÂàÜÈ°ûÁöÑÊñ∞ÊñπÊ≥ïÔºåÂú®Á∂≠ÊåÅÁ∞°ÂåñÁöÑÂñÆ‰∏ÄÊ®°Âûã„ÄÅÂñÆÊ¨°Âü∑Ë°åË®ìÁ∑¥ÁÆ°Á∑öÁöÑÂêåÊôÇÔºåÊèêÂçáËá®Â∫äÂèØËß£ÈáãÊÄß„ÄÇÂà©Áî® CheXpert Ë≥áÊñôÈõÜÂíå VisualCheXbert Ë°çÁîüÁöÑÊ®ôÁ±§ÔºåÊàëÂÄëÁ¥çÂÖ•ÈöéÂ±§Ê®ôÁ±§Áæ§ÁµÑÔºå‰ª•Êì∑ÂèñË®∫Êñ∑‰πãÈñìÂÖ∑ÊúâËá®Â∫äÊÑèÁæ©ÁöÑÈóúËÅØÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëË®≠Ë®à‰∫ÜËá™Ë®ÇÁöÑÈöéÂ±§‰∫åÂÖÉ‰∫§ÂèâÁÜµ (HBCE) ÊêçÂ§±ÂáΩÊï∏Ôºå‰ΩøÁî®Âõ∫ÂÆöÊàñË≥áÊñôÈ©ÖÂãïÁöÑÊá≤ÁΩ∞È°ûÂûã‰æÜÂº∑Âà∂Âü∑Ë°åÊ®ôÁ±§‰æùË≥¥ÊÄß„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®Ê∏¨Ë©¶ÈõÜ‰∏äÈÅîÂà∞ÂèóË©¶ËÄÖÂ∑•‰ΩúÁâπÊÄßÊõ≤Á∑ö (AUROC) ‰∏ãÁöÑÂπ≥ÂùáÈù¢Á©çÁÇ∫ 0.903„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõË¶ñË¶∫ÂåñË™™ÊòéÂíå‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÇÊâÄÊúâÁ®ãÂºèÁ¢º„ÄÅÊ®°ÂûãÁµÑÊÖãÂíåÂØ¶È©óË©≥Á¥∞Ë≥áÊñôÁöÜÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Code Simulation as a Proxy for High-order Tasks in Large Language Models**
2502.03568v1 by Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, X. Angelo Huang, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge

Many reasoning, planning, and problem-solving tasks share an intrinsic
algorithmic nature: correctly simulating each step is a sufficient condition to
solve them correctly. We collect pairs of naturalistic and synthetic reasoning
tasks to assess the capabilities of Large Language Models (LLM). While
naturalistic tasks often require careful human handcrafting, we show that
synthetic data is, in many cases, a good proxy that is much easier to collect
at scale. We leverage common constructs in programming as the counterpart of
the building blocks of naturalistic reasoning tasks, such as straight-line
programs, code that contains critical paths, and approximate and redundant
instructions. We further assess the capabilities of LLMs on sorting problems
and repeated operations via sorting algorithms and nested loops. Our synthetic
datasets further reveal that while the most powerful LLMs exhibit relatively
strong execution capabilities, the process is fragile: it is negatively
affected by memorisation and seems to rely heavily on pattern recognition. Our
contribution builds upon synthetically testing the reasoning capabilities of
LLMs as a scalable complement to handcrafted human-annotated problems.

ÊëòË¶ÅÔºöË®±Â§öÊé®ÁêÜ„ÄÅË¶èÂäÉÂíåÂïèÈ°åËß£Ê±∫‰ªªÂãôÂÖ±‰∫´‰∏ÄÂÄãÂÖßÂú®ÁöÑÊºîÁÆóÊ≥ïÊÄßË≥™ÔºöÊ≠£Á¢∫Ê®°Êì¨ÊØè‰∏ÄÊ≠•Â∞±Ë∂≥‰ª•Ê≠£Á¢∫Ëß£Ê±∫ÂÆÉÂÄë„ÄÇÊàëÂÄëÊî∂ÈõÜËá™ÁÑ∂‰∏ªÁæ©ÂíåÂêàÊàêÊé®ÁêÜ‰ªªÂãôÂ∞çÔºå‰ª•Ë©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂäüËÉΩ„ÄÇÈõñÁÑ∂Ëá™ÁÑ∂‰∏ªÁæ©‰ªªÂãôÈÄöÂ∏∏ÈúÄË¶Å‰ªîÁ¥∞ÁöÑ‰∫∫Â∑•Ë£Ω‰ΩúÔºå‰ΩÜÊàëÂÄëË°®ÊòéÂú®Ë®±Â§öÊÉÖÊ≥Å‰∏ãÔºåÂêàÊàêË≥áÊñôÊòØ‰∏ÄÂÄãÂæàÂ•ΩÁöÑ‰ª£ÁêÜÔºåËÄå‰∏îÊõ¥ÂÆπÊòìÂ§ßË¶èÊ®°Êî∂ÈõÜ„ÄÇÊàëÂÄëÂà©Áî®Á®ãÂºèË®≠Ë®à‰∏≠ÁöÑÂ∏∏Ë¶ãÂª∫ÊßãÔºå‰ΩúÁÇ∫Ëá™ÁÑ∂‰∏ªÁæ©Êé®ÁêÜ‰ªªÂãôÊßãÂª∫ÂçÄÂ°äÁöÑÂ∞çÊáâÁâ©Ôºå‰æãÂ¶ÇÁõ¥Á∑öÁ®ãÂºè„ÄÅÂåÖÂê´ÈóúÈçµË∑ØÂæëÁöÑÁ®ãÂºèÁ¢ºÔºå‰ª•ÂèäËøë‰ººÂíåÂÜóÈ§òÊåá‰ª§„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë©ï‰º∞ LLM Âú®ÊéíÂ∫èÂïèÈ°åÂíåÈáçË§áÈÅãÁÆó‰∏äÁöÑÂäüËÉΩÔºåÈÄèÈÅéÊéíÂ∫èÊºîÁÆóÊ≥ïÂíåÂ∑¢ÁãÄËø¥Âúà„ÄÇÊàëÂÄëÁöÑÂêàÊàêË≥áÊñôÈõÜÈÄ≤‰∏ÄÊ≠•Êè≠Á§∫ÔºåÈõñÁÑ∂ÊúÄÂº∑Â§ßÁöÑ LLM Ë°®ÁèæÂá∫Áõ∏Â∞çÂº∑Â§ßÁöÑÂü∑Ë°åËÉΩÂäõÔºå‰ΩÜÈÄôÂÄãÈÅéÁ®ãÂæàËÑÜÂº±ÔºöÂÆÉÂèóÂà∞Ë®òÊÜ∂ÁöÑË≤†Èù¢ÂΩ±ÈüøÔºåËÄå‰∏î‰ºº‰πéÂö¥Èáç‰æùË≥¥Ê®°ÂºèËæ®Ë≠ò„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂª∫Á´ãÂú®‰ª•ÂêàÊàêÊñπÂºèÊ∏¨Ë©¶ LLM ÁöÑÊé®ÁêÜËÉΩÂäõ‰πã‰∏äÔºå‰ΩúÁÇ∫ÊâãÂ∑•Á∑®ÂØ´‰∫∫È°ûÊ®ôË®ªÂïèÈ°åÁöÑÂèØÊì¥ÂÖÖË£úÂÖÖ„ÄÇ

##### **Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning**
2502.04381v1 by Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, Danilo Bernardo

Large Language Models (LLMs) have attained human-level accuracy on medical
question-answer (QA) benchmarks. However, their limitations in navigating
open-ended clinical scenarios have recently been shown, raising concerns about
the robustness and generalizability of LLM reasoning across diverse, real-world
medical tasks. To probe potential LLM failure modes in clinical
problem-solving, we present the medical abstraction and reasoning corpus
(M-ARC). M-ARC assesses clinical reasoning through scenarios designed to
exploit the Einstellung effect -- the fixation of thought arising from prior
experience, targeting LLM inductive biases toward inflexible pattern matching
from their training data rather than engaging in flexible reasoning. We find
that LLMs, including current state-of-the-art o1 and Gemini models, perform
poorly compared to physicians on M-ARC, often demonstrating lack of commonsense
medical reasoning and a propensity to hallucinate. In addition, uncertainty
estimation analyses indicate that LLMs exhibit overconfidence in their answers,
despite their limited accuracy. The failure modes revealed by M-ARC in LLM
medical reasoning underscore the need to exercise caution when deploying these
models in clinical settings.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÈÜ´ÁôÇÂïèÈ°åËß£Á≠î (QA) Âü∫Ê∫ñ‰∏äÈÅîÂà∞‰∫∫È°ûÂ±§Á¥öÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÊáâÂ∞çÈñãÊîæÂºèËá®Â∫äÂ†¥ÊôØ‰∏≠ÁöÑÂ±ÄÈôêÊÄßÊúÄËøëÂ∑≤Ë¢´Êè≠Á§∫ÔºåÂºïÁôº‰∫Ü‰∫∫ÂÄëÂ∞ç LLM Êé®ÁêÜÂú®Â§öÊ®£Âåñ„ÄÅÁúüÂØ¶‰∏ñÁïåÈÜ´ÁôÇ‰ªªÂãô‰∏≠ÁöÑÁ©©ÂÅ•ÊÄßÂíåÊ¶ÇÊã¨ÊÄßÁöÑÊìîÊÜÇ„ÄÇÁÇ∫‰∫ÜÊé¢Ë®éËá®Â∫äÂïèÈ°åËß£Ê±∫‰∏≠ LLM ÁöÑÊΩõÂú®ÊïÖÈöúÊ®°ÂºèÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈÜ´ÁôÇÊäΩË±°ÂíåÊé®ÁêÜË™ûÊñôÂ∫´ (M-ARC)„ÄÇM-ARC ÈÄöÈÅéÊó®Âú®Âà©Áî®ËâæË≥ìÊµ©ÊñØÈåØË¶∫ÔºàÁî±ÂÖàÂâçÁ∂ìÈ©óÁî¢ÁîüÁöÑÊÄùÁ∂≠ÂÆöÂã¢Ôºâ‰æÜË©ï‰º∞Ëá®Â∫äÊé®ÁêÜÔºåÈáùÂ∞ç LLM Ê≠∏Á¥çÂÅèË™§Ôºå‰ΩøÂÖ∂ÂæûË®ìÁ∑¥Êï∏Êìö‰∏≠ÈÄ≤Ë°åÂÉµÂåñÁöÑÊ®°ÂºèÂåπÈÖçÔºåËÄå‰∏çÊòØÈÄ≤Ë°åÈùàÊ¥ªÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂåÖÊã¨Áï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑ o1 Âíå Gemini Ê®°ÂûãÂú®ÂÖßÁöÑ LLMÔºåÂú® M-ARC ‰∏äÁöÑË°®ÁèæÈÅ†‰∏çÂ¶ÇÈÜ´ÁîüÔºåÂÆÉÂÄëÁ∂ìÂ∏∏Ë°®ÁèæÂá∫Áº∫‰πèÂ∏∏Ë≠òÊÄßÁöÑÈÜ´ÁôÇÊé®ÁêÜÂíåÁî¢ÁîüÂπªË¶∫ÁöÑÂÇæÂêë„ÄÇÊ≠§Â§ñÔºå‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÂàÜÊûêË°®ÊòéÔºåÂÑòÁÆ° LLM Ê∫ñÁ¢∫ÊÄßÊúâÈôêÔºå‰ΩÜÂÆÉÂÄëÂ∞çËá™Â∑±ÁöÑÁ≠îÊ°àË°®ÁèæÂá∫ÈÅéÂ∫¶Ëá™‰ø°„ÄÇM-ARC Êè≠Á§∫ÁöÑ LLM ÈÜ´ÁôÇÊé®ÁêÜÊïÖÈöúÊ®°ÂºèÂº∑Ë™ø‰∫ÜÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÈÉ®ÁΩ≤ÈÄô‰∫õÊ®°ÂûãÊôÇÈúÄË¶ÅË¨πÊÖé„ÄÇ

##### **Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**
2502.03396v1 by Sarah Al-Shareeda, Yasar Celik, Bilge Bilgili, Ahmed Al-Dubai, Berk Canberk

Creating a Digital Twin (DT) for Healthcare Intelligent Transportation
Systems (HITS) is a hot research trend focusing on enhancing HITS management,
particularly in emergencies where ambulance vehicles must arrive at the crash
scene on time and track their real-time location is crucial to the medical
authorities. Despite the claim of real-time representation, a temporal
misalignment persists between the physical and virtual domains, leading to
discrepancies in the ambulance's location representation. This study proposes
integrating AI predictive models, specifically Support Vector Regression (SVR)
and Deep Neural Networks (DNN), within a constructed mock DT data pipeline
framework to anticipate the medical vehicle's next location in the virtual
world. These models align virtual representations with their physical
counterparts, i.e., metaphorically offsetting the synchronization delay between
the two worlds. Trained meticulously on a historical geospatial dataset, SVR
and DNN exhibit exceptional prediction accuracy in MATLAB and Python
environments. Through various testing scenarios, we visually demonstrate the
efficacy of our methodology, showcasing SVR and DNN's key role in significantly
reducing the witnessed gap within the HITS's DT. This transformative approach
enhances real-time synchronization in emergency HITS by approximately 88% to
93%.

ÊëòË¶ÅÔºöÂª∫Á´ãÈÜ´ÁôÇÊô∫ÊÖß‰∫§ÈÄöÁ≥ªÁµ±ÔºàHITSÔºâÁöÑÊï∏‰ΩçÂàÜË∫´ÔºàDTÔºâÊòØÁÜ±ÈñÄÁöÑÁ†îÁ©∂Ë∂®Âã¢ÔºåÂÖ∂ÈáçÈªûÂú®ÊñºÊèêÂçá HITS ÁÆ°ÁêÜÔºåÁâπÂà•ÊòØÂú®ÊïëË≠∑ËªäÂøÖÈ†àÊ∫ñÊôÇÊäµÈÅîËªäÁ¶çÁèæÂ†¥ÁöÑÁ∑äÊÄ•ÊÉÖÊ≥Å‰∏≠ÔºåËøΩËπ§ÂÖ∂Âç≥ÊôÇ‰ΩçÁΩÆÂ∞çÊñºÈÜ´ÁôÇÂñÆ‰ΩçËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°ËÅ≤Á®±Âç≥ÊôÇÂëàÁèæÔºå‰ΩÜÂØ¶È´îÂíåËôõÊì¨È†òÂüü‰πãÈñì‰ªçÂ≠òÂú®ÊôÇÈñì‰∏äÁöÑÈåØ‰ΩçÔºåÂ∞éËá¥ÊïëË≠∑Ëªä‰ΩçÁΩÆÂëàÁèæ‰∏äÁöÑÂ∑ÆÁï∞„ÄÇÊú¨Á†îÁ©∂Âª∫Ë≠∞Âú®Âª∫ÊßãÁöÑËôõÊì¨ DT Ë≥áÊñôÁÆ°ÈÅìÊû∂Êßã‰∏≠Êï¥Âêà‰∫∫Â∑•Êô∫ÊÖßÈ†êÊ∏¨Ê®°ÂûãÔºåÁâπÂà•ÊòØÊîØÊè¥ÂêëÈáèÂõûÊ≠∏ÔºàSVRÔºâÂíåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºàDNNÔºâÔºå‰ª•È†êÊ∏¨ÈÜ´ÁôÇËªäËºõÂú®ËôõÊì¨‰∏ñÁïåÁöÑ‰∏ã‰∏ÄÂÄã‰ΩçÁΩÆ„ÄÇÈÄô‰∫õÊ®°ÂûãÂ∞áËôõÊì¨ÂëàÁèæËàáÂÖ∂ÂØ¶È´îÂ∞çÊáâÁâ©Â∞çÈΩäÔºå‰πüÂ∞±ÊòØË™™ÔºåÂú®ÂÖ©ÂÄã‰∏ñÁïå‰πãÈñìÊØîÂñªÊÄßÂú∞ÊäµÈä∑ÂêåÊ≠•Âª∂ÈÅ≤„ÄÇÂú®Ê≠∑Âè≤Âú∞ÁêÜÁ©∫ÈñìË≥áÊñôÈõÜ‰∏äÁ∂ìÈÅé‰ªîÁ¥∞Ë®ìÁ∑¥ÔºåSVR Âíå DNN Âú® MATLAB Âíå Python Áí∞Â¢É‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄß„ÄÇÈÄèÈÅéÂêÑÁ®ÆÊ∏¨Ë©¶ÊÉÖÂ¢ÉÔºåÊàëÂÄëË¶ñË¶∫ÂåñÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊñπÊ≥ïË´ñÁöÑÊïàËÉΩÔºåÂ±ïÁ§∫‰∫Ü SVR Âíå DNN Âú®È°ØËëóÁ∏ÆÂ∞è HITS ÁöÑ DT ‰∏≠Ë¶ãË≠âÂà∞ÁöÑÂ∑ÆË∑ùÊñπÈù¢ÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇÈÄôÁ®ÆËÆäÈù©ÊÄßÁöÑÊñπÊ≥ïÂ∞áÁ∑äÊÄ• HITS ‰∏≠ÁöÑÂç≥ÊôÇÂêåÊ≠•ÊèêÂçá‰∫ÜÂ§ßÁ¥Ñ 88% Âà∞ 93%„ÄÇ

##### **RadVLM: A Multitask Conversational Vision-Language Model for Radiology**
2502.03333v1 by Nicolas Deperrois, Hidetoshi Matsuo, Samuel Ruip√©rez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas M. Sutter, Julia E. Vogt, Jonas Kluckert, Thomas Frauenfelder, Christian Bl√ºthgen, Farhad Nooralahzadeh, Michael Krauthammer

The widespread use of chest X-rays (CXRs), coupled with a shortage of
radiologists, has driven growing interest in automated CXR analysis and
AI-assisted reporting. While existing vision-language models (VLMs) show
promise in specific tasks such as report generation or abnormality detection,
they often lack support for interactive diagnostic capabilities. In this work
we present RadVLM, a compact, multitask conversational foundation model
designed for CXR interpretation. To this end, we curate a large-scale
instruction dataset comprising over 1 million image-instruction pairs
containing both single-turn tasks -- such as report generation, abnormality
classification, and visual grounding -- and multi-turn, multi-task
conversational interactions. After fine-tuning RadVLM on this instruction
dataset, we evaluate it across different tasks along with re-implemented
baseline VLMs. Our results show that RadVLM achieves state-of-the-art
performance in conversational capabilities and visual grounding while remaining
competitive in other radiology tasks. Ablation studies further highlight the
benefit of joint training across multiple tasks, particularly for scenarios
with limited annotated data. Together, these findings highlight the potential
of RadVLM as a clinically relevant AI assistant, providing structured CXR
interpretation and conversational capabilities to support more effective and
accessible diagnostic workflows.

ÊëòË¶ÅÔºöËÉ∏ÈÉ® X ÂÖâ (CXR) ÁöÑÂπøÊ≥õ‰ΩøÁî®ÔºåÂä†‰∏äÊîæÂ∞ÑÁßëÈÜ´Â∏´Áü≠Áº∫Ôºå‰øÉ‰Ωø‰∫∫ÂÄëÂ∞çËá™ÂãïÂåñ CXR ÂàÜÊûêÂíå AI ËºîÂä©Â†±ÂëäÁî¢ÁîüË∂ä‰æÜË∂äÊøÉÂéöÁöÑËààË∂£„ÄÇÈõñÁÑ∂ÁèæÊúâÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Âú®ÁâπÂÆö‰ªªÂãô‰∏≠È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰æãÂ¶ÇÂ†±ÂëäÁîüÊàêÊàñÁï∞Â∏∏ÂÅµÊ∏¨Ôºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏Áº∫‰πèÂ∞ç‰∫íÂãïÂºèË®∫Êñ∑ÂäüËÉΩÁöÑÊîØÊåÅ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RadVLMÔºåÈÄôÊòØ‰∏ÄÂÄãÁ∑äÊπäÁöÑÂ§ö‰ªªÂãôÂ∞çË©±ÂºèÂü∫Á§éÊ®°ÂûãÔºåÂ∞àÁÇ∫ CXR Ëß£ÈáãËÄåË®≠Ë®à„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫Ü‰∏ÄÂÄãÂ§ßÂûãÊåá‰ª§Ë≥áÊñôÈõÜÔºåÂåÖÂê´Ë∂ÖÈÅé 100 Ëê¨ÂÄãÂΩ±ÂÉèÊåá‰ª§Â∞çÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂñÆËº™‰ªªÂãôÔºà‰æãÂ¶ÇÂ†±ÂëäÁîüÊàê„ÄÅÁï∞Â∏∏ÂàÜÈ°ûÂíåË¶ñË¶∫Âü∫Á§éÔºâÔºå‰ª•ÂèäÂ§öËº™„ÄÅÂ§ö‰ªªÂãôÂ∞çË©±‰∫íÂãï„ÄÇÂú®Â∞çÈÄôÂÄãÊåá‰ª§Ë≥áÊñôÈõÜÈÄ≤Ë°åÂæÆË™øÂæåÔºåÊàëÂÄëÂ∞ç RadVLM ÈÄ≤Ë°åË©ï‰º∞Ôºå‰∏¶ËàáÈáçÊñ∞ÂØ¶‰ΩúÁöÑÂü∫Ê∫ñ VLM ‰∏ÄËµ∑Âü∑Ë°å‰∏çÂêåÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåRadVLM Âú®Â∞çË©±ËÉΩÂäõÂíåË¶ñË¶∫Âü∫Á§éÊñπÈù¢ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇÂú®ÂÖ∂‰ªñÊîæÂ∞ÑÂ≠∏‰ªªÂãô‰∏≠‰ªçÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Á™ÅÈ°Ø‰∫ÜË∑®Â§öÂÄã‰ªªÂãôÈÄ≤Ë°åËÅØÂêàË®ìÁ∑¥ÁöÑÂ•ΩËôïÔºåÁâπÂà•ÊòØÂ∞çÊñºÂ∏∂ÊúâÊ®ôË®ªË≥áÊñôÊúâÈôêÁöÑÂ†¥ÊôØ„ÄÇÈÄô‰∫õÁôºÁèæÂÖ±ÂêåÁ™ÅÈ°Ø‰∫Ü RadVLM ‰ΩúÁÇ∫Ëá®Â∫äÁõ∏Èóú AI Âä©ÁêÜÁöÑÊΩõÂäõÔºåÊèê‰æõÁµêÊßãÂåñÁöÑ CXR Ëß£ÈáãÂíåÂ∞çË©±ËÉΩÂäõÔºå‰ª•ÊîØÊè¥Êõ¥ÊúâÊïà‰∏îÂèØÂ≠òÂèñÁöÑË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ã„ÄÇ

##### **MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**
2502.03298v1 by Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich

While increasing patients' access to medical documents improves medical care,
this benefit is limited by varying health literacy levels and complex medical
terminology. Large language models (LLMs) offer solutions by simplifying
medical information. However, evaluating LLMs for safe and patient-friendly
text generation is difficult due to the lack of standardized evaluation
resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset
created from MIMIC-IV discharge summaries through an automated pipeline
combining LLM-based question-answer generation with manual quality checks. We
use this dataset to evaluate various LLMs on patient-oriented
question-answering. Our findings reveal that general-purpose LLMs frequently
surpass biomedical-adapted models, while automated metrics correlate with human
judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the
development of LLMs to enhance patient understanding and ultimately improve
care outcomes.

ÊëòË¶ÅÔºöÂÑòÁÆ°ËÆìÊÇ£ËÄÖÊõ¥ËÉΩÂèñÂæóÈÜ´ÁôÇÊñá‰ª∂ÊúâÂä©ÊñºÊîπÂñÑÈÜ´ÁôÇÁÖßË≠∑Ôºå
‰ΩÜÊ≠§ÂÑ™ÈªûÂèóÂà∞‰∏çÂêåÁöÑÂÅ•Â∫∑Á¥†È§äÁ®ãÂ∫¶ÂíåË§áÈõúÁöÑÈÜ´ÁôÇË°ìË™ûÊâÄÈôêÂà∂„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫ÜÁ∞°ÂåñÈÜ´ÁôÇË≥áË®äÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÊ®ôÊ∫ñÂåñÁöÑË©ï‰º∞Ë≥áÊ∫êÔºåÂõ†Ê≠§Èõ£‰ª•Ë©ï‰º∞ LLM ‰ª•Á¢∫‰øùÂÖ∂ÂÆâÂÖ®‰∏îÂ∞çÊÇ£ËÄÖÂèãÂñÑÁöÑÊñáÂ≠óÁî¢Áîü„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÊ≠§Áº∫Âè£ÔºåÊàëÂÄëÈñãÁôº‰∫Ü MeDiSumQA„ÄÇMeDiSumQA ÊòØÈÄèÈÅéËá™ÂãïÂåñÊµÅÁ®ãÂæû MIMIC-IV Âá∫Èô¢ÊëòË¶Å‰∏≠Âª∫Á´ãÁöÑË≥áÊñôÈõÜÔºåÁµêÂêà‰∫ÜÂü∫Êñº LLM ÁöÑÂïèÁ≠îÁî¢ÁîüÂíåÊâãÂãïÂìÅË≥™Ê™¢Êü•„ÄÇÊàëÂÄë‰ΩøÁî®Ê≠§Ë≥áÊñôÈõÜ‰æÜË©ï‰º∞ÂêÑÁ®Æ LLM Âú®‰ª•ÊÇ£ËÄÖÁÇ∫Â∞éÂêëÁöÑÂïèÁ≠î‰∏≠„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫ÔºåÈÄöÁî® LLM Á∂ìÂ∏∏Ë∂ÖË∂äÁîüÁâ©ÈÜ´Â≠∏ÈÅ©ÊáâÊ®°ÂûãÔºåËÄåËá™ÂãïÂåñÊåáÊ®ôËàá‰∫∫È°ûÂà§Êñ∑Áõ∏Èóú„ÄÇÈÄèÈÅéÂú® PhysioNet ‰∏äÁôºÂ∏É MeDiSumQAÔºåÊàëÂÄëÊó®Âú®Êé®Âãï LLM ÁöÑÁôºÂ±ïÔºå‰ª•Â¢ûÈÄ≤ÊÇ£ËÄÖÁêÜËß£Ôºå‰∏¶ÊúÄÁµÇÊîπÂñÑÁÖßË≠∑ÊàêÊûú„ÄÇ

##### **Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**
2502.03272v1 by Matthias Schwab, Mathias Pamminger, Christian Kremser, Agnes Mayr

Purpose: To develop and evaluate a deep learning-based method that allows to
perform myocardial infarct segmentation in a fully-automated way.
  Materials and Methods: For this retrospective study, a cascaded framework of
two and three-dimensional convolutional neural networks (CNNs), specialized on
identifying ischemic myocardial scars on late gadolinium enhancement (LGE)
cardiac magnetic resonance (CMR) images, was trained on an in-house training
dataset consisting of 144 examinations. On a separate test dataset from the
same institution, including images from 152 examinations obtained between 2021
and 2023, a quantitative comparison between artificial intelligence (AI)-based
segmentations and manual segmentations was performed. Further, qualitative
assessment of segmentation accuracy was evaluated for both human and
AI-generated contours by two CMR experts in a blinded experiment.
  Results: Excellent agreement could be found between manually and
automatically calculated infarct volumes ($\rho_c$ = 0.9). The qualitative
evaluation showed that compared to human-based measurements, the experts rated
the AI-based segmentations to better represent the actual extent of infarction
significantly (p < 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On
the contrary, for segmentation of microvascular obstruction (MVO), manual
measurements were still preferred (11.3% AI, 55.6% human, 33.1% equal).
  Conclusion: This fully-automated segmentation pipeline enables CMR infarct
size to be calculated in a very short time and without requiring any
pre-processing of the input images while matching the segmentation quality of
trained human observers. In a blinded experiment, experts preferred automated
infarct segmentations more often than manual segmentations, paving the way for
a potential clinical application.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÁöÑÔºöÈñãÁôºÂíåË©ï‰º∞‰∏ÄÁ®ÆÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÂÖÅË®±‰ª•ÂÖ®Ëá™ÂãïÁöÑÊñπÂºèÂü∑Ë°åÂøÉËÇåÊ¢óÂ°ûÂàÜÂâ≤„ÄÇ
ÊùêÊñôÂíåÊñπÊ≥ïÔºöÂ∞çÊñºÈÄôÈ†ÖÂõûÈ°ßÊÄßÁ†îÁ©∂Ôºå‰∏ÄÂÄãÁî±‰∫åÁ∂≠Âíå‰∏âÁ∂≠Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁµÑÊàêÁöÑ‰∏≤ËÅØÊû∂ÊßãÔºåÂ∞àÈñÄÁî®ÊñºË≠òÂà•ÊôöÊúüÈáìÂ¢ûÂº∑ (LGE) ÂøÉËáüÁ£ÅÊåØÈÄ†ÂΩ± (CMR) ÂΩ±ÂÉè‰∏äÁöÑÁº∫Ë°ÄÊÄßÂøÉËÇåÁñ§ÁóïÔºå‰∏¶Âú®ÂåÖÂê´ 144 È†ÖÊ™¢Êü•ÁöÑÂÖßÈÉ®Ë®ìÁ∑¥Ë≥áÊñôÈõÜ‰∏äÂèóË®ì„ÄÇÂú®‰æÜËá™Âêå‰∏ÄÂÆ∂Ê©üÊßãÁöÑÁç®Á´ãÊ∏¨Ë©¶Ë≥áÊñôÈõÜ‰∏äÔºåÂåÖÊã¨ 2021 Âπ¥Ëá≥ 2023 Âπ¥ÈñìÁç≤ÂæóÁöÑ 152 È†ÖÊ™¢Êü•ÁöÑÂΩ±ÂÉèÔºåÂü∑Ë°åÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂàÜÂâ≤ÂíåÊâãÂãïÂàÜÂâ≤‰πãÈñìÁöÑÂÆöÈáèÊØîËºÉ„ÄÇÊ≠§Â§ñÔºåÁî±ÂÖ©‰Ωç CMR Â∞àÂÆ∂Âú®Áõ≤Ê∏¨ÂØ¶È©ó‰∏≠Ë©ï‰º∞‰∫∫È°ûÂíå AI ÁîüÊàêÁöÑËº™ÂªìÁöÑÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶„ÄÇ
ÁµêÊûúÔºöÂú®ÊâãÂãïÂíåËá™ÂãïË®àÁÆóÁöÑÊ¢óÂ°ûÈ´îÁ©ç‰πãÈñìÂèØ‰ª•ÁôºÁèæÊ•µ‰Ω≥ÁöÑ‰∏ÄËá¥ÊÄßÔºàœÅ_c = 0.9Ôºâ„ÄÇÂÆöÊÄßË©ï‰º∞È°ØÁ§∫ÔºåËàáÂü∫Êñº‰∫∫È°ûÁöÑÊ∏¨ÈáèÁõ∏ÊØîÔºåÂ∞àÂÆ∂Ë©ï‰º∞ AI Âü∫ÊñºÂàÜÂâ≤ËÉΩÊõ¥ËÉΩ‰ª£Ë°®Ê¢óÂ°ûÁöÑÂØ¶ÈöõÁØÑÂúçÔºåÈ°ØËëóÔºàp < 0.001ÔºâÊõ¥Â∏∏ÁôºÁîüÔºà33.4% AIÔºå25.1% ‰∫∫È°ûÔºå41.5% Áõ∏Á≠âÔºâ„ÄÇÁõ∏ÂèçÔºåÂ∞çÊñºÂæÆË°ÄÁÆ°ÈòªÂ°û (MVO) ÁöÑÂàÜÂâ≤ÔºåÊâãÂãïÊ∏¨Èáè‰ªçÁÑ∂ËºÉÂèóÈùíÁùûÔºà11.3% AIÔºå55.6% ‰∫∫È°ûÔºå33.1% Áõ∏Á≠âÔºâ„ÄÇ
ÁµêË´ñÔºöÈÄôÂÄãÂÖ®Ëá™ÂãïÂàÜÂâ≤ÁÆ°ÈÅìÂèØ‰ª•Âú®ÂæàÁü≠ÁöÑÊôÇÈñìÂÖßË®àÁÆó CMR Ê¢óÂ°ûÂ§ßÂ∞èÔºåËÄå‰∏îÁÑ°ÈúÄÂ∞çËº∏ÂÖ•ÂΩ±ÂÉèÈÄ≤Ë°å‰ªª‰ΩïÂâçËôïÁêÜÔºåÂêåÊôÇÂåπÈÖçÂèóÈÅéË®ìÁ∑¥ÁöÑ‰∫∫È°ûËßÄÂØüËÄÖÁöÑÂàÜÂâ≤ÂìÅË≥™„ÄÇÂú®Áõ≤Ê∏¨ÂØ¶È©ó‰∏≠ÔºåÂ∞àÂÆ∂ÊØîÊâãÂãïÂàÜÂâ≤Êõ¥Â∏∏ÂÅèÂ•ΩËá™ÂãïÊ¢óÂ°ûÂàÜÂâ≤ÔºåÁÇ∫ÊΩõÂú®ÁöÑËá®Â∫äÊáâÁî®Èã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ</paragraph>

##### **Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**
2502.03238v2 by Li Pan, Yupei Zhang, Qiushi Yang, Tan Li, Zhen Chen

Recently computer-aided diagnosis has demonstrated promising performance,
effectively alleviating the workload of clinicians. However, the inherent
sample imbalance among different diseases leads algorithms biased to the
majority categories, leading to poor performance for rare categories. Existing
works formulated this challenge as a long-tailed problem and attempted to
tackle it by decoupling the feature representation and classification. Yet, due
to the imbalanced distribution and limited samples from tail classes, these
works are prone to biased representation learning and insufficient classifier
calibration. To tackle these problems, we propose a new Long-tailed Medical
Diagnosis (LMD) framework for balanced medical image classification on
long-tailed datasets. In the initial stage, we develop a Relation-aware
Representation Learning (RRL) scheme to boost the representation ability by
encouraging the encoder to capture intrinsic semantic features through
different data augmentations. In the subsequent stage, we propose an Iterative
Classifier Calibration (ICC) scheme to calibrate the classifier iteratively.
This is achieved by generating a large number of balanced virtual features and
fine-tuning the encoder using an Expectation-Maximization manner. The proposed
ICC compensates for minority categories to facilitate unbiased classifier
optimization while maintaining the diagnostic knowledge in majority classes.
Comprehensive experiments on three public long-tailed medical datasets
demonstrate that our LMD framework significantly surpasses state-of-the-art
approaches. The source code can be accessed at
https://github.com/peterlipan/LMD.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåËÆ°ÁÆóÊú∫ËæÖÂä©ËØäÊñ≠Â∑≤Â±ïÁé∞Âá∫ÂèØËßÇÁöÑË°®Áé∞ÔºåÊúâÊïàÂáèËΩª‰∫Ü‰∏¥Â∫äÂåªÁîüÁöÑÂ∑•‰ΩúÈáè„ÄÇÁÑ∂ËÄåÔºå‰∏çÂêåÁñæÁóÖ‰πãÈó¥Âõ∫ÊúâÁöÑÊ†∑Êú¨‰∏çÂπ≥Ë°°ÂØºËá¥ÁÆóÊ≥ïÂÅèÂêë‰∫éÂ§öÊï∞Á±ªÂà´Ôºå‰ªéËÄåÂØºËá¥ÁΩïËßÅÁ±ªÂà´Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÁé∞ÊúâÂ∑•‰ΩúÂ∞ÜËøô‰∏ÄÊåëÊàòË°®Ëø∞‰∏∫ÈïøÂ∞æÈóÆÈ¢òÔºåÂπ∂Â∞ùËØïÈÄöËøáËß£ËÄ¶ÁâπÂæÅË°®Á§∫ÂíåÂàÜÁ±ªÊù•Ëß£ÂÜ≥ÂÆÉ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é‰∏çÂπ≥Ë°°ÂàÜÂ∏ÉÂíåÂ∞æÁ±ªÊ†∑Êú¨ÊúâÈôêÔºåËøô‰∫õÂ∑•‰ΩúÂÆπÊòìÂá∫Áé∞ÊúâÂÅèÂ∑ÆÁöÑË°®Á§∫Â≠¶‰π†ÂíåÂàÜÁ±ªÂô®Ê†°ÂáÜ‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈïøÂ∞æÂåªÂ≠¶ËØäÊñ≠ (LMD) Ê°ÜÊû∂ÔºåÁî®‰∫éÂØπÈïøÂ∞æÊï∞ÊçÆÈõÜËøõË°åÂπ≥Ë°°ÁöÑÂåªÂ≠¶ÂõæÂÉèÂàÜÁ±ª„ÄÇÂú®ÂàùÂßãÈò∂ÊÆµÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂÖ≥Á≥ªÊÑüÁü•Ë°®Á§∫Â≠¶‰π† (RRL) ÊñπÊ°àÔºåÈÄöËøáÈºìÂä±ÁºñÁ†ÅÂô®ÈÄöËøá‰∏çÂêåÁöÑÊï∞ÊçÆÂ¢ûÂº∫Êù•ÊçïËé∑ÂÜÖÂú®ËØ≠‰πâÁâπÂæÅÔºå‰ªéËÄåÊèêÂçáË°®Á§∫ËÉΩÂäõ„ÄÇÂú®ÂêéÁª≠Èò∂ÊÆµÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ëø≠‰ª£ÂàÜÁ±ªÂô®Ê†°ÂáÜ (ICC) ÊñπÊ°àÔºå‰ª•Ëø≠‰ª£ÊñπÂºèÊ†°ÂáÜÂàÜÁ±ªÂô®„ÄÇËøôÊòØÈÄöËøáÁîüÊàêÂ§ßÈáèÁöÑÂπ≥Ë°°ËôöÊãüÁâπÂæÅÂπ∂‰ΩøÁî®ÊúüÊúõÊúÄÂ§ßÂåñÊñπÂºèÂæÆË∞ÉÁºñÁ†ÅÂô®Êù•ÂÆûÁé∞ÁöÑ„ÄÇÊâÄÊèêÂá∫ÁöÑ ICC Ë°•ÂÅø‰∫ÜÂ∞ëÊï∞Á±ªÂà´Ôºå‰ª•‰øÉËøõÊó†ÂÅèÂàÜÁ±ªÂô®‰ºòÂåñÔºåÂêåÊó∂‰øùÊåÅÂ§öÊï∞Á±ªÂà´ÁöÑËØäÊñ≠Áü•ËØÜ„ÄÇÂú®‰∏â‰∏™ÂÖ¨ÂÖ±ÈïøÂ∞æÂåªÂ≠¶Êï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑ LMD Ê°ÜÊû∂ÊòéÊòæË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/peterlipan/LMD Â§ÑËé∑Âèñ„ÄÇ</paragraph>

##### **MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**
2502.03004v1 by Seonok Kim

Large Language Models (LLMs) have demonstrated impressive capabilities across
natural language processing tasks. However, their application to specialized
domains such as medicine and biology requires further optimization to ensure
factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a
domain-adapted biomedical question-answering model designed to enhance both
short-form and long-form queries. By integrating fine-tuning and
retrieval-augmented generation (RAG), MedBioLM dynamically incorporates
domain-specific knowledge, improving reasoning abilities and factual accuracy.
To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA
datasets, covering structured multiple-choice assessments and complex clinical
reasoning tasks. Fine-tuning significantly improves accuracy on benchmark
datasets, while RAG enhances factual consistency. These results highlight the
potential of domain-optimized LLMs in advancing biomedical research, medical
education, and clinical decision support.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåË¶ÅÂ∞áÂÖ∂ÊáâÁî®ÊñºÈÜ´Â≠∏ÂíåÁîüÁâ©Â≠∏Á≠âÁâπÂÆöÈ†òÂüüÔºåÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÊúÄ‰Ω≥ÂåñÔºå‰ª•Á¢∫‰øù‰∫ãÂØ¶ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÂèØÈù†ÊÄß‰ª•ÂèäËÑàÁµ°ÁöÑÊ∑±Â∫¶„ÄÇÊàëÂÄëÂºïÈÄ≤‰∫Ü MedBioLMÔºåÈÄôÊòØ‰∏ÄÂÄãÈÅ©ÊáâÈ†òÂüüÁöÑÁîüÁâ©ÈÜ´Â≠∏ÂïèÁ≠îÊ®°ÂûãÔºåÊó®Âú®Â¢ûÂº∑Áü≠ÂºèÂíåÈï∑ÂºèÊü•Ë©¢„ÄÇÈÄèÈÅéÊï¥ÂêàÂæÆË™øÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)ÔºåMedBioLM ËÉΩÂãïÊÖãÂú∞Á¥çÂÖ•È†òÂüüÁâπÂÆöÁöÑÁü•Ë≠òÔºåÂæûËÄåÊèêÂçáÊé®ÁêÜËÉΩÂäõÂíå‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄß„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÂÖ∂ÊúâÊïàÊÄßÔºåÊàëÂÄëÂ∞çÊ®°ÂûãÈÄ≤Ë°åÂæÆË™øÔºå‰ΩøÂÖ∂Ê∂µËìãÁµêÊßãÂåñÁöÑÂ§öÈáçÈÅ∏ÊìáË©ïÈáèÂíåË§áÈõúÁöÑËá®Â∫äÊé®ÁêÜ‰ªªÂãôÁ≠âÂ§öÊ®£ÂåñÁöÑÁîüÁâ©ÈÜ´Â≠∏ÂïèÁ≠îË≥áÊñôÈõÜ„ÄÇÂæÆË™øÈ°ØËëóÊèêÂçá‰∫ÜÂü∫Ê∫ñË≥áÊñôÈõÜÁöÑÊ∫ñÁ¢∫ÊÄßÔºåËÄå RAG ÂâáÂ¢ûÂº∑‰∫Ü‰∫ãÂØ¶ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÈ†òÂüüÊúÄ‰Ω≥ÂåñÁöÑ LLM Âú®Êé®ÈÄ≤ÁîüÁâ©ÈÜ´Â≠∏Á†îÁ©∂„ÄÅÈÜ´Â≠∏ÊïôËÇ≤ÂíåËá®Â∫äÊ±∫Á≠ñÊîØÊè¥ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Contrastive Token-level Explanations for Graph-based Rumour Detection**
2502.04366v1 by Daniel Wai Kit Chin, Roy Ka-Wei Lee

The widespread use of social media has accelerated the dissemination of
information, but it has also facilitated the spread of harmful rumours, which
can disrupt economies, influence political outcomes, and exacerbate public
health crises, such as the COVID-19 pandemic. While Graph Neural Network
(GNN)-based approaches have shown significant promise in automated rumour
detection, they often lack transparency, making their predictions difficult to
interpret. Existing graph explainability techniques fall short in addressing
the unique challenges posed by the dependencies among feature dimensions in
high-dimensional text embeddings used in GNN-based models. In this paper, we
introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel
framework designed to enhance the explainability of GNN-based rumour detection.
CT-LRP extends current graph explainability methods by providing token-level
explanations that offer greater granularity and interpretability. We evaluate
the effectiveness of CT-LRP across multiple GNN models trained on three
publicly available rumour detection datasets, demonstrating that it
consistently produces high-fidelity, meaningful explanations, paving the way
for more robust and trustworthy rumour detection systems.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™íÈ´îÁöÑÂª£Ê≥õ‰ΩøÁî®Âä†ÈÄü‰∫ÜË≥áË®äÁöÑÂÇ≥Êí≠Ôºå‰ΩÜ‰πü‰øÉËøõ‰∫ÜÊúâÂÆ≥Ë¨†Ë®ÄÁöÑÊï£Êí≠ÔºåÈÄôÂèØËÉΩÊúÉÊìæ‰∫ÇÁ∂ìÊøü„ÄÅÂΩ±ÈüøÊîøÊ≤ªÁµêÊûúÔºå‰∏¶Âä†ÂäáÂÖ¨ÂÖ±Ë°õÁîüÂç±Ê©üÔºå‰æãÂ¶Ç COVID-19 Â§ßÊµÅË°å„ÄÇÈõñÁÑ∂Âü∫ÊñºÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÊñπÊ≥ïÂú®Ëá™ÂãïÂåñË¨†Ë®ÄÂÅµÊ∏¨ÊñπÈù¢Â±ïÁèæ‰∫ÜÈ°ØËëóÁöÑÂâçÊôØÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏Áº∫‰πèÈÄèÊòéÂ∫¶ÔºåÈÄô‰ΩøÂæóÂÆÉÂÄëÁöÑÈ†êÊ∏¨Èõ£‰ª•Ëß£Èáã„ÄÇÁèæÊúâÁöÑÂúñÂΩ¢ÂèØËß£ÈáãÊÄßÊäÄË°ìÁÑ°Ê≥ïËß£Ê±∫ GNN Ê®°Âûã‰∏≠‰ΩøÁî®ÁöÑÁ∂≠Â∫¶ÂµåÂÖ•ÂºèÊñáÊú¨‰πãÈñìÁöÑ‰æùË≥¥ÊÄßÊâÄÂ∏∂‰æÜÁöÑÁç®ÁâπÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ∞çÊØîÊ®ôË®òÂàÜÂ±§ÈóúËÅØÊÄßÂÇ≥Êí≠ (CT-LRP)ÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∑Âü∫Êñº GNN ÁöÑË¨†Ë®ÄÂÅµÊ∏¨ÁöÑÂèØËß£ÈáãÊÄß„ÄÇCT-LRP ÈÄèÈÅéÊèê‰æõÊ®ôË®òÁ¥öÂà•ÁöÑËß£Èáã‰æÜÊì¥ÂÖÖÁï∂ÂâçÁöÑÂúñÂΩ¢ÂèØËß£ÈáãÊÄßÊñπÊ≥ïÔºåÈÄô‰∫õËß£ÈáãÊèê‰æõ‰∫ÜÊõ¥Á¥∞Á∑ªÁöÑÁ≤íÂ∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂÖ¨ÈñãÁöÑË¨†Ë®ÄÂÅµÊ∏¨Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂπæÂÄã GNN Ê®°Âûã‰∏≠Ë©ï‰º∞‰∫Ü CT-LRP ÁöÑÊúâÊïàÊÄßÔºåË≠âÊòéÂÆÉÂßãÁµÇÁî¢ÁîüÈ´ò‰øùÁúü„ÄÅÊúâÊÑèÁæ©ÁöÑËß£ÈáãÔºåÁÇ∫Êõ¥Âº∑ÂÅ•‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑË¨†Ë®ÄÂÅµÊ∏¨Á≥ªÁµ±Èã™Ë∑Ø„ÄÇ

##### **AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth**
2502.04365v1 by Jorge Garc√≠a-Torres, √òyvind Meinich-Bache, Siren Rettedal, Kjersti Engan

Approximately 10% of newborns need some assistance to start breathing and 5\%
proper ventilation. It is crucial that interventions are initiated as soon as
possible after birth. Accurate documentation of Time of Birth (ToB) is thereby
essential for documenting and improving newborn resuscitation performance.
However, current clinical practices rely on manual recording of ToB, typically
with minute precision. In this study, we present an AI-driven, video-based
system for automated ToB detection using thermal imaging, designed to preserve
the privacy of healthcare providers and mothers by avoiding the use of
identifiable visual data. Our approach achieves 91.4% precision and 97.4%
recall in detecting ToB within thermal video clips during performance
evaluation. Additionally, our system successfully identifies ToB in 96% of test
cases with an absolute median deviation of 1 second compared to manual
annotations. This method offers a reliable solution for improving ToB
documentation and enhancing newborn resuscitation outcomes.

ÊëòË¶ÅÔºöÁ¥Ñ 10% ÁöÑÊñ∞ÁîüÂÖíÈúÄË¶ÅÂçîÂä©ÊâçËÉΩÈñãÂßãÂëºÂê∏Ôºå5% ÈúÄË¶ÅÈÅ©Áï∂ÁöÑÈÄöÊ∞£„ÄÇÂú®Âá∫ÁîüÂæåÁõ°Âø´ÈñãÂßã‰ªãÂÖ•Ëá≥ÈóúÈáçË¶Å„ÄÇÊ∫ñÁ¢∫Ë®òÈåÑÂá∫ÁîüÊôÇÈñì (ToB) Â∞çÊñºË®òÈåÑÂíåÊîπÂñÑÊñ∞ÁîüÂÖíÂæ©Áî¶Ë°®ÁèæËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑËá®Â∫äÂØ¶Âãô‰æùË≥¥ÊñºÊâãÂãïË®òÈåÑ ToBÔºåÈÄöÂ∏∏Á≤æÁ¢∫Âà∞ÂàÜÈêò„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ª• AI ÁÇ∫‰∏ªÁöÑ„ÄÅÂü∫ÊñºÂΩ±ÁâáÁöÑÁ≥ªÁµ±ÔºåÁî®Êñº‰ΩøÁî®ÁÜ±ÂΩ±ÂÉèËá™ÂãïÂÅµÊ∏¨ ToBÔºåÊó®Âú®ÈÄèÈÅéÈÅøÂÖç‰ΩøÁî®ÂèØË≠òÂà•ÁöÑË¶ñË¶∫Ë≥áÊñô‰æÜ‰øùË≠∑ÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖÂíåÊØçË¶™ÁöÑÈö±ÁßÅ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Âü∑Ë°åË©ï‰º∞ÊúüÈñìÔºåÂú®ÁÜ±ÂΩ±ÂÉèÁâáÊÆµ‰∏≠ÂÅµÊ∏¨ ToB ÊôÇÈÅîÂà∞‰∫Ü 91.4% ÁöÑÁ≤æÁ¢∫Â∫¶Âíå 97.4% ÁöÑÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±Âú® 96% ÁöÑÊ∏¨Ë©¶Ê°à‰æã‰∏≠ÊàêÂäüË≠òÂà•Âá∫ ToBÔºåËàáÊâãÂãïË®ªËß£Áõ∏ÊØîÔºåÁµïÂ∞ç‰∏≠‰ΩçÊï∏ÂÅèÂ∑ÆÁÇ∫ 1 Áßí„ÄÇÊ≠§ÊñπÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÈù†ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÊîπÂñÑ ToB Ë®òÈåÑÂíåÂ¢ûÂº∑Êñ∞ÁîüÂÖíÂæ©Áî¶ÁµêÊûú„ÄÇ

##### **3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**
2502.02779v1 by Weicheng Zhu, Haoxu Huang, Huanze Tang, Rushabh Musthyala, Boyang Yu, Long Chen, Emilio Vega, Thomas O'Donnell, Seena Dehkharghani, Jennifer A. Frontera, Arjun V. Masurkar, Kara Melmed, Narges Razavian

Head computed tomography (CT) imaging is a widely-used imaging modality with
multitudes of medical indications, particularly in assessing pathology of the
brain, skull, and cerebrovascular system. It is commonly the first-line imaging
in neurologic emergencies given its rapidity of image acquisition, safety,
cost, and ubiquity. Deep learning models may facilitate detection of a wide
range of diseases. However, the scarcity of high-quality labels and
annotations, particularly among less common conditions, significantly hinders
the development of powerful models. To address this challenge, we introduce
FM-CT: a Foundation Model for Head CT for generalizable disease detection,
trained using self-supervised learning. Our approach pre-trains a deep learning
model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans
without the need for manual annotations, enabling the model to learn robust,
generalizable features. To investigate the potential of self-supervised
learning in head CT, we employed both discrimination with self-distillation and
masked image modeling, and we construct our model in 3D rather than at the
slice level (2D) to exploit the structure of head CT scans more comprehensively
and efficiently. The model's downstream classification performance is evaluated
using internal and three external datasets, encompassing both in-distribution
(ID) and out-of-distribution (OOD) data. Our results demonstrate that the
self-supervised foundation model significantly improves performance on
downstream diagnostic tasks compared to models trained from scratch and
previous 3D CT foundation models on scarce annotated datasets. This work
highlights the effectiveness of self-supervised learning in medical imaging and
sets a new benchmark for head CT image analysis in 3D, enabling broader use of
artificial intelligence for head CT-based diagnosis.

ÊëòË¶ÅÔºöÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºàCTÔºâÂΩ±ÂÉèÊòØ‰∏ÄÁ®ÆÂª£Ê≥õ‰ΩøÁî®ÁöÑÂΩ±ÂÉèÊ®°ÂºèÔºåÂÖ∑Êúâ
Â§ßÈáèÁöÑÈÜ´ÁôÇÈÅ©ÊáâÁóáÔºåÁâπÂà•ÊòØÂú®Ë©ï‰º∞ËÖ¶ÈÉ®„ÄÅÈ†≠È™®ÂíåËÖ¶Ë°ÄÁÆ°Á≥ªÁµ±ÁöÑÁóÖÁêÜÊôÇ„ÄÇÁî±ÊñºÂÖ∂ÂΩ±ÂÉèÊì∑ÂèñÈÄüÂ∫¶Âø´„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÊàêÊú¨‰ΩéÂíåÊôÆÈÅçÊÄßÔºåÈÄöÂ∏∏ÊòØÁ•ûÁ∂ìÁ∑äÊÄ•ÊÉÖÊ≥Å‰∏ãÁöÑÁ¨¨‰∏ÄÁ∑öÂΩ±ÂÉè„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂèØ‰ª•‰øÉÈÄ≤Â∞çÂêÑÁ®ÆÁñæÁóÖÁöÑÊ™¢Ê∏¨„ÄÇÁÑ∂ËÄåÔºåÈ´òÂìÅË≥™Ê®ôÁ±§ÂíåË®ªÈáãÁöÑÁ®ÄÁº∫ÔºåÁâπÂà•ÊòØÂú®ËºÉ‰∏çÂ∏∏Ë¶ãÁöÑÁñæÁóÖ‰∏≠ÔºåÈ°ØËëóÂú∞ÈòªÁ§ô‰∫ÜÂº∑Â§ßÊ®°ÂûãÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü FM-CTÔºö‰∏ÄÂÄãÁî®ÊñºÈ†≠ÈÉ® CT ÁöÑÂü∫Á§éÊ®°ÂûãÔºåÁî®ÊñºÂèØÊ¶ÇÂåñÁöÑÁñæÁóÖÊ™¢Ê∏¨Ôºå‰∏¶‰ΩøÁî®Ëá™ÊàëÁõ£Áù£Â≠∏ÁøíÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®‰∏ÄÂÄãÂåÖÂê´ 361,663 ÂÄãÈùûÂ∞çÊØî 3D È†≠ÈÉ® CT ÊéÉÊèèÁöÑÂ§ßÂûã„ÄÅÂ§öÊ®£ÂåñÁöÑÊï∏ÊìöÈõÜ‰∏äÈ†êË®ìÁ∑¥‰∏ÄÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåËÄåÁÑ°ÈúÄÊâãÂãïË®ªÈáãÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†Â≠∏ÁøíÂº∑ÂÅ•„ÄÅÂèØÊ¶ÇÂåñÁöÑÁâπÂæµ„ÄÇÁÇ∫‰∫ÜÊé¢Ë®éËá™ÊàëÁõ£Áù£Â≠∏ÁøíÂú®È†≠ÈÉ® CT ‰∏≠ÁöÑÊΩõÂäõÔºåÊàëÂÄëÂêåÊôÇÊé°Áî®‰∫ÜÂ∏∂ÊúâËá™ÊàëËí∏È§æÁöÑÂà§Âà•ÂíåÈÅÆÁΩ©ÂΩ±ÂÉèÂª∫Ê®°Ôºå‰∏¶‰∏îÊàëÂÄë‰ª• 3D ËÄå‰∏çÊòØÂàáÁâáÂ±§Á¥öÔºà2DÔºâÊßãÂª∫ÊàëÂÄëÁöÑÊ®°ÂûãÔºå‰ª•Êõ¥ÂÖ®Èù¢„ÄÅÊúâÊïàÂú∞Âà©Áî®È†≠ÈÉ® CT ÊéÉÊèèÁöÑÁµêÊßã„ÄÇË©≤Ê®°ÂûãÁöÑ‰∏ãÊ∏∏ÂàÜÈ°ûÊïàËÉΩ‰ΩøÁî®ÂÖßÈÉ®Âíå‰∏âÂÄãÂ§ñÈÉ®Êï∏ÊìöÈõÜÈÄ≤Ë°åË©ï‰º∞ÔºåÂåÖÊã¨ÂàÜ‰ΩàÂÖß (ID) ÂíåÂàÜ‰ΩàÂ§ñ (OOD) Ë≥áÊñô„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥ÁöÑÊ®°ÂûãÂíåÂÖàÂâçÂú®Á®ÄÁñèË®ªÈáãÊï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥ÁöÑ 3D CT Âü∫Á§éÊ®°ÂûãÁõ∏ÊØîÔºåËá™ÊàëÁõ£Áù£Âü∫Á§éÊ®°ÂûãÈ°ØËëóÊîπÂñÑ‰∫Ü‰∏ãÊ∏∏Ë®∫Êñ∑‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁ™ÅÈ°Ø‰∫ÜËá™ÊàëÁõ£Áù£Â≠∏ÁøíÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÁÇ∫ 3D È†≠ÈÉ® CT ÂΩ±ÂÉèÂàÜÊûêË®≠ÂÆö‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºåËÆì‰∫∫Â∑•Êô∫ÊÖßËÉΩÂ§†Êõ¥Âª£Ê≥õÂú∞Áî®ÊñºÂü∫ÊñºÈ†≠ÈÉ® CT ÁöÑË®∫Êñ∑„ÄÇ

##### **Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**
2502.02756v1 by Obed Korshie Dzikunu, Shadab Ahamed, Amirhossein Toosi, Xiaoxiao Li, Arman Rahmim

This study proposes a new loss function for deep neural networks, L1-weighted
Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of
voxels based on their classification difficulty, towards automated detection
and segmentation of metastatic prostate cancer lesions in PET/CT scans. We
obtained 380 PSMA [18-F] DCFPyL PET/CT scans of patients diagnosed with
biochemical recurrence metastatic prostate cancer. We trained two 3D
convolutional neural networks, Attention U-Net and SegResNet, and concatenated
the PET and CT volumes channel-wise as input. The performance of our custom
loss function was evaluated against the Dice and Dice Focal Loss functions. For
clinical significance, we considered a detected region of interest (ROI) as a
true positive if at least the voxel with the maximum standardized uptake value
falls within the ROI. We assessed the models' performance based on the number
of lesions in an image, tumour volume, activity, and extent of spread. The
L1DFL outperformed the comparative loss functions by at least 13% on the test
set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were
lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal
Loss yielded more false positives, whereas the Dice Loss was more sensitive to
smaller volumes and struggled to segment larger lesions accurately. They also
exhibited network-specific variations and yielded declines in segmentation
accuracy with increased tumour spread. Our results demonstrate the potential of
L1DFL to yield robust segmentation of metastatic prostate cancer lesions in
PSMA PET/CT images. The results further highlight potential complexities
arising from the variations in lesion characteristics that may influence
automated prostate cancer tumour detection and segmentation. The code is
publicly available at: https://github.com/ObedDzik/pca_segment.git.

ÊëòË¶ÅÔºö<paragraph>Êú¨Á†îÁ©∂ÈáùÂ∞çÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåL1 Âä†Ê¨ä Dice ÁÑ¶ÈªûÊêçÂ§± (L1DFL)ÔºåÂÆÉÂà©Áî® L1 ÁØÑÊï∏Ê†πÊìöÈ´îÁ¥†ÁöÑÂàÜÈ°ûÈõ£Â∫¶ÈÄ≤Ë°åËá™ÈÅ©ÊáâÂä†Ê¨äÔºåÁî®ÊñºËá™ÂãïÂÅµÊ∏¨ÂíåÂàÜÂâ≤ PET/CT ÊéÉÊèè‰∏≠ËΩâÁßªÊÄßÂâçÂàóËÖ∫ÁôåÁóÖÁÅ∂„ÄÇÊàëÂÄëÂèñÂæó 380 ÂÄãÁ∂ìË®∫Êñ∑ÁÇ∫ÁîüÂåñÂæ©ÁôºËΩâÁßªÊÄßÂâçÂàóËÖ∫ÁôåÁöÑÊÇ£ËÄÖÁöÑ PSMA [18-F] DCFPyL PET/CT ÊéÉÊèè„ÄÇÊàëÂÄëË®ìÁ∑¥‰∫ÜÂÖ©ÂÄã 3D Êç≤Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåAttention U-Net Âíå SegResNetÔºå‰∏¶Â∞á PET Âíå CT È´îÁ©çÊåâÈÄöÈÅìÈÄ£Êé•‰ΩúÁÇ∫Ëº∏ÂÖ•„ÄÇÊàëÂÄëËá™Ë®ÇÁöÑÊêçÂ§±ÂáΩÊï∏ÁöÑÊïàËÉΩËàá Dice Âíå Dice ÁÑ¶ÈªûÊêçÂ§±ÂáΩÊï∏ÈÄ≤Ë°åË©ï‰º∞„ÄÇÁÇ∫‰∫ÜËá®Â∫äÊÑèÁæ©ÔºåÊàëÂÄëÂ∞á‰∏ÄÂÄãÂÅµÊ∏¨Âà∞ÁöÑÊÑüËààË∂£ÂçÄÂüü (ROI) Ë¶ñÁÇ∫ÁúüÈôΩÊÄßÔºåÂ¶ÇÊûúËá≥Â∞ëÂÖ∑ÊúâÊúÄÂ§ßÊ®ôÊ∫ñÊîùÂèñÂÄºÁöÑÈ´îÁ¥†ËêΩÂú® ROI ÂÖß„ÄÇÊàëÂÄëÊ†πÊìöÂΩ±ÂÉè‰∏≠ÁöÑÁóÖÁÅ∂Êï∏Èáè„ÄÅËÖ´Áò§È´îÁ©ç„ÄÅÊ¥ªÊÄßÔºå‰ª•ÂèäÊì¥Êï£Á®ãÂ∫¶Ë©ï‰º∞Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇL1DFL Âú®Ê∏¨Ë©¶ÁµÑ‰∏≠Ëá≥Â∞ëÊØîÊØîËºÉÊêçÂ§±ÂáΩÊï∏È´òÂá∫ 13%„ÄÇÊ≠§Â§ñÔºåDice ÊêçÂ§±Âíå Dice ÁÑ¶ÈªûÊêçÂ§±ÁöÑ F1 ÂàÜÊï∏ÂàÜÂà•ÊØî L1DFL ‰ΩéËá≥Â∞ë 6% Âíå 34%„ÄÇDice ÁÑ¶ÈªûÊêçÂ§±Áî¢ÁîüÊõ¥Â§öÂÅáÈôΩÊÄßÔºåËÄå Dice ÊêçÂ§±Â∞çËºÉÂ∞èÈ´îÁ©çËºÉÁÇ∫ÊïèÊÑüÔºå‰∏îÈõ£‰ª•Ê∫ñÁ¢∫ÂàÜÂâ≤ËºÉÂ§ßÁóÖÁÅ∂„ÄÇÂÆÉÂÄë‰πüÂ±ïÁèæÂá∫Á∂≤Ë∑ØÁâπÂÆöÁöÑËÆäÂåñÔºå‰∏¶Èö®ËëóËÖ´Áò§Êì¥Êï£ËÄåÂ∞éËá¥ÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶‰∏ãÈôç„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé L1DFL ÂÖ∑ÊúâÂú® PSMA PET/CT ÂΩ±ÂÉè‰∏≠Áî¢ÁîüËΩâÁßªÊÄßÂâçÂàóËÖ∫ÁôåÁóÖÁÅ∂ÁöÑÂº∑ÂÅ•ÂàÜÂâ≤ÁöÑÊΩõÂäõ„ÄÇÁµêÊûúÈÄ≤‰∏ÄÊ≠•Âº∑Ë™øÁî±ÁóÖÁÅ∂ÁâπÂæµËÆäÂåñÊâÄÁî¢ÁîüÁöÑÊΩõÂú®Ë§áÈõúÊÄßÔºåÈÄôÂèØËÉΩÊúÉÂΩ±ÈüøËá™ÂãïÂåñÂâçÂàóËÖ∫ÁôåËÖ´Áò§ÂÅµÊ∏¨ÂíåÂàÜÂâ≤„ÄÇÁ®ãÂºèÁ¢ºÂÖ¨ÈñãÊñºÔºöhttps://github.com/ObedDzik/pca_segment.git„ÄÇ</paragraph>

##### **MedRAX: Medical Reasoning Agent for Chest X-ray**
2502.02673v1 by Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang

Chest X-rays (CXRs) play an integral role in driving critical decisions in
disease management and patient care. While recent innovations have led to
specialized models for various CXR interpretation tasks, these solutions often
operate in isolation, limiting their practical utility in clinical practice. We
present MedRAX, the first versatile AI agent that seamlessly integrates
state-of-the-art CXR analysis tools and multimodal large language models into a
unified framework. MedRAX dynamically leverages these models to address complex
medical queries without requiring additional training. To rigorously evaluate
its capabilities, we introduce ChestAgentBench, a comprehensive benchmark
containing 2,500 complex medical queries across 7 diverse categories. Our
experiments demonstrate that MedRAX achieves state-of-the-art performance
compared to both open-source and proprietary models, representing a significant
step toward the practical deployment of automated CXR interpretation systems.
Data and code have been publicly available at
https://github.com/bowang-lab/MedRAX

ÊëòË¶ÅÔºöËÉ∏ÈÉ® X ÂÖâÁâá (CXR) Âú®ÁñæÁóÖÁÆ°ÁêÜÂíåÊÇ£ËÄÖÁÖßË≠∑‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤ÔºåÊé®ÂãïËëóÈóúÈçµÊ±∫Á≠ñÁöÑÂà∂ÂÆö„ÄÇÂÑòÁÆ°ËøëÊúüÁöÑÂâµÊñ∞Â∑≤ÈáùÂ∞çÂêÑÁ®Æ CXR Ëß£ËÆÄ‰ªªÂãôÈñãÁôºÂá∫Â∞àÈñÄÁöÑÊ®°ÂûãÔºå‰ΩÜÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÈÄöÂ∏∏Áç®Á´ãÈÅã‰ΩúÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÂØ¶ÈöõÊïàÁî®„ÄÇÊàëÂÄëÊèêÂá∫ MedRAXÔºåÈÄôÊòØ‰∏ÄÊ¨æÈ¶ñÂâµÁöÑÂ§öÂäüËÉΩ AI ‰ª£ÁêÜÔºåÂÆÉÂ∞áÊúÄÂÖàÈÄ≤ÁöÑ CXR ÂàÜÊûêÂ∑•ÂÖ∑ÂíåÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁÑ°Á∏´Êï¥ÂêàÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÊû∂Êßã‰∏≠„ÄÇMedRAX ÂãïÊÖãÈÅãÁî®ÈÄô‰∫õÊ®°Âûã‰æÜËß£Ê±∫Ë§áÈõúÁöÑÈÜ´ÁôÇÊü•Ë©¢ÔºåËÄåÁÑ°ÈúÄÈ°çÂ§ñÁöÑË®ìÁ∑¥„ÄÇÁÇ∫‰∫ÜÂö¥Ê†ºË©ï‰º∞ÂÖ∂ÂäüËÉΩÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ChestAgentBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÔºåÂåÖÂê´ 7 ÂÄã‰∏çÂêåÈ°ûÂà•ÁöÑ 2,500 ÂÄãË§áÈõúÈÜ´ÁôÇÊü•Ë©¢„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåËàáÈñãÊ∫êÂíåÂ∞àÊúâÊ®°ÂûãÁõ∏ÊØîÔºåMedRAX ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÈÄô‰ª£Ë°®‰∫ÜËá™ÂãïÂåñ CXR Ëß£ËÆÄÁ≥ªÁµ±ÂØ¶ÈöõÈÉ®ÁΩ≤ÁöÑÈáçË¶Å‰∏ÄÊ≠•„ÄÇË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/bowang-lab/MedRAX

##### **Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription**
2502.04356v1 by Mahdi Alkaeed, Sofiat Abioye, Adnan Qayyum, Yosra Magdi Mekki, Ilhem Berrou, Mohamad Abdallah, Ala Al-Fuqaha, Muhammad Bilal, Junaid Qadir

In response to the success of proprietary Large Language Models (LLMs) such
as OpenAI's GPT-4, there is a growing interest in developing open,
non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in
academic, scientific, and non-commercial applications. Despite their inability
to match the refined functionalities of their proprietary counterparts, open
models hold immense potential to revolutionize healthcare applications. In this
paper, we examine the prospects of open-source LLMs and AIFMs for developing
healthcare applications and make two key contributions. Firstly, we present a
comprehensive survey of the current state-of-the-art open-source healthcare
LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their
utility across various healthcare tasks. Secondly, to evaluate the
general-purpose applications of open LLMs in healthcare, we present a case
study on personalized prescriptions. This task is particularly significant due
to its critical role in delivering tailored, patient-specific medications that
can greatly improve treatment outcomes. In addition, we compare the performance
of open-source models with proprietary models in settings with and without
Retrieval-Augmented Generation (RAG). Our findings suggest that, although less
refined, open LLMs can achieve performance comparable to proprietary models
when paired with grounding techniques such as RAG. Furthermore, to highlight
the clinical significance of LLMs-empowered personalized prescriptions, we
perform subjective assessment through an expert clinician. We also elaborate on
ethical considerations and potential risks associated with the misuse of
powerful LLMs and AIFMs, highlighting the need for a cautious and responsible
implementation in healthcare.

ÊëòË¶ÅÔºö<paragraph>ÁÇ∫‰∫ÜÂõûÊáâ OpenAI ÁöÑ GPT-4 Á≠âÂ∞àÊúâÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊàêÂäüÔºåÈñãÁôºÈñãÊîæ„ÄÅÈùûÂ∞àÊúâÁöÑ LLM Âíå‰∫∫Â∑•Êô∫ÊÖßÂü∫Á§éÊ®°Âûã (AIFM) ‰ª•ÈÄèÊòéÂú∞Áî®ÊñºÂ≠∏Ë°ì„ÄÅÁßëÂ≠∏ÂíåÈùûÂïÜÊ•≠ÊáâÁî®‰∏≠ÔºåÂºïËµ∑‰∫ÜË∂ä‰æÜË∂äÂ§ßÁöÑËààË∂£„ÄÇÂÑòÁÆ°ÁÑ°Ê≥ïËàáÂÖ∂Â∞àÊúâÂ∞çÊáâÁî¢ÂìÅÁöÑÁ≤æÁ∑ªÂäüËÉΩÁõ∏ÂåπÈÖçÔºå‰ΩÜÈñãÊîæÊ®°ÂûãÂú®Èù©Êñ∞ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈñãÊîæÂéüÂßãÁ¢º LLM Âíå AIFM Âú®ÈñãÁôºÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÊñπÈù¢ÁöÑÂâçÊôØÔºå‰∏¶ÊèêÂá∫‰∫ÜÂÖ©È†ÖÈóúÈçµË≤¢Áçª„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂ∞çÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÈñãÊîæÂéüÂßãÁ¢ºÈÜ´ÁôÇ‰øùÂÅ• LLM Âíå AIFM ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑË™øÊü•Ôºå‰∏¶‰ªãÁ¥π‰∫ÜÈÄô‰∫õÈñãÊîæ AIFM ÁöÑÂàÜÈ°ûÊ≥ïÔºåÂ∞çÂÆÉÂÄëÂú®ÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•‰ªªÂãô‰∏≠ÁöÑÊïàÁî®ÈÄ≤Ë°å‰∫ÜÂàÜÈ°û„ÄÇÂÖ∂Ê¨°ÔºåÁÇ∫‰∫ÜË©ï‰º∞ÈñãÊîæ LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈÄöÁî®ÊáâÁî®ÔºåÊàëÂÄëÂ∞çÂÄã‰∫∫ÂåñËôïÊñπÈÄ≤Ë°å‰∫ÜÊ°à‰æãÁ†îÁ©∂„ÄÇÈÄôÈ†Ö‰ªªÂãôÁâπÂà•ÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂú®Êèê‰æõÈáèË∫´ÂÆöÂà∂ÁöÑÊÇ£ËÄÖÁâπÂÆöËó•Áâ©ÊñπÈù¢ÁôºÊèÆËëóÈóúÈçµ‰ΩúÁî®ÔºåÂèØ‰ª•Â§ßÂ§ßÊîπÂñÑÊ≤ªÁôÇÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÈñãÊîæÂéüÂßãÁ¢ºÊ®°ÂûãËàáÂ∞àÊúâÊ®°ÂûãÂú®ÊúâÂíåÊ≤íÊúâÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÁöÑË®≠ÁΩÆ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ°‰∏çÂ§™Á≤æÁ∑ªÔºå‰ΩÜÈñãÊîæ LLM Âú®Ëàá RAG Á≠âÂü∫Á§éÊäÄË°ìÈÖçÂ∞çÊôÇÔºåÂèØ‰ª•ÂØ¶ÁèæËàáÂ∞àÊúâÊ®°ÂûãÁõ∏Áï∂ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÂº∑Ë™ø LLM Ë≥¶ËÉΩÁöÑÂÄãÊÄßÂåñËôïÊñπÁöÑËá®Â∫äÊÑèÁæ©ÔºåÊàëÂÄëÈÄöÈÅéÂ∞àÂÆ∂Ëá®Â∫äÈÜ´ÁîüÈÄ≤Ë°å‰∫Ü‰∏ªËßÄË©ï‰º∞„ÄÇÊàëÂÄëÈÇÑË©≥Á¥∞Ë™™Êòé‰∫ÜËàáÊø´Áî®Âº∑Â§ßÁöÑ LLM Âíå AIFM Áõ∏ÈóúÁöÑÂÄ´ÁêÜËÄÉÈáèÂíåÊΩõÂú®È¢®Èö™ÔºåÂº∑Ë™ø‰∫ÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Ë¨πÊÖéÂíåË≤†Ë≤¨‰ªªÂú∞ÂØ¶ÊñΩÁöÑÂøÖË¶ÅÊÄß„ÄÇ</paragraph>

##### **Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**
2502.02561v1 by Shayan Kiyani, George Pappas, Aaron Roth, Hamed Hassani

A fundamental question in data-driven decision making is how to quantify the
uncertainty of predictions in ways that can usefully inform downstream action.
This interface between prediction uncertainty and decision-making is especially
important in risk-sensitive domains, such as medicine. In this paper, we
develop decision-theoretic foundations that connect uncertainty quantification
using prediction sets with risk-averse decision-making. Specifically, we answer
three fundamental questions: (1) What is the correct notion of uncertainty
quantification for risk-averse decision makers? We prove that prediction sets
are optimal for decision makers who wish to optimize their value at risk. (2)
What is the optimal policy that a risk averse decision maker should use to map
prediction sets to actions? We show that a simple max-min decision policy is
optimal for risk-averse decision makers. Finally, (3) How can we derive
prediction sets that are optimal for such decision makers? We provide an exact
characterization in the population regime and a distribution free finite-sample
construction. Answering these questions naturally leads to an algorithm,
Risk-Averse Calibration (RAC), which follows a provably optimal design for
deriving action policies from predictions. RAC is designed to be both
practical-capable of leveraging the quality of predictions in a black-box
manner to enhance downstream utility-and safe-adhering to a user-defined risk
threshold and optimizing the corresponding risk quantile of the user's
downstream utility. Finally, we experimentally demonstrate the significant
advantages of RAC in applications such as medical diagnosis and recommendation
systems. Specifically, we show that RAC achieves a substantially improved
trade-off between safety and utility, offering higher utility compared to
existing methods while maintaining the safety guarantee.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë≥áÊñôÈ©ÖÂãïÊ±∫Á≠ñ‰∏≠Ôºå‰∏ÄÂÄãÂü∫Êú¨ÂïèÈ°åÊòØÔºåÂ¶Ç‰ΩïÈáèÂåñÈ†êÊ∏¨ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºå‰ª•ËÉΩÊúâÁî®Âú∞ÂëäÁü•‰∏ãÊ∏∏Ë°åÂãï„ÄÇ
È†êÊ∏¨‰∏çÁ¢∫ÂÆöÊÄßÂíåÊ±∫Á≠ñÂà∂ÂÆö‰πãÈñìÁöÑÈÄôÁ®Æ‰ªãÈù¢ÔºåÂú®È¢®Èö™ÊïèÊÑüÈ†òÂüü‰∏≠ÁâπÂà•ÈáçË¶ÅÔºå‰æãÂ¶ÇÈÜ´Â≠∏„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë
ÁôºÂ±ï‰∫ÜÊ±∫Á≠ñÁêÜË´ñÂü∫Á§éÔºåÂÆÉÂà©Áî®È†êÊ∏¨ÈõÜÂêàÂ∞á‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñËàáÈ¢®Èö™Ë¶èÈÅøÊ±∫Á≠ñÂà∂ÂÆöËÅØÁπ´Ëµ∑‰æÜ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂõûÁ≠î
‰∫Ü‰∏âÂÄãÂü∫Êú¨ÂïèÈ°åÔºö(1) Â∞çÊñºÈ¢®Èö™Ë¶èÈÅøÊ±∫Á≠ñËÄÖ‰æÜË™™Ôºå‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÁöÑÊ≠£Á¢∫Ê¶ÇÂøµÊòØ‰ªÄÈ∫ºÔºüÊàëÂÄëË≠âÊòéÔºåÂ∞çÊñºÂ∏åÊúõÊúÄ‰Ω≥ÂåñÂÖ∂È¢®Èö™ÂÉπÂÄºÁöÑÊ±∫Á≠ñËÄÖ‰æÜË™™ÔºåÈ†êÊ∏¨ÈõÜÂêàÊòØÊúÄ‰Ω≥ÁöÑ„ÄÇ(2)
È¢®Èö™Ë¶èÈÅøÊ±∫Á≠ñËÄÖÊáâ‰ΩøÁî®‰ªÄÈ∫ºÊúÄ‰Ω≥ÊîøÁ≠ñÔºåÂ∞áÈ†êÊ∏¨ÈõÜÂêàÊò†Â∞ÑÂà∞Ë°åÂãïÔºüÊàëÂÄëË°®ÊòéÔºåÂ∞çÊñºÈ¢®Èö™Ë¶èÈÅøÊ±∫Á≠ñËÄÖ‰æÜË™™Ôºå‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÊúÄÂ§ßÊúÄÂ∞èÊ±∫Á≠ñÊîøÁ≠ñÊòØÊúÄ‰Ω≥ÁöÑ„ÄÇÊúÄÂæåÔºå(3) ÊàëÂÄëÂ¶Ç‰ΩïÊé®Â∞éÂá∫Â∞çÊ≠§È°ûÊ±∫Á≠ñËÄÖ‰æÜË™™ÊúÄ‰Ω≥ÁöÑÈ†êÊ∏¨ÈõÜÂêàÔºüÊàëÂÄëÂú®Á∏ΩÈ´îÁØÑÂúçÂÖßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁ¢∫ÂàáÁöÑË°®ÂæµÔºå‰∏¶Êèê‰æõ‰∫Ü‰∏ÄÂÄã‰∏ç‰æùË≥¥ÂàÜ‰ΩàÁöÑÊúâÈôêÊ®£Êú¨Âª∫Êßã„ÄÇÂõûÁ≠îÈÄô‰∫õÂïèÈ°åËá™ÁÑ∂ÊúÉÂ∞éËá¥‰∏ÄÂÄãÊºîÁÆóÊ≥ïÔºåÈ¢®Èö™Ë¶èÈÅøÊ†°Ê∫ñ (RAC)ÔºåÂÆÉÈÅµÂæ™‰∏ÄÂÄãÂèØË≠âÊòéÊúÄ‰Ω≥ÁöÑË®≠Ë®àÔºåÂæûÈ†êÊ∏¨‰∏≠Êé®Â∞éÂá∫Ë°åÂãïÊîøÁ≠ñ„ÄÇRAC Ë¢´Ë®≠Ë®àÁÇ∫Êó¢ÂØ¶Áî®‚Äî‚ÄîËÉΩÂ§†‰ª•ÈªëÁõíÊñπÂºèÂà©Áî®È†êÊ∏¨ÁöÑÂìÅË≥™‰æÜÂ¢ûÂº∑‰∏ãÊ∏∏ÊïàÁî®‚Äî‚ÄîÂèàÂÆâÂÖ®‚Äî‚ÄîÈÅµÂÆà‰ΩøÁî®ËÄÖÂÆöÁæ©ÁöÑÈ¢®Èö™ÈñæÂÄºÔºå‰∏¶ÊúÄ‰Ω≥Âåñ‰ΩøÁî®ËÄÖÁöÑ‰∏ãÊ∏∏ÊïàÁî®ÁöÑÂ∞çÊáâÈ¢®Èö™ÂàÜ‰ΩçÊï∏„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂú®ÈÜ´Â≠∏Ë®∫Êñ∑ÂíåÊé®Ëñ¶Á≥ªÁµ±Á≠âÊáâÁî®‰∏≠Ôºå‰ª•ÂØ¶È©óÊñπÂºèË≠âÊòé‰∫Ü RAC ÁöÑÈ°ØËëóÂÑ™Èªû„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåRAC Âú®ÂÆâÂÖ®ÊÄßÂíåÊïàÁî®‰πãÈñìÂØ¶Áèæ‰∫ÜÈ°ØËëóÊîπÂñÑÁöÑÊäòË°∑ÔºåÂú®Á∂≠ÊåÅÂÆâÂÖ®‰øùË≠âÁöÑÂêåÊôÇÔºåÊèê‰æõ‰∫ÜÊõ¥È´òÁöÑÊïàÁî®„ÄÇ</paragraph>

##### **A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**
2502.02489v1 by Edward Ellis, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali

Ultrasound (US) imaging is clinically invaluable due to its noninvasive and
safe nature. However, interpreting US images is challenging, requires
significant expertise, and time, and is often prone to errors. Deep learning
offers assistive solutions such as segmentation. Supervised methods rely on
large, high-quality, and consistently labeled datasets, which are challenging
to curate. Moreover, these methods tend to underperform on out-of-distribution
data, limiting their clinical utility. Self-supervised learning (SSL) has
emerged as a promising alternative, leveraging unlabeled data to enhance model
performance and generalisability. We introduce a contrastive SSL approach
tailored for B-mode US images, incorporating a novel Relation Contrastive Loss
(RCL). RCL encourages learning of distinct features by differentiating positive
and negative sample pairs through a learnable metric. Additionally, we propose
spatial and frequency-based augmentation strategies for the representation
learning on US images. Our approach significantly outperforms traditional
supervised segmentation methods across three public breast US datasets,
particularly in data-limited scenarios. Notable improvements on the Dice
similarity metric include a 4% increase on 20% and 50% of the BUSI dataset,
nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4%
and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.
Furthermore, we demonstrate superior generalisability on the
out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6%
compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST
training data, respectively. Our research highlights that domain-inspired SSL
can improve US segmentation, especially under data-limited conditions.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ (US) ÂΩ±ÂÉèÁî±ÊñºÂÖ∂Èùû‰æµÂÖ•ÊÄß‰∏îÂÆâÂÖ®ÁöÑÁâπÊÄßÔºåÂú®Ëá®Â∫ä‰∏äÊ•µÂÖ∑ÂÉπÂÄº„ÄÇÁÑ∂ËÄåÔºåËß£ËÆÄË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÊôÇÈñìÔºåËÄå‰∏îÁ∂ìÂ∏∏ÂÆπÊòìÂá∫ÈåØ„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊèê‰æõ‰∫ÜËºîÂä©Ëß£Ê±∫ÊñπÊ°àÔºå‰æãÂ¶ÇÂàÜÂâ≤„ÄÇÁõ£Áù£ÂºèÊñπÊ≥ï‰æùË≥¥ÊñºÂ§ßÈáè„ÄÅÈ´òÂìÅË≥™‰∏îÊ®ôÁ±§‰∏ÄËá¥ÁöÑË≥áÊñôÈõÜÔºåËÄåÈÄôÂú®Á≠ñÂäÉ‰∏äÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊñπÊ≥ïÂú®ÂàÜ‰ΩàÂ§ñË≥áÊñô‰∏äÁöÑË°®ÁèæÂæÄÂæÄ‰∏ç‰Ω≥ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑËá®Â∫äÊïàÁî®„ÄÇËá™Áõ£Áù£Â≠∏Áøí (SSL) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂÆÉÂà©Áî®Êú™Ê®ôÁ±§Ë≥áÊñô‰æÜÂ¢ûÂº∑Ê®°ÂûãÊïàËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂ∞çÊØîÂºè SSL ÊñπÊ≥ïÔºåÂ∞àÈñÄÈáùÂ∞ç B Ê®°ÂºèË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÔºå‰∏¶Á¥çÂÖ•‰∫ÜÊñ∞Á©éÁöÑÈóú‰øÇÂ∞çÊØîÊêçÂ§± (RCL)„ÄÇRCL ÈÄèÈÅé‰∏ÄÂÄãÂèØÂ≠∏ÁøíÁöÑÊåáÊ®ôÂçÄÂàÜÊ≠£Ë≤†Ê®£Êú¨Â∞çÔºå‰æÜÈºìÂãµÂ≠∏Áøí‰∏çÂêåÁöÑÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁî®ÊñºË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏äË°®ÂæµÂ≠∏ÁøíÁöÑÁ©∫ÈñìÂíåÈ†ªÁéáÂ¢ûÂº∑Á≠ñÁï•„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®‰∏âÂÄãÂÖ¨ÈñãÁöÑ‰π≥ÊàøË∂ÖÈü≥Ê≥¢Ë≥áÊñôÈõÜ‰∏äÈ°ØËëóÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÁõ£Áù£ÂºèÂàÜÂâ≤ÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂú® Dice Áõ∏‰ººÊÄßÊåáÊ®ô‰∏äÁöÑÈ°ØËëóÊîπÈÄ≤ÂåÖÊã¨Âú® BUSI Ë≥áÊñôÈõÜÁöÑ 20% Âíå 50% ‰∏äÂ¢ûÂä†‰∫Ü 4%ÔºåÂú® BrEaST Ë≥áÊñôÈõÜÁöÑ 20% Âíå 50% ‰∏äÂ¢ûÂä†‰∫ÜËøë 6% Âíå 9%Ôºå‰ª•ÂèäÂú® UDIAT Ë≥áÊñôÈõÜÁöÑ 20% Âíå 50% ‰∏äÂàÜÂà•Â¢ûÂä†‰∫Ü 6.4% Âíå 3.7%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ÂàÜ‰ΩàÂ§ñÁöÑ UDIAT Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõÔºåËàá‰ΩøÁî® BUSI Âíå BrEaST Ë®ìÁ∑¥Ë≥áÊñôÁöÑ 20% Âíå 50% ÁöÑÁõ£Áù£ÂºèÂü∫Ê∫ñÁõ∏ÊØîÔºåÊïàËÉΩÂàÜÂà•ÊèêÂçá‰∫Ü 20.6% Âíå 13.6%„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™øÔºåÈ†òÂüüÂïüÁôºÁöÑ SSL ÂèØ‰ª•ÊîπÂñÑË∂ÖÈü≥Ê≥¢ÂàÜÂâ≤ÔºåÁâπÂà•ÊòØÂú®Ë≥áÊñôÊúâÈôêÁöÑÊ¢ù‰ª∂‰∏ã„ÄÇ

##### **Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**
2502.02438v1 by Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz

Medical multimodal large language models (MLLMs) are becoming an instrumental
part of healthcare systems, assisting medical personnel with decision making
and results analysis. Models for radiology report generation are able to
interpret medical imagery, thus reducing the workload of radiologists. As
medical data is scarce and protected by privacy regulations, medical MLLMs
represent valuable intellectual property. However, these assets are potentially
vulnerable to model stealing, where attackers aim to replicate their
functionality via black-box access. So far, model stealing for the medical
domain has focused on classification; however, existing attacks are not
effective against MLLMs. In this paper, we introduce Adversarial Domain
Alignment (ADA-STEAL), the first stealing attack against medical MLLMs.
ADA-STEAL relies on natural images, which are public and widely available, as
opposed to their medical counterparts. We show that data augmentation with
adversarial noise is sufficient to overcome the data distribution gap between
natural images and the domain-specific distribution of the victim MLLM.
Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that
Adversarial Domain Alignment enables attackers to steal the medical MLLM
without any access to medical data.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) Ê≠£Âú®ÊàêÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂçîÂä©ÈÜ´ÁôÇ‰∫∫Âì°ÈÄ≤Ë°åÊ±∫Á≠ñÂíåÁµêÊûúÂàÜÊûê„ÄÇÊîæÂ∞ÑÂ†±ÂëäÁîüÊàêÁöÑÊ®°ÂûãËÉΩÂ§†Ëß£ÈáãÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂæûËÄåÊ∏õËºïÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑÂ∑•‰ΩúË≤†Êìî„ÄÇÁî±ÊñºÈÜ´ÁôÇË≥áÊñôÁ®ÄÂ∞ë‰∏îÂèóÈö±ÁßÅÊ≥ïË¶è‰øùË≠∑ÔºåÈÜ´ÁôÇ MLLM ‰ª£Ë°®‰∫ÜÊúâÂÉπÂÄºÁöÑÊô∫ÊÖßË≤°Áî¢„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õË≥áÁî¢ÊΩõÂú®Âú∞ÂÆπÊòìÂèóÂà∞Ê®°ÂûãÁ´äÂèñÁöÑÊîªÊìäÔºåÊîªÊìäËÄÖÊó®Âú®ÈÄèÈÅéÈªëÁõíÂ≠òÂèñ‰æÜË§áË£ΩÂÖ∂ÂäüËÉΩ„ÄÇÂà∞ÁõÆÂâçÁÇ∫Ê≠¢ÔºåÈáùÂ∞çÈÜ´ÁôÇÈ†òÂüüÁöÑÊ®°ÂûãÁ´äÂèñ‰∏ÄÁõ¥Â∞àÊ≥®ÊñºÂàÜÈ°ûÔºõÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊîªÊìäÂ∞ç MLLM Ê≤íÊúâÊïà„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ∞çÊäóÂüüÂ∞çÈΩä (ADA-STEAL)ÔºåÈÄôÊòØÈáùÂ∞çÈÜ´ÁôÇ MLLM ÁöÑÁ¨¨‰∏ÄÂÄãÁ´äÂèñÊîªÊìä„ÄÇËàáÈÜ´ÁôÇÂ∞çÊáâÁâ©Áõ∏ÂèçÔºåADA-STEAL ‰æùË≥¥ÊñºÂÖ¨Èñã‰∏îÂª£Ê≥õÂèØÁî®ÁöÑËá™ÁÑ∂ÂΩ±ÂÉè„ÄÇÊàëÂÄëË°®ÊòéÔºåÂ∞çÊäóÈõúË®äÁöÑË≥áÊñôÊì¥ÂÖÖË∂≥‰ª•ÂÖãÊúçËá™ÁÑ∂ÂΩ±ÂÉèËàáÂèóÂÆ≥ËÄÖ MLLM ÁöÑÁâπÂÆöÈ†òÂüüÂàÜ‰Ωà‰πãÈñìÁöÑË≥áÊñôÂàÜ‰ΩàÂ∑ÆË∑ù„ÄÇÂú® IU X-RAY Âíå MIMIC-CXR ÊîæÂ∞ÑÂ≠∏Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óË°®ÊòéÔºåÂ∞çÊäóÂüüÂ∞çÈΩä‰ΩøÊîªÊìäËÄÖËÉΩÂ§†Âú®‰∏çÂ≠òÂèñ‰ªª‰ΩïÈÜ´ÁôÇË≥áÊñôÁöÑÊÉÖÊ≥Å‰∏ãÁ´äÂèñÈÜ´ÁôÇ MLLM„ÄÇ

##### **Test Time Training for 4D Medical Image Interpolation**
2502.02341v1 by Qikang Zhang, Yingjie Lei, Zihao Zheng, Ziyang Chen, Zhonghao Xie

4D medical image interpolation is essential for improving temporal resolution
and diagnostic precision in clinical applications. Previous works ignore the
problem of distribution shifts, resulting in poor generalization under
different distribution. A natural solution would be to adapt the model to a new
test distribution, but this cannot be done if the test input comes without a
ground truth label. In this paper, we propose a novel test time training
framework which uses self-supervision to adapt the model to a new distribution
without requiring any labels. Indeed, before performing frame interpolation on
each test video, the model is trained on the same instance using a
self-supervised task, such as rotation prediction or image reconstruction. We
conduct experiments on two publicly available 4D medical image interpolation
datasets, Cardiac and 4D-Lung. The experimental results show that the proposed
method achieves significant performance across various evaluation metrics on
both datasets. It achieves higher peak signal-to-noise ratio values, 33.73dB on
Cardiac and 34.02dB on 4D-Lung. Our method not only advances 4D medical image
interpolation but also provides a template for domain adaptation in other
fields such as image segmentation and image registration.

ÊëòË¶ÅÔºö4D ÈÜ´Â≠∏ÂΩ±ÂÉèÊèíÂÄºÂ∞çÊñºÊèêÂçáÊôÇÈñìËß£ÊûêÂ∫¶ÂèäËá®Â∫äÊáâÁî®‰∏≠ÁöÑË®∫Êñ∑Á≤æÊ∫ñÂ∫¶Ëá≥ÈóúÈáçË¶Å„ÄÇÈÅéÂæÄÁöÑÁ†îÁ©∂ÂøΩÁï•‰∫ÜÂàÜ‰ΩàËΩâÁßªÂïèÈ°åÔºåÂ∞éËá¥Âú®‰∏çÂêåÂàÜ‰Ωà‰∏ãÊ≥õÂåñËÉΩÂäõ‰∏ç‰Ω≥„ÄÇ‰∏ÄÂÄãËá™ÁÑ∂ÁöÑËß£Ê±∫ÊñπÊ°àÊòØÂ∞áÊ®°ÂûãÈÅ©ÊáâÂà∞Êñ∞ÁöÑÊ∏¨Ë©¶ÂàÜ‰ΩàÔºå‰ΩÜÂ¶ÇÊûúÊ∏¨Ë©¶Ëº∏ÂÖ•Ê≤íÊúâÁúüÂØ¶Ê®ôÁ±§ÔºåÂ∞±ÁÑ°Ê≥ïÂÅöÂà∞ÈÄô‰∏ÄÈªû„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ∏¨Ë©¶ÊôÇÈñìË®ìÁ∑¥Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî®Ëá™ÊàëÁõ£Áù£‰æÜÈÅ©ÊáâÊ®°ÂûãÂà∞‰∏ÄÂÄãÊñ∞ÁöÑÂàÜ‰ΩàÔºåËÄå‰∏çÈúÄË¶Å‰ªª‰ΩïÊ®ôÁ±§„ÄÇ‰∫ãÂØ¶‰∏äÔºåÂú®Â∞çÊØèÂÄãÊ∏¨Ë©¶ÂΩ±ÁâáÂü∑Ë°åÂπÄÊèíÂÄº‰πãÂâçÔºå‰ΩøÁî®Ëá™ÊàëÁõ£Áù£‰ªªÂãôÔºà‰æãÂ¶ÇÊóãËΩâÈ†êÊ∏¨ÊàñÂΩ±ÂÉèÈáçÂª∫ÔºâÂú®Âêå‰∏ÄÂÄãÂØ¶‰æã‰∏äË®ìÁ∑¥Ê®°Âûã„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÂÖ¨ÈñãÁöÑ 4D ÈÜ´Â≠∏ÂΩ±ÂÉèÊèíÂÄºË≥áÊñôÈõÜÔºàCardiac Âíå 4D-LungÔºâ‰∏äÈÄ≤Ë°åÂØ¶È©ó„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏≠ÈÉΩÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊïàËÉΩ„ÄÇÂÆÉÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÂ≥∞ÂÄº‰ø°Âô™ÊØîÂÄºÔºåÂú® Cardiac ‰∏äÁÇ∫ 33.73dBÔºåÂú® 4D-Lung ‰∏äÁÇ∫ 34.02dB„ÄÇÊàëÂÄëÁöÑÊäÄË°ì‰∏çÂÉÖÊé®Âãï‰∫Ü 4D ÈÜ´Â≠∏ÂΩ±ÂÉèÊèíÂÄºÔºåÈÇÑÁÇ∫ÂÖ∂‰ªñÈ†òÂüüÔºà‰æãÂ¶ÇÂΩ±ÂÉèÂàÜÂâ≤ÂíåÂΩ±ÂÉèÈÖçÊ∫ñÔºâ‰∏≠ÁöÑÈ†òÂüüÈÅ©ÊáâÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁØÑÊú¨„ÄÇ

##### **Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**
2502.02249v1 by Atharva Mangeshkumar Agrawal, Rutika Pandurang Shinde, Vasanth Kumar Bhukya, Ashmita Chakraborty, Sagar Bharat Shah, Tanmay Shukla, Sree Pradeep Kumar Relangi, Nilesh Mutyam

Large language models (LLMs) have shown impressive capabilities in natural
language processing tasks, including dialogue generation. This research aims to
conduct a novel comparative analysis of two prominent techniques, fine-tuning
with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)
framework, in the context of doctor-patient chat conversations with multiple
datasets of mixed medical domains. The analysis involves three state-of-the-art
models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient
dialogues, we comprehensively evaluate the performance of models, assessing key
metrics such as language quality (perplexity, BLEU score), factual accuracy
(fact-checking against medical knowledge bases), adherence to medical
guidelines, and overall human judgments (coherence, empathy, safety). The
findings provide insights into the strengths and limitations of each approach,
shedding light on their suitability for healthcare applications. Furthermore,
the research investigates the robustness of the models in handling diverse
patient queries, ranging from general health inquiries to specific medical
conditions. The impact of domain-specific knowledge integration is also
explored, highlighting the potential for enhancing LLM performance through
targeted data augmentation and retrieval strategies.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠Â±ïÁèæ‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÂåÖÊã¨Â∞çË©±ÁîüÊàê„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Â∞çÂÖ©Á®ÆËëóÂêçÁöÑÊäÄË°ìÈÄ≤Ë°åÊñ∞Á©éÁöÑÊØîËºÉÂàÜÊûêÔºåÂç≥ÂæÆË™ø LoRA (‰ΩéÁß©ÈÅ©Êáâ) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Ê°ÜÊû∂ÔºåÂú®ÂÖ∑ÊúâÊ∑∑ÂêàÈÜ´ÁôÇÈ†òÂüüÁöÑÂ§öÂÄãË≥áÊñôÈõÜÁöÑÈÜ´ÊÇ£ËÅäÂ§©Â∞çË©±‰∏≠„ÄÇÂàÜÊûêÊ∂âÂèä‰∏âÂÄãÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºöLlama-2„ÄÅGPT Âíå LSTM Ê®°Âûã„ÄÇÊé°Áî®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´ÊÇ£Â∞çË©±ÔºåÊàëÂÄëÂÖ®Èù¢Ë©ï‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÔºåË©ï‰º∞Ë™ûË®ÄÂìÅË≥™ÔºàÂõ∞ÊÉëÂ∫¶„ÄÅBLEU ÂàÜÊï∏Ôºâ„ÄÅ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÔºàÂ∞çÁÖßÈÜ´Â≠∏Áü•Ë≠òÂ∫´ÈÄ≤Ë°å‰∫ãÂØ¶Êü•Ê†∏Ôºâ„ÄÅÈÅµÂÆàÈÜ´ÁôÇÊåáÂçó‰ª•ÂèäÊï¥È´î‰∫∫È°ûÂà§Êñ∑ÔºàÈÄ£Ë≤´ÊÄß„ÄÅÂêåÁêÜÂøÉ„ÄÅÂÆâÂÖ®ÊÄßÔºâÁ≠âÈóúÈçµÊåáÊ®ô„ÄÇÁ†îÁ©∂ÁµêÊûúÊ∑±ÂÖ•‰∫ÜËß£‰∫ÜÊØèÁ®ÆÊñπÊ≥ïÁöÑÂÑ™ÈªûÂíåÈôêÂà∂ÔºåÈó°Êòé‰∫ÜÂÆÉÂÄëÈÅ©Áî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÁöÑÈÅ©Áï∂ÊÄß„ÄÇÊ≠§Â§ñÔºåË©≤Á†îÁ©∂Ë™øÊü•‰∫ÜÊ®°ÂûãÂú®ËôïÁêÜÂ§öÊ®£ÂåñÊÇ£ËÄÖÊü•Ë©¢ÊôÇÁöÑÁ©©ÂÅ•ÊÄßÔºåÁØÑÂúçÂæû‰∏ÄËà¨ÂÅ•Â∫∑Ë©¢ÂïèÂà∞ÁâπÂÆöÈÜ´ÁôÇÁãÄÊ≥Å„ÄÇÈÇÑÊé¢Ë®é‰∫ÜÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊï¥ÂêàÁöÑÂΩ±ÈüøÔºåÂº∑Ë™ø‰∫ÜÈÄöÈÅéÊúâÈáùÂ∞çÊÄßÁöÑË≥áÊñôÊì¥ÂÖÖÂíåÊ™¢Á¥¢Á≠ñÁï•‰æÜÂ¢ûÂº∑ LLM ÊÄßËÉΩÁöÑÊΩõÂäõ„ÄÇ

##### **Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**
2502.02618v1 by F. Xavier Gaya-Morey, Jose M. Buades-Rubio, Philippe Palanque, Raquel Lacuesta, Cristina Manresa-Yee

The rapid aging of the global population has highlighted the need for
technologies to support elderly, particularly in healthcare and emotional
well-being. Facial expression recognition (FER) systems offer a non-invasive
means of monitoring emotional states, with applications in assisted living,
mental health support, and personalized care. This study presents a systematic
review of deep learning-based FER systems, focusing on their applications for
the elderly population. Following a rigorous methodology, we analyzed 31
studies published over the last decade, addressing challenges such as the
scarcity of elderly-specific datasets, class imbalances, and the impact of
age-related facial expression differences. Our findings show that convolutional
neural networks remain dominant in FER, and especially lightweight versions for
resource-constrained environments. However, existing datasets often lack
diversity in age representation, and real-world deployment remains limited.
Additionally, privacy concerns and the need for explainable artificial
intelligence emerged as key barriers to adoption. This review underscores the
importance of developing age-inclusive datasets, integrating multimodal
solutions, and adopting XAI techniques to enhance system usability,
reliability, and trustworthiness. We conclude by offering recommendations for
future research to bridge the gap between academic progress and real-world
implementation in elderly care.

ÊëòË¶ÅÔºöÂÖ®ÁêÉ‰∫∫Âè£Âø´ÈÄüËÄÅÈæÑÂåñÁ™ÅÊòæ‰∫ÜÂØπÊäÄÊúØÁöÑÈúÄÊ±ÇÔºå‰ª•ÊîØÊåÅËÄÅÂπ¥‰∫∫ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂåªÁñó‰øùÂÅ•ÂíåÊÉÖÁª™ÂÅ•Â∫∑ÊñπÈù¢„ÄÇÈù¢ÈÉ®Ë°®ÊÉÖËØÜÂà´ (FER) Á≥ªÁªüÊèê‰æõ‰∫Ü‰∏ÄÁßçÈùû‰æµÂÖ•ÊÄßÁöÑÊÉÖÁª™Áä∂ÊÄÅÁõëÊµãÊâãÊÆµÔºåÂú®ËæÖÂä©ÁîüÊ¥ª„ÄÅÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅÂíå‰∏™ÊÄßÂåñÊä§ÁêÜ‰∏≠ÂæóÂà∞Â∫îÁî®„ÄÇÊú¨Á†îÁ©∂ÂØπÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ FER Á≥ªÁªüËøõË°å‰∫ÜÁ≥ªÁªüÁöÑÂõûÈ°æÔºåÈáçÁÇπÂÖ≥Ê≥®ÂÆÉ‰ª¨Âú®ËÄÅÂπ¥‰∫∫Áæ§‰∏≠ÁöÑÂ∫îÁî®„ÄÇÈÅµÂæ™‰∏•Ê†ºÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂú®ËøáÂéªÂçÅÂπ¥‰∏≠ÂèëË°®ÁöÑ 31 È°πÁ†îÁ©∂ÔºåËß£ÂÜ≥‰∫ÜËØ∏Â¶ÇËÄÅÂπ¥‰∫∫ÁâπÂÆöÊï∞ÊçÆÈõÜÁöÑÁ®ÄÁº∫ÊÄß„ÄÅÁ±ªÂà´‰∏çÂπ≥Ë°°‰ª•Âèä‰∏éÂπ¥ÈæÑÁõ∏ÂÖ≥ÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÂ∑ÆÂºÇÁöÑÂΩ±ÂìçÁ≠âÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂú® FER ‰∏≠‰ªçÁÑ∂Âç†‰∏ªÂØºÂú∞‰ΩçÔºåÁâπÂà´ÊòØÈíàÂØπËµÑÊ∫êÂèóÈôêÁéØÂ¢ÉÁöÑËΩªÈáèÁ∫ßÁâàÊú¨„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊï∞ÊçÆÈõÜÂæÄÂæÄÁº∫‰πèÂπ¥ÈæÑ‰ª£Ë°®ÊÄßÁöÑÂ§öÊ†∑ÊÄßÔºåÂπ∂‰∏îÁé∞ÂÆû‰∏ñÁïåÁöÑÈÉ®ÁΩ≤‰ªçÁÑ∂ÊúâÈôê„ÄÇÊ≠§Â§ñÔºåÈöêÁßÅÈóÆÈ¢òÂíåÂØπÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑÈúÄÊ±ÇÂ∑≤Êàê‰∏∫ÈááÁî®ËøáÁ®ã‰∏≠ÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇÊú¨Ê¨°ÂÆ°Êü•Âº∫Ë∞É‰∫ÜÂºÄÂèëÂåÖÂÆπÂπ¥ÈæÑÁöÑÊï∞ÊçÆÈõÜ„ÄÅÊï¥ÂêàÂ§öÊ®°ÂºèËß£ÂÜ≥ÊñπÊ°à‰ª•ÂèäÈááÁî® XAI ÊäÄÊúØ‰ª•Â¢ûÂº∫Á≥ªÁªüÂèØÁî®ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÂèØ‰ø°Â∫¶ÁöÑÈáçË¶ÅÊÄß„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂª∫ËÆÆÔºå‰ª•Âº•ÂêàÂ≠¶ÊúØËøõÂ±ï‰∏éËÄÅÂπ¥Êä§ÁêÜ‰∏≠ÁöÑÁé∞ÂÆû‰∏ñÁïåÂÆûÊñΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**
2502.02109v1 by Yuxiao Cheng, Xinxin Song, Ziqian Wang, Qin Zhong, Kunlun He, Jinli Suo

Recent advances in deep learning (DL) have prompted the development of
high-performing early warning score (EWS) systems, predicting clinical
deteriorations such as acute kidney injury, acute myocardial infarction, or
circulatory failure. DL models have proven to be powerful tools for various
tasks but come with the cost of lacking interpretability and limited
generalizability, hindering their clinical applications. To develop a practical
EWS system applicable to various outcomes, we propose causally-informed
explainable early prediction model, which leverages causal discovery to
identify the underlying causal relationships of prediction and thus owns two
unique advantages: demonstrating the explicit interpretation of the prediction
while exhibiting decent performance when applied to unfamiliar environments.
Benefiting from these features, our approach achieves superior accuracy for 6
different critical deteriorations and achieves better generalizability across
different patient groups, compared to various baseline algorithms. Besides, we
provide explicit causal pathways to serve as references for assistant clinical
diagnosis and potential interventions. The proposed approach enhances the
practical application of deep learning in various medical scenarios.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí (DL) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰øÉ‰ΩøÈñãÁôºÂá∫È´òÊÄßËÉΩÊó©ÊúüÈ†êË≠¶Ë©ïÂàÜ (EWS) Á≥ªÁµ±ÔºåÈ†êÊ∏¨ÊÄ•ÊÄßËÖéËáüÊêçÂÇ∑„ÄÅÊÄ•ÊÄßÂøÉËÇåÊ¢óÂ°ûÊàñÂæ™Áí∞Ë°∞Á´≠Á≠âËá®Â∫äÊÉ°Âåñ„ÄÇDL Ê®°ÂûãÂ∑≤Ë¢´Ë≠âÊòéÊòØÂêÑÁ®Æ‰ªªÂãôÁöÑÂº∑Â§ßÂ∑•ÂÖ∑Ôºå‰ΩÜ‰ª£ÂÉπÊòØÁº∫‰πèÂèØËß£ÈáãÊÄßÂíåÊúâÈôêÁöÑÊ¶ÇÊã¨ÊÄßÔºåÈòªÁ§ô‰∫ÜÂÖ∂Ëá®Â∫äÊáâÁî®„ÄÇÁÇ∫‰∫ÜÈñãÁôºÈÅ©Áî®ÊñºÂêÑÁ®ÆÁµêÊûúÁöÑÂØ¶Áî® EWS Á≥ªÁµ±ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂõ†ÊûúÈóú‰øÇËß£ÈáãÊÄßÊó©ÊúüÈ†êÊ∏¨Ê®°ÂûãÔºåÂÆÉÂà©Áî®Âõ†ÊûúÁôºÁèæ‰æÜË≠òÂà•È†êÊ∏¨ÁöÑÊΩõÂú®Âõ†ÊûúÈóú‰øÇÔºåÂæûËÄåÊìÅÊúâÂÖ©ÂÄãÁç®ÁâπÁöÑÂÑ™ÈªûÔºöÂ±ïÁ§∫È†êÊ∏¨ÁöÑÊòéÁ¢∫Ëß£ÈáãÔºåÂêåÊôÇÂú®ÊáâÁî®Êñº‰∏çÁÜüÊÇâÁöÑÁí∞Â¢ÉÊôÇË°®ÁèæÂá∫ËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇÂæóÁõäÊñºÈÄô‰∫õÁâπÊÄßÔºåËàáÂêÑÁ®ÆÂü∫Á∑öÊºîÁÆóÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® 6 Á®Æ‰∏çÂêåÁöÑÂç±ÈáçÊÉ°Âåñ‰∏≠ÂØ¶Áèæ‰∫ÜÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰∏¶Âú®‰∏çÂêåÁöÑÊÇ£ËÄÖÁæ§È´î‰∏≠ÂØ¶Áèæ‰∫ÜÊõ¥Â•ΩÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÊòéÁ¢∫ÁöÑÂõ†ÊûúÈÄîÂæëÔºå‰ΩúÁÇ∫ËºîÂä©Ëá®Â∫äË®∫Êñ∑ÂíåÊΩõÂú®Âπ≤È†êÊé™ÊñΩÁöÑÂèÉËÄÉ„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÊ∑±Â∫¶Â≠∏ÁøíÂú®ÂêÑÁ®ÆÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÁöÑÂØ¶ÈöõÊáâÁî®„ÄÇ

##### **JingFang: A Traditional Chinese Medicine Large Language Model of Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment**
2502.04345v1 by Yehan Yan, Tianhao Ma, Ruotai Li, Xinhan Zheng, Guodong Shan, Chisheng Li

Traditional Chinese medicine (TCM) plays a vital role in health protection
and disease treatment, but its practical application requires extensive medical
knowledge and clinical experience. Existing TCM Large Language Models (LLMs)
exhibit critical limitations of uncomprehensive medical consultation and
diagnoses, and inaccurate syndrome differentiation-based treatment. To address
these issues, this study establishes JingFang (JF): a novel TCM Large Language
Model that demonstrates the expert-level capability of medical diagnosis and
syndrome differentiation-based treatment. We innovate a Multi-agent Dynamic
Collaborative Chain-of-Thought Mechanism (MDCCTM) for medical consultation,
enabling JF with effective and accurate diagnostic ability. In addition, a
Syndrome Agent and a Dual-Stage Retrieval Scheme (DSRS) are developed to
significantly enhance the capacity of JF for disease treatment based on
syndrome differentiation. JingFang not only facilitates the application of LLMs
but also promotes the effective practice of TCM in human health protection and
disease treatment.

ÊëòË¶ÅÔºö‰∏≠ÈÜ´Ëó•Âú®‰øùÂÅ•ËàáÁñæÁóÖÊ≤ªÁôÇ‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤Ôºå‰ΩÜÂÖ∂ÂØ¶ÂãôÊáâÁî®ÈúÄË¶ÅÊ∑±ÂéöÁöÑÈÜ´Â≠∏Áü•Ë≠òËàáËá®Â∫äÁ∂ìÈ©ó„ÄÇÁèæÊúâÁöÑ‰∏≠ÈÜ´Â§ßË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂ≠òÂú®ËëóÈÜ´ÁôÇË´ÆË©¢ËàáË®∫Êñ∑‰∏çÂÖ®Èù¢„ÄÅÁóáÂÄôÂàÜÂûãÊ≤ªÁôÇ‰∏çÊ∫ñÁ¢∫ÁöÑÈáçÂ§ßÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊú¨Á†îÁ©∂Âª∫Á´ã‰∫ÜÁ≤æÊñπÔºàJFÔºâÔºö‰∏ÄÂÄãÊñ∞Á©éÁöÑ‰∏≠ÈÜ´Â§ßË™ûË®ÄÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂ∞àÂÆ∂Á¥öÁöÑÈÜ´ÁôÇË®∫Êñ∑ËàáÁóáÂÄôÂàÜÂûãÊ≤ªÁôÇËÉΩÂäõ„ÄÇÊàëÂÄëÂâµÊñ∞‰∫Ü‰∏ÄÂÄãÂ§öÊô∫ËÉΩÈ´îÂãïÊÖãÂçî‰ΩúÊÄùËÄÉÈèàÊ©üÂà∂ÔºàMDCCTMÔºâÁî®ÊñºÈÜ´ÁôÇË´ÆË©¢ÔºåËÆì JF ÂÖ∑ÂÇôÊúâÊïà‰∏îÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈÇÑÈñãÁôº‰∫Ü‰∏ÄÂÄãÁóáÂÄôÊô∫ËÉΩÈ´îÂíå‰∏ÄÂÄãÈõôÈöéÊÆµÊ™¢Á¥¢ÊñπÊ°àÔºàDSRSÔºâÔºå‰ª•È°ØËëóÂ¢ûÂº∑ JF Âü∫ÊñºÁóáÂÄôÂàÜÂûãÁöÑÁñæÁóÖÊ≤ªÁôÇËÉΩÂäõ„ÄÇÁ≤æÊñπ‰∏çÂÉÖ‰øÉÈÄ≤‰∫Ü LLM ÁöÑÊáâÁî®Ôºå‰πüÊé®Âãï‰∫Ü‰∏≠ÈÜ´Ëó•Âú®‰∫∫È°û‰øùÂÅ•ËàáÁñæÁóÖÊ≤ªÁôÇ‰∏≠ÁöÑÊúâÊïàÂØ¶Ë∏ê„ÄÇ

##### **An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**
2502.01789v1 by Jiazi Tian, Liqin Wang, Pedram Fard, Valdery Moura Junior, Deborah Blacker, Jennifer S. Haas, Chirag Patel, Shawn N. Murphy, Lidia M. V. R. Moura, Hossein Estiri

Early identification of cognitive concerns is critical but often hindered by
subtle symptom presentation. This study developed and validated a fully
automated, multi-agent AI workflow using LLaMA 3 8B to identify cognitive
concerns in 3,338 clinical notes from Mass General Brigham. The agentic
workflow, leveraging task-specific agents that dynamically collaborate to
extract meaningful insights from clinical notes, was compared to an
expert-driven benchmark. Both workflows achieved high classification
performance, with F1-scores of 0.90 and 0.91, respectively. The agentic
workflow demonstrated improved specificity (1.00) and achieved prompt
refinement in fewer iterations. Although both workflows showed reduced
performance on validation data, the agentic workflow maintained perfect
specificity. These findings highlight the potential of fully automated
multi-agent AI workflows to achieve expert-level accuracy with greater
efficiency, offering a scalable and cost-effective solution for detecting
cognitive concerns in clinical settings.

ÊëòË¶ÅÔºöÂèäÊó©Ëæ®Ë≠òË™çÁü•ÂïèÈ°åËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂ∏∏Â∏∏ÂèóÂà∞ÁóáÁãÄÂëàÁèæÈÅéÊñºÁ¥∞ÂæÆÁöÑÈòªÁ§ô„ÄÇÊú¨Á†îÁ©∂ÈñãÁôº‰∏¶È©óË≠â‰∫Ü‰∏ÄÂÄãÂÖ®Ëá™ÂãïÂåñ„ÄÅÂ§öÈáç‰ª£ÁêÜÁöÑ AI Â∑•‰ΩúÊµÅÁ®ãÔºå‰ΩøÁî® LLaMA 3 8B ‰æÜËæ®Ë≠ò‰æÜËá™È∫ªÁúÅÁ∏ΩÈÜ´Èô¢Â∏ÉËêäÊ†πÂàÜÈô¢ÁöÑ 3,338 ÂâáËá®Â∫äÁ≠ÜË®ò‰∏≠ÁöÑË™çÁü•ÂïèÈ°å„ÄÇÈÄôÂÄã‰ª£ÁêÜÂ∑•‰ΩúÊµÅÁ®ãÂà©Áî®‰∫ÜÁâπÂÆö‰ªªÂãôÁöÑ‰ª£ÁêÜÔºåÈÄô‰∫õ‰ª£ÁêÜÊúÉÂãïÊÖãÂêà‰ΩúÂæûËá®Â∫äÁ≠ÜË®ò‰∏≠ËêÉÂèñÂá∫ÊúâÊÑèÁæ©ÁöÑË¶ãËß£Ôºå‰∏¶ËàáÂ∞àÂÆ∂È©ÖÂãïÁöÑÂü∫Ê∫ñÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄôÂÖ©ÂÄãÂ∑•‰ΩúÊµÅÁ®ãÈÉΩÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÂàÜÈ°ûÊïàËÉΩÔºåF1 ÂàÜÊï∏ÂàÜÂà•ÁÇ∫ 0.90 Âíå 0.91„ÄÇ‰ª£ÁêÜÂ∑•‰ΩúÊµÅÁ®ãÂ±ïÁèæÂá∫Êõ¥Â•ΩÁöÑÁâπÁï∞ÊÄßÔºà1.00ÔºâÔºå‰∏¶‰∏îÂú®Êõ¥Â∞ëÁöÑÂèçË¶ÜÈÅãÁÆó‰∏≠ÈÅîÂà∞‰∫ÜÊèêÁ§∫Á≤æÁÖâ„ÄÇÂÑòÁÆ°ÈÄôÂÖ©ÂÄãÂ∑•‰ΩúÊµÅÁ®ãÂú®È©óË≠âË≥áÊñô‰∏äÁöÑÊïàËÉΩÈÉΩÈôç‰Ωé‰∫ÜÔºå‰ΩÜ‰ª£ÁêÜÂ∑•‰ΩúÊµÅÁ®ãÁ∂≠ÊåÅ‰∫ÜÂÆåÁæéÁöÑÁâπÁï∞ÊÄß„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÂÖ®Ëá™ÂãïÂåñÂ§öÈáç‰ª£ÁêÜ AI Â∑•‰ΩúÊµÅÁ®ãÁöÑÊΩõÂäõÔºåÂÆÉÂÄëËÉΩ‰ª•Êõ¥È´òÁöÑÊïàÁéáÈÅîÂà∞Â∞àÂÆ∂Á¥öÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÁÇ∫Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÂÅµÊ∏¨Ë™çÁü•ÂïèÈ°åÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÊì¥ÂÖÖ‰∏îÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate Cancer MRI Diagnosis**
2502.03482v1 by Chacha Chen, Han Liu, Jiamin Yang, Benjamin M. Mervak, Bora Kalaycioglu, Grace Lee, Emre Cakmakli, Matteo Bonatti, Sridhar Pudu, Osman Kahraman, Gul Gizem Pamuk, Aytekin Oto, Aritrick Chatterjee, Chenhao Tan

Despite the growing interest in human-AI decision making, experimental
studies with domain experts remain rare, largely due to the complexity of
working with domain experts and the challenges in setting up realistic
experiments. In this work, we conduct an in-depth collaboration with
radiologists in prostate cancer diagnosis based on MRI images. Building on
existing tools for teaching prostate cancer diagnosis, we develop an interface
and conduct two experiments to study how AI assistance and performance feedback
shape the decision making of domain experts. In Study 1, clinicians were asked
to provide an initial diagnosis (human), then view the AI's prediction, and
subsequently finalize their decision (human-AI team). In Study 2 (after a
memory wash-out period), the same participants first received aggregated
performance statistics from Study 1, specifically their own performance, the
AI's performance, and their human-AI team performance, and then directly viewed
the AI's prediction before making their diagnosis (i.e., no independent initial
diagnosis). These two workflows represent realistic ways that clinical AI tools
might be used in practice, where the second study simulates a scenario where
doctors can adjust their reliance and trust on AI based on prior performance
feedback. Our findings show that, while human-AI teams consistently outperform
humans alone, they still underperform the AI due to under-reliance, similar to
prior studies with crowdworkers. Providing clinicians with performance feedback
did not significantly improve the performance of human-AI teams, although
showing AI decisions in advance nudges people to follow AI more. Meanwhile, we
observe that the ensemble of human-AI teams can outperform AI alone, suggesting
promising directions for human-AI collaboration.

ÊëòË¶ÅÔºöÂÑòÁÆ°‰∫∫ÂÄëÂ∞ç‰∫∫È°ûËàá AI Ê±∫Á≠ñÂà∂ÂÆöË∂ä‰æÜË∂äÊÑüËààË∂£Ôºå‰ΩÜËàáÈ†òÂüüÂ∞àÂÆ∂Âêà‰ΩúÁöÑÂØ¶È©óÁ†îÁ©∂‰ªçÁÑ∂ÂæàÂ∞ëË¶ãÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂõ†ÁÇ∫ËàáÈ†òÂüüÂ∞àÂÆ∂Âêà‰ΩúÁöÑË§áÈõúÊÄßÔºå‰ª•ÂèäÂú®Ë®≠ÂÆöÂØ¶ÈöõÂØ¶È©óÊôÇÈù¢Ëá®ÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëËàáÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°åÊ∑±ÂÖ•Âêà‰ΩúÔºåÂü∫Êñº MRI ÂΩ±ÂÉèË®∫Êñ∑ÂâçÂàóËÖ∫Áôå„ÄÇÂª∫Á´ãÂú®Áî®ÊñºÊïôÊéàÂâçÂàóËÖ∫ÁôåË®∫Êñ∑ÁöÑÁèæÊúâÂ∑•ÂÖ∑‰∏äÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ªãÈù¢‰∏¶ÈÄ≤Ë°å‰∫ÜÂÖ©È†ÖÂØ¶È©óÔºå‰ª•Á†îÁ©∂ AI ÂçîÂä©ÂíåÊïàËÉΩÂõûÈ•ãÂ¶Ç‰ΩïÂ°ëÈÄ†È†òÂüüÂ∞àÂÆ∂ÁöÑÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÂú®Á†îÁ©∂ 1 ‰∏≠ÔºåË¶ÅÊ±ÇËá®Â∫äÈÜ´Â∏´Êèê‰æõÂàùÊ≠•Ë®∫Êñ∑Ôºà‰∫∫È°ûÔºâÔºåÁÑ∂ÂæåÊ™¢Ë¶ñ AI ÁöÑÈ†êÊ∏¨Ôºå‰∏¶Èö®ÂæåÁ¢∫ÂÆö‰ªñÂÄëÁöÑÊ±∫Á≠ñÔºà‰∫∫È°û-AI ÂúòÈöäÔºâ„ÄÇÂú®Á†îÁ©∂ 2ÔºàÁ∂ìÈÅé‰∏ÄÊÆµË®òÊÜ∂Ê∏ÖÈô§ÊúüÔºâ‰∏≠ÔºåÂêå‰∏Ä‰ΩçÂèÉËàáËÄÖÈ¶ñÂÖàÊî∂Âà∞Á†îÁ©∂ 1 ÁöÑÂΩôÁ∏ΩÊïàËÉΩÁµ±Ë®àË≥áÊñôÔºåÁâπÂà•ÊòØ‰ªñÂÄëËá™Â∑±ÁöÑÊïàËÉΩ„ÄÅAI ÁöÑÊïàËÉΩÔºå‰ª•Âèä‰ªñÂÄëÁöÑ‰∫∫È°û-AI ÂúòÈöäÊïàËÉΩÔºåÁÑ∂ÂæåÂú®ÂÅöÂá∫Ë®∫Êñ∑ÂâçÁõ¥Êé•Ê™¢Ë¶ñ AI ÁöÑÈ†êÊ∏¨ÔºàÂç≥ÔºåÊ≤íÊúâÁç®Á´ãÁöÑÂàùÊ≠•Ë®∫Êñ∑Ôºâ„ÄÇÈÄôÂÖ©ÂÄãÂ∑•‰ΩúÊµÅÁ®ã‰ª£Ë°®‰∫ÜËá®Â∫ä AI Â∑•ÂÖ∑Âú®ÂØ¶Âãô‰∏≠ÂèØËÉΩË¢´‰ΩøÁî®ÁöÑÊñπÂºèÔºåÂÖ∂‰∏≠Á¨¨‰∫åÂÄãÁ†îÁ©∂Ê®°Êì¨‰∫ÜÈÜ´ÁîüÂèØ‰ª•Ê†πÊìöÂÖàÂâçÁöÑÊïàËÉΩÂõûÈ•ãË™øÊï¥‰ªñÂÄëÂ∞ç AI ÁöÑ‰æùË≥¥Âíå‰ø°‰ªªÁöÑÊÉÖÊ≥Å„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ°‰∫∫È°û-AI ÂúòÈöäÂßãÁµÇÂÑ™ÊñºÂñÆÁç®ÁöÑ‰∫∫È°ûÔºå‰ΩÜÁî±Êñº‰æùË≥¥‰∏çË∂≥Ôºå‰ªñÂÄë‰ªçÁÑ∂Ë°®Áèæ‰∏çÂ¶Ç AIÔºåÈÄôËàá‰πãÂâçÈáùÂ∞çÁæ§ÁúæÂ∑•‰ΩúËÄÖÁöÑÁ†îÁ©∂È°û‰ºº„ÄÇÂÑòÁÆ°‰∫ãÂÖàÈ°ØÁ§∫ AI Ê±∫Á≠ñÊúÉ‰øÉ‰Ωø‰∫∫ÂÄëÊõ¥Â§öÂú∞ÈÅµÂæ™ AIÔºå‰ΩÜÂêëËá®Â∫äÈÜ´Â∏´Êèê‰æõÊïàËÉΩÂõûÈ•ã‰∏¶Êú™È°ØËëóÊîπÂñÑ‰∫∫È°û-AI ÂúòÈöäÁöÑÊïàËÉΩ„ÄÇÂêåÊôÇÔºåÊàëÂÄëËßÄÂØüÂà∞‰∫∫È°û-AI ÂúòÈöäÁöÑÈõÜÂêàÂèØ‰ª•ÂÑ™ÊñºÂñÆÁç®ÁöÑ AIÔºåÈÄôË°®Êòé‰∫Ü‰∫∫È°û-AI Âêà‰ΩúÁöÑÂâçÊôØ„ÄÇ

##### **Improving Transformer World Models for Data-Efficient RL**
2502.01591v1 by Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy

We present an approach to model-based RL that achieves a new state of the art
performance on the challenging Craftax-classic benchmark, an open-world 2D
survival game that requires agents to exhibit a wide range of general abilities
-- such as strong generalization, deep exploration, and long-term reasoning.
With a series of careful design choices aimed at improving sample efficiency,
our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps,
significantly outperforming DreamerV3, which achieves 53.2%, and, for the first
time, exceeds human performance of 65.0%. Our method starts by constructing a
SOTA model-free baseline, using a novel policy architecture that combines CNNs
and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna
with warmup", which trains the policy on real and imaginary data, (b) "nearest
neighbor tokenizer" on image patches, which improves the scheme to create the
transformer world model (TWM) inputs, and (c) "block teacher forcing", which
allows the TWM to reason jointly about the future tokens of the next timestep.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÊ®°ÂûãÁöÑ RL ÊñπÊ≥ïÔºåÂú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ Craftax-classic Âü∫Ê∫ñ‰∏äÂØ¶Áèæ‰∫ÜÊñ∞ÁöÑÊäÄË°ìÊ∞¥Ê∫ñÔºåÈÄôÊòØ‰∏ÄÂÄãÈñãÊîæ‰∏ñÁïåÁöÑ 2D ÁîüÂ≠òÈÅäÊà≤ÔºåË¶ÅÊ±Ç‰ª£ÁêÜ‰∫∫Â±ïÁèæÂª£Ê≥õÁöÑ‰∏ÄËà¨ËÉΩÂäõÔºå‰æãÂ¶ÇÂº∑Â§ßÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÅÊ∑±ÂÖ•Êé¢Á¥¢ÂíåÈï∑ÊúüÊé®ÁêÜ„ÄÇÈÄöÈÅé‰∏ÄÁ≥ªÂàóÊó®Âú®ÊèêÈ´òÊ®£Êú¨ÊïàÁéáÁöÑ‰ªîÁ¥∞Ë®≠Ë®àÈÅ∏ÊìáÔºåÊàëÂÄëÁöÑ MBRL ÊºîÁÆóÊ≥ïÂú®ÂÉÖ 1M Áí∞Â¢ÉÊ≠•È©üÂæåÂ∞±ÂØ¶Áèæ‰∫Ü 67.4% ÁöÑÁçéÂãµÔºåÈ°ØËëóÂÑ™Êñº DreamerV3ÔºàÂØ¶Áèæ 53.2%ÔºâÔºå‰∏¶‰∏îÈ¶ñÊ¨°Ë∂ÖÈÅé‰∫Ü‰∫∫È°ûÁöÑ 65.0% ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÁöÑÊºîÁÆóÊ≥ïÈ¶ñÂÖàÈÄöÈÅé‰ΩøÁî®ÁµêÂêà CNN Âíå RNN ÁöÑÊñ∞Á©éÁ≠ñÁï•Êû∂Êßã‰æÜÂª∫Êßã‰∏ÄÂÄã SOTA ÁÑ°Ê®°ÂûãÂü∫Á∑ö„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂ∞çÊ®ôÊ∫ñ MBRL Ë®≠ÂÆöÊñ∞Â¢û‰∫Ü‰∏âÈ†ÖÊîπÈÄ≤Ôºö(a)„ÄåÂ∏∂ÁÜ±Ë∫´ÁöÑ Dyna„ÄçÔºåÂÆÉÂú®ÁúüÂØ¶ÂíåÂÅáÊÉ≥Ë≥áÊñô‰∏äË®ìÁ∑¥Á≠ñÁï•Ôºå(b) ÂΩ±ÂÉèË≤ºÁâáÁöÑ„ÄåÊúÄËøëÈÑ∞‰ª£Á¢ºÂåñÂô®„ÄçÔºåÂÆÉÊîπÈÄ≤‰∫ÜÂª∫Á´ãËΩâÊèõÂô®‰∏ñÁïåÊ®°Âûã (TWM) Ëº∏ÂÖ•ÁöÑÊñπÊ°àÔºå‰ª•Âèä (c)„ÄåÂçÄÂ°äÊïôÂ∏´Âº∑Âà∂„ÄçÔºåÂÆÉÂÖÅË®± TWM ÂÖ±ÂêåÊé®ÁêÜ‰∏ã‰∏ÄÂÄãÊôÇÈñìÊ≠•Èï∑ÁöÑÊú™‰æÜ‰ª£Á¢º„ÄÇ

##### **Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**
2502.01377v1 by Zhi Zhang, Yan Liu, Mengxia Gao, Yu Yang, Jiannong Cao, Wai Kai Hou, Shirley Li, Sonata Yau, Yun Kwok Wing, Tatia M. C. Lee

Psychological resilience, defined as the ability to rebound from adversity,
is crucial for mental health. Compared with traditional resilience assessments
through self-reported questionnaires, resilience assessments based on
neurological data offer more objective results with biological markers, hence
significantly enhancing credibility. This paper proposes a novel data-efficient
model to address the scarcity of neurological data. We employ Neuro
Kolmogorov-Arnold Networks as the structure of the prediction model. In the
training stage, a new trait-informed multimodal representation algorithm with a
smart chunk technique is proposed to learn the shared latent space with limited
data. In the test stage, a new noise-informed inference algorithm is proposed
to address the low signal-to-noise ratio of the neurological data. The proposed
model not only shows impressive performance on both public datasets and
self-constructed datasets but also provides some valuable psychological
hypotheses for future research.

ÊëòË¶ÅÔºöÂøÉÁêÜÈüåÊÄßÔºåÂÆöÁæ©ÁÇ∫ÂæûÈÄÜÂ¢É‰∏≠ÂèçÂΩàÁöÑËÉΩÂäõÔºåÂ∞çÂøÉÁêÜÂÅ•Â∫∑Ëá≥ÈóúÈáçË¶Å„ÄÇËàáÈÄöÈÅéËá™ÊàëÂ†±ÂëäÂïèÂç∑ÁöÑÂÇ≥Áµ±ÈüåÊÄßË©ï‰º∞Áõ∏ÊØîÔºåÂü∫ÊñºÁ•ûÁ∂ìÊï∏ÊìöÁöÑÈüåÊÄßË©ï‰º∞Êèê‰æõ‰∫ÜÊõ¥ÂÆ¢ËßÄÁöÑÁµêÊûúÂíåÁîüÁâ©Ê®ôË®òÔºåÂæûËÄåÈ°ØËëóÊèêÈ´ò‰∫ÜÂèØ‰ø°Â∫¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊï∏ÊìöÈ´òÊïàÊ®°Âûã‰æÜËß£Ê±∫Á•ûÁ∂ìÊï∏ÊìöÁöÑÁ®ÄÁº∫ÊÄß„ÄÇÊàëÂÄëÊé°Áî®Á•ûÁ∂ìÁßëÁàæËé´Âì•ÁæÖÂ§´-ÈòøË´æÂæ∑Á∂≤Ë∑Ø‰ΩúÁÇ∫È†êÊ∏¨Ê®°ÂûãÁöÑÁµêÊßã„ÄÇÂú®Ë®ìÁ∑¥ÈöéÊÆµÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÁâπÂæµ‰ø°ÊÅØÂ§öÊ®°ÊÖãË°®Á§∫ÁÆóÊ≥ïÔºåÊé°Áî®Êô∫ËÉΩÂ°äÊäÄË°ìÔºå‰ª•ÊúâÈôêÁöÑÊï∏ÊìöÂ≠∏ÁøíÂÖ±‰∫´ÊΩõÂú®Á©∫Èñì„ÄÇÂú®Ê∏¨Ë©¶ÈöéÊÆµÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂô™ËÅ≤‰ø°ÊÅØÊé®ÁêÜÁÆóÊ≥ïÔºå‰ª•Ëß£Ê±∫Á•ûÁ∂ìÊï∏ÊìöÁöÑ‰ø°Âô™ÊØî‰ΩéÁöÑÂïèÈ°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°Âûã‰∏çÂÉÖÂú®ÂÖ¨ÂÖ±Êï∏ÊìöÈõÜÂíåËá™ÊßãÊï∏ÊìöÈõÜ‰∏äÈÉΩÈ°ØÁ§∫Âá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊÄßËÉΩÔºåÈÇÑÁÇ∫Êú™‰æÜÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏Ä‰∫õÊúâÂÉπÂÄºÁöÑÂøÉÁêÜÂÅáË®≠„ÄÇ

##### **OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**
2502.01243v1 by Chengfeng Zhou, Ji Wang, Juanjuan Qin, Yining Wang, Ling Sun, Weiwei Dai

Large language models (LLMs) have shown significant promise across various
medical applications, with ophthalmology being a notable area of focus. Many
ophthalmic tasks have shown substantial improvement through the integration of
LLMs. However, before these models can be widely adopted in clinical practice,
evaluating their capabilities and identifying their limitations is crucial. To
address this research gap and support the real-world application of LLMs, we
introduce the OphthBench, a specialized benchmark designed to assess LLM
performance within the context of Chinese ophthalmic practices. This benchmark
systematically divides a typical ophthalmic clinical workflow into five key
scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each
scenario, we developed multiple tasks featuring diverse question types,
resulting in a comprehensive benchmark comprising 9 tasks and 591 questions.
This comprehensive framework allows for a thorough assessment of LLMs'
capabilities and provides insights into their practical application in Chinese
ophthalmology. Using this benchmark, we conducted extensive experiments and
analyzed the results from 39 popular LLMs. Our evaluation highlights the
current gap between LLM development and its practical utility in clinical
settings, providing a clear direction for future advancements. By bridging this
gap, we aim to unlock the potential of LLMs and advance their development in
ophthalmology.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÈÜ´ÁôÇÊáâÁî®‰∏≠Â∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÊΩõÂäõÔºåÂÖ∂‰∏≠ÁúºÁßëÊòØ‰∏ÄÂÄãÂÄºÂæóÈóúÊ≥®ÁöÑÈáçË¶ÅÈ†òÂüü„ÄÇË®±Â§öÁúºÁßë‰ªªÂãôÂ∑≤ÈÄèÈÅéÊï¥Âêà LLM ËÄåÂ§ßÂπÖÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÄô‰∫õÊ®°ÂûãËÉΩÂª£Ê≥õÊáâÁî®ÊñºËá®Â∫äÂØ¶Âãô‰πãÂâçÔºåË©ï‰º∞ÂÖ∂ËÉΩÂäõ‰∏¶ÊâæÂá∫ÂÖ∂ÈôêÂà∂Ëá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÁ†îÁ©∂Â∑ÆË∑ù‰∏¶ÊîØÊè¥ LLM ÁöÑÂØ¶ÈöõÊáâÁî®ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü OphthBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÈñÄÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÊó®Âú®Ë©ï‰º∞ LLM Âú®‰∏≠ÂúãÁúºÁßëÂØ¶Âãô‰∏≠ÁöÑË°®Áèæ„ÄÇÊ≠§Âü∫Ê∫ñÊ∏¨Ë©¶Á≥ªÁµ±ÊÄßÂú∞Â∞áÂÖ∏ÂûãÁúºÁßëËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÂäÉÂàÜÁÇ∫‰∫îÂÄãÈóúÈçµÊÉÖÂ¢ÉÔºöÊïôËÇ≤„ÄÅÂàÜÊµÅ„ÄÅË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂíåÈ†êÂæå„ÄÇÂ∞çÊñºÊØèÂÄãÊÉÖÂ¢ÉÔºåÊàëÂÄëÈñãÁôº‰∫ÜÂ§öÈ†Ö‰ªªÂãôÔºåÂåÖÂê´Â§öÊ®£ÂåñÁöÑÂïèÈ°åÈ°ûÂûãÔºåÊúÄÂæåÁµÑÊàê‰∏ÄÂÄãÂåÖÂê´ 9 È†Ö‰ªªÂãôÂíå 591 ÂÄãÂïèÈ°åÁöÑÁ∂úÂêàÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÊ≠§Á∂úÂêàÊû∂ÊßãÂèØÂæπÂ∫ïË©ï‰º∞ LLM ÁöÑËÉΩÂäõÔºå‰∏¶Êèê‰æõÂÖ∂Âú®‰∏≠ÂúãÁúºÁßëÁöÑÂØ¶ÈöõÊáâÁî®Ë¶ãËß£„ÄÇ‰ΩøÁî®Ê≠§Âü∫Ê∫ñÊ∏¨Ë©¶ÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶ÂàÜÊûê‰∫Ü‰æÜËá™ 39 ÂÄãÁÜ±ÈñÄ LLM ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Âº∑Ë™ø‰∫Ü LLM ÈñãÁôºËàáÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÂØ¶ÈöõÊïàÁî®‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÁÇ∫Êú™‰æÜÁöÑÈÄ≤Â±ïÊèê‰æõ‰∫ÜÊòéÁ¢∫ÁöÑÊñπÂêë„ÄÇÈÄèÈÅéÂΩåÂêàÊ≠§Â∑ÆË∑ùÔºåÊàëÂÄëÊó®Âú®ÈáãÊîæ LLM ÁöÑÊΩõÂäõÔºå‰∏¶‰øÉÈÄ≤ÂÖ∂Âú®ÁúºÁßëÁöÑÁôºÂ±ï„ÄÇ

##### **MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**
2502.01158v1 by Alejandro Guerra-Manzanares, Farah E. Shamout

Multimodal fusion leverages information across modalities to learn better
feature representations with the goal of improving performance in fusion-based
tasks. However, multimodal datasets, especially in medical settings, are
typically smaller than their unimodal counterparts, which can impede the
performance of multimodal models. Additionally, the increase in the number of
modalities is often associated with an overall increase in the size of the
multimodal network, which may be undesirable in medical use cases. Utilizing
smaller unimodal encoders may lead to sub-optimal performance, particularly
when dealing with high-dimensional clinical data. In this paper, we propose the
Modality-INformed knowledge Distillation (MIND) framework, a multimodal model
compression approach based on knowledge distillation that transfers knowledge
from ensembles of pre-trained deep neural networks of varying sizes into a
smaller multimodal student. The teacher models consist of unimodal networks,
allowing the student to learn from diverse representations. MIND employs
multi-head joint fusion models, as opposed to single-head models, enabling the
use of unimodal encoders in the case of unimodal samples without requiring
imputation or masking of absent modalities. As a result, MIND generates an
optimized multimodal model, enhancing both multimodal and unimodal
representations. It can also be leveraged to balance multimodal learning during
training. We evaluate MIND on binary and multilabel clinical prediction tasks
using time series data and chest X-ray images. Additionally, we assess the
generalizability of the MIND framework on three non-medical multimodal
multiclass datasets. Experimental results demonstrate that MIND enhances the
performance of the smaller multimodal network across all five tasks, as well as
various fusion methods and multimodal architectures, compared to
state-of-the-art baselines.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅËûçÂêàÂà©Áî®Ë∑®Ê®°ÊÄÅÁöÑ‰ø°ÊÅØÊù•Â≠¶‰π†Êõ¥Â•ΩÁöÑÁâπÂæÅË°®Á§∫ÔºåÁõÆÊ†áÊòØÊèêÂçáÂü∫‰∫éËûçÂêàÁöÑ‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂåªÁñóÁéØÂ¢É‰∏≠ÔºåÈÄöÂ∏∏ÊØîÂÆÉ‰ª¨ÁöÑÂçïÊ®°ÊÄÅÂØπÂ∫îÊï∞ÊçÆÈõÜÂ∞èÔºåËøô‰ºöÈòªÁ¢çÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊ®°ÊÄÅÊï∞ÈáèÁöÑÂ¢ûÂä†ÈÄöÂ∏∏‰∏éÂ§öÊ®°ÊÄÅÁΩëÁªúÂ∞∫ÂØ∏ÁöÑÊï¥‰ΩìÂ¢ûÂä†Áõ∏ÂÖ≥ÔºåËøôÂú®ÂåªÁñóÁî®‰æã‰∏≠ÂèØËÉΩÊòØ‰∏çÂèØÂèñÁöÑ„ÄÇÂà©Áî®ËæÉÂ∞èÁöÑÂçïÊ®°ÊÄÅÁºñÁ†ÅÂô®ÂèØËÉΩ‰ºöÂØºËá¥Ê¨°‰ºòÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÈ´òÁª¥‰∏¥Â∫äÊï∞ÊçÆÊó∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊ®°ÊÄÅ‰ø°ÊÅØÁü•ËØÜËí∏È¶è (MIND) Ê°ÜÊû∂ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÁü•ËØÜËí∏È¶èÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂéãÁº©ÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÊù•Ëá™‰∏çÂêåÂ§ßÂ∞èÁöÑÈ¢ÑËÆ≠ÁªÉÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÁöÑÈõÜÂêà‰∏≠ÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞‰∏Ä‰∏™ËæÉÂ∞èÁöÑÂ§öÊ®°ÊÄÅÂ≠¶Áîü‰∏≠„ÄÇÊïôÂ∏àÊ®°ÂûãÁî±ÂçïÊ®°ÊÄÅÁΩëÁªúÁªÑÊàêÔºåÂÖÅËÆ∏Â≠¶Áîü‰ªé‰∏çÂêåÁöÑË°®Á§∫‰∏≠Â≠¶‰π†„ÄÇMIND ÈááÁî®Â§öÂ§¥ËÅîÂêàËûçÂêàÊ®°ÂûãÔºåËÄå‰∏çÊòØÂçïÂ§¥Ê®°ÂûãÔºå‰ªéËÄåËÉΩÂ§üÂú®ÂçïÊ®°ÊÄÅÊ†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®ÂçïÊ®°ÊÄÅÁºñÁ†ÅÂô®ÔºåËÄå‰∏çÈúÄË¶ÅÁº∫Â§±Ê®°ÊÄÅÁöÑÊèíË°•ÊàñÊé©ËîΩ„ÄÇÂõ†Ê≠§ÔºåMIND ÁîüÊàê‰∫Ü‰∏Ä‰∏™ÁªèËøá‰ºòÂåñÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÂ¢ûÂº∫‰∫ÜÂ§öÊ®°ÊÄÅÂíåÂçïÊ®°ÊÄÅË°®Á§∫„ÄÇÂÆÉËøòÂèØ‰ª•Áî®Êù•Âú®ËÆ≠ÁªÉÊúüÈó¥Âπ≥Ë°°Â§öÊ®°ÊÄÅÂ≠¶‰π†„ÄÇÊàë‰ª¨‰ΩøÁî®Êó∂Èó¥Â∫èÂàóÊï∞ÊçÆÂíåËÉ∏ÈÉ® X Â∞ÑÁ∫øÂõæÂÉèÂØπ‰∫åÂÖÉÂíåÂ§öÊ†áÁ≠æ‰∏¥Â∫äÈ¢ÑÊµã‰ªªÂä°ËØÑ‰º∞‰∫Ü MIND„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËØÑ‰º∞‰∫Ü MIND Ê°ÜÊû∂Âú®‰∏â‰∏™ÈùûÂåªÁñóÂ§öÊ®°ÊÄÅÂ§öÂàÜÁ±ªÊï∞ÊçÆÈõÜ‰∏äÁöÑÊ≥õÂåñÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåMIND Â¢ûÂº∫‰∫ÜËæÉÂ∞èÁöÑÂ§öÊ®°ÊÄÅÁΩëÁªúÂú®ÊâÄÊúâ‰∫î‰∏™‰ªªÂä°‰ª•ÂèäÂêÑÁßçËûçÂêàÊñπÊ≥ïÂíåÂ§öÊ®°ÊÄÅÊû∂ÊûÑ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

##### **Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**
2502.01141v1 by Qian Chen, Stefanie Rinderle-Ma, Lijie Wen

Most existing process compliance monitoring approaches detect compliance
violations in an ex post manner. Only predicate prediction focuses on
predicting them. However, predicate prediction provides a binary yes/no notion
of compliance, lacking the ability to measure to which extent an ongoing
process instance deviates from the desired state as specified in constraints.
Here, being able to quantify the magnitude of violation would provide
organizations with deeper insights into their operational performance, enabling
informed decision making to reduce or mitigate the risk of non-compliance.
Thus, we propose two predictive compliance monitoring approaches to close this
research gap. The first approach reformulates the binary classification problem
as a hybrid task that considers both classification and regression, while the
second employs a multi-task learning method to explicitly predict the
compliance status and the magnitude of violation for deviant cases
simultaneously. In this work, we focus on temporal constraints as they are
significant in almost any application domain, e.g., health care. The evaluation
on synthetic and real-world event logs demonstrates that our approaches are
capable of quantifying the magnitude of violations while maintaining comparable
performance for compliance predictions achieved by state-of-the-art approaches.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÊµÅÁ®ãÂêàË¶èÁõ£ÊéßÊñπÊ≥ïÂ§ßÂ§öÊúÉÂú®‰∫ãÂæåÂÅµÊ∏¨Âà∞ÂêàË¶èÈÅïË¶è„ÄÇÂè™ÊúâË¨ÇË©ûÈ†êÊ∏¨Â∞àÊ≥®ÊñºÈ†êÊ∏¨ÈÄô‰∫õÈÅïË¶è„ÄÇÁÑ∂ËÄåÔºåË¨ÇË©ûÈ†êÊ∏¨Êèê‰æõÁöÑÊòØÂêàË¶èËàáÂê¶ÁöÑ‰∫åÂÖÉÊ¶ÇÂøµÔºåÁÑ°Ê≥ïË°°ÈáèÊ≠£Âú®ÈÄ≤Ë°åÁöÑÊµÅÁ®ãÂØ¶‰æãÂÅèÈõ¢Á¥ÑÊùü‰∏≠ÊâÄÊåáÂÆö‰πãÁêÜÊÉ≥ÁãÄÊÖãÁöÑÁ®ãÂ∫¶„ÄÇÂú®Ê≠§ÔºåËÉΩÂ§†ÈáèÂåñÈÅïË¶èÁöÑÂö¥ÈáçÁ®ãÂ∫¶ÔºåÂ∞áËÉΩËÆìÁµÑÁπîÊ∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÁáüÈÅãÁ∏æÊïàÔºå‰∏¶ËÉΩÊìöÊ≠§ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºå‰ª•Èôç‰ΩéÊàñÊ∏õËºï‰∏çÂêàË¶èÁöÑÈ¢®Èö™„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ÂÖ©Á®ÆÈ†êÊ∏¨ÂêàË¶èÁõ£ÊéßÊñπÊ≥ï‰æÜÂ°´Ë£úÊ≠§Á†îÁ©∂Á©∫ÁôΩ„ÄÇÁ¨¨‰∏ÄÁ®ÆÊñπÊ≥ïÂ∞á‰∫åÂÖÉÂàÜÈ°ûÂïèÈ°åÈáçÊñ∞Ë°®Ëø∞ÁÇ∫ÂêåÊôÇËÄÉÈáèÂàÜÈ°ûÂíåÂõûÊ≠∏ÁöÑÊ∑∑Âêà‰ªªÂãôÔºåËÄåÁ¨¨‰∫åÁ®ÆÊñπÊ≥ïÂâáÊé°Áî®Â§ö‰ªªÂãôÂ≠∏ÁøíÊñπÊ≥ïÔºåÂêåÊôÇÊòéÁ¢∫È†êÊ∏¨ÂêàË¶èÁãÄÊÖãÂíåÂÅèÂ∑ÆÊ°à‰æãÁöÑÈÅïË¶èÂö¥ÈáçÁ®ãÂ∫¶„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÊôÇÈñìÁ¥ÑÊùüÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂπæ‰πéÂú®‰ªª‰ΩïÊáâÁî®È†òÂüüÔºà‰æãÂ¶ÇÈÜ´ÁôÇ‰øùÂÅ•Ôºâ‰∏≠ÈÉΩÂæàÈáçË¶Å„ÄÇÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïå‰∫ã‰ª∂Ë®òÈåÑ‰∏äÁöÑË©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÂ§†ÈáèÂåñÈÅïË¶èÁöÑÂö¥ÈáçÁ®ãÂ∫¶ÔºåÂêåÊôÇÁ∂≠ÊåÅËàáÁèæÊúâÊñπÊ≥ïÊâÄÈÅîÊàêÁöÑÂêàË¶èÈ†êÊ∏¨Áõ∏Áï∂ÁöÑÁ∏æÊïà„ÄÇ

##### **Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**
2502.01108v1 by Mithun Saha, Maxwell A. Xu, Wanting Mao, Sameer Neupane, James M. Rehg, Santosh Kumar

Photoplethysmography (PPG)-based foundation models are gaining traction due
to the widespread use of PPG in biosignal monitoring and their potential to
generalize across diverse health applications. In this paper, we introduce
Pulse-PPG, the first open-source PPG foundation model trained exclusively on
raw PPG data collected over a 100-day field study with 120 participants.
Existing PPG foundation models are either open-source but trained on clinical
data or closed-source, limiting their applicability in real-world settings. We
evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its
performance against a state-of-the-art foundation model trained on clinical
data. Our results demonstrate that Pulse-PPG, trained on uncurated field data,
exhibits superior generalization across clinical and mobile health applications
in both lab and field settings. This suggests that exposure to real-world
variability enables the model to learn fine-grained representations, making it
more adaptable across tasks. Furthermore, pre-training on field data
surprisingly outperforms its pre-training on clinical data in many tasks,
reinforcing the importance of training on real-world, diverse datasets. To
encourage further advancements in robust foundation models leveraging field
data, we plan to release Pulse-PPG, providing researchers with a powerful
resource for developing more generalizable PPG-based models.

ÊëòË¶ÅÔºöÂü∫ÊñºÂÖâÈõªÂÆπÁ©çÊèèË®òË°ì (PPG) ÁöÑÂü∫Á§éÊ®°ÂûãÁî±Êñº PPG Âú®ÁîüÁâ©Ë®äËôüÁõ£Êéß‰∏≠ÁöÑÂª£Ê≥õ‰ΩøÁî®ÂèäÂÖ∂Âú®ÂêÑÁ®ÆÂÅ•Â∫∑ÊáâÁî®‰∏≠Êé®Âª£ÁöÑÊΩõÂäõËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π Pulse-PPGÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢º PPG Âü∫Á§éÊ®°ÂûãÔºåÂ∞àÈñÄÈáùÂ∞çÂú®ÁÇ∫Êúü 100 Â§©ÁöÑÁèæÂ†¥Á†îÁ©∂‰∏≠Êî∂ÈõÜÁöÑ 120 ‰ΩçÂèÉËàáËÄÖÁöÑÂéüÂßã PPG Ë≥áÊñôÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÁèæÊúâÁöÑ PPG Âü∫Á§éÊ®°ÂûãË¶Å‰∏çÊòØÈñãÊîæÂéüÂßãÁ¢ºÔºå‰ΩÜË®ìÁ∑¥ÊñºËá®Â∫äË≥áÊñôÔºå‰∏çÁÑ∂Â∞±ÊòØÈñâÊ∫êÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÁúüÂØ¶‰∏ñÁïå‰∏≠ÁöÑÊáâÁî®ÊÄß„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü Pulse-PPG Âú®Â§öÂÄãË≥áÊñôÈõÜÂíå‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÁöÑË°®ÁèæÔºå‰∏¶Â∞áÂÖ∂ÊïàËÉΩËàáË®ìÁ∑¥ÊñºËá®Â∫äË≥áÊñôÁöÑÊúÄÊñ∞Âü∫Á§éÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåË®ìÁ∑¥ÊñºÊú™Êï¥ÁêÜÁèæÂ†¥Ë≥áÊñôÁöÑ Pulse-PPG Âú®ÂØ¶È©óÂÆ§ÂíåÁèæÂ†¥Áí∞Â¢É‰∏≠ÔºåÂú®Ëá®Â∫äÂíåË°åÂãïÂÅ•Â∫∑ÊáâÁî®‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄôË°®ÊòéÊé•Ëß∏ÁúüÂØ¶‰∏ñÁïåÁöÑËÆäÁï∞ÊÄß‰ΩøÊ®°ÂûãËÉΩÂ§†Â≠∏ÁøíÁ¥∞Á≤íÂ∫¶ÁöÑË°®Á§∫Ôºå‰ΩøÂÖ∂Êõ¥ËÉΩÈÅ©ÊáâÂêÑÁ®Æ‰ªªÂãô„ÄÇÊ≠§Â§ñÔºå‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÁèæÂ†¥Ë≥áÊñôÁöÑÈ†êË®ìÁ∑¥Âú®Ë®±Â§ö‰ªªÂãô‰∏≠ÂÑ™ÊñºËá®Â∫äË≥áÊñôÁöÑÈ†êË®ìÁ∑¥ÔºåÈÄôÂº∑Âåñ‰∫ÜÂú®ÁúüÂØ¶‰∏ñÁïå„ÄÅÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÈáçË¶ÅÊÄß„ÄÇÁÇ∫‰∫ÜÈºìÂãµÂú®Âà©Áî®ÁèæÂ†¥Ë≥áÊñôÁöÑÂº∑ÂÅ•Âü∫Á§éÊ®°ÂûãÊñπÈù¢ÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ïÔºåÊàëÂÄëË®àÁï´ÁôºÂ∏É Pulse-PPGÔºåÁÇ∫Á†îÁ©∂‰∫∫Âì°Êèê‰æõ‰∏ÄÂÄãÂº∑Â§ßÁöÑË≥áÊ∫êÔºåÁî®ÊñºÈñãÁôºÊõ¥ÂÖ∑Ê≥õÂåñÊÄßÁöÑÂü∫Êñº PPG ÁöÑÊ®°Âûã„ÄÇ

##### **Tutorial on Using Machine Learning and Deep Learning Models for Mental Illness Detection**
2502.04342v1 by Yeyubei Zhang, Zhongyan Wang, Zhanyi Ding, Yexin Tian, Jianglai Dai, Xiaorui Shen, Yunchong Liu, Yuchen Cao

Social media has become an important source for understanding mental health,
providing researchers with a way to detect conditions like depression from
user-generated posts. This tutorial provides practical guidance to address
common challenges in applying machine learning and deep learning methods for
mental health detection on these platforms. It focuses on strategies for
working with diverse datasets, improving text preprocessing, and addressing
issues such as imbalanced data and model evaluation. Real-world examples and
step-by-step instructions demonstrate how to apply these techniques
effectively, with an emphasis on transparency, reproducibility, and ethical
considerations. By sharing these approaches, this tutorial aims to help
researchers build more reliable and widely applicable models for mental health
research, contributing to better tools for early detection and intervention.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™íÈ´îÂ∑≤ÊàêÁÇ∫‰∫ÜËß£ÂøÉÁêÜÂÅ•Â∫∑ÁöÑÈáçË¶Å‰æÜÊ∫êÔºå
ÁÇ∫Á†îÁ©∂‰∫∫Âì°Êèê‰æõ‰∏ÄÁ®ÆÊñπÂºèÔºåÂæû‰ΩøÁî®ËÄÖÁôºÂ∏ÉÁöÑË≤ºÊñá‰∏≠ÂÅµÊ∏¨ÊÜÇÈ¨±ÁóáÁ≠âÁãÄÊ≥Å„ÄÇ
Êú¨ÊïôÂ≠∏Êèê‰æõÂØ¶ÂãôÊåáÂçóÔºåË™™ÊòéÂ¶Ç‰ΩïËôïÁêÜÂú®ÈÄô‰∫õÂπ≥Âè∞‰∏ä‰ΩøÁî®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÈÄ≤Ë°åÂøÉÁêÜÂÅ•Â∫∑ÂÅµÊ∏¨ÊôÇÂ∏∏Ë¶ãÁöÑÊåëÊà∞„ÄÇ
ÂÆÉÂ∞àÊ≥®ÊñºËôïÁêÜ‰∏çÂêåË≥áÊñôÈõÜ„ÄÅÊîπÂñÑÊñáÂ≠óÂâçËôïÁêÜÔºå‰ª•ÂèäËôïÁêÜ‰∏çÂπ≥Ë°°Ë≥áÊñôÂíåÊ®°ÂûãË©ï‰º∞Á≠âÂïèÈ°åÁöÑÁ≠ñÁï•„ÄÇ
ÂØ¶ÈöõÁØÑ‰æãÂíåÈÄêÊ≠•Ë™™ÊòéÁ§∫ÁØÑÂ¶Ç‰ΩïÊúâÊïàÊáâÁî®ÈÄô‰∫õÊäÄË°ìÔºå‰∏¶Âº∑Ë™øÈÄèÊòéÂ∫¶„ÄÅÂèØË§áË£ΩÊÄßÔºå‰ª•ÂèäÂÄ´ÁêÜËÄÉÈáè„ÄÇ
ÈÄèÈÅéÂàÜ‰∫´ÈÄô‰∫õÊñπÊ≥ïÔºåÊú¨ÊïôÂ≠∏ÊåáÂçóÊó®Âú®ÂçîÂä©Á†îÁ©∂‰∫∫Âì°Âª∫ÊßãÊõ¥ÂèØÈù†‰∏îÂª£Ê≥õÈÅ©Áî®ÁöÑÂøÉÁêÜÂÅ•Â∫∑Á†îÁ©∂Ê®°ÂûãÔºå
ÈÄ≤ËÄåÊúâÂä©ÊñºÊó©ÊúüÂÅµÊ∏¨Âíå‰ªãÂÖ•ÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**
2502.01691v1 by Hadas Ben-Atya, Naama Gavrielov, Zvi Badash, Gili Focht, Ruth Cytter-Kuint, Talar Hagopian, Dan Turner, Moti Freiman

Reliable extraction of structured data from radiology reports using Large
Language Models (LLMs) remains challenging, especially for complex, non-English
texts like Hebrew. This study introduces an agent-based uncertainty-aware
approach to improve the trustworthiness of LLM predictions in medical
applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease
patients (from 2010 to 2023) across three medical centers. A subset of 512
reports was manually annotated for six gastrointestinal organs and 15
pathological findings, while the remaining reports were automatically annotated
using HSMP-BERT. Structured data extraction was performed using Llama 3.1
(Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed
six semantically equivalent prompts to estimate uncertainty. An Agent-Based
Decision Model integrated multiple prompt outputs into five confidence levels
for calibrated uncertainty and was compared against three entropy-based models.
Performance was evaluated using accuracy, F1 score, precision, recall, and
Cohen's Kappa before and after filtering high-uncertainty cases. The
agent-based model outperformed the baseline across all metrics, achieving an F1
score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering
high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to
0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated
clear separation between correct and incorrect predictions, with the
agent-based model providing the most well-calibrated uncertainty estimates. By
incorporating uncertainty-aware prompt ensembles and an agent-based decision
model, this approach enhances the performance and reliability of LLMs in
structured data extraction from radiology reports, offering a more
interpretable and trustworthy solution for high-stakes medical applications.

ÊëòË¶ÅÔºö<paragraph>‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂæûÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠ÂèØÈù†Âú∞ÊèêÂèñÁµêÊßãÂåñÊï∏Êìö‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂ∞çÊñºÂ∏å‰ºØ‰æÜË™ûÁ≠âË§áÈõúÁöÑÈùûËã±Ë™ûÊñáÊú¨„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº‰ª£ÁêÜÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÊÑüÁü•ÊñπÊ≥ïÔºå‰ª•ÊèêÈ´ò LLM È†êÊ∏¨Âú®ÈÜ´ÁôÇÊáâÁî®‰∏≠ÁöÑÂèØ‰ø°Â∫¶„ÄÇÊàëÂÄëÂàÜÊûê‰∫Ü‰æÜËá™‰∏âÂÄãÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑ 9,683 ‰ªΩÂÖãÈöÜÊ∞èÁóáÊÇ£ËÄÖÁöÑÂ∏å‰ºØ‰æÜË™ûÊîæÂ∞ÑÁßëÂ†±ÂëäÔºàÂæû 2010 Âπ¥Âà∞ 2023 Âπ¥Ôºâ„ÄÇÂÖ∂‰∏≠ 512 ‰ªΩÂ†±ÂëäÁöÑÊâãÂãïË®ªÈáãÂåÖÊã¨ÂÖ≠ÂÄãËÉÉËÖ∏Âô®ÂÆòÂíå 15 ÂÄãÁóÖÁêÜÁôºÁèæÔºåËÄåÂÖ∂È§òÂ†±ÂëäÂâá‰ΩøÁî® HSMP-BERT Ëá™ÂãïË®ªÈáã„ÄÇÁµêÊßãÂåñÊï∏ÊìöÊèêÂèñ‰ΩøÁî® Llama 3.1ÔºàLlama 3-8b-instructÔºâËàáË≤ùËëâÊñØÊèêÁ§∫ÈõÜÂêàÔºàBayesPEÔºâÈÄ≤Ë°åÔºåÂÆÉÊé°Áî®ÂÖ≠ÂÄãË™ûÁæ©Á≠âÊïàÊèêÁ§∫‰æÜ‰º∞Ë®à‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÂü∫Êñº‰ª£ÁêÜÁöÑÊ±∫Á≠ñÊ®°ÂûãÂ∞áÂ§öÂÄãÊèêÁ§∫Ëº∏Âá∫Êï¥ÂêàÂà∞‰∫îÂÄãÁΩÆ‰ø°Â∫¶Á¥öÂà•‰∏≠‰ª•Ê†°Ê∫ñ‰∏çÁ¢∫ÂÆöÊÄßÔºå‰∏¶Ëàá‰∏âÂÄãÂü∫ÊñºÁÜµÁöÑÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇÂú®ÈÅéÊøæÊéâÈ´òÂ∫¶‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊÉÖÊ≥Å‰πãÂâçÂíå‰πãÂæåÔºå‰ΩøÁî®Ê∫ñÁ¢∫Â∫¶„ÄÅF1 ÂàÜÊï∏„ÄÅÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå Cohen's Kappa Ë©ï‰º∞ÊÄßËÉΩ„ÄÇÂü∫Êñº‰ª£ÁêÜÁöÑÊ®°ÂûãÂú®ÊâÄÊúâÊåáÊ®ô‰∏äÈÉΩÂÑ™ÊñºÂü∫Á∑öÔºåF1 ÂàÜÊï∏ÈÅîÂà∞ 0.3967ÔºåÂè¨ÂõûÁéáÈÅîÂà∞ 0.6437ÔºåCohen's Kappa ÈÅîÂà∞ 0.3006„ÄÇÂú®ÈÅéÊøæÊéâÈ´òÂ∫¶‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊÉÖÊ≥ÅÔºàÂ§ßÊñºÊàñÁ≠âÊñº 0.5ÔºâÂæåÔºåF1 ÂàÜÊï∏ÊèêÈ´òÂà∞ 0.4787ÔºåKappa ÊèêÈ´òÂà∞ 0.4258„ÄÇ‰∏çÁ¢∫ÂÆöÊÄßÁõ¥ÊñπÂúñÈ°ØÁ§∫‰∫ÜÊ≠£Á¢∫È†êÊ∏¨Âíå‰∏çÊ≠£Á¢∫È†êÊ∏¨‰πãÈñìÁöÑÊòéÈ°ØÂçÄÂà•ÔºåÂü∫Êñº‰ª£ÁêÜÁöÑÊ®°ÂûãÊèê‰æõ‰∫ÜÊ†°Ê∫ñÊúÄÂ•ΩÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à„ÄÇÈÄöÈÅéÁµêÂêà‰∏çÁ¢∫ÂÆöÊÄßÊÑüÁü•ÊèêÁ§∫ÈõÜÂêàÂíåÂü∫Êñº‰ª£ÁêÜÁöÑÊ±∫Á≠ñÊ®°ÂûãÔºåÈÄôÁ®ÆÊñπÊ≥ïÂ¢ûÂº∑‰∫Ü LLM Âú®ÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠ÁµêÊßãÂåñÊï∏ÊìöÊèêÂèñ‰∏≠ÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄßÔºåÁÇ∫È´òÈ¢®Èö™ÈÜ´ÁôÇÊáâÁî®Êèê‰æõ‰∫ÜÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ</paragraph>

##### **Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**
2502.01685v1 by Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha

Existing methods for analyzing linguistic content from picture descriptions
for assessment of cognitive-linguistic impairment often overlook the
participant's visual narrative path, which typically requires eye tracking to
assess. Spatio-semantic graphs are a useful tool for analyzing this narrative
path from transcripts alone, however they are limited by the need for manual
tagging of content information units (CIUs). In this paper, we propose an
automated approach for estimation of spatio-semantic graphs (via automated
extraction of CIUs) from the Cookie Theft picture commonly used in
cognitive-linguistic analyses. The method enables the automatic
characterization of the visual semantic path during picture description.
Experiments demonstrate that the automatic spatio-semantic graphs effectively
differentiate between cognitively impaired and unimpaired speakers. Statistical
analyses reveal that the features derived by the automated method produce
comparable results to the manual method, with even greater group differences
between clinical groups of interest. These results highlight the potential of
the automated approach for extracting spatio-semantic features in developing
clinical speech models for cognitive impairment assessment.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÁî®ÊñºÂàÜÊûêÂúñÂÉèÊèèËø∞‰∏≠ÁöÑË™ûË®ÄÂÖßÂÆπÁöÑÊñπÊ≥ïÔºåÁî®ÊñºË©ï‰º∞Ë™çÁü•Ë™ûË®ÄÈöúÁ§ôÔºåÈÄöÂ∏∏ÊúÉÂøΩÁï•ÂèÉËàáËÄÖÁöÑË¶ñË¶∫Êïò‰∫ãË∑ØÂæëÔºåÈÄôÈÄöÂ∏∏ÈúÄË¶ÅÁúºÁêÉËøΩËπ§‰æÜË©ï‰º∞„ÄÇÊôÇÁ©∫Ë™ûÁæ©ÂúñÊòØ‰∏ÄÁ®ÆÊúâÁî®ÁöÑÂ∑•ÂÖ∑ÔºåÂèØ‰ª•ÂÉÖÂæûËΩâÈåÑÊú¨‰∏≠ÂàÜÊûêÊ≠§Êïò‰∫ãË∑ØÂæëÔºå‰ΩÜÊòØÂÆÉÂÄëÂèóÂà∞ÊâãÂãïÊ®ôË®òÂÖßÂÆπË≥áË®äÂñÆÂÖÉ (CIU) ÁöÑÈúÄÊ±ÇÊâÄÈôêÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËá™ÂãïÂåñÊñπÊ≥ïÔºåÁî®ÊñºÂæûË™çÁü•Ë™ûË®ÄÂàÜÊûê‰∏≠Â∏∏Áî®ÁöÑ Cookie Theft ÂúñÂÉè‰º∞Ë®àÊôÇÁ©∫Ë™ûÁæ©ÂúñÔºàÈÄöÈÅéËá™ÂãïÊèêÂèñ CIUÔºâ„ÄÇË©≤ÊñπÊ≥ïËÉΩÂ§†Ëá™ÂãïË°®ÂæµÂúñÁâáÊèèËø∞ÊúüÈñìÁöÑË¶ñË¶∫Ë™ûÁæ©Ë∑ØÂæë„ÄÇÂØ¶È©óË°®ÊòéÔºåËá™ÂãïÊôÇÁ©∫Ë™ûÁæ©ÂúñÊúâÊïàÂú∞ÂçÄÂàÜ‰∫ÜË™çÁü•ÂèóÊêçÂíåÊú™ÂèóÊêçÁöÑË™™Ë©±ËÄÖ„ÄÇÁµ±Ë®àÂàÜÊûêË°®ÊòéÔºåËá™ÂãïÂåñÊñπÊ≥ïË°çÁîüÁöÑÁâπÂæµÁî¢Áîü‰∫ÜËàáÊâãÂãïÊñπÊ≥ïÁõ∏Áï∂ÁöÑÁµêÊûúÔºåÁîöËá≥Âú®ÊÑüËààË∂£ÁöÑËá®Â∫äÁµÑ‰πãÈñìÁî¢Áîü‰∫ÜÊõ¥Â§ßÁöÑÁµÑÂ∑ÆÁï∞„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÂá∫‰∫ÜËá™ÂãïÂåñÊñπÊ≥ïÂú®ÊèêÂèñÊôÇÁ©∫Ë™ûÁæ©ÁâπÂæµ‰ª•ÈñãÁôºÁî®ÊñºË™çÁü•ÈöúÁ§ôË©ï‰º∞ÁöÑËá®Â∫äË™ûÈü≥Ê®°ÂûãÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**
2502.00712v1 by Shengtian Sang, Hassan Jahanandish, Cynthia Xinran Li, Indrani Bhattachary, Jeong Hoon Lee, Lichun Zhang, Sulaiman Vesal, Pejman Ghanouni, Richard Fan, Geoffrey A. Sonn, Mirabela Rusu

Prostate cancer is a major cause of cancer-related deaths in men, where early
detection greatly improves survival rates. Although MRI-TRUS fusion biopsy
offers superior accuracy by combining MRI's detailed visualization with TRUS's
real-time guidance, it is a complex and time-intensive procedure that relies
heavily on manual annotations, leading to potential errors. To address these
challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation
method that identifies prostate tumors directly in TRUS images without
requiring manual annotations. Unlike traditional multimodal fusion approaches
that rely on naive data concatenation, our method integrates a
registration-segmentation framework to align and leverage spatial information
between MRI and TRUS modalities. This alignment enhances segmentation accuracy
and reduces reliance on manual effort. Our approach was validated on a dataset
of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient
of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132)
methods, with significant improvements (p $<$ 0.01). This framework
demonstrates the potential for reducing the complexity of prostate cancer
diagnosis and provides a flexible architecture applicable to other multimodal
medical imaging tasks.

ÊëòË¶ÅÔºöÂâçÂàóËÖ∫ÁôåÊòØÁî∑ÊÄßÁôåÁóáÁõ∏ÈóúÊ≠ª‰∫°ÁöÑ‰∏ªË¶ÅÂéüÂõ†ÔºåÊó©ÊúüÁôºÁèæÂèØÂ§ßÂπÖÊèêÂçáÂ≠òÊ¥ªÁéá„ÄÇÂÑòÁÆ° MRI-TRUS ËûçÂêàÂàáÁâáÊ™¢Êü•ÁµêÂêà‰∫Ü MRI ÁöÑË©≥Á¥∞Ë¶ñË¶∫ÂåñËàá TRUS ÁöÑÂç≥ÊôÇÂ∞éÂºïÔºåÂèØÊèê‰æõÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂÆÉÊòØ‰∏ÄÁ®Æ‰ª∞Ë≥¥Â§ßÈáèÊâãÂãïË®ªËß£ÁöÑË§áÈõú‰∏îËÄóÊôÇÁöÑÁ®ãÂ∫èÔºåÂÆπÊòìÂ∞éËá¥ÈåØË™§„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÖ®Ëá™ÂãïÁöÑ MRI-TRUS ËûçÂêàÂºèÂàÜÂâ≤ÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Âú® TRUS ÂΩ±ÂÉè‰∏≠Áõ¥Êé•Ëæ®Ë≠òÂá∫ÂâçÂàóËÖ∫ËÖ´Áò§ÔºåËÄå‰∏çÈúÄË¶ÅÊâãÂãïË®ªËß£„ÄÇËàá‰æùË≥¥ÊñºÂ§©ÁúüË≥áÊñô‰∏≤Êé•ÁöÑÂÇ≥Áµ±Â§öÊ®°ÊÖãËûçÂêàÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÊï¥Âêà‰∫Ü‰∏ÄÂÄãÈÖçÊ∫ñÂàÜÂâ≤Êû∂ÊßãÔºå‰ª•Â∞çÈΩä‰∏¶Âà©Áî® MRI Ëàá TRUS Ê®°ÊÖã‰πãÈñìÁöÑÁ©∫ÈñìË≥áË®ä„ÄÇÈÄôÁ®ÆÂ∞çÈΩäÊèêÂçá‰∫ÜÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶Ôºå‰∏¶Ê∏õÂ∞ë‰∫ÜÂ∞çÊâãÂãï‰ΩúÊ•≠ÁöÑ‰æùË≥¥„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂ∑≤ÈÄöÈÅé‰æÜËá™ Stanford ÈÜ´Èô¢ÁöÑ 1,747 ‰ΩçÊÇ£ËÄÖÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÈ©óË≠âÔºåÈÅîÂà∞‰∫Ü 0.212 ÁöÑÂπ≥Âùá Dice ‰øÇÊï∏ÔºåÂÑ™ÊñºÂÉÖ‰ΩøÁî® TRUS (0.117) ÂíåÂ§©ÁúüÁöÑ MRI-TRUS ËûçÂêà (0.132) ÊñπÊ≥ïÔºå‰∏¶ÊúâÈ°ØËëóÁöÑÊîπÂñÑÔºàp < 0.01Ôºâ„ÄÇÈÄôÂÄãÊû∂ÊßãË≠âÊòé‰∫ÜÈôç‰ΩéÂâçÂàóËÖ∫ÁôåË®∫Êñ∑Ë§áÈõúÊÄßÁöÑÊΩõÂäõÔºå‰∏¶Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈÅ©Áî®ÊñºÂÖ∂‰ªñÂ§öÊ®°ÊÖãÈÜ´Â≠∏ÂΩ±ÂÉè‰ªªÂãôÁöÑÂΩàÊÄßÊû∂Êßã„ÄÇ

##### **TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**
2502.00695v1 by Linglong Wu, Xuhao Shan, Ruiquan Ge, Ruoyu Liang, Chi Zhang, Yonghong Li, Ahmed Elazab, Huoling Luo, Yunbi Liu, Changmiao Wang

Chronic liver disease represents a significant health challenge worldwide and
accurate prognostic evaluations are essential for personalized treatment plans.
Recent evidence suggests that integrating multimodal data, such as computed
tomography imaging, radiomic features, and clinical information, can provide
more comprehensive prognostic information. However, modalities have an inherent
heterogeneity, and incorporating additional modalities may exacerbate the
challenges of heterogeneous data fusion. Moreover, existing multimodal fusion
methods often struggle to adapt to richer medical modalities, making it
difficult to capture inter-modal relationships. To overcome these limitations,
We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).
Specifically, we develop an Intra-Modality Aggregation module and a
Triple-Modal Cross-Attention Fusion module, which are designed to eliminate
intra-modality redundancy and extract cross-modal information, respectively.
Furthermore, we design a Triple-Modal Feature Fusion loss function to align
feature representations across modalities. Extensive experiments on the liver
prognosis dataset demonstrate that our approach significantly outperforms
existing state-of-the-art unimodal models and other multi-modal techniques. Our
code is available at https://github.com/Mysterwll/liver.git.

ÊëòË¶ÅÔºöÊÖ¢ÊÄßËÇùÁóÖÂú®ÂÖ®ÁêÉËåÉÂõ¥ÂÜÖ‰ª£Ë°®ËëóÈáçÂ§ßÁöÑÂÅ•Â∫∑ÊåëÊà∞ÔºåËÄåÊ∫ñÁ¢∫ÁöÑÈ†êÂæåË©ï‰º∞Â∞çÊñºÂÄã‰∫∫ÂåñÊ≤ªÁôÇË®àÁï´Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÁöÑË≠âÊìöË°®ÊòéÔºåÊï¥ÂêàÂ§öÊ®°ÊÖãË≥áÊñôÔºà‰æãÂ¶ÇÈõªËÖ¶Êñ∑Â±§ÂΩ±ÂÉè„ÄÅÊîæÂ∞ÑÁâπÂæµÂíåËá®Â∫äË≥áË®äÔºâÂèØ‰ª•Êèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑÈ†êÂæåË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÊ®°ÊÖãÂÖ∑ÊúâÂÖßÂú®Áï∞Ë≥™ÊÄßÔºåËÄåÁ¥çÂÖ•È°çÂ§ñÁöÑÊ®°ÊÖãÂèØËÉΩÊúÉÂä†ÂäáÁï∞Ë≥™ÂåñË≥áÊñôËûçÂêàÁöÑÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑÂ§öÊ®°ÊÖãËûçÂêàÊñπÊ≥ïÈÄöÂ∏∏Èõ£‰ª•ÈÅ©ÊáâÊõ¥Ë±êÂØåÁöÑÈÜ´ÁôÇÊ®°ÊÖãÔºåÈÄô‰ΩøÂæóÈõ£‰ª•ÊçïÊçâÊ®°ÊÖãÈñìÁöÑÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏âÊ®°ÊÖã‰∫§‰∫íÊÖ¢ÊÄßËÇùËáüÁ∂≤Ë∑Ø (TMI-CLNet)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊ®°ÊÖãÂÖßËÅöÂêàÊ®°ÁµÑÂíå‰∏ÄÂÄã‰∏âÊ®°ÊÖã‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊ®°ÁµÑÔºåÂÆÉÂÄëÂàÜÂà•Êó®Âú®Ê∂àÈô§Ê®°ÊÖãÂÖßÂÜóÈ§òÂíåÊèêÂèñË∑®Ê®°ÊÖãË≥áË®ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄã‰∏âÊ®°ÊÖãÁâπÂæµËûçÂêàÊêçÂ§±ÂáΩÊï∏Ôºå‰ª•Â∞çÈΩäË∑®Ê®°ÊÖãÁöÑÁâπÂæµË°®Á§∫„ÄÇÂú®ËÇùËáüÈ†êÂæåË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÂñÆÊ®°ÊÖãÊ®°ÂûãÂíåÂÖ∂‰ªñÂ§öÊ®°ÊÖãÊäÄË°ì„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/Mysterwll/liver.git ‰∏äÂèñÂæó„ÄÇ

##### **Enhanced Convolutional Neural Networks for Improved Image Classification**
2502.00663v1 by Xiaoran Yang, Shuhan Yu, Wenxi Xu

Image classification is a fundamental task in computer vision with diverse
applications, ranging from autonomous systems to medical imaging. The CIFAR-10
dataset is a widely used benchmark to evaluate the performance of
classification models on small-scale, multi-class datasets. Convolutional
Neural Networks (CNNs) have demonstrated state-of-the-art results; however,
they often suffer from overfitting and suboptimal feature representation when
applied to challenging datasets like CIFAR-10. In this paper, we propose an
enhanced CNN architecture that integrates deeper convolutional blocks, batch
normalization, and dropout regularization to achieve superior performance. The
proposed model achieves a test accuracy of 84.95%, outperforming baseline CNN
architectures. Through detailed ablation studies, we demonstrate the
effectiveness of the enhancements and analyze the hierarchical feature
representations. This work highlights the potential of refined CNN
architectures for tackling small-scale image classification problems
effectively.

ÊëòË¶ÅÔºöÂΩ±ÂÉèÂàÜÈ°ûÊòØÈõªËÖ¶Ë¶ñË¶∫‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÊáâÁî®ÁØÑÂúçÂª£Ê≥õÔºåÂæûËá™ÂãïÁ≥ªÁµ±Âà∞ÈÜ´Â≠∏ÂΩ±ÂÉèÁöÜÊúâ„ÄÇCIFAR-10 Ë≥áÊñôÈõÜÊòØ‰∏ÄÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞ÂàÜÈ°ûÊ®°ÂûãÂú®Â∞èË¶èÊ®°„ÄÅÂ§öÈ°ûÂà•Ë≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩ„ÄÇÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Â∑≤Â±ïÁèæÂá∫ÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûúÔºõÁÑ∂ËÄåÔºåÁï∂ÊáâÁî®Êñº CIFAR-10 Á≠âÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÈõÜÊôÇÔºåÂÆÉÂÄëÂ∏∏Â∏∏ÊúÉÁôºÁîüÈÅéÂ∫¶Êì¨ÂêàÂíåÊ¨°‰Ω≥ÁâπÂæµË°®Á§∫ÁöÑÂïèÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ¢ûÂº∑ÁöÑ CNN Êû∂ÊßãÔºåÂÆÉÊï¥Âêà‰∫ÜÊõ¥Ê∑±ÁöÑÂç∑Á©çÂçÄÂ°ä„ÄÅÊâπÊ¨°Ê≠£Ë¶èÂåñÂíå‰∏≠Êñ∑Ê≠£Ë¶èÂåñÔºå‰ª•ÈÅîÊàêÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 84.95% ÁöÑÊ∏¨Ë©¶Ê∫ñÁ¢∫Â∫¶ÔºåÂÑ™ÊñºÂü∫Ê∫ñ CNN Êû∂Êßã„ÄÇÈÄèÈÅéË©≥Á¥∞ÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåÊàëÂÄëË≠âÊòé‰∫ÜÈÄô‰∫õÂ¢ûÂº∑ÂäüËÉΩÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÂàÜÊûê‰∫ÜÈöéÂ±§ÂºèÁâπÂæµË°®Á§∫„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁ™ÅÈ°Ø‰∫ÜÁ≤æÈÄ≤ÁöÑ CNN Êû∂ÊßãÂú®ÊúâÊïàËß£Ê±∫Â∞èË¶èÊ®°ÂΩ±ÂÉèÂàÜÈ°ûÂïèÈ°å‰∏äÁöÑÊΩõÂäõ„ÄÇ

##### **Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**
2502.00619v1 by Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon, Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li

Ensuring fairness in medical image segmentation is critical due to biases in
imbalanced clinical data acquisition caused by demographic attributes (e.g.,
age, sex, race) and clinical factors (e.g., disease severity). To address these
challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired
by optimal control theory. We provide a comprehensive analysis of its
underlying mechanisms and clarify dMoE's role in adapting to heterogeneous
distributions in medical image segmentation. Furthermore, we integrate dMoE
into multiple network architectures, demonstrating its broad applicability
across diverse medical image analysis tasks. By incorporating demographic and
clinical factors, dMoE achieves state-of-the-art performance on two 2D
benchmark datasets and a 3D in-house dataset. Our results highlight the
effectiveness of dMoE in mitigating biases from imbalanced distributions,
offering a promising approach to bridging control theory and medical image
segmentation within fairness learning paradigms. The source code will be made
available.

ÊëòË¶ÅÔºöÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÔºåÁî±Êñº‰∫∫Âè£Â±¨ÊÄßÔºà‰æãÂ¶ÇÂπ¥ÈΩ°„ÄÅÊÄßÂà•„ÄÅÁ®ÆÊóèÔºâÂíåËá®Â∫äÂõ†Á¥†Ôºà‰æãÂ¶ÇÁñæÁóÖÂö¥ÈáçÁ®ãÂ∫¶ÔºâÂ∞éËá¥‰∏çÂπ≥Ë°°ÁöÑËá®Â∫äÊï∏ÊìöÊé°ÈõÜ‰∏≠Â≠òÂú®ÂÅèÂ∑ÆÔºåÂõ†Ê≠§Á¢∫‰øùÂÖ¨Âπ≥ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂèóÊúÄÂÑ™ÊéßÂà∂ÁêÜË´ñÂïüÁôºÁöÑÊÑüÁü•Ê∑∑ÂêàÂ∞àÂÆ∂ (dMoE)„ÄÇÊàëÂÄëÂ∞çÂÖ∂Â∫ïÂ±§Ê©üÂà∂ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÂàÜÊûêÔºå‰∏¶ÈáêÊ∏Ö‰∫Ü dMoE Âú®ÈÅ©ÊáâÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÁöÑÁï∞Ë≥™ÂàÜ‰Ωà‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞á dMoE Êï¥ÂêàÂà∞Â§öÂÄãÁ∂≤Ë∑ØÊû∂Êßã‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂêÑÁ®ÆÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰ªªÂãô‰∏≠ÁöÑÂª£Ê≥õÈÅ©Áî®ÊÄß„ÄÇÈÄöÈÅéÁ¥çÂÖ•‰∫∫Âè£Áµ±Ë®àÂíåËá®Â∫äÂõ†Á¥†ÔºådMoE Âú®ÂÖ©ÂÄã 2D Âü∫Ê∫ñÊï∏ÊìöÈõÜÂíå‰∏ÄÂÄã 3D ÂÖßÈÉ®Êï∏ÊìöÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÂá∫‰∫Ü dMoE Âú®Ê∏õËºï‰∏çÂπ≥Ë°°ÂàÜ‰ΩàÁöÑÂÅèÂ∑ÆÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÁÇ∫Âú®ÂÖ¨Âπ≥ÊÄßÂ≠∏ÁøíÁØÑ‰æã‰∏≠Ê©ãÊé•ÊéßÂà∂ÁêÜË´ñÂíåÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÊñπÊ≥ï„ÄÇÂéüÂßãÁ¢ºÂ∞áÊúÉÂÖ¨Èñã„ÄÇ

##### **Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**
2502.00568v1 by Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti

Emerging research has highlighted that artificial intelligence based
multimodal fusion of digital pathology and transcriptomic features can improve
cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.
However, such direct fusion for joint decision is impractical in real clinical
settings, where histopathology is still the gold standard for diagnosis and
transcriptomic tests are rarely requested, at least in the public healthcare
system. With our novel diffusion based crossmodal generative AI model PathoGen,
we show that genomic expressions synthesized from digital histopathology
jointly predicts cancer grading and patient survival risk with high accuracy
(state-of-the-art performance), certainty (through conformal coverage
guarantee) and interpretability (through distributed attention maps). PathoGen
code is available for open use by the research community through GitHub at
https://github.com/Samiran-Dey/PathoGen.

ÊëòË¶ÅÔºöÊñ∞ËààÁ†îÁ©∂Âº∑Ë™øÔºåÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÂ§öÊ®°ÊÖãËûçÂêàÊï∏‰ΩçÁóÖÁêÜÂ≠∏ÂíåËΩâÈåÑÁµÑÁâπÂæµÔºåÂèØ‰ª•ÊîπÂñÑÁôåÁóáË®∫Êñ∑ÔºàÂàÜÁ¥ö/ÂàÜÂûãÔºâÂíåÈ†êÂæåÔºàÂ≠òÊ¥ªÈ¢®Èö™ÔºâÈ†êÊ∏¨„ÄÇ
ÁÑ∂ËÄåÔºåÈÄôÁ®ÆÁõ¥Êé•ËûçÂêàÂ∞çÊñºËÅØÂêàÊ±∫Á≠ñÂú®ÂØ¶ÈöõËá®Â∫äÁí∞Â¢É‰∏≠ÊòØ‰∏çÂàáÂØ¶ÈöõÁöÑÔºåÂõ†ÁÇ∫Âú®ÂØ¶ÈöõËá®Â∫äÁí∞Â¢É‰∏≠ÔºåÁµÑÁπîÁóÖÁêÜÂ≠∏‰ªçÁÑ∂ÊòØË®∫Êñ∑ÁöÑÈªÉÈáëÊ®ôÊ∫ñÔºåËÄåËΩâÈåÑÁµÑÊ™¢Ê∏¨ÂæàÂ∞ëË¢´Ë¶ÅÊ±ÇÔºåËá≥Â∞ëÂú®ÂÖ¨ÂÖ±ÈÜ´ÁôÇÁ≥ªÁµ±‰∏≠ÊòØÂ¶ÇÊ≠§„ÄÇÈÄèÈÅéÊàëÂÄëÊñ∞Á©éÁöÑÂü∫ÊñºÊì¥Êï£ÁöÑË∑®Ê®°ÊÖãÁîüÊàêÂºè AI Ê®°Âûã PathoGenÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂæûÊï∏‰ΩçÁµÑÁπîÁóÖÁêÜÂ≠∏ÂêàÊàêÁöÑÂü∫Âõ†È´îË°®ÈÅîÔºåÂèØ‰ª•ÂÖ±ÂêåÈ†êÊ∏¨ÁôåÁóáÂàÜÁ¥öÂíåÊÇ£ËÄÖÂ≠òÊ¥ªÈ¢®Èö™ÔºåÂÖ∑ÊúâÈ´òÊ∫ñÁ¢∫Â∫¶ÔºàÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºâ„ÄÅÁ¢∫ÂÆöÊÄßÔºàÈÄèÈÅé‰øùÂΩ¢Ë¶ÜËìã‰øùË≠âÔºâÂíåÂèØËß£ÈáãÊÄßÔºàÈÄèÈÅéÂàÜ‰ΩàÂºèÊ≥®ÊÑèÂäõÂúñÔºâ„ÄÇPathoGen Á®ãÂºèÁ¢ºÂèØÈÄèÈÅé GitHub ‰∏äÁöÑ https://github.com/Samiran-Dey/PathoGenÔºåÈñãÊîæ‰æõÁ†îÁ©∂Á§æÁæ§‰ΩøÁî®„ÄÇ

##### **Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**
2502.00495v1 by Mohammad Saleh Torkestani, Robert Davis, Abdolhossein Sarrafzadeh

Time constraints on doctor patient interaction and restricted access to
specialists under the managed care system led to increasingly referring to
computers as a medical information source and a self-health-care management
tool. However, research show that less than 40% of information seekers
indicated that online information helped them to make a decision about their
health. Searching multiple web sites that need basic computer skills, lack of
interaction and no face to face interaction in most search engines and some
social issues, led us to develop a specialized life-like agent that would
overcome mentioned problems.

ÊëòË¶ÅÔºöÁî±ÊñºÁÆ°ÁêÜÂºèÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰∏≠ÈÜ´Â∏´ËàáÁóÖÊÇ£‰∫íÂãïÊôÇÈñìÊúâÈôêÔºå‰∏îÂ∞àÁßëÈÜ´Â∏´ÁöÑÂèñÂæóÂèóÈôêÔºåÂõ†Ê≠§Ë∂ä‰æÜË∂äÂ§ö‰∫∫Â∞áÈõªËÖ¶Ë¶ñÁÇ∫ÈÜ´ÁôÇË≥áË®ä‰æÜÊ∫êÂíåËá™Êàë‰øùÂÅ•ÁÆ°ÁêÜÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂È°ØÁ§∫Ôºå‰∏çÂà∞ 40% ÁöÑË≥áË®äÂ∞ãÊ±ÇËÄÖË°®Á§∫ÔºåÁ∑ö‰∏äË≥áË®äÊúâÂä©Êñº‰ªñÂÄëÂÅöÂá∫ÂÅ•Â∫∑Ê±∫Á≠ñ„ÄÇÊêúÂ∞ãÈúÄË¶ÅÂü∫Êú¨ÈõªËÖ¶ÊäÄËÉΩÁöÑË®±Â§öÁ∂≤Á´ô„ÄÅÁº∫‰πè‰∫íÂãïÔºå‰ª•ÂèäÂ§ßÂ§öÊï∏ÊêúÂ∞ãÂºïÊìéÂíå‰∏Ä‰∫õÁ§æ‰∫§Ë≠∞È°å‰∏≠Ê≤íÊúâÈù¢Â∞çÈù¢ÁöÑ‰∫íÂãïÔºåÈÄô‰∫õÂéüÂõ†‰øÉ‰ΩøÊàëÂÄëÈñãÁôºÂá∫‰∏ÄÂÄãÁâπÊÆä‰∏îÈÄºÁúüÁöÑ‰ª£ÁêÜ‰∫∫Ôºå‰ª•ÂÖãÊúç‰∏äËø∞ÂïèÈ°å„ÄÇ

##### **Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**
2502.00451v1 by Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych

Mental illness is a widespread and debilitating condition with substantial
societal and personal costs. Traditional diagnostic and treatment approaches,
such as self-reported questionnaires and psychotherapy sessions, often impose
significant burdens on both patients and clinicians, limiting accessibility and
efficiency. Recent advances in Artificial Intelligence (AI), particularly in
Natural Language Processing and multimodal techniques, hold great potential for
recognizing and addressing conditions such as depression, anxiety, bipolar
disorder, schizophrenia, and post-traumatic stress disorder. However, privacy
concerns, including the risk of sensitive data leakage from datasets and
trained models, remain a critical barrier to deploying these AI systems in
real-world clinical settings. These challenges are amplified in multimodal
methods, where personal identifiers such as voice and facial data can be
misused. This paper presents a critical and comprehensive study of the privacy
challenges associated with developing and deploying AI models for mental
health. We further prescribe potential solutions, including data anonymization,
synthetic data generation, and privacy-preserving model training, to strengthen
privacy safeguards in practical applications. Additionally, we discuss
evaluation frameworks to assess the privacy-utility trade-offs in these
approaches. By addressing these challenges, our work aims to advance the
development of reliable, privacy-aware AI tools to support clinical
decision-making and improve mental health outcomes.

ÊëòË¶ÅÔºöÁ≤æÁ•ûÁñæÁóÖÊòØ‰∏ÄÁ®ÆÂª£Ê≥õ‰∏îÊúÉ‰Ωø‰∫∫Ë°∞Âº±ÁöÑÁñæÁóÖÔºåÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÁ§æÊúÉÂíåÂÄã‰∫∫ÊàêÊú¨„ÄÇÂÇ≥Áµ±ÁöÑË®∫Êñ∑ËàáÊ≤ªÁôÇÊñπÊ≥ïÔºå‰æãÂ¶ÇËá™ÊàëÂ†±ÂëäÂïèÂç∑ÂíåÂøÉÁêÜÊ≤ªÁôÇÁôÇÁ®ãÔºåÈÄöÂ∏∏ÊúÉÂ∞çÊÇ£ËÄÖÂíåËá®Â∫äÈÜ´ÁîüÈÄ†ÊàêÈáçÂ§ßË≤†ÊìîÔºåÈôêÂà∂‰∫ÜÂèØÂèäÊÄßÂíåÊïàÁéá„ÄÇ‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÂ§öÊ®°ÂºèÊäÄË°ìÊñπÈù¢ÔºåÂú®Ëæ®Ë≠òÂíåËôïÁêÜÊÜÇÈ¨±Áóá„ÄÅÁÑ¶ÊÖÆÁóá„ÄÅË∫ÅÈ¨±Áóá„ÄÅÁ≤æÁ•ûÂàÜË£ÇÁóáÂíåÂâµÂÇ∑ÂæåÂ£ìÂäõÁóáÂÄôÁæ§Á≠âÁñæÁóÖÊñπÈù¢ÂÖ∑ÊúâÊ•µÂ§ßÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÈö±ÁßÅÂïèÈ°åÔºåÂåÖÊã¨Ë≥áÊñôÈõÜÂíåË®ìÁ∑¥Ê®°Âûã‰∏≠ÊïèÊÑüË≥áÊñôÂ§ñÊ¥©ÁöÑÈ¢®Èö™Ôºå‰ªçÁÑ∂ÊòØÈÄô‰∫õ AI Á≥ªÁµ±Âú®ÁúüÂØ¶Ëá®Â∫äÁí∞Â¢É‰∏≠ÈÉ®ÁΩ≤ÁöÑ‰∏ÄÈ†ÖÈóúÈçµÈöúÁ§ô„ÄÇÈÄô‰∫õÊåëÊà∞Âú®Â§öÊ®°ÂºèÊñπÊ≥ï‰∏≠ÊúÉË¢´ÊîæÂ§ßÔºåÂõ†ÁÇ∫Ë™ûÈü≥ÂíåÈù¢ÈÉ®Ë≥áÊñôÁ≠âÂÄã‰∫∫Ë≠òÂà•Ë≥áÊñôÂèØËÉΩÊúÉË¢´Êø´Áî®„ÄÇÊú¨ÊñáÂ∞çËàáÈñãÁôºÂíåÈÉ®ÁΩ≤Áî®ÊñºÂøÉÁêÜÂÅ•Â∫∑ÁöÑ AI Ê®°ÂûãÁõ∏ÈóúÁöÑÈö±ÁßÅÊåëÊà∞ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÊâπÂà§‰∏îÂÖ®Èù¢ÁöÑÁ†îÁ©∂„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜÊΩõÂú®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂåÖÊã¨Ë≥áÊñôÂåøÂêçÂåñ„ÄÅÂêàÊàêË≥áÊñôÁî¢ÁîüÂíåÈö±ÁßÅ‰øùË≠∑Ê®°ÂûãË®ìÁ∑¥Ôºå‰ª•Âä†Âº∑ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÈö±ÁßÅ‰øùÈöú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫ÜË©ï‰º∞ÈÄô‰∫õÊñπÊ≥ï‰∏≠Èö±ÁßÅËàáÊïàÁî®ÂèñÊç®ÁöÑË©ï‰º∞Êû∂Êßã„ÄÇÈÄèÈÅéËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Êé®ÈÄ≤ÂèØÈù†„ÄÅÈáçË¶ñÈö±ÁßÅÁöÑ AI Â∑•ÂÖ∑ÁöÑÈñãÁôºÔºå‰ª•ÊîØÊåÅËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö‰∏¶ÊîπÂñÑÂøÉÁêÜÂÅ•Â∫∑ÊàêÊûú„ÄÇ

##### **EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**
2502.00205v1 by Omar H. Khater, Abdul Jabbar Siddiqui, M. Shamim Hossain

Sustainable agriculture plays a crucial role in ensuring world food security
for consumers. A critical challenge faced by sustainable precision agriculture
is weed growth, as weeds share essential resources with the crops, such as
water, soil nutrients, and sunlight, which notably affect crop yields. The
traditional methods employed to combat weeds include the usage of chemical
herbicides and manual weed removal methods. However, these could damage the
environment and pose health hazards. The adoption of automated computer vision
technologies and ground agricultural consumer electronic vehicles in precision
agriculture offers sustainable, low-carbon solutions. However, prior works
suffer from issues such as low accuracy and precision and high computational
expense. This work proposes EcoWeedNet, a novel model with enhanced weed
detection performance without adding significant computational complexity,
aligning with the goals of low-carbon agricultural practices. Additionally, our
model is lightweight and optimal for deployment on ground-based consumer
electronic agricultural vehicles and robots. The effectiveness of the proposed
model is demonstrated through comprehensive experiments on the CottonWeedDet12
benchmark dataset reflecting real-world scenarios. EcoWeedNet achieves
performance close to that of large models yet with much fewer parameters.
(approximately 4.21% of the parameters and 6.59% of the GFLOPs of YOLOv4). This
work contributes effectively to the development of automated weed detection
methods for next-generation agricultural consumer electronics featuring lower
energy consumption and lower carbon footprint. This work paves the way forward
for sustainable agricultural consumer technologies.

ÊëòË¶ÅÔºöÊ∞∏Á∫åËæ≤Ê•≠Âú®Á¢∫‰øù‰∏ñÁïåÁ≥ßÈ£üÂÆâÂÖ®ÊñπÈù¢ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤
Â∞çÊñºÊ∞∏Á∫åÁ≤æÊ∫ñËæ≤Ê•≠‰æÜË™™ÔºåÈõúËçâÁöÑÁîüÈï∑ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫ÈõúËçâËàáËæ≤‰ΩúÁâ©ÂÖ±‰∫´Ê∞¥„ÄÅÂúüÂ£§È§äÂàÜÂíåÈôΩÂÖâÁ≠âÂü∫Êú¨Ë≥áÊ∫êÔºåÈÄôÊúÉÈ°ØËëóÂΩ±ÈüøËæ≤‰ΩúÁâ©ÁöÑÁî¢Èáè„ÄÇÂÇ≥Áµ±‰∏äÁî®ÊñºÂ∞çÊäóÈõúËçâÁöÑÊñπÊ≥ïÂåÖÊã¨‰ΩøÁî®ÂåñÂ≠∏Èô§ËçâÂäëÂíå‰∫∫Â∑•Èô§ËçâÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂèØËÉΩÊúÉÊêçÂÆ≥Áí∞Â¢É‰∏¶ÈÄ†ÊàêÂÅ•Â∫∑Âç±ÂÆ≥„ÄÇÂú®Á≤æÊ∫ñËæ≤Ê•≠‰∏≠Êé°Áî®Ëá™ÂãïÂåñÈõªËÖ¶Ë¶ñË¶∫ÊäÄË°ìÂíåÂú∞Èù¢Ëæ≤Ê•≠Áî®Ê∂àË≤ªÈõªÂ≠êËªäËºõÊèê‰æõ‰∫ÜÊ∞∏Á∫åÁöÑ‰ΩéÁ¢≥Ëß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑÁ†îÁ©∂Â≠òÂú®Ê∫ñÁ¢∫Â∫¶ÂíåÁ≤æÁ¢∫Â∫¶‰Ωé‰ª•ÂèäË®àÁÆóÊàêÊú¨È´òÁ≠âÂïèÈ°å„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰∫Ü EcoWeedNetÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑÊ®°ÂûãÔºåÂÖ∑ÊúâÂ¢ûÂº∑ÁöÑÈõúËçâÂÅµÊ∏¨ÊïàËÉΩÔºåËÄå‰∏çÊúÉÂ¢ûÂä†È°ØËëóÁöÑË®àÁÆóË§áÈõúÂ∫¶ÔºåÁ¨¶Âêà‰ΩéÁ¢≥Ëæ≤Ê•≠ÂØ¶ÂãôÁöÑÁõÆÊ®ô„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãËºïÂ∑ßÔºåÊúÄÈÅ©ÊñºÈÉ®ÁΩ≤Âú®Âú∞Èù¢Ê∂àË≤ªÈõªÂ≠êËæ≤Ê•≠ËªäËºõÂíåÊ©üÂô®‰∫∫‰∏ä„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂ∑≤ÈÄöÈÅéÂú®ÂèçÊò†ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÁöÑ CottonWeedDet12 Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂÖ®Èù¢ÂØ¶È©óÂæóÂà∞Ë≠âÊòé„ÄÇEcoWeedNet ÁöÑÊïàËÉΩÊé•ËøëÂ§ßÂûãÊ®°ÂûãÔºå‰ΩÜÂèÉÊï∏ÂçªÂ∞ëÂæóÂ§ö„ÄÇ(Â§ßÁ¥ÑÊòØ YOLOv4 ÂèÉÊï∏ÁöÑ 4.21% Âíå GFLOP ÁöÑ 6.59%)„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊúâÊïàÂú∞‰øÉÈÄ≤‰∫Ü‰∏ã‰∏Ä‰ª£Ëæ≤Ê•≠Ê∂àË≤ªÈõªÂ≠êÁî¢ÂìÅÁöÑËá™ÂãïÂåñÈõúËçâÂÅµÊ∏¨ÊñπÊ≥ïÁöÑÈñãÁôºÔºåÈÄô‰∫õÁî¢ÂìÅÁöÑÁâπÈªûÊòØËÉΩËÄóÊõ¥‰Ωé„ÄÅÁ¢≥Ë∂≥Ë∑°Êõ¥‰Ωé„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Ê∞∏Á∫åËæ≤Ê•≠Ê∂àË≤ªÊäÄË°ìÈã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

##### **DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**
2502.00196v1 by Abdurrahim Yilmaz, Furkan Yuceyalcin, Ece Gokyayla, Donghee Choi, Ozan Erdem Ali Anil Demircali, Rahmetullah Varol, Ufuk Gorkem Kirabali, Gulsum Gencoglan, Joram M. Posma, Burak Temelkuran

A major barrier to developing vision large language models (LLMs) in
dermatology is the lack of large image--text pairs dataset. We introduce
DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated
from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for
dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using
Gemini 2.0, we used clinically related prompts and self-instruct method to
generate diverse and rich synthetic texts. Metadata of the datasets were
incorporated into the input prompts by targeting to reduce potential
hallucinations. The resulting dataset builds upon open access dermatological
image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have
permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary
Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We
anticipate this dataset to support and accelerate AI research in dermatology.
Data and code underlying this work are accessible at
https://github.com/abdurrahimyilmaz/DermaSynth.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÁöÆËÜöÁßëÁôºÂ±ïÁöÑ‰∏ÄÂ§ßÈöúÁ§ôÊòØÁº∫‰πèÂ§ßÈáèÁöÑÂΩ±ÂÉèÊñáÂ≠óÂ∞çÊáâË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÂºïÈÄ≤ DermaSynthÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± 92,020 ÂÄãÂêàÊàêÂΩ±ÂÉèÊñáÂ≠óÂ∞çÊáâË≥áÊñôÁµÑÊàêÁöÑË≥áÊñôÈõÜÔºåÈÄô‰∫õË≥áÊñôÂ∞çÊáâË≥áÊñôÊòØÂæû 45,205 ÂÄãÂΩ±ÂÉèÔºà13,568 ÂÄãËá®Â∫äÂΩ±ÂÉèÂíå 35,561 ÂÄãÁöÆËÜöÈè°ÂΩ±ÂÉèÔºâ‰∏≠Á≠ñÂäÉËÄå‰æÜÁöÑÔºåÁî®ÊñºÁöÆËÜöÁßëÁõ∏ÈóúÁöÑËá®Â∫ä‰ªªÂãô„ÄÇÂà©Áî®ÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºå‰ΩøÁî® Gemini 2.0ÔºåÊàëÂÄë‰ΩøÁî®ËàáËá®Â∫äÁõ∏ÈóúÁöÑÊèêÁ§∫ÂíåËá™ÊàëÊåáÂ∞éÊñπÊ≥ï‰æÜÁî¢ÁîüÂ§öÊ®£‰∏îË±êÂØåÁöÑÂêàÊàêÊñáÂ≠ó„ÄÇË≥áÊñôÈõÜÁöÑÂÖÉË≥áÊñôÊúÉÁ¥çÂÖ•Ëº∏ÂÖ•ÊèêÁ§∫‰∏≠ÔºåÁõÆÊ®ôÊòØÊ∏õÂ∞ëÊΩõÂú®ÁöÑÂπªË¶∫„ÄÇÁî¢ÁîüÁöÑË≥áÊñôÈõÜÂª∫Á´ãÂú®ÈñãÊîæÂèñÁî®ÁöÑÁöÆËÜöÁßëÂΩ±ÂÉèÂÑ≤Â≠òÂ∫´ÔºàDERM12345„ÄÅBCN20000„ÄÅPAD-UFES-20„ÄÅSCIN Âíå HIBAÔºâ‰πã‰∏äÔºåÈÄô‰∫õÂÑ≤Â≠òÂ∫´ÊìÅÊúâÂØ¨È¨ÜÁöÑ CC-BY-4.0 licenses„ÄÇÊàëÂÄëÈÇÑÂ∞ç‰∏ÄÂÄãÂàùÊ≠•ÁöÑ Llama-3.2-11B-Vision-Instruct Ê®°ÂûãÔºåDermatoLlama 1.0ÔºåÂú® 5,000 ÂÄãÊ®£Êú¨‰∏äÈÄ≤Ë°åÂæÆË™ø„ÄÇÊàëÂÄëÈ†êË®àÈÄôÂÄãË≥áÊñôÈõÜÂ∞áÊîØÊè¥ÂíåÂä†ÈÄüÁöÆËÜöÁßëÁöÑ AI Á†îÁ©∂„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/abdurrahimyilmaz/DermaSynth ÂèñÂæó„ÄÇ

##### **Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**
2502.00146v1 by Hassan Jahanandish, Shengtian Sang, Cynthia Xinran Li, Sulaiman Vesal, Indrani Bhattacharya, Jeong Hoon Lee, Richard Fan, Geoffrey A. Sonna, Mirabela Rusu

Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target
suspicious prostate lesions. This has led to artificial intelligence (AI)
applications improving MRI-based detection of clinically significant prostate
cancer (CsPCa). However, MRI-detected lesions must still be mapped to
transrectal ultrasound (TRUS) images during biopsy, which results in missing
CsPCa. This study systematically evaluates a multimodal AI framework
integrating MRI and TRUS image sequences to enhance CsPCa identification. The
study included 3110 patients from three cohorts across two institutions who
underwent prostate biopsy. The proposed framework, based on the 3D UNet
architecture, was evaluated on 1700 test cases, comparing performance to
unimodal AI models that use either MRI or TRUS alone. Additionally, the
proposed model was compared to radiologists in a cohort of 110 patients. The
multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice
(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared
to radiologists, the multimodal model showed higher specificity (88% vs. 78%)
and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings
demonstrate the potential of multimodal AI to improve CsPCa lesion targeting
during biopsy and treatment planning, surpassing current unimodal models and
radiologists; ultimately improving outcomes for prostate cancer patients.

ÊëòË¶ÅÔºö<paragraph>Âú®Ê¥ªÊ™¢ÂâçÔºåÁ£ÅÊåØÈÄ†ÂΩ±ÔºàMRIÔºâÊ≠£Ë∂ä‰æÜË∂äÂ∏∏Ë¢´Áî®ÊñºÈéñÂÆöÂèØÁñëÁöÑÊîùË≠∑ËÖ∫ÁóÖÁÅ∂„ÄÇÈÄôÂ∞éËá¥‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊáâÁî®Á®ãÂºèÊîπÂñÑ‰∫Ü‰ª• MRI ÁÇ∫Âü∫Á§éÁöÑËá®Â∫äÈ°ØËëóÊîùË≠∑ËÖ∫ÁôåÔºàCsPCaÔºâÊ™¢Ê∏¨„ÄÇÁÑ∂ËÄåÔºåÂú®Ê¥ªÊ™¢ÊúüÈñìÔºåÁî± MRI ÂÅµÊ∏¨Âà∞ÁöÑÁóÖÁÅ∂‰ªçÂøÖÈ†àÂ∞çÊáâÂà∞Á∂ìÁõ¥ËÖ∏Ë∂ÖÈü≥Ê≥¢ÔºàTRUSÔºâÂΩ±ÂÉèÔºåÈÄôÂ∞éËá¥ÈåØÂ§± CsPCa„ÄÇÊú¨Á†îÁ©∂Á≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÊÖã AI Êû∂ÊßãÔºåÊï¥Âêà MRI Âíå TRUS ÂΩ±ÂÉèÂ∫èÂàóÔºå‰ª•Â¢ûÂº∑ CsPCa Ë≠òÂà•„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Á¥çÂÖ•‰∫Ü‰æÜËá™ÂÖ©ÂÆ∂Ê©üÊßãÁöÑ‰∏âÂÄãÁæ§ÁµÑ‰∏≠ÁöÑ 3110 ÂêçÊÇ£ËÄÖÔºå‰ªñÂÄëÊé•Âèó‰∫ÜÊîùË≠∑ËÖ∫Ê¥ªÊ™¢„ÄÇÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÂü∫Êñº 3D UNet Êû∂ÊßãÔºåÂú® 1700 ÂÄãÊ∏¨Ë©¶Ê°à‰æã‰∏≠ÈÄ≤Ë°åË©ï‰º∞Ôºå‰∏¶Â∞áÂÖ∂ÊïàËÉΩËàáÂÉÖ‰ΩøÁî® MRI Êàñ TRUS ÁöÑÂñÆÊ®°ÊÖã AI Ê®°ÂûãÈÄ≤Ë°åÊØîËºÉ„ÄÇÊ≠§Â§ñÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãËàá‰∏ÄÂÄãÁî± 110 ÂêçÊÇ£ËÄÖÁµÑÊàêÁöÑÁæ§ÁµÑ‰∏≠ÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°åÊØîËºÉ„ÄÇËàáÂñÆÊ®°ÊÖã MRIÔºà73%„ÄÅ30%ÔºâÂíå TRUS Ê®°ÂûãÔºà49%„ÄÅ27%ÔºâÁõ∏ÊØîÔºåÂ§öÊ®°ÊÖã AI ÊñπÊ≥ïÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÊïèÊÑüÂ∫¶Ôºà80%ÔºâÂíåÁóÖÁÅ∂ DiceÔºà42%Ôºâ„ÄÇËàáÊîæÂ∞ÑÁßëÈÜ´Â∏´Áõ∏ÊØîÔºåÂ§öÊ®°ÊÖãÊ®°ÂûãÈ°ØÁ§∫Âá∫Êõ¥È´òÁöÑÁâπÁï∞ÊÄßÔºà88% Â∞ç 78%ÔºâÂíåÁóÖÁÅ∂ DiceÔºà38% Â∞ç 33%ÔºâÔºå‰∏îÊïèÊÑüÂ∫¶Áõ∏Áï∂Ôºà79%Ôºâ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË≠âÊòé‰∫ÜÂ§öÊ®°ÊÖã AI Âú®Ê¥ªÊ™¢ÂíåÊ≤ªÁôÇË®àÁï´ÊúüÈñìÊîπÂñÑ CsPCa ÁóÖÁÅ∂ÈéñÂÆöÁöÑÊΩõÂäõÔºåË∂ÖË∂ä‰∫ÜÁõÆÂâçÁöÑÂñÆÊ®°ÊÖãÊ®°ÂûãÂíåÊîæÂ∞ÑÁßëÈÜ´Â∏´ÔºõÊúÄÁµÇÊîπÂñÑ‰∫ÜÊîùË≠∑ËÖ∫ÁôåÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊàêÊûú„ÄÇ</paragraph>

##### **AIN: The Arabic INclusive Large Multimodal Model**
2502.00094v2 by Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan

Amid the swift progress of large language models (LLMs) and their evolution
into large multimodal models (LMMs), significant strides have been made in
high-resource languages such as English and Chinese. While Arabic LLMs have
seen notable progress, Arabic LMMs remain largely unexplored, often narrowly
focusing on a few specific aspects of the language and visual understanding. To
bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal
Model-designed to excel across diverse domains. AIN is an English-Arabic
bilingual LMM designed to excel in English and Arabic, leveraging carefully
constructed 3.6 million high-quality Arabic-English multimodal data samples.
AIN demonstrates state-of-the-art Arabic performance, while also possessing
strong English-language visual capabilities. On the recent CAMEL-Bench
benchmark comprising 38 sub-domains including, multi-image understanding,
complex visual perception, handwritten document understanding, video
understanding, medical imaging, plant diseases, and remote sensing-based land
use understanding, our AIN demonstrates strong performance with the 7B model
outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains
and 38 sub-domains. AIN's superior capabilities position it as a significant
step toward empowering Arabic speakers with advanced multimodal generative AI
tools across diverse applications.

ÊëòË¶ÅÔºöÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âø´ÈÄüÁôºÂ±ïÔºå‰∏¶ÊºîËÆäÊàêÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) ÁöÑÈÅéÁ®ã‰∏≠ÔºåËã±Ë™ûÂíå‰∏≠ÊñáÁ≠âÈ´òË≥áÊ∫êË™ûË®ÄÂ∑≤ÂèñÂæóÈáçÂ§ßÈÄ≤Â±ï„ÄÇÈõñÁÑ∂ÈòøÊãâ‰ºØË™û LLM Â∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºå‰ΩÜÈòøÊãâ‰ºØË™û LMM ‰ªçÊú™Ë¢´Âª£Ê≥õÊé¢Á¥¢ÔºåÈÄöÂ∏∏Âè™ÁãπÈöòÂú∞ÈóúÊ≥®Ë™ûË®ÄÂíåË¶ñË¶∫ÁêÜËß£ÁöÑÂπæÂÄãÁâπÂÆöÊñπÈù¢„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄôÈ†ÖÂ∑ÆË∑ùÔºåÊàëÂÄëÊé®Âá∫‰∫Ü AINÔºåÂç≥ÈòøÊãâ‰ºØË™ûÂåÖÂÆπÊÄßÂ§öÊ®°ÊÖãÊ®°ÂûãÔºåÊó®Âú®Âú®‰∏çÂêåÈ†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤„ÄÇAIN ÊòØ‰∏ÄÂÄãËã±Ë™û-ÈòøÊãâ‰ºØË™ûÈõôË™û LMMÔºåÊó®Âú®Á≤æÈÄöËã±Ë™ûÂíåÈòøÊãâ‰ºØË™ûÔºåÂà©Áî®Á≤æÂøÉÂª∫ÊßãÁöÑ 360 Ëê¨ÂÄãÈ´òÂìÅË≥™ÈòøÊãâ‰ºØË™û-Ëã±Ë™ûÂ§öÊ®°ÊÖãÊï∏ÊìöÊ®£Êú¨„ÄÇAIN Â±ïÁ§∫‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÈòøÊãâ‰ºØË™ûÊïàËÉΩÔºåÂêåÊôÇ‰πüÂÖ∑ÂÇôÂº∑Â§ßÁöÑËã±Ë™ûË¶ñË¶∫ËÉΩÂäõ„ÄÇÂú®ÊúÄËøëÁöÑ CAMEL-Bench Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂåÖÂê´ 38 ÂÄãÂ≠êÈ†òÂüüÔºåÂåÖÊã¨Â§öÂΩ±ÂÉèÁêÜËß£„ÄÅË§áÈõúË¶ñË¶∫ÊÑüÁü•„ÄÅÊâãÂØ´Êñá‰ª∂ÁêÜËß£„ÄÅÂΩ±ÁâáÁêÜËß£„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÅÊ§çÁâ©ÁñæÁóÖÂíåÂü∫ÊñºÈÅôÊ∏¨ÁöÑÂúüÂú∞‰ΩøÁî®ÁêÜËß£ÔºåÊàëÂÄëÁöÑ AIN Ë°®ÁèæÂá∫Ëâ≤ÔºåÂÖ∂‰∏≠ 7B Ê®°ÂûãÂú®ÂÖ´ÂÄãÈ†òÂüüÂíå 38 ÂÄãÂ≠êÈ†òÂüüÁöÑÂπ≥ÂùáÁµïÂ∞çÂ¢ûÁõäÁÇ∫ 3.4%ÔºåÂÑ™Êñº GPT-4o„ÄÇAIN ÁöÑÂçìË∂äËÉΩÂäõ‰ΩøÂÖ∂ÊàêÁÇ∫ÊúùËëóË≥¶‰∫àÈòøÊãâ‰ºØË™û‰ΩøÁî®ËÄÖÈÄ≤ÈöéÂ§öÊ®°ÊÖãÁîüÊàêÂºè AI Â∑•ÂÖ∑ÈÇÅÂá∫ÁöÑÈáçË¶Å‰∏ÄÊ≠•ÔºåÂèØÁî®ÊñºÂêÑÁ®ÆÊáâÁî®„ÄÇ

##### **Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**
2501.19338v1 by Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente Istv√°n L√°nczi, Andras Jakab

Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.

ÊëòË¶ÅÔºö<paragraph>ÈñãÁôºÁî®ÊñºËá™ÂãïÂàÜÊûêËá®Â∫äËÉéÂÖíÂíåÊñ∞ÁîüÂÖí MRI Ë≥áÊñôÁöÑÊñ∞ÊñπÊ≥ïÂèóÂà∞Ê®ôË®ªÁóÖÁêÜË≥áÊñôÈõÜÁ®ÄÂ∞ëÂíåÈö±ÁßÅÂïèÈ°åÁöÑÈôêÂà∂ÔºåÈÄô‰∫õÂïèÈ°åÈÄöÂ∏∏ÊúÉÈôêÂà∂Ë≥áÊñôÂÖ±‰∫´ÔºåÂæûËÄåÈòªÁ§ôÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄë‰ª•ÂÖ©Á®ÆÊñπÂºèËß£Ê±∫ÈÄôÂÄãÂïèÈ°å„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Fetal&Neonatal-DDPMÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊì¥Êï£Ê®°ÂûãÊû∂ÊßãÔºåÊó®Âú®ÂæûË™ûÁæ©Ê®ôÁ±§ÂΩ±ÂÉèÁîüÊàêÈ´òÂìÅË≥™ÁöÑÂêàÊàêÁóÖÁêÜËÉéÂÖíÂíåÊñ∞ÁîüÂÖí MRI„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÈÄèÈÅéÂΩ¢ÊÖãÊîπËÆä‰æÜ‰øÆÊîπÂÅ•Â∫∑ÁöÑÊ®ôÁ±§ÂΩ±ÂÉèÔºå‰ª•Ê®°Êì¨ËÖ¶ÂÆ§Êì¥Â§ß„ÄÅÂ∞èËÖ¶ÂíåÊ©ãËÖ¶Â∞èËÖ¶ÁôºËÇ≤‰∏çÂÖ®‰ª•ÂèäÂ∞èÈ†≠Áï∏ÂΩ¢Á≠âÊÉÖÊ≥ÅÔºåÂæûËÄåÂ¢ûÂº∑Ë®ìÁ∑¥Ë≥áÊñô„ÄÇÈÄèÈÅéÂà©Áî® Fetal&Neonatal-DDPMÔºåÊàëÂÄëÂæûÈÄô‰∫õ‰øÆÊîπÂæåÁöÑÁóÖÁêÜÊ®ôÁ±§ÂΩ±ÂÉè‰∏≠ÂêàÊàê‰∫ÜÈÄºÁúüÁöÑÁóÖÁêÜ MRI„ÄÇÊîæÂ∞ÑÁßëÈÜ´Â∏´Ë©ï‰º∞ÂêàÊàê MRI ÁöÑÂìÅË≥™ÂíåË®∫Êñ∑ÂÉπÂÄºÈ°ØËëóÂÑ™ÊñºÁúüÂØ¶ MRIÔºàp < 0.05ÔºâÔºåÂ±ïÁ§∫‰∫ÜË°ÄÁÆ°ÂíåËÑàÁµ°Âè¢Á≠âÁâπÂæµÔºå‰∏¶ÊîπÂñÑ‰∫ÜËàáÊ®ôÁ±§Ë®ªËß£ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂêàÊàêÁóÖÁêÜË≥áÊñôÂ¢ûÂº∑‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑ nnUNet ÂàÜÂâ≤ÊïàËÉΩÔºåÁâπÂà•ÊòØÂ∞çÊñºÂö¥ÈáçÁöÑËÖ¶ÂÆ§Êì¥Â§ßÁóÖ‰æãÔºåÂÖ∂‰∏≠ËÖ¶ÂÆ§ÂàÜÂâ≤ÔºàDice ÂàÜÊï∏Ôºö0.9253 Â∞ç 0.7317ÔºâÁöÑÊîπÂñÑÊúÄÂ§ß„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÁîüÊàêÂºè AI ‰ΩúÁÇ∫Ë≥áÊñôÊì¥ÂÖÖËΩâÂûãÂ∑•ÂÖ∑ÁöÑÊΩõÂäõÔºåÂú®ÁóÖÁêÜÁóÖ‰æã‰∏≠Êèê‰æõ‰∫ÜÊîπÂñÑÁöÑÂàÜÂâ≤ÊïàËÉΩ„ÄÇÈÄôÈ†ÖÁôºÂ±ï‰ª£Ë°®‰∫ÜÊîπÂñÑÁî¢ÂâçÂΩ±ÂÉèÂàÜÊûêÂíåÂàÜÂâ≤Ê∫ñÁ¢∫ÊÄßÁöÑÈáçË¶Å‰∏ÄÊ≠•Ôºå‰πüÁÇ∫ÈÄèÈÅéÁîüÊàêÁóÖÁêÜÂΩ±ÂÉèË≥áÊñô‰æÜÈÄ≤Ë°åË≥áÊñôÂåøÂêçÂåñÊèê‰æõ‰∫ÜÊñ∞ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**
2501.19271v1 by Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett

Concept-based explanation methods, such as concept bottleneck models (CBMs),
aim to improve the interpretability of machine learning models by linking their
decisions to human-understandable concepts, under the critical assumption that
such concepts can be accurately attributed to the network's feature space.
However, this foundational assumption has not been rigorously validated, mainly
because the field lacks standardised metrics and benchmarks to assess the
existence and spatial alignment of such concepts. To address this, we propose
three metrics: the concept global importance metric, the concept existence
metric, and the concept location metric, including a technique for visualising
concept activations, i.e., concept activation mapping. We benchmark post-hoc
CBMs to illustrate their capabilities and challenges. Through qualitative and
quantitative experiments, we demonstrate that, in many cases, even the most
important concepts determined by post-hoc CBMs are not present in input images;
moreover, when they are present, their saliency maps fail to align with the
expected regions by either activating across an entire object or misidentifying
relevant concept-specific regions. We analyse the root causes of these
limitations, such as the natural correlation of concepts. Our findings
underscore the need for more careful application of concept-based explanation
techniques especially in settings where spatial interpretability is critical.

ÊëòË¶ÅÔºöÂü∫ÊñºÊ¶ÇÂøµÁöÑËß£ÈáãÊñπÊ≥ïÔºå‰æãÂ¶ÇÊ¶ÇÂøµÁì∂È†∏Ê®°Âûã (CBM)ÔºåÊó®Âú®ÈÄèÈÅéÂ∞áÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊ±∫Á≠ñËàá‰∫∫È°ûÂèØÁêÜËß£ÁöÑÊ¶ÇÂøµÈÄ£ÁµêÔºå‰æÜÊèêÂçáÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºåÂÖ∂ÈóúÈçµÂÅáË®≠ÁÇ∫Ê≠§È°ûÊ¶ÇÂøµÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Ê≠∏Âõ†ÊñºÁ∂≤Ë∑ØÁöÑÁâπÂæµÁ©∫Èñì„ÄÇÁÑ∂ËÄåÔºåÊ≠§È†ÖÂü∫Á§éÂÅáË®≠Â∞öÊú™Á∂ìÈÅéÂö¥Ê†ºÈ©óË≠âÔºå‰∏ªË¶ÅÊòØÂõ†ÁÇ∫Ë©≤È†òÂüüÁº∫‰πèÊ®ôÊ∫ñÂåñÊåáÊ®ôÂíåÂü∫Ê∫ñ‰æÜË©ï‰º∞Ê≠§È°ûÊ¶ÇÂøµÁöÑÂ≠òÂú®ÂíåÁ©∫ÈñìÂ∞çÈΩä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏âÈ†ÖÊåáÊ®ôÔºöÊ¶ÇÂøµÊï¥È´îÈáçË¶ÅÊÄßÊåáÊ®ô„ÄÅÊ¶ÇÂøµÂ≠òÂú®ÊåáÊ®ôÂíåÊ¶ÇÂøµ‰ΩçÁΩÆÊåáÊ®ôÔºåÂåÖÊã¨‰∏ÄÁ®ÆÁî®ÊñºË¶ñË¶∫ÂåñÊ¶ÇÂøµÊ¥ªÂåñÔºåÂç≥Ê¶ÇÂøµÊ¥ªÂåñÂ∞çÊáâÁöÑÊäÄË°ì„ÄÇÊàëÂÄëÂ∞ç‰∫ãÂæå CBM ÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶Ôºå‰ª•Ë™™ÊòéÂÆÉÂÄëÁöÑËÉΩÂäõÂíåÊåëÊà∞„ÄÇÈÄèÈÅéÂÆöÊÄßÂíåÂÆöÈáèÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéÔºåÂú®Ë®±Â§öÊÉÖÊ≥Å‰∏ãÔºåÂç≥‰ΩøÊòØÁî±‰∫ãÂæå CBM Á¢∫ÂÆöÁöÑÊúÄÈáçË¶ÅÊ¶ÇÂøµ‰πü‰∏çÂ≠òÂú®ÊñºËº∏ÂÖ•ÂΩ±ÂÉè‰∏≠ÔºõÊ≠§Â§ñÔºåÁï∂ÂÆÉÂÄëÂ≠òÂú®ÊôÇÔºåÂÆÉÂÄëÁöÑÈ°ØËëóÊÄßÂúñÁÑ°Ê≥ïËàáÈ†êÊúüÁöÑÂçÄÂüüÂ∞çÈΩäÔºåÂéüÂõ†ÂèØËÉΩÊòØÂÆÉÂÄëÂú®Êï¥ÂÄãÁâ©‰ª∂‰∏≠Ê¥ªÂåñÔºåÊàñÈåØË™§Ëæ®Ë≠òÂá∫Áõ∏ÈóúÁöÑÊ¶ÇÂøµÁâπÂÆöÂçÄÂüü„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÈÄô‰∫õÈôêÂà∂ÁöÑÊ†πÊú¨ÂéüÂõ†Ôºå‰æãÂ¶ÇÊ¶ÇÂøµÁöÑËá™ÁÑ∂Áõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂº∑Ë™øÈúÄË¶ÅÊõ¥Â∞èÂøÉÂú∞ÊáâÁî®Âü∫ÊñºÊ¶ÇÂøµÁöÑËß£ÈáãÊäÄË°ìÔºåÁâπÂà•ÊòØÂú®Á©∫ÈñìÂèØËß£ÈáãÊÄßËá≥ÈóúÈáçË¶ÅÁöÑË®≠ÂÆö‰∏≠„ÄÇ

##### **Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**
2501.19176v1 by Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi

Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.

ÊëòË¶ÅÔºöÂÖ®Ë¶ñÈáéÊï∏‰Ωç‰π≥ÊàøÊîùÂΩ± (FFDM) ÊòØÂ∏∏Ë¶è‰π≥ÁôåÁØ©Ê™¢ÁöÑ‰∏ªË¶ÅÂΩ±ÂÉèÊ®°ÂºèÔºõÁÑ∂ËÄåÔºåÂ∞çÊñº‰π≥ÊàøÁµÑÁπîÁ∑ªÂØÜÊàñÁ∫ñÁ∂≠ÂõäËÖ´ÁóÖËÆäÁöÑÊÇ£ËÄÖÔºåÂÖ∂ÊúâÊïàÊÄßÂèóÂà∞ÈôêÂà∂„ÄÇÂ∞çÊØîÂ¢ûÂº∑ÂÖâË≠ú‰π≥ÊàøÊîùÂΩ± (CESM) ÊòØ‰∏ÄÁ®Æ‰∫åÁ¥öÂΩ±ÂÉèÊäÄË°ìÔºåÂèØÊèêÂçáËÖ´Áò§ÂÅµÊ∏¨ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºËºÉÈ´òÁöÑËºªÂ∞ÑÊõùÈú≤„ÄÅÂ∞çÊØîÂäëÁöÑ‰ΩøÁî®ÂíåÊúâÈôêÁöÑÂèØÂèäÊÄßÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊáâÁî®„ÄÇÂõ†Ê≠§ÔºåCESM ÈÄöÂ∏∏ÂÉÖ‰øùÁïôÂú®ÁâπÂÆöÊÉÖÊ≥Å‰∏ã‰ΩøÁî®ÔºåÂÑòÁÆ° CESM ÁöÑË®∫Êñ∑ÊïàËÉΩËºÉ‰Ω≥Ôºå‰ΩÜË®±Â§öÊÇ£ËÄÖ‰ªçÂè™ËÉΩ‰æùË≥¥ FFDM„ÄÇÈõñÁÑ∂ÂàáÁâáÊ™¢Êü•‰ªçÁÑ∂ÊòØÊòéÁ¢∫Ë®∫Êñ∑ÁöÑÈªÉÈáëÊ®ôÊ∫ñÔºå‰ΩÜÈÄôÊòØ‰∏ÄÁ®Æ‰æµÂÖ•ÊÄßÁ®ãÂ∫èÔºåÂèØËÉΩÊúÉËÆìÊÇ£ËÄÖÊÑüÂà∞‰∏çÈÅ©„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÂ§öÊ®°Âºè„ÄÅÂ§öË¶ñÂúñÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÈÄ≤Ë°åËôõÊì¨ÂàáÁâáÊ™¢Êü•ÔºåÂ∞á FFDM Âíå CESM Ê®°ÂºèÊï¥ÂêàÂú®È†≠Â∞æÂêëÂíåÂÖßÂ§ñÂÅ¥ÊñúË¶ñÂúñ‰∏≠Ôºå‰ª•Â∞áÁóÖÁÅ∂ÂàÜÈ°ûÁÇ∫ÊÉ°ÊÄßÊàñËâØÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ CESM Ë≥áÊñôÁº∫Â§±ÁöÑÊåëÊà∞ÔºåÊàëÂÄëÂà©Áî®ÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÂæû FFDM ÊéÉÊèè‰∏≠Êé®ÁÆó CESM ÂΩ±ÂÉè„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊï¥Âêà CESM Ê®°ÂºèÂ∞çÊñºÊèêÂçáËôõÊì¨ÂàáÁâáÊ™¢Êü•ÁöÑÊïàËÉΩËá≥ÈóúÈáçË¶Å„ÄÇÁï∂ÁúüÂØ¶ CESM Ë≥áÊñôÁº∫Â§±ÊôÇÔºåÂêàÊàê CESM ÂΩ±ÂÉèË¢´Ë≠âÊòéÊòØÊúâÊïàÁöÑÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÂñÆÁç®‰ΩøÁî® FFDMÔºåÁâπÂà•ÊòØÂú®ÁµêÂêà FFDM Âíå CESM Ê®°ÂºèÁöÑÂ§öÊ®°ÂºèÈÖçÁΩÆ‰∏≠„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÊΩõÂäõÊîπÂñÑË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ãÔºåÁÇ∫Ëá®Â∫äÈÜ´Â∏´Êèê‰æõÂ¢ûÂº∑ÁöÑÊô∫ÊÖßÂ∑•ÂÖ∑Ôºå‰ª•ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÂíåÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§Â§ñÔºå‰ΩúÁÇ∫Â∞çÁ†îÁ©∂Á§æÁæ§ÁöÑË≤¢ÁçªÔºåÊàëÂÄëÂÖ¨ÈñãÁôºÂ∏ÉÂú®ÂØ¶È©ó‰∏≠‰ΩøÁî®ÁöÑË≥áÊñôÈõÜÔºå‰ª•‰øÉÈÄ≤Ê≠§È†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•ÈÄ≤Â±ï„ÄÇ

##### **Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**
2501.19086v1 by Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang

X-ray imaging is pivotal in medical diagnostics, offering non-invasive
insights into a range of health conditions. Recently, vision-language models,
such as the Contrastive Language-Image Pretraining (CLIP) model, have
demonstrated potential in improving diagnostic accuracy by leveraging
large-scale image-text datasets. However, since CLIP was not initially designed
for medical images, several CLIP-like models trained specifically on medical
images have been developed. Despite their enhanced performance, issues of
fairness - particularly regarding demographic attributes - remain largely
unaddressed. In this study, we perform a comprehensive fairness analysis of
CLIP-like models applied to X-ray image classification. We assess their
performance and fairness across diverse patient demographics and disease
categories using zero-shot inference and various fine-tuning techniques,
including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation
(LoRA), and full fine-tuning. Our results indicate that while fine-tuning
improves model accuracy, fairness concerns persist, highlighting the need for
further fairness interventions in these foundational models.

ÊëòË¶ÅÔºöX ÂÖâÂΩ±ÂÉèÂú®ÈÜ´ÁôÇË®∫Êñ∑‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåËÉΩÊèê‰æõÂêÑÁ®ÆÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑÈùû‰æµÂÖ•ÊÄßË¶ãËß£„ÄÇÊúÄËøëÔºåË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºà‰æãÂ¶ÇÂ∞çÊØîË™ûË®ÄÂΩ±ÂÉèÈ†êË®ìÁ∑¥ (CLIP) Ê®°ÂûãÔºâÂ∑≤Ë≠âÊòéÊúâÊΩõÂäõÈÄèÈÅéÂà©Áî®Â§ßË¶èÊ®°ÂΩ±ÂÉèÊñáÂ≠óË≥áÊñôÈõÜ‰æÜÊîπÂñÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº CLIP ÊúÄÂàù‰∏¶ÈùûË®≠Ë®àÁî®ÊñºÈÜ´ÁôÇÂΩ±ÂÉèÔºåÂõ†Ê≠§Â∑≤Á∂ìÈñãÁôº‰∫ÜÊï∏ÂÄãÁâπÂà•ÈáùÂ∞çÈÜ´ÁôÇÂΩ±ÂÉèË®ìÁ∑¥ÁöÑÈ°û‰ºº CLIP Ê®°Âûã„ÄÇÂÑòÁÆ°ÂÆÉÂÄëÁöÑÊïàËÉΩÊúâÊâÄÊèêÂçáÔºå‰ΩÜÂÖ¨Âπ≥ÊÄßÁöÑÂïèÈ°åÔºàÁâπÂà•ÊòØÈóúÊñº‰∫∫Âè£Áµ±Ë®àÂ±¨ÊÄßÔºâ‰ªçÂ§ßÂ§öÊú™Áç≤Ëß£Ê±∫„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÊáâÁî®Êñº X ÂÖâÂΩ±ÂÉèÂàÜÈ°ûÁöÑÈ°û‰ºº CLIP Ê®°ÂûãÂü∑Ë°åÂÖ®Èù¢ÁöÑÂÖ¨Âπ≥ÊÄßÂàÜÊûê„ÄÇÊàëÂÄë‰ΩøÁî®Èõ∂Ê¨°Â≠∏ÁøíÊé®Ë´ñÂíåÂêÑÁ®ÆÂæÆË™øÊäÄË°ìÔºàÂåÖÊã¨Á∑öÊÄßÊé¢Êü•„ÄÅÂ§öÂ±§ÊÑüÁü•Âô® (MLP)„ÄÅ‰ΩéÁß©ÈÅ©Êáâ (LoRA) ÂíåÂÆåÊï¥ÂæÆË™øÔºâ‰æÜË©ï‰º∞ÂÆÉÂÄëÂú®‰∏çÂêåÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àÂíåÁñæÁóÖÈ°ûÂà•‰∏≠ÁöÑÊïàËÉΩÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈõñÁÑ∂ÂæÆË™øÊúÉÊîπÂñÑÊ®°ÂûãÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÂÖ¨Âπ≥ÊÄßÂïèÈ°å‰ªçÁÑ∂Â≠òÂú®ÔºåÂº∑Ë™øÈúÄË¶ÅÂú®ÈÄô‰∫õÂü∫Á§éÊ®°Âûã‰∏≠ÈÄ≤‰∏ÄÊ≠•Êé°ÂèñÂÖ¨Âπ≥ÊÄßÂπ≤È†êÊé™ÊñΩ„ÄÇ

##### **Survey and Improvement Strategies for Gene Prioritization with Large Language Models**
2501.18794v1 by Matthew Neeley, Guantong Qi, Guanchu Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, Fan Xia, Pengfei Liu, Zhandong Liu, Xia Hu

Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.

ÊëòË¶ÅÔºöÁΩïË¶ãÁñæÁóÖÁî±ÊñºÊÇ£ËÄÖÊï∏ÊìöÊúâÈôêÂíåÈÅ∫ÂÇ≥Â§öÊ®£ÊÄßÔºåË®∫Êñ∑Ëµ∑‰æÜÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂÑòÁÆ°ËÆäÁï∞ÂÑ™ÂÖàÁ¥öÊéíÂ∫èÊäÄË°ìÈÄ≤Ê≠•Ôºå‰ΩÜË®±Â§öÁóÖ‰æã‰ªçÊú™ÂæóÂà∞Ë®∫Êñ∑„ÄÇÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´Â≠∏ËÄÉË©¶‰∏≠Ë°®ÁèæËâØÂ•ΩÔºå‰ΩÜÂÆÉÂÄëÂú®Ë®∫Êñ∑ÁΩïË¶ãÈÅ∫ÂÇ≥ÁñæÁóÖÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂ∞öÊú™ÂæóÂà∞Ë©ï‰º∞„ÄÇÁÇ∫‰∫ÜË≠òÂà•Ëá¥ÁóÖÂü∫Âõ†ÔºåÊàëÂÄëÂ∞çÂêÑÁ®Æ LLM ÈÄ≤Ë°å‰∫ÜÂü∫Âõ†ÂÑ™ÂÖàÁ¥öÊéíÂ∫èÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇ‰ΩøÁî®Â§öÊô∫ËÉΩÈ´îÂíå‰∫∫È°ûË°®ÂûãÊú¨‰Ωì (HPO) ÂàÜÈ°ûÔºåÊàëÂÄëÊ†πÊìöË°®ÂûãÂíåÂèØËß£Ê±∫ÊÄßÂ∞çÊÇ£ËÄÖÈÄ≤Ë°å‰∫ÜÂàÜÈ°û„ÄÇÈö®ËëóÂü∫Âõ†ÁµÑÂ§ßÂ∞èÁöÑÂ¢ûÂä†ÔºåLLM ÊÄßËÉΩ‰∏ãÈôçÔºåÂõ†Ê≠§ÊàëÂÄë‰ΩøÁî®ÂàÜËÄåÊ≤ª‰πãÁ≠ñÁï•Â∞á‰ªªÂãôÂàÜËß£ÁÇ∫Êõ¥Â∞èÁöÑÂ≠êÈõÜ„ÄÇÂú®Âü∫Á∑ö‰∏≠ÔºåGPT-4 ÂÑ™ÊñºÂÖ∂‰ªñ LLMÔºåÂú®Ê≠£Á¢∫ÊéíÂ∫èËá¥ÁóÖÂü∫Âõ†ÊñπÈù¢ÈÅîÂà∞Ëøë 30% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂ§öÊô∫ËÉΩÈ´îÂíå HPO ÊñπÊ≥ïÊúâÂä©ÊñºÂçÄÂàÜËß£Ê±∫Êúâ‰ø°ÂøÉÁöÑÁóÖ‰æãÂíåÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁóÖ‰æãÔºåÂº∑Ë™øÂ∑≤Áü•Âü∫Âõ†-Ë°®ÂûãÈóúËÅØÂíåË°®ÂûãÁâπÁï∞ÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁôºÁèæÂÖ∑ÊúâÁâπÂÆöË°®ÂûãÊàñÊòéÁ¢∫ÈóúËÅØÁöÑÁóÖ‰æãÂæóÂà∞Êõ¥Ê∫ñÁ¢∫ÁöÑËß£Ê±∫„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëËßÄÂØüÂà∞Â∞çÁ†îÁ©∂ÂÖÖÂàÜÁöÑÂü∫Âõ†ÂíåËº∏ÂÖ•È†ÜÂ∫èÊïèÊÑüÊÄßÁöÑÂÅèÂ∑ÆÔºåÈÄôÈòªÁ§ô‰∫ÜÂü∫Âõ†ÂÑ™ÂÖàÁ¥öÊéíÂ∫è„ÄÇÊàëÂÄëÁöÑÂàÜËÄåÊ≤ª‰πãÁ≠ñÁï•ÈÄöÈÅéÂÖãÊúçÈÄô‰∫õÂÅèÂ∑Æ‰æÜÊèêÈ´òÊ∫ñÁ¢∫ÊÄß„ÄÇÈÄöÈÅéÂà©Áî® HPO ÂàÜÈ°û„ÄÅÊñ∞Á©éÁöÑÂ§öÊô∫ËÉΩÈ´îÊäÄË°ìÂíåÊàëÂÄëÁöÑ LLM Á≠ñÁï•ÔºåÊàëÂÄëËàáÊàëÂÄëÁöÑÂü∫Á∑öË©ï‰º∞Áõ∏ÊØîÊèêÈ´ò‰∫ÜËá¥ÁóÖÂü∫Âõ†Ë≠òÂà•Ê∫ñÁ¢∫ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁ∞°Âåñ‰∫ÜÁΩïË¶ãÁñæÁóÖÁöÑË®∫Êñ∑Ôºå‰øÉÈÄ≤‰∫ÜÂ∞çÊú™Ëß£Ê±∫ÁóÖ‰æãÁöÑÈáçÊñ∞ÂàÜÊûêÔºå‰∏¶Âä†ÈÄü‰∫ÜÂü∫Âõ†ÁôºÁèæÔºåÊîØÊåÅ‰∫ÜÈù∂ÂêëË®∫Êñ∑ÂíåÊ≤ªÁôÇÁöÑÈñãÁôº„ÄÇ

##### **Synthetic Data Generation for Augmenting Small Samples**
2501.18741v1 by Dan Liu, Samer El Kababji, Nicholas Mitsakakis, Lisa Pilgram, Thomas Walters, Mark Clemons, Greg Pond, Alaa El-Hussuna, Khaled El Emam

Small datasets are common in health research. However, the generalization
performance of machine learning models is suboptimal when the training datasets
are small. To address this, data augmentation is one solution. Augmentation
increases sample size and is seen as a form of regularization that increases
the diversity of small datasets, leading them to perform better on unseen data.
We found that augmentation improves prognostic performance for datasets that:
have fewer observations, with smaller baseline AUC, have higher cardinality
categorical variables, and have more balanced outcome variables. No specific
generative model consistently outperformed the others. We developed a decision
support model that can be used to inform analysts if augmentation would be
useful. For seven small application datasets, augmenting the existing data
results in an increase in AUC between 4.31% (AUC from 0.71 to 0.75) and 43.23%
(AUC from 0.51 to 0.73), with an average 15.55% relative improvement,
demonstrating the nontrivial impact of augmentation on small datasets
(p=0.0078). Augmentation AUC was higher than resampling only AUC (p=0.016). The
diversity of augmented datasets was higher than the diversity of resampled
datasets (p=0.046).

ÊëòË¶ÅÔºöÂú®ÂÅ•Â∫∑Á†îÁ©∂‰∏≠ÔºåÂ∞èÂûãÊï∞ÊçÆÈõÜÂæàÂ∏∏ËßÅ„ÄÇÁÑ∂ËÄåÔºåÂΩìËÆ≠ÁªÉÊï∞ÊçÆÈõÜËæÉÂ∞èÊó∂ÔºåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩÂπ∂‰∏çÁêÜÊÉ≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊï∞ÊçÆÂ¢ûÂº∫ÊòØ‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°à„ÄÇÂ¢ûÂº∫Â¢ûÂä†‰∫ÜÊ†∑Êú¨ÈáèÔºåÂπ∂Ë¢´ËßÜ‰∏∫‰∏ÄÁßçÊ≠£ÂàôÂåñÂΩ¢ÂºèÔºåÂÆÉÂ¢ûÂä†‰∫ÜÂ∞èÂûãÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄå‰ΩøÂÖ∂Âú®Êú™ËßÅÊï∞ÊçÆ‰∏äË°®Áé∞ÂæóÊõ¥Â•Ω„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂ¢ûÂº∫ÊèêÈ´ò‰∫Ü‰ª•‰∏ãÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÊµãÊÄßËÉΩÔºöÂÖ∑ÊúâËæÉÂ∞ëÁöÑËßÇÊµãÂÄº„ÄÅËæÉÂ∞èÁöÑÂü∫Á∫ø AUC„ÄÅËæÉÈ´òÁöÑÂü∫Êï∞ÂàÜÁ±ªÂèòÈáè‰ª•ÂèäÊõ¥Âπ≥Ë°°ÁöÑÁªìÊûúÂèòÈáè„ÄÇÊ≤°ÊúâÁâπÂÆöÁöÑÁîüÊàêÊ®°ÂûãÂßãÁªà‰ºò‰∫éÂÖ∂‰ªñÊ®°Âûã„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂÜ≥Á≠ñÊîØÊåÅÊ®°ÂûãÔºåÂèØÁî®‰∫éÂëäÁü•ÂàÜÊûêÂ∏àÂ¢ûÂº∫ÊòØÂê¶ÊúâÁî®„ÄÇÂØπ‰∫é‰∏É‰∏™Â∞èÂûãÂ∫îÁî®Á®ãÂ∫èÊï∞ÊçÆÈõÜÔºåÂ¢ûÂº∫Áé∞ÊúâÊï∞ÊçÆÂØºËá¥ AUC Â¢ûÂä† 4.31%ÔºàAUC ‰ªé 0.71 Â¢ûÂä†Âà∞ 0.75ÔºâÂíå 43.23%ÔºàAUC ‰ªé 0.51 Â¢ûÂä†Âà∞ 0.73ÔºâÔºåÂπ≥ÂùáÁõ∏ÂØπÊîπËøõ 15.55%ÔºåËøôË°®Êòé‰∫ÜÂ¢ûÂº∫ÂØπÂ∞èÂûãÊï∞ÊçÆÈõÜÁöÑÈùûÂπ≥Âá°ÂΩ±ÂìçÔºàp=0.0078Ôºâ„ÄÇÂ¢ûÂº∫ AUC È´ò‰∫é‰ªÖÈáçÊñ∞ÈááÊ†∑ÁöÑ AUCÔºàp=0.016Ôºâ„ÄÇÂ¢ûÂº∫Êï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄßÈ´ò‰∫éÈáçÊñ∞ÈááÊ†∑Êï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄßÔºàp=0.046Ôºâ„ÄÇ

##### **A Multi-Layered Large Language Model Framework for Disease Prediction**
2502.00063v1 by Malak Mohamed, Rokaia Emad, Ali Hamdi

Social telehealth has revolutionized healthcare by enabling patients to share
symptoms and receive medical consultations remotely. Users frequently post
symptoms on social media and online health platforms, generating a vast
repository of medical data that can be leveraged for disease classification and
symptom severity assessment. Large language models (LLMs), such as LLAMA3,
GPT-3.5 Turbo, and BERT, process complex medical data to enhance disease
classification. This study explores three Arabic medical text preprocessing
techniques: text summarization, text refinement, and Named Entity Recognition
(NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best
performance was achieved using CAMeL-BERT with NER-augmented text (83% type
classification, 69% severity assessment). Non-fine-tuned models performed
poorly (13%-20% type classification, 40%-49% severity assessment). Integrating
LLMs into social telehealth systems enhances diagnostic accuracy and treatment
outcomes.

ÊëòË¶ÅÔºöÁ§æ‰∫§ÈÅ†Ë∑ùÈÜ´ÁôÇÈÄèÈÅéËÆìÊÇ£ËÄÖÂèØ‰ª•ÈÅ†Ë∑ùÂàÜ‰∫´ÁóáÁãÄ‰∏¶Êé•ÂèóÈÜ´ÁôÇË´ÆË©¢ÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•„ÄÇ‰ΩøÁî®ËÄÖÁ∂ìÂ∏∏Âú®Á§æÁæ§Â™íÈ´îÂíåÁ∑ö‰∏äÂÅ•Â∫∑Âπ≥Âè∞‰∏äÁôºÂ∏ÉÁóáÁãÄÔºåÁî¢Áîü‰∫ÜÈæêÂ§ßÁöÑÈÜ´ÁôÇË≥áÊñôÂ∫´ÔºåÂèØÂà©Áî®ÊñºÁñæÁóÖÂàÜÈ°ûÂíåÁóáÁãÄÂö¥ÈáçÊÄßË©ï‰º∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç LLAMA3„ÄÅGPT-3.5 Turbo Âíå BERTÔºåËôïÁêÜË§áÈõúÁöÑÈÜ´ÁôÇË≥áÊñô‰ª•Â¢ûÂº∑ÁñæÁóÖÂàÜÈ°û„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü‰∏âÁ®ÆÈòøÊãâ‰ºØË™ûÈÜ´ÁôÇÊñáÂ≠óÂâçËôïÁêÜÊäÄË°ìÔºöÊñáÂ≠óÊëòË¶Å„ÄÅÊñáÂ≠óÁ≤æÁÖâÂíåÂëΩÂêçÂØ¶È´îËæ®Ë≠ò (NER)„ÄÇ‰ΩøÁî® LoRA Ë©ï‰º∞ CAMeL-BERT„ÄÅAraBERT Âíå Asafaya-BERTÔºå‰ΩøÁî®ÂÖ∑ NER Â¢ûÂº∑ÊñáÂ≠óÁöÑ CAMeL-BERT Áç≤ÂæóÊúÄ‰Ω≥ÊïàËÉΩÔºà83% È°ûÂûãÂàÜÈ°ûÔºå69% Âö¥ÈáçÊÄßË©ï‰º∞Ôºâ„ÄÇÊú™Á∂ìÈÅéÂæÆË™øÁöÑÊ®°ÂûãÊïàËÉΩ‰∏ç‰Ω≥Ôºà13%-20% È°ûÂûãÂàÜÈ°ûÔºå40%-49% Âö¥ÈáçÊÄßË©ï‰º∞Ôºâ„ÄÇÂ∞á LLM Êï¥ÂêàÂà∞Á§æ‰∫§ÈÅ†Ë∑ùÈÜ´ÁôÇÁ≥ªÁµ±‰∏≠ÂèØÂ¢ûÂº∑Ë®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÊ≤ªÁôÇÁµêÊûú„ÄÇ

##### **A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**
2501.18367v1 by Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li

In medical time series disease diagnosis, two key challenges are
identified.First, the high annotation cost of medical data leads to overfitting
in models trained on label-limited, single-center datasets. To address this, we
propose incorporating external data from related tasks and leveraging AE-GAN to
extract prior knowledge,providing valuable references for downstream tasks.
Second, many existing studies employ contrastive learning to derive more
generalized medical sequence representations for diagnostic tasks, usually
relying on manually designed diverse positive and negative sample
pairs.However, these approaches are complex, lack generalizability, and fail to
adaptively capture disease-specific features across different conditions.To
overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework),
a framework that integrates a multi-head attention mechanism and adaptively
learns representations from different views through inter-view and intra-view
contrastive learning strategies.Additionally, the pre-trained AE-GAN is used to
reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process.Experiments on three
target datasets demonstrate that our method consistently outperforms seven
other baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease.

ÊëòË¶ÅÔºöÂú®ÂåªÁñóÊó∂Èó¥Â∫èÂàóÁñæÁóÖËØäÊñ≠‰∏≠ÔºåÁ°ÆÂÆö‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇÈ¶ñÂÖàÔºåÂåªÁñóÊï∞ÊçÆÁöÑÊ†áÊ≥®ÊàêÊú¨È´òÔºåÂØºËá¥Âú®Ê†áÁ≠æÂèóÈôêÁöÑÂçï‰∏≠ÂøÉÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÂá∫Áé∞ËøáÊãüÂêà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÂêàÂπ∂Êù•Ëá™Áõ∏ÂÖ≥‰ªªÂä°ÁöÑÂ§ñÈÉ®Êï∞ÊçÆÔºåÂπ∂Âà©Áî® AE-GAN ÊèêÂèñÂÖàÈ™åÁü•ËØÜÔºå‰∏∫‰∏ãÊ∏∏‰ªªÂä°Êèê‰æõÊúâ‰ª∑ÂÄºÁöÑÂèÇËÄÉ„ÄÇÂÖ∂Ê¨°ÔºåËÆ∏Â§öÁé∞ÊúâÁöÑÁ†îÁ©∂ÈááÁî®ÂØπÊØîÂ≠¶‰π†Êù•Êé®ÂØºÂá∫Êõ¥ÈÄöÁî®ÁöÑÂåªÁñóÂ∫èÂàóË°®Á§∫ÔºåÁî®‰∫éËØäÊñ≠‰ªªÂä°ÔºåÈÄöÂ∏∏‰æùËµñ‰∫éÊâãÂä®ËÆæËÆ°ÁöÑÂêÑÁßçÊ≠£Ë¥üÊ†∑Êú¨ÂØπ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂ§çÊùÇÔºåÁº∫‰πèÈÄöÁî®ÊÄßÔºåÂπ∂‰∏îÊó†Ê≥ïËá™ÈÄÇÂ∫îÂú∞ÊçïËé∑‰∏çÂêåÊù°‰ª∂‰∏ãÁöÑÁâπÂÆöÁñæÁóÖÁâπÂæÅ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü LMCFÔºàÂèØÂ≠¶‰π†ÁöÑÂ§öËßÜÂõæÂØπÊØîÊ°ÜÊû∂ÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÈõÜÊàê‰∫ÜÂ§öÂ§¥Ê≥®ÊÑèÊú∫Âà∂ÁöÑÊ°ÜÊû∂ÔºåÂπ∂ÈÄöËøáËßÜÂõæÈó¥ÂíåËßÜÂõæÂÜÖÂØπÊØîÂ≠¶‰π†Á≠ñÁï•Ëá™ÈÄÇÂ∫îÂú∞Â≠¶‰π†Êù•Ëá™‰∏çÂêåËßÜÂõæÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÈ¢ÑËÆ≠ÁªÉÁöÑ AE-GAN Áî®‰∫éÈáçÂª∫ÁõÆÊ†áÊï∞ÊçÆ‰∏≠ÁöÑÂ∑ÆÂºÇ‰Ωú‰∏∫ÁñæÁóÖÊ¶ÇÁéáÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ÈõÜÊàêÂà∞ÂØπÊØîÂ≠¶‰π†ËøáÁ®ã‰∏≠„ÄÇÂú®‰∏â‰∏™ÁõÆÊ†áÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂßãÁªà‰ºò‰∫éÂÖ∂‰ªñ‰∏É‰∏™Âü∫Á∫øÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂ÂØπÂåªÁñó‰øùÂÅ•Â∫îÁî®ÔºàÂ¶ÇÂøÉËÇåÊ¢óÂ°û„ÄÅÈòøÂ∞îËå®Êµ∑ÈªòÁóÖÂíåÂ∏ïÈáëÊ£ÆÁóÖÁöÑËØäÊñ≠ÔºâÁöÑÈáçÂ§ßÂΩ±Âìç„ÄÇ

##### **MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**
2501.18362v1 by Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou

We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.

ÊëòË¶ÅÔºöÊàëÂÄëÊé®Âá∫‰∫Ü MedXpertQAÔºåÈÄôÊòØ‰∏ÄÂÄãÊ•µÂÖ∑ÊåëÊà∞ÊÄß‰∏îÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞Â∞àÂÆ∂Á¥öÁöÑÈÜ´Â≠∏Áü•Ë≠òÂíåÂÖàÈÄ≤ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇMedXpertQA ÂåÖÂê´ 4,460 ÂÄãÂïèÈ°åÔºåÊ∂µËìã 17 ÂÄãÂ∞àÁßëÂíå 11 ÂÄãË∫´È´îÁ≥ªÁµ±„ÄÇÂÆÉÂåÖÂê´ÂÖ©ÂÄãÂ≠êÈõÜÔºåÊñáÊú¨Áî®ÊñºÊñáÊú¨Ë©ï‰º∞ÔºåMM Áî®ÊñºÂ§öÊ®°ÂºèË©ï‰º∞„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåMM ÂºïÂÖ•‰∫ÜÂ∞àÂÆ∂Á¥öËÄÉË©¶È°åÁõÆÔºåÂÖ∂‰∏≠ÂåÖÂê´Â§öÊ®£ÂåñÁöÑÂΩ±ÂÉèÂíåË±êÂØåÁöÑËá®Â∫äË≥áË®äÔºåÂåÖÊã¨ÊÇ£ËÄÖË®òÈåÑÂíåÊ™¢Êü•ÁµêÊûúÔºåÈÄôËÆìÂÆÉÊúâÂà•ÊñºÂÇ≥Áµ±ÁöÑÈÜ´Â≠∏Â§öÊ®°ÂºèÂü∫Ê∫ñÔºåÂæåËÄÖÊòØÂæûÂΩ±ÂÉèÊ®ôÈ°å‰∏≠Áî¢ÁîüÁöÑÁ∞°ÂñÆÂïèÁ≠îÂ∞ç„ÄÇMedXpertQA Êé°Áî®Âö¥Ê†ºÁöÑÈÅéÊøæÂíåÊì¥ÂÖÖÔºå‰ª•Ëß£Ê±∫ MedQA Á≠âÁèæÊúâÂü∫Ê∫ñÁöÑÈõ£Â∫¶‰∏çË∂≥ÂïèÈ°åÔºå‰∏¶Á¥çÂÖ•Â∞àÁßëÂßîÂì°ÊúÉÂïèÈ°å‰ª•ÊèêÈ´òËá®Â∫äÁõ∏ÈóúÊÄßÂíåÂÖ®Èù¢ÊÄß„ÄÇÊàëÂÄëÂü∑Ë°åË≥áÊñôÂêàÊàê‰ª•Èôç‰ΩéË≥áÊñôÂ§ñÊ¥©È¢®Èö™Ôºå‰∏¶ÈÄ≤Ë°åÂ§öËº™Â∞àÂÆ∂ÂØ©Êü•‰ª•Á¢∫‰øùÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÂú® MedXpertQA ‰∏äË©ï‰º∞‰∫Ü 16 ÂÄãÈ†òÂÖàÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÈÜ´Â≠∏ËàáÁèæÂØ¶‰∏ñÁïåÁöÑÊ±∫Á≠ñÂà∂ÂÆöÊúâÂØÜÂàáÁöÑËÅØÁπ´ÔºåÊèê‰æõ‰∫ÜË±êÂØå‰∏îÂÖ∑‰ª£Ë°®ÊÄßÁöÑÁí∞Â¢ÉÔºåÁî®ÊñºË©ï‰º∞Ë∂ÖË∂äÊï∏Â≠∏ÂíåÁ®ãÂºèÁ¢ºÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ª•Êé®ÁêÜÁÇ∫Â∞éÂêëÁöÑÂ≠êÈõÜÔºå‰ª•Âà©ÊñºË©ï‰º∞È°û o1 ÁöÑÊ®°Âûã„ÄÇ

##### **CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**
2501.18328v1 by Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai

MRI imputation aims to synthesize the missing modality from one or more
available ones, which is highly desirable since it reduces scanning costs and
delivers comprehensive MRI information to enhance clinical diagnosis. In this
paper, we propose a unified model, CodeBrain, designed to adapt to various
brain MRI imputation scenarios. The core design lies in casting various
inter-modality transformations as a full-modality code prediction task. To this
end, CodeBrain is trained in two stages: Reconstruction and Code Prediction.
First, in the Reconstruction stage, we reconstruct each MRI modality, which is
mapped into a shared latent space followed by a scalar quantization. Since such
quantization is lossy and the code is low dimensional, another MRI modality
belonging to the same subject is randomly selected to generate common features
to supplement the code and boost the target reconstruction. In the second
stage, we train another encoder by a customized grading loss to predict the
full-modality codes from randomly masked MRI samples, supervised by the
corresponding quantized codes generated from the first stage. In this way, the
inter-modality transformation is achieved by mapping the instance-specific
codes in a finite scalar space. We evaluated the proposed CodeBrain model on
two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments
demonstrate that our CodeBrain model achieves superior imputation performance
compared to four existing methods, establishing a new state of the art for
unified brain MRI imputation. Codes will be released.

ÊëòË¶ÅÔºöMRI Ë£úÂÆåÊó®Âú®Âæû‰∏ÄÂÄãÊàñÂ§öÂÄãÂèØÁî®ÊñπÂºè‰∏≠ÂêàÊàêÈÅ∫Â§±ÁöÑÊ®°ÊÖãÔºåÈÄôÊòØÈùûÂ∏∏ÁêÜÊÉ≥ÁöÑÔºåÂõ†ÁÇ∫ÂÆÉÈôç‰Ωé‰∫ÜÊéÉÊèèÊàêÊú¨Ôºå‰∏¶Êèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑ MRI Ë≥áË®ä‰ª•Â¢ûÂº∑Ëá®Â∫äË®∫Êñ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁµ±‰∏ÄÊ®°Âûã CodeBrainÔºåÊó®Âú®ÈÅ©ÊáâÂêÑÁ®ÆËÖ¶ÈÉ® MRI Ë£úÂÆåÂ†¥ÊôØ„ÄÇÊ†∏ÂøÉË®≠Ë®àÂú®ÊñºÂ∞áÂêÑÁ®ÆÊ®°ÊÖãÈñìËΩâÊèõËΩâÊèõÁÇ∫ÂÖ®Ê®°ÊÖãÁ¢ºÈ†êÊ∏¨‰ªªÂãô„ÄÇÁÇ∫Ê≠§ÔºåCodeBrain ÂàÜÂÖ©ÂÄãÈöéÊÆµÈÄ≤Ë°åË®ìÁ∑¥ÔºöÈáçÂª∫ÂíåÁ¢ºÈ†êÊ∏¨„ÄÇÈ¶ñÂÖàÔºåÂú®ÈáçÂª∫ÈöéÊÆµÔºåÊàëÂÄëÈáçÂª∫ÊØèÂÄã MRI Ê®°ÊÖãÔºåÂÆÉË¢´Êò†Â∞ÑÂà∞‰∏ÄÂÄãÂÖ±‰∫´ÊΩõÂú®Á©∫ÈñìÔºåÁÑ∂ÂæåÈÄ≤Ë°åÊ®ôÈáèÈáèÂåñ„ÄÇÁî±ÊñºÈÄôÁ®ÆÈáèÂåñÊòØÊúâÊêçÁöÑÔºå‰∏¶‰∏îÁ¢ºÁöÑÁ∂≠Â∫¶Âæà‰ΩéÔºåÂõ†Ê≠§Èö®Ê©üÈÅ∏ÊìáÂ±¨ÊñºÂêå‰∏ÄÂÄãÂèóË©¶ËÄÖÁöÑÂè¶‰∏ÄÂÄã MRI Ê®°ÊÖã‰æÜÁî¢ÁîüÂÖ±ÂêåÁâπÂæµ‰ª•Ë£úÂÖÖÁ¢º‰∏¶ÊèêÂçáÁõÆÊ®ôÈáçÂª∫„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊàëÂÄëÈÄöÈÅéËá™Ë®ÇÂàÜÁ¥öÊêçÂ§±Ë®ìÁ∑¥Âè¶‰∏ÄÂÄãÁ∑®Á¢ºÂô®ÔºåÂæûÈö®Ê©üÈÅÆÁΩ©ÁöÑ MRI Ê®£Êú¨È†êÊ∏¨ÂÖ®Ê®°ÊÖãÁ¢ºÔºå‰∏¶Áî±Á¨¨‰∏ÄÈöéÊÆµÁî¢ÁîüÁöÑÂ∞çÊáâÈáèÂåñÁ¢ºÈÄ≤Ë°åÁõ£Áù£„ÄÇÈÄôÊ®£ÔºåÊ®°ÊÖãÈñìËΩâÊèõÊòØÈÄöÈÅéÂ∞áÁâπÂÆöÊñº‰æãÈ†ÖÁöÑÁ¢ºÊò†Â∞ÑÂà∞‰∏ÄÂÄãÊúâÈôêÁöÑÊ®ôÈáèÁ©∫Èñì‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÂÖ¨ÈñãÁöÑËÖ¶ÈÉ® MRI Ë≥áÊñôÈõÜÔºàÂç≥ IXI Âíå BraTS 2023Ôºâ‰∏äË©ï‰º∞‰∫ÜÊâÄÊèêÂá∫ÁöÑ CodeBrain Ê®°Âûã„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòéÔºåËàáÂõõÁ®ÆÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑ CodeBrain Ê®°ÂûãÂØ¶Áèæ‰∫ÜÂÑ™Áï∞ÁöÑË£úÂÆåÊïàËÉΩÔºåÁÇ∫Áµ±‰∏ÄÁöÑËÖ¶ÈÉ® MRI Ë£úÂÆåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊäÄË°ìÊ∞¥Ê∫ñ„ÄÇÁ¢ºÂ∞áÊúÉÈáãÂá∫„ÄÇ

##### **A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**
2501.18294v1 by Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki

Lung cancer is a major issue in worldwide public health, requiring early
diagnosis using stable techniques. This work begins a thorough investigation of
the use of machine learning (ML) methods for precise classification of lung
cancer stages. A cautious analysis is performed to overcome overfitting issues
in model performance, taking into account minimum child weight and learning
rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM,
Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF),
CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted.
Furthermore, the correlation between features and targets is examined using the
deep neural network (DNN) model and thus their capability in detecting complex
patternsis established. It is argued that several ML models can be capable of
classifying lung cancer stages with great accuracy. In spite of the complexity
of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic
Regression excel with superior performance. The models perform better than the
others in lung cancer prediction on the complete set of comparative metrics
like accuracy, precision, recall, and F-1 score

ÊëòË¶ÅÔºöËÇ∫ÁôåÊòØÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÁöÑ‰∏ÄÂ§ßÂïèÈ°åÔºåÈúÄË¶Å‰ΩøÁî®Á©©ÂÆöÁöÑÊäÄË°ìÈÄ≤Ë°åÊó©ÊúüË®∫Êñ∑„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈñãÂßãÂæπÂ∫ïË™øÊü•‰ΩøÁî®Ê©üÂô®Â≠∏Áøí (ML) ÊñπÊ≥ïÁ≤æÁ¢∫ÂàÜÈ°ûËÇ∫ÁôåÂàÜÊúüÁöÑ‰ΩøÁî®ÊÉÖÊ≥Å„ÄÇÂü∑Ë°åË¨πÊÖéÁöÑÂàÜÊûê‰ª•ÂÖãÊúçÊ®°ÂûãÊïàËÉΩ‰∏≠ÁöÑÈÅéÂ∫¶Êì¨ÂêàÂïèÈ°åÔºå‰∏¶ËÄÉÊÖÆÊúÄÂ∞èÂ≠êÊ¨äÈáçÂíåÂ≠∏ÁøíÁéá„ÄÇ‰∏ÄÁµÑÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÔºåÂåÖÊã¨ XGBoost (XGB)„ÄÅLGBM„ÄÅAdaboost„ÄÅÈÇèËºØËø¥Ê≠∏ (LR)„ÄÅÊ±∫Á≠ñÊ®π (DT)„ÄÅÈö®Ê©üÊ£ÆÊûó (RF)„ÄÅCatBoost Âíå k ÊúÄËøëÈÑ∞ (k-NN)Ôºå‰ª•ÊúâÊ¢ùÁêÜÁöÑÊñπÂºèÂü∑Ë°å‰∏¶ÈÄ≤Ë°åÂ∞çÊØî„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN) Ê®°ÂûãÊ™¢Êü•ÁâπÂæµÂíåÁõÆÊ®ô‰πãÈñìÁöÑÈóúËÅØÊÄßÔºåÂæûËÄåÂª∫Á´ãÂÆÉÂÄëÂú®Ê™¢Ê∏¨Ë§áÈõúÊ®°Âºè‰∏≠ÁöÑËÉΩÂäõ„ÄÇÊúâ‰∫∫Ë™çÁÇ∫ÔºåÂ§öÂÄã ML Ê®°ÂûãËÉΩÂ§†‰ª•ÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶Â∞çËÇ∫ÁôåÂàÜÊúüÈÄ≤Ë°åÂàÜÈ°û„ÄÇÂÑòÁÆ° DNN Êû∂ÊßãÂæàË§áÈõúÔºå‰ΩÜÂÇ≥Áµ± ML Ê®°ÂûãÔºàÂ¶Ç XGBoost„ÄÅLGBM ÂíåÈÇèËºØËø¥Ê≠∏ÔºâË°®ÁèæÂá∫Ëâ≤ÔºåÊïàËÉΩÂÑ™Áï∞„ÄÇÈÄô‰∫õÊ®°ÂûãÂú®ËÇ∫ÁôåÈ†êÊ∏¨‰∏≠Ë°®ÁèæÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåÂú®Ê∫ñÁ¢∫Â∫¶„ÄÅÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F-1 ÂàÜÊï∏Á≠âÂÆåÊï¥ÁöÑÊØîËºÉÊåáÊ®ô‰∏≠Ë°®ÁèæÂá∫Ëâ≤„ÄÇ

##### **The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**
2501.18270v1 by Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, S√©raphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szab√≥, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia

Artificial intelligence has significantly advanced skin cancer diagnosis by
enabling rapid and accurate detection of malignant lesions. In this domain,
most publicly available image datasets consist of single, isolated skin lesions
positioned at the center of the image. While these lesion-centric datasets have
been fundamental for developing diagnostic algorithms, they lack the context of
the surrounding skin, which is critical for improving lesion detection. The
iToBoS dataset was created to address this challenge. It includes 16,954 images
of skin regions from 100 participants, captured using 3D total body
photography. Each image roughly corresponds to a $7 \times 9$ cm section of
skin with all suspicious lesions annotated using bounding boxes. Additionally,
the dataset provides metadata such as anatomical location, age group, and sun
damage score for each image. This dataset aims to facilitate training and
benchmarking of algorithms, with the goal of enabling early detection of skin
cancer and deployment of this technology in non-clinical environments.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÈÄèÈÅéÂø´ÈÄü‰∏îÊ∫ñÁ¢∫ÂÅµÊ∏¨ÊÉ°ÊÄßÁóÖÁÅ∂ÔºåÂ§ßÂπÖÊèêÂçáÁöÆËÜöÁôåÁöÑË®∫Êñ∑„ÄÇÂú®ÈÄôÂÄãÈ†òÂüü‰∏≠ÔºåÂ§ßÂ§öÊï∏ÂÖ¨ÈñãÁöÑÂΩ±ÂÉèË≥áÊñôÈõÜÈÉΩÂåÖÂê´ÂñÆ‰∏Ä„ÄÅÂ≠§Á´ãÁöÑÁöÆËÜöÁóÖÁÅ∂ÔºåÁΩÆÊñºÂΩ±ÂÉèÁöÑ‰∏≠Â§Æ„ÄÇÂÑòÁÆ°ÈÄô‰∫õ‰ª•ÁóÖÁÅ∂ÁÇ∫‰∏≠ÂøÉÁöÑË≥áÊñôÈõÜÂ∞çÊñºÈñãÁôºË®∫Êñ∑ÊºîÁÆóÊ≥ïËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂÆÉÂÄëÂçªÁº∫‰πèÂë®ÂúçÁöÆËÜöÁöÑËÉåÊôØÔºåÈÄôÂ∞çÊñºÊîπÂñÑÁóÖÁÅ∂ÂÅµÊ∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇiToBoS Ë≥áÊñôÈõÜÁöÑÂª∫Á´ãÂ∞±ÊòØÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞„ÄÇÂÆÉÂåÖÂê´ 100 ‰ΩçÂèÉËàáËÄÖÁöÑ 16,954 ÂºµÁöÆËÜöÂçÄÂüüÂΩ±ÂÉèÔºå‰ΩøÁî® 3D ÂÖ®Ë∫´ÊîùÂΩ±ÊäÄË°ìÊì∑Âèñ„ÄÇÊØèÂºµÂΩ±ÂÉèÂ§ßËá¥Â∞çÊáâÊñº $7 \times 9$ ÂÖ¨ÂàÜÁöÑÁöÆËÜöÂçÄÂüüÔºåÊâÄÊúâÂèØÁñëÁóÖÁÅ∂ÈÉΩ‰ΩøÁî®ÈÇäÁïåÊ°ÜÊ®ôË®ª„ÄÇÊ≠§Â§ñÔºåË©≤Ë≥áÊñôÈõÜÈÇÑÊèê‰æõÊØèÂºµÂΩ±ÂÉèÁöÑÂÖÉË≥áÊñôÔºå‰æãÂ¶ÇËß£Ââñ‰ΩçÁΩÆ„ÄÅÂπ¥ÈΩ°ÁµÑÂíåÊó•Êõ¨ÊêçÂÇ∑Ë©ïÂàÜ„ÄÇÊ≠§Ë≥áÊñôÈõÜÊó®Âú®‰øÉÈÄ≤ÊºîÁÆóÊ≥ïÁöÑË®ìÁ∑¥ÂíåÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÁõÆÊ®ôÊòØÂØ¶ÁèæÁöÆËÜöÁôåÁöÑÊó©ÊúüÂÅµÊ∏¨Ôºå‰∏¶Â∞áÊ≠§ÊäÄË°ìÈÉ®ÁΩ≤Âú®ÈùûËá®Â∫äÁí∞Â¢É‰∏≠„ÄÇ

##### **Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**
2501.18237v1 by Malte T√∂lle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt

A patient undergoes multiple examinations in each hospital stay, where each
provides different facets of the health status. These assessments include
temporal data with varying sampling rates, discrete single-point measurements,
therapeutic interventions such as medication administration, and images. While
physicians are able to process and integrate diverse modalities intuitively,
neural networks need specific modeling for each modality complicating the
training procedure. We demonstrate that this complexity can be significantly
reduced by visualizing all information as images along with unstructured text
and subsequently training a conventional vision-text transformer. Our approach,
Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not
only simplifies data preprocessing and modeling but also outperforms current
state-of-the-art methods in predicting in-hospital mortality and phenotyping,
as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities
include patient's clinical measurements, medications, X-ray images, and
electrocardiography scans. We hope our work inspires advancements in
multi-modal medical AI by reducing the training complexity to (visual) prompt
engineering, thus lowering entry barriers and enabling no-code solutions for
training. The source code will be made publicly available.

ÊëòË¶ÅÔºöÂú®ÊØèÊ¨°‰ΩèÈô¢ÊúüÈñìÔºåÊÇ£ËÄÖÊúÉÊé•ÂèóÂ§öÈ†ÖÊ™¢Êü•ÔºåÊØè‰∏ÄÈ†ÖÊ™¢Êü•ÈÉΩËÉΩÊèê‰æõÂÅ•Â∫∑ÁãÄÊÖãÁöÑ‰∏çÂêåÈù¢Âêë„ÄÇÈÄô‰∫õË©ï‰º∞ÂåÖÊã¨ÂÖ∑Êúâ‰∏çÂêåÂèñÊ®£ÁéáÁöÑÊôÇÈñìË≥áÊñô„ÄÅÈõ¢Êï£ÂñÆÈªûÊ∏¨ÈáèÂÄº„ÄÅÊ≤ªÁôÇ‰ªãÂÖ•ÔºàÂ¶ÇËó•Áâ©ÁÆ°ÁêÜÔºâÂíåÂΩ±ÂÉè„ÄÇÈõñÁÑ∂ÈÜ´ÁîüËÉΩÂ§†Áõ¥ËßÄÂú∞ËôïÁêÜÂíåÊï¥Âêà‰∏çÂêåÁöÑÊ®°ÂºèÔºå‰ΩÜÁ•ûÁ∂ìÁ∂≤Ë∑ØÈúÄË¶ÅÈáùÂ∞çÊØèÁ®ÆÊ®°ÂºèÈÄ≤Ë°åÁâπÂÆöÁöÑÂª∫Ê®°ÔºåÈÄô‰ΩøÂæóË®ìÁ∑¥Á®ãÂ∫èËÆäÂæóË§áÈõú„ÄÇÊàëÂÄëË≠âÊòéÔºåÈÄöÈÅéÂ∞áÊâÄÊúâË≥áË®äË¶ñË¶∫ÂåñÁÇ∫ÂΩ±ÂÉèÔºå‰∏¶ÁµêÂêàÈùûÁµêÊßãÂåñÊñáÂ≠óÔºåÈö®ÂæåË®ìÁ∑¥‰∏ÄÂÄãÂÇ≥Áµ±ÁöÑË¶ñË¶∫ÊñáÂ≠óËΩâÊèõÂô®ÔºåÂèØ‰ª•Â§ßÂπÖÈôç‰ΩéÈÄôÁ®ÆË§áÈõúÊÄß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂç≥Áî®Êñº‰∏çË¶èÂâáÊé°Ê®£Â§öÊ®°ÂºèÊ∏¨ÈáèÁöÑË¶ñË¶∫ËΩâÊèõÂô® (ViTiMM)Ôºå‰∏çÂÉÖÁ∞°Âåñ‰∫ÜË≥áÊñôÈ†êËôïÁêÜÂíåÂª∫Ê®°ÔºåËÄå‰∏îÂú®È†êÊ∏¨Èô¢ÂÖßÊ≠ª‰∫°ÁéáÂíåË°®ÂûãÊñπÈù¢‰πüÂÑ™ÊñºÁõÆÂâçÁöÑÊúÄÊñ∞ÊñπÊ≥ïÔºåÈÄôÊòØÊ†πÊìö MIMIC-IV Ë≥áÊñôÈõÜ‰∏≠ÁöÑ 6,175 ÂêçÊÇ£ËÄÖË©ï‰º∞ÁöÑ„ÄÇÈÄô‰∫õÊ®°ÂºèÂåÖÊã¨ÊÇ£ËÄÖÁöÑËá®Â∫äÊ∏¨ÈáèÂÄº„ÄÅËó•Áâ©„ÄÅX ÂÖâÂΩ±ÂÉèÂíåÂøÉÈõªÂúñÊéÉÊèè„ÄÇÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÂ∑•‰ΩúËÉΩÈÄèÈÅéÈôç‰ΩéË®ìÁ∑¥Ë§áÈõúÂ∫¶Âà∞ÔºàË¶ñË¶∫ÔºâÊèêÁ§∫Â∑•Á®ãÔºåÂæûËÄåÈôç‰ΩéÈÄ≤ÂÖ•ÈñÄÊ™ªÔºå‰∏¶ÁÇ∫Ë®ìÁ∑¥ÂïüÁî®ÁÑ°Á®ãÂºèÁ¢ºËß£Ê±∫ÊñπÊ°àÔºåÈÄ≤ËÄåÊøÄÂãµÂ§öÊ®°ÂºèÈÜ´ÁôÇ AI ÁöÑÈÄ≤Ê≠•„ÄÇÂéüÂßãÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**
2501.18108v1 by Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino

Despite the growing potential of older adult care technologies, the adoption
of these technologies remains challenging. In this work, we conducted a
focus-group session with family caregivers to scope designs of the older adult
care technology. We then developed a high-fidelity prototype and conducted its
qualitative study with professional caregivers and older adults to understand
their perspectives on the system functionalities. This system monitors abnormal
activity patterns of older adults using wireless motion sensors and machine
learning models and supports interactive dialogue responses to explain abnormal
activity patterns of older adults to caregivers and allow older adults
proactively sharing their status with caregivers for an adequate intervention.
Both older adults and professional caregivers appreciated that our system can
provide a faster, personalized service while proactively controlling what
information is to be shared through interactive dialogue responses. We further
discuss other considerations to realize older adult technology in practice.

ÊëòË¶ÅÔºöÂÑòÁÆ°ËÄÅÂπ¥‰∫∫ÁÖßË≠∑ÊäÄË°ìÁöÑÊΩõÂäõÊó•ÁõäÂ¢ûÈï∑Ôºå‰ΩÜÊé°Áî®ÈÄô‰∫õÊäÄË°ì‰ªçÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëËàáÂÆ∂Â∫≠ÁÖßË≠∑ËÄÖÈÄ≤Ë°åÁÑ¶ÈªûÂ∞èÁµÑÊúÉË≠∞Ôºå‰ª•ÁïåÂÆöËÄÅÂπ¥‰∫∫ÁÖßË≠∑ÊäÄË°ìÁöÑË®≠Ë®àÁØÑÂúç„ÄÇÊé•ËëóÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÈ´ò‰øùÁúüÂéüÂûãÔºå‰∏¶ËàáÂ∞àÊ•≠ÁÖßË≠∑ËÄÖÂíåËÄÅÂπ¥‰∫∫ÈÄ≤Ë°åË≥™ÊÄßÁ†îÁ©∂Ôºå‰ª•‰∫ÜËß£‰ªñÂÄëÂ∞çÁ≥ªÁµ±ÂäüËÉΩÁöÑËßÄÈªû„ÄÇÊ≠§Á≥ªÁµ±‰ΩøÁî®ÁÑ°Á∑öÂãï‰ΩúÊÑüÊ∏¨Âô®ÂíåÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁõ£ÊéßËÄÅÂπ¥‰∫∫ÁöÑÁï∞Â∏∏Ê¥ªÂãïÊ®°ÂºèÔºå‰∏¶ÊîØÊè¥‰∫íÂãïÂºèÂ∞çË©±ÂõûÊáâÔºåÂêëÁÖßË≠∑ËÄÖËß£ÈáãËÄÅÂπ¥‰∫∫ÁöÑÁï∞Â∏∏Ê¥ªÂãïÊ®°ÂºèÔºå‰∏¶ËÆìËÄÅÂπ¥‰∫∫‰∏ªÂãïËàáÁÖßË≠∑ËÄÖÂàÜ‰∫´‰ªñÂÄëÁöÑÁãÄÊÖãÔºå‰ª•ÈÄ≤Ë°åÈÅ©Áï∂ÁöÑ‰ªãÂÖ•„ÄÇËÄÅÂπ¥‰∫∫ÂíåÂ∞àÊ•≠ÁÖßË≠∑ËÄÖÈÉΩËÆöË≥ûÊàëÂÄëÁöÑÁ≥ªÁµ±ËÉΩÊèê‰æõÊõ¥Âø´ÈÄü„ÄÅÂÄã‰∫∫ÂåñÁöÑÊúçÂãôÔºåÂêåÊôÇÈÄèÈÅé‰∫íÂãïÂºèÂ∞çË©±ÂõûÊáâ‰∏ªÂãïÊéßÂà∂Ë¶ÅÂàÜ‰∫´Âì™‰∫õË≥áË®ä„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë®éË´ñÂÖ∂‰ªñËÄÉÈáèÂõ†Á¥†Ôºå‰ª•Âú®ÂØ¶Âãô‰∏≠ÂØ¶ÁèæËÄÅÂπ¥‰∫∫ÊäÄË°ì„ÄÇ

##### **Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**
2501.18081v1 by Pratik S. Sachdeva, Tom van Nuenen

The rapid adoption of large language models (LLMs) has spurred extensive
research into their encoded moral norms and decision-making processes. Much of
this research relies on prompting LLMs with survey-style questions to assess
how well models are aligned with certain demographic groups, moral beliefs, or
political ideologies. While informative, the adherence of these approaches to
relatively superficial constructs tends to oversimplify the complexity and
nuance underlying everyday moral dilemmas. We argue that auditing LLMs along
more detailed axes of human interaction is of paramount importance to better
assess the degree to which they may impact human beliefs and actions. To this
end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am
I the Asshole" (AITA) community on Reddit, where users seek moral judgments on
everyday conflicts from other community members. We prompted seven LLMs to
assign blame and provide explanations for over 10,000 AITA moral dilemmas. We
then compared the LLMs' judgments and explanations to those of Redditors and to
each other, aiming to uncover patterns in their moral reasoning. Our results
demonstrate that large language models exhibit distinct patterns of moral
judgment, varying substantially from human evaluations on the AITA subreddit.
LLMs demonstrate moderate to high self-consistency but low inter-model
agreement. Further analysis of model explanations reveals distinct patterns in
how models invoke various moral principles. These findings highlight the
complexity of implementing consistent moral reasoning in artificial systems and
the need for careful evaluation of how different models approach ethical
judgment. As LLMs continue to be used in roles requiring ethical
decision-making such as therapists and companions, careful evaluation is
crucial to mitigate potential biases and limitations.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÊé°Áî®Â∑≤‰øÉ‰Ωø‰∫∫ÂÄëÊ∑±ÂÖ•Á†îÁ©∂ÂÖ∂Á∑®Á¢ºÁöÑÈÅìÂæ∑Ë¶èÁØÑÂíåÊ±∫Á≠ñÈÅéÁ®ã„ÄÇË®±Â§öÈÄôÈ°ûÁ†îÁ©∂‰æùË≥¥Êñº‰ª•Ë™øÊü•ÂºèÂïèÈ°åÊèêÁ§∫ LLMÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãËàáÁâπÂÆö‰∫∫Âè£Áæ§È´î„ÄÅÈÅìÂæ∑‰ø°ÂøµÊàñÊîøÊ≤ªÊÑèË≠òÂΩ¢ÊÖãÁöÑÂ•ëÂêàÁ®ãÂ∫¶„ÄÇÂÑòÁÆ°ÊúâÊèê‰æõË≥áË®äÔºå‰ΩÜÈÄô‰∫õÊñπÊ≥ïÂ∞çÁõ∏Â∞çËÜöÊ∑∫ÁöÑÁµêÊßãÁöÑÂ†ÖÊåÅÂÇæÂêëÊñºÈÅéÂ∫¶Á∞°ÂåñÊó•Â∏∏ÈÅìÂæ∑Âõ∞Â¢ÉËÉåÂæåÁöÑË§áÈõúÊÄßÂíåÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÊ≤øËëóÊõ¥Ë©≥Á¥∞ÁöÑ‰∫∫È°û‰∫íÂãïËª∏Á∑öÂØ©Êü• LLM Â∞çÊñºÊõ¥Â•ΩÂú∞Ë©ï‰º∞ÂÆÉÂÄëÂèØËÉΩÂΩ±Èüø‰∫∫È°û‰ø°ÂøµÂíåË°åÁÇ∫ÁöÑÁ®ãÂ∫¶Ëá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊ†πÊìö Reddit ‰∏ä„ÄåÊàëÊòØÊ∑∑ËõãÂóé„Äç(AITA) Á§æÁæ§Ë©ï‰º∞ LLM Âú®Ë§áÈõúÁöÑÊó•Â∏∏ÈÅìÂæ∑Âõ∞Â¢É‰∏≠Ôºå‰ΩøÁî®ËÄÖÂú®ÂÖ∂‰∏≠Â∞ãÊ±ÇÂÖ∂‰ªñÁ§æÁæ§ÊàêÂì°Â∞çÊó•Â∏∏Ë°ùÁ™ÅÁöÑÈÅìÂæ∑Âà§Êñ∑„ÄÇÊàëÂÄëÊèêÁ§∫‰∏ÉÂÄã LLM Â∞çË∂ÖÈÅé 10,000 ÂÄã AITA ÈÅìÂæ∑Âõ∞Â¢ÉÂàÜÈÖçË≤¨‰ªª‰∏¶Êèê‰æõËß£Èáã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂ∞á LLM ÁöÑÂà§Êñ∑ÂíåËß£ÈáãËàá Reddit ‰ΩøÁî®ËÄÖÁöÑÂà§Êñ∑ÂíåËß£Èáã‰ª•ÂèäÂΩºÊ≠§ÈÄ≤Ë°åÊØîËºÉÔºåÊó®Âú®Êè≠Á§∫ÂÖ∂ÈÅìÂæ∑Êé®ÁêÜ‰∏≠ÁöÑÊ®°Âºè„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ±ïÁèæÂá∫‰∏çÂêåÁöÑÈÅìÂæ∑Âà§Êñ∑Ê®°ÂºèÔºåËàá AITA Â≠êÁâàÂ°ä‰∏äÁöÑ‰∫∫È°ûË©ï‰º∞ÊúâÂæàÂ§ßÂ∑ÆÁï∞„ÄÇLLM Ë°®ÁèæÂá∫‰∏≠Â∫¶Âà∞È´òÂ∫¶ÁöÑËá™Êàë‰∏ÄËá¥ÊÄßÔºå‰ΩÜÊ®°ÂûãÈñìÂçîË≠∞‰Ωé„ÄÇÈÄ≤‰∏ÄÊ≠•ÂàÜÊûêÊ®°ÂûãËß£ÈáãÊè≠Á§∫‰∫ÜÊ®°ÂûãÂ¶Ç‰ΩïÊè¥ÂºïÂêÑÁ®ÆÈÅìÂæ∑ÂéüÂâáÁöÑ‰∏çÂêåÊ®°Âºè„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÂú®‰∫∫Â∑•Á≥ªÁµ±‰∏≠ÂØ¶ÊñΩ‰∏ÄËá¥ÁöÑÈÅìÂæ∑Êé®ÁêÜÁöÑË§áÈõúÊÄßÔºå‰ª•Âèä‰ªîÁ¥∞Ë©ï‰º∞‰∏çÂêåÊ®°ÂûãÂ¶Ç‰ΩïÈÄ≤Ë°åÈÅìÂæ∑Âà§Êñ∑ÁöÑÂøÖË¶ÅÊÄß„ÄÇÈö®Ëëó LLM ÊåÅÁ∫åÁî®ÊñºÈúÄË¶ÅÈÅìÂæ∑Ê±∫Á≠ñÁöÑËßíËâ≤Ôºå‰æãÂ¶ÇÊ≤ªÁôÇÂ∏´Âíå‰º¥‰æ∂Ôºå‰ªîÁ¥∞Ë©ï‰º∞Â∞çÊñºÊ∏õËºïÊΩõÂú®ÂÅèË¶ãÂíåÈôêÂà∂Ëá≥ÈóúÈáçË¶Å„ÄÇ

##### **Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**
2501.18071v1 by Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino

Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖ (DM) ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑÂÖ®ÁêÉÂÅ•Â∫∑Ë≠∞È°åÔºåÂøÖÈ†àÁõ°Êó©Ë®∫Êñ∑‰∏¶Â¶•ÂñÑÁÆ°ÁêÜ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÁ≥ñÂ∞øÁóÖÈ†êÊ∏¨Êû∂ÊßãÔºå‰ΩøÁî®Ê©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÔºå‰∏¶Êê≠ÈÖçÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑•ÂÖ∑Ôºå‰æÜÊé¢Ë®é ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇË≥áÊñôÂâçËôïÁêÜÂü∫ÊñºÂêàÊàêÂ∞ëÊï∏ÈÅéÊé°Ê®£ÊäÄË°ì (SMOTE) ÂíåÁâπÂæµÁ∏ÆÊîæÔºåÁî®ÊñºÁ≥ñÂ∞øÁóÖ‰∫åÂÖÉÂÅ•Â∫∑ÊåáÊ®ôË≥áÊñôÈõÜÔºå‰ª•ËôïÁêÜÈ°ûÂà•‰∏çÂπ≥Ë°°ÂíåËá®Â∫äÁâπÂæµÁöÑÂèØËÆäÊÄß„ÄÇÊï¥ÂêàÊ®°ÂûãÊèê‰æõ‰∫ÜÈ´òÊ∫ñÁ¢∫Â∫¶ÔºåÊ∏¨Ë©¶Ê∫ñÁ¢∫Â∫¶ÁÇ∫ 92.50%ÔºåROC-AUC ÁÇ∫ 0.975„ÄÇÊ†πÊìöÊ®°ÂûãËß£ÈáãÔºåBMI„ÄÅÂπ¥ÈΩ°„ÄÅ‰∏ÄËà¨ÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÅÊî∂ÂÖ•ÂíåË∫´È´îÊ¥ªÂãïÊòØÊúÄÂÖ∑ÂΩ±ÈüøÂäõÁöÑÈ†êÊ∏¨Âõ†Â≠ê„ÄÇÊú¨Á†îÁ©∂ÁµêÊûúË°®ÊòéÔºåML ÁµêÂêà XAI ÊòØ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÂºèÔºåÂèØ‰ª•ÈñãÁôºÂá∫Ê∫ñÁ¢∫‰∏îÂú®ÈÅãÁÆó‰∏äÈÄèÊòéÁöÑÂ∑•ÂÖ∑ÔºåÁî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±„ÄÇ

##### **Current Pathology Foundation Models are unrobust to Medical Center Differences**
2501.18055v2 by Edwin D. de Jong, Eric Marcus, Jonas Teuwen

Pathology Foundation Models (FMs) hold great promise for healthcare. Before
they can be used in clinical practice, it is essential to ensure they are
robust to variations between medical centers. We measure whether pathology FMs
focus on biological features like tissue and cancer type, or on the well known
confounding medical center signatures introduced by staining procedure and
other differences. We introduce the Robustness Index. This novel robustness
metric reflects to what degree biological features dominate confounding
features. Ten current publicly available pathology FMs are evaluated. We find
that all current pathology foundation models evaluated represent the medical
center to a strong degree. Significant differences in the robustness index are
observed. Only one model so far has a robustness index greater than one,
meaning biological features dominate confounding features, but only slightly. A
quantitative approach to measure the influence of medical center differences on
FM-based prediction performance is described. We analyze the impact of
unrobustness on classification performance of downstream models, and find that
cancer-type classification errors are not random, but specifically attributable
to same-center confounders: images of other classes from the same medical
center. We visualize FM embedding spaces, and find these are more strongly
organized by medical centers than by biological factors. As a consequence, the
medical center of origin is predicted more accurately than the tissue source
and cancer type. The robustness index introduced here is provided with the aim
of advancing progress towards clinical adoption of robust and reliable
pathology FMs.

ÊëòË¶ÅÔºöÁóÖÁêÜÂü∫Á§éÊ®°Âûã (FM) Â∞çÈÜ´ÁôÇ‰øùÂÅ•ËÄåË®ÄÊ•µÂÖ∑ÊΩõÂäõ„ÄÇÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠‰ΩøÁî®‰πãÂâçÔºåÂøÖÈ†àÁ¢∫‰øùÂÆÉÂÄëËÉΩÊáâÂ∞çÈÜ´ÁôÇ‰∏≠ÂøÉ‰πãÈñìÁöÑÂ∑ÆÁï∞„ÄÇÊàëÂÄëË°°ÈáèÁóÖÁêÜ FM ÊòØÂê¶ËëóÈáçÊñºÁµÑÁπîÂíåÁôåÁóáÈ°ûÂûãÁ≠âÁîüÁâ©ÁâπÂæµÔºåÊàñËëóÈáçÊñºÊüìËâ≤Á®ãÂ∫èÂíåÂÖ∂‰ªñÂ∑ÆÁï∞ÊâÄÂ∞éËá¥ÁöÑÁúæÊâÄÂë®Áü•Ê∑∑Ê∑ÜÈÜ´ÁôÇ‰∏≠ÂøÉÁâπÂæµ„ÄÇÊàëÂÄëÂºïÈÄ≤‰∫ÜÁ©©ÂÅ•ÊÄßÊåáÊ®ô„ÄÇÈÄôÈ†ÖÊñ∞Á©éÁöÑÁ©©ÂÅ•ÊÄßÊåáÊ®ôÂèçÊò†‰∫ÜÁîüÁâ©ÁâπÂæµ‰∏ªÂ∞éÊ∑∑Ê∑ÜÁâπÂæµÁöÑÁ®ãÂ∫¶„ÄÇË©ï‰º∞‰∫ÜÂçÅÈ†ÖÁõÆÂâçÂÖ¨ÈñãÊèê‰æõÁöÑÁóÖÁêÜ FM„ÄÇÊàëÂÄëÁôºÁèæÔºåÊâÄÊúâÁõÆÂâçË©ï‰º∞ÁöÑÁóÖÁêÜÂü∫Á§éÊ®°ÂûãÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ª£Ë°®‰∫ÜÈÜ´ÁôÇ‰∏≠ÂøÉ„ÄÇËßÄÂØüÂà∞Á©©ÂÅ•ÊÄßÊåáÊ®ôÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÂà∞ÁõÆÂâçÁÇ∫Ê≠¢ÔºåÂè™Êúâ‰∏ÄÈ†ÖÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÊåáÊ®ôÂ§ßÊñº‰∏ÄÔºåË°®Á§∫ÁîüÁâ©ÁâπÂæµ‰∏ªÂ∞éÊ∑∑Ê∑ÜÁâπÂæµÔºå‰ΩÜÂÉÖÁï•ÂæÆ‰∏ªÂ∞é„ÄÇÊèèËø∞‰∫ÜË°°ÈáèÈÜ´ÁôÇ‰∏≠ÂøÉÂ∑ÆÁï∞Â∞çÂü∫Êñº FM ÁöÑÈ†êÊ∏¨ÊïàËÉΩÂΩ±ÈüøÁöÑÈáèÂåñÊñπÊ≥ï„ÄÇÊàëÂÄëÂàÜÊûê‰∫Ü‰∏çÁ©©ÂÅ•ÊÄßÂ∞ç‰∏ãÊ∏∏Ê®°ÂûãÂàÜÈ°ûÊïàËÉΩÁöÑÂΩ±ÈüøÔºåÁôºÁèæÁôåÁóáÈ°ûÂûãÂàÜÈ°ûÈåØË™§‰∏¶ÈùûÈö®Ê©üÔºåËÄåÊòØÁâπÂà•Ê≠∏Âõ†ÊñºÂêå‰∏≠ÂøÉÊ∑∑Ê∑ÜÂõ†Â≠êÔºö‰æÜËá™Âêå‰∏ÄÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑÂÖ∂‰ªñÈ°ûÂà•ÂΩ±ÂÉè„ÄÇÊàëÂÄëË¶ñË¶∫Âåñ FM ÂµåÂÖ•Á©∫ÈñìÔºåÁôºÁèæÈÄô‰∫õÁ©∫ÈñìÊòØÁî±ÈÜ´ÁôÇ‰∏≠ÂøÉËÄåÈùûÁîüÁâ©Âõ†Á¥†Êõ¥Âº∑ÊúâÂäõÂú∞ÁµÑÁπî„ÄÇÂõ†Ê≠§ÔºåÊØîÁµÑÁπî‰æÜÊ∫êÂíåÁôåÁóáÈ°ûÂûãÊõ¥Ê∫ñÁ¢∫Âú∞È†êÊ∏¨‰∫ÜÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑ‰æÜÊ∫ê„ÄÇÂú®Ê≠§ÂºïÂÖ•Á©©ÂÅ•ÊÄßÊåáÊ®ôÔºåÁõÆÁöÑÊòØÊé®ÈÄ≤ÊúùËëóËá®Â∫äÊé°Áî®Á©©ÂÅ•‰∏îÂèØÈù†ÁöÑÁóÖÁêÜ FM ÁöÑÈÄ≤Â±ï„ÄÇ

##### **Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**
2501.17860v1 by Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen

Current medical AI systems often fail to replicate real-world clinical
reasoning, as they are predominantly trained and evaluated on static text and
question-answer tasks. These tuning methods and benchmarks overlook critical
aspects like evidence-based reasoning and handling distracting information. To
bridge this gap, we introduce a novel benchmark that simulates real-world
diagnostic scenarios, integrating noise and difficulty levels aligned with
USMLE standards. Moreover, we explore dialogue-based fine-tuning, which
transforms static datasets into conversational formats to better capture
iterative reasoning processes. Experiments show that dialogue-tuned models
outperform traditional methods, with improvements of $9.64\%$ in multi-round
reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our
findings highlight dialogue tuning as a promising approach for advancing
clinically aligned and robust medical AI systems.

ÊëòË¶ÅÔºöÁõÆÂâçÁöÑÈÜ´ÁôÇ AI Á≥ªÁµ±Â∏∏ÁÑ°Ê≥ïË§áË£ΩÁúüÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÊé®ÁêÜÔºåÂõ†ÁÇ∫ÂÆÉÂÄë‰∏ªË¶ÅÂú®ÈùúÊÖãÊñáÂ≠óÂíåÂïèÁ≠î‰ªªÂãô‰∏äÂèóË®ìÂíåË©ï‰º∞„ÄÇÈÄô‰∫õË™øÊï¥ÊñπÊ≥ïÂíåÂü∫Ê∫ñÂøΩÁï•‰∫ÜÂü∫ÊñºË≠âÊìöÁöÑÊé®ÁêÜÂíåËôïÁêÜÂàÜÊï£Ë≥áË®äÁ≠âÈóúÈçµÈù¢Âêë„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ®°Êì¨ÁúüÂØ¶‰∏ñÁïåË®∫Êñ∑ÊÉÖÂ¢ÉÁöÑÂÖ®Êñ∞Âü∫Ê∫ñÔºåÊï¥ÂêàËàá USMLE Ê®ôÊ∫ñ‰∏ÄËá¥ÁöÑÈõúË®äÂíåÈõ£Â∫¶Á≠âÁ¥ö„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰ª•Â∞çË©±ÁÇ∫Âü∫Á§éÁöÑÂæÆË™øÔºåÂ∞áÈùúÊÖãË≥áÊñôÈõÜËΩâÊèõÁÇ∫Â∞çË©±Ê†ºÂºèÔºå‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÂèçË¶ÜÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåÂ∞çË©±ÂæÆË™øÊ®°ÂûãÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂú®Â§öËº™Êé®ÁêÜÊÉÖÂ¢É‰∏≠ÊèêÂçá‰∫Ü 9.64%ÔºåÂú®ÊúâÈõúË®äÁöÑÁí∞Â¢É‰∏≠ÊèêÂçá‰∫Ü 6.18% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÁôºÁèæÂº∑Ë™øÂ∞çË©±ÂæÆË™øÊòØ‰∏ÄÁ®ÆÊúâÊúõÊé®ÈÄ≤ËàáËá®Â∫äÁõ∏Á¨¶‰∏îÂº∑ÂÅ•ÁöÑÈÜ´ÁôÇ AI Á≥ªÁµ±ÁöÑÊñπÊ≥ï„ÄÇ

##### **GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**
2501.17855v1 by Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee

Robot caregiving should be personalized to meet the diverse needs of care
recipients -- assisting with tasks as needed, while taking user agency in
action into account. In physical tasks such as handover, bathing, dressing, and
rehabilitation, a key aspect of this diversity is the functional range of
motion (fROM), which can vary significantly between individuals. In this work,
we learn to predict personalized fROM as a way to generalize robot
decision-making in a wide range of caregiving tasks. We propose a novel
data-driven method for predicting personalized fROM using functional assessment
scores from occupational therapy. We develop a neural model that learns to
embed functional assessment scores into a latent representation of the user's
physical function. The model is trained using motion capture data collected
from users with emulated mobility limitations. After training, the model
predicts personalized fROM for new users without motion capture. Through
simulated experiments and a real-robot user study, we show that the
personalized fROM predictions from our model enable the robot to provide
personalized and effective assistance while improving the user's agency in
action. See our website for more visualizations:
https://emprise.cs.cornell.edu/grace/.

ÊëòË¶ÅÔºöÊ©üÂô®‰∫∫ÁÖßË≠∑ÊáâÊ†πÊìöÁÖßË≠∑Â∞çË±°ÁöÑ‰∏çÂêåÈúÄÊ±ÇÈÄ≤Ë°åÂÆ¢Ë£ΩÂåñÔºåÂú®ÈúÄË¶ÅÊôÇÂçîÂä©Âü∑Ë°å‰ªªÂãôÔºåÂêåÊôÇËÄÉÈáè‰ΩøÁî®ËÄÖÁöÑËá™‰∏ªË°åÂãï„ÄÇÂú®Áßª‰∫§„ÄÅÊ≤êÊµ¥„ÄÅÁ©øË°£ÂíåÂæ©ÂÅ•Á≠âË∫´È´î‰ªªÂãô‰∏≠ÔºåÈÄôÁ®ÆÂ§öÊ®£ÊÄßÁöÑÈóúÈçµÈù¢ÂêëÊòØÂäüËÉΩÊÄßÂãï‰ΩúÁØÑÂúç (fROM)ÔºåËÄåÈÄôÂú®‰∏çÂêåÂÄãÈ´î‰πãÈñìÂèØËÉΩÂ∑ÆÁï∞ÂæàÂ§ß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ≠∏ÁøíÈ†êÊ∏¨ÂÆ¢Ë£ΩÂåñ fROMÔºå‰ΩúÁÇ∫Âú®Âª£Ê≥õÁÖßË≠∑‰ªªÂãô‰∏≠Ê¶ÇÂåñÊ©üÂô®‰∫∫Ê±∫Á≠ñÂà∂ÂÆöÁöÑ‰∏ÄÁ®ÆÊñπÂºè„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ËÅ∑ËÉΩÊ≤ªÁôÇÂäüËÉΩË©ï‰º∞ÂàÜÊï∏‰æÜÈ†êÊ∏¨ÂÆ¢Ë£ΩÂåñ fROM ÁöÑÊñ∞Á©éË≥áÊñôÈ©ÖÂãïÊñπÊ≥ï„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÁ•ûÁ∂ìÊ®°ÂûãÔºåÂ≠∏ÁøíÂ∞áÂäüËÉΩË©ï‰º∞ÂàÜÊï∏ÂµåÂÖ•Âà∞‰ΩøÁî®ËÄÖÁöÑË∫´È´îÂäüËÉΩÊΩõÂú®Ë°®Âæµ‰∏≠„ÄÇË©≤Ê®°Âûã‰ΩøÁî®ÂæûÂÖ∑ÊúâÊ®°Êì¨Ë°åÂãïÈôêÂà∂ÁöÑ‰ΩøÁî®ËÄÖÊî∂ÈõÜÁöÑÂãï‰ΩúÊì∑ÂèñË≥áÊñôÈÄ≤Ë°åË®ìÁ∑¥„ÄÇË®ìÁ∑¥ÂæåÔºåË©≤Ê®°ÂûãÊúÉÁÇ∫Ê≤íÊúâÂãï‰ΩúÊì∑ÂèñÁöÑÊñ∞‰ΩøÁî®ËÄÖÈ†êÊ∏¨ÂÆ¢Ë£ΩÂåñ fROM„ÄÇÈÄèÈÅéÊ®°Êì¨ÂØ¶È©óÂíåÁúüÂØ¶Ê©üÂô®‰∫∫‰ΩøÁî®ËÄÖÁ†îÁ©∂ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊ®°ÂûãÁöÑÂÆ¢Ë£ΩÂåñ fROM È†êÊ∏¨‰ΩøÊ©üÂô®‰∫∫ËÉΩÂ§†Êèê‰æõÂÆ¢Ë£ΩÂåñ‰∏îÊúâÊïàÁöÑÂçîÂä©ÔºåÂêåÊôÇÊèêÈ´ò‰ΩøÁî®ËÄÖÁöÑËá™‰∏ªË°åÂãï„ÄÇË´ãÂèÉÈñ±ÊàëÂÄëÁöÑÁ∂≤Á´ô‰ª•ÂèñÂæóÊõ¥Â§öË¶ñË¶∫ÂåñË≥áÊñôÔºöhttps://emprise.cs.cornell.edu/grace/„ÄÇ

##### **Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**
2502.00055v1 by Ljubisa Bojic, Zorica Dodevska, Yashar Deldjoo, Nenad Pantelic

Given the exponential advancement in AI technologies and the potential
escalation of harmful effects from recommendation systems, it is crucial to
simulate and evaluate these effects early on. Doing so can help prevent
possible damage to both societies and technology companies. This paper
introduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel
simulation framework leveraging Large Language Models (LLMs) to explore the
impacts of different content recommendation setups on user engagement and
polarization in social networks. By creating diverse AI agents (AgentPrompts)
with descriptive, static, and dynamic attributes, we assess their autonomous
behaviour across three scenarios: Plurality, Balanced, and Similarity. Our
findings reveal that the Similarity Scenario, which aligns content with user
preferences, maximizes engagement while potentially fostering echo chambers.
Conversely, the Plurality Scenario promotes diverse interactions but produces
mixed engagement results. Our study emphasizes the need for a careful balance
in recommender system designs to enhance user satisfaction while mitigating
societal polarization. It underscores the unique value and challenges of
incorporating LLMs into simulation environments. The benefits of RecSysLLMsP
lie in its potential to calculate polarization effects, which is crucial for
assessing societal impacts and determining user engagement levels with diverse
recommender system setups. This advantage is essential for developing and
maintaining a successful business model for social media companies. However,
the study's limitations revolve around accurately emulating reality. Future
efforts should validate the similarity in behaviour between real humans and
AgentPrompts and establish metrics for measuring polarization scores.

ÊëòË¶ÅÔºö<paragraph>Èö®Ëëó AI ÊäÄË°ìÁöÑÊåáÊï∏Á¥öÈÄ≤Ê≠•Ôºå‰ª•ÂèäÊé®Ëñ¶Á≥ªÁµ±ÈÄ†ÊàêÁöÑÊúâÂÆ≥ÂΩ±ÈüøÁöÑÊΩõÂú®ÂçáÁ¥öÔºåÊèêÊó©Ê®°Êì¨ÂíåË©ï‰º∞ÈÄô‰∫õÂΩ±ÈüøËá≥ÈóúÈáçË¶Å„ÄÇÈÄôÈ∫ºÂÅöÊúâÂä©ÊñºÈò≤Ê≠¢Â∞çÁ§æÊúÉÂíåÁßëÊäÄÂÖ¨Âè∏ÈÄ†ÊàêÁöÑÊΩõÂú®ÊêçÂÆ≥„ÄÇÈÄôÁØáË´ñÊñá‰ªãÁ¥π‰∫ÜÊé®Ëñ¶Á≥ªÁµ± LLM ÈÅäÊ®ÇÂ†¥ (RecSysLLMsP)Ôºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ®°Êì¨Êû∂ÊßãÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÊé¢Á¥¢‰∏çÂêåÂÖßÂÆπÊé®Ëñ¶Ë®≠ÂÆöÂ∞çÁ§æÁæ§Á∂≤Ë∑Ø‰∏≠ÁöÑ‰ΩøÁî®ËÄÖÂèÉËàáÂ∫¶ÂíåÊ•µÂåñÁèæË±°ÁöÑÂΩ±Èüø„ÄÇËóâÁî±ÂâµÈÄ†ÂÖ∑ÊúâÊèèËø∞ÊÄß„ÄÅÈùúÊÖãÂíåÂãïÊÖãÂ±¨ÊÄßÁöÑÂ§öÊ®£Âåñ AI ‰ª£ÁêÜÁ®ãÂºè (AgentPrompts)ÔºåÊàëÂÄëË©ï‰º∞ÂÆÉÂÄëÂú®‰∏âÁ®ÆÊÉÖÂ¢É‰∏≠ÁöÑËá™‰∏ªË°åÁÇ∫ÔºöÂ§öÂÖÉÊÄß„ÄÅÂπ≥Ë°°ÊÄßÂíåÁõ∏‰ººÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåËàá‰ΩøÁî®ËÄÖÂÅèÂ•Ω‰∏ÄËá¥ÁöÑÁõ∏‰ººÊÄßÊÉÖÂ¢ÉÔºåÊúÄÂ§ßÂåñ‰∫ÜÂèÉËàáÂ∫¶ÔºåÂêåÊôÇÊΩõÂú®‰øÉÈÄ≤‰∫ÜÂêåÊ∫´Â±§„ÄÇÁõ∏ÂèçÂú∞ÔºåÂ§öÂÖÉÊÄßÊÉÖÂ¢É‰øÉÈÄ≤‰∫ÜÂ§öÊ®£ÂåñÁöÑ‰∫íÂãïÔºå‰ΩÜÁî¢Áîü‰∫ÜÂèÉÂ∑Æ‰∏çÈΩäÁöÑÂèÉËàáÂ∫¶ÁµêÊûú„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÂú®Êé®Ëñ¶Á≥ªÁµ±Ë®≠Ë®à‰∏≠‰ªîÁ¥∞ÂèñÂæóÂπ≥Ë°°ÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•ÊèêÂçá‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶ÔºåÂêåÊôÇÊ∏õËºïÁ§æÊúÉÊ•µÂåñÁèæË±°„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂ∞á LLM Á¥çÂÖ•Ê®°Êì¨Áí∞Â¢ÉÁöÑÁç®ÁâπÂÉπÂÄºÂíåÊåëÊà∞„ÄÇRecSysLLMsP ÁöÑÂÑ™Âã¢Âú®ÊñºÂÆÉË®àÁÆóÊ•µÂåñÊïàÊáâÁöÑÊΩõÂäõÔºåÈÄôÂ∞çÊñºË©ï‰º∞Á§æÊúÉÂΩ±ÈüøÂíåÁ¢∫ÂÆö‰ΩøÁî®ËÄÖÂèÉËàáÂ∫¶ËàáÂ§öÊ®£ÂåñÊé®Ëñ¶Á≥ªÁµ±Ë®≠ÂÆöÁöÑÂ±§Á¥öËá≥ÈóúÈáçË¶Å„ÄÇÈÄôÂÄãÂÑ™Âã¢Â∞çÊñºÁ§æÁæ§Â™íÈ´îÂÖ¨Âè∏ÈñãÁôºÂíåÁ∂≠ÊåÅÊàêÂäüÁöÑÂïÜÊ•≠Ê®°ÂºèËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÄôÈ†ÖÁ†îÁ©∂ÁöÑÈôêÂà∂Âú®ÊñºÁ≤æÁ¢∫Âú∞Ê®°Êì¨ÁèæÂØ¶„ÄÇÊú™‰æÜÁöÑÂä™ÂäõÊáâË©≤È©óË≠âÁúüÂØ¶‰∫∫È°ûÂíå AgentPrompts ‰πãÈñìË°åÁÇ∫ÁöÑÁõ∏‰ººÊÄßÔºå‰∏¶Âª∫Á´ãË°°ÈáèÊ•µÂåñÂàÜÊï∏ÁöÑÊåáÊ®ô„ÄÇ</paragraph>

##### **Tonguescape: Exploring Language Models Understanding of Vowel Articulation**
2501.17643v1 by Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe

Vowels are primarily characterized by tongue position. Humans have discovered
these features of vowel articulation through their own experience and explicit
objective observation such as using MRI. With this knowledge and our
experience, we can explain and understand the relationship between tongue
positions and vowels, and this knowledge is helpful for language learners to
learn pronunciation. Since language models (LMs) are trained on a large amount
of data that includes linguistic and medical fields, our preliminary studies
indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align
textual information with visual information. One question arises: do LMs
associate real tongue positions with vowel articulation? In this study, we
created video and image datasets from the existing real-time MRI dataset and
investigated whether LMs can understand vowel articulation based on tongue
positions using vision-based information. Our findings suggest that LMs exhibit
potential for understanding vowels and tongue positions when reference examples
are provided while they have difficulties without them. Our code for dataset
building is available on GitHub.

ÊëòË¶ÅÔºöÂÖÉÈü≥‰∏ªË¶ÅÁî±ËàåÈ†≠‰ΩçÁΩÆÊ±∫ÂÆö„ÄÇ‰∫∫È°ûÈÄèÈÅéËá™Â∑±ÁöÑÁ∂ìÈ©óÂíåÊòéÁ¢∫ÁöÑÂÆ¢ËßÄËßÄÂØüÔºà‰æãÂ¶Ç‰ΩøÁî® MRIÔºâÁôºÁèæ‰∫ÜÂÖÉÈü≥ÁôºÈü≥ÁöÑÈÄô‰∫õÁâπÂæµ„ÄÇÊúâ‰∫ÜÈÄô‰∫õÁü•Ë≠òÂíåÁ∂ìÈ©óÔºåÊàëÂÄëÂèØ‰ª•Ëß£ÈáãÂíåÁêÜËß£ËàåÈ†≠‰ΩçÁΩÆÂíåÂÖÉÈü≥‰πãÈñìÁöÑÈóú‰øÇÔºåËÄåÈÄô‰∫õÁü•Ë≠òÂ∞çË™ûË®ÄÂ≠∏ÁøíËÄÖÂ≠∏ÁøíÁôºÈü≥ÂæàÊúâÂπ´Âä©„ÄÇÁî±ÊñºË™ûË®ÄÊ®°Âûã (LM) ÊòØÂú®ÂåÖÂê´Ë™ûË®ÄÂ≠∏ÂíåÈÜ´Â≠∏È†òÂüüÁöÑÂ§ßÈáèË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÔºåÊàëÂÄëÁöÑÂàùÊ≠•Á†îÁ©∂Ë°®ÊòéÔºåLM ËÉΩÂ§†Ëß£ÈáãÂÖÉÈü≥ÁöÑÁôºÈü≥Ê©üÂà∂„ÄÇÁÑ∂ËÄåÔºåÂ∞ö‰∏çÊ∏ÖÊ•öÂ§öÊ®°ÊÖã LMÔºà‰æãÂ¶ÇË¶ñË¶∫ LMÔºâÊòØÂê¶Â∞áÊñáÂ≠óË≥áË®äËàáË¶ñË¶∫Ë≥áË®äÂ∞çÈΩä„ÄÇ‰∏ÄÂÄãÂïèÈ°åÁî¢Áîü‰∫ÜÔºöLM ÊòØÂê¶Â∞áÁúüÂØ¶ÁöÑËàåÈ†≠‰ΩçÁΩÆËàáÂÖÉÈü≥ÁôºÈü≥ËÅØÁπ´Ëµ∑‰æÜÔºüÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂæûÁèæÊúâÁöÑÂç≥ÊôÇ MRI Ë≥áÊñôÈõÜ‰∏≠Âª∫Á´ã‰∫ÜÂΩ±ÁâáÂíåÂΩ±ÂÉèË≥áÊñôÈõÜÔºå‰∏¶Êé¢Ë®é LM ÊòØÂê¶ËÉΩÊ†πÊìöËàåÈ†≠‰ΩçÁΩÆ‰ΩøÁî®Âü∫ÊñºË¶ñË¶∫ÁöÑË≥áË®ä‰æÜÁêÜËß£ÂÖÉÈü≥ÁôºÈü≥„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁï∂Êèê‰æõÂèÉËÄÉÁØÑ‰æãÊôÇÔºåLM ÂÖ∑ÊúâÁêÜËß£ÂÖÉÈü≥ÂíåËàåÈ†≠‰ΩçÁΩÆÁöÑÊΩõÂäõÔºåËÄåÊ≤íÊúâÂèÉËÄÉÁØÑ‰æãÊôÇÂâáÊúâÂõ∞Èõ£„ÄÇÊàëÂÄëÁî®ÊñºÂª∫Á´ãË≥áÊñôÈõÜÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® GitHub ‰∏äÂèñÂæó„ÄÇ

##### **Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**
2501.18645v2 by Manish Sanwal

Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to
provide step-by-step rationales, improving performance on complex tasks.
Despite its benefits, vanilla CoT often fails to fully verify intermediate
inferences and can produce misleading explanations. In this work, we propose
Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that
systematically segments the reasoning process into multiple layers, each
subjected to external checks and optional user feedback. We expand on the key
concepts, present three scenarios -- medical triage, financial risk assessment,
and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT
in terms of transparency, correctness, and user engagement. By integrating
references from recent arXiv papers on interactive explainability, multi-agent
frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves
the way for more reliable and grounded explanations in high-stakes domains.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂà©Áî®ÊÄùËÄÉÈèàÔºàCoTÔºâÊèêÁ§∫Êèê‰æõÈÄêÊ≠•ÁöÑÁêÜÁî±ÔºåÊèêÂçáË§áÈõú‰ªªÂãôÁöÑË°®Áèæ„ÄÇÂÑòÁÆ°ÊúâÂÖ∂Â•ΩËôïÔºå‰ΩÜÈ¶ôËçâ CoT Â∏∏Â∏∏ÁÑ°Ê≥ïÂÆåÂÖ®È©óË≠â‰∏≠ÈñìÊé®Ë´ñÔºå‰∏îÂèØËÉΩÊúÉÁî¢ÁîüË™§Â∞éÊÄßÁöÑËß£Èáã„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÂàÜÂ±§ÊÄùËÄÉÈèàÔºàÂàÜÂ±§ CoTÔºâÊèêÁ§∫Ôºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÂÆÉÊúâÁ≥ªÁµ±Âú∞Â∞áÊé®ÁêÜÈÅéÁ®ãÂçÄÈöîÊàêÂ§öÂÄãÂ±§Á¥öÔºåÊØèÂÄãÂ±§Á¥öÈÉΩÁ∂ìÈÅéÂ§ñÈÉ®Ê™¢Êü•ÂíåÂèØÈÅ∏ÊìáÁöÑ‰ΩøÁî®ËÄÖÂõûÈ•ã„ÄÇÊàëÂÄëÊì¥Â±ïÈóúÈçµÊ¶ÇÂøµÔºåÊèêÂá∫‰∏âÂÄãÂ†¥ÊôØ‚Äî‚ÄîÈÜ´ÁôÇÂàÜÊµÅ„ÄÅË≤°ÂãôÈ¢®Èö™Ë©ï‰º∞ÂíåÊïèÊç∑Â∑•Á®ã‚Äî‚Äî‰∏¶Â±ïÁ§∫ÂàÜÂ±§ CoT Âú®ÈÄèÊòéÂ∫¶„ÄÅÊ≠£Á¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÂèÉËàáÂ∫¶ÊñπÈù¢Â¶Ç‰ΩïË∂ÖË∂äÈ¶ôËçâ CoT„ÄÇÈÄèÈÅéÊï¥Âêà‰æÜËá™ËøëÊúü arXiv Ë´ñÊñá‰∏≠ÈóúÊñº‰∫íÂãïÂèØËß£ÈáãÊÄß„ÄÅÂ§öÈáç‰ª£ÁêÜÊû∂ÊßãÂíåÂü∫Êñº‰ª£ÁêÜÁöÑÂçî‰ΩúÁöÑÂèÉËÄÉÊñáÁçªÔºåÊàëÂÄëË™™ÊòéÂàÜÂ±§ CoT Â¶Ç‰ΩïÁÇ∫È´òÈ¢®Èö™È†òÂüü‰∏≠Êõ¥ÂèØÈù†‰∏îÊúâÊ†πÊìöÁöÑËß£ÈáãÈã™Ë∑Ø„ÄÇ

##### **An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**
2501.17555v1 by Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian

Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms
that account for less than 5% of all pancreatic malignancies, with an incidence
of only 1-1.5 cases per 100,000. Early detection of pNETs is critical for
improving patient survival, but the rarity of pNETs makes segmenting them from
CT a very challenging problem. So far, there has not been a dataset
specifically for pNETs available to researchers. To address this issue, we
propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography
(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,
containing data from 469 patients. This is the first dataset solely dedicated
to pNETs, distinguishing it from previous collections. Additionally, we provide
the baseline detection networks with a new slice-wise weight loss function
designed for the UNet-based model, improving the overall pNET segmentation
performance. We hope that our dataset can enhance the understanding and
diagnosis of pNET Tumors within the medical community, facilitate the
development of more accurate diagnostic tools, and ultimately improve patient
outcomes and advance the field of oncology.

ÊëòË¶ÅÔºöËÉ∞ËáüÁ•ûÁ∂ìÂÖßÂàÜÊ≥åËÖ´Áò§ (pNETs) ÊòØÈùûÂ∏∏ÁΩïË¶ãÁöÑÂÖßÂàÜÊ≥åËÖ´Áò§ÔºåÂÉÖ‰ΩîÊâÄÊúâËÉ∞ËáüÊÉ°ÊÄßËÖ´Áò§ÁöÑ‰∏çÂà∞ 5%ÔºåÊØè 100,000 ‰∫∫‰∏≠ÂÉÖÁôºÁîü 1-1.5 ÂÄãÁóÖ‰æã„ÄÇÊó©ÊúüÁôºÁèæ pNETs Â∞çÊîπÂñÑÊÇ£ËÄÖÂ≠òÊ¥ªÁéáËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜ pNETs ÁöÑÁΩïË¶ãÊÄß‰ΩøÂæóÂæû CT ‰∏≠ÂàÜÂâ≤ÂÆÉÂÄëÊàêÁÇ∫‰∏ÄÂÄãÈùûÂ∏∏ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°å„ÄÇÂà∞ÁõÆÂâçÁÇ∫Ê≠¢ÔºåÈÇÑÊ≤íÊúâÂ∞àÈñÄÈáùÂ∞ç pNETs ÁöÑÊï∏ÊìöÈõÜÂèØ‰æõÁ†îÁ©∂‰∫∫Âì°‰ΩøÁî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã pNETs Êï∏ÊìöÈõÜÔºå‰∏ÄÂÄãÂ∞àÊ≥®ÊñºËÉ∞ËáüÁ•ûÁ∂ìÂÖßÂàÜÊ≥åËÖ´Áò§ÁöÑÊ®ôË®ªËâØÂ•ΩÁöÑÂ∞çÊØîÂ¢ûÂº∑ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CECT) Êï∏ÊìöÈõÜÔºåÂåÖÂê´‰æÜËá™ 469 ÂêçÊÇ£ËÄÖÁöÑÊï∏Êìö„ÄÇÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄÈáùÂ∞ç pNETs ÁöÑÊï∏ÊìöÈõÜÔºåÈÄô‰ΩøÂÖ∂ÊúâÂà•Êñº‰πãÂâçÁöÑÊî∂ÈõÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁÇ∫Âü∫Á∑öÊ™¢Ê∏¨Á∂≤Ë∑ØÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Êñº UNet Ê®°ÂûãË®≠Ë®àÁöÑÂàáÁâáÂä†Ê¨äÊêçÂ§±ÂáΩÊï∏ÔºåÊîπÂñÑ‰∫ÜÊï¥È´î pNET ÂàÜÂâ≤ÊÄßËÉΩ„ÄÇÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÊï∏ÊìöÈõÜËÉΩÂ§†Â¢ûÂº∑ÈÜ´Â≠∏ÁïåÂ∞ç pNET ËÖ´Áò§ÁöÑÁêÜËß£ÂíåË®∫Êñ∑Ôºå‰øÉÈÄ≤Êõ¥Ê∫ñÁ¢∫ÁöÑË®∫Êñ∑Â∑•ÂÖ∑ÁöÑÈñãÁôºÔºåÊúÄÁµÇÊîπÂñÑÊÇ£ËÄÖÁöÑÈ†êÂæå‰∏¶Êé®ÈÄ≤ËÖ´Áò§Â≠∏È†òÂüü„ÄÇ

##### **LLM Assistance for Pediatric Depression**
2501.17510v1 by Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive

Traditional depression screening methods, such as the PHQ-9, are particularly
challenging for children in pediatric primary care due to practical
limitations. AI has the potential to help, but the scarcity of annotated
datasets in mental health, combined with the computational costs of training,
highlights the need for efficient, zero-shot approaches. In this work, we
investigate the feasibility of state-of-the-art LLMs for depressive symptom
extraction in pediatric settings (ages 6-24). This approach aims to complement
traditional screening and minimize diagnostic errors.
  Our findings show that all LLMs are 60% more efficient than word match, with
Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the
extraction of more rare symptoms like "sleep problems" (F1: 0.92) and
"self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and
recall (0.60), performing well in categories like "Feeling depressed" (0.69)
and "Weight change" (0.78). Llama 3, with the highest recall (0.90),
overgeneralizes symptoms, making it less suitable for this type of analysis.
Challenges include the complexity of clinical notes and overgeneralization from
PHQ-9 scores. The main challenges faced by LLMs include navigating the complex
structure of clinical notes with content from different times in the patient
trajectory, as well as misinterpreting elevated PHQ-9 scores.
  We finally demonstrate the utility of symptom annotations provided by Flan as
features in an ML algorithm, which differentiates depression cases from
controls with high precision of 0.78, showing a major performance boost
compared to a baseline that does not use these features.

ÊëòË¶ÅÔºö<paragraph>ÂÇ≥Áµ±ÁöÑÊÜÇÈ¨±ÁóáÁØ©Ê™¢ÊñπÊ≥ïÔºå‰æãÂ¶Ç PHQ-9ÔºåÁî±ÊñºÂØ¶ÈöõÈôêÂà∂ÔºåÂ∞çÊñºÂ∞èÂÖíÁßëÂàùÁ¥öÁÖßË≠∑‰∏≠ÁöÑÂÖíÁ´•‰æÜË™™ÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇAI ÊúâÂèØËÉΩÊèê‰æõÂπ´Âä©Ôºå‰ΩÜÂøÉÁêÜÂÅ•Â∫∑‰∏≠Ë®ªËß£Ë≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÔºåÂä†‰∏äË®ìÁ∑¥ÁöÑÈÅãÁÆóÊàêÊú¨ÔºåÁ™ÅÈ°Ø‰∫ÜÂ∞çÊúâÊïàÁéáÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÁöÑÈúÄÊ±Ç„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú®Â∞èÂÖíÁßëÁí∞Â¢ÉÔºà6-24 Ê≠≤Ôºâ‰∏≠ÊèêÂèñÊÜÇÈ¨±ÁóáÁãÄÁöÑÂèØË°åÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÊó®Âú®Ë£úÂÖÖÂÇ≥Áµ±ÁØ©Ê™¢‰∏¶Â∞áË®∫Êñ∑ÈåØË™§ÈôçËá≥ÊúÄ‰Ωé„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊúâ LLM ÁöÑÊïàÁéáÈÉΩÊØîÂ≠óË©ûÊØîÂ∞çÈ´òÂá∫ 60%ÔºåËÄå Flan Âú®Á≤æÁ¢∫Â∫¶ÊñπÈù¢È†òÂÖàÔºàÂπ≥Âùá F1Ôºö0.65ÔºåÁ≤æÁ¢∫Â∫¶Ôºö0.78ÔºâÔºåÂú®ÊèêÂèñËºÉÁΩïË¶ãÁöÑÁóáÁãÄÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰æãÂ¶Ç„ÄåÁù°Áú†ÂïèÈ°å„ÄçÔºàF1Ôºö0.92ÔºâÂíå„ÄåËá™ÊàëÂé≠ÊÉ°„ÄçÔºàF1Ôºö0.8Ôºâ„ÄÇPhi Âú®Á≤æÁ¢∫Â∫¶Ôºà0.44ÔºâÂíåÂè¨ÂõûÁéáÔºà0.60Ôºâ‰πãÈñìÂèñÂæóÂπ≥Ë°°ÔºåÂú®„ÄåÊÑüÂà∞Ê≤ÆÂñ™„ÄçÔºà0.69ÔºâÂíå„ÄåÈ´îÈáçÊîπËÆä„ÄçÔºà0.78ÔºâÁ≠âÈ°ûÂà•‰∏≠Ë°®ÁèæËâØÂ•Ω„ÄÇÊìÅÊúâÊúÄÈ´òÂè¨ÂõûÁéáÔºà0.90ÔºâÁöÑ Llama 3 ÊúÉÈÅéÂ∫¶Ê¶ÇÊã¨ÁóáÁãÄÔºå‰ΩøÂÖ∂‰∏çÂ§™ÈÅ©ÂêàÊ≠§È°ûÂàÜÊûê„ÄÇÊåëÊà∞ÂåÖÊã¨Ëá®Â∫äÁ≠ÜË®òÁöÑË§áÈõúÊÄßÂíå PHQ-9 ÂàÜÊï∏ÁöÑÈÅéÂ∫¶Ê¶ÇÊã¨„ÄÇLLM Èù¢Ëá®ÁöÑ‰∏ªË¶ÅÊåëÊà∞ÂåÖÊã¨Âú®ÊÇ£ËÄÖÊ≠∑Á®ã‰∏≠‰∏çÂêåÊôÇÈñìÁöÑÂÖßÂÆπ‰∏≠Â∞éËà™Ëá®Â∫äÁ≠ÜË®òÁöÑË§áÈõúÁµêÊßãÔºå‰ª•ÂèäË™§Ëß£ PHQ-9 ÂàÜÊï∏ÂçáÈ´ò„ÄÇÊàëÂÄëÊúÄÂæåÂ±ïÁ§∫‰∫Ü Flan Êèê‰æõÁöÑÁóáÁãÄË®ªËß£‰ΩúÁÇ∫Ê©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ï‰∏≠ÁâπÂæµÁöÑÊïàÁî®ÔºåÂÆÉ‰ª• 0.78 ÁöÑÈ´òÁ≤æÁ¢∫Â∫¶Â∞áÊÜÇÈ¨±ÁóáÁóÖ‰æãËàáÂ∞çÁÖßÁµÑÂçÄÂàÜÈñã‰æÜÔºåËàá‰∏ç‰ΩøÁî®ÈÄô‰∫õÁâπÂæµÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåÈ°ØÁ§∫Âá∫‰∏ªË¶ÅÁöÑÊïàËÉΩÊèêÂçá„ÄÇ</paragraph>

##### **Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**
2502.00052v1 by Gonzalo I√±aki Quintana, Laurence Vancamberg, Vincent Jugnon, Agn√®s Desolneux, Mathilde Mougeot

This work studies the relationship between Contrastive Learning and Domain
Adaptation from a theoretical perspective. The two standard contrastive losses,
NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to
the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely
used for Domain Adaptation. Our work shows that minimizing the contrastive
losses decreases the CMMD and simultaneously improves class-separability,
laying the theoretical groundwork for the use of Contrastive Learning in the
context of Domain Adaptation. Due to the relevance of Domain Adaptation in
medical imaging, we focused the experiments on mammography images. Extensive
experiments on three mammography datasets - synthetic patches, clinical (real)
patches, and clinical (real) images - show improved Domain Adaptation,
class-separability, and classification performance, when minimizing the
Supervised Contrastive loss.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÂæûÁêÜË´ñËßíÂ∫¶Êé¢Ë®éÂ∞çÊØîÂ≠∏ÁøíËàáÈ†òÂüüÈÅ©Êáâ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÂÖ©Á®ÆÊ®ôÊ∫ñÂ∞çÊØîÊêçÂ§±ÔºåNT-Xent ÊêçÂ§±ÔºàËá™ÊàëÁõ£Áù£ÔºâÂíåÁõ£Áù£Â∞çÊØîÊêçÂ§±ÔºåËàáÂª£Ê≥õÁî®ÊñºÈ†òÂüüÈÅ©ÊáâÁöÑÂ∑ÆÁï∞ÊÄßÊ∏¨ÈáèÊ®ôÊ∫ñÈ°ûÂà•Âπ≥ÂùáÊúÄÂ§ßÂ∑ÆÁï∞ÔºàCMMDÔºâÊúâÈóú„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊúÄÂ∞èÂåñÂ∞çÊØîÊêçÂ§±ÊúÉÈôç‰Ωé CMMDÔºåÂêåÊôÇÊèêÈ´òÈ°ûÂà•ÂèØÂàÜÈõ¢ÊÄßÔºåÁÇ∫Âú®È†òÂüüÈÅ©Êáâ‰∏≠‰ΩøÁî®Â∞çÊØîÂ≠∏ÁøíÂ•†ÂÆöÁêÜË´ñÂü∫Á§é„ÄÇÁî±ÊñºÈ†òÂüüÈÅ©ÊáâÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁõ∏ÈóúÊÄßÔºåÊàëÂÄëÂ∞áÂØ¶È©óÈáçÈªûÊîæÂú®‰π≥ÊàøÊîùÂΩ±ÂúñÂÉè‰∏ä„ÄÇÂú®‰∏âÂÄã‰π≥ÊàøÊîùÂΩ±Êï∏ÊìöÈõÜÔºàÂêàÊàêË≤ºÁâá„ÄÅËá®Â∫äÔºàÁúüÂØ¶ÔºâË≤ºÁâáÂíåËá®Â∫äÔºàÁúüÂØ¶ÔºâÂúñÂÉèÔºâ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË°®ÊòéÔºåÂú®ÊúÄÂ∞èÂåñÁõ£Áù£Â∞çÊØîÊêçÂ§±ÊôÇÔºåÈ†òÂüüÈÅ©Êáâ„ÄÅÈ°ûÂà•ÂèØÂàÜÈõ¢ÊÄßÂíåÂàÜÈ°ûÊÄßËÉΩÂæóÂà∞ÊîπÂñÑ„ÄÇ

##### **Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**
2501.17343v1 by Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo

Quantizing deep neural networks ,reducing the precision (bit-width) of their
computations, can remarkably decrease memory usage and accelerate processing,
making these models more suitable for large-scale medical imaging applications
with limited computational resources. However, many existing methods studied
"fake quantization", which simulates lower precision operations during
inference, but does not actually reduce model size or improve real-world
inference speed. Moreover, the potential of deploying real 3D low-bit
quantization on modern GPUs is still unexplored. In this study, we introduce a
real post-training quantization (PTQ) framework that successfully implements
true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation
models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,
ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use
TensorRT to perform fake quantization for both weights and activations with
unlabeled calibration dataset. Second, we convert this fake quantization into
real quantization via TensorRT engine on real GPUs, resulting in real-world
reductions in model size and inference latency. Extensive experiments
demonstrate that our framework effectively performs 8-bit quantization on GPUs
without sacrificing model performance. This advancement enables the deployment
of efficient deep learning models in medical imaging applications where
computational resources are constrained. The code and models have been
released, including U-Net, TransUNet pretrained on the BTCV dataset for
abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset
for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and
VISTA3D pretrained on TotalSegmentator V2 for full body (104-label)
segmentation. https://github.com/hrlblab/PTQ.

ÊëòË¶ÅÔºö<paragraph>ÈáèÂåñÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºåÈôç‰ΩéÂÖ∂ËÆ°ÁÆóÁöÑÁ≤æÂ∫¶Ôºà‰ΩçÂÆΩÔºâÔºåÂèØ‰ª•ÊòæËëóÂáèÂ∞ëÂÜÖÂ≠ò‰ΩøÁî®ÈáèÂπ∂Âä†ÈÄüÂ§ÑÁêÜÔºå‰ΩøËøô‰∫õÊ®°ÂûãÊõ¥ÈÄÇÂêà‰∫éÂÖ∑ÊúâÊúâÈôêËÆ°ÁÆóËµÑÊ∫êÁöÑÂ§ßËßÑÊ®°ÂåªÂ≠¶ÂΩ±ÂÉèÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåËÆ∏Â§öÁé∞ÊúâÊñπÊ≥ïÁ†îÁ©∂‰∫Ü‚Äú‰º™ÈáèÂåñ‚ÄùÔºåÂÆÉÂú®Êé®ÁêÜÊúüÈó¥Ê®°ÊãüËæÉ‰ΩéÁ≤æÂ∫¶ÁöÑÊìç‰ΩúÔºå‰ΩÜÂÆûÈôÖ‰∏äÂπ∂Ê≤°ÊúâÂáèÂ∞ëÊ®°ÂûãÂ§ßÂ∞èÊàñÊèêÈ´òÂÆûÈôÖÊé®ÁêÜÈÄüÂ∫¶„ÄÇÊ≠§Â§ñÔºåÂú®Áé∞‰ª£ GPU ‰∏äÈÉ®ÁΩ≤ÁúüÊ≠£ÁöÑ 3D ‰Ωé‰ΩçÈáèÂåñÁöÑÊΩúÂäõ‰ªçÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÁúüÊ≠£ÁöÑËÆ≠ÁªÉÂêéÈáèÂåñ (PTQ) Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÊàêÂäüÂú∞Âú®ÊúÄÂÖàËøõÁöÑ (SOTA) 3D ÂåªÂ≠¶ÂàÜÂâ≤Ê®°ÂûãÔºàÂç≥ U-Net„ÄÅSegResNet„ÄÅSwinUNETR„ÄÅnnU-Net„ÄÅUNesT„ÄÅTransUNet„ÄÅST-UNet Âíå VISTA3DÔºâ‰∏äÂÆûÁé∞‰∫ÜÁúüÊ≠£ÁöÑ 8 ‰ΩçÈáèÂåñ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÊ∂âÂèä‰∏§‰∏™‰∏ªË¶ÅÊ≠•È™§„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨‰ΩøÁî® TensorRT ÂØπÊùÉÈáçÂíåÊøÄÊ¥ªËøõË°å‰º™ÈáèÂåñÔºåÂπ∂‰ΩøÁî®Êú™Ê†áËÆ∞ÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨Â∞ÜËøôÁßç‰º™ÈáèÂåñÈÄöËøáÁúüÂÆû GPU ‰∏äÁöÑ TensorRT ÂºïÊìéËΩ¨Êç¢‰∏∫ÁúüÊ≠£ÁöÑÈáèÂåñÔºå‰ªéËÄåÂú®Ê®°ÂûãÂ§ßÂ∞èÂíåÊé®ÁêÜÂª∂ËøüÊñπÈù¢ÂÆûÁé∞‰∫ÜÂÆûÈôÖÁöÑÂáèÂ∞ë„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú® GPU ‰∏äÊúâÊïàÂú∞ÊâßË°å 8 ‰ΩçÈáèÂåñÔºåËÄå‰∏ç‰ºöÁâ∫Áâ≤Ê®°ÂûãÊÄßËÉΩ„ÄÇËøô‰∏ÄËøõÊ≠•‰ΩøÂæóÂú®ËÆ°ÁÆóËµÑÊ∫êÂèóÈôêÁöÑÂåªÂ≠¶ÂΩ±ÂÉèÂ∫îÁî®‰∏≠ÈÉ®ÁΩ≤È´òÊïàÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÊàê‰∏∫ÂèØËÉΩ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂ∑≤ÁªèÂèëÂ∏ÉÔºåÂåÖÊã¨ U-Net„ÄÅTransUNETÔºåÂú® BTCV Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÁî®‰∫éËÖπÈÉ®Ôºà13 Ê†áÁ≠æÔºâÂàÜÂâ≤ÔºåUNesT Âú® Whole Brain Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÁî®‰∫éÂÖ®ËÑëÔºà133 Ê†áÁ≠æÔºâÂàÜÂâ≤Ôºå‰ª•Âèä nnU-Net„ÄÅSegResNet„ÄÅSwinUNETR Âíå VISTA3D Âú® TotalSegmentator V2 ‰∏äÈ¢ÑËÆ≠ÁªÉÁî®‰∫éÂÖ®Ë∫´Ôºà104 Ê†áÁ≠æÔºâÂàÜÂâ≤„ÄÇhttps://github.com/hrlblab/PTQ„ÄÇ</paragraph>

##### **Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**
2501.17338v1 by Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang

Generative Language Models rely on autoregressive decoding to produce the
output sequence token by token. Many tasks such as preference optimization,
require the model to produce task-level output consisting of multiple tokens
directly by selecting candidates from a pool as predictions. Determining a
task-level prediction from candidates using the ordinary token-level decoding
mechanism is constrained by time-consuming decoding and interrupted gradients
by discrete token selection. Existing works have been using decoding-free
candidate selection methods to obtain candidate probability from initial output
logits over vocabulary. Though these estimation methods are widely used, they
are not systematically evaluated, especially on end tasks. We introduce an
evaluation of a comprehensive collection of decoding-free candidate selection
approaches on a comprehensive set of tasks, including five multiple-choice QA
tasks with a small candidate pool and four clinical decision tasks with a
massive amount of candidates, some with 10k+ options. We evaluate the
estimation methods paired with a wide spectrum of foundation LMs covering
different architectures, sizes and training paradigms. The results and insights
from our analysis inform the future model design.

ÊëòË¶ÅÔºöÁîüÊàêË™ûË®ÄÊ®°Âûã‰æùÈù†Ëá™Ëø¥Ê≠∏Ëß£Á¢º‰æÜÈÄêÂÄãÁ¨¶ËôüÁî¢ÁîüËº∏Âá∫Â∫èÂàó„ÄÇË®±Â§ö‰ªªÂãôÔºàÂ¶ÇÂÅèÂ•ΩÊúÄ‰Ω≥ÂåñÔºâË¶ÅÊ±ÇÊ®°ÂûãÁõ¥Êé•ÂæûÂÄôÈÅ∏Ê±†‰∏≠ÈÅ∏ÊìáÈ†êÊ∏¨ÔºåÁî¢ÁîüÁî±Â§öÂÄãÁ¨¶ËôüÁµÑÊàêÁöÑ‰ªªÂãôÁ¥öÂà•Ëº∏Âá∫„ÄÇ‰ΩøÁî®‰∏ÄËà¨ÁöÑÁ¨¶ËôüÁ¥öÂà•Ëß£Á¢ºÊ©üÂà∂ÂæûÂÄôÈÅ∏ËÄÖ‰∏≠Á¢∫ÂÆö‰ªªÂãôÁ¥öÂà•È†êÊ∏¨ÂèóÂà∞ËÄóÊôÇÁöÑËß£Á¢ºÂíåÈõ¢Êï£Á¨¶ËôüÈÅ∏Êìá‰∏≠Êñ∑ÁöÑÊ¢ØÂ∫¶ÁöÑÁ¥ÑÊùü„ÄÇÁèæÊúâÂ∑•‰Ωú‰∏ÄÁõ¥‰ΩøÁî®ÁÑ°Ëß£Á¢ºÂÄôÈÅ∏ËÄÖÈÅ∏ÊìáÊñπÊ≥ïÂæûÂàùÂßãËº∏Âá∫ÈÇèËºØÂÄº‰∏≠Áç≤ÂæóÂÄôÈÅ∏ËÄÖÊ©üÁéá„ÄÇÂÑòÁÆ°ÈÄô‰∫õ‰º∞Ë®àÊñπÊ≥ïË¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÂÆÉÂÄë‰∏¶Êú™Á∂ìÈÅéÁ≥ªÁµ±Ë©ï‰º∞ÔºåÁâπÂà•ÊòØÂú®ÊúÄÁµÇ‰ªªÂãô‰∏ä„ÄÇÊàëÂÄëÈáùÂ∞çÂÖ®Èù¢ÁöÑ‰ªªÂãôÈõÜÔºåÂåÖÊã¨‰∫îÂÄãÂÖ∑ÊúâÂ∞èÂûãÂÄôÈÅ∏ËÄÖÊ±†ÁöÑÂ§öÈÅ∏È°åÂïèÁ≠î‰ªªÂãôÂíåÂõõÂÄãÂÖ∑ÊúâÂ§ßÈáèÂÄôÈÅ∏ËÄÖÁöÑËá®Â∫äÊ±∫Á≠ñ‰ªªÂãôÔºàÂÖ∂‰∏≠‰∏Ä‰∫õÊúâ 10k+ ÈÅ∏È†ÖÔºâÔºåÂ∞çÂÖ®Èù¢ÁöÑÁÑ°Ëß£Á¢ºÂÄôÈÅ∏ËÄÖÈÅ∏ÊìáÊñπÊ≥ïÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëË©ï‰º∞ËàáÂª£Ê≥õÂü∫Á§éË™ûË®ÄÊ®°ÂûãÈÖçÂ∞çÁöÑ‰º∞Ë®àÊñπÊ≥ïÔºåÈÄô‰∫õÊ®°ÂûãÊ∂µËìã‰∏çÂêåÁöÑÊû∂Êßã„ÄÅÂ§ßÂ∞èÂíåË®ìÁ∑¥ÁØÑ‰æã„ÄÇÊàëÂÄëÂàÜÊûêÁöÑÁµêÊûúÂíåË¶ãËß£ÁÇ∫Êú™‰æÜÁöÑÊ®°ÂûãË®≠Ë®àÊèê‰æõ‰∫ÜË≥áË®ä„ÄÇ

##### **Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**
2501.17326v1 by Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang

Clinical diagnosis prediction models, when provided with a patient's medical
history, aim to detect potential diseases early, facilitating timely
intervention and improving prognostic outcomes. However, the inherent scarcity
of patient data and large disease candidate space often pose challenges in
developing satisfactory models for this intricate task. The exploration of
leveraging Large Language Models (LLMs) for encapsulating clinical decision
processes has been limited. We introduce MERA, a clinical diagnosis prediction
model that bridges pertaining natural language knowledge with medical practice.
We apply hierarchical contrastive learning on a disease candidate ranking list
to alleviate the large decision space issue. With concept memorization through
fine-tuning, we bridge the natural language clinical knowledge with medical
codes. Experimental results on MIMIC-III and IV datasets show that MERA
achieves the state-of-the-art diagnosis prediction performance and dramatically
elevates the diagnosis prediction capabilities of generative LMs.

ÊëòË¶ÅÔºöËá®Â∫äË®∫Êñ∑È†êÊ∏¨Ê®°ÂûãÂú®Êèê‰æõÊÇ£ËÄÖÁóÖÊ≠∑ÁöÑÂêåÊôÇÔºåÊó®Âú®ÂèäÊó©ÁôºÁèæÊΩõÂú®ÁñæÁóÖÔºå‰øÉÈÄ≤ÂèäÊôÇÂπ≤È†ê‰∏¶ÊîπÂñÑÈ†êÂæåÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåÊÇ£ËÄÖÊï∏ÊìöÁöÑÂõ∫ÊúâÁ®ÄÁº∫ÊÄßÂíåÂ§ßÈáèÁöÑÁñæÁóÖÂÄôÈÅ∏Á©∫ÈñìÈÄöÂ∏∏Â∞çÈñãÁôº‰ª§‰∫∫ÊªøÊÑèÁöÑÊ®°Âûã‰ª•ÊáâÂ∞çÈÄôÈ†ÖË§áÈõúÁöÑ‰ªªÂãôÊßãÊàêÊåëÊà∞„ÄÇÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂ∞ÅË£ùËá®Â∫äÊ±∫Á≠ñÊµÅÁ®ãÁöÑÊé¢Á¥¢ÂèóÂà∞ÈôêÂà∂„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü MERAÔºåÈÄôÊòØ‰∏ÄÂÄãËá®Â∫äË®∫Êñ∑È†êÊ∏¨Ê®°ÂûãÔºåÂÆÉÂ∞áÁõ∏ÈóúÁöÑËá™ÁÑ∂Ë™ûË®ÄÁü•Ë≠òËàáÈÜ´ÁôÇÂØ¶Ë∏êËÅØÁπ´Ëµ∑‰æÜ„ÄÇÊàëÂÄëÂú®ÁñæÁóÖÂÄôÈÅ∏ÊéíÂêçÊ∏ÖÂñÆ‰∏äÊáâÁî®ÂàÜÂ±§Â∞çÊØîÂ≠∏ÁøíÔºå‰ª•Á∑©Ëß£Â§ßÂûãÊ±∫Á≠ñÁ©∫ÈñìÂïèÈ°å„ÄÇÈÄöÈÅéÂæÆË™øÊ¶ÇÂøµË®òÊÜ∂ÔºåÊàëÂÄëÂ∞áËá™ÁÑ∂Ë™ûË®ÄËá®Â∫äÁü•Ë≠òËàáÈÜ´ÁôÇ‰ª£Á¢ºËÅØÁπ´Ëµ∑‰æÜ„ÄÇÂú® MIMIC-III Âíå IV Êï∏ÊìöÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåMERA ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑË®∫Êñ∑È†êÊ∏¨ÊÄßËÉΩÔºå‰∏¶È°ØËëóÊèêÂçá‰∫ÜÁîüÊàêÂºè LM ÁöÑË®∫Êñ∑È†êÊ∏¨ËÉΩÂäõ„ÄÇ

##### **Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**
2501.17286v1 by Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu

Background: The radiation oncology clinical practice involves many steps
relying on the dynamic interplay of abundant text data. Large language models
have displayed remarkable capabilities in processing complex text information.
But their direct applications in specific fields like radiation oncology remain
underexplored.
  Purpose: This study aims to investigate whether fine-tuning LLMs with domain
knowledge can improve the performance on Task (1) treatment regimen generation,
Task (2) treatment modality selection (photon, proton, electron, or
brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.
  Methods: Data for 15,724 patient cases were extracted. Cases where patients
had a single diagnostic record, and a clearly identifiable primary treatment
plan were selected for preprocessing and manual annotation to have 7,903 cases
of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.
Each case was used to construct a pair consisting of patient diagnostics
details and an answer (treatment regimen, treatment modality, or ICD-10 code
respectively) for the supervised fine-tuning of these three tasks. Open source
LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the
Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for
the fine-tuned models and original models. Clinical evaluation was performed on
Task (1) by radiation oncologists, while precision, recall, and F-1 score were
evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used
to statistically analyze the results.
  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with
p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the
fine-tuned LLMs-generated treatment regimens were clinically acceptable.
Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÊîæÂ∞ÑËÇøÁò§‰∏¥Â∫äÂÆûË∑µÊ∂âÂèäËÆ∏Â§öÊ≠•È™§ÔºåËøô‰∫õÊ≠•È™§‰æùËµñ‰∫é‰∏∞ÂØåÊñáÊú¨Êï∞ÊçÆÁöÑÂä®ÊÄÅ‰∫§‰∫í„ÄÇÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÊñáÊú¨‰ø°ÊÅØÊñπÈù¢Ë°®Áé∞Âá∫‰∫ÜÂçìË∂äÁöÑËÉΩÂäõ„ÄÇ‰ΩÜÂÆÉ‰ª¨Âú®ÊîæÂ∞ÑËÇøÁò§Á≠âÁâπÂÆöÈ¢ÜÂüüÁöÑÁõ¥Êé•Â∫îÁî®‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ
ÁõÆÁöÑÔºöÊú¨Á†îÁ©∂Êó®Âú®Ë∞ÉÊü•ÈÄöËøáÈ¢ÜÂüüÁü•ËØÜÂæÆË∞É LLM ÊòØÂê¶ÂèØ‰ª•ÊèêÈ´ò‰ªªÂä° (1) Ê≤ªÁñóÊñπÊ°àÁîüÊàê„ÄÅ‰ªªÂä° (2) Ê≤ªÁñóÊñπÂºèÈÄâÊã©ÔºàÂÖâÂ≠ê„ÄÅË¥®Â≠ê„ÄÅÁîµÂ≠êÊàñËøëË∑ùÁ¶ªÊîæÂ∞ÑÊ≤ªÁñóÔºâÂíå‰ªªÂä° (3) ÊîæÂ∞ÑËÇøÁò§‰∏≠ ICD-10 ‰ª£Á†ÅÈ¢ÑÊµãÁöÑÊÄßËÉΩ„ÄÇ
ÊñπÊ≥ïÔºöÊèêÂèñ‰∫Ü 15,724 ‰æãÊÇ£ËÄÖÁóÖ‰æãÁöÑÊï∞ÊçÆ„ÄÇÈÄâÊã©‰∫ÜÊÇ£ËÄÖÊúâÂçï‰∏ÄËØäÊñ≠ËÆ∞ÂΩï‰∏îÊúâÊòéÁ°ÆÂèØËØÜÂà´ÁöÑ‰∏ªË¶ÅÊ≤ªÁñóËÆ°ÂàíÁöÑÁóÖ‰æãÔºåËøõË°åÈ¢ÑÂ§ÑÁêÜÂíåÊâãÂä®Ê≥®ÈáäÔºåÂæóÂà∞ 7,903 ‰æãÊÇ£ËÄÖËØäÊñ≠„ÄÅÊ≤ªÁñóËÆ°Âàí„ÄÅÊ≤ªÁñóÊñπÂºèÂíå ICD-10 ‰ª£Á†Å„ÄÇÊØè‰∏™ÁóÖ‰æãÈÉΩÁî®‰∫éÊûÑÂª∫‰∏ÄÂØπÔºåÂåÖÊã¨ÊÇ£ËÄÖËØäÊñ≠ËØ¶ÊÉÖÂíåÁ≠îÊ°àÔºàÂàÜÂà´ÊòØÊ≤ªÁñóÊñπÊ°à„ÄÅÊ≤ªÁñóÊñπÂºèÊàñ ICD-10 ‰ª£Á†ÅÔºâÔºåÁî®‰∫éËøô‰∏â‰∏™‰ªªÂä°ÁöÑÁõëÁù£ÂæÆË∞É„ÄÇÂºÄÊ∫ê LLaMA2-7B Âíå Mistral-7B Ê®°ÂûãË¢´Áî®‰∫é‰ΩøÁî®‰ΩéÁß©ÈÄºËøëÊñπÊ≥ïËøõË°åÂæÆË∞É„ÄÇÊä•Âëä‰∫ÜÂæÆË∞ÉÊ®°ÂûãÂíåÂéüÂßãÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíå ROUGE-1 ÂàÜÊï∞„ÄÇ‰ªªÂä° (1) Áî±ÊîæÂ∞ÑËÇøÁò§ÁßëÂåªÂ∏àËøõË°å‰∏¥Â∫äËØÑ‰º∞ÔºåËÄå‰ªªÂä° (2) Âíå (3) ÂàôËØÑ‰º∞‰∫ÜÁ≤æÁ°ÆÂ∫¶„ÄÅÂè¨ÂõûÁéáÂíå F-1 ÂàÜÊï∞„ÄÇÂçï‰æß Wilcoxon Á¨¶Âè∑Áß©Ê£ÄÈ™åÁî®‰∫éÂØπÁªìÊûúËøõË°åÁªüËÆ°ÂàÜÊûê„ÄÇ
ÁªìÊûúÔºöÂæÆË∞ÉÂêéÁöÑ LLM Âú®ÊâÄÊúâ‰ªªÂä°‰∏≠ÈÉΩ‰ºò‰∫éÂéüÂßã LLMÔºåp ÂÄº <= 0.001„ÄÇ‰∏¥Â∫äËØÑ‰º∞Ë°®ÊòéÔºåË∂ÖËøá 60% ÁöÑÂæÆË∞É LLM ÁîüÊàêÁöÑÊ≤ªÁñóÊñπÊ°àÂú®‰∏¥Â∫ä‰∏äÊòØÂèØÊé•ÂèóÁöÑ„ÄÇÁ≤æÁ°ÆÂ∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∞ÊòæÁ§∫ÂæÆË∞ÉÂêéÁöÑ LLM ÊÄßËÉΩÂæóÂà∞ÊîπÂñÑ„ÄÇ</paragraph>

##### **ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**
2501.17260v1 by Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee

Optical Coherence Tomography (OCT) is a non-invasive imaging modality
essential for diagnosing various eye diseases. Despite its clinical
significance, developing OCT-based diagnostic tools faces challenges, such as
limited public datasets, sparse annotations, and privacy concerns. Although
deep learning has made progress in automating OCT analysis, these challenges
remain unresolved. To address these limitations, we introduce the Vision
Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a
novel framework designed to enhance feature extraction and improve diagnostic
accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,
Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining
phase leverages the OCTMNIST dataset (97,477 unlabeled images across four
disease classes) with data augmentation to create dual-augmented views. A
Vision Transformer (ViT-Base) backbone extracts features, while a negative
cosine similarity loss aligns feature representations. Pretraining is conducted
over 50 epochs with a learning rate of 0.0001 and momentum of 0.999.
Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using
10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of
0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming
existing SSP-based methods.

ÊëòË¶ÅÔºöÂÖâÂ≠∏Áõ∏Âπ≤Êñ∑Â±§ÊéÉÊèèÔºàOCTÔºâÊòØ‰∏ÄÁ®ÆÈùû‰æµÂÖ•ÂºèÂΩ±ÂÉèÊ®°ÂºèÔºåÂ∞çÊñºË®∫Êñ∑ÂêÑÁ®ÆÁúºÁñæËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°ÂÖ∂Ëá®Â∫äÊÑèÁæ©ÈáçÂ§ßÔºå‰ΩÜÈñãÁôºÂü∫Êñº OCT ÁöÑË®∫Êñ∑Â∑•ÂÖ∑Èù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶ÇÂÖ¨ÂÖ±Êï∏ÊìöÈõÜÊúâÈôê„ÄÅË®ªËß£Á®ÄÁñèÂíåÈö±ÁßÅÂïèÈ°å„ÄÇÂÑòÁÆ°Ê∑±Â∫¶Â≠∏ÁøíÂú®Ëá™ÂãïÂåñ OCT ÂàÜÊûêÊñπÈù¢ÂèñÂæó‰∫ÜÈÄ≤Â±ïÔºå‰ΩÜÈÄô‰∫õÊåëÊà∞‰ªçÁÑ∂Ê≤íÊúâËß£Ê±∫„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂü∫Êñº Vision Transformer ÁöÑÈõôÊµÅËá™Áõ£Áù£È†êË®ìÁ∑¥Á∂≤Ë∑ØÔºàViT-2SPNÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∑ÁâπÂæµÊèêÂèñ‰∏¶ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÇViT-2SPN Êé°Áî®‰∏âÈöéÊÆµÂ∑•‰ΩúÊµÅÁ®ãÔºöÁõ£Áù£È†êË®ìÁ∑¥„ÄÅËá™Áõ£Áù£È†êË®ìÁ∑¥ÔºàSSPÔºâÂíåÁõ£Áù£ÂæÆË™ø„ÄÇÈ†êË®ìÁ∑¥ÈöéÊÆµÂà©Áî® OCTMNIST Êï∏ÊìöÈõÜÔºàË∑®Ë∂äÂõõÂÄãÁñæÁóÖÈ°ûÂà•ÁöÑ 97,477 ÂºµÊú™Ê®ôË®òÂΩ±ÂÉèÔºâÂíåÊï∏ÊìöÊì¥ÂÖÖ‰æÜÂª∫Á´ãÈõôÈáçÊì¥ÂÖÖÁöÑÊ™¢Ë¶ñ„ÄÇË¶ñË¶∫ËΩâÊèõÂô®ÔºàViT-BaseÔºâ‰∏ªÂππÊèêÂèñÁâπÂæµÔºåËÄåË≤†È§òÂº¶Áõ∏‰ººÂ∫¶ÊêçÂ§±ÂâáÊ†°Ê∫ñÁâπÂæµË°®Á§∫„ÄÇÈ†êË®ìÁ∑¥Âú® 50 ÂÄã‰∏ñ‰ª£‰∏≠ÈÄ≤Ë°åÔºåÂ≠∏ÁøíÁéáÁÇ∫ 0.0001ÔºåÂãïËÉΩÁÇ∫ 0.999„ÄÇÂæÆË™øÂú® OCTMNIST ÁöÑÂàÜÂ±§ 5.129% Â≠êÈõÜ‰∏äÂü∑Ë°åÔºå‰ΩøÁî® 10 ÂÄç‰∫§ÂèâÈ©óË≠â„ÄÇViT-2SPN ÈÅîÂà∞‰∫Ü 0.93 ÁöÑÂπ≥Âùá AUC„ÄÅ0.77 ÁöÑÊ∫ñÁ¢∫Áéá„ÄÅ0.81 ÁöÑÁ≤æÁ¢∫Â∫¶„ÄÅ0.75 ÁöÑÂè¨ÂõûÁéáÂíå 0.76 ÁöÑ F1 ÂàÜÊï∏ÔºåÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Êñº SSP ÁöÑÊñπÊ≥ï„ÄÇ

##### **A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**
2501.17160v1 by Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham

Early detection of COVID-19 is crucial for effective treatment and
controlling its spread. This study proposes a novel hybrid deep learning model
for detecting COVID-19 from CT scan images, designed to assist overburdened
medical professionals. Our proposed model leverages the strengths of VGG16,
DenseNet121, and MobileNetV2 to extract features, followed by Principal
Component Analysis (PCA) for dimensionality reduction, after which the features
are stacked and classified using a Support Vector Classifier (SVC). We
conducted comparative analysis between the proposed hybrid model and individual
pre-trained CNN models, using a dataset of 2,108 training images and 373 test
images comprising both COVID-positive and non-COVID images. Our proposed hybrid
model achieved an accuracy of 98.93%, outperforming the individual models in
terms of precision, recall, F1 scores, and ROC curve performance.

ÊëòË¶ÅÔºöÊó©ÊúüÂÅµÊ∏¨ COVID-19 Â∞çÊúâÊïàÊ≤ªÁôÇÂíåÊéßÂà∂ÂÖ∂ÂÇ≥Êí≠Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ∑∑ÂêàÊ®°ÂûãÔºåÁî®ÊñºÂæûÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨ COVID-19ÔºåÊó®Âú®ÂçîÂä©Ë≤†ÊìîÈÅéÈáçÁöÑÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÂà©Áî® VGG16„ÄÅDenseNet121 Âíå MobileNetV2 ÁöÑÂÑ™Èªû‰æÜËêÉÂèñÁâπÂæµÔºåÊé•ËëóÈÄ≤Ë°å‰∏ªÊàêÂàÜÂàÜÊûê (PCA) ‰ª•ÈÄ≤Ë°åÈôçÁ∂≠ÔºåÁÑ∂ÂæåÂ∞áÁâπÂæµÂ†ÜÁñä‰∏¶‰ΩøÁî®ÊîØÊåÅÂêëÈáèÂàÜÈ°ûÂô® (SVC) ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÂ∞çÊèêÂá∫ÁöÑÊ∑∑ÂêàÊ®°ÂûãÂíåÂÄãÂà•È†êË®ìÁ∑¥ÁöÑ CNN Ê®°ÂûãÈÄ≤Ë°åÊØîËºÉÂàÜÊûêÔºå‰ΩøÁî®ÂåÖÂê´ 2,108 ÂºµË®ìÁ∑¥ÂΩ±ÂÉèÂíå 373 ÂºµÊ∏¨Ë©¶ÂΩ±ÂÉèÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ COVID-19 ÈôΩÊÄßÂΩ±ÂÉèÂíåÈùû COVID-19 ÂΩ±ÂÉè„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ∑∑ÂêàÊ®°ÂûãÈÅîÂà∞‰∫Ü 98.93% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂú®Á≤æÊ∫ñÂ∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅF1 ÂàÜÊï∏Âíå ROC Êõ≤Á∑öÊïàËÉΩÊñπÈù¢ÂÑ™ÊñºÂÄãÂà•Ê®°Âûã„ÄÇ

##### **Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**
2501.17152v1 by Reza Ghorbani, Jyothi Rikhab Chand, Chu-Yu Lee, Mathews Jacob, Merry Mani

Three-dimensional (3D) multi-slab acquisition is a technique frequently
employed in high-resolution diffusion-weighted MRI in order to achieve the best
signal-to-noise ratio (SNR) efficiency. However, this technique is limited by
slab boundary artifacts that cause intensity fluctuations and aliasing between
slabs which reduces the accuracy of anatomical imaging. Addressing this issue
is crucial for advancing diffusion MRI quality and making high-resolution
imaging more feasible for clinical and research applications. In this work, we
propose a regularized slab profile encoding (PEN) method within a Plug-and-Play
ADMM framework, incorporating multi-scale energy (MuSE) regularization to
effectively improve the slab combined reconstruction. Experimental results
demonstrate that the proposed method significantly improves image quality
compared to non-regularized and TV-regularized PEN approaches. The regularized
PEN framework provides a more robust and efficient solution for high-resolution
3D diffusion MRI, potentially enabling clearer, more reliable anatomical
imaging across various applications.

ÊëòË¶ÅÔºö‰∏âÁ∂≠ (3D) Â§öÂ±§ÊùøÊì∑ÂèñÊòØ‰∏ÄÁ®ÆÊäÄË°ìÔºåÁ∂ìÂ∏∏‰ΩøÁî®ÊñºÈ´òËß£ÊûêÂ∫¶Êì¥Êï£Âä†Ê¨ä MRIÔºå‰ª•ÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑË®äËôüÈõúË®äÊØî (SNR) ÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÊäÄË°ìÂèóÂà∞Â±§ÊùøÈÇäÁïåÂÅΩÂΩ±ÁöÑÈôêÂà∂ÔºåÊúÉÈÄ†ÊàêÂº∑Â∫¶Ê≥¢ÂãïÂíåÂ±§Êùø‰πãÈñìÁöÑÊ∑∑ÁñäÔºåÈôç‰ΩéËß£ÂâñÂΩ±ÂÉèÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÂ∞çÊñºÊèêÂçáÊì¥Êï£ MRI ÂìÅË≥™Ëá≥ÈóúÈáçË¶ÅÔºå‰∏¶‰ΩøÈ´òËß£ÊûêÂ∫¶ÂΩ±ÂÉèÊõ¥ÈÅ©Áî®ÊñºËá®Â∫äÂíåÁ†îÁ©∂ÊáâÁî®„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂú® Plug-and-Play ADMM Êû∂ÊßãÂÖßÊèêÂá∫Ê≠£Ë¶èÂåñÁöÑÂ±§ÊùøËº™ÂªìÁ∑®Á¢º (PEN) ÊñπÊ≥ïÔºå‰∏¶ÁµêÂêàÂ§öÂ∞∫Â∫¶ËÉΩÈáè (MuSE) Ê≠£Ë¶èÂåñÔºå‰ª•ÊúâÊïàÊîπÂñÑÂ±§ÊùøÁµÑÂêàÈáçÂª∫„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåËàáÈùûÊ≠£Ë¶èÂåñÂíå TV Ê≠£Ë¶èÂåñ PEN ÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÈ°ØËëóÊèêÂçá‰∫ÜÂΩ±ÂÉèÂìÅË≥™„ÄÇÊ≠£Ë¶èÂåñÁöÑ PEN Êû∂ÊßãÁÇ∫È´òËß£ÊûêÂ∫¶ 3D Êì¥Êï£ MRI Êèê‰æõÊõ¥Âº∑Âõ∫‰∏îÊúâÊïàÁéáÁöÑËß£Ê±∫ÊñπÊ°àÔºåÊΩõÂú®ÂèØÂØ¶ÁèæÊõ¥Ê∏ÖÊô∞„ÄÅÊõ¥ÂèØÈù†ÁöÑËß£ÂâñÂΩ±ÂÉèÔºåÈÅ©Áî®ÊñºÂêÑÁ®ÆÊáâÁî®„ÄÇ

##### **Irony Detection, Reasoning and Understanding in Zero-shot Learning**
2501.16884v1 by Peiling Yi, Yuhan Xia

Irony is a powerful figurative language (FL) on social media that can
potentially mislead various NLP tasks, such as recommendation systems,
misinformation checks, and sentiment analysis. Understanding the implicit
meaning of this kind of subtle language is essential to mitigate irony's
negative impact on NLP tasks. However, building models to understand irony
presents a unique set of challenges, because irony is a complex form of
language that often relies on context, tone, and subtle cues to convey meaning
that is opposite or different from the literal interpretation. Large language
models, such as ChatGPT, are increasingly able to capture implicit and
contextual information. In this study, we investigate the generalization,
reasoning and understanding ability of ChatGPT on irony detection across six
different genre irony detection datasets. Our findings suggest that ChatGPT
appears to show an enhanced language understanding and reasoning ability. But
it needs to be very careful in prompt engineering design. Thus, we propose a
prompt engineering design framework IDADP to achieve higher irony detection
accuracy, improved understanding of irony, and more effective explanations
compared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain
via experiments that the practice generated under the framework is likely to be
the promised solution to resolve the generalization issues of LLMs.

ÊëòË¶ÅÔºöÂèçË´∑ÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÁ§æ‰∫§Â™íÈ´îÊØîÂñªË™ûË®Ä (FL)ÔºåÂèØËÉΩÊúÉË™§Â∞éÂêÑÁ®Æ NLP ‰ªªÂãôÔºå‰æãÂ¶ÇÊé®Ëñ¶Á≥ªÁµ±„ÄÅÈåØË™§Ë®äÊÅØÊ™¢Êü•ÂíåÊÉÖÁ∑íÂàÜÊûê„ÄÇÁêÜËß£ÈÄôÁ®ÆÂæÆÂ¶ôË™ûË®ÄÁöÑÈö±Âê´Âê´Áæ©Â∞çÊñºÊ∏õËºïÂèçË´∑Â∞ç NLP ‰ªªÂãôÁöÑË≤†Èù¢ÂΩ±ÈüøËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊ®°Âûã‰æÜÁêÜËß£ÂèçË´∑ÊúÉÂ∏∂‰æÜ‰∏ÄÁ≥ªÂàóÁç®ÁâπÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫ÂèçË´∑ÊòØ‰∏ÄÁ®ÆË§áÈõúÁöÑË™ûË®ÄÂΩ¢ÂºèÔºåÈÄöÂ∏∏‰æùË≥¥Êñº‰∏ä‰∏ãÊñá„ÄÅË™ûÊ∞£ÂíåÂæÆÂ¶ôÁöÑÁ∑öÁ¥¢‰æÜÂÇ≥ÈÅîËàáÂ≠óÈù¢Ëß£ÈáãÁõ∏ÂèçÊàñ‰∏çÂêåÁöÑÂê´Áæ©„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºå‰æãÂ¶Ç ChatGPTÔºåË∂ä‰æÜË∂äËÉΩÂ§†ÊçïÊçâÈö±Âê´Âíå‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü ChatGPT Âú®ÂÖ≠ÂÄã‰∏çÂêåÈ°ûÂûãÂèçË´∑Ê™¢Ê∏¨Êï∏ÊìöÈõÜ‰∏äÁöÑÂèçË´∑Ê™¢Ê∏¨ÁöÑÊ¶ÇÊã¨„ÄÅÊé®ÁêÜÂíåÁêÜËß£ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåChatGPT ‰ºº‰πéË°®ÁèæÂá∫Â¢ûÂº∑ÁöÑË™ûË®ÄÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ‰ΩÜÂÆÉÈúÄË¶ÅÂú®ÊèêÁ§∫Â∑•Á®ãË®≠Ë®à‰∏≠ÈùûÂ∏∏Â∞èÂøÉ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊèêÁ§∫Â∑•Á®ãË®≠Ë®àÊ°ÜÊû∂ IDADPÔºå‰ª•ÂØ¶ÁèæÊõ¥È´òÁöÑÂèçË´∑Ê™¢Ê∏¨Ê∫ñÁ¢∫Â∫¶„ÄÅÊîπÈÄ≤ÁöÑÂèçË´∑ÁêÜËß£‰ª•ÂèäËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑ ChatGPT Èõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÁõ∏ÊØîÊõ¥ÊúâÊïàÁöÑËß£Èáã„ÄÇ‰∏¶ÈÄöÈÅéÂØ¶È©óÁ¢∫ÂÆöÂú®Ë©≤Ê°ÜÊû∂‰∏ãÁî¢ÁîüÁöÑÂØ¶Ë∏êÂæàÂèØËÉΩÊòØËß£Ê±∫ LLM Ê¶ÇÊã¨ÂïèÈ°åÁöÑÊâøË´æËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**
2501.17207v1 by Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang

Functional brain connectome is crucial for deciphering the neural mechanisms
underlying cognitive functions and neurological disorders. Graph deep learning
models have recently gained tremendous popularity in this field. However, their
actual effectiveness in modeling the brain connectome remains unclear. In this
study, we re-examine graph deep learning models based on four large-scale
neuroimaging studies encompassing diverse cognitive and clinical outcomes.
Surprisingly, we find that the message aggregation mechanism, a hallmark of
graph deep learning models, does not help with predictive performance as
typically assumed, but rather consistently degrades it. To address this issue,
we propose a hybrid model combining a linear model with a graph attention
network through dual pathways, achieving robust predictions and enhanced
interpretability by revealing both localized and global neural connectivity
patterns. Our findings urge caution in adopting complex deep learning models
for functional brain connectome analysis, emphasizing the need for rigorous
experimental designs to establish tangible performance gains and perhaps more
importantly, to pursue improvements in model interpretability.

ÊëòË¶ÅÔºöÂäüËÉΩÊÄßËÖ¶ÈÄ£Êé•È´îÂ∞çÊñºÁ†¥Ë≠ØË™çÁü•ÂäüËÉΩÂíåÁ•ûÁ∂ìÁñæÁóÖËÉåÂæåÁöÑÊ©üÂà∂Ëá≥ÈóúÈáçË¶Å„ÄÇÂúñÂΩ¢Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊúÄËøëÂú®ÈÄôÂÄãÈ†òÂüüÁç≤ÂæóÊ•µÂ§ßÁöÑÊ≠°Ëøé„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®Âª∫Ê®°ËÖ¶ÈÄ£Êé•È´îÁöÑÂØ¶ÈöõÊïàËÉΩ‰ªç‰∏çÊòéÁ¢∫„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊ†πÊìöÂõõÈ†ÖÊ∂µËìã‰∏çÂêåË™çÁü•ÂíåËá®Â∫äÁµêÊûúÁöÑÂ§ßË¶èÊ®°Á•ûÁ∂ìÂΩ±ÂÉèÁ†îÁ©∂ÔºåÈáçÊñ∞Ê™¢Ë¶ñÂúñÂΩ¢Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæË®äÊÅØËÅöÂêàÊ©üÂà∂ÔºàÂúñÂΩ¢Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊ®ôË™åÔºâ‰∏¶‰∏çÂÉèÈÄöÂ∏∏ÂÅáË®≠ÁöÑÈÇ£Ê®£ÊúâÂä©ÊñºÈ†êÊ∏¨ÊïàËÉΩÔºåÂèçËÄåÊåÅÁ∫åÈôç‰ΩéÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÊ®°ÂûãÔºåÈÄèÈÅéÈõôË∑ØÂæëÁµêÂêàÁ∑öÊÄßÊ®°ÂûãËàáÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÔºåÈÅîÊàêÁ©©ÂÅ•ÁöÑÈ†êÊ∏¨ÂíåÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÔºåÊñπÊ≥ïÊòØÊè≠Èú≤Â±ÄÈÉ®ÂíåÊï¥È´îÁöÑÁ•ûÁ∂ìÈÄ£Êé•Ê®°Âºè„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊï¶‰øÉÂú®Êé°Áî®Ë§áÈõúÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÂäüËÉΩÊÄßËÖ¶ÈÄ£Êé•È´îÂàÜÊûêÊôÇ‰øùÊåÅË¨πÊÖéÔºåÂº∑Ë™øÈúÄË¶ÅÂö¥Ë¨πÁöÑÂØ¶È©óË®≠Ë®àÔºå‰ª•Âª∫Á´ãÂÖ∑È´îÁöÑÊïàËÉΩÂ¢ûÁõäÔºåÊàñË®±Êõ¥ÈáçË¶ÅÁöÑÊòØÔºåËøΩÊ±ÇÊ®°ÂûãÂèØËß£ÈáãÊÄßÁöÑÊîπÈÄ≤„ÄÇ

##### **Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**
2501.17206v1 by Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao

This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Á¥¢‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÊï¥ÂêàÁ§æÊúÉËºîÂä©Ê©üÂô®‰∫∫„ÄÅÂº∑ÂåñÂ≠∏Áøí (RL)„ÄÅÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåËá®Â∫äÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÊñºÊ®°Êì¨Áí∞Â¢É‰∏≠Ôºå‰ª•Êé®ÈÄ≤Â§±Êô∫ÁóáÁÖßË≠∑„ÄÇÈÄôÁ®ÆÊï¥ÂêàËß£Ê±∫‰∫ÜÂ§±Êô∫ÁóáÁÖßË≠∑‰∏≠Á§æÊúÉËºîÂä©Ê©üÂô®‰∫∫ÂØ¶È©óÊï∏ÊìöÊúâÈôêÁöÑÈáçÂ§ßÊåëÊà∞ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂãïÊÖãÁöÑÊ®°Êì¨Áí∞Â¢ÉÔºåÁúüÂØ¶Âú∞Ê®°Êì¨Â§±Êô∫ÁóáÊÇ£ËÄÖ (PLWD) ÂíåÊ©üÂô®‰∫∫ÁÖßË≠∑ËÄÖ‰πãÈñìÁöÑ‰∫íÂãï„ÄÇÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÂºïÂÖ•‰∫ÜÊ©üÁéáÊ®°Âûã‰æÜË°®Á§∫ PLWD ÁöÑË™çÁü•ÂíåÊÉÖÁ∑íÁãÄÊÖãÔºå‰∏¶ÁµêÂêà‰∫ÜÂü∫Êñº LLM ÁöÑË°åÁÇ∫Ê®°Êì¨‰æÜÊ®°Êì¨‰ªñÂÄëÁöÑÂèçÊáâ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈñãÁôº‰∏¶Ë®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÈÅ©ÊáâÊÄß RL Á≥ªÁµ±Ôºå‰Ωø Pepper Á≠â‰∫∫ÂΩ¢Ê©üÂô®‰∫∫ËÉΩÂ§†Ê†πÊìö PLWD ÁöÑË™çÁü•ÂíåÊÉÖÁ∑íÁãÄÊÖãÊèê‰æõÊÉÖÂ¢ÉÊÑüÁü•ÂíåÂÄã‰∫∫ÂåñÁöÑ‰∫íÂãïÂíåÂçîÂä©„ÄÇË©≤Êû∂Êßã‰πüÊ¶ÇÊã¨Âà∞ÈõªËÖ¶‰ª£ÁêÜÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÁöÑÂ§öÂäüËÉΩÊÄß„ÄÇÁµêÊûúË°®ÊòéÔºåÁî± LLM Â¢ûÂº∑ÁöÑ RL Á≥ªÁµ±ÊúâÊïàÂú∞Ëß£ÈáãÂíåÂõûÊáâ PLWD ÁöÑË§áÈõúÈúÄÊ±ÇÔºåÊèê‰æõÈáèË∫´ÊâìÈÄ†ÁöÑÁÖßË≠∑Á≠ñÁï•„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÈÄèÈÅéÊèê‰æõ‰∏ÄÂÄãÂèØËá™Ë®ÇÁöÑ AI È©ÖÂãïÁÖßË≠∑Âπ≥Âè∞Ôºå‰øÉÈÄ≤Â∞çÂ§±Êô∫ÁóáÁõ∏ÈóúÊåëÊà∞ÁöÑ‰∫ÜËß£Ôºå‰∏¶‰øÉÈÄ≤ËºîÂä©ÊäÄË°ìÁöÑÂçî‰ΩúÂâµÊñ∞ÔºåÁÇ∫‰∫∫Ê©ü‰∫íÂãïÂíå‰∫∫Ê©ü‰∫íÂãïÂÅöÂá∫Ë≤¢Áçª„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÂèØËÉΩÊèêÈ´ò PLWD ÁöÑÁç®Á´ãÊÄßÂíåÁîüÊ¥ªÂìÅË≥™ÔºåÂêåÊôÇÊ∏õËºïÁÖßË≠∑ËÄÖÁöÑË≤†ÊìîÔºåÂº∑Ë™ø‰∫Ü‰∫íÂãïÂ∞éÂêë AI Á≥ªÁµ±Âú®Â§±Êô∫ÁóáÁÖßË≠∑‰∏≠ÁöÑËΩâÂûã‰ΩúÁî®„ÄÇ

##### **Efficient Knowledge Distillation of SAM for Medical Image Segmentation**
2501.16740v1 by Kunal Dasharath Patil, Gowthamaan Palani, Ganapathy Krishnamurthi

The Segment Anything Model (SAM) has set a new standard in interactive image
segmentation, offering robust performance across various tasks. However, its
significant computational requirements limit its deployment in real-time or
resource-constrained environments. To address these challenges, we propose a
novel knowledge distillation approach, KD SAM, which incorporates both encoder
and decoder optimization through a combination of Mean Squared Error (MSE) and
Perceptual Loss. This dual-loss framework captures structural and semantic
features, enabling the student model to maintain high segmentation accuracy
while reducing computational complexity. Based on the model evaluation on
datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast
Ultrasound, we demonstrate that KD SAM achieves comparable or superior
performance to the baseline models, with significantly fewer parameters. KD SAM
effectively balances segmentation accuracy and computational efficiency, making
it well-suited for real-time medical image segmentation applications in
resource-constrained environments.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) Â∑≤Âú®‰∫íÂãïÂºèÂΩ±ÂÉèÂàÜÂâ≤‰∏≠Ê®πÁ´ãÊñ∞Ê®ôÊ∫ñÔºåÂú®ÂêÑÈ†Ö‰ªªÂãô‰∏≠ÁöÜËÉΩÊèê‰æõÁ©©ÂÅ•ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÈæêÂ§ßÁöÑÈÅãÁÆóÈúÄÊ±ÇÈôêÂà∂‰∫ÜÂÆÉÂú®Âç≥ÊôÇÊàñË≥áÊ∫êÂèóÈôêÁí∞Â¢É‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁü•Ë≠òËí∏È§æÊñπÊ≥ïÔºåKD SAMÔºåÂÆÉÈÄèÈÅéÁµêÂêàÂùáÊñπË™§Â∑Æ (MSE) ÂíåÊÑüÁü•ÊêçÂ§±ÔºåÂ∞áÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÊúÄ‰Ω≥ÂåñÁ¥çÂÖ•ÂÖ∂‰∏≠„ÄÇÊ≠§ÈõôÈáçÊêçÂ§±Êû∂ÊßãÊì∑ÂèñÁµêÊßãÂíåË™ûÁæ©ÁâπÂæµÔºåËÆìÂ≠∏ÁîüÊ®°ÂûãËÉΩÂ§†Âú®Èôç‰ΩéÈÅãÁÆóË§áÈõúÂ∫¶ÁöÑÂêåÊôÇÔºåÁ∂≠ÊåÅÈ´òÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶„ÄÇÊ†πÊìöÂú® Kvasir-SEG„ÄÅISIC 2017„ÄÅËÉéÂÖíÈ†≠ÈÉ®Ë∂ÖÈü≥Ê≥¢Âíå‰π≥ÊàøË∂ÖÈü≥Ê≥¢Á≠âË≥áÊñôÈõÜ‰∏äÁöÑÊ®°ÂûãË©ï‰º∞ÔºåÊàëÂÄëË≠âÊòé KD SAM ÈÅîÂà∞‰∫ÜËàáÂü∫Ê∫ñÊ®°ÂûãÁõ∏Áï∂ÊàñÊõ¥ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºå‰∏îÂèÉÊï∏ÊòéÈ°ØÊõ¥Â∞ë„ÄÇKD SAM ÊúâÊïàÂú∞Âπ≥Ë°°‰∫ÜÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶ÂíåÈÅãÁÆóÊïàÁéáÔºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©ÂêàÂú®Ë≥áÊ∫êÂèóÈôêÁí∞Â¢É‰∏≠ÁöÑÂç≥ÊôÇÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊáâÁî®„ÄÇ

##### **VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**
2501.16672v1 by Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour

Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá®Â∫äÈÜ´Â≠∏‰∏≠ÁîüÊàêÊñáÊú¨ÁöÑ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÔºåÁº∫‰πèÁ¢∫‰øùÁöÑÊñπÊ≥ï„ÄÇVeriFact ÊòØ‰∏ÄÁ®Æ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÔºåÁµêÂêà‰∫ÜÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÂíå LLM-as-a-JudgeÔºåÁî®ÊñºÈ©óË≠â LLM ÁîüÊàêÁöÑÊñáÊú¨ÊòØÂê¶Âü∫ÊñºÁóÖ‰∫∫ÁöÑÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) Áç≤ÂæóÁóÖ‰∫∫ÁöÑÁóÖÊ≠∑‰∫ãÂØ¶ÊîØÊåÅ„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÈÄôÂÄãÁ≥ªÁµ±ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü VeriFact-BHCÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑË≥áÊñôÈõÜÔºåÂ∞áÂá∫Èô¢ÊëòË¶Å‰∏≠ÁöÑÁ∞°Ë¶Å‰ΩèÈô¢ÁóÖÁ®ãÂàÜËß£Êàê‰∏ÄÁµÑÁ∞°ÂñÆÁöÑÈô≥Ëø∞Ôºå‰∏¶Áî±Ëá®Â∫äÈÜ´ÁîüË®ªËß£ÊØè‰∏ÄÂÄãÈô≥Ëø∞ÊòØÂê¶Áç≤ÂæóÁóÖ‰∫∫ÁöÑ EHR ÁóÖÊ≠∑ÊëòË¶ÅÊîØÊåÅ„ÄÇÂÑòÁÆ°Ëá®Â∫äÈÜ´Áîü‰πãÈñìÁöÑÊúÄÈ´ò‰∏ÄËá¥ÊÄßÁÇ∫ 88.5%Ôºå‰ΩÜËàáÂéªÂô™ÂíåË£ÅÊ±∫ÁöÑÂπ≥Âùá‰∫∫È°ûËá®Â∫äÈÜ´ÁîüÂü∫Êú¨‰∫ãÂØ¶Áõ∏ÊØîÔºåVeriFact ÁöÑ‰∏ÄËá¥ÊÄßÈ´òÈÅî 92.7%ÔºåÈÄôË°®Êòé VeriFact Ë∂ÖË∂ä‰∫ÜÂπ≥ÂùáËá®Â∫äÈÜ´ÁîüÊ†πÊìöÁóÖ‰∫∫ÁöÑÁóÖÊ≠∑Ê™¢Êü•ÊñáÊú¨‰∫ãÂØ¶ÁöÑËÉΩÂäõ„ÄÇVeriFact ÂèØËÉΩÊúÉÈÄèÈÅéÁßªÈô§ÁõÆÂâçÁöÑË©ï‰º∞Áì∂È†∏ÔºåÂä†ÈÄüÂü∫Êñº LLM ÁöÑ EHR ÊáâÁî®Á®ãÂºèÁöÑÈñãÁôº„ÄÇ

##### **Vision-based autonomous structural damage detection using data-driven methods**
2501.16662v2 by Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei

This study addresses the urgent need for efficient and accurate damage
detection in wind turbine structures, a crucial component of renewable energy
infrastructure. Traditional inspection methods, such as manual assessments and
non-destructive testing (NDT), are often costly, time-consuming, and prone to
human error. To tackle these challenges, this research investigates advanced
deep learning algorithms for vision-based structural health monitoring (SHM). A
dataset of wind turbine surface images, featuring various damage types and
pollution, was prepared and augmented for enhanced model training. Three
algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to
detect and classify surface damage. The models were trained and evaluated on a
dataset split into training, testing, and evaluation subsets (80%-10%-10%).
Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50
and high processing speed, making it suitable for real-time inspections. By
optimizing hyperparameters like learning rate and batch size, the models'
accuracy and efficiency improved further. YOLOv7 demonstrated significant
advancements in detection precision and execution speed, especially for
real-time applications. However, challenges such as dataset limitations and
environmental variability were noted, suggesting future work on segmentation
methods and larger datasets. This research underscores the potential of
vision-based deep learning techniques to transform SHM practices by reducing
costs, enhancing safety, and improving reliability, thus contributing to the
sustainable maintenance of critical infrastructure and supporting the longevity
of wind energy systems.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ëß£Ê±∫‰∫ÜÈ¢®ÂäõÊ∏¶Ëº™Ê©üÁµêÊßã‰∏≠Ëø´ÂàáÈúÄË¶ÅÁöÑÊúâÊïà‰∏îÊ∫ñÁ¢∫ÁöÑÊêçÂÇ∑Ê™¢Ê∏¨ÔºåÈÄôÊòØÂèØÂÜçÁîüËÉΩÊ∫êÂü∫Á§éË®≠ÊñΩÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂÇ≥Áµ±ÁöÑÊ™¢Êü•ÊñπÊ≥ïÔºå‰æãÂ¶ÇÊâãÂãïË©ï‰º∞ÂíåÈùûÁ†¥Â£ûÊÄßÊ™¢Ê∏¨ (NDT)ÔºåÈÄöÂ∏∏ÊàêÊú¨È´òÊòÇ„ÄÅËÄóÊôÇ‰∏îÂÆπÊòìÂá∫ÈåØ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Á†îÁ©∂Ë™øÊü•‰∫ÜÁî®ÊñºÂü∫ÊñºË¶ñË¶∫ÁöÑÁµêÊßãÂÅ•Â∫∑Áõ£Ê∏¨ (SHM) ÁöÑÂÖàÈÄ≤Ê∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ï„ÄÇÊ∫ñÂÇô‰∫Ü‰∏ÄÁµÑÈ¢®ÂäõÊ∏¶Ëº™Ê©üË°®Èù¢ÂΩ±ÂÉèÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÊêçÂ£ûÈ°ûÂûãÂíåÊ±°ÊüìÔºå‰∏¶Êì¥ÂÖÖ‰∫ÜÂ¢ûÂº∑Ê®°ÂûãË®ìÁ∑¥„ÄÇÊé°Áî®‰∫Ü‰∏âÁ®ÆÊºîÁÆóÊ≥ï‚Äî‚ÄîYOLOv7„ÄÅÂÖ∂ËºïÈáèÁ¥öËÆäÈ´îÂíå Faster R-CNN‚Äî‚Äî‰æÜÊ™¢Ê∏¨ÂíåÂàÜÈ°ûË°®Èù¢ÊêçÂ£û„ÄÇÈÄô‰∫õÊ®°ÂûãÂú®ÂàÜÂâ≤ÊàêË®ìÁ∑¥„ÄÅÊ∏¨Ë©¶ÂíåË©ï‰º∞Â≠êÈõÜÔºà80%-10%-10%ÔºâÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÂíåË©ï‰º∞„ÄÇÁµêÊûúË°®ÊòéÔºåYOLOv7 ÂÑ™ÊñºÂÖ∂‰ªñÊºîÁÆóÊ≥ïÔºåÂØ¶Áèæ‰∫Ü 82.4% ÁöÑ mAP@50 ÂíåËºÉÈ´òÁöÑËôïÁêÜÈÄüÂ∫¶Ôºå‰ΩøÂÖ∂ÈÅ©Áî®ÊñºÂç≥ÊôÇÊ™¢Êü•„ÄÇÈÄöÈÅéÊúÄ‰Ω≥ÂåñÂ≠∏ÁøíÁéáÂíåÊâπÊ¨°Â§ßÂ∞èÁ≠âË∂ÖÂèÉÊï∏ÔºåÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÈÄ≤‰∏ÄÊ≠•ÊèêÈ´ò„ÄÇYOLOv7 Âú®Ê™¢Ê∏¨Á≤æÂ∫¶ÂíåÂü∑Ë°åÈÄüÂ∫¶ÊñπÈù¢Ë°®ÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÁâπÂà•ÊòØÂ∞çÊñºÂç≥ÊôÇÊáâÁî®Á®ãÂºè„ÄÇÁÑ∂ËÄåÔºåÊ≥®ÊÑèÂà∞Ë≥áÊñôÈõÜÈôêÂà∂ÂíåÁí∞Â¢ÉËÆäÁï∞ÊÄßÁ≠âÊåëÊà∞ÔºåÈÄôË°®ÊòéÊú™‰æÜÂú®ÂàÜÂâ≤ÊñπÊ≥ïÂíåÊõ¥Â§ßÁöÑË≥áÊñôÈõÜÊñπÈù¢ÁöÑÂ∑•‰Ωú„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂü∫ÊñºË¶ñË¶∫ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÂú®ËΩâÊèõ SHM ÂØ¶ÂãôÊñπÈù¢ÁöÑÊΩõÂäõÔºåÊñπÊ≥ïÊòØÈôç‰ΩéÊàêÊú¨„ÄÅÂ¢ûÂº∑ÂÆâÂÖ®ÊÄß‰∏¶ÊèêÈ´òÂèØÈù†ÊÄßÔºåÂæûËÄåÊúâÂä©ÊñºÁ∂≠Ë≠∑ÈóúÈçµÂü∫Á§éË®≠ÊñΩÁöÑÂèØÊåÅÁ∫åÊÄß‰∏¶ÊîØÊåÅÈ¢®ËÉΩÁ≥ªÁµ±ÁöÑÈï∑Â£ΩÂëΩ„ÄÇ

##### **Molecular-driven Foundation Model for Oncologic Pathology**
2501.16652v1 by Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood

Foundation models are reshaping computational pathology by enabling transfer
learning, where models pre-trained on vast datasets can be adapted for
downstream diagnostic, prognostic, and therapeutic response tasks. Despite
these advances, foundation models are still limited in their ability to encode
the entire gigapixel whole-slide images without additional training and often
lack complementary multimodal data. Here, we introduce Threads, a slide-level
foundation model capable of generating universal representations of whole-slide
images of any size. Threads was pre-trained using a multimodal learning
approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained
tissue sections, paired with corresponding genomic and transcriptomic profiles
- the largest such paired dataset to be used for foundation model development
to date. This unique training paradigm enables Threads to capture the tissue's
underlying molecular composition, yielding powerful representations applicable
to a wide array of downstream tasks. In extensive benchmarking across 54
oncology tasks, including clinical subtyping, grading, mutation prediction,
immunohistochemistry status determination, treatment response prediction, and
survival prediction, Threads outperformed all baselines while demonstrating
remarkable generalizability and label efficiency. It is particularly well
suited for predicting rare events, further emphasizing its clinical utility. We
intend to make the model publicly available for the broader community.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÈÄèÈÅéÂïüÁî®ËΩâÁßªÂ≠∏Áøí‰æÜÈáçÂ°ëË®àÁÆóÁóÖÁêÜÂ≠∏ÔºåÂÖ∂‰∏≠È†êÂÖàÂú®ÈæêÂ§ßË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØÈÅ©ÊáâÊñº‰∏ãÊ∏∏Ë®∫Êñ∑„ÄÅÈ†êÂæåÂíåÊ≤ªÁôÇÂèçÊáâ‰ªªÂãô„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õÈÄ≤Â±ïÔºåÂü∫Á§éÊ®°ÂûãÂú®Á∑®Á¢ºÊï¥ÂÄãÂçÉÂÖÜÂÉèÁ¥†ÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉèÁöÑËÉΩÂäõ‰∏ä‰ªçÊúâÈôêÔºå‰∏îÁ∂ìÂ∏∏Áº∫‰πèË£úÂÖÖÂ§öÊ®°ÂºèË≥áÊñô„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π ThreadsÔºåÈÄôÊòØ‰∏ÄÂÄãÂπªÁáàÁâáÂ±§Á¥öÂü∫Á§éÊ®°ÂûãÔºåËÉΩÂ§†Áî¢Áîü‰ªª‰ΩïÂ§ßÂ∞èÁöÑÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉèÁöÑÈÄöÁî®Ë°®Á§∫„ÄÇThreads ‰ΩøÁî®Â§öÊ®°ÂºèÂ≠∏ÁøíÊñπÊ≥ïÈ†êÂÖàË®ìÁ∑¥Ôºå‰∏¶ÈáùÂ∞ç 47,171 ÂÄãËòáÊú®Á≤æÂíåÊõôÁ¥Ö (H&E) ÊüìËâ≤ÁöÑÁµÑÁπîÂàáÁâáÁöÑÂ§öÂÖÉÁæ§ÁµÑÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰∏¶Êê≠ÈÖçÂ∞çÊáâÁöÑÂü∫Âõ†È´îÂíåËΩâÈåÑÁµÑÁâπÂæµÊ™îÔºåÈÄôÊòØËøÑ‰ªäÁÇ∫Ê≠¢Áî®ÊñºÂü∫Á§éÊ®°ÂûãÈñãÁôºÁöÑÊúÄÂ§ßÊ≠§È°ûÈÖçÂ∞çË≥áÊñôÈõÜ„ÄÇÈÄôÁ®ÆÁç®ÁâπÁöÑË®ìÁ∑¥ÁØÑ‰æã‰Ωø Threads ËÉΩÂ§†Êì∑ÂèñÁµÑÁπîÁöÑÂü∫Á§éÂàÜÂ≠êÁµÑÊàêÔºåÁî¢ÁîüÂº∑Â§ßÁöÑË°®Á§∫ÔºåÈÅ©Áî®ÊñºÂª£Ê≥õÁöÑ‰∏ãÊ∏∏‰ªªÂãô„ÄÇÂú®Ê∂µËìã 54 ÂÄãËÖ´Áò§Â≠∏‰ªªÂãôÁöÑÂª£Ê≥õÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂåÖÊã¨Ëá®Â∫äÂàÜÂûã„ÄÅÂàÜÁ¥ö„ÄÅÁ™ÅËÆäÈ†êÊ∏¨„ÄÅÂÖçÁñ´ÁµÑÁπîÂåñÂ≠∏ÁãÄÊÖãÂà§ÂÆö„ÄÅÊ≤ªÁôÇÂèçÊáâÈ†êÊ∏¨ÂíåÂ≠òÊ¥ªÈ†êÊ∏¨ÔºåThreads ÂÑ™ÊñºÊâÄÊúâÂü∫Ê∫ñÔºåÂêåÊôÇÂ±ïÁèæÂá∫È°ØËëóÁöÑÊ¶ÇÊã¨ÊÄßÂíåÊ®ôÁ±§ÊïàÁéá„ÄÇÂÆÉÁâπÂà•ÈÅ©ÂêàÈ†êÊ∏¨ÁΩïË¶ã‰∫ã‰ª∂ÔºåÈÄ≤‰∏ÄÊ≠•Âº∑Ë™øÂÖ∂Ëá®Â∫äÊïàÁî®„ÄÇÊàëÂÄëÊâìÁÆóËÆìË©≤Ê®°ÂûãÂÖ¨ÈñãÔºå‰æõÊõ¥Âª£Ê≥õÁöÑÁ§æÁæ§‰ΩøÁî®„ÄÇ

##### **Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**
2502.00045v1 by Yi Mao, Andrew Perrault

Municipal inspections are an important part of maintaining the quality of
goods and services. In this paper, we approach the problem of intelligently
scheduling service inspections to maximize their impact, using the case of food
establishment inspections in Chicago as a case study. The Chicago Department of
Public Health (CDPH) inspects thousands of establishments each year, with a
substantial fail rate (over 3,000 failed inspection reports in 2023). To
balance the objectives of ensuring adherence to guidelines, minimizing
disruption to establishments, and minimizing inspection costs, CDPH assigns
each establishment an inspection window every year and guarantees that they
will be inspected exactly once during that window. These constraints create a
challenge for a restless multi-armed bandit (RMAB) approach, for which there
are no existing methods. We develop an extension to Whittle index-based systems
for RMABs that can guarantee action window constraints and frequencies, and
furthermore can be leveraged to optimize action window assignments themselves.
Briefly, we combine MDP reformulation and integer programming-based lookahead
to maximize the impact of inspections subject to constraints. A neural
network-based supervised learning model is developed to model state transitions
of real Chicago establishments using public CDPH inspection records, which
demonstrates 10\% AUC improvements compared with directly predicting
establishments' failures. Our experiments not only show up to 24\% (in
simulation) or 33\% (on real data) reward improvements resulting from our
approach but also give insight into the impact of scheduling constraints.

ÊëòË¶ÅÔºöÂ∏ÇÊîøÊ™¢Êü•ÊòØÁ∂≠ÊåÅÂïÜÂìÅÂíåÊúçÂãôÂìÅË≥™ÁöÑÈáçË¶Å‰∏ÄÁí∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ª•ËäùÂä†Âì•ÁöÑÈ£üÂìÅÊ©üÊßãÊ™¢Êü•ÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÊé¢Ë®éÂ¶Ç‰ΩïÈÄèÈÅéÊô∫ÊÖßÊéíÁ®ãÊúçÂãôÊ™¢Êü•‰ª•ÊúÄÂ§ßÂåñÂÖ∂ÂΩ±ÈüøÂäõ„ÄÇËäùÂä†Âì•ÂÖ¨ÂÖ±Ë°õÁîüÈÉ® (CDPH) ÊØèÂπ¥Ê™¢Êü•Êï∏ÂçÉÂÆ∂Ê©üÊßãÔºå‰∏îÊúâÁõ∏Áï∂È´òÁöÑ‰∏çÂêàÊ†ºÁéáÔºà2023 Âπ¥ÊúâË∂ÖÈÅé 3,000 ‰ªΩ‰∏çÂêàÊ†ºÊ™¢Êü•Â†±ÂëäÔºâ„ÄÇÁÇ∫‰∫ÜÂπ≥Ë°°Á¢∫‰øùÈÅµÂæ™Ê∫ñÂâá„ÄÅÊúÄÂ§ßÁ®ãÂ∫¶Ê∏õÂ∞ëÂ∞çÊ©üÊßãÁöÑÂπ≤ÊìæÂíåÊúÄÂ§ßÁ®ãÂ∫¶Ê∏õÂ∞ëÊ™¢Êü•ÊàêÊú¨ÁöÑÁõÆÊ®ôÔºåCDPH ÊØèÂπ¥ÁÇ∫ÊØèÂÆ∂Ê©üÊßãÂàÜÈÖç‰∏ÄÂÄãÊ™¢Êü•ÊôÇÊÆµÔºå‰∏¶‰øùË≠âÂú®Ë©≤ÊôÇÊÆµÂÖßÂè™Ê™¢Êü•‰∏ÄÊ¨°„ÄÇÈÄô‰∫õÈôêÂà∂Â∞ç‰∏çÈñìÊñ∑Â§öËáÇË≥≠ÂçöÊ©ü (RMAB) ÊñπÊ≥ïÊßãÊàêÊåëÊà∞ÔºåÁõÆÂâçÂ∞öÁÑ°ÁèæÊúâÊñπÊ≥ï„ÄÇÊàëÂÄëÈáùÂ∞ç RMAB ÁöÑ Whittle ÊåáÊï∏Á≥ªÁµ±ÈñãÁôº‰∫Ü‰∏ÄÂÄãÂª∂‰º∏ÔºåÂèØ‰ª•‰øùË≠âÂãï‰ΩúÊôÇÊÆµÈôêÂà∂ÂíåÈ†ªÁéáÔºåÊ≠§Â§ñÈÇÑÂèØ‰ª•Âà©Áî®ÂÆÉ‰æÜÂÑ™ÂåñÂãï‰ΩúÊôÇÊÆµÂàÜÈÖçÊú¨Ë∫´„ÄÇÁ∞°ËÄåË®Ä‰πãÔºåÊàëÂÄëÁµêÂêà MDP ÈáçÊñ∞Âà∂ÂÆöÂíåÂü∫ÊñºÊï¥Êï∏Ë¶èÂäÉÁöÑÂâçÁûªÊÄßÔºå‰ª•ÊúÄÂ§ßÂåñÁ¨¶ÂêàÈôêÂà∂Ê¢ù‰ª∂ÁöÑÊ™¢Êü•ÂΩ±Èüø„ÄÇ‰ΩøÁî®ÂÖ¨ÈñãÁöÑ CDPH Ê™¢Êü•Ë®òÈåÑÈñãÁôº‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÁõ£Áù£ÂºèÂ≠∏ÁøíÊ®°ÂûãÔºåÁî®ÊñºÊ®°Êì¨ËäùÂä†Âì•ÂØ¶ÈöõÊ©üÊßãÁöÑÁãÄÊÖãËΩâÊèõÔºåËàáÁõ¥Êé•È†êÊ∏¨Ê©üÊßãÁöÑÂ§±ÊïóÁõ∏ÊØîÔºåÈ°ØÁ§∫Âá∫ AUC ÊèêÂçá‰∫Ü 10%„ÄÇÊàëÂÄëÁöÑÂØ¶È©ó‰∏çÂÉÖÈ°ØÁ§∫ÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∏∂‰æÜÁöÑÁçéÂãµÊèêÂçáÈ´òÈÅî 24%ÔºàÊ®°Êì¨‰∏≠ÔºâÊàñ 33%ÔºàÂØ¶ÈöõË≥áÊñô‰∏≠ÔºâÔºåÈÇÑÊ∑±ÂÖ•‰∫ÜËß£‰∫ÜÊéíÁ®ãÈôêÂà∂ÁöÑÂΩ±Èüø„ÄÇ

##### **Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**
2501.16282v1 by Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu

Understanding brain disorders is crucial for accurate clinical diagnosis and
treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a
promising approach to interpreting medical images with the support of text
descriptions. However, previous research has primarily focused on 2D medical
images, leaving richer spatial information of 3D images under-explored, and
single-modality-based methods are limited by overlooking the critical clinical
information contained in other modalities. To address this issue, this paper
proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck
layer to learn new knowledge and instill it into the original pre-trained
knowledge. The major idea is to incorporate a lightweight bottleneck layer to
train fewer parameters while capturing essential information and utilize a
Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal
data within a unified representation space. Extensive experiments demonstrated
the effectiveness of our approach in integrating multimodal data to
significantly improve the diagnosis accuracy without high computational costs,
highlighting the potential to enhance real-world diagnostic workflows.

ÊëòË¶ÅÔºö‰∫ÜËß£ËÖ¶ÈÉ®ÁñæÁóÖÂ∞çÊñºÊ∫ñÁ¢∫ÁöÑËá®Â∫äË®∫Êñ∑ÂíåÊ≤ªÁôÇËá≥ÈóúÈáçË¶Å„ÄÇÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âú®ÊñáÊú¨ÊèèËø∞ÁöÑÊîØÊè¥‰∏ãË©ÆÈáãÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú® 2D ÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂøΩÁï•‰∫Ü 3D ÂΩ±ÂÉèÊõ¥Ë±êÂØåÁöÑÁ©∫ÈñìË≥áË®äÔºåËÄåÂñÆ‰∏ÄÊ®°ÊÖãÊñπÊ≥ïÂèóÂà∞ÂøΩË¶ñÂÖ∂‰ªñÊ®°ÊÖã‰∏≠ÈóúÈçµËá®Â∫äË≥áË®äÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü Brain-AdapterÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫Ü‰∏ÄÂÄãÈ°çÂ§ñÁöÑÁì∂È†∏Â±§‰æÜÂ≠∏ÁøíÊñ∞Áü•Ë≠ò‰∏¶Â∞áÂÖ∂ÁÅåËº∏Âà∞ÂéüÂßãÈ†êË®ìÁ∑¥ÁöÑÁü•Ë≠ò‰∏≠„ÄÇ‰∏ªË¶ÅÁöÑÊÉ≥Ê≥ïÊòØÁµêÂêà‰∏ÄÂÄãËºïÈáèÁ¥öÁì∂È†∏Â±§ÔºåÂú®Êì∑ÂèñÂøÖË¶ÅË≥áË®äÁöÑÂêåÊôÇË®ìÁ∑¥ËºÉÂ∞ëÁöÑÂèÉÊï∏Ôºå‰∏¶Âà©Áî®Â∞çÊØîË™ûË®ÄÂΩ±ÂÉèÈ†êË®ìÁ∑¥ (CLIP) Á≠ñÁï•Âú®Áµ±‰∏ÄÁöÑË°®Á§∫Á©∫Èñì‰∏≠Â∞çÈΩäÂ§öÊ®°ÊÖãË≥áÊñô„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Êï¥ÂêàÂ§öÊ®°ÊÖãË≥áÊñô‰ª•È°ØËëóÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåËÄå‰∏çÊúÉÈÄ†ÊàêÈ´òÈÅãÁÆóÊàêÊú¨ÔºåÁ™ÅÈ°Ø‰∫ÜÂ¢ûÂº∑ÁúüÂØ¶‰∏ñÁïåË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ãÁöÑÊΩõÂäõ„ÄÇ

##### **Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**
2501.16215v1 by Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li

Large language models (LLMs) exhibit remarkable capabilities in visual
inspection of medical time-series data, achieving proficiency comparable to
human clinicians. However, their broad scope limits domain-specific precision,
and proprietary weights hinder fine-tuning for specialized datasets. In
contrast, small specialized models (SSMs) excel in targeted tasks but lack the
contextual reasoning required for complex clinical decision-making. To address
these challenges, we propose ConMIL (Conformalized Multiple Instance Learning),
a decision-support SSM that integrates seamlessly with LLMs. By using Multiple
Instance Learning (MIL) to identify clinically significant signal segments and
conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'
interpretative capabilities for medical time-series analysis. Experimental
results demonstrate that ConMIL significantly improves the performance of
state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,
\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for
confident samples in arrhythmia detection and sleep staging, compared to
standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the
potential of ConMIL to bridge task-specific precision and broader contextual
reasoning, enabling more reliable and interpretable AI-driven clinical decision
support.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóË≥áÊñôÁöÑË¶ñË¶∫Ê™¢Êü•‰∏≠Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõÔºåÈÅîÂà∞‰∫ÜËàá‰∫∫È°ûËá®Â∫äÈÜ´ÁîüÁõ∏Áï∂ÁöÑÁÜüÁ∑¥Â∫¶„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁöÑÂª£Ê≥õÁØÑÂúçÈôêÂà∂‰∫ÜÁâπÂÆöÈ†òÂüüÁöÑÁ≤æÁ¢∫Â∫¶ÔºåËÄåÂ∞àÊúâÊ¨äÈáçÈòªÁ§ô‰∫ÜÈáùÂ∞çÁâπÂÆöË≥áÊñôÈõÜÁöÑÂæÆË™ø„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂ∞èÂûãÂ∞àÁî®Ê®°Âûã (SSM) Âú®ÁõÆÊ®ô‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÁº∫‰πèË§áÈõúËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊâÄÈúÄÁöÑËÉåÊôØÊé®ÁêÜ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü ConMILÔºàÂÖ±ÂΩ¢Â§öÂØ¶‰æãÂ≠∏ÁøíÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãËàá LLM ÁÑ°Á∏´Êï¥ÂêàÁöÑÊ±∫Á≠ñÊîØÊè¥ SSM„ÄÇÈÄèÈÅé‰ΩøÁî®Â§öÂØ¶‰æãÂ≠∏Áøí (MIL) ‰æÜË≠òÂà•Ëá®Â∫äÈ°ØËëóË®äËôüÂçÄÊÆµÔºå‰∏¶Â∞çÊ†°Ê∫ñÁöÑÈõÜÂêàÂÄºËº∏Âá∫ÈÄ≤Ë°åÂÖ±ÂΩ¢È†êÊ∏¨ÔºåConMIL Â¢ûÂº∑‰∫Ü LLM Â∞çÈÜ´ÁôÇÊôÇÈñìÂ∫èÂàóÂàÜÊûêÁöÑËß£ÈáãËÉΩÂäõ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåConMIL ÊòéÈ°ØÊîπÂñÑ‰∫ÜÊúÄÂÖàÈÄ≤ LLM ÁöÑÊïàËÉΩÔºå‰æãÂ¶Ç ChatGPT4.0 Âíå Qwen2-VL-7B„ÄÇÂÖ∑È´î‰æÜË™™Ôºå\ConMIL{}- ÊîØÊè¥ÁöÑ Qwen2-VL-7B Âú®ÂøÉÂæã‰∏çÊï¥ÂÅµÊ∏¨ÂíåÁù°Áú†ÂàÜÊúü‰∏≠ÔºåÂ∞çÊñºÊúâ‰ø°ÂøÉÁöÑÊ®£Êú¨ÈÅîÂà∞‰∫Ü 94.92% Âíå 96.82% ÁöÑÁ≤æÁ¢∫Â∫¶ÔºåËÄåÁç®Á´ã LLM ÁöÑÊ∫ñÁ¢∫Â∫¶ÂÉÖÁÇ∫ 46.13% Âíå 13.16%„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫Ü ConMIL Âú®Ê©ãÊé•ÁâπÂÆö‰ªªÂãôÁöÑÁ≤æÁ¢∫Â∫¶ÂíåÊõ¥Âª£Ê≥õÁöÑËÉåÊôØÊé®ÁêÜÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂæûËÄåÂØ¶ÁèæÊõ¥ÂèØÈù†‰∏îÂèØËß£ÈáãÁöÑ AI È©ÖÂãïËá®Â∫äÊ±∫Á≠ñÊîØÊè¥„ÄÇ

