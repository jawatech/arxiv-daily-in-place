{"2411.06646": {"publish_time": "2024-11-11", "title": "Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data", "paper_summary": "When training deep neural networks, a model's generalization error is often\nobserved to follow a power scaling law dependent both on the model size and the\ndata size. Perhaps the best known example of such scaling laws are for\ntransformer-based large language models, where networks with billions of\nparameters are trained on trillions of tokens of text. Yet, despite sustained\nwidespread interest, a rigorous understanding of why transformer scaling laws\nexist is still missing. To answer this question, we establish novel statistical\nestimation and mathematical approximation theories for transformers when the\ninput data are concentrated on a low-dimensional manifold. Our theory predicts\na power law between the generalization error and both the training data size\nand the network size for transformers, where the power depends on the intrinsic\ndimension $d$ of the training data. Notably, the constructed model architecture\nis shallow, requiring only logarithmic depth in $d$. By leveraging\nlow-dimensional data structures under a manifold hypothesis, we are able to\nexplain transformer scaling laws in a way which respects the data geometry.\nMoreover, we test our theory with empirical observation by training LLMs on\nnatural language datasets. We find the observed empirical data scaling laws\nclosely agree with our theoretical predictions. Taken together, these results\nrigorously show the intrinsic dimension of data to be a crucial quantity\naffecting transformer scaling laws in both theory and practice.", "paper_summary_zh": "\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u901a\u5e38\u4f1a\u89c2\u5bdf\u5230\u6a21\u578b\u7684\u6cdb\u5316\u8bef\u5dee\u9075\u5faa\u4e00\u79cd\u5e42\u6b21\u7f29\u653e\u5b9a\u5f8b\uff0c\u8be5\u5b9a\u5f8b\u540c\u65f6\u53d6\u51b3\u4e8e\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u5927\u5c0f\u3002\u8fd9\u79cd\u7f29\u653e\u5b9a\u5f8b\u6700\u8457\u540d\u7684\u4f8b\u5b50\u53ef\u80fd\u662f\u57fa\u4e8e Transformer \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5176\u4e2d\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u7684\u7f51\u7edc\u5728\u6570\u4e07\u4ebf\u4e2a\u6587\u672c\u6807\u8bb0\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u6301\u7eed\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u5bf9\u4e8e Transformer \u7f29\u653e\u5b9a\u5f8b\u4e3a\u4f55\u5b58\u5728\uff0c\u4ecd\u7136\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u89e3\u3002\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u8f93\u5165\u6570\u636e\u96c6\u4e2d\u4e8e\u4f4e\u7ef4\u6d41\u5f62\u65f6\uff0c\u4e3a Transformer \u5efa\u7acb\u4e86\u65b0\u9896\u7684\u7edf\u8ba1\u4f30\u8ba1\u548c\u6570\u5b66\u903c\u8fd1\u7406\u8bba\u3002\u6211\u4eec\u7684\u7406\u8bba\u9884\u6d4b\u4e86\u6cdb\u5316\u8bef\u5dee\u4e0e\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u548c Transformer \u7684\u7f51\u7edc\u5927\u5c0f\u4e4b\u95f4\u7684\u5e42\u5f8b\uff0c\u5176\u4e2d\u5e42\u53d6\u51b3\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5185\u5728\u7ef4\u6570 $d$\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6784\u5efa\u7684\u6a21\u578b\u67b6\u6784\u662f\u6d45\u5c42\u7684\uff0c\u53ea\u9700\u8981 $d$ \u4e2d\u7684\u5bf9\u6570\u6df1\u5ea6\u3002\u901a\u8fc7\u5229\u7528\u6d41\u5f62\u5047\u8bbe\u4e0b\u7684\u4f4e\u7ef4\u6570\u636e\u7ed3\u6784\uff0c\u6211\u4eec\u80fd\u591f\u4ee5\u5c0a\u91cd\u6570\u636e\u51e0\u4f55\u7684\u65b9\u5f0f\u6765\u89e3\u91ca Transformer \u7f29\u653e\u5b9a\u5f8b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 LLM \u6765\u7528\u7ecf\u9a8c\u89c2\u5bdf\u68c0\u9a8c\u6211\u4eec\u7684\u7406\u8bba\u3002\u6211\u4eec\u53d1\u73b0\u89c2\u5bdf\u5230\u7684\u7ecf\u9a8c\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u4e0e\u6211\u4eec\u7684\u7406\u8bba\u9884\u6d4b\u975e\u5e38\u543b\u5408\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e25\u683c\u5730\u8868\u660e\uff0c\u6570\u636e\u7684\u5185\u5728\u7ef4\u6570\u662f\u5f71\u54cd\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d Transformer \u7f29\u653e\u5b9a\u5f8b\u7684\u5173\u952e\u91cf\u3002", "author": "Alex Havrilla et.al.", "authors": "Alex Havrilla, Wenjing Liao", "id": "2411.06646v1", "paper_url": "http://arxiv.org/abs/2411.06646v1", "repo": "https://github.com/dahoas/transformer_manifolds_learning"}}