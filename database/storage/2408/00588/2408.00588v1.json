{"2408.00588": {"publish_time": "2024-07-25", "title": "Closing the gap between open-source and commercial large language models for medical evidence summarization", "paper_summary": "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u603b\u7ed3\u533b\u5b66\u8bc1\u636e\u65b9\u9762\u5177\u6709\u5f88\u5927\u7684\u524d\u666f\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4e13\u6709 LLM \u7684\u5e94\u7528\u4e0a\u3002\u4f7f\u7528\u4e13\u6709 LLM \u4f1a\u5f15\u5165\u591a\u4e2a\u98ce\u9669\u56e0\u7d20\uff0c\u5305\u62ec\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u4f9b\u5e94\u5546\u4f9d\u8d56\u6027\u3002\u867d\u7136\u5f00\u6e90 LLM \u5141\u8bb8\u66f4\u597d\u7684\u900f\u660e\u5ea6\u548c\u5b9a\u5236\uff0c\u4f46\u5b83\u4eec\u7684\u6027\u80fd\u4e0e\u4e13\u6709 LLM \u76f8\u6bd4\u8fd8\u6709\u6240\u4e0d\u8db3\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8c03\u67e5\u4e86\u5fae\u8c03\u5f00\u6e90 LLM \u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u5728\u603b\u7ed3\u533b\u5b66\u8bc1\u636e\u65b9\u9762\u7684\u6027\u80fd\u3002\u5229\u7528\u57fa\u51c6\u6570\u636e\u96c6 MedReview\uff0c\u5176\u4e2d\u5305\u542b 8,161 \u5bf9\u7cfb\u7edf\u8bc4\u4ef7\u548c\u6458\u8981\uff0c\u6211\u4eec\u5fae\u8c03\u4e86\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90 LLM\uff0c\u5373 PRIMERA\u3001LongT5 \u548c Llama-2\u3002\u603b\u4f53\u800c\u8a00\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684 LLM \u5728 ROUGE-L \u4e2d\u589e\u52a0\u4e86 9.89\uff0895% \u7f6e\u4fe1\u533a\u95f4\uff1a8.94-10.81\uff09\uff0c\u5728 METEOR \u5206\u6570\u4e2d\u589e\u52a0\u4e86 13.21\uff0895% \u7f6e\u4fe1\u533a\u95f4\uff1a12.05-14.37\uff09\uff0c\u5728 CHRF \u5206\u6570\u4e2d\u589e\u52a0\u4e86 15.82\uff0895% \u7f6e\u4fe1\u533a\u95f4\uff1a13.89-16.44\uff09\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 LongT5 \u7684\u6027\u80fd\u63a5\u8fd1\u4e8e\u96f6\u955c\u5934\u8bbe\u7f6e\u4e0b\u7684 GPT-3.5\u3002\u6b64\u5916\uff0c\u8f83\u5c0f\u7684\u5fae\u8c03\u6a21\u578b\u6709\u65f6\u751a\u81f3\u8868\u73b0\u51fa\u4f18\u4e8e\u8f83\u5927\u7684\u96f6\u955c\u5934\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e0a\u8ff0\u6539\u8fdb\u8d8b\u52bf\u4e5f\u4f53\u73b0\u5728\u4eba\u7c7b\u548c GPT4 \u6a21\u62df\u8bc4\u4f30\u4e2d\u3002\u6211\u4eec\u7684\u7ed3\u679c\u53ef\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u9009\u62e9\uff0c\u4ee5\u5b8c\u6210\u9700\u8981\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u533b\u5b66\u8bc1\u636e\u603b\u7ed3\u3002", "author": "Gongbo Zhang et.al.", "authors": "Gongbo Zhang, Qiao Jin, Yiliang Zhou, Song Wang, Betina R. Idnay, Yiming Luo, Elizabeth Park, Jordan G. Nestor, Matthew E. Spotnitz, Ali Soroush, Thomas Campion, Zhiyong Lu, Chunhua Weng, Yifan Peng", "id": "2408.00588v1", "paper_url": "http://arxiv.org/abs/2408.00588v1", "repo": "null"}}