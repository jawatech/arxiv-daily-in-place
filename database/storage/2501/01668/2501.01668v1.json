{"2501.01668": {"publish_time": "2025-01-03", "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis", "paper_summary": "Current inference scaling methods, such as Self-consistency and Best-of-N,\nhave proven effective in improving the accuracy of LLMs on complex reasoning\ntasks. However, these methods rely heavily on the quality of candidate\nresponses and are unable to produce correct answers when all candidates are\nincorrect. In this paper, we propose a novel inference scaling strategy,\nCoT-based Synthesizer, which leverages CoT reasoning to synthesize superior\nanswers by analyzing complementary information from multiple candidate\nresponses, even when all candidate responses are flawed. To enable a\nlightweight and cost-effective implementation, we introduce an automated data\ngeneration pipeline that creates diverse training data. This allows smaller\nLLMs trained on this data to improve the inference accuracy of larger models,\nincluding API-based LLMs. Experimental results across four benchmark datasets\nwith seven policy models demonstrate that our method significantly enhances\nperformance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH\ndataset. The corresponding training data and code are publicly available on\nhttps://github.com/RUCKBReasoning/CoT-based-Synthesizer.", "paper_summary_zh": "\u76ee\u524d\u7684\u63a8\u8ad6\u64f4\u5145\u65b9\u6cd5\uff08\u4f8b\u5982\u81ea\u6d3d\u6027\u548c\u6700\u4f73 N\uff09\u5df2\u88ab\u8b49\u660e\u6709\u6548\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u6e96\u78ba\u6027\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u6975\u5ea6\u4f9d\u8cf4\u5019\u9078\u56de\u61c9\u7684\u54c1\u8cea\uff0c\u4e14\u7576\u6240\u6709\u5019\u9078\u56de\u61c9\u7686\u4e0d\u6b63\u78ba\u6642\uff0c\u7121\u6cd5\u7522\u751f\u6b63\u78ba\u7684\u7b54\u6848\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u63a8\u8ad6\u64f4\u5145\u7b56\u7565\uff0c\u57fa\u65bc CoT \u7684\u5408\u6210\u5668\uff0c\u5b83\u5229\u7528 CoT \u63a8\u7406\u4f86\u5206\u6790\u4f86\u81ea\u591a\u500b\u5019\u9078\u56de\u61c9\u7684\u4e92\u88dc\u8cc7\u8a0a\uff0c\u5373\u4f7f\u6240\u6709\u5019\u9078\u56de\u61c9\u90fd\u6709\u7f3a\u9677\uff0c\u4e5f\u80fd\u5408\u6210\u51fa\u512a\u7570\u7684\u7b54\u6848\u3002\u70ba\u4e86\u5be6\u73fe\u8f15\u91cf\u4e14\u5177\u6210\u672c\u6548\u76ca\u7684\u5be6\u4f5c\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u81ea\u52d5\u5316\u8cc7\u6599\u7522\u751f\u7ba1\u7dda\uff0c\u7528\u4f86\u5efa\u7acb\u591a\u6a23\u5316\u7684\u8a13\u7df4\u8cc7\u6599\u3002\u9019\u5141\u8a31\u5728\u9019\u4e9b\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u63d0\u5347\u5927\u578b\u6a21\u578b\uff08\u5305\u62ec\u57fa\u65bc API \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u7684\u63a8\u8ad6\u6e96\u78ba\u6027\u3002\u5728\u56db\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e03\u500b\u7b56\u7565\u6a21\u578b\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u8457\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u5728 MATH \u8cc7\u6599\u96c6\u4e0a\uff0cLlama3-8B \u7372\u5f97 11.8% \u7684\u63d0\u5347\uff0c\u800c GPT-4o \u7372\u5f97 10.3% \u7684\u63d0\u5347\u3002\u5c0d\u61c9\u7684\u8a13\u7df4\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc https://github.com/RUCKBReasoning/CoT-based-Synthesizer\u3002", "author": "Bohan Zhang et.al.", "authors": "Bohan Zhang, Xiaokang Zhang, Jing Zhang, Jifan Yu, Sijia Luo, Jie Tang", "id": "2501.01668v1", "paper_url": "http://arxiv.org/abs/2501.01668v1", "repo": "null"}}