{"2410.14268": {"publish_time": "2024-10-18", "title": "MoDification: Mixture of Depths Made Easy", "paper_summary": "Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u9577\u6587\u672c\u6548\u7387\u5df2\u6210\u70ba\u670d\u52d9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u71b1\u9580\u8a71\u984c\u3002\u6df1\u5ea6\u6df7\u5408 (MoD) \u88ab\u63d0\u8b70\u70ba\u964d\u4f4e\u5ef6\u9072\u548c\u8a18\u61b6\u9ad4\u7684\u5b8c\u7f8e\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe MoD \u5e7e\u4e4e\u7121\u6cd5\u8f49\u63db\u73fe\u6709\u7684 LLM\uff0c\u800c\u7121\u9700\u5728\u5927\u91cf token \u4e0a\u9032\u884c\u4ee3\u50f9\u9ad8\u6602\u7684\u8a13\u7df4\u3002\u70ba\u4e86\u5be6\u73fe\u5f9e\u4efb\u4f55 LLM \u8f49\u63db\u70ba MoD\uff0c\u6211\u5011\u5c55\u793a\u4e86 MoD \u4e2d\u7684 top-k \u7b97\u5b50\u61c9\u63d0\u5347\u70ba\u95be\u503c-p \u7b97\u5b50\uff0c\u4e26\u4e14\u9084\u61c9\u8abf\u6574\u67b6\u69cb\u548c\u6578\u64da\u3002\u6240\u6709\u9019\u4e9b\u8a2d\u8a08\u69cb\u6210\u4e86\u6211\u5011\u7a31\u70ba MoDification \u7684\u65b9\u6cd5\u3002\u900f\u904e\u6db5\u84cb\u5f9e 3B \u5230 70B \u7684\u6a21\u578b\u898f\u6a21\u7684\u5168\u9762\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86 MoDification \u5728\u6548\u7387\u548c\u6709\u6548\u6027\u4e4b\u9593\u53d6\u5f97\u4e86\u6975\u4f73\u7684\u5e73\u8861\u3002\u8207\u539f\u59cb LLM \u76f8\u6bd4\uff0cMoDification \u53ef\u4ee5\u5be6\u73fe\u5ef6\u9072\u901f\u5ea6\u6700\u9ad8\u63d0\u5347 ~1.2 \u500d\uff0c\u8a18\u61b6\u9ad4\u6e1b\u5c11 ~1.8 \u500d\uff0c\u5c24\u5176\u662f\u5728\u9577\u6587\u672c\u61c9\u7528\u4e2d\u3002", "author": "Chen Zhang et.al.", "authors": "Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song", "id": "2410.14268v1", "paper_url": "http://arxiv.org/abs/2410.14268v1", "repo": "null"}}