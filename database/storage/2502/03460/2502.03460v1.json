{"2502.03460": {"publish_time": "2025-02-05", "title": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training", "paper_summary": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks.", "paper_summary_zh": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u56e0\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u5907\u53d7\u5b66\u672f\u754c\u548c\u4ea7\u4e1a\u754c\u7684\u5173\u6ce8\u3002\u4e3a\u4e86\u83b7\u5f97\u5177\u6709\u5f3a\u5927\u6027\u80fd\u7684 SLM\uff0c\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u538b\u7f29/\u526a\u679d\u73b0\u6709\u7684\u8d85\u5927\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u5e76\u4e14\u4e0e\u9884\u8bad\u7ec3\u76f8\u6bd4\u6709\u6240\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u7cfb\u5217\u52a0\u901f\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e2\u6d89\u53ca\u7ed3\u6784\u5316\u526a\u679d\uff0c\u53c8\u6d89\u53ca\u6a21\u578b\u8bad\u7ec3\u3002\u6211\u4eec\u53d1\u73b0 1) \u5206\u5c42\u81ea\u9002\u5e94\u526a\u679d (Adapt-Pruner) \u5728 LLM \u4e2d\u975e\u5e38\u6709\u6548\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684\u526a\u679d\u6280\u672f\u6709\u663e\u8457\u7684\u6539\u8fdb\uff0c2) \u81ea\u9002\u5e94\u526a\u679d\u914d\u5907\u8fdb\u4e00\u6b65\u7684\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0e\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff0c3) \u6e10\u8fdb\u526a\u679d\u901a\u8fc7\u5c06\u526a\u679d\u4e0e\u8bad\u7ec3\u4ea4\u9519\u8fdb\u884c\uff0c\u5e76\u4e14\u4e00\u6b21\u4ec5\u79fb\u9664\u4e00\u5c0f\u90e8\u5206\u795e\u7ecf\u5143\uff08\u7ea6 5%\uff09\uff0c\u4ece\u800c\u5e26\u6765\u4e86\u975e\u5e73\u51e1\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728 LLaMA-3.1-8B \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdapt-Pruner \u5728\u5e38\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u5ea6\u65b9\u9762\u6bd4 LLM-Pruner\u3001FLAP \u548c SliceGPT \u7b49\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5e73\u5747\u9ad8\u51fa 1%-7%\u3002\u6b64\u5916\uff0cAdapt-Pruner \u901a\u8fc7\u4ece\u5176\u66f4\u5927\u7684\u5bf9\u5e94\u6a21\u578b\u4e2d\u526a\u679d\uff0c\u5c06 MobileLLM-125M \u5728 MMLU \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u6062\u590d\u5230 600M\uff0c\u5e76\u4e14\u53d1\u73b0\u4e86\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8d85\u8fc7 LLaMA-3.2-1B \u7684\u65b0 1B \u6a21\u578b\u3002", "author": "Boyao Wang et.al.", "authors": "Boyao Wang, Rui Pan, Shizhe Diao, Xingyuan Pan, Jipeng Zhang, Renjie Pi, Tong Zhang", "id": "2502.03460v1", "paper_url": "http://arxiv.org/abs/2502.03460v1", "repo": "null"}}