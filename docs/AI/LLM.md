
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-16**|**Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media**|Ross Deans Kristensen-McLachlan et.al.|[2410.12791v1](http://arxiv.org/abs/2410.12791v1)|null|
|**2024-10-16**|**Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception**|Jihao Zhao et.al.|[2410.12788v1](http://arxiv.org/abs/2410.12788v1)|null|
|**2024-10-16**|**JudgeBench: A Benchmark for Evaluating LLM-based Judges**|Sijun Tan et.al.|[2410.12784v1](http://arxiv.org/abs/2410.12784v1)|null|
|**2024-10-16**|**In-Context Learning Enables Robot Action Prediction in LLMs**|Yida Yin et.al.|[2410.12782v1](http://arxiv.org/abs/2410.12782v1)|null|
|**2024-10-16**|**Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts**|Hongcheng Gao et.al.|[2410.12777v1](http://arxiv.org/abs/2410.12777v1)|null|
|**2024-10-16**|**Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**|Yingya Li et.al.|[2410.12774v1](http://arxiv.org/abs/2410.12774v1)|null|
|**2024-10-16**|**Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions**|Zhenyu Jiang et.al.|[2410.12773v1](http://arxiv.org/abs/2410.12773v1)|null|
|**2024-10-16**|**Vaccinating Federated Learning for Robust Modulation Classification in Distributed Wireless Networks**|Hunmin Lee et.al.|[2410.12772v1](http://arxiv.org/abs/2410.12772v1)|null|
|**2024-10-16**|**Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models**|Luis Barroso-Luque et.al.|[2410.12771v1](http://arxiv.org/abs/2410.12771v1)|null|
|**2024-10-16**|**SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation**|Jaehong Yoon et.al.|[2410.12761v1](http://arxiv.org/abs/2410.12761v1)|null|
|**2024-10-16**|**Unitary Multi-Margin BERT for Robust Natural Language Processing**|Hao-Yuan Chang et.al.|[2410.12759v1](http://arxiv.org/abs/2410.12759v1)|null|
|**2024-10-16**|**StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples**|Ajay Patel et.al.|[2410.12757v1](http://arxiv.org/abs/2410.12757v1)|null|
|**2024-10-16**|**Comparative Analysis of Extrinsic Factors for NER in French**|Grace Yang et.al.|[2410.12750v1](http://arxiv.org/abs/2410.12750v1)|null|
|**2024-10-16**|**CREAM: Consistency Regularized Self-Rewarding Language Models**|Zhaoyang Wang et.al.|[2410.12735v1](http://arxiv.org/abs/2410.12735v1)|null|
|**2024-10-16**|**Counterfactual Generative Modeling with Variational Causal Inference**|Yulun Wu et.al.|[2410.12730v1](http://arxiv.org/abs/2410.12730v1)|null|
|**2024-10-16**|**Transformer based super-resolution downscaling for regional reanalysis: Full domain vs tiling approaches**|Antonio Pérez et.al.|[2410.12728v1](http://arxiv.org/abs/2410.12728v1)|null|
|**2024-10-16**|**WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation**|João Matos et.al.|[2410.12722v1](http://arxiv.org/abs/2410.12722v1)|null|
|**2024-10-16**|**HEnRY: A Multi-Agent System Framework for Multi-Domain Contexts**|Emmanuele Lacavalla et.al.|[2410.12720v1](http://arxiv.org/abs/2410.12720v1)|[link](https://github.com/2mmanu/henry)|
|**2024-10-16**|**FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**|Zhenheng Tang et.al.|[2410.12707v1](http://arxiv.org/abs/2410.12707v1)|null|
|**2024-10-16**|**WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines**|Genta Indra Winata et.al.|[2410.12705v1](http://arxiv.org/abs/2410.12705v1)|null|
|**2024-10-16**|**Sarcasm Detection in a Less-Resourced Language**|Lazar Đoković et.al.|[2410.12704v1](http://arxiv.org/abs/2410.12704v1)|null|
|**2024-10-16**|**Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization**|Xingqi Wang et.al.|[2410.12700v1](http://arxiv.org/abs/2410.12700v1)|[link](https://github.com/achernarwang/LiVO)|
|**2024-10-16**|**VividMed: Vision Language Model with Versatile Visual Grounding for Medicine**|Lingxiao Luo et.al.|[2410.12694v1](http://arxiv.org/abs/2410.12694v1)|null|
|**2024-10-16**|**Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce**|Nedjma Ousidhoum et.al.|[2410.12691v1](http://arxiv.org/abs/2410.12691v1)|null|
|**2024-10-16**|**Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**|Mohamad Abdi et.al.|[2410.12686v1](http://arxiv.org/abs/2410.12686v1)|null|
|**2024-10-16**|**Context Matters: Leveraging Contextual Features for Time Series Forecasting**|Sameep Chattopadhyay et.al.|[2410.12672v1](http://arxiv.org/abs/2410.12672v1)|null|
|**2024-10-16**|**Hamiltonian bridge: A physics-driven generative framework for targeted pattern control**|Vishaal Krishnan et.al.|[2410.12665v1](http://arxiv.org/abs/2410.12665v1)|null|
|**2024-10-16**|**Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models**|Shicheng Xu et.al.|[2410.12662v1](http://arxiv.org/abs/2410.12662v1)|null|
|**2024-10-16**|**Evaluating Morphological Compositional Generalization in Large Language Models**|Mete Ismayilzada et.al.|[2410.12656v1](http://arxiv.org/abs/2410.12656v1)|null|
|**2024-10-16**|**Constrained Posterior Sampling: Time Series Generation with Hard Constraints**|Sai Shankar Narasimhan et.al.|[2410.12652v1](http://arxiv.org/abs/2410.12652v1)|null|
|**2024-10-16**|**Explainable Moral Values: a neuro-symbolic approach to value classification**|Nicolas Lazzari et.al.|[2410.12631v1](http://arxiv.org/abs/2410.12631v1)|null|
|**2024-10-16**|**From Measurement Instruments to Training Data: Leveraging Theory-Driven Synthetic Training Data for Measuring Social Constructs**|Lukas Birkenmaier et.al.|[2410.12622v1](http://arxiv.org/abs/2410.12622v1)|null|
|**2024-10-16**|**Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety, Toxicity, and Legal Reasoning**|Ruimeng Ye et.al.|[2410.12621v1](http://arxiv.org/abs/2410.12621v1)|null|
|**2024-10-16**|**Exploring Model Kinship for Merging Large Language Models**|Yedi Hu et.al.|[2410.12613v1](http://arxiv.org/abs/2410.12613v1)|null|
|**2024-10-16**|**Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning on Knowledge Graphs**|Kai Wang et.al.|[2410.12609v1](http://arxiv.org/abs/2410.12609v1)|null|
|**2024-10-16**|**Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning**|Vernon Y. H. Toh et.al.|[2410.12608v1](http://arxiv.org/abs/2410.12608v1)|null|
|**2024-10-16**|**Low-Rank Adversarial PGD Attack**|Dayana Savostianova et.al.|[2410.12607v1](http://arxiv.org/abs/2410.12607v1)|null|
|**2024-10-16**|**CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization**|Yixi Ding et.al.|[2410.12601v1](http://arxiv.org/abs/2410.12601v1)|null|
|**2024-10-16**|**On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs**|Herun Wan et.al.|[2410.12600v1](http://arxiv.org/abs/2410.12600v1)|null|
|**2024-10-16**|**Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting**|Wei Chen et.al.|[2410.12593v1](http://arxiv.org/abs/2410.12593v1)|null|
|**2024-10-16**|**Rethinking Visual Counterfactual Explanations Through Region Constraint**|Bartlomiej Sobieski et.al.|[2410.12591v1](http://arxiv.org/abs/2410.12591v1)|null|
|**2024-10-16**|**Can We Reverse In-Context Knowledge Edits?**|Paul Youssef et.al.|[2410.12586v1](http://arxiv.org/abs/2410.12586v1)|null|
|**2024-10-16**|**STRUX: An LLM for Decision-Making with Structured Explanations**|Yiming Lu et.al.|[2410.12583v1](http://arxiv.org/abs/2410.12583v1)|null|
|**2024-10-16**|**On the Utility of Domain Modeling Assistance with Large Language Models**|Meriem Ben Chaaben et.al.|[2410.12577v1](http://arxiv.org/abs/2410.12577v1)|null|
|**2024-10-16**|**Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving**|Sihao Wu et.al.|[2410.12568v1](http://arxiv.org/abs/2410.12568v1)|null|
|**2024-10-16**|**Development of Image Collection Method Using YOLO and Siamese Network**|Chan Young Shin et.al.|[2410.12561v1](http://arxiv.org/abs/2410.12561v1)|null|
|**2024-10-16**|**A Claim Decomposition Benchmark for Long-form Answer Verification**|Zhihao Zhang et.al.|[2410.12558v1](http://arxiv.org/abs/2410.12558v1)|null|
|**2024-10-16**|**LLM-based Translation Inference with Iterative Bilingual Understanding**|Andong Chen et.al.|[2410.12543v1](http://arxiv.org/abs/2410.12543v1)|null|
|**2024-10-16**|**Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making**|Stelios Triantafyllou et.al.|[2410.12539v1](http://arxiv.org/abs/2410.12539v1)|null|
|**2024-10-16**|**Characterizing Behavioral Differences and Adaptations of Automated Vehicles and Human Drivers at Unsignalized Intersections: Insights from Waymo and Lyft Open Datasets**|Saeed Rahmani et.al.|[2410.12538v1](http://arxiv.org/abs/2410.12538v1)|null|
|**2024-10-16**|**Is Complex Query Answering Really Complex?**|Cosimo Gregucci et.al.|[2410.12537v1](http://arxiv.org/abs/2410.12537v1)|null|
|**2024-10-16**|**MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration**|Jinjie Wei et.al.|[2410.12532v1](http://arxiv.org/abs/2410.12532v1)|null|
|**2024-10-16**|**Spectrum Sharing using Deep Reinforcement Learning in Vehicular Networks**|Riya Dinesh Deshpande et.al.|[2410.12521v1](http://arxiv.org/abs/2410.12521v1)|null|
|**2024-10-16**|**FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction**|Akriti Jain et.al.|[2410.12513v1](http://arxiv.org/abs/2410.12513v1)|null|
|**2024-10-16**|**Advancing Fairness in Natural Language Processing: From Traditional Methods to Explainability**|Fanny Jourdan et.al.|[2410.12511v1](http://arxiv.org/abs/2410.12511v1)|null|
|**2024-10-16**|**Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions**|Ilias Tachmazidis et.al.|[2410.12509v1](http://arxiv.org/abs/2410.12509v1)|null|
|**2024-10-16**|**DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning**|Jiabao Wei et.al.|[2410.12501v1](http://arxiv.org/abs/2410.12501v1)|null|
|**2024-10-16**|**With a Grain of SALT: Are LLMs Fair Across Social Dimensions?**|Samee Arif et.al.|[2410.12499v1](http://arxiv.org/abs/2410.12499v1)|null|
|**2024-10-16**|**End-to-end Planner Training for Language Modeling**|Nathan Cornille et.al.|[2410.12492v1](http://arxiv.org/abs/2410.12492v1)|null|
|**2024-10-16**|**Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL**|Jared Joselowitz et.al.|[2410.12491v1](http://arxiv.org/abs/2410.12491v1)|null|
|**2024-10-16**|**Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective**|Yongxin Zhu et.al.|[2410.12490v1](http://arxiv.org/abs/2410.12490v1)|[link](https://github.com/DAMO-NLP-SG/DiGIT)|
|**2024-10-16**|**SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling**|Loris Gaven et.al.|[2410.12481v1](http://arxiv.org/abs/2410.12481v1)|null|
|**2024-10-16**|**KcMF: A Knowledge-compliant Framework for Schema and Entity Matching with Fine-tuning-free LLMs**|Yongqin Xu et.al.|[2410.12480v1](http://arxiv.org/abs/2410.12480v1)|null|
|**2024-10-16**|**MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models**|Boyang Xue et.al.|[2410.12478v1](http://arxiv.org/abs/2410.12478v1)|null|
|**2024-10-16**|**Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial Generation**|Zerui Xu et.al.|[2410.12476v1](http://arxiv.org/abs/2410.12476v1)|null|
|**2024-10-16**|**Unifying Economic and Language Models for Enhanced Sentiment Analysis of the Oil Market**|Himmet Kaplan et.al.|[2410.12473v1](http://arxiv.org/abs/2410.12473v1)|null|
|**2024-10-16**|**Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels**|Leo Kohlenberg et.al.|[2410.12470v1](http://arxiv.org/abs/2410.12470v1)|null|
|**2024-10-16**|**Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios**|Zhi Chen et.al.|[2410.12468v1](http://arxiv.org/abs/2410.12468v1)|null|
|**2024-10-16**|**Bridging the Language Gaps in Large Language Models with Inference-Time Cross-Lingual Intervention**|Weixuan Wang et.al.|[2410.12462v1](http://arxiv.org/abs/2410.12462v1)|null|
|**2024-10-16**|**The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph**|Minghao Wu et.al.|[2410.12458v1](http://arxiv.org/abs/2410.12458v1)|null|
|**2024-10-16**|**Sharpness-Aware Black-Box Optimization**|Feiyang Ye et.al.|[2410.12457v1](http://arxiv.org/abs/2410.12457v1)|null|
|**2024-10-16**|**Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation for Korean LLMs**|Hyeonwoo Kim et.al.|[2410.12445v1](http://arxiv.org/abs/2410.12445v1)|null|
|**2024-10-16**|**Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar Question Generation Using Large Language Models**|Mengze Hong et.al.|[2410.12444v1](http://arxiv.org/abs/2410.12444v1)|null|
|**2024-10-16**|**Reconstruction of Differentially Private Text Sanitization via Large Language Models**|Shuchao Pang et.al.|[2410.12443v1](http://arxiv.org/abs/2410.12443v1)|null|
|**2024-10-16**|**Conformity in Large Language Models**|Xiaochen Zhu et.al.|[2410.12428v1](http://arxiv.org/abs/2410.12428v1)|null|
|**2024-10-16**|**Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding**|Daichi Hayakawa et.al.|[2410.12413v1](http://arxiv.org/abs/2410.12413v1)|null|
|**2024-10-16**|**Revealing the Barriers of Language Agents in Planning**|Jian Xie et.al.|[2410.12409v1](http://arxiv.org/abs/2410.12409v1)|null|
|**2024-10-16**|**Beyond Coarse-Grained Matching in Video-Text Retrieval**|Aozhu Chen et.al.|[2410.12407v1](http://arxiv.org/abs/2410.12407v1)|null|
|**2024-10-16**|**ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs**|Jingming Zhuo et.al.|[2410.12405v1](http://arxiv.org/abs/2410.12405v1)|null|
|**2024-10-16**|**Tracking Universal Features Through Fine-Tuning and Model Merging**|Niels Horn et.al.|[2410.12391v1](http://arxiv.org/abs/2410.12391v1)|null|
|**2024-10-16**|**A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetic**|Lennert De Smet et.al.|[2410.12389v1](http://arxiv.org/abs/2410.12389v1)|null|
|**2024-10-16**|**Prompt Compression for Large Language Models: A Survey**|Zongqian Li et.al.|[2410.12388v1](http://arxiv.org/abs/2410.12388v1)|null|
|**2024-10-16**|**HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks**|Fengji Zhang et.al.|[2410.12381v1](http://arxiv.org/abs/2410.12381v1)|null|
|**2024-10-16**|**Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models**|Amin Abolghasemi et.al.|[2410.12380v1](http://arxiv.org/abs/2410.12380v1)|null|
|**2024-10-16**|**HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims**|Yejun Yoon et.al.|[2410.12377v1](http://arxiv.org/abs/2410.12377v1)|null|
|**2024-10-16**|**ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing**|Qingming Lin et.al.|[2410.12376v1](http://arxiv.org/abs/2410.12376v1)|null|
|**2024-10-16**|**PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking**|Markus J. Buehler et.al.|[2410.12375v1](http://arxiv.org/abs/2410.12375v1)|null|
|**2024-10-16**|**Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance**|Yaxi Lu et.al.|[2410.12361v1](http://arxiv.org/abs/2410.12361v1)|null|
|**2024-10-16**|**Towards Neural Scaling Laws for Time Series Foundation Models**|Qingren Yao et.al.|[2410.12360v1](http://arxiv.org/abs/2410.12360v1)|null|
|**2024-10-16**|**GECTurk WEB: An Explainable Online Platform for Turkish Grammatical Error Detection and Correction**|Ali Gebeşçe et.al.|[2410.12350v1](http://arxiv.org/abs/2410.12350v1)|null|
|**2024-10-16**|**TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant**|Guopeng Li et.al.|[2410.12342v1](http://arxiv.org/abs/2410.12342v1)|null|
|**2024-10-16**|**A linguistic analysis of undesirable outcomes in the era of generative AI**|Daniele Gambetta et.al.|[2410.12341v1](http://arxiv.org/abs/2410.12341v1)|null|
|**2024-10-16**|**Understanding the Role of LLMs in Multimodal Evaluation Benchmarks**|Botian Jiang et.al.|[2410.12329v1](http://arxiv.org/abs/2410.12329v1)|null|
|**2024-10-16**|**Neuron-based Personality Trait Induction in Large Language Models**|Jia Deng et.al.|[2410.12327v1](http://arxiv.org/abs/2410.12327v1)|null|
|**2024-10-16**|**Optimizing Low-Resource Language Model Training: Comprehensive Analysis of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches**|Kosuke Akimoto et.al.|[2410.12325v1](http://arxiv.org/abs/2410.12325v1)|null|
|**2024-10-16**|**Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up**|Jiahao Yuan et.al.|[2410.12323v1](http://arxiv.org/abs/2410.12323v1)|null|
|**2024-10-16**|**UTF:Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification**|Jiacheng Cai et.al.|[2410.12318v1](http://arxiv.org/abs/2410.12318v1)|null|
|**2024-10-16**|**FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization**|Cheng Yu et.al.|[2410.12312v1](http://arxiv.org/abs/2410.12312v1)|null|
|**2024-10-16**|**Open Domain Question Answering with Conflicting Contexts**|Siyi Liu et.al.|[2410.12311v1](http://arxiv.org/abs/2410.12311v1)|null|
|**2024-10-16**|**Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors**|Weixuan Wang et.al.|[2410.12299v1](http://arxiv.org/abs/2410.12299v1)|null|

#### Abstracts
##### **Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media**
2410.12791v1 by Ross Deans Kristensen-McLachlan, Rebecca M. M. Hicke, Márton Kardos, Mette Thunø

Does the People's Republic of China (PRC) interfere with European elections
through ethnic Chinese diaspora media? This question forms the basis of an
ongoing research project exploring how PRC narratives about European elections
are represented in Chinese diaspora media, and thus the objectives of PRC news
media manipulation. In order to study diaspora media efficiently and at scale,
it is necessary to use techniques derived from quantitative text analysis, such
as topic modelling. In this paper, we present a pipeline for studying
information dynamics in Chinese media. Firstly, we present KeyNMF, a new
approach to static and dynamic topic modelling using transformer-based
contextual embedding models. We provide benchmark evaluations to demonstrate
that our approach is competitive on a number of Chinese datasets and metrics.
Secondly, we integrate KeyNMF with existing methods for describing information
dynamics in complex systems. We apply this pipeline to data from five news
sites, focusing on the period of time leading up to the 2024 European
parliamentary elections. Our methods and results demonstrate the effectiveness
of KeyNMF for studying information dynamics in Chinese media and lay groundwork
for further work addressing the broader research questions.

摘要：中华人民共和国（PRC）是否通过海外华文媒体干预欧洲选举？这个问题构成了一个正在进行的研究项目的基础，该项目探讨了有关欧洲选举的中华人民共和国叙述是如何在海外华文媒体中呈现的，以及中华人民共和国新闻媒体操纵的目标。为了有效且大规模地研究海外媒体，有必要使用源自定量文本分析的技术，例如主题建模。在本文中，我们提出了一个用于研究中文媒体中信息动态的管道。首先，我们提出了 KeyNMF，这是一种使用基于 Transformer 的上下文嵌入模型进行静态和动态主题建模的新方法。我们提供了基准评估来证明我们的方法在许多中文数据集和指标上具有竞争力。其次，我们将 KeyNMF 与描述复杂系统中信息动态的现有方法相集成。我们将此管道应用于来自五个新闻网站的数据，重点关注 2024 年欧洲议会选举前的这段时间。我们的方法和结果证明了 KeyNMF 在研究中文媒体中信息动态方面的有效性，并为进一步解决更广泛的研究问题奠定了基础。

##### **Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception**
2410.12788v1 by Jihao Zhao, Zhiyuan Ji, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li

Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed two strategies based on LLMs: Margin
Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform
binary classification on whether consecutive sentences need to be segmented,
making decisions based on the probability difference obtained from margin
sampling. The latter precisely identifies text chunk boundaries by analyzing
the characteristics of perplexity distribution. Additionally, considering the
inherent complexity of different texts, we propose a strategy that combines
Meta-Chunking with dynamic merging to achieve a balance between fine-grained
and coarse-grained text chunking. Experiments conducted on eleven datasets
demonstrate that Meta-Chunking can more efficiently improve the performance of
single-hop and multi-hop question answering based on RAG. For instance, on the
2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only
consuming 45.8% of the time. Our code is available at
https://github.com/IAAR-Shanghai/Meta-Chunking.

摘要：檢索增強生成 (RAG) 雖然是大型語言模型 (LLM) 的可行補充，但其管道中經常忽略文本分塊的關鍵方面，這會影響知識密集型任務的品質。本文介紹了元分塊的概念，它指的是句子和段落之間的粒度，由段落中具備深入語言邏輯關聯的一系列句子組成。為了實作元分塊，我們基於 LLM 設計了兩種策略：邊緣取樣分塊和困惑度分塊。前者採用 LLM 對連續的句子是否需要分段進行二元分類，根據邊緣取樣獲得的機率差異做出決策。後者透過分析困惑度分佈的特徵精確識別文本塊界線。此外，考量到不同文本的固有複雜性，我們提出了一種結合元分塊與動態合併的策略，以在細粒度和粗粒度文本分塊之間取得平衡。在 11 個資料集上進行的實驗證明，元分塊可以更有效地提升基於 RAG 的單跳和多跳問題解答效能。例如，在 2WikiMultihopQA 資料集上，它比相似性分塊高出 1.32 個百分點，同時僅耗費 45.8% 的時間。我們的程式碼可在 https://github.com/IAAR-Shanghai/Meta-Chunking 取得。

##### **JudgeBench: A Benchmark for Evaluating LLM-based Judges**
2410.12784v1 by Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, Ion Stoica

LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .

摘要：<paragraph>基於 LLM 的評審已成為人類評估的可擴充替代方案，且愈來愈常被用於評估、比較和改善模型。然而，基於 LLM 的評審本身的可靠性卻鮮少受到審查。隨著 LLM 變得更先進，其回應也變得更複雜，需要更強大的評審來評估它們。現有的基準主要關注評審與人類偏好的吻合度，但通常無法考量更具挑戰性的任務，其中群眾外包的人類偏好是事實和邏輯正確性的不良指標。為了解決這個問題，我們提出一個新穎的評估架構，以客觀評估基於 LLM 的評審。根據這個架構，我們提出 JudgeBench，一個用於評估基於 LLM 的評審在跨越知識、推理、數學和編碼的具挑戰性回應配對上的基準。JudgeBench 藉由一個新穎的管道，將現有的困難資料集轉換成具挑戰性的回應配對，其中偏好標籤反映出客觀正確性。我們對一系列提示式評審、微調評審、多重代理評審和獎勵模型進行的全面評估顯示，JudgeBench 構成的挑戰顯著大於先前的基準，許多強大的模型（例如 GPT-4o）的表現僅比隨機猜測好一點。總體而言，JudgeBench 提供了一個可靠的平台，用於評估愈來愈先進的基於 LLM 的評審。資料和程式碼可於 https://github.com/ScalerLab/JudgeBench 取得。</paragraph>

##### **In-Context Learning Enables Robot Action Prediction in LLMs**
2410.12782v1 by Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei Herzig

Recently, Large Language Models (LLMs) have achieved remarkable success using
in-context learning (ICL) in the language domain. However, leveraging the ICL
capabilities within LLMs to directly predict robot actions remains largely
unexplored. In this paper, we introduce RoboPrompt, a framework that enables
off-the-shelf text-only LLMs to directly predict robot actions through ICL
without training. Our approach first heuristically identifies keyframes that
capture important moments from an episode. Next, we extract end-effector
actions from these keyframes as well as the estimated initial object poses, and
both are converted into textual descriptions. Finally, we construct a
structured template to form ICL demonstrations from these textual descriptions
and a task instruction. This enables an LLM to directly predict robot actions
at test time. Through extensive experiments and analysis, RoboPrompt shows
stronger performance over zero-shot and ICL baselines in simulated and
real-world settings.

摘要：近期，大型语言模型 (LLM) 在语言领域使用情境学习 (ICL) 取得了显著的成功。然而，利用 LLM 内的 ICL 能力直接预测机器人动作在很大程度上仍未得到探索。在本文中，我们介绍了 RoboPrompt，这是一个框架，它使现成的纯文本 LLM 能够通过 ICL 直接预测机器人动作，而无需训练。我们的方法首先启发式地识别关键帧，以捕捉剧集中的重要时刻。接下来，我们从这些关键帧中提取末端执行器动作以及估计的初始物体姿态，并将两者转换为文本描述。最后，我们构建了一个结构化模板，以根据这些文本描述和任务说明形成 ICL 演示。这使 LLM 能够在测试时直接预测机器人动作。通过广泛的实验和分析，RoboPrompt 在模拟和实际环境中显示出比零样本和 ICL 基线更强的性能。

##### **Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts**
2410.12777v1 by Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin

With the rapid progress of diffusion-based content generation, significant
efforts are being made to unlearn harmful or copyrighted concepts from
pretrained diffusion models (DMs) to prevent potential model misuse. However,
it is observed that even when DMs are properly unlearned before release,
malicious finetuning can compromise this process, causing DMs to relearn the
unlearned concepts. This occurs partly because certain benign concepts (e.g.,
"skin") retained in DMs are related to the unlearned ones (e.g., "nudity"),
facilitating their relearning via finetuning. To address this, we propose
meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an
unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes
malicious finetuning on unlearned concepts, the related benign concepts
retained within it will be triggered to self-destruct, hindering the relearning
of unlearned concepts. Our meta-unlearning framework is compatible with most
existing unlearning methods, requiring only the addition of an
easy-to-implement meta objective. We validate our approach through empirical
experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4
and SDXL), supported by extensive ablation studies. Our code is available at
https://github.com/sail-sg/Meta-Unlearning.

摘要：<paragraph>隨著基於擴散的內容生成技術的快速進展，人們正致力於解除預訓練擴散模型 (DM) 中有害或受版權保護的概念，以防止潛在的模型誤用。然而，觀察發現，即使在 DM 在發布前已適當地解除學習，惡意的微調仍可能損害此程序，導致 DM 重新學習已解除學習的概念。這部分是因為 DM 中保留的某些良性概念（例如「皮膚」）與已解除學習的概念（例如「裸露」）相關，從而通過微調促進它們的重新學習。為了解決這個問題，我們提出對 DM 進行元解除學習。直觀地說，一個經過元解除學習的 DM 在按原樣使用時應該表現得像一個未解除學習的 DM；此外，如果經過元解除學習的 DM 對未解除學習的概念進行惡意微調，其中保留的相關良性概念將被觸發自毀，從而阻礙重新學習未解除學習的概念。我們的元解除學習框架與大多數現有的解除學習方法相容，只需要新增一個易於實現的元目標。我們通過對 Stable Diffusion 模型（SD-v1-4 和 SDXL）的元解除學習概念進行實證實驗來驗證我們的做法，並得到廣泛的消融研究的支援。我們的程式碼可在 https://github.com/sail-sg/Meta-Unlearning 取得。</paragraph>

##### **Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**
2410.12774v1 by Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova

The success of multi-task learning can depend heavily on which tasks are
grouped together. Naively grouping all tasks or a random set of tasks can
result in negative transfer, with the multi-task models performing worse than
single-task models. Though many efforts have been made to identify task
groupings and to measure the relatedness among different tasks, it remains a
challenging research topic to define a metric to identify the best task
grouping out of a pool of many potential task combinations. We propose a metric
of task relatedness based on task difficulty measured by pointwise V-usable
information (PVI). PVI is a recently proposed metric to estimate how much
usable information a dataset contains given a model. We hypothesize that tasks
with not statistically different PVI estimates are similar enough to benefit
from the joint learning process. We conduct comprehensive experiments to
evaluate the feasibility of this metric for task grouping on 15 NLP datasets in
the general, biomedical, and clinical domains. We compare the results of the
joint learners against single learners, existing baseline methods, and recent
large language models, including Llama 2 and GPT-4. The results show that by
grouping tasks with similar PVI estimates, the joint learners yielded
competitive results with fewer total parameters, with consistent performance
across domains.

摘要：多任務學習的成功很大程度上取決於將哪些任務分組在一起。天真地將所有任務或一組隨機任務分組可能會導致負向遷移，多任務模型的表現會比單任務模型差。儘管已做出許多努力來識別任務分組並衡量不同任務之間的關聯性，但定義一個指標以從許多潛在任務組合中識別出最佳任務分組仍然是一個具有挑戰性的研究課題。我們提出了一個基於點式 V 可用資訊 (PVI) 衡量的任務難度來衡量任務相關性的指標。PVI 是一個最近提出的指標，用於估計給定模型資料集包含多少可用資訊。我們假設 PVI 估計在統計上沒有差異的任務足夠相似，可以從聯合學習過程中受益。我們進行了全面的實驗，以評估此指標在 15 個一般、生物醫學和臨床領域的 NLP 資料集上進行任務分組的可行性。我們將聯合學習者的結果與單一學習者、現有的基準方法和最近的大語言模型（包括 Llama 2 和 GPT-4）進行比較。結果表明，通過將具有相似 PVI 估計值的任務分組，聯合學習者以較少的總參數產生了具有競爭力的結果，並且在各個領域中表現一致。

##### **Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions**
2410.12773v1 by Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, Yuke Zhu

Humanoid robots, with their human-like embodiment, have the potential to
integrate seamlessly into human environments. Critical to their coexistence and
cooperation with humans is the ability to understand natural language
communications and exhibit human-like behaviors. This work focuses on
generating diverse whole-body motions for humanoid robots from language
descriptions. We leverage human motion priors from extensive human motion
datasets to initialize humanoid motions and employ the commonsense reasoning
capabilities of Vision Language Models (VLMs) to edit and refine these motions.
Our approach demonstrates the capability to produce natural, expressive, and
text-aligned humanoid motions, validated through both simulated and real-world
experiments. More videos can be found at
https://ut-austin-rpl.github.io/Harmon/.

摘要：類人機器人，具有類人的體現，具有無縫整合到人類環境中的潛力。對於它們與人類的共存和合作至關重要的是理解自然語言溝通並表現出類人的行為的能力。這項工作重點在於根據語言描述為類人機器人產生多樣化的全身動作。我們利用來自廣泛人類動作數據集的人類動作先驗來初始化類人動作，並採用 Vision Language Models (VLMs) 的常識推理能力來編輯和完善這些動作。我們的做法展示了產生自然、富有表現力和與文本對齊的類人動作的能力，通過模擬和現實世界的實驗得到驗證。更多影片可以在 https://ut-austin-rpl.github.io/Harmon/ 找到。

##### **Vaccinating Federated Learning for Robust Modulation Classification in Distributed Wireless Networks**
2410.12772v1 by Hunmin Lee, Hongju Seong, Wonbin Kim, Hyeokchan Kwon, Daehee Seo

Automatic modulation classification (AMC) serves a vital role in ensuring
efficient and reliable communication services within distributed wireless
networks. Recent developments have seen a surge in interest in deep neural
network (DNN)-based AMC models, with Federated Learning (FL) emerging as a
promising framework. Despite these advancements, the presence of various noises
within the signal exerts significant challenges while optimizing models to
capture salient features. Furthermore, existing FL-based AMC models commonly
rely on linear aggregation strategies, which face notable difficulties in
integrating locally fine-tuned parameters within practical non-IID (Independent
and Identically Distributed) environments, thereby hindering optimal learning
convergence. To address these challenges, we propose FedVaccine, a novel FL
model aimed at improving generalizability across signals with varying noise
levels by deliberately introducing a balanced level of noise. This is
accomplished through our proposed harmonic noise resilience approach, which
identifies an optimal noise tolerance for DNN models, thereby regulating the
training process and mitigating overfitting. Additionally, FedVaccine overcomes
the limitations of existing FL-based AMC models' linear aggregation by
employing a split-learning strategy using structural clustering topology and
local queue data structure, enabling adaptive and cumulative updates to local
models. Our experimental results, including IID and non-IID datasets as well as
ablation studies, confirm FedVaccine's robust performance and superiority over
existing FL-based AMC approaches across different noise levels. These findings
highlight FedVaccine's potential to enhance the reliability and performance of
AMC systems in practical wireless network environments.

摘要：<paragraph>自動調變分類 (AMC) 在確保分布式無線網路中的通訊服務有效率且可靠方面扮演至關重要的角色。近期發展已看到對基於深度神經網路 (DNN) 的 AMC 模型的興趣激增，其中聯合學習 (FL) 成為一個有前途的架構。儘管有這些進展，但訊號中存在各種雜訊在最佳化模型以擷取顯著特徵時造成重大挑戰。此外，現有的基於 FL 的 AMC 模型通常依賴線性聚合策略，在整合實際非 IID（獨立且同分布）環境中的局部微調參數時面臨顯著困難，從而阻礙最佳學習收斂。為了應對這些挑戰，我們提出 FedVaccine，一種新的 FL 模型，旨在透過刻意引入平衡程度的雜訊來改善不同雜訊層級訊號間的泛化能力。這透過我們提出的諧波雜訊復原力方法來達成，該方法識別 DNN 模型的最佳雜訊容忍度，從而規範訓練過程並減輕過度擬合。此外，FedVaccine 透過使用結構化叢集拓撲和局部佇列資料結構來採用分割學習策略，克服現有基於 FL 的 AMC 模型線性聚合的限制，讓局部模型能夠進行適應性和累積性更新。我們的實驗結果，包括 IID 和非 IID 資料集以及消融研究，確認 FedVaccine 在不同雜訊層級中表現強健，且優於現有的基於 FL 的 AMC 方法。這些發現突顯 FedVaccine 提升實際無線網路環境中 AMC 系統可靠性和效能的潛力。</paragraph>

##### **Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models**
2410.12771v1 by Luis Barroso-Luque, Muhammed Shuaibi, Xiang Fu, Brandon M. Wood, Misko Dzamba, Meng Gao, Ammar Rizvi, C. Lawrence Zitnick, Zachary W. Ulissi

The ability to discover new materials with desirable properties is critical
for numerous applications from helping mitigate climate change to advances in
next generation computing hardware. AI has the potential to accelerate
materials discovery and design by more effectively exploring the chemical space
compared to other computational methods or by trial-and-error. While
substantial progress has been made on AI for materials data, benchmarks, and
models, a barrier that has emerged is the lack of publicly available training
data and open pre-trained models. To address this, we present a Meta FAIR
release of the Open Materials 2024 (OMat24) large-scale open dataset and an
accompanying set of pre-trained models. OMat24 contains over 110 million
density functional theory (DFT) calculations focused on structural and
compositional diversity. Our EquiformerV2 models achieve state-of-the-art
performance on the Matbench Discovery leaderboard and are capable of predicting
ground-state stability and formation energies to an F1 score above 0.9 and an
accuracy of 20 meV/atom, respectively. We explore the impact of model size,
auxiliary denoising objectives, and fine-tuning on performance across a range
of datasets including OMat24, MPtraj, and Alexandria. The open release of the
OMat24 dataset and models enables the research community to build upon our
efforts and drive further advancements in AI-assisted materials science.

摘要：發現具有理想特性的新材料的能力對於從幫助減緩氣候變遷到推進下一代運算硬體的眾多應用至關重要。與其他運算方法或試錯法相比，人工智慧有潛力透過更有效地探索化學空間來加速材料發現和設計。儘管在材料資料、基準和模型方面的人工智慧已取得實質進展，但出現了一個障礙，那就是缺乏公開可用的訓練資料和開放的預訓練模型。為了解決這個問題，我們提出了 Meta FAIR 發布的開放材料 2024 (OMat24) 大型開放資料集和一組配套的預訓練模型。OMat24 包含超過 1.1 億個密度泛函理論 (DFT) 計算，重點在於結構和組成多樣性。我們的 EquiformerV2 模型在 Matbench Discovery 排行榜上取得了最先進的效能，並且能夠預測基態穩定性和形成能，分別達到 0.9 以上的 F1 分數和 20 meV/atom 的準確度。我們探討了模型大小、輔助去噪目標和微調對 OMat24、MPtraj 和 Alexandria 等一系列資料集效能的影響。OMat24 資料集和模型的開放發布使研究社群能夠建立在我們的努力之上，並推動人工智慧輔助材料科學的進一步發展。

##### **SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation**
2410.12761v1 by Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal

Recent advances in diffusion models have significantly enhanced their ability
to generate high-quality images and videos, but they have also increased the
risk of producing unsafe content. Existing unlearning/editing-based methods for
safe generation remove harmful concepts from models but face several
challenges: (1) They cannot instantly remove harmful concepts without training.
(2) Their safe generation capabilities depend on collected training data. (3)
They alter model weights, risking degradation in quality for content unrelated
to toxic concepts. To address these, we propose SAFREE, a novel, training-free
approach for safe T2I and T2V, that does not alter the model's weights.
Specifically, we detect a subspace corresponding to a set of toxic concepts in
the text embedding space and steer prompt embeddings away from this subspace,
thereby filtering out harmful content while preserving intended semantics. To
balance the trade-off between filtering toxicity and preserving safe concepts,
SAFREE incorporates a novel self-validating filtering mechanism that
dynamically adjusts the denoising steps when applying the filtered embeddings.
Additionally, we incorporate adaptive re-attention mechanisms within the
diffusion latent space to selectively diminish the influence of features
related to toxic concepts at the pixel level. In the end, SAFREE ensures
coherent safety checking, preserving the fidelity, quality, and safety of the
output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I
generation compared to training-free baselines and effectively filters targeted
concepts while maintaining high-quality images. It also shows competitive
results against training-based methods. We extend SAFREE to various T2I
backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE
provides a robust and adaptable safeguard for ensuring safe visual generation.

摘要：最近擴散模型的進展顯著提升了它們生成高品質影像和影片的能力，但它們也增加了產生不安全內容的風險。現有的基於遺忘/編輯的方法用於安全生成，會從模型中移除有害的概念，但面臨幾個挑戰：(1) 它們無法在沒有訓練的情況下立即移除有害概念。(2) 它們的安全生成能力取決於收集到的訓練資料。(3) 它們會改變模型權重，冒著與有毒概念無關的內容品質下降的風險。為了解決這些問題，我們提出 SAFREE，一種新穎的、免訓練的安全 T2I 和 T2V 方法，不會改變模型的權重。具體來說，我們在文字嵌入空間中偵測一個對應於一組有毒概念的子空間，並引導提示嵌入遠離這個子空間，從而過濾掉有害內容，同時保留預期的語義。為了平衡過濾毒性和保留安全概念之間的取捨，SAFREE 結合了一種新穎的自驗證過濾機制，在應用過濾嵌入時動態調整去噪步驟。此外，我們在擴散潛在空間中結合了自適應重新注意機制，以選擇性地減弱與像素級別的有毒概念相關特徵的影響。最後，SAFREE 確保了一致的安全檢查，保留了輸出的保真度、品質和安全性。與免訓練基線相比，SAFREE 在抑制 T2I 生成中的不安全內容方面實現了 SOTA 效能，並有效過濾目標概念，同時保持高品質的影像。它也顯示出與基於訓練的方法相比具有競爭力的結果。我們將 SAFREE 延伸到各種 T2I 主幹和 T2V 任務，展示了它的靈活性與泛化性。SAFREE 為確保安全的視覺生成提供了一個強健且適應性強的保障。

##### **Unitary Multi-Margin BERT for Robust Natural Language Processing**
2410.12759v1 by Hao-Yuan Chang, Kang L. Wang

Recent developments in adversarial attacks on deep learning leave many
mission-critical natural language processing (NLP) systems at risk of
exploitation. To address the lack of computationally efficient adversarial
defense methods, this paper reports a novel, universal technique that
drastically improves the robustness of Bidirectional Encoder Representations
from Transformers (BERT) by combining the unitary weights with the multi-margin
loss. We discover that the marriage of these two simple ideas amplifies the
protection against malicious interference. Our model, the unitary multi-margin
BERT (UniBERT), boosts post-attack classification accuracies significantly by
5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,
the pre-attack and post-attack accuracy tradeoff can be adjusted via a single
scalar parameter to best fit the design requirements for the target
applications.

摘要：深度學習中對抗攻擊的最新發展讓許多任務關鍵的自然語言處理 (NLP) 系統面臨被利用的風險。為了解決缺乏計算效率高的對抗防禦方法，本文報告了一種新穎、通用的技術，它透過結合單元權重和多邊界損失，大幅提升了來自 Transformer 的雙向編碼器表示 (BERT) 的穩健性。我們發現這兩個簡單想法的結合放大了對惡意干擾的防護。我們的模型，單元多邊界 BERT (UniBERT)，將後攻擊分類準確率大幅提升了 5.3% 至 73.8%，同時維持有競爭力的前攻擊準確率。此外，前攻擊和後攻擊準確率的權衡可以透過單一標量參數進行調整，以最佳符合目標應用程式的設計需求。

##### **StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples**
2410.12757v1 by Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris Callison-Burch

Style representations aim to embed texts with similar writing styles closely
and texts with different styles far apart, regardless of content. However, the
contrastive triplets often used for training these representations may vary in
both style and content, leading to potential content leakage in the
representations. We introduce StyleDistance, a novel approach to training
stronger content-independent style embeddings. We use a large language model to
create a synthetic dataset of near-exact paraphrases with controlled style
variations, and produce positive and negative examples across 40 distinct style
features for precise contrastive learning. We assess the quality of our
synthetic data and embeddings through human and automatic evaluations.
StyleDistance enhances the content-independence of style embeddings, which
generalize to real-world benchmarks and outperform leading style
representations in downstream applications. Our model can be found at
https://huggingface.co/StyleDistance/styledistance .

摘要：風格表示旨在將具有相似寫作風格的文本緊密嵌入，而將具有不同風格的文本遠遠分開，而不管內容如何。然而，通常用於訓練這些表示的對比三元組可能在風格和內容上有所不同，從而導致表示中潛在的內容洩漏。我們引入了 StyleDistance，這是一種訓練更強大的與內容無關的風格嵌入的新方法。我們使用一個大型語言模型來創建一個具有受控風格變化的近似同義詞的合成數據集，並針對 40 個不同的風格特徵產生正例和反例，以進行精確的對比學習。我們通過人工和自動評估來評估我們合成數據和嵌入的質量。StyleDistance 增強了風格嵌入的內容獨立性，它可以推廣到現實世界的基準，並且在下游應用中優於領先的風格表示。我們的模型可以在 https://huggingface.co/StyleDistance/styledistance 找到。

##### **Comparative Analysis of Extrinsic Factors for NER in French**
2410.12750v1 by Grace Yang, Zhiyi Li, Yandong Liu, Jungyeul Park

Named entity recognition (NER) is a crucial task that aims to identify
structured information, which is often replete with complex, technical terms
and a high degree of variability. Accurate and reliable NER can facilitate the
extraction and analysis of important information. However, NER for other than
English is challenging due to limited data availability, as the high expertise,
time, and expenses are required to annotate its data. In this paper, by using
the limited data, we explore various factors including model structure, corpus
annotation scheme and data augmentation techniques to improve the performance
of a NER model for French. Our experiments demonstrate that these approaches
can significantly improve the model's F1 score from original CRF score of 62.41
to 79.39. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance where the size of data is limited.

摘要：命名實體辨識 (NER) 是一項關鍵的任務，旨在辨識結構化的資訊，這些資訊通常充斥著複雜的技術術語和高度的可變性。準確且可靠的 NER 能夠促進重要資訊的萃取和分析。然而，除了英文之外的 NER 具有挑戰性，因為資料的取得有限，而且標註資料需要高度的專業知識、時間和金錢成本。在本文中，我們透過使用有限的資料，探討各種因素，包括模型結構、語料標註方案和資料擴充技術，以提升法語 NER 模型的效能。我們的實驗證明，這些方法可以顯著地提升模型的 F1 分數，從原本的 CRF 分數 62.41 提升至 79.39。我們的研究結果顯示，考慮不同的外在因素並結合這些技術，對於提升資料量有限情況下的 NER 效能而言是一種有前途的方法。

##### **CREAM: Consistency Regularized Self-Rewarding Language Models**
2410.12735v1 by Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao

Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the rewarding consistency across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.

摘要：<paragraph>最近的自獎勵大型語言模型 (LLM) 已成功應用 LLM 作為評判，以迭代方式改善對齊效能，而無需人類註解偏好資料。這些方法通常利用相同的 LLM 作為策略模型（產生回應）和獎勵模型（對這些回應進行評分和排名）。排名的回應然後用作偏好對，以透過直接對齊技術（例如 DPO）訓練 LLM。然而，值得注意的是，在整個過程中，沒有保證獎勵和排名的準確性，這對於確保準確的獎勵和高品質的偏好資料至關重要。來自相對較小的 LLM（例如，7B 參數）的經驗結果也表明，在某些情況下，經過多次迭代後，來自自獎勵的改進可能會減弱，我們假設這是由於獎勵系統中累積的偏差。這種偏差可能會導致不可靠的偏好資料，以訓練 LLM。為了解決這個問題，我們首先制定和分析了自獎勵語言模型的廣義迭代偏好微調架構。然後，我們將正則化引入這個廣義架構，以減輕自獎勵過程中過於自信的偏好標籤。基於這個理論見解，我們提出了一個一致性正則化自我獎勵語言模型 (CREAM)，它利用不同迭代之間的獎勵一致性來規範自我獎勵訓練，幫助模型從更可靠的偏好資料中學習。有了這個明確的正則化，我們的經驗結果證明了 CREAM 在改善獎勵一致性和對齊效能方面的優越性。程式碼已公開發布於 https://github.com/Raibows/CREAM。</paragraph>

##### **Counterfactual Generative Modeling with Variational Causal Inference**
2410.12730v1 by Yulun Wu, Louie McConnell, Claudia Iriondo

Estimating an individual's potential outcomes under counterfactual treatments
is a challenging task for traditional causal inference and supervised learning
approaches when the outcome is high-dimensional (e.g. gene expressions, facial
images) and covariates are relatively limited. In this case, to predict one's
outcomes under counterfactual treatments, it is crucial to leverage individual
information contained in its high-dimensional observed outcome in addition to
the covariates. Prior works using variational inference in counterfactual
generative modeling have been focusing on neural adaptations and model variants
within the conditional variational autoencoder formulation, which we argue is
fundamentally ill-suited to the notion of counterfactual in causal inference.
In this work, we present a novel variational Bayesian causal inference
framework and its theoretical backings to properly handle counterfactual
generative modeling tasks, through which we are able to conduct counterfactual
supervision end-to-end during training without any counterfactual samples, and
encourage latent disentanglement that aids the correct identification of causal
effect in counterfactual generations. In experiments, we demonstrate the
advantage of our framework compared to state-of-the-art models in
counterfactual generative modeling on multiple benchmarks.

摘要：在結果為高維度（例如基因表達、人臉影像）且協變數相對有限時，估計個體在反事實處理下的潛在結果，對於傳統因果推論和監督式學習方法而言是一項艱鉅的任務。在這種情況下，為了預測某人在反事實處理下的結果，除了協變數外，還必須利用其高維度觀察結果中包含的個別資訊。先前使用變異推論在反事實生成式建模中的研究，一直專注於條件變異自動編碼器公式中的神經適應和模型變異，我們認為這從根本上不適合因果推論中的反事實概念。在這項研究中，我們提出了一個新穎的變異貝氏因果推論架構及其理論基礎，以適當處理反事實生成式建模任務，透過這個架構，我們能夠在訓練過程中進行反事實監督，而無需任何反事實樣本，並鼓勵潛在解開，有助於正確識別反事實生成中的因果效應。在實驗中，我們展示了我們架構的優點，與最先進的模型相比，在多個基準上進行反事實生成式建模。

##### **Transformer based super-resolution downscaling for regional reanalysis: Full domain vs tiling approaches**
2410.12728v1 by Antonio Pérez, Mario Santa Cruz, Daniel San Martín, José Manuel Gutiérrez

Super-resolution (SR) is a promising cost-effective downscaling methodology
for producing high-resolution climate information from coarser counterparts. A
particular application is downscaling regional reanalysis outputs (predictand)
from the driving global counterparts (predictor). This study conducts an
intercomparison of various SR downscaling methods focusing on temperature and
using the CERRA reanalysis (5.5 km resolution, produced with a regional
atmospheric model driven by ERA5) as example. The method proposed in this work
is the Swin transformer and two alternative methods are used as benchmark
(fully convolutional U-Net and convolutional and dense DeepESD) as well as the
simple bicubic interpolation. We compare two approaches, the standard one using
the full domain as input and a more scalable tiling approach, dividing the full
domain into tiles that are used as input. The methods are trained to downscale
CERRA surface temperature, based on temperature information from the driving
ERA5; in addition, the tiling approach includes static orographic information.
We show that the tiling approach, which requires spatial transferability, comes
at the cost of a lower performance (although it outperforms some full-domain
benchmarks), but provides an efficient scalable solution that allows SR
reduction on a pan-European scale and is valuable for real-time applications.

摘要：超解析度 (SR) 是一種有前途且具成本效益的降採樣方法，可根據較粗略的對應資料產生高解析度的氣候資訊。一種特定應用是將區域再分析輸出 (預測值) 從驅動的全球對應資料 (預測器) 降採樣。本研究針對各種 SR 降採樣方法進行比對，重點在於溫度，並以 CERRA 再分析 (5.5 公里解析度，由區域大氣模式驅動，ERA5 產生) 為例。本研究提出的方法是 Swin Transformer，並使用兩種替代方法作為基準 (全卷積 U-Net 及卷積和密集 DeepESD)，以及簡單的雙三次插值。我們比較兩種方法，一種是使用完整網域作為輸入的標準方法，另一種是更具可擴充性的切片方法，將完整網域分割成用作輸入的切片。這些方法經過訓練，可根據驅動 ERA5 的溫度資訊，將 CERRA 地表溫度降採樣；此外，切片方法包括靜態地形資訊。我們顯示，需要空間可傳輸性的切片方法，會以較低效能為代價 (雖然它優於某些完整網域基準)，但提供了有效的可擴充性解決方案，允許在泛歐洲規模上進行 SR 縮減，並對即時應用有價值。

##### **WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation**
2410.12722v1 by João Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, Jack Gallifant

Multimodal/vision language models (VLMs) are increasingly being deployed in
healthcare settings worldwide, necessitating robust benchmarks to ensure their
safety, efficacy, and fairness. Multiple-choice question and answer (QA)
datasets derived from national medical examinations have long served as
valuable evaluation tools, but existing datasets are largely text-only and
available in a limited subset of languages and countries. To address these
challenges, we present WorldMedQA-V, an updated multilingual, multimodal
benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V
includes 568 labeled multiple-choice QAs paired with 568 medical images from
four countries (Brazil, Israel, Japan, and Spain), covering original languages
and validated English translations by native clinicians, respectively. Baseline
performance for common open- and closed-source models are provided in the local
language and English translations, and with and without images provided to the
model. The WorldMedQA-V benchmark aims to better match AI systems to the
diverse healthcare environments in which they are deployed, fostering more
equitable, effective, and representative applications.

摘要：多模态/视觉语言模型 (VLM) 正越来越多地部署在全球医疗环境中，因此需要稳健的基准来确保其安全性、有效性和公平性。从国家医学考试中衍生的多项选择问答 (QA) 数据集长期以来一直作为有价值的评估工具，但现有数据集在很大程度上仅限于文本，并且仅在有限的语言和国家/地区中可用。为了应对这些挑战，我们提出了 WorldMedQA-V，这是一个更新的多语言、多模态基准数据集，旨在评估医疗保健中的 VLM。WorldMedQA-V 包括 568 个标记的多项选择问答，以及来自巴西、以色列、日本和西班牙四个国家的 568 张医学图像，涵盖原始语言和经过当地临床医生验证的英语翻译。常见开放源代码和闭源代码模型的基线性能以当地语言和英语翻译提供，并根据是否向模型提供图像而提供。WorldMedQA-V 基准旨在更好地将人工智能系统与部署它们的各种医疗保健环境相匹配，从而促进更公平、更有效和更具代表性的应用程序。

##### **HEnRY: A Multi-Agent System Framework for Multi-Domain Contexts**
2410.12720v1 by Emmanuele Lacavalla, Shuyi Yang, Riccardo Crupi, Joseph E. Gonzalez

This project, named HEnRY, aims to introduce a Multi-Agent System (MAS) into
Intesa Sanpaolo. The name HEnRY summarizes the project's core principles: the
Hierarchical organization of agents in a layered structure for efficient
resource management; Efficient optimization of resources and operations to
enhance overall performance; Reactive ability of agents to quickly respond to
environmental stimuli; and Yielding adaptability and flexibility of agents to
handle unexpected situations. The discussion covers two distinct research
paths: the first focuses on the system architecture, and the second on the
collaboration between agents. This work is not limited to the specific
structure of the Intesa Sanpaolo context; instead, it leverages existing
research in MAS to introduce a new solution. Since Intesa Sanpaolo is organized
according to a model that aligns with international corporate governance best
practices, this approach could also be relevant to similar scenarios.

摘要：此專案名為 HEnRY，旨在為 Intesa Sanpaolo 引入多代理系統 (MAS)。HEnRY 這個名稱總結了專案的核心原則：以分層結構分層組織代理，以有效管理資源；有效最佳化資源和作業，以提升整體效能；代理的反應能力，能快速回應環境刺激；以及代理的適應力和靈活性，以處理意外情況。討論涵蓋兩條不同的研究途徑：第一個專注於系統架構，第二個專注於代理之間的協作。這項工作不限於 Intesa Sanpaolo 背景的特定結構；相反地，它利用 MAS 中現有的研究來引進新的解決方案。由於 Intesa Sanpaolo 的組織方式符合國際公司治理最佳實務，因此此方法也可能與類似情境相關。

##### **FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**
2410.12707v1 by Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, Xiaowen Chu

To alleviate hardware scarcity in training large deep neural networks (DNNs),
particularly large language models (LLMs), we present FusionLLM, a
decentralized training system designed and implemented for training DNNs using
geo-distributed GPUs across different computing clusters or individual devices.
Decentralized training faces significant challenges regarding system design and
efficiency, including: 1) the need for remote automatic differentiation (RAD),
2) support for flexible model definitions and heterogeneous software, 3)
heterogeneous hardware leading to low resource utilization or the straggler
problem, and 4) slow network communication. To address these challenges, in the
system design, we represent the model as a directed acyclic graph of operators
(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the
edge represents the data dependency between operators. Based on this design, 1)
users are allowed to customize any DNN without caring low-level operator
implementation; 2) we enable the task scheduling with the more fine-grained
sub-tasks, offering more optimization space; 3) a DAG runtime executor can
implement RAD withour requiring the consistent low-level ML framework versions.
  To enhance system efficiency, we implement a workload estimator and design an
OP-Fence scheduler to cluster devices with similar bandwidths together and
partition the DAG to increase throughput. Additionally, we propose an AdaTopK
compressor to adaptively compress intermediate activations and gradients at the
slowest communication links. To evaluate the convergence and efficiency of our
system and algorithms, we train ResNet-101 and GPT-2 on three real-world
testbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental
results demonstrate that our system and method can achieve 1.45 - 9.39x speedup
compared to baseline methods while ensuring convergence.

摘要：<paragraph>為了減輕訓練大型深度神經網路 (DNN) 的硬體短缺問題，尤其是大型語言模型 (LLM)，我們提出了 FusionLLM，一個分散式訓練系統，其設計和實作是用於訓練跨不同運算叢集或個別裝置的地理分散式 GPU 的 DNN。分散式訓練在系統設計和效率方面面臨重大挑戰，包括：1) 需要遠端自動微分 (RAD)，2) 支援彈性的模型定義和異質軟體，3) 異質硬體導致資源利用率低或落後問題，以及 4) 網路通訊速度慢。為了應對這些挑戰，在系統設計中，我們將模型表示為一個有向非循環圖 (OP-DAG) 的運算子。DAG 中的每個節點代表 DNN 中的運算子，而邊緣代表運算子之間的資料依賴性。基於此設計，1) 使用者可以自訂任何 DNN，而不用考慮低階運算子實作；2) 我們啟用任務排程，並使用更細緻的子任務，提供更多最佳化空間；3) DAG 執行時間執行器可以實作 RAD，而不需要一致的低階 ML 架構版本。為了提升系統效率，我們實作一個工作負載估計器，並設計一個 OP-Fence 排程器，將頻寬類似的裝置分組在一起，並分割 DAG 以增加處理量。此外，我們提出一個 AdaTopK 壓縮器，以自適應方式壓縮最慢通訊連結上的中間啟動和梯度。為了評估我們系統和演算法的收斂性和效率，我們在三個真實世界的測試平台上訓練 ResNet-101 和 GPT-2，使用 48 個 GPU 連接到 8 Mbps~10 Gbps 網路。實驗結果表明，我們的系統和方法可以比基準方法快 1.45 - 9.39 倍，同時確保收斂。</paragraph>

##### **WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines**
2410.12705v1 by Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo

Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.

摘要：視覺語言模型 (VLM) 經常在文化特定知識上遇到困難，特別是在英語以外的語言和代表性不足的文化背景中。為了評估他們對此類知識的理解，我們引入了 WorldCuisines，這是一個用於多語言和多元文化、視覺基礎語言理解的大規模基準測試。此基準測試包括一個視覺問答 (VQA) 資料集，其中包含 30 種語言和方言的文字影像對，涵蓋 9 個語言家族，並擁有超過 100 萬個資料點，使其成為迄今為止最大的多元文化 VQA 基準測試。它包括識別菜餚名稱及其來源的任務。我們提供了兩種大小（12k 和 60k 個例項）的評估資料集以及一個訓練資料集（100 萬個例項）。我們的研究結果表明，雖然 VLM 在正確的位置背景下表現得更好，但它們在對抗背景下以及預測特定區域美食和語言時遇到困難。為了支持未來的研究，我們發布了一個包含註釋食物條目和影像的知識庫以及 VQA 資料。

##### **Sarcasm Detection in a Less-Resourced Language**
2410.12704v1 by Lazar Đoković, Marko Robnik-Šikonja

The sarcasm detection task in natural language processing tries to classify
whether an utterance is sarcastic or not. It is related to sentiment analysis
since it often inverts surface sentiment. Because sarcastic sentences are
highly dependent on context, and they are often accompanied by various
non-verbal cues, the task is challenging. Most of related work focuses on
high-resourced languages like English. To build a sarcasm detection dataset for
a less-resourced language, such as Slovenian, we leverage two modern
techniques: a machine translation specific medium-size transformer model, and a
very large generative language model. We explore the viability of translated
datasets and how the size of a pretrained transformer affects its ability to
detect sarcasm. We train ensembles of detection models and evaluate models'
performance. The results show that larger models generally outperform smaller
ones and that ensembling can slightly improve sarcasm detection performance.
Our best ensemble approach achieves an $\text{F}_1$-score of 0.765 which is
close to annotators' agreement in the source language.

摘要：自然語言處理中的諷刺偵測任務嘗試分類一段話是否具有諷刺意味。這與情緒分析有關，因為它通常會反轉表面情緒。由於諷刺句子高度依賴於上下文，而且通常伴隨著各種非語言線索，因此這項任務具有挑戰性。大多數相關工作都專注於英語等資源豐富的語言。為了建立一個較少資源的語言（例如斯洛維尼亞語）的諷刺偵測資料集，我們利用了兩種現代技術：一個機器翻譯專用的中型Transformer模型，以及一個非常大的生成式語言模型。我們探討了翻譯資料集的可行性，以及預訓練Transformer的規模如何影響其偵測諷刺的能力。我們訓練了偵測模型的集合，並評估模型的效能。結果顯示，較大的模型通常優於較小的模型，而且集合可以稍微提升諷刺偵測效能。我們最好的集合方法達到了 0.765 的 $\text{F}_1$- 分數，這接近於來源語言中註解者的一致性。

##### **Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization**
2410.12700v1 by Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia

Recent advancements in diffusion models trained on large-scale data have
enabled the generation of indistinguishable human-level images, yet they often
produce harmful content misaligned with human values, e.g., social bias, and
offensive content. Despite extensive research on Large Language Models (LLMs),
the challenge of Text-to-Image (T2I) model alignment remains largely
unexplored. Addressing this problem, we propose LiVO (Lightweight Value
Optimization), a novel lightweight method for aligning T2I models with human
values. LiVO only optimizes a plug-and-play value encoder to integrate a
specified value principle with the input prompt, allowing the control of
generated images over both semantics and values. Specifically, we design a
diffusion model-tailored preference optimization loss, which theoretically
approximates the Bradley-Terry model used in LLM alignment but provides a more
flexible trade-off between image quality and value conformity. To optimize the
value encoder, we also develop a framework to automatically construct a
text-image preference dataset of 86k (prompt, aligned image, violating image,
value principle) samples. Without updating most model parameters and through
adaptive value selection from the input prompt, LiVO significantly reduces
harmful outputs and achieves faster convergence, surpassing several strong
baselines and taking an initial step towards ethically aligned T2I models.

摘要：近期在大型资料训练的扩散模型有了重大进展，使得生成难以分辨的人类等级影像成为可能，但它们经常产生与人类价值观相悖的有害内容，例如社会偏见和攻击性内容。尽管对大型语言模型 (LLM) 进行了广泛研究，文本到图像 (T2I) 模型对齐的挑战在很大程度上仍未得到探索。为了解决这个问题，我们提出了 LiVO（轻量级价值优化），这是一种新颖的轻量级方法，用于将 T2I 模型与人类价值观对齐。LiVO 仅优化即插即用的值编码器，以将指定的值原则与输入提示整合在一起，从而允许对生成图像进行语义和价值控制。具体来说，我们设计了一个针对扩散模型量身定制的偏好优化损失，它在理论上近似于用于 LLM 对齐的 Bradley-Terry 模型，但在图像质量和价值一致性之间提供了一个更灵活的权衡。为了优化值编码器，我们还开发了一个框架，以自动构建一个包含 86k（提示、对齐图像、违规图像、价值原则）样本的文本图像偏好数据集。在不更新大多数模型参数的情况下，并且通过从输入提示中自适应地选择值，LiVO 大幅减少了有害输出并实现了更快的收敛，超越了几个强大的基准，并朝着符合道德标准的 T2I 模型迈出了第一步。

##### **VividMed: Vision Language Model with Versatile Visual Grounding for Medicine**
2410.12694v1 by Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen

Recent advancements in Vision Language Models (VLMs) have demonstrated
remarkable promise in generating visually grounded responses. However, their
application in the medical domain is hindered by unique challenges. For
instance, most VLMs rely on a single method of visual grounding, whereas
complex medical tasks demand more versatile approaches. Additionally, while
most VLMs process only 2D images, a large portion of medical images are 3D. The
lack of medical data further compounds these obstacles. To address these
challenges, we present VividMed, a vision language model with versatile visual
grounding for medicine. Our model supports generating both semantic
segmentation masks and instance-level bounding boxes, and accommodates various
imaging modalities, including both 2D and 3D data. We design a three-stage
training procedure and an automatic data synthesis pipeline based on open
datasets and models. Besides visual grounding tasks, VividMed also excels in
other common downstream tasks, including Visual Question Answering (VQA) and
report generation. Ablation studies empirically show that the integration of
visual grounding ability leads to improved performance on these tasks. Our code
is publicly available at https://github.com/function2-llx/MMMM.

摘要：最近在视觉语言模型 (VLM) 方面的进步已展示出在生成视觉基础响应方面的非凡前景。然而，它们在医学领域的应用受到独特的挑战的阻碍。例如，大多数 VLM 依赖于一种视觉基础方法，而复杂的医学任务需要更多样化的方法。此外，虽然大多数 VLM 只处理二维图像，但大部分医学图像都是三维的。缺乏医学数据进一步加剧了这些障碍。为了应对这些挑战，我们提出了 VividMed，这是一种具有多功能医学视觉基础的视觉语言模型。我们的模型支持生成语义分割掩模和实例级边界框，并适用于各种成像方式，包括二维和三维数据。我们设计了一个三阶段训练程序和一个基于开放数据集和模型的自动数据合成管道。除了视觉基础任务之外，VividMed 在其他常见的下游任务中也表现出色，包括视觉问答 (VQA) 和报告生成。消融研究凭经验证明，视觉基础能力的整合导致了这些任务的性能提升。我们的代码在 https://github.com/function2-llx/MMMM 上公开提供。

##### **Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce**
2410.12691v1 by Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad

Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.

摘要：語言是一種象徵資本，以許多方式影響人們的生活（Bourdieu，1977、1991）。它是一種強大的工具，說明了身分、文化、傳統和整體社會。因此，特定語言中的資料應視為不只是代幣集合。良好的資料收集和標記實務是建構以人為中心且具有社會意識的技術的關鍵。雖然 NLP 社群對中等至低資源語言有越來越高的興趣，但這方面的研究必須克服獨特的挑戰，例如資料稀少以及取得適當註解者的管道。在本論文中，我們收集了直接參與並受中低資源語言 NLP 人工製品影響者的回饋。我們對回應進行量化和質性分析，並重點說明與 (1) 資料品質（例如語言和文化資料的適用性）相關的主要問題；以及 (2) 常見註解實務（例如線上社群服務的誤用）的倫理。根據這些發現，我們提出幾項建議，用於建立反映其使用者文化環境的高品質語言人工製品，同時尊重資料工作者的尊嚴和勞動。

##### **Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**
2410.12686v1 by Mohamad Abdi, Gerardo Hemosillo Valadez, Halid Ziya Yerebakan

Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.

摘要：解剖標誌在醫學影像中對於導航和異常偵測至關重要。像 Llama-2 等現代大型語言模型 (LLM) 有望自動化將這些標誌對應到影像資料中相應位置的自由文字放射線學報告的對應。最近的研究提出，LLM 可能會發展出生成式過程的連貫表徵。受到這些見解的啟發，我們調查了 LLM 是否能準確表示解剖標誌的空間位置。透過 Llama-2 模型的實驗，我們發現它們可以用線性方式表示空間中的解剖標誌，並對不同的提示具有相當的穩健性。這些結果強調了 LLM 提高醫學影像工作流程效率和準確性的潛力。

##### **Context Matters: Leveraging Contextual Features for Time Series Forecasting**
2410.12672v1 by Sameep Chattopadhyay, Pulkit Paliwal, Sai Shankar Narasimhan, Shubhankar Agarwal, Sandeep P. Chinchali

Time series forecasts are often influenced by exogenous contextual features
in addition to their corresponding history. For example, in financial settings,
it is hard to accurately predict a stock price without considering public
sentiments and policy decisions in the form of news articles, tweets, etc.
Though this is common knowledge, the current state-of-the-art (SOTA)
forecasting models fail to incorporate such contextual information, owing to
its heterogeneity and multimodal nature. To address this, we introduce
ContextFormer, a novel plug-and-play method to surgically integrate multimodal
contextual information into existing pre-trained forecasting models.
ContextFormer effectively distills forecast-specific information from rich
multimodal contexts, including categorical, continuous, time-varying, and even
textual information, to significantly enhance the performance of existing base
forecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on
a range of real-world datasets spanning energy, traffic, environmental, and
financial domains.

摘要：時間序列預測通常會受到外生脈絡特徵的影響，除了對應的歷史記錄之外。例如，在金融環境中，在不考慮新聞文章、推文等形式的公眾情緒和政策決策的情況下，很難準確預測股票價格。儘管這是常識，但當前最先進 (SOTA) 預測模型無法整合此類脈絡資訊，因為它的異質性和多模態性質。為了解決這個問題，我們引入了 ContextFormer，這是一種新穎的即插即用方法，用於將多模態脈絡資訊外科手術式整合到現有的預訓練預測模型中。ContextFormer 有效地從豐富的多模態脈絡中提取預測特定資訊，包括分類、連續、時變，甚至文字資訊，以顯著增強現有基礎預測器的效能。ContextFormer 在涵蓋能源、交通、環境和金融領域的一系列真實世界資料集上，比 SOTA 預測模型高出 30%。

##### **Hamiltonian bridge: A physics-driven generative framework for targeted pattern control**
2410.12665v1 by Vishaal Krishnan, Sumit Sinha, L. Mahadevan

Patterns arise spontaneously in a range of systems spanning the sciences, and
their study typically focuses on mechanisms to understand their evolution in
space-time. Increasingly, there has been a transition towards controlling these
patterns in various functional settings, with implications for engineering.
Here, we combine our knowledge of a general class of dynamical laws for pattern
formation in non-equilibrium systems, and the power of stochastic optimal
control approaches to present a framework that allows us to control patterns at
multiple scales, which we dub the "Hamiltonian bridge". We use a mapping
between stochastic many-body Lagrangian physics and deterministic Eulerian
pattern forming PDEs to leverage our recent approach utilizing the
Feynman-Kac-based adjoint path integral formulation for the control of
interacting particles and generalize this to the active control of patterning
fields. We demonstrate the applicability of our computational framework via
numerical experiments on the control of phase separation with and without a
conserved order parameter, self-assembly of fluid droplets, coupled
reaction-diffusion equations and finally a phenomenological model for
spatio-temporal tissue differentiation. We interpret our numerical experiments
in terms of a theoretical understanding of how the underlying physics shapes
the geometry of the pattern manifold, altering the transport paths of patterns
and the nature of pattern interpolation. We finally conclude by showing how
optimal control can be utilized to generate complex patterns via an iterative
control protocol over pattern forming pdes which can be casted as gradient
flows. All together, our study shows how we can systematically build in
physical priors into a generative framework for pattern control in
non-equilibrium systems across multiple length and time scales.

摘要：<paragraph>在跨越科學領域的一系列系統中，模式會自發產生，而它們的研究通常集中在機制上，以了解它們在時空中的演化。越來越多的趨勢是朝向在各種功能設定中控制這些模式，並對工程產生影響。在這裡，我們結合我們對非平衡系統中模式形成的一般類別動力定律的知識，以及隨機最優控制方法的力量，提出了一個框架，允許我們控制多個尺度的模式，我們稱之為「哈密頓橋」。我們使用隨機多體拉格朗日物理和確定性歐拉模式形成偏微分方程之間的映射，以利用我們最近的方法，利用基於費曼-卡茨的伴隨路徑積分公式來控制相互作用的粒子，並將其推廣到模式場的主動控制。我們通過對相分離控制的數值實驗（有和沒有守恆序參數）、流體液滴的自組裝、耦合反應擴散方程，以及最後一個時空組織分化的現象學模型，來證明我們的計算框架的適用性。我們根據對基礎物理如何塑造模式流形的幾何形狀、改變模式的傳輸路徑和模式插值的性質的理論理解，來解釋我們的數值實驗。最後，我們通過展示如何利用最優控制通過模式形成偏微分方程的迭代控制協議來生成複雜模式（可以轉換為梯度流）來總結。總之，我們的研究表明，我們如何系統地將物理先驗構建到非平衡系統中跨多個長度和時間尺度的模式控制的生成框架中。</paragraph>

##### **Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models**
2410.12662v1 by Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng

Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).

摘要：大型视觉语言模型（LVLMs）中的视觉语言对齐成功使 LLM 能够理解视觉输入。然而，我们发现现有的视觉语言对齐方法无法将 LLM 中文本的现有安全机制转移到视觉中，这导致了有毒图像中的漏洞。为了探索这个问题的原因，我们对 LVLMs 的安全机制在何处以及如何操作给出了深刻的解释，并在文本和视觉之间进行了比较分析。我们发现，特定 transformer 层的隐藏状态在安全机制的成功激活中起着至关重要的作用，而当前方法在隐藏状态级别的视觉语言对齐还不够。这导致隐藏状态中输入图像与文本的语义发生了变化，因此误导了安全机制。为了解决这个问题，我们为 LVLMs 提出了一种新颖的文本指导视觉语言对齐方法 (TGA)。TGA 检索与输入视觉相关的文本，并使用它们来指导视觉在 LLM 中投影到隐藏状态空间。实验表明，TGA 不仅成功地将基本 LLM 中文本的安全机制转移到 LVLMs 的视觉语言对齐中的视觉中，而无需对视觉方式进行任何安全微调，而且还保持了各种视觉任务（安全且良好）的总体性能。

##### **Evaluating Morphological Compositional Generalization in Large Language Models**
2410.12656v1 by Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman

Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.

摘要：大型語言模型（LLM）在各種自然語言生成和理解任務中展現出顯著進展。然而，它們的語言概化能力仍存在疑問，引發了這些模型是否像人類一樣學習語言的疑慮。人類在語言使用中展現出組合概化和語言創造力，而 LLM 在何種程度上複製這些能力，特別是在形態學方面，仍未得到充分探討。在這項工作中，我們透過組合性的觀點系統性地探討 LLM 的形態概化能力。我們將形態定義為組合原語，並設計了一系列新的生成和區分任務來評估形態生產力和系統性。我們專注於像土耳其語和芬蘭語等黏著語，評估了多個最先進的指令微調多語言模型，包括 GPT-4 和 Gemini。我們的分析顯示，LLM 在形態組合概化方面遇到困難，特別是在應用於新的字根時，隨著形態複雜性的增加，效能急劇下降。雖然模型可以比隨機情況更好地識別個別形態組合，但它們的效能缺乏系統性，導致與人類相比出現顯著的準確性差距。

##### **Constrained Posterior Sampling: Time Series Generation with Hard Constraints**
2410.12652v1 by Sai Shankar Narasimhan, Shubhankar Agarwal, Litu Rout, Sanjay Shakkottai, Sandeep P. Chinchali

Generating realistic time series samples is crucial for stress-testing models
and protecting user privacy by using synthetic data. In engineering and
safety-critical applications, these samples must meet certain hard constraints
that are domain-specific or naturally imposed by physics or nature. Consider,
for example, generating electricity demand patterns with constraints on peak
demand times. This can be used to stress-test the functioning of power grids
during adverse weather conditions. Existing approaches for generating
constrained time series are either not scalable or degrade sample quality. To
address these challenges, we introduce Constrained Posterior Sampling (CPS), a
diffusion-based sampling algorithm that aims to project the posterior mean
estimate into the constraint set after each denoising update. Notably, CPS
scales to a large number of constraints (~100) without requiring additional
training. We provide theoretical justifications highlighting the impact of our
projection step on sampling. Empirically, CPS outperforms state-of-the-art
methods in sample quality and similarity to real time series by around 10% and
42%, respectively, on real-world stocks, traffic, and air quality datasets.

摘要：生成逼真的時間序列範例對於壓力測試模型和使用合成資料保護使用者隱私至關重要。在工程和安全關鍵應用中，這些範例必須符合某些特定於領域或自然而然由物理或自然界強加的硬約束。例如，考慮在尖峰需求時間的約束下產生用電需求模式。這可用於在惡劣天氣條件下對電網的功能進行壓力測試。現有的用於產生受約束時間序列的方法不是無法擴展，就是會降低範例品質。為了應對這些挑戰，我們引入了受約束後驗抽樣 (CPS)，這是一種基於擴散的抽樣演算法，其目標是在每次去噪更新後將後驗平均估計值投影到約束集中。值得注意的是，CPS 可以擴展到大量的約束（約 100 個），而不需要額外的訓練。我們提供了理論依據，強調了我們的投影步驟對抽樣的影響。根據經驗，CPS 在範例品質和與實際時間序列的相似性方面優於最先進的方法，分別在實際股票、交通和空氣品質資料集上提高了約 10% 和 42%。

##### **Explainable Moral Values: a neuro-symbolic approach to value classification**
2410.12631v1 by Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti

This work explores the integration of ontology-based reasoning and Machine
Learning techniques for explainable value classification. By relying on an
ontological formalization of moral values as in the Moral Foundations Theory,
relying on the DnS Ontology Design Pattern, the \textit{sandra} neuro-symbolic
reasoner is used to infer values (fomalized as descriptions) that are
\emph{satisfied by} a certain sentence. Sentences, alongside their structured
representation, are automatically generated using an open-source Large Language
Model. The inferred descriptions are used to automatically detect the value
associated with a sentence. We show that only relying on the reasoner's
inference results in explainable classification comparable to other more
complex approaches. We show that combining the reasoner's inferences with
distributional semantics methods largely outperforms all the baselines,
including complex models based on neural network architectures. Finally, we
build a visualization tool to explore the potential of theory-based values
classification, which is publicly available at http://xmv.geomeaning.com/.

摘要：這項工作探討了基於本体論推理和機器學習技術整合用於可解釋價值分類。透過依賴於道德基礎理論中道德價值的本体形式化，依賴於 DnS 本体設計模式，\textit{sandra} 神經符號推理器用於推論由特定句子「滿足」的值 (形式化為描述)。句子與其結構化表示一起使用開源大型語言模型自動生成。推論出的描述用於自動偵測與句子相關的值。我們展示僅依賴於推理器的推論結果，就能產生可解釋的分類，與其他較複雜的方法相當。我們展示將推理器的推論與分配語義方法結合，大幅優於所有基線，包括基於神經網路架構的複雜模型。最後，我們建立一個視覺化工具來探討基於理論的價值分類的潛力，該工具可公開取得，網址為 http://xmv.geomeaning.com/。

##### **From Measurement Instruments to Training Data: Leveraging Theory-Driven Synthetic Training Data for Measuring Social Constructs**
2410.12622v1 by Lukas Birkenmaier, Matthias Roth, Indira Sen

Computational text classification is a challenging task, especially for
multi-dimensional social constructs. Recently, there has been increasing
discussion that synthetic training data could enhance classification by
offering examples of how these constructs are represented in texts. In this
paper, we systematically examine the potential of theory-driven synthetic
training data for improving the measurement of social constructs. In
particular, we explore how researchers can transfer established knowledge from
measurement instruments in the social sciences, such as survey scales or
annotation codebooks, into theory-driven generation of synthetic data. Using
two studies on measuring sexism and political topics, we assess the added value
of synthetic training data for fine-tuning text classification models. Although
the results of the sexism study were less promising, our findings demonstrate
that synthetic data can be highly effective in reducing the need for labeled
data in political topic classification. With only a minimal drop in
performance, synthetic data allows for substituting large amounts of labeled
data. Furthermore, theory-driven synthetic data performed markedly better than
data generated without conceptual information in mind.

摘要：計算文本分類是一項具有挑戰性的任務，特別是對於多維度的社會建構而言。最近，關於合成訓練資料可以通過提供這些建構如何在文本中呈現的範例來增強分類的討論越來越多。在本文中，我們系統地探討了理論驅動的合成訓練資料在改進社會建構測量方面的潛力。具體而言，我們探討了研究人員如何將社會科學中測量工具（例如調查量表或註釋手冊）中的既有知識轉移到合成資料的理論驅動生成中。使用關於衡量性別歧視和政治話題的兩項研究，我們評估了合成訓練資料在微調文本分類模型方面的附加價值。儘管性別歧視研究的結果不太令人滿意，但我們的研究結果表明，合成資料在減少政治話題分類中對標籤資料的需求方面可以非常有效。合成資料僅會導致性能略有下降，從而允許替換大量的標籤資料。此外，理論驅動的合成資料的表現明顯優於在構思時沒有考慮概念性資訊的資料。

##### **Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety, Toxicity, and Legal Reasoning**
2410.12621v1 by Ruimeng Ye, Yang Xiao, Bo Hui

As large language models (LLMs) continue to advance, ensuring their alignment
with human values becomes increasingly critical. Traditional alignment methods
heavily rely on human feedback to fine-tune models. With the emergence of
superhuman models whose outputs may surpass human understanding, evaluating and
aligning these models using human judgments poses significant challenges. To
address the challenges, recent works use weak supervisors to elicit knowledge
from much stronger models. However, there are important disanalogies between
the empirical setup in the existing works and the genuine goal of alignment. We
remark that existing works investigate the phenomenon of weak-to-strong
generation in analogous setup (i.e., binary classification), rather than
practical alignment-relevant tasks (e.g., safety). In this paper, we bridge
this gap by extending weak-to-strong generation to the context of practical
alignment. We empirically demonstrate the widespread phenomenon of
weak-to-strong generation in three complicated alignment tasks: safety,
toxicity, and legal reasoning}. Furthermore, we explore efficient strategies
for improving alignment performance to enhance the quality of model outcomes.
Lastly, we summarize and analyze the challenges and potential solutions in
regard to specific alignment tasks, which we hope to catalyze the research
progress on the topic of weak-to-strong generalization. Our code is released at
https://github.com/yeruimeng/WTS.git.

摘要：隨著大型語言模型 (LLM) 的持續進展，確保它們與人類價值觀保持一致變得越來越重要。傳統對齊方法嚴重依賴人類回饋來微調模型。隨著超越人類理解能力的超人類模型的出現，使用人類判斷來評估和對齊這些模型構成了重大挑戰。為了應對這些挑戰，最近的工作使用弱監督者從更強大的模型中引出知識。然而，現有工作中的經驗設置與對齊的真正目標之間存在重要的差異。我們注意到，現有工作研究了類比設置（即二元分類）中的弱到強生成現象，而不是實際與對齊相關的任務（例如安全性）。在本文中，我們通過將弱到強生成擴展到實際對齊的背景來彌合這一差距。我們通過三個複雜的對齊任務（安全性、毒性和法律推理）經驗性地展示了弱到強生成的廣泛現象。此外，我們探討了提高對齊性能以增強模型結果品質的有效策略。最後，我們總結和分析了與具體對齊任務相關的挑戰和潛在解決方案，我們希望催化弱到強泛化的主題研究進展。我們的代碼發布在 https://github.com/yeruimeng/WTS.git。

##### **Exploring Model Kinship for Merging Large Language Models**
2410.12613v1 by Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen

Model merging has become one of the key technologies for enhancing the
capabilities and efficiency of Large Language Models (LLMs). However, our
understanding of the expected performance gains and principles when merging any
two models remains limited. In this work, we introduce model kinship, the
degree of similarity or relatedness between LLMs, analogous to biological
evolution. With comprehensive empirical analysis, we find that there is a
certain relationship between model kinship and the performance gains after
model merging, which can help guide our selection of candidate models. Inspired
by this, we propose a new model merging strategy: Top-k Greedy Merging with
Model Kinship, which can yield better performance on benchmark datasets.
Specifically, we discover that using model kinship as a criterion can assist us
in continuously performing model merging, alleviating the degradation (local
optima) in model evolution, whereas model kinship can serve as a guide to
escape these traps. Code is available at
https://github.com/zjunlp/ModelKinship.

摘要：模型合并已成为提升大型语言模型 (LLM) 能力和效率的关键技术之一。然而，对于合并任何两个模型时预期的性能提升和原理，我们的理解仍然有限。在这项工作中，我们引入了模型亲缘关系，即 LLM 之间的相似性或相关性程度，类似于生物进化。通过全面的实证分析，我们发现模型亲缘关系与模型合并后的性能提升之间存在一定的关系，这可以帮助我们指导候选模型的选择。受此启发，我们提出了一种新的模型合并策略：基于模型亲缘关系的 Top-k 贪婪合并，它可以在基准数据集上产生更好的性能。具体来说，我们发现使用模型亲缘关系作为标准可以帮助我们持续执行模型合并，缓解模型演化中的退化（局部最优），而模型亲缘关系可以作为逃离这些陷阱的指南。代码可在 https://github.com/zjunlp/ModelKinship 获得。

##### **Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning on Knowledge Graphs**
2410.12609v1 by Kai Wang, Siqiang Luo

Inspired by the success of artificial general intelligence, there is a trend
towards developing Graph Foundation Models that excel in generalization across
various graph tasks and domains. However, current models often require
extensive training or fine-tuning to capture structural and semantic insights
on new graphs, which limits their versatility. In this work, we explore graph
foundation models from the perspective of zero-shot reasoning on Knowledge
Graphs (KGs). Our focus is on utilizing KGs as a unified topological structure
to tackle diverse tasks, while addressing semantic isolation challenges in KG
reasoning to effectively integrate diverse semantic and structural features.
This brings us new methodological insights into KG reasoning, as well as high
generalizability towards foundation models in practice. Methodologically, we
introduce SCORE, a unified graph reasoning framework that effectively
generalizes diverse graph tasks using zero-shot learning. At the core of SCORE
is semantic conditional message passing, a technique designed to capture both
structural and semantic invariances in graphs, with theoretical backing for its
expressive power. Practically, we evaluate the zero-shot reasoning capability
of SCORE using 38 diverse graph datasets, covering node-level, link-level, and
graph-level tasks across multiple domains. Our experiments reveal a substantial
performance improvement over prior foundation models and supervised baselines,
highlighting the efficacy and adaptability of our approach.

摘要：受到人工通用智能的成功啟發，現正流行開發圖形基礎模型，這些模型擅長於在各種圖形任務和領域中進行概括。然而，當前的模型通常需要大量的訓練或微調，才能擷取新圖形中的結構和語義見解，這限制了它們的多功能性。在這項工作中，我們從知識圖譜 (KG) 的零次推理角度探討圖形基礎模型。我們的重點是利用 KG 作為統一的拓撲結構來處理各種任務，同時解決 KG 推理中的語義孤立挑戰，以有效整合各種語義和結構特徵。這為我們帶來了 KG 推理的新方法論見解，以及在實務中對基礎模型的高泛化能力。在方法論上，我們引入了 SCORE，一個統一的圖形推理框架，它使用零次學習有效地概括了各種圖形任務。SCORE 的核心是語義條件訊息傳遞，這是一種旨在擷取圖形中結構和語義不變性的技術，並有理論支持其表現能力。在實務上，我們使用 38 個不同的圖形資料集評估 SCORE 的零次推理能力，涵蓋跨多個領域的節點層級、連結層級和圖形層級任務。我們的實驗顯示，與先前的基礎模型和監督基準相比，我們的效能有顯著的提升，突顯了我們方法的有效性和適應性。

##### **Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning**
2410.12608v1 by Vernon Y. H. Toh, Deepanway Ghosal, Soujanya Poria

Large language models (LLMs) have shown increasing proficiency in solving
mathematical reasoning problems. However, many current open-source LLMs often
still make calculation and semantic understanding errors in their intermediate
reasoning steps. In this work, we propose PROVE, a simple yet effective
framework that uses program-based verification as a heuristic to filter out
potentially incorrect reasoning paths before aggregating the final answers.
Instead of relying on vanilla majority voting, our approach rejects solutions
whose corresponding program outputs are inconsistent with the generated
solution, aggregating only those validated by Python programs. We conducted
extensive experiments on 13 open-source LLMs from various model families and
sizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We
demonstrate that PROVE consistently outperforms vanilla majority voting as a
heuristic for solving mathematical reasoning tasks across all datasets and
model sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from
48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for
Llama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%
to 59.51% for Llama-2-7B-chat. Our codes are available at
https://github.com/declare-lab/prove.

摘要：大型語言模型 (LLM) 在解決數學推理問題方面已展現出越來越高的能力。然而，許多當前開放原始碼的 LLM 仍經常在其中間推理步驟中產生計算和語義理解錯誤。在此研究中，我們提出 PROVE，一個簡單但有效的架構，它使用基於程式的驗證作為一種啟發式方法，在匯總最終答案之前過濾掉潛在不正確的推理路徑。我們的做法並非依賴於香草多數決，而是拒絕其對應程式輸出與產生解不相符的解，僅匯總由 Python 程式驗證的解。我們對來自各種模型系列和規模的 13 個開放原始碼 LLM 進行了廣泛的實驗，模型參數範圍從 0.5B 到 13B，涵蓋七個數學基準。我們證明 PROVE 在所有資料集和模型規模中始終優於香草多數決，作為解決數學推理任務的啟發式方法。值得注意的是，PROVE 將 Qwen2-0.5B-Instruct 的 GSM8K 基準準確率從 48.85% 提升至 53.83%，將 Llama-3.2-1B-Instruct 的準確率從 65.66% 提升至 73.01%，將 Gemma-2-2b-it 的準確率從 73.39% 提升至 79.61%，將 Llama-2-7B-chat 的準確率從 41.32% 提升至 59.51%。我們的程式碼可在 https://github.com/declare-lab/prove 中取得。

##### **Low-Rank Adversarial PGD Attack**
2410.12607v1 by Dayana Savostianova, Emanuele Zangrando, Francesco Tudisco

Adversarial attacks on deep neural network models have seen rapid development
and are extensively used to study the stability of these networks. Among
various adversarial strategies, Projected Gradient Descent (PGD) is a widely
adopted method in computer vision due to its effectiveness and quick
implementation, making it suitable for adversarial training. In this work, we
observe that in many cases, the perturbations computed using PGD predominantly
affect only a portion of the singular value spectrum of the original image,
suggesting that these perturbations are approximately low-rank. Motivated by
this observation, we propose a variation of PGD that efficiently computes a
low-rank attack. We extensively validate our method on a range of standard
models as well as robust models that have undergone adversarial training. Our
analysis indicates that the proposed low-rank PGD can be effectively used in
adversarial training due to its straightforward and fast implementation coupled
with competitive performance. Notably, we find that low-rank PGD often performs
comparably to, and sometimes even outperforms, the traditional full-rank PGD
attack, while using significantly less memory.

摘要：對深度神經網路模型的對抗攻擊已經快速發展，並廣泛用於研究這些網路的穩定性。在各種對抗策略中，投影梯度下降 (PGD) 是一種在電腦視覺中廣泛採用的方法，因為它的有效性和快速實作，使其適合於對抗訓練。在這項工作中，我們觀察到在許多情況下，使用 PGD 計算的擾動主要只影響原始影像的奇異值頻譜的一部分，這表示這些擾動大約是低秩的。受到這個觀察的啟發，我們提出 PGD 的一種變體，可以有效地計算低秩攻擊。我們廣泛驗證了我們的方法，包括一系列標準模型以及經過對抗訓練的強健模型。我們的分析表明，所提出的低秩 PGD 可以有效地用於對抗訓練，因為它的實作簡單快速，並且效能具有競爭力。值得注意的是，我們發現低秩 PGD 通常表現得與傳統全秩 PGD 攻擊相當，甚至有時表現得更好，同時使用的記憶體卻少得多。

##### **CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization**
2410.12601v1 by Yixi Ding, Jiaying Wu, Tongyao Zhu, Yanxia Qin, Qian Liu, Min-Yen Kan

To broaden the dissemination of scientific knowledge to diverse audiences,
scientific document summarization must simultaneously control multiple
attributes such as length and empirical focus. However, existing research
typically focuses on controlling single attributes, leaving the compositional
control of multiple attributes underexplored. To address this gap, we introduce
CCSBench, a benchmark for compositional controllable summarization in the
scientific domain. Our benchmark enables fine-grained control over both
explicit attributes (e.g., length), which are objective and straightforward,
and implicit attributes (e.g., empirical focus), which are more subjective and
conceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our findings reveal significant
limitations in large language models' ability to balance trade-offs between
control attributes, especially implicit ones that require deeper understanding
and abstract reasoning.

摘要：為擴大科學知識在不同受眾間的傳播，科學文獻摘要必須同時控制多重屬性，例如長度和實證重點。然而，現有研究通常專注於控制單一屬性，而忽略了多重屬性的組成控制。為了解決這個差距，我們引入了 CCSBench，一個在科學領域中用於組成可控摘要的基準。我們的基準能對顯式屬性（例如長度）和隱式屬性（例如實證重點）進行精細的控制，前者是客觀且直接的，而後者則是更主觀且抽象的。我們對 GPT-4、LLaMA2 和其他流行的 LLM 在各種設定下進行了廣泛的實驗。我們的研究結果揭露了大型語言模型在平衡控制屬性之間的權衡時有顯著的限制，尤其是需要更深入理解和抽象推理的隱式屬性。

##### **On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs**
2410.12600v1 by Herun Wan, Minnan Luo, Zhixiong Su, Guang Dai, Xiang Zhao

Evidence-enhanced detectors present remarkable abilities in identifying
malicious social text with related evidence. However, the rise of large
language models (LLMs) brings potential risks of evidence pollution to confuse
detectors. This paper explores how to manipulate evidence, simulating potential
misuse scenarios including basic pollution, and rephrasing or generating
evidence by LLMs. To mitigate its negative impact, we propose three defense
strategies from both the data and model sides, including machine-generated text
detection, a mixture of experts, and parameter updating. Extensive experiments
on four malicious social text detection tasks with ten datasets present that
evidence pollution, especially the generate strategy, significantly compromises
existing detectors. On the other hand, the defense strategies could mitigate
evidence pollution, but they faced limitations for practical employment, such
as the need for annotated data and huge inference costs. Further analysis
illustrates that polluted evidence is of high quality, would compromise the
model calibration, and could ensemble to amplify the negative impact.

摘要：證據增強式偵測器在識別具有相關證據的惡意社群文字方面展現出卓越的能力。然而，大型語言模型 (LLM) 的興起帶來了證據污染的潛在風險，可能會混淆偵測器。本文探討如何操縱證據，模擬潛在的誤用情境，包括基本的污染，以及透過 LLM 改寫或產生證據。為了減輕其負面影響，我們從資料和模型兩方面提出三種防禦策略，包括機器產生的文字偵測、專家組合，以及參數更新。在四個惡意社群文字偵測任務上進行的廣泛實驗，使用十個資料集，顯示證據污染，特別是產生策略，顯著地損害了現有的偵測器。另一方面，防禦策略可以減輕證據污染，但它們在實際應用上會遇到限制，例如需要有註解的資料和龐大的推論成本。進一步的分析說明，受到污染的證據品質很高，會損害模型校準，並且可以透過整合來擴大負面影響。

##### **Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting**
2410.12593v1 by Wei Chen, Yuxuan Liang

The widespread deployment of sensing devices leads to a surge in data for
spatio-temporal forecasting applications such as traffic flow, air quality, and
wind energy. Although spatio-temporal graph neural networks have achieved
success in modeling various static spatio-temporal forecasting scenarios,
real-world spatio-temporal data are typically received in a streaming manner,
and the network continuously expands with the installation of new sensors.
Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges:
the inefficiency of retraining models over newly arrived data and the
detrimental effects of catastrophic forgetting over long-term history. To
address these challenges, we propose a novel prompt tuning-based continuous
forecasting method, following two fundamental tuning principles guided by
empirical and theoretical analysis: expand and compress, which effectively
resolve the aforementioned problems with lightweight tuning parameters.
Specifically, we integrate the base spatio-temporal graph neural network with a
continuous prompt pool, utilizing stored prompts (i.e., few learnable
parameters) in memory, and jointly optimize them with the base spatio-temporal
graph neural network. This method ensures that the model sequentially learns
from the spatio-temporal data stream to accomplish tasks for corresponding
periods. Extensive experimental results on multiple real-world datasets
demonstrate the multi-faceted superiority of our method over the
state-of-the-art baselines, including effectiveness, efficiency, universality,
etc.

摘要：感測裝置的廣泛部署導致時空預測應用程式的資料激增，例如交通流量、空氣品質和風能。儘管時空圖形神經網路已成功建模各種靜態時空預測情境，但現實世界的時空資料通常是以串流方式接收，而且網路會隨著新感測器的安裝而持續擴展。因此，串流情境中的時空預測面臨兩項挑戰：對新抵達資料重新訓練模型的效率低下，以及長期歷史中災難性遺忘的有害影響。為了解決這些挑戰，我們提出了一種基於提示調整的連續預測新方法，遵循經驗和理論分析指導的兩個基本調整原則：擴展和壓縮，有效解決上述問題，且調整參數較輕量。具體來說，我們將基礎時空圖形神經網路與連續提示池整合，利用儲存在記憶體中的儲存提示（即少數可學習參數），並與基礎時空圖形神經網路共同最佳化。此方法確保模型從時空資料串流中循序學習，以完成對應期間的任務。在多個真實世界資料集上進行的廣泛實驗結果證明了我們的方法在有效性、效率、通用性等方面優於現有技術的基準。

##### **Rethinking Visual Counterfactual Explanations Through Region Constraint**
2410.12591v1 by Bartlomiej Sobieski, Jakub Grzywaczewski, Bartlomiej Sadlej, Matthew Tivnan, Przemyslaw Biecek

Visual counterfactual explanations (VCEs) have recently gained immense
popularity as a tool for clarifying the decision-making process of image
classifiers. This trend is largely motivated by what these explanations promise
to deliver -- indicate semantically meaningful factors that change the
classifier's decision. However, we argue that current state-of-the-art
approaches lack a crucial component -- the region constraint -- whose absence
prevents from drawing explicit conclusions, and may even lead to faulty
reasoning due to phenomenons like confirmation bias. To address the issue of
previous methods, which modify images in a very entangled and widely dispersed
manner, we propose region-constrained VCEs (RVCEs), which assume that only a
predefined image region can be modified to influence the model's prediction. To
effectively sample from this subclass of VCEs, we propose Region-Constrained
Counterfactual Schr\"odinger Bridges (RCSB), an adaptation of a tractable
subclass of Schr\"odinger Bridges to the problem of conditional inpainting,
where the conditioning signal originates from the classifier of interest. In
addition to setting a new state-of-the-art by a large margin, we extend RCSB to
allow for exact counterfactual reasoning, where the predefined region contains
only the factor of interest, and incorporating the user to actively interact
with the RVCE by predefining the regions manually.

摘要：視覺反事實解釋（VCE）最近作為一種用於釐清圖像分類器決策過程的工具而廣受歡迎。這種趨勢在很大程度上是由這些解釋所承諾提供的內容所推動的，即指出改變分類器決策的語義有意義的因素。然而，我們認為當前的最先進方法缺乏一個關鍵組成部分——區域約束——其缺失會妨礙得出明確的結論，甚至可能由於確認偏誤等現象而導致錯誤的推理。為了解決先前方法的問題，這些方法以非常糾纏和廣泛分散的方式修改圖像，我們提出了區域約束 VCE（RVCE），它假設只能修改預定義的圖像區域來影響模型的預測。為了有效地從此 VCE 子類中進行採樣，我們提出了區域約束反事實薛丁格橋（RCSB），這是薛丁格橋的一個易處理子類的適應，用於條件性內繪的難題，其中條件信號來自感興趣的分類器。除了大幅設定新的最先進標準外，我們還擴展了 RCSB 以允許進行精確的反事實推理，其中預定義區域僅包含感興趣的因素，並通過手動預定義區域讓使用者主動與 RVCE 互動。

##### **Can We Reverse In-Context Knowledge Edits?**
2410.12586v1 by Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert

In-context knowledge editing (IKE) enables efficient modification of large
language model (LLM) outputs without parameter changes and at zero-cost.
However, it can be misused to manipulate responses opaquely, e.g., insert
misinformation or offensive content. Such malicious interventions could be
incorporated into high-level wrapped APIs where the final input prompt is not
shown to end-users. To address this issue, we investigate the detection and
reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected
with high accuracy (F1 > 80\%) using only the top-10 output probabilities of
the next token, even in a black-box setting, e.g. proprietary LLMs with limited
output information. Further, we introduce the novel task of reversing IKE-edits
using specially tuned reversal tokens. We explore using both continuous and
discrete reversal tokens, achieving over 80\% accuracy in recovering original,
unedited outputs across multiple LLMs. Our continuous reversal tokens prove
particularly effective, with minimal impact on unedited prompts. Through
analysis of output distributions, attention patterns, and token rankings, we
provide insights into IKE's effects on LLMs and how reversal tokens mitigate
them. This work represents a significant step towards enhancing LLM resilience
against potential misuse of in-context editing, improving their transparency
and trustworthiness.

摘要：在語境中知識編輯 (IKE) 能夠有效修改大型語言模型 (LLM) 的輸出，無需參數變更且成本為零。
然而，它可能會被誤用來不透明地操縱回應，例如插入錯誤資訊或攻擊性內容。這種惡意的干預可能會被整合到高階包裝 API 中，其中最終輸入提示不會顯示給最終使用者。為了解決這個問題，我們研究了 IKE 編輯的偵測和還原。首先，我們展示了 IKE 編輯可以使用下一個代碼的僅前 10 個輸出機率來偵測，準確率很高 (F1 > 80%)，即使在黑盒設定中，例如具有有限輸出資訊的專有 LLM。此外，我們引入了使用特別調整的反轉代碼來還原 IKE 編輯的新穎任務。我們探討使用連續和離散的反轉代碼，在多個 LLM 中恢復原始未編輯輸出的準確率達 80% 以上。我們的連續反轉代碼被證明特別有效，對未編輯提示的影響最小。透過分析輸出分佈、注意力模式和代碼排名，我們提供了對 IKE 對 LLM 的影響以及反轉代碼如何減輕這些影響的見解。這項工作代表了在增強 LLM 對語境編輯潛在誤用的復原力、改善其透明度和可信度方面邁出的重要一步。

##### **STRUX: An LLM for Decision-Making with Structured Explanations**
2410.12583v1 by Yiming Lu, Yebowen Hu, Hassan Foroosh, Wei Jin, Fei Liu

Countless decisions shape our daily lives, and it is paramount to understand
the how and why behind these choices. In this paper, we introduce a new LLM
decision-making framework called STRUX, which enhances LLM decision-making by
providing structured explanations. These include favorable and adverse facts
related to the decision, along with their respective strengths. STRUX begins by
distilling lengthy information into a concise table of key facts. It then
employs a series of self-reflection steps to determine which of these facts are
pivotal, categorizing them as either favorable or adverse in relation to a
specific decision. Lastly, we fine-tune an LLM to identify and prioritize these
key facts to optimize decision-making. STRUX has been evaluated on the
challenging task of forecasting stock investment decisions based on earnings
call transcripts and demonstrated superior performance against strong
baselines. It enhances decision transparency by allowing users to understand
the impact of different factors, representing a meaningful step towards
practical decision-making with LLMs.

摘要：無數的決定形塑著我們的日常生活，而理解這些選擇背後的原因和方式至關重要。在本文中，我們介紹了一個名為 STRUX 的 LLM 決策制定框架，它透過提供結構化的說明來增強 LLM 決策制定。這些說明包括與決策相關的有利和不利事實，以及它們各自的優缺點。STRUX 首先將冗長的資訊提煉成一個簡潔的關鍵事實表格。然後，它採用一系列自我反省步驟來確定哪些事實至關重要，並將它們分類為有利或不利於特定決策。最後，我們微調 LLM 以識別和優先考慮這些關鍵事實，以優化決策制定。STRUX 已針對根據收益電話記錄預測股票投資決策的挑戰性任務進行評估，並證明其相對於強大的基準具有優異的表現。它透過讓使用者了解不同因素的影響來增強決策透明度，代表著朝著使用 LLM 進行實際決策邁出了一大步。

##### **On the Utility of Domain Modeling Assistance with Large Language Models**
2410.12577v1 by Meriem Ben Chaaben, Lola Burgueño, Istvan David, Houari Sahraoui

Model-driven engineering (MDE) simplifies software development through
abstraction, yet challenges such as time constraints, incomplete domain
understanding, and adherence to syntactic constraints hinder the design
process. This paper presents a study to evaluate the usefulness of a novel
approach utilizing large language models (LLMs) and few-shot prompt learning to
assist in domain modeling. The aim of this approach is to overcome the need for
extensive training of AI-based completion models on scarce domain-specific
datasets and to offer versatile support for various modeling activities,
providing valuable recommendations to software modelers. To support this
approach, we developed MAGDA, a user-friendly tool, through which we conduct a
user study and assess the real-world applicability of our approach in the
context of domain modeling, offering valuable insights into its usability and
effectiveness.

摘要：模型驅動工程 (MDE) 透過抽象簡化軟體開發，但時間限制、不完整的領域理解和對語法約束的堅持等挑戰阻礙了設計流程。本文提出了一項研究，以評估一種新方法的實用性，該方法利用大型語言模型 (LLM) 和少次提示學習來協助領域建模。此方法的目標是克服在稀缺的特定領域資料集上對基於 AI 的完成模型進行廣泛訓練的需求，並為各種建模活動提供多功能支援，為軟體建模人員提供有價值的建議。為了支援此方法，我們開發了一個使用者友善的工具 MAGDA，透過該工具，我們進行使用者研究並評估我們的方法在領域建模中的實際適用性，提供對其可用性和有效性的寶貴見解。

##### **Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving**
2410.12568v1 by Sihao Wu, Jiaxu Liu, Xiangyu Yin, Guangliang Cheng, Meng Fang, Xingyu Zhao, Xinping Yi, Xiaowei Huang

The integration of Large Language Models (LLMs) into autonomous driving
systems demonstrates strong common sense and reasoning abilities, effectively
addressing the pitfalls of purely data-driven methods. Current LLM-based agents
require lengthy inference times and face challenges in interacting with
real-time autonomous driving environments. A key open question is whether we
can effectively leverage the knowledge from LLMs to train an efficient and
robust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel
\underline{\textbf{R}}obust \underline{\textbf{A}}daptive
\underline{\textbf{P}}olicy \underline{\textbf{I}}nfusion and
\underline{\textbf{D}}istillation framework, which trains specialized
mix-of-policy RL agents using data synthesized by an LLM-based driving agent
and online adaptation. RAPID features three key designs: 1) utilization of
offline data collected from an LLM agent to distil expert knowledge into RL
policies for faster real-time inference; 2) introduction of robust distillation
in RL to inherit both performance and robustness from LLM-based teacher; and 3)
employment of a mix-of-policy approach for joint decision decoding with a
policy adapter. Through fine-tuning via online environment interaction, RAPID
reduces the forgetting of LLM knowledge while maintaining adaptability to
different tasks. Extensive experiments demonstrate RAPID's capability to
effectively integrate LLM knowledge into scaled-down RL policies in an
efficient, adaptable, and robust way. Code and checkpoints will be made
publicly available upon acceptance.

摘要：大型語言模型 (LLM) 整合到自動駕駛系統中，展現出強大的常識和推理能力，有效解決純數據驅動方法的缺陷。當前基於 LLM 的代理需要冗長的推論時間，並在與實時自動駕駛環境互動時面臨挑戰。一個關鍵的開放性問題是，我們是否能有效利用 LLM 的知識來訓練一個有效率且強健的強化學習 (RL) 代理。本文介紹 RAPID，一個新穎的\underline{\textbf{R}}obust \underline{\textbf{A}}daptive \underline{\textbf{P}}olicy \underline{\textbf{I}}nfusion and \underline{\textbf{D}}istillation 框架，它使用由基於 LLM 的駕駛代理和線上適應合成的數據，來訓練專門的混合策略 RL 代理。RAPID 具有三個關鍵設計：1) 利用從 LLM 代理收集的離線數據，將專家知識提煉到 RL 政策中，以加快實時推論；2) 在 RL 中引入強健的提煉，以繼承基於 LLM 的教師的效能和強健性；3) 採用混合策略方法，使用策略適配器進行聯合決策解碼。透過線上環境互動進行微調，RAPID 減少了 LLM 知識的遺忘，同時保持對不同任務的適應性。廣泛的實驗證明了 RAPID 有能力以有效率、適應性和強健的方式，將 LLM 知識有效整合到縮小的 RL 政策中。程式碼和檢查點將在接受後公開。

##### **Development of Image Collection Method Using YOLO and Siamese Network**
2410.12561v1 by Chan Young Shin, Ah Hyun Lee, Jun Young Lee, Ji Min Lee, Soo Jin Park

As we enter the era of big data, collecting high-quality data is very
important. However, collecting data by humans is not only very time-consuming
but also expensive. Therefore, many scientists have devised various methods to
collect data using computers. Among them, there is a method called web
crawling, but the authors found that the crawling method has a problem in that
unintended data is collected along with the user. The authors found that this
can be filtered using the object recognition model YOLOv10. However, there are
cases where data that is not properly filtered remains. Here, image
reclassification was performed by additionally utilizing the distance output
from the Siamese network, and higher performance was recorded than other
classification models. (average \_f1 score YOLO+MobileNet
0.678->YOLO+SiameseNet 0.772)) The user can specify a distance threshold to
adjust the balance between data deficiency and noise-robustness. The authors
also found that the Siamese network can achieve higher performance with fewer
resources because the cropped images are used for object recognition when
processing images in the Siamese network. (Class 20 mean-based f1 score,
non-crop+Siamese(MobileNetV3-Small) 80.94 -> crop
preprocessing+Siamese(MobileNetV3-Small) 82.31) In this way, the image
retrieval system that utilizes two consecutive models to reduce errors can save
users' time and effort, and build better quality data faster and with fewer
resources than before.

摘要：隨著我們進入大數據時代，收集高品質的數據非常重要。然而，由人類收集數據不僅非常耗時，而且成本高昂。因此，許多科學家已經設計出各種使用電腦收集數據的方法。其中，有一種方法稱為網路爬蟲，但作者發現爬蟲方法存在一個問題，即會在收集使用者的同時收集到意外的數據。作者發現這可以使用物件辨識模型 YOLOv10 來過濾。然而，有時候會有一些數據無法適當過濾。在此，透過額外使用 Siamese 網路的距離輸出執行影像重新分類，並且記錄到的效能比其他分類模型高。（平均 _f1 分數 YOLO+MobileNet 0.678->YOLO+SiameseNet 0.772）使用者可以指定距離閾值來調整數據不足和抗雜訊的平衡。作者還發現，由於在 Siamese 網路中處理影像時，會使用裁切的影像進行物件辨識，因此 Siamese 網路可以使用較少的資源來達成較高的效能。（類別 20 的平均 f1 分數，未裁切+Siamese（MobileNetV3-Small）80.94 -> 裁切預處理+Siamese（MobileNetV3-Small）82.31）透過這種方式，使用兩個連續的模型來減少錯誤的影像檢索系統可以節省使用者的時間和精力，並比以前更快、使用更少的資源來建立品質更好的數據。

##### **A Claim Decomposition Benchmark for Long-form Answer Verification**
2410.12558v1 by Zhihao Zhang, Yixing Fan, Ruqing Zhang, Jiafeng Guo

The advancement of LLMs has significantly boosted the performance of complex
long-form question answering tasks. However, one prominent issue of LLMs is the
generated "hallucination" responses that are not factual. Consequently,
attribution for each claim in responses becomes a common solution to improve
the factuality and verifiability. Existing researches mainly focus on how to
provide accurate citations for the response, which largely overlook the
importance of identifying the claims or statements for each response. To bridge
this gap, we introduce a new claim decomposition benchmark, which requires
building system that can identify atomic and checkworthy claims for LLM
responses. Specifically, we present the Chinese Atomic Claim Decomposition
Dataset (CACDD), which builds on the WebCPM dataset with additional expert
annotations to ensure high data quality. The CACDD encompasses a collection of
500 human-annotated question-answer pairs, including a total of 4956 atomic
claims. We further propose a new pipeline for human annotation and describe the
challenges of this task. In addition, we provide experiment results on
zero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the
claim decomposition is highly challenging and requires further explorations.
All code and data are publicly available at
\url{https://github.com/FBzzh/CACDD}.

摘要：大型語言模型的進步顯著提升了複雜長篇問答任務的性能。然而，大型語言模型的一個顯著問題是產生的「幻覺」回應並非事實。因此，回應中每個說法的歸因成為改善事實性和可驗證性的常見解決方案。現有的研究主要集中在如何為回應提供準確的引用，這在很大程度上忽視了識別每個回應的說法或陳述的重要性。為了彌補這一差距，我們引入了一個新的說法分解基準，它需要建立一個系統，該系統可以識別大型語言模型回應的原子和可驗證的說法。具體來說，我們提出了中文原子說法分解數據集 (CACDD)，它建立在 WebCPM 數據集之上，並增加了專家註釋以確保高數據質量。CACDD 包含一系列 500 個人工註釋的問題答案對，總共包括 4956 個原子說法。我們進一步提出了人工註釋的新管道，並描述了此任務的挑戰。此外，我們提供了零次學習、少次學習和微調大型語言模型的實驗結果作為基準。結果表明，說法分解極具挑戰性，需要進一步探索。所有代碼和數據都可以在 \url{https://github.com/FBzzh/CACDD} 公開獲得。

##### **LLM-based Translation Inference with Iterative Bilingual Understanding**
2410.12543v1 by Andong Chen, Kehai Chen, Yang Xiang, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min zhang

The remarkable understanding and generation capabilities of large language
models (LLMs) have greatly improved translation performance. However, incorrect
understanding of the sentence to be translated can degrade translation quality.
To address this issue, we proposed a novel Iterative Bilingual Understanding
Translation (IBUT) method based on the cross-lingual capabilities of LLMs and
the dual characteristics of translation tasks. The cross-lingual capability of
LLMs enables the generation of contextual understanding for both the source and
target languages separately. Furthermore, the dual characteristics allow IBUT
to generate effective cross-lingual feedback, iteratively refining contextual
understanding, thereby reducing errors and improving translation performance.
Experimental results showed that the proposed IBUT outperforms several strong
comparison methods, especially being generalized to multiple domains (e.g.,
news, commonsense, and cultural translation benchmarks).

摘要：大型語言模型 (LLM) 出色的理解和生成能力已大幅提升翻譯效能。然而，對待翻譯句子的理解不正確，可能會降低翻譯品質。為了解決這個問題，我們提出了一種基於 LLM 的跨語言能力和翻譯任務雙重特性的創新迭代雙語理解翻譯 (IBUT) 方法。LLM 的跨語言能力可分別為原始語言和目標語言產生脈絡理解。此外，雙重特性允許 IBUT 產生有效的跨語言回饋，反覆精進脈絡理解，從而減少錯誤並提升翻譯效能。實驗結果顯示，所提出的 IBUT 優於多種強大的對比方法，特別是能廣泛運用於多個領域（例如新聞、常識和文化翻譯基準）。

##### **Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making**
2410.12539v1 by Stelios Triantafyllou, Aleksa Sukovic, Yasaman Zolfimoselo, Goran Radanovic

We address the challenge of explaining counterfactual outcomes in multi-agent
Markov decision processes. In particular, we aim to explain the total
counterfactual effect of an agent's action on the outcome of a realized
scenario through its influence on the environment dynamics and the agents'
behavior. To achieve this, we introduce a novel causal explanation formula that
decomposes the counterfactual effect by attributing to each agent and state
variable a score reflecting their respective contributions to the effect.
First, we show that the total counterfactual effect of an agent's action can be
decomposed into two components: one measuring the effect that propagates
through all subsequent agents' actions and another related to the effect that
propagates through the state transitions. Building on recent advancements in
causal contribution analysis, we further decompose these two effects as
follows. For the former, we consider agent-specific effects -- a causal concept
that quantifies the counterfactual effect of an agent's action that propagates
through a subset of agents. Based on this notion, we use Shapley value to
attribute the effect to individual agents. For the latter, we consider the
concept of structure-preserving interventions and attribute the effect to state
variables based on their "intrinsic" contributions. Through extensive
experimentation, we demonstrate the interpretability of our decomposition
approach in a Gridworld environment with LLM-assisted agents and a sepsis
management simulator.

摘要：我們探討了解釋多智能體馬可夫決策過程中反事實結果的挑戰。特別是，我們旨在透過影響環境動態和智能體的行為，解釋智能體的動作對已實現情境的結果的總體反事實影響。為此，我們引進了一個新穎的因果解釋公式，透過將反事實影響分解，將其歸因於每個智能體和狀態變數，並給予一個反映其對影響的各自貢獻的分數。首先，我們展示了智能體動作的總體反事實影響可以分解成兩個組成部分：一個測量透過所有後續智能體動作傳播的影響，另一個則與透過狀態轉換傳播的影響有關。建立在因果貢獻分析的最新進展上，我們進一步將這兩個影響分解如下。對於前者，我們考慮特定於智能體的影響——一個量化了智能體動作的反事實影響並透過智能體子集傳播的因果概念。基於這個概念，我們使用 Shapley 值將影響歸因於個別智能體。對於後者，我們考慮結構保留介入的概念，並根據其「內在」貢獻將影響歸因於狀態變數。透過廣泛的實驗，我們證明了我們的分解方法在有 LLM 協助的智能體的網格世界環境和敗血症管理模擬器中的可解釋性。

##### **Characterizing Behavioral Differences and Adaptations of Automated Vehicles and Human Drivers at Unsignalized Intersections: Insights from Waymo and Lyft Open Datasets**
2410.12538v1 by Saeed Rahmani, Zhenlin, Xu, Simeon C. Calvert, Bart van Arem

The integration of autonomous vehicles (AVs) into transportation systems
presents an unprecedented opportunity to enhance road safety and efficiency.
However, understanding the interactions between AVs and human-driven vehicles
(HVs) at intersections remains an open research question. This study aims to
bridge this gap by examining behavioral differences and adaptations of AVs and
HVs at unsignalized intersections by utilizing two comprehensive AV datasets
from Waymo and Lyft. Using a systematic methodology, the research identifies
and analyzes merging and crossing conflicts by calculating key safety and
efficiency metrics, including time to collision (TTC), post-encroachment time
(PET), maximum required deceleration (MRD), time advantage (TA), and speed and
acceleration profiles. The findings reveal a paradox in mixed traffic flow:
while AVs maintain larger safety margins, their conservative behavior can lead
to unexpected situations for human drivers, potentially causing unsafe
conditions. From a performance point of view, human drivers exhibit more
consistent behavior when interacting with AVs versus other HVs, suggesting AVs
may contribute to harmonizing traffic flow patterns. Moreover, notable
differences were observed between Waymo and Lyft vehicles, which highlights the
importance of considering manufacturer-specific AV behaviors in traffic
modeling and management strategies for the safe integration of AVs. The
processed dataset utilized in this study is openly published to foster the
research on AV-HV interactions.

摘要：自動駕駛車輛 (AV) 整合到交通系統中，提供了一個前所未有的機會來提升道路安全和效率。然而，了解自動駕駛車輛和人類駕駛車輛 (HV) 在交叉路口的互動，仍然是一個開放的研究問題。本研究旨在透過利用來自 Waymo 和 Lyft 的兩個全面的自動駕駛車輛數據集，探討自動駕駛車輛和人類駕駛車輛在沒有信號燈的交叉路口中的行為差異和適應性，以彌補這個差距。本研究使用系統化的方法，透過計算關鍵的安全和效率指標（包括碰撞時間 (TTC)、侵佔後時間 (PET)、最大所需減速度 (MRD)、時間優勢 (TA) 以及速度和加速度曲線），來識別和分析匯入和交叉衝突。研究結果揭示了混合交通流中的矛盾現象：雖然自動駕駛車輛維持較大的安全裕度，但其保守的行為可能會導致人類駕駛發生意外狀況，進而造成不安全的狀況。從性能的角度來看，人類駕駛在與自動駕駛車輛互動時，表現出比與其他人類駕駛車輛互動時更一致的行為，這表示自動駕駛車輛可能有助於協調交通流模式。此外，在 Waymo 和 Lyft 車輛之間觀察到顯著的差異，這突顯了在交通建模和管理策略中考量特定製造商的自動駕駛車輛行為的重要性，以安全整合自動駕駛車輛。本研究中使用的已處理數據集公開發布，以促進對自動駕駛車輛與人類駕駛車輛互動的研究。

##### **Is Complex Query Answering Really Complex?**
2410.12537v1 by Cosimo Gregucci, Bo Xiong, Daniel Hernandez, Lorenzo Loconte, Pasquale Minervini, Steffen Staab, Antonio Vergari

Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum
as a challenging reasoning task. In this paper, we show that the current
benchmarks for CQA are not really complex, and the way they are built distorts
our perception of progress in this field. For example, we find that in these
benchmarks, most queries (up to 98% for some query types) can be reduced to
simpler problems, e.g., link prediction, where only one link needs to be
predicted. The performance of state-of-the-art CQA models drops significantly
when such models are evaluated on queries that cannot be reduced to easier
types. Thus, we propose a set of more challenging benchmarks, composed of
queries that require models to reason over multiple hops and better reflect the
construction of real-world KGs. In a systematic empirical investigation, the
new benchmarks show that current methods leave much to be desired from current
CQA methods.

摘要：複雜查詢回答（CQA）在知識圖譜（KG）上正逐漸成為一項具有挑戰性的推理任務。在本文中，我們將表明 CQA 的當前基準並非真正複雜，且其建構方式扭曲了我們對此領域進展的認知。例如，我們發現，在這些基準中，大多數查詢（對於某些查詢類型而言高達 98%）都可以簡化為更簡單的問題，例如連結預測，其中只需要預測一個連結。當此類模型在無法簡化為更簡單類型的查詢上進行評估時，最先進的 CQA 模型的效能會大幅下降。因此，我們提出了一組更具挑戰性的基準，其中包含需要模型透過多個跳躍進行推理並能更好地反映真實世界 KG 建構的查詢。在系統性的實證調查中，新的基準表明，目前的 CQA 方法與理想狀態仍有很大的差距。

##### **MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration**
2410.12532v1 by Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang

Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.

摘要：大型語言模型 (LLM) 驅動的互動系統目前在醫療保健領域顯示出潛在的希望。儘管它們具有非凡的能力，LLM 在複雜的醫療應用中通常缺乏個人化建議和診斷分析，從而導致幻覺和效能瓶頸。為了應對這些挑戰，本文提出了 MedAide，一個基於 LLM 的全方位醫療多代理協作架構，用於專業醫療保健服務。具體來說，MedAide 首先透過檢索增強生成來執行查詢改寫，以達成準確的醫療意圖理解。緊接著，我們設計一個脈絡編碼器來取得意圖原型嵌入，用於透過相似度比對來辨識細粒度的意圖。根據意圖相關性，已啟動的代理有效地協作，以提供整合的決策分析。在四個具有複合意圖的醫療基準上進行了廣泛的實驗。自動化指標和專家醫師評估的實驗結果顯示，MedAide 優於目前的 LLM，並提升了它們的醫療能力和策略推理。

##### **Spectrum Sharing using Deep Reinforcement Learning in Vehicular Networks**
2410.12521v1 by Riya Dinesh Deshpande, Faheem A. Khan, Qasim Zeeshan Ahmed

As the number of devices getting connected to the vehicular network grows
exponentially, addressing the numerous challenges of effectively allocating
spectrum in dynamic vehicular environment becomes increasingly difficult.
Traditional methods may not suffice to tackle this issue. In vehicular networks
safety critical messages are involved and it is important to implement an
efficient spectrum allocation paradigm for hassle free communication as well as
manage the congestion in the network. To tackle this, a Deep Q Network (DQN)
model is proposed as a solution, leveraging its ability to learn optimal
strategies over time and make decisions. The paper presents a few results and
analyses, demonstrating the efficacy of the DQN model in enhancing spectrum
sharing efficiency. Deep Reinforcement Learning methods for sharing spectrum in
vehicular networks have shown promising outcomes, demonstrating the system's
ability to adjust to dynamic communication environments. Both SARL and MARL
models have exhibited successful rates of V2V communication, with the
cumulative reward of the RL model reaching its maximum as training progresses.

摘要：隨著連接到車輛網路的裝置數量呈指數成長，在動態的車輛環境中有效分配頻譜的眾多挑戰變得越來越困難。傳統方法可能不足以解決此問題。在車輛網路中涉及安全關鍵訊息，並且實施有效的頻譜分配範例對於無障礙通訊以及管理網路中的壅塞非常重要。為了解決此問題，提出了深度 Q 網路 (DQN) 模型作為解決方案，利用其隨著時間推移學習最佳策略和做出決策的能力。本文提出了一些結果和分析，證明了 DQN 模型在提高頻譜共享效率方面的效能。車輛網路中用於共享頻譜的深度強化學習方法已顯示出有希望的結果，證明了系統適應動態通訊環境的能力。SARL 和 MARL 模型都展現了成功的 V2V 通訊速率，RL 模型的累積回報隨著訓練的進行而達到最大值。

##### **FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction**
2410.12513v1 by Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal

Auto-regressive Large Language Models (LLMs) demonstrate remarkable
performance across domanins such as vision and language processing. However,
due to sequential processing through a stack of transformer layers,
autoregressive decoding faces significant computation/latency challenges,
particularly in resource constrained environments like mobile and edge devices.
Existing approaches in literature that aim to improve latency via skipping
layers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics
where tokens exit at pre-determined layers irrespective of input sequence. Both
the above strategies have limitations - the former cannot be applied to handle
KV Caching necessary for speed-ups in modern framework and the latter does not
capture the variation in layer importance across tasks or more generally,
across input sequences. To address both limitations, we propose FIRST, an
algorithm that reduces inference latency by using layer-specific routers to
select a subset of transformer layers adaptively for each input sequence - the
prompt (during prefill stage) decides which layers will be skipped during
decoding. FIRST preserves compatibility with KV caching enabling faster
inference while being quality-aware. FIRST is model-agnostic and can be easily
enabled on any pre-trained LLM. We further improve performance by incorporating
LoRA adapters for fine-tuning on external datasets, enhancing task-specific
accuracy while maintaining latency benefits. Our approach reveals that input
adaptivity is critical - indeed, different task-specific middle layers play a
crucial role in evolving hidden representations depending on task. Extensive
experiments show that FIRST significantly reduces latency while retaining
competitive performance (as compared to baselines), making our approach an
efficient solution for LLM deployment in low-resource environments.

摘要：自動回歸大型語言模型 (LLM) 在視覺和語言處理等領域表現出色。然而，由於通過堆疊轉換器層進行順序處理，自動回歸解碼面臨著顯著的計算/延遲挑戰，特別是在移動和邊緣設備等資源受限的環境中。現有文獻中旨在通過跳過層來改善延遲的方法有兩種不同的風格 - 1) 早期退出 2) 與輸入無關的啟發式方法，其中令牌在預先確定的層退出，而與輸入序列無關。上述兩種策略都有局限性 - 前者不能用於處理現代框架中加速所需的 KV 快取，而後者沒有捕捉到跨任務或更一般地跨輸入序列的層重要性的變化。為了解決這兩個限制，我們提出了 FIRST，這是一種通過使用特定於層的路由器自適應地為每個輸入序列選擇轉換器層子集來減少推論延遲的演算法 - 提示 (在預填充階段) 決定在解碼過程中將跳過哪些層。FIRST 保持與 KV 快取的相容性，從而在保證品質的同時實現更快的推論。FIRST 與模型無關，並且可以輕鬆地在任何預先訓練的 LLM 上啟用。我們進一步通過整合 LoRA 適配器來改進在外部資料集上的微調效能，在保持延遲優勢的同時提高特定任務的準確度。我們的做法表明，輸入自適應性至關重要 - 事實上，不同的特定任務中間層在根據任務演化隱藏表示方面發揮著至關重要的作用。大量的實驗表明，FIRST 在保持競爭效能的同時顯著降低了延遲 (與基線相比)，使我們的做法成為在低資源環境中部署 LLM 的有效解決方案。

##### **Advancing Fairness in Natural Language Processing: From Traditional Methods to Explainability**
2410.12511v1 by Fanny Jourdan

The burgeoning field of Natural Language Processing (NLP) stands at a
critical juncture where the integration of fairness within its frameworks has
become an imperative. This PhD thesis addresses the need for equity and
transparency in NLP systems, recognizing that fairness in NLP is not merely a
technical challenge but a moral and ethical necessity, requiring a rigorous
examination of how these technologies interact with and impact diverse human
populations. Through this lens, this thesis undertakes a thorough investigation
into the development of equitable NLP methodologies and the evaluation of
biases that prevail in current systems.
  First, it introduces an innovative algorithm to mitigate biases in
multi-class classifiers, tailored for high-risk NLP applications, surpassing
traditional methods in both bias mitigation and prediction accuracy. Then, an
analysis of the Bios dataset reveals the impact of dataset size on
discriminatory biases and the limitations of standard fairness metrics. This
awareness has led to explorations in the field of explainable AI, aiming for a
more complete understanding of biases where traditional metrics are limited.
Consequently, the thesis presents COCKATIEL, a model-agnostic explainability
method that identifies and ranks concepts in Transformer models, outperforming
previous approaches in sentiment analysis tasks. Finally, the thesis
contributes to bridging the gap between fairness and explainability by
introducing TaCo, a novel method to neutralize bias in Transformer model
embeddings.
  In conclusion, this thesis constitutes a significant interdisciplinary
endeavor that intertwines explicability and fairness to challenge and reshape
current NLP paradigms. The methodologies and critiques presented contribute to
the ongoing discourse on fairness in machine learning, offering actionable
solutions for more equitable and responsible AI systems.

摘要：<paragraph>自然語言處理 (NLP) 蓬勃發展的領域正處於一個關鍵時刻，在其中，公平性的整合已成為其架構中的一項要務。本博士論文探討了 NLP 系統中公平性和透明性的需求，並認識到 NLP 中的公平性不僅僅是一項技術挑戰，更是一項道德和倫理必要性，需要嚴格審查這些技術如何與不同的人群互動並對其產生影響。透過此觀點，本論文對公平 NLP 方法的開發以及當前系統中普遍存在的偏差進行了徹底調查。
首先，它引入了一種創新的演算法來減輕多類別分類器中的偏差，專門針對高風險 NLP 應用，在偏差減輕和預測準確性方面都優於傳統方法。然後，對 Bios 資料集的分析揭示了資料集大小對歧視性偏差的影響以及標準公平性指標的限制。這種認知已導致在可解釋 AI 領域的探索，旨在更全面地了解傳統指標受限的偏差。因此，本論文提出了 COCKATIEL，這是一種與模型無關的可解釋性方法，它可以識別和排列 Transformer 模型中的概念，在情緒分析任務中優於先前的做法。最後，本論文透過引入 TaCo，一種在 Transformer 模型嵌入中消除偏差的新方法，為彌合公平性和可解釋性之間的差距做出了貢獻。
總之，本論文構成了一項重要的跨學科努力，它將可解釋性和公平性交織在一起，以挑戰和重塑當前的 NLP 典範。提出的方法和批評有助於關於機器學習中公平性的持續討論，為更公平且負責任的 AI 系統提供了可行的解決方案。</paragraph>

##### **Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions**
2410.12509v1 by Ilias Tachmazidis, Sotiris Batsakis, Grigoris Antoniou

Large Language Models (LLMs) have gained prominence in the AI landscape due
to their exceptional performance. Thus, it is essential to gain a better
understanding of their capabilities and limitations, among others in terms of
nonmonotonic reasoning. This paper proposes a benchmark that corresponds to
various defeasible rule-based reasoning patterns. We modified an existing
benchmark for defeasible logic reasoners by translating defeasible rules into
text suitable for LLMs. We conducted preliminary experiments on nonmonotonic
rule-based reasoning using ChatGPT and compared it with reasoning patterns
defined by defeasible logic.

摘要：大型語言模型 (LLM) 在人工智慧領域中獲得顯著地位，因為它們具有傑出的表現。因此，有必要對它們的能力和限制有更深入的了解，其中包括非單調推理。本文提出了對應於各種可推翻規則推理模式的基準。我們修改了一個現有的可推翻邏輯推理基準，將可推翻規則轉換為適合 LLM 的文字。我們對使用 ChatGPT 的非單調規則推理進行了初步實驗，並將其與可推翻邏輯定義的推理模式進行了比較。

##### **DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning**
2410.12501v1 by Jiabao Wei, Zhiyuan Ma

Virtual Try-ON (VTON) aims to synthesis specific person images dressed in
given garments, which recently receives numerous attention in online shopping
scenarios. Currently, the core challenges of the VTON task mainly lie in the
fine-grained semantic extraction (i.e.,deep semantics) of the given reference
garments during depth estimation and effective texture preservation when the
garments are synthesized and warped onto human body. To cope with these issues,
we propose DH-VTON, a deep text-driven virtual try-on model featuring a special
hybrid attention learning strategy and deep garment semantic preservation
module. By standing on the shoulder of a well-built pre-trained
paint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this
work. Specifically, to extract the deep semantics of the garments, we first
introduce InternViT-6B as fine-grained feature learner, which can be trained to
align with the large-scale intrinsic knowledge with deep text semantics
(e.g.,"neckline" or "girdle") to make up for the deficiency of the commonly
adopted CLIP encoder. Based on this, to enhance the customized dressing
abilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+)
module and propose to leverage a fresh hybrid attention strategy for training,
which can adaptively integrate fine-grained characteristics of the garments
into the different layers of the VTON model, so as to achieve multi-scale
features preservation effects. Extensive experiments on several representative
datasets demonstrate that our method outperforms previous diffusion-based and
GAN-based approaches, showing competitive performance in preserving garment
details and generating authentic human images.

摘要：虛擬試穿（VTON）旨在合成特定人物穿著指定服裝的影像，最近在線上購物情境中受到廣泛關注。目前，VTON 任務的核心挑戰主要在於深度估計期間指定參考服裝的細緻語義萃取（即深度語義），以及在服裝合成並變形到人體上時有效保留紋理。為了應對這些問題，我們提出 DH-VTON，一種以特殊混合注意力學習策略和深度服裝語義保留模組為特色的深度文字驅動虛擬試穿模型。在這項工作中，我們建立在一個建構良好的預訓練範例繪製（簡稱 PBE）方法的基礎上，提出我們的 DH-VTON 處理程序。具體來說，為了萃取服裝的深度語義，我們首先引入 InternViT-6B 作為細緻特徵學習器，它可以訓練與具有深度文字語義（例如，「領口」或「腰帶」）的大規模內在知識保持一致，以彌補常採用的 CLIP 編碼器的不足。基於此，為了增強客製化穿著能力，我們進一步引入服裝特徵控制網路 Plus（簡稱 GFC+）模組，並提出利用新的混合注意力策略進行訓練，它可以自適應地將服裝的細緻特徵整合到 VTON 模型的不同層中，以實現多尺度特徵保留效果。在幾個具有代表性的資料集上進行的廣泛實驗表明，我們的模型優於先前的基於擴散和基於 GAN 的方法，在保留服裝細節和生成真實的人體影像方面展現出競爭力的效能。

##### **With a Grain of SALT: Are LLMs Fair Across Social Dimensions?**
2410.12499v1 by Samee Arif, Zohaib Khan, Agha Ali Raza, Awais Athar

This paper presents an analysis of biases in open-source Large Language
Models (LLMs) across various genders, religions, and races. We introduce a
methodology for generating a bias detection dataset using seven bias triggers:
General Debate, Positioned Debate, Career Advice, Story Generation,
Problem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to
generate a diverse set of prompts for each trigger across various genders,
religious and racial groups. We evaluate models from Llama and Gemma family on
the generated dataset. We anonymise the LLM-generated text associated with each
group using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.
To quantify bias in the LLM-generated text we use the number of wins and losses
in the pairwise comparison. Our analysis spans three languages, English,
German, and Arabic to explore how language influences bias manifestation. Our
findings reveal that LLMs exhibit strong polarization toward certain groups
across each category, with a notable consistency observed across models.
However, when switching languages, variations and anomalies emerge, often
attributable to cultural cues and contextual differences.

摘要：這篇論文分析了開放原始碼大型語言模型 (LLM) 中各種性別、宗教和種族偏見。我們引入了一種生成偏見檢測資料集的方法，使用七個偏見觸發器：一般性辯論、立場辯論、職業建議、故事生成、問題解決、求職信撰寫和履歷生成。我們使用 GPT-4o 為各種性別、宗教和種族群體的每個觸發器生成一組多樣的提示。我們根據生成的資料集評估了 Llama 和 Gemma 家族的模型。我們使用 GPT-4o-mini 對與每個群組相關的 LLM 生成的文字進行匿名化，並使用 GPT-4o-as-a-Judge 進行成對比較。為了量化 LLM 生成的文字中的偏見，我們使用成對比較中的勝利和失敗次數。我們的分析涵蓋了英語、德語和阿拉伯語三種語言，以探討語言如何影響偏見表現。我們的研究結果表明，LLM 對每個類別中的某些群體表現出強烈的兩極分化，並且在各個模型中觀察到顯著的一致性。然而，在切換語言時，會出現變異和異常，通常歸因於文化線索和背景差異。

##### **End-to-end Planner Training for Language Modeling**
2410.12492v1 by Nathan Cornille, Florian Mai, Jingyuan Sun, Marie-Francine Moens

Through end-to-end training to predict the next token, LLMs have become
valuable tools for various tasks. Enhancing their core training in language
modeling can improve numerous downstream applications. A successful approach to
enhance language modeling uses a separate planning module to predict abstract
labels of future sentences and conditions the LM on these predictions. However,
this method is non-differentiable, preventing joint end-to-end tuning of the
planner with the LM. We propose an effective method to improve this approach by
enabling joint fine-tuning of the planner and the LM. We show that a naive way
of approximating the gradient of selecting a label via the straight-through
estimator is not effective. Instead, we propose to use the predicted label
probabilities as mixing weights to condition the LM on a weighted average of
label embeddings in a differentiable manner. This not only enables joint
fine-tuning of the planner and the LM, but also allows the LM to draw on the
full label distribution predicted by the planner, retaining more information.
Our experimental results show consistent improvements in perplexity.

摘要：透過端對端訓練來預測下一個符號，LLM 已成為各種任務的寶貴工具。增強其在語言模型中的核心訓練可以改善許多下游應用程式。增強語言模型的一個成功方法是使用一個獨立的規劃模組來預測未來句子的抽象標籤，並根據這些預測對 LM 進行條件化。然而，這種方法是不可微分的，阻止了規劃器與 LM 的聯合端對端調整。我們提出了一種有效的方法來改善這種方法，方法是讓規劃器和 LM 能夠進行聯合微調。我們表明，通過直通估計器來近似選擇標籤的梯度是一種天真的方式，這種方式是無效的。相反，我們建議使用預測的標籤機率作為混合權重，以可微分的方式對 LM 進行條件化，使其根據標籤嵌入的加權平均值進行條件化。這不僅能讓規劃器和 LM 進行聯合微調，還能讓 LM 利用規劃器預測的完整標籤分佈，從而保留更多資訊。我們的實驗結果顯示困惑度持續改善。

##### **Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL**
2410.12491v1 by Jared Joselowitz, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo

Large language models (LLMs) trained with Reinforcement Learning from Human
Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying
reward functions and decision-making processes remain opaque. This paper
introduces a novel approach to interpreting LLMs by applying inverse
reinforcement learning (IRL) to recover their implicit reward functions. We
conduct experiments on toxicity-aligned LLMs of varying sizes, extracting
reward models that achieve up to 80.40% accuracy in predicting human
preferences. Our analysis reveals key insights into the non-identifiability of
reward functions, the relationship between model size and interpretability, and
potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward
models can be used to fine-tune new LLMs, resulting in comparable or improved
performance on toxicity benchmarks. This work provides a new lens for
understanding and improving LLM alignment, with implications for the
responsible development and deployment of these powerful systems.

摘要：大型语言模型（LLM）经过人类反馈强化学习（RLHF）训练，展示了非凡的能力，但其底层奖励函数和决策过程仍然不透明。本文介绍了一种通过应用逆向强化学习（IRL）来解释 LLM 的新方法，以恢复其隐式奖励函数。我们对不同规模的毒性对齐 LLM 进行了实验，提取了奖励模型，其在预测人类偏好方面实现了高达 80.40% 的准确度。我们的分析揭示了奖励函数不可识别的关键见解、模型大小与可解释性之间的关系以及 RLHF 过程中的潜在缺陷。我们证明了 IRL 衍生的奖励模型可用于微调新的 LLM，从而在毒性基准上产生可比或改进的性能。这项工作为理解和改进 LLM 对齐提供了一个新的视角，对这些强大系统的负责任开发和部署具有影响。

##### **Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective**
2410.12490v1 by Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, Lidong Bing

Latent-based image generative models, such as Latent Diffusion Models (LDMs)
and Mask Image Models (MIMs), have achieved notable success in image generation
tasks. These models typically leverage reconstructive autoencoders like VQGAN
or VAE to encode pixels into a more compact latent space and learn the data
distribution in the latent space instead of directly from pixels. However, this
practice raises a pertinent question: Is it truly the optimal choice? In
response, we begin with an intriguing observation: despite sharing the same
latent space, autoregressive models significantly lag behind LDMs and MIMs in
image generation. This finding contrasts sharply with the field of NLP, where
the autoregressive model GPT has established a commanding presence. To address
this discrepancy, we introduce a unified perspective on the relationship
between latent space and generative models, emphasizing the stability of latent
space in image generative modeling. Furthermore, we propose a simple but
effective discrete image tokenizer to stabilize the latent space for image
generative modeling. Experimental results show that image autoregressive
modeling with our tokenizer (DiGIT) benefits both image understanding and image
generation with the next token prediction principle, which is inherently
straightforward for GPT models but challenging for other generative models.
Remarkably, for the first time, a GPT-style autoregressive model for images
outperforms LDMs, which also exhibits substantial improvement akin to GPT when
scaling up model size. Our findings underscore the potential of an optimized
latent space and the integration of discrete tokenization in advancing the
capabilities of image generative models. The code is available at
\url{https://github.com/DAMO-NLP-SG/DiGIT}.

摘要：<paragraph>潛在圖像生成模型，例如潛在擴散模型 (LDM) 和遮罩圖像模型 (MIM)，在圖像生成任務中取得了顯著的成功。這些模型通常利用重建自動編碼器，例如 VQGAN 或 VAE，將像素編碼成更緊湊的潛在空間，並在潛在空間中學習資料分佈，而不是直接從像素中學習。但是，這種做法引發了一個相關的問題：這真的是最佳選擇嗎？作為回應，我們從一個有趣的觀察開始：儘管共享相同的潛在空間，但自迴歸模型在圖像生成中顯著落後於 LDM 和 MIM。這一發現與 NLP 領域形成鮮明對比，在 NLP 領域，自迴歸模型 GPT 已確立了主導地位。為了解決這種差異，我們對潛在空間和生成模型之間的關係引入了一個統一的觀點，強調了潛在空間在圖像生成建模中的穩定性。此外，我們提出了一個簡單但有效的離散圖像分詞器，以穩定圖像生成建模的潛在空間。實驗結果表明，使用我們的分詞器 (DiGIT) 的圖像自迴歸建模既有利於圖像理解，也有利於圖像生成，其下一個符號預測原理本質上對 GPT 模型來說很簡單，但對其他生成模型來說卻很有挑戰性。值得注意的是，對於圖像，GPT 風格自迴歸模型首次優於 LDM，這也表現出類似於 GPT 在擴大模型規模時的顯著改進。我們的發現強調了優化潛在空間和整合離散分詞在提升圖像生成模型能力方面的潛力。程式碼可在 \url{https://github.com/DAMO-NLP-SG/DiGIT} 獲得。</paragraph>

##### **SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling**
2410.12481v1 by Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer

The past years have seen Large Language Models (LLMs) strive not only as
generative models but also as agents solving textual sequential decision-making
tasks. When facing complex environments where their zero-shot abilities are
insufficient, recent work showed online Reinforcement Learning (RL) could be
used for the LLM agent to discover and learn efficient strategies
interactively. However, most prior work sticks to on-policy algorithms, which
greatly reduces the scope of methods such agents could use for both exploration
and exploitation, such as experience replay and hindsight relabeling. Yet, such
methods may be key for LLM learning agents, and in particular when designing
autonomous intrinsically motivated agents sampling and pursuing their own goals
(i.e. autotelic agents). This paper presents and studies an adaptation of Soft
Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves
the path towards autotelic LLM agents that learn online but can also outperform
on-policy methods in more classic multi-goal RL environments.

摘要：近年來，大型語言模型 (LLM) 不僅作為生成模型，也作為解決文本順序決策任務的代理而努力。在面對其零次學習能力不足的複雜環境時，最近的研究表明，線上強化學習 (RL) 可用於 LLM 代理互動式地發現和學習有效策略。然而，大多數先前的研究都堅持使用策略梯度演算法，這大大減少了此類代理在探索和開發中可使用的方法的範圍，例如經驗重播和回顧式重新標記。然而，此類方法可能是 LLM 學習代理的關鍵，特別是在設計自主的內在動機代理，對自己的目標進行取樣和追求時 (即自動目標代理)。本文提出並研究了 Soft Actor-Critic 和回顧式重新標記對 LLM 代理的改編。我們的模型不僅為學習線上自動目標 LLM 代理鋪平了道路，還可以勝過更經典的多目標 RL 環境中的策略梯度演算法。

##### **KcMF: A Knowledge-compliant Framework for Schema and Entity Matching with Fine-tuning-free LLMs**
2410.12480v1 by Yongqin Xu, Huan Li, Ke Chen, Lidan Shou

Schema and entity matching tasks are crucial for data integration and
management. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. In this paper, we present the Knowledge-Compliant Matching
Framework (KcMF), an LLM-based approach that addresses these issues without the
need for domain-specific fine-tuning. KcMF employs a pseudo-code-based task
decomposition strategy to adopt task-specific natural language statements that
guide LLM reasoning and reduce confusion. We also propose two mechanisms,
Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain
knowledge sets when unstructured domain knowledge is lacking. Additionally, we
introduce a result-ensembling strategy to leverage multiple knowledge sources
and suppress poorly formatted outputs. Comprehensive evaluations on schema and
entity matching tasks demonstrate that KcMF outperforms previous non-LLM
state-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes
effectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across
different LLMs.

摘要：架構和實體比對任務對於資料整合和管理至關重要。儘管大型語言模型 (LLM) 已在這些任務中展現出令人滿意的結果，但它們卻會出現幻覺，且對任務指示感到困惑。在本文中，我們提出知識相容比對架構 (KcMF)，這是一種基於 LLM 的方法，可解決這些問題，而無需特定領域的微調。KcMF 採用基於偽程式碼的任務分解策略，以採用特定任務的自然語言陳述，引導 LLM 推理並減少困惑。我們還提出兩種機制，資料集作為知識 (DaK) 和範例作為知識 (EaK)，在缺乏非結構化領域知識時建立領域知識集。此外，我們引入結果整合策略，以利用多個知識來源並抑制格式不佳的輸出。架構和實體比對任務的綜合評估表明，KcMF 在平均 F1 分數方面優於先前的非 LLM 最先進 (SOTA) 方法 22.9%，並與 SOTA 微調 LLM 有效競爭。此外，KcMF 在不同的 LLM 中具有良好的泛化性。

##### **MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models**
2410.12478v1 by Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong

The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.

摘要：大型語言模型 (LLM) 產生幻覺的傾向引發了人們對其可靠性的擔憂。因此，指示世代可信度程度的信心估計變得至關重要。然而，目前 LLM 對英語以外語言的信心估計仍未得到充分探討。本文通過對多語言信心估計 (MlingConf) 進行全面的調查來解決這一差距，重點關注語言不可知 (LA) 和特定語言 (LS) 任務，以探索多語言信心估計在不同任務上的效能和語言優勢效應。基準包含四個經過仔細檢查和人工評估的高品質多語言資料集，用於 LA 任務，以及一個針對特定語言的社會、文化和地理背景量身打造的 LS 任務。我們的實驗表明，在 LA 任務中，英語在信心估計中表現出比其他語言顯著的語言優勢，而在 LS 任務中，使用與問題相關的語言提示 LLM 在多語言信心估計中表現出更好的語言優勢。這些現象激發了一種簡單但有效的原生語調提示策略，通過為 LS 任務採用特定語言的提示，有效提高了 LLM 在 LS 任務上的可靠性和準確性。

##### **Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial Generation**
2410.12476v1 by Zerui Xu, Fang Wu, Tianfan Fu, Yue Zhao

Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.

摘要：機器學習 (ML) 在臨床領域展現出前景。然而，它受到資料稀少性和倫理考量的限制，因為臨床試驗的產生會帶來嚴峻的挑戰，原因在於嚴格的隱私法規、高昂的成本，以及進行有人類參與者研究所需的時間很長。儘管大型語言模型 (LLM) 在一般生成任務中進展神速，它們在促進合成臨床試驗生成方面的潛力尚未被充分探討。為了解決這個差距，我們引進一個創新的檢索推理少樣本框架，它利用 LLM 來產生人工但逼真且多樣的臨床試驗，並具有二元成功/失敗標籤。在來自 \url{ClinicalTrials.gov} 資料庫的實際臨床試驗中進行的實驗證明，我們的合成資料可以有效地擴充實際資料集。此外，透過將預先訓練的模型微調為合成臨床試驗資料集上的二元分類器，我們證明了這種擴充增強了模型訓練，以進行下游任務，例如試驗結果預測。我們的研究結果表明，用於合成臨床試驗生成的 LLM 有望加速臨床研究，並維護患者隱私的倫理標準。程式碼已公開在 https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4。

##### **Unifying Economic and Language Models for Enhanced Sentiment Analysis of the Oil Market**
2410.12473v1 by Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun, Martin Tschudy

Crude oil, a critical component of the global economy, has its prices
influenced by various factors such as economic trends, political events, and
natural disasters. Traditional prediction methods based on historical data have
their limits in forecasting, but recent advancements in natural language
processing bring new possibilities for event-based analysis. In particular,
Language Models (LM) and their advancement, the Generative Pre-trained
Transformer (GPT), have shown potential in classifying vast amounts of natural
language. However, these LMs often have difficulty with domain-specific
terminology, limiting their effectiveness in the crude oil sector. Addressing
this gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude
oil market. The results indicate that CrudeBERT's sentiment scores align more
closely with the WTI Futures curve and significantly enhance price predictions,
underscoring the crucial role of integrating economic principles into LMs.

摘要：原油是全球經濟的重要組成部分，其價格受經濟趨勢、政治事件和自然災害等各種因素影響。基於歷史數據的傳統預測方法在預測方面存在局限性，但自然語言處理的最新進展為基於事件的分析帶來了新的可能性。特別是，語言模型 (LM) 及其進步，生成式預訓練Transformer (GPT)，已顯示出對大量自然語言進行分類的潛力。然而，這些 LM 通常難以處理特定領域的術語，這限制了它們在原油領域的有效性。為了解決這一差距，我們引入了 CrudeBERT，這是一個專門針對原油市場進行微調的 LM。結果表明，CrudeBERT 的情緒分數與 WTI 期貨曲線更為一致，並顯著提高了價格預測，這強調了將經濟原理整合到 LM 中至關重要的作用。

##### **Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels**
2410.12470v1 by Leo Kohlenberg, Leonard Horns, Frederic Sadrieh, Nils Kiele, Matthis Clausen, Konstantin Ketterer, Avetis Navasardyan, Tamara Czinczoll, Gerard de Melo, Ralf Herbrich

Annotating large datasets can be challenging. However, crowd-sourcing is
often expensive and can lack quality, especially for non-trivial tasks. We
propose a method of using LLMs as few-shot learners for annotating data in a
complex natural language task where we learn a standalone model to predict
usage options for products from customer reviews. We also propose a new
evaluation metric for this scenario, HAMS4, that can be used to compare a set
of strings with multiple reference sets. Learning a custom model offers
individual control over energy efficiency and privacy measures compared to
using the LLM directly for the sequence-to-sequence task. We compare this data
annotation approach with other traditional methods and demonstrate how LLMs can
enable considerable cost savings. We find that the quality of the resulting
data exceeds the level attained by third-party vendor services and that
GPT-4-generated labels even reach the level of domain experts. We make the code
and generated labels publicly available.

摘要：標註大型資料集可能具有挑戰性。然而，群眾外包通常很昂貴，而且可能缺乏品質，尤其是對於非平凡的任務。我們提出一種將 LLM 用作少次學習器的方法，用於在複雜的自然語言任務中標註資料，其中我們學習一個獨立的模型來預測客戶評論中產品的使用選項。我們還為此情境提出一個新的評估指標 HAMS4，可用於比較一組字串與多個參考組。與直接將 LLM 用於序列到序列任務相比，學習自訂模型提供對能源效率和隱私措施的個別控制。我們將此資料標註方法與其他傳統方法進行比較，並展示 LLM 如何節省可觀的成本。我們發現，產生的資料品質超過第三方供應商服務達到的水準，而 GPT-4 生成的標籤甚至達到領域專家的水準。我們公開程式碼和產生的標籤。

##### **Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios**
2410.12468v1 by Zhi Chen, Lingxiao Jiang

In recent years, AI-based software engineering has progressed from
pre-trained models to advanced agentic workflows, with Software Development
Agents representing the next major leap. These agents, capable of reasoning,
planning, and interacting with external environments, offer promising solutions
to complex software engineering tasks. However, while much research has
evaluated code generated by large language models (LLMs), comprehensive studies
on agent-generated patches, particularly in real-world settings, are lacking.
This study addresses that gap by evaluating 4,892 patches from 10 top-ranked
agents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on
their impact on code quality. Our analysis shows no single agent dominated,
with 170 issues unresolved, indicating room for improvement. Even for patches
that passed unit tests and resolved issues, agents made different file and
function modifications compared to the gold patches from repository developers,
revealing limitations in the benchmark's test case coverage. Most agents
maintained code reliability and security, avoiding new bugs or vulnerabilities;
while some agents increased code complexity, many reduced code duplication and
minimized code smells. Finally, agents performed better on simpler codebases,
suggesting that breaking complex tasks into smaller sub-tasks could improve
effectiveness. This study provides the first comprehensive evaluation of
agent-generated patches on real-world GitHub issues, offering insights to
advance AI-driven software development.

摘要：在最近几年，基于 AI 的软件工程已从预先训练的模型发展到先进的代理工作流程，其中软件开发代理代表了下一个重大飞跃。这些代理能够推理、规划和与外部环境互动，为复杂的软件工程任务提供了有希望的解决方案。然而，虽然许多研究已经评估了大型语言模型（LLM）生成的代码，但对代理生成的补丁的全面研究，特别是在实际环境中，仍然缺乏。本研究通过评估来自 SWE-Bench Verified 的 500 个实际 GitHub 问题上的 10 个顶级代理的 4,892 个补丁来解决这一差距，重点关注它们对代码质量的影响。我们的分析表明没有一个代理占主导地位，有 170 个问题未解决，表明有改进的空间。即使对于通过单元测试并解决问题的补丁，代理也对文件和函数进行了与存储库开发人员的黄金补丁不同的修改，揭示了基准测试用例覆盖范围的局限性。大多数代理维护了代码的可靠性和安全性，避免了新的错误或漏洞；而一些代理增加了代码复杂性，许多代理减少了代码重复并最大程度地减少了代码异味。最后，代理在更简单的代码库上表现得更好，这表明将复杂的任务分解成更小的子任务可以提高效率。本研究提供了对实际 GitHub 问题上代理生成的补丁的首次全面评估，为推进 AI 驱动的软件开发提供了见解。

##### **Bridging the Language Gaps in Large Language Models with Inference-Time Cross-Lingual Intervention**
2410.12462v1 by Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch

Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing but exhibit significant performance gaps among different
languages. Most existing approaches to address these disparities rely on
pretraining or fine-tuning, which are resource-intensive. To overcome these
limitations without incurring significant costs, we propose Inference-Time
Cross-Lingual Intervention (INCLINE), a novel framework that enhances LLM
performance on low-performing (source) languages by aligning their internal
representations with those of high-performing (target) languages during
inference. INCLINE initially learns alignment matrices using parallel sentences
from source and target languages through a Least-Squares optimization, and then
applies these matrices during inference to transform the low-performing
language representations toward the high-performing language space. Extensive
experiments on nine benchmarks with five LLMs demonstrate that INCLINE
significantly improves performance across diverse tasks and languages, compared
to recent strong baselines. Our analysis demonstrates that INCLINE is highly
cost-effective and applicable to a wide range of applications. In addition, we
release the code to foster research along this line:
https://github.com/weixuan-wang123/INCLINE.

摘要：大型語言模型 (LLM) 在自然語言處理方面表現出非凡的能力，但在不同語言之間的效能差距顯著。現有方法解決這些差異大多依賴於預訓練或微調，而這些方法需要大量資源。為了克服這些限制而又不產生顯著成本，我們提出了推理時間跨語言介入 (INCLINE)，這是一個創新的架構，透過在推理過程中將 LLM 的內部表徵與效能較高的（目標）語言對齊，來增強 LLM 在效能較低的（來源）語言上的效能。INCLINE 最初使用來自來源語言和目標語言的平行句子，透過最小平方最佳化來學習對齊矩陣，然後在推理過程中應用這些矩陣，將效能較低的語言表徵轉換為效能較高的語言空間。在使用五個 LLM 的九個基準上進行的廣泛實驗證明，與最近的強大基準線相比，INCLINE 大幅提升了各種任務和語言的效能。我們的分析證明 INCLINE 非常具有成本效益，且適用於各種應用。此外，我們釋出程式碼，以促進這方面的研究：
https://github.com/weixuan-wang123/INCLINE。

##### **The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph**
2410.12458v1 by Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari

The performance of large language models (LLMs) in natural language
processing (NLP) tasks is significantly influenced by the quality and diversity
of data used for supervised fine-tuning (SFT). Current data selection methods
often focus solely on quality or diversity, leading to underperforming models
due to suboptimal training data. In this paper, we introduce GraphFilter, a
novel method that represents the dataset as a bipartite graph, linking
sentences to their constituent n-grams. This representation effectively
captures the relationships between sentences and linguistic patterns,
facilitating the selection of sentences that enhance n-gram diversity. To
balance quality and diversity during selection, we propose a priority function
that combines the quality metric with the diversity metric in a multiplicative
manner. GraphFilter iteratively selects high-priority sentences, updates the
bipartite graph by removing covered n-grams, and re-calculates priorities to
reflect the evolving data landscape. We conduct extensive experiments using
three model backbones across six widely used benchmarks. The results
demonstrate that GraphFilter outperforms all nine baseline approaches,
achieving superior model performance and computational efficiency. Our analyses
validate the effectiveness of our design choices, examine the subsets selected
by GraphFilter and other methods, highlight the importance of instruction
diversity, and explore the role of quality and diversity in relation to subset
sizes. GraphFilter establishes a new foundation for effective data selection
strategies, encouraging further research in data selection for LLMs.

摘要：大型語言模型 (LLM) 在自然語言處理 (NLP) 任務中的表現，受到用於監督微調 (SFT) 的資料品質和多樣性顯著影響。目前的資料選取方法通常只關注品質或多樣性，導致訓練資料次佳，進而造成模型表現不佳。在本文中，我們介紹 GraphFilter，一種新穎的方法，它將資料集表示為二部圖，將句子連結到其組成 n-gram。這種表示方式有效捕捉句子和語言模式之間的關係，有助於選擇能提升 n-gram 多樣性的句子。為了在選取過程中平衡品質和多樣性，我們提出優先函數，以乘法方式結合品質指標和多樣性指標。GraphFilter 迭代選取高優先級句子，透過移除已涵蓋 n-gram 來更新二部圖，並重新計算優先級以反映不斷變化的資料樣貌。我們使用三個模型主幹在六個廣泛使用的基準上進行廣泛的實驗。結果顯示，GraphFilter 優於所有九種基線方法，達到卓越的模型效能和運算效率。我們的分析驗證了我們設計選擇的有效性，檢驗 GraphFilter 和其他方法選取的子集，強調指令多樣性的重要性，並探討品質和多樣性與子集大小的關係。GraphFilter 為有效的資料選取策略奠定新的基礎，鼓勵進一步研究 LLM 的資料選取。

##### **Sharpness-Aware Black-Box Optimization**
2410.12457v1 by Feiyang Ye, Yueming Lyu, Xuehao Wang, Masashi Sugiyama, Yu Zhang, Ivor Tsang

Black-box optimization algorithms have been widely used in various machine
learning problems, including reinforcement learning and prompt fine-tuning.
However, directly optimizing the training loss value, as commonly done in
existing black-box optimization methods, could lead to suboptimal model quality
and generalization performance. To address those problems in black-box
optimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO)
algorithm, which applies a sharpness-aware minimization strategy to improve the
model generalization. Specifically, the proposed SABO method first
reparameterizes the objective function by its expectation over a Gaussian
distribution. Then it iteratively updates the parameterized distribution by
approximated stochastic gradients of the maximum objective value within a small
neighborhood around the current solution in the Gaussian distribution space.
Theoretically, we prove the convergence rate and generalization bound of the
proposed SABO algorithm. Empirically, extensive experiments on the black-box
prompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO
method in improving model generalization performance.

摘要：黑盒優化演算法已被廣泛用於各種機器學習問題中，包括強化學習和提示微調。
然而，直接優化訓練損失值，就像現有黑盒優化方法中常見的那樣，可能會導致次優模型品質和泛化效能。為了解決黑盒優化中的那些問題，我們提出了一種新穎的敏銳度感知黑盒優化 (SABO) 演算法，它應用一種敏銳度感知最小化策略來改善模型泛化。具體來說，所提出的 SABO 方法首先透過其在高斯分佈上的期望值對目標函數進行重新參數化。然後，它透過高斯分佈空間中目前解附近一個小鄰域內最大目標值的近似隨機梯度，反覆更新參數化分佈。理論上，我們證明了所提出的 SABO 演算法的收斂速度和泛化界限。根據經驗，在黑盒提示微調任務上進行的廣泛實驗證明了所提出的 SABO 方法在改善模型泛化效能方面的有效性。

##### **Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation for Korean LLMs**
2410.12445v1 by Hyeonwoo Kim, Dahyun Kim, Jihoo Kim, Sukyung Lee, Yungi Kim, Chanjun Park

The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean
Large Language Models (LLMs), yet it has certain limitations. Notably, the
disconnect between quantitative improvements on the overly academic leaderboard
benchmarks and the qualitative impact of the models should be addressed.
Furthermore, the benchmark suite is largely composed of translated versions of
their English counterparts, which may not fully capture the intricacies of the
Korean language. To address these issues, we propose Open Ko-LLM Leaderboard2,
an improved version of the earlier Open Ko-LLM Leaderboard. The original
benchmarks are entirely replaced with new tasks that are more closely aligned
with real-world capabilities. Additionally, four new native Korean benchmarks
are introduced to better reflect the distinct characteristics of the Korean
language. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide
a more meaningful evaluation for advancing Korean LLMs.

摘要：開放式 Ko-LLM 排行榜對於基準測試韓文大型語言模型 (LLM) 發揮了重要作用，但仍有某些限制。特別是，過於學術的排行榜基準測試在量化改進與模型的質化影響之間的脫節應該得到解決。此外，基準測試套件主要由其英文對應版本的翻譯組成，可能無法完全捕捉韓語的複雜性。為了解決這些問題，我們提出了開放式 Ko-LLM 排行榜 2，這是早期開放式 Ko-LLM 排行榜的改良版本。原始基準測試完全替換為與實際能力更緊密結合的新任務。此外，還引入了四個新的原生韓語基準測試，以更好地反映韓語的獨特特徵。透過這些改進，開放式 Ko-LLM 排行榜 2 旨在為提升韓語 LLM 提供更有意義的評估。

##### **Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar Question Generation Using Large Language Models**
2410.12444v1 by Mengze Hong, Yuanfeng Song, Di Jiang, Lu Wang, Zichang Guo, Chen Jason Zhang

Reliable responses of service chatbots are often achieved by employing
retrieval-based methods that restrict answers to a knowledge base comprising
predefined question-answer pairs (QA pairs). To accommodate potential
variations in how a customer's query may be expressed, it emerges as the
favored solution to augment these QA pairs with similar questions that are
possibly diverse while remaining semantic consistency. This augmentation task
is known as Similar Question Generation (SQG). Traditional methods that heavily
rely on human efforts or rule-based techniques suffer from limited diversity or
significant semantic deviation from the source question, only capable of
producing a finite number of useful questions.
  To address these limitations, we propose an SQG approach based on Large
Language Models (LLMs), capable of producing a substantial number of diverse
questions while maintaining semantic consistency to the source QA pair. This is
achieved by leveraging LLMs' natural language understanding capability through
fine-tuning with specially designed prompts. The experiments conducted on a
real customer-service dataset demonstrate that our method surpasses baseline
methods by a significant margin in terms of semantic diversity. Human
evaluation further confirms that integrating the answer that reflects the
customer's intention is crucial for increasing the number of generated
questions that meet business requirements.

摘要：服務聊天機器人的可靠回應通常透過採用檢索式方法來達成，此方法將答案限制在包含預先定義的問題解答對（問答對）的知識庫中。為了適應客戶查詢可能表達的方式的潛在變異，擴充這些問答對以納入在語意一致性的前提下可能不同的類似問題，逐漸成為受到青睞的解決方案。此擴充任務稱為類似問題產生（SQG）。傳統方法過度依賴人力或基於規則的技術，會造成多樣性受限或與原始問題產生顯著的語意偏差，僅能產生有限數量的有用問題。
為了解決這些限制，我們提出一個基於大型語言模型（LLM）的 SQG 方法，此方法能夠產生大量多樣化的問題，同時維持與原始問答對的語意一致性。這透過利用 LLM 的自然語言理解能力，並透過特別設計的提示進行微調來達成。在實際的客戶服務資料集上進行的實驗顯示，我們的模型在語意多樣性方面大幅超越基線模型。人工評估進一步證實，整合反映客戶意圖的答案對於增加符合業務需求的產生問題數量至關重要。

##### **Reconstruction of Differentially Private Text Sanitization via Large Language Models**
2410.12443v1 by Shuchao Pang, Zhigang Lu, Haichen Wang, Peng Fu, Yongbin Zhou, Minhui Xue, Bo Li

Differential privacy (DP) is the de facto privacy standard against privacy
leakage attacks, including many recently discovered ones against large language
models (LLMs). However, we discovered that LLMs could reconstruct the
altered/removed privacy from given DP-sanitized prompts. We propose two attacks
(black-box and white-box) based on the accessibility to LLMs and show that LLMs
could connect the pair of DP-sanitized text and the corresponding private
training data of LLMs by giving sample text pairs as instructions (in the
black-box attacks) or fine-tuning data (in the white-box attacks). To
illustrate our findings, we conduct comprehensive experiments on modern LLMs
(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,
Claude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used
datasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and
sentence-level DP. The experimental results show promising recovery rates,
e.g., the black-box attacks against the word-level DP over WikiMIA dataset gave
72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on
ChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study
indicates that these well-known LLMs have emerged as a new security risk for
existing DP text sanitization approaches in the current environment.

摘要：差分隱私 (DP) 是針對隱私外洩攻擊的事實隱私標準，包括最近針對大型語言模型 (LLM) 發現的許多攻擊。然而，我們發現 LLM 可以從給定的 DP 清理提示中重建已變更/移除的隱私。我們提出了兩種攻擊（黑盒和白盒），基於對 LLM 的可訪問性，並展示 LLM 可以通過提供範例文字對作為指令（在黑盒攻擊中）或微調資料（在白盒攻擊中）來連接 DP 清理文字和對應的 LLM 私人訓練資料。為了說明我們的發現，我們對現代 LLM（例如 LLaMA-2、LLaMA-3、ChatGPT-3.5、ChatGPT-4、ChatGPT-4o、Claude-3、Claude-3.5、OPT、GPT-Neo、GPT-J、Gemma-2 和 Pythia）進行了全面的實驗，使用常用的資料集（例如 WikiMIA、Pile-CC 和 Pile-Wiki）針對字元級和句子級 DP。實驗結果顯示出有希望的恢復率，例如，針對 WikiMIA 資料集的字元級 DP 的黑盒攻擊在 LLaMA-2 (70B) 上獲得 72.18%，在 LLaMA-3 (70B) 上獲得 82.39%，在 Gemma-2 上獲得 75.35%，在 ChatGPT-4o 上獲得 91.2%，在 Claude-3.5 (Sonnet) 上獲得 94.01%。更緊急的是，這項研究表明，這些眾所周知的 LLM 已成為當前環境中現有 DP 文字清理方法的新安全風險。

##### **Conformity in Large Language Models**
2410.12428v1 by Xiaochen Zhu, Caiqi Zhang, Tom Stafford, Nigel Collier, Andreas Vlachos

The conformity effect describes the tendency of individuals to align their
responses with the majority. Studying this bias in large language models (LLMs)
is crucial, as LLMs are increasingly used in various information-seeking and
decision-making tasks as conversation partners to improve productivity. Thus,
conformity to incorrect responses can compromise their effectiveness. In this
paper, we adapt psychological experiments to examine the extent of conformity
in state-of-the-art LLMs. Our findings reveal that all models tested exhibit
varying levels of conformity toward the majority, regardless of their initial
choice or correctness, across different knowledge domains. Notably, we are the
first to show that LLMs are more likely to conform when they are more uncertain
in their own prediction. We further explore factors that influence conformity,
such as training paradigms and input characteristics, finding that
instruction-tuned models are less susceptible to conformity, while increasing
the naturalness of majority tones amplifies conformity. Finally, we propose two
interventions--Devil's Advocate and Question Distillation--to mitigate
conformity, providing insights into building more robust language models.

摘要：從眾效應描述了個人將自己的反應與多數人保持一致的傾向。在大型語言模型（LLM）中研究這種偏見至關重要，因為 LLM 正逐漸用於各種資訊搜尋和決策制定任務中，作為對話夥伴以提高生產力。因此，對不正確反應的從眾效應會損害它們的有效性。在本文中，我們調整了心理實驗以檢查最先進的 LLM 中從眾效應的程度。我們的研究結果顯示，所有測試的模型都表現出不同程度的從眾效應，無論它們的初始選擇或正確性如何，以及在不同的知識領域中。值得注意的是，我們首次表明，當 LLM 對自己的預測不確定時，它們更有可能從眾。我們進一步探討了影響從眾效應的因素，例如訓練範例和輸入特徵，發現經過指令調整的模型不易受到從眾效應的影響，而增加多數語氣的自然性會放大從眾效應。最後，我們提出了兩種干預措施——魔鬼辯護人和問題提煉——以減輕從眾效應，並提供深入見解以建構更強大的語言模型。

##### **Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding**
2410.12413v1 by Daichi Hayakawa, Issei Sato

In this study, we provide constructive proof that Transformers can recognize
and generate hierarchical language efficiently with respect to model size, even
without the need for a specific positional encoding. Specifically, we show that
causal masking and a starting token enable Transformers to compute positional
information and depth within hierarchical structures. We demonstrate that
Transformers without positional encoding can generate hierarchical languages.
Furthermore, we suggest that explicit positional encoding might have a
detrimental effect on generalization with respect to sequence length.

摘要：在這項研究中，我們提供了建構性的證明，證明 Transformer 可以有效地識別和產生階層語言，即使不需要特定位置編碼。具體來說，我們展示了因果遮罩和起始標記使 Transformer 能夠計算階層結構中的位置資訊和深度。我們證明了沒有位置編碼的 Transformer 可以產生階層語言。此外，我們建議明確的位置編碼可能會對序列長度的一般化產生不利影響。

##### **Revealing the Barriers of Language Agents in Planning**
2410.12409v1 by Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao

Autonomous planning has been an ongoing pursuit since the inception of
artificial intelligence. Based on curated problem solvers, early planning
agents could deliver precise solutions for specific tasks but lacked
generalization. The emergence of large language models (LLMs) and their
powerful reasoning capabilities has reignited interest in autonomous planning
by automatically generating reasonable solutions for given tasks. However,
prior research and our experiments show that current language agents still lack
human-level planning abilities. Even the state-of-the-art reasoning model,
OpenAI o1, achieves only 15.6% on one of the complex real-world planning
benchmarks. This highlights a critical question: What hinders language agents
from achieving human-level planning? Although existing studies have highlighted
weak performance in agent planning, the deeper underlying issues and the
mechanisms and limitations of the strategies proposed to address them remain
insufficiently understood. In this work, we apply the feature attribution study
and identify two key factors that hinder agent planning: the limited role of
constraints and the diminishing influence of questions. We also find that
although current strategies help mitigate these challenges, they do not fully
resolve them, indicating that agents still have a long way to go before
reaching human-level intelligence.

摘要：自主規劃自人工智慧創立以來一直是持續追求的目標。基於精選的問題解決者，早期的規劃代理可以提供特定任務的精確解決方案，但缺乏概括性。大型語言模型 (LLM) 和其強大的推理能力的出現，重新點燃了對自主規劃的興趣，透過自動為給定的任務產生合理的解決方案。然而，先前的研究和我們的實驗顯示，當前的語言代理仍然缺乏人類層級的規劃能力。即使是目前最先進的推理模型 OpenAI o1，在一個複雜的真實世界規劃基準中也只達到 15.6%。這突顯了一個關鍵問題：是什麼阻礙語言代理達到人類層級的規劃？儘管現有研究強調了代理規劃中的弱項，但更深層的根本問題以及為了解決這些問題而提出的策略機制和限制，仍然了解不足。在這項工作中，我們應用特徵歸因研究，並找出阻礙代理規劃的兩個關鍵因素：約束的有限作用和問題的影響遞減。我們也發現，儘管當前的策略有助於緩解這些挑戰，但並未完全解決它們，這表示代理在達到人類層級的智慧之前，仍有很長一段路要走。

##### **Beyond Coarse-Grained Matching in Video-Text Retrieval**
2410.12407v1 by Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek

Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.

摘要：影片文字擷取已取得顯著進展，然而，模型辨別字幕中細微差異的能力仍需要驗證。在本文中，我們介紹一種新的細粒度評估方法。我們的做法可應用於現有資料集，透過自動產生具有名詞、動詞、形容詞、副詞和介系詞等單字細微變化的困難負面測試字幕。我們使用四種最先進的模型對兩個標準基準（MSR-VTT 和 VATEX）和兩個特別策展且豐富詳細說明的資料集（VLN-UVO 和 VLN-OOPS）進行全面實驗，產生許多新見解：1）我們的分析顯示，目前的評估基準無法偵測模型感知單字細微差異的能力，2）我們的細粒度評估強調模型在區分此類細微變化的困難。為了增強細粒度理解，我們提出一個新的基準，可以輕鬆地與目前的方法結合。針對我們細粒度評估的實驗證明，這種方法增強了模型理解細粒度差異的能力。

##### **ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs**
2410.12405v1 by Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen

Large language models (LLMs) have demonstrated impressive capabilities across
various tasks, but their performance is highly sensitive to the prompts
utilized. This variability poses challenges for accurate assessment and user
satisfaction. Current research frequently overlooks instance-level prompt
variations and their implications on subjective evaluations. To address these
shortcomings, we introduce ProSA, a framework designed to evaluate and
comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity
metric, PromptSensiScore, and leverages decoding confidence to elucidate
underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers
that prompt sensitivity fluctuates across datasets and models, with larger
models exhibiting enhanced robustness. We observe that few-shot examples can
alleviate this sensitivity issue, and subjective evaluations are also
susceptible to prompt sensitivities, particularly in complex,
reasoning-oriented tasks. Furthermore, our findings indicate that higher model
confidence correlates with increased prompt robustness. We believe this work
will serve as a helpful tool in studying prompt sensitivity of LLMs. The
project is released at: https://github.com/open-compass/ProSA .

摘要：大型語言模型 (LLM) 已在各種任務中展現出令人印象深刻的能力，但其效能對所使用的提示非常敏感。這種變異性對準確評估和使用者滿意度構成挑戰。目前的研究所常忽略個別提示變異及其對主觀評估的影響。為了解決這些缺點，我們引入了 ProSA，一個旨在評估和理解 LLM 中提示敏感度的架構。ProSA 結合了一種新穎的敏感度量度 PromptSensiScore，並利用解碼信心來闡明底層機制。我們廣泛的研究涵蓋多項任務，發現提示敏感度在資料集和模型之間有所波動，而較大的模型表現出增強的穩健性。我們觀察到少數範例可以緩解這種敏感度問題，而主觀評估也容易受到提示敏感度的影響，特別是在複雜的、以推理為導向的任務中。此外，我們的研究結果表明，較高的模型信心與增加的提示穩健性相關。我們相信這項工作將成為研究 LLM 提示敏感度的有用工具。該專案已發布於：https://github.com/open-compass/ProSA。

##### **Tracking Universal Features Through Fine-Tuning and Model Merging**
2410.12391v1 by Niels Horn, Desmond Elliott

We study how features emerge, disappear, and persist across models fine-tuned
on different domains of text. More specifically, we start from a base one-layer
Transformer language model that is trained on a combination of the BabyLM
corpus, and a collection of Python code from The Stack. This base model is
adapted to two new domains of text: TinyStories, and the Lua programming
language, respectively; and then these two models are merged using these two
models using spherical linear interpolation. Our exploration aims to provide
deeper insights into the stability and transformation of features across
typical transfer-learning scenarios using small-scale models and sparse
auto-encoders.

摘要：我們研究特徵如何出現在不同文本領域上進行微調的模型中、消失和持續存在。更具體地說，我們從一個基本的一層 Transformer 語言模型開始，該模型在 BabyLM 語料庫的組合和 The Stack 中的 Python 程式碼集合上進行訓練。這個基本模型適應了兩個新的文本領域：TinyStories 和 Lua 程式語言；然後使用球面線性插值將這兩個模型合併。我們的探索旨在提供更深入的見解，了解在使用小規模模型和稀疏自動編碼器進行典型的遷移學習場景時，特徵的穩定性和轉換。

##### **A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetic**
2410.12389v1 by Lennert De Smet, Pedro Zuidberg Dos Martires

As illustrated by the success of integer linear programming, linear integer
arithmetic is a powerful tool for modelling combinatorial problems.
Furthermore, the probabilistic extension of linear programming has been used to
formulate problems in neurosymbolic AI. However, two key problems persist that
prevent the adoption of neurosymbolic techniques beyond toy problems. First,
probabilistic inference is inherently hard, #P-hard to be precise. Second, the
discrete nature of integers renders the construction of meaningful gradients
challenging, which is problematic for learning. In order to mitigate these
issues, we formulate linear arithmetic over integer-valued random variables as
tensor manipulations that can be implemented in a straightforward fashion using
modern deep learning libraries. At the core of our formulation lies the
observation that the addition of two integer-valued random variables can be
performed by adapting the fast Fourier transform to probabilities in the
log-domain. By relying on tensor operations we obtain a differentiable data
structure, which unlocks, virtually for free, gradient-based learning. In our
experimental validation we show that tensorising probabilistic linear integer
arithmetic and leveraging the fast Fourier transform allows us to push the
state of the art by several orders of magnitude in terms of inference and
learning times.

摘要：正如整数线性规划的成功所展示的那样，线性整数算术是建模组合问题的强大工具。此外，线性规划的概率扩展已被用于制定神经符号 AI 中的问题。然而，仍然存在两个关键问题，阻止了神经符号技术在玩具问题之外的应用。首先，概率推理本质上是困难的，准确地说，#P-hard。其次，整数的离散性质使得有意义的梯度的构建具有挑战性，这对学习来说是有问题的。为了缓解这些问题，我们将基于整数随机变量的线性算术公式化为张量操作，可以使用现代深度学习库以直接的方式实现。我们公式化的核心在于观察到，两个整数随机变量的加法可以通过将快速傅里叶变换调整为对数域中的概率来执行。通过依赖张量运算，我们获得了可微的数据结构，它几乎免费地解锁了基于梯度的学习。在我们的实验验证中，我们表明张量化概率线性整数算术并利用快速傅里叶变换使我们能够在推理和学习时间方面将最先进的技术提升几个数量级。

##### **Prompt Compression for Large Language Models: A Survey**
2410.12388v1 by Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier

Leveraging large language models (LLMs) for complex natural language tasks
typically requires long-form prompts to convey detailed requirements and
information, which results in increased memory usage and inference costs. To
mitigate these challenges, multiple efficient methods have been proposed, with
prompt compression gaining significant research interest. This survey provides
an overview of prompt compression techniques, categorized into hard prompt
methods and soft prompt methods. First, the technical approaches of these
methods are compared, followed by an exploration of various ways to understand
their mechanisms, including the perspectives of attention optimization,
Parameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic
language. We also examine the downstream adaptations of various prompt
compression techniques. Finally, the limitations of current prompt compression
methods are analyzed, and several future directions are outlined, such as
optimizing the compression encoder, combining hard and soft prompts methods,
and leveraging insights from multimodality.

摘要：利用大型語言模型 (LLM) 處理複雜的自然語言任務通常需要長篇提示來傳達詳細的要求和資訊，這會增加記憶體使用量和推論成本。為了減輕這些挑戰，已提出多種有效的方法，其中提示壓縮獲得了顯著的研究興趣。本調查提供了提示壓縮技術的概述，分為硬提示方法和軟提示方法。首先，比較這些方法的技術方法，然後探討理解其機制的各種方法，包括關注最佳化、參數有效微調 (PEFT)、模態融合和新的合成語言。我們還檢查了各種提示壓縮技術的下游適應。最後，分析了當前提示壓縮方法的限制，並概述了幾個未來的方向，例如最佳化壓縮編碼器、結合硬提示和軟提示方法，以及利用多模態的見解。

##### **HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks**
2410.12381v1 by Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung

Coding tasks have been valuable for evaluating Large Language Models (LLMs),
as they demand the comprehension of high-level instructions, complex reasoning,
and the implementation of functional programs -- core capabilities for
advancing Artificial General Intelligence. Despite the progress in Large
Multimodal Models (LMMs), which extend LLMs with visual perception and
understanding capabilities, there remains a notable lack of coding benchmarks
that rigorously assess these models, particularly in tasks that emphasize
visual reasoning. To address this gap, we introduce HumanEval-V, a novel and
lightweight benchmark specifically designed to evaluate LMMs' visual
understanding and reasoning capabilities through code generation. HumanEval-V
includes 108 carefully crafted, entry-level Python coding tasks derived from
platforms like CodeForces and Stack Overflow. Each task is adapted by modifying
the context and algorithmic patterns of the original problems, with visual
elements redrawn to ensure distinction from the source, preventing potential
data leakage. LMMs are required to complete the code solution based on the
provided visual context and a predefined Python function signature outlining
the task requirements. Every task is equipped with meticulously handcrafted
test cases to ensure a thorough and reliable evaluation of model-generated
solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering
significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1
and 36.4% pass@10, while open-weight models with 70B parameters score below 4%
pass@1. Ablation studies further reveal the limitations of current LMMs in
vision reasoning and coding capabilities. These results underscore key areas
for future research to enhance LMMs' capabilities. We have open-sourced our
code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.

摘要：<paragraph>編碼任務對於評估大型語言模型 (LLM) 很有價值，
因為它們需要理解高層級指令、複雜推理，
以及實作函式程式 -- 人工通用智慧進步的核心能力。儘管大型多模態模型 (LMM) 有進展，它擴充了 LLM 的視覺感知和理解能力，但仍然顯著缺乏編碼基準，嚴格評估這些模型，特別是在強調視覺推理的任務中。為了填補這個缺口，我們引入了 HumanEval-V，一個新穎且輕量化的基準，專門設計用於透過程式碼產生來評估 LMM 的視覺理解和推理能力。HumanEval-V 包含 108 個精心製作的入門級 Python 編碼任務，衍生自 CodeForces 和 Stack Overflow 等平台。每個任務透過修改原始問題的背景和演算法模式來改編，並重新繪製視覺元素以確保與來源區分，防止潛在資料外洩。LMM 需要根據提供的視覺背景和預先定義的 Python 函式簽章來完成程式碼解決方案，概述任務需求。每個任務都配備了精心製作的測試案例，以確保對模型產生的解決方案進行徹底且可靠的評估。我們使用 HumanEval-V 評估了 19 個最先進的 LMM，發現了重大挑戰。GPT-4o 等專有模型僅達到 13% 的 pass@1 和 36.4% 的 pass@10，而具有 70B 參數的開放權重模型得分低於 4% 的 pass@1。消融研究進一步揭示了當前 LMM 在視覺推理和編碼能力方面的限制。這些結果強調了未來研究以增強 LMM 能力的主要領域。我們已在 https://github.com/HumanEval-V/HumanEval-V-Benchmark 開放我們的程式碼和基準。</paragraph>

##### **Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models**
2410.12380v1 by Amin Abolghasemi, Leif Azzopardi, Seyyed Hadi Hashemi, Maarten de Rijke, Suzan Verberne

Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.

摘要：<paragraph>在檢索擴充生成（RAG）中，將答案歸因於來源文件是一種用於增強模型輸出可驗證性的方法。
先前的工作主要集中於改進和評估 RAG 中大型語言模型（LLM）的歸因品質，但這可能會導致在答案的歸因中引發偏差。我們在 RAG 管線中定義並檢視了 LLM 評估的兩個面向，即歸因敏感度和關於作者資訊的偏差。我們明確地告知 LLM 來源文件的作者，指示它歸因其答案，並分析 (i) LLM 的輸出對來源文件的作者有多敏感，以及 (ii) LLM 是否對人類撰寫或 AI 生成的來源文件表現出偏差。我們設計了一個實驗設定，在其中我們使用反事實評估來研究三個 LLM 在 RAG 管線中的歸因敏感度和偏差。我們的結果顯示，將作者資訊新增到來源文件可以顯著改變 LLM 的歸因品質，介於 3% 到 18%。此外，我們顯示 LLM 可能對明確的人類作者身份有歸因偏差，這可以用作先前工作發現的競爭假設，該發現顯示 LLM 生成的內容可能比人類撰寫的內容更受青睞。我們的發現表明，來源文件的元資料會影響 LLM 的信任，以及它們如何歸因其答案。此外，我們的研究強調歸因偏差和敏感度是 LLM 中脆性的新面向。</paragraph>

##### **HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims**
2410.12377v1 by Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park

To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a
system that only employs publicly available large language models (LLMs) for
each step of automated fact-checking, dubbed the Herd of Open LLMs for
verifying real-world claims (HerO). HerO employs multiple LLMs for each step of
automated fact-checking. For evidence retrieval, a language model is used to
enhance a query by generating hypothetical fact-checking documents. We prompt
pretrained and fine-tuned LLMs for question generation and veracity prediction
by crafting prompts with retrieved in-context samples. HerO achieved 2nd place
on the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of
open LLMs for verifying real-world claims. For future research, we make our
code publicly available at https://github.com/ssu-humane/HerO.

摘要：為了應對由 FEVER-24 主辦的 AVeriTeC 共享任務，我們引入了一個系統，它只在自動查證的每一步中採用公開可用的大型語言模型 (LLM)，並將其稱為用於驗證真實世界聲明的開放 LLM 群集 (HerO)。HerO 在自動查證的每一步中採用多個 LLM。對於證據檢索，語言模型用於通過生成假設性的查證文件來增強查詢。我們通過使用檢索到的上下文範本製作提示，提示預訓練和微調的 LLM 進行問題生成和真實性預測。HerO 在排行榜上獲得第 2 名，AVeriTeC 得分為 0.57，這表明開放式 LLM 在驗證真實世界聲明方面具有潛力。對於未來的研究，我們在 https://github.com/ssu-humane/HerO 上公開了我們的代碼。

##### **ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing**
2410.12376v1 by Qingming Lin, Rui Hu, Huaxia Li, Sensen Wu, Yadong Li, Kai Fang, Hailin Feng, Zhenhong Du, Liuchang Xu

Vector data is one of the two core data structures in geographic information
science (GIS), essential for accurately storing and representing geospatial
information. Shapefile, the most widely used vector data format, has become the
industry standard supported by all major geographic information systems.
However, processing this data typically requires specialized GIS knowledge and
skills, creating a barrier for researchers from other fields and impeding
interdisciplinary research in spatial data analysis. Moreover, while large
language models (LLMs) have made significant advancements in natural language
processing and task automation, they still face challenges in handling the
complex spatial and topological relationships inherent in GIS vector data. To
address these challenges, we propose ShapefileGPT, an innovative framework
powered by LLMs, specifically designed to automate Shapefile tasks.
ShapefileGPT utilizes a multi-agent architecture, in which the planner agent is
responsible for task decomposition and supervision, while the worker agent
executes the tasks. We developed a specialized function library for handling
Shapefiles and provided comprehensive API documentation, enabling the worker
agent to operate Shapefiles efficiently through function calling. For
evaluation, we developed a benchmark dataset based on authoritative textbooks,
encompassing tasks in categories such as geometric operations and spatial
queries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the
GPT series models. In comparison to traditional LLMs, ShapefileGPT effectively
handles complex vector data analysis tasks, overcoming the limitations of
traditional LLMs in spatial analysis. This breakthrough opens new pathways for
advancing automation and intelligence in the GIS field, with significant
potential in interdisciplinary data analysis and application contexts.

摘要：向量資料是地理資訊科學 (GIS) 中的兩個核心資料結構之一，對於準確儲存和表示地理空間資訊至關重要。Shapefile 是使用最廣泛的向量資料格式，已成為所有主要地理資訊系統支援的產業標準。然而，處理這些資料通常需要專業的 GIS 知識和技能，為來自其他領域的研究人員製造障礙，並阻礙空間資料分析的跨領域研究。此外，儘管大型語言模型 (LLM) 在自然語言處理和任務自動化方面取得重大進展，但在處理 GIS 向量資料中固有的複雜空間和拓撲關係時仍面臨挑戰。為了應對這些挑戰，我們提出了 ShapefileGPT，這是一個由 LLM 驅動的創新框架，專門設計用於自動化 Shapefile 任務。ShapefileGPT 使用多代理架構，其中規劃代理負責任務分解和監督，而工作代理則執行任務。我們開發了一個專門的功能函式庫來處理 Shapefile，並提供了全面的 API 文件，使工作代理能夠透過函式呼叫有效地操作 Shapefile。為了進行評估，我們根據權威教科書開發了一個基準資料集，涵蓋幾何運算和空間查詢等類別中的任務。ShapefileGPT 的任務成功率達到 95.24%，優於 GPT 系列模型。與傳統 LLM 相比，ShapefileGPT 能有效處理複雜的向量資料分析任務，克服了傳統 LLM 在空間分析中的限制。這項突破為推動 GIS 領域的自動化和智慧化開闢了新途徑，在跨領域資料分析和應用情境中具有顯著的潛力。

##### **PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking**
2410.12375v1 by Markus J. Buehler

PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.

摘要：PRefLexOR（用於探索性推理優化的基於偏好的遞迴語言建模）將偏好優化與強化學習中的概念相結合，使模型能夠通過反覆推理改進來自我教學。我們提出了一種遞迴學習方法，讓模型參與多步驟推理、重新審視和改進中間步驟，然後在訓練和推理階段產生最終輸出。通過多個訓練階段，模型首先學習通過優化首選和非首選響應之間的對數幾率，使其推理與準確的決策路徑保持一致。在此過程中，PRefLexOR 通過從隨機文本塊生成問題和檢索增強來構建一個動態知識圖，從整個訓練語料庫中提取相關細節以進行語境化。在第二階段，偏好優化通過使用拒絕採樣來微調推理質量，從而增強模型性能，同時連續產生原位訓練數據，同時掩蓋推理步驟。在思考令牌框架內進行遞迴優化會引入迭代反饋迴路，其中模型會改進推理，從而實現更深入的連貫性、一致性和適應性。在只有 30 億個參數的小語言模型中實現，我們應該讓即使是很小的模型也能通過迭代的方式教會自己以更大的深度和反思能力進行推理。我們的實現非常直接，可以整合到任何現有的預訓練 LLM 中。我們將我們的示例重點放在生物材料科學應用上，並在從域內到跨域應用等各種案例研究中演示了該方法。使用包括思考和反思模式在內的推理策略，我們構建了一個多代理遞迴自我改進推理方法，以通過在推理時間重複採樣來連續改進響應。

##### **Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance**
2410.12361v1 by Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, Weiwen Liu, Yasheng Wang, Zhiyuan Liu, Fangming Liu, Maosong Sun

Agents powered by large language models have shown remarkable abilities in
solving complex tasks. However, most agent systems remain reactive, limiting
their effectiveness in scenarios requiring foresight and autonomous
decision-making. In this paper, we tackle the challenge of developing proactive
agents capable of anticipating and initiating tasks without explicit human
instructions. We propose a novel data-driven approach for this problem.
Firstly, we collect real-world human activities to generate proactive task
predictions. These predictions are then labeled by human annotators as either
accepted or rejected. The labeled data is used to train a reward model that
simulates human judgment and serves as an automatic evaluator of the
proactiveness of LLM agents. Building on this, we develop a comprehensive data
generation pipeline to create a diverse dataset, ProactiveBench, containing
6,790 events. Finally, we demonstrate that fine-tuning models with the proposed
ProactiveBench can significantly elicit the proactiveness of LLM agents.
Experimental results show that our fine-tuned model achieves an F1-Score of
66.47% in proactively offering assistance, outperforming all open-source and
close-source models. These results highlight the potential of our method in
creating more proactive and effective agent systems, paving the way for future
advancements in human-agent collaboration.

摘要：<paragraph>由大型語言模型驅動的代理已在解決複雜任務方面展現出非凡的能力。然而，大多數代理系統仍然被動，這限制了它們在需要預見性和自主決策制定場景中的效能。在本文中，我們將應對開發主動代理的挑戰，這些代理能夠預期並啟動任務，而無需明確的人類指令。我們針對此問題提出了一種新穎的數據驅動方法。首先，我們收集真實世界的人類活動以生成主動任務預測。然後，這些預測由人類註解者標記為接受或拒絕。標記的數據用於訓練一個獎勵模型，該模型模擬人類判斷，並作為 LLM 代理主動性的自動評估器。在此基礎上，我們開發了一個全面的數據生成管道，以創建一個多樣化的數據集 ProactiveBench，其中包含 6,790 個事件。最後，我們證明使用所提出的 ProactiveBench 微調模型可以顯著引發 LLM 代理的積極性。實驗結果表明，我們微調後的模型在主動提供協助方面達到了 66.47% 的 F1 分數，優於所有開源和閉源模型。這些結果突出了我們的方法在創造更主動和有效的代理系統方面的潛力，為未來人機協作的進步鋪平了道路。</paragraph>

##### **Towards Neural Scaling Laws for Time Series Foundation Models**
2410.12360v1 by Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin, Shirui Pan

Scaling laws offer valuable insights into the design of time series
foundation models (TSFMs). However, previous research has largely focused on
the scaling laws of TSFMs for in-distribution (ID) data, leaving their
out-of-distribution (OOD) scaling behavior and the influence of model
architectures less explored. In this work, we examine two common TSFM
architectures, encoder-only and decoder-only Transformers, and investigate
their scaling behavior on both ID and OOD data. These models are trained and
evaluated across varying parameter counts, compute budgets, and dataset sizes.
Our experiments reveal that the log-likelihood loss of TSFMs exhibits similar
scaling behavior in both OOD and ID settings. We further compare the scaling
properties across different architectures, incorporating two state-of-the-art
TSFMs as case studies, showing that model architecture plays a significant role
in scaling. The encoder-only Transformers demonstrate better scalability than
the decoder-only Transformers, while the architectural enhancements in the two
advanced TSFMs primarily improve ID performance but reduce OOD scalability.
While scaling up TSFMs is expected to drive performance breakthroughs, the lack
of a comprehensive understanding of TSFM scaling laws has hindered the
development of a robust framework to guide model scaling. We fill this gap in
this work by synthesizing our findings and providing practical guidelines for
designing and scaling larger TSFMs with enhanced model capabilities.

摘要：<paragraph>規模定律為時序基礎模型 (TSFM) 的設計提供了有價值的見解。然而，先前的研究主要集中在 TSFM 的分佈內 (ID) 資料的規模定律，較少探討它們的分佈外 (OOD) 規模行為和模型架構的影響。在這項工作中，我們檢驗了兩種常見的 TSFM 架構，僅編碼器和僅解碼器 Transformer，並研究它們在 ID 和 OOD 資料上的規模行為。這些模型經過訓練並在不同的參數數量、運算預算和資料集大小中進行評估。我們的實驗表明，TSFM 的對數似然損失在 OOD 和 ID 設定中都表現出類似的規模行為。我們進一步比較了不同架構的規模屬性，將兩個最先進的 TSFM 作為案例研究，表明模型架構在規模中扮演著重要的角色。僅編碼器 Transformer 比僅解碼器 Transformer 具有更好的可擴充性，而兩個進階 TSFM 中的架構增強主要改善了 ID 效能，但降低了 OOD 可擴充性。儘管擴充 TSFM 預計將推動效能突破，但缺乏對 TSFM 規模定律的全面了解阻礙了制定一個健全的架構來指導模型規模。我們透過綜合我們的發現並提供實用的指南，來填補這項工作中的空白，以設計和擴充具有增強模型功能的更大 TSFM。</paragraph>

##### **GECTurk WEB: An Explainable Online Platform for Turkish Grammatical Error Detection and Correction**
2410.12350v1 by Ali Gebeşçe, Gözde Gül Şahin

Sophisticated grammatical error detection/correction tools are available for
a small set of languages such as English and Chinese. However, it is not
straightforward -- if not impossible -- to adapt them to morphologically rich
languages with complex writing rules like Turkish which has more than 80
million speakers. Even though several tools exist for Turkish, they primarily
focus on spelling errors rather than grammatical errors and lack features such
as web interfaces, error explanations and feedback mechanisms. To fill this
gap, we introduce GECTurk WEB, a light, open-source, and flexible web-based
system that can detect and correct the most common forms of Turkish writing
errors, such as the misuse of diacritics, compound and foreign words, pronouns,
light verbs along with spelling mistakes. Our system provides native speakers
and second language learners an easily accessible tool to detect/correct such
mistakes and also to learn from their mistakes by showing the explanation for
the violated rule(s). The proposed system achieves 88,3 system usability score,
and is shown to help learn/remember a grammatical rule (confirmed by 80% of the
participants). The GECTurk WEB is available both as an offline tool at
https://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.

摘要：對於少數語言（例如英文和中文），有提供精密的文法錯誤偵測/修正工具。然而，要將這些工具套用在形態豐富且書寫規則複雜的語言（例如有超過 8000 萬名使用者土耳其語）並不容易，甚至是不可能的。儘管有幾種工具可供土耳其語使用，但它們主要著重於拼寫錯誤，而非文法錯誤，而且缺乏網路介面、錯誤說明和回饋機制等功能。為了填補這個缺口，我們推出了 GECTurk WEB，這是一個輕量、開放原始碼且彈性的網路系統，可以偵測並修正最常見的土耳其語寫作錯誤，例如變音符號、複合字、外來語、代名詞、輕動詞的誤用，以及拼寫錯誤。我們的系統為母語人士和第二語言學習者提供了一個容易取得的工具，可以偵測/修正這些錯誤，並透過顯示違規規則的說明，從他們的錯誤中學習。建議的系統達到了 88.3 的系統可用性評分，並顯示有助於學習/記住文法規則（80% 的參與者證實）。GECTurk WEB 可作為離線工具在 https://github.com/GGLAB-KU/gecturkweb 取得，或在 www.gecturk.net 線上取得。

##### **TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant**
2410.12342v1 by Guopeng Li, Qiang Wang, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia

Most knowledge distillation (KD) methodologies predominantly focus on
teacher-student pairs with similar architectures, such as both being
convolutional neural networks (CNNs). However, the potential and flexibility of
KD can be greatly improved by expanding it to novel Cross-Architecture KD
(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be
transferred flexibly to a given student. The primary challenge in CAKD lies in
the substantial feature gaps between heterogeneous models, originating from the
distinction of their inherent inductive biases and module functions. To this
end, we introduce an assistant model as a bridge to facilitate smooth feature
knowledge transfer between heterogeneous teachers and students. More
importantly, within our proposed design principle, the assistant model combines
the advantages of cross-architecture inductive biases and module functions by
merging convolution and attention modules derived from both student and teacher
module functions. Furthermore, we observe that heterogeneous features exhibit
diverse spatial distributions in CAKD, hindering the effectiveness of
conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a
spatial-agnostic InfoNCE loss to align features after spatial smoothing,
thereby improving the feature alignments in CAKD. Our proposed method is
evaluated across some homogeneous model pairs and arbitrary heterogeneous
combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance
for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on
ImageNet-1K. Our code and models will be released.

摘要：<paragraph>大多数知识蒸馏 (KD) 方法论主要关注具有相似架构的师生对，例如都是卷积神经网络 (CNN)。然而，KD 的潜力和灵活性可以通过将其扩展到新颖的跨架构 KD (CAKD) 来极大提高，其中同质和异质教师的知识可以灵活地转移到给定的学生。CAKD 中的主要挑战在于异构模型之间存在实质性的特征差距，这源于其固有的归纳偏差和模块功能的差异。为此，我们引入了一个辅助模型作为桥梁，以促进异构教师和学生之间平滑的特征知识转移。更重要的是，在我们提出的设计原则中，辅助模型通过合并源自学生和教师模块功能的卷积和注意力模块，结合了跨架构归纳偏差和模块功能的优势。此外，我们观察到异构特征在 CAKD 中表现出不同的空间分布，阻碍了传统的逐像素均方误差 (MSE) 损失的有效性。因此，我们利用空间无关的 InfoNCE 损失在空间平滑后对齐特征，从而改善 CAKD 中的特征对齐。我们提出的方法在一些同构模型对和 CNN、ViT 和 MLP 的任意异构组合中进行评估，在 CIFAR-100 上以 11.47% 的最大增益和 ImageNet-1K 上以 3.67% 的最大增益实现了蒸馏模型的最新性能。我们的代码和模型将被发布。</paragraph>

##### **A linguistic analysis of undesirable outcomes in the era of generative AI**
2410.12341v1 by Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo

Recent research has focused on the medium and long-term impacts of generative
AI, posing scientific and societal challenges mainly due to the detection and
reliability of machine-generated information, which is projected to form the
major content on the Web soon. Prior studies show that LLMs exhibit a lower
performance in generation tasks (model collapse) as they undergo a fine-tuning
process across multiple generations on their own generated content
(self-consuming loop). In this paper, we present a comprehensive simulation
framework built upon the chat version of LLama2, focusing particularly on the
linguistic aspects of the generated content, which has not been fully examined
in existing studies. Our results show that the model produces less lexical rich
content across generations, reducing diversity. The lexical richness has been
measured using the linguistic measures of entropy and TTR as well as
calculating the POSTags frequency. The generated content has also been examined
with an $n$-gram analysis, which takes into account the word order, and
semantic networks, which consider the relation between different words. These
findings suggest that the model collapse occurs not only by decreasing the
content diversity but also by distorting the underlying linguistic patterns of
the generated text, which both highlight the critical importance of carefully
choosing and curating the initial input text, which can alleviate the model
collapse problem. Furthermore, we conduct a qualitative analysis of the
fine-tuned models of the pipeline to compare their performances on generic NLP
tasks to the original model. We find that autophagy transforms the initial
model into a more creative, doubtful and confused one, which might provide
inaccurate answers and include conspiracy theories in the model responses,
spreading false and biased information on the Web.

摘要：<paragraph>最近的研究专注于生成式 AI 的中期和长期影响，主要由于机器生成信息的检测和可靠性，预计这将很快形成网络上的主要内容，从而带来科学和社会挑战。先前的研究表明，LLM 在生成任务（模型崩溃）中表现出较低的性能，因为它们在自己的生成内容上跨多个世代进行微调过程（自耗循环）。在本文中，我们提出了一个基于 LLama2 聊天版本的综合模拟框架，特别关注生成内容的语言方面，这在现有研究中尚未得到充分检验。我们的结果表明，该模型在各个世代中产生的词汇丰富度较低，从而降低了多样性。词汇丰富度已使用熵和 TTR 的语言度量以及计算 POSTags 频率来衡量。还使用考虑单词顺序的 n-gram 分析和考虑不同单词之间关系的语义网络检查了生成的内容。这些发现表明，模型崩溃不仅通过降低内容多样性发生，还通过扭曲生成文本的底层语言模式发生，这两者都突出了仔细选择和策划初始输入文本的关键重要性，这可以缓解模型崩溃问题。此外，我们对流水线的微调模型进行定性分析，以将其在通用 NLP 任务上的性能与原始模型进行比较。我们发现自噬将初始模型转化为更具创造力、怀疑和困惑的模型，这可能会提供不准确的答案，并在模型响应中包含阴谋论，在网络上散布虚假和有偏差的信息。</paragraph>

##### **Understanding the Role of LLMs in Multimodal Evaluation Benchmarks**
2410.12329v1 by Botian Jiang, Lei Li, Xiaonan Li, Zhaowei Li, Xiachong Feng, Lingpeng Kong, Qi Liu, Xipeng Qiu

The rapid advancement of Multimodal Large Language Models (MLLMs) has been
accompanied by the development of various benchmarks to evaluate their
capabilities. However, the true nature of these evaluations and the extent to
which they assess multimodal reasoning versus merely leveraging the underlying
Large Language Model (LLM) backbone remain unclear. This paper presents a
comprehensive investigation into the role of LLM backbones in MLLM evaluation,
focusing on two critical aspects: the degree to which current benchmarks truly
assess multimodal reasoning and the influence of LLM prior knowledge on
performance. Specifically, we introduce a modified evaluation protocol to
disentangle the contributions of the LLM backbone from multimodal integration,
and an automatic knowledge identification technique for diagnosing whether LLMs
equip the necessary knowledge for corresponding multimodal questions. Our study
encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key
findings reveal that some benchmarks allow high performance even without visual
inputs and up to 50\% of error rates can be attributed to insufficient world
knowledge in the LLM backbone, indicating a heavy reliance on language
capabilities. To address knowledge deficiencies, we propose a knowledge
augmentation pipeline that achieves significant performance gains, with
improvements of up to 60\% on certain datasets, resulting in a approximately 4x
increase in performance. Our work provides crucial insights into the role of
the LLM backbone in MLLMs, and highlights the need for more nuanced
benchmarking approaches.

摘要：隨著多模態大型語言模型 (MLLM) 的快速進展，各種基準測試也隨之發展，用於評估其能力。然而，這些評估的真正性質以及它們評估多模態推理的程度，相對於僅利用基礎大型語言模型 (LLM) 主幹仍不清楚。本文對 LLM 主幹在 MLLM 評估中的作用進行了全面調查，重點關注兩個關鍵方面：當前基準測試真正評估多模態推理的程度，以及 LLM 先驗知識對效能的影響。具體來說，我們引入了一個修改後的評估協議，以區分 LLM 主幹對多模態整合的貢獻，以及一種自動知識識別技術，用於診斷 LLM 是否具備對應多模態問題的必要知識。我們的研究涵蓋了四個不同的 MLLM 基準測試和八個最先進的 MLLM。關鍵發現表明，即使沒有視覺輸入，一些基準測試也能允許高性能，並且高達 50% 的錯誤率可歸因於 LLM 主幹中世界知識不足，這表明對語言能力的嚴重依賴。為了解決知識缺陷，我們提出了一個知識擴充管道，可實現顯著的性能提升，在某些數據集上提升高達 60%，導致性能提升約 4 倍。我們的研究對 LLM 主幹在 MLLM 中的作用提供了至關重要的見解，並強調了對更細緻的基準測試方法的需求。

##### **Neuron-based Personality Trait Induction in Large Language Models**
2410.12327v1 by Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Wayne Xin Zhao, Ji-Rong Wen

Large language models (LLMs) have become increasingly proficient at
simulating various personality traits, an important capability for supporting
related applications (e.g., role-playing). To further improve this capacity, in
this paper, we present a neuron-based approach for personality trait induction
in LLMs, with three major technical contributions. First, we construct
PersonalityBench, a large-scale dataset for identifying and evaluating
personality traits in LLMs. This dataset is grounded in the Big Five
personality traits from psychology and is designed to assess the generative
capabilities of LLMs towards specific personality traits. Second, by leveraging
PersonalityBench, we propose an efficient method for identifying
personality-related neurons within LLMs by examining the opposite aspects of a
given trait. Third, we develop a simple yet effective induction method that
manipulates the values of these identified personality-related neurons. This
method enables fine-grained control over the traits exhibited by LLMs without
training and modifying model parameters. Extensive experiments validate the
efficacy of our neuron identification and trait induction methods. Notably, our
approach achieves comparable performance as fine-tuned models, offering a more
efficient and flexible solution for personality trait induction in LLMs. We
provide access to all the mentioned resources at
https://github.com/RUCAIBox/NPTI.

摘要：大型語言模型 (LLM) 在模擬各種人格特質方面變得越來越熟練，這對於支援相關應用程式（例如角色扮演）來說是一項重要的能力。為了進一步提升這種能力，我們在本文中提出了一種基於神經元的 LLM 人格特質誘導方法，並有三大技術貢獻。首先，我們構建了 PersonalityBench，這是一個用於識別和評估 LLM 中人格特質的大規模資料集。此資料集以心理學中的五大性格特質為基礎，旨在評估 LLM 對特定人格特質的生成能力。其次，透過利用 PersonalityBench，我們提出了一種有效的方法，透過檢視特定特質的相反面向來識別 LLM 中與人格相關的神經元。第三，我們開發了一種簡單但有效的方法，用於調整這些已識別與人格相關的神經元的值。這種方法可以在不訓練和修改模型參數的情況下，精細地控制 LLM 所表現出的特質。廣泛的實驗驗證了我們的神經元識別和特質誘導方法的效能。值得注意的是，我們的做法達到了與微調模型相當的效能，為 LLM 中的人格特質誘導提供了更有效率且更靈活的解決方案。我們在 https://github.com/RUCAIBox/NPTI 提供所有上述資源的存取權。

##### **Optimizing Low-Resource Language Model Training: Comprehensive Analysis of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches**
2410.12325v1 by Kosuke Akimoto, Masafumi Oyamada

In this paper, we address the challenge of optimizing training setups for
Large Language Models (LLMs) of low-resource language with a limited amount of
corpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training
to utilize the limited target language corpus efficiently. However, there is
still a lack of understanding about the optimal hyperparameter setups for
combining these three approaches to train LLMs. We exhaustively explore
training setups for low-resource language LLM, combining these three
approaches, and found the following insights for efficiently reducing the cost
of hyperparameter search: (1) As the amount of target language corpus
decreases, the optimal training approach shifts from monolingual single-stage
training to multi-lingual two-stage training at a compute budget dependent
threshold. (2) The optimal model scale remains stable regardless of the amount
of target language corpus, allowing the use of the compute-optimal scale of
monolingual training. (3) The optimal number of epochs can be extrapolated from
smaller-scale experiments to larger scale using our proposed model. Also, we
provide evidence that, in single-stage training, the target language validation
loss follows a power law with respect to the target language ratio, with an
exponent independent of the amount of data, model scale, and language pair.

摘要：<paragraph>在本文中，我們探討了使用有限語料庫，針對低資源語言的大型語言模型 (LLM) 最佳化訓練設定的挑戰。現有作品採用多個世代、多語言和兩階段訓練，以有效利用有限的目標語言語料庫。然而，對於結合這三種方法來訓練 LLM 的最佳超參數設定，目前仍缺乏了解。我們針對低資源語言 LLM 的訓練設定進行了詳盡的探討，結合了這三種方法，並發現了以下見解，可有效減少超參數搜尋的成本：(1) 隨著目標語言語料庫的數量減少，最佳訓練方法會從單語單階段訓練轉移到多語言兩階段訓練，具體取決於運算預算。 (2) 最佳模型規模會保持穩定，與目標語言語料庫的數量無關，這讓我們可以使用單語訓練的運算最佳規模。 (3) 最佳世代數可以使用我們提出的模型，從小規模實驗外推到更大規模。此外，我們提供了證據，在單階段訓練中，目標語言驗證損失會遵循目標語言比率的冪律，且指數與資料量、模型規模和語言配對無關。</paragraph>

##### **Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up**
2410.12323v1 by Jiahao Yuan, Dehui Du, Hao Zhang, Zixiang Di, Usman Naseem

Large language models (LLMs) have shown remarkable performance in reasoning
tasks but face limitations in mathematical and complex logical reasoning.
Existing methods to improve LLMs' logical capabilities either involve traceable
or verifiable logical sequences that generate more reliable responses by
constructing logical structures yet increase computational costs, or introduces
rigid logic template rules, reducing flexibility. In this paper, we propose
Reversal of Thought (RoT), a novel framework aimed at enhancing the logical
reasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning
warm-up strategy, which integrates logical symbols for pseudocode planning
through meta-cognitive mechanisms and pairwise preference self-evaluation to
generate task-specific prompts solely through demonstrations, aligning with
LLMs' cognitive preferences shaped by Reinforcement Learning with Human
Feedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference
Manager to assess knowledge boundaries and further expand LLMs' reasoning
capabilities by aggregating solution logic for known tasks and stylistic
templates for unknown tasks. Experiments across various tasks demonstrate that
RoT surpasses existing baselines in both reasoning accuracy and efficiency.

摘要：大型語言模型 (LLM) 在推理任務中展現出卓越的表現，但在數學和複雜邏輯推理方面卻面臨限制。現有的改進 LLM 邏輯能力的方法，要不就是涉及可追溯或可驗證的邏輯序列，藉由建構邏輯結構來產生更可靠的回應，但會增加運算成本；要不就是引入僵化的邏輯範本規則，降低靈活性。在本文中，我們提出逆向思考 (RoT)，這是一個新穎的架構，旨在增強 LLM 的邏輯推理能力。RoT 利用偏好引導的反向推理熱身策略，透過元認知機制和成對偏好自我評估，將邏輯符號整合到偽代碼規劃中，以產生僅透過示範就能產生的特定任務提示，並與 LLM 的認知偏好（由人類回饋增強學習 (RLHF) 所形塑）保持一致。透過逆向推理，我們利用認知偏好管理員來評估知識界線，並透過彙總已知任務的解決方案邏輯和未知任務的文體範本，進一步擴展 LLM 的推理能力。在各種任務中的實驗證明，RoT 在推理準確度和效率方面都超越了現有的基準。

##### **UTF:Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification**
2410.12318v1 by Jiacheng Cai, Jiahao Yu, Yangguang Shao, Yuhang Wu, Xinyu Xing

Fingerprinting large language models (LLMs) is essential for verifying model
ownership, ensuring authenticity, and preventing misuse. Traditional
fingerprinting methods often require significant computational overhead or
white-box verification access. In this paper, we introduce UTF, a novel and
efficient approach to fingerprinting LLMs by leveraging under-trained tokens.
Under-trained tokens are tokens that the model has not fully learned during its
training phase. By utilizing these tokens, we perform supervised fine-tuning to
embed specific input-output pairs into the model. This process allows the LLM
to produce predetermined outputs when presented with certain inputs,
effectively embedding a unique fingerprint. Our method has minimal overhead and
impact on model's performance, and does not require white-box access to target
model's ownership identification. Compared to existing fingerprinting methods,
UTF is also more effective and robust to fine-tuning and random guess.

摘要：大型語言模型 (LLM) 指紋辨識對於驗證模型所有權、確保真實性及防止濫用至關重要。傳統指紋辨識方法通常需要大量的運算負擔或白盒驗證存取權。在本文中，我們介紹 UTF，一種新穎且有效率的方法，透過利用未充分訓練的符號來辨識 LLM 指紋。未充分訓練的符號是模型在其訓練階段中尚未完全學會的符號。透過利用這些符號，我們執行監督微調，將特定輸入輸出對嵌入到模型中。此程序允許 LLM 在呈現特定輸入時產生預先決定的輸出，有效地嵌入唯一的指紋。我們的技術對模型效能的負擔和影響很小，且不需要白盒存取權就能辨識目標模型的所有權。與現有的指紋辨識方法相比，UTF 對微調和隨機猜測也更有效且更強健。

##### **FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization**
2410.12312v1 by Cheng Yu, Haoyu Xie, Lei Shang, Yang Liu, Jun Dan, Baigui Sun, Liefeng Bo

In the field of human-centric personalized image generation, the
adapter-based method obtains the ability to customize and generate portraits by
text-to-image training on facial data. This allows for identity-preserved
personalization without additional fine-tuning in inference. Although there are
improvements in efficiency and fidelity, there is often a significant
performance decrease in test following ability, controllability, and diversity
of generated faces compared to the base model. In this paper, we analyze that
the performance degradation is attributed to the failure to decouple identity
features from other attributes during extraction, as well as the failure to
decouple the portrait generation training from the overall generation task. To
address these issues, we propose the Face Adapter with deCoupled Training
(FACT) framework, focusing on both model architecture and training strategy. To
decouple identity features from others, we leverage a transformer-based
face-export encoder and harness fine-grained identity features. To decouple the
portrait generation training, we propose Face Adapting Increment
Regularization~(FAIR), which effectively constrains the effect of face adapters
on the facial region, preserving the generative ability of the base model.
Additionally, we incorporate a face condition drop and shuffle mechanism,
combined with curriculum learning, to enhance facial controllability and
diversity. As a result, FACT solely learns identity preservation from training
data, thereby minimizing the impact on the original text-to-image capabilities
of the base model. Extensive experiments show that FACT has both
controllability and fidelity in both text-to-image generation and inpainting
solutions for portrait generation.

摘要：在以人为本的个性化图像生成领域，基于适配器的方法通过面部数据上的文本到图像训练获得了定制和生成人像的能力。这允许在推理中进行身份保留的个性化，而无需额外的微调。虽然效率和保真度有所提高，但与基础模型相比，生成的面的测试跟随能力、可控性和多样性通常会显着下降。在本文中，我们分析了性能下降归因于在提取过程中未能将身份特征与其他属性分离，以及未能将人像生成训练与整体生成任务分离。为了解决这些问题，我们提出了具有解耦训练 (FACT) 框架的面部适配器，重点关注模型架构和训练策略。为了将身份特征与其他特征分离，我们利用基于 transformer 的面部导出编码器并利用细粒度的身份特征。为了分离人像生成训练，我们提出了面部自适应增量正则化~(FAIR)，它有效地限制了面部适配器对人脸区域的影响，保留了基础模型的生成能力。此外，我们结合课程学习，加入了面部条件丢弃和洗牌机制，以增强面部可控性和多样性。因此，FACT 仅从训练数据中学习身份保留，从而最大程度地减少对基础模型的原始文本到图像功能的影响。大量的实验表明，FACT 在文本到图像生成和人像生成修复解决方案中都具有可控性和保真度。

##### **Open Domain Question Answering with Conflicting Contexts**
2410.12311v1 by Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth

Open domain question answering systems frequently rely on information
retrieved from large collections of text (such as the Web) to answer questions.
However, such collections of text often contain conflicting information, and
indiscriminately depending on this information may result in untruthful and
inaccurate answers. To understand the gravity of this problem, we collect a
human-annotated dataset, Question Answering with Conflicting Contexts (QACC),
and find that as much as 25% of unambiguous, open domain questions can lead to
conflicting contexts when retrieved using Google Search. We evaluate and
benchmark three powerful Large Language Models (LLMs) with our dataset QACC and
demonstrate their limitations in effectively addressing questions with
conflicting information. To explore how humans reason through conflicting
contexts, we request our annotators to provide explanations for their
selections of correct answers. We demonstrate that by finetuning LLMs to
explain their answers, we can introduce richer information into their training
that guide them through the process of reasoning with conflicting contexts.

摘要：開放領域問題解答系統經常依賴從大量文本集合（例如網路）中擷取的資訊來回答問題。然而，此類文本集合經常包含相互矛盾的資訊，而無差別地依賴這些資訊可能會導致不真實且不準確的答案。為了了解此問題的嚴重性，我們收集了一個人工標註的資料集，即帶有衝突脈絡的問題解答 (QACC)，並發現多達 25% 的明確開放領域問題在使用 Google 搜尋擷取時可能會導致衝突脈絡。我們使用我們的資料集 QACC 評估和比較了三個強大的大型語言模型 (LLM)，並展示了它們在有效處理具有衝突資訊的問題上的局限性。為了探討人類如何透過衝突脈絡進行推理，我們要求我們的標註者為他們選擇的正確答案提供解釋。我們證明，透過微調 LLM 以解釋其答案，我們可以將更豐富的資訊引入其訓練中，以引導它們完成處理衝突脈絡的過程。

##### **Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors**
2410.12299v1 by Weixuan Wang, Jingyuan Yang, Wei Peng

Large language models (LLMs) have achieved remarkable performance across many
tasks, yet aligning them with desired behaviors remains challenging. Activation
intervention has emerged as an effective and economical method to modify the
behavior of LLMs. Despite considerable interest in this area, current
intervention methods exclusively employ a fixed steering vector to modify model
activations, lacking adaptability to diverse input semantics. To address this
limitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel
method that constructs a dynamic steering vector to intervene model activations
at inference time. More specifically, SADI utilizes activation differences in
contrastive pairs to precisely identify critical elements of an LLM (i.e.,
attention heads, hidden states, and neurons) for targeted intervention. During
inference, SADI dynamically steers model behavior by scaling element-wise
activations based on the directions of input semantics. Experimental results
show that SADI outperforms established baselines by substantial margins,
improving task performance without training. SADI's cost-effectiveness and
generalizability across various LLM backbones and tasks highlight its potential
as a versatile alignment technique. In addition, we release the code to foster
research along this line:https://github.com/weixuan-wang123/SADI.

摘要：大型語言模型 (LLM) 已在許多任務中取得顯著的表現，但要讓它們與所需的行為保持一致仍然具有挑戰性。啟動介入已成為一種有效且經濟的方法，可以修改 LLM 的行為。儘管對這個領域有相當大的興趣，但目前的介入方法僅使用固定的導向向量來修改模型啟動，缺乏適應不同輸入語義的能力。為了解決這個限制，我們提出語義自適應動態介入 (SADI)，這是一種創新的方法，用於建立動態導向向量，以便在推理時介入模型啟動。更具體地說，SADI 利用對比對中的啟動差異，精確地識別 LLM 的關鍵元素（即注意力頭、隱藏狀態和神經元），以進行有針對性的介入。在推理期間，SADI 根據輸入語義的方向，通過調整元素級啟動，動態地引導模型行為。實驗結果表明，SADI 以大幅的幅度優於已建立的基準，在沒有訓練的情況下提高了任務性能。SADI 在各種 LLM 主幹和任務中的成本效益和通用性突出了它作為一種多功能對齊技術的潛力。此外，我們發布代碼以促進沿此線的研究：https://github.com/weixuan-wang123/SADI。

