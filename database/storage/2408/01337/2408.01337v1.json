{"2408.01337": {"publish_time": "2024-08-02", "title": "MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models", "paper_summary": "Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.", "paper_summary_zh": "<paragraph>\u591a\u6a21\u614b\u6a21\u578b\u540c\u6642\u8655\u7406\u97f3\u8a0a\u548c\u8a9e\u8a00\uff0c\u5728\u97f3\u8a0a\u7406\u89e3\u65b9\u9762\u6975\u5177\u524d\u666f\uff0c\u4e26\u4e14\u5728\u97f3\u6a02\u9818\u57df\u4e2d\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u5730\u88ab\u63a1\u7528\u3002\n\u900f\u904e\u5141\u8a31\u4f7f\u7528\u8005\u900f\u904e\u6587\u5b57\u67e5\u8a62\u4e26\u53d6\u5f97\u95dc\u65bc\u7279\u5b9a\u97f3\u8a0a\u8f38\u5165\u7684\u8cc7\u8a0a\uff0c\u9019\u4e9b\u6a21\u578b\u6709\u6f5b\u529b\u900f\u904e\u57fa\u65bc\u8a9e\u8a00\u7684\u4ecb\u9762\u555f\u7528\u5404\u7a2e\u97f3\u6a02\u7406\u89e3\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u8a55\u4f30\u6703\u5e36\u4f86\u76f8\u7576\u5927\u7684\u6311\u6230\uff0c\u800c\u4e14\u76ee\u524d\u4ecd\u4e0d\u6e05\u695a\u5982\u4f55\u6709\u6548\u8a55\u4f30\u5b83\u5011\u6b63\u78ba\u8a6e\u91cb\u8207\u97f3\u6a02\u76f8\u95dc\u8f38\u5165\u7684\u80fd\u529b\u3002\n\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 MuChoMusic\uff0c\u4e00\u500b\u5c08\u6ce8\u65bc\u97f3\u8a0a\u7684\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\u4e2d\u8a55\u4f30\u97f3\u6a02\u7406\u89e3\u7684\u57fa\u6e96\u3002MuChoMusic \u5305\u542b 1,187 \u500b\u591a\u91cd\u9078\u64c7\u984c\uff0c\u5168\u90e8\u7531\u4eba\u985e\u8a3b\u89e3\u8005\u9a57\u8b49\uff0c\u6db5\u84cb\u4f86\u81ea\u5169\u500b\u516c\u958b\u97f3\u6a02\u8cc7\u6599\u96c6\u7684 644 \u9996\u97f3\u6a02\u66f2\u76ee\uff0c\u4e26\u6db5\u84cb\u4e86\u5404\u7a2e\u985e\u578b\u3002\u57fa\u6e96\u4e2d\u7684\u554f\u984c\u65e8\u5728\u8a55\u4f30\u8de8\u8d8a\u5e7e\u500b\u7dad\u5ea6\u7684\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\uff0c\u9019\u4e9b\u7dad\u5ea6\u6db5\u84cb\u4e86\u57fa\u672c\u7684\u97f3\u6a02\u6982\u5ff5\u53ca\u5176\u8207\u6587\u5316\u548c\u529f\u80fd\u80cc\u666f\u7684\u95dc\u4fc2\u3002\u900f\u904e\u57fa\u6e96\u63d0\u4f9b\u7684\u6574\u9ad4\u5206\u6790\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u4e94\u500b\u958b\u6e90\u6a21\u578b\uff0c\u4e26\u627e\u51fa\u5e7e\u500b\u7f3a\u9677\uff0c\u5305\u62ec\u904e\u5ea6\u4f9d\u8cf4\u8a9e\u8a00\u6a21\u614b\uff0c\u6307\u51fa\u9700\u8981\u66f4\u597d\u7684\u591a\u6a21\u614b\u6574\u5408\u3002\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u90fd\u662f\u958b\u6e90\u7684\u3002</paragraph>", "author": "Benno Weck et.al.", "authors": "Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov", "id": "2408.01337v1", "paper_url": "http://arxiv.org/abs/2408.01337v1", "repo": "null"}}