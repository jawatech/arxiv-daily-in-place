{"2411.13009": {"publish_time": "2024-11-20", "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts", "paper_summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\uff0c\n\u5b83\u5011\u5728\u8f03\u9577\u7684\u8108\u7d61\u7406\u89e3\u548c\u9ad8\u904b\u7b97\u6210\u672c\u65b9\u9762\u4ecd\u9762\u81e8\u6311\u6230\u3002\u70ba\u4e86\u5e73\u8861\u6548\u7387\u8207\u54c1\u8cea\uff0c\u6211\u5011\u5f15\u5165\u4e86 LLMSteer\uff0c\u4e00\u500b\u7121\u9700\u5fae\u8abf\u7684\u67b6\u69cb\uff0c\u900f\u904e\u8207\u67e5\u8a62\u7121\u95dc\u7684\u6ce8\u610f\u529b\u5f15\u5c0e\u4f86\u589e\u5f37 LLM\u3002\u5728\u71b1\u9580\u7684 LLM \u548c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u6e2c\u8a66\uff0cLLMSteer \u5c07\u6548\u80fd\u5dee\u8ddd\u8207\u57fa\u6e96\u7e2e\u5c0f\u4e86 65.9%\uff0c\u4e26\u5c07\u57f7\u884c\u6642\u9593\u5ef6\u9072\u6e1b\u5c11\u4e86 4.8 \u500d\uff0c\u8207\u6700\u8fd1\u7684\u6ce8\u610f\u529b\u5f15\u5c0e\u65b9\u6cd5\u76f8\u6bd4\u3002", "author": "Zhuohan Gu et.al.", "authors": "Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang", "id": "2411.13009v1", "paper_url": "http://arxiv.org/abs/2411.13009v1", "repo": "null"}}