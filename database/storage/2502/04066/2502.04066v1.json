{"2502.04066": {"publish_time": "2025-02-06", "title": "Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training", "paper_summary": "The GPT-4 technical report from OpenAI suggests that model performance on\nspecific tasks can be predicted prior to training, though methodologies remain\nunspecified. This approach is crucial for optimizing resource allocation and\nensuring data alignment with target tasks. To achieve this vision, we focus on\npredicting performance on Closed-book Question Answering (CBQA) tasks, which\nare closely tied to pre-training data and knowledge retention. We address three\nmajor challenges: 1) mastering the entire pre-training process, especially data\nconstruction; 2) evaluating a model's knowledge retention; and 3) predicting\ntask-specific knowledge retention using only information available prior to\ntraining. To tackle these challenges, we pre-train three large language models\n(i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the\npre-training data with knowledge triples and assess knowledge retention using\nestablished methods. Additionally, we introduce the SMI metric, an\ninformation-theoretic measure that quantifies the relationship between\npre-training data, model size, and task-specific knowledge retention. Our\nexperiments reveal a strong linear correlation ($\\text{R}^2 > 0.84$) between\nthe SMI metric and the model's accuracy on CBQA tasks across models of varying\nsizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are\navailable at https://github.com/yuhui1038/SMI.", "paper_summary_zh": "OpenAI \u7684 GPT-4 \u6280\u8853\u5831\u544a\u6307\u51fa\uff0c\u5118\u7ba1\u65b9\u6cd5\u4ecd\u672a\u660e\u78ba\uff0c\u4f46\u53ef\u4ee5\u5728\u8a13\u7df4\u524d\u9810\u6e2c\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u3002\u9019\u7a2e\u65b9\u6cd5\u5c0d\u65bc\u512a\u5316\u8cc7\u6e90\u914d\u7f6e\u548c\u78ba\u4fdd\u8cc7\u6599\u8207\u76ee\u6a19\u4efb\u52d9\u7684\u4e00\u81f4\u6027\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u5be6\u73fe\u9019\u500b\u9858\u666f\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u9810\u6e2c\u9589\u5377\u5f0f\u554f\u7b54 (CBQA) \u4efb\u52d9\u7684\u8868\u73fe\uff0c\u9019\u4e9b\u4efb\u52d9\u8207\u9810\u8a13\u7df4\u8cc7\u6599\u548c\u77e5\u8b58\u4fdd\u7559\u5bc6\u5207\u76f8\u95dc\u3002\u6211\u5011\u89e3\u6c7a\u4e86\u4e09\u5927\u6311\u6230\uff1a1) \u638c\u63e1\u6574\u500b\u9810\u8a13\u7df4\u904e\u7a0b\uff0c\u7279\u5225\u662f\u8cc7\u6599\u5efa\u69cb\uff1b2) \u8a55\u4f30\u6a21\u578b\u7684\u77e5\u8b58\u4fdd\u7559\uff1b\u4ee5\u53ca 3) \u50c5\u4f7f\u7528\u8a13\u7df4\u524d\u53ef\u5f97\u7684\u8cc7\u8a0a\u9810\u6e2c\u7279\u5b9a\u4efb\u52d9\u7684\u77e5\u8b58\u4fdd\u7559\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u4f7f\u7528 56 \u842c\u7f8e\u5143\u548c 52 \u842c GPU \u5c0f\u6642\u9810\u8a13\u7df4\u4e86\u4e09\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08\u5373 1.6B\u30017B \u548c 13B\uff09\u3002\u6211\u5011\u4f7f\u7528\u77e5\u8b58\u4e09\u5143\u7d44\u5206\u6790\u9810\u8a13\u7df4\u8cc7\u6599\uff0c\u4e26\u4f7f\u7528\u65e2\u5b9a\u65b9\u6cd5\u8a55\u4f30\u77e5\u8b58\u4fdd\u7559\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 SMI \u6307\u6a19\uff0c\u9019\u662f\u4e00\u7a2e\u8cc7\u8a0a\u7406\u8ad6\u6e2c\u91cf\uff0c\u7528\u65bc\u91cf\u5316\u9810\u8a13\u7df4\u8cc7\u6599\u3001\u6a21\u578b\u5927\u5c0f\u548c\u7279\u5b9a\u4efb\u52d9\u77e5\u8b58\u4fdd\u7559\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u6211\u5011\u7684\u5be6\u9a57\u63ed\u793a\u4e86\u4e00\u500b\u5f37\u70c8\u7684\u7dda\u6027\u76f8\u95dc\u6027\uff08$\\text{R}^2 > 0.84$\uff09\uff0c\u5728\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\uff08\u5373 1.1B\u30011.6B\u30017B \u548c 13B\uff09\u4e2d\uff0cSMI \u6307\u6a19\u548c\u6a21\u578b\u5728 CBQA \u4efb\u52d9\u4e0a\u7684\u6e96\u78ba\u6027\u4e4b\u9593\u3002\u8cc7\u6599\u96c6\u3001\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/yuhui1038/SMI \u53d6\u5f97\u3002", "author": "Changhao Jiang et.al.", "authors": "Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang", "id": "2502.04066v1", "paper_url": "http://arxiv.org/abs/2502.04066v1", "repo": "null"}}