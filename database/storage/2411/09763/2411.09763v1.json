{"2411.09763": {"publish_time": "2024-11-14", "title": "Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review Outcomes Across Multiple Platforms", "paper_summary": "While previous studies have demonstrated that Large Language Models (LLMs)\ncan predict peer review outcomes to some extent, this paper builds on that by\nintroducing two new contexts and employing a more robust method - averaging\nmultiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions,\nbased on reviewer guidelines and using only the submitted titles and abstracts,\nfailed to predict peer review outcomes for F1000Research (Spearman's rho=0.00).\nHowever, it produced mostly weak positive correlations with the quality\ndimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality,\nrho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive\ncorrelation for papers from the International Conference on Learning\nRepresentations (ICLR) (rho=0.38). Including the full text of articles\nsignificantly increased the correlation for ICLR (rho=0.46) and slightly\nimproved it for F1000Research (rho=0.09), while it had variable effects on the\nfour quality dimension correlations for SciPost LaTeX files. The use of\nchain-of-thought system prompts slightly increased the correlation for\nF1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and\nfurther decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for\noriginality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the\nresults suggest that in some contexts, ChatGPT can produce weak pre-publication\nquality assessments. However, the effectiveness of these assessments and the\noptimal strategies for employing them vary considerably across different\nplatforms, journals, and conferences. Additionally, the most suitable inputs\nfor ChatGPT appear to differ depending on the platform.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u5148\u524d\u7684\u7814\u7a76\u5df2\u8b49\u5be6\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\n\u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u9810\u6e2c\u540c\u884c\u8a55\u5be9\u7d50\u679c\uff0c\u4f46\u672c\u6587\u5efa\u7acb\u5728\u8a72\u57fa\u790e\u4e0a\uff0c\n\u5f15\u5165\u4e86\u5169\u500b\u65b0\u7684\u8108\u7d61\uff0c\u4e26\u63a1\u7528\u66f4\u5f37\u5065\u7684\u65b9\u6cd5 - \u5e73\u5747\n\u591a\u500b ChatGPT \u5206\u6578\u3002\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5e73\u5747 30 \u500b ChatGPT \u9810\u6e2c\uff0c\n\u6839\u64da\u5be9\u67e5\u54e1\u6307\u5357\uff0c\u50c5\u4f7f\u7528\u63d0\u4ea4\u7684\u6a19\u984c\u548c\u6458\u8981\uff0c\n\u7121\u6cd5\u9810\u6e2c F1000Research \u7684\u540c\u884c\u8a55\u5be9\u7d50\u679c\uff08Spearman's rho=0.00\uff09\u3002\n\u7136\u800c\uff0c\u5b83\u7522\u751f\u4e86\u8207 SciPost Physics \u7684\u54c1\u8cea\u5411\u5ea6\u5448\u5f31\u6b63\u76f8\u95dc\uff08rho=0.25 \u8868\u793a\u6709\u6548\u6027\uff0crho=0.25 \u8868\u793a\u7368\u5275\u6027\uff0c\nrho=0.20 \u8868\u793a\u91cd\u8981\u6027\uff0crho = 0.08 \u8868\u793a\u6e05\u6670\u5ea6\uff09\u4ee5\u53ca\u8207\u570b\u969b\u5b78\u7fd2\u8868\u5fb5\u6703\u8b70\uff08ICLR\uff09\u8ad6\u6587\u7684\u4e2d\u7b49\u6b63\u76f8\u95dc\n\uff08rho=0.38\uff09\u3002\u5305\u542b\u6587\u7ae0\u7684\u5168\u6587\u986f\u8457\u589e\u52a0\u4e86 ICLR \u7684\u76f8\u95dc\u6027\uff08rho=0.46\uff09\uff0c\u4e26\u7565\u5fae\n\u6539\u5584\u4e86 F1000Research\uff08rho=0.09\uff09\uff0c\u800c\u5b83\u5c0d SciPost LaTeX \u6a94\u6848\u7684\u56db\u500b\u54c1\u8cea\u5411\u5ea6\u76f8\u95dc\u6027\u6709\u4e0d\u540c\u7684\u5f71\u97ff\u3002\u4f7f\u7528\n\u601d\u8003\u93c8\u7cfb\u7d71\u63d0\u793a\u7565\u5fae\u589e\u52a0\u4e86 F1000Research \u7684\u76f8\u95dc\u6027\uff08rho=0.10\uff09\uff0c\u7565\u5fae\u964d\u4f4e\u4e86 ICLR \u7684\u76f8\u95dc\u6027\uff08rho=0.37\uff09\uff0c\u4e26\n\u9032\u4e00\u6b65\u964d\u4f4e\u4e86 SciPost Physics \u7684\u76f8\u95dc\u6027\uff08rho=0.16 \u8868\u793a\u6709\u6548\u6027\uff0crho=0.18 \u8868\u793a\u7368\u5275\u6027\uff0crho=0.18 \u8868\u793a\u91cd\u8981\u6027\uff0crho=0.05 \u8868\u793a\u6e05\u6670\u5ea6\uff09\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\n\u7d50\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0cChatGPT \u53ef\u4ee5\u7522\u751f\u5f31\u7684\u51fa\u7248\u524d\u54c1\u8cea\u8a55\u4f30\u3002\u7136\u800c\uff0c\u9019\u4e9b\u8a55\u4f30\u7684\u6709\u6548\u6027\u4ee5\u53ca\u63a1\u7528\u5b83\u5011\u7684\u6700\u4f73\u7b56\u7565\u5728\u4e0d\u540c\u7684\n\u5e73\u53f0\u3001\u671f\u520a\u548c\u6703\u8b70\u4e4b\u9593\u6709\u5f88\u5927\u5dee\u7570\u3002\u6b64\u5916\uff0cChatGPT \u6700\u5408\u9069\u7684\u8f38\u5165\u4f3c\u4e4e\u6703\u6839\u64da\u5e73\u53f0\u800c\u6709\u6240\u4e0d\u540c\u3002</paragraph>", "author": "Mike Thelwall et.al.", "authors": "Mike Thelwall, Abdullah Yaghi", "id": "2411.09763v1", "paper_url": "http://arxiv.org/abs/2411.09763v1", "repo": "null"}}