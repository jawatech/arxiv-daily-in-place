{"2412.04449": {"publish_time": "2024-12-05", "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay", "paper_summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.", "paper_summary_zh": "\u5118\u7ba1\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u9f90\u5927\u7684\u8a13\u7df4\u548c\u63a8\u8ad6\u6210\u672c\u963b\u7919\u4e86\u5b83\u5011\u7684\u9032\u6b65\u3002\u5927\u90e8\u5206\u904b\u7b97\u6e90\u81eaTransformer\u89e3\u78bc\u5668\u8655\u7406\u7684\u9f90\u5927\u8996\u89ba\u7b26\u865f\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u900f\u904e\u5229\u7528\u6df1\u5ea6\u6df7\u5408 (MoD) \u6a5f\u5236\u4f86\u5efa\u7acb\u9ad8\u6548\u7684 MLLM\uff0c\u5176\u4e2d\u6bcf\u500bTransformer\u89e3\u78bc\u5668\u5c64\u6703\u9078\u64c7\u5fc5\u8981\u7684\u8996\u89ba\u7b26\u865f\u4f86\u8655\u7406\uff0c\u540c\u6642\u7565\u904e\u591a\u9918\u7684\u7b26\u865f\u3002\u7136\u800c\uff0c\u5c07 MoD \u6574\u5408\u5230 MLLM \u4e26\u4e0d\u5bb9\u6613\u3002\u70ba\u4e86\u61c9\u5c0d\u8a13\u7df4\u548c\u63a8\u8ad6\u7a69\u5b9a\u6027\u4ee5\u53ca\u8a13\u7df4\u8cc7\u6599\u6709\u9650\u7684\u6311\u6230\uff0c\u6211\u5011\u900f\u904e\u5169\u7a2e\u5275\u65b0\u8a2d\u8a08\u4f86\u8abf\u6574 MoD \u6a21\u7d44\uff1atanh \u9598\u63a7\u6b0a\u91cd\u6b63\u898f\u5316 (TanhNorm) \u548c\u5c0d\u7a31\u7b26\u865f\u91cd\u65b0\u52a0\u6b0a (STRing)\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u8996\u89ba\u7b26\u865f\u5728\u8f03\u6df1\u7684\u5c64\u4e2d\u8868\u73fe\u51fa\u66f4\u9ad8\u7684\u5197\u9918\uff0c\u56e0\u6b64\u8a2d\u8a08\u4e86\u4e00\u500b\u6f38\u9032\u6bd4\u7387\u8870\u6e1b (PRD) \u7b56\u7565\uff0c\u5b83\u6703\u9010\u5c64\u9010\u6f38\u964d\u4f4e\u7b26\u865f\u4fdd\u7559\u7387\uff0c\u4e26\u63a1\u7528\u504f\u79fb\u9918\u5f26\u6642\u7a0b\u3002\u9019\u500b\u95dc\u9375\u8a2d\u8a08\u5b8c\u5168\u91cb\u653e\u4e86 MoD \u7684\u6f5b\u529b\uff0c\u986f\u8457\u63d0\u5347\u4e86\u6211\u5011\u6a21\u578b\u7684\u6548\u7387\u548c\u6548\u80fd\u3002\u70ba\u4e86\u9a57\u8b49\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5728 14 \u500b\u57fa\u6e96\u4e0a\u5c0d\u5169\u500b\u57fa\u6e96\u6a21\u578b\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u6211\u5011\u7684\u6a21\u578b p-MoD \u5728\u63a8\u8ad6\u671f\u9593\u50c5\u6709 55.6% \u7684 TFLOP \u548c 53.8% \u7684 KV \u5feb\u53d6\u5132\u5b58\uff0c\u4ee5\u53ca\u8a13\u7df4\u671f\u9593 77.7% \u7684 GPU \u5c0f\u6642\uff0c\u5c31\u80fd\u5920\u9054\u5230\u6216\u751a\u81f3\u8d85\u8d8a\u57fa\u6e96\u6a21\u578b\u7684\u6548\u80fd\u3002", "author": "Jun Zhang et.al.", "authors": "Jun Zhang, Desen Meng, Ji Qi, Zhenpeng Huang, Tao Wu, Limin Wang", "id": "2412.04449v1", "paper_url": "http://arxiv.org/abs/2412.04449v1", "repo": "null"}}