{"2408.09530": {"publish_time": "2024-08-18", "title": "PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding", "paper_summary": "The previous advancements in pathology image understanding primarily involved\ndeveloping models tailored to specific tasks. Recent studies has demonstrated\nthat the large vision-language model can enhance the performance of various\ndownstream tasks in medical image understanding. In this study, we developed a\ndomain-specific large language-vision assistant (PA-LLaVA) for pathology image\nunderstanding. Specifically, (1) we first construct a human pathology\nimage-text dataset by cleaning the public medical image-text data for\ndomain-specific alignment; (2) Using the proposed image-text data, we first\ntrain a pathology language-image pretraining (PLIP) model as the specialized\nvisual encoder for pathology image, and then we developed scale-invariant\nconnector to avoid the information loss caused by image scaling; (3) We adopt\ntwo-stage learning to train PA-LLaVA, first stage for domain alignment, and\nsecond stage for end to end visual question \\& answering (VQA) task. In\nexperiments, we evaluate our PA-LLaVA on both supervised and zero-shot VQA\ndatasets, our model achieved the best overall performance among multimodal\nmodels of similar scale. The ablation experiments also confirmed the\neffectiveness of our design. We posit that our PA-LLaVA model and the datasets\npresented in this work can promote research in field of computational\npathology. All codes are available at:\nhttps://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA}{https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA", "paper_summary_zh": "\u4ee5\u5f80\u5728\u75c5\u7406\u5f71\u50cf\u7406\u89e3\u65b9\u9762\u7684\u9032\u5c55\uff0c\u4e3b\u8981\u96c6\u4e2d\u65bc\u958b\u767c\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684\u6a21\u578b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u589e\u5f37\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u5728\u91ab\u5b78\u5f71\u50cf\u7406\u89e3\u4e2d\u7684\u8868\u73fe\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u7279\u5b9a\u9818\u57df\u7684\u5927\u578b\u8a9e\u8a00\u8996\u89ba\u52a9\u7406 (PA-LLaVA)\uff0c\u7528\u65bc\u75c5\u7406\u5f71\u50cf\u7406\u89e3\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c(1) \u6211\u5011\u9996\u5148\u901a\u904e\u6e05\u7406\u516c\u5171\u91ab\u5b78\u5f71\u50cf\u6587\u672c\u6578\u64da\uff0c\u4ee5\u9032\u884c\u7279\u5b9a\u9818\u57df\u6821\u6e96\uff0c\u4f86\u69cb\u5efa\u4eba\u985e\u75c5\u7406\u5f71\u50cf\u6587\u672c\u6578\u64da\u96c6\uff1b(2) \u4f7f\u7528\u63d0\u51fa\u7684\u5f71\u50cf\u6587\u672c\u6578\u64da\uff0c\u6211\u5011\u9996\u5148\u8a13\u7df4\u4e00\u500b\u75c5\u7406\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (PLIP) \u6a21\u578b\uff0c\u4f5c\u70ba\u75c5\u7406\u5f71\u50cf\u7684\u5c08\u7528\u8996\u89ba\u7de8\u78bc\u5668\uff0c\u7136\u5f8c\u6211\u5011\u958b\u767c\u4e86\u5c3a\u5ea6\u4e0d\u8b8a\u9023\u63a5\u5668\uff0c\u4ee5\u907f\u514d\u56e0\u5f71\u50cf\u7e2e\u653e\u800c\u9020\u6210\u7684\u8cc7\u8a0a\u640d\u5931\uff1b(3) \u6211\u5011\u63a1\u7528\u5169\u968e\u6bb5\u5b78\u7fd2\u4f86\u8a13\u7df4 PA-LLaVA\uff0c\u7b2c\u4e00\u968e\u6bb5\u9032\u884c\u9818\u57df\u6821\u6e96\uff0c\u7b2c\u4e8c\u968e\u6bb5\u9032\u884c\u7aef\u5230\u7aef\u7684\u8996\u89ba\u554f\u7b54 (VQA) \u4efb\u52d9\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5728\u76e3\u7763\u5f0f\u548c\u96f6\u6b21 VQA \u6578\u64da\u96c6\u4e0a\u8a55\u4f30\u6211\u5011\u7684 PA-LLaVA\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u898f\u6a21\u76f8\u4f3c\u7684\u591a\u6a21\u614b\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u6574\u9ad4\u8868\u73fe\u3002\u6d88\u878d\u5be6\u9a57\u4e5f\u8b49\u5be6\u4e86\u6211\u5011\u8a2d\u8a08\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u8a8d\u70ba\uff0c\u6211\u5011\u7684 PA-LLaVA \u6a21\u578b\u548c\u9019\u9805\u5de5\u4f5c\u4e2d\u63d0\u51fa\u7684\u6578\u64da\u96c6\u53ef\u4ee5\u4fc3\u9032\u8a08\u7b97\u75c5\u7406\u5b78\u9818\u57df\u7684\u7814\u7a76\u3002\u6240\u6709\u7a0b\u5f0f\u78bc\u90fd\u53ef\u4ee5\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\nhttps://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA", "author": "Dawei Dai et.al.", "authors": "Dawei Dai, Yuanhui Zhang, Long Xu, Qianlan Yang, Xiaojing Shen, Shuyin Xia, Guoyin Wang", "id": "2408.09530v1", "paper_url": "http://arxiv.org/abs/2408.09530v1", "repo": "https://github.com/ddw2aigroup2cqupt/pa-llava"}}