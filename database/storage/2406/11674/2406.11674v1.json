{"2406.11674": {"publish_time": "2024-06-17", "title": "Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference", "paper_summary": "The increasing size of large language models (LLMs) challenges their usage on\nresource-constrained platforms. For example, memory on modern GPUs is\ninsufficient to hold LLMs that are hundreds of Gigabytes in size. Offloading is\na popular method to escape this constraint by storing weights of an LLM model\nto host CPU memory and SSD, then loading each weight to GPU before every use.\nIn our case study of offloaded inference, we found that due to the low\nbandwidth between storage devices and GPU, the latency of transferring large\nmodel weights from its offloaded location to GPU memory becomes the critical\nbottleneck with actual compute taking nearly 0% of runtime. To effectively\nreduce the weight transfer latency, we propose a novel sparse format that\ncompresses the unstructured sparse pattern of pruned LLM weights to non-zero\nvalues with high compression ratio and low decompression overhead. Endor\nachieves this by expressing the positions of non-zero elements with a bitmap.\nCompared to offloaded inference using the popular Huggingface Accelerate,\napplying Endor accelerates OPT-66B by 1.70x and Llama2-70B by 1.78x. When\ndirect weight transfer from SSD to GPU is leveraged, Endor achieves 2.25x\nspeedup on OPT-66B and 2.37x speedup on Llama2-70B.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5c3a\u5bf8\u8d8a\u6765\u8d8a\u5927\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u4f7f\u7528\u6784\u6210\u6311\u6218\u3002\u4f8b\u5982\uff0c\u73b0\u4ee3 GPU \u4e0a\u7684\u5185\u5b58\u4e0d\u8db3\u4ee5\u5bb9\u7eb3\u6570\u767e GB \u5927\u5c0f\u7684 LLM\u3002\u5378\u8f7d\u662f\u4e00\u79cd\u6d41\u884c\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c06 LLM \u6a21\u578b\u7684\u6743\u91cd\u5b58\u50a8\u5230\u4e3b\u673a CPU \u5185\u5b58\u548c SSD \u4e2d\u6765\u6446\u8131\u6b64\u9650\u5236\uff0c\u7136\u540e\u5728\u6bcf\u6b21\u4f7f\u7528\u4e4b\u524d\u5c06\u6bcf\u4e2a\u6743\u91cd\u52a0\u8f7d\u5230 GPU \u4e2d\u3002\u5728\u6211\u4eec\u7684\u5378\u8f7d\u63a8\u7406\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u7531\u4e8e\u5b58\u50a8\u8bbe\u5907\u548c GPU \u4e4b\u95f4\u7684\u5e26\u5bbd\u8f83\u4f4e\uff0c\u5c06\u5927\u578b\u6a21\u578b\u6743\u91cd\u4ece\u5176\u5378\u8f7d\u4f4d\u7f6e\u4f20\u8f93\u5230 GPU \u5185\u5b58\u7684\u5ef6\u8fdf\u6210\u4e3a\u5173\u952e\u74f6\u9888\uff0c\u5b9e\u9645\u8ba1\u7b97\u4ec5\u5360\u8fd0\u884c\u65f6\u7684 0%\u3002\u4e3a\u4e86\u6709\u6548\u51cf\u5c11\u6743\u91cd\u4f20\u8f93\u5ef6\u8fdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u683c\u5f0f\uff0c\u8be5\u683c\u5f0f\u5c06\u4fee\u526a\u7684 LLM \u6743\u91cd\u7684\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6a21\u5f0f\u538b\u7f29\u4e3a\u975e\u96f6\u503c\uff0c\u5177\u6709\u9ad8\u538b\u7f29\u6bd4\u548c\u4f4e\u89e3\u538b\u7f29\u5f00\u9500\u3002Endor \u901a\u8fc7\u4f7f\u7528\u4f4d\u56fe\u8868\u793a\u975e\u96f6\u5143\u7d20\u7684\u4f4d\u7f6e\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u4e0e\u4f7f\u7528\u6d41\u884c\u7684 Huggingface Accelerate \u8fdb\u884c\u5378\u8f7d\u63a8\u7406\u76f8\u6bd4\uff0c\u5e94\u7528 Endor \u53ef\u5c06 OPT-66B \u52a0\u901f 1.70 \u500d\uff0c\u5c06 Llama2-70B \u52a0\u901f 1.78 \u500d\u3002\u5f53\u5229\u7528\u4ece SSD \u5230 GPU \u7684\u76f4\u63a5\u6743\u91cd\u4f20\u8f93\u65f6\uff0cEndor \u5728 OPT-66B \u4e0a\u5b9e\u73b0\u4e86 2.25 \u500d\u7684\u52a0\u901f\uff0c\u5728 Llama2-70B \u4e0a\u5b9e\u73b0\u4e86 2.37 \u500d\u7684\u52a0\u901f\u3002", "author": "Donghyeon Joo et.al.", "authors": "Donghyeon Joo, Ramyad Hadidi, Soheil Feizi, Bahar Asgari", "id": "2406.11674v1", "paper_url": "http://arxiv.org/abs/2406.11674v1", "repo": "null"}}