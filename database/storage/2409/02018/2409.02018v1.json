{"2409.02018": {"publish_time": "2024-09-03", "title": "TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation", "paper_summary": "In healthcare, medical image segmentation is crucial for accurate disease\ndiagnosis and the development of effective treatment strategies. Early\ndetection can significantly aid in managing diseases and potentially prevent\ntheir progression. Machine learning, particularly deep convolutional neural\nnetworks, has emerged as a promising approach to addressing segmentation\nchallenges. Traditional methods like U-Net use encoding blocks for local\nrepresentation modeling and decoding blocks to uncover semantic relationships.\nHowever, these models often struggle with multi-scale objects exhibiting\nsignificant variations in texture and shape, and they frequently fail to\ncapture long-range dependencies in the input data. Transformers designed for\nsequence-to-sequence predictions have been proposed as alternatives, utilizing\nglobal self-attention mechanisms. Yet, they can sometimes lack precise\nlocalization due to insufficient granular details. To overcome these\nlimitations, we introduce TransDAE: a novel approach that reimagines the\nself-attention mechanism to include both spatial and channel-wise associations\nacross the entire feature space, while maintaining computational efficiency.\nAdditionally, TransDAE enhances the skip connection pathway with an inter-scale\ninteraction module, promoting feature reuse and improving localization\naccuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on\nthe Synaps multi-organ dataset, even without relying on pre-trained weights.", "paper_summary_zh": "\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u533b\u5b66\u5f71\u50cf\u5206\u5272\u5bf9\u4e8e\u51c6\u786e\u7684\u75be\u75c5\u8bca\u65ad\u548c\u6709\u6548\u6cbb\u7597\u7b56\u7565\u7684\u5f00\u53d1\u81f3\u5173\u91cd\u8981\u3002\u65e9\u671f\u68c0\u6d4b\u53ef\u4ee5\u6781\u5927\u5730\u5e2e\u52a9\u63a7\u5236\u75be\u75c5\uff0c\u5e76\u53ef\u80fd\u9632\u6b62\u75be\u75c5\u8fdb\u5c55\u3002\u673a\u5668\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5df2\u6210\u4e3a\u89e3\u51b3\u5206\u5272\u6311\u6218\u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002U-Net \u7b49\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u7f16\u7801\u5757\u8fdb\u884c\u5c40\u90e8\u8868\u793a\u5efa\u6a21\u548c\u89e3\u7801\u5757\u6765\u63ed\u793a\u8bed\u4e49\u5173\u7cfb\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u5904\u7406\u5728\u7eb9\u7406\u548c\u5f62\u72b6\u4e0a\u8868\u73b0\u51fa\u663e\u7740\u53d8\u5316\u7684\u591a\u5c3a\u5ea6\u5bf9\u8c61\uff0c\u5e76\u4e14\u5b83\u4eec\u7ecf\u5e38\u65e0\u6cd5\u6355\u83b7\u8f93\u5165\u6570\u636e\u4e2d\u7684\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002\u4e13\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u9884\u6d4b\u800c\u8bbe\u8ba1\u7684 Transformer \u5df2\u88ab\u63d0\u51fa\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7c92\u5ea6\u7ec6\u8282\u4e0d\u8db3\uff0c\u5b83\u4eec\u6709\u65f6\u53ef\u80fd\u7f3a\u4e4f\u7cbe\u786e\u7684\u5b9a\u4f4d\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 TransDAE\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u91cd\u65b0\u6784\u60f3\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u5305\u542b\u6574\u4e2a\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7a7a\u95f4\u548c\u901a\u9053\u5173\u8054\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0cTransDAE \u901a\u8fc7\u5c3a\u5ea6\u95f4\u4ea4\u4e92\u6a21\u5757\u589e\u5f3a\u4e86\u8df3\u8dc3\u8fde\u63a5\u8def\u5f84\uff0c\u4fc3\u8fdb\u4e86\u7279\u5f81\u91cd\u7528\u5e76\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6743\u91cd\uff0cTransDAE \u5728 Synaps \u591a\u5668\u5b98\u6570\u636e\u96c6\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "author": "Bobby Azad et.al.", "authors": "Bobby Azad, Pourya Adibfar, Kaiqun Fu", "id": "2409.02018v1", "paper_url": "http://arxiv.org/abs/2409.02018v1", "repo": "null"}}