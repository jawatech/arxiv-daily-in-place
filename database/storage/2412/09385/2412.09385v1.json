{"2412.09385": {"publish_time": "2024-12-12", "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", "paper_summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.", "paper_summary_zh": "<paragraph>\u6211\u5011\u59d4\u8a17 16 \u500b\u6700\u5148\u9032\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f30\u8a08\u4eba\u5de5\u901a\u7528\u667a\u6167 (AGI) \u5728 2030 \u5e74\u51fa\u73fe\u7684\u53ef\u80fd\u6027\u3002\u70ba\u4e86\u8a55\u4f30\u9019\u4e9b\u9810\u6e2c\u7684\u54c1\u8cea\uff0c\u6211\u5011\u5be6\u65bd\u4e86\u4e00\u9805\u81ea\u52d5\u5316\u7684\u540c\u5115\u5be9\u67e5\u6d41\u7a0b (LLM-PR)\u3002LLM \u7684\u4f30\u8a08\u5dee\u7570\u5f88\u5927\uff0c\u5f9e 3% (Reka-Core) \u5230 47.6% (GPT-4o) \u4e0d\u7b49\uff0c\u4e2d\u4f4d\u6578\u70ba 12.5%\u3002\u9019\u4e9b\u4f30\u8a08\u8207\u6700\u8fd1\u5c08\u5bb6\u8abf\u67e5\u5bc6\u5207\u4e00\u81f4\uff0c\u8a72\u8abf\u67e5\u9810\u6e2c AGI \u5728 2027 \u5e74\u51fa\u73fe\u7684\u53ef\u80fd\u6027\u70ba 10%\uff0c\u9019\u5f37\u8abf\u4e86 LLM \u5728\u9810\u6e2c\u8907\u96dc\u3001\u63a8\u6e2c\u6027\u5834\u666f\u4e2d\u7684\u76f8\u95dc\u6027\u3002LLM-PR \u6d41\u7a0b\u8868\u73fe\u51fa\u5f88\u5f37\u7684\u53ef\u9760\u6027\uff0c\u9019\u7531\u9ad8\u985e\u5167\u76f8\u95dc\u4fc2\u6578 (ICC = 0.79) \u8b49\u660e\uff0c\u53cd\u6620\u51fa\u6a21\u578b\u4e4b\u9593\u8a55\u5206\u7684\u986f\u8457\u4e00\u81f4\u6027\u3002\u5728\u9019\u4e9b\u6a21\u578b\u4e2d\uff0cPplx-70b-online \u6210\u70ba\u8868\u73fe\u6700\u4f73\u8005\uff0c\u800c Gemini-1.5-pro-api \u6392\u540d\u6700\u4f4e\u3002\u8207\u5916\u90e8\u57fa\u6e96\uff08\u4f8b\u5982 LMSYS Chatbot Arena\uff09\u7684\u4ea4\u53c9\u6bd4\u8f03\u986f\u793a\uff0cLLM \u6392\u540d\u5728\u4e0d\u540c\u7684\u8a55\u4f30\u65b9\u6cd5\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u9019\u8868\u660e\u73fe\u6709\u7684\u57fa\u6e96\u53ef\u80fd\u7121\u6cd5\u6982\u62ec\u8207 AGI \u9810\u6e2c\u76f8\u95dc\u7684\u4e00\u4e9b\u6280\u80fd\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u57fa\u65bc\u5916\u90e8\u57fa\u6e96\u7684\u52a0\u6b0a\u65b9\u6848\u7684\u4f7f\u7528\uff0c\u512a\u5316\u4e86 LLM \u9810\u6e2c\u8207\u4eba\u985e\u5c08\u5bb6\u9810\u6e2c\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5206\u6790\u5c0e\u81f4\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7684\u300cAGI \u57fa\u6e96\u300d\uff0c\u65e8\u5728\u5f37\u8abf AGI \u76f8\u95dc\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\u5dee\u7570\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63d0\u4f9b\u4e86\u5c0d LLM \u5728\u63a8\u6e2c\u6027\u3001\u8de8\u5b78\u79d1\u9810\u6e2c\u4efb\u52d9\u4e2d\u7684\u80fd\u529b\u7684\u898b\u89e3\uff0c\u4e26\u5f37\u8abf\u4e86\u5728\u8907\u96dc\u3001\u4e0d\u78ba\u5b9a\u7684\u73fe\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u8a55\u4f30 AI \u6548\u80fd\u6642\uff0c\u5c0d\u5275\u65b0\u8a55\u4f30\u6846\u67b6\u65e5\u76ca\u589e\u9577\u7684\u9700\u6c42\u3002</paragraph>", "author": "Fabrizio Davide et.al.", "authors": "Fabrizio Davide, Pietro Torre, Andrea Gaggioli", "id": "2412.09385v1", "paper_url": "http://arxiv.org/abs/2412.09385v1", "repo": "null"}}