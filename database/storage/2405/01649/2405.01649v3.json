{"2405.01649": {"publish_time": "2024-05-02", "title": "Improving Complex Reasoning over Knowledge Graph with Logic-Aware Curriculum Tuning", "paper_summary": "Answering complex queries over incomplete knowledge graphs (KGs) is a\nchallenging job. Most previous works have focused on learning entity/relation\nembeddings and simulating first-order logic operators with various neural\nnetworks. However, they are bottlenecked by the inability to share world\nknowledge to improve logical reasoning, thus resulting in suboptimal\nperformance. In this paper, we propose a complex reasoning schema over KG upon\nlarge language models (LLMs), containing a curriculum-based logical-aware\ninstruction tuning framework, named LACT. Specifically, we augment the\narbitrary first-order logical queries via binary tree decomposition, to\nstimulate the reasoning capability of LLMs. To address the difficulty gap among\ndifferent types of complex queries, we design a simple and flexible logic-aware\ncurriculum learning framework. Experiments across widely used datasets\ndemonstrate that LACT has substantial improvements~(brings an average +5.5% MRR\nscore) over advanced methods, achieving the new state-of-the-art. Our code and\nmodel will be released at GitHub and huggingface soon.", "paper_summary_zh": "\u56de\u7b54\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31 (KG) \u4e0a\u7684\u590d\u6742\u67e5\u8be2\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u5de5\u4f5c\u3002\u5927\u591a\u6570\u4ee5\u524d\u7684\u4f5c\u54c1\u90fd\u4e13\u6ce8\u4e8e\u5b66\u4e60\u5b9e\u4f53/\u5173\u7cfb\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u4e00\u9636\u903b\u8f91\u8fd0\u7b97\u7b26\u3002\u7136\u800c\uff0c\u5b83\u4eec\u56e0\u65e0\u6cd5\u5171\u4eab\u4e16\u754c\u77e5\u8bc6\u6765\u6539\u8fdb\u903b\u8f91\u63a8\u7406\u800c\u6210\u4e3a\u74f6\u9888\uff0c\u4ece\u800c\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7684 KG \u4e0a\u7684\u590d\u6742\u63a8\u7406\u6a21\u5f0f\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u8bfe\u7a0b\u7684\u903b\u8f91\u611f\u77e5\u6307\u4ee4\u8c03\u6574\u6846\u67b6\uff0c\u540d\u4e3a LACT\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u4e8c\u53c9\u6811\u5206\u89e3\u6765\u6269\u5145\u4efb\u610f\u4e00\u9636\u903b\u8f91\u67e5\u8be2\uff0c\u4ee5\u6fc0\u53d1 LLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0d\u540c\u7c7b\u578b\u590d\u6742\u67e5\u8be2\u4e4b\u95f4\u7684\u96be\u5ea6\u5dee\u5f02\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u4e14\u7075\u6d3b\u7684\u903b\u8f91\u611f\u77e5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLACT \u5728\u5148\u8fdb\u65b9\u6cd5\u4e0a\u6709\u4e86\u5b9e\u8d28\u6027\u7684\u6539\u8fdb\uff08\u5e26\u6765\u4e86\u5e73\u5747 +5.5% \u7684 MRR \u5206\u6570\uff09\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f88\u5feb\u5728 GitHub \u548c huggingface \u4e0a\u53d1\u5e03\u3002", "author": "Tianle Xia et.al.", "authors": "Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao", "id": "2405.01649v3", "paper_url": "http://arxiv.org/abs/2405.01649v3", "repo": "null"}}