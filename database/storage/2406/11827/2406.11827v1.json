{"2406.11827": {"publish_time": "2024-06-17", "title": "WPO: Enhancing RLHF with Weighted Preference Optimization", "paper_summary": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B\nmodel on the leaderboard. We will release the code and models at\nhttps://github.com/wzhouad/WPO.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u66f4\u7b26\u5408\u4eba\u985e\u50f9\u503c\u89c0\u7684\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u96e2\u7dda\u504f\u597d\u6700\u4f73\u5316\uff08\u504f\u597d\u6578\u64da\u5f9e\u5176\u4ed6\u6a21\u578b\u53d6\u5f97\uff09\u56e0\u5176\u6210\u672c\u6548\u76ca\u548c\u53ef\u64f4\u5145\u6027\u800c\u5ee3\u6cdb\u63a1\u7528\u3002\u7136\u800c\uff0c\u96e2\u7dda\u504f\u597d\u6700\u4f73\u5316\u7d93\u5e38\u6703\u56e0\u7528\u65bc\u6578\u64da\u6536\u96c6\u7684\u653f\u7b56\u548c\u76ee\u6a19\u653f\u7b56\u4e4b\u9593\u7684\u5206\u914d\u5dee\u8ddd\u800c\u53d7\u82e6\uff0c\u5c0e\u81f4\u6b21\u4f73\u6700\u4f73\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u7b56\u7565\uff0c\u900f\u904e\u6a21\u64ec\u5728\u7dda\u653f\u7b56\u5b78\u7fd2\u8207\u96e2\u7dda\u504f\u597d\u6578\u64da\u4f86\u6e1b\u8f15\u6b64\u554f\u984c\u3002\u6211\u5011\u7684\u52a0\u6b0a\u504f\u597d\u6700\u4f73\u5316 (WPO) \u65b9\u6cd5\u6703\u8abf\u6574\u96e2\u7dda\u6578\u64da\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u5728\u7dda\u653f\u7b56\u6578\u64da\uff0c\u65b9\u6cd5\u662f\u6839\u64da\u7576\u524d\u653f\u7b56\u7684\u6a5f\u7387\u91cd\u65b0\u52a0\u6b0a\u504f\u597d\u914d\u5c0d\u3002\u6b64\u65b9\u6cd5\u4e0d\u50c5\u89e3\u6c7a\u4e86\u5206\u914d\u5dee\u8ddd\u554f\u984c\uff0c\u9084\u589e\u5f37\u4e86\u6700\u4f73\u5316\u6d41\u7a0b\uff0c\u800c\u4e0d\u6703\u7522\u751f\u984d\u5916\u6210\u672c\u3002\u6211\u5011\u5728\u5305\u62ec Alpaca Eval 2 \u548c MT-bench \u5728\u5167\u7684\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u4e0a\u9a57\u8b49\u4e86\u6211\u5011\u7684\u6a21\u578b\u3002WPO \u4e0d\u50c5\u5728 Alpaca Eval 2 \u4e0a\u6bd4\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u9ad8\u51fa 5.6%\uff0c\u9084\u6839\u64da Llama-3-8B-Instruct \u5efa\u7acb\u4e86\u8207 GPT-4-turbo \u76f8\u6297\u8861\u7684 48.6% \u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\uff0c\u4f7f\u5176\u6210\u70ba\u6392\u884c\u699c\u4e0a\u6700\u5f37\u5927\u7684 8B \u6a21\u578b\u3002\u6211\u5011\u5c07\u5728 https://github.com/wzhouad/WPO \u91cb\u51fa\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u3002", "author": "Wenxuan Zhou et.al.", "authors": "Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu", "id": "2406.11827v1", "paper_url": "http://arxiv.org/abs/2406.11827v1", "repo": "https://github.com/wzhouad/wpo"}}