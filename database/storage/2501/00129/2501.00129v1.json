{"2501.00129": {"publish_time": "2024-12-30", "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection", "paper_summary": "Introduction: Healthcare AI models often inherit biases from their training\ndata. While efforts have primarily targeted bias in structured data, mental\nhealth heavily depends on unstructured data. This study aims to detect and\nmitigate linguistic differences related to non-biological differences in the\ntraining data of AI models designed to assist in pediatric mental health\nscreening. Our objectives are: (1) to assess the presence of bias by evaluating\noutcome parity across sex subgroups, (2) to identify bias sources through\ntextual distribution analysis, and (3) to develop a de-biasing method for\nmental health text data. Methods: We examined classification parity across\ndemographic groups and assessed how gendered language influences model\npredictions. A data-centric de-biasing method was applied, focusing on\nneutralizing biased terms while retaining salient clinical information. This\nmethodology was tested on a model for automatic anxiety detection in pediatric\npatients. Results: Our findings revealed a systematic under-diagnosis of female\nadolescent patients, with a 4% lower accuracy and a 9% higher False Negative\nRate (FNR) compared to male patients, likely due to disparities in information\ndensity and linguistic differences in patient notes. Notes for male patients\nwere on average 500 words longer, and linguistic similarity metrics indicated\ndistinct word distributions between genders. Implementing our de-biasing\napproach reduced diagnostic bias by up to 27%, demonstrating its effectiveness\nin enhancing equity across demographic groups. Discussion: We developed a\ndata-centric de-biasing framework to address gender-based content disparities\nwithin clinical text. By neutralizing biased language and enhancing focus on\nclinically essential information, our approach demonstrates an effective\nstrategy for mitigating bias in AI healthcare models trained on text.", "paper_summary_zh": "<paragraph>\u5f15\u8a00\uff1a\u91ab\u7642\u4fdd\u5065 AI \u6a21\u578b\u901a\u5e38\u6703\u5f9e\u5176\u8a13\u7df4\u8cc7\u6599\u4e2d\u7e7c\u627f\u504f\u898b\u3002\u96d6\u7136\u52aa\u529b\u4e3b\u8981\u91dd\u5c0d\u7d50\u69cb\u5316\u8cc7\u6599\u4e2d\u7684\u504f\u898b\uff0c\u4f46\u5fc3\u7406\u5065\u5eb7\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8cf4\u65bc\u975e\u7d50\u69cb\u5316\u8cc7\u6599\u3002\u672c\u7814\u7a76\u65e8\u5728\u6aa2\u6e2c\u4e26\u6e1b\u8f15\u8207\u8a2d\u8a08\u7528\u65bc\u5354\u52a9\u5152\u7ae5\u5fc3\u7406\u5065\u5eb7\u7be9\u6aa2\u7684 AI \u6a21\u578b\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u975e\u751f\u7269\u5dee\u7570\u76f8\u95dc\u7684\u8a9e\u8a00\u5dee\u7570\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\uff1a(1) \u900f\u904e\u8a55\u4f30\u4e0d\u540c\u6027\u5225\u5b50\u7fa4\u9ad4\u7684\u7d50\u679c\u5e73\u50f9\u4f86\u8a55\u4f30\u504f\u898b\u7684\u5b58\u5728\uff0c(2) \u900f\u904e\u6587\u672c\u5206\u4f48\u5206\u6790\u4f86\u627e\u51fa\u504f\u898b\u4f86\u6e90\uff0c\u4ee5\u53ca (3) \u958b\u767c\u4e00\u7a2e\u5fc3\u7406\u5065\u5eb7\u6587\u672c\u8cc7\u6599\u7684\u53bb\u504f\u898b\u65b9\u6cd5\u3002\u65b9\u6cd5\uff1a\u6211\u5011\u6aa2\u67e5\u4e86\u4e0d\u540c\u4eba\u53e3\u7fa4\u9ad4\u7684\u5206\u985e\u5e73\u50f9\uff0c\u4e26\u8a55\u4f30\u4e86\u6027\u5225\u8a9e\u8a00\u5982\u4f55\u5f71\u97ff\u6a21\u578b\u9810\u6e2c\u3002\u61c9\u7528\u4e86\u4e00\u7a2e\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u53bb\u504f\u898b\u65b9\u6cd5\uff0c\u5c08\u6ce8\u65bc\u5728\u4fdd\u7559\u986f\u8457\u81e8\u5e8a\u8cc7\u8a0a\u7684\u540c\u6642\u4e2d\u548c\u6709\u504f\u898b\u7684\u8853\u8a9e\u3002\u6b64\u65b9\u6cd5\u5728\u4e00\u500b\u7528\u65bc\u5152\u7ae5\u60a3\u8005\u81ea\u52d5\u7126\u616e\u6aa2\u6e2c\u7684\u6a21\u578b\u4e0a\u9032\u884c\u4e86\u6e2c\u8a66\u3002\u7d50\u679c\uff1a\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u5c0d\u5973\u6027\u9752\u5c11\u5e74\u60a3\u8005\u7684\u7cfb\u7d71\u6027\u8a3a\u65b7\u4e0d\u8db3\uff0c\u8207\u7537\u6027\u60a3\u8005\u76f8\u6bd4\uff0c\u6e96\u78ba\u7387\u4f4e\u4e86 4%\uff0c\u5047\u9670\u6027\u7387 (FNR) \u9ad8\u4e86 9%\uff0c\u9019\u53ef\u80fd\u662f\u7531\u65bc\u60a3\u8005\u5099\u8a3b\u4e2d\u8cc7\u8a0a\u5bc6\u5ea6\u548c\u8a9e\u8a00\u5dee\u7570\u7684\u5dee\u7570\u3002\u7537\u6027\u60a3\u8005\u7684\u5099\u8a3b\u5e73\u5747\u9577 500 \u500b\u5b57\uff0c\u8a9e\u8a00\u76f8\u4f3c\u6027\u6307\u6a19\u986f\u793a\u4e0d\u540c\u6027\u5225\u4e4b\u9593\u7684\u5b57\u8a5e\u5206\u4f48\u622a\u7136\u4e0d\u540c\u3002\u5be6\u65bd\u6211\u5011\u7684\u53bb\u504f\u898b\u65b9\u6cd5\u5c07\u8a3a\u65b7\u504f\u898b\u964d\u4f4e\u4e86 27%\uff0c\u8b49\u660e\u4e86\u5176\u5728\u63d0\u5347\u4e0d\u540c\u4eba\u53e3\u7fa4\u9ad4\u4e4b\u9593\u516c\u5e73\u6027\u7684\u6709\u6548\u6027\u3002\u8a0e\u8ad6\uff1a\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u53bb\u504f\u898b\u67b6\u69cb\uff0c\u7528\u65bc\u89e3\u6c7a\u81e8\u5e8a\u6587\u672c\u4e2d\u7684\u57fa\u65bc\u6027\u5225\u7684\u5167\u5bb9\u5dee\u7570\u3002\u900f\u904e\u4e2d\u548c\u6709\u504f\u898b\u7684\u8a9e\u8a00\u548c\u52a0\u5f37\u5c0d\u81e8\u5e8a\u5fc5\u8981\u8cc7\u8a0a\u7684\u95dc\u6ce8\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5c55\u793a\u4e86\u4e00\u7a2e\u6709\u6548\u7b56\u7565\uff0c\u7528\u65bc\u6e1b\u8f15\u5728\u6587\u672c\u4e0a\u8a13\u7df4\u7684 AI \u91ab\u7642\u4fdd\u5065\u6a21\u578b\u4e2d\u7684\u504f\u898b\u3002</paragraph>", "author": "Julia Ive et.al.", "authors": "Julia Ive, Paulina Bondaronek, Vishal Yadav, Daniel Santel, Tracy Glauser, Tina Cheng, Jeffrey R. Strawn, Greeshma Agasthya, Jordan Tschida, Sanghyun Choo, Mayanka Chandrashekar, Anuj J. Kapadia, John Pestian", "id": "2501.00129v1", "paper_url": "http://arxiv.org/abs/2501.00129v1", "repo": "null"}}