{"2405.10853": {"publish_time": "2024-05-17", "title": "The Future of Large Language Model Pre-training is Federated", "paper_summary": "Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources we can leverage for pre-training. Federated learning (FL) has the\npotential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. This would mobilize more computational and data\nresources while matching or potentially exceeding centralized performance. We\nfurther show the effectiveness of the federated training scales with model size\nand present our approach for training a billion-scale federated LLM using\nlimited resources. This will help data-rich actors to become the protagonists\nof LLMs pre-training instead of leaving the stage to compute-rich actors alone.", "paper_summary_zh": "\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u8fd9\u8981\u5f52\u529f\u4e8e\u5b83\u4eec\u63a5\u53d7\u8fc7\u524d\u6240\u672a\u6709\u7684\u6570\u636e\u8bad\u7ec3\u3002\u6b63\u5982\u65e2\u5b9a\u7684\u7f29\u653e\u5b9a\u5f8b\u6240\u8868\u660e\u7684\uff0cLLM \u672a\u6765\u6027\u80fd\u7684\u63d0\u5347\u53d6\u51b3\u4e8e\u6211\u4eec\u53ef\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u8ba1\u7b97\u91cf\u548c\u6570\u636e\u6e90\u3002\u8054\u90a6\u5b66\u4e60 (FL) \u6709\u53ef\u80fd\u91ca\u653e\u5730\u7403\u4e0a\u5927\u90e8\u5206\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u8fd9\u4e9b\u8d44\u6e90\u76ee\u524d\u5e76\u672a\u5f97\u5230\u4ee5\u6570\u636e\u4e2d\u5fc3\u4e3a\u4e2d\u5fc3\u7684\u5f53\u524d LLM \u5b9e\u8df5\u8bad\u7ec3\u65b9\u6cd5\u7684\u5145\u5206\u5229\u7528\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u3001\u7075\u6d3b\u3001\u53ef\u590d\u5236\u7684 FL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u8de8\u673a\u6784\u8fdb\u884c\u5927\u89c4\u6a21\u534f\u4f5c\u4ee5\u8bad\u7ec3 LLM\u3002\u8fd9\u5c06\u8c03\u52a8\u66f4\u591a\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\uff0c\u540c\u65f6\u5339\u914d\u6216\u53ef\u80fd\u8d85\u8fc7\u96c6\u4e2d\u5f0f\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u8054\u90a6\u8bad\u7ec3\u89c4\u6a21\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6211\u4eec\u4f7f\u7528\u6709\u9650\u8d44\u6e90\u8bad\u7ec3\u5341\u4ebf\u7ea7\u8054\u90a6 LLM \u7684\u65b9\u6cd5\u3002\u8fd9\u5c06\u5e2e\u52a9\u6570\u636e\u4e30\u5bcc\u7684\u53c2\u4e0e\u8005\u6210\u4e3a LLM \u9884\u8bad\u7ec3\u7684\u4e3b\u89d2\uff0c\u800c\u4e0d\u662f\u5c06\u821e\u53f0\u7559\u7ed9\u4ec5\u6709\u7684\u8ba1\u7b97\u4e30\u5bcc\u7684\u53c2\u4e0e\u8005\u3002", "author": "Lorenzo Sani et.al.", "authors": "Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, Nicholas D. Lane", "id": "2405.10853v1", "paper_url": "http://arxiv.org/abs/2405.10853v1", "repo": "null"}}