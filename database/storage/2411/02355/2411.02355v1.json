{"2411.02355": {"publish_time": "2024-11-04", "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization", "paper_summary": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u91cf\u5316\u5728\u63a8\u8ad6\u52a0\u901f\u65b9\u9762\u7684\u666e\u53ca\uff0c\u4f46\u5c0d\u65bc\u5404\u7a2e\u91cf\u5316\u683c\u5f0f\u76f8\u95dc\u7684\u6e96\u78ba\u6027\u6548\u80fd\u6b0a\u8861\u4ecd\u5b58\u5728\u8457\u76f8\u7576\u5927\u7684\u4e0d\u78ba\u5b9a\u6027\u3002\n\u6211\u5011\u91dd\u5c0d\u91cf\u5316\u6e96\u78ba\u6027\u63d0\u51fa\u4e86\u4e00\u9805\u5168\u9762\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u5728\u6574\u500b Llama-3.1 \u6a21\u578b\u7cfb\u5217\u4e2d\uff0c\u91dd\u5c0d\u5b78\u8853\u57fa\u6e96\u548c\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u8a55\u4f30\u71b1\u9580\u7684\u91cf\u5316\u683c\u5f0f (FP8\u3001INT8\u3001INT4)\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u91cf\u5316\u6a21\u578b\u7522\u751f\u7684\u6587\u5b57\u8207\u5176\u672a\u58d3\u7e2e\u5c0d\u61c9\u6587\u5b57\u4e4b\u9593\u7684\u5dee\u7570\u3002\u9664\u4e86\u57fa\u6e96\u4e4b\u5916\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u4e9b\u91cf\u5316\u6539\u9032\uff0c\u8b93\u6211\u5011\u5f97\u4ee5\u7372\u5f97\u6700\u5148\u9032\u7684\u6e96\u78ba\u6027\u5fa9\u539f\u7d50\u679c\u3002\u6211\u5011\u7684\u8abf\u67e5\u6db5\u84cb\u8d85\u904e 500,000 \u500b\u500b\u5225\u8a55\u4f30\uff0c\u7522\u751f\u4e86\u5e7e\u500b\u95dc\u9375\u767c\u73fe\uff1a(1) FP8 \u6b0a\u91cd\u548c\u555f\u7528\u91cf\u5316 (W8A8-FP) \u5728\u6240\u6709\u6a21\u578b\u898f\u6a21\u4e2d\u90fd\u662f\u7121\u640d\u7684\uff0c(2) INT8 \u6b0a\u91cd\u548c\u555f\u7528\u91cf\u5316 (W8A8-INT) \u5728\u7d93\u904e\u9069\u7576\u8abf\u6574\u5f8c\uff0c\u4ee4\u4eba\u9a5a\u8a1d\u5730\u50c5\u9020\u6210 1-3% \u7684\u6e96\u78ba\u5ea6\u4e0b\u964d\uff0c\u4ee5\u53ca (3) INT4 \u50c5\u6b0a\u91cd\u91cf\u5316 (W4A16-INT) \u8207 8 \u4f4d\u5143\u6574\u6578\u6b0a\u91cd\u548c\u555f\u7528\u91cf\u5316\u5177\u6709\u7af6\u722d\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u7d66\u5b9a\u90e8\u7f72\u74b0\u5883\u7684\u300c\u6700\u4f73\u300d\u683c\u5f0f\u554f\u984c\uff0c\u6211\u5011\u4f7f\u7528\u5404\u7a2e GPU \u67b6\u69cb\u4e0a\u7684\u71b1\u9580\u958b\u6e90 vLLM \u6846\u67b6\u9032\u884c\u63a8\u8ad6\u6548\u80fd\u5206\u6790\u3002\u6211\u5011\u767c\u73fe W4A16 \u70ba\u540c\u6b65\u90e8\u7f72\u548c\u4e2d\u968e GPU \u4e0a\u7684\u975e\u540c\u6b65\u90e8\u7f72\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u6210\u672c\u6548\u76ca\u3002\u540c\u6642\uff0cW8A8 \u683c\u5f0f\u5728\u4e2d\u5927\u578b\u6a21\u578b\u65bc\u9ad8\u968e GPU \u4e0a\u7684\u975e\u540c\u6b65\u300c\u9023\u7e8c\u6279\u6b21\u8655\u7406\u300d\u90e8\u7f72\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u6211\u5011\u7684\u7d50\u679c\u63d0\u4f9b\u4e86\u4e00\u7d44\u5be6\u7528\u7684\u6e96\u5247\uff0c\u53ef\u7528\u65bc\u5728\u5404\u7a2e\u898f\u6a21\u548c\u6548\u80fd\u9700\u6c42\u4e2d\u90e8\u7f72\u91cf\u5316\u7684 LLM\u3002", "author": "Eldar Kurtic et.al.", "authors": "Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh", "id": "2411.02355v1", "paper_url": "http://arxiv.org/abs/2411.02355v1", "repo": "null"}}