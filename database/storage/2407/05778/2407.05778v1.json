{"2407.05778": {"publish_time": "2024-07-08", "title": "When is the consistent prediction likely to be a correct prediction?", "paper_summary": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer\nobtained through large language models (LLMs) is more likely to be correct. In\nthis paper, we challenge this argument and propose a nuanced correction. Our\nobservations indicate that consistent answers derived through more computation\ni.e. longer reasoning texts, rather than simply the most consistent answer\nacross all outputs, are more likely to be correct. This is predominantly\nbecause we demonstrate that LLMs can autonomously produce chain-of-thought\n(CoT) style reasoning with no custom prompts merely while generating longer\nresponses, which lead to consistent predictions that are more accurate. In the\nzero-shot setting, by sampling Mixtral-8x7B model multiple times and\nconsidering longer responses, we achieve 86% of its self-consistency\nperformance obtained through zero-shot CoT prompting on the GSM8K and\nMultiArith datasets. Finally, we demonstrate that the probability of LLMs\ngenerating a longer response is quite low, highlighting the need for decoding\nstrategies conditioned on output length.", "paper_summary_zh": "\u81ea\u6211\u4e00\u81f4\u6027\uff08Wang \u7b49\u4eba\uff0c2023 \u5e74\uff09\u8868\u660e\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u83b7\u5f97\u7684\u6700\u4e00\u81f4\u7684\u7b54\u6848\u66f4\u6709\u53ef\u80fd\u662f\u6b63\u786e\u7684\u3002\u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e00\u8bba\u70b9\u63d0\u51fa\u8d28\u7591\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u81f4\u5165\u5fae\u7684\u4fee\u6b63\u3002\u6211\u4eec\u7684\u89c2\u5bdf\u8868\u660e\uff0c\u901a\u8fc7\u66f4\u591a\u8ba1\u7b97\uff08\u5373\u66f4\u957f\u7684\u63a8\u7406\u6587\u672c\uff09\u5f97\u51fa\u7684\u3001\u800c\u4e0d\u662f\u6240\u6709\u8f93\u51fa\u4e2d\u6700\u4e00\u81f4\u7684\u7b54\u6848\uff0c\u66f4\u6709\u53ef\u80fd\u662f\u6b63\u786e\u7684\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u6211\u4eec\u8bc1\u660e\u4e86 LLM \u53ef\u4ee5\u81ea\u4e3b\u751f\u6210\u601d\u7ef4\u94fe (CoT) \u6837\u5f0f\u7684\u63a8\u7406\uff0c\u800c\u65e0\u9700\u5728\u751f\u6210\u66f4\u957f\u7684\u54cd\u5e94\u65f6\u4ec5\u4ec5\u662f\u81ea\u5b9a\u4e49\u63d0\u793a\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u66f4\u51c6\u786e\u7684\u4e00\u81f4\u9884\u6d4b\u3002\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u591a\u6b21\u91c7\u6837 Mixtral-8x7B \u6a21\u578b\u5e76\u8003\u8651\u66f4\u957f\u7684\u54cd\u5e94\uff0c\u6211\u4eec\u5728 GSM8K \u548c MultiArith \u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u96f6\u6837\u672c CoT \u63d0\u793a\u83b7\u5f97\u7684\u81ea\u6211\u4e00\u81f4\u6027\u6027\u80fd\u8fbe\u5230 86%\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 LLM \u751f\u6210\u66f4\u957f\u54cd\u5e94\u7684\u6982\u7387\u975e\u5e38\u4f4e\uff0c\u8fd9\u7a81\u51fa\u4e86\u5bf9\u4ee5\u8f93\u51fa\u957f\u5ea6\u4e3a\u6761\u4ef6\u7684\u89e3\u7801\u7b56\u7565\u7684\u9700\u6c42\u3002", "author": "Alex Nguyen et.al.", "authors": "Alex Nguyen, Dheeraj Mekala, Chengyu Dong, Jingbo Shang", "id": "2407.05778v1", "paper_url": "http://arxiv.org/abs/2407.05778v1", "repo": "null"}}