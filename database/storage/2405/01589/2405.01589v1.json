{"2405.01589": {"publish_time": "2024-04-29", "title": "GPT-4 passes most of the 297 written Polish Board Certification Examinations", "paper_summary": "Introduction: Recently, the effectiveness of Large Language Models (LLMs) has\nincreased rapidly, allowing them to be used in a great number of applications.\nHowever, the risks posed by the generation of false information through LLMs\nsignificantly limit their applications in sensitive areas such as healthcare,\nhighlighting the necessity for rigorous validations to determine their utility\nand reliability. To date, no study has extensively compared the performance of\nLLMs on Polish medical examinations across a broad spectrum of specialties on a\nvery large dataset. Objectives: This study evaluated the performance of three\nGenerative Pretrained Transformer (GPT) models on the Polish Board\nCertification Exam (Pa\\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which\nconsists of 297 tests. Methods: We developed a software program to download and\nprocess PES exams and tested the performance of GPT models using OpenAI\nApplication Programming Interface. Results: Our findings reveal that GPT-3.5\ndid not pass any of the analyzed exams. In contrast, the GPT-4 models\ndemonstrated the capability to pass the majority of the exams evaluated, with\nthe most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The\nperformance of the GPT models varied significantly, displaying excellence in\nexams related to certain specialties while completely failing others.\nConclusions: The significant progress and impressive performance of LLM models\nhold great promise for the increased application of AI in the field of medicine\nin Poland. For instance, this advancement could lead to the development of\nAI-based medical assistants for healthcare professionals, enhancing the\nefficiency and accuracy of medical services.", "paper_summary_zh": "", "author": "Jakub Pokrywka et.al.", "authors": "Jakub Pokrywka,Jeremi Kaczmarek,Edward Gorzela\u0144czyk", "id": "2405.01589v1", "paper_url": "http://arxiv.org/abs/2405.01589v1", "repo": "null"}}