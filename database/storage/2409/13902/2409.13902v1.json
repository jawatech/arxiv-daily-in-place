{"2409.13902": {"publish_time": "2024-09-20", "title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology", "paper_summary": "Despite the potential of Large Language Models (LLMs) in medicine, they may\ngenerate responses lacking supporting evidence or based on hallucinated\nevidence. While Retrieval Augment Generation (RAG) is popular to address this\nissue, few studies implemented and evaluated RAG in downstream domain-specific\napplications. We developed a RAG pipeline with 70,000 ophthalmology-specific\ndocuments that retrieve relevant documents to augment LLMs during inference\ntime. In a case study on long-form consumer health questions, we systematically\nevaluated the responses including over 500 references of LLMs with and without\nRAG on 100 questions with 10 healthcare professionals. The evaluation focuses\non factuality of evidence, selection and ranking of evidence, attribution of\nevidence, and answer accuracy and completeness. LLMs without RAG provided 252\nreferences in total. Of which, 45.3% hallucinated, 34.1% consisted of minor\nerrors, and 20.6% were correct. In contrast, LLMs with RAG significantly\nimproved accuracy (54.5% being correct) and reduced error rates (18.8% with\nminor hallucinations and 26.7% with errors). 62.5% of the top 10 documents\nretrieved by RAG were selected as the top references in the LLM response, with\nan average ranking of 4.9. The use of RAG also improved evidence attribution\n(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight\ndecreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47\nto 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited\nhallucinated and erroneous evidence in the responses, raising concerns for\ndownstream applications in the medical domain. RAG substantially reduced the\nproportion of such evidence but encountered challenges.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u91ab\u5b78\u9818\u57df\u5177\u6709\u6f5b\u529b\uff0c\u4f46\u5b83\u5011\u53ef\u80fd\u6703\u7522\u751f\u7f3a\u4e4f\u652f\u6301\u8b49\u64da\u6216\u57fa\u65bc\u865b\u69cb\u8b49\u64da\u7684\u56de\u61c9\u3002\u96d6\u7136\u6aa2\u7d22\u64f4\u5145\u751f\u6210\uff08RAG\uff09\u5f88\u53d7\u6b61\u8fce\uff0c\u7528\u65bc\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5728\u4e0b\u6e38\u7279\u5b9a\u9818\u57df\u7684\u61c9\u7528\u4e2d\u5be6\u65bd\u548c\u8a55\u4f30 RAG\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b RAG \u7ba1\u7dda\uff0c\u5176\u4e2d\u5305\u542b 70,000 \u4efd\u7279\u5b9a\u65bc\u773c\u79d1\u7684\u6587\u4ef6\uff0c\u9019\u4e9b\u6587\u4ef6\u6703\u5728\u63a8\u7406\u6642\u9593\u6aa2\u7d22\u76f8\u95dc\u6587\u4ef6\u4ee5\u64f4\u5145 LLM\u3002\u5728\u91dd\u5c0d\u9577\u7bc7\u6d88\u8cbb\u8005\u5065\u5eb7\u554f\u984c\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u4e86 LLM \u7684\u56de\u61c9\uff0c\u5305\u62ec 100 \u500b\u554f\u984c\u4e2d 500 \u591a\u500b\u5f15\u7528\uff0c\u5176\u4e2d 10 \u500b\u554f\u984c\u7531 10 \u4f4d\u91ab\u7642\u4fdd\u5065\u5c08\u696d\u4eba\u54e1\u63d0\u51fa\u3002\u8a55\u4f30\u91cd\u9ede\u5728\u65bc\u8b49\u64da\u7684\u771f\u5be6\u6027\u3001\u8b49\u64da\u7684\u9078\u64c7\u548c\u6392\u540d\u3001\u8b49\u64da\u7684\u6b78\u56e0\uff0c\u4ee5\u53ca\u7b54\u6848\u7684\u6e96\u78ba\u6027\u548c\u5b8c\u6574\u6027\u3002\u6c92\u6709 RAG \u7684 LLM \u7e3d\u5171\u63d0\u4f9b\u4e86 252 \u500b\u53c3\u8003\u3002\u5176\u4e2d\uff0c45.3% \u662f\u865b\u69cb\u7684\uff0c34.1% \u5305\u542b\u8f15\u5fae\u932f\u8aa4\uff0c20.6% \u662f\u6b63\u78ba\u7684\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5e36\u6709 RAG \u7684 LLM \u5927\u5e45\u63d0\u9ad8\u4e86\u6e96\u78ba\u5ea6\uff0854.5% \u662f\u6b63\u78ba\u7684\uff09\u4e26\u964d\u4f4e\u4e86\u932f\u8aa4\u7387\uff0818.8% \u6709\u8f15\u5fae\u865b\u69cb\uff0c26.7% \u6709\u932f\u8aa4\uff09\u3002RAG \u6aa2\u7d22\u7684\u524d 10 \u4efd\u6587\u4ef6\u4e2d\u6709 62.5% \u88ab\u9078\u70ba LLM \u56de\u61c9\u4e2d\u7684\u9996\u8981\u53c3\u8003\uff0c\u5e73\u5747\u6392\u540d\u70ba 4.9\u3002RAG \u7684\u4f7f\u7528\u4e5f\u6539\u9032\u4e86\u8b49\u64da\u6b78\u56e0\uff08\u5728 5 \u5206\u91cf\u8868\u4e0a\u5f9e 1.85 \u589e\u52a0\u5230 2.49\uff0cP<0.001\uff09\uff0c\u5118\u7ba1\u6e96\u78ba\u5ea6\uff08\u5f9e 3.52 \u964d\u4f4e\u5230 3.23\uff0cP=0.03\uff09\u548c\u5b8c\u6574\u6027\uff08\u5f9e 3.47 \u964d\u4f4e\u5230 3.27\uff0cP=0.17\uff09\u7565\u6709\u4e0b\u964d\u3002\u7d50\u679c\u8868\u660e\uff0cLLM \u5728\u56de\u61c9\u4e2d\u7d93\u5e38\u8868\u73fe\u51fa\u865b\u69cb\u548c\u932f\u8aa4\u7684\u8b49\u64da\uff0c\u9019\u5f15\u8d77\u4e86\u5c0d\u91ab\u7642\u9818\u57df\u4e0b\u6e38\u61c9\u7528\u7a0b\u5e8f\u7684\u64d4\u6182\u3002RAG \u5927\u5e45\u6e1b\u5c11\u4e86\u6b64\u985e\u8b49\u64da\u7684\u6bd4\u4f8b\uff0c\u4f46\u9047\u5230\u4e86\u6311\u6230\u3002", "author": "Aidan Gilson et.al.", "authors": "Aidan Gilson, Xuguang Ai, Thilaka Arunachalam, Ziyou Chen, Ki Xiong Cheong, Amisha Dave, Cameron Duic, Mercy Kibe, Annette Kaminaka, Minali Prasad, Fares Siddig, Maxwell Singer, Wendy Wong, Qiao Jin, Tiarnan D. L. Keenan, Xia Hu, Emily Y. Chew, Zhiyong Lu, Hua Xu, Ron A. Adelman, Yih-Chung Tham, Qingyu Chen", "id": "2409.13902v1", "paper_url": "http://arxiv.org/abs/2409.13902v1", "repo": "null"}}