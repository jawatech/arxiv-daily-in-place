
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-07**|**QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving**|Yujun Lin et.al.|[2405.04532v1](http://arxiv.org/abs/2405.04532v1)|[link](https://github.com/mit-han-lab/qserve)|
|**2024-05-07**|**NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts**|Shudan Zhang et.al.|[2405.04520v1](http://arxiv.org/abs/2405.04520v1)|null|
|**2024-05-07**|**xLSTM: Extended Long Short-Term Memory**|Maximilian Beck et.al.|[2405.04517v1](http://arxiv.org/abs/2405.04517v1)|null|
|**2024-05-07**|**A Transformer with Stack Attention**|Jiaoda Li et.al.|[2405.04515v1](http://arxiv.org/abs/2405.04515v1)|[link](https://github.com/rycolab/stack-transformer)|
|**2024-05-07**|**Switchable Decision: Dynamic Neural Generation Networks**|Shujian Zhang et.al.|[2405.04513v1](http://arxiv.org/abs/2405.04513v1)|null|
|**2024-05-07**|**Toward In-Context Teaching: Adapting Examples to Students' Misconceptions**|Alexis Ross et.al.|[2405.04495v1](http://arxiv.org/abs/2405.04495v1)|null|
|**2024-05-07**|**TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters**|Jonathan Wilder Lavington et.al.|[2405.04491v1](http://arxiv.org/abs/2405.04491v1)|null|
|**2024-05-07**|**Towards Continual Knowledge Graph Embedding via Incremental Distillation**|Jiajun Liu et.al.|[2405.04453v1](http://arxiv.org/abs/2405.04453v1)|[link](https://github.com/seukgcode/incde)|
|**2024-05-07**|**POV Learning: Individual Alignment of Multimodal Models using Human Perception**|Simon Werner et.al.|[2405.04443v1](http://arxiv.org/abs/2405.04443v1)|null|
|**2024-05-07**|**AugmenTory: A Fast and Flexible Polygon Augmentation Library**|Tanaz Ghahremani et.al.|[2405.04442v1](http://arxiv.org/abs/2405.04442v1)|null|
|**2024-05-07**|**DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model**|DeepSeek-AI et.al.|[2405.04434v1](http://arxiv.org/abs/2405.04434v1)|[link](https://github.com/deepseek-ai/deepseek-v2)|
|**2024-05-07**|**Vision Mamba: A Comprehensive Survey and Taxonomy**|Xiao Liu et.al.|[2405.04404v1](http://arxiv.org/abs/2405.04404v1)|[link](https://github.com/lx6c78/vision-mamba-a-comprehensive-survey-and-taxonomy)|
|**2024-05-07**|**Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs**|Antonio Bikić et.al.|[2405.04386v1](http://arxiv.org/abs/2405.04386v1)|null|
|**2024-05-07**|**Leveraging LSTM and GAN for Modern Malware Detection**|Ishita Gupta et.al.|[2405.04373v1](http://arxiv.org/abs/2405.04373v1)|null|
|**2024-05-07**|**Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs**|Martin Marzidovšek et.al.|[2405.04372v1](http://arxiv.org/abs/2405.04372v1)|null|
|**2024-05-07**|**Global Scale Self-Supervised Channel Charting with Sensor Fusion**|Omid Esrafilian et.al.|[2405.04357v1](http://arxiv.org/abs/2405.04357v1)|null|
|**2024-05-07**|**Revisiting character-level adversarial attacks**|Elias Abad Rocamora et.al.|[2405.04346v1](http://arxiv.org/abs/2405.04346v1)|[link](https://github.com/lions-epfl/charmer)|
|**2024-05-07**|**Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**|Zhihao Wen et.al.|[2405.04336v1](http://arxiv.org/abs/2405.04336v1)|null|
|**2024-05-07**|**A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI**|Hannah Chafetz et.al.|[2405.04333v1](http://arxiv.org/abs/2405.04333v1)|null|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325v1](http://arxiv.org/abs/2405.04325v1)|null|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324v1](http://arxiv.org/abs/2405.04324v1)|[link](https://github.com/ibm-granite/granite-code-models)|
|**2024-05-07**|**Beyond human subjectivity and error: a novel AI grading system**|Alexandra Gobrecht et.al.|[2405.04323v1](http://arxiv.org/abs/2405.04323v1)|null|
|**2024-05-07**|**Cross-IQA: Unsupervised Learning for Image Quality Assessment**|Zhen Zhang et.al.|[2405.04311v1](http://arxiv.org/abs/2405.04311v1)|null|
|**2024-05-07**|**Improving Offline Reinforcement Learning with Inaccurate Simulators**|Yiwen Hou et.al.|[2405.04307v1](http://arxiv.org/abs/2405.04307v1)|null|
|**2024-05-07**|**A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**|Raiyan Rahman et.al.|[2405.04305v1](http://arxiv.org/abs/2405.04305v1)|null|
|**2024-05-07**|**Accelerating Speculative Decoding using Dynamic Speculation Length**|Jonathan Mamou et.al.|[2405.04304v1](http://arxiv.org/abs/2405.04304v1)|null|
|**2024-05-07**|**Behaviour Planning: A Toolkit for Diverse Planning**|Mustafa F Abdelwahed et.al.|[2405.04300v1](http://arxiv.org/abs/2405.04300v1)|null|
|**2024-05-07**|**Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework**|Xiangpeng Wan et.al.|[2405.04294v1](http://arxiv.org/abs/2405.04294v1)|null|
|**2024-05-07**|**Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning**|Sayantan Pal et.al.|[2405.04292v1](http://arxiv.org/abs/2405.04292v1)|null|
|**2024-05-07**|**Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore**|Junchao Wu et.al.|[2405.04286v1](http://arxiv.org/abs/2405.04286v1)|null|
|**2024-05-07**|**On the Foundations of Earth and Climate Foundation Models**|Xiao Xiang Zhu et.al.|[2405.04285v1](http://arxiv.org/abs/2405.04285v1)|null|
|**2024-05-07**|**Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats**|Arne Rubehn et.al.|[2405.04271v1](http://arxiv.org/abs/2405.04271v1)|null|
|**2024-05-07**|**VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting**|Alireza Koochali et.al.|[2405.04252v1](http://arxiv.org/abs/2405.04252v1)|null|
|**2024-05-07**|**Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks**|Caelin Kaplan et.al.|[2405.04249v1](http://arxiv.org/abs/2405.04249v1)|null|
|**2024-05-07**|**Exploring Correlations of Self-supervised Tasks for Graphs**|Taoran Fang et.al.|[2405.04245v1](http://arxiv.org/abs/2405.04245v1)|null|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219v1](http://arxiv.org/abs/2405.04219v1)|null|
|**2024-05-07**|**NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions**|Elliot Gestrin et.al.|[2405.04215v1](http://arxiv.org/abs/2405.04215v1)|null|
|**2024-05-07**|**Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks**|Sondre Glimsdal et.al.|[2405.04212v1](http://arxiv.org/abs/2405.04212v1)|null|
|**2024-05-07**|**NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator**|Mohit Upadhyay et.al.|[2405.04206v1](http://arxiv.org/abs/2405.04206v1)|null|
|**2024-05-07**|**FedStale: leveraging stale client updates in federated learning**|Angelo Rodio et.al.|[2405.04171v1](http://arxiv.org/abs/2405.04171v1)|null|
|**2024-05-07**|**D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models**|Duygu Altinok et.al.|[2405.04170v1](http://arxiv.org/abs/2405.04170v1)|null|
|**2024-05-07**|**LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection**|Jasraj Singh et.al.|[2405.04165v1](http://arxiv.org/abs/2405.04165v1)|null|
|**2024-05-07**|**MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization**|Gunjan Balde et.al.|[2405.04163v1](http://arxiv.org/abs/2405.04163v1)|[link](https://github.com/gb-kgp/medvoc)|
|**2024-05-07**|**A Causal Explainable Guardrails for Large Language Models**|Zhixuan Chu et.al.|[2405.04160v1](http://arxiv.org/abs/2405.04160v1)|null|
|**2024-05-07**|**GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness**|Nabil Al-Dhamari et.al.|[2405.04138v1](http://arxiv.org/abs/2405.04138v1)|null|
|**2024-05-07**|**Enriched BERT Embeddings for Scholarly Publication Classification**|Benjamin Wolff et.al.|[2405.04136v1](http://arxiv.org/abs/2405.04136v1)|null|
|**2024-05-07**|**In-context Learning for Automated Driving Scenarios**|Ziqi Zhou et.al.|[2405.04135v1](http://arxiv.org/abs/2405.04135v1)|null|
|**2024-05-07**|**Fine-grained Speech Sentiment Analysis in Chinese Psychological Support Hotlines Based on Large-scale Pre-trained Model**|Zhonglong Chen et.al.|[2405.04128v1](http://arxiv.org/abs/2405.04128v1)|[link](https://github.com/czl0914/psy_hotline_analysis)|
|**2024-05-07**|**Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling**|Riccardo Simionato et.al.|[2405.04124v1](http://arxiv.org/abs/2405.04124v1)|null|
|**2024-05-07**|**Policy Learning with a Language Bottleneck**|Megha Srivastava et.al.|[2405.04118v1](http://arxiv.org/abs/2405.04118v1)|[link](https://github.com/meghabyte/bottleneck)|
|**2024-05-07**|**A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model**|Tianxiu Xie et.al.|[2405.04108v1](http://arxiv.org/abs/2405.04108v1)|null|
|**2024-05-07**|**Continual Learning in the Presence of Repetition**|Hamed Hemati et.al.|[2405.04101v1](http://arxiv.org/abs/2405.04101v1)|null|
|**2024-05-07**|**Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes**|Ammarah Hashmi et.al.|[2405.04097v1](http://arxiv.org/abs/2405.04097v1)|null|
|**2024-05-07**|**Going Proactive and Explanatory Against Malware Concept Drift**|Yiling He et.al.|[2405.04095v1](http://arxiv.org/abs/2405.04095v1)|null|
|**2024-05-07**|**DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects**|Da Fu et.al.|[2405.04093v1](http://arxiv.org/abs/2405.04093v1)|null|
|**2024-05-07**|**Optimizing Language Model's Reasoning Abilities with Weak Supervision**|Yongqi Tong et.al.|[2405.04086v1](http://arxiv.org/abs/2405.04086v1)|null|
|**2024-05-07**|**Counterfactual and Semifactual Explanations in Abstract Argumentation: Formal Foundations, Complexity and Computation**|Gianvincenzo Alfano et.al.|[2405.04081v1](http://arxiv.org/abs/2405.04081v1)|null|
|**2024-05-07**|**WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer**|Kumar Shubham et.al.|[2405.04078v1](http://arxiv.org/abs/2405.04078v1)|null|
|**2024-05-07**|**A simple theory for training response of deep neural networks**|Kenichi Nakazato et.al.|[2405.04074v1](http://arxiv.org/abs/2405.04074v1)|null|
|**2024-05-07**|**FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference**|Runheng Liu et.al.|[2405.04065v1](http://arxiv.org/abs/2405.04065v1)|null|
|**2024-05-07**|**Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT**|Hassan Shakil et.al.|[2405.04053v1](http://arxiv.org/abs/2405.04053v1)|null|
|**2024-05-07**|**Learning Linear Block Error Correction Codes**|Yoni Choukroun et.al.|[2405.04050v1](http://arxiv.org/abs/2405.04050v1)|[link](https://github.com/yonilc/e2e_dc_ecct)|
|**2024-05-07**|**Philosophy of Cognitive Science in the Age of Deep Learning**|Raphaël Millière et.al.|[2405.04048v1](http://arxiv.org/abs/2405.04048v1)|null|
|**2024-05-07**|**Feature Map Convergence Evaluation for Functional Module**|Ludan Zhang et.al.|[2405.04041v1](http://arxiv.org/abs/2405.04041v1)|null|
|**2024-05-07**|**Utilizing GPT to Enhance Text Summarization: A Strategy to Minimize Hallucinations**|Hassan Shakil et.al.|[2405.04039v1](http://arxiv.org/abs/2405.04039v1)|null|
|**2024-05-07**|**Locally Differentially Private In-Context Learning**|Chunyan Zheng et.al.|[2405.04032v1](http://arxiv.org/abs/2405.04032v1)|null|
|**2024-05-07**|**Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties**|S. Akshay et.al.|[2405.04015v1](http://arxiv.org/abs/2405.04015v1)|null|
|**2024-05-07**|**Structured Click Control in Transformer-based Interactive Segmentation**|Long Xu et.al.|[2405.04009v1](http://arxiv.org/abs/2405.04009v1)|[link](https://github.com/hahamyt/scc)|
|**2024-05-07**|**Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches**|Chen Zhu-Tian et.al.|[2405.03998v1](http://arxiv.org/abs/2405.03998v1)|null|
|**2024-05-07**|**TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks**|Guanqiao Qu et.al.|[2405.03990v1](http://arxiv.org/abs/2405.03990v1)|null|
|**2024-05-07**|**Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application**|Jian Jia et.al.|[2405.03988v1](http://arxiv.org/abs/2405.03988v1)|null|
|**2024-05-07**|**Factors Influencing User Willingness To Use SORA**|Gustave Florentin Nkoulou Mvondo et.al.|[2405.03986v1](http://arxiv.org/abs/2405.03986v1)|null|
|**2024-05-07**|**TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments**|Ziyu Liu et.al.|[2405.03974v1](http://arxiv.org/abs/2405.03974v1)|null|
|**2024-05-07**|**ERATTA: Extreme RAG for Table To Answers with Large Language Models**|Sohini Roychowdhury et.al.|[2405.03963v1](http://arxiv.org/abs/2405.03963v1)|null|
|**2024-05-07**|**ESIHGNN: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition**|Xupeng Zha et.al.|[2405.03960v1](http://arxiv.org/abs/2405.03960v1)|null|
|**2024-05-07**|**Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model**|Joo Young Choi et.al.|[2405.03958v1](http://arxiv.org/abs/2405.03958v1)|null|
|**2024-05-07**|**HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech**|Zhongren Dong et.al.|[2405.03952v1](http://arxiv.org/abs/2405.03952v1)|null|
|**2024-05-07**|**Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**|Jiayuan Chen et.al.|[2405.03943v1](http://arxiv.org/abs/2405.03943v1)|[link](https://github.com/the-real-jerrychen/trans)|
|**2024-05-07**|**Long Context Alignment with Short Instructions and Synthesized Positions**|Wenhao Wu et.al.|[2405.03939v1](http://arxiv.org/abs/2405.03939v1)|null|
|**2024-05-07**|**CleanGraph: Human-in-the-loop Knowledge Graph Refinement and Completion**|Tyler Bikaun et.al.|[2405.03932v1](http://arxiv.org/abs/2405.03932v1)|[link](https://github.com/nlp-tlp/cleangraph)|
|**2024-05-07**|**Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations**|Jaesung Park et.al.|[2405.03929v1](http://arxiv.org/abs/2405.03929v1)|null|
|**2024-05-07**|**A Roadmap for Multilingual, Multimodal Domain Independent Deception Detection**|Dainis Boumber et.al.|[2405.03920v1](http://arxiv.org/abs/2405.03920v1)|null|
|**2024-05-06**|**OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs**|Jiahao Nick Li et.al.|[2405.03901v1](http://arxiv.org/abs/2405.03901v1)|null|
|**2024-05-06**|**Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows**|Minjae Cho et.al.|[2405.03892v1](http://arxiv.org/abs/2405.03892v1)|null|
|**2024-05-06**|**Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management**|Ravikumar Balakrishnan et.al.|[2405.03891v1](http://arxiv.org/abs/2405.03891v1)|null|
|**2024-05-06**|**Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer**|Huihong Shi et.al.|[2405.03882v1](http://arxiv.org/abs/2405.03882v1)|null|
|**2024-05-06**|**Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions**|Ziye Qin et.al.|[2405.03873v1](http://arxiv.org/abs/2405.03873v1)|null|
|**2024-05-06**|**AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration**|Widad Elouataoui et.al.|[2405.03870v1](http://arxiv.org/abs/2405.03870v1)|null|
|**2024-05-06**|**Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions**|Anshuman Chhabra et.al.|[2405.03869v1](http://arxiv.org/abs/2405.03869v1)|null|
|**2024-05-06**|**Learning Planning Abstractions from Language**|Weiyu Liu et.al.|[2405.03864v1](http://arxiv.org/abs/2405.03864v1)|null|
|**2024-05-06**|**Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration**|Razan Baltaji et.al.|[2405.03862v1](http://arxiv.org/abs/2405.03862v1)|null|
|**2024-05-06**|**VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images**|Anna Penzkofer et.al.|[2405.03852v1](http://arxiv.org/abs/2405.03852v1)|null|
|**2024-05-06**|**Self-Improving Customer Review Response Generation Based on LLMs**|Guy Azov et.al.|[2405.03845v1](http://arxiv.org/abs/2405.03845v1)|null|
|**2024-05-06**|**A Novel Cross-band CSI Prediction Scheme for Multi-band Fingerprint based Localization**|Yuan Ruihao et.al.|[2405.03842v1](http://arxiv.org/abs/2405.03842v1)|null|
|**2024-05-06**|**Guylingo: The Republic of Guyana Creole Corpora**|Christopher Clarke et.al.|[2405.03832v1](http://arxiv.org/abs/2405.03832v1)|null|
|**2024-05-06**|**Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence**|Silvan Ferreira et.al.|[2405.03825v1](http://arxiv.org/abs/2405.03825v1)|null|
|**2024-05-06**|**Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models**|Evan King et.al.|[2405.03821v1](http://arxiv.org/abs/2405.03821v1)|null|
|**2024-05-06**|**SocialFormer: Social Interaction Modeling with Edge-enhanced Heterogeneous Graph Transformers for Trajectory Prediction**|Zixu Wang et.al.|[2405.03809v1](http://arxiv.org/abs/2405.03809v1)|null|
|**2024-05-06**|**Synthetic Data from Diffusion Models Improve Drug Discovery Prediction**|Bing Hu et.al.|[2405.03799v1](http://arxiv.org/abs/2405.03799v1)|null|
|**2024-05-06**|**Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models**|Dengyi Liu et.al.|[2405.03794v1](http://arxiv.org/abs/2405.03794v1)|null|

#### Abstracts
##### **QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving**
2405.04532v1 by Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

Quantization can accelerate large language model (LLM) inference. Going
beyond INT8 quantization, the research community is actively exploring even
lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization
techniques only accelerate low-batch, edge LLM inference, failing to deliver
performance gains in large-batch, cloud-based LLM serving. We uncover a
critical issue: existing INT4 quantization methods suffer from significant
runtime overhead (20-90%) when dequantizing either weights or partial sums on
GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization
algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands
for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented
by the QServe inference library that achieves measured speedup. The key insight
driving QServe is that the efficiency of LLM serving on GPUs is critically
influenced by operations on low-throughput CUDA cores. Building upon this
insight, in QoQ algorithm, we introduce progressive quantization that can allow
low dequantization overhead in W4A8 GEMM. Additionally, we develop
SmoothAttention to effectively mitigate the accuracy degradation incurred by
4-bit KV quantization. In the QServe system, we perform compute-aware weight
reordering and take advantage of register-level parallelism to reduce
dequantization latency. We also make fused attention memory-bound, harnessing
the performance gain brought by KV4 quantization. As a result, QServe improves
the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x
on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to
TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput
than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of
LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.

摘要：

##### **NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts**
2405.04520v1 by Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, Jie Tang

Large language models (LLMs) have manifested strong ability to generate codes
for productive activities. However, current benchmarks for code synthesis, such
as HumanEval, MBPP, and DS-1000, are predominantly oriented towards
introductory tasks on algorithm and data science, insufficiently satisfying
challenging requirements prevalent in real-world coding. To fill this gap, we
propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror
the complexity and variety of scenarios in real coding tasks. NCB comprises 402
high-quality problems in Python and Java, meticulously selected from natural
user queries from online coding services, covering 6 different domains. Noting
the extraordinary difficulty in creating testing cases for real-world queries,
we also introduce a semi-automated pipeline to enhance the efficiency of test
case construction. Comparing with manual solutions, it achieves an efficiency
increase of more than 4 times. Our systematic experiments on 39 LLMs find that
performance gaps on NCB between models with close HumanEval scores could still
be significant, indicating a lack of focus on practical code synthesis
scenarios or over-specified optimization on HumanEval. On the other hand, even
the best-performing GPT-4 is still far from satisfying on NCB. The evaluation
toolkit and development set are available at
https://github.com/THUDM/NaturalCodeBench.

摘要：

##### **xLSTM: Extended Long Short-Term Memory**
2405.04517v1 by Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter

In the 1990s, the constant error carousel and gating were introduced as the
central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have
stood the test of time and contributed to numerous deep learning success
stories, in particular they constituted the first Large Language Models (LLMs).
However, the advent of the Transformer technology with parallelizable
self-attention at its core marked the dawn of a new era, outpacing LSTMs at
scale. We now raise a simple question: How far do we get in language modeling
when scaling LSTMs to billions of parameters, leveraging the latest techniques
from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we
introduce exponential gating with appropriate normalization and stabilization
techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM
with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that
is fully parallelizable with a matrix memory and a covariance update rule.
Integrating these LSTM extensions into residual block backbones yields xLSTM
blocks that are then residually stacked into xLSTM architectures. Exponential
gating and modified memory structures boost xLSTM capabilities to perform
favorably when compared to state-of-the-art Transformers and State Space
Models, both in performance and scaling.

摘要：

##### **A Transformer with Stack Attention**
2405.04515v1 by Jiaoda Li, Jennifer C. White, Mrinmaya Sachan, Ryan Cotterell

Natural languages are believed to be (mildly) context-sensitive. Despite
underpinning remarkably capable large language models, transformers are unable
to model many context-free language tasks. In an attempt to address this
limitation in the modeling power of transformer-based language models, we
propose augmenting them with a differentiable, stack-based attention mechanism.
Our stack-based attention mechanism can be incorporated into any
transformer-based language model and adds a level of interpretability to the
model. We show that the addition of our stack-based attention mechanism enables
the transformer to model some, but not all, deterministic context-free
languages.

摘要：

##### **Switchable Decision: Dynamic Neural Generation Networks**
2405.04513v1 by Shujian Zhang, Korawat Tanwisuth, Chengyue Gong, Pengcheng He, Mingyuan Zhou

Auto-regressive generation models achieve competitive performance across many
different NLP tasks such as summarization, question answering, and
classifications. However, they are also known for being slow in inference,
which makes them challenging to deploy in real-time applications. We propose a
switchable decision to accelerate inference by dynamically assigning
computation resources for each data instance. Automatically making decisions on
where to skip and how to balance quality and computation cost with constrained
optimization, our dynamic neural generation networks enforce the efficient
inference path and determine the optimized trade-off. Experiments across
question answering, summarization, and classification benchmarks show that our
method benefits from less computation cost during inference while keeping the
same accuracy. Extensive experiments and ablation studies demonstrate that our
method can be general, effective, and beneficial for many NLP tasks.

摘要：

##### **Toward In-Context Teaching: Adapting Examples to Students' Misconceptions**
2405.04495v1 by Alexis Ross, Jacob Andreas

When a teacher provides examples for a student to study, these examples must
be informative, enabling a student to progress from their current state toward
a target concept or skill. Good teachers must therefore simultaneously infer
what students already know and adapt their teaching to students' changing state
of knowledge. There is increasing interest in using computational models,
particularly large language models, as pedagogical tools. As students, language
models in particular have shown a remarkable ability to adapt to new tasks
given small numbers of examples. But how effectively can these models adapt as
teachers to students of different types? To study this question, we introduce a
suite of models and evaluation methods we call AdapT. AdapT has two components:
(1) a collection of simulated Bayesian student models that can be used for
evaluation of automated teaching methods; (2) a platform for evaluation with
human students, to characterize the real-world effectiveness of these methods.
We additionally introduce (3) AToM, a new probabilistic model for adaptive
teaching that jointly infers students' past beliefs and optimizes for the
correctness of future beliefs. In evaluations of simulated students across
three learning domains (fraction arithmetic, English morphology, function
learning), AToM systematically outperforms LLM-based and standard Bayesian
teaching models. In human experiments, both AToM and LLMs outperform
non-adaptive random example selection. Our results highlight both the
difficulty of the adaptive teaching task and the potential of learned adaptive
models for solving it.

摘要：

##### **TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters**
2405.04491v1 by Jonathan Wilder Lavington, Ke Zhang, Vasileios Lioutas, Matthew Niedoba, Yunpeng Liu, Dylan Green, Saeid Naderiparizi, Xiaoxuan Liang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood

The training, testing, and deployment, of autonomous vehicles requires
realistic and efficient simulators. Moreover, because of the high variability
between different problems presented in different autonomous systems, these
simulators need to be easy to use, and easy to modify. To address these
problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.
TorchDriveEnv is a lightweight reinforcement learning benchmark programmed
entirely in Python, which can be modified to test a number of different factors
in learned vehicle behavior, including the effect of varying kinematic models,
agent types, and traffic control patterns. Most importantly unlike many replay
based simulation approaches, TorchDriveEnv is fully integrated with a state of
the art behavioral simulation API. This allows users to train and evaluate
driving models alongside data driven Non-Playable Characters (NPC) whose
initializations and driving behavior are reactive, realistic, and diverse. We
illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common
reinforcement learning baselines in both training and validation environments.
Our experiments show that TorchDriveEnv is easy to use, but difficult to solve.

摘要：

##### **Towards Continual Knowledge Graph Embedding via Incremental Distillation**
2405.04453v1 by Jiajun Liu, Wenjun Ke, Peng Wang, Ziyu Shang, Jinhua Gao, Guozheng Li, Ke Ji, Yanhe Liu

Traditional knowledge graph embedding (KGE) methods typically require
preserving the entire knowledge graph (KG) with significant training costs when
new knowledge emerges. To address this issue, the continual knowledge graph
embedding (CKGE) task has been proposed to train the KGE model by learning
emerging knowledge efficiently while simultaneously preserving decent old
knowledge. However, the explicit graph structure in KGs, which is critical for
the above goal, has been heavily ignored by existing CKGE methods. On the one
hand, existing methods usually learn new triples in a random order, destroying
the inner structure of new KGs. On the other hand, old triples are preserved
with equal priority, failing to alleviate catastrophic forgetting effectively.
In this paper, we propose a competitive method for CKGE based on incremental
distillation (IncDE), which considers the full use of the explicit graph
structure in KGs. First, to optimize the learning order, we introduce a
hierarchical strategy, ranking new triples for layer-by-layer learning. By
employing the inter- and intra-hierarchical orders together, new triples are
grouped into layers based on the graph structure features. Secondly, to
preserve the old knowledge effectively, we devise a novel incremental
distillation mechanism, which facilitates the seamless transfer of entity
representations from the previous layer to the next one, promoting old
knowledge preservation. Finally, we adopt a two-stage training paradigm to
avoid the over-corruption of old knowledge influenced by under-trained new
knowledge. Experimental results demonstrate the superiority of IncDE over
state-of-the-art baselines. Notably, the incremental distillation mechanism
contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR)
score.

摘要：

##### **POV Learning: Individual Alignment of Multimodal Models using Human Perception**
2405.04443v1 by Simon Werner, Katharina Christ, Laura Bernardy, Marion G. Müller, Achim Rettinger

Aligning machine learning systems with human expectations is mostly attempted
by training with manually vetted human behavioral samples, typically explicit
feedback. This is done on a population level since the context that is
capturing the subjective Point-Of-View (POV) of a concrete person in a specific
situational context is not retained in the data. However, we argue that
alignment on an individual level can boost the subjective predictive
performance for the individual user interacting with the system considerably.
Since perception differs for each person, the same situation is observed
differently. Consequently, the basis for decision making and the subsequent
reasoning processes and observable reactions differ. We hypothesize that
individual perception patterns can be used for improving the alignment on an
individual level. We test this, by integrating perception information into
machine learning systems and measuring their predictive performance
wrt.~individual subjective assessments. For our empirical study, we collect a
novel data set of multimodal stimuli and corresponding eye tracking sequences
for the novel task of Perception-Guided Crossmodal Entailment and tackle it
with our Perception-Guided Multimodal Transformer. Our findings suggest that
exploiting individual perception signals for the machine learning of subjective
human assessments provides a valuable cue for individual alignment. It does not
only improve the overall predictive performance from the point-of-view of the
individual user but might also contribute to steering AI systems towards every
person's individual expectations and values.

摘要：

##### **AugmenTory: A Fast and Flexible Polygon Augmentation Library**
2405.04442v1 by Tanaz Ghahremani, Mohammad Hoseyni, Mohammad Javad Ahmadi, Pouria Mehrabi, Amirhossein Nikoofard

Data augmentation is a key technique for addressing the challenge of limited
datasets, which have become a major component in the training procedures of
image processing. Techniques such as geometric transformations and color space
adjustments have been thoroughly tested for their ability to artificially
expand training datasets and generate semi-realistic data for training
purposes. Data augmentation is the most important key to addressing the
challenge of limited datasets, which have become a major component of image
processing training procedures. Data augmentation techniques, such as geometric
transformations and color space adjustments, are thoroughly tested for their
ability to artificially expand training datasets and generate semi-realistic
data for training purposes. Polygons play a crucial role in instance
segmentation and have seen a surge in use across advanced models, such as
YOLOv8. Despite their growing popularity, the lack of specialized libraries
hampers the polygon-augmentation process. This paper introduces a novel
solution to this challenge, embodied in the newly developed AugmenTory library.
Notably, AugmenTory offers reduced computational demands in both time and space
compared to existing methods. Additionally, the library includes a
postprocessing thresholding feature. The AugmenTory package is publicly
available on GitHub, where interested users can access the source code:
https://github.com/Smartory/AugmenTory

摘要：

##### **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model**
2405.04434v1 by DeepSeek-AI

We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model
characterized by economical training and efficient inference. It comprises 236B
total parameters, of which 21B are activated for each token, and supports a
context length of 128K tokens. DeepSeek-V2 adopts innovative architectures
including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees
efficient inference through significantly compressing the Key-Value (KV) cache
into a latent vector, while DeepSeekMoE enables training strong models at an
economical cost through sparse computation. Compared with DeepSeek 67B,
DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves
42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum
generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality
and multi-source corpus consisting of 8.1T tokens, and further perform
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock
its potential. Evaluation results show that, even with only 21B activated
parameters, DeepSeek-V2 and its chat versions still achieve top-tier
performance among open-source models. The model checkpoints are available at
"https://github.com/deepseek-ai/DeepSeek-V2".

摘要：

##### **Vision Mamba: A Comprehensive Survey and Taxonomy**
2405.04404v1 by Xiao Liu, Chenxu Zhang, Lei Zhang

State Space Model (SSM) is a mathematical model used to describe and analyze
the behavior of dynamic systems. This model has witnessed numerous applications
in several fields, including control theory, signal processing, economics and
machine learning. In the field of deep learning, state space models are used to
process sequence data, such as time series analysis, natural language
processing (NLP) and video understanding. By mapping sequence data to state
space, long-term dependencies in the data can be better captured. In
particular, modern SSMs have shown strong representational capabilities in NLP,
especially in long sequence modeling, while maintaining linear time complexity.
Notably, based on the latest state-space models, Mamba merges time-varying
parameters into SSMs and formulates a hardware-aware algorithm for efficient
training and inference. Given its impressive efficiency and strong long-range
dependency modeling capability, Mamba is expected to become a new AI
architecture that may outperform Transformer. Recently, a number of works have
attempted to study the potential of Mamba in various fields, such as general
vision, multi-modal, medical image analysis and remote sensing image analysis,
by extending Mamba from natural language domain to visual domain. To fully
understand Mamba in the visual domain, we conduct a comprehensive survey and
present a taxonomy study. This survey focuses on Mamba's application to a
variety of visual tasks and data types, and discusses its predecessors, recent
advances and far-reaching impact on a wide range of domains. Since Mamba is now
on an upward trend, please actively notice us if you have new findings, and new
progress on Mamba will be included in this survey in a timely manner and
updated on the Mamba project at
https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.

摘要：

##### **Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs**
2405.04386v1 by Antonio Bikić, Sayan Mukherjee

Artificial neural networks (ANNs) perform extraordinarily on numerous tasks
including classification or prediction, e.g., speech processing and image
classification. These new functions are based on a computational model that is
enabled to select freely all necessary internal model parameters as long as it
eventually delivers the functionality it is supposed to exhibit. Here, we
review the connection between the model parameter selection in machine learning
(ML) algorithms running on ANNs and the epistemological theory of neopragmatism
focusing on the theory's utility and anti-representationalist aspects. To
understand the consequences of the model parameter selection of an ANN, we
suggest using neopragmatist theories whose implications are well studied.
Incidentally, neopragmatism's notion of optimization is also based on utility
considerations. This means that applying this approach elegantly reveals the
inherent connections between optimization in ML, using a numerical method
during the learning phase, and optimization in the ethical theory of
consequentialism, where it occurs as a maxim of action. We suggest that these
connections originate from the way relevance is calculated in ML systems. This
could ultimately reveal a tendency for specific actions in ML systems.

摘要：

##### **Leveraging LSTM and GAN for Modern Malware Detection**
2405.04373v1 by Ishita Gupta, Sneha Kumari, Priya Jha, Mohona Ghosh

The malware booming is a cyberspace equal to the effect of climate change to
ecosystems in terms of danger. In the case of significant investments in
cybersecurity technologies and staff training, the global community has become
locked up in the eternal war with cyber security threats. The multi-form and
changing faces of malware are continuously pushing the boundaries of the
cybersecurity practitioners employ various approaches like detection and
mitigate in coping with this issue. Some old mannerisms like signature-based
detection and behavioral analysis are slow to adapt to the speedy evolution of
malware types. Consequently, this paper proposes the utilization of the Deep
Learning Model, LSTM networks, and GANs to amplify malware detection accuracy
and speed. A fast-growing, state-of-the-art technology that leverages raw
bytestream-based data and deep learning architectures, the AI technology
provides better accuracy and performance than the traditional methods.
Integration of LSTM and GAN model is the technique that is used for the
synthetic generation of data, leading to the expansion of the training
datasets, and as a result, the detection accuracy is improved. The paper uses
the VirusShare dataset which has more than one million unique samples of the
malware as the training and evaluation set for the presented models. Through
thorough data preparation including tokenization, augmentation, as well as
model training, the LSTM and GAN models convey the better performance in the
tasks compared to straight classifiers. The research outcomes come out with 98%
accuracy that shows the efficiency of deep learning plays a decisive role in
proactive cybersecurity defense. Aside from that, the paper studies the output
of ensemble learning and model fusion methods as a way to reduce biases and
lift model complexity.

摘要：

##### **Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs**
2405.04372v1 by Martin Marzidovšek, Janja Francé, Vid Podpečan, Stanka Vadnjal, Jožica Dolenc, Patricija Mozetič

In this study, explainable machine learning techniques are applied to predict
the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful
algal blooms. By analysing a newly created 28-year dataset containing records
of toxic phytoplankton in mussel farming areas and toxin concentrations in
mussels (Mytilus galloprovincialis), we train and evaluate the performance of
ML models to accurately predict diarrhetic shellfish poisoning (DSP) events.
The random forest model provided the best prediction of positive toxicity
results based on the F1 score. Explainability methods such as permutation
importance and SHAP identified key species (Dinophysis fortii and D. caudata)
and environmental factors (salinity, river discharge and precipitation) as the
best predictors of DSP outbreaks. These findings are important for improving
early warning systems and supporting sustainable aquaculture practices.

摘要：

##### **Global Scale Self-Supervised Channel Charting with Sensor Fusion**
2405.04357v1 by Omid Esrafilian, Mohsen Ahadi, Florian Kaltenberger, David Gesbert

The sensing and positioning capabilities foreseen in 6G have great potential
for technology advancements in various domains, such as future smart cities and
industrial use cases. Channel charting has emerged as a promising technology in
recent years for radio frequency-based sensing and localization. However, the
accuracy of these techniques is yet far behind the numbers envisioned in 6G. To
reduce this gap, in this paper, we propose a novel channel charting technique
capitalizing on the time of arrival measurements from surrounding Transmission
Reception Points (TRPs) along with their locations and leveraging sensor fusion
in channel charting by incorporating laser scanner data during the training
phase of our algorithm. The proposed algorithm remains self-supervised during
training and test phases, requiring no geometrical models or user position
ground truth. Simulation results validate the achievement of a sub-meter level
localization accuracy using our algorithm 90% of the time, outperforming the
state-of-the-art channel charting techniques and the traditional
triangulation-based approaches.

摘要：

##### **Revisiting character-level adversarial attacks**
2405.04346v1 by Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios G. Chrysos, Volkan Cevher

Adversarial attacks in Natural Language Processing apply perturbations in the
character or token levels. Token-level attacks, gaining prominence for their
use of gradient-based methods, are susceptible to altering sentence semantics,
leading to invalid adversarial examples. While character-level attacks easily
maintain semantics, they have received less attention as they cannot easily
adopt popular gradient-based methods, and are thought to be easy to defend.
Challenging these beliefs, we introduce Charmer, an efficient query-based
adversarial attack capable of achieving high attack success rate (ASR) while
generating highly similar adversarial examples. Our method successfully targets
both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,
Charmer improves the ASR in 4.84% points and the USE similarity in 8% points
with respect to the previous art. Our implementation is available in
https://github.com/LIONS-EPFL/Charmer.

摘要：

##### **Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**
2405.04336v1 by Zhihao Wen, Yuan Fang, Pengcheng Wei, Fayao Liu, Zhenghua Chen, Min Wu

Predicting Remaining Useful Life (RUL) plays a crucial role in the
prognostics and health management of industrial systems that involve a variety
of interrelated sensors. Given a constant stream of time series sensory data
from such systems, deep learning models have risen to prominence at identifying
complex, nonlinear temporal dependencies in these data. In addition to the
temporal dependencies of individual sensors, spatial dependencies emerge as
important correlations among these sensors, which can be naturally modelled by
a temporal graph that describes time-varying spatial relationships. However,
the majority of existing studies have relied on capturing discrete snapshots of
this temporal graph, a coarse-grained approach that leads to loss of temporal
information. Moreover, given the variety of heterogeneous sensors, it becomes
vital that such inherent heterogeneity is leveraged for RUL prediction in
temporal sensor graphs. To capture the nuances of the temporal and spatial
relationships and heterogeneous characteristics in an interconnected graph of
sensors, we introduce a novel model named Temporal and Heterogeneous Graph
Neural Networks (THGNN). Specifically, THGNN aggregates historical data from
neighboring nodes to accurately capture the temporal dynamics and spatial
correlations within the stream of sensor data in a fine-grained manner.
Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address
the diversity of sensor types, significantly improving the model's capacity to
learn the heterogeneity in the data sources. Finally, we have validated the
effectiveness of our approach through comprehensive experiments. Our empirical
findings demonstrate significant advancements on the N-CMAPSS dataset,
achieving improvements of up to 19.2% and 31.6% in terms of two different
evaluation metrics over state-of-the-art methods.

摘要：

##### **A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI**
2405.04333v1 by Hannah Chafetz, Sampriti Saxena, Stefaan G. Verhulst

Since late 2022, generative AI has taken the world by storm, with widespread
use of tools including ChatGPT, Gemini, and Claude. Generative AI and large
language model (LLM) applications are transforming how individuals find and
access data and knowledge. However, the intricate relationship between open
data and generative AI, and the vast potential it holds for driving innovation
in this field remain underexplored areas. This white paper seeks to unpack the
relationship between open data and generative AI and explore possible
components of a new Fourth Wave of Open Data: Is open data becoming AI ready?
Is open data moving towards a data commons approach? Is generative AI making
open data more conversational? Will generative AI improve open data quality and
provenance? Towards this end, we provide a new Spectrum of Scenarios framework.
This framework outlines a range of scenarios in which open data and generative
AI could intersect and what is required from a data quality and provenance
perspective to make open data ready for those specific scenarios. These
scenarios include: pertaining, adaptation, inference and insight generation,
data augmentation, and open-ended exploration. Through this process, we found
that in order for data holders to embrace generative AI to improve open data
access and develop greater insights from open data, they first must make
progress around five key areas: enhance transparency and documentation, uphold
quality and integrity, promote interoperability and standards, improve
accessibility and useability, and address ethical considerations.

摘要：

##### **Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**
2405.04325v1 by Atharvan Dogra, Ameet Deshpande, John Nay, Tanmay Rajpurohit, Ashwin Kalyan, Balaraman Ravindran

Recent developments in large language models (LLMs), while offering a
powerful foundation for developing natural language agents, raise safety
concerns about them and the autonomous agents built upon them. Deception is one
potential capability of AI agents of particular concern, which we refer to as
an act or statement that misleads, hides the truth, or promotes a belief that
is not true in its entirety or in part. We move away from the conventional
understanding of deception through straight-out lying, making objective selfish
decisions, or giving false information, as seen in previous AI safety research.
We target a specific category of deception achieved through obfuscation and
equivocation. We broadly explain the two types of deception by analogizing them
with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out
of a hidden trap door or (ii) (our focus) the audience is completely distracted
to see the magician bring out the rabbit right in front of them using sleight
of hand or misdirection. Our novel testbed framework displays intrinsic
deception capabilities of LLM agents in a goal-driven environment when directed
to be deceptive in their natural language generations in a two-agent
adversarial dialogue system built upon the legislative task of "lobbying" for a
bill. Along the lines of a goal-driven environment, we show developing
deceptive capacity through a reinforcement learning setup, building it around
the theories of language philosophy and cognitive psychology. We find that the
lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through
subsequent reinforcement trials of adversarial interactions, and our deception
detection mechanism shows a detection capability of up to 92%. Our results
highlight potential issues in agent-human interaction, with agents potentially
manipulating humans towards its programmed end-goal.

摘要：

##### **Granite Code Models: A Family of Open Foundation Models for Code Intelligence**
2405.04324v1 by Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, Rameswar Panda

Large Language Models (LLMs) trained on code are revolutionizing the software
development process. Increasingly, code LLMs are being integrated into software
development environments to improve the productivity of human programmers, and
LLM-based agents are beginning to show promise for handling complex tasks
autonomously. Realizing the full potential of code LLMs requires a wide range
of capabilities, including code generation, fixing bugs, explaining and
documenting code, maintaining repositories, and more. In this work, we
introduce the Granite series of decoder-only code models for code generative
tasks, trained with code written in 116 programming languages. The Granite Code
models family consists of models ranging in size from 3 to 34 billion
parameters, suitable for applications ranging from complex application
modernization tasks to on-device memory-constrained use cases. Evaluation on a
comprehensive set of tasks demonstrates that Granite Code models consistently
reaches state-of-the-art performance among available open-source code LLMs. The
Granite Code model family was optimized for enterprise software development
workflows and performs well across a range of coding tasks (e.g. code
generation, fixing and explanation), making it a versatile all around code
model. We release all our Granite Code models under an Apache 2.0 license for
both research and commercial use.

摘要：

##### **Beyond human subjectivity and error: a novel AI grading system**
2405.04323v1 by Alexandra Gobrecht, Felix Tuma, Moritz Möller, Thomas Zöller, Mark Zakhvatkin, Alexandra Wuttig, Holger Sommerfeldt, Sven Schütt

The grading of open-ended questions is a high-effort, high-impact task in
education. Automating this task promises a significant reduction in workload
for education professionals, as well as more consistent grading outcomes for
students, by circumventing human subjectivity and error. While recent
breakthroughs in AI technology might facilitate such automation, this has not
been demonstrated at scale. It this paper, we introduce a novel automatic short
answer grading (ASAG) system. The system is based on a fine-tuned open-source
transformer model which we trained on large set of exam data from university
courses across a large range of disciplines. We evaluated the trained model's
performance against held-out test data in a first experiment and found high
accuracy levels across a broad spectrum of unseen questions, even in unseen
courses. We further compared the performance of our model with that of
certified human domain experts in a second experiment: we first assembled
another test dataset from real historical exams - the historic grades contained
in that data were awarded to students in a regulated, legally binding
examination process; we therefore considered them as ground truth for our
experiment. We then asked certified human domain experts and our model to grade
the historic student answers again without disclosing the historic grades.
Finally, we compared the hence obtained grades with the historic grades (our
ground truth). We found that for the courses examined, the model deviated less
from the official historic grades than the human re-graders - the model's
median absolute error was 44 % smaller than the human re-graders', implying
that the model is more consistent than humans in grading. These results suggest
that leveraging AI enhanced grading can reduce human subjectivity, improve
consistency and thus ultimately increase fairness.

摘要：

##### **Cross-IQA: Unsupervised Learning for Image Quality Assessment**
2405.04311v1 by Zhen Zhang

Automatic perception of image quality is a challenging problem that impacts
billions of Internet and social media users daily. To advance research in this
field, we propose a no-reference image quality assessment (NR-IQA) method
termed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA
method can learn image quality features from unlabeled image data. We construct
the pretext task of synthesized image reconstruction to unsupervised extract
the image quality information based ViT block. The pretrained encoder of
Cross-IQA is used to fine-tune a linear regression model for score prediction.
Experimental results show that Cross-IQA can achieve state-of-the-art
performance in assessing the low-frequency degradation information (e.g., color
change, blurring, etc.) of images compared with the classical full-reference
IQA and NR-IQA under the same datasets.

摘要：

##### **Improving Offline Reinforcement Learning with Inaccurate Simulators**
2405.04307v1 by Yiwen Hou, Haoyuan Sun, Jinming Ma, Feng Wu

Offline reinforcement learning (RL) provides a promising approach to avoid
costly online interaction with the real environment. However, the performance
of offline RL highly depends on the quality of the datasets, which may cause
extrapolation error in the learning process. In many robotic applications, an
inaccurate simulator is often available. However, the data directly collected
from the inaccurate simulator cannot be directly used in offline RL due to the
well-known exploration-exploitation dilemma and the dynamic gap between
inaccurate simulation and the real environment. To address these issues, we
propose a novel approach to combine the offline dataset and the inaccurate
simulation data in a better manner. Specifically, we pre-train a generative
adversarial network (GAN) model to fit the state distribution of the offline
dataset. Given this, we collect data from the inaccurate simulator starting
from the distribution provided by the generator and reweight the simulated data
using the discriminator. Our experimental results in the D4RL benchmark and a
real-world manipulation task confirm that our method can benefit more from both
inaccurate simulator and limited offline datasets to achieve better performance
than the state-of-the-art methods.

摘要：

##### **A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**
2405.04305v1 by Raiyan Rahman, Christopher Indris, Goetz Bramesfeld, Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang

Aphid infestations are one of the primary causes of extensive damage to wheat
and sorghum fields and are one of the most common vectors for plant viruses,
resulting in significant agricultural yield losses. To address this problem,
farmers often employ the inefficient use of harmful chemical pesticides that
have negative health and environmental impacts. As a result, a large amount of
pesticide is wasted on areas without significant pest infestation. This brings
to attention the urgent need for an intelligent autonomous system that can
locate and spray sufficiently large infestations selectively within the complex
crop canopies. We have developed a large multi-scale dataset for aphid cluster
detection and segmentation, collected from actual sorghum fields and
meticulously annotated to include clusters of aphids. Our dataset comprises a
total of 54,742 image patches, showcasing a variety of viewpoints, diverse
lighting conditions, and multiple scales, highlighting its effectiveness for
real-world applications. In this study, we trained and evaluated four real-time
semantic segmentation models and three object detection models specifically for
aphid cluster segmentation and detection. Considering the balance between
accuracy and efficiency, Fast-SCNN delivered the most effective segmentation
results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames
per second (FPS). For object detection, RT-DETR exhibited the best overall
performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and
72.55 on an NVIDIA V100 GPU. Our experiments further indicate that aphid
cluster segmentation is more suitable for assessing aphid infestations than
using detection models.

摘要：

##### **Accelerating Speculative Decoding using Dynamic Speculation Length**
2405.04304v1 by Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz

Speculative decoding is a promising method for reducing the inference latency
of large language models. The effectiveness of the method depends on the
speculation length (SL) - the number of tokens generated by the draft model at
each iteration. The vast majority of speculative decoding approaches use the
same SL for all iterations. In this work, we show that this practice is
suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization
method that uses a classifier to dynamically adjust the SL at each iteration,
while provably preserving the decoding quality. Experiments with four
benchmarks demonstrate average speedup gains of 10.3% relative to our best
baselines.

摘要：

##### **Behaviour Planning: A Toolkit for Diverse Planning**
2405.04300v1 by Mustafa F Abdelwahed, Joan Espasa, Alice Toniolo, Ian P. Gent

Diverse planning is the problem of generating plans with distinct
characteristics. This is valuable for many real-world scenarios, including
applications related to plan recognition and business process automation. In
this work, we introduce \emph{Behaviour Planning}, a diverse planning toolkit
that can characterise and generate diverse plans based on modular diversity
models. We present a qualitative framework for describing diversity models, a
planning approach for generating plans aligned with any given diversity model,
and provide a practical implementation of an SMT-based behaviour planner. We
showcase how the qualitative approach offered by Behaviour Planning allows it
to overcome various challenges faced by previous approaches. Finally, the
experimental evaluation shows the effectiveness of Behaviour Planning in
generating diverse plans compared to state-of-the-art approaches.

摘要：

##### **Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework**
2405.04294v1 by Xiangpeng Wan, Haicheng Deng, Kai Zou, Shiqi Xu

Structured finance, which involves restructuring diverse assets into
securities like MBS, ABS, and CDOs, enhances capital market efficiency but
presents significant due diligence challenges. This study explores the
integration of artificial intelligence (AI) with traditional asset review
processes to improve efficiency and accuracy in structured finance. Using both
open-sourced and close-sourced large language models (LLMs), we demonstrate
that AI can automate the verification of information between loan applications
and bank statements effectively. While close-sourced models such as GPT-4 show
superior performance, open-sourced models like LLAMA3 offer a cost-effective
alternative. Dual-agent systems further increase accuracy, though this comes
with higher operational costs. This research highlights AI's potential to
minimize manual errors and streamline due diligence, suggesting a broader
application of AI in financial document analysis and risk management.

摘要：

##### **Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning**
2405.04292v1 by Sayantan Pal, Souvik Das, Rohini K. Srihari

This study introduces 'clickbait spoiling', a novel technique designed to
detect, categorize, and generate spoilers as succinct text responses,
countering the curiosity induced by clickbait content. By leveraging a
multi-task learning framework, our model's generalization capabilities are
significantly enhanced, effectively addressing the pervasive issue of
clickbait. The crux of our research lies in generating appropriate spoilers, be
it a phrase, an extended passage, or multiple, depending on the spoiler type
required. Our methodology integrates two crucial techniques: a refined spoiler
categorization method and a modified version of the Question Answering (QA)
mechanism, incorporated within a multi-task learning paradigm for optimized
spoiler extraction from context. Notably, we have included fine-tuning methods
for models capable of handling longer sequences to accommodate the generation
of extended spoilers. This research highlights the potential of sophisticated
text processing techniques in tackling the omnipresent issue of clickbait,
promising an enhanced user experience in the digital realm.

摘要：

##### **Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore**
2405.04286v1 by Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang

The efficacy of an large language model (LLM) generated text detector depends
substantially on the availability of sizable training data. White-box zero-shot
detectors, which require no such data, are nonetheless limited by the
accessibility of the source model of the LLM-generated text. In this paper, we
propose an simple but effective black-box zero-shot detection approach,
predicated on the observation that human-written texts typically contain more
grammatical errors than LLM-generated texts. This approach entails computing
the Grammar Error Correction Score (GECScore) for the given text to distinguish
between human-written and LLM-generated text. Extensive experimental results
show that our method outperforms current state-of-the-art (SOTA) zero-shot and
supervised methods, achieving an average AUROC of 98.7% and showing strong
robustness against paraphrase and adversarial perturbation attacks.

摘要：

##### **On the Foundations of Earth and Climate Foundation Models**
2405.04285v1 by Xiao Xiang Zhu, Zhitong Xiong, Yi Wang, Adam J. Stewart, Konrad Heidler, Yuanyuan Wang, Zhenghang Yuan, Thomas Dujardin, Qingsong Xu, Yilei Shi

Foundation models have enormous potential in advancing Earth and climate
sciences, however, current approaches may not be optimal as they focus on a few
basic features of a desirable Earth and climate foundation model. Crafting the
ideal Earth foundation model, we define eleven features which would allow such
a foundation model to be beneficial for any geoscientific downstream
application in an environmental- and human-centric manner.We further shed light
on the way forward to achieve the ideal model and to evaluate Earth foundation
models. What comes after foundation models? Energy efficient adaptation,
adversarial defenses, and interpretability are among the emerging directions.

摘要：

##### **Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats**
2405.04271v1 by Arne Rubehn, Jessica Nieder, Robert Forkel, Johann-Mattis List

When comparing speech sounds across languages, scholars often make use of
feature representations of individual sounds in order to determine fine-grained
sound similarities. Although binary feature systems for large numbers of speech
sounds have been proposed, large-scale computational applications often face
the challenges that the proposed feature systems -- even if they list features
for several thousand sounds -- only cover a smaller part of the numerous speech
sounds reflected in actual cross-linguistic data. In order to address the
problem of missing data for attested speech sounds, we propose a new approach
that can create binary feature vectors dynamically for all sounds that can be
represented in the the standardized version of the International Phonetic
Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS)
reference catalog. Since CLTS is actively used in large data collections,
covering more than 2,000 distinct language varieties, our procedure for the
generation of binary feature vectors provides immediate access to a very large
collection of multilingual wordlists. Testing our feature system in different
ways on different datasets proves that the system is not only useful to provide
a straightforward means to compare the similarity of speech sounds, but also
illustrates its potential to be used in future cross-linguistic machine
learning applications.

摘要：

##### **VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting**
2405.04252v1 by Alireza Koochali, Ensiye Tahaei, Andreas Dengel, Sheraz Ahmed

This paper presents VAEneu, an innovative autoregressive method for multistep
ahead univariate probabilistic time series forecasting. We employ the
conditional VAE framework and optimize the lower bound of the predictive
distribution likelihood function by adopting the Continuous Ranked Probability
Score (CRPS), a strictly proper scoring rule, as the loss function. This novel
pipeline results in forecasting sharp and well-calibrated predictive
distribution. Through a comprehensive empirical study, VAEneu is rigorously
benchmarked against 12 baseline models across 12 datasets. The results
unequivocally demonstrate VAEneu's remarkable forecasting performance. VAEneu
provides a valuable tool for quantifying future uncertainties, and our
extensive empirical study lays the foundation for future comparative studies
for univariate multistep ahead probabilistic forecasting.

摘要：

##### **Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks**
2405.04249v1 by Caelin Kaplan, Tareq Si Salem, Angelo Rodio, Chuan Xu, Giovanni Neglia

As Internet of Things (IoT) technology advances, end devices like sensors and
smartphones are progressively equipped with AI models tailored to their local
memory and computational constraints. Local inference reduces communication
costs and latency; however, these smaller models typically underperform
compared to more sophisticated models deployed on edge servers or in the cloud.
Cooperative Inference Systems (CISs) address this performance trade-off by
enabling smaller devices to offload part of their inference tasks to more
capable devices. These systems often deploy hierarchical models that share
numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize
strategies like early exits or ordered dropout. In such instances, Federated
Learning (FL) may be employed to jointly train the models within a CIS. Yet,
traditional training methods have overlooked the operational dynamics of CISs
during inference, particularly the potential high heterogeneity in serving
rates across clients. To address this gap, we propose a novel FL approach
designed explicitly for use in CISs that accounts for these variations in
serving rates. Our framework not only offers rigorous theoretical guarantees,
but also surpasses state-of-the-art (SOTA) training algorithms for CISs,
especially in scenarios where inference request rates or data availability are
uneven among clients.

摘要：

##### **Exploring Correlations of Self-supervised Tasks for Graphs**
2405.04245v1 by Taoran Fang, Wei Zhou, Yifei Sun, Kaiqiao Han, Lvbin Ma, Yang Yang

Graph self-supervised learning has sparked a research surge in training
informative representations without accessing any labeled data. However, our
understanding of graph self-supervised learning remains limited, and the
inherent relationships between various self-supervised tasks are still
unexplored. Our paper aims to provide a fresh understanding of graph
self-supervised learning based on task correlations. Specifically, we evaluate
the performance of the representations trained by one specific task on other
tasks and define correlation values to quantify task correlations. Through this
process, we unveil the task correlations between various self-supervised tasks
and can measure their expressive capabilities, which are closely related to
downstream performance. By analyzing the correlation values between tasks
across various datasets, we reveal the complexity of task correlations and the
limitations of existing multi-task learning methods. To obtain more capable
representations, we propose Graph Task Correlation Modeling (GraphTCM) to
illustrate the task correlations and utilize it to enhance graph
self-supervised training. The experimental results indicate that our method
significantly outperforms existing methods across various downstream tasks.

摘要：

##### **Iterative Experience Refinement of Software-Developing Agents**
2405.04219v1 by Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, Maosong Sun

Autonomous agents powered by large language models (LLMs) show significant
potential for achieving high autonomy in various scenarios such as software
development. Recent research has shown that LLM agents can leverage past
experiences to reduce errors and enhance efficiency. However, the static
experience paradigm, reliant on a fixed collection of past experiences acquired
heuristically, lacks iterative refinement and thus hampers agents'
adaptability. In this paper, we introduce the Iterative Experience Refinement
framework, enabling LLM agents to refine experiences iteratively during task
execution. We propose two fundamental patterns: the successive pattern,
refining based on nearest experiences within a task batch, and the cumulative
pattern, acquiring experiences across all previous task batches. Augmented with
our heuristic experience elimination, the method prioritizes high-quality and
frequently-used experiences, effectively managing the experience space and
enhancing efficiency. Extensive experiments show that while the successive
pattern may yield superior results, the cumulative pattern provides more stable
performance. Moreover, experience elimination facilitates achieving better
performance using just 11.54% of a high-quality subset.

摘要：

##### **NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions**
2405.04215v1 by Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp

Today's classical planners are powerful, but modeling input tasks in formats
such as PDDL is tedious and error-prone. In contrast, planning with Large
Language Models (LLMs) allows for almost any input text, but offers no
guarantees on plan quality or even soundness. In an attempt to merge the best
of these two approaches, some work has begun to use LLMs to automate parts of
the PDDL creation process. However, these methods still require various degrees
of expert input. We present NL2Plan, the first domain-agnostic offline
LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the
necessary information from a short text prompt before creating a complete PDDL
description of both the domain and the problem, which is finally solved by a
classical planner. We evaluate NL2Plan on four planning domains and find that
it solves 10 out of 15 tasks - a clear improvement over a plain
chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover,
in two out of the five failure cases, instead of returning an invalid plan,
NL2Plan reports that it failed to solve the task. In addition to using NL2Plan
in end-to-end mode, users can inspect and correct all of its intermediate
results, such as the PDDL representation, increasing explainability and making
it an assistive tool for PDDL creation.

摘要：

##### **Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks**
2405.04212v1 by Sondre Glimsdal, Sebastian Østby, Tobias M. Brambo, Eirik M. Vinje

Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve
real-world problems using TMs. Several frameworks already exist that provide
access to TM implementations. However, these either lack features or have a
research-first focus. GT is an easy-to-use framework that aims to lower the
complexity and provide a production-ready TM implementation that is great for
experienced practitioners and beginners. To this end, GT establishes a clear
separation between training and inference. A C++ backend with a Python
interface provides competitive training and inference performance, with the
option of running in pure Python. It also integrates support for critical
components such as exporting trained models, hyper-parameter search, and
cross-validation out-of-the-box.

摘要：

##### **NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator**
2405.04206v1 by Mohit Upadhyay, Rohan Juneja, Weng-Fai Wong, Li-Shiuan Peh

Attention mechanisms are becoming increasingly popular, being used in neural
network models in multiple domains such as natural language processing (NLP)
and vision applications, especially at the edge. However, attention layers are
difficult to map onto existing neuro accelerators since they have a much higher
density of non-linear operations, which lead to inefficient utilization of
today's vector units. This work introduces NOVA, a NoC-based Vector Unit that
can perform non-linear operations within the NoC of the accelerators, and can
be overlaid onto existing neuro accelerators to map attention layers at the
edge. Our results show that the NOVA architecture is up to 37.8x more
power-efficient than state-of-the-art hardware approximators when running
existing attention-based neural networks.

摘要：

##### **FedStale: leveraging stale client updates in federated learning**
2405.04171v1 by Angelo Rodio, Giovanni Neglia

Federated learning algorithms, such as FedAvg, are negatively affected by
data heterogeneity and partial client participation. To mitigate the latter
problem, global variance reduction methods, like FedVARP, leverage stale model
updates for non-participating clients. These methods are effective under
homogeneous client participation. Yet, this paper shows that, when some clients
participate much less than others, aggregating updates with different levels of
staleness can detrimentally affect the training process. Motivated by this
observation, we introduce FedStale, a novel algorithm that updates the global
model in each round through a convex combination of "fresh" updates from
participating clients and "stale" updates from non-participating ones. By
adjusting the weight in the convex combination, FedStale interpolates between
FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and
stale updates equally. Our analysis of FedStale convergence yields the
following novel findings: i) it integrates and extends previous FedAvg and
FedVARP analyses to heterogeneous client participation; ii) it underscores how
the least participating client influences convergence error; iii) it provides
practical guidelines to best exploit stale updates, showing that their
usefulness diminishes as data heterogeneity decreases and participation
heterogeneity increases. Extensive experiments featuring diverse levels of
client data and participation heterogeneity not only confirm these findings but
also show that FedStale outperforms both FedAvg and FedVARP in many settings.

摘要：

##### **D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models**
2405.04170v1 by Duygu Altinok

Large language models (LLMs) have garnered significant attention and
widespread usage due to their impressive performance in various tasks. However,
they are not without their own set of challenges, including issues such as
hallucinations, factual inconsistencies, and limitations in
numerical-quantitative reasoning. Evaluating LLMs in miscellaneous reasoning
tasks remains an active area of research. Prior to the breakthrough of LLMs,
Transformers had already proven successful in the medical domain, effectively
employed for various natural language understanding (NLU) tasks. Following this
trend, LLMs have also been trained and utilized in the medical domain, raising
concerns regarding factual accuracy, adherence to safety protocols, and
inherent limitations. In this paper, we focus on evaluating the natural
language inference capabilities of popular open-source and closed-source LLMs
using clinical trial reports as the dataset. We present the performance results
of each LLM and further analyze their performance on a development set,
particularly focusing on challenging instances that involve medical
abbreviations and require numerical-quantitative reasoning. Gemini, our leading
LLM, achieved a test set F1-score of 0.748, securing the ninth position on the
task scoreboard. Our work is the first of its kind, offering a thorough
examination of the inference capabilities of LLMs within the medical domain.

摘要：

##### **LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection**
2405.04165v1 by Jasraj Singh, Fang Liu, Hong Xu, Bee Chin Ng, Wei Zhang

Nowadays, Information spreads at an unprecedented pace in social media and
discerning truth from misinformation and fake news has become an acute societal
challenge. Machine learning (ML) models have been employed to identify fake
news but are far from perfect with challenging problems like limited accuracy,
interpretability, and generalizability. In this paper, we enhance ML-based
solutions with linguistics input and we propose LingML, linguistic-informed ML,
for fake news detection. We conducted an experimental study with a popular
dataset on fake news during the pandemic. The experiment results show that our
proposed solution is highly effective. There are fewer than two errors out of
every ten attempts with only linguistic input used in ML and the knowledge is
highly explainable. When linguistics input is integrated with advanced
large-scale ML models for natural language processing, our solution outperforms
existing ones with 1.8% average error rate. LingML creates a new path with
linguistics to push the frontier of effective and efficient fake news
detection. It also sheds light on real-world multi-disciplinary applications
requiring both ML and domain expertise to achieve optimal performance.

摘要：

##### **MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization**
2405.04163v1 by Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly

This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for
fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and
PEGASUS for improved medical text summarization. In contrast to existing domain
adaptation approaches in summarization, MEDVOC treats vocabulary as an
optimizable parameter and optimizes the PLM vocabulary based on fragment score
conditioned only on the downstream task's reference summaries. Unlike previous
works on vocabulary adaptation (limited only to classification tasks),
optimizing vocabulary based on summarization tasks requires an extremely costly
intermediate fine-tuning step on large summarization datasets. To that end, our
novel fragment score-based hyperparameter search very significantly reduces
this fine-tuning time -- from 450 days to less than 2 days on average.
Furthermore, while previous works on vocabulary adaptation are often primarily
tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs
(with varying model vocabulary sizes, pre-training objectives, and model sizes)
-- bridging the limited vocabulary overlap between the biomedical literature
domain and PLMs. MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in
zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV)
concentrations. Our human evaluation shows MEDVOC generates more faithful
medical summaries (88% compared to 59% in baselines). We make the codebase
publicly available at https://github.com/gb-kgp/MEDVOC.

摘要：

##### **A Causal Explainable Guardrails for Large Language Models**
2405.04160v1 by Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, Kui Ren

Large Language Models (LLMs) have shown impressive performance in natural
language tasks, but their outputs can exhibit undesirable attributes or biases.
Existing methods for steering LLMs towards desired attributes often assume
unbiased representations and rely solely on steering prompts. However, the
representations learned from pre-training can introduce semantic biases that
influence the steering process, leading to suboptimal results. We propose
LLMGuardaril, a novel framework that incorporates causal analysis and
adversarial learning to obtain unbiased steering representations in LLMs.
LLMGuardaril systematically identifies and blocks the confounding effects of
biases, enabling the extraction of unbiased steering representations.
Additionally, it includes an explainable component that provides insights into
the alignment between the generated output and the desired direction.
Experiments demonstrate LLMGuardaril's effectiveness in steering LLMs towards
desired attributes while mitigating biases. Our work contributes to the
development of safe and reliable LLMs that align with desired attributes. We
discuss the limitations and future research directions, highlighting the need
for ongoing research to address the ethical implications of large language
models.

摘要：

##### **GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness**
2405.04138v1 by Nabil Al-Dhamari, Nathan Clarke

This study explores the limitations of traditional Cybersecurity Awareness
and Training (CSAT) programs and proposes an innovative solution using
Generative Pre-Trained Transformers (GPT) to address these shortcomings.
Traditional approaches lack personalization and adaptability to individual
learning styles. To overcome these challenges, the study integrates GPT models
to deliver highly tailored and dynamic cybersecurity learning expe-riences.
Leveraging natural language processing capabilities, the proposed approach
personalizes training modules based on individual trainee pro-files, helping to
ensure engagement and effectiveness. An experiment using a GPT model to provide
a real-time and adaptive CSAT experience through generating customized training
content. The findings have demonstrated a significant improvement over
traditional programs, addressing issues of en-gagement, dynamicity, and
relevance. GPT-powered CSAT programs offer a scalable and effective solution to
enhance cybersecurity awareness, provid-ing personalized training content that
better prepares individuals to miti-gate cybersecurity risks in their specific
roles within the organization.

摘要：

##### **Enriched BERT Embeddings for Scholarly Publication Classification**
2405.04136v1 by Benjamin Wolff, Eva Seidlmayer, Konrad U. Förstner

With the rapid expansion of academic literature and the proliferation of
preprints, researchers face growing challenges in manually organizing and
labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses
this challenge organized as a competition. The goal is to develop a classifier
capable of predicting one of 123 predefined classes from the Open Research
Knowledge Graph (ORKG) taxonomy of research fields for a given article.This
paper presents our results. Initially, we enrich the dataset (containing
English scholarly articles sourced from ORKG and arXiv), then leverage
different pre-trained language Models (PLMs), specifically BERT, and explore
their efficacy in transfer learning for this downstream task. Our experiments
encompass feature-based and fine-tuned transfer learning approaches using
diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and
SPECTER2. We conduct hyperparameter tuning and investigate the impact of data
augmentation from bibliographic databases such as OpenAlex, Semantic Scholar,
and Crossref. Our results demonstrate that fine-tuning pre-trained models
substantially enhances classification performance, with SPECTER2 emerging as
the most accurate model. Moreover, enriching the dataset with additional
metadata improves classification outcomes significantly, especially when
integrating information from S2AG, OpenAlex and Crossref. Our best-performing
approach achieves a weighted F1-score of 0.7415. Overall, our study contributes
to the advancement of reliable automated systems for scholarly publication
categorization, offering a potential solution to the laborious manual curation
process, thereby facilitating researchers in efficiently locating relevant
resources.

摘要：

##### **In-context Learning for Automated Driving Scenarios**
2405.04135v1 by Ziqi Zhou, Jingyue Zhang, Jingyuan Zhang, Boyue Wang, Tianyu Shi, Alaa Khamis

One of the key challenges in current Reinforcement Learning (RL)-based
Automated Driving (AD) agents is achieving flexible, precise, and human-like
behavior cost-effectively. This paper introduces an innovative approach
utilizing Large Language Models (LLMs) to intuitively and effectively optimize
RL reward functions in a human-centric way. We developed a framework where
instructions and dynamic environment descriptions are input into the LLM. The
LLM then utilizes this information to assist in generating rewards, thereby
steering the behavior of RL agents towards patterns that more closely resemble
human driving. The experimental results demonstrate that this approach not only
makes RL agents more anthropomorphic but also reaches better performance.
Additionally, various strategies for reward-proxy and reward-shaping are
investigated, revealing the significant impact of prompt design on shaping an
AD vehicle's behavior. These findings offer a promising direction for the
development of more advanced and human-like automated driving systems. Our
experimental data and source code can be found here.

摘要：

##### **Fine-grained Speech Sentiment Analysis in Chinese Psychological Support Hotlines Based on Large-scale Pre-trained Model**
2405.04128v1 by Zhonglong Chen, Changwei Song, Yining Chen, Jianqiang Li, Guanghui Fu, Yongsheng Tong, Qing Zhao

Suicide and suicidal behaviors remain significant challenges for public
policy and healthcare. In response, psychological support hotlines have been
established worldwide to provide immediate help to individuals in mental
crises. The effectiveness of these hotlines largely depends on accurately
identifying callers' emotional states, particularly underlying negative
emotions indicative of increased suicide risk. However, the high demand for
psychological interventions often results in a shortage of professional
operators, highlighting the need for an effective speech emotion recognition
model. This model would automatically detect and analyze callers' emotions,
facilitating integration into hotline services. Additionally, it would enable
large-scale data analysis of psychological support hotline interactions to
explore psychological phenomena and behaviors across populations. Our study
utilizes data from the Beijing psychological support hotline, the largest
suicide hotline in China. We analyzed speech data from 105 callers containing
20,630 segments and categorized them into 11 types of negative emotions. We
developed a negative emotion recognition model and a fine-grained multi-label
classification model using a large-scale pre-trained model. Our experiments
indicate that the negative emotion recognition model achieves a maximum
F1-score of 76.96%. However, it shows limited efficacy in the fine-grained
multi-label classification task, with the best model achieving only a 41.74%
weighted F1-score. We conducted an error analysis for this task, discussed
potential future improvements, and considered the clinical application
possibilities of our study. All the codes are public available.

摘要：

##### **Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling**
2405.04124v1 by Riccardo Simionato, Stefano Fasciani

Analog electronic circuits are at the core of an important category of
musical devices. The nonlinear features of their electronic components give
analog musical devices a distinctive timbre and sound quality, making them
highly desirable. Artificial neural networks have rapidly gained popularity for
the emulation of analog audio effects circuits, particularly recurrent
networks. While neural approaches have been successful in accurately modeling
distortion circuits, they require architectural improvements that account for
parameter conditioning and low latency response. In this article, we explore
the application of recent machine learning advancements for virtual analog
modeling. We compare State Space models and Linear Recurrent Units against the
more common Long Short Term Memory networks. These have shown promising ability
in sequence to sequence modeling tasks, showing a notable improvement in signal
history encoding. Our comparative study uses these black box neural modeling
techniques with a variety of audio effects. We evaluate the performance and
limitations using multiple metrics aiming to assess the models' ability to
accurately replicate energy envelopes, frequency contents, and transients in
the audio signal. To incorporate control parameters we employ the Feature wise
Linear Modulation method. Long Short Term Memory networks exhibit better
accuracy in emulating distortions and equalizers, while the State Space model,
followed by Long Short Term Memory networks when integrated in an encoder
decoder structure, outperforms others in emulating saturation and compression.
When considering long time variant characteristics, the State Space model
demonstrates the greatest accuracy. The Long Short Term Memory and, in
particular, Linear Recurrent Unit networks present more tendency to introduce
audio artifacts.

摘要：

##### **Policy Learning with a Language Bottleneck**
2405.04118v1 by Megha Srivastava, Cedric Colas, Dorsa Sadigh, Jacob Andreas

Modern AI systems such as self-driving cars and game-playing agents achieve
superhuman performance, but often lack human-like features such as
generalization, interpretability and human inter-operability. Inspired by the
rich interactions between language and decision-making in humans, we introduce
Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI
agents to generate linguistic rules that capture the strategies underlying
their most rewarding behaviors. PLLB alternates between a rule generation step
guided by language models, and an update step where agents learn new policies
guided by rules. In a two-player communication game, a maze solving task, and
two image reconstruction tasks, we show that PLLB agents are not only able to
learn more interpretable and generalizable behaviors, but can also share the
learned rules with human users, enabling more effective human-AI coordination.

摘要：

##### **A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model**
2405.04108v1 by Tianxiu Xie, Keke Gai, Jing Yu, Liehuang Zhu, Kim-Kwang Raymond Choo

Recent booming development of Generative Artificial Intelligence (GenAI) has
facilitated an emerging model commercialization for the purpose of
reinforcement on model performance, such as licensing or trading Deep Neural
Network (DNN) models. However, DNN model trading may trigger concerns of the
unauthorized replications or misuses over the model, so that the benefit of the
model ownership will be violated. Model identity auditing is a challenging
issue in protecting intellectual property of DNN models and verifying the
integrity and ownership of models for guaranteeing trusts in transactions is
one of the critical obstacles. In this paper, we focus on the above issue and
propose a novel Accumulator-enabled Auditing for Distributed Identity of DNN
Model (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to
protect data and function privacy while ensuring the lightweight on-chain
ownership verification. The proposed model presents a scheme of identity
records via configuring model weight checkpoints with corresponding
zero-knowledge proofs, which incorporates predicates to capture incremental
state changes in model weight checkpoints. Our scheme ensures both
computational integrity of DNN training process and programmability, so that
the uniqueness of the weight checkpoint sequence in a DNN model is preserved,
ensuring the correctness of the model identity auditing. In addition, A2-DIDM
also addresses privacy protections in distributed identity via a proposed
method of accumulators. We systematically analyze the security and robustness
of our proposed model and further evaluate the effectiveness and usability of
auditing DNN model identities.

摘要：

##### **Continual Learning in the Presence of Repetition**
2405.04101v1 by Hamed Hemati, Lorenzo Pellegrini, Xiaotian Duan, Zixuan Zhao, Fangfang Xia, Marc Masana, Benedikt Tscheschner, Eduardo Veas, Yuxiang Zheng, Shiji Zhao, Shao-Yuan Li, Sheng-Jun Huang, Vincenzo Lomonaco, Gido M. van de Ven

Continual learning (CL) provides a framework for training models in
ever-evolving environments. Although re-occurrence of previously seen objects
or tasks is common in real-world problems, the concept of repetition in the
data stream is not often considered in standard benchmarks for CL. Unlike with
the rehearsal mechanism in buffer-based strategies, where sample repetition is
controlled by the strategy, repetition in the data stream naturally stems from
the environment. This report provides a summary of the CLVision challenge at
CVPR 2023, which focused on the topic of repetition in class-incremental
learning. The report initially outlines the challenge objective and then
describes three solutions proposed by finalist teams that aim to effectively
exploit the repetition in the stream to learn continually. The experimental
results from the challenge highlight the effectiveness of ensemble-based
solutions that employ multiple versions of similar modules, each trained on
different but overlapping subsets of classes. This report underscores the
transformative potential of taking a different perspective in CL by employing
repetition in the data stream to foster innovative strategy design.

摘要：

##### **Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes**
2405.04097v1 by Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang

The emergence of contemporary deepfakes has attracted significant attention
in machine learning research, as artificial intelligence (AI) generated
synthetic media increases the incidence of misinterpretation and is difficult
to distinguish from genuine content. Currently, machine learning techniques
have been extensively studied for automatically detecting deepfakes. However,
human perception has been less explored. Malicious deepfakes could ultimately
cause public and social problems. Can we humans correctly perceive the
authenticity of the content of the videos we watch? The answer is obviously
uncertain; therefore, this paper aims to evaluate the human ability to discern
deepfake videos through a subjective study. We present our findings by
comparing human observers to five state-ofthe-art audiovisual deepfake
detection models. To this end, we used gamification concepts to provide 110
participants (55 native English speakers and 55 non-native English speakers)
with a webbased platform where they could access a series of 40 videos (20 real
and 20 fake) to determine their authenticity. Each participant performed the
experiment twice with the same 40 videos in different random orders. The videos
are manually selected from the FakeAVCeleb dataset. We found that all AI models
performed better than humans when evaluated on the same 40 videos. The study
also reveals that while deception is not impossible, humans tend to
overestimate their detection capabilities. Our experimental results may help
benchmark human versus machine performance, advance forensics analysis, and
enable adaptive countermeasures.

摘要：

##### **Going Proactive and Explanatory Against Malware Concept Drift**
2405.04095v1 by Yiling He, Junchi Lei, Zhan Qin, Kui Ren

Deep learning-based malware classifiers face significant challenges due to
concept drift. The rapid evolution of malware, especially with new families,
can depress classification accuracy to near-random levels. Previous research
has primarily focused on detecting drift samples, relying on expert-led
analysis and labeling for model retraining. However, these methods often lack a
comprehensive understanding of malware concepts and provide limited guidance
for effective drift adaptation, leading to unstable detection performance and
high human labeling costs.
  To address these limitations, we introduce DREAM, a novel system designed to
surpass the capabilities of existing drift detectors and to establish an
explanatory drift adaptation process. DREAM enhances drift detection through
model sensitivity and data autonomy. The detector, trained in a semi-supervised
approach, proactively captures malware behavior concepts through classifier
feedback. During testing, it utilizes samples generated by the detector itself,
eliminating reliance on extensive training data. For drift adaptation, DREAM
enlarges human intervention, enabling revisions of malware labels and concept
explanations embedded within the detector's latent space. To ensure a
comprehensive response to concept drift, it facilitates a coordinated update
process for both the classifier and the detector. Our evaluation shows that
DREAM can effectively improve the drift detection accuracy and reduce the
expert analysis effort in adaptation across different malware datasets and
classifiers.

摘要：

##### **DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects**
2405.04093v1 by Da Fu, Mingfei Rong, Eun-Hu Kim, Hao Huang, Witold Pedrycz

Accurate classification of fine-grained images remains a challenge in
backbones based on convolutional operations or self-attention mechanisms. This
study proposes novel dual-current neural networks (DCNN), which combine the
advantages of convolutional operations and self-attention mechanisms to improve
the accuracy of fine-grained image classification. The main novel design
features for constructing a weakly supervised learning backbone model DCNN
include (a) extracting heterogeneous data, (b) keeping the feature map
resolution unchanged, (c) expanding the receptive field, and (d) fusing global
representations and local features. Experimental results demonstrated that
using DCNN as the backbone network for classifying certain fine-grained
benchmark datasets achieved performance advantage improvements of 13.5--19.5%
and 2.2--12.9%, respectively, compared to other advanced convolution or
attention-based fine-grained backbones.

摘要：

##### **Optimizing Language Model's Reasoning Abilities with Weak Supervision**
2405.04086v1 by Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang

While Large Language Models (LLMs) have demonstrated proficiency in handling
complex queries, much of the past work has depended on extensively annotated
datasets by human experts. However, this reliance on fully-supervised
annotations poses scalability challenges, particularly as models and data
requirements grow. To mitigate this, we explore the potential of enhancing
LLMs' reasoning abilities with minimal human supervision. In this work, we
introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of
the model using a small collection of annotated questions. Then it iteratively
improves LLMs by learning from the differences in responses from the SFT and
unfinetuned models on unlabeled questions. Our approach provides an efficient
approach without relying heavily on extensive human-annotated explanations.
However, current reasoning benchmarks typically only include golden-reference
answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly
supervised benchmark that comprises 25,147 complex questions, answers, and
human-generated rationales across various domains, such as brainteasers,
puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of
our dataset is the inclusion of 10,000 unannotated questions, enabling us to
explore utilizing fewer supersized data to boost LLMs' inference capabilities.
Our experiments underscore the significance of \textsc{PuzzleBen}, as well as
the effectiveness of our methodology as a promising direction in future
endeavors. Our dataset and code will be published soon on \texttt{Anonymity
Link}.

摘要：

##### **Counterfactual and Semifactual Explanations in Abstract Argumentation: Formal Foundations, Complexity and Computation**
2405.04081v1 by Gianvincenzo Alfano, Sergio Greco, Francesco Parisi, Irina Trubitsyna

Explainable Artificial Intelligence and Formal Argumentation have received
significant attention in recent years. Argumentation-based systems often lack
explainability while supporting decision-making processes. Counterfactual and
semifactual explanations are interpretability techniques that provide insights
into the outcome of a model by generating alternative hypothetical instances.
While there has been important work on counterfactual and semifactual
explanations for Machine Learning models, less attention has been devoted to
these kinds of problems in argumentation. In this paper, we explore
counterfactual and semifactual reasoning in abstract Argumentation Framework.
We investigate the computational complexity of counterfactual- and
semifactual-based reasoning problems, showing that they are generally harder
than classical argumentation problems such as credulous and skeptical
acceptance. Finally, we show that counterfactual and semifactual queries can be
encoded in weak-constrained Argumentation Framework, and provide a
computational strategy through ASP solvers.

摘要：

##### **WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer**
2405.04078v1 by Kumar Shubham, Aishwarya Jayagopal, Syed Mohammed Danish, Prathosh AP, Vaibhav Rajan

Cancer, a leading cause of death globally, occurs due to genomic changes and
manifests heterogeneously across patients. To advance research on personalized
treatment strategies, the effectiveness of various drugs on cells derived from
cancers (`cell lines') is experimentally determined in laboratory settings.
Nevertheless, variations in the distribution of genomic data and drug responses
between cell lines and humans arise due to biological and environmental
differences. Moreover, while genomic profiles of many cancer patients are
readily available, the scarcity of corresponding drug response data limits the
ability to train machine learning models that can predict drug response in
patients effectively. Recent cancer drug response prediction methods have
largely followed the paradigm of unsupervised domain-invariant representation
learning followed by a downstream drug response classification step.
Introducing supervision in both stages is challenging due to heterogeneous
patient response to drugs and limited drug response data. This paper addresses
these challenges through a novel representation learning method in the first
phase and weak supervision in the second. Experimental results on real patient
data demonstrate the efficacy of our method (WISER) over state-of-the-art
alternatives on predicting personalized drug response.

摘要：

##### **A simple theory for training response of deep neural networks**
2405.04074v1 by Kenichi Nakazato

Deep neural networks give us a powerful method to model the training
dataset's relationship between input and output. We can regard that as a
complex adaptive system consisting of many artificial neurons that work as an
adaptive memory as a whole. The network's behavior is training dynamics with a
feedback loop from the evaluation of the loss function. We already know the
training response can be constant or shows power law-like aging in some ideal
situations. However, we still have gaps between those findings and other
complex phenomena, like network fragility. To fill the gap, we introduce a very
simple network and analyze it. We show the training response consists of some
different factors based on training stages, activation functions, or training
methods. In addition, we show feature space reduction as an effect of
stochastic training dynamics, which can result in network fragility. Finally,
we discuss some complex phenomena of deep networks.

摘要：

##### **FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference**
2405.04065v1 by Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu

Retrieval-Augmented Language Modeling (RALM) by integrating large language
models (LLM) with relevant documents from an external corpus is a proven method
for enabling the LLM to generate information beyond the scope of its
pre-training corpus. Previous work using utilizing retrieved content by simply
prepending retrieved contents to the input poses a high runtime issue, which
degrades the inference efficiency of the LLMs because they fail to use the
Key-Value (KV) cache efficiently. In this paper, we propose \textsc{FlashBack},
a modular RALM designed to improve the inference efficiency of RALM with
appending context pattern while maintaining decent performance after specific
fine-tuning without heavily destruct the knowledge integrity of the LLM.
\textsc{FlashBack} appends retrieved documents at the end of the context for
efficiently utilizing the KV cache instead of prepending them. Our experiment
shows that the inference speed of \textsc{FlashBack} is up to $4\times$ faster
than the prepending method on a 7B LLM (Llama 2). Via bypassing unnecessary
re-computation, it demonstrates an advancement by achieving significantly
faster inference speed, and this heightened efficiency will substantially
reduce inferential cost. Our code will be publicly available.

摘要：

##### **Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT**
2405.04053v1 by Hassan Shakil, Atqiya Munawara Mahi, Phuoc Nguyen, Zeydy Ortiz, Mamoun T. Mardini

This research examines the effectiveness of OpenAI's GPT models as
independent evaluators of text summaries generated by six transformer-based
models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS.
We evaluated these summaries based on essential properties of high-quality
summary - conciseness, relevance, coherence, and readability - using
traditional metrics such as ROUGE and Latent Semantic Analysis (LSA). Uniquely,
we also employed GPT not as a summarizer but as an evaluator, allowing it to
independently assess summary quality without predefined metrics. Our analysis
revealed significant correlations between GPT evaluations and traditional
metrics, particularly in assessing relevance and coherence. The results
demonstrate GPT's potential as a robust tool for evaluating text summaries,
offering insights that complement established metrics and providing a basis for
comparative analysis of transformer-based models in natural language processing
tasks.

摘要：

##### **Learning Linear Block Error Correction Codes**
2405.04050v1 by Yoni Choukroun, Lior Wolf

Error correction codes are a crucial part of the physical communication
layer, ensuring the reliable transfer of data over noisy channels. The design
of optimal linear block codes capable of being efficiently decoded is of major
concern, especially for short block lengths. While neural decoders have
recently demonstrated their advantage over classical decoding techniques, the
neural design of the codes remains a challenge. In this work, we propose for
the first time a unified encoder-decoder training of binary linear block codes.
To this end, we adapt the coding setting to support efficient and
differentiable training of the code for end-to-end optimization over the order
two Galois field. We also propose a novel Transformer model in which the
self-attention masking is performed in a differentiable fashion for the
efficient backpropagation of the code gradient. Our results show that (i) the
proposed decoder outperforms existing neural decoding on conventional codes,
(ii) the suggested framework generates codes that outperform the {analogous}
conventional codes, and (iii) the codes we developed not only excel with our
decoder but also show enhanced performance with traditional decoding
techniques.

摘要：

##### **Philosophy of Cognitive Science in the Age of Deep Learning**
2405.04048v1 by Raphaël Millière

Deep learning has enabled major advances across most areas of artificial
intelligence research. This remarkable progress extends beyond mere engineering
achievements and holds significant relevance for the philosophy of cognitive
science. Deep neural networks have made significant strides in overcoming the
limitations of older connectionist models that once occupied the centre stage
of philosophical debates about cognition. This development is directly relevant
to long-standing theoretical debates in the philosophy of cognitive science.
Furthermore, ongoing methodological challenges related to the comparative
evaluation of deep neural networks stand to benefit greatly from
interdisciplinary collaboration with philosophy and cognitive science. The time
is ripe for philosophers to explore foundational issues related to deep
learning and cognition; this perspective paper surveys key areas where their
contributions can be especially fruitful.

摘要：

##### **Feature Map Convergence Evaluation for Functional Module**
2405.04041v1 by Ludan Zhang, Chaoyi Chen, Lei He, Keqiang Li

Autonomous driving perception models are typically composed of multiple
functional modules that interact through complex relationships to accomplish
environment understanding. However, perception models are predominantly
optimized as a black box through end-to-end training, lacking independent
evaluation of functional modules, which poses difficulties for interpretability
and optimization. Pioneering in the issue, we propose an evaluation method
based on feature map analysis to gauge the convergence of model, thereby
assessing functional modules' training maturity. We construct a quantitative
metric named as the Feature Map Convergence Score (FMCS) and develop Feature
Map Convergence Evaluation Network (FMCE-Net) to measure and predict the
convergence degree of models respectively. FMCE-Net achieves remarkable
predictive accuracy for FMCS across multiple image classification experiments,
validating the efficacy and robustness of the introduced approach. To the best
of our knowledge, this is the first independent evaluation method for
functional modules, offering a new paradigm for the training assessment towards
perception models.

摘要：

##### **Utilizing GPT to Enhance Text Summarization: A Strategy to Minimize Hallucinations**
2405.04039v1 by Hassan Shakil, Zeydy Ortiz, Grant C. Forbes

In this research, we uses the DistilBERT model to generate extractive summary
and the T5 model to generate abstractive summaries. Also, we generate hybrid
summaries by combining both DistilBERT and T5 models. Central to our research
is the implementation of GPT-based refining process to minimize the common
problem of hallucinations that happens in AI-generated summaries. We evaluate
unrefined summaries and, after refining, we also assess refined summaries using
a range of traditional and novel metrics, demonstrating marked improvements in
the accuracy and reliability of the summaries. Results highlight significant
improvements in reducing hallucinatory content, thereby increasing the factual
integrity of the summaries.

摘要：

##### **Locally Differentially Private In-Context Learning**
2405.04032v1 by Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixin Jiang, Shaoyang Song, Chunlai Zhou

Large pretrained language models (LLMs) have shown surprising In-Context
Learning (ICL) ability. An important application in deploying large language
models is to augment LLMs with a private database for some specific task. The
main problem with this promising commercial use is that LLMs have been shown to
memorize their training data and their prompt data are vulnerable to membership
inference attacks (MIA) and prompt leaking attacks. In order to deal with this
problem, we treat LLMs as untrusted in privacy and propose a locally
differentially private framework of in-context learning(LDP-ICL) in the
settings where labels are sensitive. Considering the mechanisms of in-context
learning in Transformers by gradient descent, we provide an analysis of the
trade-off between privacy and utility in such LDP-ICL for classification.
Moreover, we apply LDP-ICL to the discrete distribution estimation problem. In
the end, we perform several experiments to demonstrate our analysis results.

摘要：

##### **Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties**
2405.04015v1 by S. Akshay, Krishnendu Chatterjee, Tobias Meggendorfer, Đorđe Žikelić

Markov Decision Processes (MDPs) are a classical model for decision making in
the presence of uncertainty. Often they are viewed as state transformers with
planning objectives defined with respect to paths over MDP states. An
increasingly popular alternative is to view them as distribution transformers,
giving rise to a sequence of probability distributions over MDP states. For
instance, reachability and safety properties in modeling robot swarms or
chemical reaction networks are naturally defined in terms of probability
distributions over states. Verifying such distributional properties is known to
be hard and often beyond the reach of classical state-based verification
techniques.
  In this work, we consider the problems of certified policy (i.e. controller)
verification and synthesis in MDPs under distributional reach-avoidance
specifications. By certified we mean that, along with a policy, we also aim to
synthesize a (checkable) certificate ensuring that the MDP indeed satisfies the
property. Thus, given the target set of distributions and an unsafe set of
distributions over MDP states, our goal is to either synthesize a certificate
for a given policy or synthesize a policy along with a certificate, proving
that the target distribution can be reached while avoiding unsafe
distributions. To solve this problem, we introduce the novel notion of
distributional reach-avoid certificates and present automated procedures for
(1) synthesizing a certificate for a given policy, and (2) synthesizing a
policy together with the certificate, both providing formal guarantees on
certificate correctness. Our experimental evaluation demonstrates the ability
of our method to solve several non-trivial examples, including a multi-agent
robot-swarm model, to synthesize certified policies and to certify existing
policies.

摘要：

##### **Structured Click Control in Transformer-based Interactive Segmentation**
2405.04009v1 by Long Xu, Yongquan Chen, Rui Huang, Feng Wu, Shiwu Lai

Click-point-based interactive segmentation has received widespread attention
due to its efficiency. However, it's hard for existing algorithms to obtain
precise and robust responses after multiple clicks. In this case, the
segmentation results tend to have little change or are even worse than before.
To improve the robustness of the response, we propose a structured click intent
model based on graph neural networks, which adaptively obtains graph nodes via
the global similarity of user-clicked Transformer tokens. Then the graph nodes
will be aggregated to obtain structured interaction features. Finally, the dual
cross-attention will be used to inject structured interaction features into
vision Transformer features, thereby enhancing the control of clicks over
segmentation results. Extensive experiments demonstrated the proposed algorithm
can serve as a general structure in improving Transformer-based interactive
segmenta?tion performance. The code and data will be released at
https://github.com/hahamyt/scc.

摘要：

##### **Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches**
2405.03998v1 by Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, Elena Glassman

Crafting effective prompts for code generation or editing with Large Language
Models (LLMs) is not an easy task. Particularly, the absence of immediate,
stable feedback during prompt crafting hinders effective interaction, as users
are left to mentally imagine possible outcomes until the code is generated. In
response, we introduce Language-Oriented Code Sketching, an interactive
approach that provides instant, incremental feedback in the form of code
sketches (i.e., incomplete code outlines) during prompt crafting. This approach
converts a prompt into a code sketch by leveraging the inherent linguistic
structures within the prompt and applying classic natural language processing
techniques. The sketch then serves as an intermediate placeholder that not only
previews the intended code structure but also guides the LLM towards the
desired code, thereby enhancing human-LLM interaction. We conclude by
discussing the approach's applicability and future plans.

摘要：

##### **TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks**
2405.03990v1 by Guanqiao Qu, Zheng Lin, Fangming Liu, Xianhao Chen, Kaibin Huang

Next-generation mobile networks are expected to facilitate fast AI model
downloading to end users. By caching models on edge servers, mobile networks
can deliver models to end users with low latency, resulting in a paradigm
called edge model caching. In this paper, we develop a novel model placement
scheme, called parameter-sharing model caching (TrimCaching). TrimCaching
exploits the key observation that a wide range of AI models, such as
convolutional neural networks or large language models, can share a significant
proportion of parameter blocks containing reusable knowledge, thereby improving
storage efficiency. To this end, we formulate a parameter-sharing model
placement problem to maximize the cache hit ratio in multi-edge wireless
networks by balancing the fundamental tradeoff between storage efficiency and
service latency. We show that the formulated problem is a submodular
maximization problem with submodular constraints, for which no polynomial-time
approximation algorithm exists. To overcome this challenge, we study an
important special case, where a small fixed number of parameter blocks are
shared across models, which often holds in practice. In such a case, a
polynomial-time algorithm with $\left(1-\epsilon\right)/2$-approximation
guarantee is developed. Subsequently, we address the original problem for the
general case by developing a greedy algorithm. Simulation results demonstrate
that the proposed TrimCaching framework significantly improves the cache hit
ratio compared with state-of-the-art content caching without exploiting shared
parameters in AI models.

摘要：

##### **Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application**
2405.03988v1 by Jian Jia, Yipei Wang, Yan Li, Honggang Chen, Xuehan Bai, Zhaocheng Liu, Jian Liang, Quan Chen, Han Li, Peng Jiang, Kun Gai

Contemporary recommender systems predominantly rely on collaborative
filtering techniques, employing ID-embedding to capture latent associations
among users and items. However, this approach overlooks the wealth of semantic
information embedded within textual descriptions of items, leading to
suboptimal performance in cold-start scenarios and long-tail user
recommendations. Leveraging the capabilities of Large Language Models (LLMs)
pretrained on massive text corpus presents a promising avenue for enhancing
recommender systems by integrating open-world domain knowledge. In this paper,
we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework
that synergizes open-world knowledge with collaborative knowledge. We address
computational complexity concerns by utilizing pretrained LLMs as item encoders
and freezing LLM parameters to avoid catastrophic forgetting and preserve
open-world knowledge. To bridge the gap between the open-world and
collaborative domains, we design a twin-tower structure supervised by the
recommendation task and tailored for practical industrial application. Through
offline experiments on the large-scale industrial dataset and online
experiments on A/B tests, we demonstrate the efficacy of our approach.

摘要：

##### **Factors Influencing User Willingness To Use SORA**
2405.03986v1 by Gustave Florentin Nkoulou Mvondo, Ben Niu

Sora promises to redefine the way visual content is created. Despite its
numerous forecasted benefits, the drivers of user willingness to use the
text-to-video (T2V) model are unknown. This study extends the extended unified
theory of acceptance and use of technology (UTAUT2) with perceived realism and
novelty value. Using a purposive sampling method, we collected data from 940
respondents in the US and analyzed the sample using covariance-based structural
equation modeling and fuzzy set qualitative comparative analysis (fsQCA). The
findings reveal that all hypothesized relationships are supported, with
perceived realism emerging as the most influential driver, followed by novelty
value. Moreover, fsQCA identifies five configurations leading to high and low
willingness to use, and the model demonstrates high predictive validity,
contributing to theory advancement. Our study provides valuable insights for
developers and marketers, offering guidance for strategic decisions to promote
the widespread adoption of T2V models.

摘要：

##### **TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments**
2405.03974v1 by Ziyu Liu, Tong Zhou, Yukui Luo, Xiaolin Xu

Trusted Execution Environments (TEEs) have become a promising solution to
secure DNN models on edge devices. However, the existing solutions either
provide inadequate protection or introduce large performance overhead. Taking
both security and performance into consideration, this paper presents TBNet, a
TEE-based defense framework that protects DNN model from a neural architectural
perspective. Specifically, TBNet generates a novel Two-Branch substitution
model, to respectively exploit (1) the computational resources in the untrusted
Rich Execution Environment (REE) for latency reduction and (2) the
physically-isolated TEE for model protection. Experimental results on a
Raspberry Pi across diverse DNN model architectures and datasets demonstrate
that TBNet achieves efficient model protection at a low cost.

摘要：

##### **ERATTA: Extreme RAG for Table To Answers with Large Language Models**
2405.03963v1 by Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra

Large language models (LLMs) with residual augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. However, the choice of use-cases that incorporate RAG with LLMs have been
either generic or extremely domain specific, thereby questioning the
scalability and generalizability of RAG-LLM approaches. In this work, we
propose a unique LLM-based system where multiple LLMs can be invoked to enable
data authentication, user query routing, data retrieval and custom prompting
for question answering capabilities from data tables that are highly varying
and large in size. Our system is tuned to extract information from
Enterprise-level data products and furnish real time responses under 10
seconds. One prompt manages user-to-data authentication followed by three
prompts to route, fetch data and generate a customizable prompt natural
language responses. Additionally, we propose a five metric scoring module that
detects and reports hallucinations in the LLM responses. Our proposed system
and scoring metrics achieve >90% confidence scores across hundreds of user
queries in the sustainability, financial health and social media domains.
Extensions to the proposed extreme RAG architectures can enable heterogeneous
source querying using LLMs.

摘要：

##### **ESIHGNN: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition**
2405.03960v1 by Xupeng Zha, Huan Zhao, Zixing Zhang

Conversational Emotion Recognition (CER) aims to predict the emotion
expressed by an utterance (referred to as an ``event'') during a conversation.
Existing graph-based methods mainly focus on event interactions to comprehend
the conversational context, while overlooking the direct influence of the
speaker's emotional state on the events. In addition, real-time modeling of the
conversation is crucial for real-world applications but is rarely considered.
Toward this end, we propose a novel graph-based approach, namely Event-State
Interactions infused Heterogeneous Graph Neural Network (ESIHGNN), which
incorporates the speaker's emotional state and constructs a heterogeneous
event-state interaction graph to model the conversation. Specifically, a
heterogeneous directed acyclic graph neural network is employed to dynamically
update and enhance the representations of events and emotional states at each
turn, thereby improving conversational coherence and consistency. Furthermore,
to further improve the performance of CER, we enrich the graph's edges with
external knowledge. Experimental results on four publicly available CER
datasets show the superiority of our approach and the effectiveness of the
introduced heterogeneous event-state interaction graph.

摘要：

##### **Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model**
2405.03958v1 by Joo Young Choi, Jaesung R. Park, Inkyu Park, Jaewoong Cho, Albert No, Ernest K. Ryu

Current state-of-the-art diffusion models employ U-Net architectures
containing convolutional and (qkv) self-attention layers. The U-Net processes
images while being conditioned on the time embedding input for each sampling
step and the class or caption embedding input corresponding to the desired
conditional generation. Such conditioning involves scale-and-shift operations
to the convolutional layers but does not directly affect the attention layers.
While these standard architectural choices are certainly effective, not
conditioning the attention layers feels arbitrary and potentially suboptimal.
In this work, we show that simply adding LoRA conditioning to the attention
layers without changing or tuning the other parts of the U-Net architecture
improves the image generation quality. For example, a drop-in addition of LoRA
conditioning to EDM diffusion model yields FID scores of 1.91/1.75 for
unconditional and class-conditional CIFAR-10 generation, improving upon the
baseline of 1.97/1.79.

摘要：

##### **HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech**
2405.03952v1 by Zhongren Dong, Zixing Zhang, Weixiang Xu, Jing Han, Jianjun Ou, Björn W. Schuller

Automatically detecting Alzheimer's Disease (AD) from spontaneous speech
plays an important role in its early diagnosis. Recent approaches highly rely
on the Transformer architectures due to its efficiency in modelling long-range
context dependencies. However, the quadratic increase in computational
complexity associated with self-attention and the length of audio poses a
challenge when deploying such models on edge devices. In this context, we
construct a novel framework, namely Hierarchical Attention-Free Transformer
(HAFFormer), to better deal with long speech for AD detection. Specifically, we
employ an attention-free module of Multi-Scale Depthwise Convolution to replace
the self-attention and thus avoid the expensive computation, and a GELU-based
Gated Linear Unit to replace the feedforward layer, aiming to automatically
filter out the redundant information. Moreover, we design a hierarchical
structure to force it to learn a variety of information grains, from the frame
level to the dialogue level. By conducting extensive experiments on the
ADReSS-M dataset, the introduced HAFFormer can achieve competitive results
(82.6% accuracy) with other recent work, but with significant computational
complexity and model size reduction compared to the standard Transformer. This
shows the efficiency of HAFFormer in dealing with long audio for AD detection.

摘要：

##### **Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**
2405.03943v1 by Jiayuan Chen, Changchang Yin, Yuanlong Wang, Ping Zhang

Deep learning-based predictive models, leveraging Electronic Health Records
(EHR), are receiving increasing attention in healthcare. An effective
representation of a patient's EHR should hierarchically encompass both the
temporal relationships between historical visits and medical events, and the
inherent structural information within these elements. Existing patient
representation methods can be roughly categorized into sequential
representation and graphical representation. The sequential representation
methods focus only on the temporal relationships among longitudinal visits. On
the other hand, the graphical representation approaches, while adept at
extracting the graph-structured relationships between various medical events,
fall short in effectively integrate temporal information. To capture both types
of information, we model a patient's EHR as a novel temporal heterogeneous
graph. This graph includes historical visits nodes and medical events nodes. It
propagates structured information from medical event nodes to visit nodes and
utilizes time-aware visit nodes to capture changes in the patient's health
status. Furthermore, we introduce a novel temporal graph transformer (TRANS)
that integrates temporal edge features, global positional encoding, and local
structural encoding into heterogeneous graph convolution, capturing both
temporal and structural information. We validate the effectiveness of TRANS
through extensive experiments on three real-world datasets. The results show
that our proposed approach achieves state-of-the-art performance.

摘要：

##### **Long Context Alignment with Short Instructions and Synthesized Positions**
2405.03939v1 by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, Sujian Li

Effectively handling instructions with extremely long context remains a
challenge for Large Language Models (LLMs), typically necessitating
high-quality long data and substantial computational resources. This paper
introduces Step-Skipping Alignment (SkipAlign), a new technique designed to
enhance the long-context capabilities of LLMs in the phase of alignment without
the need for additional efforts beyond training with original data length.
SkipAlign is developed on the premise that long-range dependencies are
fundamental to enhancing an LLM's capacity of long context. Departing from
merely expanding the length of input samples, SkipAlign synthesizes long-range
dependencies from the aspect of positions indices. This is achieved by the
strategic insertion of skipped positions within instruction-following samples,
which utilizes the semantic structure of the data to effectively expand the
context. Through extensive experiments on base models with a variety of context
window sizes, SkipAlign demonstrates its effectiveness across a spectrum of
long-context tasks. Particularly noteworthy is that with a careful selection of
the base model and alignment datasets, SkipAlign with only 6B parameters
achieves it's best performance and comparable with strong baselines like
GPT-3.5-Turbo-16K on LongBench.

摘要：

##### **CleanGraph: Human-in-the-loop Knowledge Graph Refinement and Completion**
2405.03932v1 by Tyler Bikaun, Michael Stewart, Wei Liu

This paper presents CleanGraph, an interactive web-based tool designed to
facilitate the refinement and completion of knowledge graphs. Maintaining the
reliability of knowledge graphs, which are grounded in high-quality and
error-free facts, is crucial for real-world applications such as
question-answering and information retrieval systems. These graphs are often
automatically assembled from textual sources by extracting semantic triples via
information extraction. However, assuring the quality of these extracted
triples, especially when dealing with large or low-quality datasets, can pose a
significant challenge and adversely affect the performance of downstream
applications. CleanGraph allows users to perform Create, Read, Update, and
Delete (CRUD) operations on their graphs, as well as apply models in the form
of plugins for graph refinement and completion tasks. These functionalities
enable users to enhance the integrity and reliability of their graph data. A
demonstration of CleanGraph and its source code can be accessed at
https://github.com/nlp-tlp/CleanGraph under the MIT License.

摘要：

##### **Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations**
2405.03929v1 by Jaesung Park, Sungchul Hong, Yoonseo Cho, Jong-June Jeon

Sea ice at the North Pole is vital to global climate dynamics. However,
accurately forecasting sea ice poses a significant challenge due to the
intricate interaction among multiple variables. Leveraging the capability to
integrate multiple inputs and powerful performances seamlessly, many studies
have turned to neural networks for sea ice forecasting. This paper introduces a
novel deep architecture named Unicorn, designed to forecast weekly sea ice. Our
model integrates multiple time series images within its architecture to enhance
its forecasting performance. Moreover, we incorporate a bottleneck layer within
the U-Net architecture, serving as neural ordinary differential equations with
convolution operations, to capture the spatiotemporal dynamics of latent
variables. Through real data analysis with datasets spanning from 1998 to 2021,
our proposed model demonstrates significant improvements over state-of-the-art
models in the sea ice concentration forecasting task. It achieves an average
MAE improvement of 12% compared to benchmark models. Additionally, our method
outperforms existing approaches in sea ice extent forecasting, achieving a
classification performance improvement of approximately 18%. These experimental
results show the superiority of our proposed model.

摘要：

##### **A Roadmap for Multilingual, Multimodal Domain Independent Deception Detection**
2405.03920v1 by Dainis Boumber, Rakesh M. Verma, Fatima Zahra Qachfar

Deception, a prevalent aspect of human communication, has undergone a
significant transformation in the digital age. With the globalization of online
interactions, individuals are communicating in multiple languages and mixing
languages on social media, with varied data becoming available in each language
and dialect. At the same time, the techniques for detecting deception are
similar across the board. Recent studies have shown the possibility of the
existence of universal linguistic cues to deception across domains within the
English language; however, the existence of such cues in other languages
remains unknown. Furthermore, the practical task of deception detection in
low-resource languages is not a well-studied problem due to the lack of labeled
data. Another dimension of deception is multimodality. For example, a picture
with an altered caption in fake news or disinformation may exist. This paper
calls for a comprehensive investigation into the complexities of deceptive
language across linguistic boundaries and modalities within the realm of
computer security and natural language processing and the possibility of using
multilingual transformer models and labeled data in various languages to
universally address the task of deception detection.

摘要：

##### **OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs**
2405.03901v1 by Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, Michelle Li

The progression to "Pervasive Augmented Reality" envisions easy access to
multimodal information continuously. However, in many everyday scenarios, users
are occupied physically, cognitively or socially. This may increase the
friction to act upon the multimodal information that users encounter in the
world. To reduce such friction, future interactive interfaces should
intelligently provide quick access to digital actions based on users' context.
To explore the range of possible digital actions, we conducted a diary study
that required participants to capture and share the media that they intended to
perform actions on (e.g., images or audio), along with their desired actions
and other contextual information. Using this data, we generated a holistic
design space of digital follow-up actions that could be performed in response
to different types of multimodal sensory inputs. We then designed OmniActions,
a pipeline powered by large language models (LLMs) that processes multimodal
sensory inputs and predicts follow-up actions on the target information
grounded in the derived design space. Using the empirical data collected in the
diary study, we performed quantitative evaluations on three variations of LLM
techniques (intent classification, in-context learning and finetuning) and
identified the most effective technique for our task. Additionally, as an
instantiation of the pipeline, we developed an interactive prototype and
reported preliminary user feedback about how people perceive and react to the
action predictions and its errors.

摘要：

##### **Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows**
2405.03892v1 by Minjae Cho, Jonathan P. How, Chuangchuang Sun

Despite notable successes of Reinforcement Learning (RL), the prevalent use
of an online learning paradigm prevents its widespread adoption, especially in
hazardous or costly scenarios. Offline RL has emerged as an alternative
solution, learning from pre-collected static datasets. However, this offline
learning introduces a new challenge known as distributional shift, degrading
the performance when the policy is evaluated on scenarios that are
Out-Of-Distribution (OOD) from the training dataset. Most existing offline RL
resolves this issue by regularizing policy learning within the information
supported by the given dataset. However, such regularization overlooks the
potential for high-reward regions that may exist beyond the dataset. This
motivates exploring novel offline learning techniques that can make
improvements beyond the data support without compromising policy performance,
potentially by learning causation (cause-and-effect) instead of correlation
from the dataset. In this paper, we propose the MOOD-CRL (Model-based Offline
OOD-Adapting Causal RL) algorithm, which aims to address the challenge of
extrapolation for offline policy training through causal inference instead of
policy-regularizing methods. Specifically, Causal Normalizing Flow (CNF) is
developed to learn the transition and reward functions for data generation and
augmentation in offline policy evaluation and training. Based on the
data-invariant, physics-based qualitative causal graph and the observational
data, we develop a novel learning scheme for CNF to learn the quantitative
structural causal model. As a result, CNF gains predictive and counterfactual
reasoning capabilities for sequential decision-making tasks, revealing a high
potential for OOD adaptation. Our CNF-based offline RL approach is validated
through empirical evaluations, outperforming model-free and model-based methods
by a significant margin.

摘要：

##### **Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management**
2405.03891v1 by Ravikumar Balakrishnan, Marius Arvinte, Nageen Himayat, Hosein Nikopour, Hassnaa Moustafa

Adversarial machine learning, focused on studying various attacks and
defenses on machine learning (ML) models, is rapidly gaining importance as ML
is increasingly being adopted for optimizing wireless systems such as Open
Radio Access Networks (O-RAN). A comprehensive modeling of the security threats
and the demonstration of adversarial attacks and defenses on practical AI based
O-RAN systems is still in its nascent stages. We begin by conducting threat
modeling to pinpoint attack surfaces in O-RAN using an ML-based Connection
management application (xApp) as an example. The xApp uses a Graph Neural
Network trained using Deep Reinforcement Learning and achieves on average 54%
improvement in the coverage rate measured as the 5th percentile user data
rates. We then formulate and demonstrate evasion attacks that degrade the
coverage rates by as much as 50% through injecting bounded noise at different
threat surfaces including the open wireless medium itself. Crucially, we also
compare and contrast the effectiveness of such attacks on the ML-based xApp and
a non-ML based heuristic. We finally develop and demonstrate robust
training-based defenses against the challenging physical/jamming-based attacks
and show a 15% improvement in the coverage rates when compared to employing no
defense over a range of noise budgets

摘要：

##### **Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer**
2405.03882v1 by Huihong Shi, Haikuo Shao, Wendong Mao, Zhongfeng Wang

Motivated by the huge success of Transformers in the field of natural
language processing (NLP), Vision Transformers (ViTs) have been rapidly
developed and achieved remarkable performance in various computer vision tasks.
However, their huge model sizes and intensive computations hinder ViTs'
deployment on embedded devices, calling for effective model compression
methods, such as quantization. Unfortunately, due to the existence of
hardware-unfriendly and quantization-sensitive non-linear operations,
particularly {Softmax}, it is non-trivial to completely quantize all operations
in ViTs, yielding either significant accuracy drops or non-negligible hardware
costs. In response to challenges associated with \textit{standard ViTs}, we
focus our attention towards the quantization and acceleration for
\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but
also integrate linear attention with low computational complexity, and propose
\emph{Trio-ViT} accordingly. Specifically, at the algorithm level, we develop a
{tailored post-training quantization engine} taking the unique activation
distributions of Softmax-free efficient ViTs into full consideration, aiming to
boost quantization accuracy. Furthermore, at the hardware level, we build an
accelerator dedicated to the specific Convolution-Transformer hybrid
architecture of efficient ViTs, thereby enhancing hardware efficiency.
Extensive experimental results consistently prove the effectiveness of our
Trio-ViT framework. {Particularly, we can gain up to
$\uparrow$$\mathbf{7.2}\times$ and $\uparrow$$\mathbf{14.6}\times$ FPS under
comparable accuracy over state-of-the-art ViT accelerators, as well as
$\uparrow$$\mathbf{5.9}\times$ and $\uparrow$$\mathbf{2.0}\times$ DSP
efficiency.} Codes will be released publicly upon acceptance.

摘要：

##### **Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions**
2405.03873v1 by Ziye Qin, Siyan Li, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han

Dilemma zones at signalized intersections present a commonly occurring but
unsolved challenge for both drivers and traffic operators. Onsets of the yellow
lights prompt varied responses from different drivers: some may brake abruptly,
compromising the ride comfort, while others may accelerate, increasing the risk
of red-light violations and potential safety hazards. Such diversity in
drivers' stop-or-go decisions may result from not only surrounding traffic
conditions, but also personalized driving behaviors. To this end, identifying
personalized driving behaviors and integrating them into advanced driver
assistance systems (ADAS) to mitigate the dilemma zone problem presents an
intriguing scientific question. In this study, we employ a game engine-based
(i.e., CARLA-enabled) driving simulator to collect high-resolution vehicle
trajectories, incoming traffic signal phase and timing information, and
stop-or-go decisions from four subject drivers in various scenarios. This
approach allows us to analyze personalized driving behaviors in dilemma zones
and develop a Personalized Transformer Encoder to predict individual drivers'
stop-or-go decisions. The results show that the Personalized Transformer
Encoder improves the accuracy of predicting driver decision-making in the
dilemma zone by 3.7% to 12.6% compared to the Generic Transformer Encoder, and
by 16.8% to 21.6% over the binary logistic regression model.

摘要：

##### **AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration**
2405.03870v1 by Widad Elouataoui

The widespread adoption of big data has ushered in a new era of data-driven
decision-making, transforming numerous industries and sectors. However, the
efficacy of these decisions hinges on the quality of the underlying data. Poor
data quality can result in inaccurate analyses and deceptive conclusions.
Managing the vast volume, velocity, and variety of data sources presents
significant challenges, heightening the importance of addressing big data
quality issues. While there has been increased attention from both academia and
industry, current approaches often lack comprehensiveness and universality.
They tend to focus on limited metrics, neglecting other dimensions of data
quality. Moreover, existing methods are often context-specific, limiting their
applicability across different domains. There is a clear need for intelligent,
automated approaches leveraging artificial intelligence (AI) for advanced data
quality corrections.
  To bridge these gaps, this Ph.D. thesis proposes a novel set of
interconnected frameworks aimed at enhancing big data quality comprehensively.
Firstly, we introduce new quality metrics and a weighted scoring system for
precise data quality assessment. Secondly, we present a generic framework for
detecting various quality anomalies using AI models. Thirdly, we propose an
innovative framework for correcting detected anomalies through predictive
modeling. Additionally, we address metadata quality enhancement within big data
ecosystems. These frameworks are rigorously tested on diverse datasets,
demonstrating their efficacy in improving big data quality. Finally, the thesis
concludes with insights and suggestions for future research directions.

摘要：

##### **Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions**
2405.03869v1 by Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu

Influence functions offer a robust framework for assessing the impact of each
training data sample on model predictions, serving as a prominent tool in
data-centric learning. Despite their widespread use in various tasks, the
strong convexity assumption on the model and the computational cost associated
with calculating the inverse of the Hessian matrix pose constraints,
particularly when analyzing large deep models. This paper focuses on a
classical data-centric scenario--trimming detrimental samples--and addresses
both challenges within a unified framework. Specifically, we establish an
equivalence transformation between identifying detrimental training samples via
influence functions and outlier gradient detection. This transformation not
only presents a straightforward and Hessian-free formulation but also provides
profound insights into the role of the gradient in sample impact. Moreover, it
relaxes the convexity assumption of influence functions, extending their
applicability to non-convex deep models. Through systematic empirical
evaluations, we first validate the correctness of our proposed outlier gradient
analysis on synthetic datasets and then demonstrate its effectiveness in
detecting mislabeled samples in vision models, selecting data samples for
improving performance of transformer models for natural language processing,
and identifying influential samples for fine-tuned Large Language Models.

摘要：

##### **Learning Planning Abstractions from Language**
2405.03864v1 by Weiyu Liu, Geng Chen, Joy Hsu, Jiayuan Mao, Jiajun Wu

This paper presents a framework for learning state and action abstractions in
sequential decision-making domains. Our framework, planning abstraction from
language (PARL), utilizes language-annotated demonstrations to automatically
discover a symbolic and abstract action space and induce a latent state
abstraction based on it. PARL consists of three stages: 1) recovering
object-level and action concepts, 2) learning state abstractions, abstract
action feasibility, and transition models, and 3) applying low-level policies
for abstract actions. During inference, given the task description, PARL first
makes abstract action plans using the latent transition and feasibility
functions, then refines the high-level plan using low-level policies. PARL
generalizes across scenarios involving novel object instances and environments,
unseen concept compositions, and tasks that require longer planning horizons
than settings it is trained on.

摘要：

##### **Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration**
2405.03862v1 by Razan Baltaji, Babak Hemmatian, Lav R. Varshney

This study explores the sources of instability in maintaining cultural
personas and opinions within multi-agent LLM systems. Drawing on simulations of
inter-cultural collaboration and debate, we analyze agents' pre- and
post-discussion private responses alongside chat transcripts to assess the
stability of cultural personas and the impact of opinion diversity on group
outcomes. Our findings suggest that multi-agent discussions can encourage
collective decisions that reflect diverse perspectives, yet this benefit is
tempered by the agents' susceptibility to conformity due to perceived peer
pressure and challenges in maintaining consistent personas and opinions.
Counterintuitively, instructions that encourage debate in support of one's
opinions increase the rate of inconstancy. Without addressing the factors we
identify, the full potential of multi-agent frameworks for producing more
culturally diverse AI outputs will remain untapped.

摘要：

##### **VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images**
2405.03852v1 by Anna Penzkofer, Lei Shi, Andreas Bulling

While Vector Symbolic Architectures (VSAs) are promising for modelling
spatial cognition, their application is currently limited to artificially
generated images and simple spatial queries. We propose VSA4VQA - a novel 4D
implementation of VSAs that implements a mental representation of natural
images for the challenging task of Visual Question Answering (VQA). VSA4VQA is
the first model to scale a VSA to complex spatial queries. Our method is based
on the Semantic Pointer Architecture (SPA) to encode objects in a
hyperdimensional vector space. To encode natural images, we extend the SPA to
include dimensions for object's width and height in addition to their spatial
location. To perform spatial queries we further introduce learned spatial query
masks and integrate a pre-trained vision-language model for answering
attribute-related questions. We evaluate our method on the GQA benchmark
dataset and show that it can effectively encode natural images, achieving
competitive performance to state-of-the-art deep learning methods for zero-shot
VQA.

摘要：

##### **Self-Improving Customer Review Response Generation Based on LLMs**
2405.03845v1 by Guy Azov, Tatiana Pelc, Adi Fledel Alon, Gila Kamhi

Previous studies have demonstrated that proactive interaction with user
reviews has a positive impact on the perception of app users and encourages
them to submit revised ratings. Nevertheless, developers encounter challenges
in managing a high volume of reviews, particularly in the case of popular apps
with a substantial influx of daily reviews. Consequently, there is a demand for
automated solutions aimed at streamlining the process of responding to user
reviews. To address this, we have developed a new system for generating
automatic responses by leveraging user-contributed documents with the help of
retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
Our solution, named SCRABLE, represents an adaptive customer review response
automation that enhances itself with self-optimizing prompts and a judging
mechanism based on LLMs. Additionally, we introduce an automatic scoring
mechanism that mimics the role of a human evaluator to assess the quality of
responses generated in customer review domains. Extensive experiments and
analyses conducted on real-world datasets reveal that our method is effective
in producing high-quality responses, yielding improvement of more than 8.5%
compared to the baseline. Further validation through manual examination of the
generated responses underscores the efficacy our proposed system.

摘要：

##### **A Novel Cross-band CSI Prediction Scheme for Multi-band Fingerprint based Localization**
2405.03842v1 by Yuan Ruihao, Huang Kaixuan, Zhang Shunqing

Because of the advantages of computation complexity compared with traditional
localization algorithms, fingerprint based localization is getting increasing
demand. Expanding the fingerprint database from the frequency domain by channel
reconstruction can improve localization accuracy. However, in a mobility
environment, the channel reconstruction accuracy is limited by the time-varying
parameters. In this paper, we proposed a system to extract the time-varying
parameters based on space-alternating generalized expectation maximization
(SAGE) algorithm, then used variational auto-encoder (VAE) to reconstruct the
channel state information on another channel. The proposed scheme is tested on
the data generated by the deep-MIMO channel model. Mathematical analysis for
the viability of our system is also shown in this paper.

摘要：

##### **Guylingo: The Republic of Guyana Creole Corpora**
2405.03832v1 by Christopher Clarke, Roland Daynauth, Charlene Wilkinson, Hubert Devonish, Jason Mars

While major languages often enjoy substantial attention and resources, the
linguistic diversity across the globe encompasses a multitude of smaller,
indigenous, and regional languages that lack the same level of computational
support. One such region is the Caribbean. While commonly labeled as "English
speaking", the ex-British Caribbean region consists of a myriad of Creole
languages thriving alongside English. In this paper, we present Guylingo: a
comprehensive corpus designed for advancing NLP research in the domain of
Creolese (Guyanese English-lexicon Creole), the most widely spoken language in
the culturally rich nation of Guyana. We first outline our framework for
gathering and digitizing this diverse corpus, inclusive of colloquial
expressions, idioms, and regional variations in a low-resource language. We
then demonstrate the challenges of training and evaluating NLP models for
machine translation in Creole. Lastly, we discuss the unique opportunities
presented by recent NLP advancements for accelerating the formal adoption of
Creole languages as official languages in the Caribbean.

摘要：

##### **Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence**
2405.03825v1 by Silvan Ferreira, Ivanovitch Silva, Allan Martins

Recent developments in Large Language Models (LLMs) have significantly
expanded their applications across various domains. However, the effectiveness
of LLMs is often constrained when operating individually in complex
environments. This paper introduces a transformative approach by organizing
LLMs into community-based structures, aimed at enhancing their collective
intelligence and problem-solving capabilities. We investigate different
organizational models-hierarchical, flat, dynamic, and federated-each
presenting unique benefits and challenges for collaborative AI systems. Within
these structured communities, LLMs are designed to specialize in distinct
cognitive tasks, employ advanced interaction mechanisms such as direct
communication, voting systems, and market-based approaches, and dynamically
adjust their governance structures to meet changing demands. The implementation
of such communities holds substantial promise for improve problem-solving
capabilities in AI, prompting an in-depth examination of their ethical
considerations, management strategies, and scalability potential. This position
paper seeks to lay the groundwork for future research, advocating a paradigm
shift from isolated to synergistic operational frameworks in AI research and
application.

摘要：

##### **Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models**
2405.03821v1 by Evan King, Haoxiang Yu, Sahil Vartak, Jenna Jacob, Sangsu Lee, Christine Julien

Everyday devices like light bulbs and kitchen appliances are now embedded
with so many features and automated behaviors that they have become complicated
to actually use. While such "smart" capabilities can better support users'
goals, the task of learning the "ins and outs" of different devices is
daunting. Voice assistants aim to solve this problem by providing a natural
language interface to devices, yet such assistants cannot understand
loosely-constrained commands, they lack the ability to reason about and explain
devices' behaviors to users, and they rely on connectivity to intrusive cloud
infrastructure. Toward addressing these issues, we propose thoughtful things:
devices that leverage lightweight, on-device language models to take actions
and explain their behaviors in response to unconstrained user commands. We
propose an end-to-end framework that leverages formal modeling, automated
training data synthesis, and generative language models to create devices that
are both capable and thoughtful in the presence of unconstrained user goals and
inquiries. Our framework requires no labeled data and can be deployed
on-device, with no cloud dependency. We implement two thoughtful things (a lamp
and a thermostat) and deploy them on real hardware, evaluating their practical
performance.

摘要：

##### **SocialFormer: Social Interaction Modeling with Edge-enhanced Heterogeneous Graph Transformers for Trajectory Prediction**
2405.03809v1 by Zixu Wang, Zhigang Sun, Juergen Luettin, Lavdim Halilaj

Accurate trajectory prediction is crucial for ensuring safe and efficient
autonomous driving. However, most existing methods overlook complex
interactions between traffic participants that often govern their future
trajectories. In this paper, we propose SocialFormer, an agent
interaction-aware trajectory prediction method that leverages the semantic
relationship between the target vehicle and surrounding vehicles by making use
of the road topology. We also introduce an edge-enhanced heterogeneous graph
transformer (EHGT) as the aggregator in a graph neural network (GNN) to encode
the semantic and spatial agent interaction information. Additionally, we
introduce a temporal encoder based on gated recurrent units (GRU) to model the
temporal social behavior of agent movements. Finally, we present an information
fusion framework that integrates agent encoding, lane encoding, and agent
interaction encoding for a holistic representation of the traffic scene. We
evaluate SocialFormer for the trajectory prediction task on the popular
nuScenes benchmark and achieve state-of-the-art performance.

摘要：

##### **Synthetic Data from Diffusion Models Improve Drug Discovery Prediction**
2405.03799v1 by Bing Hu, Ashish Saragadam, Anita Layton, Helen Chen

Artificial intelligence (AI) is increasingly used in every stage of drug
development. Continuing breakthroughs in AI-based methods for drug discovery
require the creation, improvement, and refinement of drug discovery data. We
posit a new data challenge that slows the advancement of drug discovery AI:
datasets are often collected independently from each other, often with little
overlap, creating data sparsity. Data sparsity makes data curation difficult
for researchers looking to answer key research questions requiring values posed
across multiple datasets. We propose a novel diffusion GNN model Syngand
capable of generating ligand and pharmacokinetic data end-to-end. We show and
provide a methodology for sampling pharmacokinetic data for existing ligands
using our Syngand model. We show the initial promising results on the efficacy
of the Syngand-generated synthetic target property data on downstream
regression tasks with AqSolDB, LD50, and hERG central. Using our proposed model
and methodology, researchers can easily generate synthetic ligand data to help
them explore research questions that require data spanning multiple datasets.

摘要：

##### **Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models**
2405.03794v1 by Dengyi Liu, Minghao Wang, Andrew G. Catlin

Academic researchers and social media entities grappling with the
identification of hate speech face significant challenges, primarily due to the
vast scale of data and the dynamic nature of hate speech. Given the ethical and
practical limitations of large predictive models like ChatGPT in directly
addressing such sensitive issues, our research has explored alternative
advanced transformer-based and generative AI technologies since 2019.
Specifically, we developed a new data labeling technique and established a
proof of concept targeting anti-Semitic hate speech, utilizing a variety of
transformer models such as BERT (arXiv:1810.04805), DistillBERT
(arXiv:1910.01108), RoBERTa (arXiv:1907.11692), and LLaMA-2 (arXiv:2307.09288),
complemented by the LoRA fine-tuning approach (arXiv:2106.09685). This paper
delineates and evaluates the comparative efficacy of these cutting-edge methods
in tackling the intricacies of hate speech detection, highlighting the need for
responsible and carefully managed AI applications within sensitive contexts.

摘要：

