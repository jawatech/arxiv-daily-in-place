{"2406.16743": {"publish_time": "2024-06-24", "title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization", "paper_summary": "With the widespread application of Large Language Models (LLMs), it has\nbecome a significant concern to ensure their safety and prevent harmful\nresponses. While current safe-alignment methods based on instruction\nfine-tuning and Reinforcement Learning from Human Feedback (RLHF) can\neffectively reduce harmful responses from LLMs, they often require high-quality\ndatasets and heavy computational overhead during model training. Another way to\nalign language models is to modify the logit of tokens in model outputs without\nheavy training. Recent studies have shown that contrastive decoding can enhance\nthe performance of language models by reducing the likelihood of confused\ntokens. However, these methods require the manual selection of contrastive\nmodels or instruction templates. To this end, we propose Adversarial\nContrastive Decoding (ACD), an optimization-based framework to generate two\nopposite system prompts for prompt-based contrastive decoding. ACD only needs\nto apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min\nfor each model) without training the target model. Experiments conducted on\nextensive models and benchmarks demonstrate that the proposed method achieves\nmuch better safety performance than previous model training-free decoding\nmethods without sacrificing its original generation ability.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5ee3\u6cdb\u61c9\u7528\uff0c\u78ba\u4fdd\u5176\u5b89\u5168\u6027\u4e26\u9632\u6b62\u6709\u5bb3\u56de\u61c9\u5df2\u6210\u70ba\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\u3002\u96d6\u7136\u57fa\u65bc\u6307\u4ee4\u5fae\u8abf\u548c\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u7684\u73fe\u6709\u5b89\u5168\u5c0d\u9f4a\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u6e1b\u5c11 LLM \u7684\u6709\u5bb3\u56de\u61c9\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u9700\u8981\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u548c\u5728\u6a21\u578b\u8a13\u7df4\u671f\u9593\u5927\u91cf\u7684\u8a08\u7b97\u8ca0\u64d4\u3002\u5c0d\u9f4a\u8a9e\u8a00\u6a21\u578b\u7684\u53e6\u4e00\u7a2e\u65b9\u6cd5\u662f\u5728\u4e0d\u9032\u884c\u5927\u91cf\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u4fee\u6539\u6a21\u578b\u8f38\u51fa\u4e2d\u6a19\u8a18\u7684 logit\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c0d\u6bd4\u89e3\u78bc\u53ef\u4ee5\u900f\u904e\u964d\u4f4e\u6df7\u6dc6\u6a19\u8a18\u7684\u53ef\u80fd\u6027\u4f86\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u9700\u8981\u624b\u52d5\u9078\u64c7\u5c0d\u6bd4\u6a21\u578b\u6216\u6307\u4ee4\u7bc4\u672c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5c0d\u6297\u6027\u5c0d\u6bd4\u89e3\u78bc (ACD)\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u6700\u4f73\u5316\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u70ba\u57fa\u65bc\u63d0\u793a\u7684\u5c0d\u6bd4\u89e3\u78bc\u7522\u751f\u5169\u500b\u76f8\u53cd\u7684\u7cfb\u7d71\u63d0\u793a\u3002ACD \u53ea\u9700\u8981\u5728\u4e00\u500b\u76f8\u7576\u5c0f\u7684\u9328\u5b9a\u8cc7\u6599\u96c6\u4e0a\u5957\u7528\u8f15\u91cf\u7d1a\u63d0\u793a\u8abf\u6574\uff08\u6bcf\u500b\u6a21\u578b\u5c0f\u65bc 3 \u5206\u9418\uff09\uff0c\u800c\u7121\u9700\u8a13\u7df4\u76ee\u6a19\u6a21\u578b\u3002\u5728\u5ee3\u6cdb\u7684\u6a21\u578b\u548c\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u5148\u524d\u7684\u6a21\u578b\u8a13\u7df4\u514d\u8cbb\u89e3\u78bc\u65b9\u6cd5\u5be6\u73fe\u4e86\u66f4\u597d\u7684\u5b89\u5168\u6027\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u5176\u539f\u59cb\u7684\u751f\u6210\u80fd\u529b\u3002", "author": "Zhengyue Zhao et.al.", "authors": "Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen", "id": "2406.16743v1", "paper_url": "http://arxiv.org/abs/2406.16743v1", "repo": "null"}}