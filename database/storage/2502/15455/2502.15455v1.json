{"2502.15455": {"publish_time": "2025-02-21", "title": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning", "paper_summary": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6210\u672c\u65b9\u9762\u6210\u672c\u9ad8\u5f97\u4ee4\u4eba\u671b\u800c\u537b\u6b65\u3002\u4f4e\u79e9\u9069\u61c9 (LoRA) \u4f5c\u70ba\u6700\u6d41\u884c\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u4e4b\u4e00\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u904e\u4e0b\u6295\u5f71\u77e9\u9663 A \u2208 Rmxr \u548c\u982d\u77e9\u9663 B \u2208 Rrxn \u7684\u4e58\u7a4d\u4f86\u8fd1\u4f3c\u6a21\u578b\u8b8a\u5316 \u0394W \u2208 Rmxn\uff0c\u5176\u4e2d r \u226a min(m, n)\u3002\u5728\u5be6\u969b\u5834\u666f\u4e2d\uff0cLLM \u6703\u91dd\u5c0d\u4f86\u81ea\u591a\u500b\u7db2\u57df\u7684\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u57f7\u884c\u8de8\u8d8a\u5404\u7a2e\u9818\u57df\u7684\u4efb\u52d9\uff0c\u9ad4\u73fe\u591a\u4efb\u52d9\u5b78\u7fd2 (MTL)\u3002LoRA \u5728\u9019\u7a2e\u8907\u96dc\u7684\u5834\u666f\u4e2d\u5e38\u5e38\u8868\u73fe\u4e0d\u4f73\u3002\u70ba\u4e86\u589e\u5f37 LoRA \u5728\u591a\u4efb\u52d9\u5b78\u7fd2\u4e2d\u7684\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86 R-LoRA\uff0c\u5b83\u7d50\u5408\u4e86\u591a\u982d\u96a8\u6a5f\u5316\u3002\u591a\u982d\u96a8\u6a5f\u5316\u901a\u904e\u591a\u982d\u96a8\u6a5f\u521d\u59cb\u5316\u548c\u591a\u982d\u4e2d\u65b7\u4f86\u4f7f\u982d\u77e9\u9663\u591a\u6a23\u5316\uff0c\u5f9e\u800c\u5728\u7dad\u8b77\u5171\u4eab\u77e5\u8b58\u8868\u793a\u7684\u540c\u6642\uff0c\u66f4\u6709\u6548\u5730\u5b78\u7fd2\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u7279\u5fb5\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8868\u660e\uff0cR-LoRA \u66f4\u5584\u65bc\u64f7\u53d6\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u77e5\u8b58\uff0c\u5f9e\u800c\u63d0\u9ad8\u591a\u4efb\u52d9\u5834\u666f\u4e2d\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/jinda-liu/R-LoRA \u53d6\u5f97\u3002", "author": "Jinda Liu et.al.", "authors": "Jinda Liu, Yi Chang, Yuan Wu", "id": "2502.15455v1", "paper_url": "http://arxiv.org/abs/2502.15455v1", "repo": "null"}}