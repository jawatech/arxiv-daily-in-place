{"2501.18922": {"publish_time": "2025-01-31", "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "paper_summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.", "paper_summary_zh": "\u77e5\u8b58\u5eab\u554f\u7b54 (KBQA) \u7684\u76ee\u6a19\u662f\u4f7f\u7528\u5927\u898f\u6a21\u7d50\u69cb\u5316\u77e5\u8b58\u5eab (KB) \u4f86\u56de\u7b54\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6709\u9032\u5c55\uff0c\u4f46 KBQA \u4ecd\u9762\u81e8 KB \u610f\u8b58\u8584\u5f31\u3001\u6548\u80fd\u8207\u6548\u7387\u5931\u8861\uff0c\u4ee5\u53ca\u9ad8\u5ea6\u4f9d\u8cf4\u8a3b\u91cb\u8cc7\u6599\u7684\u6311\u6230\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa KBQA-o1\uff0c\u4e00\u7a2e\u7d50\u5408\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b (MCTS) \u7684\u65b0\u7a4e\u667a\u80fd KBQA \u65b9\u6cd5\u3002\u5b83\u5f15\u5165\u4e86\u57fa\u65bc ReAct \u7684\u4ee3\u7406\u7a0b\u5e8f\u6d41\u7a0b\uff0c\u7528\u65bc\u9010\u6b65\u7522\u751f\u908f\u8f2f\u5f62\u5f0f\u4e26\u63a2\u7d22 KB \u74b0\u5883\u3002\u6b64\u5916\uff0c\u5b83\u63a1\u7528 MCTS\uff0c\u4e00\u7a2e\u7531\u7b56\u7565\u548c\u734e\u52f5\u6a21\u578b\u9a45\u52d5\u7684\u555f\u767c\u5f0f\u641c\u5c0b\u65b9\u6cd5\uff0c\u4ee5\u5e73\u8861\u4ee3\u7406\u63a2\u7d22\u7684\u6548\u80fd\u548c\u641c\u5c0b\u7a7a\u9593\u3002\u900f\u904e\u555f\u767c\u5f0f\u63a2\u7d22\uff0cKBQA-o1 \u6703\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u8a3b\u91cb\uff0c\u4ee5\u4fbf\u900f\u904e\u6f38\u9032\u5f0f\u5fae\u8abf\u9032\u4e00\u6b65\u6539\u5584\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cKBQA-o1 \u5728\u8a3b\u91cb\u8cc7\u6599\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u512a\u65bc\u5148\u524d\u7684\u4f4e\u8cc7\u6e90 KBQA \u65b9\u6cd5\uff0c\u5c07 Llama-3.1-8B \u6a21\u578b\u7684 GrailQA F1 \u6548\u80fd\u63d0\u5347\u81f3 78.5%\uff0c\u800c\u5148\u524d\u4f7f\u7528 GPT-3.5-turbo \u7684 sota \u65b9\u6cd5\u50c5\u70ba 48.5%\u3002", "author": "Haoran Luo et.al.", "authors": "Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan", "id": "2501.18922v1", "paper_url": "http://arxiv.org/abs/2501.18922v1", "repo": "https://github.com/lhrlab/kbqa-o1"}}