{"2502.04778": {"publish_time": "2025-02-07", "title": "Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning", "paper_summary": "The primary focus of offline reinforcement learning (RL) is to manage the\nrisk of hazardous exploitation of out-of-distribution actions. An effective\napproach to achieve this goal is through behavior regularization, which\naugments conventional RL objectives by incorporating constraints that enforce\nthe policy to remain close to the behavior policy. Nevertheless, existing\nliterature on behavior-regularized RL primarily focuses on explicit policy\nparameterizations, such as Gaussian policies. Consequently, it remains unclear\nhow to extend this framework to more advanced policy parameterizations, such as\ndiffusion models. In this paper, we introduce BDPO, a principled\nbehavior-regularized RL framework tailored for diffusion-based policies,\nthereby combining the expressive power of diffusion policies and the robustness\nprovided by regularization. The key ingredient of our method is to calculate\nthe Kullback-Leibler (KL) regularization analytically as the accumulated\ndiscrepancies in reverse-time transition kernels along the diffusion\ntrajectory. By integrating the regularization, we develop an efficient\ntwo-time-scale actor-critic RL algorithm that produces the optimal policy while\nrespecting the behavior constraint. Comprehensive evaluations conducted on\nsynthetic 2D tasks and continuous control tasks from the D4RL benchmark\nvalidate its effectiveness and superior performance.", "paper_summary_zh": "\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2 (RL) \u7684\u4e3b\u8981\u91cd\u9ede\u662f\u7ba1\u7406\u975e\u5206\u4f48\u52d5\u4f5c\u7684\u5371\u96aa\u5229\u7528\u98a8\u96aa\u3002\u5be6\u73fe\u6b64\u76ee\u6a19\u7684\u4e00\u7a2e\u6709\u6548\u65b9\u6cd5\u662f\u900f\u904e\u884c\u70ba\u6b63\u5247\u5316\uff0c\u5b83\u900f\u904e\u7d0d\u5165\u7d04\u675f\u4f86\u64f4\u5145\u50b3\u7d71\u7684 RL \u76ee\u6a19\uff0c\u4ee5\u5f37\u5236\u653f\u7b56\u4fdd\u6301\u63a5\u8fd1\u884c\u70ba\u653f\u7b56\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u73fe\u6709\u7684\u884c\u70ba\u6b63\u5247\u5316 RL \u6587\u737b\u4e3b\u8981\u96c6\u4e2d\u5728\u660e\u78ba\u7684\u653f\u7b56\u53c3\u6578\u5316\uff0c\u4f8b\u5982\u9ad8\u65af\u653f\u7b56\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5c07\u6b64\u67b6\u69cb\u64f4\u5c55\u5230\u66f4\u9032\u968e\u7684\u653f\u7b56\u53c3\u6578\u5316\uff0c\u4f8b\u5982\u64f4\u6563\u6a21\u578b\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 BDPO\uff0c\u4e00\u500b\u5c08\u9580\u91dd\u5c0d\u57fa\u65bc\u64f4\u6563\u7684\u653f\u7b56\u7684\u539f\u5247\u6027\u884c\u70ba\u6b63\u5247\u5316 RL \u67b6\u69cb\uff0c\u5f9e\u800c\u7d50\u5408\u4e86\u64f4\u6563\u653f\u7b56\u7684\u8868\u73fe\u529b\u8207\u6b63\u5247\u5316\u63d0\u4f9b\u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u65b9\u6cd5\u7684\u95dc\u9375\u8981\u7d20\u662f\u5c07 Kullback-Leibler (KL) \u6b63\u5247\u5316\u5206\u6790\u8a08\u7b97\u70ba\u6cbf\u8457\u64f4\u6563\u8ecc\u8de1\u7684\u53cd\u5411\u6642\u9593\u8f49\u63db\u6838\u5fc3\u4e2d\u7d2f\u7a4d\u7684\u5dee\u7570\u3002\u900f\u904e\u6574\u5408\u6b63\u5247\u5316\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u6709\u6548\u7387\u7684\u4e8c\u6642\u6a19\u5c3a\u52d5\u4f5c-\u8a55\u8ad6\u5bb6 RL \u6f14\u7b97\u6cd5\uff0c\u53ef\u7522\u751f\u6700\u4f73\u653f\u7b56\uff0c\u540c\u6642\u9075\u5b88\u884c\u70ba\u7d04\u675f\u3002\u5728 D4RL \u57fa\u6e96\u7684\u5408\u6210 2D \u4efb\u52d9\u548c\u9023\u7e8c\u63a7\u5236\u4efb\u52d9\u4e0a\u9032\u884c\u7684\u5168\u9762\u8a55\u4f30\u9a57\u8b49\u4e86\u5176\u6709\u6548\u6027\u548c\u512a\u7570\u7684\u6548\u80fd\u3002", "author": "Chen-Xiao Gao et.al.", "authors": "Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Chenjun Xiao, Yang Yu, Zongzhang Zhang", "id": "2502.04778v1", "paper_url": "http://arxiv.org/abs/2502.04778v1", "repo": "null"}}