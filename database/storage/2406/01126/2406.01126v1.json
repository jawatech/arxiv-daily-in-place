{"2406.01126": {"publish_time": "2024-06-03", "title": "TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine", "paper_summary": "Large language models (LLMs) have performed remarkably well in various\nnatural language processing tasks by benchmarking, including in the Western\nmedical domain. However, the professional evaluation benchmarks for LLMs have\nyet to be covered in the traditional Chinese medicine(TCM) domain, which has a\nprofound history and vast influence. To address this research gap, we introduce\nTCM-Bench, an comprehensive benchmark for evaluating LLM performance in TCM. It\ncomprises the TCM-ED dataset, consisting of 5,473 questions sourced from the\nTCM Licensing Exam (TCMLE), including 1,300 questions with authoritative\nanalysis. It covers the core components of TCMLE, including TCM basis and\nclinical practice. To evaluate LLMs beyond accuracy of question answering, we\npropose TCMScore, a metric tailored for evaluating the quality of answers\ngenerated by LLMs for TCM related questions. It comprehensively considers the\nconsistency of TCM semantics and knowledge. After conducting comprehensive\nexperimental analyses from diverse perspectives, we can obtain the following\nfindings: (1) The unsatisfactory performance of LLMs on this benchmark\nunderscores their significant room for improvement in TCM. (2) Introducing\ndomain knowledge can enhance LLMs' performance. However, for in-domain models\nlike ZhongJing-TCM, the quality of generated analysis text has decreased, and\nwe hypothesize that their fine-tuning process affects the basic LLM\ncapabilities. (3) Traditional metrics for text generation quality like Rouge\nand BertScore are susceptible to text length and surface semantic ambiguity,\nwhile domain-specific metrics such as TCMScore can further supplement and\nexplain their evaluation results. These findings highlight the capabilities and\nlimitations of LLMs in the TCM and aim to provide a more profound assistance to\nmedical research.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5305\u62ec\u5728\u897f\u65b9\u91ab\u5b78\u9818\u57df\u3002\u7136\u800c\uff0cLLM \u7684\u5c08\u696d\u8a55\u4f30\u57fa\u6e96\u5c1a\u672a\u6db5\u84cb\u4e2d\u91ab\u9818\u57df\uff0c\u800c\u4e2d\u91ab\u9818\u57df\u64c1\u6709\u6df1\u539a\u7684\u6b77\u53f2\u548c\u5ee3\u6cdb\u7684\u5f71\u97ff\u529b\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u6211\u5011\u5f15\u5165\u4e86 TCM-Bench\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc\u8a55\u4f30 LLM \u5728\u4e2d\u91ab\u9818\u57df\u8868\u73fe\u7684\u7d9c\u5408\u57fa\u6e96\u3002\u5b83\u5305\u542b TCM-ED \u6578\u64da\u96c6\uff0c\u5176\u4e2d\u5305\u542b 5,473 \u500b\u4f86\u81ea\u4e2d\u91ab\u57f7\u7167\u8003\u8a66 (TCMLE) \u7684\u554f\u984c\uff0c\u5305\u62ec 1,300 \u500b\u5177\u6709\u6b0a\u5a01\u5206\u6790\u7684\u554f\u984c\u3002\u5b83\u6db5\u84cb\u4e86 TCMLE \u7684\u6838\u5fc3\u7d44\u6210\u90e8\u5206\uff0c\u5305\u62ec\u4e2d\u91ab\u57fa\u790e\u548c\u81e8\u5e8a\u5be6\u8e10\u3002\u70ba\u4e86\u8a55\u4f30 LLM \u8d85\u8d8a\u554f\u984c\u56de\u7b54\u7684\u6e96\u78ba\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TCMScore\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u7528\u65bc\u8a55\u4f30 LLM \u70ba\u4e2d\u91ab\u76f8\u95dc\u554f\u984c\u751f\u6210\u7684\u7b54\u6848\u8cea\u91cf\u7684\u6307\u6a19\u3002\u5b83\u5168\u9762\u8003\u616e\u4e86\u4e2d\u91ab\u8a9e\u7fa9\u548c\u77e5\u8b58\u7684\u4e00\u81f4\u6027\u3002\u5728\u5f9e\u4e0d\u540c\u89d2\u5ea6\u9032\u884c\u7d9c\u5408\u5be6\u9a57\u5206\u6790\u5f8c\uff0c\u6211\u5011\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4e0b\u767c\u73fe\uff1a(1) LLM \u5728\u6b64\u57fa\u6e96\u4e0a\u7684\u8868\u73fe\u4e0d\u76e1\u4eba\u610f\uff0c\u9019\u51f8\u986f\u4e86\u5b83\u5011\u5728\u4e2d\u91ab\u9818\u57df\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002(2) \u5f15\u5165\u9818\u57df\u77e5\u8b58\u53ef\u4ee5\u63d0\u5347 LLM \u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u50cf ZhongJing-TCM \u9019\u6a23\u7684\u9818\u57df\u6a21\u578b\uff0c\u751f\u6210\u7684\u5206\u6790\u6587\u672c\u7684\u8cea\u91cf\u6709\u6240\u4e0b\u964d\uff0c\u6211\u5011\u5047\u8a2d\u5b83\u5011\u7684\u5fae\u8abf\u904e\u7a0b\u5f71\u97ff\u4e86\u57fa\u672c\u7684 LLM \u80fd\u529b\u3002(3) \u50b3\u7d71\u7684\u6587\u672c\u751f\u6210\u8cea\u91cf\u6307\u6a19\uff0c\u4f8b\u5982 Rouge \u548c BertScore\uff0c\u5bb9\u6613\u53d7\u5230\u6587\u672c\u9577\u5ea6\u548c\u8868\u9762\u8a9e\u7fa9\u6a21\u7cca\u6027\u7684\u5f71\u97ff\uff0c\u800c\u50cf TCMScore \u9019\u6a23\u7684\u7279\u5b9a\u9818\u57df\u6307\u6a19\u53ef\u4ee5\u9032\u4e00\u6b65\u88dc\u5145\u548c\u89e3\u91cb\u5b83\u5011\u7684\u8a55\u4f30\u7d50\u679c\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u51fa\u4e86 LLM \u5728\u4e2d\u91ab\u9818\u57df\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e26\u65e8\u5728\u70ba\u91ab\u5b78\u7814\u7a76\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u5e6b\u52a9\u3002</paragraph>", "author": "Wenjing Yue et.al.", "authors": "Wenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan, Huanran Zheng, Pengfei Wang, Changzhi Sun, Xin Ma", "id": "2406.01126v1", "paper_url": "http://arxiv.org/abs/2406.01126v1", "repo": "null"}}