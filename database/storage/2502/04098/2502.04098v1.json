{"2502.04098": {"publish_time": "2025-02-06", "title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "paper_summary": "Vision-language models (VLMs) excel in tasks such as visual question\nanswering and image captioning. However, VLMs are often limited by their use of\npretrained image encoders, like CLIP, leading to image understanding errors\nthat hinder overall performance. On top of that, real-world applications often\nrequire the model to be continuously adapted as new and often limited data\ncontinuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation\nwith Structured Updates), a robust and computationally efficient method for\nselectively updating image encoders within VLMs. LoRSU introduces structured\nand localized parameter updates, effectively correcting performance on\npreviously error-prone data while preserving the model's general robustness.\nOur approach leverages theoretical insights to identify and update only the\nmost critical parameters, achieving significant resource efficiency.\nSpecifically, we demonstrate that LoRSU reduces computational overhead by over\n25x compared to full VLM updates, without sacrificing performance. Experimental\nresults on VQA tasks in the few-shot continual learning setting, validate\nLoRSU's scalability, efficiency, and effectiveness, making it a compelling\nsolution for image encoder adaptation in resource-constrained environments.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u8996\u89ba\u554f\u984c\u89e3\u7b54\u548c\u5f71\u50cf\u6a19\u984c\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0cVLM \u7d93\u5e38\u53d7\u5230\u5176\u4f7f\u7528\u9810\u8a13\u7df4\u5f71\u50cf\u7de8\u78bc\u5668\uff08\u4f8b\u5982 CLIP\uff09\u7684\u9650\u5236\uff0c\u5c0e\u81f4\u5f71\u50cf\u7406\u89e3\u932f\u8aa4\uff0c\u9032\u800c\u963b\u7919\u6574\u9ad4\u6548\u80fd\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u73fe\u5be6\u4e16\u754c\u7684\u61c9\u7528\u7a0b\u5f0f\u901a\u5e38\u8981\u6c42\u6a21\u578b\u6301\u7e8c\u9069\u61c9\uff0c\u56e0\u70ba\u65b0\u7684\u4e14\u7d93\u5e38\u6709\u9650\u7684\u8cc7\u6599\u6703\u6301\u7e8c\u6e67\u5165\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa LoRSU\uff08\u7d50\u69cb\u5316\u66f4\u65b0\u7684\u4f4e\u79e9\u9069\u61c9\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u5f37\u5065\u4e14\u904b\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u5f0f\uff0c\u7528\u65bc\u9078\u64c7\u6027\u5730\u66f4\u65b0 VLM \u4e2d\u7684\u5f71\u50cf\u7de8\u78bc\u5668\u3002LoRSU \u5f15\u5165\u7d50\u69cb\u5316\u4e14\u5c40\u90e8\u5316\u7684\u53c3\u6578\u66f4\u65b0\uff0c\u6709\u6548\u5730\u4fee\u6b63\u5148\u524d\u5bb9\u6613\u51fa\u932f\u8cc7\u6599\u7684\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u7559\u6a21\u578b\u7684\u6574\u9ad4\u5f37\u5065\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u7406\u8ad6\u898b\u89e3\u4f86\u8b58\u5225\u548c\u66f4\u65b0\u6700\u91cd\u8981\u7684\u53c3\u6578\uff0c\u9032\u800c\u9054\u6210\u986f\u8457\u7684\u8cc7\u6e90\u6548\u7387\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8b49\u660e LoRSU \u5c07\u904b\u7b97\u958b\u92b7\u6e1b\u5c11\u4e86 25 \u500d\u4ee5\u4e0a\uff08\u8207\u5b8c\u6574\u7684 VLM \u66f4\u65b0\u76f8\u6bd4\uff09\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u6548\u80fd\u3002\u5728\u5c11\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u7684 VQA \u4efb\u52d9\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\u9a57\u8b49\u4e86 LoRSU \u7684\u53ef\u64f4\u5145\u6027\u3001\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u4f7f\u5176\u6210\u70ba\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u5f71\u50cf\u7de8\u78bc\u5668\u9069\u61c9\u7684\u5f15\u4eba\u6ce8\u76ee\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Aristeidis Panos et.al.", "authors": "Aristeidis Panos, Rahaf Aljundi, Daniel Olmeda Reino, Richard E. Turner", "id": "2502.04098v1", "paper_url": "http://arxiv.org/abs/2502.04098v1", "repo": "null"}}