{"2406.09948": {"publish_time": "2024-06-14", "title": "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages", "paper_summary": "Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u7f3a\u4e4f\u5c0d\u65e5\u5e38\u751f\u6d3b\u7684\u6587\u5316\u7279\u5b9a\u77e5\u8b58\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u7684\u5730\u5340\u548c\u975e\u82f1\u8a9e\u8a9e\u8a00\u4e2d\u3002\u73fe\u6709\u7684\u7528\u65bc\u8a55\u4f30 LLM \u6587\u5316\u654f\u611f\u5ea6\u7684\u57fa\u6e96\u50c5\u9650\u65bc\u55ae\u4e00\u8a9e\u8a00\u6216\u5f9e\u7db2\u4e0a\u4f86\u6e90\uff08\u4f8b\u5982\u7dad\u57fa\u767e\u79d1\uff09\u6536\u96c6\uff0c\u800c\u9019\u4e9b\u4f86\u6e90\u4e26\u4e0d\u80fd\u53cd\u6620\u4e0d\u540c\u5730\u5340\u7684\u65e5\u5e38\u5e73\u6de1\u751f\u6d3b\u65b9\u5f0f\u3002\u4e5f\u5c31\u662f\u8aaa\uff0c\u4eba\u5011\u5728\u751f\u65e5\u6176\u795d\u6d3b\u52d5\u4e2d\u5403\u7684\u98df\u7269\u3001\u4ed6\u5011\u901a\u5e38\u4f7f\u7528\u7684\u9999\u6599\u3001\u5e74\u8f15\u4eba\u6f14\u594f\u7684\u6a02\u5668\u6216\u4ed6\u5011\u5728\u5b78\u6821\u7df4\u7fd2\u7684\u904b\u52d5\u662f\u5e38\u898b\u7684\u6587\u5316\u77e5\u8b58\uff0c\u4f46\u5728\u5bb9\u6613\u6536\u96c6\u7684\u7db2\u4e0a\u4f86\u6e90\u4e2d\u4e26\u4e0d\u5e38\u898b\uff0c\u5c24\u5176\u662f\u5c0d\u65bc\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6587\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 BLEnD\uff0c\u9019\u662f\u4e00\u500b\u624b\u5de5\u88fd\u4f5c\u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u5728\u4e0d\u540c\u6587\u5316\u548c\u8a9e\u8a00\u4e2d\u7684\u65e5\u5e38\u77e5\u8b58\u3002BLEnD \u5305\u542b\u4f86\u81ea 16 \u500b\u570b\u5bb6/\u5730\u5340\u7684 52.6k \u500b\u554f\u7b54\u5c0d\uff0c\u4f7f\u7528 13 \u7a2e\u4e0d\u540c\u7684\u8a9e\u8a00\uff0c\u5305\u62ec\u963f\u59c6\u54c8\u62c9\u8a9e\u3001\u963f\u85a9\u59c6\u8a9e\u3001\u4e9e\u585e\u62dc\u7136\u8a9e\u3001\u8c6a\u85a9\u8a9e\u548c\u5dfd\u4ed6\u8a9e\u7b49\u8cc7\u6e90\u5331\u4e4f\u7684\u8a9e\u8a00\u3002\u6211\u5011\u69cb\u5efa\u57fa\u6e96\u4ee5\u5305\u542b\u5169\u7a2e\u683c\u5f0f\u7684\u554f\u984c\uff1a\u7c21\u7b54\u548c\u591a\u9078\u3002\u6211\u5011\u5c55\u793a\u4e86 LLM \u5c0d\u5728\u7db2\u4e0a\u9ad8\u5ea6\u4ee3\u8868\u7684\u6587\u5316\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u5728\u7c21\u7b54\u683c\u5f0f\u4e2d\uff0c\u8868\u73fe\u6700\u597d\u7684\u6a21\u578b GPT-4 \u7684\u6700\u5927\u5dee\u7570\u70ba 57.34%\u3002\u5c0d\u65bc\u7531\u4e2d\u7b49\u81f3\u9ad8\u8cc7\u6e90\u8a9e\u8a00\u4ee3\u8868\u7684\u6587\u5316\uff0cLLM \u5728\u5176\u7576\u5730\u8a9e\u8a00\u4e2d\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u4f46\u5c0d\u65bc\u7531\u8cc7\u6e90\u5331\u4e4f\u7684\u8a9e\u8a00\u4ee3\u8868\u7684\u6587\u5316\uff0cLLM \u5728\u82f1\u8a9e\u4e2d\u7684\u8868\u73fe\u512a\u65bc\u7576\u5730\u8a9e\u8a00\u3002\u6211\u5011\u5728 https://github.com/nlee0212/BLEnD \u4e0a\u516c\u958b\u6211\u5011\u7684\u6578\u64da\u96c6\u3002", "author": "Junho Myung et.al.", "authors": "Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, V\u00edctor Guti\u00e9rrez-Basulto, Yazm\u00edn Ib\u00e1\u00f1ez-Garc\u00eda, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh", "id": "2406.09948v1", "paper_url": "http://arxiv.org/abs/2406.09948v1", "repo": "https://github.com/nlee0212/blend"}}