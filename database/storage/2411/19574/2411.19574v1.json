{"2411.19574": {"publish_time": "2024-11-29", "title": "KV Shifting Attention Enhances Language Modeling", "paper_summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.", "paper_summary_zh": "\u76ee\u524d\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u65bc\u50c5\u89e3\u78bc\u7d50\u69cb\u7684 Transformer\uff0c\u5b83\u5177\u6709\u5f37\u5927\u7684\u60c5\u5883\u5167\u5b78\u7fd2 (ICL) \u80fd\u529b\u3002\u4e00\u822c\u8a8d\u70ba\u5176 ICL \u80fd\u529b\u7684\u91cd\u8981\u57fa\u790e\u662f\u6b78\u7d0d\u982d\u6a5f\u5236\uff0c\u9019\u9700\u8981\u81f3\u5c11\u5169\u5c64\u6ce8\u610f\u529b\u3002\u70ba\u4e86\u66f4\u6709\u6548\u5730\u5be6\u73fe\u6a21\u578b\u6b78\u7d0d\u7684\u80fd\u529b\uff0c\u6211\u5011\u91cd\u65b0\u5be9\u8996\u4e86\u6b78\u7d0d\u982d\u6a5f\u5236\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e KV \u8f49\u79fb\u6ce8\u610f\u529b\u3002\u6211\u5011\u5f9e\u7406\u8ad6\u4e0a\u8b49\u660e\u4e86 KV \u8f49\u79fb\u6ce8\u610f\u529b\u964d\u4f4e\u4e86\u6a21\u578b\u5c0d\u6b78\u7d0d\u982d\u6a5f\u5236\u7684\u6df1\u5ea6\u548c\u5bec\u5ea6\u7684\u8981\u6c42\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cKV \u8f49\u79fb\u6ce8\u610f\u529b\u6709\u5229\u65bc\u5b78\u7fd2\u6b78\u7d0d\u982d\u548c\u8a9e\u8a00\u5efa\u6a21\uff0c\u9019\u5c0e\u81f4\u4e86\u5f9e\u73a9\u5177\u6a21\u578b\u5230\u64c1\u6709\u8d85\u904e 10B \u53c3\u6578\u7684\u9810\u8a13\u7df4\u6a21\u578b\u7684\u66f4\u597d\u7684\u6027\u80fd\u6216\u66f4\u5feb\u7684\u6536\u6582\u3002", "author": "Mingyu Xu et.al.", "authors": "Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen", "id": "2411.19574v1", "paper_url": "http://arxiv.org/abs/2411.19574v1", "repo": "null"}}