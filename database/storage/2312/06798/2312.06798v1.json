{"2312.06798": {"publish_time": "2023-12-05", "title": "Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety", "paper_summary": "Explainability and Safety engender Trust. These require a model to exhibit\nconsistency and reliability. To achieve these, it is necessary to use and\nanalyze data and knowledge with statistical and symbolic AI methods relevant to\nthe AI application - neither alone will do. Consequently, we argue and seek to\ndemonstrate that the NeuroSymbolic AI approach is better suited for making AI a\ntrusted AI system. We present the CREST framework that shows how Consistency,\nReliability, user-level Explainability, and Safety are built on NeuroSymbolic\nmethods that use data and knowledge to support requirements for critical\napplications such as health and well-being. This article focuses on Large\nLanguage Models (LLMs) as the chosen AI system within the CREST framework. LLMs\nhave garnered substantial attention from researchers due to their versatility\nin handling a broad array of natural language processing (NLP) scenarios. For\nexample, ChatGPT and Google's MedPaLM have emerged as highly promising\nplatforms for providing information in general and health-related queries,\nrespectively. Nevertheless, these models remain black boxes despite\nincorporating human feedback and instruction-guided tuning. For instance,\nChatGPT can generate unsafe responses despite instituting safety guardrails.\nCREST presents a plausible approach harnessing procedural and graph-based\nknowledge within a NeuroSymbolic framework to shed light on the challenges\nassociated with LLMs.", "paper_summary_zh": "", "author": "Manas Gaur et.al.", "authors": "Manas Gaur,Amit Sheth", "id": "2312.06798v1", "paper_url": "http://arxiv.org/abs/2312.06798v1", "repo": "null"}}