{"2411.12240": {"publish_time": "2024-11-19", "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages", "paper_summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u57fa\u65bcTransformer\u67b6\u69cb\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u5404\u7a2e\u9818\u57df\uff0c\u5176\u4e2d\u5206\u8a5e\u5728\u5b83\u5011\u7684\u9810\u8655\u7406\u548c\u5fae\u8abf\u968e\u6bb5\u626e\u6f14\u4e86\u95dc\u9375\u89d2\u8272\u3002\u5728\u591a\u8a9e\u8a00\u6a21\u578b\u4e2d\uff0c\u7279\u5225\u662f\u91dd\u5c0d\u5370\u5ea6\u8a9e\u8a00\u91cf\u8eab\u6253\u9020\u7684\u6a21\u578b\uff0c\u6709\u6548\u7684\u5206\u8a5e\u5c0d\u65bc\u512a\u5316\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u5168\u9762\u8a55\u4f30\u4e86 12 \u500b LLM \u6240\u4f7f\u7528\u7684\u5206\u8a5e\u5668\uff0c\u6db5\u84cb\u5370\u5ea6 22 \u7a2e\u5b98\u65b9\u8a9e\u8a00\uff0c\u91cd\u9ede\u6bd4\u8f03\u5b83\u5011\u7684\u5206\u8a5e\u8655\u7406\u6548\u7387\u3002\u6211\u5011\u5728\u5206\u6790\u4e2d\u63a1\u7528\u6a19\u6e96\u5316\u5e8f\u5217\u9577\u5ea6 (NSL) \u4f5c\u70ba\u95dc\u9375\u6307\u6a19\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cSUTRA \u5206\u8a5e\u5668\u512a\u65bc\u6240\u6709\u5176\u4ed6\u6a21\u578b\uff0c\u5305\u62ec\u591a\u500b\u5370\u5ea6\u8a9e\u8a00\u5c08\u7528\u6a21\u578b\uff0c\u5728 14 \u7a2e\u8a9e\u8a00\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u503c\u5f97\u6ce8\u610f\u7684\u898b\u89e3\u5305\u62ec SUTRA \u5206\u8a5e\u5668\u5728\u8655\u7406\u5370\u5ea6\u8a9e\u8a00\u65b9\u9762\u7684\u512a\u7570\u8868\u73fe\u3001GPT-4o \u5728\u8655\u7406\u5370\u5ea6\u8a9e\u8a00\u65b9\u9762\u512a\u65bc\u5176\u524d\u8eab GPT-4\uff0c\u4ee5\u53ca Project Indus \u5728\u67d0\u4e9b\u8a9e\u8a00\u4e2d\u7684\u6548\u80fd\u6709\u9650\u3002\u9019\u9805\u7814\u7a76\u5f37\u8abf\u4e86\u70ba\u591a\u8a9e\u8a00\u548c\u5370\u5ea6\u4e2d\u5fc3\u6a21\u578b\u958b\u767c\u76ee\u6a19\u5206\u8a5e\u7b56\u7565\u81f3\u95dc\u91cd\u8981\uff0c\u70ba\u672a\u4f86\u6539\u9032\u5206\u8a5e\u5668\u8a2d\u8a08\u5960\u5b9a\u57fa\u790e\uff0c\u4ee5\u589e\u5f37\u8a9e\u8a00\u6db5\u84cb\u7bc4\u570d\u548c\u6a21\u578b\u6548\u7387\u3002", "author": "S. Tamang et.al.", "authors": "S. Tamang, D. J. Bora", "id": "2411.12240v1", "paper_url": "http://arxiv.org/abs/2411.12240v1", "repo": "null"}}