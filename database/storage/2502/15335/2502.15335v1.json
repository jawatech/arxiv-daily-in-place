{"2502.15335": {"publish_time": "2025-02-21", "title": "Stepwise Informativeness Search for Improving LLM Reasoning", "paper_summary": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u5c55\u900f\u904e\u7522\u751f\u81ea\u7531\u6587\u672c\u7684\u7406\u7531\uff0c\u986f\u8457\u5730\u6539\u5584\u4e86\u591a\u6b65\u9a5f\u63a8\u7406\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0cLLM \u50be\u5411\u65bc\u5728\u9577\u8a9e\u5883\u7684\u904e\u7a0b\u4e2d\u5931\u53bb\u7126\u9ede\u3002\u9019\u5f15\u767c\u4e86\u7591\u616e\uff0c\u96a8\u8457\u63a8\u7406\u7684\u9032\u884c\uff0cLLM \u5728\u89e3\u78bc\u5f8c\u7e8c\u6b65\u9a5f\u6642\u53ef\u80fd\u6703\u5ffd\u7565\u5148\u524d\u6b65\u9a5f\u4e2d\u7684\u8cc7\u8a0a\uff0c\u5c0e\u81f4\u7522\u751f\u4e0d\u53ef\u9760\u4e14\u5197\u9918\u7684\u7406\u7531\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u6307\u5c0e LLM \u7522\u751f\u66f4\u6e96\u78ba\u4e14\u7c21\u6f54\u7684\u5206\u6b65\u7406\u7531\uff0c\u65b9\u6cd5\u662f\uff1a(1) \u4e3b\u52d5\u5f15\u7528\u672a\u5145\u5206\u5229\u7528\u7684\u5148\u524d\u6b65\u9a5f\u4e2d\u7684\u8cc7\u8a0a\uff0c\u4ee5\u53ca (2) \u6700\u5c0f\u5316\u65b0\u6b65\u9a5f\u548c\u73fe\u6709\u6b65\u9a5f\u4e4b\u9593\u7684\u5197\u9918\u8cc7\u8a0a\u3002\u6211\u5011\u5f15\u5165\u4e86\u9010\u6b65\u8cc7\u8a0a\u6027\u641c\u5c0b\uff0c\u4e00\u7a2e\u63a8\u7406\u6642\u9593\u6a39\u72c0\u641c\u5c0b\u67b6\u69cb\uff0c\u5b83\u5305\u542b\u5169\u500b\u9078\u64c7\u555f\u767c\u6cd5\uff1a\u4ee5\u57fa\u790e\u70ba\u5c0e\u5411\u7684\u9078\u64c7\uff0c\u5b83\u512a\u5148\u8003\u616e\u5728\u672a\u5145\u5206\u5229\u7528\u7684\u6b65\u9a5f\u4e0a\u7d66\u4e88\u8f03\u9ad8\u6ce8\u610f\u529b\u7684\u6b65\u9a5f\uff1b\u4ee5\u53ca\u4ee5\u65b0\u7a4e\u6027\u70ba\u5c0e\u5411\u7684\u9078\u64c7\uff0c\u5b83\u9f13\u52f5\u5177\u6709\u65b0\u7a4e\u7d50\u8ad6\u7684\u6b65\u9a5f\u3002\u5728\u7406\u7531\u7522\u751f\u671f\u9593\uff0c\u6211\u5011\u4f7f\u7528\u4e00\u7a2e\u81ea\u6211\u57fa\u790e\u7b56\u7565\uff0c\u5b83\u63d0\u793a LLM \u5728\u6bcf\u500b\u6b65\u9a5f\u7684\u63a8\u8ad6\u4e4b\u524d\u660e\u78ba\u5f15\u7528\u76f8\u95dc\u7684\u5148\u524d\u6b65\u9a5f\u4ee5\u63d0\u4f9b\u524d\u63d0\u3002\u5728\u56db\u500b\u63a8\u7406\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u900f\u904e\u7522\u751f\u54c1\u8cea\u66f4\u9ad8\u7684\u7406\u7531\uff08\u932f\u8aa4\u548c\u5197\u9918\u6e1b\u5c11\uff09\u4f86\u6539\u5584\u63a8\u7406\u6e96\u78ba\u5ea6\u3002", "author": "Siyuan Wang et.al.", "authors": "Siyuan Wang, Enda Zhao, Zhongyu Wei, Xiang Ren", "id": "2502.15335v1", "paper_url": "http://arxiv.org/abs/2502.15335v1", "repo": "null"}}