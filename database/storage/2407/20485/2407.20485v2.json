{"2407.20485": {"publish_time": "2024-07-30", "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder", "paper_summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u56e0 KV \u5feb\u53d6\u800c\u9762\u81e8\u8a18\u61b6\u9ad4\u74f6\u9838\u554f\u984c\uff0c\u5c24\u5176\u662f\u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u3002\n\u5148\u524d\u7684\u7814\u7a76\u63d0\u51fa\u4e86 KV \u5feb\u53d6\u58d3\u7e2e\u6280\u8853\uff0c\u8a72\u6280\u8853\u6839\u64da\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u8b58\u5225\u4e0d\u91cd\u8981\u7684\u7b26\u865f\uff0c\u4e26\u5f9e KV \u5feb\u53d6\u4e2d\u79fb\u9664\u5176\u9805\u76ee\uff0c\u4e26\u6307\u51fa\u53ea\u6709\u5c11\u6578\u7b26\u865f\u5728\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u73fe\u6709\u7684\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u4e26\u4e0d\u9069\u5408 Transformer \u89e3\u78bc\u5668\u7d50\u69cb\u3002\u5728\u89e3\u78bc\u5668\u6a21\u578b\u4e2d\uff0c\u7531\u65bc\u906e\u7f69\u6548\u61c9\uff0c\u6ce8\u610f\u529b\u5206\u6578\u7d2f\u7a4d\u7684\u6b21\u6578\u6703\u6839\u64da\u7b26\u865f\u51fa\u73fe\u7684\u9806\u5e8f\u800c\u6709\u6240\u4e0d\u540c\uff0c\u5c0e\u81f4\u7b26\u865f\u4e4b\u9593\u7684\u6bd4\u8f03\u4e0d\u5747\u52fb\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5e36\u6709\u907a\u5fd8\u56e0\u5b50\u7684\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578 (A2SF) \u6280\u8853\uff0c\u8a72\u6280\u8853\u5728\u6ce8\u610f\u529b\u5206\u6578\u7d2f\u7a4d\u904e\u7a0b\u4e2d\u5f15\u5165\u4e86\u907a\u5fd8\u56e0\u5b50\u3002A2SF \u5c0d\u7531\u820a\u7b26\u865f\u7522\u751f\u7684\u904e\u53bb\u6ce8\u610f\u529b\u5206\u6578\u65bd\u52a0\u61f2\u7f70\uff0c\u65b9\u6cd5\u662f\u96a8\u8457\u6642\u9593\u7684\u63a8\u79fb\uff0c\u53cd\u8986\u5c07\u907a\u5fd8\u56e0\u5b50\u4e58\u4ee5\u6ce8\u610f\u529b\u5206\u6578\u3002\u56e0\u6b64\uff0c\u8f03\u820a\u7684\u7b26\u865f\u6703\u6536\u5230\u8f03\u5927\u7684\u61f2\u7f70\uff0c\u5f9e\u800c\u70ba\u4e0d\u540c\u5e74\u9f61\u7684\u7b26\u865f\u63d0\u4f9b\u516c\u5e73\u6027\u3002\u900f\u904e\u7b26\u865f\u4e4b\u9593\u7684\u516c\u5e73\u6bd4\u8f03\uff0c\u6211\u5011\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u9078\u64c7\u91cd\u8981\u7684\u7b26\u865f\u3002\u6211\u5011\u5df2\u7d93\u5728 OPT \u548c LLaMA \u6a21\u578b\u4e2d\u9a57\u8b49\u4e86 A2SF \u7684\u6e96\u78ba\u6027\u6539\u9032\uff0c\u800c A2SF \u5c07 LLaMA 2 \u5728 1-shot \u548c 0-shot \u4e0a\u7684\u6e96\u78ba\u6027\u5206\u5225\u63d0\u9ad8\u4e86 7.8% \u548c 5.1%\u3002</paragraph>", "author": "Hyun-rae Jo et.al.", "authors": "Hyun-rae Jo, Dongkun Shin", "id": "2407.20485v2", "paper_url": "http://arxiv.org/abs/2407.20485v2", "repo": "null"}}