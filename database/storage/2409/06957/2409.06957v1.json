{"2409.06957": {"publish_time": "2024-09-11", "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation", "paper_summary": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u4e2d\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u5e6b\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9075\u5faa\u6307\u793a\u4e26\u63d0\u4f9b\u6709\u76ca\u4e14\u7121\u5bb3\u56de\u61c9\u7684\u4e3b\u8981\u6280\u8853\u4e4b\u4e00\u3002\u96d6\u7136\u5b58\u5728\u76f4\u63a5\u7b56\u7565\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u4f46\u6700\u5148\u9032\u7684 LLM \u5728 RLHF \u4e2d\u63a1\u7528\u57fa\u65bc RL \u7684\u65b9\u6cd5\uff08\u901a\u5e38\u70ba PPO\uff09\uff0c\u4ee5\u8a13\u7df4\u7b56\u7565\u6839\u64da\u5f9e\u504f\u597d\u6578\u64da\u4e2d\u5b78\u7fd2\u5230\u7684\u734e\u52f5\u6a21\u578b\u4f86\u7522\u751f\u826f\u597d\u7684\u56de\u61c9\u3002\u9019\u4e9b\u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6230\u662f\u4e2d\u9593\u734e\u52f5\u6a21\u578b\u7684\u4e0d\u6e96\u78ba\u6027\uff0c\u7279\u5225\u662f\u5728\u9700\u8981\u9577\u6642\u9593\u4e14\u8907\u96dc\u7684\u63a8\u7406\u624d\u80fd\u70ba\u56de\u61c9\u8a55\u5206\u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u4e2d\u3002\u6211\u5011\u767c\u73fe\u734e\u52f5\u6a21\u578b\u7684\u53ef\u9760\u6027\u56e0\u5206\u914d\u4e0d\u540c\u734e\u52f5\u7684\u56de\u61c9\u800c\u7570\u3002\u9019\u4fc3\u4f7f\u6211\u5011\u904e\u6ffe\u6389\u734e\u52f5\u53ef\u80fd\u4e0d\u53ef\u9760\u7684\u6a23\u672c\uff0c\u4ee5\u63d0\u9ad8\u7b56\u7565\u5b78\u7fd2\u671f\u9593\u7684\u4fe1\u566a\u6bd4\uff0c\u5f9e\u800c\u7522\u751f\u7528\u65bc\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316\u7684\u7b56\u7565\u904e\u6ffe (PF-PPO)\u3002\u70ba\u4e86\u91dd\u5c0d\u7d66\u5b9a\u7684\u734e\u52f5\u6a21\u578b\u9078\u64c7\u9069\u7576\u7684\u7b56\u7565\u904e\u6ffe\u7b56\u7565\uff0c\u904e\u6ffe\u6a23\u672c\u4e0a\u734e\u52f5\u8207\u5be6\u969b\u5206\u6578\u4e4b\u9593\u7684\u6c7a\u5b9a\u4fc2\u6578 ($R^2$) \u53ef\u4f5c\u70ba\u826f\u597d\u7684\u6307\u6a19\uff0c\u4e26\u5e6b\u52a9\u6211\u5011\u627e\u5230\u5e7e\u7a2e\u6709\u524d\u9014\u7684\u7b56\u7565\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u5be6\u9a57\u4f86\u9a57\u8b49 PF-PPO \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e26\u767c\u73fe PF-PPO \u7684\u4e00\u4e9b\u8b8a\u9ad4\u975e\u5e38\u6709\u6548\uff0c\u4e26\u4e14\u5728 HumanEval\u3001MBPP \u548c\u4e00\u500b\u65b0\u7684\u3001\u66f4\u5177\u6311\u6230\u6027\u7684 LeetCode \u7af6\u8cfd\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u5728 70 \u5104\u500b\u53c3\u6578\u6a21\u578b\u4e0a\u5be6\u73fe\u4e86\u65b0\u7684\u6700\u5148\u9032\u6548\u80fd\u3002", "author": "Wei Shen et.al.", "authors": "Wei Shen, Chuheng Zhang", "id": "2409.06957v1", "paper_url": "http://arxiv.org/abs/2409.06957v1", "repo": "https://github.com/swtheing/pf-ppo-rlhf"}}