{"2407.15720": {"publish_time": "2024-07-22", "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability", "paper_summary": "Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach composite\ntasks, especially those not encountered during the pretraining phase, remains\nan open question and largely ununderstood. In this study, we delve into the ICL\ncapabilities of LLMs on composite tasks, with only simple tasks as in-context\nexamples. We develop a test suite of composite tasks that include linguistic\nand logical challenges and perform empirical studies across different LLM\nfamilies. We observe that models exhibit divergent behaviors: (1) For simpler\ncomposite tasks that apply distinct mapping mechanisms to different input\nsegments, the models demonstrate decent compositional ability, while scaling up\nthe model enhances this ability; (2) for more complex composite tasks that\ninvolving reasoning multiple steps, where each step represent one task, models\ntypically underperform, and scaling up generally provide no improvements. We\noffer theoretical analysis in a simplified setting, explaining that models\nexhibit compositional capability when the task handles different input parts\nseparately. We believe our work sheds new light on the capabilities of LLMs in\nsolving composite tasks regarding the nature of the tasks and model scale. Our\ndataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u8a31\u591a\u4eba\u5de5\u667a\u6167\u554f\u984c\u7684\u5f37\u5927\u5de5\u5177\uff0c\u4e26\u5c55\u73fe\u51fa\u986f\u8457\u7684\u8a9e\u5883\u5b78\u7fd2 (ICL) \u80fd\u529b\u3002\u7d44\u5408\u80fd\u529b\uff0c\u89e3\u6c7a\u7d50\u5408\u5169\u500b\u6216\u66f4\u591a\u7c21\u55ae\u4efb\u52d9\u7684\u672a\u898b\u904e\u8907\u96dc\u4efb\u52d9\uff0c\u662f\u4eba\u5de5\u901a\u7528\u667a\u6167\u7684\u5fc5\u8981\u63a8\u7406\u80fd\u529b\u3002\u5118\u7ba1 LLM \u7372\u5f97\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5b83\u5011\u5982\u4f55\u8655\u7406\u8907\u5408\u4efb\u52d9\uff0c\u7279\u5225\u662f\u5728\u9810\u8a13\u7df4\u968e\u6bb5\u672a\u9047\u5230\u7684\u4efb\u52d9\uff0c\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u4e14\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5c1a\u672a\u7406\u89e3\u7684\u554f\u984c\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e LLM \u5728\u8907\u5408\u4efb\u52d9\u4e0a\u7684 ICL \u80fd\u529b\uff0c\u50c5\u4f7f\u7528\u7c21\u55ae\u4efb\u52d9\u4f5c\u70ba\u8a9e\u5883\u7bc4\u4f8b\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u5957\u5305\u542b\u8a9e\u8a00\u548c\u908f\u8f2f\u6311\u6230\u7684\u8907\u5408\u4efb\u52d9\u6e2c\u8a66\u5957\u4ef6\uff0c\u4e26\u5c0d\u4e0d\u540c\u7684 LLM \u5bb6\u65cf\u9032\u884c\u5be6\u8b49\u7814\u7a76\u3002\u6211\u5011\u89c0\u5bdf\u5230\u6a21\u578b\u8868\u73fe\u51fa\u4e0d\u540c\u7684\u884c\u70ba\uff1a(1) \u5c0d\u65bc\u5c07\u4e0d\u540c\u7684\u6620\u5c04\u6a5f\u5236\u61c9\u7528\u65bc\u4e0d\u540c\u8f38\u5165\u5340\u6bb5\u7684\u8f03\u7c21\u55ae\u7684\u8907\u5408\u4efb\u52d9\uff0c\u6a21\u578b\u8868\u73fe\u51fa\u826f\u597d\u7684\u7d44\u5408\u80fd\u529b\uff0c\u800c\u64f4\u5927\u6a21\u578b\u6703\u589e\u5f37\u9019\u7a2e\u80fd\u529b\uff1b(2) \u5c0d\u65bc\u6d89\u53ca\u591a\u500b\u6b65\u9a5f\u63a8\u7406\u7684\u66f4\u8907\u96dc\u7684\u8907\u5408\u4efb\u52d9\uff0c\u5176\u4e2d\u6bcf\u500b\u6b65\u9a5f\u4ee3\u8868\u4e00\u500b\u4efb\u52d9\uff0c\u6a21\u578b\u901a\u5e38\u8868\u73fe\u4e0d\u4f73\uff0c\u800c\u64f4\u5927\u898f\u6a21\u901a\u5e38\u4e0d\u6703\u5e36\u4f86\u4efb\u4f55\u6539\u9032\u3002\u6211\u5011\u5728\u7c21\u5316\u7684\u74b0\u5883\u4e2d\u63d0\u4f9b\u7406\u8ad6\u5206\u6790\uff0c\u8aaa\u660e\u7576\u4efb\u52d9\u5c07\u4e0d\u540c\u7684\u8f38\u5165\u90e8\u5206\u5206\u958b\u8655\u7406\u6642\uff0c\u6a21\u578b\u6703\u8868\u73fe\u51fa\u7d44\u5408\u80fd\u529b\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u7814\u7a76\u70ba LLM \u5728\u89e3\u6c7a\u8907\u5408\u4efb\u52d9\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u6d89\u53ca\u4efb\u52d9\u7684\u6027\u8cea\u548c\u6a21\u578b\u898f\u6a21\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728\n{\\url{https://github.com/OliverXUZY/LLM_Compose}} \u53d6\u5f97\u3002", "author": "Zhuoyan Xu et.al.", "authors": "Zhuoyan Xu, Zhenmei Shi, Yingyu Liang", "id": "2407.15720v1", "paper_url": "http://arxiv.org/abs/2407.15720v1", "repo": "https://github.com/oliverxuzy/llm_compose"}}