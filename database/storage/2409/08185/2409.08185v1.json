{"2409.08185": {"publish_time": "2024-09-12", "title": "Fine-tuning Large Language Models for Entity Matching", "paper_summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.", "paper_summary_zh": "\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u5176\u9ad8\u96f6\u6b21\u5b78\u7fd2\u8868\u73fe\u4ee5\u53ca\u5c0d\u672a\u898b\u5be6\u9ad4\u9032\u884c\u6cdb\u5316\u7684\u80fd\u529b\uff0c\u662f\u5be6\u9ad4\u914d\u5c0d\u4e2d\u9810\u5148\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u7684\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u73fe\u6709\u95dc\u65bc\u4f7f\u7528 LLM \u9032\u884c\u5be6\u9ad4\u914d\u5c0d\u7684\u7814\u7a76\u5df2\u5c08\u6ce8\u65bc\u63d0\u793a\u5de5\u7a0b\u548c\u60c5\u5883\u5b78\u7fd2\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u5fae\u8abf LLM \u4ee5\u9032\u884c\u5be6\u9ad4\u914d\u5c0d\u7684\u6f5b\u529b\u3002\u6211\u5011\u6cbf\u8457\u5169\u500b\u9762\u5411\u5206\u6790\u5fae\u8abf\uff1a1) \u8a13\u7df4\u7bc4\u4f8b\u7684\u8868\u5fb5\uff0c\u6211\u5011\u5728\u5176\u4e2d\u5617\u8a66\u5c07\u4e0d\u540c\u985e\u578b\u7684 LLM \u751f\u6210\u7684\u8aaa\u660e\u65b0\u589e\u5230\u8a13\u7df4\u7d44\uff0c\u4ee5\u53ca 2) \u4f7f\u7528 LLM \u9078\u64c7\u548c\u7522\u751f\u8a13\u7df4\u7bc4\u4f8b\u3002\u9664\u4e86\u5728\u4f86\u6e90\u8cc7\u6599\u96c6\u4e0a\u7684\u914d\u5c0d\u6548\u80fd\u4e4b\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u5fae\u8abf\u5982\u4f55\u5f71\u97ff\u6a21\u578b\u5c0d\u5176\u4ed6\u9818\u57df\u5167\u8cc7\u6599\u96c6\u4ee5\u53ca\u8de8\u4e3b\u984c\u9818\u57df\u9032\u884c\u6cdb\u5316\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u5fae\u8abf\u986f\u8457\u63d0\u5347\u8f03\u5c0f\u578b\u6a21\u578b\u7684\u6548\u80fd\uff0c\u800c\u8f03\u5927\u578b\u6a21\u578b\u7684\u7d50\u679c\u5247\u597d\u58de\u53c3\u534a\u3002\u5fae\u8abf\u4e5f\u63d0\u5347\u4e86\u5c0d\u9818\u57df\u5167\u8cc7\u6599\u96c6\u7684\u6cdb\u5316\uff0c\u540c\u6642\u640d\u5bb3\u4e86\u8de8\u9818\u57df\u8f49\u79fb\u3002\u6211\u5011\u986f\u793a\uff0c\u5c07\u7d50\u69cb\u5316\u8aaa\u660e\u65b0\u589e\u5230\u8a13\u7df4\u7d44\u5c0d\u56db\u500b LLM \u4e2d\u7684\u4e09\u500b\u7684\u6548\u80fd\u6709\u6b63\u9762\u7684\u5f71\u97ff\uff0c\u800c\u6240\u63d0\u51fa\u7684\u7bc4\u4f8b\u9078\u64c7\u548c\u7522\u751f\u65b9\u6cd5\u50c5\u63d0\u5347\u4e86 Llama 3.1 8B \u7684\u6548\u80fd\uff0c\u540c\u6642\u964d\u4f4e\u4e86 GPT-4o Mini \u7684\u6548\u80fd\u3002", "author": "Aaron Steiner et.al.", "authors": "Aaron Steiner, Ralph Peeters, Christian Bizer", "id": "2409.08185v1", "paper_url": "http://arxiv.org/abs/2409.08185v1", "repo": "https://github.com/wbsg-uni-mannheim/tailormatch"}}