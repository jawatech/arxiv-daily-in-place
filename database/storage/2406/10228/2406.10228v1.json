{"2406.10228": {"publish_time": "2024-06-14", "title": "VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models", "paper_summary": "The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u6a21\u578b (MMLM) \u7684\u5feb\u901f\u9032\u5c55\u5c55\u793a\u4e86\u5b83\u5011\u5728\u8655\u7406\u7d50\u5408\u8996\u89ba\u548c\u8a9e\u8a00\u4efb\u52d9\u4e0a\u7684\u9a5a\u4eba\u80fd\u529b\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u76ee\u524d\u7684\u6a21\u578b\u548c\u57fa\u6e96\u90fd\u8fce\u5408\u4e86\u8996\u89ba\u548c\u6587\u672c\u8a9e\u5883\u7bc4\u570d\u72f9\u7a84\u7684\u5834\u666f\u3002\u9019\u4e9b\u6a21\u578b\u5728\u9762\u5c0d\u8907\u96dc\u7684\u7406\u89e3\u4efb\u52d9\u6642\u5e38\u5e38\u8868\u73fe\u4e0d\u4f73\uff0c\u9019\u6d89\u53ca\u5728\u6587\u672c\u548c\u5716\u50cf\u5f62\u5f0f\u4e2d\u700f\u89bd\u5927\u91cf\u7121\u95dc\u4e14\u53ef\u80fd\u5177\u6709\u8aa4\u5c0e\u6027\u7684\u8cc7\u8a0a\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u9805\u65b0\u7684\u3001\u66f4\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u7a31\u70ba\u4ea4\u932f\u5716\u50cf\u6587\u672c\u7406\u89e3 (IITC)\u3002\u6b64\u4efb\u52d9\u6311\u6230\u6a21\u578b\u8fa8\u5225\u548c\u5ffd\u7565\u5716\u50cf\u548c\u6587\u672c\u4e2d\u7684\u591a\u9918\u5143\u7d20\uff0c\u4ee5\u6e96\u78ba\u56de\u7b54\u554f\u984c\u4e26\u9075\u5faa\u8907\u96dc\u7684\u6307\u793a\u4f86\u7cbe\u78ba\u6307\u51fa\u76f8\u95dc\u5716\u50cf\u3002\u70ba\u4e86\u652f\u6301\u6b64\u4efb\u52d9\uff0c\u6211\u5011\u9032\u4e00\u6b65\u88fd\u4f5c\u4e86\u4e00\u500b\u65b0\u7684 VEGA \u8cc7\u6599\u96c6\uff0c\u5c08\u9580\u91dd\u5c0d\u79d1\u5b78\u5167\u5bb9\u7684 IITC \u4efb\u52d9\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u500b\u5b50\u4efb\u52d9\uff0c\u5716\u50cf\u6587\u672c\u95dc\u806f (ITA)\uff0c\u4ee5\u63d0\u5347\u5716\u50cf\u6587\u672c\u95dc\u806f\u6280\u80fd\u3002\u6211\u5011\u5c0d\u56db\u500b\u9818\u5148\u7684\u9589\u6e90\u6a21\u578b\u4ee5\u53ca\u4f7f\u7528 VEGA \u7684\u5404\u7a2e\u958b\u6e90\u6a21\u578b\u7684\u8a55\u4f30\uff0c\u7a81\u986f\u4e86 IITC \u7684\u56b4\u8b39\u6027\u3002\u5373\u4f7f\u662f\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u4f8b\u5982 Gemini-1.5-pro \u548c GPT4V\uff0c\u4e5f\u53ea\u53d6\u5f97\u4e86\u9069\u5ea6\u7684\u6210\u529f\u3002\u901a\u904e\u63a1\u7528\u591a\u4efb\u52d9\u3001\u591a\u5c3a\u5ea6\u7684\u5f8c\u8a13\u7df4\u7b56\u7565\uff0c\u6211\u5011\u70ba MLLM \u5728 IITC \u4efb\u52d9\u4e0a\u8a2d\u5b9a\u4e86\u4e00\u500b\u7a69\u5065\u7684\u57fa\u7dda\uff0c\u5728\u5716\u50cf\u95dc\u806f\u4e2d\u9054\u5230 85.8% \u7684\u6e96\u78ba\u7387\u548c 0.508 \u7684 Rouge \u5206\u6578\u3002\u9019\u4e9b\u7d50\u679c\u9a57\u8b49\u4e86\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5728\u63d0\u5347 MLLM \u5c0d\u7d30\u5fae\u5dee\u5225\u7684\u5716\u50cf\u6587\u672c\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Chenyu Zhou et.al.", "authors": "Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, Rongrong Ji", "id": "2406.10228v1", "paper_url": "http://arxiv.org/abs/2406.10228v1", "repo": "null"}}