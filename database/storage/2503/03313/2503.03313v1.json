{"2503.03313": {"publish_time": "2025-03-05", "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models", "paper_summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.", "paper_summary_zh": "<paragraph>\u6587\u5b57\u5c6c\u6027\u5716 (TAG)\uff0c\u5176\u4e2d\u6bcf\u500b\u7bc0\u9ede\u90fd\u8207\u6587\u5b57\u63cf\u8ff0\u76f8\u95dc\u806f\uff0c\u5728\u73fe\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\u7121\u8655\u4e0d\u5728\u3002\u5b83\u5011\u901a\u5e38\u5c55\u73fe\u51fa\u7368\u7279\u7684\u7d50\u69cb\u548c\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\uff0c\u4fc3\u4f7f\u4eba\u5011\u958b\u767c\u4e00\u7a2e\u5716\u57fa\u790e\u6a21\u578b (GFM)\uff0c\u4f7f\u5176\u80fd\u5920\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u5716\u548c\u4efb\u52d9\u3002\u5118\u7ba1\u4eba\u5011\u4ed8\u51fa\u4e86\u5de8\u5927\u7684\u52aa\u529b\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5716\u795e\u7d93\u7db2\u7d61 (GNN) \u6574\u5408\u5230 TAG \u4e2d\uff0c\u4f46\u73fe\u6709\u65b9\u6cd5\u4ecd\u53d7\u9650\u65bc\u5169\u968e\u6bb5\u5c0d\u9f4a\u7684\u89e3\u8026\u67b6\u69cb\uff0c\u9650\u5236\u4e86\u5b83\u5011\u7684\u5354\u540c\u6f5b\u529b\u3002\u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u73fe\u6709\u65b9\u6cd5\u5c07\u8a5e\u5f59\u8868\u5916 (OOV) \u7684\u8a5e\u7b26\u5206\u914d\u7d66\u5716\u7bc0\u9ede\uff0c\u5c0e\u81f4\u7279\u5b9a\u65bc\u5716\u7684\u8a9e\u7fa9\u3001\u8a5e\u7b26\u7206\u70b8\u4ee5\u53ca\u8207\u9762\u5411\u4efb\u52d9\u7684\u63d0\u793a\u6a21\u677f\u4e0d\u517c\u5bb9\uff0c\u9019\u963b\u7919\u4e86\u8de8\u5716\u548c\u8de8\u4efb\u52d9\u7684\u53ef\u9077\u79fb\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 PromptGFM\uff0c\u9019\u662f\u4e00\u7a2e\u57fa\u65bc\u5716\u8a5e\u5f59\u5b78\u7fd2\u7684\u591a\u529f\u80fd TAG GFM\u3002PromptGFM \u5305\u542b\u5169\u500b\u95dc\u9375\u7d44\u4ef6\uff1a(1) \u5716\u7406\u89e3\u6a21\u7d44\uff0c\u5b83\u660e\u78ba\u63d0\u793a LLM \u5728\u6587\u672c\u7a7a\u9593\u5167\u8907\u88fd\u6700\u4f73\u7684 GNN \u5de5\u4f5c\u6d41\u7a0b\uff0c\u4fc3\u9032 GNN-LLM \u7684\u7121\u7e2b\u6574\u5408\u548c\u512a\u96c5\u7684\u5716\u6587\u5c0d\u9f4a\uff1b(2) \u5716\u63a8\u7406\u6a21\u7d44\uff0c\u5b83\u5efa\u7acb\u4e86\u4e00\u500b\u57fa\u65bc\u8a9e\u8a00\u7684\u5716\u8a5e\u5f59\uff0c\u78ba\u4fdd\u8868\u9054\u6027\u3001\u53ef\u9077\u79fb\u6027\u548c\u53ef\u64f4\u5c55\u6027\uff0c\u5f9e\u800c\u5be6\u73fe LLM \u5fae\u8abf\u7684\u53ef\u8b80\u6307\u4ee4\u3002\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u5728\u4e0d\u540c\u5716\u548c\u4efb\u52d9\u4e2d\u7684\u512a\u8d8a\u6027\u548c\u53ef\u9077\u79fb\u6027\u3002\u7a0b\u5f0f\u78bc\u5728\u6b64\u8655\u53ef\u7528\uff1ahttps://github.com/agiresearch/PromptGFM\u3002</paragraph>\n", "author": "Xi Zhu et.al.", "authors": "Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang", "id": "2503.03313v1", "paper_url": "http://arxiv.org/abs/2503.03313v1", "repo": "null"}}