
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-22**|**Robust Representation Consistency Model via Contrastive Denoising**|Jiachen Lei et.al.|[2501.13094v1](http://arxiv.org/abs/2501.13094v1)|[link](https://github.com/jiachenlei/rrcm)|
|**2025-01-22**|**Boosting MCTS with Free Energy Minimization**|Mawaba Pascal Dao et.al.|[2501.13083v1](http://arxiv.org/abs/2501.13083v1)|null|
|**2025-01-22**|**Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment**|Melissa Kazemi Rad et.al.|[2501.13080v1](http://arxiv.org/abs/2501.13080v1)|null|
|**2025-01-22**|**Autonomy-of-Experts Models**|Ang Lv et.al.|[2501.13074v1](http://arxiv.org/abs/2501.13074v1)|null|
|**2025-01-22**|**AdaWM: Adaptive World Model based Planning for Autonomous Driving**|Hang Wang et.al.|[2501.13072v1](http://arxiv.org/abs/2501.13072v1)|null|
|**2025-01-22**|**Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning**|Bohao Yang et.al.|[2501.13042v1](http://arxiv.org/abs/2501.13042v1)|[link](https://github.com/bernard-yang/mmsci_table)|
|**2025-01-22**|**Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review**|Andrii Zahorodnii et.al.|[2501.13014v1](http://arxiv.org/abs/2501.13014v1)|null|
|**2025-01-22**|**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking**|Sebastian Farquhar et.al.|[2501.13011v1](http://arxiv.org/abs/2501.13011v1)|null|
|**2025-01-22**|**Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament**|Yantao Liu et.al.|[2501.13007v1](http://arxiv.org/abs/2501.13007v1)|[link](https://github.com/thu-keg/pairwiserm)|
|**2025-01-22**|**Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities**|Florian Kankowski et.al.|[2501.12980v1](http://arxiv.org/abs/2501.12980v1)|null|
|**2025-01-22**|**FlanEC: Exploring Flan-T5 for Post-ASR Error Correction**|Moreno La Quatra et.al.|[2501.12979v1](http://arxiv.org/abs/2501.12979v1)|[link](https://github.com/morenolaquatra/flanec)|
|**2025-01-22**|**OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models**|Chongren Sun et.al.|[2501.12975v1](http://arxiv.org/abs/2501.12975v1)|[link](https://github.com/sunchongren/onioneval)|
|**2025-01-22**|**Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs**|Jan Corazza et.al.|[2501.12972v1](http://arxiv.org/abs/2501.12972v1)|null|
|**2025-01-22**|**It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act**|Kristof Meding et.al.|[2501.12962v1](http://arxiv.org/abs/2501.12962v1)|null|
|**2025-01-22**|**Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference**|Weizhi Fei et.al.|[2501.12959v1](http://arxiv.org/abs/2501.12959v1)|null|
|**2025-01-22**|**GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models**|Pengxiang Zhao et.al.|[2501.12956v1](http://arxiv.org/abs/2501.12956v1)|null|
|**2025-01-22**|**Multifractal hopscotch in "Hopscotch" by Julio Cortazar**|Jakub Dec et.al.|[2501.12955v1](http://arxiv.org/abs/2501.12955v1)|null|
|**2025-01-22**|**Punctuation patterns in "Finnegans Wake" by James Joyce are largely translation-invariant**|Krzysztof Bartnicki et.al.|[2501.12954v1](http://arxiv.org/abs/2501.12954v1)|null|
|**2025-01-22**|**DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**|DeepSeek-AI et.al.|[2501.12948v1](http://arxiv.org/abs/2501.12948v1)|null|
|**2025-01-22**|**PreciseCam: Precise Camera Control for Text-to-Image Generation**|Edurne Bernal-Berdun et.al.|[2501.12910v1](http://arxiv.org/abs/2501.12910v1)|null|
|**2025-01-22**|**FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces**|Zhenran Xu et.al.|[2501.12909v1](http://arxiv.org/abs/2501.12909v1)|null|
|**2025-01-22**|**Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration**|Offa Kingsleigh et.al.|[2501.12901v1](http://arxiv.org/abs/2501.12901v1)|null|
|**2025-01-22**|**Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback**|Yafu Li et.al.|[2501.12895v1](http://arxiv.org/abs/2501.12895v1)|[link](https://github.com/yafuly/tpo)|
|**2025-01-22**|**Learning Graph Node Embeddings by Smooth Pair Sampling**|Konstantin Kutzkov et.al.|[2501.12884v1](http://arxiv.org/abs/2501.12884v1)|null|
|**2025-01-22**|**WisdomBot: Tuning Large Language Models with Artificial Intelligence Knowledge**|Jingyuan Chen et.al.|[2501.12877v1](http://arxiv.org/abs/2501.12877v1)|null|
|**2025-01-22**|**Mutation-Guided LLM-based Test Generation at Meta**|Christopher Foster et.al.|[2501.12862v1](http://arxiv.org/abs/2501.12862v1)|null|
|**2025-01-22**|**ACEBench: Who Wins the Match Point in Tool Learning?**|Chen Chen et.al.|[2501.12851v1](http://arxiv.org/abs/2501.12851v1)|null|
|**2025-01-22**|**GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model for Multi-organ Segmentation**|Ruicheng Zhang et.al.|[2501.12844v1](http://arxiv.org/abs/2501.12844v1)|[link](https://github.com/sysuzrc/gamed-snake)|
|**2025-01-22**|**Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home**|Viktor Moskvoretskii et.al.|[2501.12835v1](http://arxiv.org/abs/2501.12835v1)|null|
|**2025-01-22**|**Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek**|John Pavlopoulos et.al.|[2501.12826v1](http://arxiv.org/abs/2501.12826v1)|null|
|**2025-01-22**|**Machine Learning Modeling for Multi-order Human Visual Motion Processing**|Zitang Sun et.al.|[2501.12810v1](http://arxiv.org/abs/2501.12810v1)|[link](https://github.com/anoymized/multi-order-motion-model)|
|**2025-01-22**|**Revisit Self-Debugging with Self-Generated Tests for Code Generation**|Xiancai Chen et.al.|[2501.12793v1](http://arxiv.org/abs/2501.12793v1)|null|
|**2025-01-22**|**Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana**|Simone Filice et.al.|[2501.12789v1](http://arxiv.org/abs/2501.12789v1)|null|
|**2025-01-22**|**Data re-uploading in Quantum Machine Learning for time series: application to traffic forecasting**|Nikolaos Schetakis et.al.|[2501.12776v1](http://arxiv.org/abs/2501.12776v1)|null|
|**2025-01-22**|**Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation**|Duc Hau Nguyen et.al.|[2501.12775v1](http://arxiv.org/abs/2501.12775v1)|[link](https://github.com/kihansi95/linkmedia_attentionplausibilitybyconstraint)|
|**2025-01-22**|**LLMs as Repositories of Factual Knowledge: Limitations and Solutions**|Seyed Mahed Mousavi et.al.|[2501.12774v1](http://arxiv.org/abs/2501.12774v1)|null|
|**2025-01-22**|**NExtLong: Toward Effective Long-Context Training without Long Documents**|Chaochen Gao et.al.|[2501.12766v1](http://arxiv.org/abs/2501.12766v1)|null|
|**2025-01-22**|**Estimating the Conformal Prediction Threshold from Noisy Labels**|Coby Penso et.al.|[2501.12749v1](http://arxiv.org/abs/2501.12749v1)|[link](https://github.com/cobypenso/noise-aware-conformal-prediction)|
|**2025-01-22**|**EvidenceMap: Unleashing the Power of Small Language Models with Evidence Analysis for Biomedical Question Answering**|Chang Zong et.al.|[2501.12746v1](http://arxiv.org/abs/2501.12746v1)|null|
|**2025-01-22**|**A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering**|Matteo Esposito et.al.|[2501.12728v1](http://arxiv.org/abs/2501.12728v1)|null|
|**2025-01-22**|**Practical quantum federated learning and its experimental demonstration**|Zhi-Ping Liu et.al.|[2501.12709v1](http://arxiv.org/abs/2501.12709v1)|null|
|**2025-01-22**|**Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression**|Kai Yoshida et.al.|[2501.12698v1](http://arxiv.org/abs/2501.12698v1)|null|
|**2025-01-22**|**Growth strategies for arbitrary DAG neural architectures**|Stella Douka et.al.|[2501.12690v1](http://arxiv.org/abs/2501.12690v1)|null|
|**2025-01-22**|**Extracting General-use Transformers for Low-resource Languages via Knowledge Distillation**|Jan Christian Blaise Cruz et.al.|[2501.12660v1](http://arxiv.org/abs/2501.12660v1)|null|
|**2025-01-22**|**The potential -- and the pitfalls -- of using pre-trained language models as cognitive science theories**|Raj Sanjay Shah et.al.|[2501.12651v1](http://arxiv.org/abs/2501.12651v1)|null|
|**2025-01-22**|**Dynamics of Toxicity in Political Podcasts**|Naquee Rizwan et.al.|[2501.12640v1](http://arxiv.org/abs/2501.12640v1)|null|
|**2025-01-22**|**Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors**|Jingyang Ke et.al.|[2501.12633v1](http://arxiv.org/abs/2501.12633v1)|null|
|**2025-01-22**|**Towards Robust Multi-tab Website Fingerprinting**|Xinhao Deng et.al.|[2501.12622v1](http://arxiv.org/abs/2501.12622v1)|null|
|**2025-01-22**|**Distillation Quantification for Large Language Models**|Sunbowen Lee et.al.|[2501.12619v1](http://arxiv.org/abs/2501.12619v1)|[link](https://github.com/aegis1863/llms-distillation-quantification)|
|**2025-01-22**|**Deep Learning-Based Identification of Inconsistent Method Names: How Far Are We?**|Taiming Wang et.al.|[2501.12617v1](http://arxiv.org/abs/2501.12617v1)|null|
|**2025-01-22**|**GATE: Adaptive Learning with Working Memory by Information Gating in Multi-lamellar Hippocampal Formation**|Yuechen Liu et.al.|[2501.12615v1](http://arxiv.org/abs/2501.12615v1)|null|
|**2025-01-22**|**T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation**|Lijun Li et.al.|[2501.12612v1](http://arxiv.org/abs/2501.12612v1)|null|
|**2025-01-22**|**BLR-MoE: Boosted Language-Routing Mixture of Experts for Domain-Robust Multilingual E2E ASR**|Guodong Ma et.al.|[2501.12602v1](http://arxiv.org/abs/2501.12602v1)|null|
|**2025-01-22**|**Kimi k1.5: Scaling Reinforcement Learning with LLMs**|Kimi Team et.al.|[2501.12599v1](http://arxiv.org/abs/2501.12599v1)|null|
|**2025-01-22**|**FedGrAINS: Personalized SubGraph Federated Learning with Adaptive Neighbor Sampling**|Emir Ceyani et.al.|[2501.12592v1](http://arxiv.org/abs/2501.12592v1)|null|
|**2025-01-22**|**Leveraging LLMs to Create a Haptic Devices' Recommendation System**|Yang Liu et.al.|[2501.12573v1](http://arxiv.org/abs/2501.12573v1)|null|
|**2025-01-22**|**O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning**|Haotian Luo et.al.|[2501.12570v1](http://arxiv.org/abs/2501.12570v1)|[link](https://github.com/stardewxxx/o1-pruner)|
|**2025-01-22**|**Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review**|Rock Yuren Pang et.al.|[2501.12557v1](http://arxiv.org/abs/2501.12557v1)|null|
|**2025-01-21**|**Human-like conceptual representations emerge from language prediction**|Ningyu Xu et.al.|[2501.12547v1](http://arxiv.org/abs/2501.12547v1)|null|
|**2025-01-21**|**Comparative Approaches to Sentiment Analysis Using Datasets in Major European and Arabic Languages**|Mikhail Krasitskii et.al.|[2501.12540v1](http://arxiv.org/abs/2501.12540v1)|null|
|**2025-01-21**|**Compositional Instruction Following with Language Models and Reinforcement Learning**|Vanya Cohen et.al.|[2501.12539v1](http://arxiv.org/abs/2501.12539v1)|null|
|**2025-01-21**|**Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related with Post COVID-19 Condition**|Juan Andres Medina Florez et.al.|[2501.12538v1](http://arxiv.org/abs/2501.12538v1)|null|
|**2025-01-21**|**Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy**|Khaoula Chehbouni et.al.|[2501.12537v1](http://arxiv.org/abs/2501.12537v1)|null|
|**2025-01-21**|**Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs**|Zheng Li et.al.|[2501.12536v1](http://arxiv.org/abs/2501.12536v1)|null|
|**2025-01-21**|**Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**|Jiaqi Guo et.al.|[2501.12524v1](http://arxiv.org/abs/2501.12524v1)|null|
|**2025-01-21**|**An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts**|Dhia Elhaq Rzig et.al.|[2501.12521v1](http://arxiv.org/abs/2501.12521v1)|null|
|**2025-01-21**|**Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting**|Josh Bruegger et.al.|[2501.12489v1](http://arxiv.org/abs/2501.12489v1)|[link](https://github.com/marcozullich/punches-object-detection)|
|**2025-01-21**|**The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws**|Tian Jin et.al.|[2501.12486v1](http://arxiv.org/abs/2501.12486v1)|null|
|**2025-01-21**|**fabSAM: A Farmland Boundary Delineation Method Based on the Segment Anything Model**|Yufeng Xie et.al.|[2501.12487v1](http://arxiv.org/abs/2501.12487v1)|null|
|**2025-01-21**|**R2D2: Remembering, Reflecting and Dynamic Decision Making for Web Agents**|Tenghao Huang et.al.|[2501.12485v1](http://arxiv.org/abs/2501.12485v1)|null|
|**2025-01-21**|**Adaptive PII Mitigation Framework for Large Language Models**|Shubhi Asthana et.al.|[2501.12465v1](http://arxiv.org/abs/2501.12465v1)|null|
|**2025-01-21**|**Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications**|Shubhi Asthana et.al.|[2501.12456v1](http://arxiv.org/abs/2501.12456v1)|null|
|**2025-01-21**|**Learning segmentation from point trajectories**|Laurynas Karazija et.al.|[2501.12392v1](http://arxiv.org/abs/2501.12392v1)|null|
|**2025-01-21**|**Physics of Skill Learning**|Ziming Liu et.al.|[2501.12391v1](http://arxiv.org/abs/2501.12391v1)|[link](https://github.com/kindxiaoming/physics_of_skill_learning)|
|**2025-01-21**|**MMVU: Measuring Expert-Level Multi-Discipline Video Understanding**|Yilun Zhao et.al.|[2501.12380v1](http://arxiv.org/abs/2501.12380v1)|[link](https://github.com/yale-nlp/mmvu)|
|**2025-01-21**|**Enhancing Retrosynthesis with Conformer: A Template-Free Method**|Jiaxi Zhuang et.al.|[2501.12434v1](http://arxiv.org/abs/2501.12434v1)|null|
|**2025-01-21**|**Video Depth Anything: Consistent Depth Estimation for Super-Long Videos**|Sili Chen et.al.|[2501.12375v2](http://arxiv.org/abs/2501.12375v2)|null|
|**2025-01-21**|**Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists**|Thomas F. Eisenmann et.al.|[2501.12374v1](http://arxiv.org/abs/2501.12374v1)|[link](https://github.com/andreskarjus/genaiexperiment)|
|**2025-01-21**|**Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL**|Yeounoh Chung et.al.|[2501.12372v1](http://arxiv.org/abs/2501.12372v1)|null|
|**2025-01-21**|**Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models**|Samira Abnar et.al.|[2501.12370v1](http://arxiv.org/abs/2501.12370v1)|null|
|**2025-01-21**|**InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model**|Yuhang Zang et.al.|[2501.12368v1](http://arxiv.org/abs/2501.12368v1)|[link](https://github.com/internlm/internlm-xcomposer)|
|**2025-01-21**|**Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models**|Tabinda Aman et.al.|[2501.12433v1](http://arxiv.org/abs/2501.12433v1)|null|
|**2025-01-21**|**Test-time regression: a unifying framework for designing sequence models with associative memory**|Ke Alexander Wang et.al.|[2501.12352v1](http://arxiv.org/abs/2501.12352v1)|null|
|**2025-01-21**|**Treefix: Enabling Execution with a Tree of Prefixes**|Beatriz Souza et.al.|[2501.12339v1](http://arxiv.org/abs/2501.12339v1)|null|
|**2025-01-21**|**FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**|Phuoc Duong Huy Chu et.al.|[2501.12336v1](http://arxiv.org/abs/2501.12336v1)|null|
|**2025-01-21**|**Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration**|Thomas Walshe et.al.|[2501.12332v1](http://arxiv.org/abs/2501.12332v1)|null|
|**2025-01-21**|**UI-TARS: Pioneering Automated GUI Interaction with Native Agents**|Yujia Qin et.al.|[2501.12326v1](http://arxiv.org/abs/2501.12326v1)|[link](https://github.com/bytedance/ui-tars)|
|**2025-01-21**|**LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations**|Hasan Abu-Rasheed et.al.|[2501.12300v1](http://arxiv.org/abs/2501.12300v1)|null|
|**2025-01-21**|**RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning**|Jiacheng Zuo et.al.|[2501.12296v1](http://arxiv.org/abs/2501.12296v1)|[link](https://github.com/jiachengzuo/ralad)|
|**2025-01-21**|**Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation**|Dongsheng Zhu et.al.|[2501.12432v1](http://arxiv.org/abs/2501.12432v1)|null|
|**2025-01-21**|**Modality Interactive Mixture-of-Experts for Fake News Detection**|Yifan Liu et.al.|[2501.12431v1](http://arxiv.org/abs/2501.12431v1)|null|
|**2025-01-21**|**With Great Backbones Comes Great Adversarial Transferability**|Erik Arakelyan et.al.|[2501.12275v1](http://arxiv.org/abs/2501.12275v1)|null|
|**2025-01-21**|**Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement**|Maosong Cao et.al.|[2501.12273v1](http://arxiv.org/abs/2501.12273v1)|null|
|**2025-01-21**|**CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**|Cristiano Patrício et.al.|[2501.12266v1](http://arxiv.org/abs/2501.12266v1)|null|
|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243v1](http://arxiv.org/abs/2501.12243v1)|null|
|**2025-01-21**|**InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models**|Pha Nguyen et.al.|[2501.12231v1](http://arxiv.org/abs/2501.12231v1)|null|
|**2025-01-21**|**SCFCRC: Simultaneously Counteract Feature Camouflage and Relation Camouflage for Fraud Detection**|Xiaocheng Zhang et.al.|[2501.12430v1](http://arxiv.org/abs/2501.12430v1)|null|
|**2025-01-21**|**Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model**|Kazi Hasan Ibn Arif et.al.|[2501.12206v1](http://arxiv.org/abs/2501.12206v1)|null|
|**2025-01-21**|**An End-to-End Approach for Korean Wakeword Systems with Speaker Authentication**|Geonwoo Seo et.al.|[2501.12194v1](http://arxiv.org/abs/2501.12194v1)|[link](https://github.com/gws8820/securewakeword-model)|
|**2025-01-21**|**Fuel Efficiency Analysis of the Public Transportation System Based on the Gaussian Mixture Model Clustering**|Zhipeng Ma et.al.|[2501.12429v1](http://arxiv.org/abs/2501.12429v1)|null|

#### Abstracts
##### **Robust Representation Consistency Model via Contrastive Denoising**
2501.13094v1 by Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Zhongjia Ba, Kui Ren, Jun Zhu, Anima Anandkumar

Robustness is essential for deep neural networks, especially in
security-sensitive applications. To this end, randomized smoothing provides
theoretical guarantees for certifying robustness against adversarial
perturbations. Recently, diffusion models have been successfully employed for
randomized smoothing to purify noise-perturbed samples before making
predictions with a standard classifier. While these methods excel at small
perturbation radii, they struggle with larger perturbations and incur a
significant computational overhead during inference compared to classical
methods. To address this, we reformulate the generative modeling task along the
diffusion trajectories in pixel space as a discriminative task in the latent
space. Specifically, we use instance discrimination to achieve consistent
representations along the trajectories by aligning temporally adjacent points.
After fine-tuning based on the learned representations, our model enables
implicit denoising-then-classification via a single prediction, substantially
reducing inference costs. We conduct extensive experiments on various datasets
and achieve state-of-the-art performance with minimal computation budget during
inference. For example, our method outperforms the certified accuracy of
diffusion-based methods on ImageNet across all perturbation radii by 5.3% on
average, with up to 11.6% at larger radii, while reducing inference costs by
85$\times$ on average. Codes are available at:
https://github.com/jiachenlei/rRCM.

摘要：對於深度神經網路來說，穩健性至關重要，尤其是在安全敏感的應用中。為此，隨機平滑提供了理論保證，可以證明對抗擾動的穩健性。最近，擴散模型已成功用於隨機平滑，以在使用標準分類器進行預測之前淨化受雜訊擾動的樣本。儘管這些方法在小擾動半徑方面表現出色，但它們在較大的擾動中會遇到困難，並且與傳統方法相比，在推理過程中會產生大量的計算開銷。為了解決這個問題，我們將像素空間中的擴散軌跡上的生成式建模任務重新表述為潛在空間中的判別任務。具體來說，我們使用實例判別來通過對齊時間相鄰的點來實現沿著軌跡的一致表示。在根據學習到的表示進行微調後，我們的模型能夠通過單一預測實現隱式去噪然後分類，從而大幅降低推理成本。我們在各種資料集上進行了廣泛的實驗，並在推理過程中以最小的計算預算實現了最先進的效能。例如，我們的模型在 ImageNet 上所有擾動半徑的認證準確度都比基於擴散的方法高出 5.3%，在較大半徑時高達 11.6%，同時將推理成本平均降低了 85 倍。程式碼可在以下位置取得：
https://github.com/jiachenlei/rRCM。

##### **Boosting MCTS with Free Energy Minimization**
2501.13083v1 by Mawaba Pascal Dao, Adrian Peter

Active Inference, grounded in the Free Energy Principle, provides a powerful
lens for understanding how agents balance exploration and goal-directed
behavior in uncertain environments. Here, we propose a new planning framework,
that integrates Monte Carlo Tree Search (MCTS) with active inference objectives
to systematically reduce epistemic uncertainty while pursuing extrinsic
rewards. Our key insight is that MCTS already renowned for its search
efficiency can be naturally extended to incorporate free energy minimization by
blending expected rewards with information gain. Concretely, the Cross-Entropy
Method (CEM) is used to optimize action proposals at the root node, while tree
expansions leverage reward modeling alongside intrinsic exploration bonuses.
This synergy allows our planner to maintain coherent estimates of value and
uncertainty throughout planning, without sacrificing computational
tractability. Empirically, we benchmark our planner on a diverse set of
continuous control tasks, where it demonstrates performance gains over both
standalone CEM and MCTS with random rollouts.

摘要：主動推論基於自由能原理，提供了一個強大的視角，用於理解代理如何在不確定的環境中平衡探索和目標導向行為。在此，我們提出了一個新的規劃框架，它將蒙地卡羅樹搜尋 (MCTS) 與主動推論目標相整合，以在追求外在獎勵的同時系統性地減少認識論的不確定性。我們的關鍵見解是，MCTS 已以其搜尋效率而聞名，它可以通過將預期獎勵與資訊獲取相結合，自然地擴展到納入自由能最小化。具體來說，交叉熵方法 (CEM) 用於最佳化根節點的動作建議，而樹擴展則利用獎勵建模以及內在探索獎勵。這種協同作用使我們的規劃器能夠在整個規劃過程中維持對價值和不確定性的連貫估計，而不會犧牲計算的可行性。根據經驗，我們在各種連續控制任務上對我們的規劃器進行了基準測試，它展示了相較於獨立的 CEM 和具有隨機滾動的 MCTS，它在效能上有所提升。

##### **Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment**
2501.13080v1 by Melissa Kazemi Rad, Huy Nghiem, Andy Luo, Sahil Wadhwa, Mohammad Sorower, Stephen Rawls

Large Language Models (LLMs) have demonstrated powerful capabilities that
render them valuable in different applications, including conversational AI
products. It is paramount to ensure the security and reliability of these
products by mitigating their vulnerabilities towards malicious user
interactions, which can lead to the exposure of great risks and reputational
repercussions. In this work, we present a comprehensive study on the efficacy
of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs
that serve as input moderation guardrails. We systematically explore various
tuning methods by leveraging a small set of training data to adapt these models
as proxy defense mechanisms to detect malicious inputs and provide a reasoning
for their verdicts, thereby preventing the exploitation of conversational
agents. We rigorously evaluate the efficacy and robustness of different tuning
strategies to generalize across diverse adversarial and malicious query types.
Our experimental results outline the potential of alignment processes tailored
to a varied range of harmful input queries, even with constrained data
resources. These techniques significantly enhance the safety of conversational
AI systems and provide a feasible framework for deploying more secure and
trustworthy AI-driven interactions.

摘要：大型語言模型 (LLM) 已展現強大的能力，使其在不同的應用程式中有價值，包括對話式 AI 產品。透過減輕其對惡意使用者互動的漏洞，確保這些產品的安全性和可靠性至關重要，這可能會導致重大風險和名譽損害。在這項工作中，我們對微調和調整不同 LLM 的思考鏈 (CoT) 回應的功效進行全面研究，這些回應可用作輸入審核防護措施。我們系統性地探索各種調整方法，利用一小組訓練資料來調整這些模型，作為代理防禦機制來偵測惡意輸入並為其判決提供理由，從而防止對話式代理的利用。我們嚴格評估不同調整策略的功效和穩健性，以概括各種對抗性和惡意查詢類型。我們的實驗結果概述了調整流程的潛力，這些流程針對各種有害輸入查詢進行調整，即使在資料資源受限的情況下也是如此。這些技術顯著增強了對話式 AI 系統的安全性，並提供了一個可行的框架，用於部署更安全且值得信賴的 AI 驅動互動。

##### **Autonomy-of-Experts Models**
2501.13074v1 by Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan

Mixture-of-Experts (MoE) models mostly use a router to assign tokens to
specific expert modules, activating only partial parameters and often
outperforming dense models. We argue that the separation between the router's
decision-making and the experts' execution is a critical yet overlooked issue,
leading to suboptimal expert selection and ineffective learning. To address
this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which
experts autonomously select themselves to process inputs. AoE is based on the
insight that an expert is aware of its own capacity to effectively process a
token, an awareness reflected in the scale of its internal activations. In AoE,
routers are removed; instead, experts pre-compute internal activations for
inputs and are ranked based on their activation norms. Only the top-ranking
experts proceed with the forward pass, while the others abort. The overhead of
pre-computing activations is reduced through a low-rank weight factorization.
This self-evaluating-then-partner-comparing approach ensures improved expert
selection and effective learning. We pre-train language models having 700M up
to 4B parameters, demonstrating that AoE outperforms traditional MoE models
with comparable efficiency.

摘要：混合专家 (MoE) 模型主要使用路由器將代幣分配給特定專家模組，只啟動部分參數，並且通常優於稠密模型。我們認為路由器的決策制定與專家的執行之間的區別是一個關鍵但被忽視的問題，導致專家選擇次佳和學習無效。為了解決這個問題，我們提出專家自主 (AoE)，一種新的 MoE 典範，其中專家自主選擇自己來處理輸入。AoE 基於這樣的見解：專家知道自己有效處理代幣的能力，這種意識反映在其內部激活的規模中。在 AoE 中，路由器被移除；相反，專家會預先計算輸入的內部激活，並根據其激活範數進行排名。只有排名最高的專家才會繼續進行前向傳遞，而其他專家則中斷。通過低階權重分解，可以減少預先計算激活的開銷。這種自評然後與合作夥伴比較的方法確保了改進的專家選擇和有效的學習。我們預先訓練了具有 700M 到 4B 參數的語言模型，證明 AoE 優於具有可比效率的傳統 MoE 模型。

##### **AdaWM: Adaptive World Model based Planning for Autonomous Driving**
2501.13072v1 by Hang Wang, Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang

World model based reinforcement learning (RL) has emerged as a promising
approach for autonomous driving, which learns a latent dynamics model and uses
it to train a planning policy. To speed up the learning process, the
pretrain-finetune paradigm is often used, where online RL is initialized by a
pretrained model and a policy learned offline. However, naively performing such
initialization in RL may result in dramatic performance degradation during the
online interactions in the new task. To tackle this challenge, we first analyze
the performance degradation and identify two primary root causes therein: the
mismatch of the planning policy and the mismatch of the dynamics model, due to
distribution shift. We further analyze the effects of these factors on
performance degradation during finetuning, and our findings reveal that the
choice of finetuning strategies plays a pivotal role in mitigating these
effects. We then introduce AdaWM, an Adaptive World Model based planning
method, featuring two key steps: (a) mismatch identification, which quantifies
the mismatches and informs the finetuning strategy, and (b) alignment-driven
finetuning, which selectively updates either the policy or the model as needed
using efficient low-rank updates. Extensive experiments on the challenging
CARLA driving tasks demonstrate that AdaWM significantly improves the
finetuning process, resulting in more robust and efficient performance in
autonomous driving systems.

摘要：基於世界模型的強化學習 (RL) 已成為自動駕駛的潛力方法，它學習潛在動態模型並利用它來訓練規劃政策。為了加速學習過程，通常會使用預訓練微調範例，其中線上 RL 由預訓練模型和離線學習的政策初始化。然而，在 RL 中天真地執行這種初始化可能會導致在新的任務中線上互動期間的戲劇性效能下降。為了應對這個挑戰，我們首先分析效能下降並找出兩個主要的根本原因：規劃政策的不匹配和動態模型的不匹配，這是由於分佈轉移。我們進一步分析了這些因素對微調期間效能下降的影響，我們的發現表明微調策略的選擇在減輕這些影響方面發揮了關鍵作用。然後我們介紹 AdaWM，一種基於自適應世界模型的規劃方法，具有兩個關鍵步驟：(a) 不匹配識別，它量化不匹配並告知微調策略，以及 (b) 對齊驅動微調，它根據需要使用有效的低秩更新選擇性地更新政策或模型。在具有挑戰性的 CARLA 駕駛任務上進行的廣泛實驗表明，AdaWM 大大改進了微調過程，從而在自動駕駛系統中產生了更強大、更有效的效能。

##### **Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning**
2501.13042v1 by Bohao Yang, Yingji Zhang, Dong Liu, André Freitas, Chenghua Lin

Recent large language models (LLMs) have advanced table understanding
capabilities but rely on converting tables into text sequences. While
multimodal large language models (MLLMs) enable direct visual processing, they
face limitations in handling scientific tables due to fixed input image
resolutions and insufficient numerical reasoning capabilities. We present a
comprehensive framework for multimodal scientific table understanding and
reasoning with dynamic input image resolutions. Our framework consists of three
key components: (1) MMSci-Pre, a domain-specific table structure learning
dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,
an instruction tuning dataset with 12K samples across three table-based tasks,
and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically
designed to evaluate numerical reasoning capabilities. Extensive experiments
demonstrate that our domain-specific approach with 52K scientific table images
achieves superior performance compared to 150K general-domain tables,
highlighting the importance of data quality over quantity. Our proposed
table-based MLLMs with dynamic input resolutions show significant improvements
in both general table understanding and numerical reasoning capabilities, with
strong generalisation to held-out datasets. Our code and data are publicly
available at https://github.com/Bernard-Yang/MMSci_Table.

摘要：最近的大型語言模型 (LLM) 提升了表格理解能力，但依賴於將表格轉換為文字序列。雖然多模態大型語言模型 (MLLM) 能直接進行視覺處理，但由於固定的輸入影像解析度和不足的數字推理能力，在處理科學表格時面臨限制。我們提出一個多模態科學表格理解和推理的全面架構，並採用動態輸入影像解析度。我們的架構包含三個關鍵組成部分：(1) MMSci-Pre，一個特定領域的表格結構學習資料集，包含 52K 個科學表格結構辨識範例，(2) MMSci-Ins，一個指令調整資料集，在三項基於表格的任務中包含 12K 個範例，以及 (3) MMSci-Eval，一個基準，包含 3,114 個測試範例，專門用於評估數字推理能力。廣泛的實驗證明，我們採用 52K 個科學表格影像的特定領域方法，與 150K 個一般領域表格相比，能達成更好的效能，突顯資料品質比數量重要的概念。我們提出的基於表格的 MLLM 採用動態輸入解析度，在一般表格理解和數字推理能力方面都展現顯著的進步，並能有效泛化到保留的資料集。我們的程式碼和資料已公開發布於 https://github.com/Bernard-Yang/MMSci_Table。

##### **Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review**
2501.13014v1 by Andrii Zahorodnii, Jasper J. F. van den Bosch, Ian Charest, Christopher Summerfield, Ila R. Fiete

This study proposes a data-driven framework for enhancing the accuracy and
efficiency of scientific peer review through an open, bottom-up process that
estimates reviewer quality. Traditional closed peer review systems, while
essential for quality control, are often slow, costly, and subject to biases
that can impede scientific progress. Here, we introduce a method that evaluates
individual reviewer reliability by quantifying agreement with community
consensus scores and applying Bayesian weighting to refine paper quality
assessments. We analyze open peer review data from two major scientific
conferences, and demonstrate that reviewer-specific quality scores
significantly improve the reliability of paper quality estimation. Perhaps
surprisingly, we find that reviewer quality scores are unrelated to authorship
quality. Our model incorporates incentive structures to recognize high-quality
reviewers and encourage broader coverage of submitted papers, thereby
mitigating the common "rich-get-richer" pitfall of social media. These findings
suggest that open peer review, with mechanisms for estimating and incentivizing
reviewer quality, offers a scalable and equitable alternative for scientific
publishing, with potential to enhance the speed, fairness, and transparency of
the peer review process.

摘要：本研究提出了一個資料驅動的架構，透過一個開放的由下而上的流程來提升科學同行評審的準確度和效率，此流程會評估審稿人的品質。傳統的封閉式同行評審系統對於品質控管來說固然重要，但通常很緩慢、成本高，且容易有偏見，可能會阻礙科學進展。在此，我們介紹一個方法，透過量化與社群共識分數的一致性，並套用貝氏加權來改善論文品質評量，以評估個別審稿人的可靠性。我們分析了兩場大型科學會議的開放式同行評審資料，並證明特定審稿人的品質分數大幅提升了論文品質評估的可靠性。或許令人驚訝的是，我們發現審稿人品質分數與作者品質無關。我們的模型納入了激勵機制，以表彰高品質審稿人，並鼓勵對提交論文進行更廣泛的涵蓋，從而減輕社群媒體常見的「富者愈富」陷阱。這些發現表明，開放式同行評審，具備評估和激勵審稿人品質的機制，為科學出版提供了一個可擴充且公平的替代方案，有潛力提升同行評審流程的速度、公平性和透明度。

##### **MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking**
2501.13011v1 by Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah

Future advanced AI systems may learn sophisticated strategies through
reinforcement learning (RL) that humans cannot understand well enough to safely
evaluate. We propose a training method which avoids agents learning undesired
multi-step plans that receive high reward (multi-step "reward hacks") even if
humans are not able to detect that the behaviour is undesired. The method,
Myopic Optimization with Non-myopic Approval (MONA), works by combining
short-sighted optimization with far-sighted reward. We demonstrate that MONA
can prevent multi-step reward hacking that ordinary RL causes, even without
being able to detect the reward hacking and without any extra information that
ordinary RL does not get access to. We study MONA empirically in three settings
which model different misalignment failure modes including 2-step environments
with LLMs representing delegated oversight and encoded reasoning and
longer-horizon gridworld environments representing sensor tampering.

摘要：未來進階的 AI 系統可能透過人類無法充分理解以安全評估的強化學習 (RL) 來學習複雜的策略。我們提出了一種訓練方法，可避免代理學習即使人類無法偵測到行為不受歡迎，也會獲得高獎勵的非預期多步驟計畫（多步驟「獎勵破解」）。這種方法，近視優化搭配遠視獎勵 (MONA)，是透過結合短視優化與遠見獎勵來運作。我們示範 MONA 可以防止一般 RL 造成的步驟獎勵破解，即使無法偵測到獎勵破解，且沒有任何一般 RL 無法取得的額外資訊。我們在三種設定中以經驗研究 MONA，其中建模了不同的失準失敗模式，包括具有代表委派監督和編碼推理的 LLM 的 2 步驟環境，以及代表感測器破壞的較長時程網格世界環境。

##### **Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament**
2501.13007v1 by Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li

Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large
Language Models (LLMs), relies on reward models to select the best candidate
solution from multiple generations. However, traditional reward models often
assign arbitrary and inconsistent scores, limiting their effectiveness. To
address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a
knockout tournament for BoN sampling. Instead of assigning absolute scores,
given one math problem, Pairwise RM evaluates two candidate solutions'
correctness simultaneously. This approach eliminates the need for arbitrary
scoring and enables cross-validation of solutions through parallel comparison.
In the knockout tournament, Pairwise RM conducts pairwise comparisons between
candidate solutions and eliminates the incorrect ones iteratively. We construct
\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from
NumiaMath and annotated using \texttt{gemini-1.5-flash}, and train the Pairwise
RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench
demonstrate significant improvements over traditional discriminative reward
models. And a 40\% to 60\% relative improvement is achieved on the top 50\%
challenging problems.

摘要：最佳 N (BoN) 抽樣，一種大型語言模型 (LLM) 測試時縮放的常見策略，依賴於獎勵模型從多個世代中選出最佳候選解。然而，傳統的獎勵模型通常會分配任意且不一致的分數，限制了它們的有效性。為了解決這個問題，我們提出了一種配對獎勵模型 (配對 RM)，並結合淘汰賽進行 BoN 抽樣。配對 RM 沒有分配絕對分數，而是給予一個數學問題，同時評估兩個候選解的正確性。這種方法消除了對任意計分的需要，並能透過並行比較對解進行交叉驗證。在淘汰賽中，配對 RM 在候選解之間進行配對比較，並反覆淘汰不正確的解。我們構建了 \ourdataset，一個從 NumiaMath 衍生的 443K 個配對比較的規模化資料集，並使用 \texttt{gemini-1.5-flash} 進行註解，並透過監督微調訓練配對 RM。在 MATH-500 和奧林匹克基準上的實驗證明了相較於傳統的判別獎勵模型有顯著的改進。並且在最具挑戰性的問題前 50% 中，獲得了 40% 到 60% 的相對改進。

##### **Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities**
2501.12980v1 by Florian Kankowski, Torgrim Solstad, Sina Zarriess, Oliver Bott

In this paper, we compare data generated with mono- and multilingual LLMs
spanning a range of model sizes with data provided by human participants in an
experimental setting investigating well-established discourse biases. Beyond
the comparison as such, we aim to develop a benchmark to assess the
capabilities of LLMs with discourse biases as a robust proxy for more general
discourse understanding capabilities. More specifically, we investigated
Implicit Causality verbs, for which psycholinguistic research has found
participants to display biases with regard to three phenomena:\ the
establishment of (i) coreference relations (Experiment 1), (ii) coherence
relations (Experiment 2), and (iii) the use of particular referring expressions
(Experiments 3 and 4). With regard to coreference biases we found only the
largest monolingual LLM (German Bloom 6.4B) to display more human-like biases.
For coherence relation, no LLM displayed the explanation bias usually found for
humans. For referring expressions, all LLMs displayed a preference for
referring to subject arguments with simpler forms than to objects. However, no
bias effect on referring expression was found, as opposed to recent studies
investigating human biases.

摘要：<paragraph>在本文中，我们将比较单语言和多语言 LLM 生成的资料，这些资料涵盖了一系列模型规模，并与人类参与者在实验环境中提供的资料进行比较，以调查已确立的话语偏差。除了比较本身之外，我们的目标是制定一个基准来评估 LLM 的能力，其中话语偏差作为更一般话语理解能力的可靠代理。更具体地说，我们调查了隐含因果关系动词，心理语言学研究发现参与者在以下三个现象方面表现出偏差：(i) 指称关系的建立（实验 1）、(ii) 连贯关系（实验 2）和 (iii) 特定指称表达式的使用（实验 3 和 4）。关于指称偏差，我们发现只有最大的单语言 LLM（German Bloom 6.4B）表现出更像人类的偏差。对于连贯关系，没有 LLM 表现出通常在人类中发现的解释偏差。对于指称表达，所有 LLM 都表现出比对象更喜欢使用较简单的形式来指称主语论点的偏好。然而，与最近研究人类偏差的研究相反，没有发现指称表达的偏差效应。</paragraph>

##### **FlanEC: Exploring Flan-T5 for Post-ASR Error Correction**
2501.12979v1 by Moreno La Quatra, Valerio Mario Salerno, Yu Tsao, Sabato Marco Siniscalchi

In this paper, we present an encoder-decoder model leveraging Flan-T5 for
post-Automatic Speech Recognition (ASR) Generative Speech Error Correction
(GenSEC), and we refer to it as FlanEC. We explore its application within the
GenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a
single output sentence. By utilizing n-best lists from ASR models, we aim to
improve the linguistic correctness, accuracy, and grammaticality of final ASR
transcriptions. Specifically, we investigate whether scaling the training data
and incorporating diverse datasets can lead to significant improvements in
post-ASR error correction. We evaluate FlanEC using the HyPoradise dataset,
providing a comprehensive analysis of the model's effectiveness in this domain.
Furthermore, we assess the proposed approach under different settings to
evaluate model scalability and efficiency, offering valuable insights into the
potential of instruction-tuned encoder-decoder models for this task.

摘要：在本文中，我們提出了一個編碼器-解碼器模型，利用 Flan-T5 進行自動語音辨識 (ASR) 後的生成式語音錯誤校正 (GenSEC)，我們稱之為 FlanEC。我們在 GenSEC 框架內探索其應用，通過將 n 個最佳假設映射到一個單一的輸出句子來增強 ASR 輸出。通過利用 ASR 模型中的 n 個最佳列表，我們的目標是提高最終 ASR 轉錄的語言正確性、準確性和語法性。具體來說，我們研究了擴充訓練數據和納入多樣化數據集是否會對 ASR 後錯誤校正產生顯著的改進。我們使用 HyPoradise 數據集評估 FlanEC，對模型在這個領域的有效性進行了全面的分析。此外，我們在不同的設置下評估所提出的方法，以評估模型的可擴展性和效率，為這種任務的指令調整編碼器-解碼器模型的潛力提供了寶貴的見解。

##### **OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models**
2501.12975v1 by Chongren Sun, Yuran Li, Di Wu, Benoit Boulet

Large Language Models (LLMs) are highly capable but require significant
computational resources for both training and inference. Within the LLM family,
smaller models (those with fewer than 10 billion parameters) also perform well
across various tasks. However, these smaller models share similar limitations
to their larger counterparts, including the tendency to hallucinate. Despite
the existence of many benchmarks to evaluate hallucination in LLMs, few have
specifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely
varying performance across different benchmarks. In this paper, we introduce
OnionEval, a multi-layer structured framework with a specific metric called the
context-influence score (CI), designed to effectively assess the
fact-conflicting hallucination tendencies of small LLMs across different
contextual levels. Our experimental results reveal a key feature of SLLMs: they
excel in factual analysis but face challenges with context reasoning. Further
investigation shows that a simple Chain-of-Thought strategy can significantly
reduce these limitations, improving the practical usefulness of SLLMs in
real-world applications.

摘要：大型語言模型 (LLM) 的能力極強，但無論是在訓練或推理上，都需要大量的運算資源。在 LLM 家族中，較小的模型（參數少於 100 億個）也能在各種任務中表現良好。然而，這些較小的模型與較大的模型一樣，有類似的限制，包括出現幻覺的傾向。儘管有許多基準來評估 LLM 中的幻覺，但鮮少專注於小型 LLM (SLLM)。此外，SLLM 在不同的基準中表現差異很大。在本文中，我們介紹了 OnionEval，這是一個多層結構框架，有一個稱為上下文影響分數 (CI) 的特定指標，旨在有效評估不同上下文層級中小型 LLM 的事實衝突幻覺傾向。我們的實驗結果揭示了 SLLM 的一個關鍵特徵：它們在事實分析中表現出色，但在上下文推理方面面臨挑戰。進一步的調查顯示，一個簡單的思考鏈策略可以顯著減少這些限制，提高 SLLM 在實際應用中的實用性。

##### **Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs**
2501.12972v1 by Jan Corazza, Ivan Gavran, Gabriela Moreira, Daniel Neider

When blockchain systems are said to be trustless, what this really means is
that all the trust is put into software. Thus, there are strong incentives to
ensure blockchain software is correct -- vulnerabilities here cost millions and
break businesses. One of the most powerful ways of establishing software
correctness is by using formal methods. Approaches based on formal methods,
however, induce a significant overhead in terms of time and expertise required
to successfully employ them. Our work addresses this critical disadvantage by
automating the creation of a formal model -- a mathematical abstraction of the
software system -- which is often a core task when employing formal methods. We
perform model synthesis in three phases: we first transpile the code into model
stubs; then we "fill in the blanks" using a large language model (LLM);
finally, we iteratively repair the generated model, on both syntactical and
semantical level. In this way, we significantly reduce the amount of time
necessary to create formal models and increase accessibility of valuable
software verification methods that rely on them. The practical context of our
work was reducing the time-to-value of using formal models for correctness
audits of smart contracts.

摘要：當區塊鏈系統被稱作無需信任時，這實際上意味著
所有的信任都賦予了軟體。因此，有強烈的誘因來
確保區塊鏈軟體是正確的——此處的漏洞會造成數百萬損失並
讓企業破產。建立軟體正確性的最有力的方法之一是使用形式化方法。
然而，基於形式化方法的方法會導致在時間和專業知識方面的顯著開銷
才能成功地使用它們。我們的作品透過自動化形式化模型的建立來解決這個重大的缺點——軟體系統的數學抽象——這通常是使用形式化方法時的核心任務。我們
分三個階段執行模型合成：我們首先將程式碼轉譯成模型存根；然後我們使用大型語言模型 (LLM)「填補空白」；
最後，我們在語法和語義層面上反覆修復產生的模型。透過這種方式，我們大幅減少建立形式化模型所需的時間，並增加依賴它們的寶貴軟體驗證方法的可及性。我們工作的實際背景是減少使用形式化模型進行智慧合約正確性稽核的時間價值。

##### **It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act**
2501.12962v1 by Kristof Meding

What constitutes a fair decision? This question is not only difficult for
humans but becomes more challenging when Artificial Intelligence (AI) models
are used. In light of discriminatory algorithmic behaviors, the EU has recently
passed the AI Act, which mandates specific rules for AI models, incorporating
both traditional legal non-discrimination regulations and machine learning
based algorithmic fairness concepts. This paper aims to bridge these two
different concepts in the AI Act through: First a high-level introduction of
both concepts targeting legal and computer science-oriented scholars, and
second an in-depth analysis of the AI Act's relationship between legal
non-discrimination regulations and algorithmic fairness. Our analysis reveals
three key findings: (1.), most non-discrimination regulations target only
high-risk AI systems. (2.), the regulation of high-risk systems encompasses
both data input requirements and output monitoring, though these regulations
are often inconsistent and raise questions of computational feasibility. (3.)
Regulations for General Purpose AI Models, such as Large Language Models that
are not simultaneously classified as high-risk systems, currently lack
specificity compared to other regulations. Based on these findings, we
recommend developing more specific auditing and testing methodologies for AI
systems. This paper aims to serve as a foundation for future interdisciplinary
collaboration between legal scholars and computer science-oriented machine
learning researchers studying discrimination in AI systems.

摘要：什麼構成一個公平的決定？這個問題不僅對人類來說很難，當使用人工智慧（AI）模型時，這個問題會變得更具挑戰性。鑑於演算法行為具有歧視性，歐盟最近通過了 AI 法案，該法案對 AI 模型制定了具體規則，結合了傳統的法律反歧視法規和基於機器學習的演算法公平性概念。本文旨在通過以下方式彌合 AI 法案中的這兩個不同概念：首先，針對法律和電腦科學學者的高層級介紹，其次，深入分析 AI 法案在法律反歧視法規和演算法公平性之間的關係。我們的分析揭示了三個關鍵發現：(1.)，大多數反歧視法規只針對高風險 AI 系統。(2.)，高風險系統的法規包含資料輸入需求和輸出監控，儘管這些法規通常不一致，並引發了計算可行性的問題。(3.)，通用 AI 模型的法規，例如未同時歸類為高風險系統的大語言模型，與其他法規相比目前缺乏具體性。根據這些發現，我們建議為 AI 系統開發更具體的稽核和測試方法。本文旨在作為法律學者和以電腦科學為導向的機器學習研究人員之間未來跨學科合作的基礎，研究 AI 系統中的歧視。

##### **Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference**
2501.12959v1 by Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

Although applications involving long-context inputs are crucial for the
effective utilization of large language models (LLMs), they also result in
increased computational costs and reduced performance. To address this
challenge, we propose an efficient, training-free prompt compression method
that retains key information within compressed prompts. We identify specific
attention heads in transformer-based LLMs, which we designate as evaluator
heads, that are capable of selecting tokens in long inputs that are most
significant for inference. Building on this discovery, we develop EHPC, an
Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly
"skim through" input prompts by leveraging only the first few layers with
evaluator heads during the pre-filling stage, subsequently passing only the
important tokens to the model for inference. EHPC achieves state-of-the-art
results across two mainstream benchmarks: prompt compression and long-context
inference acceleration. Consequently, it effectively reduces the complexity and
costs associated with commercial API calls. We further demonstrate that EHPC
attains competitive results compared to key-value cache-based acceleration
methods, thereby highlighting its potential to enhance the efficiency of LLMs
for long-context tasks.

摘要：儘管涉及長語境輸入的應用程式對於大語言模型 (LLM) 的有效利用至關重要，但它們也導致運算成本增加和效能降低。為了應對這個挑戰，我們提出了一種有效率、無需訓練的提示壓縮方法，它保留了壓縮提示中的關鍵資訊。我們在基於轉換器的 LLM 中識別出特定的注意力頭，我們將它們指定為評估器頭，它們能夠在長輸入中選擇對推論最重要的代幣。基於這個發現，我們開發了 EHPC，一種基於評估器頭的提示壓縮方法，它使 LLM 能夠僅利用預填入階段中帶有評估器頭的前幾層快速「瀏覽」輸入提示，隨後僅將重要的代幣傳遞給模型進行推論。EHPC 在兩個主流基準上達到了最先進的結果：提示壓縮和長語境推論加速。因此，它有效地降低了與商業 API 呼叫相關的複雜性和成本。我們進一步證明，與基於快取的加速方法相比，EHPC 達到了具有競爭力的結果，從而突顯了它增強 LLM 在長語境任務中的效率的潛力。

##### **GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models**
2501.12956v1 by Pengxiang Zhao, Xiaoming Yuan

Large Language Models (LLMs) face significant deployment challenges due to
their substantial resource requirements. While low-bit quantized weights can
reduce memory usage and improve inference efficiency, current hardware lacks
native support for mixed-precision General Matrix Multiplication (mpGEMM),
resulting in inefficient dequantization-based implementations. Moreover,
uniform quantization methods often fail to capture weight distributions
adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive
Non-Uniform Quantization), a layer-wise post-training non-uniform quantization
framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ
achieves superior quantization performance by utilizing a training-free,
GPU-adaptive optimization algorithm to efficiently reduce layer-wise
quantization errors. Extensive experiments demonstrate GANQ's ability to reduce
the perplexity gap from the FP16 baseline compared to state-of-the-art methods
for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single
NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup
over the baseline, advancing memory and inference efficiency in LLM deployment.

摘要：大型語言模型 (LLM) 因其龐大的資源需求而面臨重大的部署挑戰。儘管低位元量化權重可以減少記憶體使用量並提升推論效率，但目前的硬體缺乏對混合精度一般矩陣乘法 (mpGEMM) 的原生支援，導致非量化為基礎的實作效率不彰。此外，均勻量化方法通常無法充分擷取權重分佈，導致效能下降。我們提出 GANQ (GPU 自適應非均勻量化)，一種針對硬體有效查找表格型 mpGEMM 最佳化的層級後訓練非均勻量化架構。GANQ 透過利用無需訓練的 GPU 自適應最佳化演算法，有效降低層級量化誤差，達成優異的量化效能。廣泛的實驗證明 GANQ 降低困惑度間隔的能力，與 3 位元和 4 位元量化的最先進方法相比，從 FP16 基準線降低困惑度間隔。此外，當部署在單一 NVIDIA RTX 4090 GPU 上時，GANQ 的量化模型可達到比基準線快 2.57 倍的速度，提升 LLM 部署中的記憶體和推論效率。

##### **Multifractal hopscotch in "Hopscotch" by Julio Cortazar**
2501.12955v1 by Jakub Dec, Michał Dolina, Stanisław Drożdż, Jarosław Kwapień, Tomasz Stanisz

Punctuation is the main factor introducing correlations in natural language
written texts and it crucially impacts their overall effectiveness,
expressiveness, and readability. Punctuation marks at the end of sentences are
of particular importance as their distribution can determine various complexity
features of written natural language. Here, the sentence length variability
(SLV) time series representing "Hopscotch" by Julio Cortazar are subjected to
quantitative analysis with an attempt to identify their distribution type,
long-memory effects, and potential multiscale patterns. The analyzed novel is
an important and innovative piece of literature whose essential property is
freedom of movement between its building blocks given to a reader by the
author. The statistical consequences of this freedom are closely investigated
in both the original, Spanish version of the novel, and its translations into
English and Polish. Clear evidence of rich multifractality in the SLV dynamics,
with a left-sided asymmetry, however, is observed in all three language
versions as well as in the versions with differently ordered chapters.

摘要：標點符號是自然語言書面文本中引入關聯性的主要因素，並且它對其整體有效性、表達性和可讀性產生至關重要的影響。句子結尾的標點符號特別重要，因為它們的分布可以決定書面自然語言的各種複雜性特徵。在此，由胡里奧·科塔薩爾撰寫的「跳房子」句子長度可變性 (SLV) 時間序列經過定量分析，試圖識別它們的分布類型、長記憶效應和潛在的多尺度模式。所分析的小說是一部重要且創新的文學作品，其基本屬性是作者賦予讀者的構建模組之間的自由移動。這種自由的統計後果在小說的西班牙語原版及其英文和波蘭文譯本中都受到密切研究。然而，在三種語言版本以及章節順序不同的版本中，都觀察到 SLV 動力學中豐富的多重分形性，具有左偏不對稱性。

##### **Punctuation patterns in "Finnegans Wake" by James Joyce are largely translation-invariant**
2501.12954v1 by Krzysztof Bartnicki, Stanisław Drożdż, Jarosław Kwapień, Tomasz Stanisz

The complexity characteristics of texts written in natural languages are
significantly related to the rules of punctuation. In particular, the distances
between punctuation marks measured by the number of words quite universally
follow the family of Weibull distributions known from survival analyses.
However, the values of two parameters marking specific forms of these
distributions distinguish specific languages. This is such a strong constraint
that the punctuation distributions of texts translated from the original
language into another adopt quantitative characteristics of the target
language. All these changes take place within Weibull distributions such that
the corresponding hazard functions are always increasing. Recent previous
research shows that James Joyce's famous "Finnegans Wake" is subject to such
extreme distribution from the Weibull family that the corresponding hazard
function is clearly decreasing. At the same time, the distances of sentence
ending punctuation marks, determining the variability of sentence length, have
an almost perfect multifractal organization, so far to such an extent found
nowhere else in the literature. In the present contribution based on several
available translations (Dutch, French, German, Polish, Russian) of "Finnegans
Wake", it is shown that the punctuation characteristics of this work remain
largely translation invariant, contrary to the common cases. These observations
may constitute further evidence that "Finnegans Wake" is a translinguistic work
in this respect as well, in line with Joyce's original intention.

摘要：自然語言書寫文本的複雜性特徵與標點符號規則有顯著相關性。特別是，以字數測量的標點符號之間的距離普遍遵循威布爾分布族，這是從生存分析中得知的。然而，標記這些分布特定形式的兩個參數的值區分了特定語言。這是一個如此強烈的約束，以至於從原始語言翻譯成另一種語言的標點分佈採用目標語言的量化特徵。所有這些變化都發生在威布爾分佈中，使得對應的風險函數始終增加。最近先前的研究表明，詹姆斯·喬伊斯的著名作品「芬尼根的守靈夜」受到威布爾家族的極端分佈，以至於對應的風險函數明顯下降。與此同時，句子結束標點符號的距離決定了句長的可變性，具有幾乎完美的多分形組織，到目前為止，在其他文獻中還沒有發現這種程度。在基於「芬尼根的守靈夜」的幾種可用翻譯（荷蘭語、法語、德語、波蘭語、俄語）的當前貢獻中，表明這部作品的標點符號特徵在很大程度上保持翻譯不變，這與常見情況相反。這些觀察可能構成進一步的證據，證明「芬尼根的守靈夜」在這個方面也是一部跨語言的作品，這符合喬伊斯的最初意圖。

##### **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**
2501.12948v1 by DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and
DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement
learning (RL) without supervised fine-tuning (SFT) as a preliminary step,
demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero
naturally emerges with numerous powerful and intriguing reasoning behaviors.
However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we
introduce DeepSeek-R1, which incorporates multi-stage training and cold-start
data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217
on reasoning tasks. To support the research community, we open-source
DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,
70B) distilled from DeepSeek-R1 based on Qwen and Llama.

摘要：我們介紹了我們的第一代推理模型，DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 是一個通過大規模強化學習 (RL) 訓練的模型，沒有監督微調 (SFT) 作為預備步驟，展示了卓越的推理能力。通過 RL，DeepSeek-R1-Zero 自然而然地出現了許多強大而有趣的推理行為。然而，它遇到了可讀性差和語言混雜等挑戰。為了解決這些問題並進一步增強推理性能，我們引入了 DeepSeek-R1，它在 RL 之前結合了多階段訓練和冷啟動數據。DeepSeek-R1 在推理任務上達到了與 OpenAI-o1-1217 相當的性能。為了支持研究社群，我們開放了 DeepSeek-R1-Zero、DeepSeek-R1 和六個基於 Qwen 和 Llama 從 DeepSeek-R1 提煉出的稠密模型 (1.5B、7B、8B、14B、32B、70B) 的原始碼。

##### **PreciseCam: Precise Camera Control for Text-to-Image Generation**
2501.12910v1 by Edurne Bernal-Berdun, Ana Serrano, Belen Masia, Matheus Gadelha, Yannick Hold-Geoffroy, Xin Sun, Diego Gutierrez

Images as an artistic medium often rely on specific camera angles and lens
distortions to convey ideas or emotions; however, such precise control is
missing in current text-to-image models. We propose an efficient and general
solution that allows precise control over the camera when generating both
photographic and artistic images. Unlike prior methods that rely on predefined
shots, we rely solely on four simple extrinsic and intrinsic camera parameters,
removing the need for pre-existing geometry, reference 3D objects, and
multi-view data. We also present a novel dataset with more than 57,000 images,
along with their text prompts and ground-truth camera parameters. Our
evaluation shows precise camera control in text-to-image generation, surpassing
traditional prompt engineering approaches. Our data, model, and code are
publicly available at https://graphics.unizar.es/projects/PreciseCam2024.

摘要：影像作為一種藝術媒介，經常依賴特定的相機角度和鏡頭變形來傳達想法或情緒；然而，這種精確的控制在目前的文字轉影像模型中卻付之闕如。我們提出一個有效且通用的解決方案，允許在生成攝影和藝術影像時精確控制相機。有別於依賴預先定義的鏡頭的先前方法，我們僅依賴四個簡單的外在和內在相機參數，消除了對預先存在的幾何形狀、參考 3D 物件和多視圖資料的需求。我們還提供了一個包含超過 57,000 張影像的新穎資料集，以及其文字提示和地面實況相機參數。我們的評估顯示在文字轉影像生成中精確控制相機，超越傳統的提示工程方法。我們的資料、模型和程式碼可在 https://graphics.unizar.es/projects/PreciseCam2024 公開取得。

##### **FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces**
2501.12909v1 by Zhenran Xu, Longyue Wang, Jifang Wang, Zhouyi Li, Senbao Shi, Xue Yang, Yiyu Wang, Baotian Hu, Jun Yu, Min Zhang

Virtual film production requires intricate decision-making processes,
including scriptwriting, virtual cinematography, and precise actor positioning
and actions. Motivated by recent advances in automated decision-making with
language agent-based societies, this paper introduces FilmAgent, a novel
LLM-based multi-agent collaborative framework for end-to-end film automation in
our constructed 3D virtual spaces. FilmAgent simulates various crew roles,
including directors, screenwriters, actors, and cinematographers, and covers
key stages of a film production workflow: (1) idea development transforms
brainstormed ideas into structured story outlines; (2) scriptwriting elaborates
on dialogue and character actions for each scene; (3) cinematography determines
the camera setups for each shot. A team of agents collaborates through
iterative feedback and revisions, thereby verifying intermediate scripts and
reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key
aspects. Human evaluation shows that FilmAgent outperforms all baselines across
all aspects and scores 3.98 out of 5 on average, showing the feasibility of
multi-agent collaboration in filmmaking. Further analysis reveals that
FilmAgent, despite using the less advanced GPT-4o model, surpasses the
single-agent o1, showing the advantage of a well-coordinated multi-agent
system. Lastly, we discuss the complementary strengths and weaknesses of
OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.

摘要：虛擬電影製作需要複雜的決策過程，包括編劇、虛擬電影攝影、以及精準的演員定位和動作。受到語言代理人為基礎的社會自動決策制定近期進展的激勵，本文介紹了 FilmAgent，一個新穎的 LLM 為基礎的多重代理人合作架構，用於在我們建構的 3D 虛擬空間中進行端到端的電影自動化。FilmAgent 模擬了各種劇組角色，包括導演、編劇、演員和電影攝影師，涵蓋了電影製作工作流程的主要階段：(1) 構思開發將集思廣益的想法轉化為結構化的故事大綱；(2) 編劇為每個場景的對話和角色動作進行詳細說明；(3) 電影攝影決定每個鏡頭的攝影機設置。一組代理人透過反覆的回饋和修改進行合作，從而驗證中間腳本並減少幻覺。我們在 15 個想法和 4 個關鍵面向評估產生的影片。人類評估顯示，FilmAgent 在所有面向都優於所有基準，平均得分為 5 分中的 3.98 分，顯示了多重代理人在電影製作中合作的可行性。進一步的分析顯示，FilmAgent 儘管使用了較不先進的 GPT-4o 模型，但超越了單一代理人 o1，顯示了一個協調良好的多重代理人系統的優勢。最後，我們討論了 OpenAI 的文字轉影片模型 Sora 和我們的 FilmAgent 在電影製作中的互補優缺點。

##### **Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration**
2501.12901v1 by Offa Kingsleigh, Alfred Abercrombie, David Woolstencroft, Beorhtric Meadowcroft, Marcus Irvin

Contextual Partitioning introduces an innovative approach to enhancing the
architectural design of large-scale computational models through the dynamic
segmentation of parameters into context-aware regions. This methodology
emphasizes the importance of task-specific specialization, achieved through
adaptive parameter allocation mechanisms that align with the linguistic
features of input data. Experimental evaluations demonstrated substantial
improvements in accuracy, perplexity, and contextual coherence across a variety
of linguistic tasks, highlighting the adaptability and scalability of the
proposed framework. By reducing redundancy and enhancing computational
efficiency, Contextual Partitioning not only streamlines model operations but
also expands the scope of applications for advanced language processing
systems. The approach operates autonomously, requiring no external fine-tuning,
thereby addressing a significant limitation in conventional parameter
optimization techniques. Empirical results demonstrate the effectiveness of
gradient-driven segmentation, enabling models to dynamically recalibrate and
specialize in response to task-specific demands. Furthermore, resource
utilization metrics reveal notable reductions in memory usage and training
times, confirming the efficiency of the approach. Observations from qualitative
analyses illustrate improved contextual coherence and logical flow in generated
outputs, reinforcing the practical value of this technique. The findings
collectively demonstrate the potential for Contextual Partitioning to redefine
the scalability and adaptability of computational language architectures in
diverse and complex domains.

摘要：語境分割引入了一種創新方法，透過將參數動態區隔成具有語境感知區域，來增強大型運算模型的架構設計。此方法強調任務特定專業化的重要性，透過與輸入資料的語言特徵相符的自適應參數配置機制來達成。實驗評估顯示在各種語言任務中，準確度、困惑度和語境一致性都有顯著的提升，突顯了所提出架構的適應性和可擴充性。透過減少冗餘並增強運算效率，語境分割不僅簡化了模型操作，也擴展了進階語言處理系統的應用範圍。此方法自主運作，不需要外部微調，從而解決了傳統參數最佳化技術中的一項重大限制。經驗結果證明了梯度驅動分割的有效性，使模型能夠動態重新校準並根據特定任務的需求進行專業化。此外，資源使用量指標顯示記憶體使用量和訓練時間顯著減少，證實了此方法的效率。從品質分析的觀察結果顯示，生成的輸出語境一致性和邏輯流程獲得改善，強化了此技術的實用價值。這些發現共同證明了語境分割有潛力重新定義運算語言架構在多元且複雜領域中的可擴充性和適應性。

##### **Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback**
2501.12895v1 by Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, Yu Cheng

Large language models (LLMs) demonstrate impressive performance but lack the
flexibility to adapt to human preferences quickly without retraining. In this
work, we introduce Test-time Preference Optimization (TPO), a framework that
aligns LLM outputs with human preferences during inference, removing the need
to update model parameters. Rather than relying on purely numerical rewards,
TPO translates reward signals into textual critiques and uses them as textual
rewards to iteratively refine its response. Evaluations on benchmarks covering
instruction following, preference alignment, safety, and mathematics reveal
that TPO progressively improves alignment with human preferences. Notably,
after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can
surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO
scales efficiently with both the search width and depth during inference.
Through case studies, we illustrate how TPO exploits the innate capacity of LLM
to interpret and act upon reward signals. Our findings establish TPO as a
practical, lightweight alternative for test-time preference optimization,
achieving alignment on the fly. Our code is publicly available at
https://github.com/yafuly/TPO.

摘要：大型語言模型 (LLM) 表現出色，但缺乏靈活性，無法在不重新訓練的情況下快速適應人類偏好。在這項工作中，我們引入了測試時偏好最佳化 (TPO)，這是一個在推理期間將 LLM 輸出與人類偏好對齊的框架，消除了更新模型參數的需要。TPO 沒有依賴純粹的數字獎勵，而是將獎勵訊號轉換為文字批評，並將它們作為文字獎勵來反覆改善其回應。在涵蓋指令遵循、偏好對齊、安全性以及數學的基準測試評估中發現，TPO 逐漸改善了與人類偏好的對齊。值得注意的是，僅經過幾個 TPO 步驟後，最初未對齊的 Llama-3.1-70B-SFT 模型就可以超越對齊的對應模型 Llama-3.1-70B-Instruct。此外，TPO 在推理期間可以有效地擴充搜尋廣度和深度。透過案例研究，我們說明了 TPO 如何利用 LLM 解釋和根據獎勵訊號採取行動的內在能力。我們的發現將 TPO 定位為一種實用的、輕量級的測試時偏好最佳化替代方案，可即時實現對齊。我們的程式碼公開於 https://github.com/yafuly/TPO。

##### **Learning Graph Node Embeddings by Smooth Pair Sampling**
2501.12884v1 by Konstantin Kutzkov

Random walk-based node embedding algorithms have attracted a lot of attention
due to their scalability and ease of implementation. Previous research has
focused on different walk strategies, optimization objectives, and embedding
learning models. Inspired by observations on real data, we take a different
approach and propose a new regularization technique. More precisely, the
frequencies of node pairs generated by the skip-gram model on random walk node
sequences follow a highly skewed distribution which causes learning to be
dominated by a fraction of the pairs. We address the issue by designing an
efficient sampling procedure that generates node pairs according to their {\em
smoothed frequency}. Theoretical and experimental results demonstrate the
advantages of our approach.

摘要：基於隨機遊走的節點嵌入演算法由於其可擴充性和易於實作而備受關注。先前的研究專注於不同的遊走策略、最佳化目標和嵌入式學習模型。受到真實資料觀察的啟發，我們採取不同的方法並提出一個新的正則化技術。更精確地說，在隨機遊走節點序列上由跳躍語法模型產生的節點對頻率遵循一個高度偏斜的分布，這導致學習受到部分對的支配。我們透過設計一個有效率的抽樣程序來解決這個問題，該程序根據節點對的「平滑頻率」來產生節點對。理論和實驗結果證明了我們方法的優點。

##### **WisdomBot: Tuning Large Language Models with Artificial Intelligence Knowledge**
2501.12877v1 by Jingyuan Chen, Tao Wu, Wei Ji, Fei Wu

Large language models (LLMs) have emerged as powerful tools in natural
language processing (NLP), showing a promising future of artificial generated
intelligence (AGI). Despite their notable performance in the general domain,
LLMs have remained suboptimal in the field of education, owing to the unique
challenges presented by this domain, such as the need for more specialized
knowledge, the requirement for personalized learning experiences, and the
necessity for concise explanations of complex concepts. To address these
issues, this paper presents a novel LLM for education named WisdomBot, which
combines the power of LLMs with educational theories, enabling their seamless
integration into educational contexts. To be specific, we harness
self-instructed knowledge concepts and instructions under the guidance of
Bloom's Taxonomy as training data. To further enhance the accuracy and
professionalism of model's response on factual questions, we introduce two key
enhancements during inference, i.e., local knowledge base retrieval
augmentation and search engine retrieval augmentation during inference. We
substantiate the effectiveness of our approach by applying it to several
Chinese LLMs, thereby showcasing that the fine-tuned models can generate more
reliable and professional responses.

摘要：大型語言模型（LLM）已成為自然語言處理（NLP）中的強大工具，展示了人工智慧（AGI）的未來前景。儘管在一般領域有顯著的表現，但 LLM 在教育領域仍未達到最佳狀態，這是由於該領域提出的獨特挑戰，例如需要更多專業知識、個性化學習體驗的要求，以及對複雜概念的簡潔解釋。為了解決這些問題，本文提出了一個名為 WisdomBot 的教育 LLM，它結合了 LLM 的力量和教育理論，使其能夠無縫整合到教育環境中。具體來說，我們利用布魯姆分類法指導下的自學知識概念和說明作為訓練資料。為了進一步提高模型對事實問題的回應的準確性和專業性，我們在推理過程中引入了兩項關鍵的改進，即，局部知識庫檢索擴充和搜索引擎檢索擴充。我們通過將其應用於幾個中文 LLM 來驗證我們方法的有效性，從而展示微調模型可以產生更可靠和專業的回應。

##### **Mutation-Guided LLM-based Test Generation at Meta**
2501.12862v1 by Christopher Foster, Abhishek Gulati, Mark Harman, Inna Harper, Ke Mao, Jillian Ritchey, Hervé Robert, Shubho Sengupta

This paper describes Meta's ACH system for mutation-guided LLM-based test
generation. ACH generates relatively few mutants (aka simulated faults),
compared to traditional mutation testing. Instead, it focuses on generating
currently undetected faults that are specific to an issue of concern. From
these currently uncaught faults, ACH generates tests that can catch them,
thereby `killing' the mutants and consequently hardening the platform against
regressions. We use privacy concerns to illustrate our approach, but ACH can
harden code against {\em any} type of regression. In total, ACH was applied to
10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from
which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also
deploys an LLM-based equivalent mutant detection agent that achieves a
precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple
pre-processing). ACH was used by Messenger and WhatsApp test-a-thons where
engineers accepted 73% of its tests, judging 36% to privacy relevant. We
conclude that ACH hardens code against specific concerns and that, even when
its tests do not directly tackle the specific concern, engineers find them
useful for their other benefits.

摘要：本文描述了 Meta 的 ACH 系统，用于基于 LLM 的变异引导测试生成。与传统的变异测试相比，ACH 生成的变异体（又称模拟故障）相对较少。相反，它专注于生成特定于关注问题的当前未检测到的故障。从这些当前未捕获的故障中，ACH 生成了可以捕获它们的测试，从而“杀死”变异体，进而增强平台对回归的抵抗力。我们使用隐私问题来说明我们的方法，但 ACH 可以针对任何类型的回归强化代码。总而言之，ACH 已应用于 Meta 部署的 7 个软件平台中的 10,795 个 Android Kotlin 类，从中生成了 9,095 个变异体和 571 个隐私强化测试用例。ACH 还部署了基于 LLM 的等效变异体检测代理，其精度达到 0.79，召回率达到 0.47（使用简单的预处理后分别升至 0.95 和 0.96）。ACH 被 Messenger 和 WhatsApp 的测试马拉松使用，工程师接受了其 73% 的测试，其中 36% 被认为与隐私相关。我们得出的结论是，ACH 针对特定问题强化了代码，即使其测试没有直接解决特定问题，工程师也会发现它们对其他好处很有用。

##### **ACEBench: Who Wins the Match Point in Tool Learning?**
2501.12851v1 by Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu

Large language models (LLMs) have demonstrated significant potential in
decision-making and reasoning, especially when combined with various tools to
effectively solve complex problems. However, existing evaluation systems for
assessing LLM function calling capabilities have several limitations: (1)
limited evaluation scenarios, lacking assessments in real multi-turn dialogue
contexts; (2) narrow evaluation dimensions, lacking detailed assessments for
fine-grained function calls; (3) relying on LLMs or real API executions for
result evaluation, which introduces significant overhead. To address these
issues, we propose a comprehensive evaluation system named ACEBench. This
system is meticulously designed to encompass a wide spectrum of function
calling scenarios. Moreover, it categorizes these scenarios into three primary
types according to the evaluation methodology: Normal, Special, and Agent.
Normal evaluates function calls in basic scenarios; Special evaluates function
calls in scenarios with vague or incomplete instructions; Agent introduces
multi-agent interactions to simulate function calling evaluation in real-world
multi-turn interactions. We conducted extensive experiments on ACEBench,
analyzing various LLMs in-depth and performing a more granular analysis of
error causes across different data types.

摘要：大型語言模型 (LLM) 在決策和推理方面展現出顯著的潛力，特別是在結合各種工具以有效解決複雜問題時。然而，現有的評估系統在評估 LLM 函數呼叫能力時有幾個限制：(1) 評估場景有限，缺乏在真實多輪對話情境中的評估；(2) 評估維度狹窄，缺乏對細粒度函數呼叫的詳細評估；(3) 依賴 LLM 或真實 API 執行來進行結果評估，這會造成顯著的開銷。為了解決這些問題，我們提出了一個名為 ACEBench 的綜合評估系統。此系統經過精心設計，涵蓋了廣泛的函數呼叫場景。此外，它根據評估方法將這些場景分為三種類型：一般、特殊和代理。一般會在基本場景中評估函數呼叫；特殊會在具有模糊或不完整指令的場景中評估函數呼叫；代理會引入多代理互動，以模擬在真實世界多輪互動中的函數呼叫評估。我們在 ACEBench 上進行了廣泛的實驗，深入分析了各種 LLM，並對不同數據類型的錯誤原因進行了更細緻的分析。

##### **GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model for Multi-organ Segmentation**
2501.12844v1 by Ruicheng Zhang, Haowei Guo, Zeyu Zhang, Puxin Yan, Shen Zhao

Multi-organ segmentation is a critical yet challenging task due to complex
anatomical backgrounds, blurred boundaries, and diverse morphologies. This
study introduces the Gradient-aware Adaptive Momentum Evolution Deep Snake
(GAMED-Snake) model, which establishes a novel paradigm for contour-based
segmentation by integrating gradient-based learning with adaptive momentum
evolution mechanisms. The GAMED-Snake model incorporates three major
innovations: First, the Distance Energy Map Prior (DEMP) generates a
pixel-level force field that effectively attracts contour points towards the
true boundaries, even in scenarios with complex backgrounds and blurred edges.
Second, the Differential Convolution Inception Module (DCIM) precisely extracts
comprehensive energy gradients, significantly enhancing segmentation accuracy.
Third, the Adaptive Momentum Evolution Mechanism (AMEM) employs cross-attention
to establish dynamic features across different iterations of evolution,
enabling precise boundary alignment for diverse morphologies. Experimental
results on four challenging multi-organ segmentation datasets demonstrate that
GAMED-Snake improves the mDice metric by approximately 2% compared to
state-of-the-art methods. Code will be available at
https://github.com/SYSUzrc/GAMED-Snake.

摘要：多器官分割是一項關鍵且具有挑戰性的任務，其原因在於複雜的解剖學背景、模糊的邊界和多樣化的形態。本研究引入了梯度感知自適應動量演化深度蛇模型 (GAMED-Snake)，它建立了一個基於輪廓的分割新範例，方法是將基於梯度的學習與自適應動量演化機制整合在一起。GAMED-Snake 模型包含三大創新：首先，距離能量圖先驗 (DEMP) 會產生一個像素級力的場，它能有效地吸引輪廓點朝向真實的邊界，即使在背景複雜且邊緣模糊的情況下也能發揮作用。其次，差分卷積起始模組 (DCIM) 精確地提取了全面的能量梯度，顯著地提高了分割準確度。第三，自適應動量演化機制 (AMEM) 採用交叉注意來建立不同演化迭代之間的動態特徵，從而實現對不同形態的精確邊界對齊。在四個具有挑戰性的多器官分割資料集上的實驗結果表明，與最先進的方法相比，GAMED-Snake 將 mDice 指標提高了大約 2%。程式碼將在 https://github.com/SYSUzrc/GAMED-Snake 上提供。

##### **Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home**
2501.12835v1 by Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, Alexander Panchenko

Retrieval Augmented Generation (RAG) improves correctness of Question
Answering (QA) and addresses hallucinations in Large Language Models (LLMs),
yet greatly increase computational costs. Besides, RAG is not always needed as
may introduce irrelevant information. Recent adaptive retrieval methods
integrate LLMs' intrinsic knowledge with external information appealing to LLM
self-knowledge, but they often neglect efficiency evaluations and comparisons
with uncertainty estimation techniques. We bridge this gap by conducting a
comprehensive analysis of 35 adaptive retrieval methods, including 8 recent
approaches and 27 uncertainty estimation techniques, across 6 datasets using 10
metrics for QA performance, self-knowledge, and efficiency. Our findings show
that uncertainty estimation techniques often outperform complex pipelines in
terms of efficiency and self-knowledge, while maintaining comparable QA
performance.

摘要：檢索增強生成（RAG）改善了問題回答（QA）的正確性，並解決了大型語言模型（LLM）中的幻覺，但大幅增加了運算成本。此外，RAG 並非總是需要，因為可能會引入無關資訊。最近的自適應檢索方法將 LLM 的內在知識與外部資訊整合，吸引 LLM 的自我知識，但它們常常忽略效率評估和與不確定性估計技術的比較。我們透過對 35 種自適應檢索方法進行全面分析來彌補此差距，其中包括 8 種近期方法和 27 種不確定性估計技術，使用 10 種量度衡量 6 個資料集的 QA 效能、自我知識和效率。我們的研究結果顯示，不確定性估計技術在效率和自我知識方面通常優於複雜的管道，同時維持相當的 QA 效能。

##### **Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek**
2501.12826v1 by John Pavlopoulos, Juli Bakagianni, Kanella Pouli, Maria Gavriilidou

Natural Language Processing (NLP) for lesser-resourced languages faces
persistent challenges, including limited datasets, inherited biases from
high-resource languages, and the need for domain-specific solutions. This study
addresses these gaps for Modern Greek through three key contributions. First,
we evaluate the performance of open-source (Llama-70b) and closed-source
(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset
availability, revealing task-specific strengths, weaknesses, and parity in
their performance. Second, we expand the scope of Greek NLP by reframing
Authorship Attribution as a tool to assess potential data usage by LLMs in
pre-training, with high 0-shot accuracy suggesting ethical implications for
data provenance. Third, we showcase a legal NLP case study, where a Summarize,
Translate, and Embed (STE) methodology outperforms the traditional TF-IDF
approach for clustering \emph{long} legal texts. Together, these contributions
provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps
in model evaluation, task innovation, and real-world impact.

摘要：自然語言處理 (NLP) 針對資源較少的語言面臨持續的挑戰，包括資料集有限、繼承自資源豐富語言的偏見，以及對特定領域解決方案的需求。本研究透過三個關鍵貢獻解決現代希臘語的這些差距。首先，我們評估開源 (Llama-70b) 和閉源 (GPT-4o mini) 大型語言模型 (LLM) 在七項核心 NLP 任務上的效能，並提供資料集可用性，揭露其任務特定的優勢、劣勢和效能平價。其次，我們透過將作者歸因重新定義為一種評估 LLM 在預訓練中潛在資料使用情況的工具，擴展希臘語 NLP 的範圍，並以高 0 次學習準確度暗示資料來源的倫理意涵。第三，我們展示一個法律 NLP 案例研究，其中摘要、翻譯和嵌入 (STE) 方法優於傳統 TF-IDF 方法，用於分群\emph{長篇}法律文本。這些貢獻共同提供了一份路線圖，以推動資源較少語言中的 NLP，縮小模型評估、任務創新和現實世界影響之間的差距。

##### **Machine Learning Modeling for Multi-order Human Visual Motion Processing**
2501.12810v1 by Zitang Sun, Yen-Ju Chen, Yung-Hao Yang, Yuan Li, Shin'ya Nishida

Our research aims to develop machines that learn to perceive visual motion as
do humans. While recent advances in computer vision (CV) have enabled DNN-based
models to accurately estimate optical flow in naturalistic images, a
significant disparity remains between CV models and the biological visual
system in both architecture and behavior. This disparity includes humans'
ability to perceive the motion of higher-order image features (second-order
motion), which many CV models fail to capture because of their reliance on the
intensity conservation law. Our model architecture mimics the cortical V1-MT
motion processing pathway, utilizing a trainable motion energy sensor bank and
a recurrent graph network. Supervised learning employing diverse naturalistic
videos allows the model to replicate psychophysical and physiological findings
about first-order (luminance-based) motion perception. For second-order motion,
inspired by neuroscientific findings, the model includes an additional sensing
pathway with nonlinear preprocessing before motion energy sensing, implemented
using a simple multilayer 3D CNN block. When exploring how the brain acquired
the ability to perceive second-order motion in natural environments, in which
pure second-order signals are rare, we hypothesized that second-order
mechanisms were critical when estimating robust object motion amidst optical
fluctuations, such as highlights on glossy surfaces. We trained our
dual-pathway model on novel motion datasets with varying material properties of
moving objects. We found that training to estimate object motion from
non-Lambertian materials naturally endowed the model with the capacity to
perceive second-order motion, as can humans. The resulting model effectively
aligns with biological systems while generalizing to both first- and
second-order motion phenomena in natural scenes.

摘要：<paragraph>我們的研究旨在開發出能像人類一樣學習感知視覺運動的機器。儘管電腦視覺 (CV) 的最新進展已讓基於深度神經網路 (DNN) 的模型能準確估計自然影像中的光流，但 CV 模型與生物視覺系統在架構和行為上仍有顯著差異。這種差異包括人類感知高階影像特徵（二階運動）的能力，由於依賴強度守恆定律，許多 CV 模型無法捕捉到這一點。我們的模型架構模擬了皮質 V1-MT 運動處理路徑，利用可訓練的運動能量感測器組和遞迴圖形網路。採用多樣化自然影片的監督式學習，讓模型能夠複製關於一階（基於亮度）運動感知的心理物理學和生理學發現。對於二階運動，受神經科學發現的啟發，該模型包含了一個額外的感測路徑，在運動能量感測之前進行非線性預處理，並使用簡單的多層 3D CNN 塊進行實作。在探討大腦如何獲得在自然環境中感知二階運動的能力時，其中純二階訊號很少見，我們假設在估計光學波動中的穩健物體運動時，二階機制至關重要，例如光滑表面上的亮點。我們在具有不同移動物體材料屬性的新運動資料集上訓練了我們的雙路徑模型。我們發現，從非朗伯體材料估計物體運動的訓練自然而然地賦予了模型感知二階運動的能力，就像人類一樣。由此產生的模型有效地與生物系統保持一致，同時推廣到自然場景中的一階和二階運動現象。</paragraph>

##### **Revisit Self-Debugging with Self-Generated Tests for Code Generation**
2501.12793v1 by Xiancai Chen, Zhengwei Tao, Kechi Zhang, Changzhi Zhou, Wanli Gu, Yuanpeng He, Mengdi Zhang, Xunliang Cai, Haiyan Zhao, Zhi Jin

Large language models (LLMs) have shown significant advancements in code
generation, but still face challenges on tasks beyond their basic capabilities.
Recently, the notion of self-debugging has been proposed to boost the
performance of code generation by leveraging execution feedback from tests.
Despite its promise, the availability of high-quality tests in real-world
scenarios is limited. In this context, self-debugging with self-generated tests
is a promising solution but lacks a full exploration of its limitations and
practical potential. Therefore, we investigate its efficacy on diverse
programming problems. To deepen our understanding, we propose two distinct
paradigms for the process: post-execution and in-execution self-debugging.
Within the scope of self-contained Python programming tasks, we find that
post-execution self-debugging struggles on basic problems but shows potential
for improvement on competitive ones, due to the bias introduced by
self-generated tests. On the other hand, in-execution self-debugging enables
LLMs to mitigate the bias by solely leveraging intermediate states during
execution, thereby enhancing code generation.

摘要：大型語言模型 (LLM) 在程式碼生成方面展現出顯著的進步，但仍面臨超出其基本能力的任務挑戰。最近，已提出自我除錯的概念，藉由利用測試的執行回饋來提升程式碼生成的效能。儘管前景看好，但在實際情況中，高品質測試的取得受到限制。在此背景下，使用自我產生的測試進行自我除錯是一個有前景的解決方案，但缺乏對其限制和實際潛力的完整探討。因此，我們研究其在不同程式設計問題上的效能。為了加深我們的理解，我們為此程序提出兩種不同的典範：執行後和執行中自我除錯。在獨立的 Python 程式設計任務範圍內，我們發現執行後自我除錯在基本問題上會遇到困難，但在競爭激烈的問題上顯示出改進的潛力，這歸因於自我產生的測試所帶來的偏差。另一方面，執行中自我除錯使 LLM 能夠僅利用執行期間的中間狀態來減輕偏差，從而增強程式碼生成。

##### **Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana**
2501.12789v1 by Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan, Yoelle Maarek

Evaluating Retrieval-Augmented Generation (RAG) systems, especially in
domain-specific contexts, requires benchmarks that address the distinctive
requirements of the applicative scenario. Since real data can be hard to
obtain, a common strategy is to use LLM-based methods to generate synthetic
data. Existing solutions are general purpose: given a document, they generate a
question to build a Q&A pair. However, although the generated questions can be
individually good, they are typically not diverse enough to reasonably cover
the different ways real end-users can interact with the RAG system. We
introduce here DataMorgana, a tool for generating highly customizable and
diverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana
enables detailed configurations of user and question categories and provides
control over their distribution within the benchmark. It uses a lightweight
two-stage process, ensuring efficiency and fast iterations, while generating
benchmarks that reflect the expected traffic. We conduct a thorough line of
experiments, showing quantitatively and qualitatively that DataMorgana
surpasses existing tools and approaches in producing lexically, syntactically,
and semantically diverse question sets across domain-specific and
general-knowledge corpora. DataMorgana will be made available to selected teams
in the research community, as first beta testers, in the context of the
upcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025.

摘要：針對檢索增強生成 (RAG) 系統進行評估，特別是在特定領域的脈絡中，需要基準來滿足應用場景的獨特需求。由於真實資料可能難以取得，因此常見的策略是使用 LLM 為基礎的方法來生成合成資料。現有的解決方案是通用的：給定文件後，它們會產生一個問題來建立問答配對。然而，儘管產生的問題在個別上可能很好，但通常不夠多元化，無法合理涵蓋實際最終使用者與 RAG 系統互動的不同方式。我們在此介紹 DataMorgana，這是一個用於生成高度自訂化且多元化的合成問答基準，專門針對 RAG 應用。DataMorgana 能夠詳細設定使用者和問題類別，並提供對基準中其分佈的控制。它使用輕量化的兩階段流程，確保效率和快速迭代，同時產生反映預期流量的基準。我們進行了一系列徹底的實驗，定量和定性地顯示 DataMorgana 在產生跨特定領域和一般知識語料庫的詞彙、句法和語義多元化問題集方面勝過現有的工具和方法。DataMorgana 將提供給研究社群中選定的團隊，作為第一批測試人員，在 2025 年 2 月初宣布的 SIGIR'2025 LiveRAG 挑戰中使用。

##### **Data re-uploading in Quantum Machine Learning for time series: application to traffic forecasting**
2501.12776v1 by Nikolaos Schetakis, Paolo Bonfini, Negin Alisoltani, Konstantinos Blazakis, Symeon I. Tsintzos, Alexis Askitopoulos, Davit Aghamalyan, Panagiotis Fafoutellis, Eleni I. Vlahogianni

Accurate traffic forecasting plays a crucial role in modern Intelligent
Transportation Systems (ITS), as it enables real-time traffic flow management,
reduces congestion, and improves the overall efficiency of urban transportation
networks. With the rise of Quantum Machine Learning (QML), it has emerged a new
paradigm possessing the potential to enhance predictive capabilities beyond
what classical machine learning models can achieve. In the present work we
pursue a heuristic approach to explore the potential of QML, and focus on a
specific transport issue. In particular, as a case study we investigate a
traffic forecast task for a major urban area in Athens (Greece), for which we
possess high-resolution data. In this endeavor we explore the application of
Quantum Neural Networks (QNN), and, notably, we present the first application
of quantum data re-uploading in the context of transport forecasting. This
technique allows quantum models to better capture complex patterns, such as
traffic dynamics, by repeatedly encoding classical data into a quantum state.
Aside from providing a prediction model, we spend considerable effort in
comparing the performance of our hybrid quantum-classical neural networks with
classical deep learning approaches. Our results show that hybrid models achieve
competitive accuracy with state-of-the-art classical methods, especially when
the number of qubits and re-uploading blocks is increased. While the classical
models demonstrate lower computational demands, we provide evidence that
increasing the complexity of the quantum model improves predictive accuracy.
These findings indicate that QML techniques, and specifically the data
re-uploading approach, hold promise for advancing traffic forecasting models
and could be instrumental in addressing challenges inherent in ITS
environments.

摘要：準確的交通預測在現代智慧運輸系統 (ITS) 中扮演著至關重要的角色，因為它能進行即時的交通流量管理、減少壅塞，並提升整體的城市交通運輸網絡效率。隨著量子機器學習 (QML) 的興起，出現了一種新的典範，它具備超越傳統機器學習模型所能達到的預測能力的潛力。在目前的工作中，我們採用啟發式方法來探索 QML 的潛力，並專注於特定的交通問題。特別是，作為一個案例研究，我們調查了雅典（希臘）一個主要都市地區的交通預測任務，我們擁有該地區的高解析度資料。在這個努力中，我們探討了量子神經網路 (QNN) 的應用，而且特別是，我們展示了量子資料重新上傳在交通預測脈絡中的首次應用。此技術讓量子模型能透過重複將傳統資料編碼成量子態，來更好地擷取複雜模式，例如交通動態。除了提供一個預測模型之外，我們花費了相當大的精力來比較我們的混合量子-傳統神經網路與傳統深度學習方法的效能。我們的結果顯示，混合模型能達到與最先進的傳統方法相媲美的準確度，特別是在量子位元和重新上傳區塊的數量增加時。儘管傳統模型展現出較低的運算需求，我們提供了證據證明增加量子模型的複雜度會提升預測準確度。這些發現表明，QML 技術，特別是資料重新上傳方法，有望推進交通預測模型，並可能有助於解決 ITS 環境中固有的挑戰。

##### **Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation**
2501.12775v1 by Duc Hau Nguyen, Cyrielle Mallart, Guillaume Gravier, Pascale Sébillot

Attention mechanism is contributing to the majority of recent advances in
machine learning for natural language processing. Additionally, it results in
an attention map that shows the proportional influence of each input in its
decision. Empirical studies postulate that attention maps can be provided as an
explanation for model output. However, it is still questionable to ask whether
this explanation helps regular people to understand and accept the model output
(the plausibility of the explanation). Recent studies show that attention
weights in the RNN encoders are hardly plausible because they spread on input
tokens. We thus propose 3 additional constraints to the learning objective
function to improve the plausibility of the attention map: regularization to
increase the attention weight sparsity, semi-supervision to supervise the map
by a heuristic and supervision by human annotation. Results show that all
techniques can improve the attention map plausibility at some level. We also
observe that specific instructions for human annotation might have a negative
effect on classification performance. Beyond the attention map, the result of
experiments on text classification tasks also shows that no matter how the
constraint brings the gain, the contextualization layer plays a crucial role in
finding the right space for finding plausible tokens.

摘要：注意力机制促成了自然語言處理機器學習最近的大部分進展。此外，它會產生一個注意力圖，顯示每個輸入在其決策中所佔的比例影響。實證研究假設注意力圖可以作為模型輸出的解釋。然而，是否這種解釋有助於一般人理解並接受模型輸出（解釋的合理性）仍有待商榷。最近的研究表明，RNN 編碼器中的注意力權重幾乎不合理，因為它們散佈在輸入符號上。因此，我們對學習目標函數提出了 3 個額外的約束，以提高注意力圖的合理性：正則化以增加注意力權重稀疏性、半監督以通過啟發式監督圖以及通過人工註解進行監督。結果表明，所有技術都可以在某種程度上提高注意力圖的合理性。我們還觀察到，人工註解的具體說明可能會對分類性能產生負面影響。除了注意力圖之外，文本分類任務的實驗結果還表明，無論約束如何帶來收益，情境化層在找到合理符號的正確空間中都發揮著至關重要的作用。

##### **LLMs as Repositories of Factual Knowledge: Limitations and Solutions**
2501.12774v1 by Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi

LLMs' sources of knowledge are data snapshots containing factual information
about entities collected at different timestamps and from different media types
(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to
change due to updates through time from past to present. Equally important are
the inconsistencies and inaccuracies occurring in different information
sources. Consequently, the model's knowledge about an entity may be perturbed
while training over the sequence of snapshots or at inference time, resulting
in inconsistent and inaccurate model performance. In this work, we study the
appropriateness of Large Language Models (LLMs) as repositories of factual
knowledge. We consider twenty-four state-of-the-art LLMs that are either
closed-, partially (weights), or fully (weight and training data) open-source.
We evaluate their reliability in responding to time-sensitive factual questions
in terms of accuracy and consistency when prompts are perturbed. We further
evaluate the effectiveness of state-of-the-art methods to improve LLMs'
accuracy and consistency. We then propose "ENtity-Aware Fine-tuning" (ENAF), a
soft neurosymbolic approach aimed at providing a structured representation of
entities during fine-tuning to improve the model's performance.

摘要：大型語言模型的知識來源是資料快照，其中包含在不同時間戳和不同媒體類型（例如 wiki、社群媒體等）收集的關於實體的真實資訊。此類非結構化知識會隨著時間從過去到現在的更新而改變。同樣重要的是，不同資訊來源中出現的不一致性和不準確性。因此，模型在快照序列或推理時間上進行訓練時，對實體的知識可能會受到干擾，導致模型效能不一致且不準確。在這項工作中，我們研究大型語言模型 (LLM) 作為事實知識儲存庫的適當性。我們考慮了 24 個最先進的 LLM，它們可能是封閉、部分（權重）或完全（權重和訓練資料）開源的。我們評估了它們在提示受到干擾時對時間敏感的事實問題的回應的可靠性，包括準確性和一致性。我們進一步評估了最先進的方法在提高 LLM 的準確性和一致性方面的有效性。然後我們提出「實體感知微調」(ENAF)，這是一種軟神經符號方法，旨在在微調過程中提供實體的結構化表示，以改善模型的效能。

##### **NExtLong: Toward Effective Long-Context Training without Long Documents**
2501.12766v1 by Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu

Large language models (LLMs) with extended context windows have made
significant strides yet remain a challenge due to the scarcity of long
documents. Existing methods tend to synthesize long-context data but lack a
clear mechanism to reinforce the long-range dependency modeling. To address
this limitation, we propose NExtLong, a novel framework for synthesizing
long-context data through Negative document Extension. NExtLong decomposes a
document into multiple meta-chunks and extends the context by interleaving hard
negative distractors retrieved from pretraining corpora. This approach compels
the model to discriminate long-range dependent context from distracting
content, enhancing its ability to model long-range dependencies. Extensive
experiments demonstrate that NExtLong achieves significant performance
improvements on the HELMET and RULER benchmarks compared to existing
long-context synthesis approaches and leading models, which are trained on
non-synthetic long documents. These findings highlight NExtLong's ability to
reduce reliance on non-synthetic long documents, making it an effective
framework for developing advanced long-context LLMs.

摘要：大型語言模型 (LLM) 具有延伸的上下文窗口，已取得重大進展，但由於缺乏長篇文件，因此仍然是一個挑戰。現有方法傾向於合成長語境資料，但缺乏明確的機制來加強長程依賴性建模。為了解決這個限制，我們提出了 NExtLong，這是一個通過負面文件擴展來合成長語境資料的新穎框架。NExtLong 將文件分解為多個元塊，並通過交錯從預訓練語料庫中檢索到的硬負面干擾項來擴展上下文。這種方法迫使模型區分長程依賴上下文和干擾內容，增強其建模長程依賴性的能力。廣泛的實驗表明，與現有的長語境合成方法和在非合成長文件中訓練的領先模型相比，NExtLong 在 HELMET 和 RULER 基準上取得了顯著的性能提升。這些發現突顯了 NExtLong 降低對非合成長文件的依賴性的能力，使其成為開發先進長語境 LLM 的有效框架。

##### **Estimating the Conformal Prediction Threshold from Noisy Labels**
2501.12749v1 by Coby Penso, Jacob Goldberger, Ethan Fetaya

Conformal Prediction (CP) is a method to control prediction uncertainty by
producing a small prediction set, ensuring a predetermined probability that the
true class lies within this set. This is commonly done by defining a score,
based on the model predictions, and setting a threshold on this score using a
validation set. In this study, we address the problem of CP calibration when we
only have access to a validation set with noisy labels. We show how we can
estimate the noise-free conformal threshold based on the noisy labeled data.
Our solution is flexible and can accommodate various modeling assumptions
regarding the label contamination process, without needing any information
about the underlying data distribution or the internal mechanisms of the
machine learning classifier. We develop a coverage guarantee for uniform noise
that is effective even in tasks with a large number of classes. We dub our
approach Noise-Aware Conformal Prediction (NACP) and show on several natural
and medical image classification datasets, including ImageNet, that it
significantly outperforms current noisy label methods and achieves results
comparable to those obtained with a clean validation set.

摘要：共形预测 (CP) 是一種透過產生一個小型預測集合來控制預測不確定性的方法，確保真正的類別落在這個集合內的預先確定的機率。這通常是透過定義一個基於模型預測的分數來完成，並使用驗證集合對這個分數設定一個閾值。在本研究中，我們探討了當我們只能存取具有雜訊標籤的驗證集合時，CP 校正的問題。我們展示了如何根據雜訊標籤資料估計無雜訊的共形閾值。我們的解決方案具有彈性，並且可以適應關於標籤污染過程的各種建模假設，而不需要任何關於底層資料分佈或機器學習分類器內部機制的資訊。我們開發了一個對於均勻雜訊的覆蓋保證，即使在具有大量類別的任務中也很有效。我們將我們的做法稱為雜訊感知共形預測 (NACP)，並在幾個自然和醫學影像分類資料集（包括 ImageNet）上展示了它顯著優於目前的雜訊標籤方法，並且達到了與使用乾淨驗證集合獲得的結果相當的結果。

##### **EvidenceMap: Unleashing the Power of Small Language Models with Evidence Analysis for Biomedical Question Answering**
2501.12746v1 by Chang Zong, Jian Wan, Lei Zhang

Current LLM-based approaches improve question answering performance by
leveraging the internal reasoning abilities of models or incorporating external
knowledge. However, when humans address professional problems, it is essential
to explicitly analyze the multifaceted relationships from multiple pieces and
diverse sources of evidence to achieve better answers. In this study, we
propose a novel generative question answering framework for the biomedical
domain, named EvidenceMap, which explicitly learns and incorporates evidence
analysis with small language models (SLMs). The framework describes an evidence
map for each question and fully utilizes an SLM to derive the representation of
the supportive evaluation, the logical correlation, and the summarization of
the related evidence, which facilitates an analysis-augmented generation with
another SLM in an autoregressive way. Extensive experiments have shown that
introducing an evidence analysis learning process can significantly outperform
larger models and popular LLM reasoning methods.

摘要：現有的 LLM 方法透過利用模型的內部推理能力或整合外部知識來提升問題解答效能。然而，當人類面對專業問題時，必須明確分析來自多個片段和不同證據來源的多面向關係，才能獲得更好的解答。在本研究中，我們針對生物醫學領域提出一個新穎的生成式問題解答架構，名為 EvidenceMap，此架構明確學習並整合證據分析與小型語言模型 (SLM)。此架構會為每個問題描述一個證據映射，並充分利用 SLM 來衍生支持性評估、邏輯關聯和相關證據的摘要，這有助於以自迴歸方式使用另一個 SLM 進行分析增強生成。廣泛的實驗顯示，引入證據分析學習流程可以顯著優於較大的模型和流行的 LLM 推理方法。

##### **A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering**
2501.12728v1 by Matteo Esposito, Mikel Robredo, Murali Sridharan, Guilherme Horta Travassos, Rafael Peñaloza, Valentina Lenarduzzi

Context: Empirical Software Engineering (ESE) drives innovation in SE through
qualitative and quantitative studies. However, concerns about the correct
application of empirical methodologies have existed since the 2006 Dagstuhl
seminar on SE. Objective: To analyze three decades of SE research, identify
mistakes in statistical methods, and evaluate experts' ability to detect and
address these issues. Methods: We conducted a literature survey of ~27,000
empirical studies, using LLMs to classify statistical methodologies as adequate
or inadequate. Additionally, we selected 30 primary studies and held a workshop
with 33 ESE experts to assess their ability to identify and resolve statistical
issues. Results: Significant statistical issues were found in the primary
studies, and experts showed limited ability to detect and correct these
methodological problems, raising concerns about the broader ESE community's
proficiency in this area. Conclusions. Despite our study's eventual
limitations, its results shed light on recurring issues from promoting
information copy-and-paste from past authors' works and the continuous
publication of inadequate approaches that promote dubious results and
jeopardize the spread of the correct statistical strategies among researchers.
Besides, it justifies further investigation into empirical rigor in software
engineering to expose these recurring issues and establish a framework for
reassessing our field's foundation of statistical methodology application.
Therefore, this work calls for critically rethinking and reforming data
analysis in empirical software engineering, paving the way for our work soon.

摘要：<paragraph>脈絡：經驗軟體工程（ESE）透過質化和量化研究推動軟體工程（SE）的創新。然而，自 2006 年達格斯圖爾研討會關於 SE 以來，對於經驗方法正確應用的疑慮一直存在。目標：分析三十年的 SE 研究，找出統計方法中的錯誤，並評估專家發現和解決這些問題的能力。方法：我們對約 27,000 項經驗研究進行文獻調查，使用 LLM 將統計方法分類為適當或不適當。此外，我們挑選了 30 項主要研究，並舉辦了一場工作坊，邀請 33 位 ESE 專家評估他們找出和解決統計問題的能力。結果：在主要研究中發現有顯著的統計問題，而專家展現出有限的能力來發現和修正這些方法學問題，這引發了對於更廣泛的 ESE 社群在此領域的熟練度疑慮。結論：儘管我們的研究最終有其限制，但其結果揭露了反覆出現的問題，包括從過去作者的作品中推廣資訊複製貼上，以及持續發表不適當的方法，這些方法會推廣可疑的結果，並危害正確的統計策略在研究人員之間的傳播。此外，這證明了進一步調查軟體工程中的經驗嚴謹性是合理的，以揭露這些反覆出現的問題，並建立一個架構來重新評估我們領域的統計方法應用基礎。因此，這項工作呼籲批判性地重新思考和改革經驗軟體工程中的資料分析，為我們的工作在不久的將來鋪路。</paragraph>

##### **Practical quantum federated learning and its experimental demonstration**
2501.12709v1 by Zhi-Ping Liu, Xiao-Yu Cao, Hao-Wen Liu, Xiao-Ran Sun, Yu Bao, Yu-Shuo Lu, Hua-Lei Yin, Zeng-Bing Chen

Federated learning is essential for decentralized, privacy-preserving model
training in the data-driven era. Quantum-enhanced federated learning leverages
quantum resources to address privacy and scalability challenges, offering
security and efficiency advantages beyond classical methods. However, practical
and scalable frameworks addressing privacy concerns in the quantum computing
era remain undeveloped. Here, we propose a practical quantum federated learning
framework on quantum networks, utilizing distributed quantum secret keys to
protect local model updates and enable secure aggregation with
information-theoretic security. We experimentally validate our framework on a
4-client quantum network with a scalable structure. Extensive numerical
experiments on both quantum and classical datasets show that adding a quantum
client significantly enhances the trained global model's ability to classify
multipartite entangled and non-stabilizer quantum datasets. Simulations further
demonstrate scalability to 200 clients with classical models trained on the
MNIST dataset, reducing communication costs by $75\%$ through advanced model
compression techniques and achieving rapid training convergence. Our work
provides critical insights for building scalable, efficient, and quantum-secure
machine learning systems for the coming quantum internet era.

摘要：聯邦學習對於資料驅動時代的分散式、隱私保護模型訓練至關重要。量子增強聯邦學習利用量子資源來解決隱私和可擴充性挑戰，提供超越傳統方法的安全性和效率優勢。然而，在量子運算時代解決隱私問題的實用且可擴充性的框架仍未開發。在此，我們提出一個在量子網路上的實用量子聯邦學習框架，利用分散式量子密鑰來保護局部模型更新，並啟用具有資訊理論安全性的安全聚合。我們在具有可擴充性結構的 4 個用戶端量子網路中，以實驗方式驗證我們的框架。在量子和經典資料集上的大量數值實驗顯示，加入一個量子用戶端顯著增強了已訓練的全球模型對多方糾纏和非穩定器量子資料集進行分類的能力。模擬進一步證明了對 200 個用戶端的可擴充性，這些用戶端使用在 MNIST 資料集上訓練的經典模型，透過進階模型壓縮技術降低了 75% 的通訊成本，並實現了快速的訓練收斂。我們的研究為建構可擴充、高效且量子安全的機器學習系統，以迎接即將到來的量子網際網路時代，提供了重要的見解。

##### **Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression**
2501.12698v1 by Kai Yoshida, Masahiro Mizukami, Seiya Kawano, Canasai Kruengkrai, Hiroaki Sugiyama, Koichiro Yoshino

To improve user engagement during conversations with dialogue systems, we
must improve individual dialogue responses and dialogue impressions such as
consistency, personality, and empathy throughout the entire dialogue. While
such dialogue systems have been developing rapidly with the help of large
language models (LLMs), reinforcement learning from AI feedback (RLAIF) has
attracted attention to align LLM-based dialogue models for such dialogue
impressions. In RLAIF, a reward model based on another LLM is used to create a
training signal for an LLM-based dialogue model using zero-shot/few-shot
prompting techniques. However, evaluating an entire dialogue only by prompting
LLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs
prepared reward models corresponding to 12 metrics related to the impression of
the entire dialogue for evaluating dialogue responses. We tuned our dialogue
models using the reward model signals as feedback to improve the impression of
the system. The results of automatic and human evaluations showed that tuning
the dialogue model using our reward model corresponding to dialogue impression
improved the evaluation of individual metrics and the naturalness of the
dialogue response.

摘要：為了在對話系統對話期間提升使用者的參與度，我們必須改善個別對話回應以及對話印象，例如在整個對話中的一致性、個性與同理心。雖然此類對話系統在大型語言模型 (LLM) 的協助下快速發展，但來自 AI 回饋的強化學習 (RLAIF) 已引起注意，可調整基於 LLM 的對話模型以獲得此類對話印象。在 RLAIF 中，基於另一個 LLM 的獎勵模型用於使用零次/少次提示技術為基於 LLM 的對話模型建立訓練訊號。不過，僅透過提示 LLM 就評估整個對話具有挑戰性。在這項研究中，LLM 的監督微調 (SFT) 準備了與 12 項指標對應的獎勵模型，這些指標與評估對話回應的對話整體印象有關。我們使用獎勵模型訊號作為回饋微調我們的對話模型，以改善系統的印象。自動和人工評估結果顯示，使用與對話印象對應的獎勵模型微調對話模型，改善了個別指標的評估以及對話回應的自然性。

##### **Growth strategies for arbitrary DAG neural architectures**
2501.12690v1 by Stella Douka, Manon Verbockhaven, Théo Rudkiewicz, Stéphane Rivaud, François P Landes, Sylvain Chevallier, Guillaume Charpiat

Deep learning has shown impressive results obtained at the cost of training
huge neural networks. However, the larger the architecture, the higher the
computational, financial, and environmental costs during training and
inference. We aim at reducing both training and inference durations. We focus
on Neural Architecture Growth, which can increase the size of a small model
when needed, directly during training using information from the
backpropagation. We expand existing work and freely grow neural networks in the
form of any Directed Acyclic Graph by reducing expressivity bottlenecks in the
architecture. We explore strategies to reduce excessive computations and steer
network growth toward more parameter-efficient architectures.

摘要：深度學習已展現驚人的成果，但代價是訓練龐大的神經網路。然而，架構越大，在訓練和推論期間的運算、財務和環境成本就越高。我們的目標是縮短訓練和推論時間。我們專注於神經架構成長，它可以在需要時直接在訓練期間使用反向傳播中的資訊來增加小型模型的大小。我們擴展現有工作，並以有向無環圖的形式自由地發展神經網路，方法是減少架構中的表達能力瓶頸。我們探索策略以減少過多的運算，並引導網路成長朝向更具參數效率的架構。

##### **Extracting General-use Transformers for Low-resource Languages via Knowledge Distillation**
2501.12660v1 by Jan Christian Blaise Cruz, Alham Fikri Aji

In this paper, we propose the use of simple knowledge distillation to produce
smaller and more efficient single-language transformers from Massively
Multilingual Transformers (MMTs) to alleviate tradeoffs associated with the use
of such in low-resource settings. Using Tagalog as a case study, we show that
these smaller single-language models perform on-par with strong baselines in a
variety of benchmark tasks in a much more efficient manner. Furthermore, we
investigate additional steps during the distillation process that improves the
soft-supervision of the target language, and provide a number of analyses and
ablations to show the efficacy of the proposed method.

摘要：在本文中，我們建議使用簡單的知識蒸餾，從大型多語言轉換器 (MMT) 中產生更小、更有效率的單一語言轉換器，以減輕在資源不足的環境中使用此類轉換器相關的權衡。以他加祿語為例，我們表明這些較小的單一語言模型在各種基準任務中以更有效率的方式執行與強大基準線相同的操作。此外，我們在蒸餾過程中研究了改進目標語言軟監督的附加步驟，並提供了一些分析和消融，以顯示所提出方法的功效。

##### **The potential -- and the pitfalls -- of using pre-trained language models as cognitive science theories**
2501.12651v1 by Raj Sanjay Shah, Sashank Varma

Many studies have evaluated the cognitive alignment of Pre-trained Language
Models (PLMs), i.e., their correspondence to adult performance across a range
of cognitive domains. Recently, the focus has expanded to the developmental
alignment of these models: identifying phases during training where
improvements in model performance track improvements in children's thinking
over development. However, there are many challenges to the use of PLMs as
cognitive science theories, including different architectures, different
training data modalities and scales, and limited model interpretability. In
this paper, we distill lessons learned from treating PLMs, not as engineering
artifacts but as cognitive science and developmental science models. We review
assumptions used by researchers to map measures of PLM performance to measures
of human performance. We identify potential pitfalls of this approach to
understanding human thinking, and we end by enumerating criteria for using PLMs
as credible accounts of cognition and cognitive development.

摘要：許多研究評估了預訓練語言模型 (PLM) 的認知對齊，即它們在各種認知領域中與成人表現的對應關係。最近，焦點已擴展到這些模型的發展對齊：識別訓練期間的階段，其中模型表現的進步追蹤兒童思維在發展過程中的進步。然而，將 PLM 用作認知科學理論存在許多挑戰，包括不同的架構、不同的訓練數據模式和規模，以及有限的模型可解釋性。在本文中，我們從將 PLM 視為認知科學和發展科學模型，而不是工程人工製品中吸取教訓。我們回顧了研究人員用於將 PLM 效能測量對應到人類效能測量的假設。我們識別了這種理解人類思維方法的潛在缺陷，並最後列舉了將 PLM 用作認知和認知發展的可信說明的標準。

##### **Dynamics of Toxicity in Political Podcasts**
2501.12640v1 by Naquee Rizwan, Nayandeep Deb, Sarthak Roy, Vishwajeet Singh Solanki, Kiran Garimella, Animesh Mukherjee

Toxicity in digital media poses significant challenges, yet little attention
has been given to its dynamics within the rapidly growing medium of podcasts.
This paper addresses this gap by analyzing political podcast data to study the
emergence and propagation of toxicity, focusing on conversation
chains-structured reply patterns within podcast transcripts. Leveraging
state-of-the-art transcription models and advanced conversational analysis
techniques, we systematically examine toxic discourse in over 30 popular
political podcasts in the United States. Our key contributions include: (1)
creating a comprehensive dataset of transcribed and diarized political
podcasts, identifying thousands of toxic instances using Google's Perspective
API, (2) uncovering concerning trends where a majority of episodes contain at
least one toxic instance, (3) introducing toxic conversation chains and
analyzing their structural and linguistic properties, revealing characteristics
such as longer durations, repetitive patterns, figurative language, and
emotional cues tied to anger and annoyance, (4) identifying demand-related
words like 'want', 'like', and 'know' as precursors to toxicity, and (5)
developing predictive models to anticipate toxicity shifts based on annotated
change points. Our findings provide critical insights into podcast toxicity and
establish a foundation for future research on real-time monitoring and
intervention mechanisms to foster healthier discourse in this influential
medium.

摘要：數位媒體中的毒性帶來了重大的挑戰，但對於在快速成長的播客媒體中其動態性卻鮮少受到關注。本文透過分析政治播客資料來探討這個差距，研究毒性的出現與傳播，特別專注於播客記錄中的對話鏈——結構化的回覆模式。運用最先進的轉錄模型和進階的對話分析技術，我們系統性地檢視美國超過 30 個熱門政治播客中的有毒言論。我們的關鍵貢獻包括：(1) 建立一個包含轉錄和日記化政治播客的綜合資料集，使用 Google 的 Perspective API 找出數千個有毒個案，(2) 揭露令人擔憂的趨勢，其中大部分集數都至少包含一個有毒個案，(3) 提出有毒對話鏈並分析其結構和語言特性，揭露諸如較長持續時間、重複模式、比喻語言，以及與憤怒和煩惱相關的情緒線索等特徵，(4) 找出與需求相關的字詞，例如「想要」、「喜歡」和「知道」，作為毒性的前兆，以及 (5) 根據註解的變更點開發預測模型，以預測毒性的轉變。我們的研究結果提供了對播客毒性的重要見解，並為未來在這個有影響力的媒體中建立即時監控和介入機制的相關研究奠定基礎，以促進更健康的論述。

##### **Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors**
2501.12633v1 by Jingyang Ke, Feiyang Wu, Jiyi Wang, Jeffrey Markowitz, Anqi Wu

Traditional approaches to studying decision-making in neuroscience focus on
simplified behavioral tasks where animals perform repetitive, stereotyped
actions to receive explicit rewards. While informative, these methods constrain
our understanding of decision-making to short timescale behaviors driven by
explicit goals. In natural environments, animals exhibit more complex,
long-term behaviors driven by intrinsic motivations that are often
unobservable. Recent works in time-varying inverse reinforcement learning (IRL)
aim to capture shifting motivations in long-term, freely moving behaviors.
However, a crucial challenge remains: animals make decisions based on their
history, not just their current state. To address this, we introduce SWIRL
(SWitching IRL), a novel framework that extends traditional IRL by
incorporating time-varying, history-dependent reward functions. SWIRL models
long behavioral sequences as transitions between short-term decision-making
processes, each governed by a unique reward function. SWIRL incorporates
biologically plausible history dependency to capture how past decisions and
environmental contexts shape behavior, offering a more accurate description of
animal decision-making. We apply SWIRL to simulated and real-world animal
behavior datasets and show that it outperforms models lacking history
dependency, both quantitatively and qualitatively. This work presents the first
IRL model to incorporate history-dependent policies and rewards to advance our
understanding of complex, naturalistic decision-making in animals.

摘要：傳統研究神經科學中決策制定方法，專注於簡化行為任務，讓動物執行重複、刻板的動作以獲得明確獎勵。這些方法雖然提供資訊，但會將我們對決策制定的理解限制在受明確目標驅動的短期行為。在自然環境中，動物表現出更複雜、長期的行為，這些行為是由通常無法觀察到的內在動機所驅動。時間變異反向強化學習 (IRL) 的近期研究旨在捕捉長期、自由移動行為中不斷變化的動機。然而，一個關鍵挑戰仍然存在：動物根據其歷史做出決定，而不仅仅是其當前狀態。為了解決這個問題，我們引入了 SWIRL（SWitching IRL），這是一個新的框架，透過整合時變、與歷史相關的獎勵函數來擴充傳統的 IRL。SWIRL 模型將長行為序列建模為短期決策制定過程之間的轉換，每個過程都由一個獨特的獎勵函數所控制。SWIRL 結合了生物學上合理的歷史依賴性，以捕捉過去的決定和環境背景如何影響行為，提供動物決策制定更準確的描述。我們將 SWIRL 應用於模擬和真實世界的動物行為數據集，並證明它在量化和質化方面都優於缺乏歷史依賴性的模型。這項工作提出了第一個結合與歷史相關的策略和獎勵的 IRL 模型，以增進我們對動物複雜、自然決策制定的理解。

##### **Towards Robust Multi-tab Website Fingerprinting**
2501.12622v1 by Xinhao Deng, Xiyuan Zhao, Qilei Yin, Zhuotao Liu, Qi Li, Mingwei Xu, Ke Xu, Jianping Wu

Website fingerprinting enables an eavesdropper to determine which websites a
user is visiting over an encrypted connection. State-of-the-art website
fingerprinting (WF) attacks have demonstrated effectiveness even against
Tor-protected network traffic. However, existing WF attacks have critical
limitations on accurately identifying websites in multi-tab browsing sessions,
where the holistic pattern of individual websites is no longer preserved, and
the number of tabs opened by a client is unknown a priori. In this paper, we
propose ARES, a novel WF framework natively designed for multi-tab WF attacks.
ARES formulates the multi-tab attack as a multi-label classification problem
and solves it using the novel Transformer-based models. Specifically, ARES
extracts local patterns based on multi-level traffic aggregation features and
utilizes the improved self-attention mechanism to analyze the correlations
between these local patterns, effectively identifying websites. We implement a
prototype of ARES and extensively evaluate its effectiveness using our
large-scale datasets collected over multiple months. The experimental results
illustrate that ARES achieves optimal performance in several realistic
scenarios. Further, ARES remains robust even against various WF defenses.

摘要：網站指紋辨識讓竊聽者得以在加密連線中判斷使用者造訪哪些網站。最先進的網站指紋辨識 (WF) 攻擊已證明即使針對 Tor 保護的網路流量也能發揮效用。然而，現有的 WF 攻擊在準確辨識多標籤瀏覽會話中的網站時有嚴重的限制，其中個別網站的整體模式不再保留，而且無法事先得知客戶端開啟的標籤數量。在本文中，我們提出 ARES，一個專門設計用於多標籤 WF 攻擊的新穎 WF 框架。ARES 將多標籤攻擊制定為多標籤分類問題，並使用新穎的基於 Transformer 的模型解決它。具體來說，ARES 根據多層級流量彙總特徵萃取局部模式，並利用改進的自我注意機制分析這些局部模式之間的關聯性，有效地辨識網站。我們實作 ARES 的原型，並使用我們在數個月內收集的大規模資料集廣泛評估其效能。實驗結果顯示，ARES 在多種實際情況下都能達到最佳效能。此外，即使針對各種 WF 防禦，ARES 仍然保持穩健。

##### **Distillation Quantification for Large Language Models**
2501.12619v1 by Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni

Model distillation is a technique for transferring knowledge from large
language models (LLMs) to smaller ones, aiming to create resource-efficient yet
high-performing models. However, excessive distillation can lead to
homogenization, reducing diversity among models and impairing their ability to
robustly handle complex or novel tasks. These limitations underscore the need
to systematically quantify the distillation process and its impact. In this
work, we propose a framework to evaluate and quantify model distillation. Our
method addresses two key aspects: (1) Identifying identity cognition
contradictions to assess discrepancies in how models perceive and represent
identity-related information, and (2) Analyzing multi-granularity response
similarities across models to measure the extent of homogenization.
Experimental results demonstrate two key insights: (1) Well-known closed-source
and open-source LLMs usually exhibit high distillation degrees, except for
Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees
compared to aligned LLMs. By offering a systematic approach to improve the
transparency of LLM data distillation, we call for LLMs with more independent
development and more transparent technical reports to improve LLMs' robustness
and safety. The code and data are available under
https://github.com/Aegis1863/LLMs-Distillation-Quantification.

摘要：模型蒸餾是一種將知識從大型語言模型 (LLM) 轉移到較小模型的技術，目的是建立資源效率高但效能良好的模型。然而，過度的蒸餾可能導致同質化，減少模型之間的多樣性，並損害它們穩健處理複雜或新穎任務的能力。這些限制強調了系統化量化蒸餾過程及其影響的必要性。在這項工作中，我們提出了一個評估和量化模型蒸餾的框架。我們的模型探討了兩個關鍵面向：(1) 識別身分認知矛盾，以評估模型在感知和呈現身分相關資訊的方式上的差異，以及 (2) 分析模型之間的多粒度回應相似性，以衡量同質化的程度。實驗結果證明了兩個關鍵見解：(1) 除了 Claude、Doubao 和 Gemini 之外，眾所周知的閉源和開源 LLM 通常表現出高度的蒸餾程度。(2) 與對齊的 LLM 相比，基礎 LLM 顯示出更高的蒸餾程度。透過提供一種系統化的方法來提高 LLM 資料蒸餾的透明度，我們呼籲開發出更獨立且技術報告更透明的 LLM，以提高 LLM 的穩健性和安全性。程式碼和資料可在 https://github.com/Aegis1863/LLMs-Distillation-Quantification 下取得。

##### **Deep Learning-Based Identification of Inconsistent Method Names: How Far Are We?**
2501.12617v1 by Taiming Wang, Yuxia Zhang, Lin Jiang, Yi Tang, Guangjie Li, Hui Liu

Concise and meaningful method names are crucial for program comprehension and
maintenance. However, method names may become inconsistent with their
corresponding implementations, causing confusion and errors. Several deep
learning (DL)-based approaches have been proposed to identify such
inconsistencies, with initial evaluations showing promising results. However,
these evaluations typically use a balanced dataset, where the number of
inconsistent and consistent names are equal. This setup, along with flawed
dataset construction, leads to false positives, making reported performance
less reliable in real-world scenarios, where most method names are consistent.
In this paper, we present an empirical study that evaluates state-of-the-art
DL-based methods for identifying inconsistent method names. We create a new
benchmark by combining automatic identification from commit histories and
manual developer inspections, reducing false positives. We evaluate five
representative DL approaches (one retrieval-based and four generation-based) on
this benchmark. Our results show that performance drops substantially when
moving from the balanced dataset to the new benchmark. We further conduct
quantitative and qualitative analyses to understand the strengths and
weaknesses of the approaches. Retrieval-based methods perform well on simple
methods and those with popular name sub-tokens but fail due to inefficient
representation techniques. Generation-based methods struggle with inaccurate
similarity calculations and immature name generation. Based on these findings,
we propose improvements using contrastive learning and large language models
(LLMs). Our study suggests that significant improvements are needed before
these DL approaches can be effectively applied to real-world software systems.

摘要：簡潔且有意義的方法名稱對於程式理解和維護至關重要。然而，方法名稱可能會與其對應的實作不一致，造成混淆和錯誤。已經提出了一些基於深度學習 (DL) 的方法來識別此類不一致，初步評估顯示出有希望的結果。然而，這些評估通常使用平衡的資料集，其中不一致和一致名稱的數量相等。此設定連同有缺陷的資料集建構導致誤報，使得報告的效能不太可靠，在現實世界場景中，大多數方法名稱是一致的。在本文中，我們提出一個經驗研究，評估最先進的基於 DL 的方法來識別不一致的方法名稱。我們透過結合來自提交記錄的自動識別和手動開發人員檢查來建立一個新的基準，減少誤報。我們在此基準上評估五種具有代表性的 DL 方法（一種基於檢索和四種基於生成）。我們的結果顯示，從平衡的資料集轉移到新的基準時，效能大幅下降。我們進一步進行量化和質化分析，以了解這些方法的優點和缺點。基於檢索的方法在簡單的方法和具有熱門名稱子代幣的方法上表現良好，但由於低效率的表示技術而失敗。基於生成的的方法難以進行不準確的相似性計算和不成熟的名稱生成。根據這些發現，我們提出使用對比學習和大語言模型 (LLM) 的改進。我們的研究表明，在這些 DL 方法可以有效應用於現實世界的軟體系統之前，需要顯著的改進。

##### **GATE: Adaptive Learning with Working Memory by Information Gating in Multi-lamellar Hippocampal Formation**
2501.12615v1 by Yuechen Liu, Zishun Wang, Chen Qiao, Zongben Xu

Hippocampal formation (HF) can rapidly adapt to varied environments and build
flexible working memory (WM). To mirror the HF's mechanism on generalization
and WM, we propose a model named Generalization and Associative Temporary
Encoding (GATE), which deploys a 3-D multi-lamellar dorsoventral (DV)
architecture, and learns to build up internally representation from externally
driven information layer-wisely. In each lamella, regions of HF:
EC3-CA1-EC5-EC3 forms a re-entrant loop that discriminately maintains
information by EC3 persistent activity, and selectively readouts the retained
information by CA1 neurons. CA3 and EC5 further provides gating function that
controls these processes. After learning complex WM tasks, GATE forms neuron
representations that align with experimental records, including splitter, lap,
evidence, trace, delay-active cells, as well as conventional place cells.
Crucially, DV architecture in GATE also captures information, range from
detailed to abstract, which enables a rapid generalization ability when cue,
environment or task changes, with learned representations inherited. GATE
promises a viable framework for understanding the HF's flexible memory
mechanisms and for progressively developing brain-inspired intelligent systems.

摘要：海馬迴（HF）能快速適應各種環境並建立彈性的工作記憶（WM）。為了反映 HF 在概化和 WM 上的機制，我們提出一個名為概化和聯想暫時編碼（GATE）的模型，它部署了一個 3D 多層背腹（DV）架構，並學習從外部驅動的信息逐層建立內部表徵。在每個層板中，HF 區域：EC3-CA1-EC5-EC3 形成一個重入迴路，通過 EC3 持續活動來區別性地維護信息，並通過 CA1 神經元選擇性地讀取保留的信息。CA3 和 EC5 進一步提供控制這些過程的閘控功能。在學習複雜的 WM 任務後，GATE 形成與實驗記錄一致的神經元表徵，包括分離器、迴路、證據、痕跡、延遲激活細胞以及傳統位置細胞。至關重要的是，GATE 中的 DV 架構還捕獲了從具體到抽象的信息，當線索、環境或任務發生變化時，這使得能夠快速概化能力，並繼承學習到的表徵。GATE 為理解 HF 的靈活記憶機制和逐步開發受大腦啟發的智能系統提供了一個可行的框架。

##### **T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation**
2501.12612v1 by Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, Xihui Liu, Lu Sheng, Jing Shao

Text-to-image (T2I) models have rapidly advanced, enabling the generation of
high-quality images from text prompts across various domains. However, these
models present notable safety concerns, including the risk of generating
harmful, biased, or private content. Current research on assessing T2I safety
remains in its early stages. While some efforts have been made to evaluate
models on specific safety dimensions, many critical risks remain unexplored. To
address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I
models across three key domains: toxicity, fairness, and bias. We build a
detailed hierarchy of 12 tasks and 44 categories based on these three domains,
and meticulously collect 70K corresponding prompts. Based on this taxonomy and
prompt set, we build a large-scale T2I dataset with 68K manually annotated
images and train an evaluator capable of detecting critical risks that previous
work has failed to identify, including risks that even ultra-large proprietary
models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion
models on T2ISafety and reveal several concerns including persistent issues
with racial fairness, a tendency to generate toxic content, and significant
variation in privacy protection across the models, even with defense methods
like concept erasing. Data and evaluator are released under
https://github.com/adwardlee/t2i_safety.

摘要：文本到图像 (T2I) 模型迅速发展，能够根据跨越各种领域的文本提示生成高质量图像。然而，这些模型提出了明显的安全性问题，包括生成有害、有偏见或私有内容的风险。当前对 T2I 安全性评估的研究仍处于早期阶段。虽然已经做出了一些努力来评估特定安全维度上的模型，但许多关键风险仍未得到探索。为了解决这一差距，我们引入了 T2ISafety，这是一个安全基准，用于评估 T2I 模型在三个关键领域：毒性、公平性和偏见。我们基于这三个领域构建了一个包含 12 个任务和 44 个类别的详细层次结构，并精心收集了 70K 个相应的提示。基于此分类法和提示集，我们构建了一个包含 68K 个手动注释图像的大规模 T2I 数据集，并训练了一个能够检测先前工作未能识别的关键风险的评估器，包括即使是 GPT 等超大型专有模型也无法正确检测的风险。我们在 T2ISafety 上评估了 12 个突出的扩散模型，并揭示了几个问题，包括种族公平性的持续问题、生成有害内容的倾向，以及即使采用概念擦除等防御方法，模型之间的隐私保护也存在显着差异。数据和评估器在 https://github.com/adwardlee/t2i_safety 下发布。

##### **BLR-MoE: Boosted Language-Routing Mixture of Experts for Domain-Robust Multilingual E2E ASR**
2501.12602v1 by Guodong Ma, Wenxuan Wang, Lifeng Zhou, Yuting Yang, Yuke Li, Binbin Du

Recently, the Mixture of Expert (MoE) architecture, such as LR-MoE, is often
used to alleviate the impact of language confusion on the multilingual ASR
(MASR) task. However, it still faces language confusion issues, especially in
mismatched domain scenarios. In this paper, we decouple language confusion in
LR-MoE into confusion in self-attention and router. To alleviate the language
confusion in self-attention, based on LR-MoE, we propose to apply attention-MoE
architecture for MASR. In our new architecture, MoE is utilized not only on
feed-forward network (FFN) but also on self-attention. In addition, to improve
the robustness of the LID-based router on language confusion, we propose expert
pruning and router augmentation methods. Combining the above, we get the
boosted language-routing MoE (BLR-MoE) architecture. We verify the
effectiveness of the proposed BLR-MoE in a 10,000-hour MASR dataset.

摘要：近期，专家混合（MoE）架构，例如 LR-MoE，通常用于减轻语言混淆对多语言 ASR（MASR）任务的影响。然而，它仍然面临语言混淆问题，尤其是在不匹配的领域场景中。在本文中，我们将 LR-MoE 中的语言混淆解耦为自注意力和路由器中的混淆。为了减轻自注意力中的语言混淆，基于 LR-MoE，我们提出为 MASR 应用注意力-MoE 架构。在我们的新架构中，MoE 不仅用于前馈网络（FFN），还用于自注意力。此外，为了提高基于 LID 的路由器对语言混淆的鲁棒性，我们提出了专家剪枝和路由器增强方法。结合上述方法，我们得到了增强语言路由 MoE（BLR-MoE）架构。我们在一个 10,000 小时的 MASR 数据集中验证了所提出的 BLR-MoE 的有效性。

##### **Kimi k1.5: Scaling Reinforcement Learning with LLMs**
2501.12599v1 by Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang

Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior published work has not produced competitive results. In
light of this, we report on the training practice of Kimi k1.5, our latest
multi-modal LLM trained with RL, including its RL training techniques,
multi-modal data recipes, and infrastructure optimization. Long context scaling
and improved policy optimization methods are key ingredients of our approach,
which establishes a simplistic, effective RL framework without relying on more
complex techniques such as Monte Carlo tree search, value functions, and
process reward models. Notably, our system achieves state-of-the-art reasoning
performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,
96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching
OpenAI's o1. Moreover, we present effective long2short methods that use
long-CoT techniques to improve short-CoT models, yielding state-of-the-art
short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on
LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and
Claude Sonnet 3.5 by a large margin (up to +550%).

摘要：語言模型預訓練加上下一個符號預測已證明對於擴充運算有效，但受限於可用的訓練資料量。擴充強化學習 (RL) 為持續改善人工智慧解鎖了一個新軸，承諾大型語言模型 (LLM) 能透過學習探索獎勵來擴充他們的訓練資料。然而，之前發布的作品尚未產生有競爭力的結果。有鑑於此，我們回報 Kimi k1.5 的訓練實務，這是我們最新的多模態 LLM，使用 RL 訓練，包括其 RL 訓練技巧、多模態資料食譜和基礎架構最佳化。長脈絡擴充和改善的策略最佳化方法是我們方法中的關鍵要素，建立了一個簡潔、有效的 RL 架構，而不依賴於更複雜的技巧，例如蒙地卡羅樹狀搜尋、價值函數和處理獎勵模型。值得注意的是，我們的系統在多個基準和模式中達成最先進的推理效能——例如，AIME 中的 77.5、MATH 500 中的 96.2、Codeforces 中的 94 百分位、MathVista 中的 74.9——匹配 OpenAI 的 o1。此外，我們提出有效的 long2short 方法，使用 long-CoT 技巧來改善 short-CoT 模型，產生最先進的 short-CoT 推理結果——例如，AIME 中的 60.8、MATH500 中的 94.6、LiveCodeBench 中的 47.3——大幅超越現有的 short-CoT 模型，例如 GPT-4o 和 Claude Sonnet 3.5（高達 +550%）。

##### **FedGrAINS: Personalized SubGraph Federated Learning with Adaptive Neighbor Sampling**
2501.12592v1 by Emir Ceyani, Han Xie, Baturalp Buyukates, Carl Yang, Salman Avestimehr

Graphs are crucial for modeling relational and biological data. As datasets
grow larger in real-world scenarios, the risk of exposing sensitive information
increases, making privacy-preserving training methods like federated learning
(FL) essential to ensure data security and compliance with privacy regulations.
Recently proposed personalized subgraph FL methods have become the de-facto
standard for training personalized Graph Neural Networks (GNNs) in a federated
manner while dealing with the missing links across clients' subgraphs due to
privacy restrictions. However, personalized subgraph FL faces significant
challenges due to the heterogeneity in client subgraphs, such as degree
distributions among the nodes, which complicate federated training of graph
models. To address these challenges, we propose \textit{FedGrAINS}, a novel
data-adaptive and sampling-based regularization method for subgraph FL.
FedGrAINS leverages generative flow networks (GFlowNets) to evaluate node
importance concerning clients' tasks, dynamically adjusting the message-passing
step in clients' GNNs. This adaptation reflects task-optimized sampling aligned
with a trajectory balance objective. Experimental results demonstrate that the
inclusion of \textit{FedGrAINS} as a regularizer consistently improves the FL
performance compared to baselines that do not leverage such regularization.

摘要：圖形對於關係和生物資料建模至關重要。隨著資料集在實際情況中越來越大，暴露敏感資訊的風險也隨之增加，這使得像聯合學習 (FL) 這樣的隱私保護訓練方法對於確保資料安全和符合隱私法規至關重要。最近提出的個性化子圖 FL 方法已成為訓練個性化圖形神經網路 (GNN) 的事實標準，同時處理由於隱私限制而導致客戶端子圖中遺失的連結。然而，個性化子圖 FL 由於客戶端子圖中的異質性而面臨重大挑戰，例如節點之間的度數分配，這使得圖形模型的聯合訓練複雜化。為了應對這些挑戰，我們提出了 \textit{FedGrAINS}，一種用於子圖 FL 的新穎資料自適應和基於抽樣的正規化方法。FedGrAINS 利用生成流網路 (GFlowNets) 來評估節點重要性，涉及客戶端任務，動態調整客戶端 GNN 中的訊息傳遞步驟。這種適應反映了與軌跡平衡目標一致的任務最佳化抽樣。實驗結果表明，將 \textit{FedGrAINS} 作為正規化項納入，與不利用此類正規化的基準線相比，始終改善 FL 效能。

##### **Leveraging LLMs to Create a Haptic Devices' Recommendation System**
2501.12573v1 by Yang Liu, Haiwei Dong, Abdulmotaleb El Saddik

Haptic technology has seen significant growth, yet a lack of awareness of
existing haptic device design knowledge hinders development. This paper
addresses these limitations by leveraging advancements in Large Language Models
(LLMs) to develop a haptic agent, focusing specifically on Grounded Force
Feedback (GFF) devices recommendation. Our approach involves automating the
creation of a structured haptic device database using information from research
papers and product specifications. This database enables the recommendation of
relevant GFF devices based on user queries. To ensure precise and contextually
relevant recommendations, the system employs a dynamic retrieval method that
combines both conditional and semantic searches. Benchmarking against the
established UEQ and existing haptic device searching tools, the proposed haptic
recommendation agent ranks in the top 10\% across all UEQ categories with mean
differences favoring the agent in nearly all subscales, and maintains no
significant performance bias across different user groups, showcasing superior
usability and user satisfaction.

摘要：觸覺技術已顯著成長，但缺乏對現有觸覺裝置設計知識的認識阻礙了發展。這篇論文透過利用大型語言模型 (LLM) 的進展來開發觸覺代理，特別著重於基礎力回饋 (GFF) 裝置建議，來解決這些限制。我們的做法包括使用研究論文和產品規格中的資訊，自動建立結構化的觸覺裝置資料庫。此資料庫能根據使用者查詢建議相關的 GFF 裝置。為了確保精確且符合脈絡的建議，系統採用動態擷取方法，結合條件式和語意搜尋。根據既定的 UEQ 和現有的觸覺裝置搜尋工具進行基準測試，建議的觸覺推薦代理在所有 UEQ 類別中排名在前 10%，平均差異有利於代理在幾乎所有子量表中，並且在不同的使用者群組中沒有顯著的效能偏差，展示出優異的可用性和使用者滿意度。

##### **O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning**
2501.12570v1 by Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, Dacheng Tao

Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended
reasoning processes similar to how humans ponder over complex problems. This
reasoning paradigm significantly enhances the model's problem-solving abilities
and has achieved promising results. However, long-thought reasoning process
leads to a substantial increase in inference time. A pressing challenge is
reducing the inference overhead of long-thought LLMs while ensuring accuracy.
In this paper, we experimentally demonstrate that long-thought reasoning models
struggle to effectively allocate token budgets based on problem difficulty and
reasoning redundancies. To address this, we propose Length-Harmonizing
Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while
maintaining accuracy. This effective fine-tuning method first estimates the
LLM's baseline performance through pre-sampling and then uses RL-style
fine-tuning to encourage the model to generate shorter reasoning processes
under accuracy constraints. This allows the model to achieve efficient
reasoning with lower redundancy while maintaining accuracy. Experiments on
various mathematical reasoning benchmarks show that O1-Pruner not only
significantly reduces inference overhead but also achieves higher accuracy,
providing a novel and promising solution to this challenge. Our code is coming
soon at https://github.com/StarDewXXX/O1-Pruner

摘要：最近，人们长期思考的推理 LLM，例如 OpenAI 的 O1，采用了类似于人类思考复杂问题的扩展推理过程。这种推理范式显著增强了模型的解决问题的能力，并取得了可喜的成果。然而，长期思考的推理过程导致推理时间大幅增加。一个紧迫的挑战是在确保准确性的同时减少长期思考的 LLM 的推理开销。在本文中，我们通过实验表明，长期思考的推理模型难以根据问题难度和推理冗余有效分配标记预算。为了解决这个问题，我们提出了长度协调微调 (O1-Pruner)，旨在最大程度减少推理开销，同时保持准确性。这种有效的微调方法首先通过预采样来估计 LLM 的基线性能，然后使用 RL 风格的微调来鼓励模型在准确性约束下生成更短的推理过程。这允许模型在保持准确性的同时实现高效推理，并降低冗余。在各种数学推理基准上的实验表明，O1-Pruner 不仅显着降低了推理开销，而且还实现了更高的准确性，为这一挑战提供了一种新颖且有前途的解决方案。我们的代码即将在 https://github.com/StarDewXXX/O1-Pruner 上发布

##### **Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review**
2501.12557v1 by Rock Yuren Pang, Hope Schroeder, Kynnedy Simone Smith, Solon Barocas, Ziang Xiao, Emily Tseng, Danielle Bragg

Large language models (LLMs) have been positioned to revolutionize HCI, by
reshaping not only the interfaces, design patterns, and sociotechnical systems
that we study, but also the research practices we use. To-date, however, there
has been little understanding of LLMs' uptake in HCI. We address this gap via a
systematic literature review of 153 CHI papers from 2020-24 that engage with
LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in
HCI projects; (3) contribution types; and (4) acknowledged limitations and
risks. We find LLM work in 10 diverse domains, primarily via empirical and
artifact contributions. Authors use LLMs in five distinct roles, including as
research tools or simulated users. Still, authors often raise validity and
reproducibility concerns, and overwhelmingly study closed models. We outline
opportunities to improve HCI research with and on LLMs, and provide guiding
questions for researchers to consider the validity and appropriateness of
LLM-related work.

摘要：大型語言模型 (LLM) 已被定位為革命性的 HCI，不僅重塑我們研究的介面、設計模式和社會技術系統，還重塑我們使用的研究實務。然而，到目前為止，對於 LLM 在 HCI 中的採用了解甚少。我們透過系統性地回顧 2020-24 年間 153 篇與 LLM 相關的 CHI 論文，來解決這個差距。我們分類：(1) 應用 LLM 的領域；(2) LLM 在 HCI 專案中的角色；(3) 貢獻類型；以及 (4) 已知的限制和風險。我們發現 LLM 的工作分佈在 10 個不同的領域，主要是透過實證和人工製品的貢獻。作者在五個不同的角色中使用 LLM，包括作為研究工具或模擬使用者。儘管如此，作者經常提出有效性和可複製性的疑慮，並且絕大多數研究封閉模型。我們概述了利用 LLM 和研究 LLM 來改善 HCI 研究的機會，並提供指導性問題，供研究人員考量與 LLM 相關工作的有效性和適切性。

##### **Human-like conceptual representations emerge from language prediction**
2501.12547v1 by Ningyu Xu, Qi Zhang, Chao Du, Qiang Luo, Xipeng Qiu, Xuanjing Huang, Menghan Zhang

Recent advances in large language models (LLMs) provide a new opportunity to
address the long-standing question of how concepts are represented and
organized in the mind, which is central to unravelling the nature of human
cognition. Here, we reframed the classic reverse dictionary task to simulate
human concept inference in context and investigated the emergence of human-like
conceptual representations within LLMs. We found that LLMs were able to infer
concepts from definitional descriptions and construct representation spaces
that converge towards a shared, context-independent structure. These
representations effectively predicted human behavioural judgments and aligned
well with neural activity patterns in the human brain, offering evidence for
biological plausibility. These findings demonstrate that human-like conceptual
representations and organization can naturally emerge from language prediction,
even without real-world grounding. Our work supports the view that LLMs serve
as valuable tools for understanding complex human cognition and paves the way
for better alignment between artificial and human intelligence.

摘要：大型語言模型 (LLM) 的最新進展為我們提供了一個新的機會，可以解決概念如何在心智中被表徵和組織這個長久以來的問題，而這對於解開人類認知的本質至關重要。在此，我們重新建構了經典的逆向字典任務，以模擬人類在特定情境中的概念推論，並探討人類類似概念表徵在 LLM 中出現的現象。我們發現 LLM 能夠從定義描述中推論概念，並建構收斂於共享、與情境無關的結構的表徵空間。這些表徵有效地預測了人類的行為判斷，並與人類大腦中的神經活動模式非常吻合，為生物學上的可信度提供了證據。這些發現表明，即使沒有真實世界的依據，人類類似的概念表徵和組織也能自然地從語言預測中浮現。我們的研究支持 LLM 可作為理解複雜人類認知的寶貴工具的觀點，並為人工智慧和人類智慧之間更好的對齊鋪平了道路。

##### **Comparative Approaches to Sentiment Analysis Using Datasets in Major European and Arabic Languages**
2501.12540v1 by Mikhail Krasitskii, Olga Kolesnikova, Liliana Chanona Hernandez, Grigori Sidorov, Alexander Gelbukh

This study explores transformer-based models such as BERT, mBERT, and XLM-R
for multi-lingual sentiment analysis across diverse linguistic structures. Key
contributions include the identification of XLM-R superior adaptability in
morphologically complex languages, achieving accuracy levels above 88%. The
work highlights fine-tuning strategies and emphasizes their significance for
improving sentiment classification in underrepresented languages.

摘要：本研究探討了 BERT、mBERT 和 XLM-R 等基於轉換器的模型，用於跨越不同語言結構的多語言情感分析。主要貢獻包括識別 XLM-R 在形態複雜的語言中具有出色的適應性，達到 88% 以上的準確度。這項工作重點介紹了微調策略，並強調了它們對改善代表性不足的語言的情感分類的重要性。

##### **Compositional Instruction Following with Language Models and Reinforcement Learning**
2501.12539v1 by Vanya Cohen, Geraud Nangue Tasse, Nakul Gopalan, Steven James, Matthew Gombolay, Ray Mooney, Benjamin Rosman

Combining reinforcement learning with language grounding is challenging as
the agent needs to explore the environment while simultaneously learning
multiple language-conditioned tasks. To address this, we introduce a novel
method: the compositionally-enabled reinforcement learning language agent
(CERLLA). Our method reduces the sample complexity of tasks specified with
language by leveraging compositional policy representations and a semantic
parser trained using reinforcement learning and in-context learning. We
evaluate our approach in an environment requiring function approximation and
demonstrate compositional generalization to novel tasks. Our method
significantly outperforms the previous best non-compositional baseline in terms
of sample complexity on 162 tasks designed to test compositional
generalization. Our model attains a higher success rate and learns in fewer
steps than the non-compositional baseline. It reaches a success rate equal to
an oracle policy's upper-bound performance of 92%. With the same number of
environment steps, the baseline only reaches a success rate of 80%.

摘要：將強化學習與語言基礎結合是一項挑戰，因為代理需要在同時學習多項語言條件化的任務時探索環境。為了解決這個問題，我們引入了一種新方法：組成式增強學習語言代理 (CERLLA)。我們的做法透過利用組成式政策表示和使用強化學習和情境學習訓練的語意解析器，減少了語言指定的任務的範例複雜性。我們在需要函數逼近的環境中評估我們的做法，並展示了對新任務的組成式概括。我們的做法在 162 項用於測試組成式概括的任務中，在範例複雜性方面顯著優於先前的最佳非組成式基準。我們的模型達到了更高的成功率，並且比非組成式基準學習的步驟更少。它達到了與神諭政策 92% 的上限效能相等的成功率。在相同的環境步驟數下，基準僅達到 80% 的成功率。

##### **Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related with Post COVID-19 Condition**
2501.12538v1 by Juan Andres Medina Florez, Shaina Raza, Rashida Lynn, Zahra Shakeri, Brendan T. Smith, Elham Dolatabadi

Understanding the prevalence, disparities, and symptom variations of Post
COVID-19 Condition (PCC) for vulnerable populations is crucial to improving
care and addressing intersecting inequities. This study aims to develop a
comprehensive framework for integrating social determinants of health (SDOH)
into PCC research by leveraging NLP techniques to analyze disparities and
variations in SDOH representation within PCC case reports. Following
construction of a PCC Case Report Corpus, comprising over 7,000 case reports
from the LitCOVID repository, a subset of 709 reports were annotated with 26
core SDOH-related entity types using pre-trained named entity recognition (NER)
models, human review, and data augmentation to improve quality, diversity and
representation of entity types. An NLP pipeline integrating NER, natural
language inference (NLI), trigram and frequency analyses was developed to
extract and analyze these entities. Both encoder-only transformer models and
RNN-based models were assessed for the NER objective.
  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models
in generalizability to distinct sentence structures and greater class sparsity.
Exploratory analysis revealed variability in entity richness, with prevalent
entities like condition, age, and access to care, and underrepresentation of
sensitive categories like race and housing status. Trigram analysis highlighted
frequent co-occurrences among entities, including age, gender, and condition.
The NLI objective (entailment and contradiction analysis) showed attributes
like "Experienced violence or abuse" and "Has medical insurance" had high
entailment rates (82.4%-80.3%), while attributes such as "Is
female-identifying," "Is married," and "Has a terminal condition" exhibited
high contradiction rates (70.8%-98.5%).

摘要：<paragraph>了解弱勢群體的後 COVID-19 狀況 (PCC) 的流行率、差異和症狀變化對於改善照護和解決交叉不平等至關重要。本研究旨在透過利用自然語言處理技術分析 PCC 病例報告中 SDOH 的表現差異和變化，開發一個整合健康社會決定因素 (SDOH) 進入 PCC 研究的綜合架構。在建構一個 PCC 病例報告語料庫（包含來自 LitCOVID 資料庫的 7,000 多個病例報告）後，使用預先訓練的名稱實體辨識 (NER) 模型、人工審查和資料擴充對 709 份報告進行註解，其中包含 26 個核心 SDOH 相關實體類型，以提高實體類型的品質、多樣性和表現。開發了一個整合 NER、自然語言推論 (NLI)、三元組和頻率分析的 NLP 管線，以萃取和分析這些實體。針對 NER 目標評估了僅編碼器轉換器模型和基於 RNN 的模型。微調後的僅編碼器 BERT 模型在一般化到不同的句子結構和更大的類別稀疏性方面優於傳統的基於 RNN 的模型。探索性分析顯示實體豐富度存在變異性，普遍的實體包括狀況、年齡和取得照護的管道，而種族和居住狀況等敏感類別的表現則不足。三元組分析強調了實體之間的頻繁共現，包括年齡、性別和狀況。NLI 目標（蘊涵和矛盾分析）顯示「經歷過暴力或虐待」和「有醫療保險」等屬性具有很高的蘊涵率（82.4%-80.3%），而「認同自己是女性」、「已婚」和「有末期疾病」等屬性則表現出很高的矛盾率（70.8%-98.5%）。</paragraph>

##### **Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy**
2501.12537v1 by Khaoula Chehbouni, Martine De Cock, Gilles Caporossi, Afaf Taik, Reihaneh Rabbany, Golnoosh Farnadi

The increased screen time and isolation caused by the COVID-19 pandemic have
led to a significant surge in cases of online grooming, which is the use of
strategies by predators to lure children into sexual exploitation. Previous
efforts to detect grooming in industry and academia have involved accessing and
monitoring private conversations through centrally-trained models or sending
private conversations to a global server. In this work, we implement a
privacy-preserving pipeline for the early detection of sexual predators. We
leverage federated learning and differential privacy in order to create safer
online spaces for children while respecting their privacy. We investigate
various privacy-preserving implementations and discuss their benefits and
shortcomings. Our extensive evaluation using real-world data proves that
privacy and utility can coexist with only a slight reduction in utility.

摘要：由於 COVID-19 疫情導致螢幕時間增加和隔離，導致網路誘騙案件大幅增加，網路誘騙是指掠奪者使用策略引誘兒童進行性剝削。先前產業和學術界偵測誘騙的作法，包含透過集中訓練的模型存取和監控私人對話，或將私人對話傳送至全球伺服器。在這項工作中，我們實作一個注重隱私的管道，用於早期偵測性掠奪者。我們利用聯盟式學習和差分隱私，在尊重兒童隱私的同時，為他們建立更安全的網路空間。我們調查各種注重隱私的實作，並探討其優點和缺點。我們使用真實世界資料進行廣泛評估，證明隱私和效用可以共存，而效用只會略微降低。

##### **Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs**
2501.12536v1 by Zheng Li, Zhipeng Bao, Haoming Meng, Haotian Shi, Qianwen Li, Handong Yao, Xiaopeng Li

This paper presents the development of a comprehensive dataset capturing
interactions between Autonomous Vehicles (AVs) and traffic control devices,
specifically traffic lights and stop signs. Derived from the Waymo Motion
dataset, our work addresses a critical gap in the existing literature by
providing real-world trajectory data on how AVs navigate these traffic control
devices. We propose a methodology for identifying and extracting relevant
interaction trajectory data from the Waymo Motion dataset, incorporating over
37,000 instances with traffic lights and 44,000 with stop signs. Our
methodology includes defining rules to identify various interaction types,
extracting trajectory data, and applying a wavelet-based denoising method to
smooth the acceleration and speed profiles and eliminate anomalous values,
thereby enhancing the trajectory quality. Quality assessment metrics indicate
that trajectories obtained in this study have anomaly proportions in
acceleration and jerk profiles reduced to near-zero levels across all
interaction categories. By making this dataset publicly available, we aim to
address the current gap in datasets containing AV interaction behaviors with
traffic lights and signs. Based on the organized and published dataset, we can
gain a more in-depth understanding of AVs' behavior when interacting with
traffic lights and signs. This will facilitate research on AV integration into
existing transportation infrastructures and networks, supporting the
development of more accurate behavioral models and simulation tools.

摘要：本文介绍了一套综合数据集的开发，该数据集捕捉了自动驾驶汽车 (AV) 与交通控制设备（特别是交通信号灯和停车标志）之间的交互。我们的工作源自 Waymo Motion 数据集，通过提供有关自动驾驶汽车如何导航这些交通控制设备的真实世界轨迹数据，解决了现有文献中的一个关键空白。我们提出了一种从 Waymo Motion 数据集中识别和提取相关交互轨迹数据的方法，其中包含超过 37,000 个交通信号灯实例和 44,000 个停车标志实例。我们的方法包括定义规则以识别各种交互类型、提取轨迹数据以及应用基于小波的去噪方法来平滑加速度和速度曲线并消除异常值，从而提高轨迹质量。质量评估指标表明，本研究中获得的轨迹在所有交互类别中将加速度和加加速度曲线中的异常比例降低到接近零的水平。通过公开此数据集，我们旨在解决当前包含自动驾驶汽车与交通信号灯和标志交互行为的数据集中存在的空白。基于组织和发布的数据集，我们可以更深入地了解自动驾驶汽车在与交通信号灯和标志交互时的行为。这将促进对自动驾驶汽车集成到现有交通基础设施和网络的研究，支持更准确的行为模型和仿真工具的开发。

##### **Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**
2501.12524v1 by Jiaqi Guo, Yunnan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos

With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a
promising technique for COVID-19 detection, due to its non-invasive nature,
affordability, and portability. In response, researchers have focused on
developing AI-based scoring systems to provide real-time diagnostic support.
However, the limited size and lack of proper annotation in publicly available
ultrasound datasets pose significant challenges for training a robust AI model.
This paper proposes MeDiVLAD, a novel pipeline to address the above issue for
multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage
self-knowledge distillation to pretrain a vision transformer (ViT) without
label and aggregate frame-level features via dual-level VLAD aggregation. We
show that with minimal finetuning, MeDiVLAD outperforms conventional
fully-supervised methods in both frame- and video-level scoring, while offering
classification reasoning with exceptional quality. This superior performance
enables key applications such as the automatic identification of critical lung
pathology areas and provides a robust solution for broader medical video
classification tasks.

摘要：隨著 COVID-19 大流行的到來，超音波影像已成為一種有前途的 COVID-19 檢測技術，因為它具有非侵入性、價格實惠且可攜帶等特性。有鑑於此，研究人員專注於開發基於 AI 的評分系統，以提供即時的診斷支援。然而，公開可用的超音波資料集規模有限且缺乏適當的註解，這對訓練穩健的 AI 模型構成重大挑戰。本文提出 MeDiVLAD，這是一種新穎的管道，用於解決上述多層級肺部超音波 (LUS) 嚴重度評分的議題。具體來說，我們利用自我知識蒸餾技術，在沒有標籤的情況下預訓練視覺轉換器 (ViT)，並透過雙層級 VLAD 聚合來彙總幀級特徵。我們證明，透過最小的微調，MeDiVLAD 在幀級和影片級評分中都優於傳統的全監督式方法，同時提供品質極佳的分類推理。這種優異的效能支援了關鍵應用，例如自動識別肺部病灶區域，並為更廣泛的醫學影片分類任務提供穩健的解決方案。

##### **An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts**
2501.12521v1 by Dhia Elhaq Rzig, Dhruba Jyoti Paul, Kaiser Pister, Jordan Henkel, Foyzul Hassan

The tidal wave of advancements in Large Language Models (LLMs) has led to
their swift integration into application-level logic. Many software systems now
use prompts to interact with these black-box models, combining natural language
with dynamic values interpolated at runtime, to perform tasks ranging from
sentiment analysis to question answering. Due to the programmatic and
structured natural language aspects of these prompts, we refer to them as
Developer Prompts. Unlike traditional software artifacts, Dev Prompts blend
natural language instructions with artificial languages such as programming and
markup languages, thus requiring specialized tools for analysis, distinct from
classical software evaluation methods.
  In response to this need, we introduce PromptDoctor, a tool explicitly
designed to detect and correct issues of Dev Prompts. PromptDoctor identifies
and addresses problems related to bias, vulnerability, and sub-optimal
performance in Dev Prompts, helping mitigate their possible harms. In our
analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573
Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%
were vulnerable to prompt injection attacks. Additionally, 3,310 were amenable
to automated prompt optimization. To address these issues, we applied
PromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased
68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev
Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts.
Finally, we developed a PromptDoctor VSCode extension, enabling developers to
easily enhance Dev Prompts in their existing development workflows. The data
and source code for this work are available at

摘要：大型語言模型 (LLM) 的進展浪潮導致它們迅速整合到應用程式層級邏輯中。許多軟體系統現在使用提示與這些黑盒模型互動，結合自然語言與執行時內插的動態值，以執行從情緒分析到問題解答的各種任務。由於這些提示具有程式化和結構化的自然語言方面，我們將它們稱為開發人員提示。與傳統軟體人工製品不同，開發人員提示將自然語言指令與人工語言（例如程式設計和標記語言）混合在一起，因此需要專門的分析工具，不同於傳統的軟體評估方法。
為了滿足這個需求，我們引入了 PromptDoctor，這是一個專門設計用於偵測和修正開發人員提示問題的工具。PromptDoctor 識別並解決與開發人員提示中的偏差、漏洞和次佳效能相關的問題，有助於減輕其可能的危害。在我們對 2,173 個開發人員提示的分析中，這些提示被選為 40,573 個開發人員提示的代表性樣本，我們發現 3.46% 包含一種或多種形式的偏差，10.75% 容易受到提示注入攻擊。此外，3,310 個提示可以進行自動化提示最佳化。為了解決這些問題，我們將 PromptDoctor 應用於我們發現的錯誤開發人員提示。PromptDoctor 消除了 68.29% 有偏差的開發人員提示的偏差，強化了 41.81% 容易受到攻擊的開發人員提示，並改善了 37.1% 次佳開發人員提示的效能。最後，我們開發了一個 PromptDoctor VSCode 擴充功能，讓開發人員可以輕鬆地在現有的開發工作流程中增強開發人員提示。這項工作的資料和原始碼可在

##### **Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting**
2501.12489v1 by Josh Bruegger, Diana Ioana Catana, Vanja Macovaz, Matias Valdenegro-Toro, Matthia Sabatelli, Marco Zullich

The attribution of the author of an art piece is typically a laborious manual
process, usually relying on subjective evaluations of expert figures. However,
there are some situations in which quantitative features of the artwork can
support these evaluations. The extraction of these features can sometimes be
automated, for instance, with the use of Machine Learning (ML) techniques. An
example of these features is represented by repeated, mechanically impressed
patterns, called punches, present chiefly in 13th and 14th-century panel
paintings from Tuscany. Previous research in art history showcased a strong
connection between the shapes of punches and specific artists or workshops,
suggesting the possibility of using these quantitative cues to support the
attribution. In the present work, we first collect a dataset of large-scale
images of these panel paintings. Then, using YOLOv10, a recent and popular
object detection model, we train a ML pipeline to perform object detection on
the punches contained in the images. Due to the large size of the images, the
detection procedure is split across multiple frames by adopting a
sliding-window approach with overlaps, after which the predictions are combined
for the whole image using a custom non-maximal suppression routine. Our results
indicate how art historians working in the field can reliably use our method
for the identification and extraction of punches.

摘要：藝術品作者的歸屬通常是費力的手動過程，通常依賴於專家人物的主觀評估。然而，有些情況下，藝術品的量化特徵可以支持這些評估。這些特徵的提取有時可以自動化，例如，使用機器學習 (ML) 技術。這些特徵的一個例子是由重複的、機械壓印的圖案表示，稱為衝壓，主要存在於 13 世紀和 14 世紀托斯卡納的平板畫中。藝術史中的先前研究展示了衝壓形狀與特定藝術家或工作室之間的密切聯繫，表明可以使用這些量化線索來支持歸屬。在目前的工作中，我們首先收集了這些平板畫的大規模圖像數據集。然後，使用 YOLOv10，一個最近流行的目標檢測模型，我們訓練了一個機器學習管道對圖像中包含的衝壓進行目標檢測。由於圖像尺寸較大，採用帶有重疊的滑動窗口方法將檢測過程分佈在多個幀中，然後使用自定義非極大抑制例程將預測組合到整個圖像中。我們的結果表明，在該領域工作的藝術史學家可以可靠地使用我們的方法來識別和提取衝壓。

##### **The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws**
2501.12486v1 by Tian Jin, Ahmed Imtiaz Humayun, Utku Evci, Suvinay Subramanian, Amir Yazdanbakhsh, Dan Alistarh, Gintare Karolina Dziugaite

Pruning eliminates unnecessary parameters in neural networks; it offers a
promising solution to the growing computational demands of large language
models (LLMs). While many focus on post-training pruning, sparse
pre-training--which combines pruning and pre-training into a single
phase--provides a simpler alternative. In this work, we present the first
systematic exploration of optimal sparse pre-training configurations for LLMs
through an examination of 80 unique pruning schedules across different sparsity
levels and training durations. We find that initiating pruning at 25% of total
training compute and concluding at 75% achieves near-optimal final evaluation
loss. These findings provide valuable insights for efficient and effective
sparse pre-training of LLMs. Furthermore, we propose a new scaling law that
modifies the Chinchilla scaling law to use the average parameter count over
pre-training. Through empirical and theoretical validation, we demonstrate that
this modified scaling law accurately models evaluation loss for both sparsely
and densely pre-trained LLMs, unifying scaling laws across pre-training
paradigms. Our findings indicate that while sparse pre-training achieves the
same final model quality as dense pre-training for equivalent compute budgets,
it provides substantial benefits through reduced model size, enabling
significant potential computational savings during inference.

摘要：修剪消除了神經網路中不必要的參數；它為大型語言模型 (LLM) 不斷增長的運算需求提供了一個有前途的解決方案。雖然許多人專注於訓練後修剪，但稀疏預訓練（將修剪和預訓練結合到一個階段）提供了一個更簡單的替代方案。在這項工作中，我們通過檢查 80 個不同的稀疏度級別和訓練持續時間的獨特修剪時間表，對 LLM 的最佳稀疏預訓練配置進行了首次系統性探索。我們發現，從總訓練運算的 25% 開始修剪並在 75% 結束，可以達到接近最佳的最終評估損失。這些發現為 LLM 的高效且有效的稀疏預訓練提供了寶貴的見解。此外，我們提出了新的縮放定律，該定律修改了 Chinchilla 縮放定律，以使用預訓練期間的平均參數計數。通過經驗和理論驗證，我們證明了這個修改後的縮放定律可以準確地模擬稀疏和密集預訓練 LLM 的評估損失，統一了預訓練範例中的縮放定律。我們的發現表明，雖然稀疏預訓練在相同的運算預算下實現了與密集預訓練相同的最終模型品質，但它通過減少模型大小提供了顯著的優點，從而可以在推理期間實現顯著的潛在運算節省。

##### **fabSAM: A Farmland Boundary Delineation Method Based on the Segment Anything Model**
2501.12487v1 by Yufeng Xie, Hanzhi Wu, Hongxiang Tong, Lei Xiao, Wenwen Zhou, Ling Li, Thomas Cherico Wanger

Delineating farmland boundaries is essential for agricultural management such
as crop monitoring and agricultural census. Traditional methods using remote
sensing imagery have been efficient but limited in generalisation. The Segment
Anything Model (SAM), known for its impressive zero shot performance, has been
adapted for remote sensing tasks through prompt learning and fine tuning. Here,
we propose a SAM based farmland boundary delineation framework 'fabSAM' that
combines a Deeplabv3+ based Prompter and SAM. Also, a fine tuning strategy was
introduced to enable SAMs decoder to improve the use of prompt information.
Experimental results on the AI4Boundaries and AI4SmallFarms datasets have shown
that fabSAM has a significant improvement in farmland region identification and
boundary delineation. Compared to zero shot SAM, fabSAM surpassed it by 23.5%
and 15.1% in mIOU on the AI4Boundaries and AI4SmallFarms datasets,
respectively. For Deeplabv3+, fabSAM outperformed it by 4.9% and 12.5% in mIOU,
respectively. These results highlight the effectiveness of fabSAM, which also
means that we can more easily obtain the global farmland region and boundary
maps from open source satellite image datasets like Sentinel2.

摘要：劃定農田邊界對於農業管理（例如作物監控和農業普查）至關重要。傳統使用遙測影像的方法效率很高，但普遍性有限。以其令人印象深刻的零次學習表現而聞名的 Segment Anything Model (SAM) 已透過提示學習和微調，改編為遙測任務。在此，我們提出一個基於 SAM 的農田邊界描繪架構「fabSAM」，它結合了基於 Deeplabv3+ 的提示器和 SAM。此外，還引入了一種微調策略，讓 SAM 解碼器能夠改善提示資訊的使用。在 AI4Boundaries 和 AI4SmallFarms 資料集上的實驗結果顯示，fabSAM 在農田區域識別和邊界描繪方面有顯著的進步。與零次學習 SAM 相比，fabSAM 在 AI4Boundaries 和 AI4SmallFarms 資料集上的 mIOU 分別超越了它 23.5% 和 15.1%。與 Deeplabv3+ 相比，fabSAM 在 mIOU 上分別超越了它 4.9% 和 12.5%。這些結果突顯了 fabSAM 的有效性，這也意味著我們可以更容易地從 Sentinel2 等開源衛星影像資料集取得全球農田區域和邊界地圖。

##### **R2D2: Remembering, Reflecting and Dynamic Decision Making for Web Agents**
2501.12485v1 by Tenghao Huang, Kinjal Basu, Ibrahim Abdelaziz, Pavan Kapanipathi, Jonathan May, Muhao Chen

The proliferation of web agents necessitates advanced navigation and
interaction strategies within complex web environments. Current models often
struggle with efficient navigation and action execution due to limited
visibility and understanding of web structures. Our proposed R2D2 framework
addresses these challenges by integrating two paradigms: Remember and Reflect.
The Remember paradigm utilizes a replay buffer that aids agents in
reconstructing the web environment dynamically, thus enabling the formulation
of a detailed ``map'' of previously visited pages. This helps in reducing
navigational errors and optimizing the decision-making process during web
interactions. Conversely, the Reflect paradigm allows agents to learn from past
mistakes by providing a mechanism for error analysis and strategy refinement,
enhancing overall task performance. We evaluate R2D2 using the WEBARENA
benchmark, demonstrating significant improvements over existing methods,
including a 50% reduction in navigation errors and a threefold increase in task
completion rates. Our findings suggest that a combination of memory-enhanced
navigation and reflective learning promisingly advances the capabilities of web
agents, potentially benefiting various applications such as automated customer
service and personal digital assistants.

摘要：網路代理的激增需要在複雜的網路環境中進階的導航和互動策略。由於對網路結構的能見度和理解有限，目前的模型常常在有效導航和動作執行上遇到困難。我們提出的 R2D2 架構透過整合「記憶」和「反思」兩個範例來解決這些挑戰。記憶範例利用重播緩衝區，協助代理動態重建網路環境，進而形成先前造訪頁面的詳細「地圖」。這有助於減少導航錯誤，並在網路互動中最佳化決策制定過程。相反地，反思範例允許代理透過提供錯誤分析和策略精進的機制，從過去的錯誤中學習，進而提升整體任務執行。我們使用 WEBARENA 基準評估 R2D2，展示出相較於現有方法的顯著改善，包括導航錯誤減少 50%，以及任務完成率增加三倍。我們的發現顯示，結合記憶增強導航和反思學習，有望提升網路代理的能力，進而造福各種應用，例如自動化客戶服務和個人數位助理。

##### **Adaptive PII Mitigation Framework for Large Language Models**
2501.12465v1 by Shubhi Asthana, Ruchi Mahindru, Bing Zhang, Jorge Sanz

Artificial Intelligence (AI) faces growing challenges from evolving data
protection laws and enforcement practices worldwide. Regulations like GDPR and
CCPA impose strict compliance requirements on Machine Learning (ML) models,
especially concerning personal data use. These laws grant individuals rights
such as data correction and deletion, complicating the training and deployment
of Large Language Models (LLMs) that rely on extensive datasets. Public data
availability does not guarantee its lawful use for ML, amplifying these
challenges.
  This paper introduces an adaptive system for mitigating risk of Personally
Identifiable Information (PII) and Sensitive Personal Information (SPI) in
LLMs. It dynamically aligns with diverse regulatory frameworks and integrates
seamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses
advanced NLP techniques, context-aware analysis, and policy-driven masking to
ensure regulatory compliance.
  Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for
Passport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon
Comprehend (0.54). In human evaluations, the system achieved an average user
trust score of 4.6/5, with participants acknowledging its accuracy and
transparency. Observations demonstrate stricter anonymization under GDPR
compared to CCPA, which permits pseudonymization and user opt-outs. These
results validate the system as a scalable and robust solution for enterprise
privacy compliance.

摘要：人工智慧 (AI) 面臨來自全球不斷演進的資料保護法規和執法實務的挑戰越來越大。GDPR 和 CCPA 等法規對機器學習 (ML) 模型實施嚴格的合規要求，特別是關於個人資料的使用。這些法規賦予個人資料更正和刪除等權利，這使得依賴於廣泛資料集的大型語言模型 (LLM) 的訓練和部署變得複雜。公共資料的可用性並不能保證其在 ML 中的合法使用，這放大了這些挑戰。
本文介紹了一個適應性系統，用於降低大型語言模型 (LLM) 中個人可識別資訊 (PII) 和敏感個人資訊 (SPI) 的風險。它動態地與不同的法規架構保持一致，並無縫整合到治理、風險和合規 (GRC) 系統中。該系統使用先進的 NLP 技術、情境感知分析和策略驅動的遮罩來確保法規遵循。
基準測試突顯了該系統的有效性，護照號碼的 F1 分數為 0.95，優於 Microsoft Presidio (0.33) 和 Amazon Comprehend (0.54) 等工具。在人類評估中，該系統獲得了 4.6/5 的平均使用者信任分數，參與者肯定了其準確性和透明性。觀察結果表明，與允許假名化和使用者選擇退出的 CCPA 相比，GDPR 下的匿名化更加嚴格。這些結果驗證了該系統作為企業隱私合規的可擴展且穩健的解決方案。

##### **Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications**
2501.12456v1 by Shubhi Asthana, Bing Zhang, Ruchi Mahindru, Chad DeLuca, Anna Lisa Gentile, Sandeep Gopisetty

The adoption of Large Language Models (LLMs) has revolutionized AI
applications but poses significant challenges in safeguarding user privacy.
Ensuring compliance with privacy regulations such as GDPR and CCPA while
addressing nuanced privacy risks requires robust and scalable frameworks. This
paper presents a detailed study of OneShield Privacy Guard, a framework
designed to mitigate privacy risks in user inputs and LLM outputs across
enterprise and open-source settings. We analyze two real-world deployments:(1)
a multilingual privacy-preserving system integrated with Data and Model
Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an
open-source repository emphasizing automated triaging and community-driven
refinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting
sensitive entities like dates, names, and phone numbers across 26 languages,
outperforming state-of-the-art tool such as StarPII and Presidio by up to 12\%.
Deployment 2, with an average F1 score of 0.86, reduced manual effort by over
300 hours in three months, accurately flagging 8.25\% of 1,256 pull requests
for privacy risks with enhanced context sensitivity. These results demonstrate
OneShield's adaptability and efficacy in diverse environments, offering
actionable insights for context-aware entity recognition, automated compliance,
and ethical AI adoption. This work advances privacy-preserving frameworks,
supporting user trust and compliance across operational contexts.

摘要：大型語言模型 (LLM) 的採用徹底改變了 AI 應用，但在保護使用者隱私方面卻帶來重大挑戰。在解決細微的隱私風險時，確保符合 GDPR 和 CCPA 等隱私法規，需要強大且可擴充的架構。本文詳細探討了 OneShield Privacy Guard，這是一個旨在減輕使用者輸入和 LLM 輸出中隱私風險的架構，適用於企業和開源設定。我們分析了兩個實際部署：(1) 與資料和模型工廠整合的多語言隱私保護系統，專注於企業規模的資料治理；(2) PR Insights，一個強調自動分流和社群驅動改良的開源儲存庫。在部署 1 中，OneShield 在偵測 26 種語言中日期、姓名和電話號碼等敏感實體時，達到了 0.95 的 F1 分數，優於 StarPII 和 Presidio 等最先進的工具，高出 12%。部署 2 的平均 F1 分數為 0.86，在三個月內減少了超過 300 小時的徒手工作，準確地標記了 1,256 個提交請求的 8.25%，指出隱私風險並提高了情境敏感度。這些結果證明了 OneShield 在不同環境中的適應性和有效性，為情境感知實體辨識、自動化合規和道德 AI 採用提供了可行的見解。這項工作推動了隱私保護架構，支援使用者信任和跨作業情境的合規性。

##### **Learning segmentation from point trajectories**
2501.12392v1 by Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi

We consider the problem of segmenting objects in videos based on their motion
and no other forms of supervision. Prior work has often approached this problem
by using the principle of common fate, namely the fact that the motion of
points that belong to the same object is strongly correlated. However, most
authors have only considered instantaneous motion from optical flow. In this
work, we present a way to train a segmentation network using long-term point
trajectories as a supervisory signal to complement optical flow. The key
difficulty is that long-term motion, unlike instantaneous motion, is difficult
to model -- any parametric approximation is unlikely to capture complex motion
patterns over long periods of time. We instead draw inspiration from subspace
clustering approaches, proposing a loss function that seeks to group the
trajectories into low-rank matrices where the motion of object points can be
approximately explained as a linear combination of other point tracks. Our
method outperforms the prior art on motion-based segmentation, which shows the
utility of long-term motion and the effectiveness of our formulation.

摘要：我們考慮基於運動和沒有其他形式的監督來分割影片中的物體的問題。先前的研究通常透過使用共同命運的原理來探討這個問題，也就是屬於同一個物體的點的運動具有強關聯性。然而，大多數作者只考慮了光流的瞬時運動。在這項工作中，我們提出一個使用長期點軌跡作為監督訊號來訓練分割網路的方法，以補充光流。關鍵的難處在於，長期運動不像瞬時運動，難以建模——任何參數近似都不太可能捕捉到長時期的複雜運動模式。我們改為從子空間聚類方法中汲取靈感，提出一個損失函數，用於將軌跡分組成低秩矩陣，其中物體點的運動可以近似解釋為其他點軌跡的線性組合。我們的模型在基於運動的分割方面優於先前的技術，這顯示了長期運動的效用和我們公式的有效性。

##### **Physics of Skill Learning**
2501.12391v1 by Ziming Liu, Yizhou Liu, Eric J. Michaud, Jeff Gore, Max Tegmark

We aim to understand physics of skill learning, i.e., how skills are learned
in neural networks during training. We start by observing the Domino effect,
i.e., skills are learned sequentially, and notably, some skills kick off
learning right after others complete learning, similar to the sequential fall
of domino cards. To understand the Domino effect and relevant behaviors of
skill learning, we take physicists' approach of abstraction and simplification.
We propose three models with varying complexities -- the Geometry model, the
Resource model, and the Domino model, trading between reality and simplicity.
The Domino effect can be reproduced in the Geometry model, whose resource
interpretation inspires the Resource model, which can be further simplified to
the Domino model. These models present different levels of abstraction and
simplification; each is useful to study some aspects of skill learning. The
Geometry model provides interesting insights into neural scaling laws and
optimizers; the Resource model sheds light on the learning dynamics of
compositional tasks; the Domino model reveals the benefits of modularity. These
models are not only conceptually interesting -- e.g., we show how Chinchilla
scaling laws can emerge from the Geometry model, but also are useful in
practice by inspiring algorithmic development -- e.g., we show how simple
algorithmic changes, motivated by these toy models, can speed up the training
of deep learning models.

摘要：我們旨在了解技能學習的物理學，即在訓練過程中如何學習神經網路中的技能。我們從觀察多米諾骨牌效應開始，即技能是按順序學習的，值得注意的是，某些技能會在其他技能完成學習後立即開始學習，類似於多米諾骨牌按順序倒下的情況。為了了解多米諾骨牌效應和技能學習相關行為，我們採用物理學家的抽象化和簡化方法。我們提出了三種具有不同複雜性的模型——幾何模型、資源模型和多米諾模型，在現實和簡潔性之間進行權衡。多米諾骨牌效應可以在幾何模型中重現，其資源解釋激發了資源模型，而資源模型可以進一步簡化為多米諾模型。這些模型呈現出不同的抽象化和簡化層級；每一個都可用於研究技能學習的某些方面。幾何模型提供了對神經網路擴充法則和最佳化器的有趣見解；資源模型闡明了組合任務的學習動態；多米諾模型揭示了模組化的優點。這些模型不僅在概念上很有趣——例如，我們展示了欽奇拉擴充法則如何從幾何模型中出現，而且在實務上也很有用，因為它激發了演算法的開發——例如，我們展示了受這些玩具模型啟發的簡單演算法變更如何能加速深度學習模型的訓練。

##### **MMVU: Measuring Expert-Level Multi-Discipline Video Understanding**
2501.12380v1 by Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan

We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark
for evaluating foundation models in video understanding. MMVU includes 3,000
expert-annotated questions spanning 27 subjects across four core disciplines:
Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to
prior benchmarks, MMVU features three key advancements. First, it challenges
models to apply domain-specific knowledge and perform expert-level reasoning to
analyze specialized-domain videos, moving beyond the basic visual perception
typically assessed in current video benchmarks. Second, each example is
annotated by human experts from scratch. We implement strict data quality
controls to ensure the high quality of the dataset. Finally, each example is
enriched with expert-annotated reasoning rationals and relevant domain
knowledge, facilitating in-depth analysis. We conduct an extensive evaluation
of 32 frontier multimodal foundation models on MMVU. The latest
System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest
performance among the tested models. However, they still fall short of matching
human expertise. Through in-depth error analyses and case studies, we offer
actionable insights for future advancements in expert-level,
knowledge-intensive video understanding for specialized domains.

摘要：我們介紹 MMVU，一個全面的專家級、多領域基準，用於評估影片理解中的基礎模型。MMVU 包含 3,000 個專家註解問題，涵蓋四個核心領域的 27 個科目：科學、醫療保健、人文與社會科學以及工程。與先前的基準相比，MMVU 具備三大進展。首先，它挑戰模型應用特定領域的知識，並執行專家級推理，以分析特定領域的影片，超越當前影片基準中通常評估的基本視覺感知。其次，每個範例都是由人類專家從頭開始註解。我們實施嚴格的資料品質控管，以確保資料集的高品質。最後，每個範例都豐富了專家註解的推理原理和相關領域知識，促進深入分析。我們對 32 個前沿多模態基礎模型進行了廣泛的 MMVU 評估。最新的 System-2 能力模型 o1 和 Gemini 2.0 Flash Thinking 在測試模型中獲得最高效能。然而，它們仍然無法與人類專業知識相匹配。透過深入的錯誤分析和案例研究，我們為特定領域的專家級、知識密集型影片理解的未來進展提供了可行的見解。

##### **Enhancing Retrosynthesis with Conformer: A Template-Free Method**
2501.12434v1 by Jiaxi Zhuang, Qian Zhang, Ying Qian

Retrosynthesis plays a crucial role in the fields of organic synthesis and
drug development, where the goal is to identify suitable reactants that can
yield a target product molecule. Although existing methods have achieved
notable success, they typically overlook the 3D conformational details and
internal spatial organization of molecules. This oversight makes it challenging
to predict reactants that conform to genuine chemical principles, particularly
when dealing with complex molecular structures, such as polycyclic and
heteroaromatic compounds. In response to this challenge, we introduce a novel
transformer-based, template-free approach that incorporates 3D conformer data
and spatial information. Our approach includes an Atom-align Fusion module that
integrates 3D positional data at the input stage, ensuring correct alignment
between atom tokens and their respective 3D coordinates. Additionally, we
propose a Distance-weighted Attention mechanism that refines the self-attention
process, constricting the model s focus to relevant atom pairs in 3D space.
Extensive experiments on the USPTO-50K dataset demonstrate that our model
outperforms previous template-free methods, setting a new benchmark for the
field. A case study further highlights our method s ability to predict
reasonable and accurate reactants.

摘要：逆合成在有机合成和药物开发领域中扮演着至关重要的角色，其目标是识别出能够产生目标产物分子的合适的反应物。虽然现有的方法已经取得了显著的成功，但它们通常忽略了分子的 3D 构象细节和内部空间组织。这种疏忽使得预测符合真实化学原理的反应物变得具有挑战性，尤其是在处理复杂分子结构（例如多环和杂芳族化合物）时。为了应对这一挑战，我们引入了一种新颖的基于 Transformer 的无模板方法，该方法结合了 3D 构象数据和空间信息。我们的方法包括一个原子对齐融合模块，该模块在输入阶段集成了 3D 位置数据，确保原子标记与其各自的 3D 坐标之间正确对齐。此外，我们提出了一种距离加权注意力机制，该机制优化了自注意力过程，将模型的焦点限制在 3D 空间中的相关原子对上。在 USPTO-50K 数据集上的大量实验表明，我们的模型优于以往的无模板方法，为该领域树立了新的基准。案例研究进一步突出了我们方法预测合理且准确的反应物的能力。

##### **Video Depth Anything: Consistent Depth Estimation for Super-Long Videos**
2501.12375v2 by Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang

Depth Anything has achieved remarkable success in monocular depth estimation
with strong generalization ability. However, it suffers from temporal
inconsistency in videos, hindering its practical applications. Various methods
have been proposed to alleviate this issue by leveraging video generation
models or introducing priors from optical flow and camera poses. Nonetheless,
these methods are only applicable to short videos (< 10 seconds) and require a
trade-off between quality and computational efficiency. We propose Video Depth
Anything for high-quality, consistent depth estimation in super-long videos
(over several minutes) without sacrificing efficiency. We base our model on
Depth Anything V2 and replace its head with an efficient spatial-temporal head.
We design a straightforward yet effective temporal consistency loss by
constraining the temporal depth gradient, eliminating the need for additional
geometric priors. The model is trained on a joint dataset of video depth and
unlabeled images, similar to Depth Anything V2. Moreover, a novel
key-frame-based strategy is developed for long video inference. Experiments
show that our model can be applied to arbitrarily long videos without
compromising quality, consistency, or generalization ability. Comprehensive
evaluations on multiple video benchmarks demonstrate that our approach sets a
new state-of-the-art in zero-shot video depth estimation. We offer models of
different scales to support a range of scenarios, with our smallest model
capable of real-time performance at 30 FPS.

摘要：Depth Anything 在单目深度估计中取得了显著的成功，具有很强的泛化能力。然而，它在视频中存在时间不一致的问题，阻碍了其实际应用。已经提出了各种方法来通过利用视频生成模型或引入光流和相机位姿先验来缓解这个问题。尽管如此，这些方法仅适用于短视频（< 10 秒），并且需要在质量和计算效率之间进行权衡。我们提出了视频深度 Anything，用于在超长视频（几分钟以上）中进行高质量、一致的深度估计，而不会牺牲效率。我们基于 Depth Anything V2 构建我们的模型，并用一个高效的时空头部替换它的头部。我们通过约束时间深度梯度设计了一个简单但有效的时序一致性损失，消除了对附加几何先验的需求。该模型在与 Depth Anything V2 类似的视频深度和未标记图像的联合数据集上进行训练。此外，还开发了一种新颖的关键帧策略用于长视频推理。实验表明，我们的模型可以应用于任意长的视频，而不会损害质量、一致性或泛化能力。对多个视频基准的综合评估表明，我们的方法在零镜头视频深度估计中树立了新的最先进水平。我们提供不同规模的模型来支持各种场景，我们最小的模型能够以 30 FPS 的速度实时执行。

##### **Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists**
2501.12374v1 by Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan

Novel capacities of generative AI to analyze and generate cultural artifacts
raise inevitable questions about the nature and value of artistic education and
human expertise. Has AI already leveled the playing field between professional
artists and laypeople, or do trained artistic expressive capacity, curation
skills and experience instead enhance the ability to use these new tools? In
this pre-registered study, we conduct experimental comparisons between 50
active artists and a demographically matched sample of laypeople. We designed
two tasks to approximate artistic practice for testing their capabilities in
both faithful and creative image creation: replicating a reference image, and
moving as far away as possible from it. We developed a bespoke platform where
participants used a modern text-to-image model to complete both tasks. We also
collected and compared participants' sentiments towards AI. On average, artists
produced more faithful and creative outputs than their lay counterparts,
although only by a small margin. While AI may ease content creation,
professional expertise is still valuable - even within the confined space of
generative AI itself. Finally, we also explored how well an exemplary
vision-capable large language model (GPT-4o) would complete the same tasks, if
given the role of an image generation agent, and found it performed on par in
copying but outperformed even artists in the creative task. The very best
results were still produced by humans in both tasks. These outcomes highlight
the importance of integrating artistic skills with AI training to prepare
artists and other visual professionals for a technologically evolving
landscape. We see a potential in collaborative synergy with generative AI,
which could reshape creative industries and education in the arts.

摘要：生成式 AI 分析和生成文化製品的新穎能力
引發了關於藝術教育的性質和價值以及
人類專家知識的不可避免問題。AI 是否已經拉平了專業
藝術家和外行之間的競爭環境，或者訓練有素的藝術表現能力、策展
技巧和經驗反而增強了使用這些新工具的能力？在
這項預先註冊的研究中，我們對 50
位活躍藝術家和人口統計匹配的外行樣本進行了實驗比較。我們設計
了兩個任務來近似藝術實踐，以測試它們在
忠實和創造性圖像創作中的能力：複製參考圖像，並
盡可能遠離它。我們開發了一個訂製平台，讓
參與者使用現代文字轉圖像模型來完成這兩個任務。我們還
收集並比較了參與者對 AI 的情緒。平均而言，藝術家
產生的忠實且有創意的產出多於他們的非專業對手，
儘管只是一個很小的差距。雖然 AI 可能簡化了內容創作，
專業知識仍然有價值 - 即使在生成式 AI 本身的受限空間內也是如此。最後，我們還探討了一個範例
具有視覺能力的大語言模型 (GPT-4o) 在
充當圖像生成代理的情況下完成相同任務的效果如何，並發現它在
複製中表現得相當出色，但在創造性任務中甚至優於藝術家。最棒的
結果仍然是由人類在兩個任務中產生的。這些結果強調了將藝術技能與 AI 訓練相結合以準備
藝術家和其他視覺專業人士應對技術發展
情勢的重要性。我們看到與生成式 AI 合作協同的潛力，
這可能會重塑創意產業和藝術教育。

##### **Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL**
2501.12372v1 by Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan

Large Language Models (LLMs) have demonstrated impressive capabilities across
a range of natural language processing tasks. In particular, improvements in
reasoning abilities and the expansion of context windows have opened new
avenues for leveraging these powerful models. NL2SQL is challenging in that the
natural language question is inherently ambiguous, while the SQL generation
requires a precise understanding of complex data schema and semantics. One
approach to this semantic ambiguous problem is to provide more and sufficient
contextual information.
  In this work, we explore the performance and the latency trade-offs of the
extended context window (a.k.a., long context) offered by Google's
state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various
contextual information, including column example values, question and SQL query
pairs, user-provided hints, SQL documentation, and schema. To the best of our
knowledge, this is the first work to study how the extended context window and
extra contextual information can help NL2SQL generation with respect to both
accuracy and latency cost. We show that long context LLMs are robust and do not
get lost in the extended contextual information. Additionally, our long-context
NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve a strong
performance with 67.41\% on BIRD benchmark (dev) without finetuning and
expensive self-consistency based techniques.

摘要：大型語言模型 (LLM) 已在各種自然語言處理任務中展示出令人印象深刻的能力。特別是，推理能力的提升和上下文視窗的擴展為利用這些強大的模型開闢了新途徑。NL2SQL 具有挑戰性，因為自然語言問題本質上是模稜兩可的，而 SQL 生成需要精確理解複雜的數據模式和語義。解決這個語義模糊問題的一種方法是提供更多且充分的上下文資訊。
在這項工作中，我們探討了 Google 最先進的 LLM (\textit{gemini-1.5-pro}) 提供的擴展上下文視窗（又稱長上下文）的效能和延遲權衡。我們研究了各種上下文資訊的影響，包括欄位範例值、問題和 SQL 查詢配對、使用者提供的提示、SQL 文件和模式。據我們所知，這是第一個研究擴展上下文視窗和額外上下文資訊如何能在準確性和延遲成本方面協助 NL2SQL 生成的研究。我們展示了長上下文 LLM 是強健的，不會迷失在擴展的上下文資訊中。此外，我們基於 Google 的 \textit{gemini-pro-1.5} 的長上下文 NL2SQL 管線在 BIRD 基準測試 (dev) 上達到了 67.41% 的強勁效能，而無需微調和昂貴的基於自我一致性的技術。

##### **Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models**
2501.12370v1 by Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak

Scaling the capacity of language models has consistently proven to be a
reliable approach for improving performance and unlocking new capabilities.
Capacity can be primarily defined by two dimensions: the number of model
parameters and the compute per example. While scaling typically involves
increasing both, the precise interplay between these factors and their combined
contribution to overall capacity remains not fully understood. We explore this
relationship in the context of sparse Mixture-of-Expert models (MoEs), which
allow scaling the number of parameters without proportionally increasing the
FLOPs per example. We investigate how varying the sparsity level, i.e., the
ratio of non-active to total parameters, affects model performance in terms of
both pretraining and downstream performance. We find that under different
constraints (e.g. parameter size and total training compute), there is an
optimal level of sparsity that improves both training efficiency and model
performance. These results provide a better understanding of the impact of
sparsity in scaling laws for MoEs and complement existing works in this area,
offering insights for designing more efficient architectures.

摘要：擴展語言模型的容量一直被證明是改善效能並解鎖新功能的可靠方法。容量主要可以由兩個面向定義：模型參數數量和每個範例的運算。雖然擴展通常包含增加兩者，但這些因素之間的精確交互作用及其對整體容量的共同貢獻仍未完全了解。我們在稀疏混合專家模型 (MoE) 的背景下探討這種關係，它允許擴展參數數量，而不會成比例地增加每個範例的 FLOP。我們研究改變稀疏程度（即非活動參數與總參數的比率）如何影響模型在預訓練和下游效能方面的效能。我們發現，在不同的限制條件（例如參數大小和總訓練運算）下，存在最佳稀疏程度，可同時改善訓練效率和模型效能。這些結果讓我們對稀疏性在 MoE 的擴展定律中的影響有更好的了解，並補充了這方面的現有工作，提供了設計更有效率的架構的見解。

##### **InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model**
2501.12368v1 by Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang

Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at https://github.com/InternLM/InternLM-XComposer

摘要：儘管大型視覺語言模型 (LVLMs) 在視覺理解方面表現出色，但它們偶爾會產生不正確的輸出。雖然具有強化學習或測試時縮放的回饋模型 (RMs) 提供了提高生成品質的可能性，但仍存在一個關鍵差距：公開的多模態 LVLMs RMs 很少，而且專有模型的實作細節往往不清楚。我們以 InternLM-XComposer2.5-Reward (IXC-2.5-Reward) 來彌補這個差距，這是一個簡單但有效的多模態回饋模型，它將 LVLMs 與人類偏好保持一致。為了確保 IXC-2.5-Reward 的穩健性和多功能性，我們建立了一個跨越不同領域（例如指令遵循、一般理解、文字豐富的文件、數學推理和影片理解）的高品質多模態偏好語料庫，其中包含文字、影像和影片輸入。IXC-2.5-Reward 在最新的多模態回饋模型基準測試中取得了優異的成果，並在純文字回饋模型基準測試中展現了競爭力。我們進一步展示了 IXC-2.5-Reward 的三個關鍵應用：(1) 提供 RL 訓練的監督訊號。我們將 IXC-2.5-Reward 與近端策略最佳化 (PPO) 整合，產生 IXC-2.5-Chat，在指令遵循和多模態開放式對話方面持續改善；(2) 從候選回應中選出最佳回應，以進行測試時縮放；(3) 從現有的影像和影片教學調整訓練資料中過濾異常值或雜訊樣本。為了確保可複製性並促進進一步研究，我們已在 https://github.com/InternLM/InternLM-XComposer 開放原始碼的所有模型權重和訓練配方

##### **Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models**
2501.12433v1 by Tabinda Aman, Mohammad Nadeem, Shahab Saquib Sohail, Mohammad Anas, Erik Cambria

Animal stereotypes are deeply embedded in human culture and language. They
often shape our perceptions and expectations of various species. Our study
investigates how animal stereotypes manifest in vision-language models during
the task of image generation. Through targeted prompts, we explore whether
DALL-E perpetuates stereotypical representations of animals, such as "owls as
wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped
instances where the model consistently generates images aligned with cultural
biases. The current work is the first of its kind to examine animal
stereotyping in vision-language models systematically and to highlight a
critical yet underexplored dimension of bias in AI-generated visual content.

摘要：動物刻板印象深深植根於人類文化和語言中。它們
通常塑造我們對各種物種的看法和期望。我們的研究
探討了在影像生成任務中，動物刻板印象如何在視覺語言模型中顯現。透過有針對性的提示，我們探討
DALL-E 是否延續了動物的刻板印象，例如「貓頭鷹是明智的」、「狐狸是不忠誠的」等。我們的發現揭示了顯著的刻板印象，其中該模型始終生成與文化
偏見一致的影像。目前的研究是同類型研究中第一個系統性地檢視視覺語言模型中的動物刻板印象，並強調 AI 生成的視覺內容中一個關鍵但未充分探討的偏見面向。

##### **Test-time regression: a unifying framework for designing sequence models with associative memory**
2501.12352v1 by Ke Alexander Wang, Jiaxin Shi, Emily B. Fox

Sequences provide a remarkably general way to represent and process
information. This powerful abstraction has placed sequence modeling at the
center of modern deep learning applications, inspiring numerous architectures
from transformers to recurrent networks. While this fragmented development has
yielded powerful models, it has left us without a unified framework to
understand their fundamental similarities and explain their effectiveness. We
present a unifying framework motivated by an empirical observation: effective
sequence models must be able to perform associative recall. Our key insight is
that memorizing input tokens through an associative memory is equivalent to
performing regression at test-time. This regression-memory correspondence
provides a framework for deriving sequence models that can perform associative
recall, offering a systematic lens to understand seemingly ad-hoc architectural
choices. We show numerous recent architectures -- including linear attention
models, their gated variants, state-space models, online learners, and softmax
attention -- emerge naturally as specific approaches to test-time regression.
Each architecture corresponds to three design choices: the relative importance
of each association, the regressor function class, and the optimization
algorithm. This connection leads to new understanding: we provide theoretical
justification for QKNorm in softmax attention, and we motivate higher-order
generalizations of softmax attention. Beyond unification, our work unlocks
decades of rich statistical tools that can guide future development of more
powerful yet principled sequence models.

摘要：序列提供了一個非常通用的方式來表示和處理資訊。這種強大的抽象化已將序列模型置於現代深度學習應用程式的核心，從Transformer到遞迴網路激發了許多架構。雖然這種分散的發展產生了強大的模型，但它讓我們沒有統一的架構來理解它們的基本相似性並解釋它們的有效性。我們提出了一個由經驗觀察激勵的統一架構：有效的序列模型必須能夠執行關聯式回憶。我們的關鍵見解是，透過聯想記憶體記憶輸入代幣等同於在測試時執行迴歸。這種迴歸記憶對應提供了一個框架，用於推導可以執行關聯式回憶的序列模型，提供了一個系統的透鏡來理解看似臨時的架構選擇。我們展示了許多最近的架構——包括線性注意力模型、它們的門控變體、狀態空間模型、線上學習器和 softmax 注意力——自然而然地出現作為測試時間迴歸的具體方法。每個架構都對應於三個設計選擇：每個關聯的相對重要性、回歸函數類和最佳化演算法。這種聯繫導致了新的理解：我們為 softmax 注意力中的 QKNorm 提供了理論依據，並且我們激勵了 softmax 注意力的高階概括。除了統一之外，我們的研究還解鎖了數十年的豐富統計工具，這些工具可以指導更強大但有原則的序列模型的未來發展。

##### **Treefix: Enabling Execution with a Tree of Prefixes**
2501.12339v1 by Beatriz Souza, Michael Pradel

The ability to execute code is a prerequisite for various dynamic program
analyses. Learning-guided execution has been proposed as an approach to enable
the execution of arbitrary code snippets by letting a neural model predict
likely values for any missing variables. Although state-of-the-art
learning-guided execution approaches, such as LExecutor, can enable the
execution of a relative high amount of code, they are limited to predicting a
restricted set of possible values and do not use any feedback from previous
executions to execute even more code. This paper presents Treefix, a novel
learning-guided execution approach that leverages LLMs to iteratively create
code prefixes that enable the execution of a given code snippet. The approach
addresses the problem in a multi-step fashion, where each step uses feedback
about the code snippet and its execution to instruct an LLM to improve a
previously generated prefix. This process iteratively creates a tree of
prefixes, a subset of which is returned to the user as prefixes that maximize
the number of executed lines in the code snippet. In our experiments with two
datasets of Python code snippets, Treefix achieves 25% and 7% more coverage
relative to the current state of the art in learning-guided execution, covering
a total of 84% and 82% of all lines in the code snippets.

摘要：執行程式碼的能力是各種動態程式分析的前提。學習導向執行已被提議為一種方法，讓神經模型預測任何遺失變數的可能值，從而能夠執行任意程式碼片段。儘管最先進的學習導向執行方法（例如 LExecutor）可以執行相對大量的程式碼，但它們僅限於預測一組可能的受限值，並且不使用先前執行的任何回饋來執行更多程式碼。本文提出 Treefix，這是一種新穎的學習導向執行方法，它利用 LLM 迭代建立程式碼前綴，以執行給定的程式碼片段。該方法以多步驟方式解決問題，其中每一步都使用關於程式碼片段及其執行的回饋，來指導 LLM 改進先前生成的字首。此過程反覆建立一個前綴樹，其中的一部分作為前綴回傳給使用者，以最大化程式碼片段中執行行的數量。在我們使用兩個 Python 程式碼片段資料集進行的實驗中，Treefix 在學習導向執行中取得了比目前最先進的技術高出 25% 和 7% 的覆蓋率，總共涵蓋了程式碼片段中所有行的 84% 和 82%。

##### **FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**
2501.12336v1 by Phuoc Duong Huy Chu

This paper presents results of our system for CoMeDi Shared Task, focusing on
Subtask 2: Disagreement Ranking. Our system leverages sentence embeddings
generated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep
neural regression model incorporating batch normalization and dropout for
improved generalization. By predicting the mean of pairwise judgment
differences between annotators, our method explicitly targets disagreement
ranking, diverging from traditional "gold label" aggregation approaches. We
optimized our system with a customized architecture and training procedure,
achieving competitive performance in Spearman correlation against mean
disagreement labels. Our results highlight the importance of robust embeddings,
effective model architecture, and careful handling of judgment differences for
ranking disagreement in multilingual contexts. These findings provide insights
into the use of contextualized representations for ordinal judgment tasks and
open avenues for further refinement of disagreement prediction models.

摘要：本文展示了我們在 CoMeDi 共享任務系統中的結果，重點在
子任務 2：分歧排名。我們的系統利用 paraphrase-xlm-r-multilingual-v1 模型產生的句子嵌入，結合深度
神經迴歸模型，並加入批次正規化和中斷以改善概化。透過預測註解者之間成對判斷差異的平均值，我們的
方法明確針對分歧排名，偏離傳統的「黃金標籤」聚合方法。我們使用自訂架構和訓練程序優化系統，
在與平均分歧標籤的 Spearman 相關性中獲得競爭力表現。我們的結果強調了穩健嵌入、有效模型架構和
謹慎處理判斷差異對於在多語言環境中對分歧進行排名的重要性。這些發現提供了使用情境化表徵進行序數判斷任務的見解，並為進一步優化分歧預測模型開闢了道路。

##### **Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration**
2501.12332v1 by Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong

Acquiring labelled training data remains a costly task in real world machine
learning projects to meet quantity and quality requirements. Recently Large
Language Models (LLMs), notably GPT-4, have shown great promises in labelling
data with high accuracy. However, privacy and cost concerns prevent the
ubiquitous use of GPT-4. In this work, we explore effectively leveraging
open-source models for automatic labelling. We identify integrating label
schema as a promising technology but found that naively using the label
description for classification leads to poor performance on high cardinality
tasks. To address this, we propose Retrieval Augmented Classification (RAC) for
which LLM performs inferences for one label at a time using corresponding label
schema; we start with the most related label and iterates until a label is
chosen by the LLM. We show that our method, which dynamically integrates label
description, leads to performance improvements in labelling tasks. We further
show that by focusing only on the most promising labels, RAC can trade off
between label quality and coverage - a property we leverage to automatically
label our internal datasets.

摘要：在實際機器學習專案中，取得標籤訓練資料仍然是一項成本高昂的任務，以滿足數量和品質需求。最近，大型語言模型 (LLM)，尤其是 GPT-4，在標籤資料上展現出高精準度的優異表現。然而，隱私和成本考量阻礙了 GPT-4 的廣泛使用。在這項工作中，我們探討如何有效利用開源模型進行自動標籤。我們發現整合標籤架構是一項有前途的技術，但發現天真地使用標籤說明進行分類會導致高基數任務的效能不佳。為了解決這個問題，我們提出了檢索增強分類 (RAC)，其中 LLM 使用對應的標籤架構一次對一個標籤執行推論；我們從最相關的標籤開始，並反覆運算，直到 LLM 選擇一個標籤。我們展示了我們的方法（動態整合標籤說明）會提升標籤任務的效能。我們進一步展示，透過僅專注於最有希望的標籤，RAC 可以權衡標籤品質和涵蓋範圍，我們利用這項特性自動標籤我們的內部資料集。

##### **UI-TARS: Pioneering Automated GUI Interaction with Native Agents**
2501.12326v1 by Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi

This paper introduces UI-TARS, a native GUI agent model that solely perceives
the screenshots as input and performs human-like interactions (e.g., keyboard
and mouse operations). Unlike prevailing agent frameworks that depend on
heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts
and workflows, UI-TARS is an end-to-end model that outperforms these
sophisticated frameworks. Experiments demonstrate its superior performance:
UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating
perception, grounding, and GUI task execution. Notably, in the OSWorld
benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15
steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,
UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several
key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of
GUI screenshots for context-aware understanding of UI elements and precise
captioning; (2) Unified Action Modeling, which standardizes actions into a
unified space across platforms and achieves precise grounding and interaction
through large-scale action traces; (3) System-2 Reasoning, which incorporates
deliberate reasoning into multi-step decision making, involving multiple
reasoning patterns such as task decomposition, reflection thinking, milestone
recognition, etc. (4) Iterative Training with Reflective Online Traces, which
addresses the data bottleneck by automatically collecting, filtering, and
reflectively refining new interaction traces on hundreds of virtual machines.
Through iterative training and reflection tuning, UI-TARS continuously learns
from its mistakes and adapts to unforeseen situations with minimal human
intervention. We also analyze the evolution path of GUI agents to guide the
further development of this domain.

摘要：本文介紹了 UI-TARS，一種原生 GUI 代理模型，它僅將螢幕截圖視為輸入，並執行類似人類的互動（例如，鍵盤和滑鼠操作）。與依賴於專家精心製作的提示和工作流程的盛行代理架構不同，UI-TARS 是一個端到端模型，其效能優於這些複雜的架構。實驗證明了其卓越的效能：UI-TARS 在 10 多個 GUI 代理基準測試中取得 SOTA 效能，評估感知、基礎和 GUI 任務執行。值得注意的是，在 OSWorld 基準測試中，UI-TARS 在 50 個步驟中取得 24.6 分，在 15 個步驟中取得 22.7 分，優於 Claude（分別為 22.0 和 14.9）。在 AndroidWorld 中，UI-TARS 取得 46.6 分，超越 GPT-4o（34.5）。UI-TARS 融合了多項關鍵創新：(1) 增強感知：利用大規模 GUI 螢幕截圖資料集，以情境感知的方式理解 UI 元素並進行精確標題說明；(2) 統一動作建模，將動作標準化到跨平台的統一空間，並透過大規模動作追蹤，實現精確的基礎和互動；(3) 系統 2 推理，將深思熟慮的推理納入多步驟決策制定中，涉及多種推理模式，例如任務分解、反思思考、里程碑識別等；(4) 透過反思性線上追蹤進行反覆訓練，透過在數百台虛擬機器上自動收集、過濾和反思性地精煉新的互動追蹤，來解決資料瓶頸問題。透過反覆訓練和反思調整，UI-TARS 不斷從其錯誤中學習，並在最少的人為干預下適應無法預見的情況。我們也分析了 GUI 代理的演進路徑，以引導此領域的進一步發展。

##### **LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations**
2501.12300v1 by Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi

While learning personalization offers great potential for learners, modern
practices in higher education require a deeper consideration of domain models
and learning contexts, to develop effective personalization algorithms. This
paper introduces an innovative approach to higher education curriculum
modelling that utilizes large language models (LLMs) for knowledge graph (KG)
completion, with the goal of creating personalized learning-path
recommendations. Our research focuses on modelling university subjects and
linking their topics to corresponding domain models, enabling the integration
of learning modules from different faculties and institutions in the student's
learning path. Central to our approach is a collaborative process, where LLMs
assist human experts in extracting high-quality, fine-grained topics from
lecture materials. We develop a domain, curriculum, and user models for
university modules and stakeholders. We implement this model to create the KG
from two study modules: Embedded Systems and Development of Embedded Systems
Using FPGA. The resulting KG structures the curriculum and links it to the
domain models. We evaluate our approach through qualitative expert feedback and
quantitative graph quality metrics. Domain experts validated the relevance and
accuracy of the model, while the graph quality metrics measured the structural
properties of our KG. Our results show that the LLM-assisted graph completion
approach enhances the ability to connect related courses across disciplines to
personalize the learning experience. Expert feedback also showed high
acceptance of the proposed collaborative approach for concept extraction and
classification.

摘要：<paragraph>在學習個人化提供學習者巨大潛力的同時，高等教育中的現代實務需要更深入地考慮領域模型和學習情境，以開發有效的個人化演算法。本文介紹了一種創新的高等教育課程建模方法，該方法利用大型語言模型 (LLM) 來完成知識圖譜 (KG)，目的是建立個人化的學習路徑建議。我們的研究重點在於建模大學科目，並將它們的主題連結到對應的領域模型，從而能夠將來自不同院系和機構的學習模組整合到學生的學習路徑中。我們的做法核心是一個協作流程，其中 LLM 協助人類專家從講義材料中萃取高品質、細緻的主題。我們為大學模組和利害關係人開發了領域、課程和使用者模型。我們實作這個模型，從兩個研究模組建立 KG：嵌入式系統和使用 FPGA 的嵌入式系統開發。產生的 KG 建構了課程並將其連結到領域模型。我們透過定性專家回饋和定量圖形品質指標來評估我們的做法。領域專家驗證了模型的相關性和準確性，而圖形品質指標則測量了我們 KG 的結構特性。我們的結果顯示，LLM 輔助的圖形完成方法增強了跨學科連結相關課程的能力，以個人化學習體驗。專家回饋也顯示高度接受所提出的協作方法，用於概念萃取和分類。</paragraph>

##### **RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning**
2501.12296v1 by Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue

In the pursuit of robust autonomous driving systems, models trained on
real-world datasets often struggle to adapt to new environments, particularly
when confronted with corner cases such as extreme weather conditions.
Collecting these corner cases in the real world is non-trivial, which
necessitates the use of simulators for validation. However,the high
computational cost and the domain gap in data distribution have hindered the
seamless transition between real and simulated driving scenarios. To tackle
this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving
(RALAD), a novel framework designed to bridge the real-to-sim gap at a low
cost. RALAD features three primary designs, including (1) domain adaptation via
an enhanced Optimal Transport (OT) method that accounts for both individual and
grouped image distances, (2) a simple and unified framework that can be applied
to various models, and (3) efficient fine-tuning techniques that freeze the
computationally expensive layers while maintaining robustness. Experimental
results demonstrate that RALAD compensates for the performance degradation in
simulated environments while maintaining accuracy in real-world scenarios
across three different models. Taking Cross View as an example, the mIOU and
mAP metrics in real-world scenarios remain stable before and after RALAD
fine-tuning, while in simulated environments,the mIOU and mAP metrics are
improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of
our approach is reduced by approximately 88.1%. Our code is available at
https://github.com/JiachengZuo/RALAD.git.

摘要：<paragraph>在追求穩健的自動駕駛系統時，使用真實世界資料集訓練的模型通常難以適應新環境，特別是在遇到極端天氣條件等極端情況時。在現實世界中收集這些極端情況並非易事，這需要使用模擬器進行驗證。然而，高計算成本和資料分佈中的領域差距阻礙了真實和模擬駕駛場景之間的無縫過渡。為了應對這一挑戰，我們提出了用於自動駕駛的檢索增強學習 (RALAD)，這是一個新穎的框架，旨在以低成本彌合真實與模擬的差距。RALAD 具有三項主要設計，包括 (1) 透過一種增強的最佳傳輸 (OT) 方法進行領域適應，該方法考慮了個別和群組影像距離，(2) 一個簡單且統一的框架，可應用於各種模型，以及 (3) 有效的微調技術，可在維持穩健性的同時凍結計算成本高的層。實驗結果表明，RALAD 補償了模擬環境中的效能下降，同時在三個不同的模型中維持了真實世界場景中的準確性。以 Cross View 為例，RALAD 微調前後，真實世界場景中的 mIOU 和 mAP 指標保持穩定，而在模擬環境中，mIOU 和 mAP 指標分別提高了 10.30% 和 12.29%。此外，我們的方法的重新訓練成本降低了大約 88.1%。我們的程式碼可在 https://github.com/JiachengZuo/RALAD.git 中取得。</paragraph>

##### **Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation**
2501.12432v1 by Dongsheng Zhu, Weixian Shi, Zhengliang Shi, Zhaochun Ren, Shuaiqiang Wang, Lingyong Yan, Dawei Yin

Although current Large Language Models (LLMs) exhibit impressive
capabilities, performing complex real-world tasks still requires tool learning.
Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to
interact with external environments, but they are limited in perceptual scope
and lack adequate task-planning capability. To address these limitations, other
studies introduce the first Search-based Decision Tree (DFSDT), which still
suffers from the high computational cost. In this paper, we introduce a novel
parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).
First, we transform traditional tree-based tool search paths into Directed
Acyclic Graph (DAG) structure, generating a high-quality parallel tool
invocation dataset. The DTA-Llama is then trained on the dataset to learn to
iteratively divide the current task into several parallel tool invocation
sub-tasks and aggregate the invocation results to decide the next actions.
Furthermore, we introduce an efficient inference framework inspired by the
Process/Threads mechanism when applying the DTA-Llama to practical tasks.
Experimental results show that our approach substantially enhances task
performance while reducing token consumption and inference time. Llama2-7B,
using our method, is comparable to the official parallel function calling
method of GPT-3.5. The relevant code, dataset, and model weights are available
at https://corn0205.github.io/

摘要：儘管目前的大型語言模型 (LLM) 展現出令人印象深刻的能力，但執行複雜的真實世界任務仍需要工具學習。主流方法（例如 CoT/ReAct）依賴逐步工具呼叫與外部環境互動，但它們的感知範圍有限，且缺乏足夠的任務規劃能力。為了解決這些限制，其他研究引入了第一個基於搜尋的決策樹 (DFSDT)，但仍有很高的運算成本。在本文中，我們介紹了一種新穎的平行工具呼叫範例，DTA-Llama（分而合之 Llama）。首先，我們將傳統的基於樹的工具搜尋路徑轉換為有向無環圖 (DAG) 結構，產生高品質的平行工具呼叫資料集。然後在資料集上訓練 DTA-Llama，學習反覆將當前任務分成幾個平行工具呼叫子任務，並彙總呼叫結果以決定後續動作。此外，我們在將 DTA-Llama 應用於實際任務時，引入了一個受 Process/Threads 機制啟發的高效推論框架。實驗結果表明，我們的做法大幅提升了任務效能，同時減少了符號消耗和推論時間。使用我們方法的 Llama2-7B，可與 GPT-3.5 的官方平行函式呼叫方法相媲美。相關程式碼、資料集和模型權重可在 https://corn0205.github.io/ 取得

##### **Modality Interactive Mixture-of-Experts for Fake News Detection**
2501.12431v1 by Yifan Liu, Yaokun Liu, Zelin Li, Ruichen Yao, Yang Zhang, Dong Wang

The proliferation of fake news on social media platforms disproportionately
impacts vulnerable populations, eroding trust, exacerbating inequality, and
amplifying harmful narratives. Detecting fake news in multimodal contexts --
where deceptive content combines text and images -- is particularly challenging
due to the nuanced interplay between modalities. Existing multimodal fake news
detection methods often emphasize cross-modal consistency but ignore the
complex interactions between text and visual elements, which may complement,
contradict, or independently influence the predicted veracity of a post. To
address these challenges, we present Modality Interactive Mixture-of-Experts
for Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts
framework designed to enhance multimodal fake news detection by explicitly
modeling modality interactions through an interaction gating mechanism. Our
approach models modality interactions by evaluating two key aspects of modality
interactions: unimodal prediction agreement and semantic alignment. The
hierarchical structure of MIMoE-FND allows for distinct learning pathways
tailored to different fusion scenarios, adapting to the unique characteristics
of each modality interaction. By tailoring fusion strategies to diverse
modality interaction scenarios, MIMoE-FND provides a more robust and nuanced
approach to multimodal fake news detection. We evaluate our approach on three
real-world benchmarks spanning two languages, demonstrating its superior
performance compared to state-of-the-art methods. By enhancing the accuracy and
interpretability of fake news detection, MIMoE-FND offers a promising tool to
mitigate the spread of misinformation, with the potential to better safeguard
vulnerable communities against its harmful effects.

摘要：社群媒體平台上假新聞的激增對弱勢族群造成不成比例的影響，侵蝕信任、加劇不平等，並擴大有害的敘述。在多模態脈絡中偵測假新聞（其中具有欺騙性的內容結合文字和影像）特別具有挑戰性，這是因為不同模態之間的交互作用具有細微差別。現有的多模態假新聞偵測方法通常強調跨模態一致性，但忽略了文字和視覺元素之間的複雜交互作用，這些交互作用可能補充、矛盾或獨立影響貼文的預測真實性。為了應對這些挑戰，我們提出用於假新聞偵測的模態互動專家混合（MIMoE-FND），這是一個新穎的分層式專家混合架構，旨在透過互動閘控機制明確地建模模態交互作用，以增強多模態假新聞偵測。我們的做法透過評估模態交互作用的兩個關鍵面向（單模態預測一致性和語義對齊）來建模模態交互作用。MIMoE-FND 的分層結構允許不同的學習路徑，針對不同的融合情境量身打造，適應每個模態交互作用的獨特特徵。透過針對不同的模態交互作用情境量身打造融合策略，MIMoE-FND 提供一種更強大且細緻的多模態假新聞偵測方法。我們在橫跨兩種語言的三個真實世界基準上評估我們的做法，證明其效能優於最先進的方法。透過增強假新聞偵測的準確性和可解釋性，MIMoE-FND 提供了一個有希望的工具來減輕錯誤訊息的散布，並有可能更好地保護弱勢族群免於其有害影響。

##### **With Great Backbones Comes Great Adversarial Transferability**
2501.12275v1 by Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, Aram H. Markosyan

Advances in self-supervised learning (SSL) for machine vision have improved
representation robustness and model performance, giving rise to pre-trained
backbones like \emph{ResNet} and \emph{ViT} models tuned with SSL methods such
as \emph{SimCLR}. Due to the computational and data demands of pre-training,
the utilization of such backbones becomes a strenuous necessity. However,
employing these backbones may inherit vulnerabilities to adversarial attacks.
While adversarial robustness has been studied under \emph{white-box} and
\emph{black-box} settings, the robustness of models tuned on pre-trained
backbones remains largely unexplored. Additionally, the role of tuning
meta-information in mitigating exploitation risks is unclear. This work
systematically evaluates the adversarial robustness of such models across
$20,000$ combinations of tuning meta-information, including fine-tuning
techniques, backbone families, datasets, and attack types. We propose using
proxy models to transfer attacks, simulating varying levels of target knowledge
by fine-tuning these proxies with diverse configurations. Our findings reveal
that proxy-based attacks approach the effectiveness of \emph{white-box}
methods, even with minimal tuning knowledge. We also introduce a naive
"backbone attack," leveraging only the backbone to generate adversarial
samples, which outperforms \emph{black-box} attacks and rivals \emph{white-box}
methods, highlighting critical risks in model-sharing practices. Finally, our
ablations reveal how increasing tuning meta-information impacts attack
transferability, measuring each meta-information combination.

摘要：<paragraph>機器視覺的自監督學習 (SSL) 進步提升了表示穩健性和模型效能，產生預先訓練的骨幹，例如使用 SSL 方法（例如 SimCLR）調整的 ResNet 和 ViT 模型。由於預先訓練的運算和資料需求，使用此類骨幹變得極為必要。然而，採用這些骨幹可能會繼承對抗攻擊的漏洞。儘管在白盒和黑盒設定下已研究對抗穩健性，但預先訓練骨幹上調整的模型的穩健性仍未得到充分探討。此外，調整元資訊在減輕利用風險中的作用尚不清楚。本研究系統性地評估此類模型在 20,000 種調整元資訊組合中的對抗穩健性，包括微調技術、骨幹系列、資料集和攻擊類型。我們建議使用代理模型來傳輸攻擊，透過使用不同的組態微調這些代理模型來模擬不同層級的目標知識。我們的研究結果顯示，即使調整知識最少，基於代理的攻擊也接近白盒方法的有效性。我們還引入了一個天真的「骨幹攻擊」，僅利用骨幹產生對抗樣本，其表現優於黑盒攻擊，並與白盒方法相抗衡，突顯模型分享實務中的關鍵風險。最後，我們的消融實驗揭示了增加調整元資訊如何影響攻擊可傳遞性，並衡量每個元資訊組合。</paragraph>

##### **Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement**
2501.12273v1 by Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen

The quality of Supervised Fine-Tuning (SFT) data plays a critical role in
enhancing the conversational capabilities of Large Language Models (LLMs).
However, as LLMs become more advanced, the availability of high-quality
human-annotated SFT data has become a significant bottleneck, necessitating a
greater reliance on synthetic training data. In this work, we introduce Condor,
a novel two-stage synthetic data generation framework that incorporates World
Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data
at scale. Our experimental results demonstrate that a base model fine-tuned on
only 20K Condor-generated samples achieves superior performance compared to
counterparts. The additional refinement stage in Condor further enables
iterative self-improvement for LLMs at various scales (up to 72B), validating
the effectiveness of our approach. Furthermore, our investigation into the
scaling for synthetic data in post-training reveals substantial unexplored
potential for performance improvements, opening promising avenues for future
research.

摘要：監督式微調 (SFT) 資料的品質對於增強大型語言模型 (LLM) 的對話能力發揮關鍵作用。然而，隨著 LLM 變得越來越先進，高品質人工標註 SFT 資料的可用性已成為一個重大的瓶頸，這使得對合成訓練資料的依賴性越來越高。在這項工作中，我們介紹了 Condor，一個新穎的兩階段合成資料生成架構，它結合了世界知識樹和自我反省精煉，以大規模產生高品質的 SFT 資料。我們的實驗結果表明，僅針對 20K 個 Condor 生成的範例進行微調的基本模型，與同類型模型相比，可獲得優越的效能。Condor 中的額外精煉階段進一步讓 LLM 能夠在各種規模（最高 72B）下進行反覆自我改善，驗證了我們方法的有效性。此外，我們對訓練後合成資料的擴展性進行調查，揭示了效能改善的巨大未開發潛力，為未來的研究開闢了有前景的途徑。

##### **CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**
2501.12266v1 by Cristiano Patrício, Isabel Rio-Torto, Jaime S. Cardoso, Luís F. Teixeira, João C. Neves

The main challenges limiting the adoption of deep learning-based solutions in
medical workflows are the availability of annotated data and the lack of
interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the
latter by constraining the final disease prediction on a set of predefined and
human-interpretable concepts. However, the increased interpretability achieved
through these concept-based explanations implies a higher annotation burden.
Moreover, if a new concept needs to be added, the whole system needs to be
retrained. Inspired by the remarkable performance shown by Large
Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet
effective, methodology, CBVLM, which tackles both of the aforementioned
challenges. First, for each concept, we prompt the LVLM to answer if the
concept is present in the input image. Then, we ask the LVLM to classify the
image based on the previous concept predictions. Moreover, in both stages, we
incorporate a retrieval module responsible for selecting the best examples for
in-context learning. By grounding the final diagnosis on the predicted
concepts, we ensure explainability, and by leveraging the few-shot capabilities
of LVLMs, we drastically lower the annotation cost. We validate our approach
with extensive experiments across four medical datasets and twelve LVLMs (both
generic and medical) and show that CBVLM consistently outperforms CBMs and
task-specific supervised methods without requiring any training and using just
a few annotated examples. More information on our project page:
https://cristianopatricio.github.io/CBVLM/.

摘要：限制在醫療工作流程中採用基於深度學習的解決方案的主要挑戰是標記資料的可用性以及此類系統的可解釋性不足。概念瓶頸模型 (CBM) 透過限制一組預定義且人類可解釋的概念對最終疾病預測，來解決後者。然而，透過這些基於概念的解釋所實現的可解釋性提升，意味著更高的標記負擔。此外，如果需要新增一個新概念，則需要重新訓練整個系統。受到大型視覺語言模型 (LVLMs) 在小樣本設定中展現的卓越效能啟發，我們提出了一個簡單但有效的 CBVLM 方法，來解決上述兩個挑戰。首先，對於每個概念，我們提示 LVLM 回答輸入影像中是否包含該概念。然後，我們要求 LVLM 根據先前的概念預測對影像進行分類。此外，在兩個階段中，我們都納入一個檢索模組，負責選出最適合於情境學習的範例。透過將最終診斷建立在預測概念之上，我們確保了可解釋性，並透過利用 LVLMs 的小樣本能力，我們大幅降低了標記成本。我們透過四個醫療資料集和十二個 LVLM（通用和醫療）的廣泛實驗驗證了我們的作法，並顯示 CBVLM 在無需任何訓練且僅使用少數標記範例的情況下，始終優於 CBM 和特定於任務的監督式方法。更多資訊請見我們的專案頁面：https://cristianopatricio.github.io/CBVLM/。

##### **FOCUS: First Order Concentrated Updating Scheme**
2501.12243v1 by Yizhou Liu, Ziming Liu, Jeff Gore

Large language models (LLMs) demonstrate remarkable performance, and
improving their pre-training process appears to be key to enhancing their
capabilities further. Based on the documented success of Adam, learning rate
decay, and weight decay, we hypothesize that the pre-training loss landscape
features a narrowing valley structure. Through experiments with synthetic loss
functions, we discover that when gradient query noise is high relative to the
valley's sharpness, Adam's performance falls behind that of Signum because Adam
reduces the effective step size too drastically. This observation led us to
develop FOCUS, an optimizer that enhances Signum by incorporating attraction
toward moving averaged parameters, allowing it to handle noise better while
maintaining larger step sizes. In training GPT-2, FOCUS proves to be more
stable than Signum and faster than Adam. These results suggest that gradient
noise may be an underappreciated limiting factor in LLM training, and FOCUS
offers promising solutions.

摘要：大型語言模型 (LLM) 展現出卓越的效能，而改善其預訓練程序似乎是進一步提升其功能的關鍵。根據 Adam、學習率衰減和權重衰減的已記錄成功，我們假設預訓練損失景觀具有縮小的谷地結構。透過對合成損失函數的實驗，我們發現當梯度查詢雜訊相對於谷地的銳利度較高時，Adam 的效能會落後於 Signum，因為 Adam 會過度大幅度地減少有效步驟大小。這個觀察讓我們開發出 FOCUS，一種透過納入對移動平均參數的吸引力來增強 Signum 的最佳化器，讓它可以在維持較大步驟大小的同時更好地處理雜訊。在訓練 GPT-2 時，FOCUS 證明比 Signum 更穩定，而且比 Adam 更快。這些結果表明，梯度雜訊可能是 LLM 訓練中一個未被充分重視的限制因素，而 FOCUS 提供了有希望的解決方案。

##### **InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models**
2501.12231v1 by Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min

The improved competence of generative models can help building multi-modal
virtual assistants that leverage modalities beyond language. By observing
humans performing multi-step tasks, one can build assistants that have
situational awareness of actions and tasks being performed, enabling them to
cater assistance based on this understanding. In this paper, we develop a
Context-aware Instructional Task Assistant with Multi-modal Large Language
Models (InsTALL) that leverages an online visual stream (e.g. a user's screen
share or video recording) and responds in real-time to user queries related to
the task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal
model on task videos and paired textual data, and 2) automatically extracts
task graph from video data and leverages it at training and inference time. We
show InsTALL achieves state-of-the-art performance across proposed sub-tasks
considered for multimodal activity understanding -- task recognition (TR),
action recognition (AR), next action prediction (AP), and plan prediction (PP)
-- and outperforms existing baselines on two novel sub-tasks related to
automatic error identification.

摘要：生成模型能力的提升有助于构建利用语言之外的多模态虚拟助手。通过观察人类执行多步骤任务，可以构建对正在执行的动作和任务有情境感知的助手，使他们能够根据这种理解提供帮助。在本文中，我们开发了一个具有多模态大语言模型的上下文感知指令任务助手 (InsTALL)，该助手利用在线视觉流（例如用户的屏幕共享或视频录制），并实时响应与手头任务相关的用户查询。为了提供有用的帮助，InsTALL 1) 在任务视频和配对文本数据上训练多模态模型，以及 2) 从视频数据中自动提取任务图，并在训练和推理时间利用它。我们展示了 InsTALL 在考虑用于多模态活动理解的提议子任务中实现了最先进的性能——任务识别 (TR)、动作识别 (AR)、下一个动作预测 (AP) 和计划预测 (PP)——并且在与自动错误识别相关的两个新子任务上优于现有的基准。

##### **SCFCRC: Simultaneously Counteract Feature Camouflage and Relation Camouflage for Fraud Detection**
2501.12430v1 by Xiaocheng Zhang, Zhuangzhuang Ye, GuoPing Zhao, Jianing Wang, Xiaohong Su

In fraud detection, fraudsters often interact with many benign users,
camouflaging their features or relations to hide themselves. Most existing work
concentrates solely on either feature camouflage or relation camouflage, or
decoupling feature learning and relation learning to avoid the two camouflage
from affecting each other. However, this inadvertently neglects the valuable
information derived from features or relations, which could mutually enhance
their adversarial camouflage strategies. In response to this gap, we propose
SCFCRC, a Transformer-based fraud detector that Simultaneously Counteract
Feature Camouflage and Relation Camouflage. SCFCRC consists of two components:
Feature Camouflage Filter and Relation Camouflage Refiner. The feature
camouflage filter utilizes pseudo labels generated through label propagation to
train the filter and uses contrastive learning that combines instance-wise and
prototype-wise to improve the quality of features. The relation camouflage
refiner uses Mixture-of-Experts(MoE) network to disassemble the multi-relations
graph into multiple substructures and divide and conquer them to mitigate the
degradation of detection performance caused by relation camouflage.
Furthermore, we introduce a regularization method for MoE to enhance the
robustness of the model. Extensive experiments on two fraud detection benchmark
datasets demonstrate that our method outperforms state-of-the-art baselines.

摘要：<paragraph>在欺詐偵測中，欺詐者通常會與許多良性使用者互動，偽裝其特徵或關係以隱藏自己。大多數現有工作僅專注於特徵偽裝或關係偽裝，或解耦特徵學習和關係學習，以避免兩種偽裝互相影響。然而，這無意中忽視了從特徵或關係中衍生的有價值資訊，這可能會相互增強其對抗性偽裝策略。為了應對這一差距，我們提出了 SCFCRC，這是一個基於 Transformer 的欺詐檢測器，可同時對抗特徵偽裝和關係偽裝。SCFCRC 由兩個組成部分組成：特徵偽裝濾波器和關係偽裝精煉器。特徵偽裝濾波器利用透過標籤傳播產生的偽標籤來訓練濾波器，並使用結合了實例級和原型級的對比學習來提高特徵品質。關係偽裝精煉器使用 Mixture-of-Experts(MoE) 網路將多關係圖分解成多個子結構，並分而治之，以減輕關係偽裝造成的偵測效能下降。此外，我們為 MoE 引入了一個正規化方法，以增強模型的穩健性。在兩個欺詐偵測基準資料集上進行的廣泛實驗證明，我們的模型優於最先進的基準。</paragraph>

##### **Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model**
2501.12206v1 by Kazi Hasan Ibn Arif, Sajib Acharjee Dip, Khizar Hussain, Lang Zhang, Chris Thomas

Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities in understanding and describing visual content, achieving
state-of-the-art performance across various vision-language tasks. However,
these models frequently exhibit hallucination behavior, where they generate
descriptions containing objects or details absent in the input image. Our work
investigates this phenomenon by analyzing attention patterns across transformer
layers and heads, revealing that hallucinations often stem from progressive
degradation of visual grounding in deeper layers. We propose a novel attention
modification approach that combines selective token emphasis and head-specific
modulation to maintain visual grounding throughout the generation process. Our
method introduces two key components: (1) a dual-stream token selection
mechanism that identifies and prioritizes both locally informative and
spatially significant visual tokens, and (2) an attention head-specific
modulation strategy that differentially amplifies visual information processing
based on measured visual sensitivity of individual attention heads. Through
extensive experimentation on the MSCOCO dataset, we demonstrate that our
approach reduces hallucination rates by up to 62.3\% compared to baseline
models while maintaining comparable task performance. Our analysis reveals that
selectively modulating tokens across attention heads with varying levels of
visual sensitivity can significantly improve visual grounding without requiring
model retraining.

摘要：大型視覺語言模型 (LVLMs) 已展現出在理解和描述視覺內容方面的卓越能力，在各種視覺語言任務中取得了最先進的表現。然而，這些模型經常表現出幻覺行為，它們會產生包含輸入影像中不存在的物件或細節的描述。我們的研究透過分析Transformer層和頭部的注意力模式來探討這種現象，揭示幻覺通常源於較深層的視覺基礎逐漸退化。我們提出了一種新穎的注意力修改方法，結合選擇性的權標強調和特定於頭部的調變，以在整個生成過程中維持視覺基礎。我們的技術引入了兩個關鍵組成部分：(1) 一種雙串流權標選擇機制，用於識別和優先處理局部資訊性和空間顯著性的視覺權標，以及 (2) 一種注意力頭部特定調變策略，根據個別注意力頭部的測量視覺敏感度，對視覺資訊處理進行不同的放大。透過在 MSCOCO 資料集上進行廣泛的實驗，我們證明了與基準模型相比，我們的技術將幻覺率降低了多達 62.3%，同時維持了相當的任務表現。我們的分析顯示，選擇性地調變具有不同視覺敏感度層級的注意力頭部的權標，可以在不需重新訓練模型的情況下，顯著改善視覺基礎。

##### **An End-to-End Approach for Korean Wakeword Systems with Speaker Authentication**
2501.12194v1 by Geonwoo Seo

Wakeword detection plays a critical role in enabling AI assistants to listen
to user voices and interact effectively. However, for languages other than
English, there is a significant lack of pre-trained wakeword models.
Additionally, systems that merely determine the presence of a wakeword can pose
serious privacy concerns. In this paper, we propose an end-to-end approach that
trains wakewords for Non-English languages, particulary Korean, and uses this
to develop a Voice Authentication model to protect user privacy. Our
implementation employs an open-source platform OpenWakeWord, which performs
wakeword detection using an FCN (Fully-Connected Network) architecture. Once a
wakeword is detected, our custom-developed code calculates cosine similarity
for robust user authentication. Experimental results demonstrate the
effectiveness of our approach, achieving a 16.79% and a 6.6% Equal Error Rate
(EER) each in the Wakeword Detection and the Voice Authentication. These
findings highlight the model's potential in providing secure and accurate
wakeword detection and authentication for Korean users.

摘要：喚醒字偵測在讓 AI 助理聆聽使用者聲音並有效互動中扮演關鍵角色。然而，對於非英語語言來說，預先訓練的喚醒字模型存在著顯著的缺乏。此外，僅僅判定喚醒字存在的系統可能會造成嚴重的隱私問題。在本文中，我們提出一個端對端的方法，用於訓練非英語語言（特別是韓語）的喚醒字，並使用它來開發一個語音驗證模型以保護使用者隱私。我們的實作採用一個開源平台 OpenWakeWord，它使用 FCN（全連接網路）架構來執行喚醒字偵測。一旦偵測到喚醒字，我們自訂開發的程式碼會計算餘弦相似度以進行穩健的使用者驗證。實驗結果證明了我們方法的有效性，在喚醒字偵測和語音驗證中分別達到 16.79% 和 6.6% 的等錯率（EER）。這些發現突顯了該模型在為韓語使用者提供安全且準確的喚醒字偵測和驗證方面的潛力。

##### **Fuel Efficiency Analysis of the Public Transportation System Based on the Gaussian Mixture Model Clustering**
2501.12429v1 by Zhipeng Ma, Bo Nørregaard Jørgensen, Zheng Ma

Public transportation is a major source of greenhouse gas emissions,
highlighting the need to improve bus fuel efficiency. Clustering algorithms
assist in analyzing fuel efficiency by grouping data into clusters, but
irrelevant features may complicate the analysis and choosing the optimal number
of clusters remains a challenging task. Therefore, this paper employs the
Gaussian mixture models to cluster the solo fuel-efficiency dataset. Moreover,
an integration method that combines the Silhouette index, Calinski-Harabasz
index, and Davies-Bouldin index is developed to select the optimal cluster
numbers. A dataset with 4006 bus trips in North Jutland, Denmark is utilized as
the case study. Trips are first split into three groups, then one group is
divided further, resulting in four categories: extreme, normal, low, and
extremely low fuel efficiency. A preliminary study using visualization analysis
is conducted to investigate how driving behaviors and route conditions affect
fuel efficiency. The results indicate that both individual driving habits and
route characteristics have a significant influence on fuel efficiency.

摘要：公共運輸是溫室氣體排放的主要來源，突顯了改善公車燃油效率的必要性。分群演算法有助於透過將資料分組成群集來分析燃油效率，但無關特徵可能會使分析複雜化，而選擇最佳群集數量仍然是一項具有挑戰性的任務。因此，本文採用高斯混合模型來分群單一燃油效率資料集。此外，還開發了一種結合輪廓指數、Calinski-Harabasz 指數和 Davies-Bouldin 指數的整合方法來選擇最佳群集數量。丹麥北日德蘭半島 4006 次公車行程的資料集被用作案例研究。行程首先分成三組，然後進一步將一組分開，形成四個類別：極端、正常、低和極低燃油效率。使用視覺化分析進行初步研究，以調查駕駛行為和路線狀況如何影響燃油效率。結果表明，個人的駕駛習慣和路線特徵都對燃油效率有顯著影響。

