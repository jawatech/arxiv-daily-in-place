{"2501.08951": {"publish_time": "2025-01-15", "title": "Analyzing the Ethical Logic of Six Large Language Models", "paper_summary": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.", "paper_summary_zh": "\u9019\u9805\u7814\u7a76\u63a2\u8a0e\u4e86\u516d\u7a2e\u8457\u540d\u7684\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u502b\u7406\u63a8\u7406\uff1aOpenAI GPT-4o\u3001Meta LLaMA 3.1\u3001Perplexity\u3001Anthropic Claude 3.5 Sonnet\u3001Google Gemini \u548c Mistral 7B\u3002\u8a72\u7814\u7a76\u63a2\u8a0e\u4e86\u9019\u4e9b\u6a21\u578b\u5982\u4f55\u95e1\u8ff0\u548c\u61c9\u7528\u502b\u7406\u908f\u8f2f\uff0c\u7279\u5225\u662f\u5728\u56de\u61c9\u9053\u5fb7\u5169\u96e3\u554f\u984c\uff0c\u4f8b\u5982\u96fb\u8eca\u96e3\u984c\u548c\u6d77\u56e0\u8328\u5169\u96e3\u554f\u984c\u6642\u3002\u8a72\u7814\u7a76\u8df3\u812b\u50b3\u7d71\u7684\u5c0d\u9f4a\u7814\u7a76\uff0c\u63a1\u7528\u53ef\u89e3\u91cb\u6027\u900f\u660e\u5ea6\u67b6\u69cb\uff0c\u4fc3\u4f7f\u6a21\u578b\u89e3\u91cb\u5176\u502b\u7406\u63a8\u7406\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u4e09\u7a2e\u65e2\u5b9a\u7684\u502b\u7406\u985e\u578b\u5b78\u9032\u884c\u5206\u6790\uff1a\u5f8c\u679c\u8ad6 - \u7fa9\u52d9\u8ad6\u5206\u6790\u3001\u9053\u5fb7\u57fa\u790e\u7406\u8ad6\u548c\u79d1\u723e\u4f2f\u683c\u9053\u5fb7\u767c\u5c55\u968e\u6bb5\u6a21\u578b\u3002\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cLLM \u8868\u73fe\u51fa\u9ad8\u5ea6\u4e00\u81f4\u7684\u502b\u7406\u908f\u8f2f\uff0c\u7279\u5fb5\u5728\u65bc\u7406\u6027\u4e3b\u7fa9\u3001\u5f8c\u679c\u8ad6\u7684\u5f37\u8abf\uff0c\u6c7a\u7b56\u901a\u5e38\u512a\u5148\u8003\u616e\u6700\u5c0f\u5316\u50b7\u5bb3\u548c\u516c\u5e73\u6027\u3002\u5118\u7ba1\u9810\u8a13\u7df4\u548c\u6a21\u578b\u67b6\u69cb\u76f8\u4f3c\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u4e4b\u9593\u51fa\u73fe\u4e86\u7d30\u5fae\u4e14\u986f\u8457\u7684\u502b\u7406\u63a8\u7406\u5dee\u7570\uff0c\u53cd\u6620\u4e86\u5fae\u8abf\u548c\u5f8c\u8a13\u7df4\u904e\u7a0b\u7684\u5dee\u7570\u3002\u9019\u4e9b\u6a21\u578b\u59cb\u7d42\u8868\u73fe\u51fa\u535a\u5b78\u3001\u8b39\u614e\u548c\u81ea\u6211\u610f\u8b58\uff0c\u5448\u73fe\u51fa\u985e\u4f3c\u65bc\u9053\u5fb7\u54f2\u5b78\u7814\u7a76\u751f\u5c64\u7d1a\u8ad6\u8ff0\u7684\u502b\u7406\u63a8\u7406\u3002\u9019\u4e9b\u7cfb\u7d71\u9a5a\u4eba\u5730\u4e00\u81f4\u5730\u63cf\u8ff0\u5b83\u5011\u7684\u502b\u7406\u63a8\u7406\u6bd4\u5178\u578b\u4eba\u985e\u9053\u5fb7\u908f\u8f2f\u7684\u7279\u6027\u66f4\u70ba\u8907\u96dc\u3002", "author": "W. Russell Neuman et.al.", "authors": "W. Russell Neuman, Chad Coleman, Manan Shah", "id": "2501.08951v1", "paper_url": "http://arxiv.org/abs/2501.08951v1", "repo": "null"}}