{"2408.09632": {"publish_time": "2024-08-19", "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression", "paper_summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5c55\u73fe\u5404\u7a2e\u4efb\u52d9\u7684\u5353\u8d8a\u6548\u80fd\uff0c\u91cd\u5851\u4e86\u4eba\u5de5\u667a\u6167\u7684\u7248\u5716\u3002\u7136\u800c\uff0c\u9f90\u5927\u7684\u904b\u7b97\u9700\u6c42\u4f7f\u5f97\u5b83\u5011\u96e3\u4ee5\u90e8\u7f72\u5728\u8cc7\u6e90\u6709\u9650\u7684\u88dd\u7f6e\u4e0a\u3002\u6700\u8fd1\uff0c\u4f7f\u7528\u4f4e\u79e9\u77e9\u9663\u6280\u8853\u7684\u58d3\u7e2e\u65b9\u6cd5\u5df2\u5c55\u73fe\u6f5b\u529b\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u5c0e\u81f4\u6e96\u78ba\u5ea6\u4e0b\u964d\u6216\u5728\u53c3\u6578\u548c\u63a8\u8ad6\u5ef6\u9072\u4e2d\u5f15\u5165\u986f\u8457\u7684\u8ca0\u64d4\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u6a21\u7d44\u5316\u5206\u89e3 (MoDeGPT)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u7d50\u69cb\u5316\u58d3\u7e2e\u67b6\u69cb\uff0c\u4e0d\u9700\u8981\u6062\u5fa9\u5fae\u8abf\uff0c\u540c\u6642\u89e3\u6c7a\u4e86\u4e0a\u8ff0\u7f3a\u9ede\u3002MoDeGPT \u5c07 Transformer \u5340\u584a\u5206\u5272\u6210\u7531\u77e9\u9663\u5c0d\u7d44\u6210\u7684\u6a21\u7d44\uff0c\u4e26\u900f\u904e\u91cd\u5efa\u6a21\u7d44\u7d1a\u5225\u7684\u8f38\u51fa\uff0c\u4f86\u964d\u4f4e\u96b1\u85cf\u7dad\u5ea6\u3002MoDeGPT \u662f\u57fa\u65bc\u4e00\u500b\u7406\u8ad6\u67b6\u69cb\u958b\u767c\u7684\uff0c\u8a72\u67b6\u69cb\u5229\u7528\u4e86\u4e09\u7a2e\u5b8c\u5584\u7684\u77e9\u9663\u5206\u89e3\u6f14\u7b97\u6cd5\u2014\u2014Nystr\\\"om \u8fd1\u4f3c\u3001CR \u5206\u89e3\u548c SVD\u2014\u2014\u4e26\u5c07\u5b83\u5011\u61c9\u7528\u65bc\u6211\u5011\u91cd\u65b0\u5b9a\u7fa9\u7684 Transformer \u6a21\u7d44\u3002\u6211\u5011\u7684\u5168\u9762\u5be6\u9a57\u986f\u793a\uff0cMoDeGPT \u5728\u6c92\u6709\u53cd\u5411\u50b3\u64ad\u7684\u60c5\u6cc1\u4e0b\uff0c\u8207\u4f9d\u8cf4\u65bc\u68af\u5ea6\u8cc7\u8a0a\u7684\u5148\u524d\u7d50\u69cb\u5316\u58d3\u7e2e\u65b9\u6cd5\u76f8\u5339\u914d\u6216\u8d85\u8d8a\u5b83\u5011\uff0c\u4e26\u5728\u58d3\u7e2e 13B \u6a21\u578b\u6642\u7bc0\u7701\u4e86 98% \u7684\u904b\u7b97\u6210\u672c\u3002\u5728 \\textsc{Llama}-2/3 \u548c OPT \u6a21\u578b\u4e0a\uff0cMoDeGPT \u5728 25-30% \u7684\u58d3\u7e2e\u7387\u4e0b\u7dad\u6301\u4e86 90-95% \u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u3002\u6b64\u5916\uff0c\u58d3\u7e2e\u53ef\u4ee5\u5728\u55ae\u500b GPU \u4e0a\u5728\u6578\u5c0f\u6642\u5167\u5b8c\u6210\uff0c\u4e26\u5c07\u63a8\u8ad6\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 46%\u3002", "author": "Chi-Heng Lin et.al.", "authors": "Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu", "id": "2408.09632v1", "paper_url": "http://arxiv.org/abs/2408.09632v1", "repo": "null"}}