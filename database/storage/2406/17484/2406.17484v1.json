{"2406.17484": {"publish_time": "2024-06-25", "title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "paper_summary": "Large language models (LLMs) have shown substantial progress in natural\nlanguage understanding and generation, proving valuable especially in the\nmedical field. Despite advancements, challenges persist due to the complexity\nand diversity inherent in medical tasks, which can be categorized as\nknowledge-intensive tasks and alignment-required tasks. Previous approaches\neither ignore the latter task or focus on a minority of tasks and hence lose\ngeneralization. To address these drawbacks, we propose a progressive\nfine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise\naggregator to encode diverse knowledge in the first stage and filter out\ndetrimental information. In the second stage, we drop the Noise Aggregator to\navoid the interference of suboptimal representation and leverage an additional\nalignment module optimized towards an orthogonal direction to the knowledge\nspace to mitigate knowledge forgetting. Based on this two-stage paradigm, we\nproposed a Medical LLM through decoupling Clinical Alignment and Knowledge\nAggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)\nperformance on over 20 medical tasks, as well as SOTA results on specific\nmedical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all\ndemonstrate significant improvements over existing models with similar model\nsizes.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u7279\u5225\u662f\u5728\u91ab\u7642\u9818\u57df\u8b49\u660e\u4e86\u5176\u50f9\u503c\u3002\u5118\u7ba1\u6709\u9032\u5c55\uff0c\u4f46\u7531\u65bc\u91ab\u7642\u4efb\u52d9\u56fa\u6709\u7684\u8907\u96dc\u6027\u548c\u591a\u6a23\u6027\uff0c\u6311\u6230\u4f9d\u7136\u5b58\u5728\uff0c\u9019\u4e9b\u4efb\u52d9\u53ef\u6b78\u985e\u70ba\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u548c\u9700\u8981\u5c0d\u9f4a\u7684\u4efb\u52d9\u3002\u5148\u524d\u7684\u505a\u6cd5\u8981\u4e48\u5ffd\u7565\u5f8c\u8005\u4efb\u52d9\uff0c\u8981\u4e48\u5c08\u6ce8\u65bc\u5c11\u6578\u4efb\u52d9\uff0c\u56e0\u6b64\u5931\u53bb\u4e86\u6982\u62ec\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6f38\u9032\u5f0f\u5fae\u8abf\u7ba1\u9053\u3002\u6b64\u7ba1\u9053\u5728\u7b2c\u4e00\u968e\u6bb5\u63a1\u7528\u77e5\u8b58\u805a\u5408\u5668\u548c\u566a\u97f3\u805a\u5408\u5668\u5c0d\u4e0d\u540c\u7684\u77e5\u8b58\u9032\u884c\u7de8\u78bc\uff0c\u4e26\u904e\u6ffe\u6389\u6709\u5bb3\u4fe1\u606f\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u6211\u5011\u653e\u68c4\u566a\u97f3\u805a\u5408\u5668\uff0c\u4ee5\u907f\u514d\u6b21\u512a\u8868\u793a\u7684\u5e72\u64fe\uff0c\u4e26\u5229\u7528\u984d\u5916\u5c0d\u9f4a\u6a21\u7d44\uff0c\u8a72\u6a21\u7d44\u91dd\u5c0d\u8207\u77e5\u8b58\u7a7a\u9593\u6b63\u4ea4\u7684\u65b9\u5411\u9032\u884c\u6700\u4f73\u5316\uff0c\u4ee5\u6e1b\u8f15\u77e5\u8b58\u907a\u5fd8\u3002\u57fa\u65bc\u9019\u500b\u5169\u968e\u6bb5\u7bc4\u4f8b\uff0c\u6211\u5011\u901a\u904e\u89e3\u8026\u81e8\u5e8a\u5c0d\u9f4a\u548c\u77e5\u8b58\u805a\u5408 (MedCare) \u63d0\u51fa\u4e86\u4e00\u500b\u91ab\u7642 LLM\uff0c\u5176\u8a2d\u8a08\u65e8\u5728\u5728 20 \u591a\u9805\u91ab\u7642\u4efb\u52d9\u4e0a\u5be6\u73fe\u6700\u5148\u9032 (SOTA) \u6548\u80fd\uff0c\u4ee5\u53ca\u5728\u7279\u5b9a\u91ab\u7642\u5c0d\u9f4a\u4efb\u52d9\u4e0a\u5be6\u73fe SOTA \u7d50\u679c\u3002MedCare \u7684\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\uff081.8B\u30017B\u300114B\uff09\u5747\u986f\u793a\u51fa\u6bd4\u5177\u6709\u985e\u4f3c\u6a21\u578b\u5927\u5c0f\u7684\u73fe\u6709\u6a21\u578b\u6709\u986f\u8457\u7684\u6539\u9032\u3002", "author": "Yusheng Liao et.al.", "authors": "Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang", "id": "2406.17484v1", "paper_url": "http://arxiv.org/abs/2406.17484v1", "repo": "null"}}