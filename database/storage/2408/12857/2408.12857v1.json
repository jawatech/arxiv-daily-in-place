{"2408.12857": {"publish_time": "2024-08-23", "title": "Memory-Efficient LLM Training with Online Subspace Descent", "paper_summary": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u4e00\u7cfb\u5217\u9ad8\u6548\u8a18\u61b6\u9ad4 LLM \u8a13\u7df4\u6f14\u7b97\u6cd5\u7372\u5f97\u6975\u5927\u6b61\u8fce\u3002\u9019\u4e9b\u65b9\u6cd5\u5229\u7528\u68af\u5ea6\u7684\u4f4e\u968e\u7d50\u69cb\uff0c\u4f7f\u7528\u5947\u7570\u503c\u5206\u89e3 (SVD) \u627e\u5230\u7684\u6295\u5f71\u77e9\u9663\u5c07\u6700\u4f73\u5316\u5668\u72c0\u614b\u6295\u5f71\u5230\u5b50\u7a7a\u9593\u4e2d\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6f14\u7b97\u6cd5\u7684\u6536\u6582\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u5176\u6295\u5f71\u77e9\u9663\u7684\u66f4\u65b0\u898f\u5247\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u4f9b\u6295\u5f71\u77e9\u9663\u4efb\u610f\u66f4\u65b0\u898f\u5247\u7684\\emph{\u7b2c\u4e00\u500b}\u6536\u6582\u4fdd\u8b49\u3002\u6b64\u4fdd\u8b49\u901a\u5e38\u9069\u7528\u65bc\u53ef\u900f\u904e\u54c8\u5bc6\u9813\u4e0b\u964d\u5206\u6790\u7684\u6700\u4f73\u5316\u5668\uff0c\u5305\u62ec\u6700\u5e38\u898b\u7684 LION\u3001Adam \u7b49\u3002\u53d7\u5230\u6211\u5011\u7406\u8ad6\u7406\u89e3\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u7dda\u4e0a\u5b50\u7a7a\u9593\u4e0b\u964d\uff0c\u4e00\u7a2e\u6c92\u6709 SVD \u7684\u65b0\u7cfb\u5217\u5b50\u7a7a\u9593\u4e0b\u964d\u6700\u4f73\u5316\u5668\u3002\u7dda\u4e0a\u5b50\u7a7a\u9593\u4e0b\u964d\u4e26\u975e\u4f7f\u7528\u7279\u5fb5\u5411\u91cf\u66f4\u65b0\u6295\u5f71\u77e9\u9663\uff0c\u800c\u662f\u4f7f\u7528\u7dda\u4e0a PCA \u66f4\u65b0\u6295\u5f71\u77e9\u9663\u3002\u7dda\u4e0a\u5b50\u7a7a\u9593\u4e0b\u964d\u5177\u5099\u5f48\u6027\uff0c\u4e14\u50c5\u6703\u5c0d\u8a13\u7df4\u9020\u6210\u6975\u5c0f\u984d\u5916\u7684\u8ca0\u64d4\u3002\u6211\u5011\u5c55\u793a\uff0c\u5c0d\u65bc\u5728 C4 \u8cc7\u6599\u96c6\u4e0a\u9810\u8a13\u7df4 LLaMA \u6a21\u578b\uff08\u53c3\u6578\u7bc4\u570d\u5f9e 60M \u5230 7B\uff09\u7684\u4efb\u52d9\uff0c\u7dda\u4e0a\u5b50\u7a7a\u9593\u4e0b\u964d\u5728\u4e0d\u540c\u8a2d\u5b9a\u4e0b\u5747\u80fd\u9054\u6210\u6bd4\u73fe\u6709\u4f4e\u968e\u8a13\u7df4\u65b9\u6cd5\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u548c\u66f4\u597d\u7684\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\uff0c\u4e26\u7e2e\u5c0f\u8207\u5168\u968e\u57fa\u7dda\u7684\u5dee\u8ddd\u3002</paragraph>", "author": "Kaizhao Liang et.al.", "authors": "Kaizhao Liang, Bo Liu, Lizhang Chen, Qiang Liu", "id": "2408.12857v1", "paper_url": "http://arxiv.org/abs/2408.12857v1", "repo": "https://github.com/kyleliang919/online-subspace-descent"}}