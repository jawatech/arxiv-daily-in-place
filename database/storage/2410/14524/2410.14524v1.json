{"2410.14524": {"publish_time": "2024-10-18", "title": "Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance", "paper_summary": "Self-supervised pre-training of deep learning models with contrastive\nlearning is a widely used technique in image analysis. Current findings\nindicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular\ncharacteristics of these images. We hypothesize that the similarity of medical\nimages hinders the success of contrastive learning in the medical imaging\ndomain. To this end, we investigate different strategies based on deep\nembedding, information theory, and hashing in order to identify and reduce\nredundancy in medical pre-training datasets. The effect of these different\nreduction strategies on contrastive learning is evaluated on two pre-training\ndatasets and several downstream classification tasks. In all of our\nexperiments, dataset reduction leads to a considerable performance gain in\ndownstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the\nCOVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST\nClassification Challenge and 0.73 to 0.83 for a brain hemorrhage classification\ntask. Furthermore, pre-training is up to nine times faster due to the dataset\nreduction. In conclusion, the proposed approach highlights the importance of\ndataset quality and provides a transferable approach to improve contrastive\npre-training for classification downstream tasks on medical images.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u7684\u5c0d\u6bd4\u5b78\u7fd2\u81ea\u76e3\u7763\u9810\u8a13\u7df4\u662f\u4e00\u7a2e\u5ee3\u6cdb\u7528\u65bc\u5f71\u50cf\u5206\u6790\u7684\u6280\u8853\u3002\u76ee\u524d\u7684\u767c\u73fe\u986f\u793a\u5c0d\u6bd4\u9810\u8a13\u7df4\u5728\u91ab\u5b78\u5f71\u50cf\u4e0a\u5177\u6709\u5f37\u5927\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u9032\u4e00\u6b65\u7684\u7814\u7a76\u5c0d\u65bc\u7d0d\u5165\u9019\u4e9b\u5f71\u50cf\u7684\u7279\u5b9a\u7279\u5fb5\u662f\u5fc5\u8981\u7684\u3002\u6211\u5011\u5047\u8a2d\u91ab\u5b78\u5f71\u50cf\u7684\u76f8\u4f3c\u6027\u963b\u7919\u4e86\u5c0d\u6bd4\u5b78\u7fd2\u5728\u91ab\u5b78\u5f71\u50cf\u9818\u57df\u7684\u6210\u529f\u3002\u70ba\u6b64\uff0c\u6211\u5011\u7814\u7a76\u4e86\u57fa\u65bc\u6df1\u5ea6\u5d4c\u5165\u3001\u8cc7\u8a0a\u7406\u8ad6\u548c\u96dc\u6e4a\u7684\u4e0d\u540c\u7b56\u7565\uff0c\u4ee5\u8b58\u5225\u548c\u6e1b\u5c11\u91ab\u5b78\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u4e2d\u7684\u5197\u9918\u3002\u9019\u4e9b\u4e0d\u540c\u7684\u7c21\u5316\u7b56\u7565\u5c0d\u6bd4\u5b78\u7fd2\u7684\u5f71\u97ff\u5728\u5169\u500b\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u5e7e\u500b\u4e0b\u6e38\u5206\u985e\u4efb\u52d9\u4e2d\u9032\u884c\u8a55\u4f30\u3002\u5728\u6211\u5011\u6240\u6709\u7684\u5be6\u9a57\u4e2d\uff0c\u8cc7\u6599\u96c6\u7c21\u5316\u90fd\u5c0e\u81f4\u4e86\u4e0b\u6e38\u4efb\u52d9\u7684\u986f\u8457\u6548\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\uff0cCOVID CT \u5206\u985e\u5927\u6311\u6230\u7684 AUC \u5206\u6578\u5f9e 0.78 \u63d0\u5347\u81f3 0.83\uff0cOrganSMNIST \u5206\u985e\u6311\u6230\u5f9e 0.97 \u63d0\u5347\u81f3 0.98\uff0c\u8166\u51fa\u8840\u5206\u985e\u4efb\u52d9\u5f9e 0.73 \u63d0\u5347\u81f3 0.83\u3002\u6b64\u5916\uff0c\u7531\u65bc\u8cc7\u6599\u96c6\u7c21\u5316\uff0c\u9810\u8a13\u7df4\u901f\u5ea6\u6700\u9ad8\u53ef\u63d0\u5347\u4e5d\u500d\u3002\u7e3d\u4e4b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7a81\u986f\u4e86\u8cc7\u6599\u96c6\u54c1\u8cea\u7684\u91cd\u8981\u6027\uff0c\u4e26\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u8f49\u79fb\u7684\u65b9\u6cd5\u4f86\u6539\u5584\u91ab\u5b78\u5f71\u50cf\u4e0a\u5206\u985e\u4e0b\u6e38\u4efb\u52d9\u7684\u5c0d\u6bd4\u9810\u8a13\u7df4\u3002", "author": "Daniel Wolf et.al.", "authors": "Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Michael G\u00f6tz, Timo Ropinski", "id": "2410.14524v1", "paper_url": "http://arxiv.org/abs/2410.14524v1", "repo": "https://github.com/Wolfda95/Less_is_More"}}