{"2504.03635": {"publish_time": "2025-04-04", "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9700\u8981\u8907\u96dc\u63a8\u7406\u7684\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u6a21\u578b\u898f\u6a21\u5927\u5c0f\u5c0d\u5176\u63a8\u7406\u80fd\u529b\u7684\u5f71\u97ff\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7684\u7406\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5408\u6210\u7684\u591a\u8df3\u63a8\u7406\u74b0\u5883\uff0c\u5176\u8a2d\u8a08\u65e8\u5728\u5bc6\u5207\u8907\u88fd\u771f\u5be6\u4e16\u754c\u5927\u898f\u6a21\u77e5\u8b58\u5716\u8b5c\u7684\u7d50\u69cb\u548c\u5206\u4f48\u3002\u6211\u5011\u7684\u63a8\u7406\u4efb\u52d9\u5305\u62ec\u5b8c\u6210\u5716\u8b5c\u4e2d\u7f3a\u5931\u7684\u908a\uff0c\u9019\u9700\u8981\u9ad8\u7d1a\u7684\u591a\u8df3\u63a8\u7406\uff0c\u4e26\u6a21\u4eff\u771f\u5be6\u4e16\u754c\u7684\u63a8\u7406\u5834\u666f\u3002\u70ba\u4e86\u8a55\u4f30\u9019\u4e00\u9ede\uff0c\u6211\u5011\u50c5\u4f7f\u7528\u4f86\u81ea\u4e0d\u5b8c\u6574\u5716\u8b5c\u7684\u4e09\u5143\u7d44\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (LM)\uff0c\u4e26\u8a55\u4f30\u5b83\u5011\u63a8\u65b7\u7f3a\u5931\u908a\u7684\u80fd\u529b\u3002\u6709\u8da3\u7684\u662f\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u904e\u5ea6\u53c3\u6578\u5316\u6703\u7531\u65bc\u904e\u5ea6\u8a18\u61b6\u800c\u640d\u5bb3\u63a8\u7406\u6027\u80fd\u3002\u6211\u5011\u7814\u7a76\u4e86\u5f71\u97ff\u9019\u7a2e U \u578b\u640d\u5931\u66f2\u7dda\u7684\u4e0d\u540c\u56e0\u7d20\uff0c\u5305\u62ec\u5716\u8b5c\u7d50\u69cb\u3001\u6a21\u578b\u5927\u5c0f\u548c\u8a13\u7df4\u6b65\u9a5f\u3002\u70ba\u4e86\u9810\u6e2c\u7279\u5b9a\u77e5\u8b58\u5716\u8b5c\u7684\u6700\u4f73\u6a21\u578b\u5927\u5c0f\uff0c\u6211\u5011\u767c\u73fe\u4e86\u4e00\u7a2e\u7d93\u9a57\u7e2e\u653e\u6bd4\u4f8b\uff0c\u5b83\u5c07\u77e5\u8b58\u5716\u8b5c\u641c\u7d22\u71b5\u7dda\u6027\u6620\u5c04\u5230\u6700\u4f73\u6a21\u578b\u5927\u5c0f\u3002\u9019\u9805\u5de5\u4f5c\u70ba LLM \u4e2d\u7684\u898f\u6a21\u5927\u5c0f\u548c\u63a8\u7406\u4e4b\u9593\u7684\u95dc\u4fc2\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u95e1\u660e\u4e86\u512a\u5316\u5176\u63a8\u7406\u4efb\u52d9\u6027\u80fd\u7684\u53ef\u80fd\u65b9\u6cd5\u3002\n", "author": "Xinyi Wang et.al.", "authors": "Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen", "id": "2504.03635v1", "paper_url": "http://arxiv.org/abs/2504.03635v1", "repo": "null"}}