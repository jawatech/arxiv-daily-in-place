{"2411.15207": {"publish_time": "2024-11-20", "title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "paper_summary": "Recent advancements in vision-language pre-training via contrastive learning\nhave significantly improved performance across computer vision tasks. However,\nin the medical domain, obtaining multimodal data is often costly and\nchallenging due to privacy, sensitivity, and annotation complexity. To mitigate\ndata scarcity while boosting model performance, we introduce \\textbf{Uni-Mlip},\na unified self-supervision framework specifically designed to enhance medical\nvision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,\nuni-modality, and fused-modality self-supervision techniques at the data-level\nand the feature-level. Additionally, Uni-Mlip tailors uni-modal image\nself-supervision to accommodate the unique characteristics of medical images.\nOur experiments across datasets of varying scales demonstrate that Uni-Mlip\nsignificantly surpasses current state-of-the-art methods in three key\ndownstream tasks: image-text retrieval, image classification, and visual\nquestion answering (VQA).", "paper_summary_zh": "\u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\u9032\u884c\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u7684\u6700\u65b0\u9032\u5c55\uff0c\u5df2\u986f\u8457\u63d0\u5347\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5728\u91ab\u5b78\u9818\u57df\u4e2d\uff0c\u53d6\u5f97\u591a\u6a21\u614b\u8cc7\u6599\u901a\u5e38\u6210\u672c\u9ad8\u6602\u4e14\u5177\u6311\u6230\u6027\uff0c\u539f\u56e0\u5728\u65bc\u96b1\u79c1\u3001\u654f\u611f\u6027\u548c\u6a19\u8a3b\u7684\u8907\u96dc\u6027\u3002\u70ba\u4e86\u5728\u63d0\u5347\u6a21\u578b\u6548\u80fd\u7684\u540c\u6642\u6e1b\u8f15\u8cc7\u6599\u7a00\u5c11\u7684\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 \\textbf{Uni-Mlip}\uff0c\u9019\u662f\u4e00\u500b\u7d71\u4e00\u7684\u81ea\u6211\u76e3\u7763\u67b6\u69cb\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u589e\u5f37\u91ab\u5b78\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u3002Uni-Mlip \u5728\u8cc7\u6599\u5c64\u7d1a\u548c\u7279\u5fb5\u5c64\u7d1a\u7121\u7e2b\u6574\u5408\u8de8\u6a21\u614b\u3001\u55ae\u6a21\u614b\u548c\u878d\u5408\u6a21\u614b\u7684\u81ea\u6211\u76e3\u7763\u6280\u8853\u3002\u6b64\u5916\uff0cUni-Mlip \u91dd\u5c0d\u55ae\u6a21\u614b\u5f71\u50cf\u81ea\u6211\u76e3\u7763\u9032\u884c\u8abf\u6574\uff0c\u4ee5\u9069\u61c9\u91ab\u5b78\u5f71\u50cf\u7684\u7368\u7279\u7279\u6027\u3002\u6211\u5011\u5728\u4e0d\u540c\u898f\u6a21\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\uff0cUni-Mlip \u5728\u4e09\u500b\u95dc\u9375\u7684\u4e0b\u6e38\u4efb\u52d9\u4e2d\u986f\u8457\u8d85\u8d8a\u76ee\u524d\u6700\u5148\u9032\u7684\u65b9\u6cd5\uff1a\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u3001\u5f71\u50cf\u5206\u985e\u548c\u8996\u89ba\u554f\u7b54 (VQA)\u3002", "author": "Ameera Bawazir et.al.", "authors": "Ameera Bawazir, Kebin Wu, Wenbin Li", "id": "2411.15207v1", "paper_url": "http://arxiv.org/abs/2411.15207v1", "repo": "null"}}