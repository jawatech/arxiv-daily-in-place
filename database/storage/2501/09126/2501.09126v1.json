{"2501.09126": {"publish_time": "2025-01-15", "title": "Augmenting Human-Annotated Training Data with Large Language Model Generation and Distillation in Open-Response Assessment", "paper_summary": "Large Language Models (LLMs) like GPT-4o can help automate text\nclassification tasks at low cost and scale. However, there are major concerns\nabout the validity and reliability of LLM outputs. By contrast, human coding is\ngenerally more reliable but expensive to procure at scale. In this study, we\npropose a hybrid solution to leverage the strengths of both. We combine\nhuman-coded data and synthetic LLM-produced data to fine-tune a classical\nmachine learning classifier, distilling both into a smaller BERT model. We\nevaluate our method on a human-coded test set as a validity measure for LLM\noutput quality. In three experiments, we systematically vary LLM-generated\nsamples' size, variety, and consistency, informed by best practices in LLM\ntuning. Our findings indicate that augmenting datasets with synthetic samples\nimproves classifier performance, with optimal results achieved at an 80%\nsynthetic to 20% human-coded data ratio. Lower temperature settings of 0.3,\ncorresponding to less variability in LLM generations, produced more stable\nimprovements but also limited model learning from augmented samples. In\ncontrast, higher temperature settings (0.7 and above) introduced greater\nvariability in performance estimates and, at times, lower performance. Hence,\nLLMs may produce more uniform output that classifiers overfit to earlier or\nproduce more diverse output that runs the risk of deteriorating model\nperformance through information irrelevant to the prediction task. Filtering\nout inconsistent synthetic samples did not enhance performance. We conclude\nthat integrating human and LLM-generated data to improve text classification\nmodels in assessment offers a scalable solution that leverages both the\naccuracy of human coding and the variety of LLM outputs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5982 GPT-4o \u53ef\u4ee5\u5354\u52a9\u81ea\u52d5\u5316\u6587\u672c\u5206\u985e\u4efb\u52d9\uff0c\u6210\u672c\u4f4e\u4e14\u53ef\u64f4\u5145\u3002\u7136\u800c\uff0c\u5c0d\u65bc LLM \u8f38\u51fa\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u6709\u5f88\u5927\u7684\u7591\u616e\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u5de5\u7de8\u78bc\u901a\u5e38\u66f4\u53ef\u9760\uff0c\u4f46\u5927\u898f\u6a21\u63a1\u8cfc\u6210\u672c\u9ad8\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u6df7\u5408\u89e3\u6c7a\u65b9\u6848\uff0c\u4ee5\u5229\u7528\u5169\u8005\u7684\u512a\u9ede\u3002\u6211\u5011\u7d50\u5408\u4eba\u5de5\u7de8\u78bc\u8cc7\u6599\u548c LLM \u7522\u751f\u7684\u5408\u6210\u8cc7\u6599\uff0c\u5fae\u8abf\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\u5206\u985e\u5668\uff0c\u5c07\u5169\u8005\u7cbe\u7149\u6210\u8f03\u5c0f\u7684 BERT \u6a21\u578b\u3002\u6211\u5011\u4f7f\u7528\u4eba\u5de5\u7de8\u78bc\u7684\u6e2c\u8a66\u96c6\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0c\u4f5c\u70ba LLM \u8f38\u51fa\u54c1\u8cea\u7684\u6709\u6548\u6027\u6307\u6a19\u3002\u5728\u4e09\u500b\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u6539\u8b8a LLM \u7522\u751f\u7684\u7bc4\u4f8b\u5927\u5c0f\u3001\u7a2e\u985e\u548c\u4e00\u81f4\u6027\uff0c\u4e26\u6839\u64da LLM \u8abf\u6821\u7684\u6700\u4f73\u5be6\u52d9\u4f86\u9032\u884c\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528\u5408\u6210\u7bc4\u4f8b\u64f4\u5145\u8cc7\u6599\u96c6\u53ef\u4ee5\u63d0\u5347\u5206\u985e\u5668\u7684\u6548\u80fd\uff0c\u6700\u4f73\u7d50\u679c\u51fa\u73fe\u5728\u5408\u6210\u8cc7\u6599\u8207\u4eba\u5de5\u7de8\u78bc\u8cc7\u6599\u7684\u6bd4\u4f8b\u70ba 80% \u6bd4 20% \u6642\u3002\u6eab\u5ea6\u8a2d\u5b9a\u8f03\u4f4e\uff080.3\uff09\uff0c\u4ee3\u8868 LLM \u7522\u751f\u7684\u8b8a\u7570\u6027\u8f03\u4f4e\uff0c\u53ef\u4ee5\u7522\u751f\u8f03\u7a69\u5b9a\u7684\u63d0\u5347\uff0c\u4f46\u4e5f\u6703\u9650\u5236\u6a21\u578b\u5f9e\u64f4\u5145\u7bc4\u4f8b\u4e2d\u5b78\u7fd2\u3002\u76f8\u53cd\u5730\uff0c\u6eab\u5ea6\u8a2d\u5b9a\u8f03\u9ad8\uff080.7 \u4ee5\u4e0a\uff09\u6703\u9020\u6210\u6548\u80fd\u4f30\u8a08\u7684\u8b8a\u7570\u6027\u8f03\u5927\uff0c\u6709\u6642\u6548\u80fd\u4e5f\u6703\u8f03\u4f4e\u3002\u56e0\u6b64\uff0cLLM \u7522\u751f\u7684\u8f38\u51fa\u53ef\u80fd\u8f03\u70ba\u4e00\u81f4\uff0c\u5c0e\u81f4\u5206\u985e\u5668\u904e\u5ea6\u64ec\u5408\u5230\u8f03\u65e9\u7684\u8cc7\u6599\uff0c\u6216\u7522\u751f\u8f03\u591a\u6a23\u5316\u7684\u8f38\u51fa\uff0c\u6709\u98a8\u96aa\u6703\u56e0\u70ba\u8207\u9810\u6e2c\u4efb\u52d9\u7121\u95dc\u7684\u8cc7\u8a0a\u800c\u964d\u4f4e\u6a21\u578b\u6548\u80fd\u3002\u904e\u6ffe\u6389\u4e0d\u4e00\u81f4\u7684\u5408\u6210\u7bc4\u4f8b\u4e26\u672a\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u8ad6\u662f\uff0c\u6574\u5408\u4eba\u5de5\u548c LLM \u7522\u751f\u7684\u8cc7\u6599\u4f86\u6539\u5584\u8a55\u4f30\u4e2d\u7684\u6587\u672c\u5206\u985e\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u540c\u6642\u5229\u7528\u4e86\u4eba\u5de5\u7de8\u78bc\u7684\u6e96\u78ba\u6027\u548c LLM \u8f38\u51fa\u7684\u591a\u6a23\u6027\u3002", "author": "Conrad Borchers et.al.", "authors": "Conrad Borchers, Danielle R. Thomas, Jionghao Lin, Ralph Abboud, Kenneth R. Koedinger", "id": "2501.09126v1", "paper_url": "http://arxiv.org/abs/2501.09126v1", "repo": "null"}}