{"2410.12096": {"publish_time": "2024-10-15", "title": "Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning", "paper_summary": "Graph representation learning, involving both node features and graph\nstructures, is crucial for real-world applications but often encounters\npervasive noise. State-of-the-art methods typically address noise by focusing\nseparately on node features with large language models (LLMs) and on graph\nstructures with graph structure learning models (GSLMs). In this paper, we\nintroduce LangGSL, a robust framework that integrates the complementary\nstrengths of pre-trained language models and GSLMs to jointly enhance both node\nfeature and graph structure learning. In LangGSL, we first leverage LLMs to\nfilter noise in the raw data and extract valuable cleaned information as\nfeatures, enhancing the synergy of downstream models. During the mutual\nlearning phase in LangGSL, the core idea is to leverage the relatively small\nlanguage model (LM) to process local attributes and generate reliable\npseudo-labels and informative node embeddings, which are then integrated into\nthe GSLM's prediction phase. This approach enriches the global context and\nenhances overall performance. Meanwhile, GSLM refines the evolving graph\nstructure constructed from the LM's output, offering updated labels back to the\nLM as additional guidance, thus facilitating a more effective mutual learning\nprocess. The LM and GSLM work synergistically, complementing each other's\nstrengths and offsetting weaknesses within a variational information-maximizing\nframework, resulting in enhanced node features and a more robust graph\nstructure. Extensive experiments on diverse graph datasets of varying scales\nand across different task scenarios demonstrate the scalability and\neffectiveness of the proposed approach.", "paper_summary_zh": "\u5716\u8868\u8868\u793a\u5b78\u7fd2\u65e2\u6d89\u53ca\u7bc0\u9ede\u7279\u5fb5\u53c8\u6d89\u53ca\u5716\u5f62\u7d50\u69cb\uff0c\u5c0d\u65bc\u73fe\u5be6\u4e16\u754c\u7684\u61c9\u7528\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u7d93\u5e38\u6703\u9047\u5230\u666e\u904d\u7684\u566a\u97f3\u3002\u6700\u5148\u9032\u7684\u65b9\u6cd5\u901a\u5e38\u901a\u904e\u5206\u5225\u95dc\u6ce8\u5177\u6709\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7bc0\u9ede\u7279\u5fb5\u548c\u5177\u6709\u5716\u5f62\u7d50\u69cb\u5b78\u7fd2\u6a21\u578b (GSLM) \u7684\u5716\u5f62\u7d50\u69cb\u4f86\u89e3\u6c7a\u566a\u97f3\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 LangGSL\uff0c\u9019\u662f\u4e00\u500b\u5f37\u5927\u7684\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u548c GSLM \u7684\u4e92\u88dc\u512a\u52e2\uff0c\u4ee5\u5171\u540c\u589e\u5f37\u7bc0\u9ede\u7279\u5fb5\u548c\u5716\u5f62\u7d50\u69cb\u5b78\u7fd2\u3002\u5728 LangGSL \u4e2d\uff0c\u6211\u5011\u9996\u5148\u5229\u7528 LLM \u4f86\u904e\u6ffe\u539f\u59cb\u6578\u64da\u4e2d\u7684\u566a\u97f3\uff0c\u4e26\u63d0\u53d6\u6709\u50f9\u503c\u7684\u5df2\u6e05\u7406\u4fe1\u606f\u4f5c\u70ba\u7279\u5fb5\uff0c\u589e\u5f37\u4e0b\u6e38\u6a21\u578b\u7684\u5354\u540c\u4f5c\u7528\u3002\u5728 LangGSL \u4e2d\u7684\u76f8\u4e92\u5b78\u7fd2\u968e\u6bb5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u76f8\u5c0d\u8f03\u5c0f\u7684\u8a9e\u8a00\u6a21\u578b (LM) \u4f86\u8655\u7406\u5c40\u90e8\u5c6c\u6027\u4e26\u751f\u6210\u53ef\u9760\u7684\u507d\u6a19\u7c64\u548c\u4fe1\u606f\u8c50\u5bcc\u7684\u7bc0\u9ede\u5d4c\u5165\uff0c\u7136\u5f8c\u5c07\u5b83\u5011\u96c6\u6210\u5230 GSLM \u7684\u9810\u6e2c\u968e\u6bb5\u3002\u9019\u7a2e\u65b9\u6cd5\u8c50\u5bcc\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u4e26\u589e\u5f37\u4e86\u6574\u9ad4\u6027\u80fd\u3002\u540c\u6642\uff0cGSLM \u512a\u5316\u4e86\u5f9e LM \u8f38\u51fa\u69cb\u5efa\u7684\u6f14\u5316\u5716\u5f62\u7d50\u69cb\uff0c\u5c07\u66f4\u65b0\u7684\u6a19\u7c64\u4f5c\u70ba\u9644\u52a0\u6307\u5c0e\u53cd\u994b\u7d66 LM\uff0c\u5f9e\u800c\u4fc3\u9032\u66f4\u6709\u6548\u7684\u76f8\u4e92\u5b78\u7fd2\u904e\u7a0b\u3002LM \u548c GSLM \u5354\u540c\u5de5\u4f5c\uff0c\u5728\u8b8a\u5206\u4fe1\u606f\u6700\u5927\u5316\u6846\u67b6\u5167\u4e92\u88dc\u5404\u81ea\u7684\u512a\u52e2\u4e26\u5f4c\u88dc\u5f31\u9ede\uff0c\u5f9e\u800c\u589e\u5f37\u7bc0\u9ede\u7279\u5fb5\u4e26\u5f62\u6210\u66f4\u5f37\u5927\u7684\u5716\u5f62\u7d50\u69cb\u3002\u5728\u4e0d\u540c\u898f\u6a21\u548c\u4e0d\u540c\u4efb\u52d9\u5834\u666f\u7684\u591a\u6a23\u5316\u5716\u5f62\u6578\u64da\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u53ef\u64f4\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "author": "Guangxin Su et.al.", "authors": "Guangxin Su, Yifan Zhu, Wenjie Zhang, Hanchen Wang, Ying Zhang", "id": "2410.12096v1", "paper_url": "http://arxiv.org/abs/2410.12096v1", "repo": "null"}}