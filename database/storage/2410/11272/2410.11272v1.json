{"2410.11272": {"publish_time": "2024-10-15", "title": "Cognitive Overload Attack:Prompt Injection for Long Context", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming tasks across various domains without needing explicit retraining.\nThis capability, known as In-Context Learning (ICL), while impressive, exposes\nLLMs to a variety of adversarial prompts and jailbreaks that manipulate\nsafety-trained LLMs into generating undesired or harmful output. In this paper,\nwe propose a novel interpretation of ICL in LLMs through the lens of cognitive\nneuroscience, by drawing parallels between learning in human cognition with\nICL. We applied the principles of Cognitive Load Theory in LLMs and empirically\nvalidate that similar to human cognition, LLMs also suffer from cognitive\noverload a state where the demand on cognitive processing exceeds the available\ncapacity of the model, leading to potential errors. Furthermore, we\ndemonstrated how an attacker can exploit ICL to jailbreak LLMs through\ndeliberately designed prompts that induce cognitive overload on LLMs, thereby\ncompromising the safety mechanisms of LLMs. We empirically validate this threat\nmodel by crafting various cognitive overload prompts and show that advanced\nmodels such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, Llama-3-70B-Instruct,\nGemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack\nsuccess rates of up to 99.99%. Our findings highlight critical vulnerabilities\nin LLMs and underscore the urgency of developing robust safeguards. We propose\nintegrating insights from cognitive load theory into the design and evaluation\nof LLMs to better anticipate and mitigate the risks of adversarial attacks. By\nexpanding our experiments to encompass a broader range of models and by\nhighlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of\nsafer and more reliable AI systems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5728\u57f7\u884c\u5404\u7a2e\u9818\u57df\u4efb\u52d9\u6642\uff0c\u7121\u9700\u660e\u78ba\u91cd\u65b0\u8a13\u7df4\u7684\u986f\u8457\u80fd\u529b\u3002\u9019\u7a2e\u7a31\u70ba\u60c5\u5883\u4e2d\u5b78\u7fd2 (ICL) \u7684\u80fd\u529b\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u6703\u8b93 LLM \u66b4\u9732\u65bc\u5404\u7a2e\u5c0d\u6297\u63d0\u793a\u548c\u8d8a\u7344\uff0c\u9019\u4e9b\u63d0\u793a\u548c\u8d8a\u7344\u6703\u64cd\u7e31\u5b89\u5168\u8a13\u7df4\u7684 LLM\uff0c\u8b93\u5b83\u5011\u7522\u751f\u4e0d\u53d7\u6b61\u8fce\u6216\u6709\u5bb3\u7684\u8f38\u51fa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u8a8d\u77e5\u795e\u7d93\u79d1\u5b78\u7684\u89c0\u9ede\uff0c\u63d0\u51fa\u5c0d LLM \u4e2d ICL \u7684\u65b0\u8a6e\u91cb\uff0c\u65b9\u6cd5\u662f\u5c07\u4eba\u985e\u8a8d\u77e5\u4e2d\u7684\u5b78\u7fd2\u8207 ICL \u9032\u884c\u985e\u6bd4\u3002\u6211\u5011\u5c07\u8a8d\u77e5\u8ca0\u8f09\u7406\u8ad6\u7684\u539f\u5247\u61c9\u7528\u65bc LLM\uff0c\u4e26\u900f\u904e\u5be6\u8b49\u9a57\u8b49\uff0c\u8207\u4eba\u985e\u8a8d\u77e5\u985e\u4f3c\uff0cLLM \u4e5f\u6703\u906d\u53d7\u8a8d\u77e5\u8d85\u8f09\uff0c\u9019\u662f\u4e00\u7a2e\u8a8d\u77e5\u8655\u7406\u9700\u6c42\u8d85\u904e\u6a21\u578b\u53ef\u7528\u5bb9\u91cf\u7684\u72c0\u614b\uff0c\u53ef\u80fd\u5c0e\u81f4\u932f\u8aa4\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u653b\u64ca\u8005\u5982\u4f55\u5229\u7528 ICL \u900f\u904e\u7cbe\u5fc3\u8a2d\u8a08\u7684\u63d0\u793a\uff0c\u5728 LLM \u4e0a\u8a98\u767c\u8a8d\u77e5\u8d85\u8f09\uff0c\u9032\u800c\u8d8a\u7344\uff0c\u5f9e\u800c\u5371\u5bb3 LLM \u7684\u5b89\u5168\u6a5f\u5236\u3002\u6211\u5011\u900f\u904e\u88fd\u4f5c\u5404\u7a2e\u8a8d\u77e5\u8d85\u8f09\u63d0\u793a\uff0c\u4e26\u5c55\u793a GPT-4\u3001Claude-3.5 Sonnet\u3001Claude-3 OPUS\u3001Llama-3-70B-Instruct\u3001Gemini-1.0-Pro \u548c Gemini-1.5-Pro \u7b49\u5148\u9032\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u8d8a\u7344\uff0c\u653b\u64ca\u6210\u529f\u7387\u9ad8\u9054 99.99%\uff0c\u5f9e\u800c\u5be6\u8b49\u9a57\u8b49\u4e86\u9019\u500b\u5a01\u8105\u6a21\u578b\u3002\u6211\u5011\u7684\u767c\u73fe\u7a81\u986f\u4e86 LLM \u4e2d\u7684\u56b4\u91cd\u6f0f\u6d1e\uff0c\u4e26\u5f37\u8abf\u4e86\u958b\u767c\u5f37\u5927\u9632\u8b77\u63aa\u65bd\u7684\u8feb\u5207\u6027\u3002\u6211\u5011\u5efa\u8b70\u5c07\u8a8d\u77e5\u8ca0\u8f09\u7406\u8ad6\u7684\u898b\u89e3\u6574\u5408\u5230 LLM \u7684\u8a2d\u8a08\u548c\u8a55\u4f30\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u9810\u6e2c\u548c\u6e1b\u8f15\u5c0d\u6297\u653b\u64ca\u7684\u98a8\u96aa\u3002\u900f\u904e\u64f4\u5c55\u6211\u5011\u7684\u5be6\u9a57\uff0c\u6db5\u84cb\u66f4\u5ee3\u6cdb\u7684\u6a21\u578b\uff0c\u4e26\u5f37\u8abf LLM \u7684 ICL \u4e2d\u7684\u6f0f\u6d1e\uff0c\u6211\u5011\u65e8\u5728\u78ba\u4fdd\u958b\u767c\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684 AI \u7cfb\u7d71\u3002", "author": "Bibek Upadhayay et.al.", "authors": "Bibek Upadhayay, Vahid Behzadan, Amin Karbasi", "id": "2410.11272v1", "paper_url": "http://arxiv.org/abs/2410.11272v1", "repo": "null"}}