{"2408.04237": {"publish_time": "2024-08-08", "title": "Learning to Rewrite: Generalized LLM-Generated Text Detection", "paper_summary": "Large language models (LLMs) can be abused at scale to create non-factual\ncontent and spread disinformation. Detecting LLM-generated content is essential\nto mitigate these risks, but current classifiers often fail to generalize in\nopen-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated\ncontent less frequently, which can be used for detection and naturally\ngeneralizes to unforeseen data. However, we find that the rewriting edit\ndistance between human and LLM content can be indistinguishable across domains,\nleading to detection failures. We propose training an LLM to rewrite input\ntext, producing minimal edits for LLM-generated content and more edits for\nhuman-written text, deriving a distinguishable and generalizable edit distance\ndifference across different domains. Experiments on text from 21 independent\ndomains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that\nour classifier outperforms the state-of-the-art zero-shot classifier by up to\n20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work\nsuggests that LLM can effectively detect machine-generated text if they are\ntrained properly.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u80fd\u6703\u88ab\u5927\u898f\u6a21\u6feb\u7528\uff0c\u7528\u65bc\u5efa\u7acb\u975e\u4e8b\u5be6\u5167\u5bb9\u4e26\u6563\u5e03\u932f\u8aa4\u8a0a\u606f\u3002\u5075\u6e2c LLM \u751f\u6210\u7684\u5167\u5bb9\u5c0d\u65bc\u6e1b\u8f15\u9019\u4e9b\u98a8\u96aa\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u5206\u985e\u5668\u901a\u5e38\u7121\u6cd5\u5728\u958b\u653e\u4e16\u754c\u7684\u74b0\u5883\u4e2d\u9032\u884c\u6982\u5316\u3002\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0cLLM \u50be\u5411\u65bc\u6e1b\u5c11\u6539\u5beb LLM \u751f\u6210\u7684\u5167\u5bb9\uff0c\u9019\u53ef\u7528\u65bc\u5075\u6e2c\u4e26\u81ea\u7136\u6982\u5316\u5230\u7121\u6cd5\u9810\u898b\u7684\u8cc7\u6599\u3002\u7136\u800c\uff0c\u6211\u5011\u767c\u73fe\u4eba\u985e\u548c LLM \u5167\u5bb9\u4e4b\u9593\u7684\u6539\u5beb\u7de8\u8f2f\u8ddd\u96e2\u5728\u4e0d\u540c\u9818\u57df\u4e2d\u53ef\u80fd\u7121\u6cd5\u5340\u5206\uff0c\u5c0e\u81f4\u5075\u6e2c\u5931\u6557\u3002\u6211\u5011\u5efa\u8b70\u8a13\u7df4 LLM \u6539\u5beb\u8f38\u5165\u6587\u5b57\uff0c\u5c0d LLM \u751f\u6210\u7684\u5167\u5bb9\u7522\u751f\u6700\u5c0f\u7684\u7de8\u8f2f\uff0c\u5c0d\u4eba\u985e\u64b0\u5beb\u7684\u6587\u5b57\u7522\u751f\u8f03\u591a\u7684\u7de8\u8f2f\uff0c\u5f9e\u800c\u5f97\u51fa\u4e0d\u540c\u9818\u57df\u4e2d\u53ef\u5340\u5206\u4e14\u53ef\u6982\u5316\u7684\u7de8\u8f2f\u8ddd\u96e2\u5dee\u7570\u3002\u91dd\u5c0d\u4f86\u81ea 21 \u500b\u7368\u7acb\u9818\u57df\u548c\u4e09\u500b\u71b1\u9580 LLM\uff08\u4f8b\u5982 GPT-4o\u3001Gemini \u548c Llama-3\uff09\u7684\u6587\u5b57\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u5206\u985e\u5668\u5728 AUROC \u5206\u6578\u4e0a\u6bd4\u6700\u5148\u9032\u7684\u96f6\u6b21\u5b78\u7fd2\u5206\u985e\u5668\u9ad8\u51fa 20.6%\uff0c\u5728 F1 \u5206\u6578\u4e0a\u6bd4\u6539\u5beb\u5206\u985e\u5668\u9ad8\u51fa 9.2%\u3002\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u53ea\u8981\u7d93\u904e\u9069\u7576\u7684\u8a13\u7df4\uff0cLLM \u53ef\u4ee5\u6709\u6548\u5075\u6e2c\u6a5f\u5668\u7522\u751f\u7684\u6587\u5b57\u3002", "author": "Wei Hao et.al.", "authors": "Wei Hao, Ran Li, Weiliang Zhao, Junfeng Yang, Chengzhi Mao", "id": "2408.04237v1", "paper_url": "http://arxiv.org/abs/2408.04237v1", "repo": "null"}}