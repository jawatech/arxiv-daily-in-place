{"2411.04642": {"publish_time": "2024-11-07", "title": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language Models", "paper_summary": "Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00 (VL) \u6a21\u578b\u5f15\u8d77\u4e86\u76f8\u7576\u5927\u7684\u7814\u7a76\u8208\u8da3\uff1b\n\u7136\u800c\uff0c\u5b83\u5011\u5728\u6709\u6548\u8655\u7406\u5f71\u50cf\u4e2d\u7684\u6587\u5b57\u6642\u4ecd\u9762\u81e8\u6311\u6230\u3002\n\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u7814\u7a76\u4eba\u54e1\u958b\u767c\u4e86\u5169\u7a2e\u65b9\u6cd5\u3002\u7b2c\u4e00\u7a2e\u65b9\u6cd5\u6d89\u53ca\u5229\u7528\u5916\u90e8\u5149\u5b78\u5b57\u5143\u8fa8\u8b58 (OCR)\n\u5de5\u5177\u5f9e\u5f71\u50cf\u4e2d\u64f7\u53d6\u6587\u5b57\u8cc7\u8a0a\uff0c\u7136\u5f8c\u5c07\u5176\u9810\u5148\u52a0\u5230\n\u5176\u4ed6\u6587\u5b57\u8f38\u5165\u3002\u7b2c\u4e8c\u7a2e\u7b56\u7565\u5c08\u6ce8\u65bc\u63a1\u7528\u6975\u9ad8\u89e3\u6790\u5ea6\u7684\u5f71\u50cf\u4f86\u6539\u5584\u6587\u5b57\u8fa8\u8b58\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\n\u6211\u5011\u5c08\u6ce8\u65bc\u900f\u904e\u5f15\u5165\u4e00\u7a2e\u540d\u70ba TAP-VL \u7684\u65b0\u65b9\u6cd5\u4f86\u589e\u5f37\u7b2c\u4e00\u7a2e\u7b56\u7565\uff0c\u5b83\u5c07 OCR \u8cc7\u8a0a\u8996\u70ba\u4e00\u7a2e\u4e0d\u540c\u7684\u65b9\u5f0f\uff0c\u4e26\u5c07\u5176\u7121\u7e2b\u6574\u5408\u5230\u4efb\u4f55 VL \u6a21\u578b\u4e2d\u3002TAP-VL \u63a1\u7528\u8f15\u91cf\u7d1a\u7684\u57fa\u65bc\u8f49\u63db\u5668\u7684\nOCR \u6a21\u7d44\u4f86\u63a5\u6536\u5177\u6709\u7248\u9762\u8cc7\u8a0a\u7684 OCR\uff0c\u5c07\u5176\u58d3\u7e2e\u6210\u4e00\u500b\u77ed\u7684\u56fa\u5b9a\u9577\u5ea6\u5e8f\u5217\uff0c\u4f5c\u70ba\u8f38\u5165\u5230 LLM\u3002\u6700\u521d\uff0c\u6211\u5011\u5c0d OCR \u6a21\u7d44\u9032\u884c\u8207\u6a21\u578b\u7121\u95dc\u7684\u9810\u8a13\u7df4\uff0c\u4f7f\u7528\u672a\u6a19\u8a18\u7684\u6587\u6a94\uff0c\u7136\u5f8c\n\u900f\u904e\u7c21\u77ed\u7684\u5fae\u8abf\u5c07\u5176\u6574\u5408\u5230\u4efb\u4f55 VL \u67b6\u69cb\u4e2d\u3002\n\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5c07 TAP-VL \u61c9\u7528\u65bc\u6548\u80fd\u6700\u4f73\u7684 VL \u6a21\u578b\u6642\uff0c\u5728\u5834\u666f\u6587\u5b57\u548c\n\u57fa\u65bc\u6587\u4ef6\u7684 VL \u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u90fd\u80fd\u6301\u7e8c\u6539\u5584\u6548\u80fd\u3002", "author": "Jonathan Fhima et.al.", "authors": "Jonathan Fhima, Elad Ben Avraham, Oren Nuriel, Yair Kittenplon, Roy Ganz, Aviad Aberdam, Ron Litman", "id": "2411.04642v1", "paper_url": "http://arxiv.org/abs/2411.04642v1", "repo": "null"}}