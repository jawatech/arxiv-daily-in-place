{"2405.05741": {"publish_time": "2024-05-09", "title": "Can large language models understand uncommon meanings of common words?", "paper_summary": "Large language models (LLMs) like ChatGPT have shown significant advancements\nacross diverse natural language understanding (NLU) tasks, including\nintelligent dialogue and autonomous agents. Yet, lacking widely acknowledged\ntesting mechanisms, answering `whether LLMs are stochastic parrots or genuinely\ncomprehend the world' remains unclear, fostering numerous studies and sparking\nheated debates. Prevailing research mainly focuses on surface-level NLU,\nneglecting fine-grained explorations. However, such explorations are crucial\nfor understanding their unique comprehension mechanisms, aligning with human\ncognition, and finally enhancing LLMs' general NLU capacities. To address this\ngap, our study delves into LLMs' nuanced semantic comprehension capabilities,\nparticularly regarding common words with uncommon meanings. The idea stems from\nfoundational principles of human communication within psychology, which\nunderscore accurate shared understandings of word semantics. Specifically, this\npaper presents the innovative construction of a Lexical Semantic Comprehension\n(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing\nboth fine-grained and cross-lingual dimensions. Introducing models of both\nopen-source and closed-source, varied scales and architectures, our extensive\nempirical experiments demonstrate the inferior performance of existing models\nin this basic lexical-meaning understanding task. Notably, even the\nstate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%\nand 22.3%, respectively. Additionally, multiple advanced prompting techniques\nand retrieval-augmented generation are also introduced to help alleviate this\ntrouble, yet limitations persist. By highlighting the above critical\nshortcomings, this research motivates further investigation and offers novel\ninsights for developing more intelligent LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 ChatGPT\uff0c\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u4efb\u52d9\u4e2d\u5df2\u5c55\u73fe\u51fa\u986f\u8457\u7684\u9032\u5c55\uff0c\u5305\u62ec\u667a\u80fd\u5c0d\u8a71\u548c\u81ea\u4e3b\u4ee3\u7406\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u5ee3\u6cdb\u8a8d\u53ef\u7684\u6e2c\u8a66\u6a5f\u5236\uff0c\u56de\u7b54\u300cLLM \u662f\u96a8\u6a5f\u9e1a\u9d61\u9084\u662f\u771f\u6b63\u7406\u89e3\u4e16\u754c\u300d\u7684\u554f\u984c\u4ecd\u7136\u4e0d\u660e\u78ba\uff0c\u9019\u4e5f\u4fc3\u6210\u4e86\u8a31\u591a\u7814\u7a76\u548c\u5f15\u767c\u6fc0\u70c8\u7684\u8faf\u8ad6\u3002\u73fe\u884c\u7684\u7814\u7a76\u4e3b\u8981\u95dc\u6ce8\u65bc\u8868\u9762\u5c64\u7684 NLU\uff0c\u5ffd\u7565\u4e86\u7d30\u7dfb\u7684\u63a2\u8a0e\u3002\u7136\u800c\uff0c\u6b64\u985e\u63a2\u8a0e\u5c0d\u65bc\u7406\u89e3\u5176\u7368\u7279\u7684\u7406\u89e3\u6a5f\u5236\u3001\u8207\u4eba\u985e\u8a8d\u77e5\u4fdd\u6301\u4e00\u81f4\uff0c\u4e26\u6700\u7d42\u589e\u5f37 LLM \u7684\u4e00\u822c NLU \u80fd\u529b\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u7684\u7814\u7a76\u6df1\u5165\u63a2\u8a0e\u4e86 LLM \u7684\u7d30\u5fae\u8a9e\u7fa9\u7406\u89e3\u80fd\u529b\uff0c\u7279\u5225\u662f\u95dc\u65bc\u5177\u6709\u4e0d\u5c0b\u5e38\u542b\u7fa9\u7684\u5e38\u898b\u8a5e\u5f59\u3002\u9019\u500b\u60f3\u6cd5\u6e90\u81ea\u5fc3\u7406\u5b78\u4e2d\u4eba\u985e\u6e9d\u901a\u7684\u57fa\u672c\u539f\u7406\uff0c\u5b83\u5f37\u8abf\u6e96\u78ba\u5171\u4eab\u5c0d\u8a5e\u5f59\u8a9e\u7fa9\u7684\u7406\u89e3\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5275\u65b0\u7684\u8a5e\u5f59\u8a9e\u7fa9\u7406\u89e3 (LeSC) \u8cc7\u6599\u96c6\u5efa\u69cb\uff0c\u4e26\u63a1\u7528\u65b0\u7a4e\u7684\u8a55\u4f30\u6307\u6a19\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u6db5\u84cb\u7d30\u7dfb\u548c\u8de8\u8a9e\u8a00\u7dad\u5ea6\u7684\u57fa\u6e96\u3002\u900f\u904e\u5f15\u5165\u958b\u6e90\u548c\u9589\u6e90\u3001\u4e0d\u540c\u898f\u6a21\u548c\u67b6\u69cb\u7684\u6a21\u578b\uff0c\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u8b49\u5be6\u9a57\u8b49\u660e\u4e86\u73fe\u6709\u6a21\u578b\u5728\u9019\u500b\u57fa\u672c\u7684\u8a5e\u5f59\u542b\u7fa9\u7406\u89e3\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u8f03\u5dee\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u662f\u73fe\u4eca\u6700\u5148\u9032\u7684 LLM GPT-4 \u548c GPT-3.5 \u4e5f\u5206\u5225\u843d\u5f8c\u65bc 16 \u6b72\u7684\u4eba\u985e 3.9% \u548c 22.3%\u3002\u6b64\u5916\uff0c\u4e5f\u5f15\u5165\u4e86\u591a\u7a2e\u9032\u968e\u63d0\u793a\u6280\u8853\u548c\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff0c\u4ee5\u5e6b\u52a9\u7de9\u89e3\u9019\u500b\u554f\u984c\uff0c\u4f46\u9650\u5236\u4ecd\u7136\u5b58\u5728\u3002\u900f\u904e\u5f37\u8abf\u4e0a\u8ff0\u95dc\u9375\u7f3a\u9ede\uff0c\u672c\u7814\u7a76\u6fc0\u52f5\u4e86\u9032\u4e00\u6b65\u7684\u8abf\u67e5\uff0c\u4e26\u70ba\u958b\u767c\u66f4\u667a\u80fd\u7684 LLM \u63d0\u4f9b\u4e86\u65b0\u7a4e\u7684\u898b\u89e3\u3002", "author": "Jinyang Wu et.al.", "authors": "Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao", "id": "2405.05741v1", "paper_url": "http://arxiv.org/abs/2405.05741v1", "repo": "null"}}