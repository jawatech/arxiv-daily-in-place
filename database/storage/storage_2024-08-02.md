# arxiv-daily
 Automated deployment @ 2024-08-02 08:58:22 Asia/Taipei
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/storage).

## AI

### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-31**|**Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**|Atsuyuki Miyai et.al.|[2407.21794v1](http://arxiv.org/abs/2407.21794v1)|null|
|**2024-07-31**|**Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**|Richard Ren et.al.|[2407.21792v1](http://arxiv.org/abs/2407.21792v1)|null|
|**2024-07-31**|**Vision-Language Model Based Handwriting Verification**|Mihir Chauhan et.al.|[2407.21788v1](http://arxiv.org/abs/2407.21788v1)|null|
|**2024-07-31**|**Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**|Bradley Brown et.al.|[2407.21787v1](http://arxiv.org/abs/2407.21787v1)|null|
|**2024-07-31**|**The Llama 3 Herd of Models**|Abhimanyu Dubey et.al.|[2407.21783v1](http://arxiv.org/abs/2407.21783v1)|null|
|**2024-07-31**|**Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**|Felix Ocker et.al.|[2407.21778v1](http://arxiv.org/abs/2407.21778v1)|null|
|**2024-07-31**|**ShieldGemma: Generative AI Content Moderation Based on Gemma**|Wenjun Zeng et.al.|[2407.21772v1](http://arxiv.org/abs/2407.21772v1)|null|
|**2024-07-31**|**MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**|Xi Victoria Lin et.al.|[2407.21770v1](http://arxiv.org/abs/2407.21770v1)|null|
|**2024-07-31**|**HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**|Junwei He et.al.|[2407.21742v1](http://arxiv.org/abs/2407.21742v1)|null|
|**2024-07-31**|**Contrastive Factor Analysis**|Zhibin Duan et.al.|[2407.21740v2](http://arxiv.org/abs/2407.21740v2)|null|
|**2024-07-31**|**A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**|Mothilal Asokan et.al.|[2407.21739v1](http://arxiv.org/abs/2407.21739v1)|null|
|**2024-07-31**|**Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**|Joseph Geo Benjamin et.al.|[2407.21738v1](http://arxiv.org/abs/2407.21738v1)|null|
|**2024-07-31**|**Open-Vocabulary Audio-Visual Semantic Segmentation**|Ruohao Guo et.al.|[2407.21721v1](http://arxiv.org/abs/2407.21721v1)|null|
|**2024-07-31**|**Social Learning through Interactions with Other Agents: A Survey**|Dylan hillier et.al.|[2407.21713v1](http://arxiv.org/abs/2407.21713v1)|null|
|**2024-07-31**|**Adaptive Retrieval-Augmented Generation for Conversational Systems**|Xi Wang et.al.|[2407.21712v1](http://arxiv.org/abs/2407.21712v1)|null|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**|Ming Zhang et.al.|[2407.21693v1](http://arxiv.org/abs/2407.21693v1)|null|
|**2024-07-31**|**Dynamic Object Queries for Transformer-based Incremental Object Detection**|Jichuan Zhang et.al.|[2407.21687v1](http://arxiv.org/abs/2407.21687v1)|null|
|**2024-07-31**|**Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**|Krishan Agyakari Raja Babu et.al.|[2407.21674v1](http://arxiv.org/abs/2407.21674v1)|null|
|**2024-07-31**|**Universal Approximation Theory: Foundations for Parallelism in Neural Networks**|Wei Wang et.al.|[2407.21670v1](http://arxiv.org/abs/2407.21670v1)|null|
|**2024-07-31**|**Synth-Empathy: Towards High-Quality Synthetic Empathy Data**|Hao Liang et.al.|[2407.21669v1](http://arxiv.org/abs/2407.21669v1)|null|
|**2024-07-31**|**An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**|Aswini Kumar Patra et.al.|[2407.21666v1](http://arxiv.org/abs/2407.21666v1)|null|
|**2024-07-31**|**Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**|Yue Xu et.al.|[2407.21659v2](http://arxiv.org/abs/2407.21659v2)|null|
|**2024-07-31**|**Spatial Transformer Network YOLO Model for Agricultural Object Detection**|Yash Zambre et.al.|[2407.21652v1](http://arxiv.org/abs/2407.21652v1)|null|
|**2024-07-31**|**Human interaction classifier for LLM based chatbot**|Diego Mart√≠n et.al.|[2407.21647v1](http://arxiv.org/abs/2407.21647v1)|null|
|**2024-07-31**|**Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**|Shanbo Cheng et.al.|[2407.21646v1](http://arxiv.org/abs/2407.21646v1)|null|
|**2024-07-31**|**Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**|Hermione Warr et.al.|[2407.21638v1](http://arxiv.org/abs/2407.21638v1)|null|
|**2024-07-31**|**Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**|Xiang Luo et.al.|[2407.21633v1](http://arxiv.org/abs/2407.21633v1)|null|
|**2024-07-31**|**TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**|Gabriel Loiseau et.al.|[2407.21630v1](http://arxiv.org/abs/2407.21630v1)|null|
|**2024-07-31**|**Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**|Pedro Sarmento et.al.|[2407.21615v1](http://arxiv.org/abs/2407.21615v1)|null|
|**2024-07-31**|**Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**|Jiafeng Zhong et.al.|[2407.21611v1](http://arxiv.org/abs/2407.21611v1)|null|
|**2024-07-31**|**Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**|Shoujin Huang et.al.|[2407.21600v1](http://arxiv.org/abs/2407.21600v1)|null|
|**2024-07-31**|**Voxel Scene Graph for Intracranial Hemorrhage**|Antoine P. Sanner et.al.|[2407.21580v1](http://arxiv.org/abs/2407.21580v1)|null|
|**2024-07-31**|**A Performance Study of LLM-Generated Code on Leetcode**|Tristan Coignion et.al.|[2407.21579v1](http://arxiv.org/abs/2407.21579v1)|null|
|**2024-07-31**|**Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**|Kit M. Bransby et.al.|[2407.21577v1](http://arxiv.org/abs/2407.21577v1)|null|
|**2024-07-31**|**PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**|Min Jae Jung et.al.|[2407.21571v1](http://arxiv.org/abs/2407.21571v1)|null|
|**2024-07-31**|**Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**|Jun Zhou et.al.|[2407.21560v1](http://arxiv.org/abs/2407.21560v1)|null|
|**2024-07-31**|**Operator-based semantics for choice programs: is choosing losing? (full version)**|Jesse Heyninck et.al.|[2407.21556v1](http://arxiv.org/abs/2407.21556v1)|null|
|**2024-07-31**|**Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**|Jiang Li et.al.|[2407.21536v1](http://arxiv.org/abs/2407.21536v1)|null|
|**2024-07-31**|**Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**|Ziya Zhou et.al.|[2407.21531v1](http://arxiv.org/abs/2407.21531v1)|null|
|**2024-07-31**|**Data Contamination Report from the 2024 CONDA Shared Task**|Oscar Sainz et.al.|[2407.21530v1](http://arxiv.org/abs/2407.21530v1)|null|
|**2024-07-31**|**Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**|Lingxi Cui et.al.|[2407.21523v1](http://arxiv.org/abs/2407.21523v1)|null|
|**2024-07-31**|**Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**|I. M. Chernenkiy et.al.|[2407.21516v1](http://arxiv.org/abs/2407.21516v1)|null|
|**2024-07-31**|**Interpreting and learning voice commands with a Large Language Model for a robot system**|Stanislau Stankevich et.al.|[2407.21512v1](http://arxiv.org/abs/2407.21512v1)|null|
|**2024-07-31**|**FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**|Yuna Yan et.al.|[2407.21507v1](http://arxiv.org/abs/2407.21507v1)|null|
|**2024-07-31**|**MaskUno: Switch-Split Block For Enhancing Instance Segmentation**|Jawad Haidar et.al.|[2407.21498v1](http://arxiv.org/abs/2407.21498v1)|null|
|**2024-07-31**|**Generative Expressive Conversational Speech Synthesis**|Rui Liu et.al.|[2407.21491v2](http://arxiv.org/abs/2407.21491v2)|null|
|**2024-07-31**|**Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**|Junxuan Yu et.al.|[2407.21490v1](http://arxiv.org/abs/2407.21490v1)|null|
|**2024-07-31**|**Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**|Giuliano Martinelli et.al.|[2407.21489v1](http://arxiv.org/abs/2407.21489v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xiny Pan et.al.|[2407.21483v2](http://arxiv.org/abs/2407.21483v2)|null|
|**2024-07-31**|**On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**|Nick Rossenbach et.al.|[2407.21476v1](http://arxiv.org/abs/2407.21476v1)|null|
|**2024-07-31**|**Fine-gained Zero-shot Video Sampling**|Dengsheng Chen et.al.|[2407.21475v1](http://arxiv.org/abs/2407.21475v1)|null|
|**2024-07-31**|**An Invertible State Space for Process Trees**|Gero Kolhof et.al.|[2407.21468v1](http://arxiv.org/abs/2407.21468v1)|null|
|**2024-07-31**|**Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**|Mengtian Kang et.al.|[2407.21467v1](http://arxiv.org/abs/2407.21467v1)|null|
|**2024-07-31**|**KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**|Gilang Fajar Febrian et.al.|[2407.21459v1](http://arxiv.org/abs/2407.21459v1)|null|
|**2024-07-31**|**TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**|Zhaolan Huang et.al.|[2407.21453v1](http://arxiv.org/abs/2407.21453v1)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**|Taiji Li et.al.|[2407.21443v1](http://arxiv.org/abs/2407.21443v1)|null|
|**2024-07-31**|**QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**|Ritvik Setty et.al.|[2407.21441v2](http://arxiv.org/abs/2407.21441v2)|null|
|**2024-07-31**|**MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**|Zhanpeng Chen et.al.|[2407.21439v1](http://arxiv.org/abs/2407.21439v1)|null|
|**2024-07-31**|**Deformable 3D Shape Diffusion Model**|Dengsheng Chen et.al.|[2407.21428v1](http://arxiv.org/abs/2407.21428v1)|null|
|**2024-07-31**|**Cost-Effective Hallucination Detection for LLMs**|Simon Valentin et.al.|[2407.21424v1](http://arxiv.org/abs/2407.21424v1)|null|
|**2024-07-31**|**Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**|Zhengxuan Wu et.al.|[2407.21417v1](http://arxiv.org/abs/2407.21417v1)|null|
|**2024-07-31**|**Towards interfacing large language models with ASR systems using confidence measures and prompting**|Maryam Naderi et.al.|[2407.21414v1](http://arxiv.org/abs/2407.21414v1)|null|
|**2024-07-31**|**GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**|Yanxu Mao et.al.|[2407.21384v1](http://arxiv.org/abs/2407.21384v1)|null|
|**2024-07-31**|**An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**|Hongxun Zhou et.al.|[2407.21376v1](http://arxiv.org/abs/2407.21376v1)|null|
|**2024-07-31**|**Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**|Danfeng Guo et.al.|[2407.21368v1](http://arxiv.org/abs/2407.21368v1)|null|
|**2024-07-31**|**ProSpec RL: Plan Ahead, then Execute**|Liangliang Liu et.al.|[2407.21359v1](http://arxiv.org/abs/2407.21359v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|null|
|**2024-07-31**|**Small Object Few-shot Segmentation for Vision-based Industrial Inspection**|Zilong Zhang et.al.|[2407.21351v1](http://arxiv.org/abs/2407.21351v1)|null|
|**2024-07-31**|**Differentially Private Block-wise Gradient Shuffle for Deep Learning**|David Zagardo et.al.|[2407.21347v1](http://arxiv.org/abs/2407.21347v1)|null|
|**2024-07-31**|**Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**|Jingyao Wu et.al.|[2407.21344v1](http://arxiv.org/abs/2407.21344v1)|null|
|**2024-07-31**|**Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**|David Valencia et.al.|[2407.21338v1](http://arxiv.org/abs/2407.21338v1)|null|
|**2024-07-31**|**Performance of Recent Large Language Models for a Low-Resourced Language**|Ravindu Jayakody et.al.|[2407.21330v1](http://arxiv.org/abs/2407.21330v1)|null|
|**2024-07-31**|**MetaOpenFOAM: an LLM-based multi-agent framework for CFD**|Yuxuan Chena et.al.|[2407.21320v1](http://arxiv.org/abs/2407.21320v1)|null|
|**2024-07-31**|**Big Cooperative Learning**|Yulai Cong et.al.|[2407.21319v1](http://arxiv.org/abs/2407.21319v1)|null|
|**2024-07-31**|**Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**|Zehui Wu et.al.|[2407.21315v2](http://arxiv.org/abs/2407.21315v2)|null|
|**2024-07-31**|**EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**|Ali Abedi et.al.|[2407.21311v1](http://arxiv.org/abs/2407.21311v1)|null|
|**2024-07-31**|**Implementing Streaming algorithm and k-means clusters to RAG**|Haoyu Kang et.al.|[2407.21300v1](http://arxiv.org/abs/2407.21300v1)|null|
|**2024-07-31**|**Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**|Kaustav Bhattacharjee et.al.|[2407.21299v1](http://arxiv.org/abs/2407.21299v1)|null|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-31**|**Robust Box Prompt based SAM for Medical Image Segmentation**|Yuhao Huang et.al.|[2407.21284v1](http://arxiv.org/abs/2407.21284v1)|null|
|**2024-07-31**|**Multi-Level Querying using A Knowledge Pyramid**|Rubing Chen et.al.|[2407.21276v1](http://arxiv.org/abs/2407.21276v1)|null|
|**2024-07-31**|**Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**|Rohini Banerjee et.al.|[2407.21273v1](http://arxiv.org/abs/2407.21273v1)|null|
|**2024-07-31**|**DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**|Dongwon Son et.al.|[2407.21267v1](http://arxiv.org/abs/2407.21267v1)|null|
|**2024-07-31**|**Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**|Alimohammad Beigi et.al.|[2407.21264v1](http://arxiv.org/abs/2407.21264v1)|null|
|**2024-07-31**|**Lifelong Person Search**|Jae-Won Yang et.al.|[2407.21252v1](http://arxiv.org/abs/2407.21252v1)|null|
|**2024-07-30**|**Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**|Anqi Zhang et.al.|[2407.21248v1](http://arxiv.org/abs/2407.21248v1)|null|
|**2024-07-30**|**Informed Correctors for Discrete Diffusion Models**|Yixiu Zhao et.al.|[2407.21243v1](http://arxiv.org/abs/2407.21243v1)|null|
|**2024-07-30**|**Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**|Ngoc Son Nguyen et.al.|[2407.21229v1](http://arxiv.org/abs/2407.21229v1)|null|
|**2024-07-30**|**Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**|Florian Tambon et.al.|[2407.21227v1](http://arxiv.org/abs/2407.21227v1)|null|
|**2024-07-30**|**AI methods for approximate compiling of unitaries**|David Kremer et.al.|[2407.21225v1](http://arxiv.org/abs/2407.21225v1)|null|
|**2024-07-30**|**LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**|H. Emre Erdem et.al.|[2407.21204v1](http://arxiv.org/abs/2407.21204v1)|null|
|**2024-07-30**|**GenRec: Generative Personalized Sequential Recommendation**|Panfeng Cao et.al.|[2407.21191v1](http://arxiv.org/abs/2407.21191v1)|null|
|**2024-07-30**|**AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**|Maisha Binte Rashid et.al.|[2407.21174v1](http://arxiv.org/abs/2407.21174v1)|null|
|**2024-07-30**|**Decomposed Prompting to Answer Questions on a Course Discussion Board**|Brandon Jaipersaud et.al.|[2407.21170v1](http://arxiv.org/abs/2407.21170v1)|null|
|**2024-07-30**|**Understanding Public Safety Trends in Calgary through data mining**|Zack Dewis et.al.|[2407.21163v1](http://arxiv.org/abs/2407.21163v1)|null|
|**2024-07-30**|**Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**|Alaa Aljabari et.al.|[2407.21153v1](http://arxiv.org/abs/2407.21153v1)|null|
|**2024-07-30**|**Private Collaborative Edge Inference via Over-the-Air Computation**|Selim F. Yilmaz et.al.|[2407.21151v1](http://arxiv.org/abs/2407.21151v1)|null|
|**2024-07-30**|**Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**|Mayanka Chandrashekar et.al.|[2407.21149v1](http://arxiv.org/abs/2407.21149v1)|null|

#### Abstracts
##### **Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**
2407.21794v1 by Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa

Detecting out-of-distribution (OOD) samples is crucial for ensuring the
safety of machine learning systems and has shaped the field of OOD detection.
Meanwhile, several other problems are closely related to OOD detection,
including anomaly detection (AD), novelty detection (ND), open set recognition
(OSR), and outlier detection (OD). To unify these problems, a generalized OOD
detection framework was proposed, taxonomically categorizing these five
problems. However, Vision Language Models (VLMs) such as CLIP have
significantly changed the paradigm and blurred the boundaries between these
fields, again confusing researchers. In this survey, we first present a
generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD
detection, and OD in the VLM era. Our framework reveals that, with some field
inactivity and integration, the demanding challenges have become OOD detection
and AD. In addition, we also highlight the significant shift in the definition,
problem settings, and benchmarks; we thus feature a comprehensive review of the
methodology for OOD detection, including the discussion over other related
tasks to clarify their relationship to OOD detection. Finally, we explore the
advancements in the emerging Large Vision Language Model (LVLM) era, such as
GPT-4V. We conclude this survey with open challenges and future directions.

ÊëòË¶ÅÔºöÂÅµÊ∏¨Áï∞Â∏∏Ê®£Êú¨ (OOD) Â∞çÊñºÁ¢∫‰øùÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±ÁöÑÂÆâÂÖ®ÊÄßËá≥ÈóúÈáçË¶ÅÔºå‰∏¶ÂΩ¢Â°ë‰∫Ü OOD ÂÅµÊ∏¨È†òÂüü„ÄÇÂêåÊôÇÔºåÈÇÑÊúâË®±Â§öÂÖ∂‰ªñÂïèÈ°åËàá OOD ÂÅµÊ∏¨ÊÅØÊÅØÁõ∏ÈóúÔºåÂåÖÊã¨Áï∞Â∏∏ÂÅµÊ∏¨ (AD)„ÄÅÊñ∞Á©éÊÄßÂÅµÊ∏¨ (ND)„ÄÅÈñãÊîæÈõÜË≠òÂà• (OSR) ÂíåÈõ¢Áæ§ÂÄºÂÅµÊ∏¨ (OD)„ÄÇÁÇ∫‰∫ÜÁµ±‰∏ÄÈÄô‰∫õÂïèÈ°åÔºåÊèêÂá∫‰∫ÜÂª£Áæ©ÁöÑ OOD ÂÅµÊ∏¨Êû∂ÊßãÔºåÂ∞áÈÄô‰∫îÂÄãÂïèÈ°åÂàÜÈ°û„ÄÇÁÑ∂ËÄåÔºåÂÉè CLIP Á≠âË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Â∑≤Â§ßÂπÖÊîπËÆäÂÖ∏ÁØÑÔºå‰∏¶Ê®°Á≥ä‰∫ÜÈÄô‰∫õÈ†òÂüü‰πãÈñìÁöÑÁïåÁ∑öÔºåÂÜçÊ¨°ËÆìÁ†îÁ©∂‰∫∫Âì°ÊÑüÂà∞Âõ∞ÊÉë„ÄÇÂú®ÈÄôÈ†ÖË™øÊü•‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫Âª£Áæ©ÁöÑ OOD ÂÅµÊ∏¨ v2ÔºåÊ¶ÇÊã¨‰∫Ü AD„ÄÅND„ÄÅOSR„ÄÅOOD ÂÅµÊ∏¨Âíå OD Âú® VLM ÊôÇ‰ª£ÁöÑÊºîÈÄ≤„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊè≠Á§∫ÔºåÁî±ÊñºÊüê‰∫õÈ†òÂüüÁöÑ‰∏çÊ¥ªË∫çÂíåÊï¥ÂêàÔºåÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÂ∑≤ÊàêÁÇ∫ OOD ÂÅµÊ∏¨Âíå AD„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰πüÈáçÈªûË™™ÊòéÂÆöÁæ©„ÄÅÂïèÈ°åË®≠ÂÆöÂíåÂü∫Ê∫ñÁöÑÈáçÂ§ßËΩâËÆäÔºõÂõ†Ê≠§ÔºåÊàëÂÄëÂ∞ç OOD ÂÅµÊ∏¨ÁöÑÊñπÊ≥ïË´ñÈÄ≤Ë°åÂÖ®Èù¢Ê™¢Ë¶ñÔºåÂåÖÊã¨Ë®éË´ñÂÖ∂‰ªñÁõ∏Èóú‰ªªÂãô‰ª•ÈáêÊ∏ÖÂÆÉÂÄëËàá OOD ÂÅµÊ∏¨ÁöÑÈóú‰øÇ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éÊñ∞ËààÁöÑÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLM) ÊôÇ‰ª£ÁöÑÈÄ≤Â±ïÔºå‰æãÂ¶Ç GPT-4V„ÄÇÊàëÂÄë‰ª•ÈñãÊîæÊåëÊà∞ÂíåÊú™‰æÜÊñπÂêë‰ΩúÁÇ∫ÈÄôÈ†ÖË™øÊü•ÁöÑÁµêË´ñ„ÄÇ

##### **Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**
2407.21792v1 by Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks

As artificial intelligence systems grow more powerful, there has been
increasing interest in "AI safety" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling "safetywashing" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ËÆäÂæóË∂ä‰æÜË∂äÂº∑Â§ßÔºåÂ∞çÊñº„ÄåAI ÂÆâÂÖ®„ÄçÁöÑÁ†îÁ©∂ËààË∂£‰πüËàáÊó•‰ø±Â¢ûÔºå‰ª•Âõ†ÊáâÊñ∞ËààÂíåÊú™‰æÜÁöÑÈ¢®Èö™„ÄÇÁÑ∂ËÄåÔºåAI ÂÆâÂÖ®È†òÂüüÁöÑÂÆöÁæ©‰ªçÁÑ∂ÂæàÊ®°Á≥äÔºåË°°ÈáèÊ®ôÊ∫ñ‰πü‰∏ç‰∏ÄËá¥ÔºåÂ∞éËá¥Á†îÁ©∂‰∫∫Âì°Â¶Ç‰ΩïÂÅöÂá∫Ë≤¢ÁçªÊÑüÂà∞Âõ∞ÊÉë„ÄÇAI ÂÆâÂÖ®Âü∫Ê∫ñËàá‰∏äÊ∏∏‰∏ÄËà¨ËÉΩÂäõÔºà‰æãÂ¶Ç‰∏ÄËà¨Áü•Ë≠òÂíåÊé®ÁêÜÔºâ‰πãÈñìÈóú‰øÇ‰∏çÊòéÁ¢∫ÔºåÈÄ≤‰∏ÄÊ≠•Âä†Âäá‰∫ÜÈÄôÁ®Æ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂ∞ç AI ÂÆâÂÖ®Âü∫Ê∫ñÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂæåË®≠ÂàÜÊûêÔºåÊ†πÊìöÊï∏ÂçÅÂÄãÊ®°ÂûãÂØ¶Ë≠âÂàÜÊûêÂÆÉÂÄëËàá‰∏ÄËà¨ËÉΩÂäõÁöÑÁõ∏ÈóúÊÄßÔºå‰∏¶Â∞ç AI ÂÆâÂÖ®‰∏≠ÁöÑÁèæÊúâÊñπÂêëÈÄ≤Ë°åË™øÊü•„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåË®±Â§öÂÆâÂÖ®Âü∫Ê∫ñËàá‰∏äÊ∏∏Ê®°ÂûãËÉΩÂäõÈ´òÂ∫¶Áõ∏ÈóúÔºåÈÄôÂèØËÉΩÊúÉÂ∞éËá¥„ÄåÂÆâÂÖ®ÊºÇÁôΩ„Äç‚Äî‚ÄîÂ∞áËÉΩÂäõÁöÑÊèêÂçáË™§Ë™çÁÇ∫ÊòØÂÆâÂÖ®ÊÄßÁöÑÈÄ≤Ê≠•„ÄÇÊ†πÊìöÈÄô‰∫õÁ†îÁ©∂ÁµêÊûúÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂØ¶Ë≠âÂü∫Á§éÔºåÁî®ÊñºÈñãÁôºÊõ¥ÊúâÊÑèÁæ©ÁöÑÂÆâÂÖ®ÊåáÊ®ôÔºå‰∏¶Âú®Ê©üÂô®Â≠∏ÁøíÁ†îÁ©∂ËÉåÊôØ‰∏ãÂ∞á AI ÂÆâÂÖ®ÂÆöÁæ©ÁÇ∫‰∏ÄÁµÑÊòéÁ¢∫ÁïåÂÆöÁöÑÁ†îÁ©∂ÁõÆÊ®ôÔºåÈÄô‰∫õÁõÆÊ®ôÂú®ÂØ¶Ë≠â‰∏äÂèØ‰ª•Ëàá‰∏ÄËà¨ËÉΩÂäõÁöÑÈÄ≤Ê≠•ÂçÄÂàÜÈñã‰æÜ„ÄÇÈÄèÈÅéÈÄôÈ∫ºÂÅöÔºåÊàëÂÄëÊó®Âú®ÁÇ∫ AI ÂÆâÂÖ®Á†îÁ©∂Êèê‰æõ‰∏ÄÂÄãÊõ¥Âö¥Ë¨πÁöÑÊû∂ÊßãÔºåÊé®ÈÄ≤ÂÆâÂÖ®Ë©ï‰º∞ÁöÑÁßëÂ≠∏Ôºå‰∏¶ÈáêÊ∏ÖÈÇÅÂêëÂèØË°°ÈáèÈÄ≤Â±ïÁöÑÈÅìË∑Ø„ÄÇ

##### **Vision-Language Model Based Handwriting Verification**
2407.21788v1 by Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, Bina Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari

Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.

ÊëòË¶ÅÔºöÊâãÂØ´È©óË≠âÂú®Êñá‰ª∂ÈëëË≠ò‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂèóÂà∞Êñá‰ª∂ÈëëË≠òÂ∞àÂÆ∂ÁöÑÊá∑ÁñëÔºåÂéüÂõ†Âú®ÊñºÂÆÉÂÄëÁº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏î‰æùË≥¥ÊñºÂ§ßÈáèÁöÑË®ìÁ∑¥Ë≥áÊñôÂíåÊâãÂ∑•ÁâπÂæµ„ÄÇÊú¨ÊñáÊé¢Ë®é‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)Ôºå‰æãÂ¶Ç OpenAI ÁöÑ GPT-4o Âíå Google ÁöÑ PaliGemmaÔºå‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÈÄöÈÅéÂà©Áî®ÂÆÉÂÄëÁöÑË¶ñË¶∫ÂïèÁ≠îËÉΩÂäõÂíå 0-shot ÊÄùÊÉ≥Èèà (CoT) Êé®ÁêÜÔºåÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÁÇ∫Ê®°ÂûãÊ±∫Á≠ñÊèê‰æõÊ∏ÖÊô∞„ÄÅ‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑËß£Èáã„ÄÇÊàëÂÄëÂú® CEDAR ÊâãÂØ´Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåVLM Êèê‰æõ‰∫ÜÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÔºåÊ∏õÂ∞ë‰∫ÜÂ∞çÂ§ßÂûãË®ìÁ∑¥Ë≥áÊñôÈõÜÁöÑÈúÄÊ±ÇÔºå‰∏¶‰∏îÂèØ‰ª•Êõ¥Â•ΩÂú∞ÈÅ©Êáâ‰∏çÂêåÁöÑÊâãÂØ´È¢®Ê†º„ÄÇÁÑ∂ËÄåÔºåÁµêÊûúË°®ÊòéÔºåÂü∫Êñº CNN ÁöÑ ResNet-18 Êû∂ÊßãÂÑ™Êñº‰ΩøÁî® GPT-4oÔºàÊ∫ñÁ¢∫ÁéáÔºö70%ÔºâÂíåÁõ£Áù£ÂæÆË™ø PaliGemmaÔºàÊ∫ñÁ¢∫ÁéáÔºö71%ÔºâÁöÑ 0-shot CoT ÊèêÁ§∫Â∑•Á®ãÊñπÊ≥ïÔºåÂú® CEDAR AND Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 84% ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫Ü VLM Âú®Áî¢Áîü‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑÊ±∫Á≠ñÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂêåÊôÇ‰πüÂº∑Ë™ø‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÂçáËàáÂ∞àÊ•≠Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁõ∏ÂåπÈÖçÁöÑÊïàËÉΩÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**
2407.21787v1 by Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R√©, Azalia Mirhoseini

Scaling the amount of compute used to train language models has dramatically
improved their capabilities. However, when it comes to inference, we often
limit the amount of compute to only one attempt per problem. Here, we explore
inference compute as another axis for scaling by increasing the number of
generated samples. Across multiple tasks and models, we observe that coverage -
the fraction of problems solved by any attempt - scales with the number of
samples over four orders of magnitude. In domains like coding and formal
proofs, where all answers can be automatically verified, these increases in
coverage directly translate into improved performance. When we apply repeated
sampling to SWE-bench Lite, the fraction of issues solved with
DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250
samples, outperforming the single-attempt state-of-the-art of 43% which uses
more capable frontier models. Moreover, using current API pricing, amplifying
the cheaper DeepSeek model with five samples is more cost-effective and solves
more issues than paying a premium for one sample from GPT-4o or Claude 3.5
Sonnet. Interestingly, the relationship between coverage and the number of
samples is often log-linear and can be modelled with an exponentiated power
law, suggesting the existence of inference-time scaling laws. Finally, we find
that identifying correct samples out of many generations remains an important
direction for future research in domains without automatic verifiers. When
solving math word problems from GSM8K and MATH, coverage with Llama-3 models
grows to over 95% with 10,000 samples. However, common methods to pick correct
solutions from a sample collection, such as majority voting or reward models,
plateau beyond several hundred samples and fail to fully scale with the sample
budget.

ÊëòË¶ÅÔºö<paragraph>Êì¥Â§ßÁî®ÊñºË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÈÅãÁÆóÈáèÂ∑≤Â§ßÂπÖÊèêÂçáÂÖ∂ÂäüËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÄ≤Ë°åÊé®Ë´ñÊôÇÔºåÊàëÂÄëÈÄöÂ∏∏Â∞áÈÅãÁÆóÈáèÈôêÂà∂Âú®ÊØèÂÄãÂïèÈ°åÂÉÖÂòóË©¶‰∏ÄÊ¨°„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ∞áÊé®Ë´ñÈÅãÁÆóË¶ñÁÇ∫Âè¶‰∏ÄÁ®ÆÊì¥Â±ïËª∏ÔºåËóâÁî±Â¢ûÂä†ÁîüÊàêÁØÑ‰æãÁöÑÊï∏Èáè‰æÜÈÄ≤Ë°åÊì¥Â±ï„ÄÇÂú®Â§öÂÄã‰ªªÂãôÂíåÊ®°Âûã‰∏≠ÔºåÊàëÂÄëËßÄÂØüÂà∞Ë¶ÜËìãÁéáÔºà‰ªª‰ΩïÂòóË©¶Ëß£Ê±∫ÂïèÈ°åÁöÑÂàÜÊï∏ÔºâÊúÉÈö®ËëóÁØÑ‰æãÊï∏ÈáèËÄåÊì¥Â±ïÔºåË∂ÖÈÅéÂõõÂÄãÊï∏ÈáèÁ¥ö„ÄÇÂú®Á∑®Á¢ºÂíåÂΩ¢ÂºèÂåñË≠âÊòéÁ≠âÈ†òÂüü‰∏≠ÔºåÊâÄÊúâÁ≠îÊ°àÈÉΩÂèØ‰ª•Ëá™ÂãïÈ©óË≠âÔºåÈÄô‰∫õË¶ÜËìãÁéáÁöÑÂ¢ûÂä†ÊúÉÁõ¥Êé•ËΩâÂåñÁÇ∫ÊïàËÉΩÁöÑÊèêÂçá„ÄÇÁï∂ÊàëÂÄëÂ∞áÈáçË§áÊäΩÊ®£Â•óÁî®Êñº SWE-bench Lite ÊôÇÔºå‰ΩøÁî® DeepSeek-V2-Coder-Instruct Ëß£Ê±∫ÂïèÈ°åÁöÑÂàÜÊï∏Âæû‰∏ÄÂÄãÁØÑ‰æãÁöÑ 15.9% ÊèêÂçáÂà∞ 250 ÂÄãÁØÑ‰æãÁöÑ 56%ÔºåÂÑ™Êñº‰ΩøÁî®ÂäüËÉΩÊõ¥Âº∑Â§ßÁöÑÂâçÊ≤øÊ®°ÂûãËÄåÈÅîÂà∞ÁöÑ 43% ÂñÆÊ¨°ÂòóË©¶ÊúÄÂÖàÈÄ≤Ê∞¥Ê∫ñ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®ÁõÆÂâçÁöÑ API ÂÆöÂÉπÔºå‰ª•‰∫îÂÄãÁØÑ‰æãÊì¥ÂÖÖËºÉ‰æøÂÆúÁöÑ DeepSeek Ê®°ÂûãÊØîÊîØ‰ªòÊ∫¢ÂÉπÂèñÂæó GPT-4o Êàñ Claude 3.5 Sonnet ÁöÑ‰∏ÄÂÄãÁØÑ‰æãÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÔºå‰∏îËÉΩËß£Ê±∫Êõ¥Â§öÂïèÈ°å„ÄÇÊúâË∂£ÁöÑÊòØÔºåË¶ÜËìãÁéáËàáÁØÑ‰æãÊï∏Èáè‰πãÈñìÁöÑÈóú‰øÇÈÄöÂ∏∏ÊòØÂ∞çÊï∏Á∑öÊÄßÁöÑÔºå‰∏îÂèØÁî®ÊåáÊï∏ÂÜ™ÂæãÂª∫Ê®°ÔºåÈÄôË°®Á§∫Â≠òÂú®Êé®Ë´ñÊôÇÈñìÊì¥Â±ïÂæã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁôºÁèæÂæûË®±Â§ö‰∏ñ‰ª£‰∏≠ÊâæÂá∫Ê≠£Á¢∫ÁØÑ‰æã‰ªçÁÑ∂ÊòØÊ≤íÊúâËá™ÂãïÈ©óË≠âÂô®ÁöÑÈ†òÂüü‰∏≠Êú™‰æÜÁ†îÁ©∂ÁöÑÈáçË¶ÅÊñπÂêë„ÄÇÂú®Ëß£Ê±∫ GSM8K Âíå MATH ÁöÑÊï∏Â≠∏ÊñáÂ≠óÈ°åÊôÇÔºå‰ΩøÁî® Llama-3 Ê®°ÂûãÁöÑË¶ÜËìãÁéáÊúÉÂú® 10,000 ÂÄãÁØÑ‰æã‰∏≠ÊàêÈï∑Âà∞Ë∂ÖÈÅé 95%„ÄÇÁÑ∂ËÄåÔºåÂæûÁØÑ‰æãÈõÜÂêà‰∏≠ÊåëÈÅ∏Ê≠£Á¢∫Ëß£Á≠îÁöÑÂ∏∏Ë¶ãÊñπÊ≥ïÔºà‰æãÂ¶ÇÂ§öÊï∏Ê±∫ÊàñÁçéÂãµÊ®°ÂûãÔºâÊúÉÂú®Êï∏ÁôæÂÄãÁØÑ‰æãÂæåÈÅîÂà∞Âπ≥Á©©ÊúüÔºå‰∏îÁÑ°Ê≥ïÂÆåÂÖ®Èö®ËëóÁØÑ‰æãÈ†êÁÆóËÄåÊì¥Â±ï„ÄÇ</paragraph>

##### **The Llama 3 Herd of Models**
2407.21783v1 by Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm√°n, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao

Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.

ÊëòË¶ÅÔºöÁèæ‰ª£‰∫∫Â∑•Êô∫ÊÖß (AI) Á≥ªÁµ±Áî±Âü∫Á§éÊ®°ÂûãÊèê‰æõÂãïÂäõ„ÄÇ
Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁµÑÊñ∞ÁöÑÂü∫Á§éÊ®°ÂûãÔºåÁ®±ÁÇ∫ Llama 3„ÄÇÂÆÉÊòØ‰∏ÄÁæ§Ë™ûË®ÄÊ®°ÂûãÔºåÊú¨Ê©üÊîØÊè¥Â§öË™ûË®Ä„ÄÅÁ∑®Á¢º„ÄÅÊé®ÁêÜÂíåÂ∑•ÂÖ∑‰ΩøÁî®„ÄÇÊàëÂÄëÊúÄÂ§ßÁöÑÊ®°ÂûãÊòØ‰∏ÄÂÄãÂÖ∑Êúâ 405B ÂèÉÊï∏ÂíåÊúÄÂ§ö 128K ‰ª§ÁâåÁöÑ‰∏ä‰∏ãÊñáË¶ñÁ™óÁöÑÂØÜÈõÜTransformer„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÂ∞ç Llama 3 ÁöÑÂª£Ê≥õÂØ¶Ë≠âË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæ Llama 3 Âú®Â§ßÈáè‰ªªÂãô‰∏äÊèê‰æõ‰∫ÜËàá GPT-4 Á≠âÈ†òÂÖàË™ûË®ÄÊ®°ÂûãÁõ∏Áï∂ÁöÑÂìÅË≥™„ÄÇÊàëÂÄëÂÖ¨ÈñãÁôºÂ∏É Llama 3ÔºåÂåÖÊã¨ 405B ÂèÉÊï∏Ë™ûË®ÄÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥ÂíåÂæåË®ìÁ∑¥ÁâàÊú¨Ôºå‰ª•ÂèäÊàëÂÄëÁöÑ Llama Guard 3 Ê®°ÂûãÔºåÁî®ÊñºËº∏ÂÖ•ÂíåËº∏Âá∫ÂÆâÂÖ®ÊÄß„ÄÇÊú¨ÊñáÈÇÑÊèê‰æõ‰∫ÜÂ∞áÂΩ±ÂÉè„ÄÅÂΩ±ÁâáÂíåË™ûÈü≥ÂäüËÉΩÊï¥ÂêàÂà∞ Llama 3 ‰∏≠ÁöÑÂØ¶È©óÁµêÊûúÔºåÊñπÊ≥ïÊòØÊé°Áî®ÁµÑÂêàÂºèÊñπÊ≥ï„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÈÄôÁ®ÆÊñπÊ≥ïÂú®ÂΩ±ÂÉè„ÄÅÂΩ±ÁâáÂíåË™ûÈü≥Ëæ®Ë≠ò‰ªªÂãô‰∏äË°®ÁèæÂá∫ËàáÊúÄÂÖàÈÄ≤ÊäÄË°ìÁõ∏Áï∂ÁöÑÁ´∂Áà≠Âäõ„ÄÇÁî±ÊñºÈÄô‰∫õÊ®°Âûã‰ªçËôïÊñºÈñãÁôºÈöéÊÆµÔºåÂõ†Ê≠§Â∞öÊú™Âª£Ê≥õÁôºÂ∏É„ÄÇ

##### **Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**
2407.21778v1 by Felix Ocker, Daniel Tanneberg, Julian Eggert, Michael Gienger

We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫Ü Tulip ‰ª£ÁêÜÔºå‰∏ÄÁ®ÆÂü∫ÊñºËá™‰∏ª LLM ÁöÑ‰ª£ÁêÜÊû∂ÊßãÔºåÂèØ‰ª•Â∞çÂåÖÂê´Â§ßÈáèÂ∑•ÂÖ∑ÁöÑÂ∑•ÂÖ∑Â∫´ÈÄ≤Ë°åÂª∫Á´ã„ÄÅËÆÄÂèñ„ÄÅÊõ¥Êñ∞ÂíåÂà™Èô§Â≠òÂèñ„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÂØ¶‰ΩúÁõ∏ÂèçÔºåTulip ‰ª£ÁêÜ‰∏çÊúÉÂ∞áÊâÄÊúâÂèØÁî®Â∑•ÂÖ∑ÁöÑÊèèËø∞Á∑®Á¢ºÂú®Á≥ªÁµ±ÊèêÁ§∫‰∏≠ÔºåÈÄôÊúÉË®àÂÖ•Ê®°ÂûãÁöÑÂÖßÂÆπË¶ñÁ™óÔºåÊàñÂµåÂÖ•Êï¥ÂÄãÊèêÁ§∫‰ª•Êì∑ÂèñÂêàÈÅ©ÁöÑÂ∑•ÂÖ∑„ÄÇÁõ∏ÂèçÂú∞ÔºåTulip ‰ª£ÁêÜÂèØ‰ª•Âú®ÂÖ∂ÂèØÂª∂‰º∏Â∑•ÂÖ∑Â∫´‰∏≠ÈÅûËø¥ÊêúÂ∞ãÂêàÈÅ©ÁöÑÂ∑•ÂÖ∑ÔºåÁØÑ‰æãÂØ¶‰ΩúÁÇ∫ÂêëÈáèÂÑ≤Â≠ò„ÄÇTulip ‰ª£ÁêÜÊû∂ÊßãÂ§ßÂπÖÈôç‰Ωé‰∫ÜÊé®Ë´ñÊàêÊú¨ÔºåÂÖÅË®±‰ΩøÁî®ÁîöËá≥Â§ßÂûãÂ∑•ÂÖ∑Â∫´Ôºå‰∏¶ËÆì‰ª£ÁêÜË™øÊï¥ÂíåÂª∂‰º∏ÂÖ∂Â∑•ÂÖ∑ÁµÑ„ÄÇÊàëÂÄëÂú®Êï∏Â≠∏ËÉåÊôØ‰∏≠‰ΩøÁî®Â§öÈ†ÖÊ∂àËûçÁ†îÁ©∂Ë©ï‰º∞Êû∂ÊßãÔºå‰∏¶ÈÄèÈÅéÊ©üÂô®‰∫∫ÊáâÁî®Á®ãÂºèÂ±ïÁ§∫ÂÖ∂Ê¶ÇÊã¨ÊÄß„ÄÇÂèØ‰ª•Âú® github.com/HRI-EU/tulip_agent ÂèñÂæóÂèÉËÄÉÂØ¶‰ΩúÂíåÂü∫Ê∫ñ„ÄÇ

##### **ShieldGemma: Generative AI Content Moderation Based on Gemma**
2407.21772v1 by Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez

We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫ ShieldGemmaÔºåÈÄôÊòØ‰∏ÄÂ•óÂª∫ÊßãÊñº Gemma2 ÁöÑÂÖ®Èù¢ LLM ÂÆâÂÖ®ÂÖßÂÆπÂØ©Ê†∏Ê®°ÂûãÂ•ó‰ª∂„ÄÇÈÄô‰∫õÊ®°ÂûãÊèê‰æõÂÅ•ÂÖ®„ÄÅÊúÄÂÖàÈÄ≤ÁöÑÂÆâÂÖ®ÊÄßÈ¢®Èö™È†êÊ∏¨ÔºåÊ∂µËìã‰ΩøÁî®ËÄÖËº∏ÂÖ•Âíå LLM Áî¢ÁîüÁöÑËº∏Âá∫‰∏≠ÁöÑ‰∏ªË¶ÅÂç±ÂÆ≥È°ûÂûãÔºàÈú≤È™®ÊÄßÂÖßÂÆπ„ÄÅÂç±Èö™ÂÖßÂÆπ„ÄÅÈ®∑Êìæ„ÄÅ‰ªáÊÅ®Ë®ÄË´ñÔºâ„ÄÇÈÄèÈÅéÂú®ÂÖ¨ÈñãÂíåÂÖßÈÉ®Âü∫Ê∫ñ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåÊàëÂÄëÂ±ïÁ§∫Âá∫ÂÑ™ÊñºÁèæÊúâÊ®°ÂûãÁöÑÂçìË∂äÊïàËÉΩÔºå‰æãÂ¶Ç Llama GuardÔºàÂú®ÂÖ¨ÈñãÂü∫Ê∫ñ‰∏ä +10.8% AU-PRCÔºâÂíå WildCardÔºà+4.3%Ôºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑ LLM Ë≥áÊñôÁ≠ñÁÆ°ÊµÅÁ®ãÔºåÈÅ©Áî®ÊñºÂêÑÁ®ÆÂÆâÂÖ®Áõ∏Èóú‰ªªÂãôÂèäÂÖ∂‰ªñ‰ªªÂãô„ÄÇÊàëÂÄëÂ∑≤Â±ïÁ§∫Âá∫Â∞çÊñº‰∏ªË¶ÅË®ìÁ∑¥Âú®ÂêàÊàêË≥áÊñô‰∏äÁöÑÊ®°ÂûãÁöÑÂº∑Â§ßÊ≥õÂåñÊïàËÉΩ„ÄÇÈÄèÈÅéÈáãÂá∫ ShieldGemmaÔºåÊàëÂÄëÁÇ∫Á†îÁ©∂Á§æÁæ§Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË≥áÊ∫êÔºåÊé®ÈÄ≤ LLM ÂÆâÂÖ®ÊÄß‰∏¶ÂçîÂä©ÈñãÁôº‰∫∫Âì°Âª∫Á´ãÊõ¥ÊúâÊïàÁöÑÂÖßÂÆπÂØ©Ê†∏Ëß£Ê±∫ÊñπÊ°à„ÄÇ

##### **MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**
2407.21770v1 by Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan

We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)
architecture designed for pre-training mixed-modal, early-fusion language
models. MoMa processes images and text in arbitrary sequences by dividing
expert modules into modality-specific groups. These groups exclusively process
designated tokens while employing learned routing within each group to maintain
semantically informed adaptivity. Our empirical results reveal substantial
pre-training efficiency gains through this modality-specific parameter
allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,
featuring 4 text experts and 4 image experts, achieves impressive FLOPs
savings: 3.7x overall, with 2.6x for text and 5.2x for image processing
compared to a compute-equivalent dense baseline, measured by pre-training loss.
This outperforms the standard expert-choice MoE with 8 mixed-modal experts,
which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).
Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs
savings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination
hurts performance in causal inference due to increased sensitivity to router
accuracy. These results demonstrate MoMa's potential to significantly advance
the efficiency of mixed-modal, early-fusion language model pre-training, paving
the way for more resource-efficient and capable multimodal AI systems.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄë‰ªãÁ¥π MoMaÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®°ÊÖãÊÑüÁü•Ê∑∑ÂêàÂ∞àÂÆ∂ (MoE) Êû∂ÊßãÔºåÂ∞àÁÇ∫Ê∑∑ÂêàÊ®°ÊÖã„ÄÅÊó©ÊúüËûçÂêàË™ûË®ÄÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥ËÄåË®≠Ë®à„ÄÇMoMa ÈÄèÈÅéÂ∞áÂ∞àÂÆ∂Ê®°ÁµÑÂàÜÁÇ∫Ê®°ÊÖãÁâπÂÆöÁæ§ÁµÑÔºå‰ª•‰ªªÊÑèÈ†ÜÂ∫èËôïÁêÜÂΩ±ÂÉèÂíåÊñáÂ≠ó„ÄÇÈÄô‰∫õÁæ§ÁµÑÊúÉÁç®Ëá™ËôïÁêÜÊåáÂÆöÁöÑ‰ª£Á¢ºÔºåÂêåÊôÇÂú®ÊØèÂÄãÁæ§ÁµÑÂÖß‰ΩøÁî®Â∑≤Â≠∏ÁøíÁöÑË∑ØÁî±Ôºå‰ª•Á∂≠ÊåÅË™ûÁæ©ÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúÈ°ØÁ§∫ÔºåÈÄèÈÅéÈÄôÁ®ÆÊ®°ÊÖãÁâπÂÆöÂèÉÊï∏ÈÖçÁΩÆÔºåÂèØÂ§ßÂπÖÊèêÂçáÈ†êË®ìÁ∑¥ÊïàÁéá„ÄÇÂú® 1 ÂÖÜÂÄã‰ª£Á¢ºÁöÑË®ìÁ∑¥È†êÁÆó‰∏ãÔºåMoMa 1.4B Ê®°ÂûãÈÖçÂÇô 4 ÂÄãÊñáÂ≠óÂ∞àÂÆ∂Âíå 4 ÂÄãÂΩ±ÂÉèÂ∞àÂÆ∂ÔºåÂèØÁØÄÁúÅ‰ª§‰∫∫È©öË±îÁöÑ FLOPÔºöÊï¥È´îËÄåË®ÄÁÇ∫ 3.7 ÂÄçÔºåÊñáÂ≠óËôïÁêÜÁÇ∫ 2.6 ÂÄçÔºåÂΩ±ÂÉèËôïÁêÜÁÇ∫ 5.2 ÂÄçÔºåÈÄôÊòØ‰ª•È†êË®ìÁ∑¥ÊêçÂ§±Ê∏¨ÈáèÔºå‰∏¶ËàáÈÅãÁÆóÁ≠âÊïàÁöÑÂØÜÈõÜÂü∫Ê∫ñÁ∑öÁõ∏ÊØîËºÉ„ÄÇÈÄôÂÑ™ÊñºÊ®ôÊ∫ñÁöÑÂ∞àÂÆ∂ÈÅ∏Êìá MoEÔºåÂæåËÄÖÈÖçÂÇô 8 ÂÄãÊ∑∑ÂêàÊ®°ÊÖãÂ∞àÂÆ∂ÔºåÂèØÁØÄÁúÅÊï¥È´î FLOP 3 ÂÄçÔºàÊñáÂ≠óÁÇ∫ 3 ÂÄçÔºåÂΩ±ÂÉèÁÇ∫ 2.8 ÂÄçÔºâ„ÄÇÂ∞á MoMa ËàáÊ∑∑ÂêàÊ∑±Â∫¶ (MoD) ÁµêÂêàÔºåÂèØÈÄ≤‰∏ÄÊ≠•Â∞áÈ†êË®ìÁ∑¥ FLOP ÁØÄÁúÅÊèêÂçáËá≥Êï¥È´î 4.2 ÂÄçÔºàÊñáÂ≠óÔºö3.4 ÂÄçÔºåÂΩ±ÂÉèÔºö5.3 ÂÄçÔºâÔºåÂÑòÁÆ°ÈÄôÁ®ÆÁµÑÂêàÊúÉÂõ†Ë∑ØÁî±Âô®Á≤æÁ¢∫Â∫¶ÊïèÊÑüÂ∫¶Â¢ûÂä†ËÄåÊêçÂÆ≥Âõ†ÊûúÊé®ÁêÜÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫Ü MoMa Âú®ÊèêÂçáÊ∑∑ÂêàÊ®°ÊÖã„ÄÅÊó©ÊúüËûçÂêàË™ûË®ÄÊ®°ÂûãÈ†êË®ìÁ∑¥ÊïàÁéáÊñπÈù¢ÁöÑÊΩõÂäõÔºåÁÇ∫Êõ¥ÂÖ∑Ë≥áÊ∫êÊïàÁéá‰∏îÂäüËÉΩÂº∑Â§ßÁöÑÂ§öÊ®°ÊÖã AI Á≥ªÁµ±Èã™Ë∑Ø„ÄÇ</paragraph>

##### **HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**
2407.21742v1 by Junwei He, Qianqian Xu, Yangbangyan Jiang, Zitai Wang, Yuchen Sun, Qingming Huang

With the progressive advancements in deep graph learning, out-of-distribution
(OOD) detection for graph data has emerged as a critical challenge. While the
efficacy of auxiliary datasets in enhancing OOD detection has been extensively
studied for image and text data, such approaches have not yet been explored for
graph data. Unlike Euclidean data, graph data exhibits greater diversity but
lower robustness to perturbations, complicating the integration of outliers. To
tackle these challenges, we propose the introduction of \textbf{H}ybrid
External and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE)
to improve graph OOD detection performance. Our framework involves using
realistic external graph data from various domains and synthesizing internal
outliers within ID subgroups to address the poor robustness and presence of OOD
samples within the ID class. Furthermore, we develop a boundary-aware OE loss
that adaptively assigns weights to outliers, maximizing the use of high-quality
OOD samples while minimizing the impact of low-quality ones. Our proposed HGOE
framework is model-agnostic and designed to enhance the effectiveness of
existing graph OOD detection models. Experimental results demonstrate that our
HGOE framework can significantly improve the performance of existing OOD
detection models across all 8 real datasets.

ÊëòË¶ÅÔºöÈö®ËëóÊ∑±Â∫¶ÂúñÂΩ¢Â≠∏ÁøíÁöÑÈÄ≤Ê≠•ÔºåÂúñÂΩ¢Ë≥áÊñôÁöÑÂàÜÂ∏ÉÂ§ñ (OOD) ÂÅµÊ∏¨Â∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞„ÄÇÈõñÁÑ∂ËºîÂä©Ë≥áÊñôÈõÜÂú®Â¢ûÂº∑ÂΩ±ÂÉèÂíåÊñáÂ≠óË≥áÊñôÁöÑ OOD ÂÅµÊ∏¨ÊñπÈù¢Â∑≤Áç≤ÂæóÂª£Ê≥õÁ†îÁ©∂Ôºå‰ΩÜÊ≠§È°ûÊñπÊ≥ïÂ∞öÊú™Êé¢Á¥¢ÂúñÂΩ¢Ë≥áÊñô„ÄÇËàáÊ≠êÂπæÈáåÂæóË≥áÊñô‰∏çÂêåÔºåÂúñÂΩ¢Ë≥áÊñôÂ±ïÁèæÂá∫Êõ¥Â§ßÁöÑÂ§öÊ®£ÊÄßÔºå‰ΩÜÂ∞çÊìæÂãïÁöÑÈ≠ØÊ£íÊÄßËºÉ‰ΩéÔºå‰ΩøÈõ¢Áæ§ÂÄºÁöÑÊï¥ÂêàË§áÈõúÂåñ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ÂºïÂÖ•\textbf{H}ybrid\textbf{E}xternal and \textbf{I}nternal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE) ‰ª•ÊèêÂçáÂúñÂΩ¢ OOD ÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊ∂âÂèä‰ΩøÁî®‰æÜËá™‰∏çÂêåÈ†òÂüüÁöÑÁúüÂØ¶Â§ñÈÉ®ÂúñÂΩ¢Ë≥áÊñôÔºå‰∏¶Âú® ID Â≠êÁæ§‰∏≠ÂêàÊàêÂÖßÈÉ®Èõ¢Áæ§ÂÄºÔºå‰ª•Ëß£Ê±∫ ID È°ûÂà•‰∏≠ OOD Ê®£Êú¨ÁöÑÈ≠ØÊ£íÊÄßÂ∑ÆÂíåÂ≠òÂú®ÊÄßÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÈÇäÁïåÊÑüÁü• OE ÊêçÂ§±ÔºåÂèØÈÅ©ÊáâÊÄßÂú∞Â∞áÊ¨äÈáçÂàÜÈÖçÁµ¶Èõ¢Áæ§ÂÄºÔºåÊúÄÂ§ßÂåñÈ´òÂìÅË≥™ OOD Ê®£Êú¨ÁöÑ‰ΩøÁî®ÔºåÂêåÊôÇÊúÄÂ∞èÂåñ‰ΩéÂìÅË≥™Ê®£Êú¨ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑ HGOE Êû∂ÊßãËàáÊ®°ÂûãÁÑ°ÈóúÔºåÊó®Âú®ÊèêÂçáÁèæÊúâÂúñÂΩ¢ OOD ÂÅµÊ∏¨Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑ HGOE Êû∂ÊßãÂèØ‰ª•È°ØËëóÊèêÂçáÁèæÊúâ OOD ÂÅµÊ∏¨Ê®°ÂûãÂú®ÊâÄÊúâ 8 ÂÄãÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏≠ÁöÑÊïàËÉΩ„ÄÇ

##### **Contrastive Factor Analysis**
2407.21740v2 by Zhibin Duan, Tiansheng Wen, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou

Factor analysis, often regarded as a Bayesian variant of matrix
factorization, offers superior capabilities in capturing uncertainty, modeling
complex dependencies, and ensuring robustness. As the deep learning era
arrives, factor analysis is receiving less and less attention due to their
limited expressive ability. On the contrary, contrastive learning has emerged
as a potent technique with demonstrated efficacy in unsupervised
representational learning. While the two methods are different paradigms,
recent theoretical analysis has revealed the mathematical equivalence between
contrastive learning and matrix factorization, providing a potential
possibility for factor analysis combined with contrastive learning. Motivated
by the interconnectedness of contrastive learning, matrix factorization, and
factor analysis, this paper introduces a novel Contrastive Factor Analysis
framework, aiming to leverage factor analysis's advantageous properties within
the realm of contrastive learning. To further leverage the interpretability
properties of non-negative factor analysis, which can learn disentangled
representations, contrastive factor analysis is extended to a non-negative
version. Finally, extensive experimental validation showcases the efficacy of
the proposed contrastive (non-negative) factor analysis methodology across
multiple key properties, including expressiveness, robustness,
interpretability, and accurate uncertainty estimation.

ÊëòË¶ÅÔºöÂõ†Â≠êÂàÜÊûêÈÄöÂ∏∏Ë¢´ËßÜ‰∏∫Áü©ÈòµÂàÜËß£ÁöÑË¥ùÂè∂ÊñØÂèò‰ΩìÔºåÂú®ÊçïËé∑‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂª∫Ê®°Â§çÊùÇ‰æùËµñÂÖ≥Á≥ªÂíåÁ°Æ‰øùÁ®≥ÂÅ•ÊÄßÊñπÈù¢ÂÖ∑ÊúâÂçìË∂äÁöÑËÉΩÂäõ„ÄÇÈöèÁùÄÊ∑±Â∫¶Â≠¶‰π†Êó∂‰ª£ÁöÑÂà∞Êù•ÔºåÂõ†Â≠êÂàÜÊûêÁî±‰∫éÂÖ∂Ë°®ËææËÉΩÂäõÊúâÈôêËÄåÂèóÂà∞Ë∂äÊù•Ë∂äÂ∞ëÁöÑÂÖ≥Ê≥®„ÄÇÁõ∏ÂèçÔºåÂØπÊØîÂ≠¶‰π†Â∑≤ÁªèÊàê‰∏∫‰∏ÄÁßçÊúâÊïàÁöÑÊäÄÊúØÔºåÂú®Êó†ÁõëÁù£Ë°®ÂæÅÂ≠¶‰π†‰∏≠ÊòæÁ§∫Âá∫ÂäüÊïà„ÄÇËôΩÁÑ∂Ëøô‰∏§ÁßçÊñπÊ≥ïÊòØ‰∏çÂêåÁöÑËåÉÂºèÔºå‰ΩÜÊúÄËøëÁöÑÁêÜËÆ∫ÂàÜÊûêÊè≠Á§∫‰∫ÜÂØπÊØîÂ≠¶‰π†ÂíåÁü©ÈòµÂàÜËß£‰πãÈó¥ÁöÑÊï∞Â≠¶Á≠â‰ª∑ÊÄßÔºå‰∏∫Âõ†Â≠êÂàÜÊûêÁªìÂêàÂØπÊØîÂ≠¶‰π†Êèê‰æõ‰∫ÜÊΩúÂú®ÁöÑÂèØËÉΩÊÄß„ÄÇÂèóÂØπÊØîÂ≠¶‰π†„ÄÅÁü©ÈòµÂàÜËß£ÂíåÂõ†Â≠êÂàÜÊûêÁöÑÁõ∏‰∫íËÅîÁ≥ªÁöÑÂêØÂèëÔºåÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂØπÊØîÂõ†Â≠êÂàÜÊûêÊ°ÜÊû∂ÔºåÊó®Âú®Âà©Áî®Âõ†Â≠êÂàÜÊûêÂú®ÂØπÊØîÂ≠¶‰π†È¢ÜÂüü‰∏≠ÁöÑ‰ºòÂäøÁâπÊÄß„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Âà©Áî®ÈùûË¥üÂõ†Â≠êÂàÜÊûêÁöÑÂèØËß£ÈáäÊÄßÔºåÂÆÉÂèØ‰ª•Â≠¶‰π†Ëß£ËÄ¶Ë°®ÂæÅÔºåÂ∞ÜÂØπÊØîÂõ†Â≠êÂàÜÊûêÊâ©Â±ïÂà∞ÈùûË¥üÁâàÊú¨„ÄÇÊúÄÂêéÔºåÂπøÊ≥õÁöÑÂÆûÈ™åÈ™åËØÅÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÂØπÊØîÔºàÈùûË¥üÔºâÂõ†Â≠êÂàÜÊûêÊñπÊ≥ïÂú®Â§ö‰∏™ÂÖ≥ÈîÆÂ±ûÊÄßÔºàÂåÖÊã¨Ë°®ËææÊÄß„ÄÅÈ≤ÅÊ£íÊÄß„ÄÅÂèØËß£ÈáäÊÄßÂíåÂáÜÁ°ÆÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°Ôºâ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

##### **A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**
2407.21739v1 by Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar

Adapting foundation models for medical image analysis requires finetuning
them on a considerable amount of data because of extreme distribution shifts
between natural (source) data used for pretraining and medical (target) data.
However, collecting task-specific medical data for such finetuning at a central
location raises many privacy concerns. Although Federated learning (FL)
provides an effective means for training on private decentralized data,
communication costs in federating large foundation models can quickly become a
significant bottleneck, impacting the solution's scalability. In this work, we
address this problem of efficient communication while ensuring effective
learning in FL by combining the strengths of Parameter-Efficient Fine-tuning
(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)
in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical
image segmentation. Unlike prior works that utilize LoRA and finetune the
entire decoder, we critically analyze the contribution of each granular
component of SAM on finetuning performance. Thus, we identify specific layers
to be federated that are very efficient in terms of communication cost while
producing on-par accuracy. Our experiments show that retaining the parameters
of the SAM model (including most of the decoder) in their original state during
adaptation is beneficial because fine-tuning on small datasets tends to distort
the inherent capabilities of the underlying foundation model. On Fed-KiTS, our
approach decreases communication cost (~48x) compared to full fine-tuning while
increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach
performs similar to SAMed while achieving ~2.8x reduction in communication and
parameters to be finetuned. We further validate our approach with experiments
on Fed-IXI and Prostate MRI datasets.

ÊëòË¶ÅÔºö<paragraph>Áî±ÊñºÈ†êË®ìÁ∑¥ÊâÄÁî®ÁöÑËá™ÁÑ∂Ôºà‰æÜÊ∫êÔºâË≥áÊñôÂíåÈÜ´ÁôÇÔºàÁõÆÊ®ôÔºâË≥áÊñô‰πãÈñìÁöÑÊ•µÁ´ØÂàÜ‰ΩàËΩâÁßªÔºåÂõ†Ê≠§Â∞áÂü∫Á§éÊ®°ÂûãË™øÊï¥Áî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈúÄË¶ÅÂú®Â§ßÈáèË≥áÊñô‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÂæÆË™ø„ÄÇ
ÁÑ∂ËÄåÔºåÂú®‰∏≠ÂøÉ‰ΩçÁΩÆÊî∂ÈõÜÊ≠§È°ûÂæÆË™øÁöÑÁâπÂÆö‰ªªÂãôÈÜ´ÁôÇË≥áÊñôÊúÉÂºïÁôºË®±Â§öÈö±ÁßÅÂïèÈ°å„ÄÇÂÑòÁÆ°ËÅØÂêàÂ≠∏Áøí (FL) Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂú®ÁßÅÊúâÂàÜÊï£ÂºèË≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÂú®ËÅØÂêàÂ§ßÂûãÂü∫Á§éÊ®°ÂûãÊôÇÔºåÈÄöË®äÊàêÊú¨ÂèØËÉΩÊúÉËøÖÈÄüÊàêÁÇ∫‰∏ÄÂÄãÈáçÂ§ßÁì∂È†∏ÔºåÂΩ±ÈüøËß£Ê±∫ÊñπÊ°àÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÁµêÂêàÂèÉÊï∏È´òÊïàÂæÆË™ø (PEFT) Âíå FL ÁöÑÂÑ™Âã¢ÔºåËß£Ê±∫‰∫ÜÂú®Á¢∫‰øù FL ‰∏≠ÊúâÊïàÂ≠∏ÁøíÁöÑÂêåÊôÇÈÄ≤Ë°åÈ´òÊïàÈÄöË®äÁöÑÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ª•ËÅØÂêàÁöÑÊñπÂºèÁ†îÁ©∂Âç≥ÊèíÂç≥Áî®‰ΩéÁß©ÈÅ©ÈÖçÂô® (LoRA)Ôºå‰ª•Ë™øÊï¥ÂçÄÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ‰ª•ÈÄ≤Ë°å 3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇËàáÂà©Áî® LoRA ÂíåÂæÆË™øÊï¥ÂÄãËß£Á¢ºÂô®ÁöÑÂÖàÂâçÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞ÂàÜÊûê‰∫Ü SAM ÁöÑÊØèÂÄãÁ≤íÁãÄÁµÑÊàêÈÉ®ÂàÜÂ∞çÂæÆË™øÊïàËÉΩÁöÑË≤¢Áçª„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁ¢∫ÂÆö‰∫ÜÂú®ÈÄöË®äÊàêÊú¨ÊñπÈù¢ÈùûÂ∏∏È´òÊïàÁöÑÁâπÂÆöÂ±§ÔºåÂêåÊôÇÁî¢Áîü‰∫ÜÂêåÁ≠âÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÂú®Ë™øÊï¥ÈÅéÁ®ã‰∏≠Â∞á SAM Ê®°ÂûãÁöÑÂèÉÊï∏ÔºàÂåÖÊã¨Â§ßÈÉ®ÂàÜËß£Á¢ºÂô®Ôºâ‰øùÁïôÂú®ÂÖ∂ÂéüÂßãÁãÄÊÖãÊòØÊúâÁõäÁöÑÔºåÂõ†ÁÇ∫Âú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂæÆË™øÂæÄÂæÄÊúÉÊâ≠Êõ≤Âü∫Á§éÊ®°ÂûãÁöÑÂÖßÂú®ËÉΩÂäõ„ÄÇÂú® Fed-KiTS ‰∏äÔºåËàáÂÆåÂÖ®ÂæÆË™øÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈôç‰Ωé‰∫ÜÈÄöË®äÊàêÊú¨ÔºàÁ¥Ñ 48 ÂÄçÔºâÔºåÂêåÊôÇÊèêÈ´ò‰∫Ü 3D ÂàÜÂâ≤‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºàÁ¥Ñ 6% ÁöÑÈ™∞Â≠êÂàÜÊï∏Ôºâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËàá SAMed È°û‰ººÔºåÂêåÊôÇÂ∞áÈÄöË®äÂíåÂæÖÂæÆË™øÂèÉÊï∏Ê∏õÂ∞ë‰∫ÜÁ¥Ñ 2.8 ÂÄç„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄöÈÅéÂú® Fed-IXI Âíå Prostate MRI Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ï„ÄÇ</paragraph>

##### **Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**
2407.21738v1 by Joseph Geo Benjamin, Mothilal Asokan, Amna Alhosani, Hussain Alasmawi, Werner Gerhard Diehl, Leanne Bricker, Karthik Nandakumar, Mohammad Yaqub

Self-supervised learning (SSL) methods are popular since they can address
situations with limited annotated data by directly utilising the underlying
data distribution. However, the adoption of such methods is not explored enough
in ultrasound (US) imaging, especially for fetal assessment. We investigate the
potential of dual-encoder SSL in utilizing unlabelled US video data to improve
the performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)
classification using limited labelled 2D US images. We study 7 SSL approaches
based on reconstruction, contrastive loss, distillation, and information theory
and evaluate them extensively on a large private US dataset. Our observations
and findings are consolidated from more than 500 downstream training
experiments under different settings. Our primary observation shows that for
SSL training, the variance of the dataset is more crucial than its size because
it allows the model to learn generalisable representations, which improve the
performance of downstream tasks. Overall, the BarlowTwins method shows robust
performance, irrespective of the training settings and data variations, when
used as an initialisation for downstream tasks. Notably, full fine-tuning with
1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and
outperforms other SSL initialisations by at least 4% in F1-score, thus making
it a promising candidate for transfer learning from US video to image data.

ÊëòË¶ÅÔºöËá™ÁõëÁù£Â≠¶‰π† (SSL) ÊñπÊ≥ïÂæàÂèóÊ¨¢ËøéÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøáÁõ¥Êé•Âà©Áî®Â∫ïÂ±ÇÊï∞ÊçÆÂàÜÂ∏ÉÊù•Ëß£ÂÜ≥Â∏¶Ê≥®ÈáäÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ„ÄÇÁÑ∂ËÄåÔºåÂú®Ë∂ÖÂ£∞ (US) ÊàêÂÉè‰∏≠ÔºåÁâπÂà´ÊòØÂØπ‰∫éËÉéÂÑøËØÑ‰º∞ÔºåÂØπËøôÁßçÊñπÊ≥ïÁöÑÈááÁî®Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫ÜÂèåÁºñÁ†ÅÂô® SSL Âú®Âà©Áî®Êú™Ê†áËÆ∞ÁöÑ US ËßÜÈ¢ëÊï∞ÊçÆ‰ª•‰ΩøÁî®ÊúâÈôêÊ†áËÆ∞ÁöÑ 2D US ÂõæÂÉèÊèêÈ´òÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰∏ãÊ∏∏Ê†áÂáÜËÉéÂÑøÂøÉËÑèÂπ≥Èù¢ (SFCP) ÂàÜÁ±ªÊÄßËÉΩÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü 7 ÁßçÂü∫‰∫éÈáçÂª∫„ÄÅÂØπÊØîÊçüÂ§±„ÄÅËí∏È¶èÂíå‰ø°ÊÅØËÆ∫ÁöÑ SSL ÊñπÊ≥ïÔºåÂπ∂Âú®‰∏Ä‰∏™Â§ßÂûãÁßÅÊúâ US Êï∞ÊçÆÈõÜ‰∏äÂØπÂÆÉ‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞„ÄÇÊàë‰ª¨ÁöÑËßÇÂØüÂíåÂèëÁé∞Êù•Ëá™ 500 Â§ö‰∏™‰∏çÂêåËÆæÁΩÆ‰∏ãÁöÑ‰∏ãÊ∏∏ËÆ≠ÁªÉÂÆûÈ™å„ÄÇÊàë‰ª¨ÁöÑ‰∏ªË¶ÅËßÇÂØüÁªìÊûúË°®ÊòéÔºåÂØπ‰∫é SSL ËÆ≠ÁªÉÔºåÊï∞ÊçÆÈõÜÁöÑÊñπÂ∑ÆÊØîÂÖ∂Â§ßÂ∞èÊõ¥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉÂÖÅËÆ∏Ê®°ÂûãÂ≠¶‰π†ÂèØÊé®ÂπøÁöÑË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´ò‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåBarlowTwins ÊñπÊ≥ïË°®Áé∞Âá∫Á®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåÊó†ËÆ∫ËÆ≠ÁªÉËÆæÁΩÆÂíåÊï∞ÊçÆÂèòÂåñÂ¶Ç‰ΩïÔºåÂΩìÁî®‰Ωú‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂàùÂßãÂåñÊó∂„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºå‰ΩøÁî® 1% ÁöÑÊ†áËÆ∞Êï∞ÊçÆËøõË°åÂÆåÂÖ®ÂæÆË∞ÉÂú® F1 ÂàÜÊï∞‰∏ä‰ºò‰∫é ImageNet ÂàùÂßãÂåñ 12%ÔºåÂπ∂‰∏îÂú® F1 ÂàÜÊï∞‰∏ä‰ºò‰∫éÂÖ∂‰ªñ SSL ÂàùÂßãÂåñËá≥Â∞ë 4%ÔºåÂõ†Ê≠§‰ΩøÂÖ∂Êàê‰∏∫‰ªé US ËßÜÈ¢ëÂà∞ÂõæÂÉèÊï∞ÊçÆÁöÑËøÅÁßªÂ≠¶‰π†ÁöÑÊúâÂ∏åÊúõÁöÑÂÄôÈÄâËÄÖ„ÄÇ

##### **Open-Vocabulary Audio-Visual Semantic Segmentation**
2407.21721v1 by Ruohao Guo, Liao Qu, Dantong Niu, Yanyu Qi, Wenzhen Yue, Ji Shi, Bowei Xing, Xianghua Ying

Audio-visual semantic segmentation (AVSS) aims to segment and classify
sounding objects in videos with acoustic cues. However, most approaches operate
on the close-set assumption and only identify pre-defined categories from
training data, lacking the generalization ability to detect novel categories in
practical applications. In this paper, we introduce a new task: open-vocabulary
audio-visual semantic segmentation, extending AVSS task to open-world scenarios
beyond the annotated label space. This is a more challenging task that requires
recognizing all categories, even those that have never been seen nor heard
during training. Moreover, we propose the first open-vocabulary AVSS framework,
OV-AVSS, which mainly consists of two parts: 1) a universal sound source
localization module to perform audio-visual fusion and locate all potential
sounding objects and 2) an open-vocabulary classification module to predict
categories with the help of the prior knowledge from large-scale pre-trained
vision-language models. To properly evaluate the open-vocabulary AVSS, we split
zero-shot training and testing subsets based on the AVSBench-semantic
benchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong
segmentation and zero-shot generalization ability of our model on all
categories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base
categories and 29.14% mIoU on novel categories, exceeding the state-of-the-art
zero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.
The code is available at https://github.com/ruohaoguo/ovavss.

ÊëòË¶ÅÔºö<paragraph>Èü≥Ë¶ñË™ûÁæ©ÂàÜÂâ≤ÔºàAVSSÔºâÊó®Âú®‰ΩøÁî®ËÅΩË¶∫Á∑öÁ¥¢Â∞çÂΩ±Áâá‰∏≠ÁöÑÁôºËÅ≤Áâ©È´îÈÄ≤Ë°åÂàÜÂâ≤ÂíåÂàÜÈ°û„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩÈÅã‰ΩúÂú®Â∞ÅÈñâÈõÜÂêàÂÅáË®≠‰∏äÔºå‰∏îÂÉÖÂæûË®ìÁ∑¥Ë≥áÊñô‰∏≠Ë≠òÂà•È†êÂÖàÂÆöÁæ©ÁöÑÈ°ûÂà•ÔºåÁº∫‰πèÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂÅµÊ∏¨Êñ∞È°ûÂà•ÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊñ∞‰ªªÂãôÔºöÈñãÊîæË©ûÂΩôÈü≥Ë¶ñË™ûÁæ©ÂàÜÂâ≤ÔºåÂ∞á AVSS ‰ªªÂãôÊì¥Â±ïÂà∞Ê®ôË®ªÊ®ôÁ±§Á©∫Èñì‰πãÂ§ñÁöÑÈñãÊîæ‰∏ñÁïåÂ†¥ÊôØ„ÄÇÈÄôÊòØ‰∏ÄÈ†ÖÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÈúÄË¶ÅËæ®Ë≠òÊâÄÊúâÈ°ûÂà•ÔºåÂç≥‰ΩøÊòØÈÇ£‰∫õÂú®Ë®ìÁ∑¥ÊúüÈñìÂæûÊú™Ë¶ãÈÅéÊàñËÅΩÈÅéÁöÑÈ°ûÂà•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁ¨¨‰∏ÄÂÄãÈñãÊîæË©ûÂΩô AVSS Êû∂Êßã OV-AVSSÔºåÂÆÉ‰∏ªË¶ÅÂåÖÂê´ÂÖ©ÂÄãÈÉ®ÂàÜÔºö1) ‰∏ÄÂÄãÈÄöÁî®Èü≥Ê∫êÂÆö‰ΩçÊ®°ÁµÑÔºåÁî®ÊñºÂü∑Ë°åÈü≥Ë¶ñËûçÂêà‰∏¶ÂÆö‰ΩçÊâÄÊúâÊΩõÂú®ÁôºËÅ≤Áâ©È´îÔºå‰ª•Âèä 2) ‰∏ÄÂÄãÈñãÊîæË©ûÂΩôÂàÜÈ°ûÊ®°ÁµÑÔºåÂú®‰æÜËá™Â§ßÂûãÈ†êË®ìÁ∑¥Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂÖàÈ©óÁü•Ë≠òÁöÑÂπ´Âä©‰∏ãÈ†êÊ∏¨È°ûÂà•„ÄÇÁÇ∫‰∫ÜÈÅ©Áï∂Âú∞Ë©ï‰º∞ÈñãÊîæË©ûÂΩô AVSSÔºåÊàëÂÄëÊ†πÊìö AVSBench-semantic Âü∫Ê∫ñÔºàÂç≥ AVSBench-OVÔºâÊãÜÂàÜ‰∫ÜÈõ∂Ê¨°Â≠∏ÁøíË®ìÁ∑¥ÂíåÊ∏¨Ë©¶Â≠êÈõÜ„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊâÄÊúâÈ°ûÂà•‰∏äÂÖ∑ÊúâÂº∑Â§ßÁöÑÂàÜÂâ≤ÂíåÈõ∂Ê¨°Â≠∏ÁøíÊ¶ÇÂåñËÉΩÂäõ„ÄÇÂú® AVSBench-OV Ë≥áÊñôÈõÜ‰∏äÔºåOV-AVSS Âú®Âü∫Á§éÈ°ûÂà•‰∏äÈÅîÂà∞ 55.43% mIoUÔºåÂú®Êñ∞Á©éÈ°ûÂà•‰∏äÈÅîÂà∞ 29.14% mIoUÔºåÊØîÊúÄÂÖàÈÄ≤ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÈ´òÂá∫ 41.88%/20.61%ÔºåÊØîÈñãÊîæË©ûÂΩôÊñπÊ≥ïÈ´òÂá∫ 10.2%/11.6%„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/ruohaoguo/ovavss ÂèñÂæó„ÄÇ</paragraph>

##### **Social Learning through Interactions with Other Agents: A Survey**
2407.21713v1 by Dylan hillier, Cheston Tan, Jing Jiang

Social learning plays an important role in the development of human
intelligence. As children, we imitate our parents' speech patterns until we are
able to produce sounds; we learn from them praising us and scolding us; and as
adults, we learn by working with others. In this work, we survey the degree to
which this paradigm -- social learning -- has been mirrored in machine
learning. In particular, since learning socially requires interacting with
others, we are interested in how embodied agents can and have utilised these
techniques. This is especially in light of the degree to which recent advances
in natural language processing (NLP) enable us to perform new forms of social
learning. We look at how behavioural cloning and next-token prediction mirror
human imitation, how learning from human feedback mirrors human education, and
how we can go further to enable fully communicative agents that learn from each
other. We find that while individual social learning techniques have been used
successfully, there has been little unifying work showing how to bring them
together into socially embodied agents.

ÊëòË¶ÅÔºöÁ§æÊúÉÂ≠∏ÁøíÂú®‰∫∫È°ûÊô∫ËÉΩÁôºÂ±ï‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤„ÄÇË∫´ÁÇ∫Â≠©Á´•ÊôÇÔºåÊàëÂÄëÊ®°‰ªøÁà∂ÊØçÁöÑË™™Ë©±Ê®°ÂºèÔºåÁõ¥Âà∞ÊàëÂÄëËÉΩÂ§†ÁôºÂá∫ËÅ≤Èü≥ÔºõÊàëÂÄëÂæû‰ªñÂÄëÁöÑËÆöÁæéËàáË≤¨ÁΩµ‰∏≠Â≠∏ÁøíÔºõËÄåË∫´ÁÇ∫ÊàêÂπ¥‰∫∫ÊôÇÔºåÊàëÂÄëÂæûËàá‰ªñ‰∫∫Âêà‰Ωú‰∏≠Â≠∏Áøí„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË™øÊü•‰∫ÜÈÄôÂÄãÂÖ∏ÁØÑÔºàÁ§æÊúÉÂ≠∏ÁøíÔºâÂú®Ê©üÂô®Â≠∏Áøí‰∏≠Ë¢´ÂèçÊò†ÁöÑÁ®ãÂ∫¶„ÄÇÁâπÂà•Âú∞ÔºåÁî±ÊñºÁ§æÊúÉÂ≠∏ÁøíÈúÄË¶ÅËàá‰ªñ‰∫∫‰∫íÂãïÔºåÊàëÂÄëÊúâËààË∂£‰∫ÜËß£ÂÖ∑Ë∫´‰ª£ÁêÜÂ¶Ç‰ΩïËÉΩÂ§†‰∏¶Â∑≤Á∂ìÂà©Áî®ÈÄô‰∫õÊäÄË°ì„ÄÇÈÄôÁâπÂà•ÊòØÈëëÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰ΩøÊàëÂÄëËÉΩÂ§†Âü∑Ë°åÊñ∞ÁöÑÁ§æÊúÉÂ≠∏ÁøíÂΩ¢Âºè„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜË°åÁÇ∫Ë§áË£ΩÂíå‰∏ã‰∏ÄÂÄã‰ª£Âπ£È†êÊ∏¨Â¶Ç‰ΩïÂèçÊò†‰∫∫È°ûÊ®°‰ªø„ÄÅÂæû‰∫∫È°ûÂõûÈ•ã‰∏≠Â≠∏ÁøíÂ¶Ç‰ΩïÂèçÊò†‰∫∫È°ûÊïôËÇ≤Ôºå‰ª•ÂèäÊàëÂÄëÂ¶Ç‰ΩïÈÄ≤‰∏ÄÊ≠•ÂØ¶ÁèæËÉΩÂ§†ÂΩºÊ≠§Â≠∏ÁøíÁöÑÂÆåÂÖ®Ê∫ùÈÄö‰ª£ÁêÜ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂÑòÁÆ°ÂÄãÂà•ÁöÑÁ§æÊúÉÂ≠∏ÁøíÊäÄË°ìÂ∑≤Ë¢´ÊàêÂäü‰ΩøÁî®Ôºå‰ΩÜÈÆÆÂ∞ëÊúâÁµ±‰∏ÄÁöÑÂ∑•‰ΩúÂ±ïÁ§∫Â¶Ç‰ΩïÂ∞áÂÆÉÂÄëÂ∏∂ÂÖ•Á§æÊúÉÂÖ∑Ë∫´‰ª£ÁêÜ‰∏≠„ÄÇ

##### **Adaptive Retrieval-Augmented Generation for Conversational Systems**
2407.21712v1 by Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz

Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊï¥ÂêàÂà∞Â∞çË©±Á≥ªÁµ±ÈñãÁôº‰∏≠Áç≤ÂæóÊàêÂäüÔºåË®±Â§öÁ†îÁ©∂Â∑≤È°ØÁ§∫Êì∑ÂèñÂíåÊì¥ÂÖÖÂ§ñÈÉ®Áü•Ë≠òÂ∞çÊñºÊèê‰æõË≥áË®äÊÄßÂõûÊáâÁöÑÊúâÊïàÊÄß„ÄÇÂõ†Ê≠§ÔºåË®±Â§öÁèæÊúâÁ†îÁ©∂ÈÄöÂ∏∏ÂÅáË®≠Â∞çË©±Á≥ªÁµ±‰∏≠Á∏ΩÊòØÈúÄË¶ÅÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG)ÔºåËÄåÊ≤íÊúâÊòéÁ¢∫ÁöÑÊéßÂà∂„ÄÇÈÄôÂºïÁôº‰∫ÜÈóúÊñºÈÄôÁ®ÆÂøÖË¶ÅÊÄßÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞Êé¢Ë®éÁ≥ªÁµ±ÂõûÊáâÁöÑÊØèÂÄãËΩâÊäòÊòØÂê¶ÈÉΩÈúÄË¶ÅÊì¥ÂÖÖÂ§ñÈÉ®Áü•Ë≠ò„ÄÇÁâπÂà•ÊòØÔºåÈÄèÈÅéÂà©Áî®‰∫∫È°ûÂ∞çÈÅ©ÊáâÊÄßÊì¥ÂÖÖÁöÑ‰∫åÂÖÉÈÅ∏ÊìáÁöÑÂà§Êñ∑ÔºåÊàëÂÄëÈñãÁôº‰∫Ü RAGateÔºå‰∏ÄÁ®ÆÈñòÊéßÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂ∞çË©±Ë™ûÂ¢ÉÂíåÁõ∏ÈóúËº∏ÂÖ•ÈÄ≤Ë°åÂª∫Ê®°Ôºå‰ª•È†êÊ∏¨Â∞çË©±Á≥ªÁµ±ÊòØÂê¶ÈúÄË¶Å RAG ‰æÜÊîπÂñÑÂõûÊáâ„ÄÇÊàëÂÄëÂ∞çË®≠Ë®àÂíåÂ∞á RAGate ÊáâÁî®ÊñºÂ∞çË©±Ê®°ÂûãÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶Â∞ç‰∏çÂêåÁöÑÂ∞çË©±Â†¥ÊôØÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÂíåÂàÜÊûêË°®Êòé RAGate Âú®Âü∫Êñº RAG ÁöÑÂ∞çË©±Á≥ªÁµ±‰∏≠ÊúâÊïàÊáâÁî®ÊñºË≠òÂà•Á≥ªÁµ±ÂõûÊáâÔºå‰ª•ÈÅ©Áï∂ÁöÑ RAG Áç≤ÂæóÈ´òÂìÅË≥™ÁöÑÂõûÊáâÂíåÈ´òÁîüÊàê‰ø°ÂøÉ„ÄÇÊú¨Á†îÁ©∂ÈÇÑÁ¢∫ÂÆö‰∫ÜÁîüÊàê‰ø°ÂøÉÊ∞¥Ê∫ñËàáÊì¥ÂÖÖÁü•Ë≠òÁõ∏ÈóúÊÄß‰πãÈñìÁöÑÈóúËÅØÊÄß„ÄÇ

##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas N√ºrnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

ÊëòË¶ÅÔºöÊú¨‰ΩìÊòØÁâπÂÆöÈ†òÂüü‰∏≠Áü•Ë≠òÁöÑÂΩ¢ÂºèÂåñË°®Á§∫ÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµÑÁπîÂíåÁêÜËß£Ë§áÈõúÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊú¨‰ΩìÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îËÄóÊôÇÁöÑÂä™Âäõ„ÄÇChEBI ÊòØÂåñÂ≠∏È†òÂüü‰∏≠‰∏ÄÂÄãËëóÂêçÁöÑÊú¨‰ΩìÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË≥áÊ∫êÔºåÁî®ÊñºÂÆöÁæ©ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Â±¨ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÉÖÊ∂µËìã‰∫ÜÂåñÂ≠∏È†òÂüüÂø´ÈÄüÂ¢ûÈï∑ÁöÑÁü•Ë≠ò‰∏≠ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰∏¶‰∏îÊ≤íÊúâÊèê‰æõÁßëÂ≠∏ÊñáÁçªÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂÆÉÊ∂âÂèä‰ΩøÁî®‰æÜËá™ Chebi ÁöÑÁü•Ë≠òÊì¥ÂÖÖÁèæÊúâÁöÑË®ªÈáãÊñáÊú¨Ë™ûÊñôÂ∫´Ôºå‰∏¶ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Ë≠òÂà•ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Âú®ÁßëÂ≠∏ÊñáÊú¨‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÁµêÂêàÊú¨‰ΩìÁü•Ë≠òÂíå LLM ÁöÑË™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÊàëÂÄëÂú®Ë≠òÂà•ÁßëÂ≠∏ÊñáÁçª‰∏≠ÁöÑÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî®ÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû‰∏ÄÁµÑ 8,000 ÁØá ChemRxiv ÊñáÁ´†‰∏≠ÊèêÂèñÂÆÉÂÄëÔºå‰∏¶ÊáâÁî®Á¨¨‰∫åÂÄã LLM ‰æÜÂª∫Á´ã‰∏ÄÂÄãÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî® (CEAR) ÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂÆÉÊèê‰æõË£úÂÖÖ ChEBI ÁöÑË≥áË®äÔºå‰∏¶ÊúâÂä©ÊñºÊì¥ÂÖÖÂÆÉ„ÄÇ

##### **TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**
2407.21693v1 by Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang

Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented
conversations, including information gathering. How to utilize ToD accurately,
efficiently and effectively for information gathering has always been a
critical and challenging task. Recent studies have demonstrated that Large
Language Models (LLMs) excel in dialogue, instruction generation, and
reasoning, and can significantly enhance the performance of TOD through
fine-tuning. However, current datasets primarily cater to user-led systems and
are limited to predefined specific scenarios and slots, thereby necessitating
improvements in the proactiveness, diversity, and capabilities of TOD. In this
study, we present a detailed multi-domain task-oriented data construction
process for conversations, and a Chinese dialogue dataset generated based on
this process, \textbf{TransferTOD}, which authentically simulates human-machine
dialogues in 30 popular life service scenarios. Leveraging this dataset, we
trained a \textbf{TransferTOD-7B} model using full-parameter fine-tuning,
showcasing notable abilities in slot filling and questioning. Our work has
demonstrated its strong generalization capabilities in various downstream
scenarios, significantly enhancing both data utilization efficiency and system
performance. The data is released in
https://github.com/KongLongGeFDU/TransferTOD.

ÊëòË¶ÅÔºö<paragraph>Èù¢Âêë‰ªªÂãôÁöÑÂ∞çË©± (TOD) Á≥ªÁµ±Êó®Âú®ÊúâÊïàÂú∞ËôïÁêÜÈù¢Âêë‰ªªÂãôÁöÑÂ∞çË©±ÔºåÂåÖÊã¨Ë≥áË®äÊî∂ÈõÜ„ÄÇÂ¶Ç‰ΩïÊ∫ñÁ¢∫„ÄÅÊúâÊïàÂú∞Âà©Áî® TOD ÈÄ≤Ë°åË≥áË®äÊî∂ÈõÜ‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÈóúÈçµ‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Â∞çË©±„ÄÅÊåá‰ª§ÁîüÊàêÂíåÊé®ÁêÜÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏¶‰∏îÂèØ‰ª•ÈÄèÈÅéÂæÆË™øÈ°ØËëóÊèêÂçá TOD ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑË≥áÊñôÈõÜ‰∏ªË¶ÅËøéÂêà‰ΩøÁî®ËÄÖ‰∏ªÂ∞éÁöÑÁ≥ªÁµ±Ôºå‰∏¶‰∏îÂÉÖÈôêÊñºÈ†êÂÆöÁæ©ÁöÑÁâπÂÆöÂ†¥ÊôØÂíåÊôÇÊÆµÔºåÂõ†Ê≠§ÊúâÂøÖË¶ÅÊèêÂçá TOD ÁöÑ‰∏ªÂãïÊÄß„ÄÅÂ§öÊ®£ÊÄßÂíåËÉΩÂäõ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË©≥Á¥∞ÁöÑÂ§öÈ†òÂüüÈù¢Âêë‰ªªÂãôË≥áÊñôÂª∫ÊßãÊµÅÁ®ãÔºå‰ª•Âèä‰∏ÄÂÄãÂü∫ÊñºÊ≠§ÊµÅÁ®ãÁîüÊàêÁöÑ‰∏≠ÊñáÂ∞çË©±Ë≥áÊñôÈõÜÔºå\textbf{TransferTOD}ÔºåÂÆÉÁúüÂØ¶Âú∞Ê®°Êì¨‰∫Ü 30 ÂÄãÊµÅË°åÁîüÊ¥ªÊúçÂãôÂ†¥ÊôØ‰∏≠ÁöÑ‰∫∫Ê©üÂ∞çË©±„ÄÇÂà©Áî®Ê≠§Ë≥áÊñôÈõÜÔºåÊàëÂÄë‰ΩøÁî®ÂÖ®ÂèÉÊï∏ÂæÆË™øË®ìÁ∑¥‰∫Ü‰∏ÄÂÄã \textbf{TransferTOD-7B} Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂú®ÊôÇÊÆµÂ°´Ë£úÂíåÊèêÂïèÊñπÈù¢ÁöÑÈ°ØËëóËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂêÑÁ®Æ‰∏ãÊ∏∏Â†¥ÊôØ‰∏≠ÁöÑÂº∑Â§ßÊ≥õÂåñËÉΩÂäõÔºåÈ°ØËëóÊèêÂçá‰∫ÜË≥áÊñôÂà©Áî®ÁéáÂíåÁ≥ªÁµ±ÊïàËÉΩ„ÄÇË≥áÊñôÂ∑≤Áôº‰ΩàÂú® https://github.com/KongLongGeFDU/TransferTOD„ÄÇ</paragraph>

##### **Dynamic Object Queries for Transformer-based Incremental Object Detection**
2407.21687v1 by Jichuan Zhang, Wei Li, Shuang Cheng, Ya-Li Li, Shengjin Wang

Incremental object detection (IOD) aims to sequentially learn new classes,
while maintaining the capability to locate and identify old ones. As the
training data arrives with annotations only with new classes, IOD suffers from
catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue
through knowledge distillation and exemplar replay, ignoring the conflict
between limited model capacity and increasing knowledge. In this paper, we
explore \textit{dynamic object queries} for incremental object detection built
on Transformer architecture. We propose the \textbf{Dy}namic object
\textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which
incrementally expands the model representation ability to achieve
stability-plasticity tradeoff. First, a new set of learnable object queries are
fed into the decoder to represent new classes. These new object queries are
aggregated with those from previous phases to adapt both old and new knowledge
well. Second, we propose the isolated bipartite matching for object queries in
different phases, based on disentangled self-attention. The interaction among
the object queries at different phases is eliminated to reduce inter-class
confusion. Thanks to the separate supervision and computation over object
queries, we further present the risk-balanced partial calibration for effective
exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly
surpasses the state-of-the-art methods, with limited parameter overhead. Code
will be made publicly available.

ÊëòË¶ÅÔºöÂ¢ûÈáèÂºèÁâ©‰ª∂ÂÅµÊ∏¨ (IOD) Êó®Âú®Âæ™Â∫èÊº∏ÈÄ≤Âú∞Â≠∏ÁøíÊñ∞È°ûÂà•ÔºåÂêåÊôÇÁ∂≠ÊåÅÂÆö‰ΩçÂíåËæ®Ë≠òËàäÈ°ûÂà•ÁöÑËÉΩÂäõ„ÄÇÁî±ÊñºË®ìÁ∑¥Ë≥áÊñôÂÉÖÈôÑÊúâÊñ∞È°ûÂà•ÁöÑË®ªËß£ÔºåIOD ÊúÉÁôºÁîüÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ï‰∏ªË¶ÅÈÄèÈÅéÁü•Ë≠òËêÉÂèñÂíåÁØÑ‰æãÈáçÊí≠‰æÜËß£Ê±∫ÈÅ∫ÂøòÂïèÈ°åÔºåÂçªÂøΩÁï•‰∫ÜÊ®°ÂûãÂÆπÈáèÊúâÈôêÂíåÁü•Ë≠òÂ¢ûÂä†‰πãÈñìÁöÑË°ùÁ™Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢Âª∫Á´ãÂú® Transformer Êû∂Êßã‰∏äÁöÑÂ¢ûÈáèÂºèÁâ©‰ª∂ÂÅµÊ∏¨ÁöÑ„ÄåÂãïÊÖãÁâ©‰ª∂Êü•Ë©¢„Äç„ÄÇÊàëÂÄëÊèêÂá∫‰ª•ÂãïÊÖãÁâ©‰ª∂Êü•Ë©¢ÁÇ∫Âü∫Á§éÁöÑÂÅµÊ∏¨ËÆäÂΩ¢ÈáëÂâõ (DyQ-DETR)ÔºåÂÆÉÊúÉÈÄêÊ≠•Êì¥Â±ïÊ®°ÂûãË°®Á§∫ËÉΩÂäõÔºå‰ª•ÈÅîÊàêÁ©©ÂÆöÊÄßÂíåÂèØÂ°ëÊÄßÁöÑÊäòË°∑„ÄÇÈ¶ñÂÖàÔºåÂ∞á‰∏ÄÁµÑÊñ∞ÁöÑÂèØÂ≠∏ÁøíÁâ©‰ª∂Êü•Ë©¢Ëº∏ÂÖ•Ëß£Á¢ºÂô®Ôºå‰ª•Ë°®Á§∫Êñ∞È°ûÂà•„ÄÇÈÄô‰∫õÊñ∞ÁöÑÁâ©‰ª∂Êü•Ë©¢ÊúÉËàáÂâç‰∏ÄÈöéÊÆµÁöÑÊü•Ë©¢Âêà‰ΩµÔºå‰ª•ÈÅ©Áï∂Âú∞Ë™øÊï¥ËàäÊúâÂíåÊñ∞ÁöÑÁü•Ë≠ò„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫Âü∫ÊñºËß£ÈñãËá™ÊàëÊ≥®ÊÑèÂäõÁöÑ„ÄÅÁî®Êñº‰∏çÂêåÈöéÊÆµÁâ©‰ª∂Êü•Ë©¢ÁöÑÂ≠§Á´ã‰∫åÈÉ®ÂåπÈÖç„ÄÇÊ∂àÈô§‰∏çÂêåÈöéÊÆµÁâ©‰ª∂Êü•Ë©¢‰πãÈñìÁöÑ‰∫íÂãïÔºå‰ª•Ê∏õÂ∞ëÈ°ûÂà•ÈñìÁöÑÊ∑∑Ê∑Ü„ÄÇÁî±ÊñºÁâ©‰ª∂Êü•Ë©¢ÊúâÁç®Á´ãÁöÑÁõ£Áù£ÂíåÈÅãÁÆóÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫È¢®Èö™Âπ≥Ë°°ÁöÑÈÉ®ÂàÜÊ†°Ê≠£Ôºå‰ª•ÊúâÊïàÂú∞ÈáçÊí≠ÁØÑ‰æã„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòéÔºåDyQ-DETR Â§ßÂπÖË∂ÖË∂äÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºå‰∏îÂèÉÊï∏ÈñãÈä∑ÊúâÈôê„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**
2407.21674v1 by Krishan Agyakari Raja Babu, Rachana Sathish, Mrunal Pattanaik, Rahul Venkataramani

Synthetic data is becoming increasingly integral in data-scarce fields such
as medical imaging, serving as a substitute for real data. However, its
inherent statistical characteristics can significantly impact downstream tasks,
potentially compromising deployment performance. In this study, we empirically
investigate this issue and uncover a critical phenomenon: downstream neural
networks often exploit spurious distinctions between real and synthetic data
when there is a strong correlation between the data source and the task label.
This exploitation manifests as \textit{simplicity bias}, where models overly
rely on superficial features rather than genuine task-related complexities.
Through principled experiments, we demonstrate that the source of data (real
vs.\ synthetic) can introduce spurious correlating factors leading to poor
performance during deployment when the correlation is absent. We first
demonstrate this vulnerability on a digit classification task, where the model
spuriously utilizes the source of data instead of the digit to provide an
inference. We provide further evidence of this phenomenon in a medical imaging
problem related to cardiac view classification in echocardiograms, particularly
distinguishing between 2-chamber and 4-chamber views. Given the increasing role
of utilizing synthetic datasets, we hope that our experiments serve as
effective guidelines for the utilization of synthetic datasets in model
training.

ÊëòË¶ÅÔºöÂêàÊàêË≥áÊñôÂú®Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÈ†òÂüü‰∏≠ËÆäÂæóË∂ä‰æÜË∂ä‰∏çÂèØÊàñÁº∫Ôºå‰æãÂ¶ÇÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÁî®‰ΩúÁúüÂØ¶Ë≥áÊñôÁöÑÊõø‰ª£ÂìÅ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÂÖßÂú®ÁöÑÁµ±Ë®àÁâπÊÄßÊúÉÈ°ØËëóÂΩ±Èüø‰∏ãÊ∏∏‰ªªÂãôÔºåÂèØËÉΩÊêçÂÆ≥ÈÉ®ÁΩ≤ÊïàËÉΩ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂØ¶Ë≠âË™øÊü•Ê≠§ÂïèÈ°åÔºå‰∏¶Êè≠Èú≤‰∏ÄÂÄãÈóúÈçµÁèæË±°ÔºöÁï∂Ë≥áÊñô‰æÜÊ∫êËàá‰ªªÂãôÊ®ôÁ±§‰πãÈñìÊúâÂæàÂº∑ÁöÑÁõ∏ÈóúÊÄßÊôÇÔºå‰∏ãÊ∏∏Á•ûÁ∂ìÁ∂≤Ë∑ØÈÄöÂ∏∏ÊúÉÂà©Áî®ÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñô‰πãÈñìÁöÑËôõÂÅáÂçÄÂà•„ÄÇÈÄôÁ®ÆÂà©Áî®Ë°®ÁèæÁÇ∫„ÄåÁ∞°ÂåñÂÅèÂ∑Æ„ÄçÔºåÂÖ∂‰∏≠Ê®°ÂûãÈÅéÂ∫¶‰æùË≥¥Ë°®Èù¢ÁâπÂæµÔºåËÄå‰∏çÊòØÁúüÊ≠£ÁöÑËàá‰ªªÂãôÁõ∏ÈóúÁöÑË§áÈõúÊÄß„ÄÇÈÄèÈÅéÊúâÂéüÂâáÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéË≥áÊñô‰æÜÊ∫êÔºàÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñôÔºâÂèØËÉΩÊúÉÂºïÂÖ•ËôõÂÅáÁöÑÁõ∏ÈóúÂõ†Á¥†ÔºåÂ∞éËá¥Âú®Áõ∏ÈóúÊÄß‰∏çÂ≠òÂú®ÊôÇÈÉ®ÁΩ≤ÊúüÈñìÊïàËÉΩ‰∏ç‰Ω≥„ÄÇÊàëÂÄëÈ¶ñÂÖàÂú®Êï∏Â≠óÂàÜÈ°û‰ªªÂãô‰∏≠Ë≠âÊòéÊ≠§ÊºèÊ¥ûÔºåÂÖ∂‰∏≠Ê®°ÂûãËôõÂÅáÂú∞Âà©Áî®Ë≥áÊñô‰æÜÊ∫êËÄåÈùûÊï∏Â≠ó‰æÜÊèê‰æõÊé®Ë´ñ„ÄÇÊàëÂÄëÂú®ËàáË∂ÖÈü≥Ê≥¢ÂøÉËáüË¶ñÈáéÂàÜÈ°ûÁõ∏ÈóúÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂïèÈ°å‰∏≠ÈÄ≤‰∏ÄÊ≠•Êèê‰æõÊ≠§ÁèæË±°ÁöÑË≠âÊìöÔºåÁâπÂà•ÊòØÂçÄÂàÜ‰∫åËÖîÂíåÂõõËÖîË¶ñÈáé„ÄÇÈëëÊñºÂêàÊàêË≥áÊñôÈõÜÁöÑ‰ΩøÁî®ËßíËâ≤Êó•ÁõäÂ¢ûÂä†ÔºåÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÂØ¶È©óËÉΩ‰ΩúÁÇ∫Âú®Ê®°ÂûãË®ìÁ∑¥‰∏≠Âà©Áî®ÂêàÊàêË≥áÊñôÈõÜÁöÑÊúâÊïàÊåáÂçó„ÄÇ

##### **Universal Approximation Theory: Foundations for Parallelism in Neural Networks**
2407.21670v1 by Wei Wang, Qing Li

Neural networks are increasingly evolving towards training large models with
big data, a method that has demonstrated superior performance across many
tasks. However, this approach introduces an urgent problem: current deep
learning models are predominantly serial, meaning that as the number of network
layers increases, so do the training and inference times. This is unacceptable
if deep learning is to continue advancing. Therefore, this paper proposes a
deep learning parallelization strategy based on the Universal Approximation
Theorem (UAT). From this foundation, we designed a parallel network called
Para-Former to test our theory. Unlike traditional serial models, the inference
time of Para-Former does not increase with the number of layers, significantly
accelerating the inference speed of multi-layer networks. Experimental results
validate the effectiveness of this network.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ≠£ÊúùË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãËàáÂ∑®ÈáèË≥áÊñôÁöÑÊñπÂêëÁôºÂ±ïÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂú®Ë®±Â§ö‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÊïàËÉΩÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂ∏∂‰æÜ‰∫Ü‰∏ÄÂÄãËø´ÂàáÁöÑÂïèÈ°åÔºöÁõÆÂâçÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏ªË¶ÅÊòØ‰∏≤ÂàóÁöÑÔºåÈÄôË°®Á§∫Á∂≤Ë∑ØÂ±§Êï∏Ë∂äÂ§öÔºåË®ìÁ∑¥ÂíåÊé®Ë´ñÊôÇÈñì‰πüÊúÉË∂äÈï∑„ÄÇÂ¶ÇÊûúÊ∑±Â∫¶Â≠∏ÁøíË¶ÅÁπºÁ∫åÈÄ≤Ê≠•ÔºåÈÄôÊòØÁÑ°Ê≥ïÊé•ÂèóÁöÑ„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÊ≥õÂáΩÈÄºËøëÂÆöÁêÜ (UAT) ÁöÑÊ∑±Â∫¶Â≠∏Áøí‰∏¶Ë°åÂåñÁ≠ñÁï•„ÄÇÂü∫ÊñºÊ≠§Âü∫Á§éÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ Para-Former ÁöÑÂπ≥Ë°åÁ∂≤Ë∑Ø‰æÜÊ∏¨Ë©¶ÊàëÂÄëÁöÑÁêÜË´ñ„ÄÇËàáÂÇ≥Áµ±‰∏≤ÂàóÊ®°Âûã‰∏çÂêåÔºåPara-Former ÁöÑÊé®Ë´ñÊôÇÈñì‰∏çÊúÉÈö®ËëóÂ±§Êï∏Â¢ûÂä†ËÄåÂ¢ûÂä†ÔºåÂ§ßÂπÖÂä†ÈÄüÂ§öÂ±§Á∂≤Ë∑ØÁöÑÊé®Ë´ñÈÄüÂ∫¶„ÄÇÂØ¶È©óÁµêÊûúÈ©óË≠â‰∫ÜÊ≠§Á∂≤Ë∑ØÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Synth-Empathy: Towards High-Quality Synthetic Empathy Data**
2407.21669v1 by Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang, Linkun Sun, Bihui Yu, Conghui He, Wentao Zhang

In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capabilities has become a crucial
prerequisite. Consequently, managing and understanding empathetic datasets have
gained increasing significance. However, empathetic data are typically
human-labeled, leading to insufficient datasets and wasted human labor. In this
work, we present Synth-Empathy, an LLM-based data generation and quality and
diversity selection pipeline that automatically generates high-quality
empathetic data while discarding low-quality data. With the data generated from
a low empathetic model, we are able to further improve empathetic response
performance and achieve state-of-the-art (SoTA) results across multiple
benchmarks. Moreover, our model achieves SoTA performance on various human
evaluation benchmarks, demonstrating its effectiveness and robustness in
real-world applications. Furthermore, we show the trade-off between data
quantity and quality, providing insights into empathetic data generation and
selection.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÈÄ≤Â±ïÔºåÂØ¶ÁèæÂÑ™Áï∞ÁöÑÂêåÁêÜÂøÉÂõûÊáâËÉΩÂäõÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈóúÈçµÂÖàÊ±∫Ê¢ù‰ª∂„ÄÇÂõ†Ê≠§ÔºåÁÆ°ÁêÜÂíåÁêÜËß£ÂêåÁêÜÂøÉË≥áÊñôÈõÜËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂêåÁêÜÂøÉË≥áÊñôÈÄöÂ∏∏ÊòØÁî±‰∫∫È°ûÊ®ôË®òÔºåÂ∞éËá¥Ë≥áÊñôÈõÜ‰∏çË∂≥‰∏îÊµ™Ë≤ª‰∫∫Âäõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Synth-EmpathyÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑË≥áÊñôÁîüÊàê„ÄÅÂìÅË≥™ÂíåÂ§öÊ®£ÊÄßÈÅ∏ÊìáÁÆ°ÈÅìÔºåÂèØ‰ª•Ëá™ÂãïÁîüÊàêÈ´òÂìÅË≥™ÁöÑÂêåÁêÜÂøÉË≥áÊñôÔºåÂêåÊôÇÊç®Ê£Ñ‰ΩéÂìÅË≥™Ë≥áÊñô„ÄÇÈÄèÈÅéÂæû‰ΩéÂêåÁêÜÂøÉÊ®°ÂûãÁîüÊàêÁöÑË≥áÊñôÔºåÊàëÂÄëËÉΩÂ§†ÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑÂêåÁêÜÂøÉÂõûÊáâË°®ÁèæÔºå‰∏¶Âú®Â§öÂÄãÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ (SoTA) ÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂêÑÁ®Æ‰∫∫È°ûË©ï‰º∞Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÉΩÈÅîÂà∞‰∫Ü SoTA ÁöÑË°®ÁèæÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜË≥áÊñôÊï∏ÈáèÂíåÂìÅË≥™‰πãÈñìÁöÑÊ¨äË°°ÔºåÊèê‰æõ‰∫ÜÂ∞çÂêåÁêÜÂøÉË≥áÊñôÁîüÊàêÂíåÈÅ∏ÊìáÁöÑË¶ãËß£„ÄÇ

##### **An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**
2407.21666v1 by Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo

Early detection of drought stress is critical for taking timely measures for
reducing crop loss before the drought impact becomes irreversible. The subtle
phenotypical and physiological changes in response to drought stress are
captured by non-invasive imaging techniques and these imaging data serve as
valuable resource for machine learning methods to identify drought stress.
While convolutional neural networks (CNNs) are in wide use, vision transformers
(ViTs) present a promising alternative in capturing long-range dependencies and
intricate spatial relationships, thereby enhancing the detection of subtle
indicators of drought stress. We propose an explainable deep learning pipeline
that leverages the power of ViTs for drought stress detection in potato crops
using aerial imagery. We applied two distinct approaches: a synergistic
combination of ViT and support vector machine (SVM), where ViT extracts
intricate spatial features from aerial images, and SVM classifies the crops as
stressed or healthy and an end-to-end approach using a dedicated classification
layer within ViT to directly detect drought stress. Our key findings explain
the ViT model's decision-making process by visualizing attention maps. These
maps highlight the specific spatial features within the aerial images that the
ViT model focuses as the drought stress signature. Our findings demonstrate
that the proposed methods not only achieve high accuracy in drought stress
identification but also shedding light on the diverse subtle plant features
associated with drought stress. This offers a robust and interpretable solution
for drought stress monitoring for farmers to undertake informed decisions for
improved crop management.

ÊëòË¶ÅÔºöÂèäÊó©ÂÅµÊ∏¨‰πæÊó±Â£ìÂäõÔºåÂ∞çÊñºÂú®‰πæÊó±ÂΩ±ÈüøËÆäÂæó‰∏çÂèØÈÄÜËΩâ‰πãÂâçÊé°ÂèñÈÅ©ÊôÇÊé™ÊñΩ‰ª•Ê∏õÂ∞ë‰ΩúÁâ©ÊêçÂ§±Ëá≥ÈóúÈáçË¶Å„ÄÇÈùû‰æµÂÖ•ÂºèÂΩ±ÂÉèÊäÄË°ìÂèØÊçïÊçâÂà∞Â∞ç‰πæÊó±Â£ìÂäõÁî¢ÁîüÁöÑÁ¥∞ÂæÆË°®ÂûãÂíåÁîüÁêÜËÆäÂåñÔºåËÄåÈÄô‰∫õÂΩ±ÂÉèË≥áÊñôÂèØ‰ΩúÁÇ∫Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ïË≠òÂà•‰πæÊó±Â£ìÂäõÁöÑÂØ∂Ë≤¥Ë≥áÊ∫ê„ÄÇÈõñÁÑ∂Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Ë¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜË¶ñË¶∫Transformer (ViT) Âú®ÊçïÊçâÈï∑Á®ã‰æùË≥¥Èóú‰øÇÂíåË§áÈõúÁ©∫ÈñìÈóú‰øÇÊñπÈù¢Êèê‰æõ‰∫ÜÊúâÂâçÊôØÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜÂ∞ç‰πæÊó±Â£ìÂäõÁöÑÁ¥∞ÂæÆÊåáÊ®ôÁöÑÂÅµÊ∏¨„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ∑±Â∫¶Â≠∏ÁøíÁÆ°ÈÅìÔºåÂÆÉÂà©Áî® ViT ÁöÑÂäüËÉΩ‰æÜÂÅµÊ∏¨È¶¨Èà¥ËñØ‰ΩúÁâ©‰∏≠ÁöÑ‰πæÊó±Â£ìÂäõÔºå‰∏¶‰ΩøÁî®Ëà™ÊãçÂΩ±ÂÉè„ÄÇÊàëÂÄëÊáâÁî®ÂÖ©Á®Æ‰∏çÂêåÁöÑÊñπÊ≥ïÔºöViT ÂíåÊîØÊè¥ÂêëÈáèÊ©ü (SVM) ÁöÑÂçîÂêåÁµÑÂêàÔºåÂÖ∂‰∏≠ ViT ÂæûËà™ÊãçÂΩ±ÂÉè‰∏≠ËêÉÂèñË§áÈõúÁöÑÁ©∫ÈñìÁâπÂæµÔºåËÄå SVM Â∞á‰ΩúÁâ©ÂàÜÈ°ûÁÇ∫ÂèóÂ£ìÊàñÂÅ•Â∫∑Ôºå‰ª•Âèä‰ΩøÁî® ViT ÂÖßÁöÑÂ∞àÁî®ÂàÜÈ°ûÂ±§ÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ï‰æÜÁõ¥Êé•ÂÅµÊ∏¨‰πæÊó±Â£ìÂäõ„ÄÇÊàëÂÄëÁöÑÈóúÈçµÁôºÁèæÈÄèÈÅéË¶ñË¶∫ÂåñÊ≥®ÊÑèÂäõÂúñ‰æÜËß£Èáã ViT Ê®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÈÄô‰∫õÂúñÁ™ÅÈ°Ø‰∫Ü ViT Ê®°ÂûãÈóúÊ≥®ÁöÑËà™ÊãçÂΩ±ÂÉè‰∏≠ÁöÑÁâπÂÆöÁ©∫ÈñìÁâπÂæµÔºå‰ΩúÁÇ∫‰πæÊó±Â£ìÂäõÁöÑÁâπÂæµ„ÄÇÊàëÂÄëÁöÑÁôºÁèæË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏çÂÉÖÂú®‰πæÊó±Â£ìÂäõË≠òÂà•‰∏≠ÂØ¶Áèæ‰∫ÜÈ´òÊ∫ñÁ¢∫Â∫¶ÔºåËÄå‰∏îÈÇÑÈó°Êòé‰∫ÜËàá‰πæÊó±Â£ìÂäõÁõ∏ÈóúÁöÑÂêÑÁ®ÆÁ¥∞ÂæÆÊ§çÁâ©ÁâπÂæµ„ÄÇÈÄôÁÇ∫‰πæÊó±Â£ìÂäõÁõ£ÊéßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ß‰∏îÂèØËß£ÈáãÁöÑËß£Ê±∫ÊñπÊ°àÔºåËÆìËæ≤Ê∞ëÂèØ‰ª•ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºå‰ª•ÊîπÂñÑ‰ΩúÁâ©ÁÆ°ÁêÜ„ÄÇ

##### **Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**
2407.21659v2 by Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang

Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.

ÊëòË¶ÅÔºö<paragraph>Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Êì¥Â±ï‰∫Ü LLM ÂÖ®Èù¢ÁêÜËß£Ë¶ñË¶∫Ë≥áË®äÁöÑËÉΩÂäõÔºåÂú®Ë®±Â§ö‰ª•Ë¶ñË¶∫ÁÇ∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏≠ÂèñÂæóÈ°ØËëóÁöÑË°®Áèæ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊúÄËøëÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåÈÄô‰∫õÊ®°ÂûãÂÆπÊòìÂèóÂà∞Ë∂äÁçÑÊîªÊìäÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâùÂâäÊäÄË°ìÔºåÊÉ°ÊÑè‰ΩøÁî®ËÄÖÂèØ‰ª•Á†¥Â£ûÁõÆÊ®ôÊ®°ÂûãÁöÑÂÆâÂÖ®Â∞çÈΩäÔºå‰∏¶Áî¢ÁîüÂÖ∑ÊúâË™§Â∞éÊÄßÂíåÂç±ÂÆ≥ÊÄßÁöÑÁ≠îÊ°à„ÄÇÈÄôÁ®ÆÊΩõÂú®Â®ÅËÑÖÊòØÁî± LLM ÁöÑÂõ∫ÊúâÊºèÊ¥ûÂíåË¶ñË¶∫Ëº∏ÂÖ•ÂºïÂÖ•ÁöÑÊõ¥Â§ßÊîªÊìäÁØÑÂúçÊâÄÈÄ†ÊàêÁöÑ„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ VLM Â∞çÊäóË∂äÁçÑÊîªÊìäÁöÑÂÆâÂÖ®ÊÄßÔºåÁ†îÁ©∂‰∫∫Âì°ÈñãÁôº‰∫ÜÂêÑÁ®ÆÈò≤Á¶¶ÊäÄË°ì„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈúÄË¶Å‰øÆÊîπÊ®°ÂûãÁöÑÂÖßÈÉ®ÁµêÊßãÔºåÊàñÂú®Êé®ÁêÜÈöéÊÆµÈúÄË¶ÅÂ§ßÈáèÁöÑË®àÁÆóË≥áÊ∫ê„ÄÇÂ§öÊ®°ÊÖãË≥áË®äÊòØ‰∏ÄÊääÈõôÈù¢ÂàÉ„ÄÇÈõñÁÑ∂ÂÆÉÂ¢ûÂä†‰∫ÜÊîªÊìäÈ¢®Èö™Ôºå‰ΩÜÂÆÉ‰πüÊèê‰æõ‰∫ÜÂèØ‰ª•Â¢ûÂº∑Èò≤Ë≠∑Êé™ÊñΩÁöÑÈ°çÂ§ñË≥áÊñô„ÄÇÂèóÊ≠§ÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫Ë∑®Ê®°ÊÖãË≥áË®äÂÅµÊ∏¨Âô® ($\textit{CIDER}$)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑË∂äÁçÑÂÅµÊ∏¨Âô®ÔºåÊó®Âú®Ë≠òÂà•ÊÉ°ÊÑèÊìæÂãïÁöÑÂΩ±ÂÉèËº∏ÂÖ•ÔºåÂà©Áî®ÊúâÂÆ≥Êü•Ë©¢ÂíåÂ∞çÊäóÊÄßÂΩ±ÂÉè‰πãÈñìÁöÑË∑®Ê®°ÊÖãÁõ∏‰ººÊÄß„ÄÇÈÄôÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑË∑®Ê®°ÊÖãË≥áË®äÂÅµÊ∏¨Âô® $\textit{CIDER}$ Áç®Á´ãÊñºÁõÆÊ®ô VLMÔºå‰∏¶‰∏îÈúÄË¶ÅËºÉÂ∞ëÁöÑË®àÁÆóÊàêÊú¨„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü $\textit{CIDER}$ ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºå‰ª•ÂèäÂÆÉÂ∞çÁôΩÁõíÂíåÈªëÁõí VLM ÁöÑÂèØÁßªÊ§çÊÄß„ÄÇ</paragraph>

##### **Spatial Transformer Network YOLO Model for Agricultural Object Detection**
2407.21652v1 by Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples

Object detection plays a crucial role in the field of computer vision by
autonomously identifying and locating objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.

ÊëòË¶ÅÔºöÁõÆÊ®ôÂÅµÊ∏¨Âú®ÈõªËÖ¶Ë¶ñË¶∫È†òÂüü‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤ÔºåÂÆÉËÉΩËá™‰∏ªËæ®Ë≠ò‰∏¶ÂÆö‰ΩçÊÑüËààË∂£ÁöÑÁõÆÊ®ô„ÄÇYou Only Look Once (YOLO) Ê®°ÂûãÊòØ‰∏ÄÂÄãÊúâÊïàÁöÑÂñÆÊ¨°ÂÅµÊ∏¨Âô®„ÄÇÁÑ∂ËÄåÔºåYOLO Âú®Èõú‰∫ÇÊàñÈÉ®ÂàÜÈÅÆÊìãÁöÑÂ†¥ÊôØ‰∏≠ÊúÉÈù¢Ëá®ÊåëÊà∞Ôºå‰∏¶‰∏îÂú®ËôïÁêÜÂ∞èËÄåÂ∞çÊØîÂ∫¶‰ΩéÁöÑÁõÆÊ®ôÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂ∞áÁ©∫ÈñìËΩâÊèõÁ∂≤Ë∑Ø (STN) Êï¥ÂêàÂà∞ YOLO ‰∏≠‰ª•ÊèêÂçáÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑ STN-YOLO Êó®Âú®ÈÄèÈÅéÂú®ÂÅµÊ∏¨ÈÅéÁ®ãÂâçÂ∞àÊ≥®ÊñºÂΩ±ÂÉèÁöÑÈáçË¶ÅÂçÄÂüü‰∏¶ÊîπÂñÑÊ®°ÂûãÁöÑÁ©∫Èñì‰∏çËÆäÊÄßÔºå‰æÜÊèêÂçáÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë≥™ÈáèÂíåÊï∏Èáè‰∏äÈÉΩÊîπÂñÑ‰∫ÜÁõÆÊ®ôÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü STN Ê®°ÁµÑ‰∏≠‰∏çÂêåÂÆö‰ΩçÁ∂≤Ë∑ØÁöÑÂΩ±ÈüøÔºå‰ª•ÂèäÊ®°ÂûãÂú®‰∏çÂêåÁ©∫ÈñìËΩâÊèõ‰∏≠ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÂ∞á STN-YOLO ÊáâÁî®ÊñºËæ≤Ê•≠ÁõÆÊ®ôÂÅµÊ∏¨ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰ª•Âèä‰æÜËá™ÊúÄÂÖàÈÄ≤ÁöÑÊ§çÁâ©Ë°®ÂûãÊ∫´ÂÆ§Ë®≠ÊñΩÁöÑÊñ∞Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Human interaction classifier for LLM based chatbot**
2407.21647v1 by Diego Mart√≠n, Jordi Sanchez, Xavier Vizca√≠no

This study investigates different approaches to classify human interactions
in an artificial intelligence-based environment, specifically for Applus+
IDIADA's intelligent agent AIDA. The main objective is to develop a classifier
that accurately identifies the type of interaction received (Conversation,
Services, or Document Translation) to direct requests to the appropriate
channel and provide a more specialized and efficient service. Various models
are compared, including LLM-based classifiers, KNN using Titan and Cohere
embeddings, SVM, and artificial neural networks. Results show that SVM and ANN
models with Cohere embeddings achieve the best overall performance, with
superior F1 scores and faster execution times compared to LLM-based approaches.
The study concludes that the SVM model with Cohere embeddings is the most
suitable option for classifying human interactions in the AIDA environment,
offering an optimal balance between accuracy and computational efficiency.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂú®Âü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÁí∞Â¢É‰∏≠Â∞ç‰∫∫È°û‰∫íÂãïÈÄ≤Ë°åÂàÜÈ°ûÁöÑ‰∏çÂêåÊñπÊ≥ïÔºåÁâπÂà•ÊòØÈáùÂ∞ç Applus+ IDIADA ÁöÑÊô∫ÊÖß‰ª£ÁêÜ‰∫∫ AIDA„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÈñãÁôº‰∏ÄÂÄãÂàÜÈ°ûÂô®ÔºåËÉΩÁ≤æÁ¢∫Ë≠òÂà•Êé•Êî∂Âà∞ÁöÑ‰∫íÂãïÈ°ûÂûãÔºàÂ∞çË©±„ÄÅÊúçÂãôÊàñÊñá‰ª∂ÁøªË≠ØÔºâÔºå‰ª•Â∞áË´ãÊ±ÇÂ∞éÂêëÈÅ©Áï∂ÁöÑÁÆ°ÈÅìÔºå‰∏¶Êèê‰æõÊõ¥Â∞àÊ•≠‰∏îÊúâÊïàÁéáÁöÑÊúçÂãô„ÄÇÊØîËºÉ‰∫ÜÂêÑÁ®ÆÊ®°ÂûãÔºåÂåÖÊã¨Âü∫Êñº LLM ÁöÑÂàÜÈ°ûÂô®„ÄÅ‰ΩøÁî® Titan Âíå Cohere ÂÖßÂµåÁöÑ KNN„ÄÅSVM Âíå‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊé°Áî® Cohere ÂÖßÂµåÁöÑ SVM Âíå ANN Ê®°ÂûãÂèØÈÅîÊàêÊúÄ‰Ω≥Êï¥È´îÊïàËÉΩÔºåËàáÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÂÑ™Áï∞ÁöÑ F1 ÂàÜÊï∏ÂíåÊõ¥Âø´ÁöÑÂü∑Ë°åÊôÇÈñì„ÄÇÁ†îÁ©∂ÁµêË´ñÊòØÔºåÊé°Áî® Cohere ÂÖßÂµåÁöÑ SVM Ê®°ÂûãÊúÄÈÅ©ÂêàÂú® AIDA Áí∞Â¢É‰∏≠Â∞ç‰∫∫È°û‰∫íÂãïÈÄ≤Ë°åÂàÜÈ°ûÔºåÂú®Ê∫ñÁ¢∫Â∫¶ÂíåÈÅãÁÆóÊïàÁéá‰πãÈñìÂèñÂæóÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇ

##### **Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**
2407.21646v1 by Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang

In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Ë∑®Ë™ûË®Ä‰ª£ÁêÜ‚Äî‚ÄîÂêåËÅ≤ÂÇ≥Ë≠Ø (CLAS)Ôºå‰∏ÄÂÄãÈ´òÂìÅË≥™‰∏îÈ°û‰ºº‰∫∫È°ûÁöÑÂêåËÅ≤ÂÇ≥Ë≠Ø (SiST) Á≥ªÁµ±„ÄÇÂèóÂà∞Â∞àÊ•≠‰∫∫È°ûÂè£Ë≠ØÂì°ÁöÑÂïüÁôºÔºåÊàëÂÄëÂà©Áî®‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊï∏ÊìöÈ©ÖÂãïËÆÄÂØ´Á≠ñÁï•‰æÜÂπ≥Ë°°ÁøªË≠ØÂìÅË≥™ÂíåÂª∂ÈÅ≤„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÁøªË≠ØÈ†òÂüüÂÖßË°ìË™ûÁöÑÊåëÊà∞ÔºåCLAS Êé°Áî®Â§öÊ®°ÂºèÊ™¢Á¥¢Ê®°ÁµÑ‰æÜÁç≤ÂèñÁõ∏ÈóúË≥áË®ä‰ª•Êì¥ÂÖÖÁøªË≠Ø„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊîØÊè¥‰∏ãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèØ‰ª•ÈÄèÈÅéËÄÉÈáèËº∏ÂÖ•Èü≥Ë®ä„ÄÅÊ≠∑Âè≤ËÑàÁµ°ÂíåÊ™¢Á¥¢Ë≥áË®ä‰æÜÁî¢ÁîüÂÆπÈåØÁøªË≠Ø„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±‰ª•È°ØËëóÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÂÖ∂‰ªñÁ≥ªÁµ±„ÄÇËàáÂ∞àÊ•≠‰∫∫È°ûÂè£Ë≠ØÂì°‰øùÊåÅ‰∏ÄËá¥ÔºåÊàëÂÄë‰ΩøÁî®Êõ¥Â•ΩÁöÑ‰∫∫È°ûË©ïÈáèÊåáÊ®ô‚Äî‚ÄîÊúâÊïàË≥áË®äÊØî‰æã (VIP) ‰æÜË©ïÈáè CLASÔºåË©≤ÊåáÊ®ôË°°ÈáèÂèØ‰ª•ÊàêÂäüÂÇ≥ÈÅîÁµ¶ËÅΩÁúæÁöÑË≥áË®äÈáè„ÄÇÂú®ÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÔºåÊºîË¨õÈÄöÂ∏∏‰∏çÊµÅÊö¢„ÄÅÈùûÊ≠£Âºè‰∏î‰∏çÊ∏Ö‰∏çÊ•öÔºåCLAS Âú®‰∏≠Ë≠ØËã±ÂíåËã±Ë≠Ø‰∏≠ÁøªË≠ØÊñπÂêëÂàÜÂà•ÈÅîÂà∞ 81.3% Âíå 78.0% ÁöÑ VIP„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÊúÄÂÖàÈÄ≤ÁöÑÂïÜÁî®ÊàñÈñãÊ∫êÁ≥ªÁµ±ÂÉÖÈÅîÂà∞ 35.4% Âíå 41.6%„ÄÇÂú®Ê•µÂõ∞Èõ£ÁöÑË≥áÊñôÈõÜ‰∏äÔºåÂÖ∂‰ªñÁ≥ªÁµ±ÁöÑ VIP ‰ΩéÊñº 13%ÔºåCLAS ‰ªçËÉΩÈÅîÂà∞ 70% ÁöÑ VIP„ÄÇ</paragraph>

##### **Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**
2407.21638v1 by Hermione Warr, Yasin Ibrahim, Daniel R. McGowan, Konstantinos Kamnitsas

Automation of medical image interpretation could alleviate bottlenecks in
diagnostic workflows, and has become of particular interest in recent years due
to advancements in natural language processing. Great strides have been made
towards automated radiology report generation via AI, yet ensuring clinical
accuracy in generated reports is a significant challenge, hindering deployment
of such methods in clinical practice. In this work we propose a quality control
framework for assessing the reliability of AI-generated radiology reports with
respect to semantics of diagnostic importance using modular auxiliary auditing
components (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings
show that incorporating ACs in the form of disease-classifiers can enable
auditing that identifies more reliable reports, resulting in higher F1 scores
compared to unfiltered generated reports. Additionally, leveraging the
confidence of the AC labels further improves the audit's effectiveness.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂà§ËÆÄÁöÑËá™ÂãïÂåñÂèØ‰ª•Ê∏õËºïË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÁì∂È†∏Ôºå‰∏¶‰∏îÁî±ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Ê≠•ÔºåÂú®ËøëÂπ¥‰æÜÁâπÂà•ÂèóÂà∞ÈáçË¶ñ„ÄÇÂú®ÈÄèÈÅé AI Ëá™ÂãïÁîüÊàêÊîæÂ∞ÑÁ∑öÂ†±ÂëäÊñπÈù¢Â∑≤Á∂ìÂèñÂæó‰∫ÜÈï∑Ë∂≥ÁöÑÈÄ≤Â±ïÔºåÁÑ∂ËÄåÁ¢∫‰øùÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊ∫ñÁ¢∫ÊÄßÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÈòªÁ§ô‰∫ÜÊ≠§È°ûÊñπÊ≥ïÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂìÅË≥™ÊéßÂà∂Êû∂ÊßãÔºåÁî®ÊñºË©ï‰º∞ AI ÁîüÊàêÁöÑÊîæÂ∞ÑÁ∑öÂ†±ÂëäÁöÑÂèØÈù†ÊÄßÔºå‰∏¶‰ΩøÁî®Ê®°ÁµÑÂåñËºîÂä©Á®ΩÊ†∏ÂÖÉ‰ª∂ (AC) ÈáùÂ∞çË®∫Êñ∑ÈáçË¶ÅÊÄßÁöÑË™ûÁæ©ÈÄ≤Ë°åË©ï‰º∞„ÄÇÂú® MIMIC-CXR Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁÆ°ÈÅìÔºåÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫Ôºå‰ª•ÁñæÁóÖÂàÜÈ°ûÂô®ÁöÑÂΩ¢ÂºèÁ¥çÂÖ• AC ÂèØ‰ª•ÂïüÁî®Á®ΩÊ†∏Ôºå‰ª•Ë≠òÂà•Êõ¥ÂèØÈù†ÁöÑÂ†±ÂëäÔºåËàáÊú™Á∂ìÁØ©ÈÅ∏ÁöÑÁîüÊàêÂ†±ÂëäÁõ∏ÊØîÔºåÊúÉÁî¢ÁîüÊõ¥È´òÁöÑ F1 ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÈÄ≤‰∏ÄÊ≠•Âà©Áî® AC Ê®ôÁ±§ÁöÑ‰ø°ÂøÉÂèØ‰ª•ÊèêÈ´òÁ®ΩÊ†∏ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**
2407.21633v1 by Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang

Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.

ÊëòË¶ÅÔºöÈõ∂Ê¨°ÁôºË©±ÁãÄÊÖãËøΩËπ§ (DST) Êó®Âú®ËÆìÂ∞çË©±Á≥ªÁµ±ËÉΩÂ§†Âú®Ê≤íÊúâÊâãÂãïË®ªËß£ÊàñÂª£Ê≥õÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãËΩâÊèõÂà∞‰∏çÁÜüÊÇâÁöÑÈ†òÂüü„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄèÈÅéÂ∞áÊèêÁ§∫ÂµåÂÖ•Ë™ûË®ÄÊ®°Âûã (LM) ‰æÜÈÅîÊàêÊ≠§ÁõÆÊ®ô„ÄÇÂ∏∏Ë¶ãÁöÑÊñπÊ≥ïÂåÖÊã¨Âú®Ëº∏ÂÖ•Â±§Êï¥ÂêàÊèêÁ§∫ÊàñÂú®ÊØèÂÄãTransformerÂ±§‰∏≠ÂºïÂÖ•ÂèØÂ≠∏ÁøíËÆäÊï∏„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊØèÁ®ÆÁ≠ñÁï•ÈÉΩÂ±ïÁèæÂá∫Âõ∫ÊúâÁöÑÈôêÂà∂„ÄÇÊï¥ÂêàÂú®Ëº∏ÂÖ•Â±§ÁöÑÊèêÁ§∫ÊúâÊú™ÂÖÖÂàÜÂà©Áî®ÁöÑÈ¢®Èö™ÔºåÂÆÉÂÄëÁöÑÂΩ±ÈüøÂèØËÉΩÊúÉÈö®ËëóÂæåÁ∫åÁöÑTransformerÂ±§ËÄåÈÅûÊ∏õ„ÄÇÁõ∏ÂèçÂú∞ÔºåÂú®ÊØèÂÄãÂ±§‰∏≠Âä†ÂÖ•ÂèØÂ≠∏ÁøíËÆäÊï∏ÊúÉ‰ΩøË®ìÁ∑¥ÈÅéÁ®ãË§áÈõúÂåñ‰∏¶Â¢ûÂä†Êé®Ë´ñÂª∂ÈÅ≤„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞ÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫ÈõôÈáç‰ΩéÁß©ÈÅ©Êáâ (DualLoRA)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞àÁÇ∫Èõ∂Ê¨° DST Ë®≠Ë®àÁöÑÂç≥ÊèíÂç≥Áî®Êû∂Êßã„ÄÇDualLoRA ÁµêÂêà‰∫ÜÂÖ©ÂÄã‰∏çÂêåÁöÑ‰ΩéÁß©ÈÅ©Êáâ (LoRA) ÂÖÉ‰ª∂ÔºåÈáùÂ∞çÂ∞çË©±ÂÖßÂÆπËôïÁêÜÂíåÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÔºå‰ª•Á¢∫‰øùÊèêÁ§∫Âú®TransformerÊ®°ÂûãÂ±§‰∏≠ÂÖ®Èù¢ÁôºÊèÆÂΩ±ÈüøÂäõ„ÄÇÈÄôÊòØÂú®‰∏çÁî¢ÁîüÈ°çÂ§ñÊé®Ë´ñÂª∂ÈÅ≤ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÁöÑÔºåÂ±ïÁ§∫‰∫ÜËàáÁèæÊúâÊû∂ÊßãÁöÑÊúâÊïàÊï¥Âêà„ÄÇÈÄèÈÅéÂú® MultiWOZ Âíå SGD Ë≥áÊñôÈõÜ‰∏äÁöÑÂö¥Ê†ºË©ï‰º∞ÔºåDualLoRA Âú®Â§öÂÄãÈ†òÂüüÂ±ïÁèæÂá∫È°ØËëóÁöÑÊîπÈÄ≤ÔºåÂú®Èõ∂Ê¨°Ë®≠ÂÆö‰∏≠ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÂü∫Ê∫ñÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÊñºÊ≠§ËôïÂèñÂæóÔºö\url{https://github.com/suntea233/DualLoRA}„ÄÇ

##### **TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**
2407.21630v1 by Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi

Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.

ÊëòË¶ÅÔºö‰ΩúËÄÖÊ∑∑Ê∑ÜÊó®Âú®ÈÄöËøáÊîπÂèò‰∏éÊñáÊú¨‰ΩúËÄÖÁõ∏ÂÖ≥ÁöÑÂÜô‰ΩúÈ£éÊ†º„ÄÅËØçÊ±á„ÄÅËØ≠Ê≥ïÂíåÂÖ∂‰ªñËØ≠Ë®ÄÁâπÂæÅÊù•ÈöêËóè‰ΩúËÄÖÂú®ÊñáÊú¨‰∏≠ÁöÑË∫´‰ªΩ„ÄÇËøôÁßçÊîπÂèòÈúÄË¶ÅÂπ≥Ë°°ÈöêÁßÅÂíåÂÆûÁî®ÊÄß„ÄÇËôΩÁÑ∂Âº∫Â§ßÁöÑÊ∑∑Ê∑ÜÊäÄÊúØÂèØ‰ª•ÊúâÊïàÂú∞ÈöêËóè‰ΩúËÄÖÁöÑË∫´‰ªΩÔºå‰ΩÜÂÆÉ‰ª¨ÈÄöÂ∏∏‰ºöÈôç‰ΩéÊñáÊú¨ÁöÑË¥®ÈáèÂíåÂÆûÁî®ÊÄßÔºå‰ΩøÂÖ∂Êó†Ê≥ïËææÂà∞È¢ÑÊúüÁõÆÁöÑ„ÄÇÁõ∏ÂèçÔºåÁª¥ÊåÅÈ´òÂÆûÁî®ÊÄßÂæÄÂæÄ‰ºöÊèê‰æõ‰∏çË∂≥ÁöÑÈöêÁßÅÔºå‰ΩøÂæóÂØπÊâãÊõ¥ÂÆπÊòìÂØπ‰ΩúËÄÖËøõË°åÂéªÂåøÂêçÂåñ„ÄÇÂõ†Ê≠§ÔºåÂú®‰∏§‰∏™Áõ∏‰∫íÂÜ≤Á™ÅÁöÑÁõÆÊ†á‰πãÈó¥ÂèñÂæóÊúÄ‰Ω≥Âπ≥Ë°°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü TAROTÔºöÈù¢Âêë‰ªªÂä°ÁöÑ‰ΩúËÄÖÊ∑∑Ê∑ÜÔºå‰ΩøÁî®Á≠ñÁï•‰ºòÂåñÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÁöÑÊó†ÁõëÁù£‰ΩúËÄÖÊ∑∑Ê∑ÜÊñπÊ≥ïÔºåÂÖ∂ÁõÆÊ†áÊòØÈÄöËøáÈáçÊñ∞ÁîüÊàêËÄÉËôëÂÖ∂‰∏ãÊ∏∏ÂÆûÁî®ÊÄßÁöÑÊï¥‰∏™ÊñáÊú¨Êù•‰ºòÂåñÈöêÁßÅÂÆûÁî®ÊÄßÊùÉË°°„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Á≠ñÁï•‰ºòÂåñ‰Ωú‰∏∫Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã‰∏äÁöÑÂæÆË∞ÉËåÉÂºèÔºå‰ª•‰æøÈÄöËøá‰øùÁïô‰ΩúËÄÖË∫´‰ªΩÂíå‰∏ãÊ∏∏‰ªªÂä°ÂÆûÁî®ÊÄßÊù•ÈáçÂÜôÊñáÊú¨„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰øùÁïôÂÆûÁî®ÊÄßÁöÑÂêåÊó∂Â§ßÂ§ßÈôç‰Ωé‰∫ÜÊîªÂáªËÄÖÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Â∞ÜÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°ÂûãÂÖ¨ÂºÄÊèê‰æõ„ÄÇ

##### **Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**
2407.21615v1 by Pedro Sarmento, Jackson Loth, Mathieu Barthet

Generative AI models have recently blossomed, significantly impacting
artistic and musical traditions. Research investigating how humans interact
with and deem these models is therefore crucial. Through a listening and
reflection study, we explore participants' perspectives on AI- vs
human-generated progressive metal, in symbolic format, using rock music as a
control group. AI-generated examples were produced by ProgGP, a
Transformer-based model. We propose a mixed methods approach to assess the
effects of generation type (human vs. AI), genre (progressive metal vs. rock),
and curation process (random vs. cherry-picked). This combines quantitative
feedback on genre congruence, preference, creativity, consistency, playability,
humanness, and repeatability, and qualitative feedback to provide insights into
listeners' experiences. A total of 32 progressive metal fans completed the
study. Our findings validate the use of fine-tuning to achieve genre-specific
specialization in AI music generation, as listeners could distinguish between
AI-generated rock and progressive metal. Despite some AI-generated excerpts
receiving similar ratings to human music, listeners exhibited a preference for
human compositions. Thematic analysis identified key features for genre and AI
vs. human distinctions. Finally, we consider the ethical implications of our
work in promoting musical data diversity within MIR research by focusing on an
under-explored genre.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI Ê®°ÂûãËøëÊúüËì¨ÂãÉÁôºÂ±ïÔºåÂ∞çËóùË°ìÂíåÈü≥Ê®ÇÂÇ≥Áµ±Áî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇÂõ†Ê≠§ÔºåÊé¢Ë®é‰∫∫È°ûÂ¶Ç‰ΩïËàáÈÄô‰∫õÊ®°Âûã‰∫íÂãï‰∏¶Ë©ïÊñ∑ÂÆÉÂÄëÁöÑÁ†îÁ©∂Ëá≥ÈóúÈáçË¶Å„ÄÇÈÄèÈÅéËÅÜËÅΩËàáÂèçÊÄùÁ†îÁ©∂ÔºåÊàëÂÄë‰ª•Á¨¶ËôüÊ†ºÂºèÊé¢Ë®éÂèÉËàáËÄÖÂ∞ç AI Ëàá‰∫∫È°ûÁî¢ÁîüÁöÑÂâçË°õÈáëÂ±¨Èü≥Ê®ÇÁöÑËßÄÈªûÔºå‰∏¶‰ª•ÊêñÊªæÈü≥Ê®Ç‰ΩúÁÇ∫Â∞çÁÖßÁµÑ„ÄÇAI ÁîüÊàêÁöÑÁØÑ‰æãÁî± ProgGPÔºå‰∏ÄÂÄãÂü∫Êñº Transformer ÁöÑÊ®°ÂûãË£Ω‰Ωú„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÊñπÊ≥ï‰æÜË©ï‰º∞ÁîüÊàêÈ°ûÂûãÔºà‰∫∫È°ûËàá AIÔºâ„ÄÅÈ°ûÂûãÔºàÂâçË°õÈáëÂ±¨ËàáÊêñÊªæÔºâÂíåÁ≠ñÂ±ïÊµÅÁ®ãÔºàÈö®Ê©üËàáÁ≤æÈÅ∏ÔºâÁöÑÂΩ±Èüø„ÄÇÈÄôÁµêÂêà‰∫ÜÂ∞çÈ°ûÂûã‰∏ÄËá¥ÊÄß„ÄÅÂÅèÂ•Ω„ÄÅÂâµÈÄ†Âäõ„ÄÅ‰∏ÄËá¥ÊÄß„ÄÅÂèØÊºîÂ•èÊÄß„ÄÅ‰∫∫ÊÄßÂåñÂíåÂèØÈáçË§áÊÄßÁöÑÈáèÂåñÂõûÈ•ãÔºå‰ª•ÂèäÂÆöÊÄßÂõûÈ•ãÔºå‰ª•Êèê‰æõÂ∞çËÅΩÁúæÈ´îÈ©óÁöÑË¶ãËß£„ÄÇÂÖ±Êúâ 32 ‰ΩçÂâçË°õÈáëÂ±¨Ê®ÇËø∑ÂÆåÊàêÈÄôÈ†ÖÁ†îÁ©∂„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈ©óË≠â‰∫ÜÂæÆË™øÂú® AI Èü≥Ê®ÇÁîüÊàê‰∏≠ÂØ¶ÁèæÁâπÂÆöÈ°ûÂûãÂ∞àÊ•≠ÂåñÁöÑÁî®ÈÄîÔºåÂõ†ÁÇ∫ËÅΩÁúæÂèØ‰ª•ÂçÄÂàÜ AI ÁîüÊàêÁöÑÊêñÊªæÂíåÂâçË°õÈáëÂ±¨„ÄÇÂÑòÁÆ°‰∏Ä‰∫õ AI ÁîüÊàêÁöÑÁâáÊÆµÁç≤ÂæóËàá‰∫∫È°ûÈü≥Ê®ÇÈ°û‰ººÁöÑË©ïÂàÜÔºå‰ΩÜËÅΩÁúæË°®ÁèæÂá∫Â∞ç‰∫∫È°û‰ΩúÂìÅÁöÑÂÅèÂ•Ω„ÄÇ‰∏ªÈ°åÂàÜÊûêË≠òÂà•Âá∫È°ûÂûãÂíå AI Ëàá‰∫∫È°ûÂçÄÂà•ÁöÑ‰∏ªË¶ÅÁâπÂæµ„ÄÇÊúÄÂæåÔºåÊàëÂÄëËÄÉÊÖÆ‰∫ÜÊàëÂÄëÁöÑÂ∑•‰ΩúÂú®‰øÉÈÄ≤ MIR Á†îÁ©∂‰∏≠ÁöÑÈü≥Ê®ÇË≥áÊñôÂ§öÊ®£ÊÄßÊñπÈù¢ÁöÑÂÄ´ÁêÜÂΩ±ÈüøÔºåÈáçÈªûÊîæÂú®‰∏ÄÂÄãÊé¢Á¥¢‰∏çË∂≥ÁöÑÈ°ûÂûã‰∏ä„ÄÇ

##### **Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**
2407.21611v1 by Jiafeng Zhong, Bin Li, Jiangyan Yi

The task of partially spoofed audio localization aims to accurately determine
audio authenticity at a frame level. Although some works have achieved
encouraging results, utilizing boundary information within a single model
remains an unexplored research topic. In this work, we propose a novel method
called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of
two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The
former assembles the intra-frame and inter-frame information to extract
discriminative boundary features that are subsequently used for boundary
position detection and authenticity decision, while the latter leverages
boundary prediction results to explicitly control the feature interaction
between frames, which achieves effective discrimination between real and fake
frames. Experimental results on PartialSpoof database demonstrate our proposed
method achieves the best performance. The code is available at
https://github.com/media-sec-lab/BAM.

ÊëòË¶ÅÔºöÈÉ®ÂàÜÊ¨∫È™óÈü≥È¢ëÂÆö‰ΩçÁöÑ‰ªªÂä°Êó®Âú®ÂáÜÁ°ÆÁ°ÆÂÆöÂ∏ßÁ∫ßÂà´ÁöÑÈü≥È¢ëÁúüÂÆûÊÄß„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰ΩúÂìÅÂèñÂæó‰∫Ü‰ª§‰∫∫ÈºìËàûÁöÑÁªìÊûúÔºå‰ΩÜÂú®Âçï‰∏™Ê®°Âûã‰∏≠Âà©Áî®ËæπÁïå‰ø°ÊÅØ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Â∞öÊú™Êé¢Á¥¢ÁöÑÁ†îÁ©∂ËØæÈ¢ò„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËæπÁïåÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ (BAM) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöËæπÁïåÂ¢ûÂº∫ÂíåËæπÁïåÈÄêÂ∏ßÊ≥®ÊÑèÂäõ„ÄÇÂâçËÄÖÊ±áÈõÜÂ∏ßÂÜÖÂíåÂ∏ßÈó¥‰ø°ÊÅØ‰ª•ÊèêÂèñÂà§Âà´ËæπÁïåÁâπÂæÅÔºåÈöèÂêéÁî®‰∫éËæπÁïå‰ΩçÁΩÆÊ£ÄÊµãÂíåÁúüÂÆûÊÄßÂÜ≥Á≠ñÔºåËÄåÂêéËÄÖÂà©Áî®ËæπÁïåÈ¢ÑÊµãÁªìÊûúÊòæÂºèÊéßÂà∂Â∏ß‰πãÈó¥ÁöÑÁâπÂæÅ‰∫§‰∫íÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂØπÁúüÂÆûÂ∏ßÂíåËôöÂÅáÂ∏ßÁöÑÊúâÊïàÂå∫ÂàÜ„ÄÇPartialSpoof Êï∞ÊçÆÂ∫ì‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/media-sec-lab/BAM Ëé∑Âæó„ÄÇ

##### **Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**
2407.21600v1 by Shoujin Huang, Guanxiong Luo, Yuwan Wang, Kexin Yang, Lingyan Zhang, Jingzhe Liu, Hua Guo, Min Wang, Mengye Lyu

Simultaneous multislice (SMS) imaging is a powerful technique for
accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS
reconstruction remains challenging due to the complex signal interactions
between and within the excited slices. This study presents a robust SMS MRI
reconstruction method using deep generative priors. Starting from Gaussian
noise, we leverage denoising diffusion probabilistic models (DDPM) to gradually
recover the individual slices through reverse diffusion iterations while
imposing data consistency from the measured k-space under readout concatenation
framework. The posterior sampling procedure is designed such that the DDPM
training can be performed on single-slice images without special adjustments
for SMS tasks. Additionally, our method integrates a low-frequency enhancement
(LFE) module to address a practical issue that SMS-accelerated fast spin echo
(FSE) and echo-planar imaging (EPI) sequences cannot easily embed
autocalibration signals. Extensive experiments demonstrate that our approach
consistently outperforms existing methods and generalizes well to unseen
datasets. The code is available at https://github.com/Solor-pikachu/ROGER after
the review process.

ÊëòË¶ÅÔºöÂêåÊôÇÂ§öÂàáÁâá (SMS) ÂΩ±ÂÉèÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊäÄË°ìÔºåÁî®ÊñºÂä†ÈÄüÁ£ÅÊåØÈÄ†ÂΩ± (MRI) ÁöÑÊì∑Âèñ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊøÄÁôºÂàáÁâá‰πãÈñìÂíå‰πãÂÖßÁöÑË§áÈõúË®äËôü‰∫§‰∫í‰ΩúÁî®ÔºåSMS ÈáçÂª∫‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ê∑±Â∫¶ÁîüÊàêÂÖàÈ©óÁöÑÂº∑ÂÅ• SMS MRI ÈáçÂª∫ÊñπÊ≥ï„ÄÇÂæûÈ´òÊñØÈõúË®äÈñãÂßãÔºåÊàëÂÄëÂà©Áî®ÂéªÂô™Êì¥Êï£Ê©üÁéáÊ®°Âûã (DDPM) ÈÄèÈÅéÂèçÂêëÊì¥Êï£ÂèçË¶ÜÈÅãÁÆó‰æÜÈÄêÊº∏ÊÅ¢Âæ©ÂÄãÂà•ÂàáÁâáÔºåÂêåÊôÇÂú®ËÆÄÂèñ‰∏≤Êé•Êû∂Êßã‰∏ãÊñΩÂä†Ê∏¨Èáè k Á©∫ÈñìÁöÑË≥áÊñô‰∏ÄËá¥ÊÄß„ÄÇÂæåÈ©óÊäΩÊ®£Á®ãÂ∫èÁöÑË®≠Ë®à‰ΩøÂæó DDPM Ë®ìÁ∑¥ÂèØ‰ª•Âú®ÂñÆÂàáÁâáÂΩ±ÂÉè‰∏äÂü∑Ë°åÔºåÁÑ°ÈúÄÈáùÂ∞ç SMS ‰ªªÂãôÈÄ≤Ë°åÁâπÊÆäË™øÊï¥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫Ü‰∏ÄÂÄã‰ΩéÈ†ªÂ¢ûÂº∑ (LFE) Ê®°ÁµÑÔºå‰ª•Ëß£Ê±∫‰∏ÄÂÄãÂØ¶ÈöõÂïèÈ°åÔºåÂç≥ SMS Âä†ÈÄüÁöÑÂø´ÈÄüËá™ÊóãÂõûÊ≥¢ (FSE) ÂíåÂõûÊ≥¢Âπ≥Èù¢ÂΩ±ÂÉè (EPI) Â∫èÂàóÁÑ°Ê≥ïËºïÈ¨ÜÂµåÂÖ•Ëá™ÂãïÊ†°Ê≠£Ë®äËôü„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂßãÁµÇÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÔºå‰∏¶‰∏îÂèØ‰ª•ÂæàÂ•ΩÂú∞Êé®Âª£Âà∞Êú™Ë¶ãÁöÑË≥áÊñôÈõÜ„ÄÇÁ®ãÂºèÁ¢ºÂú®ÂØ©Êü•Á®ãÂ∫èÂæåÂèØÊñº https://github.com/Solor-pikachu/ROGER ÂèñÂæó„ÄÇ

##### **Voxel Scene Graph for Intracranial Hemorrhage**
2407.21580v1 by Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay

Patients with Intracranial Hemorrhage (ICH) face a potentially
life-threatening condition, and patient-centered individualized treatment
remains challenging due to possible clinical complications. Deep-Learning-based
methods can efficiently analyze the routinely acquired head CTs to support the
clinical decision-making. The majority of early work focuses on the detection
and segmentation of ICH, but do not model the complex relations between ICH and
adjacent brain structures. In this work, we design a tailored object detection
method for ICH, which we unite with segmentation-grounded Scene Graph
Generation (SGG) methods to learn a holistic representation of the clinical
cerebral scene. To the best of our knowledge, this is the first application of
SGG for 3D voxel images. We evaluate our method on two head-CT datasets and
demonstrate that our model can recall up to 74% of clinically relevant
relations. This work lays the foundation towards SGG for 3D voxel data. The
generated Scene Graphs can already provide insights for the clinician, but are
also valuable for all downstream tasks as a compact and interpretable
representation.

ÊëòË¶ÅÔºöËÖ¶Âá∫Ë°Ä (ICH) ÊÇ£ËÄÖÈù¢Ëá®ÂèØËÉΩÂç±ÂèäÁîüÂëΩÁöÑÁãÄÊ≥ÅÔºåÁî±ÊñºÂèØËÉΩÁöÑËá®Â∫ä‰ΩµÁôºÁóáÔºå‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÂÄã‰∫∫ÂåñÊ≤ªÁôÇ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂàÜÊûêÂ∏∏Ë¶èÁç≤ÂæóÁöÑÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºå‰ª•ÊîØÊåÅËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÂ§ßÂ§öÊï∏Êó©ÊúüÂ∑•‰ΩúÈÉΩÈõÜ‰∏≠Âú® ICH ÁöÑÊ™¢Ê∏¨ÂíåÂàÜÂâ≤Ôºå‰ΩÜÊ≤íÊúâÂ∞ç ICH ÂíåÁõ∏ÈÑ∞Â§ßËÖ¶ÁµêÊßã‰πãÈñìÁöÑË§áÈõúÈóú‰øÇÈÄ≤Ë°åÂª∫Ê®°„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÈáùÂ∞ç ICH ÁöÑÂÆ¢Ë£ΩÂåñÁõÆÊ®ôÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÊàëÂÄëÂ∞áÂÖ∂ËàáÂü∫ÊñºÂàÜÂâ≤ÁöÑÂ†¥ÊôØÂúñÁîüÊàê (SGG) ÊñπÊ≥ïÁµêÂêàÔºå‰ª•Â≠∏ÁøíËá®Â∫äËÖ¶ÈÉ®Â†¥ÊôØÁöÑÊï¥È´îË°®Âæµ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØ SGG Á¨¨‰∏ÄÊ¨°ÊáâÁî®Êñº 3D È´îÁ¥†ÂΩ±ÂÉè„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÊï∏ÊìöÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºå‰∏¶Ë≠âÊòéÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Âè¨ÂõûÈ´òÈÅî 74% ÁöÑËá®Â∫äÁõ∏ÈóúÈóú‰øÇ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ 3D È´îÁ¥†Êï∏ÊìöÁöÑ SGG Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇÁîüÊàêÁöÑÂ†¥ÊôØÂúñÂ∑≤Á∂ìÂèØ‰ª•ÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõË¶ãËß£Ôºå‰ΩÜÂ∞çÊñºÊâÄÊúâ‰∏ãÊ∏∏‰ªªÂãôËÄåË®ÄÔºåÂÆÉ‰πüÊòØ‰∏ÄÁ®ÆÁ≤æÁ∞°‰∏îÂèØËß£ÈáãÁöÑË°®ÂæµÔºåÂõ†Ê≠§ÈùûÂ∏∏ÊúâÂÉπÂÄº„ÄÇ

##### **A Performance Study of LLM-Generated Code on Leetcode**
2407.21579v1 by Tristan Coignion, Cl√©ment Quinton, Romain Rouvoy

This study evaluates the efficiency of code generation by Large Language
Models (LLMs) and measures their performance against human-crafted solutions
using a dataset from Leetcode. We compare 18 LLMs, considering factors such as
model temperature and success rate, and their impact on code performance. This
research introduces a novel method for measuring and comparing the speed of
LLM-generated code, revealing that LLMs produce code with comparable
performance, irrespective of the adopted LLM. We also find that LLMs are
capable of generating code that is, on average, more efficient than the code
written by humans. The paper further discusses the use of Leetcode as a
benchmarking dataset, the limitations imposed by potential data contamination,
and the platform's measurement reliability. We believe that our findings
contribute to a better understanding of LLM capabilities in code generation and
set the stage for future optimizations in the field.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ®ãÂºèÁ¢ºÁî¢ÁîüÊïàÁéáÔºå‰∏¶‰ΩøÁî®‰æÜËá™ Leetcode ÁöÑË≥áÊñôÈõÜË°°ÈáèÂÆÉÂÄëËàá‰∫∫Â∑•Ë£Ω‰ΩúÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü 18 ÂÄã LLMÔºåËÄÉÈáè‰∫ÜÊ®°ÂûãÊ∫´Â∫¶ÂíåÊàêÂäüÁéáÁ≠âÂõ†Á¥†Ôºå‰ª•ÂèäÂÆÉÂÄëÂ∞çÁ®ãÂºèÁ¢ºÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ï‰æÜË°°ÈáèÂíåÊØîËºÉ LLM ÁîüÊàêÁöÑÁ®ãÂºèÁ¢ºÁöÑÈÄüÂ∫¶ÔºåÊè≠Á§∫‰∫Ü LLM Áî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÂÖ∑ÊúâÂèØÊØîËºÉÁöÑÊïàËÉΩÔºåËÄåËàáÊé°Áî®ÁöÑ LLM ÁÑ°Èóú„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåLLM ËÉΩÂ§†Áî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÂπ≥ÂùáËÄåË®ÄÊØî‰∫∫Â∑•Á∑®ÂØ´ÁöÑÁ®ãÂºèÁ¢ºÊõ¥ÊúâÊïàÁéá„ÄÇÊú¨ÊñáÈÄ≤‰∏ÄÊ≠•Ë®éË´ñ‰∫Ü‰ΩøÁî® Leetcode ‰ΩúÁÇ∫Âü∫Ê∫ñË≥áÊñôÈõÜ„ÄÅÊΩõÂú®Ë≥áÊñôÊ±°ÊüìÊâÄÈÄ†ÊàêÁöÑÈôêÂà∂Ôºå‰ª•ÂèäË©≤Âπ≥Âè∞ÁöÑÊ∏¨ÈáèÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁõ∏‰ø°ÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊúâÂä©ÊñºÊõ¥‰∫ÜËß£ LLM Âú®Á®ãÂºèÁ¢ºÁî¢ÁîüÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÊú™‰æÜÊúÄ‰Ω≥ÂåñÂ•†ÂÆöÂü∫Á§é„ÄÇ

##### **Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**
2407.21577v1 by Kit M. Bransby, Woo-jin Cho Kim, Jorge Oliveira, Alex Thorley, Arian Beqiri, Alberto Gomez, Agisilaos Chartsias

Building an echocardiography view classifier that maintains performance in
real-life cases requires diverse multi-site data, and frequent updates with
newly available data to mitigate model drift. Simply fine-tuning on new
datasets results in "catastrophic forgetting", and cannot adapt to variations
of view labels between sites. Alternatively, collecting all data on a single
server and re-training may not be feasible as data sharing agreements may
restrict image transfer, or datasets may only become available at different
times. Furthermore, time and cost associated with re-training grows with every
new dataset. We propose a class-incremental learning method which learns an
expert network for each dataset, and combines all expert networks with a score
fusion model. The influence of ``unqualified experts'' is minimised by
weighting each contribution with a learnt in-distribution score. These weights
promote transparency as the contribution of each expert is known during
inference. Instead of using the original images, we use learned features from
each dataset, which are easier to share and raise fewer licensing and privacy
concerns. We validate our work on six datasets from multiple sites,
demonstrating significant reductions in training time while improving view
classification performance.

ÊëòË¶ÅÔºöÂª∫Á´ã‰∏ÄÂÄãÂú®ÂØ¶ÈöõÊÉÖÊ≥Å‰∏≠ËÉΩÁ∂≠ÊåÅÊïàËÉΩÁöÑÂøÉËáüË∂ÖÈü≥Ê≥¢Ê™¢Ë¶ñÂàÜÈ°ûÂô®ÔºåÈúÄË¶ÅÂ§öÊ®£ÂåñÁöÑÂ§öÁ´ôÈªûË≥áÊñôÔºå‰ª•ÂèäÈ†ªÁπÅ‰ΩøÁî®Êñ∞ÂèñÂæóÁöÑË≥áÊñôÈÄ≤Ë°åÊõ¥Êñ∞Ôºå‰ª•Ê∏õËºïÊ®°ÂûãÊºÇÁßª„ÄÇÂñÆÁ¥îÈáùÂ∞çÊñ∞ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÂæÆË™øÊúÉÂ∞éËá¥„ÄåÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄçÔºå‰∏îÁÑ°Ê≥ïÈÅ©ÊáâÁ´ôÈªû‰πãÈñìË¶ñÂúñÊ®ôÁ±§ÁöÑÂ∑ÆÁï∞„ÄÇÊàñËÄÖÔºåÂú®ÂñÆ‰∏Ä‰º∫ÊúçÂô®‰∏äÊî∂ÈõÜÊâÄÊúâË≥áÊñô‰∏¶ÈáçÊñ∞Ë®ìÁ∑¥ÂèØËÉΩ‰∏çÂèØË°åÔºåÂõ†ÁÇ∫Ë≥áÊñôÂàÜ‰∫´ÂçîË≠∞ÂèØËÉΩÊúÉÈôêÂà∂ÂΩ±ÂÉèÂÇ≥Ëº∏ÔºåÊàñË≥áÊñôÈõÜÂèØËÉΩÂè™Âú®‰∏çÂêåÁöÑÊôÇÈñìÈªûÂèñÂæó„ÄÇÊ≠§Â§ñÔºåÊØèÊ¨°‰ΩøÁî®Êñ∞ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÈáçÊñ∞Ë®ìÁ∑¥ÊâÄËä±Ë≤ªÁöÑÊôÇÈñìÂíåÊàêÊú¨ÈÉΩÊúÉÂ¢ûÂä†„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈ°ûÂà•ÈÅûÂ¢ûÂ≠∏ÁøíÊñπÊ≥ïÔºåÊúÉÁÇ∫ÊØèÂÄãË≥áÊñôÈõÜÂ≠∏Áøí‰∏ÄÂÄãÂ∞àÂÆ∂Á∂≤Ë∑ØÔºå‰∏¶Â∞áÊâÄÊúâÂ∞àÂÆ∂Á∂≤Ë∑ØËàá‰∏ÄÂÄãÂàÜÊï∏ËûçÂêàÊ®°ÂûãÁµêÂêà„ÄÇÈÄèÈÅé‰ΩøÁî®Â≠∏ÁøíÂà∞ÁöÑÂàÜ‰ΩàÂÖßÂàÜÊï∏Â∞çÊØèÂÄãË≤¢ÁçªÈÄ≤Ë°åÂä†Ê¨äÔºåÂèØ‰ª•Â∞á„Äå‰∏çÂêàÊ†ºÂ∞àÂÆ∂„ÄçÁöÑÂΩ±ÈüøÈôçÂà∞ÊúÄ‰Ωé„ÄÇÈÄô‰∫õÊ¨äÈáçÊúÉÊèêÂçáÈÄèÊòéÂ∫¶ÔºåÂõ†ÁÇ∫Âú®Êé®Ë´ñÈÅéÁ®ã‰∏≠ÊúÉÁü•ÈÅìÊØèÂÄãÂ∞àÂÆ∂ÁöÑË≤¢Áçª„ÄÇÊàëÂÄë‰∏ç‰ΩøÁî®ÂéüÂßãÂΩ±ÂÉèÔºåËÄåÊòØ‰ΩøÁî®ÂæûÊØèÂÄãË≥áÊñôÈõÜ‰∏≠Â≠∏ÁøíÂà∞ÁöÑÁâπÂæµÔºåÈÄô‰∫õÁâπÂæµÊõ¥ÂÆπÊòìÂàÜ‰∫´Ôºå‰∏îÊúÉÂºïÁôºËºÉÂ∞ëÁöÑÊéàÊ¨äÂíåÈö±ÁßÅÂïèÈ°å„ÄÇÊàëÂÄëÂú®Â§öÂÄãÁ´ôÈªûÁöÑÂÖ≠ÂÄãË≥áÊñôÈõÜ‰∏äÈ©óË≠âÊàëÂÄëÁöÑÊàêÊûúÔºåË≠âÊòéÂú®ÊîπÂñÑË¶ñÂúñÂàÜÈ°ûÊïàËÉΩÁöÑÂêåÊôÇÔºåÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜË®ìÁ∑¥ÊôÇÈñì„ÄÇ

##### **PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**
2407.21571v1 by Min Jae Jung, JooHee Kim

Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊåÅÁ∫åÂ≠∏Áøí‰∏≠ÊúÉÈÅ≠ÈÅáÈáçÂ§ßÊåëÊà∞ÔºåÂõ†ÁÇ∫ÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÊúÉÂ∞éËá¥Êñ∞Ë≥áË®äË¶ÜËìãÂÖàÂâçÁç≤ÂæóÁöÑÁü•Ë≠ò„ÄÇÊ≠§ÈôêÂà∂ÊúÉÈÄ†ÊàêÂ§ßÈáèÁöÑÁí∞Â¢ÉÂíåÁ∂ìÊøüÊµ™Ë≤ª„ÄÇÊú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü PMoEÔºå‰∏ÄÁ®ÆÈùûÂ∞çÁ®±TransformerÁöÑÂ∞àÂÆ∂Êº∏ÈÄ≤Ê∑∑ÂêàÔºåÂÖ∂ÁõÆÊ®ôÊòØÈÄèÈÅé‰ΩøÁî®ÈùûÂ∞çÁ®±Ë®≠Ë®à‰æÜÊúÄÂ∞èÂåñÈÅ∫ÂøòÔºåÂÖ∂‰∏≠Ê∑∫Â±§Â∞àÈñÄÁî®Êñº‰∏ÄËà¨Áü•Ë≠òÔºåËÄåÊ∑±Â±§ÂâáÁî®ÊñºÊñ∞Áü•Ë≠ò„ÄÇPMoE Âú®Ê∑±Â±§‰∏≠Âä†ÂÖ•Êº∏ÈÄ≤Êñ∞Â¢ûÁöÑÂ∞àÂÆ∂Ôºå‰ª•Âèä‰∏ÄÂÄãË∑ØÁî±Âô®ÔºåÂèØÊúâÊïàÂú∞Â∞áÊñ∞Áü•Ë≠òÂàÜÈÖçÁµ¶ÈÅ©Áï∂ÁöÑÂ∞àÂÆ∂„ÄÇË∑ØÁî±Âô®ËàáÊ∑±Â±§Áõ∏ÈÑ∞Ôºå‰ΩøÁî®Êï¥ÂêàÂæåÁöÑË≥áË®äÂΩôÁ∏ΩÊ∑±Â∫¶ÁâπÂæµ„ÄÇÈÄôËÆìË∑ØÁî±Âô®Âæó‰ª•ÊúâÊïàÂü∑Ë°åÔºåÂ∞áÊñ∞Áü•Ë≠òÂàÜÈÖçÁµ¶ÈÅ©Áï∂ÁöÑÂ∞àÂÆ∂ÔºåËÄåÈÄô‰∫õÂ∞àÂÆ∂ÊúÉÂú®Ê∑±Â±§‰∏≠ÈÄêÊº∏Â¢ûÂä†„ÄÇÂú® TRACE Ë≥áÊñôÈõÜÂíå‰∏ÄËà¨Ë™ûË®ÄÁêÜËß£Ë≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÊâÄÊèêÂá∫ÁöÑ PMoE ÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇ

##### **Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**
2407.21560v1 by Jun Zhou, Dongyang Yu, Kamran Aziz, Fangfang Su, Qing Zhang, Fei Li, Donghong Ji

Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.

ÊëòË¶ÅÔºöÁ¥∞Á≤íÂ∫¶ÊÉÖÁ∑íÂàÜÊûêÊ∂âÂèäÂæûÊñáÊú¨Ë≥áÊñô‰∏≠ËêÉÂèñÂíåÁµÑÁπîÊÉÖÁ∑íÂÖÉÁ¥†„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÁ∂ìÂ∏∏ÂøΩÁï•È°ûÂà•Ë™ûÁæ©ÂåÖÂê´ÂíåÈáçÁñäÁöÑÂïèÈ°åÔºå‰ª•ÂèäÁõÆÊ®ôÂ∫èÂàóÂÖßÂú®ÁöÑÁµêÊßãÊ®°Âºè„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∏ÄÂÄãÁîüÊàêÂºèÊÉÖÁ∑íÂàÜÊûêÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çËàáÈ°ûÂà•Ë™ûÁæ©ÂåÖÂê´ÂíåÈáçÁñäÁõ∏ÈóúÁöÑÊåëÊà∞ÔºåÂºïÂÖ•‰∫ÜÊΩõÂú®È°ûÂà•ÂàÜ‰ΩàËÆäÊï∏„ÄÇÈÄèÈÅéÈáçÂª∫ËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô®ÁöÑËº∏ÂÖ•ÔºåÊ®°ÂûãÂ≠∏ÁøíÈ°ûÂà•ËàáÊñáÂ≠ó‰πãÈñìÈóú‰øÇÁöÑÂº∑Â∫¶ÔºåÂæûËÄåÊîπÂñÑÂ∫èÂàóÁîüÊàê„ÄÇÊ≠§Â§ñÔºåÂà©Áî®Ê®πÁãÄÁµêÊßãÂíåÁ¥ÑÊùüÂºèËß£Á¢ºÁ≠ñÁï•‰æÜÂà©Áî®ÁµêÊßãÊ®°ÂºèÔºåÈÄ≤ËÄåÊ∏õÂ∞ëÊêúÂ∞ãÁ©∫Èñì‰∏¶Ë¶èÁØÑÁîüÊàêÈÅéÁ®ã„ÄÇÂú® Restaurant-ACOS Âíå Laptop-ACOS Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéËàáÂü∫Á∑öÊ®°ÂûãÁõ∏ÊØîÊúâÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇÊ∂àËûçÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Á¢∫Ë™ç‰∫ÜÊΩõÂú®È°ûÂà•ÂàÜ‰ΩàÂíåÁ¥ÑÊùüÂºèËß£Á¢ºÁ≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Operator-based semantics for choice programs: is choosing losing? (full version)**
2407.21556v1 by Jesse Heyninck

Choice constructs are an important part of the language of logic programming,
yet the study of their semantics has been a challenging task. So far, only
two-valued semantics have been studied, and the different proposals for such
semantics have not been compared in a principled way. In this paper, an
operator-based framework allow for the definition and comparison of different
semantics in a principled way is proposed.

ÊëòË¶ÅÔºöÈÅ∏ÊìáÂª∫ÊßãÊòØÈÇèËºØÁ®ãÂºèË®≠Ë®àË™ûË®Ä‰∏≠ÈáçË¶ÅÁöÑÈÉ®ÂàÜÔºå
ÁÑ∂ËÄåÁ†îÁ©∂ÂÖ∂Ë™ûÊÑèÂ≠∏‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÁõÆÂâçÁÇ∫Ê≠¢Ôºå
ÂÉÖÁ†îÁ©∂‰∫Ü‰∫åÂÄºË™ûÊÑèÂ≠∏Ôºå‰∏îÂ∞çÊñºÊ≠§È°ûË™ûÊÑèÂ≠∏ÁöÑ‰∏çÂêåÊèêÊ°àÂ∞öÊú™‰ª•ÊúâÂéüÂâáÁöÑÊñπÂºèÈÄ≤Ë°åÊØîËºÉ„ÄÇÊú¨Êñá‰∏≠Ôºå
ÊèêÂá∫‰∫ÜÂü∫ÊñºÈÅãÁÆóÂ≠êÁöÑÊû∂ÊßãÔºåÂÖÅË®±‰ª•ÊúâÂéüÂâáÁöÑÊñπÂºèÂÆöÁæ©ÂíåÊØîËºÉ‰∏çÂêåÁöÑË™ûÊÑèÂ≠∏„ÄÇ

##### **Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**
2407.21536v1 by Jiang Li, Xiaoping Wang, Zhigang Zeng

Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂØπËØùÊÉÖÊÑüËØÜÂà´ÔºàMERCÔºâÊúÄËøëÂºïËµ∑‰∫ÜÂ§ßÈáèÁ†îÁ©∂ÂÖ≥Ê≥®„ÄÇÁé∞ÊúâÁöÑ MERC ÊñπÊ≥ïÈù¢‰∏¥ÁùÄ‰∏Ä‰∫õÊåëÊàòÔºöÔºà1ÔºâÂÆÉ‰ª¨Êú™ËÉΩÂÖÖÂàÜÂà©Áî®Áõ¥Êé•ÁöÑÊ®°ÊÄÅÈó¥Á∫øÁ¥¢ÔºåÂèØËÉΩÂØºËá¥‰∏çÂ§™ÂΩªÂ∫ïÁöÑË∑®Ê®°ÊÄÅÂª∫Ê®°ÔºõÔºà2ÔºâÂÆÉ‰ª¨Âú®ÊØè‰∏™ÁΩëÁªúÂ±ÇÂêåÊó∂‰ªéÁõ∏ÂêåÂíå‰∏çÂêåÁöÑÊ®°ÊÄÅ‰∏≠ÊèêÂèñ‰ø°ÊÅØÔºåÂèØËÉΩÂºïÂèëÊù•Ëá™Â§öÊ∫êÊï∞ÊçÆËûçÂêàÁöÑÂÜ≤Á™ÅÔºõÔºà3ÔºâÂÆÉ‰ª¨Áº∫‰πèÊ£ÄÊµãÂä®ÊÄÅÊÉÖÊÑüÂèòÂåñÊâÄÈúÄÁöÑÊïèÊç∑ÊÄßÔºåÂèØËÉΩÂØºËá¥ÂØπÊÉÖÊÑüÊÄ•ÂâßËΩ¨ÂèòÁöÑËØùËØ≠ËøõË°å‰∏çÂáÜÁ°ÆÁöÑÂàÜÁ±ª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ GraphSmile ÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éËøΩË∏™Â§öÊ®°ÊÄÅÂØπËØù‰∏≠ÁöÑÂ§çÊùÇÊÉÖÊÑüÁ∫øÁ¥¢„ÄÇGraphSmile ÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåÂç≥ GSF Âíå SDP Ê®°Âùó„ÄÇGSF Â∑ßÂ¶ôÂú∞Âà©Áî®ÂõæÁªìÊûÑÈÄêÂ±Ç‰∫§ÊõøÂêåÂåñÊ®°ÊÄÅÈó¥ÂíåÊ®°ÊÄÅÂÜÖÊÉÖÊÑü‰æùËµñÂÖ≥Á≥ªÔºåÂÖÖÂàÜÊçïÊçâË∑®Ê®°ÊÄÅÁ∫øÁ¥¢ÔºåÂêåÊó∂ÊúâÊïàËßÑÈÅøËûçÂêàÂÜ≤Á™Å„ÄÇSDP ÊòØ‰∏ÄÈ°πËæÖÂä©‰ªªÂä°ÔºåÁî®‰∫éÊòéÁ°ÆÊèèÁªòËØùËØ≠‰πãÈó¥ÁöÑÊÉÖÊÑüÂä®ÊÄÅÔºåÊèêÂçáÊ®°ÂûãÂå∫ÂàÜÊÉÖÊÑüÂ∑ÆÂºÇÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåGraphSmile ÂèØ‰ª•ÊØ´‰∏çË¥πÂäõÂú∞Â∫îÁî®‰∫éÂØπËØù‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÊÉÖÊÑüÂàÜÊûêÔºàMSACÔºâÔºåÊâìÈÄ†‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÊÉÖÊÑüÊ®°ÂûãÔºåËÉΩÂ§üÊâßË°å MERC Âíå MSAC ‰ªªÂä°„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÁöÑÂÆûËØÅÁªìÊûúË°®ÊòéÔºåGraphSmile ÂèØ‰ª•Â§ÑÁêÜÂ§çÊùÇÁöÑÊÉÖÊÑüÂíåÊÉÖÊÑüÊ®°ÂºèÔºåÊòéÊòæ‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ

##### **Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**
2407.21531v1 by Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu Wang, Emmanouil Benetos, Wei Xue, Yike Guo

Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.

ÊëòË¶ÅÔºöÁ¨¶ËôüÈü≥Ê®ÇÈ°û‰ººÊñºË™ûË®ÄÔºåÂèØ‰ª•Áî®Èõ¢Êï£Á¨¶ËôüÁ∑®Á¢º„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç GPT-4 Âíå Llama2ÔºåÁöÑÊáâÁî®Êì¥Â±ïÂà∞Á¨¶ËôüÈü≥Ê®ÇÈ†òÂüüÔºåÂåÖÊã¨ÁêÜËß£ÂíåÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÂæàÂ∞ëÊúâÁ†îÁ©∂Êé¢Ë®éÈÄô‰∫õ LLM Â¶Ç‰ΩïÂü∑Ë°åÈÄ≤ÈöéÈü≥Ê®ÇÁêÜËß£ÂíåÊ¢ù‰ª∂ÁîüÊàêÔºåÁâπÂà•ÊòØÂæûÂ§öÊ≠•È©üÊé®ÁêÜÁöÑËßíÂ∫¶‰æÜÁúãÔºåÈÄôÊòØÊ¢ù‰ª∂Âºè„ÄÅÂèØÁ∑®ËºØÂíå‰∫íÂãïÂºè‰∫∫Ê©üÂÖ±ÂêåÂâµ‰ΩúÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÈù¢Âêë„ÄÇÊú¨Á†îÁ©∂Â∞ç LLM Âú®Á¨¶ËôüÈü≥Ê®ÇËôïÁêÜ‰∏≠ÁöÑËÉΩÂäõÂíåÈôêÂà∂ÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑË™øÊü•„ÄÇÊàëÂÄëÁôºÁèæÔºåÁõÆÂâçÁöÑ LLM Âú®Ê≠åÊõ≤Â±§Á¥öÁöÑÂ§öÊ≠•È©üÈü≥Ê®ÇÊé®ÁêÜ‰∏≠Ë°®Áèæ‰∏ç‰Ω≥Ôºå‰∏¶‰∏îÂú®ËôïÁêÜË§áÈõúÁöÑÈü≥Ê®Ç‰ªªÂãôÊôÇÈÄöÂ∏∏ÁÑ°Ê≥ïÂà©Áî®ÊâÄÂ≠∏ÁöÑÈü≥Ê®ÇÁü•Ë≠ò„ÄÇÂ∞ç LLM ÂõûÊáâÁöÑÂàÜÊûêÁ™ÅÂá∫‰∫ÜÂÆÉÂÄëÁöÑÂÑ™Áº∫Èªû„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåLLM Êú¨Ë≥™‰∏ä‰∏¶Êú™Áç≤ÂæóÈÄ≤ÈöéÁöÑÈü≥Ê®ÇËÉΩÂäõÔºåÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÊõ¥Â∞àÊ≥®ÊñºÂΩåÂêàÈü≥Ê®ÇÁü•Ë≠òÂíåÊé®ÁêÜ‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰ª•ÊîπÂñÑÈü≥Ê®ÇÂÆ∂ÁöÑÂÖ±ÂêåÂâµ‰ΩúÈ´îÈ©ó„ÄÇ

##### **Data Contamination Report from the 2024 CONDA Shared Task**
2407.21530v1 by Oscar Sainz, Iker Garc√≠a-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang

The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.

ÊëòË¶ÅÔºöÁ¨¨‰∏ÄÂ±ÜË≥áÊñôÊ±°ÊüìÂ∑•‰ΩúÂùäÔºàCONDA 2024ÔºâÂ∞àÊ≥®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠Ë≥áÊñôÊ±°ÊüìÁöÑÊâÄÊúâÁõ∏ÈóúÈù¢ÂêëÔºåÂÖ∂‰∏≠Ë≥áÊñôÊ±°ÊüìË¢´ÁêÜËß£ÁÇ∫Ë©ïÈáèË≥áÊñôÂåÖÂê´Âú®Áî®ÊñºË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥Ë™ûÊñôÂ∫´‰∏≠ÔºåÈÄ≤ËÄåÂΩ±ÈüøË©ïÈáèÁµêÊûúÁöÑÊÉÖÊ≥Å„ÄÇÂ∑•‰ΩúÂùä‰øÉÈÄ≤‰∫Ü‰∏ÄÈ†ÖÂÖ±Âêå‰ªªÂãôÔºå‰ª•Êî∂ÈõÜË≠âÊìöÔºå‰∫ÜËß£Áï∂ÂâçÂèØÁî®Ë≥áÊñôÈõÜÂíåÊ®°Âûã‰∏≠ÁöÑË≥áÊñôÊ±°Êüì„ÄÇÂÖ±Âêå‰ªªÂãôÂíåÁõ∏ÈóúË≥áÊñôÂ∫´ÁöÑÁõÆÊ®ôÊòØÂçîÂä©Á§æÁæ§‰∫ÜËß£ÂïèÈ°åÁöÑÁ®ãÂ∫¶Ôºå‰∏¶ÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÈÅøÂÖçÂõûÂ†±Â∑≤Áü•ÂèóÊ±°ÊüìË≥áÊ∫êÁöÑË©ïÈáèÁµêÊûú„ÄÇÂÖ±Âêå‰ªªÂãôÊèê‰æõ‰∏ÄÂÄãÁµêÊßãÂåñ„ÄÅÈõÜ‰∏≠ÁöÑÂÖ¨ÈñãË≥áÊñôÂ∫´ÔºåÁî®ÊñºÊî∂ÈõÜÊ±°ÊüìË≠âÊìöÔºå‰∏¶ÈñãÊîæÁ§æÁæ§ÈÄèÈÅé GitHub Ê±†Ë´ãÊ±ÇÊèê‰æõË≤¢Áçª„ÄÇÈÄô‰ªΩÁ¨¨‰∏Ä‰ªΩÂΩôÁ∑®Ë´ñÊñáÊòØÊ†πÊìö‰æÜËá™ 23 ‰ΩçË≤¢ÁçªËÄÖÁöÑ 91 ÂÄãÂèóÊ±°Êüì‰æÜÊ∫êÊâÄÂõûÂ†±ÁöÑ 566 ÂÄãÊ¢ùÁõÆ„ÄÇÂÄãÂà•Ê±°Êüì‰∫ã‰ª∂ÁöÑË©≥Á¥∞Ë≥áË®äÂèØÂú®Âπ≥Âè∞‰∏äÂèñÂæó„ÄÇÂπ≥Âè∞ÊåÅÁ∫å‰∏äÁ∑öÔºåÈñãÊîæÁ§æÁæ§Êèê‰æõË≤¢Áçª„ÄÇ

##### **Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**
2407.21523v1 by Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, Gang Chen

Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant
high-quality tabular data for model training remains a significant obstacle.
Numerous works have focused on tabular data augmentation (TDA) to enhance the
original table with additional data, thereby improving downstream ML tasks.
Recently, there has been a growing interest in leveraging the capabilities of
generative AI for TDA. Therefore, we believe it is time to provide a
comprehensive review of the progress and future prospects of TDA, with a
particular emphasis on the trending generative AI. Specifically, we present an
architectural view of the TDA pipeline, comprising three main procedures:
pre-augmentation, augmentation, and post-augmentation. Pre-augmentation
encompasses preparation tasks that facilitate subsequent TDA, including error
handling, table annotation, table simplification, table representation, table
indexing, table navigation, schema matching, and entity matching. Augmentation
systematically analyzes current TDA methods, categorized into retrieval-based
methods, which retrieve external data, and generation-based methods, which
generate synthetic data. We further subdivide these methods based on the
granularity of the augmentation process at the row, column, cell, and table
levels. Post-augmentation focuses on the datasets, evaluation and optimization
aspects of TDA. We also summarize current trends and future directions for TDA,
highlighting promising opportunities in the era of generative AI. In addition,
the accompanying papers and related resources are continuously updated and
maintained in the GitHub repository at
https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect
ongoing advancements in the field.

ÊëòË¶ÅÔºöË°®Ê†ºË≥áÊñôÁöÑÊ©üÂô®Â≠∏Áøí (ML) ÁÑ°ÊâÄ‰∏çÂú®Ôºå‰ΩÜÂèñÂæóÂ§ßÈáèÁöÑÂÑ™Ë≥™Ë°®Ê†ºË≥áÊñô‰ª•ÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÈöúÁ§ô„ÄÇË®±Â§öÁ†îÁ©∂Â∞àÊ≥®ÊñºË°®Ê†ºË≥áÊñôÊì¥ÂÖÖ (TDA)Ôºå‰ª•‰ΩøÁî®È°çÂ§ñÁöÑË≥áÊñô‰æÜÂ¢ûÂº∑ÂéüÂßãË°®Ê†ºÔºåÂæûËÄåÊîπÂñÑ‰∏ãÊ∏∏ ML ‰ªªÂãô„ÄÇÊúÄËøëÔºå‰∫∫ÂÄëÂ∞çÂà©Áî®ÁîüÊàêÂºè AI ÁöÑÂäüËÉΩÈÄ≤Ë°å TDA Áî¢ÁîüË∂ä‰æÜË∂äÂ§ßÁöÑËààË∂£„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁõ∏‰ø°ÁèæÂú®ÊòØÊôÇÂÄôÂ∞ç TDA ÁöÑÈÄ≤Â±ïÂíåÊú™‰æÜÂâçÊôØÈÄ≤Ë°åÂÖ®Èù¢ÂõûÈ°ßÔºåÁâπÂà•Âº∑Ë™øË∂®Âã¢ÊÄßÁöÑÁîüÊàêÂºè AI„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü TDA ÁÆ°Á∑öÁöÑÊû∂ÊßãË¶ñÂúñÔºåÂåÖÂê´‰∏âÂÄã‰∏ªË¶ÅÁ®ãÂ∫èÔºöÈ†êÊì¥ÂÖÖ„ÄÅÊì¥ÂÖÖÂíåÂæåÊì¥ÂÖÖ„ÄÇÈ†êÊì¥ÂÖÖÂåÖÂê´ÊúâÂä©ÊñºÂæåÁ∫å TDA ÁöÑÊ∫ñÂÇô‰ªªÂãôÔºåÂåÖÊã¨ÈåØË™§ËôïÁêÜ„ÄÅË°®Ê†ºË®ªËß£„ÄÅË°®Ê†ºÁ∞°Âåñ„ÄÅË°®Ê†ºË°®Á§∫„ÄÅË°®Ê†ºÁ¥¢Âºï„ÄÅË°®Ê†ºÂ∞éË¶Ω„ÄÅÊû∂ÊßãÊØîÂ∞çÂíåÂØ¶È´îÊØîÂ∞ç„ÄÇÊì¥ÂÖÖÁ≥ªÁµ±ÊÄßÂú∞ÂàÜÊûêÁõÆÂâçÁöÑ TDA ÊñπÊ≥ïÔºåÂàÜÈ°ûÁÇ∫Âü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ïÔºàÊ™¢Á¥¢Â§ñÈÉ®Ë≥áÊñôÔºâÂíåÂü∫ÊñºÁîüÊàêÁöÑÁöÑÊñπÊ≥ïÔºàÁîüÊàêÂêàÊàêË≥áÊñôÔºâ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ê†πÊìöÂàó„ÄÅÊ¨Ñ„ÄÅÂÑ≤Â≠òÊ†ºÂíåË°®Ê†ºÂ±§Á¥öÁöÑÊì¥ÂÖÖÁ®ãÂ∫èÁ≤íÂ∫¶Á¥∞ÂàÜÈÄô‰∫õÊñπÊ≥ï„ÄÇÂæåÊì¥ÂÖÖÂ∞àÊ≥®Êñº TDA ÁöÑË≥áÊñôÈõÜ„ÄÅË©ï‰º∞ÂíåÊúÄ‰Ω≥ÂåñÈù¢Âêë„ÄÇÊàëÂÄë‰πüÁ∏ΩÁµê‰∫Ü TDA ÁõÆÂâçÁöÑË∂®Âã¢ÂíåÊú™‰æÜÊñπÂêëÔºåÂº∑Ë™ø‰∫ÜÁîüÊàêÂºè AI ÊôÇ‰ª£ÂÖÖÊªøÂ∏åÊúõÁöÑÊ©üÊúÉ„ÄÇÊ≠§Â§ñÔºåÈö®ÈôÑÁöÑË´ñÊñáÂíåÁõ∏ÈóúË≥áÊ∫êÊúÉÊåÅÁ∫åÊõ¥Êñ∞‰∏¶‰øùÂ≠òÂú® GitHub ÂÑ≤Â≠òÂ∫´‰∏≠ÔºåÁ∂≤ÂùÄÁÇ∫ https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentationÔºå‰ª•ÂèçÊò†Ë©≤È†òÂüüÁöÑÊåÅÁ∫åÈÄ≤Â±ï„ÄÇ

##### **Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**
2407.21516v1 by I. M. Chernenkiy, Y. A. Drach, S. R. Mustakimova, V. V. Kazantseva, N. A. Ushakov, S. K. Efetov, M. V. Feldsherov

Colorectal cancer is the third-most common cancer in the Western Hemisphere.
The segmentation of colorectal and colorectal cancer by computed tomography is
an urgent problem in medicine. Indeed, a system capable of solving this problem
will enable the detection of colorectal cancer at early stages of the disease,
facilitate the search for pathology by the radiologist, and significantly
accelerate the process of diagnosing the disease. However, scientific
publications on medical image processing mostly use closed, non-public data.
This paper presents an extension of the Medical Decathlon dataset with
colorectal markups in order to improve the quality of segmentation algorithms.
An experienced radiologist validated the data, categorized it into subsets by
quality, and published it in the public domain. Based on the obtained results,
we trained neural network models of the UNet architecture with 5-part
cross-validation and achieved a Dice metric quality of $0.6988 \pm 0.3$. The
published markups will improve the quality of colorectal cancer detection and
simplify the radiologist's job for study description.

ÊëòË¶ÅÔºöÂ§ßËÖ∏ÁôåÊòØË•øÂçäÁêÉÁ¨¨‰∏âÂ∏∏Ë¶ãÁöÑÁôåÁóá„ÄÇ
Âà©Áî®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂ∞çÂ§ßËÖ∏ÁôåËàáÂ§ßËÖ∏ÁôåÈÄ≤Ë°åÂàÜÊÆµÊòØÈÜ´Â≠∏‰∏äÁöÑÁ∑äÊÄ•ÂïèÈ°å„ÄÇ‰∫ãÂØ¶‰∏äÔºå‰∏ÄÂÄãËÉΩÂ§†Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÁöÑÁ≥ªÁµ±Â∞áËÉΩÂ§†Âú®ÁñæÁóÖÁöÑÊó©ÊúüÈöéÊÆµÂÅµÊ∏¨Â§ßËÖ∏ÁôåÔºåÂçîÂä©ÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∞ãÊâæÁóÖÁêÜÔºå‰∏¶È°ØËëóÂä†ÈÄüË®∫Êñ∑ÁñæÁóÖÁöÑÈÅéÁ®ã„ÄÇÁÑ∂ËÄåÔºåÈóúÊñºÈÜ´Â≠∏ÂΩ±ÂÉèËôïÁêÜÁöÑÁßëÂ≠∏ÂàäÁâ©Â§ßÂ§ö‰ΩøÁî®Â∞ÅÈñâ„ÄÅÈùûÂÖ¨ÈñãÁöÑË≥áÊñô„ÄÇÈÄôÁØáË´ñÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∏∂ÊúâÂ§ßËÖ∏Ê®ôË®òÁöÑÈÜ´Â≠∏ÂçÅÈ†ÖÂÖ®ËÉΩË≥áÊñôÈõÜÁöÑÂª∂‰º∏Ôºå‰ª•ÊèêÈ´òÂàÜÊÆµÊºîÁÆóÊ≥ïÁöÑÂìÅË≥™„ÄÇ‰∏Ä‰ΩçÁ∂ìÈ©óË±êÂØåÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´È©óË≠â‰∫ÜË≥áÊñôÔºåÂ∞áÂÖ∂‰æùÂìÅË≥™ÂàÜÈ°ûÊàêÂ≠êÈõÜÔºå‰∏¶Â∞áÂÖ∂ÁôºÂ∏ÉÂú®ÂÖ¨ÂÖ±È†òÂüü„ÄÇÊ†πÊìöÁç≤ÂæóÁöÑÁµêÊûúÔºåÊàëÂÄëË®ìÁ∑¥‰∫ÜÂÖ∑Êúâ 5 ÈÉ®ÂàÜ‰∫§ÂèâÈ©óË≠âÁöÑ UNet Êû∂ÊßãÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºå‰∏¶ÈÅîÂà∞‰∫Ü $0.6988 \pm 0.3$ ÁöÑ Dice ÊåáÊ®ôÂìÅË≥™„ÄÇÁôºÂ∏ÉÁöÑÊ®ôË®òÂ∞áÊèêÈ´òÂ§ßËÖ∏ÁôåÂÅµÊ∏¨ÁöÑÂìÅË≥™Ôºå‰∏¶Á∞°ÂåñÊîæÂ∞ÑÁßëÈÜ´Â∏´Á†îÁ©∂ÊèèËø∞ÁöÑÂ∑•‰Ωú„ÄÇ

##### **Interpreting and learning voice commands with a Large Language Model for a robot system**
2407.21512v1 by Stanislau Stankevich, Wojciech Dudek

Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.

ÊëòË¶ÅÔºöÊ©üÂô®‰∫∫Ë∂ä‰æÜË∂äÊôÆÈÅçÊñºÂ∑•Ê•≠ÂíåÊó•Â∏∏ÁîüÊ¥ª‰∏≠Ôºå‰æãÂ¶ÇÂú®Ë≠∑ÁêÜ‰πãÂÆ∂ÔºåÂÆÉÂÄëÂèØ‰ª•ÂçîÂä©Âì°Â∑•„ÄÇ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞ÊòØÈñãÁôºÁõ¥Ë¶∫Âºè‰ªãÈù¢Ôºå‰ª•‰æøËºïÈ¨ÜÊ∫ùÈÄö„ÄÇ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç GPT-4ÔºåÂ¢ûÂº∑‰∫ÜÊ©üÂô®‰∫∫ÁöÑËÉΩÂäõÔºåÂÖÅË®±Âç≥ÊôÇ‰∫íÂãïÂíåÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊ≠§Êï¥ÂêàÊîπÂñÑ‰∫ÜÊ©üÂô®‰∫∫ÁöÑÈÅ©ÊáâÊÄßÂíåÂäüËÉΩÊÄß„ÄÇÊ≠§Â∞àÊ°àÂ∞àÊ≥®ÊñºÂ∞á LLM ËàáË≥áÊñôÂ∫´Âêà‰ΩµÔºå‰ª•ÊîπÂñÑÊ±∫Á≠ñÂà∂ÂÆöÔºå‰∏¶ËÆìÁü•Ë≠òÊì∑ÂèñÂèØÁî®ÊñºË´ãÊ±ÇË©ÆÈáãÂïèÈ°å„ÄÇ

##### **FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**
2407.21507v1 by Yuna Yan, Xin Zhang, Lixin Li, Wensheng Lin, Rui Li, Wenchi Cheng, Zhu Han

In this paper, we address the problem of image semantic communication in a
multi-user deployment scenario and propose a federated learning (FL) strategy
for a Swin Transformer-based semantic communication system (FSSC). Firstly, we
demonstrate that the adoption of a Swin Transformer for joint source-channel
coding (JSCC) effectively extracts semantic information in the communication
system. Next, the FL framework is introduced to collaboratively learn a global
model by aggregating local model parameters, rather than directly sharing
clients' data. This approach enhances user privacy protection and reduces the
workload on the server or mobile edge. Simulation evaluations indicate that our
method outperforms the typical JSCC algorithm and traditional separate-based
communication algorithms. Particularly after integrating local semantics, the
global aggregation model has further increased the Peak Signal-to-Noise Ratio
(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Êé¢ËÆ®‰∫ÜÂ§öÁî®Êà∑ÈÉ®ÁΩ≤Âú∫ÊôØ‰∏≠ÁöÑÂõæÂÉèËØ≠‰πâÈÄö‰ø°ÈóÆÈ¢òÔºåÂπ∂ÈíàÂØπÂü∫‰∫é Swin Transformer ÁöÑËØ≠‰πâÈÄö‰ø°Á≥ªÁªü (FSSC) ÊèêÂá∫‰∫Ü‰∏ÄÁßçËÅîÈÇ¶Â≠¶‰π† (FL) Á≠ñÁï•„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÈááÁî® Swin Transformer ËøõË°åËÅîÂêàÊ∫ê‰ø°ÈÅìÁºñÁ†Å (JSCC) ÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÂèñÈÄö‰ø°Á≥ªÁªü‰∏≠ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇÊé•‰∏ãÊù•ÔºåÂºïÂÖ• FL Ê°ÜÊû∂ÔºåÈÄöËøáËÅöÂêàÊú¨Âú∞Ê®°ÂûãÂèÇÊï∞Êù•Âçè‰ΩúÂ≠¶‰π†ÂÖ®Â±ÄÊ®°ÂûãÔºåËÄå‰∏çÊòØÁõ¥Êé•ÂÖ±‰∫´ÂÆ¢Êà∑Á´ØÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜÁî®Êà∑ÈöêÁßÅ‰øùÊä§ÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÊúçÂä°Âô®ÊàñÁßªÂä®ËæπÁºòÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇ‰ªøÁúüËØÑ‰º∞Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ºò‰∫éÂÖ∏ÂûãÁöÑ JSCC ÁÆóÊ≥ïÂíå‰º†ÁªüÁöÑÂü∫‰∫éÂàÜÁ¶ªÁöÑÈÄö‰ø°ÁÆóÊ≥ï„ÄÇÁâπÂà´ÊòØÂú®Êï¥Âêà‰∫ÜÂ±ÄÈÉ®ËØ≠‰πâ‰πãÂêéÔºåÂÖ®Â±ÄËÅöÂêàÊ®°ÂûãËøõ‰∏ÄÊ≠•Â∞ÜÂ≥∞ÂÄº‰ø°Âô™ÊØî (PSNR) ÊèêÈ´ò‰∫Ü 2dB ‰ª•‰∏äÔºåÂÖÖÂàÜËØÅÊòé‰∫ÜÊàë‰ª¨ÁÆóÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **MaskUno: Switch-Split Block For Enhancing Instance Segmentation**
2407.21498v1 by Jawad Haidar, Marc Mouawad, Imad Elhajj, Daniel Asmar

Instance segmentation is an advanced form of image segmentation which, beyond
traditional segmentation, requires identifying individual instances of
repeating objects in a scene. Mask R-CNN is the most common architecture for
instance segmentation, and improvements to this architecture include steps such
as benefiting from bounding box refinements, adding semantics, or backbone
enhancements. In all the proposed variations to date, the problem of competing
kernels (each class aims to maximize its own accuracy) persists when models try
to synchronously learn numerous classes. In this paper, we propose mitigating
this problem by replacing mask prediction with a Switch-Split block that
processes refined ROIs, classifies them, and assigns them to specialized mask
predictors. We name the method MaskUno and test it on various models from the
literature, which are then trained on multiple classes using the benchmark COCO
dataset. An increase in the mean Average Precision (mAP) of 2.03% was observed
for the high-performing DetectoRS when trained on 80 classes. MaskUno proved to
enhance the mAP of instance segmentation models regardless of the number and
typ

ÊëòË¶ÅÔºöÂØ¶‰æãÂàÜÂâ≤ÊòØÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÈÄ≤ÈöéÂΩ¢ÂºèÔºåÈô§‰∫ÜÂÇ≥Áµ±ÂàÜÂâ≤‰πãÂ§ñÔºåÈÇÑÈúÄË¶ÅË≠òÂà•Â†¥ÊôØ‰∏≠ÈáçË§áÁâ©È´îÁöÑÂÄãÂà•ÂØ¶‰æã„ÄÇMask R-CNN ÊòØÂØ¶‰æãÂàÜÂâ≤ÊúÄÂ∏∏Ë¶ãÁöÑÊû∂ÊßãÔºåËÄåÂ∞çÊ≠§Êû∂ÊßãÁöÑÊîπÈÄ≤ÂåÖÊã¨ÂæûÈÇäÁïåÊ°ÜÂÑ™Âåñ„ÄÅÂä†ÂÖ•Ë™ûÊÑèÊàñ‰∏ªÂππÂ¢ûÂº∑Á≠âÊ≠•È©ü„ÄÇÂú®ËøÑ‰ªäÁÇ∫Ê≠¢ÊèêÂá∫ÁöÑÊâÄÊúâËÆäÈ´î‰∏≠ÔºåÁï∂Ê®°ÂûãÂòóË©¶ÂêåÊ≠•Â≠∏ÁøíÂ§öÂÄãÈ°ûÂà•ÊôÇÔºåÁ´∂Áà≠Ê†∏ÂøÉÁöÑÂïèÈ°åÔºàÊØèÂÄãÈ°ûÂà•ÈÉΩÊó®Âú®ÊúÄÂ§ßÂåñÂÖ∂Ëá™Ë∫´Ê∫ñÁ¢∫Â∫¶Ôºâ‰ªçÁÑ∂Â≠òÂú®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÈÄèÈÅéÂ∞áÈÅÆÁΩ©È†êÊ∏¨ÊõøÊèõÁÇ∫ËôïÁêÜÁ≤æÁ∑ª ROI„ÄÅÂ∞çÂÖ∂ÂàÜÈ°û‰∏¶Â∞áÂÖ∂ÂàÜÈÖçÁµ¶Â∞àÈñÄÈÅÆÁΩ©È†êÊ∏¨Âô®ÁöÑÈñãÈóúÂàÜÂâ≤ÂçÄÂ°ä‰æÜÊ∏õËºïÊ≠§ÂïèÈ°å„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÂëΩÂêçÁÇ∫ MaskUnoÔºå‰∏¶Âú®‰æÜËá™ÊñáÁçªÁöÑÂêÑÁ®ÆÊ®°Âûã‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÁÑ∂Âæå‰ΩøÁî®Âü∫Ê∫ñ COCO Ë≥áÊñôÈõÜÂ∞çÂÖ∂ÈÄ≤Ë°åÂ§öÂÄãÈ°ûÂà•ÁöÑË®ìÁ∑¥„ÄÇÂú® 80 ÂÄãÈ°ûÂà•‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÈ´òÊÄßËÉΩ DetectoRS ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶ (mAP) ËßÄÂØüÂà∞Â¢ûÂä†‰∫Ü 2.03%„ÄÇÁÑ°Ë´ñÈ°ûÂà•Êï∏ÈáèÂíå

##### **Generative Expressive Conversational Speech Synthesis**
2407.21491v2 by Rui Liu, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li

Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.

ÊëòË¶ÅÔºöÂ∞çË©±ÂºèË™ûÈü≥ÂêàÊàê (CSS) Êó®Âú®‰ª•ÈÅ©Áï∂ÁöÑË™™Ë©±È¢®Ê†ºÂú®‰ΩøÁî®ËÄÖ‰ª£ÁêÜÂ∞çË©±Ë®≠ÂÆö‰∏≠Ë°®ÈÅîÁõÆÊ®ôË©±Ë™û„ÄÇÁèæÊúâÁöÑ CSS ÊñπÊ≥ïÊé°Áî®ÊúâÊïàÁöÑÂ§öÊ®°ÂºèË™ûÂ¢ÉÂª∫Ê®°ÊäÄË°ì‰æÜÈÅîÊàêÂêåÁêÜÁêÜËß£ÂíåË°®ÈÅî„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄëÁ∂ìÂ∏∏ÈúÄË¶ÅË®≠Ë®àË§áÈõúÁöÑÁ∂≤Ë∑ØÊû∂Êßã‰∏¶Á≤æÂøÉÊúÄ‰Ω≥ÂåñÂÖ∂‰∏≠ÁöÑÊ®°ÁµÑ„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÂåÖÂê´ËÖ≥Êú¨Ë®òÈåÑÈ¢®Ê†ºÁöÑÂ∞èË¶èÊ®°Ë≥áÊñôÈõÜÁöÑÈôêÂà∂Ôºå‰ªñÂÄëÁ∂ìÂ∏∏ÁÑ°Ê≥ïÊ®°Êì¨ÁúüÂØ¶Ëá™ÁÑ∂ÁöÑÂ∞çË©±È¢®Ê†º„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÁîüÊàêÂºèË°®ÈÅîÊÄß CSS Á≥ªÁµ±ÔºåÁ®±ÁÇ∫ GPT-Talker„ÄÇÊàëÂÄëÂ∞áÂ§öËº™Â∞çË©±Ê≠∑Âè≤ÁöÑÂ§öÊ®°ÂºèË≥áË®äËΩâÊèõÁÇ∫Èõ¢Êï£ÁöÑÁ¨¶ËôüÂ∫èÂàóÔºå‰∏¶Â∞áÂÆÉÂÄëÁÑ°Á∏´Êï¥Âêà‰ª•ÂΩ¢ÊàêÂÖ®Èù¢ÁöÑ‰ΩøÁî®ËÄÖ‰ª£ÁêÜÂ∞çË©±Ë™ûÂ¢É„ÄÇÂà©Áî® GPT ÁöÑÂäüËÉΩÔºåÊàëÂÄëÈ†êÊ∏¨ÂõûÊáâÁöÑÁ¨¶ËôüÂ∫èÂàóÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë™ûÁæ©ÂíåÈ¢®Ê†ºÁü•Ë≠òÔºå‰ª•‰æõ‰ª£ÁêÜ‰ΩøÁî®„ÄÇÂú®ÈÇ£‰πãÂæåÔºåÁî±Â∞çË©±Ë±êÂØåÁöÑ VITS ÂêàÊàêÂØåÊúâË°®ÁèæÂäõÁöÑÂ∞çË©±ÂºèË™ûÈü≥Ôºå‰ª•Âêë‰ΩøÁî®ËÄÖÊèê‰æõÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ NCSSD ÁöÑÂ§ßÂûãËá™ÁÑ∂ CSS Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰ª•Âç≥ËààÈ¢®Ê†ºËá™ÁÑ∂Ë®òÈåÑÁöÑÂ∞çË©±ÂºèË™ûÈü≥ÂíåÂæûÈõªË¶ñÁØÄÁõÆ‰∏≠Êì∑ÂèñÁöÑÂ∞çË©±„ÄÇÂÆÉÂåÖÂê´‰∏≠ÊñáÂíåËã±ÊñáÔºåÁ∏ΩÊôÇÈï∑ÁÇ∫ 236 Â∞èÊôÇ„ÄÇÊàëÂÄëÂ∞ç NCSSD ÁöÑÂèØÈù†ÊÄßÂíåÊàëÂÄë GPT-Talker ÁöÑÊúâÊïàÊÄßÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©ó„ÄÇ‰∏ªËßÄÂíåÂÆ¢ËßÄË©ï‰º∞ÈÉΩË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Ëá™ÁÑ∂ÊÄßÂíåË°®ÁèæÂäõÊñπÈù¢È°ØËëóÂÑ™ÊñºÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑ CSS Á≥ªÁµ±„ÄÇÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÈõÜÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/AI-S2-Lab/GPT-Talker„ÄÇ

##### **Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**
2407.21490v1 by Junxuan Yu, Rusi Chen, Yongsong Zhou, Yanlin Chen, Yaofei Duan, Yuhao Huang, Han Zhou, Tan Tao, Xin Yang, Dong Ni

Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÊòØË®∫Êñ∑ÂøÉËáüÁñæÁóÖÁöÑ‰∏ªË¶ÅÊñπÂºèÔºå
‰ΩÜÊúâÈôêÁöÑÊï∏ÊìöÂ∞çËá®Â∫äÊïôÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíË®ìÁ∑¥ÈÉΩÊßãÊàêÊåëÊà∞„ÄÇÊúÄËøëÔºåÂΩ±ÁâáÁîüÊàêÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Á∑©Ëß£Ê≠§ÂïèÈ°åÁöÑ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑËæ¶Ê≥ïÂú®ÁîüÊàêÈÅéÁ®ã‰∏≠ÈÄöÂ∏∏‰æùË≥¥Êï¥È´îÊ¢ù‰ª∂ÔºåÈòªÁ§ô‰∫ÜÂ∞çÁâπÂÆöÂøÉËáüÁµêÊßãÁöÑÈùàÊ¥ªÈÅãÂãïÊéßÂà∂„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂèØËß£Èáã‰∏îÂèØÊéßÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÁîüÊàêÊñπÊ≥ïÔºå‰ª•ÂàùÂßãÂπÄÂíåÈÅãÂãïÊõ≤Á∑ö‰ΩúÁÇ∫ÊåáÂ∞é„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÊúâ‰∏âÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæûÊØèÂÄãÂøÉËáüÂ≠êÁµêÊßã‰∏≠ÊèêÂèñÈÅãÂãïË≥áË®ä‰ª•Âª∫ÊßãÈÅãÂãïÊõ≤Á∑öÔºåËÆìÊì¥Êï£Ê®°ÂûãËÉΩÂ§†ÈÄèÈÅé‰øÆÊîπÈÄô‰∫õÊõ≤Á∑ö‰æÜÂêàÊàêÂÆ¢Ë£ΩÂåñÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±Áâá„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁµêÊßãÂà∞ÈÅãÂãïÂ∞çÈΩäÊ®°ÁµÑÔºåÂÆÉÂèØ‰ª•Â∞áË™ûÁæ©ÁâπÂæµÂ∞çÊáâÂà∞ÂøÉËáüÁµêÊßã‰∏≠ÁöÑÈÅãÂãïÊõ≤Á∑ö„ÄÇÁ¨¨‰∏âÔºå‰ΩçÁΩÆÊÑüÁü•Ê≥®ÊÑèÂäõÊ©üÂà∂Êó®Âú®Âà©Áî®ÂÖ∑ÊúâÁµêÊßã‰ΩçÁΩÆË≥áË®äÁöÑÈ´òÊñØÈÅÆÁΩ©‰æÜÂ¢ûÂº∑ÂΩ±ÁâáÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂú®‰∏âÂÄãË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑËæ¶Ê≥ïÂú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñËæ¶Ê≥ï„ÄÇÂÆåÊï¥Á®ãÂºèÁ¢ºÂ∞áÂú® https://github.com/mlmi-2024-72/ECM ‰∏äÈáãÂá∫„ÄÇ

##### **Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**
2407.21489v1 by Giuliano Martinelli, Edoardo Barba, Roberto Navigli

Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.

ÊëòË¶ÅÔºöÂ§ßÂûãËá™ÂõûÂΩíÁîüÊàêÊ®°ÂûãÂ∑≤Êàê‰∏∫Âú®Â§öÈ°πËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞ÊúÄÈ´òÊÄßËÉΩÁöÑÂü∫Áü≥„ÄÇÁÑ∂ËÄåÔºåËøΩÊ±ÇÂçìË∂äÊàêÊûúÁöÑÂÜ≤Âä®ÊúâÊó∂‰ºöÂØºËá¥Âú®Ê≤°ÊúâËØ¶Â∞ΩÂÆûÈ™åÁöÑÊÉÖÂÜµ‰∏ãËøáÊó©ÊõøÊç¢Á≤æÂøÉËÆæËÆ°ÁöÑÁâπÂÆö‰ªªÂä°ÊñπÊ≥ï„ÄÇÂÖ±ÊåáÊ∂àËß£‰ªªÂä°‰πü‰∏ç‰æãÂ§ñÔºõÊâÄÊúâÊúÄËøëÁöÑÊúÄÊñ∞Ëß£ÂÜ≥ÊñπÊ°àÈÉΩÈááÁî®Â§ßÂûãÁîüÊàêËá™ÂõûÂΩíÊ®°ÂûãÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÂà§Âà´Á≥ªÁªü„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáÂºïÂÖ• Maverick Êù•ÊåëÊàòËøô‰∏ÄÊúÄÊñ∞Ë∂ãÂäøÔºåMaverick ÊòØ‰∏ÄÊ¨æÁ≤æÂøÉËÆæËÆ°‰ΩÜÁÆÄÂçïÁöÑÁÆ°ÈÅìÔºåÂÆÉÂèØ‰ª•Âú®Â≠¶ÊúØÈ¢ÑÁÆóÁöÑÈôêÂà∂ÂÜÖËøêË°åÊúÄÂÖàËøõÁöÑÂÖ±ÊåáÊ∂àËß£Á≥ªÁªüÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂ§öËææ 130 ‰∫øÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåËÄåÂèÇÊï∞Âç¥Â∞ëËá≥ 5 ‰∫ø„ÄÇMaverick Âú® CoNLL-2012 Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÂÜÖÂ≠òËµÑÊ∫êÊúÄÂ§ö‰∏∫ 0.006 ÂÄçÔºåÂπ∂‰∏î‰∏é‰πãÂâçÁöÑÊúÄÂÖàËøõÁ≥ªÁªüÁõ∏ÊØîÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü 170 ÂÄç„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÁ≥ªÂàó‰∏çÂêåÁöÑÂÆûÈ™åÂπøÊ≥õÈ™åËØÅ‰∫Ü Maverick Ê°ÜÊû∂ÁöÑÈ≤ÅÊ£íÊÄßÔºåÊä•Âëä‰∫ÜÂú®Êï∞ÊçÆÁ®ÄÁº∫„ÄÅÈïøÊñáÊ°£ÂíåÂüüÂ§ñËÆæÁΩÆ‰∏≠ÂØπÂÖàÂâçÁ≥ªÁªüÁöÑÊîπËøõ„ÄÇÊàë‰ª¨Âú® https://github.com/SapienzaNLP/maverick-coref ‰∏äÂèëÂ∏ÉÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°Âûã‰ª•Áî®‰∫éÁ†îÁ©∂ÁõÆÁöÑ„ÄÇ

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v2 by Xiny Pan, Daniel Hern√°ndez, Philipp Seifer, Ralf L√§mmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥ÔºåÊàëÂÄëÂ∑≤Á∂ìÁúãÂà∞Â§ßÂûãÁü•Ë≠òÂúñË≠úÁöÑÂá∫ÁèæÔºåÁµêÂêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑË≥áË®ä„ÄÇÊúâÊôÇÔºåÊ≠§Ë≥áË®ä‰ª•Â∞çÂÖ∂‰ªñÊñ∑Ë®ÄÁöÑÊñ∑Ë®ÄÂΩ¢ÂºèÊèê‰æõÔºåÂÆöÁæ©Êñ∑Ë®ÄÊúâÊïàÁöÑËÑàÁµ°„ÄÇRDF ÁöÑÊúÄÊñ∞Êì¥ÂÖÖÂÖÅË®±Â∞çÊñ∑Ë®ÄÈÄ≤Ë°åÈô≥Ëø∞ÔºåÁ®±ÁÇ∫ RDF-starÔºåÁõÆÂâçÊ≠£Âú®‰øÆË®ÇÁÇ∫ W3C Ê®ôÊ∫ñ„ÄÇÁÑ∂ËÄåÔºåÊ≤íÊúâÈáùÂ∞çÈÄô‰∫õ RDF-star Èô≥Ëø∞ÁöÑË™ûÁæ©ÊèêÂá∫Âª∫Ë≠∞Ôºå‰πüÊ≤íÊúâÂÖßÂª∫ÁöÑÈÅã‰ΩúÂ∑•ÂÖ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂõõÂÄºÈÇèËºØÁöÑÁü•Ë≠ò RDF-star ÂÖÉË≥áÊñôÊü•Ë©¢Ë™ûË®ÄÔºåÁ®±ÁÇ∫ eSPARQL„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÊì¥ÂÖÖ‰∫Ü RDF-star ÁöÑÊü•Ë©¢Ë™ûË®Ä SPARQL-starÔºå‰∏¶‰ΩøÁî®Êñ∞ÁöÑ FROM Â≠êÂè•È°ûÂûã‰æÜ‰øÉÈÄ≤ÈÅã‰ΩúÔºåÂåÖÊã¨Â§öÈáç‰∏îÊúâÊôÇË°ùÁ™ÅÁöÑ‰ø°Âøµ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÂèØ‰ª•Ë°®ÈÅîÂõõÂÄãÁî®‰æãÊü•Ë©¢ÔºåÂåÖÊã¨‰∏ãÂàóÂäüËÉΩÔºö(i) Êü•Ë©¢ÂÄã‰∫∫ÁöÑ‰ø°ÂøµÔºå(ii) ‰ø°ÂøµÁöÑÂΩôÁ∏ΩÔºå(iii) Êü•Ë©¢Ë™∞ËàáÊüê‰∫∫Ë°ùÁ™ÅÔºå‰ª•Âèä (iv) ÈóúÊñº‰ø°ÂøµÁöÑ‰ø°ÂøµÔºàÂç≥‰ø°ÂøµÁöÑÂ∑¢ÁãÄÔºâ„ÄÇ

##### **On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**
2407.21476v1 by Nick Rossenbach, Ralf Schl√ºter, Sakriani Sakti

The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÊñáÂ≠óËΩâË™ûÈü≥ (TTS) Á≥ªÁµ±ÁöÑÂø´ÈÄüÁôºÂ±ïÔºå‰ΩøÂÖ∂Âæó‰ª•ÊáâÁî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÂÖ∂‰ªñÈ†òÂüüÔºå‰æãÂ¶ÇËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÊàñÂè£Ë™ûÁøªË≠Ø (SLT)„ÄÇÁî±Êñº TTS Êû∂ÊßãÂèäÂÖ∂Êì¥ÂÖÖÂäüËÉΩÁ®ÆÈ°ûÁπÅÂ§öÔºåÂõ†Ê≠§ÈÅ∏ÊìáÂì™ÂÄã TTS Á≥ªÁµ±‰æÜÈÄ≤Ë°åÂêàÊàêË≥áÊñôÂª∫Á´ã‰∏¶ÈùûÊòì‰∫ã„ÄÇÊàëÂÄëÂú®ÂêàÊàêË≥áÊñôÁî¢ÁîüÁØÑÂúçÂÖßÊØîËºÉ‰∫îÁ®Æ‰∏çÂêåÁöÑ TTS Ëß£Á¢ºÂô®Êû∂ÊßãÔºå‰ª•È°ØÁ§∫Â∞çÂü∫Êñº CTC ÁöÑË™ûÈü≥Ëæ®Ë≠òË®ìÁ∑¥ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂ∞áËæ®Ë≠òÁµêÊûúËàáÂèØË®àÁÆóÁöÑÊåáÊ®ôÔºà‰æãÂ¶Ç NISQA MOS ÂíåÂèØÁêÜËß£Â∫¶ÔºâÈÄ≤Ë°åÊØîËºÉÔºåÁôºÁèæËàá ASR ÊïàËÉΩ‰∏¶ÁÑ°ÊòéÁ¢∫ÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞ÔºåÂ∞çÊñºË≥áÊñôÁî¢ÁîüÔºåËá™Ëø¥Ê≠∏Ëß£Á¢ºÁöÑË°®ÁèæÂÑ™ÊñºÈùûËá™Ëø¥Ê≠∏Ëß£Á¢ºÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈáèÂåñ TTS Ê≥õÂåñËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇ

##### **Fine-gained Zero-shot Video Sampling**
2407.21475v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

Incorporating a temporal dimension into pretrained image diffusion models for
video generation is a prevalent approach. However, this method is
computationally demanding and necessitates large-scale video datasets. More
critically, the heterogeneity between image and video datasets often results in
catastrophic forgetting of the image expertise. Recent attempts to directly
extract video snippets from image diffusion models have somewhat mitigated
these problems. Nevertheless, these methods can only generate brief video clips
with simple movements and fail to capture fine-grained motion or non-grid
deformation. In this paper, we propose a novel Zero-Shot video Sampling
algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling
high-quality video clips from existing image synthesis methods, such as Stable
Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$
utilizes the dependency noise model and temporal momentum attention to ensure
content consistency and animation coherence, respectively. This ability enables
it to excel in related tasks, such as conditional and context-specialized video
generation and instruction-guided video editing. Experimental results
demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in
zero-shot video generation, occasionally outperforming recent supervised
methods.
  Homepage: \url{https://densechen.github.io/zss/}.

ÊëòË¶ÅÔºöÂ∞áÊôÇÈñìÁ∂≠Â∫¶Êï¥ÂêàÂà∞È†êË®ìÁ∑¥ÂΩ±ÂÉèÊì¥Êï£Ê®°Âûã‰∏≠Ôºå‰ª•ÈÄ≤Ë°åÂΩ±ÁâáÁîüÊàêÔºåÊòØ‰∏ÄÁ®ÆÊôÆÈÅçÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂú®ÈÅãÁÆó‰∏äË¶ÅÊ±ÇÂæàÈ´òÔºå‰∏îÈúÄË¶ÅÂ§ßË¶èÊ®°ÁöÑÂΩ±ÁâáË≥áÊñôÈõÜ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂΩ±ÂÉèË≥áÊñôÈõÜÂíåÂΩ±ÁâáË≥áÊñôÈõÜ‰πãÈñìÁöÑÁï∞Ë≥™ÊÄßÔºåÈÄöÂ∏∏ÊúÉÂ∞éËá¥ÂΩ±ÂÉèÂ∞àÊ•≠Áü•Ë≠òÁöÑÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇËøëÊúüÂòóË©¶Áõ¥Êé•ÂæûÂΩ±ÂÉèÊì¥Êï£Ê®°Âûã‰∏≠Êì∑ÂèñÂΩ±ÁâáÁâáÊÆµÔºåÂú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äÁ∑©Ëß£‰∫ÜÈÄô‰∫õÂïèÈ°å„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊñπÊ≥ïÂè™ËÉΩÁîüÊàêÂÖ∑ÊúâÁ∞°ÂñÆÂãï‰ΩúÁöÑÁ∞°Áü≠ÂΩ±ÁâáÁâáÊÆµÔºåËÄå‰∏îÁÑ°Ê≥ïÊçïÊçâÁ¥∞Á≤íÂ∫¶ÁöÑÂãï‰ΩúÊàñÈùûÁ∂≤Ê†ºËÆäÂΩ¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂΩ±ÁâáÊé°Ê®£ÊºîÁÆóÊ≥ïÔºåË°®Á§∫ÁÇ∫ $\mathcal{ZS}^2$ÔºåÂÆÉËÉΩÂ§†Áõ¥Êé•ÂæûÁèæÊúâÁöÑÂΩ±ÂÉèÂêàÊàêÊñπÊ≥ïÔºà‰æãÂ¶Ç Stable DiffusionÔºâ‰∏≠Êé°Ê®£È´òÂìÅË≥™ÁöÑÂΩ±ÁâáÁâáÊÆµÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïË®ìÁ∑¥ÊàñÊúÄ‰Ω≥Âåñ„ÄÇÂÖ∑È´î‰æÜË™™Ôºå$\mathcal{ZS}^2$ ÂàÜÂà•Âà©Áî®‰æùË≥¥ÈõúË®äÊ®°ÂûãÂíåÊôÇÈñìÂãïÈáèÊ≥®ÊÑèÂäõÔºå‰ª•Á¢∫‰øùÂÖßÂÆπ‰∏ÄËá¥ÊÄßÂíåÂãïÁï´ÈÄ£Ë≤´ÊÄß„ÄÇÈÄôÁ®ÆËÉΩÂäõ‰ΩøÂÆÉËÉΩÂ§†Âú®Áõ∏Èóú‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰æãÂ¶ÇÊ¢ù‰ª∂ÂºèÂíåÁâπÂÆöÊñºÂÖßÂÆπÁöÑÂΩ±ÁâáÁîüÊàêÔºå‰ª•ÂèäÊåá‰ª§ÂºïÂ∞éÁöÑÂΩ±ÁâáÁ∑®ËºØ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºå$\mathcal{ZS}^2$ Âú®Èõ∂Ê¨°Â≠∏ÁøíÂΩ±ÁâáÁîüÊàê‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂÅ∂ÁàæÂÑ™ÊñºÊúÄËøëÁöÑÁõ£Áù£ÂºèÊñπÊ≥ï„ÄÇ‰∏ªÈ†ÅÔºö\url{https://densechen.github.io/zss/}„ÄÇ

##### **An Invertible State Space for Process Trees**
2407.21468v1 by Gero Kolhof, Sebastiaan J. van Zelst

Process models are, like event data, first-class citizens in most process
mining approaches. Several process modeling formalisms have been proposed and
used, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,
little research addresses the formal properties of process trees and the
corresponding potential to improve the efficiency of solving common
computational problems. Therefore, in this paper, we propose an invertible
state space definition for process trees and demonstrate that the corresponding
state space graph is isomorphic to the state space graph of the tree's inverse.
Our result supports the development of novel, time-efficient, decomposition
strategies for applications of process trees. Our experiments confirm that our
state space definition allows for the adoption of bidirectional state space
search, which significantly improves the overall performance of state space
searches.

ÊëòË¶ÅÔºöÂú®Â§ßÂ§öÊï∞ÊµÅÁ®ãÊåñÊéòÊñπÊ≥ï‰∏≠ÔºåÊµÅÁ®ãÊ®°Âûã‰∏é‰∫ã‰ª∂Êï∞ÊçÆÁ±ª‰ººÔºåÈÉΩÊòØ‰∏ÄÁ≠âÂÖ¨Ê∞ë„ÄÇÂ∑≤ÁªèÊèêÂá∫Âπ∂‰ΩøÁî®‰∫ÜÂ§öÁßçÊµÅÁ®ãÂª∫Ê®°ÂΩ¢Âºè‰∏ª‰πâÔºå‰æãÂ¶ÇÔºåPetri ÁΩë„ÄÅBPMN ÂíåÊµÅÁ®ãÊ†ë„ÄÇÂ∞ΩÁÆ°ÂÆÉ‰ª¨ÁªèÂ∏∏‰ΩøÁî®Ôºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂Ê∂âÂèäÊµÅÁ®ãÊ†ëÁöÑÂΩ¢ÂºèÂ±ûÊÄß‰ª•ÂèäÊèêÈ´òËß£ÂÜ≥Â∏∏ËßÅËÆ°ÁÆóÈóÆÈ¢òÁöÑÊïàÁéáÁöÑÁõ∏Â∫îÊΩúÂäõ„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊµÅÁ®ãÊ†ëÁöÑÂèØÈÄÜÁä∂ÊÄÅÁ©∫Èó¥ÂÆö‰πâÔºåÂπ∂ËØÅÊòé‰∫ÜÁõ∏Â∫îÁöÑÁä∂ÊÄÅÁ©∫Èó¥Âõæ‰∏éÊ†ëÁöÑÈÄÜÁä∂ÊÄÅÁ©∫Èó¥ÂõæÂêåÊûÑ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÊîØÊåÅ‰∏∫ÊµÅÁ®ãÊ†ëÁöÑÂ∫îÁî®Á®ãÂ∫èÂºÄÂèëÊñ∞È¢ñ„ÄÅÁúÅÊó∂ÁöÑÂàÜËß£Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÅÂÆûÔºåÊàë‰ª¨ÁöÑÁä∂ÊÄÅÁ©∫Èó¥ÂÆö‰πâÂÖÅËÆ∏ÈááÁî®ÂèåÂêëÁä∂ÊÄÅÁ©∫Èó¥ÊêúÁ¥¢ÔºåËøôÊòæËëóÊèêÈ´ò‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥ÊêúÁ¥¢ÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

##### **Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**
2407.21467v1 by Mengtian Kang, Yansong Hu, Shuo Gao, Yuanyuan Liu, Hongbei Meng, Xuemeng Li, Xuhang Chen, Hubin Zhao, Jing Fu, Guohua Hu, Wei Wang, Yanning Dai, Arokia Nathan, Peter Smielewski, Ningli Wang, Shiming Li

Childhood myopia constitutes a significant global health concern. It exhibits
an escalating prevalence and has the potential to evolve into severe,
irreversible conditions that detrimentally impact familial well-being and
create substantial economic costs. Contemporary research underscores the
importance of precisely predicting myopia progression to enable timely and
effective interventions, thereby averting severe visual impairment in children.
Such predictions predominantly rely on subjective clinical assessments, which
are inherently biased and resource-intensive, thus hindering their widespread
application. In this study, we introduce a novel, high-accuracy method for
quantitatively predicting the myopic trajectory and myopia risk in children
using only fundus images and baseline refraction data. This approach was
validated through a six-year longitudinal study of 3,408 children in Henan,
utilizing 16,211 fundus images and corresponding refractive data. Our method
based on deep learning demonstrated predictive accuracy with an error margin of
0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of
developing myopia and high myopia, respectively. These findings confirm the
utility of our model in supporting early intervention strategies and in
significantly reducing healthcare costs, particularly by obviating the need for
additional metadata and repeated consultations. Furthermore, our method was
designed to rely only on fundus images and refractive error data, without the
need for meta data or multiple inquiries from doctors, strongly reducing the
associated medical costs and facilitating large-scale screening. Our model can
even provide good predictions based on only a single time measurement.
Consequently, the proposed method is an important means to reduce medical
inequities caused by economic disparities.

ÊëòË¶ÅÔºöÂÖíÁ´•ËøëË¶ñÊßãÊàêÂÖ®ÁêÉÈáçË¶ÅÁöÑÂÅ•Â∫∑ÂïèÈ°å„ÄÇÂÆÉÈ°ØÁ§∫Âá∫Êó•ÁõäÂ¢ûÂä†ÁöÑÁõõË°åÁéáÔºå‰∏¶ÂèØËÉΩÊºîËÆäÊàêÂö¥Èáç„ÄÅ‰∏çÂèØÈÄÜËΩâÁöÑÁãÄÊ≥ÅÔºåÂ∞çÂÆ∂Â∫≠Á¶èÁ•âÈÄ†Êàê‰∏çÂà©ÂΩ±ÈüøÔºå‰∏¶Áî¢ÁîüÂ§ßÈáèÁöÑÁ∂ìÊøüÊàêÊú¨„ÄÇÁèæ‰ª£Á†îÁ©∂Âº∑Ë™øÁ≤æÊ∫ñÈ†êÊ∏¨ËøëË¶ñÈÄ≤Â±ïÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂØ¶ÁèæÂèäÊôÇÊúâÊïàÁöÑÂπ≤È†êÔºåÂæûËÄåÈÅøÂÖçÂÖíÁ´•Âá∫ÁèæÂö¥ÈáçÁöÑË¶ñÂäõÊêçÂÆ≥„ÄÇÊ≠§È°ûÈ†êÊ∏¨‰∏ªË¶Å‰æùË≥¥‰∏ªËßÄÁöÑËá®Â∫äË©ï‰º∞ÔºåÂÖ∂Êú¨Ë∫´ÂÖ∑ÊúâÂÅèË¶ã‰∏îË≥áÊ∫êÂØÜÈõÜÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©é„ÄÅÈ´òÁ≤æÁ¢∫Â∫¶ÁöÑÊñπÊ≥ïÔºåÂÉÖ‰ΩøÁî®ÁúºÂ∫ïÂúñÂÉèÂíåÂü∫Á∑öÂ±àÂÖâÊï∏ÊìöÔºåÂ∞±ËÉΩÂÆöÈáèÈ†êÊ∏¨ÂÖíÁ´•ÁöÑËøëË¶ñËªåË∑°ÂíåËøëË¶ñÈ¢®Èö™„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈÄöÈÅéÂ∞çÊ≤≥ÂçóÁúÅ 3,408 ÂêçÂÖíÁ´•ÈÄ≤Ë°åÁÇ∫ÊúüÂÖ≠Âπ¥ÁöÑÁ∏±ÂêëÁ†îÁ©∂ÔºåÂà©Áî® 16,211 ÂºµÁúºÂ∫ïÂúñÂÉèÂíåÁõ∏ÊáâÁöÑÂ±àÂÖâÊï∏ÊìöÈÄ≤Ë°å‰∫ÜÈ©óË≠â„ÄÇÊàëÂÄëÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÂπ¥Ë™§Â∑ÆÁØÑÂúçÁÇ∫ 0.311DÔºåÈ†êÊ∏¨ÁôºÁîüËøëË¶ñÂíåÈ´òÂ∫¶ËøëË¶ñÁöÑÈ¢®Èö™ÁöÑ AUC ÂàÜÊï∏ÂàÜÂà•ÁÇ∫ 0.944 Âíå 0.995„ÄÇÈÄô‰∫õÁôºÁèæË≠âÂØ¶‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîØÊåÅÊó©ÊúüÂπ≤È†êÁ≠ñÁï•ÂíåÈ°ØËëóÈôç‰ΩéÈÜ´ÁôÇ‰øùÂÅ•ÊàêÊú¨ÊñπÈù¢ÁöÑÊïàÁî®ÔºåÁâπÂà•ÊòØÈÄöÈÅéÊ∂àÈô§Â∞çÈ°çÂ§ñÂÖÉÊï∏ÊìöÂíåÈáçË§áË´ÆË©¢ÁöÑÈúÄË¶Å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊñπÊ≥ïË¢´Ë®≠Ë®àÁÇ∫ÂÉÖ‰æùË≥¥ÁúºÂ∫ïÂúñÂÉèÂíåÂ±àÂÖâ‰∏çÊ≠£Êï∏ÊìöÔºåËÄåÁÑ°ÈúÄÂÖÉÊï∏ÊìöÊàñÈÜ´ÁîüÁöÑÂ§öÊ¨°Ë©¢ÂïèÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜÁõ∏ÈóúÁöÑÈÜ´ÁôÇÊàêÊú¨Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÂ§ßË¶èÊ®°ÁØ©Êü•„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÁîöËá≥ÂèØ‰ª•ÂÉÖÊ†πÊìöÂñÆÊ¨°ÊôÇÈñìÊ∏¨ÈáèÊèê‰æõËâØÂ•ΩÁöÑÈ†êÊ∏¨„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊòØÊ∏õÂ∞ëÁî±Á∂ìÊøüÂ∑ÆË∑ùÈÄ†ÊàêÁöÑÈÜ´ÁôÇ‰∏çÂπ≥Á≠âÁöÑÈáçË¶ÅÊâãÊÆµ„ÄÇ

##### **KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**
2407.21459v1 by Gilang Fajar Febrian, Grazziela Figueredo

Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñôÂ∞çÊñºË≠âÊìöÂü∫Á§éÁöÑÊîøÁ≠ñÂà∂ÂÆöÂíåÊîπÂñÑÂÖ¨ÂÖ±ÊúçÂãôËá≥ÈóúÈáçË¶ÅÔºåÂåÖÊã¨Âç∞Â∞ºÂÖ±ÂíåÂúãË≤°ÊîøÈÉ®„ÄÇÁÑ∂ËÄåÔºåÊîøÂ∫úË≤°ÂãôË≥áÊñôÂíåÊ≥ïË¶èÁöÑË§áÈõúÊÄßÂíåÂãïÊÖãÊÄßÂèØËÉΩÊúÉÈòªÁ§ôÊ±∫Á≠ñ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÁöÑÊΩõÂäõÔºåÈáçÈªûÈóúÊ≥®Âç∞Â∞ºÁöÑË≤°ÂãôË≥áÊñôÂíåÊ≥ïË¶è„ÄÇÈõñÁÑ∂ LLM Âú®ÈáëËûçÈ†òÂüüÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÂú®Âç∞Â∞ºÂÖ¨ÂÖ±ÈÉ®ÈñÄÁöÑ‰ΩøÁî®Â∞öÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÊú¨Á†îÁ©∂Êé°Áî®Ëø≠‰ª£ÊµÅÁ®ãÔºå‰ΩøÁî®Â∏∂ÊúâÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)„ÄÅÊèêÁ§∫Â∑•Á®ãÂíåÂæÆË™øÁöÑ LangChain ‰æÜÈñãÁôº KemenkeuGPT„ÄÇË≥áÊñôÈõÜÂæû 2003 Âπ¥Âà∞ 2023 Âπ¥Êî∂ÈõÜËá™Âç∞Â∞ºË≤°ÊîøÈÉ®„ÄÅÂç∞Â∞ºÁµ±Ë®àÂ±ÄÂíåÂúãÈöõË≤®Âπ£Âü∫ÈáëÁµÑÁπî (IMF)„ÄÇÂ∞çË≤°ÊîøÈÉ®ÂÆòÂì°ÁöÑË™øÊü•ÂíåË®™Ë´áÂëäÁü•„ÄÅÂ¢ûÂº∑ÂíåÂæÆË™ø‰∫ÜÊ®°Âûã„ÄÇÊàëÂÄë‰ΩøÁî®‰∫∫È°ûÂõûÈ•ã„ÄÅÂü∫Êñº LLM ÁöÑË©ï‰º∞ÂíåÂü∫Ê∫ñÂ∞çÊ®°ÂûãÈÄ≤Ë°å‰∫ÜË©ï‰º∞„ÄÇË©≤Ê®°ÂûãÁöÑÊ∫ñÁ¢∫ÁéáÂæû 35% ÊèêÈ´òÂà∞ 61%ÔºåÊ≠£Á¢∫ÁéáÂæû 48% ÊèêÈ´òÂà∞ 64%„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêË©ï‰º∞ (RAGAS) Ê°ÜÊû∂Ë°®ÊòéÔºåKemenkeuGPT ÈÅîÂà∞‰∫Ü 44% ÁöÑÊ≠£Á¢∫ÁéáÔºå73% ÁöÑÂø†ÂØ¶Â∫¶Ôºå40% ÁöÑÁ≤æÁ¢∫Â∫¶Âíå 60% ÁöÑÂè¨ÂõûÁéáÔºåÂÑ™ÊñºÂÖ∂‰ªñÂπæÂÄãÂü∫Á§éÊ®°Âûã„ÄÇÂ∞çË≤°ÊîøÈÉ®Â∞àÂÆ∂ÁöÑÊé°Ë®™Ë°®ÊòéÔºåKemenkeuGPT ÊúâÂèØËÉΩÊàêÁÇ∫Ê±∫Á≠ñÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÈ†êË®àÈÄô‰∫õÁµêÊûúÊúÉÈö®ËëóÊåÅÁ∫åÁöÑ‰∫∫È°ûÂõûÈ•ãËÄåÂæóÂà∞ÊîπÂñÑ„ÄÇ</paragraph>

##### **TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**
2407.21453v1 by Zhaolan Huang, Adrien Tousnakhoff, Polina Kozyr, Roman Rehausen, Felix Bie√ümann, Robert Lachlan, Cedric Adjih, Emmanuel Baccelli

Monitoring biodiversity at scale is challenging. Detecting and identifying
species in fine grained taxonomies requires highly accurate machine learning
(ML) methods. Training such models requires large high quality data sets. And
deploying these models to low power devices requires novel compression
techniques and model architectures. While species classification methods have
profited from novel data sets and advances in ML methods, in particular neural
networks, deploying these state of the art models to low power devices remains
difficult. Here we present a comprehensive empirical comparison of various
tinyML neural network architectures and compression techniques for species
classification. We focus on the example of bird song detection, more concretely
a data set curated for studying the corn bunting bird species. The data set is
released along with all code and experiments of this study. In our experiments
we compare predictive performance, memory and time complexity of classical
spectrogram based methods and recent approaches operating on raw audio signal.
Our results indicate that individual bird species can be robustly detected with
relatively simple architectures that can be readily deployed to low power
devices.

ÊëòË¶ÅÔºöÂ§ßË¶èÊ®°Áõ£ÊéßÁîüÁâ©Â§öÊ®£ÊÄßÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Á¥∞Á≤íÂ∫¶ÂàÜÈ°ûÊ≥ï‰∏≠Ê™¢Ê∏¨ÂíåË≠òÂà•Áâ©Á®ÆÈúÄË¶ÅÈ´òÂ∫¶Ê∫ñÁ¢∫ÁöÑÊ©üÂô®Â≠∏Áøí (ML) ÊñπÊ≥ï„ÄÇË®ìÁ∑¥Ê≠§È°ûÊ®°ÂûãÈúÄË¶ÅÂ§ßÈáèÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜ„ÄÇËÄåÂ∞áÈÄô‰∫õÊ®°ÂûãÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆÈúÄË¶ÅÂâµÊñ∞ÁöÑÂ£ìÁ∏ÆÊäÄË°ìÂíåÊ®°ÂûãÊû∂Êßã„ÄÇÈõñÁÑ∂Áâ©Á®ÆÂàÜÈ°ûÊñπÊ≥ïÂèóÁõäÊñºÊñ∞Á©éÁöÑË≥áÊñôÈõÜÂíåÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÈÄ≤Ê≠•ÔºåÁâπÂà•ÊòØÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰ΩÜÂ∞áÈÄô‰∫õÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆ‰ªçÁÑ∂ÂæàÂõ∞Èõ£„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂêÑÁ®Æ tinyML Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÂíåÁâ©Á®ÆÂàÜÈ°ûÂ£ìÁ∏ÆÊäÄË°ìÁöÑÂÖ®Èù¢ÂØ¶Ë≠âÊØîËºÉ„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÈ≥•È≥¥ËÅ≤Ê™¢Ê∏¨ÁöÑÁØÑ‰æãÔºåÊõ¥ÂÖ∑È´îÂú∞Ë™™ÔºåÊòØÁÇ∫Á†îÁ©∂ÁéâÁ±≥ÈµêÈ≥•È°ûÁâ©Á®ÆËÄåÁ≠ñÂ±ïÁöÑË≥áÊñôÈõÜ„ÄÇË©≤Ë≥áÊñôÈõÜËàáÊú¨Á†îÁ©∂ÁöÑÊâÄÊúâÁ®ãÂºèÁ¢ºÂíåÂØ¶È©ó‰∏ÄËµ∑ÁôºÂ∏É„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂü∫ÊñºÁ∂ìÂÖ∏ËÅ≤Ë≠úÂúñÁöÑÊñπÊ≥ïÂíåËôïÁêÜÂéüÂßãÈü≥Ë®äË®äËôüÁöÑÊúÄÊñ∞ÊñπÊ≥ïÁöÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÅË®òÊÜ∂È´îÂíåÊôÇÈñìË§áÈõúÂ∫¶„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂèØ‰ª•ÈÄèÈÅéÁõ∏Â∞çÁ∞°ÂñÆÁöÑÊû∂ÊßãÁ©©ÂÅ•Âú∞Ê™¢Ê∏¨ÂÄãÂà•È≥•È°ûÁâ©Á®ÆÔºåÈÄô‰∫õÊû∂ÊßãÂèØ‰ª•ËºïÊòìÂú∞ÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆ„ÄÇ

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

ÊëòË¶ÅÔºöÁé∞ÂÆû‰∏ñÁïåÁöÑÂØºËà™ÈÄöÂ∏∏Ê∂âÂèäÂ§ÑÁêÜÊÑèÂ§ñÁöÑÈöúÁ¢çÔºå‰æãÂ¶ÇÂÖ≥ÁùÄÁöÑÈó®„ÄÅÁßªÂä®ÁöÑÁâ©‰ΩìÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÂÆû‰Ωì„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑËßÜËßâÂíåËØ≠Ë®ÄÂØºËà™ (VLN) ‰ªªÂä°ÈÄöÂ∏∏ÂÅáËÆæÊåá‰ª§‰∏éÂõ∫ÂÆöÁöÑÂíåÈ¢ÑÂÆö‰πâÁöÑÂØºËà™ÂõæÂÆåÂÖ®‰∏ÄËá¥ÔºåÊ≤°Êúâ‰ªª‰ΩïÈöúÁ¢ç„ÄÇËøôÁßçÂÅáËÆæÂøΩÁï•‰∫ÜÂÆûÈôÖÂØºËà™ÂõæÂíåÁªôÂÆöÊåá‰ª§‰∏≠ÊΩúÂú®ÁöÑÂ∑ÆÂºÇÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂÆ§ÂÜÖÂíåÂÆ§Â§ñ‰ª£ÁêÜÂá∫Áé∞ÈáçÂ§ßÊïÖÈöú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÈÄöËøá‰øÆÊîπÂØºËà™ÂõæÂíåËßÜËßâËßÇÂØüÔºåÂ∞ÜÂêÑÁßçÈöúÁ¢çÊï¥ÂêàÂà∞ R2R Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂºïÂÖ•‰∫ÜÂàõÊñ∞Êï∞ÊçÆÈõÜÂíå‰ªªÂä°ÔºåÂç≥Â∏¶ÊúâÊÑèÂ§ñÈöúÁ¢çÁöÑ R2R (R2R-UNO)„ÄÇR2R-UNO ÂåÖÂê´ÂêÑÁßçÁ±ªÂûãÂíåÊï∞ÈáèÁöÑË∑ØÂæÑÈöúÁ¢çÔºå‰ª•ÁîüÊàê VLN Á†îÁ©∂ÁöÑÊåá‰ª§-Áé∞ÂÆû‰∏çÂåπÈÖç„ÄÇÂú® R2R-UNO ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑ VLN ÊñπÊ≥ïÂú®Èù¢ÂØπÊ≠§Á±ª‰∏çÂåπÈÖçÊó∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàòÔºåËøôË°®ÊòéÂÆÉ‰ª¨‰∏•Ê†ºÈÅµÂæ™Êåá‰ª§ÔºåËÄå‰∏çÊòØËá™ÈÄÇÂ∫îÂú∞ÂØºËà™„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ObVLNÔºàÂèóÈòª VLNÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨ËØæÁ®ãËÆ≠ÁªÉÁ≠ñÁï•ÂíåËôöÊãüÂõæÊûÑÂª∫Ôºå‰ª•Â∏ÆÂä©‰ª£ÁêÜÊúâÊïàÂú∞ÈÄÇÂ∫îÂèóÈòªÁéØÂ¢É„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåObVLN ‰∏ç‰ªÖÂú®Êó†ÈöúÁ¢çÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÂú®ÊÑèÂ§ñÈöúÁ¢ç‰∏≠‰πüËé∑Âæó‰∫ÜÂÆûË¥®ÊÄßÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

##### **Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**
2407.21443v1 by Taiji Li, Zhi Li, Yin Zhang

Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊïàËÉΩÔºå‰ΩÜÂÆÉÂÄë‰ªçÈ£ΩÂèóÁ®±ÁÇ∫ÂπªË¶∫ÁöÑËôõÂÅá‰∏ç‰∏ÄËá¥ÂïèÈ°åÊâÄËã¶„ÄÇ‰æãÂ¶ÇÔºåLLM ÊúâÊôÇÊúÉÁî¢ÁîüËàáÂéüÂßãÊñáÁ´†‰∏çÂêåÁöÑÂÖßÂÆπÔºå‰∏îÂÅèÂ•ΩÊì∑ÂèñÂá∫ÁèæÂú®ÂÖßÂÆπÈñãÈ†≠ÂíåÁµêÂ∞æÁöÑË≥áË®äÔºåÁâπÂà•ÊòØÂú®Èï∑ÁØáÊñá‰ª∂ÊëòË¶Å‰∏≠„ÄÇÂèóÂà∞ÈÄô‰∫õÁôºÁèæÁöÑÂïüÁôºÔºåÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅé‰øÉ‰Ωø LLM Êõ¥ÂÖ¨Âπ≥‰∏îÂø†ÂØ¶Âú∞ËôïÁêÜÊï¥ÁØáÊñáÁ´†Ôºå‰æÜÊîπÂñÑÂÖ∂Âú®ÊëòË¶Å‰∏≠ÁöÑÁúüÂØ¶ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊëòË¶ÅÁî¢ÁîüÁ≠ñÁï•ÔºåÁ®±ÁÇ∫ SliSumÔºåÂÆÉÂà©Áî®ÊªëÂãïË¶ñÁ™óÂíåËá™Êàë‰∏ÄËá¥ÊÄßÁöÑÊ¶ÇÂøµ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåSliSum Â∞áÂéüÂßãÊñáÁ´†ÂàÜÊàêÈáçÁñäÁöÑË¶ñÁ™óÔºå‰∏¶Âà©Áî® LLM ÁÇ∫Ë¶ñÁ™ó‰∏≠ÁöÑÂÖßÂÆπÁî¢ÁîüÂ±ÄÈÉ®ÊëòË¶Å„ÄÇÊúÄÂæåÔºåSliSum ‰ΩøÁî®Áæ§ÈõÜÂíåÂ§öÊï∏Ê±∫ÊºîÁÆóÊ≥ïÂΩôÁ∏ΩÊâÄÊúâÂ±ÄÈÉ®ÊëòË¶ÅÔºå‰ª•Áî¢ÁîüÊõ¥Âø†ÂØ¶ÁöÑÊï¥ÁØáÊñáÁ´†ÊëòË¶Å„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåSliSum Âú®Á∞°Áü≠ÂíåÈï∑ÁØáÊñáÂ≠óÊëòË¶Å‰∏≠ÔºåÈ°ØËëóÊèêÂçá‰∫Ü LLaMA-2„ÄÅClaude-2 Âíå GPT-3.5 Á≠â‰∏çÂêå LLM ÁöÑÁúüÂØ¶ÊÄßÔºåÂêåÊôÇÁ∂≠ÊåÅÂÖ∂ÊµÅÊö¢Â∫¶ÂíåË≥áË®äÈáèÔºå‰∏îÁÑ°ÈúÄÈ°çÂ§ñÁöÑÂæÆË™øÂíåË≥áÊ∫ê„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°åÂÆöÊÄßÂíåÂÆöÈáèÁ†îÁ©∂Ôºå‰ª•Êé¢Ë®é SliSum ÁöÑÈÅã‰ΩúÂéüÁêÜÔºå‰ª•Âèä SliSum ‰∏≠ÁöÑË∂ÖÂèÉÊï∏Â∞çÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇ

##### **QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**
2407.21441v2 by Ritvik Setty, Vinay Setty

Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.

ÊëòË¶ÅÔºöÈ©óË≠âÊü•Ê†∏‰∫ãÂØ¶ÁöÑËÅ≤ÊòéÔºåÂç≥‰ΩøÂ∞ç‰∫∫È°û‰æÜË™™Ôºå‰πüÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïÂ∑≤Ë≠âÊòéÔºåÂ∞áËÅ≤ÊòéÂàÜËß£ÊàêÁõ∏ÈóúÂïèÈ°å‰ª•Êî∂ÈõÜË≠âÊìöÔºåÂèØ‰ª•ÊèêÈ´òÊü•Ê†∏‰∫ãÂØ¶Á®ãÂ∫èÁöÑÊïàÁéá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÁ∂ìÈ©óË≠âÊìöÔºåË™™ÊòéÈÄôÁ®ÆÂïèÈ°åÂàÜËß£ÂèØ‰ª•ÊúâÊïàËá™ÂãïÂåñ„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜËºÉÂ∞èÁöÑÁîüÊàêÊ®°ÂûãÔºåÈáùÂ∞çÂïèÈ°åÁîüÊàê‰ªªÂãôÈÄ≤Ë°åÂæÆË™øÔºå‰ΩøÁî®‰æÜËá™ÂêÑÁ®ÆË≥áÊñôÈõÜÁöÑË≥áÊñôÊì¥ÂÖÖÔºåÊØîÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑË°®ÁèæÈ´òÂá∫ 8%„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºå‰ΩøÁî®Ê©üÂô®Áî¢ÁîüÁöÑÂïèÈ°åÊâÄÊ™¢Á¥¢Âà∞ÁöÑË≠âÊìöÔºåË¢´Ë≠âÊòéÊØîÂæû‰∫∫Â∑•Êí∞ÂØ´ÁöÑÂïèÈ°å‰∏≠Áç≤ÂæóÁöÑË≠âÊìöÔºåÂ∞çÊü•Ê†∏‰∫ãÂØ¶Êõ¥ÊúâÊïà„ÄÇÊàëÂÄë‰πüÂ∞çÂàÜËß£ÁöÑÂïèÈ°åÈÄ≤Ë°åÊâãÂãïË©ï‰º∞Ôºå‰ª•Ë©ï‰º∞ÊâÄÁî¢ÁîüÂïèÈ°åÁöÑÂìÅË≥™„ÄÇ

##### **MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**
2407.21439v1 by Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo

Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã (MLLM) Â∑≤Âú®Â§ÑÁêÜÂíåÁîüÊàêË∑®Â§ö‰∏™Êï∞ÊçÆÊ®°ÊÄÅÔºàÂåÖÊã¨ÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÔºâÁöÑÂÜÖÂÆπÊñπÈù¢Â±ïÁ§∫Âá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåMLLM ÁöÑ‰∏Ä‰∏™ÈáçÂ§ßÁº∫ÁÇπÊòØÂÆÉ‰ª¨‰æùËµñ‰∫éÈùôÊÄÅËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥‰ø°ÊÅØËøáÊó∂Âíå‰∏ä‰∏ãÊñáÊÑüÁü•ÊúâÈôê„ÄÇËøôÁßçÈùôÊÄÅÁâπÊÄßÈòªÁ¢ç‰∫ÜÂÆÉ‰ª¨Êèê‰æõÂáÜÁ°Æ„ÄÅÊúÄÊñ∞ÁöÑÂìçÂ∫îÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä®ÊÄÅÊàñÂø´ÈÄüÂèòÂåñÁöÑ‰∏ä‰∏ãÊñá‰∏≠„ÄÇÈõÜÊàêÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (Multimodal RAG) Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜËØ•Á≥ªÁªü‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞Â§öÁ≤íÂ∫¶Âô™Â£∞ÂØπÂ∫î (MNC) ÈóÆÈ¢òÔºåÂÖ∂‰∏≠Ê∂âÂèä‰∏§ÁßçÁ±ªÂûãÁöÑÂô™Â£∞ÔºöÁ≤óÁ≤íÂ∫¶ÔºàÊü•ËØ¢Ê†áÈ¢òÔºâÂíåÁªÜÁ≤íÂ∫¶ÔºàÊü•ËØ¢ÂõæÂÉèÔºâ„ÄÇËøôÁßçÂô™Â£∞ÈòªÁ¢ç‰∫ÜÂáÜÁ°ÆÁöÑÊ£ÄÁ¥¢ÂíåÁîüÊàê„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü RagLLaVAÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÁü•ËØÜÂ¢ûÂº∫ÈáçÊñ∞ÊéíÂ∫èÂíåÂô™Â£∞Ê≥®ÂÖ•ËÆ≠ÁªÉÔºå‰ª•Ëß£ÂÜ≥Ëøô‰∫õÈôêÂà∂„ÄÇÊàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™ÁÆÄÂçï‰ΩÜÊúâÊïàÁöÑÊåá‰ª§Ê®°ÊùøÂØπ MLLM ËøõË°åÊåá‰ª§ÂæÆË∞ÉÔºå‰ª•ËØ±ÂØºÂÖ∂ÊéíÂ∫èËÉΩÂäõÔºåÂπ∂Â∞ÜÂÖ∂Áî®‰ΩúÈáçÊñ∞ÊéíÂ∫èÂô®Êù•Á≤æÁ°ÆËøáÊª§Ââç k ‰∏™Ê£ÄÁ¥¢Âà∞ÁöÑÂõæÂÉè„ÄÇÂØπ‰∫éÁîüÊàêÔºåÊàë‰ª¨Âú®Êï∞ÊçÆÂíåÊ†áËÆ∞Á∫ßÂà´Âú®ËÆ≠ÁªÉÊúüÈó¥Ê≥®ÂÖ•ËßÜËßâÂô™Â£∞Ôºå‰ª•Â¢ûÂº∫ÁîüÊàêÂô®ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑÂ≠êÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåËøô‰∫õÊï∞ÊçÆÈõÜÈúÄË¶ÅÊ£ÄÁ¥¢ÂíåÊé®ÁêÜÂõæÂÉèÊâçËÉΩÂõûÁ≠îÁªôÂÆöÁöÑÊü•ËØ¢„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúËØÅÊòé‰∫Ü RagLLaVA Âú®ÂáÜÁ°ÆÊ£ÄÁ¥¢ÂíåÁ®≥ÂÅ•ÁîüÊàêÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂèØÂú® https://github.com/IDEA-FinAI/RagLLaVA Ëé∑Âæó„ÄÇ

##### **Deformable 3D Shape Diffusion Model**
2407.21428v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

The Gaussian diffusion model, initially designed for image generation, has
recently been adapted for 3D point cloud generation. However, these adaptations
have not fully considered the intrinsic geometric characteristics of 3D shapes,
thereby constraining the diffusion model's potential for 3D shape manipulation.
To address this limitation, we introduce a novel deformable 3D shape diffusion
model that facilitates comprehensive 3D shape manipulation, including point
cloud generation, mesh deformation, and facial animation. Our approach
innovatively incorporates a differential deformation kernel, which deconstructs
the generation of geometric structures into successive non-rigid deformation
stages. By leveraging a probabilistic diffusion model to simulate this
step-by-step process, our method provides a versatile and efficient solution
for a wide range of applications, spanning from graphics rendering to facial
expression animation. Empirical evidence highlights the effectiveness of our
approach, demonstrating state-of-the-art performance in point cloud generation
and competitive results in mesh deformation. Additionally, extensive visual
demonstrations reveal the significant potential of our approach for practical
applications. Our method presents a unique pathway for advancing 3D shape
manipulation and unlocking new opportunities in the realm of virtual reality.

ÊëòË¶ÅÔºöÈ´òÊñØÊì¥Êï£Ê®°ÂûãÊúÄÂàùÊòØÁÇ∫ÂΩ±ÂÉèÁîüÊàêËÄåË®≠Ë®àÔºåÊúÄËøëÂ∑≤ÊîπÁî®Êñº 3D ÈªûÈõ≤ÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊîπÁ∑®‰∏¶Êú™ÂÖÖÂàÜËÄÉÊÖÆ 3D ÂΩ¢ÁãÄÁöÑÂÖßÂú®Âπæ‰ΩïÁâπÂæµÔºåÂæûËÄåÈôêÂà∂‰∫ÜÊì¥Êï£Ê®°ÂûãÂú® 3D ÂΩ¢ÁãÄÊìç‰ΩúÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÈôêÂà∂ÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂèØËÆäÂΩ¢ 3D ÂΩ¢ÁãÄÊì¥Êï£Ê®°ÂûãÔºåË©≤Ê®°ÂûãÊúâÂä©ÊñºÈÄ≤Ë°åÂÖ®Èù¢ÁöÑ 3D ÂΩ¢ÁãÄÊìç‰ΩúÔºåÂåÖÊã¨ÈªûÈõ≤ÁîüÊàê„ÄÅÁ∂≤Ê†ºËÆäÂΩ¢ÂíåÈù¢ÈÉ®ÂãïÁï´„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂâµÊñ∞Âú∞Á¥çÂÖ•‰∫ÜÂæÆÂàÜËÆäÂΩ¢Ê†∏ÔºåÂ∞áÂπæ‰ΩïÁµêÊßãÁöÑÁîüÊàêËß£ÊßãÁÇ∫ÈÄ£Á∫åÁöÑÈùûÂâõÊÄßËÆäÂΩ¢ÈöéÊÆµ„ÄÇÈÄèÈÅéÂà©Áî®Ê©üÁéáÊì¥Êï£Ê®°Âûã‰æÜÊ®°Êì¨Ê≠§ÈÄêÊ≠•Á®ãÂ∫èÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂ§öÂäüËÉΩ‰∏îÊúâÊïàÁéáÁöÑËß£Ê±∫ÊñπÊ°àÔºåÈÅ©Áî®ÊñºÂæûÂúñÂΩ¢Ê∏≤ÊüìÂà∞Èù¢ÈÉ®Ë°®ÊÉÖÂãïÁï´ÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÁ∂ìÈ©óË≠âÊìöÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫ÜÂú®ÈªûÈõ≤ÁîüÊàê‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂú®Á∂≤Ê†ºËÆäÂΩ¢‰∏≠Áç≤ÂæóÁ´∂Áà≠ÂäõÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÂª£Ê≥õÁöÑË¶ñË¶∫Á§∫ÁØÑÊè≠Á§∫‰∫ÜÊàëÂÄëÂÅöÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁÇ∫Êé®ÈÄ≤ 3D ÂΩ¢ÁãÄÊìç‰ΩúÂíåÂú®ËôõÊì¨ÂØ¶Â¢ÉÈ†òÂüüÈñãÂïüÊñ∞Ê©üÊúÉÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑÈÄîÂæë„ÄÇ

##### **Cost-Effective Hallucination Detection for LLMs**
2407.21424v1 by Simon Valentin, Jinmiao Fu, Gianluca Detommaso, Shaoyuan Xu, Giovanni Zappella, Bryan Wang

Large language models (LLMs) can be prone to hallucinations - generating
unreliable outputs that are unfaithful to their inputs, external facts or
internally inconsistent. In this work, we address several challenges for
post-hoc hallucination detection in production settings. Our pipeline for
hallucination detection entails: first, producing a confidence score
representing the likelihood that a generated answer is a hallucination; second,
calibrating the score conditional on attributes of the inputs and candidate
response; finally, performing detection by thresholding the calibrated score.
We benchmark a variety of state-of-the-art scoring methods on different
datasets, encompassing question answering, fact checking, and summarization
tasks. We employ diverse LLMs to ensure a comprehensive assessment of
performance. We show that calibrating individual scoring methods is critical
for ensuring risk-aware downstream decision making. Based on findings that no
individual score performs best in all situations, we propose a multi-scoring
framework, which combines different scores and achieves top performance across
all datasets. We further introduce cost-effective multi-scoring, which can
match or even outperform more expensive detection methods, while significantly
reducing computational overhead.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÆπÊòìÂá∫ÁèæÂπªË¶∫ - ÁîüÊàê‰∏çÂø†ÊñºÂÖ∂Ëº∏ÂÖ•„ÄÅÂ§ñÈÉ®‰∫ãÂØ¶ÊàñÂÖßÈÉ®‰∏ç‰∏ÄËá¥ÁöÑ‰∏çÂèØÈù†Ëº∏Âá∫„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÁîüÁî¢Áí∞Â¢É‰∏≠‰∫ãÂæåÂπªË¶∫Ê™¢Ê∏¨ÁöÑÂπæÂÄãÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÂπªË¶∫Ê™¢Ê∏¨ÁÆ°ÈÅìÂåÖÊã¨ÔºöÈ¶ñÂÖàÔºåÁî¢Áîü‰∏ÄÂÄãÁΩÆ‰ø°Â∫¶ÂàÜÊï∏ÔºåË°®Á§∫ÁîüÊàêÁöÑÁ≠îÊ°àÊòØÂπªË¶∫ÁöÑÂèØËÉΩÊÄßÔºõÂÖ∂Ê¨°ÔºåÊ†πÊìöËº∏ÂÖ•ÂíåÂÄôÈÅ∏ÈüøÊáâÁöÑÂ±¨ÊÄßÊ†°Ê∫ñÂàÜÊï∏ÔºõÊúÄÂæåÔºåÈÄöÈÅéÂ∞çÊ†°Ê∫ñÂàÜÊï∏ÈÄ≤Ë°åÈñæÂÄºËôïÁêÜ‰æÜÂü∑Ë°åÊ™¢Ê∏¨„ÄÇÊàëÂÄëÂ∞ç‰∏çÂêåÁöÑË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂêÑÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑË©ïÂàÜÊñπÊ≥ïÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂåÖÊã¨ÂïèÁ≠î„ÄÅ‰∫ãÂØ¶Êü•Ê†∏ÂíåÊëòË¶Å‰ªªÂãô„ÄÇÊàëÂÄëÊé°Áî®‰∏çÂêåÁöÑ LLM ‰æÜÁ¢∫‰øùÂ∞çÊÄßËÉΩÈÄ≤Ë°åÂÖ®Èù¢Ë©ï‰º∞„ÄÇÊàëÂÄëË°®ÊòéÔºåÊ†°Ê∫ñÂÄãÂà•Ë©ïÂàÜÊñπÊ≥ïÂ∞çÊñºÁ¢∫‰øùÈ¢®Èö™ÊÑüÁü•ÁöÑ‰∏ãÊ∏∏Ê±∫Á≠ñÂà∂ÂÆöËá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÊ≤íÊúâ‰ªª‰ΩïÂÄãÂà•ÂàÜÊï∏Âú®ÊâÄÊúâÊÉÖÊ≥Å‰∏ãË°®ÁèæÊúÄ‰Ω≥ÁöÑÁôºÁèæÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öË©ïÂàÜÊ°ÜÊû∂ÔºåÂÆÉÁµêÂêà‰∫Ü‰∏çÂêåÁöÑÂàÜÊï∏Ôºå‰∏¶Âú®ÊâÄÊúâË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰∫ÜÂÖ∑ÊúâÊàêÊú¨ÊïàÁõäÁöÑÂ§öË©ïÂàÜÔºåÂÆÉÂèØ‰ª•ÂåπÈÖçÁîöËá≥ÂÑ™ÊñºÊõ¥ÊòÇË≤¥ÁöÑÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÂêåÊôÇÈ°ØËëóÈôç‰ΩéË®àÁÆóÈñãÈä∑„ÄÇ

##### **Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**
2407.21417v1 by Zhengxuan Wu, Yuhao Zhang, Peng Qi, Yumo Xu, Rujun Han, Yian Zhang, Jifan Chen, Bonan Min, Zhiheng Huang

Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.

ÊëòË¶ÅÔºöÁèæ‰ª£Ë™ûË®ÄÊ®°Âûã (LM) Âú®Âø†ÂØ¶ÁöÑÂêåÊôÇÈúÄË¶ÅÈÅµÂæ™‰∫∫È°ûÁöÑÊåáÁ§∫ÔºõÁÑ∂ËÄåÔºåÂÆÉÂÄëÂ∏∏Â∏∏ÁÑ°Ê≥ïÂêåÊôÇÂØ¶ÁèæÈÄôÂÖ©Èªû„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂÖ∑È´îË≠âÊìöÔºåË™™ÊòéÂú®‰ΩøÁî®ÈÄô‰∫õÁõÆÊ®ôË®ìÁ∑¥ LM ÊôÇÔºåÂú®ÈÅµÂæ™ÊåáÁ§∫ÔºàÂç≥ÈÅµÂæ™ÈñãÊîæÂºèÊåáÁ§∫ÔºâÂíåÂø†ÂØ¶Â∫¶ÔºàÂç≥Âú®Áµ¶ÂÆöÁöÑËÉåÊôØ‰∏≠Âª∫Á´ãÂõûÊáâÔºâ‰πãÈñìÂ≠òÂú®Ê¨äË°°„ÄÇ‰æãÂ¶ÇÔºåÂú®ÈÅµÂæ™ÊåáÁ§∫ÁöÑÊï∏ÊìöÈõÜ‰∏äÂæÆË™ø LLaMA-7B ÊúÉÈôç‰ΩéÂÖ∂Âø†ÂØ¶Â∫¶„ÄÇÁõ∏ÂèçÔºåÂú®ÈÄ≤‰∏ÄÊ≠•ÈáùÂ∞çÈúÄË¶Å‰∏ä‰∏ãÊñá‰æùÊìöÁöÑ‰ªªÂãôÈÄ≤Ë°åÂÑ™ÂåñÊôÇÔºåÁ∂ìÈÅéÊåáÁ§∫Ë™øÊï¥ÁöÑ Vicuna-7B Âú®ÈÅµÂæ™ÊåáÁ§∫ÊñπÈù¢ÁöÑË°®ÁèæÊúÉ‰∏ãÈôç„ÄÇ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑË£úÊïëÊé™ÊñΩÊòØ‰ΩøÁî®Êï∏ÊìöÊ∑∑ÂêàÁöÑÂ§ö‰ªªÂãôÂ≠∏Áøí (MTL)Ôºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÈÅ†Êú™ÈÅîÂà∞ÂçîÂêåÊïàÊáâ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂÆÉ‰æùË≥¥ÊñºÊãíÁµïÊé°Ê®£ÔºåÁî®ÊñºÊåÅÁ∫åËá™ÊåáÂ∞éË™øÊï¥ (ReSet)ÔºåÂÆÉÊòéÈ°ØÂÑ™ÊñºÈ¶ôËçâ MTL„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæÂ∞ëÂç≥ÊòØÂ§öÔºåÂõ†ÁÇ∫‰ΩøÁî®È´òÂìÅË≥™‰ΩÜÊï∏ÈáèÈ°ØËëóËºÉÂ∞ëÔºàÂ∞ë‰∏âÂÄçÔºâÁöÑÊï∏ÊìöË®ìÁ∑¥ ReSet ÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊúâÂä©ÊñºÊõ¥Â•ΩÂú∞ÁêÜËß£ LM Â∞çÈΩäË®ìÁ∑¥‰∏≠ÁöÑÁõÆÊ®ôÂ∑ÆÁï∞„ÄÇ

##### **Towards interfacing large language models with ASR systems using confidence measures and prompting**
2407.21414v1 by Maryam Naderi, Enno Hermann, Alexandre Nanchen, Sevada Hovsepyan, Mathew Magimai. -Doss

As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂèÉÊï∏Ë¶èÊ®°ÂíåÂäüËÉΩ‰∏ä‰∏çÊñ∑Â¢ûÈï∑Ôºå‰æãÂ¶ÇÈÄèÈÅéÊèêÁ§∫ÈÄ≤Ë°å‰∫íÂãïÔºåÂÆÉÂÄëÈñãÂïü‰∫ÜËàáËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) Á≥ªÁµ±‰∫íÂãïÁöÑÊñ∞ÊñπÂºèÔºåËÄå‰∏çÂÜçÂÉÖÈôêÊñºÈáçÊñ∞Ë©ïÂàÜ n ÂÄãÊúÄ‰Ω≥Ê∏ÖÂñÆ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü‰ΩøÁî® LLM Â∞ç ASR ËΩâÈåÑÈÄ≤Ë°å‰∫ãÂæåÊ†°Ê≠£„ÄÇÁÇ∫‰∫ÜÈÅøÂÖçÂú®ÂèØËÉΩÊ∫ñÁ¢∫ÁöÑËΩâÈåÑ‰∏≠ÂºïÂÖ•ÈåØË™§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Êñº‰ø°ÂøÉÁöÑÈÅéÊøæÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÂèØ‰ª•ÊèêÂçáÁ´∂Áà≠ÂäõËºÉ‰ΩéÁöÑ ASR Á≥ªÁµ±ÁöÑÊïàËÉΩ„ÄÇ

##### **GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**
2407.21384v1 by Yanxu Mao, Peipei Liu, Tiehan Cui

Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.

ÊëòË¶ÅÔºö<paragraph>Êñá‰ª∂Á¥öÈóú‰øÇËêÉÂèñ (DocRE) Êó®Âú®ÂæûÈùûÁµêÊßãÊñá‰ª∂ÊñáÂ≠ó‰∏≠ËêÉÂèñÂØ¶È´îÈñìÁöÑÈóú‰øÇ„ÄÇËàáÂè•Â≠êÁ¥öÈóú‰øÇËêÉÂèñÁõ∏ÊØîÔºåÂÆÉÈúÄË¶ÅÂæûÊõ¥Âª£Ê≥õÁöÑÊñáÂ≠óËÑàÁµ°‰∏≠ÈÄ≤Ë°åÊõ¥Ë§áÈõúÁöÑË™ûÁæ©ÁêÜËß£„ÄÇÁõÆÂâçÔºå‰∏Ä‰∫õÁ†îÁ©∂Âà©Áî®Ë≠âÊìöÂè•Â≠ê‰∏≠ÁöÑÈÇèËºØË¶èÂâá‰æÜÂ¢ûÂº∑ DocRE ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®Ê≤íÊúâÊèê‰æõË≠âÊìöÂè•Â≠êÁöÑË≥áÊñô‰∏≠ÔºåÁ†îÁ©∂‰∫∫Âì°ÈÄöÂ∏∏ÊúÉÈÄèÈÅéË≠âÊìöÊì∑Âèñ (ER) ÁÇ∫Êï¥ÂÄãÊñá‰ª∂ÂèñÂæó‰∏Ä‰ªΩË≠âÊìöÂè•Â≠êÊ∏ÖÂñÆ„ÄÇÂõ†Ê≠§ÔºåDocRE Èù¢Ëá®ÂÖ©È†ÖÊåëÊà∞ÔºöÈ¶ñÂÖàÔºåË≠âÊìöÂíåÂØ¶È´îÂ∞ç‰πãÈñìÁöÑÈóúËÅØÊÄßËºÉÂº±ÔºõÂÖ∂Ê¨°ÔºåÁÑ°Ê≥ïÂÖÖÂàÜËêÉÂèñÈÅ†Ë∑ùÈõ¢Â§öÂØ¶È´î‰πãÈñìÁöÑË§áÈõú‰∫§ÂèâÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ GEGAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®Êñº DocRE ÁöÑÊñ∞ÂûãÊ®°Âûã„ÄÇË©≤Ê®°ÂûãÂà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂª∫ÊßãÂ§öÂÄãÊ¨äÈáçÁü©Èô£ÔºåÂºïÂ∞éÊ≥®ÊÑèÂäõÂàÜÈÖçÂà∞Ë≠âÊìöÂè•Â≠ê„ÄÇÂÆÉÈÇÑÊé°Áî®Â§öÂ∞∫Â∫¶Ë°®Á§∫ËÅöÂêà‰æÜÂ¢ûÂº∑ ER„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊï¥ÂêàÊúÄÊúâÊïàÁöÑË≠âÊìöË≥áË®äÔºåÁÇ∫Ê®°ÂûãÂØ¶‰ΩúÂÆåÂÖ®Áõ£Áù£ÂºèÂíåÂº±Áõ£Áù£ÂºèË®ìÁ∑¥ÊµÅÁ®ã„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÔºöDocRED„ÄÅRe-DocRED Âíå Revisit-DocRED ‰∏äË©ï‰º∞ GEGA Ê®°Âûã„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÁöÑ SOTA Ê®°ÂûãÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ∑≤Áç≤ÂæóÂÖ®Èù¢ÁöÑÊîπÈÄ≤„ÄÇ</paragraph>

##### **An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**
2407.21376v1 by Hongxun Zhou, Xiangyu Chen, Ye Yuan

A dynamic weighted directed graph (DWDG) is commonly encountered in various
application scenarios. It involves extensive dynamic interactions among
numerous nodes. Most existing approaches explore the intricate temporal
patterns hidden in a DWDG from the purely data-driven perspective, which
suffers from accuracy loss when a DWDG exhibits strong fluctuations over time.
To address this issue, this study proposes a novel
Extended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a
DWDG from the model-driven perspective. Its main idea is divided into the
following two-fold ideas: a) adopting a control model, i.e., the Extended
Kalman Filter (EKF), to track the complex temporal patterns precisely with its
nonlinear state-transition and observation functions; and b) introducing an
alternating least squares (ALS) algorithm to train the latent features (LFs)
alternatively for precisely representing a DWDG. Empirical studies on DWDG
datasets demonstrate that the proposed EKLF model outperforms state-of-the-art
models in prediction accuracy and computational efficiency for missing edge
weights of a DWDG. It unveils the potential for precisely representing a DWDG
by incorporating a control model.

ÊëòË¶ÅÔºöÂãïÊÖãÂä†Ê¨äÊúâÂêëÂúñ (DWDG) Â∏∏Ë¶ãÊñºÂêÑÁ®ÆÊáâÁî®ÊÉÖÂ¢É‰∏≠„ÄÇÂÆÉÊ∂âÂèäÁúæÂ§öÁØÄÈªû‰πãÈñìÂª£Ê≥õÁöÑÂãïÊÖã‰∫§‰∫í„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÂæûÁ¥îÁ≤πÁöÑÊï∏ÊìöÈ©ÖÂãïËßÄÈªûÊé¢Á¥¢Èö±ËóèÂú® DWDG ‰∏≠ÁöÑË§áÈõúÊôÇÈñìÊ®°ÂºèÔºåÁï∂ DWDG Èö®ÊôÇÈñìÊé®ÁßªÂá∫ÁèæÂäáÁÉàÊ≥¢ÂãïÊôÇÔºåÈÄôÁ®ÆÊñπÊ≥ïÊúÉÂ∞éËá¥Ê∫ñÁ¢∫Â∫¶‰∏ãÈôç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊì¥Â±ïÂç°ÁàæÊõºÊøæÊ≥¢Âô®ÁµêÂêàÊΩõÂú®ÁâπÂæµ (EKLF) Ê®°ÂûãÔºåÂæûÊ®°ÂûãÈ©ÖÂãïÁöÑËßíÂ∫¶Ë°®Á§∫ DWDG„ÄÇÂÖ∂‰∏ªË¶ÅÊÄùÊÉ≥ÂèØÂàÜÁÇ∫‰ª•‰∏ãÂÖ©ÂÄãÊñπÈù¢Ôºöa) Êé°Áî®ÊéßÂà∂Ê®°ÂûãÔºåÂç≥Êì¥Â±ïÂç°ÁàæÊõºÊøæÊ≥¢Âô® (EKF)Ôºå‰ª•ÂÖ∂ÈùûÁ∑öÊÄßÁãÄÊÖãËΩâÁßªÂíåËßÄÊ∏¨ÂáΩÊï∏Á≤æÁ¢∫ËøΩËπ§Ë§áÈõúÁöÑÊôÇÈñìÊ®°ÂºèÔºõb) Â∞éÂÖ•‰∫§ÊõøÊúÄÂ∞è‰∫å‰πò (ALS) ÊºîÁÆóÊ≥ïÔºå‰∫§ÊõøË®ìÁ∑¥ÊΩõÂú®ÁâπÂæµ (LF)Ôºå‰ª•Á≤æÁ¢∫Ë°®Á§∫ DWDG„ÄÇÈáùÂ∞ç DWDG Ë≥áÊñôÈõÜÁöÑÂØ¶Ë≠âÁ†îÁ©∂Ë°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑ EKLF Ê®°ÂûãÂú®È†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÂíåË®àÁÆóÊïàÁéáÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÁî®ÊñºÈ†êÊ∏¨ DWDG ÁöÑÁº∫Â§±ÈÇäÊ¨äÈáç„ÄÇÂÆÉÊè≠Á§∫‰∫ÜÈÄèÈÅéÁµêÂêàÊéßÂà∂Ê®°ÂûãÁ≤æÁ¢∫Ë°®Á§∫ DWDG ÁöÑÊΩõÂäõ„ÄÇ

##### **Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**
2407.21368v1 by Danfeng Guo, Demetri Terzopoulos

Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.

ÊëòË¶ÅÔºöÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) Âú®ËøëÂπ¥‰æÜÂèñÂæóÈ°ØËëóÁöÑÊàêÂäüÔºå‰∏¶Â∑≤Êì¥Â±ïÂà∞ÈÜ´ÁôÇÈ†òÂüü„ÄÇÂÑòÁÆ°Âú®ÈÜ´Â≠∏Ë¶ñË¶∫ÂïèÁ≠î (VQA) ‰ªªÂãô‰∏≠Ë°®Áèæ‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜÈÜ´Â≠∏ LVLMs (MLVLMs) ‰ªçÂ≠òÂú®ÂπªË¶∫ÂïèÈ°åÔºåÂ∞éËá¥ÂÆÉÂÄëÁÑ°Ê≥ïË®∫Êñ∑Âá∫Ë§áÈõúÁöÑÁóÖÁêÜ„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñô‰∏çÂπ≥Ë°°ÔºåÂÆÉÂÄëÂæàÂÆπÊòìÁÑ°Ê≥ïÂ≠∏ÁøíÂ∞ëÊï∏ÁóÖÁêÜ„ÄÇÊàëÂÄëÊèêÂá∫ÂÖ©Á®ÆÈáùÂ∞ç MLVLMs ÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰ª•Ê∏õÂ∞ëÂπªË¶∫‰∏¶ÊîπÂñÑ VQA ÊïàËÉΩ„ÄÇÂú®Á¨¨‰∏ÄÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÊèê‰æõÊü•Ë©¢ÁóÖÁêÜÁöÑË©≥Á¥∞Ë™™Êòé„ÄÇÂú®Á¨¨‰∫åÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÂæÆË™ø‰∏ÄÂÄã‰æøÂÆú„ÄÅÊïàËÉΩ‰∏ç‰Ω≥ÁöÑÂ≠∏ÁøíÂô®Ôºå‰ª•Âú®ÁâπÂÆöÊåáÊ®ô‰∏äÁç≤ÂæóÈ´òÊïàËÉΩÔºå‰∏¶‰ª•ÊñáÂ≠óÊñπÂºèÂêë MLVLM Êèê‰æõÂÖ∂Âà§Êñ∑„ÄÇÂú® MIMIC-CXR-JPG Âíå Chexpert Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÂæåÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÊîπÂñÑ‰∫ÜË®∫Êñ∑ F1 ÂàÜÊï∏ÔºåÊúÄÈ´òÊèêÂçáÂπÖÂ∫¶ÁÇ∫ 0.27„ÄÇÊàëÂÄëÈÇÑÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊèêÁ§∫Á≠ñÁï•ÂèØ‰ª•Êì¥Â±ïÂà∞‰∏ÄËà¨ÁöÑ LVLM È†òÂüü„ÄÇÊ†πÊìö POPE ÊåáÊ®ôÔºåÂÆÉÊúâÊïàÂú∞ÊäëÂà∂‰∫ÜÁèæÊúâ LVLMs ÁöÑÂÅáÈô∞ÊÄßÈ†êÊ∏¨Ôºå‰∏¶Â∞áÂè¨ÂõûÁéáÊèêÈ´ò‰∫ÜÁ¥Ñ 0.07„ÄÇ

##### **ProSpec RL: Plan Ahead, then Execute**
2407.21359v1 by Liangliang Liu, Yi Guan, BoRan Wang, Rujia Shen, Yi Lin, Chaoran Kong, Lian Yan, Jingchi Jiang

Imagining potential outcomes of actions before execution helps agents make
more informed decisions, a prospective thinking ability fundamental to human
cognition. However, mainstream model-free Reinforcement Learning (RL) methods
lack the ability to proactively envision future scenarios, plan, and guide
strategies. These methods typically rely on trial and error to adjust policy
functions, aiming to maximize cumulative rewards or long-term value, even if
such high-reward decisions place the environment in extremely dangerous states.
To address this, we propose the Prospective (ProSpec) RL method, which makes
higher-value, lower-risk optimal decisions by imagining future n-stream
trajectories. Specifically, ProSpec employs a dynamic model to predict future
states (termed "imagined states") based on the current state and a series of
sampled actions. Furthermore, we integrate the concept of Model Predictive
Control and introduce a cycle consistency constraint that allows the agent to
evaluate and select the optimal actions from these trajectories. Moreover,
ProSpec employs cycle consistency to mitigate two fundamental issues in RL:
augmenting state reversibility to avoid irreversible events (low risk) and
augmenting actions to generate numerous virtual trajectories, thereby improving
data efficiency. We validated the effectiveness of our method on the DMControl
benchmarks, where our approach achieved significant performance improvements.
Code will be open-sourced upon acceptance.

ÊëòË¶ÅÔºöÂú®Âü∑Ë°åÂãï‰ΩúÂâçÊÉ≥ÂÉèÂÖ∂ÊΩõÂú®ÁµêÊûúÔºåÊúâÂä©Êñº‰ª£ÁêÜ‰∫∫ÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞ç‰∫∫È°ûË™çÁü•Ëá≥ÈóúÈáçË¶ÅÁöÑÂâçÁûªÊÄßÊÄùËÄÉËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑÁÑ°Ê®°ÂûãÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÁº∫‰πè‰∏ªÂãïË®≠ÊÉ≥Êú™‰æÜÂ†¥ÊôØ„ÄÅË¶èÂäÉÂíåÊåáÂ∞éÁ≠ñÁï•ÁöÑËÉΩÂäõ„ÄÇÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰æùË≥¥ÊñºË©¶ÈåØ‰æÜË™øÊï¥Á≠ñÁï•ÂáΩÊï∏ÔºåÊó®Âú®ÊúÄÂ§ßÂåñÁ¥ØÁ©çÁçéÂãµÊàñÈï∑ÊúüÂÉπÂÄºÔºåÂç≥‰ΩøÈÄôÊ®£ÁöÑÁç≤ÂèñÈ´òÁçéÂãµÊ±∫Á≠ñÊúÉËÆìÁí∞Â¢ÉËôïÊñºÊ•µÂ∫¶Âç±Èö™ÁöÑÁãÄÊÖã„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂâçÁûªÊÄß (ProSpec) RL ÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÊÉ≥ÂÉèÊú™‰æÜÁöÑ n ‰∏≤ÊµÅËªåË∑°‰æÜÂÅöÂá∫ÂÉπÂÄºÊõ¥È´ò„ÄÅÈ¢®Èö™Êõ¥‰ΩéÁöÑÊúÄ‰Ω≥Ê±∫Á≠ñ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåProSpec ‰ΩøÁî®ÂãïÊÖãÊ®°ÂûãÊ†πÊìöÁï∂ÂâçÁãÄÊÖãÂíå‰∏ÄÁ≥ªÂàóÊé°Ê®£Âãï‰Ωú‰æÜÈ†êÊ∏¨Êú™‰æÜÁöÑÁãÄÊÖãÔºàÁ®±ÁÇ∫„ÄåÊÉ≥ÂÉèÁãÄÊÖã„ÄçÔºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊï¥Âêà‰∫ÜÊ®°ÂûãÈ†êÊ∏¨ÊéßÂà∂ÁöÑÊ¶ÇÂøµÔºå‰∏¶ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÈÄ±Êúü‰∏ÄËá¥ÊÄßÁ¥ÑÊùüÔºåÂÖÅË®±‰ª£ÁêÜ‰∫∫ÂæûÈÄô‰∫õËªåË∑°‰∏≠Ë©ï‰º∞ÂíåÈÅ∏ÊìáÊúÄ‰Ω≥Âãï‰Ωú„ÄÇÊ≠§Â§ñÔºåProSpec ‰ΩøÁî®ÈÄ±Êúü‰∏ÄËá¥ÊÄß‰æÜÁ∑©Ëß£ RL ‰∏≠ÁöÑÂÖ©ÂÄãÂü∫Êú¨ÂïèÈ°åÔºöÂ¢ûÂä†ÁãÄÊÖãÂèØÈÄÜÊÄß‰ª•ÈÅøÂÖç‰∏çÂèØÈÄÜ‰∫ã‰ª∂Ôºà‰ΩéÈ¢®Èö™ÔºâÂíåÂ¢ûÂä†Âãï‰Ωú‰ª•Áî¢ÁîüÂ§ßÈáèÁöÑËôõÊì¨ËªåË∑°ÔºåÂæûËÄåÊèêÈ´òË≥áÊñôÊïàÁéá„ÄÇÊàëÂÄëÂú® DMControl Âü∫Ê∫ñ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂú®Ë¢´Êé•ÂèóÂæåÈñãÊ∫ê„ÄÇ

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÂèØÈù†„ÄÅÁµêÊßãÂåñ„ÄÅÁâπÂÆöÊñºÈ†òÂüü‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•Ë≠òÔºå‰æÜË£úÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåKG Âíå LLM ÈÄöÂ∏∏ÊòØÂàÜÈñãÈñãÁôºÔºå‰∏¶‰∏îÂøÖÈ†àÂú®Ë®ìÁ∑¥ÂæåÊï¥Âêà„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü Tree-of-TraversalsÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Êé®ÁêÜÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩËÆìÈªëÁõí LLM ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄã KG„ÄÇË©≤ÊºîÁÆóÊ≥ïÁÇ∫ LLM Êèê‰æõËàá KG ‰ªãÈù¢ÁöÑÂãï‰ΩúÔºå‰∏¶ËÆì LLM ËÉΩÂú®ÂèØËÉΩÁöÑÊÄùËÄÉÂíåÂãï‰Ωú‰∏äÂü∑Ë°åÊ®πÁãÄÊêúÂ∞ãÔºå‰ª•ÊâæÂá∫È´òÂ∫¶‰ø°ÂøÉÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁÜ±ÈñÄÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåTree-of-Traversals Â§ßÂπÖÊèêÂçá‰∫ÜÂïèÈ°åËß£Á≠îÂíå KG ÂïèÈ°åËß£Á≠î‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/amazon-science/tree-of-traversals} ÂèñÂæó

##### **Small Object Few-shot Segmentation for Vision-based Industrial Inspection**
2407.21351v1 by Zilong Zhang, Chang Niu, Zhibin Zhao, Xingwu Zhang, Xuefeng Chen

Vision-based industrial inspection (VII) aims to locate defects quickly and
accurately. Supervised learning under a close-set setting and industrial
anomaly detection, as two common paradigms in VII, face different problems in
practical applications. The former is that various and sufficient defects are
difficult to obtain, while the latter is that specific defects cannot be
located. To solve these problems, in this paper, we focus on the few-shot
semantic segmentation (FSS) method, which can locate unseen defects conditioned
on a few annotations without retraining. Compared to common objects in natural
images, the defects in VII are small. This brings two problems to current FSS
methods: 1 distortion of target semantics and 2 many false positives for
backgrounds. To alleviate these problems, we propose a small object few-shot
segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the
resizing of the original image and correctly indicate the intensity of target
semantics. SOFS achieves this idea via the non-resizing procedure and the
prototype intensity downsampling of support annotations. To alleviate 2, we
design an abnormal prior map in SOFS to guide the model to reduce false
positives and propose a mixed normal Dice loss to preferentially prevent the
model from predicting false positives. SOFS can achieve FSS and few-shot
anomaly detection determined by support masks. Diverse experiments substantiate
the superior performance of SOFS. Code is available at
https://github.com/zhangzilongc/SOFS.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºË¶ñË¶∫ÁöÑÂ∑•Ê•≠Ê™¢Ê∏¨ (VII) Êó®Âú®Âø´ÈÄü‰∏îÊ∫ñÁ¢∫Âú∞ÊâæÂá∫Áº∫Èô∑„ÄÇÊúâÁõ£Áù£Â≠∏ÁøíÂú®Â∞ÅÈñâÂºèË®≠ÂÆö‰∏ãÂíåÂ∑•Ê•≠Áï∞Â∏∏ÂÅµÊ∏¨Ôºå‰ΩúÁÇ∫ VII ‰∏≠ÂÖ©ÂÄãÂ∏∏Ë¶ãÁöÑÁØÑ‰æãÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠Èù¢Ëá®‰∏çÂêåÁöÑÂïèÈ°å„ÄÇÂâçËÄÖÊòØÈõ£‰ª•ÂèñÂæóÂêÑÁ®Æ‰∏îË∂≥Â§†ÁöÑÁº∫Èô∑ÔºåËÄåÂæåËÄÖÂâáÊòØÁÑ°Ê≥ïÊâæÂá∫ÁâπÂÆöÁöÑÁº∫Èô∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂ∞ëÊ®£Êú¨Ë™ûÊÑèÂàÜÂâ≤ (FSS) ÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Âú®‰∏çÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊ†πÊìöÂ∞ëÊï∏Ë®ªËß£ÊâæÂá∫Êú™Ë¶ãÈÅéÁöÑÁº∫Èô∑„ÄÇËàáËá™ÁÑ∂ÂΩ±ÂÉè‰∏≠ÁöÑÂ∏∏Ë¶ãÁâ©‰ª∂Áõ∏ÊØîÔºåVII ‰∏≠ÁöÑÁº∫Èô∑ÂæàÂ∞è„ÄÇÈÄôÁÇ∫ÁõÆÂâçÁöÑ FSS ÊñπÊ≥ïÂ∏∂‰æÜÂÖ©ÂÄãÂïèÈ°åÔºö1 ÁõÆÊ®ôË™ûÊÑèÁöÑÂ§±ÁúüÂíå 2 ËÉåÊôØÁöÑË®±Â§öÂÅáÈôΩÊÄß„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ∞èÂûãÁâ©‰ª∂Â∞ëÊ®£Êú¨ÂàÜÂâ≤ (SOFS) Ê®°Âûã„ÄÇÊ∏õËºï 1 ÁöÑÈóúÈçµÊÉ≥Ê≥ïÊòØÈÅøÂÖçË™øÊï¥ÂéüÂßãÂΩ±ÂÉèÁöÑÂ§ßÂ∞èÔºå‰∏¶Ê≠£Á¢∫ÊåáÂá∫ÁõÆÊ®ôË™ûÊÑèÁöÑÂº∑Â∫¶„ÄÇSOFS ÈÄèÈÅéÈùûË™øÊï¥Â§ßÂ∞èÁöÑÁ®ãÂ∫èÂíåÊîØÊè¥Ë®ªËß£ÁöÑÂéüÂûãÂº∑Â∫¶‰∏ãÊé°Ê®£‰æÜÈÅîÊàêÈÄôÂÄãÊÉ≥Ê≥ï„ÄÇÁÇ∫‰∫ÜÊ∏õËºï 2ÔºåÊàëÂÄëÂú® SOFS ‰∏≠Ë®≠Ë®à‰∏ÄÂÄãÁï∞Â∏∏ÂÖàÈ©óÂú∞ÂúñÔºåÂºïÂ∞éÊ®°ÂûãÊ∏õÂ∞ëÂÅáÈôΩÊÄßÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÊ≠£Ë¶è Dice ÊêçÂ§±Ôºå‰ª•ÂÑ™ÂÖàÈò≤Ê≠¢Ê®°ÂûãÈ†êÊ∏¨ÂÅáÈôΩÊÄß„ÄÇSOFS ÂèØ‰ª•ÈÅîÊàêÁî±ÊîØÊè¥ÈÅÆÁΩ©Ê±∫ÂÆöÁöÑ FSS ÂíåÂ∞ëÊ®£Êú¨Áï∞Â∏∏ÂÅµÊ∏¨„ÄÇÂêÑÁ®ÆÂØ¶È©óË≠âÂØ¶‰∫Ü SOFS ÁöÑÂÑ™Áï∞ÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/zhangzilongc/SOFS ÂèñÂæó„ÄÇ</paragraph>

##### **Differentially Private Block-wise Gradient Shuffle for Deep Learning**
2407.21347v1 by David Zagardo

Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)
introduces statistical noise on top of gradients drawn from a Gaussian
distribution to ensure privacy. This paper introduces the novel Differentially
Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.
BloGS builds off of existing private deep learning literature, but makes a
definitive shift by taking a probabilistic approach to gradient noise
introduction through shuffling modeled after information theoretic privacy
analyses. The theoretical results presented in this paper show that the
combination of shuffling, parameter-specific block size selection, batch layer
clipping, and gradient accumulation allows DP-BloGS to achieve training times
close to that of non-private training while maintaining similar privacy and
utility guarantees to DP-SGD. DP-BloGS is found to be significantly more
resistant to data extraction attempts than DP-SGD. The theoretical results are
validated by the experimental findings.

ÊëòË¶ÅÔºöÂÇ≥Áµ±Â∑ÆÂàÜÁßÅ‰∫∫Èö®Ê©üÊ¢ØÂ∫¶‰∏ãÈôçÔºàDP-SGDÔºâ
Âú®ÂæûÈ´òÊñØÂàÜ‰Ωà‰∏≠Áπ™Ë£ΩÁöÑÊ¢ØÂ∫¶‰∏äÂºïÂÖ•Áµ±Ë®àÂô™ËÅ≤‰ª•Á¢∫‰øùÈö±ÁßÅ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÁî®ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÖ®Êñ∞Â∑ÆÂàÜÁßÅ‰∫∫ÂçÄÂ°äÊ¢ØÂ∫¶Ê∑∑Ê¥óÔºàDP-BloGSÔºâÊºîÁÆóÊ≥ï„ÄÇ
BloGS Âª∫ÊßãÊñºÁèæÊúâÁöÑÁßÅ‰∫∫Ê∑±Â∫¶Â≠∏ÁøíÊñáÁçª‰πã‰∏äÔºå‰ΩÜÈÄèÈÅéÊé°Áî®Ê©üÁéáÊñπÊ≥ï‰æÜÂºïÂÖ•Ê¢ØÂ∫¶Âô™ËÅ≤Ôºå‰∏¶ÈÄèÈÅéÊ®°Êì¨Ë≥áË®äÁêÜË´ñÈö±ÁßÅÂàÜÊûê‰∏≠ÁöÑÊ¥óÁâåÈÄ≤Ë°åÂª∫Ê®°ÔºåÂæûËÄåÂÅöÂá∫ÊòéÁ¢∫ÁöÑËΩâËÆä„ÄÇÊú¨Êñá‰∏≠ÊèêÂá∫ÁöÑÁêÜË´ñÁµêÊûúË°®ÊòéÔºåÊ¥óÁâå„ÄÅÁâπÂÆöÊñºÂèÉÊï∏ÁöÑÂçÄÂ°äÂ§ßÂ∞èÈÅ∏Êìá„ÄÅÊâπÊ¨°Â±§Ë£ÅÂâ™ÂíåÊ¢ØÂ∫¶Á¥ØÁ©çÁöÑÁµÑÂêà‰Ωø DP-BloGS ËÉΩÂ§†ÂØ¶ÁèæÊé•ËøëÊñºÈùûÁßÅ‰∫∫Ë®ìÁ∑¥ÁöÑË®ìÁ∑¥ÊôÇÈñìÔºåÂêåÊôÇ‰øùÊåÅËàá DP-SGD È°û‰ººÁöÑÈö±ÁßÅÂíåÊïàÁî®‰øùË≠â„ÄÇÁôºÁèæ DP-BloGS ÊØî DP-SGD Â∞çË≥áÊñôÊèêÂèñÂòóË©¶ÂÖ∑ÊúâÈ°ØËëóÊõ¥È´òÁöÑÊäµÊäóÂäõ„ÄÇÂØ¶È©óÁµêÊûúÈ©óË≠â‰∫ÜÁêÜË´ñÁµêÊûú„ÄÇ

##### **Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**
2407.21344v1 by Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah

There has been a significant focus on modelling emotion ambiguity in recent
years, with advancements made in representing emotions as distributions to
capture ambiguity. However, there has been comparatively less effort devoted to
the consideration of temporal dependencies in emotion distributions which
encodes ambiguity in perceived emotions that evolve smoothly over time.
Recognizing the benefits of using constrained dynamical neural ordinary
differential equations (CD-NODE) to model time series as dynamic processes, we
propose an ambiguity-aware dual-constrained Neural ODE approach to model the
dynamics of emotion distributions on arousal and valence. In our approach, we
utilize ODEs parameterised by neural networks to estimate the distribution
parameters, and we integrate additional constraints to restrict the range of
the system outputs to ensure the validity of predicted distributions. We
evaluated our proposed system on the publicly available RECOLA dataset and
observed very promising performance across a range of evaluation metrics.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÊÉÖÁ∑íÊ®°Á≥äÂª∫Ê®°ÂÇôÂèóÈóúÊ≥®ÔºåÂú®Ë°®Á§∫ÊÉÖÁ∑íÁÇ∫ÂàÜ‰Ωà‰ª•ÊçïÊçâÊ®°Á≥äÊÄßÊñπÈù¢ÂèñÂæóÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåÁõ∏Â∞çËÄåË®ÄÔºåËºÉÂ∞ëÈóúÊ≥®ÊÉÖÁ∑íÂàÜ‰Ωà‰∏≠ÁöÑÊôÇÈñì‰æùË≥¥ÊÄßÔºåËÄåÊôÇÈñì‰æùË≥¥ÊÄßÁ∑®Á¢º‰∫ÜÈö®ËëóÊôÇÈñìÊé®ÁßªËÄåÂπ≥Á©©ÊºîËÆäÁöÑÊÑüÁü•ÊÉÖÁ∑í‰∏≠ÁöÑÊ®°Á≥äÊÄß„ÄÇË™çË≠òÂà∞‰ΩøÁî®Á¥ÑÊùüÂãïÊÖãÁ•ûÁ∂ìÂ∏∏ÂæÆÂàÜÊñπÁ®ã (CD-NODE) Â∞áÊôÇÈñìÂ∫èÂàóÂª∫Ê®°ÁÇ∫ÂãïÊÖãÈÅéÁ®ãÁöÑÂ•ΩËôïÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°Á≥äÊÑüÁü•ÈõôÁ¥ÑÊùüÁ•ûÁ∂ì ODE ÊñπÊ≥ïÔºå‰ª•Âª∫Ê®°ÂñöÈÜíÂíåÊïàÂÉπÁöÑÊÉÖÁ∑íÂàÜ‰ΩàÂãïÊÖã„ÄÇÂú®ÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏≠ÔºåÊàëÂÄëÂà©Áî®Á•ûÁ∂ìÁ∂≤Ë∑ØÂèÉÊï∏ÂåñÁöÑ ODE ‰æÜ‰º∞Ë®àÂàÜ‰ΩàÂèÉÊï∏Ôºå‰∏¶Êï¥ÂêàÈ°çÂ§ñÁöÑÁ¥ÑÊùüÊ¢ù‰ª∂‰æÜÈôêÂà∂Á≥ªÁµ±Ëº∏Âá∫ÁöÑÁØÑÂúçÔºå‰ª•Á¢∫‰øùÈ†êÊ∏¨ÂàÜ‰ΩàÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂú®ÂÖ¨ÈñãÁöÑ RECOLA Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÁ≥ªÁµ±Ôºå‰∏¶Âú®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏≠ËßÄÂØüÂà∞ÈùûÂ∏∏ÊúâÂ∏åÊúõÁöÑË°®Áèæ„ÄÇ

##### **Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**
2407.21338v1 by David Valencia, Henry Williams, Yuning Xing, Trevor Gee, Minas Liarokapis, Bruce A. MacDonald

Reinforcement Learning (RL) has been widely used to solve tasks where the
environment consistently provides a dense reward value. However, in real-world
scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are
indispensable for discovering efficient exploration strategies and aiding the
learning process. In this work, inspired by intrinsic motivation theory, we
postulate that the intrinsic stimuli of novelty and surprise can assist in
improving exploration in complex, sparsely rewarded environments. We introduce
a novel sample-efficient method able to learn directly from pixels, an
image-based extension of TD3 with an autoencoder called \textit{NaSA-TD3}. The
experiments demonstrate that NaSA-TD3 is easy to train and an efficient method
for tackling complex continuous-control robotic tasks, both in simulated
environments and real-world settings. NaSA-TD3 outperforms existing
state-of-the-art RL image-based methods in terms of final performance without
requiring pre-trained models or human demonstrations.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏Áøí (RL) Â∑≤Âª£Ê≥õÁî®ÊñºËß£Ê±∫Áí∞Â¢ÉÊåÅÁ∫åÊèê‰æõÂØÜÈõÜÁçéÂãµÂÄºÁöÑ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠ÔºåÁçéÂãµÈÄöÂ∏∏ÂèØËÉΩÂÆöÁæ©‰∏ç‰Ω≥ÊàñÁ®ÄÁñè„ÄÇËºîÂä©Ë®äËôüÂ∞çÊñºÁôºÁèæÊúâÊïàÁöÑÊé¢Á¥¢Á≠ñÁï•ÂíåÂçîÂä©Â≠∏ÁøíÈÅéÁ®ãÊòØ‰∏çÂèØÊàñÁº∫ÁöÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÂèóÂà∞ÂÖßÂú®ÂãïÊ©üÁêÜË´ñÁöÑÂïüÁôºÔºåÊàëÂÄëÂÅáË®≠Êñ∞Â•áÂíåÈ©öÂ•áÁöÑÂÖßÂú®Âà∫ÊøÄÊúâÂä©ÊñºÊîπÂñÑÂú®Ë§áÈõú„ÄÅÁçéÂãµÁ®ÄÁñèÁöÑÁí∞Â¢É‰∏≠Êé¢Á¥¢„ÄÇÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®£Êú¨ÊúâÊïàÁéáÊñπÊ≥ïÔºåËÉΩÂ§†Áõ¥Êé•ÂæûÂÉèÁ¥†Â≠∏ÁøíÔºå‰∏ÄÁ®Æ TD3 ÁöÑÂü∫ÊñºÂΩ±ÂÉèÁöÑÂª∂‰º∏ÔºåÂ∏∂ÊúâÁ®±ÁÇ∫ \textit{NaSA-TD3} ÁöÑËá™ÂãïÁ∑®Á¢ºÂô®„ÄÇÂØ¶È©óË≠âÊòéÔºåNaSA-TD3 ÊòìÊñºË®ìÁ∑¥Ôºå‰∏îÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®ÊñºËß£Ê±∫Ë§áÈõúÁöÑÈÄ£Á∫åÊéßÂà∂Ê©üÂô®‰∫∫‰ªªÂãôÔºåÁÑ°Ë´ñÊòØÂú®Ê®°Êì¨Áí∞Â¢ÉÊàñÁúüÂØ¶‰∏ñÁïåË®≠ÂÆö‰∏≠„ÄÇNaSA-TD3 Âú®ÊúÄÁµÇÊïàËÉΩÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ RL Âü∫ÊñºÂΩ±ÂÉèÁöÑÊñπÊ≥ïÔºåËÄå‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÊàñ‰∫∫È°ûÁ§∫ÁØÑ„ÄÇ

##### **Performance of Recent Large Language Models for a Low-Resourced Language**
2407.21330v1 by Ravindu Jayakody, Gihan Dias

Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÅéÂéª‰∏ÄÂπ¥‰∏≠Â±ïÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈô§‰∫Ü GPT Âíå Llama ÁöÑÊñ∞ÁâàÊú¨‰πãÂ§ñÔºåÊúÄËøëÈÇÑÊé®Âá∫‰∫ÜÂÖ∂‰ªñÂπæÁ®Æ LLM„ÄÇÂÖ∂‰∏≠‰∏Ä‰∫õÊòØÈñãÊîæÊ®°ÂûãÔºåÂèØ‰æõ‰∏ãËºâÂíå‰øÆÊîπ„ÄÇ
ÂÑòÁÆ°Â§öË™ûË®ÄÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∑≤Á∂ìÂ≠òÂú®‰∏ÄÊÆµÊôÇÈñìÔºå‰ΩÜÂÆÉÂÄëÂú®ÂÉß‰ºΩÁæÖË™ûÁ≠â‰ΩéË≥áÊ∫êË™ûË®Ä‰∏äÁöÑË°®Áèæ‰∏ÄÁõ¥ÂæàÂ∑Æ„ÄÇÊàëÂÄëÁõ¥Êé•Âú®ÂÉß‰ºΩÁæÖË™ûÂíåÈÄöÈÅéÁøªË≠ØÊàêËã±Ë™ûÂíåÂæûËã±Ë™ûÁøªË≠ØÁöÑÊñπÂºèË©ï‰º∞‰∫ÜÂõõÁ®ÆÊúÄÊñ∞ÁöÑ LLM ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÈÇÑË©ï‰º∞‰∫ÜÂÆÉÂÄëÂú®Â∞ëÈáèÂæÆË™øÊï∏Êìö‰∏ãÁöÑÂæÆË™øËÉΩÂäõ„ÄÇClaude Âíå GPT 4o ÈñãÁÆ±Âç≥Áî®Ë°®ÁèæËâØÂ•ΩÔºå‰∏¶‰∏îÊØî‰πãÂâçÁöÑÁâàÊú¨ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇLlama Âíå Mistral Ë°®Áèæ‰∏ç‰Ω≥Ôºå‰ΩÜÂæÆË™øÂæåË°®ÁèæÂá∫‰∫Ü‰∏Ä‰∫õÊîπÈÄ≤ÁöÑÂ∏åÊúõ„ÄÇ

##### **MetaOpenFOAM: an LLM-based multi-agent framework for CFD**
2407.21320v1 by Yuxuan Chena, Xu Zhua, Hua Zhoua, Zhuyin Rena

Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and post-processing, etc. MetaOpenFOAM harnesses the
power of MetaGPT's assembly line paradigm, which assigns diverse roles to
various agents, efficiently breaking down complex CFD tasks into manageable
subtasks. Langchain further complements MetaOpenFOAM by integrating
Retrieval-Augmented Generation (RAG) technology, which enhances the framework's
ability by integrating a searchable database of OpenFOAM tutorials for LLMs.
Tests on a benchmark for natural language-based CFD solver, consisting of 8 CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The 8 CFD
simulation tasks include compressible and incompressible flows, 2D and 3D
flows, heat transfer, and combustion, demonstrating the ability to automate CFD
simulations using only natural language input and iteratively correct errors to
achieve the desired simulation at a low cost. An ablation study was conducted
to verify the necessity of each component in the multi-agent system and the RAG
technology. A sensitivity study on the randomness of LLM showed that LLM with
low randomness can obtain more stable and accurate results. Additionally,
MetaOpenFOAM own the ability to identify and modify key parameters in user
requirements and excels in correcting bugs when failures occur, with or without
human participation, which demonstrates the generalization of MetaOpenFOAM.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Âü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ‰ª£ÁêÜÁ§æÁæ§ÔºåÂú®Ëá™ÂãïÂåñÂïèÈ°åËß£Ê±∫ÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Â±ï„ÄÇË®àÁÆóÊµÅÈ´îÂäõÂ≠∏ (CFD) ‰ΩúÁÇ∫‰∏ÄÂÄãË§áÈõúÁöÑÂïèÈ°åÔºåÂú®ÈúÄË¶ÅÁ≤æÂØÜËß£Ê±∫ÊñπÊ°àÁöÑËá™ÂãïÂåñÊ®°Êì¨‰∏≠ÊèêÂá∫‰∫ÜÁç®ÁâπÁöÑÊåëÊà∞„ÄÇMetaOpenFOAM ‰ΩúÁÇ∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÈáç‰ª£ÁêÜÂçî‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®ÂÉÖ‰ª•Ëá™ÁÑ∂Ë™ûË®Ä‰ΩúÁÇ∫Ëº∏ÂÖ•‰æÜÂÆåÊàê CFD Ê®°Êì¨‰ªªÂãô„ÄÇÈÄô‰∫õÊ®°Êì¨‰ªªÂãôÂåÖÊã¨Á∂≤Ê†ºÂâçËôïÁêÜ„ÄÅÊ®°Êì¨ÂíåÂæåËôïÁêÜÁ≠â„ÄÇMetaOpenFOAM Âà©Áî®‰∫Ü MetaGPT ÁµÑË£ùÁ∑öÁØÑ‰æãÁöÑÂäõÈáèÔºåÂ∞á‰∏çÂêåÁöÑËßíËâ≤ÂàÜÈÖçÁµ¶‰∏çÂêåÁöÑ‰ª£ÁêÜÔºåÊúâÊïàÂú∞Â∞áË§áÈõúÁöÑ CFD ‰ªªÂãôÂàÜËß£ÁÇ∫ÂèØÁÆ°ÁêÜÁöÑÂ≠ê‰ªªÂãô„ÄÇLangchain ÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅéÊï¥ÂêàÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰æÜË£úÂÖÖ MetaOpenFOAMÔºåÈÄôÈÄèÈÅéÊï¥ÂêàÂèØÊêúÂ∞ãÁöÑ OpenFOAM ÊïôÂ≠∏Ë≥áÊñôÂ∫´‰æÜÂ¢ûÂº∑ LLM ÁöÑÊ°ÜÊû∂ËÉΩÂäõ„ÄÇÂú®‰∏ÄÂÄãÂü∫ÊñºËá™ÁÑ∂Ë™ûË®ÄÁöÑ CFD Ê±ÇËß£Âô®ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂåÖÂê´ 8 ÂÄã CFD Ê®°Êì¨‰ªªÂãôÔºåÁµêÊûúÈ°ØÁ§∫ MetaOpenFOAM Âú®ÊØèÂÄãÊ∏¨Ë©¶‰∏≠ÈÉΩÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÈÄöÈÅéÁéá (85%)ÔºåÊØèÂÄãÊ∏¨Ë©¶Ê°à‰æãÂπ≥ÂùáÂè™Ëä±Ë≤ª 0.22 ÁæéÂÖÉ„ÄÇÈÄô 8 ÂÄã CFD Ê®°Êì¨‰ªªÂãôÂåÖÊã¨ÂèØÂ£ìÁ∏ÆÂíå‰∏çÂèØÂ£ìÁ∏ÆÊµÅ„ÄÅ2D Âíå 3D ÊµÅ„ÄÅÁÜ±ÂÇ≥ÈÅûÂíåÁáÉÁáíÔºåÂ±ïÁ§∫‰∫ÜÂÉÖ‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•ÂíåÂèçË¶ÜÊõ¥Ê≠£ÈåØË™§Â∞±ËÉΩËá™ÂãïÂü∑Ë°å CFD Ê®°Êì¨ÁöÑËÉΩÂäõÔºå‰ª•‰ΩéÊàêÊú¨ÂØ¶ÁèæÊâÄÈúÄÁöÑÊ®°Êì¨„ÄÇÈÄ≤Ë°å‰∫ÜÊ∂àËûçÁ†îÁ©∂‰ª•È©óË≠âÂ§öÈáç‰ª£ÁêÜÁ≥ªÁµ±Âíå RAG ÊäÄË°ì‰∏≠ÊØèÂÄãÁµÑÊàêÁöÑÂøÖË¶ÅÊÄß„ÄÇÂ∞ç LLM Èö®Ê©üÊÄßÁöÑÊïèÊÑüÊÄßÁ†îÁ©∂Ë°®ÊòéÔºåÈö®Ê©üÊÄß‰ΩéÁöÑ LLM ÂèØ‰ª•Áç≤ÂæóÊõ¥Á©©ÂÆöÂíåÊ∫ñÁ¢∫ÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåMetaOpenFOAM ÊìÅÊúâË≠òÂà•Âíå‰øÆÊîπ‰ΩøÁî®ËÄÖÈúÄÊ±Ç‰∏≠ÈóúÈçµÂèÉÊï∏ÁöÑËÉΩÂäõÔºå‰∏¶‰∏îÂú®ÁôºÁîüÊïÖÈöúÊôÇÊìÖÈï∑‰øÆÊ≠£ÈåØË™§ÔºåÁÑ°Ë´ñÊòØÂê¶Êúâ‰∫∫ÂèÉËàáÔºåÈÄôË≠âÊòé‰∫Ü MetaOpenFOAM ÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇ</paragraph>

##### **Big Cooperative Learning**
2407.21319v1 by Yulai Cong

Cooperation plays a pivotal role in the evolution of human intelligence;
moreover, it also underlies the recent revolutionary advancement of artificial
intelligence (AI) that is driven by foundation models. Specifically, we reveal
that the training of foundation models can be interpreted as a form of big
cooperative learning (\textit{abbr.} big learning), where massive learning
individuals/tasks \emph{cooperate} to approach the unique essence of data from
diverse perspectives of data prediction, leveraging a universal model. The
presented big learning therefore unifies most training objectives of foundation
models within a consistent framework, where their underlying assumptions are
exposed simultaneously. We design tailored simulations to demonstrate the
principle of big learning, based on which we provide learning-perspective
justifications for the successes of foundation models, with interesting
side-products. Furthermore, we reveal that big learning is a new dimension for
upgrading conventional machine learning paradigms, valuable for endowing
reinvigorations to associated applications; as an illustrative example, we
propose the BigLearn-GAN, which is a novel adversarially-trained foundation
model with versatile data sampling capabilities. Code is available at
\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.

ÊëòË¶ÅÔºöÂêà‰ΩúÂú®‰∫∫È°ûÊô∫ËÉΩÁöÑÊºîÂåñ‰∏≠ÊâÆÊºîËëóËàâË∂≥ËºïÈáçÁöÑËßíËâ≤Ôºõ
Ê≠§Â§ñÔºåÂÆÉ‰πüÊîØÊíêËëóËøëÊúüÁî±Âü∫Á§éÊ®°ÂûãÊé®ÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÈù©ÂëΩÊÄßÈÄ≤Â±ï„ÄÇÂÖ∑È´îËÄåË®ÄÔºåÊàëÂÄëÊè≠Á§∫‰∫ÜÂü∫Á§éÊ®°ÂûãÁöÑË®ìÁ∑¥ÂèØ‰ª•Ë¢´Ëß£ÈáãÁÇ∫‰∏ÄÁ®ÆÂ§ßÂûãÂêà‰ΩúÂ≠∏ÁøíÔºàÁ∞°Á®±Â§ßÂ≠∏ÁøíÔºâÁöÑÂΩ¢ÂºèÔºåÂÖ∂‰∏≠Â§ßÈáèÁöÑÂ≠∏ÁøíÂÄãÈ´î/‰ªªÂãô„ÄåÂêà‰Ωú„ÄçÂæûË≥áÊñôÈ†êÊ∏¨ÁöÑ‰∏çÂêåËßÄÈªû‰æÜÊé•ËøëË≥áÊñôÁöÑÁç®ÁâπÊú¨Ë≥™ÔºåÂà©Áî®‰∏ÄÂÄãÈÄöÁî®Ê®°Âûã„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑ„ÄåÂ§ßÂ≠∏Áøí„ÄçÂú®‰∏ÄÂÄã‰∏ÄËá¥ÁöÑÊû∂Êßã‰∏≠Áµ±‰∏Ä‰∫ÜÂü∫Á§éÊ®°ÂûãÁöÑÂ§ßÈÉ®ÂàÜË®ìÁ∑¥ÁõÆÊ®ôÔºåÂêåÊôÇÊè≠Èú≤‰∫ÜÂÖ∂ËÉåÂæåÁöÑÂÅáË®≠„ÄÇÊàëÂÄëË®≠Ë®à‰∫ÜÂÆ¢Ë£ΩÂåñÁöÑÊ®°Êì¨‰æÜÂ±ïÁ§∫Â§ßÂ≠∏ÁøíÁöÑÂéüÁêÜÔºå‰∏¶Ê†πÊìöÊ≠§ÂéüÁêÜÔºåÁÇ∫Âü∫Á§éÊ®°ÂûãÁöÑÊàêÂäüÊèê‰æõÂ≠∏ÁøíËßÄÈªûÁöÑÂêàÁêÜÂåñÔºå‰∏¶Áî¢ÁîüÊúâË∂£ÁöÑÂâØÁî¢ÂìÅ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊè≠Á§∫‰∫ÜÂ§ßÂ≠∏ÁøíÊòØÂçáÁ¥öÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÁØÑ‰æãÁöÑÊñ∞Èù¢ÂêëÔºåÂ∞çÊñºË≥¶‰∫àÁõ∏ÈóúÊáâÁî®Á®ãÂºèÊñ∞ÁöÑÊ¥ªÂäõÈùûÂ∏∏ÊúâÂÉπÂÄºÔºõ‰ΩúÁÇ∫‰∏ÄÂÄãË™™ÊòéÊÄßÁöÑÁØÑ‰æãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü BigLearn-GANÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ∞çÊäóÂºèË®ìÁ∑¥Âü∫Á§éÊ®°ÂûãÔºåÂÖ∑ÂÇôÂ§öÂäüËÉΩÁöÑË≥áÊñôÂèñÊ®£ËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \texttt{https://github.com/YulaiCong/BigCooperativeLearning} ÂèñÂæó„ÄÇ

##### **Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**
2407.21315v2 by Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg

This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åË™ûÈü≥ÁöÑÊÉÖÁ∑íÂÅµÊ∏¨„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áË™ûÈü≥ÁâπÂæµËΩâÊèõÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÊèèËø∞Ôºå‰æÜËß£Ê±∫ LLM Âú®ËôïÁêÜÈü≥Ë®äËº∏ÂÖ•ÊôÇÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂ∞áÈÄô‰∫õÊèèËø∞Êï¥ÂêàÂà∞ÊñáÂ≠óÊèêÁ§∫‰∏≠ÔºåËÆì LLM ËÉΩÂ§†Âú®‰∏ç‰øÆÊîπÊû∂ÊßãÁöÑÊÉÖÊ≥Å‰∏ãÂü∑Ë°åÂ§öÊ®°ÊÖãÁöÑÊÉÖÁ∑íÂàÜÊûê„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜÔºöIEMOCAP Âíå MELD ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊäÄË°ìÔºåÈ°ØÁ§∫Âá∫Âú®ÊÉÖÁ∑íËæ®Ë≠òÊ∫ñÁ¢∫Â∫¶‰∏äÂ§ßÂπÖÊèêÂçáÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ´òÂìÅË≥™ÁöÑÈü≥Ë®äË≥áÊñô„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÂä†ÂÖ•Ë™ûÈü≥ÊèèËø∞ËÆì IEMOCAP ‰∏äÁöÑÂä†Ê¨ä F1 ÂàÜÊï∏ÊèêÂçá‰∫Ü 2 ÂÄãÁôæÂàÜÈªûÔºàÂæû 70.111% Âà∞ 72.596%Ôºâ„ÄÇÊàëÂÄë‰πüÊØîËºÉ‰∫ÜÂêÑÁ®Æ LLM Êû∂ÊßãÔºå‰∏¶Êé¢Á¥¢‰∫Ü‰∏çÂêåÁâπÂæµË°®ÂæµÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÁöÑÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÈÄôÁ®ÆÊäÄË°ìÂú®ÊèêÂçá LLM ÊÉÖÁ∑íÂÅµÊ∏¨ËÉΩÂäõÁöÑÊΩõÂäõÔºå‰∏¶Âº∑Ë™ø‰∫ÜÈü≥Ë®äÂìÅË≥™Âú®Âü∫ÊñºË™ûÈü≥ÁöÑÊÉÖÁ∑íËæ®Ë≠ò‰ªªÂãô‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÂ∞áÂú® Github ‰∏äÈáãÂá∫ÂéüÂßãÁ¢º„ÄÇ

##### **EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**
2407.21311v1 by Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah

Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,
where the distribution of training (source) data differs from that of testing
(target) data. Many models have been developed to tackle this problem, and
recently vision transformers (ViTs) have shown promising results. However, the
complexity and large number of trainable parameters of ViTs restrict their
deployment in practical applications. This underscores the need for an
efficient model that not only reduces trainable parameters but also allows for
adjustable complexity based on specific needs while delivering comparable
performance. To achieve this, in this paper we introduce an Efficient
Unsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which
is a self-supervised ViT, as a feature extractor followed by a simplified
bottleneck of fully connected layers to refine features for enhanced domain
adaptation. Additionally, EUDA employs the synergistic domain alignment loss
(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)
losses, to balance adaptation by minimizing classification errors in the source
domain while aligning the source and target domain distributions. The
experimental results indicate the effectiveness of EUDA in producing comparable
results as compared with other state-of-the-art methods in domain adaptation
with significantly fewer trainable parameters, between 42% to 99.7% fewer. This
showcases the ability to train the model in a resource-limited environment. The
code of the model is available at: https://github.com/A-Abedi/EUDA.

ÊëòË¶ÅÔºöÁÑ°Áõ£Áù£ÂüüÈÅ©Êáâ (UDA) Êó®Âú®Ê∏õËºïÂüüÂÅèÁßªÂïèÈ°åÔºåÂÖ∂‰∏≠Ë®ìÁ∑¥ (‰æÜÊ∫ê) Ë≥áÊñôÁöÑÂàÜÈÖçËàáÊ∏¨Ë©¶ (ÁõÆÊ®ô) Ë≥áÊñôÁöÑÂàÜÈÖç‰∏çÂêå„ÄÇÂ∑≤ÈñãÁôºË®±Â§öÊ®°Âûã‰æÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåËÄåÊúÄËøëÁöÑË¶ñË¶∫ËΩâÊèõÂô® (ViT) Â∑≤È°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåViT ÁöÑË§áÈõúÊÄßÂíåÂ§ßÈáèÁöÑÂèØË®ìÁ∑¥ÂèÉÊï∏ÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÈÄôÂº∑Ë™ø‰∫ÜÂ∞ç‰∏ÄÁ®ÆÊúâÊïàÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºåË©≤Ê®°Âûã‰∏çÂÉÖÂèØ‰ª•Ê∏õÂ∞ëÂèØË®ìÁ∑¥ÂèÉÊï∏ÔºåÈÇÑÂèØ‰ª•Ê†πÊìöÁâπÂÆöÈúÄÊ±ÇË™øÊï¥Ë§áÈõúÊÄßÔºåÂêåÊôÇÊèê‰æõÂèØÊØîËºÉÁöÑÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∏ÄÈªûÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑÁÑ°Áõ£Áù£ÂüüÈÅ©Êáâ (EUDA) Ê°ÜÊû∂„ÄÇEUDA ‰ΩøÁî® DINOv2ÔºåÈÄôÊòØ‰∏ÄÂÄãËá™Áõ£Áù£ÁöÑ ViTÔºå‰ΩúÁÇ∫‰∏ÄÂÄãÁâπÂæµÊèêÂèñÂô®ÔºåÂæåÈù¢ÊòØ‰∏ÄÂÄãÁ∞°ÂåñÁöÑÂÖ®ÈÄ£Êé•Â±§Áì∂È†∏Ôºå‰ª•ÂÑ™ÂåñÁâπÂæµ‰ª•Â¢ûÂº∑ÂüüÈÅ©Êáâ„ÄÇÊ≠§Â§ñÔºåEUDA Êé°Áî®ÂçîÂêåÂüüÂ∞çÈΩäÊêçÂ§± (SDAL)ÔºåÂÆÉÊï¥Âêà‰∫Ü‰∫§ÂèâÁÜµ (CE) ÂíåÊúÄÂ§ßÂπ≥ÂùáÂ∑ÆÁï∞ (MMD) ÊêçÂ§±ÔºåÈÄöÈÅéÊúÄÂ∞èÂåñ‰æÜÊ∫êÂüü‰∏≠ÁöÑÂàÜÈ°ûÈåØË™§ÂêåÊôÇÂ∞çÈΩä‰æÜÊ∫êÂíåÁõÆÊ®ôÂüüÂàÜÈÖç‰æÜÂπ≥Ë°°ÈÅ©Êáâ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÂüüÈÅ©ÊáâÊñπÊ≥ïÁõ∏ÊØîÔºåEUDA Âú®Áî¢ÁîüÂèØÊØîËºÉÁöÑÁµêÊûúÊñπÈù¢ÊòØÊúâÊïàÁöÑÔºåÂèØË®ìÁ∑¥ÂèÉÊï∏È°ØËëóÊ∏õÂ∞ëÔºåÊ∏õÂ∞ë‰∫Ü 42% Âà∞ 99.7%„ÄÇÈÄôÂ±ïÁ§∫‰∫ÜÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠Ë®ìÁ∑¥Ê®°ÂûãÁöÑËÉΩÂäõ„ÄÇÊ®°ÂûãÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/A-Abedi/EUDA ‰∏≠ÂèñÂæó„ÄÇ

##### **Implementing Streaming algorithm and k-means clusters to RAG**
2407.21300v1 by Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang

Retrieval-augmented generation (RAG) has achieved great success in
information retrieval to assist large models because it builds an external
knowledge database. However, it also has many problems: it consumes a lot of
memory because of the huge database. When faced with massive streaming data, it
is unable to update the established index database in time. To save the memory
of building the database and maintain accuracy simultaneously, we proposed a
new approach combining a streaming algorithm and k-means cluster with RAG. Our
approach applies a streaming algorithm to update the index and reduce memory
consumption. Then use the k-means algorithm to cluster documents with high
similarities together, the query time will be shortened by doing this. We
conducted comparative experiments on four methods, and the results show that
RAG with streaming algorithm and k-means cluster performs well in accuracy and
memory. For massive streaming data, we find that our method behaves better than
traditional RAG

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÂºèÁîüÊàêÔºàRAGÔºâÂú®Ë≥áË®äÊ™¢Á¥¢ÊñπÈù¢Â∑≤ÂèñÂæóÂ∑®Â§ßÊàêÂäüÔºåÂèØÂçîÂä©Â§ßÂûãÊ®°ÂûãÂª∫ÁΩÆÂ§ñÈÉ®Áü•Ë≠òË≥áÊñôÂ∫´„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰πüÂ≠òÂú®Ë®±Â§öÂïèÈ°åÔºöÁî±ÊñºË≥áÊñôÂ∫´ÈæêÂ§ßÔºåÂÆÉÊúÉÊ∂àËÄóÂ§ßÈáèË®òÊÜ∂È´î„ÄÇÁï∂Èù¢Â∞çÂ§ßÈáè‰∏≤ÊµÅË≥áÊñôÊôÇÔºåÂÆÉÁÑ°Ê≥ïÂç≥ÊôÇÊõ¥Êñ∞Â∑≤Âª∫Á´ãÁöÑÁ¥¢ÂºïË≥áÊñôÂ∫´„ÄÇÁÇ∫‰∫ÜÂêåÊôÇÁØÄÁúÅÂª∫ÁΩÆË≥áÊñôÂ∫´ÁöÑË®òÊÜ∂È´î‰∏¶Á∂≠ÊåÅÊ∫ñÁ¢∫ÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁµêÂêà‰∏≤ÊµÅÊºîÁÆóÊ≥ïÂíå k Âπ≥ÂùáÂÄºÁæ§ÈõÜËàá RAG ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÊáâÁî®‰∏≤ÊµÅÊºîÁÆóÊ≥ï‰æÜÊõ¥Êñ∞Á¥¢Âºï‰∏¶Ê∏õÂ∞ëË®òÊÜ∂È´îÊ∂àËÄó„ÄÇÁÑ∂Âæå‰ΩøÁî® k Âπ≥ÂùáÂÄºÊºîÁÆóÊ≥ïÂ∞áÁõ∏‰ººÂ∫¶È´òÁöÑÊñá‰ª∂ÂàÜÁæ§ÔºåÈÄôÊ®£ÂèØ‰ª•Á∏ÆÁü≠Êü•Ë©¢ÊôÇÈñì„ÄÇÊàëÂÄëÂ∞çÂõõÁ®ÆÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÊØîËºÉÂØ¶È©óÔºåÁµêÊûúÈ°ØÁ§∫ÔºåÊé°Áî®‰∏≤ÊµÅÊºîÁÆóÊ≥ïÂíå k Âπ≥ÂùáÂÄºÁæ§ÈõÜÁöÑ RAG Âú®Ê∫ñÁ¢∫ÊÄßÂíåË®òÊÜ∂È´îÊñπÈù¢Ë°®ÁèæËâØÂ•Ω„ÄÇÂ∞çÊñºÂ§ßÈáè‰∏≤ÊµÅË≥áÊñôÔºåÊàëÂÄëÁôºÁèæÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÂÇ≥Áµ± RAG

##### **Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**
2407.21299v1 by Kaustav Bhattacharjee, Soumya Kundu, Indrasis Chakraborty, Aritra Dasgupta

Net load forecasting is crucial for energy planning and facilitating informed
decision-making regarding trade and load distributions. However, evaluating
forecasting models' performance against benchmark models remains challenging,
thereby impeding experts' trust in the model's performance. In this context,
there is a demand for technological interventions that allow scientists to
compare models across various timeframes and solar penetration levels. This
paper introduces a visual analytics-based application designed to compare the
performance of deep-learning-based net load forecasting models with other
models for probabilistic net load forecasting. This application employs
carefully selected visual analytic interventions, enabling users to discern
differences in model performance across different solar penetration levels,
dataset resolutions, and hours of the day over multiple months. We also present
observations made using our application through a case study, demonstrating the
effectiveness of visualizations in aiding scientists in making informed
decisions and enhancing trust in net load forecasting models.

ÊëòË¶ÅÔºöÊ∑®ËºâËç∑È†êÊ∏¨Â∞çÊñºËÉΩÊ∫êË¶èÂäÉÂíå‰øÉÈÄ≤ÊúâÈóúË≤øÊòìÂíåË≤†ËºâÂàÜÈÖçÁöÑÊòéÊô∫Ê±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåË©ï‰º∞È†êÊ∏¨Ê®°ÂûãÁõ∏Â∞çÊñºÂü∫Ê∫ñÊ®°ÂûãÁöÑÊïàËÉΩ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂæûËÄåÈòªÁ§ôÂ∞àÂÆ∂Â∞çÊ®°ÂûãÊïàËÉΩÁöÑ‰ø°‰ªª„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÈúÄË¶ÅÊäÄË°ìÂπ≤È†êÔºåÂÖÅË®±ÁßëÂ≠∏ÂÆ∂Âú®ÂêÑÁ®ÆÊôÇÈñìÁØÑÂúçÂíåÂ§™ÈôΩËÉΩÊª≤ÈÄèÁéáÂ±§Á¥ö‰∏≠ÊØîËºÉÊ®°Âûã„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË¶ñË¶∫ÂàÜÊûêÁöÑÊáâÁî®Á®ãÂºèÔºåÊó®Âú®ÊØîËºÉÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãËàáÂÖ∂‰ªñÊ©üÁéáÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊ≠§ÊáâÁî®Á®ãÂºèÊé°Áî®‰ªîÁ¥∞ÊåëÈÅ∏ÁöÑË¶ñË¶∫ÂàÜÊûêÂπ≤È†êÊé™ÊñΩÔºå‰ΩøÁî®Êà∂ËÉΩÂ§†Ëæ®Âà•Ê®°ÂûãÊïàËÉΩÁöÑÂ∑ÆÁï∞ÔºåÈÄô‰∫õÂ∑ÆÁï∞Â≠òÂú®Êñº‰∏çÂêåÁöÑÂ§™ÈôΩËÉΩÊª≤ÈÄèÁéáÂ±§Á¥ö„ÄÅË≥áÊñôÈõÜËß£ÊûêÂ∫¶ÂíåÂ§öÂÄãÊúàÁöÑÁôΩÂ§©ÊôÇÊÆµ‰∏≠„ÄÇÊàëÂÄë‰πüÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®ÊàëÂÄëÁöÑÊáâÁî®Á®ãÂºèÊâÄÂÅöÁöÑËßÄÂØüÔºåË≠âÊòéË¶ñË¶∫ÂåñÂú®ÂçîÂä©ÁßëÂ≠∏ÂÆ∂ÂÅöÂá∫ÊòéÊô∫Ê±∫Á≠ñÂíåÂ¢ûÂº∑Â∞çÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãÁöÑ‰ø°‰ªªÊñπÈù¢ÊòØÊúâÊïàÁöÑ„ÄÇ

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÂèØËÉΩ‰ΩøË®±Â§öÈ†òÂüüÂèóÁõä„ÄÇÁ´ØÂà∞Á´ØËá™ÂãïÈßïÈßõ (e2eAD) ÊòØÂÖ∏ÂûãÈ†òÂüü‰πã‰∏ÄÔºåÂõ†ÁÇ∫ LLM ÊîØÊè¥Ë∂ä‰æÜË∂äÂ§öÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§Èù¢Ëá®Êñ∞ÁöÑÊ©üÊúÉ„ÄÇÂú®Ê≠§ÔºåÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SimpleLLM4AD ÁöÑ e2eAD ÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠Ôºåe2eAD ‰ªªÂãôÂàÜÁÇ∫ÂõõÂÄãÈöéÊÆµÔºåÂàÜÂà•ÊòØÊÑüÁü•„ÄÅÈ†êÊ∏¨„ÄÅË¶èÂäÉÂíåË°åÁÇ∫„ÄÇÊØèÂÄãÈöéÊÆµÂåÖÂê´Â§öÂÄãË¶ñË¶∫ÂïèÁ≠î (VQA) ÈÖçÂ∞çÔºå‰∏î VQA ÈÖçÂ∞çÁõ∏‰∫íÈÄ£Êé•ÔºåÊßãÂª∫‰∏ÄÂÄãÁ®±ÁÇ∫ÂúñÂΩ¢ VQA (GVQA) ÁöÑÂúñÂΩ¢„ÄÇÈÄèÈÅé VLM ÂàÜÈöéÊÆµÊé®ÁêÜ GVQA ‰∏≠ÁöÑÊØèÂÄã VQA ÈÖçÂ∞çÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÂØ¶ÁèæÁ´ØÂà∞Á´ØÈßïÈßõ„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠ÔºåÊé°Áî®Ë¶ñË¶∫Transformer (ViT) Ê®°Âûã‰æÜËôïÁêÜ nuScenes Ë¶ñË¶∫Ë≥áÊñôÔºåÂêåÊôÇÂà©Áî® VLM ‰æÜË©ÆÈáãÂíåÊé®ÁêÜÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠ÊèêÂèñÁöÑË≥áË®ä„ÄÇÂú®ÊÑüÁü•ÈöéÊÆµÔºåÁ≥ªÁµ±Ë≠òÂà•ÂíåÂàÜÈ°ûÈßïÈßõÁí∞Â¢É‰∏≠ÁöÑÁâ©‰ª∂„ÄÇÈ†êÊ∏¨ÈöéÊÆµÊ∂âÂèäÈ†êÊ∏¨ÈÄô‰∫õÁâ©‰ª∂ÁöÑÊΩõÂú®ÁßªÂãï„ÄÇË¶èÂäÉÈöéÊÆµÂà©Áî®Êî∂ÈõÜÁöÑË≥áË®ä‰æÜÂà∂ÂÆöÈßïÈßõÁ≠ñÁï•ÔºåÁ¢∫‰øùËá™ÂãïÈßïÈßõÊ±ΩËªäÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéá„ÄÇÊúÄÂæåÔºåË°åÁÇ∫ÈöéÊÆµÂ∞áË¶èÂäÉÁöÑÂãï‰ΩúËΩâÊèõÁÇ∫ËªäËºõÂèØÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåSimpleLLM4AD Âú®Ë§áÈõúÁöÑÈßïÈßõÂ†¥ÊôØ‰∏≠ÂØ¶Áèæ‰∫ÜÁ´∂Áà≠Âäõ„ÄÇ

##### **Robust Box Prompt based SAM for Medical Image Segmentation**
2407.21284v1 by Yuhao Huang, Xin Yang, Han Zhou, Yan Cao, Haoran Dou, Fajin Dong, Dong Ni

The Segment Anything Model (SAM) can achieve satisfactory segmentation
performance under high-quality box prompts. However, SAM's robustness is
compromised by the decline in box quality, limiting its practicality in
clinical reality. In this study, we propose a novel Robust Box prompt based SAM
(\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts
with different qualities. Our contribution is three-fold. First, we propose a
prompt refinement module to implicitly perceive the potential targets, and
output the offsets to directly transform the low-quality box prompt into a
high-quality one. We then provide an online iterative strategy for further
prompt refinement. Second, we introduce a prompt enhancement module to
automatically generate point prompts to assist the box-promptable segmentation
effectively. Last, we build a self-information extractor to encode the prior
information from the input image. These features can optimize the image
embeddings and attention calculation, thus, the robustness of SAM can be
further enhanced. Extensive experiments on the large medical segmentation
dataset including 99,299 images, 5 modalities, and 25 organs/targets validated
the efficacy of our proposed RoBox-SAM.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ÂèØ‰ª•Âú®È´òË¥®ÈáèÊ°ÜÊèêÁ§∫‰∏ãÂÆûÁé∞‰ª§‰∫∫Êª°ÊÑèÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåSAM ÁöÑÈ≤ÅÊ£íÊÄßÂõ†Ê°ÜË¥®ÈáèÁöÑ‰∏ãÈôçËÄåÂèóÂà∞ÊçüÂÆ≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏¥Â∫äÁé∞ÂÆû‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫é SAM ÁöÑÊñ∞ÂûãÈ≤ÅÊ£íÊ°ÜÊèêÁ§∫Ôºà**RoBox-SAM**ÔºâÔºå‰ª•Á°Æ‰øù SAM Âú®ÂÖ∑Êúâ‰∏çÂêåË¥®ÈáèÁöÑÊèêÁ§∫‰∏ãÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÊòØ‰∏âÊñπÈù¢ÁöÑ„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÊèêÂá∫‰∏Ä‰∏™ÊèêÁ§∫‰ºòÂåñÊ®°ÂùóÔºå‰ª•ÈöêÂºèÊÑüÁü•ÊΩúÂú®ÁõÆÊ†áÔºåÂπ∂ËæìÂá∫ÂÅèÁßªÈáèÔºå‰ª•Áõ¥Êé•Â∞Ü‰ΩéË¥®ÈáèÊ°ÜÊèêÁ§∫ËΩ¨Êç¢‰∏∫È´òË¥®ÈáèÊèêÁ§∫„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âú®Á∫øËø≠‰ª£Á≠ñÁï•Ôºå‰ª•‰æøËøõ‰∏ÄÊ≠•‰ºòÂåñÊèêÁ§∫„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫Â¢ûÂº∫Ê®°ÂùóÔºå‰ª•Ëá™Âä®ÁîüÊàêÁÇπÊèêÁ§∫Ôºå‰ª•ÊúâÊïàÂú∞ËæÖÂä©Ê°ÜÊèêÁ§∫ÂàÜÊÆµ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ëá™‰ø°ÊÅØÊèêÂèñÂô®Ôºå‰ª•ÂØπÊù•Ëá™ËæìÂÖ•ÂõæÂÉèÁöÑÂÖàÈ™å‰ø°ÊÅØËøõË°åÁºñÁ†Å„ÄÇËøô‰∫õÁâπÂæÅÂèØ‰ª•‰ºòÂåñÂõæÂÉèÂµåÂÖ•ÂíåÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÂõ†Ê≠§ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ SAM ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®ÂåÖÊã¨ 99,299 Âº†ÂõæÂÉè„ÄÅ5 ÁßçÊñπÂºèÂíå 25 ‰∏™Âô®ÂÆò/ÁõÆÊ†áÁöÑÂ§ßÂûãÂåªÂ≠¶ÂàÜÊÆµÊï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÂπøÊ≥õÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑ RoBox-SAM ÁöÑÂäüÊïà„ÄÇ

##### **Multi-Level Querying using A Knowledge Pyramid**
2407.21276v1 by Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li

This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÁèæÊúâÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ï‰∏≠ÔºåÂ∞çÊñºÁ≤æÊ∫ñÂ∫¶ÁöÑÊèêÂçáÈúÄÊ±ÇÔºåÈÄô‰∫õÊñπÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂ¢ûÂº∑Âè¨ÂõûÁéá„ÄÇÊàëÂÄëÂú® RAG Êû∂Êßã‰∏≠ÊèêÂá∫‰∏ÄÂÄãÂ§öÂ±§Áü•Ë≠òÈáëÂ≠óÂ°îÊñπÊ≥ïÔºå‰ª•Âú®Á≤æÊ∫ñÂ∫¶ÂíåÂè¨ÂõûÁéá‰πãÈñìÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇÁü•Ë≠òÈáëÂ≠óÂ°îÂåÖÂê´‰∏âÂÄãÂ±§Á¥öÔºöÊú¨‰Ωì„ÄÅÁü•Ë≠òÂúñË≠ú (KG) ÂíåÂü∫ÊñºÂçÄÂ°äÁöÑÂéüÂßãÊñáÂ≠ó„ÄÇÊàëÂÄëÊé°Áî®Ë∑®Â±§Â¢ûÂº∑ÊäÄË°ìÔºå‰ª•ÂØ¶ÁèæÂÖ®Èù¢ÁöÑÁü•Ë≠òÊ∂µËìãÁØÑÂúçÂíåÊú¨‰ΩìÊû∂ÊßãÂèäÂØ¶‰æãÁöÑÂãïÊÖãÊõ¥Êñ∞„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øùÁ∑äÊπäÊÄßÔºåÊàëÂÄëÂà©Áî®Ë∑®Â±§ÈÅéÊøæÊñπÊ≥ïÔºå‰ª•ÈÄ≤Ë°å KG ‰∏≠ÁöÑÁü•Ë≠òÊøÉÁ∏Æ„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÁ®±ÁÇ∫ PolyRAGÔºåÂÆÉÈÅµÂæ™ÁÄëÂ∏ÉÊ®°ÂûãÈÄ≤Ë°åÊ™¢Á¥¢ÔºåÂæûÈáëÂ≠óÂ°îÈ†ÇÁ´ØÈñãÂßãÔºå‰∏¶Âêë‰∏ãÈÄ≤Ë°åÔºåÁõ¥Âà∞Áç≤ÂæóÁ¢∫ÂÆöÁöÑÁ≠îÊ°à„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ©ÂÄãÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊ™¢Á¥¢Âü∫Ê∫ñÔºå‰∏ÄÂÄãÂú®Â≠∏Ë°ìÈ†òÂüüÔºåÂè¶‰∏ÄÂÄãÂú®ÈáëËûçÈ†òÂüü„ÄÇÈÄô‰∫õÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤ÈÄöÈÅéÂÖ®Èù¢ÂØ¶È©óÈ©óË≠âÔºåÂÖ∂Ë°®ÁèæÂÑ™Êñº 19 Á®Æ SOTA ÊñπÊ≥ï„ÄÇ‰∏ÄÂÄã‰ª§‰∫∫ÊåØÂ•ÆÁöÑËßÄÂØüÁµêÊûúÊòØÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∑≤Á∂ìÂ¢ûÂº∑‰∫Ü GPT-4ÔºåÈÄöÈÅéÂ∞áÂÖ∂ÊïàËÉΩÂæû 0.1636 ÊèêÂçáËá≥ 0.8109ÔºåÊèê‰æõ‰∫Ü 395% ÁöÑ F1 Â¢ûÁõä„ÄÇ

##### **Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**
2407.21273v1 by Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski

Efficient intravascular access in trauma and critical care significantly
impacts patient outcomes. However, the availability of skilled medical
personnel in austere environments is often limited. Autonomous robotic
ultrasound systems can aid in needle insertion for medication delivery and
support non-experts in such tasks. Despite advances in autonomous needle
insertion, inaccuracies in vessel segmentation predictions pose risks.
Understanding the uncertainty of predictive models in ultrasound imaging is
crucial for assessing their reliability. We introduce MSU-Net, a novel
multistage approach for training an ensemble of U-Nets to yield accurate
ultrasound image segmentation maps. We demonstrate substantial improvements,
18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model
transparency, and trustworthiness. By highlighting areas of model certainty,
MSU-Net can guide safe needle insertions, empowering non-experts to accomplish
such tasks.

ÊëòË¶ÅÔºöÂú®ÂâµÂÇ∑ÂíåÈáçÁóáÁÖßË≠∑‰∏≠ÔºåÊúâÊïàÁöÑË°ÄÁÆ°ÂÖßÈÄöË∑ØÊúÉÈ°ØËëóÂΩ±ÈüøÁóÖÊÇ£ÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåÂú®ÊÉ°Âä£ÁöÑÁí∞Â¢É‰∏≠ÔºåÁÜüÁ∑¥ÁöÑÈÜ´ÁôÇ‰∫∫Âì°ÂæÄÂæÄ‰∏çË∂≥„ÄÇËá™‰∏ªÊ©üÂô®‰∫∫Ë∂ÖÈü≥Ê≥¢Á≥ªÁµ±ÂèØ‰ª•ÂçîÂä©ÈáùÈ†≠ÊèíÂÖ•Ôºå‰ª•Êèê‰æõËó•Áâ©‰∏¶ÊîØÊè¥ÈùûÂ∞àÂÆ∂Âü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇÂÑòÁÆ°Ëá™‰∏ªÈáùÈ†≠ÊèíÂÖ•ÊäÄË°ìÈÄ≤Ê≠•Ôºå‰ΩÜË°ÄÁÆ°ÂàÜÂâ≤È†êÊ∏¨ÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄßÊúÉÈÄ†ÊàêÈ¢®Èö™„ÄÇ‰∫ÜËß£Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏≠È†êÊ∏¨Ê®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂ∞çÊñºË©ï‰º∞ÂÖ∂ÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂºïÈÄ≤ MSU-NetÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ§öÈöéÊÆµÊñπÊ≥ïÔºåÁî®ÊñºË®ìÁ∑¥‰∏ÄÁµÑ U-Net ‰ª•Áî¢ÁîüÊ∫ñÁ¢∫ÁöÑË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂàÜÂâ≤Âúñ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ§ßÂπÖÊîπÂñÑÔºåÊØîÂñÆ‰∏ÄÁöÑËíôÂú∞Âç°ÁæÖ U-Net ÊîπÂñÑ‰∫Ü 18.1%ÔºåÂ¢ûÂº∑‰∫Ü‰∏çÁ¢∫ÂÆöÊÄßË©ï‰º∞„ÄÅÊ®°ÂûãÈÄèÊòéÂ∫¶ÂíåÂèØ‰ø°Â∫¶„ÄÇÈÄèÈÅéÂº∑Ë™øÊ®°ÂûãÁ¢∫ÂÆöÊÄßÁöÑÂçÄÂüüÔºåMSU-Net ÂèØ‰ª•ÂºïÂ∞éÂÆâÂÖ®ÁöÑÈáùÈ†≠ÊèíÂÖ•ÔºåËÆìÈùûÂ∞àÂÆ∂‰πüËÉΩÂü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇ

##### **DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**
2407.21267v1 by Dongwon Son, Sanghyeon Son, Jaehyung Kim, Beomjoon Kim

We present DEF-oriCORN, a framework for language-directed manipulation tasks.
By leveraging a novel object-based scene representation and
diffusion-model-based state estimation algorithm, our framework enables
efficient and robust manipulation planning in response to verbal commands, even
in tightly packed environments with sparse camera views without any
demonstrations. Unlike traditional representations, our representation affords
efficient collision checking and language grounding. Compared to
state-of-the-art baselines, our framework achieves superior estimation and
motion planning performance from sparse RGB images and zero-shot generalizes to
real-world scenarios with diverse materials, including transparent and
reflective objects, despite being trained exclusively in simulation. Our code
for data generation, training, inference, and pre-trained weights are publicly
available at: https://sites.google.com/view/def-oricorn/home.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ DEF-oriCORNÔºå‰∏ÄÂÄãÁî®ÊñºË™ûË®ÄÂ∞éÂêëÊìç‰Ωú‰ªªÂãôÁöÑÊ°ÜÊû∂„ÄÇ
ÈÄèÈÅéÂà©Áî®Êñ∞Á©éÁöÑÂü∫ÊñºÁâ©‰ª∂ÁöÑÂ†¥ÊôØË°®Á§∫Âíå
Âü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑÁãÄÊÖã‰º∞Ë®àÊºîÁÆóÊ≥ïÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ËÉΩÂ§†
ÊúâÊïà‰∏îÁ©©ÂÅ•Âú∞ÈÄ≤Ë°åÊìç‰ΩúË¶èÂäÉÔºå‰ª•ÂõûÊáâÂè£È†≠Êåá‰ª§ÔºåÂç≥‰Ωø
Âú®Á∑äÂØÜÂ∞ÅÈñâÁöÑÁí∞Â¢É‰∏≠Ôºå‰ΩøÁî®Á®ÄÁñèÁõ∏Ê©üË¶ñËßíÔºå‰∏îÁÑ°‰ªª‰Ωï
Á§∫ÁØÑ„ÄÇËàáÂÇ≥Áµ±Ë°®Á§∫Ê≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑË°®Á§∫Ê≥ïÊèê‰æõ
ÊúâÊïàÁöÑÁ¢∞ÊíûÊ™¢Êü•ÂíåË™ûË®ÄÂü∫Á§é„ÄÇËàá
ÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ÂæûÁ®ÄÁñè RGB ÂΩ±ÂÉè‰∏≠Áç≤ÂæóÂÑ™Áï∞ÁöÑ‰º∞Ë®àÂíå
Âãï‰ΩúË¶èÂäÉÊïàËÉΩÔºå‰∏¶Âú®Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊÉÖÊ≥Å‰∏ãÊ¶ÇÊã¨Âà∞
ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÊùêÊñôÔºåÂåÖÊã¨ÈÄèÊòéÂíå
ÂèçÂ∞ÑÁâ©È´îÔºåÂÑòÁÆ°ÂÉÖÂú®Ê®°Êì¨‰∏≠ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑË≥áÊñôÁî¢Áîü„ÄÅË®ìÁ∑¥„ÄÅÊé®Ë´ñÂíåÈ†êÂÖàË®ìÁ∑¥Ê¨äÈáçÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñºÔºöhttps://sites.google.com/view/def-oricorn/home„ÄÇ

##### **Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**
2407.21264v1 by Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu

Model attribution for machine-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.

ÊëòË¶ÅÔºöÊ©üÂô®Áî¢ÁîüÁöÑÈåØË™§Ë®äÊÅØÁöÑÊ®°ÂûãÊ≠∏Âõ†Â∞ç‰∫ÜËß£ÂÖ∂Ëµ∑Ê∫êÂíåÊ∏õËºïÂÖ∂ÂÇ≥Êí≠ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÈÄôÈ†Ö‰ªªÂãôÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Áèæ‰ª£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúÉÁî¢ÁîüÂÖ∑ÊúâÈ°û‰∫∫ÂìÅË≥™ÁöÑÈåØË™§Ë®äÊÅØ„ÄÇÊ≠§Â§ñÔºåÁî®ÊñºÁî¢ÁîüÈåØË™§Ë®äÊÅØÁöÑÊèêÁ§∫ÊñπÊ≥ïÁöÑÂ§öÊ®£ÊÄß‰ΩøÂæóÊ∫ñÁ¢∫ÁöÑ‰æÜÊ∫êÊ≠∏Âõ†ËÆäÂæóË§áÈõú„ÄÇÈÄô‰∫õÊñπÊ≥ïÂºïÂÖ•‰∫ÜÁâπÂÆöÊñºÈ†òÂüüÁöÑÂäüËÉΩÔºåÈÄô‰∫õÂäüËÉΩÂèØËÉΩÊúÉÊé©ËìãÊ®°ÂûãÁöÑÂü∫Êú¨ÁâπÂæµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞áÊ®°ÂûãÊ≠∏Âõ†ÁöÑÊ¶ÇÂøµÂºïÂÖ•ÁÇ∫È†òÂüüÊ¶ÇÂåñÂïèÈ°åÔºåÂÖ∂‰∏≠ÊØèÂÄãÊèêÁ§∫ÊñπÊ≥ï‰ª£Ë°®‰∏ÄÂÄãÁç®ÁâπÁöÑÈ†òÂüü„ÄÇÊàëÂÄëË™çÁÇ∫Ôºå‰∏ÄÂÄãÊúâÊïàÁöÑÊ≠∏Âõ†Ê®°ÂûãÂøÖÈ†à‰∏çËÆäÊñºÈÄô‰∫õÁâπÂÆöÊñºÈ†òÂüüÁöÑÂäüËÉΩ„ÄÇÂÆÉÈÇÑÊáâË©≤ËÉΩÂ§†Ë≠òÂà•ÊâÄÊúâÂ†¥ÊôØ‰∏≠ÁöÑÂéüÂßãÊ®°ÂûãÔºåÂèçÊò†ÁèæÂØ¶‰∏ñÁïåÁöÑÂÅµÊ∏¨ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁõ£Áù£Â∞çÊØîÂ≠∏ÁøíÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊ≠§ÊñπÊ≥ïÊó®Âú®Â¢ûÂº∑Ê®°ÂûãÂ∞çÊèêÁ§∫ËÆäÂåñÁöÑÈ≠ØÊ£íÊÄßÔºå‰∏¶Â∞àÊ≥®ÊñºÂçÄÂàÜ‰∏çÂêåÁöÑ‰æÜÊ∫ê LLM„ÄÇÊàëÂÄëÈÄèÈÅéÊ∂âÂèä‰∏âÁ®ÆÂ∏∏Ë¶ãÊèêÁ§∫ÊñπÊ≥ïÁöÑÂö¥Ê†ºÂØ¶È©ó‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºö``ÈñãÊîæÂºè''„ÄÅ``ÈáçÂØ´''Âíå``ÊîπÂØ´''Ôºå‰ª•Âèä‰∏âÁ®ÆÈÄ≤Èöé LLMÔºö``llama 2''„ÄÅ``chatgpt''Âíå``vicuna''„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Ê®°ÂûãÊ≠∏Âõ†‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂú®Â§öÊ®£Âåñ‰∏îÊú™Ë¶ãÁöÑË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Lifelong Person Search**
2407.21252v1 by Jae-Won Yang, Seungbin Hong, Jae-Young Sim

Person search is the task to localize a query person in gallery datasets of
scene images. Existing methods have been mainly developed to handle a single
target dataset only, however diverse datasets are continuously given in
practical applications of person search. In such cases, they suffer from the
catastrophic knowledge forgetting in the old datasets when trained on new
datasets. In this paper, we first introduce a novel problem of lifelong person
search (LPS) where the model is incrementally trained on the new datasets while
preserving the knowledge learned in the old datasets. We propose an end-to-end
LPS framework that facilitates the knowledge distillation to enforce the
consistency learning between the old and new models by utilizing the prototype
features of the foreground persons as well as the hard background proposals in
the old domains. Moreover, we also devise the rehearsal-based instance matching
to further improve the discrimination ability in the old domains by using the
unlabeled person instances additionally. Experimental results demonstrate that
the proposed method achieves significantly superior performance of both the
detection and re-identification to preserve the knowledge learned in the old
domains compared with the existing methods.

ÊëòË¶ÅÔºö‰∫∫Áâ©ÊêúÂ∞ãÊòØÂ∞áÊü•Ë©¢‰∫∫Áâ©ÂÆö‰ΩçÂú®Â†¥ÊôØÂΩ±ÂÉèÁöÑË≥áÊñôÂ∫´Ë≥áÊñôÈõÜ‰∏≠ÁöÑ‰ªªÂãô„ÄÇÁèæÊúâÊñπÊ≥ï‰∏ªË¶ÅÈñãÁôºÁÇ∫ÂÉÖËôïÁêÜÂñÆ‰∏ÄÁõÆÊ®ôË≥áÊñôÈõÜÔºåÁÑ∂ËÄåÂú®‰∫∫Áâ©ÊêúÂ∞ãÁöÑÂØ¶ÈöõÊáâÁî®‰∏≠ÔºåÊúÉÊåÅÁ∫åÊèê‰æõÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜ„ÄÇÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÁï∂Âú®Êñ∞ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÊôÇÔºåËàäË≥áÊñôÈõÜÊúÉÈÅ≠ÂèóÁÅΩÈõ£ÊÄßÁöÑÁü•Ë≠òÈÅ∫Âøò„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥πÁµÇË∫´‰∫∫Áâ©ÊêúÂ∞ã (LPS) ÁöÑÊñ∞ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê®°ÂûãÊúÉÈÄêÊ≠•Âú®Êñ∞ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÔºåÂêåÊôÇ‰øùÁïôÂú®ËàäË≥áÊñôÈõÜ‰∏≠Â≠¶Âà∞ÁöÑÁü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ´ØÂ∞çÁ´ØÁöÑ LPS Êû∂ÊßãÔºåÈÄèÈÅéÂà©Áî®ÂâçÊôØ‰∫∫Áâ©ÁöÑÂéüÂûãÁâπÂæµÂíåËàäÁ∂≤Âüü‰∏≠ÁöÑÁ°¨ËÉåÊôØÊèêÊ°àÔºå‰øÉÈÄ≤Áü•Ë≠òËêÉÂèñÔºå‰ª•Âº∑Âà∂ËàäÊ®°ÂûãÂíåÊñ∞Ê®°Âûã‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄßÂ≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑË®≠Ë®à‰∫ÜÂü∫ÊñºÂΩ©ÊéíÁöÑÂØ¶‰æãÂåπÈÖçÔºå‰ª•ÈÄèÈÅéÂè¶Â§ñ‰ΩøÁî®Êú™Ê®ôË®òÁöÑ‰∫∫Áâ©ÂØ¶‰æãÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçáËàäÁ∂≤Âüü‰∏≠ÁöÑËæ®Âà•ËÉΩÂäõ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂÅµÊ∏¨ÂíåÂÜçËæ®Ë≠òÊñπÈù¢ÈÉΩÁç≤ÂæóÈ°ØËëóÁöÑÂÑ™Áï∞ÊïàËÉΩÔºå‰ª•‰øùÁïôÂú®ËàäÁ∂≤Âüü‰∏≠Â≠∏Âà∞ÁöÑÁü•Ë≠ò„ÄÇ

##### **Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**
2407.21248v1 by Anqi Zhang, Chaofeng Wu

While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is "certain but wrong", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.

ÊëòË¶ÅÔºö<paragraph>ÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÁî±ÊñºÂÆÉÂÄë‰∏çÈÄèÊòéÁöÑË®ìÁ∑¥Ë≥áÊñôÔºåÂ∞çÊñºÈö±ÁßÅ„ÄÅÂÆâÂÖ®ÊÄßÔºå‰ª•ÂèäÁâàÊ¨äÁöÑÂïèÈ°åÊ≠£ÈÄêÊº∏ÊµÆÁèæÔºåÈÄô‰πüÂ∏∂‰æÜ‰∫ÜÂú®Ë°®Ê†º‰∏≠ÂÅµÊ∏¨È†êË®ìÁ∑¥Ë≥áÊñôÁöÑÂïèÈ°å„ÄÇÁõÆÂâçËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÁöÑÊñπÊ≥ïÔºåÂà©Áî®‰∫ÜÊ©üÂô®Â≠∏ÁøíÈö±ÁßÅ‰∏≠ÊâÄÊé¢Ë®éÁöÑÊäÄË°ìÔºå‰æãÂ¶ÇÊàêÂì°Ë∫´ÂàÜÊé®Ë´ñÊîªÊìä (MIA)ÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥Êñº LLM ÁöÑÈÄêÂ≠óË®òÊÜ∂ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®Æ‰æùË≥¥ÊÄßÂ∏∂‰æÜ‰∫ÜÊåëÊà∞ÔºåÁâπÂà•ÊòØËÄÉÈáèÂà∞Â§ßÈáèÁöÑË®ìÁ∑¥Ë≥áÊñôÂíåÂèóÈôêÁöÑÊúâÊïàË®ìÁ∑¥Ëº™Ê¨°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈÅ©ÊáâÊÄßÈ†êË®ìÁ∑¥Ë≥áÊñôÂÅµÊ∏¨ÊñπÊ≥ïÔºåÂèØ‰ª•Ê∏õËºïÈÄôÁ®Æ‰æùË≥¥ÊÄßÔºå‰∏¶ÊúâÊïàÂú∞Êì¥Â§ßË≠òÂà•ÁØÑÂúç„ÄÇÊàëÂÄëÁöÑÈÅ©ÊáâÊÄßÊñπÊ≥ïÂÆö‰ΩçËº∏ÂÖ•ÁöÑ„Äå‰ª§‰∫∫È©öË®ùÁöÑÁ¨¶Ëôü„Äç„ÄÇÂ¶ÇÊûúÁ¨¶ËôüÁöÑÈ†êÊ∏¨ÊòØ„ÄåÁ¢∫ÂÆö‰ΩÜÈåØË™§„ÄçÔºåÂâáË©≤Á¨¶ËôüÂ∞ç LLM ‰æÜË™™ÊòØ‰ª§‰∫∫È©öË®ùÁöÑÔºåÈÄôÊåáÁöÑÊòØÊ©üÁéáÂàÜ‰ΩàÁöÑÈ¶ôËæ≤ÁÜµ‰ΩéÔºå‰∏îÂêåÊôÇÂÖ∑Êúâ‰ΩéÊñºÁúüÂØ¶Á¨¶ËôüÁöÑÊ©üÁéá„ÄÇÈÄèÈÅé‰ΩøÁî®‰ª§‰∫∫È©öË®ùÁöÑÁ¨¶ËôüÁöÑÈ†êÊ∏¨Ê©üÁéá‰æÜË°°Èáè„Äå‰ª§‰∫∫È©öË®ù„ÄçÔºåÂÅµÊ∏¨ÊñπÊ≥ïÊòØÂü∫Êñº‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÂÅáË®≠ÔºåËàáÁúãÂà∞Êú™Ë¶ãÈÅéÁöÑË≥áÊñôÁõ∏ÊØîÔºåÁúãÂà∞Â∑≤Ë¶ãÈÅéÁöÑË≥áÊñôÂ∞çÊñºÊ®°Âûã‰æÜË™™ËºÉ‰∏ç‰ª§‰∫∫È©öË®ù„ÄÇÊ≠§ÊñπÊ≥ïÂèØ‰ª•ÊáâÁî®ÔºåËÄåÁÑ°ÈúÄÂ≠òÂèñÈ†êË®ìÁ∑¥Ë≥áÊñôË™ûÊñôÂ∫´ÊàñÈ°çÂ§ñÁöÑË®ìÁ∑¥Ôºå‰æãÂ¶ÇÂèÉËÄÉÊ®°Âûã„ÄÇËàáÂú®ÂêÑÁ®ÆÂü∫Ê∫ñÂíåÊ®°Âûã‰∏äÈÄ≤Ë°åÁöÑ‰∏çÂêåÂØ¶È©ó‰∏≠ÁèæÊúâÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁèæ‰∫Ü‰∏ÄËá¥ÁöÑÈÄ≤Ê≠•ÔºåÈÅîÂà∞‰∫Ü 29.5% ÁöÑÊúÄÂ§ßÊîπÈÄ≤„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÂü∫ÊñºÊñ∞Êû∂ÊßãÈñãÁôºÁöÑÊñ∞Âü∫Ê∫ñ Dolma-BookÔºåÂÆÉÊé°Áî®Âú®Ê®°ÂûãË®ìÁ∑¥ÂâçÂæåÊî∂ÈõÜÁöÑÊõ∏Á±çË≥áÊñô‰æÜÊèê‰æõÈÄ≤‰∏ÄÊ≠•ÁöÑË©ï‰º∞„ÄÇ</paragraph>

##### **Informed Correctors for Discrete Diffusion Models**
2407.21243v1 by Yixiu Zhao, Jiaxin Shi, Lester Mackey, Scott Linderman

Discrete diffusion modeling is a promising framework for modeling and
generating data in discrete spaces. To sample from these models, different
strategies present trade-offs between computation and sample quality. A
predominant sampling strategy is predictor-corrector $\tau$-leaping, which
simulates the continuous time generative process with discretized predictor
steps and counteracts the accumulation of discretization error via corrector
steps. However, for absorbing state diffusion, an important class of discrete
diffusion models, the standard forward-backward corrector can be ineffective in
fixing such errors, resulting in subpar sample quality. To remedy this problem,
we propose a family of informed correctors that more reliably counteracts
discretization error by leveraging information learned by the model. For
further efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm
that better utilizes each model evaluation, while still enjoying the speed and
flexibility of $\tau$-leaping. Across several real and synthetic datasets, we
show that $k$-Gillespie's with informed correctors reliably produces higher
quality samples at lower computational cost.

ÊëòË¶ÅÔºöÈõ¢Êï£Êì¥Êï£Ê®°ÂûãÊòØ‰∏ÄÁ®ÆÁî®ÊñºÂ∞çÈõ¢Êï£Á©∫Èñì‰∏≠ÁöÑË≥áÊñôÈÄ≤Ë°åÂª∫Ê®°ÂíåÁîüÊàêÁöÑÂæàÊúâÂâçÊôØÁöÑÊ°ÜÊû∂„ÄÇÁÇ∫‰∫ÜÂæûÈÄô‰∫õÊ®°Âûã‰∏≠ÂèñÊ®£Ôºå‰∏çÂêåÁöÑÁ≠ñÁï•Âú®ÈÅãÁÆóÂíåÊ®£Êú¨ÂìÅË≥™‰πãÈñìÈÄ≤Ë°åÊ¨äË°°„ÄÇ‰∏ÄÁ®Æ‰∏ªË¶ÅÁöÑÂèñÊ®£Á≠ñÁï•ÊòØÈ†êÊ∏¨Ê†°Ê≠£ $\tau$-Ë∑≥Ë∫çÔºåÂÆÉ‰ΩøÁî®Èõ¢Êï£ÂåñÈ†êÊ∏¨Ê≠•È©üÊ®°Êì¨ÈÄ£Á∫åÊôÇÈñìÁîüÊàêÈÅéÁ®ãÔºå‰∏¶ÈÄèÈÅéÊ†°Ê≠£Ê≠•È©üÊäµÊ∂àÈõ¢Êï£ÂåñË™§Â∑ÆÁöÑÁ¥ØÁ©ç„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂê∏Êî∂ÁãÄÊÖãÊì¥Êï£ÔºàÈõ¢Êï£Êì¥Êï£Ê®°Âûã‰∏≠ÁöÑ‰∏ÄÂÄãÈáçË¶ÅÈ°ûÂà•ÔºâÔºåÊ®ôÊ∫ñÁöÑÂâçÂêëÂæåÂêëÊ†°Ê≠£Âô®Âú®‰øÆÊ≠£Ê≠§È°ûË™§Â∑ÆÊôÇÂèØËÉΩÁÑ°ÊïàÔºåÂ∞éËá¥Ê¨°‰Ω≥ÁöÑÊ®£Êú¨ÂìÅË≥™„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁöÑÁü•ÊÉÖÊ†°Ê≠£Âô®ÔºåÈÄô‰∫õÊ†°Ê≠£Âô®ÈÄèÈÅéÂà©Áî®Ê®°ÂûãÂ≠∏ÁøíÂà∞ÁöÑË≥áË®äÔºåÊõ¥ÂèØÈù†Âú∞ÊäµÊ∂àÈõ¢Êï£ÂåñË™§Â∑Æ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊïàÁéáÔºåÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü $k$-GillespieÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂèñÊ®£ÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩÊõ¥Â•ΩÂú∞Âà©Áî®ÊØèÂÄãÊ®°ÂûãË©ï‰º∞ÔºåÂêåÊôÇ‰ªç‰∫´Êúâ $\tau$-Ë∑≥Ë∫çÁöÑÈÄüÂ∫¶ÂíåÈùàÊ¥ªÊÄß„ÄÇÂú®ÂπæÂÄãÁúüÂØ¶ÂíåÂêàÊàêË≥áÊñôÈõÜ‰∏äÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®Áü•ÊÉÖÊ†°Ê≠£Âô®ÁöÑ $k$-Gillespie ËÉΩÂèØÈù†Âú∞‰ª•ËºÉ‰ΩéÁöÑÈÅãÁÆóÊàêÊú¨Áî¢ÁîüËºÉÈ´òÂìÅË≥™ÁöÑÊ®£Êú¨„ÄÇ

##### **Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**
2407.21229v1 by Ngoc Son Nguyen, Van Son Nguyen, Tung Le

Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.

ÊëòË¶ÅÔºöË¶ñË¶∫ÂïèÁ≠î (VQA) ÊúÄËøëÂ∑≤ÊàêÁÇ∫ÊΩõÂú®ÁöÑÁ†îÁ©∂È†òÂüüÔºåÂê∏Âºï‰∫ÜË®±Â§ö‰∫∫Â∑•Êô∫ÊÖßÂíåÈõªËÖ¶Ë¶ñË¶∫È†òÂüüÁöÑÁ†îÁ©∂‰∫∫Âì°„ÄÇÂÑòÁÆ°Ëã±Ë™û‰∏≠ÊúâË®±Â§öÊñπÊ≥ïÔºå‰ΩÜÈ°ØËëóÁº∫‰πèÂ∞àÈñÄÁÇ∫ÁâπÂÆöË™ûË®ÄÔºàÂ∞§ÂÖ∂ÊòØË∂äÂçóË™ûÔºâÈñãÁôºÁöÑÁ≥ªÁµ±„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÈÄèÈÅéÂ∞çË∂äÂçóË¶ñË¶∫ÂïèÁ≠î (ViVQA) Ë≥áÊñôÈõÜÈÄ≤Ë°åÂÖ®Èù¢ÂØ¶È©óÔºåÂ±ïÁ§∫ÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÁöÑÊúâÊïàÊÄßÔºå‰ª•ÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ù„ÄÇÁÇ∫‰∫ÜÂõûÊáâÁ§æÁæ§ÁöÑËààË∂£ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊ®°ÂûãÔºåÂ¢ûÂº∑‰∫ÜÂΩ±ÂÉèË°®ÂæµËÉΩÂäõÔºåÂæûËÄåÊîπÂñÑ‰∫Ü ViVQA Á≥ªÁµ±ÁöÑÊï¥È´îÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫ÜÂáçÁµêÂñÆÊ®°ÊÖãÊ®°Âûã (BLIP-2) ÁöÑÂºïÂ∞éË™ûË®ÄÂΩ±ÂÉèÈ†êË®ìÁ∑¥ÂíåÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø EfficientNetÔºå‰ª•ÂæûÂΩ±ÂÉè‰∏≠ÊèêÂèñÂíåËôïÁêÜÂ±ÄÈÉ®ÂíåÂÖ®ÂüüÁâπÂæµ„ÄÇÈÄôÁ®ÆÊï¥ÂêàÂà©Áî®‰∫ÜÂü∫Êñº Transformer ÁöÑÊû∂Êßã‰æÜÊì∑ÂèñÂÖ®Èù¢ÁöÑËÑàÁµ°Ë≥áË®äÔºå‰ª•ÂèäÂç∑Á©çÁ∂≤Ë∑Ø‰æÜÊì∑ÂèñË©≥Á¥∞ÁöÑÂ±ÄÈÉ®ÁâπÂæµ„ÄÇÈÄèÈÅéÂáçÁµêÈÄô‰∫õÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÂèÉÊï∏ÔºåÊàëÂÄëÂ§ßÂπÖÈôç‰Ωé‰∫ÜÈÅãÁÆóÊàêÊú¨ÂíåË®ìÁ∑¥ÊôÇÈñìÔºåÂêåÊôÇÁ∂≠ÊåÅÈ´òÊïàËÉΩ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈ°ØËëóÊîπÂñÑ‰∫ÜÂΩ±ÂÉèË°®ÂæµÔºå‰∏¶Â¢ûÂº∑‰∫ÜÁèæÊúâ VQA Á≥ªÁµ±ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®Âü∫ÊñºÈÄöÁî®Â§öÊ®°ÊÖãÂü∫Á§éÊ®°Âûã (BEiT-3) ÁöÑÂ§öÊ®°ÊÖãËûçÂêàÊ®°ÁµÑÔºåËûçÂêàË¶ñË¶∫ÂíåÊñáÂ≠óÁâπÂæµ‰πãÈñìÁöÑË≥áË®ä„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãË∂ÖË∂ä‰∫ÜÁ´∂Áà≠Âü∫Ê∫ñÔºåÈÅîÂà∞‰∫Ü‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÈÄôÂú® ViVQA Ë≥áÊñôÈõÜÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠Ê∫ñÁ¢∫ÁéáÈÅî 71.04% ‰∏≠ÁâπÂà•ÊòéÈ°ØÔºåÊ®ôË™åËëóÊàëÂÄëÁ†îÁ©∂È†òÂüüÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/nngocson2002/ViVQA ÂèñÂæó„ÄÇ

##### **Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**
2407.21227v1 by Florian Tambon, Amin Nikanjam, Foutse Khomh, Giuliano Antoniol

Large Language Models (LLMs) show promising potential in Software
Engineering, especially for code-related tasks like code completion and code
generation. LLMs' evaluation is generally centred around general metrics
computed over benchmarks. While painting a macroscopic view of the benchmarks
and of the LLMs' capacity, it is unclear how each programming task in these
benchmarks assesses the capabilities of the LLMs. In particular, the difficulty
level of the tasks in the benchmarks is not reflected in the score used to
report the performance of the model. Yet, a model achieving a 90% score on a
benchmark of predominantly easy tasks is likely less capable than a model
achieving a 90% score on a benchmark containing predominantly difficult tasks.
This paper devises a framework, HardEval, for assessing task difficulty for
LLMs and crafting new tasks based on identified hard tasks. The framework uses
a diverse array of prompts for a single task across multiple LLMs to obtain a
difficulty score for each task of a benchmark. Using two code generation
benchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably
identify the hard tasks within those benchmarks, highlighting that only 21% of
HumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis
of task difficulty, we also characterize 6 practical hard task topics which we
used to generate new hard tasks. Orthogonal to current benchmarking evaluation
efforts, HardEval can assist researchers and practitioners in fostering better
assessments of LLMs. The difficulty score can be used to identify hard tasks
within existing benchmarks. This, in turn, can be leveraged to generate more
hard tasks centred around specific topics either for evaluation or improvement
of LLMs. HardEval generalistic approach can be applied to other domains such as
code completion or Q/A.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËªüÈ´îÂ∑•Á®ã‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫ÊúüÂæÖÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®ËàáÁ®ãÂºèÁ¢ºÁõ∏ÈóúÁöÑ‰ªªÂãô‰∏≠Ôºå‰æãÂ¶ÇÁ®ãÂºèÁ¢ºÂÆåÊàêÂíåÁ®ãÂºèÁ¢ºÁîüÊàê„ÄÇLLM ÁöÑË©ï‰º∞ÈÄöÂ∏∏ÈõÜ‰∏≠Âú®Âü∫Ê∫ñ‰∏äË®àÁÆóÂá∫ÁöÑÈÄöÁî®ÊåáÊ®ô„ÄÇÈõñÁÑ∂ÊèèÁπ™Âá∫Âü∫Ê∫ñÂíå LLM ËÉΩÂäõÁöÑÂ∑®ËßÄËßÄÈªûÔºå‰ΩÜÁõÆÂâçÂ∞ö‰∏çÊ∏ÖÊ•öÈÄô‰∫õÂü∫Ê∫ñ‰∏≠ÁöÑÊØèÂÄãÁ®ãÂºèË®≠Ë®à‰ªªÂãôÂ¶Ç‰ΩïË©ï‰º∞ LLM ÁöÑËÉΩÂäõ„ÄÇÁâπÂà•ÊòØÔºåÂü∫Ê∫ñ‰∏≠‰ªªÂãôÁöÑÈõ£Â∫¶Á≠âÁ¥ö‰∏¶Êú™ÂèçÊò†Âú®Áî®ÊñºÂ†±ÂëäÊ®°ÂûãÊïàËÉΩÁöÑÂàÜÊï∏‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂú®‰∏ªË¶ÅÁî±ÂÆπÊòì‰ªªÂãôÁµÑÊàêÁöÑÂü∫Ê∫ñ‰∏äÈÅîÂà∞ 90% ÂàÜÊï∏ÁöÑÊ®°ÂûãÔºåÂÖ∂ËÉΩÂäõÂèØËÉΩ‰ΩéÊñºÂú®‰∏ªË¶ÅÁî±Âõ∞Èõ£‰ªªÂãôÁµÑÊàêÁöÑÂü∫Ê∫ñ‰∏äÈÅîÂà∞ 90% ÂàÜÊï∏ÁöÑÊ®°Âûã„ÄÇÊú¨ÊñáË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊû∂Êßã HardEvalÔºåÁî®ÊñºË©ï‰º∞ LLM ÁöÑ‰ªªÂãôÈõ£Â∫¶Ôºå‰∏¶Ê†πÊìöË≠òÂà•Âá∫ÁöÑÂõ∞Èõ£‰ªªÂãôÂª∫Á´ãÊñ∞‰ªªÂãô„ÄÇË©≤Êû∂ÊßãÈáùÂ∞çÂñÆ‰∏Ä‰ªªÂãô‰ΩøÁî®Â§öÂÄã LLM ÁöÑÂêÑÁ®ÆÊèêÁ§∫Ôºå‰ª•ÂèñÂæóÂü∫Ê∫ñ‰∏≠ÊØèÂÄã‰ªªÂãôÁöÑÈõ£Â∫¶ÂàÜÊï∏„ÄÇ‰ΩøÁî®ÂÖ©ÂÄãÁ®ãÂºèÁ¢ºÁîüÊàêÂü∫Ê∫ñ HumanEval+ Âíå ClassEvalÔºåÊàëÂÄëÂ±ïÁ§∫ HardEval ÂèØ‰ª•ÂèØÈù†Âú∞Ë≠òÂà•ÈÄô‰∫õÂü∫Ê∫ñ‰∏≠ÁöÑÂõ∞Èõ£‰ªªÂãôÔºå‰∏¶Âº∑Ë™øÂè™Êúâ 21% ÁöÑ HumanEval+ Âíå 27% ÁöÑ ClassEval ‰ªªÂãôÂ∞ç LLM ‰æÜË™™ÊòØÂõ∞Èõ£ÁöÑ„ÄÇÈÄèÈÅéÂ∞ç‰ªªÂãôÈõ£Â∫¶ÁöÑÂàÜÊûêÔºåÊàëÂÄëÈÇÑÊèèËø∞‰∫Ü 6 ÂÄãÂØ¶ÈöõÁöÑÂõ∞Èõ£‰ªªÂãô‰∏ªÈ°åÔºåÊàëÂÄëÁî®ÈÄô‰∫õ‰∏ªÈ°å‰æÜÁî¢ÁîüÊñ∞ÁöÑÂõ∞Èõ£‰ªªÂãô„ÄÇËàáÁõÆÂâçÁöÑÂü∫Ê∫ñË©ï‰º∞Â∑•‰ΩúÊ≠£‰∫§ÔºåHardEval ÂèØ‰ª•ÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠‰∫∫Âì°‰øÉÈÄ≤Â∞ç LLM ÁöÑÊõ¥‰Ω≥Ë©ï‰º∞„ÄÇÈõ£Â∫¶ÂàÜÊï∏ÂèØÁî®ÊñºË≠òÂà•ÁèæÊúâÂü∫Ê∫ñ‰∏≠ÁöÑÂõ∞Èõ£‰ªªÂãô„ÄÇÂèçÈÅé‰æÜÔºåÈÄôÂèØ‰ª•Áî®ÊñºÁî¢ÁîüÊõ¥Â§öÂúçÁπûÁâπÂÆö‰∏ªÈ°åÁöÑÂõ∞Èõ£‰ªªÂãôÔºå‰ª•Áî®ÊñºË©ï‰º∞ÊàñÊîπÈÄ≤ LLM„ÄÇHardEval ÁöÑÈÄöÁî®ÊñπÊ≥ïÂèØ‰ª•ÊáâÁî®ÊñºÂÖ∂‰ªñÈ†òÂüüÔºå‰æãÂ¶ÇÁ®ãÂºèÁ¢ºÂÆåÊàêÊàñÂïèÁ≠î„ÄÇ

##### **AI methods for approximate compiling of unitaries**
2407.21225v1 by David Kremer, Victor Villar, Sanjay Vishwakarma, Ismael Faro, Juan Cruz-Benito

This paper explores artificial intelligence (AI) methods for the approximate
compiling of unitaries, focusing on the use of fixed two-qubit gates and
arbitrary single-qubit rotations typical in superconducting hardware. Our
approach involves three main stages: identifying an initial template that
approximates the target unitary, predicting initial parameters for this
template, and refining these parameters to maximize the fidelity of the
circuit. We propose AI-driven approaches for the first two stages, with a deep
learning model that suggests initial templates and an autoencoder-like model
that suggests parameter values, which are refined through gradient descent to
achieve the desired fidelity. We demonstrate the method on 2 and 3-qubit
unitaries, showcasing promising improvements over exhaustive search and random
parameter initialization. The results highlight the potential of AI to enhance
the transpiling process, supporting more efficient quantum computations on
current and future quantum hardware.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÔºåÁî®ÊñºËøë‰ººÁ∑®Ë≠ØÈÖâÁÆóÂ≠êÔºåÈáçÈªûÂú®Êñº‰ΩøÁî®Ë∂ÖÂ∞éÁ°¨È´î‰∏≠Â∏∏Ë¶ãÁöÑÂõ∫ÂÆöÈõô‰ΩçÂÖÉÈñòÂíå‰ªªÊÑèÂñÆ‰ΩçÂÖÉÊóãËΩâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰∏âÂÄã‰∏ªË¶ÅÈöéÊÆµÔºöÊâæÂá∫Ëøë‰ººÁõÆÊ®ôÈÖâÁÆóÂ≠êÁöÑÂàùÂßãÁØÑÊú¨„ÄÅÈ†êÊ∏¨Ê≠§ÁØÑÊú¨ÁöÑÂàùÂßãÂèÉÊï∏Ôºå‰ª•ÂèäÂæÆË™øÈÄô‰∫õÂèÉÊï∏‰ª•ÊúÄÂ§ßÂåñÈõªË∑ØÁöÑ‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÊèêÂá∫ÈáùÂ∞çÂâçÂÖ©ÂÄãÈöéÊÆµÁöÑ AI È©ÖÂãïÊñπÊ≥ïÔºå‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂª∫Ë≠∞ÂàùÂßãÁØÑÊú¨Ôºå‰ª•ÂèäÈ°û‰ººËá™ÂãïÁ∑®Á¢ºÂô®ÁöÑÊ®°ÂûãÂª∫Ë≠∞ÂèÉÊï∏ÂÄºÔºåÈÄèÈÅéÊ¢ØÂ∫¶‰∏ãÈôçÂæÆË™øÈÄô‰∫õÂèÉÊï∏‰ª•ÈÅîÂà∞ÊâÄÈúÄÁöÑ‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÂú® 2 Âíå 3 ‰ΩçÂÖÉÈÖâÁÆóÂ≠ê‰∏äÂ±ïÁ§∫Ê≠§ÊñπÊ≥ïÔºåÂ±ïÁ§∫Âá∫ÊØîÁ™ÆËàâÊêúÂ∞ãÂíåÈö®Ê©üÂèÉÊï∏ÂàùÂßãÂåñÊõ¥Â•ΩÁöÑÊîπÈÄ≤„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫Ü AI Â¢ûÂº∑ËΩâË≠ØÈÅéÁ®ãÁöÑÊΩõÂäõÔºåÊîØÊè¥Âú®ÁèæÊúâÂíåÊú™‰æÜÁöÑÈáèÂ≠êÁ°¨È´î‰∏äÈÄ≤Ë°åÊõ¥ÊúâÊïàÁéáÁöÑÈáèÂ≠êÈÅãÁÆó„ÄÇ

##### **LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**
2407.21204v1 by H. Emre Erdem, Henry Leung

Static noise maps depicting long-term noise levels over wide areas are
valuable urban planning assets for municipalities in decreasing noise exposure
of residents. However, non-traffic noise sources with transient behavior, which
people complain frequently, are usually ignored by static maps. We propose here
a dynamic noise mapping approach using the data collected via low-power
wide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT)
infrastructure, which is one of the most common communication backbones for
smart cities. Noise mapping based on LPWAN is challenging due to the low data
rates of these protocols. The proposed dynamic noise mapping approach
diminishes the negative implications of data rate limitations using machine
learning (ML) for event and location prediction of non-traffic sources based on
the scarce data. The strength of these models lies in their consideration of
the spatial variance in acoustic behavior caused by the buildings in urban
settings. The effectiveness of the proposed method and the accuracy of the
resulting dynamic maps are evaluated in field tests. The results show that the
proposed system can decrease the map error caused by non-traffic sources up to
51% and can stay effective under significant packet losses.

ÊëòË¶ÅÔºöÈùúÊÖãÂô™Èü≥Âú∞ÂúñÊèèÁπ™‰∫ÜÂª£ÈóäÂçÄÂüüÁöÑÈï∑ÊúüÂô™Èü≥Ê∞¥Âπ≥ÔºåÂ∞çÊñºÈôç‰ΩéÂ±ÖÊ∞ëÁöÑÂô™Èü≥ÊõùÈú≤ÔºåÊòØÂ∏ÇÊîøÂñÆ‰ΩçÊúâÂÉπÂÄºÁöÑÈÉΩÂ∏ÇË¶èÂäÉË≥áÁî¢„ÄÇÁÑ∂ËÄåÔºå‰∫∫ÂÄëÁ∂ìÂ∏∏Êä±ÊÄ®ÁöÑÈùû‰∫§ÈÄöÂô™Èü≥‰æÜÊ∫êÂÖ∑ÊúâÊö´ÊÖãË°åÁÇ∫ÔºåÈÄöÂ∏∏ÊúÉË¢´ÈùúÊÖãÂú∞ÂúñÂøΩÁï•„ÄÇÊàëÂÄëÂú®Ê≠§ÊèêÂá∫‰ΩøÁî®ÈÄèÈÅé‰ΩéÂäüÁéáÂª£ÂüüÁ∂≤Ë∑ØÔºàLPWANÔºåÁâπÂà•ÊòØ LoRaWANÔºâÊâÄÊî∂ÈõÜÁöÑË≥áÊñôÔºå‰ª•ÂèäÂü∫ÊñºÁâ©ËÅØÁ∂≤ÔºàIoTÔºâÂü∫Á§éË®≠ÊñΩÁöÑÂãïÊÖãÂô™Èü≥Áπ™Ë£ΩÊñπÊ≥ïÔºåÈÄôÊòØÊô∫ÊÖßÂüéÂ∏ÇÊúÄÂ∏∏Ë¶ãÁöÑÈÄöË®ä‰∏ªÂππ‰πã‰∏Ä„ÄÇÁî±ÊñºÈÄô‰∫õÂçîÂÆöÁöÑË≥áÊñôÈÄüÁéá‰ΩéÔºåÂõ†Ê≠§Âü∫Êñº LPWAN ÁöÑÂô™Èü≥Áπ™Ë£ΩÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊâÄÊèêÂá∫ÁöÑÂãïÊÖãÂô™Èü≥Áπ™Ë£ΩÊñπÊ≥ïÈÄèÈÅéÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâ‰æÜÈ†êÊ∏¨Èùû‰∫§ÈÄö‰æÜÊ∫êÁöÑ‰∫ã‰ª∂Âíå‰ΩçÁΩÆÔºå‰ª•Ê∏õÂ∞ëË≥áÊñôÈÄüÁéáÈôêÂà∂ÁöÑË≤†Èù¢ÂΩ±ÈüøÔºåËÄåË≥áÊñôÈùûÂ∏∏Á®ÄÂ∞ë„ÄÇÈÄô‰∫õÊ®°ÂûãÁöÑÂÑ™ÈªûÂú®ÊñºËÄÉÈáè‰∫ÜÈÉΩÂ∏ÇÁí∞Â¢É‰∏≠Âª∫ÁØâÁâ©ÊâÄÈÄ†ÊàêÁöÑËÅ≤Â≠∏Ë°åÁÇ∫ÁöÑÁ©∫ÈñìËÆäÁï∞„ÄÇÂú®ÂØ¶Âú∞Ê∏¨Ë©¶‰∏≠Ë©ï‰º∞‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß‰ª•ÂèäÁî¢ÁîüÁöÑÂãïÊÖãÂú∞ÂúñÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±ÂèØ‰ª•Â∞áÈùû‰∫§ÈÄö‰æÜÊ∫êÈÄ†ÊàêÁöÑÊò†Â∞ÑË™§Â∑ÆÈôç‰ΩéÂ§öÈÅî 51%Ôºå‰∏îÂú®Â∞ÅÂåÖÈÅ∫Â§±ÁéáÈ°ØËëóÁöÑÊÉÖÊ≥Å‰∏ã‰ªçËÉΩ‰øùÊåÅÊúâÊïàÊÄß„ÄÇ

##### **GenRec: Generative Personalized Sequential Recommendation**
2407.21191v1 by Panfeng Cao, Pietro Lio

Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.

ÊëòË¶ÅÔºöÂ∫èÂàóÊé®Ëñ¶‰ªªÂãôÊòØÂæûÊ≠∑Âè≤‰ΩøÁî®ËÄÖÈ†ÖÁõÆ‰∫íÂãïË≥áÊñô‰∏≠Êì∑ÂèñÈö±Ëóè‰ΩøÁî®ËÄÖÂÅèÂ•Ω„ÄÇÈÄèÈÅéÂà©Áî®Âü∫ÊñºÂàÜÈ°ûÁöÑÂ≠∏ÁøíÊñπÊ≥ïÔºåÂ∑≤Âú®ÈÄôÂÄãÈ†òÂüüÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇÂèóÂà∞Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠„ÄåÈ†êË®ìÁ∑¥„ÄÅÊèêÁ§∫ÂíåÈ†êÊ∏¨„ÄçÁöÑÊñ∞ËààÂÖ∏ÁØÑÂïüÁôºÔºåÊàëÂÄëÂ∞áÂ∫èÂàóÊé®Ëñ¶Ë¶ñÁÇ∫Â∫èÂàóÂà∞Â∫èÂàóÁî¢Áîü‰ªªÂãôÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ÁîüÊàêÂºèÊé®Ëñ¶ (GenRec) ÁöÑÊñ∞Á©éÊ®°Âûã„ÄÇËàáÂ≠∏ÁøíÊòéÁ¢∫‰ΩøÁî®ËÄÖÂíåÈ†ÖÁõÆË°®ÂæµÁöÑÂü∫ÊñºÂàÜÈ°ûÁöÑÊ®°Âûã‰∏çÂêåÔºåGenRec Âà©Áî® Transformer ÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÔºå‰∏¶Êé°Áî®ÈÅÆËîΩÈ†ÖÁõÆÈ†êÊ∏¨ÁõÆÊ®ôÔºå‰ª•ÊúâÊïàÂ≠∏ÁøíÈö±ËóèÁöÑÈõôÂêëÂ∫èÂàóÊ®°Âºè„ÄÇËàáÁèæÊúâÁöÑÁîüÊàêÂºèÂ∫èÂàóÊé®Ëñ¶Ê®°Âûã‰∏çÂêåÔºåGenRec ‰∏ç‰æùË≥¥ÊâãÂãïË®≠Ë®àÁöÑÁ°¨ÊèêÁ§∫„ÄÇGenRec ÁöÑËº∏ÂÖ•ÊòØÊñáÂ≠ó‰ΩøÁî®ËÄÖÈ†ÖÁõÆÂ∫èÂàóÔºåËÄåËº∏Âá∫ÊòØÊéíÂêçÊúÄÂâçÈù¢ÁöÑ‰∏ã‰∏ÄÂÄãÈ†ÖÁõÆ„ÄÇÊ≠§Â§ñÔºåGenRec ÈùûÂ∏∏Á≤æÁ∞°ÔºåÂè™ÈúÄÂπæÂÄãÂ∞èÊôÇÂç≥ÂèØÂú®‰ΩéË≥áÊ∫êË®≠ÂÆö‰∏≠ÊúâÊïàË®ìÁ∑¥Ôºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©Áî®ÊñºÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÔºå‰∏¶ÊúâÂä©ÊñºÂú®Â∫èÂàóÊé®Ëñ¶È†òÂüüÊ∞ë‰∏ªÂåñÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé GenRec ÂèØ‰ª•Ê¶ÇÊã¨Âà∞ÂêÑÁ®ÆÂÖ¨ÈñãÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÔºå‰∏¶ÂèñÂæóÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÂØ¶È©ó‰πüÈ©óË≠â‰∫ÜÊâÄÊèêÂá∫ÁöÑÈÅÆËîΩÈ†ÖÁõÆÈ†êÊ∏¨ÁõÆÊ®ôÁöÑÊúâÊïàÊÄßÔºåÂÆÉÂ∞áÊ®°ÂûãÊïàËÉΩÂ§ßÂπÖÊèêÂçá„ÄÇ

##### **AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**
2407.21174v1 by Maisha Binte Rashid, Pablo Rivas

Multimodal machine learning models that combine visual and textual data are
increasingly being deployed in critical applications, raising significant
safety and security concerns due to their vulnerability to adversarial attacks.
This paper presents an effective strategy to enhance the robustness of
multimodal image captioning models against such attacks. By leveraging the Fast
Gradient Sign Method (FGSM) to generate adversarial examples and incorporating
adversarial training techniques, we demonstrate improved model robustness on
two benchmark datasets: Flickr8k and COCO. Our findings indicate that
selectively training only the text decoder of the multimodal architecture shows
performance comparable to full adversarial training while offering increased
computational efficiency. This targeted approach suggests a balance between
robustness and training costs, facilitating the ethical deployment of
multimodal AI systems across various domains.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁªìÂêà‰∫ÜËßÜËßâÂíåÊñáÊú¨Êï∞ÊçÆÔºå
Ë∂äÊù•Ë∂äÂ§öÂú∞Áî®‰∫éÂÖ≥ÈîÆÂ∫îÁî®Á®ãÂ∫è‰∏≠ÔºåÁî±‰∫éÂÖ∂ÂÆπÊòìÂèóÂà∞ÂØπÊäóÊÄßÊîªÂáªÔºåÂõ†Ê≠§ÂºïÂèë‰∫ÜÈáçÂ§ßÁöÑ
ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÁ≠ñÁï•Êù•Â¢ûÂº∫
Â§öÊ®°ÊÄÅÂõæÂÉèÂ≠óÂπïÊ®°ÂûãÂØπËøôÁßçÊîªÂáªÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÈÄöËøáÂà©Áî®Âø´ÈÄü
Ê¢ØÂ∫¶Á¨¶Âè∑ÊñπÊ≥ï (FGSM) ÁîüÊàêÂØπÊäóÊÄßÁ§∫‰æãÂπ∂ÁªìÂêà
ÂØπÊäóÊÄßËÆ≠ÁªÉÊäÄÊúØÔºåÊàë‰ª¨Âú®‰∏§‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºöFlickr8k Âíå COCO ‰∏äÂ±ïÁ§∫‰∫ÜÊîπËøõÁöÑÊ®°ÂûãÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®Êòé
ÈÄâÊã©ÊÄßÂú∞‰ªÖËÆ≠ÁªÉÂ§öÊ®°ÊÄÅÊû∂ÊûÑÁöÑÊñáÊú¨Ëß£Á†ÅÂô®ÊòæÁ§∫
ÊÄßËÉΩ‰∏éÂÆåÂÖ®ÂØπÊäóÊÄßËÆ≠ÁªÉÁõ∏ÂΩìÔºåÂêåÊó∂Êèê‰æõÊõ¥È´òÁöÑ
ËÆ°ÁÆóÊïàÁéá„ÄÇËøôÁßçÊúâÈíàÂØπÊÄßÁöÑÊñπÊ≥ïË°®Êòé‰∫Ü
È≤ÅÊ£íÊÄßÂíåËÆ≠ÁªÉÊàêÊú¨‰πãÈó¥ÁöÑÂπ≥Ë°°Ôºå‰øÉËøõ‰∫Ü
Â§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂú®ÂêÑ‰∏™È¢ÜÂüüÁöÑÈÅìÂæ∑ÈÉ®ÁΩ≤„ÄÇ

##### **Decomposed Prompting to Answer Questions on a Course Discussion Board**
2407.21170v1 by Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang

We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏¶Ë©ï‰º∞‰∏ÄÂÄãÂïèÈ°åÂõûÁ≠îÁ≥ªÁµ±ÔºåË©≤Á≥ªÁµ±‰ΩøÁî®ÂàÜËß£ÊèêÁ§∫‰æÜÂàÜÈ°ûÂíåÂõûÁ≠îÂ≠∏ÁîüÂú®Ë™≤Á®ãË®éË´ñÂçÄ‰∏äÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÂïèÈ°åÂàÜÈ°ûÁÇ∫ÂõõÁ®ÆÈ°ûÂûã‰πã‰∏ÄÔºöÊ¶ÇÂøµ„ÄÅ‰ΩúÊ•≠„ÄÅÂæåÂã§ÂíåÁÑ°Ê≥ïÂõûÁ≠î„ÄÇÈÄô‰ΩøÊàëÂÄëËÉΩÂ§†ÈáùÂ∞ç‰∏çÂêåÈ°ûÂûãÁöÑÂïèÈ°åÊé°Áî®‰∏çÂêåÁöÑÂõûÁ≠îÁ≠ñÁï•„ÄÇ‰ΩøÁî® GPT-3 ÁöÑËÆäÈ´îÔºåÊàëÂÄëÈÅîÂà∞‰∫Ü 81% ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÊàëÂÄëÁöÑÁ≥ªÁµ±Âú®ÂõûÁ≠îÊ©üÂô®Â≠∏ÁøíË™≤Á®ã‰∏≠ÁöÑÊ¶ÇÂøµÊÄßÂïèÈ°åÊôÇÁöÑË°®ÁèæÂíåÂêÑÁ®ÆÂ§±ÊïóÊ®°Âºè„ÄÇ

##### **Understanding Public Safety Trends in Calgary through data mining**
2407.21163v1 by Zack Dewis, Apratim Sen, Jeffrey Wong, Yujia Zhang

This paper utilizes statistical data from various open datasets in Calgary to
to uncover patterns and insights for community crimes, disorders, and traffic
incidents. Community attributes like demographics, housing, and pet
registration were collected and analyzed through geospatial visualization and
correlation analysis. Strongly correlated features were identified using the
chi-square test, and predictive models were built using association rule mining
and machine learning algorithms. The findings suggest that crime rates are
closely linked to factors such as population density, while pet registration
has a smaller impact. This study offers valuable insights for city managers to
enhance community safety strategies.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÂà©Áî®Âç°Âä†Âà©ÂêÑÁ®ÆÈñãÊîæË≥áÊñôÈõÜÁöÑÁµ±Ë®àÊï∏ÊìöÔºåÊè≠Á§∫Á§æÂçÄÁäØÁΩ™„ÄÅÂ§±Â∫èÂíå‰∫§ÈÄö‰∫ãÊïÖÁöÑÊ®°ÂºèÂíåË¶ãËß£„ÄÇÈÄöÈÅéÂú∞ÁêÜÁ©∫ÈñìÂèØË¶ñÂåñÂíåÁõ∏ÈóúÊÄßÂàÜÊûêÊî∂ÈõÜÂíåÂàÜÊûê‰∫ÜÁ§æÂçÄÂ±¨ÊÄßÔºå‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®à„ÄÅ‰ΩèÊàøÂíåÂØµÁâ©Ë®ªÂÜä„ÄÇ‰ΩøÁî®Âç°ÊñπÊ™¢È©óË≠òÂà•Âá∫Áõ∏ÈóúÊÄßÂæàÂº∑ÁöÑÁâπÂæµÔºå‰∏¶‰ΩøÁî®ÈóúËÅØË¶èÂâáÊåñÊéòÂíåÊ©üÂô®Â≠∏ÁøíÁÆóÊ≥ïÊßãÂª∫È†êÊ∏¨Ê®°Âûã„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁäØÁΩ™ÁéáËàá‰∫∫Âè£ÂØÜÂ∫¶Á≠âÂõ†Á¥†ÂØÜÂàáÁõ∏ÈóúÔºåËÄåÂØµÁâ©Ë®ªÂÜäÁöÑÂΩ±ÈüøËºÉÂ∞è„ÄÇÊú¨Á†îÁ©∂ÁÇ∫ÂüéÂ∏ÇÁÆ°ÁêÜËÄÖÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ª•Â¢ûÂº∑Á§æÂçÄÂÆâÂÖ®Á≠ñÁï•„ÄÇ

##### **Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**
2407.21153v1 by Alaa Aljabari, Lina Duaibes, Mustafa Jarrar, Mohammed Khalilia

Event-argument extraction is a challenging task, particularly in Arabic due
to sparse linguistic resources. To fill this gap, we introduce the \hadath
corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument
annotations. We used three types of event arguments: $agent$, $location$, and
$date$, which we annotated as relation types. Our inter-annotator agreement
evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score.
Additionally, we propose a novel method for event relation extraction using
BERT, in which we treat the task as text entailment. This method achieves an
$F_1$-score of $94.01\%$. To further evaluate the generalization of our
proposed method, we collected and annotated another out-of-domain corpus (about
$80$k tokens) called \testNLI and used it as a second test set, on which our
approach achieved promising results ($83.59\%$ $F_1$-score). Last but not
least, we propose an end-to-end system for event-arguments extraction. This
system is implemented as part of SinaTools, and both corpora are publicly
available at {\small \url{https://sina.birzeit.edu/wojood}}

ÊëòË¶ÅÔºö‰∫ã‰ª∂Ë´ñÂÖÉËêÉÂèñÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÁâπÂà•ÊòØÂ∞çÊñºÈòøÊãâ‰ºØË™ûÔºåÂõ†ÁÇ∫Ë™ûË®ÄË≥áÊ∫êÁ®ÄÂ∞ë„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü\hadathË™ûÊñôÂ∫´Ôºà55 Ëê¨ÂÄãË©ûÂΩôÔºâÔºå‰ΩúÁÇ∫WojoodÁöÑÂª∂‰º∏Ôºå‰∏¶Âä†ÂÖ•‰∫Ü‰∫ã‰ª∂Ë´ñÂÖÉË®ªËß£„ÄÇÊàëÂÄë‰ΩøÁî®‰∫Ü‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰∫ã‰ª∂Ë´ñÂÖÉÔºö$agent$„ÄÅ$location$Âíå$date$ÔºåÊàëÂÄëÂ∞áÂÆÉÂÄëË®ªËß£ÁÇ∫Èóú‰øÇÈ°ûÂûã„ÄÇÊàëÂÄëÁöÑÊ®ôË®ªËÄÖÈñì‰∏ÄËá¥ÊÄßË©ï‰º∞ÂæóÂá∫ÁöÑKappaË©ïÂàÜÁÇ∫82.23%ÔºåF1Ë©ïÂàÜÁÇ∫87.2%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®BERTÈÄ≤Ë°å‰∫ã‰ª∂Èóú‰øÇËêÉÂèñÁöÑÊñ∞ÊñπÊ≥ïÔºåÂú®Ë©≤ÊñπÊ≥ï‰∏≠ÔºåÊàëÂÄëÂ∞á‰ªªÂãôË¶ñÁÇ∫ÊñáÊú¨ËòäÊ∂µ„ÄÇÊ≠§ÊñπÊ≥ïÈÅîÂà∞‰∫Ü94.01%ÁöÑF1Ë©ïÂàÜ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Ë©ï‰º∞ÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊ≥õÂåñÊÄßÔºåÊàëÂÄëÊî∂ÈõÜ‰∏¶Ë®ªËß£‰∫ÜÂè¶‰∏ÄÂÄãÈ†òÂüüÂ§ñÁöÑË™ûÊñôÂ∫´ÔºàÁ¥Ñ8Ëê¨ÂÄãË©ûÂΩôÔºâÔºåÁ®±ÁÇ∫\testNLIÔºå‰∏¶Â∞áÂÖ∂Áî®‰ΩúÁ¨¨‰∫åÂÄãÊ∏¨Ë©¶ÈõÜÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Ë©≤Ê∏¨Ë©¶ÈõÜ‰∏äÂèñÂæó‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºàF1Ë©ïÂàÜÁÇ∫83.59%Ôºâ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑ‰∏ÄÈªûÊòØÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰∫ã‰ª∂Ë´ñÂÖÉËêÉÂèñÁöÑÁ´ØÂà∞Á´ØÁ≥ªÁµ±„ÄÇÊ≠§Á≥ªÁµ±‰ΩúÁÇ∫SinaToolsÁöÑ‰∏ÄÈÉ®ÂàÜÂØ¶‰ΩúÔºå‰∏¶‰∏îÂÖ©ÂÄãË™ûÊñôÂ∫´ÈÉΩÂèØ‰ª•Âú®{\small \url{https://sina.birzeit.edu/wojood}}ÂÖ¨ÈñãÂèñÂæó„ÄÇ

##### **Private Collaborative Edge Inference via Over-the-Air Computation**
2407.21151v1 by Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz

We consider collaborative inference at the wireless edge, where each client's
model is trained independently on their local datasets. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
Specifically, we propose different methods for ensemble and multi-view
classification that exploit over-the-air computation. We show that these
schemes perform better than their orthogonal counterparts with statistically
significant differences while using fewer resources and providing privacy
guarantees. We also provide experimental results verifying the benefits of the
proposed over-the-air multi-user inference approach and perform an ablation
study to demonstrate the effectiveness of our design choices. We share the
source code of the framework publicly on Github to facilitate further research
and reproducibility.

ÊëòË¶ÅÔºöÊàëÂÄëÂú®ÁÑ°Á∑öÈÇäÁ∑£ËÄÉÊÖÆÂçî‰ΩúÊé®Ë´ñÔºåÂÖ∂‰∏≠ÊØèÂÄãÂÆ¢Êà∂Á´ØÁöÑÊ®°ÂûãÈÉΩÊòØÁç®Á´ãÂú®‰ªñÂÄëÁöÑÊú¨Âú∞Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ„ÄÇÂÆ¢Êà∂Á´ØÊúÉË¢´Âπ≥Ë°åÊü•Ë©¢‰ª•Âçî‰ΩúÂÅöÂá∫Ê∫ñÁ¢∫ÁöÑÊ±∫Á≠ñ„ÄÇÈô§‰∫ÜÊúÄÂ§ßÂåñÊé®Ë´ñÊ∫ñÁ¢∫ÊÄß‰πãÂ§ñÔºåÊàëÂÄë‰πüÊÉ≥Ë¶ÅÁ¢∫‰øùÊú¨Âú∞Ê®°ÂûãÁöÑÈö±ÁßÅÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂà©Áî®Â§öÈáçÂ≠òÂèñÈÄöÈÅìÁöÑÁñäÂä†Â±¨ÊÄß‰æÜÂØ¶‰ΩúÈ†ªÂØ¨ÊïàÁéáÈ´òÁöÑÂ§ö‰ΩøÁî®ËÄÖÊé®Ë´ñÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∏çÂêåÁöÑÊñπÊ≥ïÁî®ÊñºÊï¥È´îÂíåÂ§öË¶ñËßíÂàÜÈ°ûÔºåÂà©Áî®Á©∫‰∏≠ÈÅãÁÆó„ÄÇÊàëÂÄëÂ±ïÁ§∫ÈÄô‰∫õÊñπÊ°àÊØîÂÆÉÂÄëÁöÑÊ≠£‰∫§Â∞çÊáâÊñπÊ°àË°®ÁèæÂæóÊõ¥Â•ΩÔºåÂÖ∑ÊúâÁµ±Ë®à‰∏äÁöÑÈ°ØËëóÂ∑ÆÁï∞ÔºåÂêåÊôÇ‰ΩøÁî®ËºÉÂ∞ëÁöÑË≥áÊ∫ê‰∏¶Êèê‰æõÈö±ÁßÅ‰øùË≠â„ÄÇÊàëÂÄëÈÇÑÊèê‰æõÂØ¶È©óÁµêÊûúÔºåÈ©óË≠âÊâÄÊèêÂá∫ÁöÑÁ©∫‰∏≠Â§ö‰ΩøÁî®ËÄÖÊé®Ë´ñÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºå‰∏¶ÈÄ≤Ë°åÊ∂àËûçÁ†îÁ©∂‰ª•Ë≠âÊòéÊàëÂÄëÁöÑË®≠Ë®àÈÅ∏ÊìáÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂú® Github ‰∏äÂÖ¨ÈñãÂàÜ‰∫´Ê°ÜÊû∂ÁöÑÂéüÂßãÁ¢ºÔºå‰ª•Âà©ÊñºÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÈáçÁèæÊÄß„ÄÇ

##### **Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**
2407.21149v1 by Mayanka Chandrashekar, Ian Goethert, Md Inzamam Ul Haque, Benjamin McMahon, Sayera Dhaubhadel, Kathryn Knight, Joseph Erdos, Donna Reagan, Caroline Taylor, Peter Kuzmak, John Michael Gaziano, Eileen McAllister, Lauren Costa, Yuk-Lam Ho, Kelly Cho, Suzanne Tamang, Samah Fodeh-Jarad, Olga S. Ovchinnikova, Amy C. Justice, Jacob Hinkle, Ioana Danciu

Objectives: This study aims to assess the impact of domain shift on chest
X-ray classification accuracy and to analyze the influence of ground truth
label quality and demographic factors such as age group, sex, and study year.
Materials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset
for deep learning-based multilabel classification using ground truth labels
from radiology reports extracted using the CheXpert and CheXbert Labeler. We
compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and
Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR
dataset comprises over 259k chest X-ray images spanning between the years 2010
and 2022. Results: The validation of ground truth and the assessment of
multi-label classification performance across various NLP extraction tools
revealed that the VA-CXR dataset exhibited lower disagreement rates than the
MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores
between models utilizing CheXpert and CheXbert. When evaluating multi-label
classification performance across different datasets, minimal domain shift was
observed in unseen datasets, except for the label "Enlarged Cardiomediastinum."
The study year's subgroup analyses exhibited the most significant variations in
multi-label classification model performance. These findings underscore the
importance of considering domain shifts in chest X-ray classification tasks,
particularly concerning study years. Conclusion: Our study reveals the
significant impact of domain shift and demographic factors on chest X-ray
classification, emphasizing the need for improved transfer learning and
equitable model development. Addressing these challenges is crucial for
advancing medical imaging and enhancing patient care.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÊ®ôÔºöÊú¨Á†îÁ©∂Êó®Âú®Ë©ï‰º∞È†òÂüüËΩâÁßªÂ∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁ≤æÂ∫¶ÁöÑÂΩ±ÈüøÔºå‰∏¶ÂàÜÊûêÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÂìÅË≥™ÂíåÂπ¥ÈΩ°ÁµÑ„ÄÅÊÄßÂà•ÂíåÁ†îÁ©∂Âπ¥‰ªΩÁ≠â‰∫∫Âè£Âõ†Á¥†ÁöÑÂΩ±Èüø„ÄÇ
ÊùêÊñôÂíåÊñπÊ≥ïÔºöÊàëÂÄë‰ΩøÁî® DenseNet121 Ê®°ÂûãÈ†êË®ìÁ∑¥ MIMIC-CXR Ë≥áÊñôÈõÜÔºå‰ΩøÁî®Âæû‰ΩøÁî® CheXpert Âíå CheXbert Ê®ôÁ±§Âô®ÂæûÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠ÊèêÂèñÁöÑÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÈÄ≤Ë°åÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°û„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü MIMIC-CXR ÂíåÈÄÄ‰ºçËªç‰∫∫ÂÅ•Â∫∑ÁÆ°ÁêÜÂ±ÄËÉ∏ÈÉ® X ÂÖâË≥áÊñôÈõÜ (VA-CXR) ‰∏ä 14 ÂÄãËÉ∏ÈÉ® X ÂÖâÊ®ôÁ±§ÁöÑÊÄßËÉΩ„ÄÇVA-CXR Ë≥áÊñôÈõÜÂåÖÂê´Ë∂ÖÈÅé 259k ÂºµËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÔºåÊôÇÈñìË∑®Â∫¶ÁÇ∫ 2010 Âπ¥Ëá≥ 2022 Âπ¥„ÄÇÁµêÊûúÔºöÂü∫Êú¨‰∫ãÂØ¶ÁöÑÈ©óË≠âÂíåÂ∞çÂêÑÁ®Æ NLP ÊèêÂèñÂ∑•ÂÖ∑ÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÁöÑË©ï‰º∞È°ØÁ§∫ÔºåVA-CXR Ë≥áÊñôÈõÜË°®ÁèæÂá∫ÁöÑÂàÜÊ≠ßÁéá‰ΩéÊñº MIMIC-CXR Ë≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî® CheXpert Âíå CheXbert ÁöÑÊ®°Âûã‰πãÈñìÁöÑ AUC ÂæóÂàÜÂ≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇÂú®Ë©ï‰º∞‰∏çÂêåË≥áÊñôÈõÜ‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÊôÇÔºåÈô§‰∫ÜÊ®ôÁ±§„ÄåÂøÉÁ∏±ÈöîÂ¢ûÂ§ß„Äç‰πãÂ§ñÔºåÂú®Êú™Ë¶ãË≥áÊñôÈõÜ‰∏≠ËßÄÂØüÂà∞ÁöÑÈ†òÂüüËΩâÁßªÂæàÂ∞è„ÄÇÁ†îÁ©∂Âπ¥‰ªΩÁöÑÂ≠êÁæ§ÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊ®°ÂûãÊÄßËÉΩËÆäÂåñÊúÄÂ§ß„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÂú®ËÉ∏ÈÉ® X ÂÖâÂàÜÈ°û‰ªªÂãô‰∏≠ËÄÉÊÖÆÈ†òÂüüËΩâÁßªÁöÑÈáçË¶ÅÊÄßÔºåÁâπÂà•ÊòØÈóúÊñºÁ†îÁ©∂Âπ¥‰ªΩ„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫ÜÈ†òÂüüËΩâÁßªÂíå‰∫∫Âè£Âõ†Á¥†Â∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁöÑÈ°ØËëóÂΩ±ÈüøÔºåÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂÖ¨Âπ≥Ê®°ÂûãÈñãÁôºÁöÑÂøÖË¶ÅÊÄß„ÄÇÊáâÂ∞çÈÄô‰∫õÊåëÊà∞Â∞çÊñºÊé®ÈÄ≤ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÂä†Âº∑ÊÇ£ËÄÖË≠∑ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>


### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xiny Pan et.al.|[2407.21483v2](http://arxiv.org/abs/2407.21483v2)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|null|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-30**|**Be aware of overfitting by hyperparameter optimization!**|Igor V. Tetko et.al.|[2407.20786v1](http://arxiv.org/abs/2407.20786v1)|null|
|**2024-07-30**|**Harvesting Textual and Structured Data from the HAL Publication Repository**|Francis Kulumba et.al.|[2407.20595v1](http://arxiv.org/abs/2407.20595v1)|null|
|**2024-07-30**|**CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**|Tianshi Zheng et.al.|[2407.20564v1](http://arxiv.org/abs/2407.20564v1)|null|
|**2024-07-30**|**Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**|Hossein Rajaby Faghihi et.al.|[2407.20513v1](http://arxiv.org/abs/2407.20513v1)|null|
|**2024-07-29**|**What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**|Navapat Nananukul et.al.|[2407.20382v1](http://arxiv.org/abs/2407.20382v1)|null|
|**2024-07-29**|**MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**|Zehui Chen et.al.|[2407.20183v1](http://arxiv.org/abs/2407.20183v1)|[link](https://github.com/internlm/mindsearch)|
|**2024-07-29**|**rLLM: Relational Table Learning with LLMs**|Weichen Li et.al.|[2407.20157v1](http://arxiv.org/abs/2407.20157v1)|[link](https://github.com/rllm-project/rllm)|
|**2024-07-29**|**Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**|Yunsheng Wang et.al.|[2407.19643v2](http://arxiv.org/abs/2407.19643v2)|[link](https://github.com/iamryanshengwang/prometheus-chatbot)|
|**2024-07-29**|**TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**|Selma Wanna et.al.|[2407.19616v1](http://arxiv.org/abs/2407.19616v1)|null|
|**2024-07-27**|**Semantic Communication Enhanced by Knowledge Graph Representation Learning**|Nour Hello et.al.|[2407.19338v1](http://arxiv.org/abs/2407.19338v1)|null|
|**2024-07-26**|**GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**|Yuchen Shen et.al.|[2407.19039v1](http://arxiv.org/abs/2407.19039v1)|null|
|**2024-07-26**|**Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**|Yuni Susanti et.al.|[2407.18752v3](http://arxiv.org/abs/2407.18752v3)|[link](https://github.com/littleflow3r/kg-structure-as-prompt)|
|**2024-07-26**|**Using GPT-4 to guide causal machine learning**|Anthony C. Constantinou et.al.|[2407.18607v1](http://arxiv.org/abs/2407.18607v1)|null|
|**2024-07-26**|**Multi-turn Response Selection with Commonsense-enhanced Language Models**|Yuandong Wang et.al.|[2407.18479v1](http://arxiv.org/abs/2407.18479v1)|null|
|**2024-07-25**|**Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**|Sindhura Kommu et.al.|[2407.18181v1](http://arxiv.org/abs/2407.18181v1)|null|
|**2024-07-24**|**MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**|Arya Bulusu et.al.|[2407.17544v1](http://arxiv.org/abs/2407.17544v1)|[link](https://github.com/emergenceai/mathviz-e)|
|**2024-07-23**|**Ranking protein-protein models with large language models and graph neural networks**|Xiaotong Xu et.al.|[2407.16375v1](http://arxiv.org/abs/2407.16375v1)|[link](https://github.com/haddocking/deeprank-gnn-esm)|
|**2024-07-23**|**PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**|Jaeyoung Kim et.al.|[2407.16329v1](http://arxiv.org/abs/2407.16329v1)|null|
|**2024-07-23**|**Graph-Structured Speculative Decoding**|Zhuocheng Gong et.al.|[2407.16207v1](http://arxiv.org/abs/2407.16207v1)|null|
|**2024-07-23**|**Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**|Yannick Assogba et.al.|[2407.21049v1](http://arxiv.org/abs/2407.21049v1)|null|
|**2024-07-23**|**Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**|Yang Liu et.al.|[2407.16127v1](http://arxiv.org/abs/2407.16127v1)|[link](https://github.com/nju-websoft/dift)|
|**2024-07-22**|**Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**|Soojin Yoon et.al.|[2407.15588v1](http://arxiv.org/abs/2407.15588v1)|[link](https://github.com/eralign/eralign)|
|**2024-07-22**|**Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**|Huanjing Zhao et.al.|[2407.15431v1](http://arxiv.org/abs/2407.15431v1)|null|
|**2024-07-22**|**LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**|Jiaxing Zhang et.al.|[2407.15351v2](http://arxiv.org/abs/2407.15351v2)|null|
|**2024-07-21**|**Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**|Yu Zhang et.al.|[2407.15141v1](http://arxiv.org/abs/2407.15141v1)|null|
|**2024-07-20**|**On the Design and Analysis of LLM-Based Algorithms**|Yanxi Chen et.al.|[2407.14788v1](http://arxiv.org/abs/2407.14788v1)|[link](https://github.com/modelscope/agentscope)|
|**2024-07-19**|**LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**|Chen-Chia Chang et.al.|[2407.18269v1](http://arxiv.org/abs/2407.18269v1)|null|
|**2024-07-19**|**Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**|Suvajit Patra et.al.|[2407.14224v1](http://arxiv.org/abs/2407.14224v1)|null|
|**2024-07-19**|**Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**|Quan Li et.al.|[2407.13989v1](http://arxiv.org/abs/2407.13989v1)|null|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699v1](http://arxiv.org/abs/2407.13699v1)|null|
|**2024-07-18**|**MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**|Guoli Yin et.al.|[2407.18961v2](http://arxiv.org/abs/2407.18961v2)|[link](https://github.com/apple/axlearn)|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725v1](http://arxiv.org/abs/2407.12725v1)|null|
|**2024-07-17**|**Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**|Youmin Ko et.al.|[2407.12703v3](http://arxiv.org/abs/2407.12703v3)|null|
|**2024-07-17**|**Abstraction Alignment: Comparing Model and Human Conceptual Relationships**|Angie Boggust et.al.|[2407.12543v1](http://arxiv.org/abs/2407.12543v1)|[link](https://github.com/mitvis/abstraction-alignment)|
|**2024-07-17**|**Struct-X: Enhancing Large Language Models Reasoning with Structured Data**|Xiaoyu Tan et.al.|[2407.12522v1](http://arxiv.org/abs/2407.12522v1)|null|
|**2024-07-17**|**Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**|Alexander R. Pelletier et.al.|[2407.12888v1](http://arxiv.org/abs/2407.12888v1)|[link](https://github.com/pinglab-utils/rugged)|
|**2024-07-16**|**A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**|He Chang et.al.|[2407.11638v1](http://arxiv.org/abs/2407.11638v1)|null|
|**2024-07-16**|**Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**|Kai Guo et.al.|[2407.12068v2](http://arxiv.org/abs/2407.12068v2)|null|
|**2024-07-16**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393v2](http://arxiv.org/abs/2407.11393v2)|[link](https://github.com/SamsungLabs/CIC-BART-SSA)|
|**2024-07-15**|**Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**|Shengjie Ma et.al.|[2407.10805v1](http://arxiv.org/abs/2407.10805v1)|null|
|**2024-07-15**|**Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**|Rui Yang et.al.|[2407.10794v1](http://arxiv.org/abs/2407.10794v1)|[link](https://github.com/irenezihuili/cgprompt)|
|**2024-07-15**|**GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**|Hannah Sansford et.al.|[2407.10793v1](http://arxiv.org/abs/2407.10793v1)|null|
|**2024-07-15**|**Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**|W. J. Meijer et.al.|[2407.10743v1](http://arxiv.org/abs/2407.10743v1)|null|
|**2024-07-14**|**AutoGRAMS: Autonomous Graphical Agent Modeling Software**|Ben Krause et.al.|[2407.10049v1](http://arxiv.org/abs/2407.10049v1)|[link](https://github.com/autograms/autograms)|
|**2024-07-13**|**FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**|Dimitris Papadopoulos et.al.|[2407.09888v1](http://arxiv.org/abs/2407.09888v1)|[link](https://github.com/lighteternal/farfetched_nlp)|
|**2024-07-12**|**GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**|Lecheng Kong et.al.|[2407.09709v1](http://arxiv.org/abs/2407.09709v1)|[link](https://github.com/jiaruifeng/gofa)|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450v1](http://arxiv.org/abs/2407.09450v1)|null|
|**2024-07-12**|**The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**|Matteo Belenchia et.al.|[2407.09441v1](http://arxiv.org/abs/2407.09441v1)|null|
|**2024-07-12**|**Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**|David N. Palacio et.al.|[2407.08983v1](http://arxiv.org/abs/2407.08983v1)|null|
|**2024-07-12**|**Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**|Ke Ji et.al.|[2407.08959v1](http://arxiv.org/abs/2407.08959v1)|null|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694v1](http://arxiv.org/abs/2407.08694v1)|null|
|**2024-07-11**|**Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**|Haoyi Xiong et.al.|[2407.08516v3](http://arxiv.org/abs/2407.08516v3)|null|
|**2024-07-10**|**A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**|Arastoo Zibaeirad et.al.|[2407.07966v1](http://arxiv.org/abs/2407.07966v1)|null|
|**2024-07-10**|**Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**|Hao-Tien Lewis Chiang et.al.|[2407.07775v2](http://arxiv.org/abs/2407.07775v2)|null|
|**2024-07-10**|**Teaching Transformers Causal Reasoning through Axiomatic Training**|Aniket Vashishtha et.al.|[2407.07612v1](http://arxiv.org/abs/2407.07612v1)|null|
|**2024-07-10**|**STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**|Aaron Zolnai-Lucas et.al.|[2407.12860v1](http://arxiv.org/abs/2407.12860v1)|[link](https://github.com/aaronzo/STAGE)|
|**2024-07-10**|**GLBench: A Comprehensive Benchmark for Graph with Large Language Models**|Yuhan Li et.al.|[2407.07457v2](http://arxiv.org/abs/2407.07457v2)|[link](https://github.com/nineabyss/glbench)|
|**2024-07-09**|**Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**|Ruiran Su et.al.|[2407.07038v1](http://arxiv.org/abs/2407.07038v1)|null|
|**2024-07-09**|**Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**|Yu-Guan Hsieh et.al.|[2407.06723v1](http://arxiv.org/abs/2407.06723v1)|null|
|**2024-07-09**|**Combining Knowledge Graphs and Large Language Models**|Amanda Kau et.al.|[2407.06564v1](http://arxiv.org/abs/2407.06564v1)|null|
|**2024-07-09**|**FuncEvalGMN: Evaluating Functional Correctness of SQL via Graph Matching Network**|Yi Zhan et.al.|[2407.14530v1](http://arxiv.org/abs/2407.14530v1)|null|
|**2024-07-08**|**MST5 -- Multilingual Question Answering over Knowledge Graphs**|Nikit Srivastava et.al.|[2407.06041v1](http://arxiv.org/abs/2407.06041v1)|[link](https://github.com/dice-group/MST5)|
|**2024-07-08**|**Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding**|Aaron Lohner et.al.|[2407.05910v1](http://arxiv.org/abs/2407.05910v1)|null|
|**2024-07-08**|**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**|Jiaqi Chen et.al.|[2407.05890v1](http://arxiv.org/abs/2407.05890v1)|null|
|**2024-07-08**|**KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions**|Yanxu Zhu et.al.|[2407.05868v1](http://arxiv.org/abs/2407.05868v1)|[link](https://github.com/yanxuzhu/kg-fpq)|
|**2024-07-07**|**Language Models Encode Collaborative Signals in Recommendation**|Leheng Sheng et.al.|[2407.05441v1](http://arxiv.org/abs/2407.05441v1)|[link](https://github.com/lehengthu/alpharec)|
|**2024-07-07**|**LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models**|Weizhi Tang et.al.|[2407.05434v1](http://arxiv.org/abs/2407.05434v1)|[link](https://github.com/rutatang/ltlbench)|
|**2024-07-05**|**Leveraging Graph Structures to Detect Hallucinations in Large Language Models**|Noa Nonkes et.al.|[2407.04485v1](http://arxiv.org/abs/2407.04485v1)|[link](https://github.com/noanonkes/Hallucination-Detection-in-LLMs)|
|**2024-07-05**|**AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**|Petr Anokhin et.al.|[2407.04363v1](http://arxiv.org/abs/2407.04363v1)|[link](https://github.com/airi-institute/arigraph)|
|**2024-07-04**|**Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM**|Peiran Yao et.al.|[2407.04067v1](http://arxiv.org/abs/2407.04067v1)|[link](https://github.com/U-Alberta/AMRS3)|
|**2024-07-04**|**Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning**|Lei Yu et.al.|[2407.03779v1](http://arxiv.org/abs/2407.03779v1)|null|
|**2024-07-03**|**BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations**|Zhantao Yang et.al.|[2407.03314v1](http://arxiv.org/abs/2407.03314v1)|null|
|**2024-07-03**|**Knowledge-based Consistency Testing of Large Language Models**|Sai Sathiesh Rajan et.al.|[2407.12830v1](http://arxiv.org/abs/2407.12830v1)|null|
|**2024-07-03**|**GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models**|Zike Yuan et.al.|[2407.02936v1](http://arxiv.org/abs/2407.02936v1)|[link](https://github.com/zikeyuan/gracore)|
|**2024-07-03**|**Croppable Knowledge Graph Embedding**|Yushan Zhu et.al.|[2407.02779v1](http://arxiv.org/abs/2407.02779v1)|null|
|**2024-07-02**|**Reasoning in Large Language Models: A Geometric Perspective**|Romain Cosentino et.al.|[2407.02678v1](http://arxiv.org/abs/2407.02678v1)|null|
|**2024-07-02**|**Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison**|Devam Mondal et.al.|[2407.02659v1](http://arxiv.org/abs/2407.02659v1)|null|
|**2024-07-02**|**Multi-Peptide: Multimodality Leveraged Language-Graph Learning of Peptide Properties**|Srivathsan Badrinarayanan et.al.|[2407.03380v1](http://arxiv.org/abs/2407.03380v1)|[link](https://github.com/srivathsanb14/multipeptide)|
|**2024-07-02**|**Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**|Pritish Sahu et.al.|[2407.02352v1](http://arxiv.org/abs/2407.02352v1)|null|
|**2024-07-02**|**Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**|Nishant Balepur et.al.|[2407.01992v1](http://arxiv.org/abs/2407.01992v1)|null|
|**2024-07-01**|**CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents**|Tianqi Xu et.al.|[2407.01511v1](http://arxiv.org/abs/2407.01511v1)|[link](https://github.com/camel-ai/crab)|
|**2024-07-01**|**Dynamic Few-Shot Learning for Knowledge Graph Question Answering**|Jacopo D'Abramo et.al.|[2407.01409v1](http://arxiv.org/abs/2407.01409v1)|null|
|**2024-07-01**|**Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters**|Daniil Gurgurov et.al.|[2407.01406v2](http://arxiv.org/abs/2407.01406v2)|[link](https://github.com/d-gurgurov/Injecting-Commonsense-Knowledge-into-LLMs)|
|**2024-07-01**|**SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model**|Lingyue Fu et.al.|[2407.01245v2](http://arxiv.org/abs/2407.01245v2)|null|
|**2024-07-01**|**Revisiting Random Walks for Learning on Graphs**|Jinwoo Kim et.al.|[2407.01214v1](http://arxiv.org/abs/2407.01214v1)|[link](https://github.com/jw9730/random-walk)|
|**2024-07-01**|**LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation**|Longchao Da et.al.|[2407.00994v2](http://arxiv.org/abs/2407.00994v2)|null|
|**2024-06-30**|**Actionable Cyber Threat Intelligence using Knowledge Graphs and Large Language Models**|Romy Fieblinger et.al.|[2407.02528v1](http://arxiv.org/abs/2407.02528v1)|null|
|**2024-06-30**|**Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs**|Yifei Zhang et.al.|[2407.00653v1](http://arxiv.org/abs/2407.00653v1)|null|
|**2024-06-29**|**BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science**|Xinna Lin et.al.|[2407.00466v1](http://arxiv.org/abs/2407.00466v1)|[link](https://github.com/westlake-autolab/biokgbench.github.io)|
|**2024-06-29**|**GraphArena: Benchmarking Large Language Models on Graph Computational Problems**|Jianheng Tang et.al.|[2407.00379v1](http://arxiv.org/abs/2407.00379v1)|[link](https://github.com/squareroot3/grapharena)|
|**2024-06-29**|**Teola: Towards End-to-End Optimization of LLM-based Applications**|Xin Tan et.al.|[2407.00326v1](http://arxiv.org/abs/2407.00326v1)|null|
|**2024-06-28**|**Into the Unknown: Generating Geospatial Descriptions for New Environments**|Tzuf Paz-Argaman et.al.|[2406.19967v1](http://arxiv.org/abs/2406.19967v1)|null|
|**2024-06-27**|**Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning**|Miyoung Ko et.al.|[2406.19502v1](http://arxiv.org/abs/2406.19502v1)|[link](https://github.com/kaistai/knowledge-reasoning)|
|**2024-06-27**|**Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**|Hao Fei et.al.|[2406.19255v1](http://arxiv.org/abs/2406.19255v1)|null|
|**2024-06-27**|**TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**|Wen Zhang et.al.|[2406.18916v1](http://arxiv.org/abs/2406.18916v1)|null|

#### Abstracts
##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas N√ºrnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

ÊëòË¶ÅÔºöÊú¨‰ΩìÊòØÁâπÂÆöÈ†òÂüü‰∏≠Áü•Ë≠òÁöÑÂΩ¢ÂºèÂåñË°®Á§∫ÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµÑÁπîÂíåÁêÜËß£Ë§áÈõúÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊú¨‰ΩìÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îËÄóÊôÇÁöÑÂä™Âäõ„ÄÇChEBI ÊòØÂåñÂ≠∏È†òÂüü‰∏≠‰∏ÄÂÄãËëóÂêçÁöÑÊú¨‰ΩìÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË≥áÊ∫êÔºåÁî®ÊñºÂÆöÁæ©ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Â±¨ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÉÖÊ∂µËìã‰∫ÜÂåñÂ≠∏È†òÂüüÂø´ÈÄüÂ¢ûÈï∑ÁöÑÁü•Ë≠ò‰∏≠ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰∏¶‰∏îÊ≤íÊúâÊèê‰æõÁßëÂ≠∏ÊñáÁçªÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂÆÉÊ∂âÂèä‰ΩøÁî®‰æÜËá™ Chebi ÁöÑÁü•Ë≠òÊì¥ÂÖÖÁèæÊúâÁöÑË®ªÈáãÊñáÊú¨Ë™ûÊñôÂ∫´Ôºå‰∏¶ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Ë≠òÂà•ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Âú®ÁßëÂ≠∏ÊñáÊú¨‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÁµêÂêàÊú¨‰ΩìÁü•Ë≠òÂíå LLM ÁöÑË™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÊàëÂÄëÂú®Ë≠òÂà•ÁßëÂ≠∏ÊñáÁçª‰∏≠ÁöÑÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî®ÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû‰∏ÄÁµÑ 8,000 ÁØá ChemRxiv ÊñáÁ´†‰∏≠ÊèêÂèñÂÆÉÂÄëÔºå‰∏¶ÊáâÁî®Á¨¨‰∫åÂÄã LLM ‰æÜÂª∫Á´ã‰∏ÄÂÄãÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî® (CEAR) ÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂÆÉÊèê‰æõË£úÂÖÖ ChEBI ÁöÑË≥áË®äÔºå‰∏¶ÊúâÂä©ÊñºÊì¥ÂÖÖÂÆÉ„ÄÇ

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v2 by Xiny Pan, Daniel Hern√°ndez, Philipp Seifer, Ralf L√§mmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥ÔºåÊàëÂÄëÂ∑≤Á∂ìÁúãÂà∞Â§ßÂûãÁü•Ë≠òÂúñË≠úÁöÑÂá∫ÁèæÔºåÁµêÂêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑË≥áË®ä„ÄÇÊúâÊôÇÔºåÊ≠§Ë≥áË®ä‰ª•Â∞çÂÖ∂‰ªñÊñ∑Ë®ÄÁöÑÊñ∑Ë®ÄÂΩ¢ÂºèÊèê‰æõÔºåÂÆöÁæ©Êñ∑Ë®ÄÊúâÊïàÁöÑËÑàÁµ°„ÄÇRDF ÁöÑÊúÄÊñ∞Êì¥ÂÖÖÂÖÅË®±Â∞çÊñ∑Ë®ÄÈÄ≤Ë°åÈô≥Ëø∞ÔºåÁ®±ÁÇ∫ RDF-starÔºåÁõÆÂâçÊ≠£Âú®‰øÆË®ÇÁÇ∫ W3C Ê®ôÊ∫ñ„ÄÇÁÑ∂ËÄåÔºåÊ≤íÊúâÈáùÂ∞çÈÄô‰∫õ RDF-star Èô≥Ëø∞ÁöÑË™ûÁæ©ÊèêÂá∫Âª∫Ë≠∞Ôºå‰πüÊ≤íÊúâÂÖßÂª∫ÁöÑÈÅã‰ΩúÂ∑•ÂÖ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂõõÂÄºÈÇèËºØÁöÑÁü•Ë≠ò RDF-star ÂÖÉË≥áÊñôÊü•Ë©¢Ë™ûË®ÄÔºåÁ®±ÁÇ∫ eSPARQL„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÊì¥ÂÖÖ‰∫Ü RDF-star ÁöÑÊü•Ë©¢Ë™ûË®Ä SPARQL-starÔºå‰∏¶‰ΩøÁî®Êñ∞ÁöÑ FROM Â≠êÂè•È°ûÂûã‰æÜ‰øÉÈÄ≤ÈÅã‰ΩúÔºåÂåÖÊã¨Â§öÈáç‰∏îÊúâÊôÇË°ùÁ™ÅÁöÑ‰ø°Âøµ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÂèØ‰ª•Ë°®ÈÅîÂõõÂÄãÁî®‰æãÊü•Ë©¢ÔºåÂåÖÊã¨‰∏ãÂàóÂäüËÉΩÔºö(i) Êü•Ë©¢ÂÄã‰∫∫ÁöÑ‰ø°ÂøµÔºå(ii) ‰ø°ÂøµÁöÑÂΩôÁ∏ΩÔºå(iii) Êü•Ë©¢Ë™∞ËàáÊüê‰∫∫Ë°ùÁ™ÅÔºå‰ª•Âèä (iv) ÈóúÊñº‰ø°ÂøµÁöÑ‰ø°ÂøµÔºàÂç≥‰ø°ÂøµÁöÑÂ∑¢ÁãÄÔºâ„ÄÇ

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

ÊëòË¶ÅÔºöÁé∞ÂÆû‰∏ñÁïåÁöÑÂØºËà™ÈÄöÂ∏∏Ê∂âÂèäÂ§ÑÁêÜÊÑèÂ§ñÁöÑÈöúÁ¢çÔºå‰æãÂ¶ÇÂÖ≥ÁùÄÁöÑÈó®„ÄÅÁßªÂä®ÁöÑÁâ©‰ΩìÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÂÆû‰Ωì„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑËßÜËßâÂíåËØ≠Ë®ÄÂØºËà™ (VLN) ‰ªªÂä°ÈÄöÂ∏∏ÂÅáËÆæÊåá‰ª§‰∏éÂõ∫ÂÆöÁöÑÂíåÈ¢ÑÂÆö‰πâÁöÑÂØºËà™ÂõæÂÆåÂÖ®‰∏ÄËá¥ÔºåÊ≤°Êúâ‰ªª‰ΩïÈöúÁ¢ç„ÄÇËøôÁßçÂÅáËÆæÂøΩÁï•‰∫ÜÂÆûÈôÖÂØºËà™ÂõæÂíåÁªôÂÆöÊåá‰ª§‰∏≠ÊΩúÂú®ÁöÑÂ∑ÆÂºÇÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂÆ§ÂÜÖÂíåÂÆ§Â§ñ‰ª£ÁêÜÂá∫Áé∞ÈáçÂ§ßÊïÖÈöú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÈÄöËøá‰øÆÊîπÂØºËà™ÂõæÂíåËßÜËßâËßÇÂØüÔºåÂ∞ÜÂêÑÁßçÈöúÁ¢çÊï¥ÂêàÂà∞ R2R Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂºïÂÖ•‰∫ÜÂàõÊñ∞Êï∞ÊçÆÈõÜÂíå‰ªªÂä°ÔºåÂç≥Â∏¶ÊúâÊÑèÂ§ñÈöúÁ¢çÁöÑ R2R (R2R-UNO)„ÄÇR2R-UNO ÂåÖÂê´ÂêÑÁßçÁ±ªÂûãÂíåÊï∞ÈáèÁöÑË∑ØÂæÑÈöúÁ¢çÔºå‰ª•ÁîüÊàê VLN Á†îÁ©∂ÁöÑÊåá‰ª§-Áé∞ÂÆû‰∏çÂåπÈÖç„ÄÇÂú® R2R-UNO ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑ VLN ÊñπÊ≥ïÂú®Èù¢ÂØπÊ≠§Á±ª‰∏çÂåπÈÖçÊó∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàòÔºåËøôË°®ÊòéÂÆÉ‰ª¨‰∏•Ê†ºÈÅµÂæ™Êåá‰ª§ÔºåËÄå‰∏çÊòØËá™ÈÄÇÂ∫îÂú∞ÂØºËà™„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ObVLNÔºàÂèóÈòª VLNÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨ËØæÁ®ãËÆ≠ÁªÉÁ≠ñÁï•ÂíåËôöÊãüÂõæÊûÑÂª∫Ôºå‰ª•Â∏ÆÂä©‰ª£ÁêÜÊúâÊïàÂú∞ÈÄÇÂ∫îÂèóÈòªÁéØÂ¢É„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåObVLN ‰∏ç‰ªÖÂú®Êó†ÈöúÁ¢çÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÂú®ÊÑèÂ§ñÈöúÁ¢ç‰∏≠‰πüËé∑Âæó‰∫ÜÂÆûË¥®ÊÄßÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÂèØÈù†„ÄÅÁµêÊßãÂåñ„ÄÅÁâπÂÆöÊñºÈ†òÂüü‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•Ë≠òÔºå‰æÜË£úÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåKG Âíå LLM ÈÄöÂ∏∏ÊòØÂàÜÈñãÈñãÁôºÔºå‰∏¶‰∏îÂøÖÈ†àÂú®Ë®ìÁ∑¥ÂæåÊï¥Âêà„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü Tree-of-TraversalsÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Êé®ÁêÜÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩËÆìÈªëÁõí LLM ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄã KG„ÄÇË©≤ÊºîÁÆóÊ≥ïÁÇ∫ LLM Êèê‰æõËàá KG ‰ªãÈù¢ÁöÑÂãï‰ΩúÔºå‰∏¶ËÆì LLM ËÉΩÂú®ÂèØËÉΩÁöÑÊÄùËÄÉÂíåÂãï‰Ωú‰∏äÂü∑Ë°åÊ®πÁãÄÊêúÂ∞ãÔºå‰ª•ÊâæÂá∫È´òÂ∫¶‰ø°ÂøÉÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁÜ±ÈñÄÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåTree-of-Traversals Â§ßÂπÖÊèêÂçá‰∫ÜÂïèÈ°åËß£Á≠îÂíå KG ÂïèÈ°åËß£Á≠î‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/amazon-science/tree-of-traversals} ÂèñÂæó

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÂèØËÉΩ‰ΩøË®±Â§öÈ†òÂüüÂèóÁõä„ÄÇÁ´ØÂà∞Á´ØËá™ÂãïÈßïÈßõ (e2eAD) ÊòØÂÖ∏ÂûãÈ†òÂüü‰πã‰∏ÄÔºåÂõ†ÁÇ∫ LLM ÊîØÊè¥Ë∂ä‰æÜË∂äÂ§öÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§Èù¢Ëá®Êñ∞ÁöÑÊ©üÊúÉ„ÄÇÂú®Ê≠§ÔºåÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SimpleLLM4AD ÁöÑ e2eAD ÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠Ôºåe2eAD ‰ªªÂãôÂàÜÁÇ∫ÂõõÂÄãÈöéÊÆµÔºåÂàÜÂà•ÊòØÊÑüÁü•„ÄÅÈ†êÊ∏¨„ÄÅË¶èÂäÉÂíåË°åÁÇ∫„ÄÇÊØèÂÄãÈöéÊÆµÂåÖÂê´Â§öÂÄãË¶ñË¶∫ÂïèÁ≠î (VQA) ÈÖçÂ∞çÔºå‰∏î VQA ÈÖçÂ∞çÁõ∏‰∫íÈÄ£Êé•ÔºåÊßãÂª∫‰∏ÄÂÄãÁ®±ÁÇ∫ÂúñÂΩ¢ VQA (GVQA) ÁöÑÂúñÂΩ¢„ÄÇÈÄèÈÅé VLM ÂàÜÈöéÊÆµÊé®ÁêÜ GVQA ‰∏≠ÁöÑÊØèÂÄã VQA ÈÖçÂ∞çÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÂØ¶ÁèæÁ´ØÂà∞Á´ØÈßïÈßõ„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠ÔºåÊé°Áî®Ë¶ñË¶∫Transformer (ViT) Ê®°Âûã‰æÜËôïÁêÜ nuScenes Ë¶ñË¶∫Ë≥áÊñôÔºåÂêåÊôÇÂà©Áî® VLM ‰æÜË©ÆÈáãÂíåÊé®ÁêÜÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠ÊèêÂèñÁöÑË≥áË®ä„ÄÇÂú®ÊÑüÁü•ÈöéÊÆµÔºåÁ≥ªÁµ±Ë≠òÂà•ÂíåÂàÜÈ°ûÈßïÈßõÁí∞Â¢É‰∏≠ÁöÑÁâ©‰ª∂„ÄÇÈ†êÊ∏¨ÈöéÊÆµÊ∂âÂèäÈ†êÊ∏¨ÈÄô‰∫õÁâ©‰ª∂ÁöÑÊΩõÂú®ÁßªÂãï„ÄÇË¶èÂäÉÈöéÊÆµÂà©Áî®Êî∂ÈõÜÁöÑË≥áË®ä‰æÜÂà∂ÂÆöÈßïÈßõÁ≠ñÁï•ÔºåÁ¢∫‰øùËá™ÂãïÈßïÈßõÊ±ΩËªäÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéá„ÄÇÊúÄÂæåÔºåË°åÁÇ∫ÈöéÊÆµÂ∞áË¶èÂäÉÁöÑÂãï‰ΩúËΩâÊèõÁÇ∫ËªäËºõÂèØÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåSimpleLLM4AD Âú®Ë§áÈõúÁöÑÈßïÈßõÂ†¥ÊôØ‰∏≠ÂØ¶Áèæ‰∫ÜÁ´∂Áà≠Âäõ„ÄÇ

##### **Be aware of overfitting by hyperparameter optimization!**
2407.20786v1 by Igor V. Tetko, Ruud van Deursen, Guillaume Godin

Hyperparameter optimization is very frequently employed in machine learning.
However, an optimization of a large space of parameters could result in
overfitting of models. In recent studies on solubility prediction the authors
collected seven thermodynamic and kinetic solubility datasets from different
data sources. They used state-of-the-art graph-based methods and compared
models developed for each dataset using different data cleaning protocols and
hyperparameter optimization. In our study we showed that hyperparameter
optimization did not always result in better models, possibly due to
overfitting when using the same statistical measures. Similar results could be
calculated using pre-set hyperparameters, reducing the computational effort by
around 10,000 times. We also extended the previous analysis by adding a
representation learning method based on Natural Language Processing of smiles
called Transformer CNN. We show that across all analyzed sets using exactly the
same protocol, Transformer CNN provided better results than graph-based methods
for 26 out of 28 pairwise comparisons by using only a tiny fraction of time as
compared to other methods. Last but not least we stressed the importance of
comparing calculation results using exactly the same statistical measures.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí‰∏≠ÈùûÂ∏∏È†ªÁπÅÂú∞‰ΩøÁî®Ë∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ„ÄÇ
ÁÑ∂ËÄåÔºåÂ∞çÂ§ßÂèÉÊï∏Á©∫ÈñìÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÂèØËÉΩÊúÉÂ∞éËá¥Ê®°ÂûãÈÅéÊì¨Âêà„ÄÇÂú®ÊúÄËøëÂ∞çÊ∫∂Ëß£Â∫¶È†êÊ∏¨ÁöÑÁ†îÁ©∂‰∏≠Ôºå‰ΩúËÄÖÂæû‰∏çÂêåÁöÑÊï∏ÊìöÊ∫êÊî∂ÈõÜ‰∫Ü‰∏ÉÂÄãÁÜ±ÂäõÂ≠∏ÂíåÂãïÂäõÂ≠∏Ê∫∂Ëß£Â∫¶Êï∏ÊìöÈõÜ„ÄÇ‰ªñÂÄë‰ΩøÁî®‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÔºå‰∏¶ÊØîËºÉ‰∫Ü‰ΩøÁî®‰∏çÂêåÁöÑÊï∏ÊìöÊ∏ÖÊ¥óÂçîË≠∞ÂíåË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥ÂåñÁÇ∫ÊØèÂÄãÊï∏ÊìöÈõÜÈñãÁôºÁöÑÊ®°Âûã„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëË°®ÊòéË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ‰∏¶ÈùûÁ∏ΩÊòØÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÊ®°ÂûãÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÂú®‰ΩøÁî®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨ÈáèÊôÇÁôºÁîüÈÅéÊì¨Âêà„ÄÇÂèØ‰ª•‰ΩøÁî®È†êË®≠ÁöÑË∂ÖÂèÉÊï∏Ë®àÁÆóÈ°û‰ººÁöÑÁµêÊûúÔºåÂæûËÄåÂ∞áË®àÁÆóÂ∑•‰ΩúÈáèÊ∏õÂ∞ëÁ¥Ñ 10,000 ÂÄç„ÄÇÊàëÂÄëÈÇÑÈÄöÈÅéÊ∑ªÂä†Âü∫ÊñºÁ¨ëÂÆπÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑË°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºàÁ®±ÁÇ∫ Transformer CNNÔºâ‰æÜÊì¥Â±ïÂÖàÂâçÁöÑÂàÜÊûê„ÄÇÊàëÂÄëË°®ÊòéÔºåÂú®‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂçîË≠∞Â∞çÊâÄÊúâÂàÜÊûêÁöÑÈõÜÂêàÈÄ≤Ë°åÂàÜÊûêÊôÇÔºåTransformer CNN Âú® 28 ÂÄãÊàêÂ∞çÊØîËºÉ‰∏≠Êúâ 26 ÂÄãÊØîËºÉÊØîÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÁµêÊûúÔºåËÄåËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÁî®ÁöÑÊôÇÈñìÂè™ÊòØÂæàÂ∞èÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂº∑Ë™ø‰∫Ü‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨Èáè‰æÜÊØîËºÉË®àÁÆóÁµêÊûúÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Harvesting Textual and Structured Data from the HAL Publication Repository**
2407.20595v1 by Francis Kulumba, Wissam Antoun, Guillaume Vimont, Laurent Romary

HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.

ÊëòË¶ÅÔºöHALÔºàÁ∑ö‰∏äË∂ÖÈÄ£ÁµêÊñáÁ´†ÔºâÊòØÊ≥ïÂúãÂúãÂÆ∂Âá∫ÁâàÁâ©Ë≥áÊñôÂ∫´Ôºå
Â§ßÂ§öÊï∏È´òÁ≠âÊïôËÇ≤ÂíåÁ†îÁ©∂ÁµÑÁπîÈÉΩ‰ΩøÁî®ÂÆÉ‰æÜÂà∂ÂÆöÈñãÊîæÁßëÂ≠∏
ÊîøÁ≠ñ„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÊï∏‰ΩçÂúñÊõ∏È§®ÔºåÂÆÉÊòØ‰∏ÄÂÄãË±êÂØåÁöÑÂ≠∏Ë°ìÊñá‰ª∂Ë≥áÊñôÂ∫´Ôºå
‰ΩÜÂÆÉÂú®ÈÄ≤ÈöéÁ†îÁ©∂ÁöÑÊΩõÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®„ÄÇÊàëÂÄëÊèêÂá∫
HALvestÔºå‰∏ÄÂÄãÁç®ÁâπÁöÑË≥áÊñôÈõÜÔºåÂÆÉÂΩåË£ú‰∫ÜÂºïÊñáÁ∂≤Ë∑ØÂíå
Âú® HAL ‰∏äÊèê‰∫§ÁöÑË´ñÊñáÂÖ®Êñá‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÈÄèÈÅéÁØ©ÈÅ∏ HAL
‰∏≠ÁöÑÂ≠∏Ë°ìÂá∫ÁâàÂìÅ‰æÜÂª∫Á´ãÊàëÂÄëÁöÑË≥áÊñôÈõÜÔºåÊúÄÂæåÂæóÂà∞Á¥Ñ 70 Ëê¨‰ªΩÊñá‰ª∂Ôºå
Ê∂µËìã 13 ÂÄãÂ∑≤Ë≠òÂà•È†òÂüüÁöÑ 34 Á®ÆË™ûË®ÄÔºåÈÅ©ÂêàË™ûË®ÄÊ®°Âûã
Ë®ìÁ∑¥Ôºå‰∏¶Áî¢ÁîüÁ¥Ñ 165 ÂÑÑÂÄãË©ûÂΩôÔºàÂÖ∂‰∏≠Ê≥ïÊñáÊúâ 80 ÂÑÑÂÄãÔºå
Ëã±ÊñáÊúâ 70 ÂÑÑÂÄãÔºåÊòØÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑË™ûË®ÄÔºâ„ÄÇÊàëÂÄëÂ∞á
ÊØèÁØáË´ñÊñáÁöÑÂÖÉË≥áÊñôËΩâÊèõÊàêÂºïÊñáÁ∂≤Ë∑ØÔºåÁî¢Áîü‰∏ÄÂÄãÊúâÂêë
Áï∞Ë≥™ÂúñÂΩ¢„ÄÇÊ≠§ÂúñÂΩ¢ÂåÖÂê´Âú® HAL ‰∏äÂîØ‰∏ÄË≠òÂà•ÁöÑ‰ΩúËÄÖÔºå‰ª•Âèä
ÊâÄÊúâÂÖ¨ÈñãÊèê‰∫§ÁöÑË´ñÊñáÂèäÂÖ∂ÂºïÊñá„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãÂü∫Ê∫ñ
‰ΩøÁî®Ë≥áÊñôÈõÜÈÄ≤Ë°å‰ΩúËÄÖÊ≠∏Â±¨ÔºåÂØ¶‰Ωú‰∏ÄÁ≥ªÂàó
ÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Ë°®Á§∫Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÈÄ£ÁµêÈ†êÊ∏¨Ôºå
‰∏¶Ë®éË´ñÊàëÂÄëÁî¢ÁîüÁöÑÁü•Ë≠òÂúñÂΩ¢ÁµêÊßãÁöÑÂØ¶Áî®ÊÄß„ÄÇ

##### **CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**
2407.20564v1 by Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song

While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÂèØÈÄèÈÅéÂæûÂª£Ê≥õÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏≠Áç≤ÂèñË±êÂØåÁöÑ‰∫ãÂØ¶Áü•Ë≠òÔºåÂü∑Ë°åÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÁ∂úÂêàÈÅãÁî®‰∏¶‰ª•Ë§áÈõúÁöÑÊñπÂºèÈÅãÁî®Ê≠§Áü•Ë≠òÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜÁöÑËÉΩÂäõ‰ªçÊúâÂæÖÈÄ≤‰∏ÄÊ≠•Êé¢Ë®é„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅé‰∏ÄÂÄãËá™ÂãïÁîüÊàêÁöÑ‰∏ÄËà¨È†òÂüüÂíåÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂúñË°®Ë§áÈõúÊé®ÁêÜÂïèÈ°åÁöÑÊñ∞Âü∫Ê∫ñÔºåÂ∞çÊúÄÂÖàÈÄ≤ÁöÑ LLM Ë§áÈõúÈÇèËºØÊé®ÁêÜËÉΩÂäõÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óÊé°Áî®Â§öÊ®£ÂåñÁöÑÊÉÖÂ¢ÉÂ≠∏ÁøíÊäÄË°ìÔºåÊè≠Á§∫Âá∫ LLM ÊìÖÈï∑Â∞ç‰∏ÄËà¨‰∏ñÁïåÁü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜÔºå‰ΩÜÂú®ËôïÁêÜÁâπÂÆöÈ†òÂüüÁöÑÂ∞àÊ•≠Áü•Ë≠òÊôÇÂâáÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ΩøÁî®ÊòéÁ¢∫ÁöÑÊÄùËÄÉÈèàÊ¢ùÁ§∫ÁØÑÈÄ≤Ë°åÊèêÁ§∫ÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM Âú®ÂÖ∑ÊúâÂ§öÊ®£ÂåñÈÇèËºØÈÅãÁÆóÁöÑË§áÈõúÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÊúâË∂£ÁöÑÊòØÔºåÊàëÂÄëÁöÑÂèóÊéßË©ï‰º∞Êè≠Èú≤‰∫Ü‰∏ÄÂÄã‰∏çÂ∞çÁ®±ÊÄßÔºåÂÖ∂‰∏≠ LLM Â±ïÁèæÂá∫Âú®ÈõÜÂêàËÅØÈõÜÈÅãÁÆóÊñπÈù¢ÁöÑÁÜüÁ∑¥Â∫¶Ôºå‰ΩÜÂú®ÈõÜÂêà‰∫§ÈõÜÊñπÈù¢ÂçªÈ°ØÂæóÁõ∏Áï∂ÂêÉÂäõÔºåËÄåÈõÜÂêà‰∫§ÈõÜÊ≠£ÊòØÈÇèËºØÊé®ÁêÜÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂæåÁ∫åÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞áÂÖ¨ÈñãÁôºÂ∏ÉÊàëÂÄëÁöÑË©ï‰º∞Âü∫Ê∫ñÂíåÁ®ãÂºèÁ¢º„ÄÇ

##### **Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**
2407.20513v1 by Hossein Rajaby Faghihi, Aliakbar Nafar, Andrzej Uszok, Hamid Karimian, Parisa Kordjamshidi

This paper presents a conversational pipeline for crafting domain knowledge
for complex neuro-symbolic models through natural language prompts. It
leverages large language models to generate declarative programs in the
DomiKnowS framework. The programs in this framework express concepts and their
relationships as a graph in addition to logical constraints between them. The
graph, later, can be connected to trainable neural models according to those
specifications. Our proposed pipeline utilizes techniques like dynamic
in-context demonstration retrieval, model refinement based on feedback from a
symbolic parser, visualization, and user interaction to generate the tasks'
structure and formal knowledge representation. This approach empowers domain
experts, even those not well-versed in ML/AI, to formally declare their
knowledge to be incorporated in customized neural models in the DomiKnowS
framework.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çË©±ÂºèÁÆ°ÈÅìÔºåÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊèêÁ§∫ÔºåÁÇ∫Ë§áÈõúÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊ®°ÂûãÂª∫Á´ãÈ†òÂüüÁü•Ë≠ò„ÄÇÂÆÉÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú® DomiKnowS Ê°ÜÊû∂‰∏≠Áî¢ÁîüÂÆ£ÂëäÂºèÁ®ãÂºè„ÄÇÊ≠§Ê°ÜÊû∂‰∏≠ÁöÑÁ®ãÂºèÊúÉÂ∞áÊ¶ÇÂøµÂèäÂÖ∂Èóú‰øÇË°®Á§∫ÁÇ∫ÂúñÂΩ¢Ôºå‰∏¶Âú®ÂÆÉÂÄë‰πãÈñìÂä†‰∏äÈÇèËºØÁ¥ÑÊùü„ÄÇ‰πãÂæåÔºåÂèØ‰ª•Ê†πÊìöÈÄô‰∫õË¶èÊ†ºÂ∞áÂúñÂΩ¢ÈÄ£Êé•Âà∞ÂèØË®ìÁ∑¥ÁöÑÁ•ûÁ∂ìÊ®°Âûã„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÁÆ°ÈÅìÂà©Áî®ÂãïÊÖãÊÉÖÂ¢É‰∏≠Á§∫ÁØÑÊ™¢Á¥¢„ÄÅÂü∫ÊñºÁ¨¶ËôüËß£ÊûêÂô®ÂõûÈ•ãÁöÑÊ®°ÂûãÁ≤æÁÖâ„ÄÅË¶ñË¶∫ÂåñÂíå‰ΩøÁî®ËÄÖ‰∫íÂãïÁ≠âÊäÄË°ìÔºå‰ª•Áî¢Áîü‰ªªÂãôÁµêÊßãÂíåÂΩ¢ÂºèÁü•Ë≠òË°®Á§∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÆìÈ†òÂüüÂ∞àÂÆ∂ÔºåÂç≥‰ΩøÊòØ‰∏çÁÜüÊÇâÊ©üÂô®Â≠∏ÁøíÔºè‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∫∫Ôºå‰πüËÉΩÊ≠£ÂºèÂÆ£Âëä‰ªñÂÄëÁöÑÁü•Ë≠òÔºå‰∏¶Â∞áÂÖ∂Á¥çÂÖ• DomiKnowS Ê°ÜÊû∂‰∏≠ÁöÑËá™Ë®ÇÁ•ûÁ∂ìÊ®°Âûã„ÄÇ

##### **What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**
2407.20382v1 by Navapat Nananukul, Wichayaporn Wongkamjan

Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.

ÊëòË¶ÅÔºöËßíËâ≤ÊâÆÊºîÈÅäÊà≤ (RPG) ÁÇ∫Áé©ÂÆ∂Êèê‰æõ‰∏ÄÂÄãË±êÂØå‰∏î‰∫íÂãïÁöÑ‰∏ñÁïå‰æõÂÖ∂Êé¢Á¥¢„ÄÇÂ∞çË©±‰ΩúÁÇ∫ÈñãÁôºËÄÖËàáÁé©ÂÆ∂‰πãÈñìÁöÑ‰∏ªË¶ÅÊ∫ùÈÄöÊñπÂºèÔºå‰ª•ÊåáÂçó„ÄÅNPC ‰∫íÂãïÂíåË™™ÊïÖ‰∫ãÁ≠âÂêÑÁ®ÆÂΩ¢ÂºèÂëàÁèæ„ÄÇÈõñÁÑ∂Â§ßÂ§öÊï∏ÈÅäÊà≤‰æùË≥¥ÊñºÊõ∏Èù¢ËÖ≥Êú¨‰æÜÂÆöÁæ©‰∏ªÁ∑öÊïÖ‰∫ãÂíåËßíËâ≤ÂÄãÊÄßÔºå‰ΩÜÈÄèÈÅéËßíËâ≤‰πãÈñìÁöÑÈñíËÅä‰∫íÂãïÔºåÂèØ‰ª•Â§ßÂπÖÊèêÂçáÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ∞çË©±Â°´ÂÖÖÊ°ÜÊû∂ÔºåÂà©Áî®Áî±Áü•Ë≠òÂúñË≠úÂ¢ûÂº∑ÁöÑ LLM ‰æÜÁî¢ÁîüÂãïÊÖã‰∏îÁ¨¶ÂêàÊÉÖÂ¢ÉÁöÑÂ∞çË©±‰∫íÂãï„ÄÇÊàëÂÄëÂú® Final Fantasy VII Remake ÂíåÂØ∂ÂèØÂ§¢ÁöÑÁí∞Â¢É‰∏≠Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊ°ÜÊû∂ÔºåÊèê‰æõ‰∫ÜÂÆöÊÄßÂíåÂÆöÈáèÁöÑË≠âÊìöÔºåË≠âÊòé‰∫Ü GPT-4 ÂÖ∑ÂÇô‰ª•ÂÆöÁæ©Â•ΩÁöÑÂÄãÊÄßË°åÂãï‰∏¶Áî¢ÁîüÂ∞çË©±ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªçÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºå‰æãÂ¶Ç GPT-4 ÈÅéÊñºÊ≠£Èù¢ÔºåÊàñËÄÖËºÉÁÇ∫Á¥∞ÂæÆÁöÑÂÄãÊÄßÔºå‰æãÂ¶ÇÊàêÁÜüÂ∫¶ÔºåÂæÄÂæÄÂìÅË≥™‰ΩéÊñºËºÉÊòéÈ°ØÁöÑÁâπË≥™Ôºå‰æãÂ¶ÇËÜΩÊÄØ„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÂçîÂä©ÈñãÁôºËÄÖÊâìÈÄ†Êõ¥Á¥∞Á∑ªÁöÑÂ°´ÂÖÖÂ∞çË©±ÔºåÂæûËÄåË±êÂØåÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü‰∏¶ÊèêÂçáÊï¥È´î RPG È´îÈ©ó„ÄÇ

##### **MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**
2407.20183v1 by Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao

Information seeking and integration is a complex cognitive task that consumes
enormous time and effort. Inspired by the remarkable progress of Large Language
Models, recent works attempt to solve this task by combining LLMs and search
engines. However, these methods still obtain unsatisfying performance due to
three challenges: (1) complex requests often cannot be accurately and
completely retrieved by the search engine once (2) corresponding information to
be integrated is spread over multiple web pages along with massive noise, and
(3) a large number of web pages with long contents may quickly exceed the
maximum context length of LLMs. Inspired by the cognitive process when humans
solve these problems, we introduce MindSearch to mimic the human minds in web
information seeking and integration, which can be instantiated by a simple yet
effective LLM-based multi-agent framework. The WebPlanner models the human mind
of multi-step information seeking as a dynamic graph construction process: it
decomposes the user query into atomic sub-questions as nodes in the graph and
progressively extends the graph based on the search result from WebSearcher.
Tasked with each sub-question, WebSearcher performs hierarchical information
retrieval with search engines and collects valuable information for WebPlanner.
The multi-agent design of MindSearch enables the whole framework to seek and
integrate information parallelly from larger-scale (e.g., more than 300) web
pages in 3 minutes, which is worth 3 hours of human effort. MindSearch
demonstrates significant improvement in the response quality in terms of depth
and breadth, on both close-set and open-set QA problems. Besides, responses
from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web
and Perplexity.ai applications, which implies that MindSearch can already
deliver a competitive solution to the proprietary AI search engine.

ÊëòË¶ÅÔºöË≥áË®äÊêúÂ∞ãËàáÊï¥ÂêàÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑË™çÁü•‰ªªÂãôÔºåÊúÉËÄóË≤ªÂ§ßÈáèÊôÇÈñìËàáÁ≤æÂäõ„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈ°ØËëóÈÄ≤Â±ïÁöÑÂïüÁôº‰∏ãÔºåËøëÊúüÁ†îÁ©∂ÂòóË©¶ÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËàáÊêúÂ∞ãÂºïÊìé‰æÜËß£Ê±∫Ê≠§‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ï‰ªçÂõ†‰∏âÈ†ÖÊåëÊà∞ËÄåÁÑ°Ê≥ïÁç≤Âæó‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩÔºö(1) Ë§áÈõúÁöÑÊü•Ë©¢ÈÄöÂ∏∏ÁÑ°Ê≥ïÁî±ÊêúÂ∞ãÂºïÊìé‰∏ÄÊ¨°Ê∫ñÁ¢∫‰∏îÂÆåÊï¥Âú∞Êì∑ÂèñÔºå(2) Ë¶ÅÊï¥ÂêàÁöÑÂ∞çÊáâË≥áË®äÊï£Â∏ÉÂú®Â§öÂÄãÁ∂≤È†Å‰∏≠‰∏î‰º¥Èö®ËëóÂ§ßÈáèÈõúË®äÔºå‰ª•Âèä (3) Â§ßÈáèÂÖßÂÆπÈÅéÈï∑ÁöÑÁ∂≤È†ÅÂèØËÉΩÊúÉÂø´ÈÄüË∂ÖÈÅéÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊúÄÂ§ßËÑàÁµ°Èï∑Â∫¶„ÄÇÂú®‰∫∫È°ûËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÁöÑË™çÁü•ÈÅéÁ®ã‰∏≠Áç≤ÂæóÈùàÊÑüÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MindSearch ‰æÜÊ®°Êì¨‰∫∫È°ûÂøÉÊô∫Âú®Á∂≤È†ÅË≥áË®äÊêúÂ∞ãËàáÊï¥Âêà‰∏≠ÁöÑË°åÁÇ∫ÔºåÈÄôÂèØ‰ª•Áî®‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂ§ö‰ª£ÁêÜÊû∂Êßã‰æÜÂØ¶‰æãÂåñ„ÄÇWebPlanner ‰ª•ÂãïÊÖãÂúñÂΩ¢Âª∫ÊßãÈÅéÁ®ã‰æÜÂª∫Ê®°‰∫∫È°ûÂøÉÊô∫ÁöÑÂ§öÊ≠•È©üË≥áË®äÊêúÂ∞ãÔºöÂÆÉÂ∞á‰ΩøÁî®ËÄÖÊü•Ë©¢ÂàÜËß£ÊàêÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÔºå‰ΩúÁÇ∫ÂéüÂ≠êÂåñÂ≠êÂïèÈ°åÔºå‰∏¶Ê†πÊìö WebSearcher ÁöÑÊêúÂ∞ãÁµêÊûúÈÄêÊ≠•Âª∂‰º∏ÂúñÂΩ¢„ÄÇWebSearcher ‰ª•ÊØèÂÄãÂ≠êÂïèÈ°åÁÇ∫‰ªªÂãôÔºåÂü∑Ë°åÊêúÂ∞ãÂºïÊìéÁöÑÂàÜÂ±§ÂºèË≥áË®äÊì∑ÂèñÔºå‰∏¶ÁÇ∫ WebPlanner Êî∂ÈõÜÊúâÂÉπÂÄºÁöÑË≥áË®ä„ÄÇMindSearch ÁöÑÂ§ö‰ª£ÁêÜË®≠Ë®àËÆìÊï¥ÂÄãÊû∂ÊßãÂèØ‰ª•Âú® 3 ÂàÜÈêòÂÖßÂπ≥Ë°åÂú∞ÂæûÊõ¥Â§ßË¶èÊ®°Ôºà‰æãÂ¶ÇË∂ÖÈÅé 300 ÂÄãÔºâÁöÑÁ∂≤È†Å‰∏≠ÊêúÂ∞ã‰∏¶Êï¥ÂêàË≥áË®äÔºåÈÄôÁõ∏Áï∂Êñº 3 Â∞èÊôÇÁöÑ‰∫∫Âäõ„ÄÇMindSearch Âú®Ê∑±Â∫¶ÂíåÂª£Â∫¶ÊñπÈù¢ÈÉΩÈ°ØËëóÊèêÂçá‰∫ÜÂõûÊáâÂìÅË≥™ÔºåÁÑ°Ë´ñÊòØÂú®Â∞ÅÈñâÂºèÊàñÈñãÊîæÂºèÂïèÁ≠îÂïèÈ°å‰∏ä„ÄÇÊ≠§Â§ñÔºå‰∫∫È°ûÊõ¥ÂÅèÂ•ΩÂü∫Êñº InternLM2.5-7B ÁöÑ MindSearch ÂõûÊáâÔºåÂãùÈÅé ChatGPT-Web Âíå Perplexity.ai ÊáâÁî®Á®ãÂºèÔºåÈÄôË°®Á§∫ MindSearch Â∑≤Á∂ìÂèØ‰ª•ÁÇ∫Â∞àÊúâ AI ÊêúÂ∞ãÂºïÊìéÊèê‰æõÊúâÁ´∂Áà≠ÂäõÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **rLLM: Relational Table Learning with LLMs**
2407.20157v1 by Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, Jianhua Li

We introduce rLLM (relationLLM), a PyTorch library designed for Relational
Table Learning (RTL) with Large Language Models (LLMs). The core idea is to
decompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural
Networks into standardized modules, to enable the fast construction of novel
RTL-type models in a simple "combine, align, and co-train" manner. To
illustrate the usage of rLLM, we introduce a simple RTL method named
\textbf{BRIDGE}. Additionally, we present three novel relational tabular
datasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope
rLLM can serve as a useful and easy-to-use development framework for
RTL-related tasks. Our code is available at:
https://github.com/rllm-project/rllm.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫Ü rLLM (relationLLM)Ôºå‰∏ÄÂÄãÂ∞àÁÇ∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈóú‰øÇË°®Â≠∏Áøí (RTL) ÊâÄË®≠Ë®àÁöÑ PyTorch ÂáΩÂºèÂ∫´„ÄÇÊ†∏ÂøÉÊ¶ÇÂøµÊòØÂ∞áÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅLLM ÂíåË°®Á•ûÁ∂ìÁ∂≤Ë∑ØÂàÜËß£ÁÇ∫Ê®ôÊ∫ñÂåñÊ®°ÁµÑÔºå‰ª•‰æø‰ª•Á∞°ÂñÆÁöÑ„ÄåÁµÑÂêà„ÄÅÂ∞çÈΩäÂíåÂÖ±ÂêåË®ìÁ∑¥„ÄçÊñπÂºèÂø´ÈÄüÂª∫ÊßãÊñ∞Âûã RTL È°ûÂûãÊ®°Âûã„ÄÇÁÇ∫‰∫ÜË™™Êòé rLLM ÁöÑÁî®Ê≥ïÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂêçÁÇ∫ \textbf{BRIDGE} ÁöÑÁ∞°ÂñÆ RTL ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÂº∑ÂåñÁ∂ìÂÖ∏Ë≥áÊñôÈõÜ‰æÜÂëàÁèæ‰∏âÂÄãÊñ∞Á©éÁöÑÈóú‰øÇË°®Ê†ºË≥áÊñôÈõÜ (TML1M„ÄÅTLF2K Âíå TACM12K)„ÄÇÊàëÂÄëÂ∏åÊúõ rLLM ËÉΩÂ§†‰ΩúÁÇ∫ RTL Áõ∏Èóú‰ªªÂãôÊúâÁî®ÁöÑ‰∏îÊòìÊñº‰ΩøÁî®ÁöÑÈñãÁôºÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö
https://github.com/rllm-project/rllm„ÄÇ

##### **Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**
2407.19643v2 by Yunsheng Wang, Songhao Chen, Kevin Jin

Knowledge graphs (KGs) are essential in applications such as network
alignment, question-answering, and recommender systems (RSs) since they offer
structured relational data that facilitate the inference of indirect
relationships. However, the development of KG-based RSs capable of processing
user inputs in natural language faces significant challenges. Firstly, natural
language processing units must effectively handle the ambiguity and variability
in human language to interpret user intents accurately. Secondly, the system
must precisely identify and link entities, like product names, to their
corresponding nodes in KGs. To overcome these challenges, supported by Lenovo,
we developed a novel chatbot called "Prometheus," which integrates a KG with a
large language model (LLM), specifically designed for recommending computer
components. This chatbot can accurately decode user requests and deliver
personalized recommendations derived from KGs, ensuring precise comprehension
and response to their computer setup needs.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Âú®Á∂≤Ë∑ØÊØîÂ∞ç„ÄÅÂïèÁ≠îÂíåÊé®Ëñ¶Á≥ªÁµ± (RS) Á≠âÊáâÁî®‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊèê‰æõÁµêÊßãÂåñÁöÑÈóú‰øÇË≥áÊñôÔºåÊúâÂä©ÊñºÊé®Êñ∑ÈñìÊé•Èóú‰øÇ„ÄÇÁÑ∂ËÄåÔºåÈñãÁôºËÉΩÂ§†ËôïÁêÜËá™ÁÑ∂Ë™ûË®Ä‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂü∫Êñº KG ÁöÑ RS Èù¢Ëá®ËëóÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂñÆÂÖÉÂøÖÈ†àÊúâÊïàËôïÁêÜ‰∫∫È°ûË™ûË®Ä‰∏≠ÁöÑÊ®°Á≥äÊÄßÂíåËÆäÁï∞ÊÄßÔºåÊâçËÉΩÊ∫ñÁ¢∫Âú∞Ëß£Èáã‰ΩøÁî®ËÄÖÊÑèÂúñ„ÄÇÂÖ∂Ê¨°ÔºåÁ≥ªÁµ±ÂøÖÈ†àÊ∫ñÁ¢∫Ë≠òÂà•ÂíåÈÄ£ÁµêÂØ¶È´îÔºà‰æãÂ¶ÇÁî¢ÂìÅÂêçÁ®±ÔºâÂà∞ KG ‰∏≠Â∞çÊáâÁöÑÁØÄÈªû„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÂú®ËÅØÊÉ≥ÁöÑÊîØÊè¥‰∏ãÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÊ¨æÂêçÁÇ∫„ÄåÊôÆÁæÖÁ±≥‰øÆÊñØ„ÄçÁöÑÊñ∞ËÅäÂ§©Ê©üÂô®‰∫∫ÔºåÂÆÉÂ∞á KG ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÂ∞àÈñÄÁî®ÊñºÊé®Ëñ¶ÈõªËÖ¶ÁµÑ‰ª∂„ÄÇÊ≠§ËÅäÂ§©Ê©üÂô®‰∫∫ÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Ëß£Á¢º‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÔºå‰∏¶Êèê‰æõÂæû KG ‰∏≠Ë°çÁîüÁöÑÂÄã‰∫∫ÂåñÊé®Ëñ¶ÔºåÁ¢∫‰øùÁ≤æÁ¢∫ÁêÜËß£ÂíåÂõûÊáâÂÖ∂ÈõªËÖ¶Ë®≠ÂÆöÈúÄÊ±Ç„ÄÇ

##### **TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**
2407.19616v1 by Selma Wanna, Ryan Barron, Nick Solovyev, Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov

Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.

ÊëòË¶ÅÔºö‰∏ªÈ°åÂª∫Ê®°ÊòØ‰∏ÄÁ®ÆÂæûÂ§ßÈáèÈùûÁµêÊßãÂåñÊñáÊú¨‰∏≠ÁµÑÁπîÂíåÊèêÂèñ‰∏ªÈ°åÁöÑÊäÄË°ì„ÄÇÈùûË≤†Áü©Èô£ÂàÜËß£ (NMF) ÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÁÑ°Áõ£Áù£ÊñπÊ≥ïÔºåÂÆÉÂ∞áË©ûÈ†ª-ÈÄÜÊñá‰ª∂È†ªÁéá (TF-IDF) Áü©Èô£ÂàÜËß£ÁÇ∫ÊΩõÂú®‰∏ªÈ°åÔºå‰∏¶ÊìöÊ≠§Â∞çÊï∏ÊìöÈõÜÈÄ≤Ë°åÂàÜÊÆµ„ÄÇÂÑòÁÆ° NMF ÂèØÁî®ÊñºÂº∑Ë™øÊ®°ÂºèÂíåÁæ§ÁµÑÊñá‰ª∂Ôºå‰ΩÜÂÆÉ‰∏çÊèê‰æõÊòéÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§ÔºåÈÄôÈúÄË¶Å‰∏ªÈ°åÂ∞àÂÆ∂ (SME) ÊâãÂãïÂàÜÈÖçÊ®ôÁ±§„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÁî®ÊñºËá™ÂãïÊ®ôË®òÈÄöÈÅé NMF ÈÄ≤Ë°åÁæ§ÁµÑÁöÑÊñá‰ª∂Ôºå‰∏¶Ëá™ÂãïÁ¢∫ÂÆöÊ®°Âûã (NMFk)„ÄÇÈÄöÈÅéÂà©Áî® NMFk ÁöÑËº∏Âá∫‰∏¶Êé°Áî®ÊèêÁ§∫Â∑•Á®ãÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁîüÊàêÊ∫ñÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§„ÄÇÊàëÂÄëÂ∞çË∂ÖÈÅé 34,000 ÁØáÈóúÊñºÁü•Ë≠òÂúñË≠úÁöÑÁßëÂ≠∏ÊëòË¶ÅÈÄ≤Ë°åÁöÑÊ°à‰æãÁ†îÁ©∂Ë≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Â¢ûÂº∑Áü•Ë≠òÁÆ°ÁêÜÂíåÊñá‰ª∂ÁµÑÁπîÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Semantic Communication Enhanced by Knowledge Graph Representation Learning**
2407.19338v1 by Nour Hello, Paolo Di Lorenzo, Emilio Calvanese Strinati

This paper investigates the advantages of representing and processing
semantic knowledge extracted into graphs within the emerging paradigm of
semantic communications. The proposed approach leverages semantic and pragmatic
aspects, incorporating recent advances on large language models (LLMs) to
achieve compact representations of knowledge to be processed and exchanged
between intelligent agents. This is accomplished by using the cascade of LLMs
and graph neural networks (GNNs) as semantic encoders, where information to be
shared is selected to be meaningful at the receiver. The embedding vectors
produced by the proposed semantic encoder represent information in the form of
triplets: nodes (semantic concepts entities), edges(relations between
concepts), nodes. Thus, semantic information is associated with the
representation of relationships among elements in the space of semantic concept
abstractions. In this paper, we investigate the potential of achieving high
compression rates in communication by incorporating relations that link
elements within graph embeddings. We propose sending semantic symbols solely
equivalent to node embeddings through the wireless channel and inferring the
complete knowledge graph at the receiver. Numerical simulations illustrate the
effectiveness of leveraging knowledge graphs to semantically compress and
transmit information.

ÊëòË¶ÅÔºöÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®ËØ≠‰πâÈÄö‰ø°ÁöÑÊñ∞ÂÖ¥ËåÉ‰æã‰∏≠Â∞ÜÊèêÂèñÂà∞Âõæ‰∏≠ÁöÑËØ≠‰πâÁü•ËØÜË°®Á§∫ÂíåÂ§ÑÁêÜÁöÑ‰ºòÂäø„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂà©Áî®ËØ≠‰πâÂíåËØ≠Áî®ÊñπÈù¢ÔºåÁªìÂêà‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ËøõÂ±ïÔºå‰ª•ÂÆûÁé∞Ë¶ÅÂ§ÑÁêÜÂíåÂú®Êô∫ËÉΩ‰ª£ÁêÜ‰πãÈó¥‰∫§Êç¢ÁöÑÁü•ËØÜÁöÑÁ¥ßÂáëË°®Á§∫„ÄÇËøôÊòØÈÄöËøá‰ΩøÁî® LLM ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ÁöÑÁ∫ßËÅî‰Ωú‰∏∫ËØ≠‰πâÁºñÁ†ÅÂô®Êù•ÂÆåÊàêÁöÑÔºåÂÖ∂‰∏≠Ë¶ÅÂÖ±‰∫´ÁöÑ‰ø°ÊÅØË¢´ÈÄâÊã©‰∏∫ÂØπÊé•Êî∂ËÄÖÊúâÊÑè‰πâ„ÄÇÁî±ÊâÄÊèêÂá∫ÁöÑËØ≠‰πâÁºñÁ†ÅÂô®‰∫ßÁîüÁöÑÂµåÂÖ•ÂêëÈáè‰ª•‰∏âÂÖÉÁªÑÁöÑÂΩ¢ÂºèË°®Á§∫‰ø°ÊÅØÔºöËäÇÁÇπÔºàËØ≠‰πâÊ¶ÇÂøµÂÆû‰ΩìÔºâ„ÄÅËæπÔºàÊ¶ÇÂøµ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºâ„ÄÅËäÇÁÇπ„ÄÇÂõ†Ê≠§ÔºåËØ≠‰πâ‰ø°ÊÅØ‰∏éËØ≠‰πâÊ¶ÇÂøµÊäΩË±°Á©∫Èó¥‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÂÖ≥Á≥ªÁöÑË°®Á§∫Áõ∏ÂÖ≥ËÅî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÈÄöËøáÂêàÂπ∂Â∞ÜÂõæÂµåÂÖ•‰∏≠ÁöÑÂÖÉÁ¥†ËÅîÁ≥ªËµ∑Êù•ÁöÑÂÖ≥ËÅîÊù•ÂÆûÁé∞È´òÂéãÁº©ÁéáÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Âª∫ËÆÆ‰ªÖÈÄöËøáÊó†Á∫ø‰ø°ÈÅìÂèëÈÄÅËØ≠‰πâÁ¨¶Âè∑ÔºåËøô‰∫õÁ¨¶Âè∑ÂÆåÂÖ®Á≠âÊïà‰∫éËäÇÁÇπÂµåÂÖ•ÔºåÂπ∂Âú®Êé•Êî∂Âô®Â§ÑÊé®Êñ≠Âá∫ÂÆåÊï¥ÁöÑÁü•ËØÜÂõæ„ÄÇÊï∞ÂÄºÊ®°ÊãüËØ¥Êòé‰∫ÜÂà©Áî®Áü•ËØÜÂõæËØ≠‰πâÂéãÁº©Âíå‰º†Ëæì‰ø°ÊÅØÁöÑÊúâÊïàÊÄß„ÄÇ

##### **GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**
2407.19039v1 by Yuchen Shen, Barnab√°s P√≥czos

With the increasing attention to molecular machine learning, various
innovations have been made in designing better models or proposing more
comprehensive benchmarks. However, less is studied on the data preprocessing
schedule for molecular graphs, where a different view of the molecular graph
could potentially boost the model's performance. Inspired by the Byte-Pair
Encoding (BPE) algorithm, a subword tokenization method popularly adopted in
Natural Language Processing, we propose GraphBPE, which tokenizes a molecular
graph into different substructures and acts as a preprocessing schedule
independent of the model architectures. Our experiments on 3 graph-level
classification and 3 graph-level regression datasets show that data
preprocessing could boost the performance of models for molecular graphs, and
GraphBPE is effective for small classification datasets and it performs on par
with other tokenization methods across different model architectures.

ÊëòË¶ÅÔºöÈö®ËëóÂàÜÂ≠êÊ©üÂô®Â≠∏ÁøíÂèóÂà∞ÁöÑÈóúÊ≥®Â∫¶Ë∂ä‰æÜË∂äÈ´òÔºåÂú®Ë®≠Ë®àÊõ¥Â•ΩÁöÑÊ®°ÂûãÊàñÊèêÂá∫Êõ¥ÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÊñπÈù¢Â∑≤Á∂ìÊúâ‰∫ÜÂêÑÁ®ÆÂâµÊñ∞„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂàÜÂ≠êÂúñÁöÑÊï∏ÊìöÈ†êËôïÁêÜË®àÁï´Á†îÁ©∂ËºÉÂ∞ëÔºåÂú®Ë©≤Ë®àÁï´‰∏≠ÔºåÂàÜÂ≠êÂúñÁöÑ‰∏çÂêåË¶ñÂúñÂèØËÉΩÊúÉÊèêÂçáÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÂèóÂà∞Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠Âª£Ê≥õÊé°Áî®ÁöÑÂ≠êË©ûÂΩôÊ®ôË®òÂåñÊñπÊ≥ï Byte-Pair Á∑®Á¢º (BPE) ÊºîÁÆóÊ≥ïÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphBPEÔºåÂÆÉÂ∞áÂàÜÂ≠êÂúñÊ®ôË®òÂåñÁÇ∫‰∏çÂêåÁöÑÂ≠êÁµêÊßãÔºå‰∏¶‰ΩúÁÇ∫ËàáÊ®°ÂûãÊû∂ÊßãÁÑ°ÈóúÁöÑÈ†êËôïÁêÜË®àÁï´„ÄÇÊàëÂÄëÂú® 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÂíå 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂõûÊ≠∏Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåË≥áÊñôÈ†êËôïÁêÜÂèØ‰ª•ÊèêÂçáÂàÜÂ≠êÂúñÊ®°ÂûãÁöÑÊïàËÉΩÔºåËÄå GraphBPE Â∞çÊñºÂ∞èÂûãÂàÜÈ°ûË≥áÊñôÈõÜÊúâÊïàÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊ®°ÂûãÊû∂Êßã‰∏≠ËàáÂÖ∂‰ªñÊ®ôË®òÂåñÊñπÊ≥ïË°®ÁèæÁõ∏Áï∂„ÄÇ

##### **Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**
2407.18752v3 by Yuni Susanti, Michael F√§rber

Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.

ÊëòË¶ÅÔºöÂõ†ÊûúÁôºÁèæÊó®Âú®Ê†πÊìöËßÄÊ∏¨Êï∏Êìö‰º∞Ë®àËÆäÊï∏‰πãÈñìÁöÑÂõ†ÊûúÁµêÊßã„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑËßÄÈªû‰æÜËß£Ê±∫Âõ†ÊûúÁôºÁèæÂïèÈ°åÔºåÊñπÊ≥ïÊòØÊé®Ë´ñËàáËÆäÊï∏Áõ∏ÈóúÁöÑÂÖÉÊï∏ÊìöÔºåËÄå‰∏çÊòØÂÆÉÂÄëÁöÑÂØ¶ÈöõÊï∏ÊìöÂÄºÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫Âü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂ∞èË™ûË®ÄÊ®°Âûã (SLMÔºåÂÆöÁæ©ÁÇ∫ÂèÉÊï∏Â∞ëÊñº 10 ÂÑÑÁöÑ LLM) ÁöÑËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÈÄ≤Ë°åÂü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü KG Structure as PromptÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞á‰æÜËá™Áü•Ë≠òÂúñË≠úÁöÑÁµêÊßãË≥áË®äÔºå‰æãÂ¶ÇÂÖ±ÂêåÈÑ∞Â±ÖÁØÄÈªûÂíåÂÖÉË∑ØÂæëÔºåÊï¥ÂêàÂà∞Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏Áøí‰∏≠Ôºå‰ª•Â¢ûÂº∑ SLM ÁöÑËÉΩÂäõ„ÄÇÂú®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆö‰∏ãÔºåÈáùÂ∞ç‰∏âÁ®ÆÈ°ûÂûãÁöÑÁîüÁâ©ÈÜ´Â≠∏ÂíåÈñãÊîæÈ†òÂüüË≥áÊñôÈõÜÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË∂ÖË∂ä‰∫ÜÂ§ßÂ§öÊï∏Âü∫Ê∫ñÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÂú®ÂÆåÊï¥Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂÇ≥Áµ±ÂæÆË™øÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈÄ≤‰∏ÄÊ≠•Á™ÅÂá∫‰∫Ü SLM ÁöÑÂº∑Â§ßÂäüËÉΩÔºöÁµêÂêàÁü•Ë≠òÂúñË≠úÂíåÂü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÔºåSLM Â±ïÁ§∫‰∫ÜË∂ÖË∂äÂÖ∑ÊúâÊõ¥Â§öÂèÉÊï∏ÁöÑ LLM ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® GitHub ‰∏äÂèñÂæó„ÄÇ

##### **Using GPT-4 to guide causal machine learning**
2407.18607v1 by Anthony C. Constantinou, Neville K. Kitson, Alessio Zanga

Since its introduction to the public, ChatGPT has had an unprecedented
impact. While some experts praised AI advancements and highlighted their
potential risks, others have been critical about the accuracy and usefulness of
Large Language Models (LLMs). In this paper, we are interested in the ability
of LLMs to identify causal relationships. We focus on the well-established
GPT-4 (Turbo) and evaluate its performance under the most restrictive
conditions, by isolating its ability to infer causal relationships based solely
on the variable labels without being given any context, demonstrating the
minimum level of effectiveness one can expect when it is provided with
label-only information. We show that questionnaire participants judge the GPT-4
graphs as the most accurate in the evaluated categories, closely followed by
knowledge graphs constructed by domain experts, with causal Machine Learning
(ML) far behind. We use these results to highlight the important limitation of
causal ML, which often produces causal graphs that violate common sense,
affecting trust in them. However, we show that pairing GPT-4 with causal ML
overcomes this limitation, resulting in graphical structures learnt from real
data that align more closely with those identified by domain experts, compared
to structures learnt by causal ML alone. Overall, our findings suggest that
despite GPT-4 not being explicitly designed to reason causally, it can still be
a valuable tool for causal representation, as it improves the causal discovery
process of causal ML algorithms that are designed to do just that.

ÊëòË¶ÅÔºöËá™ ChatGPT ÂêëÂÖ¨‰ºóÂèëÂ∏É‰ª•Êù•ÔºåÂÆÉ‰∫ßÁîü‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÂΩ±Âìç„ÄÇËôΩÁÑ∂‰∏Ä‰∫õ‰∏ìÂÆ∂ËµûÊâ¨‰∫Ü AI ÁöÑËøõÊ≠•Âπ∂Âº∫Ë∞É‰∫ÜÂÖ∂ÊΩúÂú®È£éÈô©Ôºå‰ΩÜÂÖ∂‰ªñ‰∫∫‰∏ÄÁõ¥ÊâπËØÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÁî®ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂØπ LLM ËØÜÂà´Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÊÑüÂÖ¥Ë∂£„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊàêÁÜüÁöÑ GPT-4ÔºàTurboÔºâÔºåÂπ∂Âú®ÊúÄ‰∏•Ê†ºÁöÑÊù°‰ª∂‰∏ãËØÑ‰º∞ÂÖ∂ÊÄßËÉΩÔºåÈÄöËøáÂ≠§Á´ãÂÖ∂‰ªÖÊ†πÊçÆÂèòÈáèÊ†áÁ≠æÊé®Êñ≠Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÔºåËÄå‰∏çÊèê‰æõ‰ªª‰Ωï‰∏ä‰∏ãÊñáÔºåÂ±ïÁ§∫‰∫ÜÂΩì‰ªÖÊèê‰æõÊ†áÁ≠æ‰ø°ÊÅØÊó∂‰∫∫‰ª¨ÂèØ‰ª•È¢ÑÊúüÁöÑÊúÄ‰ΩéÊúâÊïàÊÄßÊ∞¥Âπ≥„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÈóÆÂç∑ÂèÇ‰∏éËÄÖËÆ§‰∏∫ GPT-4 ÂõæÂΩ¢Âú®ËØÑ‰º∞Á±ªÂà´‰∏≠ÊòØÊúÄÂáÜÁ°ÆÁöÑÔºåÁ¥ßÈöèÂÖ∂ÂêéÁöÑÊòØÁî±È¢ÜÂüü‰∏ìÂÆ∂ÊûÑÂª∫ÁöÑÁü•ËØÜÂõæË∞±ÔºåÂõ†ÊûúÊú∫Âô®Â≠¶‰π† (ML) ËøúËøúËêΩÂêé„ÄÇÊàë‰ª¨‰ΩøÁî®Ëøô‰∫õÁªìÊûúÊù•Âº∫Ë∞ÉÂõ†Êûú ML ÁöÑÈáçË¶ÅÂ±ÄÈôêÊÄßÔºåÂÆÉÁªèÂ∏∏‰∫ßÁîüËøùËÉåÂ∏∏ËØÜÁöÑÂõ†ÊûúÂõæÔºåÂΩ±Âìç‰∫∫‰ª¨ÂØπÂÆÉ‰ª¨ÁöÑ‰ø°‰ªª„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨Ë°®ÊòéÂ∞Ü GPT-4 ‰∏éÂõ†Êûú ML ÈÖçÂØπÂèØ‰ª•ÂÖãÊúçËøô‰∏ÄÈôêÂà∂Ôºå‰ªéËÄå‰∫ßÁîü‰ªéÁúüÂÆûÊï∞ÊçÆ‰∏≠Â≠¶Âà∞ÁöÑÂõæÂΩ¢ÁªìÊûÑÔºå‰∏éÈ¢ÜÂüü‰∏ìÂÆ∂ËØÜÂà´ÁöÑÁªìÊûÑÁõ∏ÊØîÔºåÊõ¥Á¥ßÂØÜÂú∞‰∏é‰πãÂØπÈΩêÔºåËÄå‰∏çÊòØ‰ªÖÁî±Âõ†Êûú ML Â≠¶Âà∞ÁöÑÁªìÊûÑ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ° GPT-4 Âπ∂Êú™ÊòéÁ°ÆËÆæËÆ°‰∏∫Âõ†ÊûúÊé®ÁêÜÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÂèØ‰ª•Êàê‰∏∫Âõ†ÊûúË°®Á§∫ÁöÑÂÆùË¥µÂ∑•ÂÖ∑ÔºåÂõ†‰∏∫ÂÆÉÊîπËøõ‰∫ÜÊó®Âú®ÊâßË°åÊ≠§Êìç‰ΩúÁöÑÂõ†Êûú ML ÁÆóÊ≥ïÁöÑÂõ†ÊûúÂèëÁé∞ËøáÁ®ã„ÄÇ

##### **Multi-turn Response Selection with Commonsense-enhanced Language Models**
2407.18479v1 by Yuandong Wang, Xuhui Ren, Tong Chen, Yuxiao Dong, Nguyen Quoc Viet Hung, Jie Tang

As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫È´òÁ¥ö‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÂÄãÂàÜÊîØÔºåÂ∞çË©±Á≥ªÁµ±Ê≠£Ëì¨ÂãÉÁôºÂ±ï„ÄÇÂ§öËº™ÂõûÊáâÁî®Êà∂ÂõûÊáâÈÅ∏ÊìáÊòØÂ∞çË©±Á≥ªÁµ±‰∏≠‰∏ÄÂÄãÈÄöÁî®ÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÂú®ËÉåÊôØË≥áË®äÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÁöÑÂçîÂä©‰∏ãÔºåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÂú®Ê≠§ÂïèÈ°å‰∏äÁöÑË°®ÁèæÁç≤Ëá¥‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÂøΩÁï•‰∫ÜÂ§ñÈÉ®Â∏∏Ë≠òÁü•Ë≠òÁöÑÈáçË¶ÅÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊöπÁæÖÁ∂≤Ë∑ØÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãËàá‰∏ÄÂÄãÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàSinLGÔºâÂêà‰Ωµ„ÄÇSinLG Âà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºàPLMÔºâ‰æÜÊçïÊçâË™ûÂ¢ÉÂíåÂõûÊáâÂÄôÈÅ∏‰∏≠ÁöÑË©ûÂΩôÈóúËÅØÔºå‰∏¶Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠úÊé®ÁêÜÊúâÁî®ÁöÑÂ∏∏Ë≠ò„ÄÇGNN Êó®Âú®ÂçîÂä© PLM ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶ÂñöÈÜíÂÖ∂Áõ∏ÈóúË®òÊÜ∂‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑË°®Áèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠ú‰∏≠ÊèêÂèñÁõ∏ÈóúÊ¶ÇÂøµ‰ΩúÁÇ∫ÁØÄÈªûÔºå‰ª•ÊßãÂª∫‰∏ÄÂÄãÂ≠êÂúñÔºåÂÖ∂‰∏≠Ë™ûÂ¢ÉÂõûÊáâÂ∞ç‰ΩúÁÇ∫ÊØèÂÄãÁØÑ‰æãÁöÑË∂ÖÁ¥öÁØÄÈªû„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈÄèÈÅé PLM Âíå GNN ÁÇ∫Ë™ûÂ¢ÉÂõûÊáâÂ∞çÂ≠∏ÁøíÂÖ©ÂÄãË°®Á§∫„ÄÇÂÖ©ÂÄãË°®Á§∫‰πãÈñìÁöÑÁõ∏‰ººÊÄßÊêçÂ§±Áî®ÊñºÂ∞áÂ∏∏Ë≠òÁü•Ë≠òÂæû GNN ËΩâÁßªÂà∞ PLM„ÄÇÁÑ∂ÂæåÂÉÖ‰ΩøÁî® PLM ‰æÜÈÄ≤Ë°åÁ∑ö‰∏äÊé®Ë´ñÔºå‰ª•‰æø‰øùË≠âÊïàÁéá„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞ç PERSONA-CHAT Ë≥áÊñôÈõÜÁöÑÂÖ©ÂÄãËÆäÈ´îÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÈÄôË≠âÊòéÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°à‰∏çÂÉÖÂèØ‰ª•ÊèêÈ´ò PLM ÁöÑÊïàËÉΩÔºåÈÇÑËÉΩÂØ¶ÁèæÈ´òÊïàÁöÑÊé®Ë´ñ„ÄÇ

##### **Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**
2407.18181v1 by Sindhura Kommu, Yizhi Wang, Yue Wang, Xuan Wang

Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing
(scRNA-seq) data is a complex challenge that requires capturing the intricate
relationships between genes and their regulatory interactions. In this study,
we tackle this challenge by leveraging the single-cell BERT-based pre-trained
transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to
augment structured biological knowledge from existing GRNs. We introduce a
novel joint graph learning approach that combines the rich contextual
representations learned by pre-trained single-cell language models with the
structured knowledge encoded in GRNs using graph neural networks (GNNs). By
integrating these two modalities, our approach effectively reasons over boththe
gene expression level constraints provided by the scRNA-seq data and the
structured biological knowledge inherent in GRNs. We evaluate our method on
human cell benchmark datasets from the BEELINE study with cell type-specific
ground truth networks. The results demonstrate superior performance over
current state-of-the-art baselines, offering a deeper understanding of cellular
regulatory mechanisms.

ÊëòË¶ÅÔºöÂæûÂñÆÁ¥∞ËÉû RNA ÂÆöÂ∫è (scRNA-seq) Ë≥áÊñôÊé®Ë´ñÂü∫Âõ†Ë™øÊéßÁ∂≤Ë∑Ø (GRN) ÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÊéåÊè°Âü∫Âõ†ËàáÂÖ∂Ë™øÊéß‰∫§‰∫í‰ΩúÁî®‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂà©Áî®Âú®Âª£Ê≥õÁöÑÊú™Ê®ôË®ò scRNA-seq Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂñÆÁ¥∞ËÉû BERT Âü∫ÊñºÈ†êË®ìÁ∑¥ËΩâÊèõÂô®Ê®°Âûã (scBERT)Ôºå‰æÜÂÖãÊúçÊ≠§ÊåëÊà∞Ôºå‰ª•Êì¥ÂÖÖÁèæÊúâ GRN ‰∏≠ÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠ò„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËÅØÂêàÂúñÂΩ¢Â≠∏ÁøíÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÈ†êË®ìÁ∑¥ÂñÆÁ¥∞ËÉûË™ûË®ÄÊ®°ÂûãÊâÄÂ≠∏ÁøíÂà∞ÁöÑË±êÂØåËÑàÁµ°Ë°®ÂæµÔºå‰ª•Âèä‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞ç GRN ‰∏≠Á∑®Á¢ºÁöÑÁµêÊßãÂåñÁü•Ë≠ò„ÄÇÈÄèÈÅéÊï¥ÂêàÈÄôÂÖ©Á®ÆÊñπÂºèÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊúâÊïàÂú∞Â∞ç scRNA-seq Ë≥áÊñôÊèê‰æõÁöÑÂü∫Âõ†Ë°®ÁèæÂ±§Á¥öÁ¥ÑÊùüÂíå GRN ‰∏≠Âõ∫ÊúâÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÊàëÂÄë‰ΩøÁî® BEELINE Á†îÁ©∂‰∏≠ÁöÑ‰∫∫È°ûÁ¥∞ËÉûÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰ª•ÂèäÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöÁöÑÂü∫Êú¨‰∫ãÂØ¶Á∂≤Ë∑ØÔºå‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊñπÊ≥ï„ÄÇÁµêÊûúË≠âÊòéÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÔºåÊèê‰æõ‰∫ÜÂ∞çÁ¥∞ËÉûË™øÊéßÊ©üÂà∂ÁöÑÊõ¥Ê∑±ÂÖ•ÁêÜËß£„ÄÇ

##### **MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**
2407.17544v1 by Arya Bulusu, Brandon Man, Ashish Jagmohan, Aditya Vempaty, Jennifer Mari-Wyka, Deepak Akkil

There has been significant recent interest in harnessing LLMs to control
software systems through multi-step reasoning, planning and tool-usage. While
some promising results have been obtained, application to specific domains
raises several general issues including the control of specialized domain
tools, the lack of existing datasets for training and evaluation, and the
non-triviality of automated system evaluation and improvement. In this paper,
we present a case-study where we examine these issues in the context of a
specific domain. Specifically, we present an automated math visualizer and
solver system for mathematical pedagogy. The system orchestrates mathematical
solvers and math graphing tools to produce accurate visualizations from simple
natural language commands. We describe the creation of specialized data-sets,
and also develop an auto-evaluator to easily evaluate the outputs of our system
by comparing them to ground-truth expressions. We have open sourced the
data-sets and code for the proposed system.

ÊëòË¶ÅÔºöÊúÄËøëÔºå‰∫∫‰ª¨ÂØπÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Êù•ÈÄöËøáÂ§öÊ≠•È™§Êé®ÁêÜ„ÄÅËßÑÂàíÂíåÂ∑•ÂÖ∑‰ΩøÁî®Êù•ÊéßÂà∂ËΩØ‰ª∂Á≥ªÁªü‰∫ßÁîü‰∫ÜÊûÅÂ§ßÁöÑÂÖ¥Ë∂£„ÄÇËôΩÁÑ∂Â∑≤ÁªèÂèñÂæó‰∫Ü‰∏Ä‰∫õÊúâÂ∏åÊúõÁöÑÁªìÊûúÔºå‰ΩÜÂ∫îÁî®‰∫éÁâπÂÆöÈ¢ÜÂüü‰ºöÂºïÂèëÂá†‰∏™ÊôÆÈÅçÊÄßÈóÆÈ¢òÔºåÂåÖÊã¨ÂØπ‰∏ì‰∏öÈ¢ÜÂüüÂ∑•ÂÖ∑ÁöÑÊéßÂà∂„ÄÅÁº∫‰πèÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞ÁöÑÁé∞ÊúâÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäËá™Âä®ÂåñÁ≥ªÁªüËØÑ‰º∞ÂíåÊîπËøõÁöÑÈùûÂπ≥Âá°ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê°à‰æãÁ†îÁ©∂ÔºåÂÖ∂‰∏≠Êàë‰ª¨Á†îÁ©∂‰∫ÜÁâπÂÆöÈ¢ÜÂüüËÉåÊôØ‰∏ãÁöÑËøô‰∫õÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™Áî®‰∫éÊï∞Â≠¶ÊïôËÇ≤ÁöÑËá™Âä®ÂåñÊï∞Â≠¶ÂèØËßÜÂåñÂô®ÂíåÊ±ÇËß£Âô®Á≥ªÁªü„ÄÇËØ•Á≥ªÁªüÂçèË∞ÉÊï∞Â≠¶Ê±ÇËß£Âô®ÂíåÊï∞Â≠¶ÁªòÂõæÂ∑•ÂÖ∑Ôºå‰ª•Ê†πÊçÆÁÆÄÂçïÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂëΩ‰ª§ÁîüÊàêÂáÜÁ°ÆÁöÑÂèØËßÜÂåñÊïàÊûú„ÄÇÊàë‰ª¨ÊèèËø∞‰∫Ü‰∏ìÈó®Êï∞ÊçÆÈõÜÁöÑÂàõÂª∫ÔºåËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ËØÑ‰º∞Âô®ÔºåÈÄöËøáÂ∞ÜÊàë‰ª¨ÁöÑÁ≥ªÁªüËæìÂá∫‰∏éÁúüÂÆûË°®ËææÂºèËøõË°åÊØîËæÉÔºåËΩªÊùæËØÑ‰º∞ÂÖ∂ËæìÂá∫„ÄÇÊàë‰ª¨Â∑≤ÁªèÂºÄÊ∫ê‰∫ÜÊâÄÊèêËÆÆÁ≥ªÁªüÁöÑ‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜ„ÄÇ

##### **Ranking protein-protein models with large language models and graph neural networks**
2407.16375v1 by Xiaotong Xu, Alexandre M. J. J. Bonvin

Protein-protein interactions (PPIs) are associated with various diseases,
including cancer, infections, and neurodegenerative disorders. Obtaining
three-dimensional structural information on these PPIs serves as a foundation
to interfere with those or to guide drug design. Various strategies can be
followed to model those complexes, all typically resulting in a large number of
models. A challenging step in this process is the identification of good models
(near-native PPI conformations) from the large pool of generated models. To
address this challenge, we previously developed DeepRank-GNN-esm, a graph-based
deep learning algorithm for ranking modelled PPI structures harnessing the
power of protein language models. Here, we detail the use of our software with
examples. DeepRank-GNN-esm is freely available at
https://github.com/haddocking/DeepRank-GNN-esm

ÊëòË¶ÅÔºöËõãÁôΩ-ËõãÁôΩ‰∫§‰∫í‰ΩúÁî® (PPI) ËàáÂêÑÁ®ÆÁñæÁóÖÁõ∏ÈóúÔºåÂåÖÊã¨ÁôåÁóá„ÄÅÊÑüÊüìÂíåÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖ„ÄÇÂèñÂæóÈÄô‰∫õ PPI ÁöÑ‰∏âÁ∂≠ÁµêÊßãË≥áË®äÔºå‰ΩúÁÇ∫Âπ≤ÊìæÂÆÉÂÄëÊàñÂºïÂ∞éËó•Áâ©Ë®≠Ë®àÁöÑÂü∫Á§é„ÄÇÂèØ‰ª•ÈÅµÂæ™ÂêÑÁ®ÆÁ≠ñÁï•‰æÜÂª∫Ê®°ÈÄô‰∫õË§áÂêàÈ´îÔºåÊâÄÊúâÈÄô‰∫õÁ≠ñÁï•ÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑÊ®°Âûã„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑÊåëÊà∞ÊÄßÊ≠•È©üÔºåÊòØÂæûÂ§ßÈáèÁî¢ÁîüÁöÑÊ®°Âûã‰∏≠ÊâæÂá∫Â•ΩÁöÑÊ®°ÂûãÔºàÊé•ËøëÂéüÁîü PPI ÊßãË±°Ôºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄë‰πãÂâçÈñãÁôº‰∫Ü DeepRank-GNN-esmÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÔºåÁî®ÊñºÂ∞çÂª∫Ê®°ÁöÑ PPI ÁµêÊßãÈÄ≤Ë°åÊéíÂêçÔºåÂà©Áî®ËõãÁôΩË≥™Ë™ûË®ÄÊ®°ÂûãÁöÑÂäõÈáè„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫ÜÊàëÂÄëËªüÈ´îÁöÑ‰ΩøÁî®ÁØÑ‰æã„ÄÇDeepRank-GNN-esm ÂèØÂú® https://github.com/haddocking/DeepRank-GNN-esm ÂÖçË≤ªÂèñÂæó

##### **PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**
2407.16329v1 by Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo

Acute stroke demands prompt diagnosis and treatment to achieve optimal
patient outcomes. However, the intricate and irregular nature of clinical data
associated with acute stroke, particularly blood pressure (BP) measurements,
presents substantial obstacles to effective visual analytics and
decision-making. Through a year-long collaboration with experienced
neurologists, we developed PhenoFlow, a visual analytics system that leverages
the collaboration between human and Large Language Models (LLMs) to analyze the
extensive and complex data of acute ischemic stroke patients. PhenoFlow
pioneers an innovative workflow, where the LLM serves as a data wrangler while
neurologists explore and supervise the output using visualizations and natural
language interactions. This approach enables neurologists to focus more on
decision-making with reduced cognitive load. To protect sensitive patient
information, PhenoFlow only utilizes metadata to make inferences and synthesize
executable codes, without accessing raw patient data. This ensures that the
results are both reproducible and interpretable while maintaining patient
privacy. The system incorporates a slice-and-wrap design that employs temporal
folding to create an overlaid circular visualization. Combined with a linear
bar graph, this design aids in exploring meaningful patterns within irregularly
measured BP data. Through case studies, PhenoFlow has demonstrated its
capability to support iterative analysis of extensive clinical datasets,
reducing cognitive load and enabling neurologists to make well-informed
decisions. Grounded in long-term collaboration with domain experts, our
research demonstrates the potential of utilizing LLMs to tackle current
challenges in data-driven clinical decision-making for acute ischemic stroke
patients.

ÊëòË¶ÅÔºö<paragraph>ÊÄ•ÊÄß‰∏≠È¢®ÈúÄË¶ÅËøÖÈÄüË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºåÊâçËÉΩÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑÁóÖ‰∫∫Ê≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåËàáÊÄ•ÊÄß‰∏≠È¢®Áõ∏ÈóúÁöÑËá®Â∫äË≥áÊñôË§áÈõú‰∏î‰∏çË¶èÂâáÔºåÁâπÂà•ÊòØË°ÄÂ£ì (BP) Ê∏¨ÈáèÔºåÂ∞çÊúâÊïàÁöÑË¶ñË¶∫ÂàÜÊûêÂíåÊ±∫Á≠ñÂà∂ÂÆöÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÈÄèÈÅéËàáÁ∂ìÈ©óË±êÂØåÁöÑÁ•ûÁ∂ìÁßëÈÜ´Â∏´Èï∑ÈÅî‰∏ÄÂπ¥ÁöÑÂêà‰ΩúÔºåÊàëÂÄëÈñãÁôº‰∫Ü PhenoFlowÔºåÈÄôÊòØ‰∏ÄÂÄãË¶ñË¶∫ÂàÜÊûêÁ≥ªÁµ±ÔºåÂà©Áî®‰∫∫ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πãÈñìÁöÑÂçî‰Ωú‰æÜÂàÜÊûêÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖÁöÑÂª£Ê≥õ‰∏îË§áÈõúË≥áÊñô„ÄÇPhenoFlow ÈñãÂâµ‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÖ∂‰∏≠ LLM Êìî‰ªªË≥áÊñôÊï¥ÁêÜÂì°ÔºåËÄåÁ•ûÁ∂ìÁßëÈÜ´Â∏´Ââá‰ΩøÁî®Ë¶ñË¶∫ÂåñÂíåËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãï‰æÜÊé¢Á¥¢ÂíåÁõ£Áù£Ëº∏Âá∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†Êõ¥Â∞àÊ≥®ÊñºÊ±∫Á≠ñÂà∂ÂÆöÔºåÂêåÊôÇÈôç‰ΩéË™çÁü•Ë≤†Êìî„ÄÇÁÇ∫‰∫Ü‰øùË≠∑ÊïèÊÑüÁöÑÁóÖ‰∫∫Ë≥áË®äÔºåPhenoFlow ÂÉÖÂà©Áî®ÂÖÉË≥áÊñôÈÄ≤Ë°åÊé®Ë´ñ‰∏¶ÂêàÊàêÂèØÂü∑Ë°åÁ®ãÂºèÁ¢ºÔºåËÄå‰∏çÊúÉÂ≠òÂèñÂéüÂßãÁóÖ‰∫∫Ë≥áÊñô„ÄÇÈÄôÁ¢∫‰øù‰∫ÜÁµêÊûúÊó¢ÂèØÈáçÁèæÂèàÂèØËß£ÈáãÔºåÂêåÊôÇÁ∂≠Ë≠∑ÁóÖ‰∫∫ÁöÑÈö±ÁßÅ„ÄÇË©≤Á≥ªÁµ±Êé°Áî®ÂàÜÊÆµÂíåÂåÖË£ùË®≠Ë®àÔºåÊé°Áî®ÊôÇÈñìÊë∫Áñä‰æÜÂª∫Á´ãÁñäÂä†ÁöÑÂúìÂΩ¢Ë¶ñË¶∫Âåñ„ÄÇÁµêÂêàÁ∑öÊÄßÈï∑Ê¢ùÂúñÔºåÊ≠§Ë®≠Ë®àÊúâÂä©ÊñºÊé¢Á¥¢‰∏çË¶èÂâáÊ∏¨ÈáèË°ÄÂ£ìË≥áÊñô‰∏≠ÁöÑÊúâÊÑèÁæ©Ê®°Âºè„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåPhenoFlow Â∑≤Ë≠âÊòéÂÖ∂ÊîØÊè¥Â∞çÂª£Ê≥õËá®Â∫äË≥áÊñôÈõÜÈÄ≤Ë°åÂèçË¶ÜÂàÜÊûêÁöÑËÉΩÂäõÔºåÈôç‰ΩéË™çÁü•Ë≤†Êìî‰∏¶‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ª•ËàáÈ†òÂüüÂ∞àÂÆ∂Èï∑ÊúüÂêà‰ΩúÁÇ∫Âü∫Á§éÔºåË≠âÊòé‰∫ÜÂà©Áî® LLM ‰æÜÊáâÂ∞çÁï∂ÂâçÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖË≥áÊñôÈ©ÖÂãïËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Graph-Structured Speculative Decoding**
2407.16207v1 by Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan

Speculative decoding has emerged as a promising technique to accelerate the
inference of Large Language Models (LLMs) by employing a small language model
to draft a hypothesis sequence, which is then validated by the LLM. The
effectiveness of this approach heavily relies on the balance between
performance and efficiency of the draft model. In our research, we focus on
enhancing the proportion of draft tokens that are accepted to the final output
by generating multiple hypotheses instead of just one. This allows the LLM more
options to choose from and select the longest sequence that meets its
standards. Our analysis reveals that hypotheses produced by the draft model
share many common token sequences, suggesting a potential for optimizing
computation. Leveraging this observation, we introduce an innovative approach
utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This
structure enables us to efficiently predict and merge recurring token
sequences, vastly reducing the computational demands of the draft model. We
term this approach Graph-structured Speculative Decoding (GSD). We apply GSD
across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and
observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly
surpassing standard speculative decoding.

ÊëòË¶ÅÔºö<paragraph>Êé®Ê∏¨ÊÄßËß£Á¢ºÂ∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊäÄË°ìÔºåÂèØÈÄöÈÅé‰ΩøÁî®Â∞èÂûãË™ûË®ÄÊ®°ÂûãËµ∑ËçâÂÅáË®≠Â∫èÂàóÔºåÁÑ∂ÂæåÁî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È©óË≠âË©≤Â∫èÂàóÔºåÂæûËÄåÂä†ÈÄüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜ„ÄÇÊ≠§ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºËçâÁ®øÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéá‰πãÈñìÁöÑÂπ≥Ë°°„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÈÄöÈÅéÁîüÊàêÂ§öÂÄãÂÅáË®≠ËÄå‰∏çÊòØÂè™ÁîüÊàê‰∏ÄÂÄãÂÅáË®≠‰æÜÊèêÈ´òË¢´Êé•ÂèóÁÇ∫ÊúÄÁµÇËº∏Âá∫ÁöÑËçâÁ®ø‰ª§ÁâåÁöÑÊØî‰æã„ÄÇÈÄôÂÖÅË®± LLM Âæû‰∏≠ÈÅ∏ÊìáÊõ¥Â§öÈÅ∏È†ÖÔºå‰∏¶ÈÅ∏ÊìáÁ¨¶ÂêàÂÖ∂Ê®ôÊ∫ñÁöÑÊúÄÈï∑Â∫èÂàó„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË°®ÊòéÔºåËçâÁ®øÊ®°ÂûãÁî¢ÁîüÁöÑÂÅáË®≠ÂÖ±‰∫´Ë®±Â§öÂÖ¨ÂÖ±‰ª§ÁâåÂ∫èÂàóÔºåÈÄôË°®ÊòéÂÑ™ÂåñË®àÁÆóÁöÑÂèØËÉΩÊÄß„ÄÇÂà©Áî®ÈÄô‰∏ÄËßÄÂØüÁµêÊûúÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ‰æÜÁÆ°ÁêÜÂ∑≤Á∑®Âà∂ÁöÑÂÅáË®≠„ÄÇÈÄôÁ®ÆÁµêÊßã‰ΩøÊàëÂÄëËÉΩÂ§†ÊúâÊïàÂú∞È†êÊ∏¨ÂíåÂêà‰ΩµÈáçË§áÁöÑ‰ª§ÁâåÂ∫èÂàóÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËçâÁ®øÊ®°ÂûãÁöÑË®àÁÆóÈúÄÊ±Ç„ÄÇÊàëÂÄëÂ∞áÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫ÂúñÁµêÊßãÊé®Ê∏¨ÊÄßËß£Á¢º (GSD)„ÄÇÊàëÂÄëÂ∞á GSD ÊáâÁî®Êñº‰∏ÄÁ≥ªÂàó LLMÔºåÂåÖÊã¨‰∏ÄÂÄã 700 ÂÑÑÂèÉÊï∏ÁöÑ LLaMA-2 Ê®°ÂûãÔºå‰∏¶ËßÄÂØüÂà∞È°ØËëóÁöÑÂä†ÈÄüÔºåÂæû 1.73 ÂÄçÂà∞ 1.96 ÂÄçÔºåÈ°ØËëóË∂ÖÈÅéÊ®ôÊ∫ñÊé®Ê∏¨ÊÄßËß£Á¢º„ÄÇ</paragraph>

##### **Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**
2407.21049v1 by Yannick Assogba, Donghao Ren

As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools

ÊëòË¶ÅÔºöÈö®ËëóË™ûË®ÄÊ®°ÂûãÊîØÊè¥ÁöÑÂÖßÂÆπÂ§ßÂ∞èË∂ä‰æÜË∂äÂ§ßÔºåË©ï‰º∞ÂÖ∂ÊúâÊïàÂà©Áî®Ë©≤ÂÖßÂÆπÁöÑËÉΩÂäõËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÂπæÂÄãÁ®ãÂºèÁ¢ºÁîüÊàêÊ®°ÂûãËôïÁêÜÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇÁöÑËÉΩÂäõÔºå‰ΩøÁî®‰∏ÄÁµÑÂ§öÊ≠•È©üÈóúÈçµÊ™¢Á¥¢‰ªªÂãôÔºåÂú®Èï∑ÈÅî 8k ‰ª§ÁâåÁöÑÂÖßÂÆπË¶ñÁ™ó‰∏≠„ÄÇ‰ªªÂãôÈÄêÊº∏Â¢ûÂä†Èõ£Â∫¶Ôºå‰∏¶ÂÖÅË®±Â∞çÊ®°ÂûãÂäüËÉΩÈÄ≤Ë°åÊØîÊµÅË°åÁöÑÈáùÈ†≠‰πæËçâÂ†ÜÊ∏¨Ë©¶Êõ¥Á¥∞Á∑ªÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæÔºåÁï∂ÂáΩÂºèÂèÉÁÖßÁ®çÂæåÂú®ÊèêÁ§∫‰∏≠ÂÆöÁæ©ÁöÑÂè¶‰∏ÄÂÄãÂáΩÂºèÊôÇÔºåÊïàËÉΩÊúÉÈ°ØËëó‰∏ãÈôçÔºàÊúÄÂ§ö 2 ÂÄçÔºâ„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞Ôºå‰ΩøÁî®ÊªëÂãïË¶ñÁ™óÊ≥®ÊÑèÊ©üÂà∂ÁöÑÊ®°ÂûãÈõ£‰ª•ËôïÁêÜË∂ÖÂá∫ÂñÆ‰∏ÄË¶ñÁ™óÂ§ßÂ∞èÁöÑÂèÉÁÖß„ÄÇÊàëÂÄë‰ΩøÁî®ÂëºÂè´ÂúñÂΩ¢Ë≥áË®äÂü∑Ë°åÁ∞°ÂñÆÁöÑÊèêÁ§∫‰øÆÊîπÔºå‰ª•Â∞áÂ§öÊ≠•È©üÊ™¢Á¥¢ÊïàËÉΩÊèêÂçáËá≥ 3 ÂÄç„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÁ™ÅÈ°Ø‰∫ÜÈï∑ÂÖßÂÆπÊïàËÉΩÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰∏¶ÊöóÁ§∫‰∫ÜÁ®ãÂºèÁ¢ºÂÆåÊàêÂ∑•ÂÖ∑ÁöÑÊèêÁ§∫Âª∫ÊßãÁ≠ñÁï•

##### **Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**
2407.16127v1 by Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu

Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.

ÊëòË¶ÅÔºöÂÇ≥Áµ±Áü•Ë≠òÂúñË≠úÔºàKGÔºâÂÆåÊàêÂäüËÉΩÊ®°ÂûãÂ≠∏ÁøíÂµåÂÖ•Ôºå‰ª•È†êÊ∏¨ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊúÄËøëÁöÑÂ∑•‰ΩúÂòóË©¶‰ª•Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª•ÊñáÂ≠óÁîüÊàêÁöÑÊñπÂºèÂÆåÊàê KG„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄëÈúÄË¶ÅÂ∞á LLM ÁöÑËº∏Âá∫Âü∫Á§éÂª∫Á´ãÂú® KG ÂØ¶È´î‰∏äÔºåÈÄô‰∏çÂèØÈÅøÂÖçÂú∞ÊúÉÂ∏∂‰æÜÈåØË™§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂæÆË™øÊ°ÜÊû∂ DIFTÔºåÊó®Âú®ÈáãÊîæ LLM ÁöÑ KG ÂÆåÊàêÂäüËÉΩÔºå‰∏¶ÈÅøÂÖçÂü∫Á§éÈåØË™§„ÄÇÁµ¶ÂÆö‰∏ÄÂÄã‰∏çÂÆåÊï¥ÁöÑ‰∫ãÂØ¶ÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãËºïÈáèÁ¥öÊ®°Âûã‰æÜÁç≤ÂæóÂÄôÈÅ∏ÂØ¶È´îÔºå‰∏¶ÂæÆË™ø‰∏ÄÂÄã LLMÔºå‰∏¶‰ΩøÁî®Ëæ®Âà•Êåá‰ª§ÂæûÁµ¶ÂÆöÁöÑÂÄôÈÅ∏È†Ö‰∏≠ÈÅ∏ÊìáÊ≠£Á¢∫ÁöÑÂØ¶È´î„ÄÇÁÇ∫‰∫ÜÂú®Ê∏õÂ∞ëÊåá‰ª§Êï∏ÊìöÁöÑÂêåÊôÇÊèêÂçáÊïàËÉΩÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãÊà™Êñ∑ÊäΩÊ®£ÊñπÊ≥ï‰æÜÈÅ∏ÊìáÊúâÁî®ÁöÑ‰∫ãÂØ¶‰ª•ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶Â∞á KG ÂµåÂÖ•Ê≥®ÂÖ•Âà∞ LLM ‰∏≠„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**
2407.15588v1 by Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee

Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge.Existing methods, mostly
supervised, face challenges in obtaining labeled entity pairs. To address this,
recent studies have shifted towards a self-supervised and unsupervised
frameworks. Despite their effectiveness, these approaches have limitations: (1)
they mainly focus on entity features, neglecting the semantic information of
relations, (2) they assume isomorphism between source and target graphs,
leading to noise and reduced alignment accuracy, and (3) they are susceptible
to noise in the textual features, especially when encountering inconsistent
translations or Out-Of-Vocabulary (OOV) problems.
  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual
EA framework that jointly performs Entity-level and Relation-level Alignment
using semantic textual features of relations and entities. Its refinement
process iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification
process examines the entities' neighbor triples as the linearized text. This
\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,
achieving near-perfect alignment even in the presence of noisy textual features
of entities. Our extensive experiments demonstrate that robustness and general
applicability of \proposed improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.

ÊëòË¶ÅÔºöË∑®Ë™ûË®ÄÂØ¶È´îÂ∞çÈΩä (EA) ËÉΩÂ§†Êï¥Âêà‰∏çÂêåË™ûË®Ä‰∏≠ÁöÑÂ§öÂÄãÁü•Ë≠òÂúñË≠ú (KG)ÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÁÑ°Á∏´Âú∞Â≠òÂèñÂ§öÂÖÉ‰∏îÂÖ®Èù¢ÁöÑÁü•Ë≠ò„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÊòØÊúâÁõ£Áù£ÁöÑÔºåÂú®ÂèñÂæóÊ®ôË®òÂØ¶È´îÂ∞çÊôÇÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤ËΩâÂêëËá™Áõ£Áù£ÂíåÁÑ°Áõ£Áù£ÁöÑÊû∂Êßã„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÊúâ‰ª•‰∏ãÈôêÂà∂Ôºö(1) ÂÆÉÂÄë‰∏ªË¶ÅÈóúÊ≥®ÂØ¶È´îÁâπÂæµÔºåÂøΩÁï•Èóú‰øÇÁöÑË™ûÁæ©Ë≥áË®äÔºå(2) ÂÆÉÂÄëÂÅáË®≠‰æÜÊ∫êÂúñË≠úÂíåÁõÆÊ®ôÂúñË≠ú‰πãÈñìÂêåÊßãÔºåÂ∞éËá¥ÈõúË®äÂíåÂ∞çÈΩäÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéÔºå(3) ÂÆÉÂÄëÂÆπÊòìÂèóÂà∞ÊñáÂ≠óÁâπÂæµ‰∏≠ÁöÑÈõúË®äÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂú®ÈÅáÂà∞‰∏ç‰∏ÄËá¥ÁöÑÁøªË≠ØÊàñË©ûÂΩôÂ§ñÂïèÈ°å (OOV) ÊôÇ„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ ERAlignÔºå‰∏ÄÂÄãÁÑ°Áõ£Áù£‰∏îÁ©©ÂÅ•ÁöÑË∑®Ë™ûË®Ä EA Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî®Èóú‰øÇÂíåÂØ¶È´îÁöÑË™ûÁæ©ÊñáÂ≠óÁâπÂæµÔºåÂêåÊôÇÂü∑Ë°åÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩä„ÄÇÂÆÉÁöÑÁ≤æÁÖâÁ®ãÂ∫èÈÄèÈÅéÊ†πÊìöÈÑ∞Êé•‰∏âÂÖÉÁµÑÂåπÈÖçËûçÂêàÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩäÔºåÂèçË¶ÜÂ¢ûÂº∑ÁµêÊûú„ÄÇÈ°çÂ§ñÁöÑÈ©óË≠âÁ®ãÂ∫èÂ∞áÂØ¶È´îÁöÑÈÑ∞Êé•‰∏âÂÖÉÁµÑË¶ñÁÇ∫Á∑öÊÄßÂåñÊñáÂ≠óÈÄ≤Ë°åÊ™¢Êü•„ÄÇÈÄôÂÄãÂö¥Ê†ºË©ï‰º∞Â∞çÈΩäÁµêÊûúÁöÑ„ÄåÂ∞çÈΩäÂíåÈ©óË≠â„ÄçÁÆ°Á∑öÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÂØ¶È´îÁöÑÈõúË®äÊñáÂ≠óÁâπÂæµÊôÇ‰πüËÉΩÈÅîÊàêËøë‰πéÂÆåÁæéÁöÑÂ∞çÈΩä„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºå\proposed ÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÈÅ©Áî®ÊÄßÊèêÂçá‰∫Ü EA ‰ªªÂãôÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊúâÊïàÊÄßÔºåÂ∞çÁü•Ë≠òÂ∞éÂêëÊáâÁî®Á®ãÂºèÊúâÈ°ØËëóÁöÑË≤¢Áçª„ÄÇ

##### **Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**
2407.15431v1 by Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang

The text-attributed graph (TAG) is one kind of important real-world
graph-structured data with each node associated with raw texts. For TAGs,
traditional few-shot node classification methods directly conduct training on
the pre-processed node features and do not consider the raw texts. The
performance is highly dependent on the choice of the feature pre-processing
method. In this paper, we propose P2TAG, a framework designed for few-shot node
classification on TAGs with graph pre-training and prompting. P2TAG first
pre-trains the language model (LM) and graph neural network (GNN) on TAGs with
self-supervised loss. To fully utilize the ability of language models, we adapt
the masked language modeling objective for our framework. The pre-trained model
is then used for the few-shot node classification with a mixed prompt method,
which simultaneously considers both text and graph information. We conduct
experiments on six real-world TAGs, including paper citation networks and
product co-purchasing networks. Experimental results demonstrate that our
proposed framework outperforms existing graph few-shot learning methods on
these datasets with +18.98% ~ +35.98% improvements.

ÊëòË¶ÅÔºöÊñáÊú¨Â±ûÊÄßÂõæ (TAG) ÊòØ‰∏ÄÁßçÈáçË¶ÅÁöÑÁúüÂÆû‰∏ñÁïåÂõæÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂÖ∂‰∏≠ÊØè‰∏™ËäÇÁÇπÈÉΩ‰∏éÂéüÂßãÊñáÊú¨Áõ∏ÂÖ≥ËÅî„ÄÇÂØπ‰∫é TAGÔºå‰º†ÁªüÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÊñπÊ≥ïÁõ¥Êé•ÂØπÈ¢ÑÂ§ÑÁêÜÁöÑËäÇÁÇπÁâπÂæÅËøõË°åËÆ≠ÁªÉÔºåËÄå‰∏çËÄÉËôëÂéüÂßãÊñáÊú¨„ÄÇÊÄßËÉΩÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÁâπÂæÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ïÁöÑÈÄâÊã©„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü P2TAGÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ TAG ‰∏äÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªËÆæËÆ°ÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÂõæÈ¢ÑËÆ≠ÁªÉÂíåÊèêÁ§∫„ÄÇP2TAG È¶ñÂÖà‰ΩøÁî®Ëá™ÊàëÁõëÁù£ÊçüÂ§±ÂØπ TAG ‰∏äÁöÑËØ≠Ë®ÄÊ®°Âûã (LM) ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏∫Êàë‰ª¨ÁöÑÊ°ÜÊû∂Ë∞ÉÊï¥‰∫ÜÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÁõÆÊ†á„ÄÇÁÑ∂Âêé‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÔºåÈááÁî®Ê∑∑ÂêàÊèêÁ§∫ÊñπÊ≥ïÔºåÂêåÊó∂ËÄÉËôëÊñáÊú¨ÂíåÂõæ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÂØπÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ TAG ËøõË°å‰∫ÜÂÆûÈ™åÔºåÂåÖÊã¨ËÆ∫ÊñáÂºïÁî®ÁΩëÁªúÂíå‰∫ßÂìÅÂÖ±ÂêåË¥≠‰π∞ÁΩëÁªú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÂ∞ëÊï∞ÈïúÂ§¥Â≠¶‰π†ÊñπÊ≥ïÔºåÊîπËøõ‰∫Ü +18.98% ~ +35.98%„ÄÇ

##### **LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**
2407.15351v2 by Jiaxing Zhang, Jiayi Liu, Dongsheng Luo, Jennifer Neville, Hua Wei

Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.

ÊëòË¶ÅÔºöËøëÊúüÁ†îÁ©∂Ë©¶ÂúñÈÄèÈÅéÂ§öÁ®ÆÈùûÁõ£Áù£ÂºèÂ≠∏ÁøíÊ®°Âûã‰æÜÊèê‰æõÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÂèØËß£ÈáãÊÄß„ÄÇÁî±ÊñºË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÔºåÁõÆÂâçÁöÑÊºîÁÆóÊ≥ïÂÆπÊòìÂèóÂà∞Â≠∏ÁøíÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Áü•Ë≠òÂµåÂÖ•Âà∞ GNN Ëß£ÈáãÁ∂≤Ë∑Ø‰∏≠Ôºå‰ª•ÈÅøÂÖçÂ≠∏ÁøíÂÅèÂ∑ÆÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ∞á LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ (BI) Ê®°ÁµÑÊ≥®ÂÖ•Ôºå‰ª•Ê∏õËºïÂ≠∏ÁøíÂÅèÂ∑Æ„ÄÇBI Ê®°ÁµÑÁöÑÊïàËÉΩÂ∑≤Âú®ÁêÜË´ñ‰∏äÂíåÂØ¶È©ó‰∏äÂæóÂà∞Ë≠âÂØ¶„ÄÇÊàëÂÄëÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄëÂ∑•‰ΩúÁöÑÂâµÊñ∞‰πãËôïÂú®ÊñºÂÖ©ÈÉ®ÂàÜÔºö1. ÊàëÂÄëÊèê‰æõ LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ‰ª•ÊîπÂñÑÁèæÊúâÊºîÁÆóÊ≥ïÊïàËÉΩÁöÑÂèØËÉΩÊÄß‰πãÊñ∞ËßÄÈªûÔºõ2. ÊàëÂÄëÁéáÂÖàË®éË´ñ GNN Ëß£ÈáãÂïèÈ°å‰∏≠ÁöÑÂ≠∏ÁøíÂÅèÂ∑ÆÂïèÈ°å„ÄÇ

##### **Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**
2407.15141v1 by Yu Zhang, Ruijie Yu, Kaipeng Zeng, Ding Li, Feng Zhu, Xiaokang Yang, Yaohui Jin, Yanyan Xu

High-throughput reaction condition (RC) screening is fundamental to chemical
synthesis. However, current RC screening suffers from laborious and costly
trial-and-error workflows. Traditional computer-aided synthesis planning (CASP)
tools fail to find suitable RCs due to data sparsity and inadequate reaction
representations. Nowadays, large language models (LLMs) are capable of tackling
chemistry-related problems, such as molecule design, and chemical logic Q\&A
tasks. However, LLMs have not yet achieved accurate predictions of chemical
reaction conditions. Here, we present MM-RCR, a text-augmented multimodal LLM
that learns a unified reaction representation from SMILES, reaction graphs, and
textual corpus for chemical reaction recommendation (RCR). To train MM-RCR, we
construct 1.2 million pair-wised Q\&A instruction datasets. Our experimental
results demonstrate that MM-RCR achieves state-of-the-art performance on two
open benchmark datasets and exhibits strong generalization capabilities on
out-of-domain (OOD) and High-Throughput Experimentation (HTE) datasets. MM-RCR
has the potential to accelerate high-throughput condition screening in chemical
synthesis.

ÊëòË¶ÅÔºöÈ´òÈÄöÈáèÂèçÊáâÊ¢ù‰ª∂ (RC) ÁØ©ÈÅ∏ÊòØÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÂü∫Á§é„ÄÇÁÑ∂ËÄåÔºåÁï∂ÂâçÁöÑ RC ÁØ©ÈÅ∏ÊúÉÈÅáÂà∞ÁπÅÁë£‰∏îÊòÇË≤¥ÁöÑË©¶ÈåØÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂÇ≥Áµ±ÁöÑÈõªËÖ¶ËºîÂä©ÂêàÊàêË¶èÂäÉ (CASP) Â∑•ÂÖ∑ÁÑ°Ê≥ïÊâæÂà∞ÂêàÈÅ©ÁöÑ RCÔºåÈÄôÊòØÂõ†ÁÇ∫Ë≥áÊñôÁ®ÄÁñè‰∏îÂèçÊáâË°®Á§∫‰∏çË∂≥„ÄÇÂ¶Ç‰ªäÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂ§†Ëß£Ê±∫ËàáÂåñÂ≠∏Áõ∏ÈóúÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂàÜÂ≠êË®≠Ë®àÂíåÂåñÂ≠∏ÈÇèËºØÂïèÁ≠î‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåLLM Â∞öÊú™ÈÅîÊàêÂåñÂ≠∏ÂèçÊáâÊ¢ù‰ª∂ÁöÑÊ∫ñÁ¢∫È†êÊ∏¨„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ MM-RCRÔºå‰∏ÄÂÄãÊñáÊú¨Â¢ûÂº∑ÁöÑÂ§öÊ®°ÊÖã LLMÔºåÂÆÉÂæû SMILES„ÄÅÂèçÊáâÂúñÂíåÊñáÊú¨Ë™ûÊñôÂ∫´Â≠∏ÁøíÁµ±‰∏ÄÁöÑÂèçÊáâË°®Á§∫Ôºå‰ª•ÈÄ≤Ë°åÂåñÂ≠∏ÂèçÊáâÊé®Ëñ¶ (RCR)„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥ MM-RCRÔºåÊàëÂÄëÂª∫Êßã‰∫Ü 120 Ëê¨Â∞çÈÖçÂ∞çÁöÑÂïèÁ≠îÊåá‰ª§Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåMM-RCR Âú®ÂÖ©ÂÄãÈñãÊîæÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶Âú®È†òÂüüÂ§ñ (OOD) ÂíåÈ´òÈÄöÈáèÂØ¶È©ó (HTE) Ë≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇMM-RCR ÊúâÂèØËÉΩÂä†ÈÄüÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÈ´òÈÄöÈáèÊ¢ù‰ª∂ÁØ©ÈÅ∏„ÄÇ

##### **On the Design and Analysis of LLM-Based Algorithms**
2407.14788v1 by Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou

We initiate a formal investigation into the design and analysis of LLM-based
algorithms, i.e. algorithms that contain one or multiple calls of large
language models (LLMs) as sub-routines and critically rely on the capabilities
of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt
engineering to complicated LLM-powered agent systems and compound AI systems,
have achieved remarkable empirical success, the design and optimization of them
have mostly relied on heuristics and trial-and-errors, which is largely due to
a lack of formal and analytical study for these algorithms. To fill this gap,
we start by identifying the computational-graph representation of LLM-based
algorithms, the design principle of task decomposition, and some key
abstractions, which then facilitate our formal analysis for the accuracy and
efficiency of LLM-based algorithms, despite the black-box nature of LLMs. We
further consider parallel decomposition for a case study, providing extensive
analytical and empirical study for four concrete examples of this pattern. Our
proposed framework holds promise for advancing LLM-based algorithms, by
revealing the reasons behind curious empirical phenomena, guiding the choices
of hyperparameters, predicting the empirical performance of algorithms, and
inspiring new algorithm design. To promote further study of LLM-based
algorithms, we release our source code at
https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®≠Ë®àÂíåÂàÜÊûêÂ±ïÈñãÊ≠£ÂºèË™øÊü•ÔºåÂç≥ÂåÖÂê´‰∏ÄÂÄãÊàñÂ§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Â≠êÂ∏∏ÂºèÂëºÂè´ÁöÑÊºîÁÆóÊ≥ïÔºå‰∏¶Ê•µÂ∫¶‰æùË≥¥ LLM ÁöÑÂäüËÉΩ„ÄÇÂÑòÁÆ°Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÂæûÂ∏∂ÊèêÁ§∫Â∑•Á®ãÁöÑÂü∫Êú¨ LLM ÂëºÂè´Âà∞Ë§áÈõúÁöÑ LLM È©ÖÂãïÁöÑ‰ª£ÁêÜÁ≥ªÁµ±ÂíåË§áÂêàÂºè AI Á≥ªÁµ±ÔºåÂ∑≤ÂèñÂæóÈ°ØËëóÁöÑÂØ¶Ë≠âÊàêÂäüÔºå‰ΩÜÂÖ∂Ë®≠Ë®àÂíåÊúÄ‰Ω≥ÂåñÂ§ßÂ§ö‰æùË≥¥Ë©¶È©óÊ≥ïÂíåÈåØË™§ÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂõ†ÁÇ∫Áº∫‰πèÂ∞çÈÄô‰∫õÊºîÁÆóÊ≥ïÁöÑÊ≠£ÂºèÂíåÂàÜÊûêÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÂæûË≠òÂà•Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®àÁÆóÂúñË°®Á§∫„ÄÅ‰ªªÂãôÂàÜËß£ÁöÑË®≠Ë®àÂéüÂâáÔºå‰ª•Âèä‰∏Ä‰∫õÈóúÈçµÊäΩË±°ÂåñÈñãÂßãÔºåÁÑ∂Âæå‰øÉÈÄ≤ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÈÄ≤Ë°åÊ≠£ÂºèÂàÜÊûêÔºåÂÑòÁÆ° LLM Êú¨Ë∫´ÂÖ∑ÊúâÈªëÁõíÁâπÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ËÄÉÊÖÆ‰∏¶Ë°åÂàÜËß£‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÁÇ∫Ê≠§Ê®°ÂºèÁöÑÂõõÂÄãÂÖ∑È´îÁØÑ‰æãÊèê‰æõÂª£Ê≥õÁöÑÂàÜÊûêÂíåÂØ¶Ë≠âÁ†îÁ©∂„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÊúâÊúõÊé®ÈÄ≤Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÊñπÊ≥ïÊòØÊè≠Á§∫Â•áÊÄ™ÁöÑÂØ¶Ë≠âÁèæË±°ËÉåÂæåÁöÑÂéüÂõ†„ÄÅÊåáÂ∞éË∂ÖÂèÉÊï∏ÁöÑÈÅ∏Êìá„ÄÅÈ†êÊ∏¨ÊºîÁÆóÊ≥ïÁöÑÂØ¶Ë≠âÊïàËÉΩÔºå‰∏¶ÊøÄÁôºÊñ∞ÁöÑÊºîÁÆóÊ≥ïË®≠Ë®à„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Â∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàëÂÄëÂú® https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm/ ÁôºÂ∏ÉÊàëÂÄëÁöÑÂéüÂßãÁ¢º„ÄÇ</paragraph>

##### **LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**
2407.18269v1 by Chen-Chia Chang, Yikang Shan, Shaoze Fan, Jing Li, Shun Zhang, Ningyuan Cao, Yiran Chen, Xin Zhang

In the realm of electronic and electrical engineering, automation of analog
circuit is increasingly vital given the complexity and customized requirements
of modern applications. However, existing methods only develop search-based
algorithms that require many simulation iterations to design a custom circuit
topology, which is usually a time-consuming process. To this end, we introduce
LaMAGIC, a pioneering language model-based topology generation model that
leverages supervised finetuning for automated analog circuit design. LaMAGIC
can efficiently generate an optimized circuit design from the custom
specification in a single pass. Our approach involves a meticulous development
and analysis of various input and output formulations for circuit. These
formulations can ensure canonical representations of circuits and align with
the autoregressive nature of LMs to effectively addressing the challenges of
representing analog circuits as graphs. The experimental results show that
LaMAGIC achieves a success rate of up to 96\% under a strict tolerance of 0.01.
We also examine the scalability and adaptability of LaMAGIC, specifically
testing its performance on more complex circuits. Our findings reveal the
enhanced effectiveness of our adjacency matrix-based circuit formulation with
floating-point input, suggesting its suitability for handling intricate circuit
designs. This research not only demonstrates the potential of language models
in graph generation, but also builds a foundational framework for future
explorations in automated analog circuit design.

ÊëòË¶ÅÔºöÂú®ÈõªÂ≠êÂíåÈõªÊ∞£Â∑•Á®ãÈ†òÂüü‰∏≠ÔºåËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË∂ä‰æÜË∂äÈáçË¶ÅÔºåÂõ†ÁÇ∫Áèæ‰ª£ÊáâÁî®Á®ãÂºèÂÖ∑ÊúâË§áÈõú‰∏îÂÆ¢Ë£ΩÂåñÁöÑÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÂÉÖÈñãÁôºÂü∫ÊñºÊêúÂ∞ãÁöÑÊºîÁÆóÊ≥ïÔºåÈúÄË¶ÅË®±Â§öÊ®°Êì¨ÂèçË¶ÜÈÅãÁÆóÊâçËÉΩË®≠Ë®àÂÆ¢Ë£ΩÂåñÈõªË∑ØÊãìÊí≤ÔºåÈÄôÈÄöÂ∏∏ÊòØ‰∏ÄÂÄãËÄóÊôÇÁöÑÈÅéÁ®ã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü LaMAGICÔºå‰∏ÄÂÄãÂü∫ÊñºÂÖàÈ©ÖË™ûË®ÄÊ®°ÂûãÁöÑÊãìÊí≤ÁîüÊàêÊ®°ÂûãÔºåÂÆÉÂà©Áî®Áõ£Áù£ÂæÆË™øÈÄ≤Ë°åËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à„ÄÇLaMAGIC ÂèØ‰ª•ÊúâÊïàÁéáÂú∞ÂæûÂÆ¢Ë£ΩÂåñË¶èÊ†º‰∏≠ÁîüÊàêÊúÄ‰Ω≥ÂåñÁöÑÈõªË∑ØË®≠Ë®àÔºåÂè™ÈúÄ‰∏ÄÊ¨°ÈÄöÈÅé„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰ªîÁ¥∞ÈñãÁôºÂíåÂàÜÊûêÈõªË∑ØÁöÑÂêÑÁ®ÆËº∏ÂÖ•ÂíåËº∏Âá∫ÂÖ¨Âºè„ÄÇÈÄô‰∫õÂÖ¨ÂºèÂèØ‰ª•Á¢∫‰øùÈõªË∑ØÁöÑÊ®ôÊ∫ñË°®Á§∫Ôºå‰∏¶Ëàá LM ÁöÑËá™Ëø¥Ê≠∏ÊÄßË≥™‰øùÊåÅ‰∏ÄËá¥Ôºå‰ª•ÊúâÊïàËß£Ê±∫Â∞áÈ°ûÊØîÈõªË∑ØË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁöÑÊåëÊà∞„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåLaMAGIC Âú® 0.01 ÁöÑÂö¥Ê†ºÂÆπÂ∑Æ‰∏ãÂØ¶Áèæ‰∫ÜÈ´òÈÅî 96% ÁöÑÊàêÂäüÁéá„ÄÇÊàëÂÄëÈÇÑÊ™¢Êü•‰∫Ü LaMAGIC ÁöÑÂèØÊì¥ÂÖÖÊÄßÂíåÈÅ©ÊáâÊÄßÔºåÁâπÂà•ÊòØÊ∏¨Ë©¶‰∫ÜÂÆÉÂú®Êõ¥Ë§áÈõúÈõªË∑Ø‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÊàëÂÄëÂü∫ÊñºÈÑ∞Êé•Áü©Èô£ÁöÑÈõªË∑ØÂÖ¨ÂºèËàáÊµÆÈªûËº∏ÂÖ•ÁöÑÂ¢ûÂº∑ÊïàËÉΩÔºåË°®ÊòéÂÆÉÈÅ©Áî®ÊñºËôïÁêÜË§áÈõúÁöÑÈõªË∑ØË®≠Ë®à„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜË™ûË®ÄÊ®°ÂûãÂú®ÂúñÂΩ¢ÁîüÊàê‰∏≠ÁöÑÊΩõÂäõÔºå‰πüÁÇ∫Êú™‰æÜÂú®Ëá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à‰∏≠ÁöÑÊé¢Á¥¢Âª∫Á´ã‰∫ÜÂü∫Á§éÊ°ÜÊû∂„ÄÇ

##### **Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**
2407.14224v1 by Suvajit Patra, Arkadip Maitra, Megha Tiwari, K. Kumaran, Swathy Prabhu, Swami Punyeshwarananda, Soumitra Samanta

Automatic Sign Language (SL) recognition is an important task in the computer
vision community. To build a robust SL recognition system, we need a
considerable amount of data which is lacking particularly in Indian sign
language (ISL). In this paper, we propose a large-scale isolated ISL dataset
and a novel SL recognition model based on skeleton graph structure. The dataset
covers 2,002 daily used common words in the deaf community recorded by 20 (10
male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL
recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)
by utilizing the human upper body skeleton graph structure. The HWGAT tries to
capture distinctive motions by giving attention to different body parts induced
by the human skeleton graph structure. The utility of the proposed dataset and
the usefulness of our model are evaluated through extensive experiments. We
pre-trained the proposed model on the proposed dataset and fine-tuned it across
different sign language datasets further boosting the performance of 1.10,
0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL
respectively compared to the existing state-of-the-art skeleton-based models.

ÊëòË¶ÅÔºöËá™ÂãïÊâãË™û (SL) Ë≠òÂà•ÊòØÈõªËÖ¶Ë¶ñË¶∫Á§æÁæ§‰∏≠ÁöÑÈáçË¶Å‰ªªÂãô„ÄÇË¶ÅÂª∫Á´ãÂº∑ÂÅ•ÁöÑ SL Ë≠òÂà•Á≥ªÁµ±ÔºåÊàëÂÄëÈúÄË¶ÅÂ§ßÈáèÁöÑË≥áÊñôÔºåËÄåÈÄôÂú®Âç∞Â∫¶ÊâãË™û (ISL) ‰∏≠ÁâπÂà•Áº∫‰πè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ§ßË¶èÊ®°ÁöÑÂ≠§Á´ã ISL Ë≥áÊñôÈõÜÔºå‰ª•Âèä‰∏ÄÂÄãÂü∫ÊñºÈ™®Êû∂ÂúñÁµêÊßãÁöÑÊñ∞Âûã SL Ë≠òÂà•Ê®°Âûã„ÄÇË©≤Ë≥áÊñôÈõÜÊ∂µËìã 2,002 ÂÄãËÅæÂïûÁ§æÁæ§‰∏≠Â∏∏Áî®ÁöÑÊó•Â∏∏ÂñÆÂ≠óÔºåÁî± 20 ‰Ωç (10 Áî∑ 10 Â•≥) ËÅæÂïûÊàê‰∫∫ÊâãË™ûËÄÖÈåÑË£ΩÔºàÂåÖÂê´ 40033 ÈÉ®ÂΩ±ÁâáÔºâ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã SL Ë≠òÂà•Ê®°ÂûãÔºåÂç≥ÂàÜÂ±§Ë¶ñÁ™óÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (HWGAT)ÔºåÂà©Áî®‰∫∫È´î‰∏äÂçäË∫´È™®Êû∂ÂúñÁµêÊßã„ÄÇHWGAT ÂòóË©¶ÈÄèÈÅéÈóúÊ≥®Áî±‰∫∫È´îÈ™®Êû∂ÂúñÁµêÊßãË™òÂ∞éÁöÑ‰∏çÂêåË∫´È´îÈÉ®‰Ωç‰æÜÊçïÊçâÁç®ÁâπÁöÑÂãï‰Ωú„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜÁöÑÊïàÁî®ÂíåÊàëÂÄëÊ®°ÂûãÁöÑÊúâÁî®ÊÄß„ÄÇÊàëÂÄëÂú®ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜ‰∏äÈ†êË®ìÁ∑¥ÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºå‰∏¶Âú®‰∏çÂêåÁöÑÊâãË™ûË≥áÊñôÈõÜ‰∏äÂæÆË™øÂÆÉÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰∫Ü INCLUDE„ÄÅLSA64„ÄÅAUTSL Âíå WLASL ‰∏ä 1.10„ÄÅ0.46„ÄÅ0.78 Âíå 6.84 ÂÄãÁôæÂàÜÈªûÁöÑÊïàËÉΩÔºåÂàÜÂà•ËàáÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÈ™®Êû∂ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇ

##### **Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**
2407.13989v1 by Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang

Graphs have emerged as critical data structures for content analysis in
various domains, such as social network analysis, bioinformatics, and
recommendation systems. Node classification, a fundamental task in this
context, is typically tackled using graph neural networks (GNNs).
Unfortunately, conventional GNNs still face challenges in scenarios with few
labeled nodes, despite the prevalence of few-shot node classification tasks in
real-world applications. To address this challenge, various approaches have
been proposed, including graph meta-learning, transfer learning, and methods
based on Large Language Models (LLMs). However, traditional meta-learning and
transfer learning methods often require prior knowledge from base classes or
fail to exploit the potential advantages of unlabeled nodes. Meanwhile,
LLM-based methods may overlook the zero-shot capabilities of LLMs and rely
heavily on the quality of generated contexts. In this paper, we propose a novel
approach that integrates LLMs and GNNs, leveraging the zero-shot inference and
reasoning capabilities of LLMs and employing a Graph-LLM-based active learning
paradigm to enhance GNNs' performance. Extensive experiments demonstrate the
effectiveness of our model in improving node classification accuracy with
considerably limited labeled data, surpassing state-of-the-art baselines by
significant margins.

ÊëòË¶ÅÔºöÂúñË°®Â∑≤ÊàêÁÇ∫ÂêÑÁ®ÆÈ†òÂüü‰∏≠ÂÖßÂÆπÂàÜÊûêÁöÑÈóúÈçµÊï∏ÊìöÁµêÊßãÔºå‰æãÂ¶ÇÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÁîüÁâ©Ë≥áË®äÂ≠∏ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÁØÄÈªûÂàÜÈ°ûÊòØÊ≠§ËÑàÁµ°‰∏≠ÁöÑÂü∫Êú¨‰ªªÂãôÔºåÈÄöÂ∏∏‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ‰æÜËôïÁêÜ„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂÑòÁÆ°ÁèæÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÊôÆÈÅçÂ≠òÂú®Â∞ëÊ®£Êú¨ÁØÄÈªûÂàÜÈ°û‰ªªÂãôÔºå‰ΩÜÂÇ≥Áµ±ÁöÑ GNN Âú®Ê®ôË®òÁØÄÈªûÂæàÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ã‰ªçÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÂ∑≤ÊèêÂá∫ÂêÑÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÂúñÂΩ¢ÂÖÉÂ≠∏Áøí„ÄÅÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂÖÉÂ≠∏ÁøíÂíåÈÅ∑ÁßªÂ≠∏ÁøíÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶Å‰æÜËá™Âü∫Á§éÈ°ûÂà•ÁöÑÂÖàÈ©óÁü•Ë≠òÔºåÊàñËÄÖÁÑ°Ê≥ïÂà©Áî®Êú™Ê®ôË®òÁØÄÈªûÁöÑÊΩõÂú®ÂÑ™Âã¢„ÄÇÂêåÊôÇÔºåÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂèØËÉΩÊúÉÂøΩË¶ñ LLM ÁöÑÈõ∂Ê®£Êú¨ËÉΩÂäõÔºå‰∏¶‰∏îÈÅéÂ∫¶‰æùË≥¥ÁîüÊàêË™ûÂ¢ÉÁöÑÂìÅË≥™„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÊï¥Âêà‰∫Ü LLM Âíå GNNÔºåÂà©Áî® LLM ÁöÑÈõ∂Ê®£Êú¨Êé®Ë´ñÂíåÊé®ÁêÜËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫Êñº Graph-LLM ÁöÑ‰∏ªÂãïÂ≠∏ÁøíÁØÑ‰æã‰æÜÂ¢ûÂº∑ GNN ÁöÑÊïàËÉΩ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîπÈÄ≤ÁØÄÈªûÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË®òÊï∏ÊìöÁõ∏Áï∂ÊúâÈôêÔºåÈ°ØËëóË∂ÖË∂ä‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñ„ÄÇ

##### **A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**
2407.13699v1 by Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, Farshad Navah, Amirmohammad Kazemeini

Recommender Systems (RS) play an integral role in enhancing user experiences
by providing personalized item suggestions. This survey reviews the progress in
RS inclusively from 2017 to 2024, effectively connecting theoretical advances
with practical applications. We explore the development from traditional RS
techniques like content-based and collaborative filtering to advanced methods
involving deep learning, graph-based models, reinforcement learning, and large
language models. We also discuss specialized systems such as context-aware,
review-based, and fairness-aware RS. The primary goal of this survey is to
bridge theory with practice. It addresses challenges across various sectors,
including e-commerce, healthcare, and finance, emphasizing the need for
scalable, real-time, and trustworthy solutions. Through this survey, we promote
stronger partnerships between academic research and industry practices. The
insights offered by this survey aim to guide industry professionals in
optimizing RS deployment and to inspire future research directions, especially
in addressing emerging technological and societal trends

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÈ´îÈ©ó‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤ÔºåÈÄèÈÅéÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÂïÜÂìÅÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖË™øÊü•ÂõûÈ°ß‰∫Ü RS Âú® 2017 Âπ¥Âà∞ 2024 Âπ¥ÈñìÁöÑÈÄ≤Â±ïÔºåÊúâÊïàÂú∞Â∞áÁêÜË´ñÈÄ≤Â±ïËàáÂØ¶ÈöõÊáâÁî®ÈÄ£ÁµêËµ∑‰æÜ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÂæûÂÇ≥Áµ±ÁöÑ RS ÊäÄË°ìÔºå‰æãÂ¶ÇÂü∫ÊñºÂÖßÂÆπÂíåÂçîÂêåÈÅéÊøæÔºåÂà∞Ê∂âÂèäÊ∑±Â∫¶Â≠∏Áøí„ÄÅÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ®°Âûã„ÄÅÂº∑ÂåñÂ≠∏ÁøíÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁ≠âÂÖàÈÄ≤ÊñπÊ≥ïÁöÑÁôºÂ±ï„ÄÇÊàëÂÄë‰πüË®éË´ñ‰∫ÜÂ∞àÈñÄÁöÑÁ≥ªÁµ±Ôºå‰æãÂ¶ÇÊÉÖÂ¢ÉÊÑüÁü•„ÄÅÂü∫ÊñºË©ïË´ñÂíåÂÖ¨Âπ≥ÊÑüÁü•ÁöÑ RS„ÄÇÈÄôÈ†ÖË™øÊü•ÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂ∞áÁêÜË´ñËàáÂØ¶ÂãôÁµêÂêàËµ∑‰æÜ„ÄÇÂÆÉËß£Ê±∫‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑÊåëÊà∞ÔºåÂåÖÊã¨ÈõªÂ≠êÂïÜÂãô„ÄÅÈÜ´ÁôÇ‰øùÂÅ•ÂíåÈáëËûçÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂèØÊì¥ÂÖÖ„ÄÅÂç≥ÊôÇÂíåÂèØ‰ø°Ë≥¥ÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÈúÄÊ±Ç„ÄÇÈÄèÈÅéÈÄôÈ†ÖË™øÊü•ÔºåÊàëÂÄë‰øÉÈÄ≤‰∫ÜÂ≠∏Ë°ìÁ†îÁ©∂ÂíåÁî¢Ê•≠ÂØ¶Âãô‰πãÈñìÊõ¥Âº∑Â§ßÁöÑÂ§•‰º¥Èóú‰øÇ„ÄÇÈÄôÈ†ÖË™øÊü•Êèê‰æõÁöÑË¶ãËß£Êó®Âú®ÂºïÂ∞éÁî¢Ê•≠Â∞àÊ•≠‰∫∫Â£´ÂÑ™Âåñ RS ÈÉ®ÁΩ≤Ôºå‰∏¶ÊøÄÂãµÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÁâπÂà•ÊòØÂú®Ëß£Ê±∫Êñ∞ËààÁöÑÊäÄË°ìÂíåÁ§æÊúÉË∂®Âã¢ÊñπÈù¢„ÄÇ

##### **MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**
2407.18961v2 by Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, Zirui Wang

Recent advances in large language models (LLMs) have increased the demand for
comprehensive benchmarks to evaluate their capabilities as human-like agents.
Existing benchmarks, while useful, often focus on specific application
scenarios, emphasizing task completion but failing to dissect the underlying
skills that drive these outcomes. This lack of granularity makes it difficult
to deeply discern where failures stem from. Additionally, setting up these
environments requires considerable effort, and issues of unreliability and
reproducibility sometimes arise, especially in interactive tasks. To address
these limitations, we introduce the Massive Multitask Agent Understanding
(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need
for complex environment setups. It evaluates models across five domains,
including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine
Learning coding, Contest-level programming and Mathematics, and covers five
essential capabilities: Understanding, Reasoning, Planning, Problem-solving,
and Self-correction. With a total of 20 meticulously designed tasks
encompassing over 3K distinct prompts, MMAU provides a comprehensive framework
for evaluating the strengths and limitations of LLM agents. By testing 18
representative models on MMAU, we provide deep and insightful analyses.
Ultimately, MMAU not only sheds light on the capabilities and limitations of
LLM agents but also enhances the interpretability of their performance.
Datasets and evaluation scripts of MMAU are released at
https://github.com/apple/axlearn/tree/main/docs/research/mmau.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ¢ûÂä†‰∫ÜÂ∞çÂÖ®Èù¢Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÈúÄÊ±ÇÔºå‰ª•Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫È°û‰∫∫‰ª£ÁêÜÁöÑËÉΩÂäõ„ÄÇÁèæÊúâÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÈõñÁÑ∂ÊúâÁî®Ôºå‰ΩÜÈÄöÂ∏∏Â∞àÊ≥®ÊñºÂÖ∑È´îÁöÑÊáâÁî®Â†¥ÊôØÔºåÂº∑Ë™ø‰ªªÂãôÂÆåÊàêÔºå‰ΩÜÊú™ËÉΩÂâñÊûêÈ©ÖÂãïÈÄô‰∫õÁµêÊûúÁöÑÂ∫ïÂ±§ÊäÄËÉΩ„ÄÇÈÄôÁ®ÆÁº∫‰πèÁ≤íÂ∫¶‰ΩøÂæóÈõ£‰ª•Ê∑±ÂÖ•Ëæ®Âà•Â§±ÊïóÁöÑÊ†πÊ∫ê„ÄÇÊ≠§Â§ñÔºåË®≠ÁΩÆÈÄô‰∫õÁí∞Â¢ÉÈúÄË¶ÅÂ§ßÈáèÁöÑÁ≤æÂäõÔºåÊúâÊôÇÊúÉÂá∫Áèæ‰∏çÂèØÈù†ÊÄßÂíåÂèØÈáçË§áÊÄßÁöÑÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®‰∫íÂãï‰ªªÂãô‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§ßË¶èÊ®°Â§ö‰ªªÂãô‰ª£ÁêÜÁêÜËß£ (MMAU) Âü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂÆÉÂÖ∑ÊúâÂÖ®Èù¢ÁöÑÈõ¢Á∑ö‰ªªÂãôÔºåÊ∂àÈô§‰∫ÜÂ∞çË§áÈõúÁí∞Â¢ÉË®≠ÁΩÆÁöÑÈúÄÊ±Ç„ÄÇÂÆÉË∑®Ë∂ä‰∫îÂÄãÈ†òÂüüË©ï‰º∞Ê®°ÂûãÔºåÂåÖÊã¨Â∑•ÂÖ∑‰ΩøÁî®„ÄÅÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ÂïèÁ≠î„ÄÅÊï∏ÊìöÁßëÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíÁ∑®Á¢º„ÄÅÁ´∂Ë≥ΩÁ¥öÁ∑®Á®ãÂíåÊï∏Â≠∏Ôºå‰∏¶Ê∂µËìã‰∫îÈ†ÖÂü∫Êú¨ËÉΩÂäõÔºöÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅË¶èÂäÉ„ÄÅÂïèÈ°åËß£Ê±∫ÂíåËá™ÊàëÁ≥æÊ≠£„ÄÇMMAU Á∏ΩÂÖ±ÂåÖÂê´ 20 È†ÖÁ≤æÂøÉË®≠Ë®àÁöÑ‰ªªÂãôÔºåÊ∂µËìãË∂ÖÈÅé 3K ÂÄã‰∏çÂêåÁöÑÊèêÁ§∫ÔºåÁÇ∫Ë©ï‰º∞ LLM ‰ª£ÁêÜÁöÑÂÑ™Âã¢ÂíåÂ±ÄÈôêÊÄßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊ°ÜÊû∂„ÄÇÈÄöÈÅéÂú® MMAU ‰∏äÊ∏¨Ë©¶ 18 ÂÄã‰ª£Ë°®ÊÄßÊ®°ÂûãÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ËÄåÊúâË¶ãÂú∞ÁöÑÂàÜÊûê„ÄÇÊúÄÁµÇÔºåMMAU ‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ‰ª£ÁêÜÁöÑËÉΩÂäõÂíåÂ±ÄÈôêÊÄßÔºåÈÇÑÂ¢ûÂº∑‰∫ÜÂÖ∂ÊÄßËÉΩÁöÑÂèØËß£ÈáãÊÄß„ÄÇMMAU ÁöÑÊï∏ÊìöÈõÜÂíåË©ï‰º∞ËÖ≥Êú¨Â∑≤ÁôºÂ∏ÉÂú® https://github.com/apple/axlearn/tree/main/docs/research/mmau„ÄÇ

##### **Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**
2407.12725v1 by Ben Yao, Yazhou Zhang, Qiuchi Li, Jing Qin

Elaborating a series of intermediate reasoning steps significantly improves
the ability of large language models (LLMs) to solve complex problems, as such
steps would evoke LLMs to think sequentially. However, human sarcasm
understanding is often considered an intuitive and holistic cognitive process,
in which various linguistic, contextual, and emotional cues are integrated to
form a comprehensive understanding of the speaker's true intention, which is
argued not be limited to a step-by-step reasoning process. To verify this
argument, we introduce a new prompting framework called SarcasmCue, which
contains four prompting strategies, $viz.$ chain of contradiction (CoC), graph
of cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits
LLMs to detect human sarcasm by considering sequential and non-sequential
prompting methods. Through a comprehensive empirical comparison on four
benchmarking datasets, we show that the proposed four prompting methods
outperforms standard IO prompting, CoT and ToT with a considerable margin, and
non-sequential prompting generally outperforms sequential prompting.

ÊëòË¶ÅÔºöÈÄöÈÅéÈó°Ëø∞‰∏ÄÁ≥ªÂàó‰∏≠ÈñìÊé®ÁêÜÊ≠•È©üÔºåÂ§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÁöÑËÉΩÂäõÔºåÂõ†ÁÇ∫ÈÄô‰∫õÊ≠•È©üÊúÉ‰øÉ‰Ωø LLM ÊåâÈ†ÜÂ∫èÊÄùËÄÉ„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÁöÑË´∑Âà∫ÁêÜËß£ÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ÊòØ‰∏ÄÁ®ÆÁõ¥Ë¶∫‰∏îÂÖ®Èù¢ÁöÑË™çÁü•ÈÅéÁ®ãÔºåÂÖ∂‰∏≠ÂêÑÁ®ÆË™ûË®Ä„ÄÅË™ûÂ¢ÉÂíåÊÉÖÁ∑íÁ∑öÁ¥¢Êï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•ÂÖ®Èù¢‰∫ÜËß£Ë™™Ë©±ËÄÖÁöÑÁúüÂØ¶ÊÑèÂúñÔºåÈÄôË¢´Ë™çÁÇ∫‰∏çÂÉÖÈôêÊñºÂæ™Â∫èÊº∏ÈÄ≤ÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãË´ñÈªûÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊèêÁ§∫Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫ SarcasmCueÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõõÁ®ÆÊèêÁ§∫Á≠ñÁï•ÔºåÂç≥ÁüõÁõæÈèà (CoC)„ÄÅÁ∑öÁ¥¢Âúñ (GoC)„ÄÅÁ∑öÁ¥¢Ë¢ã (BoC) ÂíåÁ∑öÁ¥¢ÂºµÈáè (ToC)ÔºåÂÆÉÂºïÁôº LLM ÈÄöÈÅéËÄÉÊÖÆÈ†ÜÂ∫èÂíåÈùûÈ†ÜÂ∫èÊèêÁ§∫ÊñπÊ≥ï‰æÜÊ™¢Ê∏¨‰∫∫È°ûÁöÑË´∑Âà∫„ÄÇÈÄöÈÅéÂ∞çÂõõÂÄãÂü∫Ê∫ñÊï∏ÊìöÈõÜÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂØ¶Ë≠âÊØîËºÉÔºåÊàëÂÄëË°®ÊòéÊâÄÊèêÂá∫ÁöÑÂõõÁ®ÆÊèêÁ§∫ÊñπÊ≥ï‰ª•Áõ∏Áï∂Â§ßÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÊ®ôÊ∫ñ IO ÊèêÁ§∫„ÄÅCoT Âíå ToTÔºå‰∏¶‰∏îÈùûÈ†ÜÂ∫èÊèêÁ§∫ÈÄöÂ∏∏ÂÑ™ÊñºÈ†ÜÂ∫èÊèêÁ§∫„ÄÇ

##### **Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**
2407.12703v3 by Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim

Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
encode only textual information, neglecting various topological structures of
knowledge graphs (KGs). In this paper, we empirically validate the significant
relations between the structural properties of KGs and the performance of the
PLM-based methods. To leverage the structural knowledge, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) that combines (i)
subgraph-aware mini-batching to encourage hard negative sampling, and (ii) a
new contrastive learning method to focus more on harder entities and harder
negative triples in terms of the structural properties. To the best of our
knowledge, this is the first study to comprehensively incorporate the
structural inductive bias of the subgraphs into fine-tuning PLMs. Extensive
experiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our
code is available.

ÊëòË¶ÅÔºöÂæÆË™øÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) Ëøë‰æÜÈ°ØÁ§∫Âá∫ÊîπÂñÑÁü•Ë≠òÂúñË≠úÂÆåÊàêÂäüËÉΩ (KGC) ÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Âü∫Êñº PLM ÁöÑÊñπÊ≥ïÂÉÖÁ∑®Á¢ºÊñáÂ≠óË≥áË®äÔºåÂøΩÁï•‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÂêÑÁ®ÆÊãìÊí≤ÁµêÊßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÁ∂ìÈ©óÈ©óË≠â‰∫Ü KG ÁöÑÁµêÊßãÂ±¨ÊÄßËàáÂü∫Êñº PLM ÁöÑÊñπÊ≥ïÊïàËÉΩ‰πãÈñìÁöÑÈáçË¶ÅÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂà©Áî®ÁµêÊßãÁü•Ë≠òÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®Êñº KGC ÁöÑÂ≠êÂúñÊÑüÁü•Ë®ìÁ∑¥Êû∂Êßã (SATKGC)ÔºåÂÆÉÁµêÂêà‰∫ÜÔºö(i) Â≠êÂúñÊÑüÁü•Â∞èÊâπÊ¨°ËôïÁêÜ‰ª•ÈºìÂãµÂõ∞Èõ£Ë≤†Èù¢ÊäΩÊ®£Ôºå‰ª•Âèä (ii) ‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºåÂú®ÁµêÊßãÂ±¨ÊÄßÊñπÈù¢Êõ¥Â∞àÊ≥®ÊñºÊõ¥Âõ∞Èõ£ÁöÑÂØ¶È´îÂíåÊõ¥Âõ∞Èõ£ÁöÑË≤†‰∏âÂÖÉÁµÑ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÂ≠êÂúñÁöÑÁµêÊßãÊ≠∏Á¥çÂÅèË™§ÂÖ®Èù¢Á¥çÂÖ• PLM ÂæÆË™øÁöÑÁ†îÁ©∂„ÄÇÂú®ÂõõÂÄã KGC Âü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü SATKGC ÁöÑÂÑ™Ë∂äÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÁèæÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Abstraction Alignment: Comparing Model and Human Conceptual Relationships**
2407.12543v1 by Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan

Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like "bolo tie" and "car tire" to more general concepts
like "CEO" and "model". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.

ÊëòË¶ÅÔºöÊäΩË±°Âåñ‚Äî‚ÄîÂ∞áÁâπÂÆöÁØÑ‰æãÊ¶ÇÊã¨ÁÇ∫Âª£Ê≥õÂèØÈáçË§á‰ΩøÁî®ÁöÑÊ®°ÂºèÁöÑÈÅéÁ®ã‚Äî‚ÄîÊòØ‰∫∫ÂÄëÊúâÊïàËôïÁêÜÂíåÂÑ≤Â≠òË≥áË®äÔºå‰∏¶Â∞áÂÖ∂Áü•Ë≠òÊáâÁî®ÊñºÊñ∞Ë≥áÊñôÁöÑÊ†∏ÂøÉ„ÄÇÊúâÂ∏åÊúõÁöÑÊòØÔºåÁ†îÁ©∂È°ØÁ§∫ ML Ê®°ÂûãÂ≠∏ÁøíË∑®Ë∂äÊäΩË±°Â±§Á¥öÁöÑË°®ÂæµÔºåÂæû„ÄåÁ¥∞È†òÂ∏∂„ÄçÂíå„ÄåÊ±ΩËªäËº™ËÉé„ÄçÁ≠âÂÖ∑È´îÊ¶ÇÂøµÂà∞„ÄåÂü∑Ë°åÈï∑„ÄçÂíå„ÄåÊ®°Âûã„ÄçÁ≠âÊõ¥‰∏ÄËà¨ÁöÑÊ¶ÇÂøµ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊäÄË°ìÂ≠§Á´ãÂú∞ÂàÜÊûêÈÄô‰∫õË°®ÂæµÔºåÂ∞áÂ≠∏ÁøíÂà∞ÁöÑÊ¶ÇÂøµË¶ñÁÇ∫Áç®Á´ãÁöÑÁî¢Áâ©ÔºåËÄå‰∏çÊòØÊäΩË±°ÁöÑÁõ∏‰∫íÈÄ£ÁµêÁ∂≤Ë∑Ø„ÄÇÂõ†Ê≠§ÔºåÂÑòÁÆ°ÊàëÂÄëÂèØ‰ª•Ë≠òÂà•Ê®°ÂûãÁî®‰æÜÁî¢ÁîüÂÖ∂Ëº∏Âá∫ÁöÑÊ¶ÇÂøµÔºå‰ΩÜÂæàÈõ£Ë©ï‰º∞ÂÆÉÊòØÂê¶Â≠∏ÁøíÂà∞Ê¶ÇÂøµÁöÑ‰∫∫È°ûÂ∞çÈΩäÊäΩË±°ÔºåÈÄô‰∫õÊ¶ÇÂøµÂ∞áÊ¶ÇÊã¨Âà∞Êñ∞ÁöÑË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊäΩË±°Â∞çÈΩäÔºå‰∏ÄÁ®ÆË°°ÈáèÊ®°ÂûãÂ≠∏ÁøíÁöÑÊäΩË±°ËàáÈ†êÊúüÁöÑÊäΩË±°‰πãÈñì‰∏ÄËá¥ÊÄßÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊ®°ÂûãËº∏Âá∫Ëàá‰∫∫È°ûÊäΩË±°ÂúñÂΩ¢Ôºà‰æãÂ¶ÇË™ûË®ÄÈóú‰øÇÊàñÈÜ´ÁôÇÁñæÁóÖÂ±§Á¥öÁµêÊßãÔºâÈÄ≤Ë°åÊØîËºÉ‰æÜÈáèÂåñÊäΩË±°Â∞çÈΩä„ÄÇÂú®Ëß£ÈáãÂΩ±ÂÉèÊ®°Âûã„ÄÅÂü∫Ê∫ñË™ûË®ÄÊ®°ÂûãÂíåÂàÜÊûêÈÜ´ÁôÇË≥áÊñôÈõÜÁöÑË©ï‰º∞‰ªªÂãô‰∏≠ÔºåÊäΩË±°Â∞çÈΩäÊèê‰æõ‰∫ÜÂ∞çÊ®°ÂûãË°åÁÇ∫ÂíåË≥áÊñôÈõÜÂÖßÂÆπÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£ÔºåÊ†πÊìöËàá‰∫∫È°ûÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÂçÄÂàÜÈåØË™§ÔºåÊì¥Â±ïÁï∂ÂâçÊ®°ÂûãÂìÅË≥™ÊåáÊ®ôÁöÑË©≥Á¥∞Á®ãÂ∫¶Ôºå‰∏¶Êè≠Á§∫ÊîπÂñÑÁèæÊúâ‰∫∫È°ûÊäΩË±°ÁöÑÊñπÊ≥ï„ÄÇ

##### **Struct-X: Enhancing Large Language Models Reasoning with Structured Data**
2407.12522v1 by Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi

Structured data, rich in logical and relational information, has the
potential to enhance the reasoning abilities of large language models (LLMs).
Still, its integration poses a challenge due to the risk of overwhelming LLMs
with excessive tokens and irrelevant context information. To address this, we
propose Struct-X, a novel framework that operates through five key phases:
``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize
structured data. It begins by encoding structured data into a topological space
using graph embeddings, followed by filling in missing entity information with
knowledge retrieval modules, and filtering out irrelevant tokens via a
self-supervised module. The final phase involves constructing a topological
network with selected tokens to further reduce the total token length for more
effective LLM inference. Additionally, Struct-X includes an Auxiliary Module
trained to generate prompts, aiding LLMs in analyzing structured data.
Extensive experiments on benchmarks, including the knowledge graph
question-answer task and the long document reading comprehension task, show
that Struct-X notably improves LLM reasoning, demonstrating the effectiveness
of structured data augmentation in improving LLM inference with complex input
context.

ÊëòË¶ÅÔºöÁµêÊßãÂåñË≥áÊñôÂØåÂê´ÈÇèËºØÂíåÈóú‰øÇË≥áË®äÔºåÊúâÊΩõÂäõÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºÈÅéÂ§öÁ¨¶ËôüÂíåÁÑ°ÈóúËÑàÁµ°Ë≥áË®äÂèØËÉΩÊúÉËÆì LLM ‰∏çÂ†™Ë≤†Ëç∑ÔºåÂõ†Ê≠§Êï¥ÂêàÊ≠§È°ûË≥áÊñôÊßãÊàê‰∫Ü‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ Struct-XÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄèÈÅé‰∫îÂÄãÈóúÈçµÈöéÊÆµÈÅã‰ΩúÁöÑÊñ∞Á©éÊû∂ÊßãÔºö``ËÆÄÂèñ-Âª∫Ê®°-Â°´Ë£ú-ÂèçÊÄù-Êé®ÁêÜ''ÔºåÊúâÊïàÂú∞ËÆì LLM ËÉΩÂ§†Âà©Áî®ÁµêÊßãÂåñË≥áÊñô„ÄÇÂÆÉÈ¶ñÂÖà‰ΩøÁî®ÂúñÂΩ¢ÂµåÂÖ•Â∞áÁµêÊßãÂåñË≥áÊñôÁ∑®Á¢ºÂà∞ÊãìÊí≤Á©∫Èñì‰∏≠ÔºåÊé•ËëóÂà©Áî®Áü•Ë≠òÊì∑ÂèñÊ®°ÁµÑÂ°´Ë£úÈÅ∫Â§±ÁöÑÂØ¶È´îË≥áË®äÔºå‰∏¶ÈÄèÈÅéËá™ÊàëÁõ£Áù£Ê®°ÁµÑÁØ©ÈÅ∏Âá∫ÁÑ°ÈóúÁ¨¶Ëôü„ÄÇÊúÄÂæå‰∏ÄÂÄãÈöéÊÆµÊ∂âÂèäÂª∫Êßã‰∏ÄÂÄãÊãìÊí≤Á∂≤Ë∑ØÔºåÂÖ∂‰∏≠ÂåÖÂê´ÈÅ∏ÂÆöÁöÑÁ¨¶ËôüÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Ê∏õÂ∞ëÁ∏ΩÁ¨¶ËôüÈï∑Â∫¶Ôºå‰ª•‰æøÊõ¥ÊúâÊïàÂú∞ÈÄ≤Ë°å LLM Êé®Ë´ñ„ÄÇÊ≠§Â§ñÔºåStruct-X ÈÇÑÂåÖÊã¨‰∏ÄÂÄãËºîÂä©Ê®°ÁµÑÔºåÁ∂ìÈÅéË®ìÁ∑¥ÂèØ‰ª•Áî¢ÁîüÊèêÁ§∫ÔºåÂçîÂä© LLM ÂàÜÊûêÁµêÊßãÂåñË≥áÊñô„ÄÇÂú®Âü∫Ê∫ñ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óÔºåÂåÖÊã¨Áü•Ë≠òÂúñË≠úÂïèÁ≠î‰ªªÂãôÂíåÈï∑ÁØáÊñá‰ª∂Èñ±ËÆÄÁêÜËß£‰ªªÂãôÔºåÈ°ØÁ§∫ Struct-X ÊòéÈ°ØÊîπÂñÑ‰∫Ü LLM Êé®ÁêÜÔºåË≠âÊòé‰∫ÜÁµêÊßãÂåñË≥áÊñôÊì¥ÂÖÖÂú®ÊîπÂñÑ LLM Êé®Ë´ñÊôÇÁöÑÊúâÊïàÊÄßÔºåÁâπÂà•ÊòØÂú®Ëº∏ÂÖ•ËÑàÁµ°Ë§áÈõúÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇ

##### **Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**
2407.12888v1 by Alexander R. Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping

The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.

ÊëòË¶ÅÔºö<paragraph>Áèæ‰ªäÂ§ßÈáèÁöÑÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÂ∞çË©¶ÂúñÊúâÊïàÊ∂àÂåñ„ÄÅËôïÁêÜÂíåÁêÜËß£ÈÄô‰∫õÁôºÁèæÁöÑÁ†îÁ©∂‰∫∫Âì°ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫Âú®ÈÄôÂÄãË§áÈõú‰∏îÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÁí∞Â¢É‰∏≠Â∞éËà™ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåLLM ÂèØËÉΩÊúÉÂ∞éËá¥ÂπªË¶∫ÂèçÊáâÔºåÈÄô‰ΩøÂæóÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) Â∞çÊñºÁç≤ÂæóÊ∫ñÁ¢∫Ë≥áË®äËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÂÄãÂçîÂÆö‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RUGGEDÔºàÂúñÂΩ¢Â∞éÂºïÂèØËß£ÈáãÁñæÁóÖÂçÄÂàÜÁöÑÊ™¢Á¥¢ÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊè¥Á†îÁ©∂‰∫∫Âì°ÈÄ≤Ë°åÁü•Ë≠òÊï¥ÂêàÂíåÂÅáË®≠Áî¢ÁîüÔºåÊâæÂá∫Á∂ìÈÅéÈ©óË≠âÁöÑÈÄ≤Â±ïË∑ØÂæë„ÄÇ‰æÜËá™Âá∫ÁâàÁâ©ÂíåÁü•Ë≠òÂ∫´ÁöÑÁõ∏ÈóúÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÊúÉÈÄèÈÅéÊñáÊú¨Êé¢ÂãòÈóúËÅØÂàÜÊûêÂíåÁñæÁóÖÁØÄÈªûÁöÑÂèØËß£ÈáãÂúñÂΩ¢È†êÊ∏¨Ê®°ÂûãÈÄ≤Ë°åÊ™¢Èñ±„ÄÅÊï¥ÂêàÂíåËêÉÂèñÔºåÈ†êÊ∏¨Ëó•Áâ©ÂíåÁñæÁóÖ‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØ„ÄÇÈÄô‰∫õÂàÜÊûêÈÄ£ÂêåÁîüÁâ©ÈÜ´Â≠∏ÊñáÊú¨ÊúÉÊï¥ÂêàÂà∞‰∏ÄÂÄãÊû∂Êßã‰∏≠ÔºåË©≤Êû∂Êßã‰øÉÈÄ≤‰ΩøÁî®ËÄÖÂ∞éÂêëÁöÑÊ©üÂà∂Èó°ÊòéÔºå‰ª•ÂèäÈÄèÈÅé RAG ÂïüÁî®ÁöÑ LLM ÈÄ≤Ë°åÂÅáË®≠Êé¢Ë®é„ÄÇ‰∏ÄÂÄãËá®Â∫ä‰ΩøÁî®Ê°à‰æãÂ±ïÁ§∫‰∫Ü RUGGED Ë©ï‰º∞ÂíåÊé®Ëñ¶Áî®ÊñºÂøÉÂæãÂ§±Â∏∏ÊÄßÂøÉËÇåÁóÖËÆä (ACM) ÂíåÊì¥ÂºµÂûãÂøÉËÇåÁóÖËÆä (DCM) ÁöÑÊ≤ªÁôÇÊñπÊ≥ïÁöÑËÉΩÂäõÔºåÂàÜÊûêËôïÊñπËó•Áâ©ÁöÑÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®ÂíåÊú™Êé¢Á¥¢ÁöÑÁî®ÈÄî„ÄÇÈÄôÂÄãÂπ≥Âè∞Â∞á LLM ÂπªË¶∫ÈôçÂà∞ÊúÄ‰ΩéÔºåÊèê‰æõÂèØÊìç‰ΩúÁöÑË¶ãËß£Ôºå‰∏¶ÊîπÂñÑÊñ∞Ê≤ªÁôÇÊñπÊ≥ïÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**
2407.11638v1 by He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.

ÊëòË¶ÅÔºöËøëÊúüÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁßçËµÑÊñôÊé¢Âãò‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÊûÅÂ§ßÁöÑÊΩúÂäõÔºå‰æãÂ¶ÇÁü•ËØÜÈóÆÁ≠î„ÄÅÊï∞Â≠¶Êé®ÁêÜÂíåÂ∏∏ËØÜÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåLLM Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÊÄßÂú∞Ë∞ÉÊü•ÂÖ∂Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞„ÄÇÁî±‰∫éÁº∫‰πèÂêåÊó∂ÂåÖÂê´ÂõæË°®ÂíåÊñáÊú¨ËµÑÊñôÁöÑÈ´òÂìÅË¥®Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨È¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ MidEast-TE-mini ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂü∫‰∫éÊ≠§Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Á∫øÊñπÊ≥ïÔºåÂÖ∂ÁâπÁÇπÊòØÂêÑÁßçËæìÂÖ•Ê†ºÂºèÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Ê®°Âùó„ÄÇ‰ªéÂπøÊ≥õÁöÑÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞Áõ¥Êé•Â∞ÜÂéüÂßãÊñáÊú¨Êï¥ÂêàÂà∞ LLM ÁöÑËæìÂÖ•‰∏≠Âπ∂‰∏ç‰ºöÂ¢ûÂº∫Èõ∂Ê¨°Â≠¶‰π†Â§ñÊé®ÊÄßËÉΩ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂú®ÁâπÂÆöÂ§çÊùÇ‰∫ã‰ª∂‰∏≠Á∫≥ÂÖ•ÂéüÂßãÊñáÊú¨Âπ∂ÂæÆË∞É LLM ‰ºöÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊ£ÄÁ¥¢Ê®°ÂùóÁöÑÂ¢ûÂº∫ÔºåLLM ÂèØ‰ª•ÊúâÊïàÂú∞ÊçïÊçâÈöêËóèÂú®ÂéÜÂè≤‰∫ã‰ª∂‰∏≠ÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÊ®°Âºè„ÄÇÂêåÊó∂ÔºåËØ∏Â¶ÇÊµÅË°åÂ∫¶ÂÅèÂ∑ÆÂíåÈïøÂ∞æÈóÆÈ¢òÁ≠âÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®‰∫é LLM ‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫é RAG ÁöÑÊñπÊ≥ï‰∏≠„ÄÇËøô‰∫õÂèëÁé∞‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑ‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïÁöÑÁêÜËß£ÔºåËøòÁ™ÅÂá∫‰∫ÜÂá†‰∏™ÊúâÂâçÊôØÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËøôÈ°πÂÖ®Èù¢ÁöÑËØÑ‰º∞ÔºåËøûÂêåÂ∑≤Á°ÆÂÆöÁöÑÁ†îÁ©∂Êú∫‰ºöÔºåÂ∞ÜÊûÅÂ§ßÂú∞‰øÉËøõÈÄöËøá LLM ËøõË°åÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÁöÑÊú™Êù•Á†îÁ©∂„ÄÇ

##### **Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**
2407.12068v2 by Kai Guo, Zewen Liu, Zhikai Chen, Hongzhi Wen, Wei Jin, Jiliang Tang, Yi Chang

Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing tasks. Recently, several LLMs-based
pipelines have been developed to enhance learning on graphs with text
attributes, showcasing promising performance. However, graphs are well-known to
be susceptible to adversarial attacks and it remains unclear whether LLMs
exhibit robustness in learning on graphs. To address this gap, our work aims to
explore the potential of LLMs in the context of adversarial attacks on graphs.
Specifically, we investigate the robustness against graph structural and
textual perturbations in terms of two dimensions: LLMs-as-Enhancers and
LLMs-as-Predictors. Through extensive experiments, we find that, compared to
shallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior
robustness against structural and textual attacks.Based on these findings, we
carried out additional analyses to investigate the underlying causes.
Furthermore, we have made our benchmark library openly available to facilitate
quick and fair evaluations, and to encourage ongoing innovative research in
this field.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÈÉΩÂ±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊúÄËøëÔºåÂ∑≤ÈñãÁôºÂá∫Â§öÂÄãÂü∫Êñº LLM ÁöÑÁÆ°ÈÅìÔºå‰ª•Â¢ûÂº∑ÂÖ∑ÊúâÊñáÂ≠óÂ±¨ÊÄßÁöÑÂúñÂΩ¢Â≠∏ÁøíÔºåÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂúñÂΩ¢ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊîªÊìäÔºåËÄå LLM Âú®ÂúñÂΩ¢Â≠∏Áøí‰∏≠ÊòØÂê¶Â±ïÁèæÂá∫Á©©ÂÅ•ÊÄß‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Êé¢Ë®é LLM Âú®ÂúñÂΩ¢Â∞çÊäóÊÄßÊîªÊìä‰∏≠ÁöÑÊΩõÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈáùÂ∞çÂÖ©ÂÄãÈù¢ÂêëÊé¢Ë®éÂÖ∂Â∞çÂúñÂΩ¢ÁµêÊßãÂíåÊñáÂ≠óÊìæÂãïÁöÑÁ©©ÂÅ•ÊÄßÔºöLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÁôºÁèæÔºåËàáÊ∑∫Â±§Ê®°ÂûãÁõ∏ÊØîÔºåLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®Âú®ÁµêÊßãÊÄßÂíåÊñáÂ≠óÊîªÊìä‰∏≠ÈÉΩÊèê‰æõÂÑ™Áï∞ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÈ°çÂ§ñÁöÑÂàÜÊûê‰æÜÊé¢Ë®éÂÖ∂Ê†πÊú¨ÂéüÂõ†„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∑≤ÂÖ¨ÈñãÊàëÂÄëÁöÑÂü∫Ê∫ñÂ∫´Ôºå‰ª•Âà©Âø´ÈÄü‰∏îÂÖ¨Âπ≥ÁöÑË©ï‰º∞Ôºå‰∏¶ÈºìÂãµÊåÅÁ∫åÈÄ≤Ë°åÈÄôÊñπÈù¢ÁöÑÂâµÊñ∞Á†îÁ©∂„ÄÇ

##### **CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**
2407.11393v2 by Kalliopi Basioti, Mohamed A. Abdelsalam, Federico Fancellu, Vladimir Pavlovic, Afsaneh Fazly

Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.

ÊëòË¶ÅÔºöÂèØÊéßÂõæÂÉèÊ†áÊ≥® (CIC) Êó®Âú®ÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰ª•ÊèèËø∞ÂõæÂÉèÔºåÊù°‰ª∂ÊòØÊ†πÊçÆÊúÄÁªàÁî®Êà∑Êèê‰æõÁöÑËµÑËÆØÔºå‰æãÂ¶ÇÂå∫Âüü„ÄÅÂÆû‰ΩìÊàñÊÑüÂÖ¥Ë∂£ÁöÑ‰∫ã‰ª∂„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂõæÂÉèËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂåÖÂê´ÊèèËø∞Êï¥‰∏™ÂõæÂÉèÁöÑÊ†áÊ≥®Ôºå‰ΩøÂÖ∂Êó†Ê≥ïÊúâÊïàËÆ≠ÁªÉ CIC Ê®°ÂûãÔºåËÄåËøô‰∫õÊ®°ÂûãÊúâÂèØËÉΩÂÖ≥Ê≥®‰ªª‰ΩïÂå∫ÂüüÊàñÂÖ≥Á≥ªÁöÑÂ≠êÈõÜ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂÖ®Ëá™Âä®ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Âª∫Á´ãÂú®‰∏éÂõæÂÉèÂÖ≥ËÅîÁöÑÁé∞ÊúâÊ†áÊ≥®ÈõÜ‰πã‰∏äÁöÑÁªü‰∏ÄÁªìÊûÑÂåñËØ≠‰πâË°®Á§∫Êù•ÊäΩÊ†∑ÂÖ∂‰ªñËÅöÁÑ¶‰∏îËßÜËßâÊé•Âú∞ÁöÑÊ†áÊ≥®„ÄÇÊàë‰ª¨Âà©Áî®Ë∑®ËØ≠Ë®ÄÂõæÂºèËØ≠‰πâÂΩ¢ÂºèÂåñÊäΩË±°ÊÑè‰πâË°®Á§∫ (AMR) Êù•ÁºñÁ†ÅÂÆû‰Ωì‰πãÈó¥ÊâÄÊúâÂèØËÉΩÁöÑÁ©∫Èó¥ËØ≠‰πâÂÖ≥Á≥ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂΩìÂâçÊñπÊ≥ï‰∏≠‰ªÖÂÖ≥Ê≥®ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÊàë‰ª¨‰ΩøÁî®ËøôÁßçÁªìÊûÑÂåñËØ≠‰πâÂ¢ûÂº∫ (SSA) Ê°ÜÊû∂Êù•Â¢ûÂº∫Áé∞ÊúâÁöÑÂõæÂÉèÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºå‰ΩøÂÖ∂Êé•Âú∞‰∏îÂèØÊéßÁöÑÊ†áÊ≥®ÔºåÂ¢ûÂä†ÂÆÉ‰ª¨ÁöÑÁ©∫Èó¥ÂíåËØ≠‰πâÂ§öÊ†∑ÊÄß‰ª•ÂèäÁÑ¶ÁÇπË¶ÜÁõñËåÉÂõ¥„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Êñ∞Ê®°Âûã CIC-BART-SSAÔºå‰∏ìÈó®ÈíàÂØπ CIC ‰ªªÂä°ÈáèË∫´ÂÆöÂà∂ÔºåÂÖ∂ÊéßÂà∂‰ø°Âè∑Êù•Ëá™ SSA Â§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨Âá≠ÁªèÈ™åË°®ÊòéÔºå‰∏é SOTA CIC Ê®°ÂûãÁõ∏ÊØîÔºåCIC-BART-SSA ÁîüÊàêÁöÑÊ†áÊ≥®Âú®Â§öÊ†∑ÊÄßÂíåÊñáÊú¨Ë¥®ÈáèÊñπÈù¢Êõ¥ËÉú‰∏ÄÁ≠πÔºåÂú®ÂèØÊéßÊÄßÊñπÈù¢ÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåËÄå‰∏îÈáçË¶ÅÁöÑÊòØÔºåÈÄöËøáÊúâÊïàÂú∞Êé®ÂπøÂà∞ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈ´òÂ∫¶ËÅöÁÑ¶Âú∫ÊôØÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞Áº©Â∞è‰∫ÜÂπøÊ≥õÂíåÈ´òÂ∫¶ËÅöÁÑ¶ÁöÑÂèóÊéßÊ†áÊ≥®ÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ‰ª£Á†ÅÂèØ‰ªé https://github.com/SamsungLabs/CIC-BART-SSA Ëé∑Âæó„ÄÇ

##### **Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**
2407.10805v1 by Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo

Retrieval-augmented generation (RAG) has significantly advanced large
language models (LLMs) by enabling dynamic information retrieval to mitigate
knowledge gaps and hallucinations in generated content. However, these systems
often falter with complex reasoning and consistency across diverse queries. In
this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns
questions with the knowledge graph and uses it as a navigational tool, which
deepens and refines the RAG paradigm for information collection and
integration. The KG-guided navigation fosters deep and long-range associations
to uphold logical consistency and optimize the scope of retrieval for precision
and interoperability. In conjunction, factual consistency can be better ensured
through semantic similarity guided by precise directives. ToG${2.0}$ not only
improves the accuracy and reliability of LLMs' responses but also demonstrates
the potential of hybrid structured knowledge systems to significantly advance
LLM reasoning, aligning it closer to human-like performance. We conducted
extensive experiments on four public datasets to demonstrate the advantages of
our method compared to the baseline.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÈÄèÈÅéÂïüÁî®ÂãïÊÖãË≥áË®äÊ™¢Á¥¢‰æÜÊ∏õËºïÁîüÊàêÂÖßÂÆπ‰∏≠ÁöÑÁü•Ë≠òÂ∑ÆË∑ùÂíåÂπªË¶∫ÔºåÂ§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ≥ªÁµ±Âú®Ë§áÈõúÊé®ÁêÜÂíåË∑®‰∏çÂêåÊü•Ë©¢ÁöÑ‰∏ÄËá¥ÊÄßÊñπÈù¢Â∏∏Â∏∏Ë°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Think-on-Graph 2.0Ôºå‰∏ÄÂÄãÂ¢ûÂº∑ÁöÑ RAG Ê°ÜÊû∂ÔºåÂÆÉÂ∞áÂïèÈ°åËàáÁü•Ë≠òÂúñË≠úÂ∞çÈΩäÔºå‰∏¶Â∞áÂÖ∂Áî®‰ΩúÂ∞éËà™Â∑•ÂÖ∑ÔºåÈÄôÂä†Ê∑±‰∏¶ÊîπÈÄ≤‰∫Ü RAG ÂÖ∏ÁØÑÔºåÁî®ÊñºË≥áË®äÊî∂ÈõÜÂíåÊï¥Âêà„ÄÇÂèóÁü•Ë≠òÂúñË≠úÂºïÂ∞éÁöÑÂ∞éËà™‰øÉÈÄ≤‰∫ÜÊ∑±Â±§‰∏îÈï∑Á®ãÁöÑÈóúËÅØÔºå‰ª•Á∂≠ÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄß‰∏¶ÊúÄ‰Ω≥ÂåñÊ™¢Á¥¢ÁØÑÂúçÔºå‰ª•ÊèêÈ´òÁ≤æÁ¢∫Â∫¶Âíå‰∫íÊìç‰ΩúÊÄß„ÄÇÂêåÊôÇÔºå‰∫ãÂØ¶‰∏ÄËá¥ÊÄßÂèØ‰ª•ÈÄèÈÅéÁî±Á≤æÁ¢∫ÊåáÁ§∫ÂºïÂ∞éÁöÑË™ûÊÑèÁõ∏‰ººÊÄßÁç≤ÂæóÊõ¥Â•ΩÁöÑÁ¢∫‰øù„ÄÇToG${2.0}$ ‰∏çÂÉÖÊèêÂçá‰∫Ü LLM ÂõûÊáâÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄßÔºå‰πüÂ±ïÁ§∫‰∫ÜÊ∑∑ÂêàÁµêÊßãÂåñÁü•Ë≠òÁ≥ªÁµ±ÁöÑÊΩõÂäõÔºåÂèØ‰ª•Â§ßÂπÖÊèêÂçá LLM Êé®ÁêÜÔºå‰ΩøÂÖ∂Êõ¥Êé•Ëøë‰∫∫È°ûËà¨ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÂú®ÂõõÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Â±ïÁ§∫ÊàëÂÄëÁöÑÊñπÊ≥ïÁõ∏ËºÉÊñºÂü∫Á∑öÁöÑÂÑ™Âã¢„ÄÇ

##### **Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**
2407.10794v1 by Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li

Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.

ÊëòË¶ÅÔºö<paragraph>Áü•Ë≠òÂúñË≠ú (KG) Âú®‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüËá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âª£Ê≥õÊáâÁî®Êñº‰∏ãÊ∏∏‰ªªÂãôÔºå‰æãÂ¶ÇÂ¢ûÂº∑ÂïèÁ≠î (QA) Á≥ªÁµ±„ÄÇÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãÈÄöÂ∏∏ÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂ÁöÑÂ§ßÈáèÂ∑•‰Ωú„ÄÇÊúÄËøëÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë¢´Áî®ÊñºÁü•Ë≠òÂúñË≠úÂª∫Êßã (KGC)ÔºåÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÈóúÊ≥®Â±ÄÈÉ®ËßÄÈªûÔºåÂæûÂÄãÂà•Âè•Â≠êÊàñÊñá‰ª∂‰∏≠ÊèêÂèñÁü•Ë≠ò‰∏âÂÖÉÁµÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü GraphusionÔºå‰∏ÄÂÄãÂæûËá™Áî±ÊñáÊú¨‰∏≠ÈÄ≤Ë°åÈõ∂Ê¨°Â≠∏ÁøíÁöÑ KGC Ê°ÜÊû∂„ÄÇÊ†∏ÂøÉËûçÂêàÊ®°ÁµÑÊèê‰æõ‰∏âÂÖÉÁµÑÁöÑÂÖ®Â±ÄËßÄÈªûÔºåÂåÖÂê´ÂØ¶È´îÂêà‰Ωµ„ÄÅË°ùÁ™ÅËß£Ê±∫ÂíåÊñ∞‰∏âÂÖÉÁµÑÁôºÁèæ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á Graphusion ÊáâÁî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) È†òÂüüÔºå‰∏¶Âú®ÊïôËÇ≤Â†¥ÊôØ‰∏≠È©óË≠âÂÆÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü TutorQAÔºå‰∏ÄÂÄãÊñ∞ÁöÑÁî±Â∞àÂÆ∂È©óË≠âÁöÑÂúñË≠úÊé®ÁêÜÂíåÂïèÁ≠îÂü∫Ê∫ñÔºåÂåÖÂê´ÂÖ≠È†Ö‰ªªÂãôÂíåÁ∏ΩË®à 1,200 ÂÄãÂïèÁ≠îÂ∞ç„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Ë°®ÊòéÔºåGraphusion Âú®ÈÄ£ÁµêÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫Â∫¶‰∏äÊØîÁõ£Áù£ÂºèÂü∫Ê∫ñÈ´òÂá∫ 10%„ÄÇÊ≠§Â§ñÔºåÂú®Ê¶ÇÂøµÂØ¶È´îÊèêÂèñÂíåÈóú‰øÇË≠òÂà•ÁöÑ‰∫∫È°ûË©ï‰º∞‰∏≠ÔºåÂÆÉÂàÜÂà•Áç≤Âæó‰∫Ü 3 ÂàÜ‰∏≠ÁöÑ 2.92 ÂàÜÂíå 2.37 ÂàÜ„ÄÇ</paragraph>

##### **GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**
2407.10793v1 by Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada

Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂõûÊáâË©ï‰º∞ÊñπÊ≥ïÂíå‰∏ç‰∏ÄËá¥ÊÄßÂÅµÊ∏¨ÔºàÂèàÁ®±ÁÇ∫ÂπªË¶∫ÔºâÔºåÁõ∏Â∞çÊñºÊâÄÊèê‰æõÁöÑÁü•Ë≠òÔºåÂ∞çÊñº LLM ÊáâÁî®Ê≠£ËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÁõÆÂâçÁöÑÊåáÊ®ôÁÑ°Ê≥ïÊèê‰æõÂèØËß£ÈáãÁöÑÊ±∫Á≠ñ„ÄÅÁ≥ªÁµ±ÊÄßÂú∞Ê™¢Êü•ÂõûÊáâ‰∏≠ÁöÑÊâÄÊúâË≥áË®äÔºåËÄå‰∏îÂú®ÂØ¶Âãô‰∏ä‰ΩøÁî®ÊôÇÔºåÈÄöÂ∏∏ÈÅéÊñºËÄóË≤ªÈÅãÁÆóË≥áÊ∫ê„ÄÇÊàëÂÄëÊèêÂá∫ GraphEvalÔºö‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñ (KG) ÁµêÊßã‰æÜË°®Á§∫Ë≥áË®äÁöÑÂπªË¶∫Ë©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑÊäÄË°ìË≠òÂà•Âá∫ÂÆπÊòìÂá∫ÁèæÂπªË¶∫ÁöÑ KG ‰∏≠ÁâπÂÆö‰∏âÂÖÉÁµÑÔºåÂõ†Ê≠§ÊØî‰ª•ÂæÄÁöÑÊñπÊ≥ïÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂõûÊáâ‰∏≠ÂπªË¶∫ÁôºÁîüÂú®Âì™Ë£°ÔºàÂ¶ÇÊûúÊúâÁöÑË©±Ôºâ„ÄÇÊ≠§Â§ñÔºåÂ∞áÊàëÂÄëÁöÑÊñπÊ≥ïËàáÊúÄÂÖàÈÄ≤ÁöÑËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁµêÂêà‰ΩøÁî®ÔºåËàá‰ΩøÁî®ÂéüÂßã NLI Ê®°ÂûãÁõ∏ÊØîÔºåÂèØ‰ª•Âú®ÂêÑÁ®ÆÂπªË¶∫Âü∫Ê∫ñ‰∏äÊèêÈ´òÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Á¥¢‰ΩøÁî® GraphEval ‰æÜÈÄ≤Ë°åÂπªË¶∫‰øÆÊ≠£ÔºåÊñπÊ≥ïÊòØÂà©Áî® KG ÁöÑÁµêÊßãÔºåÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÂëΩÂêçÁÇ∫ GraphCorrectÔºå‰∏¶Ë≠âÊòéÂ§ßÂ§öÊï∏ÂπªË¶∫Á¢∫ÂØ¶ÂèØ‰ª•ÂæóÂà∞Á≥æÊ≠£„ÄÇ

##### **Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**
2407.10743v1 by W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil

This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

ÊëòË¶ÅÔºöÊú¨ÊñáË®éË´ñ‰∫ÜÂ∞áÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) Êì¥Â±ïÂà∞Âª£Èóä 3D Áí∞Â¢ÉÁöÑÊåëÊà∞„ÄÇËß£Ê±∫ÈÄôÂÄãÈñãÊîæÊÄßÂïèÈ°åÂ∞çÊñºÊ©üÂô®‰∫∫Âú®Ë®±Â§öÁ¨¨‰∏ÄÂèçÊáâ‰∫∫Âì°Â†¥ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÁâπÂà•Áõ∏ÈóúÔºå‰æãÂ¶ÇÊ∂µËìãÂª£ÈóäÁ©∫ÈñìÁöÑÊêúÊïë‰ªªÂãô„ÄÇÈÄô‰∫õË®≠ÂÆö‰∏≠‰ΩøÁî® LMM ÁõÆÂâçÂèóÂà∞Âö¥Ê†ºÁöÑ‰∏ä‰∏ãÊñáË¶ñÁ™óÈôêÂà∂ÔºåÈÄôÈôêÂà∂‰∫Ü LMM ÁöÑËº∏ÂÖ•Â§ßÂ∞è„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®Ë≥áÊñôÂúñÁµêÊßãÔºåÂÖÅË®± LMM Ëø≠‰ª£Êü•Ë©¢Â§ßÂûãÁí∞Â¢ÉÁöÑËºÉÂ∞èÈÉ®ÂàÜ„ÄÇÈÄèÈÅéÂ∞áË≥áÊñôÂúñËàáÂúñÂΩ¢ÈÅçÊ≠∑ÊºîÁÆóÊ≥ïÁµêÂêà‰ΩøÁî®ÔºåÊàëÂÄëÂèØ‰ª•ÂÑ™ÂÖàËÄÉÊÖÆËàáÊü•Ë©¢ÊúÄÁõ∏ÈóúÁöÑ‰ΩçÁΩÆÔºåÂæûËÄåÊèêÈ´ò 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÊàëÂÄë‰ΩøÁî® 3D Â†¥ÊôØË™™ÊòéË≥áÊñôÂúñÔºå‰ΩÜÈÄô‰∫õÂ†¥ÊôØÂèØ‰ª•ËºïÈ¨ÜÂú∞Áî±ÂÖ∂‰ªñË°®Á§∫Áí∞Â¢ÉÁöÑÂØÜÈõÜÊ®°ÂºèÂèñ‰ª£Ôºå‰æãÂ¶ÇÈªûÈõ≤ÊàñÈ´òÊñØÈªû„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂú®ÊêúÊïë‰ªªÂãôÁØÑ‰æã‰∏≠‰ΩøÁî®Ë≥áÊñôÂúñÈÄ≤Ë°åÂÖ©ÂÄã 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁî®‰æãÁöÑÊΩõÂäõ„ÄÇ

##### **AutoGRAMS: Autonomous Graphical Agent Modeling Software**
2407.10049v1 by Ben Krause, Lucia Chen, Emmanuel Kahembwe

We introduce the AutoGRAMS framework for programming multi-step interactions
with language models. AutoGRAMS represents AI agents as a graph, where each
node can execute either a language modeling instruction or traditional code.
Likewise, transitions in the graph can be governed by either language modeling
decisions or traditional branch logic. AutoGRAMS supports using variables as
memory and allows nodes to call other AutoGRAMS graphs as functions. We show
how AutoGRAMS can be used to design highly sophisticated agents, including
self-referential agents that can modify their own graph. AutoGRAMS's
graph-centric approach aids interpretability, controllability, and safety
during the design, development, and deployment of AI agents. We provide our
framework as open source at https://github.com/autograms/autograms .

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π AutoGRAMS Ê°ÜÊû∂ÔºåÁî®ÊñºÁ∑®ÂØ´ËàáË™ûË®ÄÊ®°ÂûãÁöÑÂ§öÊ≠•È©ü‰∫íÂãï„ÄÇAutoGRAMS Â∞á AI ‰ª£ÁêÜË°®Á§∫ÁÇ∫‰∏ÄÂÄãÂúñÂΩ¢ÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÂèØ‰ª•Âü∑Ë°åË™ûË®ÄÂª∫Ê®°Êåá‰ª§ÊàñÂÇ≥Áµ±‰ª£Á¢º„ÄÇÂêåÊ®£Âú∞ÔºåÂúñÂΩ¢‰∏≠ÁöÑËΩâÊèõÂèØ‰ª•Áî±Ë™ûË®ÄÂª∫Ê®°Ê±∫Á≠ñÊàñÂÇ≥Áµ±ÂàÜÊîØÈÇèËºØÊéßÂà∂„ÄÇAutoGRAMS ÊîØÊè¥‰ΩøÁî®ËÆäÊï∏‰ΩúÁÇ∫Ë®òÊÜ∂È´îÔºå‰∏¶ÂÖÅË®±ÁØÄÈªûÂëºÂè´ÂÖ∂‰ªñ AutoGRAMS ÂúñÂΩ¢‰ΩúÁÇ∫ÂáΩÂºè„ÄÇÊàëÂÄëÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® AutoGRAMS Ë®≠Ë®àÈ´òÂ∫¶Ë§áÈõúÁöÑ‰ª£ÁêÜÔºåÂåÖÊã¨ÂèØ‰ª•‰øÆÊîπËá™Ë∫´ÂúñÂΩ¢ÁöÑËá™ÂèÉÁÖß‰ª£ÁêÜ„ÄÇAutoGRAMS ‰ª•ÂúñÂΩ¢ÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ïÊúâÂä©ÊñºÂú® AI ‰ª£ÁêÜÁöÑË®≠Ë®à„ÄÅÈñãÁôºÂíåÈÉ®ÁΩ≤ÈÅéÁ®ã‰∏≠ÊèêÈ´òÂèØËß£ÈáãÊÄß„ÄÅÂèØÊéßÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊàëÂÄëÂú® https://github.com/autograms/autograms Êèê‰æõÊàëÂÄëÁöÑÊ°ÜÊû∂‰ΩúÁÇ∫ÈñãÊ∫ê„ÄÇ

##### **FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**
2407.09888v1 by Dimitris Papadopoulos, Katerina Metropoulou, Nikolaos Matsatsinis, Nikolaos Papadakis

Our collective attention span is shortened by the flood of online
information. With \textit{FarFetched}, we address the need for automated claim
validation based on the aggregated evidence derived from multiple online news
sources. We introduce an entity-centric reasoning framework in which latent
connections between events, actions, or statements are revealed via entity
mentions and represented in a graph database. Using entity linking and semantic
similarity, we offer a way for collecting and combining information from
diverse sources in order to generate evidence relevant to the user's claim.
Then, we leverage textual entailment recognition to quantitatively determine
whether this assertion is credible, based on the created evidence. Our approach
tries to fill the gap in automated claim validation for less-resourced
languages and is showcased on the Greek language, complemented by the training
of relevant semantic textual similarity (STS) and natural language inference
(NLI) models that are evaluated on translated versions of common benchmarks.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØË≥áË®äÁöÑÊ¥™ÊµÅÁ∏ÆÁü≠‰∫ÜÊàëÂÄëÁöÑÈõÜÈ´îÊ≥®ÊÑèÂäõÊôÇÈñì„ÄÇÈÄèÈÅé \textit{FarFetched}ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÊ†πÊìöÂæûÂ§öÂÄãÁ∑ö‰∏äÊñ∞ËÅû‰æÜÊ∫êÂΩôÁ∏ΩÁöÑË≠âÊìöÈÄ≤Ë°åËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰ª•ÂØ¶È´îÁÇ∫‰∏≠ÂøÉÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠‰∫ã‰ª∂„ÄÅÂãï‰ΩúÊàñÈô≥Ëø∞‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØÈÄèÈÅéÂØ¶È´îÊèêÂèäË¢´Êè≠Èú≤Ôºå‰∏¶Âú®ÂúñÂΩ¢Ë≥áÊñôÂ∫´‰∏≠Ë°®Á§∫„ÄÇ‰ΩøÁî®ÂØ¶È´îÈÄ£ÁµêÂíåË™ûÁæ©Áõ∏‰ººÊÄßÔºåÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñπÂºè‰æÜÊî∂ÈõÜÂíåÁµÑÂêà‰æÜËá™‰∏çÂêå‰æÜÊ∫êÁöÑË≥áË®äÔºå‰ª•Áî¢ÁîüËàá‰ΩøÁî®ËÄÖËÅ≤ÊòéÁõ∏ÈóúÁöÑË≠âÊìö„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®ÊñáÊú¨ËòäÊ∂µË≠òÂà•‰æÜÊ†πÊìöÂª∫Á´ãÁöÑË≠âÊìöÈáèÂåñÁ¢∫ÂÆöÊ≠§Êñ∑Ë®ÄÊòØÂê¶ÂèØ‰ø°„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË©¶ÂúñÂ°´Ë£úË≥áÊ∫êËºÉÂ∞ëÁöÑË™ûË®ÄÁöÑËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÊñπÈù¢ÁöÑÁ©∫ÁôΩÔºå‰∏¶Âú®Â∏åËáòË™û‰∏≠Â±ïÁ§∫ÔºåËºî‰ª•Â∞çÁõ∏ÈóúË™ûÁæ©ÊñáÊú¨Áõ∏‰ººÊÄß (STS) ÂíåËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁöÑË®ìÁ∑¥ÔºåÈÄô‰∫õÊ®°ÂûãÂú®Â∏∏Ë¶ãÂü∫Ê∫ñÁöÑÁøªË≠ØÁâàÊú¨‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇ

##### **GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**
2407.09709v1 by Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang

Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÔºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊàñÂ§ßÂûãË¶ñË¶∫Ê®°Âûã (LVM)ÔºåÂ∑≤ÊàêÁÇ∫ÂêÑËá™È†òÂüü‰∏≠ÊúÄÊúâÂäõÁöÑÂ∑•ÂÖ∑‰πã‰∏Ä„ÄÇÁÑ∂ËÄåÔºåËàáÊñáÊú¨ÂíåÂΩ±ÂÉèË≥áÊñô‰∏çÂêåÔºåÂúñÂΩ¢Ë≥áÊñôÊ≤íÊúâÊòéÁ¢∫ÁöÑÁµêÊßãÔºåÂ∞çÈñãÁôºÂúñÂΩ¢Âü∫Á§éÊ®°Âûã (GFM) ÊßãÊàêÊ•µÂ§ßÁöÑÊåëÊà∞„ÄÇ‰æãÂ¶ÇÔºåÁõÆÂâçË®≠Ë®àÈÄöÁî®ÂúñÂΩ¢Ê®°ÂûãÁöÑÂòóË©¶Ôºå‰∏çÊòØÂ∞áÂúñÂΩ¢Ë≥áÊñôËΩâÊèõÁÇ∫Ë™ûË®ÄÊ†ºÂºè‰ª•‰æõÂü∫Êñº LLM ÁöÑÈ†êÊ∏¨ÔºåÂ∞±ÊòØË®ìÁ∑¥ GNN Ê®°ÂûãÔºå‰∏¶‰ª• LLM ‰ΩúÁÇ∫ËºîÂä©„ÄÇÂâçËÄÖÂèØ‰ª•ËôïÁêÜÁÑ°ÈôêÁöÑ‰ªªÂãôÔºåËÄåÂæåËÄÖÂèØ‰ª•Êõ¥Â•ΩÂú∞Êì∑ÂèñÂúñÂΩ¢ÁµêÊßãÔºå‰ΩÜÁèæÊúâÁöÑÂ∑•‰ΩúÁÑ°Ê≥ïÂêåÊôÇÈÅîÊàêÈÄôÂÖ©ËÄÖ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâæÂá∫ GFM ÁöÑ‰∏âÂÄãÈóúÈçµÁêÜÊÉ≥ÁâπÊÄßÔºöËá™ÊàëÁõ£Áù£È†êË®ìÁ∑¥„ÄÅ‰ªªÂãôÊµÅÊö¢Â∫¶ÂíåÂúñÂΩ¢ÊÑüÁü•„ÄÇÁÇ∫‰∫ÜËÄÉÈáèÈÄô‰∫õÁâπÊÄßÔºåÊàëÂÄëÂ∞áÂÇ≥Áµ±ÁöÑË™ûË®ÄÂª∫Ê®°Êì¥ÂÖÖÂà∞ÂúñÂΩ¢È†òÂüüÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÁîüÊàêÂºèÂúñÂΩ¢Ë™ûË®ÄÊ®°Âûã GOFA ‰æÜËß£Ê±∫ÂïèÈ°å„ÄÇÊ≠§Ê®°ÂûãÂ∞áÈö®Ê©üÂàùÂßãÂåñÁöÑ GNN Â±§‰∫§ÈåØÊèíÂÖ•ÂáçÁµêÁöÑÈ†êË®ìÁ∑¥ LLM ‰∏≠Ôºå‰ª•‰æøË™ûÊÑèÂíåÁµêÊßãÂª∫Ê®°ËÉΩÂäõÊúâÊ©üÁµêÂêà„ÄÇGOFA Êé°Áî®Êñ∞ÊèêÂá∫ÁöÑÂúñÂΩ¢Â±§Á¥ö‰∏ã‰∏ÄÂÄãÂ≠óÈ†êÊ∏¨„ÄÅÂïèÁ≠îÂíåÁµêÊßã‰ªªÂãôÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•ÂèñÂæó‰∏äËø∞ GFM ÁâπÊÄß„ÄÇÈ†êË®ìÁ∑¥Ê®°ÂûãÈÄ≤‰∏ÄÊ≠•Âú®‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åÂæÆË™øÔºå‰ª•ÂèñÂæóËß£Ê±∫‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÂæÆË™øÊ®°ÂûãÂú®ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåË≠âÊòé‰∫ÜÂú®Èõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠Ëß£Ê±∫ÁµêÊßãÂíå‰∏ä‰∏ãÊñáÂïèÈ°åÁöÑÂº∑Â§ßËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/JiaruiFeng/GOFA ÂèñÂæó„ÄÇ

##### **Human-like Episodic Memory for Infinite Context LLMs**
2407.09450v1 by Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang

Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs,
enabling them to effectively handle practically infinite context lengths while
maintaining computational efficiency. EM-LLM organises sequences of tokens into
coherent episodic events using a combination of Bayesian surprise and
graph-theoretic boundary refinement in an on-line fashion. When needed, these
events are retrieved through a two-stage memory process, combining
similarity-based and temporally contiguous retrieval for efficient and
human-like access to relevant information. Experiments on the LongBench dataset
demonstrate EM-LLM's superior performance, outperforming the state-of-the-art
InfLLM model with an overall relative improvement of 4.3% across various tasks,
including a 33% improvement on the PassageRetrieval task. Furthermore, our
analysis reveals strong correlations between EM-LLM's event segmentation and
human-perceived events, suggesting a bridge between this artificial system and
its biological counterpart. This work not only advances LLM capabilities in
processing extended contexts but also provides a computational framework for
exploring human memory mechanisms, opening new avenues for interdisciplinary
research in AI and cognitive science.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõÔºå‰ΩÜ‰ªçÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑËÑàÁµ°ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èï∑Â∫èÂàó‰∏≠Á∂≠ÊåÅÈÄ£Ë≤´ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁöÑËÉΩÂäõ„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºå‰∫∫ËÖ¶ÊìÖÈï∑Âú®Âª£Â§ßÁöÑÊôÇÈñìÂ∞∫Â∫¶‰∏äÁµÑÁπîÂíåÊèêÂèñÊÉÖÁØÄÈ´îÈ©óÔºåË∑®Ë∂ä‰∏ÄÁîü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü EM-LLMÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞á‰∫∫È°ûÊÉÖÁØÄË®òÊÜ∂Âíå‰∫ã‰ª∂Ë™çÁü•ÁöÑÈóúÈçµÈù¢ÂêëÊï¥ÂêàÂà∞ LLM ‰∏≠ÔºåËÆìÂÆÉÂÄëËÉΩÂ§†ÊúâÊïàÂú∞ËôïÁêÜÂØ¶Èöõ‰∏äÁÑ°ÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ÔºåÂêåÊôÇÁ∂≠ÊåÅÈÅãÁÆóÊïàÁéá„ÄÇEM-LLM ‰ΩøÁî®Ë≤ùÊ∞èÈ©öÂñúÂíåÂúñË´ñÈÇäÁïåÁ≤æÁÖâÁöÑÁµÑÂêàÔºå‰ª•Á∑ö‰∏äÊñπÂºèÂ∞áÂ∫èÂàóÊ®ôË®òÁµÑÁπîÊàêÈÄ£Ë≤´ÁöÑÊÉÖÁØÄ‰∫ã‰ª∂„ÄÇÂú®ÈúÄË¶ÅÊôÇÔºåÈÄô‰∫õ‰∫ã‰ª∂ÊúÉÈÄèÈÅéÂÖ©ÈöéÊÆµÁöÑË®òÊÜ∂ÈÅéÁ®ã‰æÜÊèêÂèñÔºåÁµêÂêàÂü∫ÊñºÁõ∏‰ººÊÄßÂíåÊôÇÈñìÈÄ£Á∫åÊÄßÁöÑÊèêÂèñÔºå‰ª•ÊúâÊïà‰∏îÈ°û‰ºº‰∫∫È°ûÁöÑÊñπÂºèÂ≠òÂèñÁõ∏ÈóúË≥áË®ä„ÄÇÂú® LongBench Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË≠âÊòé‰∫Ü EM-LLM ÁöÑÂçìË∂äÊïàËÉΩÔºåÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ InfLLM Ê®°ÂûãÔºåÂú® PassageRetrieval ‰ªªÂãô‰∏≠ÊîπÈÄ≤‰∫Ü 33%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü EM-LLM ÁöÑ‰∫ã‰ª∂ÂàÜÂâ≤Ëàá‰∫∫È°ûÊÑüÁü•‰∫ã‰ª∂‰πãÈñìÁöÑÂº∑Áõ∏ÈóúÊÄßÔºåÈ°ØÁ§∫‰∫ÜÈÄôÂÄã‰∫∫Â∑•Á≥ªÁµ±ËàáÂÖ∂ÁîüÁâ©Â∞çÊáâÁâ©‰πãÈñìÁöÑÊ©ãÊ®ë„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰∏çÂÉÖÊèêÂçá‰∫Ü LLM Âú®ËôïÁêÜÂª∂‰º∏ËÑàÁµ°ÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰πüÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈÅãÁÆóÊû∂Êßã‰æÜÊé¢Á¥¢‰∫∫È°ûË®òÊÜ∂Ê©üÂà∂ÔºåÁÇ∫ AI ÂíåË™çÁü•ÁßëÂ≠∏ÁöÑË∑®È†òÂüüÁ†îÁ©∂ÈñãÂïü‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇ

##### **The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**
2407.09441v1 by Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti

Graph neural networks form a class of deep learning architectures
specifically designed to work with graph-structured data. As such, they share
the inherent limitations and problems of deep learning, especially regarding
the issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,
an original domain-specific language for the specification of graph neural
networks that aims to overcome these issues. The language's syntax is
introduced, and its meaning is rigorously defined by a denotational semantics.
An equivalent characterization in the form of an operational semantics is also
provided and, together with a type system, is used to prove the type soundness
of $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be represented
in a more user-friendly graphical visualization, and provide examples of its
generality by showing how it can be used to define some of the most popular
graph neural network models, or to develop any custom graph processing
application.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂΩ¢Êàê‰∏ÄÈ°ûÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÁâπÂà•Ë®≠Ë®àÁî®ÊñºËôïÁêÜÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÂÆÉÂÄëÂÖ∑ÊúâÊ∑±Â∫¶Â≠∏ÁøíÂõ∫ÊúâÁöÑÈôêÂà∂ÂíåÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®ÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÂïèÈ°å‰∏ä„ÄÇÊàëÂÄëÊèêÂá∫ $\mu\mathcal{G}$Ôºå‰∏ÄÁ®ÆÁî®ÊñºÊåáÂÆöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂéüÂâµÈ†òÂüüÁâπÂÆöË™ûË®ÄÔºåÊó®Âú®ÂÖãÊúçÈÄô‰∫õÂïèÈ°å„ÄÇÂºïÂÖ•‰∫ÜË™ûË®ÄÁöÑË™ûÊ≥ïÔºå‰∏¶ÈÄèÈÅéÊåáÁ§∫Ë™ûÁæ©Âö¥Ê†ºÂÆöÁæ©ÂÖ∂Âê´Áæ©„ÄÇÈÇÑÊèê‰æõ‰∫ÜÈÅãÁÆóË™ûÁæ©ÂΩ¢ÂºèÁöÑÁ≠âÊïàÁâπÂæµÊèèËø∞Ôºå‰∏¶ËàáÈ°ûÂûãÁ≥ªÁµ±‰∏ÄËµ∑Áî®ÊñºË≠âÊòé $\mu\mathcal{G}$ ÁöÑÈ°ûÂûãÂÅ•ÂÖ®ÊÄß„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á $\mu\mathcal{G}$ Á®ãÂºèË°®Á§∫ÁÇ∫Êõ¥ÂèãÂñÑÁöÑÂúñÂΩ¢Ë¶ñË¶∫ÂåñÔºå‰∏¶ÈÄèÈÅéÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî®ÂÆÉÂÆöÁæ©‰∏Ä‰∫õÊúÄÊµÅË°åÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÊàñÈñãÁôº‰ªª‰ΩïËá™Ë®ÇÂúñÂΩ¢ËôïÁêÜÊáâÁî®Á®ãÂºèÔºå‰æÜÊèê‰æõÂÖ∂ÈÄöÁî®ÊÄßÁöÑÁØÑ‰æã„ÄÇ

##### **Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**
2407.08983v1 by David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk

Trustworthiness and interpretability are inextricably linked concepts for
LLMs. The more interpretable an LLM is, the more trustworthy it becomes.
However, current techniques for interpreting LLMs when applied to code-related
tasks largely focus on accuracy measurements, measures of how models react to
change, or individual task performance instead of the fine-grained explanations
needed at prediction time for greater interpretability, and hence trust. To
improve upon this status quo, this paper introduces ASTrust, an
interpretability method for LLMs of code that generates explanations grounded
in the relationship between model confidence and syntactic structures of
programming languages. ASTrust explains generated code in the context of syntax
categories based on Abstract Syntax Trees and aids practitioners in
understanding model predictions at both local (individual code snippets) and
global (larger datasets of code) levels. By distributing and assigning model
confidence scores to well-known syntactic structures that exist within ASTs,
our approach moves beyond prior techniques that perform token-level confidence
mapping by offering a view of model confidence that directly aligns with
programming language concepts with which developers are familiar. To put
ASTrust into practice, we developed an automated visualization that illustrates
the aggregated model confidence scores superimposed on sequence, heat-map, and
graph-based visuals of syntactic structures from ASTs. We examine both the
practical benefit that ASTrust can provide through a data science study on 12
popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust
through a human study.

ÊëòË¶ÅÔºöÂèØ‰ø°Â∫¶ÂíåÂèØËß£ÈáãÊÄßÊòØ LLM ‰∏≠ÂØÜ‰∏çÂèØÂàÜÁöÑÊ¶ÇÂøµ„ÄÇLLM ÁöÑÂèØËß£ÈáãÊÄßË∂äÈ´òÔºåÂÆÉÁöÑÂèØ‰ø°Â∫¶Â∞±Ë∂äÈ´ò„ÄÇÁÑ∂ËÄåÔºåÁï∂ÊáâÁî®ÊñºËàáÁ®ãÂºèÁ¢ºÁõ∏ÈóúÁöÑ‰ªªÂãôÊôÇÔºåÁõÆÂâçËß£Èáã LLM ÁöÑÊäÄË°ì‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê∫ñÁ¢∫ÊÄßÊ∏¨Èáè„ÄÅÊ®°ÂûãÂ∞çËÆäÂåñÁöÑÂèçÊáâÊ∏¨ÈáèÊàñÂÄãÂà•‰ªªÂãôË°®ÁèæÔºåËÄå‰∏çÊòØÂú®È†êÊ∏¨ÊôÇÈñìÊâÄÈúÄÁöÑÁ¥∞Á≤íÂ∫¶Ëß£ÈáãÔºåÂæûËÄåÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂõ†Ê≠§ÊèêÈ´ò‰ø°‰ªªÂ∫¶„ÄÇÁÇ∫‰∫ÜÊîπÂñÑÈÄôÁ®ÆÁèæÁãÄÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü ASTrustÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÁ®ãÂºèÁ¢º LLM ÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÔºåÂÆÉÊúÉÊ†πÊìöÊ®°Âûã‰ø°ÂøÉËàáÁ®ãÂºèË™ûË®ÄÁöÑË™ûÊ≥ïÁµêÊßã‰πãÈñìÁöÑÈóú‰øÇÁî¢ÁîüËß£Èáã„ÄÇASTrust Âú®Âü∫ÊñºÊäΩË±°Ë™ûÊ≥ïÊ®πÁöÑË™ûÊ≥ïÈ°ûÂà•ÁöÑ‰∏ä‰∏ãÊñá‰∏≠Ëß£ÈáãÁî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÔºå‰∏¶Âπ´Âä©ÂØ¶Âãô‰∫∫Âì°Âú®Â±ÄÈÉ®ÔºàÂÄãÂà•Á®ãÂºèÁ¢ºÁâáÊÆµÔºâÂíåÂÖ®ÂüüÔºàËºÉÂ§ßÁöÑÁ®ãÂºèÁ¢ºË≥áÊñôÈõÜÔºâÂ±§Á¥ö‰∫ÜËß£Ê®°ÂûãÈ†êÊ∏¨„ÄÇÈÄèÈÅéÂ∞áÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏ÂàÜÈÖçÂíåÊåáÂÆöÁµ¶ AST ‰∏≠Â≠òÂú®ÁöÑÁúæÊâÄÂë®Áü•ÁöÑË™ûÊ≥ïÁµêÊßãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïË∂ÖË∂ä‰∫ÜÂÖàÂâçÁöÑÊäÄË°ìÔºåÈÄô‰∫õÊäÄË°ìÈÄèÈÅéÊèê‰æõËàáÈñãÁôº‰∫∫Âì°ÁÜüÊÇâÁöÑÁ®ãÂºèË™ûË®ÄÊ¶ÇÂøµÁõ¥Êé•Â∞çÈΩäÁöÑÊ®°Âûã‰ø°ÂøÉË¶ñÂúñ‰æÜÂü∑Ë°å‰ª§ÁâåÁ¥öÂà•ÁöÑ‰ø°ÂøÉÂ∞çÊáâ„ÄÇÁÇ∫‰∫ÜÂØ¶Ë∏ê ASTrustÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñË¶ñË¶∫ÂåñÂ∑•ÂÖ∑ÔºåÂÆÉË™™Êòé‰∫ÜÁñäÂä†Âú® AST Ë™ûÊ≥ïÁµêÊßãÁöÑÂ∫èÂàó„ÄÅÁÜ±ÂúñÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑË¶ñË¶∫ÊïàÊûú‰∏äÁöÑËÅöÂêàÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏„ÄÇÊàëÂÄëÊ™¢Êü•‰∫Ü ASTrust ÂèØ‰ª•ÈÄèÈÅéÂ∞ç 12 ÂÄãÊµÅË°åÁöÑ LLM Âú®‰∏ÄÁµÑÁ≤æÈÅ∏ÁöÑ GitHub ÂÑ≤Â≠òÂ∫´‰∏äÈÄ≤Ë°åË≥áÊñôÁßëÂ≠∏Á†îÁ©∂Êèê‰æõÁöÑÂØ¶ÈöõÂ•ΩËôïÔºå‰ª•ÂèäÈÄèÈÅé‰∫∫È´îÁ†îÁ©∂Êèê‰æõÁöÑ ASTrust ÁöÑÊúâÁî®ÊÄß„ÄÇ

##### **Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**
2407.08959v1 by Ke Ji, Peng Wang, Wenjun Ke, Guozheng Li, Jiajun Liu, Jingsheng Gao, Ziyu Shang

Recently, various pre-trained language models (PLMs) have been proposed to
prove their impressive performances on a wide range of few-shot tasks. However,
limited by the unstructured prior knowledge in PLMs, it is difficult to
maintain consistent performance on complex structured scenarios, such as
hierarchical text classification (HTC), especially when the downstream data is
extremely scarce. The main challenge is how to transfer the unstructured
semantic space in PLMs to the downstream domain hierarchy. Unlike previous work
on HTC which directly performs multi-label classification or uses graph neural
network (GNN) to inject label hierarchy, in this work, we study the HTC problem
under a few-shot setting to adapt knowledge in PLMs from an unstructured manner
to the downstream hierarchy. Technically, we design a simple yet effective
method named Hierarchical Iterative Conditional Random Field (HierICRF) to
search the most domain-challenging directions and exquisitely crafts
domain-hierarchy adaptation as a hierarchical iterative language modeling
problem, and then it encourages the model to make hierarchical consistency
self-correction during the inference, thereby achieving knowledge transfer with
hierarchical consistency preservation. We perform HierICRF on various
architectures, and extensive experiments on two popular HTC datasets
demonstrate that prompt with HierICRF significantly boosts the few-shot HTC
performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%
to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot
settings, while remaining SOTA hierarchical consistency performance.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂ∑≤ÁªèÊèêÂá∫‰∫ÜÂ§öÁßçÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã (PLM)Ôºå‰ª•ËØÅÊòéÂÆÉ‰ª¨Âú®ÂπøÊ≥õÁöÑÂ∞ëÈáèÊ†∑Êú¨‰ªªÂä°‰∏äÂÖ∑Êúâ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑÂÖàÈ™åÁü•ËØÜÂèóÂà∞ÈôêÂà∂ÔºåÂõ†Ê≠§Èöæ‰ª•Âú®Â§çÊùÇÁªìÊûÑÂåñÂú∫ÊôØÔºà‰æãÂ¶ÇÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª (HTC)Ôºâ‰∏≠‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏ãÊ∏∏Êï∞ÊçÆÊûÅÂÖ∂Á®ÄÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊàòÊòØÂ¶Ç‰ΩïÂ∞Ü PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑËØ≠‰πâÁ©∫Èó¥ËΩ¨ÁßªÂà∞‰∏ãÊ∏∏ÂüüÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰∏é‰ª•ÂâçÁõ¥Êé•ÊâßË°åÂ§öÊ†áÁ≠æÂàÜÁ±ªÊàñ‰ΩøÁî®ÂõæÁ•ûÁªèÁΩëÁªú (GNN) Ê≥®ÂÖ•Ê†áÁ≠æÂ±ÇÊ¨°ÁªìÊûÑÁöÑ HTC Â∑•‰Ωú‰∏çÂêåÔºåÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãÁ†îÁ©∂ HTC ÈóÆÈ¢òÔºå‰ª•Â∞Ü PLM ‰∏≠ÁöÑÁü•ËØÜ‰ªéÈùûÁªìÊûÑÂåñÊñπÂºèÈÄÇÂ∫îÂà∞‰∏ãÊ∏∏Â±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰ªéÊäÄÊúØ‰∏äËÆ≤ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Â±ÇÊ¨°Ëø≠‰ª£Êù°‰ª∂ÈöèÊú∫Âú∫ (HierICRF)Ôºå‰ª•ÊêúÁ¥¢ÊúÄÂÖ∑È¢ÜÂüüÊåëÊàòÊÄßÁöÑÊñπÂêëÔºåÂπ∂Á≤æÁªÜÂú∞Â∞ÜÈ¢ÜÂüüÂ±ÇÊ¨°ÁªìÊûÑÈÄÇÂ∫î‰Ωú‰∏∫ÂàÜÂ±ÇËø≠‰ª£ËØ≠Ë®ÄÂª∫Ê®°ÈóÆÈ¢òÔºåÁÑ∂ÂêéÂÆÉÈºìÂä±Ê®°ÂûãÂú®Êé®ÁêÜÊúüÈó¥ËøõË°åÂ±ÇÊ¨°‰∏ÄËá¥ÊÄßËá™ÊàëÊ†°Ê≠£Ôºå‰ªéËÄåÂÆûÁé∞ÂÖ∑ÊúâÂ±ÇÊ¨°‰∏ÄËá¥ÊÄß‰øùÁïôÁöÑÁü•ËØÜËΩ¨Áßª„ÄÇÊàë‰ª¨Âú®ÂêÑÁßçÊû∂ÊûÑ‰∏äÊâßË°å HierICRFÔºåÂú®‰∏§‰∏™ÊµÅË°åÁöÑ HTC Êï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî® HierICRF ÁöÑÊèêÁ§∫ÊòæÁùÄÊèêÈ´ò‰∫ÜÂ∞ëÈáèÊ†∑Êú¨ HTC ÊÄßËÉΩÔºåÂπ≥Âùá Micro-F1 ‰ªé 28.80% ÊèêÈ´òÂà∞ 1.50%ÔºåMacro-F1 ‰ªé 36.29% ÊèêÈ´òÂà∞ 1.5% Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãË∂ÖËøá‰∫Ü‰ª•ÂâçÊúÄÂÖàËøõ (SOTA) Âü∫ÂáÜÔºåÂêåÊó∂‰øùÊåÅ SOTA Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊÄßËÉΩ„ÄÇ</paragraph>

##### **Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**
2407.08694v1 by Zhiqiang Xie, Yujia Zheng, Lizi Ottens, Kun Zhang, Christos Kozyrakis, Jonathan Mace

Runtime failure and performance degradation is commonplace in modern cloud
systems. For cloud providers, automatically determining the root cause of
incidents is paramount to ensuring high reliability and availability as prompt
fault localization can enable faster diagnosis and triage for timely
resolution. A compelling solution explored in recent work is causal reasoning
using causal graphs to capture relationships between varied cloud system
performance metrics. To be effective, however, systems developers must
correctly define the causal graph of their system, which is a time-consuming,
brittle, and challenging task that increases in difficulty for large and
dynamic systems and requires domain expertise. Alternatively, automated
data-driven approaches have limited efficacy for cloud systems due to the
inherent rarity of incidents. In this work, we present Atlas, a novel approach
to automatically synthesizing causal graphs for cloud systems. Atlas leverages
large language models (LLMs) to generate causal graphs using system
documentation, telemetry, and deployment feedback. Atlas is complementary to
data-driven causal discovery techniques, and we further enhance Atlas with a
data-driven validation step. We evaluate Atlas across a range of fault
localization scenarios and demonstrate that Atlas is capable of generating
causal graphs in a scalable and generalizable manner, with performance that far
surpasses that of data-driven algorithms and is commensurate to the
ground-truth baseline.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£Èõ≤Á´ØÁ≥ªÁµ±‰∏≠ÔºåÂü∑Ë°åÊôÇÊúüÊïÖÈöúÂíåÊïàËÉΩÈôç‰ΩéÊòØÂè∏Á©∫Ë¶ãÊÖ£ÁöÑ‰∫ã„ÄÇÂ∞çÊñºÈõ≤Á´Ø‰æõÊáâÂïÜËÄåË®ÄÔºåËá™ÂãïÊâæÂá∫‰∫ã‰ª∂ÁöÑÊ†πÊú¨ÂéüÂõ†Â∞çÊñºÁ¢∫‰øùÈ´òÂèØÈù†ÊÄßÂíåÂèØÁî®ÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂèäÊôÇÁöÑÊïÖÈöúÂÆö‰ΩçÂèØ‰ª•ËÆìË®∫Êñ∑ÂíåÂàÜÈ°ûÊõ¥Âø´ÈÄüÔºå‰ª•Âà©ÊñºÂèäÊôÇËß£Ê±∫ÂïèÈ°å„ÄÇÊúÄËøëÁöÑÂ∑•‰Ωú‰∏≠Êé¢Ë®é‰∫Ü‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂç≥‰ΩøÁî®Âõ†ÊûúÂúñ‰æÜÊì∑ÂèñÂêÑÁ®ÆÈõ≤Á´ØÁ≥ªÁµ±ÊïàËÉΩÊåáÊ®ô‰πãÈñìÈóú‰øÇÁöÑÂõ†ÊûúÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÁ≥ªÁµ±ÈñãÁôº‰∫∫Âì°ÂøÖÈ†àÊ≠£Á¢∫ÂÆöÁæ©ÂÖ∂Á≥ªÁµ±ÁöÑÂõ†ÊûúÂúñÊâçËÉΩÁôºÊèÆÊïàÁî®ÔºåËÄåÈÄôÈ†Ö‰ªªÂãôËÄóÊôÇ„ÄÅËÑÜÂº±‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂ∞çÊñºÂ§ßÂûã‰∏îÂãïÊÖãÁöÑÁ≥ªÁµ±ËÄåË®ÄÈõ£Â∫¶Êõ¥È´òÔºåËÄå‰∏îÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂Áü•Ë≠ò„ÄÇÊàñËÄÖÔºåÁî±Êñº‰∫ã‰ª∂ÁöÑÂõ∫ÊúâÁ®ÄÂ∞ëÊÄßÔºåËá™ÂãïÂåñË≥áÊñôÈ©ÖÂãïÊñπÊ≥ïÂ∞çÊñºÈõ≤Á´ØÁ≥ªÁµ±ÁöÑÊïàÂäõÊúâÈôê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ AtlasÔºå‰∏ÄÁ®ÆËá™ÂãïÂêàÊàêÈõ≤Á´ØÁ≥ªÁµ±Âõ†ÊûúÂúñÁöÑÊñ∞ÊñπÊ≥ï„ÄÇAtlas Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®Á≥ªÁµ±Êñá‰ª∂„ÄÅÈÅôÊ∏¨ÂíåÈÉ®ÁΩ≤ÂõûÈ•ã‰æÜÁî¢ÁîüÂõ†ÊûúÂúñ„ÄÇAtlas ÊòØË≥áÊñôÈ©ÖÂãïÂõ†ÊûúÁôºÁèæÊäÄË°ìÁöÑË£úÂÖÖÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®Ë≥áÊñôÈ©ÖÂãïÈ©óË≠âÊ≠•È©ü‰æÜÂ¢ûÂº∑ Atlas„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÊïÖÈöúÂÆö‰ΩçÊÉÖÂ¢É‰∏≠Ë©ï‰º∞ AtlasÔºå‰∏¶Ë≠âÊòé Atlas ËÉΩÂ§†‰ª•ÂèØÊì¥ÂÖÖ‰∏îÂèØÊ¶ÇÂåñÁöÑÊñπÂºèÁî¢ÁîüÂõ†ÊûúÂúñÔºåÂÖ∂ÊïàËÉΩÈÅ†ÈÅ†Ë∂ÖÈÅéË≥áÊñôÈ©ÖÂãïÊºîÁÆóÊ≥ïÔºå‰∏¶‰∏îËàáÁúüÂØ¶Âü∫Á∑öÁõ∏Áï∂„ÄÇ

##### **Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**
2407.08516v3 by Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, Laura E. Barnes

This article explores the convergence of connectionist and symbolic
artificial intelligence (AI), from historical debates to contemporary
advancements. Traditionally considered distinct paradigms, connectionist AI
focuses on neural networks, while symbolic AI emphasizes symbolic
representation and logic. Recent advancements in large language models (LLMs),
exemplified by ChatGPT and GPT-4, highlight the potential of connectionist
architectures in handling human language as a form of symbols. The study argues
that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.
By utilizing LLMs for text-based knowledge modeling and representation, LAAs
integrate neuro-symbolic AI principles, showcasing enhanced reasoning and
decision-making capabilities. Comparing LAAs with Knowledge Graphs within the
neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking
human-like reasoning processes, scaling effectively with large datasets, and
leveraging in-context samples without explicit re-training. The research
underscores promising avenues in neuro-vector-symbolic integration,
instructional encoding, and implicit reasoning, aimed at further enhancing LAA
capabilities. By exploring the progression of neuro-symbolic AI and proposing
future research trajectories, this work advances the understanding and
development of AI technologies.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÈÄ£Á∑ö‰∏ªÁæ©ËàáÁ¨¶Ëôü‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑËûçÂêàÔºåÂæûÊ≠∑Âè≤ËæØË´ñÂà∞Áï∂‰ª£ÈÄ≤Â±ï„ÄÇÈÄ£Á∑ö‰∏ªÁæ© AI ÂÇ≥Áµ±‰∏äË¢´Ë¶ñÁÇ∫‰∏çÂêåÁöÑÁØÑ‰æãÔºåÂ∞àÊ≥®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåËÄåÁ¨¶Ëôü AI ÂâáÂº∑Ë™øÁ¨¶ËôüË°®ÂæµÂíåÈÇèËºØ„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰ª• ChatGPT Âíå GPT-4 ÁÇ∫‰æãÔºåÁ™ÅÈ°Ø‰∫ÜÈÄ£Á∑ö‰∏ªÁæ©Êû∂ÊßãÂú®Â∞á‰∫∫È°ûË™ûË®ÄË¶ñÁÇ∫Á¨¶ËôüÂΩ¢ÂºèËôïÁêÜÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁ†îÁ©∂Ë™çÁÇ∫ÔºåÁî± LLM Ë≥¶ËÉΩÁöÑËá™‰∏ª‰ª£ÁêÜ (LAA) È´îÁèæ‰∫ÜÈÄôÁ®ÆÁØÑ‰æãËûçÂêà„ÄÇÈÄèÈÅéÂà©Áî® LLM ÈÄ≤Ë°åÂü∫ÊñºÊñáÂ≠óÁöÑÁü•Ë≠òÂª∫Ê®°ÂíåË°®ÂæµÔºåLAA Êï¥Âêà‰∫ÜÁ•ûÁ∂ìÁ¨¶Ëôü AI ÂéüÂâáÔºåÂ±ïÁ§∫‰∫ÜÂ¢ûÂº∑ÁöÑÊé®ÁêÜÂíåÊ±∫Á≠ñËÉΩÂäõ„ÄÇÂú®Á•ûÁ∂ìÁ¨¶Ëôü AI ‰∏ªÈ°å‰∏≠ÊØîËºÉ LAA ËàáÁü•Ë≠òÂúñË≠úÔºåÁ™ÅÈ°Ø‰∫Ü LAA Âú®Ê®°Êì¨È°û‰∫∫Êé®ÁêÜÈÅéÁ®ã„ÄÅÊúâÊïàÊì¥ÂÖÖÂ§ßÂûãË≥áÊñôÈõÜ‰ª•ÂèäÂà©Áî®ÊÉÖÂ¢ÉÁØÑ‰æãËÄåÁÑ°ÈúÄÊòéÁ¢∫ÈáçÊñ∞Ë®ìÁ∑¥ÊñπÈù¢ÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÁ•ûÁ∂ìÂêëÈáèÁ¨¶ËôüÊï¥Âêà„ÄÅÊåá‰ª§Á∑®Á¢ºÂíåÈö±ÂºèÊé®ÁêÜ‰∏≠ÂâçÊôØÁúãÂ•ΩÁöÑÈÄîÂæëÔºåÊó®Âú®ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑ LAA ÁöÑËÉΩÂäõ„ÄÇÈÄèÈÅéÊé¢Ë®éÁ•ûÁ∂ìÁ¨¶Ëôü AI ÁöÑÈÄ≤Â±ï‰∏¶ÊèêÂá∫Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊé®ÈÄ≤‰∫ÜÂ∞ç AI ÊäÄË°ìÁöÑÁêÜËß£ÂíåÁôºÂ±ï„ÄÇ

##### **A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**
2407.07966v1 by Arastoo Zibaeirad, Farnoosh Koleini, Shengping Bi, Tao Hou, Tao Wang

In this study, we conduct a comprehensive review of smart grid security,
exploring system architectures, attack methodologies, defense strategies, and
future research opportunities. We provide an in-depth analysis of various
attack vectors, focusing on new attack surfaces introduced by advanced
components in smart grids. The review particularly includes an extensive
analysis of coordinated attacks that incorporate multiple attack strategies and
exploit vulnerabilities across various smart grid components to increase their
adverse impact, demonstrating the complexity and potential severity of these
threats. Following this, we examine innovative detection and mitigation
strategies, including game theory, graph theory, blockchain, and machine
learning, discussing their advancements in counteracting evolving threats and
associated research challenges. In particular, our review covers a thorough
examination of widely used machine learning-based mitigation strategies,
analyzing their applications and research challenges spanning across
supervised, unsupervised, semi-supervised, ensemble, and reinforcement
learning. Further, we outline future research directions and explore new
techniques and concerns. We first discuss the research opportunities for
existing and emerging strategies, and then explore the potential role of new
techniques, such as large language models (LLMs), and the emerging threat of
adversarial machine learning in the future of smart grid security.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÊô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®ÊÄßÈÄ≤Ë°åÂÖ®Èù¢Ê™¢Ë¶ñÔºåÊé¢Ë®éÁ≥ªÁµ±Êû∂Êßã„ÄÅÊîªÊìäÊñπÊ≥ï„ÄÅÈò≤Á¶¶Á≠ñÁï•ÂíåÊú™‰æÜÁöÑÁ†îÁ©∂Ê©üÊúÉ„ÄÇÊàëÂÄëÊ∑±ÂÖ•ÂàÜÊûêÂêÑÁ®ÆÊîªÊìäÂ™í‰ªãÔºåÂ∞àÊ≥®ÊñºÊô∫ÊÖßÈõªÁ∂≤‰∏≠ÂÖàÈÄ≤ÁµÑ‰ª∂ÊâÄÂºïÂÖ•ÁöÑÊñ∞ÊîªÊìäÈù¢„ÄÇÊú¨Ê™¢Ë¶ñÁâπÂà•ÂåÖÂê´Â∞çÂçîË™øÊîªÊìäÁöÑÂª£Ê≥õÂàÜÊûêÔºåÂÖ∂‰∏≠ÂåÖÂê´Â§öÁ®ÆÊîªÊìäÁ≠ñÁï•‰∏¶Âà©Áî®ÂêÑÁ®ÆÊô∫ÊÖßÈõªÁ∂≤ÁµÑ‰ª∂‰∏≠ÁöÑÊºèÊ¥û‰æÜÂ¢ûÂä†ÂÖ∂Ë≤†Èù¢ÂΩ±ÈüøÔºåÂ±ïÁ§∫ÈÄô‰∫õÂ®ÅËÑÖÁöÑË§áÈõúÊÄßÂíåÊΩõÂú®Âö¥ÈáçÊÄß„ÄÇÂú®Ê≠§‰πãÂæåÔºåÊàëÂÄëÊé¢Ë®éÂâµÊñ∞ÁöÑÂÅµÊ∏¨ÂíåÁ∑©Ëß£Á≠ñÁï•ÔºåÂåÖÊã¨ÂçöÂºàË´ñ„ÄÅÂúñË´ñ„ÄÅÂçÄÂ°äÈèàÂíåÊ©üÂô®Â≠∏ÁøíÔºåË®éË´ñÂÆÉÂÄëÂú®Â∞çÊäó‰∏çÊñ∑ÊºîËÆäÁöÑÂ®ÅËÑÖÂíåÁõ∏ÈóúÁ†îÁ©∂ÊåëÊà∞ÊñπÈù¢ÁöÑÈÄ≤Â±ï„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊ™¢Ë¶ñÊ∂µËìãÂ∞çÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑÁ∑©Ëß£Á≠ñÁï•ÁöÑÂæπÂ∫ïÊ™¢È©óÔºåÂàÜÊûêÂÆÉÂÄëÂú®Áõ£Áù£Âºè„ÄÅÈùûÁõ£Áù£Âºè„ÄÅÂçäÁõ£Áù£Âºè„ÄÅÊï¥È´îÂºèÂíåÂº∑ÂåñÂ≠∏Áøí‰∏≠ÁöÑÊáâÁî®ÂíåÁ†îÁ©∂ÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ¶ÇËø∞Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë‰∏¶Êé¢Ë®éÊñ∞ÊäÄË°ìÂíåÂïèÈ°å„ÄÇÊàëÂÄëÈ¶ñÂÖàË®éË´ñÁèæÊúâÂíåÊñ∞ËààÁ≠ñÁï•ÁöÑÁ†îÁ©∂Ê©üÊúÉÔºåÁÑ∂ÂæåÊé¢Ë®éÊñ∞ÊäÄË°ìÁöÑÊΩõÂú®‰ΩúÁî®Ôºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•ÂèäÂ∞çÊäóÂºèÊ©üÂô®Â≠∏ÁøíÂú®Êô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®Êú™‰æÜÁöÑÂ®ÅËÑÖ„ÄÇ

##### **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**
2407.07775v2 by Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, Jie Tan

An elusive goal in navigation research is to build an intelligent agent that
can understand multimodal instructions including natural language and image,
and perform useful navigation. To achieve this, we study a widely useful
category of navigation tasks we call Multimodal Instruction Navigation with
demonstration Tours (MINT), in which the environment prior is provided through
a previously recorded demonstration video. Recent advances in Vision Language
Models (VLMs) have shown a promising path in achieving this goal as it
demonstrates capabilities in perceiving and reasoning about multimodal inputs.
However, VLMs are typically trained to predict textual output and it is an open
research question about how to best utilize them in navigation. To solve MINT,
we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation
policy that combines the environment understanding and common sense reasoning
power of long-context VLMs and a robust low-level navigation policy based on
topological graphs. The high-level policy consists of a long-context VLM that
takes the demonstration tour video and the multimodal user instruction as input
to find the goal frame in the tour video. Next, a low-level policy uses the
goal frame and an offline constructed topological graph to generate robot
actions at every timestep. We evaluated Mobility VLA in a 836m^2 real world
environment and show that Mobility VLA has a high end-to-end success rates on
previously unsolved multimodal instructions such as "Where should I return
this?" while holding a plastic bin. A video demonstrating Mobility VLA can be
found here: https://youtu.be/-Tof__Q8_5s

ÊëòË¶ÅÔºö<paragraph>Â∞éËà™Á†îÁ©∂‰∏≠‰∏ÄÂÄãÈõ£‰ª•ÊçâÊë∏ÁöÑÁõÆÊ®ôÔºåÊòØÂª∫Á´ã‰∏ÄÂÄãÊô∫ËÉΩ‰ª£ÁêÜÔºåÂÆÉÂèØ‰ª•ÁêÜËß£ÂåÖÊã¨Ëá™ÁÑ∂Ë™ûË®ÄÂíåÂΩ±ÂÉèÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºå‰∏¶Âü∑Ë°åÊúâÁî®ÁöÑÂ∞éËà™„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÊ≠§ÁõÆÊ®ôÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏ÄÈ°ûÂª£Ê≥õÊúâÁî®ÁöÑÂ∞éËà™‰ªªÂãôÔºåÊàëÂÄëÁ®±‰πãÁÇ∫Á§∫ÁØÑÂ∞éË¶ΩÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Â∞éËà™ (MINT)ÔºåÂÖ∂‰∏≠Áí∞Â¢ÉÂÖàÈ©óÊòØÈÄèÈÅéÂÖàÂâçÈåÑË£ΩÁöÑÁ§∫ÁØÑÂΩ±ÁâáÊèê‰æõÁöÑ„ÄÇË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑËøëÊúüÈÄ≤Â±ïÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÊ¢ùÂØ¶ÁèæÊ≠§ÁõÆÊ®ôÁöÑÊúâÂâçÊôØË∑ØÂæëÔºåÂõ†ÁÇ∫ÂÆÉÂ±ïÁ§∫‰∫ÜÊÑüÁü•ÂíåÊé®ÁêÜÂ§öÊ®°ÊÖãËº∏ÂÖ•ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåVLM ÈÄöÂ∏∏Ë®ìÁ∑¥Áî®ÊñºÈ†êÊ∏¨ÊñáÂ≠óËº∏Âá∫ÔºåËÄåÂ¶Ç‰ΩïÊúÄ‰Ω≥Âà©Áî®ÂÆÉÂÄëÈÄ≤Ë°åÂ∞éËà™ÔºåÂâáÊòØ‰∏ÄÂÄãÈñãÊîæÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ MINTÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Mobility VLAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂàÜÂ±§ÁöÑË¶ñË¶∫-Ë™ûË®Ä-Âãï‰Ωú (VLA) Â∞éËà™ÊîøÁ≠ñÔºåÂÆÉÁµêÂêà‰∫ÜÈï∑Ë™ûÂ¢É VLM ÁöÑÁí∞Â¢ÉÁêÜËß£ÂíåÂ∏∏Ë≠òÊé®ÁêÜËÉΩÂäõÔºå‰ª•ÂèäÂü∫ÊñºÊãìÊí≤ÂúñÁöÑÂº∑ÂÅ•‰ΩéÈöéÂ∞éËà™ÊîøÁ≠ñ„ÄÇÈ´òÈöéÊîøÁ≠ñÂåÖÂê´‰∏ÄÂÄãÈï∑Ë™ûÂ¢É VLMÔºåÂÆÉÊé°Áî®Á§∫ÁØÑÂ∞éË¶ΩÂΩ±ÁâáÂíåÂ§öÊ®°ÊÖã‰ΩøÁî®ËÄÖÊåá‰ª§‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰ª•Âú®Â∞éË¶ΩÂΩ±Áâá‰∏≠ÊâæÂà∞ÁõÆÊ®ôÂπÄ„ÄÇÊé•‰∏ã‰æÜÔºå‰ΩéÈöéÊîøÁ≠ñ‰ΩøÁî®ÁõÆÊ®ôÂπÄÂíåÈõ¢Á∑öÂª∫ÊßãÁöÑÊãìÊí≤ÂúñÔºåÂú®ÊØèÂÄãÊôÇÈñìÊ≠•Áî¢ÁîüÊ©üÂô®‰∫∫Âãï‰Ωú„ÄÇÊàëÂÄëÂú® 836 Âπ≥ÊñπÂÖ¨Â∞∫ÁöÑÁúüÂØ¶‰∏ñÁïåÁí∞Â¢É‰∏≠Ë©ï‰º∞‰∫Ü Mobility VLAÔºå‰∏¶Â±ïÁ§∫‰∫Ü Mobility VLA Âú®ÂÖàÂâçÊú™Ëß£Ê±∫ÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºà‰æãÂ¶Ç„ÄåÊàëÊáâË©≤ÊääÈÄôÂÄãÂ°ëËÜ†ÁÆ±Ê≠∏ÈÇÑÂà∞Âì™Ë£°Ôºü„ÄçÔºâ‰∏äÂÖ∑ÊúâÂæàÈ´òÁöÑÁ´ØÂà∞Á´ØÊàêÂäüÁéáÔºåÂêåÊôÇÊãøËëó‰∏ÄÂÄãÂ°ëËÜ†ÁÆ±„ÄÇÂ±ïÁ§∫ Mobility VLA ÁöÑÂΩ±ÁâáÂèØ‰ª•Âú®ÈÄôË£°ÊâæÂà∞Ôºöhttps://youtu.be/-Tof__Q8_5s</paragraph>

##### **Teaching Transformers Causal Reasoning through Axiomatic Training**
2407.07612v1 by Aniket Vashishtha, Abhinav Kumar, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian, Amit Sharma

For text-based AI systems to interact in the real world, causal reasoning is
an essential skill. Since interventional data is costly to generate, we study
to what extent an agent can learn causal reasoning from passive data.
Specifically, we consider an axiomatic training setup where an agent learns
from multiple demonstrations of a causal axiom (or rule), rather than
incorporating the axiom as an inductive bias or inferring it from data values.
A key question is whether the agent would learn to generalize from the axiom
demonstrations to new scenarios. For example, if a transformer model is trained
on demonstrations of the causal transitivity axiom over small graphs, would it
generalize to applying the transitivity axiom over large graphs? Our results,
based on a novel axiomatic training scheme, indicate that such generalization
is possible. We consider the task of inferring whether a variable causes
another variable, given a causal graph structure. We find that a 67 million
parameter transformer model, when trained on linear causal chains (along with
some noisy variations) can generalize well to new kinds of graphs, including
longer causal chains, causal chains with reversed order, and graphs with
branching; even when it is not explicitly trained for such settings. Our model
performs at par (or even better) than many larger language models such as
GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework
provides a new paradigm of learning causal reasoning from passive data that can
be used to learn arbitrary axioms, as long as sufficient demonstrations can be
generated.

ÊëòË¶ÅÔºö<paragraph>Â∞çÊñºÂü∫ÊñºÊñáÂ≠óÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ËàáÁúüÂØ¶‰∏ñÁïå‰∫íÂãï‰æÜË™™ÔºåÂõ†ÊûúÊé®ÁêÜÊòØ‰∏ÄÈ†ÖÂøÖË¶ÅÁöÑÊäÄËÉΩ„ÄÇÁî±Êñº‰ªãÂÖ•Ë≥áÊñôÁöÑÁî¢ÁîüÊàêÊú¨ÂæàÈ´òÔºåÊàëÂÄëÁ†îÁ©∂‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÁ®ãÂ∫¶„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëËÄÉÊÖÆ‰∏ÄÂÄãÂÖ¨ÁêÜË®ìÁ∑¥Ë®≠ÁΩÆÔºåÂÖ∂‰∏≠‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûÂõ†ÊûúÂÖ¨ÁêÜÔºàÊàñË¶èÂâáÔºâÁöÑÂ§öÂÄãÁ§∫ÁØÑ‰∏≠Â≠∏ÁøíÔºåËÄå‰∏çÊòØÂ∞áÂÖ¨ÁêÜ‰ΩúÁÇ∫Ê≠∏Á¥çÂÅèË™§ÊàñÂæûË≥áÊñôÂÄº‰∏≠Êé®Êñ∑Âá∫‰æÜ„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØ‰ª£ÁêÜ‰∫∫ÊòØÂê¶ÊúÉÂ≠∏ÊúÉÂæûÂÖ¨ÁêÜÁ§∫ÁØÑÊé®Âª£Âà∞Êñ∞ÁöÑÂ†¥ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûú‰∏ÄÂÄãTransformerÊ®°ÂûãÂú®Â∞èÂúñË°®‰∏äÂõ†ÊûúÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÁöÑÁ§∫ÁØÑ‰∏≠Êé•ÂèóË®ìÁ∑¥ÔºåÂÆÉÊòØÂê¶ÊúÉÊé®Âª£Âà∞Âú®Â§ßÂúñË°®‰∏äÊáâÁî®ÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÔºüÊàëÂÄëÁöÑÁµêÊûúÂü∫Êñº‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ¨ÁêÜË®ìÁ∑¥ÊñπÊ°àÔºåË°®ÊòéÈÄôÊ®£ÁöÑÊ¶ÇÊã¨ÊòØÂèØËÉΩÁöÑ„ÄÇÊàëÂÄëËÄÉÊÖÆÊé®Ë´ñ‰∏ÄÂÄãËÆäÊï∏ÊòØÂê¶Â∞éËá¥Âè¶‰∏ÄÂÄãËÆäÊï∏ÁöÑ‰ªªÂãôÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂõ†ÊûúÂúñÁµêÊßã„ÄÇÊàëÂÄëÁôºÁèæ‰∏ÄÂÄã 6700 Ëê¨ÂÄãÂèÉÊï∏ÁöÑTransformerÊ®°ÂûãÔºåÂú®Á∑öÊÄßÂõ†ÊûúÈèàÔºà‰ª•Âèä‰∏Ä‰∫õÈõúË®äËÆäÂåñÔºâ‰∏äË®ìÁ∑¥ÊôÇÔºåÂèØ‰ª•ÂæàÂ•ΩÂú∞Ê¶ÇÊã¨Âà∞Êñ∞È°ûÂûãÁöÑÂúñÂΩ¢ÔºåÂåÖÊã¨Êõ¥Èï∑ÁöÑÂõ†ÊûúÈèà„ÄÅÈ†ÜÂ∫èÁõ∏ÂèçÁöÑÂõ†ÊûúÈèàÂíåÂÖ∑ÊúâÂàÜÊîØÁöÑÂúñÂΩ¢ÔºõÂç≥‰ΩøÂÆÉÊ≤íÊúâÈáùÂ∞çÊ≠§È°ûË®≠ÁΩÆÈÄ≤Ë°åÊòéÁ¢∫Ë®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæËàáË®±Â§öËºÉÂ§ßÁöÑË™ûË®ÄÊ®°ÂûãÔºà‰æãÂ¶Ç GPT-4„ÄÅGemini Pro Âíå Phi-3ÔºâÁõ∏Áï∂ÔºàÁîöËá≥Êõ¥Â•ΩÔºâ„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂÖ¨ÁêÜË®ìÁ∑¥Ê°ÜÊû∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÊñ∞ÁØÑ‰æãÔºåÂè™Ë¶ÅÂèØ‰ª•Áî¢ÁîüË∂≥Â§†ÁöÑÁ§∫ÁØÑÔºåÂ∞±ÂèØ‰ª•Áî®ÊñºÂ≠∏Áøí‰ªªÊÑèÂÖ¨ÁêÜ„ÄÇ</paragraph>

##### **STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**
2407.12860v1 by Aaron Zolnai-Lucas, Jack Boylan, Chris Hokamp, Parsa Ghaffari

We present Simplified Text-Attributed Graph Embeddings (STAGE), a
straightforward yet effective method for enhancing node features in Graph
Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our
approach leverages Large-Language Models (LLMs) to generate embeddings for
textual attributes. STAGE achieves competitive results on various node
classification benchmarks while also maintaining a simplicity in implementation
relative to current state-of-the-art (SoTA) techniques. We show that utilizing
pre-trained LLMs as embedding generators provides robust features for ensemble
GNN training, enabling pipelines that are simpler than current SoTA approaches
which require multiple expensive training and prompting stages. We also
implement diffusion-pattern GNNs in an effort to make this pipeline scalable to
graphs beyond academic benchmarks.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜÁ∞°ÂåñÊñáÂ≠óÂ±¨ÊÄßÂúñÂµåÂÖ• (STAGE)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ¢ûÂº∑ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Ê®°Âûã‰∏≠ÁöÑÁØÄÈªûÁâπÂæµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÁ∑®Á¢ºÊñáÂ≠óÂ±¨ÊÄßÂúñ (TAG)„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁÇ∫ÊñáÂ≠óÂ±¨ÊÄßÁî¢ÁîüÂµåÂÖ•„ÄÇSTAGE Âú®ÂêÑÁ®ÆÁØÄÈªûÂàÜÈ°ûÂü∫Ê∫ñ‰∏äÂèñÂæó‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºåÂêåÊôÇÂú®ÂØ¶‰Ωú‰∏ä‰πüÁ∂≠ÊåÅ‰∫ÜÁ∞°ÊΩîÊÄßÔºåÁõ∏ËºÉÊñºÁõÆÂâçÁöÑÊäÄË°ìÊ∞¥Ê∫ñ (SoTA)„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®È†êË®ìÁ∑¥ÁöÑ LLM ‰ΩúÁÇ∫ÂµåÂÖ•Áî¢ÁîüÂô®ÔºåÂèØÁÇ∫Êï¥È´î GNN Ë®ìÁ∑¥Êèê‰æõÂº∑ÂÅ•ÁöÑÁâπÂæµÔºåÈÄ≤ËÄåÂª∫ÊßãÊØîÁõÆÂâç SoTA ÂÅöÊ≥ïÊõ¥Á∞°ÂñÆÁöÑÁÆ°ÈÅìÔºåËÄåÂæåËÄÖÈúÄË¶ÅÂ§öÂÄãÊòÇË≤¥ÁöÑË®ìÁ∑¥ÂíåÊèêÁ§∫ÈöéÊÆµ„ÄÇÊàëÂÄë‰πüÂØ¶‰Ωú‰∫ÜÊì¥Êï£Ê®°Âºè GNNÔºå‰ª•ÊúüËÆìÈÄôÂÄãÁÆ°ÈÅìËÉΩÊì¥ÂÖÖÂà∞Â≠∏Ë°ìÂü∫Ê∫ñ‰πãÂ§ñÁöÑÂúñÂΩ¢„ÄÇ

##### **GLBench: A Comprehensive Benchmark for Graph with Large Language Models**
2407.07457v2 by Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li

The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÂúñË°®‰∫íÂãïÁöÑÊñπÂºèÔºåÈÄ≤ËÄåÁî¢Áîü‰∏ÄÁ®ÆÁ®±ÁÇ∫ GraphLLM ÁöÑÊñ∞ÂÖ∏ÁØÑ„ÄÇÂÑòÁÆ°ËøëÂπ¥‰æÜ GraphLLM ÊñπÊ≥ïÂø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÂÖ∑Êúâ‰∏ÄËá¥ÂØ¶È©óÂçîÂÆöÁöÑÂü∫Ê∫ñÔºåÂõ†Ê≠§Ë©≤È†òÂüüÁöÑÈÄ≤Â±ïÂíåÁêÜËß£‰ªç‰∏çÊòéÁ¢∫„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GLBenchÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÁî®ÊñºË©ï‰º∞ GraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèÂíåÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠ÁöÑÁ∂úÂêàÂü∫Ê∫ñ„ÄÇGLBench Êèê‰æõÂ∞ç‰∏çÂêåÈ°ûÂà•ÁöÑ GraphLLM ÊñπÊ≥ïÈÄ≤Ë°åÂÖ¨Âπ≥‰∏îÂæπÂ∫ïÁöÑË©ï‰º∞Ôºå‰ª•ÂèäÂÇ≥Áµ±Âü∫Ê∫ñÔºå‰æãÂ¶ÇÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÈÄèÈÅéÂ∞ç‰∏ÄÁµÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÂØ¶È©óÔºå‰∏¶Êé°Áî®‰∏ÄËá¥ÁöÑË≥áÊñôËôïÁêÜÂíåÂàÜÂâ≤Á≠ñÁï•ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂπæÂÄãÈóúÈçµÁôºÁèæ„ÄÇÈ¶ñÂÖàÔºåGraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèË®≠ÂÆö‰∏≠ÂÑ™ÊñºÂÇ≥Áµ±Âü∫Ê∫ñÔºåÂÖ∂‰∏≠ LLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®È°ØÁ§∫Âá∫ÊúÄÁ©©ÂÅ•ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ËºÉ‰∏çÊúâÊïàÔºåËÄå‰∏îÁ∂ìÂ∏∏Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂ÁöÑËº∏Âá∫ÂïèÈ°å„ÄÇÊàëÂÄëÈÇÑÊ≥®ÊÑèÂà∞ÔºåÂ∞çÊñºÁõÆÂâçÁöÑ GraphLLM ÊñπÊ≥ï‰∏¶‰∏çÂ≠òÂú®ÊòéÁ¢∫ÁöÑÁ∏ÆÊîæÂÆöÂæã„ÄÇÊ≠§Â§ñÔºåÁµêÊßãÂíåË™ûÁæ©Â∞çÊñºÊúâÊïàÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂÇ≥Ëº∏Ëá≥ÈóúÈáçË¶ÅÔºåËÄåÊàëÂÄëÊèêÂá∫ÁöÑÁ∞°ÂñÆÂü∫Ê∫ñÁîöËá≥ÂèØ‰ª•ÂÑ™ÊñºÈáùÂ∞çÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØÈáèË∫´ÊâìÈÄ†ÁöÑÂπæÂÄãÊ®°Âûã„ÄÇÂü∫Ê∫ñÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/NineAbyss/GLBench ‰∏≠ÊâæÂà∞„ÄÇ

##### **Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**
2407.07038v1 by Ruiran Su, Janet B. Pierrehumbert

This work introduces the ClimateSent-GAT Model, an innovative method that
integrates Graph Attention Networks (GATs) with techniques from natural
language processing to accurately identify and predict disagreements within
Reddit comment-reply pairs. Our model classifies disagreements into three
categories: agree, disagree, and neutral. Leveraging the inherent graph
structure of Reddit comment-reply pairs, the model significantly outperforms
existing benchmarks by capturing complex interaction patterns and sentiment
dynamics. This research advances graph-based NLP methodologies and provides
actionable insights for policymakers and educators in climate science
communication.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂‰ªãÁ¥π ClimateSent-GAT Ê®°ÂûãÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞áÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (GAT) ËàáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìÊï¥ÂêàÔºå‰ª•Ê∫ñÁ¢∫Ë≠òÂà•‰∏¶È†êÊ∏¨ Reddit ÁïôË®ÄÂõûË¶ÜÂ∞ç‰∏≠ÁöÑÂàÜÊ≠ß„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞áÂàÜÊ≠ßÂàÜÁÇ∫‰∏âÈ°ûÔºöÂêåÊÑè„ÄÅ‰∏çÂêåÊÑèÂíå‰∏≠Á´ã„ÄÇÈÄèÈÅéÂà©Áî® Reddit ÁïôË®ÄÂõûË¶ÜÂ∞çÁöÑÂÖßÂú®ÂúñÂΩ¢ÁµêÊßãÔºåÊ≠§Ê®°ÂûãËÉΩÂ§ßÂπÖË∂ÖË∂äÁèæÊúâÂü∫Ê∫ñÔºåÊçïÊçâË§áÈõúÁöÑ‰∫íÂãïÊ®°ÂºèÂíåÊÉÖÁ∑íÂãïÊÖã„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êé®Âãï‰∫ÜÂü∫ÊñºÂúñÂΩ¢ÁöÑ NLP ÊñπÊ≥ïÔºå‰∏¶ÁÇ∫Ê∞£ÂÄôÁßëÂ≠∏Ê∫ùÈÄö‰∏≠ÁöÑÊîøÁ≠ñÂà∂ÂÆöËÄÖÂíåÊïôËÇ≤Â∑•‰ΩúËÄÖÊèê‰æõÂèØË°åÁöÑË¶ãËß£„ÄÇ

##### **Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**
2407.06723v1 by Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B√©thune, Hadi Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, Marco Cuturi

Humans describe complex scenes with compositionality, using simple text
descriptions enriched with links and relationships. While vision-language
research has aimed to develop models with compositional understanding
capabilities, this is not reflected yet in existing datasets which, for the
most part, still use plain text to describe images. In this work, we propose a
new annotation strategy, graph-based captioning (GBC) that describes an image
using a labelled graph structure, with nodes of various types. The nodes in GBC
are created using, in a first stage, object detection and dense captioning
tools nested recursively to uncover and describe entity nodes, further linked
together in a second stage by highlighting, using new types of nodes,
compositions and relations among entities. Since all GBC nodes hold plain text
descriptions, GBC retains the flexibility found in natural language, but can
also encode hierarchical information in its edges. We demonstrate that GBC can
be produced automatically, using off-the-shelf multimodal LLMs and
open-vocabulary detection models, by building a new dataset, GBC10M, gathering
GBC annotations for about 10M images of the CC12M dataset. We use GBC10M to
showcase the wealth of node captions uncovered by GBC, as measured with CLIP
training. We show that using GBC nodes' annotations -- notably those stored in
composition and relation nodes -- results in significant performance boost on
downstream models when compared to other dataset formats. To further explore
the opportunities provided by GBC, we also propose a new attention mechanism
that can leverage the entire GBC graph, with encouraging experimental results
that show the extra benefits of incorporating the graph structure. Our datasets
are released at \url{https://huggingface.co/graph-based-captions}.

ÊëòË¶ÅÔºö<paragraph>‰∫∫È°û‰ΩøÁî®Á∞°ÂñÆÁöÑÊñáÂ≠óÊèèËø∞ÔºåË±êÂØåÁöÑÈÄ£ÁµêÂíåÈóú‰øÇÔºå‰æÜÊèèËø∞Ë§áÈõúÁöÑÂ†¥ÊôØ„ÄÇÈõñÁÑ∂Ë¶ñË¶∫Ë™ûË®ÄÁöÑÁ†îÁ©∂Êó®Âú®ÈñãÁôºÂÖ∑ÊúâÁµÑÂêàÁêÜËß£ËÉΩÂäõÁöÑÊ®°ÂûãÔºå‰ΩÜÁèæÊúâÁöÑÊï∏ÊìöÈõÜÂ∞öÊú™ÂèçÊò†ÈÄô‰∏ÄÈªûÔºåÈÄô‰∫õÊï∏ÊìöÈõÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªç‰ΩøÁî®Á¥îÊñáÊú¨‰æÜÊèèËø∞ÂúñÂÉè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË®ªÈáãÁ≠ñÁï•ÔºåÂü∫ÊñºÂúñË°®ÁöÑÊ®ôÈ°å (GBC)ÔºåÂÆÉ‰ΩøÁî®Ê®ôÁ±§ÂúñË°®ÁµêÊßã‰æÜÊèèËø∞ÂúñÂÉèÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÈ°ûÂûãÁöÑÁØÄÈªû„ÄÇGBC ‰∏≠ÁöÑÁØÄÈªûÊòØ‰ΩøÁî®Áâ©È´îÊ™¢Ê∏¨ÂíåÂØÜÈõÜÊ®ôÈ°åÂ∑•ÂÖ∑Âú®Á¨¨‰∏ÄÈöéÊÆµÂâµÂª∫ÁöÑÔºå‰ª•ÈÅûËø¥ÂµåÂ•óÁöÑÊñπÂºèÁôºÁèæÂíåÊèèËø∞ÂØ¶È´îÁØÄÈªûÔºå‰∏¶Âú®Á¨¨‰∫åÈöéÊÆµ‰ΩøÁî®Êñ∞È°ûÂûãÁöÑÁØÄÈªûÁ™ÅÂá∫È°ØÁ§∫ÔºåÂæûËÄåÂ∞áÂÆÉÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ£ÁµêÂú®‰∏ÄËµ∑ÔºåÂØ¶È´î‰πãÈñìÁöÑÁµÑÂêàÂíåÈóú‰øÇ„ÄÇÁî±ÊñºÊâÄÊúâ GBC ÁØÄÈªûÈÉΩÂåÖÂê´Á¥îÊñáÊú¨ÊèèËø∞ÔºåÂõ†Ê≠§ GBC ‰øùÁïô‰∫ÜËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÈùàÊ¥ªÊÄßÔºå‰ΩÜ‰πüÂèØ‰ª•Âú®ÂÖ∂ÈÇäÁ∑£Á∑®Á¢ºÂàÜÂ±§‰ø°ÊÅØ„ÄÇÊàëÂÄëË≠âÊòé‰∫Ü GBC ÂèØ‰ª•‰ΩøÁî®ÁèæÊàêÁöÑÂ§öÊ®°ÊÖã LLM ÂíåÈñãÊîæË©ûÂΩôÊ™¢Ê∏¨Ê®°ÂûãËá™ÂãïÁîüÊàêÔºåÈÄöÈÅéÊßãÂª∫‰∏ÄÂÄãÊñ∞ÁöÑÊï∏ÊìöÈõÜ GBC10MÔºåÊî∂ÈõÜ‰∫ÜÂ§ßÁ¥Ñ 10M CC12M Êï∏ÊìöÈõÜÂúñÂÉèÁöÑ GBC Ë®ªÈáã„ÄÇÊàëÂÄë‰ΩøÁî® GBC10M ‰æÜÂ±ïÁ§∫ GBC ÁôºÁèæÁöÑË±êÂØåÁØÄÈªûÊ®ôÈ°åÔºå‰∏¶‰ΩøÁî® CLIP Ë®ìÁ∑¥ÈÄ≤Ë°åÊ∏¨Èáè„ÄÇÊàëÂÄëË°®ÊòéÔºåËàáÂÖ∂‰ªñÊï∏ÊìöÈõÜÊ†ºÂºèÁõ∏ÊØîÔºå‰ΩøÁî® GBC ÁØÄÈªûÁöÑË®ªÈáã‚Äî‚ÄîÁâπÂà•ÊòØÂ≠òÂÑ≤Âú®ÁµÑÂêàÂíåÈóú‰øÇÁØÄÈªû‰∏≠ÁöÑË®ªÈáã‚Äî‚ÄîÊúÉÈ°ØËëóÊèêÂçá‰∏ãÊ∏∏Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢ GBC Êèê‰æõÁöÑÊ©üÊúÉÔºåÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊ≥®ÊÑèÊ©üÂà∂ÔºåÂÆÉÂèØ‰ª•Âà©Áî®Êï¥ÂÄã GBC ÂúñË°®Ôºå‰∏¶ÈÄöÈÅéÈºìÂãµÊÄßÁöÑÂØ¶È©óÁµêÊûúÂ±ïÁ§∫‰∫ÜÁµêÂêàÂúñË°®ÁµêÊßãÁöÑÈ°çÂ§ñÂ•ΩËôï„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÁôºÂ∏ÉÂú® \url{https://huggingface.co/graph-based-captions}„ÄÇ</paragraph>

##### **Combining Knowledge Graphs and Large Language Models**
2407.06564v1 by Amanda Kau, Xuzeng He, Aishwarya Nambissan, Aland Astudillo, Hui Yin, Amir Aryani

In recent years, Natural Language Processing (NLP) has played a significant
role in various Artificial Intelligence (AI) applications such as chatbots,
text generation, and language translation. The emergence of large language
models (LLMs) has greatly improved the performance of these applications,
showing astonishing results in language understanding and generation. However,
they still show some disadvantages, such as hallucinations and lack of
domain-specific knowledge, that affect their performance in real-world tasks.
These issues can be effectively mitigated by incorporating knowledge graphs
(KGs), which organise information in structured formats that capture
relationships between entities in a versatile and interpretable fashion.
Likewise, the construction and validation of KGs present challenges that LLMs
can help resolve. The complementary relationship between LLMs and KGs has led
to a trend that combines these technologies to achieve trustworthy results.
This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based
KGs, and LLM-KG hybrid approaches. We systematically analysed and compared
these approaches to provide a comprehensive overview highlighting key trends,
innovative techniques, and common challenges. This synthesis will benefit
researchers new to the field and those seeking to deepen their understanding of
how KGs and LLMs can be effectively combined to enhance AI applications
capabilities.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ (NLP) Âú®ÂêÑÁßç‰∫∫Â∑•Êô∫ËÉΩ (AI) Â∫îÁî®‰∏≠ÂèëÊå•‰∫ÜÈáçË¶Å‰ΩúÁî®Ôºå‰æãÂ¶ÇËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅÊñáÊú¨ÁîüÊàêÂíåËØ≠Ë®ÄÁøªËØë„ÄÇÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂá∫Áé∞ÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫ÜËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÁöÑÊÄßËÉΩÔºåÂú®ËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢ÊòæÁ§∫Âá∫ÊÉä‰∫∫ÁöÑÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨‰ªçÁÑ∂Ë°®Áé∞Âá∫‰∏Ä‰∫õÁº∫ÁÇπÔºå‰æãÂ¶ÇÂπªËßâÂíåÁº∫‰πèÁâπÂÆöÈ¢ÜÂüüÁöÑÁü•ËØÜÔºåËøô‰∫õÁº∫ÁÇπ‰ºöÂΩ±ÂìçÂÆÉ‰ª¨Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÈÄöËøáÁ∫≥ÂÖ•Áü•ËØÜÂõæË∞± (KG) ÂèØ‰ª•ÊúâÊïàÂú∞ÂáèËΩªËøô‰∫õÈóÆÈ¢òÔºåÁü•ËØÜÂõæË∞±‰ª•ÁªìÊûÑÂåñÊ†ºÂºèÁªÑÁªá‰ø°ÊÅØÔºå‰ª•Â§öÂäüËÉΩ‰∏îÂèØËß£ÈáäÁöÑÊñπÂºèÊçïËé∑ÂÆû‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂêåÊ†∑ÔºåKG ÁöÑÊûÑÂª∫ÂíåÈ™åËØÅÊèêÂá∫‰∫Ü LLM ÂèØ‰ª•Â∏ÆÂä©Ëß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇLLM Âíå KG ‰πãÈó¥ÁöÑ‰∫íË°•ÂÖ≥Á≥ªÂØºËá¥‰∫Ü‰∏ÄÁßçÂ∞ÜËøô‰∫õÊäÄÊúØÁõ∏ÁªìÂêà‰ª•ÂÆûÁé∞ÂèØ‰ø°ÁªìÊûúÁöÑË∂ãÂäø„ÄÇËøôÈ°πÂ∑•‰ΩúÊî∂ÈõÜ‰∫Ü 28 ÁØáÊ¶ÇËø∞‰∫Ü KG È©±Âä®ÁöÑ LLM„ÄÅÂü∫‰∫é LLM ÁöÑ KG Âíå LLM-KG Ê∑∑ÂêàÊñπÊ≥ïÁöÑÊñπÊ≥ïÁöÑËÆ∫Êñá„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÂàÜÊûêÂíåÊØîËæÉ‰∫ÜËøô‰∫õÊñπÊ≥ïÔºå‰ª•Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ¶ÇËø∞ÔºåÈáçÁÇπ‰ªãÁªçÂÖ≥ÈîÆË∂ãÂäø„ÄÅÂàõÊñ∞ÊäÄÊúØÂíåÂÖ±ÂêåÊåëÊàò„ÄÇËøôÁßçÁªºÂêàÂ∞Ü‰ΩøËØ•È¢ÜÂüüÁöÑÊñ∞Á†îÁ©∂‰∫∫ÂëòÂíåÈÇ£‰∫õÂØªÊ±ÇÂä†Ê∑±ÂØπÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞Ü KG Âíå LLM Áõ∏ÁªìÂêà‰ª•Â¢ûÂº∫ AI Â∫îÁî®ËÉΩÂäõÁöÑÁêÜËß£ÁöÑ‰∫∫ÂèóÁõä„ÄÇ

##### **FuncEvalGMN: Evaluating Functional Correctness of SQL via Graph Matching Network**
2407.14530v1 by Yi Zhan, Yang Sun, Han Weng, Longjie Cui, Guifeng Wang, Jiajun Xie, Yu Tian, Xiaoming Yin, Boyi Liu, Dongchi Huang

In this paper, we propose a novel graph-based methodology to evaluate the
functional correctness of SQL generation. Conventional metrics for assessing
SQL code generation, such as matching-based and execution-based methods (e.g.,
exact set match and execution accuracy), are subject to two primary
limitations. Firstly, the former fails to effectively assess functional
correctness, as different SQL queries may possess identical functionalities.
Secondly, the latter is susceptible to producing false positive samples in
evaluations. Our proposed evaluation method, \texttt{FuncEvalGMN}, does not
depend on the sufficient preparation of the test data, and it enables precise
testing of the functional correctness of the code. Firstly, we parse SQL using
a relational operator tree (ROT) called \textit{Relnode}, which contains rich
semantic information from the perspective of logical execution.Then, we
introduce a GNN-based approach for predicting the functional correctness of
generated SQL. This approach incorporates global positional embeddings to
address the limitations with the loss of topological information in
conventional graph matching frameworks. As an auxiliary contribution, we
propose a rule-based matching algorithm, Relnode Partial Matching
(\texttt{RelPM}) as a baseline. Finally, we contribute a dataset,
\texttt{Pair-Aug-Spider} with a training set and two testing sets, each
comprising pairs of SQL codes to simulate various SQL code evaluation
scenarios. The training set and one testing dataset focus on code generation
using large language models (LLMs), while the other emphasizes SQL equivalence
rewriting.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÂúñÁöÑÊñπÊ≥ï‰æÜË©ï‰º∞ SQL ÁîüÊàêÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇË©ï‰º∞ SQL Á®ãÂºèÁ¢ºÁîüÊàêÁöÑÂÇ≥Áµ±ÊåáÊ®ôÔºå‰æãÂ¶ÇÂü∫ÊñºÂåπÈÖçÂíåÂü∫ÊñºÂü∑Ë°åÁöÑÊåáÊ®ôÔºà‰æãÂ¶ÇÔºåÁ≤æÁ¢∫ÈõÜÂêàÂåπÈÖçÂíåÂü∑Ë°åÊ∫ñÁ¢∫Â∫¶ÔºâÔºåÂ≠òÂú®ÂÖ©ÂÄã‰∏ªË¶ÅÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÂâçËÄÖÁÑ°Ê≥ïÊúâÊïàË©ï‰º∞ÂäüËÉΩÊ≠£Á¢∫ÊÄßÔºåÂõ†ÁÇ∫‰∏çÂêåÁöÑ SQL Êü•Ë©¢ÂèØËÉΩÂÖ∑ÊúâÁõ∏ÂêåÁöÑÊ©üËÉΩ„ÄÇÂÖ∂Ê¨°ÔºåÂæåËÄÖÂú®Ë©ï‰º∞‰∏≠ÂÆπÊòìÁî¢ÁîüÂÅáÈôΩÊÄßÊ®£Êú¨„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑË©ï‰º∞ÊñπÊ≥ï \texttt{FuncEvalGMN} ‰∏ç‰æùË≥¥ÊñºÊ∏¨Ë©¶Ë≥áÊñôÁöÑÂÖÖÂàÜÊ∫ñÂÇôÔºå‰∏¶‰∏îÂèØ‰ª•Á≤æÁ¢∫Ê∏¨Ë©¶Á®ãÂºèÁ¢ºÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî®Á®±ÁÇ∫ \textit{Relnode} ÁöÑÈóú‰øÇÈÅãÁÆóÂÖÉÊ®π (ROT) ‰æÜËß£Êûê SQLÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂæûÈÇèËºØÂü∑Ë°åÁöÑËßíÂ∫¶‰æÜÁúãË±êÂØåÁöÑË™ûÁæ©Ë≥áË®ä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÂü∫Êñº GNN ÁöÑÊñπÊ≥ï‰æÜÈ†êÊ∏¨ÁîüÊàêÁöÑ SQL ÁöÑÂäüËÉΩÊ≠£Á¢∫ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁµêÂêà‰∫ÜÂÖ®Â±Ä‰ΩçÁΩÆÂµåÂÖ•Ôºå‰ª•Ëß£Ê±∫ÂÇ≥Áµ±ÂúñÂΩ¢ÂåπÈÖçÊ°ÜÊû∂‰∏≠ÊãìÊí≤Ë≥áË®äÈÅ∫Â§±ÁöÑÈôêÂà∂„ÄÇ‰ΩúÁÇ∫ËºîÂä©Ë≤¢ÁçªÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË¶èÂâáÁöÑÂåπÈÖçÊºîÁÆóÊ≥ïÔºåÂç≥ Relnode ÈÉ®ÂàÜÂåπÈÖç (\texttt{RelPM}) ‰ΩúÁÇ∫Âü∫Á∑ö„ÄÇÊúÄÂæåÔºåÊàëÂÄëË≤¢Áçª‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜ \texttt{Pair-Aug-Spider}ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÂÄãË®ìÁ∑¥ÈõÜÂíåÂÖ©ÂÄãÊ∏¨Ë©¶ÈõÜÔºåÊØèÂÄãÊ∏¨Ë©¶ÈõÜÈÉΩÂåÖÂê´ÊàêÂ∞çÁöÑ SQL Á®ãÂºèÁ¢º‰æÜÊ®°Êì¨ÂêÑÁ®Æ SQL Á®ãÂºèÁ¢ºË©ï‰º∞Â†¥ÊôØ„ÄÇË®ìÁ∑¥ÈõÜÂíå‰∏ÄÂÄãÊ∏¨Ë©¶Ë≥áÊñôÈõÜÂ∞àÊ≥®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÁ®ãÂºèÁ¢ºÁîüÊàêÔºåËÄåÂè¶‰∏ÄÂÄãÂâáÂº∑Ë™ø SQL Á≠âÂÉπÈáçÂØ´„ÄÇ</paragraph>

##### **MST5 -- Multilingual Question Answering over Knowledge Graphs**
2407.06041v1 by Nikit Srivastava, Mengshi Ma, Daniel Vollmers, Hamada Zahera, Diego Moussallem, Axel-Cyrille Ngonga Ngomo

Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of
knowledge stored in a graph-based model using natural language. However, the
research has largely concentrated on English, putting non-English speakers at a
disadvantage. Meanwhile, existing multilingual KGQA systems face challenges in
achieving performance comparable to English systems, highlighting the
difficulty of generating SPARQL queries from diverse languages. In this
research, we propose a simplified approach to enhance multilingual KGQA systems
by incorporating linguistic context and entity information directly into the
processing pipeline of a language model. Unlike existing methods that rely on
separate encoders for integrating auxiliary information, our strategy leverages
a single, pretrained multilingual transformer-based language model to manage
both the primary input and the auxiliary data. Our methodology significantly
improves the language model's ability to accurately convert a natural language
query into a relevant SPARQL query. It demonstrates promising results on the
most recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we
introduce and evaluate our approach on Chinese and Japanese, thereby expanding
the language diversity of the existing datasets.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË°®ÂïèÁ≠î (KGQA) Á∞°Âåñ‰∫Ü‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÂÑ≤Â≠òÂú®ÂúñÂΩ¢ÂåñÊ®°Âûã‰∏≠ÁöÑÂ§ßÈáèÁü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëã±Êñá‰∏äÔºåÈÄôÂ∞çÈùûËã±Ë™û‰ΩøÁî®ËÄÖ‰æÜË™™ÊòØ‰∏çÂà©ÁöÑ„ÄÇÂêåÊôÇÔºåÁèæÊúâÁöÑÂ§öË™ûË®Ä KGQA Á≥ªÁµ±Âú®ÈÅîÊàêËàáËã±ÊñáÁ≥ªÁµ±Áõ∏Â™≤ÁæéÁöÑÊïàËÉΩÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁ™ÅÈ°Ø‰∫ÜÂæû‰∏çÂêåË™ûË®ÄÁî¢Áîü SPARQL Êü•Ë©¢ÁöÑÂõ∞Èõ£ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∞°ÂåñÁöÑÊñπÊ≥ïÔºåÈÄöÈÅéÂ∞áË™ûË®ÄÂ≠∏ËÉåÊôØÂíåÂØ¶È´îË≥áË®äÁõ¥Êé•Á¥çÂÖ•Ë™ûË®ÄÊ®°ÂûãÁöÑËôïÁêÜÁÆ°ÈÅìÔºå‰æÜÂ¢ûÂº∑Â§öË™ûË®Ä KGQA Á≥ªÁµ±„ÄÇËàá‰æùË≥¥ÊñºÂñÆÁç®Á∑®Á¢ºÂô®‰æÜÊï¥ÂêàËºîÂä©Ë≥áË®äÁöÑÁèæÊúâÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑÁ≠ñÁï•Âà©Áî®ÂñÆ‰∏ÄÁöÑ„ÄÅÈ†êË®ìÁ∑¥ÁöÑÂ§öË™ûË®ÄËΩâÊèõÂô®Ë™ûË®ÄÊ®°Âûã‰æÜÁÆ°ÁêÜ‰∏ªË¶ÅËº∏ÂÖ•ÂíåËºîÂä©Ë≥áÊñô„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÈ°ØËëóÊèêÂçá‰∫ÜË™ûË®ÄÊ®°ÂûãÊ∫ñÁ¢∫Âú∞Â∞áËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ËΩâÊèõÁÇ∫Áõ∏Èóú SPARQL Êü•Ë©¢ÁöÑËÉΩÂäõ„ÄÇÂÆÉÂú®ÊúÄÊñ∞ÁöÑ QALD Ë≥áÊñôÈõÜÔºåÂç≥ QALD-9-Plus Âíå QALD-10 ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®‰∏≠ÊñáÂíåÊó•Êñá‰∏≠ÂºïÂÖ•‰∏¶Ë©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂæûËÄåÊì¥Â±ï‰∫ÜÁèæÊúâË≥áÊñôÈõÜÁöÑË™ûË®ÄÂ§öÊ®£ÊÄß„ÄÇ

##### **Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding**
2407.05910v1 by Aaron Lohner, Francesco Compagno, Jonathan Francis, Alessandro Oltramari

Recognizing a traffic accident is an essential part of any autonomous driving
or road monitoring system. An accident can appear in a wide variety of forms,
and understanding what type of accident is taking place may be useful to
prevent it from reoccurring. The task of being able to classify a traffic scene
as a specific type of accident is the focus of this work. We approach the
problem by likening a traffic scene to a graph, where objects such as cars can
be represented as nodes, and relative distances and directions between them as
edges. This representation of an accident can be referred to as a scene graph,
and is used as input for an accident classifier. Better results can be obtained
with a classifier that fuses the scene graph input with representations from
vision and language. This work introduces a multi-stage, multimodal pipeline to
pre-process videos of traffic accidents, encode them as scene graphs, and align
this representation with vision and language modalities for accident
classification. When trained on 4 classes, our method achieves a balanced
accuracy score of 57.77% on an (unbalanced) subset of the popular Detection of
Traffic Anomaly (DoTA) benchmark, representing an increase of close to 5
percentage points from the case where scene graph information is not taken into
account.

ÊëòË¶ÅÔºöËæ®Ë≠ò‰∫§ÈÄö‰∫ãÊïÖÊòØ‰ªª‰ΩïËá™ÂãïÈßïÈßõÊàñÈÅìË∑ØÁõ£ÊéßÁ≥ªÁµ±ÁöÑÂøÖË¶ÅÈÉ®ÂàÜ„ÄÇ‰∫ãÊïÖÂèØËÉΩ‰ª•ÂêÑÁ®ÆÂΩ¢ÂºèÂá∫ÁèæÔºå‰∫ÜËß£‰∫ãÊïÖÈ°ûÂûãÂèØËÉΩÊúâÂä©ÊñºÈò≤Ê≠¢ÂÜçÊ¨°ÁôºÁîü„ÄÇÂ∞á‰∫§ÈÄö‰∫ãÊïÖÂ†¥ÊôØÂàÜÈ°ûÁÇ∫ÁâπÂÆö‰∫ãÊïÖÈ°ûÂûãÁöÑ‰ªªÂãôÊòØÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÈáçÈªû„ÄÇÊàëÂÄëÂ∞á‰∫§ÈÄö‰∫ãÊïÖÂ†¥ÊôØÊØîÂñªÁÇ∫ÂúñÂΩ¢‰æÜËß£Ê±∫ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê±ΩËªäÁ≠âÁâ©È´îÂèØ‰ª•Ë°®Á§∫ÁÇ∫ÁØÄÈªûÔºåËÄåÂÆÉÂÄë‰πãÈñìÁöÑÁõ∏Â∞çË∑ùÈõ¢ÂíåÊñπÂêëÂâáË°®Á§∫ÁÇ∫ÈÇäÁ∑£„ÄÇÈÄôÁ®Æ‰∫ãÊïÖË°®Á§∫ÂèØ‰ª•Á®±ÁÇ∫Â†¥ÊôØÂúñÔºå‰∏¶Áî®‰Ωú‰∫ãÊïÖÂàÜÈ°ûÂô®ÁöÑËº∏ÂÖ•„ÄÇ‰ΩøÁî®Â∞áÂ†¥ÊôØÂúñËº∏ÂÖ•ËàáË¶ñË¶∫ÂíåË™ûË®ÄË°®Á§∫ËûçÂêàÁöÑÂàÜÈ°ûÂô®ÂèØ‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑÁµêÊûú„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ§öÈöéÊÆµ„ÄÅÂ§öÊ®°ÊÖãÁÆ°ÈÅìÔºåÁî®ÊñºÈ†êËôïÁêÜ‰∫§ÈÄö‰∫ãÊïÖÂΩ±Áâá„ÄÅÂ∞áÂÖ∂Á∑®Á¢ºÁÇ∫Â†¥ÊôØÂúñÔºå‰ª•ÂèäÂ∞áÊ≠§Ë°®Á§∫ËàáË¶ñË¶∫ÂíåË™ûË®ÄÊ®°ÂºèÂ∞çÈΩä‰ª•ÈÄ≤Ë°å‰∫ãÊïÖÂàÜÈ°û„ÄÇÁï∂Âú® 4 ÂÄãÈ°ûÂà•‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÁÜ±ÈñÄ‰∫§ÈÄöÁï∞Â∏∏Ê™¢Ê∏¨ (DoTA) Âü∫Ê∫ñÁöÑÔºà‰∏çÂπ≥Ë°°ÔºâÂ≠êÈõÜ‰∏äÂØ¶Áèæ‰∫Ü 57.77% ÁöÑÂπ≥Ë°°Ê∫ñÁ¢∫ÁéáÔºåÊØî‰∏çËÄÉÊÖÆÂ†¥ÊôØÂúñË≥áË®äÁöÑÊÉÖÊ≥ÅÊèêÈ´ò‰∫ÜÊé•Ëøë 5 ÂÄãÁôæÂàÜÈªû„ÄÇ

##### **Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**
2407.05890v1 by Jiaqi Chen, Bingqian Lin, Xinmin Liu, Xiaodan Liang, Kwan-Yee K. Wong

LLM-based agents have demonstrated impressive zero-shot performance in the
vision-language navigation (VLN) task. However, these zero-shot methods focus
only on solving high-level task planning by selecting nodes in predefined
navigation graphs for movements, overlooking low-level control in realistic
navigation scenarios. To bridge this gap, we propose AO-Planner, a novel
affordances-oriented planning framework for continuous VLN task. Our AO-Planner
integrates various foundation models to achieve affordances-oriented motion
planning and action decision-making, both performed in a zero-shot manner.
Specifically, we employ a visual affordances prompting (VAP) approach, where
visible ground is segmented utilizing SAM to provide navigational affordances,
based on which the LLM selects potential next waypoints and generates low-level
path planning towards selected waypoints. We further introduce a high-level
agent, PathAgent, to identify the most probable pixel-based path and convert it
into 3D coordinates to fulfill low-level motion. Experimental results on the
challenging R2R-CE benchmark demonstrate that AO-Planner achieves
state-of-the-art zero-shot performance (5.5% improvement in SPL). Our method
establishes an effective connection between LLM and 3D world to circumvent the
difficulty of directly predicting world coordinates, presenting novel prospects
for employing foundation models in low-level motion control.

ÊëòË¶ÅÔºöÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÂ∑≤Âú®Ë¶ñË¶∫Ë™ûË®ÄÂ∞éËà™ (VLN) ‰ªªÂãô‰∏≠Â±ïÁ§∫Âá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÂÉÖÂ∞àÊ≥®ÊñºÈÄèÈÅéÈÅ∏ÊìáÈ†êÂÆöÁæ©Â∞éËà™ÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªû‰æÜËß£Ê±∫È´òÈöé‰ªªÂãôË¶èÂäÉÔºåÂøΩÁï•‰∫ÜÂØ¶ÈöõÂ∞éËà™Â†¥ÊôØ‰∏≠ÁöÑ‰ΩéÈöéÊéßÂà∂„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÊ≠§Â∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫ AO-PlannerÔºå‰∏ÄÂÄãÁî®ÊñºÈÄ£Á∫å VLN ‰ªªÂãôÁöÑÊñ∞Âûã‰ª•ÂèØË≤†ÊìîÊÄßÁÇ∫Â∞éÂêëÁöÑË¶èÂäÉÊû∂Êßã„ÄÇÊàëÂÄëÁöÑ AO-Planner Êï¥ÂêàÂêÑÁ®ÆÂü∫Á§éÊ®°ÂûãÔºå‰ª•ÂØ¶Áèæ‰ª•ÂèØË≤†ÊìîÊÄßÁÇ∫Â∞éÂêëÁöÑÂãï‰ΩúË¶èÂäÉÂíåÂãï‰ΩúÊ±∫Á≠ñÔºåÂÖ©ËÄÖÈÉΩ‰ª•Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊñπÂºèÂü∑Ë°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®Ë¶ñË¶∫ÂèØË≤†ÊìîÊÄßÊèêÁ§∫ (VAP) ÊñπÊ≥ïÔºåÂÖ∂‰∏≠Âà©Áî® SAM Â∞çÂèØË¶ãÂú∞Èù¢ÈÄ≤Ë°åÂàÜÂâ≤Ôºå‰ª•Êèê‰æõÂ∞éËà™ÂèØË≤†ÊìîÊÄßÔºåLLM Ê†πÊìöÈÄô‰∫õÂèØË≤†ÊìîÊÄßÈÅ∏ÊìáÊΩõÂú®ÁöÑ‰∏ã‰∏ÄÂÄãËà™ÈªûÔºå‰∏¶ÈáùÂ∞çÊâÄÈÅ∏Ëà™ÈªûÁî¢Áîü‰ΩéÈöéË∑ØÂæëË¶èÂäÉ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰∏ÄÂÄãÈ´òÈöé‰ª£ÁêÜ PathAgentÔºå‰ª•Ë≠òÂà•ÊúÄÂèØËÉΩÁöÑÂü∫ÊñºÂÉèÁ¥†ÁöÑË∑ØÂæëÔºå‰∏¶Â∞áÂÖ∂ËΩâÊèõÁÇ∫ 3D Â∫ßÊ®ôÔºå‰ª•ÂØ¶Áèæ‰ΩéÈöéÂãï‰Ωú„ÄÇÂú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ R2R-CE Âü∫Ê∫ñÊ∏¨Ë©¶‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåAO-Planner ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩÔºàSPL ÊèêÂçá 5.5%Ôºâ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú® LLM Âíå 3D ‰∏ñÁïå‰πãÈñìÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑÈÄ£ÁµêÔºå‰ª•Ë¶èÈÅøÁõ¥Êé•È†êÊ∏¨‰∏ñÁïåÂ∫ßÊ®ôÁöÑÈõ£È°åÔºåÁÇ∫Âú®‰ΩéÈöéÂãï‰ΩúÊéßÂà∂‰∏≠Êé°Áî®Âü∫Á§éÊ®°ÂûãÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂâçÊôØ„ÄÇ

##### **KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions**
2407.05868v1 by Yanxu Zhu, Jinlin Xiao, Yuhang Wang, Jitao Sang

Recent studies have demonstrated that large language models (LLMs) are
susceptible to being misled by false premise questions (FPQs), leading to
errors in factual knowledge, know as factuality hallucination. Existing
benchmarks that assess this vulnerability primarily rely on manual
construction, resulting in limited scale and lack of scalability. In this work,
we introduce an automated, scalable pipeline to create FPQs based on knowledge
graphs (KGs). The first step is modifying true triplets extracted from KGs to
create false premises. Subsequently, utilizing the state-of-the-art
capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed
method, we present a comprehensive benchmark, the Knowledge Graph-based False
Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three
knowledge domains, at six levels of confusability, and in two task formats.
Using KG-FPQ, we conduct extensive evaluations on several representative LLMs
and provide valuable insights. The KG-FPQ dataset and code are available
at~https://github.com/yanxuzhu/KG-FPQ.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂÆπÊòìË¢´ÈîôËØØÂâçÊèêÈóÆÈ¢ò (FPQ) ËØØÂØºÔºå‰ªéËÄåÂØºËá¥‰∫ãÂÆûÁü•ËØÜÈîôËØØÔºåÂç≥‰∫ãÂÆûÂπªËßâ„ÄÇÁî®‰∫éËØÑ‰º∞Ê≠§ÊºèÊ¥ûÁöÑÁé∞ÊúâÂü∫ÂáÜ‰∏ªË¶Å‰æùËµñ‰∫éÊâãÂä®ÊûÑÂª∫ÔºåÂØºËá¥ËßÑÊ®°ÊúâÈôê‰∏îÁº∫‰πèÂèØÊâ©Â±ïÊÄß„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÁü•ËØÜÂõæË∞± (KG) ÂàõÂª∫ FPQ ÁöÑËá™Âä®ÂåñÂèØÊâ©Â±ïÁÆ°ÈÅì„ÄÇÁ¨¨‰∏ÄÊ≠•ÊòØ‰øÆÊîπ‰ªé KG ‰∏≠ÊèêÂèñÁöÑÁúü‰∏âÂÖÉÁªÑ‰ª•ÂàõÂª∫ÈîôËØØÂâçÊèê„ÄÇÈöèÂêéÔºåÂà©Áî® GPT ÁöÑÊúÄÂÖàËøõÂäüËÉΩÔºåÊàë‰ª¨ÁîüÊàê‰∫ÜËØ≠‰πâ‰∏∞ÂØåÁöÑ FPQ„ÄÇÂü∫‰∫éÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁªºÂêàÂü∫ÂáÜÔºåÂç≥Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈîôËØØÂâçÊèêÈóÆÈ¢ò (KG-FPQ)ÔºåÂÆÉÂåÖÂê´Â§ßÁ∫¶ 178k ‰∏™ FPQÔºåÊ∂µÁõñ‰∏â‰∏™Áü•ËØÜÂüüÔºåÂÖ≠‰∏™Ê∑∑Ê∑ÜÁ∫ßÂà´Âíå‰∏§Áßç‰ªªÂä°Ê†ºÂºè„ÄÇ‰ΩøÁî® KG-FPQÔºåÊàë‰ª¨ÂØπÂá†‰∏™Êúâ‰ª£Ë°®ÊÄßÁöÑ LLM ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞ÔºåÂπ∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇKG-FPQ Êï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂèØÂú®~https://github.com/yanxuzhu/KG-FPQ Ëé∑Âæó„ÄÇ

##### **Language Models Encode Collaborative Signals in Recommendation**
2407.05441v1 by Leheng Sheng, An Zhang, Yi Zhang, Yuxin Chen, Xiang Wang, Tat-Seng Chua

Recent studies empirically indicate that language models (LMs) encode rich
world knowledge beyond mere semantics, attracting significant attention across
various fields. However, in the recommendation domain, it remains uncertain
whether LMs implicitly encode user preference information. Contrary to the
prevailing understanding that LMs and traditional recommender models learn two
distinct representation spaces due to a huge gap in language and behavior
modeling objectives, this work rethinks such understanding and explores
extracting a recommendation space directly from the language representation
space. Surprisingly, our findings demonstrate that item representations, when
linearly mapped from advanced LM representations, yield superior recommendation
performance. This outcome suggests the homomorphism between the language
representation space and an effective recommendation space, implying that
collaborative signals may indeed be encoded within advanced LMs. Motivated by
these findings, we propose a simple yet effective collaborative filtering (CF)
model named AlphaRec, which utilizes language representations of item textual
metadata (e.g., titles) instead of traditional ID-based embeddings.
Specifically, AlphaRec is comprised of three main components: a multilayer
perceptron (MLP), graph convolution, and contrastive learning (CL) loss
function, making it extremely easy to implement and train. Our empirical
results show that AlphaRec outperforms leading ID-based CF models on multiple
datasets, marking the first instance of such a recommender with text embeddings
achieving this level of performance. Moreover, AlphaRec introduces a new
language-representation-based CF paradigm with several desirable advantages:
being easy to implement, lightweight, rapid convergence, superior zero-shot
recommendation abilities in new domains, and being aware of user intention.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÁ†îÁ©∂ÂØ¶Ë≠âË°®ÊòéÔºåË™ûË®ÄÊ®°Âûã (LM) Á∑®Á¢ºË±êÂØåÁöÑ‰∏ñÁïåÁü•Ë≠òÔºåË∂ÖË∂ä‰∫ÜÂñÆÁ¥îÁöÑË™ûÁæ©ÔºåÂê∏Âºï‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑÊ•µÂ§ßÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÂú®Êé®Ëñ¶È†òÂüü‰∏≠ÔºåLM ÊòØÂê¶Èö±Âê´Á∑®Á¢º‰ΩøÁî®ËÄÖÂÅèÂ•ΩË≥áË®ä‰ªç‰∏çÁ¢∫ÂÆö„ÄÇËàáÊôÆÈÅçË™çÁü•Áõ∏ÂèçÔºåLM ÂíåÂÇ≥Áµ±Êé®Ëñ¶Ê®°ÂûãÁî±ÊñºË™ûË®ÄÂíåË°åÁÇ∫Âª∫Ê®°ÁõÆÊ®ôÁöÑÂ∑®Â§ßÂ∑ÆË∑ùËÄåÂ≠∏ÁøíÂÖ©ÂÄã‰∏çÂêåÁöÑË°®Á§∫Á©∫ÈñìÔºåÈÄôÈ†ÖÂ∑•‰ΩúÈáçÊñ∞ÊÄùËÄÉÈÄôÁ®ÆÁêÜËß£Ôºå‰∏¶Êé¢Á¥¢Áõ¥Êé•ÂæûË™ûË®ÄË°®Á§∫Á©∫Èñì‰∏≠ÊèêÂèñÊé®Ëñ¶Á©∫Èñì„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁï∂ÂæûÂÖàÈÄ≤ÁöÑ LM Ë°®Á§∫‰∏≠Á∑öÊÄßÊò†Â∞ÑÊôÇÔºåÈ†ÖÁõÆË°®Á§∫ÊúÉÁî¢ÁîüÂÑ™Áï∞ÁöÑÊé®Ëñ¶ÊïàËÉΩ„ÄÇÊ≠§ÁµêÊûúË°®ÊòéË™ûË®ÄË°®Á§∫Á©∫ÈñìÂíåÊúâÊïàÁöÑÊé®Ëñ¶Á©∫Èñì‰πãÈñìÂ≠òÂú®ÂêåÊÖãÊÄßÔºåÈÄôÊÑèÂë≥ËëóÂçî‰ΩúË®äËôüÁ¢∫ÂØ¶ÂèØËÉΩÁ∑®Á¢ºÂú®ÂÖàÈÄ≤ÁöÑ LM ‰∏≠„ÄÇÂèóÈÄô‰∫õÁ†îÁ©∂ÁµêÊûúÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂçîÂêåÈÅéÊøæ (CF) Ê®°ÂûãÔºåÂêçÁÇ∫ AlphaRecÔºåÂÆÉÂà©Áî®È†ÖÁõÆÊñáÂ≠óÂÖÉË≥áÊñôÔºà‰æãÂ¶ÇÊ®ôÈ°åÔºâÁöÑË™ûË®ÄË°®Á§∫ÔºåËÄå‰∏çÊòØÂÇ≥Áµ±Âü∫Êñº ID ÁöÑÂµåÂÖ•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåAlphaRec Áî±‰∏âÂÄã‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÁµÑÊàêÔºöÂ§öÂ±§ÊÑüÁü•Âô® (MLP)„ÄÅÂúñÂΩ¢Âç∑Á©çÂíåÂ∞çÊØîÂ≠∏Áøí (CL) ÊêçÂ§±ÂáΩÊï∏Ôºå‰ΩøÂÖ∂Ê•µÊòìÊñºÂØ¶‰ΩúÂíåË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúË°®ÊòéÔºåAlphaRec Âú®Â§öÂÄãË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÈ†òÂÖàÁöÑÂü∫Êñº ID ÁöÑ CF Ê®°ÂûãÔºåÊ®ôË™åËëóÈÄôÁ®ÆÂÖ∑ÊúâÊñáÂ≠óÂµåÂÖ•ÁöÑÊé®Ëñ¶Á≥ªÁµ±È¶ñÊ¨°ÈÅîÂà∞Ê≠§ÊïàËÉΩÊ∞¥Ê∫ñ„ÄÇÊ≠§Â§ñÔºåAlphaRec ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºË™ûË®ÄË°®Á§∫ÁöÑ CF ÂÖ∏ÁØÑÔºåÂÖ∑ÊúâÂ§öÈ†ÖÁêÜÊÉ≥ÁöÑÂÑ™ÈªûÔºöÊòìÊñºÂØ¶‰Ωú„ÄÅËºïÈáèÁ¥ö„ÄÅÂø´ÈÄüÊî∂ÊñÇ„ÄÅÂú®Êñ∞ÁöÑÈ†òÂüü‰∏≠ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊé®Ëñ¶ËÉΩÂäõÔºå‰∏¶‰∏îÂèØ‰ª•‰∫ÜËß£‰ΩøÁî®ËÄÖÁöÑÊÑèÂúñ„ÄÇ</paragraph>

##### **LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models**
2407.05434v1 by Weizhi Tang, Vaishak Belle

Temporal reasoning (TR) is a critical component of artificial intelligence,
encompassing understanding and processing temporal information and
relationships between events. To discover and study the TR ability in Large
Language Models (LLMs), various datasets have been constructed in different
ways for evaluating various aspects of TR ability. Our work proposes a novel
approach to design and develop a pipeline for constructing datasets to evaluate
the TR ability of LLMs by leveraging random directed graph generation, LTL
formula, and the NuSMV model checker. Based on the pipeline, we have also
constructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TR
challenges and evaluated six LLMs with it. Furthermore, we have conducted
additional experiments to discover the impact of increasing the number of
events and formula operators on the complexity of TR problems and the
performance of LLMs. We have demonstrated that although LLMs exhibit some
promise in handling TR challenges, they still struggle with complex TR. We
expect this work can offer insights into TR ability in LLMs while also
providing a valuable tool for future TR evaluations.

ÊëòË¶ÅÔºöÊôÇÈñìÊé®ÁêÜ (TR) ÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÈ†ÖÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºå
Ê∂µËìã‰∫ÜÂ∞çÊôÇÈñìË≥áË®äÂíå‰∫ã‰ª∂‰πãÈñìÈóú‰øÇÁöÑÁêÜËß£ÂíåËôïÁêÜ„ÄÇÁÇ∫‰∫ÜÁôºÁèæÂíåÁ†îÁ©∂Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑ TR ËÉΩÂäõÔºåÂ∑≤ÈÄèÈÅéÂêÑÁ®ÆÊñπÂºèÂª∫ÊßãÂêÑÁ®ÆË≥áÊñôÈõÜÔºåÁî®ÊñºË©ï‰º∞ TR ËÉΩÂäõÁöÑÂêÑÂÄãÈù¢Âêë„ÄÇÊàëÂÄëÁöÑÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºË®≠Ë®àÂíåÈñãÁôº‰∏ÄÂÄãÂª∫ÊßãË≥áÊñôÈõÜÁöÑÁÆ°ÈÅìÔºå‰ª•Ë©ï‰º∞ LLM ÁöÑ TR ËÉΩÂäõÔºåÊñπÊ≥ïÊòØÂà©Áî®Èö®Ê©üÊúâÂêëÂúñÁîüÊàê„ÄÅLTL ÂÖ¨ÂºèÂíå NuSMV Ê®°ÂûãÊ™¢Êü•Âô®„ÄÇÊ†πÊìöÈÄôÂÄãÁÆ°ÈÅìÔºåÊàëÂÄëÈÇÑÂª∫Êßã‰∫Ü‰∏ÄÂÄãË≥áÊñôÈõÜ‰ΩúÁÇ∫Âü∫Ê∫ñÔºåÂç≥ LTLBenchÔºåÂÖ∂‰∏≠ÂåÖÂê´ 2,000 ÂÄã TR ÊåëÊà∞Ôºå‰∏¶Áî®ÂÆÉË©ï‰º∞‰∫ÜÂÖ≠ÂÄã LLM„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÈÄ≤Ë°å‰∫ÜÈ°çÂ§ñÁöÑÂØ¶È©óÔºå‰ª•ÁôºÁèæÂ¢ûÂä†‰∫ã‰ª∂Êï∏ÈáèÂíåÂÖ¨ÂºèÈÅãÁÆóÂ≠êÂ∞ç TR ÂïèÈ°åË§áÈõúÊÄßÂíå LLM ÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂ∑≤Á∂ìË≠âÊòéÔºåÂÑòÁÆ° LLM Âú®ËôïÁêÜ TR ÊåëÊà∞ÊñπÈù¢Ë°®ÁèæÂá∫‰∏Ä‰∫õÂ∏åÊúõÔºå‰ΩÜÂÆÉÂÄë‰ªçÁÑ∂Èõ£‰ª•ËôïÁêÜË§áÈõúÁöÑ TR„ÄÇÊàëÂÄëÈ†êÊúüÈÄôÈ†ÖÂ∑•‰ΩúÂèØ‰ª•Êèê‰æõÂ∞ç LLM ‰∏≠ TR ËÉΩÂäõÁöÑË¶ãËß£ÔºåÂêåÊôÇ‰πüÁÇ∫Êú™‰æÜÁöÑ TR Ë©ï‰º∞Êèê‰æõ‰∏ÄÂÄãÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Leveraging Graph Structures to Detect Hallucinations in Large Language Models**
2407.04485v1 by Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, Roxana Petcu

Large language models are extensively applied across a wide range of tasks,
such as customer support, content creation, educational tutoring, and providing
financial guidance. However, a well-known drawback is their predisposition to
generate hallucinations. This damages the trustworthiness of the information
these models provide, impacting decision-making and user confidence. We propose
a method to detect hallucinations by looking at the structure of the latent
space and finding associations within hallucinated and non-hallucinated
generations. We create a graph structure that connects generations that lie
closely in the embedding space. Moreover, we employ a Graph Attention Network
which utilizes message passing to aggregate information from neighboring nodes
and assigns varying degrees of importance to each neighbor based on their
relevance. Our findings show that 1) there exists a structure in the latent
space that differentiates between hallucinated and non-hallucinated
generations, 2) Graph Attention Networks can learn this structure and
generalize it to unseen generations, and 3) the robustness of our method is
enhanced when incorporating contrastive learning. When evaluated against
evidence-based benchmarks, our model performs similarly without access to
search-based methods.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®Æ‰ªªÂãô‰∏≠Ôºå‰æãÂ¶ÇÂÆ¢Êà∂ÊîØÊè¥„ÄÅÂÖßÂÆπÂâµ‰Ωú„ÄÅÊïôËÇ≤ËºîÂ∞éÂíåÊèê‰æõË≤°ÂãôÊåáÂ∞é„ÄÇÁÑ∂ËÄåÔºå‰∏ÄÂÄãÁúæÊâÄÂë®Áü•ÁöÑÁº∫ÈªûÊòØÂÆÉÂÄëÂÇæÂêëÊñºÁî¢ÁîüÂπªË¶∫„ÄÇÈÄôÊêçÂÆ≥‰∫ÜÈÄô‰∫õÊ®°ÂûãÊâÄÊèê‰æõË≥áË®äÁöÑÂèØ‰ø°Â∫¶ÔºåÂΩ±Èüø‰∫ÜÊ±∫Á≠ñÂà∂ÂÆöÂíå‰ΩøÁî®ËÄÖ‰ø°ÂøÉ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈÄèÈÅéËßÄÂØüÊΩõÂú®Á©∫ÈñìÁöÑÁµêÊßã‰∏¶ÊâæÂá∫ÂπªË¶∫ÂíåÈùûÂπªË¶∫ÁîüÊàê‰∏≠ÁöÑÈóúËÅØ‰æÜÂÅµÊ∏¨ÂπªË¶∫ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢ÁµêÊßãÔºåÈÄ£Êé•Âú®ÂµåÂÖ•Á©∫Èñì‰∏≠Á∑äÂØÜÁõ∏ÈÄ£ÁöÑÁîüÊàê„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé°Áî®‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÔºåÂÆÉÂà©Áî®Ë®äÊÅØÂÇ≥ÈÅû‰æÜÂΩôÁ∏Ω‰æÜËá™Áõ∏ÈÑ∞ÁØÄÈªûÁöÑË≥áË®äÔºå‰∏¶Ê†πÊìöÊØèÂÄãÁõ∏ÈÑ∞ÁØÄÈªûÁöÑÁõ∏ÈóúÊÄßÁÇ∫ÂÖ∂ÊåáÂÆö‰∏çÂêåÁ®ãÂ∫¶ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºå1) ÊΩõÂú®Á©∫Èñì‰∏≠Â≠òÂú®‰∏ÄÂÄãÁµêÊßãÔºåÂèØ‰ª•ÂçÄÂàÜÂπªË¶∫ÂíåÈùûÂπªË¶∫ÁîüÊàêÔºå2) ÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÂèØ‰ª•Â≠∏ÁøíÈÄôÂÄãÁµêÊßã‰∏¶Â∞áÂÖ∂Ê¶ÇÊã¨Âà∞Êú™Ë¶ãÁöÑÁîüÊàê‰∏≠Ôºå‰ª•Âèä 3) Áï∂Á¥çÂÖ•Â∞çÊØîÂ≠∏ÁøíÊôÇÔºåÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÊúÉÂæóÂà∞Â¢ûÂº∑„ÄÇÁï∂Ê†πÊìöÂü∫ÊñºË≠âÊìöÁöÑÂü∫Ê∫ñÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÁÑ°Ê≥ïÂèñÂæóÂü∫ÊñºÊêúÂ∞ãÁöÑÊñπÊ≥ïÁöÑÊÉÖÊ≥Å‰∏ãÔºåË°®ÁèæÂæóÈ°û‰ºº„ÄÇ

##### **AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**
2407.04363v1 by Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev

Advancements in generative AI have broadened the potential applications of
Large Language Models (LLMs) in the development of autonomous agents. Achieving
true autonomy requires accumulating and updating knowledge gained from
interactions with the environment and effectively utilizing it. Current
LLM-based approaches leverage past experiences using a full history of
observations, summarization or retrieval augmentation. However, these
unstructured memory representations do not facilitate the reasoning and
planning essential for complex decision-making. In our study, we introduce
AriGraph, a novel method wherein the agent constructs a memory graph that
integrates semantic and episodic memories while exploring the environment. This
graph structure facilitates efficient associative retrieval of interconnected
concepts, relevant to the agent's current state and goals, thus serving as an
effective environmental model that enhances the agent's exploratory and
planning capabilities. We demonstrate that our Ariadne LLM agent, equipped with
this proposed memory architecture augmented with planning and decision-making,
effectively handles complex tasks on a zero-shot basis in the TextWorld
environment. Our approach markedly outperforms established methods such as
full-history, summarization, and Retrieval-Augmented Generation in various
tasks, including the cooking challenge from the First TextWorld Problems
competition and novel tasks like house cleaning and puzzle Treasure Hunting.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI ÁöÑÈÄ≤Ê≠•Êì¥Â±ï‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™‰∏ª‰ª£ÁêÜÈñãÁôº‰∏≠ÁöÑÊΩõÂú®ÊáâÁî®„ÄÇÂØ¶ÁèæÁúüÊ≠£ÁöÑËá™‰∏ªÊÄßÈúÄË¶ÅÁ¥ØÁ©çÂíåÊõ¥Êñ∞ÂæûËàáÁí∞Â¢É‰∫íÂãï‰∏≠Áç≤ÂæóÁöÑÁü•Ë≠òÔºå‰∏¶ÊúâÊïàÂà©Áî®ÂÆÉ„ÄÇÁï∂ÂâçÁöÑÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂà©Áî®ÈÅéÂéªÁöÑÁ∂ìÈ©óÔºå‰ΩøÁî®ÂÆåÊï¥ÁöÑËßÄÂØü„ÄÅÊëòË¶ÅÊàñÊ™¢Á¥¢Êì¥ÂÖÖ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÈùûÁµêÊßãÂåñÁöÑË®òÊÜ∂Ë°®Âæµ‰∏¶‰∏çËÉΩ‰øÉÈÄ≤Ë§áÈõúÊ±∫Á≠ñÂà∂ÂÆö‰∏≠ÂøÖ‰∏çÂèØÂ∞ëÁöÑÊé®ÁêÜÂíåË¶èÂäÉ„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü AriGraphÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠‰ª£ÁêÜÊßãÂª∫‰∫Ü‰∏ÄÂÄãË®òÊÜ∂ÂúñÔºåË©≤ÂúñÂú®Êé¢Á¥¢Áí∞Â¢ÉÊôÇÊï¥Âêà‰∫ÜË™ûÁæ©ÂíåÊÉÖÁØÄË®òÊÜ∂„ÄÇÈÄôÁ®ÆÂúñÂΩ¢ÁµêÊßã‰øÉÈÄ≤‰∫ÜÁõ∏‰∫íËÅØÁπ´ÁöÑÊ¶ÇÂøµÁöÑÊúâÊïàÈóúËÅØÊÄßÊ™¢Á¥¢ÔºåËàá‰ª£ÁêÜÁöÑÁï∂ÂâçÁãÄÊÖãÂíåÁõÆÊ®ôÁõ∏ÈóúÔºåÂæûËÄå‰ΩúÁÇ∫‰∏ÄÂÄãÊúâÊïàÁöÑÁí∞Â¢ÉÊ®°ÂûãÔºåÂ¢ûÂº∑‰∫Ü‰ª£ÁêÜÁöÑÊé¢Á¥¢ÂíåË¶èÂäÉËÉΩÂäõ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑ Ariadne LLM ‰ª£ÁêÜÔºåÈÖçÂÇô‰∫ÜÈÄôÁ®ÆÊèêË≠∞ÁöÑË®òÊÜ∂Êû∂ÊßãÔºå‰∏¶Â¢ûÂº∑‰∫ÜË¶èÂäÉÂíåÊ±∫Á≠ñÂà∂ÂÆöÔºåÊúâÊïàÂú∞ËôïÁêÜ‰∫Ü TextWorld Áí∞Â¢É‰∏≠Èõ∂Ê¨°Â≠∏ÁøíÁöÑË§áÈõú‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÂÑ™ÊñºÂ∑≤Âª∫Á´ãÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂÆåÊï¥Ê≠∑Âè≤„ÄÅÊëòË¶ÅÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºåÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÔºåÂåÖÊã¨‰æÜËá™Á¨¨‰∏ÄÂÄã TextWorld ÂïèÈ°åÁ´∂Ë≥ΩÁöÑÁÉπÈ£™ÊåëÊà∞ÂíåÊàøÂ±ãÊ∏ÖÊΩîÂíåÊãºÂúñÂ∞ãÂØ∂Á≠âÊñ∞‰ªªÂãô„ÄÇ

##### **Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM**
2407.04067v1 by Peiran Yao, Kostyantyn Guzhva, Denilson Barbosa

Symbolic sentence meaning representations, such as AMR (Abstract Meaning
Representation) provide expressive and structured semantic graphs that act as
intermediates that simplify downstream NLP tasks. However, the
instruction-following capability of large language models (LLMs) offers a
shortcut to effectively solve NLP tasks, questioning the utility of semantic
graphs. Meanwhile, recent work has also shown the difficulty of using meaning
representations merely as a helpful auxiliary for LLMs. We revisit the position
of semantic graphs in syntactic simplification, the task of simplifying
sentence structures while preserving their meaning, which requires semantic
understanding, and evaluate it on a new complex and natural dataset. The
AMR-based method that we propose, AMRS$^3$, demonstrates that state-of-the-art
meaning representations can lead to easy-to-implement simplification methods
with competitive performance and unique advantages in cost, interpretability,
and generalization. With AMRS$^3$ as an anchor, we discover that syntactic
simplification is a task where semantic graphs are helpful in LLM prompting. We
propose AMRCoC prompting that guides LLMs to emulate graph algorithms for
explicit symbolic reasoning on AMR graphs, and show its potential for improving
LLM on semantic-centered tasks like syntactic simplification.

ÊëòË¶ÅÔºöÁ¨¶ËôüÂè•Â≠êÊÑèÁæ©Ë°®ÂæµÔºå‰æãÂ¶Ç AMRÔºàÊäΩË±°ÊÑèÁæ©Ë°®ÂæµÔºâÔºåÊèê‰æõË°®ÈÅîÊÄßÂíåÁµêÊßãÂåñÁöÑË™ûÁæ©ÂúñË°®Ôºå‰ΩúÁÇ∫Á∞°Âåñ‰∏ãÊ∏∏ NLP ‰ªªÂãôÁöÑ‰∏≠‰ªã„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊç∑Âæë‰æÜÊúâÊïàËß£Ê±∫ NLP ‰ªªÂãôÔºåË≥™ÁñëË™ûÁæ©ÂúñË°®ÁöÑÊïàÁî®„ÄÇÂêåÊôÇÔºåÊúÄËøëÁöÑÁ†îÁ©∂‰πüË°®ÊòéÂÉÖÂ∞áÊÑèÁæ©Ë°®ÂæµÁî®‰Ωú LLM ÁöÑËºîÂä©Â∑•ÂÖ∑ÁöÑÈõ£Â∫¶„ÄÇÊàëÂÄëÈáçÊñ∞ÂØ©Ë¶ñË™ûÁæ©ÂúñË°®Âú®Ë™ûÊ≥ïÁ∞°Âåñ‰∏≠ÁöÑ‰ΩçÁΩÆÔºåË™ûÊ≥ïÁ∞°ÂåñÁöÑ‰ªªÂãôÊòØÂú®‰øùÁïôÂè•Â≠êÁµêÊßãÁöÑÂêåÊôÇÁ∞°ÂåñÂè•Â≠êÁµêÊßãÔºåÈÄôÈúÄË¶ÅË™ûÁæ©ÁêÜËß£Ôºå‰∏¶Âú®‰∏ÄÂÄãÊñ∞ÁöÑË§áÈõú‰∏îËá™ÁÑ∂ÁöÑÊï∏ÊìöÈõÜ‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÂü∫Êñº AMR ÁöÑÊñπÊ≥ï AMRS$^3$ Ë≠âÊòé‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÑèÁæ©Ë°®ÂæµÂèØ‰ª•Â∞éËá¥ÊòìÊñºÂØ¶ÁèæÁöÑÁ∞°ÂåñÊñπÊ≥ïÔºåÂú®ÊàêÊú¨„ÄÅÂèØËß£ÈáãÊÄßÂíåÊ≥õÂåñÊñπÈù¢ÂÖ∑ÊúâÁ´∂Áà≠ÂÑ™Âã¢ÂíåÁç®ÁâπÂÑ™Âã¢„ÄÇ‰ª• AMRS$^3$ ÁÇ∫Èå®ÈªûÔºåÊàëÂÄëÁôºÁèæË™ûÊ≥ïÁ∞°ÂåñÊòØ‰∏ÄÈ†ÖË™ûÁæ©ÂúñË°®ÊúâÂä©Êñº LLM ÊèêÁ§∫ÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÊèêÂá∫ AMRCoC ÊèêÁ§∫ÔºåÊåáÂ∞é LLM Ê®°Êì¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÔºåÂ∞ç AMR ÂúñÂΩ¢ÈÄ≤Ë°åÊòéÁ¢∫ÁöÑÁ¨¶ËôüÊé®ÁêÜÔºå‰∏¶Â±ïÁ§∫ÂÖ∂Âú®ÊîπÈÄ≤ LLM Âú®‰ª•Ë™ûÁæ©ÁÇ∫‰∏≠ÂøÉÁöÑ‰ªªÂãôÔºàÂ¶ÇË™ûÊ≥ïÁ∞°ÂåñÔºâÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning**
2407.03779v1 by Lei Yu, Jingcheng Niu, Zining Zhu, Gerald Penn

In this paper, we introduce a comprehensive reformulation of the task known
as Circuit Discovery, along with DiscoGP, a novel and effective algorithm based
on differentiable masking for discovering circuits. Circuit discovery is the
task of interpreting the computational mechanisms of language models (LMs) by
dissecting their functions and capabilities into sparse subnetworks (circuits).
We identified two major limitations in existing circuit discovery efforts: (1)
a dichotomy between weight-based and connection-edge-based approaches forces
researchers to choose between pruning connections or weights, thereby limiting
the scope of mechanistic interpretation of LMs; (2) algorithms based on
activation patching tend to identify circuits that are neither functionally
faithful nor complete. The performance of these identified circuits is
substantially reduced, often resulting in near-random performance in isolation.
Furthermore, the complement of the circuit -- i.e., the original LM with the
identified circuit removed -- still retains adequate performance, indicating
that essential components of a complete circuits are missed by existing
methods.
  DiscoGP successfully addresses the two aforementioned issues and demonstrates
state-of-the-art faithfulness, completeness, and sparsity. The effectiveness of
the algorithm and its novel structure open up new avenues of gathering new
insights into the internal workings of generative AI.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ∞çÁ®±ÁÇ∫ÈõªË∑ØÁôºÁèæ‰ªªÂãôÁöÑÂÖ®Èù¢ÈáçÊñ∞Ë°®Ëø∞Ôºå‰ª•Âèä DiscoGPÔºå‰∏ÄÁ®ÆÂü∫ÊñºÂèØÂæÆÈÅÆÁΩ©ÁöÑÁôºÁèæÈõªË∑ØÁöÑÊñ∞Á©é‰∏îÊúâÊïàÁöÑÊºîÁÆóÊ≥ï„ÄÇÈõªË∑ØÁôºÁèæÊòØÈÄèÈÅéÂ∞áÂÖ∂ÂäüËÉΩÂíåËÉΩÂäõËß£ÂâñÊàêÁ®ÄÁñèÂ≠êÁ∂≤Ë∑ØÔºàÈõªË∑ØÔºâ‰æÜË©ÆÈáãË™ûË®ÄÊ®°ÂûãÔºàLMÔºâÁöÑÈÅãÁÆóÊ©üÂà∂ÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÂú®ÁèæÊúâÁöÑÈõªË∑ØÁôºÁèæÂ∑•‰Ωú‰∏≠ÁôºÁèæ‰∫ÜÂÖ©ÂÄã‰∏ªË¶ÅÁöÑÈôêÂà∂ÔºöÔºà1ÔºâÂü∫ÊñºÊ¨äÈáçÂíåÂü∫ÊñºÈÄ£Êé•ÈÇäÁ∑£ÁöÑÊñπÊ≥ï‰πãÈñìÁöÑ‰∫åÂàÜÊ≥ïËø´‰ΩøÁ†îÁ©∂‰∫∫Âì°Âú®‰øÆÂâ™ÈÄ£Êé•ÊàñÊ¨äÈáç‰πãÈñìÈÄ≤Ë°åÈÅ∏ÊìáÔºåÂæûËÄåÈôêÂà∂‰∫Ü LM Ê©üÂà∂Ë©ÆÈáãÁöÑÁØÑÂúçÔºõÔºà2ÔºâÂü∫ÊñºÂïüÁî®‰øÆË£úÁöÑÊºîÁÆóÊ≥ïÂÇæÂêëÊñºË≠òÂà•Âú®ÂäüËÉΩ‰∏äÊó¢‰∏çÂø†ÂØ¶‰πü‰∏çÂÆåÊï¥ÁöÑÈõªË∑Ø„ÄÇÈÄô‰∫õÂ∑≤Ë≠òÂà•ÈõªË∑ØÁöÑÊïàËÉΩÂ§ßÂπÖÈôç‰ΩéÔºåÈÄöÂ∏∏Â∞éËá¥Â≠§Á´ãÁöÑËøë‰πéÈö®Ê©üÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÈõªË∑ØÁöÑË£úÊï∏‚Äî‚ÄîÂç≥ÁßªÈô§Â∑≤Ë≠òÂà•ÈõªË∑ØÁöÑÂéüÂßã LM‚Äî‚Äî‰ªç‰øùÁïô‰∫ÜË∂≥Â§†ÁöÑÊïàËÉΩÔºåÈÄôË°®Á§∫ÁèæÊúâÊñπÊ≥ïÈåØÂ§±‰∫ÜÂÆåÊï¥ÈõªË∑ØÁöÑÂü∫Êú¨ÁµÑÊàêÈÉ®ÂàÜ„ÄÇ
DiscoGP ÊàêÂäüÂú∞Ëß£Ê±∫‰∫Ü‰∏äËø∞ÂÖ©ÂÄãÂïèÈ°åÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂø†ÂØ¶Â∫¶„ÄÅÂÆåÊï¥ÊÄßÂíåÁ®ÄÁñèÊÄß„ÄÇË©≤ÊºîÁÆóÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÖ∂Êñ∞Á©éÁöÑÁµêÊßãÁÇ∫Ê∑±ÂÖ•Áû≠Ëß£ÁîüÊàêÂºè AI ÁöÑÂÖßÈÉ®ÈÅã‰ΩúÈñãÈó¢‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇ</paragraph>

##### **BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations**
2407.03314v1 by Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Chaojie Mao, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng

This paper presents Bag-of-Concept Graph (BACON) to gift models with limited
linguistic abilities to taste the privilege of Vision Language Models (VLMs)
and boost downstream tasks such as detection, visual question answering (VQA),
and image generation. Since the visual scenes in physical worlds are structured
with complex relations between objects, BACON breaks down annotations into
basic minimum elements and presents them in a graph structure. Element-wise
style enables easy understanding, and structural composition liberates
difficult locating. Careful prompt design births the BACON captions with the
help of public-available VLMs and segmentation methods. In this way, we gather
a dataset with 100K annotated images, which endow VLMs with remarkable
capabilities, such as accurately generating BACON, transforming prompts into
BACON format, envisioning scenarios in the style of BACONr, and dynamically
modifying elements within BACON through interactive dialogue and more. Wide
representative experiments, including detection, VQA, and image generation
tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel
in their current cutting-edge solutions.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫ Bag-of-Concept Graph (BACON)ÔºåËµã‰∫àËØ≠Ë®ÄËÉΩÂäõÊúâÈôêÁöÑÊ®°ÂûãÂìÅÂ∞ùËßÜËßâËØ≠Ë®ÄÊ®°Âûã (VLM) ÁöÑÁâπÊùÉÔºåÂπ∂ÊèêÂçá‰∏ãÊ∏∏‰ªªÂä°Ôºå‰æãÂ¶ÇÊ£ÄÊµã„ÄÅËßÜËßâÈóÆÁ≠î (VQA) ÂíåÂõæÂÉèÁîüÊàê„ÄÇÁî±‰∫éÁâ©ÁêÜ‰∏ñÁïå‰∏≠ÁöÑËßÜËßâÂú∫ÊôØÊòØÁî±ÂØπË±°‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÊûÑÂª∫ËÄåÊàêÁöÑÔºåÂõ†Ê≠§ BACON Â∞ÜÊ≥®ÈáäÂàÜËß£‰∏∫Âü∫Êú¨ÁöÑÊúÄÂ∞èÂÖÉÁ¥†ÔºåÂπ∂‰ª•ÂõæÂΩ¢ÁªìÊûÑÂëàÁé∞ÂÆÉ‰ª¨„ÄÇÂü∫‰∫éÂÖÉÁ¥†ÁöÑÈ£éÊ†º‰æø‰∫éÁêÜËß£ÔºåÁªìÊûÑÂåñÁªÑÂêàËß£Êîæ‰∫ÜÂõ∞ÈöæÁöÑÂÆö‰Ωç„ÄÇÂú®ÂÖ¨ÂÖ±ÂèØÁî® VLM ÂíåÂàÜÂâ≤ÊñπÊ≥ïÁöÑÂ∏ÆÂä©‰∏ãÔºåÁ≤æÂøÉËÆæËÆ°ÁöÑÊèêÁ§∫ÁîüÊàê‰∫Ü BACON Ê†áÈ¢ò„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊàë‰ª¨Êî∂ÈõÜ‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 100K Âº†Ê≥®ÈáäÂõæÂÉèÁöÑÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜËµã‰∫à VLM ÊòæËëóÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂáÜÁ°ÆÁîüÊàê BACON„ÄÅÂ∞ÜÊèêÁ§∫ËΩ¨Êç¢‰∏∫ BACON Ê†ºÂºè„ÄÅ‰ª• BACONr ÁöÑÈ£éÊ†ºËÆæÊÉ≥Âú∫ÊôØÔºå‰ª•ÂèäÈÄöËøá‰∫§‰∫íÂºèÂØπËØùÂä®ÊÄÅ‰øÆÊîπ BACON ‰∏≠ÁöÑÂÖÉÁ¥†Á≠âÁ≠â„ÄÇÂπøÊ≥õÁöÑ‰ª£Ë°®ÊÄßÂÆûÈ™åÔºåÂåÖÊã¨Ê£ÄÊµã„ÄÅVQA ÂíåÂõæÂÉèÁîüÊàê‰ªªÂä°ÔºåË°®Êòé BACON ‰Ωú‰∏∫‰∏ÄÊù°ÁîüÂëΩÁ∫øÔºåÂèØ‰ª•ÂÆûÁé∞‰ª•ÂâçÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰ªªÂä°ÔºåÊàñÂú®ÂΩìÂâçÁöÑÂ∞ñÁ´ØËß£ÂÜ≥ÊñπÊ°à‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

##### **Knowledge-based Consistency Testing of Large Language Models**
2407.12830v1 by Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay

In this work, we systematically expose and measure the inconsistency and
knowledge gaps of Large Language Models (LLMs). Specifically, we propose an
automated testing framework (called KONTEST) which leverages a knowledge graph
to construct test cases. KONTEST probes and measures the inconsistencies in the
LLM's knowledge of the world via a combination of semantically-equivalent
queries and test oracles (metamorphic or ontological oracle). KONTEST further
mitigates knowledge gaps via a weighted LLM model ensemble. Using four
state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that
KONTEST generates 19.2% error inducing inputs (1917 errors from 9983 test
inputs). It also reveals a 16.5% knowledge gap across all tested LLMs.
KONTEST's mitigation method reduces LLM knowledge gap by 32.48%. Our ablation
study further shows that GPT3.5 is not suitable for knowledge-based consistency
testing because it is only 60%-68% effective in knowledge construction.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Êè≠Èú≤‰∏¶Ë°°ÈáèÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÂíåÁü•Ë≠òÂ∑ÆË∑ù„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñÊ∏¨Ë©¶Ê°ÜÊû∂ (Á®±ÁÇ∫ KONTEST)ÔºåÂÆÉÂà©Áî®Áü•Ë≠òÂúñË≠ú‰æÜÂª∫ÊßãÊ∏¨Ë©¶Ê°à‰æã„ÄÇKONTEST ÈÄöÈÅéË™ûÁæ©Á≠âÊïàÊü•Ë©¢ÂíåÊ∏¨Ë©¶È†êË®Ä (ËÆäÂΩ¢ÊàñÊú¨È´îË´ñÈ†êË®Ä) ÁöÑÁµÑÂêà‰æÜÊé¢Ê∏¨ÂíåË°°Èáè LLM Â∞ç‰∏ñÁïåÁü•Ë≠òÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇKONTEST ÈÄ≤‰∏ÄÊ≠•ÈÄöÈÅéÂä†Ê¨ä LLM Ê®°ÂûãÈõÜÊàê‰æÜÁ∑©Ëß£Áü•Ë≠òÂ∑ÆË∑ù„ÄÇ‰ΩøÁî®ÂõõÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºàFalcon„ÄÅGemini„ÄÅGPT3.5 Âíå Llama2ÔºâÔºåÊàëÂÄëË°®Êòé KONTEST ÁîüÊàê‰∫Ü 19.2% ÁöÑÈåØË™§Ë™òÁôºËº∏ÂÖ•Ôºà9983 ÂÄãÊ∏¨Ë©¶Ëº∏ÂÖ•‰∏≠ÁöÑ 1917 ÂÄãÈåØË™§Ôºâ„ÄÇÂÆÉÈÇÑÊè≠Á§∫‰∫ÜÊâÄÊúâÊ∏¨Ë©¶ÁöÑ LLM ‰∏≠Êúâ 16.5% ÁöÑÁü•Ë≠òÂ∑ÆË∑ù„ÄÇKONTEST ÁöÑÁ∑©Ëß£ÊñπÊ≥ïÂ∞á LLM Áü•Ë≠òÂ∑ÆË∑ùÊ∏õÂ∞ë‰∫Ü 32.48%„ÄÇÊàëÂÄëÁöÑÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Ë°®ÊòéÔºåGPT3.5 ‰∏çÈÅ©ÂêàÁî®ÊñºÂü∫ÊñºÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÊ∏¨Ë©¶ÔºåÂõ†ÁÇ∫ÂÆÉÂú®Áü•Ë≠òÂª∫Êßã‰∏≠Âè™Êúâ 60%-68% ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models**
2407.02936v1 by Zike Yuan, Ming Liu, Hui Wang, Bing Qin

Evaluating the graph comprehension and reasoning abilities of Large Language
Models (LLMs) is challenging and often incomplete. Existing benchmarks focus
primarily on pure graph understanding, lacking a comprehensive evaluation
across all graph types and detailed capability definitions. This paper presents
GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and
reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and
test models on pure graph and heterogeneous graphs, subdividing capabilities
into 10 distinct areas tested through 19 tasks. Our benchmark includes 11
datasets with 5,140 graphs of varying complexity. We evaluated three
closed-source and seven open-source LLMs, conducting thorough analyses from
both ability and task perspectives. Key findings reveal that semantic
enrichment enhances reasoning performance, node ordering impacts task success,
and the ability to process longer texts does not necessarily improve graph
comprehension or reasoning. GraCoRe is open-sourced at
https://github.com/ZIKEYUAN/GraCoRe

ÊëòË¶ÅÔºöË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂúñÂΩ¢ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÈÄöÂ∏∏‰∏çÂÆåÊï¥„ÄÇÁèæÊúâÁöÑÂü∫Ê∫ñ‰∏ªË¶ÅËëóÈáçÊñºÁ¥îÁ≤πÁöÑÂúñÂΩ¢ÁêÜËß£ÔºåÁº∫‰πèÂ∞çÊâÄÊúâÂúñÂΩ¢È°ûÂûãÂíåË©≥Á¥∞ÂäüËÉΩÂÆöÁæ©ÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü GraCoReÔºå‰∏ÄÂÄãÁî®ÊñºÁ≥ªÁµ±Ë©ï‰º∞ LLM ÁöÑÂúñÂΩ¢ÁêÜËß£ÂíåÊé®ÁêÜÁöÑÂü∫Ê∫ñ„ÄÇGraCoRe ‰ΩøÁî®‰∏âÂ±§ÈöéÂ±§ÂàÜÈ°ûÊ≥ïÂ∞çÊ®°ÂûãÈÄ≤Ë°åÂàÜÈ°ûÂíåÊ∏¨Ë©¶ÔºåÂ∞áÂäüËÉΩÁ¥∞ÂàÜÁÇ∫ 10 ÂÄã‰∏çÂêåÁöÑÈ†òÂüüÔºå‰∏¶ÈÄöÈÅé 19 ÂÄã‰ªªÂãôÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÊàëÂÄëÁöÑÂü∫Ê∫ñÂåÖÂê´ 11 ÂÄãÊï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 5,140 ÂÄã‰∏çÂêåË§áÈõúÂ∫¶ÁöÑÂúñÂΩ¢„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏âÂÄãÈñâÊ∫êÂíå‰∏ÉÂÄãÈñãÊ∫ê LLMÔºåÂæûËÉΩÂäõÂíå‰ªªÂãôËßíÂ∫¶ÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑÂàÜÊûê„ÄÇ‰∏ªË¶ÅÁôºÁèæË°®ÊòéË™ûÁæ©Ë±êÂØåÂåñÂ¢ûÂº∑‰∫ÜÊé®ÁêÜÊÄßËÉΩÔºåÁØÄÈªûÊéíÂ∫èÂΩ±Èüø‰ªªÂãôÊàêÂäüÔºåËÄåËôïÁêÜËºÉÈï∑ÊñáÊú¨ÁöÑËÉΩÂäõ‰∏¶‰∏ç‰∏ÄÂÆöËÉΩÊîπÂñÑÂúñÂΩ¢ÁêÜËß£ÊàñÊé®ÁêÜ„ÄÇGraCoRe Âú® https://github.com/ZIKEYUAN/GraCoRe ÈñãÊ∫ê

##### **Croppable Knowledge Graph Embedding**
2407.02779v1 by Yushan Zhu, Wen Zhang, Zhiqiang Liu, Mingyang Chen, Lei Liang, Huajun Chen

Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs)
to serve various artificial intelligence tasks. The suitable dimensions of the
embeddings depend on the storage and computing conditions of the specific
application scenarios. Once a new dimension is required, a new KGE model needs
to be trained from scratch, which greatly increases the training cost and
limits the efficiency and flexibility of KGE in serving various scenarios. In
this work, we propose a novel KGE training framework MED, through which we
could train once to get a croppable KGE model applicable to multiple scenarios
with different dimensional requirements, sub-models of the required dimensions
can be cropped out of it and used directly without any additional training. In
MED, we propose a mutual learning mechanism to improve the low-dimensional
sub-models performance and make the high-dimensional sub-models retain the
capacity that low-dimensional sub-models have, an evolutionary improvement
mechanism to promote the high-dimensional sub-models to master the knowledge
that the low-dimensional sub-models can not learn, and a dynamic loss weight to
balance the multiple losses adaptively. Experiments on 3 KGE models over 4
standard KG completion datasets, 3 real application scenarios over a real-world
large-scale KG, and the experiments of extending MED to the language model BERT
show the effectiveness, high efficiency, and flexible extensibility of MED.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñÂµåÂÖ• (KGE) ÊòØÁü•Ë≠òÂúñ (KG) Áî®ÊñºÊúçÂãôÂêÑÁ®Æ‰∫∫Â∑•Êô∫ÊÖß‰ªªÂãôÁöÑÂ∏∏Ë¶ãÊñπÊ≥ï„ÄÇÂµåÂÖ•ÁöÑÈÅ©Áï∂Á∂≠Â∫¶ÂèñÊ±∫ÊñºÁâπÂÆöÊáâÁî®Â†¥ÊôØÁöÑÂÑ≤Â≠òÂíåÈÅãÁÆóÊ¢ù‰ª∂„ÄÇ‰∏ÄÊó¶ÈúÄË¶ÅÊñ∞ÁöÑÁ∂≠Â∫¶ÔºåÂ∞±ÈúÄË¶ÅÂæûÈ†≠Ë®ìÁ∑¥Êñ∞ÁöÑ KGE Ê®°ÂûãÔºåÈÄôÂ§ßÂ§ßÂ¢ûÂä†‰∫ÜË®ìÁ∑¥ÊàêÊú¨Ôºå‰∏¶ÈôêÂà∂‰∫Ü KGE Âú®ÊúçÂãôÂêÑÁ®ÆÂ†¥ÊôØ‰∏≠ÁöÑÊïàÁéáÂíåÈùàÊ¥ªÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ KGE Ë®ìÁ∑¥Ê°ÜÊû∂ MEDÔºåÈÄöÈÅéÂÆÉÔºåÊàëÂÄëÂèØ‰ª•Ë®ìÁ∑¥‰∏ÄÊ¨°‰ª•Áç≤ÂæóÈÅ©Áî®ÊñºÂÖ∑Êúâ‰∏çÂêåÁ∂≠Â∫¶ÈúÄÊ±ÇÁöÑÂ§öÂÄãÂ†¥ÊôØÁöÑÂèØË£ÅÂâ™ KGE Ê®°ÂûãÔºåÂèØ‰ª•Âæû‰∏≠Ë£ÅÂâ™Âá∫ÊâÄÈúÄÁ∂≠Â∫¶ÁöÑÂ≠êÊ®°Âûã‰∏¶Áõ¥Êé•‰ΩøÁî®ÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÈ°çÂ§ñË®ìÁ∑¥„ÄÇÂú® MED ‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁõ∏‰∫íÂ≠∏ÁøíÊ©üÂà∂Ôºå‰ª•ÊèêÈ´ò‰ΩéÁ∂≠Â≠êÊ®°ÂûãÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÈ´òÁ∂≠Â≠êÊ®°Âûã‰øùÁïô‰ΩéÁ∂≠Â≠êÊ®°ÂûãÂÖ∑ÊúâÁöÑËÉΩÂäõÔºå‰∏ÄÁ®ÆÈÄ≤ÂåñÊîπÈÄ≤Ê©üÂà∂Ôºå‰ª•‰øÉÈÄ≤È´òÁ∂≠Â≠êÊ®°ÂûãÊéåÊè°‰ΩéÁ∂≠Â≠êÊ®°ÂûãÁÑ°Ê≥ïÂ≠∏ÁøíÁöÑÁü•Ë≠òÔºå‰ª•Âèä‰∏ÄÁ®ÆÂãïÊÖãÊêçÂ§±Ê¨äÈáçÔºå‰ª•Ëá™ÈÅ©ÊáâÂú∞Âπ≥Ë°°Â§öÈáçÊêçÂ§±„ÄÇÂú® 4 ÂÄãÊ®ôÊ∫ñ KG ÂÆåÊàêË≥áÊñôÈõÜ‰∏äÁöÑ 3 ÂÄã KGE Ê®°Âûã„ÄÅ‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÂ§ßË¶èÊ®° KG ‰∏äÁöÑ 3 ÂÄãÂØ¶ÈöõÊáâÁî®Â†¥ÊôØ‰ª•ÂèäÂ∞á MED Êì¥Â±ïÂà∞Ë™ûË®ÄÊ®°Âûã BERT ÁöÑÂØ¶È©ó‰∏≠ÔºåÂ±ïÁ§∫‰∫Ü MED ÁöÑÊúâÊïàÊÄß„ÄÅÈ´òÊïàÁéáÂíåÈùàÊ¥ªÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇ

##### **Reasoning in Large Language Models: A Geometric Perspective**
2407.02678v1 by Romain Cosentino, Sarath Shekkizhar

The advancement of large language models (LLMs) for real-world applications
hinges critically on enhancing their reasoning capabilities. In this work, we
explore the reasoning abilities of large language models (LLMs) through their
geometrical understanding. We establish a connection between the expressive
power of LLMs and the density of their self-attention graphs. Our analysis
demonstrates that the density of these graphs defines the intrinsic dimension
of the inputs to the MLP blocks. We demonstrate through theoretical analysis
and toy examples that a higher intrinsic dimension implies a greater expressive
capacity of the LLM. We further provide empirical evidence linking this
geometric framework to recent advancements in methods aimed at enhancing the
reasoning capabilities of LLMs.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÈÄ≤Â±ïÔºåÈóúÈçµÂú®ÊñºÊèêÂçáÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂπæ‰ΩïÁêÜËß£ÔºåÊé¢Ë®éÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü LLM ÁöÑË°®ÈÅîËÉΩÂäõËàáÂÖ∂Ëá™Ê≥®ÊÑèÂäõÂúñÂØÜÂ∫¶‰πãÈñìÁöÑÈóúËÅØ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË≠âÊòéÔºåÈÄô‰∫õÂúñÁöÑÂØÜÂ∫¶ÂÆöÁæ©‰∫Ü MLP Â°äËº∏ÂÖ•ÁöÑÂÖßÂú®Á∂≠Â∫¶„ÄÇÊàëÂÄëÈÄèÈÅéÁêÜË´ñÂàÜÊûêÂíåÁé©ÂÖ∑ÁØÑ‰æãË≠âÊòéÔºåËºÉÈ´òÁöÑÂÖßÂú®Á∂≠Â∫¶ÊÑèÂë≥Ëëó LLM ÂÖ∑ÊúâÊõ¥Â§ßÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êèê‰æõÁ∂ìÈ©óË≠âÊìöÔºåÂ∞áÈÄôÂÄãÂπæ‰ΩïÊ°ÜÊû∂ÈÄ£ÁµêÂà∞ÊúÄËøëÂú®Êó®Âú®Â¢ûÂº∑ LLM Êé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ï‰∏≠ÂèñÂæóÁöÑÈÄ≤Â±ï„ÄÇ

##### **Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison**
2407.02659v1 by Devam Mondal, Carlo Lipizzi

In light of recent plagiarism allegations Brough by publishers, newspapers,
and other creators of copyrighted corpora against large language model (LLM)
developers, we propose a novel system, a variant of a plagiarism detection
system, that assesses whether a knowledge source has been used in the training
or fine-tuning of a large language model. Unlike current methods, we utilize an
approach that uses Resource Description Framework (RDF) triples to create
knowledge graphs from both a source document and a LLM continuation of that
document. These graphs are then analyzed with respect to content using cosine
similarity and with respect to structure using a normalized version of graph
edit distance that shows the degree of isomorphism. Unlike traditional systems
that focus on content matching and keyword identification between a source and
target corpus, our approach enables a broader evaluation of similarity and thus
a more accurate comparison of the similarity between a source document and LLM
continuation by focusing on relationships between ideas and their organization
with regards to others. Additionally, our approach does not require access to
LLM metrics like perplexity that may be unavailable in closed large language
modeling "black-box" systems, as well as the training corpus. A prototype of
our system will be found on a hyperlinked GitHub repository.

ÊëòË¶ÅÔºöÈâ¥‰∫éÂá∫ÁâàÂïÜ„ÄÅÊä•Á∫∏ÂíåÂÖ∂‰ªñÂèóÁâàÊùÉ‰øùÊä§ËØ≠ÊñôÂ∫ìÁöÑÂàõÈÄ†ËÄÖÊúÄËøëÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂºÄÂèëËÄÖÊèêÂá∫ÁöÑÂâΩÁ™ÉÊåáÊéßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÊòØÂâΩÁ™ÉÊ£ÄÊµãÁ≥ªÁªüÁöÑ‰∏Ä‰∏™Âèò‰ΩìÔºåÂÆÉËØÑ‰º∞Áü•ËØÜÊ∫êÊòØÂê¶Â∑≤Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇ‰∏éÂΩìÂâçÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨Âà©Áî®‰∏ÄÁßç‰ΩøÁî®ËµÑÊ∫êÊèèËø∞Ê°ÜÊû∂ (RDF) ‰∏âÂÖÉÁªÑÁöÑÊñπÊ≥ï‰ªéÊ∫êÊñáÊ°£ÂíåËØ•ÊñáÊ°£ÁöÑ LLM Âª∂Áª≠‰∏≠ÂàõÂª∫Áü•ËØÜÂõæË∞±„ÄÇÁÑ∂Âêé‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÊÄßÂàÜÊûêËøô‰∫õÂõæË∞±ÁöÑÂÜÖÂÆπÔºåÂπ∂‰ΩøÁî®ÂõæÁºñËæëË∑ùÁ¶ªÁöÑÊ†áÂáÜÂåñÁâàÊú¨ÂàÜÊûêÁªìÊûÑÔºåËØ•ÁâàÊú¨ÊòæÁ§∫ÂêåÊûÑÂ∫¶„ÄÇ‰∏é‰∏ìÊ≥®‰∫éÊ∫êËØ≠ÊñôÂ∫ìÂíåÁõÆÊ†áËØ≠ÊñôÂ∫ì‰πãÈó¥ÁöÑÂÜÖÂÆπÂåπÈÖçÂíåÂÖ≥ÈîÆËØçËØÜÂà´ÁöÑ‰º†ÁªüÁ≥ªÁªü‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÂØπÁõ∏‰ººÊÄßËøõË°åÊõ¥ÂπøÊ≥õÁöÑËØÑ‰º∞Ôºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ÊØîËæÉÊ∫êÊñáÊ°£Âíå LLM Âª∂Áª≠‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÊñπÊ≥ïÊòØÂÖ≥Ê≥®ÊÄùÊÉ≥‰πãÈó¥ÁöÑÂÖ≥Á≥ª‰ª•ÂèäÂÆÉ‰ª¨‰∏éÂÖ∂‰ªñÊÄùÊÉ≥ÁöÑÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏çÈúÄË¶ÅËÆøÈóÆ LLM ÊåáÊ†áÔºå‰æãÂ¶ÇÂõ∞ÊÉëÂ∫¶ÔºåËøô‰∫õÊåáÊ†áÂú®Â∞ÅÈó≠ÁöÑÂ§ßÂûãËØ≠Ë®ÄÂª∫Ê®°‚ÄúÈªëÂå£Â≠ê‚ÄùÁ≥ªÁªü‰ª•ÂèäËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠ÂèØËÉΩ‰∏çÂèØÁî®„ÄÇÊàë‰ª¨Á≥ªÁªüÁöÑÂéüÂûãÂ∞ÜÂú®Ë∂ÖÈìæÊé•ÁöÑ GitHub Â≠òÂÇ®Â∫ì‰∏≠ÊâæÂà∞„ÄÇ

##### **Multi-Peptide: Multimodality Leveraged Language-Graph Learning of Peptide Properties**
2407.03380v1 by Srivathsan Badrinarayanan, Chakradhar Guntuboina, Parisa Mollaei, Amir Barati Farimani

Peptides are essential in biological processes and therapeutics. In this
study, we introduce Multi-Peptide, an innovative approach that combines
transformer-based language models with Graph Neural Networks (GNNs) to predict
peptide properties. We combine PeptideBERT, a transformer model tailored for
peptide property prediction, with a GNN encoder to capture both sequence-based
and structural features. By employing Contrastive Language-Image Pre-training
(CLIP), Multi-Peptide aligns embeddings from both modalities into a shared
latent space, thereby enhancing the model's predictive accuracy. Evaluations on
hemolysis and nonfouling datasets demonstrate Multi-Peptide's robustness,
achieving state-of-the-art 86.185% accuracy in hemolysis prediction. This study
highlights the potential of multimodal learning in bioinformatics, paving the
way for accurate and reliable predictions in peptide-based research and
applications.

ÊëòË¶ÅÔºöËÇΩÂú®ÁîüÁâ©ÈÅéÁ®ãÂíåÊ≤ªÁôÇ‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ§öËÇΩÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÂü∫ÊñºËΩâÊèõÂô®ÁöÑË™ûË®ÄÊ®°ÂûãÂíåÂúñÁ•ûÁ∂ìÁ∂≤Áµ° (GNN) ‰æÜÈ†êÊ∏¨ËÇΩÁöÑÊÄßË≥™„ÄÇÊàëÂÄëÁµêÂêà‰∫ÜÂ∞àÈñÄÁî®ÊñºËÇΩÊÄßË≥™È†êÊ∏¨ÁöÑËΩâÊèõÂô®Ê®°Âûã PeptideBERT Âíå GNN Á∑®Á¢ºÂô®Ôºå‰ª•ÊçïÁç≤Âü∫ÊñºÂ∫èÂàóÂíåÁµêÊßãÁöÑÁâπÂæµ„ÄÇÈÄöÈÅéÊé°Áî®Â∞çÊØîË™ûË®ÄÂúñÂÉèÈ†êË®ìÁ∑¥ (CLIP)ÔºåÂ§öËÇΩÂ∞á‰æÜËá™ÂÖ©Á®ÆÊ®°ÊÖãÁöÑÂµåÂÖ•Â∞çÈΩäÂà∞‰∏ÄÂÄãÂÖ±‰∫´ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠ÔºåÂæûËÄåÂ¢ûÂº∑Ê®°ÂûãÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÂ∞çÊ∫∂Ë°ÄÂíåÊäóÊ±°Êï∏ÊìöÈõÜÁöÑË©ï‰º∞Ë≠âÊòé‰∫ÜÂ§öËÇΩÁöÑÁ©©ÂÅ•ÊÄßÔºåÂú®Ê∫∂Ë°ÄÈ†êÊ∏¨‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑ 86.185% Ê∫ñÁ¢∫Áéá„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÁîüÁâ©‰ø°ÊÅØÂ≠∏‰∏≠Â§öÊ®°ÊÖãÂ≠∏ÁøíÁöÑÊΩõÂäõÔºåÁÇ∫Âü∫ÊñºËÇΩÁöÑÁ†îÁ©∂ÂíåÊáâÁî®‰∏≠ÁöÑÊ∫ñÁ¢∫‰∏îÂèØÈù†ÁöÑÈ†êÊ∏¨Èã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

##### **Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**
2407.02352v1 by Pritish Sahu, Karan Sikka, Ajay Divakaran

Large Visual Language Models (LVLMs) struggle with hallucinations in visual
instruction following task(s), limiting their trustworthiness and real-world
applicability. We propose Pelican -- a novel framework designed to detect and
mitigate hallucinations through claim verification. Pelican first decomposes
the visual claim into a chain of sub-claims based on first-order predicates.
These sub-claims consist of (predicate, question) pairs and can be
conceptualized as nodes of a computational graph. We then use
Program-of-Thought prompting to generate Python code for answering these
questions through flexible composition of external tools. Pelican improves over
prior work by introducing (1) intermediate variables for precise grounding of
object instances, and (2) shared computation for answering the sub-question to
enable adaptive corrections and inconsistency identification. We finally use
reasoning abilities of LLM to verify the correctness of the the claim by
considering the consistency and confidence of the (question, answer) pairs from
each sub-claim. Our experiments reveal a drop in hallucination rate by
$\sim$8%-32% across various baseline LVLMs and a 27% drop compared to
approaches proposed for hallucination mitigation on MMHal-Bench. Results on two
other benchmarks further corroborate our results.

ÊëòË¶ÅÔºöÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã (LVLMs) Âú®ËßÜËßâÊåá‰ª§ÈÅµÂæ™‰ªªÂä°‰∏≠‰ºö‰∫ßÁîüÂπªËßâÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄßÂíåÁé∞ÂÆû‰∏ñÁïåÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü Pelican‚Äî‚Äî‰∏ÄÁßçÊó®Âú®ÈÄöËøáÂ£∞ÊòéÈ™åËØÅÊù•Ê£ÄÊµãÂíåÂáèËΩªÂπªËßâÁöÑÊñ∞ÂûãÊ°ÜÊû∂„ÄÇPelican È¶ñÂÖàÊ†πÊçÆ‰∏ÄÈò∂Ë∞ìËØçÂ∞ÜËßÜËßâÂ£∞ÊòéÂàÜËß£Êàê‰∏Ä‰∏™Â≠êÂ£∞ÊòéÈìæ„ÄÇËøô‰∫õÂ≠êÂ£∞ÊòéÁî± (Ë∞ìËØç„ÄÅÈóÆÈ¢ò) ÂØπÁªÑÊàêÔºåÂèØ‰ª•Ë¢´Ê¶ÇÂøµÂåñ‰∏∫ËÆ°ÁÆóÂõæÁöÑËäÇÁÇπ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨‰ΩøÁî®ÊÄùÊÉ≥ËÆ°ÂàíÊèêÁ§∫Êù•ÁîüÊàê Python ‰ª£Á†ÅÔºåÈÄöËøáÂ§ñÈÉ®Â∑•ÂÖ∑ÁöÑÁÅµÊ¥ªÁªÑÂêàÊù•ÂõûÁ≠îËøô‰∫õÈóÆÈ¢ò„ÄÇPelican ÈÄöËøáÂºïÂÖ• (1) Áî®‰∫éÂØπË±°ÂÆû‰æãÁ≤æÁ°ÆÊé•Âú∞ÁöÑ‰∏≠Èó¥ÂèòÈáèÔºå‰ª•Âèä (2) Áî®‰∫éÂõûÁ≠îÂ≠êÈóÆÈ¢ò‰ª•ÂÆûÁé∞Ëá™ÈÄÇÂ∫îÊ†°Ê≠£Âíå‰∏ç‰∏ÄËá¥ÊÄßËØÜÂà´ÁöÑÂÖ±‰∫´ËÆ°ÁÆóÔºåÊîπËøõ‰∫Ü‰πãÂâçÁöÑÂ∑•‰Ωú„ÄÇÊàë‰ª¨ÊúÄÁªà‰ΩøÁî® LLM ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈÄöËøáËÄÉËôëÊØè‰∏™Â≠êÂ£∞ÊòéÁöÑ (ÈóÆÈ¢ò„ÄÅÁ≠îÊ°à) ÂØπÁöÑ‰∏ÄËá¥ÊÄßÂíåÁΩÆ‰ø°Â∫¶Êù•È™åËØÅÂ£∞ÊòéÁöÑÊ≠£Á°ÆÊÄß„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®ÂêÑÁßçÂü∫Á∫ø LVLMs ‰∏≠ÔºåÂπªËßâÁéá‰∏ãÈôç‰∫ÜÁ∫¶ 8%-32%Ôºå‰∏é MMHal-Bench ‰∏äÊèêÂá∫ÁöÑÂπªËßâÁºìËß£ÊñπÊ≥ïÁõ∏ÊØîÔºå‰∏ãÈôç‰∫Ü 27%„ÄÇÂú®Âè¶Â§ñ‰∏§‰∏™Âü∫ÂáÜ‰∏äÁöÑÁªìÊûúËøõ‰∏ÄÊ≠•ËØÅÂÆû‰∫ÜÊàë‰ª¨ÁöÑÁªìÊûú„ÄÇ

##### **Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**
2407.01992v1 by Nishant Balepur, Rachel Rudinger

Recent work shows that large language models (LLMs) can answer
multiple-choice questions using only the choices, but does this mean that MCQA
leaderboard rankings of LLMs are largely influenced by abilities in
choices-only settings? To answer this, we use a contrast set that probes if
LLMs over-rely on choices-only shortcuts in MCQA. While previous works build
contrast sets via expensive human annotations or model-generated data which can
be biased, we employ graph mining to extract contrast sets from existing MCQA
datasets. We use our method on UnifiedQA, a group of six commonsense reasoning
datasets with high choices-only accuracy, to build an 820-question contrast
set. After validating our contrast set, we test 12 LLMs, finding that these
models do not exhibit reliance on choice-only shortcuts when given both the
question and choices. Thus, despite the susceptibility~of MCQA to high
choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA
leaderboards just due to their ability to exploit choices-only shortcuts.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰ªÖ‰ΩøÁî®ÈÄâÈ°πÂ∞±ËÉΩÂõûÁ≠îÂ§öÈ°πÈÄâÊã©È¢òÔºå‰ΩÜËøôÊòØÂê¶Ë°®Á§∫Â§öÈ°πÈÄâÊã©ÈóÆÁ≠î (MCQA) ÊéíË°åÊ¶ú‰∏äÁöÑ LLM ‰∏ªË¶ÅÂèóÈôê‰∫é‰ªÖÈÄâÈ°πËÆæÁΩÆ‰∏≠ÁöÑËÉΩÂäõÔºü‰∏∫‰∫ÜÂõûÁ≠îËøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ΩøÁî®ÂØπÊØîÈõÜÊù•Êé¢Êü• LLM Âú® MCQA ‰∏≠ÊòØÂê¶ËøáÂ∫¶‰æùËµñ‰ªÖÈÄâÈ°πÊç∑ÂæÑ„ÄÇËôΩÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄöËøáÊòÇË¥µÁöÑ‰∫∫Â∑•Ê≥®ÈáäÊàñÂèØËÉΩÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÊ®°ÂûãÁîüÊàêÊï∞ÊçÆÊù•ÊûÑÂª∫ÂØπÊØîÈõÜÔºå‰ΩÜÊàë‰ª¨ÈááÁî®ÂõæÊåñÊéò‰ªéÁé∞Êúâ MCQA Êï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÂØπÊØîÈõÜ„ÄÇÊàë‰ª¨‰ΩøÁî®Êàë‰ª¨ÁöÑÊñπÊ≥ïÂú® UnifiedQA ‰∏äÔºåËøôÊòØ‰∏Ä‰∏™Áî±ÂÖ≠‰∏™ÂÖ∑ÊúâÈ´ò‰ªÖÈÄâÈ°πÂáÜÁ°ÆÁéáÁöÑÂ∏∏ËØÜÊé®ÁêÜÊï∞ÊçÆÈõÜÁªÑÊàêÁöÑÁªÑÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ 820 È¢òÁöÑÂØπÊØîÈõÜ„ÄÇÂú®È™åËØÅÊàë‰ª¨ÁöÑÂØπÊØîÈõÜÂêéÔºåÊàë‰ª¨ÊµãËØï‰∫Ü 12 ‰∏™ LLMÔºåÂèëÁé∞ÂΩìÂêåÊó∂ÁªôÂá∫ÈóÆÈ¢òÂíåÈÄâÈ°πÊó∂ÔºåËøô‰∫õÊ®°Âûã‰∏ç‰ºöË°®Áé∞Âá∫ÂØπ‰ªÖÈÄâÈ°πÊç∑ÂæÑÁöÑ‰æùËµñ„ÄÇÂõ†Ê≠§ÔºåÂ∞ΩÁÆ° MCQA ÂÆπÊòìÂèóÂà∞È´ò‰ªÖÈÄâÈ°πÂáÜÁ°ÆÁéáÁöÑÂΩ±ÂìçÔºå‰ΩÜÊàë‰ª¨ËÆ§‰∏∫ LLM Âú® MCQA ÊéíË°åÊ¶ú‰∏äËé∑ÂæóÈ´òÊéíÂêçÂπ∂Èùû‰ªÖ‰ªÖÂõ†‰∏∫ÂÆÉ‰ª¨Âà©Áî®‰ªÖÈÄâÈ°πÊç∑ÂæÑÁöÑËÉΩÂäõ„ÄÇ

##### **CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents**
2407.01511v1 by Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, Philip Torr, Bernard Ghanem, Guohao Li

The development of autonomous agents increasingly relies on Multimodal
Language Models (MLMs) to perform tasks described in natural language with GUI
environments, such as websites, desktop computers, or mobile phones. Existing
benchmarks for MLM agents in interactive environments are limited by their
focus on a single environment, lack of detailed and generalized evaluation
methods, and the complexities of constructing tasks and evaluators. To overcome
these limitations, we introduce Crab, the first agent benchmark framework
designed to support cross-environment tasks, incorporating a graph-based
fine-grained evaluation method and an efficient mechanism for task and
evaluator construction. Our framework supports multiple devices and can be
easily extended to any environment with a Python interface. Leveraging Crab, we
developed a cross-platform Crab Benchmark-v0 comprising 100 tasks in computer
desktop and mobile phone environments. We evaluated four advanced MLMs using
different single and multi-agent system configurations on this benchmark. The
experimental results demonstrate that the single agent with GPT-4o achieves the
best completion ratio of 35.26%. All framework code, agent code, and task
datasets are publicly available at https://github.com/camel-ai/crab.

ÊëòË¶ÅÔºöËá™‰∏ª‰ª£ÁêÜÁöÑÈñãÁôºË∂ä‰æÜË∂ä‰æùË≥¥Â§öÊ®°ÊÖãË™ûË®ÄÊ®°Âûã (MLM)Ôºå‰ª•Âú®ÂÖ∑Êúâ GUI Áí∞Â¢ÉÔºà‰æãÂ¶ÇÁ∂≤Á´ô„ÄÅÊ°å‰∏äÂûãÈõªËÖ¶ÊàñÊâãÊ©üÔºâÁöÑËá™ÁÑ∂Ë™ûË®Ä‰∏≠Âü∑Ë°å‰ªªÂãô„ÄÇÁèæÊúâÁöÑ‰∫íÂãïÁí∞Â¢É‰∏≠ MLM ‰ª£ÁêÜÁöÑÂü∫Ê∫ñÂèóÂà∞‰ª•‰∏ãÈôêÂà∂ÔºöÂÆÉÂÄëÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁí∞Â¢É„ÄÅÁº∫‰πèË©≥Á¥∞‰∏îÈÄöÁî®ÁöÑË©ï‰º∞ÊñπÊ≥ïÔºå‰ª•ÂèäÂª∫Êßã‰ªªÂãôÂíåË©ï‰º∞Âô®ÁöÑË§áÈõúÊÄß„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü CrabÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄã‰ª£ÁêÜÂü∫Ê∫ñÊû∂ÊßãÔºåÊó®Âú®ÊîØÊè¥Ë∑®Áí∞Â¢É‰ªªÂãôÔºå‰∏¶ÁµêÂêà‰∫ÜÂü∫ÊñºÂúñÂΩ¢ÁöÑÁ¥∞Á≤íÂ∫¶Ë©ï‰º∞ÊñπÊ≥ïÂíå‰ªªÂãôËàáË©ï‰º∞Âô®Âª∫ÊßãÁöÑÊúâÊïàÊ©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊîØÊè¥Â§öÁ®ÆË£ùÁΩÆÔºå‰∏¶‰∏îÂèØ‰ª•ËºïÈ¨ÜÂú∞Êì¥ÂÖÖÂà∞‰ªª‰ΩïÂÖ∑Êúâ Python ‰ªãÈù¢ÁöÑÁí∞Â¢É„ÄÇÂà©Áî® CrabÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãË∑®Âπ≥Âè∞ÁöÑ Crab Benchmark-v0ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÈõªËÖ¶Ê°å‰∏äÂûãÈõªËÖ¶ÂíåÊâãÊ©üÁí∞Â¢É‰∏≠ÁöÑ 100 ÂÄã‰ªªÂãô„ÄÇÊàëÂÄë‰ΩøÁî®‰∏çÂêåÁöÑÂñÆ‰∏ÄÂíåÂ§ö‰ª£ÁêÜÁ≥ªÁµ±ÈÖçÁΩÆÔºåÂú®ÈÄôÂÄãÂü∫Ê∫ñ‰∏äË©ï‰º∞‰∫ÜÂõõÁ®ÆÂÖàÈÄ≤ÁöÑ MLM„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂÖ∑Êúâ GPT-4o ÁöÑÂñÆ‰∏Ä‰ª£ÁêÜÂØ¶Áèæ‰∫Ü 35.26% ÁöÑÊúÄ‰Ω≥ÂÆåÊàêÁéá„ÄÇÊâÄÊúâÊû∂ÊßãÁ®ãÂºèÁ¢º„ÄÅ‰ª£ÁêÜÁ®ãÂºèÁ¢ºÂíå‰ªªÂãôË≥áÊñôÈõÜÈÉΩÂÖ¨ÈñãÊñº https://github.com/camel-ai/crab„ÄÇ

##### **Dynamic Few-Shot Learning for Knowledge Graph Question Answering**
2407.01409v1 by Jacopo D'Abramo, Andrea Zugarini, Paolo Torroni

Large language models present opportunities for innovative Question Answering
over Knowledge Graphs (KGQA). However, they are not inherently designed for
query generation. To bridge this gap, solutions have been proposed that rely on
fine-tuning or ad-hoc architectures, achieving good results but limited
out-of-domain distribution generalization. In this study, we introduce a novel
approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the
efficiency of in-context learning and semantic similarity and provides a
generally applicable solution for KGQA with state-of-the-art performance. We
run an extensive evaluation across multiple benchmark datasets and architecture
configurations.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁÇ∫Áü•Ë≠òÂúñË≠úÔºàKGQAÔºâÁöÑÂâµÊñ∞ÂïèÁ≠îÊèê‰æõ‰∫ÜÊ©üÊúÉ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰∏¶ÈùûÂ§©ÁîüÂ∞±Ë®≠Ë®àÁî®ÊñºÊü•Ë©¢ÁîüÊàê„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÂ∑≤ÊèêÂá∫‰æùË≥¥ÊñºÂæÆË™øÊàñÁâπÂÆöÊû∂ÊßãÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÁµêÊûúÔºå‰ΩÜÂüüÂ§ñÂàÜ‰ΩàÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ÂãïÊÖãÂ∞èÊ®£Êú¨Â≠∏ÁøíÔºàDFSLÔºâÁöÑÊñ∞ÊñπÊ≥ï„ÄÇDFSL ÈõÜÊàê‰∫ÜË™ûÂ¢ÉÂ≠∏ÁøíÂíåË™ûÁæ©Áõ∏‰ººÊÄßÁöÑÊïàÁéáÔºå‰∏¶ÁÇ∫ KGQA Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊôÆÈÅçÈÅ©Áî®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂÖ∑ÊúâÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÂ∞çÂ§öÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÂíåÊû∂ÊßãÈÖçÁΩÆÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑË©ï‰º∞„ÄÇ

##### **Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters**
2407.01406v2 by Daniil Gurgurov, Mareike Hartmann, Simon Ostermann

This paper explores the integration of graph knowledge from linguistic
ontologies into multilingual Large Language Models (LLMs) using adapters to
improve performance for low-resource languages (LRLs) in sentiment analysis
(SA) and named entity recognition (NER). Building upon successful
parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we
propose a similar approach for incorporating knowledge from multilingual
graphs, connecting concepts in various languages with each other through
linguistic relationships, into multilingual LLMs for LRLs. Specifically, we
focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,
Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters
fine-tuned on data extracted from the language-specific section of ConceptNet,
aiming to enable knowledge transfer across the languages covered by the
knowledge graph. We compare various fine-tuning objectives, including standard
Masked Language Modeling (MLM), MLM with full-word masking, and MLM with
targeted masking, to analyse their effectiveness in learning and integrating
the extracted graph data. Through empirical evaluation on language-specific
tasks, we assess how structured graph knowledge affects the performance of
multilingual LLMs for LRLs in SA and NER, providing insights into the potential
benefits of adapting language models for low-resource scenarios.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫Ü‰ΩøÁî®ÈÅ©ÈÖçÂô®Â∞á‰æÜËá™Ë™ûË®ÄÂ≠∏Êú¨È´îÁöÑÂúñÂΩ¢Áü•Ë≠òÊï¥ÂêàÂà∞Â§öË™ûË®ÄÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ôºå‰ª•ÊèêÂçá‰ΩéË≥áÊ∫êË™ûË®Ä (LRL) Âú®ÊÉÖÁ∑íÂàÜÊûê (SA) ÂíåÂëΩÂêçÂØ¶È´îË≠òÂà• (NER) ‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂª∫Á´ãÂú®ÊàêÂäüÁöÑÂèÉÊï∏ÊúâÊïàÂæÆË™øÊäÄË°ì‰∏äÔºå‰æãÂ¶Ç K-ADAPTER Âíå MAD-XÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈ°û‰ººÁöÑÂÅöÊ≥ïÔºåÂ∞á‰æÜËá™Â§öË™ûË®ÄÂúñÂΩ¢„ÄÅÈÄèÈÅéË™ûË®ÄÈóú‰øÇÂ∞áÂêÑÁ®ÆË™ûË®Ä‰∏≠ÁöÑÊ¶ÇÂøµÁõ∏‰∫íÈÄ£Êé•ÁöÑÁü•Ë≠òÔºåÁ¥çÂÖ• LRL ÁöÑÂ§öË™ûË®Ä LLM ‰∏≠„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂÖ´Á®Æ LRL‚Äî‚ÄîÈ¶¨Áàæ‰ªñË™û„ÄÅ‰øùÂä†Âà©‰∫ûË™û„ÄÅÂç∞Â∞ºË™û„ÄÅÂ∞ºÊ≥äÁàæË™û„ÄÅÁà™ÂìáË™û„ÄÅÁ∂≠ÂêæÁàæË™û„ÄÅËóèË™ûÂíåÂÉß‰ºΩÁæÖË™û‚Äî‚Äî‰∏¶‰ΩøÁî®Âú®Âæû ConceptNet ÁöÑË™ûË®ÄÁâπÂÆöÈÉ®ÂàÜ‰∏≠ÊèêÂèñÁöÑË≥áÊñô‰∏äÂæÆË™øÁöÑË™ûË®ÄÁâπÂÆöÈÅ©ÈÖçÂô®ÔºåÊó®Âú®ËÆìÁü•Ë≠òËΩâÁßªÂà∞Áü•Ë≠òÂúñÂΩ¢Ê∂µËìãÁöÑË™ûË®Ä‰∏≠„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂêÑÁ®ÆÂæÆË™øÁõÆÊ®ôÔºåÂåÖÊã¨Ê®ôÊ∫ñÁöÑÈÅÆÁΩ©Ë™ûË®ÄÊ®°Âûã (MLM)„ÄÅÂÖ∑ÊúâÂÖ®Ë©ûÈÅÆÁΩ©ÁöÑ MLMÔºå‰ª•ÂèäÂÖ∑ÊúâÁõÆÊ®ôÈÅÆÁΩ©ÁöÑ MLMÔºå‰ª•ÂàÜÊûêÂÆÉÂÄëÂú®Â≠∏ÁøíÂíåÊï¥ÂêàÊèêÂèñÁöÑÂúñÂΩ¢Ë≥áÊñô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÂ∞çË™ûË®ÄÁâπÂÆö‰ªªÂãôÁöÑÂØ¶Ë≠âË©ï‰º∞ÔºåÊàëÂÄëË©ï‰º∞ÁµêÊßãÂåñÂúñÂΩ¢Áü•Ë≠òÂ¶Ç‰ΩïÂΩ±ÈüøÂ§öË™ûË®Ä LLM Âú® LRL ‰∏≠ÁöÑ SA Âíå NER ÊïàËÉΩÔºå‰∏¶Ê∑±ÂÖ•‰∫ÜËß£ÁÇ∫‰ΩéË≥áÊ∫êÂ†¥ÊôØË™øÊï¥Ë™ûË®ÄÊ®°ÂûãÁöÑÊΩõÂú®Â•ΩËôï„ÄÇ

##### **SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model**
2407.01245v2 by Lingyue Fu, Hao Guan, Kounianhua Du, Jianghao Lin, Wei Xia, Weinan Zhang, Ruiming Tang, Yasheng Wang, Yong Yu

Knowledge Tracing (KT) aims to determine whether students will respond
correctly to the next question, which is a crucial task in intelligent tutoring
systems (ITS). In educational KT scenarios, transductive ID-based methods often
face severe data sparsity and cold start problems, where interactions between
individual students and questions are sparse, and new questions and concepts
consistently arrive in the database. In addition, existing KT models only
implicitly consider the correlation between concepts and questions, lacking
direct modeling of the more complex relationships in the heterogeneous graph of
concepts and questions. In this paper, we propose a Structure-aware Inductive
Knowledge Tracing model with large language model (dubbed SINKT), which, for
the first time, introduces large language models (LLMs) and realizes inductive
knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural
relationships between concepts and constructs a heterogeneous graph for
concepts and questions. Secondly, by encoding concepts and questions with LLMs,
SINKT incorporates semantic information to aid prediction. Finally, SINKT
predicts the student's response to the target question by interacting with the
student's knowledge state and the question representation. Experiments on four
real-world datasets demonstrate that SINKT achieves state-of-the-art
performance among 12 existing transductive KT models. Additionally, we explore
the performance of SINKT on the inductive KT task and provide insights into
various modules.

ÊëòË¶ÅÔºöÁü•Ë≠òËøΩËπ§ (KT) ÁöÑÁõÆÁöÑÊòØÁ¢∫ÂÆöÂ≠∏ÁîüÊòØÂê¶ËÉΩÊ≠£Á¢∫ÂõûÁ≠î‰∏ã‰∏ÄÂÄãÂïèÈ°åÔºåÈÄôÂú®Êô∫ÊÖßÂûãÊïôÂ≠∏Á≥ªÁµ± (ITS) ‰∏≠ÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÂú®ÊïôËÇ≤ KT Â†¥ÊôØ‰∏≠ÔºåÂü∫Êñº ID ÁöÑËΩâÂ∞éÊñπÊ≥ïÁ∂ìÂ∏∏Èù¢Ëá®Âö¥ÈáçÁöÑË≥áÊñôÁ®ÄÁñèÊÄßÂíåÂÜ∑ÂïüÂãïÂïèÈ°åÔºåÂÖ∂‰∏≠ÂÄãÂà•Â≠∏ÁîüÂíåÂïèÈ°å‰πãÈñìÁöÑ‰∫íÂãïÂæàÁ®ÄÁñèÔºåËÄå‰∏îÊñ∞ÁöÑÂïèÈ°åÂíåÊ¶ÇÂøµÊúÉÊåÅÁ∫åÂá∫ÁèæÂú®Ë≥áÊñôÂ∫´‰∏≠„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑ KT Ê®°ÂûãÂè™ÊúÉÈö±Âê´Âú∞ËÄÉÊÖÆÊ¶ÇÂøµÂíåÂïèÈ°å‰πãÈñìÁöÑÈóúËÅØÊÄßÔºåÁº∫‰πèÂ∞çÊ¶ÇÂøµÂíåÂïèÈ°åÁï∞Ë≥™Âúñ‰∏≠Êõ¥Ë§áÈõúÈóú‰øÇÁöÑÁõ¥Êé•Âª∫Ê®°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁµêÊßãÊÑüÁü•Ê≠∏Á¥çÁü•Ë≠òËøΩËπ§Ê®°ÂûãÔºàÁ®±ÁÇ∫ SINKTÔºâÔºåÂÆÉÈ¶ñÊ¨°ÂºïÂÖ•‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÔºå‰∏¶ÂØ¶Áèæ‰∫ÜÊ≠∏Á¥çÁü•Ë≠òËøΩËπ§„ÄÇÈ¶ñÂÖàÔºåSINKT Âà©Áî® LLM ÂºïÂÖ•Ê¶ÇÂøµ‰πãÈñìÁöÑÁµêÊßãÈóú‰øÇÔºå‰∏¶ÁÇ∫Ê¶ÇÂøµÂíåÂïèÈ°åÊßãÂª∫‰∫Ü‰∏ÄÂÄãÁï∞Ë≥™Âúñ„ÄÇÂÖ∂Ê¨°ÔºåÈÄèÈÅé‰ΩøÁî® LLM Á∑®Á¢ºÊ¶ÇÂøµÂíåÂïèÈ°åÔºåSINKT ÁµêÂêà‰∫ÜË™ûÁæ©Ë≥áË®äÔºå‰ª•ÂçîÂä©È†êÊ∏¨„ÄÇÊúÄÂæåÔºåSINKT ÈÄèÈÅéËàáÂ≠∏ÁîüÁöÑÁü•Ë≠òÁãÄÊÖãÂíåÂïèÈ°åË°®ÂæµÈÄ≤Ë°å‰∫íÂãïÔºåÈ†êÊ∏¨Â≠∏ÁîüÂ∞çÁõÆÊ®ôÂïèÈ°åÁöÑÂõûÊáâ„ÄÇÂú®ÂõõÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåSINKT Âú® 12 ÂÄãÁèæÊúâÁöÑËΩâÂ∞é KT Ê®°Âûã‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü SINKT Âú®Ê≠∏Á¥ç KT ‰ªªÂãô‰∏äÁöÑÊïàËÉΩÔºå‰∏¶Êèê‰æõ‰∫ÜÂ∞çÂêÑÁ®ÆÊ®°ÁµÑÁöÑË¶ãËß£„ÄÇ

##### **Revisiting Random Walks for Learning on Graphs**
2407.01214v1 by Jinwoo Kim, Olga Zaghen, Ayhan Suleymanzade, Youngmin Ryou, Seunghoon Hong

We revisit a simple idea for machine learning on graphs, where a random walk
on a graph produces a machine-readable record, and this record is processed by
a deep neural network to directly make vertex-level or graph-level predictions.
We refer to these stochastic machines as random walk neural networks, and show
that we can design them to be isomorphism invariant while capable of universal
approximation of graph functions in probability. A useful finding is that
almost any kind of record of random walk guarantees probabilistic invariance as
long as the vertices are anonymized. This enables us to record random walks in
plain text and adopt a language model to read these text records to solve graph
tasks. We further establish a parallelism to message passing neural networks
using tools from Markov chain theory, and show that over-smoothing in message
passing is alleviated by construction in random walk neural networks, while
over-squashing manifests as probabilistic under-reaching. We show that random
walk neural networks based on pre-trained language models can solve several
hard problems on graphs, such as separating strongly regular graphs where the
3-WL test fails, counting substructures, and transductive classification on
arXiv citation network without training. Code is available at
https://github.com/jw9730/random-walk.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÈáçÊñ∞ÂØ©Ë¶ñÂúñÂΩ¢Ê©üÂô®Â≠∏ÁøíÁöÑ‰∏ÄÂÄãÁ∞°ÂñÆÊÉ≥Ê≥ïÔºåÂÖ∂‰∏≠ÂúñÂΩ¢‰∏äÁöÑÈö®Ê©üÈÅäËµ∞ÊúÉÁî¢ÁîüÊ©üÂô®ÂèØËÆÄÁöÑË®òÈåÑÔºåËÄåÈÄôÂÄãË®òÈåÑÊúÉÁî±Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØËôïÁêÜÔºå‰ª•Áõ¥Êé•ÈÄ≤Ë°åÈ†ÇÈªûÂ±§Á¥öÊàñÂúñÂΩ¢Â±§Á¥öÁöÑÈ†êÊ∏¨„ÄÇÊàëÂÄëÂ∞áÈÄô‰∫õÈö®Ê©üÊ©üÂô®Á®±ÁÇ∫Èö®Ê©üÈÅäËµ∞Á•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰∏¶Â±ïÁ§∫ÊàëÂÄëÂèØ‰ª•Â∞áÂÆÉÂÄëË®≠Ë®àÊàêÂêåÊßã‰∏çËÆäÔºåÂêåÊôÇÂÖ∑ÂÇôÊ©üÁéá‰∏≠ÂúñÂΩ¢ÂáΩÊï∏ÁöÑÈÄöÁî®Ëøë‰ººËÉΩÂäõ„ÄÇ‰∏ÄÂÄãÊúâÁî®ÁöÑÁôºÁèæÊòØÔºåÂè™Ë¶ÅÈ†ÇÈªûÊòØÂåøÂêçÁöÑÔºåÂπæ‰πé‰ªª‰ΩïÈ°ûÂûãÁöÑÈö®Ê©üÈÅäËµ∞Ë®òÈåÑÈÉΩÂèØ‰ª•‰øùË≠âÊ©üÁéá‰∏çËÆäÊÄß„ÄÇÈÄô‰ΩøÊàëÂÄëËÉΩÂ§†‰ª•Á¥îÊñáÂ≠óË®òÈåÑÈö®Ê©üÈÅäËµ∞Ôºå‰∏¶Êé°Áî®Ë™ûË®ÄÊ®°Âûã‰æÜËÆÄÂèñÈÄô‰∫õÊñáÂ≠óË®òÈåÑÔºå‰ª•Ëß£Ê±∫ÂúñÂΩ¢‰ªªÂãô„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âª∫Á´ã‰∫Ü‰∏ÄÂÄãËàáË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂπ≥Ë°åÊÄßÔºå‰ΩøÁî®È¶¨ÂèØÂ§´ÈèàÁêÜË´ñÁöÑÂ∑•ÂÖ∑Ôºå‰∏¶Â±ïÁ§∫Ë®äÊÅØÂÇ≥ÈÅû‰∏≠ÁöÑÈÅéÂ∫¶Âπ≥ÊªëÊúÉÂõ†Èö®Ê©üÈÅäËµ∞Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊßãÈÄ†ËÄåÂæóÂà∞Á∑©Ëß£ÔºåËÄåÈÅéÂ∫¶Â£ìÁ∏ÆÂâáË°®ÁèæÁÇ∫Ê©üÁéáÊÄß‰∏çË∂≥„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂü∫ÊñºÈ†êÂÖàË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÈö®Ê©üÈÅäËµ∞Á•ûÁ∂ìÁ∂≤Ë∑ØÂèØ‰ª•Ëß£Ê±∫ÂúñÂΩ¢‰∏äÁöÑÂπæÂÄãÂõ∞Èõ£ÂïèÈ°åÔºå‰æãÂ¶ÇÂàÜÈõ¢ 3-WL Ê∏¨Ë©¶Â§±ÊïóÁöÑÂº∑Ê≠£ÂâáÂúñÂΩ¢„ÄÅË®àÁÆóÂ≠êÁµêÊßãÔºå‰ª•ÂèäÂú® arXiv ÂºïÊñáÁ∂≤Ë∑Ø‰∏≠ÈÄ≤Ë°åËΩâÂ∞éÂàÜÈ°ûÔºåËÄåÁÑ°ÈúÄË®ìÁ∑¥„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/jw9730/random-walk ÂèñÂæó„ÄÇ</paragraph>

##### **LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation**
2407.00994v2 by Longchao Da, Tiejin Chen, Lu Cheng, Hua Wei

The Large language models (LLMs) have showcased superior capabilities in
sophisticated tasks across various domains, stemming from basic question-answer
(QA), they are nowadays used as decision assistants or explainers for
unfamiliar content. However, they are not always correct due to the data
sparsity in specific domain corpus, or the model's hallucination problems.
Given this, how much should we trust the responses from LLMs? This paper
presents a novel way to evaluate the uncertainty that captures the directional
instability, by constructing a directional graph from entailment probabilities,
and we innovatively conduct Random Walk Laplacian given the asymmetric property
of a constructed directed graph, then the uncertainty is aggregated by the
derived eigenvalues from the Laplacian process. We also provide a way to
incorporate the existing work's semantics uncertainty with our proposed layer.
Besides, this paper identifies the vagueness issues in the raw response set and
proposes an augmentation approach to mitigate such a problem, we conducted
extensive empirical experiments and demonstrated the superiority of our
proposed solutions.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÂÄãÈ†òÂüüÁöÑË§áÈõú‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑËÉΩÂäõÔºåÂæûÂü∫Êú¨ÁöÑÂïèÁ≠î (QA) ÈñãÂßãÔºåÂÆÉÂÄëÁèæÂú®Ë¢´Áî®‰ΩúÊ±∫Á≠ñÂä©ÁêÜÊàñ‰∏çÁÜüÊÇâÂÖßÂÆπÁöÑË™™ÊòéËÄÖ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰∏¶‰∏çÁ∏ΩÊòØÊ≠£Á¢∫ÁöÑÔºåÂõ†ÁÇ∫ÁâπÂÆöÈ†òÂüüË™ûÊñôÂ∫´‰∏≠ÁöÑÊï∏ÊìöÁ®ÄÁñèÔºåÊàñÊ®°ÂûãÁöÑÂπªË¶∫ÂïèÈ°å„ÄÇÊúâÈëëÊñºÊ≠§ÔºåÊàëÂÄëÊáâË©≤Â§öÁõ∏‰ø° LLM ÁöÑÂõûÊáâÔºüÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ï‰æÜË©ï‰º∞ÊçïÊçâÊñπÂêë‰∏çÁ©©ÂÆöÊÄßÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÈÄöÈÅéÂæûËòäÊ∂µÊ¶ÇÁéáÊßãÈÄ†‰∏ÄÂÄãÊúâÂêëÂúñÔºå‰∏¶‰∏îÊàëÂÄëÂâµÊñ∞Âú∞ÈÄ≤Ë°åÈö®Ê©üÈÅäËµ∞ÊãâÊôÆÊãâÊñØÁÆóÂ≠êÔºåÁµ¶ÂÆö‰∏ÄÂÄãÊßãÈÄ†ÁöÑÊúâÂêëÂúñÁöÑ‰∏çÂ∞çÁ®±Â±¨ÊÄßÔºåÁÑ∂Âæå‰∏çÁ¢∫ÂÆöÊÄßÁî±ÊãâÊôÆÊãâÊñØÈÅéÁ®ã‰∏≠ÁöÑÂ∞éÂá∫ÁâπÂæµÂÄºËÅöÂêà„ÄÇÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂ∞áÁèæÊúâÂ∑•‰ΩúÁöÑË™ûÁæ©‰∏çÁ¢∫ÂÆöÊÄßËàáÊàëÂÄëÊèêÂá∫ÁöÑÂ±§ÁµêÂêàËµ∑‰æÜÁöÑÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáË≠òÂà•‰∫ÜÂéüÂßãÂõûÊáâÈõÜ‰∏≠Ê®°Á≥äÁöÑÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊì¥ÂÖÖÊñπÊ≥ï‰æÜÊ∏õËºïÈÄôÁ®ÆÂïèÈ°åÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶Ë≠âÂØ¶È©óÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÂÑ™Ë∂äÊÄß„ÄÇ

##### **Actionable Cyber Threat Intelligence using Knowledge Graphs and Large Language Models**
2407.02528v1 by Romy Fieblinger, Md Tanvirul Alam, Nidhi Rastogi

Cyber threats are constantly evolving. Extracting actionable insights from
unstructured Cyber Threat Intelligence (CTI) data is essential to guide
cybersecurity decisions. Increasingly, organizations like Microsoft, Trend
Micro, and CrowdStrike are using generative AI to facilitate CTI extraction.
This paper addresses the challenge of automating the extraction of actionable
CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs
(KGs). We explore the application of state-of-the-art open-source LLMs,
including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting
meaningful triples from CTI texts. Our methodology evaluates techniques such as
prompt engineering, the guidance framework, and fine-tuning to optimize
information extraction and structuring. The extracted data is then utilized to
construct a KG, offering a structured and queryable representation of threat
intelligence. Experimental results demonstrate the effectiveness of our
approach in extracting relevant information, with guidance and fine-tuning
showing superior performance over prompt engineering. However, while our
methods prove effective in small-scale tests, applying LLMs to large-scale data
for KG construction and Link Prediction presents ongoing challenges.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØÂ®ÅËÑÖ‰∏çÊñ∑ÊºîËÆä„ÄÇÂæûÈùûÁµêÊßãÂåñÁöÑÁ∂≤Ë∑ØÂ®ÅËÑÖÊÉÖÂ†± (CTI) Ë≥áÊñô‰∏≠ËêÉÂèñÂèØÊé°ÂèñË°åÂãïÁöÑË¶ãËß£ÔºåÂ∞çÊñºÂºïÂ∞éÁ∂≤Ë∑ØÂÆâÂÖ®Ê±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇË∂ä‰æÜË∂äÂ§öÁµÑÁπîÔºå‰æãÂ¶Ç Microsoft„ÄÅË∂®Âã¢ÁßëÊäÄÂíå CrowdStrikeÔºå‰ΩøÁî®ÁîüÊàêÂºè AI ‰æÜ‰øÉÈÄ≤ CTI ËêÉÂèñ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫Ü‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÈÄ≤Â±ïÔºåËá™ÂãïËêÉÂèñÂèØÊé°ÂèñË°åÂãïÁöÑ CTI ÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÈñãÊ∫ê LLM ÁöÑÊáâÁî®ÔºåÂåÖÊã¨ Llama 2 Á≥ªÂàó„ÄÅMistral 7B Instruct Âíå ZephyrÔºå‰ª•Âæû CTI ÊñáÂ≠ó‰∏≠ËêÉÂèñÊúâÊÑèÁæ©ÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË©ï‰º∞‰∫ÜÊèêÁ§∫Â∑•Á®ã„ÄÅÊåáÂ∞éÊû∂ÊßãÂíåÂæÆË™øÁ≠âÊäÄË°ìÔºå‰ª•ÊúÄ‰Ω≥ÂåñË≥áË®äËêÉÂèñÂíåÁµêÊßãÂåñ„ÄÇÁÑ∂ÂæåÔºåÂ∞áËêÉÂèñÁöÑË≥áÊñôÁî®ÊñºÂª∫Êßã KGÔºåÊèê‰æõÂ®ÅËÑÖÊÉÖÂ†±ÁöÑÁµêÊßãÂåñ‰∏îÂèØÊü•Ë©¢ÁöÑË°®Á§∫„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÂú®ËêÉÂèñÁõ∏ÈóúË≥áË®äÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊåáÂ∞éÂíåÂæÆË™øÈ°ØÁ§∫Âá∫ÂÑ™ÊñºÊèêÁ§∫Â∑•Á®ãÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈõñÁÑ∂ÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Â∞èË¶èÊ®°Ê∏¨Ë©¶‰∏≠Ë≠âÊòéÊúâÊïàÔºå‰ΩÜÂ∞á LLM ÊáâÁî®ÊñºÂ§ßË¶èÊ®°Ë≥áÊñô‰ª•ÈÄ≤Ë°å KG Âª∫ÊßãÂíåÈÄ£ÁµêÈ†êÊ∏¨Ôºå‰ªçÂ≠òÂú®ÊåÅÁ∫åÁöÑÊåëÊà∞„ÄÇ

##### **Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs**
2407.00653v1 by Yifei Zhang, Xintao Wang, Jiaqing Liang, Sirui Xia, Lida Chen, Yanghua Xiao

Large Language Models (LLMs) have exhibited impressive proficiency in various
natural language processing (NLP) tasks, which involve increasingly complex
reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving
new knowledge from existing one.While it has been widely studied in the context
of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored.
In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for
knowledge reasoning, including methodologies for both dataset construction and
model learning. For dataset construction, we create KnowReason via rule mining
on KGs. For model learning, we observe rule overfitting induced by naive
training. Hence, we enhance CoK with a trial-and-error mechanism that simulates
the human process of internal knowledge exploration. We conduct extensive
experiments with KnowReason. Our results show the effectiveness of CoK in
refining LLMs in not only knowledge reasoning, but also general reasoning
benchmarkms.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫È©ö‰∫∫ÁöÑËÉΩÂäõÔºåÈÄô‰∫õ‰ªªÂãôÊ∂âÂèäË∂ä‰æÜË∂äË§áÈõúÁöÑÊé®ÁêÜ„ÄÇÁü•Ë≠òÊé®ÁêÜ‰ΩúÁÇ∫Êé®ÁêÜÁöÑ‰∏ªË¶ÅÈ°ûÂûãÔºåÊó®Âú®ÂæûÊó¢ÊúâÁü•Ë≠ò‰∏≠Êé®Â∞éÂá∫Êñ∞Áü•Ë≠ò„ÄÇÂÑòÁÆ°Áü•Ë≠òÊé®ÁêÜÂ∑≤Âú®Áü•Ë≠òÂúñË≠ú (KG) ÁöÑËÉåÊôØ‰∏ãÂæóÂà∞Âª£Ê≥õÁ†îÁ©∂Ôºå‰ΩÜ LLM ‰∏≠ÁöÑÁü•Ë≠òÊé®ÁêÜ‰ªçËôïÊñºÊé¢Á¥¢ÈöéÊÆµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÁü•Ë≠òÊé®ÁêÜÁöÑÁ∂úÂêàÊ°ÜÊû∂Áü•Ë≠òÈèàÔºåÂÖ∂‰∏≠ÂåÖÊã¨Áî®ÊñºË≥áÊñôÈõÜÊßãÂª∫ÂíåÊ®°ÂûãÂ≠∏ÁøíÁöÑÊñπÊ≥ï„ÄÇÂ∞çÊñºË≥áÊñôÈõÜÊßãÂª∫ÔºåÊàëÂÄëÈÄèÈÅéÂú® KG ‰∏≠ÈÄ≤Ë°åË¶èÂâáÊåñÊéò‰æÜÂª∫Á´ã KnowReason„ÄÇÂ∞çÊñºÊ®°ÂûãÂ≠∏ÁøíÔºåÊàëÂÄëËßÄÂØüÂà∞Áî±Â§©ÁúüË®ìÁ∑¥ÂºïÁôºÁöÑË¶èÂâáÈÅéÂ∫¶Êì¨Âêà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰ΩøÁî®Ê®°Êì¨‰∫∫È°ûÂÖßÈÉ®Áü•Ë≠òÊé¢Á¥¢ÈÅéÁ®ãÁöÑË©¶ÈåØÊ©üÂà∂‰æÜÂ¢ûÂº∑ CoK„ÄÇÊàëÂÄëÂ∞ç KnowReason ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©ó„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ CoK Âú®Á≤æÁÖâ LLM ‰∏çÂÉÖÂú®Áü•Ë≠òÊé®ÁêÜÊñπÈù¢ÔºåÈÇÑÂåÖÊã¨‰∏ÄËà¨Êé®ÁêÜÂü∫Ê∫ñÊñπÈù¢ÈÉΩÈùûÂ∏∏ÊúâÊïà„ÄÇ

##### **BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science**
2407.00466v1 by Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z. Li, Kaicheng Yu

Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,
draws increasing attention, where one common approach is to build a copilot
agent driven by Large Language Models (LLMs). However, to evaluate such
systems, people either rely on direct Question-Answering (QA) to the LLM
itself, or in a biomedical experimental manner. How to precisely benchmark
biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of
scientists, understanding the literature, and introduce BioKGBench. In contrast
to traditional evaluation benchmark that only focuses on factual QA, where the
LLMs are known to have hallucination issues, we first disentangle
"Understanding Literature" into two atomic abilities, i) "Understanding" the
unstructured text from research papers by performing scientific claim
verification, and ii) Ability to interact with structured Knowledge-Graph
Question-Answering (KGQA) as a form of "Literature" grounding. We then
formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based
Retrieval-Augmented Generation (RAG) to identify the factual errors of existing
large-scale knowledge graph databases. We collect over two thousand data for
two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios
and biomedical ones, have either failed or inferior performance on our
benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors
which provide scenarios for agents to make discoveries and demonstrate the
effectiveness of our approach. The code and data are available at
https://github.com/westlake-autolab/BioKGBench.

ÊëòË¶ÅÔºö<paragraph>ËøΩÊ±ÇÁîüÁâ©ÈÜ´Â≠∏ÁßëÂ≠∏ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÔºåÂèàÁ®± AI ÁßëÂ≠∏ÂÆ∂Ôºå
Ë∂ä‰æÜË∂äÂèóÂà∞ÈóúÊ≥®ÔºåÂÖ∂‰∏≠‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÊñπÊ≥ïÊòØÂª∫Á´ãÁî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È©ÖÂãïÁöÑÂâØÈßïÈßõ‰ª£ÁêÜ„ÄÇÁÑ∂ËÄåÔºåË¶ÅË©ï‰º∞Ê≠§È°û
Á≥ªÁµ±Ôºå‰∫∫ÂÄëË¶Å‰πà‰æùË≥¥ LLM Êú¨Ë∫´ÁöÑÁõ¥Êé•ÂïèÁ≠î (QA)ÔºåË¶Å‰πà‰æùË≥¥ÁîüÁâ©ÈÜ´Â≠∏ÂØ¶È©óÊñπÂºè„ÄÇÂ¶Ç‰ΩïÂæû AI ÁßëÂ≠∏ÂÆ∂ÁöÑËßíÂ∫¶Á≤æÁ¢∫Ë©ïÈáè
ÁîüÁâ©ÈÜ´Â≠∏‰ª£ÁêÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™Êé¢Á¥¢„ÄÇ
ÁÇ∫Ê≠§ÔºåÊàëÂÄëÂæûÁßëÂ≠∏ÂÆ∂ÊúÄÈáçË¶ÅÁöÑËÉΩÂäõ‰πã‰∏ÄÔºåÂç≥ÁêÜËß£ÊñáÁçª‰∏≠Ê±≤ÂèñÈùàÊÑüÔºå‰∏¶‰ªãÁ¥π BioKGBench„ÄÇËàáÂÉÖÈóúÊ≥®‰∫ãÂØ¶ QA ÁöÑÂÇ≥Áµ±Ë©ïÈáèÂü∫Ê∫ñ‰∏çÂêåÔºåÂ∑≤Áü• LLM Âú®‰∫ãÂØ¶ QA ‰∏≠Â≠òÂú®ÂπªË¶∫ÂïèÈ°åÔºåÊàëÂÄëÈ¶ñÂÖàÂ∞á
„ÄåÁêÜËß£ÊñáÁçª„ÄçÂàÜËß£ÁÇ∫ÂÖ©Á®ÆÂü∫Êú¨ËÉΩÂäõÔºåi) ÈÄèÈÅéÂü∑Ë°åÁßëÂ≠∏‰∏ªÂºµÈ©óË≠â‰æÜ„ÄåÁêÜËß£„ÄçÁ†îÁ©∂Ë´ñÊñá‰∏≠ÁöÑÈùûÁµêÊßãÂåñÊñáÂ≠óÔºå‰ª•Âèä ii) ‰ª•„ÄåÊñáÁçª„ÄçÁÇ∫Âü∫Á§éÔºåËàáÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË°®ÂïèÁ≠î (KGQA) ‰∫íÂãïÁöÑËÉΩÂäõ„ÄÇÁÑ∂Âæå
ÊàëÂÄë‰ΩøÁî® KGQA ÂíåÂü∫ÊñºÁ∂≤ÂüüÁöÑÊ™¢Á¥¢Êì¥ÂÖÖÁî¢Áîü (RAG) Âà∂ÂÆö‰∫Ü‰∏ÄÈ†ÖÊñ∞Á©éÁöÑ‰ª£ÁêÜ‰ªªÂãôÔºåÁ®±ÁÇ∫ KGCheckÔºå‰ª•Ë≠òÂà•ÁèæÊúâÂ§ßÂûãÁü•Ë≠òÂúñË°®Ë≥áÊñôÂ∫´ÁöÑ‰∫ãÂØ¶ÈåØË™§„ÄÇÊàëÂÄëÁÇ∫
ÂÖ©ÂÄãÂü∫Êú¨‰ªªÂãôÊî∂ÈõÜ‰∫ÜÂÖ©ÂçÉÂ§öÂÄãË≥áÊñôÔºå‰ª•Âèä 225 ÂÄãÈ´òÂìÅË≥™Ë®ªËß£Ë≥áÊñôÔºå‰ª•‰ΩúÁÇ∫‰ª£ÁêÜ‰ªªÂãô„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæÊúÄÂÖàÈÄ≤ÁöÑ‰ª£ÁêÜÔºåÁÑ°Ë´ñÊòØÊó•Â∏∏ÊÉÖÂ¢ÉÈÇÑÊòØÁîüÁâ©ÈÜ´Â≠∏ÔºåÂú®ÊàëÂÄëÁöÑ
Âü∫Ê∫ñ‰∏äÈÉΩË°®Áèæ‰∏ç‰Ω≥ÊàñË°®ÁèæËºÉÂ∑Æ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂü∫Ê∫ñÔºåÁ®±ÁÇ∫ BKGAgent„ÄÇÂú®Âª£Ê≥õ‰ΩøÁî®ÁöÑÁÜ±ÈñÄÁü•Ë≠òÂúñË°®‰∏äÔºåÊàëÂÄëÁôºÁèæË∂ÖÈÅé 90 ÂÄã‰∫ãÂØ¶ÈåØË™§ÔºåÈÄô‰∫õÈåØË™§ÁÇ∫‰ª£ÁêÜÊèê‰æõ‰∫ÜÁôºÁèæÊÉÖÂ¢ÉÔºå‰∏¶Ë≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú®
https://github.com/westlake-autolab/BioKGBench ÂèñÂæó„ÄÇ</paragraph>

##### **GraphArena: Benchmarking Large Language Models on Graph Computational Problems**
2407.00379v1 by Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li

The "arms race" of Large Language Models (LLMs) demands novel, challenging,
and diverse benchmarks to faithfully examine their progresses. We introduce
GraphArena, a benchmarking tool designed to evaluate LLMs on graph
computational problems using million-scale real-world graphs from diverse
scenarios such as knowledge graphs, social networks, and molecular structures.
GraphArena offers a suite of 10 computational tasks, encompassing four
polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g.,
Travelling Salesman Problem). It features a rigorous evaluation framework that
classifies LLM outputs as correct, suboptimal (feasible but not optimal), or
hallucinatory (properly formatted but infeasible). Evaluation of 10 leading
LLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even
top-performing models struggle with larger, more complex graph problems and
exhibit hallucination issues. Despite the application of strategies such as
chain-of-thought prompting, these issues remain unresolved. GraphArena
contributes a valuable supplement to the existing LLM benchmarks and is
open-sourced at https://github.com/squareRoot3/GraphArena.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ„ÄåËªçÂÇôÁ´∂Ë≥Ω„ÄçÈúÄË¶ÅÊñ∞Á©é„ÄÅÂÖ∑ÊåëÊà∞ÊÄß‰∏îÂ§öÊ®£ÂåñÁöÑÂü∫Ê∫ñ‰æÜÂø†ÂØ¶Ê™¢È©óÂÖ∂ÈÄ≤Â∫¶„ÄÇÊàëÂÄëÊé®Âá∫ GraphArenaÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Ê∫ñÂ∑•ÂÖ∑ÔºåÊó®Âú®‰ΩøÁî®‰æÜËá™Áü•Ë≠òÂúñË≠ú„ÄÅÁ§æ‰∫§Á∂≤Ë∑ØÂíåÂàÜÂ≠êÁµêÊßãÁ≠âÂ§öÊ®£ÂåñÊÉÖÂ¢ÉÁöÑÊï∏ÁôæËê¨ÂÄãÁúüÂØ¶‰∏ñÁïåÂúñÂΩ¢ÔºåÈáùÂ∞çÂúñÂΩ¢Ë®àÁÆóÂïèÈ°åË©ï‰º∞ LLM„ÄÇGraphArena Êèê‰æõ‰∏ÄÁ≥ªÂàó 10 ÂÄãË®àÁÆó‰ªªÂãôÔºåÂåÖÂê´ÂõõÂÄãÂ§öÈ†ÖÂºèÊôÇÈñìÔºà‰æãÂ¶ÇÔºåÊúÄÁü≠Ë∑ùÈõ¢ÔºâÂíåÂÖ≠ÂÄã NP ÂÆåÂÖ®ÊåëÊà∞Ôºà‰æãÂ¶ÇÔºåÊóÖË°åÊé®Èä∑Âì°ÂïèÈ°åÔºâ„ÄÇÂÆÉÂÖ∑Êúâ‰∏ÄÂÄãÂö¥Ë¨πÁöÑË©ï‰º∞Êû∂ÊßãÔºåÂ∞á LLM Ëº∏Âá∫ÂàÜÈ°ûÁÇ∫Ê≠£Á¢∫„ÄÅÊ¨°‰Ω≥ÔºàÂèØË°å‰ΩÜÈùûÊúÄ‰Ω≥ÔºâÊàñÂπªË¶∫ÔºàÊ†ºÂºèÊ≠£Á¢∫‰ΩÜ‰∏çÂèØË°åÔºâ„ÄÇÂ∞çÂåÖÊã¨ GPT-4o Âíå LLaMA3-70B-Instruct Âú®ÂÖßÁöÑ 10 ÂÄãÈ†òÂÖà LLM ÁöÑË©ï‰º∞È°ØÁ§∫ÔºåÂç≥‰ΩøÊòØÊïàËÉΩÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÂú®ËôïÁêÜÊõ¥Â§ß„ÄÅÊõ¥Ë§áÈõúÁöÑÂúñÂΩ¢ÂïèÈ°åÊôÇ‰ªçÊúÉÈÅáÂà∞Âõ∞Èõ£Ôºå‰∏¶Âá∫ÁèæÂπªË¶∫ÂïèÈ°å„ÄÇÂÑòÁÆ°ÊáâÁî®‰∫Ü‰∏ÄÁ≥ªÂàóÁ≠ñÁï•Ôºå‰æãÂ¶ÇÊÄùËÄÉÈèàÊèêÁ§∫ÔºåÈÄô‰∫õÂïèÈ°å‰ªçÊú™Ëß£Ê±∫„ÄÇGraphArena ÁÇ∫ÁèæÊúâÁöÑ LLM Âü∫Ê∫ñÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË£úÂÖÖÔºå‰∏¶Âú® https://github.com/squareRoot3/GraphArena ÈñãÊ∫ê„ÄÇ

##### **Teola: Towards End-to-End Optimization of LLM-based Applications**
2407.00326v1 by Xin Tan, Yimin Jiang, Yitao Yang, Hong Xu

Large language model (LLM)-based applications consist of both LLM and non-LLM
components, each contributing to the end-to-end latency. Despite great efforts
to optimize LLM inference, end-to-end workflow optimization has been
overlooked. Existing frameworks employ coarse-grained orchestration with task
modules, which confines optimizations to within each module and yields
suboptimal scheduling decisions. We propose fine-grained end-to-end
orchestration, which utilizes task primitives as the basic units and represents
each query's workflow as a primitive-level dataflow graph. This explicitly
exposes a much larger design space, enables optimizations in parallelization
and pipelining across primitives of different modules, and enhances scheduling
to improve application-level performance. We build Teola, a novel orchestration
framework for LLM-based applications that implements this scheme. Comprehensive
experiments show that Teola can achieve up to 2.09x speedup over existing
systems across various popular LLM applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊáâÁî®Á®ãÂºèÁî± LLM ÂíåÈùû LLM ÂÖÉ‰ª∂ÁµÑÊàêÔºåÊØèÂÄãÂÖÉ‰ª∂ÈÉΩÊúÉÂΩ±ÈüøÁ´ØÂ∞çÁ´ØÂª∂ÈÅ≤„ÄÇÂÑòÁÆ°Â∑≤ÈáùÂ∞çÊúÄ‰Ω≥Âåñ LLM Êé®Ë´ñÂÅöÂá∫Ë®±Â§öÂä™ÂäõÔºå‰ΩÜÁ´ØÂ∞çÁ´ØÂ∑•‰ΩúÊµÅÁ®ãÊúÄ‰Ω≥ÂåñÂçªÈÅ≠Âà∞ÂøΩÁï•„ÄÇÁèæÊúâÊû∂ÊßãÊé°Áî®Á≤óÁï•ÁöÑÁ∑®ÊéíËàá‰ªªÂãôÊ®°ÁµÑÔºåÂ∞áÊúÄ‰Ω≥ÂåñÈôêÂà∂Âú®ÊØèÂÄãÊ®°ÁµÑÂÖßÔºå‰∏¶Áî¢ÁîüÊ¨°‰Ω≥ÁöÑÊéíÁ®ãÊ±∫Á≠ñ„ÄÇÊàëÂÄëÊèêÂá∫Á¥∞Á∑ªÁöÑÁ´ØÂ∞çÁ´ØÁ∑®ÊéíÔºåÂÆÉ‰ΩøÁî®‰ªªÂãôÂéüË™û‰ΩúÁÇ∫Âü∫Êú¨ÂñÆ‰ΩçÔºå‰∏¶Â∞áÊØèÂÄãÊü•Ë©¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ãË°®Á§∫ÁÇ∫ÂéüË™ûÂ±§Á¥öË≥áÊñôÊµÅÂúñ„ÄÇÈÄôÊòéÁ¢∫Âú∞Êè≠Èú≤‰∫ÜÊõ¥Â§ßÁöÑË®≠Ë®àÁ©∫ÈñìÔºåÂú®‰∏çÂêåÊ®°ÁµÑÁöÑÂéüË™û‰πãÈñìÂïüÁî®Âπ≥Ë°åÂåñÂíåÁÆ°Á∑öÊúÄ‰Ω≥ÂåñÔºå‰∏¶Âä†Âº∑ÊéíÁ®ã‰ª•ÊîπÂñÑÊáâÁî®Á®ãÂºèÂ±§Á¥öÊïàËÉΩ„ÄÇÊàëÂÄëÂª∫Êßã TeolaÔºå‰∏ÄÂÄãÂØ¶‰ΩúÊ≠§Êû∂ÊßãÁöÑ LLM ÊáâÁî®Á®ãÂºèÂâµÊñ∞Á∑®ÊéíÊû∂Êßã„ÄÇÂÖ®Èù¢ÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåTeola ËÉΩÂú®ÂêÑÁ®ÆÁÜ±ÈñÄ LLM ÊáâÁî®Á®ãÂºè‰∏≠ÔºåÊØîÁèæÊúâÁ≥ªÁµ±Âø´‰∏ä 2.09 ÂÄç„ÄÇ

##### **Into the Unknown: Generating Geospatial Descriptions for New Environments**
2406.19967v1 by Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge

Similar to vision-and-language navigation (VLN) tasks that focus on bridging
the gap between vision and language for embodied navigation, the new Rendezvous
(RVS) task requires reasoning over allocentric spatial relationships
(independent of the observer's viewpoint) using non-sequential navigation
instructions and maps. However, performance substantially drops in new
environments with no training data. Using opensource descriptions paired with
coordinates (e.g., Wikipedia) provides training data but suffers from limited
spatially-oriented text resulting in low geolocation resolution. We propose a
large-scale augmentation method for generating high-quality synthetic data for
new environments using readily available geospatial data. Our method constructs
a grounded knowledge-graph, capturing entity relationships. Sampled entities
and relations (`shop north of school') generate navigation instructions via (i)
generating numerous templates using context-free grammar (CFG) to embed
specific entities and relations; (ii) feeding the entities and relation into a
large language model (LLM) for instruction generation. A comprehensive
evaluation on RVS, showed that our approach improves the 100-meter accuracy by
45.83% on unseen environments. Furthermore, we demonstrate that models trained
with CFG-based augmentation achieve superior performance compared with those
trained with LLM-based augmentation, both in unseen and seen environments.
These findings suggest that the potential advantages of explicitly structuring
spatial information for text-based geospatial reasoning in previously unknown,
can unlock data-scarce scenarios.

ÊëòË¶ÅÔºöÈ°û‰ººÊñºÂ∞àÊ≥®ÊñºÂΩåÂêàÂÖ∑È´îÂ∞éËà™‰∏≠Ë¶ñË¶∫ËàáË™ûË®ÄÂ∑ÆË∑ùÁöÑË¶ñË¶∫Ë™ûË®ÄÂ∞éËà™ (VLN) ‰ªªÂãôÔºåÊñ∞ÁöÑÊúÉÈù¢ (RVS) ‰ªªÂãôÈúÄË¶Å‰ΩøÁî®ÈùûÈ†ÜÂ∫èÂ∞éËà™Êåá‰ª§ÂíåÂú∞ÂúñÊé®ÁêÜÁï∞‰∏≠ÂøÉÁ©∫ÈñìÈóú‰øÇÔºàËàáËßÄÂØüËÄÖÁöÑËßÄÈªûÁÑ°ÈóúÔºâ„ÄÇÁÑ∂ËÄåÔºåÂú®Ê≤íÊúâË®ìÁ∑¥Ë≥áÊñôÁöÑÊñ∞Áí∞Â¢É‰∏≠ÔºåÊïàËÉΩÊúÉÂ§ßÂπÖ‰∏ãÈôç„ÄÇ‰ΩøÁî®ËàáÂ∫ßÊ®ôÈÖçÂ∞çÁöÑÈñãÊ∫êË™™ÊòéÔºà‰æãÂ¶ÇÔºåÁ∂≠Âü∫ÁôæÁßëÔºâÊèê‰æõ‰∫ÜË®ìÁ∑¥Ë≥áÊñôÔºå‰ΩÜÁî±ÊñºÁ©∫ÈñìÂ∞éÂêëÊñáÂ≠óÊúâÈôêÔºåÂ∞éËá¥Âú∞ÁêÜ‰ΩçÁΩÆËß£ÊûêÂ∫¶‰Ωé„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂ§ßË¶èÊ®°Êì¥ÂÖÖÊñπÊ≥ïÔºå‰ΩøÁî®ÁèæÊàêÁöÑÂú∞ÁêÜÁ©∫ÈñìË≥áÊñôÁÇ∫Êñ∞Áí∞Â¢ÉÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂêàÊàêË≥áÊñô„ÄÇÊàëÂÄëÁöÑÂª∫ÊßãÊñπÊ≥ïÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂü∫Á§éÁü•Ë≠òÂúñÔºåÊì∑ÂèñÂØ¶È´îÈóú‰øÇ„ÄÇÂèñÊ®£ÁöÑÂØ¶È´îÂíåÈóú‰øÇÔºà„ÄåÂïÜÂ∫óÂú®Â≠∏Ê†°ÂåóÈÇä„ÄçÔºâÈÄèÈÅé‰ª•‰∏ãÊñπÂºèÁî¢ÁîüÂ∞éËà™Êåá‰ª§Ôºö(i) ‰ΩøÁî®ÁÑ°Èóú‰πéË™ûÂ¢ÉÁöÑÊñáÊ≥ï (CFG) Áî¢ÁîüË®±Â§öÁØÑÊú¨‰æÜÂµåÂÖ•ÁâπÂÆöÂØ¶È´îÂíåÈóú‰øÇÔºõ(ii) Â∞áÂØ¶È´îÂíåÈóú‰øÇËº∏ÂÖ•Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•Áî¢ÁîüÊåá‰ª§„ÄÇÂú® RVS ‰∏äÁöÑÂÖ®Èù¢Ë©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÊú™Ë¶ãÈÅéÁí∞Â¢É‰∏≠ÁöÑ 100 ÂÖ¨Â∞∫Ê∫ñÁ¢∫Â∫¶ÊèêÂçá‰∫Ü 45.83%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòé‰ΩøÁî®Âü∫Êñº CFG ÁöÑÊì¥ÂÖÖÊâÄË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºåÂú®Êú™Ë¶ãÈÅéÂíåË¶ãÈÅéÁí∞Â¢É‰∏≠ÔºåÈÉΩÊØî‰ΩøÁî®Âü∫Êñº LLM ÁöÑÊì¥ÂÖÖÊâÄË®ìÁ∑¥ÁöÑÊ®°ÂûãÁç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåÂú®‰ª•ÂâçÊú™Áü•ÁöÑÁí∞Â¢É‰∏≠ÔºåÊòéÁ¢∫Âª∫ÊßãÁî®ÊñºÂü∫ÊñºÊñáÂ≠óÁöÑÂú∞ÁêÜÁ©∫ÈñìÊé®ÁêÜÁöÑÁ©∫ÈñìË≥áË®äÁöÑÊΩõÂú®ÂÑ™Âã¢ÔºåÂèØ‰ª•Ëß£ÈéñË≥áÊñôÁ®ÄÂ∞ëÁöÑÂ†¥ÊôØ„ÄÇ

##### **Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning**
2406.19502v1 by Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo

Despite significant advancements, there is a limited understanding of how
large language models (LLMs) utilize knowledge for reasoning. To address this,
we propose a method that deconstructs complex real-world questions into a
graph, representing each question as a node with parent nodes of background
knowledge needed to solve the question. We develop the DepthQA dataset,
deconstructing questions into three depths: (i) recalling conceptual knowledge,
(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.
Based on a hierarchical graph, we quantify forward discrepancy, discrepancies
in LLMs' performance on simpler sub-problems versus complex questions. We also
measure backward discrepancy, where LLMs answer complex questions but struggle
with simpler ones. Our analysis shows that smaller models have more
discrepancies than larger models. Additionally, guiding models from simpler to
complex questions through multi-turn interactions improves performance across
model sizes, highlighting the importance of structured intermediate steps in
knowledge reasoning. This work enhances our understanding of LLM reasoning and
suggests ways to improve their problem-solving abilities.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÊúâÈ°ØËëóÁöÑÈÄ≤Â±ïÔºå‰ΩÜÂ∞çÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â¶Ç‰ΩïÂà©Áî®Áü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜÁöÑÁêÜËß£‰ªçÁÑ∂ÊúâÈôê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂ∞áË§áÈõúÁöÑÁúüÂØ¶‰∏ñÁïåÂïèÈ°åËß£ÊßãÊàê‰∏ÄÂÄãÂúñÂΩ¢ÔºåÂ∞áÊØèÂÄãÂïèÈ°åË°®Á§∫ÁÇ∫‰∏ÄÂÄãÁØÄÈªûÔºåÂÖ∂‰∏≠ÂåÖÂê´Ëß£Ê±∫ÂïèÈ°åÊâÄÈúÄÁöÑËÉåÊôØÁü•Ë≠òÁöÑÁà∂ÁØÄÈªû„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü DepthQA Ë≥áÊñôÈõÜÔºåÂ∞áÂïèÈ°åËß£ÊßãÊàê‰∏âÂÄãÊ∑±Â∫¶Ôºö(i) ÂõûÊÜ∂Ê¶ÇÂøµÁü•Ë≠òÔºå(ii) ÊáâÁî®Á®ãÂ∫èÁü•Ë≠òÔºå‰ª•Âèä (iii) ÂàÜÊûêÁ≠ñÁï•Áü•Ë≠ò„ÄÇÂü∫Êñº‰∏ÄÂÄãÈöéÂ±§ÂúñÂΩ¢ÔºåÊàëÂÄëÈáèÂåñ‰∫ÜÊ≠£ÂêëÂ∑ÆÁï∞ÔºåLLM Âú®ËºÉÁ∞°ÂñÆÁöÑÂ≠êÂïèÈ°åÂíåË§áÈõúÂïèÈ°å‰∏äÁöÑÊïàËÉΩÂ∑ÆÁï∞„ÄÇÊàëÂÄë‰πüÊ∏¨Èáè‰∫ÜÂèçÂêëÂ∑ÆÁï∞ÔºåÂÖ∂‰∏≠ LLM ËÉΩÂõûÁ≠îË§áÈõúÂïèÈ°åÔºå‰ΩÜÂú®ËºÉÁ∞°ÂñÆÁöÑÂïèÈ°å‰∏äÂçªÊúâÂõ∞Èõ£„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåËºÉÂ∞èÁöÑÊ®°ÂûãÊØîËºÉÂ§ßÁöÑÊ®°ÂûãÊúâÊõ¥Â§öÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÈÄèÈÅéÂ§öÂõûÂêà‰∫íÂãïÂºïÂ∞éÊ®°ÂûãÂæûËºÉÁ∞°ÂñÆÂà∞Ë§áÈõúÁöÑÂïèÈ°åÔºåÂèØ‰ª•ÊîπÂñÑÊâÄÊúâÊ®°ÂûãË¶èÊ®°ÁöÑÊïàËÉΩÔºåÁ™ÅÈ°Ø‰∫ÜÁµêÊßãÂåñ‰∏≠ÈñìÊ≠•È©üÂú®Áü•Ë≠òÊé®ÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂ¢ûÈÄ≤‰∫ÜÊàëÂÄëÂ∞ç LLM Êé®ÁêÜÁöÑÁêÜËß£Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊîπÂñÑÂÖ∂ÂïèÈ°åËß£Ê±∫ËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇ

##### **Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**
2406.19255v1 by Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan

While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.

ÊëòË¶ÅÔºö<paragraph>ÈõñÁÑ∂È†êË®ìÁ∑¥Â§ßÂûãË¶ñË®äË™ûË®ÄÊ®°Âûã (VLM) Â∑≤Â±ïÁèæÂá∫Â∞çÂêÑÁ®Æ‰∏ãÊ∏∏Ë¶ñË®äË™ûË®Ä‰ªªÂãôÁöÑÈ°ØËëóÊΩõÂäõÔºå‰ΩÜÁèæÊúâÁöÑ VLM ‰ªçÂèØËÉΩÂèóÂà∞Êüê‰∫õÂ∏∏Ë¶ãÈôêÂà∂ÁöÑÂΩ±ÈüøÔºå‰æãÂ¶ÇÁ≤óÁ≤íÂ∫¶ÁöÑË∑®Ê®°ÊÖãÂ∞çÈΩä„ÄÅÂ∞çÊôÇÈñìÂãïÊÖãÁöÑÂª∫Ê®°‰∏çË∂≥„ÄÅÂàÜÈõ¢ÁöÑË¶ñË®äË™ûË®ÄÊ™¢Ë¶ñ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ª•ÂÖ∑ÂÇôÁ¥∞Á≤íÂ∫¶ÁµêÊßãÂåñÊôÇÁ©∫Â∞çÈΩäÂ≠∏ÁøíÊñπÊ≥ï (Âç≥ Finsta) ÁöÑÂ¢ûÂº∑ VLM ÁÇ∫ÁõÆÊ®ô„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ª•Á¥∞Á≤íÂ∫¶ÁöÑÂ†¥ÊôØÂúñ (SG) ÁµêÊßãË°®Á§∫Ëº∏ÂÖ•ÊñáÂ≠óÂíåË¶ñË®äÔºåÂÖ©ËÄÖÈÄ≤‰∏ÄÊ≠•Áµ±‰∏ÄÂà∞‰∏ÄÂÄãÊï¥È´î SG (HSG) ‰∏≠Ôºå‰ª•Ê©ãÊé•ÂÖ©ÂÄãÊ®°ÊÖã„ÄÇÁÑ∂ÂæåÔºåÂª∫Á´ã‰∏ÄÂÄãÂü∫Êñº SG ÁöÑÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÊñáÂ≠ó SG (TSG) ‰ΩøÁî®ÂúñÂΩ¢ Transformer Á∑®Á¢ºÔºåËÄåË¶ñË®äÂãïÊÖã SG (DSG) Âíå HSG Ââá‰ΩøÁî®Êñ∞Á©éÁöÑÈÅûËø¥ÂúñÂΩ¢ Transformer Âª∫Ê®°Ôºå‰ª•ÈÄ≤Ë°åÁ©∫ÈñìÂíåÊôÇÈñìÁâπÂæµÂÇ≥Êí≠„ÄÇÈÄ≤‰∏ÄÊ≠•Ë®≠Ë®à‰∫Ü‰∏ÄÂÄãÊôÇÁ©∫È´òÊñØÂ∑ÆÂàÜÂúñÂΩ¢ TransformerÔºå‰ª•Â¢ûÂº∑Áâ©È´îÂú®ÊôÇÁ©∫Á∂≠Â∫¶‰∏≠ËÆäÂåñÁöÑÊÑüË¶∫„ÄÇÊé•‰∏ã‰æÜÔºåÊ†πÊìö TSG Âíå DSG ÁöÑÁ¥∞Á≤íÂ∫¶ÁµêÊßãÁâπÂæµÔºåÊàëÂÄëÂàÜÂà•Âü∑Ë°å‰ª•Áâ©‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁ©∫ÈñìÂ∞çÈΩäÂíå‰ª•Ë¨ÇË©ûÁÇ∫‰∏≠ÂøÉÁöÑÊôÇÂ∫èÂ∞çÈΩäÔºåÂ¢ûÂº∑Ë¶ñË®äË™ûË®ÄÂú®Á©∫ÈñìÂíåÊôÇÈñì‰∏äÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÂ∞áÊñπÊ≥ïË®≠Ë®àÁÇ∫‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑÁ≥ªÁµ±ÔºåÂèØ‰ª•Êï¥ÂêàÂà∞ÁèæÊúâÁöÑË®ìÁ∑¥ËâØÂ•ΩÁöÑ VLM ‰∏≠Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•Êì¥ÂÖÖË°®Á§∫ÔºåËÄåÁÑ°ÈúÄÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥Êàñ‰æùË≥¥‰∏ãÊ∏∏ÊáâÁî®Á®ãÂºè‰∏≠ÁöÑ SG Ê®ôË®ª„ÄÇÂú® 12 ÂÄãË≥áÊñôÈõÜ‰∏äÁöÑ 6 ÂÄã‰ª£Ë°®ÊÄß VL Âª∫Ê®°‰ªªÂãô‰∏≠ÔºåÁÑ°Ë´ñÊòØÂú®Ê®ôÊ∫ñË¶ñË®äÂ†¥ÊôØÈÇÑÊòØÈï∑Ê†ºÂºèË¶ñË®äÂ†¥ÊôØ‰∏≠ÔºåFinsta ÈÉΩÊåÅÁ∫åÊîπÂñÑÁèæÊúâÁöÑ 13 ÂÄãÊïàËÉΩÂº∑Â§ßÁöÑ VLMÔºå‰∏¶Âú®ÂæÆË™øÂíåÈõ∂Ê¨°Â≠∏ÁøíË®≠ÂÆö‰∏≠È°ØËëóÊõ¥Êñ∞ÁõÆÂâçÁöÑÊúÄÊñ∞ÊäÄË°ìÊúÄÁµÇ‰ªªÂãôÊïàËÉΩ„ÄÇ</paragraph>

##### **TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**
2406.18916v1 by Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen

Natural language question answering (QA) over structured data sources such as
tables and knowledge graphs (KGs) have been widely investigated, for example
with Large Language Models (LLMs). The main solutions include question to
formal query parsing and retrieval-based answer generation. However, current
methods of the former often suffer from weak generalization, failing to dealing
with multiple sources simultaneously, while the later is limited in
trustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework
that can simultaneously support multiple types of structured data in a unified
way. To this end, it adopts an LLM-friendly and unified knowledge
representation method called Condition Graph (CG), and uses an LLM and
demonstration-based two-level method for CG querying. For enhancement, it is
also equipped with dynamic demonstration retrieval. We have evaluated
UnifiedTQA with 5 benchmarks covering 3 types of structured data. It
outperforms 2 existing unified structured data QA methods and in comparison
with the baselines that are specific to a data type, it achieves
state-of-the-art on 2 of them. Further more, we demonstrates potential of our
method for more general QA tasks, QA over mixed structured data and QA across
structured data.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄÂïèÁ≠î (QA) ÈÄèÈÅéÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫êÔºà‰æãÂ¶ÇË°®Ê†ºÂíåÁü•Ë≠òÂúñË≠ú (KGs)ÔºâÂ∑≤Âª£Ê≥õÁ†îÁ©∂Ôºå‰æãÂ¶Ç‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ‰∏ªË¶ÅËß£Ê±∫ÊñπÊ°àÂåÖÊã¨ÂïèÈ°åËΩâÊèõÊàêÂΩ¢ÂºèÂåñÊü•Ë©¢Ëß£ÊûêÂíåÂü∫ÊñºÊ™¢Á¥¢ÁöÑÁ≠îÊ°àÁî¢Áîü„ÄÇÁÑ∂ËÄåÔºåÂâçËÄÖÁöÑÁèæË°åÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂº±Ê≥õÂåñÔºåÁÑ°Ê≥ïÂêåÊôÇËôïÁêÜÂ§öÂÄã‰æÜÊ∫êÔºåËÄåÂæåËÄÖÂâáÂèóÂà∞ÂèØ‰ø°Â∫¶ÁöÑÈôêÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ UnifiedTQAÔºå‰∏ÄÂÄãÂèØ‰ø°Ë≥¥ÁöÑ QA Ê°ÜÊû∂ÔºåËÉΩÂ§†‰ª•Áµ±‰∏ÄÁöÑÊñπÂºèÂêåÊôÇÊîØÊè¥Â§öÁ®ÆÈ°ûÂûãÁöÑÁµêÊßãÂåñË≥áÊñô„ÄÇÁÇ∫Ê≠§ÔºåÂÆÉÊé°Áî®‰∫Ü‰∏ÄÁ®Æ LLM ÂèãÂñÑ‰∏îÁµ±‰∏ÄÁöÑÁü•Ë≠òË°®Á§∫ÊñπÊ≥ïÔºåÁ®±ÁÇ∫Ê¢ù‰ª∂Âúñ (CG)Ôºå‰∏¶‰ΩøÁî® LLM ÂíåÂü∫ÊñºÁ§∫ÁØÑÁöÑ‰∫åÈöéÊñπÊ≥ïÈÄ≤Ë°å CG Êü•Ë©¢„ÄÇÁÇ∫‰∫ÜÂä†Âº∑ÔºåÂÆÉÈÇÑÈÖçÂÇô‰∫ÜÂãïÊÖãÁ§∫ÁØÑÊ™¢Á¥¢„ÄÇÊàëÂÄëÂ∑≤Á∂ì‰ΩøÁî®Ê∂µËìã 3 Á®ÆÈ°ûÂûãÁµêÊßãÂåñË≥áÊñôÁöÑ 5 ÂÄãÂü∫Ê∫ñË©ï‰º∞ UnifiedTQA„ÄÇÂÆÉÂÑ™Êñº 2 Á®ÆÁèæÊúâÁöÑÁµ±‰∏ÄÁµêÊßãÂåñË≥áÊñô QA ÊñπÊ≥ïÔºå‰∏¶‰∏îËàáÁâπÂÆöÊñºË≥áÊñôÈ°ûÂûãÁöÑÂü∫Á∑öÁõ∏ÊØîÔºåÂÆÉÂú®ÂÖ∂‰∏≠ 2 ÂÄãÂü∫Ê∫ñ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Êõ¥ÈÄöÁî®ÁöÑ QA ‰ªªÂãô„ÄÅÊ∑∑ÂêàÁµêÊßãÂåñË≥áÊñôÁöÑ QA ÂíåË∑®ÁµêÊßãÂåñË≥áÊñôÁöÑ QA ‰∏≠ÁöÑÊΩõÂäõ„ÄÇ


### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-31**|**A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**|Mothilal Asokan et.al.|[2407.21739v1](http://arxiv.org/abs/2407.21739v1)|null|
|**2024-07-31**|**Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**|Krishan Agyakari Raja Babu et.al.|[2407.21674v1](http://arxiv.org/abs/2407.21674v1)|null|
|**2024-07-31**|**Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**|Hermione Warr et.al.|[2407.21638v1](http://arxiv.org/abs/2407.21638v1)|null|
|**2024-07-31**|**Voxel Scene Graph for Intracranial Hemorrhage**|Antoine P. Sanner et.al.|[2407.21580v1](http://arxiv.org/abs/2407.21580v1)|null|
|**2024-07-31**|**Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**|I. M. Chernenkiy et.al.|[2407.21516v1](http://arxiv.org/abs/2407.21516v1)|null|
|**2024-07-31**|**Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**|Junxuan Yu et.al.|[2407.21490v1](http://arxiv.org/abs/2407.21490v1)|null|
|**2024-07-31**|**Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**|Mengtian Kang et.al.|[2407.21467v1](http://arxiv.org/abs/2407.21467v1)|null|
|**2024-07-31**|**Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**|Danfeng Guo et.al.|[2407.21368v1](http://arxiv.org/abs/2407.21368v1)|null|
|**2024-07-31**|**MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation Framework**|Adrian Celaya et.al.|[2407.21343v1](http://arxiv.org/abs/2407.21343v1)|null|
|**2024-07-31**|**Robust Box Prompt based SAM for Medical Image Segmentation**|Yuhao Huang et.al.|[2407.21284v1](http://arxiv.org/abs/2407.21284v1)|null|
|**2024-07-31**|**Unlocking the Potential of Binding Corporate Rules (BCRs) in Health Data Transfers**|Marcelo Corrales Compagnucci et.al.|[2407.21281v1](http://arxiv.org/abs/2407.21281v1)|null|
|**2024-07-31**|**FreqTSF: Time Series Forecasting Via Simulating Frequency Kramer-Kronig Relations**|Rujia Shen et.al.|[2407.21275v1](http://arxiv.org/abs/2407.21275v1)|null|
|**2024-07-31**|**Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**|Rohini Banerjee et.al.|[2407.21273v1](http://arxiv.org/abs/2407.21273v1)|null|
|**2024-07-30**|**Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**|Mayanka Chandrashekar et.al.|[2407.21149v1](http://arxiv.org/abs/2407.21149v1)|null|
|**2024-07-30**|**Zero Shot Health Trajectory Prediction Using Transformer**|Pawel Renc et.al.|[2407.21124v1](http://arxiv.org/abs/2407.21124v1)|null|
|**2024-07-30**|**CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning**|Yuexi Du et.al.|[2407.21011v1](http://arxiv.org/abs/2407.21011v1)|[link](https://github.com/xypb/cleft)|
|**2024-07-30**|**Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing**|Eugenio Lomurno et.al.|[2407.20830v1](http://arxiv.org/abs/2407.20830v1)|null|
|**2024-07-30**|**Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization**|Michael K√∂lle et.al.|[2407.20739v1](http://arxiv.org/abs/2407.20739v1)|null|
|**2024-07-29**|**Dense Self-Supervised Learning for Medical Image Segmentation**|Maxime Seince et.al.|[2407.20395v1](http://arxiv.org/abs/2407.20395v1)|null|
|**2024-07-29**|**Classification, Regression and Segmentation directly from k-Space in Cardiac MRI**|Ruochen Li et.al.|[2407.20108v1](http://arxiv.org/abs/2407.20108v1)|null|
|**2024-07-29**|**Robust Conformal Volume Estimation in 3D Medical Images**|Benjamin Lambert et.al.|[2407.19938v1](http://arxiv.org/abs/2407.19938v1)|[link](https://github.com/benolmbrt/wcp_miccai)|
|**2024-07-29**|**Yucca: A Deep Learning Framework For Medical Image Analysis**|Sebastian N√∏rgaard Llambias et.al.|[2407.19888v1](http://arxiv.org/abs/2407.19888v1)|[link](https://github.com/sllambias/yucca)|
|**2024-07-29**|**CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare**|Jingwei Zhu et.al.|[2407.19705v2](http://arxiv.org/abs/2407.19705v2)|[link](https://github.com/cas-siat-xinhai/collectivesft)|
|**2024-07-29**|**Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks**|Marco AF Pimentel et.al.|[2407.21072v1](http://arxiv.org/abs/2407.21072v1)|null|
|**2024-07-29**|**Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity**|Minxiao Chen et.al.|[2407.19668v1](http://arxiv.org/abs/2407.19668v1)|[link](https://github.com/faceless0124/mghstn)|
|**2024-07-28**|**Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing**|Heejoon Koo et.al.|[2407.19540v1](http://arxiv.org/abs/2407.19540v1)|null|
|**2024-07-28**|**Nudging Consent and the New Opt Out System to the Processing of Health Data in England**|Janos Meszaros et.al.|[2407.19447v1](http://arxiv.org/abs/2407.19447v1)|null|
|**2024-07-28**|**ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding**|Zhen Chen et.al.|[2407.19435v1](http://arxiv.org/abs/2407.19435v1)|[link](https://github.com/zonmgin-zhang/asi-seg)|
|**2024-07-28**|**A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy**|Meng Jiang et.al.|[2407.19422v1](http://arxiv.org/abs/2407.19422v1)|null|
|**2024-07-28**|**Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment**|Aamer Abdul Rahman et.al.|[2407.19380v1](http://arxiv.org/abs/2407.19380v1)|null|
|**2024-07-28**|**Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction**|Yuan Xue et.al.|[2407.19359v1](http://arxiv.org/abs/2407.19359v1)|null|
|**2024-07-27**|**Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification**|Santosh V. Patapati et.al.|[2407.19340v1](http://arxiv.org/abs/2407.19340v1)|null|
|**2024-07-27**|**Multi-Modal CLIP-Informed Protein Editing**|Mingze Yin et.al.|[2407.19296v1](http://arxiv.org/abs/2407.19296v1)|null|
|**2024-07-27**|**Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review**|Tongyue Shi et.al.|[2407.19256v1](http://arxiv.org/abs/2407.19256v1)|null|
|**2024-07-27**|**Channel Boosted CNN-Transformer-based Multi-Level and Multi-Scale Nuclei Segmentation**|Zunaira Rauf et.al.|[2407.19186v1](http://arxiv.org/abs/2407.19186v1)|null|
|**2024-07-26**|**Large Language Models as Co-Pilots for Causal Inference in Medical Studies**|Ahmed Alaa et.al.|[2407.19118v1](http://arxiv.org/abs/2407.19118v1)|null|
|**2024-07-26**|**Solving Robotics Problems in Zero-Shot with Vision-Language Models**|Zidan Wang et.al.|[2407.19094v1](http://arxiv.org/abs/2407.19094v1)|null|
|**2024-07-26**|**Using Large Language Models for the Interpretation of Building Regulations**|Stefan Fuchs et.al.|[2407.21060v1](http://arxiv.org/abs/2407.21060v1)|null|
|**2024-07-26**|**MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**|Shyam Dongre et.al.|[2407.20284v1](http://arxiv.org/abs/2407.20284v1)|null|
|**2024-07-26**|**Is larger always better? Evaluating and prompting large language models for non-generative medical tasks**|Yinghao Zhu et.al.|[2407.18525v1](http://arxiv.org/abs/2407.18525v1)|[link](https://github.com/yhzhu99/ehr-llm-benchmark)|
|**2024-07-26**|**A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation**|Laiyi Fu et.al.|[2407.18483v4](http://arxiv.org/abs/2407.18483v4)|[link](https://github.com/sperfu/eyedoc)|
|**2024-07-26**|**Towards Automated Solution Recipe Generation for Industrial Asset Management with LLM**|Nianjun Zhou et.al.|[2407.18992v1](http://arxiv.org/abs/2407.18992v1)|null|
|**2024-07-25**|**HDL-GPT: High-Quality HDL is All You Need**|Bhuvnesh Kumar et.al.|[2407.18423v1](http://arxiv.org/abs/2407.18423v1)|null|
|**2024-07-25**|**SCALE: Self-regulated Clustered federAted LEarning in a Homogeneous Environment**|Sai Puppala et.al.|[2407.18387v1](http://arxiv.org/abs/2407.18387v1)|null|
|**2024-07-25**|**Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**|Alessandro De Carlo et.al.|[2407.18343v2](http://arxiv.org/abs/2407.18343v2)|null|
|**2024-07-25**|**Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images**|Roberto Di Via et.al.|[2407.18125v1](http://arxiv.org/abs/2407.18125v1)|null|
|**2024-07-25**|**Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer Subtyping**|Jack Breen et.al.|[2407.18105v1](http://arxiv.org/abs/2407.18105v1)|[link](https://github.com/scjjb/MultiscalePathGraph)|
|**2024-07-25**|**HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline**|Qingyu Guo et.al.|[2407.17879v2](http://arxiv.org/abs/2407.17879v2)|null|
|**2024-07-25**|**EEG-SSM: Leveraging State-Space Model for Dementia Detection**|Xuan-The Tran et.al.|[2407.17801v1](http://arxiv.org/abs/2407.17801v1)|null|
|**2024-07-25**|**Mpox Detection Advanced: Rapid Epidemic Response Through Synthetic Data**|Yudara Kularathne et.al.|[2407.17762v1](http://arxiv.org/abs/2407.17762v1)|null|
|**2024-07-25**|**Cost-effective Instruction Learning for Pathology Vision and Language Analysis**|Kaitao Chen et.al.|[2407.17734v1](http://arxiv.org/abs/2407.17734v1)|[link](https://github.com/jlinekai/clover)|
|**2024-07-24**|**Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**|Nikolaos Ntampakis et.al.|[2407.17324v2](http://arxiv.org/abs/2407.17324v2)|null|
|**2024-07-24**|**Improving ICD coding using Chapter based Named Entities and Attentional Models**|Abhijith R. Beeravolu et.al.|[2407.17230v1](http://arxiv.org/abs/2407.17230v1)|null|
|**2024-07-24**|**Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence**|Xiaoyu Tan et.al.|[2407.17164v2](http://arxiv.org/abs/2407.17164v2)|null|
|**2024-07-24**|**SDoH-GPT: Using Large Language Models to Extract Social Determinants of Health (SDoH)**|Bernardo Consoli et.al.|[2407.17126v1](http://arxiv.org/abs/2407.17126v1)|null|
|**2024-07-24**|**SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing**|Changchang Yin et.al.|[2407.16999v1](http://arxiv.org/abs/2407.16999v1)|[link](https://github.com/yinchangchang/sepsislab)|
|**2024-07-24**|**Toward an Integrated Decision Making Framework for Optimized Stroke Diagnosis with DSA and Treatment under Uncertainty**|Nur Ahmad Khatim et.al.|[2407.16962v1](http://arxiv.org/abs/2407.16962v1)|[link](https://github.com/inteligensi/dsapomdps.jl)|
|**2024-07-23**|**AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical Knowledge Graphs and Data-Driven Quantification**|Yuheng Wang et.al.|[2407.16822v1](http://arxiv.org/abs/2407.16822v1)|[link](https://github.com/ryan315/7pgd)|
|**2024-07-23**|**Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges**|Zahraa Al Sahili et.al.|[2407.16804v1](http://arxiv.org/abs/2407.16804v1)|null|
|**2024-07-23**|**Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging**|Daniela L. Ramos et.al.|[2407.16608v1](http://arxiv.org/abs/2407.16608v1)|null|
|**2024-07-23**|**A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions**|Giorgos Lysandrou et.al.|[2407.16593v1](http://arxiv.org/abs/2407.16593v1)|null|
|**2024-07-23**|**Prompt Injection Attacks on Large Language Models in Oncology**|Jan Clusmann et.al.|[2407.18981v1](http://arxiv.org/abs/2407.18981v1)|null|
|**2024-07-23**|**Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management and Usage in Question Answering**|Pralaypati Ta et.al.|[2407.21053v1](http://arxiv.org/abs/2407.21053v1)|null|
|**2024-07-23**|**Virtue Ethics For Ethically Tunable Robotic Assistants**|Rajitha Ramanayake et.al.|[2407.16361v1](http://arxiv.org/abs/2407.16361v1)|null|
|**2024-07-23**|**PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**|Jaeyoung Kim et.al.|[2407.16329v1](http://arxiv.org/abs/2407.16329v1)|null|
|**2024-07-23**|**Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning**|Yufeng Li et.al.|[2407.16715v2](http://arxiv.org/abs/2407.16715v2)|null|
|**2024-07-22**|**Artificial Intelligence-based Decision Support Systems for Precision and Digital Health**|Nina Deliu et.al.|[2407.16062v1](http://arxiv.org/abs/2407.16062v1)|null|
|**2024-07-22**|**GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI**|Zhaojie Fang et.al.|[2407.15719v1](http://arxiv.org/abs/2407.15719v1)|[link](https://github.com/tinysqua/gfe-mamba)|
|**2024-07-22**|**Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks**|Eugenio Lomurno et.al.|[2407.15526v2](http://arxiv.org/abs/2407.15526v2)|null|
|**2024-07-22**|**A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model**|Yingxue Xu et.al.|[2407.15362v1](http://arxiv.org/abs/2407.15362v1)|null|
|**2024-07-21**|**They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models**|Mohammad Saeid Mahdavinejad et.al.|[2407.21041v1](http://arxiv.org/abs/2407.21041v1)|null|
|**2024-07-21**|**Genetic Algorithm to Optimize Design of Micro-Surgical Scissors**|Fatemeh Norouziani et.al.|[2407.15243v1](http://arxiv.org/abs/2407.15243v1)|null|
|**2024-07-21**|**MedSAGa: Few-shot Memory Efficient Medical Image Segmentation using Gradient Low-Rank Projection in SAM**|Navyansh Mahla et.al.|[2407.15042v1](http://arxiv.org/abs/2407.15042v1)|null|
|**2024-07-20**|**Mapping Patient Trajectories: Understanding and Visualizing Sepsis Prognostic Pathways from Patients Clinical Narratives**|Sudeshna Jana et.al.|[2407.21039v1](http://arxiv.org/abs/2407.21039v1)|null|
|**2024-07-20**|**Preictal Period Optimization for Deep Learning-Based Epileptic Seizure Prediction**|Petros Koutsouvelis et.al.|[2407.14876v1](http://arxiv.org/abs/2407.14876v1)|null|
|**2024-07-20**|**PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates**|Junjie Shi et.al.|[2407.14796v1](http://arxiv.org/abs/2407.14796v1)|[link](https://github.com/jun-jie-shi/passion)|
|**2024-07-19**|**Improving Representation of High-frequency Components for Medical Foundation Models**|Yuetan Chu et.al.|[2407.14651v2](http://arxiv.org/abs/2407.14651v2)|[link](https://github.com/Arturia-Pendragon-Iris/Frepa)|
|**2024-07-19**|**CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models**|Rikhiya Ghosh et.al.|[2407.14640v1](http://arxiv.org/abs/2407.14640v1)|null|
|**2024-07-19**|**Two new feature selection methods based on learn-heuristic techniques for breast cancer prediction: A comprehensive analysis**|Kamyab Karimi et.al.|[2407.14631v1](http://arxiv.org/abs/2407.14631v1)|null|
|**2024-07-19**|**Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model**|Kun Zhao et.al.|[2407.14326v1](http://arxiv.org/abs/2407.14326v1)|null|
|**2024-07-19**|**Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based Undersampling Method for Bias Reduction**|Jos√© Daniel Pascual-Triana et.al.|[2407.14210v1](http://arxiv.org/abs/2407.14210v1)|null|
|**2024-07-19**|**Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field**|Tobias Kerner et.al.|[2407.14076v2](http://arxiv.org/abs/2407.14076v2)|null|
|**2024-07-19**|**HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research**|Prerana Sanjay Kulkarni et.al.|[2407.14030v1](http://arxiv.org/abs/2407.14030v1)|null|
|**2024-07-18**|**DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention**|Xiaoya Tang et.al.|[2407.13920v1](http://arxiv.org/abs/2407.13920v1)|null|
|**2024-07-18**|**Data-Algorithm-Architecture Co-Optimization for Fair Neural Networks on Skin Lesion Dataset**|Yi Sheng et.al.|[2407.13896v1](http://arxiv.org/abs/2407.13896v1)|null|
|**2024-07-18**|**APS-USCT: Ultrasound Computed Tomography on Sparse Data via AI-Physic Synergy**|Yi Sheng et.al.|[2407.14564v1](http://arxiv.org/abs/2407.14564v1)|null|
|**2024-07-18**|**Addressing Imbalance for Class Incremental Learning in Medical Image Classification**|Xuze Hao et.al.|[2407.13768v1](http://arxiv.org/abs/2407.13768v1)|null|
|**2024-07-18**|**Shaded Route Planning Using Active Segmentation and Identification of Satellite Images**|Longchao Da et.al.|[2407.13689v1](http://arxiv.org/abs/2407.13689v1)|null|
|**2024-07-18**|**A Comparative Study on Automatic Coding of Medical Letters with Explainability**|Jamie Glen et.al.|[2407.13638v1](http://arxiv.org/abs/2407.13638v1)|[link](https://github.com/Glenj01/Medical-Coding)|
|**2024-07-18**|**A review of handcrafted and deep radiomics in neurological diseases: transitioning from oncology to clinical neuroimaging**|Elizaveta Lavrova et.al.|[2407.13813v1](http://arxiv.org/abs/2407.13813v1)|null|
|**2024-07-18**|**End-To-End Clinical Trial Matching with Large Language Models**|Dyke Ferber et.al.|[2407.13463v1](http://arxiv.org/abs/2407.13463v1)|null|
|**2024-07-18**|**CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis**|Junying Chen et.al.|[2407.13301v1](http://arxiv.org/abs/2407.13301v1)|null|
|**2024-07-18**|**NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations**|Hao Bai et.al.|[2407.13241v1](http://arxiv.org/abs/2407.13241v1)|null|
|**2024-07-17**|**Enhancing the Utility of Privacy-Preserving Cancer Classification using Synthetic Data**|Richard Osuala et.al.|[2407.12669v1](http://arxiv.org/abs/2407.12669v1)|null|
|**2024-07-17**|**Abstraction Alignment: Comparing Model and Human Conceptual Relationships**|Angie Boggust et.al.|[2407.12543v1](http://arxiv.org/abs/2407.12543v1)|[link](https://github.com/mitvis/abstraction-alignment)|
|**2024-07-17**|**Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning**|Lisandro A. Jimenez-Roa et.al.|[2407.12894v1](http://arxiv.org/abs/2407.12894v1)|null|
|**2024-07-17**|**Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions**|Marcos Fern√°ndez-Pichel et.al.|[2407.12468v2](http://arxiv.org/abs/2407.12468v2)|null|
|**2024-07-17**|**Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**|Alexander R. Pelletier et.al.|[2407.12888v1](http://arxiv.org/abs/2407.12888v1)|[link](https://github.com/pinglab-utils/rugged)|
|**2024-07-17**|**Evaluating graph-based explanations for AI-based recommender systems**|Simon Delarue et.al.|[2407.12357v1](http://arxiv.org/abs/2407.12357v1)|null|
|**2024-07-16**|**GPT-4V Cannot Generate Radiology Reports Yet**|Yuyang Jiang et.al.|[2407.12176v1](http://arxiv.org/abs/2407.12176v1)|[link](https://github.com/yuyangj0/gpt-4v-evaluation-radiology-report)|

#### Abstracts
##### **A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**
2407.21739v1 by Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar

Adapting foundation models for medical image analysis requires finetuning
them on a considerable amount of data because of extreme distribution shifts
between natural (source) data used for pretraining and medical (target) data.
However, collecting task-specific medical data for such finetuning at a central
location raises many privacy concerns. Although Federated learning (FL)
provides an effective means for training on private decentralized data,
communication costs in federating large foundation models can quickly become a
significant bottleneck, impacting the solution's scalability. In this work, we
address this problem of efficient communication while ensuring effective
learning in FL by combining the strengths of Parameter-Efficient Fine-tuning
(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)
in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical
image segmentation. Unlike prior works that utilize LoRA and finetune the
entire decoder, we critically analyze the contribution of each granular
component of SAM on finetuning performance. Thus, we identify specific layers
to be federated that are very efficient in terms of communication cost while
producing on-par accuracy. Our experiments show that retaining the parameters
of the SAM model (including most of the decoder) in their original state during
adaptation is beneficial because fine-tuning on small datasets tends to distort
the inherent capabilities of the underlying foundation model. On Fed-KiTS, our
approach decreases communication cost (~48x) compared to full fine-tuning while
increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach
performs similar to SAMed while achieving ~2.8x reduction in communication and
parameters to be finetuned. We further validate our approach with experiments
on Fed-IXI and Prostate MRI datasets.

ÊëòË¶ÅÔºö<paragraph>Áî±ÊñºÈ†êË®ìÁ∑¥ÊâÄÁî®ÁöÑËá™ÁÑ∂Ôºà‰æÜÊ∫êÔºâË≥áÊñôÂíåÈÜ´ÁôÇÔºàÁõÆÊ®ôÔºâË≥áÊñô‰πãÈñìÁöÑÊ•µÁ´ØÂàÜ‰ΩàËΩâÁßªÔºåÂõ†Ê≠§Â∞áÂü∫Á§éÊ®°ÂûãË™øÊï¥Áî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈúÄË¶ÅÂú®Â§ßÈáèË≥áÊñô‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÂæÆË™ø„ÄÇ
ÁÑ∂ËÄåÔºåÂú®‰∏≠ÂøÉ‰ΩçÁΩÆÊî∂ÈõÜÊ≠§È°ûÂæÆË™øÁöÑÁâπÂÆö‰ªªÂãôÈÜ´ÁôÇË≥áÊñôÊúÉÂºïÁôºË®±Â§öÈö±ÁßÅÂïèÈ°å„ÄÇÂÑòÁÆ°ËÅØÂêàÂ≠∏Áøí (FL) Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂú®ÁßÅÊúâÂàÜÊï£ÂºèË≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÂú®ËÅØÂêàÂ§ßÂûãÂü∫Á§éÊ®°ÂûãÊôÇÔºåÈÄöË®äÊàêÊú¨ÂèØËÉΩÊúÉËøÖÈÄüÊàêÁÇ∫‰∏ÄÂÄãÈáçÂ§ßÁì∂È†∏ÔºåÂΩ±ÈüøËß£Ê±∫ÊñπÊ°àÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÁµêÂêàÂèÉÊï∏È´òÊïàÂæÆË™ø (PEFT) Âíå FL ÁöÑÂÑ™Âã¢ÔºåËß£Ê±∫‰∫ÜÂú®Á¢∫‰øù FL ‰∏≠ÊúâÊïàÂ≠∏ÁøíÁöÑÂêåÊôÇÈÄ≤Ë°åÈ´òÊïàÈÄöË®äÁöÑÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ª•ËÅØÂêàÁöÑÊñπÂºèÁ†îÁ©∂Âç≥ÊèíÂç≥Áî®‰ΩéÁß©ÈÅ©ÈÖçÂô® (LoRA)Ôºå‰ª•Ë™øÊï¥ÂçÄÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ‰ª•ÈÄ≤Ë°å 3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇËàáÂà©Áî® LoRA ÂíåÂæÆË™øÊï¥ÂÄãËß£Á¢ºÂô®ÁöÑÂÖàÂâçÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞ÂàÜÊûê‰∫Ü SAM ÁöÑÊØèÂÄãÁ≤íÁãÄÁµÑÊàêÈÉ®ÂàÜÂ∞çÂæÆË™øÊïàËÉΩÁöÑË≤¢Áçª„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁ¢∫ÂÆö‰∫ÜÂú®ÈÄöË®äÊàêÊú¨ÊñπÈù¢ÈùûÂ∏∏È´òÊïàÁöÑÁâπÂÆöÂ±§ÔºåÂêåÊôÇÁî¢Áîü‰∫ÜÂêåÁ≠âÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÂú®Ë™øÊï¥ÈÅéÁ®ã‰∏≠Â∞á SAM Ê®°ÂûãÁöÑÂèÉÊï∏ÔºàÂåÖÊã¨Â§ßÈÉ®ÂàÜËß£Á¢ºÂô®Ôºâ‰øùÁïôÂú®ÂÖ∂ÂéüÂßãÁãÄÊÖãÊòØÊúâÁõäÁöÑÔºåÂõ†ÁÇ∫Âú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂæÆË™øÂæÄÂæÄÊúÉÊâ≠Êõ≤Âü∫Á§éÊ®°ÂûãÁöÑÂÖßÂú®ËÉΩÂäõ„ÄÇÂú® Fed-KiTS ‰∏äÔºåËàáÂÆåÂÖ®ÂæÆË™øÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈôç‰Ωé‰∫ÜÈÄöË®äÊàêÊú¨ÔºàÁ¥Ñ 48 ÂÄçÔºâÔºåÂêåÊôÇÊèêÈ´ò‰∫Ü 3D ÂàÜÂâ≤‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºàÁ¥Ñ 6% ÁöÑÈ™∞Â≠êÂàÜÊï∏Ôºâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËàá SAMed È°û‰ººÔºåÂêåÊôÇÂ∞áÈÄöË®äÂíåÂæÖÂæÆË™øÂèÉÊï∏Ê∏õÂ∞ë‰∫ÜÁ¥Ñ 2.8 ÂÄç„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄöÈÅéÂú® Fed-IXI Âíå Prostate MRI Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ï„ÄÇ</paragraph>

##### **Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**
2407.21674v1 by Krishan Agyakari Raja Babu, Rachana Sathish, Mrunal Pattanaik, Rahul Venkataramani

Synthetic data is becoming increasingly integral in data-scarce fields such
as medical imaging, serving as a substitute for real data. However, its
inherent statistical characteristics can significantly impact downstream tasks,
potentially compromising deployment performance. In this study, we empirically
investigate this issue and uncover a critical phenomenon: downstream neural
networks often exploit spurious distinctions between real and synthetic data
when there is a strong correlation between the data source and the task label.
This exploitation manifests as \textit{simplicity bias}, where models overly
rely on superficial features rather than genuine task-related complexities.
Through principled experiments, we demonstrate that the source of data (real
vs.\ synthetic) can introduce spurious correlating factors leading to poor
performance during deployment when the correlation is absent. We first
demonstrate this vulnerability on a digit classification task, where the model
spuriously utilizes the source of data instead of the digit to provide an
inference. We provide further evidence of this phenomenon in a medical imaging
problem related to cardiac view classification in echocardiograms, particularly
distinguishing between 2-chamber and 4-chamber views. Given the increasing role
of utilizing synthetic datasets, we hope that our experiments serve as
effective guidelines for the utilization of synthetic datasets in model
training.

ÊëòË¶ÅÔºöÂêàÊàêË≥áÊñôÂú®Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÈ†òÂüü‰∏≠ËÆäÂæóË∂ä‰æÜË∂ä‰∏çÂèØÊàñÁº∫Ôºå‰æãÂ¶ÇÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÁî®‰ΩúÁúüÂØ¶Ë≥áÊñôÁöÑÊõø‰ª£ÂìÅ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÂÖßÂú®ÁöÑÁµ±Ë®àÁâπÊÄßÊúÉÈ°ØËëóÂΩ±Èüø‰∏ãÊ∏∏‰ªªÂãôÔºåÂèØËÉΩÊêçÂÆ≥ÈÉ®ÁΩ≤ÊïàËÉΩ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂØ¶Ë≠âË™øÊü•Ê≠§ÂïèÈ°åÔºå‰∏¶Êè≠Èú≤‰∏ÄÂÄãÈóúÈçµÁèæË±°ÔºöÁï∂Ë≥áÊñô‰æÜÊ∫êËàá‰ªªÂãôÊ®ôÁ±§‰πãÈñìÊúâÂæàÂº∑ÁöÑÁõ∏ÈóúÊÄßÊôÇÔºå‰∏ãÊ∏∏Á•ûÁ∂ìÁ∂≤Ë∑ØÈÄöÂ∏∏ÊúÉÂà©Áî®ÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñô‰πãÈñìÁöÑËôõÂÅáÂçÄÂà•„ÄÇÈÄôÁ®ÆÂà©Áî®Ë°®ÁèæÁÇ∫„ÄåÁ∞°ÂåñÂÅèÂ∑Æ„ÄçÔºåÂÖ∂‰∏≠Ê®°ÂûãÈÅéÂ∫¶‰æùË≥¥Ë°®Èù¢ÁâπÂæµÔºåËÄå‰∏çÊòØÁúüÊ≠£ÁöÑËàá‰ªªÂãôÁõ∏ÈóúÁöÑË§áÈõúÊÄß„ÄÇÈÄèÈÅéÊúâÂéüÂâáÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéË≥áÊñô‰æÜÊ∫êÔºàÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñôÔºâÂèØËÉΩÊúÉÂºïÂÖ•ËôõÂÅáÁöÑÁõ∏ÈóúÂõ†Á¥†ÔºåÂ∞éËá¥Âú®Áõ∏ÈóúÊÄß‰∏çÂ≠òÂú®ÊôÇÈÉ®ÁΩ≤ÊúüÈñìÊïàËÉΩ‰∏ç‰Ω≥„ÄÇÊàëÂÄëÈ¶ñÂÖàÂú®Êï∏Â≠óÂàÜÈ°û‰ªªÂãô‰∏≠Ë≠âÊòéÊ≠§ÊºèÊ¥ûÔºåÂÖ∂‰∏≠Ê®°ÂûãËôõÂÅáÂú∞Âà©Áî®Ë≥áÊñô‰æÜÊ∫êËÄåÈùûÊï∏Â≠ó‰æÜÊèê‰æõÊé®Ë´ñ„ÄÇÊàëÂÄëÂú®ËàáË∂ÖÈü≥Ê≥¢ÂøÉËáüË¶ñÈáéÂàÜÈ°ûÁõ∏ÈóúÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂïèÈ°å‰∏≠ÈÄ≤‰∏ÄÊ≠•Êèê‰æõÊ≠§ÁèæË±°ÁöÑË≠âÊìöÔºåÁâπÂà•ÊòØÂçÄÂàÜ‰∫åËÖîÂíåÂõõËÖîË¶ñÈáé„ÄÇÈëëÊñºÂêàÊàêË≥áÊñôÈõÜÁöÑ‰ΩøÁî®ËßíËâ≤Êó•ÁõäÂ¢ûÂä†ÔºåÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÂØ¶È©óËÉΩ‰ΩúÁÇ∫Âú®Ê®°ÂûãË®ìÁ∑¥‰∏≠Âà©Áî®ÂêàÊàêË≥áÊñôÈõÜÁöÑÊúâÊïàÊåáÂçó„ÄÇ

##### **Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**
2407.21638v1 by Hermione Warr, Yasin Ibrahim, Daniel R. McGowan, Konstantinos Kamnitsas

Automation of medical image interpretation could alleviate bottlenecks in
diagnostic workflows, and has become of particular interest in recent years due
to advancements in natural language processing. Great strides have been made
towards automated radiology report generation via AI, yet ensuring clinical
accuracy in generated reports is a significant challenge, hindering deployment
of such methods in clinical practice. In this work we propose a quality control
framework for assessing the reliability of AI-generated radiology reports with
respect to semantics of diagnostic importance using modular auxiliary auditing
components (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings
show that incorporating ACs in the form of disease-classifiers can enable
auditing that identifies more reliable reports, resulting in higher F1 scores
compared to unfiltered generated reports. Additionally, leveraging the
confidence of the AC labels further improves the audit's effectiveness.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂà§ËÆÄÁöÑËá™ÂãïÂåñÂèØ‰ª•Ê∏õËºïË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÁì∂È†∏Ôºå‰∏¶‰∏îÁî±ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Ê≠•ÔºåÂú®ËøëÂπ¥‰æÜÁâπÂà•ÂèóÂà∞ÈáçË¶ñ„ÄÇÂú®ÈÄèÈÅé AI Ëá™ÂãïÁîüÊàêÊîæÂ∞ÑÁ∑öÂ†±ÂëäÊñπÈù¢Â∑≤Á∂ìÂèñÂæó‰∫ÜÈï∑Ë∂≥ÁöÑÈÄ≤Â±ïÔºåÁÑ∂ËÄåÁ¢∫‰øùÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊ∫ñÁ¢∫ÊÄßÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÈòªÁ§ô‰∫ÜÊ≠§È°ûÊñπÊ≥ïÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂìÅË≥™ÊéßÂà∂Êû∂ÊßãÔºåÁî®ÊñºË©ï‰º∞ AI ÁîüÊàêÁöÑÊîæÂ∞ÑÁ∑öÂ†±ÂëäÁöÑÂèØÈù†ÊÄßÔºå‰∏¶‰ΩøÁî®Ê®°ÁµÑÂåñËºîÂä©Á®ΩÊ†∏ÂÖÉ‰ª∂ (AC) ÈáùÂ∞çË®∫Êñ∑ÈáçË¶ÅÊÄßÁöÑË™ûÁæ©ÈÄ≤Ë°åË©ï‰º∞„ÄÇÂú® MIMIC-CXR Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁÆ°ÈÅìÔºåÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫Ôºå‰ª•ÁñæÁóÖÂàÜÈ°ûÂô®ÁöÑÂΩ¢ÂºèÁ¥çÂÖ• AC ÂèØ‰ª•ÂïüÁî®Á®ΩÊ†∏Ôºå‰ª•Ë≠òÂà•Êõ¥ÂèØÈù†ÁöÑÂ†±ÂëäÔºåËàáÊú™Á∂ìÁØ©ÈÅ∏ÁöÑÁîüÊàêÂ†±ÂëäÁõ∏ÊØîÔºåÊúÉÁî¢ÁîüÊõ¥È´òÁöÑ F1 ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÈÄ≤‰∏ÄÊ≠•Âà©Áî® AC Ê®ôÁ±§ÁöÑ‰ø°ÂøÉÂèØ‰ª•ÊèêÈ´òÁ®ΩÊ†∏ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Voxel Scene Graph for Intracranial Hemorrhage**
2407.21580v1 by Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay

Patients with Intracranial Hemorrhage (ICH) face a potentially
life-threatening condition, and patient-centered individualized treatment
remains challenging due to possible clinical complications. Deep-Learning-based
methods can efficiently analyze the routinely acquired head CTs to support the
clinical decision-making. The majority of early work focuses on the detection
and segmentation of ICH, but do not model the complex relations between ICH and
adjacent brain structures. In this work, we design a tailored object detection
method for ICH, which we unite with segmentation-grounded Scene Graph
Generation (SGG) methods to learn a holistic representation of the clinical
cerebral scene. To the best of our knowledge, this is the first application of
SGG for 3D voxel images. We evaluate our method on two head-CT datasets and
demonstrate that our model can recall up to 74% of clinically relevant
relations. This work lays the foundation towards SGG for 3D voxel data. The
generated Scene Graphs can already provide insights for the clinician, but are
also valuable for all downstream tasks as a compact and interpretable
representation.

ÊëòË¶ÅÔºöËÖ¶Âá∫Ë°Ä (ICH) ÊÇ£ËÄÖÈù¢Ëá®ÂèØËÉΩÂç±ÂèäÁîüÂëΩÁöÑÁãÄÊ≥ÅÔºåÁî±ÊñºÂèØËÉΩÁöÑËá®Â∫ä‰ΩµÁôºÁóáÔºå‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÂÄã‰∫∫ÂåñÊ≤ªÁôÇ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂàÜÊûêÂ∏∏Ë¶èÁç≤ÂæóÁöÑÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºå‰ª•ÊîØÊåÅËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÂ§ßÂ§öÊï∏Êó©ÊúüÂ∑•‰ΩúÈÉΩÈõÜ‰∏≠Âú® ICH ÁöÑÊ™¢Ê∏¨ÂíåÂàÜÂâ≤Ôºå‰ΩÜÊ≤íÊúâÂ∞ç ICH ÂíåÁõ∏ÈÑ∞Â§ßËÖ¶ÁµêÊßã‰πãÈñìÁöÑË§áÈõúÈóú‰øÇÈÄ≤Ë°åÂª∫Ê®°„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÈáùÂ∞ç ICH ÁöÑÂÆ¢Ë£ΩÂåñÁõÆÊ®ôÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÊàëÂÄëÂ∞áÂÖ∂ËàáÂü∫ÊñºÂàÜÂâ≤ÁöÑÂ†¥ÊôØÂúñÁîüÊàê (SGG) ÊñπÊ≥ïÁµêÂêàÔºå‰ª•Â≠∏ÁøíËá®Â∫äËÖ¶ÈÉ®Â†¥ÊôØÁöÑÊï¥È´îË°®Âæµ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØ SGG Á¨¨‰∏ÄÊ¨°ÊáâÁî®Êñº 3D È´îÁ¥†ÂΩ±ÂÉè„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÊï∏ÊìöÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºå‰∏¶Ë≠âÊòéÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Âè¨ÂõûÈ´òÈÅî 74% ÁöÑËá®Â∫äÁõ∏ÈóúÈóú‰øÇ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ 3D È´îÁ¥†Êï∏ÊìöÁöÑ SGG Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇÁîüÊàêÁöÑÂ†¥ÊôØÂúñÂ∑≤Á∂ìÂèØ‰ª•ÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõË¶ãËß£Ôºå‰ΩÜÂ∞çÊñºÊâÄÊúâ‰∏ãÊ∏∏‰ªªÂãôËÄåË®ÄÔºåÂÆÉ‰πüÊòØ‰∏ÄÁ®ÆÁ≤æÁ∞°‰∏îÂèØËß£ÈáãÁöÑË°®ÂæµÔºåÂõ†Ê≠§ÈùûÂ∏∏ÊúâÂÉπÂÄº„ÄÇ

##### **Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**
2407.21516v1 by I. M. Chernenkiy, Y. A. Drach, S. R. Mustakimova, V. V. Kazantseva, N. A. Ushakov, S. K. Efetov, M. V. Feldsherov

Colorectal cancer is the third-most common cancer in the Western Hemisphere.
The segmentation of colorectal and colorectal cancer by computed tomography is
an urgent problem in medicine. Indeed, a system capable of solving this problem
will enable the detection of colorectal cancer at early stages of the disease,
facilitate the search for pathology by the radiologist, and significantly
accelerate the process of diagnosing the disease. However, scientific
publications on medical image processing mostly use closed, non-public data.
This paper presents an extension of the Medical Decathlon dataset with
colorectal markups in order to improve the quality of segmentation algorithms.
An experienced radiologist validated the data, categorized it into subsets by
quality, and published it in the public domain. Based on the obtained results,
we trained neural network models of the UNet architecture with 5-part
cross-validation and achieved a Dice metric quality of $0.6988 \pm 0.3$. The
published markups will improve the quality of colorectal cancer detection and
simplify the radiologist's job for study description.

ÊëòË¶ÅÔºöÂ§ßËÖ∏ÁôåÊòØË•øÂçäÁêÉÁ¨¨‰∏âÂ∏∏Ë¶ãÁöÑÁôåÁóá„ÄÇ
Âà©Áî®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂ∞çÂ§ßËÖ∏ÁôåËàáÂ§ßËÖ∏ÁôåÈÄ≤Ë°åÂàÜÊÆµÊòØÈÜ´Â≠∏‰∏äÁöÑÁ∑äÊÄ•ÂïèÈ°å„ÄÇ‰∫ãÂØ¶‰∏äÔºå‰∏ÄÂÄãËÉΩÂ§†Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÁöÑÁ≥ªÁµ±Â∞áËÉΩÂ§†Âú®ÁñæÁóÖÁöÑÊó©ÊúüÈöéÊÆµÂÅµÊ∏¨Â§ßËÖ∏ÁôåÔºåÂçîÂä©ÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∞ãÊâæÁóÖÁêÜÔºå‰∏¶È°ØËëóÂä†ÈÄüË®∫Êñ∑ÁñæÁóÖÁöÑÈÅéÁ®ã„ÄÇÁÑ∂ËÄåÔºåÈóúÊñºÈÜ´Â≠∏ÂΩ±ÂÉèËôïÁêÜÁöÑÁßëÂ≠∏ÂàäÁâ©Â§ßÂ§ö‰ΩøÁî®Â∞ÅÈñâ„ÄÅÈùûÂÖ¨ÈñãÁöÑË≥áÊñô„ÄÇÈÄôÁØáË´ñÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∏∂ÊúâÂ§ßËÖ∏Ê®ôË®òÁöÑÈÜ´Â≠∏ÂçÅÈ†ÖÂÖ®ËÉΩË≥áÊñôÈõÜÁöÑÂª∂‰º∏Ôºå‰ª•ÊèêÈ´òÂàÜÊÆµÊºîÁÆóÊ≥ïÁöÑÂìÅË≥™„ÄÇ‰∏Ä‰ΩçÁ∂ìÈ©óË±êÂØåÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´È©óË≠â‰∫ÜË≥áÊñôÔºåÂ∞áÂÖ∂‰æùÂìÅË≥™ÂàÜÈ°ûÊàêÂ≠êÈõÜÔºå‰∏¶Â∞áÂÖ∂ÁôºÂ∏ÉÂú®ÂÖ¨ÂÖ±È†òÂüü„ÄÇÊ†πÊìöÁç≤ÂæóÁöÑÁµêÊûúÔºåÊàëÂÄëË®ìÁ∑¥‰∫ÜÂÖ∑Êúâ 5 ÈÉ®ÂàÜ‰∫§ÂèâÈ©óË≠âÁöÑ UNet Êû∂ÊßãÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºå‰∏¶ÈÅîÂà∞‰∫Ü $0.6988 \pm 0.3$ ÁöÑ Dice ÊåáÊ®ôÂìÅË≥™„ÄÇÁôºÂ∏ÉÁöÑÊ®ôË®òÂ∞áÊèêÈ´òÂ§ßËÖ∏ÁôåÂÅµÊ∏¨ÁöÑÂìÅË≥™Ôºå‰∏¶Á∞°ÂåñÊîæÂ∞ÑÁßëÈÜ´Â∏´Á†îÁ©∂ÊèèËø∞ÁöÑÂ∑•‰Ωú„ÄÇ

##### **Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**
2407.21490v1 by Junxuan Yu, Rusi Chen, Yongsong Zhou, Yanlin Chen, Yaofei Duan, Yuhao Huang, Han Zhou, Tan Tao, Xin Yang, Dong Ni

Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÊòØË®∫Êñ∑ÂøÉËáüÁñæÁóÖÁöÑ‰∏ªË¶ÅÊñπÂºèÔºå
‰ΩÜÊúâÈôêÁöÑÊï∏ÊìöÂ∞çËá®Â∫äÊïôÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíË®ìÁ∑¥ÈÉΩÊßãÊàêÊåëÊà∞„ÄÇÊúÄËøëÔºåÂΩ±ÁâáÁîüÊàêÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Á∑©Ëß£Ê≠§ÂïèÈ°åÁöÑ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑËæ¶Ê≥ïÂú®ÁîüÊàêÈÅéÁ®ã‰∏≠ÈÄöÂ∏∏‰æùË≥¥Êï¥È´îÊ¢ù‰ª∂ÔºåÈòªÁ§ô‰∫ÜÂ∞çÁâπÂÆöÂøÉËáüÁµêÊßãÁöÑÈùàÊ¥ªÈÅãÂãïÊéßÂà∂„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂèØËß£Èáã‰∏îÂèØÊéßÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÁîüÊàêÊñπÊ≥ïÔºå‰ª•ÂàùÂßãÂπÄÂíåÈÅãÂãïÊõ≤Á∑ö‰ΩúÁÇ∫ÊåáÂ∞é„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÊúâ‰∏âÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæûÊØèÂÄãÂøÉËáüÂ≠êÁµêÊßã‰∏≠ÊèêÂèñÈÅãÂãïË≥áË®ä‰ª•Âª∫ÊßãÈÅãÂãïÊõ≤Á∑öÔºåËÆìÊì¥Êï£Ê®°ÂûãËÉΩÂ§†ÈÄèÈÅé‰øÆÊîπÈÄô‰∫õÊõ≤Á∑ö‰æÜÂêàÊàêÂÆ¢Ë£ΩÂåñÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±Áâá„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁµêÊßãÂà∞ÈÅãÂãïÂ∞çÈΩäÊ®°ÁµÑÔºåÂÆÉÂèØ‰ª•Â∞áË™ûÁæ©ÁâπÂæµÂ∞çÊáâÂà∞ÂøÉËáüÁµêÊßã‰∏≠ÁöÑÈÅãÂãïÊõ≤Á∑ö„ÄÇÁ¨¨‰∏âÔºå‰ΩçÁΩÆÊÑüÁü•Ê≥®ÊÑèÂäõÊ©üÂà∂Êó®Âú®Âà©Áî®ÂÖ∑ÊúâÁµêÊßã‰ΩçÁΩÆË≥áË®äÁöÑÈ´òÊñØÈÅÆÁΩ©‰æÜÂ¢ûÂº∑ÂΩ±ÁâáÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂú®‰∏âÂÄãË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑËæ¶Ê≥ïÂú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñËæ¶Ê≥ï„ÄÇÂÆåÊï¥Á®ãÂºèÁ¢ºÂ∞áÂú® https://github.com/mlmi-2024-72/ECM ‰∏äÈáãÂá∫„ÄÇ

##### **Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**
2407.21467v1 by Mengtian Kang, Yansong Hu, Shuo Gao, Yuanyuan Liu, Hongbei Meng, Xuemeng Li, Xuhang Chen, Hubin Zhao, Jing Fu, Guohua Hu, Wei Wang, Yanning Dai, Arokia Nathan, Peter Smielewski, Ningli Wang, Shiming Li

Childhood myopia constitutes a significant global health concern. It exhibits
an escalating prevalence and has the potential to evolve into severe,
irreversible conditions that detrimentally impact familial well-being and
create substantial economic costs. Contemporary research underscores the
importance of precisely predicting myopia progression to enable timely and
effective interventions, thereby averting severe visual impairment in children.
Such predictions predominantly rely on subjective clinical assessments, which
are inherently biased and resource-intensive, thus hindering their widespread
application. In this study, we introduce a novel, high-accuracy method for
quantitatively predicting the myopic trajectory and myopia risk in children
using only fundus images and baseline refraction data. This approach was
validated through a six-year longitudinal study of 3,408 children in Henan,
utilizing 16,211 fundus images and corresponding refractive data. Our method
based on deep learning demonstrated predictive accuracy with an error margin of
0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of
developing myopia and high myopia, respectively. These findings confirm the
utility of our model in supporting early intervention strategies and in
significantly reducing healthcare costs, particularly by obviating the need for
additional metadata and repeated consultations. Furthermore, our method was
designed to rely only on fundus images and refractive error data, without the
need for meta data or multiple inquiries from doctors, strongly reducing the
associated medical costs and facilitating large-scale screening. Our model can
even provide good predictions based on only a single time measurement.
Consequently, the proposed method is an important means to reduce medical
inequities caused by economic disparities.

ÊëòË¶ÅÔºöÂÖíÁ´•ËøëË¶ñÊßãÊàêÂÖ®ÁêÉÈáçË¶ÅÁöÑÂÅ•Â∫∑ÂïèÈ°å„ÄÇÂÆÉÈ°ØÁ§∫Âá∫Êó•ÁõäÂ¢ûÂä†ÁöÑÁõõË°åÁéáÔºå‰∏¶ÂèØËÉΩÊºîËÆäÊàêÂö¥Èáç„ÄÅ‰∏çÂèØÈÄÜËΩâÁöÑÁãÄÊ≥ÅÔºåÂ∞çÂÆ∂Â∫≠Á¶èÁ•âÈÄ†Êàê‰∏çÂà©ÂΩ±ÈüøÔºå‰∏¶Áî¢ÁîüÂ§ßÈáèÁöÑÁ∂ìÊøüÊàêÊú¨„ÄÇÁèæ‰ª£Á†îÁ©∂Âº∑Ë™øÁ≤æÊ∫ñÈ†êÊ∏¨ËøëË¶ñÈÄ≤Â±ïÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂØ¶ÁèæÂèäÊôÇÊúâÊïàÁöÑÂπ≤È†êÔºåÂæûËÄåÈÅøÂÖçÂÖíÁ´•Âá∫ÁèæÂö¥ÈáçÁöÑË¶ñÂäõÊêçÂÆ≥„ÄÇÊ≠§È°ûÈ†êÊ∏¨‰∏ªË¶Å‰æùË≥¥‰∏ªËßÄÁöÑËá®Â∫äË©ï‰º∞ÔºåÂÖ∂Êú¨Ë∫´ÂÖ∑ÊúâÂÅèË¶ã‰∏îË≥áÊ∫êÂØÜÈõÜÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©é„ÄÅÈ´òÁ≤æÁ¢∫Â∫¶ÁöÑÊñπÊ≥ïÔºåÂÉÖ‰ΩøÁî®ÁúºÂ∫ïÂúñÂÉèÂíåÂü∫Á∑öÂ±àÂÖâÊï∏ÊìöÔºåÂ∞±ËÉΩÂÆöÈáèÈ†êÊ∏¨ÂÖíÁ´•ÁöÑËøëË¶ñËªåË∑°ÂíåËøëË¶ñÈ¢®Èö™„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈÄöÈÅéÂ∞çÊ≤≥ÂçóÁúÅ 3,408 ÂêçÂÖíÁ´•ÈÄ≤Ë°åÁÇ∫ÊúüÂÖ≠Âπ¥ÁöÑÁ∏±ÂêëÁ†îÁ©∂ÔºåÂà©Áî® 16,211 ÂºµÁúºÂ∫ïÂúñÂÉèÂíåÁõ∏ÊáâÁöÑÂ±àÂÖâÊï∏ÊìöÈÄ≤Ë°å‰∫ÜÈ©óË≠â„ÄÇÊàëÂÄëÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÂπ¥Ë™§Â∑ÆÁØÑÂúçÁÇ∫ 0.311DÔºåÈ†êÊ∏¨ÁôºÁîüËøëË¶ñÂíåÈ´òÂ∫¶ËøëË¶ñÁöÑÈ¢®Èö™ÁöÑ AUC ÂàÜÊï∏ÂàÜÂà•ÁÇ∫ 0.944 Âíå 0.995„ÄÇÈÄô‰∫õÁôºÁèæË≠âÂØ¶‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîØÊåÅÊó©ÊúüÂπ≤È†êÁ≠ñÁï•ÂíåÈ°ØËëóÈôç‰ΩéÈÜ´ÁôÇ‰øùÂÅ•ÊàêÊú¨ÊñπÈù¢ÁöÑÊïàÁî®ÔºåÁâπÂà•ÊòØÈÄöÈÅéÊ∂àÈô§Â∞çÈ°çÂ§ñÂÖÉÊï∏ÊìöÂíåÈáçË§áË´ÆË©¢ÁöÑÈúÄË¶Å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊñπÊ≥ïË¢´Ë®≠Ë®àÁÇ∫ÂÉÖ‰æùË≥¥ÁúºÂ∫ïÂúñÂÉèÂíåÂ±àÂÖâ‰∏çÊ≠£Êï∏ÊìöÔºåËÄåÁÑ°ÈúÄÂÖÉÊï∏ÊìöÊàñÈÜ´ÁîüÁöÑÂ§öÊ¨°Ë©¢ÂïèÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜÁõ∏ÈóúÁöÑÈÜ´ÁôÇÊàêÊú¨Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÂ§ßË¶èÊ®°ÁØ©Êü•„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÁîöËá≥ÂèØ‰ª•ÂÉÖÊ†πÊìöÂñÆÊ¨°ÊôÇÈñìÊ∏¨ÈáèÊèê‰æõËâØÂ•ΩÁöÑÈ†êÊ∏¨„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊòØÊ∏õÂ∞ëÁî±Á∂ìÊøüÂ∑ÆË∑ùÈÄ†ÊàêÁöÑÈÜ´ÁôÇ‰∏çÂπ≥Á≠âÁöÑÈáçË¶ÅÊâãÊÆµ„ÄÇ

##### **Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**
2407.21368v1 by Danfeng Guo, Demetri Terzopoulos

Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.

ÊëòË¶ÅÔºöÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) Âú®ËøëÂπ¥‰æÜÂèñÂæóÈ°ØËëóÁöÑÊàêÂäüÔºå‰∏¶Â∑≤Êì¥Â±ïÂà∞ÈÜ´ÁôÇÈ†òÂüü„ÄÇÂÑòÁÆ°Âú®ÈÜ´Â≠∏Ë¶ñË¶∫ÂïèÁ≠î (VQA) ‰ªªÂãô‰∏≠Ë°®Áèæ‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜÈÜ´Â≠∏ LVLMs (MLVLMs) ‰ªçÂ≠òÂú®ÂπªË¶∫ÂïèÈ°åÔºåÂ∞éËá¥ÂÆÉÂÄëÁÑ°Ê≥ïË®∫Êñ∑Âá∫Ë§áÈõúÁöÑÁóÖÁêÜ„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñô‰∏çÂπ≥Ë°°ÔºåÂÆÉÂÄëÂæàÂÆπÊòìÁÑ°Ê≥ïÂ≠∏ÁøíÂ∞ëÊï∏ÁóÖÁêÜ„ÄÇÊàëÂÄëÊèêÂá∫ÂÖ©Á®ÆÈáùÂ∞ç MLVLMs ÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰ª•Ê∏õÂ∞ëÂπªË¶∫‰∏¶ÊîπÂñÑ VQA ÊïàËÉΩ„ÄÇÂú®Á¨¨‰∏ÄÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÊèê‰æõÊü•Ë©¢ÁóÖÁêÜÁöÑË©≥Á¥∞Ë™™Êòé„ÄÇÂú®Á¨¨‰∫åÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÂæÆË™ø‰∏ÄÂÄã‰æøÂÆú„ÄÅÊïàËÉΩ‰∏ç‰Ω≥ÁöÑÂ≠∏ÁøíÂô®Ôºå‰ª•Âú®ÁâπÂÆöÊåáÊ®ô‰∏äÁç≤ÂæóÈ´òÊïàËÉΩÔºå‰∏¶‰ª•ÊñáÂ≠óÊñπÂºèÂêë MLVLM Êèê‰æõÂÖ∂Âà§Êñ∑„ÄÇÂú® MIMIC-CXR-JPG Âíå Chexpert Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÂæåÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÊîπÂñÑ‰∫ÜË®∫Êñ∑ F1 ÂàÜÊï∏ÔºåÊúÄÈ´òÊèêÂçáÂπÖÂ∫¶ÁÇ∫ 0.27„ÄÇÊàëÂÄëÈÇÑÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊèêÁ§∫Á≠ñÁï•ÂèØ‰ª•Êì¥Â±ïÂà∞‰∏ÄËà¨ÁöÑ LVLM È†òÂüü„ÄÇÊ†πÊìö POPE ÊåáÊ®ôÔºåÂÆÉÊúâÊïàÂú∞ÊäëÂà∂‰∫ÜÁèæÊúâ LVLMs ÁöÑÂÅáÈô∞ÊÄßÈ†êÊ∏¨Ôºå‰∏¶Â∞áÂè¨ÂõûÁéáÊèêÈ´ò‰∫ÜÁ¥Ñ 0.07„ÄÇ

##### **MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation Framework**
2407.21343v1 by Adrian Celaya, Evan Lim, Rachel Glenn, Brayden Mi, Alex Balsells, Tucker Netherton, Caroline Chung, Beatrice Riviere, David Fuentes

Medical imaging segmentation is a highly active area of research, with deep
learning-based methods achieving state-of-the-art results in several
benchmarks. However, the lack of standardized tools for training, testing, and
evaluating new methods makes the comparison of methods difficult. To address
this, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,
modular, and end-to-end medical imaging segmentation framework designed to
facilitate consistent training, testing, and evaluation of deep learning-based
medical imaging segmentation methods. MIST standardizes data analysis,
preprocessing, and evaluation pipelines, accommodating multiple architectures
and loss functions. This standardization ensures reproducible and fair
comparisons across different methods. We detail MIST's data format
requirements, pipelines, and auxiliary features and demonstrate its efficacy
using the BraTS Adult Glioma Post-Treatment Challenge dataset. Our results
highlight MIST's ability to produce accurate segmentation masks and its
scalability across multiple GPUs, showcasing its potential as a powerful tool
for future medical imaging research and development.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊòØ‰∏ÄÂÄãÈ´òÂ∫¶Ê¥ªË∫çÁöÑÁ†îÁ©∂È†òÂüüÔºåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÂú®Â§öÂÄãÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÁº∫‰πèÊ®ôÊ∫ñÂåñÁöÑË®ìÁ∑¥„ÄÅÊ∏¨Ë©¶ÂíåË©ï‰º∞Êñ∞ÊñπÊ≥ïÁöÑÂ∑•ÂÖ∑Ôºå‰ΩøÂæóÊñπÊ≥ïÁöÑÊØîËºÉËÆäÂæóÂõ∞Èõ£„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Â∑•ÂÖ∑ÂåÖ (MIST)Ôºå‰∏ÄÂÄãÁ∞°ÂñÆ„ÄÅÊ®°ÁµÑÂåñÂíåÁ´ØÂ∞çÁ´ØÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ê°ÜÊû∂ÔºåÊó®Âú®‰øÉÈÄ≤Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊñπÊ≥ïÁöÑ‰∏ÄËá¥Ë®ìÁ∑¥„ÄÅÊ∏¨Ë©¶ÂíåË©ï‰º∞„ÄÇMIST Ê®ôÊ∫ñÂåñ‰∫ÜÊï∏ÊìöÂàÜÊûê„ÄÅÈ†êËôïÁêÜÂíåË©ï‰º∞ÁÆ°ÈÅìÔºåÂÆπÁ¥çÂ§öÁ®ÆÊû∂ÊßãÂíåÊêçÂ§±ÂáΩÊï∏„ÄÇÈÄôÁ®ÆÊ®ôÊ∫ñÂåñÁ¢∫‰øù‰∫Ü‰∏çÂêåÊñπÊ≥ï‰πãÈñìÂèØÈáçÁèæ‰∏îÂÖ¨Âπ≥ÁöÑÊØîËºÉ„ÄÇÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫Ü MIST ÁöÑÊï∏ÊìöÊ†ºÂºèË¶ÅÊ±Ç„ÄÅÁÆ°ÈÅìÂíåËºîÂä©ÂäüËÉΩÔºå‰∏¶‰ΩøÁî® BraTS Êàê‰∫∫Á•ûÁ∂ìËÜ†Ë≥™Áò§Ê≤ªÁôÇÂæåÊåëÊà∞Êï∏ÊìöÈõÜÂ±ïÁ§∫‰∫ÜÂÆÉÁöÑÂäüÊïà„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÈ°Ø‰∫Ü MIST Áî¢ÁîüÊ∫ñÁ¢∫ÂàÜÂâ≤ÈÅÆÁΩ©ÁöÑËÉΩÂäõ‰ª•ÂèäÂÆÉË∑®Â§öÂÄã GPU ÁöÑÂèØÊì¥Â±ïÊÄßÔºåÂ±ïÁ§∫‰∫ÜÂÆÉ‰ΩúÁÇ∫Êú™‰æÜÈÜ´Â≠∏ÂΩ±ÂÉèÁ†îÁ©∂ÂíåÈñãÁôºÁöÑÊúâÂäõÂ∑•ÂÖ∑ÁöÑÊΩõÂäõ„ÄÇ

##### **Robust Box Prompt based SAM for Medical Image Segmentation**
2407.21284v1 by Yuhao Huang, Xin Yang, Han Zhou, Yan Cao, Haoran Dou, Fajin Dong, Dong Ni

The Segment Anything Model (SAM) can achieve satisfactory segmentation
performance under high-quality box prompts. However, SAM's robustness is
compromised by the decline in box quality, limiting its practicality in
clinical reality. In this study, we propose a novel Robust Box prompt based SAM
(\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts
with different qualities. Our contribution is three-fold. First, we propose a
prompt refinement module to implicitly perceive the potential targets, and
output the offsets to directly transform the low-quality box prompt into a
high-quality one. We then provide an online iterative strategy for further
prompt refinement. Second, we introduce a prompt enhancement module to
automatically generate point prompts to assist the box-promptable segmentation
effectively. Last, we build a self-information extractor to encode the prior
information from the input image. These features can optimize the image
embeddings and attention calculation, thus, the robustness of SAM can be
further enhanced. Extensive experiments on the large medical segmentation
dataset including 99,299 images, 5 modalities, and 25 organs/targets validated
the efficacy of our proposed RoBox-SAM.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ÂèØ‰ª•Âú®È´òË¥®ÈáèÊ°ÜÊèêÁ§∫‰∏ãÂÆûÁé∞‰ª§‰∫∫Êª°ÊÑèÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåSAM ÁöÑÈ≤ÅÊ£íÊÄßÂõ†Ê°ÜË¥®ÈáèÁöÑ‰∏ãÈôçËÄåÂèóÂà∞ÊçüÂÆ≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏¥Â∫äÁé∞ÂÆû‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫é SAM ÁöÑÊñ∞ÂûãÈ≤ÅÊ£íÊ°ÜÊèêÁ§∫Ôºà**RoBox-SAM**ÔºâÔºå‰ª•Á°Æ‰øù SAM Âú®ÂÖ∑Êúâ‰∏çÂêåË¥®ÈáèÁöÑÊèêÁ§∫‰∏ãÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÊòØ‰∏âÊñπÈù¢ÁöÑ„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÊèêÂá∫‰∏Ä‰∏™ÊèêÁ§∫‰ºòÂåñÊ®°ÂùóÔºå‰ª•ÈöêÂºèÊÑüÁü•ÊΩúÂú®ÁõÆÊ†áÔºåÂπ∂ËæìÂá∫ÂÅèÁßªÈáèÔºå‰ª•Áõ¥Êé•Â∞Ü‰ΩéË¥®ÈáèÊ°ÜÊèêÁ§∫ËΩ¨Êç¢‰∏∫È´òË¥®ÈáèÊèêÁ§∫„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âú®Á∫øËø≠‰ª£Á≠ñÁï•Ôºå‰ª•‰æøËøõ‰∏ÄÊ≠•‰ºòÂåñÊèêÁ§∫„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫Â¢ûÂº∫Ê®°ÂùóÔºå‰ª•Ëá™Âä®ÁîüÊàêÁÇπÊèêÁ§∫Ôºå‰ª•ÊúâÊïàÂú∞ËæÖÂä©Ê°ÜÊèêÁ§∫ÂàÜÊÆµ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ëá™‰ø°ÊÅØÊèêÂèñÂô®Ôºå‰ª•ÂØπÊù•Ëá™ËæìÂÖ•ÂõæÂÉèÁöÑÂÖàÈ™å‰ø°ÊÅØËøõË°åÁºñÁ†Å„ÄÇËøô‰∫õÁâπÂæÅÂèØ‰ª•‰ºòÂåñÂõæÂÉèÂµåÂÖ•ÂíåÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÂõ†Ê≠§ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ SAM ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®ÂåÖÊã¨ 99,299 Âº†ÂõæÂÉè„ÄÅ5 ÁßçÊñπÂºèÂíå 25 ‰∏™Âô®ÂÆò/ÁõÆÊ†áÁöÑÂ§ßÂûãÂåªÂ≠¶ÂàÜÊÆµÊï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÂπøÊ≥õÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑ RoBox-SAM ÁöÑÂäüÊïà„ÄÇ

##### **Unlocking the Potential of Binding Corporate Rules (BCRs) in Health Data Transfers**
2407.21281v1 by Marcelo Corrales Compagnucci, Mark Fenwick, Helena Haapio

This chapter explores the essential role of Binding Corporate Rules (BCRs) in
managing and facilitating secure health data transfers within corporate groups
under the EU General Data Protection Regulation (GDPR). BCRs are tailored to
ensure compliance with the GDPR and similar international data protection laws,
presenting a flexible mechanism for transferring sensitive health and genomic
data. The chapter situates BCRs within the broader spectrum of the GDPR
international data transfer mechanisms, addressing the unique challenges posed
by the sensitive nature of health data and the increased adoption of AI
technologies. The European Data Protection Board (EDPB) Recommendations 1/2022
on BCRs, issued following the Schrems II decision, are critically analyzed,
highlighting their stringent requirements and the need for a balanced approach
that prioritizes data protection and an AI governance framework. The chapter
outlines the BCR approval process, stressing the importance of streamlining
this process to encourage broader adoption. It underscores the necessity of a
multidisciplinary approach in developing BCRs, incorporating recently adopted
international standards and frameworks, which offer valuable guidance for
organizations to build trustworthy AI management systems. They guarantee the
ethical development, deployment, and operation of AI, which is essential for
its successful integration and the broader digital transformation. In
conclusion, BCRs are positioned as essential tools for secure health data
management, fostering transparency, accountability, and collaboration across
international borders. The chapter calls for proactive measures to incentivize
BCR adoption, streamline approval processes, and promote more innovative
approaches, ensuring BCRs remain a robust mechanism for global data protection
and compliance.

ÊëòË¶ÅÔºö<paragraph>Ê≠§Á´†Êé¢Ë®éÁ¥ÑÊùü‰ºÅÊ•≠Ë¶èÂâá (BCR) Âú®Ê≠êÁõü‰∏ÄËà¨Ë≥áÊñô‰øùË≠∑Ê¢ù‰æã (GDPR) ‰∏ãÁÆ°ÁêÜÂíå‰øÉÈÄ≤‰ºÅÊ•≠ÈõÜÂúòÂÖßÈÉ®ÂÆâÂÖ®ÂÅ•Â∫∑Ë≥áÊñôÂÇ≥Ëº∏ÁöÑÂü∫Êú¨ËßíËâ≤„ÄÇBCR Â∞àÈñÄÁî®ÊñºÁ¢∫‰øùÁ¨¶Âêà GDPR ÂíåÈ°û‰ººÁöÑÂúãÈöõË≥áÊñô‰øùË≠∑Ê≥ïÔºåÊèê‰æõÂÇ≥Ëº∏ÊïèÊÑüÂÅ•Â∫∑ÂíåÂü∫Âõ†ÁµÑË≥áÊñôÁöÑÂΩàÊÄßÊ©üÂà∂„ÄÇÊ≠§Á´†Â∞á BCR ÂÆö‰ΩçÂú® GDPR ÂúãÈöõË≥áÊñôÂÇ≥Ëº∏Ê©üÂà∂ÁöÑÊõ¥Âª£Ê≥õÁØÑÂúçÂÖßÔºåËß£Ê±∫ÂÅ•Â∫∑Ë≥áÊñôÊïèÊÑüÊÄßË≥™Âíå AI ÊäÄË°ìÊé°Áî®Â¢ûÂä†ÊâÄÂ∏∂‰æÜÁöÑÁç®ÁâπÊåëÊà∞„ÄÇÊ≠êÊ¥≤Ë≥áÊñô‰øùË≠∑ÂßîÂì°ÊúÉ (EDPB) Âú® Schrems II Ê±∫ÂÆöÂæåÁôºÂ∏ÉÁöÑ BCR Âª∫Ë≠∞ 1/2022 ÂèóÂà∞Âö¥Ê†ºÂàÜÊûêÔºåÂº∑Ë™øÂÖ∂Âö¥Ê†ºË¶ÅÊ±ÇÂíåÂπ≥Ë°°ÊñπÊ≥ïÁöÑÂøÖË¶ÅÊÄßÔºåË©≤ÊñπÊ≥ïÂÑ™ÂÖàËÄÉÊÖÆË≥áÊñô‰øùË≠∑Âíå AI Ê≤ªÁêÜÊû∂Êßã„ÄÇÊ≠§Á´†Ê¶ÇËø∞ BCR Ê†∏ÂáÜÁ®ãÂ∫èÔºåÂº∑Ë™øÁ∞°ÂåñÊ≠§Á®ãÂ∫è‰ª•ÈºìÂãµÊõ¥Âª£Ê≥õÊé°Áî®ÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆÉÂº∑Ë™øÂú®ÈñãÁôº BCR ÊôÇÊé°Áî®Â§öÂ≠∏ÁßëÊñπÊ≥ïÁöÑÂøÖË¶ÅÊÄßÔºåÂåÖÊã¨ÊúÄËøëÊé°Áî®ÁöÑÂúãÈöõÊ®ôÊ∫ñÂíåÊû∂ÊßãÔºåÈÄô‰∫õÊ®ôÊ∫ñÂíåÊû∂ÊßãÁÇ∫ÁµÑÁπîÂª∫Á´ãÂèØ‰ø°Ë≥¥ÁöÑ AI ÁÆ°ÁêÜÁ≥ªÁµ±Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑÊåáÂ∞é„ÄÇÂÆÉÂÄë‰øùË≠â AI ÁöÑÈÅìÂæ∑ÈñãÁôº„ÄÅÈÉ®ÁΩ≤ÂíåÈÅã‰ΩúÔºåÈÄôÂ∞çÂÖ∂ÊàêÂäüÊï¥ÂêàÂíåÊõ¥Âª£Ê≥õÁöÑÊï∏‰ΩçËΩâÂûãËá≥ÈóúÈáçË¶Å„ÄÇÁµêË´ñÊòØÔºåBCR Ë¢´ÂÆö‰ΩçÁÇ∫ÂÆâÂÖ®ÂÅ•Â∫∑Ë≥áÊñôÁÆ°ÁêÜÁöÑÂü∫Êú¨Â∑•ÂÖ∑Ôºå‰øÉÈÄ≤Ë∑®ÂúãÁïåÁöÑÈÄèÊòéÂ∫¶„ÄÅÂïèË≤¨Âà∂ÂíåÂçî‰Ωú„ÄÇÊ≠§Á´†ÂëºÁ±≤Êé°ÂèñÁ©çÊ•µÊé™ÊñΩ‰æÜÊøÄÂãµ BCR Êé°Áî®„ÄÅÁ∞°ÂåñÊ†∏ÂáÜÁ®ãÂ∫èÔºå‰∏¶‰øÉÈÄ≤Êõ¥ÂÖ∑ÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÁ¢∫‰øù BCR ‰ªçÁÑ∂ÊòØÂÖ®ÁêÉË≥áÊñô‰øùË≠∑ÂíåÂêàË¶èÊÄßÁöÑÂº∑Â§ßÊ©üÂà∂„ÄÇ</paragraph>

##### **FreqTSF: Time Series Forecasting Via Simulating Frequency Kramer-Kronig Relations**
2407.21275v1 by Rujia Shen, Liangliang Liu, Boran Wang, Yi Guan, Yang Yang, Jingchi Jiang

Time series forecasting (TSF) is immensely important in extensive
applications, such as electricity transformation, financial trade, medical
monitoring, and smart agriculture. Although Transformer-based methods can
handle time series data, their ability to predict long-term time series is
limited due to the ``anti-order" nature of the self-attention mechanism. To
address this problem, we focus on frequency domain to weaken the impact of
order in TSF and propose the FreqBlock, where we first obtain frequency
representations through the Frequency Transform Module. Subsequently, a newly
designed Frequency Cross Attention is used to obtian enhanced frequency
representations between the real and imaginary parts, thus establishing a link
between the attention mechanism and the inherent Kramer-Kronig relations
(KKRs). Our backbone network, FreqTSF, adopts a residual structure by
concatenating multiple FreqBlocks to simulate KKRs in the frequency domain and
avoid degradation problems. On a theoretical level, we demonstrate that the
proposed two modules can significantly reduce the time and memory complexity
from $\mathcal{O}(L^2)$ to $\mathcal{O}(L)$ for each FreqBlock computation.
Empirical studies on four benchmark datasets show that FreqTSF achieves an
overall relative MSE reduction of 15\% and an overall relative MAE reduction of
11\% compared to the state-of-the-art methods. The code will be available soon.

ÊëòË¶ÅÔºöÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ (TSF) Âú®Âª£Ê≥õÁöÑÊáâÁî®‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÔºå‰æãÂ¶ÇÈõªÂäõËΩâÊèõ„ÄÅÈáëËûç‰∫§Êòì„ÄÅÈÜ´ÁôÇÁõ£ÊéßÂíåÊô∫ÊÖßËæ≤Ê•≠„ÄÇÈõñÁÑ∂Âü∫Êñº Transformer ÁöÑÊñπÊ≥ïÂèØ‰ª•ËôïÁêÜÊôÇÈñìÂ∫èÂàóË≥áÊñôÔºå‰ΩÜÁî±ÊñºËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑ„ÄåÂèçÂ∫è„ÄçÁâπÊÄßÔºåÂÆÉÂÄëÈ†êÊ∏¨Èï∑ÊúüÊôÇÈñìÂ∫èÂàóÁöÑËÉΩÂäõÂèóÂà∞ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÈ†ªÂüü‰ª•Ê∏õÂº± TSF ‰∏≠È†ÜÂ∫èÁöÑÂΩ±ÈüøÔºå‰∏¶ÊèêÂá∫ FreqBlockÔºåÊàëÂÄëÈ¶ñÂÖàÈÄèÈÅéÈ†ªÁéáËΩâÊèõÊ®°ÁµÑÂèñÂæóÈ†ªÁéáË°®Á§∫„ÄÇÈö®ÂæåÔºå‰ΩøÁî®Êñ∞Ë®≠Ë®àÁöÑÈ†ªÁéá‰∫§ÂèâÊ≥®ÊÑèÂäõ‰æÜÁç≤ÂæóÂØ¶ÈÉ®ÂíåËôõÈÉ®‰πãÈñìÂ¢ûÂº∑ÁöÑÈ†ªÁéáË°®Á§∫ÔºåÂæûËÄåÂª∫Á´ãÊ≥®ÊÑèÂäõÊ©üÂà∂ÂíåÂõ∫Êúâ Kramer-Kronig Èóú‰øÇ (KKR) ‰πãÈñìÁöÑÈÄ£Áµê„ÄÇÊàëÂÄëÁöÑÈ™®ÂππÁ∂≤Ë∑Ø FreqTSF Êé°Áî®ÊÆòÂ∑ÆÁµêÊßãÔºåÈÄèÈÅé‰∏≤Êé•Â§öÂÄã FreqBlock ‰æÜÊ®°Êì¨È†ªÂüü‰∏≠ÁöÑ KKR ‰∏¶ÈÅøÂÖçÈÄÄÂåñÂïèÈ°å„ÄÇÂú®ÁêÜË´ñÂ±§Èù¢‰∏äÔºåÊàëÂÄëË≠âÊòéÊâÄÊèêÂá∫ÁöÑÂÖ©ÂÄãÊ®°ÁµÑÂèØ‰ª•È°ØËëóÈôç‰ΩéÊØèÂÄã FreqBlock Ë®àÁÆóÁöÑÊôÇÈñìÂíåË®òÊÜ∂È´îË§áÈõúÂ∫¶ÔºåÂæû $\mathcal{O}(L^2)$ Èôç‰ΩéÂà∞ $\mathcal{O}(L)$„ÄÇÂú®ÂõõÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶Ë≠âÁ†îÁ©∂È°ØÁ§∫ÔºåËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåFreqTSF ÁöÑÊï¥È´îÁõ∏Â∞ç MSE Èôç‰Ωé 15%ÔºåÊï¥È´îÁõ∏Â∞ç MAE Èôç‰Ωé 11%„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂæàÂø´Êé®Âá∫„ÄÇ

##### **Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**
2407.21273v1 by Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski

Efficient intravascular access in trauma and critical care significantly
impacts patient outcomes. However, the availability of skilled medical
personnel in austere environments is often limited. Autonomous robotic
ultrasound systems can aid in needle insertion for medication delivery and
support non-experts in such tasks. Despite advances in autonomous needle
insertion, inaccuracies in vessel segmentation predictions pose risks.
Understanding the uncertainty of predictive models in ultrasound imaging is
crucial for assessing their reliability. We introduce MSU-Net, a novel
multistage approach for training an ensemble of U-Nets to yield accurate
ultrasound image segmentation maps. We demonstrate substantial improvements,
18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model
transparency, and trustworthiness. By highlighting areas of model certainty,
MSU-Net can guide safe needle insertions, empowering non-experts to accomplish
such tasks.

ÊëòË¶ÅÔºöÂú®ÂâµÂÇ∑ÂíåÈáçÁóáÁÖßË≠∑‰∏≠ÔºåÊúâÊïàÁöÑË°ÄÁÆ°ÂÖßÈÄöË∑ØÊúÉÈ°ØËëóÂΩ±ÈüøÁóÖÊÇ£ÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåÂú®ÊÉ°Âä£ÁöÑÁí∞Â¢É‰∏≠ÔºåÁÜüÁ∑¥ÁöÑÈÜ´ÁôÇ‰∫∫Âì°ÂæÄÂæÄ‰∏çË∂≥„ÄÇËá™‰∏ªÊ©üÂô®‰∫∫Ë∂ÖÈü≥Ê≥¢Á≥ªÁµ±ÂèØ‰ª•ÂçîÂä©ÈáùÈ†≠ÊèíÂÖ•Ôºå‰ª•Êèê‰æõËó•Áâ©‰∏¶ÊîØÊè¥ÈùûÂ∞àÂÆ∂Âü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇÂÑòÁÆ°Ëá™‰∏ªÈáùÈ†≠ÊèíÂÖ•ÊäÄË°ìÈÄ≤Ê≠•Ôºå‰ΩÜË°ÄÁÆ°ÂàÜÂâ≤È†êÊ∏¨ÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄßÊúÉÈÄ†ÊàêÈ¢®Èö™„ÄÇ‰∫ÜËß£Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏≠È†êÊ∏¨Ê®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂ∞çÊñºË©ï‰º∞ÂÖ∂ÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂºïÈÄ≤ MSU-NetÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ§öÈöéÊÆµÊñπÊ≥ïÔºåÁî®ÊñºË®ìÁ∑¥‰∏ÄÁµÑ U-Net ‰ª•Áî¢ÁîüÊ∫ñÁ¢∫ÁöÑË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂàÜÂâ≤Âúñ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ§ßÂπÖÊîπÂñÑÔºåÊØîÂñÆ‰∏ÄÁöÑËíôÂú∞Âç°ÁæÖ U-Net ÊîπÂñÑ‰∫Ü 18.1%ÔºåÂ¢ûÂº∑‰∫Ü‰∏çÁ¢∫ÂÆöÊÄßË©ï‰º∞„ÄÅÊ®°ÂûãÈÄèÊòéÂ∫¶ÂíåÂèØ‰ø°Â∫¶„ÄÇÈÄèÈÅéÂº∑Ë™øÊ®°ÂûãÁ¢∫ÂÆöÊÄßÁöÑÂçÄÂüüÔºåMSU-Net ÂèØ‰ª•ÂºïÂ∞éÂÆâÂÖ®ÁöÑÈáùÈ†≠ÊèíÂÖ•ÔºåËÆìÈùûÂ∞àÂÆ∂‰πüËÉΩÂü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇ

##### **Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**
2407.21149v1 by Mayanka Chandrashekar, Ian Goethert, Md Inzamam Ul Haque, Benjamin McMahon, Sayera Dhaubhadel, Kathryn Knight, Joseph Erdos, Donna Reagan, Caroline Taylor, Peter Kuzmak, John Michael Gaziano, Eileen McAllister, Lauren Costa, Yuk-Lam Ho, Kelly Cho, Suzanne Tamang, Samah Fodeh-Jarad, Olga S. Ovchinnikova, Amy C. Justice, Jacob Hinkle, Ioana Danciu

Objectives: This study aims to assess the impact of domain shift on chest
X-ray classification accuracy and to analyze the influence of ground truth
label quality and demographic factors such as age group, sex, and study year.
Materials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset
for deep learning-based multilabel classification using ground truth labels
from radiology reports extracted using the CheXpert and CheXbert Labeler. We
compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and
Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR
dataset comprises over 259k chest X-ray images spanning between the years 2010
and 2022. Results: The validation of ground truth and the assessment of
multi-label classification performance across various NLP extraction tools
revealed that the VA-CXR dataset exhibited lower disagreement rates than the
MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores
between models utilizing CheXpert and CheXbert. When evaluating multi-label
classification performance across different datasets, minimal domain shift was
observed in unseen datasets, except for the label "Enlarged Cardiomediastinum."
The study year's subgroup analyses exhibited the most significant variations in
multi-label classification model performance. These findings underscore the
importance of considering domain shifts in chest X-ray classification tasks,
particularly concerning study years. Conclusion: Our study reveals the
significant impact of domain shift and demographic factors on chest X-ray
classification, emphasizing the need for improved transfer learning and
equitable model development. Addressing these challenges is crucial for
advancing medical imaging and enhancing patient care.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÊ®ôÔºöÊú¨Á†îÁ©∂Êó®Âú®Ë©ï‰º∞È†òÂüüËΩâÁßªÂ∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁ≤æÂ∫¶ÁöÑÂΩ±ÈüøÔºå‰∏¶ÂàÜÊûêÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÂìÅË≥™ÂíåÂπ¥ÈΩ°ÁµÑ„ÄÅÊÄßÂà•ÂíåÁ†îÁ©∂Âπ¥‰ªΩÁ≠â‰∫∫Âè£Âõ†Á¥†ÁöÑÂΩ±Èüø„ÄÇ
ÊùêÊñôÂíåÊñπÊ≥ïÔºöÊàëÂÄë‰ΩøÁî® DenseNet121 Ê®°ÂûãÈ†êË®ìÁ∑¥ MIMIC-CXR Ë≥áÊñôÈõÜÔºå‰ΩøÁî®Âæû‰ΩøÁî® CheXpert Âíå CheXbert Ê®ôÁ±§Âô®ÂæûÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠ÊèêÂèñÁöÑÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÈÄ≤Ë°åÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°û„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü MIMIC-CXR ÂíåÈÄÄ‰ºçËªç‰∫∫ÂÅ•Â∫∑ÁÆ°ÁêÜÂ±ÄËÉ∏ÈÉ® X ÂÖâË≥áÊñôÈõÜ (VA-CXR) ‰∏ä 14 ÂÄãËÉ∏ÈÉ® X ÂÖâÊ®ôÁ±§ÁöÑÊÄßËÉΩ„ÄÇVA-CXR Ë≥áÊñôÈõÜÂåÖÂê´Ë∂ÖÈÅé 259k ÂºµËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÔºåÊôÇÈñìË∑®Â∫¶ÁÇ∫ 2010 Âπ¥Ëá≥ 2022 Âπ¥„ÄÇÁµêÊûúÔºöÂü∫Êú¨‰∫ãÂØ¶ÁöÑÈ©óË≠âÂíåÂ∞çÂêÑÁ®Æ NLP ÊèêÂèñÂ∑•ÂÖ∑ÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÁöÑË©ï‰º∞È°ØÁ§∫ÔºåVA-CXR Ë≥áÊñôÈõÜË°®ÁèæÂá∫ÁöÑÂàÜÊ≠ßÁéá‰ΩéÊñº MIMIC-CXR Ë≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî® CheXpert Âíå CheXbert ÁöÑÊ®°Âûã‰πãÈñìÁöÑ AUC ÂæóÂàÜÂ≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇÂú®Ë©ï‰º∞‰∏çÂêåË≥áÊñôÈõÜ‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÊôÇÔºåÈô§‰∫ÜÊ®ôÁ±§„ÄåÂøÉÁ∏±ÈöîÂ¢ûÂ§ß„Äç‰πãÂ§ñÔºåÂú®Êú™Ë¶ãË≥áÊñôÈõÜ‰∏≠ËßÄÂØüÂà∞ÁöÑÈ†òÂüüËΩâÁßªÂæàÂ∞è„ÄÇÁ†îÁ©∂Âπ¥‰ªΩÁöÑÂ≠êÁæ§ÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊ®°ÂûãÊÄßËÉΩËÆäÂåñÊúÄÂ§ß„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÂú®ËÉ∏ÈÉ® X ÂÖâÂàÜÈ°û‰ªªÂãô‰∏≠ËÄÉÊÖÆÈ†òÂüüËΩâÁßªÁöÑÈáçË¶ÅÊÄßÔºåÁâπÂà•ÊòØÈóúÊñºÁ†îÁ©∂Âπ¥‰ªΩ„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫ÜÈ†òÂüüËΩâÁßªÂíå‰∫∫Âè£Âõ†Á¥†Â∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁöÑÈ°ØËëóÂΩ±ÈüøÔºåÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂÖ¨Âπ≥Ê®°ÂûãÈñãÁôºÁöÑÂøÖË¶ÅÊÄß„ÄÇÊáâÂ∞çÈÄô‰∫õÊåëÊà∞Â∞çÊñºÊé®ÈÄ≤ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÂä†Âº∑ÊÇ£ËÄÖË≠∑ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

##### **Zero Shot Health Trajectory Prediction Using Transformer**
2407.21124v1 by Pawel Renc, Yugang Jia, Anthony E. Samir, Jaroslaw Was, Quanzheng Li, David W. Bates, Arkadiusz Sitek

Integrating modern machine learning and clinical decision-making has great
promise for mitigating healthcare's increasing cost and complexity. We
introduce the Enhanced Transformer for Health Outcome Simulation (ETHOS), a
novel application of the transformer deep-learning architecture for analyzing
high-dimensional, heterogeneous, and episodic health data. ETHOS is trained
using Patient Health Timelines (PHTs)-detailed, tokenized records of health
events-to predict future health trajectories, leveraging a zero-shot learning
approach. ETHOS represents a significant advancement in foundation model
development for healthcare analytics, eliminating the need for labeled data and
model fine-tuning. Its ability to simulate various treatment pathways and
consider patient-specific factors positions ETHOS as a tool for care
optimization and addressing biases in healthcare delivery. Future developments
will expand ETHOS' capabilities to incorporate a wider range of data types and
data sources. Our work demonstrates a pathway toward accelerated AI development
and deployment in healthcare.

ÊëòË¶ÅÔºöÊï¥ÂêàÁèæ‰ª£Ê©üÂô®Â≠∏ÁøíËàáËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂ∞çÊñºÊ∏õËºïÈÜ´ÁôÇ‰øùÂÅ•Êó•ÁõäÂ¢ûÂä†ÁöÑÊàêÊú¨ÂíåË§áÈõúÊÄßÂÖ∑ÊúâÂæàÂ§ßÁöÑÂâçÊôØ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂÅ•Â∫∑ÁµêÊûúÊ®°Êì¨ÁöÑÂ¢ûÂº∑ÂºèTransformerÔºàETHOSÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆTransformerÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÁöÑÊñ∞Á©éÊáâÁî®ÔºåÁî®ÊñºÂàÜÊûêÈ´òÁ∂≠„ÄÅÁï∞Ë≥™‰∏îÊÉÖÁØÄÊÄßÁöÑÂÅ•Â∫∑Êï∏Êìö„ÄÇETHOS ‰ΩøÁî®ÊÇ£ËÄÖÂÅ•Â∫∑ÊôÇÈñìËª∏ (PHT) ÈÄ≤Ë°åË®ìÁ∑¥ÔºåPHT ÊòØÂÅ•Â∫∑‰∫ã‰ª∂ÁöÑË©≥Á¥∞„ÄÅÊ®ôË®òÂåñË®òÈåÑÔºåÁî®ÊñºÈ†êÊ∏¨Êú™‰æÜÁöÑÂÅ•Â∫∑ËªåË∑°Ôºå‰∏¶Âà©Áî®Èõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ï„ÄÇETHOS ‰ª£Ë°®‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•ÂàÜÊûêÂü∫Á§éÊ®°ÂûãÈñãÁôºÁöÑÈáçÂ§ßÈÄ≤Â±ïÔºåÊ∂àÈô§‰∫ÜÂ∞çÊ®ôË®òÊï∏ÊìöÂíåÊ®°ÂûãÂæÆË™øÁöÑÈúÄÊ±Ç„ÄÇÂÆÉÊ®°Êì¨ÂêÑÁ®ÆÊ≤ªÁôÇÈÄîÂæë‰∏¶ËÄÉÊÖÆÊÇ£ËÄÖÁâπÂÆöÂõ†Á¥†ÁöÑËÉΩÂäõÔºå‰Ωø ETHOS ÊàêÁÇ∫ÂÑ™ÂåñÁÖßË≠∑ÂíåËß£Ê±∫ÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõ‰∏≠ÂÅèÂ∑ÆÁöÑÂ∑•ÂÖ∑„ÄÇÊú™‰æÜÁöÑÁôºÂ±ïÂ∞áÊì¥Â±ï ETHOS ÁöÑÂäüËÉΩÔºå‰ª•Á¥çÂÖ•Êõ¥Âª£Ê≥õÁöÑÊï∏ÊìöÈ°ûÂûãÂíåÊï∏Êìö‰æÜÊ∫ê„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â±ïÁ§∫‰∫Ü‰∏ÄÊ¢ùÂä†ÈÄüÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ AI ÈñãÁôºÂíåÈÉ®ÁΩ≤ÁöÑÈÄîÂæë„ÄÇ

##### **CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning**
2407.21011v1 by Yuexi Du, Brian Chang, Nicha C. Dvornek

Recent advancements in Contrastive Language-Image Pre-training (CLIP) have
demonstrated notable success in self-supervised representation learning across
various tasks. However, the existing CLIP-like approaches often demand
extensive GPU resources and prolonged training times due to the considerable
size of the model and dataset, making them poor for medical applications, in
which large datasets are not always common. Meanwhile, the language model
prompts are mainly manually derived from labels tied to images, potentially
overlooking the richness of information within training samples. We introduce a
novel language-image Contrastive Learning method with an Efficient large
language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of
the extensive pre-trained language and visual models. Furthermore, we present
an efficient strategy for learning context-based prompts that mitigates the gap
between informative clinical diagnostic data and simple class labels. Our
method demonstrates state-of-the-art performance on multiple chest X-ray and
mammography datasets compared with various baselines. The proposed parameter
efficient framework can reduce the total trainable model size by 39% and reduce
the trainable language model to only 4% compared with the current BERT encoder.

ÊëòË¶ÅÔºöÂ∞çÊØîË™ûË®ÄÂΩ±ÂÉèÈ†êË®ìÁ∑¥ (CLIP) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤Â±ïÁèæÂá∫Âú®ÂêÑÈ†Ö‰ªªÂãô‰∏≠‰ª•Ëá™ÊàëÁõ£Áù£Ë°®ÂæµÂ≠∏ÁøíÁç≤ÂæóÈ°ØËëóÊàêÂäüÁöÑÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ CLIP È°û‰ººÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑ GPU Ë≥áÊ∫êÂíåÊº´Èï∑ÁöÑË®ìÁ∑¥ÊôÇÈñìÔºåÂõ†ÁÇ∫Ê®°ÂûãÂíåË≥áÊñôÈõÜÁöÑË¶èÊ®°ÈæêÂ§ßÔºåÈÄô‰ΩøÂæóÂÆÉÂÄë‰∏çÈÅ©ÂêàÈÜ´ÁôÇÊáâÁî®ÔºåÂõ†ÁÇ∫ÈÜ´ÁôÇÊáâÁî®‰∏≠‰∏¶‰∏çÁ∏ΩÊòØÊúÉÊúâÂ§ßÂûãË≥áÊñôÈõÜ„ÄÇÂêåÊôÇÔºåË™ûË®ÄÊ®°ÂûãÊèêÁ§∫‰∏ªË¶Å‰æÜËá™ËàáÂΩ±ÂÉèÁõ∏ÈóúÁöÑÊ®ôÁ±§ÔºåËÄåÊâãÂãïË°çÁîüÔºåÈÄôÂèØËÉΩÊúÉÂøΩÁï•Ë®ìÁ∑¥Ê®£Êú¨‰∏≠Ë±êÂØåÁöÑË≥áË®ä„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑË™ûË®ÄÂΩ±ÂÉèÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÂÄãÈ´òÊïàÁöÑÂ§ßË™ûË®ÄÊ®°ÂûãÂíåÊèêÁ§∫ÂæÆË™ø (CLEFT)ÔºåÂÆÉÂà©Áî®‰∫ÜÂª£Ê≥õÈ†êË®ìÁ∑¥ÁöÑË™ûË®ÄÂíåË¶ñË¶∫Ê®°ÂûãÁöÑÂÑ™Âã¢„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ≠∏ÁøíÂü∫ÊñºËÑàÁµ°ÊèêÁ§∫ÁöÑÊúâÊïàÁ≠ñÁï•Ôºå‰ª•Á∏ÆÂ∞èË≥áË®äË±êÂØåÁöÑËá®Â∫äË®∫Êñ∑Ë≥áÊñôÂíåÁ∞°ÂñÆÈ°ûÂà•Ê®ôÁ±§‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇËàáÂêÑÁ®ÆÂü∫Ê∫ñÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â§öÂÄãËÉ∏ÈÉ® X ÂÖâÂíå‰π≥ÊàøÊîùÂΩ±Ë≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑÂèÉÊï∏ÊúâÊïàÊû∂ÊßãÂèØ‰ª•Â∞áÁ∏ΩÈ´îÂèØË®ìÁ∑¥Ê®°ÂûãÂ§ßÂ∞èÊ∏õÂ∞ë 39%Ôºå‰∏¶Â∞áÂèØË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÊ∏õÂ∞ëÂà∞ÂÉÖ 4%ÔºåËàáÁõÆÂâçÁöÑ BERT Á∑®Á¢ºÂô®Áõ∏ÊØî„ÄÇ

##### **Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing**
2407.20830v1 by Eugenio Lomurno, Matteo Matteucci

Federated learning has emerged as a paradigm for collaborative learning,
enabling the development of robust models without the need to centralise
sensitive data. However, conventional federated learning techniques have
privacy and security vulnerabilities due to the exposure of models, parameters
or updates, which can be exploited as an attack surface. This paper presents
Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach
that uses locally generated synthetic data to facilitate collaboration between
institutions. FedKR combines advanced data generation techniques with a dynamic
aggregation process to provide greater security against privacy attacks than
existing methods, significantly reducing the attack surface. Experimental
results on generic and medical datasets show that FedKR achieves competitive
performance, with an average improvement in accuracy of 4.24% compared to
training models from local data, demonstrating particular effectiveness in data
scarcity scenarios.

ÊëòË¶ÅÔºöËÅØÈÇ¶Â≠∏ÁøíÂ∑≤ÊàêÁÇ∫Âçî‰ΩúÂ≠∏ÁøíÁöÑÂÖ∏ÁØÑÔºå
ÁÑ°ÈúÄÈõÜ‰∏≠ÊïèÊÑüË≥áÊñôÂç≥ÂèØÈñãÁôºÁ©©ÂÅ•Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊ®°Âûã„ÄÅÂèÉÊï∏
ÊàñÊõ¥Êñ∞ÁöÑÂÖ¨ÈñãÔºåÂÇ≥Áµ±ÁöÑËÅØÈÇ¶Â≠∏ÁøíÊäÄË°ìÂÖ∑ÊúâÈö±ÁßÅÂíåÂÆâÂÖ®ÊºèÊ¥ûÔºåÂèØÁî®‰ΩúÊîªÊìäÈù¢„ÄÇÊú¨ÊñáÊèêÂá∫
ËÅØÈÇ¶Áü•Ë≠òÂÜçÂà©Áî® (FedKR)Ôºå‰∏ÄÁ®ÆË∑®Â≠§Â≥∂ÁöÑËÅØÈÇ¶Â≠∏ÁøíÊñπÊ≥ï
‰ΩøÁî®Êú¨Âú∞ÁîüÊàêÁöÑÂêàÊàêË≥áÊñô‰æÜ‰øÉÈÄ≤
Ê©üÊßã‰πãÈñìÁöÑÂêà‰Ωú„ÄÇFedKR Â∞áÂÖàÈÄ≤ÁöÑË≥áÊñôÁîüÊàêÊäÄË°ìËàáÂãïÊÖã
ËÅöÂêàÈÅéÁ®ãÁõ∏ÁµêÂêàÔºå‰ª•Êèê‰æõÊØî
ÁèæÊúâÊñπÊ≥ïÊõ¥ËÉΩÊäµÁ¶¶Èö±ÁßÅÊîªÊìäÁöÑÂÆâÂÖ®‰øùÈöúÔºåÂ§ßÂπÖÁ∏ÆÂ∞èÊîªÊìäÈù¢„ÄÇÂØ¶È©ó
ÁµêÊûúÈ°ØÁ§∫ÔºåÂú®‰∏ÄËà¨ÂíåÈÜ´ÁôÇË≥áÊñôÈõÜ‰∏äÔºåFedKR ÈÅîÂà∞Á´∂Áà≠Âäõ
Ë°®ÁèæÔºåËàáË®ìÁ∑¥Ê®°ÂûãÁõ∏ÊØîÔºåÊ∫ñÁ¢∫ÁéáÂπ≥ÂùáÊèêÂçá 4.24%
‰æÜËá™Êú¨Âú∞Ë≥áÊñôÔºåÂú®Ë≥áÊñôÁ®ÄÁº∫ÁöÑÊÉÖÊ≥Å‰∏ãÂ±ïÁèæÂá∫ÁâπÂà•ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization**
2407.20739v1 by Michael K√∂lle, Karola Schneider, Sabrina Egger, Felix Topp, Thomy Phan, Philipp Altmann, Jonas N√º√ülein, Claudia Linnhoff-Popien

In recent years, Multi-Agent Reinforcement Learning (MARL) has found
application in numerous areas of science and industry, such as autonomous
driving, telecommunications, and global health. Nevertheless, MARL suffers
from, for instance, an exponential growth of dimensions. Inherent properties of
quantum mechanics help to overcome these limitations, e.g., by significantly
reducing the number of trainable parameters. Previous studies have developed an
approach that uses gradient-free quantum Reinforcement Learning and
evolutionary optimization for variational quantum circuits (VQCs) to reduce the
trainable parameters and avoid barren plateaus as well as vanishing gradients.
This leads to a significantly better performance of VQCs compared to classical
neural networks with a similar number of trainable parameters and a reduction
in the number of parameters by more than 97 \% compared to similarly good
neural networks. We extend an approach of K\"olle et al. by proposing a
Gate-Based, a Layer-Based, and a Prototype-Based concept to mutate and
recombine VQCs. Our results show the best performance for mutation-only
strategies and the Gate-Based approach. In particular, we observe a
significantly better score, higher total and own collected coins, as well as a
superior own coin rate for the best agent when evaluated in the Coin Game
environment.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÂ§öÊô∫ËÉΩÈ´îÂº∑ÂåñÂ≠∏Áøí (MARL) Â∑≤Âú®ÁßëÂ≠∏ÂíåÁî¢Ê•≠ÁöÑË®±Â§öÈ†òÂüü‰∏≠ÊâæÂà∞ÊáâÁî®Ôºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõ„ÄÅÈõª‰ø°ÂíåÂÖ®ÁêÉÂÅ•Â∫∑„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåMARL ÈÇÑÊòØÊúÉÂèóÂà∞‰æãÂ¶ÇÁ∂≠Â∫¶ÊåáÊï∏ÊàêÈï∑Á≠âÂïèÈ°åÁöÑÂΩ±Èüø„ÄÇÈáèÂ≠êÂäõÂ≠∏ÁöÑÂÖßÂú®ÁâπÊÄßÊúâÂä©ÊñºÂÖãÊúçÈÄô‰∫õÈôêÂà∂Ôºå‰æãÂ¶ÇÔºåÈÄèÈÅéÂ§ßÂπÖÊ∏õÂ∞ëÂèØË®ìÁ∑¥ÂèÉÊï∏ÁöÑÊï∏Èáè„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤ÈñãÁôºÂá∫‰∏ÄÁ®ÆÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®ÁÑ°Ê¢ØÂ∫¶ÁöÑÈáèÂ≠êÂº∑ÂåñÂ≠∏ÁøíÂíåËÆäÂàÜÈáèÂ≠êÈõªË∑Ø (VQC) ÁöÑÊºîÂåñÊúÄ‰Ω≥ÂåñÔºå‰ª•Ê∏õÂ∞ëÂèØË®ìÁ∑¥ÂèÉÊï∏‰∏¶ÈÅøÂÖçË≤ßÁò†È´òÂéüÂíåÊ¢ØÂ∫¶Ê∂àÂ§±„ÄÇËàáÂÖ∑ÊúâÈ°û‰ººÂèØË®ìÁ∑¥ÂèÉÊï∏Êï∏ÈáèÁöÑÂÇ≥Áµ±Á•ûÁ∂ìÁ∂≤Ë∑ØÁõ∏ÊØîÔºåÈÄôÊúÉËÆì VQC ÁöÑÊïàËÉΩÈ°ØËëóÊèêÂçáÔºåËÄå‰∏îËàáÂêåÊ®£ÂÑ™ËâØÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÁõ∏ÊØîÔºåÂèÉÊï∏Êï∏ÈáèÊ∏õÂ∞ë‰∫ÜË∂ÖÈÅé 97%„ÄÇÊàëÂÄëÊì¥ÂÖÖ‰∫Ü K\"olle Á≠â‰∫∫ÁöÑÊñπÊ≥ïÔºåÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÈñò„ÄÅÂü∫ÊñºÂ±§ÂíåÂü∫ÊñºÂéüÂûãÁöÑÊ¶ÇÂøµ‰æÜËÆäÁï∞ÂíåÈáçÁµÑ VQC„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂÉÖËÆäÁï∞Á≠ñÁï•ÂíåÂü∫ÊñºÈñòÁöÑÊñπÊ≥ïÂÖ∑ÊúâÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëËßÄÂØüÂà∞Âú® Coin Game Áí∞Â¢É‰∏≠ÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåÊúÄ‰Ω≥Êô∫ËÉΩÈ´îÁöÑÂæóÂàÜÈ°ØËëóÊèêÂçá„ÄÅÁ∏ΩË®àÂíåËá™Â∑±Êî∂ÈõÜÁöÑÈáëÂπ£Êï∏ÈáèËºÉÈ´òÔºå‰ª•ÂèäËá™Â∑±ÁöÑÈáëÂπ£ÊØîÁéáËºÉÈ´ò„ÄÇ

##### **Dense Self-Supervised Learning for Medical Image Segmentation**
2407.20395v1 by Maxime Seince, Loic Le Folgoc, Luiz Augusto Facury de Souza, Elsa Angelini

Deep learning has revolutionized medical image segmentation, but it relies
heavily on high-quality annotations. The time, cost and expertise required to
label images at the pixel-level for each new task has slowed down widespread
adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL)
approach for few-shot segmentation, that reduces the manual annotation burden
by learning powerful pixel-level representations directly from unlabeled
images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for
contrastive SSL on whole images. It is applied to generic encoder-decoder deep
learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance
of the learned image-level representations under intensity and spatial image
augmentations, Pix2Rep enforces equivariance of the pixel-level
representations. We demonstrate the framework on a task of cardiac MRI
segmentation. Results show improved performance compared to existing semi- and
self-supervised approaches; and a 5-fold reduction in the annotation burden for
equivalent performance versus a fully supervised U-Net baseline. This includes
a 30% (resp. 31%) DICE improvement for one-shot segmentation under
linear-probing (resp. fine-tuning). Finally, we also integrate the novel
Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even
better segmentation performance.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÂæπÂ∫ïÊîπËÆä‰∫ÜÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ôºå‰ΩÜÂÆÉÊ•µÂ∫¶‰æùË≥¥ÊñºÈ´òÂìÅË≥™ÁöÑË®ªËß£„ÄÇÁÇ∫ÊØèÂÄãÊñ∞‰ªªÂãôÊ®ôË®òÂÉèÁ¥†Â±§Á¥öÁöÑÂΩ±ÂÉèÊâÄÈúÄÁöÑÊôÇÈñì„ÄÅÊàêÊú¨ÂíåÂ∞àÊ•≠Áü•Ë≠òÔºåÂ∑≤Ê∏õÁ∑©‰∫ÜÁØÑ‰æãÁöÑÂª£Ê≥õÊé°Áî®„ÄÇÊàëÂÄëÊèêÂá∫ Pix2RepÔºå‰∏ÄÁ®ÆÈáùÂ∞çÂ∞ëÊ¨°ÂàÜÂâ≤ÁöÑËá™Áõ£Áù£ÂºèÂ≠∏Áøí (SSL) ÊñπÊ≥ïÔºåÂèØÈÄèÈÅéÁõ¥Êé•ÂæûÊú™Ê®ôË®òÁöÑÂΩ±ÂÉè‰∏≠Â≠∏ÁøíÂº∑Â§ßÁöÑÂÉèÁ¥†Â±§Á¥öË°®Á§∫Ôºå‰æÜÊ∏õËºïÊâãÂãïË®ªËß£Ë≤†Êìî„ÄÇPix2Rep ÊòØ‰∏ÄÁ®ÆÈáùÂ∞çÂÆåÊï¥ÂΩ±ÂÉèÂ∞çÊØîÂºè SSL ÁöÑÊñ∞Á©éÂÉèÁ¥†Â±§Á¥öÊêçÂ§±ÂíåÈ†êË®ìÁ∑¥ÁØÑ‰æã„ÄÇÂÆÉË¢´ÊáâÁî®ÊñºÈÄöÁî®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ê∑±Â∫¶Â≠∏Áøí‰∏ªÂππ (‰æãÂ¶Ç U-Net)„ÄÇÂ§ßÂ§öÊï∏ SSL ÊñπÊ≥ïÂº∑Âà∂Â≠∏ÁøíÁöÑÂΩ±ÂÉèÂ±§Á¥öË°®Á§∫Âú®Âº∑Â∫¶ÂíåÁ©∫ÈñìÂΩ±ÂÉèÊì¥ÂÖÖ‰∏ãÂÖ∑Êúâ‰∏çËÆäÊÄßÔºåËÄå Pix2Rep ÂâáÂº∑Âà∂ÂÉèÁ¥†Â±§Á¥öË°®Á§∫ÂÖ∑ÊúâÁ≠âËÆäÊÄß„ÄÇÊàëÂÄëÂú®ÂøÉËáü MRI ÂàÜÂâ≤‰ªªÂãô‰∏≠Â±ïÁ§∫‰∫ÜÈÄôÂÄãÊû∂Êßã„ÄÇÁµêÊûúÈ°ØÁ§∫ËàáÁèæÊúâÁöÑÂçäÁõ£Áù£ÂºèÂíåËá™Áõ£Áù£ÂºèÊñπÊ≥ïÁõ∏ÊØîÔºåÊïàËÉΩÊúâÊâÄÊèêÂçáÔºõ‰∏îÂú®ËàáÂÆåÂÖ®Áõ£Áù£Âºè U-Net Âü∫Ê∫ñÂÖ∑ÊúâÁõ∏ÂêåÊïàËÉΩÁöÑÊÉÖÊ≥Å‰∏ãÔºåË®ªËß£Ë≤†ÊìîÊ∏õÂ∞ë‰∫Ü 5 ÂÄç„ÄÇÈÄôÂåÖÊã¨Âú®Á∑öÊÄßÊé¢Ê∏¨ (resp. ÂæÆË™ø) ‰∏ãÔºåÂñÆÊ¨°ÂàÜÂâ≤ÁöÑ DICE ÊèêÂçá‰∫Ü 30% (resp. 31%)„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰πüÂ∞áÊñ∞Á©éÁöÑ Pix2Rep Ê¶ÇÂøµËàá Barlow Twins ÈùûÂ∞çÊØîÂºè SSL Êï¥ÂêàÔºåÈÄôÂ∞éËá¥‰∫ÜÊõ¥Â•ΩÁöÑÂàÜÂâ≤ÊïàËÉΩ„ÄÇ

##### **Classification, Regression and Segmentation directly from k-Space in Cardiac MRI**
2407.20108v1 by Ruochen Li, Jiazhen Pan, Youxiang Zhu, Juncheng Ni, Daniel Rueckert

Cardiac Magnetic Resonance Imaging (CMR) is the gold standard for diagnosing
cardiovascular diseases. Clinical diagnoses predominantly rely on
magnitude-only Digital Imaging and Communications in Medicine (DICOM) images,
omitting crucial phase information that might provide additional diagnostic
benefits. In contrast, k-space is complex-valued and encompasses both magnitude
and phase information, while humans cannot directly perceive. In this work, we
propose KMAE, a Transformer-based model specifically designed to process
k-space data directly, eliminating conventional intermediary conversion steps
to the image domain. KMAE can handle critical cardiac disease classification,
relevant phenotype regression, and cardiac morphology segmentation tasks. We
utilize this model to investigate the potential of k-space-based diagnosis in
cardiac MRI. Notably, this model achieves competitive classification and
regression performance compared to image-domain methods e.g. Masked
Autoencoders (MAEs) and delivers satisfactory segmentation performance with a
myocardium dice score of 0.884. Last but not least, our model exhibits robust
performance with consistent results even when the k-space is 8* undersampled.
We encourage the MR community to explore the untapped potential of k-space and
pursue end-to-end, automated diagnosis with reduced human intervention.

ÊëòË¶ÅÔºöÂøÉËáüÁ£ÅÊåØÈÄ†ÂΩ± (CMR) ÊòØË®∫Êñ∑ÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÈªÉÈáëÊ®ôÊ∫ñ„ÄÇËá®Â∫äË®∫Êñ∑‰∏ªË¶Å‰æùË≥¥ÊñºÈÜ´Â≠∏Êï∏‰ΩçÂΩ±ÂÉèÂíåÈÄöË®ä (DICOM) ÂΩ±ÂÉèÁöÑÂπÖÂ∫¶ÔºåËÄåÂøΩÁï•‰∫ÜÂèØËÉΩÊèê‰æõÈ°çÂ§ñË®∫Êñ∑Â•ΩËôïÁöÑÈóúÈçµÁõ∏‰ΩçË≥áË®ä„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåk Á©∫ÈñìÊòØË§áÊï∏ÂÄº‰∏îÂåÖÂê´ÂπÖÂ∫¶ÂíåÁõ∏‰ΩçË≥áË®äÔºå‰ΩÜ‰∫∫È°ûÁÑ°Ê≥ïÁõ¥Êé•ÊÑüÁü•„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ KMAEÔºå‰∏ÄÁ®ÆÁâπÂà•Ë®≠Ë®àÁî®ÊñºÁõ¥Êé•ËôïÁêÜ k Á©∫ÈñìË≥áÊñôÁöÑ Transformer Âü∫Á§éÊ®°ÂûãÔºåÊ∂àÈô§‰∫ÜËΩâÊèõÂà∞ÂΩ±ÂÉèÈ†òÂüüÁöÑÂÇ≥Áµ±‰∏≠‰ªãÊ≠•È©ü„ÄÇKMAE ÂèØ‰ª•ËôïÁêÜÈóúÈçµÁöÑÂøÉËáüÁñæÁóÖÂàÜÈ°û„ÄÅÁõ∏ÈóúË°®ÂûãÂõûÊ≠∏ÂíåÂøÉËáüÂΩ¢ÊÖãÂàÜÂâ≤‰ªªÂãô„ÄÇÊàëÂÄëÂà©Áî®Ê≠§Ê®°ÂûãÊé¢Ë®é k Á©∫ÈñìÂü∫Á§éË®∫Êñ∑Âú®ÂøÉËáü MRI ‰∏≠ÁöÑÊΩõÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåËàáÂΩ±ÂÉèÈ†òÂüüÊñπÊ≥ïÔºà‰æãÂ¶ÇÈÅÆÁΩ©ÂºèËá™ÂãïÁ∑®Á¢ºÂô® (MAE)ÔºâÁõ∏ÊØîÔºåÊ≠§Ê®°ÂûãÈÅîÂà∞‰∫ÜÁ´∂Áà≠ÊÄßÁöÑÂàÜÈ°ûÂíåÂõûÊ≠∏ÊïàËÉΩÔºå‰∏¶‰ª• 0.884 ÁöÑÂøÉËÇåÈ™∞Â≠êÂàÜÊï∏Êèê‰æõ‰∫Ü‰ª§‰∫∫ÊªøÊÑèÁöÑÂàÜÂâ≤ÊïàËÉΩ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑ‰∏ÄÈªûÊòØÔºåÂç≥‰ΩøÂú® k Á©∫Èñì‰∏çË∂≥Êé°Ê®£ 8* ÊôÇÔºåÊàëÂÄëÁöÑÊ®°Âûã‰πüËÉΩÂ±ïÁèæÁ©©ÂÅ•ÁöÑÊïàËÉΩÂíå‰∏ÄËá¥ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÈºìÂãµÊ†∏Á£ÅÂÖ±ÊåØÁ§æÁæ§Êé¢Á¥¢ k Á©∫ÈñìÁöÑÊú™ÈñãÁôºÊΩõÂäõÔºå‰∏¶ËøΩÊ±ÇÊ∏õÂ∞ë‰∫∫ÁÇ∫Âπ≤È†êÁöÑÁ´ØÂà∞Á´ØËá™ÂãïÂåñË®∫Êñ∑„ÄÇ

##### **Robust Conformal Volume Estimation in 3D Medical Images**
2407.19938v1 by Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat

Volumetry is one of the principal downstream applications of 3D medical image
segmentation, for example, to detect abnormal tissue growth or for surgery
planning. Conformal Prediction is a promising framework for uncertainty
quantification, providing calibrated predictive intervals associated with
automatic volume measurements. However, this methodology is based on the
hypothesis that calibration and test samples are exchangeable, an assumption
that is in practice often violated in medical image applications. A weighted
formulation of Conformal Prediction can be framed to mitigate this issue, but
its empirical investigation in the medical domain is still lacking. A potential
reason is that it relies on the estimation of the density ratio between the
calibration and test distributions, which is likely to be intractable in
scenarios involving high-dimensional data. To circumvent this, we propose an
efficient approach for density ratio estimation relying on the compressed
latent representations generated by the segmentation model. Our experiments
demonstrate the efficiency of our approach to reduce the coverage error in the
presence of covariate shifts, in both synthetic and real-world settings. Our
implementation is available at https://github.com/benolmbrt/wcp_miccai

ÊëòË¶ÅÔºöÈ´îÁ©çÊ∏¨ÈáèÊòØ 3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÁöÑ‰∏ªË¶Å‰∏ãÊ∏∏ÊáâÁî®‰πã‰∏ÄÔºå‰æãÂ¶ÇÁî®ÊñºÂÅµÊ∏¨Áï∞Â∏∏ÁµÑÁπîÁîüÈï∑ÊàñÊâãË°ìË¶èÂäÉ„ÄÇÂÖ±ÂΩ¢È†êÊ∏¨ÊòØ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÊû∂ÊßãÔºåÊèê‰æõËàáËá™ÂãïÈ´îÁ©çÈáèÊ∏¨Áõ∏ÈóúÁöÑÊ†°Ê≠£È†êÊ∏¨ÂçÄÈñì„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÊñπÊ≥ïÂü∫ÊñºÊ†°Ê≠£ÂíåÊ∏¨Ë©¶Ê®£Êú¨ÂèØ‰∫§ÊèõÁöÑÂÅáË®≠ÔºåËÄåÊ≠§ÂÅáË®≠Âú®ÂØ¶Âãô‰∏äÁ∂ìÂ∏∏Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®‰∏≠ÈÅ≠Âà∞Á†¥Â£û„ÄÇÂÖ±ÂΩ¢È†êÊ∏¨ÁöÑÂä†Ê¨äÂÖ¨ÂºèÂèØ‰ª•Ë¢´Âª∫Êßã‰æÜÊ∏õËºïÊ≠§ÂïèÈ°åÔºå‰ΩÜÂÖ∂Âú®ÈÜ´Â≠∏È†òÂüüÁöÑÁ∂ìÈ©óË™øÊü•‰ªçÁÑ∂‰∏çË∂≥„ÄÇ‰∏ÄÂÄãÊΩõÂú®ÂéüÂõ†ÊòØÂÆÉ‰æùË≥¥ÊñºÊ†°Ê≠£ÂíåÊ∏¨Ë©¶ÂàÜ‰Ωà‰πãÈñìÁöÑÂØÜÂ∫¶ÊØî‰º∞Ë®àÔºåÈÄôÂú®Ê∂âÂèäÈ´òÁ∂≠Â∫¶Ë≥áÊñôÁöÑÂ†¥ÊôØ‰∏≠ÂèØËÉΩÊòØÊ£òÊâãÁöÑ„ÄÇÁÇ∫‰∫ÜËø¥ÈÅøÊ≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊúâÊïàÁéáÁöÑÂØÜÂ∫¶ÊØî‰º∞Ë®àÊñπÊ≥ïÔºå‰æùË≥¥ÊñºÂàÜÂâ≤Ê®°ÂûãÁî¢ÁîüÁöÑÂ£ìÁ∏ÆÊΩõÂú®Ë°®Á§∫„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåË®≠ÂÆö‰∏≠Ê∏õÂ∞ëÂÖ±ËÆäÁï∞Êï∏ÂÅèÁßªÂ≠òÂú®ÊôÇÁöÑË¶ÜËìãÁéáË™§Â∑ÆÁöÑÊïàÁéá„ÄÇÊàëÂÄëÁöÑÂØ¶‰ΩúÂèØ‰ª•Âú® https://github.com/benolmbrt/wcp_miccai ÂèñÂæó

##### **Yucca: A Deep Learning Framework For Medical Image Analysis**
2407.19888v1 by Sebastian N√∏rgaard Llambias, Julia Machnio, Asbj√∏rn Munk, Jakob Ambsdorf, Mads Nielsen, Mostafa Mehdipour Ghazi

Medical image analysis using deep learning frameworks has advanced healthcare
by automating complex tasks, but many existing frameworks lack flexibility,
modularity, and user-friendliness. To address these challenges, we introduce
Yucca, an open-source AI framework available at
https://github.com/Sllambias/yucca, designed specifically for medical imaging
applications and built on PyTorch and PyTorch Lightning. Yucca features a
three-tiered architecture: Functional, Modules, and Pipeline, providing a
comprehensive and customizable solution. Evaluated across diverse tasks such as
cerebral microbleeds detection, white matter hyperintensity segmentation, and
hippocampus segmentation, Yucca achieves state-of-the-art results,
demonstrating its robustness and versatility. Yucca offers a powerful,
flexible, and user-friendly platform for medical image analysis, inviting
community contributions to advance its capabilities and impact.

ÊëòË¶ÅÔºö‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂ÈÄ≤Ë°åÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÂ∑≤Á∂ìÈÄöÈÅéËá™ÂãïÂåñË§áÈõú‰ªªÂãôÊé®Âãï‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÈÄ≤Ê≠•Ôºå‰ΩÜË®±Â§öÁèæÊúâÊ°ÜÊû∂Áº∫‰πèÈùàÊ¥ªÊÄß„ÄÅÊ®°ÁµÑÂåñÂíå‰ΩøÁî®ËÄÖÂèãÂñÑÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü YuccaÔºå‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºÁöÑ AI Ê°ÜÊû∂ÔºåÂèØÊñº https://github.com/Sllambias/yucca ÂèñÂæóÔºåÂ∞àÈñÄÁÇ∫ÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®Ë®≠Ë®àÔºå‰∏¶Âª∫Á´ãÂú® PyTorch Âíå PyTorch Lightning ‰πã‰∏ä„ÄÇYucca ÂÖ∑Êúâ‰∏âÂ±§Êû∂ÊßãÔºöÂäüËÉΩ„ÄÅÊ®°ÁµÑÂíåÁÆ°Á∑öÔºåÊèê‰æõÂÖ®Èù¢‰∏îÂèØËá™Ë®ÇÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÈÄ≤Ë°åË©ï‰º∞Ôºå‰æãÂ¶ÇËÖ¶ÂæÆÂá∫Ë°ÄÂÅµÊ∏¨„ÄÅÁôΩË≥™È´òË®äËôüÂàÜÂâ≤ÂíåÊµ∑È¶¨ÂàÜÂâ≤ÔºåYucca ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûúÔºåË≠âÊòé‰∫ÜÂÆÉÁöÑÁ©©ÂÅ•ÊÄßÂíåÂ§öÂäüËÉΩÊÄß„ÄÇYucca Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ß„ÄÅÈùàÊ¥ª‰∏î‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÂπ≥Âè∞ÔºåÊ≠°ËøéÁ§æÁæ§Ë≤¢Áçª‰ª•ÊèêÂçáÂÖ∂ËÉΩÂäõÂíåÂΩ±ÈüøÂäõ„ÄÇ

##### **CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare**
2407.19705v2 by Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny

The rapid progress in Large Language Models (LLMs) has prompted the creation
of numerous benchmarks to evaluate their capabilities.This study focuses on the
Comprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset
diversity and distribution in supervised fine-tuning (SFT) may enhance LLM
performance.Remarkably, We successfully trained a smaller base model to achieve
scores comparable to larger models, indicating that a diverse and
well-distributed dataset can optimize performance regardless of model size.This
study suggests that even smaller models may reach high performance levels with
carefully curated and varied datasets. By integrating a wide range of
instructional content, our approach addresses potential issues such as data
quality inconsistencies. Our results imply that a broader spectrum of training
data may enhance a model's ability to generalize and perform effectively across
different medical scenarios, highlighting the importance of dataset quality and
diversity in fine-tuning processes. We open-source the model for future
research at https://github.com/CAS-SIAT-XinHai/CollectiveSFT

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÈÄ≤Â±ï‰øÉÊàê‰∫ÜË®±Â§öÂü∫Ê∫ñÁöÑÂª∫Á´ãÔºå‰ª•Ë©ï‰º∞ÂÆÉÂÄëÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®Êñº‰∏≠ÊñáÁ∂úÂêàÈÜ´ÁôÇÂü∫Ê∫ñ (CMB)ÔºåÂ±ïÁ§∫‰∫ÜÁõ£Áù£ÂæÆË™ø (SFT) ‰∏≠ÁöÑË≥áÊñôÈõÜÂ§öÊ®£ÊÄßÂíåÂàÜ‰ΩàÂ¶Ç‰ΩïÂ¢ûÂº∑ LLM ÊïàËÉΩ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÊàêÂäüÂú∞Ë®ìÁ∑¥‰∫Ü‰∏ÄÂÄãËºÉÂ∞èÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰ª•ÈÅîÂà∞ËàáËºÉÂ§ßÂûãÊ®°ÂûãÁõ∏Áï∂ÁöÑÂàÜÊï∏ÔºåÈÄôË°®Êòé‰∏ÄÂÄãÂ§öÊ®£Âåñ‰∏îÂàÜ‰ΩàËâØÂ•ΩÁöÑË≥áÊñôÈõÜÂèØ‰ª•ÊúÄ‰Ω≥ÂåñÊïàËÉΩÔºåËÄåËàáÊ®°ÂûãÂ§ßÂ∞èÁÑ°Èóú„ÄÇÊú¨Á†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØËºÉÂ∞èÁöÑÊ®°ÂûãÔºåÂè™Ë¶Å‰ΩøÁî®Á∂ìÈÅé‰ªîÁ¥∞Á≠ñÂäÉ‰∏îÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜÔºå‰πüËÉΩÈÅîÂà∞È´òÊ∞¥Ê∫ñÁöÑÊïàËÉΩ„ÄÇÈÄèÈÅéÊï¥ÂêàÂª£Ê≥õÁöÑÊïôÂ≠∏ÂÖßÂÆπÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïËß£Ê±∫‰∫ÜË≥áÊñôÂìÅË≥™‰∏ç‰∏ÄËá¥Á≠âÊΩõÂú®ÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÊõ¥Âª£Ê≥õÁöÑË®ìÁ∑¥Ë≥áÊñôÁØÑÂúçÂèØËÉΩÊúÉÂ¢ûÂº∑Ê®°ÂûãÂú®‰∏çÂêåÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠Ê¶ÇÊã¨ÂíåÊúâÊïàÂü∑Ë°åÁöÑËÉΩÂäõÔºåÁ™ÅÈ°Ø‰∫ÜË≥áÊñôÈõÜÂìÅË≥™ÂíåÂ§öÊ®£ÊÄßÂú®ÂæÆË™øÈÅéÁ®ã‰∏≠ÊâÆÊºîÁöÑÈáçË¶ÅËßíËâ≤„ÄÇÊàëÂÄëÂú® https://github.com/CAS-SIAT-XinHai/CollectiveSFT ÈñãÊ∫êÊ≠§Ê®°Âûã‰ª•‰æõÂ∞á‰æÜÁ†îÁ©∂„ÄÇ

##### **Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks**
2407.21072v1 by Marco AF Pimentel, Cl√©ment Christophe, Tathagata Raha, Prateek Munjal, Praveen K Kanithi, Shadab Khan

As large language models (LLMs) continue to evolve, the need for robust and
standardized evaluation benchmarks becomes paramount. Evaluating the
performance of these models is a complex challenge that requires careful
consideration of various linguistic tasks, model architectures, and
benchmarking methodologies. In recent years, various frameworks have emerged as
noteworthy contributions to the field, offering comprehensive evaluation tests
and benchmarks for assessing the capabilities of LLMs across diverse domains.
This paper provides an exploration and critical analysis of some of these
evaluation methodologies, shedding light on their strengths, limitations, and
impact on advancing the state-of-the-art in natural language processing.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊåÅÁ∫åÊºîÈÄ≤ÔºåÂ∞çÊñºÂÅ•ÂÖ®‰∏îÊ®ôÊ∫ñÂåñÁöÑË©ï‰º∞Âü∫Ê∫ñÁöÑÈúÄÊ±ÇËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇË©ï‰º∞ÈÄô‰∫õÊ®°ÂûãÁöÑÊïàËÉΩÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑÊåëÊà∞ÔºåÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáèÂêÑÁ®ÆË™ûË®Ä‰ªªÂãô„ÄÅÊ®°ÂûãÊû∂ÊßãÂíåÂü∫Ê∫ñÊñπÊ≥ï„ÄÇËøëÂπ¥‰æÜÔºåÂêÑÁ®ÆÊû∂ÊßãÂ∑≤ÊàêÁÇ∫Ë©≤È†òÂüüÁöÑÈ°ØËëóË≤¢ÁçªÔºåÊèê‰æõÂÖ®Èù¢ÁöÑË©ï‰º∞Ê∏¨Ë©¶ÂíåÂü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞ LLM Âú®‰∏çÂêåÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∏¶ÊâπÂà§ÊÄßÂú∞ÂàÜÊûêÂÖ∂‰∏≠‰∏Ä‰∫õË©ï‰º∞ÊñπÊ≥ïÔºåÈó°ÊòéÂÖ∂ÂÑ™Èªû„ÄÅÈôêÂà∂ÂíåÂ∞çËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÈ†òÂüüÈÄ≤Ê≠•ÁöÑÂΩ±Èüø„ÄÇ

##### **Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity**
2407.19668v1 by Minxiao Chen, Haitao Yuan, Nan Jiang, Zhifeng Bao, Shangguang Wang

Traffic accidents pose a significant risk to human health and property
safety. Therefore, to prevent traffic accidents, predicting their risks has
garnered growing interest. We argue that a desired prediction solution should
demonstrate resilience to the complexity of traffic accidents. In particular,
it should adequately consider the regional background, accurately capture both
spatial proximity and semantic similarity, and effectively address the sparsity
of traffic accidents. However, these factors are often overlooked or difficult
to incorporate. In this paper, we propose a novel multi-granularity
hierarchical spatio-temporal network. Initially, we innovate by incorporating
remote sensing data, facilitating the creation of hierarchical
multi-granularity structure and the comprehension of regional background. We
construct multiple high-level risk prediction tasks to enhance model's ability
to cope with sparsity. Subsequently, to capture both spatial proximity and
semantic similarity, region feature and multi-view graph undergo encoding
processes to distill effective representations. Additionally, we propose
message passing and adaptive temporal attention module that bridges different
granularities and dynamically captures time correlations inherent in traffic
accident patterns. At last, a multivariate hierarchical loss function is
devised considering the complexity of the prediction purpose. Extensive
experiments on two real datasets verify the superiority of our model against
the state-of-the-art methods.

ÊëòË¶ÅÔºö‰∫§ÈÄö‰∫ãÊïÖÂ∞ç‰∫∫È°ûÂÅ•Â∫∑ÂíåË≤°Áî¢ÂÆâÂÖ®ÊßãÊàêÈáçÂ§ßÈ¢®Èö™„ÄÇÂõ†Ê≠§ÔºåÈ†êÊ∏¨‰∫§ÈÄö‰∫ãÊïÖÈ¢®Èö™Â∑≤ÂºïËµ∑Ë∂ä‰æÜË∂äÂ§ßÁöÑËààË∂£„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÁêÜÊÉ≥ÁöÑÈ†êÊ∏¨Ëß£Ê±∫ÊñπÊ°àÊáâÂ±ïÁèæÂá∫Â∞ç‰∫§ÈÄö‰∫ãÊïÖË§áÈõúÊÄßÁöÑÈüåÊÄß„ÄÇÂÖ∑È´îËÄåË®ÄÔºåÂÆÉÊáâÂÖÖÂàÜËÄÉÊÖÆÂçÄÂüüËÉåÊôØÔºåÊ∫ñÁ¢∫ÊçïÊçâÁ©∫ÈñìÊé•ËøëÂ∫¶ÂíåË™ûÁæ©Áõ∏‰ººÊÄßÔºå‰∏¶ÊúâÊïàËß£Ê±∫‰∫§ÈÄö‰∫ãÊïÖÁöÑÁ®ÄÁñèÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂõ†Á¥†ÈÄöÂ∏∏Ë¢´ÂøΩË¶ñÊàñÈõ£‰ª•Á¥çÂÖ•„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÁ≤íÂ∫¶ÂàÜÂ±§ÊôÇÁ©∫Á∂≤Ë∑Ø„ÄÇÊúÄÂàùÔºåÊàëÂÄëÂâµÊñ∞Âú∞Á¥çÂÖ•‰∫ÜÈÅôÊÑüÊï∏ÊìöÔºå‰øÉËøõ‰∫ÜÂàÜÂ±§Â§öÁ≤íÂ∫¶ÁµêÊßãÁöÑÂâµÂª∫ÂíåÂçÄÂüüËÉåÊôØÁöÑÁêÜËß£„ÄÇÊàëÂÄëÊßãÂª∫‰∫ÜÂ§öÂÄãÈ´òÁ¥öÈ¢®Èö™È†êÊ∏¨‰ªªÂãôÔºå‰ª•Â¢ûÂº∑Ê®°ÂûãÊáâÂ∞çÁ®ÄÁñèÊÄßÁöÑËÉΩÂäõ„ÄÇÈö®ÂæåÔºåÁÇ∫‰∫ÜÊçïÊçâÁ©∫ÈñìÊé•ËøëÂ∫¶ÂíåË™ûÁæ©Áõ∏‰ººÊÄßÔºåÂçÄÂüüÁâπÂæµÂíåÂ§öË¶ñÂúñÂúñË°®Á∂ìÈÅéÁ∑®Á¢ºÈÅéÁ®ãÔºå‰ª•ÊèêÂèñÊúâÊïàÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊ∂àÊÅØÂÇ≥ÈÅûÂíåËá™ÈÅ©ÊáâÊôÇÈñìÊ≥®ÊÑèÂäõÊ®°ÁµÑÔºåÂÆÉÊû∂Ëµ∑‰∫Ü‰∏çÂêåÁ≤íÂ∫¶‰πãÈñìÁöÑÊ©ãÊ®ëÔºå‰∏¶ÂãïÊÖãÊçïÊçâ‰∫§ÈÄö‰∫ãÊïÖÊ®°Âºè‰∏≠Âõ∫ÊúâÁöÑÊôÇÈñìÁõ∏ÈóúÊÄß„ÄÇÊúÄÂæåÔºåËÄÉÊÖÆÂà∞È†êÊ∏¨ÁõÆÁöÑÁöÑË§áÈõúÊÄßÔºåË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂ§öËÆäÈáèÂàÜÂ±§ÊêçÂ§±ÂáΩÊï∏„ÄÇÂú®ÂÖ©ÂÄãÁúüÂØ¶Êï∏ÊìöÈõÜ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊ®°ÂûãÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄß„ÄÇ

##### **Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing**
2407.19540v1 by Heejoon Koo

In this paper, we present NECHO v2, a novel framework designed to enhance the
predictive accuracy of multimodal sequential patient diagnoses under uncertain
missing visit sequences, a common challenge in clinical settings. Firstly, we
modify NECHO to handle uncertain modality representation dominance under the
imperfect data. Next, we develop a systematic knowledge distillation by
employing the modified NECHO as both teacher and student. It encompasses a
modality-wise contrastive and hierarchical distillation, transformer
representation random distillation, along with other distillations to align
representations tightly and effectively. We also utilise random erasing on
individual data points within sequences during both training and distillation
of teacher to lightly simulate scenario with missing visit information to
foster effective knowledge transfer. As a result, NECHO v2 verifies itself by
showing superiority in multimodal sequential diagnosis prediction on both
balanced and imbalanced incomplete settings on multimodal healthcare data.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü NECHO v2Ôºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∑Â§öÊ®°ÊÖãÈ†ÜÂ∫èÊÇ£ËÄÖË®∫Êñ∑ÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠Â∏∏Ë¶ãÁöÑÊåëÊà∞ÊòØ‰∏çÁ¢∫ÂÆöÈÅ∫ÊºèÁöÑË®™ÂïèÂ∫èÂàó„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰øÆÊîπ NECHO ‰ª•ËôïÁêÜ‰∏çÂÆåÁæéÊï∏Êìö‰∏ãÁöÑ‰∏çÁ¢∫ÂÆöÊ®°ÊÖãË°®Á§∫ÂÑ™Âã¢„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈÄöÈÅé‰ΩøÁî®‰øÆÊîπÂæåÁöÑ NECHO ‰ΩúÁÇ∫ÊïôÂ∏´ÂíåÂ≠∏Áîü‰æÜÈñãÁôºÁ≥ªÁµ±ÁöÑÁü•Ë≠òÊèêÁÖâ„ÄÇÂÆÉÂåÖÂê´Ê®°ÊÖãÂ∞çÊØîÂíåÂàÜÂ±§ÊèêÁÖâ„ÄÅTransformerË°®Á§∫Èö®Ê©üÊèêÁÖâ‰ª•ÂèäÂÖ∂‰ªñÊèêÁÖâÔºå‰ª•Á∑äÂØÜÊúâÊïàÂú∞Â∞çÈΩäË°®Á§∫„ÄÇÊàëÂÄëÈÇÑÂú®Ë®ìÁ∑¥ÂíåÊïôÂ∏´ÊèêÁÖâÈÅéÁ®ã‰∏≠Â∞çÂ∫èÂàó‰∏≠ÁöÑÂÄãÂà•Êï∏ÊìöÈªû‰ΩøÁî®Èö®Ê©üÊì¶Èô§Ôºå‰ª•ËºïÂæÆÊ®°Êì¨ÈÅ∫ÊºèË®™Âïè‰ø°ÊÅØÁöÑÂ†¥ÊôØÔºå‰ª•‰øÉÈÄ≤ÊúâÊïàÁöÑÁü•Ë≠òÂÇ≥ÈÅû„ÄÇÂõ†Ê≠§ÔºåNECHO v2 ÈÄöÈÅéÂú®Â§öÊ®°ÊÖãÈÜ´ÁôÇ‰øùÂÅ•Êï∏ÊìöÁöÑÂπ≥Ë°°Âíå‰∏çÂπ≥Ë°°‰∏çÂÆåÊï¥Ë®≠ÁΩÆ‰∏äÈ°ØÁ§∫Â§öÊ®°ÊÖãÈ†ÜÂ∫èË®∫Êñ∑È†êÊ∏¨ÁöÑÂÑ™Ë∂äÊÄß‰æÜÈ©óË≠âËá™Ë∫´„ÄÇ

##### **Nudging Consent and the New Opt Out System to the Processing of Health Data in England**
2407.19447v1 by Janos Meszaros, Chih-hsing Ho, Marcelo Corrales Compagnucci

This chapter examines the challenges of the revised opt out system and the
secondary use of health data in England. The analysis of this data could be
very valuable for science and medical treatment as well as for the discovery of
new drugs. For this reason, the UK government established the care.data program
in 2013. The aim of the project was to build a central nationwide database for
research and policy planning. However, the processing of personal data was
planned without proper public engagement. Research has suggested that IT
companies, such as in the Google DeepMind deal case, had access to other kinds
of sensitive data and failed to comply with data protection law. Since May
2018, the government has launched the national data opt out system with the
hope of regaining public trust. Nevertheless, there are no evidence of
significant changes in the ND opt out, compared to the previous opt out system.
Neither in the use of secondary data, nor in the choices that patients can
make. The only notorious difference seems to be in the way that these options
are communicated and framed to the patients. Most importantly, according to the
new ND opt out, the type 1 opt out option, which is the only choice that truly
stops data from being shared outside direct care, will be removed in 2020.
According to the Behavioral Law and Economics literature (Nudge Theory),
default rules, such as the revised opt out system in England, are very
powerful, because people tend to stick to the default choices made readily
available to them. The crucial question analyzed in this chapter is whether it
is desirable for the UK government to stop promoting the type 1 opt outs, and
whether this could be seen as a kind of hard paternalism.

ÊëòË¶ÅÔºö<paragraph>Êú¨Á´†Êé¢Ë®é‰∫ÜËã±Âúã‰øÆÊîπÂæåÁöÑÈÄÄÂá∫Ê©üÂà∂Âíå‰∫åÊ¨°‰ΩøÁî®ÂÅ•Â∫∑Ë≥áÊñôÊâÄÈù¢Ëá®ÁöÑÊåëÊà∞„ÄÇÂàÜÊûêÈÄô‰∫õË≥áÊñôÂ∞çÊñºÁßëÂ≠∏ÂíåÈÜ´ÁôÇÊ≤ªÁôÇ‰ª•ÂèäÁôºÁèæÊñ∞Ëó•Áâ©ËÄåË®ÄÔºåÂèØËÉΩÈùûÂ∏∏ÊúâÂÉπÂÄº„ÄÇÂü∫ÊñºÊ≠§ÂéüÂõ†ÔºåËã±ÂúãÊîøÂ∫úÊñº 2013 Âπ¥Âª∫Á´ã‰∫Ü care.data Ë®àÁï´„ÄÇË©≤Â∞àÊ°àÁöÑÁõÆÊ®ôÊòØÂª∫Á´ã‰∏ÄÂÄãÂÖ®ÂúãÊÄßÁöÑ‰∏≠Â§ÆË≥áÊñôÂ∫´Ôºå‰ª•ÈÄ≤Ë°åÁ†îÁ©∂ÂíåÊîøÁ≠ñË¶èÂäÉ„ÄÇÁÑ∂ËÄåÔºåÂÄã‰∫∫Ë≥áÊñôÁöÑËôïÁêÜÊòØÂú®Ê≤íÊúâÈÅ©Áï∂ÂÖ¨ÁúæÂèÉËàáÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åË¶èÂäÉÁöÑ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰æãÂ¶ÇÂú® Google DeepMind ‰∫§ÊòìÊ°à‰æã‰∏≠ÔºåIT ÂÖ¨Âè∏ÂèØ‰ª•Â≠òÂèñÂÖ∂‰ªñÈ°ûÂûãÁöÑÊïèÊÑüË≥áÊñôÔºå‰∏îÊú™ËÉΩÈÅµÂÆàË≥áÊñô‰øùË≠∑Ê≥ï„ÄÇËá™ 2018 Âπ¥ 5 Êúà‰ª•‰æÜÔºåÊîøÂ∫úÂ∑≤Êé®Âá∫ÂÖ®ÂúãË≥áÊñôÈÄÄÂá∫Ê©üÂà∂ÔºåÂ∏åÊúõËÉΩÈáçÊñ∞Áç≤ÂæóÂÖ¨Áúæ‰ø°‰ªª„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåËàáÂÖàÂâçÁöÑÈÄÄÂá∫Ê©üÂà∂Áõ∏ÊØîÔºå‰∏¶ÁÑ°Ë≠âÊìöÈ°ØÁ§∫ÂÖ®ÂúãË≥áÊñôÈÄÄÂá∫Ê©üÂà∂ÊúâÈ°ØËëóËÆäÂåñ„ÄÇÁÑ°Ë´ñÊòØÂú®‰∫åÊ¨°Ë≥áÊñôÁöÑ‰ΩøÁî®‰∏äÔºåÊàñÊòØÂú®ÊÇ£ËÄÖÂèØ‰ª•ÂÅöÂá∫ÁöÑÈÅ∏Êìá‰∏äÔºåÁöÜÊòØÂ¶ÇÊ≠§„ÄÇÂîØ‰∏ÄÈ°ØËëóÁöÑÂ∑ÆÁï∞‰ºº‰πéÂú®ÊñºÈÄô‰∫õÈÅ∏È†ÖÁöÑÊ∫ùÈÄöÂíåÂÇ≥ÈÅîÊñπÂºè„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊ†πÊìöÊñ∞ÁöÑÂÖ®ÂúãË≥áÊñôÈÄÄÂá∫Ê©üÂà∂ÔºåÈ°ûÂûã 1 ÈÄÄÂá∫ÈÅ∏È†ÖÔºàÈÄôÊòØÂîØ‰∏ÄÁúüÊ≠£ËÉΩÈòªÊ≠¢Ë≥áÊñôÂú®Áõ¥Êé•ÁÖßË≠∑‰πãÂ§ñË¢´ÂàÜ‰∫´ÁöÑÈÅ∏È†ÖÔºâÂ∞áÊñº 2020 Âπ¥Ë¢´ÁßªÈô§„ÄÇÊ†πÊìöË°åÁÇ∫Ê≥ïËàáÁ∂ìÊøüÂ≠∏ÊñáÁçªÔºàÊé®Ë´ñÁêÜË´ñÔºâÔºåÈ†êË®≠Ë¶èÂâáÔºà‰æãÂ¶ÇËã±Âúã‰øÆÊîπÂæåÁöÑÈÄÄÂá∫Ê©üÂà∂ÔºâÈùûÂ∏∏ÊúâÊïàÔºåÂõ†ÁÇ∫‰∫∫ÂÄëÂÇæÂêëÊñºÂ†ÖÊåÅÂÆπÊòìÂèñÂæóÁöÑÈ†êË®≠ÈÅ∏È†Ö„ÄÇÊú¨Á´†ÂàÜÊûêÁöÑÈóúÈçµÂïèÈ°åÊòØÔºåËã±ÂúãÊîøÂ∫úÂÅúÊ≠¢Êé®Âª£È°ûÂûã 1 ÈÄÄÂá∫ÊòØÂê¶ÂèØÂèñÔºå‰ª•ÂèäÈÄôÊòØÂê¶ÂèØ‰ª•Ë¶ñÁÇ∫‰∏ÄÁ®ÆÂö¥Âé≤ÁöÑÁà∂Ê¨ä‰∏ªÁæ©„ÄÇ</paragraph>

##### **ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding**
2407.19435v1 by Zhen Chen, Zongming Zhang, Wenwu Guo, Xingjian Luo, Long Bai, Jinlin Wu, Hongliang Ren, Hongbin Liu

Surgical instrument segmentation is crucial in surgical scene understanding,
thereby facilitating surgical safety. Existing algorithms directly detected all
instruments of pre-defined categories in the input image, lacking the
capability to segment specific instruments according to the surgeon's
intention. During different stages of surgery, surgeons exhibit varying
preferences and focus toward different surgical instruments. Therefore, an
instrument segmentation algorithm that adheres to the surgeon's intention can
minimize distractions from irrelevant instruments and assist surgeons to a
great extent. The recent Segment Anything Model (SAM) reveals the capability to
segment objects following prompts, but the manual annotations for prompts are
impractical during the surgery. To address these limitations in operating
rooms, we propose an audio-driven surgical instrument segmentation framework,
named ASI-Seg, to accurately segment the required surgical instruments by
parsing the audio commands of surgeons. Specifically, we propose an
intention-oriented multimodal fusion to interpret the segmentation intention
from audio commands and retrieve relevant instrument details to facilitate
segmentation. Moreover, to guide our ASI-Seg segment of the required surgical
instruments, we devise a contrastive learning prompt encoder to effectively
distinguish the required instruments from the irrelevant ones. Therefore, our
ASI-Seg promotes the workflow in the operating rooms, thereby providing
targeted support and reducing the cognitive load on surgeons. Extensive
experiments are performed to validate the ASI-Seg framework, which reveals
remarkable advantages over classical state-of-the-art and medical SAMs in both
semantic segmentation and intention-oriented segmentation. The source code is
available at https://github.com/Zonmgin-Zhang/ASI-Seg.

ÊëòË¶ÅÔºöÊâãË°ìÂô®Ê¢∞ÂàÜÂâ≤Â∞çÊñºÊâãË°ìÂ†¥ÊôØÁêÜËß£Ëá≥ÈóúÈáçË¶ÅÔºå
ÂæûËÄå‰øÉÈÄ≤ÊâãË°ìÂÆâÂÖ®„ÄÇÁèæÊúâÊºîÁÆóÊ≥ïÁõ¥Êé•ÂÅµÊ∏¨Ëº∏ÂÖ•ÂΩ±ÂÉè‰∏≠ÊâÄÊúâÈ†êÂÆöÁæ©È°ûÂà•ÁöÑÂô®Ê¢∞ÔºåÁº∫‰πèÊ†πÊìöÂ§ñÁßëÈÜ´Â∏´ÊÑèÂúñÂàÜÂâ≤ÁâπÂÆöÂô®Ê¢∞ÁöÑËÉΩÂäõ„ÄÇÂú®ÊâãË°ìÁöÑ‰∏çÂêåÈöéÊÆµÔºåÂ§ñÁßëÈÜ´Â∏´ÊúÉÂ∞ç‰∏çÂêåÁöÑÊâãË°ìÂô®Ê¢∞Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÂÅèÂ•ΩÂíåÈóúÊ≥®„ÄÇÂõ†Ê≠§Ôºå‰∏ÄÁ®ÆÈÅµÂæ™Â§ñÁßëÈÜ´Â∏´ÊÑèÂúñÁöÑÂô®Ê¢∞ÂàÜÂâ≤ÊºîÁÆóÊ≥ïÂèØ‰ª•ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Ê∏õÂ∞ëËàáÊâãË°ìÁÑ°ÈóúÁöÑÂô®Ê¢∞ÁöÑÂπ≤ÊìæÔºå‰∏¶Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂçîÂä©Â§ñÁßëÈÜ´Â∏´„ÄÇÊúÄËøëÁöÑ Segment Anything Model (SAM) Êè≠Á§∫‰∫ÜÊ†πÊìöÊèêÁ§∫ÂàÜÂâ≤Áâ©‰ª∂ÁöÑËÉΩÂäõÔºå‰ΩÜÊèêÁ§∫ÁöÑÊâãÂãïË®ªËß£Âú®ÊâãË°ìÈÅéÁ®ã‰∏≠‰∏çÂàáÂØ¶Èöõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÊâãË°ìÂÆ§‰∏≠ÁöÑÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈü≥Ë®äÈ©ÖÂãïÁöÑÊâãË°ìÂô®Ê¢∞ÂàÜÂâ≤Êû∂ÊßãÔºåÁ®±ÁÇ∫ ASI-SegÔºåÈÄöÈÅéËß£ÊûêÂ§ñÁßëÈÜ´Â∏´ÁöÑÈü≥Ë®äÂëΩ‰ª§‰æÜÊ∫ñÁ¢∫ÂàÜÂâ≤ÊâÄÈúÄÁöÑÂô®Ê¢∞„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊÑèÂúñÂ∞éÂêëÁöÑÂ§öÊ®°ÊÖãËûçÂêàÔºåÂæûÈü≥Ë®äÂëΩ‰ª§‰∏≠Ëß£ÈáãÂàÜÂâ≤ÊÑèÂúñ‰∏¶Ê™¢Á¥¢Áõ∏ÈóúÂô®Ê¢∞Á¥∞ÁØÄ‰ª•Âà©ÊñºÂàÜÂâ≤„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÊåáÂ∞éÊàëÂÄëÁöÑ ASI-Seg ÂàÜÂâ≤ÊâÄÈúÄÁöÑÂô®Ê¢∞ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂ∞çÊØîÂ≠∏ÁøíÊèêÁ§∫Á∑®Á¢ºÂô®Ôºå‰ª•ÊúâÊïàÂçÄÂàÜÊâÄÈúÄÁöÑÂô®Ê¢∞Âíå‰∏çÁõ∏ÈóúÁöÑÂô®Ê¢∞„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑ ASI-Seg ‰øÉÈÄ≤‰∫ÜÊâãË°ìÂÆ§‰∏≠ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂæûËÄåÊèê‰æõ‰∫ÜÊúâÈáùÂ∞çÊÄßÁöÑÊîØÊè¥Ôºå‰∏¶Èôç‰Ωé‰∫ÜÂ§ñÁßëÈÜ´Â∏´ÁöÑË™çÁü•Ë≤†Êìî„ÄÇÈÄ≤Ë°å‰∫ÜÂ§ßÈáèÁöÑÂØ¶È©ó‰æÜÈ©óË≠â ASI-Seg Êû∂ÊßãÔºåÈÄôÊè≠Á§∫‰∫ÜÂú®Ë™ûÁæ©ÂàÜÂâ≤ÂíåÊÑèÂúñÂ∞éÂêëÂàÜÂâ≤‰∏≠ÔºåËàáÂÇ≥Áµ±ÁöÑÊúÄÊñ∞ÊäÄË°ìÂíåÈÜ´Â≠∏ SAM Áõ∏ÊØîÔºåÂÆÉÂÖ∑ÊúâÈ°ØËëóÁöÑÂÑ™Âã¢„ÄÇÂéüÂßãÁ¢ºÂèØÂú® https://github.com/Zonmgin-Zhang/ASI-Seg Áç≤Âæó„ÄÇ

##### **A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy**
2407.19422v1 by Meng Jiang, Qing Zhao, Jianqiang Li, Fan Wang, Tianyu He, Xinyan Cheng, Bing Xiang Yang, Grace W. K. Ho, Guanghui Fu

Cognitive Behavioral Therapy (CBT) is a well-established intervention for
mitigating psychological issues by modifying maladaptive cognitive and
behavioral patterns. However, delivery of CBT is often constrained by resource
limitations and barriers to access. Advancements in artificial intelligence
(AI) have provided technical support for the digital transformation of CBT.
Particularly, the emergence of pre-training models (PTMs) and large language
models (LLMs) holds immense potential to support, augment, optimize and
automate CBT delivery. This paper reviews the literature on integrating AI into
CBT interventions. We begin with an overview of CBT. Then, we introduce the
integration of AI into CBT across various stages: pre-treatment, therapeutic
process, and post-treatment. Next, we summarized the datasets relevant to some
CBT-related tasks. Finally, we discuss the benefits and current limitations of
applying AI to CBT. We suggest key areas for future research, highlighting the
need for further exploration and validation of the long-term efficacy and
clinical utility of AI-enhanced CBT. The transformative potential of AI in
reshaping the practice of CBT heralds a new era of more accessible, efficient,
and personalized mental health interventions.

ÊëòË¶ÅÔºöË™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊòØ‰∏ÄÁ®ÆÂÆåÂñÑÁöÑÂπ≤È†êÊé™ÊñΩÔºåÈÄèÈÅéË™øÊï¥ÈÅ©Êáâ‰∏çËâØÁöÑË™çÁü•ÂíåË°åÁÇ∫Ê®°Âºè‰æÜÊ∏õËºïÂøÉÁêÜÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåCBT ÁöÑÊèê‰æõÂæÄÂæÄÂèóÂà∞Ë≥áÊ∫êÈôêÂà∂ÂíåÁç≤ÂèñÈöúÁ§ôÁöÑÈôêÂà∂„ÄÇ‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÈÄ≤Ê≠•ÁÇ∫ CBT ÁöÑÊï∏‰ΩçËΩâÂûãÊèê‰æõ‰∫ÜÊäÄË°ìÊîØÊè¥„ÄÇÁâπÂà•ÊòØÔºåÈ†êË®ìÁ∑¥Ê®°Âûã (PTM) ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõÔºåÂèØ‰ª•ÊîØÊè¥„ÄÅÊì¥ÂÖÖ„ÄÅÊúÄ‰Ω≥ÂåñÂíåËá™ÂãïÂåñ CBT ÁöÑÊèê‰æõ„ÄÇÊú¨ÊñáÂõûÈ°ß‰∫ÜÂ∞á AI Êï¥ÂêàÂà∞ CBT Âπ≤È†êÊé™ÊñΩÁöÑÊñáÁçª„ÄÇÊàëÂÄëÂæû CBT ÁöÑÊ¶ÇËø∞ÈñãÂßã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂú®ÂêÑÁ®ÆÈöéÊÆµÂ∞á AI Êï¥ÂêàÂà∞ CBT ‰∏≠ÔºöÊ≤ªÁôÇÂâç„ÄÅÊ≤ªÁôÇÈÅéÁ®ãÂíåÊ≤ªÁôÇÂæå„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜËàá‰∏Ä‰∫õ CBT Áõ∏Èóú‰ªªÂãôÁõ∏ÈóúÁöÑË≥áÊñôÈõÜ„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÂ∞á AI ÊáâÁî®Êñº CBT ÁöÑÂ•ΩËôïÂíåÁõÆÂâçÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÂª∫Ë≠∞Êú™‰æÜÁ†îÁ©∂ÁöÑ‰∏ªË¶ÅÈ†òÂüüÔºåÂº∑Ë™øÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢ÂíåÈ©óË≠â AI Â¢ûÂº∑ CBT ÁöÑÈï∑ÊúüÁôÇÊïàÂíåËá®Â∫äÊïàÁî®„ÄÇAI Âú®ÈáçÂ°ë CBT ÂØ¶Âãô‰∏≠ÁöÑËΩâÂåñÊΩõÂäõÈ†êÁ§∫Ëëó‰∏ÄÂÄãÊñ∞ÁöÑÊôÇ‰ª£ÔºåÂç≥Êõ¥ÊòìÊñºÂèñÂæó„ÄÅÊõ¥ÊúâÊïàÁéáÂíåÊõ¥ÂÄã‰∫∫ÂåñÁöÑÂøÉÁêÜÂÅ•Â∫∑Âπ≤È†êÊé™ÊñΩ„ÄÇ

##### **Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment**
2407.19380v1 by Aamer Abdul Rahman, Pranav Agarwal, Rita Noumeir, Philippe Jouvet, Vincent Michalski, Samira Ebrahimi Kahou

Offline reinforcement learning has shown promise for solving tasks in
safety-critical settings, such as clinical decision support. Its application,
however, has been limited by the lack of interpretability and interactivity for
clinicians. To address these challenges, we propose the medical decision
transformer (MeDT), a novel and versatile framework based on the
goal-conditioned reinforcement learning paradigm for sepsis treatment
recommendation. MeDT uses the decision transformer architecture to learn a
policy for drug dosage recommendation. During offline training, MeDT utilizes
collected treatment trajectories to predict administered treatments for each
time step, incorporating known treatment outcomes, target acuity scores, past
treatment decisions, and current and past medical states. This analysis enables
MeDT to capture complex dependencies among a patient's medical history,
treatment decisions, outcomes, and short-term effects on stability. Our
proposed conditioning uses acuity scores to address sparse reward issues and to
facilitate clinician-model interactions, enhancing decision-making. Following
training, MeDT can generate tailored treatment recommendations by conditioning
on the desired positive outcome (survival) and user-specified short-term
stability improvements. We carry out rigorous experiments on data from the
MIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT
recommends interventions that outperform or are competitive with existing
offline reinforcement learning methods while enabling a more interpretable,
personalized and clinician-directed approach.

ÊëòË¶ÅÔºöÈõ¢Á∑öÂº∑ÂåñÂ≠∏ÁøíÂ∑≤Â±ïÁèæÂá∫Ëß£Ê±∫Ë´∏Â¶ÇËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≠âÂÆâÂÖ®ÈóúÈçµË®≠ÂÆö‰∏≠‰ªªÂãôÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÊáâÁî®ÂèóÂà∞Ëá®Â∫äÈÜ´Â∏´Â∞çÂèØËß£ÈáãÊÄßÂíå‰∫íÂãïÊÄßÁöÑÁº∫‰πèÊâÄÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈÜ´ÁôÇÊ±∫Á≠ñËΩâÊèõÂô® (MeDT)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫ÊñºÁõÆÊ®ôÊ¢ù‰ª∂Âº∑ÂåñÂ≠∏ÁøíÁØÑ‰æãÁöÑÊñ∞Á©é‰∏îÂ§öÂäüËÉΩÁöÑÊû∂ÊßãÔºåÁî®ÊñºÊïóË°ÄÁóáÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇMeDT ‰ΩøÁî®Ê±∫Á≠ñËΩâÊèõÂô®Êû∂Êßã‰æÜÂ≠∏ÁøíËó•Áâ©ÂäëÈáèÂª∫Ë≠∞ÁöÑÊîøÁ≠ñ„ÄÇÂú®Èõ¢Á∑öË®ìÁ∑¥ÊúüÈñìÔºåMeDT Âà©Áî®Êî∂ÈõÜÁöÑÊ≤ªÁôÇËªåË∑°‰æÜÈ†êÊ∏¨ÊØèÂÄãÊôÇÈñìÊ≠•È©üÁöÑÁÆ°ÁêÜÊ≤ªÁôÇÔºå‰∏¶Á¥çÂÖ•Â∑≤Áü•ÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÅÁõÆÊ®ôÂö¥ÈáçÁ®ãÂ∫¶Ë©ïÂàÜ„ÄÅÈÅéÂéªÁöÑÊ≤ªÁôÇÊ±∫Á≠ñ‰ª•ÂèäÁï∂ÂâçÂíåÈÅéÂéªÁöÑÈÜ´ÁôÇÁãÄÊÖã„ÄÇÊ≠§ÂàÜÊûê‰Ωø MeDT ËÉΩÂ§†ÊçïÊçâÊÇ£ËÄÖÁóÖÂè≤„ÄÅÊ≤ªÁôÇÊ±∫Á≠ñ„ÄÅÁµêÊûú‰ª•ÂèäÂ∞çÁ©©ÂÆöÊÄßÁöÑÁü≠ÊúüÂΩ±Èüø‰πãÈñìÁöÑË§áÈõú‰æùË≥¥Èóú‰øÇ„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ¢ù‰ª∂‰ΩøÁî®Âö¥ÈáçÁ®ãÂ∫¶Ë©ïÂàÜ‰æÜËß£Ê±∫Á®ÄÁñèÁçéÂãµÂïèÈ°å‰∏¶‰øÉÈÄ≤Ëá®Â∫äÈÜ´Â∏´ËàáÊ®°ÂûãÁöÑ‰∫íÂãïÔºåÂæûËÄåÂ¢ûÂº∑Ê±∫Á≠ñÂà∂ÂÆö„ÄÇÂú®Ë®ìÁ∑¥‰πãÂæåÔºåMeDT ÂèØ‰ª•ÈÄöÈÅé‰ª•ÊâÄÈúÄÁöÑÊ≠£Èù¢ÁµêÊûúÔºàÂ≠òÊ¥ªÔºâÂíå‰ΩøÁî®ËÄÖÊåáÂÆöÁöÑÁü≠ÊúüÁ©©ÂÆöÊÄßÊîπÂñÑÁÇ∫Ê¢ù‰ª∂‰æÜÁî¢ÁîüÈáèË∫´ÊâìÈÄ†ÁöÑÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇÊàëÂÄëÂ∞ç‰æÜËá™ MIMIC-III Ë≥áÊñôÈõÜÁöÑË≥áÊñôÈÄ≤Ë°å‰∫ÜÂö¥Ê†ºÁöÑÂØ¶È©óÔºå‰∏¶‰ΩøÁî®ÈùûÁ≠ñÁï•Ë©ï‰º∞‰æÜË≠âÊòé MeDT Êé®Ëñ¶ÁöÑÂπ≤È†êÊé™ÊñΩÂÑ™ÊñºÊàñËàáÁèæÊúâÁöÑÈõ¢Á∑öÂº∑ÂåñÂ≠∏ÁøíÊñπÊ≥ïÂÖ∑ÊúâÁ´∂Áà≠ÂäõÔºåÂêåÊôÇÂØ¶Áèæ‰∫ÜÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄß„ÄÅÂÄãÊÄßÂåñÂíåËá®Â∫äÈÜ´Â∏´ÊåáÂ∞éÁöÑÊñπÊ≥ï„ÄÇ

##### **Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction**
2407.19359v1 by Yuan Xue, Nan Du, Anne Mottram, Martin Seneviratne, Andrew M. Dai

We propose to meta-learn an a self-supervised patient trajectory forecast
learning rule by meta-training on a meta-objective that directly optimizes the
utility of the patient representation over the subsequent clinical outcome
prediction. This meta-objective directly targets the usefulness of a
representation generated from unlabeled clinical measurement forecast for later
supervised tasks.
  The meta-learned can then be directly used in target risk prediction, and the
limited available samples can be used for further fine-tuning the model
performance. The effectiveness of our approach is tested on a real open source
patient EHR dataset MIMIC-III. We are able to demonstrate that our
attention-based patient state representation approach can achieve much better
performance for predicting target risk with low resources comparing with both
direct supervised learning and pretraining with all-observation trajectory
forecast.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêË≠∞ÈÄèÈÅéÂÖÉË®ìÁ∑¥‰æÜÂÖÉÂ≠∏Áøí‰∏ÄÂÄãËá™ÊàëÁõ£Áù£ÁöÑÊÇ£ËÄÖËªåË∑°È†êÊ∏¨Â≠∏ÁøíË¶èÂâáÔºå‰∏¶ÈÄèÈÅéÂÖÉÁõÆÊ®ôÁõ¥Êé•ÊúÄ‰Ω≥ÂåñÊÇ£ËÄÖË°®ÂæµÂú®ÂæåÁ∫åËá®Â∫äÁµêÊûúÈ†êÊ∏¨‰∏≠ÁöÑÊïàÁî®„ÄÇÊ≠§ÂÖÉÁõÆÊ®ôÁõ¥Êé•ÈáùÂ∞çÂæûÊú™Ê®ôË®òÁöÑËá®Â∫äÊ∏¨ÈáèÈ†êÊ∏¨ÊâÄÁî¢ÁîüÁöÑË°®ÂæµÂú®ÂæåÁ∫åÁõ£Áù£Âºè‰ªªÂãô‰∏≠ÁöÑÊïàÁî®„ÄÇ
ÂÖÉÂ≠∏ÁøíÂæåÔºåÂèØ‰ª•Áõ¥Êé•Áî®ÊñºÁõÆÊ®ôÈ¢®Èö™È†êÊ∏¨Ôºå‰∏îÂèØ‰ΩøÁî®ÊúâÈôêÁöÑÂèØÁî®Ê®£Êú¨ÈÄ≤‰∏ÄÊ≠•ÂæÆË™øÊ®°ÂûãÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ï‰πãÊúâÊïàÊÄßÂ∑≤Âú®‰∏ÄÂÄãÁúüÂØ¶ÁöÑÈñãÊîæÂéüÂßãÁ¢ºÊÇ£ËÄÖÈõªÂ≠êÁóÖÊ≠∑Ë≥áÊñôÈõÜ MIMIC-III ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÊàëÂÄëËÉΩÂ§†Ë≠âÊòéÔºåËàáÁõ¥Êé•Áõ£Áù£ÂºèÂ≠∏ÁøíÂíå‰ΩøÁî®ÊâÄÊúâËßÄÂØüËªåË∑°È†êÊ∏¨ÈÄ≤Ë°åÈ†êË®ìÁ∑¥Áõ∏ÊØîÔºåÊàëÂÄëÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÊÇ£ËÄÖÁãÄÊÖãË°®ÂæµÊñπÊ≥ïÂèØ‰ª•ÈÅîÂà∞Êõ¥Â•ΩÁöÑÁõÆÊ®ôÈ¢®Èö™È†êÊ∏¨ÊïàËÉΩÔºå‰∏îË≥áÊ∫êÈúÄÊ±ÇËºÉ‰Ωé„ÄÇ

##### **Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification**
2407.19340v1 by Santosh V. Patapati

Major Depressive Disorder (MDD) is a pervasive mental health condition that
affects 300 million people worldwide. This work presents a novel, BiLSTM-based
tri-modal model-level fusion architecture for the binary classification of
depression from clinical interview recordings. The proposed architecture
incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses
a two-shot learning based GPT-4 model to process text data. This is the first
work to incorporate large language models into a multi-modal architecture for
this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge
cross-validation split and Leave-One-Subject-Out cross-validation split,
surpassing all baseline models and multiple state-of-the-art models. In
Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score
of 85.95%, a precision of 80%, and a recall of 92.86%.

ÊëòË¶ÅÔºöÈáçÂ∫¶ÊÜÇÈ¨±Áóá (MDD) ÊòØ‰∏ÄÁ®ÆÊôÆÈÅçÁöÑÁ≤æÁ•ûÂÅ•Â∫∑ÁñæÁóÖÔºå
ÂΩ±ÈüøÂÖ®ÁêÉ 3 ÂÑÑ‰∫∫„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ„ÄÅÂü∫Êñº BiLSTM
ÁöÑ‰∏âÊ®°ÊÖãÊ®°ÂûãÁ¥öËûçÂêàÊû∂ÊßãÔºåÁî®ÊñºÂæûËá®Â∫äË®™Ë´áÈåÑÈü≥‰∏≠Â∞çÊÜÇÈ¨±ÁóáÈÄ≤Ë°å‰∫åÂÖÉÂàÜÈ°û„ÄÇÊâÄÊèêÂá∫ÁöÑÊû∂Êßã
ÁµêÂêà‰∫ÜÊ¢ÖÁàæÈ†ªÁéáÂÄíË≠ú‰øÇÊï∏„ÄÅÈù¢ÈÉ®Âãï‰ΩúÂñÆÂÖÉÔºå‰∏¶‰ΩøÁî®Âü∫Êñº GPT-4 ÁöÑÂÖ©Ê¨°Â≠∏ÁøíÊ®°Âûã‰æÜËôïÁêÜÊñáÊú¨Êï∏Êìö„ÄÇÈÄôÊòØÁ¨¨‰∏ÄÂÄã
Â∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁ¥çÂÖ•Â§öÊ®°ÊÖãÊû∂Êßã‰ª•Âü∑Ë°åÊ≠§‰ªªÂãôÁöÑÂ∑•‰Ωú„ÄÇÂÆÉÂú® DAIC-WOZ AVEC 2016 ÊåëÊà∞Ë≥Ω
‰∫§ÂèâÈ©óË≠âÂàÜÂâ≤ÂíåÁïô‰∏ÄÂèóË©¶ËÄÖ‰∫§ÂèâÈ©óË≠âÂàÜÂâ≤‰∏≠ÂèñÂæó‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÁµêÊûúÔºåË∂ÖË∂ä‰∫ÜÊâÄÊúâÂü∫Á∑öÊ®°ÂûãÂíåÂ§öÂÄãÊúÄÂÖàÈÄ≤ÁöÑÊ®°Âûã„ÄÇÂú®
Áïô‰∏ÄÂèóË©¶ËÄÖÊ∏¨Ë©¶‰∏≠ÔºåÂÆÉÁöÑÊ∫ñÁ¢∫ÁéáÈÅîÂà∞ 91.01%ÔºåF1 ÂàÜÊï∏
ÁÇ∫ 85.95%ÔºåÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 80%ÔºåÂè¨ÂõûÁéáÁÇ∫ 92.86%„ÄÇ

##### **Multi-Modal CLIP-Informed Protein Editing**
2407.19296v1 by Mingze Yin, Hanjing Zhou, Yiheng Zhu, Miao Lin, Yixuan Wu, Jialu Wu, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou, Jintai Chen, Jian Wu

Proteins govern most biological functions essential for life, but achieving
controllable protein discovery and optimization remains challenging. Recently,
machine learning-assisted protein editing (MLPE) has shown promise in
accelerating optimization cycles and reducing experimental workloads. However,
current methods struggle with the vast combinatorial space of potential protein
edits and cannot explicitly conduct protein editing using biotext instructions,
limiting their interactivity with human feedback. To fill these gaps, we
propose a novel method called ProtET for efficient CLIP-informed protein
editing through multi-modality learning. Our approach comprises two stages: in
the pretraining stage, contrastive learning aligns protein-biotext
representations encoded by two large language models (LLMs), respectively.
Subsequently, during the protein editing stage, the fused features from editing
instruction texts and original protein sequences serve as the final editing
condition for generating target protein sequences. Comprehensive experiments
demonstrated the superiority of ProtET in editing proteins to enhance
human-expected functionality across multiple attribute domains, including
enzyme catalytic activity, protein stability and antibody specific binding
ability. And ProtET improves the state-of-the-art results by a large margin,
leading to significant stability improvements of 16.67% and 16.90%. This
capability positions ProtET to advance real-world artificial protein editing,
potentially addressing unmet academic, industrial, and clinical needs.

ÊëòË¶ÅÔºö<paragraph>ËõãÁôΩË¥®ÊéåÁÆ°ÁùÄÁª¥ÊåÅÁîüÂëΩÊâÄÈúÄÁöÑÂ§ßÂ§öÊï∞ÁîüÁâ©ÂäüËÉΩÔºå‰ΩÜË¶ÅÂÆûÁé∞ÂèØÊéßÁöÑËõãÁôΩË¥®ÂèëÁé∞Âíå‰ºòÂåñ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊúÄËøëÔºåÊú∫Âô®Â≠¶‰π†ËæÖÂä©ËõãÁôΩË¥®ÁºñËæë (MLPE) Â∑≤ÊòæÁ§∫Âá∫Âú®Âä†ÈÄü‰ºòÂåñÂë®ÊúüÂíåÂáèÂ∞ëÂÆûÈ™åÂ∑•‰ΩúÈáèÊñπÈù¢ÁöÑÂâçÊôØ„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÊñπÊ≥ïÈöæ‰ª•Â∫îÂØπÊΩúÂú®ËõãÁôΩË¥®ÁºñËæëÁöÑÂ∑®Â§ßÁªÑÂêàÁ©∫Èó¥ÔºåÂπ∂‰∏îÊó†Ê≥ïÊòéÁ°Æ‰ΩøÁî®ÁîüÁâ©ÊñáÊú¨ËØ¥ÊòéËøõË°åËõãÁôΩË¥®ÁºñËæëÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨‰∏é‰∫∫Á±ªÂèçÈ¶àÁöÑ‰∫§‰∫íÊÄß„ÄÇ‰∏∫‰∫ÜÂ°´Ë°•Ëøô‰∫õÁ©∫ÁôΩÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ProtET ÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÈÄöËøáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ËøõË°åÈ´òÊïàÁöÑ CLIP Áü•ÊÉÖËõãÁôΩË¥®ÁºñËæë„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™Èò∂ÊÆµÔºöÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÂØπÊØîÂ≠¶‰π†Â∞ÜÂàÜÂà´Áî±‰∏§‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁºñÁ†ÅÁöÑËõãÁôΩË¥®ÁîüÁâ©ÊñáÊú¨Ë°®Á§∫ÂØπÈΩê„ÄÇÈöèÂêéÔºåÂú®ËõãÁôΩË¥®ÁºñËæëÈò∂ÊÆµÔºåÊù•Ëá™ÁºñËæëÊåá‰ª§ÊñáÊú¨ÂíåÂéüÂßãËõãÁôΩË¥®Â∫èÂàóÁöÑËûçÂêàÁâπÂæÅ‰Ωú‰∏∫ÁîüÊàêÁõÆÊ†áËõãÁôΩË¥®Â∫èÂàóÁöÑÊúÄÁªàÁºñËæëÊù°‰ª∂„ÄÇÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåProtET Âú®ÁºñËæëËõãÁôΩË¥®‰ª•Â¢ûÂº∫Ë∑®Â§ö‰∏™Â±ûÊÄßÂüüÁöÑ‰∫∫Á±ªÈ¢ÑÊúüÂäüËÉΩÊñπÈù¢ÂÖ∑Êúâ‰ºòÂäøÔºåÂåÖÊã¨ÈÖ∂ÂÇ¨ÂåñÊ¥ªÊÄß„ÄÅËõãÁôΩË¥®Á®≥ÂÆöÊÄßÂíåÊäó‰ΩìÁâπÂºÇÊÄßÁªìÂêàËÉΩÂäõ„ÄÇProtET Â∞ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÊèêÈ´ò‰∫Ü‰∏Ä‰∏™ÂæàÂ§ßÁöÑÂπÖÂ∫¶ÔºåÂØºËá¥Á®≥ÂÆöÊÄßÊòæÁùÄÊèêÈ´ò‰∫Ü 16.67% Âíå 16.90%„ÄÇËøôÁßçËÉΩÂäõ‰Ωø ProtET ËÉΩÂ§üÊé®ËøõÁé∞ÂÆû‰∏ñÁïåÁöÑ‰∫∫Â∑•ËõãÁôΩË¥®ÁºñËæëÔºåÊúâÂèØËÉΩÊª°Ë∂≥Êú™Êª°Ë∂≥ÁöÑÂ≠¶ÊúØ„ÄÅÂ∑•‰∏öÂíå‰∏¥Â∫äÈúÄÊ±Ç„ÄÇ</paragraph>

##### **Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review**
2407.19256v1 by Tongyue Shi, Jun Ma, Zihan Yu, Haowei Xu, Minqi Xiong, Meirong Xiao, Yilin Li, Huiying Zhao, Guilan Kong

With the rapid development of artificial intelligence (AI), large language
models (LLMs) have shown strong capabilities in natural language understanding,
reasoning, and generation, attracting amounts of research interest in applying
LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis
and treatment for critically ill patients who often require intensive
monitoring and interventions in intensive care units (ICUs). Can LLMs be
applied to CCM? Are LLMs just like stochastic parrots or ICU experts in
assisting clinical decision-making? This scoping review aims to provide a
panoramic portrait of the application of LLMs in CCM. Literature in seven
databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE
Xplore, and ACM Digital Library, were searched from January 1, 2019, to June
10, 2024. Peer-reviewed journal and conference articles that discussed the
application of LLMs in critical care settings were included. From an initial
619 articles, 24 were selected for final review. This review grouped
applications of LLMs in CCM into three categories: clinical decision support,
medical documentation and reporting, and medical education and doctor-patient
communication. LLMs have advantages in handling unstructured data and do not
require manual feature engineering. Meanwhile, applying LLMs to CCM faces
challenges, including hallucinations, poor interpretability, bias and alignment
challenges, and privacy and ethics issues. Future research should enhance model
reliability and interpretability, integrate up-to-date medical knowledge, and
strengthen privacy and ethical guidelines. As LLMs evolve, they could become
key tools in CCM to help improve patient outcomes and optimize healthcare
delivery. This study is the first review of LLMs in CCM, aiding researchers,
clinicians, and policymakers to understand the current status and future
potentials of LLMs in CCM.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£„ÄÅÊé®ÁêÜÂíåÁîüÊàêÊñπÈù¢Â±ïÁèæÂá∫Âº∑Â§ßÁöÑËÉΩÂäõÔºåÂê∏Âºï‰∫ÜÂ§ßÈáèÁ†îÁ©∂‰∫∫Âì°Â∞çÂ∞á LLM ÊáâÁî®ÊñºÂÅ•Â∫∑ÂíåÈÜ´Â≠∏È†òÂüüÁöÑËààË∂£„ÄÇÈáçÁóáÈÜ´Â≠∏ (CCM) ÁÇ∫ÁóÖÂç±ÊÇ£ËÄÖÊèê‰æõË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºåÈÄô‰∫õÊÇ£ËÄÖÈÄöÂ∏∏ÈúÄË¶ÅÂú®ÈáçÁóáÁõ£Ë≠∑ÁóÖÊàø (ICU) ‰∏≠ÈÄ≤Ë°åÂØÜÈõÜÁõ£ÊéßÂíåÂπ≤È†ê„ÄÇLLM ËÉΩÂê¶ÊáâÁî®Êñº CCMÔºüLLM ÂçîÂä©Ëá®Â∫äÊ±∫Á≠ñÊôÇÔºåÊòØÂê¶ÂÉÖÂÉèÈö®Ê©üÈ∏öÈµ°Êàñ ICU Â∞àÂÆ∂ÔºüÊú¨ÁØÑÂúçÂØ©Êü•Êó®Âú®Êèê‰æõ LLM Âú® CCM ‰∏≠ÊáâÁî®ÁöÑÂÖ®ÊôØÊ¶ÇÊ≥Å„ÄÇÂæû PubMed„ÄÅEmbase„ÄÅScopus„ÄÅWeb of Science„ÄÅCINAHL„ÄÅIEEE Xplore Âíå ACM Digital Library Á≠â‰∏ÉÂÄãË≥áÊñôÂ∫´‰∏≠ÊêúÂ∞ã 2019 Âπ¥ 1 Êúà 1 Êó•Ëá≥ 2024 Âπ¥ 6 Êúà 10 Êó•‰πãÈñìÁöÑÊñáÁçª„ÄÇÁ¥çÂÖ•‰∫ÜË®éË´ñ LLM Âú®ÈáçÁóáÁÖßË≠∑Áí∞Â¢É‰∏≠ÊáâÁî®ÁöÑÂêåË°åË©ïÂØ©ÊúüÂàäÂíåÊúÉË≠∞Ë´ñÊñá„ÄÇÂú®ÊúÄÂàùÁöÑ 619 ÁØáË´ñÊñá‰∏≠ÔºåÈÅ∏Âá∫ 24 ÁØáÈÄ≤Ë°åÊúÄÁµÇÂØ©Êü•„ÄÇÊú¨ÂØ©Êü•Â∞á LLM Âú® CCM ‰∏≠ÁöÑÊáâÁî®ÂàÜÁÇ∫‰∏âÈ°ûÔºöËá®Â∫äÊ±∫Á≠ñÊîØÊè¥„ÄÅÈÜ´ÁôÇÊñá‰ª∂ÂíåÂ†±ÂëäÔºå‰ª•ÂèäÈÜ´Â≠∏ÊïôËÇ≤ÂíåÈÜ´ÊÇ£Ê∫ùÈÄö„ÄÇLLM Âú®ËôïÁêÜÈùûÁµêÊßãÂåñË≥áÊñôÊñπÈù¢ÂÖ∑ÊúâÂÑ™Âã¢Ôºå‰∏î‰∏çÈúÄË¶ÅÊâãÂãïÁâπÂæµÂ∑•Á®ã„ÄÇÂêåÊôÇÔºåÂ∞á LLM ÊáâÁî®Êñº CCM Èù¢Ëá®ÊåëÊà∞ÔºåÂåÖÊã¨ÂπªË¶∫„ÄÅÂèØËß£ÈáãÊÄßÂ∑Æ„ÄÅÂÅèÂ∑ÆÂíåÂ∞çÈΩäÊåëÊà∞Ôºå‰ª•ÂèäÈö±ÁßÅÂíåÈÅìÂæ∑ÂïèÈ°å„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÂä†Âº∑Ê®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂèØËß£ÈáãÊÄßÔºåÊï¥ÂêàÊúÄÊñ∞ÁöÑÈÜ´Â≠∏Áü•Ë≠òÔºå‰∏¶Âä†Âº∑Èö±ÁßÅÂíåÈÅìÂæ∑Ê∫ñÂâá„ÄÇÈö®Ëëó LLM ÁöÑÁôºÂ±ïÔºåÂÆÉÂÄëÂèØËÉΩÊúÉÊàêÁÇ∫ CCM ‰∏≠ÁöÑÈóúÈçµÂ∑•ÂÖ∑ÔºåÊúâÂä©ÊñºÊîπÂñÑÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊàêÊûú‰∏¶ÂÑ™ÂåñÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇÊú¨Á†îÁ©∂ÊòØ LLM Âú® CCM ‰∏≠ÁöÑÁ¨¨‰∏ÄÁØáÂØ©Êü•ÔºåÊúâÂä©ÊñºÁ†îÁ©∂‰∫∫Âì°„ÄÅËá®Â∫äÈÜ´ÁîüÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖ‰∫ÜËß£ LLM Âú® CCM ‰∏≠ÁöÑÁèæÁãÄÂíåÊú™‰æÜÊΩõÂäõ„ÄÇ

##### **Channel Boosted CNN-Transformer-based Multi-Level and Multi-Scale Nuclei Segmentation**
2407.19186v1 by Zunaira Rauf, Abdul Rehman Khan, Asifullah Khan

Accurate nuclei segmentation is an essential foundation for various
applications in computational pathology, including cancer diagnosis and
treatment planning. Even slight variations in nuclei representations can
significantly impact these downstream tasks. However, achieving accurate
segmentation remains challenging due to factors like clustered nuclei, high
intra-class variability in size and shape, resemblance to other cells, and
color or contrast variations between nuclei and background. Despite the
extensive utilization of Convolutional Neural Networks (CNNs) in medical image
segmentation, they may have trouble capturing long-range dependencies crucial
for accurate nuclei delineation. Transformers address this limitation but might
miss essential low-level features. To overcome these limitations, we utilized
CNN-Transformer-based techniques for nuclei segmentation in H&E stained
histology images. In this work, we proposed two CNN-Transformer architectures,
Nuclei Hybrid Vision Transformer (NucleiHVT) and Channel Boosted Nuclei Hybrid
Vision Transformer (CB-NucleiHVT), that leverage the strengths of both CNNs and
Transformers to effectively learn nuclei boundaries in multi-organ histology
images. The first architecture, NucleiHVT is inspired by the UNet architecture
and incorporates the dual attention mechanism to capture both multi-level and
multi-scale context effectively. The CB-NucleiHVT network, on the other hand,
utilizes the concept of channel boosting to learn diverse feature spaces,
enhancing the model's ability to distinguish subtle variations in nuclei
characteristics. Detailed evaluation of two medical image segmentation datasets
shows that the proposed architectures outperform existing CNN-based,
Transformer-based, and hybrid methods. The proposed networks demonstrated
effective results both in terms of quantitative metrics, and qualitative visual
assessment.

ÊëòË¶ÅÔºöÁ≤æÁ¢∫ÁöÑÁ¥∞ËÉûÊ†∏ÂàÜÂâ≤ÊòØË®àÁÆóÁóÖÁêÜÂ≠∏‰∏≠ÂêÑÁ®ÆÊáâÁî®ÔºàÂåÖÊã¨ÁôåÁóáË®∫Êñ∑ÂíåÊ≤ªÁôÇË¶èÂäÉÔºâÁöÑÂü∫Á§é„ÄÇÂç≥‰ΩøÁ¥∞ËÉûÊ†∏Ë°®ÁèæÂΩ¢ÂºèÊúâËºïÂæÆËÆäÂåñÔºå‰πüÊúÉÂ∞çÈÄô‰∫õ‰∏ãÊ∏∏‰ªªÂãôÁî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁ¥∞ËÉûÊ†∏ËÅöÈõÜ„ÄÅÂ§ßÂ∞èÂíåÂΩ¢ÁãÄÁöÑÈ°ûÂÖßËÆäÁï∞ÊÄßÈ´ò„ÄÅËàáÂÖ∂‰ªñÁ¥∞ËÉûÁõ∏‰ºº„ÄÅÁ¥∞ËÉûÊ†∏ËàáËÉåÊôØ‰πãÈñìÁöÑÈ°èËâ≤ÊàñÂ∞çÊØîÂ∫¶ËÆäÂåñÁ≠âÂõ†Á¥†ÔºåÂØ¶ÁèæÁ≤æÁ¢∫ÂàÜÂâ≤‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂÑòÁÆ°Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÂæóÂà∞Âª£Ê≥õÊáâÁî®Ôºå‰ΩÜÂÆÉÂÄëÂèØËÉΩÈõ£‰ª•ÊçïÊçâÂ∞çÊñºÁ≤æÁ¢∫Á¥∞ËÉûÊ†∏ÊèèÁπ™Ëá≥ÈóúÈáçË¶ÅÁöÑÈï∑Á®ã‰æùË≥¥ÊÄß„ÄÇTransformer Ëß£Ê±∫‰∫ÜÈÄôÂÄãÈôêÂà∂Ôºå‰ΩÜÂèØËÉΩÊúÉÈåØÈÅéÂøÖË¶ÅÁöÑ‰ΩéÈöéÁâπÂæµ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂà©Áî®Âü∫Êñº CNN-Transformer ÁöÑÊäÄË°ìÂ∞ç H&E ÊüìËâ≤ÁöÑÁµÑÁπîÂ≠∏ÂΩ±ÂÉèÈÄ≤Ë°åÁ¥∞ËÉûÊ†∏ÂàÜÂâ≤„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ©Á®Æ CNN-Transformer Êû∂ÊßãÔºåÂç≥Á¥∞ËÉûÊ†∏Ê∑∑ÂêàË¶ñË¶∫ TransformerÔºàNucleiHVTÔºâÂíåÈÄöÈÅìÂ¢ûÂº∑Á¥∞ËÉûÊ†∏Ê∑∑ÂêàË¶ñË¶∫ TransformerÔºàCB-NucleiHVTÔºâÔºåÂÆÉÂÄëÂà©Áî® CNN Âíå Transformer ÁöÑÂÑ™Âã¢‰æÜÊúâÊïàÂ≠∏ÁøíÂ§öÂô®ÂÆòÁµÑÁπîÂ≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁ¥∞ËÉûÊ†∏ÈÇäÁïå„ÄÇÁ¨¨‰∏ÄÂÄãÊû∂Êßã NucleiHVT ÂèóÂà∞ UNet Êû∂ÊßãÁöÑÂïüÁôºÔºå‰∏¶ÁµêÂêàÈõôÊ≥®ÊÑèÂäõÊ©üÂà∂‰æÜÊúâÊïàÊçïÊçâÂ§öÂ±§Á¥öÂíåÂ§öÂ∞∫Â∫¶ÁöÑËÉåÊôØ„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåCB-NucleiHVT Á∂≤Ë∑ØÂà©Áî®ÈÄöÈÅìÂ¢ûÂº∑ÁöÑÊ¶ÇÂøµ‰æÜÂ≠∏Áøí‰∏çÂêåÁöÑÁâπÂæµÁ©∫ÈñìÔºåÂ¢ûÂº∑Ê®°ÂûãÂçÄÂàÜÁ¥∞ËÉûÊ†∏ÁâπÂæµÁ¥∞ÂæÆËÆäÂåñÁöÑËÉΩÂäõ„ÄÇÂ∞çÂÖ©ÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ë≥áÊñôÈõÜÁöÑË©≥Á¥∞Ë©ï‰º∞Ë°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Êñº CNN„ÄÅÂü∫Êñº Transformer ÂíåÊ∑∑ÂêàÊñπÊ≥ï„ÄÇÊâÄÊèêÂá∫ÁöÑÁ∂≤Ë∑ØÂú®ÈáèÂåñÊåáÊ®ôÂíåÂÆöÊÄßË¶ñË¶∫Ë©ï‰º∞ÊñπÈù¢ÈÉΩÂ±ïÁ§∫‰∫ÜÊúâÊïàÁµêÊûú„ÄÇ

##### **Large Language Models as Co-Pilots for Causal Inference in Medical Studies**
2407.19118v1 by Ahmed Alaa, Rachael V. Phillips, Emre Kƒ±cƒ±man, Laura B. Balzer, Mark van der Laan, Maya Petersen

The validity of medical studies based on real-world clinical data, such as
observational studies, depends on critical assumptions necessary for drawing
causal conclusions about medical interventions. Many published studies are
flawed because they violate these assumptions and entail biases such as
residual confounding, selection bias, and misalignment between treatment and
measurement times. Although researchers are aware of these pitfalls, they
continue to occur because anticipating and addressing them in the context of a
specific study can be challenging without a large, often unwieldy,
interdisciplinary team with extensive expertise. To address this expertise gap,
we explore the use of large language models (LLMs) as co-pilot tools to assist
researchers in identifying study design flaws that undermine the validity of
causal inferences. We propose a conceptual framework for LLMs as causal
co-pilots that encode domain knowledge across various fields, engaging with
researchers in natural language interactions to provide contextualized
assistance in study design. We provide illustrative examples of how LLMs can
function as causal co-pilots, propose a structured framework for their
grounding in existing causal inference frameworks, and highlight the unique
challenges and opportunities in adapting LLMs for reliable use in
epidemiological research.

ÊëòË¶ÅÔºöÂü∫ÊñºÁúüÂØ¶‰∏ñÁïåËá®Â∫äË≥áÊñôÁöÑÈÜ´Â≠∏Á†îÁ©∂Ôºå‰æãÂ¶ÇËßÄÂØüÊÄßÁ†îÁ©∂ÔºåÂÖ∂ÊúâÊïàÊÄßÂèñÊ±∫ÊñºÂæóÂá∫ÈÜ´ÁôÇ‰ªãÂÖ•Âõ†ÊûúÁµêË´ñÊôÇÂøÖË¶ÅÁöÑÈóúÈçµÂÅáË®≠„ÄÇË®±Â§öÂ∑≤ÁôºË°®ÁöÑÁ†îÁ©∂ÊâÄÂ≠òÂú®Áº∫Èô∑ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÈÅïÂèç‰∫ÜÈÄô‰∫õÂÅáË®≠Ôºå‰∏¶Â∞éËá¥‰∫ÜÊÆòÁïôÊ∑∑Ê∑Ü„ÄÅÈÅ∏ÊìáÂÅèË™§‰ª•ÂèäÊ≤ªÁôÇËàáÊ∏¨ÈáèÊôÇÈñì‰πãÈñìÁöÑ‰∏ç‰∏ÄËá¥Á≠âÂÅèÂ∑Æ„ÄÇÂÑòÁÆ°Á†îÁ©∂‰∫∫Âì°ÊÑèË≠òÂà∞ÈÄô‰∫õÁº∫Èô∑Ôºå‰ΩÜÂÆÉÂÄë‰ªçÁÑ∂ÊúÉÁôºÁîüÔºåÂõ†ÁÇ∫Âú®ÂÖ∑È´îÁöÑÁ†îÁ©∂ËÉåÊôØ‰∏ãÈ†êÊúü‰∏¶Ëß£Ê±∫ÈÄô‰∫õÁº∫Èô∑ÂèØËÉΩÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÈô§ÈùûÊúâ‰∏ÄÂÄãÈæêÂ§ß‰∏îÈÄöÂ∏∏Èõ£‰ª•ÊéßÂà∂ÁöÑ„ÄÅÊìÅÊúâÂª£Ê≥õÂ∞àÊ•≠Áü•Ë≠òÁöÑË∑®Â≠∏ÁßëÂúòÈöä„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÁ®ÆÂ∞àÊ•≠Áü•Ë≠òÂ∑ÆË∑ùÔºåÊàëÂÄëÊé¢Á¥¢‰∫Ü‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫ÂâØÈßïÈßõÂ∑•ÂÖ∑Ôºå‰ª•ÂçîÂä©Á†îÁ©∂‰∫∫Âì°Ë≠òÂà•Á†¥Â£ûÂõ†ÊûúÊé®Ë´ñÊúâÊïàÊÄßÁöÑÁ†îÁ©∂Ë®≠Ë®àÁº∫Èô∑„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü LLM ‰ΩúÁÇ∫Âõ†ÊûúÂâØÈßïÈßõÁöÑÊ¶ÇÂøµÊû∂ÊßãÔºåË©≤Êû∂ÊßãÁ∑®Á¢º‰∫ÜË∑®ÂêÑÁ®ÆÈ†òÂüüÁöÑÈ†òÂüüÁü•Ë≠òÔºå‰∏¶ÈÄöÈÅéËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãïËàáÁ†îÁ©∂‰∫∫Âì°‰∫íÂãïÔºå‰ª•Âú®Á†îÁ©∂Ë®≠Ë®à‰∏≠Êèê‰æõÊÉÖÂ¢ÉÂåñÁöÑÂçîÂä©„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü LLM Â¶Ç‰Ωï‰ΩúÁÇ∫Âõ†ÊûúÂâØÈßïÈßõÈÅã‰ΩúÁöÑË™™ÊòéÊÄßÁØÑ‰æãÔºåÊèêÂá∫‰∫ÜÂÆÉÂÄëÂú®ÁèæÊúâÂõ†ÊûúÊé®Ë´ñÊ°ÜÊû∂‰∏≠Êé•Âú∞ÁöÑÁµêÊßãÂåñÊ°ÜÊû∂Ôºå‰∏¶Âº∑Ë™ø‰∫ÜÂú®ÊµÅË°åÁóÖÂ≠∏Á†îÁ©∂‰∏≠ÈÅ©Êáâ LLM ‰ª•ÂØ¶ÁèæÂèØÈù†‰ΩøÁî®ÁöÑÁç®ÁâπÊåëÊà∞ÂíåÊ©üÈÅá„ÄÇ

##### **Solving Robotics Problems in Zero-Shot with Vision-Language Models**
2407.19094v1 by Zidan Wang, Rui Shen, Bradly Stadie

We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for
solving robotics problems in the zero-shot regime. By zero-shot we mean that,
for a novel environment, we feed a VLLM an image of the robot's environment and
a description of the task, and have the VLLM output the sequence of actions
necessary for the robot to complete the task. Prior work on VLLMs in robotics
has largely focused on settings where some part of the pipeline is fine-tuned,
such as tuning an LLM on robot data or training a separate vision encoder for
perception and action generation. Surprisingly, due to recent advances in the
capabilities of VLLMs, this type of fine-tuning may no longer be necessary for
many tasks. In this work, we show that with careful engineering, we can prompt
a single off-the-shelf VLLM to handle all aspects of a robotics task, from
high-level planning to low-level location-extraction and action-execution.
Wonderful Team builds on recent advances in multi-agent LLMs to partition tasks
across an agent hierarchy, making it self-corrective and able to effectively
partition and solve even long-horizon tasks. Extensive experiments on VIMABench
and real-world robotic environments demonstrate the system's capability to
handle a variety of robotic tasks, including manipulation, visual
goal-reaching, and visual reasoning, all in a zero-shot manner. These results
underscore a key point: vision-language models have progressed rapidly in the
past year, and should strongly be considered as a backbone for robotics
problems going forward.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊé®Âá∫ Wonderful TeamÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§ö‰ª£ÁêÜË¶ñË¶∫ LLM (VLLM) Êû∂ÊßãÔºåÁî®ÊñºËß£Ê±∫Èõ∂Ê¨°Â≠∏ÁøíÊ®°Âºè‰∏ãÁöÑÊ©üÂô®‰∫∫ÂïèÈ°å„ÄÇÈõ∂Ê¨°Â≠∏ÁøíÊòØÊåáÔºåÂ∞çÊñº‰∏ÄÂÄãÊñ∞Áí∞Â¢ÉÔºåÊàëÂÄëÂêë VLLM Êèê‰æõÊ©üÂô®‰∫∫Áí∞Â¢ÉÁöÑÂúñÂÉèÂíå‰ªªÂãôÊèèËø∞Ôºå‰∏¶ËÆì VLLM Ëº∏Âá∫Ê©üÂô®‰∫∫ÂÆåÊàê‰ªªÂãôÊâÄÈúÄÁöÑÂãï‰ΩúÂ∫èÂàó„ÄÇÊ©üÂô®‰∫∫È†òÂüü‰∏≠ VLLM ÁöÑÂÖàÂâçÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁÆ°ÈÅìÊüê‰∏ÄÈÉ®ÂàÜÈÄ≤Ë°åÂæÆË™øÁöÑË®≠ÂÆö‰∏äÔºå‰æãÂ¶ÇÈáùÂ∞çÊ©üÂô®‰∫∫Ë≥áÊñôÂæÆË™ø LLM ÊàñË®ìÁ∑¥‰∏ÄÂÄãÂñÆÁç®ÁöÑË¶ñË¶∫Á∑®Á¢ºÂô®‰ª•ÈÄ≤Ë°åÊÑüÁü•ÂíåÂãï‰ΩúÁî¢Áîü„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÁî±Êñº VLLM ËÉΩÂäõÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÂ∞çÊñºË®±Â§ö‰ªªÂãô‰æÜË™™ÔºåÈÄôÁ®ÆÂæÆË™øÂèØËÉΩ‰∏çÂÜçÂøÖË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄèÈÅé‰ªîÁ¥∞ÁöÑÂ∑•Á®ãË®≠Ë®àÔºåÊàëÂÄëÂèØ‰ª•ÊèêÁ§∫‰∏ÄÂÄãÂñÆ‰∏ÄÁöÑÁèæÊàê VLLM ‰æÜËôïÁêÜÊ©üÂô®‰∫∫‰ªªÂãôÁöÑÊâÄÊúâÊñπÈù¢ÔºåÂæûÈ´òÂ±§Á¥öË¶èÂäÉÂà∞‰ΩéÂ±§Á¥ö‰ΩçÁΩÆÊèêÂèñÂíåÂãï‰ΩúÂü∑Ë°å„ÄÇWonderful Team Âª∫Á´ãÂú®Â§ö‰ª£ÁêÜ LLM ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰∏äÔºå‰ª•Âú®‰ª£ÁêÜÂ±§Á¥ö‰∏≠ÂàÜÈÖç‰ªªÂãôÔºå‰ΩøÂÖ∂ÂÖ∑ÊúâËá™Êàë‰øÆÊ≠£ËÉΩÂäõÔºå‰∏¶ËÉΩÊúâÊïàÂú∞ÂàÜÈÖçÂíåËß£Ê±∫Èï∑ÈÅ†‰ªªÂãô„ÄÇÂú® VIMABench ÂíåÁúüÂØ¶Ê©üÂô®‰∫∫Áí∞Â¢É‰∏≠ÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÁ≥ªÁµ±ËôïÁêÜÂêÑÁ®ÆÊ©üÂô®‰∫∫‰ªªÂãôÁöÑËÉΩÂäõÔºåÂåÖÊã¨Êìç‰Ωú„ÄÅË¶ñË¶∫ÁõÆÊ®ôÈÅîÊàêÂíåË¶ñË¶∫Êé®ÁêÜÔºåÊâÄÊúâÈÄô‰∫õÈÉΩÊòØÂú®Èõ∂Ê¨°Â≠∏ÁøíÊ®°Âºè‰∏ãÈÄ≤Ë°åÁöÑ„ÄÇÈÄô‰∫õÁµêÊûúÂº∑Ë™ø‰∫Ü‰∏ÄÂÄãÈáçÈªûÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÂú®ÈÅéÂéª‰∏ÄÂπ¥‰∏≠ÈÄ≤Ê≠•ËøÖÈÄüÔºå‰∏¶‰∏îÊáâÂº∑ÁÉàËÄÉÊÖÆ‰ΩúÁÇ∫Ê©üÂô®‰∫∫ÂïèÈ°åÊú™‰æÜÁöÑÂü∫Á§é„ÄÇ</paragraph>

##### **Using Large Language Models for the Interpretation of Building Regulations**
2407.21060v1 by Stefan Fuchs, Michael Witbrock, Johannes Dimyadi, Robert Amor

Compliance checking is an essential part of a construction project. The
recent rapid uptake of building information models (BIM) in the construction
industry has created more opportunities for automated compliance checking
(ACC). BIM enables sharing of digital building design data that can be used for
compliance checking with legal requirements, which are conventionally conveyed
in natural language and not intended for machine processing. Creating a
computable representation of legal requirements suitable for ACC is complex,
costly, and time-consuming. Large language models (LLMs) such as the generative
pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,
can generate logically coherent text and source code responding to user
prompts. This capability could be used to automate the conversion of building
regulations into a semantic and computable representation. This paper evaluates
the performance of LLMs in translating building regulations into LegalRuleML in
a few-shot learning setup. By providing GPT-3.5 with only a few example
translations, it can learn the basic structure of the format. Using a system
prompt, we further specify the LegalRuleML representation and explore the
existence of expert domain knowledge in the model. Such domain knowledge might
be ingrained in GPT-3.5 through the broad pre-training but needs to be brought
forth by careful contextualisation. Finally, we investigate whether strategies
such as chain-of-thought reasoning and self-consistency could apply to this use
case. As LLMs become more sophisticated, the increased common sense, logical
coherence, and means to domain adaptation can significantly support ACC,
leading to more efficient and effective checking processes.

ÊëòË¶ÅÔºöÂêàË¶èÊ™¢Êü•ÊòØ‰∏ÄÂÄãÂª∫Ë®≠Â∞àÊ°àÁöÑÂøÖË¶ÅÈÉ®ÂàÜ„ÄÇÂª∫ÁØâÁî¢Ê•≠ÊúÄËøëÂø´ÈÄüÊé°Áî®Âª∫ÁØâË≥áË®äÊ®°Âûã (BIM)ÔºåÁÇ∫Ëá™ÂãïÂåñÂêàË¶èÊ™¢Êü• (ACC) ÂâµÈÄ†‰∫ÜÊõ¥Â§öÊ©üÊúÉ„ÄÇBIM ËÉΩÂ§†ÂàÜ‰∫´ÂèØ‰æõÁî®ÊñºËàáÊ≥ïÂæãË¶ÅÊ±ÇÈÄ≤Ë°åÂêàË¶èÊ™¢Êü•ÁöÑÊï∏‰ΩçÂª∫ÁØâË®≠Ë®àË≥áÊñôÔºåËÄåÈÄô‰∫õË¶ÅÊ±ÇÈÄöÂ∏∏‰ª•Ëá™ÁÑ∂Ë™ûË®ÄÂÇ≥ÈÅîÔºå‰∏î‰∏¶ÈùûÁî®ÊñºÊ©üÂô®ËôïÁêÜ„ÄÇÂª∫Á´ã‰∏ÄÂÄãÈÅ©Âêà ACC ÁöÑÊ≥ïÂæãË¶ÅÊ±ÇÂèØË®àÁÆóË°®Á§∫ÈùûÂ∏∏Ë§áÈõú„ÄÅÊòÇË≤¥‰∏îËÄóÊôÇ„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æãÂ¶ÇÁîüÊàêÂºèÈ†êÂÖàË®ìÁ∑¥ËΩâÊèõÂô® (GPT)„ÄÅGPT-3.5 Âíå GPT-4ÔºåÁÇ∫ OpenAI ÁöÑ ChatGPT Êèê‰æõÂãïÂäõÔºåÂèØ‰ª•Áî¢ÁîüÂêà‰πéÈÇèËºØÁöÑÈÄ£Ë≤´ÊñáÂ≠óÂíåÂéüÂßãÁ¢ºÔºå‰ª•ÂõûÊáâ‰ΩøÁî®ËÄÖÁöÑÊèêÁ§∫„ÄÇÊ≠§ÂäüËÉΩÂèØÁî®ÊñºËá™ÂãïÂåñÂ∞áÂª∫ÁØâÊ≥ïË¶èËΩâÊèõÁÇ∫Ë™ûÊÑèÂíåÂèØË®àÁÆóÁöÑË°®Á§∫„ÄÇÊú¨ÊñáË©ï‰º∞‰∫Ü LLM Âú®Â∞áÂª∫ÁØâÊ≥ïË¶èÁøªË≠ØÊàê LegalRuleML ÁöÑË°®ÁèæÔºå‰∏¶Êé°Áî®Â∞ëÊ¨°Â≠∏ÁøíË®≠ÂÆö„ÄÇÈÄèÈÅéÂÉÖÊèê‰æõÂ∞ëÊï∏ÁØÑ‰æãÁøªË≠ØÁµ¶ GPT-3.5ÔºåÂÆÉÂèØ‰ª•Â≠∏ÁøíÊ†ºÂºèÁöÑÂü∫Êú¨ÁµêÊßã„ÄÇ‰ΩøÁî®Á≥ªÁµ±ÊèêÁ§∫ÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÊåáÂÆö LegalRuleML Ë°®Á§∫Ôºå‰∏¶Êé¢Ë®éÊ®°Âûã‰∏≠ÊòØÂê¶Â≠òÂú®Â∞àÂÆ∂È†òÂüüÁü•Ë≠ò„ÄÇÊ≠§È°ûÈ†òÂüüÁü•Ë≠òÂèØËÉΩÈÄèÈÅéÂª£Ê≥õÁöÑÈ†êÂÖàË®ìÁ∑¥Ê§çÂÖ• GPT-3.5Ôºå‰ΩÜÈúÄË¶ÅÈÄèÈÅé‰ªîÁ¥∞ÁöÑËÑàÁµ°ÂåñÊâçËÉΩÁî¢Áîü„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éË´∏Â¶ÇÊÄùËÄÉÈèàÊé®ÁêÜÂíåËá™Êàë‰∏ÄËá¥ÊÄßÁ≠âÁ≠ñÁï•ÊòØÂê¶ÈÅ©Áî®ÊñºÊ≠§Áî®‰æã„ÄÇÈö®Ëëó LLM ËÆäÂæóÊõ¥Á≤æÁ∑ªÔºåÂ∏∏Ë≠ò„ÄÅÈÇèËºØÈÄ£Ë≤´ÊÄßÂíåÈ†òÂüüÈÅ©ÊáâÊñπÊ≥ïÁöÑÂ¢ûÂä†ÂèØ‰ª•È°ØËëóÊîØÊè¥ ACCÔºåÈÄ≤ËÄåÂ∏∂‰æÜÊõ¥ÊúâÊïàÁéá‰∏îÊúâÊïàÁöÑÊ™¢Êü•ÊµÅÁ®ã„ÄÇ

##### **MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**
2407.20284v1 by Shyam Dongre, Ritesh Chandra, Sonali Agarwal

In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåËß£Ê±∫Ê∫ñÁ¢∫ÁñæÁóÖÈ†êÊ∏¨ÂíåÂÄãÊÄßÂåñÂª∫Ë≠∞ÁöÑË§áÈõúÊÄßÊó¢Ëá≥ÈóúÈáçË¶ÅÂèàÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü MLtoGAIÔºåÂÆÉÂ∞áË™ûÁæ©Á∂≤Ë∑ØÊäÄË°ìËàáÊ©üÂô®Â≠∏Áøí (ML) Áõ∏ÁµêÂêàÔºå‰ª•Â¢ûÂº∑ÁñæÁóÖÈ†êÊ∏¨‰∏¶ÈÄèÈÅé ChatGPT Êèê‰æõ‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑË™™Êòé„ÄÇË©≤Á≥ªÁµ±ÂåÖÂê´‰∏âÂÄãÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºö‰∏ÄÂÄãÂèØÈáçË§á‰ΩøÁî®ÁöÑÁñæÁóÖÊú¨‰ΩìÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊúâÈóúÂêÑÁ®ÆÁñæÁóÖÁöÑË©≥Á¥∞Áü•Ë≠òÔºõ‰∏ÄÂÄãË®∫Êñ∑ÂàÜÈ°ûÊ®°ÂûãÔºåÂÆÉ‰ΩøÁî®ÊÇ£ËÄÖÁóáÁãÄ‰æÜÊ∫ñÁ¢∫Ê™¢Ê∏¨ÁâπÂÆöÁñæÁóÖÔºõ‰ª•ÂèäË™ûÁæ©Á∂≤Ë∑ØË¶èÂâáË™ûË®Ä (SWRL) ËàáÊú¨‰ΩìÂíå ChatGPT ÁöÑÊï¥ÂêàÔºå‰ª•Áî¢ÁîüÊ∏ÖÊô∞„ÄÅÂÄãÊÄßÂåñÁöÑÂÅ•Â∫∑Âª∫Ë≠∞„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈ°ØËëóÊèêÈ´ò‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÊòìÊñºÁêÜËß£ÁöÑÁµêÊûúÔºåËß£Ê±∫‰∫ÜÁñæÁóÖÂíå‰∏çÂêåÁóáÁãÄÁöÑË§áÈõúÊÄß„ÄÇMLtoGAI Á≥ªÁµ±Â±ïÁ§∫‰∫ÜÊ∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶ÁöÑÂØ¶Ë≥™ÊÄßÈÄ≤Ê≠•ÔºåÊúâÂä©ÊñºÈñãÁôºÊõ¥Êô∫ÊÖß‰∏îÊõ¥ÊòìÊñºÂèñÂæóÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°à„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÁµêÂêà‰∫Ü ML ÊºîÁÆóÊ≥ïÁöÑÂÑ™ÈªûÔºå‰ª•ÂèäÈÄèÈÅé ChatGPT Êèê‰æõÈÄèÊòé‰∏î‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË™™ÊòéÁöÑËÉΩÂäõÔºåÂú®È†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈÄèÈÅéÂà©Áî®Ë™ûÁæ©ÊäÄË°ìÂíåÂèØËß£ÈáãÁöÑ AIÔºåË©≤Á≥ªÁµ±ÊèêÈ´ò‰∫ÜÁñæÁóÖÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÂª∫Ë≠∞ËàáÂÄãÂà•ÊÇ£ËÄÖÁõ∏Èóú‰∏îÊòìÊñºÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÊï¥ÂêàÂÖàÈÄ≤ÊäÄË°ì‰ª•ÂÖãÊúçÈÜ´ÁôÇË®∫Êñ∑‰∏≠ÁèæÊúâÊåëÊà∞ÁöÑÊΩõÂäõÔºåÁÇ∫Êô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊú™‰æÜÁôºÂ±ïÈã™Ë∑Ø„ÄÇÊ≠§Â§ñÔºåË©≤Á≥ªÁµ±‰ΩøÁî® 200 ÂÄãÂêàÊàêÊÇ£ËÄÖË≥áÊñôË®òÈåÑÈÄ≤Ë°åÈ©óË≠âÔºåÁ¢∫‰øù‰∫ÜÁ©©ÂÅ•ÁöÑÊïàËÉΩÂíåÂèØÈù†ÊÄß„ÄÇ

##### **Is larger always better? Evaluating and prompting large language models for non-generative medical tasks**
2407.18525v1 by Yinghao Zhu, Junyi Gao, Zixiang Wang, Weibin Liao, Xiaochen Zheng, Lifang Liang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Liantao Ma

The use of Large Language Models (LLMs) in medicine is growing, but their
ability to handle both structured Electronic Health Record (EHR) data and
unstructured clinical notes is not well-studied. This study benchmarks various
models, including GPT-based LLMs, BERT-based models, and traditional clinical
predictive models, for non-generative medical tasks utilizing renowned
datasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7
traditional predictive models using the MIMIC dataset (ICU patient records) and
the TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality
and readmission prediction, disease hierarchy reconstruction, and biomedical
sentence matching, comparing both zero-shot and finetuned performance. Results
indicated that LLMs exhibited robust zero-shot predictive capabilities on
structured EHR data when using well-designed prompting strategies, frequently
surpassing traditional models. However, for unstructured medical texts, LLMs
did not outperform finetuned BERT models, which excelled in both supervised and
unsupervised tasks. Consequently, while LLMs are effective for zero-shot
learning on structured data, finetuned BERT models are more suitable for
unstructured texts, underscoring the importance of selecting models based on
specific task requirements and data characteristics to optimize the application
of NLP technology in healthcare.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´Â≠∏‰∏≠ÁöÑÊáâÁî®Êó•ÁõäÂª£Ê≥õÔºå‰ΩÜÂÆÉÂÄëÂêåÊôÇËôïÁêÜÁµêÊßãÂåñÈõªÂ≠êÁóÖÊ≠∑ (EHR) Ë≥áÊñôÂíåÈùûÁµêÊßãÂåñËá®Â∫äË®ªË®òÁöÑËÉΩÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊú¨Á†îÁ©∂ÈáùÂ∞çÂêÑÁ®ÆÊ®°ÂûãÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂåÖÊã¨Âü∫Êñº GPT ÁöÑ LLM„ÄÅÂü∫Êñº BERT ÁöÑÊ®°ÂûãÔºå‰ª•ÂèäÂÇ≥Áµ±ÁöÑËá®Â∫äÈ†êÊ∏¨Ê®°ÂûãÔºåÁî®ÊñºÂà©Áî®ËëóÂêçË≥áÊñôÈõÜÁöÑÈùûÁîüÊàêÊÄßÈÜ´ÁôÇ‰ªªÂãô„ÄÇÊàëÂÄë‰ΩøÁî® MIMIC Ë≥áÊñôÈõÜÔºàICU ÁóÖ‰∫∫Ë®òÈåÑÔºâÂíå TJH Ë≥áÊñôÈõÜÔºàÊó©Êúü COVID-19 EHR Ë≥áÊñôÔºâË©ï‰º∞‰∫Ü 14 ÂÄãË™ûË®ÄÊ®°ÂûãÔºà9 ÂÄãÂü∫Êñº GPTÔºå5 ÂÄãÂü∫Êñº BERTÔºâÂíå 7 ÂÄãÂÇ≥Áµ±È†êÊ∏¨Ê®°ÂûãÔºåÈáçÈªûÈóúÊ≥®Ê≠ª‰∫°ÁéáÂíåÂÜçÂÖ•Èô¢È†êÊ∏¨„ÄÅÁñæÁóÖÂ±§Á¥öÈáçÂª∫ÂíåÁîüÁâ©ÈÜ´Â≠∏Âè•Â≠êÈÖçÂ∞çÁ≠â‰ªªÂãôÔºå‰∏¶ÊØîËºÉ‰∫ÜÈõ∂Ê¨°Â≠∏ÁøíÂíåÂæÆË™øÂæåÁöÑÊïàËÉΩ„ÄÇÁµêÊûúË°®ÊòéÔºåLLM Âú®‰ΩøÁî®Ë®≠Ë®àËâØÂ•ΩÁöÑÊèêÁ§∫Á≠ñÁï•ÊôÇÔºåÂ∞çÁµêÊßãÂåñ EHR Ë≥áÊñôÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÈõ∂Ê¨°Â≠∏ÁøíÈ†êÊ∏¨ËÉΩÂäõÔºåÁ∂ìÂ∏∏Ë∂ÖË∂äÂÇ≥Áµ±Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÈùûÁµêÊßãÂåñÁöÑÈÜ´ÁôÇÊñáÊú¨ÔºåLLM ÁöÑË°®Áèæ‰∏çÂ¶ÇÂæÆË™øÂæåÁöÑ BERT Ê®°ÂûãÔºåÂæåËÄÖÂú®Áõ£Áù£ÂºèÂíåÈùûÁõ£Áù£Âºè‰ªªÂãô‰∏≠ÈÉΩË°®ÁèæÂá∫Ëâ≤„ÄÇÂõ†Ê≠§ÔºåÂÑòÁÆ° LLM Â∞çÊñºÁµêÊßãÂåñË≥áÊñôÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂæàÊúâÁî®Ôºå‰ΩÜÂæÆË™øÂæåÁöÑ BERT Ê®°ÂûãÊõ¥ÈÅ©ÂêàÈùûÁµêÊßãÂåñÊñáÊú¨ÔºåÈÄôÂº∑Ë™ø‰∫ÜÊ†πÊìöÁâπÂÆö‰ªªÂãôÈúÄÊ±ÇÂíåË≥áÊñôÁâπÊÄßÈÅ∏ÊìáÊ®°Âûã‰ª•ÂÑ™ÂåñÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ NLP ÊäÄË°ìÊáâÁî®‰πãÈáçË¶ÅÊÄß„ÄÇ

##### **A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation**
2407.18483v4 by Laiyi Fu, Binbin Fan, Hongkai Du, Yanxiang Feng, Chunhua Li, Huping Song

Ophthalmology consultations are crucial for diagnosing, treating, and
preventing eye diseases. However, the growing demand for consultations exceeds
the availability of ophthalmologists. By leveraging large pre-trained language
models, we can design effective dialogues for specific scenarios, aiding in
consultations. Traditional fine-tuning strategies for question-answering tasks
are impractical due to increasing model size and often ignoring patient-doctor
role function during consultations. In this paper, we propose EyeDoctor, an
ophthalmic medical questioning large language model that enhances accuracy
through doctor-patient role perception guided and an augmented knowledge base
with external disease information. Experimental results show EyeDoctor achieves
higher question-answering precision in ophthalmology consultations. Notably,
EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%
improvement in F1 scores on multi-round datasets compared to second best model
ChatGPT, highlighting the importance of doctor-patient role differentiation and
dynamic knowledge base expansion for intelligent medical consultations. EyeDoc
also serves as a free available web based service and souce code is available
at https://github.com/sperfu/EyeDoc.

ÊëòË¶ÅÔºöÁúºÁßëË´ÆË©¢Â∞çÊñºË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂíåÈ†êÈò≤ÁúºÁñæËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåË´ÆË©¢ÈúÄÊ±ÇÁöÑÂ¢ûÂä†Ë∂ÖÈÅé‰∫ÜÁúºÁßëÈÜ´ÁîüÁöÑ‰æõÊáâ„ÄÇÈÄèÈÅéÂà©Áî®Â§ßÂûãÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºåÊàëÂÄëÂèØ‰ª•ÁÇ∫ÁâπÂÆöÂ†¥ÊôØË®≠Ë®àÊúâÊïàÁöÑÂ∞çË©±ÔºåÂçîÂä©Ë´ÆË©¢„ÄÇÂÇ≥Áµ±ÁöÑÂæÆË™øÁ≠ñÁï•Â∞çÊñºÂïèÁ≠î‰ªªÂãô‰æÜË™™ÊòØ‰∏çÂàáÂØ¶ÈöõÁöÑÔºåÂõ†ÁÇ∫Ê®°ÂûãÂ§ßÂ∞èÁöÑÂ¢ûÂä†ÔºåËÄå‰∏îÂú®Ë´ÆË©¢ÊúüÈñìÂ∏∏Â∏∏ÂøΩÁï•ÊÇ£ËÄÖÂíåÈÜ´ÁîüÁöÑËßíËâ≤ÂäüËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ EyeDoctorÔºåÈÄôÊòØ‰∏ÄÂÄãÁúºÁßëÈÜ´ÁôÇÂïèÁ≠îÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÈÄèÈÅéÈÜ´ÁîüÂíåÊÇ£ËÄÖËßíËâ≤ÊÑüÁü•ÊåáÂ∞éÂíå‰∏ÄÂÄãÊì¥ÂÖÖÁöÑÂ§ñÈÉ®ÁñæÁóÖË≥áË®äÁü•Ë≠òÂ∫´‰æÜÂ¢ûÂº∑Ê∫ñÁ¢∫ÊÄß„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåEyeDoctor Âú®ÁúºÁßëË´ÆË©¢‰∏≠ÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÂïèÁ≠îÊ∫ñÁ¢∫Â∫¶„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåËàáÁ¨¨‰∫åÂ•ΩÁöÑÊ®°Âûã ChatGPT Áõ∏ÊØîÔºåEyeDoctor Âú®Â§öËº™Êï∏ÊìöÈõÜ‰∏ä Rouge-1 ÂàÜÊï∏ÊèêÈ´ò‰∫Ü 7.25%ÔºåF1 ÂàÜÊï∏ÊèêÈ´ò‰∫Ü 10.16%ÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÈÜ´ÁîüÂíåÊÇ£ËÄÖËßíËâ≤ÂçÄÂàÜÂíåÂãïÊÖãÁü•Ë≠òÂ∫´Êì¥ÂÖÖÂ∞çÊñºÊô∫ËÉΩÈÜ´ÁôÇË´ÆË©¢ÁöÑÈáçË¶ÅÊÄß„ÄÇEyeDoc ‰πü‰ΩúÁÇ∫‰∏ÄÂÄãÂÖçË≤ªÁöÑÁ∂≤Ë∑ØÊúçÂãôÔºåÂéüÂßãÁ¢ºÂèØ‰ª•Âú® https://github.com/sperfu/EyeDoc ÂèñÂæó„ÄÇ

##### **Towards Automated Solution Recipe Generation for Industrial Asset Management with LLM**
2407.18992v1 by Nianjun Zhou, Dhaval Patel, Shuxin Lin, Fearghal O'Donncha

This study introduces a novel approach to Industrial Asset Management (IAM)
by incorporating Conditional-Based Management (CBM) principles with the latest
advancements in Large Language Models (LLMs). Our research introduces an
automated model-building process, traditionally reliant on intensive
collaboration between data scientists and domain experts. We present two
primary innovations: a taxonomy-guided prompting generation that facilitates
the automatic creation of AI solution recipes and a set of LLM pipelines
designed to produce a solution recipe containing a set of artifacts composed of
documents, sample data, and models for IAM. These pipelines, guided by
standardized principles, enable the generation of initial solution templates
for heterogeneous asset classes without direct human input, reducing reliance
on extensive domain knowledge and enhancing automation. We evaluate our
methodology by assessing asset health and sustainability across a spectrum of
ten asset classes. Our findings illustrate the potential of LLMs and
taxonomy-based LLM prompting pipelines in transforming asset management,
offering a blueprint for subsequent research and development initiatives to be
integrated into a rapid client solution.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∑•Ê•≠Ë≥áÁî¢ÁÆ°ÁêÜ (IAM) ÊñπÊ≥ïÔºåÊñπÊ≥ïÊòØÂ∞áÂü∫ÊñºÊ¢ù‰ª∂ÁöÑÁÆ°ÁêÜ (CBM) ÂéüÂâáËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÁõ∏ÁµêÂêà„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñÊ®°ÂûãÂª∫ÊßãÊµÅÁ®ãÔºåÂÇ≥Áµ±‰∏ä‰æùË≥¥ÊñºÊï∏ÊìöÁßëÂ≠∏ÂÆ∂ÂíåÈ†òÂüüÂ∞àÂÆ∂‰πãÈñìÁöÑÂØÜÈõÜÂêà‰Ωú„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ©È†Ö‰∏ªË¶ÅÁöÑÂâµÊñ∞Ôºö‰∏ÄÁ®ÆÂàÜÈ°ûÂºïÂ∞éÁöÑÊèêÁ§∫ÁîüÊàêÔºåÂÆÉ‰øÉÈÄ≤‰∫Ü AI Ëß£Ê±∫ÊñπÊ°àÈÖçÊñπÔºàrecipeÔºâÁöÑËá™ÂãïÂâµÂª∫Ôºå‰ª•Âèä‰∏ÄÁµÑ LLM ÁÆ°ÈÅìÔºåÊó®Âú®Áî¢Áîü‰∏ÄÂÄãËß£Ê±∫ÊñπÊ°àÈÖçÊñπÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÁµÑÁî±Êñá‰ª∂„ÄÅÁØÑ‰æãË≥áÊñôÂíå IAM Ê®°ÂûãÁµÑÊàêÁöÑÊàêÂìÅ„ÄÇÈÄô‰∫õÁÆ°ÈÅìÂú®Ê®ôÊ∫ñÂåñÂéüÂâáÁöÑÊåáÂ∞é‰∏ãÔºåËÉΩÂ§†ÁÇ∫Áï∞Ë≥™Ë≥áÁî¢È°ûÂà•Áî¢ÁîüÂàùÂßãËß£Ê±∫ÊñπÊ°àÁØÑÊú¨ÔºåÁÑ°ÈúÄÁõ¥Êé•ÁöÑ‰∫∫Â∑•Ëº∏ÂÖ•ÔºåÂæûËÄåÊ∏õÂ∞ëÂ∞çÂª£Ê≥õÈ†òÂüüÁü•Ë≠òÁöÑ‰æùË≥¥‰∏¶Â¢ûÂº∑Ëá™ÂãïÂåñ„ÄÇÊàëÂÄëÈÄöÈÅéË©ï‰º∞ÂçÅÂÄãË≥áÁî¢È°ûÂà•ÁöÑË≥áÁî¢ÂÅ•Â∫∑ÁãÄÊ≥ÅÂíåÊ∞∏Á∫åÊÄß‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊäÄË°ì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË™™Êòé‰∫Ü LLM ÂíåÂü∫ÊñºÂàÜÈ°ûÁöÑ LLM ÊèêÁ§∫ÁÆ°Á∑öÂú®ËΩâÂûãË≥áÁî¢ÁÆ°ÁêÜÊñπÈù¢ÁöÑÊΩõÂäõÔºåÁÇ∫ÂæåÁ∫åÁöÑÁ†îÁ©∂ÂíåÈñãÁôºË®àÁï´Êèê‰æõ‰∫ÜËóçÂúñÔºåÈÄô‰∫õË®àÁï´Â∞áÊï¥ÂêàÂà∞Âø´ÈÄüÂÆ¢Êà∂Ëß£Ê±∫ÊñπÊ°à‰∏≠„ÄÇ

##### **HDL-GPT: High-Quality HDL is All You Need**
2407.18423v1 by Bhuvnesh Kumar, Saurav Nanda, Ganapathy Parthasarathy, Pawan Patil, Austin Tsai, Parivesh Choudhary

This paper presents Hardware Description Language Generative Pre-trained
Transformers (HDL-GPT), a novel approach that leverages the vast repository of
open-source High Definition Language (HDL) codes to train superior quality
large code models. The core premise of this paper is the hypothesis that
high-quality HDL is all you need to create models with exceptional performance
and broad zero-shot generalization abilities. The paper elucidates the methods
employed for the curation and augmentation of large corpora from open-source
HDL code, transforming highly variable quality data into high-quality data
through careful prompting and context maintenance. We demonstrate that the
careful selection, filtering, and augmentation of data across HDLs can yield
powerful models that surpass current state-of-the-art models. We also explore
the impact of different fine-tuning methods on the quality of results. We
describe experimental results across a range of fine-tuned SOTA LLMs,
substantiating our claims. We demonstrate improvements of 50% to 200% over SOTA
HDL models on current benchmarks in tasks ranging from HDL circuit
explanations, code generation, formal and simulation testbench creation,
triaging bugs, and fixing them. HDL-GPT opens new avenues for the development
of advanced model training techniques for circuit design tasks.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫Á°¨È´îÊèèËø∞Ë™ûË®ÄÁîüÊàêÂºèÈ†êË®ìÁ∑¥ËΩâÊèõÂô® (HDL-GPT)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂà©Áî®Â§ßÈáèÈñãÊ∫êÈ´òÂÆöÁæ©Ë™ûË®Ä (HDL) Á®ãÂºèÁ¢º‰æÜË®ìÁ∑¥ÂÑ™Ë≥™ÁöÑÂ§ßÂûãÁ®ãÂºèÁ¢ºÊ®°Âûã„ÄÇÊú¨ÊñáÁöÑÊ†∏ÂøÉÂâçÊèêÊòØÈ´òÂìÅË≥™ÁöÑ HDL ÊòØÂª∫Á´ãÂÖ∑ÊúâÂçìË∂äÊïàËÉΩÂíåÂª£Ê≥õÈõ∂Ê¨°Â≠∏ÁøíÊ¶ÇÂåñËÉΩÂäõÊ®°ÂûãÁöÑÂîØ‰∏ÄË¶ÅÁ¥†„ÄÇÊú¨ÊñáÈó°Êòé‰∫ÜÂæûÈñãÊ∫ê HDL Á®ãÂºèÁ¢ºÁ≠ñÂ±ïÂíåÊì¥ÂÖÖÂ§ßÂûãË™ûÊñôÂ∫´ÊâÄ‰ΩøÁî®ÁöÑÊñπÊ≥ïÔºåÈÄèÈÅé‰ªîÁ¥∞ÊèêÁ§∫ÂíåËÑàÁµ°Á∂≠Ë≠∑ÔºåÂ∞áÂìÅË≥™È´òÂ∫¶ËÆäÁï∞ÁöÑË≥áÊñôËΩâÊèõÊàêÈ´òÂìÅË≥™Ë≥áÊñô„ÄÇÊàëÂÄëË≠âÊòé‰∫Ü‰ªîÁ¥∞ÈÅ∏Êìá„ÄÅÁØ©ÈÅ∏ÂíåÊì¥ÂÖÖ HDL ‰∏≠ÁöÑË≥áÊñôÂèØ‰ª•Áî¢ÁîüÂº∑Â§ßÁöÑÊ®°ÂûãÔºåË∂ÖË∂äÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Ê®°Âûã„ÄÇÊàëÂÄë‰πüÊé¢Ë®é‰∫Ü‰∏çÂêåÂæÆË™øÊñπÊ≥ïÂ∞çÁµêÊûúÂìÅË≥™ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊèèËø∞‰∫ÜÈáùÂ∞ç‰∏ÄÁ≥ªÂàóÂæÆË™øÈÅéÁöÑ SOTA LLM ÁöÑÂØ¶È©óÁµêÊûúÔºå‰ª•Ë≠âÂØ¶ÊàëÂÄëÁöÑË™™Ê≥ï„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÂú®Âæû HDL ÈõªË∑ØË™™Êòé„ÄÅÁ®ãÂºèÁ¢ºÁî¢Áîü„ÄÅÊ≠£ÂºèÂíåÊ®°Êì¨Ê∏¨Ë©¶Âπ≥Âè∞Âª∫Á´ã„ÄÅÂàÜÈ°ûÈåØË™§Âà∞‰øÆÊ≠£ÈåØË™§Á≠â‰ªªÂãôÁöÑÁèæÊúâÂü∫Ê∫ñ‰∏≠ÔºåHDL-GPT ÊØî SOTA HDL Ê®°ÂûãÈÄ≤Ê≠•‰∫Ü 50% Ëá≥ 200%„ÄÇHDL-GPT ÁÇ∫ÈõªË∑ØË®≠Ë®à‰ªªÂãôÁöÑÈÄ≤ÈöéÊ®°ÂûãË®ìÁ∑¥ÊäÄË°ìÈñãÁôºÈñãÂïü‰∫ÜÊñ∞ÈÄîÂæë„ÄÇ

##### **SCALE: Self-regulated Clustered federAted LEarning in a Homogeneous Environment**
2407.18387v1 by Sai Puppala, Ismail Hossain, Md Jahangir Alam, Sajedul Talukder, Zahidur Talukder, Syed Bahauddin

Federated Learning (FL) has emerged as a transformative approach for enabling
distributed machine learning while preserving user privacy, yet it faces
challenges like communication inefficiencies and reliance on centralized
infrastructures, leading to increased latency and costs. This paper presents a
novel FL methodology that overcomes these limitations by eliminating the
dependency on edge servers, employing a server-assisted Proximity Evaluation
for dynamic cluster formation based on data similarity, performance indices,
and geographical proximity. Our integrated approach enhances operational
efficiency and scalability through a Hybrid Decentralized Aggregation Protocol,
which merges local model training with peer-to-peer weight exchange and a
centralized final aggregation managed by a dynamically elected driver node,
significantly curtailing global communication overhead. Additionally, the
methodology includes Decentralized Driver Selection, Check-pointing to reduce
network traffic, and a Health Status Verification Mechanism for system
robustness. Validated using the breast cancer dataset, our architecture not
only demonstrates a nearly tenfold reduction in communication overhead but also
shows remarkable improvements in reducing training latency and energy
consumption while maintaining high learning performance, offering a scalable,
efficient, and privacy-preserving solution for the future of federated learning
ecosystems.

ÊëòË¶ÅÔºöËÅØÈÇ¶Â≠∏Áøí (FL) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆËÆäÈù©ÊÄßÊñπÊ≥ïÔºåÁî®ÊñºÂú®‰øùË≠∑‰ΩøÁî®ËÄÖÈö±ÁßÅÁöÑÂêåÊôÇÂïüÁî®ÂàÜÊï£ÂºèÊ©üÂô®Â≠∏ÁøíÔºå‰ΩÜÂÆÉÈù¢Ëá®ËëóË´∏Â¶ÇÈÄöË®äÊïàÁéá‰ΩéÂíå‰æùË≥¥ÊñºÈõÜ‰∏≠ÂºèÂü∫Á§éË®≠ÊñΩÁ≠âÊåëÊà∞ÔºåÂ∞éËá¥Âª∂ÈÅ≤ÂíåÊàêÊú¨Â¢ûÂä†„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ FL ÊñπÊ≥ïÔºåÈÄöÈÅéÊ∂àÈô§Â∞çÈÇäÁ∑£‰º∫ÊúçÂô®ÁöÑ‰æùË≥¥ÔºåÊé°Áî®‰º∫ÊúçÂô®ËºîÂä©ÁöÑÊé•ËøëÂ∫¶Ë©ï‰º∞‰æÜÊ†πÊìöË≥áÊñôÁõ∏‰ººÊÄß„ÄÅÊïàËÉΩÊåáÊ®ôÂíåÂú∞ÁêÜÊé•ËøëÂ∫¶ÈÄ≤Ë°åÂãïÊÖãÂè¢ÈõÜÂΩ¢ÊàêÔºåÂæûËÄåÂÖãÊúç‰∫ÜÈÄô‰∫õÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊï¥ÂêàÊñπÊ≥ïÈÄèÈÅéÊ∑∑ÂêàÂºèÂàÜÊï£ÂºèËÅöÂêàÂçîÂÆö‰æÜÂ¢ûÂº∑ÈÅã‰ΩúÊïàÁéáÂíåÂèØÊì¥ÂÖÖÊÄßÔºåË©≤ÂçîÂÆöÂ∞áÊú¨Âú∞Ê®°ÂûãË®ìÁ∑¥ËàáÈªûÂ∞çÈªûÊ¨äÈáç‰∫§Êèõ‰ª•ÂèäÁî±ÂãïÊÖãÈÅ∏Âá∫ÁöÑÈ©ÖÂãïÁ®ãÂºèÁØÄÈªûÁÆ°ÁêÜÁöÑÈõÜ‰∏≠ÂºèÊúÄÁµÇËÅöÂêàÂêà‰ΩµÂú®‰∏ÄËµ∑ÔºåÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜÊï¥È´îÈÄöË®äÈñãÈä∑„ÄÇÊ≠§Â§ñÔºåË©≤ÊñπÊ≥ïÂåÖÊã¨ÂàÜÊï£ÂºèÈ©ÖÂãïÁ®ãÂºèÈÅ∏Êìá„ÄÅÊ™¢Êü•Èªû‰ª•Ê∏õÂ∞ëÁ∂≤Ë∑ØÊµÅÈáèÔºå‰ª•ÂèäÁî®ÊñºÁ≥ªÁµ±Á©©ÂÅ•ÊÄßÁöÑÂÅ•Â∫∑ÁãÄÊÖãÈ©óË≠âÊ©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂Êßã‰ΩøÁî®‰π≥ÁôåË≥áÊñôÈõÜÈÄ≤Ë°åÈ©óË≠âÔºå‰∏çÂÉÖË≠âÊòéÈÄöË®äÈñãÈä∑Ê∏õÂ∞ë‰∫ÜËøëÂçÅÂÄçÔºåËÄå‰∏îÈÇÑÈ°ØÁ§∫Âá∫Âú®Èôç‰ΩéË®ìÁ∑¥Âª∂ÈÅ≤ÂíåËÉΩÊ∫êÊ∂àËÄóÁöÑÂêåÊôÇÔºåÂ≠∏ÁøíÊïàËÉΩ‰ªç‰øùÊåÅÂæàÈ´òÁöÑÈ°ØËëóÊîπÈÄ≤ÔºåÁÇ∫ËÅØÈÇ¶Â≠∏ÁøíÁîüÊÖãÁ≥ªÁµ±ÁöÑÊú™‰æÜÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÊì¥ÂÖÖÊÄß„ÄÅÈ´òÊïà‰∏î‰øùË≠∑Èö±ÁßÅÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**
2407.18343v2 by Alessandro De Carlo, Enea Parimbelli, Nicola Melillo, Giovanna Nicora

Explainable Artificial Intelligence (XAI) is central to the debate on
integrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms
into clinical practice. High-performing AI/ML models, such as ensemble learners
and deep neural networks, often lack interpretability, hampering clinicians'
trust in their predictions. To address this, XAI techniques are being developed
to describe AI/ML predictions in human-understandable terms. One promising
direction is the adaptation of sensitivity analysis (SA) and global sensitivity
analysis (GSA), which inherently rank model inputs by their impact on
predictions. Here, we introduce a novel delta-XAI method that provides local
explanations of ML model predictions by extending the delta index, a GSA
metric. The delta-XAI index assesses the impact of each feature's value on the
predicted output for individual instances in both regression and classification
problems. We formalize the delta-XAI index and provide code for its
implementation. The delta-XAI method was evaluated on simulated scenarios using
linear regression models, with Shapley values serving as a benchmark. Results
showed that the delta-XAI index is generally consistent with Shapley values,
with notable discrepancies in models with highly impactful or extreme feature
values. The delta-XAI index demonstrated higher sensitivity in detecting
dominant features and handling extreme feature values. Qualitatively, the
delta-XAI provides intuitive explanations by leveraging probability density
functions, making feature rankings clearer and more explainable for
practitioners. Overall, the delta-XAI method appears promising for robustly
obtaining local explanations of ML model predictions. Further investigations in
real-world clinical settings will be conducted to evaluate its impact on
AI-assisted clinical workflows.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÂ∞á‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊºîÁÆóÊ≥ïÊï¥ÂêàÂà∞Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑËæØË´ñÊ†∏ÂøÉ„ÄÇÈ´òÂü∑Ë°åÊïàËÉΩÁöÑ AI/ML Ê®°ÂûãÔºå‰æãÂ¶ÇÊï¥È´îÂ≠∏ÁøíÂô®ÂíåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºåÈòªÁ§ôËá®Â∫äÈÜ´ÁîüÂ∞çÂÖ∂È†êÊ∏¨ÁöÑ‰ø°‰ªª„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊ≠£Âú®ÈñãÁôº XAI ÊäÄË°ìÔºå‰ª•‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË°ìË™ûÊèèËø∞ AI/ML È†êÊ∏¨„ÄÇ‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊñπÂêëÊòØÊé°Áî®ÊïèÊÑüÂ∫¶ÂàÜÊûê (SA) ÂíåÂÖ®ÁêÉÊïèÊÑüÂ∫¶ÂàÜÊûê (GSA)ÔºåÂÆÉÂÄëÊú¨Ë≥™‰∏äÊúÉ‰æùÊìöÊ®°ÂûãËº∏ÂÖ•Â∞çÈ†êÊ∏¨ÁöÑÂΩ±Èüø‰æÜÂ∞çÂÖ∂ÈÄ≤Ë°åÊéíÂêç„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞ÁöÑ delta-XAI ÊñπÊ≥ïÔºåÈÄèÈÅéÊì¥ÂÖÖ GSA ÊåáÊ®ô delta ÊåáÊï∏‰æÜÊèê‰æõ ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã„ÄÇdelta-XAI ÊåáÊï∏Ë©ï‰º∞ÊØèÂÄãÁâπÂæµÂÄºÂ∞çÂõûÊ≠∏ÂíåÂàÜÈ°ûÂïèÈ°å‰∏≠ÂÄãÂà•‰æãÈ†ÖÁöÑÈ†êÊ∏¨Ëº∏Âá∫‰πãÂΩ±Èüø„ÄÇÊàëÂÄëÂ∞á delta-XAI ÊåáÊï∏ÂΩ¢ÂºèÂåñÔºå‰∏¶Êèê‰æõÂÖ∂ÂØ¶‰ΩúÁöÑÁ®ãÂºèÁ¢º„ÄÇ‰ΩøÁî®Á∑öÊÄßÂõûÊ≠∏Ê®°ÂûãÂ∞çÊ®°Êì¨ÊÉÖÂ¢ÉË©ï‰º∞ delta-XAI ÊñπÊ≥ïÔºå‰∏¶‰ª• Shapley ÂÄº‰ΩúÁÇ∫Âü∫Ê∫ñ„ÄÇÁµêÊûúÈ°ØÁ§∫ delta-XAI ÊåáÊï∏ÈÄöÂ∏∏Ëàá Shapley ÂÄº‰∏ÄËá¥Ôºå‰ΩÜÂú®ÂÖ∑ÊúâÈ´òÂ∫¶ÂΩ±ÈüøÂäõÊàñÊ•µÁ´ØÁâπÂæµÂÄºÁöÑÊ®°Âûã‰∏≠Â≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇdelta-XAI ÊåáÊï∏Âú®ÂÅµÊ∏¨‰∏ªË¶ÅÁâπÂæµÂíåËôïÁêÜÊ•µÁ´ØÁâπÂæµÂÄºÊñπÈù¢Ë°®ÁèæÂá∫Êõ¥È´òÁöÑÊïèÊÑüÂ∫¶„ÄÇÂÆöÊÄßÂú∞‰æÜË™™Ôºådelta-XAI ÈÄèÈÅéÂà©Áî®Ê©üÁéáÂØÜÂ∫¶ÂáΩÊï∏Êèê‰æõÁõ¥ËßÄÁöÑËß£ÈáãÔºå‰ΩøÁâπÂæµÊéíÂêçÊõ¥Ê∏ÖÊô∞‰∏îÂ∞çÂæûÊ•≠‰∫∫Âì°‰æÜË™™Êõ¥ÂÖ∑ÂèØËß£ÈáãÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºådelta-XAI ÊñπÊ≥ïÂ∞çÊñºÁ©©ÂÅ•Âú∞ÂèñÂæó ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã‰ºº‰πéÂæàÊúâÂ∏åÊúõ„ÄÇÂ∞áÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•Ë™øÊü•Ôºå‰ª•Ë©ï‰º∞ÂÖ∂Â∞ç AI ËºîÂä©Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂΩ±Èüø„ÄÇ

##### **Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images**
2407.18125v1 by Roberto Di Via, Francesca Odone, Vito Paolo Pastore

In the last few years, deep neural networks have been extensively applied in
the medical domain for different tasks, ranging from image classification and
segmentation to landmark detection. However, the application of these
technologies in the medical domain is often hindered by data scarcity, both in
terms of available annotations and images. This study introduces a new
self-supervised pre-training protocol based on diffusion models for landmark
detection in x-ray images. Our results show that the proposed self-supervised
framework can provide accurate landmark detection with a minimal number of
available annotated training images (up to 50), outperforming ImageNet
supervised pre-training and state-of-the-art self-supervised pre-trainings for
three popular x-ray benchmark datasets. To our knowledge, this is the first
exploration of diffusion models for self-supervised learning in landmark
detection, which may offer a valuable pre-training approach in few-shot
regimes, for mitigating data scarcity.

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥‰∏≠ÔºåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÂ∑≤Âª£Ê≥õÊáâÁî®ÊñºÈÜ´ÁôÇÈ†òÂüüÁöÑ‰∏çÂêå‰ªªÂãôÔºåÂæûÂΩ±ÂÉèÂàÜÈ°ûÂíåÂàÜÂâ≤Âà∞Âú∞Ê®ôÂÅµÊ∏¨„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊäÄË°ìÂú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÊáâÁî®Â∏∏Â∏∏ÂèóÂà∞Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÈòªÁ§ôÔºåÁÑ°Ë´ñÊòØÂú®ÂèØÁî®ÁöÑË®ªËß£ÊàñÂΩ±ÂÉèÊñπÈù¢„ÄÇÊú¨Á†îÁ©∂‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑËá™Áõ£Áù£È†êË®ìÁ∑¥ÂçîÂÆöÔºåÂÆÉÊòØÂü∫ÊñºÊì¥Êï£Ê®°ÂûãÔºåÁî®Êñº X ÂÖâÂΩ±ÂÉè‰∏≠ÁöÑÂú∞Ê®ôÂÅµÊ∏¨„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑËá™Áõ£Áù£Êû∂ÊßãÂèØ‰ª•Âú®ÊúÄÂ∞ëÊï∏ÈáèÁöÑÂèØÁî®Ë®ªËß£Ë®ìÁ∑¥ÂΩ±ÂÉèÔºàÊúÄÂ§ö 50 ÂÄãÔºâ‰∏ãÊèê‰æõÊ∫ñÁ¢∫ÁöÑÂú∞Ê®ôÂÅµÊ∏¨ÔºåÂÑ™Êñº ImageNet Áõ£Áù£ÂºèÈ†êË®ìÁ∑¥‰ª•Âèä‰∏âÂÄãÁÜ±ÈñÄ X ÂÖâÂü∫Ê∫ñË≥áÊñôÈõÜÁöÑÊúÄÊñ∞Ëá™Áõ£Áù£ÂºèÈ†êË®ìÁ∑¥„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÈ¶ñÊ¨°Êé¢Ë®éÊì¥Êï£Ê®°ÂûãÁî®ÊñºÂú∞Ê®ôÂÅµÊ∏¨‰∏≠ÁöÑËá™Áõ£Áù£ÂºèÂ≠∏ÁøíÔºåÂÆÉÂèØËÉΩÂú®Â∞èÊ®£Êú¨Ë®ìÁ∑¥Ê®°Âºè‰∏≠Êèê‰æõÊúâÂÉπÂÄºÁöÑÈ†êË®ìÁ∑¥ÊñπÊ≥ïÔºå‰ª•Ê∏õËºïË≥áÊñôÁ®ÄÂ∞ëÁöÑÂïèÈ°å„ÄÇ

##### **Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer Subtyping**
2407.18105v1 by Jack Breen, Katie Allen, Kieran Zucker, Nicolas M. Orsi, Nishant Ravikumar

Computer vision models are increasingly capable of classifying ovarian
epithelial cancer subtypes, but they differ from pathologists by processing
small tissue patches at a single resolution. Multi-resolution graph models
leverage the spatial relationships of patches at multiple magnifications,
learning the context for each patch. In this study, we conduct the most
thorough validation of a graph model for ovarian cancer subtyping to date.
Seven models were tuned and trained using five-fold cross-validation on a set
of 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching
Hospitals NHS Trust. The cross-validation models were ensembled and evaluated
using a balanced hold-out test set of 100 WSIs from 30 patients, and an
external validation set of 80 WSIs from 80 patients in the Transcanadian Study.
The best-performing model, a graph model using 10x+20x magnification data, gave
balanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing,
and external validation, respectively. However, this only exceeded the
performance of attention-based multiple instance learning in external
validation, with a 93% balanced accuracy. Graph models benefitted greatly from
using the UNI foundation model rather than an ImageNet-pretrained ResNet50 for
feature extraction, with this having a much greater effect on performance than
changing the subsequent classification approach. The accuracy of the combined
foundation model and multi-resolution graph network offers a step towards the
clinical applicability of these models, with a new highest-reported performance
for this task, though further validations are still required to ensure the
robustness and usability of the models.

ÊëòË¶ÅÔºöÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãË∂ä‰æÜË∂äËÉΩÂ§†ÂàÜÈ°ûÂçµÂ∑¢‰∏äÁöÆÁôåÁöÑ‰∫ûÂûãÔºå‰ΩÜÂÆÉÂÄëËàáÁóÖÁêÜÂ≠∏ÂÆ∂‰∏çÂêåÔºåÂÆÉÂÄë‰ª•ÂñÆ‰∏ÄËß£ÊûêÂ∫¶ËôïÁêÜÂ∞èÁµÑÁπîË≤ºÁâá„ÄÇÂ§öËß£ÊûêÂ∫¶ÂúñÂΩ¢Ê®°ÂûãÂà©Áî®Â§öÂÄãÊîæÂ§ßÂÄçÁéá‰∏ãË≤ºÁâáÁöÑÁ©∫ÈñìÈóú‰øÇÔºåÂ≠∏ÁøíÊØèÂÄãË≤ºÁâáÁöÑËÉåÊôØ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÂúñÂΩ¢Ê®°ÂûãÈÄ≤Ë°å‰∫ÜËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂæπÂ∫ïÁöÑÂçµÂ∑¢Áôå‰∫ûÂûãÈ©óË≠â„ÄÇ‰ΩøÁî® 434 ÂêçÂú®Âà©Ëå≤ÊïôÂ≠∏ÈÜ´Èô¢ NHS ‰ø°Ë®óÂü∫ÈáëÊé•ÂèóÊ≤ªÁôÇÁöÑÊÇ£ËÄÖÁöÑ 1864 ÂºµÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉè (WSI) ÈÄ≤Ë°å‰∫îÂÄç‰∫§ÂèâÈ©óË≠âÔºåË™øÊï¥‰∏¶Ë®ìÁ∑¥‰∫Ü‰∏ÉÂÄãÊ®°Âûã„ÄÇÂ∞á‰∫§ÂèâÈ©óË≠âÊ®°ÂûãÈõÜÊàê‰∏¶‰ΩøÁî®‰æÜËá™ 30 ÂêçÊÇ£ËÄÖÁöÑ 100 Âºµ WSI ÁöÑÂπ≥Ë°°ÁïôÂá∫Ê∏¨Ë©¶ÈõÜÂíå‰æÜËá™ Transcanadian Á†îÁ©∂‰∏≠ 80 ÂêçÊÇ£ËÄÖÁöÑ 80 Âºµ WSI ÁöÑÂ§ñÈÉ®È©óË≠âÈõÜÈÄ≤Ë°åË©ï‰º∞„ÄÇË°®ÁèæÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÔºå‰∏ÄÂÄã‰ΩøÁî® 10 ÂÄç+20 ÂÄçÊîæÂ§ßÂÄçÁéáË≥áÊñôÁöÑÂúñÂΩ¢Ê®°ÂûãÔºåÂú®‰∫§ÂèâÈ©óË≠â„ÄÅÁïôÂá∫Ê∏¨Ë©¶ÂíåÂ§ñÈÉ®È©óË≠â‰∏≠ÂàÜÂà•Áµ¶Âá∫ 73%„ÄÅ88% Âíå 99% ÁöÑÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÉÖË∂ÖÈÅé‰∫ÜÂ§ñÈÉ®È©óË≠â‰∏≠Âü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂ§öÂØ¶‰æãÂ≠∏ÁøíÁöÑË°®ÁèæÔºåÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶ÁÇ∫ 93%„ÄÇÂúñÂΩ¢Ê®°ÂûãÂæû‰ΩøÁî® UNI Âü∫Á§éÊ®°ÂûãËÄå‰∏çÊòØ ImageNet È†êË®ìÁ∑¥ÁöÑ ResNet50 ÈÄ≤Ë°åÁâπÂæµÊèêÂèñ‰∏≠ÂèóÁõäÂå™Ê∑∫ÔºåËàáÊîπËÆäÂæåÁ∫åÂàÜÈ°ûÊñπÊ≥ïÁõ∏ÊØîÔºåÈÄôÂ∞çÊïàËÉΩÊúâÊõ¥Â§ßÁöÑÂΩ±Èüø„ÄÇÁµêÂêàÂü∫Á§éÊ®°ÂûãÂíåÂ§öËß£ÊûêÂ∫¶ÂúñÂΩ¢Á∂≤Ë∑ØÁöÑÊ∫ñÁ¢∫Â∫¶ÁÇ∫ÈÄô‰∫õÊ®°ÂûãÁöÑËá®Â∫äÊáâÁî®ÈÇÅÂá∫‰∫Ü‰∏ÄÊ≠•ÔºåÂ∞çÊñºÈÄôÈ†Ö‰ªªÂãô‰æÜË™™ÔºåÈÄôÊòØÊñ∞ÁöÑÊúÄÈ´òÂ†±ÂëäË°®ÁèæÔºåÂÑòÁÆ°‰ªçÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÈ©óË≠â‰æÜÁ¢∫‰øùÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÂíåÂèØÁî®ÊÄß„ÄÇ

##### **HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline**
2407.17879v2 by Qingyu Guo, Jiayong Wan, Songqiang Xu, Meng Li, Yuan Wang

Vision Transformer (ViT) acceleration with field programmable gate array
(FPGA) is promising but challenging. Existing FPGA-based ViT accelerators
mainly rely on temporal architectures, which process different operators by
reusing the same hardware blocks and suffer from extensive memory access
overhead. Pipelined architectures, either coarse-grained or fine-grained,
unroll the ViT computation spatially for memory access efficiency. However,
they usually suffer from significant hardware resource constraints and pipeline
bubbles induced by the global computation dependency of ViT. In this paper, we
introduce HG-PIPE, a pipelined FPGA accelerator for high-throughput and
low-latency ViT processing. HG-PIPE features a hybrid-grained pipeline
architecture to reduce on-chip buffer cost and couples the computation dataflow
and parallelism design to eliminate the pipeline bubbles. HG-PIPE further
introduces careful approximations to implement both linear and non-linear
operators with abundant Lookup Tables (LUTs), thus alleviating resource
constraints. On a ZCU102 FPGA, HG-PIPE achieves 2.78 times better throughput
and 2.52 times better resource efficiency than the prior-art accelerators,
e.g., AutoViTAcc. With a VCK190 FPGA, HG-PIPE realizes end-to-end ViT
acceleration on a single device and achieves 7118 images/s, which is 2.81 times
faster than a V100 GPU.

ÊëòË¶ÅÔºöË¶ñË¶∫ËÆäÊèõÂô® (ViT) Âä†ÈÄüËàáÁèæÂ†¥ÂèØÁ∑®Á®ãÈñòÈô£Âàó (FPGA) ÂÖÖÊªøÂâçÊôØÔºå‰ΩÜÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁèæÊúâÁöÑÂü∫Êñº FPGA ÁöÑ ViT Âä†ÈÄüÂô®‰∏ªË¶Å‰æùË≥¥ÊñºÊôÇÈñìÊû∂ÊßãÔºåÂÆÉÈÄèÈÅéÈáçË§á‰ΩøÁî®Áõ∏ÂêåÁöÑÁ°¨È´îÂçÄÂ°ä‰æÜËôïÁêÜ‰∏çÂêåÁöÑÈÅãÁÆóÂ≠êÔºå‰∏¶ÊâøÂèóÂ§ßÈáèÁöÑË®òÊÜ∂È´îÂ≠òÂèñË≤†Êìî„ÄÇÁÑ°Ë´ñÊòØÁ≤óÁ≤íÂ∫¶ÊàñÁ¥∞Á≤íÂ∫¶ÔºåÊµÅÊ∞¥Á∑öÊû∂ÊßãÈÉΩÊúÉÂú®Á©∫Èñì‰∏äÂ±ïÈñã ViT Ë®àÁÆó‰ª•ÊèêÈ´òË®òÊÜ∂È´îÂ≠òÂèñÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÈÄöÂ∏∏ÊúÉÂèóÂà∞È°ØËëóÁöÑÁ°¨È´îË≥áÊ∫êÈôêÂà∂ÂíåÁî± ViT ÁöÑÂÖ®Â±ÄË®àÁÆó‰æùË≥¥ÊÄßÊâÄÂºïÁôºÁöÑÊµÅÊ∞¥Á∑öÊ∞£Ê≥°ÂΩ±Èüø„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π HG-PIPEÔºå‰∏ÄÁ®ÆÁî®ÊñºÈ´òÈÄöÈáèÂíå‰ΩéÂª∂ÈÅ≤ ViT ËôïÁêÜÁöÑÊµÅÊ∞¥Á∑ö FPGA Âä†ÈÄüÂô®„ÄÇHG-PIPE Êé°Áî®Ê∑∑ÂêàÁ≤íÂ∫¶ÊµÅÊ∞¥Á∑öÊû∂Êßã‰ª•Èôç‰ΩéÊô∂ÁâáÁ∑©Ë°ùÊàêÊú¨Ôºå‰∏¶ÁµêÂêàË®àÁÆóË≥áÊñôÊµÅÁ®ãÂíå‰∏¶Ë°åË®≠Ë®à‰ª•Ê∂àÈô§ÊµÅÊ∞¥Á∑öÊ∞£Ê≥°„ÄÇHG-PIPE ÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰ªîÁ¥∞ÁöÑËøë‰ººÂÄºÔºå‰ª•‰ΩøÁî®Ë±êÂØåÁöÑÊü•Èñ±Ë°® (LUT) ÂØ¶‰ΩúÁ∑öÊÄßÂíåÈùûÁ∑öÊÄßÈÅãÁÆóÂ≠êÔºåÂæûËÄåÊ∏õËºïË≥áÊ∫êÈôêÂà∂„ÄÇÂú® ZCU102 FPGA ‰∏äÔºåHG-PIPE ÁöÑËôïÁêÜÈáèÊØîÁèæÊúâÂä†ÈÄüÂô®Ôºà‰æãÂ¶Ç AutoViTAccÔºâÈ´òÂá∫ 2.78 ÂÄçÔºåË≥áÊ∫êÊïàÁéáÈ´òÂá∫ 2.52 ÂÄç„ÄÇ‰ΩøÁî® VCK190 FPGAÔºåHG-PIPE Âú®ÂñÆ‰∏ÄË£ùÁΩÆ‰∏äÂØ¶ÁèæÁ´ØÂà∞Á´ØÁöÑ ViT Âä†ÈÄüÔºå‰∏¶ÂØ¶ÁèæÊØèÁßí 7118 ÂºµÂΩ±ÂÉèÔºåÊØî V100 GPU Âø´ 2.81 ÂÄç„ÄÇ

##### **EEG-SSM: Leveraging State-Space Model for Dementia Detection**
2407.17801v1 by Xuan-The Tran, Linh Le, Quoc Toan Nguyen, Thomas Do, Chin-Teng Lin

State-space models (SSMs) have garnered attention for effectively processing
long data sequences, reducing the need to segment time series into shorter
intervals for model training and inference. Traditionally, SSMs capture only
the temporal dynamics of time series data, omitting the equally critical
spectral features. This study introduces EEG-SSM, a novel state-space
model-based approach for dementia classification using EEG data. Our model
features two primary innovations: EEG-SSM temporal and EEG-SSM spectral
components. The temporal component is designed to efficiently process EEG
sequences of varying lengths, while the spectral component enhances the model
by integrating frequency-domain information from EEG signals. The synergy of
these components allows EEG-SSM to adeptly manage the complexities of
multivariate EEG data, significantly improving accuracy and stability across
different temporal resolutions. Demonstrating a remarkable 91.0 percent
accuracy in classifying Healthy Control (HC), Frontotemporal Dementia (FTD),
and Alzheimer's Disease (AD) groups, EEG-SSM outperforms existing models on the
same dataset. The development of EEG-SSM represents an improvement in the use
of state-space models for screening dementia, offering more precise and
cost-effective tools for clinical neuroscience.

ÊëòË¶ÅÔºöÁãÄÊÖãÁ©∫ÈñìÊ®°Âûã (SSM) Âõ†ÊúâÊïàËôïÁêÜÈï∑Ë≥áÊñôÂ∫èÂàóËÄåÂÇôÂèóÈóúÊ≥®ÔºåÊ∏õÂ∞ëÂ∞áÊôÇÈñìÂ∫èÂàóÂçÄÈöîÊàêËºÉÁü≠ÂçÄÈñì‰ª•ÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥ÂíåÊé®Ë´ñÁöÑÈúÄË¶Å„ÄÇÂÇ≥Áµ±‰∏äÔºåSSM Âè™Êì∑ÂèñÊôÇÈñìÂ∫èÂàóË≥áÊñôÁöÑÊôÇÈñìÂãïÊÖãÔºåÁúÅÁï•ÂêåÊ®£ÈáçË¶ÅÁöÑÈ†ªË≠úÁâπÂæµ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫ EEG-SSMÔºå‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫ÊñºÁãÄÊÖãÁ©∫ÈñìÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÁî®Êñº‰ΩøÁî® EEG Ë≥áÊñôÈÄ≤Ë°åÂ§±Êô∫ÁóáÂàÜÈ°û„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÖ∑ÊúâÂÖ©È†Ö‰∏ªË¶ÅÁöÑÂâµÊñ∞ÔºöEEG-SSM ÊôÇÈñìÂíå EEG-SSM È†ªË≠úÁµÑÊàêÈÉ®ÂàÜ„ÄÇÊôÇÈñìÁµÑÊàêÈÉ®ÂàÜÊó®Âú®ÊúâÊïàÁéáÂú∞ËôïÁêÜÈï∑Â∫¶‰∏çÂêåÁöÑ EEG Â∫èÂàóÔºåËÄåÈ†ªË≠úÁµÑÊàêÈÉ®ÂàÜÈÄèÈÅéÊï¥Âêà EEG Ë®äËôüÁöÑÈ†ªÂüüË≥áË®ä‰æÜÂ¢ûÂº∑Ê®°Âûã„ÄÇÈÄô‰∫õÁµÑÊàêÈÉ®ÂàÜÁöÑÂçîÂêå‰ΩúÁî®ËÆì EEG-SSM ËÉΩÈùàÊ¥ªÂú∞ÁÆ°ÁêÜÂ§öËÆäÈáè EEG Ë≥áÊñôÁöÑË§áÈõúÊÄßÔºåÂ§ßÂπÖÊîπÂñÑ‰∏çÂêåÊôÇÈñìËß£ÊûêÂ∫¶‰∏ãÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁ©©ÂÆöÊÄß„ÄÇEEG-SSM Âú®ÂàÜÈ°ûÂÅ•Â∫∑Â∞çÁÖßÁµÑ (HC)„ÄÅÈ°çÈ°≥ËëâÂûãÂ§±Êô∫Áóá (FTD) ÂíåÈòøËå≤Êµ∑ÈªòÁóá (AD) ÁµÑÂà•ÊôÇÂ±ïÁèæÂá∫È©ö‰∫∫ÁöÑ 91.0% Ê∫ñÁ¢∫Â∫¶ÔºåÂú®Áõ∏ÂêåÁöÑË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÁèæÊúâÊ®°Âûã„ÄÇEEG-SSM ÁöÑÈñãÁôº‰ª£Ë°®‰∫Ü‰ΩøÁî®ÁãÄÊÖãÁ©∫ÈñìÊ®°ÂûãÈÄ≤Ë°åÂ§±Êô∫ÁóáÁØ©Ê™¢ÁöÑÈÄ≤Ê≠•ÔºåÁÇ∫Ëá®Â∫äÁ•ûÁ∂ìÁßëÂ≠∏Êèê‰æõÊõ¥Á≤æÁ¢∫‰∏îÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Mpox Detection Advanced: Rapid Epidemic Response Through Synthetic Data**
2407.17762v1 by Yudara Kularathne, Prathapa Janitha, Sithira Ambepitiya, Prarththanan Sothyrajah, Thanveer Ahamed, Dinuka Wijesundara

Rapid development of disease detection models using computer vision is
crucial in responding to medical emergencies, such as epidemics or bioterrorism
events. Traditional data collection methods are often too slow in these
scenarios, requiring innovative approaches for quick, reliable model generation
from minimal data. Our study introduces a novel approach by constructing a
comprehensive computer vision model to detect Mpox lesions using only synthetic
data. Initially, these models generated a diverse set of synthetic images
representing Mpox lesions on various body parts (face, back, chest, leg, neck,
arm) across different skin tones as defined by the Fitzpatrick scale (fair,
brown, dark skin). Subsequently, we trained and tested a vision model with this
synthetic dataset to evaluate the diffusion models' efficacy in producing
high-quality training data and its impact on the vision model's medical image
recognition performance. The results were promising; the vision model achieved
a 97% accuracy rate, with 96% precision and recall for Mpox cases, and
similarly high metrics for normal and other skin disorder cases, demonstrating
its ability to correctly identify true positives and minimize false positives.
The model achieved an F1-Score of 96% for Mpox cases and 98% for normal and
other skin disorders, reflecting a balanced precision-recall relationship, thus
ensuring reliability and robustness in its predictions. Our proposed
SynthVision methodology indicates the potential to develop accurate computer
vision models with minimal data input for future medical emergencies.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®ÈõªËÖ¶Ë¶ñË¶∫Âø´ÈÄüÈñãÁôºÁñæÁóÖÊ™¢Ê∏¨Ê®°ÂûãÂ∞çÊñºÂõ†ÊáâÈÜ´ÁôÇÁ∑äÊÄ•‰∫ã‰ª∂Ôºà‰æãÂ¶ÇÊµÅË°åÁóÖÊàñÁîüÁâ©ÊÅêÊÄñ‰∏ªÁæ©‰∫ã‰ª∂ÔºâËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑË≥áÊñôÊî∂ÈõÜÊñπÊ≥ïÂú®ÈÄô‰∫õÊÉÖÊ≥Å‰∏ãÈÄöÂ∏∏Â§™ÊÖ¢ÔºåÈúÄË¶ÅÂâµÊñ∞ÁöÑÊñπÊ≥ïÊâçËÉΩÂæûÊúÄÂ∞ëË≥áÊñô‰∏≠Âø´ÈÄü„ÄÅÂèØÈù†Âú∞Áî¢ÁîüÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÂª∫Êßã‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÔºåÂÉÖ‰ΩøÁî®ÂêàÊàêË≥áÊñô‰æÜÊ™¢Ê∏¨Áå¥ÁóòÁóÖÁÅ∂„ÄÇÊúÄÂàùÔºåÈÄô‰∫õÊ®°ÂûãÁî¢Áîü‰∫Ü‰∏ÄÁµÑÂ§öÊ®£ÂåñÁöÑÂêàÊàêÂΩ±ÂÉèÔºå‰ª£Ë°®‰∫Ü‰∏çÂêåËÜöËâ≤ÔºàÊ†πÊìö Fitzpatrick ÈáèË°®ÂÆöÁæ©ÁÇ∫ÁôΩÁöô„ÄÅÊ£ïËâ≤„ÄÅÊ∑±Ëâ≤ÁöÆËÜöÔºâ‰∏ä‰∏çÂêåË∫´È´îÈÉ®‰ΩçÔºàËáâÈÉ®„ÄÅËÉåÈÉ®„ÄÅËÉ∏ÈÉ®„ÄÅËÖøÈÉ®„ÄÅÈ†∏ÈÉ®„ÄÅÊâãËáÇÔºâÁöÑÁå¥ÁóòÁóÖÁÅ∂„ÄÇÈö®ÂæåÔºåÊàëÂÄë‰ΩøÁî®ÈÄôÂÄãÂêàÊàêË≥áÊñôÈõÜË®ìÁ∑¥ÂíåÊ∏¨Ë©¶‰∏ÄÂÄãË¶ñË¶∫Ê®°ÂûãÔºå‰ª•Ë©ï‰º∞Êì¥Êï£Ê®°ÂûãÁî¢ÁîüÈ´òÂìÅË≥™Ë®ìÁ∑¥Ë≥áÊñôÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂÖ∂Â∞çË¶ñË¶∫Ê®°ÂûãÈÜ´Â≠∏ÂΩ±ÂÉèËæ®Ë≠òÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÁµêÊûú‰ª§‰∫∫ÊªøÊÑèÔºõË¶ñË¶∫Ê®°ÂûãÈÅîÂà∞‰∫Ü 97% ÁöÑÊ∫ñÁ¢∫ÁéáÔºåÁå¥ÁóòÁóÖ‰æãÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéáÁÇ∫ 96%ÔºåÊ≠£Â∏∏ÂíåÂÖ∂ÂÆÉÁöÆËÜöÁñæÁóÖÁóÖ‰æãÁöÑÊåáÊ®ô‰πüÂêåÊ®£È´òÔºåË≠âÊòé‰∫ÜÂÆÉÊ≠£Á¢∫Ëæ®Ë≠òÁúüÈôΩÊÄß‰∏¶Â∞áÂÅáÈôΩÊÄßÈôçËá≥ÊúÄ‰ΩéÁöÑËÉΩÂäõ„ÄÇË©≤Ê®°ÂûãÂú®Áå¥ÁóòÁóÖ‰æã‰∏≠ÈÅîÂà∞‰∫Ü 96% ÁöÑ F1 ÂàÜÊï∏ÔºåÂú®Ê≠£Â∏∏ÂíåÂÖ∂ÂÆÉÁöÆËÜöÁñæÁóÖ‰∏≠ÈÅîÂà∞‰∫Ü 98%ÔºåÂèçÊò†Âá∫Âπ≥Ë°°ÁöÑÊ∫ñÁ¢∫Â∫¶Âè¨ÂõûÁéáÈóú‰øÇÔºåÂæûËÄåÁ¢∫‰øùÂÖ∂È†êÊ∏¨ÁöÑÂèØÈù†ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑ SynthVision ÊñπÊ≥ïË°®ÊòéÔºåÊúâÂèØËÉΩÁÇ∫Êú™‰æÜÁöÑÈÜ´ÁôÇÁ∑äÊÄ•‰∫ã‰ª∂ÈñãÁôºÂá∫Ê∫ñÁ¢∫ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÔºå‰∏îË≥áÊñôËº∏ÂÖ•ÈáèÊúÄÂ∞ë„ÄÇ</paragraph>

##### **Cost-effective Instruction Learning for Pathology Vision and Language Analysis**
2407.17734v1 by Kaitao Chen, Mianxin Liu, Fang Yan, Lei Ma, Xiaoming Shi, Lilong Wang, Xiaosong Wang, Lifeng Zhu, Zhe Wang, Mu Zhou, Shaoting Zhang

The advent of vision-language models fosters the interactive conversations
between AI-enabled models and humans. Yet applying these models into clinics
must deal with daunting challenges around large-scale training data, financial,
and computational resources. Here we propose a cost-effective instruction
learning framework for conversational pathology named as CLOVER. CLOVER only
trains a lightweight module and uses instruction tuning while freezing the
parameters of the large language model. Instead of using costly GPT-4, we
propose well-designed prompts on GPT-3.5 for building generation-based
instructions, emphasizing the utility of pathological knowledge derived from
the Internet source. To augment the use of instructions, we construct a
high-quality set of template-based instructions in the context of digital
pathology. From two benchmark datasets, our findings reveal the strength of
hybrid-form instructions in the visual question-answer in pathology. Extensive
results show the cost-effectiveness of CLOVER in answering both open-ended and
closed-ended questions, where CLOVER outperforms strong baselines that possess
37 times more training parameters and use instruction data generated from
GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot
learning in the external clinical dataset. These findings demonstrate that
cost-effective modeling of CLOVER could accelerate the adoption of rapid
conversational applications in the landscape of digital pathology.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂá∫Áèæ‰øÉÈÄ≤‰∫Ü AI ÂïüÁî®Ê®°ÂûãËàá‰∫∫È°û‰πãÈñìÁöÑ‰∫íÂãïÂ∞çË©±„ÄÇÁÑ∂ËÄåÔºåÂ∞áÈÄô‰∫õÊ®°ÂûãÊáâÁî®ÊñºËá®Â∫äÂøÖÈ†àÊáâÂ∞çÂ§ßË¶èÊ®°Ë®ìÁ∑¥Êï∏Êìö„ÄÅË≤°ÂãôÂíåË®àÁÆóË≥áÊ∫êÁ≠âÂö¥Â≥ªÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ CLOVER ÁöÑÁ∂ìÊøüÈ´òÊïàÁöÑÊúÉË©±ÁóÖÁêÜÂ≠∏Êåá‰ª§Â≠∏ÁøíÊû∂Êßã„ÄÇCLOVER ÂÉÖË®ìÁ∑¥‰∏ÄÂÄãËºïÈáèÁ¥öÊ®°ÁµÑÔºå‰∏¶Âú®ÂáçÁµêÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂèÉÊï∏ÁöÑÂêåÊôÇ‰ΩøÁî®Êåá‰ª§ÂæÆË™ø„ÄÇÊàëÂÄëÊ≤íÊúâ‰ΩøÁî®ÊòÇË≤¥ÁöÑ GPT-4ÔºåËÄåÊòØÈáùÂ∞ç GPT-3.5 ÊèêÂá∫Ë®≠Ë®àËâØÂ•ΩÁöÑÊèêÁ§∫Ôºå‰ª•Âª∫Á´ãÂü∫ÊñºÁîüÊàêÁöÑÊåá‰ª§ÔºåÂº∑Ë™øÂæûÁ∂≤ÈöõÁ∂≤Ë∑Ø‰æÜÊ∫êË°çÁîüÁöÑÁóÖÁêÜÁü•Ë≠òÁöÑÊïàÁî®„ÄÇÁÇ∫‰∫ÜÊì¥Â±ïÊåá‰ª§ÁöÑ‰ΩøÁî®ÔºåÊàëÂÄëÂú®Êï∏‰ΩçÁóÖÁêÜÂ≠∏ÁöÑËÉåÊôØ‰∏ãÊßãÂª∫‰∫Ü‰∏ÄÁµÑÈ´òÂìÅË≥™ÁöÑÂü∫ÊñºÁØÑÊú¨ÁöÑÊåá‰ª§„ÄÇÂæûÂÖ©ÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÊ∑∑ÂêàÂΩ¢ÂºèÊåá‰ª§Âú®ÁóÖÁêÜÂ≠∏Ë¶ñË¶∫ÂïèÁ≠î‰∏≠ÁöÑÂÑ™Âã¢„ÄÇÂª£Ê≥õÁöÑÁµêÊûúÈ°ØÁ§∫‰∫Ü CLOVER Âú®ÂõûÁ≠îÈñãÊîæÂºèÂíåÂ∞ÅÈñâÂºèÂïèÈ°åÊñπÈù¢ÁöÑÁ∂ìÊøüÊïàÁõäÔºåÂÖ∂‰∏≠ CLOVER ÂÑ™ÊñºÊìÅÊúâÂ§ö 37 ÂÄçË®ìÁ∑¥ÂèÉÊï∏‰∏¶‰ΩøÁî®Âæû GPT-4 ÁîüÊàêÁöÑÊåá‰ª§Ë≥áÊñôÁöÑÂº∑Â§ßÂü∫Ê∫ñ„ÄÇÈÄèÈÅéÊåá‰ª§ÂæÆË™øÔºåCLOVER Âú®Â§ñÈÉ®Ëá®Â∫äË≥áÊñôÈõÜ‰∏≠Â±ïÁèæ‰∫ÜÂ∞èÊ®£Êú¨Â≠∏ÁøíÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÈÄô‰∫õÁôºÁèæË≠âÊòé‰∫Ü CLOVER ÁöÑÁ∂ìÊøüÈ´òÊïàÂª∫Ê®°ÂèØ‰ª•Âä†ÈÄüÂú®Êï∏‰ΩçÁóÖÁêÜÈ†òÂüüÊé°Áî®Âø´ÈÄüÂ∞çË©±ÂºèÊáâÁî®Á®ãÂºè„ÄÇ

##### **Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**
2407.17324v2 by Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis

Dementia, a debilitating neurological condition affecting millions worldwide,
presents significant diagnostic challenges. In this work, we introduce a novel
methodology for the classification of demented and non-demented elderly
patients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach
features a unique technique for selectively processing MRI slices, focusing on
the most relevant brain regions and excluding less informative sections. This
methodology is complemented by a confidence-based classification committee
composed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and
Dem3D EfficientNet. These models work synergistically to enhance
decision-making accuracy, leveraging their collective strengths. Tested on the
Open Access Series of Imaging Studies(OASIS) dataset, our method achieved an
impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,
validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
confirmed the robustness and generalizability of our approach. The use of
explainable AI (XAI) techniques and comprehensive ablation studies further
substantiate the effectiveness of our techniques, providing insights into the
decision-making process and the importance of our methodology. This research
offers a significant advancement in dementia diagnosis, providing a highly
accurate and efficient tool for clinical applications.

ÊëòË¶ÅÔºöÂ§±Êô∫ÁóáÊòØ‰∏ÄÁ®ÆÂΩ±ÈüøÂÖ®ÁêÉÊï∏ÁôæËê¨‰∫∫ÁöÑË°∞Âº±ÊÄßÁ•ûÁ∂ìÁñæÁóÖÔºåÂú®Ë®∫Êñ∑‰∏äÂÖ∑ÊúâÈáçÂ§ßÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂ§±Êô∫ÂíåÈùûÂ§±Êô∫ËÄÅÂπ¥ÊÇ£ËÄÖÈÄ≤Ë°åÂàÜÈ°ûÔºå‰ΩøÁî® 3D Â§ßËÖ¶Á£ÅÊåØÈÄ†ÂΩ± (MRI) ÊéÉÊèè„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé°Áî®‰∫Ü‰∏ÄÁ®ÆÁç®ÁâπÊäÄË°ìÔºåÁî®ÊñºÈÅ∏ÊìáÊÄßËôïÁêÜ MRI ÂàáÁâáÔºåÈáçÈªûÈóúÊ≥®ÊúÄÁõ∏ÈóúÁöÑÂ§ßËÖ¶ÂçÄÂüüÔºå‰∏¶ÊéíÈô§‰ø°ÊÅØÈáèËºÉÂ∞ëÁöÑÈÉ®ÂàÜ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁî±‰∏ÄÂÄãÂü∫Êñº‰ø°ÂøÉÁöÑÂàÜÈ°ûÂßîÂì°ÊúÉË£úÂÖÖÔºåË©≤ÂßîÂì°ÊúÉÁî±‰∏âÂÄãËá™ÂÆöÁæ©Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁµÑÊàêÔºöDem3D ResNet„ÄÅDem3D CNN Âíå Dem3D EfficientNet„ÄÇÈÄô‰∫õÊ®°ÂûãÂçîÂêåÂ∑•‰Ωú‰ª•Â¢ûÂº∑Ê±∫Á≠ñÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂà©Áî®ÂÆÉÂÄëÁöÑÈõÜÈ´îÂÑ™Âã¢„ÄÇÂú®ÂΩ±ÂÉèÁ†îÁ©∂ÈñãÊîæÂ≠òÂèñÁ≥ªÂàó (OASIS) Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 94.12% ÁöÑÈ©ö‰∫∫Ê∫ñÁ¢∫Â∫¶ÔºåË∂ÖÈÅé‰∫ÜÁèæÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂú®ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ (ADNI) Ë≥áÊñôÈõÜ‰∏äÁöÑÈ©óË≠âË≠âÂØ¶‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÊÄß„ÄÇÂèØËß£Èáã AI (XAI) ÊäÄË°ìÂíåÂÖ®Èù¢ÁöÑÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Ë≠âÂØ¶‰∫ÜÊàëÂÄëÊäÄË°ìÁöÑÊúâÊïàÊÄßÔºåÊèê‰æõ‰∫ÜÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÂíåÊàëÂÄëÊñπÊ≥ïÈáçË¶ÅÊÄßÁöÑË¶ãËß£„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Â§±Êô∫ÁóáË®∫Êñ∑Êèê‰æõ‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºåÁÇ∫Ëá®Â∫äÊáâÁî®Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÈ´òÊïàÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Improving ICD coding using Chapter based Named Entities and Attentional Models**
2407.17230v1 by Abhijith R. Beeravolu, Mirjam Jonkman, Sami Azam, Friso De Boer

Recent advancements in natural language processing (NLP) have led to
automation in various domains. However, clinical NLP often relies on benchmark
datasets that may not reflect real-world scenarios accurately. Automatic ICD
coding, a vital NLP task, typically uses outdated and imbalanced datasets like
MIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4
and 0.7 due to many false positives. Our research introduces an enhanced
approach to ICD coding that improves F1 scores by using chapter-based named
entities and attentional models. This method categorizes discharge summaries
into ICD-9 Chapters and develops attentional models with chapter-specific data,
eliminating the need to consider external data for code identification. For
categorization, we use Chapter-IV to de-bias and influence key entities and
weights without neural networks, creating accurate thresholds and providing
interpretability for human validation. Post-validation, we develop attentional
models for three frequent and three non-frequent codes from Chapter-IV using
Bidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with
Multi-head Attention architectures. The average Micro-F1 scores of 0.79 and
0.81 from these models demonstrate significant performance improvements in ICD
coding.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤Â∞éËá¥ÂêÑÁ®ÆÈ†òÂüüÁöÑËá™ÂãïÂåñ„ÄÇÁÑ∂ËÄåÔºåËá®Â∫ä NLP ÈÄöÂ∏∏‰æùË≥¥ÊñºÂü∫Ê∫ñË≥áÊñôÈõÜÔºåÈÄô‰∫õË≥áÊñôÈõÜÂèØËÉΩÁÑ°Ê≥ïÊ∫ñÁ¢∫ÂèçÊò†ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ„ÄÇËá™Âãï ICD Á∑®Á¢ºÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑ NLP ‰ªªÂãôÔºåÈÄöÂ∏∏‰ΩøÁî®ÈÅéÊôÇ‰∏î‰∏çÂπ≥Ë°°ÁöÑË≥áÊñôÈõÜÔºå‰æãÂ¶Ç MIMIC-IIIÔºåÁî±ÊñºË®±Â§öÂÅáÈôΩÊÄßÔºåÁèæÊúâÊñπÊ≥ïÁî¢ÁîüÁöÑÂæÆÂπ≥Âùá F1 ÂàÜÊï∏‰ªãÊñº 0.4 Âíå 0.7 ‰πãÈñì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂ¢ûÂº∑ÁöÑ ICD Á∑®Á¢ºÊñπÊ≥ïÔºåÈÄöÈÅé‰ΩøÁî®Âü∫ÊñºÁ´†ÁØÄÁöÑÂëΩÂêçÂØ¶È´îÂíåÊ≥®ÊÑèÂäõÊ®°Âûã‰æÜÊèêÈ´ò F1 ÂàÜÊï∏„ÄÇÊ≠§ÊñπÊ≥ïÂ∞áÂá∫Èô¢ÊëòË¶ÅÂàÜÈ°ûÁÇ∫ ICD-9 Á´†ÁØÄÔºå‰∏¶‰ΩøÁî®Á´†ÁØÄÁâπÂÆöË≥áÊñôÈñãÁôºÊ≥®ÊÑèÂäõÊ®°ÂûãÔºåÊ∂àÈô§‰∫ÜËÄÉÊÖÆÂ§ñÈÉ®Ë≥áÊñô‰ª•ÈÄ≤Ë°å‰ª£Á¢ºË≠òÂà•ÁöÑÈúÄË¶Å„ÄÇÂ∞çÊñºÂàÜÈ°ûÔºåÊàëÂÄë‰ΩøÁî® Chapter-IV ‰æÜÊ∂àÈô§ÂÅèÂ∑Æ‰∏¶ÂΩ±ÈüøÈóúÈçµÂØ¶È´îÂíåÊ¨äÈáçÔºåËÄåÁÑ°ÈúÄÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂæûËÄåÂª∫Á´ãÊ∫ñÁ¢∫ÁöÑÈñæÂÄº‰∏¶Êèê‰æõ‰∫∫È°ûÈ©óË≠âÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂú®È©óË≠â‰πãÂæåÔºåÊàëÂÄë‰ΩøÁî®Â∏∂ÊúâÊ≥®ÊÑèÂäõÂíåÂ§öÈ†≠Ê≥®ÊÑèÊû∂ÊßãÁöÑÈõôÂêëÈñÄÊéßÈÅûËø¥ÂñÆÂÖÉ (GRU) Âíå TransformerÔºåÁÇ∫ Chapter-IV ‰∏≠ÁöÑ‰∏âÂÄãÈ†ªÁπÅÂíå‰∏âÂÄãÈùûÈ†ªÁπÅ‰ª£Á¢ºÈñãÁôºÊ≥®ÊÑèÂäõÊ®°Âûã„ÄÇÈÄô‰∫õÊ®°ÂûãÁöÑÂπ≥ÂùáÂæÆ F1 ÂàÜÊï∏ÁÇ∫ 0.79 Âíå 0.81ÔºåË°®Êòé ICD Á∑®Á¢ºÁöÑÊïàËÉΩÊúâ‰∫ÜÈ°ØËëóÁöÑÊèêÂçá„ÄÇ

##### **Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence**
2407.17164v2 by Xiaoyu Tan, Bin Li, Xihe Qiu, Jingjing Huang, Yinghui Xu, Wei Chu

Integrating deep neural networks with the Hawkes process has significantly
improved predictive capabilities in finance, health informatics, and
information technology. Nevertheless, these models often face challenges in
real-world settings, particularly due to substantial label noise. This issue is
of significant concern in the medical field, where label noise can arise from
delayed updates in electronic medical records or misdiagnoses, leading to
increased prediction risks. Our research indicates that deep Hawkes process
models exhibit reduced robustness when dealing with label noise, particularly
when it affects both event types and timing. To address these challenges, we
first investigate the influence of label noise in approximated intensity
functions and present a novel framework, the Robust Deep Hawkes Process (RDHP),
to overcome the impact of label noise on the intensity function of Hawkes
models, considering both the events and their occurrences. We tested RDHP using
multiple open-source benchmarks with synthetic noise and conducted a case study
on obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting
with inherent label noise. The results demonstrate that RDHP can effectively
perform classification and regression tasks, even in the presence of noise
related to events and their timing. To the best of our knowledge, this is the
first study to successfully address both event and time label noise in deep
Hawkes process models, offering a promising solution for medical applications,
specifically in diagnosing OSAHS.

ÊëòË¶ÅÔºöÂ∞áÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØËàáÈúçÂÖãÊñØÈÅéÁ®ãÊï¥ÂêàÔºåÂ∑≤È°ØËëóÊèêÂçáÈáëËûç„ÄÅÂÅ•Â∫∑Ë≥áË®äÂ≠∏ÂíåË≥áË®äÁßëÊäÄÁöÑÈ†êÊ∏¨ËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊ®°ÂûãÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠Á∂ìÂ∏∏Èù¢Ëá®ÊåëÊà∞ÔºåÁâπÂà•ÊòØÂõ†ÁÇ∫Ê®ôÁ±§ÈõúË®äÂæàÂ§ß„ÄÇÈÄôÂÄãÂïèÈ°åÂú®ÈÜ´Â≠∏È†òÂüü‰∏≠ÁâπÂà•‰ª§‰∫∫ÊìîÊÜÇÔºåÂõ†ÁÇ∫Ê®ôÁ±§ÈõúË®äÂèØËÉΩ‰æÜËá™ÈõªÂ≠êÁóÖÊ≠∑ÁöÑÂª∂ÈÅ≤Êõ¥Êñ∞ÊàñË™§Ë®∫ÔºåÂ∞éËá¥È†êÊ∏¨È¢®Èö™Â¢ûÂä†„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËôïÁêÜÊ®ôÁ±§ÈõúË®äÊôÇÔºåÊ∑±Â∫¶ÈúçÂÖãÊñØÈÅéÁ®ãÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÊúÉÈôç‰ΩéÔºåÁâπÂà•ÊòØÁï∂ÂÆÉÂΩ±Èüø‰∫ã‰ª∂È°ûÂûãÂíåÊôÇÈñìÈªûÊôÇ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÈ¶ñÂÖàÁ†îÁ©∂Ê®ôÁ±§ÈõúË®äÂ∞çËøë‰ººÂº∑Â∫¶ÂáΩÊï∏ÁöÑÂΩ±ÈüøÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊû∂ÊßãÔºåÂç≥Á©©ÂÅ•Ê∑±Â∫¶ÈúçÂÖãÊñØÈÅéÁ®ã (RDHP)Ôºå‰ª•ÂÖãÊúçÊ®ôÁ±§ÈõúË®äÂ∞çÈúçÂÖãÊñØÊ®°ÂûãÂº∑Â∫¶ÂáΩÊï∏ÁöÑÂΩ±ÈüøÔºåÂêåÊôÇËÄÉÊÖÆ‰∫ã‰ª∂ÂèäÂÖ∂ÁôºÁîü„ÄÇÊàëÂÄë‰ΩøÁî®Â§öÂÄãÂ∏∂ÊúâÂêàÊàêÈõúË®äÁöÑÈñãÊ∫êÂü∫Ê∫ñÊ∏¨Ë©¶ RDHPÔºå‰∏¶Âú®ÁèæÂØ¶‰∏ñÁïå‰∏≠Â∞çÈòªÂ°ûÊÄßÁù°Áú†ÂëºÂê∏‰∏≠Ê≠¢‰ΩéÈÄöÊ∞£ÁóáÂÄôÁæ§ (OSAHS) ÈÄ≤Ë°åÊ°à‰æãÁ†îÁ©∂ÔºåÂÖ∂‰∏≠Â≠òÂú®Âõ∫ÊúâÁöÑÊ®ôÁ±§ÈõúË®ä„ÄÇÁµêÊûúË°®ÊòéÔºåÂç≥‰ΩøÂú®Ëàá‰∫ã‰ª∂ÂèäÂÖ∂ÊôÇÈñìÈªûÁõ∏ÈóúÁöÑÈõúË®äÂ≠òÂú®ÁöÑÊÉÖÊ≥Å‰∏ãÔºåRDHP ‰ªçËÉΩÊúâÊïàÂü∑Ë°åÂàÜÈ°ûÂíåÂõûÊ≠∏‰ªªÂãô„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊàêÂäüËß£Ê±∫Ê∑±Â∫¶ÈúçÂÖãÊñØÈÅéÁ®ãÊ®°Âûã‰∏≠‰∫ã‰ª∂ÂíåÊôÇÈñìÊ®ôÁ±§ÈõúË®äÁöÑÁ†îÁ©∂ÔºåÁÇ∫ÈÜ´ÁôÇÊáâÁî®ÔºàÁâπÂà•ÊòØÂú®Ë®∫Êñ∑ OSAHS ÊôÇÔºâÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **SDoH-GPT: Using Large Language Models to Extract Social Determinants of Health (SDoH)**
2407.17126v1 by Bernardo Consoli, Xizhi Wu, Song Wang, Xinyu Zhao, Yanshan Wang, Justin Rousseau, Tom Hartvigsen, Li Shen, Huanmei Wu, Yifan Peng, Qi Long, Tianlong Chen, Ying Ding

Extracting social determinants of health (SDoH) from unstructured medical
notes depends heavily on labor-intensive annotations, which are typically
task-specific, hampering reusability and limiting sharing. In this study we
introduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)
method leveraging contrastive examples and concise instructions to extract SDoH
without relying on extensive medical annotations or costly human intervention.
It achieved tenfold and twentyfold reductions in time and cost respectively,
and superior consistency with human annotators measured by Cohen's kappa of up
to 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the
strengths of both, ensuring high accuracy and computational efficiency while
consistently maintaining 0.90+ AUROC scores. Testing across three distinct
datasets has confirmed its robustness and accuracy. This study highlights the
potential of leveraging LLMs to revolutionize medical note classification,
demonstrating their capability to achieve highly accurate classifications with
significantly reduced time and cost.

ÊëòË¶ÅÔºöÂæûÈùûÁµêÊßãÂåñÁöÑÈÜ´ÁôÇÁ≠ÜË®ò‰∏≠ËêÉÂèñÂÅ•Â∫∑ÁöÑÁ§æÊúÉÊ±∫ÂÆöÂõ†Á¥† (SDoH) ‰ª∞Ë≥¥Â§ßÈáèÁöÑ‰∫∫Â∑•Ê®ôË®ªÔºåËÄåÈÄô‰∫õÊ®ôË®ªÈÄöÂ∏∏ÊòØÈáùÂ∞çÁâπÂÆö‰ªªÂãôÔºåÈÄôÊúÉÈòªÁ§ôÂèØÈáçË§á‰ΩøÁî®ÊÄß‰∏¶ÈôêÂà∂ÂàÜ‰∫´„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü SDoH-GPTÔºå‰∏ÄÁ®ÆÁ∞°ÂñÆ‰∏îÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Â∞çÊØîÁØÑ‰æãÂíåÁ∞°ÊΩîÁöÑÊåáÁ§∫‰æÜËêÉÂèñ SDoHÔºåËÄå‰∏çÈúÄË¶Å‰ª∞Ë≥¥Â§ßÈáèÁöÑÈÜ´ÁôÇÊ®ôË®ªÊàñÊòÇË≤¥ÁöÑ‰∫∫Â∑•‰ªãÂÖ•„ÄÇÂÆÉÂàÜÂà•Âú®ÊôÇÈñìÂíåÊàêÊú¨‰∏äÈÅîÂà∞‰∫ÜÂçÅÂÄçÂíå‰∫åÂçÅÂÄçÁöÑÈôç‰ΩéÔºå‰∏¶‰∏îËàá‰∫∫È°ûÊ®ôË®ªËÄÖÁöÑÂÑ™Áï∞‰∏ÄËá¥ÊÄßÔºåÁî± Cohen's kappa Ê∏¨ÈáèÈ´òÈÅî 0.92„ÄÇSDoH-GPT Âíå XGBoost ÁöÑÂâµÊñ∞ÁµêÂêàÂà©Áî®‰∫ÜÂÖ©ËÄÖÁöÑÂÑ™ÈªûÔºåÁ¢∫‰øù‰∫ÜÈ´òÊ∫ñÁ¢∫Â∫¶ÂíåÈÅãÁÆóÊïàÁéáÔºåÂêåÊôÇÂßãÁµÇÁ∂≠ÊåÅ 0.90+ ÁöÑ AUROC ÂàÜÊï∏„ÄÇÂú®‰∏âÂÄã‰∏çÂêåÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶Â∑≤Á∂ìÁ¢∫Ë™ç‰∫ÜÂÆÉÁöÑÁ©©ÂÅ•ÊÄßÂíåÊ∫ñÁ¢∫ÊÄß„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Á™ÅÈ°Ø‰∫ÜÂà©Áî® LLM ‰æÜÈù©Êñ∞ÈÜ´ÁôÇÁ≠ÜË®òÂàÜÈ°ûÁöÑÊΩõÂäõÔºåÂ±ïÁ§∫‰∫ÜÂÆÉÂÄëÂú®È°ØËëóÊ∏õÂ∞ëÊôÇÈñìÂíåÊàêÊú¨ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈ´òÂ∫¶Ê∫ñÁ¢∫ÂàÜÈ°ûÁöÑËÉΩÂäõ„ÄÇ

##### **SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing**
2407.16999v1 by Changchang Yin, Pin-Yu Chen, Bingsheng Yao, Dakuo Wang, Jeffrey Caterino, Ping Zhang

Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis
onset prediction and diagnosis could significantly improve the survival of
sepsis patients. Existing predictive models are usually trained on high-quality
data with few missing information, while missing values widely exist in
real-world clinical scenarios (especially in the first hours of admissions to
the hospital), which causes a significant decrease in accuracy and an increase
in uncertainty for the predictive models. The common method to handle missing
values is imputation, which replaces the unavailable variables with estimates
from the observed data. The uncertainty of imputation results can be propagated
to the sepsis prediction outputs, which have not been studied in existing works
on either sepsis prediction or uncertainty quantification. In this study, we
first define such propagated uncertainty as the variance of prediction output
and then introduce uncertainty propagation methods to quantify the propagated
uncertainty. Moreover, for the potential high-risk patients with low confidence
due to limited observations, we propose a robust active sensing algorithm to
increase confidence by actively recommending clinicians to observe the most
informative variables. We validate the proposed models in both publicly
available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The
Ohio State University Wexner Medical Center (OSUWMC). The experimental results
show that the propagated uncertainty is dominant at the beginning of admissions
to hospitals and the proposed algorithm outperforms state-of-the-art active
sensing methods. Finally, we implement a SepsisLab system for early sepsis
prediction and active sensing based on our pre-trained models. Clinicians and
potential sepsis patients can benefit from the system in early prediction and
diagnosis of sepsis.

ÊëòË¶ÅÔºöÊïóË°ÄÁóáÊòØÁæéÂúãÈÜ´Èô¢‰∏≠Ê≠ª‰∫°ÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÊïóË°ÄÁóáÁöÑÊó©ÊúüÁôº‰ΩúÈ†êÊ∏¨ÂíåË®∫Êñ∑ÂèØ‰ª•È°ØËëóÊèêÈ´òÊïóË°ÄÁóáÊÇ£ËÄÖÁöÑÂ≠òÊ¥ªÁéá„ÄÇÁèæÊúâÁöÑÈ†êÊ∏¨Ê®°ÂûãÈÄöÂ∏∏Âú®Ë≥áÊñôÂìÅË≥™È´ò‰∏îÈÅ∫Â§±Ë≥áË®äËºÉÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åË®ìÁ∑¥ÔºåËÄåÈÅ∫Â§±ÂÄºÂú®ÂØ¶ÈöõËá®Â∫äÊÉÖÂ¢É‰∏≠ÊôÆÈÅçÂ≠òÂú®ÔºàÂ∞§ÂÖ∂ÊòØÂú®ÂÖ•Èô¢ÁöÑÂâçÂπæÂÄãÂ∞èÊôÇÔºâÔºåÈÄôÊúÉÂ∞éËá¥È†êÊ∏¨Ê®°ÂûãÁöÑÊ∫ñÁ¢∫Â∫¶È°ØËëó‰∏ãÈôçÔºå‰∏¶Â¢ûÂä†‰∏çÁ¢∫ÂÆöÊÄß„ÄÇËôïÁêÜÈÅ∫Â§±ÂÄºÁöÑÂ∏∏Ë¶ãÊñπÊ≥ïÊòØÂÖßÊèíÔºåÂÆÉ‰ΩøÁî®ÂæûËßÄÊ∏¨Ë≥áÊñô‰∏≠‰º∞Ë®àÁöÑÊï∏ÂÄºÂèñ‰ª£‰∏çÂèØÁî®ÁöÑËÆäÊï∏„ÄÇÂÖßÊèíÁµêÊûúÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂèØËÉΩÊúÉÂÇ≥Êí≠Âà∞ÊïóË°ÄÁóáÈ†êÊ∏¨Ëº∏Âá∫ÔºåÈÄôÂú®ÁèæÊúâÁöÑÊïóË°ÄÁóáÈ†êÊ∏¨Êàñ‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÁ†îÁ©∂‰∏≠Â∞öÊú™Ë¢´Êé¢Ë®é„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÂ∞áÈÄôÁ®ÆÂÇ≥Êí≠ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂÆöÁæ©ÁÇ∫È†êÊ∏¨Ëº∏Âá∫ÁöÑËÆäÁï∞ÔºåÁÑ∂ÂæåÂºïÂÖ•‰∏çÁ¢∫ÂÆöÊÄßÂÇ≥Êí≠ÊñπÊ≥ï‰æÜÈáèÂåñÂÇ≥Êí≠ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÊ≠§Â§ñÔºåÂ∞çÊñºÁî±ÊñºËßÄÂØüÊúâÈôêËÄåÂ∞éËá¥‰ø°ÂøÉËºÉ‰ΩéÁöÑÊΩõÂú®È´òÈ¢®Èö™ÊÇ£ËÄÖÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂº∑Â§ßÁöÑ‰∏ªÂãïÊÑüÊ∏¨ÊºîÁÆóÊ≥ïÔºåÈÄèÈÅé‰∏ªÂãïÂª∫Ë≠∞Ëá®Â∫äÈÜ´ÁîüËßÄÂØüÊúÄÊúâË≥áË®äÊÄßÁöÑËÆäÊï∏‰æÜÂ¢ûÂä†‰ø°ÂøÉ„ÄÇÊàëÂÄëÂú®ÂÖ¨ÈñãË≥áÊñôÔºà‰æãÂ¶Ç MIMIC-III Âíå AmsterdamUMCdbÔºâÂíå‰øÑ‰∫•‰øÑÂ∑ûÁ´ãÂ§ßÂ≠∏ÈüãÂÖãÊñØÁ¥çÈÜ´Â≠∏‰∏≠ÂøÉ (OSUWMC) ÁöÑÂ∞àÊúâË≥áÊñô‰∏≠È©óË≠â‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ®°Âûã„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂÇ≥Êí≠ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂú®ÂÖ•Èô¢ÂàùÊúü‰Ωî‰∏ªÂ∞éÂú∞‰ΩçÔºåËÄåÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ïÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ‰∏ªÂãïÊÑüÊ∏¨ÊñπÊ≥ï„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊ†πÊìöÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÊïóË°ÄÁóáÂØ¶È©óÂÆ§Á≥ªÁµ±ÔºåÁî®ÊñºÊó©ÊúüÊïóË°ÄÁóáÈ†êÊ∏¨Âíå‰∏ªÂãïÊÑüÊ∏¨„ÄÇËá®Â∫äÈÜ´ÁîüÂíåÊΩõÂú®ÁöÑÊïóË°ÄÁóáÊÇ£ËÄÖÂèØ‰ª•Âú®ÊïóË°ÄÁóáÁöÑÊó©ÊúüÈ†êÊ∏¨ÂíåË®∫Êñ∑‰∏≠ÂèóÁõäÊñºË©≤Á≥ªÁµ±„ÄÇ

##### **Toward an Integrated Decision Making Framework for Optimized Stroke Diagnosis with DSA and Treatment under Uncertainty**
2407.16962v1 by Nur Ahmad Khatim, Ahmad Azmul Asmar Irfan, Amaliya Mata'ul Hayah, Mansur M. Arief

This study addresses the challenge of stroke diagnosis and treatment under
uncertainty, a critical issue given the rapid progression and severe
consequences of stroke conditions such as aneurysms, arteriovenous
malformations (AVM), and occlusions. Current diagnostic methods, including
Digital Subtraction Angiography (DSA), face limitations due to high costs and
its invasive nature. To overcome these challenges, we propose a novel approach
using a Partially Observable Markov Decision Process (POMDP) framework. Our
model integrates advanced diagnostic tools and treatment approaches with a
decision-making algorithm that accounts for the inherent uncertainties in
stroke diagnosis. Our approach combines noisy observations from CT scans,
Siriraj scores, and DSA reports to inform the subsequent treatment options. We
utilize the online solver DESPOT, which employs tree-search methods and
particle filters, to simulate potential future scenarios and guide our
strategies. The results indicate that our POMDP framework balances diagnostic
and treatment objectives, striking a tradeoff between the need for precise
stroke identification via invasive procedures like DSA and the constraints of
limited healthcare resources that necessitate more cost-effective strategies,
such as in-hospital or at-home observation, by relying only relying on
simulation rollouts and not imposing any prior knowledge. Our study offers a
significant contribution by presenting a systematic framework that optimally
integrates diagnostic and treatment processes for stroke and accounting for
various uncertainties, thereby improving care and outcomes in stroke
management.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÂú®‰∏çÁ¢∫ÂÆöÊÄß‰∏ã‰∏≠È¢®ÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇÁöÑÊåëÊà∞ÔºåÈÄôÊòØËÄÉÈáèÂà∞‰∏≠È¢®ÁãÄÊ≥ÅÔºà‰æãÂ¶ÇÂãïËÑàÁò§„ÄÅÂãïÈùúËÑàÁï∏ÂΩ¢ (AVM) ÂíåÈòªÂ°ûÔºâÁöÑÂø´ÈÄüÈÄ≤Â±ïÂíåÂö¥ÈáçÂæåÊûúËÄåÂá∫ÁèæÁöÑÈóúÈçµÂïèÈ°å„ÄÇÁõÆÂâçÁöÑË®∫Êñ∑ÊñπÊ≥ïÔºàÂåÖÊã¨Êï∏‰ΩçÊ∏õÂΩ±Ë°ÄÁÆ°ÊîùÂΩ± (DSA)ÔºâÁî±ÊñºÊàêÊú¨È´òÊòÇÂíå‰æµÂÖ•ÊÄßËÄåÈù¢Ëá®ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ÈÉ®ÂàÜÂèØËßÄÂØüÈ¶¨ÂèØÂ§´Ê±∫Á≠ñÈÅéÁ®ã (POMDP) Êû∂ÊßãÁöÑÊñ∞Á©éÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫ÜÂÖàÈÄ≤ÁöÑË®∫Êñ∑Â∑•ÂÖ∑ÂíåÊ≤ªÁôÇÊñπÊ≥ïÔºå‰ª•Âèä‰∏ÄÂÄãÊ±∫Á≠ñÊºîÁÆóÊ≥ïÔºåË©≤ÊºîÁÆóÊ≥ïËÄÉÈáè‰∫Ü‰∏≠È¢®Ë®∫Êñ∑‰∏≠Âõ∫ÊúâÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁµêÂêà‰∫Ü‰æÜËá™ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè„ÄÅSiriraj Ë©ïÂàÜÂíå DSA Â†±ÂëäÁöÑÈõúË®äËßÄÊ∏¨ÂÄºÔºå‰ª•ÂëäÁü•ÂæåÁ∫åÁöÑÊ≤ªÁôÇÈÅ∏È†Ö„ÄÇÊàëÂÄëÂà©Áî®Á∑ö‰∏äÊ±ÇËß£Âô® DESPOTÔºåÂÆÉÊé°Áî®Ê®πÁãÄÊêúÂ∞ãÊñπÊ≥ïÂíåÁ≤íÂ≠êÊøæÊ≥¢Âô®ÔºåÊ®°Êì¨ÊΩõÂú®ÁöÑÊú™‰æÜÊÉÖÂ¢É‰∏¶ÊåáÂ∞éÊàëÂÄëÁöÑÁ≠ñÁï•„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ POMDP Êû∂ÊßãÂπ≥Ë°°‰∫ÜË®∫Êñ∑ÂíåÊ≤ªÁôÇÁõÆÊ®ôÔºåÂú®ÈÄèÈÅé DSA Á≠â‰æµÂÖ•ÊÄßÁ®ãÂ∫èÁ≤æÁ¢∫Ë≠òÂà•‰∏≠È¢®ÁöÑÈúÄÊ±ÇËàáÈúÄË¶ÅÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÁ≠ñÁï•Ôºà‰æãÂ¶Ç‰ΩèÈô¢ÊàñÂ±ÖÂÆ∂ËßÄÂØüÔºâÁöÑÈÜ´ÁôÇË≥áÊ∫êÈôêÂà∂‰πãÈñìÂèñÂæóÂπ≥Ë°°ÔºåÂÉÖ‰æùË≥¥Ê®°Êì¨Êé®Âá∫‰∏î‰∏çÊñΩÂä†‰ªª‰ΩïÂÖàÈ©óÁü•Ë≠ò„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄãÁ≥ªÁµ±ÊÄßÊû∂ÊßãÔºåÊúÄ‰Ω≥ÂåñÊï¥Âêà‰∏≠È¢®ÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇÈÅéÁ®ã‰∏¶ËÄÉÈáèÂêÑÁ®Æ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂæûËÄåÊîπÂñÑ‰∏≠È¢®ÁÆ°ÁêÜÁöÑÁÖßË≠∑ÂíåÁµêÊûúÔºåÂÅöÂá∫‰∫ÜÈáçÂ§ßË≤¢Áçª„ÄÇ

##### **AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical Knowledge Graphs and Data-Driven Quantification**
2407.16822v1 by Yuheng Wang, Tianze Yu, Jiayue Cai, Sunil Kalia, Harvey Lui, Z. Jane Wang, Tim K. Lee

The 7-point checklist (7PCL) is widely used in dermoscopy to identify
malignant melanoma lesions needing urgent medical attention. It assigns point
values to seven attributes: major attributes are worth two points each, and
minor ones are worth one point each. A total score of three or higher prompts
further evaluation, often including a biopsy. However, a significant limitation
of current methods is the uniform weighting of attributes, which leads to
imprecision and neglects their interconnections. Previous deep learning studies
have treated the prediction of each attribute with the same importance as
predicting melanoma, which fails to recognize the clinical significance of the
attributes for melanoma. To address these limitations, we introduce a novel
diagnostic method that integrates two innovative elements: a Clinical
Knowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy
with Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL
attributes with diagnostic information, revealing both internal and external
associations. By employing adaptive receptive domains and weighted edges, we
establish connections among melanoma's relevant features. Concurrently, GD-DDW
emulates dermatologists' diagnostic processes, who first observe the visual
characteristics associated with melanoma and then make predictions. Our model
uses two imaging modalities for the same lesion, ensuring comprehensive feature
acquisition. Our method shows outstanding performance in predicting malignant
melanoma and its features, achieving an average AUC value of 85%. This was
validated on the EDRA dataset, the largest publicly available dataset for the
7-point checklist algorithm. Specifically, the integrated weighting system can
provide clinicians with valuable data-driven benchmarks for their evaluations.

ÊëòË¶ÅÔºö7 ÈªûÊ™¢Êü•Ë°® (7PCL) Âª£Ê≥õÁî®ÊñºÁöÆËÜöÈè°Ê™¢Êü•Ôºå‰ª•Ë≠òÂà•ÈúÄË¶ÅÁ∑äÊÄ•ÈÜ´ÁôÇÁÖßË≠∑ÁöÑÊÉ°ÊÄßÈªëËâ≤Á¥†Áò§ÁóÖÁÅ∂„ÄÇÂÆÉÁÇ∫‰∏ÉÂÄãÂ±¨ÊÄßÂàÜÈÖçÂàÜÊï∏Ôºö‰∏ªË¶ÅÂ±¨ÊÄßÂêÑÂÄºÂÖ©ÂàÜÔºåÊ¨°Ë¶ÅÂ±¨ÊÄßÂêÑÂÄº‰∏ÄÂàÜ„ÄÇÁ∏ΩÂàÜÁÇ∫‰∏âÊàñ‰ª•‰∏äË°®Á§∫ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Ë©ï‰º∞ÔºåÈÄöÂ∏∏ÂåÖÊã¨Ê¥ªÊ™¢„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÊñπÊ≥ïÁöÑ‰∏ÄÂÄãÈáçÂ§ßÈôêÂà∂ÊòØÂ±¨ÊÄßÁöÑÁµ±‰∏ÄÂä†Ê¨äÔºåÂ∞éËá¥‰∏çÁ≤æÁ¢∫‰∏îÂøΩÁï•ÂÆÉÂÄë‰πãÈñìÁöÑÁõ∏‰∫íÈóúËÅØ„ÄÇÂÖàÂâçÁöÑÊ∑±Â∫¶Â≠∏ÁøíÁ†îÁ©∂Â∞áÊØèÂÄãÂ±¨ÊÄßÁöÑÈ†êÊ∏¨Ë¶ñÁÇ∫ËàáÈ†êÊ∏¨ÈªëËâ≤Á¥†Áò§ÂêåÁ≠âÈáçË¶ÅÔºåÈÄôÊú™ËÉΩË™çË≠òÂà∞Â±¨ÊÄßÂ∞çÈªëËâ≤Á¥†Áò§ÁöÑËá®Â∫äÊÑèÁæ©„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞ÁöÑË®∫Êñ∑ÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÂÖ©ÂÄãÂâµÊñ∞ÂÖÉÁ¥†ÔºöÂü∫ÊñºËá®Â∫äÁü•Ë≠òÁöÑÊãìÊí≤Âúñ (CKTG) ÂíåÂÖ∑ÊúâÊï∏ÊìöÈ©ÖÂãïÂä†Ê¨äÊ®ôÊ∫ñÁöÑÊ¢ØÂ∫¶Ë®∫Êñ∑Á≠ñÁï• (GD-DDW)„ÄÇCKTG Â∞á 7PCL Â±¨ÊÄßËàáË®∫Êñ∑‰ø°ÊÅØÊï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÊè≠Á§∫‰∫ÜÂÖßÈÉ®ÂíåÂ§ñÈÉ®ÈóúËÅØ„ÄÇÈÄöÈÅéÊé°Áî®Ëá™ÈÅ©ÊáâÊÑüÂèóÂüüÂíåÂä†Ê¨äÈÇäÁ∑£ÔºåÊàëÂÄëÂª∫Á´ã‰∫ÜÈªëËâ≤Á¥†Áò§Áõ∏ÈóúÁâπÂæµ‰πãÈñìÁöÑËÅØÁπ´„ÄÇÂêåÊôÇÔºåGD-DDW Ê®°‰ªøÁöÆËÜöÁßëÈÜ´ÁîüÁöÑË®∫Êñ∑ÈÅéÁ®ãÔºå‰ªñÂÄëÈ¶ñÂÖàËßÄÂØüËàáÈªëËâ≤Á¥†Áò§Áõ∏ÈóúÁöÑË¶ñË¶∫ÁâπÂæµÔºåÁÑ∂ÂæåÂÅöÂá∫È†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞çÂêå‰∏ÄÂÄãÁóÖÁÅ∂‰ΩøÁî®ÂÖ©Á®ÆÊàêÂÉèÊñπÂºèÔºåÁ¢∫‰øùÂÖ®Èù¢Áç≤ÂèñÁâπÂæµ„ÄÇÊàëÂÄëÁöÑÈÄôÁ®ÆÊñπÊ≥ïÂú®È†êÊ∏¨ÊÉ°ÊÄßÈªëËâ≤Á¥†Áò§ÂèäÂÖ∂ÁâπÂæµÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÂπ≥Âùá AUC ÂÄºÈÅîÂà∞ 85%„ÄÇÈÄôÂ∑≤Âú® EDRA Êï∏ÊìöÈõÜ‰∏äÂæóÂà∞È©óË≠âÔºåË©≤Êï∏ÊìöÈõÜÊòØ 7 ÈªûÊ™¢Êü•Ë°®ÁÆóÊ≥ïÊúÄÂ§ßÁöÑÂÖ¨ÈñãÂèØÁî®Êï∏ÊìöÈõÜ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈõÜÊàêÁöÑÂä†Ê¨äÁ≥ªÁµ±ÂèØ‰ª•ÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõÊúâÂÉπÂÄºÁöÑÊï∏ÊìöÈ©ÖÂãïÂü∫Ê∫ñÔºå‰æõ‰ªñÂÄëË©ï‰º∞„ÄÇ

##### **Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges**
2407.16804v1 by Zahraa Al Sahili, Ioannis Patras, Matthew Purver

The application of machine learning (ML) in detecting, diagnosing, and
treating mental health disorders is garnering increasing attention.
Traditionally, research has focused on single modalities, such as text from
clinical notes, audio from speech samples, or video of interaction patterns.
Recently, multimodal ML, which combines information from multiple modalities,
has demonstrated significant promise in offering novel insights into human
behavior patterns and recognizing mental health symptoms and risk factors.
Despite its potential, multimodal ML in mental health remains an emerging
field, facing several complex challenges before practical applications can be
effectively developed. This survey provides a comprehensive overview of the
data availability and current state-of-the-art multimodal ML applications for
mental health. It discusses key challenges that must be addressed to advance
the field. The insights from this survey aim to deepen the understanding of the
potential and limitations of multimodal ML in mental health, guiding future
research and development in this evolving domain.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâÂú®ÂÅµÊ∏¨„ÄÅË®∫Êñ∑ÂíåÊ≤ªÁôÇÂøÉÁêÜÂÅ•Â∫∑ÁñæÁóÖ‰∏≠ÁöÑÊáâÁî®Ê≠£Ë∂ä‰æÜË∂äÂèóÂà∞ÈáçË¶ñ„ÄÇÂÇ≥Áµ±‰∏äÔºåÁ†îÁ©∂ËëóÈáçÊñºÂñÆ‰∏ÄÊ®°ÂºèÔºå‰æãÂ¶ÇËá®Â∫äÁ≠ÜË®ò‰∏≠ÁöÑÊñáÂ≠ó„ÄÅË™ûÈü≥Ê®£Êú¨‰∏≠ÁöÑÈü≥Ë®äÊàñ‰∫íÂãïÊ®°ÂºèÁöÑÂΩ±Áâá„ÄÇÊúÄËøëÔºåÁµêÂêàÂ§öÊ®°ÂºèË≥áË®äÁöÑÂ§öÊ®°ÊÖã ML Â∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÊΩõÂäõÔºåÂèØÊèê‰æõÂ∞ç‰∫∫È°ûË°åÁÇ∫Ê®°ÂºèÁöÑÊñ∞Ë¶ãËß£Ôºå‰∏¶Ë≠òÂà•ÂøÉÁêÜÂÅ•Â∫∑ÁóáÁãÄÂíåÈ¢®Èö™Âõ†Â≠ê„ÄÇÂÑòÁÆ°ÂÖ∑ÊúâÊΩõÂäõÔºåÂøÉÁêÜÂÅ•Â∫∑‰∏≠ÁöÑÂ§öÊ®°ÊÖã ML ‰ªçÊòØ‰∏ÄÂÄãÊñ∞ËààÈ†òÂüüÔºåÂú®ÂØ¶ÈöõÊáâÁî®ÂèØ‰ª•ÊúâÊïàÈñãÁôº‰πãÂâçÔºåÈù¢Ëá®Êï∏È†ÖË§áÈõúÊåëÊà∞„ÄÇÊú¨Ë™øÊü•Êèê‰æõ‰∫ÜÂøÉÁêÜÂÅ•Â∫∑‰∏≠Ë≥áÊñôÂèØÁî®ÊÄßÂíåÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÂ§öÊ®°ÊÖã ML ÊáâÁî®‰πãÂÖ®Èù¢Ê¶ÇËßÄ„ÄÇÂÆÉË®éË´ñ‰∫ÜÂøÖÈ†àËß£Ê±∫ÁöÑÈóúÈçµÊåëÊà∞Ôºå‰ª•Êé®ÂãïË©≤È†òÂüüÁöÑÈÄ≤Ê≠•„ÄÇÊú¨Ë™øÊü•ÁöÑË¶ãËß£Êó®Âú®Âä†Ê∑±Â∞çÂ§öÊ®°ÊÖã ML Âú®ÂøÉÁêÜÂÅ•Â∫∑‰∏≠ÁöÑÊΩõÂäõÂíåÈôêÂà∂ÁöÑÁêÜËß£ÔºåÂºïÂ∞éÈÄôÂÄã‰∏çÊñ∑ÊºîËÆäÈ†òÂüüÊú™‰æÜÁöÑÁ†îÁ©∂ÂíåÁôºÂ±ï„ÄÇ

##### **Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging**
2407.16608v1 by Daniela L. Ramos, Hector J. Hortua

Colorectal polyps are generally benign alterations that, if not identified
promptly and managed successfully, can progress to cancer and cause
affectations on the colon mucosa, known as adenocarcinoma. Today advances in
Deep Learning have demonstrated the ability to achieve significant performance
in image classification and detection in medical diagnosis applications.
Nevertheless, these models are prone to overfitting, and making decisions based
only on point estimations may provide incorrect predictions. Thus, to obtain a
more informed decision, we must consider point estimations along with their
reliable uncertainty quantification. In this paper, we built different Bayesian
neural network approaches based on the flexibility of posterior distribution to
develop semantic segmentation of colorectal polyp images. We found that these
models not only provide state-of-the-art performance on the segmentation of
this medical dataset but also, yield accurate uncertainty estimates. We applied
multiplicative normalized flows(MNF) and reparameterization trick on the UNET,
FPN, and LINKNET architectures tested with multiple backbones in deterministic
and Bayesian versions. We report that the FPN + EfficientnetB7 architecture
with MNF is the most promising option given its IOU of 0.94 and Expected
Calibration Error (ECE) of 0.004, combined with its superiority in identifying
difficult-to-detect colorectal polyps, which is effective in clinical areas
where early detection prevents the development of colon cancer.

ÊëòË¶ÅÔºöÂ§ßËÖ∏ÊÅØËÇâÈÄöÂ∏∏ÊòØËâØÊÄßÁóÖËÆäÔºåÂ¶ÇÊûú‰∏çÂèäÊôÇÁôºÁèæ‰∏¶ÊàêÂäüËôïÁêÜÔºåÂèØËÉΩÊúÉÊºîËÆäÊàêÁôåÁóá‰∏¶Â∞éËá¥Â§ßËÖ∏Á≤òËÜúÂèóÁ¥ØÔºåÂç≥ËÖ∫Áôå„ÄÇÂ¶Ç‰ªäÔºåÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈÄ≤Â±ïÂ∑≤Ë≠âÊòéÊúâËÉΩÂäõÂú®ÈÜ´ÁôÇË®∫Êñ∑ÊáâÁî®‰∏≠ÂØ¶ÁèæÂúñÂÉèÂàÜÈ°ûÂíåÊ™¢Ê∏¨ÁöÑÈ°ØËëóÊÄßËÉΩ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊ®°ÂûãÂÆπÊòìÈÅéÂ∫¶Êì¨ÂêàÔºå‰∏¶‰∏îÂÉÖÂü∫ÊñºÈªû‰º∞Ë®àÂÅöÂá∫Ê±∫Á≠ñÂèØËÉΩÊúÉÊèê‰æõ‰∏çÊ≠£Á¢∫ÁöÑÈ†êÊ∏¨„ÄÇÂõ†Ê≠§ÔºåÁÇ∫‰∫ÜÁç≤ÂæóÊõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºåÊàëÂÄëÂøÖÈ†àËÄÉÊÖÆÈªû‰º∞Ë®àÂèäÂÖ∂ÂèØÈù†ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂü∫ÊñºÂæåÈ©óÂàÜ‰ΩàÁöÑÈùàÊ¥ªÊÄßÊßãÂª∫‰∫Ü‰∏çÂêåÁöÑË≤ùËëâÊñØÁ•ûÁ∂ìÁ∂≤Áµ°ÊñπÊ≥ïÔºå‰ª•ÈñãÁôºÂ§ßËÖ∏ÊÅØËÇâÂúñÂÉèÁöÑË™ûÁæ©ÂàÜÂâ≤„ÄÇÊàëÂÄëÁôºÁèæÈÄô‰∫õÊ®°Âûã‰∏çÂÉÖÂú®ÈÄôÂÄãÈÜ´ÁôÇÊï∏ÊìöÈõÜÁöÑÂàÜÂâ≤‰∏äÊèê‰æõ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÈÇÑÁî¢Áîü‰∫ÜÊ∫ñÁ¢∫ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à„ÄÇÊàëÂÄëÂú®Á¢∫ÂÆöÊÄßÂíåË≤ùËëâÊñØÁâàÊú¨‰∏≠‰ΩøÁî®Â§öÂÄã‰∏ªÂππÊ∏¨Ë©¶ÁöÑ UNET„ÄÅFPN Âíå LINKNET Êû∂Êßã‰∏äÊáâÁî®‰πòÊ≥ïÊ≠∏‰∏ÄÂåñÊµÅ (MNF) ÂíåÈáçÊñ∞ÂèÉÊï∏ÂåñÊäÄÂ∑ß„ÄÇÊàëÂÄëÂ†±ÂëäË™™ÔºåÂÖ∑Êúâ MNF ÁöÑ FPN + EfficientnetB7 Êû∂ÊßãÊòØÊúÄÊúâÂ∏åÊúõÁöÑÈÅ∏ÊìáÔºåÂõ†ÁÇ∫ÂÆÉÁöÑ IOU ÁÇ∫ 0.94ÔºåÈ†êÊúüÁöÑÊ†°Ê∫ñË™§Â∑Æ (ECE) ÁÇ∫ 0.004Ôºå‰∏¶‰∏îÂú®Ë≠òÂà•Èõ£‰ª•Ê™¢Ê∏¨ÁöÑÂ§ßËÖ∏ÊÅØËÇâÊñπÈù¢ÂÖ∑ÊúâÂÑ™Ë∂äÊÄßÔºåÈÄôÂú®Êó©ÊúüÊ™¢Ê∏¨ÂèØ‰ª•Èò≤Ê≠¢ÁµêËÖ∏ÁôåÁôºÂ±ïÁöÑËá®Â∫äÈ†òÂüüÊòØÊúâÊïàÁöÑ„ÄÇ

##### **A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions**
2407.16593v1 by Giorgos Lysandrou, Roma English Owen, Vanja Popovic, Grant Le Brun, Aryo Pradipta Gema, Beatrice Alex, Elizabeth A. L. Fairley

There exists an invisible barrier between healthcare professionals'
perception of a patient's clinical experience and the reality. This barrier may
be induced by the environment that hinders patients from sharing their
experiences openly with healthcare professionals. As patients are observed to
discuss and exchange knowledge more candidly on social media, valuable insights
can be leveraged from these platforms. However, the abundance of non-patient
posts on social media necessitates filtering out such irrelevant content to
distinguish the genuine voices of patients, a task we refer to as patient voice
classification. In this study, we analyse the importance of linguistic
characteristics in accurately classifying patient voices. Our findings
underscore the essential role of linguistic and statistical text similarity
analysis in identifying common patterns among patient groups. These results
allude to even starker differences in the way patients express themselves at a
disease level and across various therapeutic domains. Additionally, we
fine-tuned a pre-trained Language Model on the combined datasets with similar
linguistic patterns, resulting in a highly accurate automatic patient voice
classification. Being the pioneering study on the topic, our focus on
extracting authentic patient experiences from social media stands as a crucial
step towards advancing healthcare standards and fostering a patient-centric
approach.

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Â∞çÊñºÊÇ£ËÄÖËá®Â∫äÁ∂ìÈ©óÁöÑË™çÁü•ËàáÂØ¶ÈöõÊÉÖÊ≥Å‰πãÈñìÂ≠òÂú®Ëëó‰∏ÄÈÅìÁÑ°ÂΩ¢ÁöÑÈöúÁ§ô„ÄÇÊ≠§ÈöúÁ§ôÂèØËÉΩÊòØÁî±Áí∞Â¢ÉÊâÄÈÄ†ÊàêÔºåÈòªÁ§ôÊÇ£ËÄÖËàáÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°ÂÖ¨ÈñãÂàÜ‰∫´‰ªñÂÄëÁöÑÁ∂ìÈ©ó„ÄÇÁî±ÊñºËßÄÂØüÂà∞ÊÇ£ËÄÖÂú®Á§æÁæ§Â™íÈ´î‰∏äÊõ¥Âù¶ÁéáÂú∞Ë®éË´ñÂíå‰∫§ÊèõÁü•Ë≠òÔºåÂõ†Ê≠§ÂèØ‰ª•ÂæûÈÄô‰∫õÂπ≥Âè∞Áç≤ÂæóÊúâÂÉπÂÄºÁöÑË¶ãËß£„ÄÇÁÑ∂ËÄåÔºåÁ§æÁæ§Â™íÈ´î‰∏äÂÖÖÊñ•ËëóÈùûÊÇ£ËÄÖË≤ºÊñáÔºåÂõ†Ê≠§ÊúâÂøÖË¶ÅÈÅéÊøæÊéâÈÄô‰∫õ‰∏çÁõ∏ÈóúÁöÑÂÖßÂÆπÔºå‰ª•ÂçÄÂàÜÊÇ£ËÄÖÁöÑÁúüÂØ¶ËÅ≤Èü≥ÔºåÊàëÂÄëÂ∞áÊ≠§‰ªªÂãôÁ®±ÁÇ∫ÊÇ£ËÄÖËÅ≤Èü≥ÂàÜÈ°û„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫ÜË™ûË®ÄÁâπÂæµÂú®Ê∫ñÁ¢∫ÂàÜÈ°ûÊÇ£ËÄÖËÅ≤Èü≥‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜË™ûË®ÄÂíåÁµ±Ë®àÊñáÂ≠óÁõ∏‰ººÊÄßÂàÜÊûêÂú®Ë≠òÂà•ÊÇ£ËÄÖÁæ§ÁµÑ‰πãÈñìÂÖ±ÂêåÊ®°Âºè‰∏≠ÁöÑÈáçË¶ÅËßíËâ≤„ÄÇÈÄô‰∫õÁµêÊûúÊöóÁ§∫‰∫ÜÊÇ£ËÄÖÂú®ÁñæÁóÖÂ±§Á¥öÂíåÂêÑÁ®ÆÊ≤ªÁôÇÈ†òÂüü‰∏≠Ë°®ÈÅîËá™Â∑±ÁöÑÊñπÂºèÂ≠òÂú®ËëóÊõ¥ÊòéÈ°ØÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ†πÊìöÂÖ∑ÊúâÈ°û‰ººË™ûË®ÄÊ®°ÂºèÁöÑÂêà‰ΩµË≥áÊñôÈõÜÂæÆË™ø‰∫ÜÈ†êÂÖàË®ìÁ∑¥Â•ΩÁöÑË™ûË®ÄÊ®°ÂûãÔºåÈÄ≤ËÄåÁî¢ÁîüÈ´òÂ∫¶Ê∫ñÁ¢∫ÁöÑËá™ÂãïÊÇ£ËÄÖËÅ≤Èü≥ÂàÜÈ°û„ÄÇ‰ΩúÁÇ∫ÈÄôÈ†Ö‰∏ªÈ°åÁöÑÈñãÂâµÊÄßÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂæûÁ§æÁæ§Â™íÈ´î‰∏≠ÊèêÂèñÁúüÂØ¶ÁöÑÊÇ£ËÄÖÁ∂ìÈ©óÔºåÈÄôÊòØÈÇÅÂêëÊèêÂçáÈÜ´ÁôÇ‰øùÂÅ•Ê®ôÊ∫ñÂíåÂüπÈ§ä‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÈÄîÂæëÁöÑÈóúÈçµ‰∏ÄÊ≠•„ÄÇ

##### **Prompt Injection Attacks on Large Language Models in Oncology**
2407.18981v1 by Jan Clusmann, Dyke Ferber, Isabella C. Wiest, Carolin V. Schneider, Titus J. Brinker, Sebastian Foersch, Daniel Truhn, Jakob N. Kather

Vision-language artificial intelligence models (VLMs) possess medical
knowledge and can be employed in healthcare in numerous ways, including as
image interpreters, virtual scribes, and general decision support systems.
However, here, we demonstrate that current VLMs applied to medical tasks
exhibit a fundamental security flaw: they can be attacked by prompt injection
attacks, which can be used to output harmful information just by interacting
with the VLM, without any access to its parameters. We performed a quantitative
study to evaluate the vulnerabilities to these attacks in four state of the art
VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus,
Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show
that all of these models are susceptible. Specifically, we show that embedding
sub-visual prompts in medical imaging data can cause the model to provide
harmful output, and that these prompts are non-obvious to human observers.
Thus, our study demonstrates a key vulnerability in medical VLMs which should
be mitigated before widespread clinical adoption.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®Ä‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÔºàVLMÔºâÂÖ∑ÂÇôÈÜ´ÁôÇÁü•Ë≠òÔºåÂèØÁî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•ÁöÑË®±Â§öÊñπÈù¢ÔºåÂåÖÊã¨ÂΩ±ÂÉèËß£ËÆÄ„ÄÅËôõÊì¨Êõ∏ÂØ´Âì°Âíå‰∏ÄËà¨Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ±„ÄÇ‰∏çÈÅéÔºåÊàëÂÄëÂú®Ê≠§Ë≠âÊòéÊáâÁî®ÊñºÈÜ´ÁôÇ‰ªªÂãôÁöÑÁèæË°å VLM Êúâ‰∏ÄÂÄãÊ†πÊú¨ÊÄßÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºöÂÆÉÂÄëÊúÉÂèóÂà∞ÊèêÁ§∫Ê≥®ÂÖ•ÊîªÊìäÔºåËÄåÈÄôÁ®ÆÊîªÊìäÂè™Ë¶ÅËàá VLM ‰∫íÂãïÔºåÂ∞±ËÉΩÁî®ÊñºËº∏Âá∫ÊúâÂÆ≥Ë≥áË®äÔºåËÄåÁÑ°È†àÂ≠òÂèñÂÖ∂ÂèÉÊï∏„ÄÇÊàëÂÄëÂü∑Ë°å‰∫Ü‰∏ÄÈ†ÖÈáèÂåñÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ÂõõÂÄãÊúÄÂÖàÈÄ≤ÁöÑ VLM Â∞çÈÄô‰∫õÊîªÊìäÁöÑËÑÜÂº±ÊÄßÔºåÈÄô‰∫õ VLM Â∑≤Ë¢´ÊèêË≠∞Áî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•ÔºöClaude 3 Opus„ÄÅClaude 3.5 Sonnet„ÄÅReka Core Âíå GPT-4o„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÁµÑ N=297 ÊîªÊìäÔºåÈ°ØÁ§∫ÊâÄÊúâÈÄô‰∫õÊ®°ÂûãÈÉΩÂÆπÊòìÂèóÂà∞ÊîªÊìä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ°ØÁ§∫Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñô‰∏≠ÂµåÂÖ•Ê¨°Ë¶ñË¶∫ÊèêÁ§∫ÔºåÊúÉÂ∞éËá¥Ê®°ÂûãÊèê‰æõÊúâÂÆ≥Ëº∏Âá∫ÔºåËÄå‰∏îÈÄô‰∫õÊèêÁ§∫Â∞ç‰∫∫È°ûËßÄÂØüËÄÖ‰æÜË™™‰∏¶‰∏çÈ°ØËÄåÊòìË¶ã„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Ë≠âÊòé‰∫ÜÈÜ´ÁôÇ VLM ‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÊºèÊ¥ûÔºåÂú®Âª£Ê≥õËá®Â∫äÊé°Áî®‰πãÂâçÊáâÂä†‰ª•Á∑©Ëß£„ÄÇ

##### **Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management and Usage in Question Answering**
2407.21053v1 by Pralaypati Ta, Bhumika Gupta, Arihant Jain, Sneha Sree C, Keerthi Ram, Mohanasankar Sivaprakasam

An automated knowledge modeling algorithm for Cancer Clinical Practice
Guidelines (CPGs) extracts the knowledge contained in the CPG documents and
transforms it into a programmatically interactable, easy-to-update structured
model with minimal human intervention. The existing automated algorithms have
minimal scope and cannot handle the varying complexity of the knowledge content
in the CPGs for different cancer types. This work proposes an improved
automated knowledge modeling algorithm to create knowledge models from the
National Comprehensive Cancer Network (NCCN) CPGs in Oncology for different
cancer types. The proposed algorithm has been evaluated with NCCN CPGs for four
different cancer types. We also proposed an algorithm to compare the knowledge
models for different versions of a guideline to discover the specific changes
introduced in the treatment protocol of a new version. We created a
question-answering (Q&A) framework with the guideline knowledge models as the
augmented knowledge base to study our ability to query the knowledge models. We
compiled a set of 32 question-answer pairs derived from two reliable data
sources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the
Q&A framework. The framework was evaluated against the question-answer pairs
from one data source, and it can generate the answers with 54.5% accuracy from
the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN
NSCLC guideline knowledge model.

ÊëòË¶ÅÔºö‰∏ÄÁ®ÆÁî®ÊñºÁôåÁóáËá®Â∫äÂØ¶ÂãôÊåáÂçó (CPG) ÁöÑËá™ÂãïÂåñÁü•Ë≠òÂª∫Ê®°ÊºîÁÆóÊ≥ïÔºåÊúÉÂæû CPG Êñá‰ª∂‰∏≠ËêÉÂèñÁü•Ë≠òÔºå‰∏¶Â∞áÂÖ∂ËΩâÊèõÊàê‰∏ÄÂÄãÂèØÁ®ãÂºèÂåñ‰∫íÂãï„ÄÅÊòìÊñºÊõ¥Êñ∞ÁöÑÁµêÊßãÂåñÊ®°ÂûãÔºå‰∏îÂè™ÈúÄÊ•µÂ∞ëÁöÑ‰∫∫ÁÇ∫‰ªãÂÖ•„ÄÇÁèæÊúâÁöÑËá™ÂãïÂåñÊºîÁÆóÊ≥ïÁØÑÂúçÂæàÂ∞èÔºåÁÑ°Ê≥ïËôïÁêÜ‰∏çÂêåÁôåÁóáÈ°ûÂûã CPG ‰∏≠Áü•Ë≠òÂÖßÂÆπÁöÑË§áÈõúÊÄßËÆäÂåñ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊîπËâØÁöÑËá™ÂãïÂåñÁü•Ë≠òÂª∫Ê®°ÊºîÁÆóÊ≥ïÔºå‰ª•ÂæûÂúãÂÆ∂Á∂úÂêàÁôåÁóáÁ∂≤Ë∑Ø (NCCN) ËÖ´Áò§Â≠∏ CPG ‰∏≠Âª∫Á´ã‰∏çÂêåÁôåÁóáÈ°ûÂûãÁöÑÁü•Ë≠òÊ®°Âûã„ÄÇÂ∑≤‰ΩøÁî®ÂõõÁ®Æ‰∏çÂêåÁôåÁóáÈ°ûÂûãÁöÑ NCCN CPG Ë©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ï„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÁî®ÊñºÊØîËºÉÊåáÂçó‰∏çÂêåÁâàÊú¨ÁöÑÁü•Ë≠òÊ®°ÂûãÔºå‰ª•ÊâæÂá∫Êñ∞ÁâàÊú¨Ê≤ªÁôÇÂçîÂÆö‰∏≠Â∞éÂÖ•ÁöÑÁâπÂÆöËÆäÊõ¥„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂïèÁ≠î (Q&A) Êû∂ÊßãÔºåÂÖ∂‰∏≠ÊåáÂçóÁü•Ë≠òÊ®°Âûã‰ΩúÁÇ∫Êì¥ÂÖÖÁöÑÁü•Ë≠òÂ∫´Ôºå‰ª•Á†îÁ©∂ÊàëÂÄëÊü•Ë©¢Áü•Ë≠òÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁ∑®Ë≠Ø‰∫Ü‰∏ÄÁµÑ 32 ÂÄãÂïèÈ°åËß£Á≠îÂ∞çÔºåÈÄô‰∫õÂ∞çÊáâÈóú‰øÇ‰æÜËá™ÂÖ©ÂÄãÂèØÈù†ÁöÑË≥áÊñô‰æÜÊ∫êÔºåÁî®ÊñºÊ≤ªÁôÇÈùûÂ∞èÁ¥∞ËÉûËÇ∫Áôå (NSCLC)Ôºå‰ª•Ë©ï‰º∞ Q&A Êû∂Êßã„ÄÇË©≤Êû∂ÊßãÊ†πÊìö‰æÜËá™‰∏ÄÂÄãË≥áÊñô‰æÜÊ∫êÁöÑÂïèÈ°åËß£Á≠îÂ∞çÈÄ≤Ë°åË©ï‰º∞ÔºåÂÆÉÂèØ‰ª•ÂæûÊ≤ªÁôÇÊºîÁÆóÊ≥ïÁî¢Áîü 54.5% Á≤æÁ¢∫Â∫¶ÁöÑÁ≠îÊ°àÔºå‰∏¶Âæû NCCN NSCLC ÊåáÂçóÁü•Ë≠òÊ®°ÂûãÁöÑË®éË´ñÈÉ®ÂàÜÁî¢Áîü 81.8% Á≤æÁ¢∫Â∫¶ÁöÑÁ≠îÊ°à„ÄÇ

##### **Virtue Ethics For Ethically Tunable Robotic Assistants**
2407.16361v1 by Rajitha Ramanayake, Vivek Nallur

The common consensus is that robots designed to work alongside or serve
humans must adhere to the ethical standards of their operational environment.
To achieve this, several methods based on established ethical theories have
been suggested. Nonetheless, numerous empirical studies show that the ethical
requirements of the real world are very diverse and can change rapidly from
region to region. This eliminates the idea of a universal robot that can fit
into any ethical context. However, creating customised robots for each
deployment, using existing techniques is challenging. This paper presents a way
to overcome this challenge by introducing a virtue ethics inspired
computational method that enables character-based tuning of robots to
accommodate the specific ethical needs of an environment. Using a simulated
elder-care environment, we illustrate how tuning can be used to change the
behaviour of a robot that interacts with an elderly resident in an
ambient-assisted environment. Further, we assess the robot's responses by
consulting ethicists to identify potential shortcomings.

ÊëòË¶ÅÔºö‰∏ÄËà¨ÂÖ±Ë≠òÊòØÔºåË®≠Ë®àÁî®ÊñºËàá‰∫∫È°û‰∏¶ËÇ©Â∑•‰ΩúÊàñÊúçÂãô‰∫∫È°ûÁöÑÊ©üÂô®‰∫∫ÂøÖÈ†àÈÅµÂÆàÂÖ∂ÈÅã‰ΩúÁí∞Â¢ÉÁöÑÈÅìÂæ∑Ê®ôÊ∫ñ„ÄÇÁÇ∫ÈÅîÊàêÊ≠§ÁõÆÁöÑÔºåÂ∑≤ÊèêÂá∫ÂπæÁ®ÆÂü∫ÊñºÊó¢ÂÆöÂÄ´ÁêÜÁêÜË´ñÁöÑÊñπÊ≥ï„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåË®±Â§öÂØ¶Ë≠âÁ†îÁ©∂È°ØÁ§∫ÔºåÁèæÂØ¶‰∏ñÁïåÁöÑÈÅìÂæ∑Ë¶ÅÊ±ÇÈùûÂ∏∏Â§öÂÖÉÔºå‰∏îÂèØËÉΩÂõ†Âú∞ÂçÄËÄåÁï∞ËÄåÂø´ÈÄüÊîπËÆä„ÄÇÈÄôÊ∂àÈô§‰∫ÜÈÄöÁî®Ê©üÂô®‰∫∫ÁöÑÊ¶ÇÂøµÔºåËÄåÈÄöÁî®Ê©üÂô®‰∫∫ÂèØ‰ª•ËûçÂÖ•‰ªª‰ΩïÈÅìÂæ∑ËÑàÁµ°„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®ÁèæÊúâÊäÄË°ìÁÇ∫ÊØèÂÄãÈÉ®ÁΩ≤Âª∫Á´ãÂÆ¢Ë£ΩÂåñÊ©üÂô®‰∫∫ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÖãÊúçÊ≠§ÊåëÊà∞ÁöÑÊñπÊ≥ïÔºåÊñπÊ≥ïÊòØÂºïÂÖ•‰∏ÄÁ®ÆÁæéÂæ∑ÂÄ´ÁêÜÂïüÁôºÁöÑÈÅãÁÆóÊñπÊ≥ïÔºå‰ΩøÊ©üÂô®‰∫∫ËÉΩÂ§†Âü∫ÊñºÁâπË≥™ÈÄ≤Ë°åË™øÊï¥Ôºå‰ª•ÈÅ©ÊáâÁí∞Â¢ÉÁöÑÁâπÂÆöÈÅìÂæ∑ÈúÄÊ±Ç„ÄÇ‰ΩøÁî®Ê®°Êì¨ÁöÑÈï∑ËÄÖÁÖßË≠∑Áí∞Â¢ÉÔºåÊàëÂÄëË™™ÊòéÂ¶Ç‰Ωï‰ΩøÁî®Ë™øÊï¥‰æÜÊîπËÆäÊ©üÂô®‰∫∫Âú®Áí∞Â¢ÉËºîÂä©Áí∞Â¢É‰∏≠ËàáÂπ¥Èï∑‰ΩèÊ∞ë‰∫íÂãïÁöÑË°åÁÇ∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË´ÆË©¢ÂÄ´ÁêÜÂ≠∏ÂÆ∂‰æÜË©ï‰º∞Ê©üÂô®‰∫∫ÁöÑÂèçÊáâÔºå‰ª•ÊâæÂá∫ÊΩõÂú®ÁöÑÁº∫Èªû„ÄÇ

##### **PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**
2407.16329v1 by Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo

Acute stroke demands prompt diagnosis and treatment to achieve optimal
patient outcomes. However, the intricate and irregular nature of clinical data
associated with acute stroke, particularly blood pressure (BP) measurements,
presents substantial obstacles to effective visual analytics and
decision-making. Through a year-long collaboration with experienced
neurologists, we developed PhenoFlow, a visual analytics system that leverages
the collaboration between human and Large Language Models (LLMs) to analyze the
extensive and complex data of acute ischemic stroke patients. PhenoFlow
pioneers an innovative workflow, where the LLM serves as a data wrangler while
neurologists explore and supervise the output using visualizations and natural
language interactions. This approach enables neurologists to focus more on
decision-making with reduced cognitive load. To protect sensitive patient
information, PhenoFlow only utilizes metadata to make inferences and synthesize
executable codes, without accessing raw patient data. This ensures that the
results are both reproducible and interpretable while maintaining patient
privacy. The system incorporates a slice-and-wrap design that employs temporal
folding to create an overlaid circular visualization. Combined with a linear
bar graph, this design aids in exploring meaningful patterns within irregularly
measured BP data. Through case studies, PhenoFlow has demonstrated its
capability to support iterative analysis of extensive clinical datasets,
reducing cognitive load and enabling neurologists to make well-informed
decisions. Grounded in long-term collaboration with domain experts, our
research demonstrates the potential of utilizing LLMs to tackle current
challenges in data-driven clinical decision-making for acute ischemic stroke
patients.

ÊëòË¶ÅÔºö<paragraph>ÊÄ•ÊÄß‰∏≠È¢®ÈúÄË¶ÅËøÖÈÄüË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºåÊâçËÉΩÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑÁóÖ‰∫∫Ê≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåËàáÊÄ•ÊÄß‰∏≠È¢®Áõ∏ÈóúÁöÑËá®Â∫äË≥áÊñôË§áÈõú‰∏î‰∏çË¶èÂâáÔºåÁâπÂà•ÊòØË°ÄÂ£ì (BP) Ê∏¨ÈáèÔºåÂ∞çÊúâÊïàÁöÑË¶ñË¶∫ÂàÜÊûêÂíåÊ±∫Á≠ñÂà∂ÂÆöÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÈÄèÈÅéËàáÁ∂ìÈ©óË±êÂØåÁöÑÁ•ûÁ∂ìÁßëÈÜ´Â∏´Èï∑ÈÅî‰∏ÄÂπ¥ÁöÑÂêà‰ΩúÔºåÊàëÂÄëÈñãÁôº‰∫Ü PhenoFlowÔºåÈÄôÊòØ‰∏ÄÂÄãË¶ñË¶∫ÂàÜÊûêÁ≥ªÁµ±ÔºåÂà©Áî®‰∫∫ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πãÈñìÁöÑÂçî‰Ωú‰æÜÂàÜÊûêÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖÁöÑÂª£Ê≥õ‰∏îË§áÈõúË≥áÊñô„ÄÇPhenoFlow ÈñãÂâµ‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÖ∂‰∏≠ LLM Êìî‰ªªË≥áÊñôÊï¥ÁêÜÂì°ÔºåËÄåÁ•ûÁ∂ìÁßëÈÜ´Â∏´Ââá‰ΩøÁî®Ë¶ñË¶∫ÂåñÂíåËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãï‰æÜÊé¢Á¥¢ÂíåÁõ£Áù£Ëº∏Âá∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†Êõ¥Â∞àÊ≥®ÊñºÊ±∫Á≠ñÂà∂ÂÆöÔºåÂêåÊôÇÈôç‰ΩéË™çÁü•Ë≤†Êìî„ÄÇÁÇ∫‰∫Ü‰øùË≠∑ÊïèÊÑüÁöÑÁóÖ‰∫∫Ë≥áË®äÔºåPhenoFlow ÂÉÖÂà©Áî®ÂÖÉË≥áÊñôÈÄ≤Ë°åÊé®Ë´ñ‰∏¶ÂêàÊàêÂèØÂü∑Ë°åÁ®ãÂºèÁ¢ºÔºåËÄå‰∏çÊúÉÂ≠òÂèñÂéüÂßãÁóÖ‰∫∫Ë≥áÊñô„ÄÇÈÄôÁ¢∫‰øù‰∫ÜÁµêÊûúÊó¢ÂèØÈáçÁèæÂèàÂèØËß£ÈáãÔºåÂêåÊôÇÁ∂≠Ë≠∑ÁóÖ‰∫∫ÁöÑÈö±ÁßÅ„ÄÇË©≤Á≥ªÁµ±Êé°Áî®ÂàÜÊÆµÂíåÂåÖË£ùË®≠Ë®àÔºåÊé°Áî®ÊôÇÈñìÊë∫Áñä‰æÜÂª∫Á´ãÁñäÂä†ÁöÑÂúìÂΩ¢Ë¶ñË¶∫Âåñ„ÄÇÁµêÂêàÁ∑öÊÄßÈï∑Ê¢ùÂúñÔºåÊ≠§Ë®≠Ë®àÊúâÂä©ÊñºÊé¢Á¥¢‰∏çË¶èÂâáÊ∏¨ÈáèË°ÄÂ£ìË≥áÊñô‰∏≠ÁöÑÊúâÊÑèÁæ©Ê®°Âºè„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåPhenoFlow Â∑≤Ë≠âÊòéÂÖ∂ÊîØÊè¥Â∞çÂª£Ê≥õËá®Â∫äË≥áÊñôÈõÜÈÄ≤Ë°åÂèçË¶ÜÂàÜÊûêÁöÑËÉΩÂäõÔºåÈôç‰ΩéË™çÁü•Ë≤†Êìî‰∏¶‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ª•ËàáÈ†òÂüüÂ∞àÂÆ∂Èï∑ÊúüÂêà‰ΩúÁÇ∫Âü∫Á§éÔºåË≠âÊòé‰∫ÜÂà©Áî® LLM ‰æÜÊáâÂ∞çÁï∂ÂâçÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖË≥áÊñôÈ©ÖÂãïËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning**
2407.16715v2 by Yufeng Li, Wenchao Zhao, Bo Dang, Xu Yan, Weimin Wang, Min Gao, Mingxuan Xiao

In clinical treatment, identifying potential adverse reactions of drugs can
help assist doctors in making medication decisions. In response to the problems
in previous studies that features are high-dimensional and sparse, independent
prediction models need to be constructed for each adverse reaction of drugs,
and the prediction accuracy is low, this paper develops an adverse drug
reaction prediction model based on knowledge graph embedding and deep learning,
which can predict experimental results. Unified prediction of adverse drug
reactions covered. Knowledge graph embedding technology can fuse the associated
information between drugs and alleviate the shortcomings of high-dimensional
sparsity in feature matrices, and the efficient training capabilities of deep
learning can improve the prediction accuracy of the model. This article builds
an adverse drug reaction knowledge graph based on drug feature data; by
analyzing the embedding effect of the knowledge graph under different embedding
strategies, the best embedding strategy is selected to obtain sample vectors;
and then a convolutional neural network model is constructed to predict adverse
reactions. The results show that under the DistMult embedding model and
400-dimensional embedding strategy, the convolutional neural network model has
the best prediction effect; the average accuracy, F_1 score, recall rate and
area under the curve of repeated experiments are better than the methods
reported in the literature. The obtained prediction model has good prediction
accuracy and stability, and can provide an effective reference for later safe
medication guidance.

ÊëòË¶ÅÔºöÂú®‰∏¥Â∫äÊ≤ªÁñó‰∏≠ÔºåËØÜÂà´ËçØÁâ©ÊΩúÂú®ÁöÑ‰∏çËâØÂèçÂ∫îÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÂÅöÂá∫Áî®ËçØÂÜ≥Á≠ñ„ÄÇÈíàÂØπ‰ª•ÂæÄÁ†îÁ©∂‰∏≠ÁâπÂæÅÈ´òÁª¥‰∏îÁ®ÄÁñè„ÄÅÈúÄË¶Å‰∏∫ÊØèÁßçËçØÁâ©‰∏çËâØÂèçÂ∫îÊûÑÂª∫Áã¨Á´ãÁöÑÈ¢ÑÊµãÊ®°Âûã„ÄÅÈ¢ÑÊµãÂáÜÁ°ÆÁéá‰ΩéÁ≠âÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁü•ËØÜÂõæË∞±ÂµåÂÖ•ÂíåÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏çËâØËçØÁâ©ÂèçÂ∫îÈ¢ÑÊµãÊ®°ÂûãÔºåÂèØ‰ª•È¢ÑÊµãÂÆûÈ™åÁªìÊûú„ÄÇË¶ÜÁõñ‰∏çËâØËçØÁâ©ÂèçÂ∫îÁöÑÁªü‰∏ÄÈ¢ÑÊµã„ÄÇÁü•ËØÜÂõæË∞±ÂµåÂÖ•ÊäÄÊúØÂèØ‰ª•ËûçÂêàËçØÁâ©‰πãÈó¥ÁöÑÂÖ≥ËÅî‰ø°ÊÅØÔºåÁºìËß£ÁâπÂæÅÁü©Èòµ‰∏≠È´òÁª¥Á®ÄÁñèÁöÑ‰∏çË∂≥ÔºåÊ∑±Â∫¶Â≠¶‰π†È´òÊïàÁöÑËÆ≠ÁªÉËÉΩÂäõÂèØ‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÁéá„ÄÇÊú¨ÊñáÂü∫‰∫éËçØÁâ©ÁâπÂæÅÊï∞ÊçÆÊûÑÂª∫‰∏çËâØËçØÁâ©ÂèçÂ∫îÁü•ËØÜÂõæË∞±ÔºõÈÄöËøáÂàÜÊûêÁü•ËØÜÂõæË∞±Âú®‰∏çÂêåÂµåÂÖ•Á≠ñÁï•‰∏ãÁöÑÂµåÂÖ•ÊïàÊûúÔºåÈÄâÂèñÊúÄ‰ºòÁöÑÂµåÂÖ•Á≠ñÁï•ÂæóÂà∞Ê†∑Êú¨ÂêëÈáèÔºõÁÑ∂ÂêéÊûÑÂª∫Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÈ¢ÑÊµã‰∏çËâØÂèçÂ∫î„ÄÇÁªìÊûúË°®ÊòéÔºåÂú®DistMultÂµåÂÖ•Ê®°ÂûãÂíå400Áª¥Â∫¶ÁöÑÂµåÂÖ•Á≠ñÁï•‰∏ãÔºåÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÁöÑÈ¢ÑÊµãÊïàÊûúÊúÄ‰Ω≥ÔºõÈáçÂ§çÂÆûÈ™åÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéá„ÄÅF_1ÂÄº„ÄÅÂè¨ÂõûÁéáÂíåÊõ≤Á∫ø‰∏ãÈù¢ÁßØÂùá‰ºò‰∫éÊñáÁåÆÊä•ÈÅìÁöÑÊñπÊ≥ï„ÄÇÊâÄËé∑ÂæóÁöÑÈ¢ÑÊµãÊ®°ÂûãÂÖ∑ÊúâËâØÂ•ΩÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÁéáÂíåÁ®≥ÂÆöÊÄßÔºåÂèØ‰ª•‰∏∫ÂêéÊúüÁöÑÂÆâÂÖ®Áî®ËçØÊåáÂØºÊèê‰æõÊúâÊïàÁöÑÂèÇËÄÉ„ÄÇ

##### **Artificial Intelligence-based Decision Support Systems for Precision and Digital Health**
2407.16062v1 by Nina Deliu, Bibhas Chakraborty

Precision health, increasingly supported by digital technologies, is a domain
of research that broadens the paradigm of precision medicine, advancing
everyday healthcare. This vision goes hand in hand with the groundbreaking
advent of artificial intelligence (AI), which is reshaping the way we diagnose,
treat, and monitor both clinical subjects and the general population. AI tools
powered by machine learning have shown considerable improvements in a variety
of healthcare domains. In particular, reinforcement learning (RL) holds great
promise for sequential and dynamic problems such as dynamic treatment regimes
and just-in-time adaptive interventions in digital health. In this work, we
discuss the opportunity offered by AI, more specifically RL, to current trends
in healthcare, providing a methodological survey of RL methods in the context
of precision and digital health. Focusing on the area of adaptive
interventions, we expand the methodological survey with illustrative case
studies that used RL in real practice.
  This invited article has undergone anonymous review and is intended as a book
chapter for the volume "Frontiers of Statistics and Data Science" edited by
Subhashis Ghoshal and Anindya Roy for the International Indian Statistical
Association Series on Statistics and Data Science, published by Springer. It
covers the material from a short course titled "Artificial Intelligence in
Precision and Digital Health" taught by the author Bibhas Chakraborty at the
IISA 2022 Conference, December 26-30 2022, at the Indian Institute of Science,
Bengaluru.

ÊëòË¶ÅÔºö<paragraph>Á≤æÊ∫ñÂÅ•Â∫∑Âú®Êï∏‰ΩçÁßëÊäÄÁöÑÊîØÊíê‰∏ãÊó•ÁõäÊôÆÂèäÔºåÊòØÊì¥Â±ïÁ≤æÊ∫ñÈÜ´ÁôÇÂÖ∏ÁØÑÁöÑÁ†îÁ©∂È†òÂüüÔºå‰øÉÈÄ≤Êó•Â∏∏‰øùÂÅ•„ÄÇÈÄôÂÄãÈ°òÊôØËàá‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÁ™ÅÁ†¥ÊÄßÈÄ≤Â±ïÊÅØÊÅØÁõ∏ÈóúÔºåÂÆÉÊ≠£Âú®ÈáçÂ°ëÊàëÂÄëË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂíåÁõ£ÊéßËá®Â∫äÂèóË©¶ËÄÖÂíå‰∏ÄËà¨Ê∞ëÁúæÁöÑÊñπÂºè„ÄÇÁî±Ê©üÂô®Â≠∏ÁøíÊîØÊè¥ÁöÑ AI Â∑•ÂÖ∑Â∑≤Âú®ÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÂ±ïÁèæÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÁâπÂà•ÊòØÔºåÂº∑ÂåñÂ≠∏Áøí (RL) Â∞çÂ∫èË≤´ÂíåÂãïÊÖãÂïèÈ°åÔºà‰æãÂ¶ÇÂãïÊÖãÊ≤ªÁôÇÊñπÊ°àÂíåÂç≥ÊôÇÈÅ©ÊáâÊÄßÂπ≤È†êÔºâÊ•µÂÖ∑ÁôºÂ±ïÂâçÊôØ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é AIÔºàÊõ¥ÂÖ∑È´îÂú∞Ë™™ÔºåRLÔºâÁÇ∫Áï∂ÂâçÈÜ´ÁôÇ‰øùÂÅ•Ë∂®Âã¢Â∏∂‰æÜÁöÑÂ•ëÊ©üÔºå‰∏¶Êèê‰æõ RL ÊñπÊ≥ïÂú®Á≤æÊ∫ñÂíåÊï∏‰ΩçÂÅ•Â∫∑È†òÂüüÁöÑÊñπÊ≥ïË´ñË™øÊü•„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÈÅ©ÊáâÊÄßÂπ≤È†êÈ†òÂüüÔºå‰∏¶ÈÄèÈÅéÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠‰ΩøÁî® RL ÁöÑË™™ÊòéÊÄßÊ°à‰æãÁ†îÁ©∂Êì¥Â±ïÊñπÊ≥ïË´ñË™øÊü•„ÄÇÈÄôÁØáÂèóÈÇÄÊñáÁ´†Â∑≤Á∂ìÈÅéÂåøÂêçÂØ©Êü•Ôºå‰∏¶‰ΩúÁÇ∫ Subhashis Ghoshal Âíå Anindya Roy Á∑®ËºØÁöÑ„ÄåÁµ±Ë®àÂ≠∏ËàáË≥áÊñôÁßëÂ≠∏ÂâçÊ≤ø„Äç‰∏ÄÊõ∏ÁöÑÁ´†ÁØÄÔºåÁî± Springer Âá∫ÁâàÁöÑÂúãÈöõÂç∞Â∫¶Áµ±Ë®àÂçîÊúÉÁµ±Ë®àÂ≠∏ËàáË≥áÊñôÁßëÂ≠∏Á≥ªÂàóÂè¢Êõ∏ÁôºË°å„ÄÇÂÆÉÊ∂µËìã‰∫ÜÁî±‰ΩúËÄÖ Bibhas Chakraborty Âú®Âç∞Â∫¶Áµ±Ë®àÂçîÊúÉ 2022 Âπ¥ÊúÉË≠∞Ôºà2022 Âπ¥ 12 Êúà 26 Êó•Ëá≥ 30 Êó•ÔºåÊñºÂç∞Â∫¶ÁßëÂ≠∏Èô¢ Bengaluru ËàâË°åÔºâÊïôÊéàÁöÑ„ÄåÁ≤æÊ∫ñËàáÊï∏‰ΩçÂÅ•Â∫∑‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß„ÄçÁü≠ÊúüË™≤Á®ãÁöÑÊïôÊùê„ÄÇ</paragraph>

##### **GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI**
2407.15719v1 by Zhaojie Fang, Shenghao Zhu, Yifei Chen, Binfeng Zou, Fan Jia, Linwei Qiu, Chang Liu, Yiyu Huang, Xiang Feng, Feiwei Qin, Changmiao Wang, Yeru Wang, Jin Fan, Changbiao Chu, Wan-Zhen Wu, Hu Zhao

Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that
often progresses from Mild Cognitive Impairment (MCI), leading to memory loss
and significantly impacting patients' lives. Clinical trials indicate that
early targeted interventions for MCI patients can potentially slow or halt the
development and progression of AD. Previous research has shown that accurate
medical classification requires the inclusion of extensive multimodal data,
such as assessment scales and various neuroimaging techniques like Magnetic
Resonance Imaging (MRI) and Positron Emission Tomography (PET). However,
consistently tracking the diagnosis of the same individual over time and
simultaneously collecting multimodal data poses significant challenges. To
address this issue, we introduce GFE-Mamba, a classifier based on Generative
Feature Extraction (GFE). This classifier effectively integrates data from
assessment scales, MRI, and PET, enabling deeper multimodal fusion. It
efficiently extracts both long and short sequence information and incorporates
additional information beyond the pixel space. This approach not only improves
classification accuracy but also enhances the interpretability and stability of
the model. We constructed datasets of over 3000 samples based on the
Alzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training
process. Our experimental results demonstrate that the GFE-Mamba model is
effective in predicting the conversion from MCI to AD and outperforms several
state-of-the-art methods. Our source code and ADNI dataset processing code are
available at https://github.com/Tinysqua/GFE-Mamba.

ÊëòË¶ÅÔºöÈòøËå≤Êµ∑ÈªòÁóá (AD) ÊòØ‰∏ÄÁ®Æ‰∏çÂèØÈÄÜÁöÑÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºå
ÈÄöÂ∏∏ÊúÉÂæûËºïÂ∫¶Ë™çÁü•ÈöúÁ§ô (MCI) ÊÉ°ÂåñÔºåÂ∞éËá¥Ë®òÊÜ∂ÂäõÊ∏õÈÄÄ
‰∏¶Â∞çÁóÖÊÇ£ÁöÑÁîüÊ¥ªÁî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇËá®Â∫äË©¶È©óÈ°ØÁ§∫Ôºå
ÈáùÂ∞ç MCI ÁóÖÊÇ£ÁöÑÊó©ÊúüÁõÆÊ®ôÊÄßÂπ≤È†êÊé™ÊñΩÔºåÊúâÂèØËÉΩÊ∏õÁ∑©ÊàñÂÅúÊ≠¢
AD ÁöÑÁôºÂ±ïÂíåÊÉ°Âåñ„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåÁ≤æÁ¢∫ÁöÑ
ÈÜ´ÁôÇÂàÜÈ°ûÈúÄË¶ÅÁ¥çÂÖ•Âª£Ê≥õÁöÑÂ§öÊ®°ÂºèÊï∏ÊìöÔºå
‰æãÂ¶ÇË©ï‰º∞ÈáèË°®ÂíåÂêÑÁ®ÆÁ•ûÁ∂ìÂΩ±ÂÉèÊäÄË°ìÔºåÂ¶ÇÁ£ÅÊåØÈÄ†ÂΩ± (MRI) ÂíåÊ≠£Â≠êÊñ∑Â±§ÊéÉÊèè (PET)„ÄÇÁÑ∂ËÄåÔºå
ÊåÅÁ∫åËøΩËπ§Âêå‰∏Ä‰ΩçÂÄãÈ´îÁöÑË®∫Êñ∑ÁµêÊûúÔºå‰∏¶ÂêåÊôÇÊî∂ÈõÜÂ§öÊ®°ÂºèÊï∏ÊìöÔºåÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫Ü
Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GFE-MambaÔºå‰∏ÄÁ®ÆÂü∫ÊñºÁîüÊàêÁâπÂæµËêÉÂèñ (GFE) ÁöÑÂàÜÈ°ûÂô®„ÄÇÊ≠§ÂàÜÈ°ûÂô®ÊúâÊïàÊï¥Âêà‰æÜËá™
Ë©ï‰º∞ÈáèË°®„ÄÅMRI Âíå PET ÁöÑÊï∏ÊìöÔºåÂØ¶ÁèæÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÂºèËûçÂêà„ÄÇÂÆÉ
ÊúâÊïàÁéáÂú∞ËêÉÂèñÂá∫Èï∑Â∫èÂàóÂíåÁü≠Â∫èÂàóË≥áË®äÔºå‰∏¶Á¥çÂÖ•ÂÉèÁ¥†Á©∫Èñì‰ª•Â§ñÁöÑÈ°çÂ§ñË≥áË®ä„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰∏çÂÉÖÊèêÂçá‰∫Ü
ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶Ôºå‰πüÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂíåÁ©©ÂÆöÊÄß„ÄÇÊàëÂÄëÊ†πÊìö
ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ÁµÑÁπî (ADNI) Âª∫Á´ã‰∫ÜË∂ÖÈÅé 3000 ÂÄãÊ®£Êú¨ÁöÑË≥áÊñôÈõÜÔºåÁî®ÊñºÂÖ©ÈöéÊÆµË®ìÁ∑¥
ÈÅéÁ®ã„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåGFE-Mamba Ê®°ÂûãÂú®È†êÊ∏¨Âæû MCI ËΩâËÆäÁÇ∫ AD ‰∏äÊòØÊúâÊïàÁöÑÔºå‰∏¶‰∏îÂÑ™ÊñºÂ§öÁ®Æ
ÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ®ãÂºèÁ¢ºÂíå ADNI Ë≥áÊñôÈõÜËôïÁêÜÁ®ãÂºèÁ¢ºÂèØÊñº https://github.com/Tinysqua/GFE-Mamba ÂèñÂæó„ÄÇ

##### **Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks**
2407.15526v2 by Eugenio Lomurno, Matteo Matteucci

Generative artificial intelligence has transformed the generation of
synthetic data, providing innovative solutions to challenges like data scarcity
and privacy, which are particularly critical in fields such as medicine.
However, the effective use of this synthetic data to train high-performance
models remains a significant challenge. This paper addresses this issue by
introducing Knowledge Recycling (KR), a pipeline designed to optimise the
generation and use of synthetic data for training downstream classifiers. At
the heart of this pipeline is Generative Knowledge Distillation (GKD), the
proposed technique that significantly improves the quality and usefulness of
the information provided to classifiers through a synthetic dataset
regeneration and soft labelling mechanism. The KR pipeline has been tested on a
variety of datasets, with a focus on six highly heterogeneous medical image
datasets, ranging from retinal images to organ scans. The results show a
significant reduction in the performance gap between models trained on real and
synthetic data, with models based on synthetic data outperforming those trained
on real data in some cases. Furthermore, the resulting models show almost
complete immunity to Membership Inference Attacks, manifesting privacy
properties missing in models trained with conventional techniques.

ÊëòË¶ÅÔºöÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÂ∑≤Á∂ìËΩâËÆä‰∫ÜÂêàÊàêË≥áÊñôÁöÑÁîüÊàêÔºåÁÇ∫Ë≥áÊñôÁ®ÄÂ∞ëÂíåÈö±ÁßÅÁ≠âÊåëÊà∞Êèê‰æõ‰∫ÜÂâµÊñ∞ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÈÄô‰∫õÊåëÊà∞Âú®ÈÜ´Â≠∏Á≠âÈ†òÂüüÁâπÂà•ÈóúÈçµ„ÄÇÁÑ∂ËÄåÔºåÊúâÊïà‰ΩøÁî®ÈÄôÁ®ÆÂêàÊàêË≥áÊñô‰æÜË®ìÁ∑¥È´òÊÄßËÉΩÊ®°Âûã‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊú¨ÊñáÈÄöÈÅéÂºïÂÖ•Áü•Ë≠òÂõûÊî∂ (KR) ‰æÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®ÂÑ™ÂåñÂêàÊàêË≥áÊñôÁöÑÁîüÊàêÂíå‰ΩøÁî®‰ª•Ë®ìÁ∑¥‰∏ãÊ∏∏ÂàÜÈ°ûÂô®ÁöÑÁÆ°ÈÅì„ÄÇÊ≠§ÁÆ°ÈÅìÁöÑÊ†∏ÂøÉÊòØÁîüÊàêÂºèÁü•Ë≠òËí∏È§æ (GKD)ÔºåÈÄôÈ†ÖÊèêË≠∞ÁöÑÊäÄË°ìÈÄöÈÅéÂêàÊàêË≥áÊñôÈõÜÂÜçÁîüÂíåËªüÊ®ôÁ±§Ê©üÂà∂ÔºåÈ°ØËëóÊîπÂñÑ‰∫ÜÊèê‰æõÁµ¶ÂàÜÈ°ûÂô®ÁöÑË≥áË®äÂìÅË≥™ÂíåÊúâÁî®ÊÄß„ÄÇKR ÁÆ°ÈÅìÂ∑≤ÈáùÂ∞çÂêÑÁ®ÆË≥áÊñôÈõÜÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÈáçÈªûÊòØÂÖ≠ÂÄãÈ´òÂ∫¶Áï∞Ë≥™ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÔºåÁØÑÂúçÂæûË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂà∞Âô®ÂÆòÊéÉÊèè„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ÁúüÂØ¶Ë≥áÊñôÂíåÂêàÊàêË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆË∑ùÈ°ØËëóÁ∏ÆÂ∞èÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÂü∫ÊñºÂêàÊàêË≥áÊñôÁöÑÊ®°ÂûãÂÑ™ÊñºÂú®ÁúüÂØ¶Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÁîüÊàêÁöÑÊ®°ÂûãÂ∞çÊàêÂì°Ë∫´ÂàÜÊé®Ë´ñÊîªÊìäÂπæ‰πéÂÆåÂÖ®ÂÖçÁñ´ÔºåË°®ÁèæÂá∫ÂÇ≥Áµ±ÊäÄË°ìË®ìÁ∑¥ÁöÑÊ®°ÂûãÊâÄÁº∫Â∞ëÁöÑÈö±ÁßÅÁâπÊÄß„ÄÇ

##### **A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model**
2407.15362v1 by Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, Ronald Cheong Kin Chan, Hao Chen

Remarkable strides in computational pathology have been made in the
task-agnostic foundation model that advances the performance of a wide array of
downstream clinical tasks. Despite the promising performance, there are still
several challenges. First, prior works have resorted to either vision-only or
vision-captions data, disregarding invaluable pathology reports and gene
expression profiles which respectively offer distinct knowledge for versatile
clinical applications. Second, the current progress in pathology FMs
predominantly concentrates on the patch level, where the restricted context of
patch-level pretraining fails to capture whole-slide patterns. Here we curated
the largest multimodal dataset consisting of H\&E diagnostic whole slide images
and their associated pathology reports and RNA-Seq data, resulting in 26,169
slide-level modality pairs from 10,275 patients across 32 cancer types. To
leverage these data for CPath, we propose a novel whole-slide pretraining
paradigm which injects multimodal knowledge at the whole-slide context into the
pathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed
paradigm revolutionizes the workflow of pretraining for CPath, which enables
the pathology FM to acquire the whole-slide context. To our knowledge, this is
the first attempt to incorporate multimodal knowledge at the slide level for
enhancing pathology FMs, expanding the modelling context from unimodal to
multimodal knowledge and from patch-level to slide-level. To systematically
evaluate the capabilities of mSTAR, extensive experiments including slide-level
unimodal and multimodal applications, are conducted across 7 diverse types of
tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.
The average performance in various slide-level applications consistently
demonstrates significant performance enhancements for mSTAR compared to SOTA
FMs.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë®àÁÆóÁóÖÁêÜÂ≠∏‰∏≠Ôºå‰ªªÂãô‰∏çÂèØÁü•Âü∫Á§éÊ®°ÂûãÂ∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºåÂèØÊèêÂçáÂª£Ê≥õ‰∏ãÊ∏∏Ëá®Â∫ä‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÂÑòÁÆ°ÊïàËÉΩ‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜ‰ªçÊúâÂπæÂÄãÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÂÉÖÊé°Áî®ÂÉÖÈôêÂΩ±ÂÉèÊàñÂΩ±ÂÉèÊ®ôÈ°åË≥áÊñôÔºåÂøΩÁï•‰∫ÜÂØ∂Ë≤¥ÁöÑÁóÖÁêÜÂ†±ÂëäÂíåÂü∫Âõ†Ë°®ÁèæÁâπÂæµÔºåËÄåÈÄô‰∫õÁâπÂæµÂàÜÂà•ÁÇ∫Â§öÂäüËÉΩËá®Â∫äÊáâÁî®Êèê‰æõ‰∫Ü‰∏çÂêåÁöÑÁü•Ë≠ò„ÄÇÂÖ∂Ê¨°ÔºåÁóÖÁêÜ FM ÁöÑÁï∂ÂâçÈÄ≤Â∫¶‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçÄÂ°äÂ±§Á¥öÔºåÂçÄÂ°äÂ±§Á¥öÈ†êË®ìÁ∑¥ÁöÑÂèóÈôêËÉåÊôØÁÑ°Ê≥ïÊì∑ÂèñÂÖ®ÂàáÁâáÊ®°Âºè„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫ÜÊúÄÂ§ßÁöÑÂ§öÊ®°ÊÖãË≥áÊñôÈõÜÔºåÂåÖÂê´ H&E Ë®∫Êñ∑ÂÖ®ÂàáÁâáÂΩ±ÂÉèÂèäÂÖ∂Áõ∏ÈóúÁóÖÁêÜÂ†±ÂëäÂíå RNA-Seq Ë≥áÊñôÔºåÂÖ±Áî¢Áîü‰æÜËá™ 32 Á®ÆÁôåÁóáÈ°ûÂûãÁöÑ 10,275 ÂêçÊÇ£ËÄÖÁöÑ 26,169 ÂÄãÂàáÁâáÂ±§Á¥öÊ®°ÊÖãÈÖçÂ∞ç„ÄÇÁÇ∫‰∫ÜÂ∞áÈÄô‰∫õË≥áÊñôÁî®Êñº CPathÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂÖ®ÂàáÁâáÈ†êË®ìÁ∑¥ÁØÑ‰æãÔºåÂ∞áÂÖ®ÂàáÁâáËÉåÊôØÁöÑÂ§öÊ®°ÊÖãÁü•Ë≠òÊ≥®ÂÖ•ÁóÖÁêÜ FMÔºåÁ®±ÁÇ∫Â§öÊ®°ÊÖãËá™ÊïôÈ†êË®ìÁ∑¥ (mSTAR)„ÄÇÊâÄÊèêÂá∫ÁöÑÁØÑ‰æãÂæπÂ∫ïÊîπËÆä‰∫Ü CPath ÁöÑÈ†êË®ìÁ∑¥Â∑•‰ΩúÊµÅÁ®ãÔºå‰ΩøÁóÖÁêÜ FM ËÉΩÂ§†Áç≤ÂèñÂÖ®ÂàáÁâáËÉåÊôØ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÈ¶ñÊ¨°ÂòóË©¶Âú®ÂàáÁâáÂ±§Á¥öÁ¥çÂÖ•Â§öÊ®°ÊÖãÁü•Ë≠ò‰ª•Â¢ûÂº∑ÁóÖÁêÜ FMÔºåÂ∞áÂª∫Ê®°ËÉåÊôØÂæûÂñÆ‰∏ÄÊ®°ÊÖãÁü•Ë≠òÊì¥Â±ïÂà∞Â§öÊ®°ÊÖãÁü•Ë≠òÔºå‰ª•ÂèäÂæûÂçÄÂ°äÂ±§Á¥öÊì¥Â±ïÂà∞ÂàáÁâáÂ±§Á¥ö„ÄÇÁÇ∫‰∫ÜÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ mSTAR ÁöÑÂäüËÉΩÔºåÊàëÂÄëÂú® 43 ÂÄãÂ≠ê‰ªªÂãô‰∏≠Â∞ç 7 Á®Æ‰∏çÂêåÈ°ûÂûãÁöÑ‰ªªÂãôÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÂåÖÊã¨ÂàáÁâáÂ±§Á¥öÁöÑÂñÆ‰∏ÄÊ®°ÊÖãÂíåÂ§öÊ®°ÊÖãÊáâÁî®ÔºåÁî¢Áîü‰∫ÜÊúÄÂ§ßÁöÑ‰∏ãÊ∏∏‰ªªÂãôÁØÑÂúç„ÄÇÂú®ÂêÑÁ®ÆÂàáÁâáÂ±§Á¥öÊáâÁî®‰∏≠ÔºåÂπ≥ÂùáÊïàËÉΩÊåÅÁ∫åÈ°ØÁ§∫ mSTAR Ëàá SOTA FM Áõ∏ÊØîÊúâÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇ</paragraph>

##### **They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models**
2407.21041v1 by Mohammad Saeid Mahdavinejad, Peyman Adibi, Amirhassan Monadjemi, Pascal Hitzler

Depression is a common mental health issue that requires prompt diagnosis and
treatment. Despite the promise of social media data for depression detection,
the opacity of employed deep learning models hinders interpretability and
raises bias concerns. We address this challenge by introducing ProtoDep, a
novel, explainable framework for Twitter-based depression detection. ProtoDep
leverages prototype learning and the generative power of Large Language Models
to provide transparent explanations at three levels: (i) symptom-level
explanations for each tweet and user, (ii) case-based explanations comparing
the user to similar individuals, and (iii) transparent decision-making through
classification weights. Evaluated on five benchmark datasets, ProtoDep achieves
near state-of-the-art performance while learning meaningful prototypes. This
multi-faceted approach offers significant potential to enhance the reliability
and transparency of depression detection on social media, ultimately aiding
mental health professionals in delivering more informed care.

ÊëòË¶ÅÔºöÊÜÇÈ¨±ÁóáÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÁ≤æÁ•ûÂÅ•Â∫∑ÂïèÈ°åÔºåÈúÄË¶ÅÂèäÊôÇË®∫Êñ∑ÂíåÊ≤ªÁôÇ„ÄÇÂÑòÁÆ°Á§æÁæ§Â™íÈ´îË≥áÊñôÊúâÊúõÁî®ÊñºÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Ôºå‰ΩÜÊâÄÊé°Áî®ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑ‰∏çÈÄèÊòéÊÄßÈòªÁ§ô‰∫ÜËß£Ôºå‰∏¶ÂºïÁôºÂÅèË¶ãÁñëÊÖÆ„ÄÇÊàëÂÄëÈÄèÈÅéÂºïÂÖ• ProtoDep ‰æÜËß£Ê±∫ÈÄôÂÄãÊåëÊà∞ÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©é‰∏îÂèØËß£ÈáãÁöÑÊû∂ÊßãÔºåÁî®ÊñºÂü∫Êñº Twitter ÁöÑÊÜÇÈ¨±ÁóáÂÅµÊ∏¨„ÄÇProtoDep Âà©Áî®ÂéüÂûãÂ≠∏ÁøíÂíåÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁîüÊàêËÉΩÂäõÔºåÂú®‰∏âÂÄãÂ±§Á¥öÊèê‰æõÈÄèÊòéÁöÑËß£ÈáãÔºö(i) Â∞çÊØèÂâáÊé®ÊñáÂíå‰ΩøÁî®ËÄÖÁöÑÁóáÁãÄÂ±§Á¥öËß£ÈáãÔºå(ii) Â∞á‰ΩøÁî®ËÄÖËàáÈ°û‰ººÂÄãÈ´îÈÄ≤Ë°åÊØîËºÉÁöÑÊÉÖÂ¢ÉÂºèËß£ÈáãÔºå‰ª•Âèä (iii) ÈÄèÈÅéÂàÜÈ°ûÊ¨äÈáçÈÄ≤Ë°åÈÄèÊòéÁöÑÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÂú®‰∫îÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåProtoDep Âú®Â≠∏ÁøíÊúâÊÑèÁæ©ÁöÑÂéüÂûãÊôÇÔºåÈÅîÂà∞‰∫ÜÊé•ËøëÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÈÄôÁ®ÆÂ§öÈù¢ÂêëÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÈ°ØËëóÁöÑÊΩõÂäõÔºå‰ª•Â¢ûÂº∑Á§æÁæ§Â™íÈ´î‰∏äÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÁöÑÂèØÈù†ÊÄßÂíåÈÄèÊòéÂ∫¶ÔºåÊúÄÁµÇÂçîÂä©ÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°Êèê‰æõÊõ¥ÊòéÊô∫ÁöÑÁÖßË≠∑„ÄÇ

##### **Genetic Algorithm to Optimize Design of Micro-Surgical Scissors**
2407.15243v1 by Fatemeh Norouziani, Veerash Palanichamy, Shivam Gupta, Onaizah Onaizah

Microrobotics is an attractive area of research as small-scale robots have
the potential to improve the precision and dexterity offered by minimally
invasive surgeries. One example of such a tool is a pair of micro-surgical
scissors that was developed for cutting of tumors or cancerous tissues present
deep inside the body such as in the brain. This task is often deemed difficult
or impossible with conventional robotic tools due to their size and dexterity.
The scissors are designed with two magnets placed a specific distance apart to
maximize deflection and generate cutting forces. However, remote actuation and
size requirements of the micro-surgical scissors limits the force that can be
generated to puncture the tissue. To address the limitation of small output
forces, we use an evolutionary algorithm to further optimize the performance of
the scissors. In this study, the design of the previously developed untethered
micro-surgical scissors has been modified and their performance is enhanced by
determining the optimal position of the magnets as well as the direction of
each magnetic moment. The developed algorithm is successfully applied to a
4-magnet configuration which results in increased net torque. This improvement
in net torque is directly translated into higher cutting forces. The new
configuration generates a cutting force of 58 mN from 80 generations of the
evolutionary algorithm which is a 1.65 times improvement from the original
design. Furthermore, the developed algorithm has the advantage that it can be
deployed with minor modifications to other microrobotic tools and systems,
opening up new possibilities for various medical procedures and applications.

ÊëòË¶ÅÔºöÂæÆÂûãÊ©üÂô®‰∫∫ÊòØ‰∏ÄÂÄãÊúâÂê∏ÂºïÂäõÁöÑÁ†îÁ©∂È†òÂüüÔºåÂõ†ÁÇ∫Â∞èÂûãÊ©üÂô®‰∫∫ÂÖ∑ÊúâÊèêÈ´òÂæÆÂâµÊâãË°ìÁöÑÁ≤æÁ¢∫Â∫¶ÂíåÈùàÊ¥ªÊÄß„ÄÇÊ≠§È°ûÂ∑•ÂÖ∑ÁöÑ‰∏ÄÂÄãÁØÑ‰æãÊòØ‰∏ÄÂ∞çÂæÆÂûãÊâãË°ìÂâ™ÂàÄÔºåÂÖ∂ÈñãÁôºÁî®ÊñºÂàáÈô§Ê∑±ËóèÊñºÈ´îÂÖßÔºà‰æãÂ¶ÇÂ§ßËÖ¶ÔºâÁöÑËÖ´Áò§ÊàñÁôåÁµÑÁπî„ÄÇÁî±ÊñºÂÇ≥Áµ±Ê©üÂô®‰∫∫Â∑•ÂÖ∑ÁöÑÂ∞∫ÂØ∏ÂíåÈùàÊ¥ªÊÄßÔºåÊ≠§‰ªªÂãôÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫Âõ∞Èõ£Êàñ‰∏çÂèØËÉΩ„ÄÇÊ≠§Ââ™ÂàÄÁöÑË®≠Ë®àÊé°Áî®ÂÖ©ÂÄãÁ£ÅÈêµÔºåÂÆÉÂÄë‰πãÈñìÁöÑË∑ùÈõ¢Ë®≠ÂÆöÁÇ∫ÁâπÂÆöÂÄºÔºå‰ª•ÊúÄÂ§ßÂåñÂÅèËΩâ‰∏¶Áî¢ÁîüÂàáÂâäÂäõ„ÄÇÁÑ∂ËÄåÔºåÂæÆÂûãÊâãË°ìÂâ™ÂàÄÁöÑÈÅ†Á®ãËá¥ÂãïÂíåÂ∞∫ÂØ∏Ë¶ÅÊ±ÇÈôêÂà∂‰∫ÜÂèØÁî¢ÁîüÁöÑÁ©øÂà∫ÁµÑÁπîÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Â∞èËº∏Âá∫ÂäõÁöÑÈôêÂà∂ÔºåÊàëÂÄë‰ΩøÁî®ÊºîÂåñÊºîÁÆóÊ≥ïÈÄ≤‰∏ÄÊ≠•ÊúÄ‰Ω≥ÂåñÂâ™ÂàÄÁöÑÊïàËÉΩ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÂÖàÂâçÈñãÁôºÁöÑÁÑ°Áπ´Áπ©ÂæÆÂûãÊâãË°ìÂâ™ÂàÄÁöÑË®≠Ë®àÂ∑≤‰øÆÊîπÔºå‰∏¶‰∏îÈÄèÈÅéÁ¢∫ÂÆöÁ£ÅÈêµÁöÑÊúÄ‰Ω≥‰ΩçÁΩÆ‰ª•ÂèäÊØèÂÄãÁ£ÅÁü©ÁöÑÊñπÂêë‰æÜÊèêÂçáÂÖ∂ÊïàËÉΩ„ÄÇÂ∑≤Â∞áÈñãÁôºÁöÑÊºîÁÆóÊ≥ïÊàêÂäüÊáâÁî®Êñº 4 Á£ÅÈêµÁµÑÊÖãÔºåÈÄôÊúÉÂ∞éËá¥Ê∑®Êâ≠ÂäõÂ¢ûÂä†„ÄÇÊ∑®Êâ≠ÂäõÁöÑÊ≠§È†ÖÊîπÂñÑÁõ¥Êé•ËΩâÂåñÁÇ∫Êõ¥È´òÁöÑÂàáÂâäÂäõ„ÄÇÊñ∞ÁöÑÁµÑÊÖãÂæûÊºîÂåñÊºîÁÆóÊ≥ïÁöÑ 80 ÂÄã‰∏ñ‰ª£Áî¢Áîü 58 mN ÁöÑÂàáÂâäÂäõÔºåÈÄôÊòØÁõ∏ËºÉÊñºÂéüÂßãË®≠Ë®àÊîπÂñÑ‰∫Ü 1.65 ÂÄç„ÄÇÊ≠§Â§ñÔºåÂ∑≤ÈñãÁôºÁöÑÊºîÁÆóÊ≥ïÂÖ∑ÊúâÂèØÈÄèÈÅéËºïÂæÆ‰øÆÊîπÈÉ®ÁΩ≤Ëá≥ÂÖ∂‰ªñÂæÆÂûãÊ©üÂô®‰∫∫Â∑•ÂÖ∑ÂíåÁ≥ªÁµ±ÁöÑÂÑ™ÈªûÔºåÁÇ∫ÂêÑÁ®ÆÈÜ´ÁôÇÁ®ãÂ∫èÂíåÊáâÁî®ÈñãÂïü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ

##### **MedSAGa: Few-shot Memory Efficient Medical Image Segmentation using Gradient Low-Rank Projection in SAM**
2407.15042v1 by Navyansh Mahla, Annie D'souza, Shubh Gupta, Bhavik Kanekar, Kshitij Sharad Jadhav

The application of large-scale models in medical image segmentation demands
substantial quantities of meticulously annotated data curated by experts along
with high computational resources, both of which are challenges in
resource-poor settings. In this study, we present the Medical Segment Anything
Model with Galore MedSAGa where we adopt the Segment Anything Model (SAM) to
achieve memory-efficient, few-shot medical image segmentation by applying
Gradient Low-Rank Projection GaLore to the parameters of the image encoder of
SAM. Meanwhile, the weights of the prompt encoder and mask decoder undergo full
parameter fine-tuning using standard optimizers. We further assess MedSAGa's
few-shot learning capabilities, reporting on its memory efficiency and
segmentation performance across multiple standard medical image segmentation
datasets. We compare it with several baseline models, including LoRA fine-tuned
SAM (SAMed) and DAE-Former. Experiments across multiple datasets and these
baseline models with different number of images for fine tuning demonstrated
that the GPU memory consumption of MedSAGa is significantly less than that of
the baseline models, achieving an average memory efficiency of 66% more than
current state-of-the-art (SOTA) models for medical image segmentation. The
combination of substantially lower memory requirements and comparable to SOTA
results in few-shot learning for medical image segmentation positions MedSAGa
as an optimal solution for deployment in resource-constrained settings.

ÊëòË¶ÅÔºöÂ§ßÂûãÊ®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÁöÑÊáâÁî®ÈúÄË¶ÅÂ§ßÈáèÁî±Â∞àÂÆ∂Á≤æÂøÉË®ªËß£ÁöÑË≥áÊñôÔºå‰ª•ÂèäÂ§ßÈáèÁöÑË®àÁÆóË≥áÊ∫êÔºåÈÄôÂÖ©ËÄÖÂú®Ë≥áÊ∫êË≤ß‰πèÁöÑÁí∞Â¢É‰∏≠ÈÉΩÊòØÊåëÊà∞„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ∑ÊúâË±êÂØåÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÈÜ´Â≠∏ÂàÜÂâ≤‰ªª‰ΩïÊ®°Âûã (MedSAGa)ÔºåÂÖ∂‰∏≠ÊàëÂÄëÊé°Áî®ÂàÜÂâ≤‰ªª‰ΩïÊ®°Âûã (SAM) ‰æÜÈÄöÈÅéÂ∞áÊ¢ØÂ∫¶‰ΩéÁß©ÊäïÂΩ± GaLore ÊáâÁî®Êñº SAM ÁöÑÂΩ±ÂÉèÁ∑®Á¢ºÂô®ÂèÉÊï∏‰æÜÂØ¶ÁèæË®òÊÜ∂È´îÊïàÁéáÈ´òÁöÑÂ∞ëÈáèÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇÂêåÊôÇÔºåÊèêÁ§∫Á∑®Á¢ºÂô®ÂíåÈÅÆÁΩ©Ëß£Á¢ºÂô®ÁöÑÊ¨äÈáç‰ΩøÁî®Ê®ôÊ∫ñÊúÄ‰Ω≥ÂåñÂô®ÈÄ≤Ë°åÂÆåÊï¥ÁöÑÂèÉÊï∏ÂæÆË™ø„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë©ï‰º∞‰∫Ü MedSAGa ÁöÑÂ∞ëÈáèÂ≠∏ÁøíËÉΩÂäõÔºåÂ†±Âëä‰∫ÜÂÖ∂Âú®Â§öÂÄãÊ®ôÊ∫ñÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ë≥áÊñôÈõÜ‰∏≠ÁöÑË®òÊÜ∂È´îÊïàÁéáÂíåÂàÜÂâ≤ÊïàËÉΩ„ÄÇÊàëÂÄëÂ∞áÂÖ∂ËàáÂπæÂÄãÂü∫Ê∫ñÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÂåÖÊã¨ÂæÆË™ø SAM (SAMed) ÁöÑ LoRA Âíå DAE-Former„ÄÇË∑®Â§öÂÄãË≥áÊñôÈõÜÂíåÈÄô‰∫õÂü∫Ê∫ñÊ®°ÂûãÁöÑÂØ¶È©óÔºå‰ΩøÁî®‰∏çÂêåÊï∏ÈáèÁöÑÂΩ±ÂÉèÈÄ≤Ë°åÂæÆË™øÔºåË≠âÊòé MedSAGa ÁöÑ GPU Ë®òÊÜ∂È´îÊ∂àËÄóÊòéÈ°Ø‰ΩéÊñºÂü∫Ê∫ñÊ®°ÂûãÔºåÊØîÁõÆÂâçÊúÄÂÖàÈÄ≤ (SOTA) ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ê®°ÂûãÂπ≥ÂùáË®òÊÜ∂È´îÊïàÁéáÈ´òÂá∫ 66%„ÄÇÂú®Â∞ëÈáèÂ≠∏Áøí‰∏≠ÔºåÂ§ßÂπÖÈôç‰ΩéË®òÊÜ∂È´îÈúÄÊ±ÇËàá SOTA Áõ∏Áï∂ÁöÑÁµêÊûúÔºå‰ΩøÂæó MedSAGa ÊàêÁÇ∫Âú®Ë≥áÊ∫êÂèóÈôêÁí∞Â¢É‰∏≠ÈÉ®ÁΩ≤ÁöÑÊúÄ‰Ω≥Ëß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Mapping Patient Trajectories: Understanding and Visualizing Sepsis Prognostic Pathways from Patients Clinical Narratives**
2407.21039v1 by Sudeshna Jana, Tirthankar Dasgupta, Lipika Dey

In recent years, healthcare professionals are increasingly emphasizing on
personalized and evidence-based patient care through the exploration of
prognostic pathways. To study this, structured clinical variables from
Electronic Health Records (EHRs) data have traditionally been employed by many
researchers. Presently, Natural Language Processing models have received great
attention in clinical research which expanded the possibilities of using
clinical narratives. In this paper, we propose a systematic methodology for
developing sepsis prognostic pathways derived from clinical notes, focusing on
diverse patient subgroups identified by exploring comorbidities associated with
sepsis and generating explanations of these subgroups using SHAP. The extracted
prognostic pathways of these subgroups provide valuable insights into the
dynamic trajectories of sepsis severity over time. Visualizing these pathways
sheds light on the likelihood and direction of disease progression across
various contexts and reveals patterns and pivotal factors or biomarkers
influencing the transition between sepsis stages, whether toward deterioration
or improvement. This empowers healthcare providers to implement more
personalized and effective healthcare strategies for individual patients.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Â£´Ë∂ä‰æÜË∂äÈáçË¶ñÈÄèÈÅéÊé¢Á¥¢È†êÂæåÈÄîÂæë‰æÜÊèê‰æõÂÄã‰∫∫Âåñ‰∏îÂü∫ÊñºË≠âÊìöÁöÑÊÇ£ËÄÖÁÖßË≠∑„ÄÇÁÇ∫Á†îÁ©∂Ê≠§Ë≠∞È°åÔºåË®±Â§öÁ†îÁ©∂‰∫∫Âì°ÂÇ≥Áµ±‰∏äÊé°Áî®ÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) Ë≥áÊñô‰∏≠ÁöÑÁµêÊßãÂåñËá®Â∫äËÆäÊï∏„ÄÇÁõÆÂâçÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊ®°ÂûãÂú®Ëá®Â∫äÁ†îÁ©∂‰∏≠ÂÇôÂèóÈáçË¶ñÔºåÊì¥Â§ß‰∫Ü‰ΩøÁî®Ëá®Â∫äÊïòËø∞ÁöÑÂèØËÉΩÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ≥ªÁµ±ÂåñÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÈñãÁôºÂæûËá®Â∫äÁ≠ÜË®ò‰∏≠Ë°çÁîüÁöÑÊïóË°ÄÁóáÈ†êÂæåÈÄîÂæëÔºåÈáçÈªûÂú®ÊñºÊé¢Á¥¢ËàáÊïóË°ÄÁóáÁõ∏ÈóúÁöÑ‰ΩµÁôºÁóáÊâÄË≠òÂà•Âá∫ÁöÑ‰∏çÂêåÊÇ£ËÄÖÂ≠êÁæ§Ôºå‰∏¶‰ΩøÁî® SHAP Áî¢ÁîüÈÄô‰∫õÂ≠êÁæ§ÁöÑË™™Êòé„ÄÇÈÄô‰∫õÂ≠êÁæ§‰∏≠ËêÉÂèñÂá∫ÁöÑÈ†êÂæåÈÄîÂæëÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£ÊïóË°ÄÁóáÂö¥ÈáçÁ®ãÂ∫¶Èö®ËëóÊôÇÈñìÊé®ÁßªÁöÑÂãïÊÖãËªåË∑°„ÄÇË¶ñË¶∫ÂåñÈÄô‰∫õÈÄîÂæëÊúâÂä©Êñº‰∫ÜËß£ÁñæÁóÖÈÄ≤Â±ïÂú®ÂêÑÁ®ÆÊÉÖÂ¢É‰∏ãÁöÑÂèØËÉΩÊÄßÂíåÊñπÂêëÔºå‰∏¶Êè≠Á§∫ÂΩ±ÈüøÊïóË°ÄÁóáÈöéÊÆµËΩâËÆäÁöÑÊ®°ÂºèÂíåÈóúÈçµÂõ†Á¥†ÊàñÁîüÁâ©Ê®ôË®òÔºåÁÑ°Ë´ñÊòØÊÉ°ÂåñÊàñÊîπÂñÑ„ÄÇÈÄô‰ΩøÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖËÉΩÂ§†ÁÇ∫ÂÄãÂà•ÊÇ£ËÄÖÂØ¶ÊñΩÊõ¥ÂÄã‰∫∫Âåñ‰∏îÊúâÊïàÁöÑÈÜ´ÁôÇ‰øùÂÅ•Á≠ñÁï•„ÄÇ

##### **Preictal Period Optimization for Deep Learning-Based Epileptic Seizure Prediction**
2407.14876v1 by Petros Koutsouvelis, Bartlomiej Chybowski, Alfredo Gonzalez-Sulser, Shima Abdullateef, Javier Escudero

Accurate prediction of epileptic seizures could prove critical for improving
patient safety and quality of life in drug-resistant epilepsy. Although deep
learning-based approaches have shown promising seizure prediction performance
using scalp electroencephalogram (EEG) signals, substantial limitations still
impede their clinical adoption. Furthermore, identifying the optimal preictal
period (OPP) for labeling EEG segments remains a challenge. Here, we not only
develop a competitive deep learning model for seizure prediction but, more
importantly, leverage it to demonstrate a methodology to comprehensively
evaluate the predictive performance in the seizure prediction task. For this,
we introduce a CNN-Transformer deep learning model to detect preictal
spatiotemporal dynamics, alongside a novel Continuous Input-Output Performance
Ratio (CIOPR) metric to determine the OPP. We trained and evaluated our model
on 19 pediatric patients of the open-access CHB-MIT dataset in a
subject-specific manner. Using the OPP of each patient, preictal and interictal
segments were correctly identified with an average sensitivity of 99.31%,
specificity of 95.34%, AUC of 99.35%, and F1- score of 97.46%, while prediction
time averaged 76.8 minutes before onset. Notably, our novel CIOPR metric
allowed outlining the impact of different preictal period definitions on
prediction time, accuracy, output stability, and transition time between
interictal and preictal states in a comprehensive and quantitative way and
highlighted the importance of considering both inter- and intra-patient
variability in seizure prediction.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫È†êÊ∏¨Áô≤ÁôáÁôº‰ΩúÂ∞çÊñºÊîπÂñÑËó•Áâ©ÊäóÊÄßÁô≤ÁôáÊÇ£ËÄÖÁöÑÂÆâÂÖ®ÊÄßËàáÁîüÊ¥ªÂìÅË≥™Ëá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂ∑≤Â±ïÁèæÂá∫‰ΩøÁî®È†≠ÁöÆËÖ¶ÈõªÂúñ (EEG) ‰ø°ËôüÈÄ≤Ë°åÁô≤ÁôáÁôº‰ΩúÈ†êÊ∏¨ÁöÑÂá∫Ëâ≤Ë°®ÁèæÔºå‰ΩÜ‰ªçÊúâÂØ¶Ë≥™ÈôêÂà∂ÈòªÁ§ôÂÖ∂Ëá®Â∫äÊáâÁî®„ÄÇÊ≠§Â§ñÔºåÊâæÂá∫Ê®ôË®ò EEG ÁâáÊÆµÁöÑÊúÄ‰Ω≥Áôº‰ΩúÂâç (OPP) ÊôÇÊúü‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰∏çÂÉÖÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÁî®ÊñºÁô≤ÁôáÁôº‰ΩúÈ†êÊ∏¨ÁöÑÁ´∂Áà≠ÊÄßÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂà©Áî®ÂÆÉ‰æÜÂ±ïÁ§∫‰∏ÄÁ®ÆÊñπÊ≥ïÔºå‰ª•ÂÖ®Èù¢Ë©ï‰º∞Áô≤ÁôáÁôº‰ΩúÈ†êÊ∏¨‰ªªÂãô‰∏≠ÁöÑÈ†êÊ∏¨ÊÄßËÉΩ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã CNN-Transformer Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æÜÂÅµÊ∏¨Áôº‰ΩúÂâçÁöÑÊôÇÁ©∫ÂãïÂäõÂ≠∏Ôºå‰ª•Âèä‰∏ÄÂÄãÊñ∞ÁöÑÈÄ£Á∫åËº∏ÂÖ•Ëº∏Âá∫ÊïàËÉΩÊØî (CIOPR) ÊåáÊ®ô‰æÜÁ¢∫ÂÆö OPP„ÄÇÊàëÂÄë‰ª•‰∏ªÈ°åÁâπÂÆöÁöÑÊñπÂºèÔºåÂú®ÈñãÊîæÂèñÁî®ÁöÑ CHB-MIT Ë≥áÊñôÈõÜÁöÑ 19 ‰ΩçÂ∞èÂÖíÊÇ£ËÄÖ‰∏äË®ìÁ∑¥‰∏¶Ë©ï‰º∞ÊàëÂÄëÁöÑÊ®°Âûã„ÄÇ‰ΩøÁî®ÊØè‰ΩçÊÇ£ËÄÖÁöÑ OPPÔºå‰ª•Âπ≥ÂùáÊïèÊÑüÂ∫¶ 99.31%„ÄÅÁâπÁï∞Â∫¶ 95.34%„ÄÅAUC 99.35% Âíå F1 ÂàÜÊï∏ 97.46% Ê≠£Á¢∫Ë≠òÂà•Áôº‰ΩúÂâçÂíåÁôº‰ΩúÈñìÊúüÔºåËÄåÈ†êÊ∏¨ÊôÇÈñìÂπ≥ÂùáÂú®Áôº‰ΩúÂâç 76.8 ÂàÜÈêò„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÊñ∞Á©éÁöÑ CIOPR ÊåáÊ®ôÂèØ‰ª•ÂÖ®Èù¢‰∏îÈáèÂåñÂú∞Ê¶ÇËø∞‰∏çÂêåÁôº‰ΩúÂâçÊôÇÊúüÂÆöÁæ©Â∞çÈ†êÊ∏¨ÊôÇÈñì„ÄÅÊ∫ñÁ¢∫Â∫¶„ÄÅËº∏Âá∫Á©©ÂÆöÊÄß‰ª•ÂèäÁôº‰ΩúÈñìÊúüÂíåÁôº‰ΩúÂâçÁãÄÊÖã‰πãÈñìÁöÑËΩâÊèõÊôÇÈñìÁöÑÂΩ±ÈüøÔºå‰∏¶Âº∑Ë™øÂú®Áô≤ÁôáÁôº‰ΩúÈ†êÊ∏¨‰∏≠ËÄÉÊÖÆÊÇ£ËÄÖÈñìÂíåÊÇ£ËÄÖÂÖßËÆäÁï∞ÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates**
2407.14796v1 by Junjie Shi, Caozhi Shang, Zhaobin Sun, Li Yu, Xin Yang, Zengqiang Yan

Incomplete multi-modal image segmentation is a fundamental task in medical
imaging to refine deployment efficiency when only partial modalities are
available. However, the common practice that complete-modality data is visible
during model training is far from realistic, as modalities can have imbalanced
missing rates in clinical scenarios. In this paper, we, for the first time,
formulate such a challenging setting and propose Preference-Aware
Self-diStillatION (PASSION) for incomplete multi-modal medical image
segmentation under imbalanced missing rates. Specifically, we first construct
pixel-wise and semantic-wise self-distillation to balance the optimization
objective of each modality. Then, we define relative preference to evaluate the
dominance of each modality during training, based on which to design task-wise
and gradient-wise regularization to balance the convergence rates of different
modalities. Experimental results on two publicly available multi-modal datasets
demonstrate the superiority of PASSION against existing approaches for modality
balancing. More importantly, PASSION is validated to work as a plug-and-play
module for consistent performance improvement across different backbones. Code
is available at https://github.com/Jun-Jie-Shi/PASSION.

ÊëòË¶ÅÔºö‰∏çÂÆåÊï¥ÁöÑÂ§öÊ®°ÊÖãÂΩ±ÂÉèÂàÜÂâ≤ÊòØÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÁî®ÊñºÂú®Âè™ÊúâÈÉ®ÂàÜÊ®°ÊÖãÂèØÁî®ÊôÇÁ≤æÈÄ≤ÈÉ®ÁΩ≤ÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÂú®Ê®°ÂûãË®ìÁ∑¥ÊúüÈñìÂèØË¶ãÂÆåÊï¥Ê®°ÊÖãË≥áÊñôÁöÑÂ∏∏Ë¶ãÂÅöÊ≥ï‰∏¶‰∏çÂàáÂØ¶ÈöõÔºåÂõ†ÁÇ∫Âú®Ëá®Â∫äÂ†¥ÊôØ‰∏≠ÔºåÊ®°ÊÖãÂèØËÉΩÊúÉÊúâ‰∏çÂπ≥Ë°°ÁöÑÈÅ∫ÊºèÁéá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÊ¨°Âà∂ÂÆö‰∫ÜÈÄôÊ®£‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑË®≠ÂÆöÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏çÂπ≥Ë°°ÈÅ∫ÊºèÁéá‰∏ã‰∏çÂÆåÊï¥Â§öÊ®°ÊÖãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÂÅèÂ•ΩÊÑüÁü•Ëá™Ëí∏È§æ (PASSION)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂª∫ÊßãÂÉèÁ¥†Á¥öÂíåË™ûÁæ©Á¥öËá™Ëí∏È§æÔºå‰ª•Âπ≥Ë°°ÊØèÂÄãÊ®°ÊÖãÁöÑÊúÄ‰Ω≥ÂåñÁõÆÊ®ô„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂÆöÁæ©Áõ∏Â∞çÂÅèÂ•Ω‰æÜË©ï‰º∞Ë®ìÁ∑¥ÊúüÈñìÊØèÂÄãÊ®°ÊÖãÁöÑ‰∏ªÂ∞éÂú∞‰ΩçÔºå‰∏¶Ê†πÊìöÊ≠§Ë®≠Ë®à‰ªªÂãôÁ¥öÂíåÊ¢ØÂ∫¶Á¥öÊ≠£ÂâáÂåñÔºå‰ª•Âπ≥Ë°°‰∏çÂêåÊ®°ÊÖãÁöÑÊî∂ÊñÇÁéá„ÄÇÂú®ÂÖ©ÂÄãÂÖ¨ÈñãÁöÑÂ§öÊ®°ÊÖãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü PASSION ÂÑ™ÊñºÁèæÊúâÊ®°ÊÖãÂπ≥Ë°°ÊñπÊ≥ï„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåPASSION Ë¢´È©óË≠âÁÇ∫Âç≥ÊèíÂç≥Áî®Ê®°ÁµÑÔºåÂèØÂú®‰∏çÂêåÁöÑ‰∏ªÂππ‰∏≠ÊåÅÁ∫åÊèêÂçáÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/Jun-Jie-Shi/PASSION ÂèñÂæó„ÄÇ

##### **Improving Representation of High-frequency Components for Medical Foundation Models**
2407.14651v2 by Yuetan Chu, Yilan Zhang, Zhongyi Han, Changchun Yang, Longxi Zhou, Gongning Luo, Xin Gao

Foundation models have recently attracted significant attention for their
impressive generalizability across diverse downstream tasks. However, these
models are demonstrated to exhibit great limitations in representing
high-frequency components and fine-grained details. In many medical imaging
tasks, the precise representation of such information is crucial due to the
inherently intricate anatomical structures, sub-visual features, and complex
boundaries involved. Consequently, the limited representation of prevalent
foundation models can result in significant performance degradation or even
failure in these tasks. To address these challenges, we propose a novel
pretraining strategy, named Frequency-advanced Representation Autoencoder
(Frepa). Through high-frequency masking and low-frequency perturbation combined
with adversarial learning, Frepa encourages the encoder to effectively
represent and preserve high-frequency components in the image embeddings.
Additionally, we introduce an innovative histogram-equalized image masking
strategy, extending the Masked Autoencoder approach beyond ViT to other
architectures such as Swin Transformer and convolutional networks. We develop
Frepa across nine medical modalities and validate it on 32 downstream tasks for
both 2D images and 3D volume data. Without fine-tuning, Frepa can outperform
other self-supervised pretraining methods and, in some cases, even surpasses
task-specific trained models. This improvement is particularly significant for
tasks involving fine-grained details, such as achieving up to a +15% increase
in DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule
detection. Further experiments quantitatively reveal that Frepa enables
superior high-frequency representations and preservation in the embeddings,
underscoring its potential for developing more generalized and universal
medical image foundation models.

ÊëòË¶ÅÔºö<paragraph>Âü∫Á§éÊ®°ÂûãÊúÄËøëÂõ†ÂÖ∂Âú®ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏≠‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊ≥õÂåñËÉΩÂäõËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂ∑≤Ë≠âÊòéÂú®Ë°®Á§∫È´òÈ†ªÁéáÁµÑÊàêÂíåÁ¥∞Á≤íÂ∫¶Á¥∞ÁØÄÊñπÈù¢Ë°®ÁèæÂá∫ÂæàÂ§ßÁöÑÂ±ÄÈôêÊÄß„ÄÇÂú®Ë®±Â§öÈÜ´Â≠∏ÂΩ±ÂÉè‰ªªÂãô‰∏≠ÔºåÁî±ÊñºÊ∂âÂèäÂõ∫ÊúâÁöÑË§áÈõúËß£ÂâñÁµêÊßã„ÄÅÊ¨°Ë¶ñË¶∫ÁâπÂæµÂíåË§áÈõúÈÇäÁïåÔºåÂõ†Ê≠§Ê∫ñÁ¢∫Ë°®Á§∫Ê≠§È°û‰ø°ÊÅØËá≥ÈóúÈáçË¶Å„ÄÇÂõ†Ê≠§ÔºåÊµÅË°åÂü∫Á§éÊ®°ÂûãÁöÑÊúâÈôêË°®Á§∫ÂèØËÉΩÊúÉÂ∞éËá¥ÈÄô‰∫õ‰ªªÂãôÁöÑÈ°ØËëóÊÄßËÉΩ‰∏ãÈôçÁîöËá≥Â§±Êïó„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫È†ªÁéáÂÖàÈÄ≤Ë°®Á§∫Ëá™ÂãïÁ∑®Á¢ºÂô® (Frepa) ÁöÑÊñ∞Á©éÈ†êË®ìÁ∑¥Á≠ñÁï•„ÄÇÈÄöÈÅéÈ´òÈ†ªÊé©ËîΩÂíå‰ΩéÈ†ªÊìæÂãïËàáÂ∞çÊäóÂ≠∏ÁøíÁõ∏ÁµêÂêàÔºåFrepa ÈºìÂãµÁ∑®Á¢ºÂô®ÊúâÊïàË°®Á§∫Âíå‰øùÁïôÂúñÂÉèÂµåÂÖ•‰∏≠ÁöÑÈ´òÈ†ªÁµÑÊàê„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÁõ¥ÊñπÂúñÂùáË°°ÂúñÂÉèÊé©ËîΩÁ≠ñÁï•ÔºåÂ∞áÊé©ËîΩËá™ÂãïÁ∑®Á¢ºÂô®ÊñπÊ≥ïÂæû ViT Êì¥Â±ïÂà∞ÂÖ∂‰ªñÊû∂ÊßãÔºå‰æãÂ¶Ç Swin Transformer ÂíåÂç∑Á©çÁ∂≤Ë∑Ø„ÄÇÊàëÂÄëÂú®‰πùÁ®ÆÈÜ´Â≠∏Ê®°Âºè‰∏ãÈñãÁôº‰∫Ü FrepaÔºå‰∏¶Âú® 32 ÂÄã‰∏ãÊ∏∏‰ªªÂãô‰∏≠Â∞çÂÖ∂ÈÄ≤Ë°å‰∫ÜÈ©óË≠âÔºåÂåÖÊã¨ 2D ÂúñÂÉèÂíå 3D È´îÁ©çÊï∏Êìö„ÄÇÂú®‰∏çÈÄ≤Ë°åÂæÆË™øÁöÑÊÉÖÊ≥Å‰∏ãÔºåFrepa ÂèØ‰ª•ÂÑ™ÊñºÂÖ∂‰ªñËá™ÊàëÁõ£Áù£È†êË®ìÁ∑¥ÊñπÊ≥ïÔºå‰∏¶‰∏îÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÁîöËá≥Ë∂ÖÈÅé‰∫ÜÁâπÂÆö‰ªªÂãôË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÈÄôÁ®ÆÊîπÈÄ≤Â∞çÊñºÊ∂âÂèäÁ¥∞Á≤íÂ∫¶Á¥∞ÁØÄÁöÑ‰ªªÂãôÂ∞§ÂÖ∂È°ØËëóÔºå‰æãÂ¶ÇÂú®Ë¶ñÁ∂≤ËÜúË°ÄÁÆ°ÂàÜÂâ≤‰∏≠Â∞á DSC ÊèêÈ´ò‰∫Ü 15%ÔºåÂú®ËÇ∫ÁµêÁØÄÊ™¢Ê∏¨‰∏≠Â∞á IoU ÊèêÈ´ò‰∫Ü 7%„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÂØ¶È©óÂÆöÈáèÂú∞Ë°®ÊòéÔºåFrepa ËÉΩÂ§†Âú®ÂµåÂÖ•‰∏≠ÂØ¶ÁèæÂÑ™Ë∂äÁöÑÈ´òÈ†ªË°®Á§∫Âíå‰øùÁïôÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂÖ∂ÈñãÁôºÊõ¥ÈÄöÁî®ÂíåÈÄöÁî®ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂü∫Á§éÊ®°ÂûãÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models**
2407.14640v1 by Rikhiya Ghosh, Oladimeji Farri, Hans-Martin von Stockhausen, Martin Schmitt, George Marica Vasile

The healthcare industry is currently experiencing an unprecedented wave of
cybersecurity attacks, impacting millions of individuals. With the discovery of
thousands of vulnerabilities each month, there is a pressing need to drive the
automation of vulnerability assessment processes for medical devices,
facilitating rapid mitigation efforts. Generative AI systems have
revolutionized various industries, offering unparalleled opportunities for
automation and increased efficiency. This paper presents a solution leveraging
Large Language Models (LLMs) to learn from historical evaluations of
vulnerabilities for the automatic assessment of vulnerabilities in the medical
devices industry. This approach is applied within the portfolio of a single
manufacturer, taking into account device characteristics, including existing
security posture and controls. The primary contributions of this paper are
threefold. Firstly, it provides a detailed examination of the best practices
for training a vulnerability Language Model (LM) in an industrial context.
Secondly, it presents a comprehensive comparison and insightful analysis of the
effectiveness of Language Models in vulnerability assessment. Finally, it
proposes a new human-in-the-loop framework to expedite vulnerability evaluation
processes.

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•Áî¢Ê•≠ÁõÆÂâçÊ≠£Á∂ìÊ≠∑‰∏ÄÂâçÊâÄÊú™ÊúâÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®ÊîªÊìäÊµ™ÊΩÆÔºåÂΩ±Èüø‰∫ÜÊï∏ÁôæËê¨‰∫∫„ÄÇÈö®ËëóÊØèÊúàÁôºÁèæÊï∏ÂçÉÂÄãÊºèÊ¥ûÔºåËø´ÂàáÈúÄË¶ÅÊé®ÂãïÈÜ´ÁôÇÂô®ÊùêÊºèÊ¥ûË©ï‰º∞Á®ãÂ∫èÁöÑËá™ÂãïÂåñÔºå‰ª•Âà©ÊñºÂø´ÈÄüÊé°ÂèñÁ∑©Ëß£Êé™ÊñΩ„ÄÇÁîüÊàêÂºè AI Á≥ªÁµ±Â∑≤Á∂ìÂæπÂ∫ïÊîπËÆä‰∫ÜÂêÑÁî¢Ê•≠ÔºåÁÇ∫Ëá™ÂãïÂåñÂíåÊèêÈ´òÊïàÁéáÊèê‰æõ‰∫ÜÁÑ°ËàáÂÄ´ÊØîÁöÑÊ©üÊúÉ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËß£Ê±∫ÊñπÊ°àÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂæûÊºèÊ¥ûÁöÑÊ≠∑Âè≤Ë©ï‰º∞‰∏≠Â≠∏ÁøíÔºå‰ª•Ëá™ÂãïË©ï‰º∞ÈÜ´ÁôÇÂô®ÊùêÁî¢Ê•≠‰∏≠ÁöÑÊºèÊ¥û„ÄÇÊ≠§ÊñπÊ≥ïÊáâÁî®ÊñºÂñÆ‰∏ÄË£ΩÈÄ†ÂïÜÁöÑÁî¢ÂìÅÁµÑÂêà‰∏≠ÔºåËÄÉÈáè‰∫ÜË£ùÁΩÆÁâπÊÄßÔºåÂåÖÊã¨ÁèæÊúâÁöÑÂÆâÂÖ®ÊÖãÂã¢ÂíåÊéßÂà∂Êé™ÊñΩ„ÄÇÊú¨ÊñáÁöÑ‰∏ªË¶ÅË≤¢ÁçªÊúâ‰∏âÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÂÆÉÊèê‰æõ‰∫ÜÂú®Áî¢Ê•≠ËÉåÊôØ‰∏ãË®ìÁ∑¥ÊºèÊ¥ûË™ûË®ÄÊ®°Âûã (LM) ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôË©≥Á¥∞Ë™™Êòé„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉÊèêÂá∫‰∫ÜË™ûË®ÄÊ®°ÂûãÂú®ÊºèÊ¥ûË©ï‰º∞‰∏≠ÁöÑÊúâÊïàÊÄß‰πãÂÖ®Èù¢ÊØîËºÉÂíåÊ∑±ÂÖ•ÂàÜÊûê„ÄÇÊúÄÂæåÔºåÂÆÉÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑ„Äå‰∫∫Ê©üÂçî‰Ωú„ÄçÊû∂ÊßãÔºå‰ª•Âä†ÈÄüÊºèÊ¥ûË©ï‰º∞Á®ãÂ∫è„ÄÇ

##### **Two new feature selection methods based on learn-heuristic techniques for breast cancer prediction: A comprehensive analysis**
2407.14631v1 by Kamyab Karimi, Ali Ghodratnama, Reza Tavakkoli-Moghaddam

Breast cancer is not preventable because of its unknown causes. However, its
early diagnosis increases patients' recovery chances. Machine learning (ML) can
be utilized to improve treatment outcomes in healthcare operations while
diminishing costs and time. In this research, we suggest two novel feature
selection (FS) methods based upon an imperialist competitive algorithm (ICA)
and a bat algorithm (BA) and their combination with ML algorithms. This study
aims to enhance diagnostic models' efficiency and present a comprehensive
analysis to help clinical physicians make much more precise and reliable
decisions than before. K-nearest neighbors, support vector machine, decision
tree, Naive Bayes, AdaBoost, linear discriminant analysis, random forest,
logistic regression, and artificial neural network are some of the methods
employed. This paper applied a distinctive integration of evaluation measures
and ML algorithms using the wrapper feature selection based on ICA (WFSIC) and
BA (WFSB) separately. We compared two proposed approaches for the performance
of the classifiers. Also, we compared our best diagnostic model with previous
works reported in the literature survey. Experimentations were performed on the
Wisconsin diagnostic breast cancer dataset. Results reveal that the proposed
framework that uses the BA with an accuracy of 99.12\%, surpasses the framework
using the ICA and most previous works. Additionally, the RF classifier in the
approach of FS based on BA emerges as the best model and outperforms others
regarding its criteria. Besides, the results illustrate the role of our
techniques in reducing the dataset dimensions up to 90\% and increasing the
performance of diagnostic models by over 99\%. Moreover, the result
demonstrates that there are more critical features than the optimum dataset
obtained by proposed FS approaches that have been selected by most ML models.

ÊëòË¶ÅÔºö<paragraph>Áî±Êñº‰π≥ÁôåÁöÑÊàêÂõ†‰∏çÊòéÔºåÂõ†Ê≠§ÁÑ°Ê≥ïÈ†êÈò≤„ÄÇÁÑ∂ËÄåÔºåÊó©ÊúüË®∫Êñ∑ÂèØÂ¢ûÂä†ÊÇ£ËÄÖÁöÑÂ∫∑Âæ©Ê©üÊúÉ„ÄÇÊ©üÂô®Â≠∏Áøí (ML) ÂèØÁî®ÊñºÊîπÂñÑÈÜ´ÁôÇ‰øùÂÅ•‰ΩúÊ•≠‰∏≠ÁöÑÊ≤ªÁôÇÊàêÊûúÔºåÂêåÊôÇÈôç‰ΩéÊàêÊú¨ÂíåÊôÇÈñì„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÂÖ©Á®ÆÂü∫ÊñºÂ∏ùÂúã‰∏ªÁæ©Á´∂Áà≠ÊºîÁÆóÊ≥ï (ICA) ÂíåËùôËù†ÊºîÁÆóÊ≥ï (BA) ÁöÑÊñ∞ÁâπÂæµÈÅ∏Êìá (FS) ÊñπÊ≥ïÔºå‰ª•ÂèäÂÆÉÂÄëËàá ML ÊºîÁÆóÊ≥ïÁöÑÁµÑÂêà„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÊèêÈ´òË®∫Êñ∑Ê®°ÂûãÁöÑÊïàÁéáÔºå‰∏¶Êèê‰æõÂÖ®Èù¢ÁöÑÂàÜÊûêÔºå‰ª•Âπ´Âä©Ëá®Â∫äÈÜ´Â∏´ÂÅöÂá∫ÊØî‰ª•ÂæÄÊõ¥Á≤æÁ¢∫‰∏îÂèØÈù†ÁöÑÊ±∫Á≠ñ„ÄÇK ÊúÄËøëÈÑ∞„ÄÅÊîØÊè¥ÂêëÈáèÊ©ü„ÄÅÊ±∫Á≠ñÊ®π„ÄÅÊ®∏Á¥†Ë≤ùÊ∞è„ÄÅAdaBoost„ÄÅÁ∑öÊÄßÂà§Âà•ÂàÜÊûê„ÄÅÈö®Ê©üÊ£ÆÊûó„ÄÅÈÇèËºØËø¥Ê≠∏Âíå‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑ØÊòØ‰∏Ä‰∫õÊâÄÊé°Áî®ÁöÑÊñπÊ≥ï„ÄÇÊú¨Êñá‰ΩøÁî®Âü∫Êñº ICA (WFSIC) Âíå BA (WFSB) ÁöÑÂåÖË£ùÁâπÂæµÈÅ∏ÊìáÔºåÊáâÁî®Ë©ï‰º∞Êé™ÊñΩÂíå ML ÊºîÁÆóÊ≥ïÁöÑÁç®ÁâπÊï¥Âêà„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂÖ©Á®ÆÊèêÂá∫ÁöÑÊñπÊ≥ï‰ª•Ë©ï‰º∞ÂàÜÈ°ûÂô®ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÊàëÂÄëÊúÄÂ•ΩÁöÑË®∫Êñ∑Ê®°ÂûãËàáÊñáÁçªË™øÊü•‰∏≠Â†±Â∞éÁöÑÂÖàÂâçÁ†îÁ©∂ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂØ¶È©óÊòØÂú®Â®ÅÊñØÂ∫∑ËæõË®∫Êñ∑‰π≥ÁôåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑ„ÄÇÁµêÊûúÈ°ØÁ§∫Ôºå‰ΩøÁî® BA ÁöÑÊì¨Ë≠∞Êû∂ÊßãÊ∫ñÁ¢∫ÁéáÁÇ∫ 99.12%ÔºåÂÑ™Êñº‰ΩøÁî® ICA ÂíåÂ§ßÂ§öÊï∏ÂÖàÂâçÁ†îÁ©∂ÁöÑÊû∂Êßã„ÄÇÊ≠§Â§ñÔºåÂü∫Êñº BA ÁöÑ FS ÊñπÊ≥ï‰∏≠ÁöÑ RF ÂàÜÈ°ûÂô®ÊàêÁÇ∫ÊúÄ‰Ω≥Ê®°ÂûãÔºå‰∏¶Âú®Ê®ôÊ∫ñÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÁµêÊûúË™™Êòé‰∫ÜÊàëÂÄëÁöÑÊäÄË°ìÂú®Â∞áË≥áÊñôÈõÜÁ∂≠Â∫¶Ê∏õÂ∞ëÂ§öÈÅî 90% ‰ª•ÂèäÂ∞áË®∫Êñ∑Ê®°ÂûãÁöÑÊïàËÉΩÊèêÈ´òË∂ÖÈÅé 99% ‰∏≠ÊâÄÊâÆÊºîÁöÑËßíËâ≤„ÄÇÊ≠§Â§ñÔºåÁµêÊûúË°®ÊòéÔºåÁî±Â§ßÂ§öÊï∏ ML Ê®°ÂûãÊâÄÈÅ∏ÂèñÁöÑÔºåÊØîÁî±ÊèêÂá∫ÁöÑ FS ÊñπÊ≥ïÊâÄÁç≤ÂæóÁöÑÊúÄ‰Ω≥Ë≥áÊñôÈõÜÊõ¥ÈáçË¶ÅÁöÑÁâπÂæµÈÇÑÊõ¥Â§ö„ÄÇ</paragraph>

##### **Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model**
2407.14326v1 by Kun Zhao, Jakub Prokop, Javier Montalt Tordera, Sadegh Mohammadi

Mammography is crucial for breast cancer surveillance and early diagnosis.
However, analyzing mammography images is a demanding task for radiologists, who
often review hundreds of mammograms daily, leading to overdiagnosis and
overtreatment. Computer-Aided Diagnosis (CAD) systems have been developed to
assist in this process, but their capabilities, particularly in lesion
segmentation, remained limited. With the contemporary advances in deep learning
their performance may be improved. Recently, vision-language diffusion models
emerged, demonstrating outstanding performance in image generation and
transferability to various downstream tasks. We aim to harness their
capabilities for breast lesion segmentation in a panoptic setting, which
encompasses both semantic and instance-level predictions. Specifically, we
propose leveraging pretrained features from a Stable Diffusion model as inputs
to a state-of-the-art panoptic segmentation architecture, resulting in accurate
delineation of individual breast lesions. To bridge the gap between natural and
medical imaging domains, we incorporated a mammography-specific MAM-E diffusion
model and BiomedCLIP image and text encoders into this framework. We evaluated
our approach on two recently published mammography datasets, CDD-CESM and
VinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82
AP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation
task, we achieved Dice scores of 38.86 and 40.92, respectively.

ÊëòË¶ÅÔºö‰π≥ÊàøÊîùÂΩ±Â∞çÊñº‰π≥ÁôåÁõ£ÊéßÂíåÊó©ÊúüË®∫Êñ∑Ëá≥ÈóúÈáçË¶Å„ÄÇ
ÁÑ∂ËÄåÔºåÂàÜÊûê‰π≥ÊàøÊîùÂΩ±ÂΩ±ÂÉèÂ∞çÊîæÂ∞ÑÁßëÈÜ´Â∏´‰æÜË™™ÊòØ‰∏ÄÈ†ÖËâ±ÈâÖÁöÑ‰ªªÂãôÔºå‰ªñÂÄë
ÊØèÂ§©Á∂ìÂ∏∏Ê™¢Èñ±Êï∏ÁôæÂºµ‰π≥ÊàøÊîùÂΩ±ÂΩ±ÂÉèÔºåÂ∞éËá¥ÈÅéÂ∫¶Ë®∫Êñ∑Âíå
ÈÅéÂ∫¶Ê≤ªÁôÇ„ÄÇÈõªËÖ¶ËºîÂä©Ë®∫Êñ∑ (CAD) Á≥ªÁµ±Â∑≤ÈñãÁôºÂá∫‰æÜ‰ª•
ÂçîÂä©Ê≠§ÊµÅÁ®ãÔºå‰ΩÜÂÖ∂ÂäüËÉΩÔºåÁâπÂà•ÊòØÂú®ÁóÖÁÅ∂
ÂàÜÂâ≤ÊñπÈù¢Ôºå‰ªçÁÑ∂ÊúâÈôê„ÄÇÈö®ËëóÊ∑±Â∫¶Â≠∏ÁøíÁöÑÁï∂‰ª£ÈÄ≤Â±ï
ÂÖ∂ÊÄßËÉΩÂèØËÉΩÊúÉÂæóÂà∞ÊîπÂñÑ„ÄÇÊúÄËøëÔºåË¶ñË¶∫Ë™ûË®ÄÊì¥Êï£Ê®°Âûã
Âá∫ÁèæÔºåÂú®ÂΩ±ÂÉèÁîüÊàêÂíå
ÂèØËΩâÁßªÂà∞ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂÇëÂá∫ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÊó®Âú®Âà©Áî®ÂÖ∂
ÂäüËÉΩÂú®ÂÖ®ÊôØË®≠ÁΩÆ‰∏≠ÈÄ≤Ë°å‰π≥ÊàøÁóÖÁÅ∂ÂàÜÂâ≤ÔºåÂÖ∂‰∏≠
ÂåÖÂê´Ë™ûÁæ©ÂíåÂØ¶‰æãÁ¥öÂà•È†êÊ∏¨„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë
Âª∫Ë≠∞Âà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑ Stable Diffusion Ê®°Âûã‰∏≠ÁöÑÁâπÂæµ‰ΩúÁÇ∫Ëº∏ÂÖ•
Âà∞ÊúÄÂÖàÈÄ≤ÁöÑÂÖ®ÊôØÂàÜÂâ≤Êû∂ÊßãÔºåÂæûËÄåÁ≤æÁ¢∫Âú∞ÊèèÁπ™ÂÄãÂà•‰π≥ÊàøÁóÖÁÅ∂„ÄÇÁÇ∫‰∫ÜÂΩåÂêàËá™ÁÑ∂Âíå
ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüü‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÊàëÂÄëÂ∞á‰π≥ÊàøÊîùÂΩ±Â∞àÁî®ÁöÑ MAM-E Êì¥Êï£
Ê®°ÂûãÂíå BiomedCLIP ÂΩ±ÂÉèÂíåÊñáÂ≠óÁ∑®Á¢ºÂô®Á¥çÂÖ•ÈÄôÂÄãÊû∂Êßã‰∏≠„ÄÇÊàëÂÄëË©ï‰º∞
ÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂÖ©ÂÄãÊúÄËøëÁôºÂ∏ÉÁöÑ‰π≥ÊàøÊîùÂΩ±Ë≥áÊñôÈõÜ CDD-CESM Âíå
VinDr-Mammo„ÄÇÂ∞çÊñºÂØ¶‰æãÂàÜÂâ≤‰ªªÂãôÔºåÊàëÂÄëÊ≥®ÊÑèÂà∞ 40.25 AP0.1 Âíå 46.82
AP0.05Ôºå‰ª•Âèä 25.44 PQ0.1 Âíå 26.92 PQ0.05„ÄÇÂ∞çÊñºË™ûÁæ©ÂàÜÂâ≤
‰ªªÂãôÔºåÊàëÂÄëÂàÜÂà•ÈÅîÂà∞‰∫Ü 38.86 Âíå 40.92 ÁöÑ Dice ÂàÜÊï∏„ÄÇ

##### **Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based Undersampling Method for Bias Reduction**
2407.14210v1 by Jos√© Daniel Pascual-Triana, Alberto Fern√°ndez, Paulo Novais, Francisco Herrera

Given the magnitude of data generation currently, both in quantity and speed,
the use of machine learning is increasingly important. When data include
protected features that might give rise to discrimination, special care must be
taken. Data quality is critical in these cases, as biases in training data can
be reflected in classification models. This has devastating consequences and
fails to comply with current regulations. Data-Centric Artificial Intelligence
proposes dataset modifications to improve its quality. Instance selection via
undersampling can foster balanced learning of classes and protected feature
values in the classifier. When such undersampling is done close to the decision
boundary, the effect on the classifier would be bolstered. This work proposes
Fair Overlap Number of Balls (Fair-ONB), an undersampling method that harnesses
the data morphology of the different data groups (obtained from the combination
of classes and protected feature values) to perform guided undersampling in the
areas where they overlap. It employs attributes of the ball coverage of the
groups, such as the radius, number of covered instances and density, to select
the most suitable areas for undersampling and reduce bias. Results show that
the Fair-ONB method reduces bias with low impact on the classifier's predictive
performance.

ÊëòË¶ÅÔºö<paragraph>Èâ¥‰∫éÂΩìÂâçÊï∞ÊçÆÁîüÊàêÁöÑËßÑÊ®°ÔºåÊó†ËÆ∫ÊòØÂú®Êï∞ÈáèËøòÊòØÈÄüÂ∫¶‰∏äÔºåÊú∫Âô®Â≠¶‰π†ÁöÑ‰ΩøÁî®ÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇÂΩìÊï∞ÊçÆÂåÖÂê´ÂèØËÉΩÂØºËá¥Ê≠ßËßÜÁöÑÂèó‰øùÊä§ÁâπÂæÅÊó∂ÔºåÂøÖÈ°ªÁâπÂà´Â∞èÂøÉ„ÄÇÂú®Ëøô‰∫õÊÉÖÂÜµ‰∏ãÔºåÊï∞ÊçÆË¥®ÈáèËá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÂÅèÂ∑ÆÂèØËÉΩ‰ºöÂèçÊò†Âú®ÂàÜÁ±ªÊ®°Âûã‰∏≠„ÄÇËøô‰ºö‰∫ßÁîüÊØÅÁÅ≠ÊÄßÁöÑÂêéÊûúÔºåÂπ∂‰∏î‰∏çÁ¨¶ÂêàÂΩìÂâçÊ≥ïËßÑ„ÄÇ‰ª•Êï∞ÊçÆ‰∏∫‰∏≠ÂøÉÁöÑ AI ÊèêÂá∫‰∫ÜÊï∞ÊçÆÈõÜ‰øÆÊîπ‰ª•ÊèêÈ´òÂÖ∂Ë¥®Èáè„ÄÇÈÄöËøáÊ¨†ÈááÊ†∑ËøõË°åÂÆû‰æãÈÄâÊã©ÂèØ‰ª•‰øÉËøõÂàÜÁ±ªÂô®‰∏≠Á±ªÂíåÂèó‰øùÊä§ÁâπÂæÅÂÄºÁöÑÂπ≥Ë°°Â≠¶‰π†„ÄÇÂΩìÊ≠§Á±ªÊ¨†ÈááÊ†∑Êé•ËøëÂÜ≥Á≠ñËæπÁïåÊó∂ÔºåÂØπÂàÜÁ±ªÂô®ÁöÑÂΩ±ÂìçÂ∞ÜÂæóÂà∞Âä†Âº∫„ÄÇËøôÈ°πÂ∑•‰ΩúÊèêÂá∫‰∫ÜÂÖ¨Âπ≥ÈáçÂè†ÁêÉÊï∞ (Fair-ONB)ÔºåËøôÊòØ‰∏ÄÁßçÊ¨†ÈááÊ†∑ÊñπÊ≥ïÔºåÂà©Áî®‰∏çÂêåÊï∞ÊçÆÁªÑÔºà‰ªéÁ±ªÂíåÂèó‰øùÊä§ÁâπÂæÅÂÄºÁöÑÁªÑÂêà‰∏≠Ëé∑ÂæóÔºâÁöÑÊï∞ÊçÆÂΩ¢ÊÄÅÔºåÂú®ÂÆÉ‰ª¨ÈáçÂè†ÁöÑÂå∫ÂüüÊâßË°åÂºïÂØºÊ¨†ÈááÊ†∑„ÄÇÂÆÉÈááÁî®ÁêÉË¶ÜÁõñÁöÑÂ±ûÊÄßÔºå‰æãÂ¶ÇÂçäÂæÑ„ÄÅË¶ÜÁõñÂÆû‰æãÊï∞ÂíåÂØÜÂ∫¶Ôºå‰ª•ÈÄâÊã©ÊúÄÈÄÇÂêàÊ¨†ÈááÊ†∑ÁöÑÂå∫ÂüüÂπ∂ÂáèÂ∞ëÂÅèÂ∑Æ„ÄÇÁªìÊûúË°®ÊòéÔºåFair-ONB ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂÅèÂ∑ÆÔºåÂØπÂàÜÁ±ªÂô®ÁöÑÈ¢ÑÊµãÊÄßËÉΩÂΩ±ÂìçÂæàÂ∞è„ÄÇ</paragraph>

##### **Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field**
2407.14076v2 by Tobias Kerner

There are many cases where LLMs are used for specific tasks in a single
domain. These usually require less general, but more domain-specific knowledge.
Highly capable, general-purpose state-of-the-art language models like GPT-4 or
Claude-3-opus can often be used for such tasks, but they are very large and
cannot be run locally, even if they were not proprietary. This can be a problem
when working with sensitive data. This paper focuses on domain-specific and
mixed-domain pretraining as potentially more efficient methods than general
pretraining for specialized language models. We will take a look at work
related to domain-specific pretraining, specifically in the medical area, and
compare benchmark results of specialized language models to general-purpose
language models.

ÊëòË¶ÅÔºöÂú®Ë®±Â§öÊÉÖÊ≥Å‰∏ãÔºåLLM Áî®ÊñºÂñÆ‰∏ÄÈ†òÂüü‰∏≠ÁöÑÁâπÂÆö‰ªªÂãô„ÄÇÈÄô‰∫õ‰ªªÂãôÈÄöÂ∏∏ÈúÄË¶ÅËºÉÂ∞ëÁöÑÈÄöÁî®Áü•Ë≠òÔºå‰ΩÜÈúÄË¶ÅÊõ¥Â§öÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠ò„ÄÇÂäüËÉΩÂº∑Â§ß„ÄÅÈÄöÁî®‰∏îÊúÄÂÖàÈÄ≤ÁöÑË™ûË®ÄÊ®°ÂûãÔºå‰æãÂ¶Ç GPT-4 Êàñ Claude-3-opusÔºåÈÄöÂ∏∏ÂèØÁî®ÊñºÊ≠§È°û‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÈùûÂ∏∏ÈæêÂ§ßÔºåÂç≥‰Ωø‰∏çÊòØÂ∞àÊúâËªüÈ´îÔºå‰πüÁÑ°Ê≥ïÂú®Êú¨Âú∞Âü∑Ë°å„ÄÇÂú®ËôïÁêÜÊïèÊÑüË≥áÊñôÊôÇÔºåÈÄôÂèØËÉΩÊúÉÊàêÁÇ∫‰∏ÄÂÄãÂïèÈ°å„ÄÇÊú¨ÊñáÈáçÈªûÊé¢Ë®éÈ†òÂüüÁâπÂÆöÂíåÊ∑∑ÂêàÈ†òÂüüÈ†êË®ìÁ∑¥Ôºå‰ΩúÁÇ∫ÊØî‰∏ÄËà¨È†êË®ìÁ∑¥Êõ¥ÊúâÊïàÁéáÁöÑÂ∞àÊ•≠Ë™ûË®ÄÊ®°ÂûãÊΩõÂú®ÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞áÊé¢Ë®éËàáÈ†òÂüüÁâπÂÆöÈ†êË®ìÁ∑¥Áõ∏ÈóúÁöÑÂ∑•‰ΩúÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇÈ†òÂüüÔºå‰∏¶ÊØîËºÉÂ∞àÊ•≠Ë™ûË®ÄÊ®°ÂûãÂíåÈÄöÁî®Ë™ûË®ÄÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûú„ÄÇ

##### **HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research**
2407.14030v1 by Prerana Sanjay Kulkarni, Muskaan Jain, Disha Sheshanarayana, Srinivasan Parthiban

Despite advancements in drug development strategies, 90% of clinical trials
fail. This suggests overlooked aspects in target validation and drug
optimization. In order to address this, we introduce HeCiX-KG,
Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from
ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines
data on previously conducted clinical trials from ClinicalTrials.gov, and
domain expertise on diseases and genes from Hetionet. This offers a thorough
resource for clinical researchers. Further, we introduce HeCiX, a system that
uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.
HeCiX shows high performance during evaluation against a range of clinically
relevant issues, proving this model to be promising for enhancing the
effectiveness of clinical research. Thus, this approach provides a more
holistic view of clinical trials and existing biological data.

ÊëòË¶ÅÔºöÂÑòÁÆ°Ëó•Áâ©ÈñãÁôºÁ≠ñÁï•ÊúâÈÄ≤Â±ïÔºå90% ÁöÑËá®Â∫äË©¶È©óÈÉΩÂ§±Êïó‰∫Ü„ÄÇÈÄôË°®Á§∫Âú®ÁõÆÊ®ôÈ©óË≠âÂíåËó•Áâ©ÊúÄ‰Ω≥ÂåñÊñπÈù¢ÊúâË¢´ÂøΩÁï•ÁöÑÂ±§Èù¢„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü HeCiX-KGÔºåHetionet-Clinicaltrials neXus Áü•Ë≠òÂúñË≠úÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞á ClinicalTrials.gov Âíå Hetionet ÁöÑË≥áÊñôËûçÂêàÂú®ÂñÆ‰∏ÄÁü•Ë≠òÂúñË≠ú‰∏≠ÁöÑÊñ∞Á©éËûçÂêà„ÄÇHeCiX-KG ÁµêÂêà‰∫Ü‰æÜËá™ ClinicalTrials.gov ÁöÑÂÖàÂâçÂü∑Ë°åËá®Â∫äË©¶È©óË≥áÊñôÔºå‰ª•Âèä‰æÜËá™ Hetionet ÁöÑÁñæÁóÖÂíåÂü∫Âõ†È†òÂüüÂ∞àÊ•≠Áü•Ë≠ò„ÄÇÈÄôÁÇ∫Ëá®Â∫äÁ†îÁ©∂‰∫∫Âì°Êèê‰æõ‰∫ÜË±êÂØåÁöÑË≥áÊ∫ê„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü HeCiXÔºå‰∏ÄÂÄã‰ΩøÁî® LangChain Â∞á HeCiX-KG Ëàá GPT-4 Êï¥ÂêàÔºå‰∏¶ÊèêÈ´òÂÖ∂ÂèØÁî®ÊÄßÁöÑÁ≥ªÁµ±„ÄÇHeCiX Âú®ÈáùÂ∞ç‰∏ÄÁ≥ªÂàóËá®Â∫äÁõ∏ÈóúÂïèÈ°åÁöÑË©ï‰º∞‰∏≠Ë°®ÁèæÂá∫È´òÊÄßËÉΩÔºåË≠âÊòé‰∫ÜÈÄôÂÄãÊ®°ÂûãÊúâÊúõÊèêÈ´òËá®Â∫äÁ†îÁ©∂ÁöÑÊúâÊïàÊÄß„ÄÇÂõ†Ê≠§ÔºåÈÄôÁ®ÆÊñπÊ≥ïÊèê‰æõ‰∫ÜÂ∞çËá®Â∫äË©¶È©óÂíåÁèæÊúâÁîüÁâ©Ë≥áÊñôÊõ¥ÂÖ®Èù¢ÁöÑÁúãÊ≥ï„ÄÇ

##### **DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention**
2407.13920v1 by Xiaoya Tang, Bodong Zhang, Beatrice S. Knudsen, Tolga Tasdizen

We here propose a novel hierarchical transformer model that adeptly
integrates the feature extraction capabilities of Convolutional Neural Networks
(CNNs) with the advanced representational potential of Vision Transformers
(ViTs). Addressing the lack of inductive biases and dependence on extensive
training datasets in ViTs, our model employs a CNN backbone to generate
hierarchical visual representations. These representations are then adapted for
transformer input through an innovative patch tokenization. We also introduce a
'scale attention' mechanism that captures cross-scale dependencies,
complementing patch attention to enhance spatial understanding and preserve
global perception. Our approach significantly outperforms baseline models on
small and medium-sized medical datasets, demonstrating its efficiency and
generalizability. The components are designed as plug-and-play for different
CNN architectures and can be adapted for multiple applications. The code is
available at https://github.com/xiaoyatang/DuoFormer.git.

ÊëòË¶ÅÔºöÊàëÂÄëÂú®Ê≠§ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÂ±§TransformerÊ®°ÂûãÔºåÂÆÉÂ∑ßÂ¶ôÂú∞Êï¥Âêà‰∫ÜÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÁâπÂæµÊì∑ÂèñËÉΩÂäõÔºå‰ª•ÂèäË¶ñË¶∫Transformer (ViT) ÁöÑÂÖàÈÄ≤Ë°®Á§∫ÊΩõÂäõ„ÄÇÈáùÂ∞ç ViT ‰∏≠Áº∫‰πèÊ≠∏Á¥çÂÅèË™§Âíå‰æùË≥¥ÊñºÂª£Ê≥õË®ìÁ∑¥Ë≥áÊñôÈõÜÁöÑÂïèÈ°åÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî® CNN ‰∏ªÂππ‰æÜÁî¢ÁîüÂàÜÂ±§Ë¶ñË¶∫Ë°®Á§∫„ÄÇÈÄô‰∫õË°®Á§∫Êé•ËëóÈÄèÈÅéÂâµÊñ∞ÁöÑÂçÄÂ°äÊ®ôË®òÂåñÔºåË™øÊï¥ÁÇ∫TransformerËº∏ÂÖ•„ÄÇÊàëÂÄë‰πüÂºïÂÖ•„ÄåÂ∞∫Â∫¶Ê≥®ÊÑèÂäõ„ÄçÊ©üÂà∂ÔºåÂÆÉÊçïÊçâË∑®Â∞∫Â∫¶‰æùË≥¥ÊÄßÔºåË£úÂÖÖÂçÄÂ°äÊ≥®ÊÑèÂäõ‰ª•Â¢ûÂº∑Á©∫ÈñìÁêÜËß£‰∏¶‰øùÁïôÂÖ®Â±ÄÊÑüÁü•„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Â∞èÂûãÂíå‰∏≠ÂûãÁöÑÈÜ´Â≠∏Ë≥áÊñôÈõÜ‰∏äÔºåÊòéÈ°ØÂÑ™ÊñºÂü∫Á∑öÊ®°ÂûãÔºåË≠âÊòé‰∫ÜÂÆÉÁöÑÊïàÁéáÂíåÂèØÊ¶ÇÂåñÊÄß„ÄÇÈÄô‰∫õÁµÑ‰ª∂Ë¢´Ë®≠Ë®àÊàêÂç≥ÊèíÂç≥Áî®ÔºåÈÅ©Áî®Êñº‰∏çÂêåÁöÑ CNN Êû∂ÊßãÔºå‰∏¶‰∏îÂèØ‰ª•Ë™øÊï¥ÁÇ∫Â§öÁ®ÆÊáâÁî®Á®ãÂºè„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/xiaoyatang/DuoFormer.git ÂèñÂæó„ÄÇ

##### **Data-Algorithm-Architecture Co-Optimization for Fair Neural Networks on Skin Lesion Dataset**
2407.13896v1 by Yi Sheng, Junhuan Yang, Jinyang Li, James Alaina, Xiaowei Xu, Yiyu Shi, Jingtong Hu, Weiwen Jiang, Lei Yang

As Artificial Intelligence (AI) increasingly integrates into our daily lives,
fairness has emerged as a critical concern, particularly in medical AI, where
datasets often reflect inherent biases due to social factors like the
underrepresentation of marginalized communities and socioeconomic barriers to
data collection. Traditional approaches to mitigating these biases have focused
on data augmentation and the development of fairness-aware training algorithms.
However, this paper argues that the architecture of neural networks, a core
component of Machine Learning (ML), plays a crucial role in ensuring fairness.
We demonstrate that addressing fairness effectively requires a holistic
approach that simultaneously considers data, algorithms, and architecture.
Utilizing Automated ML (AutoML) technology, specifically Neural Architecture
Search (NAS), we introduce a novel framework, BiaslessNAS, designed to achieve
fair outcomes in analyzing skin lesion datasets. BiaslessNAS incorporates
fairness considerations at every stage of the NAS process, leading to the
identification of neural networks that are not only more accurate but also
significantly fairer. Our experiments show that BiaslessNAS achieves a 2.55%
increase in accuracy and a 65.50% improvement in fairness compared to
traditional NAS methods, underscoring the importance of integrating fairness
into neural network architecture for better outcomes in medical AI
applications.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊó•ÁõäËûçÂÖ•ÊàëÂÄëÁöÑÊó•Â∏∏ÁîüÊ¥ªÔºåÂÖ¨Âπ≥ÊÄßÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑËÄÉÈáèÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ AI È†òÂüüÔºåÂÖ∂‰∏≠Áî±ÊñºÁ§æÊúÉÂõ†Á¥†Ôºà‰æãÂ¶ÇÈÇäÁ∑£ÂåñÁ§æÁæ§ÁöÑ‰ª£Ë°®ÊÄß‰∏çË∂≥ÂíåË≥áÊñôÊî∂ÈõÜÁöÑÁ§æÊúÉÁ∂ìÊøüÈöúÁ§ôÔºâÔºåË≥áÊñôÈõÜÂæÄÂæÄÂèçÊò†Âá∫Âõ∫ÊúâÁöÑÂÅèË¶ã„ÄÇÊ∏õËºïÈÄô‰∫õÂÅèË¶ãÁöÑÂÇ≥Áµ±ÊñπÊ≥ïËëóÈáçÊñºË≥áÊñôÊì¥ÂÖÖÂíåÈñãÁôºÊ≥®ÈáçÂÖ¨Âπ≥ÊÄßÁöÑË®ìÁ∑¥ÊºîÁÆóÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÊú¨ÊñáË´ñË≠âÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊû∂ÊßãÔºàÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâÁöÑÊ†∏ÂøÉÁµÑÊàêÈÉ®ÂàÜÔºâÂú®Á¢∫‰øùÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëË≠âÊòéÔºåÊúâÊïàËß£Ê±∫ÂÖ¨Âπ≥ÊÄßÂïèÈ°åÈúÄË¶Å‰∏ÄÁ®ÆÂÖ®Èù¢ÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂêåÊôÇËÄÉÊÖÆË≥áÊñô„ÄÅÊºîÁÆóÊ≥ïÂíåÊû∂Êßã„ÄÇÂà©Áî®Ëá™ÂãïÂåñ MLÔºàAutoMLÔºâÊäÄË°ìÔºåÁâπÂà•ÊòØÁ•ûÁ∂ìÊû∂ÊßãÊêúÂ∞ãÔºàNASÔºâÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ BiaslessNASÔºåÊó®Âú®ÂàÜÊûêÁöÆËÜöÁóÖËÆäË≥áÊñôÈõÜÊôÇÁç≤ÂæóÂÖ¨Âπ≥ÁöÑÁµêÊûú„ÄÇBiaslessNAS Âú® NAS Á®ãÂ∫èÁöÑÊØèÂÄãÈöéÊÆµÈÉΩÁ¥çÂÖ•‰∫ÜÂÖ¨Âπ≥ÊÄßËÄÉÈáèÔºåÂæûËÄåË≠òÂà•Âá∫‰∏çÂÉÖÊõ¥Ê∫ñÁ¢∫ÔºåËÄå‰∏î‰πüÈ°ØËëóÊõ¥ÂÖ¨Âπ≥ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåËàáÂÇ≥Áµ±ÁöÑ NAS ÊñπÊ≥ïÁõ∏ÊØîÔºåBiaslessNAS ÁöÑÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 2.55%ÔºåÂÖ¨Âπ≥ÊÄßÊèêÈ´ò‰∫Ü 65.50%ÔºåÈÄôÂá∏È°Ø‰∫ÜÂ∞áÂÖ¨Âπ≥ÊÄßÊï¥ÂêàÂà∞Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂Êßã‰∏≠Â∞çÊñºÊîπÂñÑÈÜ´ÁôÇ AI ÊáâÁî®‰∏≠ÁöÑÁµêÊûúÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **APS-USCT: Ultrasound Computed Tomography on Sparse Data via AI-Physic Synergy**
2407.14564v1 by Yi Sheng, Hanchen Wang, Yipei Liu, Junhuan Yang, Weiwen Jiang, Youzuo Lin, Lei Yang

Ultrasound computed tomography (USCT) is a promising technique that achieves
superior medical imaging reconstruction resolution by fully leveraging waveform
information, outperforming conventional ultrasound methods. Despite its
advantages, high-quality USCT reconstruction relies on extensive data
acquisition by a large number of transducers, leading to increased costs,
computational demands, extended patient scanning times, and manufacturing
complexities. To mitigate these issues, we propose a new USCT method called
APS-USCT, which facilitates imaging with sparse data, substantially reducing
dependence on high-cost dense data acquisition. Our APS-USCT method consists of
two primary components: APS-wave and APS-FWI. The APS-wave component, an
encoder-decoder system, preprocesses the waveform data, converting sparse data
into dense waveforms to augment sample density prior to reconstruction. The
APS-FWI component, utilizing the InversionNet, directly reconstructs the speed
of sound (SOS) from the ultrasound waveform data. We further improve the
model's performance by incorporating Squeeze-and-Excitation (SE) Blocks and
source encoding techniques. Testing our method on a breast cancer dataset
yielded promising results. It demonstrated outstanding performance with an
average Structural Similarity Index (SSIM) of 0.8431. Notably, over 82% of
samples achieved an SSIM above 0.8, with nearly 61% exceeding 0.85,
highlighting the significant potential of our approach in improving USCT image
reconstruction by efficiently utilizing sparse data.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÈõªËÖ¶Êñ∑Â±§ÊîùÂΩ± (USCT) ÊòØ‰∏ÄÁ®ÆÂæàÊúâÂâçÊôØÁöÑÊäÄË°ìÔºåÂÆÉËÉΩÈÄèÈÅéÂÖÖÂàÜÂà©Áî®Ê≥¢ÂΩ¢Ë≥áË®äÔºåÈÅîÊàêÂÑ™Áï∞ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÈáçÂª∫Ëß£ÊûêÂ∫¶ÔºåË°®ÁèæÂÑ™ÊñºÂÇ≥Áµ±Ë∂ÖÈü≥Ê≥¢ÊñπÊ≥ï„ÄÇÂÑòÁÆ°ÊúâÂÖ∂ÂÑ™ÈªûÔºåÈ´òÂìÅË≥™ÁöÑ USCT ÈáçÂª∫‰æùË≥¥ÊñºÂ§ßÈáèÊèõËÉΩÂô®Âª£Ê≥õÁöÑË≥áÊñôÊì∑ÂèñÔºåÂ∞éËá¥ÊàêÊú¨Â¢ûÂä†„ÄÅÈÅãÁÆóÈúÄÊ±Ç„ÄÅÁóÖÊÇ£ÊéÉÊèèÊôÇÈñìÂª∂Èï∑Ôºå‰ª•ÂèäË£ΩÈÄ†Ë§áÈõúÂ∫¶„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫ APS-USCT ÁöÑÊñ∞Âûã USCT ÊñπÊ≥ïÔºåÂÆÉ‰øÉÈÄ≤‰ΩøÁî®Á®ÄÁñèË≥áÊñôÈÄ≤Ë°åÂΩ±ÂÉèËôïÁêÜÔºåÂ§ßÂπÖÈôç‰ΩéÂ∞çÈ´òÊàêÊú¨ÂØÜÈõÜË≥áÊñôÊì∑ÂèñÁöÑ‰æùË≥¥„ÄÇÊàëÂÄëÁöÑ APS-USCT ÊñπÊ≥ïÂåÖÂê´ÂÖ©ÂÄã‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÔºöAPS-wave Âíå APS-FWI„ÄÇAPS-wave ÁµÑ‰ª∂ÊòØ‰∏ÄÂÄãÁ∑®Á¢ºÂô®Ëß£Á¢ºÂô®Á≥ªÁµ±ÔºåÂÆÉÈ†êÂÖàËôïÁêÜÊ≥¢ÂΩ¢Ë≥áÊñôÔºåÂ∞áÁ®ÄÁñèË≥áÊñôËΩâÊèõÁÇ∫ÂØÜÈõÜÊ≥¢ÂΩ¢Ôºå‰ª•Âú®ÈáçÂª∫‰πãÂâçÂ¢ûÂä†ÂèñÊ®£ÂØÜÂ∫¶„ÄÇAPS-FWI ÁµÑ‰ª∂Âà©Áî® InversionNetÔºåÁõ¥Êé•ÂæûË∂ÖÈü≥Ê≥¢Ê≥¢ÂΩ¢Ë≥áÊñôÈáçÂª∫Èü≥ÈÄü (SOS)„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅéÁµêÂêà Squeeze-and-Excitation (SE) ÂçÄÂ°äÂíåÂéüÂßãÁ∑®Á¢ºÊäÄË°ì‰æÜÊèêÂçáÊ®°ÂûãÊïàËÉΩ„ÄÇÂú®‰π≥ÁôåË≥áÊñôÈõÜ‰∏äÊ∏¨Ë©¶ÊàëÂÄëÁöÑÈÄôÂÄãÊñπÊ≥ïÔºåÂæóÂà∞‰∫ÜÊúâÂâçÊôØÁöÑÁµêÊûú„ÄÇÂÆÉÂ±ïÁèæ‰∫ÜÂÇëÂá∫ÁöÑÊïàËÉΩÔºåÁµêÊßãÁõ∏‰ººÊÄßÊåáÊ®ô (SSIM) Âπ≥ÂùáÁÇ∫ 0.8431„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåË∂ÖÈÅé 82% ÁöÑÊ®£Êú¨ÈÅîÊàê SSIM È´òÊñº 0.8ÔºåËøë 61% Ë∂ÖÈÅé 0.85ÔºåÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÈÄèÈÅéÊúâÊïàÂà©Áî®Á®ÄÁñèË≥áÊñô‰æÜÊîπÂñÑ USCT ÂΩ±ÂÉèÈáçÂª∫ÊñπÈù¢ÁöÑÈ°ØËëóÊΩõÂäõ„ÄÇ

##### **Addressing Imbalance for Class Incremental Learning in Medical Image Classification**
2407.13768v1 by Xuze Hao, Wenqian Ni, Xuhao Jiang, Weimin Tan, Bo Yan

Deep convolutional neural networks have made significant breakthroughs in
medical image classification, under the assumption that training samples from
all classes are simultaneously available. However, in real-world medical
scenarios, there's a common need to continuously learn about new diseases,
leading to the emerging field of class incremental learning (CIL) in the
medical domain. Typically, CIL suffers from catastrophic forgetting when
trained on new classes. This phenomenon is mainly caused by the imbalance
between old and new classes, and it becomes even more challenging with
imbalanced medical datasets. In this work, we introduce two simple yet
effective plug-in methods to mitigate the adverse effects of the imbalance.
First, we propose a CIL-balanced classification loss to mitigate the classifier
bias toward majority classes via logit adjustment. Second, we propose a
distribution margin loss that not only alleviates the inter-class overlap in
embedding space but also enforces the intra-class compactness. We evaluate the
effectiveness of our method with extensive experiments on three benchmark
datasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our
approach outperforms state-of-the-art methods.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÁ™ÅÁ†¥ÔºåÂÅáË®≠ÊâÄÊúâÈ°ûÂà•ÁöÑË®ìÁ∑¥Ê®£Êú¨ÈÉΩËÉΩÂêåÊôÇÂèñÂæó„ÄÇÁÑ∂ËÄåÔºåÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÊåÅÁ∫åÂ≠∏ÁøíÊñ∞ÁöÑÁñæÁóÖÔºåÂ∞éËá¥ÈÜ´ÁôÇÈ†òÂüü‰∏≠È°ûÂà•Â¢ûÈáèÂ≠∏Áøí (CIL) ÁöÑÊñ∞ËààÈ†òÂüü„ÄÇÈÄöÂ∏∏ÔºåCIL Âú®Ë®ìÁ∑¥Êñ∞È°ûÂà•ÊôÇÊúÉÈÅ≠ÂèóÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÈÄôÁ®ÆÁèæË±°‰∏ªË¶ÅÊòØÁî±ÊñºËàäÈ°ûÂà•ÂíåÊñ∞È°ûÂà•‰πãÈñìÁöÑ‰∏çÂπ≥Ë°°ÈÄ†ÊàêÁöÑÔºåËÄåÂú®‰∏çÂπ≥Ë°°ÁöÑÈÜ´ÁôÇË≥áÊñôÈõÜ‰∏äÔºåÈÄôÊúÉËÆäÂæóÊõ¥ÂÖ∑ÊåëÊà∞ÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂÖ©Á®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂ§ñÊéõÊñπÊ≥ï‰æÜÊ∏õËºï‰∏çÂπ≥Ë°°ÁöÑË≤†Èù¢ÂΩ±Èüø„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã CIL Âπ≥Ë°°ÂàÜÈ°ûÊêçÂ§±ÔºåÈÄèÈÅé logit Ë™øÊï¥‰æÜÊ∏õËºïÂàÜÈ°ûÂô®Â∞çÂ§öÊï∏È°ûÂà•ÁöÑÂÅèË¶ã„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂàÜ‰ΩàÈÇäÈöõÊêçÂ§±ÔºåÂÆÉ‰∏çÂÉÖÂèØ‰ª•Ê∏õËºïÂµåÂÖ•Á©∫Èñì‰∏≠ÁöÑÈ°ûÈñìÈáçÁñäÔºåÈÇÑÂèØ‰ª•Âä†Âº∑È°ûÂÖßÁ∑äÂØÜÊÄß„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÔºàCCH5000„ÄÅHAM10000 Âíå EyePACSÔºâ‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåË©ï‰º∞‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇ

##### **Shaded Route Planning Using Active Segmentation and Identification of Satellite Images**
2407.13689v1 by Longchao Da, Rohan Chhibba, Rushabh Jaiswal, Ariane Middel, Hua Wei

Heatwaves pose significant health risks, particularly due to prolonged
exposure to high summer temperatures. Vulnerable groups, especially pedestrians
and cyclists on sun-exposed sidewalks, motivate the development of a route
planning method that incorporates somatosensory temperature effects through
shade ratio consideration. This paper is the first to introduce a pipeline that
utilizes segmentation foundation models to extract shaded areas from
high-resolution satellite images. These areas are then integrated into a
multi-layered road map, enabling users to customize routes based on a balance
between distance and shade exposure, thereby enhancing comfort and health
during outdoor activities. Specifically, we construct a graph-based
representation of the road map, where links indicate connectivity and are
updated with shade ratio data for dynamic route planning. This system is
already implemented online, with a video demonstration, and will be
specifically adapted to assist travelers during the 2024 Olympic Games in
Paris.

ÊëòË¶ÅÔºöÁÜ±Êµ™ÈÄ†ÊàêÈ°ØËëóÁöÑÂÅ•Â∫∑È¢®Èö™ÔºåÁâπÂà•ÊòØÈï∑ÊôÇÈñìÊö¥Èú≤Âú®Â§èÂ≠£ÁöÑÈ´òÊ∫´‰∏ã„ÄÇÂÆπÊòìÂèóÂÇ∑ÂÆ≥ÁöÑÊóèÁæ§ÔºåÂ∞§ÂÖ∂ÊòØË°åËµ∞Âú®ÈôΩÂÖâÁõ¥Â∞Ñ‰∫∫Ë°åÈÅì‰∏äÁöÑË°å‰∫∫ÂíåËá™Ë°åËªäÈ®éÂ£´Ôºå‰øÉÊàê‰∫ÜË¶èÂäÉË∑ØÁ∑öÊñπÊ≥ïÁöÑÁôºÂ±ïÔºåÂÖ∂‰∏≠Á¥çÂÖ•‰∫ÜÈÄèÈÅéÈÅÆÈôΩÁéáËÄÉÈáè‰æÜÁî¢ÁîüÁöÑÈ´îÊÑüÊ∫´Â∫¶ÂΩ±Èüø„ÄÇÊú¨ÊñáÈ¶ñÊ¨°‰ªãÁ¥π‰∏ÄÂÄãÂà©Áî®ÂàÜÂâ≤Âü∫Á§éÊ®°ÂûãÂæûÈ´òËß£ÊûêÂ∫¶Ë°õÊòüÂΩ±ÂÉè‰∏≠Êì∑ÂèñÈô∞ÂΩ±ÂçÄÂüüÁöÑÁÆ°Á∑ö„ÄÇÈÄô‰∫õÂçÄÂüüÊé•ËëóÊï¥ÂêàÂà∞Â§öÂ±§ÈÅìË∑ØÂú∞Âúñ‰∏≠Ôºå‰ΩøÁî®Êà∂ËÉΩÂ§†Ê†πÊìöË∑ùÈõ¢ÂíåÈÅÆÈôΩÊõùÊõ¨‰πãÈñìÁöÑÂπ≥Ë°°‰æÜËá™Ë®ÇË∑ØÁ∑öÔºåÈÄ≤ËÄåÊèêÂçáÊà∂Â§ñÊ¥ªÂãïÊôÇÁöÑËàíÈÅ©Â∫¶ÂíåÂÅ•Â∫∑„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄã‰ª•ÂúñÂΩ¢ÁÇ∫Âü∫Á§éÁöÑÈÅìË∑ØÂú∞ÂúñË°®ÂæµÔºåÂÖ∂‰∏≠ÈÄ£ÁµêË°®Á§∫ÈÄ£ÈÄöÊÄßÔºå‰∏¶ÈÄèÈÅéÈÅÆÈôΩÁéáË≥áÊñôÊõ¥Êñ∞‰ª•ÈÄ≤Ë°åÂãïÊÖãË∑ØÁ∑öË¶èÂäÉ„ÄÇÊ≠§Á≥ªÁµ±Â∑≤Á∑ö‰∏äÂØ¶‰ΩúÔºå‰∏¶ÈôÑÊúâÂΩ±ÁâáÁ§∫ÁØÑÔºå‰∏îÂ∞áÁâπÂà•Ë™øÊï¥‰ª•ÂçîÂä©ÊóÖÂÆ¢ÂèÉÂä† 2024 Âπ¥Â∑¥ÈªéÂ•ßÈÅã„ÄÇ

##### **A Comparative Study on Automatic Coding of Medical Letters with Explainability**
2407.13638v1 by Jamie Glen, Lifeng Han, Paul Rayson, Goran Nenadic

This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êó®Âú®Êé¢Ë®éÂ∞áËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊäÄË°ìÂØ¶‰ΩúÊñºÈÜ´ÁôÇ‰ø°ÂáΩÁ∑®Á¢ºËá™ÂãïÂåñÔºå‰∏¶ÂÖ∑ÂÇôË¶ñË¶∫ÂåñË™™ÊòéËÉΩÂäõÂíåËºïÈáèÂåñÁöÑÊú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö„ÄÇÁõÆÂâçÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÁ∑®Á¢ºÊòØ‰∏ÄÁ®ÆÊâãÂãïÊµÅÁ®ãÔºåÊ∂âÂèäÁÇ∫ÁóÖÊÇ£Êñá‰ª∂‰∏≠ÁöÑÊØèÈ†ÖÁóÖÁóá„ÄÅÁ®ãÂ∫èÂíåËó•Áâ©ÊåáÊ¥æ‰ª£Á¢º (‰æãÂ¶ÇÔºå‰ΩøÁî® SNOMED CT ‰ª£Á¢º 56265001 Ë°®Á§∫ÂøÉËáüÁóÖ)„ÄÇÊ≠§È†òÂüüÊúâ‰ΩøÁî®ÊúÄÊñ∞ ML Ê®°ÂûãÈÄ≤Ë°åËá™ÂãïÁ∑®Á¢ºÁöÑÂàùÊ≠•Á†îÁ©∂ÔºõÁÑ∂ËÄåÔºåÁî±ÊñºÊ®°ÂûãÁöÑË§áÈõúÊÄßÂíåÂ§ßÂ∞èÔºå‰∏¶Êú™ÂØ¶ÁèæÂØ¶ÈöõÈÉ®ÁΩ≤„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•‰øÉÈÄ≤Ëá™ÂãïÁ∑®Á¢ºÂØ¶ÂãôÁöÑÂèØËÉΩÊÄßÔºåÊàëÂÄëÂú®Êú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö‰∏≠Êé¢Ë®é‰∫Ü‰∏Ä‰∫õËß£Ê±∫ÊñπÊ°àÔºõÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜË™™ÊòéÂäüËÉΩÂú® AI Ê®°ÂûãÈÄèÊòéÂ∫¶‰∏≠ÁöÑÂäüËÉΩ„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ¨ÈñãÁöÑ MIMIC-III Ë≥áÊñôÂ∫´Âíå HAN/HLAN Á∂≤Ë∑ØÊ®°ÂûãÈÄ≤Ë°å ICD ‰ª£Á¢ºÈ†êÊ∏¨„ÄÇÊàëÂÄëÈÇÑË©¶È©ó‰∫Ü ICD Âíå SNOMED CT Áü•Ë≠òÂ∫´‰πãÈñìÁöÑÂ∞çÊáâ„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÈÄô‰∫õÊ®°ÂûãÊèê‰æõ‰∫Ü 97.98% ‰ª£Á¢ºÁöÑÊúâÁî®Ë≥áË®ä„ÄÇÈÄôÈ†ÖË™øÊü•ÁµêÊûúÂèØ‰ª•ÁÇ∫ÂØ¶Âãô‰∏≠ÁöÑËá™ÂãïËá®Â∫äÁ∑®Á¢ºÂØ¶‰ΩúÊèê‰æõ‰∏Ä‰∫õË¶ãËß£Ôºå‰æãÂ¶ÇÂú®ÈÜ´Èô¢Áí∞Â¢É‰∏≠ÔºåÁî±Ëá®Â∫äÈÜ´Áîü‰ΩøÁî®ÁöÑÊú¨Âú∞ÈõªËÖ¶ÔºåÂ∞àÊ°àÈ†ÅÈù¢ \url{https://github.com/Glenj01/Medical-Coding}„ÄÇ

##### **A review of handcrafted and deep radiomics in neurological diseases: transitioning from oncology to clinical neuroimaging**
2407.13813v1 by Elizaveta Lavrova, Henry C. Woodruff, Hamza Khan, Eric Salmon, Philippe Lambin, Christophe Phillips

Medical imaging technologies have undergone extensive development, enabling
non-invasive visualization of clinical information. The traditional review of
medical images by clinicians remains subjective, time-consuming, and prone to
human error. With the recent availability of medical imaging data,
quantification have become important goals in the field. Radiomics, a
methodology aimed at extracting quantitative information from imaging data, has
emerged as a promising approach to uncover hidden biological information and
support decision-making in clinical practice. This paper presents a review of
the radiomic pipeline from the clinical neuroimaging perspective, providing a
detailed overview of each step with practical advice. It discusses the
application of handcrafted and deep radiomics in neuroimaging, stratified by
neurological diagnosis. Although radiomics shows great potential for increasing
diagnostic precision and improving treatment quality in neurology, several
limitations hinder its clinical implementation. Addressing these challenges
requires collaborative efforts, advancements in image harmonization methods,
and the establishment of reproducible and standardized pipelines with
transparent reporting. By overcoming these obstacles, radiomics can
significantly impact clinical neurology and enhance patient care.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÊäÄË°ìÂ∑≤Ê≠∑Á∂ìÂª£Ê≥õÁôºÂ±ïÔºåËÉΩ‰ª•Èùû‰æµÂÖ•ÊÄßÊñπÂºèË¶ñË¶∫ÂåñËá®Â∫äË≥áË®ä„ÄÇËá®Â∫äÈÜ´Â∏´ÂÇ≥Áµ±‰∏äÂ∞çÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÊ™¢Ë¶ñ‰ªç‰∏ªËßÄ„ÄÅËÄóÊôÇÔºå‰∏îÂÆπÊòìÁôºÁîü‰∫∫ÁÇ∫ÈåØË™§„ÄÇÈö®ËëóÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôËøëÊúüËÆäÂæóÂÆπÊòìÂèñÂæóÔºåÈáèÂåñÂ∑≤ÊàêÁÇ∫Ë©≤È†òÂüüÁöÑÈáçË¶ÅÁõÆÊ®ô„ÄÇÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏ÊòØ‰∏ÄÁ®ÆÊó®Âú®ÂæûÂΩ±ÂÉèË≥áÊñô‰∏≠ËêÉÂèñÈáèÂåñË≥áË®äÁöÑÊñπÊ≥ïÔºåÂ∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÊúõÊè≠Èú≤Èö±ËóèÁîüÁâ©Ë≥áË®ä‰∏¶ÊîØÊè¥Ëá®Â∫äÂØ¶ÂãôÊ±∫Á≠ñÂà∂ÂÆöÁöÑÊñπÊ≥ï„ÄÇÊú¨ÊñáÂõûÈ°ß‰∫ÜÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏ÁÆ°Á∑öÂú®Ëá®Â∫äÁ•ûÁ∂ìÂΩ±ÂÉèÁöÑËßÄÈªûÔºå‰∏¶Êèê‰æõÂêÑÊ≠•È©üÁöÑË©≥Á¥∞Ê¶ÇËßÄÂíåÂØ¶Áî®Âª∫Ë≠∞„ÄÇÊú¨ÊñáË®éË´ñ‰∫Ü‰∫∫Â∑•Ë£Ω‰ΩúÂíåÊ∑±Â∫¶ÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Âú®Á•ûÁ∂ìÂΩ±ÂÉè‰∏≠ÁöÑÊáâÁî®Ôºå‰∏¶‰æùÁ•ûÁ∂ìË®∫Êñ∑ÂàÜÂ±§„ÄÇÂÑòÁÆ°ÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Âú®ÊèêÈ´òÁ•ûÁ∂ìÁßëË®∫Êñ∑Á≤æÊ∫ñÂ∫¶ÂíåÊîπÂñÑÊ≤ªÁôÇÂìÅË≥™ÊñπÈù¢È°ØÁ§∫Âá∫Ê•µÂ§ßÁöÑÊΩõÂäõÔºå‰ΩÜ‰ªçÊúâËã•Âπ≤ÈôêÂà∂ÈòªÁ§ôÂÖ∂Ëá®Â∫äÊáâÁî®„ÄÇË¶ÅËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÈúÄË¶ÅÂêà‰ΩúÂä™Âäõ„ÄÅÂΩ±ÂÉèË™øÂíåÊñπÊ≥ïÁöÑÈÄ≤Â±ïÔºå‰ª•ÂèäÂª∫Á´ãÂÖ∑ÊúâÈÄèÊòéÂ†±ÂëäÁöÑÂèØË§áË£Ω‰∏îÊ®ôÊ∫ñÂåñÁöÑÁÆ°Á∑ö„ÄÇÈÄèÈÅéÂÖãÊúçÈÄô‰∫õÈöúÁ§ôÔºåÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Â∞áËÉΩÈ°ØËëóÂΩ±ÈüøËá®Â∫äÁ•ûÁ∂ìÁßë‰∏¶ÊèêÂçáÁóÖÊÇ£ÁÖßË≠∑„ÄÇ

##### **End-To-End Clinical Trial Matching with Large Language Models**
2407.13463v1 by Dyke Ferber, Lars Hilgers, Isabella C. Wiest, Marie-Elisabeth Le√ümann, Jan Clusmann, Peter Neidlinger, Jiefu Zhu, Georg W√∂lflein, Jacqueline Lammert, Maximilian Tschochohei, Heiko B√∂hme, Dirk J√§ger, Mihaela Aldea, Daniel Truhn, Christiane H√∂per, Jakob Nikolas Kather

Matching cancer patients to clinical trials is essential for advancing
treatment and patient care. However, the inconsistent format of medical free
text documents and complex trial eligibility criteria make this process
extremely challenging and time-consuming for physicians. We investigated
whether the entire trial matching process - from identifying relevant trials
among 105,600 oncology-related clinical trials on clinicaltrials.gov to
generating criterion-level eligibility matches - could be automated using Large
Language Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic
Health Records (EHRs), we demonstrate that our approach identifies relevant
candidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%
when matching patient-level information at the criterion level against a
baseline defined by human experts. Utilizing LLM feedback reveals that 39.3%
criteria that were initially considered incorrect are either ambiguous or
inaccurately annotated, leading to a total model accuracy of 92.7% after
refining our human baseline. In summary, we present an end-to-end pipeline for
clinical trial matching using LLMs, demonstrating high precision in screening
and matching trials to individual patients, even outperforming the performance
of qualified medical doctors. Our fully end-to-end pipeline can operate
autonomously or with human supervision and is not restricted to oncology,
offering a scalable solution for enhancing patient-trial matching in real-world
settings.

ÊëòË¶ÅÔºöÈÖçÂ∞çÁôåÁóáÊÇ£ËÄÖËàáËá®Â∫äË©¶È©óÂ∞çÊñºÊé®ÈÄ≤Ê≤ªÁôÇÂíåÊÇ£ËÄÖÁÖßË≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÜ´ÁôÇËá™Áî±ÊñáÊú¨Êñá‰ª∂Ê†ºÂºè‰∏ç‰∏ÄËá¥‰ª•ÂèäË§áÈõúÁöÑË©¶È©óË≥áÊ†ºÊ®ôÊ∫ñÔºå‰ΩøÂæóÈÄôÂÄãÈÅéÁ®ãÂ∞çÈÜ´Â∏´‰æÜË™™Ê•µÂÖ∑ÊåëÊà∞ÊÄß‰∏îËÄóÊôÇ„ÄÇÊàëÂÄëË™øÊü•‰∫ÜÊï¥ÂÄãË©¶È©óÈÖçÂ∞çÈÅéÁ®ã‚Äî‚ÄîÂæûÂú® clinicaltrials.gov ‰∏ä 105,600 ÂÄãËàáËÖ´Áò§Â≠∏Áõ∏ÈóúÁöÑËá®Â∫äË©¶È©ó‰∏≠ÊâæÂá∫Áõ∏ÈóúË©¶È©óÔºåÂà∞Áî¢ÁîüÊ®ôÊ∫ñÂ±§Á¥öË≥áÊ†ºÈÖçÂ∞ç‚Äî‚ÄîÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëá™ÂãïÂåñ„ÄÇÊàëÂÄë‰ΩøÁî® GPT-4o Âíå‰∏ÄÂ•ó 51 ÂÄãÂêàÊàêÁöÑÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR)ÔºåË≠âÊòéÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® 93.3% ÁöÑÊ°à‰æã‰∏≠ÊâæÂá∫Áõ∏ÈóúÂÄôÈÅ∏Ë©¶È©óÔºå‰∏¶‰∏îÂú®ÈáùÂ∞ç‰∫∫È°ûÂ∞àÂÆ∂ÂÆöÁæ©ÁöÑÂü∫Ê∫ñÔºåÊØîÂ∞çÊ®ôÊ∫ñÂ±§Á¥öÁöÑÊÇ£ËÄÖÂ±§Á¥öË≥áË®äÊôÇÔºåÈÅîÂà∞ 88.0% ÁöÑÂàùÊ≠•Ê∫ñÁ¢∫Â∫¶„ÄÇÂà©Áî® LLM ÂõûÈ•ãÈ°ØÁ§∫ÔºåÊúÄÂàùË¢´Ë™çÁÇ∫‰∏çÊ≠£Á¢∫ÁöÑ 39.3% Ê®ôÊ∫ñÔºå‰∏çÊòØÊ®°Á®úÂÖ©ÂèØÂ∞±ÊòØË®ªËß£‰∏çÊ∫ñÁ¢∫ÔºåÂú®ÊàëÂÄëÊîπÂñÑ‰∫∫È°ûÂü∫Ê∫ñÂæåÔºåÂ∞éËá¥Ê®°ÂûãÁ∏ΩÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 92.7%„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ΩøÁî® LLM ÁöÑËá®Â∫äË©¶È©óÈÖçÂ∞çÁ´ØÂà∞Á´ØÁÆ°Á∑öÔºåË≠âÊòéÂú®ÁØ©ÈÅ∏ÂíåÊØîÂ∞çË©¶È©óÂà∞ÂÄãÂà•ÊÇ£ËÄÖÊôÇÂÖ∑ÊúâÈ´òÁ≤æÊ∫ñÂ∫¶ÔºåÁîöËá≥ÂÑ™ÊñºÂêàÊ†ºÈÜ´ÁîüÁöÑË°®Áèæ„ÄÇÊàëÂÄëÂÆåÂÖ®ÁöÑÁ´ØÂà∞Á´ØÁÆ°Á∑öÂèØ‰ª•Ëá™‰∏ªÈÅã‰ΩúÊàñÂú®‰∫∫È°ûÁõ£Áù£‰∏ãÈÅã‰ΩúÔºå‰∏î‰∏çÈôêÊñºËÖ´Áò§Â≠∏ÔºåÊèê‰æõ‰∏ÄÂÄãÂèØÊì¥ÂÖÖÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÊèêÂçáÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÊÇ£ËÄÖË©¶È©óÈÖçÂ∞ç„ÄÇ

##### **CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis**
2407.13301v1 by Junying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong Wang, Xiang Wan, Benyou Wang

The field of medical diagnosis has undergone a significant transformation
with the advent of large language models (LLMs), yet the challenges of
interpretability within these models remain largely unaddressed. This study
introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of
LLM-based medical diagnostics. CoD transforms the diagnostic process into a
diagnostic chain that mirrors a physician's thought process, providing a
transparent reasoning pathway. Additionally, CoD outputs the disease confidence
distribution to ensure transparency in decision-making. This interpretability
makes model diagnostics controllable and aids in identifying critical symptoms
for inquiry through the entropy reduction of confidences. With CoD, we
developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental
results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic
benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring
controllability in diagnostic rigor.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÈÜ´ÁôÇË®∫Êñ∑È†òÂüüÁ∂ìÊ≠∑‰∫Ü‰∏ÄÂ†¥ÈáçÂ§ßËΩâÂûãÔºå‰ΩÜÈÄô‰∫õÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊåëÊà∞Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞Ëß£Ê±∫„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫ÜË®∫Êñ∑Èèà (CoD) ‰æÜÂ¢ûÂº∑Âü∫Êñº LLM ÁöÑÈÜ´ÁôÇË®∫Êñ∑ÁöÑÂèØËß£ÈáãÊÄß„ÄÇCoD Â∞áË®∫Êñ∑ÈÅéÁ®ãËΩâÊèõÁÇ∫‰∏ÄÂÄãË®∫Êñ∑ÈèàÔºåÂèçÊò†‰∫ÜÈÜ´ÁîüÁöÑÊÄùËÄÉÈÅéÁ®ãÔºåÊèê‰æõ‰∫Ü‰∏ÄÊ¢ùÈÄèÊòéÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊ≠§Â§ñÔºåCoD Ëº∏Âá∫‰∫ÜÁñæÁóÖÁΩÆ‰ø°Â∫¶ÂàÜ‰ΩàÔºå‰ª•Á¢∫‰øùÊ±∫Á≠ñÈÄèÊòéÂ∫¶„ÄÇÈÄôÁ®ÆÂèØËß£ÈáãÊÄß‰ΩøÊ®°ÂûãË®∫Êñ∑ÂèØÊéßÔºå‰∏¶ÊúâÂä©ÊñºÈÄöÈÅéÁΩÆ‰ø°Â∫¶ÁöÑÁÜµÊ∏õ‰æÜË≠òÂà•ÈúÄË¶ÅË©¢ÂïèÁöÑÈóúÈçµÁóáÁãÄ„ÄÇ‰ΩøÁî® CoDÔºåÊàëÂÄëÈñãÁôº‰∫Ü DiagnosisGPTÔºåÂÆÉËÉΩÂ§†Ë®∫Êñ∑ 9604 Á®ÆÁñæÁóÖ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåDiagnosisGPT Âú®Ë®∫Êñ∑Âü∫Ê∫ñ‰∏äÂÑ™ÊñºÂÖ∂‰ªñ LLM„ÄÇÊ≠§Â§ñÔºåDiagnosisGPT Âú®Á¢∫‰øùË®∫Êñ∑Âö¥Ë¨πÊÄßÂèØÊéßÊÄßÁöÑÂêåÊôÇÊèê‰æõ‰∫ÜÂèØËß£ÈáãÊÄß„ÄÇ

##### **NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations**
2407.13241v1 by Hao Bai, Yi Hong

Regression on medical image sequences can capture temporal image pattern
changes and predict images at missing or future time points. However, existing
geodesic regression methods limit their regression performance by a strong
underlying assumption of linear dynamics, while diffusion-based methods have
high computational costs and lack constraints to preserve image topology. In
this paper, we propose an optimization-based new framework called NODER, which
leverages neural ordinary differential equations to capture complex underlying
dynamics and reduces its high computational cost of handling high-dimensional
image volumes by introducing the latent space. We compare our NODER with two
recent regression methods, and the experimental results on ADNI and ACDC
datasets demonstrate that our method achieves the state-of-the-art performance
in 3D image regression. Our model needs only a couple of images in a sequence
for prediction, which is practical, especially for clinical situations where
extremely limited image time series are available for analysis. Our source code
is available at https://github.com/ZedKing12138/NODER-pytorch.

ÊëòË¶ÅÔºöÂõûÊ≠∏ÈÜ´ÁôÇÂΩ±ÂÉèÂ∫èÂàóÂèØ‰ª•ÊçïÊçâÊôÇÈñìÂΩ±ÂÉèÊ®°ÂºèËÆäÂåñÔºå‰∏¶È†êÊ∏¨ÈÅ∫Â§±ÊàñÊú™‰æÜÊôÇÈñìÈªûÁöÑÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊ∏¨Âú∞Á∑öËø¥Ê≠∏ÊñπÊ≥ïÈôêÂà∂ÂÖ∂Ëø¥Ê≠∏ÊïàËÉΩÔºåÂõ†ÁÇ∫ÂÖ∂Âº∑ÁÉà‰æùË≥¥Á∑öÊÄßÂãïÊÖãÁöÑÂü∫Êú¨ÂÅáË®≠ÔºåËÄåÂü∫ÊñºÊì¥Êï£ÁöÑÊñπÊ≥ïÂâáÂÖ∑ÊúâÂæàÈ´òÁöÑÈÅãÁÆóÊàêÊú¨ÔºåËÄå‰∏îÁº∫‰πè‰øùÁïôÂΩ±ÂÉèÊãìÊí≤ÁöÑÁ¥ÑÊùü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ®±ÁÇ∫ NODER ÁöÑÂü∫ÊñºÊúÄ‰Ω≥ÂåñÁöÑÂÖ®Êñ∞Êû∂ÊßãÔºåÂÆÉÂà©Áî®Á•ûÁ∂ìÂ∏∏ÂæÆÂàÜÊñπÁ®ãÂºè‰æÜÊçïÊçâË§áÈõúÁöÑÂ∫ïÂ±§ÂãïÊÖãÔºå‰∏¶ÈÄèÈÅéÂºïÂÖ•ÊΩõÂú®Á©∫Èñì‰æÜÈôç‰ΩéËôïÁêÜÈ´òÁ∂≠Â∫¶ÂΩ±ÂÉèÈ´îÁ©çÁöÑÈ´òÈÅãÁÆóÊàêÊú¨„ÄÇÊàëÂÄëÂ∞á NODER ËàáÂÖ©Á®ÆÊúÄËøëÁöÑËø¥Ê≠∏ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºåÂú® ADNI Âíå ACDC Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® 3D ÂΩ±ÂÉèËø¥Ê≠∏‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂè™ÈúÄË¶ÅÂ∫èÂàó‰∏≠ÂπæÂÄãÂΩ±ÂÉèÂç≥ÂèØÈÄ≤Ë°åÈ†êÊ∏¨ÔºåÈÄôÂæàÂØ¶Áî®ÔºåÁâπÂà•ÊòØÂú®Ëá®Â∫äÊÉÖÊ≥Å‰∏ãÔºåÊ•µÂÖ∂ÊúâÈôêÁöÑÂΩ±ÂÉèÊôÇÈñìÂ∫èÂàóÂèØ‰æõÂàÜÊûê„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/ZedKing12138/NODER-pytorch ÂèñÂæó„ÄÇ

##### **Enhancing the Utility of Privacy-Preserving Cancer Classification using Synthetic Data**
2407.12669v1 by Richard Osuala, Daniel M. Lang, Anneliese Riess, Georgios Kaissis, Zuzanna Szafranowska, Grzegorz Skorupko, Oliver Diaz, Julia A. Schnabel, Karim Lekadir

Deep learning holds immense promise for aiding radiologists in breast cancer
detection. However, achieving optimal model performance is hampered by
limitations in availability and sharing of data commonly associated to patient
privacy concerns. Such concerns are further exacerbated, as traditional deep
learning models can inadvertently leak sensitive training information. This
work addresses these challenges exploring and quantifying the utility of
privacy-preserving deep learning techniques, concretely, (i) differentially
private stochastic gradient descent (DP-SGD) and (ii) fully synthetic training
data generated by our proposed malignancy-conditioned generative adversarial
network. We assess these methods via downstream malignancy classification of
mammography masses using a transformer model. Our experimental results depict
that synthetic data augmentation can improve privacy-utility tradeoffs in
differentially private model training. Further, model pretraining on synthetic
data achieves remarkable performance, which can be further increased with
DP-SGD fine-tuning across all privacy guarantees. With this first in-depth
exploration of privacy-preserving deep learning in breast imaging, we address
current and emerging clinical privacy requirements and pave the way towards the
adoption of private high-utility deep diagnostic models. Our reproducible
codebase is publicly available at https://github.com/RichardObi/mammo_dp.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÂú®ÂçîÂä©ÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°å‰π≥ÁôåÂÅµÊ∏¨ÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºËàáÁóÖÊÇ£Èö±ÁßÅÁõ∏ÈóúÁöÑÁñëÊÖÆÔºåË≥áÊñôÁöÑÂèñÂæóËàáÂàÜ‰∫´ÂèóÂà∞ÈôêÂà∂ÔºåÈÄôÈòªÁ§ô‰∫ÜÊ®°ÂûãÈÅîÂà∞ÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇÁî±ÊñºÂÇ≥Áµ±ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂèØËÉΩÊúÉÁÑ°ÊÑèÈñìÊ¥©ÊºèÊïèÊÑüÁöÑË®ìÁ∑¥Ë≥áË®äÔºåÈÄô‰∫õÁñëÊÖÆÈÄ≤‰∏ÄÊ≠•Âä†Âäá„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êé¢Ë®é‰∏¶ÈáèÂåñÈö±ÁßÅ‰øùË≠∑Ê∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÁöÑÊïàÁî®ÔºåÂÖ∑È´î‰æÜË™™Ôºå(i) Â∑ÆÂàÜÈö±ÁßÅÈö®Ê©üÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï (DP-SGD) Âíå (ii) Áî±ÊàëÂÄëÊèêÂá∫ÁöÑÊÉ°ÊÄßËÖ´Áò§Ê¢ù‰ª∂ÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑ØÊâÄÁî¢ÁîüÁöÑÂÆåÂÖ®ÂêàÊàêË®ìÁ∑¥Ë≥áÊñôÔºå‰ª•Ëß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî®ËΩâÊèõÂô®Ê®°ÂûãÂ∞ç‰π≥ÊàøÊîùÂΩ±ËÖ´Â°äÈÄ≤Ë°å‰∏ãÊ∏∏ÊÉ°ÊÄßËÖ´Áò§ÂàÜÈ°û‰æÜË©ï‰º∞ÈÄô‰∫õÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂêàÊàêË≥áÊñôÊì¥ÂÖÖÂèØ‰ª•ÊîπÂñÑÂ∑ÆÂàÜÈö±ÁßÅÊ®°ÂûãË®ìÁ∑¥‰∏≠ÁöÑÈö±ÁßÅÊïàÁî®Ê¨äË°°„ÄÇÊ≠§Â§ñÔºåÂú®ÂêàÊàêË≥áÊñô‰∏äÈÄ≤Ë°åÊ®°ÂûãÈ†êË®ìÁ∑¥ÂèØÁç≤ÂæóÈ°ØËëóÁöÑÊïàËÉΩÔºå‰∏¶ÂèØÈÄèÈÅéÂú®ÊâÄÊúâÈö±ÁßÅ‰øùË≠â‰∏ãÈÄ≤Ë°å DP-SGD ÂæÆË™øÈÄ≤‰∏ÄÊ≠•ÊèêÂçá„ÄÇÈÄèÈÅéÈ¶ñÊ¨°Ê∑±ÂÖ•Êé¢Ë®é‰π≥ÊàøÂΩ±ÂÉè‰∏≠ÁöÑÈö±ÁßÅ‰øùË≠∑Ê∑±Â∫¶Â≠∏ÁøíÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÁï∂ÂâçÂíåÊñ∞ËààÁöÑËá®Â∫äÈö±ÁßÅÈúÄÊ±ÇÔºå‰∏¶ÁÇ∫Êé°Áî®ÁßÅÊúâÈ´òÂØ¶Áî®ÊÄßÊ∑±Â∫¶Ë®∫Êñ∑Ê®°ÂûãÈã™Ë∑Ø„ÄÇÊàëÂÄëÁöÑÂèØË§áË£ΩÁ®ãÂºèÁ¢ºÂ∫´Â∑≤ÂÖ¨ÈñãÊñº https://github.com/RichardObi/mammo_dp„ÄÇ

##### **Abstraction Alignment: Comparing Model and Human Conceptual Relationships**
2407.12543v1 by Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan

Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like "bolo tie" and "car tire" to more general concepts
like "CEO" and "model". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.

ÊëòË¶ÅÔºöÊäΩË±°Âåñ‚Äî‚ÄîÂ∞áÁâπÂÆöÁØÑ‰æãÊ¶ÇÊã¨ÁÇ∫Âª£Ê≥õÂèØÈáçË§á‰ΩøÁî®ÁöÑÊ®°ÂºèÁöÑÈÅéÁ®ã‚Äî‚ÄîÊòØ‰∫∫ÂÄëÊúâÊïàËôïÁêÜÂíåÂÑ≤Â≠òË≥áË®äÔºå‰∏¶Â∞áÂÖ∂Áü•Ë≠òÊáâÁî®ÊñºÊñ∞Ë≥áÊñôÁöÑÊ†∏ÂøÉ„ÄÇÊúâÂ∏åÊúõÁöÑÊòØÔºåÁ†îÁ©∂È°ØÁ§∫ ML Ê®°ÂûãÂ≠∏ÁøíË∑®Ë∂äÊäΩË±°Â±§Á¥öÁöÑË°®ÂæµÔºåÂæû„ÄåÁ¥∞È†òÂ∏∂„ÄçÂíå„ÄåÊ±ΩËªäËº™ËÉé„ÄçÁ≠âÂÖ∑È´îÊ¶ÇÂøµÂà∞„ÄåÂü∑Ë°åÈï∑„ÄçÂíå„ÄåÊ®°Âûã„ÄçÁ≠âÊõ¥‰∏ÄËà¨ÁöÑÊ¶ÇÂøµ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊäÄË°ìÂ≠§Á´ãÂú∞ÂàÜÊûêÈÄô‰∫õË°®ÂæµÔºåÂ∞áÂ≠∏ÁøíÂà∞ÁöÑÊ¶ÇÂøµË¶ñÁÇ∫Áç®Á´ãÁöÑÁî¢Áâ©ÔºåËÄå‰∏çÊòØÊäΩË±°ÁöÑÁõ∏‰∫íÈÄ£ÁµêÁ∂≤Ë∑Ø„ÄÇÂõ†Ê≠§ÔºåÂÑòÁÆ°ÊàëÂÄëÂèØ‰ª•Ë≠òÂà•Ê®°ÂûãÁî®‰æÜÁî¢ÁîüÂÖ∂Ëº∏Âá∫ÁöÑÊ¶ÇÂøµÔºå‰ΩÜÂæàÈõ£Ë©ï‰º∞ÂÆÉÊòØÂê¶Â≠∏ÁøíÂà∞Ê¶ÇÂøµÁöÑ‰∫∫È°ûÂ∞çÈΩäÊäΩË±°ÔºåÈÄô‰∫õÊ¶ÇÂøµÂ∞áÊ¶ÇÊã¨Âà∞Êñ∞ÁöÑË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊäΩË±°Â∞çÈΩäÔºå‰∏ÄÁ®ÆË°°ÈáèÊ®°ÂûãÂ≠∏ÁøíÁöÑÊäΩË±°ËàáÈ†êÊúüÁöÑÊäΩË±°‰πãÈñì‰∏ÄËá¥ÊÄßÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊ®°ÂûãËº∏Âá∫Ëàá‰∫∫È°ûÊäΩË±°ÂúñÂΩ¢Ôºà‰æãÂ¶ÇË™ûË®ÄÈóú‰øÇÊàñÈÜ´ÁôÇÁñæÁóÖÂ±§Á¥öÁµêÊßãÔºâÈÄ≤Ë°åÊØîËºÉ‰æÜÈáèÂåñÊäΩË±°Â∞çÈΩä„ÄÇÂú®Ëß£ÈáãÂΩ±ÂÉèÊ®°Âûã„ÄÅÂü∫Ê∫ñË™ûË®ÄÊ®°ÂûãÂíåÂàÜÊûêÈÜ´ÁôÇË≥áÊñôÈõÜÁöÑË©ï‰º∞‰ªªÂãô‰∏≠ÔºåÊäΩË±°Â∞çÈΩäÊèê‰æõ‰∫ÜÂ∞çÊ®°ÂûãË°åÁÇ∫ÂíåË≥áÊñôÈõÜÂÖßÂÆπÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£ÔºåÊ†πÊìöËàá‰∫∫È°ûÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÂçÄÂàÜÈåØË™§ÔºåÊì¥Â±ïÁï∂ÂâçÊ®°ÂûãÂìÅË≥™ÊåáÊ®ôÁöÑË©≥Á¥∞Á®ãÂ∫¶Ôºå‰∏¶Êè≠Á§∫ÊîπÂñÑÁèæÊúâ‰∫∫È°ûÊäΩË±°ÁöÑÊñπÊ≥ï„ÄÇ

##### **Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning**
2407.12894v1 by Lisandro A. Jimenez-Roa, Thiago D. Sim√£o, Zaharah Bukhsh, Tiedo Tinga, Hajo Molegraaf, Nils Jansen, Marielle Stoelinga

Large-scale infrastructure systems are crucial for societal welfare, and
their effective management requires strategic forecasting and intervention
methods that account for various complexities. Our study addresses two
challenges within the Prognostics and Health Management (PHM) framework applied
to sewer assets: modeling pipe degradation across severity levels and
developing effective maintenance policies. We employ Multi-State Degradation
Models (MSDM) to represent the stochastic degradation process in sewer pipes
and use Deep Reinforcement Learning (DRL) to devise maintenance strategies. A
case study of a Dutch sewer network exemplifies our methodology. Our findings
demonstrate the model's effectiveness in generating intelligent, cost-saving
maintenance strategies that surpass heuristics. It adapts its management
strategy based on the pipe's age, opting for a passive approach for newer pipes
and transitioning to active strategies for older ones to prevent failures and
reduce costs. This research highlights DRL's potential in optimizing
maintenance policies. Future research will aim improve the model by
incorporating partial observability, exploring various reinforcement learning
algorithms, and extending this methodology to comprehensive infrastructure
management.

ÊëòË¶ÅÔºöÂ§ßÂûãÂü∫Á§éË®≠ÊñΩÁ≥ªÁµ±Â∞çÁ§æÊúÉÁ¶èÂà©Ëá≥ÈóúÈáçË¶ÅÔºåÊúâÊïàÁÆ°ÁêÜÈÄô‰∫õÁ≥ªÁµ±ÈúÄË¶ÅÁ≠ñÁï•ÊÄßÈ†êÊ∏¨ÂíåÂπ≤È†êÊñπÊ≥ïÔºå‰∏¶ËÄÉÈáèÂêÑÁ®ÆË§áÈõúÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈáùÂ∞çÊáâÁî®Êñº‰∏ãÊ∞¥ÈÅìË≥áÁî¢ÁöÑÈ†êÊ∏¨ÂíåÂÅ•Â∫∑ÁÆ°ÁêÜ (PHM) Ê°ÜÊû∂‰∏≠ÁöÑÂÖ©ÂÄãÊåëÊà∞ÔºöÂ∞ç‰∏çÂêåÂö¥ÈáçÁ®ãÂ∫¶ÁöÑÁÆ°ÈÅìÂä£ÂåñÈÄ≤Ë°åÂª∫Ê®°Ôºå‰∏¶Âà∂ÂÆöÊúâÊïàÁöÑÁ∂≠Ë≠∑ÊîøÁ≠ñ„ÄÇÊàëÂÄëÊé°Áî®Â§öÁãÄÊÖãÂä£ÂåñÊ®°Âûã (MSDM) ‰æÜË°®Á§∫‰∏ãÊ∞¥ÈÅìÁÆ°ÈÅì‰∏≠ÁöÑÈö®Ê©üÂä£ÂåñÈÅéÁ®ãÔºå‰∏¶‰ΩøÁî®Ê∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (DRL) ‰æÜË®≠Ë®àÁ∂≠Ë≠∑Á≠ñÁï•„ÄÇËç∑Ëò≠‰∏ãÊ∞¥ÈÅìÁ∂≤Ë∑ØÁöÑÊ°à‰æãÁ†îÁ©∂Ë™™Êòé‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË≠âÊòé‰∫ÜË©≤Ê®°ÂûãÂú®Áî¢ÁîüË∂ÖË∂äÂïüÁôºÊ≥ïÁöÑÊô∫ÊÖßÂûãÁØÄÁúÅÊàêÊú¨Á∂≠Ë≠∑Á≠ñÁï•ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÂÆÉÊ†πÊìöÁÆ°ÈÅìÁöÑÂπ¥ÈΩ°Ë™øÊï¥ÂÖ∂ÁÆ°ÁêÜÁ≠ñÁï•ÔºåÈÅ∏ÊìáËºÉÊñ∞ÁöÑÁÆ°ÈÅìÊé°Áî®Ë¢´ÂãïÊñπÂºèÔºå‰∏¶ËΩâËÆäÁÇ∫ËºÉËàäÁÆ°ÈÅìÁöÑÁ©çÊ•µÁ≠ñÁï•Ôºå‰ª•Èò≤Ê≠¢ÊïÖÈöú‰∏¶Èôç‰ΩéÊàêÊú¨„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™ø‰∫Ü DRL Âú®ÊúÄ‰Ω≥ÂåñÁ∂≠Ë≠∑ÊîøÁ≠ñÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Â∞áÊó®Âú®ÈÄèÈÅéÁ¥çÂÖ•ÈÉ®ÂàÜÂèØËßÄÂØüÊÄß„ÄÅÊé¢Á¥¢ÂêÑÁ®ÆÂº∑ÂåñÂ≠∏ÁøíÊºîÁÆóÊ≥ïÔºå‰∏¶Â∞áÊ≠§ÊñπÊ≥ïÊì¥Â±ïÂà∞ÂÖ®Èù¢ÁöÑÂü∫Á§éË®≠ÊñΩÁÆ°ÁêÜÔºå‰æÜÊîπÂñÑÊ®°Âûã„ÄÇ

##### **Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions**
2407.12468v2 by Marcos Fern√°ndez-Pichel, Juan C. Pichel, David E. Losada

Search engines have traditionally served as primary tools for information
seeking. However, the new Large Language Models (LLMs) have recently
demonstrated remarkable capabilities in multiple tasks and, specifically, their
adoption as question answering systems is becoming increasingly prevalent. It
is expected that LLM-based conversational systems and traditional web engines
will continue to coexist in the future, supporting end users in various ways.
But there is a need for more scientific research on the effectiveness of both
types of systems in facilitating accurate information seeking. In this study,
we focus on their merits in answering health questions. We conducted an
extensive study comparing different web search engines, LLMs and
retrieval-augmented (RAG) approaches. Our research reveals intriguing
conclusions. For example, we observed that the quality of webpages potentially
responding to a health question does not decline as we navigate further down
the ranked lists. However, according to our evaluation, web engines are less
accurate than LLMs in finding correct answers to health questions. On the other
hand, LLMs are quite sensitive to the input prompts, and we also found out that
RAG leads to highly effective information seeking methods.

ÊëòË¶ÅÔºöÊêúÂ∞ãÂºïÊìéÂÇ≥Áµ±‰∏ä‰∏ÄÁõ¥‰ΩúÁÇ∫Â∞ãÊâæË≥áË®äÁöÑ‰∏ªË¶ÅÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÊñ∞ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÂ∑≤Âú®Â§öÈ†Ö‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂÖ∂Ë¢´Êé°Áî®ÁÇ∫ÂïèÈ°åËß£Á≠îÁ≥ªÁµ±Ê≠£ËÆäÂæóË∂ä‰æÜË∂äÊôÆÈÅç„ÄÇÈ†êË®àÊú™‰æÜÂü∫Êñº LLM ÁöÑÂ∞çË©±Á≥ªÁµ±ÂíåÂÇ≥Áµ±Á∂≤Ë∑ØÂºïÊìéÂ∞áÊåÅÁ∫åÂÖ±Â≠òÔºå‰ª•ÂêÑÁ®ÆÊñπÂºèÊîØÊè¥ÊúÄÁµÇ‰ΩøÁî®ËÄÖ„ÄÇ‰ΩÜÈúÄË¶ÅÊõ¥Â§öÁßëÂ≠∏Á†îÁ©∂‰æÜÊé¢Ë®éÈÄôÂÖ©Á®ÆÁ≥ªÁµ±Âú®‰øÉÈÄ≤Ê∫ñÁ¢∫Ë≥áË®äÊêúÂ∞ãÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂÆÉÂÄëÂú®ÂõûÁ≠îÂÅ•Â∫∑ÂïèÈ°åÊñπÈù¢ÁöÑÂÑ™Èªû„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂª£Ê≥õÁöÑÁ†îÁ©∂ÔºåÊØîËºÉ‰∫Ü‰∏çÂêåÁöÑÁ∂≤Ë∑ØÊêúÂ∞ãÂºïÊìé„ÄÅLLM ÂíåÊ™¢Á¥¢Â¢ûÂº∑ (RAG) ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫ÜÊúâË∂£ÁöÑÁµêË´ñ„ÄÇ‰æãÂ¶ÇÔºåÊàëÂÄëËßÄÂØüÂà∞ÔºåÈö®ËëóÊàëÂÄëÂú®ÊéíÂêçÊ∏ÖÂñÆ‰∏≠Âêë‰∏ãÁÄèË¶ΩÔºåÂèØËÉΩÊúÉÂõûÁ≠îÂÅ•Â∫∑ÂïèÈ°åÁöÑÁ∂≤È†ÅÂìÅË≥™‰∏¶‰∏çÊúÉ‰∏ãÈôç„ÄÇÁÑ∂ËÄåÔºåÊ†πÊìöÊàëÂÄëÁöÑË©ï‰º∞ÔºåÁ∂≤Ë∑ØÂºïÊìéÂú®Â∞ãÊâæÂÅ•Â∫∑ÂïèÈ°åÁöÑÊ≠£Á¢∫Á≠îÊ°àÊñπÈù¢‰∏çÂ¶Ç LLM Ê∫ñÁ¢∫„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåLLM Â∞çËº∏ÂÖ•ÊèêÁ§∫ÈùûÂ∏∏ÊïèÊÑüÔºåÊàëÂÄëÈÇÑÁôºÁèæ RAG Â∞éËá¥È´òÂ∫¶ÊúâÊïàÁöÑË≥áË®äÊêúÂ∞ãÊñπÊ≥ï„ÄÇ

##### **Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**
2407.12888v1 by Alexander R. Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping

The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.

ÊëòË¶ÅÔºö<paragraph>Áèæ‰ªäÂ§ßÈáèÁöÑÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÂ∞çË©¶ÂúñÊúâÊïàÊ∂àÂåñ„ÄÅËôïÁêÜÂíåÁêÜËß£ÈÄô‰∫õÁôºÁèæÁöÑÁ†îÁ©∂‰∫∫Âì°ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫Âú®ÈÄôÂÄãË§áÈõú‰∏îÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÁí∞Â¢É‰∏≠Â∞éËà™ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåLLM ÂèØËÉΩÊúÉÂ∞éËá¥ÂπªË¶∫ÂèçÊáâÔºåÈÄô‰ΩøÂæóÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) Â∞çÊñºÁç≤ÂæóÊ∫ñÁ¢∫Ë≥áË®äËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÂÄãÂçîÂÆö‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RUGGEDÔºàÂúñÂΩ¢Â∞éÂºïÂèØËß£ÈáãÁñæÁóÖÂçÄÂàÜÁöÑÊ™¢Á¥¢ÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊè¥Á†îÁ©∂‰∫∫Âì°ÈÄ≤Ë°åÁü•Ë≠òÊï¥ÂêàÂíåÂÅáË®≠Áî¢ÁîüÔºåÊâæÂá∫Á∂ìÈÅéÈ©óË≠âÁöÑÈÄ≤Â±ïË∑ØÂæë„ÄÇ‰æÜËá™Âá∫ÁâàÁâ©ÂíåÁü•Ë≠òÂ∫´ÁöÑÁõ∏ÈóúÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÊúÉÈÄèÈÅéÊñáÊú¨Êé¢ÂãòÈóúËÅØÂàÜÊûêÂíåÁñæÁóÖÁØÄÈªûÁöÑÂèØËß£ÈáãÂúñÂΩ¢È†êÊ∏¨Ê®°ÂûãÈÄ≤Ë°åÊ™¢Èñ±„ÄÅÊï¥ÂêàÂíåËêÉÂèñÔºåÈ†êÊ∏¨Ëó•Áâ©ÂíåÁñæÁóÖ‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØ„ÄÇÈÄô‰∫õÂàÜÊûêÈÄ£ÂêåÁîüÁâ©ÈÜ´Â≠∏ÊñáÊú¨ÊúÉÊï¥ÂêàÂà∞‰∏ÄÂÄãÊû∂Êßã‰∏≠ÔºåË©≤Êû∂Êßã‰øÉÈÄ≤‰ΩøÁî®ËÄÖÂ∞éÂêëÁöÑÊ©üÂà∂Èó°ÊòéÔºå‰ª•ÂèäÈÄèÈÅé RAG ÂïüÁî®ÁöÑ LLM ÈÄ≤Ë°åÂÅáË®≠Êé¢Ë®é„ÄÇ‰∏ÄÂÄãËá®Â∫ä‰ΩøÁî®Ê°à‰æãÂ±ïÁ§∫‰∫Ü RUGGED Ë©ï‰º∞ÂíåÊé®Ëñ¶Áî®ÊñºÂøÉÂæãÂ§±Â∏∏ÊÄßÂøÉËÇåÁóÖËÆä (ACM) ÂíåÊì¥ÂºµÂûãÂøÉËÇåÁóÖËÆä (DCM) ÁöÑÊ≤ªÁôÇÊñπÊ≥ïÁöÑËÉΩÂäõÔºåÂàÜÊûêËôïÊñπËó•Áâ©ÁöÑÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®ÂíåÊú™Êé¢Á¥¢ÁöÑÁî®ÈÄî„ÄÇÈÄôÂÄãÂπ≥Âè∞Â∞á LLM ÂπªË¶∫ÈôçÂà∞ÊúÄ‰ΩéÔºåÊèê‰æõÂèØÊìç‰ΩúÁöÑË¶ãËß£Ôºå‰∏¶ÊîπÂñÑÊñ∞Ê≤ªÁôÇÊñπÊ≥ïÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **Evaluating graph-based explanations for AI-based recommender systems**
2407.12357v1 by Simon Delarue, Astrid Bertrand, Tiphaine Viard

Recent years have witnessed a rapid growth of recommender systems, providing
suggestions in numerous applications with potentially high social impact, such
as health or justice. Meanwhile, in Europe, the upcoming AI Act mentions
\emph{transparency} as a requirement for critical AI systems in order to
``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly
align with this goal and extensive literature on the subject produced several
forms of such objects, graphs being one of them. Early studies in visualization
demonstrated the graphs' ability to improve user understanding, positioning
them as potentially ideal explanations. However, it remains unclear how
graph-based explanations compare to other explanation designs. In this work, we
aim to determine the effectiveness of graph-based explanations in improving
users' perception of AI-based recommendations using a mixed-methods approach.
We first conduct a qualitative study to collect users' requirements for graph
explanations. We then run a larger quantitative study in which we evaluate the
influence of various explanation designs, including enhanced graph-based ones,
on aspects such as understanding, usability and curiosity toward the AI system.
We find that users perceive graph-based explanations as more usable than
designs involving feature importance. However, we also reveal that textual
explanations lead to higher objective understanding than graph-based designs.
Most importantly, we highlight the strong contrast between participants'
expressed preferences for graph design and their actual ratings using it, which
are lower compared to textual design. These findings imply that meeting
stakeholders' expressed preferences might not alone guarantee ``good''
explanations. Therefore, crafting hybrid designs successfully balancing social
expectations with downstream performance emerges as a significant challenge.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÊé®Ëñ¶Á≥ªÁµ±Âø´ÈÄüÊàêÈï∑ÔºåÊèê‰æõÂêÑÁ®ÆÊáâÁî®Á®ãÂºèÂª∫Ë≠∞ÔºåÂÖ∑ÊúâÊΩõÂú®ÁöÑÈ´òÁ§æÊúÉÂΩ±ÈüøÂäõÔºå‰æãÂ¶ÇÂÅ•Â∫∑ÊàñÂè∏Ê≥ï„ÄÇÂêåÊôÇÔºåÂú®Ê≠êÊ¥≤ÔºåÂç≥Â∞áÂá∫Âè∞ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊ≥ïÊ°àÊèêÂà∞ÔºåÈóúÈçµ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÈúÄË¶Å„ÄåÈÄèÊòéÂ∫¶„ÄçÔºåÊâçËÉΩ„ÄåÈôç‰ΩéÂ∞çÂü∫Êú¨Ê¨äÂà©ÁöÑÈ¢®Èö™„Äç„ÄÇ‰∫ãÂæåËß£ÈáãËàáÊ≠§ÁõÆÊ®ôÁÑ°Á∏´Â∞çÈΩäÔºå‰∏îË©≤‰∏ªÈ°åÁöÑÂª£Ê≥õÊñáÁçªÁî¢Áîü‰∫ÜÂ§öÁ®ÆÊ≠§È°ûÁâ©‰ª∂ÔºåÂÖ∂‰∏≠‰∏ÄÁ®ÆÂ∞±ÊòØÂúñÂΩ¢„ÄÇË¶ñË¶∫ÂåñÁöÑÊó©ÊúüÁ†îÁ©∂Ë≠âÊòé‰∫ÜÂúñÂΩ¢ËÉΩÂ§†ÊèêÂçá‰ΩøÁî®ËÄÖÁêÜËß£ÂäõÔºåÂ∞áÂÖ∂ÂÆö‰ΩçÁÇ∫ÊΩõÂú®ÁöÑÁêÜÊÉ≥Ëß£Èáã„ÄÇÁÑ∂ËÄåÔºåÂü∫ÊñºÂúñÂΩ¢ÁöÑËß£ÈáãËàáÂÖ∂‰ªñËß£ÈáãË®≠Ë®àÁõ∏ÊØîÂ¶Ç‰ΩïÔºåÁõÆÂâç‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®‰ΩøÁî®Ê∑∑ÂêàÊñπÊ≥ïÔºå‰æÜÁ¢∫ÂÆöÂü∫ÊñºÂúñÂΩ¢ÁöÑËß£ÈáãÂú®ÊèêÂçá‰ΩøÁî®ËÄÖÂ∞çÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÂª∫Ë≠∞ÁöÑÊÑüÁü•‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÈ¶ñÂÖàÈÄ≤Ë°å‰∏ÄÈ†ÖÂÆöÊÄßÁ†îÁ©∂Ôºå‰ª•Êî∂ÈõÜ‰ΩøÁî®ËÄÖÂ∞çÂúñÂΩ¢Ëß£ÈáãÁöÑÈúÄÊ±Ç„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂü∑Ë°å‰∏ÄÈ†ÖË¶èÊ®°Êõ¥Â§ßÁöÑÂÆöÈáèÁ†îÁ©∂ÔºåË©ï‰º∞ÂêÑÁ®ÆËß£ÈáãË®≠Ë®àÁöÑÂΩ±ÈüøÔºåÂåÖÊã¨Â¢ûÂº∑ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑË®≠Ë®àÔºåÂ∞ç‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÁöÑÁêÜËß£„ÄÅÂèØÁî®ÊÄßÂíåÂ•ΩÂ•áÂøÉÁ≠âÈù¢Âêë„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ΩøÁî®ËÄÖË™çÁÇ∫Âü∫ÊñºÂúñÂΩ¢ÁöÑËß£ÈáãÊØîÊ∂âÂèäÁâπÂæµÈáçË¶ÅÊÄßÁöÑË®≠Ë®àÊõ¥ÂèØÁî®„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰πüÁôºÁèæÔºåÊñáÂ≠óËß£ÈáãÊØîÂü∫ÊñºÂúñÂΩ¢ÁöÑË®≠Ë®àÂ∏∂‰æÜÊõ¥È´òÁöÑÂÆ¢ËßÄÁêÜËß£„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÂèÉËàáËÄÖÂ∞çÂúñÂΩ¢Ë®≠Ë®àË°®ÈÅîÁöÑÂÅèÂ•ΩËàáÂØ¶Èöõ‰ΩøÁî®ÂúñÂΩ¢Ë®≠Ë®àÊôÇÁöÑË©ïÂàÜ‰πãÈñìÁöÑÂº∑ÁÉàÂ∞çÊØîÔºåËàáÊñáÂ≠óË®≠Ë®àÁõ∏ÊØîÔºå‰ΩøÁî®ÂúñÂΩ¢Ë®≠Ë®àÁöÑË©ïÂàÜËºÉ‰Ωé„ÄÇÈÄô‰∫õÁôºÁèæÊöóÁ§∫ÔºåÊªøË∂≥Âà©ÂÆ≥Èóú‰øÇ‰∫∫Ë°®ÈÅîÁöÑÂÅèÂ•ΩÔºåÂèØËÉΩÁÑ°Ê≥ïÂñÆÁç®‰øùË≠â„ÄåËâØÂ•Ω„ÄçÁöÑËß£Èáã„ÄÇÂõ†Ê≠§ÔºåÂ∑ßÂ¶ôË®≠Ë®àÊ∑∑ÂêàË®≠Ë®àÔºåÊàêÂäüÂπ≥Ë°°Á§æÊúÉÊúüÊúõËàá‰∏ãÊ∏∏ÊïàËÉΩÔºåÊàêÁÇ∫‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇ</paragraph>

##### **GPT-4V Cannot Generate Radiology Reports Yet**
2407.12176v1 by Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan

GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.

ÊëòË¶ÅÔºöGPT-4V ÊâÄË¨ÇÂº∑Â§ßÁöÑÂ§öÊ®°ÊÖãËÉΩÂäõÂºïËµ∑‰∫Ü‰∫∫ÂÄëÂ∞ç‰ΩøÁî®ÂÆÉ‰æÜËá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁ∑®ÂØ´ÁöÑËààË∂£Ôºå‰ΩÜÂçªÁº∫‰πèÂæπÂ∫ïÁöÑË©ï‰º∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞ç GPT-4V Âú®ÂÖ©ÂÄãËÉ∏ÈÉ® X ÂÖâÂ†±ÂëäÊï∏ÊìöÈõÜÔºöMIMIC-CXR Âíå IU X-Ray ‰∏äÁîüÊàêÊîæÂ∞ÑÂ†±ÂëäÈÄ≤Ë°å‰∫ÜÁ≥ªÁµ±Ë©ï‰º∞„ÄÇÊàëÂÄëÂòóË©¶ÈÄöÈÅé‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•Áõ¥Êé•‰ΩøÁî® GPT-4V ÁîüÊàêÂ†±ÂëäÔºå‰∏¶ÁôºÁèæÂÆÉÂú®Ë©ûÂΩôÊåáÊ®ôÂíåËá®Â∫äÁôÇÊïàÊåáÊ®ô‰∏äÈÉΩË°®ÁèæÂæóÂæàÁ≥üÁ≥ï„ÄÇÁÇ∫‰∫Ü‰∫ÜËß£‰ΩéÊÄßËÉΩÔºåÊàëÂÄëÂ∞á‰ªªÂãôÂàÜËß£ÁÇ∫ÂÖ©ÂÄãÊ≠•È©üÔºö1) ÂæûÂúñÂÉèÈ†êÊ∏¨ÈÜ´ÁôÇÁãÄÊ≥ÅÊ®ôÁ±§ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÊé®ÁêÜÊ≠•È©üÔºõ‰ª•Âèä 2) ÂæûÔºàÁúüÂØ¶ÔºâÊ¢ù‰ª∂ÁîüÊàêÂ†±ÂëäÁöÑÂ†±ÂëäÂêàÊàêÊ≠•È©ü„ÄÇÊàëÂÄëË°®ÊòéÔºåGPT-4V Âú®ÂúñÂÉèÊé®ÁêÜ‰∏≠ÁöÑË°®ÁèæÂßãÁµÇ‰ΩéÊñº‰∏çÂêåÁöÑÊèêÁ§∫„ÄÇ‰∫ãÂØ¶‰∏äÔºåÊ®°ÂûãÈ†êÊ∏¨Ê®ôÁ±§ÁöÑÂàÜÂ∏É‰øùÊåÅ‰∏çËÆäÔºåÁÑ°Ë´ñÂúñÂÉè‰∏äÂ≠òÂú®Âì™‰∫õÁúüÂØ¶Ê¢ù‰ª∂ÔºåÈÄôË°®ÊòéË©≤Ê®°ÂûãÊ≤íÊúâÊúâÊÑèÁæ©Âú∞Ëß£ÈáãËÉ∏ÈÉ® X ÂÖâ„ÄÇÂç≥‰ΩøÂú®Â†±ÂëäÂêàÊàê‰∏≠Áµ¶Âá∫ÁúüÂØ¶Ê¢ù‰ª∂ÔºåÂÖ∂ÁîüÊàêÁöÑÂ†±Âëä‰πü‰∏çÂ¶ÇÂæÆË™øÂæåÁöÑ LLaMA-2 Ê≠£Á¢∫ÂíåËá™ÁÑ∂„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÁöÑÁôºÁèæÂ∞çÂú®ÊîæÂ∞ÑÂ∑•‰ΩúÊµÅÁ®ã‰∏≠‰ΩøÁî® GPT-4V ÁöÑÂèØË°åÊÄßÊèêÂá∫‰∫ÜË≥™Áñë„ÄÇ


### Medical explainable AI
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-26**|**MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**|Shyam Dongre et.al.|[2407.20284v1](http://arxiv.org/abs/2407.20284v1)|null|
|**2024-07-25**|**Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**|Alessandro De Carlo et.al.|[2407.18343v2](http://arxiv.org/abs/2407.18343v2)|null|
|**2024-07-24**|**Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**|Nikolaos Ntampakis et.al.|[2407.17324v2](http://arxiv.org/abs/2407.17324v2)|null|
|**2024-07-18**|**A Comparative Study on Automatic Coding of Medical Letters with Explainability**|Jamie Glen et.al.|[2407.13638v1](http://arxiv.org/abs/2407.13638v1)|[link](https://github.com/Glenj01/Medical-Coding)|
|**2024-07-09**|**Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**|Abdul Karim Gizzini et.al.|[2407.07009v1](http://arxiv.org/abs/2407.07009v1)|null|
|**2024-07-07**|**Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**|P. N. Karthikayan et.al.|[2407.05440v1](http://arxiv.org/abs/2407.05440v1)|null|
|**2024-07-03**|**A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**|Congzhen Shi et.al.|[2407.15851v1](http://arxiv.org/abs/2407.15851v1)|null|
|**2024-07-01**|**The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**|Ximing Wen et.al.|[2407.06206v1](http://arxiv.org/abs/2407.06206v1)|null|
|**2024-06-28**|**Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**|Sai Krishna Revanth Vuruma et.al.|[2407.00167v1](http://arxiv.org/abs/2407.00167v1)|null|
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-11**|**Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**|Fatemeh Ebrahimzadeh et.al.|[2406.07114v2](http://arxiv.org/abs/2406.07114v2)|null|
|**2024-06-10**|**AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**|K M Tawsik Jawad et.al.|[2406.06728v1](http://arxiv.org/abs/2406.06728v1)|null|
|**2024-06-10**|**Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**|Yusif Ibrahimov et.al.|[2406.05984v1](http://arxiv.org/abs/2406.05984v1)|null|
|**2024-06-09**|**Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**|Zhan Zhang et.al.|[2406.05746v1](http://arxiv.org/abs/2406.05746v1)|null|
|**2024-06-07**|**Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**|Faseela Abdullakutty et.al.|[2406.12897v1](http://arxiv.org/abs/2406.12897v1)|null|
|**2024-06-07**|**Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**|Yong-Min Shin et.al.|[2406.04612v1](http://arxiv.org/abs/2406.04612v1)|[link](https://github.com/jordan7186/gatt)|
|**2024-06-04**|**Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**|Dinuka Sandun Udayantha et.al.|[2406.16908v2](http://arxiv.org/abs/2406.16908v2)|[link](https://github.com/dinuka-1999/braineocare)|
|**2024-06-01**|**Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**|Samita Bai et.al.|[2406.00532v1](http://arxiv.org/abs/2406.00532v1)|null|
|**2024-06-01**|**Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**|Alaa Nfissi et.al.|[2406.01624v2](http://arxiv.org/abs/2406.01624v2)|[link](https://github.com/alaanfissi/unveiling-hidden-factors-explainable-ai-for-feature-boosting-in-speech-emotion-recognition)|
|**2024-05-31**|**The Explanation Necessity for Healthcare AI**|Michail Mamalakis et.al.|[2406.00216v1](http://arxiv.org/abs/2406.00216v1)|null|
|**2024-05-29**|**Interdisciplinary Expertise to Advance Equitable Explainable AI**|Chloe R. Bennett et.al.|[2406.18563v1](http://arxiv.org/abs/2406.18563v1)|null|
|**2024-05-27**|**"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**|Hubert D. ZajƒÖc et.al.|[2407.11978v1](http://arxiv.org/abs/2407.11978v1)|null|
|**2024-05-26**|**Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**|Min Hun Lee et.al.|[2405.16424v1](http://arxiv.org/abs/2405.16424v1)|null|
|**2024-05-26**|**Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**|Ziming Liu et.al.|[2405.17502v1](http://arxiv.org/abs/2405.17502v1)|null|
|**2024-05-24**|**Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**|Catalina Gomez et.al.|[2407.11974v1](http://arxiv.org/abs/2407.11974v1)|null|
|**2024-05-23**|**Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**|Yingying Fang et.al.|[2406.18552v1](http://arxiv.org/abs/2406.18552v1)|null|
|**2024-05-21**|**The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**|Mohsen Jozani et.al.|[2405.13099v1](http://arxiv.org/abs/2405.13099v1)|null|
|**2024-05-17**|**ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**|Harris Bin Munawar et.al.|[2405.10645v1](http://arxiv.org/abs/2405.10645v1)|null|
|**2024-05-13**|**Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**|Camelia Oprea et.al.|[2405.07590v1](http://arxiv.org/abs/2405.07590v1)|null|
|**2024-05-10**|**XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**|Fatemeh Nazary et.al.|[2405.06270v3](http://arxiv.org/abs/2405.06270v3)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel Mir√≥-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|[link](https://github.com/zzs95/RSP_COVID)|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-19**|**Explainable AI for Fair Sepsis Mortality Predictive Model**|Chia-Hsuan Chang et.al.|[2404.13139v1](http://arxiv.org/abs/2404.13139v1)|null|
|**2024-04-19**|**Multi Class Depression Detection Through Tweets using Artificial Intelligence**|Muhammad Osama Nusrat et.al.|[2404.13104v1](http://arxiv.org/abs/2404.13104v1)|[link](https://github.com/mnusrat786/masters-thesis)|
|**2024-04-19**|**COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**|Dmytro Shvetsov et.al.|[2404.12832v2](http://arxiv.org/abs/2404.12832v2)|[link](https://github.com/dmytro-shvetsov/counterfactual-search)|
|**2024-04-15**|**Hybrid Intelligence for Digital Humanities**|Victor de Boer et.al.|[2406.15374v1](http://arxiv.org/abs/2406.15374v1)|null|
|**2024-04-14**|**Ethical Framework for Responsible Foundational Models in Medical Imaging**|Abhijit Das et.al.|[2406.11868v1](http://arxiv.org/abs/2406.11868v1)|null|
|**2024-04-09**|**Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**|Milad Yousefi et.al.|[2404.07239v1](http://arxiv.org/abs/2404.07239v1)|null|
|**2024-04-06**|**Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**|Taminul Islam et.al.|[2404.04686v1](http://arxiv.org/abs/2404.04686v1)|null|
|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|Maryam Ahmed et.al.|[2404.03892v3](http://arxiv.org/abs/2404.03892v3)|null|
|**2024-03-26**|**Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**|Andrea Ferrario et.al.|[2403.17873v1](http://arxiv.org/abs/2403.17873v1)|null|
|**2024-03-26**|**Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**|Han Yuan et.al.|[2403.18871v1](http://arxiv.org/abs/2403.18871v1)|[link](https://github.com/han-yuan-med/template-explanation)|
|**2024-03-03**|**Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**|S√©amus Lankford et.al.|[2403.01580v1](http://arxiv.org/abs/2403.01580v1)|null|
|**2024-02-28**|**Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**|Yasin Sadeghi Bazargani et.al.|[2402.18600v1](http://arxiv.org/abs/2402.18600v1)|null|
|**2024-02-22**|**Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**|A. J. Karran et.al.|[2402.15027v2](http://arxiv.org/abs/2402.15027v2)|null|
|**2024-02-12**|**Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**|Aruna Mohan et.al.|[2402.09474v2](http://arxiv.org/abs/2402.09474v2)|null|
|**2024-02-05**|**Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**|Aryan Agrawal et.al.|[2402.05127v1](http://arxiv.org/abs/2402.05127v1)|null|
|**2024-01-24**|**Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**|Timoth√©e Schmude et.al.|[2401.13324v4](http://arxiv.org/abs/2401.13324v4)|null|
|**2024-01-02**|**Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**|Vahid Ashrafimoghari et.al.|[2401.02985v1](http://arxiv.org/abs/2401.02985v1)|null|
|**2023-12-29**|**XAI for In-hospital Mortality Prediction via Multimodal ICU Data**|Xingqiao Li et.al.|[2312.17624v1](http://arxiv.org/abs/2312.17624v1)|[link](https://github.com/lixingqiao/xai-icu)|
|**2023-12-22**|**Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**|Norman Zerbe et.al.|[2401.09450v2](http://arxiv.org/abs/2401.09450v2)|null|
|**2023-12-18**|**Robust Stochastic Graph Generator for Counterfactual Explanations**|Mario Alfonso Prado-Romero et.al.|[2312.11747v2](http://arxiv.org/abs/2312.11747v2)|null|
|**2023-12-10**|**Evaluating the Utility of Model Explanations for Model Development**|Shawn Im et.al.|[2312.06032v1](http://arxiv.org/abs/2312.06032v1)|null|
|**2023-12-05**|**Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**|Manas Gaur et.al.|[2312.06798v1](http://arxiv.org/abs/2312.06798v1)|null|
|**2023-11-28**|**Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**|Jacob R. Epifano et.al.|[2311.17133v1](http://arxiv.org/abs/2311.17133v1)|null|
|**2023-11-27**|**Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**|Benjamin Keel et.al.|[2311.15719v1](http://arxiv.org/abs/2311.15719v1)|[link](https://github.com/benkeel/vae_lung_lesion_bmvc)|
|**2023-11-24**|**MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**|Nathan Blake et.al.|[2311.14471v1](http://arxiv.org/abs/2311.14471v1)|null|
|**2023-11-21**|**Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**|Robert Gorwa et.al.|[2311.12573v2](http://arxiv.org/abs/2311.12573v2)|null|
|**2023-11-20**|**Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**|Muta Tah Hira et.al.|[2311.11932v1](http://arxiv.org/abs/2311.11932v1)|null|
|**2023-11-18**|**Representing visual classification as a linear combination of words**|Shobhit Agarwal et.al.|[2311.10933v1](http://arxiv.org/abs/2311.10933v1)|[link](https://github.com/lotterlab/task_word_explainability)|
|**2023-11-03**|**Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**|Emma A. M. Stanley et.al.|[2311.02115v2](http://arxiv.org/abs/2311.02115v2)|[link](https://github.com/estanley16/simba)|
|**2023-10-29**|**Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**|Adam White et.al.|[2310.19174v1](http://arxiv.org/abs/2310.19174v1)|null|
|**2023-10-03**|**Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**|Hossein Shreim et.al.|[2310.01828v2](http://arxiv.org/abs/2310.01828v2)|[link](https://github.com/geoaigroup/geoai-ecrs2023)|
|**2023-09-26**|**Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**|Muhammad Aurangzeb Ahmad et.al.|[2311.01463v1](http://arxiv.org/abs/2311.01463v1)|null|
|**2023-09-20**|**When to Trust AI: Advances and Challenges for Certification of Neural Networks**|Marta Kwiatkowska et.al.|[2309.11196v1](http://arxiv.org/abs/2309.11196v1)|null|
|**2023-09-19**|**Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**|Juan M. Garc√≠a-G√≥mez et.al.|[2309.10424v1](http://arxiv.org/abs/2309.10424v1)|null|
|**2023-09-19**|**QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**|Thanveer Shaik et.al.|[2309.10293v3](http://arxiv.org/abs/2309.10293v3)|null|
|**2023-09-18**|**Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**|Adarsa Sivaprasad et.al.|[2309.09917v1](http://arxiv.org/abs/2309.09917v1)|null|
|**2023-08-28**|**Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**|Yanjun Gao et.al.|[2308.14321v1](http://arxiv.org/abs/2308.14321v1)|null|
|**2023-08-18**|**Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**|Yun Xin Teoh et.al.|[2308.09380v1](http://arxiv.org/abs/2308.09380v1)|null|
|**2023-08-16**|**Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**|Munib Mesinovic et.al.|[2308.08407v1](http://arxiv.org/abs/2308.08407v1)|null|
|**2023-08-11**|**FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**|Karim Lekadir et.al.|[2309.12325v3](http://arxiv.org/abs/2309.12325v3)|null|
|**2023-08-10**|**Explainable AI applications in the Medical Domain: a systematic review**|Nicoletta Prentzas et.al.|[2308.05411v1](http://arxiv.org/abs/2308.05411v1)|null|
|**2023-08-01**|**Exploring the Role of Explainability in AI-Assisted Embryo Selection**|Lucia Urcelay et.al.|[2308.02534v1](http://arxiv.org/abs/2308.02534v1)|null|
|**2023-07-26**|**A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**|Timo Speith et.al.|[2307.14246v1](http://arxiv.org/abs/2307.14246v1)|null|
|**2023-07-26**|**Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**|Barnaby Crook et.al.|[2307.14239v1](http://arxiv.org/abs/2307.14239v1)|null|
|**2023-07-26**|**Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**|Henry Fraser et.al.|[2308.02047v1](http://arxiv.org/abs/2308.02047v1)|null|
|**2023-07-21**|**eXplainable Artificial Intelligence (XAI) in aging clock models**|Alena Kalyakulina et.al.|[2307.13704v3](http://arxiv.org/abs/2307.13704v3)|null|
|**2023-07-19**|**Interpreting and Correcting Medical Image Classification with PIP-Net**|Meike Nauta et.al.|[2307.10404v2](http://arxiv.org/abs/2307.10404v2)|[link](https://github.com/m-nauta/pipnet)|
|**2023-07-15**|**Explaining and visualizing black-box models through counterfactual paths**|Bastian Pfeifer et.al.|[2307.07764v3](http://arxiv.org/abs/2307.07764v3)|[link](https://github.com/pievos101/cpath)|
|**2023-07-05**|**Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**|Toygar Tanyel et.al.|[2307.02131v5](http://arxiv.org/abs/2307.02131v5)|[link](https://github.com/toygarr/counterfactual-explanations-for-medical-research)|
|**2023-06-30**|**AI and Non AI Assessments for Dementia**|Mahboobeh Parsapoor et.al.|[2307.01210v1](http://arxiv.org/abs/2307.01210v1)|null|
|**2023-06-12**|**Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**|Ruitao Xie et.al.|[2306.07306v1](http://arxiv.org/abs/2306.07306v1)|null|
|**2023-06-09**|**HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**|Rodrigo Agerri et.al.|[2306.06029v1](http://arxiv.org/abs/2306.06029v1)|null|
|**2023-06-07**|**XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**|Eli Laird et.al.|[2306.04791v1](http://arxiv.org/abs/2306.04791v1)|null|
|**2023-06-06**|**Explainable AI using expressive Boolean formulas**|Gili Rosenberg et.al.|[2306.03976v1](http://arxiv.org/abs/2306.03976v1)|null|
|**2023-06-06**|**Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**|Yeldar Toleubay et.al.|[2306.03902v1](http://arxiv.org/abs/2306.03902v1)|null|
|**2023-06-02**|**XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**|Sujith K Mandala et.al.|[2306.01668v1](http://arxiv.org/abs/2306.01668v1)|null|
|**2023-05-26**|**A Novel real-time arrhythmia detection model using YOLOv8**|Guang Jun Nicholas Ang et.al.|[2305.16727v3](http://arxiv.org/abs/2305.16727v3)|null|
|**2023-05-22**|**Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**|Jai Vardhan et.al.|[2305.14389v2](http://arxiv.org/abs/2305.14389v2)|null|
|**2023-05-18**|**What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**|Junwei Kuang et.al.|[2305.13127v2](http://arxiv.org/abs/2305.13127v2)|null|
|**2023-05-17**|**Echoes of Biases: How Stigmatizing Language Affects AI Performance**|Yizhi Liu et.al.|[2305.10201v4](http://arxiv.org/abs/2305.10201v4)|null|
|**2023-05-05**|**Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**|Goda Klumbyte et.al.|[2305.03376v1](http://arxiv.org/abs/2305.03376v1)|null|
|**2023-04-25**|**Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**|Surjodeep Sarkar et.al.|[2304.13191v1](http://arxiv.org/abs/2304.13191v1)|null|
|**2023-04-04**|**A Brief Review of Explainable Artificial Intelligence in Healthcare**|Zahra Sadeghi et.al.|[2304.01543v1](http://arxiv.org/abs/2304.01543v1)|null|
|**2023-03-22**|**Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**|Frederik Pahde et.al.|[2303.12641v2](http://arxiv.org/abs/2303.12641v2)|[link](https://github.com/maxdreyer/reveal2revise)|
|**2023-03-11**|**Explainable AI for Time Series via Virtual Inspection Layers**|Johanna Vielhaben et.al.|[2303.06365v1](http://arxiv.org/abs/2303.06365v1)|null|
|**2023-03-08**|**Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**|Truong Thanh Hung Nguyen et.al.|[2303.04731v1](http://arxiv.org/abs/2303.04731v1)|[link](https://github.com/hungntt/xai_thyroid)|

#### Abstracts
##### **MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**
2407.20284v1 by Shyam Dongre, Ritesh Chandra, Sonali Agarwal

In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåËß£Ê±∫Ê∫ñÁ¢∫ÁñæÁóÖÈ†êÊ∏¨ÂíåÂÄãÊÄßÂåñÂª∫Ë≠∞ÁöÑË§áÈõúÊÄßÊó¢Ëá≥ÈóúÈáçË¶ÅÂèàÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü MLtoGAIÔºåÂÆÉÂ∞áË™ûÁæ©Á∂≤Ë∑ØÊäÄË°ìËàáÊ©üÂô®Â≠∏Áøí (ML) Áõ∏ÁµêÂêàÔºå‰ª•Â¢ûÂº∑ÁñæÁóÖÈ†êÊ∏¨‰∏¶ÈÄèÈÅé ChatGPT Êèê‰æõ‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑË™™Êòé„ÄÇË©≤Á≥ªÁµ±ÂåÖÂê´‰∏âÂÄãÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÔºö‰∏ÄÂÄãÂèØÈáçË§á‰ΩøÁî®ÁöÑÁñæÁóÖÊú¨‰ΩìÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊúâÈóúÂêÑÁ®ÆÁñæÁóÖÁöÑË©≥Á¥∞Áü•Ë≠òÔºõ‰∏ÄÂÄãË®∫Êñ∑ÂàÜÈ°ûÊ®°ÂûãÔºåÂÆÉ‰ΩøÁî®ÊÇ£ËÄÖÁóáÁãÄ‰æÜÊ∫ñÁ¢∫Ê™¢Ê∏¨ÁâπÂÆöÁñæÁóÖÔºõ‰ª•ÂèäË™ûÁæ©Á∂≤Ë∑ØË¶èÂâáË™ûË®Ä (SWRL) ËàáÊú¨‰ΩìÂíå ChatGPT ÁöÑÊï¥ÂêàÔºå‰ª•Áî¢ÁîüÊ∏ÖÊô∞„ÄÅÂÄãÊÄßÂåñÁöÑÂÅ•Â∫∑Âª∫Ë≠∞„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈ°ØËëóÊèêÈ´ò‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÊòìÊñºÁêÜËß£ÁöÑÁµêÊûúÔºåËß£Ê±∫‰∫ÜÁñæÁóÖÂíå‰∏çÂêåÁóáÁãÄÁöÑË§áÈõúÊÄß„ÄÇMLtoGAI Á≥ªÁµ±Â±ïÁ§∫‰∫ÜÊ∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶ÁöÑÂØ¶Ë≥™ÊÄßÈÄ≤Ê≠•ÔºåÊúâÂä©ÊñºÈñãÁôºÊõ¥Êô∫ÊÖß‰∏îÊõ¥ÊòìÊñºÂèñÂæóÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°à„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÁµêÂêà‰∫Ü ML ÊºîÁÆóÊ≥ïÁöÑÂÑ™ÈªûÔºå‰ª•ÂèäÈÄèÈÅé ChatGPT Êèê‰æõÈÄèÊòé‰∏î‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË™™ÊòéÁöÑËÉΩÂäõÔºåÂú®È†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÂíå‰ΩøÁî®ËÄÖÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈÄèÈÅéÂà©Áî®Ë™ûÁæ©ÊäÄË°ìÂíåÂèØËß£ÈáãÁöÑ AIÔºåË©≤Á≥ªÁµ±ÊèêÈ´ò‰∫ÜÁñæÁóÖÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øù‰∫ÜÂª∫Ë≠∞ËàáÂÄãÂà•ÊÇ£ËÄÖÁõ∏Èóú‰∏îÊòìÊñºÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÊï¥ÂêàÂÖàÈÄ≤ÊäÄË°ì‰ª•ÂÖãÊúçÈÜ´ÁôÇË®∫Êñ∑‰∏≠ÁèæÊúâÊåëÊà∞ÁöÑÊΩõÂäõÔºåÁÇ∫Êô∫ÊÖßÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊú™‰æÜÁôºÂ±ïÈã™Ë∑Ø„ÄÇÊ≠§Â§ñÔºåË©≤Á≥ªÁµ±‰ΩøÁî® 200 ÂÄãÂêàÊàêÊÇ£ËÄÖË≥áÊñôË®òÈåÑÈÄ≤Ë°åÈ©óË≠âÔºåÁ¢∫‰øù‰∫ÜÁ©©ÂÅ•ÁöÑÊïàËÉΩÂíåÂèØÈù†ÊÄß„ÄÇ

##### **Introducing Œ¥-XAI: a novel sensitivity-based method for local AI explanations**
2407.18343v2 by Alessandro De Carlo, Enea Parimbelli, Nicola Melillo, Giovanna Nicora

Explainable Artificial Intelligence (XAI) is central to the debate on
integrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms
into clinical practice. High-performing AI/ML models, such as ensemble learners
and deep neural networks, often lack interpretability, hampering clinicians'
trust in their predictions. To address this, XAI techniques are being developed
to describe AI/ML predictions in human-understandable terms. One promising
direction is the adaptation of sensitivity analysis (SA) and global sensitivity
analysis (GSA), which inherently rank model inputs by their impact on
predictions. Here, we introduce a novel delta-XAI method that provides local
explanations of ML model predictions by extending the delta index, a GSA
metric. The delta-XAI index assesses the impact of each feature's value on the
predicted output for individual instances in both regression and classification
problems. We formalize the delta-XAI index and provide code for its
implementation. The delta-XAI method was evaluated on simulated scenarios using
linear regression models, with Shapley values serving as a benchmark. Results
showed that the delta-XAI index is generally consistent with Shapley values,
with notable discrepancies in models with highly impactful or extreme feature
values. The delta-XAI index demonstrated higher sensitivity in detecting
dominant features and handling extreme feature values. Qualitatively, the
delta-XAI provides intuitive explanations by leveraging probability density
functions, making feature rankings clearer and more explainable for
practitioners. Overall, the delta-XAI method appears promising for robustly
obtaining local explanations of ML model predictions. Further investigations in
real-world clinical settings will be conducted to evaluate its impact on
AI-assisted clinical workflows.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÂ∞á‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊºîÁÆóÊ≥ïÊï¥ÂêàÂà∞Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑËæØË´ñÊ†∏ÂøÉ„ÄÇÈ´òÂü∑Ë°åÊïàËÉΩÁöÑ AI/ML Ê®°ÂûãÔºå‰æãÂ¶ÇÊï¥È´îÂ≠∏ÁøíÂô®ÂíåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáãÊÄßÔºåÈòªÁ§ôËá®Â∫äÈÜ´ÁîüÂ∞çÂÖ∂È†êÊ∏¨ÁöÑ‰ø°‰ªª„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊ≠£Âú®ÈñãÁôº XAI ÊäÄË°ìÔºå‰ª•‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑË°ìË™ûÊèèËø∞ AI/ML È†êÊ∏¨„ÄÇ‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊñπÂêëÊòØÊé°Áî®ÊïèÊÑüÂ∫¶ÂàÜÊûê (SA) ÂíåÂÖ®ÁêÉÊïèÊÑüÂ∫¶ÂàÜÊûê (GSA)ÔºåÂÆÉÂÄëÊú¨Ë≥™‰∏äÊúÉ‰æùÊìöÊ®°ÂûãËº∏ÂÖ•Â∞çÈ†êÊ∏¨ÁöÑÂΩ±Èüø‰æÜÂ∞çÂÖ∂ÈÄ≤Ë°åÊéíÂêç„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞ÁöÑ delta-XAI ÊñπÊ≥ïÔºåÈÄèÈÅéÊì¥ÂÖÖ GSA ÊåáÊ®ô delta ÊåáÊï∏‰æÜÊèê‰æõ ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã„ÄÇdelta-XAI ÊåáÊï∏Ë©ï‰º∞ÊØèÂÄãÁâπÂæµÂÄºÂ∞çÂõûÊ≠∏ÂíåÂàÜÈ°ûÂïèÈ°å‰∏≠ÂÄãÂà•‰æãÈ†ÖÁöÑÈ†êÊ∏¨Ëº∏Âá∫‰πãÂΩ±Èüø„ÄÇÊàëÂÄëÂ∞á delta-XAI ÊåáÊï∏ÂΩ¢ÂºèÂåñÔºå‰∏¶Êèê‰æõÂÖ∂ÂØ¶‰ΩúÁöÑÁ®ãÂºèÁ¢º„ÄÇ‰ΩøÁî®Á∑öÊÄßÂõûÊ≠∏Ê®°ÂûãÂ∞çÊ®°Êì¨ÊÉÖÂ¢ÉË©ï‰º∞ delta-XAI ÊñπÊ≥ïÔºå‰∏¶‰ª• Shapley ÂÄº‰ΩúÁÇ∫Âü∫Ê∫ñ„ÄÇÁµêÊûúÈ°ØÁ§∫ delta-XAI ÊåáÊï∏ÈÄöÂ∏∏Ëàá Shapley ÂÄº‰∏ÄËá¥Ôºå‰ΩÜÂú®ÂÖ∑ÊúâÈ´òÂ∫¶ÂΩ±ÈüøÂäõÊàñÊ•µÁ´ØÁâπÂæµÂÄºÁöÑÊ®°Âûã‰∏≠Â≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇdelta-XAI ÊåáÊï∏Âú®ÂÅµÊ∏¨‰∏ªË¶ÅÁâπÂæµÂíåËôïÁêÜÊ•µÁ´ØÁâπÂæµÂÄºÊñπÈù¢Ë°®ÁèæÂá∫Êõ¥È´òÁöÑÊïèÊÑüÂ∫¶„ÄÇÂÆöÊÄßÂú∞‰æÜË™™Ôºådelta-XAI ÈÄèÈÅéÂà©Áî®Ê©üÁéáÂØÜÂ∫¶ÂáΩÊï∏Êèê‰æõÁõ¥ËßÄÁöÑËß£ÈáãÔºå‰ΩøÁâπÂæµÊéíÂêçÊõ¥Ê∏ÖÊô∞‰∏îÂ∞çÂæûÊ•≠‰∫∫Âì°‰æÜË™™Êõ¥ÂÖ∑ÂèØËß£ÈáãÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºådelta-XAI ÊñπÊ≥ïÂ∞çÊñºÁ©©ÂÅ•Âú∞ÂèñÂæó ML Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂ±ÄÈÉ®Ëß£Èáã‰ºº‰πéÂæàÊúâÂ∏åÊúõ„ÄÇÂ∞áÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•Ë™øÊü•Ôºå‰ª•Ë©ï‰º∞ÂÖ∂Â∞ç AI ËºîÂä©Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂΩ±Èüø„ÄÇ

##### **Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**
2407.17324v2 by Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis

Dementia, a debilitating neurological condition affecting millions worldwide,
presents significant diagnostic challenges. In this work, we introduce a novel
methodology for the classification of demented and non-demented elderly
patients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach
features a unique technique for selectively processing MRI slices, focusing on
the most relevant brain regions and excluding less informative sections. This
methodology is complemented by a confidence-based classification committee
composed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and
Dem3D EfficientNet. These models work synergistically to enhance
decision-making accuracy, leveraging their collective strengths. Tested on the
Open Access Series of Imaging Studies(OASIS) dataset, our method achieved an
impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,
validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
confirmed the robustness and generalizability of our approach. The use of
explainable AI (XAI) techniques and comprehensive ablation studies further
substantiate the effectiveness of our techniques, providing insights into the
decision-making process and the importance of our methodology. This research
offers a significant advancement in dementia diagnosis, providing a highly
accurate and efficient tool for clinical applications.

ÊëòË¶ÅÔºöÂ§±Êô∫ÁóáÊòØ‰∏ÄÁ®ÆÂΩ±ÈüøÂÖ®ÁêÉÊï∏ÁôæËê¨‰∫∫ÁöÑË°∞Âº±ÊÄßÁ•ûÁ∂ìÁñæÁóÖÔºåÂú®Ë®∫Êñ∑‰∏äÂÖ∑ÊúâÈáçÂ§ßÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂ§±Êô∫ÂíåÈùûÂ§±Êô∫ËÄÅÂπ¥ÊÇ£ËÄÖÈÄ≤Ë°åÂàÜÈ°ûÔºå‰ΩøÁî® 3D Â§ßËÖ¶Á£ÅÊåØÈÄ†ÂΩ± (MRI) ÊéÉÊèè„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé°Áî®‰∫Ü‰∏ÄÁ®ÆÁç®ÁâπÊäÄË°ìÔºåÁî®ÊñºÈÅ∏ÊìáÊÄßËôïÁêÜ MRI ÂàáÁâáÔºåÈáçÈªûÈóúÊ≥®ÊúÄÁõ∏ÈóúÁöÑÂ§ßËÖ¶ÂçÄÂüüÔºå‰∏¶ÊéíÈô§‰ø°ÊÅØÈáèËºÉÂ∞ëÁöÑÈÉ®ÂàÜ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁî±‰∏ÄÂÄãÂü∫Êñº‰ø°ÂøÉÁöÑÂàÜÈ°ûÂßîÂì°ÊúÉË£úÂÖÖÔºåË©≤ÂßîÂì°ÊúÉÁî±‰∏âÂÄãËá™ÂÆöÁæ©Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁµÑÊàêÔºöDem3D ResNet„ÄÅDem3D CNN Âíå Dem3D EfficientNet„ÄÇÈÄô‰∫õÊ®°ÂûãÂçîÂêåÂ∑•‰Ωú‰ª•Â¢ûÂº∑Ê±∫Á≠ñÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂà©Áî®ÂÆÉÂÄëÁöÑÈõÜÈ´îÂÑ™Âã¢„ÄÇÂú®ÂΩ±ÂÉèÁ†îÁ©∂ÈñãÊîæÂ≠òÂèñÁ≥ªÂàó (OASIS) Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 94.12% ÁöÑÈ©ö‰∫∫Ê∫ñÁ¢∫Â∫¶ÔºåË∂ÖÈÅé‰∫ÜÁèæÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂú®ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ (ADNI) Ë≥áÊñôÈõÜ‰∏äÁöÑÈ©óË≠âË≠âÂØ¶‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÊÄß„ÄÇÂèØËß£Èáã AI (XAI) ÊäÄË°ìÂíåÂÖ®Èù¢ÁöÑÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Ë≠âÂØ¶‰∫ÜÊàëÂÄëÊäÄË°ìÁöÑÊúâÊïàÊÄßÔºåÊèê‰æõ‰∫ÜÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÂíåÊàëÂÄëÊñπÊ≥ïÈáçË¶ÅÊÄßÁöÑË¶ãËß£„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Â§±Êô∫ÁóáË®∫Êñ∑Êèê‰æõ‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºåÁÇ∫Ëá®Â∫äÊáâÁî®Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÈ´òÊïàÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **A Comparative Study on Automatic Coding of Medical Letters with Explainability**
2407.13638v1 by Jamie Glen, Lifeng Han, Paul Rayson, Goran Nenadic

This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êó®Âú®Êé¢Ë®éÂ∞áËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÊäÄË°ìÂØ¶‰ΩúÊñºÈÜ´ÁôÇ‰ø°ÂáΩÁ∑®Á¢ºËá™ÂãïÂåñÔºå‰∏¶ÂÖ∑ÂÇôË¶ñË¶∫ÂåñË™™ÊòéËÉΩÂäõÂíåËºïÈáèÂåñÁöÑÊú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö„ÄÇÁõÆÂâçÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÁ∑®Á¢ºÊòØ‰∏ÄÁ®ÆÊâãÂãïÊµÅÁ®ãÔºåÊ∂âÂèäÁÇ∫ÁóÖÊÇ£Êñá‰ª∂‰∏≠ÁöÑÊØèÈ†ÖÁóÖÁóá„ÄÅÁ®ãÂ∫èÂíåËó•Áâ©ÊåáÊ¥æ‰ª£Á¢º (‰æãÂ¶ÇÔºå‰ΩøÁî® SNOMED CT ‰ª£Á¢º 56265001 Ë°®Á§∫ÂøÉËáüÁóÖ)„ÄÇÊ≠§È†òÂüüÊúâ‰ΩøÁî®ÊúÄÊñ∞ ML Ê®°ÂûãÈÄ≤Ë°åËá™ÂãïÁ∑®Á¢ºÁöÑÂàùÊ≠•Á†îÁ©∂ÔºõÁÑ∂ËÄåÔºåÁî±ÊñºÊ®°ÂûãÁöÑË§áÈõúÊÄßÂíåÂ§ßÂ∞èÔºå‰∏¶Êú™ÂØ¶ÁèæÂØ¶ÈöõÈÉ®ÁΩ≤„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•‰øÉÈÄ≤Ëá™ÂãïÁ∑®Á¢ºÂØ¶ÂãôÁöÑÂèØËÉΩÊÄßÔºåÊàëÂÄëÂú®Êú¨Âú∞ÈõªËÖ¶Ë®≠ÂÆö‰∏≠Êé¢Ë®é‰∫Ü‰∏Ä‰∫õËß£Ê±∫ÊñπÊ°àÔºõÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜË™™ÊòéÂäüËÉΩÂú® AI Ê®°ÂûãÈÄèÊòéÂ∫¶‰∏≠ÁöÑÂäüËÉΩ„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ¨ÈñãÁöÑ MIMIC-III Ë≥áÊñôÂ∫´Âíå HAN/HLAN Á∂≤Ë∑ØÊ®°ÂûãÈÄ≤Ë°å ICD ‰ª£Á¢ºÈ†êÊ∏¨„ÄÇÊàëÂÄëÈÇÑË©¶È©ó‰∫Ü ICD Âíå SNOMED CT Áü•Ë≠òÂ∫´‰πãÈñìÁöÑÂ∞çÊáâ„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÈÄô‰∫õÊ®°ÂûãÊèê‰æõ‰∫Ü 97.98% ‰ª£Á¢ºÁöÑÊúâÁî®Ë≥áË®ä„ÄÇÈÄôÈ†ÖË™øÊü•ÁµêÊûúÂèØ‰ª•ÁÇ∫ÂØ¶Âãô‰∏≠ÁöÑËá™ÂãïËá®Â∫äÁ∑®Á¢ºÂØ¶‰ΩúÊèê‰æõ‰∏Ä‰∫õË¶ãËß£Ôºå‰æãÂ¶ÇÂú®ÈÜ´Èô¢Áí∞Â¢É‰∏≠ÔºåÁî±Ëá®Â∫äÈÜ´Áîü‰ΩøÁî®ÁöÑÊú¨Âú∞ÈõªËÖ¶ÔºåÂ∞àÊ°àÈ†ÅÈù¢ \url{https://github.com/Glenj01/Medical-Coding}„ÄÇ

##### **Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**
2407.07009v1 by Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier

The support of artificial intelligence (AI) based decision-making is a key
element in future 6G networks, where the concept of native AI will be
introduced. Moreover, AI is widely employed in different critical applications
such as autonomous driving and medical diagnosis. In such applications, using
AI as black-box models is risky and challenging. Hence, it is crucial to
understand and trust the decisions taken by these models. Tackling this issue
can be achieved by developing explainable AI (XAI) schemes that aim to explain
the logic behind the black-box model behavior, and thus, ensure its efficient
and safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST
framework that is oriented toward channel estimation in wireless
communications. The core idea of the XAI-CHEST framework is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. This
manuscript provides the detailed theoretical foundations of the XAI-CHEST
framework. In particular, we derive the analytical expressions of the XAI-CHEST
loss functions and the noise threshold fine-tuning optimization problem. Hence
the designed XAI-CHEST delivers a smart input feature selection methodology
that can further improve the overall performance while optimizing the
architecture of the employed model. Simulation results show that the XAI-CHEST
framework provides valid interpretations, where it offers an improved bit error
rate performance while reducing the required computational complexity in
comparison to the classical DL-based channel estimation.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩ (AI) ÊîØÊåÅÁöÑÊ±∫Á≠ñÂà∂ÂÆöÊòØÊú™‰æÜ 6G Á∂≤Ë∑Ø‰∏≠ÁöÑÈóúÈçµÂÖÉÁ¥†ÔºåÂÖ∂‰∏≠Â∞áÂºïÂÖ•ÂéüÁîü AI ÁöÑÊ¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºåAI Âª£Ê≥õÁî®Êñº‰∏çÂêåÁöÑÈóúÈçµÊáâÁî®‰∏≠Ôºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõÂíåÈÜ´ÁôÇË®∫Êñ∑„ÄÇÂú®ÈÄô‰∫õÊáâÁî®‰∏≠Ôºå‰ΩøÁî® AI ‰ΩúÁÇ∫ÈªëÁõíÊ®°ÂûãÊòØÊúâÈ¢®Èö™‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ„ÄÇÂõ†Ê≠§ÔºåÁêÜËß£Âíå‰ø°‰ªªÈÄô‰∫õÊ®°ÂûãÂÅöÂá∫ÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇËß£Ê±∫Ê≠§ÂïèÈ°åÁöÑÊñπÊ≥ïÊòØÈñãÁôºÂèØËß£Èáã AI (XAI) Êû∂ÊßãÔºåÊó®Âú®Ëß£ÈáãÈªëÁõíÊ®°ÂûãË°åÁÇ∫ËÉåÂæåÁöÑÈÇèËºØÔºåÂæûËÄåÁ¢∫‰øùÂÖ∂ÊúâÊïà‰∏îÂÆâÂÖ®ÁöÑÈÉ®ÁΩ≤„ÄÇÊúÄËøëÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºÊìæÂãïÁöÑ XAI-CHEST Ê°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂Èù¢ÂêëÁÑ°Á∑öÈÄö‰ø°‰∏≠ÁöÑ‰ø°ÈÅì‰º∞Ë®à„ÄÇXAI-CHEST Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöÈÅéÂú®ÁÑ°ÈóúËº∏ÂÖ•‰∏äÂºïÂÖ•È´òÂô™ËÅ≤‰æÜË≠òÂà•Áõ∏ÈóúÊ®°ÂûãËº∏ÂÖ•„ÄÇÈÄô‰ªΩÊâãÁ®øÊèê‰æõ‰∫Ü XAI-CHEST Ê°ÜÊû∂ÁöÑË©≥Á¥∞ÁêÜË´ñÂü∫Á§é„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊé®Â∞é‰∫Ü XAI-CHEST ÊêçÂ§±ÂáΩÊï∏ÂíåÂô™ËÅ≤ÈñæÂÄºÂæÆË™øÂÑ™ÂåñÂïèÈ°åÁöÑËß£ÊûêË°®ÈÅîÂºè„ÄÇÂõ†Ê≠§ÔºåË®≠Ë®àÁöÑ XAI-CHEST Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊô∫ËÉΩËº∏ÂÖ•ÁâπÂæµÈÅ∏ÊìáÊñπÊ≥ïÔºåÂèØ‰ª•Âú®ÂÑ™ÂåñÊâÄÁî®Ê®°ÂûãÁöÑÊû∂ÊßãÁöÑÂêåÊôÇÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊï¥È´îÊÄßËÉΩ„ÄÇÊ®°Êì¨ÁµêÊûúË°®ÊòéÔºåXAI-CHEST Ê°ÜÊû∂Êèê‰æõ‰∫ÜÊúâÊïàÁöÑËß£ÈáãÔºåÂú®Èôç‰ΩéÊâÄÈúÄÁöÑË®àÁÆóË§áÈõúÂ∫¶ÁöÑÂêåÊôÇÔºåÊèê‰æõ‰∫ÜÊîπÈÄ≤ÁöÑÊØîÁâπÈåØË™§ÁéáÊÄßËÉΩÔºåËÄåÈÄôËàáÂü∫ÊñºÂÇ≥Áµ± DL ÁöÑ‰ø°ÈÅì‰º∞Ë®àÁõ∏ÊØî„ÄÇ

##### **Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**
2407.05440v1 by P. N. Karthikayan, Yoga Sri Varshan V, Hitesh Gupta Kattamuri, Umarani Jayaraman

This paper presents dilated Residual Network (ResNet) models for disease
classification from retinal fundus images. Dilated convolution filters are used
to replace normal convolution filters in the higher layers of the ResNet model
(dilated ResNet) in order to improve the receptive field compared to the normal
ResNet model for disease classification. This study introduces
computer-assisted diagnostic tools that employ deep learning, enhanced with
explainable AI techniques. These techniques aim to make the tool's
decision-making process transparent, thereby enabling medical professionals to
understand and trust the AI's diagnostic decision. They are particularly
relevant in today's healthcare landscape, where there is a growing demand for
transparency in AI applications to ensure their reliability and ethical use.
The dilated ResNet is used as a replacement for the normal ResNet to enhance
the classification accuracy of retinal eye diseases and reduce the required
computing time. The dataset used in this work is the Ocular Disease Intelligent
Recognition (ODIR) dataset which is a structured ophthalmic database with eight
classes covering most of the common retinal eye diseases. The evaluation
metrics used in this work include precision, recall, accuracy, and F1 score. In
this work, a comparative study has been made between normal ResNet models and
dilated ResNet models on five variants namely ResNet-18, ResNet-34, ResNet-50,
ResNet-101, and ResNet-152. The dilated ResNet model shows promising results as
compared to normal ResNet with an average F1 score of 0.71, 0.70, 0.69, 0.67,
and 0.70 respectively for the above respective variants in ODIR multiclass
disease classification.

ÊëòË¶ÅÔºö<paragraph>Êú¨ÊñáÊèêÂá∫‰∫ÜÁî®‰∫éËßÜÁΩëËÜúÁúºÂ∫ïÂõæÂÉèÁñæÁóÖÂàÜÁ±ªÁöÑÊâ©Âº†ÊÆãÂ∑ÆÁΩëÁªú (ResNet) Ê®°Âûã„ÄÇÊâ©Âº†Âç∑ÁßØÊª§Ê≥¢Âô®Áî®‰∫éÊõøÊç¢ ResNet Ê®°ÂûãËæÉÈ´òÂ±Ç‰∏≠ÁöÑÊ≠£Â∏∏Âç∑ÁßØÊª§Ê≥¢Âô®ÔºàÊâ©Âº† ResNetÔºâÔºå‰ª•ÊîπÂñÑ‰∏éÁî®‰∫éÁñæÁóÖÂàÜÁ±ªÁöÑÊ≠£Â∏∏ ResNet Ê®°ÂûãÁõ∏ÊØîÁöÑÊÑüÂèóÈáé„ÄÇÊú¨Á†îÁ©∂‰ªãÁªç‰∫ÜÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÁöÑËÆ°ÁÆóÊú∫ËæÖÂä©ËØäÊñ≠Â∑•ÂÖ∑ÔºåÂπ∂ÈÄöËøáÂèØËß£ÈáäÁöÑ AI ÊäÄÊúØËøõË°å‰∫ÜÂ¢ûÂº∫„ÄÇËøô‰∫õÊäÄÊúØÊó®Âú®‰ΩøËØ•Â∑•ÂÖ∑ÁöÑÂÜ≥Á≠ñËøáÁ®ãÈÄèÊòéÂåñÔºå‰ªéËÄå‰ΩøÂåªÁñó‰∏ì‰∏ö‰∫∫ÂëòËÉΩÂ§üÁêÜËß£Âíå‰ø°‰ªª AI ÁöÑËØäÊñ≠ÂÜ≥Á≠ñ„ÄÇÂÆÉ‰ª¨Âú®ÂΩì‰ªäÂåªÁñó‰øùÂÅ•È¢ÜÂüüÂ∞§‰∏∫ÈáçË¶ÅÔºåÂõ†‰∏∫ÂØπ AI Â∫îÁî®Á®ãÂ∫èÁöÑÈÄèÊòéÂ∫¶ÈúÄÊ±Ç‰∏çÊñ≠Â¢ûÈïøÔºå‰ª•Á°Æ‰øùÂÖ∂ÂèØÈù†ÊÄßÂíåÈÅìÂæ∑‰ΩøÁî®„ÄÇÊâ©Âº† ResNet Áî®‰ΩúÊ≠£Â∏∏ ResNet ÁöÑÊõø‰ª£ÂìÅÔºå‰ª•ÊèêÈ´òËßÜÁΩëËÜúÁúºÁóÖÁöÑÂàÜÁ±ªÂáÜÁ°ÆÊÄßÂπ∂ÂáèÂ∞ëÊâÄÈúÄÁöÑËÆ°ÁÆóÊó∂Èó¥„ÄÇÊú¨Â∑•‰Ωú‰∏≠‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜÊòØ Ocular Disease Intelligent Recognition (ODIR) Êï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÁúºÁßëÊï∞ÊçÆÂ∫ìÔºåÂåÖÂê´ÂÖ´Á±ªÊ∂µÁõñÂ§ßÂ§öÊï∞Â∏∏ËßÅËßÜÁΩëËÜúÁúºÁóÖ„ÄÇÊú¨Â∑•‰Ωú‰∏≠‰ΩøÁî®ÁöÑËØÑ‰º∞ÊåáÊ†áÂåÖÊã¨Á≤æÁ°ÆÂ∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅÂáÜÁ°ÆÂ∫¶Âíå F1 ÂàÜÊï∞„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÂØπÊ≠£Â∏∏ ResNet Ê®°ÂûãÂíåÊâ©Âº† ResNet Ê®°ÂûãÂú®‰∫î‰∏™Âèò‰ΩìÔºàÂç≥ ResNet-18„ÄÅResNet-34„ÄÅResNet-50„ÄÅResNet-101 Âíå ResNet-152Ôºâ‰πãÈó¥ËøõË°å‰∫ÜÊØîËæÉÁ†îÁ©∂„ÄÇ‰∏éÊ≠£Â∏∏ ResNet Áõ∏ÊØîÔºåÊâ©Âº† ResNet Ê®°ÂûãÊòæÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁªìÊûúÔºåÂú® ODIR Â§öÁ±ªÁñæÁóÖÂàÜÁ±ª‰∏≠Ôºå‰∏äËø∞ÂêÑ‰∏™Âèò‰ΩìÁöÑÂπ≥Âùá F1 ÂàÜÊï∞ÂàÜÂà´‰∏∫ 0.71„ÄÅ0.70„ÄÅ0.69„ÄÅ0.67 Âíå 0.70„ÄÇ</paragraph>

##### **A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**
2407.15851v1 by Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li

The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
extant surveys on the trustworthiness of foundation models fail to address
their specific variations and applications within the medical imaging domain.
This survey paper reviews the current research on foundation models in the
major medical imaging applications, with a focus on segmentation, medical
report generation, medical question and answering (Q&A), and disease diagnosis,
which includes trustworthiness discussion in their manuscripts. We explore the
complex challenges of making foundation models for medical image analysis
trustworthy, associated with each application, and summarize the current
concerns and strategies to enhance trustworthiness. Furthermore, we explore the
future promises of these models in revolutionizing patient care. Our analysis
underscores the imperative for advancing towards trustworthy AI in medical
image analysis, advocating for a balanced approach that fosters innovation
while ensuring ethical and equitable healthcare delivery.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏äÁöÑÂø´ÈÄüÈÄ≤Â±ï‰ª£Ë°®ËëóÂú®Â¢ûÂº∑Ë®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇÊñπÈù¢ÈÇÅÂá∫‰∫Ü‰∏ÄÂ§ßÊ≠•„ÄÇÁÑ∂ËÄåÔºåÂü∫Á§éÊ®°ÂûãÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÂö¥Ê†ºÊ™¢Êü•ÂÖ∂ÂèØ‰ø°Â∫¶ÔºåÂåÖÊã¨Èö±ÁßÅ„ÄÅÁ©©ÂÅ•ÊÄß„ÄÅÂèØÈù†ÊÄß„ÄÅÂèØËß£ÈáãÊÄßÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÁï∂ÂâçÈóúÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Âü∫Á§éÊ®°ÂûãÁöÑË™øÊü•ÊñáÁçªÈ°ØÁ§∫Âá∫Áõ∏Áï∂Â§ßÁöÑÂ∑ÆË∑ùÔºåÁâπÂà•ÊòØÂú®ÂèØ‰ø°Â∫¶ÊñπÈù¢„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑÈóúÊñºÂü∫Á§éÊ®°ÂûãÂèØ‰ø°Â∫¶ÁöÑË™øÊü•Êú™ËÉΩËß£Ê±∫ÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüüÂÖßÁöÑÂÖ∑È´îËÆäÂåñÂíåÊáâÁî®„ÄÇÈÄôÁØáË™øÊü•Ë´ñÊñáÂõûÈ°ß‰∫ÜÁï∂ÂâçÈóúÊñºÂü∫Á§éÊ®°ÂûãÂú®‰∏ªË¶ÅÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®‰∏≠ÁöÑÁ†îÁ©∂ÔºåÈáçÈªûÈóúÊ≥®ÂàÜÂâ≤„ÄÅÈÜ´ÁôÇÂ†±ÂëäÁîüÊàê„ÄÅÈÜ´ÁôÇÂïèÈ°åÂíåËß£Á≠î (Q&A) ‰ª•ÂèäÁñæÁóÖË®∫Êñ∑ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊâãÁ®ø‰∏≠ÁöÑÂèØ‰ø°Â∫¶Ë®éË´ñ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜËÆìÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÂü∫Á§éÊ®°ÂûãÂÄºÂæó‰ø°Ë≥¥ÁöÑË§áÈõúÊåëÊà∞ÔºåËàáÊØèÂÄãÊáâÁî®Áõ∏ÈóúÔºå‰∏¶Á∏ΩÁµê‰∫ÜÁï∂ÂâçÊèêÈ´òÂèØ‰ø°Â∫¶ÁöÑÂïèÈ°åÂíåÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÊ®°ÂûãÂú®Èù©Êñ∞ÊÇ£ËÄÖÁÖßË≠∑ÊñπÈù¢ÁöÑÊú™‰æÜÂâçÊôØ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÂº∑Ë™ø‰∫ÜÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠ÊúùËëóÂèØ‰ø°Ë≥¥ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈÇÅÈÄ≤ÁöÑÂøÖË¶ÅÊÄßÔºåÊèêÂÄ°‰∏ÄÁ®ÆÂπ≥Ë°°ÁöÑÊñπÊ≥ïÔºåÊó¢ËÉΩ‰øÉÈÄ≤ÂâµÊñ∞ÔºåÂèàËÉΩÁ¢∫‰øùÈÅìÂæ∑ÂíåÂÖ¨Âπ≥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇ

##### **The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**
2407.06206v1 by Ximing Wen, Rosina O. Weber, Anik Sen, Darryl Hannan, Steven C. Nesbit, Vincent Chan, Alberto Goffi, Michael Morris, John C. Hunninghake, Nicholas E. Villalobos, Edward Kim, Christopher J. MacLellan

Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and
interpreting ultrasound scans right at the patient's bedside. However, the
expertise needed to interpret these images is considerable and may not always
be present in emergency situations. This reality makes algorithms such as
machine learning classifiers extremely valuable to augment human decisions.
POCUS devices are becoming available at a reasonable cost in the size of a
mobile phone. The challenge of turning POCUS devices into life-saving tools is
that interpretation of ultrasound images requires specialist training and
experience. Unfortunately, the difficulty to obtain positive training images
represents an important obstacle to building efficient and accurate
classifiers. Hence, the problem we try to investigate is how to explore
strategies to increase accuracy of classifiers trained with scarce data. We
hypothesize that training with a few data instances may not suffice for
classifiers to generalize causing them to overfit. Our approach uses an
Explainable AI-Augmented approach to help the algorithm learn more from less
and potentially help the classifier better generalize.

ÊëòË¶ÅÔºöÂ∫äÈÇäË∂ÖÈü≥Ê≥¢ (POCUS) ÊòØËá®Â∫äÈÜ´Â∏´Âú®ÊÇ£ËÄÖÂ∫äÈÇäÈÄ≤Ë°åÂíåËß£ËÆÄË∂ÖÈü≥Ê≥¢ÊéÉÊèèÁöÑÂØ¶Âãô„ÄÇÁÑ∂ËÄåÔºåËß£ËÆÄÈÄô‰∫õÂΩ±ÂÉèÊâÄÈúÄÁöÑÂ∞àÊ•≠Áü•Ë≠òÁõ∏Áï∂ÂèØËßÄÔºåËÄå‰∏îÂú®Á∑äÊÄ•ÊÉÖÊ≥Å‰∏ãÂèØËÉΩ‰∏¶ÈùûÈö®ÊôÇÂÖ∑ÂÇô„ÄÇÈÄôÁ®ÆÁèæÂØ¶ÊÉÖÊ≥Å‰ΩøÂæóÊ©üÂô®Â≠∏ÁøíÂàÜÈ°ûÂô®Á≠âÊºîÁÆóÊ≥ïÂ∞çÊñºÂä†Âº∑‰∫∫È°ûÊ±∫Á≠ñËÆäÂæóÊ•µÁÇ∫ÊúâÂÉπÂÄº„ÄÇPOCUS Ë£ùÁΩÆÊ≠£‰ª•ÂêàÁêÜÊàêÊú¨Êé®Âá∫ÔºåÂ∞∫ÂØ∏ÁÇ∫ÊâãÊ©üÂ§ßÂ∞è„ÄÇÂ∞á POCUS Ë£ùÁΩÆËΩâËÆäÁÇ∫ÊïëÁîüÂ∑•ÂÖ∑ÁöÑÊåëÊà∞Âú®ÊñºÔºåËß£ËÆÄË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÈúÄË¶ÅÂ∞àÈñÄË®ìÁ∑¥ÂíåÁ∂ìÈ©ó„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂèñÂæóÊ≠£ÂêëË®ìÁ∑¥ÂΩ±ÂÉèÁöÑÂõ∞Èõ£Â∫¶‰ª£Ë°®ËëóÂª∫ÁΩÆÊúâÊïàÁéá‰∏îÊ∫ñÁ¢∫ÁöÑÂàÜÈ°ûÂô®ÁöÑ‰∏ÄÂ§ßÈöúÁ§ô„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂòóË©¶Êé¢Ë®éÁöÑÂïèÈ°åÊòØÂ¶Ç‰ΩïÊé¢Á¥¢Á≠ñÁï•Ôºå‰ª•ÊèêÈ´ò‰ΩøÁî®Á®ÄÁñèË≥áÊñôË®ìÁ∑¥ÁöÑÂàÜÈ°ûÂô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÂÅáË®≠‰ΩøÁî®Â∞ëÊï∏Ë≥áÊñôÂØ¶‰æãÈÄ≤Ë°åË®ìÁ∑¥ÂèØËÉΩ‰∏çË∂≥‰ª•ËÆìÂàÜÈ°ûÂô®Ê¶ÇÊã¨ÔºåÂ∞éËá¥ÂÆÉÂÄëÈÅéÂ∫¶Êì¨Âêà„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®ÂèØËß£Èáã AI Â¢ûÂº∑ÊñπÊ≥ïÔºå‰ª•ÂçîÂä©ÊºîÁÆóÊ≥ïÂæûËºÉÂ∞ëÁöÑË≥áÊñô‰∏≠Â≠∏ÁøíÊõ¥Â§öÔºå‰∏¶ÊΩõÂú®ÂçîÂä©ÂàÜÈ°ûÂô®Êõ¥Â•ΩÂú∞Ê¶ÇÊã¨„ÄÇ

##### **Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**
2407.00167v1 by Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Wyatt Bellamy, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu, Ming Huang

In recent years, the United States has witnessed a significant surge in the
popularity of vaping or e-cigarette use, leading to a notable rise in cases of
e-cigarette and vaping use-associated lung injury (EVALI) that caused
hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting
the urgency to comprehend vaping behaviors and develop effective strategies for
cessation. Due to the ubiquity of social media platforms, over 4.7 billion
users worldwide use them for connectivity, communications, news, and
entertainment with a significant portion of the discourse related to health,
thereby establishing social media data as an invaluable organic data resource
for public health research. In this study, we extracted a sample dataset from
one vaping sub-community on Reddit to analyze users' quit-vaping intentions.
Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit
vaping intention detection, this study compares the outcomes of this model
against layman and clinical expert annotations. Using different prompting
strategies such as zero-shot, one-shot, few-shot and chain-of-thought
prompting, we developed 8 prompts with varying levels of detail to explain the
task to GPT-4 and also evaluated the performance of the strategies against each
other. These preliminary findings emphasize the potential of GPT-4 in social
media data analysis, especially in identifying users' subtle intentions that
may elude human detection.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÁæéÂúãË¶ãË≠â‰∫ÜÈõªÂ≠êÁÖôÊàñÈõªÂ≠êÈ¶ôËè∏‰ΩøÁî®ÁéáÂ§ßÂπÖÊøÄÂ¢ûÔºåÂ∞éËá¥ÈõªÂ≠êÁÖôÂíåÈõªÂ≠êÁÖô‰ΩøÁî®Áõ∏ÈóúËÇ∫ÊêçÂÇ∑ (EVALI) ÁóÖ‰æãÈ°ØËëóÂ¢ûÂä†ÔºåÂú® 2019 Âπ¥ EVALI ÁàÜÁôºÊúüÈñìÈÄ†Êàê‰ΩèÈô¢ÂíåÊ≠ª‰∫°ÔºåÂá∏È°Ø‰∫ÜÁêÜËß£ÈõªÂ≠êÁÖôË°åÁÇ∫ÂíåÂà∂ÂÆöÊúâÊïàÊàíËè∏Á≠ñÁï•ÁöÑËø´ÂàáÊÄß„ÄÇÁî±ÊñºÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑÊôÆÂèäÔºåÂÖ®ÁêÉË∂ÖÈÅé 47 ÂÑÑ‰ΩøÁî®ËÄÖ‰ΩøÁî®ÂÆÉÂÄëÈÄ≤Ë°åÈÄ£Áµê„ÄÅÊ∫ùÈÄö„ÄÅÊñ∞ËÅûÂíåÂ®õÊ®ÇÔºåÂÖ∂‰∏≠ÂæàÂ§ß‰∏ÄÈÉ®ÂàÜËàáÂÅ•Â∫∑Áõ∏ÈóúÔºåÂõ†Ê≠§Â∞áÁ§æÁæ§Â™íÈ´îË≥áÊñôÂª∫Á´ãÁÇ∫ÂÖ¨ÂÖ±Ë°õÁîüÁ†îÁ©∂‰∏≠ÁÑ°ÂÉπÁöÑÊúâÊ©üË≥áÊñôË≥áÊ∫ê„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂæû Reddit ‰∏ä‰∏ÄÂÄãÈõªÂ≠êÁÖôÂ≠êÁ§æÁæ§‰∏≠ÊèêÂèñ‰∏ÄÂÄãÁØÑ‰æãË≥áÊñôÈõÜÔºå‰ª•ÂàÜÊûê‰ΩøÁî®ËÄÖÁöÑÊàíÈõªÂ≠êÁÖôÊÑèÂúñ„ÄÇÂà©Áî® OpenAI ÊúÄÊñ∞ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã GPT-4 ÈÄ≤Ë°åÂè•Â≠êÂ±§Á¥öÁöÑÊàíÈõªÂ≠êÁÖôÊÑèÂúñÂÅµÊ∏¨ÔºåÊú¨Á†îÁ©∂ÊØîËºÉ‰∫ÜÊ≠§Ê®°ÂûãÁöÑÁµêÊûúËàáÂ§ñË°å‰∫∫ÂíåËá®Â∫äÂ∞àÂÆ∂Ë®ªËß£„ÄÇ‰ΩøÁî®‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰æãÂ¶ÇÈõ∂Ê¨°Â≠∏Áøí„ÄÅ‰∏ÄÊ¨°Â≠∏Áøí„ÄÅÂ∞ëÊ¨°Â≠∏ÁøíÂíåÊÄùËÄÉÈèàÊèêÁ§∫ÔºåÊàëÂÄëÈñãÁôº‰∫Ü 8 ÂÄãÊèêÁ§∫ÔºåË©≥Á¥∞Á®ãÂ∫¶‰∏çÂêåÔºåÂêë GPT-4 Ëß£Èáã‰ªªÂãôÔºå‰∏¶Ë©ï‰º∞ÈÄô‰∫õÁ≠ñÁï•ÂΩºÊ≠§‰πãÈñìÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÂàùÊ≠•ÁôºÁèæÂº∑Ë™ø‰∫Ü GPT-4 Âú®Á§æÁæ§Â™íÈ´îË≥áÊñôÂàÜÊûê‰∏≠ÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®Ë≠òÂà•‰∫∫È°ûÂÅµÊ∏¨ÂèØËÉΩÁÑ°Ê≥ïÂØüË¶∫ÁöÑ‰ΩøÁî®ËÄÖÂæÆÂ¶ôÊÑèÂúñÊñπÈù¢„ÄÇ

##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁõÆÂâçÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÈªëÁõíÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÂèØËß£ÈáãÊÄß‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÈ†òÂüüËá¥ÂäõÊñºËß£Ê±∫ÈÄôÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÈÄôÂú®ÈáëËûç„ÄÅÊ≥ïÂæãÂíåÂÅ•Â∫∑Á≠âÈ´òÈ¢®Èö™È†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁØÑÁñáË´ñÂÆöÁæ© AI Ê®°ÂûãÂèäÂÖ∂ÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊé°Áî®ÁµÑÂêàÊ®°ÂûãÁöÑÊ¶ÇÂøµÔºåÂÆÉ‰ª•ÂΩ¢ÂºèÂº¶ÂúñÁöÑÂΩ¢ÂºèÁúãÂæÖÊ®°ÂûãÔºåÈÄô‰∫õÂº¶ÂúñÊçïÁç≤‰∫ÜÊ®°ÂûãÁöÑÊäΩË±°ÁµêÊßãÂèäÂÖ∂ÂÖ∑È´îÂØ¶Áèæ„ÄÇÈÄôÁ®ÆÁ∂úÂêàËßÄÈªûÂåÖÂê´‰∫ÜÁ¢∫ÂÆöÊÄß„ÄÅÊ¶ÇÁéáÊÄßÂíåÈáèÂ≠êÊ®°Âûã„ÄÇÊàëÂÄëÂ∞áÂêÑÁ®Æ AI Ê®°Âûã‰ΩúÁÇ∫ÁµÑÂêàÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÂåÖÊã¨Á∑öÊÄßÂíåÂü∫ÊñºË¶èÂâáÁöÑÊ®°Âûã„ÄÅÔºàÈÅûËø¥ÔºâÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅTransformer„ÄÅVAEÔºå‰ª•ÂèäÂõ†ÊûúÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÊ†πÊìöÊ®°ÂûãÁöÑÁµÑÂêàÁµêÊßãÁµ¶Âá∫Ê®°ÂûãËß£ÈáãÁöÑÂÆöÁæ©ÔºåÂ±ïÁ§∫Â¶Ç‰ΩïÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜÊæÑÊ∏Ö XAI ‰∏≠ÁöÑÂ∏∏Ë¶ã‰∏ªÈ°å„ÄÇÊàëÂÄëÁôºÁèæÔºåËÆìÊ®ôÊ∫ñÁöÑ„ÄåÂÖßÂú®ÂèØËß£Èáã„ÄçÊ®°ÂûãÂ¶ÇÊ≠§ÈÄèÊòéÁöÑÂéüÂõ†Âú®ÂúñË°®‰∏≠Ë°®ÁèæÂæóÊúÄÁÇ∫Ê∏ÖÊ•ö„ÄÇÈÄôÂºïÂ∞éÊàëÂÄëÂæóÂá∫Êõ¥‰∏ÄËà¨ÁöÑÁµÑÂêàÂèØËß£ÈáãÔºàCIÔºâÊ®°ÂûãÊ¶ÇÂøµÔºåÂÆÉÂè¶Â§ñÈÇÑÂåÖÊã¨Âõ†Êûú„ÄÅÊ¶ÇÂøµÁ©∫ÈñìÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü CI Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂÑ™Âã¢„ÄÇÈ¶ñÂÖàÔºåÂÆÉÂÄëÁöÑÁµÑÂêàÁµêÊßãÂÖÅË®±Ë®àÁÆóÂÖ∂‰ªñÊÑüËààË∂£ÁöÑÈáèÔºå‰∏¶ÂèØËÉΩÈÄöÈÅéÂåπÈÖçÊ®°ÂûãÁöÑÁµêÊßã‰æÜ‰øÉÈÄ≤ÂæûÊ®°ÂûãÂà∞Ë¢´Âª∫Ê®°ÁèæË±°ÁöÑÊé®ÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉÂÄëÂÖÅË®±Â∞çÂÖ∂Ë°åÁÇ∫ÈÄ≤Ë°åÂúñËß£Ë™™ÊòéÔºåÈÄô‰∫õË™™ÊòéÂü∫ÊñºÂΩ±ÈüøÁ¥ÑÊùü„ÄÅÂúñËß£ÊâãË°ìÂíåÈáçÂØ´Ë™™Êòé„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÁöÑË®±Â§öÊú™‰æÜÊñπÂêëÔºåÊèêÂá∫‰∫ÜÂ¶Ç‰ΩïÂú®ÂØ¶Ë∏ê‰∏≠Â≠∏ÁøíÈÄôÁ®ÆÊúâÊÑèÁæ©ÁöÑÁµêÊßãÂåñÊ®°ÂûãÁöÑÂïèÈ°å„ÄÇ</paragraph>

##### **Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**
2406.07114v2 by Fatemeh Ebrahimzadeh, Ramin Safa

The concept of Metaverse has attracted a lot of attention in various fields
and one of its important applications is health and treatment. The Metaverse
has enormous potential to transform healthcare by changing patient care,
medical education, and the way teaching/learning and research are done. The
purpose of this research is to provide an introduction to the basic concepts
and fundamental technologies of the Metaverse. This paper examines the pros and
cons of the Metaverse in healthcare context and analyzes its potential from the
technology and AI perspective. In particular, the role of machine learning
methods is discussed; We will explain how machine learning algorithms can be
applied to the Metaverse generated data to gain better insights in healthcare
applications. Additionally, we examine the future visions of the Metaverse in
health delivery, by examining emerging technologies such as blockchain and also
addressing privacy concerns. The findings of this study contribute to a deeper
understanding of the applications of Metaverse in healthcare and its potential
to revolutionize the delivery of medical services.

ÊëòË¶ÅÔºöÂÖÉÂÆáÂÆôÁöÑÊ¶ÇÂøµÂú®ÂêÑÂÄãÈ†òÂüüÈÉΩÂÇôÂèóÈóúÊ≥®ÔºåÂÖ∂ÈáçË¶ÅÊáâÁî®‰πã‰∏Ä‰æøÊòØÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÂÖÉÂÆáÂÆôÊúâÂ∑®Â§ßÁöÑÊΩõÂäõÈÄèÈÅéÊîπËÆäÁóÖÊÇ£ÁÖßË≠∑„ÄÅÈÜ´Â≠∏ÊïôËÇ≤Ôºå‰ª•ÂèäÊïôÂ≠∏/Â≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÊñπÂºè‰æÜËΩâÂûãÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÁöÑÊòØÊèê‰æõÂÖÉÂÆáÂÆôÂü∫Êú¨Ê¶ÇÂøµÂíåÂü∫Á§éÊäÄË°ìÁöÑ‰ªãÁ¥π„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÁöÑÂÑ™Áº∫ÈªûÔºå‰∏¶ÂæûÊäÄË°ìÂíå AI ÁöÑËßíÂ∫¶ÂàÜÊûêÂÖ∂ÊΩõÂäõ„ÄÇÁâπÂà•ÊòØÔºåË®éË´ñ‰∫ÜÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑËßíËâ≤ÔºõÊàëÂÄëÂ∞áË™™ÊòéÂ¶Ç‰ΩïÂ∞áÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÊáâÁî®ÊñºÂÖÉÂÆáÂÆôÁî¢ÁîüÁöÑË≥áÊñôÔºå‰ª•Áç≤ÂæóÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÊñπÈù¢ÁöÑÊõ¥‰Ω≥Ë¶ãËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÊé¢Ë®éÂçÄÂ°äÈèàÁ≠âÊñ∞ËààÊäÄË°ìÔºå‰∏¶Ëß£Ê±∫Èö±ÁßÅÂïèÈ°åÔºå‰æÜÊé¢Ë®éÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑÊú™‰æÜÈ°òÊôØ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁôºÁèæÊúâÂä©ÊñºÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂÖÉÂÆáÂÆôÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•ÂèäÂÖ∂Âú®ÈÜ´ÁôÇÊúçÂãôÊèê‰æõÊñπÈù¢ÁôºÊèÆÈù©ÂëΩÊÄßËÆäÈù©ÁöÑÊΩõÂäõ„ÄÇ

##### **AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**
2406.06728v1 by K M Tawsik Jawad, Anusha Verma, Fathi Amsaad

Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with
no known ultimo cure and high morbidity. Research demonstrates that progressive
Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly
impacts kidney structure and functions, eventually leading to kidney failure.
With the progression of time, chronic kidney disease has moved from a
life-threatening disease affecting few people to a common disorder of varying
severity. The goal of this research is to visualize dominating features,
feature scores, and values exhibited for early prognosis and detection of CKD
using ensemble learning and explainable AI. For that, an AI-driven predictive
analytics approach is proposed to aid clinical practitioners in prescribing
lifestyle modifications for individual patients to reduce the rate of
progression of this disease. Our dataset is collected on body vitals from
individuals with CKD and healthy subjects to develop our proposed AI-driven
solution accurately. In this regard, blood and urine test results are provided,
and ensemble tree-based machine-learning models are applied to predict unseen
cases of CKD. Our research findings are validated after lengthy consultations
with nephrologists. Our experiments and interpretation results are compared
with existing explainable AI applications in various healthcare domains,
including CKD. The comparison shows that our developed AI models, particularly
the Random Forest model, have identified more features as significant
contributors than XgBoost. Interpretability (I), which measures the ratio of
important to masked features, indicates that our XgBoost model achieved a
higher score, specifically a Fidelity of 98\%, in this metric and naturally in
the FII index compared to competing models.

ÊëòË¶ÅÔºöÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÂª£Ê≥õÁöÑÊÖ¢ÊÄßÁñæÁóÖÔºåÊ≤íÊúâÂ∑≤Áü•ÁöÑÊúÄÁµÇÁôÇÊ≥ï‰∏îÁôºÁóÖÁéáÂæàÈ´ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄ≤Ë°åÊÄßÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÁï∞Ë≥™ÊÄßÁñæÁóÖÔºåÊúÉÈ°ØËëóÂΩ±ÈüøËÖéËáüÁµêÊßãÂíåÂäüËÉΩÔºåÊúÄÁµÇÂ∞éËá¥ËÖéË°∞Á´≠„ÄÇÈö®ËëóÊôÇÈñìÁöÑÊé®ÁßªÔºåÊÖ¢ÊÄßËÖéËáüÁóÖÂ∑≤ÂæûÂΩ±ÈüøÂ∞ëÊï∏‰∫∫ÁöÑËá¥ÂëΩÁñæÁóÖËΩâËÆäÁÇ∫‰∏ÄÁ®ÆÂö¥ÈáçÁ®ãÂ∫¶‰∏çÂêåÁöÑÂ∏∏Ë¶ãÁñæÁóÖ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ôÊòØ‰ΩøÁî®ÈõÜÊàêÂ≠∏ÁøíÂíåÂèØËß£ÈáãÁöÑ AI ÈÄ≤Ë°åÊó©ÊúüÈ†êÂæåÂíå CKD Ê™¢Ê∏¨Ôºå‰∏¶Ë¶ñË¶∫Âåñ‰∏ªÂ∞éÁâπÂæµ„ÄÅÁâπÂæµÂàÜÊï∏ÂíåË°®ÁèæÂá∫ÁöÑÂÄº„ÄÇÁÇ∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ AI È©ÖÂãïÁöÑÈ†êÊ∏¨ÂàÜÊûêÊñπÊ≥ïÔºå‰ª•Âπ´Âä©Ëá®Â∫äÈÜ´ÁîüÁÇ∫ÂÄãÂà•ÊÇ£ËÄÖÈñãÂÖ∑ÁîüÊ¥ªÊñπÂºè‰øÆÊîπÂª∫Ë≠∞Ôºå‰ª•Èôç‰ΩéÈÄôÁ®ÆÁñæÁóÖÁöÑÈÄ≤Â±ïÈÄüÂ∫¶„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÊòØÂæû CKD ÊÇ£ËÄÖÂíåÂÅ•Â∫∑ÂèóË©¶ËÄÖÁöÑË∫´È´îÁîüÂëΩÈ´îÂæµ‰∏≠Êî∂ÈõÜÁöÑÔºå‰ª•Ê∫ñÁ¢∫ÈñãÁôºÊàëÂÄëÊèêÂá∫ÁöÑ AI È©ÖÂãïÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂú®ÈÄôÊñπÈù¢ÔºåÊèê‰æõ‰∫ÜË°ÄÊ∂≤ÂíåÂ∞øÊ∂≤Ê™¢Ê∏¨ÁµêÊûúÔºå‰∏¶ÊáâÁî®Âü∫ÊñºÈõÜÊàêÊ®πÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨Êú™ÁôºÁèæÁöÑ CKD ÁóÖ‰æã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ∂ìÈÅéËàáËÖéËáüÁßëÈÜ´ÁîüÁöÑÈï∑ÊúüË´ÆË©¢ÂæåÂæóÂà∞È©óË≠â„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂíåËß£ÈáãÁµêÊûúËàáÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ÁèæÊúâÁöÑÂèØËß£Èáã AI ÊáâÁî®ÈÄ≤Ë°å‰∫ÜÊØîËºÉÔºåÂåÖÊã¨ CKD„ÄÇÊØîËºÉË°®ÊòéÔºåÊàëÂÄëÈñãÁôºÁöÑ AI Ê®°ÂûãÔºåÁâπÂà•ÊòØÈö®Ê©üÊ£ÆÊûóÊ®°ÂûãÔºåÂ∑≤Á∂ìÁ¢∫ÂÆö‰∫ÜÊØî XgBoost Êõ¥Â§ö‰ΩúÁÇ∫ÈáçË¶ÅË≤¢ÁçªËÄÖÁöÑÁâπÂæµ„ÄÇÂèØËß£ÈáãÊÄß (I) Ë°°ÈáèÈáçË¶ÅÁâπÂæµËàáÊé©ËìãÁâπÂæµÁöÑÊØîÁéáÔºåË°®ÊòéÊàëÂÄëÁöÑ XgBoost Ê®°ÂûãÂú®ÈÄôÂÄãÊåáÊ®ô‰∏≠Áç≤Âæó‰∫ÜÊõ¥È´òÁöÑÂàÜÊï∏ÔºåÁâπÂà•ÊòØ 98% ÁöÑ‰øùÁúüÂ∫¶Ôºå‰∏¶‰∏îÂú® FII ÊåáÊï∏‰∏≠Ëá™ÁÑ∂È´òÊñºÁ´∂Áà≠Ê®°Âûã„ÄÇ

##### **Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**
2406.05984v1 by Yusif Ibrahimov, Tarique Anwar, Tommy Yuan

Mental health constitutes a complex and pervasive global challenge, affecting
millions of lives and often leading to severe consequences. In this paper, we
conduct a thorough survey to explore the intersection of data science,
artificial intelligence, and mental healthcare, focusing on the recent
developments of mental disorder detection through online social media (OSM). A
significant portion of the population actively engages in OSM platforms,
creating a vast repository of personal data that holds immense potential for
mental health analytics. The paper navigates through traditional diagnostic
methods, state-of-the-art data- and AI-driven research studies, and the
emergence of explainable AI (XAI) models for mental healthcare. We review
state-of-the-art machine learning methods, particularly those based on modern
deep learning, while emphasising the need for explainability in healthcare AI
models. The experimental design section provides insights into prevalent
practices, including available datasets and evaluation approaches. We also
identify key issues and challenges in the field and propose promising future
research directions. As mental health decisions demand transparency,
interpretability, and ethical considerations, this paper contributes to the
ongoing discourse on advancing XAI in mental healthcare through social media.
The comprehensive overview presented here aims to guide researchers,
practitioners, and policymakers in developing the area of mental disorder
detection.

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑ÊßãÊàê‰∫Ü‰∏ÄÈ†ÖË§áÈõú‰∏îÊôÆÈÅçÁöÑÂÖ®ÁêÉÊåëÊà∞ÔºåÂΩ±Èüø‰∫ÜÊï∏ÁôæËê¨‰∫∫ÁöÑÁîüÊ¥ªÔºå‰∏¶Á∂ìÂ∏∏Â∞éËá¥Âö¥ÈáçÁöÑÂæåÊûú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂæπÂ∫ïÁöÑË™øÊü•Ôºå‰ª•Êé¢Á¥¢Êï∏ÊìöÁßëÂ≠∏„ÄÅ‰∫∫Â∑•Êô∫ÊÖßÂíåÂøÉÁêÜ‰øùÂÅ•ÁöÑ‰∫§ÈõÜÔºåÈáçÈªûÈóúÊ≥®ÈÄöÈÅéÁ∑ö‰∏äÁ§æ‰∫§Â™íÈ´î (OSM) ÈÄ≤Ë°åÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨ÁöÑÊúÄÊñ∞ÁôºÂ±ï„ÄÇÂæàÂ§ß‰∏ÄÈÉ®ÂàÜ‰∫∫Âè£Á©çÊ•µÂèÉËàá OSM Âπ≥Âè∞ÔºåÂâµÈÄ†‰∫Ü‰∏ÄÂÄãÈæêÂ§ßÁöÑ‰∫∫Âì°Ë≥áÊñôÂ∫´ÔºåÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÂàÜÊûêÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÇ≥Áµ±ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÅÊúÄÂÖàÈÄ≤ÁöÑË≥áÊñôÂíå AI È©ÖÂãïÁöÑÁ†îÁ©∂Ôºå‰ª•ÂèäÂøÉÁêÜ‰øùÂÅ•‰∏≠ÂèØËß£Èáã AI (XAI) Ê®°ÂûãÁöÑÂá∫Áèæ„ÄÇÊàëÂÄëÂõûÈ°ß‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºåÁâπÂà•ÊòØÈÇ£‰∫õÂü∫ÊñºÁèæ‰ª£Ê∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÂêåÊôÇÂº∑Ë™ø‰∫ÜÈÜ´ÁôÇ‰øùÂÅ• AI Ê®°Âûã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÂØ¶È©óË®≠Ë®àÈÉ®ÂàÜÊèê‰æõ‰∫ÜÂ∞çÊôÆÈÅçÂÅöÊ≥ïÁöÑË¶ãËß£ÔºåÂåÖÊã¨ÂèØÁî®ÁöÑË≥áÊñôÈõÜÂíåË©ï‰º∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÈÇÑÊâæÂá∫Ë©≤È†òÂüüÁöÑ‰∏ªË¶ÅÂïèÈ°åÂíåÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊúâÂ∏åÊúõÁöÑÊú™‰æÜÁ†îÁ©∂ÊñπÂêë„ÄÇÁî±ÊñºÂøÉÁêÜÂÅ•Â∫∑Ê±∫Á≠ñÈúÄË¶ÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈÅìÂæ∑ËÄÉÈáèÔºåÊú¨ÊñáÊúâÂä©ÊñºÊé®ÈÄ≤ÂøÉÁêÜ‰øùÂÅ•‰∏≠ÈÄèÈÅéÁ§æ‰∫§Â™íÈ´îÊé®ÈÄ≤ XAI ÁöÑÊåÅÁ∫åË®éË´ñ„ÄÇÈÄôË£°ÊèêÂá∫ÁöÑÂÖ®Èù¢Ê¶ÇËø∞Êó®Âú®ÂºïÂ∞éÁ†îÁ©∂‰∫∫Âì°„ÄÅÂæûÊ•≠‰∫∫Âì°ÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖÁôºÂ±ïÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨È†òÂüü„ÄÇ

##### **Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**
2406.05746v1 by Zhan Zhang, Qin Zhang, Yang Jiao, Lin Lu, Lin Ma, Aihua Liu, Xiao Liu, Juan Zhao, Yajun Xue, Bing Wei, Mingxia Zhang, Ru Gao, Hong Zhao, Jie Lu, Fan Li, Yang Zhang, Yiming Wang, Lei Zhang, Fengwei Tian, Jie Hu, Xin Gou

AI-aided clinical diagnosis is desired in medical care. Existing deep
learning models lack explainability and mainly focus on image analysis. The
recently developed Dynamic Uncertain Causality Graph (DUCG) approach is
causality-driven, explainable, and invariant across different application
scenarios, without problems of data collection, labeling, fitting, privacy,
bias, generalization, high cost and high energy consumption. Through close
collaboration between clinical experts and DUCG technicians, 46 DUCG models
covering 54 chief complaints were constructed. Over 1,000 diseases can be
diagnosed without triage. Before being applied in real-world, the 46 DUCG
models were retrospectively verified by third-party hospitals. The verified
diagnostic precisions were no less than 95%, in which the diagnostic precision
for every disease including uncommon ones was no less than 80%. After
verifications, the 46 DUCG models were applied in the real-world in China. Over
one million real diagnosis cases have been performed, with only 17 incorrect
diagnoses identified. Due to DUCG's transparency, the mistakes causing the
incorrect diagnoses were found and corrected. The diagnostic abilities of the
clinicians who applied DUCG frequently were improved significantly. Following
the introduction to the earlier presented DUCG methodology, the recommendation
algorithm for potential medical checks is presented and the key idea of DUCG is
extracted.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇÁÖßË≠∑‰∏≠ÈúÄË¶Å AI ËºîÂä©ÁöÑËá®Â∫äË®∫Êñ∑„ÄÇÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏î‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂΩ±ÂÉèÂàÜÊûê„ÄÇÊúÄËøëÈñãÁôºÁöÑÂãïÊÖã‰∏çÁ¢∫ÂÆöÂõ†ÊûúÈóú‰øÇÂúñ (DUCG) ÊñπÊ≥ïÊòØÂõ†ÊûúÈ©ÖÂãïÁöÑ„ÄÅÂèØËß£ÈáãÁöÑÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊáâÁî®Â†¥ÊôØ‰∏≠ÊòØ‰∏çËÆäÁöÑÔºåÊ≤íÊúâË≥áÊñôÊî∂ÈõÜ„ÄÅÊ®ôË®ò„ÄÅÊì¨Âêà„ÄÅÈö±ÁßÅ„ÄÅÂÅèË¶ã„ÄÅÊ¶ÇÂåñ„ÄÅÈ´òÊàêÊú¨ÂíåÈ´òËÉΩËÄóÁöÑÂïèÈ°å„ÄÇÈÄöÈÅéËá®Â∫äÂ∞àÂÆ∂Âíå DUCG ÊäÄË°ì‰∫∫Âì°‰πãÈñìÁöÑÂØÜÂàáÂêà‰ΩúÔºåÊßãÂª∫‰∫ÜÊ∂µËìã 54 ÂÄã‰∏ªË®¥ÁöÑ 46 ÂÄã DUCG Ê®°Âûã„ÄÇÂèØ‰ª•Âú®Ê≤íÊúâÂàÜÊµÅÁöÑÊÉÖÊ≥Å‰∏ãË®∫Êñ∑Âá∫ 1,000 Â§öÁ®ÆÁñæÁóÖ„ÄÇÂú®ÊáâÁî®ÊñºÂØ¶Èöõ‰∏ñÁïå‰πãÂâçÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Áî±Á¨¨‰∏âÊñπÈÜ´Èô¢ÂõûÊ∫ØÊÄßÈ©óË≠â„ÄÇÈ©óË≠âÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 95%ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÁΩïË¶ãÁñæÁóÖÂú®ÂÖßÁöÑÊØèÁ®ÆÁñæÁóÖÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 80%„ÄÇÈ©óË≠âÂæåÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Âú®‰∏≠ÂúãÂØ¶ÈöõÊáâÁî®„ÄÇÂ∑≤Á∂ìÂü∑Ë°å‰∫ÜË∂ÖÈÅé‰∏ÄÁôæËê¨ÂÄãÁúüÂØ¶Ë®∫Êñ∑Ê°à‰æãÔºåÂÉÖÁôºÁèæ 17 ÂÄã‰∏çÊ≠£Á¢∫ÁöÑË®∫Êñ∑„ÄÇÁî±Êñº DUCG ÁöÑÈÄèÊòéÊÄßÔºåÁôºÁèæ‰∏¶Á≥æÊ≠£‰∫ÜÂ∞éËá¥‰∏çÊ≠£Á¢∫Ë®∫Êñ∑ÁöÑÈåØË™§„ÄÇÈ†ªÁπÅÊáâÁî® DUCG ÁöÑËá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑ËÉΩÂäõÂæóÂà∞‰∫ÜÈ°ØËëóÊèêÈ´ò„ÄÇÂú®‰ªãÁ¥π‰∫ÜÂâçÈù¢ÊèêÂá∫ÁöÑ DUCG ÊñπÊ≥ïË´ñ‰πãÂæåÔºåÊèêÂá∫‰∫ÜÊΩõÂú®ÂÅ•Â∫∑Ê™¢Êü•ÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÔºå‰∏¶ÊèêÂèñ‰∫Ü DUCG ÁöÑÈóúÈçµÊÄùÊÉ≥„ÄÇ</paragraph>

##### **Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**
2406.12897v1 by Faseela Abdullakutty, Younes Akbari, Somaya Al-Maadeed, Ahmed Bouridane, Rifat Hamoudi

It is imperative that breast cancer is detected precisely and timely to
improve patient outcomes. Diagnostic methodologies have traditionally relied on
unimodal approaches; however, medical data analytics is integrating diverse
data sources beyond conventional imaging. Using multi-modal techniques,
integrating both image and non-image data, marks a transformative advancement
in breast cancer diagnosis. The purpose of this review is to explore the
burgeoning field of multimodal techniques, particularly the fusion of
histopathology images with non-image data. Further, Explainable AI (XAI) will
be used to elucidate the decision-making processes of complex algorithms,
emphasizing the necessity of explainability in diagnostic processes. This
review utilizes multi-modal data and emphasizes explainability to enhance
diagnostic accuracy, clinician confidence, and patient engagement, ultimately
fostering more personalized treatment strategies for breast cancer, while also
identifying research gaps in multi-modality and explainability, guiding future
studies, and contributing to the strategic direction of the field.

ÊëòË¶ÅÔºöÁ≤æÁ¢∫‰∏îÂèäÊôÇÂú∞ÂÅµÊ∏¨‰π≥ÁôåÂ∞çÊñºÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇË®∫Êñ∑ÊñπÊ≥ïÂÇ≥Áµ±‰∏ä‰æùË≥¥ÊñºÂñÆ‰∏ÄÊ®°ÂºèÊñπÊ≥ïÔºõÁÑ∂ËÄåÔºåÈÜ´ÁôÇË≥áÊñôÂàÜÊûêÊ≠£Âú®Êï¥ÂêàË∂ÖË∂äÂÇ≥Áµ±ÂΩ±ÂÉèÁöÑÂêÑÁ®ÆË≥áÊñô‰æÜÊ∫ê„ÄÇ‰ΩøÁî®Êï¥ÂêàÂΩ±ÂÉèÂíåÈùûÂΩ±ÂÉèË≥áÊñôÁöÑÂ§öÊ®°ÂºèÊäÄË°ìÔºåÊ®ôË™åËëó‰π≥ÁôåË®∫Êñ∑ÁöÑËÆäÈù©ÊÄßÈÄ≤Â±ï„ÄÇÊú¨ÁØáÁ∂úËø∞ÁöÑÁõÆÁöÑÊòØÊé¢Ë®éÂ§öÊ®°ÂºèÊäÄË°ìÁöÑÊñ∞ËààÈ†òÂüüÔºåÁâπÂà•ÊòØÂ∞áÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèËàáÈùûÂΩ±ÂÉèË≥áÊñôËûçÂêà„ÄÇÊ≠§Â§ñÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∞áÁî®ÊñºÈó°ÊòéË§áÈõúÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÈÅéÁ®ãÔºåÂº∑Ë™øË®∫Êñ∑ÈÅéÁ®ã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊú¨Á∂úËø∞Âà©Áî®Â§öÊ®°ÂºèË≥áÊñô‰∏¶Âº∑Ë™øÂèØËß£ÈáãÊÄßÔºå‰ª•ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÅËá®Â∫äÈÜ´Â∏´ÁöÑ‰ø°ÂøÉÂíåÊÇ£ËÄÖÂèÉËàáÂ∫¶ÔºåÊúÄÁµÇ‰øÉÈÄ≤‰π≥ÁôåÊõ¥ÂÄã‰∫∫ÂåñÁöÑÊ≤ªÁôÇÁ≠ñÁï•ÔºåÂêåÊôÇ‰πüÊâæÂá∫Â§öÊ®°ÂºèÂíåÂèØËß£ÈáãÊÄßÁöÑÁ†îÁ©∂Â∑ÆË∑ùÔºåÂºïÂ∞éÊú™‰æÜÁöÑÁ†îÁ©∂Ôºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÁ≠ñÁï•ÊñπÂêëÂÅöÂá∫Ë≤¢Áçª„ÄÇ

##### **Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**
2406.04612v1 by Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin

The self-attention mechanism has been adopted in several widely-used
message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls
the amount of information that flows along the edges of the underlying graph.
This usage of attention has made such models a baseline for studies on
explainable AI (XAI) since interpretations via attention have been popularized
in various domains (e.g., natural language processing and computer vision).
However, existing studies often use naive calculations to derive attribution
scores from attention, and do not take the precise and careful calculation of
edge attribution into consideration. In our study, we aim to fill the gap
between the widespread usage of attention-enabled MPNNs and their potential in
largely under-explored explainability, a topic that has been actively
investigated in other areas. To this end, as the first attempt, we formalize
the problem of edge attribution from attention weights in GNNs. Then, we
propose GATT, an edge attribution calculation method built upon the computation
tree. Through comprehensive experiments, we demonstrate the effectiveness of
our proposed method when evaluating attributions from GATs. Conversely, we
empirically validate that simply averaging attention weights over graph
attention layers is insufficient to interpret the GAT model's behavior. Code is
publicly available at https://github.com/jordan7186/GAtt/tree/main.

ÊëòË¶ÅÔºöËá™Ê≥®ÊÑèÂäõÊ©üÂà∂Â∑≤Ë¢´Êé°Áî®ÊñºÂ§öÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MPNN)Ôºà‰æãÂ¶Ç GATÔºâÔºåÂÆÉÂèØ‰ª•Ëá™ÈÅ©ÊáâÂú∞ÊéßÂà∂Ê≤øËëóÂ∫ïÂ±§ÂúñÂΩ¢ÈÇäÁ∑£ÊµÅÂãïÁöÑË≥áË®äÈáè„ÄÇÈÄôÁ®ÆÊ≥®ÊÑèÂäõÁöÑ‰ΩøÁî®‰ΩøÂæóÊ≠§È°ûÊ®°ÂûãÊàêÁÇ∫ÂèØËß£Èáã AI (XAI) Á†îÁ©∂ÁöÑÂü∫Á∑öÔºåÂõ†ÁÇ∫ÈÄèÈÅéÊ≥®ÊÑèÂäõÁöÑË©ÆÈáãÂ∑≤Âú®ÂêÑÁ®ÆÈ†òÂüüÔºà‰æãÂ¶ÇËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫Ôºâ‰∏≠ÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÈÄöÂ∏∏‰ΩøÁî®Â§©ÁúüÁöÑË®àÁÆóÊñπÊ≥ïÂæûÊ≥®ÊÑèÂäõ‰∏≠Êé®Â∞éÂá∫Ê≠∏Âõ†ÂàÜÊï∏Ôºå‰∏¶‰∏îÊ≤íÊúâËÄÉÊÖÆÂà∞ÈÇäÁ∑£Ê≠∏Âõ†ÁöÑÁ≤æÁ¢∫‰∏î‰ªîÁ¥∞ÁöÑË®àÁÆó„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊó®Âú®Â°´Ë£úÊ≥®ÊÑèÂäõÂïüÁî® MPNN ÁöÑÂª£Ê≥õ‰ΩøÁî®ËàáÂÆÉÂÄëÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÂèØËß£ÈáãÊÄß‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄôÂÄã‰∏ªÈ°åÂ∑≤Âú®ÂÖ∂‰ªñÈ†òÂüüÁ©çÊ•µÁ†îÁ©∂„ÄÇÁÇ∫Ê≠§Ôºå‰ΩúÁÇ∫Á¨¨‰∏ÄÊ¨°ÂòóË©¶ÔºåÊàëÂÄëÂ∞á GNN ‰∏≠Ê≥®ÊÑèÂäõÊ¨äÈáçÁöÑÈÇäÁ∑£Ê≠∏Âõ†ÂïèÈ°åÂΩ¢ÂºèÂåñ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫ GATTÔºå‰∏ÄÁ®ÆÂª∫Á´ãÂú®Ë®àÁÆóÊ®π‰∏äÁöÑÈÇäÁ∑£Ê≠∏Âõ†Ë®àÁÆóÊñπÊ≥ï„ÄÇÈÄèÈÅéÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë©ï‰º∞ GAT ÁöÑÊ≠∏Âõ†ÊôÇÊâÄÂÖ∑ÊúâÁöÑÊïàÊûú„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÊÜëÁ∂ìÈ©óÈ©óË≠â‰∫ÜÂÉÖÂ∞çÂúñÊ≥®ÊÑèÂäõÂ±§‰∏äÁöÑÊ≥®ÊÑèÂäõÊ¨äÈáçÂèñÂπ≥ÂùáÂÄº‰∏çË∂≥‰ª•Ë©ÆÈáã GAT Ê®°ÂûãÁöÑË°åÁÇ∫„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jordan7186/GAtt/tree/main„ÄÇ

##### **Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**
2406.16908v2 by Dinuka Sandun Udayantha, Kavindu Weerasinghe, Nima Wickramasinghe, Akila Abeyratne, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Anjula De Silva, Chamira U. S. Edussooriya

The neonatal period is the most vulnerable time for the development of
seizures. Seizures in the immature brain lead to detrimental consequences,
therefore require early diagnosis. The gold-standard for neonatal seizure
detection currently relies on continuous video-EEG monitoring; which involves
recording multi-channel electroencephalogram (EEG) alongside real-time video
monitoring within a neonatal intensive care unit (NICU). However, video-EEG
monitoring technology requires clinical expertise and is often limited to
technologically advanced and resourceful settings. Cost-effective new
techniques could help the medical fraternity make an accurate diagnosis and
advocate treatment without delay. In this work, a novel explainable deep
learning model to automate the neonatal seizure detection process with a
reduced EEG montage is proposed, which employs convolutional nets, graph
attention layers, and fully connected layers. Beyond its ability to detect
seizures in real-time with a reduced montage, this model offers the unique
advantage of real-time interpretability. By evaluating the performance on the
Zenodo dataset with 10-fold cross-validation, the presented model achieves an
absolute improvement of 8.31% and 42.86% in area under curve (AUC) and recall,
respectively.

ÊëòË¶ÅÔºöÊñ∞ÁîüÂÖíÊôÇÊúüÊòØÂ§ßËÖ¶ÁôºËÇ≤ÊúÄËÑÜÂº±ÁöÑÊôÇÊúüÔºåÊúÉÂ∞éËá¥Áô≤ÁôáÁôº‰Ωú„ÄÇÁô≤ÁôáÁôº‰ΩúÊúÉÂ∞çÊú™ÊàêÁÜüÁöÑÂ§ßËÖ¶ÈÄ†Êàê‰∏çËâØÂæåÊûúÔºåÂõ†Ê≠§ÈúÄË¶ÅÊó©ÊúüË®∫Êñ∑„ÄÇÁõÆÂâçÊñ∞ÁîüÂÖíÁô≤ÁôáÊ™¢Ê∏¨ÁöÑÈªÉÈáëÊ®ôÊ∫ñ‰æùË≥¥ÊñºÈÄ£Á∫åË¶ñË®äËÖ¶ÈõªÂúñÁõ£Ê∏¨ÔºõÈÄôÂåÖÊã¨Âú®Êñ∞ÁîüÂÖíÂä†Ë≠∑ÁóÖÊàø (NICU) ÂÖßË®òÈåÑÂ§öÈÄöÈÅìËÖ¶ÈõªÂúñ (EEG) ÂíåÂç≥ÊôÇË¶ñË®äÁõ£Ê∏¨„ÄÇÁÑ∂ËÄåÔºåË¶ñË®äËÖ¶ÈõªÂúñÁõ£Ê∏¨ÊäÄË°ìÈúÄË¶ÅËá®Â∫äÂ∞àÊ•≠Áü•Ë≠òÔºåËÄå‰∏îÈÄöÂ∏∏ÂÉÖÈôêÊñºÊäÄË°ìÂÖàÈÄ≤‰∏îË≥áÊ∫êË±êÂØåÁöÑÁí∞Â¢É„ÄÇÂÖ∑ÊúâÊàêÊú¨ÊïàÁõäÁöÑÊñ∞ÊäÄË°ìÂèØ‰ª•Âπ´Âä©ÈÜ´ÁôÇÁïåÂÅöÂá∫Ê∫ñÁ¢∫ÁöÑË®∫Êñ∑Ôºå‰∏¶Á´ãÂç≥ÊèêÂÄ°Ê≤ªÁôÇ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÂèØËß£ÈáãÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•Ëá™ÂãïÂåñÊñ∞ÁîüÂÖíÁô≤ÁôáÊ™¢Ê∏¨ÈÅéÁ®ãÔºå‰∏¶Ê∏õÂ∞ëËÖ¶ÈõªÂúñË£ùÁΩÆÔºåË©≤Ê®°ÂûãÊé°Áî®Âç∑Á©çÁ∂≤Ë∑Ø„ÄÅÂúñÊ≥®ÊÑèÂäõÂ±§ÂíåÂÖ®ÈÄ£Êé•Â±§„ÄÇÈô§‰∫ÜËÉΩÂ§†Âç≥ÊôÇÂÅµÊ∏¨Ê∏õÂ∞ëË£ùÁΩÆÁöÑÁô≤ÁôáÁôº‰ΩúÂ§ñÔºåÊ≠§Ê®°ÂûãÈÇÑÊèê‰æõÂç≥ÊôÇÂèØËß£ÈáãÊÄßÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÈÄèÈÅéË©ï‰º∞ Zenodo Ë≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºå‰∏¶ÈÄ≤Ë°å 10 ÂÄç‰∫§ÂèâÈ©óË≠âÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú®Êõ≤Á∑ö‰∏ãÈù¢Á©ç (AUC) ÂíåÂè¨ÂõûÁéáÂàÜÂà•ÈÅîÂà∞ 8.31% Âíå 42.86% ÁöÑÁµïÂ∞çÊîπÂñÑ„ÄÇ

##### **Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**
2406.00532v1 by Samita Bai, Sidra Nasir, Rizwan Ahmed Khan, Sheeraz Arif, Alexandre Meyer, Hubert Konik

Breast cancer (BC) stands as one of the most common malignancies affecting
women worldwide, necessitating advancements in diagnostic methodologies for
better clinical outcomes. This article provides a comprehensive exploration of
the application of Explainable Artificial Intelligence (XAI) techniques in the
detection and diagnosis of breast cancer. As Artificial Intelligence (AI)
technologies continue to permeate the healthcare sector, particularly in
oncology, the need for transparent and interpretable models becomes imperative
to enhance clinical decision-making and patient care. This review discusses the
integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and
others, with machine learning and deep learning models utilized in breast
cancer detection and classification. By investigating the modalities of breast
cancer datasets, including mammograms, ultrasounds and their processing with
AI, the paper highlights how XAI can lead to more accurate diagnoses and
personalized treatment plans. It also examines the challenges in implementing
these techniques and the importance of developing standardized metrics for
evaluating XAI's effectiveness in clinical settings. Through detailed analysis
and discussion, this article aims to highlight the potential of XAI in bridging
the gap between complex AI models and practical healthcare applications,
thereby fostering trust and understanding among medical professionals and
improving patient outcomes.

ÊëòË¶ÅÔºö‰π≥Áôå (BC) ÊòØÂΩ±ÈüøÂÖ®ÁêÉÂ•≥ÊÄßÊúÄÂ∏∏Ë¶ãÁöÑÊÉ°ÊÄßËÖ´Áò§‰πã‰∏ÄÔºåÂõ†Ê≠§ÈúÄË¶ÅÈÄ≤Ê≠•ÁöÑË®∫Êñ∑ÊñπÊ≥ïÔºå‰ª•ÊîπÂñÑËá®Â∫äÁµêÊûú„ÄÇÊú¨ÊñáÂÖ®Èù¢Êé¢Ë®é‰∫ÜÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ìÂú®‰π≥ÁôåÂÅµÊ∏¨ÂíåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊåÅÁ∫åÊª≤ÈÄèÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÁâπÂà•ÊòØÂú®ËÖ´Áò§Â≠∏‰∏≠ÔºåÈÄèÊòé‰∏îÂèØËß£ÈáãÁöÑÊ®°ÂûãÈúÄÊ±ÇËÆäÂæóÂã¢Âú®ÂøÖË°åÔºå‰ª•Â¢ûÂº∑Ëá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂíåÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§ÁØáË©ïË´ñÊé¢Ë®é‰∫ÜÂêÑÁ®Æ XAI ÊñπÊ≥ïÁöÑÊï¥ÂêàÔºå‰æãÂ¶Ç SHAP„ÄÅLIME„ÄÅGrad-CAM Á≠âÔºå‰ª•ÂèäÁî®Êñº‰π≥ÁôåÂÅµÊ∏¨ÂíåÂàÜÈ°ûÁöÑÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄèÈÅéÊé¢Ë®é‰π≥ÁôåË≥áÊñôÈõÜÁöÑÊ®°ÂºèÔºåÂåÖÊã¨‰π≥ÊàøÊîùÂΩ±„ÄÅË∂ÖÈü≥Ê≥¢ÂèäÂÖ∂Âú® AI ‰∏≠ÁöÑËôïÁêÜÔºåÊú¨ÊñáÈáçÈªûË™™Êòé XAI Â¶Ç‰ΩïËÉΩÂ∞éËá¥Êõ¥Ê∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇË®àÁï´„ÄÇÂÆÉ‰πüÊé¢Ë®é‰∫ÜÂØ¶ÊñΩÈÄô‰∫õÊäÄË°ìÁöÑÊåëÊà∞Ôºå‰ª•ÂèäÂà∂ÂÆöÊ®ôÊ∫ñÂåñË©ïÈáèÊåáÊ®ô‰ª•Ë©ï‰º∞ XAI Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄèÈÅéË©≥Á¥∞ÁöÑÂàÜÊûêÂíåË®éË´ñÔºåÊú¨ÊñáÊó®Âú®Âº∑Ë™ø XAI Âú®Á∏ÆÂ∞èË§áÈõú AI Ê®°ÂûãËàáÂØ¶ÂãôÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰πãÈñìÂ∑ÆË∑ùÁöÑÊΩõÂäõÔºåÈÄ≤ËÄå‰øÉÈÄ≤ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°‰πãÈñìÁöÑ‰ø°‰ªªËàáÁêÜËß£Ôºå‰∏¶ÊîπÂñÑÊÇ£ËÄÖÁöÑÁµêÊûú„ÄÇ

##### **Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**
2406.01624v2 by Alaa Nfissi, Wassim Bouachir, Nizar Bouguila, Brian Mishara

Speech emotion recognition (SER) has gained significant attention due to its
several application fields, such as mental health, education, and
human-computer interaction. However, the accuracy of SER systems is hindered by
high-dimensional feature sets that may contain irrelevant and redundant
information. To overcome this challenge, this study proposes an iterative
feature boosting approach for SER that emphasizes feature relevance and
explainability to enhance machine learning model performance. Our approach
involves meticulous feature selection and analysis to build efficient SER
systems. In addressing our main problem through model explainability, we employ
a feature evaluation loop with Shapley values to iteratively refine feature
sets. This process strikes a balance between model performance and
transparency, which enables a comprehensive understanding of the model's
predictions. The proposed approach offers several advantages, including the
identification and removal of irrelevant and redundant features, leading to a
more effective model. Additionally, it promotes explainability, facilitating
comprehension of the model's predictions and the identification of crucial
features for emotion determination. The effectiveness of the proposed method is
validated on the SER benchmarks of the Toronto emotional speech set (TESS),
Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of
Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion
(SAVEE) datasets, outperforming state-of-the-art methods. To the best of our
knowledge, this is the first work to incorporate model explainability into an
SER framework. The source code of this paper is publicly available via this
https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.

ÊëòË¶ÅÔºöË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) Áî±ÊñºÂÖ∂Âú®ÂøÉÁêÜÂÅ•Â∫∑„ÄÅÊïôËÇ≤Âíå‰∫∫Ê©ü‰∫íÂãïÁ≠âÂ§öÂÄãÊáâÁî®È†òÂüüËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåSER Á≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂèóÂà∞È´òÁ∂≠ÁâπÂæµÈõÜÁöÑÈòªÁ§ôÔºåÈÄô‰∫õÁâπÂæµÈõÜÂèØËÉΩÂåÖÂê´‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑË≥áË®ä„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÊåëÊà∞ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁî®Êñº SER ÁöÑËø≠‰ª£ÁâπÂæµÊèêÂçáÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂº∑Ë™øÁâπÂæµÁõ∏ÈóúÊÄßÂíåÂèØËß£ÈáãÊÄßÔºå‰ª•Â¢ûÂº∑Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰ªîÁ¥∞ÁöÑÁâπÂæµÈÅ∏ÊìáÂíåÂàÜÊûêÔºå‰ª•Âª∫Á´ãÈ´òÊïàÁöÑ SER Á≥ªÁµ±„ÄÇÁÇ∫‰∫ÜÈÄèÈÅéÊ®°ÂûãÂèØËß£ÈáãÊÄßËß£Ê±∫ÊàëÂÄëÁöÑÊ†∏ÂøÉÂïèÈ°åÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂÖ∑Êúâ Shapley ÂÄºÁöÑÁâπÂæµË©ï‰º∞Ëø¥ÂúàÔºå‰ª•ÂèçË¶ÜÊîπÂñÑÁâπÂæµÈõÜ„ÄÇÈÄôÂÄãÈÅéÁ®ãÂú®Ê®°ÂûãÊïàËÉΩÂíåÈÄèÊòéÂ∫¶‰πãÈñìÂèñÂæóÂπ≥Ë°°ÔºåÈÄô‰ΩøÂæóÊàëÂÄëËÉΩÂ§†ÂÖ®Èù¢‰∫ÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂ§öÈ†ÖÂÑ™ÈªûÔºåÂåÖÊã¨Ë≠òÂà•ÂíåÁßªÈô§‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑÁâπÂæµÔºåÂæûËÄåÂª∫Á´ãÊõ¥ÊúâÊïàÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰øÉÈÄ≤‰∫ÜÂèØËß£ÈáãÊÄßÔºåÊúâÂä©ÊñºÁêÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨‰ª•ÂèäË≠òÂà•ÊÉÖÁ∑íÊ±∫ÂÆöÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤Âú®Â§öÂÄ´Â§öÊÉÖÁ∑íË™ûÈü≥ÈõÜ (TESS)„ÄÅÊüèÊûóÊÉÖÁ∑íË™ûÈü≥Ë≥áÊñôÂ∫´ (EMO-DB)„ÄÅË≥¥ÁàæÊ£ÆÈü≥Ë®äË¶ñË¶∫ÊÉÖÁ∑íË™ûÈü≥ÂíåÊ≠åÊõ≤Ë≥áÊñôÂ∫´ (RAVDESS) ÂíåËñ©ÈáåÈü≥Ë®äË¶ñË¶∫Ë°®ÈÅîÊÉÖÁ∑í (SAVEE) Ë≥áÊñôÈõÜÁöÑ SER Âü∫Ê∫ñ‰∏äÂæóÂà∞È©óË≠âÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÊ®°ÂûãÂèØËß£ÈáãÊÄßÁ¥çÂÖ• SER Êû∂ÊßãÁöÑÁ†îÁ©∂„ÄÇÊú¨ÊñáÁöÑÂéüÂßãÁ¢ºÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂÖ¨ÈñãÂèñÂæóÔºöhttps://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition„ÄÇ

##### **The Explanation Necessity for Healthcare AI**
2406.00216v1 by Michail Mamalakis, H√©lo√Øse de Vareilles, Graham Murray, Pietro Lio, John Suckling

Explainability is often critical to the acceptable implementation of
artificial intelligence (AI). Nowhere is this more important than healthcare
where decision-making directly impacts patients and trust in AI systems is
essential. This trust is often built on the explanations and interpretations
the AI provides. Despite significant advancements in AI interpretability, there
remains the need for clear guidelines on when and to what extent explanations
are necessary in the medical context. We propose a novel categorization system
with four distinct classes of explanation necessity, guiding the level of
explanation required: patient or sample (local) level, cohort or dataset
(global) level, or both levels. We introduce a mathematical formulation that
distinguishes these categories and offers a practical framework for researchers
to determine the necessity and depth of explanations required in medical AI
applications. Three key factors are considered: the robustness of the
evaluation protocol, the variability of expert observations, and the
representation dimensionality of the application. In this perspective, we
address the question: When does an AI medical application need to be explained,
and at what level of detail?

ÊëòË¶ÅÔºöÂèØËß£ÈáäÊÄßÈÄöÂ∏∏ÂØπ‰∫é‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØÊé•ÂèóÂÆûÊñΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÔºåËøô‰∏ÄÁÇπÂ∞§‰∏∫ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÜ≥Á≠ñÁõ¥Êé•ÂΩ±ÂìçÊÇ£ËÄÖÔºåÂπ∂‰∏îÂØπ AI Á≥ªÁªüÁöÑ‰ø°‰ªªËá≥ÂÖ≥ÈáçË¶Å„ÄÇËøôÁßç‰ø°‰ªªÈÄöÂ∏∏Âª∫Á´ãÂú® AI Êèê‰æõÁöÑËß£ÈáäÂíåËØ†Èáä‰πã‰∏ä„ÄÇÂ∞ΩÁÆ° AI ÂèØËß£ÈáäÊÄßÂèñÂæó‰∫ÜÈáçÂ§ßËøõÂ±ïÔºå‰ΩÜ‰ªçÁÑ∂ÈúÄË¶ÅÊòéÁ°ÆÁöÑÊåáÂØºÊñπÈíàÔºåËØ¥ÊòéÂú®ÂåªÁñóÁéØÂ¢É‰∏≠‰ΩïÊó∂‰ª•ÂèäÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÈúÄË¶ÅËß£Èáä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂàÜÁ±ªÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÂÖ∑ÊúâÂõõÁßç‰∏çÂêåÁöÑËß£ÈáäÂøÖË¶ÅÊÄßÁ±ªÂà´ÔºåÊåáÂØºÊâÄÈúÄÁöÑËß£ÈáäÁ∫ßÂà´ÔºöÊÇ£ËÄÖÊàñÊ†∑Êú¨ÔºàÂ±ÄÈÉ®ÔºâÁ∫ßÂà´„ÄÅÈòüÂàóÊàñÊï∞ÊçÆÈõÜÔºàÂÖ®Â±ÄÔºâÁ∫ßÂà´ÔºåÊàñ‰∏§‰∏™Á∫ßÂà´„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êï∞Â≠¶ÂÖ¨ÂºèÔºåËØ•ÂÖ¨ÂºèÂå∫ÂàÜ‰∫ÜËøô‰∫õÁ±ªÂà´ÔºåÂπ∂‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÆûÁî®Ê°ÜÊû∂Ôºå‰ª•Á°ÆÂÆöÂåªÁñó AI Â∫îÁî®‰∏≠ÊâÄÈúÄÁöÑËß£ÈáäÁöÑÂøÖË¶ÅÊÄßÂíåÊ∑±Â∫¶„ÄÇËÄÉËôë‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöËØÑ‰º∞ÂçèËÆÆÁöÑÁ®≥ÂÅ•ÊÄß„ÄÅ‰∏ìÂÆ∂ËßÇÂØüÁöÑÂèØÂèòÊÄß‰ª•ÂèäÂ∫îÁî®Á®ãÂ∫èÁöÑË°®Á§∫Áª¥Êï∞„ÄÇ‰ªéËøô‰∏™ËßíÂ∫¶Êù•ÁúãÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºöAI ÂåªÁñóÂ∫îÁî®‰ΩïÊó∂ÈúÄË¶ÅËß£ÈáäÔºå‰ª•ÂèäÈúÄË¶ÅËß£ÈáäÂà∞‰ΩïÁßçÁ®ãÂ∫¶Ôºü

##### **Interdisciplinary Expertise to Advance Equitable Explainable AI**
2406.18563v1 by Chloe R. Bennett, Heather Cole-Lewis, Stephanie Farquhar, Naama Haamel, Boris Babenko, Oran Lang, Mat Fleck, Ilana Traynis, Charles Lau, Ivor Horn, Courtney Lyles

The field of artificial intelligence (AI) is rapidly influencing health and
healthcare, but bias and poor performance persists for populations who face
widespread structural oppression. Previous work has clearly outlined the need
for more rigorous attention to data representativeness and model performance to
advance equity and reduce bias. However, there is an opportunity to also
improve the explainability of AI by leveraging best practices of social
epidemiology and health equity to help us develop hypotheses for associations
found. In this paper, we focus on explainable AI (XAI) and describe a framework
for interdisciplinary expert panel review to discuss and critically assess AI
model explanations from multiple perspectives and identify areas of bias and
directions for future research. We emphasize the importance of the
interdisciplinary expert panel to produce more accurate, equitable
interpretations which are historically and contextually informed.
Interdisciplinary panel discussions can help reduce bias, identify potential
confounders, and identify opportunities for additional research where there are
gaps in the literature. In turn, these insights can suggest opportunities for
AI model improvement.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) È†òÂüüÊ≠£Âø´ÈÄüÂΩ±ÈüøËëóÂÅ•Â∫∑ËàáÈÜ´ÁôÇ‰øùÂÅ•Ôºå‰ΩÜÂ∞çÊñºÈù¢Ëá®Âª£Ê≥õÁµêÊßãÊÄßÂ£ìËø´ÁöÑ‰∫∫Áæ§‰æÜË™™ÔºåÂÅèË¶ãÂíå‰∏çËâØË°®Áèæ‰æùÁÑ∂Â≠òÂú®„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ê∏ÖÊ•öË™™ÊòéÔºåÈúÄË¶ÅÊõ¥Âö¥Ê†ºÂú∞Ê≥®ÊÑèË≥áÊñô‰ª£Ë°®ÊÄßÂíåÊ®°ÂûãÊïàËÉΩÔºå‰ª•‰øÉÈÄ≤ÂÖ¨Âπ≥ÊÄß‰∏¶Ê∏õÂ∞ëÂÅèË¶ã„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÊúâÊ©üÊúÉÈÄèÈÅéÈÅãÁî®Á§æÊúÉÊµÅË°åÁóÖÂ≠∏ÂíåÂÅ•Â∫∑ÂÖ¨Âπ≥ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÔºå‰æÜÊîπÂñÑ AI ÁöÑÂèØËß£ÈáãÊÄßÔºå‰ª•Âπ´Âä©ÊàëÂÄëÈáùÂ∞çÁôºÁèæÁöÑÈóúËÅØÊÄßÔºåÁôºÂ±ïÂÅáË®≠„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂèØËß£Èáã AI (XAI)Ôºå‰∏¶ÊèèËø∞‰∏ÄÂÄãË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂØ©Êü•Êû∂ÊßãÔºå‰ª•ÂæûÂ§öÈáçËßÄÈªûË®éË´ñÂíåÊâπÂà§ÊÄßË©ï‰º∞ AI Ê®°ÂûãÁöÑËß£ÈáãÔºå‰∏¶ÊâæÂá∫ÂÅèË¶ãÈ†òÂüüÂíåÊú™‰æÜÁ†îÁ©∂ÁöÑÊñπÂêë„ÄÇÊàëÂÄëÂº∑Ë™øË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂ∞çÊñºÁî¢ÁîüÊõ¥Ê∫ñÁ¢∫„ÄÅÂÖ¨Âπ≥ÁöÑË©ÆÈáãËá≥ÈóúÈáçË¶ÅÔºåËÄåÈÄô‰∫õË©ÆÈáãÊòØÊ†πÊìöÊ≠∑Âè≤ÂíåËÑàÁµ°ËÄå‰æÜÁöÑ„ÄÇË∑®È†òÂüüÂ∞èÁµÑË®éË´ñÊúâÂä©ÊñºÊ∏õÂ∞ëÂÅèË¶ã„ÄÅÊâæÂá∫ÊΩõÂú®ÁöÑÊ∑∑Ê∑ÜÂõ†Á¥†Ôºå‰∏¶Âú®ÊñáÁçª‰∏≠ÊúâÁº∫Âè£ÊôÇÊâæÂá∫È°çÂ§ñÁ†îÁ©∂ÁöÑÊ©üÊúÉ„ÄÇÂèçÈÅé‰æÜÔºåÈÄô‰∫õË¶ãËß£ÂèØ‰ª•Âª∫Ë≠∞ AI Ê®°ÂûãÊîπÈÄ≤ÁöÑÊ©üÊúÉ„ÄÇ

##### **"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**
2407.11978v1 by Hubert D. ZajƒÖc, Jorge M. N. Ribeiro, Silvia Ingala, Simona Gentile, Ruth Wanjohi, Samuel N. Gitau, Jonathan F. Carlsen, Michael B. Nielsen, Tariq O. Andersen

Artificial Intelligence (AI) repeatedly match or outperform radiologists in
lab experiments. However, real-world implementations of radiological AI-based
systems are found to provide little to no clinical value. This paper explores
how to design AI for clinical usefulness in different contexts. We conducted 19
design sessions and design interventions with 13 radiologists from 7 clinical
sites in Denmark and Kenya, based on three iterations of a functional AI-based
prototype. Ten sociotechnical dependencies were identified as crucial for the
design of AI in radiology. We conceptualised four technical dimensions that
must be configured to the intended clinical context of use: AI functionality,
AI medical focus, AI decision threshold, and AI Explainability. We present four
design recommendations on how to address dependencies pertaining to the medical
knowledge, clinic type, user expertise level, patient context, and user
situation that condition the configuration of these technical dimensions.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂú®ÂØ¶È©óÂÆ§ÂØ¶È©ó‰∏≠‰∏çÊñ∑Âú∞ËàáÊîæÂ∞ÑÁßëÈÜ´Â∏´ÂåπÊïµÊàñË°®ÁèæÂæóÊõ¥Âá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÊîæÂ∞ÑÁßë AI ÁÇ∫Âü∫Á§éÁ≥ªÁµ±ÁöÑÂØ¶ÈöõÂü∑Ë°åÂπæ‰πéÊ≤íÊúâÊèê‰æõËá®Â∫äÂÉπÂÄº„ÄÇÊú¨ÊñáÊé¢Ë®éÂ¶Ç‰ΩïÁÇ∫ AI Ë®≠Ë®àÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠Ëá®Â∫ä‰∏äÁöÑÊïàÁî®„ÄÇÊàëÂÄëÊ†πÊìöÂäüËÉΩÊÄß AI ÁÇ∫Âü∫Á§éÂéüÂûãÁöÑ‰∏âÊ¨°Ëø≠‰ª£ÔºåÂú®‰∏πÈ∫•ÂíåËÇØ‰∫ûÁöÑ 7 ÂÄãËá®Â∫äÂ†¥ÂüüËàá 13 ‰ΩçÊîæÂ∞ÑÁßëÈÜ´Â∏´ÈÄ≤Ë°å‰∫Ü 19 Ê¨°Ë®≠Ë®àÊúÉË≠∞ÂíåË®≠Ë®à‰ªãÂÖ•„ÄÇÂçÅÂÄãÁ§æÊúÉÊäÄË°ì‰æùË≥¥Èóú‰øÇË¢´Ë™çÁÇ∫Â∞çÊñºÊîæÂ∞ÑÁßë‰∏≠ AI ÁöÑË®≠Ë®àËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊ¶ÇÂøµÂåñ‰∫ÜÂõõÂÄãÊäÄË°ìÈù¢ÂêëÔºåÂøÖÈ†àÊ†πÊìöÈ†êÊúüÁöÑËá®Â∫ä‰ΩøÁî®ÊÉÖÂ¢ÉÈÄ≤Ë°åË®≠ÂÆöÔºöAI ÂäüËÉΩ„ÄÅAI ÈÜ´ÁôÇÈáçÈªû„ÄÅAI Ê±∫Á≠ñÈñÄÊ™ªÔºå‰ª•Âèä AI ÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ÂõõÈ†ÖË®≠Ë®àÂª∫Ë≠∞ÔºåË™™ÊòéÂ¶Ç‰ΩïËôïÁêÜËàáÈÜ´ÁôÇÁü•Ë≠ò„ÄÅË®∫ÊâÄÈ°ûÂûã„ÄÅ‰ΩøÁî®ËÄÖÂ∞àÊ•≠Áü•Ë≠òÁ≠âÁ¥ö„ÄÅÊÇ£ËÄÖÊÉÖÂ¢ÉÔºå‰ª•ÂèäÂΩ±ÈüøÈÄô‰∫õÊäÄË°ìÈù¢ÂêëË®≠ÂÆöÁöÑ‰ΩøÁî®ËÄÖÊÉÖÂ¢ÉÁõ∏ÈóúÁöÑ‰æùË≥¥Èóú‰øÇ„ÄÇ

##### **Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**
2405.16424v1 by Min Hun Lee, Silvana Xin Yi Choo, Shamala D/O Thilarajah

With advanced AI/ML, there has been growing research on explainable AI (XAI)
and studies on how humans interact with AI and XAI for effective human-AI
collaborative decision-making. However, we still have a lack of understanding
of how AI systems and XAI should be first presented to users without technical
backgrounds. In this paper, we present the findings of semi-structured
interviews with health professionals (n=12) and students (n=4) majoring in
medicine and health to study how to improve onboarding with AI and XAI. For the
interviews, we built upon human-AI interaction guidelines to create onboarding
materials of an AI system for stroke rehabilitation assessment and AI
explanations and introduce them to the participants. Our findings reveal that
beyond presenting traditional performance metrics on AI, participants desired
benchmark information, the practical benefits of AI, and interaction trials to
better contextualize AI performance, and refine the objectives and performance
of AI. Based on these findings, we highlight directions for improving
onboarding with AI and XAI and human-AI collaborative decision-making.

ÊëòË¶ÅÔºöÈö®ËëóÂÖàÈÄ≤ÁöÑ AI/MLÔºåÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂‰∏çÊñ∑Â¢ûÂä†Ôºå‰ª•ÂèäÈóúÊñº‰∫∫È°ûÂ¶Ç‰ΩïËàá AI Âíå XAI ‰∫íÂãï‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰ªçÁÑ∂Áº∫‰πèÂ∞ç AI Á≥ªÁµ±Âíå XAI ÊáâÂ¶Ç‰ΩïÈ¶ñÂÖàÂëàÁèæÁµ¶Ê≤íÊúâÊäÄË°ìËÉåÊôØÁöÑÁî®Êà∂ÁöÑ‰∫ÜËß£„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì° (n=12) Âíå‰∏ª‰øÆÈÜ´Â≠∏ÂíåÂÅ•Â∫∑ÁöÑÂ≠∏Áîü (n=4) ÈÄ≤Ë°åÂçäÁµêÊßãÂåñË®™Ë´áÁöÑÁµêÊûúÔºå‰ª•Á†îÁ©∂Â¶Ç‰ΩïÊîπÂñÑ AI Âíå XAI ÁöÑÂÖ•ÈñÄ„ÄÇÂ∞çÊñºË®™Ë´áÔºåÊàëÂÄëÂª∫Á´ãÂú®‰∫∫Ê©ü‰∫íÂãïÊ∫ñÂâá‰πã‰∏äÔºåÁÇ∫‰∏≠È¢®Â∫∑Âæ©Ë©ï‰º∞Âíå AI Ëß£ÈáãÁöÑ AI Á≥ªÁµ±ÂâµÂª∫ÂÖ•ÈñÄÊùêÊñôÔºå‰∏¶Â∞áÂÆÉÂÄë‰ªãÁ¥πÁµ¶ÂèÉËàáËÄÖ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈô§‰∫ÜÂëàÁèæÂÇ≥Áµ±ÁöÑ AI ÊÄßËÉΩÊåáÊ®ôÂ§ñÔºåÂèÉËàáËÄÖÈÇÑÂ∏åÊúõÂü∫ÂáÜ‰ø°ÊÅØ„ÄÅAI ÁöÑÂØ¶ÈöõÂ•ΩËôï‰ª•Âèä‰∫§‰∫íË©¶È©óÔºå‰ª•Êõ¥Â•ΩÂú∞Â∞á AI ÊÄßËÉΩÊÉÖÂ¢ÉÂåñÔºå‰∏¶ÂÆåÂñÑ AI ÁöÑÁõÆÊ®ôÂíåÊÄßËÉΩ„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ AI Âíå XAI ‰ª•Âèä‰∫∫Ê©üÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆöÁöÑÂÖ•ÈñÄÊñπÂêë„ÄÇ

##### **Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**
2405.17502v1 by Ziming Liu, Longjian Liu, Robert E. Heidel, Xiaopeng Zhao

This article uses machine learning (ML) and explainable artificial
intelligence (XAI) techniques to investigate the relationship between
nutritional status and mortality rates associated with Alzheimers disease (AD).
The Third National Health and Nutrition Examination Survey (NHANES III)
database is employed for analysis. The random forest model is selected as the
base model for XAI analysis, and the Shapley Additive Explanations (SHAP)
method is used to assess feature importance. The results highlight significant
nutritional factors such as serum vitamin B12 and glycated hemoglobin. The
study demonstrates the effectiveness of random forests in predicting AD
mortality compared to other diseases. This research provides insights into the
impact of nutrition on AD and contributes to a deeper understanding of disease
progression.

ÊëòË¶ÅÔºöÊú¨Êñá‰ΩøÁî®Ê©üÂô®Â≠∏Áøí (ML) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ì‰æÜÊé¢Ë®éÁáüÈ§äÁãÄÊ≥ÅËàáÈòøËå≤Êµ∑ÈªòÁóá (AD) Áõ∏ÈóúÁöÑÊ≠ª‰∫°Áéá‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊé°Áî®Á¨¨‰∏âÊ¨°ÂÖ®ÂúãÂÅ•Â∫∑ËàáÁáüÈ§äÊ™¢Êü•Ë™øÊü• (NHANES III) Ë≥áÊñôÂ∫´ÈÄ≤Ë°åÂàÜÊûê„ÄÇÈÅ∏ÊìáÈö®Ê©üÊ£ÆÊûóÊ®°Âûã‰ΩúÁÇ∫ XAI ÂàÜÊûêÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰∏¶‰ΩøÁî® Shapley Additive Explanations (SHAP) ÊñπÊ≥ï‰æÜË©ï‰º∞ÁâπÂæµÈáçË¶ÅÊÄß„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÈáçË¶ÅÁöÑÁáüÈ§äÂõ†Á¥†Ôºå‰æãÂ¶ÇË°ÄÊ∏ÖÁ∂≠ÁîüÁ¥† B12 ÂíåÁ≥ñÂåñË°ÄÁ¥ÖËõãÁôΩ„ÄÇË©≤Á†îÁ©∂Ë≠âÊòé‰∫ÜÈö®Ê©üÊ£ÆÊûóÂú®È†êÊ∏¨ AD Ê≠ª‰∫°ÁéáÊñπÈù¢Áõ∏ËºÉÊñºÂÖ∂‰ªñÁñæÁóÖÁöÑÊúâÊïàÊÄß„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÁáüÈ§äÂ∞ç AD ÁöÑÂΩ±ÈüøÁöÑË¶ãËß£Ôºå‰∏¶ÊúâÂä©ÊñºÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÁñæÁóÖÁöÑÈÄ≤Â±ï„ÄÇ

##### **Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**
2407.11974v1 by Catalina Gomez, Ruolin Wang, Katharina Breininger, Corinne Casey, Chris Bradley, Mitchell Pavlak, Alex Pham, Jithin Yohannan, Mathias Unberath

Primary care providers are vital for initial triage and referrals to
specialty care. In glaucoma, asymptomatic and fast progression can lead to
vision loss, necessitating timely referrals to specialists. However, primary
eye care providers may not identify urgent cases, potentially delaying care.
Artificial Intelligence (AI) offering explanations could enhance their referral
decisions. We investigate how various AI explanations help providers
distinguish between patients needing immediate or non-urgent specialist
referrals. We built explainable AI algorithms to predict glaucoma surgery needs
from routine eyecare data as a proxy for identifying high-risk patients. We
incorporated intrinsic and post-hoc explainability and conducted an online
study with optometrists to assess human-AI team performance, measuring referral
accuracy and analyzing interactions with AI, including agreement rates, task
time, and user experience perceptions. AI support enhanced referral accuracy
among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams
underperformed compared to AI alone. Participants believed they included AI
advice more when using the intrinsic model, and perceived it more useful and
promising. Without explanations, deviations from AI recommendations increased.
AI support did not increase workload, confidence, and trust, but reduced
challenges. On a separate test set, our black-box and intrinsic models achieved
an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We
identify opportunities of human-AI teaming for glaucoma management in primary
eye care, noting that while AI enhances referral accuracy, it also shows a
performance gap compared to AI alone, even with explanations. Human involvement
remains essential in medical decision making, underscoring the need for future
research to optimize collaboration, ensuring positive experiences and safe AI
use.

ÊëòË¶ÅÔºö<paragraph>ÂàùÁ¥ö‰øùÂÅ•Êèê‰æõËÄÖÂ∞çÊñºÊúÄÂàùÁöÑÂàÜÊµÅÂíåËΩâË®∫Âà∞Â∞àÁßëÁÖßË≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈùíÂÖâÁúºÁöÑÊÉÖÊ≥Å‰∏ãÔºåÁÑ°ÁóáÁãÄ‰∏îÂø´ÈÄüÊÉ°ÂåñÂèØËÉΩÂ∞éËá¥Ë¶ñÂäõÂñ™Â§±ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂèäÊôÇËΩâË®∫Áµ¶Â∞àÂÆ∂„ÄÇÁÑ∂ËÄåÔºåÂàùÁ¥öÁúºÁßë‰øùÂÅ•Êèê‰æõËÄÖÂèØËÉΩÁÑ°Ê≥ïË≠òÂà•Á∑äÊÄ•ÊÉÖÊ≥ÅÔºåÂèØËÉΩÊúÉÂª∂Ë™§ÁÖßË≠∑„ÄÇÊèê‰æõËß£ÈáãÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÂèØ‰ª•Âä†Âº∑‰ªñÂÄëÁöÑËΩâË®∫Ê±∫Á≠ñ„ÄÇÊàëÂÄëÁ†îÁ©∂ÂêÑÁ®Æ AI Ëß£ÈáãÂ¶Ç‰ΩïÂπ´Âä©Êèê‰æõËÄÖÂçÄÂàÜÈúÄË¶ÅÁ´ãÂç≥ÊàñÈùûÁ∑äÊÄ•Â∞àÁßëËΩâË®∫ÁöÑÊÇ£ËÄÖ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫ÜËß£ÈáãÊÄß AI ÊºîÁÆóÊ≥ïÔºå‰ª•Âæû‰æãË°åÁúºÁßëË≠∑ÁêÜË≥áÊñôÈ†êÊ∏¨ÈùíÂÖâÁúºÊâãË°ìÈúÄÊ±ÇÔºå‰ΩúÁÇ∫Ë≠òÂà•È´òÈ¢®Èö™ÊÇ£ËÄÖÁöÑ‰ª£ÁêÜ„ÄÇÊàëÂÄëÁ¥çÂÖ•‰∫ÜÂÖßÂú®Âíå‰∫ãÂæåËß£ÈáãÊÄßÔºå‰∏¶ËàáÈ©óÂÖâÂ∏´ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÁ∑ö‰∏äÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞‰∫∫Ê©üÂúòÈöäÁöÑË°®ÁèæÔºåË°°ÈáèËΩâË®∫Ê∫ñÁ¢∫Â∫¶‰∏¶ÂàÜÊûêËàá AI ÁöÑ‰∫íÂãïÔºåÂåÖÊã¨ÂêåÊÑèÁéá„ÄÅ‰ªªÂãôÊôÇÈñìÂíå‰ΩøÁî®ËÄÖÈ´îÈ©óÊÑüÁü•„ÄÇÂú® 87 ÂêçÂèÉËàáËÄÖ‰∏≠ÔºåAI ÊîØÊè¥ÊèêÈ´ò‰∫ÜËΩâË®∫Ê∫ñÁ¢∫Â∫¶Ôºà‰ΩøÁî® AI/Êú™‰ΩøÁî®ÁöÑÊØî‰æãÁÇ∫ 59.9%/50.8%ÔºâÔºåÂÑòÁÆ°‰∫∫Ê©üÂúòÈöäÁöÑË°®Áèæ‰∏çÂ¶ÇÂñÆÁç®‰ΩøÁî® AI„ÄÇÂèÉËàáËÄÖË™çÁÇ∫‰ªñÂÄëÂú®‰ΩøÁî®ÂÖßÂú®Ê®°ÂûãÊôÇÊõ¥Â§öÂú∞Á¥çÂÖ•‰∫Ü AI Âª∫Ë≠∞Ôºå‰∏¶Ë™çÁÇ∫ÂÆÉÊõ¥ÊúâÁî®‰∏îÊõ¥ÊúâÂ∏åÊúõ„ÄÇÊ≤íÊúâËß£ÈáãÔºåAI Âª∫Ë≠∞ÁöÑÂÅèÂ∑ÆÊúÉÂ¢ûÂä†„ÄÇAI ÊîØÊè¥‰∏¶Êú™Â¢ûÂä†Â∑•‰ΩúÈáè„ÄÅ‰ø°ÂøÉÂíå‰ø°‰ªªÔºå‰ΩÜÊ∏õÂ∞ë‰∫ÜÊåëÊà∞„ÄÇÂú®‰∏ÄÂÄãÂñÆÁç®ÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠ÔºåÊàëÂÄëÁöÑÈªëÁõíÂ≠êÂíåÂÖßÂú®Ê®°ÂûãÂú®È†êÊ∏¨ÊâãË°ìÁµêÊûúÊñπÈù¢ÂàÜÂà•ÈÅîÂà∞‰∫Ü 77% Âíå 71% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÊâæÂá∫Âú®ÂàùÁ¥öÁúºÁßë‰øùÂÅ•‰∏≠Ôºå‰∫∫Ê©üÂúòÈöäÂêà‰ΩúÁÆ°ÁêÜÈùíÂÖâÁúºÁöÑÊ©üÊúÉÔºå‰∏¶Ê≥®ÊÑèÂà∞ÈõñÁÑ∂ AI ÊèêÈ´ò‰∫ÜËΩâË®∫Ê∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂç≥‰ΩøÊúâËß£ÈáãÔºåÂÆÉ‰πüÈ°ØÁ§∫Âá∫ËàáÂñÆÁç®‰ΩøÁî® AI Áõ∏ÊØîÁöÑÊïàËÉΩÂ∑ÆË∑ù„ÄÇ‰∫∫È°ûÂèÉËàáÂú®ÈÜ´ÁôÇÊ±∫Á≠ñ‰∏≠‰ªçÁÑ∂Ëá≥ÈóúÈáçË¶ÅÔºåÈÄôÂº∑Ë™ø‰∫ÜÊú™‰æÜÁ†îÁ©∂ÂÑ™ÂåñÂçî‰Ωú„ÄÅÁ¢∫‰øùÊ≠£Èù¢Á∂ìÈ©óÂíåÂÆâÂÖ®‰ΩøÁî® AI ÁöÑÂøÖË¶ÅÊÄß„ÄÇ</paragraph>

##### **Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**
2406.18552v1 by Yingying Fang, Zihao Jin, Xiaodan Xing, Simon Walsh, Guang Yang

In medical imaging, particularly in early disease detection and prognosis
tasks, discerning the rationale behind an AI model's predictions is crucial for
evaluating the reliability of its decisions. Conventional explanation methods
face challenges in identifying discernible decisive features in medical image
classifications, where discriminative features are subtle or not immediately
apparent. To bridge this gap, we propose an explainable model that is equipped
with both decision reasoning and feature identification capabilities. Our
approach not only detects influential image patterns but also uncovers the
decisive features that drive the model's final predictions. By implementing our
method, we can efficiently identify and visualise class-specific features
leveraged by the data-driven model, providing insights into the decision-making
processes of deep learning models. We validated our model in the demanding
realm of medical prognosis task, demonstrating its efficacy and potential in
enhancing the reliability of AI in healthcare and in discovering new knowledge
in diseases where prognostic understanding is limited.

ÊëòË¶ÅÔºöÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÁâπÂà•ÊòØÂú®Êó©ÊúüÁñæÁóÖÊ™¢Ê∏¨ÂíåÈ†êÂæå‰ªªÂãô‰∏≠ÔºåËæ®Âà• AI Ê®°ÂûãÈ†êÊ∏¨ËÉåÂæåÁöÑÂéüÁêÜÂ∞çÊñºË©ï‰º∞ÂÖ∂Ê±∫Á≠ñÁöÑÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑËß£ÈáãÊñπÊ≥ïÂú®Ë≠òÂà•ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰∏≠ÂèØË≠òÂà•ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµÊôÇÈù¢Ëá®ÊåëÊà∞ÔºåÂÖ∂‰∏≠ÂçÄÂà•ÊÄßÁâπÂæµÂæàÂæÆÂ¶ôÊàñ‰∏¶‰∏çÊòéÈ°Ø„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂÖ∑ÂÇôÊ±∫Á≠ñÊé®ÁêÜÂíåÁâπÂæµË≠òÂà•ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÊ™¢Ê∏¨ÊúâÂΩ±ÈüøÂäõÁöÑÂΩ±ÂÉèÊ®°ÂºèÔºåÈÇÑÊè≠Á§∫‰∫ÜÊé®ÂãïÊ®°ÂûãÊúÄÁµÇÈ†êÊ∏¨ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµ„ÄÇÈÄöÈÅéÂØ¶ÊñΩÊàëÂÄëÁöÑÊ®°ÂûãÔºåÊàëÂÄëÂèØ‰ª•ÊúâÊïàË≠òÂà•ÂíåË¶ñË¶∫ÂåñÁî±Êï∏ÊìöÈ©ÖÂãïÊ®°ÂûãÂà©Áî®ÁöÑÈ°ûÁâπÂÆöÁâπÂæµÔºåÂæûËÄåÊ∑±ÂÖ•‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÊàëÂÄëÂú®Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑÈÜ´Â≠∏È†êÂæå‰ªªÂãôÈ†òÂüüÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÊèêÈ´ò AI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÁôºÁèæÈ†êÂæåÁêÜËß£ÂèóÈôêÁñæÁóÖÁöÑÊñ∞Áü•Ë≠òÊñπÈù¢ÁöÑÂäüÊïàÂíåÊΩõÂäõ„ÄÇ

##### **The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**
2405.13099v1 by Mohsen Jozani, Jason A. Williams, Ahmed Aleroud, Sarbottam Bhagat

This study explores the relationship between informational support seeking
questions, responses, and helpfulness ratings in online health communities. We
created a labeled data set of question-response pairs and developed multimodal
machine learning and deep learning models to reliably predict informational
support questions and responses. We employed explainable AI to reveal the
emotions embedded in informational support exchanges, demonstrating the
importance of emotion in providing informational support. This complex
interplay between emotional and informational support has not been previously
researched. The study refines social support theory and lays the groundwork for
the development of user decision aids. Further implications are discussed.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÁ∑ö‰∏äÂÅ•Â∫∑Á§æÁæ§‰∏≠Â∞ãÊ±ÇË≥áË®äÊîØÊåÅÁöÑÂïèÈ°å„ÄÅÂõûÊáâÔºå‰ª•ÂèäÊúâÂπ´Âä©ÁöÑË©ïÂàÜ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÁµÑÊ®ôË®òÁöÑÂïèÁ≠îÈÖçÂ∞çË≥áÊñôÈõÜÔºå‰∏¶ÈñãÁôº‰∫ÜÂ§öÊ®°ÊÖãÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•ÂèØÈù†Âú∞È†êÊ∏¨Ë≥áË®äÊîØÊåÅÂïèÈ°åÂíåÂõûÊáâ„ÄÇÊàëÂÄëÊé°Áî®ÂèØËß£ÈáãÁöÑ AI ‰æÜÊè≠Á§∫Ë≥áË®äÊîØÊåÅ‰∫§ÊµÅ‰∏≠ËòäÂê´ÁöÑÊÉÖÁ∑íÔºåË≠âÊòéÊÉÖÁ∑íÂú®Êèê‰æõË≥áË®äÊîØÊåÅ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÁ®ÆÊÉÖÁ∑íÊîØÊåÅÂíåË≥áË®äÊîØÊåÅ‰πãÈñìÁöÑË§áÈõú‰∫§‰∫í‰ΩúÁî®‰ª•Ââç‰∏¶Êú™Ë¢´Á†îÁ©∂ÈÅé„ÄÇÊú¨Á†îÁ©∂ÊîπÈÄ≤‰∫ÜÁ§æÊúÉÊîØÊåÅÁêÜË´ñÔºå‰∏¶ÁÇ∫‰ΩøÁî®ËÄÖÊ±∫Á≠ñËºîÂä©Â∑•ÂÖ∑ÁöÑÈñãÁôºÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇË®éË´ñ‰∫ÜÈÄ≤‰∏ÄÊ≠•ÁöÑÂΩ±Èüø„ÄÇ

##### **ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**
2405.10645v1 by Harris Bin Munawar, Nikolaos Misirlis

In the era of exponential technology growth, one unexpected guest has claimed
a seat in classrooms worldwide, Artificial Intelligence. Generative AI, such as
ChatGPT, promises a revolution in education, yet it arrives with a double-edged
sword. Its potential for personalized learning is offset by issues of cheating,
inaccuracies, and educators struggling to incorporate it effectively into their
lesson design. We are standing on the brink of this educational frontier, and
it is clear that we need to navigate this terrain with a lot of care. This is a
major challenge that could undermine the integrity and value of our educational
process. So, how can we turn these challenges into opportunities? When used
inappropriately, AI tools can become the perfect tool for the cut copy paste
mentality, and quickly begin to corrode critical thinking, creativity, and deep
understanding, the most important skills in our rapidly changing world.
Teachers feel that they are not equipped to leverage this technology, widening
the digital divide among educators and institutions. Addressing these concerns
calls for an in depth research approach. We will employ empirical research,
drawing on the Technology Acceptance Model, to assess the attitudes toward
generative AI among educators and students. Understanding their perceptions,
usage patterns, and hurdles is the first crucial step in creating an effective
solution. The present study will be used as a process manual for future
researchers to apply, running their own data, based on the steps explained here

ÊëòË¶ÅÔºöÂú®ÁßëÊäÄÈ£õÈÄüÁôºÂ±ïÁöÑÊôÇ‰ª£Ôºå‰∏Ä‰ΩçÊÑèÂ§ñÁöÑË®™ÂÆ¢Â∑≤Âú®ÂÖ®ÁêÉÊïôÂÆ§‰∏≠‰ΩîÊúâ‰∏ÄÂ∏≠‰πãÂú∞ÔºåÈÇ£Â∞±ÊòØ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÁîüÊàêÂºè AIÔºå‰æãÂ¶Ç ChatGPTÔºåÊâøË´æÂú®ÊïôËÇ≤È†òÂüüÊéÄËµ∑‰∏ÄÂ†¥Èù©ÂëΩÔºå‰ΩÜÂÆÉÂçªÊòØ‰∏ÄÊääÈõôÈù¢ÂàÉ„ÄÇÂÆÉÂú®ÂÄã‰∫∫ÂåñÂ≠∏ÁøíÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂçªÂõ†‰ΩúÂºä„ÄÅ‰∏çÊ∫ñÁ¢∫‰ª•ÂèäÊïôËÇ≤Â∑•‰ΩúËÄÖÈõ£‰ª•Â∞áÂÖ∂ÊúâÊïàËûçÂÖ•ÊïôÂ≠∏Ë®≠Ë®àÁ≠âÂïèÈ°åËÄåÊäµÈä∑„ÄÇÊàëÂÄëÊ≠£Á´ôÂú®ÈÄôÊïôËÇ≤ÂâçÊ≤øÁöÑÈÇäÁ∑£ÔºåÈ°ØÁÑ∂ÊàëÂÄëÈúÄË¶ÅÈùûÂ∏∏Â∞èÂøÉÂú∞Êé¢Á¥¢ÈÄôÁâáÈ†òÂüü„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂèØËÉΩÊúÉÊêçÂÆ≥ÊàëÂÄëÊïôËÇ≤ÈÅéÁ®ãÁöÑÂÆåÊï¥ÊÄßÂíåÂÉπÂÄº„ÄÇÈÇ£È∫ºÔºåÊàëÂÄëÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊåëÊà∞ËΩâÂåñÁÇ∫Ê©üÈÅáÔºüÁï∂‰∏çÈÅ©Áï∂Âú∞‰ΩøÁî®ÊôÇÔºåAI Â∑•ÂÖ∑ÂèØËÉΩÊúÉÊàêÁÇ∫Ë§áË£ΩË≤º‰∏äÂøÉÊÖãÁöÑÂÆåÁæéÂ∑•ÂÖ∑Ôºå‰∏¶ËøÖÈÄüËÖêËùïÊâπÂà§ÊÄßÊÄùÁ∂≠„ÄÅÂâµÈÄ†ÂäõÂíåÊ∑±ÂÖ•ÁêÜËß£ÔºåÈÄô‰∫õÈÉΩÊòØÊàëÂÄëÂø´ÈÄüËÆäÂåñÁöÑ‰∏ñÁïå‰∏≠ÊúÄÈáçË¶ÅÁöÑÊäÄËÉΩ„ÄÇÊïôÂ∏´ÂÄëË¶∫Âæó‰ªñÂÄëÊ≤íÊúâËÉΩÂäõÂà©Áî®ÈÄôÈ†ÖÊäÄË°ìÔºåÈÄôÊì¥Â§ß‰∫ÜÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÊ©üÊßã‰πãÈñìÁöÑÊï∏‰ΩçÈ¥ªÊ∫ù„ÄÇËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÈúÄË¶ÅÊ∑±ÂÖ•ÁöÑÁ†îÁ©∂ÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞áÊé°Áî®ÂØ¶Ë≠âÁ†îÁ©∂ÔºåÂÄüÈëëÊäÄË°ìÊé•ÂèóÊ®°ÂûãÔºå‰æÜË©ï‰º∞ÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÂ≠∏ÁîüÂ∞çÁîüÊàêÂºè AI ÁöÑÊÖãÂ∫¶„ÄÇ‰∫ÜËß£‰ªñÂÄëÁöÑÁúãÊ≥ï„ÄÅ‰ΩøÁî®Ê®°ÂºèÂíåÈöúÁ§ôÊòØÂâµÈÄ†ÊúâÊïàËß£Ê±∫ÊñπÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÈóúÈçµÊ≠•È©ü„ÄÇÊú¨Á†îÁ©∂Â∞á‰ΩúÁÇ∫Êú™‰æÜÁ†îÁ©∂‰∫∫Âì°ÊáâÁî®ÁöÑÊµÅÁ®ãÊâãÂÜäÔºåÊ†πÊìöÊ≠§ËôïË™™ÊòéÁöÑÊ≠•È©üÈÅãË°å‰ªñÂÄëËá™Â∑±ÁöÑÊï∏Êìö

##### **Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**
2405.07590v1 by Camelia Oprea, Mike Gr√ºne, Mateusz Buglowski, Lena Olivier, Thorsten Orlikowsky, Stefan Kowalewski, Mark Schoberer, Andr√© Stollenwerk

With the digitalization of health care systems, artificial intelligence
becomes more present in medicine. Especially machine learning shows great
potential for complex tasks such as time series classification, usually at the
cost of transparency and comprehensibility. This leads to a lack of trust by
humans and thus hinders its active usage. Explainable artificial intelligence
tries to close this gap by providing insight into the decision-making process,
the actual usefulness of its different methods is however unclear. This paper
proposes a user study based evaluation of the explanation method Grad-CAM with
application to a neural network for the classification of breaths in time
series neonatal ventilation data. We present the perceived usefulness of the
explainability method by different stakeholders, exposing the difficulty to
achieve actual transparency and the wish for more in-depth explanations by many
of the participants.

ÊëòË¶ÅÔºöÈö®ËëóÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊï∏‰ΩçÂåñÔºå‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´Â≠∏È†òÂüü‰∏≠ËÆäÂæóÊõ¥Âä†ÊôÆÂèä„ÄÇÁâπÂà•ÊòØÊ©üÂô®Â≠∏ÁøíÂú®ÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûÁ≠âË§áÈõú‰ªªÂãô‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÊΩõÂäõÔºå‰ΩÜÈÄöÂ∏∏ÊòØ‰ª•ÈÄèÊòéÂ∫¶ÂíåÂèØÁêÜËß£ÊÄßÁÇ∫‰ª£ÂÉπ„ÄÇÈÄôÂ∞éËá¥‰∫∫È°ûÁº∫‰πè‰ø°‰ªªÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÖ∂Á©çÊ•µ‰ΩøÁî®„ÄÇÂèØËß£ÈáãÁöÑ‰∫∫Â∑•Êô∫ÊÖßË©¶ÂúñÈÄöÈÅéÊèê‰æõÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÁöÑÊ¥ûÂØü‰æÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºå‰ΩÜÂÖ∂‰∏çÂêåÊñπÊ≥ïÁöÑÂØ¶ÈöõÊïàÁî®Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰ΩøÁî®ËÄÖÁ†îÁ©∂ÁöÑË©ï‰º∞ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫Ü Grad-CAM Ëß£ÈáãÊñπÊ≥ïÔºå‰∏¶Â∞áÂÖ∂ÊáâÁî®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰ª•ÂàÜÈ°ûÊôÇÈñìÂ∫èÂàóÊñ∞ÁîüÂÖíÂëºÂê∏Êï∏Êìö‰∏≠ÁöÑÂëºÂê∏„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏çÂêåÂà©ÁõäÁõ∏ÈóúËÄÖÂ∞çÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÊÑüÁü•ÊïàÁî®ÔºåÊè≠Á§∫‰∫ÜÂØ¶ÁèæÂØ¶ÈöõÈÄèÊòéÂ∫¶ÁöÑÈõ£Â∫¶Ôºå‰ª•ÂèäË®±Â§öÂèÉËàáËÄÖÂ∏åÊúõÁç≤ÂæóÊõ¥Ê∑±ÂÖ•ÁöÑËß£Èáã„ÄÇ

##### **XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**
2405.06270v3 by Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics
offers a promising avenue for clinical decision-making. This study outlines the
development of a novel method for zero-shot/few-shot in-context learning (ICL)
by integrating medical domain knowledge using a multi-layered structured
prompt. We also explore the efficacy of two communication styles between the
user and LLMs: the Numerical Conversational (NC) style, which processes data
incrementally, and the Natural Language Single-Turn (NL-ST) style, which
employs long narrative prompts.
  Our study systematically evaluates the diagnostic accuracy and risk factors,
including gender bias and false negative rates, using a dataset of 920 patient
records in various few-shot scenarios. Results indicate that traditional
clinical machine learning (ML) models generally outperform LLMs in zero-shot
and few-shot settings. However, the performance gap narrows significantly when
employing few-shot examples alongside effective explainable AI (XAI) methods as
sources of domain knowledge. Moreover, with sufficient time and an increased
number of examples, the conversational style (NC) nearly matches the
performance of ML models. Most notably, LLMs demonstrate comparable or superior
cost-sensitive accuracy relative to ML models.
  This research confirms that, with appropriate domain knowledge and tailored
communication strategies, LLMs can significantly enhance diagnostic processes.
The findings highlight the importance of optimizing the number of training
examples and communication styles to improve accuracy and reduce biases in LLM
applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÈÜ´ÁôÇË®∫Êñ∑Êï¥Âêà
ÁÇ∫Ëá®Â∫äÊ±∫Á≠ñÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÈÄîÂæë„ÄÇÊú¨Á†îÁ©∂Ê¶ÇËø∞‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÊñπÊ≥ïÁöÑÈñãÁôºÔºåÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí/Â∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢ÉÂ≠∏Áøí (ICL)ÔºåÊñπÊ≥ïÊòØ‰ΩøÁî®Â§öÂ±§ÁµêÊßãÂåñÊèêÁ§∫Êï¥ÂêàÈÜ´ÁôÇÈ†òÂüüÁü•Ë≠ò„ÄÇÊàëÂÄëÈÇÑÊé¢Ë®é‰∫Ü‰ΩøÁî®ËÄÖËàá LLM ‰πãÈñìÂÖ©Á®ÆÊ∫ùÈÄöÊñπÂºèÁöÑÂäüÊïàÔºöÊï∏ÂÄºÂ∞çË©± (NC) ÊñπÂºèÔºåÂÆÉÊúÉÈÄêÊ≠•ËôïÁêÜË≥áÊñôÔºå‰ª•ÂèäËá™ÁÑ∂Ë™ûË®ÄÂñÆÂõûÂêà (NL-ST) ÊñπÂºèÔºåÂÆÉÊúÉ‰ΩøÁî®Èï∑ÁØáÊïò‰∫ãÊèêÁ§∫„ÄÇ
ÊàëÂÄëÁöÑÁ†îÁ©∂Á≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÈ¢®Èö™Âõ†Â≠êÔºåÂåÖÊã¨ÊÄßÂà•ÂÅèË¶ãÂíåÂÅáÈô∞ÊÄßÁéáÔºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄãÂåÖÂê´ 920 ÂÄãÊÇ£ËÄÖË®òÈåÑÁöÑË≥áÊñôÈõÜÔºåÊé°Áî®ÂêÑÁ®ÆÂ∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢É„ÄÇÁµêÊûúË°®ÊòéÔºåÂÇ≥Áµ±ÁöÑËá®Â∫äÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÈÄöÂ∏∏Âú®Èõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÈáèÂ≠∏ÁøíË®≠ÂÆö‰∏≠Ë°®ÁèæÂÑ™Êñº LLM„ÄÇÁÑ∂ËÄåÔºåÁï∂‰ΩøÁî®Â∞ëÈáèÂ≠∏ÁøíÁØÑ‰æã‰ª•ÂèäÊúâÊïàÁöÑÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï‰ΩúÁÇ∫È†òÂüüÁü•Ë≠ò‰æÜÊ∫êÊôÇÔºåÊïàËÉΩÂ∑ÆË∑ùÊúÉÈ°ØËëóÁ∏ÆÂ∞è„ÄÇÊ≠§Â§ñÔºåÈö®ËëóÊôÇÈñìÂÖÖË∂≥ÂíåÁØÑ‰æãÊï∏ÈáèÂ¢ûÂä†ÔºåÂ∞çË©±ÊñπÂºè (NC) Âπæ‰πéÂèØ‰ª•Â™≤Áæé ML Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊúÄÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåLLM Áõ∏Â∞çÊñº ML Ê®°ÂûãÂ±ïÁèæÂá∫Áõ∏Áï∂ÊàñÊõ¥‰Ω≥ÁöÑÊàêÊú¨ÊïèÊÑüÊ∫ñÁ¢∫Â∫¶„ÄÇ
Êú¨Á†îÁ©∂Ë≠âÂØ¶ÔºåÈÄèÈÅéÈÅ©Áï∂ÁöÑÈ†òÂüüÁü•Ë≠òÂíåÈáèË∫´ÊâìÈÄ†ÁöÑÊ∫ùÈÄöÁ≠ñÁï•ÔºåLLM ÂèØ‰ª•È°ØËëóÂ¢ûÂº∑Ë®∫Êñ∑Á®ãÂ∫è„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÊúÄ‰Ω≥ÂåñË®ìÁ∑¥ÁØÑ‰æãÊï∏ÈáèÂíåÊ∫ùÈÄöÊñπÂºèÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÊ∫ñÁ¢∫Â∫¶‰∏¶Ê∏õÂ∞ë LLM ÊáâÁî®‰∏≠ÁöÑÂÅèÂ∑Æ„ÄÇ

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel Mir√≥-Nicolau, Gabriel Moy√†-Alcover, Antoni Jaume-i-Cap√≥, Manuel Gonz√°lez-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

ÊëòË¶ÅÔºöÈö®ËëóÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æùË≥¥ÊÄßÁöÑÂ¢ûÂä†ÔºåÂä†‰∏äÂÖ∂Âõ∫ÊúâÁöÑÈÄèÊòéÂ∫¶‰∏çË∂≥Ôºå‰øÉ‰Ωø‰∏ÄÂÄãÊñ∞ÁöÑÁ†îÁ©∂È†òÂüüÁôºÂ±ïÔºåÁ®±ÁÇ∫ÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÊó®Âú®ÈÄèÈÅéÊ∑±ÂÖ•‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÂéüÁêÜÔºå‰æÜÊèêÂçáÊúÄÁµÇ‰ΩøÁî®ËÄÖÂ∞çËá™ÂãïÂåñÁ≥ªÁµ±ÁöÑ‰ø°Ë≥¥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË°°Èáè‰ΩøÁî®ËÄÖÂ∞ç XAI Á≥ªÁµ±‰ø°Ë≥¥Â∫¶ÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÂÖÅË®±Â∞çÂÖ∂ÈÄ≤Ë°åÊîπÈÄ≤„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊåáÊ®ôÁµêÂêà‰∫ÜÂÆ¢ËßÄËßÄÈªû‰∏ãÁöÑÊïàËÉΩÊåáÊ®ôÂíå‰ø°Ë≥¥ÊåáÊ®ô„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÂú®‰∏ÄÂÄãÁúüÂØ¶ÁöÑÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÈÄ≤Ë°å‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Ôºö‰ΩøÁî® XAI Á≥ªÁµ±Âæû X ÂÖâÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨ËÇ∫ÁÇé„ÄÇ

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

ÊëòË¶ÅÔºöCOVID-19 Áñ´ÊÉÖÂ∞çÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÈÄ†ÊàêÂ£ìÂäõÔºåÂøÖÈ†àÈÄ≤Ë°åÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂπ≤È†êÔºå‰ª•ÊéßÂà∂ÁñæÁóÖÂÇ≥Êí≠‰∏¶Èôç‰ΩéÊ≠ª‰∫°Áéá„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÈÄèÈÅéËÉ∏ÈÉ® X ÂÖâ (CXR) ÂΩ±ÂÉèÊîπÂñÑÂ∞ç COVID-19 È†êÂæåÁöÑÁêÜËß£Âíå‰ø°Ë≥¥„ÄÇÈÄèÈÅéÊï¥ÂêàÂ§ßË¶èÊ®°È†êË®ìÁ∑¥ÂΩ±ÂÉèÁ∑®Á¢ºÂô®„ÄÅÈ¢®Èö™ÁâπÂÆö Grad-CAM ÂíåËß£ÂâñÂçÄÂüüÂÅµÊ∏¨ÊäÄË°ìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁî¢ÁîüÂçÄÂüüÂèØËß£ÈáãÁöÑÁµêÊûúÔºåÊúâÊïàÊçïÊçâÂøÖË¶ÅÁöÑÁñæÁóÖÁâπÂæµÔºåÂêåÊôÇÂ∞àÊ≥®ÊñºÁΩïË¶ã‰ΩÜÈóúÈçµÁöÑÁï∞Â∏∏ÂçÄÂüü„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûúÈÄèÈÅéÈ¢®Èö™ÂçÄÂüüÂÆö‰ΩçÊèê‰æõÂ¢ûÂº∑ÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÈÄèÊòéÂ∫¶ÔºåËÆìËá®Â∫äÈÜ´ÁîüËÉΩÂ§†Âú®Êõ¥‰∫ÜËß£È†êÂæåË¶ãËß£ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞± COVID-19 Ë®∫Êñ∑ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÂú®Â§ö‰∏≠ÂøÉÁîüÂ≠òË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºå‰∏¶ÈÄèÈÅéÈáèÂåñÂíåË≥™ÂåñË©ï‰º∞Ë≠âÊòéÂÖ∂ÊúâÊïàÊÄßÔºåÈÅîÂà∞ÂÑ™Áï∞ÁöÑ C ÊåáÊï∏Ôºà0.764 Âíå 0.727ÔºâÂíåÊôÇÈñìÁõ∏Èóú AUCÔºà0.799 Âíå 0.691Ôºâ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÂú®È¢®Èö™È†êÊ∏¨ÊñπÈù¢Ë∂ÖË∂äÂÇ≥Áµ±ÁöÑÁîüÂ≠òÂàÜÊûêÊñπÊ≥ïÔºåÊèêÂçáËá®Â∫äÊ±∫Á≠ñÁöÑËß£ÈáãÊÄßÔºå‰∏¶Â¢ûÂº∑ AI Á≥ªÁµ±ÁöÑ‰ø°Ë≥¥Â∫¶„ÄÇ

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÅéÂéªÂπæÂπ¥ÔºåËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (CDSS) ‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®Âà©Áî®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÊñπÈù¢ÁôºÊèÆ‰∫ÜÈóúÈçµ‰ΩúÁî®„ÄÇÂÑòÁÆ° AI Ê®°ÂûãÂÖ∑Êúâ‰ª§‰∫∫ÊªøÊÑèÁöÑËÉΩÂäõÔºå‰ΩÜÁº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÔºåÁâπÂà•ÊòØÂú®ÂèØÈù†ÊÄßÁÇ∫ÂøÖË¶ÅËÄÉÈáèÁöÑÈÜ´ÁôÇËÉåÊôØ‰∏ãÔºåÈÄôÂ∏∂‰æÜ‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂú®‰∏çÂΩ±ÈüøÈ†êÊ∏¨Á≤æÊ∫ñÂ∫¶ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈÄèÊòéÂ∫¶‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂç≥ Rad4XCNNÔºå‰ª•Â¢ûÂº∑ CNN Ë°çÁîüÁâπÂæµÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºåÂêåÊôÇÂÖ∑ÂÇôÊîæÂ∞ÑÁâπÂæµÂõ∫ÊúâÁöÑÂèØËß£ÈáãÊÄß„ÄÇRad4XCNN ‰∏çÂêåÊñºÂü∫ÊñºÈ°ØËëóÊÄßÂúñÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÊîæÂ∞ÑÁµÑÂ≠∏Â∞áÂèØÁêÜËß£ÁöÑÂê´Áæ©Ëàá CNN Ë°çÁîüÁâπÂæµÈóúËÅØËµ∑‰æÜÔºåÁÇ∫Ë∂ÖË∂äË¶ñË¶∫ÂåñÂúñË°®ÁöÑËß£ÈáãÊñπÊ≥ïÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÄÈªû„ÄÇÊàëÂÄë‰ª•‰π≥ÁôåÂàÜÈ°û‰ªªÂãô‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÂú®Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äË©ï‰º∞ Rad4XCNNÔºåÂåÖÊã¨‰∏ÄÂÄãÁ∑ö‰∏äË≥áÊñôÈõÜÂíåÂÖ©ÂÄãÁî®ÊñºÂÖßÈÉ®ÂíåÂ§ñÈÉ®È©óË≠âÁöÑÂÖßÈÉ®Ë≥áÊñôÈõÜ„ÄÇ‰∏Ä‰∫õÈóúÈçµÁµêÊûúÂ¶Ç‰∏ãÔºöi) Ëàá ViT Ë°çÁîüÁâπÂæµÂíåÊîæÂ∞ÑÁâπÂæµÁõ∏ÊØîÔºåCNN Ë°çÁîüÁâπÂæµ‰øùË≠â‰∫ÜÊõ¥Á©©ÂÅ•ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºõii) ÂÇ≥Áµ±ÁöÑË¶ñË¶∫ÂåñÂúñËß£ÈáãÊñπÊ≥ïÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºõiii) Rad4XCNN Ê≤íÊúâÁäßÁâ≤Ê®°ÂûãÊ∫ñÁ¢∫Â∫¶‰æÜÊèõÂèñÂÖ∂ÂèØËß£ÈáãÊÄßÔºõiv) Rad4XCNN Êèê‰æõ‰∫ÜÂÖ®Â±ÄËß£ÈáãË¶ãËß£Ôºå‰ΩøÈÜ´Â∏´ËÉΩÂ§†ÂàÜÊûêÊ®°ÂûãËº∏Âá∫ÂíåÁôºÁèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂº∑Ë™øÂ∞áÂèØËß£ÈáãÊÄßÊï¥ÂêàÂà∞ AI Ê®°Âûã‰∏≠Â∞çÊñºÂ¢ûÂº∑Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑ‰ø°‰ªªÂíåÊé°Áî®Ëá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âº∑Ë™ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂ¶Ç‰ΩïËÉΩÁ∑©Ëß£ËàáÂèØËß£Èáã AI ÊñπÊ≥ïÁõ∏ÈóúÁöÑ‰∏Ä‰∫õÁñëÊÖÆ„ÄÇ</paragraph>

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊôÆÂèäÊï¥ÂêàÔºåÂú®Ê∂âÂèä AI È©ÖÂãïÁ≥ªÁµ±ÁöÑ‰∫ãÊïÖ‰∏≠ÔºåË≤¨‰ªªÂíåÁæ©ÂãôÊ≠∏Â±¨Áî¢Áîü‰∫ÜË§áÈõúÁöÑÊåëÊà∞„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÁöÑ‰∫íÈÄ£ÊÄß„ÄÅAI ÂºïÁôº‰∫ãÊïÖÁöÑÂÄ´ÁêÜÂïèÈ°åÔºåÂä†‰∏ä AI ÊäÄË°ìÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂíåÁº∫‰πèÁõ∏ÊáâÊ≥ïË¶èÔºå‰ΩøÂæóÂÇ≥Áµ±Ë≤¨‰ªªÊ≠∏Â±¨Èù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË®àÁÆóÂèçÊÄùÂùáË°° (CRE) ÊñπÊ≥ïÔºå‰ª•Âª∫Á´ã‰∏ÄÂÄãÈÄ£Ë≤´‰∏îÂú®ÂÄ´ÁêÜ‰∏äÂèØÊé•ÂèóÁöÑË≤¨‰ªªÊ≠∏Â±¨Êû∂ÊßãÔºåÈÅ©Áî®ÊñºÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫„ÄÇË®àÁÆóÊñπÊ≥ïÊèê‰æõ‰∫ÜÁµêÊßãÂåñÁöÑÂàÜÊûêÔºåÂÖãÊúç‰∫ÜÊ¶ÇÂøµÊñπÊ≥ïÂú®ËôïÁêÜÂãïÊÖã‰∏îÂ§öÈù¢ÂêëÊÉÖÂ¢ÉÊôÇÁöÑÈôêÂà∂ÔºåÂ±ïÁ§∫‰∫ÜË©≤Êû∂ÊßãÂú®Ë≤¨‰ªªÊ≠∏Â±¨ÈÅéÁ®ã‰∏≠ÂÖ∑ÂÇôÁöÑÂèØËß£ÈáãÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜËàáÂùáË°°Ë®àÁÆó‰∏≠Á¥¢Ë≥†Áõ∏ÈóúÁöÑÂàùÂßãÂïüÂãïÂ±§Á¥öÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇÊàëÂÄë‰ª• AI ËºîÂä©ÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåË™™Êòé‰∏çÂêåÁöÑÂàùÂßãÂåñÂ¶Ç‰ΩïÂ∞éËá¥‰∏çÂêåÁöÑË≤¨‰ªªÂàÜÈÖç„ÄÇË©≤Êû∂ÊßãÊèê‰æõ‰∫ÜÂ∞ç AI ÂºïÁôº‰∫ãÊïÖ‰∏≠ÂïèË≤¨Âà∂ÁöÑÂØ∂Ë≤¥Ë¶ãËß£ÔºåÈÄèÈÅéÊåÅÁ∫åÁõ£Êéß„ÄÅ‰øÆË®ÇÂíåÂèçÊÄùÔºå‰øÉÈÄ≤‰∫ÜÊ∞∏Á∫å‰∏îÊúâÈüåÊÄßÁöÑÁ≥ªÁµ±ÁôºÂ±ï„ÄÇ

##### **Explainable AI for Fair Sepsis Mortality Predictive Model**
2404.13139v1 by Chia-Hsuan Chang, Xiaoyang Wang, Christopher C. Yang

Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÈÄèÈÅéÈ†êÊ∏¨Ê®°ÂûãÂçîÂä©ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÔºåÂ§ßÂπÖËΩâËÆä‰∫ÜËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®Á®ãÂºèÊôÇÂÖ¨Âπ≥ÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÈóúÈçµÈúÄÊ±ÇÔºå‰ª•Á¢∫‰øùÂú®‰∏çÂêåÁöÑÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àË≥áÊñô‰∏≠Áç≤ÂæóÂÖ¨Âπ≥ÁöÑÁµêÊûú„ÄÇÈÄèÈÅéÂ∞àÊ≥®ÊñºÊïóË°ÄÁóáÁõ∏ÈóúÊ≠ª‰∫°ÁéáÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÊúÉÂ≠∏Áøí‰∏ÄÂÄãÊïàËÉΩÊúÄ‰Ω≥ÂåñÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÁÑ∂ÂæåÊé°Áî®ËΩâÁßªÂ≠∏ÁøíÈÅéÁ®ã‰æÜÁî¢Áîü‰∏ÄÂÄãÂÖ∑ÊúâÊõ¥Â•ΩÂÖ¨Âπ≥ÊÄßÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÊéíÂàóÁöÑÁâπÂæµÈáçË¶ÅÊÄßÊºîÁÆóÊ≥ïÔºåÊó®Âú®Èó°ÊòéÊØèÂÄãÁâπÂæµÂú®Â¢ûÂº∑È†êÊ∏¨ÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÁöÑË≤¢Áçª„ÄÇËàáÁèæÊúâÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞àÊ≥®ÊñºËß£ÈáãÁâπÂæµÂ∞çÈ†êÊ∏¨ÊïàËÉΩÁöÑË≤¢Áçª‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁç®ÁâπÂú∞ÂΩåË£ú‰∫ÜÁêÜËß£ÊØèÂÄãÁâπÂæµÂ¶Ç‰ΩïÊúâÂä©ÊñºÂÖ¨Âπ≥ÊÄßÁöÑÂ∑ÆË∑ù„ÄÇÈÄôÈ†ÖÈÄ≤Â±ïËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÊïóË°ÄÁóáÁöÑÊ≠ª‰∫°ÁéáÂæàÈ´òÔºå‰∏îÂú®‰∏âÂàÜ‰πã‰∏ÄÁöÑÈÜ´Èô¢Ê≠ª‰∫°‰∏≠ÊâÆÊºîËëóËßíËâ≤„ÄÇÊàëÂÄëÁöÑÊ®°Âûã‰∏çÂÉÖÊúâÂä©ÊñºË≠òÂà•ÂíåÊ∏õËºïÈ†êÊ∏¨Ê®°Âûã‰∏≠ÁöÑÂÅèÂ∑ÆÔºåÈÇÑËÉΩÈÄèÈÅéÊèêÈ´òÊ®°ÂûãÈ†êÊ∏¨ÁöÑÈÄèÊòéÂ∫¶ÂíåÂÖ¨Âπ≥ÊÄß‰æÜÂüπÈ§äÈÜ´ÁôÇ‰øùÂÅ•Âà©ÁõäÁõ∏ÈóúËÄÖ‰πãÈñìÁöÑ‰ø°‰ªªÔºåÈÄ≤ËÄåÊúâÂä©ÊñºÊèê‰æõÊõ¥ÂÖ¨Âπ≥‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇ

##### **Multi Class Depression Detection Through Tweets using Artificial Intelligence**
2404.13104v1 by Muhammad Osama Nusrat, Waseem Shahzad, Saad Ahmed Jamal

Depression is a significant issue nowadays. As per the World Health
Organization (WHO), in 2023, over 280 million individuals are grappling with
depression. This is a huge number; if not taken seriously, these numbers will
increase rapidly. About 4.89 billion individuals are social media users. People
express their feelings and emotions on platforms like Twitter, Facebook,
Reddit, Instagram, etc. These platforms contain valuable information which can
be used for research purposes. Considerable research has been conducted across
various social media platforms. However, certain limitations persist in these
endeavors. Particularly, previous studies were only focused on detecting
depression and the intensity of depression in tweets. Also, there existed
inaccuracies in dataset labeling. In this research work, five types of
depression (Bipolar, major, psychotic, atypical, and postpartum) were predicted
using tweets from the Twitter database based on lexicon labeling. Explainable
AI was used to provide reasoning by highlighting the parts of tweets that
represent type of depression. Bidirectional Encoder Representations from
Transformers (BERT) was used for feature extraction and training. Machine
learning and deep learning methodologies were used to train the model. The BERT
model presented the most promising results, achieving an overall accuracy of
0.96.

ÊëòË¶ÅÔºöÁèæ‰ªäÔºåÊÜÇÈ¨±ÁóáÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑË≠∞È°å„ÄÇÊ†πÊìö‰∏ñÁïåË°õÁîüÁµÑÁπî (WHO) ÁöÑË≥áÊñôÔºåÂú® 2023 Âπ¥ÔºåË∂ÖÈÅé 2.8 ÂÑÑ‰∫∫Ê≠£Âú®ËàáÊÜÇÈ¨±ÁóáÊêèÈ¨•„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈæêÂ§ßÁöÑÊï∏Â≠óÔºõÂ¶ÇÊûú‰∏çË™çÁúüÁúãÂæÖÔºåÈÄô‰∫õÊï∏Â≠óÂ∞áÊúÉÂø´ÈÄüÂ¢ûÂä†„ÄÇÂ§ßÁ¥ÑÊúâ 48.9 ÂÑÑ‰∫∫ÊòØÁ§æÁæ§Â™íÈ´î‰ΩøÁî®ËÄÖ„ÄÇ‰∫∫ÂÄëÂú® Twitter„ÄÅFacebook„ÄÅReddit„ÄÅInstagram Á≠âÂπ≥Âè∞‰∏äË°®ÈÅîËá™Â∑±ÁöÑÊÑüÂèóÂíåÊÉÖÁ∑í„ÄÇÈÄô‰∫õÂπ≥Âè∞ÂåÖÂê´ÊúâÂÉπÂÄºÁöÑË≥áË®äÔºåÂèØÁî®ÊñºÁ†îÁ©∂ÁõÆÁöÑ„ÄÇÂ∑≤Á∂ìÂú®ÂêÑÁ®ÆÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞‰∏äÈÄ≤Ë°å‰∫ÜÂ§ßÈáèÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂä™Âäõ‰ªçÂ≠òÂú®Êüê‰∫õÈôêÂà∂„ÄÇÁâπÂà•ÊòØÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÂÉÖÂ∞àÊ≥®ÊñºÂÅµÊ∏¨Êé®Êñá‰∏≠ÁöÑÊÜÇÈ¨±ÁóáÂíåÊÜÇÈ¨±ÁóáÁöÑÂº∑Â∫¶„ÄÇÊ≠§Â§ñÔºåË≥áÊñôÈõÜÊ®ôÁ±§‰∏≠Â≠òÂú®‰∏çÊ∫ñÁ¢∫ÁöÑÊÉÖÊ≥Å„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂Â∑•‰Ωú‰∏≠Ôºå‰ΩøÁî®Âü∫ÊñºË©ûÂΩôÊ®ôÁ±§ÁöÑ Twitter Ë≥áÊñôÂ∫´‰∏≠ÁöÑÊé®ÊñáÈ†êÊ∏¨‰∫Ü‰∫îÁ®ÆÈ°ûÂûãÁöÑÊÜÇÈ¨±ÁóáÔºàÈõôÊ•µÂûã„ÄÅÈáçÂ∫¶„ÄÅÁ≤æÁ•ûÁóÖÂûã„ÄÅÈùûÂÖ∏ÂûãÂíåÁî¢ÂæåÔºâ„ÄÇÂèØËß£ÈáãÁöÑ AI Áî®ÊñºÈÄèÈÅéÂº∑Ë™ø‰ª£Ë°®ÊÜÇÈ¨±ÁóáÈ°ûÂûãÁöÑÊé®ÊñáÈÉ®ÂàÜ‰æÜÊèê‰æõÊé®ÁêÜ„ÄÇÂæû TransformersÔºàBERTÔºâ‰∏≠ÊèêÂèñÁöÑÈõôÂêëÁ∑®Á¢ºÂô®Ë°®Á§∫Áî®ÊñºÁâπÂæµÊèêÂèñÂíåË®ìÁ∑¥„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁî®ÊñºË®ìÁ∑¥Ê®°Âûã„ÄÇBERT Ê®°ÂûãÂëàÁèæÂá∫ÊúÄÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÈÅîÂà∞ 0.96 ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**
2404.12832v2 by Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman

Deep learning is dramatically transforming the field of medical imaging and
radiology, enabling the identification of pathologies in medical images,
including computed tomography (CT) and X-ray scans. However, the performance of
deep learning models, particularly in segmentation tasks, is often limited by
the need for extensive annotated datasets. To address this challenge, the
capabilities of weakly supervised semantic segmentation are explored through
the lens of Explainable AI and the generation of counterfactual explanations.
The scope of this research is development of a novel counterfactual inpainting
approach (COIN) that flips the predicted classification label from abnormal to
normal by using a generative model. For instance, if the classifier deems an
input medical image X as abnormal, indicating the presence of a pathology, the
generative model aims to inpaint the abnormal region, thus reversing the
classifier's original prediction label. The approach enables us to produce
precise segmentations for pathologies without depending on pre-existing
segmentation masks. Crucially, image-level labels are utilized, which are
substantially easier to acquire than creating detailed segmentation masks. The
effectiveness of the method is demonstrated by segmenting synthetic targets and
actual kidney tumors from CT images acquired from Tartu University Hospital in
Estonia. The findings indicate that COIN greatly surpasses established
attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an
alternative counterfactual explanation method introduced by Singla et al. This
evidence suggests that COIN is a promising approach for semantic segmentation
of tumors in CT images, and presents a step forward in making deep learning
applications more accessible and effective in healthcare, where annotated data
is scarce.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠¶‰π†Ê≠£Â§ßÂπÖËΩâËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÊîæÂ∞ÑÁ∑öÂ≠∏È†òÂüüÔºåËÉΩËæ®Ë≠òÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁóÖÁêÜÔºåÂåÖÊã¨ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) Âíå X ÂÖâÊéÉÊèè„ÄÇÁÑ∂ËÄåÔºåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®ÂàÜÂâ≤‰ªªÂãô‰∏≠ÔºåÂ∏∏Â∏∏ÂèóÂà∞Âª£Ê≥õË®ªËß£Ë≥áÊñôÈõÜÈúÄÊ±ÇÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÈÄèÈÅéÂèØËß£Èáã AI ÂíåÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÁî¢ÁîüÔºåÊé¢Á¥¢Âº±Áõ£Áù£Ë™ûÊÑèÂàÜÂâ≤ÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁØÑÂúçÊòØÈñãÁôº‰∏ÄÁ®ÆÊñ∞ÁöÑÂèç‰∫ãÂØ¶ÂÖßÊèíÊñπÊ≥ï (COIN)ÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®ÁîüÊàêÊ®°ÂûãÂ∞áÈ†êÊ∏¨ÁöÑÂàÜÈ°ûÊ®ôÁ±§ÂæûÁï∞Â∏∏ÁøªËΩâÁÇ∫Ê≠£Â∏∏„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂàÜÈ°ûÂô®Â∞áËº∏ÂÖ•ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉè X Ë¶ñÁÇ∫Áï∞Â∏∏ÔºåË°®Á§∫Â≠òÂú®ÁóÖÁêÜÔºåÂâáÁîüÊàêÊ®°ÂûãÊó®Âú®ÂÖßÊèíÁï∞Â∏∏ÂçÄÂüüÔºåÂæûËÄåÈÄÜËΩâÂàÜÈ°ûÂô®ÁöÑÂéüÂßãÈ†êÊ∏¨Ê®ôÁ±§„ÄÇÊ≠§ÊñπÊ≥ï‰ΩøÊàëÂÄëËÉΩÂ§†Áî¢ÁîüÁóÖÁêÜÁöÑÁ≤æÁ¢∫ÂàÜÂâ≤ÔºåËÄåÁÑ°ÈúÄ‰æùË≥¥ÊñºÈ†êÂÖàÂ≠òÂú®ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÂà©Áî®ÂΩ±ÂÉèÂ±§Á¥öÊ®ôÁ±§ÔºåÈÄôÊØîÂª∫Á´ãË©≥Á¥∞ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©ÂÆπÊòìÂèñÂæó„ÄÇË©≤ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÈÄèÈÅéÂàÜÂâ≤ÂêàÊàêÁõÆÊ®ôÂíåÂæûÊÑõÊ≤ôÂ∞º‰∫ûÂ°îÁàæÂúñÂ§ßÂ≠∏ÈÜ´Èô¢ÂèñÂæóÁöÑ CT ÂΩ±ÂÉè‰∏≠ÁöÑÂØ¶ÈöõËÖéËáüËÖ´Áò§‰æÜË≠âÊòé„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåCOIN ÈÅ†ÈÅ†Ë∂ÖÈÅéÂ∑≤Âª∫Á´ãÁöÑÊ≠∏Âõ†ÊñπÊ≥ïÔºå‰æãÂ¶Ç RISE„ÄÅScoreCAM Âíå LayerCAMÔºå‰ª•Âèä Singla Á≠â‰∫∫ÊèêÂá∫ÁöÑÂè¶‰∏ÄÁ®ÆÂèç‰∫ãÂØ¶Ëß£ÈáãÊñπÊ≥ï„ÄÇÊ≠§Ë≠âÊìöË°®ÊòéÔºåCOIN ÊòØ‰∏ÄÁ®ÆÂæàÊúâÂâçÈÄîÁöÑ CT ÂΩ±ÂÉè‰∏≠ËÖ´Áò§Ë™ûÊÑèÂàÜÂâ≤ÊñπÊ≥ïÔºå‰∏¶Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ËÆìÊ∑±Â∫¶Â≠∏ÁøíÊáâÁî®Êõ¥ÊòìÊñºÂèñÂæóÂíåÊõ¥ÊúâÊïàÁéáÈÇÅÈÄ≤‰∏ÄÊ≠•ÔºåÂÖ∂‰∏≠Ë®ªËß£Ë≥áÊñôÂæàÁ®ÄÂ∞ë„ÄÇ

##### **Hybrid Intelligence for Digital Humanities**
2406.15374v1 by Victor de Boer, Lise Stork

In this paper, we explore the synergies between Digital Humanities (DH) as a
discipline and Hybrid Intelligence (HI) as a research paradigm. In DH research,
the use of digital methods and specifically that of Artificial Intelligence is
subject to a set of requirements and constraints. We argue that these are
well-supported by the capabilities and goals of HI. Our contribution includes
the identification of five such DH requirements: Successful AI systems need to
be able to 1) collaborate with the (human) scholar; 2) support data criticism;
3) support tool criticism; 4) be aware of and cater to various perspectives and
5) support distant and close reading. We take the CARE principles of Hybrid
Intelligence (collaborative, adaptive, responsible and explainable) as
theoretical framework and map these to the DH requirements. In this mapping, we
include example research projects. We finally address how insights from DH can
be applied to HI and discuss open challenges for the combination of the two
disciplines.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊï∏‰Ωç‰∫∫ÊñáÂ≠∏Áßë (DH) ‰ΩúÁÇ∫‰∏ÄÈñÄÂ≠∏ÁßëËàáÊ∑∑ÂêàÊô∫ËÉΩ (HI) ‰ΩúÁÇ∫‰∏ÄÂÄãÁ†îÁ©∂ÂÖ∏ÁØÑ‰πãÈñìÁöÑÂçîÂêå‰ΩúÁî®„ÄÇÂú® DH Á†îÁ©∂‰∏≠ÔºåÊï∏‰ΩçÊñπÊ≥ïÁöÑ‰ΩøÁî®ÔºåÁâπÂà•ÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰ΩøÁî®ÔºåÂèóÂà∞‰∏ÄÁ≥ªÂàóË¶ÅÊ±ÇÂíåÈôêÂà∂„ÄÇÊàëÂÄëË™çÁÇ∫ÈÄô‰∫õË¶ÅÊ±ÇÂíåÈôêÂà∂Áç≤Âæó HI ÁöÑËÉΩÂäõÂíåÁõÆÊ®ôÁöÑÂÖÖÂàÜÊîØÊåÅ„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨ÊâæÂá∫‰∫îÂÄãÈÄôÊ®£ÁöÑ DH Ë¶ÅÊ±ÇÔºöÊàêÂäüÁöÑ AI Á≥ªÁµ±ÈúÄË¶ÅËÉΩÂ§† 1) ËàáÔºà‰∫∫È°ûÔºâÂ≠∏ËÄÖÂêà‰ΩúÔºõ2) ÊîØÊè¥Ë≥áÊñôÊâπË©ïÔºõ3) ÊîØÊè¥Â∑•ÂÖ∑ÊâπË©ïÔºõ4) ÂØüË¶∫‰∏¶ËøéÂêàÂêÑÁ®ÆËßÄÈªûÔºõ5) ÊîØÊè¥ÈÅ†Ë∑ùÂíåËøëË∑ùÈõ¢Èñ±ËÆÄ„ÄÇÊàëÂÄëÂ∞áÊ∑∑ÂêàÊô∫ËÉΩÁöÑ CARE ÂéüÂâáÔºàÂçî‰Ωú„ÄÅÈÅ©Êáâ„ÄÅË≤†Ë≤¨ÂíåÂèØËß£ÈáãÔºâ‰ΩúÁÇ∫ÁêÜË´ñÊû∂ÊßãÔºå‰∏¶Â∞áÈÄô‰∫õÂéüÂâáÂ∞çÊáâÂà∞ DH Ë¶ÅÊ±Ç„ÄÇÂú®Ê≠§Â∞çÊáâ‰∏≠ÔºåÊàëÂÄëÁ¥çÂÖ•ÁØÑ‰æãÁ†îÁ©∂Â∞àÊ°à„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éÂ¶Ç‰ΩïÂ∞á DH ÁöÑË¶ãËß£ÊáâÁî®Êñº HIÔºå‰∏¶Ë®éË´ñÁµêÂêàÈÄôÂÖ©ÂÄãÂ≠∏ÁßëÁöÑÈñãÊîæÊåëÊà∞„ÄÇ

##### **Ethical Framework for Responsible Foundational Models in Medical Imaging**
2406.11868v1 by Abhijit Das, Debesh Jha, Jasmer Sanjotra, Onkar Susladkar, Suramyaa Sarkar, Ashish Rauniyar, Nikhil Tomar, Vanshali Sharma, Ulas Bagci

Foundational models (FMs) have tremendous potential to revolutionize medical
imaging. However, their deployment in real-world clinical settings demands
extensive ethical considerations. This paper aims to highlight the ethical
concerns related to FMs and propose a framework to guide their responsible
development and implementation within medicine. We meticulously examine ethical
issues such as privacy of patient data, bias mitigation, algorithmic
transparency, explainability and accountability. The proposed framework is
designed to prioritize patient welfare, mitigate potential risks, and foster
trust in AI-assisted healthcare.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°Âûã (FM) ÂÖ∑ÊúâÂæπÂ∫ïÊîπËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïåËá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÂª£Ê≥õÁöÑÂÄ´ÁêÜËÄÉÈáè„ÄÇÊú¨ÊñáÊó®Âú®Âº∑Ë™øËàá FM Áõ∏ÈóúÁöÑÂÄ´ÁêÜÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ°ÜÊû∂‰æÜÊåáÂ∞éÂÆÉÂÄëÂú®ÈÜ´Â≠∏‰∏≠ÁöÑË≤†Ë≤¨‰ªªÈñãÁôºÂíåÂØ¶ÊñΩ„ÄÇÊàëÂÄë‰ªîÁ¥∞ÂØ©Êü•‰∫ÜÂÄ´ÁêÜÂïèÈ°åÔºå‰æãÂ¶ÇÊÇ£ËÄÖÊï∏ÊìöÈö±ÁßÅ„ÄÅÂÅèÂ∑ÆÁ∑©Ëß£„ÄÅÊºîÁÆóÊ≥ïÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÂïèË≤¨Âà∂„ÄÇÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂Êó®Âú®ÂÑ™ÂÖàËÄÉÊÖÆÊÇ£ËÄÖÁ¶èÂà©„ÄÅÊ∏õËºïÊΩõÂú®È¢®Èö™Ôºå‰∏¶ÂüπÈ§äÂ∞ç AI ËºîÂä©ÈÜ´ÁôÇ‰øùÂÅ•ÁöÑ‰ø°‰ªª„ÄÇ

##### **Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**
2404.07239v1 by Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak

Thyroid cancer is an increasing global health concern that requires advanced
diagnostic methods. The application of AI and radiomics to thyroid cancer
diagnosis is examined in this review. A review of multiple databases was
conducted in compliance with PRISMA guidelines until October 2023. A
combination of keywords led to the discovery of an English academic publication
on thyroid cancer and related subjects. 267 papers were returned from the
original search after 109 duplicates were removed. Relevant studies were
selected according to predetermined criteria after 124 articles were eliminated
based on an examination of their abstract and title. After the comprehensive
analysis, an additional six studies were excluded. Among the 28 included
studies, radiomics analysis, which incorporates ultrasound (US) images,
demonstrated its effectiveness in diagnosing thyroid cancer. Various results
were noted, some of the studies presenting new strategies that outperformed the
status quo. The literature has emphasized various challenges faced by AI
models, including interpretability issues, dataset constraints, and operator
dependence. The synthesized findings of the 28 included studies mentioned the
need for standardization efforts and prospective multicenter studies to address
these concerns. Furthermore, approaches to overcome these obstacles were
identified, such as advances in explainable AI technology and personalized
medicine techniques. The review focuses on how AI and radiomics could transform
the diagnosis and treatment of thyroid cancer. Despite challenges, future
research on multidisciplinary cooperation, clinical applicability validation,
and algorithm improvement holds the potential to improve patient outcomes and
diagnostic precision in the treatment of thyroid cancer.

ÊëòË¶ÅÔºöÁî≤ÁãÄËÖ∫ÁôåÊòØ‰∏ÄÁ®ÆÊó•ÁõäÂö¥ÈáçÁöÑÂÖ®ÁêÉÂÅ•Â∫∑ÂïèÈ°åÔºåÈúÄË¶ÅÂÖàÈÄ≤ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÇÊú¨ÁØáË©ïË´ñÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËàáÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂú®Áî≤ÁãÄËÖ∫ÁôåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÂú®Á¨¶Âêà PRISMA ÊåáÂçóÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞çÂ§öÂÄãË≥áÊñôÂ∫´ÈÄ≤Ë°å‰∫ÜÂõûÈ°ßÔºåÁõ¥Âà∞ 2023 Âπ¥ 10 Êúà„ÄÇÈÄöÈÅéÁµêÂêàÈóúÈçµÂ≠óÔºåÁôºÁèæ‰∫Ü‰∏ÄÁØáÈóúÊñºÁî≤ÁãÄËÖ∫ÁôåÂíåÁõ∏Èóú‰∏ªÈ°åÁöÑËã±ÊñáÂ≠∏Ë°ìÂá∫ÁâàÁâ©„ÄÇÂú®ÁßªÈô§ 109 ÁØáÈáçË§áÊñáÁçªÂæåÔºåÂéüÂßãÊêúÂ∞ãÂÖ±ÂõûÂÇ≥ 267 ÁØáË´ñÊñá„ÄÇÂú®Ê†πÊìöÈ†êÂÖàÁ¢∫ÂÆöÁöÑÊ®ôÊ∫ñÔºåÊ∑òÊ±∞‰∫Ü 124 ÁØáÊñáÁ´†ÁöÑÊëòË¶ÅÂíåÊ®ôÈ°åÂæåÔºåÈÅ∏Âá∫‰∫ÜÁõ∏ÈóúÁ†îÁ©∂„ÄÇÂú®ÈÄ≤Ë°åÂÖ®Èù¢ÂàÜÊûêÂæåÔºåÈ°çÂ§ñÊéíÈô§‰∫ÜÂÖ≠È†ÖÁ†îÁ©∂„ÄÇÂú®Á¥çÂÖ•ÁöÑ 28 È†ÖÁ†îÁ©∂‰∏≠ÔºåÁµêÂêàË∂ÖÈü≥Ê≥¢ (US) ÂΩ±ÂÉèÁöÑÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®Ë®∫Êñ∑Áî≤ÁãÄËÖ∫ÁôåÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂ÁµêÊûú‰∏ç‰∏ÄÔºåÊúâ‰∫õÁ†îÁ©∂ÊèêÂá∫‰∫ÜÂÑ™ÊñºÁèæÁãÄÁöÑÊñ∞Á≠ñÁï•„ÄÇÊñáÁçªÂº∑Ë™ø‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÈù¢Ëá®ÁöÑÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨ÂèØËß£ÈáãÊÄßÂïèÈ°å„ÄÅË≥áÊñôÈõÜÈôêÂà∂ÂíåÊìç‰ΩúÂì°‰æùË≥¥ÊÄß„ÄÇ28 È†ÖÁ¥çÂÖ•Á†îÁ©∂ÁöÑÁ∂úÂêàÁôºÁèæÊèêÂà∞ÔºåÈúÄË¶ÅÊ®ôÊ∫ñÂåñÂ∑•‰ΩúÂíåÂâçÁûªÊÄßÂ§ö‰∏≠ÂøÉÁ†îÁ©∂‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¢∫ÂÆö‰∫ÜÂÖãÊúçÈÄô‰∫õÈöúÁ§ôÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂèØËß£Èáã‰∫∫Â∑•Êô∫ËÉΩÊäÄË°ìÂíåÂÄã‰∫∫ÂåñÈÜ´ÁôÇÊäÄË°ìÁöÑÈÄ≤Ê≠•„ÄÇÊú¨ÁØáË©ïË´ñÈáçÈªûÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÂíåÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂ¶Ç‰ΩïËΩâËÆäÁî≤ÁãÄËÖ∫ÁôåÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇ„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÊú™‰æÜÂ∞çÂ§öÂ≠∏ÁßëÂêà‰Ωú„ÄÅËá®Â∫äÈÅ©Áî®ÊÄßÈ©óË≠âÂíåÊºîÁÆóÊ≥ïÊîπÈÄ≤ÁöÑÁ†îÁ©∂Ôºå‰ªçÊúâÊΩõÂäõÊîπÂñÑÁî≤ÁãÄËÖ∫ÁôåÊ≤ªÁôÇ‰∏≠ÁöÑÊÇ£ËÄÖÈ†êÂæåÂíåË®∫Êñ∑Á≤æÊ∫ñÂ∫¶„ÄÇ

##### **Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**
2404.04686v1 by Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena, Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah Nafidi, Mohammed Bourhia

Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºå‰π≥ÁôåÁöÑÁõõË°åÁéáËøÖÈÄüÂ¢ûÂä†Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫ÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†‰πã‰∏Ä„ÄÇÂú®ÊâÄÊúâÁôåÁóá‰∏≠Ôºå‰π≥ÁôåËøÑ‰ªäÁÇ∫Ê≠¢ÊòØÊúÄÂ∏∏Ë¶ãÁöÑ„ÄÇÊâãÂãïË®∫Êñ∑Ê≠§ÁñæÁóÖÈúÄË¶ÅÂ§ßÈáèÁöÑÊôÇÈñìÂíåÂ∞àÊ•≠Áü•Ë≠ò„ÄÇÁî±Êñº‰π≥ÁôåÁöÑÊ™¢Ê∏¨ÈÅéÁ®ãËÄóÊôÇÔºåÂõ†Ê≠§ÈÄèÈÅéÂª∫Á´ãÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨ÔºåÊúâÂä©ÊñºÈò≤Ê≠¢ÂÖ∂ÈÄ≤‰∏ÄÊ≠•Êì¥Êï£„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÂèØËß£Èáã AI Âú®ÂàÜÈ°û‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄë‰∏çÂÉÖÂèØ‰ª•Êèê‰æõÊ∫ñÁ¢∫ÁöÑÈ†êÊ∏¨ÔºåÈÇÑÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÂ¶Ç‰ΩïÂÅöÂá∫Ê±∫Á≠ñÔºåÊúâÂä©ÊñºÁêÜËß£Âíå‰ø°Ë≥¥ÂàÜÈ°ûÁµêÊûú„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∏¶ÊØîËºÉ‰∫Ü‰∫îÁ®Æ‰∏çÂêåÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÅÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏Ôºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄã‰∏ªË¶ÅÁöÑË≥áÊñôÈõÜÔºàÈÅîÂç°ÈÜ´Â≠∏Èô¢ÈÜ´Èô¢ÁöÑ 500 ÂêçÊÇ£ËÄÖÔºâ„ÄÇ‰∫îÁ®Æ‰∏çÂêåÁöÑÁõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÔºåÂåÖÊã¨Ê±∫Á≠ñÊ®π„ÄÅÈö®Ê©üÊ£ÆÊûó„ÄÅÈÇèËºØËø¥Ê≠∏„ÄÅÊú¥Á¥†Ë≤ùÊ∞èÂíå XGBoostÔºåÂ∑≤Áî®ÊñºÂú®ÊàëÂÄëÁöÑË≥áÊñôÈõÜ‰∏äÂèñÂæóÊúÄ‰Ω≥ÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂Â∞á SHAP ÂàÜÊûêÊáâÁî®Êñº XGBoost Ê®°ÂûãÔºå‰ª•Ëß£ÈáãÊ®°ÂûãÁöÑÈ†êÊ∏¨‰∏¶‰∫ÜËß£ÊØèÂÄãÁâπÂæµÂ∞çÊ®°ÂûãËº∏Âá∫ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂπæÁ®ÆÊºîÁÆóÊ≥ïÂ∞çË≥áÊñôÈÄ≤Ë°åÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰∏¶ËàáË©≤È†òÂüüÁöÑÂÖ∂‰ªñÊñáÁçªÈÄ≤Ë°åÂ∞çÊØî„ÄÇÂú®ÊúÄÂæåË©ï‰º∞ÂæåÔºåÊú¨Á†îÁ©∂ÁôºÁèæ XGBoost ÈÅîÂà∞‰∫ÜÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÊ∫ñÁ¢∫Â∫¶ÔºåÁÇ∫ 97%„ÄÇ</paragraph>

##### **Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**
2404.03892v3 by Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí (DL) Áî®ÊñºÂæû‰π≥ÊàøÊîùÂΩ±Ë°ìÂΩ±ÂÉèË®∫Êñ∑‰π≥ÁôåÁöÑÊ®°ÂûãÈÄöÂ∏∏‰ª•„ÄåÈªëÁõíÂ≠ê„ÄçÊñπÂºèÈÅã‰ΩúÔºåÈÄô‰ΩøÂæóÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Èõ£‰ª•‰ø°‰ªªÂíåÁêÜËß£ÂÖ∂Ê±∫Á≠ñÈÅéÁ®ã„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊï¥ÂêàÊû∂ÊßãÔºåÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI)Ôºå‰ª•‰ΩøÁî® CBIS-DDSM Ë≥áÊñôÈõÜÂ¢ûÂº∑‰π≥ÁôåÁöÑË®∫Êñ∑„ÄÇÊñπÊ≥ïÂåÖÂê´‰∏ÄÂÄãÁ≤æÁ¥∞ÁöÑË≥áÊñôÂâçËôïÁêÜÁÆ°Á∑öÂíåÈÄ≤ÈöéË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÔºå‰ª•Â∞çÊäóË≥áÊñôÈõÜÈôêÂà∂Ôºå‰∏¶Êé°Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÁ∂≤Ë∑ØÔºà‰æãÂ¶Ç VGG-16„ÄÅInception-V3 Âíå ResNetÔºâÈÄ≤Ë°åÈÅ∑ÁßªÂ≠∏Áøí„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÈáçÈªûÊòØË©ï‰º∞ XAI Âú®Ëß£ÈáãÊ®°ÂûãÈ†êÊ∏¨‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÈáçÈªûÂà©Áî®Ë±™ÊñØÂ§öÂ§´Ê∏¨Â∫¶ÈáèÂåñË©ï‰º∞ AI ÁîüÊàêÁöÑËß£ÈáãÂíåÂ∞àÂÆ∂Ë®ªËß£‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ∞çÊñº XAI Âú®‰øÉÈÄ≤ AI ËºîÂä©Ë®∫Êñ∑‰∏≠ÁöÑÂèØ‰ø°Â∫¶ÂíåÂÄ´ÁêÜÂÖ¨Âπ≥ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÁôºÁèæË™™Êòé‰∫Ü CNN Âíå XAI Âú®Êé®ÈÄ≤‰π≥ÁôåË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊúâÊïàÂçî‰ΩúÔºåÂæûËÄå‰øÉÈÄ≤‰∫ÜÂÖàÈÄ≤ AI ÊäÄË°ìÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊõ¥È†ÜÊö¢Êï¥Âêà„ÄÇÈÄèÈÅéÂ¢ûÂº∑ AI È©ÖÂãïÊ±∫Á≠ñÁöÑÂèØËß£ÈáãÊÄßÔºåÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ AI Á≥ªÁµ±ÂíåÈÜ´ÁôÇÂæûÊ•≠‰∫∫Âì°‰πãÈñìÁöÑÊîπÂñÑÂçî‰ΩúÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÊúÄÁµÇË±êÂØå‰∫ÜÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ†îÁ©∂ÁöÑÂΩ±ÈüøÈÅ†ÈÅ†Ë∂ÖÂá∫‰∫ÜÁõÆÂâçÁöÑÊäÄË°ì„ÄÇÂÆÉÈºìÂãµÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂Â¶Ç‰ΩïÁµêÂêàÂ§öÊ®°ÂºèË≥áÊñô‰∏¶ÊîπÂñÑ AI Ëß£ÈáãÔºå‰ª•ÊªøË∂≥Ëá®Â∫äÂØ¶ÂãôÁöÑÈúÄÊ±Ç„ÄÇ

##### **Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**
2403.17873v1 by Andrea Ferrario, Alberto Termine, Alessandro Facchini

Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.

ÊëòË¶ÅÔºö‰ª•‰∫∫‰∏∫Êú¨ÁöÑÂèØËß£Èáä AI (HCXAI) ÂÄ°ÂØºÂ∞ÜÁ§æ‰ºöÂ±ÇÈù¢Êï¥ÂêàÂà∞ AI Ëß£Èáä‰∏≠„ÄÇHCXAI ËØùËØ≠ÁöÑÊ†∏ÂøÉÊòØÁ§æ‰ºöÈÄèÊòéÂ∫¶ (ST) Ê°ÜÊû∂ÔºåÂÖ∂ÁõÆÊ†áÊòØËÆ© AI Á≥ªÁªüÁöÑÁ§æ‰ºöÁªÑÁªáËÉåÊôØÂØπÁî®Êà∑Êù•ËØ¥ÊòØÂèØÁêÜËß£ÁöÑ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âª∫ËÆÆÊâ©Â±ï ST Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰∏≠Á§æ‰ºöÈîôËØØÂΩíÂõ†ÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂøÉÁêÜÂÅ•Â∫∑Á≠âÊïèÊÑüÈ¢ÜÂüü„ÄÇ‰∫ãÂÆû‰∏äÔºåLLM ËÉΩÂ§üÂá∫Ëâ≤Âú∞Ê®°ÊãüËßíËâ≤Âíå‰∫∫Ê†ºÔºåËøôÂèØËÉΩÂØºËá¥ËÆæËÆ°ËÄÖÁöÑÊÑèÂõæÂíåÁî®Êà∑ÂØπÁ§æ‰ºöÂ±ûÊÄßÁöÑËÆ§Áü•‰πãÈó¥Âá∫Áé∞ÈîôÈÖçÔºå‰ªéËÄåÊúâÈ£éÈô©‰øÉËøõÊÉÖÁª™ÊìçÁ∫µÂíåÂç±Èô©Ë°å‰∏∫„ÄÅËÆ§Áü•‰∏çÂÖ¨Ê≠£Âíå‰∏çÂêàÁêÜÁöÑ‰ø°‰ªª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÁî®Á¨¨‰∫î‰∏™‚ÄúW ÈóÆÈ¢ò‚ÄùÊù•Â¢ûÂº∫ ST Ê°ÜÊû∂Ôºå‰ª•ÊòéÁ°ÆËÆæËÆ°ËÄÖÂíåÁî®Êà∑Ëµã‰∫à LLM ÁöÑÂÖ∑‰ΩìÁ§æ‰ºöÂ±ûÊÄß„ÄÇÊ≠§Ë°•ÂÖÖÊó®Âú®Âº•Âêà LLM ËÉΩÂäõÂíåÁî®Êà∑ËÆ§Áü•‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰øÉËøõÂü∫‰∫é LLM ÁöÑÊäÄÊúØÂú®ÈÅìÂæ∑‰∏äË¥üË¥£‰ªªÂú∞ÂºÄÂèëÂíå‰ΩøÁî®„ÄÇ

##### **Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**
2403.18871v1 by Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu

Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÊ∞£ËÉ∏ÊòØ‰∏ÄÁ®ÆÂõ†ËÇ∫ÈÉ®ËàáËÉ∏Â£Å‰πãÈñìÁï∞Â∏∏ÈõÜÊ∞£ÊâÄÂºïËµ∑ÁöÑÊÄ•ÊÄßËÉ∏ËÖîÁñæÁóÖ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê∑±Â∫¶Â≠∏ÁøíÔºàDLÔºâÊ®°ÂûãÁ∂ìÂ∏∏‰º¥Èö®ÁöÑ‰∏çÈÄèÊòéÊÄßÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÊñπÊ≥ïÂ∑≤Ë¢´ÂºïÂÖ•ÔºåÁî®ÊñºÊ¶ÇËø∞Ëàá DL Ê®°ÂûãÂÅöÂá∫ÁöÑÊ∞£ËÉ∏Ë®∫Êñ∑Áõ∏ÈóúÁöÑÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õËß£ÈáãÊúâÊôÇÊúÉËàáÂØ¶ÈöõÁóÖÁÅ∂ÂçÄÂüüÊúâÊâÄÂá∫ÂÖ•ÔºåÁ™ÅÈ°ØÂá∫ÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊñπÊ≥ïÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÂ∞áÊ∞£ËÉ∏ÁöÑËá®Â∫äÁü•Ë≠òÁ¥çÂÖ• XAI ÊñπÊ≥ïÁî¢ÁîüÁöÑÊ®°ÂûãËß£Èáã‰∏≠ÔºåÂæûËÄåÊèêÂçáÈÄô‰∫õËß£ÈáãÁöÑÂìÅË≥™„ÄÇÂà©Áî®ÊîæÂ∞ÑÁßëÈÜ´Â∏´Âª∫Á´ãÁöÑÁóÖÁÅ∂ÊèèÁπ™ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ¶ñÂÖàÁî¢Áîü‰∏ÄÂÄãÊ®°ÊùøÔºåÁî®ÊñºË°®Á§∫Ê∞£ËÉ∏ÂèØËÉΩÁôºÁîüÁöÑÂçÄÂüü„ÄÇÁÑ∂ÂæåÂ∞áÊ≠§Ê®°ÊùøÁñäÂä†Âú®Ê®°ÂûãËß£Èáã‰∏äÔºå‰ª•ÁØ©ÈÅ∏Âá∫Ë∂ÖÂá∫Ê®°ÊùøÈÇäÁïåÁöÑÁÑ°ÈóúËß£Èáã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÂÖ∂ÊïàÂäõÔºåÊàëÂÄëÂ∞ç‰∏âÁ®Æ XAI ÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÊØîËºÉÂàÜÊûêÔºåÂú®ÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏≠Ëß£ÈáãÂÖ©ÂÄã DL Ê®°ÂûãÊôÇÔºåÂàÜÂà•Êé°Áî®Âíå‰∏çÊé°Áî®ÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞é„ÄÇÁµêÊûúÔºöÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Âª∫Á´ãÊñº‰∏âÁ®Æ XAI ÊñπÊ≥ï„ÄÅÂÖ©ÂÄã DL Ê®°ÂûãÂíåÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÂçÅ‰∫åÁ®ÆÂü∫Ê∫ñÊÉÖÂ¢É‰∏≠ÔºåÂßãÁµÇÊîπÂñÑ‰∫ÜÂü∫Ê∫ñ XAI ÊñπÊ≥ï„ÄÇÂú®ÊØîËºÉÊ®°ÂûãËß£ÈáãÂíåÁúüÂØ¶ÁóÖÁÅ∂ÂçÄÂüüÊôÇÔºåÈÄèÈÅéÂü∫Ê∫ñÊïàËÉΩÁöÑÊïàËÉΩÊîπÈÄ≤Ë®àÁÆóÂá∫ÁöÑÂπ≥ÂùáÂ¢ûÈáèÁôæÂàÜÊØîÁÇ∫‰∫§ÈõÜÊØîÔºàIoUÔºâÁöÑ 97.8% ÂíåÈ™∞Â≠êÁõ∏‰ººÊÄß‰øÇÊï∏ÔºàDSCÔºâÁöÑ 94.1%„ÄÇÁµêË´ñÔºöÂú®Ê∞£ËÉ∏Ë®∫Êñ∑ÁöÑËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÁî®ÊñºÊîπÂñÑ AI Ëß£Èáã„ÄÇÊàëÂÄëÈ†êÊúüÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞éÂ∞áÈÄèÈÅéÊï¥ÂêàËá®Â∫äÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÔºåÁÇ∫Èó°Êòé AI Ê®°ÂûãÂª∫Á´ã‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**
2403.01580v1 by S√©amus Lankford

In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.

ÊëòË¶ÅÔºö<paragraph>Âú®Áï∂ÂâçÊ©üÂô®ÁøªË≠Ø (MT) È†òÂüü‰∏≠ÔºåTransformer Êû∂ÊßãËÑ´Á©éËÄåÂá∫ÔºåÊàêÁÇ∫ÈªÉÈáëÊ®ôÊ∫ñÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ´òË≥áÊ∫êË™ûË®ÄÂ∞ç„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂÖ∂Â∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÊïàËÉΩÔºåÂåÖÊã¨Ëã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûË™ûË®ÄÂ∞ç„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊú¨Á†îÁ©∂Ë≠òÂà•Âá∫ÊúÄ‰Ω≥Ë∂ÖÂèÉÊï∏ÂíåÂ≠êË©ûÊ®°ÂûãÈ°ûÂûãÔºå‰ª•È°ØËëóÊèêÈ´ò Transformer Ê®°ÂûãÂ∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÂìÅË≥™„ÄÇ
‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑÂπ≥Ë°åË≥áÊñôÈõÜÁöÑÁ®ÄÁº∫ÊúÉÈòªÁ§ô MT ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈñãÁôº‰∫Ü gaHealthÔºåÈÄôÊòØÊÑõÁàæËò≠Ë™ûÁöÑÁ¨¨‰∏ÄÂÄãÈõôË™ûÂÅ•Â∫∑Ë≥áÊñôË™ûÊñôÂ∫´„ÄÇÂ∞àÊ≥®ÊñºÂÅ•Â∫∑È†òÂüüÔºå‰ΩøÁî®Ê≠§ÂüüÂÖßË≥áÊñôÈõÜÈñãÁôºÁöÑÊ®°ÂûãÂú® BLEU ÂæóÂàÜÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂ∏∏È°ØËëóÁöÑÈÄ≤Ê≠•ÔºåËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇÈö®Âæå‰ΩøÁî®Â§öÁ∂≠ÂìÅË≥™ÊåáÊ®ôÈåØË™§ÂàÜÈ°ûÊ≥ïÈÄ≤Ë°åÁöÑ‰∫∫Â∑•Ë©ï‰º∞È°ØÁ§∫ÔºåËàáÂü∫Êñº RNN ÁöÑÂ∞çÊáâÊ®°ÂûãÁõ∏ÊØîÔºåTransformer Á≥ªÁµ±Âú®Ê∏õÂ∞ëÊ∫ñÁ¢∫ÊÄßÂíåÊµÅÊö¢ÊÄßÈåØË™§ÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊÄßËÉΩ„ÄÇ
Ê≠§Â§ñÔºåÊú¨Ë´ñÊñá‰ªãÁ¥π‰∫Ü adaptNMT Âíå adaptMLLMÔºåÈÄôÂÖ©ÂÄãÈñãÊ∫êÊáâÁî®Á®ãÂºèÁ∞°Âåñ‰∫ÜÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠ØÊ®°ÂûãÁöÑÈñãÁôº„ÄÅÂæÆË™øÂíåÈÉ®ÁΩ≤„ÄÇÈÄô‰∫õÂ∑•ÂÖ∑Â§ßÂπÖÁ∞°Âåñ‰∫ÜË®≠ÂÆöÂíåË©ï‰º∞ÊµÅÁ®ãÔºåËÆì MT Êõ¥ÂÆπÊòìËÆìÈñãÁôº‰∫∫Âì°ÂíåÁøªË≠Ø‰∫∫Âì°‰ΩøÁî®„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåadaptNMT ‰ª• OpenNMT ÁîüÊÖãÁ≥ªÁµ±ÁÇ∫Âü∫Á§éÔºåÈÄöÈÅéÂº∑Ë™øÊ®°ÂûãÈñãÁôºÁöÑÁí∞Â¢ÉË∂≥Ë∑°‰æÜ‰øÉÈÄ≤ÁîüÊÖãÂèãÂ•ΩÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ†îÁ©∂„ÄÇËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåadaptMLLM Â∞ç MLLM ÁöÑÂæÆË™øË≠âÊòé‰∫ÜËã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûÈÄôÂÖ©ÂÄã‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÊÄßËÉΩÈÄ≤Ê≠•„ÄÇ</paragraph>

##### **Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**
2402.18600v1 by Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya

Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖÔºàDMÔºâ‰ΩøÊÇ£ËÄÖÂÆπÊòìÂá∫ÁèæË°ÄÁÆ°‰ΩµÁôºÁóá„ÄÇ
Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂíåË°ÄÁÆ°ÂèçÊò†Ë∫´È´îÁöÑÂæÆË°ÄÁÆ°ÂíåÂ∑®Ë°ÄÁÆ°ÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇÂÆÉÂÄëÂèØÁî®ÊñºË®∫Êñ∑Á≥ñÂ∞øÁóÖ‰ΩµÁôºÁóáÔºåÂåÖÊã¨Á≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆäÔºàDRÔºâ„ÄÅÁ•ûÁ∂ìÁóÖËÆä„ÄÅËÖéÁóÖÂíåÂãïËÑàÁ≤•Ê®£Á°¨ÂåñÊÄßÂøÉË°ÄÁÆ°ÁñæÁóÖÔºå‰ª•ÂèäÈ†êÊ∏¨ÂøÉË°ÄÁÆ°‰∫ã‰ª∂ÁöÑÈ¢®Èö™„ÄÇÁÇ∫‰ΩøÁî®Êï∏‰ΩçÂåñË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÈÄ≤Ë°åÈ´òÈÄöÈáè DR Ê™¢Ê∏¨ËÄåÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂïüÁî®Á≥ªÁµ±Â∑≤Âú®Ëá®Â∫äÊé°Áî®„ÄÇÈô§‰∫Ü DR ÁØ©Ê™¢Â§ñÔºåAI Êï¥Âêà‰πüÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ‰æÜÊáâÂ∞çËàáÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÊï¥È´îÁÖßË≠∑Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÂÖ®Èù¢ÂõûÈ°ßÂü∫ÊñºË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÁöÑ AI ÊáâÁî®Áõ∏ÈóúÁ†îÁ©∂ÁöÑÊñáÁçªÔºåÈÄô‰∫õÁ†îÁ©∂ËàáÁ≥ñÂ∞øÁóÖÁöÑË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜÊúâÈóú„ÄÇÊàëÂÄëÂ∞áÊèèËø∞Êï¥È´î AI ËºîÂä©Á≥ñÂ∞øÁóÖÁÖßË≠∑ÁöÑÁôºÁèæÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñº DR ÁØ©Ê™¢Ôºå‰∏¶Ë®éË´ñÂØ¶ÊñΩÊ≠§È°ûÁ≥ªÁµ±ÁöÑÈöúÁ§ôÔºåÂåÖÊã¨ËàáÂÄ´ÁêÜ„ÄÅË≥áÊñôÈö±ÁßÅ„ÄÅÂÖ¨Âπ≥Â≠òÂèñÂíåÂèØËß£ÈáãÊÄßÊúâÈóúÁöÑÂïèÈ°å„ÄÇÈÄèÈÅéË©ï‰º∞ÊÇ£ËÄÖÁöÑÂÅ•Â∫∑ÁãÄÊ≥ÅÔºåÂêåÊôÇËÄÉÈáèÁ≥ñÂ∞øÁóÖ‰ΩµÁôºÁóá‰ª•ÂèäÊú™‰æÜÂøÉË°ÄÁÆ°‰ΩµÁôºÁóáÁöÑÈ¢®Èö™È†êÂæåÔºåAI ËºîÂä©Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂàÜÊûêÊúâÊΩõÂäõÊàêÁÇ∫Á≥ñÂ∞øÁóÖÊÇ£ËÄÖÁèæ‰ª£ÂåñÂÄã‰∫∫ÂåñÈÜ´ÁôÇÁöÑ‰∏≠ÂøÉÂ∑•ÂÖ∑„ÄÇ

##### **Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**
2402.15027v2 by A. J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de Arana, AM. Lesage, S. Senecal, P-M. Leger

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÂæûÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑËßíÂ∫¶Êé¢Ë®é‰∏çÂêåÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊáâÁî®Âú®ÊïôËÇ≤‰∏äÁöÑÂèØÊé•ÂèóÊÄßÔºåÂåÖÊã¨Â≠∏Áîü„ÄÅËÄÅÂ∏´ÂíåÂÆ∂Èï∑„ÄÇÊâøË™ç AI Âú®ÊïôËÇ≤‰∏äÁöÑËΩâÂûãÊΩõÂäõÔºåÂÆÉËß£Ê±∫‰∫ÜËàáË≥áÊñôÈö±ÁßÅ„ÄÅAI ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíå AI ÁöÑÈÅìÂæ∑ÈÉ®ÁΩ≤Áõ∏ÈóúÁöÑÁñëÊÖÆ„ÄÇÈÄèÈÅéÂ∞èÊèíÊõ≤ÊñπÊ≥ïÔºåÂèÉËàáËÄÖË¢´ÂëàÁèæ‰∫ÜÂõõÁ®ÆÊÉÖÂ¢ÉÔºåÂÖ∂‰∏≠ AI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈö±ÁßÅÂèóÂà∞ÊìçÁ∏±„ÄÇÂú®ÊØèÂÄãÊÉÖÂ¢ÉÂæåÔºåÂèÉËàáËÄÖÂÆåÊàê‰∫Ü‰∏ÄÈ†ÖË™øÊü•ÔºåË©≤Ë™øÊü•ÊçïÊçâ‰∫Ü‰ªñÂÄëÂ∞ç AI ÁöÑÊï¥È´îÊïàÁî®„ÄÅÂÄã‰∫∫ÊïàÁî®„ÄÅÊ≠£Áæ©„ÄÅ‰ø°ÂøÉ„ÄÅÈ¢®Èö™ÂíåÂ¶ÇÊûúÂèØÁî®Ôºå‰ΩøÁî®ÊØèÂÄãÊÉÖÂ¢ÉÁöÑ AI ÁöÑÊÑèÂúñÁöÑÁúãÊ≥ï„ÄÇË≥áÊñôËíêÈõÜÂåÖÂê´‰æÜËá™Âêà‰ΩúÊ©üÊßãÂíåÁ§æÁæ§Â™íÈ´îÊ¥ªÂãïÁöÑ 1198 ‰ΩçÂ§öÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂèÉËàáËÄÖÁöÑÊúÄÁµÇÊ®£Êú¨Ôºå‰∏¶Â∞àÊ≥®ÊñºÂ∞çÂõõÂÄã AI ‰ΩøÁî®Ê°à‰æãÁöÑÂÄãÂà•ÂõûÊáâ„ÄÇÂ∞çË≥áÊñôÁöÑË™øËß£ÂàÜÊûêË°®ÊòéÔºåÂ∞ç AI ÁöÑÊé•ÂèóÂ∫¶Âíå‰ø°‰ªªÂú®Âà©ÂÆ≥Èóú‰øÇ‰∫∫ÂúòÈ´î‰πãÈñìÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÁôºÁèæÔºåAI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÈ´ò‰ΩéÁ®ãÂ∫¶‰πãÈñìÁöÑÈóúÈçµË™øËß£ËÄÖÔºå‰ª•Âèä‰ΩøÁî®‰∏çÂêåÊïôËÇ≤ AI ÁöÑÊÑèÂúñÔºåÂåÖÊã¨ÊÑüÁü•Âà∞ÁöÑÊï¥È´îÊïàÁî®„ÄÅÊ≠£Áæ©Âíå‰ø°ÂøÉ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™øÔºåÊé•Âèó AI Âú®ÊïôËÇ≤‰∏äÁöÑÊáâÁî®ÊòØ‰∏ÄÂÄãÂæÆÂ¶ô‰∏îÂ§öÈù¢ÂêëÁöÑÂïèÈ°åÔºåÈô§‰∫Ü‰∏çÂêåÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÁúãÊ≥ïÂ§ñÔºåÈÇÑÈúÄË¶Å‰ªîÁ¥∞ËÄÉÊÖÆÂÖ∑È´îÁöÑ AI ÊáâÁî®ÂèäÂÖ∂ÁâπÂæµ„ÄÇ

##### **Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**
2402.09474v2 by Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer

Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÂèØÁ©øÊà¥ÂºèÂñÆÂ∞éÁ®ãÂøÉÈõªÂúñ (ECG) Ë£ùÁΩÆÁöÑÈÅ†Á´ØÁóÖÊÇ£Áõ£Ê∏¨Âú®Êó©ÊúüÂÅµÊ∏¨ÂøÉËáüÁñæÁóÖÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØËàáÁî®ÊñºËá™ÂãïÂåñÂøÉËáüÁñæÁóÖÂÅµÊ∏¨ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÁµêÂêà‰ΩøÁî®ÊôÇ„ÄÇÂÖàÂâçÂ∑≤ÊúâÁ†îÁ©∂ÊáâÁî®Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑ AI ÊñπÊ≥ïÈÄ≤Ë°åÂøÉËáüÁñæÁóÖÂÅµÊ∏¨„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂ∞öÊú™Ë¢´Âª£Ê≥õÊé•ÂèóÁÇ∫Ëá®Â∫äË®∫Êñ∑ÁöÑÂèØÈù†ËºîÂä©Â∑•ÂÖ∑ÔºåÈÉ®ÂàÜÂéüÂõ†Âú®ÊñºÂúçÁπûË®±Â§ö AI ÊºîÁÆóÊ≥ïÁöÑÁï∂ÂâçÈªëÁÆ±ÊÑüÁü•„ÄÇÁâπÂà•ÊòØÔºåÊúâÂøÖË¶ÅÊâæÂá∫ÊúâÂä©ÊñºÂÅöÂá∫Ê∫ñÁ¢∫Ë®∫Êñ∑ÁöÑ ECG Ë®äËôüÈóúÈçµÁâπÂæµÔºåÂæûËÄåÂ¢ûÂº∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÔºå‰ª•Ê†πÊìöÂñÆÂ∞éÁ®ã ECG Ë≥áÊñôÊâæÂá∫ÂøÉÊàøÈ°´Âãï„ÄÇÊÆòÂ∑ÆÁ∂≤Ë∑Ø (ResNet) ÊñπÊ≥ï‰πüÂ∑≤ÈñãÁôºÂá∫‰æÜÔºå‰ª•‰æøËàáË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄô‰∫õÊ®°ÂûãÊáâÁî®Êñº Chapman-Shaoxing Ë≥áÊñôÈõÜÔºå‰ª•ÂàÜÈ°ûÂøÉÊàøÈ°´ÂãïÔºå‰ª•ÂèäÂè¶‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÂøÉÂæã‰∏çÊï¥ÔºåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÔºåÂíåÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãÁöÑÂøÉË∑≥„ÄÇÈÄô‰∫õÊ®°ÂûãËÉΩÂ§†ÊâæÂá∫Ê±∫ÂÆöÊúÄÁµÇÂàÜÈ°ûÁöÑÂøÉË∑≥ÈóúÈçµÂçÄÂüüÔºå‰∏¶Âº∑Ë™ø P Ê≥¢Âíå T Ê≥¢Ôºå‰ª•ÂèäÂøÉË∑≥ÊåÅÁ∫åÊôÇÈñìÂíåË®äËôüÊåØÂπÖÂú®ÂçÄÂàÜÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãËàáÂøÉÊàøÈ°´ÂãïÂíåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ</paragraph>

##### **Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**
2402.05127v1 by Aryan Agrawal

This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ÂÖàÈÄ≤Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÂíåÊ≤ªÁôÇÁöÑÊñ∞Ê®°ÂºèÔºöÁîüÊàêÂºèÈ†êË®ìÁ∑¥Transformer 4 (GPT-4)„ÄÅLlama 2 ËÅäÂ§©Ê©üÂô®‰∫∫Âíå Gemini„ÄÇÈÄô‰∫õ LLM Á∂ìÈÅéÂæÆË™øÔºåÂÖ∑ÂÇôÂ∞àÊ•≠ÊèêÁ§∫ÔºåÂèØË®∫Êñ∑„ÄÅËß£Èáã‰∏¶Âª∫Ë≠∞ÊÜÇÈ¨±ÁóáÁöÑÊ≤ªÁôÇ‰ªãÂÖ•ÊñπÊ≥ï„ÄÇ‰∏ÄÁ®ÆÁç®ÁâπÁöÑÂ∞ëÊ¨°ÊèêÁ§∫ÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÊ†πÊìö DSM-5 Ê®ôÊ∫ñÂàÜÊûêÂíåËß£ÈáãÊÜÇÈ¨±ÁóáÁãÄÁöÑËÉΩÂäõ„ÄÇÂú®‰∫íÂãïÈöéÊÆµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÂèÉËàáÂêåÁêÜÂøÉÂ∞çË©±ÁÆ°ÁêÜÔºåÂæû PsychDB ÂíåË™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊåáÂçóÁ≠âË≥áÊ∫ê‰∏≠Ê±≤ÂèñÔºå‰øÉÈÄ≤ËàáÁ∂ìÊ≠∑ÈáçÂ∫¶ÊÜÇÈ¨±ÁóáÁöÑ‰∫∫ÂÄëÁöÑÊîØÊåÅÊÄß‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÈÄôÈ†ÖÁ†îÁ©∂ÈÇÑ‰ªãÁ¥π‰∫Ü Illuminate Ë≥áÊñôÂ∫´ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®Æ CBT Ê®°ÁµÑÔºåÊúâÂä©ÊñºÂÄãÊÄßÂåñÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰ΩøÁî® F1 ÂàÜÊï∏„ÄÅÊ∫ñÁ¢∫Áéá„ÄÅÂè¨ÂõûÁéá„ÄÅÈ§òÂº¶Áõ∏‰ººÂ∫¶ÂíåÈù¢ÂêëÂè¨ÂõûÁéáÁöÑ Gisting Ë©ï‰º∞ÊõøË∫´ (ROUGE) Á≠âÊåáÊ®ôÔºåÂú®‰∏çÂêåÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠Ë©ï‰º∞ LLM ÁöÑË°®ÁèæÔºåË≠âÊòé‰∫ÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄß„ÄÇÈÄôÁ®ÆÁ∂úÂêàÊñπÊ≥ïÁµêÂêà‰∫ÜÂ∞ñÁ´ØÁöÑ AI ËàáÊó¢ÂÆöÁöÑÂøÉÁêÜÊñπÊ≥ïÔºåÁÇ∫ÂøÉÁêÜ‰øùÂÅ•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºå‰∏¶Â±ïÁ§∫‰∫Ü LLM Âú®Èù©Êñ∞ÊÜÇÈ¨±ÁóáË®∫Êñ∑ÂíåÊ≤ªÁôÇÁ≠ñÁï•ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**
2401.13324v4 by Timoth√©e Schmude, Laura Koesten, Torsten M√∂ller, Sebastian Tschiatschek

Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÁöÑË™™ÊòéÂæàÂ∞ëËÉΩÊªøË∂≥ÂèóÊºîÁÆóÊ≥ïÊ±∫Á≠ñ (ADM) ÂΩ±ÈüøÁöÑ‰∫∫ÂÄëÁöÑË≥áË®äÈúÄÊ±Ç„ÄÇÂÇ≥ÈÅîÁöÑË≥áË®äËàáÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÈáçË¶ÅÁöÑË≥áË®ä‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÂèØËÉΩÊúÉÈòªÁ§ô‰∫ÜËß£ÂíåÈÅµÂÆàÊ≥ïË¶èÊû∂ÊßãÔºå‰æãÂ¶Ç‰∫∫Â∑•Êô∫ÊÖßÊ≥ïÊ°à„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü„ÄåXAI ÂàùÂ≠∏ËÄÖÂïèÈ°åÂ∫´„ÄçÔºöÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫Ë≥áË®äÈúÄÊ±ÇÁöÑÁõÆÈåÑÔºåÊ∂µËìãÂÖ©ÂÄã ADM ‰ΩøÁî®Ê°à‰æãÔºàÂ∞±Ê•≠È†êÊ∏¨ÂíåÂÅ•Â∫∑Áõ£Ê∏¨ÔºâÔºåÊ∂µËìãË≥áÊñô„ÄÅÁ≥ªÁµ±ËÑàÁµ°„ÄÅÁ≥ªÁµ±‰ΩøÁî®ÂíåÁ≥ªÁµ±Ë¶èÊ†ºÈ°ûÂà•„ÄÇË≥áË®äÈúÄÊ±ÇÊòØÈÄèÈÅéË®™Ë´áÁ†îÁ©∂Êî∂ÈõÜÁöÑÔºåÂèÉËàáËÄÖÂú®Ë©¢ÂïèÂæåÊî∂Âà∞Ë™™Êòé„ÄÇÂèÉËàáËÄÖÈÄ≤‰∏ÄÊ≠•ÂõûÂ†±‰ªñÂÄëÁöÑÁêÜËß£ÂíåÊ±∫Á≠ñ‰ø°ÂøÉÔºåÈ°ØÁ§∫ÈõñÁÑ∂Âú®Êî∂Âà∞Ë™™ÊòéÂæå‰ø°ÂøÉÂÇæÂêëÊñºÂ¢ûÂä†Ôºå‰ΩÜÂèÉËàáËÄÖ‰πüÈÅáÂà∞‰∫ÜÁêÜËß£ÊåëÊà∞Ôºå‰æãÂ¶ÇÁÑ°Ê≥ïË™™ÊòéÁÇ∫‰ªÄÈ∫º‰ªñÂÄëÁöÑÁêÜËß£ÊÑüË¶∫‰∏çÂÆåÊï¥„ÄÇË™™ÊòéÈÄ≤‰∏ÄÊ≠•ÂΩ±ÈüøÂèÉËàáËÄÖÂ∞çÁ≥ªÁµ±È¢®Èö™ÂíåÂ•ΩËôïÁöÑÁúãÊ≥ïÔºå‰ªñÂÄëÊúÉÊ†πÊìö‰ΩøÁî®Ê°à‰æãÁ¢∫Ë™çÊàñÊîπËÆäÈÄô‰∫õÁúãÊ≥ï„ÄÇÁï∂È¢®Èö™Ë¢´Ë™çÁÇ∫ÂæàÈ´òÊôÇÔºåÂèÉËàáËÄÖË°®Á§∫ÁâπÂà•ÊúâËààË∂£‰∫ÜËß£ÊÑèÂúñÁöÑË™™ÊòéÔºå‰æãÂ¶ÇÁÇ∫‰ªÄÈ∫º‰ª•ÂèäÁÇ∫‰∫Ü‰ªÄÈ∫ºÁõÆÁöÑËÄåÂª∫Á´ãÁ≥ªÁµ±„ÄÇÈÄèÈÅéÈÄôÈ†ÖÂ∑•‰ΩúÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÂú®Ê±∫Á≠ñÊé°Áî® ADM Á≥ªÁµ±ÊôÇÊèê‰æõÁõ∏ÈóúË≥áË®äÂíåÊåëÊà∞ÁöÑÊ¶ÇË¶ΩÔºå‰æÜÊîØÊè¥Â∞áÂèóÂΩ±ÈüøÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫Á¥çÂÖ•ÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÊúÄÂæåÁ∏ΩÁµêÊàëÂÄëÁöÑÁôºÁèæÔºåÂàóÂá∫ÂÖ≠È†ÖÈóúÈçµÂΩ±ÈüøÔºåÈÄô‰∫õÂΩ±ÈüøÊúÉÂëäÁü•Êú™‰æÜÈáùÂ∞çÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂèóÁúæË™™ÊòéÁöÑË®≠Ë®à„ÄÇ</paragraph>

##### **Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**
2401.02985v1 by Vahid Ashrafimoghari, Necdet G√ºrkan, Jordan W. Suchow

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂø´ÈÄüÊºîÈÄ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁîüÊàêÂºè AI ÁöÑÈ†òÂüüÔºåÁÇ∫ÂêÑÂÄãÈ†òÂüüÁöÑÊáâÁî®ÈñãÂïü‰∫ÜÊñ∞ÈÄîÂæëÔºå‰ΩÜÂÖ∂Âú®ÂïÜÊ•≠ÊïôËÇ≤‰∏≠ÁöÑËßíËâ≤‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂È¶ñÊ¨°ÂºïÂÖ•‰∫ÜÂü∫Ê∫ñÔºåÁî®‰ª•Ë©ï‰º∞‰∏ÉÂÄã‰∏ªË¶Å LLM ÁöÑÊïàËÉΩÔºåÂåÖÊã¨ OpenAI ÁöÑÊ®°Âûã (GPT-3.5 Turbo„ÄÅGPT-4 Âíå GPT-4 Turbo)„ÄÅGoogle ÁöÑÊ®°Âûã (PaLM 2„ÄÅGemini 1.0 Pro) Âíå Anthropic ÁöÑÊ®°Âûã (Claude 2 Âíå Claude 2.1)ÔºåÈÄô‰∫õÊ®°ÂûãÂ∞áÁî®ÊñºÁ†îÁ©∂ÁîüÂïÜÊ•≠Ë™≤Á®ãÂÖ•Â≠∏Á®ãÂ∫è‰∏≠ÁöÑÈóúÈçµËÄÉË©¶ GMAT„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§ßÂ§öÊï∏ LLM ÁöÑË°®ÁèæÈÉΩÂÑ™Êñº‰∫∫È°ûËÄÉÁîüÔºåÂÖ∂‰∏≠ GPT-4 Turbo ‰∏çÂÉÖÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåÊõ¥Ë∂ÖË∂ä‰∫ÜÈ†ÇÂ∞ñÂïÜÂ≠∏Èô¢ÁöÑÁ†îÁ©∂ÁîüÂπ≥ÂùáÂàÜÊï∏„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü GPT-4 Turbo Âú®Ëß£ÈáãÁ≠îÊ°à„ÄÅË©ï‰º∞ÂõûÊáâ„ÄÅËæ®Ë≠òÈåØË™§„ÄÅË™øÊï¥Ë™™ÊòéÂíåÁî¢ÁîüÊõø‰ª£ÊÉÖÂ¢ÉÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËàáÂâç‰∏Ä‰ª£ÁâàÊú¨Áõ∏ÊØîÔºåÊúÄÊñ∞ÁöÑ LLM ÁâàÊú¨ GPT-4 Turbo„ÄÅClaude 2.1 Âíå Gemini 1.0 Pro Âú®Êé®ÁêÜ‰ªªÂãôÊñπÈù¢ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÂá∏È°Ø‰∫ÜÂÖ∂Âú®Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÂÑòÁÆ° AI Âú®ÊïôËÇ≤„ÄÅË©ïÈáèÂíåËºîÂ∞éÊñπÈù¢ÁöÑÊâøË´æÂæàÊòéÁ¢∫Ôºå‰ΩÜ‰ªçÊúâÊåëÊà∞Â≠òÂú®„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ÁöÑÂ≠∏Ë°ìÊΩõÂäõÔºå‰πüÂº∑Ë™ø‰∫ÜÂú®ÊïôËÇ≤‰∏≠ÂØ©ÊÖéÈñãÁôºÂíåÊáâÁî® AI ÁöÑÂøÖË¶ÅÊÄß„ÄÇÈö®Ëëó AI ÊäÄË°ìÁöÑÈÄ≤Ê≠•ÔºåÂª∫Á´ã AI ‰∫íÂãïÁöÑÊû∂ÊßãÂíåÂçîÂÆö„ÄÅÈ©óË≠â AI ÁîüÊàêÁöÑÂÖßÂÆπÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÁ¢∫‰øùÂÖ®ÁêÉÂêÑÂú∞Â§öÂÖÉÂ≠∏ÁøíËÄÖÁöÑÂ≠òÂèñÊ¨äÔºå‰ª•ÂèäÂâµÈÄ†‰∏ÄÂÄã AI ÊîØÊåÅ‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠òÁöÑÊïôËÇ≤Áí∞Â¢ÉËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÁÇ∫ÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢Ë≤†Ë≤¨‰ªªÂú∞‰ΩøÁî® AI ‰æÜË±êÂØåÊïôËÇ≤È´îÈ©ó‰∏¶ÊîπÂñÑËÄÉË©¶Ê∫ñÂÇôÂíåË©ïÈáèÊñπÊ≥ïÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **XAI for In-hospital Mortality Prediction via Multimodal ICU Data**
2312.17624v1 by Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He

Predicting in-hospital mortality for intensive care unit (ICU) patients is
key to final clinical outcomes. AI has shown advantaged accuracy but suffers
from the lack of explainability. To address this issue, this paper proposes an
eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,
explainable AI solution for predicting in-hospital mortality via multimodal ICU
data. We employ multimodal learning in our framework, which can receive
heterogeneous inputs from clinical data and make decisions. Furthermore, we
introduce an explainable method, namely Layer-Wise Propagation to Transformer,
as a proper extension of the LRP method to Transformers, producing explanations
over multimodal inputs and revealing the salient features attributed to
prediction. Moreover, the contribution of each modality to clinical outcomes
can be visualized, assisting clinicians in understanding the reasoning behind
decision-making. We construct a multimodal dataset based on MIMIC-III and
MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on
benchmark datasets demonstrate that our proposed framework can achieve
reasonable interpretation with competitive prediction accuracy. In particular,
our framework can be easily transferred to other clinical tasks, which
facilitates the discovery of crucial factors in healthcare research.

ÊëòË¶ÅÔºöÈ†êÊ∏¨Âä†Ë≠∑ÁóÖÊàø (ICU) ÁóÖÊÇ£ÁöÑÈô¢ÂÖßÊ≠ª‰∫°ÁéáÊòØÊúÄÁµÇËá®Â∫äÁµêÊûúÁöÑÈóúÈçµ„ÄÇAI Â∑≤Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂçªÁº∫‰πèÂèØËß£ÈáãÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÂ§öÊ®°ÂºèÊ≠ª‰∫°ÁéáÈ†êÊ∏¨Âô® (X-MMP)ÔºåÊé°Áî®ÊúâÊïà‰∏îÂèØËß£ÈáãÁöÑ AI ÊñπÂºèÔºåËóâÁî±Â§öÊ®°Âºè ICU Ë≥áÊñô‰æÜÈ†êÊ∏¨Èô¢ÂÖßÊ≠ª‰∫°Áéá„ÄÇÊàëÂÄëÂú®Êû∂Êßã‰∏≠Êé°Áî®Â§öÊ®°ÂºèÂ≠∏ÁøíÔºåÂèØ‰ª•Êé•Êî∂‰æÜËá™Ëá®Â∫äË≥áÊñôÁöÑÁï∞Ë≥™Ëº∏ÂÖ•‰∏¶ÂÅöÂá∫Ê±∫Á≠ñ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂàÜÂ±§ÂÇ≥Êí≠Ëá≥ TransformerÔºå‰ΩúÁÇ∫ LRP ÊñπÊ≥ïÈÅ©Áï∂Âú∞Âª∂‰º∏Ëá≥ TransformerÔºåÂ∞çÂ§öÊ®°ÂºèËº∏ÂÖ•Áî¢ÁîüËß£ÈáãÔºå‰∏¶Êè≠Èú≤Ê≠∏Âõ†ÊñºÈ†êÊ∏¨ÁöÑÈ°ØËëóÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊØèÂÄãÊ®°ÂºèÂ∞çËá®Â∫äÁµêÊûúÁöÑË≤¢ÁçªÂèØ‰ª•Ë¶ñË¶∫ÂåñÔºåÂçîÂä©Ëá®Â∫äÈÜ´Â∏´‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÁêÜÁî±„ÄÇÊàëÂÄëÊ†πÊìö MIMIC-III Âíå MIMIC-III Ê≥¢ÂΩ¢Ë≥áÊñôÂ∫´ÊØîÂ∞çÂ≠êÈõÜÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÂºèË≥áÊñôÈõÜ„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂÖ®Èù¢ÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂèØ‰ª•ÈÅîÊàêÂêàÁêÜÁöÑË©ÆÈáãÔºå‰∏¶ÂÖ∑ÂÇôÁ´∂Áà≠ÂäõÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂèØ‰ª•ËºïÈ¨ÜÂú∞ËΩâÁßªÂà∞ÂÖ∂‰ªñËá®Â∫ä‰ªªÂãôÔºåÈÄôÊúâÂä©ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂‰∏≠ÁôºÁèæÈóúÈçµÂõ†Á¥†„ÄÇ

##### **Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**
2401.09450v2 by Norman Zerbe, Lars Ole Schwen, Christian Gei√üler, Katja Wiesemann, Tom Bisson, Peter Boor, Rita Carvalho, Michael Franz, Christoph Jansen, Tim-Rasmus Kiehl, Bj√∂rn Lindequist, Nora Charlotte Pohlan, Sarah Schmell, Klaus Strohmenger, Falk Zakrzewski, Markus Plass, Michael Takla, Tobias K√ºster, Andr√© Homeyer, Peter Hufnagl

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåÁóÖÁêÜÂ≠∏‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÂ∑≤Â§ßÂπÖÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË®±Â§öÊåëÊà∞ÔºåÂåÖÊã¨Â∞áÁ†îÁ©∂ÁµêÊûúËΩâÂåñÁÇ∫Ëá®Â∫äË®∫Êñ∑Áî¢ÂìÅÂú®ÊäÄË°ìÂíåÊ≥ïË¶èÊñπÈù¢ÁöÑÈöúÁ§ôÔºå‰ª•ÂèäÁº∫‰πèÊ®ôÊ∫ñÂåñ‰ªãÈù¢ÔºåÂ∞éËá¥Êï¥ÂêàÂà∞Â∏∏Ë¶èËá®Â∫äÂØ¶Âãô‰∏≠ÈÄ≤Â±ïÁ∑©ÊÖ¢„ÄÇÈñãÊîæ‰∏îËàá‰æõÊáâÂïÜÁÑ°ÈóúÁöÑ EMPAIA Ë®àÁï´ÊáâÂ∞ç‰∫ÜÈÄô‰∫õÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèê‰æõ EMPAIA ÁöÑÊàêÂ∞±ÂíåÁ∂ìÈ©óÊïôË®ìÁöÑÊ¶ÇËø∞„ÄÇEMPAIA Êï¥Âêà‰∫ÜÁóÖÁêÜÂ≠∏ AI ÁîüÊÖãÁ≥ªÁµ±ÁöÑÂêÑÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÔºåÂç≥ÁóÖÁêÜÂ≠∏ÂÆ∂„ÄÅÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÂíåÁî¢Ê•≠„ÄÇÂú®ÂØÜÂàáÂêà‰Ωú‰∏ãÔºåÊàëÂÄëÂà∂ÂÆö‰∫ÜÊäÄË°ì‰∫íÈÄöÊÄßÊ®ôÊ∫ñ„ÄÅAI Ê∏¨Ë©¶ÂíåÁî¢ÂìÅÈñãÁôºÂª∫Ë≠∞Ôºå‰ª•ÂèäÂèØËß£ÈáãÊÄßÊñπÊ≥ï„ÄÇÊàëÂÄëÂØ¶‰Ωú‰∫ÜÊ®°ÁµÑÂåñ‰∏îÈñãÊîæÂéüÂßãÁ¢ºÁöÑ EMPAIA Âπ≥Ëá∫Ôºå‰∏¶ÊàêÂäüÊï¥Âêà‰∫Ü‰æÜËá™ 8 ÂÄã‰∏çÂêå‰æõÊáâÂïÜÁöÑ 14 ÂÄãÂü∫Êñº AI ÁöÑÂΩ±ÂÉèÂàÜÊûêÊáâÁî®Á®ãÂºèÔºåÂ±ïÁ§∫‰∫Ü‰∏çÂêåÁöÑÊáâÁî®Á®ãÂºèÂ¶Ç‰Ωï‰ΩøÁî®ÂñÆ‰∏ÄÁöÑÊ®ôÊ∫ñÂåñ‰ªãÈù¢„ÄÇÊàëÂÄëÂÑ™ÂÖàËÄÉÊÖÆÈúÄÊ±ÇÔºå‰∏¶Ë©ï‰º∞‰∫Ü AI Âú®Ê≠êÊ¥≤Âíå‰∫ûÊ¥≤ÁöÑ 14 ÂÄã‰∏çÂêåÁóÖÁêÜÂØ¶È©óÂÆ§‰∏≠ÁöÑÂØ¶ÈöõËá®Â∫äÊáâÁî®„ÄÇÈô§‰∫ÜÊäÄË°ìÈñãÁôºÂ§ñÔºåÊàëÂÄëÈÇÑÁÇ∫ÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫Âª∫Á´ã‰∫Ü‰∏ÄÂÄãË´ñÂ£áÔºå‰ª•ÂàÜ‰∫´Êï∏‰ΩçÁóÖÁêÜÂ≠∏Âíå AI ÁöÑË≥áË®äÂíåÁ∂ìÈ©ó„ÄÇÂïÜÊ•≠„ÄÅËá®Â∫äÂíåÂ≠∏Ë°ìÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁèæÂú®ÂèØ‰ª•Êé°Áî® EMPAIA ÁöÑÂ∏∏Ë¶ãÈñãÊîæÂéüÂßãÁ¢º‰ªãÈù¢ÔºåÈÄôÁÇ∫Â§ßË¶èÊ®°Ê®ôÊ∫ñÂåñÂíåÁ∞°ÂåñÊµÅÁ®ãÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑÊ©üÊúÉ„ÄÇÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÂä™ÂäõÊâçËÉΩÊúâÊïà‰∏îÂª£Ê≥õÂú∞Âª∫Á´ã‰æãË°åÂØ¶È©óÂÆ§‰ΩøÁî®‰∏≠ÁöÑ AI ËºîÂä©„ÄÇÁÇ∫Ê≠§ÔºåÂ∑≤ÊàêÁ´ãÈùûÁáüÂà©ÂçîÊúÉ EMPAIA InternationalÔºå‰ª•‰ΩúÁÇ∫Ê∞∏Á∫åÂü∫Á§éÊû∂ÊßãÔºåÁπºÁ∫åÈÄ≤Ë°åÊ®ôÊ∫ñÂåñÔºå‰∏¶ÊîØÊè¥Âª£Ê≥õÂØ¶‰ΩúÂíåÂÄ°Â∞é AI ËºîÂä©Êï∏‰ΩçÁóÖÁêÜÂ≠∏ÁöÑÊú™‰æÜ„ÄÇ

##### **Robust Stochastic Graph Generator for Counterfactual Explanations**
2312.11747v2 by Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo

Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.

ÊëòË¶ÅÔºöÂèç‰∫ãÂØ¶Ëß£Èáã (CE) ÊäÄË°ìÂ∑≤ÂºïËµ∑ÈóúÊ≥®Ôºå‰ΩúÁÇ∫‰∏ÄÁ®ÆÁÇ∫Ëàá AI Á≥ªÁµ±‰∫íÂãïÁöÑ‰ΩøÁî®ËÄÖÊèê‰æõË¶ãËß£ÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåËá™ÂãïÈßïÈßõÊ±ΩËªäÁ≠âÈ†òÂüüÂª£Ê≥õÁ†îÁ©∂ÔºåÂúñÂΩ¢Âèç‰∫ãÂØ¶Ëß£Èáã (GCE) ÊñπÊ≥ïÁõ∏Â∞çËºÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇGCE ÊúÉÁî¢Áîü‰∏ÄÂÄãÈ°û‰ººÊñºÂéüÂßãÂúñÂΩ¢ÁöÑÊñ∞ÂúñÂΩ¢Ôºå‰∏¶Ê†πÊìöÂü∫Á§éÈ†êÊ∏¨Ê®°ÂûãÁî¢Áîü‰∏çÂêåÁöÑÁµêÊûú„ÄÇÂú®ÈÄô‰∫õ GCE ÊäÄË°ì‰∏≠ÔºåÂÑòÁÆ°Âú®ÂÖ∂‰ªñÈ†òÂüüÔºà‰æãÂ¶ÇËóùË°ìÈ¢®Ê†ºÂíåËá™ÁÑ∂Ë™ûË®ÄÂª∫Ê®°Ôºâ‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊàêÂ∞±Ôºå‰ΩÜÊ§çÂü∫ÊñºÁîüÊàêÊ©üÂà∂ÁöÑÊäÄË°ìÁç≤ÂæóÁöÑÈóúÊ≥®Áõ∏Â∞çÊúâÈôê„ÄÇÂ∞çÁîüÊàêÂºèËß£ÈáãÂô®ÁöÑÂÅèÂ•ΩÊ∫êÊñºÂÆÉÂÄëÂú®Êé®ÁêÜÊúüÈñìÁî¢ÁîüÂèç‰∫ãÂØ¶ÂØ¶‰æãÁöÑËÉΩÂäõÔºåÂà©Áî®Ëº∏ÂÖ•ÂúñÂΩ¢ÁöÑËá™‰∏ªÁç≤ÂèñÊìæÂãï„ÄÇÂü∫Êñº‰∏äËø∞ÁêÜÁî±ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü RSGG-CEÔºå‰∏ÄÁ®ÆÁî®ÊñºÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÊñ∞ÂûãÁ©©ÂÅ•Èö®Ê©üÂúñÂΩ¢ÁîüÊàêÂô®ÔºåËÉΩÂ§†ÂæûÂ≠∏ÁøíÂà∞ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Áî¢ÁîüÂèç‰∫ãÂØ¶ÁØÑ‰æãÔºåËÄÉÊÖÆÈÉ®ÂàÜÊúâÂ∫èÁöÑÁîüÊàêÂ∫èÂàó„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÂÆöÈáèÂíåÂÆöÊÄßÂàÜÊûêÔºå‰ª•ÊØîËºÉ RSGG-CE ÁöÑÊïàËÉΩËàá SoA ÁîüÊàêÂºèËß£ÈáãÂô®ÔºåÂº∑Ë™øÂÖ∂Â¢ûÂº∑‰∫ÜÁî¢ÁîüÂêàÁêÜËß£ÈáãÂÄôÈÅ∏ÁöÑËÉΩÂäõ„ÄÇ

##### **Evaluating the Utility of Model Explanations for Model Development**
2312.06032v1 by Shawn Im, Jacob Andreas, Yilun Zhou

One of the motivations for explainable AI is to allow humans to make better
and more informed decisions regarding the use and deployment of AI models. But
careful evaluations are needed to assess whether this expectation has been
fulfilled. Current evaluations mainly focus on algorithmic properties of
explanations, and those that involve human subjects often employ subjective
questions to test human's perception of explanation usefulness, without being
grounded in objective metrics and measurements. In this work, we evaluate
whether explanations can improve human decision-making in practical scenarios
of machine learning model development. We conduct a mixed-methods user study
involving image data to evaluate saliency maps generated by SmoothGrad,
GradCAM, and an oracle explanation on two tasks: model selection and
counterfactual simulation. To our surprise, we did not find evidence of
significant improvement on these tasks when users were provided with any of the
saliency maps, even the synthetic oracle explanation designed to be simple to
understand and highly indicative of the answer. Nonetheless, explanations did
help users more accurately describe the models. These findings suggest caution
regarding the usefulness and potential for misunderstanding in saliency-based
explanations.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI ÁöÑÂãïÊ©ü‰πã‰∏ÄÊòØËÆì‰∫∫ÂÄëÂú®‰ΩøÁî®ÂíåÈÉ®ÁΩ≤ AI Ê®°ÂûãÊôÇÂÅöÂá∫Êõ¥Â•Ω„ÄÅÊõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇ‰ΩÜÈúÄË¶Å‰ªîÁ¥∞Ë©ï‰º∞‰ª•Ë©ï‰º∞ÊòØÂê¶Â∑≤ÈÅîÂà∞Ê≠§È†êÊúü„ÄÇÁõÆÂâçÁöÑË©ï‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëß£ÈáãÁöÑÊºîÁÆóÊ≥ïÁâπÊÄßÔºåËÄåÊ∂âÂèä‰∫∫È°ûÂèóË©¶ËÄÖÁöÑË©ï‰º∞ÈÄöÂ∏∏Êé°Áî®‰∏ªËßÄÂïèÈ°å‰æÜÊ∏¨Ë©¶‰∫∫È°ûÂ∞çËß£ÈáãÊúâÁî®ÊÄßÁöÑÁúãÊ≥ïÔºåËÄåÊ≤íÊúâÂü∫ÊñºÂÆ¢ËßÄÊåáÊ®ôÂíåÊ∏¨Èáè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË©ï‰º∞Ëß£ÈáãÊòØÂê¶ÂèØ‰ª•Âú®Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÈñãÁôºÁöÑÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÊîπÂñÑ‰∫∫È°ûÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÊ∂âÂèäÂΩ±ÂÉèË≥áÊñôÁöÑÊ∑∑ÂêàÊñπÊ≥ï‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ SmoothGrad„ÄÅGradCAM ÂíåÈ†êË®ÄËß£ÈáãÂú®ÂÖ©ÂÄã‰ªªÂãô‰∏≠Áî¢ÁîüÁöÑÈ°ØËëóÊÄßÂúñÔºöÊ®°ÂûãÈÅ∏ÊìáÂíåÂèç‰∫ãÂØ¶Ê®°Êì¨„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÊ≤íÊúâÁôºÁèæ‰ªª‰ΩïÈ°ØËëóÊÄßÂúñÔºàÂç≥‰ΩøÊòØË®≠Ë®àÁÇ∫ÊòìÊñºÁêÜËß£‰∏îÈ´òÂ∫¶ÊåáÁ§∫Á≠îÊ°àÁöÑÂêàÊàêÈ†êË®ÄËß£ÈáãÔºâËÉΩËÆì‰ΩøÁî®ËÄÖÂú®ÈÄô‰∫õ‰ªªÂãô‰∏äÈ°ØËëóÊîπÂñÑÁöÑË≠âÊìö„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåËß£ÈáãÁ¢∫ÂØ¶ÊúâÂä©Êñº‰ΩøÁî®ËÄÖÊõ¥Ê∫ñÁ¢∫Âú∞ÊèèËø∞Ê®°Âûã„ÄÇÈÄô‰∫õÁôºÁèæÊèêÁ§∫ÊàëÂÄëË¶ÅÂ∞çÂü∫ÊñºÈ°ØËëóÊÄßÁöÑËß£Èáã‰∏≠ÂèØËÉΩÂ≠òÂú®Ë™§Ëß£ÁöÑÊúâÁî®ÊÄß‰øùÊåÅË¨πÊÖé„ÄÇ

##### **Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**
2312.06798v1 by Manas Gaur, Amit Sheth

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

ÊëòË¶ÅÔºöÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÂª∫Á´ã‰ø°‰ªª„ÄÇÈÄô‰∫õÈúÄË¶Å‰∏ÄÂÄãÊ®°Âûã‰æÜÂ±ïÁ§∫‰∏ÄËá¥ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∫õÔºåÊúâÂøÖË¶Å‰ΩøÁî®ÂíåÂàÜÊûêÊï∏ÊìöÂíåÁü•Ë≠òÔºå‰∏¶‰ΩøÁî®Ëàá AI ÊáâÁî®Áõ∏ÈóúÁöÑÁµ±Ë®àÂíåÁ¨¶Ëôü AI ÊñπÊ≥ï - ÂñÆÁç®‰ΩøÁî®‰ªª‰Ωï‰∏ÄÁ®ÆÊñπÊ≥ïÈÉΩ‰∏çÊúÉÂ•èÊïà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰∏ªÂºµ‰∏¶Ë©¶ÂúñË≠âÊòé NeuroSymbolic AI ÊñπÊ≥ïÊõ¥ÈÅ©ÂêàÊñº‰Ωø AI ÊàêÁÇ∫Âèó‰ø°‰ªªÁöÑ AI Á≥ªÁµ±„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü CREST Ê°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄËá¥ÊÄß„ÄÅÂèØÈù†ÊÄß„ÄÅ‰ΩøÁî®ËÄÖÂ±§Á¥öÁöÑÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÊòØÂ¶Ç‰ΩïÂª∫Á´ãÂú® NeuroSymbolic ÊñπÊ≥ï‰∏äÁöÑÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Êï∏ÊìöÂíåÁü•Ë≠ò‰æÜÊîØÊåÅÈóúÈçµÊáâÁî®Ôºà‰æãÂ¶ÇÂÅ•Â∫∑ÂíåÁ¶èÁ•âÔºâÁöÑË¶ÅÊ±Ç„ÄÇÊú¨ÊñáÈáçÈªûÈóúÊ≥®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂõ†ÁÇ∫ÂÆÉÊòØ CREST Ê°ÜÊû∂‰∏≠ÈÅ∏ÊìáÁöÑ AI Á≥ªÁµ±„ÄÇLLM Âõ†ÂÖ∂Âú®ËôïÁêÜÂª£Ê≥õÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Â†¥ÊôØÊñπÈù¢ÁöÑÂ§öÂäüËÉΩÊÄßËÄåÂÇôÂèóÁ†îÁ©∂‰∫∫Âì°ÁöÑÈóúÊ≥®„ÄÇ‰æãÂ¶ÇÔºåChatGPT Âíå Google ÁöÑ MedPaLM Â∑≤ÊàêÁÇ∫Êèê‰æõ‰∏ÄËà¨ÂíåÂÅ•Â∫∑Áõ∏ÈóúÊü•Ë©¢‰ø°ÊÅØÁöÑÊ•µÊúâÂ∏åÊúõÁöÑÂπ≥Âè∞„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊ®°Âûã‰ªçÁÑ∂ÊòØÈªëÁõíÂ≠êÔºåÂÑòÁÆ°Á¥çÂÖ•‰∫Ü‰∫∫È°ûÂèçÈ•ãÂíåÊåá‰ª§ÂºïÂ∞éÁöÑË™øÊï¥„ÄÇ‰æãÂ¶ÇÔºåÂÑòÁÆ°Âà∂ÂÆö‰∫ÜÂÆâÂÖ®Èò≤Ë≠∑Êé™ÊñΩÔºåChatGPT ‰ªçÂèØËÉΩÁî¢Áîü‰∏çÂÆâÂÖ®ÁöÑÂõûÊáâ„ÄÇCREST ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêàÁêÜÁöÑÊñπÊ≥ïÔºåÂú® NeuroSymbolic Ê°ÜÊû∂‰∏≠Âà©Áî®Á®ãÂ∫èÂíåÂü∫ÊñºÂúñË°®ÁöÑÁü•Ë≠òÔºå‰ª•Èó°ÊòéËàá LLM Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇ

##### **Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**
2311.17133v1 by Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë∞ÉÊü•‰∫ÜÂú® COVID-19 Áñ´ÊÉÖÊúüÈó¥Âèä‰ª•ÂêéÈ¢ÑÊµãÊ≠ª‰∫°ÁéáÊó∂ÔºåÂ∑≤ÈÉ®ÁΩ≤‰∫∫Â∑•Êô∫ËÉΩ (AI) Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÅÂèØËß£ÈáäÊÄßÂíåÁ®≥ÂÅ•ÊÄß„ÄÇ‰Ωú‰∏∫ÂêåÁ±ªÁ†îÁ©∂‰∏≠ÁöÑÈ¶ñ‰æãÔºåÊàë‰ª¨ÂèëÁé∞Ë¥ùÂè∂ÊñØÁ•ûÁªèÁΩëÁªú (BNN) ÂíåÊô∫ËÉΩËÆ≠ÁªÉÊäÄÊúØËÆ©Êàë‰ª¨ÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÂèëÁîüÈáçÂ§ßÂèòÂåñÊó∂‰ªçËÉΩ‰øùÊåÅÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÂºÄÂèëÁ®≥ÂÅ•ÁöÑ AI Ê®°ÂûãÁöÑÈáçË¶ÅÊÄßÔºåÂç≥‰ΩøÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊù°‰ª∂‰∏ãÔºåËøô‰∫õÊ®°Âûã‰πüËÉΩÂåπÈÖçÊàñË∂ÖË∂ä‰∏¥Â∫äÂåªÁîüÁöÑÈ¢ÑÊµã„ÄÇÊàë‰ª¨ÂØπÊ®°ÂûãÂèØËß£ÈáäÊÄßÁöÑÊé¢Á¥¢Ë°®ÊòéÔºåÈöèÊú∫Ê®°Âûã‰ºö‰∫ßÁîüÊõ¥Â§öÊ†∑Âåñ‰∏î‰∏™ÊÄßÂåñÁöÑËß£ÈáäÔºå‰ªéËÄåÁ™ÅÂá∫‰∫ÜÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠Êèê‰æõËØ¶ÁªÜ‰∏î‰∏™ÊÄßÂåñËßÅËß£ÁöÑ AI Ê®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Âº∫Ë∞É‰∫ÜÈáèÂåñ AI Ê®°Âûã‰∏≠‰∏çÁ°ÆÂÆöÊÄßÁöÑÈáçË¶ÅÊÄßÔºåËøô‰Ωø‰∏¥Â∫äÂåªÁîüËÉΩÂ§üÊ†πÊçÆÂèØÈù†ÁöÑÈ¢ÑÊµãÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂÜ≥Á≠ñ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊèêÂÄ°Âú®ÂåªÁñó‰øùÂÅ•ÁöÑ AI Á†îÁ©∂‰∏≠‰ºòÂÖàËÄÉËôëÂÆûÊñΩÁßëÂ≠¶ÔºåÂπ∂Á°Æ‰øù AI Ëß£ÂÜ≥ÊñπÊ°àÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠ÂÆûÁî®„ÄÅÊúâÁõä‰∏îÂèØÊåÅÁª≠„ÄÇÈÄöËøáËß£ÂÜ≥ÂåªÁñó‰øùÂÅ•ÁéØÂ¢É‰∏≠ÁöÑÁã¨ÁâπÊåëÊàòÂíåÂ§çÊùÇÊÄßÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•ÂºÄÂèëÂá∫ÊúâÊïàÊîπÂñÑ‰∏¥Â∫äÂÆûË∑µÂíåÊÇ£ËÄÖÈ¢ÑÂêéÁöÑ AI Ê®°Âûã„ÄÇ

##### **Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**
2311.15719v1 by Benjamin Keel, Aaron Quyn, David Jayne, Samuel D. Relton

Lung cancer is responsible for 21% of cancer deaths in the UK and five-year
survival rates are heavily influenced by the stage the cancer was identified
at. Recent studies have demonstrated the capability of AI methods for accurate
and early diagnosis of lung cancer from routine scans. However, this evidence
has not translated into clinical practice with one barrier being a lack of
interpretable models. This study investigates the application Variational
Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions.
Proposed models were trained on lesions extracted from 3D CT scans in the
LIDC-IDRI public dataset. Latent vector representations of 2D slices produced
by the VAEs were explored through clustering to justify their quality and used
in an MLP classifier model for lung cancer diagnosis, the best model achieved
state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows
the VAE latent space separates the dataset of malignant and benign lesions
based on meaningful feature components including tumour size, shape, patient
and malignancy class. We also include a comparative analysis of the standard
Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces
the prior with a Dirichlet distribution to encourage a more explainable latent
space with disentangled feature representation. Finally, we demonstrate the
potential for latent space traversals corresponding to clinically meaningful
feature changes.

ÊëòË¶ÅÔºöËÇ∫ÁôåÂç†Ëã±ÂúãÁôåÁóáÊ≠ª‰∫°‰∫∫Êï∏ÁöÑ 21%Ôºå‰∫îÂπ¥Â≠òÊ¥ªÁéáÂæàÂ§ßÁ®ãÂ∫¶ÂèñÊ±∫ÊñºÁôåÁóáË¢´ÁôºÁèæÁöÑÈöéÊÆµ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòé‰∫∫Â∑•Êô∫ËÉΩÊñπÊ≥ïÂÖ∑ÊúâÂæû‰æãË°åÊéÉÊèè‰∏≠Ê∫ñÁ¢∫ÂèäÊó©Ë®∫Êñ∑ËÇ∫ÁôåÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊ≠§Ë≠âÊìöÂ∞öÊú™ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶ÂãôÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈöúÁ§ôÊòØÁº∫‰πèÂèØËß£ÈáãÁöÑÊ®°Âûã„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊáâÁî®ËÆäÂàÜËá™ÂãïÁ∑®Á¢ºÂô® (VAE)Ôºå‰∏ÄÁ®ÆÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÔºåÊñºËÇ∫ÁôåÁóÖÁÅ∂„ÄÇÂ∞áÊèêÂá∫ÁöÑÊ®°ÂûãË®ìÁ∑¥ÊñºÂæû LIDC-IDRI ÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏≠ÊèêÂèñÁöÑ 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁóÖÁÅ∂„ÄÇÈÄöÈÅéËÅöÈ°ûÊé¢Á¥¢‰∫Ü VAE ÁîüÊàêÁöÑ 2D ÂàáÁâáÁöÑÊΩõÂú®ÂêëÈáèË°®Á§∫Ôºå‰ª•Ë≠âÊòéÂÖ∂ÂìÅË≥™Ôºå‰∏¶Áî®ÊñºËÇ∫ÁôåË®∫Êñ∑ÁöÑ MLP ÂàÜÈ°ûÂô®Ê®°ÂûãÔºåÊúÄ‰Ω≥Ê®°ÂûãÈÅîÂà∞‰∫Ü AUC 0.98 Âíå 93.1% Ê∫ñÁ¢∫Â∫¶ÁöÑÊúÄÂÖàÈÄ≤ÊåáÊ®ô„ÄÇËÅöÈ°ûÂàÜÊûêÈ°ØÁ§∫ÔºåVAE ÊΩõÂú®Á©∫ÈñìÊ†πÊìöÊúâÊÑèÁæ©ÁöÑÁâπÂæµÁµÑÊàêÔºàÂåÖÊã¨ËÖ´Áò§Â§ßÂ∞è„ÄÅÂΩ¢ÁãÄ„ÄÅÊÇ£ËÄÖÂíåÊÉ°ÊÄßÈ°ûÂà•ÔºâÂ∞áÊÉ°ÊÄßÂíåËâØÊÄßÁóÖÁÅ∂ÁöÑÊï∏ÊìöÈõÜÂàÜÈñã„ÄÇÊàëÂÄëÈÇÑÂåÖÊã¨Ê®ôÊ∫ñÈ´òÊñØ VAE (GVAE) ÂíåÊõ¥Êñ∞ÁöÑÁãÑÂà©ÂÖãÈõ∑ VAE (DirVAE) ÁöÑÊØîËºÉÂàÜÊûêÔºåÂæåËÄÖÁî®ÁãÑÂà©ÂÖãÈõ∑ÂàÜ‰ΩàÂèñ‰ª£ÂÖàÈ©óÔºå‰ª•‰øÉÈÄ≤ÂÖ∑ÊúâËß£ÈñãÁâπÂæµË°®Á§∫ÁöÑÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÁöÑÊΩõÂú®Á©∫Èñì„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáËá®Â∫äÊúâÊÑèÁæ©ÁöÑÁâπÂæµËÆäÂåñÁõ∏ÊáâÁöÑÊΩõÂú®Á©∫ÈñìÊ©´Ë∂äÁöÑÊΩõÂäõ„ÄÇ

##### **MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**
2311.14471v1 by Nathan Blake, Hana Chockler, David A. Kelly, Santiago Calderon Pena, Akchunya Chanchal

Existing tools for explaining the output of image classifiers can be divided
into white-box, which rely on access to the model internals, and black-box,
agnostic to the model. As the usage of AI in the medical domain grows, so too
does the usage of explainability tools. Existing work on medical image
explanations focuses on white-box tools, such as gradcam. However, there are
clear advantages to switching to a black-box tool, including the ability to use
it with any classifier and the wide selection of black-box tools available. On
standard images, black-box tools are as precise as white-box. In this paper we
compare the performance of several black-box methods against gradcam on a brain
cancer MRI dataset. We demonstrate that most black-box tools are not suitable
for explaining medical image classifications and present a detailed analysis of
the reasons for their shortcomings. We also show that one black-box tool, a
causal explainability-based rex, performs as well as \gradcam.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÂúñÂÉèÂàÜÈ°ûÂô®Ëº∏Âá∫Ëß£ÈáãÂ∑•ÂÖ∑ÂèØÂàÜÁÇ∫‰æùË≥¥ÊñºÊ®°ÂûãÂÖßÈÉ®Â≠òÂèñÊ¨äÈôêÁöÑÁôΩÁõíÔºå‰ª•ÂèäËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÈªëÁõí„ÄÇÈö®Ëëó AI Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑ‰ΩøÁî®Â¢ûÂä†ÔºåÂèØËß£ÈáãÊÄßÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®‰πüÈö®‰πãÂ¢ûÂä†„ÄÇÁèæÊúâÈÜ´Â≠∏ÂΩ±ÂÉèËß£ÈáãÁöÑÂ∑•‰ΩúÈáçÈªûÂú®ÊñºÁôΩÁõíÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç gradcam„ÄÇÁÑ∂ËÄåÔºåÂàáÊèõÂà∞ÈªëÁõíÂ∑•ÂÖ∑ÊúâÊòéÈ°ØÁöÑÂÑ™ÈªûÔºåÂåÖÊã¨ËÉΩÂ§†Ëàá‰ªª‰ΩïÂàÜÈ°ûÂô®‰∏ÄËµ∑‰ΩøÁî®Ôºå‰ª•ÂèäÂª£Ê≥õÁöÑÈªëÁõíÂ∑•ÂÖ∑ÂèØ‰æõÈÅ∏Êìá„ÄÇÂú®Ê®ôÊ∫ñÂΩ±ÂÉè‰∏äÔºåÈªëÁõíÂ∑•ÂÖ∑ËàáÁôΩÁõí‰∏ÄÊ®£Á≤æÁ¢∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂ§öÁ®ÆÈªëÁõíÊñπÊ≥ïÂú®ËÖ¶Áôå MRI Ë≥áÊñôÈõÜ‰∏äËàá gradcam ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëË≠âÊòéÂ§ßÂ§öÊï∏ÈªëÁõíÂ∑•ÂÖ∑‰∏çÈÅ©ÂêàËß£ÈáãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÔºå‰∏¶Ë©≥Á¥∞ÂàÜÊûêÂÖ∂Áº∫ÈªûÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÇÑË°®Êòé‰∏ÄÁ®ÆÈªëÁõíÂ∑•ÂÖ∑ÔºåÂü∫ÊñºÂõ†ÊûúÂèØËß£ÈáãÊÄßÁöÑ rexÔºåË°®ÁèæËàá \gradcam ‰∏ÄÊ®£Â•Ω„ÄÇ

##### **Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**
2311.12573v2 by Robert Gorwa, Michael Veale

The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.

ÊëòË¶ÅÔºöAI ÁôºÂ±ïÁ§æÁæ§Êó•ÁõäÂà©Áî® Hugging Face Á≠âË®óÁÆ°‰∏≠‰ªãÔºåÊèê‰æõ‰ΩøÁî®ËÄÖ‰∏äÂÇ≥‰πãÊ®°ÂûãËàáË®ìÁ∑¥Ë≥áÊñôÁöÑÁ∞°ÊòìÂèñÂæóÁÆ°ÈÅì„ÄÇÈÄô‰∫õÊ®°ÂûãÂ∏ÇÈõÜÈôç‰Ωé‰∫ÜÊï∏ÂçÅËê¨Âêç‰ΩøÁî®ËÄÖÁöÑÊäÄË°ìÈÉ®ÁΩ≤ÈñÄÊ™ªÔºå‰ΩÜÂçªÂèØËÉΩË¢´Áî®ÊñºË®±Â§öÊΩõÂú®ÊúâÂÆ≥‰∏îÈùûÊ≥ïÁöÑÁî®ÈÄî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™™Êòé‰∫Ü AI Á≥ªÁµ±Êó¢ÂèØ‰ª•„ÄåÂåÖÂê´„ÄçÂÖßÂÆπÔºå‰πüÂèØ‰ª•‰ΩúÁÇ∫ÈñãÊîæÂºèÂ∑•ÂÖ∑ÔºåÈÄôÊèêÂá∫‰∫ÜËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÊ£òÊâãÁöÑÂπ≥Âè∞Ê≤ªÁêÜÊåëÊà∞‰πã‰∏Ä„ÄÇÊàëÂÄëÊèê‰æõ Hugging Face„ÄÅGitHub Âíå Civitai Á≠â‰∏âÂÄãË™™ÊòéÊÄßÂπ≥Âè∞‰∏äÊï∏Ëµ∑‰∫ã‰ª∂ÁöÑÊ°à‰æãÁ†îÁ©∂Ôºå‰ª•Êé¢Ë®éÊ®°ÂûãÂ∏ÇÈõÜÂ¶Ç‰ΩïÊéßÁÆ°Ê®°Âûã„ÄÇÊ†πÊìöÊ≠§ÂàÜÊûêÔºåÊàëÂÄëÊ¶ÇËø∞‰∫ÜÁî¢Ê•≠ÁÇ∫ÂõûÊáâÊéßÁÆ°ÈúÄÊ±ÇËÄåÁôºÂ±ïÁöÑÈáçË¶ÅÔºà‰ΩÜ‰ªçÊúâÈôêÔºâÂØ¶ÂãôÔºöÊéàÊ¨ä„ÄÅÂ≠òÂèñÂíå‰ΩøÁî®ÈôêÂà∂„ÄÅËá™ÂãïÂÖßÂÆπÊéßÁÆ°ÂíåÈñãÊîæÂºèÊîøÁ≠ñÁôºÂ±ï„ÄÇÂÑòÁÆ°ÁõÆÂâçÈù¢Ëá®ÁöÑÊîøÁ≠ñÊåëÊà∞Áõ∏Áï∂Âö¥Â≥ªÔºåÊàëÂÄë‰ªçÊèêÂá∫‰∫Ü‰∏Ä‰∫õÊÉ≥Ê≥ïÔºåË™™ÊòéÂπ≥Âè∞Â¶Ç‰ΩïËÉΩÊõ¥Â•ΩÂú∞ÂãïÂì°Ë≥áÊ∫êÔºå‰ΩúÁÇ∫Ë¨πÊÖé„ÄÅÂÖ¨Âπ≥ÂíåÈÅ©Â∫¶ÁöÑÊ≥ïË¶èÂ≠òÂèñÈªû„ÄÇ

##### **Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**
2311.11932v1 by Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker

Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÂíåÁõÆÊ®ôÔºöÈÄöÈÅéÊèêÂèñÈÄô‰∫õË≥áË®äÔºåÊ©üÂô®ÊàñÊ∑±Â∫¶Â≠∏Áøí (ML/DL) Âü∫ÊñºËá™‰∏ªÊï∏ÊìöÂàÜÊûêÂ∑•ÂÖ∑ÂèØ‰ª•ÂçîÂä©Ëá®Â∫äÈÜ´ÁîüÂíåÁôåÁóáÁ†îÁ©∂‰∫∫Âì°ÂæûË§áÈõúÁöÑÊï∏ÊìöÈõÜ‰∏≠ÁôºÁèæÊ®°ÂºèÂíåÈóú‰øÇ„ÄÇÊúÄËøëÂ∑≤ÁôºË°®Ë®±Â§öÂü∫Êñº DL ÁöÑÂçµÂ∑¢Áôå (OC) Êï∏ÊìöÂàÜÊûê„ÄÇÈÄô‰∫õÂàÜÊûêÂú®ÁôåÁóáÁöÑÂêÑÂÄãÊñπÈù¢Ôºà‰æãÂ¶ÇÔºåÂÆÉÂÄëÊ∂âÂèäÁöÑÂ≠êÈ†òÂüüÂíåÁôåÁóáÈ°ûÂûãÔºâÂíåÊï∏ÊìöÂàÜÊûêÂäüËÉΩÊñπÈù¢È´òÂ∫¶Â§öÊ®£Âåñ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁº∫‰πèÂ∞çÈÄô‰∫õÂàÜÊûêÂú®ÈÄô‰∫õÁâπÂæµÂíå AI ‰øùË≠â (AIA) ÊñπÈù¢ÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÈÄôÁØáÁ≥ªÁµ±ÊÄßÂõûÈ°ßÊó®Âú®ÈÄöÈÅéÊ™¢Ë¶ñÁèæÊúâÊñáÁçª‰∏¶ÊòéÁ¢∫ÈóúÊ≥®ÈóúÈçµÁâπÂæµÂíå AI ‰øùË≠âËßÄÈªûÔºå‰æÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩ„ÄÇÊñπÊ≥ïÔºö‰ΩøÁî® PRISMA Êû∂ÊßãÂú®‰∏âÂÄãÊúüÂàäË≥áÊñôÂ∫´‰∏≠ÈÄ≤Ë°åÂÖ®Èù¢ÊêúÂ∞ã„ÄÇÂàÜÊûêÂÉÖÂåÖÊã¨ 2015 Âπ¥Ëá≥ 2023 Âπ¥ÈñìÁôºË°®ÊñºÂêåË°åË©ïÂØ©ÊúüÂàäÁöÑÁ†îÁ©∂„ÄÇÁµêÊûúÔºöÂú®ÂõûÈ°ß‰∏≠ÔºåÁ∏ΩÂÖ±Ê™¢Ë¶ñ‰∫Ü 96 È†ÖÁî± DL È©ÖÂãïÁöÑÂàÜÊûê„ÄÇÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÂπæÂÄãÈóúÊñºÁî± DL È©ÖÂãïÁöÑÂçµÂ∑¢ÁôåÊï∏ÊìöÂàÜÊûêÁöÑÈáçË¶ÅË¶ãËß£Ôºö- Â§ßÂ§öÊï∏Á†îÁ©∂ 71%Ôºà96 È†Ö‰∏≠Êúâ 68 È†ÖÔºâÂ∞àÊ≥®ÊñºÊ™¢Ê∏¨ÂíåË®∫Êñ∑ÔºåËÄåÊ≤íÊúâÁ†îÁ©∂Êé¢Ë®é OC ÁöÑÈ†êÊ∏¨ÂíåÈ†êÈò≤„ÄÇ- ÈÄô‰∫õÂàÜÊûê‰∏ªË¶ÅÂü∫Êñº‰æÜËá™ÈùûÂ§öÂÖÉÊóèÁæ§ÁöÑÊ®£Êú¨Ôºà75%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 72 È†ÖÔºâÔºâÔºåÂÉÖÈôêÊñºÊüêÂÄãÂú∞ÁêÜ‰ΩçÁΩÆÊàñÂúãÂÆ∂„ÄÇ- Âè™ÊúâÂ∞ëÈÉ®ÂàÜÁ†îÁ©∂ÔºàÂÉÖ 33%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 32 È†ÖÔºâÂü∑Ë°åÊï¥ÂêàÂàÜÊûêÔºåÂÖ∂‰∏≠Â§ßÂ§öÊï∏‰ΩøÁî®ÂêåË≥™Êï∏ÊìöÔºàËá®Â∫äÊàñÁµÑÂ≠∏Ôºâ„ÄÇ- ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂè™Êúâ 8.3%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 8 È†ÖÔºâ‰ΩøÁî®Â§ñÈÉ®ÂíåÂ§öÂÖÉÊï∏ÊìöÈõÜÈ©óË≠â‰∫ÜÂÖ∂Ê®°ÂûãÔºåÂº∑Ë™ø‰∫ÜÂä†Âº∑Ê®°ÂûãÈ©óË≠âÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Âèä- Â∞á AIA Á¥çÂÖ•ÁôåÁóáÊï∏ÊìöÂàÜÊûê‰ªçËôïÊñºÈùûÂ∏∏Êó©ÊúüÁöÑÈöéÊÆµÔºõÂè™Êúâ 2.1%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 2 È†ÖÔºâÈÄèÈÅéÂèØËß£ÈáãÊÄßÊòéÁ¢∫Êé¢Ë®é‰∫Ü AIA„ÄÇ</paragraph>

##### **Representing visual classification as a linear combination of words**
2311.10933v1 by Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter

Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.

ÊëòË¶ÅÔºö<paragraph>Ëß£ÈáãÊÄßÊòØÊ∑±Â∫¶Â≠∏Áøí‰∏≠Èï∑ÊúüÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á≠âÈ´òÈ¢®Èö™È†òÂüü„ÄÇÂ∏∏Ë¶ãÁöÑËß£ÈáãÊÄßÊñπÊ≥ïÊúÉÂº∑Ë™øÈ©ÖÂãï AI Ê®°ÂûãÊ±∫Á≠ñÁöÑÂΩ±ÂÉèÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÂæàÂ§ßÁ®ãÂ∫¶‰æùË≥¥Ë™ûË®Ä‰æÜÂÇ≥ÈÅî‰∏çÂÉÖÊòØ„ÄåÂú®Âì™Ë£°„ÄçÔºåÈÇÑÊúâ„ÄåÊòØ‰ªÄÈ∫º„ÄçÁöÑËß£Èáã„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏Ëß£ÈáãÊÄßÊñπÊ≥ïÈÉΩÂ∞àÊ≥®ÊñºËß£ÈáãÂÄãÂà• AI È†êÊ∏¨ÔºåËÄå‰∏çÊòØÊèèËø∞ AI Ê®°Âûã‰∏ÄËà¨‰ΩøÁî®ÁöÑÁâπÂæµ„ÄÇÂæåËÄÖÂ∞çÊñºÊ®°ÂûãÂíåË≥áÊñôÈõÜÁ®ΩÊ†∏ÁâπÂà•ÊúâÁî®ÔºåÁîöËá≥ÂèØËÉΩÂú® AI ÊÑà‰æÜÊÑàÁî®ÊñºÊñ∞Á©é‰ªªÂãôÊôÇÁî¢ÁîüÁü•Ë≠ò„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã‰æÜËæ®Ë≠òË¶ñË¶∫ÂàÜÈ°û‰ªªÂãôÁöÑË™ûË®ÄÊèèËø∞Á¨¶ÁöÑËß£ÈáãÊÄßÁ≠ñÁï•„ÄÇÈÄèÈÅéÂà©Áî®ÂΩ±ÂÉèÂíåÊñáÂ≠ó‰πãÈñìÈ†êÂÖàË®ìÁ∑¥ÁöÑËÅØÂêàÂµåÂÖ•Á©∫ÈñìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÊñ∞ÁöÑÂàÜÈ°û‰ªªÂãô‰º∞Ë®àÁÇ∫‰∏ÄÂÄãÁ∑öÊÄßÊñáÂ≠óÁµÑÂêàÔºåÂ∞éËá¥ÊØèÂÄãÊñáÂ≠óÈÉΩÊúâÊ¨äÈáçÔºåË°®Á§∫ÂÆÉËàáÂü∫ÊñºË¶ñË¶∫ÁöÑÂàÜÈ°ûÂô®Â∞çÈΩä„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÁôºÁèæÁî¢ÁîüÁöÑÊèèËø∞Á¨¶Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äËàáËá®Â∫äÁü•Ë≠ò‰∏ÄËá¥ÔºåÂÑòÁÆ°Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑË™ûË®ÄË®ìÁ∑¥„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÁöÑÂÅöÊ≥ï‰πüÁôºÁèæ‰∫ÜÊâÄÁî®ÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏≠ÁöÑ„ÄåÊç∑ÂæëÈÄ£Á∑ö„ÄçÁöÑÂèØËÉΩÊÄß„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞Ëß£ÈáãÊÄßÁöÑÂäüËÉΩÊÄßË°°ÈáèÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖË©¶È©óËÆÄËÄÖÁ†îÁ©∂ÔºåÁôºÁèæ AI Ë≠òÂà•ÁöÑÊñáÂ≠óËÉΩËÆìÈùûÂ∞àÂÆ∂‰∫∫È°ûÂú®ÈùûÂπ≥Âá°ÁöÑÂ±§Á¥öÂü∑Ë°åÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇ‰ªªÂãô„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÁöÑÁµêÊûúÂº∑Ë™ø‰∫Ü‰ΩøÁî®Â§öÊ®°ÂºèÂü∫Á§éÊ®°Âûã‰æÜÊèê‰æõÁõ¥ËßÄÁöÑ„ÄÅÂü∫ÊñºË™ûË®ÄÁöÑË¶ñË¶∫‰ªªÂãôËß£ÈáãÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**
2311.02115v2 by Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert

Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.

ÊëòË¶ÅÔºö<paragraph>‰ΩøÁî®ÈÜ´ÁôÇÂΩ±ÂÉèË®ìÁ∑¥ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÔºåÁî®ÊñºËá®Â∫ä‰ªªÂãôÊôÇÔºåÂ∏∏ÊúÉÂú®ÊïàËÉΩ‰∏äÂ±ïÁèæÂá∫Ê¨°Áæ§È´î‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºåÂΩ¢ÊàêÂÅèË¶ã„ÄÇÁî±Êñº‰∏¶ÈùûÊâÄÊúâÁúüÂØ¶‰∏ñÁïåÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñô‰∏≠ÁöÑÂÅèË¶ã‰æÜÊ∫êÈÉΩÂÆπÊòìËæ®Ë≠òÔºåÂõ†Ê≠§ÂÖ®Èù¢Ë©ï‰º∞ÈÄô‰∫õÂÅèË¶ãÊòØÂ¶Ç‰ΩïÁ∑®Á¢ºÂà∞Ê®°Âûã‰∏≠Ôºå‰ª•ÂèäÂÅèË¶ãÁ∑©Ëß£ÊñπÊ≥ïÂú®ÊîπÂñÑÊïàËÉΩÂ∑ÆÁï∞ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÊûêÊû∂ÊßãÔºåÁî®ÊñºÁ≥ªÁµ±Âåñ‰∏îÂÆ¢ËßÄÂú∞Ë™øÊü•ÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÁöÑÂÅèË¶ãÂ∞ç AI Ê®°ÂûãÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÈñãÁôº‰∏¶Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊû∂ÊßãÔºå‰ª•ÈÄ≤Ë°åÂèóÊéßÁöÑÈõªËÖ¶Ê®°Êì¨Ë©¶È©óÔºå‰ΩøÁî®‰∏ÄÂÄãÂ∑•ÂÖ∑‰æÜË©ï‰º∞ÈÜ´ÁôÇÂΩ±ÂÉè AI ‰∏≠ÁöÑÂÅèË¶ãÔºåË©≤Â∑•ÂÖ∑Áî®ÊñºÁî¢ÁîüÂÖ∑ÊúâÂ∑≤Áü•ÁñæÁóÖÂΩ±ÈüøÂíåÂÅèË¶ã‰æÜÊ∫êÁöÑÂêàÊàêÁ£ÅÂÖ±ÊåØÂΩ±ÂÉè„ÄÇÂèØË°åÊÄßÈÄèÈÅé‰ΩøÁî®‰∏âÂÄãÂèç‰∫ãÂØ¶ÂÅèË¶ãÊÉÖÂ¢É‰æÜË°°ÈáèÊ®°Êì¨ÂÅèË¶ãÊïàÊáâÂ∞çÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂàÜÈ°ûÂô®Âíå‰∏âÂÄãÂÅèË¶ãÁ∑©Ëß£Á≠ñÁï•ÁöÑÂΩ±ÈüøÔºå‰∏¶Â±ïÁ§∫Âá∫‰æÜ„ÄÇÂàÜÊûêÈ°ØÁ§∫ÔºåÁï∂ CNN Âú®ÂêàÊàêË≥áÊñôÈõÜ‰∏äÂèóË®ìÊôÇÔºåÊ®°Êì¨ÂÅèË¶ãÊúÉÂ∞éËá¥È†êÊúüÁöÑÊ¨°Áæ§È´îÊïàËÉΩÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÈáçÊñ∞Âä†Ê¨äË¢´Ë™çÁÇ∫ÊòØÊ≠§Ë®≠ÂÆö‰∏≠ÊúÄÊàêÂäüÁöÑÂÅèË¶ãÁ∑©Ëß£Á≠ñÁï•ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËß£ÈáãÊÄß AI ÊñπÊ≥ïÂ¶Ç‰ΩïÂçîÂä©‰ΩøÁî®ÈÄôÂÄãÊû∂ÊßãË™øÊü•Ê®°Âûã‰∏≠ÂÅèË¶ãÁöÑË°®Áèæ„ÄÇÈñãÁôºÂÖ¨Âπ≥ÁöÑ AI Ê®°ÂûãÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫ÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏≠ÂèØËÉΩÂ≠òÂú®Ë®±Â§ö‰∏îÁ∂ìÂ∏∏Êú™Áü•ÁöÑÂÅèË¶ã‰æÜÊ∫ê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂÆ¢ËßÄÂú∞Á†îÁ©∂ÂÅèË¶ãÂíåÁ∑©Ëß£Á≠ñÁï•Â∞çÊ∑±Â∫¶Â≠∏ÁøíÁÆ°Á∑öÁöÑÂΩ±ÈüøÔºåÈÄôÂèØ‰ª•ÊîØÊè¥ÂÅ•ÂÖ®‰∏îË≤†Ë≤¨‰ªªÁöÑËá®Â∫ä AI ÁöÑÈñãÁôº„ÄÇ</paragraph>

##### **Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**
2310.19174v1 by Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman

Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÁÇ∫Ëá™ÂãïÈ†êÊ∏¨‰∏≠È¢®ÂæåÁóáÁãÄÂèäÂÖ∂Â∞çÂæ©ÂÅ•ÁöÑÂèçÊáâÊèê‰æõ‰∫ÜÊ•µÂ§ßÁöÑÊΩõÂäõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÈáçÂ§ßÊåëÊà∞ÂåÖÊã¨Á•ûÁ∂ìÂΩ±ÂÉèË≥áÊñôÁöÑÁ∂≠Â∫¶ÈùûÂ∏∏È´ò„ÄÅÂèØÁî®ÊñºÂ≠∏ÁøíÁöÑË≥áÊñôÈõÜË¶èÊ®°Áõ∏Â∞çËºÉÂ∞èÔºå‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÁµêÂêàÁ•ûÁ∂ìÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÔºà‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®àË≥áË®äÂíåËá®Â∫äÁâπÂæµÔºâ„ÄÇÊú¨ÊñáÊ†πÊìöÂÖ©Á®ÆÁ≠ñÁï•Ë©ï‰º∞‰∫ÜÂ§öÁ®ÆËß£Ê±∫ÊñπÊ°à„ÄÇÁ¨¨‰∏ÄÁ®ÆÊòØ‰ΩøÁî®Á∏ΩÁµê MRI ÊéÉÊèèÁöÑ 2D ÂΩ±ÂÉè„ÄÇÁ¨¨‰∫åÁ®ÆÊòØÈÅ∏ÊìáÊúâÂä©ÊñºÊèêÈ´òÂàÜÈ°ûÁ≤æÁ¢∫Â∫¶ÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂú®ÁµêÂêàÂæû MRI ‰∏≠ÊèêÂèñÁöÑÊÑüËààË∂£ÂçÄÂüüËàáË°®Ê†ºË≥áÊñôÁöÑÁ¨¶ËôüË°®Á§∫ÁöÑÂΩ±ÂÉè‰∏äË®ìÁ∑¥Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÊñ∞Á©éÊñπÊ≥ï„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÁ≥ªÂàó CNN Êû∂ÊßãÔºà2D Âíå 3DÔºâÔºåÈÄô‰∫õÊû∂ÊßãÂú® MRI ÂíåË°®Ê†ºË≥áÊñôÁöÑ‰∏çÂêåË°®Á§∫‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•È†êÊ∏¨‰∏≠È¢®ÂæåÂè£Ëø∞ÂúñÁâáÊèèËø∞ËÉΩÂäõÁöÑÁ∂úÂêàÊ∏¨ÈáèÊòØÂê¶Âú®Â§±Ë™ûÁóáÊàñÈùûÂ§±Ë™ûÁóáÁØÑÂúçÂÖß„ÄÇMRI ÂíåË°®Ê†ºË≥áÊñô‰æÜËá™ 758 ÂêçÂèÉËàá PLORAS Á†îÁ©∂ÁöÑËã±Ë™û‰∏≠È¢®ÂÄñÂ≠òËÄÖ„ÄÇÂÉÖÈáùÂ∞çÁóÖÁÅ∂Â§ßÂ∞èÁöÑÂü∫Á∑öÈÇèËºØËø¥Ê≠∏ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 0.678ÔºåÁï∂‰æùÂ∫èÂä†ÂÖ•ÂàùÂßãÁóáÁãÄÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÊôÇÔºå‰∏äÂçáËá≥ 0.757 Âíå 0.813„ÄÇÂú®ÂæûÊØèÂÄã MRI ÊéÉÊèè‰∏≠ÊèêÂèñ 8 ÂÄãÊÑüËààË∂£ÂçÄÂüü‰∏¶Âú® 2D ÊÆòÂ∑ÆÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ËàáÁóÖÁÅ∂Â§ßÂ∞è„ÄÅÂàùÂßãÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÁµêÂêàÊôÇÔºåËßÄÂØüÂà∞ÊúÄÈ´òÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ 0.854„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞áÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÁµêÂêàËµ∑‰æÜ‰ª•Áç≤ÂæóÈ´òÊñº‰∏≠È¢®ÂæåÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÂç≥‰ΩøÂú®Ê©üÂô®Â≠∏ÁøíË°ìË™û‰∏≠Ë≥áÊñôÈõÜÂæàÂ∞èÁöÑÊÉÖÊ≥Å‰∏ã‰πüÊòØÂ¶ÇÊ≠§„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫Â¶Ç‰ΩïÊîπÈÄ≤ÁõÆÂâçÁöÑÊ®°ÂûãÔºå‰ª•‰ΩøÁî®‰æÜËá™ÈÜ´Èô¢ÊéÉÊèèÂÑÄÁöÑÂΩ±ÂÉè‰æÜÂØ¶ÁèæÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**
2310.01828v2 by Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour

eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫ËôïÁêÜ‰ªªÂãôÈóúÈçµÊáâÁî®Á®ãÂºèÊôÇÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨ÈúÄÊ±ÇÔºåÁ¢∫‰øùÊé°Áî®ÈªëÁõí AI Ê®°ÂûãÁöÑÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇXAI ÁöÑÈáçË¶ÅÊÄßÊ∂µËìãÂæûÈÜ´ÁôÇ‰øùÂÅ•Âà∞ÈáëËûçÁöÑÂêÑÁ®ÆÈ†òÂüüÔºåÂú®ÈÄô‰∫õÈ†òÂüü‰∏≠Ôºå‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãËá≥ÈóúÈáçË¶Å„ÄÇÂ§ßÂ§öÊï∏Âü∫Êñº AI ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÈÄöÂ∏∏ÊòØÈªëÁõíÂ≠êÔºõÂõ†Ê≠§ÔºåÂú®ÂΩ±ÂÉèËôïÁêÜ‰∏≠Êèê‰æõÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂèØËß£ÈáãÊÄßÂ∞çÊñºÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™ÂãïÈßïÈßõÂíåÈÅôÊ∏¨ÊáâÁî®‰∏≠ÁöÑÂª£Ê≥õÊé°Áî®ÂíåÈÉ®ÁΩ≤Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÔºåÂ∑≤ÈáùÂ∞çÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÂºïÂÖ•‰∫ÜÂ§öÁ®Æ XAI ÊñπÊ≥ï„ÄÇÁõ∏ÂèçÂú∞ÔºåÂΩ±ÂÉèÂàÜÂâ≤Âú®ÂèØËß£ÈáãÊÄßÁöÑËÉåÊôØ‰∏ãÂèóÂà∞ÁöÑÈóúÊ≥®Áõ∏Â∞çËºÉÂ∞ëÔºåÂÑòÁÆ°ÂÆÉÊòØÈõªËÖ¶Ë¶ñË¶∫ÊáâÁî®‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÁâπÂà•ÊòØÂú®ÈÅôÊ∏¨‰∏≠„ÄÇÂè™ÊúâÈÉ®ÂàÜÁ†îÁ©∂ÊèêÂá∫Áî®ÊñºÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑ XAI ÊºîÁÆóÊ≥ï„ÄÇÊú¨ÊñáÊîπÁ∑®‰∫ÜÊúÄËøëÁöÑÁÑ°Ê¢ØÂ∫¶ Sobol XAI ÊñπÊ≥ï‰ª•ÈÄ≤Ë°åË™ûÊÑèÂàÜÂâ≤„ÄÇÁÇ∫‰∫ÜË°°Èáè Sobol ÊñπÊ≥ïÂú®ÂàÜÂâ≤‰∏≠ÁöÑÊïàËÉΩÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂèØÂ≠∏ÁøíÈõúË®äÊ®°ÂûãÁöÑÂÆöÈáè XAI Ë©ï‰º∞ÊñπÊ≥ï„ÄÇÊ≠§Ê®°ÂûãÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØÂú®Ëß£ÈáãÂúñ‰∏äË™òÁôºÈõúË®äÔºåÂÖ∂‰∏≠ËºÉÈ´òÁöÑË™òÁôºÈõúË®äË°®Á§∫ËºÉ‰ΩéÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÈÄ≤Ë°åÂü∫Ê∫ñÂàÜÊûê‰ª•Ë©ï‰º∞ÂíåÊØîËºÉ‰∏âÁ®Æ XAI ÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÂåÖÊã¨ Seg-Grad-CAM„ÄÅSeg-Grad-CAM++ Âíå Seg-SobolÔºå‰∏¶‰ΩøÁî®ÊâÄÊèêÂá∫ÁöÑÂü∫ÊñºÈõúË®äÁöÑË©ï‰º∞ÊäÄË°ì„ÄÇÈÄôÊßãÊàê‰∫Ü‰ΩøÁî®È´òËß£ÊûêÂ∫¶Ë°õÊòüÂΩ±ÂÉèÂü∑Ë°åÂíåË©ï‰º∞ XAI ÊñπÊ≥ïÁöÑÈ¶ñÊ¨°ÂòóË©¶„ÄÇ

##### **Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**
2311.01463v1 by Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy

Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Áü≠ÊôÇÈñìÂÖßÂ∑≤Âú®Â§öÂÄãÈ†òÂüü‰∏≠Â§ßÈáèÊøÄÂ¢û„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº‰∫ãÂØ¶ÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÂπªË¶∫Á≠âÂïèÈ°åÔºåÈÜ´ÁôÇÂíå‰øùÂÅ•È†òÂüüÂ∞çÂÖ∂Êé°Áî®Áå∂Ë±´‰∏çÊ±∫„ÄÇÈëëÊñºÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÈ´òÈ¢®Èö™ÊÄßË≥™ÔºåË®±Â§öÁ†îÁ©∂‰∫∫Âì°ÁîöËá≥Ë≠¶Âëä‰∏çË¶Å‰ΩøÁî®ÂÆÉÔºåÁõ¥Âà∞ÈÄô‰∫õÂïèÈ°åÂæóÂà∞Ëß£Ê±∫„ÄÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂØ¶ÊñΩÂíåÈÉ®ÁΩ≤ LLM ÁöÑÈóúÈçµÊòØ‰ΩøÈÄô‰∫õÊ®°ÂûãÂÄºÂæó‰ø°Ë≥¥„ÄÅÈÄèÊòéÔºàÁõ°ÂèØËÉΩÂ§öÔºâ‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂª∫Á´ãÂèØÈù†„ÄÅÂÄºÂæó‰ø°Ë≥¥ÂíåÁÑ°ÂÅèË¶ãÊ®°ÂûãÁöÑÈóúÈçµË¶ÅÁ¥†Ôºå‰ΩúÁÇ∫ÂÆÉÂÄëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂæóÂà∞Êé°Áî®ÁöÑÂøÖË¶ÅÊ¢ù‰ª∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÂ∞çÂπªË¶∫ÈÄ≤Ë°åÈáèÂåñ„ÄÅÈ©óË≠âÂíåÁ∑©Ëß£„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÂèØËÉΩÊòØ‰ªÄÈ∫ºÊ®£Â≠ê„ÄÇ

##### **When to Trust AI: Advances and Challenges for Certification of Neural Networks**
2309.11196v1 by Marta Kwiatkowska, Xiyue Zhang

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂ∑≤Âø´ÈÄüÈÄ≤Ê≠•ÔºåÁèæÂ∑≤Ê∫ñÂÇôÈÉ®ÁΩ≤ÊñºÂª£Ê≥õÁöÑÊáâÁî®Á®ãÂºè‰∏≠Ôºå‰æãÂ¶ÇËá™‰∏ªÁ≥ªÁµ±„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ„ÄÇÂèäÊó©Êé°Áî® AI ÊäÄË°ìÊñºÂØ¶ÈöõÊáâÁî®Á®ãÂºè‰∏¶ÈùûÊ≤íÊúâÂïèÈ°åÔºåÁâπÂà•ÊòØÂ∞çÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÆÉÂèØËÉΩ‰∏çÁ©©ÂÆö‰∏îÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÁØÑ‰æãÁöÑÂΩ±Èüø„ÄÇÂæûÈï∑ÈÅ†‰æÜÁúãÔºåÈúÄË¶ÅÈñãÁôºÈÅ©Áï∂ÁöÑÂÆâÂÖ®‰øùË≠âÊäÄË°ìÔºå‰ª•Ê∏õÂ∞ëÂõ†ÂèØÈÅøÂÖçÁöÑÁ≥ªÁµ±ÊïÖÈöúËÄåÈÄ†ÊàêÁöÑÊΩõÂú®ÂÇ∑ÂÆ≥Ôºå‰∏¶Á¢∫‰øùÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÊú¨ÊñáËëóÈáçÊñºË™çË≠âÂíåÂèØËß£ÈáãÊÄßÔºåÊ¶ÇËø∞‰∫ÜÂ∑≤ÈñãÁôºÁî®ÊñºÁ¢∫‰øù AI Ê±∫Á≠ñÂÆâÂÖ®ÁöÑÊäÄË°ìÔºå‰∏¶Ë®éË´ñÊú™‰æÜÁöÑÊåëÊà∞„ÄÇ

##### **Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**
2309.10424v1 by Juan M. Garc√≠a-G√≥mez, Vicent Blanes-Selva, Jos√© Carlos de Bartolom√© Cenzano, Jaime Cebolla-Cornejo, Ascensi√≥n Do√±ate-Mart√≠nez

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

ÊëòË¶ÅÔºöÊ≠êÊ¥≤Ë≠∞ÊúÉË≠∞ÊúÉÁ†îÁ©∂ÊúçÂãôÁ∏ΩÂ±ÄÂ∑≤ÁÇ∫Ê≠êÊ¥≤Ë≠∞ÊúÉË≠∞Âì°Ê∫ñÂÇô‰∫Ü‰∏Ä‰ªΩÂ†±ÂëäÔºåÂÖ∂‰∏≠ÂàóËàâ‰∫Ü‰∫∫Â∑•Êô∫ËÉΩ (AI) Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑ‰∏ÉÈ†Ö‰∏ªË¶ÅÈ¢®Èö™ÔºöAI ÈåØË™§Â∞éËá¥ÊÇ£ËÄÖÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÅÈÜ´ÁôÇ AI Â∑•ÂÖ∑Ë¢´Êø´Áî®„ÄÅAI Â≠òÂú®ÂÅèË¶ã‰∏¶Â∞éËá¥ÁèæÊúâ inequities ÊåÅÁ∫åÂ≠òÂú®„ÄÅÁº∫‰πèÈÄèÊòéÂ∫¶„ÄÅÈö±ÁßÅÂíåÂÆâÂÖ®ÂïèÈ°å„ÄÅÂïèË≤¨Â∑ÆË∑ù‰ª•ÂèäÂØ¶ÊñΩÈöúÁ§ô„ÄÇ
  Âú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂçÅÂõõÈ†ÖÂäüËÉΩÊÄßË¶ÅÊ±ÇÔºåAI Á≥ªÁµ±ÂèØ‰ª•ÂØ¶ÊñΩÈÄô‰∫õË¶ÅÊ±Ç‰æÜÈôç‰ΩéËàáÂÖ∂ÈÜ´ÁôÇÁõÆÁöÑÁõ∏ÈóúÁöÑÈ¢®Èö™ÔºöAI Ë≠∑ÁÖß„ÄÅ‰ΩøÁî®ËÄÖÁÆ°ÁêÜ„ÄÅÊ≥ïË¶èÊ™¢Êü•„ÄÅÂÉÖÈôêÂ≠∏Ë°ìÁî®ÈÄîÂÖçË≤¨ËÅ≤Êòé„ÄÅË≥áÊñôÂìÅË≥™Ë©ï‰º∞„ÄÅËá®Â∫äÈÜ´ÁîüÈõôÈáçÊ™¢Êü•„ÄÅÊåÅÁ∫åÊïàËÉΩË©ï‰º∞„ÄÅÁ®ΩÊ†∏ËøΩËπ§„ÄÅÊåÅÁ∫åÂèØÁî®ÊÄßÊ∏¨Ë©¶„ÄÅÂõûÈ°ßÂõûÊ∫Ø/Ê®°Êì¨Ê°à‰æã„ÄÅÂÅèË¶ãÊ™¢Êü•„ÄÅÂèØËß£Èáã AI„ÄÅÂä†ÂØÜÂíå‰ΩøÁî®Á∂ìÈÅéÂØ¶Âú∞Ê∏¨Ë©¶ÁöÑÁ®ãÂºèÂ∫´Ôºå‰ª•ÂèäË™ûÊÑè‰∫íÈÄöÊÄß„ÄÇ
  ÊàëÂÄëÂú®Ê≠§ÁöÑÁõÆÁöÑÊòØÊèê‰æõÊäÄË°ìËß£Ê±∫ÊñπÊ°àÁöÑÁâπÂÆöÈ´òÈöéË¶èÊ†ºÔºå‰ª•Á¢∫‰øùÊåÅÁ∫åËâØÂ•ΩÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÁî® AI Á≥ªÁµ±Ôºå‰ª•Á¨¶ÂêàÊú™‰æÜÁöÑÊ≠êÁõüÊ≥ïË¶èÊû∂ÊßãÔºåÂæûËÄå‰ΩøÊÇ£ËÄÖÂèóÁõä„ÄÇ

##### **QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**
2309.10293v3 by Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins

Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ìÂèØÁî®ÊñºÂàÜÈ°ûÁóÖÊÇ£ÁöÑË∫´È´îÊ¥ªÂãï‰∏¶È†êÊ∏¨ÈÅ†Ë∑ùÁóÖÊÇ£Áõ£ÊéßÁöÑÈáçË¶ÅÁîüÂëΩÂæµË±°„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁ≠âÈùûÁ∑öÊÄßÊ®°ÂûãÁöÑÂõûÊ≠∏ÂàÜÊûêÁî±ÊñºÂÖ∂ÈªëÁõíÂ≠êÁöÑÊÄßË≥™ËÄåÂÖ∑ÊúâÊúâÈôêÁöÑÂèØËß£ÈáãÊÄß„ÄÇÈÄôÂèØËÉΩÈúÄË¶ÅÊ±∫Á≠ñËÄÖÊ†πÊìöÈùûÁ∑öÊÄßÊ®°ÂûãÁµêÊûúÂÅöÂá∫Áõ≤ÁõÆÁöÑ‰ø°‰ª∞È£õË∫çÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠„ÄÇÂú®Èùû‰æµÂÖ•ÊÄßÁõ£Êéß‰∏≠Ôºå‰æÜËá™ËøΩËπ§ÊÑüÊ∏¨Âô®ÂíåÂÖ∂ÊòìÊÑüËá®Â∫äÂ±¨ÊÄßÁöÑÁóÖÊÇ£Ë≥áÊñôÂÖÖÁï∂È†êÊ∏¨Êú™‰æÜÁîüÂëΩÂæµË±°ÁöÑËº∏ÂÖ•ÁâπÂæµ„ÄÇËß£ÈáãÂêÑÁ®ÆÁâπÂæµÂ∞çÁõ£ÊéßÊáâÁî®Á®ãÂºèÊï¥È´îËº∏Âá∫ÁöÑË≤¢ÁçªÂ∞çÊñºËá®Â∫äÈÜ´ÁîüÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®ÊñºÈáèÂåñÂàÜÊûêÁöÑÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (QXAI) Êû∂ÊßãÔºåË©≤Êû∂ÊßãÂÖ∑ÊúâÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ï‰∏≠ÂõûÊ≠∏ÂíåÂàÜÈ°û‰ªªÂãôÁöÑ‰∫ãÂæåÊ®°ÂûãÂèØËß£ÈáãÊÄßÂíåÂÖßÂú®ÂèØËß£ÈáãÊÄß„ÄÇÈÄôÈÄèÈÅéÂà©Áî® Shapley ÂÄºÊ¶ÇÂøµ‰∏¶Â∞áÊ≥®ÊÑèÂäõÊ©üÂà∂Á¥çÂÖ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æÜÂØ¶Áèæ„ÄÇÊàëÂÄëÊé°Áî®‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑Ø (ANN) ÂíåÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÈõôÂêë LSTM (BiLSTM) Ê®°ÂûãÔºåÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÈ†êÊ∏¨ÂøÉÁéáÂíåÂàÜÈ°ûË∫´È´îÊ¥ªÂãï„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂú®È†êÊ∏¨ÂíåÂàÜÈ°û‰ªªÂãô‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÂ∞çËº∏ÂÖ•Ë≥áÊñôÈÄ≤Ë°åÂÖ®Â±ÄËß£ÈáãÂíåÂ±ÄÈÉ®Ëß£ÈáãÔºå‰ª•‰∫ÜËß£ÂêÑÁ®ÆÁóÖÊÇ£Ë≥áÊñôÁöÑÁâπÂæµË≤¢Áçª„ÄÇÊâÄÊèêÂá∫ÁöÑ QXAI Êû∂Êßã‰ΩøÁî® PPG-DaLiA Ë≥áÊñôË©ï‰º∞Ôºå‰ª•È†êÊ∏¨ÂøÉÁéáÔºå‰∏¶‰ΩøÁî®Ë°åÂãïÂÅ•Â∫∑ (MHEALTH) Ë≥áÊñôÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÂ∞çË∫´È´îÊ¥ªÂãïÈÄ≤Ë°åÂàÜÈ°û„ÄÇËíôÂú∞Âç°ÁæÖËøë‰ººÊ≥ïÊáâÁî®ÊñºË©≤Êû∂ÊßãÔºå‰ª•ÂÖãÊúç Shapley ÂÄºË®àÁÆóÊâÄÈúÄÁöÑÊôÇÈñìË§áÈõúÂ∫¶ÂíåÈ´òÈÅãÁÆóËÉΩÂäõÈúÄÊ±Ç„ÄÇ

##### **Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**
2309.09917v1 by Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren

In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.

ÊëòË¶ÅÔºöÂú®ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) Á†îÁ©∂‰∏≠Ôºå‰∏ªË¶ÅÈáçÁÇπÂú®‰∫é‰∏∫‰∏ìÂÆ∂Âíå‰ªé‰∏öËÄÖËß£ÈáäÊ®°Âûã„ÄÇÊ®°Âûã‰∏çÂèØÁü•ÂíåÂ±ÄÈÉ®Ëß£ÈáäÊñπÊ≥ïÂú®ËÆ∏Â§öÂ∫îÁî®‰∏≠Ë¢´ËÆ§‰∏∫ÊòØÂèØËß£Èáä‰∏îË∂≥Â§üÁöÑ„ÄÇÁÑ∂ËÄåÔºåÂú®ÂåªÁñó‰øùÂÅ•Á≠âÈ¢ÜÂüüÔºåÊúÄÁªàÁî®Êà∑ÊòØÁº∫‰πè‰∫∫Â∑•Êô∫ËÉΩÊàñÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÁöÑÊÇ£ËÄÖÔºåÂõ†Ê≠§Ëø´ÂàáÈúÄË¶ÅÊõ¥Êòì‰∫éÁêÜËß£‰∏îËÉΩÊøÄÂèëÂØπÊ®°ÂûãÊìç‰ΩúÁöÑ‰ø°‰ªªÁöÑÊ®°ÂûãËß£Èáä„ÄÇÊàë‰ª¨ÂÅáËÆæÁîüÊàêÂèôËø∞ÊÄß„ÄÅÊÇ£ËÄÖÁâπÂÆö‰∏îÂÖ®Â±ÄÔºàÊ®°ÂûãÊï¥‰ΩìÔºâÁöÑÊ®°ÂûãËß£ÈáäÂ∞ÜËÉΩÂ§üÊèêÈ´òÂèØÁêÜËß£ÊÄßÂπ∂ÊîØÊåÅÂÜ≥Á≠ñÂà∂ÂÆö„ÄÇÊàë‰ª¨‰ΩøÁî®ÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÂØπÊ≠§ËøõË°åÊµãËØïÔºå‰∏∫Ë¢´ËØÜÂà´‰∏∫ÊÇ£ÊúâÂÜ†ÂøÉÁóÖÈ´òÈ£éÈô©ÁöÑÊÇ£ËÄÖÁîüÊàêÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËß£Èáä„ÄÇËøô‰∫õËß£Èáä‰ºöÂëàÁé∞ÁªôÈùû‰∏ìÂÆ∂Áî®Êà∑„ÄÇÊàë‰ª¨ÂèëÁé∞Áî®Êà∑Âº∫ÁÉàÂÅèÂ•ΩÁâπÂÆöÁ±ªÂûãÁöÑËß£Èáä„ÄÇÂ§ßÂ§öÊï∞ÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂÖ®Â±ÄËß£ÈáäÔºåËÄåËæÉÂ∞èÁöÑ‰∏ÄÁªÑÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂ±ÄÈÉ®Ëß£Èáä„ÄÇÂü∫‰∫é‰ªªÂä°ÁöÑÂøÉÁêÜÊ®°ÂûãËØÑ‰º∞‰∏∫Ëøô‰∫õÂèÇ‰∏éËÄÖÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂèçÈ¶àÔºå‰ª•Â¢ûÂº∫ÂèôËø∞ÊÄßÂÖ®Â±ÄËß£Èáä„ÄÇËøôÂèçËøáÊù•ÂèàÊåáÂØº‰∫ÜÊó¢ÂÄºÂæó‰ø°ËµñÂèàÂèØÊìç‰ΩúÁöÑÂÅ•Â∫∑‰ø°ÊÅØÂ≠¶Á≥ªÁªüÁöÑËÆæËÆ°„ÄÇ

##### **Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**
2308.14321v1 by Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar

Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) Âíå‰æãË°åÊñá‰ª∂Ë®òÈåÑÂØ¶ÂãôÂú®ÁóÖÊÇ£ÁöÑÊó•Â∏∏ÁÖßË≠∑‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÊèê‰æõÂÅ•Â∫∑„ÄÅË®∫Êñ∑ÂíåÊ≤ªÁôÇÁöÑÊï¥È´îÁ¥ÄÈåÑ„ÄÇÁÑ∂ËÄåÔºåË§áÈõú‰∏îÂÜóÈï∑ÁöÑ EHR ÊïòËø∞ÊúÉËÆìÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖË∂ÖËºâÔºåÊúâË®∫Êñ∑‰∏çÊ∫ñÁ¢∫ÁöÑÈ¢®Èö™„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂÖ∂Âú®ÂêÑÁ®ÆË™ûË®Ä‰ªªÂãô‰∏äÁöÑÊΩõÂäõÔºå‰ΩÜÂÖ∂Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊáâÁî®ÈúÄË¶ÅÁ¢∫‰øùÂ∞áË®∫Êñ∑ÈåØË™§ÈôçÂà∞ÊúÄ‰ΩéÔºå‰∏¶Èò≤Ê≠¢ÁóÖÊÇ£ÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊ¶ÇËø∞‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÊï¥ÂêàÈÜ´Â≠∏Áü•Ë≠òÂúñË≠ú (KG) Âíå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂúñË≠úÊ®°ÂûãÔºöDr.KnowsÔºàÈùàÊÑü‰æÜËá™Ëá®Â∫äË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ãÔºâÔºå‰æÜÂ¢ûÂº∑ LLM Âú®Ëá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂæûÁæéÂúãÂúãÂÆ∂ÈÜ´Â≠∏ÂúñÊõ∏È§®ÁöÑÁµ±‰∏ÄÈÜ´Â≠∏Ë™ûË®ÄÁ≥ªÁµ± (UMLS) ‰∏≠Ë°çÁîüÂá∫ KGÔºåÈÄôÊòØ‰∏ÄÂÄãÂº∑Â§ßÁöÑÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂÑ≤Â≠òÂ∫´„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂê¶ÂÆö‰∫ÜÈ†êÂÖàË®ìÁ∑¥ÁöÑÈúÄË¶ÅÔºåËÄåÊòØÂ∞á KG ‰ΩúÁÇ∫ËºîÂä©Â∑•ÂÖ∑ÔºåÂçîÂä©Ëß£ÈáãÂíåÁ∏ΩÁµêË§áÈõúÁöÑÈÜ´Â≠∏Ê¶ÇÂøµ„ÄÇ‰ΩøÁî®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´Èô¢Ë≥áÊñôÈõÜÔºåÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÂ∞á LLM Ëàá KG ÁµêÂêàÁöÑÂª∫Ë≠∞ÊñπÊ≥ïÊúâÊΩõÂäõÊèêÈ´òËá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÊ¢ùÂèØËß£ÈáãÁöÑË®∫Êñ∑ÈÄîÂæëÔºåËÆìÊàëÂÄëÊõ¥Êé•ËøëÂØ¶Áèæ AI Â¢ûÂº∑ÁöÑË®∫Êñ∑Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ±„ÄÇ

##### **Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**
2308.09380v1 by Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai

Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÁî®ÊñºË®∫Êñ∑ËÜùÈ™®ÈóúÁØÄÁÇé (OA) ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÂõ†ÂÖ∂Áº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßËÄåÂèóÂà∞ÊâπË©ïÔºåÂÑòÁÆ°ÂÆÉÂÄëÈÅîÂà∞‰∫ÜÈ°û‰ººÈÜ´Â≠∏Â∞àÂÆ∂ÁöÑË°®Áèæ„ÄÇÈÄôÁ®Æ‰∏çÈÄèÊòéÊÄß‰ΩøÂæóÂÆÉÂÄëÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Èõ£‰ª•Ë¢´‰ø°‰ªª„ÄÇÊúÄËøëÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÂ∞àÈñÄÊäÄË°ìÔºåÂÆÉËÉΩÈÄèÈÅéÊè≠Á§∫È†êÊ∏¨ÁöÑÊé®Â∞éÊñπÂºè‰æÜÊèê‰æõÂ∞çÊ®°ÂûãÈ†êÊ∏¨ÁöÑ‰ø°ÂøÉÔºåÂæûËÄå‰øÉÈÄ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî® AI Á≥ªÁµ±„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÈáùÂ∞çËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊâÄ‰ΩøÁî®ÁöÑ XAI ÊäÄË°ìÁöÑÁ¨¨‰∏Ä‰ªΩË™øÊü•„ÄÇXAI ÊäÄË°ìÂæûÂÖ©ÂÄãËßíÂ∫¶ÈÄ≤Ë°åË®éË´ñÔºöË≥áÊñôÂèØËß£ÈáãÊÄßÂíåÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÇÊú¨ÊñáÁöÑÁõÆÁöÑÊòØÊèê‰æõÂ∞ç XAI Âú®Êõ¥ÂèØÈù†ÁöÑËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊΩõÂäõÁöÑÂØ∂Ë≤¥Ë¶ãËß£Ôºå‰∏¶ÈºìÂãµÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Êé°Áî®ÂÆÉ„ÄÇ

##### **Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**
2308.08407v1 by Munib Mesinovic, Peter Watkinson, Tingting Zhu

Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

ÊëòË¶ÅÔºöÊúÄËøëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®ÈÄ≤Â±ïÈ°ØÁ§∫Âá∫‰ª§‰∫∫Èõ£‰ª•ÁΩÆ‰ø°ÁöÑÊâøË´æÔºåÂú®Ë®∫Êñ∑ÂíåÁñæÁóÖÈ†êÂæåÊñπÈù¢Ë∂ÖË∂ä‰∫∫È°ûË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÈö®Ëëó‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÁöÑÊó•ÁõäË§áÈõúÔºå‰∫∫ÂÄëÂ∞çÂÖ∂‰∏çÈÄèÊòéÊÄß„ÄÅÊΩõÂú®ÂÅèÂ∑ÆÂíåÂ∞çÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÊÑüÂà∞ÊìîÊÜÇ„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øù‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±ÁöÑ‰ø°‰ªªÂíåÂèØÈù†ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨Ê®°Âûã‰∏≠ÔºåÂèØËß£ÈáãÊÄßËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇÂèØËß£ÈáãÊÄßÈÄöÂ∏∏Ë¢´Á®±ÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±Êèê‰æõÂÖ∂Ê±∫Á≠ñÈÇèËºØÊàñÊ±∫Á≠ñÊú¨Ë∫´Â∞ç‰∫∫È°ûÂà©ÁõäÁõ∏ÈóúËÄÖÁöÑÂº∑ÊúâÂäõËß£ÈáãÁöÑËÉΩÂäõ„ÄÇÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨‰∏≠ÔºåÂèØËß£ÈáãÊÄßÁöÑÂÖ∂‰ªñÊñπÈù¢ÔºåÂ¶ÇÂÖ¨Âπ≥ÊÄß„ÄÅÂÅèË¶ã„ÄÅ‰ø°‰ªªÂíåÈÄèÊòéÂ∫¶Ôºå‰πü‰ª£Ë°®‰∫ÜË∂ÖË∂äÂèØËß£ÈáãÊÄßÁöÑÈáçË¶ÅÊ¶ÇÂøµ„ÄÇÂú®Êú¨Ê¨°ÂØ©Êü•‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÊ¶ÇÂøµ‰πãÈñìÁöÑÈóú‰øÇÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁ∂ìÂ∏∏‰∏ÄËµ∑Êàñ‰∫íÊèõ‰ΩøÁî®„ÄÇÊú¨ÂØ©Êü•ÈÇÑË®éË´ñ‰∫ÜÁÇ∫Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨ÈñãÁôºÂèØËß£ÈáãÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÂº∑Ë™ø‰∫ÜÂú®Ëá®Â∫äÂØ¶Ë∏ê‰∏≠Â∞çÂ§öÁ®ÆÂ∏∏Ë¶ãÊ®°ÂºèÈÄ≤Ë°åÂÆöÈáèÂíåËá®Â∫äË©ï‰º∞ÂíåÈ©óË≠âÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂ§ñÈÉ®È©óË≠âÂíåÂ§öÊ®£ÂåñÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁõ∏ÁµêÂêàÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Â¢ûÂº∑‰ø°‰ªªÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊé°Áî®Âö¥Ê†ºÁöÑÊ∏¨Ë©¶Ôºå‰æãÂ¶Ç‰ΩøÁî®ÂÖ∑ÊúâÂ∑≤Áü•ÁîüÊàêÂõ†Á¥†ÁöÑÂêàÊàêÊï∏ÊìöÈõÜÔºåÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÂèØÈù†ÊÄß„ÄÇÈñãÊîæÁç≤ÂèñÂíå‰ª£Á¢ºÂÖ±‰∫´Ë≥áÊ∫êÂ∞çÊñºÈÄèÊòéÂ∫¶ÂíåÂèØÈáçË§áÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂæûËÄå‰øÉÈÄ≤ÂèØËß£ÈáãÁ†îÁ©∂ÁöÑÂ¢ûÈï∑ÂíåÂèØ‰ø°Â∫¶„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÂæûËá®Â∫äÈÜ´ÁîüÂà∞ÈñãÁôº‰∫∫Âì°ÔºåÊé°Áî®Á´ØÂà∞Á´ØÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞çÊñºËá®Â∫äÈ¢®Èö™È†êÊ∏¨ÁöÑÊàêÂäüËá≥ÈóúÈáçË¶Å„ÄÇ

##### **FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**
2309.12325v3 by Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A Gonz√°lez, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Horst Joachim Mayer, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, Ir√®ne Buvat, Isabell Tributsch, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor Cerd√° Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, Llu√≠s Donoso-Bach, Luis Mart√≠-Bonmat√≠, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marina Camacho, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, M√≥nica Cano Abad√≠a, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver D√≠az, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna Auss√≥, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, X√®nia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans

Despite major advances in artificial intelligence (AI) for medicine and
healthcare, the deployment and adoption of AI technologies remain limited in
real-world clinical practice. In recent years, concerns have been raised about
the technical, clinical, ethical and legal risks associated with medical AI. To
increase real world adoption, it is essential that medical AI tools are trusted
and accepted by patients, clinicians, health organisations and authorities.
This work describes the FUTURE-AI guideline as the first international
consensus framework for guiding the development and deployment of trustworthy
AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and
currently comprises 118 inter-disciplinary experts from 51 countries
representing all continents, including AI scientists, clinicians, ethicists,
and social scientists. Over a two-year period, the consortium defined guiding
principles and best practices for trustworthy AI through an iterative process
comprising an in-depth literature review, a modified Delphi survey, and online
consensus meetings. The FUTURE-AI framework was established based on 6 guiding
principles for trustworthy AI in healthcare, i.e. Fairness, Universality,
Traceability, Usability, Robustness and Explainability. Through consensus, a
set of 28 best practices were defined, addressing technical, clinical, legal
and socio-ethical dimensions. The recommendations cover the entire lifecycle of
medical AI, from design, development and validation to regulation, deployment,
and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which
provides a structured approach for constructing medical AI tools that will be
trusted, deployed and adopted in real-world practice. Researchers are
encouraged to take the recommendations into account in proof-of-concept stages
to facilitate future translation towards clinical practice of medical AI.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈÜ´Â≠∏ÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊúâÈáçÂ§ßÁöÑÈÄ≤Â±ïÔºå‰ΩÜ AI ÊäÄË°ìÁöÑÈÉ®ÁΩ≤ÂíåÊé°Áî®Âú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÂØ¶Âãô‰∏≠‰ªçÁÑ∂ÊúâÈôê„ÄÇËøëÂπ¥‰æÜÔºå‰∫∫ÂÄëÂ∞çÊñºËàáÈÜ´ÁôÇ AI Áõ∏ÈóúÁöÑÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÂÄ´ÁêÜÂíåÊ≥ïÂæãÈ¢®Èö™ÊèêÂá∫‰∫ÜÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÂ¢ûÂä†ÁèæÂØ¶‰∏ñÁïåÁöÑÊé°Áî®ÁéáÔºåÈÜ´ÁôÇ AI Â∑•ÂÖ∑ÂøÖÈ†àÁç≤ÂæóÊÇ£ËÄÖ„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÈÜ´ÁôÇÊ©üÊßãÂíåÁï∂Â±ÄÁöÑ‰ø°‰ªªÂíåÊé•Âèó„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂ∞á FUTURE-AI ÊåáÂçóÊèèËø∞ÁÇ∫ÊåáÂ∞éÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI Â∑•ÂÖ∑ÈñãÁôºÂíåÈÉ®ÁΩ≤ÁöÑÁ¨¨‰∏ÄÂÄãÂúãÈöõÂÖ±Ë≠òÊû∂Êßã„ÄÇFUTURE-AI ËÅØÁõüÊàêÁ´ãÊñº 2021 Âπ¥ÔºåÁõÆÂâçÁî±‰æÜËá™ 51 ÂÄãÂúãÂÆ∂ÁöÑ 118 ‰ΩçË∑®È†òÂüüÂ∞àÂÆ∂ÁµÑÊàêÔºå‰ª£Ë°®ÊâÄÊúâÊ¥≤ÔºåÂåÖÊã¨ AI ÁßëÂ≠∏ÂÆ∂„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÂÄ´ÁêÜÂ≠∏ÂÆ∂ÂíåÁ§æÊúÉÁßëÂ≠∏ÂÆ∂„ÄÇÂú®ÂÖ©Âπ¥ÁöÑÊôÇÈñìË£°ÔºåË©≤ËÅØÁõüÈÄöÈÅé‰∏ÄÂÄãÂèçË¶ÜÈÅãÁÆóÁöÑÈÅéÁ®ãÂÆöÁæ©‰∫ÜÂèØ‰ø°Ë≥¥ AI ÁöÑÊåáÂ∞éÂéüÂâáÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÂåÖÊã¨Ê∑±ÂÖ•ÁöÑÊñáÁçªÂõûÈ°ß„ÄÅ‰øÆÊîπÂæåÁöÑÂæ∑ÁàæËè≤Ë™øÊü•ÂíåÁ∑ö‰∏äÂÖ±Ë≠òÊúÉË≠∞„ÄÇFUTURE-AI Êû∂ÊßãÊòØÂü∫ÊñºÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI ÁöÑ 6 È†ÖÊåáÂ∞éÂéüÂâáÂª∫Á´ãÁöÑÔºåÂç≥ÂÖ¨Âπ≥ÊÄß„ÄÅÊôÆÈÅçÊÄß„ÄÅÂèØËøΩÊ∫ØÊÄß„ÄÅÂèØÁî®ÊÄß„ÄÅÁ©©ÂÅ•ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÈÄöÈÅéÂÖ±Ë≠òÔºåÂÆöÁæ©‰∫Ü‰∏ÄÁµÑ 28 È†ÖÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÊ∂µËìãÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÊ≥ïÂæãÂíåÁ§æÊúÉÂÄ´ÁêÜÂ±§Èù¢„ÄÇÂª∫Ë≠∞Ê∂µËìã‰∫ÜÈÜ´ÁôÇ AI ÁöÑÊï¥ÂÄãÁîüÂëΩÈÄ±ÊúüÔºåÂæûË®≠Ë®à„ÄÅÈñãÁôºÂíåÈ©óË≠âÂà∞Ê≥ïË¶è„ÄÅÈÉ®ÁΩ≤ÂíåÁõ£Êéß„ÄÇFUTURE-AI ÊòØ‰∏ÄÂÄãÂü∫ÊñºÈ¢®Èö™„ÄÅÁÑ°ÂÅáË®≠ÁöÑÊåáÂçóÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂª∫ÊßãÂ∞áÂú®ÁèæÂØ¶‰∏ñÁïåÂØ¶Âãô‰∏≠ÂèóÂà∞‰ø°‰ªª„ÄÅÈÉ®ÁΩ≤ÂíåÊé°Áî®ÁöÑÈÜ´ÁôÇ AI Â∑•ÂÖ∑„ÄÇÈºìÂãµÁ†îÁ©∂‰∫∫Âì°Âú®Ê¶ÇÂøµÈ©óË≠âÈöéÊÆµËÄÉÊÖÆÈÄô‰∫õÂª∫Ë≠∞Ôºå‰ª•‰øÉÈÄ≤Êú™‰æÜÂ∞áÈÜ´ÁôÇ AI ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶Âãô„ÄÇ

##### **Explainable AI applications in the Medical Domain: a systematic review**
2308.05411v1 by Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis

Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠Â∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÅÁóÖ‰∫∫ÁÖßË≠∑ÂíåÂÖ∂‰ªñÈ†òÂüü‰∏≠Âá∫Áèæ‰∫ÜÊñ∞ËààÊáâÁî®„ÄÇÈõñÁÑ∂ÈÄô‰∫õÊáâÁî®Â∑≤Âú®ÂõûÈ°ßÊÄßÁ†îÁ©∂‰∏≠Ë¢´Ë≠âÂØ¶ÊòØÊàêÂäüÁöÑÔºå‰ΩÜÂØ¶Èöõ‰∏äÂè™ÊúâÊ•µÂ∞ëÊï∏ÊáâÁî®ÊñºÂØ¶Âãô„ÄÇÈÜ´ÁôÇ AI È†òÂüüÈù¢Ëá®ËëóÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨Âª∫Á´ã‰ΩøÁî®ËÄÖ‰ø°‰ªª„ÄÅÈÅµÂÆàÊ≥ïË¶è„ÄÅ‰ΩøÁî®Ë≥áÊñôÁ¨¶ÂêàÂÄ´ÁêÜ„ÄÇÂèØËß£Èáã AI (XAI) ÁöÑÁõÆÊ®ôÊòØËÆì‰∫∫È°û‰∫ÜËß£ AI ‰∏¶Áõ∏‰ø°ÂÖ∂ÁµêÊûú„ÄÇÊú¨ÊñáÈáùÂ∞çÊúÄËøëÂπæÂπ¥ÁôºË°®ÁöÑ 198 ÁØáÊñáÁ´†ÁöÑÂÖ∑‰ª£Ë°®ÊÄßÊ®£Êú¨ÔºåÊèêÂá∫ÊúâÈóúÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥ÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑÊúÄÊñ∞ÁôºÂ±ïÁöÑÊñáÁçªÂõûÈ°ß„ÄÇÁõ∏ÈóúÊñáÁ´†ÁöÑÁ≥ªÁµ±ÊÄßÁ∂úÂêàÊï¥ÁêÜÁî¢Áîü‰∫ÜÂ§öÈ†ÖÁôºÁèæÔºö(1) ÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÂ§ßÂ§öÊé°Áî®ËàáÊ®°ÂûãÁÑ°ÈóúÁöÑ XAI ÊäÄË°ìÔºå(2) Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑ‰ΩøÁî®ÁéáÈ´òÊñºÂÖ∂‰ªñÈ°ûÂûãÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÔºå(3) ÂèØËß£ÈáãÊÄßË¢´Áî®Êñº‰øÉÈÄ≤‰ø°‰ªªÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂Â†±ÂëäÈÜ´Â∏´ÂèÉËàáËø¥ÂúàÔºå(4) Ë¶ñË¶∫Âíå‰∫íÂãïÂºè‰ΩøÁî®ËÄÖ‰ªãÈù¢Â∞çÊñºÁêÜËß£Á≥ªÁµ±ÁöÑËß£ÈáãÂíåÂª∫Ë≠∞Êõ¥ÊúâÁî®„ÄÇÈúÄË¶ÅÊõ¥Â§öÈÜ´ÁôÇÂíå AI Â∞àÂÆ∂Âêà‰ΩúÈÄ≤Ë°åÁ†îÁ©∂ÔºåÈÄôÊúâÂä©ÊñºÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑË®≠Ë®à„ÄÅÂØ¶‰ΩúÂíåË©ï‰º∞Êèê‰æõÈÅ©Áï∂Êû∂Êßã„ÄÇ

##### **Exploring the Role of Explainability in AI-Assisted Embryo Selection**
2308.02534v1 by Lucia Urcelay, Daniel Hinjos, Pablo A. Martin-Torres, Marta Gonzalez, Marta Mendez, Salva C√≠vico, Sergio √Ålvarez-Napagao, Dario Garcia-Gasulla

In Vitro Fertilization is among the most widespread treatments for
infertility. One of its main challenges is the evaluation and selection of
embryo for implantation, a process with large inter- and intra-clinician
variability. Deep learning based methods are gaining attention, but their
opaque nature compromises their acceptance in the clinical context, where
transparency in the decision making is key. In this paper we analyze the
current work in the explainability of AI-assisted embryo analysis models,
identifying the limitations. We also discuss how these models could be
integrated in the clinical context as decision support systems, considering the
needs of clinicians and patients. Finally, we propose guidelines for the sake
of increasing interpretability and trustworthiness, pushing this technology
forward towards established clinical practice.

ÊëòË¶ÅÔºöÈ´îÂ§ñÂèóÁ≤æÊòØÊ≤ªÁôÇ‰∏çÂ≠ïÁóáÊúÄÂª£Ê≥õÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇÂÖ∂‰∏ªË¶ÅÊåëÊà∞‰πã‰∏ÄÊòØË©ï‰º∞ÂíåÈÅ∏ÊìáËÉöËÉéÈÄ≤Ë°åÊ§çÂÖ•ÔºåÊ≠§ÈÅéÁ®ãÂÖ∑ÊúâÂæàÂ§ßÁöÑËá®Â∫äÈñìÂíåËá®Â∫äÂÖßËÆäÁï∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÊ≠£ÂèóÂà∞ÈóúÊ≥®Ôºå‰ΩÜÂÖ∂‰∏çÈÄèÊòéÁöÑÊÄßË≥™ÊúÉÂΩ±ÈüøÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊé•ÂèóÂ∫¶ÔºåËÄåÈÄèÊòéÂ∫¶Âú®Ê±∫Á≠ñÂà∂ÂÆö‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü AI ËºîÂä©ËÉöËÉéÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÁèæÊúâÂ∑•‰ΩúÔºå‰∏¶ÊâæÂá∫ÂÖ∂Â±ÄÈôêÊÄß„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊ®°Âûã‰ΩúÁÇ∫Ê±∫Á≠ñÊîØÊåÅÁ≥ªÁµ±Êï¥ÂêàÂà∞Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÂêåÊôÇËÄÉÊÖÆËá®Â∫äÈÜ´ÁîüÂíåÊÇ£ËÄÖÁöÑÈúÄÊ±Ç„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÁöÑÊ∫ñÂâáÔºåÊé®ÈÄ≤ÈÄôÈ†ÖÊäÄË°ìÊúùËëóÊó¢ÂÆöÁöÑËá®Â∫äÂØ¶ÂãôÈÇÅÈÄ≤„ÄÇ

##### **A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**
2307.14246v1 by Timo Speith, Markus Langer

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ã (RE) È†òÂüü‰∏≠ÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Âú®Â∞á AI ÊîØÊåÅÁöÑÁ≥ªÁµ±Ëàá‰ΩøÁî®ËÄÖÈúÄÊ±Ç„ÄÅÁ§æÊúÉÊúüÊúõÂíåÊ≥ïË¶èÊ®ôÊ∫ñÁõ∏Á¨¶ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÈ°ØËëóÔºåÂ∑≤Áç≤ÂæóË™çÂèØ„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÂèØËß£ÈáãÊÄßÂ∑≤ÊàêÁÇ∫ÂΩ±ÈüøÁ≥ªÁµ±ÂìÅË≥™ÁöÑÈáçË¶ÅÈùûÂäüËÉΩÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáãÊÄßËàáÊïàËÉΩ‰πãÈñìÁöÑÂÅáÂÆöÊ¨äË°°ÊåëÊà∞‰∫ÜÂèØËß£ÈáãÊÄßÁöÑÂÅáÂÆöÊ≠£Èù¢ÂΩ±Èüø„ÄÇÂ¶ÇÊûúÊªøË∂≥ÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁµ±ÊïàËÉΩÔºåÈÇ£È∫ºÂøÖÈ†à‰ªîÁ¥∞ËÄÉÊÖÆÈÄô‰∫õÂìÅË≥™Èù¢Âêë‰∏≠Âì™‰∏ÄÂÄãÂÑ™ÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉÂÄë‰πãÈñìÈÄ≤Ë°åÊäòË°∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞Êé¢Ë®é‰∫ÜÈÄôÁ®ÆÂÅáÂÆöÁöÑÊ¨äË°°„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÊúÄÂ•ΩÁöÑÊñπÊ≥ïÊòØ‰ª•‰∏ÄÁ®ÆÁ¥∞Á∑ªÁöÑÊñπÂºè‰æÜËôïÁêÜÔºåÈÄôÁ®ÆÊñπÂºèÂåÖÂê´Ë≥áÊ∫êÂèØÁî®ÊÄß„ÄÅÈ†òÂüüÁâπÊÄßÂíåÈ¢®Èö™ËÄÉÈáè„ÄÇÈÄèÈÅéÊèê‰æõÊú™‰æÜÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÁöÑÂü∫Á§éÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®ÊèêÂçá AI ÁöÑ RE È†òÂüü„ÄÇ

##### **Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**
2307.14239v1 by Barnaby Crook, Maximilian Schl√ºter, Timo Speith

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ãÔºàREÔºâÈ¢ÜÂüüÔºåÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÔºàXAIÔºâÂú®Â∞Ü‰∫∫Â∑•Êô∫ËÉΩÊîØÊåÅÁöÑÁ≥ªÁªü‰∏éÁî®Êà∑ÈúÄÊ±Ç„ÄÅÁ§æ‰ºöÊúüÊúõÂíåÁõëÁÆ°Ê†áÂáÜÁõ∏‰∏ÄËá¥ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÂá∏ÊòæÔºåÂπ∂Ëé∑Âæó‰∫ÜËÆ§ÂèØ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂèØËß£ÈáäÊÄßÂ∑≤Êàê‰∏∫ÂΩ±ÂìçÁ≥ªÁªüË¥®ÈáèÁöÑÈáçË¶ÅÈùûÂäüËÉΩÊÄßÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáäÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÊåëÊàò‰∫ÜÂèØËß£ÈáäÊÄßÁöÑÊ≠£Èù¢ÂΩ±Âìç„ÄÇÂ¶ÇÊûúÊª°Ë∂≥ÂèØËß£ÈáäÊÄßÁöÑË¶ÅÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁªüÊÄßËÉΩÔºåÈÇ£‰πàÂøÖÈ°ª‰ªîÁªÜËÄÉËôëËøô‰∫õË¥®ÈáèÊñπÈù¢‰∏≠ÁöÑÂì™‰∏Ä‰∏™‰ºòÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉ‰ª¨‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊâπÂà§ÊÄßÂú∞ËÄÉÂØü‰∫ÜÊâÄË∞ìÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÊúÄÂ•Ω‰ª•‰∏ÄÁßçÁªÜËá¥ÂÖ•ÂæÆÁöÑÊñπÂºèÊù•Â§ÑÁêÜÂÆÉÔºåËøôÁßçÊñπÂºèÁªìÂêà‰∫ÜËµÑÊ∫êÂèØÁî®ÊÄß„ÄÅÈ¢ÜÂüüÁâπÂæÅÂíåÈ£éÈô©ËÄÉËôë„ÄÇÈÄöËøá‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂÆûË∑µÊèê‰æõÂü∫Á°ÄÔºåËøôÈ°πÂ∑•‰ΩúÊó®Âú®Êé®Ëøõ‰∫∫Â∑•Êô∫ËÉΩÁöÑ RE È¢ÜÂüü„ÄÇ

##### **Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**
2308.02047v1 by Henry Fraser, Jose-Miguel Bello y Villarino

This paper critically evaluates the European Commission's proposed AI Act's
approach to risk management and risk acceptability for high-risk AI systems
that pose risks to fundamental rights and safety. The Act aims to promote
"trustworthy" AI with a proportionate regulatory burden. Its provisions on risk
acceptability require residual risks from high-risk systems to be reduced or
eliminated "as far as possible", having regard to the "state of the art". This
criterion, especially if interpreted narrowly, is unworkable and promotes
neither proportionate regulatory burden, nor trustworthiness. By contrast the
Parliament's most recent draft amendments to the risk management provisions
introduce "reasonableness", cost-benefit analysis, and are more transparent
about the value-laden and contextual nature of risk acceptability judgements.
This paper argues that the Parliament's approach is more workable, and better
balances the goals of proportionality and trustworthiness. It explains what
reasonableness in risk acceptability judgments would entail, drawing on
principles from negligence law and European medical devices regulation. And it
contends that the approach to risk acceptability judgments need a firm
foundation of civic legitimacy: including detailed guidance or involvement from
regulators, and meaningful input from affected stakeholders.

ÊëòË¶ÅÔºöÊú¨ÊñáÂö¥Ê†ºË©ï‰º∞Ê≠êÊ¥≤ÂßîÂì°ÊúÉÊèêÂá∫ÁöÑ AI Ê≥ïÊ°àÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÂíåÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂü∫Êú¨Ê¨äÂà©ÂíåÂÆâÂÖ®ÊßãÊàêÈ¢®Èö™ÁöÑÈ´òÈ¢®Èö™ AI Á≥ªÁµ±„ÄÇË©≤Ê≥ïÊ°àÊó®Âú®‰ª•Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†Êìî‰øÉÈÄ≤„ÄåÂÄºÂæó‰ø°Ë≥¥„ÄçÁöÑ AI„ÄÇÂÖ∂ÈóúÊñºÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊ¢ùÊ¨æË¶ÅÊ±ÇÂ∞áÈ´òÈ¢®Èö™Á≥ªÁµ±ÁöÑÊÆòÈ§òÈ¢®Èö™Ê∏õ‰ΩéÊàñÊ∂àÈô§„ÄåÁõ°ÂèØËÉΩ„ÄçÔºå‰∏¶ËÄÉÊÖÆ„ÄåÊäÄË°ìÁãÄÊÖã„Äç„ÄÇÊ≠§Ê∫ñÂâáÔºåÁâπÂà•ÊòØÂ¶ÇÊûúÁãπÁæ©Ëß£ÈáãÔºåÁÑ°Ê≥ïÂü∑Ë°åÔºåÊó¢‰∏ç‰øÉÈÄ≤Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†ÊìîÔºå‰πü‰∏ç‰øÉÈÄ≤ÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåË≠∞ÊúÉÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÊ¢ùÊ¨æÁöÑÊúÄÊñ∞‰øÆÊ≠£ËçâÊ°àÂºïÂÖ•‰∫Ü„ÄåÂêàÁêÜÊÄß„Äç„ÄÅÊàêÊú¨ÊïàÁõäÂàÜÊûêÔºå‰∏¶‰∏îÊõ¥ÈÄèÊòéÂú∞Ë™™Êòé‰∫ÜÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÂÉπÂÄºËßÄÂíåËÉåÊôØÊÄßË≥™„ÄÇÊú¨ÊñáË´ñË≠âË≠∞ÊúÉÁöÑÊñπÊ≥ïÊõ¥ÂèØË°åÔºå‰∏îËÉΩÊõ¥Â•ΩÂú∞Âπ≥Ë°°Áõ∏Á®±ÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÁöÑÁõÆÊ®ô„ÄÇÊú¨ÊñáË™™ÊòéÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑‰∏≠ÁöÑÂêàÁêÜÊÄßÊúÉÂ∏∂‰æÜ‰ªÄÈ∫ºÔºå‰∏¶Ê†πÊìöÈÅéÂ§±Ê≥ïÂíåÊ≠êÊ¥≤ÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶è‰∏≠ÁöÑÂéüÂâáÈÄ≤Ë°åË™™Êòé„ÄÇÊú¨Êñá‰∏ªÂºµÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÊñπÊ≥ïÈúÄË¶ÅÁ©©Âõ∫ÁöÑÂÖ¨Ê∞ëÂêàÊ≥ïÊÄßÂü∫Á§éÔºöÂåÖÊã¨Áõ£ÁÆ°Ê©üÊßãÁöÑË©≥Á¥∞ÊåáÂ∞éÊàñÂèÉËàáÔºå‰ª•ÂèäÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÊúâÊÑèÁæ©ÊäïÂÖ•„ÄÇ

##### **eXplainable Artificial Intelligence (XAI) in aging clock models**
2307.13704v3 by Alena Kalyakulina, Igor Yusipov, Alexey Moskalev, Claudio Franceschi, Mikhail Ivanchenko

eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the "aging clocks" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÊ©üÂô®Â≠∏Áøí‰∏≠Âø´ÈÄüÈÄ≤Â±ïÁöÑÈ†òÂüüÔºåÊó®Âú®Ëß£ÈñãË§áÈõúÊ®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇXAI Âú®ÊïèÊÑüÊáâÁî®‰∏≠ÁâπÂà•ÈúÄË¶ÅÔºå‰æãÂ¶ÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåÁï∂Ë®∫Êñ∑„ÄÅÂª∫Ë≠∞ÂíåÊ≤ªÁôÇÈÅ∏ÊìáÂèØËÉΩ‰æùË≥¥Êñº‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÂÅöÂá∫ÁöÑÊ±∫Á≠ñÊôÇ„ÄÇ‰∫∫Â∑•Êô∫ÊÖßÊñπÊ≥ï‰πüÂ∑≤Âª£Ê≥õÁî®ÊñºËÄÅÂåñÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÂú®ÈñãÁôºÁîüÁâ©ÊôÇÈêòÊ®°ÂûãÂíåË≠òÂà•ËÄÅÂåñÂíåËàáÂπ¥ÈΩ°Áõ∏ÈóúÁñæÁóÖÁöÑÁîüÁâ©Ê®ôË™åÁâ©ÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåÈÄôË£° XAI ÁöÑÊΩõÂäõÊúâÂæÖÂÖÖÂàÜË™çË≠ò„ÄÇÊàëÂÄëË®éË´ñ‰∫Ü XAI Âú®ÈñãÁôº„ÄåËÄÅÂåñÊôÇÈêò„ÄçÊñπÈù¢ÁöÑÊáâÁî®Ôºå‰∏¶Â∞çÊåâÁâπÂÆöÁîüÁêÜÁ≥ªÁµ±ÁöÑÈáçÈªûÂàÜÈ°ûÁöÑÊñáÁçªÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇ

##### **Interpreting and Correcting Medical Image Classification with PIP-Net**
2307.10404v2 by Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, J√∂rg Schl√∂tterer, Maurice van Keulen, Christin Seifert

Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.

ÊëòË¶ÅÔºöÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÊòØÂèØËß£ÈáãË®≠Ë®àÁöÑÂΩ±ÂÉèÂàÜÈ°ûÂô®Ôºå‰πüÊòØÈªëÁÆ± AI ÁöÑ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫ÜËß£ÈáãÊÄßÊ©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØ PIP-NetÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñô‰∏äËá™ÂãïÂåñË®∫Êñ∑ÊîØÊè¥ÁöÑÈÅ©Áî®ÊÄßÂíåÊΩõÂäõ„ÄÇPIP-Net Â≠∏Áøí‰∫∫È°ûÂèØÁêÜËß£ÁöÑÂéüÂûãÂΩ±ÂÉèÈÉ®ÂàÜÔºåÊàëÂÄëË©ï‰º∞ÂÖ∂Âú®È™®ÊäòÊ™¢Ê∏¨ÂíåÁöÆËÜöÁôåË®∫Êñ∑ÊñπÈù¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÁôºÁèæ PIP-Net ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãÁ¨¶ÂêàÈÜ´Â≠∏ÂàÜÈ°ûÊ®ôÊ∫ñÔºåÂêåÊôÇÂÉÖÊèê‰æõÂΩ±ÂÉèÂ±§Á¥öÈ°ûÂà•Ê®ôÁ±§„ÄÇÁî±Êñº PIP-Net Â∞çÂéüÂûãÁöÑÁÑ°Áõ£Áù£È†êË®ìÁ∑¥ÔºåÂõ†Ê≠§ÂèØ‰ª•ËºïÈ¨ÜË≠òÂà•Ë≥áÊñôÂìÅË≥™ÂïèÈ°åÔºå‰æãÂ¶Ç X ÂÖâ‰∏≠ÁöÑ‰∏çÈúÄË¶ÅÊñáÂ≠óÊàñÊ®ôÁ±§ÈåØË™§„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ¶ñÊ¨°Â±ïÁ§∫‰∫∫È°ûÂèØ‰ª•ÈÄèÈÅéÁõ¥Êé•ÂÅúÁî®‰∏çÈúÄË¶ÅÁöÑÂéüÂûã‰æÜÊâãÂãï‰øÆÊ≠£ PIP-Net ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÁî±ÊñºÂÖ∂ÂèØËß£ÈáãÊÄßÂíåÈÄ≤ÈöéÊ®°ÂûãÈô§ÈåØÁöÑÊΩõÂäõÔºåÂõ†Ê≠§ÊúâÊúõÊáâÁî®ÊñºÈÜ´ÁôÇ„ÄÇ

##### **Explaining and visualizing black-box models through counterfactual paths**
2307.07764v3 by Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek

Explainable AI (XAI) is an increasingly important area of machine learning
research, which aims to make black-box models transparent and interpretable. In
this paper, we propose a novel approach to XAI that uses the so-called
counterfactual paths generated by conditional permutations of features. The
algorithm measures feature importance by identifying sequential permutations of
features that most influence changes in model predictions. It is particularly
suitable for generating explanations based on counterfactual paths in knowledge
graphs incorporating domain knowledge. Counterfactual paths introduce an
additional graph dimension to current XAI methods in both explaining and
visualizing black-box models. Experiments with synthetic and medical data
demonstrate the practical applicability of our approach.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI (XAI) ÊòØÊ©üÂô®Â≠∏ÁøíÁ†îÁ©∂‰∏≠Êó•ÁõäÈáçË¶ÅÁöÑÈ†òÂüüÔºåÂÖ∂ÁõÆÊ®ôÊòØËÆìÈªëÁÆ±Ê®°ÂûãÈÄèÊòé‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑ XAI ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Áî±ÁâπÂæµÊ¢ù‰ª∂ÁΩÆÊèõÁî¢ÁîüÁöÑÊâÄË¨ÇÂèç‰∫ãÂØ¶Ë∑ØÂæë„ÄÇË©≤ÊºîÁÆóÊ≥ïÈÄèÈÅéË≠òÂà•ÁâπÂæµÁöÑÈ†ÜÂ∫èÁΩÆÊèõ‰æÜË°°ÈáèÁâπÂæµÈáçË¶ÅÊÄßÔºåÈÄô‰∫õÁΩÆÊèõÊúÄËÉΩÂΩ±ÈüøÊ®°ÂûãÈ†êÊ∏¨ÁöÑËÆäÂåñ„ÄÇÂÆÉÁâπÂà•ÈÅ©ÂêàÊ†πÊìöÂåÖÂê´È†òÂüüÁü•Ë≠òÁöÑÁü•Ë≠òÂúñË≠ú‰∏≠ÁöÑÂèç‰∫ãÂØ¶Ë∑ØÂæë‰æÜÁî¢ÁîüËß£Èáã„ÄÇÂèç‰∫ãÂØ¶Ë∑ØÂæëÂú®Ëß£ÈáãÂíåË¶ñË¶∫ÂåñÈªëÁÆ±Ê®°ÂûãÊôÇÔºåÁÇ∫ÁõÆÂâçÁöÑ XAI ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÈ°çÂ§ñÁöÑÂúñÂΩ¢Á∂≠Â∫¶„ÄÇ‰ΩøÁî®ÂêàÊàêÂíåÈÜ´ÁôÇË≥áÊñôÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂØ¶Áî®ÈÅ©Áî®ÊÄß„ÄÇ

##### **Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**
2307.02131v5 by Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci

The field of explainability in artificial intelligence (AI) has witnessed a
growing number of studies and increasing scholarly interest. However, the lack
of human-friendly and individual interpretations in explaining the outcomes of
machine learning algorithms has significantly hindered the acceptance of these
methods by clinicians in their research and clinical practice. To address this
issue, our study uses counterfactual explanations to explore the applicability
of "what if?" scenarios in medical research. Our aim is to expand our
understanding of magnetic resonance imaging (MRI) features used for diagnosing
pediatric posterior fossa brain tumors beyond existing boundaries. In our case
study, the proposed concept provides a novel way to examine alternative
decision-making scenarios that offer personalized and context-specific
insights, enabling the validation of predictions and clarification of
variations under diverse circumstances. Additionally, we explore the potential
use of counterfactuals for data augmentation and evaluate their feasibility as
an alternative approach in our medical research case. The results demonstrate
the promising potential of using counterfactual explanations to enhance
acceptance of AI-driven methods in clinical research.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØËß£ÈáãÊÄßÈ†òÂüü‰∏≠ÔºåÂ∑≤Á∂ìÁúãÂà∞Ë∂ä‰æÜË∂äÂ§öÁöÑÁ†îÁ©∂ÂíåÂ≠∏Ë°ìËààË∂£„ÄÇÁÑ∂ËÄåÔºåÂú®Ëß£ÈáãÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÁµêÊûúÊôÇÁº∫‰πè‰∫∫ÊÄßÂåñÂíåÂÄã‰∫∫ÂåñÁöÑË©ÆÈáãÔºåÈÄôÈ°ØËëóÈòªÁ§ô‰∫ÜËá®Â∫äÈÜ´ÁîüÂú®Á†îÁ©∂ÂíåËá®Â∫äÂØ¶Âãô‰∏≠Êé•ÂèóÈÄô‰∫õÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁöÑÁ†îÁ©∂‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÊé¢Ë®é„ÄåÂ¶ÇÊûúÔºü„ÄçÊÉÖÂ¢ÉÂú®ÈÜ´Â≠∏Á†îÁ©∂‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÊì¥Â±ïÊàëÂÄëÂ∞çÁî®ÊñºË®∫Êñ∑Â∞èÂÖíÂæåÈ°±Á™©ËÖ¶ËÖ´Áò§ÁöÑÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÁâπÂæµÁöÑÁêÜËß£ÔºåË∂ÖË∂äÁèæÊúâÁöÑÁïåÁ∑ö„ÄÇÂú®ÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊâÄÊèêÂá∫ÁöÑÊ¶ÇÂøµÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï‰æÜÊ™¢Ë¶ñÊõø‰ª£Ê±∫Á≠ñÊÉÖÂ¢ÉÔºåÊèê‰æõÂÄã‰∫∫ÂåñÂíåÁâπÂÆöÊñºÊÉÖÂ¢ÉÁöÑË¶ãËß£ÔºåÂæûËÄåËÉΩÂ§†È©óË≠âÈ†êÊ∏¨‰∏¶ÈáêÊ∏ÖÂú®‰∏çÂêåÊÉÖÊ≥Å‰∏ãÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂèç‰∫ãÂØ¶Áî®ÊñºË≥áÊñôÊì¥ÂÖÖÁöÑÊΩõÂú®Áî®ÈÄîÔºå‰∏¶Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫ÊàëÂÄëÈÜ´Â≠∏Á†îÁ©∂Ê°à‰æã‰∏≠Êõø‰ª£ÊñπÊ≥ïÁöÑÂèØË°åÊÄß„ÄÇÁµêÊûúË≠âÊòé‰∫Ü‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÂ¢ûÂº∑Ëá®Â∫äÁ†îÁ©∂‰∏≠ AI È©ÖÂãïÊñπÊ≥ïÁöÑÊé•ÂèóÂ∫¶ÁöÑÊΩõÂäõ„ÄÇ

##### **AI and Non AI Assessments for Dementia**
2307.01210v1 by Mahboobeh Parsapoor, Hamed Ghodrati, Vincenzo Dentamaro, Christopher R. Madan, Ioulietta Lazarou, Spiros Nikolopoulos, Ioannis Kompatsiaris

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

ÊëòË¶ÅÔºöÁõÆÂâç‰∫∫Â∑•Êô∫ËÉΩÈ†òÂüüÁöÑÈÄ≤Â±ïÂ∞éËá¥‰∫ÜÂêÑÁ®ÆÈ°ûÂûãÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈ©ÖÂãïÁöÑÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÁôºÂ±ïÔºåÂèØÁî®ÊñºË≠òÂà•ËôïÊñºÂ§±Êô∫ÁóáÊó©ÊúüÈöéÊÆµÁöÑÊÇ£ËÄÖ„ÄÇÂÆÉÂèØ‰ª•ÂæπÂ∫ïÊîπËÆäÂ§±Êô∫ÁóáË≠∑ÁêÜË®≠ÁΩÆ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÈÜ´ÁôÇÁïåË¶Å‰∫ÜËß£ÂêÑÁ®Æ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞Ôºå‰∏¶Ê†πÊìöÂÖ∂ÊúâÊïàÊÄß„ÄÅÊïàÁéá„ÄÅÂØ¶Áî®ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁ®ãÂ∫¶ÔºåËÄÉÊÖÆÈÅ∏ÊìáÂÆÉÂÄë‰æÜÊó©ÊúüË≠òÂà•Â§±Êô∫ÁóáÊÇ£ËÄÖ (PwD)„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰∫∫Â∑•Êô∫ËÉΩÈñãÁôº‰∫∫Âì°‰πüÊáâË©≤‰∫ÜËß£ÂêÑÁ®ÆÈùû‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞‰ª•ÂèäÊúÄËøëÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞„ÄÇÂõ†Ê≠§ÔºåÈÄôÁØáËá®Â∫äÈÜ´ÁîüÂíå‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´ÈÉΩÂèØ‰ª•Èñ±ËÆÄÁöÑË´ñÊñáÂ°´Ë£ú‰∫ÜÊñáÁçª‰∏≠ÈóúÊñºÂêëËá®Â∫äÈÜ´ÁîüËß£ÈáãÁèæÊúâÂ§±Êô∫ÁóáË≠òÂà•Ëß£Ê±∫ÊñπÊ°à‰ª•ÂèäÂêë‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´Ëß£ÈáãÊâÄÁî®ÊäÄË°ìÂíåÊúÄÂª£Ê≥õÁöÑÂ§±Êô∫ÁóáÊï∏ÊìöÈõÜÁöÑÁ©∫ÁôΩ„ÄÇÂÆÉÈÅµÂæ™Â∞ç‰∫∫Â∑•Êô∫ËÉΩÂíåÈùû‰∫∫Â∑•Êô∫ËÉΩÂ§±Êô∫ÁóáË©ï‰º∞Ë´ñÊñáÁöÑÂõûÈ°ßÔºåÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÂíåÈÜ´ÁôÇÁïåÊèê‰æõÊúâÈóúÂêÑÁ®ÆÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÂØ∂Ë≤¥‰ø°ÊÅØ„ÄÇË®éË´ñÂíåÁµêË´ñÈáçÈªû‰ªãÁ¥π‰∫ÜÊúÄÁ™ÅÂá∫ÁöÑÁ†îÁ©∂ÊñπÂêëÂíåÁèæÊúâËß£Ê±∫ÊñπÊ°àÁöÑÊàêÁÜüÂ∫¶„ÄÇ

##### **Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**
2306.07306v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Explainability poses a major challenge to artificial intelligence (AI)
techniques. Current studies on explainable AI (XAI) lack the efficiency of
extracting global knowledge about the learning task, thus suffer deficiencies
such as imprecise saliency, context-aware absence and vague meaning. In this
paper, we propose the class association embedding (CAE) approach to address
these issues. We employ an encoder-decoder architecture to embed sample
features and separate them into class-related and individual-related style
vectors simultaneously. Recombining the individual-style code of a given sample
with the class-style code of another leads to a synthetic sample with preserved
individual characters but changed class assignment, following a cyclic
adversarial learning strategy. Class association embedding distills the global
class-related features of all instances into a unified domain with well
separation between classes. The transition rules between different classes can
be then extracted and further employed to individual instances. We then propose
an active XAI framework which manipulates the class-style vector of a certain
sample along guided paths towards the counter-classes, resulting in a series of
counter-example synthetic samples with identical individual characters.
Comparing these counterfactual samples with the original ones provides a
global, intuitive illustration to the nature of the classification tasks. We
adopt the framework on medical image classification tasks, which show that more
precise saliency maps with powerful context-aware representation can be
achieved compared with existing methods. Moreover, the disease pathology can be
directly visualized via traversing the paths in the class-style space.

ÊëòË¶ÅÔºö<paragraph>ÂèØËß£ÈáãÊÄßÂ∞ç‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊßãÊàê‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÁï∂ÂâçÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂Áº∫‰πèÊèêÂèñÂ≠∏Áøí‰ªªÂãôÊï¥È´îÁü•Ë≠òÁöÑÊïàÁéáÔºåÂõ†Ê≠§Â≠òÂú®‰∏çÁ≤æÁ¢∫ÁöÑÈ°ØËëóÊÄß„ÄÅËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑÁº∫Â§±ÂíåÂê´Á≥äÊÑèÁæ©Á≠âÁº∫Èô∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫È°ûÂà•ÈóúËÅØÂµåÂÖ• (CAE) ÊñπÊ≥ï‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÊé°Áî®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂Êßã‰æÜÂµåÂÖ•Ê®£Êú¨ÁâπÂæµÔºå‰∏¶ÂêåÊôÇÂ∞áÂÆÉÂÄëÂàÜÁÇ∫È°ûÂà•Áõ∏ÈóúÂíåÂÄãÈ´îÁõ∏ÈóúÁöÑÊ®£ÂºèÂêëÈáè„ÄÇÂ∞áÁµ¶ÂÆöÊ®£Êú¨ÁöÑÂÄãÈ´îÊ®£Âºè‰ª£Á¢ºËàáÂè¶‰∏ÄÂÄãÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£Âºè‰ª£Á¢ºÈáçÊñ∞ÁµÑÂêàÔºåÊúÉÁî¢Áîü‰∏ÄÂÄãÂÖ∑Êúâ‰øùÁïôÂÄãÈ´îÁâπÂæµ‰ΩÜÊîπËÆäÈ°ûÂà•ÂàÜÈÖçÁöÑÂêàÊàêÊ®£Êú¨ÔºåÈÅµÂæ™Âæ™Áí∞Â∞çÊäóÂ≠∏ÁøíÁ≠ñÁï•„ÄÇÈ°ûÂà•ÈóúËÅØÂµåÂÖ•Â∞áÊâÄÊúâÂØ¶‰æãÁöÑÂÖ®Â±ÄÈ°ûÂà•Áõ∏ÈóúÁâπÂæµÊèêÁÖâÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÈ†òÂüü‰∏≠Ôºå‰∏¶Âú®È°ûÂà•‰πãÈñìÊúâËâØÂ•ΩÁöÑÂçÄÂàÜ„ÄÇÁÑ∂ÂæåÂèØ‰ª•ÊèêÂèñ‰∏çÂêåÈ°ûÂà•‰πãÈñìÁöÑËΩâÊèõË¶èÂâáÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊáâÁî®ÊñºÂÄãÂà•ÂØ¶‰æã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏ªÂãï XAI Ê°ÜÊû∂ÔºåÂÆÉÊ≤øËëóÂºïÂ∞éË∑ØÂæëÊìç‰ΩúÁâπÂÆöÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£ÂºèÂêëÈáèÔºåÊúùËëóÂèçÈ°ûÂà•ÁßªÂãïÔºåÂæûËÄåÁî¢Áîü‰∏ÄÁ≥ªÂàóÂÖ∑ÊúâÁõ∏ÂêåÂÄãÈ´îÁâπÂæµÁöÑÂèç‰æãÂêàÊàêÊ®£Êú¨„ÄÇÂ∞áÈÄô‰∫õÂèç‰∫ãÂØ¶Ê®£Êú¨ËàáÂéüÂßãÊ®£Êú¨ÈÄ≤Ë°åÊØîËºÉÔºåÂèØ‰ª•Â∞çÂàÜÈ°û‰ªªÂãôÁöÑÊÄßË≥™Êèê‰æõÂÖ®Â±Ä„ÄÅÁõ¥ËßÄÁöÑË™™Êòé„ÄÇÊàëÂÄëÊé°Áî®Ë©≤Ê°ÜÊû∂ÈÄ≤Ë°åÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÔºåÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÂèØ‰ª•Áç≤ÂæóÊõ¥Á≤æÁ¢∫ÁöÑÈ°ØËëóÊÄßÂúñÔºå‰∏¶ÂÖ∑ÊúâÂº∑Â§ßÁöÑËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÁñæÁóÖÁóÖÁêÜÂ≠∏ÂèØ‰ª•Áõ¥Êé•ÈÄöÈÅéÂú®È°ûÂà•Ê®£ÂºèÁ©∫Èñì‰∏≠ÈÅçÊ≠∑Ë∑ØÂæë‰æÜÈÄ≤Ë°åÂèØË¶ñÂåñ„ÄÇ</paragraph>

##### **HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**
2306.06029v1 by Rodrigo Agerri, I√±igo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova

Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.

ÊëòË¶ÅÔºöÊèê‰æõÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑ AI È†êÊ∏¨ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÂíåË§áÈõúÊÄßÁöÑ‰ªªÂãô„ÄÇË¶ÅÈ†ÜÂà©ÈÄ≤Ë°åÔºåÂÆÉÈúÄË¶ÅÂÖ∑ÂÇô‰∏ãÂàóÂõ†Á¥†ÔºöÈÅ∏ÊìáÈÅ©Áï∂ÁöÑË™™ÊòéÊôÆÈÅçÊÄß/ÁâπÊÆäÊÄßÂ±§Á¥öÔºõËÄÉÈáèË™™ÊòéÂèóÁõä‰∫∫Â∞çÊâÄËÄÉÊÖÆÁöÑ AI ‰ªªÂãôÁöÑÁÜüÊÇâÁ®ãÂ∫¶ÂÅáË®≠ÔºõÂèÉÁÖß‰øÉÊàêÊ±∫Á≠ñÁöÑÁâπÂÆöÂÖÉÁ¥†ÔºõÂà©Áî®ÂèØËÉΩ‰∏çÂ±¨ÊñºÈ†êÊ∏¨Á®ãÂ∫èÁöÑ‰∏ÄÈÉ®ÂàÜÁöÑÈ°çÂ§ñÁü•Ë≠òÔºà‰æãÂ¶ÇÂ∞àÂÆ∂Ë≠âÊìöÔºâÔºõ‰∏¶Êèê‰æõÊîØÊåÅÂê¶ÂÆöÂÅáË®≠ÁöÑË≠âÊìö„ÄÇÊúÄÂæåÔºåÁ≥ªÁµ±ÈúÄË¶Å‰ª•Ê∏ÖÊô∞ÂèØËß£Èáã‰∏îÂèØËÉΩ‰ª§‰∫∫‰ø°ÊúçÁöÑÊñπÂºèÂà∂ÂÆöË™™Êòé„ÄÇÂü∫ÊñºÈÄô‰∫õËÄÉÈáèÔºåANTIDOTE ‰øÉÊàê‰∫ÜÂèØËß£Èáã AI ÁöÑÊï¥ÂêàÈ°òÊôØÔºåÂÖ∂‰∏≠Ê∑±Â∫¶Â≠∏ÁøíÁ®ãÂ∫èÁöÑ‰ΩéÈöéÁâπÂæµËàá‰∫∫È°ûË´ñË≠âËÉΩÂäõÁöÑÈ´òÈöéÊû∂ÊßãÁõ∏ÁµêÂêà„ÄÇANTIDOTE Â∞áÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíËàáË´ñË≠âÁöÑË∑®È†òÂüüËÉΩÂäõÔºå‰æÜÊîØÊåÅÂèØËß£Èáã AI Êõ¥Âª£Ê≥õ‰∏îÂâµÊñ∞ÁöÑËßÄÈªûÔºåÂÖ∂‰∏≠Â∞çËá®Â∫äÊ°à‰æãÂØ©Ë≠∞ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÈúÄÊ±ÇËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩúÁÇ∫Ë©≤Â∞àÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÊàêÊûúÔºåÊàëÂÄëÁôºÂ∏É‰∫Ü Antidote CasiMedicos Ë≥áÊñôÈõÜÔºå‰ª•Âà©Êñº‰∏ÄËà¨ÂèØËß£Èáã AI ÁöÑÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÈÜ´ÁôÇÈ†òÂüüÁöÑË´ñË≠â„ÄÇ

##### **XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**
2306.04791v1 by Eli Laird, Ayesh Madushanka, Elfi Kraka, Corey Clark

Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºåÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÈÄ≤Â±ïËøÖÈÄüÔºåÂú®Ëó•Áâ©ÁôºÁèæ„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåÊé®Ëñ¶Á≥ªÁµ±ÊñπÈù¢ÈÉΩÊúâË®±Â§öÊñ∞ÁôºÂ±ï„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈÄ≤Â±ïÂæàÈáçË¶ÅÔºå‰ΩÜË®±Â§öÁ∂≤Ë∑ØÈÉΩÊòØ„ÄåÈªëÁõíÂ≠ê„ÄçÔºåÂ∞çÊñºÁ∂≤Ë∑ØÂà∞Â∫ïÂú®Â≠∏Áøí„Äå‰ªÄÈ∫º„Äç‰∫ÜËß£ÁîöÂ∞ë„ÄÇË®±Â§öÈ´òÈ¢®Èö™ÊáâÁî®Ôºå‰æãÂ¶ÇËó•Áâ©ÁôºÁèæÔºåÈúÄË¶ÅÊ®°ÂûãÊèê‰æõ‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑËß£ÈáãÔºå‰ª•‰æø‰ΩøÁî®ËÄÖÂèØ‰ª•Ëæ®Ë≠òÈåØË™§‰∏¶ÁôºÁèæÊñ∞Áü•Ë≠ò„ÄÇÂõ†Ê≠§ÔºåÂèØËß£Èáã AI ÊºîÁÆóÊ≥ïÁöÑÈñãÁôºÂ∞çÊñºÊàëÂÄëÁç≤Âèñ AI ÁöÑÂ•ΩËôïËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ eXplainable Insight (XInsight) ÁöÑ GNN ÂèØËß£ÈáãÊÄßÊºîÁÆóÊ≥ïÔºåÂÆÉ‰ΩøÁî® GFlowNets Áî¢ÁîüÊ®°ÂûãËß£ÈáãÂàÜ‰Ωà„ÄÇÁî±Êñº GFlowNets ÊúÉÁî¢ÁîüÊ©üÁéáËàáÁçéÂãµÊàêÊ≠£ÊØîÁöÑÁâ©‰ª∂ÔºåÂõ†Ê≠§ËàáÂÖàÂâçÂÉÖÂ≠∏ÁøíÊúÄÂ§ßÁçéÂãµÁØÑ‰æãÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåXInsight ÂèØ‰ª•Áî¢ÁîüÂ§öÊ®£ÂåñÁöÑËß£ÈáãÈõÜÂêà„ÄÇÊàëÂÄëÈÄèÈÅéÁÇ∫Âú®ÂÖ©ÂÄãÂúñÂΩ¢ÂàÜÈ°û‰ªªÂãô‰∏≠Ë®ìÁ∑¥ÁöÑ GNN Áî¢ÁîüËß£Èáã‰æÜÂ±ïÁ§∫ XInsightÔºö‰ΩøÁî® MUTAG Ë≥áÊñôÈõÜÂ∞çËá¥Á™ÅËÆäÂåñÂêàÁâ©ÈÄ≤Ë°åÂàÜÈ°ûÔºå‰∏¶‰ΩøÁî®ÊàëÂÄëÂ∑≤ÈñãÊîæÂéüÂßãÁ¢ºÁöÑÂêàÊàêË≥áÊñôÈõÜÂ∞çÈùûÁí∞ÁãÄÂúñÂΩ¢ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî® QSAR Âª∫Ê®°ÂàÜÊûêÁî¢ÁîüÁöÑÂåñÂêàÁâ©‰æÜÂ±ïÁ§∫ XInsight Ëß£ÈáãÁöÑÊïàÁî®ÔºåÊàëÂÄëÁôºÁèæ XInsight ÊúÉÁî¢ÁîüÊåâË¶™ËÑÇÊÄßÔºàÂ∑≤Áü•ÁöÑËá¥Á™ÅËÆäÁõ∏ÈóúÊÄßÔºâÂàÜÁæ§ÁöÑÂåñÂêàÁâ©„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ XInsight ÊúÉÁî¢Áîü‰∏ÄÂÄãËß£ÈáãÂàÜ‰ΩàÔºåÊè≠Á§∫Ê®°ÂûãÊâÄÂ±ïÁ§∫ÁöÑÂ∫ïÂ±§Èóú‰øÇ„ÄÇÂÆÉÂÄë‰πüÂº∑Ë™øÁî¢ÁîüÂ§öÊ®£ÂåñËß£ÈáãÈõÜÂêàÁöÑÈáçË¶ÅÊÄßÔºåÂõ†ÁÇ∫ÂÆÉ‰ΩøÊàëÂÄëËÉΩÂ§†ÁôºÁèæÊ®°Âûã‰∏≠ÁöÑÈö±ËóèÈóú‰øÇÔºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÂàÜÊûêÊèê‰æõÊúâÂÉπÂÄºÁöÑÊåáÂ∞é„ÄÇ</paragraph>

##### **Explainable AI using expressive Boolean formulas**
2306.03976v1 by Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Grant Salton, Zhihuai Zhu, Elton Yechao Zhu, Serdar Kadƒ±oƒülu, Sima E. Borujeni, Helmut G. Katzgraber

We propose and implement an interpretable machine learning classification
model for Explainable AI (XAI) based on expressive Boolean formulas. Potential
applications include credit scoring and diagnosis of medical conditions. The
Boolean formula defines a rule with tunable complexity (or interpretability),
according to which input data are classified. Such a formula can include any
operator that can be applied to one or more Boolean variables, thus providing
higher expressivity compared to more rigid rule-based and tree-based
approaches. The classifier is trained using native local optimization
techniques, efficiently searching the space of feasible formulas. Shallow rules
can be determined by fast Integer Linear Programming (ILP) or Quadratic
Unconstrained Binary Optimization (QUBO) solvers, potentially powered by
special purpose hardware or quantum devices. We combine the expressivity and
efficiency of the native local optimizer with the fast operation of these
devices by executing non-local moves that optimize over subtrees of the full
Boolean formula. We provide extensive numerical benchmarking results featuring
several baselines on well-known public datasets. Based on the results, we find
that the native local rule classifier is generally competitive with the other
classifiers. The addition of non-local moves achieves similar results with
fewer iterations, and therefore using specialized or quantum hardware could
lead to a speedup by fast proposal of non-local moves.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏¶ÂØ¶‰Ωú‰∏ÄÂÄãÂèØËß£ÈáãÊ©üÂô®Â≠∏ÁøíÂàÜÈ°ûÊ®°ÂûãÔºåÁî®ÊñºÂü∫ÊñºË°®ÈÅîÂºèÂ∏ÉÊûóÂÖ¨ÂºèÁöÑÂèØËß£Èáã AI (XAI)„ÄÇÊΩõÂú®ÊáâÁî®ÂåÖÊã¨‰ø°Áî®Ë©ïÂàÜÂíåÈÜ´ÁôÇÁãÄÊ≥ÅË®∫Êñ∑„ÄÇÂ∏ÉÊûóÂÖ¨ÂºèÂÆöÁæ©‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÂèØË™øÊï¥Ë§áÈõúÊÄßÔºàÊàñÂèØËß£ÈáãÊÄßÔºâÁöÑË¶èÂâáÔºåÊ†πÊìöË©≤Ë¶èÂâáÂ∞çËº∏ÂÖ•Êï∏ÊìöÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÊ®£ÁöÑÂÖ¨ÂºèÂèØ‰ª•ÂåÖÂê´‰ªª‰ΩïÂèØÊáâÁî®Êñº‰∏ÄÂÄãÊàñÂ§öÂÄãÂ∏ÉÊûóËÆäÊï∏ÁöÑÈÅãÁÆóÂ≠êÔºåÂæûËÄåËàáÊõ¥Âö¥Ê†ºÁöÑÂü∫ÊñºË¶èÂâáÂíåÂü∫ÊñºÊ®πÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõÊõ¥È´òÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÂàÜÈ°ûÂô®‰ΩøÁî®ÂéüÁîüÂ±ÄÈÉ®ÊúÄ‰Ω≥ÂåñÊäÄË°ìÈÄ≤Ë°åË®ìÁ∑¥ÔºåÊúâÊïàÂú∞ÊêúÁ¥¢ÂèØË°åÂÖ¨ÂºèÁöÑÁ©∫Èñì„ÄÇÊ∑∫Â±§Ë¶èÂâáÂèØ‰ª•Áî®Âø´ÈÄüÁöÑÊï¥Êï∏Á∑öÊÄßË¶èÂäÉ (ILP) Êàñ‰∫åÊ¨°ÁÑ°Á¥ÑÊùü‰∫åÂÖÉÊúÄ‰Ω≥Âåñ (QUBO) Ê±ÇËß£Âô®‰æÜÁ¢∫ÂÆöÔºåÈÄô‰∫õÊ±ÇËß£Âô®ÂèØËÉΩÁî±ÁâπÊÆäÁî®ÈÄîÁöÑÁ°¨È´îÊàñÈáèÂ≠êË£ùÁΩÆÊèê‰æõÊîØÊè¥„ÄÇÊàëÂÄëÂ∞áÂéüÁîüÂ±ÄÈÉ®ÊúÄ‰Ω≥ÂåñÂô®ÁöÑË°®ÈÅîËÉΩÂäõÂíåÊïàÁéáËàáÈÄô‰∫õË£ùÁΩÆÁöÑÂø´ÈÄüÈÅãÁÆóÁõ∏ÁµêÂêàÔºåÈÄèÈÅéÂü∑Ë°åÈùûÂ±ÄÈÉ®ÁßªÂãï‰æÜÊúÄ‰Ω≥ÂåñÂÆåÊï¥Â∏ÉÊûóÂÖ¨ÂºèÁöÑÂ≠êÊ®π„ÄÇÊàëÂÄëÊèê‰æõÂª£Ê≥õÁöÑÊï∏ÂÄºÂü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûúÔºåÂÖ∂‰∏≠ÂåÖÂê´Âú®ÁúæÊâÄÂë®Áü•ÁöÑÂÖ¨ÂÖ±Ë≥áÊñôÈõÜ‰∏ä‰ΩøÁî®Â§öÂÄãÂü∫Á∑ö„ÄÇÊ†πÊìöÁµêÊûúÔºåÊàëÂÄëÁôºÁèæÂéüÁîüÂ±ÄÈÉ®Ë¶èÂâáÂàÜÈ°ûÂô®ÈÄöÂ∏∏ËàáÂÖ∂‰ªñÂàÜÈ°ûÂô®ÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇÂä†ÂÖ•ÈùûÂ±ÄÈÉ®ÁßªÂãï‰ª•ËºÉÂ∞ëÁöÑÂèçË¶ÜÈÅãÁÆóÊ¨°Êï∏ÈÅîÊàêÈ°û‰ººÁöÑÁµêÊûúÔºåÂõ†Ê≠§‰ΩøÁî®Â∞àÁî®ÊàñÈáèÂ≠êÁ°¨È´îÂèØËÉΩÊúÉÈÄèÈÅéÂø´ÈÄüÊèêÂá∫ÈùûÂ±ÄÈÉ®ÁßªÂãï‰æÜÂä†ÈÄü„ÄÇ

##### **Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**
2306.03902v1 by Yeldar Toleubay, Don Joven Agravante, Daiki Kimura, Baihan Lin, Djallel Bouneffouf, Michiaki Tatsubori

In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.

ÊëòË¶ÅÔºöÁÇ∫‰∫ÜËß£Ê±∫ÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÁöÑÂÖ®ÁêÉÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÈÇèËºØÁ•ûÁ∂ìÁ∂≤Ë∑Ø (LNN) ÁöÑÁ•ûÁ∂ìÁ¨¶Ëôü AI ÊñπÊ≥ï‰æÜË®∫Êñ∑ÂøÉÁêÜÁñæÁóÖ„ÄÇÁî±ÊñºÁº∫‰πèÊúâÊïàÁöÑÂøÉÁêÜÁñæÁóÖÊ≤ªÁôÇÊ∂µËìãÁØÑÂúçÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÁ®Æ AI Ëß£Ê±∫ÊñπÊ°à‰æÜÂçîÂä©Ê≤ªÁôÇÂ∏´ÈÄ≤Ë°åË®∫Êñ∑„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÔºåÊ≤ªÁôÇÂ∏´ÂèØËÉΩÁÑ°Ê≥ï‰ø°‰ªªÂÆÉÂÄë„ÄÇLNN ÊòØ‰∏ÄÁ®ÆÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÔºåÂÆÉÁµêÂêà‰∫ÜÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂ≠∏ÁøíËÉΩÂäõÂíåÂü∫ÊñºÁ∂ìÂÖ∏ÈÇèËºØÁöÑ AI ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±‰ΩøÁî®‰æÜËá™Ëá®Â∫äË®™Ë´áÁöÑËº∏ÂÖ•Ë¨ÇË©û‰æÜËº∏Âá∫ÂøÉÁêÜÁñæÁóÖÈ°ûÂà•Ôºå‰∏¶‰ΩøÁî®‰∏çÂêåÁöÑË¨ÇË©ûÂâ™ÊûùÊäÄË°ì‰æÜÂØ¶ÁèæÂèØÊì¥ÂÖÖÊÄßÂíåÊõ¥È´òÁöÑÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãË¶ãËß£ÊèêÂèñÊñπÊ≥ï‰æÜÂçîÂä©Ê≤ªÁôÇÂ∏´ÈÄ≤Ë°åË®∫Êñ∑„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±Ëß£Ê±∫‰∫ÜÁï∂ÂâçÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÂïèÈ°åÔºå‰∏¶ÁÇ∫ÂøÉÁêÜÁñæÁóÖË®∫Êñ∑Êèê‰æõ‰∫ÜÊõ¥ÂÄºÂæó‰ø°Ë≥¥ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**
2306.01668v1 by Sujith K Mandala

As machine learning models become increasingly prevalent in medical
diagnostics, the need for interpretability and transparency becomes paramount.
The XAI Renaissance signifies a significant shift in the field, aiming to
redefine the interpretability of medical diagnostic models. This paper explores
the innovative approaches and methodologies within the realm of Explainable AI
(XAI) that are revolutionizing the interpretability of medical diagnostic
models. By shedding light on the underlying decision-making process, XAI
techniques empower healthcare professionals to understand, trust, and
effectively utilize these models for accurate and reliable medical diagnoses.
This review highlights the key advancements in XAI for medical diagnostics and
their potential to transform the healthcare landscape, ultimately improving
patient outcomes and fostering trust in AI-driven diagnostic systems.

ÊëòË¶ÅÔºöÈö®ËëóÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÈÜ´ÁôÇË®∫Êñ∑‰∏≠Ë∂ä‰æÜË∂äÊôÆÈÅçÔºåÂèØËß£ÈáãÊÄßÂíåÈÄèÊòéÂ∫¶ÁöÑÈúÄÊ±ÇËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇXAI Âæ©ËààÊ®ôË™åËëóË©≤È†òÂüüÁöÑÈáçÂ§ßËΩâËÆäÔºåÊó®Âú®ÈáçÊñ∞ÂÆöÁæ©ÈÜ´ÁôÇË®∫Êñ∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂèØËß£Èáã AI (XAI) È†òÂüüÂÖßÁöÑÂâµÊñ∞ÊñπÊ≥ïÂíåÊñπÊ≥ïË´ñÔºåÈÄô‰∫õÊñπÊ≥ïÂíåÊñπÊ≥ïË´ñÊ≠£Âú®Èù©Êñ∞ÈÜ´ÁôÇË®∫Êñ∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÈÄöÈÅéÈó°ÊòéÂü∫Á§éÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãÔºåXAI ÊäÄË°ì‰ΩøÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°ËÉΩÂ§†ÁêÜËß£„ÄÅ‰ø°‰ªª‰∏¶ÊúâÊïàÂú∞Âà©Áî®ÈÄô‰∫õÊ®°ÂûãÈÄ≤Ë°åÊ∫ñÁ¢∫‰∏îÂèØÈù†ÁöÑÈÜ´ÁôÇË®∫Êñ∑„ÄÇÊú¨Á∂úËø∞ÈáçÈªû‰ªãÁ¥π‰∫Ü XAI Âú®ÈÜ´ÁôÇË®∫Êñ∑ÊñπÈù¢ÁöÑÈóúÈçµÈÄ≤Â±ïÂèäÂÖ∂ËΩâËÆäÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊΩõÂäõÔºåÊúÄÁµÇÊîπÂñÑÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊïàÊûú‰∏¶ÂüπÈ§äÂ∞ç AI È©ÖÂãïÁöÑË®∫Êñ∑Á≥ªÁµ±ÁöÑ‰ø°‰ªª„ÄÇ

##### **A Novel real-time arrhythmia detection model using YOLOv8**
2305.16727v3 by Guang Jun Nicholas Ang, Aritejh Kr Goil, Henryk Chan, Jieyi Jeric Lew, Xin Chun Lee, Raihan Bin Ahmad Mustaffa, Timotius Jason, Ze Ting Woon, Bingquan Shen

In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.

ÊëòË¶ÅÔºö<paragraph>Âú®‰ª•È´òÂ∫¶ÈÄ£Êé•ÊÄßÂíåÊµÅÂãïÊÄßÁÇ∫ÁâπÂæµÁöÑÁí∞Â¢É‰∏≠ÔºåÂä†‰∏äÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÊøÄÂ¢ûÔºåÈÄöÈÅéÈÅ†Á®ãÁõ£ÊéßÂøÉË°ÄÁÆ°ÂÅ•Â∫∑‰æÜÂâäÊ∏õÈÜ´ÁôÇ‰øùÂÅ•ÊîØÂá∫ÁöÑÂøÖË¶ÅÊÄßËÆäÂæóÊõ¥Âä†ÊòéÈ°Ø„ÄÇÊ∫ñÁ¢∫Ê™¢Ê∏¨ÂíåÂàÜÈ°ûÂøÉÂæã‰∏çÊï¥Â∞çÊñºË®∫Êñ∑ÊÇ£ÊúâÂøÉËáü‰∏çË¶èÂâáÁöÑ‰∫∫Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂú®ÂÆ∂‰∏≠‰ΩøÁî®ÂøÉÈõªÂúñ (ECG) Ê∏¨ÈáèÈÄ≤Ë°åÂØ¶ÊôÇÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÁöÑÂèØË°åÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÊáâÁî®ÔºåÂà©Áî®Â∞ñÁ´ØÁöÑ You-Only-Look-Once (YOLO)v8 ÊºîÁÆóÊ≥ïÂ∞çÂñÆÂ∞éËÅØ ECG Ë®äËôüÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊêçÂ§±‰øÆÊîπ YOLOv8 Ê®°ÂûãÔºå‰∏¶ÈáùÂ∞ç MIT-BIH ÂøÉÂæã‰∏çÊï¥Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂæÆË™øÔºåÂæûËÄåÂØ¶Áèæ‰∫ÜÂØ¶ÊôÇÁöÑÊåÅÁ∫åÁõ£Êéß„ÄÇÁç≤ÂæóÁöÑÁµêÊûúË≠âÂØ¶‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË©≤Ê®°ÂûãÂú® NVIDIA Tesla V100 ‰∏äÈÅîÂà∞‰∫Ü 99.5% ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶Âíå 0.992 mAP@50Ôºå‰ª•Âèä 0.002 ÁßíÁöÑÂø´ÈÄüÊ™¢Ê∏¨ÊôÇÈñì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë™™Êòé‰∫ÜÂØ¶ÊôÇÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÁöÑÊΩõÂäõÔºå‰ΩøÁî®Êà∂ËÉΩÂ§†Âú®ÂÆ∂‰∏≠ËàíÈÅ©Âú∞Ë¶ñË¶∫ÂåñËß£ËÆÄÊ®°ÂûãËº∏Âá∫„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂ÁÇ∫Êì¥Â±ïÂà∞ÂØ¶ÊôÇÂèØËß£Èáã AI (XAI) Ê®°ÂûãÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåË©≤Ê®°ÂûãËÉΩÂ§†ÈÉ®ÁΩ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÂæûËÄåÈ°ØËëóÊé®ÈÄ≤ÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°àÁöÑÈ†òÂüü„ÄÇ</paragraph>

##### **Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**
2305.14389v2 by Jai Vardhan, Taraka Satya Krishna Teja Malisetti

Breast cancer (BC) remains a significant health threat, with no long-term
cure currently available. Early detection is crucial, yet mammography
interpretation is hindered by high false positives and negatives. With BC
incidence projected to surpass lung cancer, improving early detection methods
is vital. Thermography, using high-resolution infrared cameras, offers promise,
especially when combined with artificial intelligence (AI). This work presents
an attention-based convolutional neural network for segmentation, providing
increased speed and precision in BC detection and classification. The system
enhances images and performs cancer segmentation with explainable AI. We
propose a transformer-attention-based convolutional architecture (UNet) for
fault identification and employ Gradient-weighted Class Activation Mapping
(Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with
IRT images. The superiority of our proposed framework is confirmed when
compared with existing deep learning frameworks.

ÊëòË¶ÅÔºö‰π≥ÁôåÔºàBCÔºâ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÂÅ•Â∫∑Â®ÅËÑÖÔºåÁõÆÂâçÂ∞öÁÑ°Èï∑ÊúüÊ≤ªÁôíÁöÑÊñπÊ≥ï„ÄÇÊó©ÊúüÁôºÁèæËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜ‰π≥ÊàøÊîùÂΩ±ÁöÑÂà§ËÆÄÂçªÂèóÂà∞È´òÂÅáÈôΩÊÄßÂíåÂÅáÈô∞ÊÄßÁöÑÈòªÁ§ô„ÄÇÁî±Êñº‰π≥ÁôåÁöÑÁôºÁîüÁéáÈ†êË®àÂ∞áË∂ÖÈÅéËÇ∫ÁôåÔºåÂõ†Ê≠§ÊîπÂñÑÊó©ÊúüÊ™¢Ê∏¨ÊñπÊ≥ïËá≥ÈóúÈáçË¶Å„ÄÇÁÜ±ÂÉèÊîùÂΩ±‰ΩøÁî®È´òËß£ÊûêÂ∫¶Á¥ÖÂ§ñÁ∑öÁõ∏Ê©üÔºåÁâπÂà•ÊòØÂú®Ëàá‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁµêÂêà‰ΩøÁî®ÊôÇÔºåÊèê‰æõ‰∫ÜÂ∏åÊúõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÁî®ÊñºÂàÜÂâ≤ÔºåÂú®‰π≥ÁôåÊ™¢Ê∏¨ÂíåÂàÜÈ°û‰∏≠Êèê‰æõ‰∫ÜÊõ¥È´òÁöÑÈÄüÂ∫¶ÂíåÁ≤æÂ∫¶„ÄÇË©≤Á≥ªÁµ±Â¢ûÂº∑ÂΩ±ÂÉè‰∏¶Âü∑Ë°åÂèØËß£ÈáãÁöÑ AI ÁôåÁóáÂàÜÂâ≤„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºTransformerÊ≥®ÊÑèÂäõÁöÑÂç∑Á©çÊû∂ÊßãÔºàUNetÔºâÁî®ÊñºÊïÖÈöúË≠òÂà•Ôºå‰∏¶‰ΩøÁî®Ê¢ØÂ∫¶Âä†Ê¨äÈ°ûÊøÄÊ¥ªÊò†Â∞ÑÔºàGrad-CAMÔºâ‰æÜÂàÜÊûê UNet Êû∂Êßã‰∏≠ÂÅèË¶ãÂíåÂº±ÈªûÁöÑÂçÄÂüüÔºå‰ΩøÁî® IRT ÂΩ±ÂÉè„ÄÇËàáÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂Áõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁöÑÂÑ™Ë∂äÊÄßÂæóÂà∞Ë≠âÂØ¶„ÄÇ

##### **What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**
2305.13127v2 by Junwei Kuang, Jiaheng Xie, Zhijun Yan

Depression is the most prevalent and serious mental illness, which induces
grave financial and societal ramifications. Depression detection is key for
early intervention to mitigate those consequences. Such a high-stake decision
inherently necessitates interpretability. Although a few depression detection
studies attempt to explain the decision based on the importance score or
attention weights, these explanations misalign with the clinical depression
diagnosis criterion that is based on depressive symptoms. To fill this gap, we
follow the computational design science paradigm to develop a novel Multi-Scale
Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and
interprets depressive symptoms as well as how long they last. Extensive
empirical analyses using a large-scale dataset show that MSTPNet outperforms
state-of-the-art depression detection methods with an F1-score of 0.851. This
result also reveals new symptoms that are unnoted in the survey approach, such
as sharing admiration for a different life. We further conduct a user study to
demonstrate its superiority over the benchmarks in interpretability. This study
contributes to IS literature with a novel interpretable deep learning model for
depression detection in social media. In practice, our proposed method can be
implemented in social media platforms to provide personalized online resources
for detected depressed patients.

ÊëòË¶ÅÔºöÊÜÇÈ¨±ÁóáÊòØÊúÄÊôÆÈÅç‰∏îÂö¥ÈáçÁöÑÁ≤æÁ•ûÁñæÁóÖÔºåÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑË≤°ÂãôÂíåÁ§æÊúÉÂæåÊûú„ÄÇÊÜÇÈ¨±ÁóáÁöÑÂÅµÊ∏¨Â∞çÊñºÊó©Êúü‰ªãÂÖ•‰ª•Ê∏õËºïÈÄô‰∫õÂæåÊûúËá≥ÈóúÈáçË¶Å„ÄÇÂ¶ÇÊ≠§ÈáçÂ§ßÁöÑÊ±∫ÂÆöÊú¨Ë≥™‰∏äÈúÄË¶ÅÂèØËß£ÈáãÊÄß„ÄÇÂÑòÁÆ°‰∏Ä‰∫õÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Á†îÁ©∂ÂòóË©¶Ê†πÊìöÈáçË¶ÅÊÄßÂàÜÊï∏ÊàñÊ≥®ÊÑèÂäõÊ¨äÈáç‰æÜËß£ÈáãÈÄôÂÄãÊ±∫ÂÆöÔºå‰ΩÜÈÄô‰∫õËß£ÈáãËàáÂü∫ÊñºÊÜÇÈ¨±ÁóáÁãÄÁöÑËá®Â∫äÊÜÇÈ¨±ÁóáË®∫Êñ∑Ê®ôÊ∫ñ‰∏ç‰∏ÄËá¥„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁº∫Âè£ÔºåÊàëÂÄëÈÅµÂæ™Ë®àÁÆóË®≠Ë®àÁßëÂ≠∏ÁØÑ‰æã‰æÜÈñãÁôº‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÂ∞∫Â∫¶ÊôÇÈñìÂéüÂûãÁ∂≤Ë∑Ø (MSTPNet)„ÄÇMSTPNet ÂâµÊñ∞Âú∞ÂÅµÊ∏¨‰∏¶Ëß£ÈáãÊÜÇÈ¨±ÁóáÁãÄ‰ª•ÂèäÂÆÉÂÄëÊåÅÁ∫åÂ§ö‰πÖ„ÄÇ‰ΩøÁî®Â§ßË¶èÊ®°Ë≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶Ë≠âÂàÜÊûêÈ°ØÁ§∫ÔºåMSTPNet ‰ª• 0.851 ÁöÑ F1 ÂàÜÊï∏ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÊñπÊ≥ï„ÄÇÊ≠§ÁµêÊûúÈÇÑÊè≠Á§∫‰∫ÜË™øÊü•ÊñπÊ≥ï‰∏≠Êú™Ê≥®ÊÑèÂà∞ÁöÑÊñ∞ÁóáÁãÄÔºå‰æãÂ¶ÇÂàÜ‰∫´Â∞ç‰∏çÂêåÁîüÊ¥ªÁöÑÊ¨Ω‰Ω©„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°å‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•Ë≠âÊòéÂÖ∂Âú®ÂèØËß£ÈáãÊÄßÊñπÈù¢ÂÑ™ÊñºÂü∫Ê∫ñ„ÄÇÊú¨Á†îÁ©∂‰ª•‰∏ÄÂÄãÊñ∞Á©éÁöÑÂèØËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁÇ∫ÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Âú®Á§æÁæ§Â™íÈ´î‰∏≠ÁöÑ IS ÊñáÁçªÂÅöÂá∫Ë≤¢Áçª„ÄÇÂú®ÂØ¶Âãô‰∏äÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•ÂØ¶‰ΩúÂú®Á§æÁæ§Â™íÈ´îÂπ≥Âè∞‰∏≠Ôºå‰ª•Êèê‰æõÂÄã‰∫∫ÂåñÁöÑÁ∑ö‰∏äË≥áÊ∫êÁµ¶Ë¢´ÂÅµÊ∏¨Âá∫ÊÜÇÈ¨±ÁóáÁöÑÊÇ£ËÄÖ„ÄÇ

##### **Echoes of Biases: How Stigmatizing Language Affects AI Performance**
2305.10201v4 by Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal

Electronic health records (EHRs) serve as an essential data source for the
envisioned artificial intelligence (AI)-driven transformation in healthcare.
However, clinician biases reflected in EHR notes can lead to AI models
inheriting and amplifying these biases, perpetuating health disparities. This
study investigates the impact of stigmatizing language (SL) in EHR notes on
mortality prediction using a Transformer-based deep learning model and
explainable AI (XAI) techniques. Our findings demonstrate that SL written by
clinicians adversely affects AI performance, particularly so for black
patients, highlighting SL as a source of racial disparity in AI model
development. To explore an operationally efficient way to mitigate SL's impact,
we investigate patterns in the generation of SL through a clinicians'
collaborative network, identifying central clinicians as having a stronger
impact on racial disparity in the AI model. We find that removing SL written by
central clinicians is a more efficient bias reduction strategy than eliminating
all SL in the entire corpus of data. This study provides actionable insights
for responsible AI development and contributes to understanding clinician
behavior and EHR note writing in healthcare.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) ‰ΩúÁÇ∫È†êÊÉ≥‰∏≠Áî±‰∫∫Â∑•Êô∫ÊÖß (AI) Êé®ÂãïÁöÑÈÜ´ÁôÇ‰øùÂÅ•ËΩâÂûãÁöÑÈáçË¶ÅË≥áÊñô‰æÜÊ∫ê„ÄÇÁÑ∂ËÄåÔºåÂèçÊò†Âú® EHR ÂÇôË®ª‰∏≠ÁöÑËá®Â∫äÂÅèË¶ãÂèØËÉΩÂ∞éËá¥ AI Ê®°ÂûãÁπºÊâø‰∏¶Êì¥Â§ßÈÄô‰∫õÂÅèË¶ãÔºåÈÄ≤ËÄåÈÄ†ÊàêÂÅ•Â∫∑Â∑ÆÁï∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é EHR ÂÇôË®ª‰∏≠Ê±ôÂêçÂåñË™ûË®Ä (SL) Â∞ç‰ΩøÁî®Âü∫Êñº Transformer ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂíåÂèØËß£Èáã AI (XAI) ÊäÄË°ìÈ†êÊ∏¨Ê≠ª‰∫°ÁéáÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁî±Ëá®Â∫äÈÜ´ÁîüÊí∞ÂØ´ÁöÑ SL ÊúÉÂ∞ç AI ÊïàËÉΩÁî¢Áîü‰∏çÂà©ÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂ∞çÈªë‰∫∫ÊÇ£ËÄÖËÄåË®ÄÔºåÁ™ÅÈ°Ø SL ÊòØ AI Ê®°ÂûãÈñãÁôº‰∏≠Á®ÆÊóèÂ∑ÆÁï∞ÁöÑ‰æÜÊ∫ê„ÄÇÁÇ∫‰∫ÜÊé¢Á¥¢‰∏ÄÁ®ÆÈÅã‰Ωú‰∏äÊúâÊïàÁéáÁöÑÊñπÊ≥ï‰æÜÊ∏õËºï SL ÁöÑÂΩ±ÈüøÔºåÊàëÂÄëÈÄèÈÅéËá®Â∫äÈÜ´ÁîüÁöÑÂçî‰ΩúÁ∂≤Ë∑ØÊé¢Ë®é SL Áî¢ÁîüÁöÑÊ®°ÂºèÔºå‰∏¶ÊâæÂá∫Ê†∏ÂøÉËá®Â∫äÈÜ´ÁîüÂ∞ç AI Ê®°Âûã‰∏≠ÁöÑÁ®ÆÊóèÂ∑ÆÁï∞ÊúâËºÉÂ§ßÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁôºÁèæÔºåÁßªÈô§Áî±Ê†∏ÂøÉËá®Â∫äÈÜ´ÁîüÊí∞ÂØ´ÁöÑ SL ÊòØÊØîÊ∂àÈô§Ë≥áÊñôÈõÜ‰∏≠ÊâÄÊúâ SL Êõ¥ÊúâÊïàÁéáÁöÑÂÅèË¶ãÊ∏õÂ∞ëÁ≠ñÁï•„ÄÇÊú¨Á†îÁ©∂Êèê‰æõÂèØË°åÁöÑË¶ãËß£ÔºåÁî®ÊñºË≤†Ë≤¨‰ªªÁöÑ AI ÈñãÁôºÔºå‰∏¶ÊúâÂä©Êñº‰∫ÜËß£Ëá®Â∫äÈÜ´ÁîüË°åÁÇ∫ÂíåÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ EHR ÂÇôË®ªÊí∞ÂØ´„ÄÇ

##### **Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**
2305.03376v1 by Goda Klumbyte, Hannah Piehl, Claude Draude

Contemporary automation through AI entails a substantial amount of
behind-the-scenes human labour, which is often both invisibilised and
underpaid. Since invisible labour, including labelling and maintenance work, is
an integral part of contemporary AI systems, it remains important to sensitise
users to its role. We suggest that this could be done through explainable AI
(XAI) design, particularly feminist intersectional XAI. We propose the method
of cartography, which stems from feminist intersectional research, to draw out
a systemic perspective of AI and include dimensions of AI that pertain to
invisible labour.

ÊëòË¶ÅÔºöÁï∂‰ª£ÈÄöÈÅé AI ÁöÑËá™ÂãïÂåñÈúÄË¶ÅÂ§ßÈáèÁöÑÂπïÂæå‰∫∫ÂäõÔºåÈÄôÈÄöÂ∏∏Êó¢‰∏çÂèØË¶ã‰∏îËñ™Ë≥áÈÅé‰Ωé„ÄÇÁî±Êñº‰∏çÂèØË¶ãÁöÑÂãûÂãïÔºåÂåÖÊã¨Ê®ôÁ±§ÂíåÁ∂≠Ë≠∑Â∑•‰ΩúÔºåÊòØÁï∂‰ª£ AI Á≥ªÁµ±ÁöÑÁµÑÊàêÈÉ®ÂàÜÔºåÂõ†Ê≠§ËÆì‰ΩøÁî®ËÄÖ‰∫ÜËß£ÂÖ∂ËßíËâ≤‰ªçÁÑ∂ÂæàÈáçË¶Å„ÄÇÊàëÂÄëÂª∫Ë≠∞ÈÄôÂèØ‰ª•ÈÄèÈÅéÂèØËß£ÈáãÁöÑ AIÔºàXAIÔºâË®≠Ë®à‰æÜÂÆåÊàêÔºåÁâπÂà•ÊòØÂ•≥ÊÄß‰∏ªÁæ©‰∫§ÂèâÁöÑ XAI„ÄÇÊàëÂÄëÊèêÂá∫Ê∫êËá™Â•≥ÊÄß‰∏ªÁæ©‰∫§ÂèâÁ†îÁ©∂ÁöÑË£ΩÂúñÊñπÊ≥ïÔºå‰ª•ÊèêÂá∫ AI ÁöÑÁ≥ªÁµ±ËßÄÈªûÔºå‰∏¶Á¥çÂÖ•Ëàá‰∏çÂèØË¶ãÂãûÂãïÁõ∏ÈóúÁöÑ AI Á∂≠Â∫¶„ÄÇ

##### **Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**
2304.13191v1 by Surjodeep Sarkar, Manas Gaur, L. Chen, Muskan Garg, Biplav Srivastava, Bhaktee Dongaonkar

Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to
support the overburdened global healthcare system that gets 60 million primary
care visits, and 6 million Emergency Room (ER) visits annually. These systems
are built by clinical psychologists, psychiatrists, and Artificial Intelligence
(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role
of VMHAs is to provide emotional support through information, focusing less on
developing a reflective conversation with the patient. A more comprehensive,
safe and explainable approach is required to build responsible VMHAs to ask
follow-up questions or provide a well-informed response. This survey offers a
systematic critical review of the existing conversational agents in mental
health, followed by new insights into the improvements of VMHAs with contextual
knowledge, datasets, and their emerging role in clinical decision support. We
also provide new directions toward enriching the user experience of VMHAs with
explainability, safety, and wholesome trustworthiness. Finally, we provide
evaluation metrics and practical considerations for VMHAs beyond the current
literature to build trust between VMHAs and patients in active communications.

ÊëòË¶ÅÔºöËôõÊì¨ÂøÉÁêÜÂÅ•Â∫∑Âä©ÁêÜ (VMHA) ÊåÅÁ∫åÈÄ≤Ê≠•Ôºå‰ª•ÊîØÊè¥ÊØèÂπ¥Êúâ 6000 Ëê¨‰∫∫Ê¨°ÂàùÁ¥ö‰øùÂÅ•Â∞±Ë®∫Âíå 600 Ëê¨‰∫∫Ê¨°ÊÄ•Ë®∫ÂÆ§ (ER) Â∞±Ë®∫ÁöÑË∂ÖË≤†Ëç∑ÂÖ®ÁêÉÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÊòØÁî±Ëá®Â∫äÂøÉÁêÜÂ≠∏ÂÆ∂„ÄÅÁ≤æÁ•ûÁßëÈÜ´Â∏´Âíå‰∫∫Â∑•Êô∫ÊÖß (AI) Á†îÁ©∂‰∫∫Âì°ÁÇ∫Ë™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊâÄÂª∫Êßã„ÄÇÁõÆÂâçÔºåVMHA ÁöÑËßíËâ≤ÊòØÈÄèÈÅéË≥áË®äÊèê‰æõÊÉÖÁ∑íÊîØÊåÅÔºåËºÉÂ∞ëËëóÈáçÊñºËàáÊÇ£ËÄÖÁôºÂ±ïÂèçÊÄùÊÄßÁöÑÂ∞çË©±„ÄÇÈúÄË¶ÅÊõ¥ÂÖ®Èù¢„ÄÅÂÆâÂÖ®‰∏îÂèØËß£ÈáãÁöÑÊñπÊ≥ï‰æÜÂª∫ÊßãË≤†Ë≤¨‰ªªÁöÑ VMHAÔºå‰ª•ÊèêÂá∫ÂæåÁ∫åÂïèÈ°åÊàñÊèê‰æõÂÖÖÂàÜÁöÑÂõûÊáâ„ÄÇÈÄôÈ†ÖË™øÊü•Êèê‰æõ‰∫ÜÂ∞çÂøÉÁêÜÂÅ•Â∫∑‰∏≠ÁèæÊúâÂ∞çË©±‰ª£ÁêÜÁöÑÁ≥ªÁµ±ÊÄßÊâπÂà§ÊÄßÂõûÈ°ßÔºåÊé•ËëóÊ∑±ÂÖ•Êé¢Ë®é‰∫Ü VMHA Âú®ËÑàÁµ°Áü•Ë≠ò„ÄÅË≥áÊñôÈõÜÂíåÂÖ∂Âú®Ëá®Â∫äÊ±∫Á≠ñÊîØÊè¥‰∏≠Êñ∞ËààËßíËâ≤ÁöÑÊîπÈÄ≤„ÄÇÊàëÂÄë‰πüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÂêëÔºå‰ª•ÈÄèÈÅéÂèØËß£ÈáãÊÄß„ÄÅÂÆâÂÖ®ÊÄßËàáÊï¥È´îÂèØ‰ø°Â∫¶‰æÜË±êÂØå VMHA ÁöÑ‰ΩøÁî®ËÄÖÈ´îÈ©ó„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫ÜË©ïÈáèÊåáÊ®ôÂíå VMHA ÁöÑÂØ¶ÂãôËÄÉÈáèÔºåË∂ÖË∂äÁõÆÂâçÁöÑÊñáÁçªÔºåÂú® VMHA ËàáÊÇ£ËÄÖÁöÑÁ©çÊ•µÊ∫ùÈÄö‰∏≠Âª∫Á´ã‰ø°‰ªª„ÄÇ

##### **A Brief Review of Explainable Artificial Intelligence in Healthcare**
2304.01543v1 by Zahra Sadeghi, Roohallah Alizadehsani, Mehmet Akif Cifci, Samina Kausar, Rizwan Rehman, Priyakshi Mahanta, Pranjal Kumar Bora, Ammar Almasri, Rami S. Alkhawaldeh, Sadiq Hussain, Bilal Alatas, Afshin Shoeibi, Hossein Moosaei, Milan Hladik, Saeid Nahavandi, Panos M. Pardalos

XAI refers to the techniques and methods for building AI applications which
assist end users to interpret output and predictions of AI models. Black box AI
applications in high-stakes decision-making situations, such as medical domain
have increased the demand for transparency and explainability since wrong
predictions may have severe consequences. Model explainability and
interpretability are vital successful deployment of AI models in healthcare
practices. AI applications' underlying reasoning needs to be transparent to
clinicians in order to gain their trust. This paper presents a systematic
review of XAI aspects and challenges in the healthcare domain. The primary
goals of this study are to review various XAI methods, their challenges, and
related machine learning models in healthcare. The methods are discussed under
six categories: Features-oriented methods, global methods, concept models,
surrogate models, local pixel-based methods, and human-centric methods. Most
importantly, the paper explores XAI role in healthcare problems to clarify its
necessity in safety-critical applications. The paper intends to establish a
comprehensive understanding of XAI-related applications in the healthcare field
by reviewing the related experimental results. To facilitate future research
for filling research gaps, the importance of XAI models from different
viewpoints and their limitations are investigated.

ÊëòË¶ÅÔºöXAI ÊåáÁöÑÊòØÁî®ÊñºÂª∫Êßã AI ÊáâÁî®Á®ãÂºèÁöÑÊäÄË°ìÂíåÊñπÊ≥ïÔºåÈÄô‰∫õÊáâÁî®Á®ãÂºèÂèØÂçîÂä©ÊúÄÁµÇ‰ΩøÁî®ËÄÖË©ÆÈáã AI Ê®°ÂûãÁöÑËº∏Âá∫ÂíåÈ†êÊ∏¨„ÄÇÂú®È´òÈ¢®Èö™Ê±∫Á≠ñÊÉÖÂ¢É‰∏≠Ôºå‰æãÂ¶ÇÈÜ´ÁôÇÈ†òÂüüÔºåÈªëÁÆ± AI ÊáâÁî®Á®ãÂºèÂ¢ûÂä†‰∫ÜÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÔºåÂõ†ÁÇ∫ÈåØË™§ÁöÑÈ†êÊ∏¨ÂèØËÉΩÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑÂæåÊûú„ÄÇÊ®°ÂûãÂèØËß£ÈáãÊÄßÂíåÂèØË©ÆÈáãÊÄßÂ∞çÊñºÂú®ÈÜ´ÁôÇÂØ¶Âãô‰∏≠ÊàêÂäüÈÉ®ÁΩ≤ AI Ê®°ÂûãËá≥ÈóúÈáçË¶Å„ÄÇAI ÊáâÁî®Á®ãÂºèÁöÑÂü∫Êú¨Êé®ÁêÜÈúÄË¶ÅÂ∞çËá®Â∫äÈÜ´ÁîüÈÄèÊòéÔºåÊâçËÉΩÁç≤Âæó‰ªñÂÄëÁöÑ‰ø°‰ªª„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÈÜ´ÁôÇÈ†òÂüü‰∏≠ XAI Èù¢ÂêëÂíåÊåëÊà∞ÁöÑÁ≥ªÁµ±ÊÄßÂõûÈ°ß„ÄÇÊú¨Á†îÁ©∂ÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂõûÈ°ßÂêÑÁ®Æ XAI ÊñπÊ≥ï„ÄÅÂÖ∂ÊåëÊà∞Ôºå‰ª•ÂèäÁõ∏ÈóúÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ê©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄô‰∫õÊñπÊ≥ïÂàÜÁÇ∫ÂÖ≠È°ûË®éË´ñÔºöÈù¢ÂêëÁâπÂæµÁöÑÊñπÊ≥ï„ÄÅÊï¥È´îÊñπÊ≥ï„ÄÅÊ¶ÇÂøµÊ®°Âûã„ÄÅ‰ª£ÁêÜÊ®°Âûã„ÄÅÂ±ÄÈÉ®Âü∫ÊñºÂÉèÁ¥†ÁöÑÊñπÊ≥ïÔºå‰ª•Âèä‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ï„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊú¨ÊñáÊé¢Ë®é‰∫Ü XAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÂïèÈ°å‰∏≠ÁöÑËßíËâ≤Ôºå‰ª•ÈáêÊ∏ÖÂÖ∂Âú®ÂÆâÂÖ®ÈóúÈçµÊáâÁî®‰∏≠ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊú¨ÊñáÊó®Âú®ÈÄèÈÅéÂõûÈ°ßÁõ∏ÈóúÁöÑÂØ¶È©óÁµêÊûúÔºåÂª∫Á´ãÂ∞çÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ XAI Áõ∏ÈóúÊáâÁî®Á®ãÂºèÁöÑÂÖ®Èù¢‰∫ÜËß£„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Êú™‰æÜÁ†îÁ©∂Â°´Ë£úÁ†îÁ©∂Â∑ÆË∑ùÔºåÊú¨ÊñáÊé¢Ë®é‰∫Ü XAI Ê®°ÂûãÂæû‰∏çÂêåËßÄÈªû‰æÜÁúãÁöÑÈáçË¶ÅÊÄßÂèäÂÖ∂ÈôêÂà∂„ÄÇ

##### **Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**
2303.12641v2 by Frederik Pahde, Maximilian Dreyer, Wojciech Samek, Sebastian Lapuschkin

State-of-the-art machine learning models often learn spurious correlations
embedded in the training data. This poses risks when deploying these models for
high-stake decision-making, such as in medical applications like skin cancer
detection. To tackle this problem, we propose Reveal to Revise (R2R), a
framework entailing the entire eXplainable Artificial Intelligence (XAI) life
cycle, enabling practitioners to iteratively identify, mitigate, and
(re-)evaluate spurious model behavior with a minimal amount of human
interaction. In the first step (1), R2R reveals model weaknesses by finding
outliers in attributions or through inspection of latent concepts learned by
the model. Secondly (2), the responsible artifacts are detected and spatially
localized in the input data, which is then leveraged to (3) revise the model
behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model
correction, and (4) (re-)evaluate the model's performance and remaining
sensitivity towards the artifact. Using two medical benchmark datasets for
Melanoma detection and bone age estimation, we apply our R2R framework to VGG,
ResNet and EfficientNet architectures and thereby reveal and correct real
dataset-intrinsic artifacts, as well as synthetic variants in a controlled
setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations
to mitigate different biases. Code is available on
https://github.com/maxdreyer/Reveal2Revise.

ÊëòË¶ÅÔºöÊúÄÂÖàËøõÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÈÄöÂ∏∏‰ºöÂ≠¶‰π†ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÂµåÂÖ•ÁöÑËôöÂÅáÂÖ≥ËÅî„ÄÇËøôÂú®Â∞ÜËøô‰∫õÊ®°ÂûãÈÉ®ÁΩ≤‰∫éÈ´òÈ£éÈô©ÂÜ≥Á≠ñÊó∂‰ºöÂ∏¶Êù•È£éÈô©Ôºå‰æãÂ¶ÇÂú®ÁöÆËÇ§ÁôåÊ£ÄÊµãÁ≠âÂåªÂ≠¶Â∫îÁî®‰∏≠„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Reveal to Revise (R2R)Ôºå‰∏Ä‰∏™Ê∂µÁõñÊï¥‰∏™ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) ÁîüÂëΩÂë®ÊúüÁöÑÊ°ÜÊû∂Ôºå‰Ωø‰ªé‰∏öËÄÖËÉΩÂ§ü‰ª•ÊúÄÂ∞ëÁöÑ‰∫∫Â∑•‰∫§‰∫íËø≠‰ª£ËØÜÂà´„ÄÅÁºìËß£ÂíåÔºàÈáçÊñ∞ÔºâËØÑ‰º∞ËôöÂÅáÊ®°ÂûãË°å‰∏∫„ÄÇÂú®Á¨¨‰∏ÄÊ≠• (1) ‰∏≠ÔºåR2R ÈÄöËøáÊâæÂá∫ÂΩíÂõ†‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÊàñÈÄöËøáÊ£ÄÊü•Ê®°ÂûãÂ≠¶‰π†ÁöÑÊΩúÂú®Ê¶ÇÂøµÊù•Êè≠Á§∫Ê®°ÂûãÁöÑÂº±ÁÇπ„ÄÇÂÖ∂Ê¨° (2)ÔºåÊ£ÄÊµãË¥üË¥£ÁöÑ‰º™ÂÉèÂπ∂Âú®ËæìÂÖ•Êï∞ÊçÆ‰∏≠ËøõË°åÁ©∫Èó¥ÂÆö‰ΩçÔºåÁÑ∂ÂêéÂà©Áî®ÂÆÉÊù• (3) ‰øÆÊîπÊ®°ÂûãË°å‰∏∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∫îÁî® RRR„ÄÅCDEP Âíå ClArC ÁöÑÊñπÊ≥ïÊù•ËøõË°åÊ®°ÂûãÊ†°Ê≠£ÔºåÂπ∂ (4)ÔºàÈáçÊñ∞ÔºâËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÂíåÂØπ‰º™ÂÉèÁöÑÂâ©‰ΩôÊïèÊÑüÊÄß„ÄÇ‰ΩøÁî®‰∏§‰∏™Áî®‰∫éÈªëËâ≤Á¥†Áò§Ê£ÄÊµãÂíåÈ™®ÈæÑ‰º∞ËÆ°ÁöÑÂåªÂ≠¶Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨Â∞ÜÊàë‰ª¨ÁöÑ R2R Ê°ÜÊû∂Â∫îÁî®‰∫é VGG„ÄÅResNet Âíå EfficientNet Êû∂ÊûÑÔºå‰ªéËÄåÊè≠Á§∫ÂíåÁ∫†Ê≠£‰∫ÜÁúüÂÆûÊï∞ÊçÆÈõÜÂõ∫ÊúâÁöÑ‰º™ÂÉèÔºå‰ª•ÂèäÂèóÊéßËÆæÁΩÆ‰∏≠ÁöÑÂêàÊàêÂèò‰Ωì„ÄÇÂÆåÊàê XAI ÁîüÂëΩÂë®ÊúüÔºåÊàë‰ª¨ÊºîÁ§∫‰∫ÜÂ§ö‰∏™ R2R Ëø≠‰ª£‰ª•ÂáèËΩª‰∏çÂêåÁöÑÂÅèÂ∑Æ„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/maxdreyer/Reveal2Revise ‰∏äÊâæÂà∞„ÄÇ

##### **Explainable AI for Time Series via Virtual Inspection Layers**
2303.06365v1 by Johanna Vielhaben, Sebastian Lapuschkin, Gr√©goire Montavon, Wojciech Samek

The field of eXplainable Artificial Intelligence (XAI) has greatly advanced
in recent years, but progress has mainly been made in computer vision and
natural language processing. For time series, where the input is often not
interpretable, only limited research on XAI is available. In this work, we put
forward a virtual inspection layer, that transforms the time series to an
interpretable representation and allows to propagate relevance attributions to
this representation via local XAI methods like layer-wise relevance propagation
(LRP). In this way, we extend the applicability of a family of XAI methods to
domains (e.g. speech) where the input is only interpretable after a
transformation. Here, we focus on the Fourier transformation which is
prominently applied in the interpretation of time series and LRP and refer to
our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time
series classification settings like audio and electronic health records. We
showcase how DFT-LRP reveals differences in the classification strategies of
models trained in different domains (e.g., time vs. frequency domain) or helps
to discover how models act on spurious correlations in the data.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) È†òÂüüÂú®ËøëÂπ¥‰æÜÂèñÂæóÈï∑Ë∂≥ÈÄ≤Ê≠•Ôºå‰ΩÜÈÄ≤Â±ï‰∏ªË¶ÅÊòØÂú®ÈõªËÖ¶Ë¶ñË¶∫ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊñπÈù¢„ÄÇÂ∞çÊñºËº∏ÂÖ•ÈÄöÂ∏∏ÁÑ°Ê≥ïËß£ÈáãÁöÑÊôÇÈñìÂ∫èÂàóÔºåÂè™ÊúâÊúâÈôêÁöÑÁ†îÁ©∂ÂèØ‰æõ‰ΩøÁî® XAI„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËôõÊì¨Ê™¢Êü•Â±§ÔºåÂÆÉÂ∞áÊôÇÈñìÂ∫èÂàóËΩâÊèõÁÇ∫ÂèØËß£ÈáãÁöÑË°®Á§∫Ôºå‰∏¶ÂÖÅË®±ÈÄöÈÅéÂ±§Á¥öÁõ∏ÈóúÊÄßÂÇ≥Êí≠ (LRP) Á≠âÂ±ÄÈÉ® XAI ÊñπÊ≥ïÂ∞áÁõ∏ÈóúÊÄßÊ≠∏Âõ†ÂÇ≥Êí≠Âà∞Ê≠§Ë°®Á§∫„ÄÇËóâÊ≠§ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàó XAI ÊñπÊ≥ïÁöÑÈÅ©Áî®ÊÄßÊì¥Â±ïÂà∞Ëº∏ÂÖ•ÂÉÖÂú®ËΩâÊèõÂæåÊâçËÉΩËß£ÈáãÁöÑÈ†òÂüüÔºà‰æãÂ¶ÇË™ûÈü≥Ôºâ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂÇÖÁ´ãËëâËΩâÊèõÔºåÂÆÉ‰∏ªË¶ÅÊáâÁî®ÊñºÊôÇÈñìÂ∫èÂàóÂíå LRP ÁöÑËß£ÈáãÔºå‰∏¶Â∞áÊàëÂÄëÁöÑÁ®±‰πãÁÇ∫ DFT-LRP„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü DFT-LRP Âú®ÂêÑÁ®ÆÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûË®≠ÂÆöÔºà‰æãÂ¶ÇÈü≥Ë®äÂíåÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑÔºâ‰∏≠ÁöÑÊïàÁî®„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü DFT-LRP Â¶Ç‰ΩïÊè≠Á§∫Âú®‰∏çÂêåÈ†òÂüüÔºà‰æãÂ¶ÇÊôÇÈñìËàáÈ†ªÁéáÂüüÔºâË®ìÁ∑¥ÁöÑÊ®°ÂûãÁöÑÂàÜÈ°ûÁ≠ñÁï•Â∑ÆÁï∞ÔºåÊàñÊúâÂä©ÊñºÁôºÁèæÊ®°ÂûãÂ¶Ç‰ΩïËôïÁêÜË≥áÊñô‰∏≠ÁöÑËôõÂÅáÈóúËÅØ„ÄÇ

##### **Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**
2303.04731v1 by Truong Thanh Hung Nguyen, Van Binh Truong, Vo Thanh Khang Nguyen, Quoc Hung Cao, Quoc Khanh Nguyen

The ability to explain the prediction of deep learning models to end-users is
an important feature to leverage the power of artificial intelligence (AI) for
the medical decision-making process, which is usually considered
non-transparent and challenging to comprehend. In this paper, we apply
state-of-the-art eXplainable artificial intelligence (XAI) methods to explain
the prediction of the black-box AI models in the thyroid nodule diagnosis
application. We propose new statistic-based XAI methods, namely Kernel Density
Estimation and Density map, to explain the case of no nodule detected. XAI
methods' performances are considered under a qualitative and quantitative
comparison as feedback to improve the data quality and the model performance.
Finally, we survey to assess doctors' and patients' trust in XAI explanations
of the model's decisions on thyroid nodule images.

ÊëòË¶ÅÔºöËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûúÁöÑËÉΩÂäõÂ∞çÊúÄÁµÇ‰ΩøÁî®ËÄÖËÄåË®ÄÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÂäüËÉΩÔºåÂèØÂà©Áî®‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂäõÈáèÈÄ≤Ë°åÈÜ´ÁôÇÊ±∫Á≠ñÊµÅÁ®ãÔºåÈÄôÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ÊòØ‰∏çÈÄèÊòé‰∏îÈõ£‰ª•ÁêÜËß£ÁöÑ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÅãÁî®ÊúÄÂÖàÈÄ≤ÁöÑÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊñπÊ≥ï‰æÜËß£ÈáãÈªëÁõí AI Ê®°ÂûãÂú®Áî≤ÁãÄËÖ∫ÁµêÁØÄË®∫Êñ∑ÊáâÁî®‰∏≠ÁöÑÈ†êÊ∏¨ÁµêÊûú„ÄÇÊàëÂÄëÊèêÂá∫Êñ∞ÁöÑÂü∫ÊñºÁµ±Ë®àÁöÑ XAI ÊñπÊ≥ïÔºåÂç≥Ê†∏ÂØÜÂ∫¶‰º∞Ë®àÂíåÂØÜÂ∫¶ÂúñÔºå‰æÜËß£ÈáãÊú™Ê™¢Ê∏¨Âà∞ÁµêÁØÄÁöÑÊÉÖÊ≥Å„ÄÇXAI ÊñπÊ≥ïÁöÑÊïàËÉΩÊúÉÂú®ÂÆöÊÄßÂíåÂÆöÈáèÊØîËºÉ‰∏ãË¢´Ë¶ñÁÇ∫ÊîπÂñÑË≥áÊñôÂìÅË≥™ÂíåÊ®°ÂûãÊïàËÉΩÁöÑÂõûÈ•ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÄ≤Ë°åË™øÊü•‰ª•Ë©ï‰º∞ÈÜ´Â∏´ÂíåÊÇ£ËÄÖÂ∞ç XAI Â∞çÊ®°ÂûãÂú®Áî≤ÁãÄËÖ∫ÁµêÁØÄÂΩ±ÂÉè‰∏≠Ê±∫Á≠ñÁöÑËß£ÈáãÁöÑ‰ø°‰ªªÂ∫¶„ÄÇ

