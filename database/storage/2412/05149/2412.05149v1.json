{"2412.05149": {"publish_time": "2024-12-06", "title": "Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora", "paper_summary": "The BabyLM Challenge is a community effort to close the data-efficiency gap\nbetween human and computational language learners. Participants compete to\noptimize language model training on a fixed language data budget of 100 million\nwords or less. This year, we released improved text corpora, as well as a\nvision-and-language corpus to facilitate research into cognitively plausible\nvision language models. Submissions were compared on evaluation tasks targeting\ngrammatical ability, (visual) question answering, pragmatic abilities, and\ngrounding, among other abilities. Participants could submit to a 10M-word\ntext-only track, a 100M-word text-only track, and/or a 100M-word and image\nmultimodal track. From 31 submissions employing diverse methods, a hybrid\ncausal-masked language model architecture outperformed other approaches. No\nsubmissions outperformed the baselines in the multimodal track. In follow-up\nanalyses, we found a strong relationship between training FLOPs and average\nperformance across tasks, and that the best-performing submissions proposed\nchanges to the training data, training objective, and model architecture. This\nyear's BabyLM Challenge shows that there is still significant room for\ninnovation in this setting, in particular for image-text modeling, but\ncommunity-driven research can yield actionable insights about effective\nstrategies for small-scale language modeling.", "paper_summary_zh": "BabyLM \u6311\u6230\u8cfd\u662f\u793e\u7fa4\u7684\u5171\u540c\u52aa\u529b\uff0c\u65e8\u5728\u7e2e\u5c0f\u4eba\u985e\u548c\u8a08\u7b97\u8a9e\u8a00\u5b78\u7fd2\u8005\u4e4b\u9593\u7684\u8cc7\u6599\u6548\u7387\u5dee\u8ddd\u3002\u53c3\u8207\u8005\u7af6\u76f8\u5728\u56fa\u5b9a\u70ba 1 \u5104\u500b\u5b57\u6216\u66f4\u5c11\u7684\u8a9e\u8a00\u8cc7\u6599\u9810\u7b97\u4e2d\uff0c\u6700\u4f73\u5316\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u3002\u4eca\u5e74\uff0c\u6211\u5011\u767c\u5e03\u4e86\u6539\u826f\u7684\u6587\u5b57\u8a9e\u6599\u5eab\uff0c\u4ee5\u53ca\u4e00\u500b\u8996\u89ba\u8207\u8a9e\u8a00\u8a9e\u6599\u5eab\uff0c\u4ee5\u4fc3\u9032\u5c0d\u8a8d\u77e5\u4e0a\u5408\u7406\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u7814\u7a76\u3002\u63d0\u4ea4\u7684\u4f5c\u54c1\u5728\u8a55\u91cf\u4efb\u52d9\u4e2d\u9032\u884c\u6bd4\u8f03\uff0c\u9019\u4e9b\u4efb\u52d9\u91dd\u5c0d\u8a9e\u6cd5\u80fd\u529b\u3001\uff08\u8996\u89ba\uff09\u554f\u984c\u89e3\u7b54\u3001\u8a9e\u7528\u80fd\u529b\u548c\u57fa\u790e\u7b49\u5404\u7a2e\u80fd\u529b\u3002\u53c3\u8207\u8005\u53ef\u4ee5\u63d0\u4ea4\u5230\u50c5\u9650\u6587\u5b57\u7684 10M \u5b57\u7d44\u3001\u50c5\u9650\u6587\u5b57\u7684 100M \u5b57\u7d44\uff0c\u548c/\u6216 100M \u5b57\u548c\u5716\u50cf\u7684\u591a\u6a21\u7d44\u7d44\u3002\u5728\u63a1\u7528\u5404\u7a2e\u65b9\u6cd5\u7684 31 \u9805\u63d0\u4ea4\u4e2d\uff0c\u4e00\u500b\u6df7\u5408\u56e0\u679c\u906e\u7f69\u8a9e\u8a00\u6a21\u578b\u67b6\u69cb\u512a\u65bc\u5176\u4ed6\u65b9\u6cd5\u3002\u5728\u591a\u6a21\u7d44\u7d44\u4e2d\uff0c\u6c92\u6709\u63d0\u4ea4\u7684\u4f5c\u54c1\u512a\u65bc\u57fa\u6e96\u3002\u5728\u5f8c\u7e8c\u5206\u6790\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u8a13\u7df4 FLOP \u548c\u5404\u9805\u4efb\u52d9\u7684\u5e73\u5747\u8868\u73fe\u4e4b\u9593\u6709\u5f88\u5f37\u7684\u95dc\u4fc2\uff0c\u4e26\u4e14\u8868\u73fe\u6700\u4f73\u7684\u63d0\u4ea4\u4f5c\u54c1\u63d0\u51fa\u4e86\u8a13\u7df4\u8cc7\u6599\u3001\u8a13\u7df4\u76ee\u6a19\u548c\u6a21\u578b\u67b6\u69cb\u7684\u8b8a\u66f4\u3002\u4eca\u5e74\u7684 BabyLM \u6311\u6230\u8cfd\u986f\u793a\uff0c\u5728\u9019\u500b\u8a2d\u5b9a\u4e2d\u4ecd\u7136\u6709\u5f88\u5927\u7684\u5275\u65b0\u7a7a\u9593\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u5f71\u50cf\u6587\u5b57\u5efa\u6a21\uff0c\u4f46\u793e\u7fa4\u9a45\u52d5\u7684\u7814\u7a76\u53ef\u4ee5\u7522\u751f\u95dc\u65bc\u5c0f\u898f\u6a21\u8a9e\u8a00\u5efa\u6a21\u7684\u6709\u6548\u7b56\u7565\u7684\u53ef\u884c\u898b\u89e3\u3002", "author": "Michael Y. Hu et.al.", "authors": "Michael Y. Hu, Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Ryan Cotterell, Leshem Choshen, Alex Warstadt, Ethan Gotlieb Wilcox", "id": "2412.05149v1", "paper_url": "http://arxiv.org/abs/2412.05149v1", "repo": "null"}}