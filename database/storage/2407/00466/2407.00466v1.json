{"2407.00466": {"publish_time": "2024-06-29", "title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science", "paper_summary": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,\ndraws increasing attention, where one common approach is to build a copilot\nagent driven by Large Language Models (LLMs). However, to evaluate such\nsystems, people either rely on direct Question-Answering (QA) to the LLM\nitself, or in a biomedical experimental manner. How to precisely benchmark\nbiomedical agents from an AI Scientist perspective remains largely unexplored.\nTo this end, we draw inspiration from one most important abilities of\nscientists, understanding the literature, and introduce BioKGBench. In contrast\nto traditional evaluation benchmark that only focuses on factual QA, where the\nLLMs are known to have hallucination issues, we first disentangle\n\"Understanding Literature\" into two atomic abilities, i) \"Understanding\" the\nunstructured text from research papers by performing scientific claim\nverification, and ii) Ability to interact with structured Knowledge-Graph\nQuestion-Answering (KGQA) as a form of \"Literature\" grounding. We then\nformulate a novel agent task, dubbed KGCheck, using KGQA and domain-based\nRetrieval-Augmented Generation (RAG) to identify the factual errors of existing\nlarge-scale knowledge graph databases. We collect over two thousand data for\ntwo atomic tasks and 225 high-quality annotated data for the agent task.\nSurprisingly, we discover that state-of-the-art agents, both daily scenarios\nand biomedical ones, have either failed or inferior performance on our\nbenchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.\nOn the widely used popular knowledge graph, we discover over 90 factual errors\nwhich provide scenarios for agents to make discoveries and demonstrate the\neffectiveness of our approach. The code and data are available at\nhttps://github.com/westlake-autolab/BioKGBench.", "paper_summary_zh": "<paragraph>\u8ffd\u6c42\u751f\u7269\u91ab\u5b78\u79d1\u5b78\u7684\u4eba\u5de5\u667a\u6167\uff0c\u53c8\u7a31 AI \u79d1\u5b78\u5bb6\uff0c\n\u8d8a\u4f86\u8d8a\u53d7\u5230\u95dc\u6ce8\uff0c\u5176\u4e2d\u4e00\u7a2e\u5e38\u898b\u7684\u65b9\u6cd5\u662f\u5efa\u7acb\u7531\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9a45\u52d5\u7684\u526f\u99d5\u99db\u4ee3\u7406\u3002\u7136\u800c\uff0c\u8981\u8a55\u4f30\u6b64\u985e\n\u7cfb\u7d71\uff0c\u4eba\u5011\u8981\u4e48\u4f9d\u8cf4 LLM \u672c\u8eab\u7684\u76f4\u63a5\u554f\u7b54 (QA)\uff0c\u8981\u4e48\u4f9d\u8cf4\u751f\u7269\u91ab\u5b78\u5be6\u9a57\u65b9\u5f0f\u3002\u5982\u4f55\u5f9e AI \u79d1\u5b78\u5bb6\u7684\u89d2\u5ea6\u7cbe\u78ba\u8a55\u91cf\n\u751f\u7269\u91ab\u5b78\u4ee3\u7406\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u63a2\u7d22\u3002\n\u70ba\u6b64\uff0c\u6211\u5011\u5f9e\u79d1\u5b78\u5bb6\u6700\u91cd\u8981\u7684\u80fd\u529b\u4e4b\u4e00\uff0c\u5373\u7406\u89e3\u6587\u737b\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u4e26\u4ecb\u7d39 BioKGBench\u3002\u8207\u50c5\u95dc\u6ce8\u4e8b\u5be6 QA \u7684\u50b3\u7d71\u8a55\u91cf\u57fa\u6e96\u4e0d\u540c\uff0c\u5df2\u77e5 LLM \u5728\u4e8b\u5be6 QA \u4e2d\u5b58\u5728\u5e7b\u89ba\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u5c07\n\u300c\u7406\u89e3\u6587\u737b\u300d\u5206\u89e3\u70ba\u5169\u7a2e\u57fa\u672c\u80fd\u529b\uff0ci) \u900f\u904e\u57f7\u884c\u79d1\u5b78\u4e3b\u5f35\u9a57\u8b49\u4f86\u300c\u7406\u89e3\u300d\u7814\u7a76\u8ad6\u6587\u4e2d\u7684\u975e\u7d50\u69cb\u5316\u6587\u5b57\uff0c\u4ee5\u53ca ii) \u4ee5\u300c\u6587\u737b\u300d\u70ba\u57fa\u790e\uff0c\u8207\u7d50\u69cb\u5316\u7684\u77e5\u8b58\u5716\u8868\u554f\u7b54 (KGQA) \u4e92\u52d5\u7684\u80fd\u529b\u3002\u7136\u5f8c\n\u6211\u5011\u4f7f\u7528 KGQA \u548c\u57fa\u65bc\u7db2\u57df\u7684\u6aa2\u7d22\u64f4\u5145\u7522\u751f (RAG) \u5236\u5b9a\u4e86\u4e00\u9805\u65b0\u7a4e\u7684\u4ee3\u7406\u4efb\u52d9\uff0c\u7a31\u70ba KGCheck\uff0c\u4ee5\u8b58\u5225\u73fe\u6709\u5927\u578b\u77e5\u8b58\u5716\u8868\u8cc7\u6599\u5eab\u7684\u4e8b\u5be6\u932f\u8aa4\u3002\u6211\u5011\u70ba\n\u5169\u500b\u57fa\u672c\u4efb\u52d9\u6536\u96c6\u4e86\u5169\u5343\u591a\u500b\u8cc7\u6599\uff0c\u4ee5\u53ca 225 \u500b\u9ad8\u54c1\u8cea\u8a3b\u89e3\u8cc7\u6599\uff0c\u4ee5\u4f5c\u70ba\u4ee3\u7406\u4efb\u52d9\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u6700\u5148\u9032\u7684\u4ee3\u7406\uff0c\u7121\u8ad6\u662f\u65e5\u5e38\u60c5\u5883\u9084\u662f\u751f\u7269\u91ab\u5b78\uff0c\u5728\u6211\u5011\u7684\n\u57fa\u6e96\u4e0a\u90fd\u8868\u73fe\u4e0d\u4f73\u6216\u8868\u73fe\u8f03\u5dee\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u57fa\u6e96\uff0c\u7a31\u70ba BKGAgent\u3002\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684\u71b1\u9580\u77e5\u8b58\u5716\u8868\u4e0a\uff0c\u6211\u5011\u767c\u73fe\u8d85\u904e 90 \u500b\u4e8b\u5be6\u932f\u8aa4\uff0c\u9019\u4e9b\u932f\u8aa4\u70ba\u4ee3\u7406\u63d0\u4f9b\u4e86\u767c\u73fe\u60c5\u5883\uff0c\u4e26\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728\nhttps://github.com/westlake-autolab/BioKGBench \u53d6\u5f97\u3002</paragraph>", "author": "Xinna Lin et.al.", "authors": "Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z. Li, Kaicheng Yu", "id": "2407.00466v1", "paper_url": "http://arxiv.org/abs/2407.00466v1", "repo": "null"}}