
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-11-18**|**Bi-Mamba: Towards Accurate 1-Bit State Space Models**|Shengkun Tang et.al.|[2411.11843v1](http://arxiv.org/abs/2411.11843v1)|null|
|**2024-11-18**|**Tackling prediction tasks in relational databases with LLMs**|Marek Wydmuch et.al.|[2411.11829v1](http://arxiv.org/abs/2411.11829v1)|null|
|**2024-11-18**|**LightFFDNets: Lightweight Convolutional Neural Networks for Rapid Facial Forgery Detection**|Günel Jabbarlı et.al.|[2411.11826v1](http://arxiv.org/abs/2411.11826v1)|null|
|**2024-11-18**|**Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion**|Meng Zhou et.al.|[2411.11799v1](http://arxiv.org/abs/2411.11799v1)|[link](https://github.com/simonzhou86/en_dran)|
|**2024-11-18**|**Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods**|Egor Kovalev et.al.|[2411.11795v1](http://arxiv.org/abs/2411.11795v1)|null|
|**2024-11-18**|**Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care**|Jeffrey N. Clark et.al.|[2411.11774v1](http://arxiv.org/abs/2411.11774v1)|null|
|**2024-11-18**|**CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task**|Zishuo Feng et.al.|[2411.11770v1](http://arxiv.org/abs/2411.11770v1)|null|
|**2024-11-18**|**The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning**|Longju Bai et.al.|[2411.11758v1](http://arxiv.org/abs/2411.11758v1)|null|
|**2024-11-18**|**QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou**|Xinchen Luo et.al.|[2411.11739v1](http://arxiv.org/abs/2411.11739v1)|null|
|**2024-11-18**|**WoodYOLO: A Novel Object Detector for Wood Species Detection in Microscopic Images**|Lars Nieradzik et.al.|[2411.11738v1](http://arxiv.org/abs/2411.11738v1)|null|
|**2024-11-18**|**Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment**|Allison Huang et.al.|[2411.11731v1](http://arxiv.org/abs/2411.11731v1)|null|
|**2024-11-18**|**Lifted Model Construction without Normalisation: A Vectorised Approach to Exploit Symmetries in Factor Graphs**|Malte Luttermann et.al.|[2411.11730v1](http://arxiv.org/abs/2411.11730v1)|[link](https://github.com/StatisticalRelationalAI/AlphaAdvancedColourPassing)|
|**2024-11-18**|**Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation**|Mingchao Qi et.al.|[2411.11714v1](http://arxiv.org/abs/2411.11714v1)|[link](https://github.com/mingchaoqi/skill_transfer)|
|**2024-11-18**|**FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models**|Tao Fan et.al.|[2411.11707v1](http://arxiv.org/abs/2411.11707v1)|null|
|**2024-11-18**|**MC-LLaVA: Multi-Concept Personalized Vision-Language Model**|Ruichuan An et.al.|[2411.11706v1](http://arxiv.org/abs/2411.11706v1)|null|
|**2024-11-18**|**Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search**|Jinhao Jiang et.al.|[2411.11694v1](http://arxiv.org/abs/2411.11694v1)|null|
|**2024-11-18**|**Conceptwm: A Diffusion Model Watermark for Concept Protection**|Liangqi Lei et.al.|[2411.11688v1](http://arxiv.org/abs/2411.11688v1)|null|
|**2024-11-18**|**TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World**|Xianlong Wang et.al.|[2411.11683v1](http://arxiv.org/abs/2411.11683v1)|null|
|**2024-11-18**|**PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment**|Jiawei Li et.al.|[2411.11681v1](http://arxiv.org/abs/2411.11681v1)|null|
|**2024-11-18**|**Artificial Scientific Discovery**|Antonio Norelli et.al.|[2411.11672v1](http://arxiv.org/abs/2411.11672v1)|null|
|**2024-11-18**|**Dissecting Misalignment of Multimodal Large Language Models via Influence Function**|Lijie Hu et.al.|[2411.11667v1](http://arxiv.org/abs/2411.11667v1)|null|
|**2024-11-18**|**No-regret Exploration in Shuffle Private Reinforcement Learning**|Shaojie Bai et.al.|[2411.11647v1](http://arxiv.org/abs/2411.11647v1)|null|
|**2024-11-18**|**TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection**|Mengxuan Li et.al.|[2411.11641v1](http://arxiv.org/abs/2411.11641v1)|null|
|**2024-11-18**|**SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation**|Shiman Li et.al.|[2411.11636v1](http://arxiv.org/abs/2411.11636v1)|null|
|**2024-11-18**|**Chapter 7 Review of Data-Driven Generative AI Models for Knowledge Extraction from Scientific Literature in Healthcare**|Leon Kopitar et.al.|[2411.11635v1](http://arxiv.org/abs/2411.11635v1)|null|
|**2024-11-18**|**Federated Incremental Named Entity Recognition**|Duzhen Zhang et.al.|[2411.11623v1](http://arxiv.org/abs/2411.11623v1)|null|
|**2024-11-18**|**ST-Tree with Interpretability for Multivariate Time Series Classification**|Mingsen Du et.al.|[2411.11620v1](http://arxiv.org/abs/2411.11620v1)|[link](https://github.com/dumingsen/ST-Tree)|
|**2024-11-18**|**Signaling and Social Learning in Swarms of Robots**|Leo Cazenille et.al.|[2411.11616v1](http://arxiv.org/abs/2411.11616v1)|null|
|**2024-11-18**|**OASIS: Open Agents Social Interaction Simulations on One Million Agents**|Ziyi Yang et.al.|[2411.11581v1](http://arxiv.org/abs/2411.11581v1)|null|
|**2024-11-18**|**Hybrid Data-Driven SSM for Interpretable and Label-Free mmWave Channel Prediction**|Yiyong Sun et.al.|[2411.11576v1](http://arxiv.org/abs/2411.11576v1)|null|
|**2024-11-18**|**Topology-aware Preemptive Scheduling for Co-located LLM Workloads**|Ping Zhang et.al.|[2411.11560v1](http://arxiv.org/abs/2411.11560v1)|null|
|**2024-11-18**|**Real-Time Fitness Exercise Classification and Counting from Video Frames**|Riccardo Riccio et.al.|[2411.11548v1](http://arxiv.org/abs/2411.11548v1)|[link](https://github.com/riccardoriccio/fitness-ai-trainer-with-automatic-exercise-recognition-and-counting)|
|**2024-11-18**|**Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment**|Zhendong Liu et.al.|[2411.11543v1](http://arxiv.org/abs/2411.11543v1)|null|
|**2024-11-18**|**Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality**|Viktoriia Chekalina et.al.|[2411.11531v1](http://arxiv.org/abs/2411.11531v1)|null|
|**2024-11-18**|**A Pre-Trained Graph-Based Model for Adaptive Sequencing of Educational Documents**|Jean Vassoyan et.al.|[2411.11520v1](http://arxiv.org/abs/2411.11520v1)|null|
|**2024-11-18**|**Structure learning with Temporal Gaussian Mixture for model-based Reinforcement Learning**|Théophile Champion et.al.|[2411.11511v1](http://arxiv.org/abs/2411.11511v1)|null|
|**2024-11-18**|**Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering**|Xinyan Guan et.al.|[2411.11504v1](http://arxiv.org/abs/2411.11504v1)|null|
|**2024-11-18**|**Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models**|Chenhang Cui et.al.|[2411.11496v1](http://arxiv.org/abs/2411.11496v1)|null|
|**2024-11-18**|**Alien Recombination: Exploring Concept Blends Beyond Human Cognitive Availability in Visual Art**|Alejandro Hernandez et.al.|[2411.11494v1](http://arxiv.org/abs/2411.11494v1)|null|
|**2024-11-18**|**Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts**|Jingxuan Li et.al.|[2411.11479v1](http://arxiv.org/abs/2411.11479v1)|null|
|**2024-11-18**|**Re-examining learning linear functions in context**|Omar Naim et.al.|[2411.11465v1](http://arxiv.org/abs/2411.11465v1)|null|
|**2024-11-18**|**HistoEncoder: a digital pathology foundation model for prostate cancer**|Joona Pohjonen et.al.|[2411.11458v1](http://arxiv.org/abs/2411.11458v1)|null|
|**2024-11-18**|**Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet**|Marnix Suilen et.al.|[2411.11451v1](http://arxiv.org/abs/2411.11451v1)|null|
|**2024-11-18**|**Unveiling the Inflexibility of Adaptive Embedding in Traffic Forecasting**|Hongjun Wang et.al.|[2411.11448v1](http://arxiv.org/abs/2411.11448v1)|null|
|**2024-11-18**|**Membership Inference Attack against Long-Context Large Language Models**|Zixiong Wang et.al.|[2411.11424v1](http://arxiv.org/abs/2411.11424v1)|null|
|**2024-11-18**|**IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos**|Yunong Liu et.al.|[2411.11409v1](http://arxiv.org/abs/2411.11409v1)|null|
|**2024-11-18**|**The GECo algorithm for Graph Neural Networks Explanation**|Salvatore Calderaro et.al.|[2411.11391v1](http://arxiv.org/abs/2411.11391v1)|null|
|**2024-11-18**|**Rethinking Thinking Tokens: Understanding Why They Underperform in Practice**|Sreeram Vennam et.al.|[2411.11371v1](http://arxiv.org/abs/2411.11371v1)|null|
|**2024-11-18**|**Continual Task Learning through Adaptive Policy Self-Composition**|Shengchao Hu et.al.|[2411.11364v1](http://arxiv.org/abs/2411.11364v1)|null|
|**2024-11-18**|**MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models**|Harshita Sharma et.al.|[2411.11362v1](http://arxiv.org/abs/2411.11362v1)|null|
|**2024-11-18**|**Mitigating Knowledge Conflicts in Language Model-Driven Question Answering**|Han Cao et.al.|[2411.11344v1](http://arxiv.org/abs/2411.11344v1)|null|
|**2024-11-18**|**TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation**|Ranmin Wang et.al.|[2411.11305v1](http://arxiv.org/abs/2411.11305v1)|null|
|**2024-11-18**|**Recurrent Stochastic Configuration Networks with Incremental Blocks**|Gang Dang et.al.|[2411.11303v1](http://arxiv.org/abs/2411.11303v1)|null|
|**2024-11-18**|**Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation**|Peng Shu et.al.|[2411.11295v1](http://arxiv.org/abs/2411.11295v1)|null|
|**2024-11-18**|**LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large Language Models**|Yungi Kim et.al.|[2411.11289v1](http://arxiv.org/abs/2411.11289v1)|null|
|**2024-11-18**|**Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development**|Ranjan Sapkota et.al.|[2411.11285v1](http://arxiv.org/abs/2411.11285v1)|null|
|**2024-11-18**|**Multi-Hyperbolic Space-based Heterogeneous Graph Attention Network**|Jongmin Park et.al.|[2411.11283v1](http://arxiv.org/abs/2411.11283v1)|null|
|**2024-11-18**|**Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction**|Yucong Meng et.al.|[2411.11282v1](http://arxiv.org/abs/2411.11282v1)|null|
|**2024-11-18**|**VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently**|Keer Lu et.al.|[2411.11266v1](http://arxiv.org/abs/2411.11266v1)|null|
|**2024-11-18**|**Cross-Patient Pseudo Bags Generation and Curriculum Contrastive Learning for Imbalanced Multiclassification of Whole Slide Image**|Yonghuang Wu et.al.|[2411.11262v1](http://arxiv.org/abs/2411.11262v1)|null|
|**2024-11-18**|**Large corpora and large language models: a replicable method for automating grammatical annotation**|Cameron Morin et.al.|[2411.11260v1](http://arxiv.org/abs/2411.11260v1)|null|
|**2024-11-18**|**EXCON: Extreme Instance-based Contrastive Representation Learning of Severely Imbalanced Multivariate Time Series for Solar Flare Prediction**|Onur Vural et.al.|[2411.11249v1](http://arxiv.org/abs/2411.11249v1)|null|
|**2024-11-18**|**ZeFaV: Boosting Large Language Models for Zero-shot Fact Verification**|Son T. Luu et.al.|[2411.11247v1](http://arxiv.org/abs/2411.11247v1)|null|
|**2024-11-18**|**MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis**|Yingjie Zhou et.al.|[2411.11235v1](http://arxiv.org/abs/2411.11235v1)|null|
|**2024-11-18**|**MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs**|Shiyi Cao et.al.|[2411.11217v1](http://arxiv.org/abs/2411.11217v1)|null|
|**2024-11-17**|**Capturing Sparks of Abstraction for the ARC Challenge**|Martin Andrews et.al.|[2411.11206v1](http://arxiv.org/abs/2411.11206v1)|null|
|**2024-11-17**|**Debiasing Watermarks for Large Language Models via Maximal Coupling**|Yangxinyu Xie et.al.|[2411.11203v1](http://arxiv.org/abs/2411.11203v1)|null|
|**2024-11-17**|**PickScan: Object discovery and reconstruction from handheld interactions**|Vincent van der Brugge et.al.|[2411.11196v1](http://arxiv.org/abs/2411.11196v1)|null|
|**2024-11-17**|**Enhanced Anime Image Generation Using USE-CMHSA-GAN**|J. Lu et.al.|[2411.11179v1](http://arxiv.org/abs/2411.11179v1)|null|
|**2024-11-17**|**LLäMmlein: Compact and Competitive German-Only Language Models from Scratch**|Jan Pfister et.al.|[2411.11171v1](http://arxiv.org/abs/2411.11171v1)|null|
|**2024-11-17**|**RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer**|Jiawei Zhang et.al.|[2411.11162v1](http://arxiv.org/abs/2411.11162v1)|null|
|**2024-11-17**|**MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records**|Eric Yang et.al.|[2411.11161v1](http://arxiv.org/abs/2411.11161v1)|null|
|**2024-11-17**|**TabDeco: A Comprehensive Contrastive Framework for Decoupled Representations in Tabular Data**|Suiyao Chen et.al.|[2411.11148v1](http://arxiv.org/abs/2411.11148v1)|null|
|**2024-11-17**|**CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning**|Depeng Chen et.al.|[2411.11144v1](http://arxiv.org/abs/2411.11144v1)|null|
|**2024-11-17**|**Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks**|Deepa Anand et.al.|[2411.11105v1](http://arxiv.org/abs/2411.11105v1)|null|
|**2024-11-17**|**The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study on Media Bias Detection**|Tomas Horych et.al.|[2411.11081v1](http://arxiv.org/abs/2411.11081v1)|null|
|**2024-11-17**|**Multilingual Large Language Models: A Systematic Survey**|Shaolin Zhu et.al.|[2411.11072v1](http://arxiv.org/abs/2411.11072v1)|null|
|**2024-11-17**|**Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text**|Xiaoliang Luo et.al.|[2411.11061v1](http://arxiv.org/abs/2411.11061v1)|null|
|**2024-11-17**|**FastDraft: How to Train Your Draft**|Ofir Zafrir et.al.|[2411.11055v1](http://arxiv.org/abs/2411.11055v1)|null|
|**2024-11-17**|**SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation**|Bin Xu et.al.|[2411.11053v1](http://arxiv.org/abs/2411.11053v1)|null|
|**2024-11-17**|**Knowledge-enhanced Transformer for Multivariate Long Sequence Time-series Forecasting**|Shubham Tanaji Kakde et.al.|[2411.11046v1](http://arxiv.org/abs/2411.11046v1)|null|
|**2024-11-17**|**Wafer Map Defect Classification Using Autoencoder-Based Data Augmentation and Convolutional Neural Network**|Yin-Yin Bao et.al.|[2411.11029v1](http://arxiv.org/abs/2411.11029v1)|null|
|**2024-11-17**|**BianCang: A Traditional Chinese Medicine Large Language Model**|Sibo Wei et.al.|[2411.11027v1](http://arxiv.org/abs/2411.11027v1)|[link](https://github.com/qlu-nlp/biancang)|
|**2024-11-17**|**Time Step Generating: A Universal Synthesized Deepfake Image Detector**|Ziyue Zeng et.al.|[2411.11016v1](http://arxiv.org/abs/2411.11016v1)|[link](https://github.com/NuayHL/TimeStepGenerating)|
|**2024-11-17**|**BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation**|Haiyang Yu et.al.|[2411.11006v1](http://arxiv.org/abs/2411.11006v1)|null|
|**2024-11-17**|**Modulating Reservoir Dynamics via Reinforcement Learning for Efficient Robot Skill Synthesis**|Zahra Koulaeizadeh et.al.|[2411.10991v1](http://arxiv.org/abs/2411.10991v1)|null|
|**2024-11-17**|**VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?**|Yunlong Tang et.al.|[2411.10979v1](http://arxiv.org/abs/2411.10979v1)|null|
|**2024-11-17**|**SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration**|Jintao Zhang et.al.|[2411.10958v1](http://arxiv.org/abs/2411.10958v1)|[link](https://github.com/thu-ml/SageAttention)|
|**2024-11-17**|**IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs**|Sejun Park et.al.|[2411.10957v1](http://arxiv.org/abs/2411.10957v1)|null|
|**2024-11-17**|**A Topic-aware Comparable Corpus of Chinese Variations**|Da-Chen Lian et.al.|[2411.10955v1](http://arxiv.org/abs/2411.10955v1)|null|
|**2024-11-17**|**Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across Language Varieties**|Fahim Faisal et.al.|[2411.10954v1](http://arxiv.org/abs/2411.10954v1)|[link](https://github.com/ffaisal93/dialect_toxicity_llm_judge)|
|**2024-11-17**|**Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering**|Zeping Yu et.al.|[2411.10950v1](http://arxiv.org/abs/2411.10950v1)|[link](https://github.com/zepingyu0512/llava-mechanism)|
|**2024-11-17**|**Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry**|Wenjun Hou et.al.|[2411.10937v1](http://arxiv.org/abs/2411.10937v1)|null|
|**2024-11-17**|**Analyzing Pokémon and Mario Streamers' Twitch Chat with LLM-based User Embeddings**|Mika Hämäläinen et.al.|[2411.10934v1](http://arxiv.org/abs/2411.10934v1)|null|
|**2024-11-17**|**Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning**|Wenke Huang et.al.|[2411.10928v1](http://arxiv.org/abs/2411.10928v1)|null|
|**2024-11-17**|**Inter-linguistic Phonetic Composition (IPC): A Theoretical and Computational Approach to Enhance Second Language Pronunciation**|Jisang Park et.al.|[2411.10927v1](http://arxiv.org/abs/2411.10927v1)|null|
|**2024-11-17**|**Hyperspectral Imaging-Based Grain Quality Assessment With Limited Labelled Data**|Priyabrata Karmakar et.al.|[2411.10924v1](http://arxiv.org/abs/2411.10924v1)|null|
|**2024-11-17**|**LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection**|Danial Abshari et.al.|[2411.10918v1](http://arxiv.org/abs/2411.10918v1)|null|
|**2024-11-16**|**Bias in Large Language Models: Origin, Evaluation, and Mitigation**|Yufei Guo et.al.|[2411.10915v1](http://arxiv.org/abs/2411.10915v1)|null|
|**2024-11-16**|**BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment**|Sizhe Wang et.al.|[2411.10914v1](http://arxiv.org/abs/2411.10914v1)|null|

#### Abstracts
##### **Bi-Mamba: Towards Accurate 1-Bit State Space Models**
2411.11843v1 by Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen

The typical selective state-space model (SSM) of Mamba addresses several
limitations of Transformers, such as quadratic computational complexity with
sequence length and significant inference-time memory requirements due to the
key-value cache. However, the growing size of Mamba models continues to pose
training and deployment challenges and raises environmental concerns due to
considerable energy consumption. In this work, we introduce Bi-Mamba, a
scalable and powerful 1-bit Mamba architecture designed for more efficient
large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba
models are trained from scratch on data volume as regular LLM pertaining using
an autoregressive distillation loss. Extensive experimental results on language
modeling demonstrate that Bi-Mamba achieves performance comparable to its
full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than
post-training-binarization (PTB) Mamba baselines, while significantly reducing
memory footprint and energy consumption compared to the original Mamba model.
Our study pioneers a new linear computational complexity LLM framework under
low-bit representation and facilitates the future design of specialized
hardware tailored for efficient 1-bit Mamba-based LLMs.

摘要：典型的 Mamba 選擇性狀態空間模型 (SSM) 處理了 Transformer 的幾個限制，例如二次計算複雜度與序列長度和由於鍵值快取而導致的顯著推論時間記憶體需求。然而，Mamba 模型日益增長的規模持續對訓練和部署造成挑戰，並由於可觀的能耗而引發環境問題。在這項工作中，我們介紹了 Bi-Mamba，一種可擴充且強大的 1 位元 Mamba 架構，針對更有效率的大型語言模型而設計，具有 780M、1.3B 和 2.7B 等多種大小。Bi-Mamba 模型從頭開始訓練，資料量與使用自迴歸蒸餾損失相關的常規 LLM 相同。語言建模的廣泛實驗結果證明，與其全精度對應物（例如 FP16 或 BF16）相比，Bi-Mamba 達到了相當的效能，而且比訓練後二元化 (PTB) Mamba 基準的準確度高出許多，同時與原始 Mamba 模型相比，顯著減少了記憶體佔用空間和能耗。我們的研究開創了一個新的線性計算複雜度 LLM 框架，在低位元表示下，並促進了針對有效率的 1 位元 Mamba 基礎 LLM 而量身打造的專用硬體的未來設計。

##### **Tackling prediction tasks in relational databases with LLMs**
2411.11829v1 by Marek Wydmuch, Łukasz Borchmann, Filip Graliński

Though large language models (LLMs) have demonstrated exceptional performance
across numerous problems, their application to predictive tasks in relational
databases remains largely unexplored. In this work, we address the notion that
LLMs cannot yield satisfactory results on relational databases due to their
interconnected tables, complex relationships, and heterogeneous data types.
Using the recently introduced RelBench benchmark, we demonstrate that even a
straightforward application of LLMs achieves competitive performance on these
tasks. These findings establish LLMs as a promising new baseline for ML on
relational databases and encourage further research in this direction.

摘要：儘管大型語言模型（LLM）在許多問題上都展現出非凡的效能，但它們在關係資料庫中預測任務上的應用仍未廣泛探討。在這項研究中，我們探討了 LLM 無法在關係資料庫上產生令人滿意結果的觀念，原因是其相互連結的表格、複雜的關係和異質資料類型。使用最近推出的 RelBench 評量基準，我們證明即使是 LLM 的直接應用，也能在這些任務上達到有競爭力的效能。這些發現確立了 LLM 作為關係資料庫上機器學習有前途的新基準，並鼓勵朝此方向進行進一步的研究。

##### **LightFFDNets: Lightweight Convolutional Neural Networks for Rapid Facial Forgery Detection**
2411.11826v1 by Günel Jabbarlı, Murat Kurt

Accurate and fast recognition of forgeries is an issue of great importance in
the fields of artificial intelligence, image processing and object detection.
Recognition of forgeries of facial imagery is the process of classifying and
defining the faces in it by analyzing real-world facial images. This process is
usually accomplished by extracting features from an image, using classifier
algorithms, and correctly interpreting the results. Recognizing forgeries of
facial imagery correctly can encounter many different challenges. For example,
factors such as changing lighting conditions, viewing faces from different
angles can affect recognition performance, and background complexity and
perspective changes in facial images can make accurate recognition difficult.
Despite these difficulties, significant progress has been made in the field of
forgery detection. Deep learning algorithms, especially Convolutional Neural
Networks (CNNs), have significantly improved forgery detection performance.
  This study focuses on image processing-based forgery detection using
Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] data sets.
Both data sets consist of two classes containing real and fake facial images.
In our study, two lightweight deep learning models are proposed to conduct
forgery detection using these images. Additionally, 8 different pretrained CNN
architectures were tested on both data sets and the results were compared with
newly developed lightweight CNN models. It's shown that the proposed
lightweight deep learning models have minimum number of layers. It's also shown
that the proposed lightweight deep learning models detect forgeries of facial
imagery accurately, and computationally efficiently. Although the data set
consists only of face images, the developed models can also be used in other
two-class object recognition problems.

摘要：<paragraph>準確快速地辨識偽造品在人工智能、影像處理和物件偵測領域中是一個非常重要的議題。辨識臉部影像的偽造品是透過分析真實世界的臉部影像來分類和定義其中的臉孔。這個程序通常會從影像中萃取特徵、使用分類器演算法，並正確地詮釋結果。正確地辨識臉部影像的偽造品可能會遭遇許多不同的挑戰。例如，光線條件改變、從不同角度觀看臉孔等因素可能會影響辨識效能，而且背景複雜度和臉部影像中的透視變化可能會讓準確辨識變得困難。儘管有這些困難，偽造品偵測領域中已經有顯著的進展。深度學習演算法，特別是卷積神經網路 (CNN)，已經顯著地改善偽造品偵測效能。本研究專注於使用 Fake-Vs-Real-Faces (Hard) [10] 和 140k Real and Fake Faces [61] 資料集的基於影像處理的偽造品偵測。兩個資料集都包含兩個類別，其中包含真實和假的臉部影像。在我們的研究中，提出了兩個輕量級深度學習模型來使用這些影像進行偽造品偵測。此外，在兩個資料集上測試了 8 種不同的預訓練 CNN 架構，並將結果與新開發的輕量級 CNN 模型進行比較。結果顯示，所提出的輕量級深度學習模型具有最少的層數。結果也顯示，所提出的輕量級深度學習模型可以準確地偵測臉部影像的偽造品，而且計算效率高。儘管資料集僅包含臉部影像，但已開發的模型也可以用於其他兩類別的物件辨識問題。</paragraph>

##### **Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion**
2411.11799v1 by Meng Zhou, Yuxuan Zhang, Xiaolan Xu, Jiayi Wang, Farzad Khalvati

Multimodal medical image fusion is a crucial task that combines complementary
information from different imaging modalities into a unified representation,
thereby enhancing diagnostic accuracy and treatment planning. While deep
learning methods, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly advanced fusion performance, some of the
existing CNN-based methods fall short in capturing fine-grained multiscale and
edge features, leading to suboptimal feature integration. Transformer-based
models, on the other hand, are computationally intensive in both the training
and fusion stages, making them impractical for real-time clinical use.
Moreover, the clinical application of fused images remains unexplored. In this
paper, we propose a novel CNN-based architecture that addresses these
limitations by introducing a Dilated Residual Attention Network Module for
effective multiscale feature extraction, coupled with a gradient operator to
enhance edge detail learning. To ensure fast and efficient fusion, we present a
parameter-free fusion strategy based on the weighted nuclear norm of softmax,
which requires no additional computations during training or inference.
Extensive experiments, including a downstream brain tumor classification task,
demonstrate that our approach outperforms various baseline methods in terms of
visual quality, texture preservation, and fusion speed, making it a possible
practical solution for real-world clinical applications. The code will be
released at https://github.com/simonZhou86/en_dran.

摘要：多模态医学图像融合是一项至关重要的任务，它将来自不同成像方式的互补信息融合到一个统一的表示中，从而提高诊断准确性和治疗计划。虽然深度学习方法，尤其是卷积神经网络 (CNN) 和 Transformer，已经显著提升了融合性能，但一些现有的基于 CNN 的方法在捕捉细粒度多尺度和边缘特征方面存在不足，导致次优特征集成。另一方面，基于 Transformer 的模型在训练和融合阶段计算量很大，这使得它们不适用于实时临床使用。此外，融合图像的临床应用仍未得到探索。在本文中，我们提出了一种新颖的基于 CNN 的架构，通过引入膨胀残差注意力网络模块来解决这些限制，以进行有效的多分辨率特征提取，并结合梯度算子来增强边缘细节学习。为了确保快速而高效的融合，我们提出了一种基于 softmax 的加权核范数的参数化融合策略，它在训练或推理过程中不需要额外的计算。广泛的实验，包括下游脑肿瘤分类任务，表明我们的方法在视觉质量、纹理保留和融合速度方面优于各种基准方法，使其成为现实世界临床应用的可能实用解决方案。代码将在 https://github.com/simonZhou86/en_dran 发布。

##### **Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods**
2411.11795v1 by Egor Kovalev, Georgii Bychkov, Khaled Abud, Aleksandr Gushchin, Anna Chistyakova, Sergey Lavrushkin, Dmitriy Vatolin, Anastasia Antsiferova

Adversarial robustness of neural networks is an increasingly important area
of research, combining studies on computer vision models, large language models
(LLMs), and others. With the release of JPEG AI - the first standard for
end-to-end neural image compression (NIC) methods - the question of its
robustness has become critically significant. JPEG AI is among the first
international, real-world applications of neural-network-based models to be
embedded in consumer devices. However, research on NIC robustness has been
limited to open-source codecs and a narrow range of attacks. This paper
proposes a new methodology for measuring NIC robustness to adversarial attacks.
We present the first large-scale evaluation of JPEG AI's robustness, comparing
it with other NIC models. Our evaluation results and code are publicly
available online (link is hidden for a blind review).

摘要：神經網路對抗攻擊的穩健性是一個日益重要的研究領域，結合了電腦視覺模型、大型語言模型 (LLM) 等方面的研究。隨著 JPEG AI 的發布，JPEG AI 是第一個端到端神經影像壓縮 (NIC) 方法標準，其穩健性問題變得至關重要。JPEG AI 是第一批將基於神經網路的模型應用於國際現實世界並嵌入消費者裝置的應用程式。然而，關於 NIC 穩健性的研究僅限於開源編解碼器和範圍狹窄的攻擊。本文提出了一種新的方法來衡量 NIC 對抗攻擊的穩健性。我們展示了 JPEG AI 穩健性的第一次大規模評估，並將其與其他 NIC 模型進行比較。我們的評估結果和程式碼已公開在網路上（連結因盲審而隱藏）。

##### **Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care**
2411.11774v1 by Jeffrey N. Clark, Matthew Wragg, Emily Nielsen, Miquel Perello-Nieto, Nawid Keshtmand, Michael Ambler, Shiv Sharma, Christopher P. Bourdeaux, Amberly Brigden, Raul Santos-Rodriguez

There is a growing need to understand how digital systems can support
clinical decision-making, particularly as artificial intelligence (AI) models
become increasingly complex and less human-interpretable. This complexity
raises concerns about trustworthiness, impacting safe and effective adoption of
such technologies. Improved understanding of decision-making processes and
requirements for explanations coming from decision support tools is a vital
component in providing effective explainable solutions. This is particularly
relevant in the data-intensive, fast-paced environments of intensive care units
(ICUs). To explore these issues, group interviews were conducted with seven ICU
clinicians, representing various roles and experience levels. Thematic analysis
revealed three core themes: (T1) ICU decision-making relies on a wide range of
factors, (T2) the complexity of patient state is challenging for shared
decision-making, and (T3) requirements and capabilities of AI decision support
systems. We include design recommendations from clinical input, providing
insights to inform future AI systems for intensive care.

摘要：隨著人工智慧 (AI) 模型變得越來越複雜，且越來越難以被人理解，了解數位系統如何支援臨床決策的需求也日益增加。這種複雜性引發了對可信度的疑慮，影響了此類技術的安全且有效採用。改善對決策制定流程的理解，以及對決策支援工具所提供說明的要求，是提供有效可解釋解決方案的重要組成部分。這在資料密集、快節奏的加護病房 (ICU) 環境中特別相關。為了探討這些問題，對七位 ICU 臨床醫師進行了小組訪談，這些醫師代表了不同的角色和經驗層級。主題分析揭露了三個核心主題：(T1) ICU 決策制定依賴於廣泛的因素，(T2) 病患狀態的複雜性對共同決策制定構成挑戰，以及 (T3) AI 決策支援系統的要求和能力。我們納入了臨床輸入的設計建議，提供見解以提供資訊給未來用於加護的 AI 系統。

##### **CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task**
2411.11770v1 by Zishuo Feng, Feng Cao

The task of converting Hanyu Pinyin abbreviations to Chinese characters
represents a significant branch within the domain of Chinese Spelling
Correction (CSC). This task is typically one of text-length alignment, however,
due to the limited informational content in pinyin abbreviations, achieving
accurate conversion is challenging. In this paper, we propose CNMBert which
stands for zh-CN Pinyin Multi-mask Bert Model as a solution to this issue.
CNMBert surpasses few-shot GPT models, achieving a 59.63% MRR on a
10,424-sample Hanyu Pinyin abbreviation test dataset.

摘要：將漢語拼音縮寫轉換為中文字元的任務代表了中文拼寫糾正 (CSC) 領域中的重要分支。然而，由於拼音縮寫中資訊含量有限，此任務通常是文本長度對齊，因此要達到準確轉換具有挑戰性。在本文中，我們提出 CNMBert，代表 zh-CN Pinyin Multi-mask Bert 模型，作為解決此問題的方法。CNMBert 超越了少次數 GPT 模型，在 10,424 個樣本的漢語拼音縮寫測試資料集上達到了 59.63% 的 MRR。

##### **The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning**
2411.11758v1 by Longju Bai, Angana Borah, Oana Ignat, Rada Mihalcea

Large Multimodal Models (LMMs) exhibit impressive performance across various
multimodal tasks. However, their effectiveness in cross-cultural contexts
remains limited due to the predominantly Western-centric nature of most data
and models. Conversely, multi-agent models have shown significant capability in
solving complex tasks. Our study evaluates the collective performance of LMMs
in a multi-agent interaction setting for the novel task of cultural image
captioning. Our contributions are as follows: (1) We introduce MosAIC, a
Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs
with distinct cultural personas; (2) We provide a dataset of culturally
enriched image captions in English for images from China, India, and Romania
across three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable
metric for evaluating cultural information within image captions; and (4) We
show that the multi-agent interaction outperforms single-agent models across
different metrics, and offer valuable insights for future research. Our dataset
and models can be accessed at https://github.com/MichiganNLP/MosAIC.

摘要：大型多模态模型 (LMM) 在各种多模态任务中表现出色。然而，由于大多数数据和模型本质上以西方为中心，它们在跨文化环境中的有效性仍然受到限制。相反，多主体模型在解决复杂任务方面显示出显著的能力。我们的研究评估了 LMM 在多主体交互设置中对文化图像字幕的新颖任务的集体表现。我们的贡献如下：(1) 我们引入了 MosAIC，一个多主体框架，使用具有不同文化角色的 LMM 增强跨文化图像字幕；(2) 我们提供了一个数据集，其中包含来自中国、印度和罗马尼亚的图像的文化丰富的图像字幕，跨越三个数据集：GeoDE、GD-VCR、CVQA；(3) 我们提出了一个文化适应性指标，用于评估图像字幕中的文化信息；(4) 我们表明，多主体交互在不同指标上优于单主体模型，并为未来的研究提供了宝贵的见解。我们的数据集和模型可在 https://github.com/MichiganNLP/MosAIC 获得。

##### **QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou**
2411.11739v1 by Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, Changqing Qiu, Jiaqi Zhang, Xu Zhang, Zhiheng Yan, Jingming Zhang, Simin Zhang, Mingxing Wen, Zhaojie Liu, Kun Gai, Guorui Zhou

In recent years, with the significant evolution of multi-modal large models,
many recommender researchers realized the potential of multi-modal information
for user interest modeling. In industry, a wide-used modeling architecture is a
cascading paradigm: (1) first pre-training a multi-modal model to provide
omnipotent representations for downstream services; (2) The downstream
recommendation model takes the multi-modal representation as additional input
to fit real user-item behaviours. Although such paradigm achieves remarkable
improvements, however, there still exist two problems that limit model
performance: (1) Representation Unmatching: The pre-trained multi-modal model
is always supervised by the classic NLP/CV tasks, while the recommendation
models are supervised by real user-item interaction. As a result, the two
fundamentally different tasks' goals were relatively separate, and there was a
lack of consistent objective on their representations; (2) Representation
Unlearning: The generated multi-modal representations are always stored in
cache store and serve as extra fixed input of recommendation model, thus could
not be updated by recommendation model gradient, further unfriendly for
downstream training. Inspired by the two difficulties challenges in downstream
tasks usage, we introduce a quantitative multi-modal framework to customize the
specialized and trainable multi-modal information for different downstream
models.

摘要：近年來，隨著多模態大型模型的顯著演進，
許多推薦研究者體認到多模態資訊在使用者興趣建模上的潛力。在產業中，一個廣為使用的建模架構為
串接式範例：(1) 首先預訓練一個多模態模型，以提供下游服務的全能表示；(2) 下游推薦模型將多模態表示作為額外輸入，以符合真實使用者-項目行為。儘管此範例獲得顯著的進步，然而，仍存在兩個限制模型效能的問題：(1) 表示不匹配：預訓練的多模態模型總是受到傳統 NLP/CV 任務的監督，而推薦模型則受到真實使用者-項目互動的監督。因此，這兩個根本不同的任務目標相對獨立，並且它們的表示缺乏一致的目標；(2) 表示遺忘：產生的多模態表示總是儲存在快取儲存區中，並作為推薦模型的額外固定輸入，因此無法由推薦模型梯度更新，進一步不利於下游訓練。受到下游任務使用中的兩個困難挑戰啟發，我們引入一個量化的多模態框架，以自訂特定且可訓練的多模態資訊，供不同的下游模型使用。

##### **WoodYOLO: A Novel Object Detector for Wood Species Detection in Microscopic Images**
2411.11738v1 by Lars Nieradzik, Henrike Stephani, Jördis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Stephanie Wrage, Janis Keuper

Wood species identification plays a crucial role in various industries, from
ensuring the legality of timber products to advancing ecological conservation
efforts. This paper introduces WoodYOLO, a novel object detection algorithm
specifically designed for microscopic wood fiber analysis. Our approach adapts
the YOLO architecture to address the challenges posed by large, high-resolution
microscopy images and the need for high recall in localization of the cell type
of interest (vessel elements). Our results show that WoodYOLO significantly
outperforms state-of-the-art models, achieving performance gains of 12.9% and
6.5% in F2 score over YOLOv10 and YOLOv7, respectively. This improvement in
automated wood cell type localization capabilities contributes to enhancing
regulatory compliance, supporting sustainable forestry practices, and promoting
biodiversity conservation efforts globally.

摘要：木材種類辨識在各產業中扮演著至關重要的角色，從確保木材產品的合法性到促進生態保育工作。本文介紹 WoodYOLO，一種專為微觀木材纖維分析而設計的新型物件偵測演算法。我們的做法是調整 YOLO 架構以應對大型高解析度顯微影像所帶來的挑戰，以及在定位目標細胞類型（管胞元素）時對高召回率的需求。我們的結果顯示，WoodYOLO 的表現明顯優於現有技術模型，在 F2 分數上分別較 YOLOv10 和 YOLOv7 提升了 12.9% 和 6.5%。自動化木材細胞類型定位能力的提升有助於加強法規遵循、支持永續林業實務，並在全球推動生物多樣性保育工作。

##### **Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment**
2411.11731v1 by Allison Huang, Yulu Niki Pi, Carlos Mougan

We explore how large language models (LLMs) can be influenced by prompting
them to alter their initial decisions and align them with established ethical
frameworks. Our study is based on two experiments designed to assess the
susceptibility of LLMs to moral persuasion. In the first experiment, we examine
the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally
ambiguous scenarios and observing how a Persuader Agent attempts to modify the
Base Agent's initial decisions. The second experiment evaluates the
susceptibility of LLMs to align with predefined ethical frameworks by prompting
them to adopt specific value alignments rooted in established philosophical
theories. The results demonstrate that LLMs can indeed be persuaded in morally
charged scenarios, with the success of persuasion depending on factors such as
the model used, the complexity of the scenario, and the conversation length.
Notably, LLMs of distinct sizes but from the same company produced markedly
different outcomes, highlighting the variability in their susceptibility to
ethical persuasion.

摘要：我們探討大型語言模型 (LLM) 如何透過提示來改變其最初的決定，並使其與既定的道德框架保持一致。我們的研究基於兩個實驗，旨在評估 LLM 對道德說服的敏感性。在第一個實驗中，我們透過評估基礎代理人 LLM 在道德模糊情境中的表現，並觀察說服者代理人如何嘗試修改基礎代理人的初始決定，來探討對道德模糊性的敏感性。第二個實驗評估了 LLM 與預定義道德框架保持一致的敏感性，方法是提示他們採用根植於既定哲學理論的特定價值觀。結果表明，LLM 確實可以在道德情境中受到說服，說服的成功取決於模型、情境的複雜性和對話長度等因素。值得注意的是，來自同一家公司的不同規模的 LLM 產生了顯著不同的結果，突顯了它們對道德說服的敏感性存在差異。

##### **Lifted Model Construction without Normalisation: A Vectorised Approach to Exploit Symmetries in Factor Graphs**
2411.11730v1 by Malte Luttermann, Ralf Möller, Marcel Gehrke

Lifted probabilistic inference exploits symmetries in a probabilistic model
to allow for tractable probabilistic inference with respect to domain sizes of
logical variables. We found that the current state-of-the-art algorithm to
construct a lifted representation in form of a parametric factor graph misses
symmetries between factors that are exchangeable but scaled differently,
thereby leading to a less compact representation. In this paper, we propose a
generalisation of the advanced colour passing (ACP) algorithm, which is the
state of the art to construct a parametric factor graph. Our proposed algorithm
allows for potentials of factors to be scaled arbitrarily and efficiently
detects more symmetries than the original ACP algorithm. By detecting strictly
more symmetries than ACP, our algorithm significantly reduces online query
times for probabilistic inference when the resulting model is applied, which we
also confirm in our experiments.

摘要：提升的概率推理利用概率模型中的对称性，允许对逻辑变量的域大小进行易处理的概率推理。我们发现当前最先进的算法以参数因子图的形式构建提升表示，但它错失了可交换但缩放不同的因子之间的对称性，从而导致表示不够紧凑。在本文中，我们提出了高级颜色传递 (ACP) 算法的概括，这是构建参数因子图的最新技术。我们提出的算法允许因子电势任意缩放，并且比原始 ACP 算法更有效地检测更多对称性。通过检测比 ACP 更严格的对称性，我们的算法在应用所得模型时显著减少了概率推理的在线查询时间，这一点我们也在实验中得到了证实。

##### **Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation**
2411.11714v1 by Mingchao Qi, Yuanjin Li, Xing Liu, Zhengxiong Liu, Panfeng Huang

Deploying robots in open-world environments involves complex tasks
characterized by long sequences and rich interactions, necessitating efficient
transfer of robotic skills across diverse and complex scenarios. To address
this challenge, we propose a skill library framework based on knowledge graphs,
which endows robots with high-level skill awareness and spatial semantic
understanding. The framework hierarchically organizes operational knowledge by
constructing a "task graph" and a "scene graph" to represent task and scene
semantic information, respectively. We introduce a "state graph" to facilitate
interaction between high-level task planning and low-level scene information.
Furthermore, we propose a hierarchical transfer framework for operational
skills. At the task level, the framework integrates contextual learning and
chain-of-thought prompting within a four-stage prompt paradigm, leveraging
large language models' (LLMs) reasoning and generalization capabilities to
achieve task-level subtask sequence transfer. At the motion level, an adaptive
trajectory transfer method is developed using the A* algorithm and the skill
library, enabling motion-level adaptive trajectory transfer. At the physical
level, we introduce an adaptive contour extraction and posture perception
method based on tactile perception. This method dynamically obtains
high-precision contour and posture information from visual-tactile texture data
and adjusts transferred skills, such as contact positions and postures, to
ensure effectiveness in new environments. Experimental results validate the
effectiveness of the proposed methods. Project
website:https://github.com/MingchaoQi/skill_transfer

摘要：<paragraph>在开放世界环境中部署机器人涉及复杂的任务，其特点是序列长、交互丰富，需要在不同且复杂的场景中高效地转移机器人技能。为了应对这一挑战，我们提出一个基于知识图谱的技能库框架，它赋予机器人高级技能意识和空间语义理解。该框架通过构建“任务图”和“场景图”来分层组织操作知识，分别表示任务和场景语义信息。我们引入一个“状态图”来促进高级任务规划和低级场景信息之间的交互。此外，我们提出了一个操作技能的分层转移框架。在任务层面，该框架在一个四阶段提示范式中集成了上下文学习和思想链提示，利用大语言模型 (LLM) 的推理和泛化能力来实现任务级子任务序列转移。在运动层面，使用 A* 算法和技能库开发了一种自适应轨迹转移方法，实现运动级自适应轨迹转移。在物理层面，我们引入了一种基于触觉感知的自适应轮廓提取和姿态感知方法。该方法从视觉触觉纹理数据中动态获取高精度的轮廓和姿态信息，并调整转移的技能，例如接触位置和姿态，以确保在新的环境中有效。实验结果验证了所提出方法的有效性。项目网站：https://github.com/MingchaoQi/skill_transfer</paragraph>

##### **FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models**
2411.11707v1 by Tao Fan, Yan Kang, Guoqiang Ma, Lixin Fan, Kai Chen, Qiang Yang

By adapting Large Language Models (LLMs) to domain-specific tasks or
enriching them with domain-specific knowledge, we can fully harness the
capabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous
mutual enhancement between the server's LLM and the downstream clients' Small
Language Models (SLMs). To address this, we propose FedCoLLM, a novel and
parameter-efficient federated framework designed for co-tuning LLMs and SLMs.
This approach is aimed at adaptively transferring server-side LLMs knowledge to
clients' SLMs while simultaneously enriching the LLMs with domain insights from
the clients. To accomplish this, FedCoLLM utilizes lightweight adapters in
conjunction with SLMs, facilitating knowledge exchange between server and
clients in a manner that respects data privacy while also minimizing
computational and communication overhead. Our evaluation of FedCoLLM, utilizing
various public LLMs and SLMs across a range of NLP text generation tasks,
reveals that the performance of clients' SLMs experiences notable improvements
with the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM
achieves comparable performance to that obtained through direct fine-tuning on
clients' data.

摘要：透過將大型語言模型 (LLM) 調整到特定領域任務或豐富它們的特定領域知識，我們可以充分利用 LLM 的功能。儘管如此，伺服器的 LLM 和下游客戶端的小型語言模型 (SLM) 之間，在同時相互增強方面仍存在差距。為了解決這個問題，我們提出了 FedCoLLM，一個新穎且參數效率高的聯合框架，專門用於共同調整 LLM 和 SLM。此方法旨在自適應地將伺服器端的 LLM 知識傳輸到客戶端的 SLM，同時豐富 LLM 對客戶端領域見解。為了達成此目的，FedCoLLM 利用輕量級適配器與 SLM 結合，促進伺服器和客戶端之間的知識交流，同時尊重資料隱私，並將運算和通訊開銷降到最低。我們利用各種公共 LLM 和 SLM 評估 FedCoLLM，涵蓋一系列 NLP 文字生成任務，結果顯示客戶端的 SLM 在 LLM 的協助下，其效能有顯著提升。同時，透過 FedCoLLM 增強的 LLM，可達成與在客戶端資料上進行直接微調所獲得的效能相當。

##### **MC-LLaVA: Multi-Concept Personalized Vision-Language Model**
2411.11706v1 by Ruichuan An, Sihan Yang, Ming Lu, Kai Zeng, Yulin Luo, Ying Chen, Jiajun Cao, Hao Liang, Qi She, Shanghang Zhang, Wentao Zhang

Current vision-language models (VLMs) show exceptional abilities across
diverse tasks including visual question answering. To enhance user experience
in practical applications, recent studies investigate VLM personalization to
understand user-provided concepts. However, existing studies mainly focus on
single-concept personalization, neglecting the existence and interplay of
multiple concepts, which limits the real-world applicability of personalized
VLMs. In this paper, we propose the first multi-concept personalization method
named MC-LLaVA along with a high-quality multi-concept personalization dataset.
Specifically, MC-LLaVA uses a joint training strategy incorporating multiple
concepts in a single training step, allowing VLMs to perform accurately in
multi-concept personalization. To reduce the cost of joint training, MC-LLaVA
leverages visual token information for concept token initialization, yielding
improved concept representation and accelerating joint training. To advance
multi-concept personalization research, we further contribute a high-quality
dataset. We carefully collect images from various movies that contain multiple
characters and manually generate the multi-concept question-answer samples. Our
dataset features diverse movie types and question-answer types. We conduct
comprehensive qualitative and quantitative experiments to demonstrate that
MC-LLaVA can achieve impressive multi-concept personalized responses, paving
the way for VLMs to become better user-specific assistants. The code and
dataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.

摘要：目前的視覺語言模型 (VLM) 在包括視覺問答在內的各種任務中展現出非凡的能力。為了提升實際應用中的使用者體驗，最近的研究探討了 VLM 個人化，以了解使用者提供的概念。然而，現有的研究主要集中在單一概念個人化，忽略了多個概念的存在和交互作用，這限制了個人化 VLM 的實際應用性。在本文中，我們提出了第一個名為 MC-LLaVA 的多概念個人化方法，以及一個高品質的多概念個人化資料集。具體來說，MC-LLaVA 使用一個聯合訓練策略，在單一訓練步驟中納入多個概念，讓 VLM 能夠在多概念個人化中準確執行。為了降低聯合訓練的成本，MC-LLaVA 利用視覺標記資訊進行概念標記初始化，產生改進的概念表示並加速聯合訓練。為了推進多概念個人化研究，我們進一步貢獻了一個高品質的資料集。我們仔細從包含多個角色的各種電影中收集影像，並手動產生多概念問答範例。我們的資料集具有多樣化的電影類型和問答類型。我們進行了全面的定性和定量實驗，以證明 MC-LLaVA 能夠實現令人印象深刻的多概念個人化回應，為 VLM 成為更好的使用者特定助理鋪平了道路。程式碼和資料集將公開於 https://github.com/arctanxarc/MC-LLaVA。

##### **Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search**
2411.11694v1 by Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, Ji-Rong Wen

Recently, test-time scaling has garnered significant attention from the
research community, largely due to the substantial advancements of the o1 model
released by OpenAI. By allocating more computational resources during the
inference phase, large language models~(LLMs) can extensively explore the
solution space by generating more thought tokens or diverse solutions, thereby
producing more accurate responses. However, developing an o1-like reasoning
approach is challenging, and researchers have been making various attempts to
advance this open area of research. In this paper, we present a preliminary
exploration into enhancing the reasoning abilities of LLMs through
reward-guided tree search algorithms. This framework is implemented by
integrating the policy model, reward model, and search algorithm. It is
primarily constructed around a tree search algorithm, where the policy model
navigates a dynamically expanding tree guided by a specially trained reward
model. We thoroughly explore various design considerations necessary for
implementing this framework and provide a detailed report of the technical
aspects. To assess the effectiveness of our approach, we focus on mathematical
reasoning tasks and conduct extensive evaluations on four challenging datasets,
significantly enhancing the reasoning abilities of LLMs.

摘要：最近，测试时间缩放引起了研究界的极大关注，这在很大程度上归功于 OpenAI 发布的 o1 模型的重大进步。通过在推理阶段分配更多计算资源，大型语言模型 (LLM) 可以通过生成更多思想标记或不同的解决方案来广泛探索解决方案空间，从而产生更准确的响应。然而，开发类似 o1 的推理方法具有挑战性，研究人员一直在尝试推进这一开放的研究领域。在本文中，我们提出了通过奖励引导树搜索算法来增强 LLM 推理能力的初步探索。该框架通过集成策略模型、奖励模型和搜索算法来实现。它主要围绕树搜索算法构建，其中策略模型在经过专门训练的奖励模型的指导下导航动态扩展的树。我们彻底探索了实现此框架所需的不同设计考虑因素，并提供了技术方面的详细报告。为了评估我们方法的有效性，我们专注于数学推理任务，并在四个具有挑战性的数据集上进行广泛的评估，显著增强了 LLM 的推理能力。

##### **Conceptwm: A Diffusion Model Watermark for Concept Protection**
2411.11688v1 by Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu

The personalization techniques of diffusion models succeed in generating
specific concepts but also pose threats to copyright protection and illegal
use. Model Watermarking is an effective method to prevent the unauthorized use
of subject-driven or style-driven image generation, safeguarding concept
copyrights. However, under the goal of concept-oriented protection, current
watermarking schemes typically add watermarks to all images rather than
applying them in a refined manner targeted at specific concepts. Additionally,
the personalization techniques of diffusion models can easily remove
watermarks. Existing watermarking methods struggle to achieve fine-grained
watermark embedding with a few images of specific concept and prevent removal
of watermarks through personalized fine-tuning. Therefore, we introduce a novel
concept-oriented watermarking framework that seamlessly embeds imperceptible
watermarks into the concept of diffusion models. We conduct extensive
experiments and ablation studies to verify our framework. Our code is available
at https://anonymous.4open.science/r/Conceptwm-4EB3/.

摘要：擴散模型的個人化技術成功產生了特定概念，但也對版權保護和非法使用構成威脅。模型浮水印是一種預防未經授權使用主題驅動或風格驅動的影像生成，以保護概念版權的有效方法。然而，在以概念為導向的保護目標下，目前的浮水印方案通常會將浮水印加入所有影像，而不是以精緻的方式針對特定概念進行套用。此外，擴散模型的個人化技術可以輕鬆地移除浮水印。現有的浮水印方法難以實現使用少量特定概念影像的細粒度浮水印嵌入，並防止透過個人化微調移除浮水印。因此，我們引入一個新的以概念為導向的浮水印架構，可將難以察覺的浮水印無縫嵌入擴散模型的概念中。我們進行了廣泛的實驗和消融研究，以驗證我們的架構。我們的程式碼可在 https://anonymous.4open.science/r/Conceptwm-4EB3/ 取得。

##### **TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World**
2411.11683v1 by Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Peijin Guo, Yichen Wang, Wei Wan, Aishan Liu, Leo Yu Zhang

Robotic manipulation refers to the autonomous handling and interaction of
robots with objects using advanced techniques in robotics and artificial
intelligence. The advent of powerful tools such as large language models (LLMs)
and large vision-language models (LVLMs) has significantly enhanced the
capabilities of these robots in environmental perception and decision-making.
However, the introduction of these intelligent agents has led to security
threats such as jailbreak attacks and adversarial attacks.
  In this research, we take a further step by proposing a backdoor attack
specifically targeting robotic manipulation and, for the first time,
implementing backdoor attack in the physical world. By embedding a backdoor
visual language model into the visual perception module within the robotic
system, we successfully mislead the robotic arm's operation in the physical
world, given the presence of common items as triggers. Experimental evaluations
in the physical world demonstrate the effectiveness of the proposed backdoor
attack.

摘要：機器人操作是指機器人使用機器人和人工智慧的先進技術，自主處理和與物體互動。強大工具的出現，例如大型語言模型 (LLM) 和大型視覺語言模型 (LVLMs)，已顯著增強了這些機器人在環境感知和決策制定方面的能力。然而，這些智慧型代理的引入導致了安全威脅，例如越獄攻擊和對抗攻擊。在這個研究中，我們進一步提出了一種特別針對機器人操作的後門攻擊，並首次在物理世界中實施後門攻擊。透過將後門視覺語言模型嵌入機器人系統中的視覺感知模組，我們成功地誤導了機器人手臂在物理世界中的操作，因為存在著作為觸發器的常見物品。在物理世界中的實驗評估證明了所提出的後門攻擊的有效性。

##### **PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment**
2411.11681v1 by Jiawei Li, Xinyue Liang, Yizhe Yang, Chong Feng, Yang Gao

Process supervision enhances the performance of large language models in
reasoning tasks by providing feedback at each step of chain-of-thought
reasoning. However, due to the lack of effective process supervision methods,
even advanced large language models are prone to logical errors and redundant
reasoning. We claim that the effectiveness of process supervision significantly
depends on both the accuracy and the length of reasoning chains. Moreover, we
identify that these factors exhibit a nonlinear relationship with the overall
reward score of the reasoning process. Inspired by these insights, we propose a
novel process supervision paradigm, PSPO*, which systematically outlines the
workflow from reward model training to policy optimization, and highlights the
importance of nonlinear rewards in process supervision. Based on PSPO*, we
develop the PSPO-WRS, which considers the number of reasoning steps in
determining reward scores and utilizes an adjusted Weibull distribution for
nonlinear reward shaping. Experimental results on six mathematical reasoning
datasets demonstrate that PSPO-WRS consistently outperforms current mainstream
models.

摘要：流程監督透過在思考鏈條推理的每一步提供回饋，增強大型語言模型在推理任務中的表現。然而，由於缺乏有效的流程監督方法，即使是先進的大型語言模型也容易出現邏輯錯誤和冗餘推理。我們主張流程監督的有效性在很大程度上取決於推理鏈條的準確性和長度。此外，我們發現這些因素與推理過程的整體獎勵分數呈現非線性關係。受到這些見解的啟發，我們提出了一個新穎的流程監督範例 PSPO*，它系統性地概述了從獎勵模型訓練到策略優化的工作流程，並強調了非線性獎勵在流程監督中的重要性。基於 PSPO*，我們開發了 PSPO-WRS，它在確定獎勵分數時考慮了推理步驟的數量，並利用調整過的威布爾分佈進行非線性獎勵調整。在六個數學推理資料集上的實驗結果表明，PSPO-WRS 持續優於當前的主流模型。

##### **Artificial Scientific Discovery**
2411.11672v1 by Antonio Norelli

Rooted in the explosion of deep learning over the past decade, this thesis
spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts
needed to realize the vision of an artificial scientist: a machine with the
capacity to autonomously generate original research and contribute to the
expansion of human knowledge. The investigation begins with {\sc Olivaw}, an
AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is
unable to communicate it. This realization leads to the development of the
Explanatory Learning (EL) framework, a formalization of the problem faced by a
scientist when trying to explain a new phenomenon to their peers. The effective
EL prescriptions allow us to crack Zendo, a board game simulating the
scientific endeavor. This success comes with a fundamental insight: an
artificial scientist must develop its own interpretation of the language used
to explain its findings. This perspective then leads us to see modern
multimodal models as interpreters, and to devise a new way to build
interpretable and cost-effective CLIP-like models: by coupling two unimodal
models using little multimodal data and no further training. Finally, we
discuss what ChatGPT and its siblings are still missing to become artificial
scientists, and introduce Odeen, a benchmark about interpreting explanations
that sees LLMs going no further than random chance while being instead fully
solved by humans.

摘要：植根於過去十年深度學習的爆炸性發展，本論文從 AlphaGo 跨越到 ChatGPT，以實證檢驗實現人工科學家願景所需的基本概念：具備自主產生原始研究和貢獻於人類知識擴展能力的機器。這項研究始於 {\sc Olivaw}，一個類似 AlphaGo Zero 的代理，它從頭開始發現奧賽羅知識，但無法傳達它。這個發現導致了解釋性學習 (EL) 框架的發展，這是科學家在嘗試向同儕解釋新現象時所面臨問題的形式化。有效的 EL 處方讓我們破解了 Zendo，一個模擬科學努力的棋盤遊戲。這個成功伴隨著一個基本的見解：人工科學家必須發展它自己對用於解釋其發現的語言的詮釋。這個觀點讓我們將現代多模態模型視為詮釋器，並設計出一種建立可解釋且具成本效益的類似 CLIP 的模型的新方法：使用少量多模態資料和不進一步訓練來結合兩個單模態模型。最後，我們討論了 ChatGPT 和它的兄弟們成為人工科學家還缺少什麼，並介紹了 Odeen，一個關於解釋說明的基準，它看到 LLM 沒有比隨機機會走得更遠，而人類卻完全解決了它。

##### **Dissecting Misalignment of Multimodal Large Language Models via Influence Function**
2411.11667v1 by Lijie Hu, Chenyang Ren, Huanyi Xie, Khouloud Saadi, Shu Yang, Jingfeng Zhang, Di Wang

Multi-modal Large Language models (MLLMs) are always trained on data from
diverse and unreliable sources, which may contain misaligned or mislabeled
text-image pairs. This frequently causes robustness issues and hallucinations,
leading to performance degradation. Data valuation is an efficient way to
detect and trace these misalignments. Nevertheless, existing methods are
computationally expensive for MLLMs. While computationally efficient, the
classical influence functions are inadequate for contrastive learning models
because they were originally designed for pointwise loss. Additionally,
contrastive learning involves minimizing the distance between the modalities of
positive samples and maximizing the distance between the modalities of negative
samples. This requires us to evaluate the influence of samples from both
perspectives. To tackle these challenges, we introduce the Extended Influence
Function for Contrastive Loss (ECIF), an influence function crafted for
contrastive loss. ECIF considers both positive and negative samples and
provides a closed-form approximation of contrastive learning models,
eliminating the need for retraining. Building upon ECIF, we develop a series of
algorithms for data evaluation in MLLM, misalignment detection, and
misprediction trace-back tasks. Experimental results demonstrate our ECIF
advances the transparency and interpretability of MLLMs by offering a more
accurate assessment of data impact and model alignment compared to traditional
baseline methods.

摘要：多模态大语言模型 (MLLM) 总是针对来自不同且不可靠来源的数据进行训练，其中可能包含未对齐或标记错误的文本图像对。这经常导致鲁棒性问题和幻觉，从而导致性能下降。数据评估是一种检测和追踪这些未对齐的有效方法。然而，现有方法对 MLLM 来说计算成本很高。虽然计算效率高，但经典影响函数不适用于对比学习模型，因为它们最初是为逐点损失设计的。此外，对比学习涉及最小化正样本模态之间的距离，并最大化负样本模态之间的距离。这要求我们从两个角度评估样本的影响。为了应对这些挑战，我们引入了对比损失的扩展影响函数 (ECIF)，这是一种专门为对比损失设计的的影响函数。ECIF 考虑正负样本，并提供对比学习模型的闭式近似，无需重新训练。在 ECIF 的基础上，我们开发了一系列用于 MLLM 中的数据评估、未对齐检测和错误预测回溯任务的算法。实验结果表明，与传统的基线方法相比，我们的 ECIF 通过提供对数据影响和模型对齐的更准确评估，提高了 MLLM 的透明度和可解释性。

##### **No-regret Exploration in Shuffle Private Reinforcement Learning**
2411.11647v1 by Shaojie Bai, Mohammad Sadegh Talebi, Chengcheng Zhao, Peng Cheng, Jiming Chen

Differential privacy (DP) has recently been introduced into episodic
reinforcement learning (RL) to formally address user privacy concerns in
personalized services. Previous work mainly focuses on two trust models of DP:
the central model, where a central agent is responsible for protecting users'
sensitive data, and the (stronger) local model, where the protection occurs
directly on the user side. However, they either require a trusted central agent
or incur a significantly higher privacy cost, making it unsuitable for many
scenarios. This work introduces a trust model stronger than the central model
but with a lower privacy cost than the local model, leveraging the emerging
\emph{shuffle} model of privacy. We present the first generic algorithm for
episodic RL under the shuffle model, where a trusted shuffler randomly permutes
a batch of users' data before sending it to the central agent. We then
instantiate the algorithm using our proposed shuffle Privatizer, relying on a
shuffle private binary summation mechanism. Our analysis shows that the
algorithm achieves a near-optimal regret bound comparable to that of the
centralized model and significantly outperforms the local model in terms of
privacy cost.

摘要：差分隐私 (DP) 最近已引入到情境强化学习 (RL) 中，以正式解决个性化服务中的用户隐私问题。先前的工作主要关注 DP 的两种信任模型：中心模型，其中一个中心代理负责保护用户的敏感数据，以及（更强的）本地模型，其中保护直接发生在用户端。然而，它们要么需要一个受信任的中心代理，要么产生明显更高的隐私成本，使其不适用于许多场景。这项工作引入了一个比中心模型更强但隐私成本低于本地模型的信任模型，利用了新兴的隐私“混洗”模型。我们提出了混洗模型下情境 RL 的第一个通用算法，其中一个受信任的混洗器在将一批用户数据发送到中心代理之前随机排列这些数据。然后，我们使用我们提出的混洗私有化器实例化该算法，依靠混洗私有二进制求和机制。我们的分析表明，该算法实现了与集中式模型相当的近似最优后悔界，并且在隐私成本方面明显优于本地模型。

##### **TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection**
2411.11641v1 by Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang

Time series anomaly detection aims to identify unusual patterns in data or
deviations from systems' expected behavior. The reconstruction-based methods
are the mainstream in this task, which learn point-wise representation via
unsupervised learning. However, the unlabeled anomaly points in training data
may cause these reconstruction-based methods to learn and reconstruct anomalous
data, resulting in the challenge of capturing normal patterns. In this paper,
we propose a time series anomaly detection method based on implicit neural
representation (INR) reconstruction, named TSINR, to address this challenge.
Due to the property of spectral bias, TSINR enables prioritizing low-frequency
signals and exhibiting poorer performance on high-frequency abnormal data.
Specifically, we adopt INR to parameterize time series data as a continuous
function and employ a transformer-based architecture to predict the INR of
given data. As a result, the proposed TSINR method achieves the advantage of
capturing the temporal continuity and thus is more sensitive to discontinuous
anomaly data. In addition, we further design a novel form of INR continuous
function to learn inter- and intra-channel information, and leverage a
pre-trained large language model to amplify the intense fluctuations in
anomalies. Extensive experiments demonstrate that TSINR achieves superior
overall performance on both univariate and multivariate time series anomaly
detection benchmarks compared to other state-of-the-art reconstruction-based
methods. Our codes are available.

摘要：時間序列異常偵測旨在識別資料中的異常模式或系統預期行為的偏差。重建式方法是此項任務的主流，它透過非監督式學習來學習逐點表示。然而，訓練資料中未標記的異常點可能會導致這些重建式方法學習和重建異常資料，造成難以擷取正常模式的挑戰。在本文中，我們提出了一種基於隱式神經表示 (INR) 重建的時間序列異常偵測方法，稱為 TSINR，以解決此挑戰。由於頻譜偏差的特性，TSINR 能夠優先處理低頻訊號，並在高頻異常資料中表現較差。具體來說，我們採用 INR 將時間序列資料參數化為一個連續函數，並採用基於 Transformer 的架構來預測給定資料的 INR。因此，提出的 TSINR 方法具備擷取時間連續性的優點，因此對不連續的異常資料更為敏感。此外，我們進一步設計了一種新的 INR 連續函數形式來學習通道間和通道內的資訊，並利用預先訓練的大語言模型來放大異常中的強烈波動。大量的實驗證明，與其他最先進的基於重建的方法相比，TSINR 在單變量和多變量時間序列異常偵測基準上都取得了優異的整體效能。我們的程式碼已公開。

##### **SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation**
2411.11636v1 by Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song

Deep learning-based medical image segmentation helps assist diagnosis and
accelerate the treatment process while the model training usually requires
large-scale dense annotation datasets. Weakly semi-supervised medical image
segmentation is an essential application because it only requires a small
amount of scribbles and a large number of unlabeled data to train the model,
which greatly reduces the clinician's effort to fully annotate images. To
handle the inadequate supervisory information challenge in weakly
semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label
(SP${}^3$) learning method is proposed, using the structural information
contained in superpixel for supplemental information. Specifically, the
annotation of scribbles is propagated to superpixels and thus obtains a dense
annotation for supervised training. Since the quality of pseudo-labels is
limited by the low-quality annotation, the beneficial superpixels selected by
dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to
alleviate the negative impact of noise in pseudo-label, superpixel-level
uncertainty is incorporated to guide the pseudo-label supervision for stable
learning. Our method achieves state-of-the-art performance on both tumor and
organ segmentation datasets under the WSSS setting, using only 3\% of the
annotation workload compared to fully supervised methods and attaining
approximately 80\% Dice score. Additionally, our method outperforms eight
weakly and semi-supervised methods under both weakly supervised and
semi-supervised settings. Results of extensive experiments validate the
effectiveness and annotation efficiency of our weakly semi-supervised
segmentation, which can assist clinicians in achieving automated segmentation
for organs or tumors quickly and ultimately benefit patients.

摘要：<paragraph>基於深度學習的醫學影像分割有助於診斷並加速治療過程，而模型訓練通常需要大規模密集標註的資料集。弱半監督醫學影像分割是一項重要的應用，因為它只需要少量的塗鴉和大量的未標註資料來訓練模型，這大大減少了臨床醫生完全標註影像的工作量。為了應對弱半監督分割 (WSSS) 中監督資訊不足的挑戰，提出了一種超像素傳播偽標籤 (SP${}^3$) 學習方法，利用超像素中包含的結構資訊作為補充資訊。具體來說，將塗鴉的標註傳播到超像素，從而獲得用於監督訓練的密集標註。由於偽標籤的品質受到低品質標註的限制，因此使用動態閾值選取的有利超像素來精緻偽標籤。此外，為了減輕偽標籤中雜訊的負面影響，將超像素層級的不確定性納入其中，以指導偽標籤監督以進行穩定的學習。我們的模型在 WSSS 設定下，在腫瘤和器官分割資料集上都達到了最先進的效能，與完全監督的方法相比，只使用了 3% 的標註工作量，並達到了約 80% 的 Dice 分數。此外，我們的模型在弱監督和半監督設定下都優於八種弱監督和半監督方法。廣泛實驗的結果驗證了我們弱半監督分割的有效性和標註效率，這可以協助臨床醫生快速實現器官或腫瘤的自動分割，並最終使患者受益。</paragraph>

##### **Chapter 7 Review of Data-Driven Generative AI Models for Knowledge Extraction from Scientific Literature in Healthcare**
2411.11635v1 by Leon Kopitar, Primoz Kocbek, Lucija Gosak, Gregor Stiglic

This review examines the development of abstractive NLP-based text
summarization approaches and compares them to existing techniques for
extractive summarization. A brief history of text summarization from the 1950s
to the introduction of pre-trained language models such as Bidirectional
Encoder Representations from Transformer (BERT) and Generative Pre-training
Transformers (GPT) are presented. In total, 60 studies were identified in
PubMed and Web of Science, of which 29 were excluded and 24 were read and
evaluated for eligibility, resulting in the use of seven studies for further
analysis. This chapter also includes a section with examples including an
example of a comparison between GPT-3 and state-of-the-art GPT-4 solutions in
scientific text summarisation. Natural language processing has not yet reached
its full potential in the generation of brief textual summaries. As there are
acknowledged concerns that must be addressed, we can expect gradual
introduction of such models in practise.

摘要：本篇評論探討了抽象式基於自然語言處理的文字摘要方法的發展，並將其與現有的擷取式摘要技術進行比較。簡述了從 1950 年代到引入預先訓練好的語言模型（例如來自 Transformer 的雙向編碼器表徵 (BERT) 和生成式預訓練 Transformer (GPT)）的文字摘要歷史。總共在 PubMed 和 Web of Science 中找出 60 項研究，其中 29 項被排除，24 項被閱讀並評估資格，最後使用 7 項研究進行進一步分析。本章還包含一個範例區段，其中包含 GPT-3 和最先進的 GPT-4 解決方案在科學文字摘要中的比較範例。自然語言處理在產生簡潔的文字摘要方面尚未發揮其全部潛力。由於存在公認的疑慮必須解決，我們可以預期在實務中逐漸導入此類模型。

##### **Federated Incremental Named Entity Recognition**
2411.11623v1 by Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dong Yu

Federated Named Entity Recognition (FNER) boosts model training within each
local client by aggregating the model updates of decentralized local clients,
without sharing their private data. However, existing FNER methods assume fixed
entity types and local clients in advance, leading to their ineffectiveness in
practical applications. In a more realistic scenario, local clients receive new
entity types continuously, while new local clients collecting novel data may
irregularly join the global FNER training. This challenging setup, referred to
here as Federated Incremental NER, renders the global model suffering from
heterogeneous forgetting of old entity types from both intra-client and
inter-client perspectives. To overcome these challenges, we propose a
Local-Global Forgetting Defense (LGFD) model. Specifically, to address
intra-client forgetting, we develop a structural knowledge distillation loss to
retain the latent space's feature structure and a pseudo-label-guided
inter-type contrastive loss to enhance discriminative capability over different
entity types, effectively preserving previously learned knowledge within local
clients. To tackle inter-client forgetting, we propose a task switching monitor
that can automatically identify new entity types under privacy protection and
store the latest old global model for knowledge distillation and
pseudo-labeling. Experiments demonstrate significant improvement of our LGFD
model over comparison methods.

摘要：聯邦命名實體識別 (FNER) 透過彙總分散式本地用戶端的模型更新，在每個本地用戶端內提升模型訓練，而無需分享他們的私人資料。然而，現有的 FNER 方法預先假設固定的實體類型和本地用戶端，導致它們在實際應用中效率不彰。在更實際的場景中，本地用戶端會持續接收新的實體類型，而收集新資料的新本地用戶端可能會不定期加入全球 FNER 訓練。此具有挑戰性的設定，在此稱為聯邦增量 NER，會使全球模型遭受來自用戶端內部和用戶端之間觀點的舊實體類型異質性遺忘。為了克服這些挑戰，我們提出了一個局部-全局遺忘防禦 (LGFD) 模型。具體來說，為了解決用戶端內部遺忘，我們開發了一個結構知識蒸餾損失，以保留潛在空間的特徵結構，以及一個偽標籤引導的類型間對比損失，以增強對不同實體類型的辨別能力，有效地保留先前在本地用戶端中學習到的知識。為了應對用戶端之間的遺忘，我們提出了一個任務切換監控器，它可以在隱私保護下自動識別新的實體類型，並儲存最新的舊全局模型，以進行知識蒸餾和偽標籤。實驗證明了我們的 LGFD 模型比比較方法有顯著的改進。

##### **ST-Tree with Interpretability for Multivariate Time Series Classification**
2411.11620v1 by Mingsen Du, Yanxuan Wei, Yingxia Tang, Xiangwei Zheng, Shoushui Wei, Cun Ji

Multivariate time series classification is of great importance in practical
applications and is a challenging task. However, deep neural network models
such as Transformers exhibit high accuracy in multivariate time series
classification but lack interpretability and fail to provide insights into the
decision-making process. On the other hand, traditional approaches based on
decision tree classifiers offer clear decision processes but relatively lower
accuracy. Swin Transformer (ST) addresses these issues by leveraging
self-attention mechanisms to capture both fine-grained local patterns and
global patterns. It can also model multi-scale feature representation learning,
thereby providing a more comprehensive representation of time series features.
To tackle the aforementioned challenges, we propose ST-Tree with
interpretability for multivariate time series classification. Specifically, the
ST-Tree model combines ST as the backbone network with an additional neural
tree model. This integration allows us to fully leverage the advantages of ST
in learning time series context while providing interpretable decision
processes through the neural tree. This enables researchers to gain clear
insights into the model's decision-making process and extract meaningful
interpretations. Through experimental evaluations on 10 UEA datasets, we
demonstrate that the ST-Tree model improves accuracy in multivariate time
series classification tasks and provides interpretability through visualizing
the decision-making process across different datasets.

摘要：多變量時間序列分類在實際應用中非常重要，且是一項具有挑戰性的任務。然而，如 Transformer 等深度神經網路模型在多變量時間序列分類中展現出很高的準確度，但缺乏可解釋性，且無法提供決策過程的見解。另一方面，基於決策樹分類器的傳統方法提供了清晰的決策流程，但準確度相對較低。Swin Transformer (ST) 透過利用自注意力機制來擷取精細的局部模式和全局模式，來解決這些問題。它也可以建模多尺度特徵表示學習，從而提供更全面的時間序列特徵表示。為了應對上述挑戰，我們提出了具有可解釋性的 ST-Tree，用於多變量時間序列分類。具體來說，ST-Tree 模型將 ST 作為主幹網路，並結合一個額外的神經樹模型。這種整合讓我們能夠充分利用 ST 在學習時間序列背景方面的優勢，同時透過神經樹提供可解釋的決策流程。這使研究人員能夠清楚了解模型的決策過程，並提取有意義的解釋。透過對 10 個 UEA 資料集的實驗評估，我們證明了 ST-Tree 模型提高了多變量時間序列分類任務的準確度，並透過視覺化不同資料集的決策過程來提供可解釋性。

##### **Signaling and Social Learning in Swarms of Robots**
2411.11616v1 by Leo Cazenille, Maxime Toquebiau, Nicolas Lobato-Dauzier, Alessia Loi, Loona Macabre, Nathanael Aubert-Kato, Anthony Genot, Nicolas Bredeche

This paper investigates the role of communication in improving coordination
within robot swarms, focusing on a paradigm where learning and execution occur
simultaneously in a decentralized manner. We highlight the role communication
can play in addressing the credit assignment problem (individual contribution
to the overall performance), and how it can be influenced by it. We propose a
taxonomy of existing and future works on communication, focusing on information
selection and physical abstraction as principal axes for classification: from
low-level lossless compression with raw signal extraction and processing to
high-level lossy compression with structured communication models. The paper
reviews current research from evolutionary robotics, multi-agent (deep)
reinforcement learning, language models, and biophysics models to outline the
challenges and opportunities of communication in a collective of robots that
continuously learn from one another through local message exchanges,
illustrating a form of social learning.

摘要：本文探討了通訊在改善機器人蜂群協調中的角色，重點在於一種典範，其中學習和執行以分散的方式同時發生。我們強調了通訊在解決信用分配問題（對整體效能的個別貢獻）中所扮演的角色，以及它如何受到影響。我們提出了現有和未來通訊工作的分類法，重點在於將資訊選擇和物理抽象作為分類的主要軸線：從具有原始訊號提取和處理的低階無失真壓縮到具有結構化通訊模型的高階有失真壓縮。本文回顧了演化機器人、多重代理（深度）強化學習、語言模型和生物物理模型的現行研究，以概述一群機器人透過局部訊息交換持續彼此學習的通訊挑戰和機會，說明了一種社會學習的形式。

##### **OASIS: Open Agents Social Interaction Simulations on One Million Agents**
2411.11581v1 by Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Wanli Ouyang, Yu Qiao, Philip Torr, Jing Shao

There has been a growing interest in enhancing rule-based agent-based models
(ABMs) for social media platforms (\emph{i.e.}, X, Reddit) with more realistic
large language model (LLM) agents, thereby allowing for a more nuanced study of
complex systems. As a result, several LLM-based ABMs have been proposed in the
past year. While they hold promise, each simulator is specifically designed to
study a particular scenario, making it time-consuming and resource-intensive to
explore other phenomena using the same ABM. Additionally, these models simulate
only a limited number of agents, whereas real-world social media platforms
involve millions of users. To this end, we propose OASIS, a generalizable and
scalable social media simulator. OASIS is designed based on real-world social
media platforms, incorporating dynamically updated environments (\emph{i.e.},
dynamic social networks and post information), diverse action spaces
(\emph{i.e.}, following, commenting), and recommendation systems (\emph{i.e.},
interest-based and hot-score-based). Additionally, OASIS supports large-scale
user simulations, capable of modeling up to one million users. With these
features, OASIS can be easily extended to different social media platforms to
study large-scale group phenomena and behaviors. We replicate various social
phenomena, including information spreading, group polarization, and herd
effects across X and Reddit platforms. Moreover, we provide observations of
social phenomena at different agent group scales. We observe that the larger
agent group scale leads to more enhanced group dynamics and more diverse and
helpful agents' opinions. These findings demonstrate OASIS's potential as a
powerful tool for studying complex systems in digital environments.

摘要：<paragraph>近年來，對於增強基於規則的基於代理的模型 (ABM) 以用於社交媒體平台（例如 X、Reddit）的興趣日益濃厚，其中加入了更逼真的大型語言模型 (LLM) 代理，從而允許對複雜系統進行更細緻的研究。因此，過去一年中已經提出了多個基於 LLM 的 ABM。雖然它們很有希望，但每個模擬器都是專門設計用於研究特定情境，這使得使用相同的 ABM 來探索其他現象既費時又費資源。此外，這些模型只模擬有限數量的代理，而現實世界的社交媒體平台涉及數百萬用戶。為此，我們提出了 OASIS，一個可概括且可擴充的社交媒體模擬器。OASIS 是根據現實世界的社交媒體平台設計的，它結合了動態更新的環境（例如動態社交網路和文章資訊）、多樣化的動作空間（例如關注、留言）和推薦系統（例如基於興趣和熱門評分）。此外，OASIS 支援大規模使用者模擬，能夠模擬多達一百萬名使用者。有了這些功能，OASIS 可以輕鬆擴充到不同的社交媒體平台，以研究大規模群體現象和行為。我們複製了各種社會現象，包括資訊散布、群體兩極化和羊群效應，橫跨 X 和 Reddit 平台。此外，我們提供了不同代理群組規模的社會現象觀察結果。我們觀察到，較大的代理群組規模會導致更增強的群組動態以及更多元且有用的代理意見。這些發現證明了 OASIS 作為研究數位環境中複雜系統的強大工具的潛力。</paragraph>

##### **Hybrid Data-Driven SSM for Interpretable and Label-Free mmWave Channel Prediction**
2411.11576v1 by Yiyong Sun, Jiajun He, Zhidi Lin, Wenqiang Pu, Feng Yin, Hing Cheung So

Accurate prediction of mmWave time-varying channels is essential for
mitigating the issue of channel aging in complex scenarios owing to high user
mobility. Existing channel prediction methods have limitations: classical
model-based methods often struggle to track highly nonlinear channel dynamics
due to limited expert knowledge, while emerging data-driven methods typically
require substantial labeled data for effective training and often lack
interpretability. To address these issues, this paper proposes a novel hybrid
method that integrates a data-driven neural network into a conventional
model-based workflow based on a state-space model (SSM), implicitly tracking
complex channel dynamics from data without requiring precise expert knowledge.
Additionally, a novel unsupervised learning strategy is developed to train the
embedded neural network solely with unlabeled data. Theoretical analyses and
ablation studies are conducted to interpret the enhanced benefits gained from
the hybrid integration. Numerical simulations based on the 3GPP mmWave channel
model corroborate the superior prediction accuracy of the proposed method,
compared to state-of-the-art methods that are either purely model-based or
data-driven. Furthermore, extensive experiments validate its robustness against
various challenging factors, including among others severe channel variations
and high noise levels.

摘要：準確預測 mmWave 時變通道對於在複雜場景中緩解通道老化問題至關重要，這是因為使用者的高度流動性。現有的通道預測方法有其限制：基於經典模型的方法通常難以追蹤高度非線性的通道動態，這是因為專家知識有限，而新興的資料驅動方法通常需要大量的標籤資料才能進行有效的訓練，而且通常缺乏可解釋性。為了解決這些問題，本文提出了一種新穎的混合方法，將資料驅動的神經網路整合到基於狀態空間模型 (SSM) 的傳統基於模型的工作流程中，從資料中隱式追蹤複雜的通道動態，而不需要精確的專家知識。此外，還開發了一種新穎的無監督學習策略，僅使用未標籤資料來訓練嵌入式神經網路。進行理論分析和消融研究，以詮釋從混合整合中獲得的增強優勢。基於 3GPP mmWave 通道模型的數值模擬證實了所提出的方法具有優異的預測準確度，與純粹基於模型或資料驅動的最新方法相比。此外，廣泛的實驗驗證了其對各種挑戰性因素的穩健性，包括嚴重的通道變化和高噪音水準等。

##### **Topology-aware Preemptive Scheduling for Co-located LLM Workloads**
2411.11560v1 by Ping Zhang, Lei Su, Jinjie Yang, Xin Chen

Hosting diverse large language model workloads in a unified resource pool
through co-location is cost-effective. For example, long-running chat services
generally follow diurnal traffic patterns, which inspire co-location of batch
jobs to fulfill resource valleys between successive peaks, and thus to saturate
resource allocation in cluster-wide scope. These heterogeneous workloads often
have different business priorities, and therefore preemption can be leveraged
for resource elasticity. However, workloads often have distinct topology
preferences as well. The resources released by lower-priority instances may
fail to meet the requirements of high-priority online services which are
usually latency-sensitive. The root cause behind such mis-match is a lack of
topology awareness of resource scheduler, especially during preemption. To
bridge this gap, we develop a fine-grained topology-aware method for preemptive
scheduling of hybrid workloads. The method ensures that the resources freed by
preempted tasks adhere to the topological affinity needs of high-priority
preemptors in a guaranteed or best-effort manner. This dynamic alignment
significantly increases the efficiency of preemption and improves overall
scheduled performance for LLM workloads by $55\%$.

摘要：將各種大型語言模型工作負載透過共置於統一的資源池中進行託管，這是一種具有成本效益的做法。例如，長時間執行的聊天服務通常遵循晝夜流量模式，這會激勵批次作業共置以填補連續高峰之間的資源谷，從而使整個叢集範圍的資源配置飽和。這些異質工作負載通常具有不同的業務優先順序，因此可以利用搶占來提高資源彈性。然而，工作負載通常也有不同的拓撲偏好。低優先順序實例釋放的資源可能無法滿足通常對延遲很敏感的高優先順序線上服務的需求。這種不匹配背後的主要原因是資源排程器缺乏拓撲感知，特別是在搶占期間。為了彌補這個差距，我們開發了一種細緻的拓撲感知方法，用於混合工作負載的搶占式排程。此方法確保被搶占任務釋放的資源以保證或盡力而為的方式符合高優先順序搶占者的拓撲親和性需求。這種動態調整顯著提高了搶占效率，並將 LLM 工作負載的整體排程效能提升了 55%。

##### **Real-Time Fitness Exercise Classification and Counting from Video Frames**
2411.11548v1 by Riccardo Riccio

This paper introduces a novel method for real-time exercise classification
using a Bidirectional Long Short-Term Memory (BiLSTM) neural network. Existing
exercise recognition approaches often rely on synthetic datasets, raw
coordinate inputs sensitive to user and camera variations, and fail to fully
exploit the temporal dependencies in exercise movements. These issues limit
their generalizability and robustness in real-world conditions, where lighting,
camera angles, and user body types vary.
  To address these challenges, we propose a BiLSTM-based model that leverages
invariant features, such as joint angles, alongside raw coordinates. By using
both angles and (x, y, z) coordinates, the model adapts to changes in
perspective, user positioning, and body differences, improving generalization.
Training on 30-frame sequences enables the BiLSTM to capture the temporal
context of exercises and recognize patterns evolving over time.
  We compiled a dataset combining synthetic data from the InfiniteRep dataset
and real-world videos from Kaggle and other sources. This dataset includes four
common exercises: squat, push-up, shoulder press, and bicep curl. The model was
trained and validated on these diverse datasets, achieving an accuracy of over
99% on the test set. To assess generalizability, the model was tested on 2
separate test sets representative of typical usage conditions. Comparisons with
the previous approach from the literature are present in the result section
showing that the proposed model is the best-performing one.
  The classifier is integrated into a web application providing real-time
exercise classification and repetition counting without manual exercise
selection.
  Demo and datasets are available at the following GitHub Repository:
https://github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting.

摘要：<paragraph>本文介紹了一種使用雙向長短期記憶 (BiLSTM) 神經網路進行實時運動分類的新方法。現有的運動識別方法通常依賴於合成資料集、對使用者和相機變化敏感的原始座標輸入，並且無法充分利用運動動作中的時間依賴性。這些問題限制了它們在現實世界條件下的泛化性和穩健性，在這些條件下，光線、相機角度和使用者身體類型會有所不同。
  為了應對這些挑戰，我們提出了一個基於 BiLSTM 的模型，該模型利用了關節角度等不變特徵以及原始座標。通過使用角度和 (x, y, z) 座標，該模型適應了視角、使用者定位和身體差異的變化，從而改進了泛化性。在 30 幀序列上進行訓練使 BiLSTM 能夠捕捉運動的時間上下文並識別隨時間推移而演化的模式。
  我們編制了一個資料集，結合了來自 InfiniteRep 資料集的合成資料和來自 Kaggle 和其他來源的真實世界影片。此資料集包含四個常見的運動：深蹲、伏地挺身、肩推和二頭肌彎舉。該模型在這些不同的資料集上進行了訓練和驗證，在測試集上達到了超過 99% 的準確率。為了評估泛化性，該模型在 2 個代表典型使用條件的獨立測試集上進行了測試。結果部分中存在與文獻中先前方法的比較，表明所提出的模型是執行效果最佳的一個。
  該分類器被整合到一個網路應用程式中，提供實時運動分類和重複次數計算，無需手動選擇運動。
  示範和資料集可在以下 GitHub 儲存庫中取得：
https://github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting。</paragraph>

##### **Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment**
2411.11543v1 by Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng

Benefiting from the powerful capabilities of Large Language Models (LLMs),
pre-trained visual encoder models connected to LLMs form Vision Language Models
(VLMs). However, recent research shows that the visual modality in VLMs is
highly vulnerable, allowing attackers to bypass safety alignment in LLMs
through visually transmitted content, launching harmful attacks. To address
this challenge, we propose a progressive concept-based alignment strategy,
PSA-VLM, which incorporates safety modules as concept bottlenecks to enhance
visual modality safety alignment. By aligning model predictions with specific
safety concepts, we improve defenses against risky images, enhancing
explainability and controllability while minimally impacting general
performance. Our method is obtained through two-stage training. The low
computational cost of the first stage brings very effective performance
improvement, and the fine-tuning of the language model in the second stage
further improves the safety performance. Our method achieves state-of-the-art
results on popular VLM safety benchmark.

摘要：受益於大型語言模型 (LLM) 的強大功能，連接到 LLM 的預訓練視覺編碼器模型形成了視覺語言模型 (VLM)。然而，最近的研究表明，VLM 中的視覺模態非常脆弱，允許攻擊者通過視覺傳輸內容繞過 LLM 中的安全對齊，發起有害攻擊。為了應對這一挑戰，我們提出了一種漸進式的基於概念的對齊策略 PSA-VLM，它將安全模組整合為概念瓶頸，以增強視覺模態的安全對齊。通過將模型預測與特定安全概念對齊，我們改進了對風險圖像的防禦，同時在最小程度地影響一般效能的情況下增強了解力和可控性。我們的這種方法是透過兩階段訓練獲得的。第一階段的低運算成本帶來了非常有效的效能提升，而第二階段對語言模型的微調進一步提升了安全性效能。我們的這種方法在流行的 VLM 安全基準上達到了最先進的結果。

##### **Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality**
2411.11531v1 by Viktoriia Chekalina, Anton Razzigaev, Elizaveta Goncharova, Andrey Kuznetsov

In this paper we present an approach to reduce hallucinations in Large
Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional
modality. Our method involves transforming input text into a set of KG
embeddings and using an adapter to integrate these embeddings into the language
model space, without relying on external retrieval processes.
  To facilitate this, we created WikiEntities, a dataset containing over 3
million Wikipedia texts annotated with entities from Wikidata and their
corresponding embeddings from PyTorch-BigGraph. This dataset serves as a
valuable resource for training Entity Linking models and adapting the described
method to various LLMs using specialized adapters.
  Our method does not require fine-tuning of the language models themselves;
instead, we only train the adapter. This ensures that the model's performance
on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA
2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and
demonstrated that our approach improves performance on the HaluEval, True-False
benchmarks and FEVER dataset. The results indicate that incorporating KGs as a
new modality can effectively reduce hallucinations and improve the factual
accuracy of language models, all without the need for external retrieval.

摘要：<paragraph>在本文中，我們提出了一種方法，透過將知識圖譜 (KG) 作為附加方式納入大型語言模型 (LLM)，以減少幻覺。我們的做法包括將輸入文字轉換成一組 KG 嵌入，並使用適配器將這些嵌入整合到語言模型空間，而無需依賴外部檢索程序。
為了促進這一點，我們建立了 WikiEntities，這是一個包含超過 300 萬個維基百科文字的資料集，其中附有來自 Wikidata 的實體註解，以及它們來自 PyTorch-BigGraph 的對應嵌入。此資料集作為訓練實體連結模型和使用專門適配器將所述方法調整到各種 LLM 的寶貴資源。
我們的做法不需要微調語言模型本身；相反，我們只訓練適配器。這確保了模型在其他任務上的效能不受影響。我們使用此資料集訓練了 Mistral 7B、LLaMA 2-7B (聊天) 和 LLaMA 3-8B (指令) 模型的適配器，並證明了我們的做法改善了 HaluEval、真假基準和 FEVER 資料集的效能。結果表明，將 KG 作為一種新方式納入可以有效減少幻覺，並提高語言模型的事實準確性，而無需外部檢索。</paragraph>

##### **A Pre-Trained Graph-Based Model for Adaptive Sequencing of Educational Documents**
2411.11520v1 by Jean Vassoyan, Anan Schütt, Jill-Jênn Vie, Arun-Balajiee Lekshmi-Narayanan, Elisabeth André, Nicolas Vayatis

Massive Open Online Courses (MOOCs) have greatly contributed to making
education more accessible.However, many MOOCs maintain a rigid,
one-size-fits-all structure that fails to address the diverse needs and
backgrounds of individual learners.Learning path personalization aims to
address this limitation, by tailoring sequences of educational content to
optimize individual student learning outcomes.Existing approaches, however,
often require either massive student interaction data or extensive expert
annotation, limiting their broad application.In this study, we introduce a
novel data-efficient framework for learning path personalization that operates
without expert annotation.Our method employs a flexible recommender system
pre-trained with reinforcement learning on a dataset of raw course
materials.Through experiments on semi-synthetic data, we show that this
pre-training stage substantially improves data-efficiency in a range of
adaptive learning scenarios featuring new educational materials.This opens up
new perspectives for the design of foundation models for adaptive learning.

摘要：大型開放式線上課程 (MOOC) 為教育的可近性做出了巨大貢獻。然而，許多 MOOC 維持著僵化、一體適用的結構，無法滿足個別學習者的多元需求和背景。學習路徑個人化旨在解決此限制，透過調整教育內容的順序，以最佳化個別學生的學習成果。然而，現有方法通常需要大量的學生互動資料或大量的專家註解，限制了其廣泛應用。在本研究中，我們引入了一個新穎的資料有效框架，用於學習路徑個人化，在沒有專家註解的情況下運作。我們的方使用一個靈活的推薦系統，並在原始課程資料的資料集上使用強化學習進行預訓練。透過在半合成資料上的實驗，我們展示了此預訓練階段大幅改善了各種適應性學習情境的資料效率，這些情境包含新的教育資料。這為適應性學習基礎模型的設計開啟了新的視野。

##### **Structure learning with Temporal Gaussian Mixture for model-based Reinforcement Learning**
2411.11511v1 by Théophile Champion, Marek Grześ, Howard Bowman

Model-based reinforcement learning refers to a set of approaches capable of
sample-efficient decision making, which create an explicit model of the
environment. This model can subsequently be used for learning optimal policies.
In this paper, we propose a temporal Gaussian Mixture Model composed of a
perception model and a transition model. The perception model extracts discrete
(latent) states from continuous observations using a variational Gaussian
mixture likelihood. Importantly, our model constantly monitors the collected
data searching for new Gaussian components, i.e., the perception model performs
a form of structure learning (Smith et al., 2020; Friston et al., 2018; Neacsu
et al., 2022) as it learns the number of Gaussian components in the mixture.
Additionally, the transition model learns the temporal transition between
consecutive time steps by taking advantage of the Dirichlet-categorical
conjugacy. Both the perception and transition models are able to forget part of
the data points, while integrating the information they provide within the
prior, which ensure fast variational inference. Finally, decision making is
performed with a variant of Q-learning which is able to learn Q-values from
beliefs over states. Empirically, we have demonstrated the model's ability to
learn the structure of several mazes: the model discovered the number of states
and the transition probabilities between these states. Moreover, using its
learned Q-values, the agent was able to successfully navigate from the starting
position to the maze's exit.

摘要：<paragraph>基於模型的強化學習是指一組能夠進行樣本有效決策的方法，它能建立環境的明確模型。此模型隨後可用於學習最佳策略。在本文中，我們提出一個由感知模型和轉換模型組成的時序高斯混合模型。感知模型使用變異高斯混合似然從連續觀測中提取離散（潛在）狀態。重要的是，我們的模型持續監控收集的資料，尋找新的高斯成分，也就是說，感知模型執行一種結構學習（Smith 等人，2020 年；Friston 等人，2018 年；Neacsu 等人，2022 年），因為它學習混合中高斯成分的數量。此外，轉換模型利用 Dirichlet-categorical 共軛學習連續時間步之間的時序轉換。感知模型和轉換模型都能忘記部分資料點，同時整合它們在先驗中提供的資訊，這確保了快速的變異推論。最後，決策制定是透過 Q 學習的變體進行的，它能夠從狀態信念中學習 Q 值。根據經驗，我們已經證明了該模型學習多個迷宮結構的能力：該模型發現了狀態數量和這些狀態之間的轉換機率。此外，代理使用其學習的 Q 值，能夠從起點成功導航到迷宮出口。</paragraph>

##### **Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering**
2411.11504v1 by Xinyan Guan, Yanjiang Liu, Xinyu Lu, Boxi Cao, Ben He, Xianpei Han, Le Sun, Jie Lou, Bowen Yu, Yaojie Lu, Hongyu Lin

The evolution of machine learning has increasingly prioritized the
development of powerful models and more scalable supervision signals. However,
the emergence of foundation models presents significant challenges in providing
effective supervision signals necessary for further enhancing their
capabilities. Consequently, there is an urgent need to explore novel
supervision signals and technical approaches. In this paper, we propose
verifier engineering, a novel post-training paradigm specifically designed for
the era of foundation models. The core of verifier engineering involves
leveraging a suite of automated verifiers to perform verification tasks and
deliver meaningful feedback to foundation models. We systematically categorize
the verifier engineering process into three essential stages: search, verify,
and feedback, and provide a comprehensive review of state-of-the-art research
developments within each stage. We believe that verifier engineering
constitutes a fundamental pathway toward achieving Artificial General
Intelligence.

摘要：機器學習的演進越來越優先考慮開發強大的模型和更具可擴充性的監督信號。然而，基礎模型的出現對提供進一步增強其能力所需的有效監督信號提出了重大挑戰。因此，迫切需要探索創新的監督信號和技術方法。在本文中，我們提出了驗證器工程，這是一種專門為基礎模型時代設計的新穎後訓練範例。驗證器工程的核心涉及利用一組自動化驗證器來執行驗證任務，並向基礎模型提供有意義的回饋。我們將驗證器工程流程系統化地分類為三個基本階段：搜尋、驗證和回饋，並對每個階段中最先進的研究發展提供全面的回顧。我們相信驗證器工程構成了實現人工通用智慧的基本途徑。

##### **Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models**
2411.11496v1 by Chenhang Cui, Gelei Deng, An Zhang, Jingnan Zheng, Yicong Li, Lianli Gao, Tianwei Zhang, Tat-Seng Chua

Recent advances in Large Vision-Language Models (LVLMs) have showcased strong
reasoning abilities across multiple modalities, achieving significant
breakthroughs in various real-world applications. Despite this great success,
the safety guardrail of LVLMs may not cover the unforeseen domains introduced
by the visual modality. Existing studies primarily focus on eliciting LVLMs to
generate harmful responses via carefully crafted image-based jailbreaks
designed to bypass alignment defenses. In this study, we reveal that a safe
image can be exploited to achieve the same jailbreak consequence when combined
with additional safe images and prompts. This stems from two fundamental
properties of LVLMs: universal reasoning capabilities and safety snowball
effect. Building on these insights, we propose Safety Snowball Agent (SSA), a
novel agent-based framework leveraging agents' autonomous and tool-using
abilities to jailbreak LVLMs. SSA operates through two principal stages: (1)
initial response generation, where tools generate or retrieve jailbreak images
based on potential harmful intents, and (2) harmful snowballing, where refined
subsequent prompts induce progressively harmful outputs. Our experiments
demonstrate that \ours can use nearly any image to induce LVLMs to produce
unsafe content, achieving high success jailbreaking rates against the latest
LVLMs. Unlike prior works that exploit alignment flaws, \ours leverages the
inherent properties of LVLMs, presenting a profound challenge for enforcing
safety in generative multimodal systems. Our code is avaliable at
\url{https://github.com/gzcch/Safety_Snowball_Agent}.

摘要：<paragraph>大型視覺語言模型 (LVLMs) 的最新進展展示了跨多種模態的強大推理能力，在各種真實世界應用中取得重大突破。儘管取得了如此巨大的成功，LVLMs 的安全防護欄可能無法涵蓋視覺模態引入的不可預見領域。現有研究主要集中在引誘 LVLMs 透過精心設計的基於影像的越獄來產生有害的回應，旨在繞過校準防禦。在本研究中，我們揭示了一幅安全的影像可以被利用，與其他安全的影像和提示結合後，達成相同的越獄後果。這源於 LVLMs 的兩個基本屬性：通用的推理能力和安全滾雪球效應。基於這些見解，我們提出了安全滾雪球代理 (SSA)，一個新穎的基於代理的架構，利用代理的自主和使用工具的能力來越獄 LVLMs。SSA 透過兩個主要階段運作：(1) 初始回應產生，其中工具根據潛在的惡意意圖產生或擷取越獄影像，以及 (2) 有害的滾雪球，其中經過優化的後續提示會誘發逐漸有害的輸出。我們的實驗證明，\ours 可以使用幾乎任何影像來誘導 LVLMs 產生不安全的內容，對抗最新 LVLMs 達到了很高的成功越獄率。與利用校準缺陷的先前研究不同，\ours 利用了 LVLMs 的內在屬性，對在生成式多模態系統中強制執行安全提出了嚴峻的挑戰。我們的程式碼可以在 \url{https://github.com/gzcch/Safety_Snowball_Agent} 取得。</paragraph>

##### **Alien Recombination: Exploring Concept Blends Beyond Human Cognitive Availability in Visual Art**
2411.11494v1 by Alejandro Hernandez, Levin Brinkmann, Ignacio Serna, Nasim Rahaman, Hassan Abu Alhaija, Hiromu Yakura, Mar Canet Sola, Bernhard Schölkopf, Iyad Rahwan

While AI models have demonstrated remarkable capabilities in constrained
domains like game strategy, their potential for genuine creativity in
open-ended domains like art remains debated. We explore this question by
examining how AI can transcend human cognitive limitations in visual art
creation. Our research hypothesizes that visual art contains a vast unexplored
space of conceptual combinations, constrained not by inherent incompatibility,
but by cognitive limitations imposed by artists' cultural, temporal,
geographical and social contexts.
  To test this hypothesis, we present the Alien Recombination method, a novel
approach utilizing fine-tuned large language models to identify and generate
concept combinations that lie beyond human cognitive availability. The system
models and deliberately counteracts human availability bias, the tendency to
rely on immediately accessible examples, to discover novel artistic
combinations.
  This system not only produces combinations that have never been attempted
before within our dataset but also identifies and generates combinations that
are cognitively unavailable to all artists in the domain. Furthermore, we
translate these combinations into visual representations, enabling the
exploration of subjective perceptions of novelty. Our findings suggest that
cognitive unavailability is a promising metric for optimizing artistic novelty,
outperforming merely temperature scaling without additional evaluation
criteria. This approach uses generative models to connect previously
unconnected ideas, providing new insight into the potential of framing
AI-driven creativity as a combinatorial problem.

摘要：儘管 AI 模型在受限領域（如遊戲策略）中已展現出卓越的能力，但它們在開放領域（如藝術）中的真正創造力潛力仍有待商榷。我們透過探討 AI 如何超越人類認知限制來創作視覺藝術，來探討這個問題。我們的研究假設，視覺藝術包含一個廣大的未探索概念組合空間，其限制並非來自於固有的不相容性，而是來自於藝術家的文化、時間、地理和社會背景所施加的認知限制。
為了測試這個假設，我們提出了「異星重組」方法，這是一種新穎的方法，利用微調的大型語言模型來識別和產生超出人類認知能力的概念組合。該系統建模並故意抵消人類的可用性偏差，即依賴於立即可取得的範例的傾向，以發現新穎的藝術組合。
這個系統不僅產生了我們資料集中前所未有的組合，還識別並產生了該領域的所有藝術家在認知上都無法取得的組合。此外，我們將這些組合轉換成視覺表徵，讓主觀的新穎性知覺得以探討。我們的研究結果表明，認知不可用性是一個很有前景的指標，可以用來最佳化藝術新穎性，其表現優於僅在沒有額外評估標準的情況下進行溫度調整。這種方法使用生成模型來連接先前未連接的想法，為將 AI 驅動的創造力視為組合問題的潛力提供了新的見解。

##### **Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts**
2411.11479v1 by Jingxuan Li, Yuning Yang, Shengqi Yang, Yizhou Zhao, Ying Nian Wu

The rapid advancement of Vision-Language Models (VLMs) has expanded
multimodal applications, yet evaluations often focus on basic tasks like object
recognition, overlooking abstract aspects such as personalities and values. To
address this gap, we introduce Value-Spectrum, a visual question-answering
benchmark aimed at assessing VLMs based on Schwartz's value dimensions, which
capture core values guiding people's beliefs and actions across cultures. We
constructed a vectorized database of over 50,000 short videos sourced from
TikTok, YouTube Shorts, and Instagram Reels, covering multiple months and a
wide array of topics such as family, health, hobbies, society, and technology.
We also developed a VLM agent pipeline to automate video browsing and analysis.
Benchmarking representative VLMs on Value-Spectrum reveals significant
differences in their responses to value-oriented content, with most models
exhibiting a preference for hedonistic topics. Beyond identifying natural
preferences, we explored the ability of VLM agents to adopt specific personas
when explicitly prompted, revealing insights into the models' adaptability in
role-playing scenarios. These findings highlight the potential of
Value-Spectrum as a comprehensive evaluation set for tracking VLM advancements
in value-based tasks and for developing more sophisticated role-playing AI
agents.

摘要：視覺語言模型 (VLM) 的快速進步擴展了多模態應用，但評估通常集中在基本任務上，例如物體辨識，而忽略了抽象層面，例如人格和價值觀。為了解決這個差距，我們引入了 Value-Spectrum，這是一個視覺問答基準，旨在根據 Schwartz 的價值觀向度評估 VLM，它捕捉到跨文化引導人們信念和行為的核心價值觀。我們構建了一個向量化資料庫，其中包含超過 50,000 個短影片，這些影片來自 TikTok、YouTube Shorts 和 Instagram Reels，涵蓋多個月和各種主題，例如家庭、健康、嗜好、社會和技術。我們還開發了一個 VLM 代理管線，以自動化影片瀏覽和分析。在 Value-Spectrum 上對具代表性的 VLM 進行基準測試，揭示了它們對價值導向內容的回應存在顯著差異，大多數模型都偏好享樂主義主題。除了找出自然偏好之外，我們還探討了 VLM 代理在明確提示下採用特定角色的能力，揭示了模型在角色扮演場景中的適應性。這些發現突顯了 Value-Spectrum 作為一個綜合評估集的潛力，用於追蹤 VLM 在基於價值的任務中的進步，並開發更複雜的角色扮演 AI 代理。

##### **Re-examining learning linear functions in context**
2411.11465v1 by Omar Naim, Guilhem Fouilhé, Nicholas Asher

In context learning (ICL) is an attractive method of solving a wide range of
problems. Inspired by Garg et al. (2022), we look closely at ICL in a variety
of train and test settings for several transformer models of different sizes
trained from scratch. Our study complements prior work by pointing out several
systematic failures of these models to generalize to data not in the training
distribution, thereby showing some limitations of ICL. We find that models
adopt a strategy for this task that is very different from standard solutions.

摘要：情境學習 (ICL) 是一種解決廣泛問題的誘人方法。受 Garg 等人 (2022) 的啟發，我們仔細研究了 ICL 在各種訓練和測試設定中，針對多個不同大小的Transformer模型從頭開始訓練。我們的研究補充了先前的研究，指出了這些模型在對訓練分佈中沒有的資料進行泛化的幾個系統性失敗，從而顯示了 ICL 的一些限制。我們發現模型採用了一種與標準解決方案截然不同的策略來執行這項任務。

##### **HistoEncoder: a digital pathology foundation model for prostate cancer**
2411.11458v1 by Joona Pohjonen, Abderrahim-Oussama Batouche, Antti Rannikko, Kevin Sandeman, Andrew Erickson, Esa Pitkanen, Tuomas Mirtti

Foundation models are trained on massive amounts of data to distinguish
complex patterns and can be adapted to a wide range of downstream tasks with
minimal computational resources. Here, we develop a foundation model for
prostate cancer digital pathology called HistoEncoder by pre-training on 48
million prostate tissue tile images. We demonstrate that HistoEncoder features
extracted from tile images with similar histological patterns map closely
together in the feature space. HistoEncoder outperforms models pre-trained with
natural images, even without fine-tuning or with 1000 times less training data.
We describe two use cases that leverage the capabilities of HistoEncoder by
fine-tuning the model with a limited amount of data and computational
resources. First, we show how HistoEncoder can be used to automatically
annotate large-scale datasets with high accuracy. Second, we combine histomics
with commonly used clinical nomograms, significantly improving prostate
cancer-specific death survival models. Foundation models such as HistoEncoder
can allow organizations with limited resources to build effective clinical
software tools without needing extensive datasets or significant amounts of
computing.

摘要：基礎模型在大量資料上進行訓練，以區分複雜模式，並可適應廣泛的下游任務，且只需最少的運算資源。在此，我們開發了一個名為 HistoEncoder 的攝護腺癌數位病理基礎模型，方法是預先在 4800 萬張攝護腺組織切片影像上進行訓練。我們證明了從具有相似組織學模式的切片影像中萃取的 HistoEncoder 特徵，在特徵空間中緊密地對應在一起。即使不進行微調或訓練資料減少 1000 倍，HistoEncoder 的表現仍優於預先使用自然影像進行訓練的模型。我們描述了兩個使用案例，它們透過使用有限的資料和運算資源微調模型，來利用 HistoEncoder 的功能。首先，我們展示了如何使用 HistoEncoder 自動註解大規模資料集，並具有高準確度。其次，我們將組織學與常用的臨床列線圖結合，大幅改善了攝護腺癌特異性死亡存活模型。像 HistoEncoder 這樣的基礎模型，可讓資源有限的組織建立有效的臨床軟體工具，而無需廣泛的資料集或大量的運算。

##### **Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet**
2411.11451v1 by Marnix Suilen, Thom Badings, Eline M. Bovy, David Parker, Nils Jansen

Markov decision processes (MDPs) are a standard model for sequential
decision-making problems and are widely used across many scientific areas,
including formal methods and artificial intelligence (AI). MDPs do, however,
come with the restrictive assumption that the transition probabilities need to
be precisely known. Robust MDPs (RMDPs) overcome this assumption by instead
defining the transition probabilities to belong to some uncertainty set. We
present a gentle survey on RMDPs, providing a tutorial covering their
fundamentals. In particular, we discuss RMDP semantics and how to solve them by
extending standard MDP methods such as value iteration and policy iteration. We
also discuss how RMDPs relate to other models and how they are used in several
contexts, including reinforcement learning and abstraction techniques. We
conclude with some challenges for future work on RMDPs.

摘要：馬可夫決策過程 (MDP) 是用於循序決策問題的標準模型，並廣泛用於許多科學領域，包括形式方法和人工智慧 (AI)。然而，MDP 有一個限制性的假設，即轉換機率需要精確得知。穩健 MDP (RMDP) 克服了這個假設，而是定義轉換機率屬於某個不確定性集合。我們針對 RMDP 提出一個溫和的調查，提供涵蓋其基礎知識的教學課程。特別是，我們討論 RMDP 語義以及如何透過擴充標準 MDP 方法（例如值迭代和策略迭代）來解決它們。我們還討論 RMDP 如何與其他模型相關，以及如何在幾個情境中使用它們，包括強化學習和抽象技術。最後，我們提出一些未來 RMDP 工作的挑戰。

##### **Unveiling the Inflexibility of Adaptive Embedding in Traffic Forecasting**
2411.11448v1 by Hongjun Wang, Jiyuan Chen, Lingyu Zhang, Renhe Jiang, Xuan Song

Spatiotemporal Graph Neural Networks (ST-GNNs) and Transformers have shown
significant promise in traffic forecasting by effectively modeling temporal and
spatial correlations. However, rapid urbanization in recent years has led to
dynamic shifts in traffic patterns and travel demand, posing major challenges
for accurate long-term traffic prediction. The generalization capability of
ST-GNNs in extended temporal scenarios and cross-city applications remains
largely unexplored. In this study, we evaluate state-of-the-art models on an
extended traffic benchmark and observe substantial performance degradation in
existing ST-GNNs over time, which we attribute to their limited inductive
capabilities. Our analysis reveals that this degradation stems from an
inability to adapt to evolving spatial relationships within urban environments.
To address this limitation, we reconsider the design of adaptive embeddings and
propose a Principal Component Analysis (PCA) embedding approach that enables
models to adapt to new scenarios without retraining. We incorporate PCA
embeddings into existing ST-GNN and Transformer architectures, achieving marked
improvements in performance. Notably, PCA embeddings allow for flexibility in
graph structures between training and testing, enabling models trained on one
city to perform zero-shot predictions on other cities. This adaptability
demonstrates the potential of PCA embeddings in enhancing the robustness and
generalization of spatiotemporal models.

摘要：時空圖神經網路 (ST-GNN) 和 Transformer 在交通預測方面已展現出顯著的潛力，能有效地建模時間和空間相關性。然而，近年來的快速都市化導致交通模式和出行需求出現動態變化，對準確的長期交通預測構成重大挑戰。ST-GNN 在擴展時間場景和跨城市應用中的泛化能力在很大程度上仍未得到探索。在本研究中，我們在一個擴展的交通基準上評估了最先進的模型，並觀察到現有 ST-GNN 隨著時間推移而出現顯著的效能下降，我們將其歸因於其有限的歸納能力。我們的分析表明，這種下降源於無法適應城市環境中不斷變化的空間關係。為了解決這個限制，我們重新考慮了自適應嵌入的設計，並提出了一種主成分分析 (PCA) 嵌入方法，使模型能夠在不重新訓練的情況下適應新的場景。我們將 PCA 嵌入整合到現有的 ST-GNN 和 Transformer 架構中，在效能方面取得顯著的提升。值得注意的是，PCA 嵌入允許在訓練和測試之間的圖形結構中具有靈活性，使在一個城市上訓練的模型能夠對其他城市執行零次學習預測。這種適應性證明了 PCA 嵌入在增強時空模型的穩健性和泛化能力方面的潛力。

##### **Membership Inference Attack against Long-Context Large Language Models**
2411.11424v1 by Zixiong Wang, Gaoyang Liu, Yang Yang, Chen Wang

Recent advances in Large Language Models (LLMs) have enabled them to overcome
their context window limitations, and demonstrate exceptional retrieval and
reasoning capacities on longer context. Quesion-answering systems augmented
with Long-Context Language Models (LCLMs) can automatically search massive
external data and incorporate it into their contexts, enabling faithful
predictions and reducing issues such as hallucinations and knowledge staleness.
Existing studies targeting LCLMs mainly concentrate on addressing the so-called
lost-in-the-middle problem or improving the inference effiencicy, leaving their
privacy risks largely unexplored. In this paper, we aim to bridge this gap and
argue that integrating all information into the long context makes it a
repository of sensitive information, which often contains private data such as
medical records or personal identities. We further investigate the membership
privacy within LCLMs external context, with the aim of determining whether a
given document or sequence is included in the LCLMs context. Our basic idea is
that if a document lies in the context, it will exhibit a low generation loss
or a high degree of semantic similarity to the contents generated by LCLMs. We
for the first time propose six membership inference attack (MIA) strategies
tailored for LCLMs and conduct extensive experiments on various popular models.
Empirical results demonstrate that our attacks can accurately infer membership
status in most cases, e.g., 90.66% attack F1-score on Multi-document QA
datasets with LongChat-7b-v1.5-32k, highlighting significant risks of
membership leakage within LCLMs input contexts. Furthermore, we examine the
underlying reasons why LCLMs are susceptible to revealing such membership
information.

摘要：<paragraph>大型語言模型 (LLM) 的最新進展已讓它們克服
其背景窗口限制，並在較長的背景中展現出非凡的檢索和
推理能力。加強 Long-Context Language Models (LCLM) 的問題解答系統可以自動搜尋大量
外部資料並將其納入其背景中，進而實現忠實
預測並減少幻覺和知識老舊等問題。針對 LCLM 的現有研究主要專注於解決所謂的
迷失在中間問題或改善推論效率，在很大程度上未探討其
隱私風險。在本文中，我們旨在彌合這一差距並
論證將所有資訊整合到長背景中會使其成為敏感資訊的儲存庫，其中通常包含私人資料，例如
病歷或個人身分。我們進一步調查 LCLM 外部背景中的成員身分隱私，目的是確定給定的
文件或序列是否包含在 LCLM 背景中。我們的基本想法是
如果文件位於背景中，它將展現出較低的產生損失
或與 LCLM 產生的內容有高度語義相似性。我們
首次提出六種針對 LCLM 量身打造的成員身分推論攻擊 (MIA) 策略，並對各種熱門模型進行廣泛的實驗。
實證結果證明，我們的攻擊在多數情況下可以準確推論成員身分狀態，例如，在使用 LongChat-7b-v1.5-32k 的多文件問答
資料集上攻擊 F1 得分為 90.66%，突顯了 LCLM 輸入背景中成員身分洩漏的重大風險。此外，我們探討了 LCLM 容易揭露此類成員身分
資訊的根本原因。</paragraph>

##### **IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos**
2411.11409v1 by Yunong Liu, Cristobal Eyzaguirre, Manling Li, Shubh Khanna, Juan Carlos Niebles, Vineeth Ravi, Saumitra Mishra, Weiyu Liu, Jiajun Wu

Shape assembly is a ubiquitous task in daily life, integral for constructing
complex 3D structures like IKEA furniture. While significant progress has been
made in developing autonomous agents for shape assembly, existing datasets have
not yet tackled the 4D grounding of assembly instructions in videos, essential
for a holistic understanding of assembly in 3D space over time. We introduce
IKEA Video Manuals, a dataset that features 3D models of furniture parts,
instructional manuals, assembly videos from the Internet, and most importantly,
annotations of dense spatio-temporal alignments between these data modalities.
To demonstrate the utility of IKEA Video Manuals, we present five applications
essential for shape assembly: assembly plan generation, part-conditioned
segmentation, part-conditioned pose estimation, video object segmentation, and
furniture assembly based on instructional video manuals. For each application,
we provide evaluation metrics and baseline methods. Through experiments on our
annotated data, we highlight many challenges in grounding assembly instructions
in videos to improve shape assembly, including handling occlusions, varying
viewpoints, and extended assembly sequences.

摘要：形狀組裝在日常生活中是一項普遍的任務，對於建構像 IKEA 家具一樣複雜的 3D 結構至關重要。儘管在開發形狀組裝的自主代理方面已取得顯著進展，但現有的資料集尚未解決影片中組裝說明的 4D 基礎，這對於整體了解 3D 空間中隨時間推移的組裝至關重要。我們介紹了 IKEA Video Manuals，這是一個資料集，其中包含家具零件的 3D 模型、教學手冊、來自網際網路的組裝影片，最重要的是，這些資料模式之間密集時空對齊的註解。為了展示 IKEA Video Manuals 的實用性，我們提出了形狀組裝的五項基本應用：組裝計畫產生、零件條件分割、零件條件姿勢估計、影片物件分割，以及基於教學影片手冊的家具組裝。對於每個應用程式，我們提供評估指標和基準方法。透過我們註解資料的實驗，我們強調了在影片中建立組裝說明以改善形狀組裝的許多挑戰，包括處理遮擋、不同的視角和延伸的組裝順序。

##### **The GECo algorithm for Graph Neural Networks Explanation**
2411.11391v1 by Salvatore Calderaro, Domenico Amato, Giosuè Lo Bosco, Riccardo Rizzo, Filippo Vella

Graph Neural Networks (GNNs) are powerful models that can manage complex data
sources and their interconnection links. One of GNNs' main drawbacks is their
lack of interpretability, which limits their application in sensitive fields.
In this paper, we introduce a new methodology involving graph communities to
address the interpretability of graph classification problems. The proposed
method, called GECo, exploits the idea that if a community is a subset of graph
nodes densely connected, this property should play a role in graph
classification. This is reasonable, especially if we consider the
message-passing mechanism, which is the basic mechanism of GNNs. GECo analyzes
the contribution to the classification result of the communities in the graph,
building a mask that highlights graph-relevant structures. GECo is tested for
Graph Convolutional Networks on six artificial and four real-world graph
datasets and is compared to the main explainability methods such as
PGMExplainer, PGExplainer, GNNExplainer, and SubgraphX using four different
metrics. The obtained results outperform the other methods for artificial graph
datasets and most real-world datasets.

摘要：圖形神經網路 (GNN) 是強大的模型，可以管理複雜的資料來源及其互連連結。GNN 的主要缺點之一是它們缺乏可解釋性，這限制了它們在敏感領域的應用。在本文中，我們引入了一種新的方法，涉及圖形社群，以解決圖形分類問題的可解釋性。所提出的方法稱為 GECo，它利用了這樣的想法：如果一個社群是圖形節點的子集，且連接緊密，則此屬性應在圖形分類中發揮作用。這是合理的，特別是如果我們考慮訊息傳遞機制，這是 GNN 的基本機制。GECo 分析了社群對圖形分類結果的貢獻，建立了一個遮罩來突顯與圖形相關的結構。GECo 在六個人工和四個真實世界圖形資料集上測試了圖形卷積網路，並使用四種不同的指標與主要的解釋性方法（例如 PGMExplainer、PGExplainer、GNNExplainer 和 SubgraphX）進行比較。獲得的結果優於人工圖形資料集和其他大多數真實世界資料集的其他方法。

##### **Rethinking Thinking Tokens: Understanding Why They Underperform in Practice**
2411.11371v1 by Sreeram Vennam, David Valente, David Herel, Ponnurangam Kumaraguru

Thinking Tokens (TT) have been proposed as an unsupervised method to
facilitate reasoning in language models. However, despite their conceptual
appeal, our findings show that TTs marginally improves performance and
consistently underperforms compared to Chain-of-Thought (CoT) reasoning across
multiple benchmarks. We hypothesize that this underperformance stems from the
reliance on a single embedding for TTs, which results in inconsistent learning
signals and introduces noisy gradients. This paper provides a comprehensive
empirical analysis to validate this hypothesis and discusses the implications
for future research on unsupervised reasoning in LLMs.

摘要：思考代幣 (TT) 已被提議作為一種非監督式方法，以促進語言模型中的推理。然而，儘管其概念吸引力，我們的發現顯示，TT 僅微幅提升效能，且在多個基準測試中持續表現不如基於思考鏈 (CoT) 的推理。我們假設這種表現不佳源自於對 TT 的單一嵌入依賴，這導致不一致的學習訊號並引入了雜訊梯度。本文提供了全面的經驗分析，以驗證此假設，並討論對未來 LLM 中非監督式推理研究的影響。

##### **Continual Task Learning through Adaptive Policy Self-Composition**
2411.11364v1 by Shengchao Hu, Yuhang Zhou, Ziqing Fan, Jifeng Hu, Li Shen, Ya Zhang, Dacheng Tao

Training a generalizable agent to continually learn a sequence of tasks from
offline trajectories is a natural requirement for long-lived agents, yet
remains a significant challenge for current offline reinforcement learning (RL)
algorithms. Specifically, an agent must be able to rapidly adapt to new tasks
using newly collected trajectories (plasticity), while retaining knowledge from
previously learned tasks (stability). However, systematic analyses of this
setting are scarce, and it remains unclear whether conventional continual
learning (CL) methods are effective in continual offline RL (CORL) scenarios.
In this study, we develop the Offline Continual World benchmark and demonstrate
that traditional CL methods struggle with catastrophic forgetting, primarily
due to the unique distribution shifts inherent to CORL scenarios. To address
this challenge, we introduce CompoFormer, a structure-based continual
transformer model that adaptively composes previous policies via a meta-policy
network. Upon encountering a new task, CompoFormer leverages semantic
correlations to selectively integrate relevant prior policies alongside newly
trained parameters, thereby enhancing knowledge sharing and accelerating the
learning process. Our experiments reveal that CompoFormer outperforms
conventional CL methods, particularly in longer task sequences, showcasing a
promising balance between plasticity and stability.

摘要：訓練一個可概括化的代理人持續從離線軌跡中學習一系列任務，是長期代理人的自然需求，但對於目前的離線強化學習 (RL) 演算法來說仍然是一項重大的挑戰。具體來說，代理人必須能夠使用新收集的軌跡（可塑性）快速適應新任務，同時保留從先前學習任務中獲得的知識（穩定性）。然而，對此設定的系統分析很少，而且傳統的持續學習 (CL) 方法是否在持續離線 RL (CORL) 場景中有效，仍不清楚。在這項研究中，我們開發了離線持續世界基準，並證明傳統的 CL 方法會面臨災難性遺忘，這主要是由於 CORL 場景固有的獨特分佈轉移。為了應對這項挑戰，我們引入了 CompoFormer，這是一個基於結構的持續轉換器模型，可透過元政策網路自適應地組合先前的政策。在遇到新任務時，CompoFormer 會利用語義關聯性，有選擇性地將相關的先前政策與新訓練的參數整合在一起，從而增強知識共享並加速學習過程。我們的實驗表明，CompoFormer 優於傳統的 CL 方法，特別是在較長的任務序列中，展示了可塑性和穩定性之間的平衡。

##### **MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models**
2411.11362v1 by Harshita Sharma, Valentina Salvatelli, Shaury Srivastav, Kenza Bouzid, Shruthi Bannur, Daniel C. Castro, Maximilian Ilse, Sam Bond-Taylor, Mercy Prasanna Ranjit, Fabian Falck, Fernando Pérez-García, Anton Schwaighofer, Hannah Richardson, Maria Teodora Wetscherek, Stephanie L. Hyland, Javier Alvarez-Valle

There is growing interest in applying AI to radiology report generation,
particularly for chest X-rays (CXRs). This paper investigates whether
incorporating pixel-level information through segmentation masks can improve
fine-grained image interpretation of multimodal large language models (MLLMs)
for radiology report generation. We introduce MAIRA-Seg, a segmentation-aware
MLLM framework designed to utilize semantic segmentation masks alongside CXRs
for generating radiology reports. We train expert segmentation models to obtain
mask pseudolabels for radiology-specific structures in CXRs. Subsequently,
building on the architectures of MAIRA, a CXR-specialised model for report
generation, we integrate a trainable segmentation tokens extractor that
leverages these mask pseudolabels, and employ mask-aware prompting to generate
draft radiology reports. Our experiments on the publicly available MIMIC-CXR
dataset show that MAIRA-Seg outperforms non-segmentation baselines. We also
investigate set-of-marks prompting with MAIRA and find that MAIRA-Seg
consistently demonstrates comparable or superior performance. The results
confirm that using segmentation masks enhances the nuanced reasoning of MLLMs,
potentially contributing to better clinical outcomes.

摘要：放射學報告生成中對應用 AI 的興趣與日俱增，特別是對於胸部 X 光片 (CXR)。本文探討是否透過分段遮罩納入畫素層級資訊，可以改善多模態大型語言模型 (MLLM) 的細緻影像解讀，用於放射學報告生成。我們引入 MAIRA-Seg，一個具備區分感知的分段 MLLM 架構，旨在將語意分段遮罩與 CXR 一起用於產生放射學報告。我們訓練專家分段模型，以取得 CXR 中放射學特定結構的遮罩偽標籤。隨後，建構在 MAIRA 的架構上，一個專門用於報告生成的 CXR 模型，我們整合一個可訓練的分段標記萃取器，利用這些遮罩偽標籤，並採用具備遮罩感知的提示來產生放射學報告草稿。我們在公開的 MIMIC-CXR 資料集上進行的實驗顯示，MAIRA-Seg 優於非分段基準。我們也探討了具備 MAIRA 的標記集合提示，發現 MAIRA-Seg 持續展現可比較的或更優異的效能。結果證實，使用分段遮罩增強了 MLLM 的細緻推理，潛在有助於更好的臨床結果。

##### **Mitigating Knowledge Conflicts in Language Model-Driven Question Answering**
2411.11344v1 by Han Cao, Zhaoyang Zhang, Xiangtian Li, Chufan Wu, Hansong Zhang, Wenqing Zhang

Knowledge-aware sequence to sequence generation tasks such as document
question answering and abstract summarization typically requires two types of
knowledge: encoded parametric knowledge and retrieved contextual information.
Previous work show improper correlation between parametric knowledge and
answers in the training set could cause the model ignore input information at
test time, resulting in un-desirable model behaviour such as over-stability and
hallucination. In this work, we argue that hallucination could be mitigated via
explicit correlation between input source and generated content. We focus on a
typical example of hallucination, entity-based knowledge conflicts in question
answering, where correlation of entities and their description at training time
hinders model behaviour during inference.

摘要：知識感知序列到序列生成任務，例如文件問題回答和摘要摘要，通常需要兩種類型的知識：編碼參數知識和檢索的上下文信息。
先前的工作表明，訓練集中參數知識和答案之間的不適當相關性可能會導致模型在測試時忽略輸入信息，從而導致不希望的模型行為，例如過度穩定性和幻覺。在這項工作中，我們認為幻覺可以通過輸入源和生成內容之間的明確相關性來減輕。我們專注於幻覺的一個典型例子，即問題回答中的基於實體的知識衝突，其中實體及其在訓練時間的描述的相關性會阻礙推理過程中的模型行為。

##### **TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation**
2411.11305v1 by Ranmin Wang, Limin Zhuang, Hongkun Chen, Boyan Xu, Ruichu Cai

The advancement of medical image segmentation techniques has been propelled
by the adoption of deep learning techniques, particularly UNet-based
approaches, which exploit semantic information to improve the accuracy of
segmentations. However, the order of organs in scanned images has been
disregarded by current medical image segmentation approaches based on UNet.
Furthermore, the inherent network structure of UNet does not provide direct
capabilities for integrating temporal information. To efficiently integrate
temporal information, we propose TP-UNet that utilizes temporal prompts,
encompassing organ-construction relationships, to guide the segmentation UNet
model. Specifically, our framework is featured with cross-attention and
semantic alignment based on unsupervised contrastive learning to combine
temporal prompts and image features effectively. Extensive evaluations on two
medical image segmentation datasets demonstrate the state-of-the-art
performance of TP-UNet. Our implementation will be open-sourced after
acceptance.

摘要：醫療影像分割技術的進步，是由深度學習技術的採用推動的，特別是基於 UNet 的方法，它利用語義資訊來提高分割的準確性。然而，目前基於 UNet 的醫學影像分割方法忽略了掃描影像中器官的順序。此外，UNet 的固有網路結構不提供整合時間資訊的直接能力。為了有效整合時間資訊，我們提出 TP-UNet，它利用時間提示，包含器官建構關係，來引導分割 UNet 模型。具體來說，我們的架構以基於無監督對比學習的交叉注意力和語義對齊為特色，以有效結合時間提示和影像特徵。在兩個醫學影像分割資料集上的廣泛評估證明了 TP-UNet 的最先進效能。我們的實作將在接受後開放原始碼。

##### **Recurrent Stochastic Configuration Networks with Incremental Blocks**
2411.11303v1 by Gang Dang, Dainhui Wang

Recurrent stochastic configuration networks (RSCNs) have shown promise in
modelling nonlinear dynamic systems with order uncertainty due to their
advantages of easy implementation, less human intervention, and strong
approximation capability. This paper develops the original RSCNs with block
increments, termed block RSCNs (BRSCNs), to further enhance the learning
capacity and efficiency of the network. BRSCNs can simultaneously add multiple
reservoir nodes (subreservoirs) during the construction. Each subreservoir is
configured with a unique structure in the light of a supervisory mechanism,
ensuring the universal approximation property. The reservoir feedback matrix is
appropriately scaled to guarantee the echo state property of the network.
Furthermore, the output weights are updated online using a projection
algorithm, and the persistent excitation conditions that facilitate parameter
convergence are also established. Numerical results over a time series
prediction, a nonlinear system identification task, and two industrial data
predictive analyses demonstrate that the proposed BRSCN performs favourably in
terms of modelling efficiency, learning, and generalization performance,
highlighting their significant potential for coping with complex dynamics.

摘要：遞迴隨機組態網路 (RSCN) 在建模具有階數不確定性的非線性動態系統方面已展現出潛力，因為它具有易於實作、減少人為干預和強大的近似能力等優點。本文以區塊增量開發原始 RSCN，稱為區塊 RSCN (BRSCN)，以進一步增強網路的學習能力和效率。BRSCN 能在建構期間同時加入多個儲存節點 (次儲存器)。每個次儲存器根據監督機制配置有獨特結構，確保通用近似屬性。儲存器回饋矩陣適當縮放以保證網路的迴響狀態屬性。此外，輸出權重使用投影演算法進行線上更新，並建立促進參數收斂的持續激勵條件。時間序列預測、非線性系統辨識任務和兩個產業資料預測分析的數值結果證明，所提出的 BRSCN 在建模效率、學習和一般化效能方面表現良好，突顯出其應付複雜動態的顯著潛力。

##### **Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation**
2411.11295v1 by Peng Shu, Junhao Chen, Zhengliang Liu, Hui Wang, Zihao Wu, Tianyang Zhong, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Yifan Zhou, Constance Owl, Xiaoming Zhai, Ninghao Liu, Claudio Saunt, Tianming Liu

Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of tasks and domains. However, their performance in low-resource
language translation, particularly when translating into these languages,
remains underexplored. This gap poses significant challenges, as linguistic
barriers hinder the cultural preservation and development of minority
communities. To address this issue, this paper introduces a novel
retrieval-based method that enhances translation quality for low-resource
languages by focusing on key terms, which involves translating keywords and
retrieving corresponding examples from existing data. To evaluate the
effectiveness of this method, we conducted experiments translating from English
into three low-resource languages: Cherokee, a critically endangered indigenous
language of North America; Tibetan, a historically and culturally significant
language in Asia; and Manchu, a language with few remaining speakers. Our
comparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B,
highlights the significant challenges these models face when translating into
low-resource languages. In contrast, our retrieval-based method shows promise
in improving both word-level accuracy and overall semantic understanding by
leveraging existing resources more effectively.

摘要：大型語言模型 (LLM) 已展現出在廣泛任務與領域的卓越成就。然而，它們在低資源語言翻譯方面的表現，特別是翻譯成這些語言時，仍未得到充分的探索。由於語言障礙阻礙了少數族群的文化保存和發展，因此這個差距帶來了重大的挑戰。為了解決這個問題，本文介紹了一種創新的基於檢索的方法，透過專注於關鍵術語來增強低資源語言的翻譯品質，這包括翻譯關鍵字和從現有資料中檢索對應的範例。為了評估這種方法的有效性，我們進行了從英語翻譯成三種低資源語言的實驗：切羅基語，一種瀕臨滅絕的北美原住民語言；藏語，一種在亞洲具有歷史和文化意義的語言；以及滿語，一種剩餘的使用者很少的語言。我們與 GPT-4o 和 LLaMA 3.1 405B 的零次學習表現進行比較，強調了這些模型在翻譯成低資源語言時所面臨的重大挑戰。相比之下，我們的基於檢索的方法顯示出透過更有效地利用現有資源來改善詞彙層級的準確度和整體語義理解的潛力。

##### **LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large Language Models**
2411.11289v1 by Yungi Kim, Hyunsoo Ha, Seonghoon Yang, Sukyung Lee, Jihoo Kim, Chanjun Park

Creating high-quality, large-scale datasets for large language models (LLMs)
often relies on resource-intensive, GPU-accelerated models for quality
filtering, making the process time-consuming and costly. This dependence on
GPUs limits accessibility for organizations lacking significant computational
infrastructure. To address this issue, we introduce the Lightweight,
Purpose-driven (LP) Data Pipeline, a framework that operates entirely on CPUs
to streamline the processes of dataset extraction, filtering, and curation.
Based on our four core principles, the LP Data Pipeline significantly reduces
preparation time and cost while maintaining high data quality. Importantly, our
pipeline enables the creation of purpose-driven datasets tailored to specific
domains and languages, enhancing the applicability of LLMs in specialized
contexts. We anticipate that our pipeline will lower the barriers to LLM
development, enabling a wide range of organizations to access LLMs more easily.

摘要：建立高品質、大規模的大語言模型 (LLM) 資料集，通常仰賴耗費資源、GPU 加速的模型進行品質過濾，使得這個過程耗時且昂貴。對缺乏大量運算基礎建設的組織而言，這種對 GPU 的依賴限制了其可近性。為了解決這個問題，我們引進了輕量化、目的導向 (LP) 資料管道，這是一個完全在 CPU 上運作的架構，用於簡化資料集萃取、過濾和整理的流程。根據我們的四項核心原則，LP 資料管道大幅減少了準備時間和成本，同時維持高資料品質。重要的是，我們的管道能建立針對特定領域和語言量身打造的目的導向資料集，增強 LLM 在特定脈絡中的適用性。我們預期我們的管道將降低 LLM 開發的門檻，讓廣泛的組織能更輕易地存取 LLM。

##### **Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development**
2411.11285v1 by Ranjan Sapkota, Achyut Paudel, Manoj Karkee

Currently, deep learning-based instance segmentation for various applications
(e.g., Agriculture) is predominantly performed using a labor-intensive process
involving extensive field data collection using sophisticated sensors, followed
by careful manual annotation of images, presenting significant logistical and
financial challenges to researchers and organizations. The process also slows
down the model development and training process. In this study, we presented a
novel method for deep learning-based instance segmentation of apples in
commercial orchards that eliminates the need for labor-intensive field data
collection and manual annotation. Utilizing a Large Language Model (LLM), we
synthetically generated orchard images and automatically annotated them using
the Segment Anything Model (SAM) integrated with a YOLO11 base model. This
method significantly reduces reliance on physical sensors and manual data
processing, presenting a major advancement in "Agricultural AI". The synthetic,
auto-annotated dataset was used to train the YOLO11 model for Apple instance
segmentation, which was then validated on real orchard images. The results
showed that the automatically generated annotations achieved a Dice Coefficient
of 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask
annotations. All YOLO11 configurations, trained solely on these synthetic
datasets with automated annotations, accurately recognized and delineated
apples, highlighting the method's efficacy. Specifically, the YOLO11m-seg
configuration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on
test images collected from a commercial orchard. Additionally, the YOLO11l-seg
configuration outperformed other models in validation on 40 LLM-generated
images, achieving the highest mask precision and mAP@50 metrics.
  Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM

摘要：<paragraph>目前，針對各種應用（例如農業）的深度學習實例分割，主要透過勞力密集的程序執行，包括使用精密感測器廣泛收集現場資料，接著仔細手動標註影像，對研究人員和組織而言，這會造成顯著的後勤和財務挑戰。此程序也會減緩模型開發和訓練的過程。在此研究中，我們提出了一種創新的方法，可針對商業果園中的蘋果執行深度學習實例分割，無需勞力密集的現場資料收集和手動標註。我們利用大型語言模型 (LLM) 合成產生果園影像，並使用與 YOLO11 基礎模型整合的 Segment Anything Model (SAM) 自動標註這些影像。這種方法大幅降低對實體感測器和手動資料處理的依賴性，代表「農業 AI」的一大進步。合成自動標註的資料集用於訓練 YOLO11 模型，以進行蘋果實例分割，接著在真實果園影像中驗證。結果顯示，自動產生的標註達到了 0.9513 的 Dice 係數和 0.9303 的 IoU，驗證了遮罩標註的準確性和重疊性。所有 YOLO11 組態僅使用這些具有自動化標註的合成資料集進行訓練，就能準確辨識和描繪蘋果，突顯了此方法的效能。具體來說，YOLO11m-seg 組態在從商業果園收集的測試影像上達到了 0.902 的遮罩準確度和 0.833 的遮罩 mAP@50。此外，YOLO11l-seg 組態在針對 40 張 LLM 生成的影像進行驗證時，優於其他模型，達到了最高的遮罩準確度和 mAP@50 指標。
關鍵字：YOLO、SAM、SAMv2、YOLO11、YOLOv11、Segment Anything、YOLO-SAM</paragraph>

##### **Multi-Hyperbolic Space-based Heterogeneous Graph Attention Network**
2411.11283v1 by Jongmin Park, Seunghoon Han, Jong-Ryul Lee, Sungsu Lim

To leverage the complex structures within heterogeneous graphs, recent
studies on heterogeneous graph embedding use a hyperbolic space, characterized
by a constant negative curvature and exponentially increasing space, which
aligns with the structural properties of heterogeneous graphs. However, despite
heterogeneous graphs inherently possessing diverse power-law structures, most
hyperbolic heterogeneous graph embedding models use a single hyperbolic space
for the entire heterogeneous graph, which may not effectively capture the
diverse power-law structures within the heterogeneous graph. To address this
limitation, we propose Multi-hyperbolic Space-based heterogeneous Graph
Attention Network (MSGAT), which uses multiple hyperbolic spaces to effectively
capture diverse power-law structures within heterogeneous graphs. We conduct
comprehensive experiments to evaluate the effectiveness of MSGAT. The
experimental results demonstrate that MSGAT outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

摘要：為了利用異質圖形中的複雜結構，異質圖形嵌入的近期研究使用雙曲空間，其特徵是恆定的負曲率和指數增長的空間，這與異質圖形的結構性質一致。然而，儘管異質圖形本質上擁有不同的冪律結構，但大多數雙曲異質圖形嵌入模型對整個異質圖形使用單一的雙曲空間，這可能無法有效捕捉異質圖形中的不同冪律結構。為了解決這個限制，我們提出基於多重雙曲空間的異質圖形注意力網路 (MSGAT)，它使用多重雙曲空間來有效捕捉異質圖形中的不同冪律結構。我們進行全面的實驗來評估 MSGAT 的有效性。實驗結果表明，MSGAT 在各種圖形機器學習任務中優於最先進的基準，有效地捕捉了異質圖形的複雜結構。

##### **Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction**
2411.11282v1 by Yucong Meng, Zhiwei Yang, Minghong Duan, Yonghong Shi, Zhijian Song

Magnetic resonance imaging (MRI) is a crucial tool for clinical diagnosis
while facing the challenge of long scanning time. To reduce the acquisition
time, fast MRI reconstruction aims to restore high-quality images from the
undersampled k-space. Existing methods typically train deep learning models to
map the undersampled data to artifact-free MRI images. However, these studies
often overlook the unique properties of k-space and directly apply general
networks designed for image processing to k-space recovery, leaving the precise
learning of k-space largely underexplored. In this work, we propose a
continuous k-space recovery network from a new perspective of implicit neural
representation with image domain guidance, which boosts the performance of MRI
reconstruction. Specifically, (1) an implicit neural representation based
encoder-decoder structure is customized to continuously query unsampled
k-values. (2) an image guidance module is designed to mine the semantic
information from the low-quality MRI images to further guide the k-space
recovery. (3) a multi-stage training strategy is proposed to recover dense
k-space progressively. Extensive experiments conducted on CC359, fastMRI, and
IXI datasets demonstrate the effectiveness of our method and its superiority
over other competitors.

摘要：磁共振成像 (MRI) 對於臨床診斷至關重要，但卻面臨掃描時間長的問題。為了縮短擷取時間，快速 MRI 重建旨在從欠採樣 k 空間恢復高品質影像。現有方法通常訓練深度學習模型，將欠採樣資料對應到沒有偽影的 MRI 影像。然而，這些研究常常忽略 k 空間的獨特屬性，並直接套用設計用於影像處理的一般網路到 k 空間重建，導致 k 空間的精確學習在很大程度上仍未被探索。在這項工作中，我們從隱式神經表徵與影像網域引導的新觀點提出一個連續的 k 空間重建網路，提升 MRI 重建的效能。具體來說，(1) 根據隱式神經表徵設計編碼器-解碼器結構，用於連續查詢未採樣的 k 值。(2) 設計一個影像引導模組，從低品質的 MRI 影像中挖掘語義資訊，進一步引導 k 空間重建。(3) 提出一個多階段訓練策略，用於逐步重建密集的 k 空間。在 CC359、fastMRI 和 IXI 資料集上進行的大量實驗證明了我們方法的有效性，以及其優於其他競爭對手的優越性。

##### **VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently**
2411.11266v1 by Keer Lu, Keshi Zhao, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang

Large Language Models (LLMs) exhibit remarkable capabilities in handling
multiple tasks across domains due to their emergent properties. These
capabilities are further augmented during the Supervised Fine-Tuning (SFT)
phase. Despite their potential, existing work mainly focuses on domain-specific
enhancements during fine-tuning, the challenge of which lies in catastrophic
forgetting of knowledge across other domains. In this study, we introduce
VersaTune, a novel data composition framework designed for enhancing LLMs'
overall multi-ability performances during fine-tuning. We categorize knowledge
into distinct domains including law, medicine, finance, science, code. We begin
with detecting the distribution of domain-specific knowledge within the base
model, followed by the composition of training data that aligns with the
model's existing knowledge distribution. During the fine-tuning process,
weights of different domains are dynamically adjusted based on their learnable
potential and forgetting degree. Experimental results demonstrate that
VersaTune achieves significant improvements in multi-domain performance, with a
35.21% enhancement in comprehensive multi-domain tasks. Additionally, in
scenarios where specific domain optimization is required, VersaTune reduces the
degradation of performance in other domains by 38.77%, without compromising the
target domain's training efficacy.

摘要：大型語言模型（LLM）由於其新興屬性，在處理跨領域的多項任務時表現出顯著的能力。這些能力在監督微調（SFT）階段進一步增強。儘管具有潛力，現有工作主要集中於微調期間特定領域的增強，其挑戰在於跨其他領域的知識災難性遺忘。在本研究中，我們引入了 VersaTune，這是一個新穎的數據組合框架，旨在增強 LLM 在微調期間的整體多能力表現。我們將知識分類為不同的領域，包括法律、醫學、金融、科學、程式碼。我們從檢測基礎模型中特定領域知識的分布開始，然後組合與模型現有知識分布一致的訓練資料。在微調過程中，不同領域的權重會根據其可學習的潛力與遺忘程度進行動態調整。實驗結果表明，VersaTune 在多領域效能方面取得了顯著的進步，在綜合多領域任務中提升了 35.21%。此外，在需要特定領域最佳化的場景中，VersaTune 將其他領域的效能下降降低了 38.77%，而不會影響目標領域的訓練效能。

##### **Cross-Patient Pseudo Bags Generation and Curriculum Contrastive Learning for Imbalanced Multiclassification of Whole Slide Image**
2411.11262v1 by Yonghuang Wu, Xuan Xie, Xinyuan Niu, Chengqian Zhao, Jinhua Yu

Pathology computing has dramatically improved pathologists' workflow and
diagnostic decision-making processes. Although computer-aided diagnostic
systems have shown considerable value in whole slide image (WSI) analysis, the
problem of multi-classification under sample imbalance remains an intractable
challenge. To address this, we propose learning fine-grained information by
generating sub-bags with feature distributions similar to the original WSIs.
Additionally, we utilize a pseudo-bag generation algorithm to further leverage
the abundant and redundant information in WSIs, allowing efficient training in
unbalanced-sample multi-classification tasks. Furthermore, we introduce an
affinity-based sample selection and curriculum contrastive learning strategy to
enhance the stability of model representation learning. Unlike previous
approaches, our framework transitions from learning bag-level representations
to understanding and exploiting the feature distribution of multi-instance
bags. Our method demonstrates significant performance improvements on three
datasets, including tumor classification and lymph node metastasis. On average,
it achieves a 4.39-point improvement in F1 score compared to the second-best
method across the three tasks, underscoring its superior performance.

摘要：病理计算已大幅改善病理学家的工作流程和诊断决策过程。虽然计算机辅助诊断系统已在全切片图像 (WSI) 分析中显示出相当大的价值，但样本不平衡下的多分类问题仍然是一个棘手的挑战。为了解决这个问题，我们提出通过生成具有类似于原始 WSI 的特征分布的子袋来学习细粒度信息。此外，我们利用伪袋生成算法来进一步利用 WSI 中丰富且冗余的信息，从而允许在不平衡样本多分类任务中进行高效训练。此外，我们引入了一种基于亲和力的样本选择和课程对比学习策略来增强模型表示学习的稳定性。与以前的方法不同，我们的框架从学习袋级表示转换到理解和利用多实例袋的特征分布。我们的方法在三个数据集上展示了显著的性能提升，包括肿瘤分类和淋巴结转移。平均而言，与三个任务中的第二好方法相比，它的 F1 得分提高了 4.39 分，突出了其卓越的性能。

##### **Large corpora and large language models: a replicable method for automating grammatical annotation**
2411.11260v1 by Cameron Morin, Matti Marttinen Larsson

Much linguistic research relies on annotated datasets of features extracted
from text corpora, but the rapid quantitative growth of these corpora has
created practical difficulties for linguists to manually annotate large data
samples. In this paper, we present a replicable, supervised method that
leverages large language models for assisting the linguist in grammatical
annotation through prompt engineering, training, and evaluation. We introduce a
methodological pipeline applied to the case study of formal variation in the
English evaluative verb construction 'consider X (as) (to be) Y', based on the
large language model Claude 3.5 Sonnet and corpus data from Davies' NOW and
EnTenTen21 (SketchEngine). Overall, we reach a model accuracy of over 90% on
our held-out test samples with only a small amount of training data, validating
the method for the annotation of very large quantities of tokens of the
construction in the future. We discuss the generalisability of our results for
a wider range of case studies of grammatical constructions and grammatical
variation and change, underlining the value of AI copilots as tools for future
linguistic research.

摘要：許多語言學研究依賴從文本語料庫中擷取特徵的註解資料集，但這些語料庫的快速數量成長已為語言學家手動註解大型資料樣本造成實際困難。在本文中，我們提出一個可複製的監督式方法，該方法利用大型語言模型透過提示工程、訓練和評估來協助語言學家進行語法註解。我們介紹了一個方法論管道，應用於英語評估動詞結構「consider X (as) (to be) Y」形式變化的案例研究，基於大型語言模型 Claude 3.5 Sonnet 和來自 Davies' NOW 和 EnTenTen21（SketchEngine）的語料庫資料。總體而言，我們在持出測試樣本上達到超過 90% 的模型準確度，僅使用少量訓練資料，驗證了未來註解大量結構標記的方法。我們討論了我們的結果對更廣泛的語法結構和語法變異和變化的案例研究的一般化，強調了人工智慧副駕駛作為未來語言學研究工具的價值。

##### **EXCON: Extreme Instance-based Contrastive Representation Learning of Severely Imbalanced Multivariate Time Series for Solar Flare Prediction**
2411.11249v1 by Onur Vural, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi

In heliophysics research, predicting solar flares is crucial due to their
potential to impact both space-based systems and Earth's infrastructure
substantially. Magnetic field data from solar active regions, recorded by solar
imaging observatories, are transformed into multivariate time series to enable
solar flare prediction using temporal window-based analysis. In the realm of
multivariate time series-driven solar flare prediction, addressing severe class
imbalance with effective strategies for multivariate time series representation
learning is key to developing robust predictive models. Traditional methods
often struggle with overfitting to the majority class in prediction tasks where
major solar flares are infrequent. This work presents EXCON, a contrastive
representation learning framework designed to enhance classification
performance amidst such imbalances. EXCON operates through four stages:
obtaining core features from multivariate time series data; selecting
distinctive contrastive representations for each class to maximize inter-class
separation; training a temporal feature embedding module with a custom extreme
reconstruction loss to minimize intra-class variation; and applying a
classifier to the learned embeddings for robust classification. The proposed
method leverages contrastive learning principles to map similar instances
closer in the feature space while distancing dissimilar ones, a strategy not
extensively explored in solar flare prediction tasks. This approach not only
addresses class imbalance but also offers a versatile solution applicable to
univariate and multivariate time series across binary and multiclass
classification problems. Experimental results, including evaluations on the
benchmark solar flare dataset and multiple time series archive datasets with
binary and multiclass labels, demonstrate EXCON's efficacy in enhancing
classification performance.

摘要：<paragraph>在太陽物理研究中，預測太陽耀斑至關重要，因為它們有可能對太空系統和地球基礎設施造成重大影響。由太陽成像觀測站記錄的太陽活動區域的磁場數據被轉換為多變量時間序列，以使用基於時間窗口的分析來預測太陽耀斑。在多變量時間序列驅動的太陽耀斑預測領域，使用有效的多變量時間序列表示學習策略來解決嚴重的類別不平衡是開發穩健預測模型的關鍵。傳統方法通常難以應對預測任務中多數類別的過度擬合，其中主要的太陽耀斑並不頻繁。這項工作提出了 EXCON，這是一個對比表示學習框架，旨在提高此類不平衡中的分類性能。EXCON 通過四個階段運作：從多變量時間序列數據中獲取核心特徵；為每個類別選擇獨特的對比表示，以最大化類間分離；使用自定義極端重建損失訓練時間特徵嵌入模塊，以最小化類內變異；並將分類器應用於學習的嵌入，以進行穩健分類。所提出的方法利用對比學習原理將相似的實例在特徵空間中映射得更近，同時遠離不同的實例，這是一種在太陽耀斑預測任務中尚未廣泛探索的策略。這種方法不僅解決了類別不平衡，還提供了一個通用的解決方案，適用於二元和多類別分類問題中的單變量和多變量時間序列。實驗結果，包括對基準太陽耀斑數據集和具有二元和多類別標籤的多分時間序列歸檔數據集的評估，證明了 EXCON 在增強分類性能方面的功效。</paragraph>

##### **ZeFaV: Boosting Large Language Models for Zero-shot Fact Verification**
2411.11247v1 by Son T. Luu, Hiep Nguyen, Trung Vo, Le-Minh Nguyen

In this paper, we propose ZeFaV - a zero-shot based fact-checking
verification framework to enhance the performance on fact verification task of
large language models by leveraging the in-context learning ability of large
language models to extract the relations among the entities within a claim,
re-organized the information from the evidence in a relationally logical form,
and combine the above information with the original evidence to generate the
context from which our fact-checking model provide verdicts for the input
claims. We conducted empirical experiments to evaluate our approach on two
multi-hop fact-checking datasets including HoVer and FEVEROUS, and achieved
potential results results comparable to other state-of-the-art fact
verification task methods.

摘要：在本文中，我們提出 ZeFaV - 一個基於零次學習的事實查核驗證框架，用於透過利用大型語言模型的上下文學習能力來增強大型語言模型在事實查核任務上的表現，以提取斷言中實體之間的關係，以關係邏輯形式重新組織證據中的資訊，並將上述資訊與原始證據結合起來，以產生我們的事實查核模型為輸入斷言提供判決的背景。我們進行了實證實驗，以評估我們在包括 HoVer 和 FEVEROUS 在內的兩個多跳事實查核資料集上的方法，並取得了與其他最先進的事實查核任務方法相當的潛在結果。

##### **MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis**
2411.11235v1 by Yingjie Zhou, Zicheng Zhang, Jiezhang Cao, Jun Jia, Yanwei Jiang, Farong Wen, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai

Artificial Intelligence (AI) has demonstrated significant capabilities in
various fields, and in areas such as human-computer interaction (HCI), embodied
intelligence, and the design and animation of virtual digital humans, both
practitioners and users are increasingly concerned with AI's ability to
understand and express emotion. Consequently, the question of whether AI can
accurately interpret human emotions remains a critical challenge. To date, two
primary classes of AI models have been involved in human emotion analysis:
generative models and Multimodal Large Language Models (MLLMs). To assess the
emotional capabilities of these two classes of models, this study introduces
MEMO-Bench, a comprehensive benchmark consisting of 7,145 portraits, each
depicting one of six different emotions, generated by 12 Text-to-Image (T2I)
models. Unlike previous works, MEMO-Bench provides a framework for evaluating
both T2I models and MLLMs in the context of sentiment analysis. Additionally, a
progressive evaluation approach is employed, moving from coarse-grained to
fine-grained metrics, to offer a more detailed and comprehensive assessment of
the sentiment analysis capabilities of MLLMs. The experimental results
demonstrate that existing T2I models are more effective at generating positive
emotions than negative ones. Meanwhile, although MLLMs show a certain degree of
effectiveness in distinguishing and recognizing human emotions, they fall short
of human-level accuracy, particularly in fine-grained emotion analysis. The
MEMO-Bench will be made publicly available to support further research in this
area.

摘要：人工智慧 (AI) 已在各領域展現顯著能力，而在人機互動 (HCI)、具身智慧，以及虛擬數位人類的設計與動畫等領域，實務工作者和使用者愈來愈關注 AI 理解和表達情緒的能力。因此，AI 是否能準確詮釋人類情緒的問題，仍是關鍵挑戰。迄今為止，兩大類型的 AI 模型已參與人類情緒分析：生成模型和多模態大型語言模型 (MMLM)。為了評估這兩類模型的情緒能力，本研究引入了 MEMO-Bench，這是一個由 7,145 張肖像組成的綜合基準測試，每一張都描繪了六種不同情緒之一，由 12 個文字轉圖像 (T2I) 模型產生。與先前的研究不同，MEMO-Bench 提供了一個評估 T2I 模型和 MMLM 在情緒分析背景下的架構。此外，採用漸進式評估方法，從粗略指標轉向細緻指標，以提供更詳細且全面的 MMLM 情緒分析能力評估。實驗結果顯示，現有的 T2I 模型在產生正面情緒方面比負面情緒更有效。同時，儘管 MMLM 在區分和辨識人類情緒方面展現一定程度的有效性，但仍未達到人類等級的準確度，特別是在細緻的情緒分析方面。MEMO-Bench 將公開提供，以支持此領域的進一步研究。

##### **MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs**
2411.11217v1 by Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng, Joseph E. Gonzalez, Matei Zaharia, Ion Stoica

Efficient deployment of large language models, particularly Mixture of
Experts (MoE), on resource-constrained platforms presents significant
challenges, especially in terms of computational efficiency and memory
utilization. The MoE architecture, renowned for its ability to increase model
capacity without a proportional increase in inference cost, greatly reduces the
token generation latency compared with dense models. However, the large model
size makes MoE models inaccessible to individuals without high-end GPUs. In
this paper, we propose a high-throughput MoE batch inference system, that
significantly outperforms past work. MoE-Lightning introduces a novel
CPU-GPU-I/O pipelining schedule, CGOPipe, with paged weights to achieve high
resource utilization, and a performance model, HRM, based on a Hierarchical
Roofline Model we introduce to help find policies with higher throughput than
existing systems. MoE-Lightning can achieve up to 10.3x higher throughput than
state-of-the-art offloading-enabled LLM inference systems for Mixtral 8x7B on a
single T4 GPU (16GB). When the theoretical system throughput is bounded by the
GPU memory, MoE-Lightning can reach the throughput upper bound with 2-3x less
CPU memory, significantly increasing resource utilization. MoE-Lightning also
supports efficient batch inference for much larger MoEs (e.g., Mixtral 8x22B
and DBRX) on multiple low-cost GPUs (e.g., 2-4 T4).

摘要：<paragraph>在資源受限的平台上有效部署大型語言模型，特別是混合專家 (MoE)，會帶來重大的挑戰，特別是在運算效率和記憶體使用方面。MoE 架構以其在不增加推論成本的情況下增加模型容量的能力而聞名，與密集模型相比，它大大降低了標記產生延遲。然而，龐大的模型規模使得沒有高端 GPU 的個人無法使用 MoE 模型。在本文中，我們提出一個高吞吐量的 MoE 批次推論系統，它顯著優於過去的工作。MoE-Lightning 引入了一個新穎的 CPU-GPU-I/O 管道排程，CGOPipe，使用分頁權重來實現高資源利用率，以及一個基於我們引入的分層屋頂線模型的效能模型 HRM，以幫助找到比現有系統具有更高吞吐量的策略。對於單個 T4 GPU（16GB）上的 Mixtral 8x7B，MoE-Lightning 可以比支援卸載的最新 LLM 推論系統實現高達 10.3 倍的吞吐量。當理論系統吞吐量受 GPU 記憶體限制時，MoE-Lightning 可以以少 2-3 倍的 CPU 記憶體達到吞吐量上限，從而顯著提高資源利用率。MoE-Lightning 還支援在多個低成本 GPU（例如 2-4 T4）上對更大的 MoE（例如 Mixtral 8x22B 和 DBRX）進行有效批次推論。</paragraph>

##### **Capturing Sparks of Abstraction for the ARC Challenge**
2411.11206v1 by Martin Andrews

Excellent progress has been made recently in solving ARC Challenge problems.
However, it seems that new techniques may be required to push beyond 60%
accuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'
many of the problems (when given the input and output grids), which makes
discovering solutions by LLM-lead program search somewhat futile.
  In this work, LLM 'understanding' is attempted from a stronger starting
position : An LLM is given complete solutions to tasks in code, and then asked
to explain how the task is being solved at various levels of abstraction.
Specifically, the LLM was given code solutions implemented in arc-dsl-llm (an
LLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code
refactored into reusable functional chunks; (c) problem solution steps; and (d)
high-level problem-solving tactics.
  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM
output - in a form that could be used in downstream tasks with Local LLMs
eligible to enter the ARC Prize.
  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the
Gemini LLM-generated data (along with the generation code) are made Open
Source.

摘要：最近在解決 ARC Challenge 問題方面取得了顯著進展。
然而，似乎需要新的技術才能將準確率提升至 60% 以上。
即使是商用大型語言模型 (LLM) 也難以「理解」許多問題（在提供輸入和輸出網格時），這使得透過 LLM 引導的程式搜尋來發現解決方案有些徒勞無功。
在這項工作中，嘗試從更強的起點來「理解」LLM：給予 LLM 任務的完整程式碼解決方案，然後要求它說明任務是如何在不同的抽象層級中獲得解決的。
具體來說，LLM 被給予使用 arc-dsl-llm（Hodel 的 arc-dsl 的 LLM 可讀版本）實作的程式碼解決方案，以取得：(a) 附註程式碼；(b) 重構為可重複使用的函數區塊的程式碼；(c) 問題解決步驟；以及 (d) 高階問題解決策略。
我們證明了「抽象火花」可以從 LLM 輸出中萃取出來，其形式可用於下游任務，而符合資格進入 ARC 獎項的在地 LLM。
arc-dsl-llm DSL 架構（包含重新設計的解決方案）和 Gemini LLM 生成的資料（連同產生程式碼）都是開放原始碼。

##### **Debiasing Watermarks for Large Language Models via Maximal Coupling**
2411.11203v1 by Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie J. Su, Ruixun Zhang

Watermarking language models is essential for distinguishing between human
and machine-generated text and thus maintaining the integrity and
trustworthiness of digital communication. We present a novel green/red list
watermarking approach that partitions the token set into ``green'' and ``red''
lists, subtly increasing the generation probability for green tokens. To
correct token distribution bias, our method employs maximal coupling, using a
uniform coin flip to decide whether to apply bias correction, with the result
embedded as a pseudorandom watermark signal. Theoretical analysis confirms this
approach's unbiased nature and robust detection capabilities. Experimental
results show that it outperforms prior techniques by preserving text quality
while maintaining high detectability, and it demonstrates resilience to
targeted modifications aimed at improving text quality. This research provides
a promising watermarking solution for language models, balancing effective
detection with minimal impact on text quality.

摘要：<paragraph>浮水印語言模型對於區分人類和機器產生的文字至關重要，進而維護數位溝通的完整性和可信度。我們提出一個新穎的綠色/紅色清單浮水印方法，將代碼集分割成「綠色」和「紅色」清單，巧妙地增加綠色代碼的產生機率。為了修正代碼分配偏差，我們的方法採用最大耦合，使用均勻的拋硬幣來決定是否套用偏差修正，結果嵌入為一個偽亂數浮水印訊號。理論分析證實了此方法的無偏差性質和穩健的偵測能力。實驗結果顯示，它在維持高偵測率的同時，優於先前的技術，並保留文字品質，且證明對於旨在提升文字品質的目標修改具有韌性。此研究提供了一個有前景的語言模型浮水印解決方案，在有效的偵測和對文字品質的最小影響之間取得平衡。</paragraph>

##### **PickScan: Object discovery and reconstruction from handheld interactions**
2411.11196v1 by Vincent van der Brugge, Marc Pollefeys, Joshua B. Tenenbaum, Ayush Tewari, Krishna Murthy Jatavallabhula

Reconstructing compositional 3D representations of scenes, where each object
is represented with its own 3D model, is a highly desirable capability in
robotics and augmented reality. However, most existing methods rely heavily on
strong appearance priors for object discovery, therefore only working on those
classes of objects on which the method has been trained, or do not allow for
object manipulation, which is necessary to scan objects fully and to guide
object discovery in challenging scenarios. We address these limitations with a
novel interaction-guided and class-agnostic method based on object
displacements that allows a user to move around a scene with an RGB-D camera,
hold up objects, and finally outputs one 3D model per held-up object. Our main
contribution to this end is a novel approach to detecting user-object
interactions and extracting the masks of manipulated objects. On a
custom-captured dataset, our pipeline discovers manipulated objects with 78.3%
precision at 100% recall and reconstructs them with a mean chamfer distance of
0.90cm. Compared to Co-Fusion, the only comparable interaction-based and
class-agnostic baseline, this corresponds to a reduction in chamfer distance of
73% while detecting 99% fewer false positives.

摘要：重建場景的合成 3D 表示，其中每個物件都用其自己的 3D 模型表示，在機器人和擴增實境中是一種非常理想的能力。然而，大多數現有方法過度依賴物件發現的強外觀先驗，因此只適用於方法已受過訓練的那些類別的物件，或者不允許物件操作，這對於完整掃描物件和在具有挑戰性的場景中引導物件發現是必要的。我們透過一種基於物件位移的新穎互動引導且與類別無關的方法來解決這些限制，該方法允許使用者使用 RGB-D 相機在場景中移動、拿著物件，最後為每個拿著的物件輸出一個 3D 模型。我們對此目的的主要貢獻是一種新穎的方法，用於偵測使用者物件互動並提取被操作物件的遮罩。在自訂擷取的資料集上，我們的管線以 100% 召回率發現了 78.3% 精確度的被操作物件，並以 0.90 公分的平均錢弗距離重建它們。與 Co-Fusion 相比，這是唯一可比較的基於互動且與類別無關的基準，這相當於錢弗距離減少了 73%，同時偵測到少 99% 的誤報。

##### **Enhanced Anime Image Generation Using USE-CMHSA-GAN**
2411.11179v1 by J. Lu

With the growing popularity of ACG (Anime, Comics, and Games) culture,
generating high-quality anime character images has become an important research
topic. This paper introduces a novel Generative Adversarial Network model,
USE-CMHSA-GAN, designed to produce high-quality anime character images. The
model builds upon the traditional DCGAN framework, incorporating USE and CMHSA
modules to enhance feature extraction capabilities for anime character images.
Experiments were conducted on the anime-face-dataset, and the results
demonstrate that USE-CMHSA-GAN outperforms other benchmark models, including
DCGAN, VAE-GAN, and WGAN, in terms of FID and IS scores, indicating superior
image quality. These findings suggest that USE-CMHSA-GAN is highly effective
for anime character image generation and provides new insights for further
improving the quality of generative models.

摘要：隨著 ACG（動畫、漫畫、遊戲）文化的普及，
生成高品質的動漫角色圖像已成為重要的研究議題。
本文介紹了一種新穎的生成對抗網路模型，
USE-CMHSA-GAN，旨在產生高品質的動漫角色圖像。
該模型建構於傳統的 DCGAN 框架之上，並結合 USE 和 CMHSA
模組來增強動漫角色圖像的特徵提取能力。
在 anime-face-dataset 上進行了實驗，結果
證明 USE-CMHSA-GAN 在 FID 和 IS 分數方面優於其他基準模型，包括
DCGAN、VAE-GAN 和 WGAN，這表示具有優異的
圖像品質。這些發現表明 USE-CMHSA-GAN 對於動漫角色圖像生成非常有效，並提供了新的見解以進一步
提升生成模型的品質。

##### **LLäMmlein: Compact and Competitive German-Only Language Models from Scratch**
2411.11171v1 by Jan Pfister, Julia Wunderle, Andreas Hotho

We create two German-only decoder models, LL\"aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\"aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.

摘要：我們從頭開始透明地建立兩個僅限德語的解碼器模型，LL\"aMmlein 120M 和 1B，並與訓練數據一起發布，供德語 NLP 研究社群使用。模型訓練涉及幾個關鍵步驟，包括廣泛的資料預處理、建立自訂德語分詞器、訓練本身，以及在各種基準上評估最終模型。在整個訓練過程中，使用 SuperGLEBer 基準儲存和分析多個檢查點，以監控模型的學習動態。與 SuperGLEBer 基準上的最先進模型相比，兩個 LL\"aMmlein 模型表現出色，始終與參數規模類似的模型匹配或超越它們。結果顯示，模型品質會如預期般隨著規模擴大，但在某些任務上的效能提升在早期就達到平穩期，為未來模型開發的資源配置提供了寶貴的見解。

##### **RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer**
2411.11162v1 by Jiawei Zhang

This paper builds upon our previous work on the Reconciled Polynomial Network
(RPN). The original RPN model was designed under the assumption of input data
independence, presuming the independence among both individual instances within
data batches and attributes in each data instance. However, this assumption
often proves invalid for function learning tasks involving complex,
interdependent data such as language, images, time series, and graphs. Ignoring
such data interdependence may inevitably lead to significant performance
degradation.
  To overcome these limitations, we introduce the new Reconciled Polynomial
Network (version 2), namely RPN 2, in this paper. By incorporating data and
structural interdependence functions, RPN 2 explicitly models data
interdependence via new component functions in its architecture.
  This enhancement not only significantly improves RPN 2's learning performance
but also substantially expands its unifying potential, enabling it to encompass
a broader range of contemporary dominant backbone models within its canonical
representation. These backbones include, but are not limited to, convolutional
neural networks (CNNs), recurrent neural networks (RNNs), graph neural networks
(GNNs), and Transformers. Our analysis reveals that the fundamental
distinctions among these backbone models primarily stem from their diverse
approaches to defining the interdependence functions. Furthermore, this unified
representation opens up new opportunities for designing innovative
architectures with the potential to surpass the performance of these dominant
backbones.

摘要：本文建立在我们先前关于协调多项式网络 (RPN) 的工作之上。最初的 RPN 模型是在输入数据独立性的假设下设计的，假定数据批次中各个实例之间的独立性以及每个数据实例中的属性之间的独立性。然而，对于涉及复杂相互依赖数据（例如语言、图像、时间序列和图形）的功能学习任务，这种假设通常被证明是无效的。忽略此类数据相互依赖性不可避免地会导致性能显着下降。
为了克服这些限制，我们在本文中引入了新的协调多项式网络（版本 2），即 RPN 2。通过结合数据和结构相互依赖函数，RPN 2 通过其架构中的新组件函数明确地对数据相互依赖性进行建模。
这种增强不仅显着提高了 RPN 2 的学习性能，而且还大幅扩展了其统一潜力，使其能够在其规范表示中包含更广泛的当代主干模型。这些主干包括但不限于卷积神经网络 (CNN)、循环神经网络 (RNN)、图神经网络 (GNN) 和 Transformer。我们的分析表明，这些主干模型之间的根本区别主要源于它们定义相互依赖函数的不同方法。此外，这种统一表示为设计创新架构开辟了新的机会，这些架构有可能超越这些主干的性能。

##### **MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records**
2411.11161v1 by Eric Yang, Pengfei Hu, Xiaoxue Han, Yue Ning

The adoption of digital systems in healthcare has resulted in the
accumulation of vast electronic health records (EHRs), offering valuable data
for machine learning methods to predict patient health outcomes. However,
single-visit records of patients are often neglected in the training process
due to the lack of annotations of next-visit information, thereby limiting the
predictive and expressive power of machine learning models. In this paper, we
present a novel framework MPLite that utilizes Multi-aspect Pretraining with
Lab results through a light-weight neural network to enhance medical concept
representation and predict future health outcomes of individuals. By
incorporating both structured medical data and additional information from lab
results, our approach fully leverages patient admission records. We design a
pretraining module that predicts medical codes based on lab results, ensuring
robust prediction by fusing multiple aspects of features. Our experimental
evaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements
over existing models in diagnosis prediction and heart failure prediction
tasks, achieving a higher weighted-F1 and recall with MPLite. This work reveals
the potential of integrating diverse aspects of data to advance predictive
modeling in healthcare.

摘要：數位系統在醫療保健中的採用導致了大量電子健康記錄 (EHR) 的累積，這些記錄提供了有價值的資料，可供機器學習方法用來預測患者的健康結果。然而，由於缺乏下次就診資訊的註解，患者的單次就診記錄在訓練過程中常常被忽略，因此限制了機器學習模型的預測和表達能力。在本文中，我們提出了一個創新的框架 MPLite，它利用透過輕量級神經網路進行多面向預訓練與實驗室結果，來增強醫療概念的表徵並預測個人的未來健康結果。透過結合結構化的醫療資料和來自實驗室結果的額外資訊，我們的做法充分利用了患者的入院記錄。我們設計了一個預訓練模組，根據實驗室結果預測醫療代碼，確保透過融合特徵的各個面向來進行穩健的預測。我們使用 MIMIC-III 和 MIMIC-IV 資料集進行的實驗評估證明，在診斷預測和心臟衰竭預測任務中，我們的模型優於現有的模型，使用 MPLite 達到了更高的加權 F1 和召回率。這項工作揭示了整合資料中不同面向的潛力，以推進醫療保健中的預測建模。

##### **TabDeco: A Comprehensive Contrastive Framework for Decoupled Representations in Tabular Data**
2411.11148v1 by Suiyao Chen, Jing Wu, Yunxiao Wang, Cheng Ji, Tianpei Xie, Daniel Cociorva, Michael Sharps, Cecile Levasseur, Hakan Brunzell

Representation learning is a fundamental aspect of modern artificial
intelligence, driving substantial improvements across diverse applications.
While selfsupervised contrastive learning has led to significant advancements
in fields like computer vision and natural language processing, its adaptation
to tabular data presents unique challenges. Traditional approaches often
prioritize optimizing model architecture and loss functions but may overlook
the crucial task of constructing meaningful positive and negative sample pairs
from various perspectives like feature interactions, instance-level patterns
and batch-specific contexts. To address these challenges, we introduce TabDeco,
a novel method that leverages attention-based encoding strategies across both
rows and columns and employs contrastive learning framework to effectively
disentangle feature representations at multiple levels, including features,
instances and data batches. With the innovative feature decoupling hierarchies,
TabDeco consistently surpasses existing deep learning methods and leading
gradient boosting algorithms, including XG-Boost, CatBoost, and LightGBM,
across various benchmark tasks, underscoring its effectiveness in advancing
tabular data representation learning.

摘要：表徵學習是現代人工智慧的基本面向，推動各項應用產生大幅進步。
雖然自監督對比學習已在電腦視覺和自然語言處理等領域帶來重大進展，但其在表格資料的應用面臨獨特挑戰。傳統方法通常優先最佳化模型架構和損失函數，但可能忽略從特徵交互作用、實例層級模式和批次特定脈絡等不同觀點建構有意義的正負樣本對這項關鍵任務。為了應對這些挑戰，我們引進 TabDeco，這是一種創新的方法，利用基於注意力的編碼策略橫跨列和欄，並採用對比學習架構，在多個層級有效解開特徵表徵，包括特徵、實例和資料批次。透過創新的特徵解耦層級，TabDeco 在各種基準任務中始終超越現有的深度學習方法和領先的梯度提升演算法，包括 XG-Boost、CatBoost 和 LightGBM，凸顯其在推進表格資料表徵學習方面的效能。

##### **CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning**
2411.11144v1 by Depeng Chen, Xiao Liu, Jie Cui, Hong Zhong

Since machine learning model is often trained on a limited data set, the
model is trained multiple times on the same data sample, which causes the model
to memorize most of the training set data. Membership Inference Attacks (MIAs)
exploit this feature to determine whether a data sample is used for training a
machine learning model. However, in realistic scenarios, it is difficult for
the adversary to obtain enough qualified samples that mark accurate identity
information, especially since most samples are non-members in real world
applications. To address this limitation, in this paper, we propose a new
attack method called CLMIA, which uses unsupervised contrastive learning to
train an attack model without using extra membership status information.
Meanwhile, in CLMIA, we require only a small amount of data with known
membership status to fine-tune the attack model. Experimental results
demonstrate that CLMIA performs better than existing attack methods for
different datasets and model structures, especially with data with less marked
identity information. In addition, we experimentally find that the attack
performs differently for different proportions of labeled identity information
for member and non-member data. More analysis proves that our attack method
performs better with less labeled identity information, which applies to more
realistic scenarios.

摘要：由於機器學習模型通常在有限的資料集上訓練，模型會在同一個資料樣本上訓練多次，這會導致模型記住訓練集資料的大部分內容。成員身分推論攻擊 (MIA) 會利用這個特點來判斷資料樣本是否用於訓練機器學習模型。然而，在實際情況下，攻擊者很難取得足夠的合格樣本來標記準確的身分資訊，特別是因為在實際應用中，大多數樣本都不是成員。為了解決這個限制，我們在本文中提出了一種新的攻擊方法，稱為 CLMIA，它使用無監督對比學習來訓練攻擊模型，而不需要使用額外的成員身分狀態資訊。同時，在 CLMIA 中，我們只需要少量的已知成員身分狀態資料來微調攻擊模型。實驗結果表明，CLMIA 在不同的資料集和模型結構上比現有的攻擊方法表現得更好，特別是對於標記身分資訊較少的資料。此外，我們透過實驗發現，對於成員和非成員資料的標籤身分資訊的不同比例，攻擊的表現也不同。進一步的分析證明，我們的攻擊方法在標籤身分資訊較少的情況下表現得更好，這適用於更實際的情況。

##### **Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks**
2411.11105v1 by Deepa Anand, Bipul Das, Vyshnav Dangeti, Antony Jerald, Rakesh Mullick, Uday Patil, Pakhi Sharma, Prasad Sudhakar

In a setting where segmentation models have to be built for multiple
datasets, each with its own corresponding label set, a straightforward way is
to learn one model for every dataset and its labels. Alternatively, multi-task
architectures with shared encoders and multiple segmentation heads or shared
weights with compound labels can also be made use of. This work proposes a
novel label sharing framework where a shared common label space is constructed
and each of the individual label sets are systematically mapped to the common
labels. This transforms multiple datasets with disparate label sets into a
single large dataset with shared labels, and therefore all the segmentation
tasks can be addressed by learning a single model. This eliminates the need for
task specific adaptations in network architectures and also results in
parameter and data efficient models. Furthermore, label sharing framework is
naturally amenable for incremental learning where segmentations for new
datasets can be easily learnt. We experimentally validate our method on various
medical image segmentation datasets, each involving multi-label segmentation.
Furthermore, we demonstrate the efficacy of the proposed method in terms of
performance and incremental learning ability vis-a-vis alternative methods.

摘要：在必須為多個資料集建立分割模型的設定中，每個資料集都有自己對應的標籤集，一個直接的方法是為每個資料集及其標籤學習一個模型。或者，也可以利用具有共享編碼器和多個分割頭或具有複合標籤的共享權重的多任務架構。這項工作提出了一個新穎的標籤共享框架，其中構建了一個共享的共同標籤空間，並且每個單獨的標籤集都系統性地映射到共同標籤。這將具有不同標籤集的多個資料集轉換為具有共享標籤的單一大型資料集，因此所有分割任務都可以通過學習單一模型來解決。這消除了對網路架構中特定任務適應的需求，並且還產生了參數和資料有效率的模型。此外，標籤共享框架自然適用於增量學習，其中可以輕鬆學習新資料集的分割。我們在涉及多標籤分割的各種醫學影像分割資料集上對我們的模型進行實驗驗證。此外，我們根據效能和增量學習能力證明了所提出模型的有效性，相對於其他方法。

##### **The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study on Media Bias Detection**
2411.11081v1 by Tomas Horych, Christoph Mandl, Terry Ruas, Andre Greiner-Petter, Bela Gipp, Akiko Aizawa, Timo Spinde

High annotation costs from hiring or crowdsourcing complicate the creation of
large, high-quality datasets needed for training reliable text classifiers.
Recent research suggests using Large Language Models (LLMs) to automate the
annotation process, reducing these costs while maintaining data quality. LLMs
have shown promising results in annotating downstream tasks like hate speech
detection and political framing. Building on the success in these areas, this
study investigates whether LLMs are viable for annotating the complex task of
media bias detection and whether a downstream media bias classifier can be
trained on such data. We create annolexical, the first large-scale dataset for
media bias classification with over 48000 synthetically annotated examples. Our
classifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by
5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or
outperforms the model trained on human-labeled data when evaluated on two media
bias benchmark datasets (BABE and BASIL). This study demonstrates how our
approach significantly reduces the cost of dataset creation in the media bias
domain and, by extension, the development of classifiers, while our subsequent
behavioral stress-testing reveals some of its current limitations and
trade-offs.

摘要：高昂的標註成本（來自於聘請或群眾外包）使得建立大型、高品質的資料集變得複雜，而這些資料集對於訓練可靠的文字分類器來說是必要的。最近的研究建議使用大型語言模型 (LLM) 來自動化標註流程，在維持資料品質的同時降低這些成本。LLM 在標註下游任務方面顯示出有希望的結果，例如仇恨言論偵測和政治取向。本研究建立在這些領域的成功基礎上，探討 LLM 是否可行於標註媒體偏見偵測的複雜任務，以及是否可以針對此類資料訓練下游媒體偏見分類器。我們建立了 annolexical，這是第一個針對媒體偏見分類的大規模資料集，包含超過 48,000 個合成標註範例。針對此資料集微調的分類器，在馬修斯相關係數 (MCC) 上比所有標註器 LLM 高出 5-9%，並且在兩個媒體偏見基準資料集 (BABE 和 BASIL) 上的評估結果接近或優於以人工標註資料訓練的模型。本研究展示了我們的做法如何大幅降低媒體偏見領域中資料集建立的成本，進而降低分類器的開發成本，而我們後續的行為壓力測試揭示了一些其當前的限制和權衡。

##### **Multilingual Large Language Models: A Systematic Survey**
2411.11072v1 by Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong

This paper provides a comprehensive survey of the latest research on
multilingual large language models (MLLMs). MLLMs not only are able to
understand and generate language across linguistic boundaries, but also
represent an important advancement in artificial intelligence. We first discuss
the architecture and pre-training objectives of MLLMs, highlighting the key
components and methodologies that contribute to their multilingual
capabilities. We then discuss the construction of multilingual pre-training and
alignment datasets, underscoring the importance of data quality and diversity
in enhancing MLLM performance. An important focus of this survey is on the
evaluation of MLLMs. We present a detailed taxonomy and roadmap covering the
assessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human
values, safety, interpretability and specialized applications. Specifically, we
extensively discuss multilingual evaluation benchmarks and datasets, and
explore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs
from black to white boxes, we also address the interpretability of multilingual
capabilities, cross-lingual transfer and language bias within these models.
Finally, we provide a comprehensive review of real-world applications of MLLMs
across diverse domains, including biology, medicine, computer science,
mathematics and law. We showcase how these models have driven innovation and
improvements in these specialized fields while also highlighting the challenges
and opportunities in deploying MLLMs within diverse language communities and
application scenarios.We listed the paper related in this survey and publicly
available at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers .

摘要：<paragraph>這篇論文提供了多語言大型語言模型（MLLM）最新研究的全面調查。MLLM 不僅能夠跨越語言界限理解和生成語言，而且還代表了人工智能的重要進展。我們首先討論 MLLM 的架構和預訓練目標，重點介紹有助於其多語言能力的關鍵組成部分和方法。然後，我們討論多語言預訓練和對齊資料集的建構，強調資料品質和多樣性在提升 MLLM 效能方面的重要性。本調查的一個重點是 MLLM 的評估。我們提出了涵蓋 MLLM 跨語言知識、推理、與人類價值觀的一致性、安全性、可解釋性和專業應用評估的詳細分類法和路線圖。具體來說，我們廣泛討論多語言評估基準和資料集，並探討將 LLM 本身用作多語言評估器的用途。為了將 MLLM 從黑盒子提升到白盒子，我們還探討了這些模型中多語言能力、跨語言轉移和語言偏見的可解釋性。最後，我們對 MLLM 在生物學、醫學、電腦科學、數學和法律等不同領域的實際應用進行了全面的回顧。我們展示了這些模型如何推動這些專業領域的創新和改進，同時也強調了在不同的語言社群和應用場景中部署 MLLM 的挑戰和機遇。我們列出了本調查中相關的論文，並在 https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers 公開提供。</paragraph>

##### **Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text**
2411.11061v1 by Xiaoliang Luo, Michael Ramscar, Bradley C. Love

The impressive performance of large language models (LLMs) has led to their
consideration as models of human language processing. Instead, we suggest that
the success of LLMs arises from the flexibility of the transformer learning
architecture. To evaluate this conjecture, we trained LLMs on scientific texts
that were either in a forward or backward format. Despite backward text being
inconsistent with the structure of human languages, we found that LLMs
performed equally well in either format on a neuroscience benchmark, eclipsing
human expert performance for both forward and backward orders. Our results are
consistent with the success of transformers across diverse domains, such as
weather prediction and protein design. This widespread success is attributable
to LLM's ability to extract predictive patterns from any sufficiently
structured input. Given their generality, we suggest caution in interpreting
LLM's success in linguistic tasks as evidence for human-like mechanisms.

摘要：大型語言模型（LLM）的驚人表現，導致它們被視為人類語言處理模型。相反，我們認為 LLM 的成功來自於Transformer學習架構的靈活性。為了評估這個猜想，我們在科學文本上訓練 LLM，這些文本採用順序或倒序格式。儘管倒序文本與人類語言的結構不一致，但我們發現 LLM 在神經科學基準上以任何格式表現得一樣好，超越了人類專家對順序和倒序的表現。我們的結果與Transformer在不同領域的成功一致，例如天氣預測和蛋白質設計。這種廣泛的成功歸因於 LLM 從任何充分結構化的輸入中提取預測模式的能力。鑑於它們的普遍性，我們建議在將 LLM 在語言任務中的成功解釋為類似人類的機制的證據時要謹慎。

##### **FastDraft: How to Train Your Draft**
2411.11055v1 by Ofir Zafrir, Igor Margulis, Dorin Shteyman, Guy Boudoukh

Speculative Decoding has gained popularity as an effective technique for
accelerating the auto-regressive inference process of Large Language Models
(LLMs). However, Speculative Decoding entirely relies on the availability of
efficient draft models, which are often lacking for many existing language
models due to a stringent constraint of vocabulary incompatibility. In this
work we introduce FastDraft, a novel and efficient approach for pre-training
and aligning a draft model to any large language model by incorporating
efficient pre-training, followed by fine-tuning over synthetic datasets
generated by the target model. We demonstrate FastDraft by training two highly
parameter efficient drafts for the popular Phi-3-mini and Llama-3.1-8B models.
Using FastDraft, we were able to produce a draft with approximately 10 billion
tokens on a single server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2
accelerators in under 24 hours. Our results show that the draft model achieves
impressive results in key metrics of acceptance rate, block efficiency and up
to 3x memory bound speed up when evaluated on code completion and up to 2x in
summarization, text completion and instruction tasks. We validate our
theoretical findings through benchmarking on the latest Intel$^\circledR$
Core$^{\tiny \text{TM}}$ Ultra, achieving a wall-clock time speedup of up to
2x, indicating a significant reduction in runtime. Due to its high quality,
FastDraft unlocks large language models inference on AI-PC and other
edge-devices.

摘要：<paragraph>推測解碼作為一種加速大型語言模型 (LLM) 自迴歸推論過程的有效技術而獲得普及。然而，推測解碼完全依賴於高效草稿模型的可用性，而這對於許多現有語言模型來說往往是缺乏的，因為存在詞彙不兼容的嚴格約束。在本文中，我們介紹了 FastDraft，這是一種新穎且高效的方法，用於預訓練和將草稿模型與任何大型語言模型對齊，方法是結合高效的預訓練，然後根據目標模型生成的合成數據集進行微調。我們通過訓練兩個高度參數高效的草稿來展示 FastDraft，用於流行的 Phi-3-mini 和 Llama-3.1-8B 模型。使用 FastDraft，我們能夠在不到 24 小時的時間裡，使用 8 個 Intel$^\circledR$ Gaudi$^\circledR$ 2 加速器在單個伺服器上生成約 100 億個代幣的草稿。我們的結果表明，在代碼完成時評估時，草稿模型在接受率、區塊效率和記憶體限制速度方面取得了令人印象深刻的結果，分別高達 3 倍和 2 倍，在摘要、文本完成和指令任務中也取得了同樣的結果。我們通過在最新的 Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra 上進行基準測試驗證了我們的理論發現，實現了高達 2 倍的時脈時間加速，表明執行時間顯著減少。由於其高品質，FastDraft 在 AI-PC 和其他邊緣設備上解鎖了大型語言模型推論。</paragraph>

##### **SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation**
2411.11053v1 by Bin Xu, Yiguan Lin, Yinghao Li, YangGao

Large language models demonstrate exceptional performance in simple code
generation tasks but still face challenges in tackling complex problems. These
challenges may stem from insufficient reasoning and problem decomposition
capabilities. To address this issue, we propose a reasoning-augmented data
generation process, SRA-MCTS, which guides the model to autonomously generate
high-quality intermediate reasoning paths. This creates a positive feedback
loop, enabling continuous improvement. Our method operates entirely through the
model itself without requiring additional supervision. By synthesizing natural
language reasoning paths and translating them into executable code, the
approach ensures analytical accuracy and enhances the success rate in solving
complex tasks. Experimental results show that, even without additional
supervisory signals, our method achieves performance improvements across
different model scales, demonstrating the significant potential of
self-improvement in small models. Furthermore, the method remains robust when
traditional Chain-of-Thought (CoT) approaches exhibit performance degradation,
with notable improvements observed in diversity metrics such as pass@10. We
encourage further exploration of reasoning processes within training data to
enhance the ability of language models to address complex problems.

摘要：大型語言模型在簡單的程式碼生成任務中表現出色，但在解決複雜問題時仍面臨挑戰。這些挑戰可能源於推理和問題分解能力不足。為了解決這個問題，我們提出了一個推理增強資料生成過程 SRA-MCTS，它引導模型自主生成高品質的推理路徑。這會產生一個正向回饋迴路，實現持續改進。我們的模型完全透過模型本身運作，不需要額外的監督。透過綜合自然語言推理路徑並將其轉換為可執行程式碼，此方法確保分析準確性，並提高解決複雜任務的成功率。實驗結果表明，即使沒有額外的監督訊號，我們的模型也能在不同的模型規模中實現效能提升，證明了小型模型中自我改進的巨大潛力。此外，當傳統的思想鏈 (CoT) 方法表現出效能下降時，此方法仍然穩健，在多樣性指標（例如 pass@10）中觀察到顯著的改進。我們鼓勵進一步探索訓練資料中的推理過程，以增強語言模型解決複雜問題的能力。

##### **Knowledge-enhanced Transformer for Multivariate Long Sequence Time-series Forecasting**
2411.11046v1 by Shubham Tanaji Kakde, Rony Mitra, Jasashwi Mandal, Manoj Kumar Tiwari

Multivariate Long Sequence Time-series Forecasting (LSTF) has been a critical
task across various real-world applications. Recent advancements focus on the
application of transformer architectures attributable to their ability to
capture temporal patterns effectively over extended periods. However, these
approaches often overlook the inherent relationships and interactions between
the input variables that could be drawn from their characteristic properties.
In this paper, we aim to bridge this gap by integrating information-rich
Knowledge Graph Embeddings (KGE) with state-of-the-art transformer-based
architectures. We introduce a novel approach that encapsulates conceptual
relationships among variables within a well-defined knowledge graph, forming
dynamic and learnable KGEs for seamless integration into the transformer
architecture. We investigate the influence of this integration into seminal
architectures such as PatchTST, Autoformer, Informer, and Vanilla Transformer.
Furthermore, we thoroughly investigate the performance of these
knowledge-enhanced architectures along with their original implementations for
long forecasting horizons and demonstrate significant improvement in the
benchmark results. This enhancement empowers transformer-based architectures to
address the inherent structural relation between variables. Our
knowledge-enhanced approach improves the accuracy of multivariate LSTF by
capturing complex temporal and relational dynamics across multiple domains. To
substantiate the validity of our model, we conduct comprehensive experiments
using Weather and Electric Transformer Temperature (ETT) datasets.

摘要：多變量長序列時間序列預測 (LSTF) 一直是各種實際應用中的一項關鍵任務。最近的進展著重於Transformer架構的應用，其原因在於它們能夠有效地捕捉長時間的時序模式。然而，這些方法通常會忽略輸入變數之間的內在關係和交互作用，而這些關係和交互作用可以從它們的特性中得出。在本文中，我們旨在透過將資訊豐富的知識圖譜嵌入 (KGE) 與最先進的基於Transformer的架構整合，來彌合這項差距。我們提出了一種新穎的方法，它將變數之間的概念關係封裝在一個定義良好的知識圖譜中，形成動態且可學習的 KGE，以便無縫整合到Transformer架構中。我們探討了這種整合對開創性架構（例如 PatchTST、Autoformer、Informer 和 Vanilla Transformer）的影響。此外，我們徹底探討了這些知識增強架構的效能，以及它們在長時間預測範圍內的原始實作，並證明基準結果有顯著的改善。這種增強讓基於Transformer的架構能夠解決變數之間的內在結構關係。我們知識增強的方法透過捕捉跨多個領域的複雜時序和關係動態，來改善多變量 LSTF 的準確度。為了證實我們模型的有效性，我們使用天氣和電力Transformer溫度 (ETT) 資料集進行了全面的實驗。

##### **Wafer Map Defect Classification Using Autoencoder-Based Data Augmentation and Convolutional Neural Network**
2411.11029v1 by Yin-Yin Bao, Er-Chao Li, Hong-Qiang Yang, Bin-Bin Jia

In semiconductor manufacturing, wafer defect maps (WDMs) play a crucial role
in diagnosing issues and enhancing process yields by revealing critical defect
patterns. However, accurately categorizing WDM defects presents significant
challenges due to noisy data, unbalanced defect classes, and the complexity of
failure modes. To address these challenges, this study proposes a novel method
combining a self-encoder-based data augmentation technique with a convolutional
neural network (CNN). By introducing noise into the latent space, the
self-encoder enhances data diversity and mitigates class imbalance, thereby
improving the model's generalization capabilities. The augmented dataset is
subsequently used to train the CNN, enabling it to deliver precise
classification of both common and rare defect patterns. Experimental results on
the WM-811K dataset demonstrate that the proposed method achieves a
classification accuracy of 98.56%, surpassing Random Forest, SVM, and Logistic
Regression by 19%, 21%, and 27%, respectively. These findings highlight the
robustness and effectiveness of the proposed approach, offering a reliable
solution for wafer defect detection and classification.

摘要：在半導體製造中，晶圓缺陷地圖 (WDM) 在診斷問題和透過揭露關鍵缺陷模式來提升製程良率方面扮演至關重要的角色。然而，由於資料雜訊、不平衡的缺陷類別以及故障模式的複雜性，精確分類 WDM 缺陷會帶來重大的挑戰。為了應對這些挑戰，本研究提出了一種新穎的方法，結合基於自編碼器的資料擴充技術與卷積神經網路 (CNN)。透過在潛在空間中引入雜訊，自編碼器增強了資料的多樣性並減輕類別不平衡，進而改善模型的概化能力。擴充後的資料集隨後用於訓練 CNN，使其能夠對常見和罕見的缺陷模式進行精確分類。在 WM-811K 資料集上的實驗結果證明，所提出的方法達到了 98.56% 的分類準確率，分別比隨機森林、SVM 和邏輯迴歸高出 19%、21% 和 27%。這些發現突顯了所提出方法的穩健性和有效性，為晶圓缺陷檢測和分類提供了一個可靠的解決方案。

##### **BianCang: A Traditional Chinese Medicine Large Language Model**
2411.11027v1 by Sibo Wei, Xueping Peng, Yi-fei Wang, Jiasheng Si, Weiyu Zhang, Wenpeng Lu, Xiaoming Wu, Yinglong Wang

The rise of large language models (LLMs) has driven significant progress in
medical applications, including traditional Chinese medicine (TCM). However,
current medical LLMs struggle with TCM diagnosis and syndrome differentiation
due to substantial differences between TCM and modern medical theory, and the
scarcity of specialized, high-quality corpora. This paper addresses these
challenges by proposing BianCang, a TCM-specific LLM, using a two-stage
training process that first injects domain-specific knowledge and then aligns
it through targeted stimulation. To enhance diagnostic and differentiation
capabilities, we constructed pre-training corpora, instruction-aligned datasets
based on real hospital records, and the ChP-TCM dataset derived from the
Pharmacopoeia of the People's Republic of China. We compiled extensive TCM and
medical corpora for continuous pre-training and supervised fine-tuning,
building a comprehensive dataset to refine the model's understanding of TCM.
Evaluations across 11 test sets involving 29 models and 4 tasks demonstrate the
effectiveness of BianCang, offering valuable insights for future research.
Code, datasets, and models are available at
https://github.com/QLU-NLP/BianCang.

摘要：大型語言模型 (LLM) 的興起推動了醫療應用領域的重大進展，包括中醫學 (TCM)。然而，由於中醫學與現代醫學理論之間存在著實質性的差異，以及缺乏專業、高品質的語料庫，當前的醫學 LLM 在中醫診斷和證候鑑別方面遇到了困難。本文通過提出 BianCang，一種特定於中醫學的 LLM，來應對這些挑戰，使用一個兩階段訓練過程，首先注入特定領域的知識，然後通過有針對性的刺激來對齊它。為了增強診斷和鑑別能力，我們構建了預訓練語料庫、基於真實醫院記錄的指令對齊數據集，以及源自中華人民共和國藥典的 ChP-TCM 數據集。我們編譯了大量的 TCM 和醫學語料庫，用於持續的預訓練和監督微調，構建了一個全面的數據集來完善模型對 TCM 的理解。涉及 29 個模型和 4 項任務的 11 個測試集的評估證明了 BianCang 的有效性，為未來的研究提供了有價值的見解。程式碼、數據集和模型可在 https://github.com/QLU-NLP/BianCang 獲得。

##### **Time Step Generating: A Universal Synthesized Deepfake Image Detector**
2411.11016v1 by Ziyue Zeng, Haoyuan Liu, Dingjie Peng, Luoxu Jing, Hiroshi Watanabe

Currently, high-fidelity text-to-image models are developed in an
accelerating pace. Among them, Diffusion Models have led to a remarkable
improvement in the quality of image generation, making it vary challenging to
distinguish between real and synthesized images. It simultaneously raises
serious concerns regarding privacy and security. Some methods are proposed to
distinguish the diffusion model generated images through reconstructing.
However, the inversion and denoising processes are time-consuming and heavily
reliant on the pre-trained generative model. Consequently, if the pre-trained
generative model meet the problem of out-of-domain, the detection performance
declines. To address this issue, we propose a universal synthetic image
detector Time Step Generating (TSG), which does not rely on pre-trained models'
reconstructing ability, specific datasets, or sampling algorithms. Our method
utilizes a pre-trained diffusion model's network as a feature extractor to
capture fine-grained details, focusing on the subtle differences between real
and synthetic images. By controlling the time step t of the network input, we
can effectively extract these distinguishing detail features. Then, those
features can be passed through a classifier (i.e. Resnet), which efficiently
detects whether an image is synthetic or real. We test the proposed TSG on the
large-scale GenImage benchmark and it achieves significant improvements in both
accuracy and generalizability.

摘要：目前，高保真文字轉圖像模型正以加速的速度發展。其中，擴散模型已顯著提升影像生成的品質，讓人難以區分真實影像與合成影像。這同時也引發了對於隱私和安全的嚴重疑慮。有人提出了一些方法，透過重建來區分擴散模型產生的影像。然而，反轉和去噪的過程耗時且高度依賴預先訓練的生成模型。因此，如果預先訓練的生成模型遇到領域外問題，偵測效能就會下降。為了解決這個問題，我們提出了一種通用的合成影像偵測器時間步驟生成 (TSG)，它不依賴預先訓練模型的重建能力、特定資料集或抽樣演算法。我們的技術利用預先訓練的擴散模型的網路作為特徵萃取器，以擷取細微的細節，並專注於真實影像與合成影像之間的細微差異。透過控制網路輸入的時間步驟 t，我們可以有效萃取這些區別性的細節特徵。然後，這些特徵可以傳遞到分類器 (例如 Resnet)，它可以有效偵測影像是否為合成或真實。我們在大型 GenImage 基準上測試所提出的 TSG，結果在準確度和概括性方面都有顯著的提升。

##### **BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation**
2411.11006v1 by Haiyang Yu, Tian Xie, Jiaping Gui, Pengyang Wang, Ping Yi, Yue Wu

We introduce BackdoorMBTI, the first backdoor learning toolkit and benchmark
designed for multimodal evaluation across three representative modalities from
eleven commonly used datasets. BackdoorMBTI provides a systematic backdoor
learning pipeline, encompassing data processing, data poisoning, backdoor
training, and evaluation. The generated poison datasets and backdoor models
enable detailed evaluation of backdoor defense methods. Given the diversity of
modalities, BackdoorMBTI facilitates systematic evaluation across different
data types. Furthermore, BackdoorMBTI offers a standardized approach to
handling practical factors in backdoor learning, such as issues related to data
quality and erroneous labels. We anticipate that BackdoorMBTI will expedite
future research in backdoor defense methods within a multimodal context. Code
is available at https://anonymous.4open.science/r/BackdoorMBTI-D6A1/README.md.

摘要：我們推出 BackdoorMBTI，這是第一個後門學習工具包和基準，
專為來自十一種常用資料集的三種代表性模態進行多模態評估而設計。BackdoorMBTI 提供一個系統性的後門學習管道，包含資料處理、資料中毒、後門訓練和評估。產生的毒害資料集和後門模型可詳細評估後門防禦方法。鑑於模態的多樣性，BackdoorMBTI 便於跨不同資料類型進行系統性評估。此外，BackdoorMBTI 提供一種標準化方法來處理後門學習中的實際因素，例如與資料品質和錯誤標籤相關的問題。我們預計 BackdoorMBTI 將加速未來在多模態環境中進行後門防禦方法的研究。程式碼可在 https://anonymous.4open.science/r/BackdoorMBTI-D6A1/README.md 取得。

##### **Modulating Reservoir Dynamics via Reinforcement Learning for Efficient Robot Skill Synthesis**
2411.10991v1 by Zahra Koulaeizadeh, Erhan Oztop

A random recurrent neural network, called a reservoir, can be used to learn
robot movements conditioned on context inputs that encode task goals. The
Learning is achieved by mapping the random dynamics of the reservoir modulated
by context to desired trajectories via linear regression. This makes the
reservoir computing (RC) approach computationally efficient as no iterative
gradient descent learning is needed. In this work, we propose a novel RC-based
Learning from Demonstration (LfD) framework that not only learns to generate
the demonstrated movements but also allows online modulation of the reservoir
dynamics to generate movement trajectories that are not covered by the initial
demonstration set. This is made possible by using a Reinforcement Learning (RL)
module that learns a policy to output context as its actions based on the robot
state. Considering that the context dimension is typically low, learning with
the RL module is very efficient. We show the validity of the proposed model
with systematic experiments on a 2 degrees-of-freedom (DOF) simulated robot
that is taught to reach targets, encoded as context, with and without obstacle
avoidance constraint. The initial data set includes a set of reaching
demonstrations which are learned by the reservoir system. To enable reaching
out-of-distribution targets, the RL module is engaged in learning a policy to
generate dynamic contexts so that the generated trajectory achieves the desired
goal without any learning in the reservoir system. Overall, the proposed model
uses an initial learned motor primitive set to efficiently generate diverse
motor behaviors guided by the designed reward function. Thus the model can be
used as a flexible and effective LfD system where the action repertoire can be
extended without new data collection.

摘要：一個隨機的遞迴神經網路，稱為儲存器，可用於學習機器人動作，條件取決於編碼任務目標的背景輸入。學習是透過線性回歸將儲存器的隨機動態映射到目標軌跡來實現的，並由背景調製。由於無需反覆的梯度下降學習，因此儲存器計算 (RC) 方法在計算上很有效率。在這項工作中，我們提出了一個基於 RC 的示範學習 (LfD) 框架，該框架不僅學習產生示範動作，還允許在線調製儲存器動態以產生未被初始示範集涵蓋的運動軌跡。這可以透過使用強化學習 (RL) 模組來實現，該模組學習一種策略，根據機器人狀態將背景作為其動作輸出。考慮到背景維度通常很低，因此使用 RL 模組進行學習非常有效率。我們在一個 2 自由度 (DOF) 模擬機器人上進行系統實驗，展示了所提出模型的有效性，該機器人在沒有障礙物迴避約束的情況下被教導以背景編碼的方式到達目標。初始資料集包括一組到達示範，這些示範由儲存器系統學習。為了能夠觸及分佈外的目標，RL 模組參與學習一種策略來產生動態背景，以便產生的軌跡在儲存器系統中無需任何學習即可實現所需的目標。總的來說，所提出的模型使用最初學習的運動基本集來有效產生由設計的獎勵函數引導的多樣化運動行為。因此，該模型可用作一種靈活且有效的 LfD 系統，其中動作庫可以在不收集新資料的情況下進行擴充。

##### **VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?**
2411.10979v1 by Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, Pooyan Fazli, Chenliang Xu

The advancement of Multimodal Large Language Models (MLLMs) has enabled
significant progress in multimodal understanding, expanding their capacity to
analyze video content. However, existing evaluation benchmarks for MLLMs
primarily focus on abstract video comprehension, lacking a detailed assessment
of their ability to understand video compositions, the nuanced interpretation
of how visual elements combine and interact within highly compiled video
contexts. We introduce VidComposition, a new benchmark specifically designed to
evaluate the video composition understanding capabilities of MLLMs using
carefully curated compiled videos and cinematic-level annotations.
VidComposition includes 982 videos with 1706 multiple-choice questions,
covering various compositional aspects such as camera movement, angle, shot
size, narrative structure, character actions and emotions, etc. Our
comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a
significant performance gap between human and model capabilities. This
highlights the limitations of current MLLMs in understanding complex, compiled
video compositions and offers insights into areas for further improvement. The
leaderboard and evaluation code are available at
https://yunlong10.github.io/VidComposition/.

摘要：多模态大型语言模型 (MLLM) 的进步促进了多模态理解的重大进展，扩大了其分析视频内容的能力。然而，现有的 MLLM 评估基准主要侧重于抽象视频理解，缺乏对其理解视频构图的能力的详细评估，即视觉元素在高度编译的视频上下文中如何组合和交互的细微解释。我们引入了 VidComposition，这是一个专门设计的新基准，用于使用精心策划的编译视频和电影级注释来评估 MLLM 的视频构图理解能力。VidComposition 包含 982 个视频和 1706 个多项选择题，涵盖了各种构图方面，如摄像机运动、角度、镜头大小、叙事结构、人物动作和情绪等。我们对 33 个开源和专有 MLLM 的综合评估揭示了人类和模型能力之间存在明显的性能差距。这突出了当前 MLLM 在理解复杂、编译的视频构图方面的局限性，并提供了对需要进一步改进的领域的见解。排行榜和评估代码可在 https://yunlong10.github.io/VidComposition/ 获得。

##### **SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration**
2411.10958v1 by Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen

Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. SageAttention utilizes
8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit
accumulator, and precision-enhancing methods, implementing an accurate and 2x
speedup kernel compared to FlashAttention2. To further enhance the efficiency
of attention computation while maintaining precision, we propose
SageAttention2, which utilizes significantly faster 4-bit matrix multiplication
(Matmul) alongside additional precision-enhancing techniques. First, we propose
to quantize matrixes $(Q, K)$ to INT4 in a warp-level granularity and quantize
matrixes $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$
and $V$, enhancing the accuracy of attention with INT4 $QK$ and FP8 $PV$.
Third, we analyze the quantization accuracy across timesteps and layers, then
propose an adaptive quantization method to ensure the end-to-end metrics over
various models. The operations per second (OPS) of SageAttention2 surpass
FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively.
Comprehensive experiments confirm that our approach incurs negligible
end-to-end metrics loss across diverse models, including those for large
language processing, image generation, and video generation. The codes are
available at https://github.com/thu-ml/SageAttention.

摘要：<paragraph>儘管線性層的量化已廣泛使用，但其在加速注意力處理上的應用仍然有限。SageAttention 使用 8 位元矩陣乘法、16 位元矩陣乘法搭配 16 位元累加器，以及精準度提升方法，實作一個精準且速度為 FlashAttention2 2 倍的核。為了在維持精準度的同時進一步提升注意力運算的效率，我們提出 SageAttention2，它使用顯著更快的 4 位元矩陣乘法 (Matmul) 搭配其他精準度提升技術。首先，我們提出將矩陣 $(Q, K)$ 量化為 INT4，以 warp 層級的顆粒度，並將矩陣 $(\widetilde P, V)$ 量化為 FP8。其次，我們提出一個方法來平滑 $Q$ 和 $V$，使用 INT4 $QK$ 和 FP8 $PV$ 提升注意力的精準度。第三，我們分析了時間步長和層級之間的量化精準度，然後提出一個自適應量化方法，以確保各種模型的端到端指標。SageAttention2 的每秒運算次數 (OPS) 分別比 RTX4090 上的 FlashAttention2 和 xformers 快約 3 倍和 5 倍。全面的實驗證實，我們的做法會造成各種模型的端到端指標損失可以忽略不計，包括大型語言處理、影像生成和影片生成的模型。程式碼可於 https://github.com/thu-ml/SageAttention 取得。</paragraph>

##### **IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs**
2411.10957v1 by Sejun Park, Joo Young Park, Hyunwoo Park

This paper addresses domain adaptation challenges in graph data resulting
from chronological splits. In a transductive graph learning setting, where each
node is associated with a timestamp, we focus on the task of Semi-Supervised
Node Classification (SSNC), aiming to classify recent nodes using labels of
past nodes. Temporal dependencies in node connections create domain shifts,
causing significant performance degradation when applying models trained on
historical data into recent data. Given the practical relevance of this
scenario, addressing domain adaptation in chronological split data is crucial,
yet underexplored. We propose Imposing invariance with Message Passing in
Chronological split Temporal Graphs (IMPaCT), a method that imposes invariant
properties based on realistic assumptions derived from temporal graph
structures. Unlike traditional domain adaptation approaches which rely on
unverifiable assumptions, IMPaCT explicitly accounts for the characteristics of
chronological splits. The IMPaCT is further supported by rigorous mathematical
analysis, including a derivation of an upper bound of the generalization error.
Experimentally, IMPaCT achieves a 3.8% performance improvement over current
SOTA method on the ogbn-mag graph dataset. Additionally, we introduce the
Temporal Stochastic Block Model (TSBM), which replicates temporal graphs under
varying conditions, demonstrating the applicability of our methods to general
spatial GNNs.

摘要：本文探讨了由时间顺序分割产生的图数据中的领域适应挑战。在转导图学习设置中，每个节点都与时间戳相关联，我们专注于半监督节点分类 (SSNC) 的任务，旨在使用过去节点的标签对最近的节点进行分类。节点连接中的时间依赖性会产生领域转移，导致在将针对历史数据训练的模型应用于最近数据时性能大幅下降。鉴于此场景的实际相关性，解决时间顺序分割数据中的领域适应至关重要，但尚未得到充分探索。我们提出了在时间顺序分割时间图 (IMPaCT) 中通过消息传递强制不变性，这是一种基于从时间图结构中得出的现实假设来强制不变性属性的方法。与依赖不可验证假设的传统领域适应方法不同，IMPaCT 明确考虑了时间顺序分割的特征。IMPaCT 还得到严格的数学分析的支持，包括泛化误差的上界推导。在实验中，IMPaCT 在 ogbn-mag 图数据集上比当前 SOTA 方法提高了 3.8% 的性能。此外，我们引入了时间随机块模型 (TSBM)，它在不同条件下复制时间图，展示了我们的方法对一般空间 GNN 的适用性。

##### **A Topic-aware Comparable Corpus of Chinese Variations**
2411.10955v1 by Da-Chen Lian, Shu-Kai Hsieh

This study aims to fill the gap by constructing a topic-aware comparable
corpus of Mainland Chinese Mandarin and Taiwanese Mandarin from the social
media in Mainland China and Taiwan, respectively. Using Dcard for Taiwanese
Mandarin and Sina Weibo for Mainland Chinese, we create a comparable corpus
that updates regularly and reflects modern language use on social media.

摘要：本研究旨在透過建構一個主題感知的對比語料庫，以填補中國大陸普通話和台灣普通話的差距，分別取自中國大陸和台灣的社群媒體。我們使用 Dcard 作為台灣普通話和新浪微博作為中國大陸普通話，建立一個對比語料庫，定期更新並反映社群媒體上的現代語言使用。

##### **Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across Language Varieties**
2411.10954v1 by Fahim Faisal, Md Mushfiqur Rahman, Antonios Anastasopoulos

There has been little systematic study on how dialectal differences affect
toxicity detection by modern LLMs. Furthermore, although using LLMs as
evaluators ("LLM-as-a-judge") is a growing research area, their sensitivity to
dialectal nuances is still underexplored and requires more focused attention.
In this paper, we address these gaps through a comprehensive toxicity
evaluation of LLMs across diverse dialects. We create a multi-dialect dataset
through synthetic transformations and human-assisted translations, covering 10
language clusters and 60 varieties. We then evaluated three LLMs on their
ability to assess toxicity across multilingual, dialectal, and LLM-human
consistency. Our findings show that LLMs are sensitive in handling both
multilingual and dialectal variations. However, if we have to rank the
consistency, the weakest area is LLM-human agreement, followed by dialectal
consistency. Code repository:
\url{https://github.com/ffaisal93/dialect_toxicity_llm_judge}

摘要：目前對於方言差異如何影響現代 LLM 的毒性檢測，鮮有系統性的研究。此外，儘管將 LLM 用作評估器（「LLM 作為評判」）是一個不斷發展的研究領域，但 LLM 對方言細微差別的敏感性仍未得到充分探索，需要更集中的關注。在本文中，我們通過對不同方言的 LLM 進行全面的毒性評估來解決這些差距。我們通過合成轉換和人工輔助翻譯創建了一個多方言數據集，涵蓋 10 個語言群集和 60 個變體。然後，我們評估了三個 LLM 在跨多語言、方言和 LLM-human 一致性方面評估毒性的能力。我們的研究結果表明，LLM 在處理多語言和方言變體方面都很敏感。然而，如果我們必須對一致性進行排名，最弱的領域是 LLM-human 一致性，其次是方言一致性。代碼儲存庫：\url{https://github.com/ffaisal93/dialect_toxicity_llm_judge}

##### **Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering**
2411.10950v1 by Zeping Yu, Sophia Ananiadou

Understanding the mechanisms behind Large Language Models (LLMs) is crucial
for designing improved models and strategies. While recent studies have yielded
valuable insights into the mechanisms of textual LLMs, the mechanisms of
Multi-modal Large Language Models (MLLMs) remain underexplored. In this paper,
we apply mechanistic interpretability methods to analyze the visual question
answering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms
between VQA and textual QA (TQA) in color answering tasks and find that: a) VQA
exhibits a mechanism similar to the in-context learning mechanism observed in
TQA; b) the visual features exhibit significant interpretability when
projecting the visual embeddings into the embedding space; and c) Llava
enhances the existing capabilities of the corresponding textual LLM Vicuna
during visual instruction tuning. Based on these findings, we develop an
interpretability tool to help users and researchers identify important visual
locations for final predictions, aiding in the understanding of visual
hallucination. Our method demonstrates faster and more effective results
compared to existing interpretability approaches. Code:
\url{https://github.com/zepingyu0512/llava-mechanism}

摘要：了解大型語言模型 (LLM) 背後的機制對於設計改進的模型和策略至關重要。雖然最近的研究對文本 LLM 的機制產生了有價值的見解，但多模態大型語言模型 (MLLM) 的機制仍未得到充分探索。在本文中，我們應用機制可解釋性方法來分析第一個 MLLM Llava 中的視覺問答 (VQA) 機制。我們比較了 VQA 和文本 QA (TQA) 在顏色回答任務中的機制，發現：a) VQA 表現出與在 TQA 中觀察到的情境學習機制類似的機制；b) 在將視覺嵌入投影到嵌入空間時，視覺特徵表現出顯著的可解釋性；c) Llava 在視覺指令調整期間增強了相應文本 LLM Vicuna 的現有能力。根據這些發現，我們開發了一個可解釋性工具，以幫助用戶和研究人員識別最終預測的重要視覺位置，從而有助於理解視覺幻覺。與現有的可解釋性方法相比，我們的模型展示了更快速、更有效率的結果。代碼：\url{https://github.com/zepingyu0512/llava-mechanism}

##### **Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry**
2411.10937v1 by Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu

Comprehensively understanding surgical scenes in Surgical Visual Question
Answering (Surgical VQA) requires reasoning over multiple objects. Previous
approaches address this task using cross-modal fusion strategies to enhance
reasoning ability. However, these methods often struggle with limited scene
understanding and question comprehension, and some rely on external resources
(e.g., pre-extracted object features), which can introduce errors and
generalize poorly across diverse surgical environments. To address these
challenges, we propose SCAN, a simple yet effective memory-augmented framework
that leverages Multimodal LLMs to improve surgical context comprehension via
Self-Contained Inquiry. SCAN operates autonomously, generating two types of
memory for context augmentation: Direct Memory (DM), which provides multiple
candidates (or hints) to the final answer, and Indirect Memory (IM), which
consists of self-contained question-hint pairs to capture broader scene
context. DM directly assists in answering the question, while IM enhances
understanding of the surgical scene beyond the immediate query. Reasoning over
these object-aware memories enables the model to accurately interpret images
and respond to questions. Extensive experiments on three publicly available
Surgical VQA datasets demonstrate that SCAN achieves state-of-the-art
performance, offering improved accuracy and robustness across various surgical
scenarios.

摘要：全面理解外科視覺問題回答（外科 VQA）中的外科場景需要對多個物件進行推理。先前的
方法使用跨模態融合策略來解決此任務，以增強推理能力。然而，這些方法通常難以應付受限的場景
理解和問題理解，有些方法依賴於外部資源（例如，預先提取的物件特徵），這可能會引入錯誤，
並且在不同的外科環境中泛化性差。為了解決這些挑戰，我們提出了 SCAN，一個簡單但有效的記憶增強框架
，它利用多模態 LLM 來透過自我包含式詢問來改善外科背景理解。SCAN 自主運作，產生兩種記憶類型
用於背景擴充：直接記憶（DM），它為最終答案提供多個候選（或提示），以及間接記憶（IM），它
包含自我包含式的問題提示配對，以擷取更廣泛的場景背景。DM 直接協助回答問題，而 IM 增強
對外科場景的理解，超越立即的查詢。對這些物件感知記憶進行推理，使模型能夠準確地解釋影像
並回答問題。在三個公開可用的外科 VQA 資料集上進行的廣泛實驗證明，SCAN 達到了最先進的
效能，在各種外科場景中提供更高的準確性和穩健性。

##### **Analyzing Pokémon and Mario Streamers' Twitch Chat with LLM-based User Embeddings**
2411.10934v1 by Mika Hämäläinen, Jack Rueter, Khalid Alnajjar

We present a novel digital humanities method for representing our Twitch
chatters as user embeddings created by a large language model (LLM). We cluster
these embeddings automatically using affinity propagation and further narrow
this clustering down through manual analysis. We analyze the chat of one stream
by each Twitch streamer: SmallAnt, DougDoug and PointCrow. Our findings suggest
that each streamer has their own type of chatters, however two categories
emerge for all of the streamers: supportive viewers and emoji and reaction
senders. Repetitive message spammers is a shared chatter category for two of
the streamers.

摘要：我們提出一個創新的數位人文方法，以透過大型語言模型 (LLM) 建立的使用者嵌入來表示我們的 Twitch 聊天室使用者。我們使用關聯傳播自動對這些嵌入進行分群，並透過手動分析進一步縮小這個分群。我們分析了每個 Twitch 串流主的一個串流聊天：SmallAnt、DougDoug 和 PointCrow。我們的研究結果表明，每個串流主都有自己的聊天室使用者類型，然而對於所有串流主來說，出現了兩個類別：支持性的觀眾以及表情符號和反應發送者。重複訊息垃圾郵件發送者是兩個串流主的共同聊天室使用者類別。

##### **Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning**
2411.10928v1 by Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye

Multimodal Large Language Model (MLLM) have demonstrated strong
generalization capabilities across diverse distributions and tasks, largely due
to extensive pre-training datasets. Fine-tuning MLLM has become a common
practice to improve performance on specific downstream tasks. However, during
fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during
pre-training, which can result in a decline in generalization abilities. To
balance the trade-off between generalization and specialization, we propose
measuring the parameter importance for both pre-trained and fine-tuning
distributions, based on frozen pre-trained weight magnitude and accumulated
fine-tuning gradient values. We further apply an importance-aware weight
allocation strategy, selectively updating relatively important parameters for
downstream tasks. We conduct empirical evaluations on both image captioning and
visual question-answering tasks using various MLLM architectures. The
comprehensive experimental analysis demonstrates the effectiveness of the
proposed solution, highlighting the efficiency of the crucial modules in
enhancing downstream specialization performance while mitigating generalization
degradation in MLLM Fine-Tuning.

摘要：多模态大型语言模型 (MLLM) 在各种分布和任务中展示出强大的泛化能力，这在很大程度上归功于广泛的预训练数据集。微调 MLLM 已成为提高特定下游任务性能的常见做法。然而，在微调过程中，MLLM 经常面临忘记预训练期间获得的知识的风险，这可能导致泛化能力下降。为了平衡泛化和专业化之间的权衡，我们建议测量预训练和微调分布的参数重要性，基于冻结的预训练权重幅度和累积的微调梯度值。我们进一步应用了重要性感知权重分配策略，有选择地更新了下游任务的相对重要参数。我们使用各种 MLLM 架构对图像字幕和视觉问答任务进行了经验评估。全面的实验分析证明了所提出解决方案的有效性，强调了关键模块在提高下游专业化性能的同时减轻 MLLM 微调中泛化退化的效率。

##### **Inter-linguistic Phonetic Composition (IPC): A Theoretical and Computational Approach to Enhance Second Language Pronunciation**
2411.10927v1 by Jisang Park, Minu Kim, DaYoung Hong, Jongha Lee

Learners of a second language (L2) often unconsciously substitute unfamiliar
L2 phonemes with similar phonemes from their native language (L1), even though
native speakers of the L2 perceive these sounds as distinct and
non-interchangeable. This phonemic substitution leads to deviations from the
standard phonological patterns of the L2, creating challenges for learners in
acquiring accurate L2 pronunciation. To address this, we propose
Inter-linguistic Phonetic Composition (IPC), a novel computational method
designed to minimize incorrect phonological transfer by reconstructing L2
phonemes as composite sounds derived from multiple L1 phonemes. Tests with two
automatic speech recognition models demonstrated that when L2 speakers produced
IPC-generated composite sounds, the recognition rate of target L2 phonemes
improved by 20% compared to when their pronunciation was influenced by original
phonological transfer patterns. The improvement was observed within a
relatively shorter time frame, demonstrating rapid acquisition of the composite
sound.

摘要：第二語言 (L2) 的學習者常常不自覺地用母語 (L1) 中相似的音素取代不熟悉的 L2 音素，即使 L2 的母語人士會將這些聲音視為不同且不可互換的。這種音素替換會導致偏離 L2 的標準音韻模式，為學習者正確發音 L2 造成挑戰。為了解決這個問題，我們提出跨語言音素組合 (IPC)，這是一種新穎的運算方法，旨在透過將 L2 音素重建為源自多個 L1 音素的複合聲音，來最小化不正確的音韻轉移。使用兩個自動語音辨識模型進行的測試顯示，當 L2 說話者產生 IPC 生成的複合聲音時，目標 L2 音素的辨識率比其發音受到原始音韻轉移模式影響時提高了 20%。這種進步在相對較短的時間範圍內觀察到，顯示出快速習得複合聲音。

##### **Hyperspectral Imaging-Based Grain Quality Assessment With Limited Labelled Data**
2411.10924v1 by Priyabrata Karmakar, Manzur Murshed, Shyh Wei Teng

Recently hyperspectral imaging (HSI)-based grain quality assessment has
gained research attention. However, unlike other imaging modalities, HSI data
lacks sufficient labelled samples required to effectively train deep
convolutional neural network (DCNN)-based classifiers. In this paper, we
present a novel approach to grain quality assessment using HSI combined with
few-shot learning (FSL) techniques. Traditional methods for grain quality
evaluation, while reliable, are invasive, time-consuming, and costly. HSI
offers a non-invasive, real-time alternative by capturing both spatial and
spectral information. However, a significant challenge in applying DCNNs for
HSI-based grain classification is the need for large labelled databases, which
are often difficult to obtain. To address this, we explore the use of FSL,
which enables models to perform well with limited labelled data, making it a
practical solution for real-world applications where rapid deployment is
required. We also explored the application of FSL for the classification of
hyperspectral images of bulk grains to enable rapid quality assessment at
various receival points in the grain supply chain. We evaluated the performance
of few-shot classifiers in two scenarios: first, classification of grain types
seen during training, and second, generalisation to unseen grain types, a
crucial feature for real-world applications. In the first scenario, we
introduce a novel approach using pre-computed collective class prototypes
(CCPs) to enhance inference efficiency and robustness. In the second scenario,
we assess the model's ability to classify novel grain types using limited
support examples. Our experimental results show that despite using very limited
labelled data for training, our FSL classifiers accuracy is comparable to that
of a fully trained classifier trained using a significantly larger labelled
database.

摘要：<paragraph>最近，基于高光谱成像 (HSI) 的谷物品质评估已获得研究关注。然而，与其他成像方式不同，HSI 数据缺乏足够标记的样本，无法有效训练基于深度卷积神经网络 (DCNN) 的分类器。在本文中，我们提出了一种使用 HSI 结合少量样本学习 (FSL) 技术进行谷物品质评估的新方法。传统的谷物品质评估方法虽然可靠，但具有侵入性、耗时且成本高。HSI 通过捕获空间和光谱信息，提供了一种非侵入式、实时的替代方案。然而，将 DCNN 应用于基于 HSI 的谷物分类时面临的一项重大挑战是需要大量标记的数据库，而这些数据库通常难以获得。为了解决这个问题，我们探索了 FSL 的使用，它使模型能够在有限的标记数据下表现良好，使其成为需要快速部署的实际应用的实用解决方案。我们还探讨了 FSL 在散装谷物的高光谱图像分类中的应用，以在谷物供应链中的各个接收点实现快速质量评估。我们评估了少量样本分类器在两种情况下的性能：首先，对训练期间看到的谷物类型进行分类，其次，对未见过的谷物类型进行泛化，这是实际应用的关键特征。在第一种情况下，我们引入了一种使用预先计算的集体类原型 (CCP) 的新方法来提高推理效率和鲁棒性。在第二种情况下，我们评估了模型使用有限的支持示例对新谷物类型进行分类的能力。我们的实验结果表明，尽管在训练中使用了非常有限的标记数据，但我们的 FSL 分类器的准确性与使用明显更大的标记数据库训练的完全训练的分类器的准确性相当。</paragraph>

##### **LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection**
2411.10918v1 by Danial Abshari, Chenglong Fu, Meera Sridhar

Modern industrial infrastructures rely heavily on Cyber-Physical Systems
(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic
effects. To reduce these risks, anomaly detection methods based on physical
invariants have been developed. However, these methods often require
domain-specific expertise to manually define invariants, making them costly and
difficult to scale. To address this limitation, we propose a novel approach to
extract physical invariants from CPS testbeds for anomaly detection. Our
insight is that CPS design documentation often contains semantically rich
descriptions of physical procedures, which can profile inter-correlated
dynamics among system components. Leveraging the built-in physics and
engineering knowledge of recent generative AI models, we aim to automate this
traditionally manual process, improving scalability and reducing costs. This
work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)
workflow with a customized prompting system tailored for CPS documentation,
enabling accurate extraction of semantic information and inference of physical
invariants from complex, multimodal content. Then, rather than directly
applying the inferred invariants for anomaly detection, we introduce an
innovative statistics-based learning approach that integrates these invariants
into the training dataset. This method addresses limitations such as
hallucination and concept drift, enhancing the reliability of the model. We
evaluate our approach on real-world public CPS security dataset which contains
86 data points and 58 attacking cases. The results show that our approach
achieves a high precision of 0.923, accurately detecting anomalies while
minimizing false alarms.

摘要：<paragraph>現代工業基礎設施高度依賴網路物理系統 (CPS)，但這些系統容易遭受網路攻擊，可能造成災難性的影響。為了降低這些風險，已經開發出基於物理不變量的異常偵測方法。然而，這些方法通常需要特定領域的專業知識來手動定義不變量，這使得它們成本高昂且難以擴展。為了解決這個限制，我們提出了一種從 CPS 測試環境中提取物理不變量的創新方法，用於異常偵測。我們的見解是，CPS 設計文件通常包含語義豐富的物理程序描述，可以剖析系統組件之間相互關聯的動態。利用最近生成式 AI 模型中內建的物理和工程知識，我們旨在自動化這個傳統的手動流程，提高可擴展性並降低成本。這項工作專注於設計和最佳化檢索增強生成 (RAG) 工作流程，並使用針對 CPS 文件量身打造的提示系統，能夠準確提取語義資訊並從複雜的多模式內容中推斷出物理不變量。然後，我們不是直接應用推斷出的不變量進行異常偵測，而是引入了一種創新的基於統計的學習方法，將這些不變量整合到訓練資料集中。這種方法解決了幻覺和概念漂移等限制，增強了模型的可靠性。我們在包含 86 個資料點和 58 個攻擊案例的真實世界公開 CPS 安全資料集上評估了我們的做法。結果表明，我們的做法達到了 0.923 的高精度，準確地偵測異常，同時將誤報降到最低。</paragraph>

##### **Bias in Large Language Models: Origin, Evaluation, and Mitigation**
2411.10915v1 by Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu

Large Language Models (LLMs) have revolutionized natural language processing,
but their susceptibility to biases poses significant challenges. This
comprehensive review examines the landscape of bias in LLMs, from its origins
to current mitigation strategies. We categorize biases as intrinsic and
extrinsic, analyzing their manifestations in various NLP tasks. The review
critically assesses a range of bias evaluation methods, including data-level,
model-level, and output-level approaches, providing researchers with a robust
toolkit for bias detection. We further explore mitigation strategies,
categorizing them into pre-model, intra-model, and post-model techniques,
highlighting their effectiveness and limitations. Ethical and legal
implications of biased LLMs are discussed, emphasizing potential harms in
real-world applications such as healthcare and criminal justice. By
synthesizing current knowledge on bias in LLMs, this review contributes to the
ongoing effort to develop fair and responsible AI systems. Our work serves as a
comprehensive resource for researchers and practitioners working towards
understanding, evaluating, and mitigating bias in LLMs, fostering the
development of more equitable AI technologies.

摘要：大型語言模型 (LLM) 徹底改變了自然語言處理，但它們對偏見的敏感性帶來了重大挑戰。這篇綜合評論探討了 LLM 中偏見的現況，從其起源到當前的緩解策略。我們將偏見分類為內在和外在，分析它們在各種 NLP 任務中的表現。該評論批判性地評估了一系列偏見評估方法，包括資料層級、模型層級和輸出層級方法，為研究人員提供了用於偏見偵測的強大工具包。我們進一步探討緩解策略，將它們分類為模型前、模型中和模型後技術，強調它們的有效性和限制。討論了有偏見的 LLM 的道德和法律影響，強調在醫療保健和刑事司法等實際應用中潛在的危害。透過綜合當前關於 LLM 中偏見的知識，本評論有助於持續努力開發公平且負責任的人工智慧系統。我們的研究成果可作為研究人員和實務工作者的綜合資源，用於了解、評估和減輕 LLM 中的偏見，促進更公平的人工智慧技術的發展。

##### **BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment**
2411.10914v1 by Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen

Reinforcement Learning with Human Feedback (RLHF) is the key to the success
of large language models (LLMs) in recent years. In this work, we first
introduce the concepts of knowledge breadth and knowledge depth, which measure
the comprehensiveness and depth of an LLM or knowledge source respectively. We
reveal that the imbalance in the number of prompts and responses can lead to a
potential disparity in breadth and depth learning within alignment tuning
datasets by showing that even a simple uniform method for balancing the number
of instructions and responses can lead to significant improvements. Building on
this, we further propose Balanced Preference Optimization (BPO), designed to
dynamically augment the knowledge depth of each sample. BPO is motivated by the
observation that the usefulness of knowledge varies across samples,
necessitating tailored learning of knowledge depth. To achieve this, we
introduce gradient-based clustering, estimating the knowledge informativeness
and usefulness of each augmented sample based on the model's optimization
direction. Our experimental results across various benchmarks demonstrate that
BPO outperforms other baseline methods in alignment tuning while maintaining
training efficiency. Furthermore, we conduct a detailed analysis of each
component of BPO, providing guidelines for future research in preference data
optimization.

摘要：近年來，人類回饋強化學習 (RLHF) 是大型語言模型 (LLM) 成功發展的關鍵。在這項研究中，我們首先介紹知識廣度和知識深度的概念，分別用於衡量 LLM 或知識來源的全面性和深度。我們揭露提示和回應數量的不平衡可能導致對齊調整資料集中廣度和深度學習的潛在差異，並說明即使是平衡指令和回應數量的一種簡單統一方法，也能帶來顯著的改善。在此基礎上，我們進一步提出平衡偏好最佳化 (BPO)，旨在動態增加每個範例的知識深度。BPO 的靈感來自於一個觀察：知識的效用會因範例而異，因此需要客製化學習知識深度。為了達成這個目標，我們引入基於梯度的分群，根據模型的最佳化方向來估計每個已增加範例的知識資訊量和效用。我們在各種基準上的實驗結果顯示，BPO 在對齊調整中優於其他基準方法，同時維持訓練效率。此外，我們對 BPO 的每個組成部分進行詳細分析，為偏好資料最佳化的未來研究提供指導方針。

