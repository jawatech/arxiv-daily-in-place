{"2411.19876": {"publish_time": "2024-11-29", "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states", "paper_summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u5730\u4f7f\u7528\uff0c\u4f46\u5c0d\u6210\u54e1\u63a8\u8ad6\u7684\u64d4\u6182\u4e5f\u8207\u65e5\u4ff1\u589e\u3002\u5148\u524d\u7684\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u9ed1\u7bb1\u5230\u7070\u7bb1\u6a21\u578b\uff0c\u56e0\u6b64\u5ffd\u8996\u4e86 LLM \u5167\u90e8\u8cc7\u8a0a\u7684\u6f5b\u5728\u597d\u8655\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u7dda\u6027\u63a2\u6e2c (LP) \u4f5c\u70ba\u4e00\u7a2e\u65b9\u6cd5\uff0c\u900f\u904e\u6aa2\u67e5 LLM \u7684\u5167\u90e8\u6fc0\u6d3b\u4f86\u5075\u6e2c\u6210\u54e1\u63a8\u8ad6\u653b\u64ca (MIA)\u3002\u6211\u5011\u7684\u65b9\u6cd5\u7a31\u70ba LUMIA\uff0c\u5b83\u9010\u5c64\u61c9\u7528 LP \u4ee5\u53d6\u5f97\u95dc\u65bc\u6a21\u578b\u5167\u90e8\u904b\u4f5c\u7684\u7d30\u7c92\u5ea6\u8cc7\u6599\u3002\u6211\u5011\u5728\u5e7e\u500b\u6a21\u578b\u67b6\u69cb\u3001\u5927\u5c0f\u548c\u8cc7\u6599\u96c6\u4e0a\u6e2c\u8a66\u6b64\u65b9\u6cd5\uff0c\u5305\u62ec\u55ae\u6a21\u614b\u548c\u591a\u6a21\u614b\u4efb\u52d9\u3002\u5728\u55ae\u6a21\u614b MIA \u4e2d\uff0cLUMIA \u5728\u66f2\u7dda\u4e0b\u9762\u7a4d (AUC) \u4e0a\u6bd4\u5148\u524d\u7684\u6280\u8853\u5e73\u5747\u589e\u52a0\u4e86 15.71%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLUMIA \u5728 65.33% \u7684\u6848\u4f8b\u4e2d\u9054\u5230 AUC>60%\uff0c\u8207\u76ee\u524d\u6280\u8853\u76f8\u6bd4\u589e\u52a0\u4e86 46.80%\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u63ed\u793a\u4e86\u95dc\u9375\u898b\u89e3\uff0c\u4f8b\u5982 MIA \u6700\u5bb9\u6613\u88ab\u5075\u6e2c\u5230\u7684\u6a21\u578b\u5c64\u3002\u5728\u591a\u6a21\u614b\u6a21\u578b\u4e2d\uff0cLP \u6307\u51fa\u8996\u89ba\u8f38\u5165\u53ef\u4ee5\u986f\u8457\u6709\u52a9\u65bc\u5075\u6e2c MIA\uff0c\u5728 85.90% \u7684\u5be6\u9a57\u4e2d\u9054\u5230 AUC>60%\u3002", "author": "Luis Ibanez-Lissen et.al.", "authors": "Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro", "id": "2411.19876v1", "paper_url": "http://arxiv.org/abs/2411.19876v1", "repo": "null"}}