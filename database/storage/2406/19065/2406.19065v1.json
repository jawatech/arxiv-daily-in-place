{"2406.19065": {"publish_time": "2024-06-27", "title": "STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis", "paper_summary": "The rapid evolution of large language models (LLMs) holds promise for\nreforming the methodology of spatio-temporal data mining. However, current\nworks for evaluating the spatio-temporal understanding capability of LLMs are\nsomewhat limited and biased. These works either fail to incorporate the latest\nlanguage models or only focus on assessing the memorized spatio-temporal\nknowledge. To address this gap, this paper dissects LLMs' capability of\nspatio-temporal data into four distinct dimensions: knowledge comprehension,\nspatio-temporal reasoning, accurate computation, and downstream applications.\nWe curate several natural language question-answer tasks for each category and\nbuild the benchmark dataset, namely STBench, containing 13 distinct tasks and\nover 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs,\nsuch as GPT-4o, Gemma and Mistral. Experimental results reveal that existing\nLLMs show remarkable performance on knowledge comprehension and spatio-temporal\nreasoning tasks, with potential for further enhancement on other tasks through\nin-context learning, chain-of-though prompting, and fine-tuning. The code and\ndatasets of STBench are released on https://github.com/LwbXc/STBench.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\uff0c\u6709\u671b\u6539\u9769\u6642\u7a7a\u8cc7\u6599\u63a2\u52d8\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76ee\u524d\u7528\u65bc\u8a55\u4f30 LLM \u6642\u7a7a\u7406\u89e3\u80fd\u529b\u7684\u7814\u7a76\uff0c\u591a\u5c11\u6709\u4e9b\u9650\u5236\u548c\u504f\u898b\u3002\u9019\u4e9b\u7814\u7a76\u8981\u4e0d\u662f\u6c92\u6709\u7d0d\u5165\u6700\u65b0\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u5c31\u662f\u53ea\u8457\u91cd\u65bc\u8a55\u4f30\u8a18\u61b6\u7684\u6642\u7a7a\u77e5\u8b58\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u672c\u6587\u5c07 LLM \u5c0d\u6642\u7a7a\u8cc7\u6599\u7684\u80fd\u529b\u5256\u6790\u6210\u56db\u500b\u4e0d\u540c\u7684\u9762\u5411\uff1a\u77e5\u8b58\u7406\u89e3\u3001\u6642\u7a7a\u63a8\u7406\u3001\u6e96\u78ba\u904b\u7b97\u548c\u4e0b\u6e38\u61c9\u7528\u3002\u6211\u5011\u70ba\u6bcf\u500b\u985e\u5225\u7b56\u5283\u4e86\u5e7e\u500b\u81ea\u7136\u8a9e\u8a00\u554f\u7b54\u4efb\u52d9\uff0c\u4e26\u5efa\u7acb\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u4e5f\u5c31\u662f STBench\uff0c\u5176\u4e2d\u5305\u542b 13 \u500b\u4e0d\u540c\u7684\u4efb\u52d9\u548c\u8d85\u904e 60,000 \u500b\u554f\u7b54\u914d\u5c0d\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86 13 \u500b LLM \u7684\u80fd\u529b\uff0c\u4f8b\u5982 GPT-4o\u3001Gemma \u548c Mistral\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u73fe\u6709\u7684 LLM \u5728\u77e5\u8b58\u7406\u89e3\u548c\u6642\u7a7a\u63a8\u7406\u4efb\u52d9\u4e0a\u8868\u73fe\u5091\u51fa\uff0c\u4e26\u6709\u6f5b\u529b\u900f\u904e\u60c5\u5883\u5b78\u7fd2\u3001\u601d\u8003\u93c8\u63d0\u793a\u548c\u5fae\u8abf\uff0c\u5728\u5176\u4ed6\u4efb\u52d9\u4e0a\u9032\u4e00\u6b65\u63d0\u5347\u3002STBench \u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u5df2\u65bc https://github.com/LwbXc/STBench \u4e0a\u767c\u5e03\u3002", "author": "Wenbin Li et.al.", "authors": "Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi", "id": "2406.19065v1", "paper_url": "http://arxiv.org/abs/2406.19065v1", "repo": "https://github.com/lwbxc/stbench"}}