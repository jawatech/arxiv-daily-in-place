{"2501.04142": {"publish_time": "2025-01-07", "title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems", "paper_summary": "As machine learning (ML) systems increasingly impact critical sectors such as\nhiring, financial risk assessments, and criminal justice, the imperative to\nensure fairness has intensified due to potential negative implications. While\nmuch ML fairness research has focused on enhancing training data and processes,\naddressing the outputs of already deployed systems has received less attention.\nThis paper introduces 'BiasGuard', a novel approach designed to act as a\nfairness guardrail in production ML systems. BiasGuard leverages Test-Time\nAugmentation (TTA) powered by Conditional Generative Adversarial Network\n(CTGAN), a cutting-edge generative AI model, to synthesize data samples\nconditioned on inverted protected attribute values, thereby promoting equitable\noutcomes across diverse groups. This method aims to provide equal opportunities\nfor both privileged and unprivileged groups while significantly enhancing the\nfairness metrics of deployed systems without the need for retraining. Our\ncomprehensive experimental analysis across diverse datasets reveals that\nBiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%\ncompared to non-mitigated benchmarks. Additionally, BiasGuard outperforms\nexisting post-processing methods in improving fairness, positioning it as an\neffective tool to safeguard against biases when retraining the model is\nimpractical.", "paper_summary_zh": "\u96a8\u8457\u6a5f\u5668\u5b78\u7fd2 (ML) \u7cfb\u7d71\u65e5\u76ca\u5f71\u97ff\u62db\u8058\u3001\u8ca1\u52d9\u98a8\u96aa\u8a55\u4f30\u548c\u5211\u4e8b\u53f8\u6cd5\u7b49\u95dc\u9375\u90e8\u9580\uff0c\u7531\u65bc\u6f5b\u5728\u7684\u8ca0\u9762\u5f71\u97ff\uff0c\u78ba\u4fdd\u516c\u5e73\u6027\u7684\u5fc5\u8981\u6027\u4e5f\u96a8\u4e4b\u52a0\u5287\u3002\u96d6\u7136\u8a31\u591a ML \u516c\u5e73\u6027\u7814\u7a76\u90fd\u5c08\u6ce8\u65bc\u52a0\u5f37\u8a13\u7df4\u8cc7\u6599\u548c\u6d41\u7a0b\uff0c\u4f46\u5c0d\u65bc\u5df2\u90e8\u7f72\u7cfb\u7d71\u7684\u8f38\u51fa\uff0c\u5247\u8f03\u5c11\u53d7\u5230\u95dc\u6ce8\u3002\u672c\u6587\u4ecb\u7d39\u300cBiasGuard\u300d\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4f5c\u70ba\u751f\u7522 ML \u7cfb\u7d71\u4e2d\u7684\u516c\u5e73\u6027\u9632\u8b77\u63aa\u65bd\u3002BiasGuard \u900f\u904e\u689d\u4ef6\u751f\u6210\u5c0d\u6297\u7db2\u8def (CTGAN) \u9a45\u52d5\u7684\u6e2c\u8a66\u6642\u9593\u64f4\u5145 (TTA) \u4f86\u63d0\u5347\u6548\u80fd\uff0cCTGAN \u662f\u4e00\u7a2e\u5c16\u7aef\u7684\u751f\u6210\u5f0f AI \u6a21\u578b\uff0c\u53ef\u6839\u64da\u53cd\u5411\u53d7\u4fdd\u8b77\u5c6c\u6027\u503c\u4f86\u5408\u6210\u8cc7\u6599\u6a23\u672c\uff0c\u5f9e\u800c\u4fc3\u9032\u4e0d\u540c\u7fa4\u9ad4\u4e4b\u9593\u7684\u516c\u5e73\u7d50\u679c\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u70ba\u7279\u6b0a\u548c\u975e\u7279\u6b0a\u7fa4\u9ad4\u63d0\u4f9b\u5e73\u7b49\u7684\u6a5f\u6703\uff0c\u540c\u6642\u5927\u5e45\u63d0\u5347\u5df2\u90e8\u7f72\u7cfb\u7d71\u7684\u516c\u5e73\u6027\u6307\u6a19\uff0c\u800c\u7121\u9700\u91cd\u65b0\u8a13\u7df4\u3002\u6211\u5011\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5168\u9762\u5be6\u9a57\u5206\u6790\u986f\u793a\uff0cBiasGuard \u53ef\u5c07\u516c\u5e73\u6027\u63d0\u5347 31%\uff0c\u800c\u8207\u672a\u7de9\u89e3\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u6e96\u78ba\u5ea6\u50c5\u964d\u4f4e 0.09%\u3002\u6b64\u5916\uff0cBiasGuard \u5728\u6539\u5584\u516c\u5e73\u6027\u65b9\u9762\u512a\u65bc\u73fe\u6709\u7684\u5f8c\u8655\u7406\u65b9\u6cd5\uff0c\u4f7f\u5176\u6210\u70ba\u5728\u91cd\u65b0\u8a13\u7df4\u6a21\u578b\u4e0d\u5207\u5be6\u969b\u6642\uff0c\u7528\u65bc\u9632\u7bc4\u504f\u5dee\u7684\u6709\u6548\u5de5\u5177\u3002", "author": "Nurit Cohen-Inger et.al.", "authors": "Nurit Cohen-Inger, Seffi Cohen, Neomi Rabaev, Lior Rokach, Bracha Shapira", "id": "2501.04142v1", "paper_url": "http://arxiv.org/abs/2501.04142v1", "repo": "null"}}