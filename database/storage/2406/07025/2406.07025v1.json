{"2406.07025": {"publish_time": "2024-06-11", "title": "Entropy-Reinforced Planning with Large Language Models for Drug Discovery", "paper_summary": "The objective of drug discovery is to identify chemical compounds that\npossess specific pharmaceutical properties toward a binding target. Existing\nlarge language models (LLMS) can achieve high token matching scores in terms of\nlikelihood for molecule generation. However, relying solely on LLM decoding\noften results in the generation of molecules that are either invalid due to a\nsingle misused token, or suboptimal due to unbalanced exploration and\nexploitation as a consequence of the LLMs prior experience. Here we propose\nERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an\nentropy-reinforced planning algorithm to enhance the Transformer decoding\nprocess and strike a balance between exploitation and exploration. ERP aims to\nachieve improvements in multiple properties compared to direct sampling from\nthe Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human\ncancer cell target protein (RTCB) benchmarks and demonstrated that, in both\nbenchmarks, ERP consistently outperforms the current state-of-the-art algorithm\nby 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such\nimprovement is robust across Transformer models trained with different\nobjectives. Finally, to further illustrate the capabilities of ERP, we tested\nour algorithm on three code generation benchmarks and outperformed the current\nstate-of-the-art approach as well. Our code is publicly available at:\nhttps://github.com/xuefeng-cs/ERP.", "paper_summary_zh": "\u85e5\u7269\u767c\u73fe\u7684\u76ee\u6a19\u662f\u627e\u51fa\u5c0d\u7d50\u5408\u6a19\u9776\u5177\u6709\u7279\u5b9a\u85e5\u7406\u7279\u6027\u7684\u5316\u5b78\u5316\u5408\u7269\u3002\u73fe\u6709\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u5728\u5206\u5b50\u7522\u751f\u7684\u53ef\u80fd\u6027\u65b9\u9762\u9054\u5230\u5f88\u9ad8\u7684\u7b26\u865f\u5339\u914d\u5206\u6578\u3002\u7136\u800c\uff0c\u50c5\u4f9d\u8cf4 LLM \u89e3\u78bc\u5e38\u5e38\u6703\u7522\u751f\u7121\u6548\u7684\u5206\u5b50\uff0c\u539f\u56e0\u662f\u55ae\u4e00\u7b26\u865f\u8aa4\u7528\uff0c\u6216\u56e0\u70ba LLM \u5148\u524d\u7684\u7d93\u9a57\u5c0e\u81f4\u63a2\u7d22\u548c\u5229\u7528\u4e0d\u5e73\u8861\u800c\u5c0e\u81f4\u6b21\u4f73\u7d50\u679c\u3002\u5728\u6b64\uff0c\u6211\u5011\u63d0\u51fa ERP\uff0c\u4e5f\u5c31\u662fTransformer\u89e3\u78bc\u7684\u71b5\u589e\u5f37\u898f\u5283\uff0c\u5b83\u63a1\u7528\u71b5\u589e\u5f37\u898f\u5283\u6f14\u7b97\u6cd5\u4f86\u589e\u5f37Transformer\u89e3\u78bc\u7a0b\u5e8f\uff0c\u4e26\u5728\u5229\u7528\u548c\u63a2\u7d22\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u8207\u76f4\u63a5\u5f9eTransformer\u53d6\u6a23\u76f8\u6bd4\uff0cERP \u65e8\u5728\u6539\u5584\u591a\u91cd\u7279\u6027\u3002\u6211\u5011\u5728 SARS-CoV-2 \u75c5\u6bd2 (3CLPro) \u548c\u4eba\u985e\u764c\u7d30\u80de\u76ee\u6a19\u86cb\u767d\u8cea (RTCB) \u57fa\u6e96\u4e0a\u8a55\u4f30 ERP\uff0c\u4e26\u8b49\u660e\u5728\u5169\u500b\u57fa\u6e96\u4e2d\uff0cERP \u90fd\u6bd4\u73fe\u6709\u7684\u6700\u65b0\u6f14\u7b97\u6cd5\u9ad8\u51fa 1-5%\uff0c\u6bd4\u57fa\u6e96\u9ad8\u51fa 5-10%\u3002\u6b64\u5916\uff0c\u9019\u7a2e\u6539\u9032\u5728\u4f7f\u7528\u4e0d\u540c\u76ee\u6a19\u8a13\u7df4\u7684Transformer\u6a21\u578b\u4e2d\u90fd\u5f88\u7a69\u56fa\u3002\u6700\u5f8c\uff0c\u70ba\u4e86\u9032\u4e00\u6b65\u8aaa\u660e ERP \u7684\u80fd\u529b\uff0c\u6211\u5011\u5728\u4e09\u500b\u7a0b\u5f0f\u78bc\u7522\u751f\u57fa\u6e96\u4e0a\u6e2c\u8a66\u6211\u5011\u7684\u6f14\u7b97\u6cd5\uff0c\u4e26\u4e5f\u512a\u65bc\u73fe\u6709\u7684\u6700\u65b0\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc\uff1ahttps://github.com/xuefeng-cs/ERP\u3002", "author": "Xuefeng Liu et.al.", "authors": "Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens", "id": "2406.07025v1", "paper_url": "http://arxiv.org/abs/2406.07025v1", "repo": "null"}}