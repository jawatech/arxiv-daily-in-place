{"2405.17512": {"publish_time": "2024-05-27", "title": "On Fairness of Low-Rank Adaptation of Large Models", "paper_summary": "Low-rank adaptation of large models, particularly LoRA, has gained traction\ndue to its computational efficiency. This efficiency, contrasted with the\nprohibitive costs of full-model fine-tuning, means that practitioners often\nturn to LoRA and sometimes without a complete understanding of its\nramifications. In this study, we focus on fairness and ask whether LoRA has an\nunexamined impact on utility, calibration, and resistance to membership\ninference across different subgroups (e.g., genders, races, religions) compared\nto a full-model fine-tuning baseline. We present extensive experiments across\nvision and language domains and across classification and generation tasks\nusing ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly,\nexperiments suggest that while one can isolate cases where LoRA exacerbates\nmodel bias across subgroups, the pattern is inconsistent -- in many cases, LoRA\nhas equivalent or even improved fairness compared to the base model or its full\nfine-tuning baseline. We also examine the complications of evaluating\nfine-tuning fairness relating to task design and model token bias, calling for\nmore careful fairness evaluations in future work.", "paper_summary_zh": "\u4f4e\u79e9\u5927\u578b\u6a21\u578b\u7684\u9069\u61c9\uff0c\u7279\u5225\u662f LoRA\uff0c\u7531\u65bc\u5176\u8a08\u7b97\u6548\u7387\u800c\u7372\u5f97\u95dc\u6ce8\u3002\u6b64\u6548\u7387\u8207\u5168\u6a21\u578b\u5fae\u8abf\u7684\u7981\u6b62\u6210\u672c\u5f62\u6210\u5c0d\u6bd4\uff0c\u9019\u8868\u793a\u5f9e\u696d\u8005\u7d93\u5e38\u6c42\u52a9\u65bc LoRA\uff0c\u6709\u6642\u537b\u4e0d\u5b8c\u5168\u4e86\u89e3\u5176\u5f8c\u679c\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u516c\u5e73\u6027\uff0c\u4e26\u8a62\u554f LoRA \u662f\u5426\u5c0d\u6548\u7528\u3001\u6821\u6e96\u548c\u8207\u5168\u6a21\u578b\u5fae\u8abf\u57fa\u6e96\u7dda\u76f8\u6bd4\uff0c\u5c0d\u4e0d\u540c\u5b50\u7fa4\uff08\u4f8b\u5982\u6027\u5225\u3001\u7a2e\u65cf\u3001\u5b97\u6559\uff09\u7684\u6210\u54e1\u63a8\u8ad6\u7684\u62b5\u6297\u529b\u6709\u672a\u7d93\u6aa2\u9a57\u7684\u5f71\u97ff\u3002\u6211\u5011\u5728\u8996\u89ba\u548c\u8a9e\u8a00\u9818\u57df\u4ee5\u53ca\u4f7f\u7528 ViT-Base\u3001Swin-v2-Large\u3001Llama-2 7B \u548c Mistral 7B \u7684\u5206\u985e\u548c\u751f\u6210\u4efb\u52d9\u4e2d\u5c55\u793a\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u6709\u8da3\u7684\u662f\uff0c\u5be6\u9a57\u8868\u660e\uff0c\u96d6\u7136\u4eba\u5011\u53ef\u4ee5\u627e\u51fa LoRA \u5728\u5b50\u7fa4\u4e2d\u52a0\u5287\u6a21\u578b\u504f\u5dee\u7684\u60c5\u6cc1\uff0c\u4f46\u6a21\u5f0f\u4e26\u4e0d\u4e00\u81f4\u2014\u2014\u5728\u8a31\u591a\u60c5\u6cc1\u4e0b\uff0c\u8207\u57fa\u790e\u6a21\u578b\u6216\u5176\u5168\u5fae\u8abf\u57fa\u6e96\u7dda\u76f8\u6bd4\uff0cLoRA \u5177\u6709\u76f8\u7b49\u751a\u81f3\u66f4\u597d\u7684\u516c\u5e73\u6027\u3002\u6211\u5011\u9084\u6aa2\u9a57\u4e86\u8207\u4efb\u52d9\u8a2d\u8a08\u548c\u6a21\u578b\u4ee3\u5e63\u504f\u5dee\u76f8\u95dc\u7684\u5fae\u8abf\u516c\u5e73\u6027\u8a55\u4f30\u7684\u8907\u96dc\u6027\uff0c\u547c\u7c72\u5728\u672a\u4f86\u7684\u7814\u7a76\u4e2d\u9032\u884c\u66f4\u4ed4\u7d30\u7684\u516c\u5e73\u6027\u8a55\u4f30\u3002", "author": "Zhoujie Ding et.al.", "authors": "Zhoujie Ding, Ken Ziyu Liu, Pura Peetathawatchai, Berivan Isik, Sanmi Koyejo", "id": "2405.17512v1", "paper_url": "http://arxiv.org/abs/2405.17512v1", "repo": "null"}}