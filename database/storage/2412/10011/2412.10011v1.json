{"2412.10011": {"publish_time": "2024-12-13", "title": "Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework", "paper_summary": "Speech emotion recognition (SER) is crucial for enhancing affective computing\nand enriching the domain of human-computer interaction. However, the main\nchallenge in SER lies in selecting relevant feature representations from speech\nsignals with lower computational costs. In this paper, we propose a lightweight\nSER architecture that integrates attention-based local feature blocks (ALFBs)\nto capture high-level relevant feature vectors from speech signals. We also\nincorporate a global feature block (GFB) technique to capture sequential,\nglobal information and long-term dependencies in speech signals. By aggregating\nattention-based local and global contextual feature vectors, our model\neffectively captures the internal correlation between salient features that\nreflect complex human emotional cues. To evaluate our approach, we extracted\nfour types of spectral features from speech audio samples: mel-frequency\ncepstral coefficients, mel-spectrogram, root mean square value, and\nzero-crossing rate. Through a 5-fold cross-validation strategy, we tested the\nproposed method on five multi-lingual standard benchmark datasets: TESS,\nRAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of\n99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate\nthat our model achieves state-of-the-art (SOTA) performance compared to most\nexisting methods.", "paper_summary_zh": "\u8a9e\u97f3\u60c5\u7dd2\u8fa8\u8b58 (SER) \u5c0d\u65bc\u52a0\u5f37\u60c5\u611f\u904b\u7b97\u548c\u8c50\u5bcc\u4eba\u6a5f\u4e92\u52d5\u9818\u57df\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0cSER \u7684\u4e3b\u8981\u6311\u6230\u5728\u65bc\u4ee5\u8f03\u4f4e\u7684\u904b\u7b97\u6210\u672c\u5f9e\u8a9e\u97f3\u8a0a\u865f\u4e2d\u9078\u64c7\u76f8\u95dc\u7279\u5fb5\u8868\u5fb5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u8f15\u91cf\u7d1a SER \u67b6\u69cb\uff0c\u5b83\u6574\u5408\u4e86\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u5c40\u90e8\u7279\u5fb5\u5340\u584a (ALFB) \u4ee5\u5f9e\u8a9e\u97f3\u8a0a\u865f\u4e2d\u64f7\u53d6\u9ad8\u5c64\u7d1a\u76f8\u95dc\u7279\u5fb5\u5411\u91cf\u3002\u6211\u5011\u9084\u7d50\u5408\u4e86\u5168\u5c40\u7279\u5fb5\u5340\u584a (GFB) \u6280\u8853\uff0c\u4ee5\u64f7\u53d6\u8a9e\u97f3\u8a0a\u865f\u4e2d\u7684\u9806\u5e8f\u3001\u5168\u5c40\u8cc7\u8a0a\u548c\u9577\u671f\u4f9d\u8cf4\u6027\u3002\u900f\u904e\u5f59\u7e3d\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8108\u7d61\u7279\u5fb5\u5411\u91cf\uff0c\u6211\u5011\u7684\u6a21\u578b\u6709\u6548\u5730\u64f7\u53d6\u4e86\u53cd\u6620\u8907\u96dc\u4eba\u985e\u60c5\u7dd2\u7dda\u7d22\u7684\u986f\u8457\u7279\u5fb5\u4e4b\u9593\u7684\u5167\u90e8\u95dc\u806f\u6027\u3002\u70ba\u4e86\u8a55\u4f30\u6211\u5011\u7684\u4f5c\u6cd5\uff0c\u6211\u5011\u5f9e\u8a9e\u97f3\u97f3\u8a0a\u7bc4\u4f8b\u4e2d\u8403\u53d6\u4e86\u56db\u7a2e\u985e\u578b\u7684\u983b\u8b5c\u7279\u5fb5\uff1a\u6885\u723e\u983b\u7387\u5012\u983b\u7387\u8b5c\u4fc2\u6578\u3001\u6885\u723e\u983b\u8b5c\u5716\u3001\u5747\u65b9\u6839\u503c\u548c\u96f6\u4ea4\u8d8a\u7387\u3002\u900f\u904e 5 \u500d\u4ea4\u53c9\u9a57\u8b49\u7b56\u7565\uff0c\u6211\u5011\u5728\u4e94\u500b\u591a\u8a9e\u8a00\u6a19\u6e96\u57fa\u6e96\u8cc7\u6599\u96c6\uff1aTESS\u3001RAVDESS\u3001BanglaSER\u3001SUBESCO \u548c Emo-DB \u4e0a\u6e2c\u8a66\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4e26\u5206\u5225\u7372\u5f97\u4e86 99.65%\u300194.88%\u300198.12%\u300197.94% \u548c 97.19% \u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u5927\u591a\u6578\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u9054\u5230\u4e86\u6700\u5148\u9032 (SOTA) \u7684\u6548\u80fd\u3002", "author": "Niloy Kumar Kundu et.al.", "authors": "Niloy Kumar Kundu, Sarah Kobir, Md. Rayhan Ahmed, Tahmina Aktar, Niloya Roy", "id": "2412.10011v1", "paper_url": "http://arxiv.org/abs/2412.10011v1", "repo": "null"}}