{"2409.07146": {"publish_time": "2024-09-11", "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling", "paper_summary": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing\ncontext-aware memory reading and adaptive forgetting to improve memory capacity\nwhile maintaining compact recurrent state size. This design greatly enhances\nboth training and inference efficiency through GLA's hardware-efficient\ntraining algorithm and reduced state size. Additionally, retaining the softmax\noperation is particularly beneficial in \"finetuning pretrained Transformers to\nRNNs\" (T2R) settings, reducing the need for extensive training from scratch.\nExtensive experiments confirm GSA's superior performance in scenarios requiring\nin-context recall and in T2R settings.", "paper_summary_zh": "\u7dda\u6027\u6ce8\u610f\u529bTransformer\u53ca\u5176\u9580\u63a7\u8b8a\u9ad4\u56e0\u80fd\u9032\u884c\u5e73\u884c\u8a13\u7df4\u548c\u9ad8\u6548\u905e\u8ff4\u63a8\u8ad6\u800c\u53d7\u5230\u8b9a\u8b7d\uff0c\u4f46\u8207\u50b3\u7d71Transformer\u76f8\u6bd4\uff0c\u5728\u9700\u8981\u53ec\u56de\u7684\u5bc6\u96c6\u4efb\u52d9\u4e2d\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e14\u9700\u8981\u5927\u91cf\u8cc7\u6e90\u624d\u80fd\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u9580\u63a7\u6642\u9699\u6ce8\u610f\u529b (GSA)\uff0c\u5b83\u901a\u904e\u7d50\u5408\u53d7\u9580\u63a7\u7dda\u6027\u6ce8\u610f\u529b (GLA) \u555f\u767c\u7684\u9580\u63a7\u6a5f\u5236\u4f86\u589e\u5f37\u5e36\u908a\u754c\u8a18\u61b6\u63a7\u5236 (ABC) \u7684\u6ce8\u610f\u529b\u3002\u5be6\u8cea\u4e0a\uff0cGSA \u5305\u542b\u4e00\u500b\u901a\u904e softmax \u9023\u7d50\u7684\u5169\u5c64 GLA\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u8a18\u61b6\u9ad4\u8b80\u53d6\u548c\u81ea\u9069\u61c9\u907a\u5fd8\u4f86\u6539\u5584\u8a18\u61b6\u9ad4\u5bb9\u91cf\uff0c\u540c\u6642\u4fdd\u6301\u7dca\u6e4a\u7684\u905e\u8ff4\u72c0\u614b\u5927\u5c0f\u3002\u9019\u7a2e\u8a2d\u8a08\u901a\u904e GLA \u7684\u786c\u9ad4\u9ad8\u6548\u8a13\u7df4\u6f14\u7b97\u6cd5\u548c\u7e2e\u5c0f\u7684\u72c0\u614b\u5927\u5c0f\uff0c\u6975\u5927\u5730\u63d0\u9ad8\u4e86\u8a13\u7df4\u548c\u63a8\u8ad6\u6548\u7387\u3002\u6b64\u5916\uff0c\u5728\u300c\u5fae\u8abf\u9810\u8a13\u7df4Transformer\u5230 RNN\u300d(T2R) \u8a2d\u5b9a\u4e2d\u4fdd\u7559 softmax \u64cd\u4f5c\u7279\u5225\u6709\u76ca\uff0c\u6e1b\u5c11\u4e86\u5f9e\u982d\u958b\u59cb\u9032\u884c\u5927\u91cf\u8a13\u7df4\u7684\u9700\u8981\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u5be6\u4e86 GSA \u5728\u9700\u8981\u4e0a\u4e0b\u6587\u53ec\u56de\u548c T2R \u8a2d\u5b9a\u4e2d\u7684\u5834\u666f\u4e2d\u7684\u5353\u8d8a\u6548\u80fd\u3002", "author": "Yu Zhang et.al.", "authors": "Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, Guohong Fu", "id": "2409.07146v1", "paper_url": "http://arxiv.org/abs/2409.07146v1", "repo": "null"}}