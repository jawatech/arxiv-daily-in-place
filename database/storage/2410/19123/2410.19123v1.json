{"2410.19123": {"publish_time": "2024-10-24", "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design", "paper_summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6fc0\u589e\u5c0e\u81f4\u63a1\u7528\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\uff0c\u8a72\u67b6\u69cb\u52d5\u614b\u5229\u7528\u5c08\u696d\u5b50\u7db2\u8def\u4f86\u63d0\u9ad8\u6548\u7387\u548c\u6548\u80fd\u3002\u5118\u7ba1\u6709\u5176\u512a\u9ede\uff0cMoE \u6a21\u578b\u5728\u63a8\u8ad6\u671f\u9593\u4ecd\u9762\u81e8\u91cd\u5927\u6311\u6230\uff0c\u5305\u62ec\u7531\u65bc\u6a21\u578b\u67b6\u69cb\u548c\u7cfb\u7d71\u653f\u7b56\u4e4b\u9593\u7684\u8a2d\u8a08\u9078\u64c7\u4e0d\u4e00\u81f4\uff0c\u5c0e\u81f4\u8a18\u61b6\u9ad4\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u548c\u6279\u6b21\u8655\u7406\u6b21\u4f73\u3002\u6b64\u5916\uff0c\u5f9e\u982d\u8a13\u7df4 MoE \u7684\u50b3\u7d71\u65b9\u6cd5\u5728\u6210\u672c\u65b9\u9762\u8d8a\u4f86\u8d8a\u4ee4\u4eba\u671b\u800c\u537b\u6b65\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6 Read-ME\uff0c\u5c07\u9810\u5148\u8a13\u7df4\u7684\u5bc6\u96c6 LLM \u8f49\u63db\u6210\u8f03\u5c0f\u7684 MoE \u6a21\u578b\uff08\u8207\u300c\u5347\u7d1a\u300d\u901a\u624d MoE \u76f8\u53cd\uff09\uff0c\u907f\u514d\u5f9e\u982d\u8a13\u7df4\u7684\u9ad8\u6602\u6210\u672c\u3002\u6211\u5011\u7684\u505a\u6cd5\u63a1\u7528\u6fc0\u6d3b\u7a00\u758f\u6027\u4f86\u63d0\u53d6\u5c08\u5bb6\u3002\u70ba\u4e86\u7d44\u6210\u5c08\u5bb6\uff0c\u6211\u5011\u6aa2\u8996\u5ee3\u6cdb\u63a1\u7528\u7684\u9010\u5c64\u8def\u7531\u5668\u8a2d\u8a08\u4e26\u986f\u793a\u5176\u5197\u9918\uff0c\u56e0\u6b64\u6211\u5011\u5f15\u5165\u4e86\u8207 MoE \u4e3b\u5e79\u5206\u96e2\u7684\u9810\u9598\u9580\u8def\u7531\u5668\uff0c\u5b83\u6709\u52a9\u65bc\u7cfb\u7d71\u53cb\u5584\u7684\u9810\u5148\u904b\u7b97\u548c\u524d\u77bb\u6392\u7a0b\uff0c\u589e\u5f37\u5c08\u5bb6\u611f\u77e5\u7684\u6279\u6b21\u8655\u7406\u548c\u5feb\u53d6\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u5171\u540c\u8a2d\u8a08\u89e3\u6c7a\u4e86\u6f14\u7b97\u6cd5\u548c\u7cfb\u7d71\u65b9\u9762\u7684\u95dc\u9375\u5dee\u8ddd\uff0c\u70ba\u8cc7\u6e90\u53d7\u9650\u8a2d\u5b9a\u4e2d\u7684 LLM \u63a8\u8ad6\u5efa\u7acb\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002Read-ME \u512a\u65bc\u5176\u4ed6\u6d41\u884c\u7684\u985e\u4f3c\u898f\u6a21\u958b\u6e90\u5bc6\u96c6\u6a21\u578b\uff0c\u5728 MMLU \u4e0a\u5be6\u73fe\u9ad8\u9054 10.1% \u7684\u6539\u9032\uff0c\u4e26\u5c07\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u9072\u6642\u9593\u6539\u5584\u9ad8\u9054 6.1%\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u53d6\u5f97\uff1a\nhttps://github.com/VITA-Group/READ-ME\u3002</paragraph>", "author": "Ruisi Cai et.al.", "authors": "Ruisi Cai, Yeonju Ro, Geon-Woo Kim, Peihao Wang, Babak Ehteshami Bejnordi, Aditya Akella, Zhangyang Wang", "id": "2410.19123v1", "paper_url": "http://arxiv.org/abs/2410.19123v1", "repo": "null"}}