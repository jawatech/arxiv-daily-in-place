{"2412.02252": {"publish_time": "2024-12-03", "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity", "paper_summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f8b\u5982 GPT \u548c LLaMA \u7cfb\u5217\uff0c\u5176\u4e0d\u65b7\u589e\u52a0\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u6539\u5584\u4e86\u5b83\u5011\u8655\u7406\u8907\u96dc\u3001\u9577\u6587\u672c\u4efb\u52d9\u7684\u80fd\u529b\uff0c\u4f46\u4ee3\u50f9\u662f\u63a8\u8ad6\u6548\u7387\uff0c\u7279\u5225\u662f\u5728\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8907\u96dc\u6027\u65b9\u9762\u3002\u73fe\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u9078\u64c7\u6027\u4fdd\u7559\u6a19\u8a18\u548c\u57fa\u65bc\u7a97\u53e3\u7684\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u6709\u98a8\u96aa\u6703\u4e1f\u68c4\u672a\u4f86\u6587\u672c\u751f\u6210\u6240\u9700\u7684\u6a19\u8a18\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u900f\u904e\u6e1b\u5c11\u4e0d\u91cd\u8981\u6a19\u8a18\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8ca0\u8f09\uff0c\u800c\u4e0d\u662f\u4e1f\u68c4\u5b83\u5011\uff0c\u4f86\u589e\u5f37 LLM \u6548\u7387\uff0c\u800c\u4e0d\u6703\u640d\u5931\u6a19\u8a18\u3002\u6211\u5011\u89e3\u6c7a\u4e86\u5169\u500b\u6311\u6230\uff1a1) \u8abf\u67e5\u4e0a\u4e0b\u6587\u4e2d\u91cd\u8981\u6a19\u8a18\u7684\u5206\u5e03\uff0c\u767c\u73fe\u6700\u8fd1\u7684\u6a19\u8a18\u6bd4\u4e0a\u4e0b\u6587\u4e2d\u8f03\u9060\u7684\u6a19\u8a18\u66f4\u91cd\u8981\uff0c\u4ee5\u53ca 2) \u900f\u904e\u5728\u5404\u5c64\u4e4b\u9593\u5171\u4eab\u6ce8\u610f\u529b\u5206\u6578\uff0c\u4f86\u512a\u5316\u9060\u7aef\u6a19\u8a18\u7684\u8cc7\u6e90\u3002\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u7bc0\u7701\u4e86 35% \u7684 KV \u5feb\u53d6\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u6548\u80fd\u3002", "author": "Da Ma et.al.", "authors": "Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu", "id": "2412.02252v1", "paper_url": "http://arxiv.org/abs/2412.02252v1", "repo": "null"}}