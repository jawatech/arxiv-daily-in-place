{"2410.18693": {"publish_time": "2024-10-24", "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch", "paper_summary": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet.", "paper_summary_zh": "\u9ad8\u54c1\u8cea\u8cc7\u6599\u7684\u53d6\u5f97\u662f\u63d0\u5347 LLM \u63a8\u7406\u80fd\u529b\u6700\u91cd\u8981\u7684\u56e0\u7d20\u4e4b\u4e00\u3002\u73fe\u6709\u7814\u7a76\u5df2\u8b49\u5be6\u5f9e\u7a2e\u5b50\u554f\u984c\u6216\u77e5\u8b58\u5eab\u5efa\u7acb\u66f4\u591a\u6307\u4ee4\u8cc7\u6599\u7684\u6709\u6548\u6027\u3002\u6700\u8fd1\u7684\u7814\u7a76\u6307\u51fa\uff0c\u6301\u7e8c\u64f4\u5145\u5f37\u5927\u6a21\u578b\uff08\u4f8b\u5982 GPT-4\uff09\u7684\u8cc7\u6599\u5408\u6210\uff0c\u53ef\u4ee5\u9032\u4e00\u6b65\u5f15\u767c\u63a8\u7406\u6548\u80fd\u3002\u5118\u7ba1\u524d\u666f\u770b\u597d\uff0c\u958b\u6e90\u793e\u7fa4\u4ecd\u7f3a\u4e4f\u898f\u6a21\u5316\u7684\u512a\u8cea\u8cc7\u6599\u548c\u50f9\u683c\u5408\u7406\u7684\u53ef\u64f4\u5145\u8cc7\u6599\u5408\u6210\u65b9\u6cd5\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u9032 ScaleQuest\uff0c\u9019\u662f\u4e00\u500b\u53ef\u64f4\u5145\u4e14\u65b0\u7a4e\u7684\u8cc7\u6599\u5408\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u300c\u5c0f\u898f\u6a21\u300d\uff08\u4f8b\u5982 7B\uff09\u958b\u6e90\u6a21\u578b\u5f9e\u982d\u7522\u751f\u554f\u984c\uff0c\u800c\u7121\u9700\u5177\u5099\u8907\u96dc\u64f4\u5145\u7d04\u675f\u7684\u7a2e\u5b50\u8cc7\u6599\u3002\u85c9\u7531\u6709\u6548\u7387\u7684 ScaleQuest\uff0c\u6211\u5011\u81ea\u52d5\u5efa\u69cb\u4e86\u4e00\u500b\u7531 100 \u842c\u500b\u554f\u984c\u89e3\u6c7a\u914d\u5c0d\u7d44\u6210\u7684\u6578\u5b78\u63a8\u7406\u8cc7\u6599\u96c6\uff0c\u5176\u6548\u80fd\u512a\u65bc\u73fe\u6709\u7684\u958b\u6e90\u8cc7\u6599\u96c6\u3002\u5b83\u53ef\u4ee5\u666e\u904d\u63d0\u5347\u4e3b\u6d41\u958b\u6e90\u6a21\u578b\uff08\u5373 Mistral\u3001Llama3\u3001DeepSeekMath \u548c Qwen2-Math\uff09\u7684\u6548\u80fd\uff0c\u5728 MATH \u4e0a\u7372\u5f97 29.2% \u5230 46.4% \u7684\u589e\u76ca\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u50c5\u4f7f\u7528\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5fae\u8abf Qwen2-Math-7B-Base \u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5\u8d85\u8d8a Qwen2-Math-7B-Instruct\uff0c\u9019\u662f\u4e00\u500b\u5728\u5c01\u9589\u539f\u59cb\u78bc\u8cc7\u6599\u4e0a\u5f37\u5927\u4e14\u5c0d\u9f4a\u826f\u597d\u7684\u6a21\u578b\uff0c\u4ee5\u53ca GPT-4-Turbo \u548c Claude-3.5 Sonnet \u7b49\u5c08\u6709\u6a21\u578b\u3002", "author": "Yuyang Ding et.al.", "authors": "Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, Min Zhang", "id": "2410.18693v1", "paper_url": "http://arxiv.org/abs/2410.18693v1", "repo": "https://github.com/yyding1/scalequest"}}