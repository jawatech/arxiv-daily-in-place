{"2410.15977": {"publish_time": "2024-10-21", "title": "Enabling Energy-Efficient Deployment of Large Language Models on Memristor Crossbar: A Synergy of Large and Small", "paper_summary": "Large language models (LLMs) have garnered substantial attention due to their\npromising applications in diverse domains. Nevertheless, the increasing size of\nLLMs comes with a significant surge in the computational requirements for\ntraining and deployment. Memristor crossbars have emerged as a promising\nsolution, which demonstrated a small footprint and remarkably high energy\nefficiency in computer vision (CV) models. Memristors possess higher density\ncompared to conventional memory technologies, making them highly suitable for\neffectively managing the extreme model size associated with LLMs. However,\ndeploying LLMs on memristor crossbars faces three major challenges. Firstly,\nthe size of LLMs increases rapidly, already surpassing the capabilities of\nstate-of-the-art memristor chips. Secondly, LLMs often incorporate multi-head\nattention blocks, which involve non-weight stationary multiplications that\ntraditional memristor crossbars cannot support. Third, while memristor\ncrossbars excel at performing linear operations, they are not capable of\nexecuting complex nonlinear operations in LLM such as softmax and layer\nnormalization. To address these challenges, we present a novel architecture for\nthe memristor crossbar that enables the deployment of state-of-the-art LLM on a\nsingle chip or package, eliminating the energy and time inefficiencies\nassociated with off-chip communication. Our testing on BERT_Large showed\nnegligible accuracy loss. Compared to traditional memristor crossbars, our\narchitecture achieves enhancements of up to 39X in area overhead and 18X in\nenergy consumption. Compared to modern TPU/GPU systems, our architecture\ndemonstrates at least a 68X reduction in the area-delay product and a\nsignificant 69% energy consumption reduction.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u56e0\u5176\u5728\u5404\u7a2e\u9818\u57df\u4e2d\u5177\u6709\u5ee3\u6cdb\u7684\u61c9\u7528\u524d\u666f\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u5118\u7ba1\u5982\u6b64\uff0cLLM \u7684\u898f\u6a21\u65e5\u76ca\u9f90\u5927\uff0c\u5c0d\u8a13\u7df4\u548c\u90e8\u7f72\u7684\u8a08\u7b97\u9700\u6c42\u4e5f\u5927\u5e45\u589e\u52a0\u3002\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u5728\u8a08\u7b97\u6a5f\u8996\u89ba (CV) \u6a21\u578b\u4e2d\u5c55\u793a\u4e86\u8f03\u5c0f\u7684\u4f54\u7528\u7a7a\u9593\u548c\u6975\u9ad8\u7684\u80fd\u6548\u3002\u8207\u50b3\u7d71\u8a18\u61b6\u9ad4\u6280\u8853\u76f8\u6bd4\uff0c\u61b6\u963b\u5668\u5177\u6709\u66f4\u9ad8\u7684\u5bc6\u5ea6\uff0c\u9019\u4f7f\u5176\u975e\u5e38\u9069\u5408\u6709\u6548\u7ba1\u7406\u8207 LLM \u76f8\u95dc\u7684\u6975\u7aef\u6a21\u578b\u5927\u5c0f\u3002\u7136\u800c\uff0c\u5728\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u4e0a\u90e8\u7f72 LLM \u9762\u81e8\u4e09\u5927\u6311\u6230\u3002\u9996\u5148\uff0cLLM \u7684\u898f\u6a21\u8fc5\u901f\u64f4\u5927\uff0c\u5df2\u7d93\u8d85\u904e\u4e86\u6700\u5148\u9032\u7684\u61b6\u963b\u5668\u6676\u7247\u7684\u6548\u80fd\u3002\u5176\u6b21\uff0cLLM \u901a\u5e38\u5305\u542b\u591a\u982d\u6ce8\u610f\u529b\u5340\u584a\uff0c\u5176\u4e2d\u6d89\u53ca\u975e\u6b0a\u91cd\u5e73\u7a69\u4e58\u6cd5\uff0c\u800c\u50b3\u7d71\u7684\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u7121\u6cd5\u652f\u63f4\u3002\u7b2c\u4e09\uff0c\u5118\u7ba1\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u64c5\u9577\u57f7\u884c\u7dda\u6027\u904b\u7b97\uff0c\u4f46\u5b83\u5011\u7121\u6cd5\u57f7\u884c LLM \u4e2d\u7684\u8907\u96dc\u975e\u7dda\u6027\u904b\u7b97\uff0c\u4f8b\u5982 softmax \u548c\u5c64\u6a19\u6e96\u5316\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7528\u65bc\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u7684\u65b0\u67b6\u69cb\uff0c\u8a72\u67b6\u69cb\u53ef\u4ee5\u5728\u55ae\u4e00\u6676\u7247\u6216\u5c01\u88dd\u4e0a\u90e8\u7f72\u6700\u5148\u9032\u7684 LLM\uff0c\u5f9e\u800c\u6d88\u9664\u8207\u6676\u7247\u5916\u901a\u8a0a\u76f8\u95dc\u7684\u80fd\u8017\u548c\u6642\u9593\u6548\u7387\u4f4e\u4e0b\u3002\u6211\u5011\u5728 BERT_Large \u4e0a\u7684\u6e2c\u8a66\u986f\u793a\u51fa\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u7684\u6e96\u78ba\u5ea6\u640d\u5931\u3002\u8207\u50b3\u7d71\u7684\u61b6\u963b\u5668\u4ea4\u53c9\u9663\u5217\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728\u9762\u7a4d\u958b\u92b7\u65b9\u9762\u63d0\u5347\u4e86 39 \u500d\uff0c\u5728\u80fd\u8017\u65b9\u9762\u63d0\u5347\u4e86 18 \u500d\u3002\u8207\u73fe\u4ee3 TPU/GPU \u7cfb\u7d71\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728\u9762\u7a4d\u5ef6\u9072\u4e58\u7a4d\u65b9\u9762\u81f3\u5c11\u6e1b\u5c11\u4e86 68 \u500d\uff0c\u80fd\u8017\u65b9\u9762\u986f\u8457\u6e1b\u5c11\u4e86 69%\u3002", "author": "Zhehui Wang et.al.", "authors": "Zhehui Wang, Tao Luo, Cheng Liu, Weichen Liu, Rick Siow Mong Goh, Weng-Fai Wong", "id": "2410.15977v1", "paper_url": "http://arxiv.org/abs/2410.15977v1", "repo": "null"}}