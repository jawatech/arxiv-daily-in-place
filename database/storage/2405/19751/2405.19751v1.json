{"2405.19751": {"publish_time": "2024-05-30", "title": "HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization", "paper_summary": "Diffusion Transformers (DiTs) have recently gained substantial attention in\nboth industrial and academic fields for their superior visual generation\ncapabilities, outperforming traditional diffusion models that use U-Net.\nHowever,the enhanced performance of DiTs also comes with high parameter counts\nand implementation costs, seriously restricting their use on resource-limited\ndevices such as mobile phones. To address these challenges, we introduce the\nHybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training\nquantization method that utilizes 4-bit floating-point (FP) precision on both\nweights and activations for DiT inference. Compared to fixed-point quantization\n(e.g., INT8), FP quantization, complemented by our proposed clipping range\nselection mechanism, naturally aligns with the data distribution within DiT,\nresulting in a minimal quantization error. Furthermore, HQ-DiT also implements\na universal identity mathematical transform to mitigate the serious\nquantization error caused by the outliers. The experimental results demonstrate\nthat DiT can achieve extremely low-precision quantization (i.e., 4 bits) with\nnegligible impact on performance. Our approach marks the first instance where\nboth weights and activations in DiTs are quantized to just 4 bits, with only a\n0.12 increase in sFID on ImageNet.", "paper_summary_zh": "\u64f4\u6563Transformer\uff08DiT\uff09\u6700\u8fd1\u5728\u7522\u696d\u548c\u5b78\u8853\u9818\u57df\u7372\u5f97\u6975\u5927\u7684\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5011\u7684\u8996\u89ba\u751f\u6210\u80fd\u529b\u512a\u65bc\u4f7f\u7528 U-Net \u7684\u50b3\u7d71\u64f4\u6563\u6a21\u578b\u3002\u7136\u800c\uff0cDiT \u589e\u5f37\u7684\u6548\u80fd\u4e5f\u4f34\u96a8\u8457\u9ad8\u53c3\u6578\u6578\u91cf\u548c\u5be6\u4f5c\u6210\u672c\uff0c\u56b4\u91cd\u9650\u5236\u4e86\u5b83\u5011\u5728\u8cc7\u6e90\u6709\u9650\u7684\u88dd\u7f6e\uff08\u4f8b\u5982\u624b\u6a5f\uff09\u4e0a\u7684\u4f7f\u7528\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 DiT \u7684\u6df7\u5408\u6d6e\u9ede\u91cf\u5316\uff08HQ-DiT\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u9ad8\u6548\u7684\u8a13\u7df4\u5f8c\u91cf\u5316\u65b9\u6cd5\uff0c\u5728 DiT \u63a8\u8ad6\u4e2d\u5c0d\u6b0a\u91cd\u548c\u555f\u7528\u90fd\u4f7f\u7528 4 \u4f4d\u5143\u6d6e\u9ede\uff08FP\uff09\u7cbe\u5ea6\u3002\u8207\u5b9a\u9ede\u91cf\u5316\uff08\u4f8b\u5982 INT8\uff09\u76f8\u6bd4\uff0cFP \u91cf\u5316\u642d\u914d\u6211\u5011\u63d0\u51fa\u7684\u88c1\u5207\u7bc4\u570d\u9078\u64c7\u6a5f\u5236\uff0c\u81ea\u7136\u5730\u8207 DiT \u4e2d\u7684\u8cc7\u6599\u5206\u4f48\u4e00\u81f4\uff0c\u5f9e\u800c\u7522\u751f\u6700\u5c0f\u7684\u91cf\u5316\u8aa4\u5dee\u3002\u6b64\u5916\uff0cHQ-DiT \u9084\u5be6\u4f5c\u4e86\u4e00\u500b\u901a\u7528\u7684\u6046\u7b49\u6578\u5b78\u8f49\u63db\uff0c\u4ee5\u6e1b\u8f15\u7570\u5e38\u503c\u9020\u6210\u7684\u56b4\u91cd\u91cf\u5316\u8aa4\u5dee\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cDiT \u53ef\u4ee5\u5be6\u73fe\u6975\u4f4e\u7cbe\u5ea6\u7684\u91cf\u5316\uff08\u5373 4 \u4f4d\u5143\uff09\uff0c\u4e14\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u3002\u6211\u5011\u7684\u505a\u6cd5\u6a19\u8a8c\u8457 DiT \u4e2d\u7684\u6b0a\u91cd\u548c\u555f\u7528\u9996\u6b21\u88ab\u91cf\u5316\u5230\u53ea\u6709 4 \u4f4d\u5143\uff0c\u5728 ImageNet \u4e0a\u7684 sFID \u53ea\u589e\u52a0\u4e86 0.12\u3002", "author": "Wenxuan Liu et.al.", "authors": "Wenxuan Liu, Saiqian Zhang", "id": "2405.19751v1", "paper_url": "http://arxiv.org/abs/2405.19751v1", "repo": "null"}}