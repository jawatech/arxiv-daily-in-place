{"2407.10362": {"publish_time": "2024-07-14", "title": "LAB-Bench: Measuring Capabilities of Language Models for Biology Research", "paper_summary": "There is widespread optimism that frontier Large Language Models (LLMs) and\nLLM-augmented systems have the potential to rapidly accelerate scientific\ndiscovery across disciplines. Today, many benchmarks exist to measure LLM\nknowledge and reasoning on textbook-style science questions, but few if any\nbenchmarks are designed to evaluate language model performance on practical\ntasks required for scientific research, such as literature search, protocol\nplanning, and data analysis. As a step toward building such benchmarks, we\nintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of\nover 2,400 multiple choice questions for evaluating AI systems on a range of\npractical biology research capabilities, including recall and reasoning over\nliterature, interpretation of figures, access and navigation of databases, and\ncomprehension and manipulation of DNA and protein sequences. Importantly, in\ncontrast to previous scientific benchmarks, we expect that an AI system that\ncan achieve consistently high scores on the more difficult LAB-Bench tasks\nwould serve as a useful assistant for researchers in areas such as literature\nsearch and molecular cloning. As an initial assessment of the emergent\nscientific task capabilities of frontier language models, we measure\nperformance of several against our benchmark and report results compared to\nhuman expert biology researchers. We will continue to update and expand\nLAB-Bench over time, and expect it to serve as a useful tool in the development\nof automated research systems going forward. A public subset of LAB-Bench is\navailable for use at the following URL:\nhttps://huggingface.co/datasets/futurehouse/lab-bench", "paper_summary_zh": "\u8a9e\u8a00\u5927\u578b\u6a21\u578b\uff08LLM\uff09\u548c LLM \u589e\u5f37\u7cfb\u7d71\u666e\u904d\u88ab\u6a02\u89c0\u8a8d\u70ba\u6709\u6f5b\u529b\u80fd\u5feb\u901f\u52a0\u901f\u5404\u9818\u57df\u7684\u79d1\u5b78\u767c\u73fe\u3002\u73fe\u4eca\uff0c\u8a31\u591a\u57fa\u6e96\u5b58\u5728\u7528\u65bc\u8861\u91cf LLM \u77e5\u8b58\u548c\u6559\u79d1\u66f8\u5f0f\u79d1\u5b78\u554f\u984c\u7684\u63a8\u7406\uff0c\u4f46\u5f88\u5c11\u6709\u57fa\u6e96\u88ab\u8a2d\u8a08\u7528\u65bc\u8a55\u4f30\u8a9e\u8a00\u6a21\u578b\u5728\u79d1\u5b78\u7814\u7a76\u4e2d\u6240\u9700\u5be6\u969b\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u4f8b\u5982\u6587\u737b\u641c\u5c0b\u3001\u5be6\u9a57\u898f\u756b\u548c\u8cc7\u6599\u5206\u6790\u3002\u4f5c\u70ba\u5efa\u69cb\u6b64\u985e\u57fa\u6e96\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u5011\u5f15\u5165\u4e86\u8a9e\u8a00\u4ee3\u7406\u751f\u7269\u57fa\u6e96\uff08LAB-Bench\uff09\uff0c\u4e00\u500b\u5305\u542b\u8d85\u904e 2,400 \u500b\u591a\u9078\u984c\u7684\u5ee3\u6cdb\u8cc7\u6599\u96c6\uff0c\u7528\u65bc\u8a55\u4f30 AI \u7cfb\u7d71\u5728\u5404\u7a2e\u5be6\u969b\u751f\u7269\u7814\u7a76\u80fd\u529b\u4e0a\u7684\u8868\u73fe\uff0c\u5305\u62ec\u6587\u737b\u7684\u56de\u6eaf\u548c\u63a8\u7406\u3001\u5716\u8868\u7684\u89e3\u8b80\u3001\u8cc7\u6599\u5eab\u7684\u5b58\u53d6\u548c\u5c0e\u89bd\uff0c\u4ee5\u53ca DNA \u548c\u86cb\u767d\u8cea\u5e8f\u5217\u7684\u7406\u89e3\u548c\u64cd\u4f5c\u3002\u91cd\u8981\u7684\u662f\uff0c\u8207\u5148\u524d\u7684\u79d1\u5b78\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u9810\u671f\u4e00\u500b\u80fd\u5728\u8f03\u56f0\u96e3\u7684 LAB-Bench \u4efb\u52d9\u4e2d\u6301\u7e8c\u7372\u5f97\u9ad8\u5206\u7684 AI \u7cfb\u7d71\uff0c\u5c07\u80fd\u6210\u70ba\u7814\u7a76\u4eba\u54e1\u5728\u6587\u737b\u641c\u5c0b\u548c\u5206\u5b50\u8907\u88fd\u7b49\u9818\u57df\u4e2d\u7684\u6709\u7528\u52a9\u624b\u3002\u4f5c\u70ba\u5c0d\u524d\u6cbf\u8a9e\u8a00\u6a21\u578b\u65b0\u8208\u79d1\u5b78\u4efb\u52d9\u80fd\u529b\u7684\u521d\u6b65\u8a55\u4f30\uff0c\u6211\u5011\u6e2c\u91cf\u4e86\u5e7e\u500b\u6a21\u578b\u91dd\u5c0d\u6211\u5011\u57fa\u6e96\u7684\u8868\u73fe\uff0c\u4e26\u56de\u5831\u8207\u4eba\u985e\u5c08\u5bb6\u751f\u7269\u7814\u7a76\u4eba\u54e1\u6bd4\u8f03\u5f8c\u7684\u7d50\u679c\u3002\u6211\u5011\u5c07\u6301\u7e8c\u66f4\u65b0\u548c\u64f4\u5145 LAB-Bench\uff0c\u4e26\u9810\u671f\u5b83\u5c07\u6210\u70ba\u672a\u4f86\u81ea\u52d5\u5316\u7814\u7a76\u7cfb\u7d71\u958b\u767c\u4e2d\u7684\u4e00\u500b\u6709\u7528\u5de5\u5177\u3002LAB-Bench \u7684\u516c\u958b\u5b50\u96c6\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u4f7f\u7528\uff1a\nhttps://huggingface.co/datasets/futurehouse/lab-bench", "author": "Jon M. Laurent et.al.", "authors": "Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, Samuel G. Rodriques", "id": "2407.10362v1", "paper_url": "http://arxiv.org/abs/2407.10362v1", "repo": "null"}}