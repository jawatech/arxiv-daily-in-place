{"2406.04127": {"publish_time": "2024-06-06", "title": "Are We Done with MMLU?", "paper_summary": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\ntaxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually\nre-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we\ndemonstrate significant discrepancies with the model performance metrics that\nwere originally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. Therefore, we open up MMLU-Redux for additional annotation\nhttps://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.", "paper_summary_zh": "\u4e5f\u8a31\u4e0d\u662f\u3002\u6211\u5011\u8b58\u5225\u4e26\u5206\u6790\u5ee3\u6cdb\u63a1\u7528\u7684 Massive Multitask Language Understanding (MMLU) \u57fa\u6e96\u4e2d\u7684\u932f\u8aa4\u3002\u5118\u7ba1 MMLU \u5ee3\u6cdb\u63a1\u7528\uff0c\u4f46\u6211\u5011\u7684\u5206\u6790\u8b49\u660e\u4e86\u8a31\u591a\u6a21\u7cca LLM \u771f\u5be6\u529f\u80fd\u7684\u57fa\u672c\u4e8b\u5be6\u932f\u8aa4\u3002\u4f8b\u5982\uff0c\u6211\u5011\u767c\u73fe\u75c5\u6bd2\u5b78\u5b50\u96c6\u4e2d 57% \u7684\u5206\u6790\u554f\u984c\u5305\u542b\u932f\u8aa4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u4f7f\u7528\u65b0\u932f\u8aa4\u5206\u985e\u6cd5\u8b58\u5225\u8cc7\u6599\u96c6\u932f\u8aa4\u7684\u5168\u9762\u67b6\u69cb\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5efa\u7acb\u4e86 MMLU-Redux\uff0c\u5b83\u662f 30 \u500b MMLU \u4e3b\u984c\u4e2d 3,000 \u500b\u624b\u52d5\u91cd\u65b0\u8a3b\u89e3\u554f\u984c\u7684\u5b50\u96c6\u3002\u4f7f\u7528 MMLU-Redux\uff0c\u6211\u5011\u8b49\u660e\u4e86\u8207\u6700\u521d\u5831\u544a\u7684\u6a21\u578b\u6548\u80fd\u6307\u6a19\u5b58\u5728\u986f\u8457\u5dee\u7570\u3002\u6211\u5011\u7684\u7d50\u679c\u5f37\u70c8\u4e3b\u5f35\u4fee\u6539 MMLU \u932f\u8aa4\u767e\u51fa\u7684\u554f\u984c\uff0c\u4ee5\u589e\u5f37\u5176\u4f5c\u70ba\u57fa\u6e96\u7684\u672a\u4f86\u6548\u7528\u548c\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u6211\u5011\u958b\u653e MMLU-Redux \u4ee5\u4f9b\u9032\u4e00\u6b65\u8a3b\u89e3 https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux\u3002", "author": "Aryo Pradipta Gema et.al.", "authors": "Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini", "id": "2406.04127v1", "paper_url": "http://arxiv.org/abs/2406.04127v1", "repo": "null"}}