{"2502.03304": {"publish_time": "2025-02-05", "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning", "paper_summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6807\u51c6\n\u4e00\u9636 (FO) \u5fae\u8c03\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\uff0c\u6781\u5927\u5730\n\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6700\u8fd1\uff0c\u96f6\u9636 (ZO) \u4f18\u5316\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u7701\u5185\u5b58\u8bad\u7ec3\u8303\u4f8b\u8131\u9896\u800c\u51fa\uff0c\u5b83\u907f\u514d\u4e86\u53cd\u5411\u4f20\u9012\n\u5e76\u4e14\u4ec5\u4f9d\u9760\u524d\u5411\u4f20\u9012\u8fdb\u884c\u68af\u5ea6\u4f30\u8ba1\uff0c\u4f7f\u5176\n\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002\u7136\u800c\uff0cZO \u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u8fdc\u8fdc\u843d\u540e\u4e8e FO \u65b9\u6cd5\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\n\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9010\u5c42\u5dee\u5f02\u5206\u6790\uff0c\u63ed\u793a\u4e86 FO \u548c ZO \u4f18\u5316\u4e0d\u540c\u7684\u66f4\u65b0\u6a21\u5f0f\u3002\u4e3a\u4e86\u4ece\u7814\u7a76\u7ed3\u679c\u4e2d\u4f53\u73b0 FO \u65b9\u6cd5\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) \u4f18\u5316\u3002DiZO \u901a\u8fc7\u5c06\u6295\u5f71\u5408\u5e76\u5230 ZO \u66f4\u65b0\u4e2d\u6765\u8fdb\u884c\u5dee\u5f02\u9a71\u52a8\u7684\u5c42\u9002\u5e94\uff0c\n\u751f\u6210\u4e0e\u9010\u5c42\u4e2a\u4f53\u4f18\u5316\u9700\u6c42\u7cbe\u786e\u5339\u914d\u7684\u4e0d\u540c\u5e45\u5ea6\u66f4\u65b0\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cDiZO \u663e\u7740\u51cf\u5c11\u4e86\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u541e\u5410\u91cf\uff0c\u5c06\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3 GPU \u5c0f\u65f6\u6570\u51cf\u5c11\u4e86 48%\u3002\u6b64\u5916\uff0cDiZO\n\u5728\u5bf9 RoBERTa-large\u3001OPT \u7cfb\u5217\u548c Llama \u7cfb\u5217\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6709\u4ee3\u8868\u6027\u7684 ZO \u57fa\u7ebf\n\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u9700\u8981\u5927\u91cf\u5185\u5b58\u7684 FO \u5fae\u8c03\u3002", "author": "Qitao Tan et.al.", "authors": "Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Jin Lu, Geng Yuan", "id": "2502.03304v1", "paper_url": "http://arxiv.org/abs/2502.03304v1", "repo": "null"}}