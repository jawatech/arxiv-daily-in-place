{"2408.15901": {"publish_time": "2024-08-28", "title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts", "paper_summary": "Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.", "paper_summary_zh": "\u6548\u7387\u3001\u5c08\u7cbe\u5ea6\u4ee5\u53ca\u9069\u61c9\u65b0\u8cc7\u6599\u5206\u4f48\u662f\u7576\u524d\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u96e3\u4ee5\u7d50\u5408\u7684\u54c1\u8cea\u3002\u5c08\u5bb6\u6df7\u5408 (MoE) \u67b6\u69cb\u4e00\u76f4\u662f\u91cd\u8981\u7684\u7814\u7a76\u91cd\u9ede\uff0c\u56e0\u70ba\u5176\u5167\u5728\u7684\u689d\u4ef6\u5f0f\u904b\u7b97\u53ef\u5be6\u73fe\u9019\u4e9b\u7406\u60f3\u7684\u7279\u6027\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u5c07\u5bc6\u96c6\u7684\u5c08\u5bb6\u6a21\u578b\u300c\u5347\u7d1a\u300d\u81f3 MoE\uff0c\u76ee\u6a19\u662f\u6539\u5584\u5c08\u7cbe\u5ea6\u7684\u540c\u6642\uff0c\u4e5f\u80fd\u8f15\u9b06\u5730\u52a0\u5165\u9069\u61c9\u65b0\u4efb\u52d9\u7684\u80fd\u529b\u3002\u6211\u5011\u5f15\u5165\u4e86 Nexus\uff0c\u4e00\u7a2e\u5177\u5099\u81ea\u9069\u61c9\u8def\u7531\u7684\u589e\u5f37\u5f0f MoE \u67b6\u69cb\uff0c\u5176\u4e2d\u6a21\u578b\u5b78\u7fd2\u5f9e\u7db2\u57df\u8868\u793a\u4e2d\u6295\u5c04\u5c08\u5bb6\u5d4c\u5165\u3002\u9019\u7a2e\u65b9\u6cd5\u8b93 Nexus \u80fd\u5920\u5728\u6700\u521d\u7684\u5347\u7d1a\u5f8c\uff0c\u900f\u904e\u500b\u5225\u8a13\u7df4\u7684\u5bc6\u96c6\u6a21\u578b\u9748\u6d3b\u5730\u52a0\u5165\u65b0\u7684\u5c08\u5bb6\uff0c\u800c\u7121\u9700\u91dd\u5c0d\u672a\u898b\u7684\u8cc7\u6599\u7db2\u57df\u9032\u884c\u5927\u898f\u6a21\u7684 MoE \u8a13\u7df4\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0cNexus \u5728\u6700\u521d\u7684\u5347\u7d1a\u4e2d\u53d6\u5f97\u4e86\u6bd4\u57fa\u6e96\u9ad8\u9054 2.1% \u7684\u76f8\u5c0d\u589e\u76ca\uff0c\u4ee5\u53ca\u900f\u904e\u4f7f\u7528\u6709\u9650\u7684\u5fae\u8abf\u8cc7\u6599\u4f86\u64f4\u5145 MoE \u7684\u65b0\u5c08\u5bb6\u6642\uff0c\u53d6\u5f97\u4e86 18.8% \u7684\u76f8\u5c0d\u589e\u76ca\u3002Nexus \u7684\u9019\u7a2e\u9748\u6d3b\u6027\u5c0d\u65bc\u5efa\u7acb\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc\u751f\u614b\u7cfb\u7d71\u81f3\u95dc\u91cd\u8981\uff0c\u8b93\u6bcf\u500b\u4f7f\u7528\u8005\u90fd\u80fd\u6839\u64da\u81ea\u5df1\u7684\u9700\u6c42\u6301\u7e8c\u7d44\u88dd\u81ea\u5df1\u7684 MoE \u7d44\u5408\u3002", "author": "Nikolas Gritsch et.al.", "authors": "Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet \u00dcst\u00fcn", "id": "2408.15901v1", "paper_url": "http://arxiv.org/abs/2408.15901v1", "repo": "null"}}