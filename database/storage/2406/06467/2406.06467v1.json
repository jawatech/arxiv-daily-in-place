{"2406.06467": {"publish_time": "2024-06-10", "title": "How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad", "paper_summary": "Can Transformers predict new syllogisms by composing established ones? More\ngenerally, what type of targets can be learned by such models from scratch?\nRecent works show that Transformers can be Turing-complete in terms of\nexpressivity, but this does not address the learnability objective. This paper\nputs forward the notion of 'distribution locality' to capture when weak\nlearning is efficiently achievable by regular Transformers, where the locality\nmeasures the least number of tokens required in addition to the tokens\nhistogram to correlate nontrivially with the target. As shown experimentally\nand theoretically under additional assumptions, distributions with high\nlocality cannot be learned efficiently. In particular, syllogisms cannot be\ncomposed on long chains. Furthermore, we show that (i) an agnostic scratchpad\ncannot help to break the locality barrier, (ii) an educated scratchpad can help\nif it breaks the locality at each step, (iii) a notion of 'inductive\nscratchpad' can both break the locality and improve the out-of-distribution\ngeneralization, e.g., generalizing to almost double input size for some\narithmetic tasks.", "paper_summary_zh": "Transformer \u80fd\u5426\u900f\u904e\u7d44\u5408\u5df2\u5efa\u7acb\u7684\u4e09\u6bb5\u8ad6\u6cd5\u4f86\u9810\u6e2c\u65b0\u7684\u4e09\u6bb5\u8ad6\u6cd5\uff1f\u66f4\u666e\u904d\u5730\u8aaa\uff0c\u6b64\u985e\u6a21\u578b\u53ef\u4ee5\u5f9e\u982d\u5b78\u7fd2\u54ea\u7a2e\u985e\u578b\u7684\u76ee\u6a19\uff1f\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0cTransformer \u5728\u8868\u9054\u80fd\u529b\u65b9\u9762\u53ef\u4ee5\u662f\u5716\u9748\u5b8c\u5099\u7684\uff0c\u4f46\u9019\u4e26\u672a\u89e3\u6c7a\u53ef\u5b78\u7fd2\u6027\u76ee\u6a19\u3002\u672c\u6587\u63d0\u51fa\u4e86\u300c\u5206\u4f48\u5c40\u90e8\u6027\u300d\u7684\u6982\u5ff5\uff0c\u4ee5\u6355\u6349\u5728\u6b63\u898f Transformer \u4e2d\u4f55\u6642\u53ef\u4ee5\u6709\u6548\u9054\u6210\u5f31\u5b78\u7fd2\uff0c\u5176\u4e2d\u5c40\u90e8\u6027\u8861\u91cf\u9664\u4e86\u76f4\u65b9\u5716\u4e4b\u5916\uff0c\u8207\u76ee\u6a19\u975e\u5e73\u51e1\u5730\u76f8\u95dc\u806f\u6240\u9700\u7684\u6700\u5c0f\u4ee4\u724c\u6578\u3002\u5982\u5728\u984d\u5916\u7684\u5047\u8a2d\u4e0b\u900f\u904e\u5be6\u9a57\u548c\u7406\u8ad6\u6240\u793a\uff0c\u5177\u6709\u9ad8\u5c40\u90e8\u6027\u7684\u5206\u4f48\u7121\u6cd5\u6709\u6548\u5b78\u7fd2\u3002\u7279\u5225\u662f\uff0c\u4e09\u6bb5\u8ad6\u6cd5\u7121\u6cd5\u5728\u9577\u93c8\u4e0a\u7d44\u6210\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e (i) \u4e0d\u53ef\u77e5\u5099\u5fd8\u9304\u7121\u6cd5\u5354\u52a9\u7a81\u7834\u5c40\u90e8\u6027\u969c\u7919\uff0c(ii) \u53d7\u904e\u6559\u80b2\u7684\u5099\u5fd8\u9304\u5982\u679c\u5728\u6bcf\u500b\u6b65\u9a5f\u4e2d\u6253\u7834\u5c40\u90e8\u6027\uff0c\u5c31\u80fd\u63d0\u4f9b\u5354\u52a9\uff0c(iii) \u300c\u6b78\u7d0d\u5099\u5fd8\u9304\u300d\u7684\u6982\u5ff5\u65e2\u80fd\u6253\u7834\u5c40\u90e8\u6027\uff0c\u53c8\u80fd\u6539\u5584\u5206\u4f48\u5916\u6982\u5316\uff0c\u4f8b\u5982\uff0c\u5c0d\u65bc\u67d0\u4e9b\u7b97\u8853\u4efb\u52d9\uff0c\u6982\u5316\u5230\u5e7e\u4e4e\u662f\u8f38\u5165\u5927\u5c0f\u7684\u5169\u500d\u3002", "author": "Emmanuel Abbe et.al.", "authors": "Emmanuel Abbe, Samy Bengio, Aryo Lotfi, Colin Sandon, Omid Saremi", "id": "2406.06467v1", "paper_url": "http://arxiv.org/abs/2406.06467v1", "repo": "https://github.com/aryol/inductive-scratchpad"}}