{"2410.23528": {"publish_time": "2024-10-31", "title": "Large Language Models for Patient Comments Multi-Label Classification", "paper_summary": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4o-Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.", "paper_summary_zh": "\u75c5\u60a3\u9ad4\u9a57\u548c\u7167\u8b77\u54c1\u8cea\u5c0d\u65bc\u91ab\u9662\u7684\u6c38\u7e8c\u7d93\u71df\u548c\u8072\u8b7d\u81f3\u95dc\u91cd\u8981\u3002\u5206\u6790\u75c5\u60a3\u56de\u994b\u610f\u898b\u80fd\u63d0\u4f9b\u5bf6\u8cb4\u7684\u898b\u89e3\uff0c\u4e86\u89e3\u75c5\u60a3\u6eff\u610f\u5ea6\u548c\u6cbb\u7642\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u8a55\u8ad6\u7684\u975e\u7d50\u69cb\u5316\u7279\u6027\u5c0d\u9075\u5faa\u76e3\u7763\u5f0f\u5b78\u7fd2\u5178\u7bc4\u7684\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\u65b9\u6cd5\u69cb\u6210\u6311\u6230\u3002\u9019\u662f\u56e0\u70ba\u7f3a\u4e4f\u6a19\u7c64\u8cc7\u6599\uff0c\u800c\u4e14\u9019\u4e9b\u6587\u5b57\u5305\u542b\u8a31\u591a\u7d30\u5fae\u5dee\u5225\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u4f4f\u9662\u75c5\u60a3\u5728\u51fa\u9662\u5f8c\u5206\u4eab\u7684\u8a55\u8ad6\u7684\u591a\u6a19\u7c64\u6587\u5b57\u5206\u985e (MLTC)\u3002\u5229\u7528 GPT-4o-Turbo \u9032\u884c\u5206\u985e\u3002\u7136\u800c\uff0c\u9451\u65bc\u75c5\u60a3\u8a55\u8ad6\u7684\u654f\u611f\u6027\u8cea\uff0c\u5728\u900f\u904e\u53d7\u4fdd\u8b77\u5065\u5eb7\u8cc7\u8a0a (PHI) \u5075\u6e2c\u67b6\u69cb\u5c07\u8cc7\u6599\u63d0\u4f9b\u7d66 LLM \u4e4b\u524d\uff0c\u6703\u52a0\u5165\u4e00\u5c64\u5b89\u5168\u9632\u8b77\uff0c\u4ee5\u78ba\u4fdd\u75c5\u60a3\u7684\u53bb\u8b58\u5225\u5316\u3002\u6b64\u5916\uff0c\u9084\u5be6\u9a57\u4e86\u63d0\u793a\u5de5\u7a0b\u67b6\u69cb\u3001\u96f6\u6b21\u5b78\u7fd2\u3001\u60c5\u5883\u4e2d\u5b78\u7fd2\u548c\u601d\u8003\u93c8\u63d0\u793a\u3002\u7d50\u679c\u986f\u793a\uff0c\u7121\u8ad6\u662f\u9075\u5faa\u96f6\u6b21\u6216\u5c11\u6b21\u8a2d\u5b9a\uff0cGPT-4o-Turbo \u90fd\u512a\u65bc\u50b3\u7d71\u65b9\u6cd5\u548c\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM)\uff0c\u4e26\u4ee5 76.12% \u7684 F1 \u5206\u6578\u548c 73.61% \u7684\u52a0\u6b0a F1 \u5206\u6578\u9054\u5230\u6700\u9ad8\u7684\u6574\u9ad4\u6548\u80fd\uff0c\u7dca\u63a5\u5728\u5f8c\u7684\u5247\u662f\u5c11\u6b21\u5b78\u7fd2\u7d50\u679c\u3002\u96a8\u5f8c\uff0c\u9032\u884c\u4e86\u7d50\u679c\u8207\u5176\u4ed6\u75c5\u60a3\u9ad4\u9a57\u7d50\u69cb\u5316\u8b8a\u6578\uff08\u4f8b\u5982\u8a55\u5206\uff09\u7684\u95dc\u806f\u6027\u5206\u6790\u3002\u672c\u7814\u7a76\u900f\u904e\u61c9\u7528 LLM \u4f86\u5f37\u5316 MLTC\uff0c\u70ba\u91ab\u7642\u5f9e\u696d\u4eba\u54e1\u63d0\u4f9b\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u6df1\u5165\u4e86\u89e3\u75c5\u60a3\u56de\u994b\u610f\u898b\u4e26\u63d0\u4f9b\u8fc5\u901f\u4e14\u9069\u7576\u7684\u56de\u61c9\u3002", "author": "Hajar Sakai et.al.", "authors": "Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin", "id": "2410.23528v1", "paper_url": "http://arxiv.org/abs/2410.23528v1", "repo": "null"}}