{"2407.21670": {"publish_time": "2024-07-31", "title": "Universal Approximation Theory: Foundations for Parallelism in Neural Networks", "paper_summary": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.", "paper_summary_zh": "\u795e\u7d93\u7db2\u8def\u6b63\u671d\u8a13\u7df4\u5927\u578b\u6a21\u578b\u8207\u5de8\u91cf\u8cc7\u6599\u7684\u65b9\u5411\u767c\u5c55\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u8a31\u591a\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u512a\u7570\u6548\u80fd\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u5e36\u4f86\u4e86\u4e00\u500b\u8feb\u5207\u7684\u554f\u984c\uff1a\u76ee\u524d\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u4e3b\u8981\u662f\u4e32\u5217\u7684\uff0c\u9019\u8868\u793a\u7db2\u8def\u5c64\u6578\u8d8a\u591a\uff0c\u8a13\u7df4\u548c\u63a8\u8ad6\u6642\u9593\u4e5f\u6703\u8d8a\u9577\u3002\u5982\u679c\u6df1\u5ea6\u5b78\u7fd2\u8981\u7e7c\u7e8c\u9032\u6b65\uff0c\u9019\u662f\u7121\u6cd5\u63a5\u53d7\u7684\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u6cdb\u51fd\u903c\u8fd1\u5b9a\u7406 (UAT) \u7684\u6df1\u5ea6\u5b78\u7fd2\u4e26\u884c\u5316\u7b56\u7565\u3002\u57fa\u65bc\u6b64\u57fa\u790e\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u7a31\u70ba Para-Former \u7684\u5e73\u884c\u7db2\u8def\u4f86\u6e2c\u8a66\u6211\u5011\u7684\u7406\u8ad6\u3002\u8207\u50b3\u7d71\u4e32\u5217\u6a21\u578b\u4e0d\u540c\uff0cPara-Former \u7684\u63a8\u8ad6\u6642\u9593\u4e0d\u6703\u96a8\u8457\u5c64\u6578\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5927\u5e45\u52a0\u901f\u591a\u5c64\u7db2\u8def\u7684\u63a8\u8ad6\u901f\u5ea6\u3002\u5be6\u9a57\u7d50\u679c\u9a57\u8b49\u4e86\u6b64\u7db2\u8def\u7684\u6709\u6548\u6027\u3002", "author": "Wei Wang et.al.", "authors": "Wei Wang, Qing Li", "id": "2407.21670v1", "paper_url": "http://arxiv.org/abs/2407.21670v1", "repo": "null"}}