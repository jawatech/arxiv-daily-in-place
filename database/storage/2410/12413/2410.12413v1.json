{"2410.12413": {"publish_time": "2024-10-16", "title": "Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding", "paper_summary": "In this study, we provide constructive proof that Transformers can recognize\nand generate hierarchical language efficiently with respect to model size, even\nwithout the need for a specific positional encoding. Specifically, we show that\ncausal masking and a starting token enable Transformers to compute positional\ninformation and depth within hierarchical structures. We demonstrate that\nTransformers without positional encoding can generate hierarchical languages.\nFurthermore, we suggest that explicit positional encoding might have a\ndetrimental effect on generalization with respect to sequence length.", "paper_summary_zh": "\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5efa\u69cb\u6027\u7684\u8b49\u660e\uff0c\u8b49\u660e Transformer \u53ef\u4ee5\u6709\u6548\u5730\u8b58\u5225\u548c\u7522\u751f\u968e\u5c64\u8a9e\u8a00\uff0c\u5373\u4f7f\u4e0d\u9700\u8981\u7279\u5b9a\u4f4d\u7f6e\u7de8\u78bc\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c55\u793a\u4e86\u56e0\u679c\u906e\u7f69\u548c\u8d77\u59cb\u6a19\u8a18\u4f7f Transformer \u80fd\u5920\u8a08\u7b97\u968e\u5c64\u7d50\u69cb\u4e2d\u7684\u4f4d\u7f6e\u8cc7\u8a0a\u548c\u6df1\u5ea6\u3002\u6211\u5011\u8b49\u660e\u4e86\u6c92\u6709\u4f4d\u7f6e\u7de8\u78bc\u7684 Transformer \u53ef\u4ee5\u7522\u751f\u968e\u5c64\u8a9e\u8a00\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u8b70\u660e\u78ba\u7684\u4f4d\u7f6e\u7de8\u78bc\u53ef\u80fd\u6703\u5c0d\u5e8f\u5217\u9577\u5ea6\u7684\u4e00\u822c\u5316\u7522\u751f\u4e0d\u5229\u5f71\u97ff\u3002", "author": "Daichi Hayakawa et.al.", "authors": "Daichi Hayakawa, Issei Sato", "id": "2410.12413v1", "paper_url": "http://arxiv.org/abs/2410.12413v1", "repo": "null"}}