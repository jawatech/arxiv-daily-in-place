{"2406.15111": {"publish_time": "2024-06-21", "title": "Investigating the impact of 2D gesture representation on co-speech gesture generation", "paper_summary": "Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.", "paper_summary_zh": "\u5171\u8a9e\u624b\u52e2\u5728\u4eba\u985e\u548c\u5177\u8eab\u5c0d\u8a71\u4ee3\u7406 (ECA) \u4e4b\u9593\u7684\u4e92\u52d5\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u6700\u8fd1\u7684\u6df1\u5ea6\u5b78\u7fd2\u65b9\u6cd5\u80fd\u5920\u7522\u751f\u8207\u8a9e\u97f3\u540c\u6b65\u7684\u903c\u771f\u3001\u81ea\u7136\u7684\u5171\u8a9e\u624b\u52e2\uff0c\u4f46\u6b64\u985e\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u6578\u64da\u3002\u900f\u904e\u4eba\u985e\u59ff\u52e2\u5075\u6e2c\u6a21\u578b\u5f9e YouTube \u7b49\u4f86\u6e90\u5f59\u7de8\u5f71\u7247\u7684\u300c\u91ce\u5916\u300d\u8cc7\u6599\u96c6\u63d0\u4f9b\u4e86\u4e00\u500b\u89e3\u6c7a\u65b9\u6848\uff0c\u65b9\u6cd5\u662f\u63d0\u4f9b\u8207\u8a9e\u97f3\u914d\u5c0d\u7684 2D \u9aa8\u67b6\u5e8f\u5217\u3002\u540c\u6642\uff0c\u5275\u65b0\u7684\u63d0\u5347\u6a21\u578b\u61c9\u904b\u800c\u751f\uff0c\u80fd\u5920\u5c07\u9019\u4e9b 2D \u59ff\u52e2\u5e8f\u5217\u8f49\u63db\u70ba\u5176 3D \u5c0d\u61c9\u7269\uff0c\u5f9e\u800c\u7522\u751f\u9f90\u5927\u4e14\u591a\u6a23\u5316\u7684 3D \u624b\u52e2\u8cc7\u6599\u96c6\u3002\u7136\u800c\uff0c\u884d\u751f\u7684 3D \u59ff\u52e2\u4f30\u8a08\u672c\u8cea\u4e0a\u662f\u4e00\u500b\u507d\u57fa\u790e\u771f\u5be6\u503c\uff0c\u800c\u5be6\u969b\u7684\u57fa\u790e\u771f\u5be6\u503c\u662f 2D \u904b\u52d5\u6578\u64da\u3002\u9019\u7a2e\u5340\u5225\u5f15\u767c\u4e86\u95dc\u65bc\u624b\u52e2\u8868\u793a\u7dad\u5ea6\u5c0d\u751f\u6210\u52d5\u4f5c\u54c1\u8cea\u7684\u5f71\u97ff\u554f\u984c\uff0c\u9019\u500b\u4e3b\u984c\u64da\u6211\u5011\u6240\u77e5\u4ecd\u672a\u5f97\u5230\u5ee3\u6cdb\u63a2\u8a0e\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u8a13\u7df4\u6578\u64da\u7684\u7dad\u5ea6\uff082D \u6216 3D \u95dc\u7bc0\u5ea7\u6a19\uff09\u5c0d\u591a\u6a21\u614b\u8a9e\u97f3\u5230\u624b\u52e2\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6211\u5011\u4f7f\u7528\u63d0\u5347\u6a21\u578b\u5c07 2D \u751f\u6210\u7684\u8eab\u9ad4\u59ff\u52e2\u5e8f\u5217\u8f49\u63db\u70ba 3D\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u76f4\u63a5\u5728 3D \u4e2d\u751f\u6210\u7684\u52d5\u4f5c\u5e8f\u5217\u8207\u5728 2D \u4e2d\u751f\u6210\u4e26\u63d0\u5347\u5230 3D \u4f5c\u70ba\u5f8c\u8655\u7406\u7684\u52d5\u4f5c\u9032\u884c\u6bd4\u8f03\u3002", "author": "Teo Guichoux et.al.", "authors": "Teo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud", "id": "2406.15111v1", "paper_url": "http://arxiv.org/abs/2406.15111v1", "repo": "null"}}