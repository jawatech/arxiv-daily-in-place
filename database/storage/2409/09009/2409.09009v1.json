{"2409.09009": {"publish_time": "2024-09-13", "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "paper_summary": "Direct speech translation (ST) models often struggle with rare words.\nIncorrect translation of these words can have severe consequences, impacting\ntranslation quality and user trust. While rare word translation is inherently\nchallenging for neural models due to sparse learning signals, real-world\nscenarios often allow access to translations of past recordings on similar\ntopics. To leverage these valuable resources, we propose a\nretrieval-and-demonstration approach to enhance rare word translation accuracy\nin direct ST models. First, we adapt existing ST models to incorporate\nretrieved examples for rare word translation, which allows the model to benefit\nfrom prepended examples, similar to in-context learning. We then develop a\ncross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to\nlocate suitable examples. We demonstrate that standard ST models can be\neffectively adapted to leverage examples for rare word translation, improving\nrare word translation accuracy over the baseline by 17.6% with gold examples\nand 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval\napproach outperforms other modalities and exhibits higher robustness to unseen\nspeakers. Our code is publicly available\n(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).", "paper_summary_zh": "\u76f4\u8bd1 (ST) \u6a21\u578b\u901a\u5e38\u96be\u4ee5\u5904\u7406\u7f55\u89c1\u5b57\u8bcd\u3002\n\u8fd9\u4e9b\u5b57\u8bcd\u7ffb\u8bd1\u4e0d\u6b63\u786e\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u5f71\u54cd\u7ffb\u8bd1\u54c1\u8d28\u548c\u4f7f\u7528\u8005\u4fe1\u4efb\u3002\u867d\u7136\u7f55\u89c1\u5b57\u8bcd\u7ffb\u8bd1\u5bf9\u795e\u7ecf\u6a21\u578b\u6765\u8bf4\u672c\u6765\u5c31\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b66\u4e60\u4fe1\u53f7\u7a00\u758f\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u60c5\u5883\u901a\u5e38\u53ef\u4ee5\u5b58\u53d6\u7c7b\u4f3c\u4e3b\u9898\u8fc7\u53bb\u5f55\u97f3\u7684\u7ffb\u8bd1\u3002\u4e3a\u4e86\u5584\u7528\u8fd9\u4e9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u64b7\u53d6\u4e0e\u793a\u8303\u65b9\u6cd5\u6765\u63d0\u5347\u76f4\u63a5 ST \u6a21\u578b\u4e2d\u7f55\u89c1\u5b57\u8bcd\u7ffb\u8bd1\u7684\u51c6\u786e\u5ea6\u3002\u9996\u5148\uff0c\u6211\u4eec\u8c03\u6574\u73b0\u6709\u7684 ST \u6a21\u578b\uff0c\u7eb3\u5165\u7f55\u89c1\u5b57\u8bcd\u7ffb\u8bd1\u7684\u64b7\u53d6\u8303\u4f8b\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u53d7\u76ca\u4e8e\u524d\u7f6e\u8303\u4f8b\uff0c\u7c7b\u4f3c\u4e8e\u8bed\u5883\u5b66\u4e60\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e00\u4e2a\u8de8\u6a21\u6001\uff08\u8bed\u97f3\u8f6c\u8bed\u97f3\u3001\u8bed\u97f3\u8f6c\u6587\u5b57\u3001\u6587\u5b57\u8f6c\u6587\u5b57\uff09\u64b7\u53d6\u5668\u6765\u627e\u51fa\u5408\u9002\u7684\u8303\u4f8b\u3002\u6211\u4eec\u793a\u8303\u6807\u51c6 ST \u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u8c03\u6574\u6765\u5584\u7528\u7f55\u89c1\u5b57\u8bcd\u7ffb\u8bd1\u7684\u8303\u4f8b\uff0c\u4ee5\u9ec4\u91d1\u8303\u4f8b\u63d0\u5347\u7f55\u89c1\u5b57\u8bcd\u7ffb\u8bd1\u51c6\u786e\u5ea6 17.6%\uff0c\u4ee5\u64b7\u53d6\u8303\u4f8b\u63d0\u5347 8.5%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u8bed\u97f3\u8f6c\u8bed\u97f3\u64b7\u53d6\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6a21\u6001\uff0c\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u8005\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5065\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u516c\u5f00\uff08https://github.com/SiqiLii/Retrieve-and-Demonstration-ST\uff09\u3002", "author": "Siqi Li et.al.", "authors": "Siqi Li, Danni Liu, Jan Niehues", "id": "2409.09009v1", "paper_url": "http://arxiv.org/abs/2409.09009v1", "repo": "https://github.com/siqilii/retrieve-and-demonstration-st"}}