{"2409.02389": {"publish_time": "2024-09-04", "title": "Multi-modal Situated Reasoning in 3D Scenes", "paper_summary": "Situation awareness is essential for understanding and reasoning about 3D\nscenes in embodied AI agents. However, existing datasets and benchmarks for\nsituated understanding are limited in data modality, diversity, scale, and task\nscope. To address these limitations, we propose Multi-modal Situated Question\nAnswering (MSQA), a large-scale multi-modal situated reasoning dataset,\nscalably collected leveraging 3D scene graphs and vision-language models (VLMs)\nacross a diverse range of real-world 3D scenes. MSQA includes 251K situated\nquestion-answering pairs across 9 distinct question categories, covering\ncomplex scenarios within 3D scenes. We introduce a novel interleaved\nmulti-modal input setting in our benchmark to provide text, image, and point\ncloud for situation and question description, resolving ambiguity in previous\nsingle-modality convention (e.g., text). Additionally, we devise the\nMulti-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'\nsituated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN\nhighlight the limitations of existing vision-language models and underscore the\nimportance of handling multi-modal interleaved inputs and situation modeling.\nExperiments on data scaling and cross-domain transfer further demonstrate the\nefficacy of leveraging MSQA as a pre-training dataset for developing more\npowerful situated reasoning models.", "paper_summary_zh": "\u60c5\u5883\u611f\u77e5\u5c0d\u65bc\u7406\u89e3\u548c\u63a8\u7406\u5177\u8eab AI \u4ee3\u7406\u4e2d\u7684 3D \u5834\u666f\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u8cc7\u6599\u96c6\u548c\u57fa\u6e96\u5728\u8cc7\u6599\u6a21\u614b\u3001\u591a\u6a23\u6027\u3001\u898f\u6a21\u548c\u4efb\u52d9\u7bc4\u570d\u65b9\u9762\u5c0d\u65bc\u60c5\u5883\u7406\u89e3\u4f86\u8aaa\u662f\u6709\u9650\u7684\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u591a\u6a21\u614b\u60c5\u5883\u554f\u7b54 (MSQA)\uff0c\u9019\u662f\u4e00\u500b\u5927\u578b\u591a\u6a21\u614b\u60c5\u5883\u63a8\u7406\u8cc7\u6599\u96c6\uff0c\u53ef\u900f\u904e\u5229\u7528 3D \u5834\u666f\u5716\u548c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c 3D \u5834\u666f\u4e2d\u9032\u884c\u53ef\u64f4\u5145\u6536\u96c6\u3002MSQA \u5305\u542b 251K \u500b\u60c5\u5883\u554f\u7b54\u5c0d\uff0c\u6db5\u84cb 9 \u500b\u4e0d\u540c\u7684\u554f\u984c\u985e\u5225\uff0c\u6db5\u84cb 3D \u5834\u666f\u4e2d\u7684\u8907\u96dc\u5834\u666f\u3002\u6211\u5011\u5728\u57fa\u6e96\u4e2d\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u4ea4\u932f\u591a\u6a21\u614b\u8f38\u5165\u8a2d\u5b9a\uff0c\u4ee5\u63d0\u4f9b\u6587\u5b57\u3001\u5f71\u50cf\u548c\u9ede\u96f2\uff0c\u7528\u65bc\u60c5\u5883\u548c\u554f\u984c\u63cf\u8ff0\uff0c\u89e3\u6c7a\u4ee5\u524d\u55ae\u4e00\u6a21\u614b\u6163\u4f8b\uff08\u4f8b\u5982\u6587\u5b57\uff09\u4e2d\u7684\u6b67\u7fa9\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u591a\u6a21\u614b\u60c5\u5883\u4e0b\u4e00\u6b65\u5c0e\u822a (MSNN) \u57fa\u6e96\uff0c\u4ee5\u8a55\u4f30\u6a21\u578b\u7684\u5c0e\u822a\u60c5\u5883\u63a8\u7406\u3002MSQA \u548c MSNN \u7684\u7d9c\u5408\u8a55\u4f30\u7a81\u986f\u4e86\u73fe\u6709\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u9650\u5236\uff0c\u4e26\u5f37\u8abf\u4e86\u8655\u7406\u591a\u6a21\u614b\u4ea4\u932f\u8f38\u5165\u548c\u60c5\u5883\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002\u8cc7\u6599\u64f4\u5145\u548c\u8de8\u9818\u57df\u8f49\u79fb\u7684\u5be6\u9a57\u9032\u4e00\u6b65\u8b49\u660e\u4e86\u5229\u7528 MSQA \u4f5c\u70ba\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u4f86\u958b\u767c\u66f4\u5f37\u5927\u7684\u60c5\u5883\u63a8\u7406\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "author": "Xiongkun Linghu et.al.", "authors": "Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang", "id": "2409.02389v1", "paper_url": "http://arxiv.org/abs/2409.02389v1", "repo": "null"}}