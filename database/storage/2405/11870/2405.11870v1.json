{"2405.11870": {"publish_time": "2024-05-20", "title": "Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process", "paper_summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback\n(RLHF) are two fundamental processes for enhancing the capabilities of Language\nModels (LMs) post pre-training, aligning them better with human preferences.\nAlthough SFT advances in training efficiency, RLHF delivers better alignment,\nthus they are often combined. However, common practices simply apply them\nsequentially without unifying their optimization targets, resulting in a\ntrade-off between fitting different objectives, and ignoring the opportunities\nto bridge the paradigm gap and take the strength from both. To obtain a unified\nunderstanding, we interpret SFT and RLHF using two sub-processes -- Preference\nEstimation and Transition Optimization -- defined at token level within the\nMarkov Decision Process (MDP) framework. This modeling shows that SFT is only a\nspecialized case of RLHF with inferior estimation and optimization. RLHF\nevaluates the quality of model's entire generated answer, whereas SFT only\nscores predicted tokens based on preceding tokens from target answers.\nTherefore, SFT overestimates the ability of model, leading to inferior\noptimization. Building on this view, we introduce Intuitive Fine-tuning (IFT)\nto integrate SFT and RLHF into a single process. IFT captures LMs' intuitive\nsense of the entire answers through a temporal residual connection, while using\na single policy and the same volume of non-preference-labeled data as SFT. Our\nexperiments show that IFT performs comparably or even superiorly to sequential\nrecipes of SFT and some typical alignment methods across several tasks,\nparticularly those requires generation, reasoning, and fact-following\nabilities. An explainable Frozen Lake game further validates the effectiveness\nof IFT.", "paper_summary_zh": "\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u548c\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u5169\u7a2e\u7528\u65bc\u589e\u5f37\u8a9e\u8a00\u6a21\u578b (LM) \u80fd\u529b\u7684\u57fa\u672c\u6d41\u7a0b\uff0c\u5728\u9810\u8a13\u7df4\u5f8c\u8b93\u5b83\u5011\u66f4\u7b26\u5408\u4eba\u985e\u504f\u597d\u3002\u5118\u7ba1 SFT \u5728\u8a13\u7df4\u6548\u7387\u65b9\u9762\u53d6\u5f97\u9032\u5c55\uff0c\u4f46 RLHF \u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6bd4\u5c0d\uff0c\u56e0\u6b64\u5b83\u5011\u7d93\u5e38\u7d50\u5408\u4f7f\u7528\u3002\u7136\u800c\uff0c\u5e38\u898b\u505a\u6cd5\u53ea\u662f\u5c07\u5b83\u5011\u6309\u9806\u5e8f\u61c9\u7528\uff0c\u800c\u6c92\u6709\u7d71\u4e00\u5176\u6700\u4f73\u5316\u76ee\u6a19\uff0c\u5c0e\u81f4\u5728\u7b26\u5408\u4e0d\u540c\u76ee\u6a19\u4e4b\u9593\u9032\u884c\u6b0a\u8861\uff0c\u4e26\u5ffd\u7565\u4e86\u5f4c\u5408\u5178\u7bc4\u5dee\u8ddd\u548c\u5229\u7528\u5169\u8005\u512a\u52e2\u7684\u6a5f\u6703\u3002\u70ba\u4e86\u7372\u5f97\u7d71\u4e00\u7684\u7406\u89e3\uff0c\u6211\u5011\u4f7f\u7528\u5169\u500b\u5b50\u6d41\u7a0b\u4f86\u8a6e\u91cb SFT \u548c RLHF\u2014\u2014\u504f\u597d\u4f30\u8a08\u548c\u8f49\u63db\u6700\u4f73\u5316\u2014\u2014\u9019\u4e9b\u5b50\u6d41\u7a0b\u5728\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b (MDP) \u6846\u67b6\u4e2d\u5728\u4ee4\u724c\u5c64\u7d1a\u5b9a\u7fa9\u3002\u9019\u7a2e\u5efa\u6a21\u986f\u793a\uff0cSFT \u53ea\u662f RLHF \u7684\u4e00\u7a2e\u7279\u6b8a\u60c5\u6cc1\uff0c\u5177\u6709\u8f03\u5dee\u7684\u4f30\u8a08\u548c\u6700\u4f73\u5316\u3002RLHF \u8a55\u4f30\u6a21\u578b\u6574\u500b\u751f\u6210\u7b54\u6848\u7684\u54c1\u8cea\uff0c\u800c SFT \u50c5\u6839\u64da\u76ee\u6a19\u7b54\u6848\u4e2d\u7684\u524d\u7f6e\u4ee4\u724c\u5c0d\u9810\u6e2c\u4ee4\u724c\u9032\u884c\u8a55\u5206\u3002\u56e0\u6b64\uff0cSFT \u9ad8\u4f30\u4e86\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5c0e\u81f4\u6700\u4f73\u5316\u8f03\u5dee\u3002\u57fa\u65bc\u6b64\u89c0\u9ede\uff0c\u6211\u5011\u5f15\u5165\u4e86\u76f4\u89c0\u5fae\u8abf (IFT) \u4f86\u5c07 SFT \u548c RLHF \u6574\u5408\u5230\u55ae\u4e00\u6d41\u7a0b\u4e2d\u3002IFT \u901a\u904e\u66ab\u614b\u6b98\u5dee\u9023\u63a5\u6355\u6349 LM \u5c0d\u6574\u500b\u7b54\u6848\u7684\u76f4\u89c0\u611f\u89ba\uff0c\u540c\u6642\u4f7f\u7528\u55ae\u4e00\u7b56\u7565\u548c\u8207 SFT \u76f8\u540c\u9ad4\u7a4d\u7684\u975e\u504f\u597d\u6a19\u7c64\u6578\u64da\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cIFT \u5728\u591a\u9805\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8207 SFT \u548c\u4e00\u4e9b\u5178\u578b\u6bd4\u5c0d\u65b9\u6cd5\u7684\u9806\u5e8f\u914d\u65b9\u76f8\u7576\u751a\u81f3\u512a\u8d8a\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u9700\u8981\u751f\u6210\u3001\u63a8\u7406\u548c\u4e8b\u5be6\u9075\u5faa\u80fd\u529b\u7684\u4efb\u52d9\u3002\u4e00\u500b\u53ef\u89e3\u91cb\u7684 Frozen Lake \u904a\u6232\u9032\u4e00\u6b65\u9a57\u8b49\u4e86 IFT \u7684\u6709\u6548\u6027\u3002", "author": "Ermo Hua et.al.", "authors": "Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, Bowen Zhou", "id": "2405.11870v1", "paper_url": "http://arxiv.org/abs/2405.11870v1", "repo": "null"}}