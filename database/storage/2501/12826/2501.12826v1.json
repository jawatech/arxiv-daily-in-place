{"2501.12826": {"publish_time": "2025-01-22", "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek", "paper_summary": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.", "paper_summary_zh": "\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u91dd\u5c0d\u8cc7\u6e90\u8f03\u5c11\u7684\u8a9e\u8a00\u9762\u81e8\u6301\u7e8c\u7684\u6311\u6230\uff0c\u5305\u62ec\u8cc7\u6599\u96c6\u6709\u9650\u3001\u7e7c\u627f\u81ea\u8cc7\u6e90\u8c50\u5bcc\u8a9e\u8a00\u7684\u504f\u898b\uff0c\u4ee5\u53ca\u5c0d\u7279\u5b9a\u9818\u57df\u89e3\u6c7a\u65b9\u6848\u7684\u9700\u6c42\u3002\u672c\u7814\u7a76\u900f\u904e\u4e09\u500b\u95dc\u9375\u8ca2\u737b\u89e3\u6c7a\u73fe\u4ee3\u5e0c\u81d8\u8a9e\u7684\u9019\u4e9b\u5dee\u8ddd\u3002\u9996\u5148\uff0c\u6211\u5011\u8a55\u4f30\u958b\u6e90 (Llama-70b) \u548c\u9589\u6e90 (GPT-4o mini) \u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4e03\u9805\u6838\u5fc3 NLP \u4efb\u52d9\u4e0a\u7684\u6548\u80fd\uff0c\u4e26\u63d0\u4f9b\u8cc7\u6599\u96c6\u53ef\u7528\u6027\uff0c\u63ed\u9732\u5176\u4efb\u52d9\u7279\u5b9a\u7684\u512a\u52e2\u3001\u52a3\u52e2\u548c\u6548\u80fd\u5e73\u50f9\u3002\u5176\u6b21\uff0c\u6211\u5011\u900f\u904e\u5c07\u4f5c\u8005\u6b78\u56e0\u91cd\u65b0\u5b9a\u7fa9\u70ba\u4e00\u7a2e\u8a55\u4f30 LLM \u5728\u9810\u8a13\u7df4\u4e2d\u6f5b\u5728\u8cc7\u6599\u4f7f\u7528\u60c5\u6cc1\u7684\u5de5\u5177\uff0c\u64f4\u5c55\u5e0c\u81d8\u8a9e NLP \u7684\u7bc4\u570d\uff0c\u4e26\u4ee5\u9ad8 0 \u6b21\u5b78\u7fd2\u6e96\u78ba\u5ea6\u6697\u793a\u8cc7\u6599\u4f86\u6e90\u7684\u502b\u7406\u610f\u6db5\u3002\u7b2c\u4e09\uff0c\u6211\u5011\u5c55\u793a\u4e00\u500b\u6cd5\u5f8b NLP \u6848\u4f8b\u7814\u7a76\uff0c\u5176\u4e2d\u6458\u8981\u3001\u7ffb\u8b6f\u548c\u5d4c\u5165 (STE) \u65b9\u6cd5\u512a\u65bc\u50b3\u7d71 TF-IDF \u65b9\u6cd5\uff0c\u7528\u65bc\u5206\u7fa4\\emph{\u9577\u7bc7}\u6cd5\u5f8b\u6587\u672c\u3002\u9019\u4e9b\u8ca2\u737b\u5171\u540c\u63d0\u4f9b\u4e86\u4e00\u4efd\u8def\u7dda\u5716\uff0c\u4ee5\u63a8\u52d5\u8cc7\u6e90\u8f03\u5c11\u8a9e\u8a00\u4e2d\u7684 NLP\uff0c\u7e2e\u5c0f\u6a21\u578b\u8a55\u4f30\u3001\u4efb\u52d9\u5275\u65b0\u548c\u73fe\u5be6\u4e16\u754c\u5f71\u97ff\u4e4b\u9593\u7684\u5dee\u8ddd\u3002", "author": "John Pavlopoulos et.al.", "authors": "John Pavlopoulos, Juli Bakagianni, Kanella Pouli, Maria Gavriilidou", "id": "2501.12826v1", "paper_url": "http://arxiv.org/abs/2501.12826v1", "repo": "null"}}