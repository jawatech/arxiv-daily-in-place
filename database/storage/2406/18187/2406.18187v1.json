{"2406.18187": {"publish_time": "2024-06-26", "title": "Selective Prompting Tuning for Personalized Conversations with LLMs", "paper_summary": "In conversational AI, personalizing dialogues with persona profiles and\ncontextual understanding is essential. Despite large language models' (LLMs)\nimproved response coherence, effective persona integration remains a challenge.\nIn this work, we first study two common approaches for personalizing LLMs:\ntextual prompting and direct fine-tuning. We observed that textual prompting\noften struggles to yield responses that are similar to the ground truths in\ndatasets, while direct fine-tuning tends to produce repetitive or overly\ngeneric replies. To alleviate those issues, we propose \\textbf{S}elective\n\\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for\npersonalized conversations in a selective way. Concretely, SPT initializes a\nset of soft prompts and uses a trainable dense retriever to adaptively select\nsuitable soft prompts for LLMs according to different input contexts, where the\nprompt retriever is dynamically updated through feedback from the LLMs.\nAdditionally, we propose context-prompt contrastive learning and prompt fusion\nlearning to encourage the SPT to enhance the diversity of personalized\nconversations. Experiments on the CONVAI2 dataset demonstrate that SPT\nsignificantly enhances response diversity by up to 90\\%, along with\nimprovements in other critical performance indicators. Those results highlight\nthe efficacy of SPT in fostering engaging and personalized dialogue generation.\nThe SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available\nfor further exploration.", "paper_summary_zh": "\u5728\u5c0d\u8a71\u5f0f AI \u4e2d\uff0c\u4f7f\u7528\u89d2\u8272\u8a2d\u5b9a\u6a94\u548c\u60c5\u5883\u7406\u89e3\u4f86\u500b\u4eba\u5316\u5c0d\u8a71\u81f3\u95dc\u91cd\u8981\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6539\u5584\u4e86\u56de\u61c9\u7684\u9023\u8cab\u6027\uff0c\u4f46\u6709\u6548\u6574\u5408\u89d2\u8272\u8a2d\u5b9a\u6a94\u4ecd\u7136\u662f\u4e00\u500b\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u7814\u7a76\u4e86\u5169\u7a2e\u7528\u65bc\u500b\u4eba\u5316 LLM \u7684\u5e38\u898b\u65b9\u6cd5\uff1a\u6587\u5b57\u63d0\u793a\u548c\u76f4\u63a5\u5fae\u8abf\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u6587\u5b57\u63d0\u793a\u901a\u5e38\u96e3\u4ee5\u7522\u751f\u8207\u8cc7\u6599\u96c6\u4e2d\u7684\u57fa\u672c\u4e8b\u5be6\u76f8\u4f3c\u7684\u56de\u61c9\uff0c\u800c\u76f4\u63a5\u5fae\u8abf\u5247\u50be\u5411\u65bc\u7522\u751f\u91cd\u8907\u6216\u904e\u65bc\u901a\u7528\u7684\u56de\u8986\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86**\u9078**\u64c7\u6027**\u63d0**\u793a**\u8abf**\u6821 (SPT)\uff0c\u5b83\u4ee5\u9078\u64c7\u6027\u7684\u65b9\u5f0f\u67d4\u548c\u5730\u63d0\u793a LLM \u9032\u884c\u500b\u4eba\u5316\u5c0d\u8a71\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSPT \u521d\u59cb\u5316\u4e00\u7d44\u8edf\u63d0\u793a\uff0c\u4e26\u4f7f\u7528\u53ef\u8a13\u7df4\u7684\u5bc6\u96c6\u6aa2\u7d22\u5668\u6839\u64da\u4e0d\u540c\u7684\u8f38\u5165\u60c5\u5883\u81ea\u9069\u61c9\u5730\u70ba LLM \u9078\u64c7\u5408\u9069\u7684\u8edf\u63d0\u793a\uff0c\u5176\u4e2d\u63d0\u793a\u6aa2\u7d22\u5668\u6703\u900f\u904e LLM \u7684\u56de\u994b\u9032\u884c\u52d5\u614b\u66f4\u65b0\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u60c5\u5883\u63d0\u793a\u5c0d\u6bd4\u5b78\u7fd2\u548c\u63d0\u793a\u878d\u5408\u5b78\u7fd2\uff0c\u4ee5\u9f13\u52f5 SPT \u63d0\u5347\u500b\u4eba\u5316\u5c0d\u8a71\u7684\u591a\u6a23\u6027\u3002\u5728 CONVAI2 \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0cSPT \u5c07\u56de\u61c9\u591a\u6a23\u6027\u986f\u8457\u63d0\u5347\u4e86 90%\uff0c\u540c\u6642\u4e5f\u6539\u5584\u4e86\u5176\u4ed6\u91cd\u8981\u7684\u6548\u80fd\u6307\u6a19\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86 SPT \u5728\u4fc3\u9032\u5f15\u4eba\u5165\u52dd\u4e14\u500b\u4eba\u5316\u7684\u5c0d\u8a71\u7522\u751f\u65b9\u9762\u7684\u6548\u529b\u3002SPT \u6a21\u578b\u7a0b\u5f0f\u78bc (https://github.com/hqsiswiliam/SPT) \u5df2\u516c\u958b\uff0c\u4f9b\u9032\u4e00\u6b65\u63a2\u7d22\u3002", "author": "Qiushi Huang et.al.", "authors": "Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang", "id": "2406.18187v1", "paper_url": "http://arxiv.org/abs/2406.18187v1", "repo": "https://github.com/hqsiswiliam/SPT"}}