{"2406.17636": {"publish_time": "2024-06-25", "title": "Aligning Diffusion Models with Noise-Conditioned Perception", "paper_summary": "Recent advancements in human preference optimization, initially developed for\nLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,\nenhancing prompt alignment, visual appeal, and user preference. Unlike LMs,\nDiffusion Models typically optimize in pixel or VAE space, which does not align\nwell with human perception, leading to slower and less efficient training\nduring the preference alignment stage. We propose using a perceptual objective\nin the U-Net embedding space of the diffusion model to address these issues.\nOur approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct\nPreference Optimization (DPO), Contrastive Preference Optimization (CPO), and\nsupervised fine-tuning (SFT) within this embedding space. This method\nsignificantly outperforms standard latent-space implementations across various\nmetrics, including quality and computational cost. For SDXL, our approach\nprovides 60.8\\% general preference, 62.2\\% visual appeal, and 52.1\\% prompt\nfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,\nwhile significantly reducing compute. Our approach not only improves the\nefficiency and quality of human preference alignment for diffusion models but\nis also easily integrable with other optimization techniques. The training code\nand LoRA weights will be available here:\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1", "paper_summary_zh": "\u6700\u8fd1\u5728\u4eba\u985e\u504f\u597d\u6700\u4f73\u5316\u65b9\u9762\u7684\u9032\u5c55\uff0c\u6700\u521d\u662f\u70ba\u8a9e\u8a00\u6a21\u578b (LM) \u958b\u767c\u7684\uff0c\u5df2\u986f\u793a\u51fa\u5c0d\u6587\u5b57\u5230\u5f71\u50cf\u7684\u64f4\u6563\u6a21\u578b\u5f88\u6709\u524d\u666f\uff0c\u589e\u5f37\u63d0\u793a\u5c0d\u9f4a\u3001\u8996\u89ba\u5438\u5f15\u529b\u548c\u7528\u6236\u504f\u597d\u3002\u8207 LM \u4e0d\u540c\uff0c\u64f4\u6563\u6a21\u578b\u901a\u5e38\u5728\u50cf\u7d20\u6216 VAE \u7a7a\u9593\u4e2d\u9032\u884c\u6700\u4f73\u5316\uff0c\u9019\u8207\u4eba\u985e\u611f\u77e5\u4e0d\u7b26\uff0c\u5c0e\u81f4\u5728\u504f\u597d\u5c0d\u9f4a\u968e\u6bb5\u8a13\u7df4\u66f4\u6162\u4e14\u6548\u7387\u66f4\u4f4e\u3002\u6211\u5011\u5efa\u8b70\u5728\u64f4\u6563\u6a21\u578b\u7684 U-Net \u5d4c\u5165\u7a7a\u9593\u4e2d\u4f7f\u7528\u611f\u77e5\u76ee\u6a19\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\u3001\u5c0d\u6bd4\u504f\u597d\u6700\u4f73\u5316 (CPO) \u548c\u5728\u9019\u500b\u5d4c\u5165\u7a7a\u9593\u5167\u7684\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u4f86\u5fae\u8abf Stable Diffusion 1.5 \u548c XL\u3002\u9019\u7a2e\u65b9\u6cd5\u5728\u5404\u7a2e\u6307\u6a19\u4e0a\u660e\u986f\u512a\u65bc\u6a19\u6e96\u7684\u6f5b\u5728\u7a7a\u9593\u5be6\u4f5c\uff0c\u5305\u62ec\u54c1\u8cea\u548c\u904b\u7b97\u6210\u672c\u3002\u5c0d\u65bc SDXL\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728 PartiPrompts \u8cc7\u6599\u96c6\u4e0a\u63d0\u4f9b 60.8% \u7684\u4e00\u822c\u504f\u597d\u300162.2% \u7684\u8996\u89ba\u5438\u5f15\u529b\u548c 52.1% \u7684\u63d0\u793a\u9075\u5faa\uff0c\u540c\u6642\u986f\u8457\u6e1b\u5c11\u904b\u7b97\u3002\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u50c5\u63d0\u9ad8\u4e86\u64f4\u6563\u6a21\u578b\u7684\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7684\u6548\u7387\u548c\u54c1\u8cea\uff0c\u800c\u4e14\u5f88\u5bb9\u6613\u8207\u5176\u4ed6\u6700\u4f73\u5316\u6280\u8853\u6574\u5408\u3002\u8a13\u7df4\u7a0b\u5f0f\u78bc\u548c LoRA \u6b0a\u91cd\u5c07\u5728\u6b64\u8655\u63d0\u4f9b\uff1a\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1", "author": "Alexander Gambashidze et.al.", "authors": "Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, Ilya Makarov", "id": "2406.17636v1", "paper_url": "http://arxiv.org/abs/2406.17636v1", "repo": "null"}}