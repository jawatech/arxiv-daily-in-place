{"2405.18203": {"publish_time": "2024-05-28", "title": "IAPT: Instruction-Aware Prompt Tuning for Large Language Models", "paper_summary": "Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.", "paper_summary_zh": "\u8edf\u63d0\u793a\u8abf\u6574\u662f\u4e00\u7a2e\u5ee3\u6cdb\u7814\u7a76\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5b83\u6709\u4e00\u500b\u660e\u986f\u7684\u7f3a\u9ede\uff1a\u5fc5\u9808\u5c07\u8a31\u591a\u8edf\u6a19\u8a18\u63d2\u5165\u8f38\u5165\u5e8f\u5217\u4e2d\u4ee5\u4fdd\u8b49\u4e0b\u6e38\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u5728\u5927\u578b\u8a9e\u8a00\u5efa\u6a21 (LLM) \u6642\u4ee3\uff0c\u8edf\u63d0\u793a\u8abf\u6574\u4e0d\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA) \u53d7\u91cd\u8996\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u63d0\u793a\u8abf\u6574\u65b9\u6cd5\uff0c\u5373\u6307\u4ee4\u611f\u77e5\u63d0\u793a\u8abf\u6574 (IAPT)\uff0c\u5b83\u53ea\u9700\u8981\u56db\u500b\u8edf\u6a19\u8a18\u3002\u9996\u5148\uff0c\u6211\u5011\u5728\u6bcf\u500b Transformer \u5c64\u5b89\u88dd\u4e00\u500b\u53c3\u6578\u9ad8\u6548\u7684\u8edf\u63d0\u793a\u751f\u6210\u5668\uff0c\u70ba\u6bcf\u500b\u8f38\u5165\u6307\u4ee4\u751f\u6210\u7368\u7279\u7684\u8edf\u63d0\u793a\u3002\u751f\u6210\u7684\u8edf\u63d0\u793a\u53ef\u4ee5\u770b\u4f5c\u662f\u8f38\u5165\u6307\u4ee4\u7684\u8a9e\u7fa9\u6458\u8981\uff0c\u4e26\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u6307\u5c0e\u8f38\u51fa\u751f\u6210\u3002\u5176\u6b21\uff0c\u8edf\u63d0\u793a\u751f\u6210\u5668\u662f\u5177\u6709\u74f6\u9838\u67b6\u69cb\u7684\u6a21\u7d44\uff0c\u5305\u62ec\u81ea\u6ce8\u610f\u529b\u6c60\u5316\u64cd\u4f5c\u3001\u5169\u500b\u7dda\u6027\u6295\u5f71\u548c\u4e00\u500b\u6fc0\u6d3b\u51fd\u6578\u3002\u8a66\u9a57\u8868\u660e\uff0c\u4e0d\u540c Transformer \u5c64\u7684\u63d0\u793a\u751f\u6210\u5668\u9700\u8981\u4e0d\u540c\u7684\u6fc0\u6d3b\u51fd\u6578\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u6709\u7406\u51fd\u6578\u7684\u5e6b\u52a9\u81ea\u52d5\u5b78\u7fd2\u63d0\u793a\u751f\u6210\u5668\u7684\u7368\u7279\u6fc0\u6d3b\u51fd\u6578\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u4efb\u52d9\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff1a(a) \u6211\u5011\u7684 IAPT \u65b9\u6cd5\u53ef\u4ee5\u5728\u5177\u6709\u53ef\u6bd4\u8f03\u53ef\u8abf\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\u512a\u65bc\u6700\u8fd1\u7684\u57fa\u6e96\u3002(b) \u5728\u55ae\u9aa8\u5e79\u591a\u79df\u6236\u8a2d\u7f6e\u4e0b\uff0c\u6211\u5011\u7684 IAPT \u65b9\u6cd5\u6bd4 LoRA \u66f4\u6709\u6548\u7387\u3002", "author": "Wei Zhu et.al.", "authors": "Wei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan Ni, Xiaoling Wang, Guotong Xie", "id": "2405.18203v1", "paper_url": "http://arxiv.org/abs/2405.18203v1", "repo": "null"}}