{"2411.18506": {"publish_time": "2024-11-27", "title": "LLM-ABBA: Understanding time series via symbolic approximation", "paper_summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6642\u9593\u5e8f\u5217\u65b9\u9762\u7684\u6210\u529f\u5df2\u5728\u5148\u524d\u7684\u7814\u7a76\u4e2d\u5f97\u5230\u8b49\u660e\u3002\u5229\u7528\u7b26\u865f\u6642\u9593\u5e8f\u5217\u8868\u793a\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5f4c\u5408 LLM \u548c\u6642\u9593\u5e8f\u5217\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u7136\u800c\uff0c\u5269\u4e0b\u7684\u6311\u6230\u662f\u5229\u7528\u7b26\u865f\u6216\u73fe\u6709 LLM \u6a19\u8a18\u4f86\u5229\u7528\u96b1\u85cf\u5728\u6642\u9593\u5e8f\u5217\u4e2d\u7684\u8a9e\u7fa9\u8cc7\u8a0a\uff0c\u540c\u6642\u6839\u64da\u6642\u9593\u5e8f\u5217\u7684\u96b1\u85cf\u8cc7\u8a0a\u5c0d\u9f4a LLM \u7684\u5d4c\u5165\u7a7a\u9593\u3002\u7a31\u70ba\u81ea\u9069\u61c9\u5e03\u6717\u6a4b\u7b26\u865f\u805a\u5408 (ABBA) \u7684\u7b26\u865f\u6642\u9593\u5e8f\u5217\u8fd1\u4f3c (STSA) \u65b9\u6cd5\u5728\u900f\u904e\u632f\u5e45\u548c\u9031\u671f\u4f86\u5efa\u6a21\u6642\u9593\u5e8f\u5217\u6a21\u5f0f\uff0c\u540c\u6642\u4f7f\u7528\u73fe\u6709 LLM \u6a19\u8a18\u65b9\u9762\uff0c\u5c55\u73fe\u51fa\u5728\u4fdd\u7559\u986f\u8457\u6642\u9593\u5e8f\u5217\u7279\u5fb5\u65b9\u9762\u7684\u51fa\u8272\u529f\u6548\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e00\u7a2e\u7a31\u70ba LLM-ABBA \u7684\u65b9\u6cd5\uff0c\u5b83\u5c07 ABBA \u6574\u5408\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u57f7\u884c\u5404\u7a2e\u4e0b\u6e38\u6642\u9593\u5e8f\u5217\u4efb\u52d9\u3002\u900f\u904e\u5c07\u6642\u9593\u5e8f\u5217\u7b26\u865f\u5316\uff0cLLM-ABBA \u8207 UCR \u548c\u4e09\u500b\u91ab\u7642\u6642\u9593\u5e8f\u5217\u5206\u985e\u4efb\u52d9\u4e2d\u7684\u6700\u65b0\u6280\u8853 (SOTA) \u76f8\u6bd4\uff0c\u5177\u6709\u660e\u986f\u7684\u512a\u52e2\u3002\u540c\u6642\uff0cABBA \u4e2d\u5f15\u5165\u4e86\u56fa\u5b9a\u591a\u908a\u5f62\u93c8\u6280\u5de7\uff0c\u900f\u904e\u5927\u5e45\u6e1b\u8f15\u5f9e\u7b26\u865f\u8f49\u63db\u70ba\u6578\u503c\u6642\uff0c\u56e0\u8aa4\u7528\u7b26\u865f\u800c\u7522\u751f\u7684\u7d2f\u7a4d\u8aa4\u5dee\u5f71\u97ff\uff0c\u4f86\u907f\u514d\u5728\u9810\u6e2c\u4efb\u52d9\u671f\u9593\u51fa\u73fe\u660e\u986f\u7684\u6f02\u79fb\u3002\u5728\u6642\u9593\u5e8f\u5217\u56de\u6b78\u4efb\u52d9\u4e2d\uff0cLLM-ABBA \u5728\u6642\u9593\u5e8f\u5217\u5916\u5728\u56de\u6b78 (TSER) \u57fa\u6e96\u4e0a\u9054\u5230\u4e86\u65b0\u7684 SOTA\u3002\u8207\u6700\u8fd1\u7684 SOTA \u6642\u9593\u5e8f\u5217\u9810\u6e2c\u7d50\u679c\u76f8\u6bd4\uff0cLLM-ABBA \u4e5f\u5c55\u73fe\u51fa\u5177\u7af6\u722d\u529b\u7684\u9810\u6e2c\u80fd\u529b\u3002\u6211\u5011\u76f8\u4fe1\u9019\u500b\u67b6\u69cb\u4e5f\u53ef\u4ee5\u7121\u7e2b\u5730\u64f4\u5145\u5230\u5176\u4ed6\u6642\u9593\u5e8f\u5217\u4efb\u52d9\u3002", "author": "Erin Carson et.al.", "authors": "Erin Carson, Xinye Chen, Cheng Kang", "id": "2411.18506v3", "paper_url": "http://arxiv.org/abs/2411.18506v3", "repo": "null"}}