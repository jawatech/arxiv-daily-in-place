{"2409.18764": {"publish_time": "2024-09-27", "title": "Charting the Future: Using Chart Question-Answering for Scalable Evaluation of LLM-Driven Data Visualizations", "paper_summary": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u8996\u89ba\u554f\u7b54 (VQA) \u6a21\u578b\u4f86\u81ea\u52d5\u5316 LLM \u751f\u6210\u7684\u8cc7\u6599\u8996\u89ba\u5316\u7684\u8a55\u4f30\u3002\u50b3\u7d71\u7684\u8a55\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u4eba\u5de5\u5224\u65b7\uff0c\u9019\u65e2\u6602\u8cb4\u53c8\u4e0d\u5177\u53ef\u64f4\u5c55\u6027\uff0c\u6216\u8005\u50c5\u95dc\u6ce8\u8cc7\u6599\u6e96\u78ba\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u8996\u89ba\u6e9d\u901a\u7684\u6709\u6548\u6027\u3002\u900f\u904e\u4f7f\u7528 VQA \u6a21\u578b\uff0c\u6211\u5011\u8a55\u4f30\u8cc7\u6599\u8868\u793a\u54c1\u8cea\u548c\u5716\u8868\u7684\u6574\u9ad4\u6e9d\u901a\u6e05\u6670\u5ea6\u3002\u5be6\u9a57\u4f7f\u7528\u5169\u500b\u9818\u5148\u7684 VQA \u57fa\u6e96\u8cc7\u6599\u96c6\uff0cChartQA \u548c PlotQA\uff0c\u4ee5\u53ca\u7531 OpenAI \u7684 GPT-3.5 Turbo \u548c Meta \u7684 Llama 3.1 70B-Instruct \u6a21\u578b\u751f\u6210\u7684\u8996\u89ba\u5316\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6839\u64da VQA \u6548\u80fd\u6307\u6a19\uff0cLLM \u751f\u6210\u7684\u5716\u8868\u7121\u6cd5\u6bd4\u5f97\u4e0a\u539f\u59cb\u975e LLM \u751f\u6210\u7684\u5716\u8868\u6e96\u78ba\u6027\u3002\u6b64\u5916\uff0c\u5118\u7ba1\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u5c11\u6b21\u63d0\u793a\u986f\u8457\u63d0\u5347\u4e86\u5716\u8868\u751f\u6210\u7684\u6e96\u78ba\u6027\uff0c\u4f46\u5728 LLM \u80fd\u5b8c\u5168\u6bd4\u5f97\u4e0a\u4eba\u5de5\u751f\u6210\u7684\u5716\u8868\u7684\u7cbe\u6e96\u5ea6\u4e4b\u524d\uff0c\u4ecd\u6709\u76f8\u7576\u5927\u7684\u9032\u6b65\u7a7a\u9593\u3002\u9019\u7a81\u986f\u4e86\u6211\u5011\u5de5\u4f5c\u7684\u50f9\u503c\uff0c\u5b83\u900f\u904e\u7121\u9700\u4eba\u5de5\u6a19\u8a3b\u4f86\u52a0\u5feb\u53cd\u8986\u904b\u7b97\uff0c\u5f9e\u800c\u52a0\u901f\u6b64\u9818\u57df\u7684\u9032\u5c55\u3002", "author": "James Ford et.al.", "authors": "James Ford, Xingmeng Zhao, Dan Schumacher, Anthony Rios", "id": "2409.18764v1", "paper_url": "http://arxiv.org/abs/2409.18764v1", "repo": "null"}}