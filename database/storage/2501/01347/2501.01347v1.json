{"2501.01347": {"publish_time": "2025-01-02", "title": "AdaptVC: High Quality Voice Conversion with Adaptive Learning", "paper_summary": "The goal of voice conversion is to transform the speech of a source speaker\nto sound like that of a reference speaker while preserving the original\ncontent. A key challenge is to extract disentangled linguistic content from the\nsource and voice style from the reference. While existing approaches leverage\nvarious methods to isolate the two, a generalization still requires further\nattention, especially for robustness in zero-shot scenarios. In this paper, we\nachieve successful disentanglement of content and speaker features by tuning\nself-supervised speech features with adapters. The adapters are trained to\ndynamically encode nuanced features from rich self-supervised features, and the\ndecoder fuses them to produce speech that accurately resembles the reference\nwith minimal loss of content. Moreover, we leverage a conditional flow matching\ndecoder with cross-attention speaker conditioning to further boost the\nsynthesis quality and efficiency. Subjective and objective evaluations in a\nzero-shot scenario demonstrate that the proposed method outperforms existing\nmodels in speech quality and similarity to the reference speech.", "paper_summary_zh": "\u8a9e\u97f3\u8f49\u63db\u7684\u76ee\u6a19\u662f\u5c07\u4f86\u6e90\u8aaa\u8a71\u8005\u7684\u8a9e\u97f3\u8f49\u63db\u6210\u807d\u8d77\u4f86\u50cf\u53c3\u8003\u8aaa\u8a71\u8005\u7684\u8072\u97f3\uff0c\u540c\u6642\u4fdd\u7559\u539f\u59cb\u5167\u5bb9\u3002\u4e00\u500b\u95dc\u9375\u6311\u6230\u662f\u5f9e\u4f86\u6e90\u4e2d\u63d0\u53d6\u51fa\u7cfe\u7e8f\u7684\u8a9e\u8a00\u5167\u5bb9\uff0c\u4ee5\u53ca\u5f9e\u53c3\u8003\u4e2d\u63d0\u53d6\u51fa\u8072\u97f3\u98a8\u683c\u3002\u96d6\u7136\u73fe\u6709\u7684\u65b9\u6cd5\u5229\u7528\u5404\u7a2e\u65b9\u6cd5\u4f86\u9694\u96e2\u9019\u5169\u8005\uff0c\u4f46\u6982\u62ec\u5316\u4ecd\u9700\u8981\u9032\u4e00\u6b65\u95dc\u6ce8\uff0c\u7279\u5225\u662f\u5728\u96f6\u6b21\u5b78\u7fd2\u5834\u666f\u4e2d\u7684\u7a69\u5065\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u901a\u904e\u4f7f\u7528\u9069\u914d\u5668\u8abf\u6574\u81ea\u6211\u76e3\u7763\u7684\u8a9e\u97f3\u7279\u5fb5\uff0c\u6210\u529f\u5730\u89e3\u958b\u4e86\u5167\u5bb9\u548c\u8aaa\u8a71\u8005\u7279\u5fb5\u7684\u7cfe\u7e8f\u3002\u9069\u914d\u5668\u7d93\u904e\u8a13\u7df4\uff0c\u53ef\u4ee5\u5f9e\u8c50\u5bcc\u7684\u81ea\u6211\u76e3\u7763\u7279\u5fb5\u4e2d\u52d5\u614b\u7de8\u78bc\u7d30\u5fae\u7279\u5fb5\uff0c\u4e26\u4e14\u89e3\u78bc\u5668\u5c07\u5b83\u5011\u878d\u5408\u8d77\u4f86\uff0c\u4ee5\u7522\u751f\u6e96\u78ba\u985e\u4f3c\u65bc\u53c3\u8003\u7684\u8a9e\u97f3\uff0c\u540c\u6642\u6700\u5c0f\u5316\u5167\u5bb9\u640d\u5931\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u689d\u4ef6\u6d41\u5339\u914d\u89e3\u78bc\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u8aaa\u8a71\u8005\u689d\u4ef6\u4f86\u9032\u4e00\u6b65\u63d0\u5347\u5408\u6210\u54c1\u8cea\u548c\u6548\u7387\u3002\u5728\u96f6\u6b21\u5b78\u7fd2\u5834\u666f\u4e2d\u7684\u4e3b\u89c0\u548c\u5ba2\u89c0\u8a55\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8a9e\u97f3\u54c1\u8cea\u548c\u8207\u53c3\u8003\u8a9e\u97f3\u7684\u76f8\u4f3c\u6027\u65b9\u9762\u512a\u65bc\u73fe\u6709\u7684\u6a21\u578b\u3002", "author": "Jaehun Kim et.al.", "authors": "Jaehun Kim, Ji-Hoon Kim, Yeunju Choi, Tan Dat Nguyen, Seongkyu Mun, Joon Son Chung", "id": "2501.01347v1", "paper_url": "http://arxiv.org/abs/2501.01347v1", "repo": "null"}}