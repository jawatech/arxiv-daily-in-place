{"2412.11834": {"publish_time": "2024-12-16", "title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture", "paper_summary": "In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.", "paper_summary_zh": "\u4e3a\u4e86\u8ba9\u57fa\u7840\u6a21\u578b\u66f4\u52a0\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u6211\u4eec\u7684\u60f3\u6cd5\u662f\u5c06\u5e8f\u5217\u8f6c\u6362\u548c\u72b6\u6001\u8f6c\u6362\u7ed3\u5408\u8d77\u6765\u3002\u9996\u5148\uff0c\u6211\u4eec\u5728\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u7b97\u6cd5\u4e2d\u8bc1\u660e\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u7684\u53ef\u7528\u6027\uff0c\u8fd9\u5c06\u6df7\u5408\u4e8c\u6b21\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e86 4% \u4ee5\u4e0a\uff0c\u4ee5\u786e\u4fdd\u7ec4\u5408\u5e8f\u5217\u8f6c\u6362\u7edf\u4e00\u4f4d\u7f6e\u7f16\u7801\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u52a8\u6001\u63a9\u7801\u6ce8\u610f\u529b\uff0c\u8fd9\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u591a\u67e5\u8be2\u5173\u8054\u53ec\u56de\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86 100% \u7684\u51c6\u786e\u5ea6\uff0c\u4e0e\u4e8c\u6b21\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86 150% \u4ee5\u4e0a\uff0c\u4ee5\u786e\u4fdd\u7ec4\u5408\u5e8f\u5217\u8f6c\u6362\u6709\u9009\u62e9\u5730\u8fc7\u6ee4\u76f8\u5173\u4fe1\u606f\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8de8\u57df\u4e13\u5bb6\u6df7\u5408\uff0c\u8fd9\u4f7f\u5f97\u4e13\u5bb6\u68c0\u7d22\u7684\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4e13\u5bb6\u6df7\u5408\u5feb\u4e86 8 \u5230 10 \u500d\uff0c\u4e13\u5bb6\u6570\u91cf\u8d85\u8fc7 1024 \u4e2a\uff0c\u4ee5\u786e\u4fdd\u7ec4\u5408\u72b6\u6001\u8f6c\u6362\u5feb\u901f\u68c0\u7d22\u6df7\u5408\u3002\u6700\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u8fd9\u4e9b\u53ef\u4ee5\u5f62\u6210\u57fa\u7840\u6a21\u578b\u7684\u77e9\u9635\u7b97\u6cd5\uff1aWonderful Matrices\uff0c\u5b83\u53ef\u4ee5\u6210\u4e3a\u6d41\u884c\u6a21\u578b\u67b6\u6784\u7684\u7ade\u4e89\u5bf9\u624b\u3002", "author": "Jingze Shi et.al.", "authors": "Jingze Shi, Bingheng Wu", "id": "2412.11834v1", "paper_url": "http://arxiv.org/abs/2412.11834v1", "repo": "https://github.com/losercheems/doge"}}