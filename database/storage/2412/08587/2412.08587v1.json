{"2412.08587": {"publish_time": "2024-12-11", "title": "Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning", "paper_summary": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance.", "paper_summary_zh": "<paragraph>\u50c5\u7de8\u78bc\u5668\u6a21\u578b\uff08\u4f8b\u5982 BERT\u3001RoBERTa\uff09\u548c\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff0c\u4f8b\u5982 Llama3\uff09\u5df2\u88ab\u5ee3\u6cdb\u7528\u65bc\u6587\u672c\u5206\u985e\u4efb\u52d9\u3002\n\u7136\u800c\uff0c\u7f3a\u4e4f\u6bd4\u8f03\u7de8\u78bc\u5668\u6a21\u578b\u548c LLM \u5728\u6587\u672c\u5206\u985e\u4e2d\u7684\u6548\u80fd\u7684\u7cfb\u7d71\u6027\u7814\u7a76\uff0c\u7279\u5225\u662f\u5728\u6d89\u53ca\u5fae\u8abf\u6642\u3002\u672c\u7814\u7a76\u63a1\u7528\u4e86\u5404\u7a2e\u4e0d\u540c\u7684\u6a21\u578b\u548c\u65b9\u6cd5\uff0c\u5728\u5927\u5c0f\u548c\u67b6\u69cb\u4e0a\u6709\u6240\u4e0d\u540c\uff0c\u4e26\u4e14\u5305\u62ec\u5fae\u8abf\u548c\u9810\u8a13\u7df4\u65b9\u6cd5\u3002\u6211\u5011\u9996\u5148\u8a55\u4f30\u4e86\u9019\u4e9b LLM \u5728 20 \u500b\u65b0\u805e\u7d44 (20NG) \u548c MASSIVE \u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\uff0c\u4e26\u5c07\u5176\u8207\u50c5\u7de8\u78bc\u5668 RoBERTa \u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u5c07\u591a\u500b\u5206\u985e\u4efb\u52d9\uff08\u5305\u62ec\u610f\u5716\u5075\u6e2c\u548c\u69fd\u4f4d\u586b\u88dc\uff09\u7d50\u5408\u5230\u4e00\u500b\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u4f86\u81ea\u5169\u500b\u8cc7\u6599\u96c6\u7684\u8cc7\u6599\uff0c\u63a2\u7d22\u4e86\u5169\u7a2e\u6a21\u578b\u985e\u578b\u7684\u591a\u4efb\u52d9\u529f\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u7d93\u904e\u5b8c\u5168\u5fae\u8abf\u7684 Llama3-70B \u6a21\u578b\u5728\u5404\u7a2e\u5206\u985e\u4efb\u52d9\u548c\u8cc7\u6599\u96c6\u4e0a\u512a\u65bc RoBERTa-large \u548c\u5176\u4ed6\u89e3\u78bc\u5668 LLM\u3002\u6b64\u5916\uff0c\u5408\u4f75\u7684\u591a\u4efb\u52d9\u5fae\u8abf LLM \u5728\u5169\u500b\u4efb\u52d9\u4e2d\u90fd\u8207\u96d9\u6a21\u578b\u8a2d\u5b9a\u7684\u6548\u80fd\u76f8\u5339\u914d\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7de8\u78bc\u5668\u548c LLM \u6a21\u578b\u5728\u6587\u672c\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u5168\u9762\u57fa\u6e96\uff0c\u4e26\u5c55\u793a\u4e86\u4e00\u7a2e\u7d50\u5408\u5169\u500b\u6216\u66f4\u591a\u5b8c\u5168\u5fae\u8abf\u7684\u89e3\u78bc\u5668 LLM \u4ee5\u964d\u4f4e\u5ef6\u9072\u548c\u7b49\u6548\u6548\u80fd\u7684\u65b9\u6cd5\u3002</paragraph>", "author": "Hang Zhao et.al.", "authors": "Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang", "id": "2412.08587v1", "paper_url": "http://arxiv.org/abs/2412.08587v1", "repo": "null"}}