{"2407.10671": {"publish_time": "2024-07-15", "title": "Qwen2 Technical Report", "paper_summary": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\nsupplementary materials including example code on GitHub3. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.", "paper_summary_zh": "<paragraph>\u6b64\u5831\u544a\u4ecb\u7d39\u4e86 Qwen2 \u7cfb\u5217\uff0c\u9019\u662f\u6211\u5011\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u5927\u898f\u6a21\u591a\u6a21\u614b\u6a21\u578b\u7684\u6700\u65b0\u6210\u54e1\u3002\u6211\u5011\u767c\u5e03\u4e86\u4e00\u5957\u5168\u9762\u7684\u57fa\u790e\u548c\u6307\u4ee4\u8abf\u6574\u8a9e\u8a00\u6a21\u578b\uff0c\u5305\u542b\u5f9e 0.5 \u5230 720 \u5104\u7684\u53c3\u6578\u7bc4\u570d\uff0c\u5177\u6709\u5bc6\u96c6\u6a21\u578b\u548c\u5c08\u5bb6\u6df7\u5408\u6a21\u578b\u3002Qwen2 \u8d85\u8d8a\u4e86\u5927\u591a\u6578\u5148\u524d\u7684\u958b\u653e\u6b0a\u91cd\u6a21\u578b\uff0c\u5305\u62ec\u5176\u524d\u8eab Qwen1.5\uff0c\u4e26\u5728\u8a9e\u8a00\u7406\u89e3\u3001\u751f\u6210\u3001\u591a\u8a9e\u8a00\u80fd\u529b\u3001\u7de8\u78bc\u3001\u6578\u5b78\u548c\u63a8\u7406\u7b49\u4e0d\u540c\u57fa\u6e96\u4e0a\u5c55\u73fe\u51fa\u8207\u5c08\u6709\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\n\u65d7\u8266\u6a21\u578b Qwen2-72B \u5c55\u793a\u4e86\u975e\u51e1\u7684\u6548\u80fd\uff1a\u4f5c\u70ba\u57fa\u790e\u8a9e\u8a00\u6a21\u578b\uff0c\u5728 MMLU \u4e0a\u70ba 84.2\uff0c\u5728 GPQA \u4e0a\u70ba 37.9\uff0c\u5728 HumanEval \u4e0a\u70ba 64.6\uff0c\u5728 GSM8K \u4e0a\u70ba 89.5\uff0c\u5728 BBH \u4e0a\u70ba 82.4\u3002\u6307\u4ee4\u8abf\u6574\u8b8a\u9ad4 Qwen2-72B-Instruct \u5728 MT-Bench \u4e0a\u9054\u5230 9.1\uff0c\u5728 Arena-Hard \u4e0a\u9054\u5230 48.1\uff0c\u5728 LiveCodeBench \u4e0a\u9054\u5230 35.7\u3002\u6b64\u5916\uff0cQwen2 \u5c55\u793a\u4e86\u5f37\u5927\u7684\u591a\u8a9e\u8a00\u80fd\u529b\uff0c\u7cbe\u901a\u7d04 30 \u7a2e\u8a9e\u8a00\uff0c\u6db5\u84cb\u82f1\u8a9e\u3001\u4e2d\u6587\u3001\u897f\u73ed\u7259\u8a9e\u3001\u6cd5\u8a9e\u3001\u5fb7\u8a9e\u3001\u963f\u62c9\u4f2f\u8a9e\u3001\u4fc4\u8a9e\u3001\u97d3\u8a9e\u3001\u65e5\u8a9e\u3001\u6cf0\u8a9e\u3001\u8d8a\u5357\u8a9e\u7b49\uff0c\u5f37\u8abf\u4e86\u5b83\u7684\u591a\u529f\u80fd\u6027\u548c\u5168\u7403\u5f71\u97ff\u529b\u3002\n\u70ba\u4e86\u4fc3\u9032\u793e\u7fa4\u5275\u65b0\u548c\u53ef\u53ca\u6027\uff0c\u6211\u5011\u5df2\u5728 Hugging Face1 \u548c ModelScope2 \u4e0a\u516c\u958b\u4e86 Qwen2 \u6a21\u578b\u6b0a\u91cd\uff0c\u4ee5\u53ca\u5728 GitHub3 \u4e0a\u5305\u542b\u7bc4\u4f8b\u7a0b\u5f0f\u78bc\u7684\u88dc\u5145\u8cc7\u6599\u3002\u9019\u4e9b\u5e73\u53f0\u9084\u5305\u62ec\u91cf\u5316\u3001\u5fae\u8abf\u548c\u90e8\u7f72\u7684\u8cc7\u6e90\uff0c\u4fc3\u9032\u4e86\u5ee3\u6cdb\u7684\u61c9\u7528\u548c\u7814\u7a76\u5de5\u4f5c\u3002</paragraph>", "author": "An Yang et.al.", "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, Zhihao Fan", "id": "2407.10671v1", "paper_url": "http://arxiv.org/abs/2407.10671v1", "repo": "null"}}