{"2407.07026": {"publish_time": "2024-07-09", "title": "Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition", "paper_summary": "With the proliferation of social media posts in recent years, the need to\ndetect sentiments in multimodal (image-text) content has grown rapidly. Since\nposts are user-generated, the image and text from the same post can express\ndifferent or even contradictory sentiments, leading to potential\n\\textbf{sentiment discrepancy}. However, existing works mainly adopt a\nsingle-branch fusion structure that primarily captures the consistent sentiment\nbetween image and text. The ignorance or implicit modeling of discrepant\nsentiment results in compromised unimodal encoding and limited performances. In\nthis paper, we propose a semantics Completion and Decomposition (CoDe) network\nto resolve the above issue. In the semantics completion module, we complement\nimage and text representations with the semantics of the OCR text embedded in\nthe image, helping bridge the sentiment gap. In the semantics decomposition\nmodule, we decompose image and text representations with exclusive projection\nand contrastive learning, thereby explicitly capturing the discrepant sentiment\nbetween modalities. Finally, we fuse image and text representations by\ncross-attention and combine them with the learned discrepant sentiment for\nfinal classification. Extensive experiments conducted on four multimodal\nsentiment datasets demonstrate the superiority of CoDe against SOTA methods.", "paper_summary_zh": "\u96a8\u8457\u8fd1\u5e74\u4f86\u793e\u7fa4\u5a92\u9ad4\u8cbc\u6587\u7684\u6fc0\u589e\uff0c\u5075\u6e2c\u591a\u6a21\u614b\uff08\u5716\u50cf\u6587\u5b57\uff09\u5167\u5bb9\u7684\u60c5\u7dd2\u7684\u9700\u6c42\u4e5f\u8fc5\u901f\u589e\u9577\u3002\u7531\u65bc\u8cbc\u6587\u662f\u7531\u4f7f\u7528\u8005\u7522\u751f\u7684\uff0c\u4f86\u81ea\u540c\u4e00\u500b\u8cbc\u6587\u7684\u5716\u50cf\u548c\u6587\u5b57\u53ef\u80fd\u8868\u9054\u51fa\u4e0d\u540c\u751a\u81f3\u77db\u76fe\u7684\u60c5\u7dd2\uff0c\u5c0e\u81f4\u6f5b\u5728\u7684**\u60c5\u7dd2\u5dee\u7570**\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u4f5c\u54c1\u4e3b\u8981\u63a1\u7528\u55ae\u5206\u652f\u878d\u5408\u7d50\u69cb\uff0c\u4e3b\u8981\u64f7\u53d6\u5716\u50cf\u548c\u6587\u5b57\u4e4b\u9593\u4e00\u81f4\u7684\u60c5\u7dd2\u3002\u5c0d\u77db\u76fe\u60c5\u7dd2\u7684\u5ffd\u7565\u6216\u96b1\u5f0f\u5efa\u6a21\u5c0e\u81f4\u53d7\u640d\u7684\u55ae\u6a21\u614b\u7de8\u78bc\u548c\u6709\u9650\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u8a9e\u7fa9\u5b8c\u6210\u548c\u5206\u89e3 (CoDe) \u7db2\u8def\u4f86\u89e3\u6c7a\u4e0a\u8ff0\u554f\u984c\u3002\u5728\u8a9e\u7fa9\u5b8c\u6210\u6a21\u7d44\u4e2d\uff0c\u6211\u5011\u4ee5\u5d4c\u5165\u5728\u5716\u50cf\u4e2d\u7684 OCR \u6587\u5b57\u7684\u8a9e\u7fa9\u4f86\u88dc\u5145\u5716\u50cf\u548c\u6587\u5b57\u8868\u793a\uff0c\u6709\u52a9\u65bc\u7e2e\u5c0f\u60c5\u7dd2\u5dee\u8ddd\u3002\u5728\u8a9e\u7fa9\u5206\u89e3\u6a21\u7d44\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u7368\u5bb6\u6295\u5f71\u548c\u5c0d\u6bd4\u5b78\u7fd2\u4f86\u5206\u89e3\u5716\u50cf\u548c\u6587\u5b57\u8868\u793a\uff0c\u5f9e\u800c\u660e\u78ba\u64f7\u53d6\u6a21\u614b\u4e4b\u9593\u7684\u77db\u76fe\u60c5\u7dd2\u3002\u6700\u5f8c\uff0c\u6211\u5011\u900f\u904e\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5716\u50cf\u548c\u6587\u5b57\u8868\u793a\uff0c\u4e26\u5c07\u5b83\u5011\u8207\u5b78\u7fd2\u5230\u7684\u77db\u76fe\u60c5\u7dd2\u7d50\u5408\u8d77\u4f86\u9032\u884c\u6700\u7d42\u5206\u985e\u3002\u5728\u56db\u500b\u591a\u6a21\u614b\u60c5\u7dd2\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 CoDe \u512a\u65bc SOTA \u65b9\u6cd5\u3002", "author": "Daiqing Wu et.al.", "authors": "Daiqing Wu, Dongbao Yang, Huawen Shen, Can Ma, Yu Zhou", "id": "2407.07026v1", "paper_url": "http://arxiv.org/abs/2407.07026v1", "repo": "null"}}