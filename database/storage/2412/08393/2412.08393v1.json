{"2412.08393": {"publish_time": "2024-12-11", "title": "Learning to Reason via Self-Iterative Process Feedback for Small Language Models", "paper_summary": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval.", "paper_summary_zh": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u66f4\u6709\u6548\u7387\u3001\u66f4\u5177\u6210\u672c\u6548\u76ca\u4e14\u66f4\u5177\u53ef\u5b9a\u5236\u6027\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u63a8\u7406\u7b49\u7279\u5b9a\u9886\u57df\u901a\u5e38\u8868\u73b0\u4e0d\u4f73\u3002\u8fc7\u53bb\u589e\u5f3a SLM \u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u76d1\u7763\u5fae\u8c03\u548c\u84b8\u998f\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5916\u90e8\u4fe1\u53f7\uff0c\u5bfc\u81f4 SLM \u5bf9\u6709\u9650\u7684\u76d1\u7763\u4fe1\u53f7\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u4f7f SLM \u80fd\u591f\u4ece\u81ea\u6211\u8fed\u4ee3\u53cd\u9988\u4e2d\u5b66\u4e60\u63a8\u7406\u3002\u901a\u8fc7\u7ed3\u5408\u6bd4\u503c\u504f\u597d\u4f18\u5316 (ORPO)\uff0c\u6211\u4eec\u4f7f\u7528\u81ea\u8eab\u751f\u6210\u7684\u6b63\u8d1f\u4fe1\u53f7\u5bf9 SLM \u8fdb\u884c\u5fae\u8c03\u548c\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u91c7\u6837\u7684\u63a8\u7406\u6a21\u62df\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u5956\u52b1\u7684\u8fc7\u7a0b\u76d1\u7763\u4ee5\u8fdb\u884c\u504f\u597d\u5bf9\u9f50\u3002\u4e0e\u76d1\u7763\u5fae\u8c03 (SFT) \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 GSM8K \u4e0a\u5c06 Gemma-2B \u7684\u6027\u80fd\u63d0\u9ad8\u4e86 12.43\uff08Acc\uff09\uff0c\u5728 MBPP \u4e0a\u63d0\u9ad8\u4e86 3.95\uff08Pass@1\uff09\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fd8\u5728 MMLU_Math \u548c HumanEval \u4e0a\u5c55\u793a\u4e86\u51fa\u8272\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "author": "Kaiyuan Chen et.al.", "authors": "Kaiyuan Chen, Jin Wang, Xuejie Zhang", "id": "2412.08393v1", "paper_url": "http://arxiv.org/abs/2412.08393v1", "repo": "null"}}