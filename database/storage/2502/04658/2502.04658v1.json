{"2502.04658": {"publish_time": "2025-02-07", "title": "Shifting Attention to You: Personalized Brain-Inspired AI Models", "paper_summary": "The integration of human and artificial intelligence represents a scientific\nopportunity to advance our understanding of information processing, as each\nsystem offers unique computational insights that can enhance and inform the\nother. The synthesis of human cognitive principles with artificial intelligence\nhas the potential to produce more interpretable and functionally aligned\ncomputational models, while simultaneously providing a formal framework for\ninvestigating the neural mechanisms underlying perception, learning, and\ndecision-making through systematic model comparisons and representational\nanalyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive\nprocesses. We took a stepwise approach, fine-tuning the Contrastive\nLanguage-Image Pre-training (CLIP) model with large-scale behavioral decisions,\ngroup-level neural data, and finally, participant-level neural data within a\nbroader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human\nsimilarity judgments while indirectly aligning it with dynamic representations\ncaptured via MEG. To further gain mechanistic insights into the temporal\nevolution of cognitive processes, we introduced a model specifically fine-tuned\non millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in\nenhanced temporal alignment with human neural processing while still showing\nimprovement on behavioral alignment. Finally, we trained individualized models\non participant-specific neural data, effectively capturing individualized\nneural dynamics and highlighting the potential for personalized AI systems.\nThese personalized systems have far-reaching implications for the fields of\nmedicine, cognitive research, human-computer interfaces, and AI development.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u8207\u4eba\u5de5\u667a\u6167\u7684\u6574\u5408\u4ee3\u8868\u4e86\u4e00\u9805\u79d1\u5b78\u6a5f\u6703\uff0c\u53ef\u4ee5\u589e\u9032\u6211\u5011\u5c0d\u8cc7\u8a0a\u8655\u7406\u7684\u7406\u89e3\uff0c\u56e0\u70ba\u6bcf\u500b\u7cfb\u7d71\u90fd\u63d0\u4f9b\u7368\u7279\u7684\u904b\u7b97\u898b\u89e3\uff0c\u53ef\u4ee5\u589e\u5f37\u4e26\u544a\u77e5\u53e6\u4e00\u500b\u7cfb\u7d71\u3002\u4eba\u985e\u8a8d\u77e5\u539f\u5247\u8207\u4eba\u5de5\u667a\u6167\u7684\u7d9c\u5408\u5177\u6709\u7522\u751f\u66f4\u5177\u53ef\u89e3\u91cb\u6027\u548c\u529f\u80fd\u6027\u5c0d\u9f4a\u904b\u7b97\u6a21\u578b\u7684\u6f5b\u529b\uff0c\u540c\u6642\u70ba\u900f\u904e\u7cfb\u7d71\u6027\u6a21\u578b\u6bd4\u8f03\u548c\u8868\u5fb5\u5206\u6790\u4f86\u8abf\u67e5\u77e5\u89ba\u3001\u5b78\u7fd2\u548c\u6c7a\u7b56\u80cc\u5f8c\u7684\u985e\u795e\u7d93\u6a5f\u5236\u63d0\u4f9b\u4e00\u500b\u6b63\u5f0f\u67b6\u69cb\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u500b\u4eba\u5316\u7684\u5927\u8166\u555f\u767c\u6a21\u578b\uff0c\u6574\u5408\u4e86\u4eba\u985e\u884c\u70ba\u5d4c\u5165\u548c\u795e\u7d93\u8cc7\u6599\uff0c\u4ee5\u8207\u8a8d\u77e5\u904e\u7a0b\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u5011\u63a1\u53d6\u9010\u6b65\u65b9\u6cd5\uff0c\u5fae\u8abf\u5c0d\u6bd4\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u6a21\u578b\uff0c\u4e26\u4f7f\u7528\u5927\u898f\u6a21\u884c\u70ba\u6c7a\u7b56\u3001\u7fa4\u7d44\u5c64\u7d1a\u795e\u7d93\u8cc7\u6599\uff0c\u6700\u5f8c\u5728\u6211\u5011\u547d\u540d\u70ba CLIP \u4eba\u985e\u70ba\u57fa\u790e\u5206\u6790 (CLIP-HBA) \u7684\u66f4\u5ee3\u6cdb\u67b6\u69cb\u4e2d\u4f7f\u7528\u53c3\u8207\u8005\u5c64\u7d1a\u795e\u7d93\u8cc7\u6599\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u884c\u70ba\u8cc7\u6599\u4e0a\u9032\u884c\u5fae\u8abf\u589e\u5f37\u4e86\u5176\u9810\u6e2c\u4eba\u985e\u76f8\u4f3c\u6027\u5224\u65b7\u7684\u80fd\u529b\uff0c\u540c\u6642\u9593\u63a5\u5730\u5c07\u5176\u8207\u900f\u904e MEG \u64f7\u53d6\u7684\u52d5\u614b\u8868\u5fb5\u5c0d\u9f4a\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u7372\u5f97\u5c0d\u8a8d\u77e5\u904e\u7a0b\u6642\u9593\u6f14\u5316\u7684\u6a5f\u5236\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5c08\u9580\u91dd\u5c0d\u6beb\u79d2\u7d1a MEG \u795e\u7d93\u52d5\u614b\u9032\u884c\u5fae\u8abf\u7684\u6a21\u578b (CLIP-HBA-MEG)\u3002\u6b64\u6a21\u578b\u589e\u5f37\u4e86\u8207\u4eba\u985e\u795e\u7d93\u8655\u7406\u7684\u6642\u9593\u5c0d\u9f4a\uff0c\u540c\u6642\u4ecd\u986f\u793a\u51fa\u884c\u70ba\u5c0d\u9f4a\u7684\u6539\u5584\u3002\u6700\u5f8c\uff0c\u6211\u5011\u91dd\u5c0d\u53c3\u8207\u8005\u7279\u5b9a\u7684\u795e\u7d93\u8cc7\u6599\u8a13\u7df4\u4e86\u500b\u5225\u5316\u6a21\u578b\uff0c\u6709\u6548\u64f7\u53d6\u500b\u5225\u5316\u7684\u795e\u7d93\u52d5\u614b\uff0c\u4e26\u7a81\u986f\u4e86\u500b\u4eba\u5316 AI \u7cfb\u7d71\u7684\u6f5b\u529b\u3002\u9019\u4e9b\u500b\u4eba\u5316\u7cfb\u7d71\u5c0d\u91ab\u5b78\u3001\u8a8d\u77e5\u7814\u7a76\u3001\u4eba\u6a5f\u4ecb\u9762\u548c AI \u958b\u767c\u9818\u57df\u5177\u6709\u6df1\u9060\u7684\u5f71\u97ff\u3002</paragraph>", "author": "Stephen Chong Zhao et.al.", "authors": "Stephen Chong Zhao, Yang Hu, Jason Lee, Andrew Bender, Trisha Mazumdar, Mark Wallace, David A. Tovar", "id": "2502.04658v1", "paper_url": "http://arxiv.org/abs/2502.04658v1", "repo": "null"}}