{"2411.02310": {"publish_time": "2024-11-04", "title": "MdEval: Massively Multilingual Code Debugging", "paper_summary": "Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u76f4\u63a5\u6839\u64da\u6709\u554f\u984c\u7684\u7a0b\u5f0f\u78bc\u7247\u6bb5\u7522\u751f\u6b63\u78ba\u7684\u7a0b\u5f0f\u78bc\uff0c\u5728\u7a0b\u5f0f\u78bc\u9664\u932f\u4e0a\u53d6\u5f97\u986f\u8457\u7684\u9032\u5c55\u3002\u7a0b\u5f0f\u78bc\u57fa\u6e96\uff0c\u901a\u5e38\u5305\u542b\u6709\u554f\u984c\u7684\u7a0b\u5f0f\u78bc\u7247\u6bb5\u53ca\u5176\u76f8\u95dc\u7684\u6e2c\u8a66\u6848\u4f8b\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u7684\u9664\u932f\u80fd\u529b\u3002\u7136\u800c\uff0c\u8a31\u591a\u73fe\u6709\u7684\u57fa\u6e96\u4e3b\u8981\u5c08\u6ce8\u65bc Python\uff0c\u4e14\u5728\u8a9e\u8a00\u591a\u6a23\u6027\u65b9\u9762\u901a\u5e38\u53d7\u5230\u9650\u5236\uff08\u4f8b\u5982 DebugBench \u548c DebugEval\uff09\u3002\u70ba\u4e86\u900f\u904e LLM \u63a8\u52d5\u591a\u8a9e\u8a00\u9664\u932f\u9818\u57df\uff0c\u6211\u5011\u63d0\u51fa\u7b2c\u4e00\u500b\u5927\u898f\u6a21\u591a\u8a9e\u8a00\u9664\u932f\u57fa\u6e96\uff0c\u5176\u4e2d\u5305\u542b 18 \u7a2e\u7a0b\u5f0f\u8a9e\u8a00\u7684 3.6K \u500b\u6e2c\u8a66\u7bc4\u4f8b\uff0c\u6db5\u84cb\u81ea\u52d5\u7a0b\u5f0f\u4fee\u5fa9 (APR) \u4efb\u52d9\u3001\u7a0b\u5f0f\u78bc\u6aa2\u95b1 (CR) \u4efb\u52d9\u548c\u932f\u8aa4\u8b58\u5225 (BI) \u4efb\u52d9\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u5c07\u932f\u8aa4\u6ce8\u5165\u6b63\u78ba\u7684\u591a\u8a9e\u8a00\u67e5\u8a62\u548c\u89e3\u6c7a\u65b9\u6848 (xDebugGen) \u4e2d\uff0c\u4f86\u5c0e\u5165\u9664\u932f\u6307\u4ee4\u8a9e\u6599\u5eab MDEVAL-INSTRUCT\u3002\u6b64\u5916\uff0c\u4e00\u500b\u591a\u8a9e\u8a00\u9664\u932f\u5668 xDebugCoder \u8a13\u7df4\u65bc MDEVAL-INSTRUCT \u4e0a\uff0c\u4f5c\u70ba\u4e00\u500b\u5f37\u5927\u7684\u57fa\u6e96\uff0c\u7279\u5225\u7528\u65bc\u8655\u7406\u5404\u7a2e\u7a0b\u5f0f\u8a9e\u8a00\u7684\u932f\u8aa4\uff08\u4f8b\u5982 Rust \u8a9e\u8a00\u4e2d\u7684\u300c\u7f3a\u5c11 Mut\u300d\u548c C \u8a9e\u8a00\u4e2d\u7684\u300c\u8aa4\u7528\u5de8\u96c6\u5b9a\u7fa9\u300d\uff09\u3002\u6211\u5011\u5728 MDEVAL \u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0c\u958b\u6e90\u6a21\u578b\u548c\u5c01\u9589\u539f\u59cb\u78bc LLM\uff08\u4f8b\u5982 GPT \u548c Claude \u7cfb\u5217\uff09\u4e4b\u9593\u5b58\u5728\u986f\u8457\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u7a81\u986f\u51fa\u591a\u8a9e\u8a00\u7a0b\u5f0f\u78bc\u9664\u932f\u60c5\u5883\u4e2d\u4ecd\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002", "author": "Shukai Liu et.al.", "authors": "Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li", "id": "2411.02310v1", "paper_url": "http://arxiv.org/abs/2411.02310v1", "repo": "null"}}