{"2407.11798": {"publish_time": "2024-07-16", "title": "PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation", "paper_summary": "Inference of Large Language Models (LLMs) across computer clusters has become\na focal point of research in recent times, with many acceleration techniques\ntaking inspiration from CPU speculative execution. These techniques reduce\nbottlenecks associated with memory bandwidth, but also increase end-to-end\nlatency per inference run, requiring high speculation acceptance rates to\nimprove performance. Combined with a variable rate of acceptance across tasks,\nspeculative inference techniques can result in reduced performance.\nAdditionally, pipeline-parallel designs require many user requests to maintain\nmaximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative\nacceleration technique to reduce inter-token latency and improve system\nutilization for single-request scenarios while also improving tolerance to low\nspeculation acceptance rates and low-bandwidth interconnects. PipeInfer\nexhibits up to a 2.15$\\times$ improvement in generation speed over standard\nspeculative inference. PipeInfer achieves its improvement through Continuous\nAsynchronous Speculation and Early Inference Cancellation, the former improving\nlatency and generation speed by running single-token inference simultaneously\nwith several speculative runs, while the latter improves speed and latency by\nskipping the computation of invalidated runs, even in the middle of inference.", "paper_summary_zh": "\u8fd1\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u96fb\u8166\u53e2\u96c6\u4e2d\u7684\u63a8\u8ad6\u5df2\u6210\u70ba\u7814\u7a76\u7684\u91cd\u9ede\uff0c\u8a31\u591a\u52a0\u901f\u6280\u8853\u5f9e CPU \u63a8\u6e2c\u57f7\u884c\u4e2d\u6c72\u53d6\u9748\u611f\u3002\u9019\u4e9b\u6280\u8853\u6e1b\u5c11\u4e86\u8207\u8a18\u61b6\u9ad4\u983b\u5bec\u76f8\u95dc\u7684\u74f6\u9838\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u6bcf\u500b\u63a8\u8ad6\u904b\u884c\u7684\u7aef\u5230\u7aef\u5ef6\u9072\uff0c\u9700\u8981\u5f88\u9ad8\u7684\u63a8\u6e2c\u63a5\u53d7\u7387\u624d\u80fd\u63d0\u5347\u6548\u80fd\u3002\u7d50\u5408\u8de8\u4efb\u52d9\u7684\u53ef\u8b8a\u63a5\u53d7\u7387\uff0c\u63a8\u6e2c\u63a8\u8ad6\u6280\u8853\u53ef\u80fd\u6703\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7ba1\u7dda\u5e73\u884c\u8a2d\u8a08\u9700\u8981\u8a31\u591a\u4f7f\u7528\u8005\u8981\u6c42\u624d\u80fd\u7dad\u6301\u6700\u5927\u7684\u4f7f\u7528\u7387\u3002\u4f5c\u70ba\u88dc\u6551\u63aa\u65bd\uff0c\u6211\u5011\u63d0\u51fa PipeInfer\uff0c\u4e00\u7a2e\u7ba1\u7dda\u63a8\u6e2c\u52a0\u901f\u6280\u8853\uff0c\u7528\u65bc\u6e1b\u5c11\u4ee3\u5e63\u9593\u5ef6\u9072\u4e26\u6539\u5584\u55ae\u4e00\u8981\u6c42\u5834\u666f\u7684\u7cfb\u7d71\u4f7f\u7528\u7387\uff0c\u540c\u6642\u4e5f\u63d0\u9ad8\u5c0d\u4f4e\u63a8\u6e2c\u63a5\u53d7\u7387\u548c\u4f4e\u983b\u5bec\u4e92\u9023\u7684\u5bb9\u5fcd\u5ea6\u3002\u8207\u6a19\u6e96\u63a8\u6e2c\u63a8\u8ad6\u76f8\u6bd4\uff0cPipeInfer \u5728\u7522\u751f\u901f\u5ea6\u4e0a\u5c55\u73fe\u51fa\u9ad8\u9054 2.15 \u500d\u7684\u9032\u6b65\u3002PipeInfer \u900f\u904e\u9023\u7e8c\u975e\u540c\u6b65\u63a8\u6e2c\u548c\u65e9\u671f\u63a8\u8ad6\u53d6\u6d88\u4f86\u9054\u6210\u9032\u6b65\uff0c\u524d\u8005\u900f\u904e\u540c\u6642\u57f7\u884c\u55ae\u4e00\u4ee3\u5e63\u63a8\u8ad6\u548c\u591a\u500b\u63a8\u6e2c\u904b\u884c\u4f86\u6539\u5584\u5ef6\u9072\u548c\u7522\u751f\u901f\u5ea6\uff0c\u800c\u5f8c\u8005\u900f\u904e\u7565\u904e\u7121\u6548\u904b\u884c\u7684\u8a08\u7b97\uff08\u5373\u4f7f\u662f\u5728\u63a8\u8ad6\u904e\u7a0b\u4e2d\uff09\u4f86\u6539\u5584\u901f\u5ea6\u548c\u5ef6\u9072\u3002", "author": "Branden Butler et.al.", "authors": "Branden Butler, Sixing Yu, Arya Mazaheri, Ali Jannesari", "id": "2407.11798v1", "paper_url": "http://arxiv.org/abs/2407.11798v1", "repo": "null"}}