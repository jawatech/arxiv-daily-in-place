{"2409.19993": {"publish_time": "2024-09-30", "title": "Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges", "paper_summary": "The advancement of Large Language Models (LLMs) has significantly impacted\nvarious domains, including Web search, healthcare, and software development.\nHowever, as these models scale, they become more vulnerable to cybersecurity\nrisks, particularly backdoor attacks. By exploiting the potent memorization\ncapacity of LLMs, adversaries can easily inject backdoors into LLMs by\nmanipulating a small portion of training data, leading to malicious behaviors\nin downstream applications whenever the hidden backdoor is activated by the\npre-defined triggers. Moreover, emerging learning paradigms like instruction\ntuning and reinforcement learning from human feedback (RLHF) exacerbate these\nrisks as they rely heavily on crowdsourced data and human feedback, which are\nnot fully controlled. In this paper, we present a comprehensive survey of\nemerging backdoor threats to LLMs that appear during LLM development or\ninference, and cover recent advancement in both defense and detection\nstrategies for mitigating backdoor threats to LLMs. We also outline key\nchallenges in addressing these threats, highlighting areas for future research.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u6b65\u986f\u8457\u5f71\u97ff\u4e86\u5404\u7a2e\u9818\u57df\uff0c\u5305\u62ec\u7db2\u8def\u641c\u5c0b\u3001\u91ab\u7642\u4fdd\u5065\u548c\u8edf\u9ad4\u958b\u767c\u3002\u7136\u800c\uff0c\u96a8\u8457\u9019\u4e9b\u6a21\u578b\u7684\u64f4\u5c55\uff0c\u5b83\u5011\u66f4\u5bb9\u6613\u53d7\u5230\u7db2\u8def\u5b89\u5168\u98a8\u96aa\u7684\u5f71\u97ff\uff0c\u7279\u5225\u662f\u5f8c\u9580\u653b\u64ca\u3002\u900f\u904e\u5229\u7528 LLM \u5f37\u5927\u7684\u8a18\u61b6\u80fd\u529b\uff0c\u5c0d\u624b\u53ef\u4ee5\u900f\u904e\u64cd\u4f5c\u4e00\u5c0f\u90e8\u5206\u8a13\u7df4\u8cc7\u6599\uff0c\u8f15\u9b06\u5730\u5c07\u5f8c\u9580\u6ce8\u5165 LLM\uff0c\u5c0e\u81f4\u60e1\u610f\u884c\u70ba\u51fa\u73fe\u5728\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u4e2d\uff0c\u53ea\u8981\u9810\u5148\u5b9a\u7fa9\u7684\u89f8\u767c\u5668\u555f\u52d5\u4e86\u96b1\u85cf\u7684\u5f8c\u9580\u3002\u6b64\u5916\uff0c\u65b0\u8208\u7684\u5b78\u7fd2\u7bc4\u4f8b\uff0c\u4f8b\u5982\u6307\u4ee4\u5fae\u8abf\u548c\u4f86\u81ea\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u6703\u52a0\u5287\u9019\u4e9b\u98a8\u96aa\uff0c\u56e0\u70ba\u5b83\u5011\u56b4\u91cd\u4f9d\u8cf4\u7fa4\u773e\u5916\u5305\u8cc7\u6599\u548c\u4eba\u985e\u56de\u994b\uff0c\u800c\u9019\u4e9b\u8cc7\u6599\u548c\u56de\u994b\u4e26\u672a\u53d7\u5230\u5b8c\u5168\u63a7\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u91dd\u5c0d LLM \u958b\u767c\u6216\u63a8\u8ad6\u671f\u9593\u51fa\u73fe\u7684\u65b0\u8208\u5f8c\u9580\u5a01\u8105\u63d0\u51fa\u5168\u9762\u7684\u8abf\u67e5\uff0c\u4e26\u6db5\u84cb\u4e86\u7de9\u89e3\u5f8c\u9580\u5a01\u8105\u7684\u9632\u79a6\u548c\u5075\u6e2c\u7b56\u7565\u7684\u6700\u65b0\u9032\u5c55\u3002\u6211\u5011\u4e5f\u6982\u8ff0\u4e86\u56e0\u61c9\u9019\u4e9b\u5a01\u8105\u7684\u4e3b\u8981\u6311\u6230\uff0c\u4e26\u91cd\u9ede\u8aaa\u660e\u672a\u4f86\u7814\u7a76\u9818\u57df\u3002", "author": "Qin Liu et.al.", "authors": "Qin Liu, Wenjie Mo, Terry Tong, Jiashu Xu, Fei Wang, Chaowei Xiao, Muhao Chen", "id": "2409.19993v1", "paper_url": "http://arxiv.org/abs/2409.19993v1", "repo": "null"}}