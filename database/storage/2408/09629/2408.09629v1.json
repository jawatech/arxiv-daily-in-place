{"2408.09629": {"publish_time": "2024-08-19", "title": "A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic Text Classification", "paper_summary": "Transformer models have achieved state-of-the-art results, with Large\nLanguage Models (LLMs), an evolution of first-generation transformers (1stTR),\nbeing considered the cutting edge in several NLP tasks. However, the literature\nhas yet to conclusively demonstrate that LLMs consistently outperform 1stTRs\nacross all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and\nBART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis\ndatasets. The results indicate that open LLMs may moderately outperform or\nmatch 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this\nsubstantial cost for only moderate gains, the practical applicability of these\nmodels in cost-sensitive scenarios is questionable. In this context, a\nconfidence-based strategy that seamlessly integrates 1stTRs with open LLMs\nbased on prediction certainty is proposed. High-confidence documents are\nclassified by the more cost-effective 1stTRs, while uncertain cases are handled\nby LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned\nversions. Experiments in sentiment analysis demonstrate that our solution not\nonly outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely\nwith fine-tuned LLMs at a fraction of the cost.", "paper_summary_zh": "Transformer\u6a21\u578b\u5df2\u53d6\u5f97\u6700\u5148\u9032\u7684\u6210\u679c\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u7b2c\u4e00\u4ee3Transformer (1stTR) \u7684\u6f14\u9032\uff0c\u88ab\u8a8d\u70ba\u662f\u591a\u9805 NLP \u4efb\u52d9\u7684\u5c16\u7aef\u6280\u8853\u3002\u7136\u800c\uff0c\u6587\u737b\u5c1a\u672a\u78ba\u947f\u5730\u8b49\u660e\uff0cLLM \u5728\u6240\u6709 NLP \u4efb\u52d9\u4e2d\u90fd\u6301\u7e8c\u512a\u65bc 1stTR\u3002\u672c\u7814\u7a76\u6bd4\u8f03\u4e86\u4e09\u500b 1stTR\uff08BERT\u3001RoBERTa \u548c BART\uff09\u8207\u5169\u500b\u958b\u653e\u5f0f LLM\uff08Llama 2 \u548c Bloom\uff09\uff0c\u6db5\u84cb 11 \u500b\u60c5\u7dd2\u5206\u6790\u8cc7\u6599\u96c6\u3002\u7d50\u679c\u986f\u793a\uff0c\u958b\u653e\u5f0f LLM \u5728 11 \u500b\u8cc7\u6599\u96c6\u4e2d\u6709 8 \u500b\u8cc7\u6599\u96c6\u53ef\u80fd\u9069\u5ea6\u512a\u65bc\u6216\u5339\u914d 1stTR\uff0c\u4f46\u50c5\u9650\u65bc\u5fae\u8abf\u6642\u3002\u8003\u91cf\u5230\u5fae\u8abf\u50c5\u80fd\u5e36\u4f86\u9069\u5ea6\u7684\u589e\u76ca\uff0c\u4f46\u6210\u672c\u537b\u76f8\u7576\u9ad8\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u6210\u672c\u654f\u611f\u5834\u666f\u4e2d\u7684\u5be6\u969b\u9069\u7528\u6027\u4ee4\u4eba\u8cea\u7591\u3002\u5728\u6b64\u8108\u7d61\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u4fe1\u5fc3\u7684\u7b56\u7565\uff0c\u53ef\u6839\u64da\u9810\u6e2c\u78ba\u5b9a\u6027\u5c07 1stTR \u8207\u958b\u653e\u5f0f LLM \u7121\u7e2b\u6574\u5408\u3002\u7531\u8f03\u5177\u6210\u672c\u6548\u76ca\u7684 1stTR \u5206\u985e\u9ad8\u4fe1\u5fc3\u6587\u4ef6\uff0c\u800c\u7531 LLM \u5728\u96f6\u6b21\u5b78\u7fd2\u6216\u5c11\u6b21\u5b78\u7fd2\u6a21\u5f0f\u4e2d\u8655\u7406\u4e0d\u78ba\u5b9a\u7684\u6848\u4f8b\uff0c\u6210\u672c\u9060\u4f4e\u65bc\u5fae\u8abf\u7248\u672c\u3002\u60c5\u7dd2\u5206\u6790\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u89e3\u6c7a\u65b9\u6848\u4e0d\u50c5\u512a\u65bc 1stTR\u3001\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u6b21\u5b78\u7fd2 LLM\uff0c\u800c\u4e14\u5728\u6210\u672c\u50c5\u70ba\u5fae\u8abf LLM \u7684\u4e00\u5c0f\u90e8\u5206\u4e0b\uff0c\u4e5f\u80fd\u8207\u5fae\u8abf LLM \u7dca\u5bc6\u7af6\u722d\u3002", "author": "Claudio M. V. de Andrade et.al.", "authors": "Claudio M. V. de Andrade, Washington Cunha, Davi Reis, Adriana Silvina Pagano, Leonardo Rocha, Marcos Andr\u00e9 Gon\u00e7alves", "id": "2408.09629v1", "paper_url": "http://arxiv.org/abs/2408.09629v1", "repo": "null"}}