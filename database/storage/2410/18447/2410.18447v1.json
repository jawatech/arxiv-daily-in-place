{"2410.18447": {"publish_time": "2024-10-24", "title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis", "paper_summary": "Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities.", "paper_summary_zh": "\u76e3\u7763\u5fae\u8abf (SFT) \u662f\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5de5\u5177\u547c\u53eb\u529f\u80fd\u7684\u5e38\u898b\u65b9\u6cd5\uff0c\u8a13\u7df4\u8cc7\u6599\u901a\u5e38\u662f\u5408\u6210\u8cc7\u6599\u3002\u76ee\u524d\u7684\u8cc7\u6599\u5408\u6210\u6d41\u7a0b\u901a\u5e38\u6d89\u53ca\u62bd\u6a23\u4e00\u7d44\u5de5\u5177\u3001\u6839\u64da\u9019\u4e9b\u5de5\u5177\u5236\u5b9a\u9700\u6c42\uff0c\u4e26\u7522\u751f\u547c\u53eb\u9673\u8ff0\u3002\u7136\u800c\uff0c\u96a8\u6a5f\u62bd\u6a23\u7684\u5de5\u5177\u7f3a\u4e4f\u95dc\u806f\u6027\uff0c\u4f7f\u5f97\u5b83\u5011\u96e3\u4ee5\u7d44\u5408\uff0c\u5f9e\u800c\u964d\u4f4e\u8cc7\u6599\u7684\u591a\u6a23\u6027\u3002\u6b64\u5916\uff0c\u76ee\u524d\u7684\u5de5\u4f5c\u5ffd\u7565\u4e86\u5c0d\u8a71\u56de\u5408\u4e4b\u9593\u7684\u9023\u8cab\u6027\uff0c\u5c0e\u81f4\u5408\u6210\u8cc7\u6599\u8207\u73fe\u5be6\u4e16\u754c\u5834\u666f\u4e4b\u9593\u5b58\u5728\u5dee\u8ddd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u5716\u5f62\u7684\u62bd\u6a23\u7b56\u7565\u4f86\u62bd\u53d6\u66f4\u591a\u76f8\u95dc\u7684\u5de5\u5177\u7d44\u5408\uff0c\u4ee5\u53ca\u4e00\u500b\u8a08\u756b\u751f\u6210\u7b56\u7565\u4f86\u5efa\u7acb\u8a08\u756b\uff0c\u4ee5\u5f15\u5c0e\u9023\u8cab\u5c0d\u8a71\u7684\u5408\u6210\u3002\u6211\u5011\u6574\u5408\u9019\u5169\u7a2e\u7b56\u7565\uff0c\u4e26\u4f7f\u591a\u500b\u4ee3\u7406\u80fd\u5920\u4e92\u52d5\u5730\u5408\u6210\u5c0d\u8a71\u8cc7\u6599\uff0c\u5f9e\u800c\u7522\u751f\u6211\u5011\u7684\u5de5\u5177\u547c\u53eb\u8cc7\u6599\u5408\u6210\u7ba1\u7dda ToolFlow\u3002\u8cc7\u6599\u54c1\u8cea\u8a55\u4f30\u8b49\u660e\u4e86\u6211\u5011\u5408\u6210\u5c0d\u8a71\u7684\u81ea\u7136\u6027\u548c\u9023\u8cab\u6027\u6709\u4e86\u6539\u9032\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4f7f\u7528 ToolFlow \u751f\u6210\u7684 8,000 \u500b\u5408\u6210\u5c0d\u8a71\u5728 LLaMA-3.1-8B \u4e0a\u61c9\u7528 SFT\u3002\u7d50\u679c\u8868\u660e\uff0c\u8a72\u6a21\u578b\u5be6\u73fe\u4e86\u8207 GPT-4 \u76f8\u7576\u751a\u81f3\u8d85\u8d8a GPT-4 \u7684\u5de5\u5177\u547c\u53eb\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u6301\u5f37\u5927\u7684\u901a\u7528\u80fd\u529b\u3002", "author": "Zezhong Wang et.al.", "authors": "Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong", "id": "2410.18447v1", "paper_url": "http://arxiv.org/abs/2410.18447v1", "repo": "null"}}