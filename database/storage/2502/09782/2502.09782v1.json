{"2502.09782": {"publish_time": "2025-02-13", "title": "Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models", "paper_summary": "The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.", "paper_summary_zh": "\u96a8\u8457\u65e5\u5e38\u88dd\u7f6e\u4e2d\u9ea5\u514b\u98a8\u7684\u666e\u53ca\u7387\u8d8a\u4f86\u8d8a\u9ad8\uff0c\u4ee5\u53ca\u5c0d\u7dda\u4e0a\u670d\u52d9\u7684\u4f9d\u8cf4\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u91dd\u5c0d\u9375\u76e4\u7684\u8072\u5b78\u5074\u4fe1\u9053\u653b\u64ca (ASCA) \u98a8\u96aa\u4e5f\u96a8\u4e4b\u64f4\u5927\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u6df1\u5ea6\u5b78\u7fd2\u6280\u8853\uff0c\u7279\u5225\u662f\u8996\u89baTransformer (VT) \u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u589e\u5f37\u6b64\u985e\u653b\u64ca\u7684\u6709\u6548\u6027\u548c\u9069\u7528\u6027\u3002\u6211\u5011\u63d0\u51fa\u5c0d\u5148\u524d\u7814\u7a76\u7684\u91cd\u5927\u6539\u9032\uff0c\u5176\u4e2d CoAtNet \u6a21\u578b\u9054\u5230\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u8207\u5148\u524d\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684 CoAtNet \u5c0d\u65bc\u900f\u904e\u667a\u6167\u578b\u624b\u6a5f (Phone) \u8a18\u9304\u7684\u6309\u9375\u986f\u793a\u6709 5.0% \u7684\u6539\u9032\uff0c\u800c\u900f\u904e Zoom \u8a18\u9304\u7684\u6309\u9375\u5247\u6709 5.9% \u7684\u6539\u9032\u3002\u6211\u5011\u4e5f\u8a55\u4f30\u4e86Transformer\u67b6\u69cb\u548c\u8a9e\u8a00\u6a21\u578b\uff0c\u5176\u4e2d\u6700\u4f73 VT \u6a21\u578b\u7b26\u5408 CoAtNet \u7684\u6548\u80fd\u3002\u4e00\u9805\u95dc\u9375\u9032\u5c55\u662f\u5f15\u5165\u4e86\u9069\u7528\u65bc\u771f\u5be6\u4e16\u754c\u5834\u666f\u7684\u96dc\u8a0a\u7de9\u89e3\u65b9\u6cd5\u3002\u900f\u904e\u4f7f\u7528 LLM \u9032\u884c\u8108\u7d61\u7406\u89e3\uff0c\u6211\u5011\u53ef\u4ee5\u5075\u6e2c\u4e26\u4fee\u6b63\u6709\u96dc\u8a0a\u74b0\u5883\u4e2d\u7684\u932f\u8aa4\u6309\u9375\uff0c\u9032\u800c\u589e\u5f37 ASCA \u6548\u80fd\u3002\u6b64\u5916\uff0c\u7d93\u904e\u5fae\u8abf\u7684\u8f15\u91cf\u7d1a\u8a9e\u8a00\u6a21\u578b\u642d\u914d\u4f4e\u968e\u9069\u61c9 (LoRA) \u53ef\u63d0\u4f9b\u8207\u5177\u5099\u591a 67 \u500d\u53c3\u6578\u7684\u91cd\u91cf\u7d1a\u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\u3002VT \u548c LLM \u7684\u6574\u5408\u6539\u9032\u4e86 ASCA \u7de9\u89e3\u7684\u5be6\u7528\u9069\u7528\u6027\uff0c\u6a19\u8a8c\u8457\u9996\u6b21\u4f7f\u7528\u9019\u4e9b\u6280\u8853\u4f86\u89e3\u6c7a\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u7684 ASCA \u548c\u932f\u8aa4\u4fee\u6b63\u3002", "author": "Jin Hyun Park et.al.", "authors": "Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai", "id": "2502.09782v1", "paper_url": "http://arxiv.org/abs/2502.09782v1", "repo": "null"}}