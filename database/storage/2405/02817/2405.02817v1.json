{"2405.02817": {"publish_time": "2024-05-05", "title": "HuixiangDou-CR: Coreference Resolution in Group Chats", "paper_summary": "How to eliminate pronominal reference in group chats? In this work, we have\npreprocessed 58k authentic chat data and manually annotated 2.3k questions. The\nreliability of this annotation was confirmed by the scaling law. After this, we\nconducted fine-tuning on Qwen models, ranging from 0.5B to 32B parameters. The\noptimal version improved 29.07 in F1 score. This confirms the viability of\nfine-tuning Large Language Model (LLM) for downstream Natural Language\nProcessing (NLP) tasks. Our contributions are: 1) Created Supervised\nFine-Tuning (SFT) training data in alpaca format, along with a set of Low-Rank\nAdaptation (LoRA) weights, and 2) Developed a method for acquiring high-quality\ndata leveraging scaling law principle. The script, raw data with alpaca format\nand experiments track are open-sourced on Github\nhttps://github.com/InternLM/HuixiangDou/tree/main/web/tools, HuggingFace\nhttps://huggingface.co/tpoisonooo and WandB\nhttps://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo . The\nprivacy of the data involved has been authorized by users.", "paper_summary_zh": "", "author": "Huanjun Kong et.al.", "authors": "Huanjun Kong", "id": "2405.02817v1", "paper_url": "http://arxiv.org/abs/2405.02817v1", "repo": "https://github.com/internlm/huixiangdou"}}