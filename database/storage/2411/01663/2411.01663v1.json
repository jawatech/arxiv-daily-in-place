{"2411.01663": {"publish_time": "2024-11-03", "title": "Unlocking the Theory Behind Scaling 1-Bit Neural Networks", "paper_summary": "Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an\nimpressive combination of efficiency and performance that rivals traditional\nLLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the\nperformance of these 1-bit LLMs progressively improves as the number of\nparameters increases, hinting at the potential existence of a Scaling Law for\n1-bit Neural Networks. In this paper, we present the first theoretical result\nthat rigorously establishes this scaling law for 1-bit models. We prove that,\ndespite the constraint of weights restricted to $\\{-1, +1\\}$, the dynamics of\nmodel training inevitably align with kernel behavior as the network width\ngrows. This theoretical breakthrough guarantees convergence of the 1-bit model\nto an arbitrarily small loss as width increases. Furthermore, we introduce the\nconcept of the generalization difference, defined as the gap between the\noutputs of 1-bit networks and their full-precision counterparts, and\ndemonstrate that this difference maintains a negligible level as network width\nscales. Building on the work of Kaplan et al. (2020), we conclude by examining\nhow the training loss scales as a power-law function of the model size, dataset\nsize, and computational resources utilized for training. Our findings\nunderscore the promising potential of scaling 1-bit neural networks, suggesting\nthat int1 could become the standard in future neural network precision.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c1 \u4f4d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6d6e\u73fe\uff0c\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u7387\u548c\u6548\u80fd\u7d50\u5408\uff0c\u53ef\u5ab2\u7f8e\u50b3\u7d71 LLM\u3002\u738b\u7b49\u4eba (2023)\uff1b\u99ac\u7b49\u4eba (2024) \u7684\u7814\u7a76\u6307\u51fa\uff0c\u9019\u4e9b 1 \u4f4d LLM \u7684\u6548\u80fd\u6703\u96a8\u8457\u53c3\u6578\u6578\u91cf\u589e\u52a0\u800c\u9010\u6b65\u63d0\u5347\uff0c\u6697\u793a 1 \u4f4d\u795e\u7d93\u7db2\u8def\u5b58\u5728\u898f\u6a21\u5f8b\u7684\u53ef\u80fd\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u7b2c\u4e00\u500b\u7406\u8ad6\u7d50\u679c\uff0c\u56b4\u8b39\u5730\u5efa\u7acb 1 \u4f4d\u6a21\u578b\u7684\u898f\u6a21\u5f8b\u3002\u6211\u5011\u8b49\u660e\uff0c\u5118\u7ba1\u6b0a\u91cd\u9650\u5236\u5728 $\\{-1, +1\\}$ \u7684\u7d04\u675f\u4e0b\uff0c\u6a21\u578b\u8a13\u7df4\u7684\u52d5\u614b\u6703\u96a8\u8457\u7db2\u8def\u5bec\u5ea6\u589e\u52a0\u800c\u4e0d\u53ef\u907f\u514d\u5730\u8207\u6838\u5fc3\u7684\u884c\u70ba\u4e00\u81f4\u3002\u9019\u500b\u7406\u8ad6\u7a81\u7834\u4fdd\u8b49 1 \u4f4d\u6a21\u578b\u7684\u6536\u6582\u6027\uff0c\u8b93\u640d\u5931\u96a8\u8457\u5bec\u5ea6\u589e\u52a0\u800c\u4efb\u610f\u5730\u8b8a\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u6cdb\u5316\u5dee\u7570\u7684\u6982\u5ff5\uff0c\u5b9a\u7fa9\u70ba 1 \u4f4d\u7db2\u8def\u548c\u5b83\u5011\u7684\u5b8c\u5168\u7cbe\u78ba\u5ea6\u5c0d\u61c9\u7269\u4e4b\u9593\u7684\u8f38\u51fa\u5dee\u8ddd\uff0c\u4e26\u8b49\u660e\u9019\u500b\u5dee\u7570\u6703\u96a8\u8457\u7db2\u8def\u5bec\u5ea6\u898f\u6a21\u800c\u7dad\u6301\u5728\u53ef\u5ffd\u7565\u7684\u5c64\u7d1a\u3002\u5efa\u7acb\u5728 Kaplan \u7b49\u4eba (2020) \u7684\u7814\u7a76\u4e0a\uff0c\u6211\u5011\u6700\u5f8c\u63a2\u8a0e\u8a13\u7df4\u640d\u5931\u5982\u4f55\u4ee5\u51aa\u5f8b\u51fd\u6578\u7684\u5f62\u5f0f\u96a8\u8457\u6a21\u578b\u5927\u5c0f\u3001\u8cc7\u6599\u96c6\u5927\u5c0f\u548c\u7528\u65bc\u8a13\u7df4\u7684\u8a08\u7b97\u8cc7\u6e90\u800c\u898f\u6a21\u5316\u3002\u6211\u5011\u7684\u767c\u73fe\u5f37\u8abf\u898f\u6a21\u5316 1 \u4f4d\u795e\u7d93\u7db2\u8def\u7684\u6f5b\u529b\uff0c\u8868\u660e int1 \u53ef\u80fd\u6703\u6210\u70ba\u672a\u4f86\u795e\u7d93\u7db2\u8def\u7cbe\u5ea6\u7684\u6a19\u6e96\u3002</paragraph>", "author": "Majid Daliri et.al.", "authors": "Majid Daliri, Zhao Song, Chiwun Yang", "id": "2411.01663v1", "paper_url": "http://arxiv.org/abs/2411.01663v1", "repo": "null"}}