{"2406.04873": {"publish_time": "2024-06-07", "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion Prior", "paper_summary": "Video-to-video synthesis models face significant challenges, such as ensuring\nconsistent character generation across frames, maintaining smooth temporal\ntransitions, and preserving quality during fast motion. The introduction of\njoint fully cross-frame self-attention mechanisms has improved character\nconsistency, but this comes at the cost of increased computational complexity.\nThis full cross-frame self-attention mechanism also incorporates redundant\ndetails and limits the number of frames that can be jointly edited due to its\ncomputational cost. Moreover, the lack of frames in cross-frame attention\nadversely affects temporal consistency and visual quality. To address these\nlimitations, we propose a new adaptive motion-guided cross-frame attention\nmechanism that drastically reduces complexity while preserving semantic details\nand temporal consistency. Specifically, we selectively incorporate the moving\nregions of successive frames in cross-frame attention and sparsely include\nstationary regions based on optical flow sampling. This technique allows for an\nincreased number of jointly edited frames without additional computational\noverhead. For longer duration of video editing, existing methods primarily\nfocus on frame interpolation or flow-warping from jointly edited keyframes,\nwhich often results in blurry frames or reduced temporal consistency. To\nimprove this, we introduce KV-caching of jointly edited frames and reuse the\nsame KV across all intermediate frames, significantly enhancing both\nintermediate frame quality and temporal consistency. Overall, our\nmotion-sampling method enables the use of around three times more keyframes\nthan existing joint editing methods while maintaining superior prediction\nquality. Ada-VE achieves up to 4x speed-up when using fully-extended\nself-attention across 40 frames for joint editing, without compromising visual\nquality or temporal consistency.", "paper_summary_zh": "\u5f71\u7247\u5230\u5f71\u7247\u7684\u5408\u6210\u6a21\u578b\u9762\u4e34\u7740\u91cd\u5927\u7684\u6311\u6218\uff0c\u4f8b\u5982\u786e\u4fdd\u6574\u4e2a\u5e27\u4e2d\u89d2\u8272\u751f\u6210\u7684\u4e00\u81f4\u6027\u3001\u7ef4\u6301\u6d41\u7545\u7684\u65f6\u95f4\u8f6c\u6362\uff0c\u4ee5\u53ca\u5728\u5feb\u901f\u79fb\u52a8\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8d28\u91cf\u3002\u5f15\u5165\u8054\u5408\u5168\u5e27\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6539\u5584\u4e86\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u4ee5\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u6027\u4e3a\u4ee3\u4ef7\u3002\u8fd9\u79cd\u5168\u5e27\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fd8\u7eb3\u5165\u4e86\u5197\u4f59\u7684\u7ec6\u8282\uff0c\u5e76\u7531\u4e8e\u5176\u8ba1\u7b97\u6210\u672c\u800c\u9650\u5236\u4e86\u53ef\u4ee5\u8054\u5408\u7f16\u8f91\u7684\u5e27\u6570\u3002\u6b64\u5916\uff0c\u8de8\u5e27\u6ce8\u610f\u529b\u4e2d\u7f3a\u5c11\u5e27\u5bf9\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u5f15\u5bfc\u7684\u8de8\u5e27\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8be5\u673a\u5236\u5927\u5e45\u964d\u4f4e\u4e86\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bed\u4e49\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u8de8\u5e27\u6ce8\u610f\u529b\u4e2d\u9009\u62e9\u6027\u5730\u7eb3\u5165\u8fde\u7eed\u5e27\u7684\u79fb\u52a8\u533a\u57df\uff0c\u5e76\u57fa\u4e8e\u5149\u6d41\u91c7\u6837\u7a00\u758f\u5730\u5305\u542b\u9759\u6b62\u533a\u57df\u3002\u6b64\u6280\u672f\u5141\u8bb8\u589e\u52a0\u8054\u5408\u7f16\u8f91\u7684\u5e27\u6570\uff0c\u800c\u4e0d\u4f1a\u589e\u52a0\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u5bf9\u4e8e\u8f83\u957f\u65f6\u95f4\u7684\u5f71\u7247\u7f16\u8f91\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4ece\u8054\u5408\u7f16\u8f91\u7684\u5173\u952e\u5e27\u8fdb\u884c\u5e27\u63d2\u503c\u6216\u6d41\u53d8\u5f62\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u6a21\u7cca\u7684\u5e27\u6216\u964d\u4f4e\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5728\u8054\u5408\u7f16\u8f91\u7684\u5e27\u4e2d\u5f15\u5165\u4e86 KV \u7f13\u5b58\uff0c\u5e76\u5728\u6240\u6709\u4e2d\u95f4\u5e27\u4e2d\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684 KV\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u4e2d\u95f4\u5e27\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u8fd0\u52a8\u91c7\u6837\u65b9\u6cd5\u80fd\u591f\u6bd4\u73b0\u6709\u7684\u8054\u5408\u7f16\u8f91\u65b9\u6cd5\u4f7f\u7528\u5927\u7ea6\u4e09\u500d\u7684\u5173\u952e\u5e27\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u9884\u6d4b\u8d28\u91cf\u3002Ada-VE \u5728\u8054\u5408\u7f16\u8f91 40 \u5e27\u65f6\u4f7f\u7528\u5b8c\u5168\u6269\u5c55\u7684\u81ea\u6ce8\u610f\u529b\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe 4 \u500d\u7684\u52a0\u901f\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u89c6\u89c9\u8d28\u91cf\u6216\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "author": "Tanvir Mahmud et.al.", "authors": "Tanvir Mahmud, Mustafa Munir, Radu Marculescu, Diana Marculescu", "id": "2406.04873v1", "paper_url": "http://arxiv.org/abs/2406.04873v1", "repo": "null"}}