{"2412.06685": {"publish_time": "2024-12-09", "title": "Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone", "paper_summary": "Recent advances in learning decision-making policies can largely be\nattributed to training expressive policy models, largely via imitation\nlearning. While imitation learning discards non-expert data, reinforcement\nlearning (RL) can still learn from suboptimal data. However, instantiating RL\ntraining of a new policy class often presents a different challenge: most deep\nRL machinery is co-developed with assumptions on the policy class and backbone,\nresulting in poor performance when the policy class changes. For instance, SAC\nutilizes a low-variance reparameterization policy gradient for Gaussian\npolicies, but this is unstable for diffusion policies and intractable for\nautoregressive categorical policies. To address this issue, we develop an\noffline RL and online fine-tuning approach called policy-agnostic RL (PA-RL)\nthat can effectively train multiple policy classes, with varying architectures\nand sizes. We build off the basic idea that a universal supervised learning\nloss can replace the policy improvement step in RL, as long as it is applied on\n\"optimized\" actions. To obtain these optimized actions, we first sample\nmultiple actions from a base policy, and run global optimization (i.e.,\nre-ranking multiple action samples using the Q-function) and local optimization\n(i.e., running gradient steps on an action sample) to maximize the critic on\nthese candidates. PA-RL enables fine-tuning diffusion and transformer policies\nwith either autoregressive tokens or continuous action outputs, at different\nsizes, entirely via actor-critic RL. Moreover, PA-RL improves the performance\nand sample-efficiency by up to 2 times compared to existing offline RL and\nonline fine-tuning methods. We show the first result that successfully\nfine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, an\nonline RL fine-tuning algorithm, improving from 40% to 70% in the real world in\n40 minutes.", "paper_summary_zh": "\u8fd1\u6765\u5728\u5b66\u4e60\u51b3\u7b56\u5236\u5b9a\u653f\u7b56\u65b9\u9762\u53d6\u5f97\u7684\u8fdb\u5c55\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u5f52\u56e0\u4e8e\u8bad\u7ec3\u8868\u73b0\u529b\u653f\u7b56\u6a21\u578b\uff0c\u4e3b\u8981\u662f\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u3002\u867d\u7136\u6a21\u4eff\u5b66\u4e60\u4f1a\u4e22\u5f03\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60 (RL) \u4ecd\u53ef\u4ece\u6b21\u4f18\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u7136\u800c\uff0c\u5b9e\u4f8b\u5316\u65b0\u653f\u7b56\u7c7b\u7684 RL \u8bad\u7ec3\u901a\u5e38\u4f1a\u5e26\u6765\u4e0d\u540c\u7684\u6311\u6218\uff1a\u5927\u591a\u6570\u6df1\u5ea6 RL \u673a\u5236\u90fd\u4e0e\u5bf9\u653f\u7b56\u7c7b\u548c\u4e3b\u5e72\u7684\u5047\u8bbe\u5171\u540c\u5f00\u53d1\uff0c\u5bfc\u81f4\u5728\u653f\u7b56\u7c7b\u53d1\u751f\u53d8\u5316\u65f6\u6027\u80fd\u4e0d\u4f73\u3002\u4f8b\u5982\uff0cSAC \u5bf9\u9ad8\u65af\u653f\u7b56\u4f7f\u7528\u4f4e\u65b9\u5dee\u91cd\u65b0\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\uff0c\u4f46\u8fd9\u5bf9\u6269\u6563\u7b56\u7565\u4e0d\u7a33\u5b9a\uff0c\u5bf9\u81ea\u56de\u5f52\u5206\u7c7b\u7b56\u7565\u6765\u8bf4\u96be\u4ee5\u5904\u7406\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a\u7b56\u7565\u4e0d\u53ef\u77e5 RL (PA-RL) \u7684\u79bb\u7ebf RL \u548c\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8bad\u7ec3\u5177\u6709\u4e0d\u540c\u67b6\u6784\u548c\u5927\u5c0f\u7684\u591a\u4e2a\u7b56\u7565\u7c7b\u3002\u6211\u4eec\u5efa\u7acb\u5728\u8fd9\u6837\u4e00\u4e2a\u57fa\u672c\u601d\u60f3\u4e4b\u4e0a\uff1a\u53ea\u8981\u5e94\u7528\u4e8e\u201c\u4f18\u5316\u201d\u64cd\u4f5c\uff0c\u901a\u7528\u76d1\u7763\u5b66\u4e60\u635f\u5931\u5c31\u53ef\u4ee5\u66ff\u4ee3 RL \u4e2d\u7684\u7b56\u7565\u6539\u8fdb\u6b65\u9aa4\u3002\u4e3a\u4e86\u83b7\u5f97\u8fd9\u4e9b\u4f18\u5316\u7684\u64cd\u4f5c\uff0c\u6211\u4eec\u9996\u5148\u4ece\u57fa\u672c\u7b56\u7565\u4e2d\u91c7\u6837\u591a\u4e2a\u64cd\u4f5c\uff0c\u5e76\u8fd0\u884c\u5168\u5c40\u4f18\u5316\uff08\u5373\uff0c\u4f7f\u7528 Q \u51fd\u6570\u5bf9\u591a\u4e2a\u64cd\u4f5c\u6837\u672c\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff09\u548c\u5c40\u90e8\u4f18\u5316\uff08\u5373\uff0c\u5bf9\u64cd\u4f5c\u6837\u672c\u8fd0\u884c\u68af\u5ea6\u6b65\u9aa4\uff09\u4ee5\u6700\u5927\u5316\u8fd9\u4e9b\u5019\u9009\u8005\u7684\u6279\u8bc4\u8005\u3002PA-RL \u542f\u7528\u5fae\u8c03\u6269\u6563\u548c\u8f6c\u6362\u5668\u7b56\u7565\uff0c\u5177\u6709\u81ea\u56de\u5f52\u6807\u8bb0\u6216\u8fde\u7eed\u64cd\u4f5c\u8f93\u51fa\uff0c\u5728\u4e0d\u540c\u5927\u5c0f\u4e0b\uff0c\u5b8c\u5168\u901a\u8fc7 actor-critic RL\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u7684\u79bb\u7ebf RL \u548c\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0cPA-RL \u5c06\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u63d0\u9ad8\u4e86 2 \u500d\u3002\u6211\u4eec\u5c55\u793a\u4e86\u7b2c\u4e00\u4e2a\u6210\u529f\u5fae\u8c03 OpenVLA \u7684\u7ed3\u679c\uff0cOpenVLA \u662f\u4e00\u79cd 7B \u901a\u624d\u673a\u5668\u4eba\u7b56\u7565\uff0c\u4f7f\u7528 Cal-QL\uff08\u4e00\u79cd\u5728\u7ebf RL \u5fae\u8c03\u7b97\u6cd5\uff09\u81ea\u4e3b\u8fd0\u884c\uff0c\u5728 40 \u5206\u949f\u5185\u4ece\u73b0\u5b9e\u4e16\u754c\u7684 40% \u63d0\u9ad8\u5230 70%\u3002", "author": "Max Sobol Mark et.al.", "authors": "Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, Aviral Kumar", "id": "2412.06685v1", "paper_url": "http://arxiv.org/abs/2412.06685v1", "repo": "null"}}