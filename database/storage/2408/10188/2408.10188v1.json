{"2408.10188": {"publish_time": "2024-08-19", "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos", "paper_summary": "Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)\nsystem that enables long-context training and inference, enabling 2M context\nlength training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster\nthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in\ntext-only settings. Moreover, it seamlessly integrates with Hugging Face\nTransformers. For model training, we propose a five-stage pipeline comprising\nalignment, pre-training, context extension, and long-short joint supervised\nfine-tuning. Regarding datasets, we meticulously construct large-scale visual\nlanguage pre-training datasets and long video instruction-following datasets to\nsupport our multi-stage training process. The full-stack solution extends the\nfeasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and\nimproves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%\naccuracy in 1400-frames video (274k context length) needle in a haystack.\nLongVILA-8B also demonstrates a consistent improvement in performance on long\nvideos within the VideoMME benchmark as the video frames increase.", "paper_summary_zh": "\u9577\u6587\u672c\u80fd\u529b\u5c0d\u65bc\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u63a8\u51fa LongVILA\uff0c\u9019\u662f\u4e00\u500b\u91dd\u5c0d\u9577\u6587\u672c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u5b8c\u6574\u89e3\u6c7a\u65b9\u6848\uff0c\u5305\u62ec\u7cfb\u7d71\u3001\u6a21\u578b\u8a13\u7df4\u548c\u8cc7\u6599\u96c6\u958b\u767c\u3002\u5728\u7cfb\u7d71\u65b9\u9762\uff0c\u6211\u5011\u5f15\u5165\u4e86\u7b2c\u4e00\u500b\u591a\u6a21\u614b\u5e8f\u5217\u5e73\u884c\u5316 (MM-SP) \u7cfb\u7d71\uff0c\u5b83\u652f\u63f4\u9577\u6587\u672c\u8a13\u7df4\u548c\u63a8\u8ad6\uff0c\u5728 256 \u500b GPU \u4e0a\u9032\u884c 2M \u6587\u672c\u9577\u5ea6\u8a13\u7df4\u3002MM-SP \u4e5f\u975e\u5e38\u6709\u6548\u7387\uff0c\u6bd4 Ring-Style \u5e8f\u5217\u5e73\u884c\u5316\u5feb 2.1 \u500d - 5.7 \u500d\uff0c\u6bd4 Megatron-LM \u5728\u7d14\u6587\u5b57\u8a2d\u5b9a\u4e2d\u5feb 1.1 \u500d - 1.4 \u500d\u3002\u6b64\u5916\uff0c\u5b83\u8207 Hugging Face Transformers \u5b8c\u7f8e\u6574\u5408\u3002\u5c0d\u65bc\u6a21\u578b\u8a13\u7df4\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5305\u542b\u5c0d\u9f4a\u3001\u9810\u8a13\u7df4\u3001\u6587\u672c\u5ef6\u4f38\u548c\u9577\u77ed\u806f\u5408\u76e3\u7763\u5fae\u8abf\u7684\u4e94\u968e\u6bb5\u7ba1\u9053\u3002\u95dc\u65bc\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u7cbe\u5fc3\u69cb\u5efa\u4e86\u5927\u898f\u6a21\u7684\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u9577\u5f71\u7247\u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\uff0c\u4ee5\u652f\u63f4\u6211\u5011\u7684\u591a\u968e\u6bb5\u8a13\u7df4\u6d41\u7a0b\u3002\u9019\u500b\u5b8c\u6574\u89e3\u6c7a\u65b9\u6848\u5c07 VILA \u7684\u53ef\u884c\u5e40\u6578\u64f4\u5927\u4e86 128 \u500d\uff08\u5f9e 8 \u5e40\u64f4\u5c55\u5230 1024 \u5e40\uff09\uff0c\u4e26\u5c07\u9577\u5f71\u7247\u5b57\u5e55\u8a55\u5206\u5f9e 2.00 \u63d0\u5347\u5230 3.26\uff081.6 \u500d\uff09\uff0c\u5728 1400 \u5e40\u5f71\u7247\uff08274k \u6587\u672c\u9577\u5ea6\uff09\u4e2d\u9054\u5230 99.5% \u7684\u5927\u6d77\u6488\u91dd\u6e96\u78ba\u5ea6\u3002LongVILA-8B \u4e5f\u8b49\u660e\u4e86\u96a8\u8457\u5f71\u7247\u5e40\u6578\u589e\u52a0\uff0c\u5728 VideoMME \u57fa\u6e96\u4e2d\uff0c\u9577\u5f71\u7247\u7684\u6548\u80fd\u6301\u7e8c\u63d0\u5347\u3002", "author": "Fuzhao Xue et.al.", "authors": "Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han", "id": "2408.10188v1", "paper_url": "http://arxiv.org/abs/2408.10188v1", "repo": "https://github.com/nvlabs/vila"}}