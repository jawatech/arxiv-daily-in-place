{"2409.06411": {"publish_time": "2024-09-10", "title": "Length Desensitization in Directed Preference Optimization", "paper_summary": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.", "paper_summary_zh": "\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08DPO\uff09\u5ee3\u6cdb\u7528\u65bc\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2\uff08RLHF\uff09\u968e\u6bb5\uff0c\u4ee5\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8207\u4eba\u985e\u504f\u597d\u7d50\u5408\uff0c\u5f9e\u800c\u589e\u5f37\u5176\u7121\u5bb3\u6027\u548c\u6548\u80fd\u3002\u7136\u800c\uff0c\u89c0\u5bdf\u5230 DPO \u50be\u5411\u65bc\u904e\u5ea6\u6700\u4f73\u5316\u5197\u9577\u6027\uff0c\u9019\u53ef\u80fd\u6703\u5c0d\u6548\u80fd\u548c\u4f7f\u7528\u8005\u9ad4\u9a57\u9020\u6210\u8ca0\u9762\u5f71\u97ff\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d DPO \u7684\u6700\u4f73\u5316\u76ee\u6a19\u9032\u884c\u6df1\u5165\u7684\u7406\u8ad6\u5206\u6790\uff0c\u4e26\u63ed\u793a\u5176\u96b1\u542b\u734e\u52f5\u8207\u8cc7\u6599\u9577\u5ea6\u4e4b\u9593\u7684\u5f37\u70c8\u95dc\u806f\u3002\u9019\u7a2e\u95dc\u806f\u6703\u8aa4\u5c0e\u6700\u4f73\u5316\u65b9\u5411\uff0c\u5c0e\u81f4 DPO \u8a13\u7df4\u671f\u9593\u9577\u5ea6\u654f\u611f\uff0c\u4e26\u5c0e\u81f4\u5197\u9577\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9577\u5ea6\u53bb\u654f\u5316\u6539\u5584\u65b9\u6cd5\uff0c\u7a31\u70ba LD-DPO\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u900f\u904e\u5c07\u986f\u5f0f\u7684\u9577\u5ea6\u504f\u597d\uff08\u76f8\u5c0d\u4e0d\u91cd\u8981\uff09\u8207\u5176\u4ed6\u96b1\u542b\u504f\u597d\u5206\u958b\uff0c\u8b93 DPO \u5c0d\u8cc7\u6599\u9577\u5ea6\u53bb\u654f\u5316\uff0c\u5f9e\u800c\u80fd\u66f4\u6709\u6548\u5730\u5b78\u7fd2\u5167\u5728\u504f\u597d\u3002\u6211\u5011\u5229\u7528 Llama2-13B\u3001Llama3-8B \u548c Qwen2-7B \u7684\u5169\u500b\u8a2d\u5b9a\uff08\u57fa\u790e\u548c\u6307\u4ee4\uff09\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u9032\u884c\u5be6\u9a57\u9a57\u8b49\uff0c\u5305\u62ec MT-Bench \u548c AlpacaEval 2\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cLD-DPO \u6301\u7e8c\u512a\u65bc DPO \u548c\u5176\u4ed6\u57fa\u7dda\u65b9\u6cd5\uff0c\u8207 DPO \u76f8\u6bd4\uff0c\u9577\u5ea6\u6e1b\u5c11\u4e86 10-40%\uff0c\u7522\u751f\u4e86\u66f4\u7c21\u6f54\u7684\u56de\u61c9\u3002\u6211\u5011\u9032\u884c\u4e86\u6df1\u5165\u7684\u5be6\u9a57\u5206\u6790\uff0c\u4ee5\u8b49\u660e LD-DPO \u78ba\u5be6\u53ef\u4ee5\u5be6\u73fe\u9577\u5ea6\u53bb\u654f\u5316\uff0c\u4e26\u4f7f\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u985e\u771f\u5be6\u504f\u597d\u3002", "author": "Wei Liu et.al.", "authors": "Wei Liu, Yang Bai, Chengcheng Han, Rongxiang Weng, Jun Xu, Xuezhi Cao, Jingang Wang, Xunliang Cai", "id": "2409.06411v1", "paper_url": "http://arxiv.org/abs/2409.06411v1", "repo": "null"}}