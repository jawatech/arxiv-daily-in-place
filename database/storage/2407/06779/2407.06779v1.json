{"2407.06779": {"publish_time": "2024-07-09", "title": "Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical Questions", "paper_summary": "Our team participated in the BioASQ 2024 Task12b and Synergy tasks to build a\nsystem that can answer biomedical questions by retrieving relevant articles and\nsnippets from the PubMed database and generating exact and ideal answers. We\npropose a two-level information retrieval and question-answering system based\non pre-trained large language models (LLM), focused on LLM prompt engineering\nand response post-processing. We construct prompts with in-context few-shot\nexamples and utilize post-processing techniques like resampling and malformed\nresponse detection. We compare the performance of various pre-trained LLM\nmodels on this challenge, including Mixtral, OpenAI GPT and Llama2. Our\nbest-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP\nscore on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score\nfor factoid questions and 0.50 F1 score for list questions in Task 12b.", "paper_summary_zh": "\u6211\u5011\u7684\u5718\u968a\u53c3\u8207\u4e86 BioASQ 2024 Task12b \u548c Synergy \u4efb\u52d9\uff0c\u5efa\u7acb\u4e00\u500b\u7cfb\u7d71\uff0c\u900f\u904e\u64f7\u53d6 PubMed \u8cc7\u6599\u5eab\u4e2d\u7684\u76f8\u95dc\u6587\u7ae0\u548c\u6458\u8981\uff0c\u4e26\u7522\u751f\u7cbe\u78ba\u4e14\u7406\u60f3\u7684\u7b54\u6848\uff0c\u4f86\u56de\u7b54\u751f\u7269\u91ab\u5b78\u554f\u984c\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u9810\u5148\u8a13\u7df4\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u96d9\u5c64\u8cc7\u8a0a\u6aa2\u7d22\u548c\u554f\u7b54\u7cfb\u7d71\uff0c\u5c08\u6ce8\u65bc LLM \u63d0\u793a\u5de5\u7a0b\u548c\u56de\u61c9\u5f8c\u8655\u7406\u3002\u6211\u5011\u4f7f\u7528\u8108\u7d61\u4e2d\u7684\u5c11\u6b21\u7bc4\u4f8b\u4f86\u5efa\u69cb\u63d0\u793a\uff0c\u4e26\u5229\u7528\u91cd\u65b0\u53d6\u6a23\u548c\u7578\u5f62\u56de\u61c9\u5075\u6e2c\u7b49\u5f8c\u8655\u7406\u6280\u8853\u3002\u6211\u5011\u6bd4\u8f03\u5404\u7a2e\u9810\u5148\u8a13\u7df4\u7684 LLM \u6a21\u578b\u5728\u9019\u500b\u6311\u6230\u4e2d\u7684\u6548\u80fd\uff0c\u5305\u62ec Mixtral\u3001OpenAI GPT \u548c Llama2\u3002\u6211\u5011\u6548\u80fd\u6700\u4f73\u7684\u7cfb\u7d71\u5728\u6587\u4ef6\u6aa2\u7d22\u4e2d\u9054\u5230 0.14 MAP \u5206\u6578\uff0c\u5728\u6458\u8981\u6aa2\u7d22\u4e2d\u9054\u5230 0.05 MAP \u5206\u6578\uff0c\u5728\u662f\u975e\u984c\u4e2d\u9054\u5230 0.96 F1 \u5206\u6578\uff0c\u5728\u4e8b\u5be6\u984c\u4e2d\u9054\u5230 0.38 MRR \u5206\u6578\uff0c\u5728 Task 12b \u7684\u5217\u8868\u984c\u4e2d\u9054\u5230 0.50 F1 \u5206\u6578\u3002", "author": "Wenxin Zhou et.al.", "authors": "Wenxin Zhou, Thuy Hang Ngo", "id": "2407.06779v1", "paper_url": "http://arxiv.org/abs/2407.06779v1", "repo": "null"}}