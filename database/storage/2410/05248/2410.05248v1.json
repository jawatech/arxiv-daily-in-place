{"2410.05248": {"publish_time": "2024-10-07", "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "paper_summary": "To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u8a98\u5c0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4e92\u52d5\u9a45\u52d5\u4efb\u52d9\u4e2d\u7522\u751f\u60f3\u8981\u7684\u884c\u70ba\uff0c\u6307\u4ee4\u8abf\u6574\u968e\u6bb5\u901a\u5e38\u4f7f\u7528\u4e0b\u4e00\u500b\u7b26\u865f\u9810\u6e2c (NTP) \u640d\u5931\u5728\u6307\u4ee4\u56de\u61c9\u914d\u5c0d\u4e0a\u8a13\u7df4 LLM\u3002\u5148\u524d\u65e8\u5728\u6539\u5584\u6307\u4ee4\u8abf\u6574\u6548\u80fd\u7684\u7814\u7a76\uff0c\u901a\u5e38\u5f37\u8abf\u9700\u8981\u54c1\u8cea\u66f4\u9ad8\u7684\u76e3\u7763\u5fae\u8abf (SFT) \u8cc7\u6599\u96c6\uff0c\u9019\u901a\u5e38\u6d89\u53ca\u4f7f\u7528\u5c08\u6709 LLM \u9032\u884c\u6602\u8cb4\u7684\u8cc7\u6599\u7be9\u9078\u6216\u7531\u4eba\u5de5\u8a3b\u89e3\u4eba\u54e1\u9032\u884c\u5927\u91cf\u8cc7\u6599\u7522\u751f\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e26\u672a\u5145\u5206\u5229\u7528\u8cc7\u6599\u96c6\u7684\u5167\u5728\u5c6c\u6027\uff0c\u5c0e\u81f4\u9ad8\u6602\u7684\u904b\u7b97\u548c\u4eba\u529b\u6210\u672c\uff0c\u56e0\u6b64\u9650\u5236\u4e86\u53ef\u64f4\u5145\u6027\u548c\u6548\u80fd\u63d0\u5347\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa SFTMix\uff0c\u9019\u662f\u4e00\u500b\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u5347\u6307\u4ee4\u8abf\u6574\u6548\u80fd\uff0c\u8d85\u8d8a\u50b3\u7d71\u7684 NTP \u5178\u7bc4\uff0c\u800c\u4e14\u4e0d\u9700\u8981\u7cbe\u5fc3\u6574\u7406\u7684\u8cc7\u6599\u96c6\u3002\u89c0\u5bdf\u5230 LLM \u5728\u8a9e\u610f\u8868\u793a\u7a7a\u9593\u4e2d\u5c55\u73fe\u51fa\u4e0d\u5747\u52fb\u7684\u4fe1\u5fc3\uff0c\u6211\u5011\u8a8d\u70ba\u5728\u6307\u4ee4\u8abf\u6574\u904e\u7a0b\u4e2d\uff0c\u5177\u6709\u4e0d\u540c\u4fe1\u5fc3\u5c64\u7d1a\u7684\u7bc4\u4f8b\u61c9\u626e\u6f14\u4e0d\u540c\u7684\u89d2\u8272\u3002\u57fa\u65bc\u9019\u500b\u898b\u89e3\uff0cSFTMix \u5229\u7528\u8a13\u7df4\u52d5\u614b\u4f86\u8b58\u5225\u5177\u6709\u4e0d\u540c\u4fe1\u5fc3\u5c64\u7d1a\u7684\u7bc4\u4f8b\uff0c\u7136\u5f8c\u61c9\u7528\u57fa\u65bc Mixup \u7684\u6b63\u898f\u5316\u4f86\u6e1b\u8f15\u5c0d\u6709\u4fe1\u5fc3\u7684\u7bc4\u4f8b\u9032\u884c\u904e\u5ea6\u64ec\u5408\uff0c\u540c\u6642\u50b3\u64ad\u76e3\u7763\u8a0a\u865f\u4ee5\u6539\u5584\u5c0d\u76f8\u5c0d\u6c92\u6709\u4fe1\u5fc3\u7684\u7bc4\u4f8b\u7684\u5b78\u7fd2\u3002\u6b64\u65b9\u6cd5\u8b93 SFTMix \u80fd\u5728\u5ee3\u6cdb\u7684\u6307\u4ee4\u9075\u5faa\u548c\u91ab\u7642\u4fdd\u5065\u9818\u57df\u7279\u5b9a SFT \u4efb\u52d9\u4e2d\u986f\u8457\u512a\u65bc NTP\uff0c\u8b49\u660e\u5176\u5c0d\u4e0d\u540c LLM \u5bb6\u65cf\u7684\u9069\u61c9\u6027\u548c\u5c0d\u4efb\u4f55\u898f\u6a21\u8cc7\u6599\u96c6\u7684\u53ef\u64f4\u5145\u6027\u3002\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u9032\u4e00\u6b65\u9a57\u8b49\u4e86 SFTMix \u8a2d\u8a08\u9078\u64c7\u7684\u7a69\u5065\u6027\uff0c\u5f37\u8abf\u5176\u5728\u66f4\u5ee3\u6cdb\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u61c9\u7528\u4e2d\u6301\u7e8c\u63d0\u5347\u4e0d\u540c LLM \u548c\u8cc7\u6599\u96c6\u6548\u80fd\u7684\u591a\u529f\u80fd\u6027\u3002</paragraph>", "author": "Yuxin Xiao et.al.", "authors": "Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao", "id": "2410.05248v1", "paper_url": "http://arxiv.org/abs/2410.05248v1", "repo": "null"}}