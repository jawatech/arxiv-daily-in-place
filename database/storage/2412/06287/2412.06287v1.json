{"2412.06287": {"publish_time": "2024-12-09", "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models", "paper_summary": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,565 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u7642\u9818\u57df\u7684\u51fa\u73fe\n\u5f37\u8abf\u4e86\u5c0d\u6a19\u6e96\u8cc7\u6599\u96c6\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u8a55\u4f30\u5176\n\u554f\u7b54 (QA) \u6548\u80fd\u3002\u5118\u7ba1\u6709\u5e7e\u500b\u91ab\u7642 QA \u7684\u57fa\u6e96\n\u8cc7\u6599\u96c6\uff0c\u4f46\u5b83\u5011\u6db5\u84cb\u4e0d\u540c\u79d1\u5225\u7684\u5e38\u8b58\uff0c\u6216\u7279\u5b9a\u65bc\u5152\u79d1\u4ee5\u5916\u7684\u79d1\u5225\u3002\n\u6b64\u5916\uff0c\u5176\u4e2d\u4e00\u4e9b\u50c5\u9650\u65bc\u5ba2\u89c0\u554f\u984c\uff0c\u4e14\u7121\u6cd5\u8861\u91cf LLM \u7684\u751f\u6210\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5b83\u5011\u7121\u6cd5\u5168\u9762\u8a55\u4f30\nLLM \u5728\u5152\u79d1\u4e2d\u7684 QA \u80fd\u529b\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7a7a\u767d\uff0c\u6211\u5011\u5efa\u69cb\nPediaBench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7528\u65bc LLM \u8a55\u4f30\u7684\u4e2d\u6587\u5152\u79d1\u8cc7\u6599\u96c6\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u5b83\u5305\u542b 4,565 \u500b\u5ba2\u89c0\u554f\u984c\u548c 1,632 \u500b\u4e3b\u89c0\n\u554f\u984c\uff0c\u6db5\u84cb 12 \u500b\u5152\u79d1\u75be\u75c5\u7d44\u3002\u5b83\u63a1\u7528\u57fa\u65bc\u4e0d\u540c\u96e3\u5ea6\u7b49\u7d1a\u7684\u7d9c\u5408\u8a55\u5206\n\u6a19\u6e96\uff0c\u4ee5\u5fb9\u5e95\u8a55\u4f30 LLM \u5728\u9075\u5faa\u8aaa\u660e\u3001\u77e5\u8b58\u7406\u89e3\u3001\n\u81e8\u5e8a\u6848\u4f8b\u5206\u6790\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u9a57\u8b49\u4e86\nPediaBench \u7684\u6709\u6548\u6027\uff0c\u5c0d 20 \u500b\u958b\u6e90\u548c\u5546\u696d LLM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\n\u900f\u904e\u6df1\u5165\u5206\u6790\u5be6\u9a57\u7d50\u679c\uff0c\u6211\u5011\u6df1\u5165\u4e86\u89e3\u4e86 LLM \u5728\u4e2d\u6587\u80cc\u666f\u4e0b\u56de\u7b54\u5152\u79d1\u554f\u984c\u7684\u80fd\u529b\uff0c\n\u5f37\u8abf\u4e86\u5b83\u5011\u7684\u9650\u5236\u4ee5\u9032\u4e00\u6b65\u6539\u9032\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u5df2\u767c\u5e03\u5728 https://github.com/ACMISLab/PediaBench\u3002", "author": "Qian Zhang et.al.", "authors": "Qian Zhang, Panfeng Chen, Jiali Li, Linkun Feng, Shuyu Liu, Mei Chen, Hui Li, Yanhao Wang", "id": "2412.06287v1", "paper_url": "http://arxiv.org/abs/2412.06287v1", "repo": "https://github.com/acmislab/pediabench"}}