{"2501.13181": {"publish_time": "2025-01-22", "title": "Learning in Log-Domain: Subthreshold Analog AI Accelerator Based on Stochastic Gradient Descent", "paper_summary": "The rapid proliferation of AI models, coupled with growing demand for edge\ndeployment, necessitates the development of AI hardware that is both\nhigh-performance and energy-efficient. In this paper, we propose a novel analog\naccelerator architecture designed for AI/ML training workloads using stochastic\ngradient descent with L2 regularization (SGDr). The architecture leverages\nlog-domain circuits in subthreshold MOS and incorporates volatile memory. We\nestablish a mathematical framework for solving SGDr in the continuous time\ndomain and detail the mapping of SGDr learning equations to log-domain\ncircuits. By operating in the analog domain and utilizing weak inversion, the\nproposed design achieves significant reductions in transistor area and power\nconsumption compared to digital implementations. Experimental results\ndemonstrate that the architecture closely approximates ideal behavior, with a\nmean square error below 0.87% and precision as low as 8 bits. Furthermore, the\narchitecture supports a wide range of hyperparameters. This work paves the way\nfor energy-efficient analog AI hardware with on-chip training capabilities.", "paper_summary_zh": "\u96a8\u8457 AI \u6a21\u578b\u5feb\u901f\u64f4\u6563\uff0c\u52a0\u4e0a\u5c0d\u908a\u7de3\u90e8\u7f72\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u9019\u4f7f\u5f97\u9700\u8981\u958b\u767c\u65e2\u9ad8\u6548\u80fd\u53c8\u7bc0\u80fd\u7684 AI \u786c\u9ad4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u985e\u6bd4\u52a0\u901f\u5668\u67b6\u69cb\uff0c\u5c08\u70ba\u4f7f\u7528\u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d\u6cd5\u642d\u914d L2 \u6b63\u898f\u5316 (SGDr) \u7684 AI/ML \u8a13\u7df4\u5de5\u4f5c\u8ca0\u8f09\u800c\u8a2d\u8a08\u3002\u6b64\u67b6\u69cb\u5229\u7528\u4e9e\u95be\u503c MOS \u4e2d\u7684\u5c0d\u6578\u57df\u96fb\u8def\uff0c\u4e26\u6574\u5408\u4e86\u63ee\u767c\u6027\u8a18\u61b6\u9ad4\u3002\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u6578\u5b78\u6846\u67b6\uff0c\u7528\u65bc\u5728\u9023\u7e8c\u6642\u9593\u57df\u4e2d\u6c42\u89e3 SGDr\uff0c\u4e26\u8a73\u7d30\u8aaa\u660e\u5c07 SGDr \u5b78\u7fd2\u65b9\u7a0b\u5f0f\u5c0d\u61c9\u5230\u5c0d\u6578\u57df\u96fb\u8def\u7684\u904e\u7a0b\u3002\u900f\u904e\u5728\u985e\u6bd4\u57df\u4e2d\u904b\u4f5c\u4e26\u5229\u7528\u5f31\u53cd\u8f49\uff0c\u8207\u6578\u4f4d\u5be6\u4f5c\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u8a2d\u8a08\u5728\u96fb\u6676\u9ad4\u9762\u7a4d\u548c\u529f\u8017\u65b9\u9762\u90fd\u5927\u5e45\u964d\u4f4e\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6b64\u67b6\u69cb\u975e\u5e38\u63a5\u8fd1\u7406\u60f3\u884c\u70ba\uff0c\u5747\u65b9\u8aa4\u5dee\u4f4e\u65bc 0.87%\uff0c\u4e14\u7cbe\u6e96\u5ea6\u4f4e\u81f3 8 \u4f4d\u5143\u3002\u6b64\u5916\uff0c\u6b64\u67b6\u69cb\u652f\u63f4\u5404\u7a2e\u8d85\u53c3\u6578\u3002\u9019\u9805\u5de5\u4f5c\u70ba\u5177\u5099\u6676\u7247\u4e0a\u8a13\u7df4\u529f\u80fd\u7684\u7bc0\u80fd\u985e\u6bd4 AI \u786c\u9ad4\u92ea\u8def\u3002", "author": "Momen K Tageldeen et.al.", "authors": "Momen K Tageldeen, Yacine Belgaid, Vivek Mohan, Zhou Wang, Emmanuel M Drakakis", "id": "2501.13181v1", "paper_url": "http://arxiv.org/abs/2501.13181v1", "repo": "null"}}