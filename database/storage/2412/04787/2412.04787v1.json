{"2412.04787": {"publish_time": "2024-12-06", "title": "Direct Quantized Training of Language Models with Stochastic Rounding", "paper_summary": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweight matrices required for straight-through estimation must be maintained\nthroughout the whole training process. To address this, we explore the\npotential of directly updating the quantized low-precision weight matrices\nwithout relying on the straight-through estimator during backpropagation,\nthereby saving memory usage during training. Specifically, we employ a\nstochastic rounding technique to minimize information loss caused by the use of\nlow-bit weights throughout training. Experimental results on our\nLLaMA-structured models indicate that (1) training with only low-precision\nweights is feasible even when they are constrained to ternary values, (2)\nextending the bit width to 8 bits results in only a 5% loss degradation\ncompared to BitNet b1.58 while offering the potential for reduced memory usage\nduring training, and (3) our models can also perform inference using ternary\nweights, showcasing their flexibility in deployment.", "paper_summary_zh": "\u5118\u7ba1\u6700\u8fd1\u7684\u91cf\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 BitNet\uff0c\u5df2\u70ba\u5728\u4f7f\u7528\u4e8c\u9032\u4f4d\u6216\u4e09\u9032\u4f4d\u6b0a\u91cd\u9032\u884c\u90e8\u7f72\u6642\u5927\u5e45\u6e1b\u5c11\u8a18\u61b6\u9ad4\u7528\u91cf\u92ea\u5e73\u4e86\u9053\u8def\uff0c\u4f46\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u4ecd\u9700\u8981\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\u7a7a\u9593\u3002\u9019\u90e8\u5206\u662f\u56e0\u70ba\u5fc5\u9808\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u7dad\u8b77\u7528\u65bc\u76f4\u901a\u4f30\u8a08\u7684\u9ad8\u7cbe\u5ea6\uff08\u5373\u672a\u91cf\u5316\uff09\u6b0a\u91cd\u77e9\u9663\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5728\u53cd\u5411\u50b3\u64ad\u904e\u7a0b\u4e2d\u4e0d\u4f9d\u8cf4\u76f4\u901a\u4f30\u8a08\u5668\u800c\u76f4\u63a5\u66f4\u65b0\u91cf\u5316\u4f4e\u7cbe\u5ea6\u6b0a\u91cd\u77e9\u9663\u7684\u53ef\u80fd\u6027\uff0c\u5f9e\u800c\u7bc0\u7701\u8a13\u7df4\u671f\u9593\u7684\u8a18\u61b6\u9ad4\u7528\u91cf\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a1\u7528\u96a8\u6a5f\u6368\u5165\u6280\u8853\uff0c\u4ee5\u6700\u5c0f\u5316\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u4f7f\u7528\u4f4e\u4f4d\u5143\u6b0a\u91cd\u9020\u6210\u7684\u8cc7\u8a0a\u907a\u5931\u3002\u6211\u5011\u5728 LLaMA \u7d50\u69cb\u6a21\u578b\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff1a(1) \u5373\u4f7f\u5c07\u6b0a\u91cd\u9650\u5236\u70ba\u4e09\u9032\u4f4d\u503c\uff0c\u50c5\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u6b0a\u91cd\u9032\u884c\u8a13\u7df4\u4e5f\u662f\u53ef\u884c\u7684\uff0c(2) \u5c07\u4f4d\u5143\u5bec\u5ea6\u64f4\u5c55\u5230 8 \u4f4d\u5143\u53ea\u6703\u5c0e\u81f4 5% \u7684\u640d\u5931\u52a3\u5316\uff0c\u800c\u8207 BitNet b1.58 \u76f8\u6bd4\uff0c\u5728\u8a13\u7df4\u671f\u9593\u63d0\u4f9b\u4e86\u6e1b\u5c11\u8a18\u61b6\u9ad4\u7528\u91cf\u7684\u53ef\u80fd\u6027\uff0c(3) \u6211\u5011\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4e09\u9032\u4f4d\u6b0a\u91cd\u9032\u884c\u63a8\u8ad6\uff0c\u5c55\u793a\u4e86\u5b83\u5011\u5728\u90e8\u7f72\u4e2d\u7684\u9748\u6d3b\u6027\u3002", "author": "Kaiyan Zhao et.al.", "authors": "Kaiyan Zhao, Tsuguchika Tabaru, Kenichi Kobayashi, Takumi Honda, Masafumi Yamazaki, Yoshimasa Tsuruoka", "id": "2412.04787v1", "paper_url": "http://arxiv.org/abs/2412.04787v1", "repo": "null"}}