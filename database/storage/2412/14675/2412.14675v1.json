{"2412.14675": {"publish_time": "2024-12-19", "title": "LLMs as mediators: Can they diagnose conflicts accurately?", "paper_summary": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes.", "paper_summary_zh": "<paragraph>\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e3a\u4e86\u80fd\u591f\u8c03\u89e3\u51b2\u7a81\uff0c\u5404\u65b9\u4e89\u7aef\u7684\u89c2\u5bdf\u8005\u5fc5\u987b\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u5176\u4e89\u7aef\u7684\u6839\u6e90\uff0c\u662f\u6e90\u4e8e\u5bf9\u4f55\u4e3a\u771f\u5b9e\uff08\u56e0\u679c\u5173\u7cfb\uff09\u7684\u4fe1\u5ff5\u5dee\u5f02\uff0c\u8fd8\u662f\u6e90\u4e8e\u5bf9\u4ed6\u4eec\u6240\u91cd\u89c6\u4e8b\u7269\u7684\u5dee\u5f02\uff08\u9053\u5fb7\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6d4b\u8bd5\u4e86 OpenAI \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b GPT 3.5 \u548c GPT 4 \u662f\u5426\u53ef\u4ee5\u6267\u884c\u6b64\u4efb\u52a1\uff0c\u4ee5\u53ca\u54ea\u79cd\u7c7b\u578b\u7684\u4e89\u7aef\u88ab\u8bc1\u660e\u5bf9 LLM \u7684\u8bca\u65ad\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u5728 Ko\\c{c}ak \u7b49\u4eba\u7684\u7814\u7a76 1 \u4e2d\u590d\u5236\u4e86\u7814\u7a76 1\u3002(2003)\uff0c\u5b83\u91c7\u7528\u5c0f\u63d2\u56fe\u8bbe\u8ba1\uff0c\u4f7f\u7528 OpenAI \u7684 GPT 3.5 \u548c GPT 4\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u8fd9\u4e24\u79cd LLM \u5bf9\u56e0\u679c\u548c\u9053\u5fb7\u89c4\u8303\u4e4b\u95f4\u7684\u533a\u522b\u5177\u6709\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u4e14\u53ef\u4ee5\u53ef\u9760\u5730\u533a\u5206\u5b83\u4eec\u3002\u5f53\u88ab\u8981\u6c42\u8bca\u65ad\u5bf9\u8bdd\u4e2d\u5206\u6b67\u7684\u6839\u6e90\u65f6\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u8fd9\u4e24\u79cd LLM \u90fd\u8868\u73b0\u51fa\u9ad8\u4f30\u56e0\u679c\u5206\u6b67\u7a0b\u5ea6\u548c\u4f4e\u4f30\u9053\u5fb7\u9519\u4f4d\u6761\u4ef6\u4e0b\u9053\u5fb7\u5206\u6b67\u7a0b\u5ea6\u7684\u503e\u5411\u3002\u5f53\u4f7f\u7528\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e8e\u67d0\u4e2a\u95ee\u9898\u7684\u5177\u4f53\u8bed\u8a00\u7684\u8fd1\u7aef\u91cf\u8868\u65f6\uff0cGPT 4 \u7684\u8fd9\u79cd\u503e\u5411\u5c24\u5176\u660e\u663e\u3002\u4f7f\u7528\u8fd1\u7aef\u6216\u8fdc\u7aef\u91cf\u8868\u65f6\uff0cGPT 3.5 \u7684\u8868\u73b0\u4e0d\u5982 GPT4 \u6216\u4eba\u7c7b\u3002\u8be5\u7814\u7a76\u9996\u6b21\u6d4b\u8bd5\u4e86\u901a\u8fc7\u8bca\u65ad\u56e0\u679c\u548c\u8bc4\u4ef7\u89c4\u8303\u4e2d\u5206\u6b67\u7684\u6839\u6e90\u6765\u4f7f\u7528 LLM \u8c03\u89e3\u51b2\u7a81\u7684\u6f5c\u529b\u3002</paragraph>", "author": "\u00d6zgecan Ko\u00e7ak et.al.", "authors": "\u00d6zgecan Ko\u00e7ak, Phanish Puranam, Af\u015far Yegin", "id": "2412.14675v1", "paper_url": "http://arxiv.org/abs/2412.14675v1", "repo": "null"}}