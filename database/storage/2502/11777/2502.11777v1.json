{"2502.11777": {"publish_time": "2025-02-17", "title": "Deep Neural Networks for Accurate Depth Estimation with Latent Space Features", "paper_summary": "Depth estimation plays a pivotal role in advancing human-robot interactions,\nespecially in indoor environments where accurate 3D scene reconstruction is\nessential for tasks like navigation and object handling. Monocular depth\nestimation, which relies on a single RGB camera, offers a more affordable\nsolution compared to traditional methods that use stereo cameras or LiDAR.\nHowever, despite recent progress, many monocular approaches struggle with\naccurately defining depth boundaries, leading to less precise reconstructions.\nIn response to these challenges, this study introduces a novel depth estimation\nframework that leverages latent space features within a deep convolutional\nneural network to enhance the precision of monocular depth maps. The proposed\nmodel features dual encoder-decoder architecture, enabling both color-to-depth\nand depth-to-depth transformations. This structure allows for refined depth\nestimation through latent space encoding. To further improve the accuracy of\ndepth boundaries and local features, a new loss function is introduced. This\nfunction combines latent loss with gradient loss, helping the model maintain\nthe integrity of depth boundaries. The framework is thoroughly tested using the\nNYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in\ncomplex indoor scenarios. The results clearly show that this approach\neffectively reduces depth ambiguities and blurring, making it a promising\nsolution for applications in human-robot interaction and 3D scene\nreconstruction.", "paper_summary_zh": "\u6df1\u5ea6\u4f30\u8a08\u5728\u63a8\u9032\u4eba\u6a5f\u4e92\u52d5\u65b9\u9762\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u7279\u5225\u662f\u5728\u5ba4\u5167\u74b0\u5883\u4e2d\uff0c\u6e96\u78ba\u7684 3D \u5834\u666f\u91cd\u5efa\u5c0d\u65bc\u5c0e\u822a\u548c\u7269\u9ad4\u8655\u7406\u7b49\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\u3002\u55ae\u76ee\u6df1\u5ea6\u4f30\u8a08\u4f9d\u8cf4\u65bc\u55ae\u500b RGB \u76f8\u6a5f\uff0c\u8207\u4f7f\u7528\u7acb\u9ad4\u76f8\u6a5f\u6216 LiDAR \u7684\u50b3\u7d71\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u7d93\u6fdf\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u5118\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u8a31\u591a\u55ae\u76ee\u65b9\u6cd5\u5728\u6e96\u78ba\u5b9a\u7fa9\u6df1\u5ea6\u908a\u754c\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96e3\uff0c\u5f9e\u800c\u5c0e\u81f4\u91cd\u5efa\u7cbe\u5ea6\u964d\u4f4e\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6df1\u5ea6\u4f30\u8a08\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u5229\u7528\u6df1\u5ea6\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u4e2d\u7684\u6f5b\u5728\u7a7a\u9593\u7279\u5fb5\u4f86\u589e\u5f37\u55ae\u76ee\u6df1\u5ea6\u5716\u7684\u7cbe\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u63a1\u7528\u96d9\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u67b6\u69cb\uff0c\u65e2\u80fd\u9032\u884c\u984f\u8272\u5230\u6df1\u5ea6\u7684\u8f49\u63db\uff0c\u53c8\u80fd\u9032\u884c\u6df1\u5ea6\u5230\u6df1\u5ea6\u7684\u8f49\u63db\u3002\u9019\u7a2e\u7d50\u69cb\u5141\u8a31\u901a\u904e\u6f5b\u5728\u7a7a\u9593\u7de8\u78bc\u9032\u884c\u7cbe\u78ba\u7684\u6df1\u5ea6\u4f30\u8a08\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u9ad8\u6df1\u5ea6\u908a\u754c\u548c\u5c40\u90e8\u7279\u5fb5\u7684\u7cbe\u5ea6\uff0c\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u640d\u5931\u51fd\u6578\u3002\u6b64\u51fd\u6578\u5c07\u6f5b\u5728\u640d\u5931\u8207\u68af\u5ea6\u640d\u5931\u76f8\u7d50\u5408\uff0c\u5e6b\u52a9\u6a21\u578b\u7dad\u8b77\u6df1\u5ea6\u908a\u754c\u7684\u5b8c\u6574\u6027\u3002\u4f7f\u7528 NYU Depth V2 \u6578\u64da\u96c6\u5c0d\u8a72\u6846\u67b6\u9032\u884c\u4e86\u5168\u9762\u6e2c\u8a66\uff0c\u5728\u8a72\u6578\u64da\u96c6\u4e0a\uff0c\u5b83\u8a2d\u5b9a\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u7279\u5225\u662f\u5728\u8907\u96dc\u7684\u5ba4\u5167\u5834\u666f\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7d50\u679c\u6e05\u695a\u5730\u8868\u660e\uff0c\u9019\u7a2e\u65b9\u6cd5\u6709\u6548\u5730\u6e1b\u5c11\u4e86\u6df1\u5ea6\u6a21\u7cca\u548c\u6a21\u7cca\uff0c\u4f7f\u5176\u6210\u70ba\u4eba\u6a5f\u4e92\u52d5\u548c 3D \u5834\u666f\u91cd\u5efa\u61c9\u7528\u4e2d\u4e00\u7a2e\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Siddiqui Muhammad Yasir et.al.", "authors": "Siddiqui Muhammad Yasir, Hyunsik Ahn", "id": "2502.11777v1", "paper_url": "http://arxiv.org/abs/2502.11777v1", "repo": "null"}}