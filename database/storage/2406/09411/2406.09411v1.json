{"2406.09411": {"publish_time": "2024-06-13", "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding", "paper_summary": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.", "paper_summary_zh": "\u6211\u5011\u63a8\u51fa MuirBench\uff0c\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\uff0c\u5c08\u6ce8\u65bc\u591a\u6a21\u614b LLM \u7684\u5065\u5168\u591a\u5716\u50cf\u7406\u89e3\u80fd\u529b\u3002MuirBench \u5305\u542b 12 \u500b\u591a\u6a23\u5316\u7684\u591a\u5716\u50cf\u4efb\u52d9\uff08\u4f8b\u5982\u5834\u666f\u7406\u89e3\u3001\u6392\u5e8f\uff09\uff0c\u6d89\u53ca 10 \u985e\u591a\u5716\u50cf\u95dc\u4fc2\uff08\u4f8b\u5982\u591a\u8996\u5716\u3001\u6642\u9593\u95dc\u4fc2\uff09\u3002MuirBench \u7531 11,264 \u5f35\u5716\u50cf\u548c 2,600 \u500b\u591a\u9078\u984c\u7d44\u6210\uff0c\u4ee5\u6210\u5c0d\u7684\u65b9\u5f0f\u5efa\u7acb\uff0c\u5176\u4e2d\u6bcf\u500b\u6a19\u6e96\u5be6\u4f8b\u90fd\u8207\u5177\u6709\u6700\u5c0f\u8a9e\u7fa9\u5dee\u7570\u7684\u4e0d\u53ef\u56de\u7b54\u8b8a\u9ad4\u914d\u5c0d\uff0c\u4ee5\u4fbf\u9032\u884c\u53ef\u9760\u7684\u8a55\u4f30\u3002\u5728 20 \u500b\u6700\u8fd1\u7684\u591a\u6a21\u614b LLM \u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u8868\u73fe\u6700\u597d\u7684\u6a21\u578b\uff0c\u4f8b\u5982 GPT-4o \u548c Gemini Pro\uff0c\u4e5f\u767c\u73fe\u89e3\u6c7a MuirBench \u5177\u6709\u6311\u6230\u6027\uff0c\u6e96\u78ba\u7387\u5206\u5225\u70ba 68.0% \u548c 49.3%\u3002\u5728\u55ae\u500b\u5716\u50cf\u4e0a\u8a13\u7df4\u7684\u958b\u6e90\u591a\u6a21\u614b LLM \u96e3\u4ee5\u6982\u62ec\u70ba\u591a\u5716\u50cf\u554f\u984c\uff0c\u6e96\u78ba\u7387\u4f4e\u65bc 33.3%\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86 MuirBench \u7684\u91cd\u8981\u6027\uff0c\u5b83\u9f13\u52f5\u793e\u7fa4\u958b\u767c\u80fd\u5920\u8d85\u8d8a\u55ae\u4e00\u5716\u50cf\u7684\u591a\u6a21\u614b LLM\uff0c\u4e26\u70ba\u672a\u4f86\u7684\u6539\u9032\u63d0\u51fa\u6f5b\u5728\u9014\u5f91\u3002", "author": "Fei Wang et.al.", "authors": "Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen", "id": "2406.09411v1", "paper_url": "http://arxiv.org/abs/2406.09411v1", "repo": "null"}}