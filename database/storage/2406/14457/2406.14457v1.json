{"2406.14457": {"publish_time": "2024-06-20", "title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue", "paper_summary": "Reinforcement learning (RL) is a powerful approach to enhance task-oriented\ndialogue (TOD) systems. However, existing RL methods tend to mainly focus on\ngeneration tasks, such as dialogue policy learning (DPL) or response generation\n(RG), while neglecting dialogue state tracking (DST) for understanding. This\nnarrow focus limits the systems to achieve globally optimal performance by\noverlooking the interdependence between understanding and generation.\nAdditionally, RL methods face challenges with sparse and delayed rewards, which\ncomplicates training and optimization. To address these issues, we extend RL\ninto both understanding and generation tasks by introducing step-by-step\nrewards throughout the token generation. The understanding reward increases as\nmore slots are correctly filled in DST, while the generation reward grows with\nthe accurate inclusion of user requests. Our approach provides a balanced\noptimization aligned with task completion. Experimental results demonstrate\nthat our approach effectively enhances the performance of TOD systems and\nachieves new state-of-the-art results on three widely used datasets, including\nMultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot\nability in low-resource settings compared to current models.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u589e\u5f37\u4efb\u52d9\u5c0e\u5411\u5c0d\u8a71 (TOD) \u7cfb\u7d71\u7684\u5f37\u5927\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 RL \u65b9\u6cd5\u50be\u5411\u65bc\u4e3b\u8981\u95dc\u6ce8\u751f\u6210\u4efb\u52d9\uff0c\u4f8b\u5982\u5c0d\u8a71\u7b56\u7565\u5b78\u7fd2 (DPL) \u6216\u56de\u61c9\u751f\u6210 (RG)\uff0c\u540c\u6642\u5ffd\u7565\u5c0d\u8a71\u72c0\u614b\u8ffd\u8e64 (DST) \u4ee5\u9032\u884c\u7406\u89e3\u3002\u9019\u7a2e\u72f9\u9698\u7684\u95dc\u6ce8\u9650\u5236\u4e86\u7cfb\u7d71\u901a\u904e\u5ffd\u8996\u7406\u89e3\u548c\u751f\u6210\u4e4b\u9593\u7684\u76f8\u4e92\u4f9d\u8cf4\u6027\u4f86\u5be6\u73fe\u5168\u5c40\u6700\u4f73\u6548\u80fd\u3002\u6b64\u5916\uff0cRL \u65b9\u6cd5\u9762\u81e8\u7a00\u758f\u548c\u5ef6\u9072\u734e\u52f5\u7684\u6311\u6230\uff0c\u9019\u4f7f\u5f97\u8a13\u7df4\u548c\u6700\u4f73\u5316\u8b8a\u5f97\u8907\u96dc\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u900f\u904e\u5728\u6574\u500b\u4ee3\u78bc\u751f\u6210\u904e\u7a0b\u4e2d\u5f15\u5165\u9010\u6b65\u734e\u52f5\uff0c\u5c07 RL \u5ef6\u4f38\u5230\u7406\u89e3\u548c\u751f\u6210\u4efb\u52d9\u4e2d\u3002\u96a8\u8457 DST \u4e2d\u6b63\u78ba\u586b\u5165\u66f4\u591a\u6642\u6bb5\uff0c\u7406\u89e3\u734e\u52f5\u6703\u589e\u52a0\uff0c\u800c\u751f\u6210\u734e\u52f5\u5247\u6703\u96a8\u8457\u6e96\u78ba\u5305\u542b\u4f7f\u7528\u8005\u8981\u6c42\u800c\u589e\u52a0\u3002\u6211\u5011\u7684\u505a\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u8207\u4efb\u52d9\u5b8c\u6210\u76f8\u4e00\u81f4\u7684\u5e73\u8861\u6700\u4f73\u5316\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6709\u6548\u5730\u589e\u5f37\u4e86 TOD \u7cfb\u7d71\u7684\u6548\u80fd\uff0c\u4e26\u5728\u4e09\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\uff0c\u5305\u62ec MultiWOZ2.0\u3001MultiWOZ2.1 \u548c In-Car \u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u9032\u6210\u679c\u3002\u8207\u76ee\u524d\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u4f4e\u8cc7\u6e90\u8a2d\u5b9a\u4e2d\u4e5f\u5c55\u73fe\u51fa\u512a\u7570\u7684\u5c11\u6b21\u5b78\u7fd2\u80fd\u529b\u3002", "author": "Huifang Du et.al.", "authors": "Huifang Du, Shuqin Li, Minghao Wu, Xuejing Feng, Yuan-Fang Li, Haofen Wang", "id": "2406.14457v1", "paper_url": "http://arxiv.org/abs/2406.14457v1", "repo": "null"}}