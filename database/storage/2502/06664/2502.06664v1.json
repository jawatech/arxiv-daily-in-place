{"2502.06664": {"publish_time": "2025-02-10", "title": "Evaluation of Deep Audio Representations for Hearables", "paper_summary": "Effectively steering hearable devices requires understanding the acoustic\nenvironment around the user. In the computational analysis of sound scenes,\nfoundation models have emerged as the state of the art to produce\nhigh-performance, robust, multi-purpose audio representations. We introduce and\nrelease Deep Evaluation of Audio Representations (DEAR), the first dataset and\nbenchmark to evaluate the efficacy of foundation models in capturing essential\nacoustic properties for hearables. The dataset includes 1,158 audio tracks,\neach 30 seconds long, created by spatially mixing proprietary monologues with\ncommercial, high-quality recordings of everyday acoustic scenes. Our benchmark\nencompasses eight tasks that assess the general context, speech sources, and\ntechnical acoustic properties of the audio scenes. Through our evaluation of\nfour general-purpose audio representation models, we demonstrate that the BEATs\nmodel significantly surpasses its counterparts. This superiority underscores\nthe advantage of models trained on diverse audio collections, confirming their\napplicability to a wide array of auditory tasks, including encoding the\nenvironment properties necessary for hearable steering. The DEAR dataset and\nassociated code are available at https://dear-dataset.github.io.", "paper_summary_zh": "\u6709\u6548\u63a7\u5236\u53ef\u807d\u8a2d\u5099\u9700\u8981\u4e86\u89e3\u4f7f\u7528\u8005\u5468\u570d\u7684\u8072\u5b78\u74b0\u5883\u3002\u5728\u8072\u97f3\u5834\u666f\u7684\u8a08\u7b97\u5206\u6790\u4e2d\uff0c\u57fa\u790e\u6a21\u578b\u5df2\u6210\u70ba\u7522\u751f\u9ad8\u6027\u80fd\u3001\u7a69\u5065\u3001\u591a\u7528\u9014\u97f3\u8a0a\u8868\u793a\u7684\u6700\u65b0\u6280\u8853\u3002\u6211\u5011\u4ecb\u7d39\u4e26\u767c\u5e03\u97f3\u8a0a\u8868\u793a\u6df1\u5ea6\u8a55\u4f30 (DEAR)\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u8a55\u4f30\u57fa\u790e\u6a21\u578b\u5728\u6355\u6349\u53ef\u807d\u8a2d\u5099\u57fa\u672c\u8072\u5b78\u5c6c\u6027\u7684\u6548\u80fd\u7684\u8cc7\u6599\u96c6\u548c\u57fa\u6e96\u3002\u8cc7\u6599\u96c6\u5305\u62ec 1,158 \u500b\u97f3\u8a0a\u8ecc\u9053\uff0c\u6bcf\u500b\u9577\u5ea6\u70ba 30 \u79d2\uff0c\u900f\u904e\u5c07\u5c08\u6709\u7368\u767d\u8207\u65e5\u5e38\u8072\u5b78\u5834\u666f\u7684\u9ad8\u54c1\u8cea\u5546\u696d\u9304\u97f3\u9032\u884c\u7a7a\u9593\u6df7\u5408\u800c\u5efa\u7acb\u3002\u6211\u5011\u7684\u57fa\u6e96\u5305\u542b\u516b\u9805\u4efb\u52d9\uff0c\u7528\u65bc\u8a55\u4f30\u97f3\u8a0a\u5834\u666f\u7684\u4e00\u822c\u80cc\u666f\u3001\u8a9e\u97f3\u4f86\u6e90\u548c\u6280\u8853\u8072\u5b78\u5c6c\u6027\u3002\u900f\u904e\u6211\u5011\u5c0d\u56db\u500b\u901a\u7528\u97f3\u8a0a\u8868\u793a\u6a21\u578b\u7684\u8a55\u4f30\uff0c\u6211\u5011\u8b49\u660e BEATs \u6a21\u578b\u986f\u8457\u8d85\u8d8a\u5176\u5c0d\u61c9\u6a21\u578b\u3002\u9019\u7a2e\u512a\u8d8a\u6027\u5f37\u8abf\u4e86\u91dd\u5c0d\u5404\u7a2e\u97f3\u8a0a\u96c6\u5408\u8a13\u7df4\u6a21\u578b\u7684\u512a\u9ede\uff0c\u8b49\u5be6\u4e86\u5b83\u5011\u9069\u7528\u65bc\u5ee3\u6cdb\u7684\u807d\u89ba\u4efb\u52d9\uff0c\u5305\u62ec\u7de8\u78bc\u53ef\u807d\u5c0e\u5411\u6240\u9700\u7684\u74b0\u5883\u5c6c\u6027\u3002DEAR \u8cc7\u6599\u96c6\u548c\u76f8\u95dc\u7a0b\u5f0f\u78bc\u53ef\u5728 https://dear-dataset.github.io/ \u53d6\u5f97\u3002", "author": "Fabian Gr\u00f6ger et.al.", "authors": "Fabian Gr\u00f6ger, Pascal Baumann, Ludovic Amruthalingam, Laurent Simon, Ruksana Giurda, Simone Lionetti", "id": "2502.06664v1", "paper_url": "http://arxiv.org/abs/2502.06664v1", "repo": "null"}}