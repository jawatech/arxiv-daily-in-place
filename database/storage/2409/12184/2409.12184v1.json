{"2409.12184": {"publish_time": "2024-09-02", "title": "Democratizing MLLMs in Healthcare: TinyLLaVA-Med for Efficient Healthcare Diagnostics in Resource-Constrained Settings", "paper_summary": "Deploying Multi-Modal Large Language Models (MLLMs) in healthcare is hindered\nby their high computational demands and significant memory requirements, which\nare particularly challenging for resource-constrained devices like the Nvidia\nJetson Xavier. This problem is particularly evident in remote medical settings\nwhere advanced diagnostics are needed but resources are limited. In this paper,\nwe introduce an optimization method for the general-purpose MLLM, TinyLLaVA,\nwhich we have adapted and renamed TinyLLaVA-Med. This adaptation involves\ninstruction-tuning and fine-tuning TinyLLaVA on a medical dataset by drawing\ninspiration from the LLaVA-Med training pipeline. Our approach successfully\nminimizes computational complexity and power consumption, with TinyLLaVA-Med\noperating at 18.9W and using 11.9GB of memory, while achieving accuracies of\n64.54% on VQA-RAD and 70.70% on SLAKE for closed-ended questions. Therefore,\nTinyLLaVA-Med achieves deployment viability in hardware-constrained\nenvironments with low computational resources, maintaining essential\nfunctionalities and delivering accuracies close to state-of-the-art models.", "paper_summary_zh": "\u90e8\u7f72\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e8e\u533b\u7597\u4fdd\u5065\u9886\u57df\u65f6\uff0c\u4f1a\u53d7\u5230\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u5e9e\u5927\u5185\u5b58\u9700\u6c42\u7684\u963b\u788d\uff0c\u800c\u8fd9\u5bf9\u4e8e Nvidia Jetson Xavier \u7b49\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u6765\u8bf4\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u95ee\u9898\u5728\u9700\u8981\u9ad8\u7ea7\u8bca\u65ad\u4f46\u8d44\u6e90\u6709\u9650\u7684\u8fdc\u7a0b\u533b\u7597\u73af\u5883\u4e2d\u5c24\u4e3a\u660e\u663e\u3002\u5728\u6b64\u7bc7\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u901a\u7528 MLLM TinyLLaVA \u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u6211\u4eec\u8c03\u6574\u5e76\u91cd\u65b0\u547d\u540d\u6b64\u65b9\u6cd5\u4e3a TinyLLaVA-Med\u3002\u6b64\u8c03\u6574\u5305\u62ec\u6307\u4ee4\u8c03\u6574\u548c\u6839\u636e LLaVA-Med \u8bad\u7ec3\u7ba1\u9053\u6c72\u53d6\u7075\u611f\uff0c\u5bf9\u533b\u5b66\u6570\u636e\u96c6\u8fdb\u884c TinyLLaVA \u5fae\u8c03\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u529f\u8017\u964d\u81f3\u6700\u4f4e\uff0cTinyLLaVA-Med \u5728 18.9W \u4e0b\u8fd0\u884c\uff0c\u4f7f\u7528 11.9GB \u5185\u5b58\uff0c\u540c\u65f6\u5728 VQA-RAD \u4e0a\u5b9e\u73b0 64.54% \u7684\u51c6\u786e\u5ea6\uff0c\u5728 SLAKE \u4e0a\u9488\u5bf9\u5c01\u95ed\u5f0f\u95ee\u9898\u5b9e\u73b0 70.70% \u7684\u51c6\u786e\u5ea6\u3002\u56e0\u6b64\uff0cTinyLLaVA-Med \u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u7ef4\u6301\u4e86\u57fa\u672c\u529f\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u6a21\u578b\u7684\u51c6\u786e\u5ea6\u3002", "author": "Aya El Mir et.al.", "authors": "Aya El Mir, Lukelo Thadei Luoga, Boyuan Chen, Muhammad Abdullah Hanif, Muhammad Shafique", "id": "2409.12184v1", "paper_url": "http://arxiv.org/abs/2409.12184v1", "repo": "null"}}