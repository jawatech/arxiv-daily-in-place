{"2409.12741": {"publish_time": "2024-09-19", "title": "Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization", "paper_summary": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fae\u8abf\u5728\u91ab\u5b78\u9818\u57df\u7684\u4f7f\u7528\u7387\u4e0d\u8db3\u3002\u5fae\u8abf\u6700\u5e38\u898b\u7684\u5169\u7a2e\u65b9\u6cd5\u662f\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u548c\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u4f46\u9bae\u5c11\u6709\u6307\u5357\u544a\u8a34\u4f7f\u7528\u8005\u4f55\u6642\u4f7f\u7528\u9019\u5169\u7a2e\u6280\u8853\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u6bd4\u8f03\u4e86 SFT \u548c DPO \u5728\u91ab\u5b78\u4e2d\u4e94\u9805\u5e38\u898b\u81ea\u7136\u8a9e\u8a00\u4efb\u52d9\u7684\u8868\u73fe\uff1a\u6587\u5b57\u8cc7\u6599\u5206\u985e\u3001\u6578\u5b57\u8cc7\u6599\u5206\u985e\u3001\u81e8\u5e8a\u63a8\u7406\u3001\u6458\u8981\u548c\u81e8\u5e8a\u5206\u6d41\u3002\u6211\u5011\u767c\u73fe\uff0c\u50c5 SFT \u5c31\u8db3\u4ee5\u61c9\u4ed8\u6587\u5b57\u8cc7\u6599\u5206\u985e\uff0c\u800c DPO \u5247\u80fd\u63d0\u5347\u81e8\u5e8a\u63a8\u7406\u3001\u6458\u8981\u548c\u81e8\u5e8a\u5206\u6d41\u7b49\u8f03\u8907\u96dc\u4efb\u52d9\u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u7d50\u679c\u78ba\u7acb\u4e86 DPO \u5fae\u8abf\u5728\u91ab\u5b78\u4e2d\u7684\u89d2\u8272\u548c\u91cd\u8981\u6027\uff0c\u4e26\u56e0\u6b64\u63d0\u9192\u76ee\u524d\u8edf\u9ad4\u7684\u4e0d\u8db3\u4e4b\u8655\uff0c\u9019\u4e9b\u4e0d\u8db3\u4e4b\u8655\u59a8\u7919\u4e86\u6b64\u6280\u8853\u7684\u5ee3\u6cdb\u90e8\u7f72\u3002", "author": "Thomas Savage et.al.", "authors": "Thomas Savage, Stephen Ma, Abdessalem Boukil, Vishwesh Patel, Ekanath Rangan, Ivan Rodriguez, Jonathan H Chen", "id": "2409.12741v2", "paper_url": "http://arxiv.org/abs/2409.12741v2", "repo": "null"}}