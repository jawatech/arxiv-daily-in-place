{"2502.12022": {"publish_time": "2025-02-17", "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving", "paper_summary": "Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.", "paper_summary_zh": "\u73fe\u6709\u7684\u6578\u5b78\u63a8\u7406\u65b9\u6cd5\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ef0\u8cf4\u601d\u8003\u93c8 (CoT) \u4f86\u9054\u5230\u6cdb\u5316\u6027\uff0c\u6216\u4f7f\u7528\u5de5\u5177\u6574\u5408\u63a8\u7406 (TIR) \u4f86\u9032\u884c\u7cbe\u78ba\u904b\u7b97\u3002\u5118\u7ba1\u5df2\u6709\u4eba\u5617\u8a66\u7d50\u5408\u9019\u4e9b\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u4e3b\u8981\u4f9d\u8cf4\u5f8c\u9078\u53d6\u6216\u9810\u5b9a\u7fa9\u7b56\u7565\uff0c\u7559\u4e0b\u4e00\u500b\u958b\u653e\u6027\u7684\u554f\u984c\uff1aLLM \u662f\u5426\u80fd\u6839\u64da\u5176\u5167\u5728\u80fd\u529b\u81ea\u4e3b\u8abf\u6574\u5176\u63a8\u7406\u7b56\u7565\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa TATA\uff08\u6839\u64da\u5176\u5929\u8ce6\u4f86\u6559\u6388 LLM\uff09\uff0c\u9019\u662f\u4e00\u500b\u9069\u61c9\u6027\u67b6\u69cb\uff0c\u8b93 LLM \u80fd\u5920\u81ea\u767c\u5730\u500b\u4eba\u5316\u5176\u63a8\u7406\u7b56\u7565\uff0c\u4e26\u8207\u5176\u5167\u5728\u7684\u5929\u8ce6\u4fdd\u6301\u4e00\u81f4\u3002TATA \u5728\u76e3\u7763\u5fae\u8abf (SFT) \u671f\u9593\u7d0d\u5165\u4e86\u57fa\u790e LLM \u611f\u77e5\u8cc7\u6599\u9078\u53d6\uff0c\u4ee5\u6839\u64da\u6a21\u578b\u7684\u7368\u7279\u80fd\u529b\u8abf\u6574\u8a13\u7df4\u8cc7\u6599\u3002\u6b64\u65b9\u6cd5\u8b93 LLM \u80fd\u5920\u5728\u6e2c\u8a66\u6642\u81ea\u4e3b\u6c7a\u5b9a\u4e26\u5957\u7528\u9069\u7576\u7684\u63a8\u7406\u7b56\u7565\u3002\u6211\u5011\u900f\u904e\u5c0d\u516d\u500b\u6578\u5b78\u63a8\u7406\u57fa\u6e96\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u4f86\u8a55\u4f30 TATA\uff0c\u4f7f\u7528\u901a\u7528\u548c\u6578\u5b78\u5c08\u7528 LLM\u3002\u7d93\u9a57\u7d50\u679c\u986f\u793a\uff0cTATA \u6709\u6548\u5730\u7d50\u5408\u4e86 CoT \u548c TIR \u7684\u4e92\u88dc\u512a\u52e2\uff0c\u8207\u50c5\u4f7f\u7528 TIR \u76f8\u6bd4\uff0c\u9054\u5230\u4e86\u512a\u8d8a\u6216\u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u6539\u5584\u4e86\u63a8\u8ad6\u6548\u7387\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u5f37\u8abf\u4e86\u5929\u8ce6\u611f\u77e5\u8cc7\u6599\u9078\u53d6\u5728\u8b93 LLM \u80fd\u5920\u505a\u51fa\u6709\u6548\u4e14\u9069\u61c9\u6027\u7684\u63a8\u7406\u6c7a\u7b56\uff0c\u4e26\u5c07\u63a8\u7406\u7b56\u7565\u8207\u6a21\u578b\u80fd\u529b\u4fdd\u6301\u4e00\u81f4\u6642\u6240\u626e\u6f14\u7684\u95dc\u9375\u89d2\u8272\u3002", "author": "Xin Xu et.al.", "authors": "Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu", "id": "2502.12022v1", "paper_url": "http://arxiv.org/abs/2502.12022v1", "repo": "null"}}