{"2411.06839": {"publish_time": "2024-11-11", "title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models", "paper_summary": "In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684 LLM-Neo \u6846\u67b6\uff0c\u53ef\u6709\u6548\u5730\u5c07\u77e5\u8b58\u5f9e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6559\u5e2b\u50b3\u8f38\u5230\u4e00\u500b\u7cbe\u7c21\u7684\u5b78\u751f\u3002\u6700\u521d\uff0c\u6211\u5011\u91cd\u65b0\u63a2\u8a0e\u77e5\u8b58\u84b8\u993e (KD) \u548c\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff0c\u4e26\u8ad6\u8b49\u5b83\u5011\u5171\u4eab\u76f8\u540c\u7684\u7bc4\u4f8b\u3002\u53d7\u6b64\u89c0\u5bdf\u555f\u767c\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u7d50\u5408 LoRA \u548c KD \u4ee5\u589e\u5f37\u77e5\u8b58\u50b3\u8f38\u6548\u7387\u7684\u7b56\u7565\u3002\u6211\u5011\u9996\u5148\u7e3d\u7d50\u4e86\u4e00\u4e9b\u95dc\u65bc\u6b64\u8a2d\u8a08\u7684\u6307\u5c0e\u65b9\u91dd\uff0c\u4e26\u9032\u4e00\u6b65\u958b\u767c LLM-Neo\u3002\u5c0d Llama 2 \u548c Llama 3 \u9032\u884c\u58d3\u7e2e\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cLLM-Neo \u512a\u65bc\u5404\u7a2e\u57fa\u7dda\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u7684 LLM-Neo \u5728 LoRA \u8b8a\u9ad4\u4e0a\u7684\u7a69\u5065\u6027\u3002\u8a13\u7df4\u904e\u7684\u6a21\u578b\u5df2\u5728\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{\u6b64\u5132\u5b58\u5eab}\u4e2d\u63d0\u4f9b\u3002", "author": "Runming Yang et.al.", "authors": "Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, Yujiu Yang", "id": "2411.06839v1", "paper_url": "http://arxiv.org/abs/2411.06839v1", "repo": "null"}}