{"2406.09041": {"publish_time": "2024-06-13", "title": "ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models", "paper_summary": "The typical process for developing LLMs involves pre-training a general\nfoundation model on massive data, followed by fine-tuning on task-specific data\nto create specialized experts. Serving these experts poses challenges, as\nloading all experts onto devices is impractical, and frequent switching between\nexperts in response to user requests incurs substantial I/O costs, increasing\nlatency and expenses. Previous approaches decompose expert weights into\npre-trained model weights and residual delta weights, then quantize the delta\nweights to reduce model size. However, these methods often lead to significant\nquantization errors at extremely low bitwidths and assume the appropriate model\nfor a user request is known in advance, which is not practical. To address\nthese issues, we introduce ME-Switch, a memory-efficient expert switching\nframework for LLM serving. ME-Switch uses mixed-precision quantization,\nselectively quantizing non-salient input channels of delta weights to extremely\nlow bits while keeping salient ones intact, significantly reducing storage\ndemands while maintaining performance. Additionally, we develop a routing\nmethod that efficiently directs user queries to the most suitable expert by\ntransforming the model selection problem into a domain classification problem.\nExtensive experiments show ME-Switch's promising memory efficiency and routing\nperformance. For example, when serving three models from the Mistral-7B family,\nME-Switch reduces model size by 1.74x while maintaining nearly lossless\nperformance on instruction, mathematical reasoning, and code generation tasks.\nFurthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B\nfamily on a single NVIDIA A100 GPU.", "paper_summary_zh": "<paragraph>\u958b\u767c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5178\u578b\u6d41\u7a0b\u6d89\u53ca\u5728\u5927\u91cf\u8cc7\u6599\u4e0a\u9810\u5148\u8a13\u7df4\u901a\u7528\u57fa\u790e\u6a21\u578b\uff0c\u7136\u5f8c\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u4ee5\u5efa\u7acb\u5c08\u9580\u7684\u5c08\u5bb6\u3002\u63d0\u4f9b\u9019\u4e9b\u5c08\u5bb6\u670d\u52d9\u6703\u5e36\u4f86\u6311\u6230\uff0c\u56e0\u70ba\u5c07\u6240\u6709\u5c08\u5bb6\u8f09\u5165\u88dd\u7f6e\u4e26\u4e0d\u5be6\u969b\uff0c\u800c\u4e14\u6839\u64da\u4f7f\u7528\u8005\u8981\u6c42\u5728\u5c08\u5bb6\u4e4b\u9593\u983b\u7e41\u5207\u63db\u6703\u7522\u751f\u5927\u91cf\u7684 I/O \u6210\u672c\uff0c\u589e\u52a0\u5ef6\u9072\u548c\u8cbb\u7528\u3002\u5148\u524d\u7684\u505a\u6cd5\u5c07\u5c08\u5bb6\u6b0a\u91cd\u5206\u89e3\u70ba\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u6b0a\u91cd\u548c\u6b98\u5dee delta \u6b0a\u91cd\uff0c\u7136\u5f8c\u5c0d delta \u6b0a\u91cd\u9032\u884c\u91cf\u5316\u4ee5\u6e1b\u5c11\u6a21\u578b\u5927\u5c0f\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u5728\u6975\u4f4e\u7684\u4f4d\u5bec\u7522\u751f\u986f\u8457\u7684\u91cf\u5316\u8aa4\u5dee\uff0c\u4e26\u5047\u8a2d\u4e8b\u5148\u5df2\u77e5\u9053\u4f7f\u7528\u8005\u8981\u6c42\u7684\u9069\u7576\u6a21\u578b\uff0c\u9019\u4e26\u4e0d\u5be6\u969b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 ME-Switch\uff0c\u4e00\u7a2e\u7528\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u670d\u52d9\u7684\u8a18\u61b6\u9ad4\u9ad8\u6548\u5c08\u5bb6\u5207\u63db\u67b6\u69cb\u3002ME-Switch \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff0c\u9078\u64c7\u6027\u5730\u5c07 delta \u6b0a\u91cd\u7684\u975e\u986f\u8457\u8f38\u5165\u901a\u9053\u91cf\u5316\u70ba\u6975\u4f4e\u7684\u4f4d\u5143\uff0c\u540c\u6642\u4fdd\u6301\u986f\u8457\u901a\u9053\u7684\u5b8c\u6574\u6027\uff0c\u986f\u8457\u964d\u4f4e\u5132\u5b58\u9700\u6c42\uff0c\u540c\u6642\u7dad\u6301\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u8def\u7531\u65b9\u6cd5\uff0c\u900f\u904e\u5c07\u6a21\u578b\u9078\u64c7\u554f\u984c\u8f49\u63db\u70ba\u7db2\u57df\u5206\u985e\u554f\u984c\uff0c\u6709\u6548\u5730\u5c07\u4f7f\u7528\u8005\u67e5\u8a62\u5f15\u5c0e\u81f3\u6700\u5408\u9069\u7684\u5c08\u5bb6\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u986f\u793a ME-Switch \u5177\u6709\u826f\u597d\u7684\u8a18\u61b6\u9ad4\u6548\u7387\u548c\u8def\u7531\u6548\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u63d0\u4f9b Mistral-7B \u7cfb\u5217\u4e2d\u7684\u4e09\u500b\u6a21\u578b\u6642\uff0cME-Switch \u5c07\u6a21\u578b\u5927\u5c0f\u6e1b\u5c11\u4e86 1.74 \u500d\uff0c\u540c\u6642\u5728\u6307\u4ee4\u3001\u6578\u5b78\u63a8\u7406\u548c\u7a0b\u5f0f\u78bc\u7522\u751f\u4efb\u52d9\u4e0a\u7dad\u6301\u5e7e\u4e4e\u7121\u640d\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0cME-Switch \u53ef\u4ee5\u6709\u6548\u5730\u5728\u55ae\u4e00\u7684 NVIDIA A100 GPU \u4e0a\u63d0\u4f9b Mistral-7B \u7cfb\u5217\u4e2d\u7684 16 \u500b\u6a21\u578b\u3002</paragraph>", "author": "Jing Liu et.al.", "authors": "Jing Liu, Ruihao Gong, Mingyang Zhang, Yefei He, Jianfei Cai, Bohan Zhuang", "id": "2406.09041v1", "paper_url": "http://arxiv.org/abs/2406.09041v1", "repo": "null"}}