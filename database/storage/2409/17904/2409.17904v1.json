{"2409.17904": {"publish_time": "2024-09-26", "title": "Learning to Love Edge Cases in Formative Math Assessment: Using the AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy", "paper_summary": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.", "paper_summary_zh": "\u672c\u8ad6\u6587\u4ecb\u7d39\u4e86 AMMORE\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 53,000 \u500b\u6578\u5b78\u958b\u653e\u5f0f\u554f\u7b54\u914d\u5c0d\u7684\u65b0\u8cc7\u6599\u96c6\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u4f86\u81ea Rori\uff0c\u9019\u662f\u5e7e\u500b\u975e\u6d32\u570b\u5bb6\u7684\u5b78\u751f\u6240\u4f7f\u7528\u7684\u5b78\u7fd2\u5e73\u53f0\uff0c\u4e26\u9032\u884c\u4e86\u5169\u9805\u5be6\u9a57\u4f86\u8a55\u4f30\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8a55\u5206\u7279\u5225\u5177\u6709\u6311\u6230\u6027\u7684\u5b78\u751f\u7b54\u6848\u3002AMMORE \u8cc7\u6599\u96c6\u555f\u7528\u4e86\u5404\u7a2e\u6f5b\u5728\u5206\u6790\uff0c\u4e26\u63d0\u4f9b\u4e86\u7814\u7a76\u5b78\u751f\u6578\u5b78\u80fd\u529b\u5728\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u73fe\u5be6\u4e16\u754c\u6559\u80b2\u74b0\u5883\u4e2d\u7684\u91cd\u8981\u8cc7\u6e90\u3002\u5728\u5be6\u9a57 1 \u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u5404\u7a2e LLM \u9a45\u52d5\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u96f6\u6b21\u5b78\u7fd2\u3001\u5c11\u6b21\u5b78\u7fd2\u548c\u601d\u8003\u93c8\u63d0\u793a\uff0c\u4f86\u8a55\u5206\u898f\u5247\u5f0f\u5206\u985e\u5668\u7121\u6cd5\u6e96\u78ba\u8a55\u5206\u7684 1% \u5b78\u751f\u7b54\u6848\u3002\u6211\u5011\u767c\u73fe\uff0c\u8868\u73fe\u6700\u597d\u7684\u65b9\u6cd5\u2014\u2014\u601d\u8003\u93c8\u63d0\u793a\u2014\u2014\u6e96\u78ba\u5730\u8a55\u5206\u4e86\u9019\u4e9b\u908a\u7de3\u6848\u4f8b\u7684 92%\uff0c\u6709\u6548\u5730\u5c07\u8a55\u5206\u7684\u6574\u9ad4\u6e96\u78ba\u5ea6\u5f9e 98.7% \u63d0\u5347\u81f3 99.9%\u3002\u5728\u5be6\u9a57 2 \u4e2d\uff0c\u6211\u5011\u65e8\u5728\u901a\u904e\u5c07\u8868\u73fe\u6700\u597d\u7684\u57fa\u65bc LLM \u7684\u65b9\u6cd5\u7522\u751f\u7684\u6210\u7e3e\u50b3\u905e\u7d66\u8c9d\u6c0f\u77e5\u8b58\u8ffd\u8e64 (BKT) \u6a21\u578b\uff0c\u4f86\u66f4\u597d\u5730\u4e86\u89e3\u6539\u9032\u7684\u8a55\u5206\u6e96\u78ba\u6027\u7684\u5f8c\u679c\u6709\u6548\u6027\uff0c\u8a72\u6a21\u578b\u4f30\u8a08\u4e86\u5b78\u751f\u5c0d\u7279\u5b9a\u8ab2\u7a0b\u7684\u638c\u63e1\u7a0b\u5ea6\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u500b\u5225\u554f\u984c\u5c64\u9762\u4e0a\u6a21\u578b\u6e96\u78ba\u5ea6\u7684\u76f8\u5c0d\u9069\u5ea6\u7684\u6539\u9032\u53ef\u80fd\u6703\u5c0e\u81f4\u5b78\u751f\u638c\u63e1\u7a0b\u5ea6\u4f30\u8a08\u7684\u986f\u8457\u8b8a\u5316\u3002\u76ee\u524d\u7528\u65bc\u8a55\u5206\u5b78\u751f\u7684\u57fa\u65bc\u898f\u5247\u7684\u5206\u985e\u5668\u932f\u8aa4\u5730\u5206\u985e\u4e86\u5b78\u751f\u5728\u5b8c\u6210\u8ab2\u7a0b\u4e2d\u7684 6.9% \u7684\u638c\u63e1\u72c0\u614b\uff0c\u4f7f\u7528 LLM \u601d\u8003\u93c8\u65b9\u6cd5\u5c07\u6b64\u932f\u8aa4\u5206\u985e\u7387\u964d\u4f4e\u81f3 2.6% \u7684\u5b78\u751f\u3002\u7d9c\u4e0a\u6240\u8ff0\uff0c\u9019\u4e9b\u767c\u73fe\u8868\u660e LLM \u53ef\u80fd\u662f K-12 \u6578\u5b78\u6559\u80b2\u4e2d\u8a55\u5206\u958b\u653e\u5f0f\u554f\u984c\u7684\u5bf6\u8cb4\u5de5\u5177\uff0c\u6709\u53ef\u80fd\u4fc3\u9032\u5728\u5f62\u6210\u6027\u8a55\u4f30\u4e2d\u66f4\u5ee3\u6cdb\u5730\u63a1\u7528\u958b\u653e\u5f0f\u554f\u984c\u3002", "author": "Owen Henkel et.al.", "authors": "Owen Henkel, Hannah Horne-Robinson, Maria Dyshel, Nabil Ch, Baptiste Moreau-Pernet, Ralph Abood", "id": "2409.17904v1", "paper_url": "http://arxiv.org/abs/2409.17904v1", "repo": "null"}}