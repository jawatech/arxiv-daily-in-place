{"2408.01129": {"publish_time": "2024-08-02", "title": "A Survey of Mamba", "paper_summary": "Deep learning, as a vital technique, has sparked a notable revolution in\nartificial intelligence. As the most representative architecture, Transformers\nhave empowered numerous advanced models, especially the large language models\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space\nmodels, has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering from three main\naspects: the advancements of Mamba-based models, the techniques of adapting\nMamba to diverse data, and the applications where Mamba can excel.\nSpecifically, we first recall the foundational knowledge of various\nrepresentative deep learning models and the details of Mamba as preliminaries.\nThen, to showcase the significance of Mamba, we comprehensively review the\nrelated studies focusing on Mamba models' architecture design, data\nadaptability, and applications. Finally, we present an discussion of current\nlimitations and explore various promising research directions to provide deeper\ninsights for future investigations.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u4f5c\u70ba\u4e00\u9805\u91cd\u8981\u6280\u8853\uff0c\u5728\u4eba\u5de5\u667a\u6167\u9818\u57df\u5f15\u767c\u4e86\u4e00\u5834\u986f\u8457\u7684\u9769\u547d\u3002\u4f5c\u70ba\u6700\u5177\u4ee3\u8868\u6027\u7684\u67b6\u69cb\uff0cTransformer \u5df2\u8ce6\u80fd\u591a\u7a2e\u9032\u968e\u6a21\u578b\uff0c\u7279\u5225\u662f\u5305\u542b\u6578\u5341\u5104\u500b\u53c3\u6578\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u6210\u70ba\u6df1\u5ea6\u5b78\u7fd2\u7684\u57fa\u77f3\u3002\u5118\u7ba1\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u5c31\uff0cTransformer \u4ecd\u7136\u9762\u81e8\u56fa\u6709\u7684\u9650\u5236\uff0c\u7279\u5225\u662f\u6ce8\u610f\u529b\u8a08\u7b97\u7684\u4e8c\u6b21\u904b\u7b97\u8907\u96dc\u5ea6\u6240\u5c0e\u81f4\u7684\u8017\u6642\u63a8\u8ad6\u3002\u6700\u8fd1\uff0c\u4e00\u7a2e\u540d\u70ba Mamba \u7684\u65b0\u67b6\u69cb\uff0c\u5f9e\u7d93\u5178\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u5df2\u6210\u70ba\u69cb\u5efa\u57fa\u790e\u6a21\u578b\u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u8207\u5e8f\u5217\u9577\u5ea6\u76f8\u95dc\u7684\u8fd1\u7dda\u6027\u53ef\u64f4\u5145\u6027\u7684\u540c\u6642\uff0c\u63d0\u4f9b\u4e86\u8207 Transformer \u76f8\u7576\u7684\u5efa\u6a21\u80fd\u529b\u3002\u9019\u6fc0\u767c\u4e86\u8d8a\u4f86\u8d8a\u591a\u7684\u7814\u7a76\u7a4d\u6975\u63a2\u7d22 Mamba \u5728\u4e0d\u540c\u9818\u57df\u5be6\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u7684\u6f5b\u529b\u3002\u9451\u65bc\u9019\u7a2e\u5feb\u901f\u7684\u6f14\u8b8a\uff0c\u8feb\u5207\u9700\u8981\u9032\u884c\u7cfb\u7d71\u6027\u7684\u56de\u9867\uff0c\u4ee5\u6574\u5408\u73fe\u6709\u7684 Mamba \u8ce6\u80fd\u6a21\u578b\uff0c\u5c0d\u9019\u7a2e\u65b0\u8208\u6a21\u578b\u67b6\u69cb\u63d0\u4f9b\u5168\u9762\u7684\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6b21\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u5c0d\u6700\u8fd1\u8207 Mamba \u76f8\u95dc\u7684\u7814\u7a76\u9032\u884c\u4e86\u6df1\u5165\u8abf\u67e5\uff0c\u6db5\u84cb\u4e86\u4e09\u500b\u4e3b\u8981\u65b9\u9762\uff1a\u57fa\u65bc Mamba \u7684\u6a21\u578b\u7684\u9032\u5c55\u3001\u5c07 Mamba \u9069\u61c9\u65bc\u4e0d\u540c\u8cc7\u6599\u7684\u6280\u8853\uff0c\u4ee5\u53ca Mamba \u53ef\u4ee5\u767c\u63ee\u512a\u52e2\u7684\u61c9\u7528\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u56de\u9867\u4e86\u5404\u7a2e\u5177\u6709\u4ee3\u8868\u6027\u7684\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u7684\u57fa\u672c\u77e5\u8b58\u548c Mamba \u7684\u8a73\u7d30\u8cc7\u8a0a\u4f5c\u70ba\u9810\u5099\u77e5\u8b58\u3002\u7136\u5f8c\uff0c\u70ba\u4e86\u5c55\u793a Mamba \u7684\u91cd\u8981\u6027\uff0c\u6211\u5011\u5168\u9762\u56de\u9867\u4e86\u5c08\u6ce8\u65bc Mamba \u6a21\u578b\u7684\u67b6\u69cb\u8a2d\u8a08\u3001\u8cc7\u6599\u9069\u61c9\u6027\u548c\u61c9\u7528\u76f8\u95dc\u7684\u7814\u7a76\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5c0d\u7576\u524d\u9650\u5236\u7684\u8a0e\u8ad6\uff0c\u4e26\u63a2\u8a0e\u4e86\u5404\u7a2e\u6709\u524d\u9014\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u671f\u70ba\u672a\u4f86\u7684\u8abf\u67e5\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u898b\u89e3\u3002", "author": "Haohao Qu et.al.", "authors": "Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Xin Xu, Qing Li", "id": "2408.01129v1", "paper_url": "http://arxiv.org/abs/2408.01129v1", "repo": "null"}}