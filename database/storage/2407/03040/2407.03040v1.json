{"2407.03040": {"publish_time": "2024-07-03", "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model", "paper_summary": "Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields.", "paper_summary_zh": "\u6307\u4ee4\u5fae\u8abf\u4f5c\u70ba\u4e00\u7a2e\u6709\u6548\u6280\u8853\uff0c\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u51fa\u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u4f46\u5982\u4f55\u5f9e\u539f\u59cb\u6587\u4ef6\u751f\u6210\u5b63\u7bc0\u6027\u591a\u8f2a\u5c0d\u8a71\u4ee5\u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u4ecd\u9700\u8981\u9032\u4e00\u6b65\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba R2S \u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5c0d\u8a71\u908f\u8f2f\u7684 CoD \u93c8\u4f86\u6307\u5c0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u751f\u6210\u77e5\u8b58\u5bc6\u96c6\u578b\u591a\u8f2a\u5c0d\u8a71\u4ee5\u9032\u884c\u6307\u4ee4\u5fae\u8abf\u3002\u901a\u904e\u5c07\u4f86\u81ea\u958b\u6e90\u6578\u64da\u96c6\u548c\u7279\u5b9a\u9818\u57df\u7db2\u8def\u722c\u87f2\u6587\u4ef6\u7684\u539f\u59cb\u6587\u4ef6\u6574\u5408\u5230\u57fa\u6e96 K-BENCH \u4e2d\uff0c\u6211\u5011\u6db5\u84cb\u4e86\u7dad\u57fa\u767e\u79d1\uff08\u82f1\u8a9e\uff09\u3001\u79d1\u5b78\uff08\u4e2d\u6587\uff09\u548c\u6587\u7269\uff08\u4e2d\u6587\uff09\u7b49\u591a\u500b\u9818\u57df\u3002\u6211\u5011\u7684\u505a\u6cd5\u9996\u5148\u6c7a\u5b9a\u7576\u524d\u5c0d\u8a71\u7684\u908f\u8f2f\u6d41\uff0c\u7136\u5f8c\u63d0\u793a LLM \u7522\u751f\u95dc\u9375\u77ed\u8a9e\u4ee5\u7372\u53d6\u76f8\u95dc\u56de\u61c9\u5167\u5bb9\u3002\u9019\u7a2e\u65b9\u6cd5\u80fd\u5920\u5275\u5efa G I NSTRUCT \u6307\u4ee4\u6578\u64da\u96c6\uff0c\u5728\u5c0d\u8a71\u5f0f\u4e92\u52d5\u4e2d\u4fdd\u7559\u539f\u59cb\u6587\u4ef6\u77e5\u8b58\u3002\u5229\u7528\u9019\u500b\u6578\u64da\u96c6\uff0c\u6211\u5011\u5fae\u8abf\u4e86 GLLM\uff0c\u9019\u662f\u4e00\u500b\u65e8\u5728\u5c07\u539f\u59cb\u6587\u4ef6\u8f49\u63db\u70ba\u7d50\u69cb\u5316\u591a\u8f2a\u5c0d\u8a71\u7684\u6a21\u578b\uff0c\u5f9e\u800c\u5c07\u5168\u9762\u7684\u9818\u57df\u77e5\u8b58\u6ce8\u5165 SFT \u6a21\u578b\u4ee5\u589e\u5f37\u6307\u4ee4\u5fae\u8abf\u3002\u9019\u9805\u5de5\u4f5c\u6a19\u8a8c\u8457\u5728\u8655\u7406\u548c\u751f\u6210\u66f4\u6e96\u78ba\u3001\u5728\u8a9e\u5883\u4e0a\u66f4\u5fae\u5999\u7684\u56de\u61c9\u65b9\u9762\uff0c\u9081\u9032\u4e86\u6539\u9032 LLM \u7684\u9069\u61c9\u6027\u548c\u6709\u6548\u6027\u7684\u6b65\u4f10\uff0c\u9019\u4e9b\u56de\u61c9\u8de8\u8d8a\u4e86\u5404\u500b\u9818\u57df\u3002", "author": "Xia Hou et.al.", "authors": "Xia Hou, Qifeng Li, Jian Yang, Tongliang Li, Linzheng Chai, Xianjie Wu, Hangyuan Ji, Zhoujun Li, Jixuan Nie, Jingbo Dun, Wenfeng Song", "id": "2407.03040v1", "paper_url": "http://arxiv.org/abs/2407.03040v1", "repo": "null"}}