{"2406.09315": {"publish_time": "2024-06-13", "title": "Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers", "paper_summary": "In this paper, we show how Transformers can be interpreted as dense\nExpectation-Maximization algorithms performed on Bayesian Nets. Based on the\nabove interpretation, we propose a new model design paradigm, namely Vertical\nLoRA (VLoRA), which reduces the parameter count dramatically while preserving\nperformance. In VLoRA, a model consists of layers, each of which recursively\nlearns an increment based on the previous layer. We then apply LoRA\ndecomposition to the increments. VLoRA works on the base model, which is\northogonal to LoRA, meaning they can be used together. We do experiments on\nvarious tasks and models. The results show that 1) with VLoRA, the Transformer\nmodel parameter count can be reduced dramatically and 2) the performance of the\noriginal model is preserved. The source code is available at\n\\url{https://github.com/neverUseThisName/vlora}", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5982\u4f55\u5c07 Transformer \u89e3\u91cb\u70ba\u5728\u8c9d\u6c0f\u7db2\u8def\u4e2d\u57f7\u884c\u7684\u5bc6\u96c6\u671f\u671b\u6700\u5927\u5316\u6f14\u7b97\u6cd5\u3002\u6839\u64da\u4e0a\u8ff0\u89e3\u91cb\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u6a21\u578b\u8a2d\u8a08\u7bc4\u4f8b\uff0c\u5373\u5782\u76f4 LoRA (VLoRA)\uff0c\u5b83\u5728\u4fdd\u7559\u6548\u80fd\u7684\u540c\u6642\u5927\u5e45\u6e1b\u5c11\u4e86\u53c3\u6578\u6578\u91cf\u3002\u5728 VLoRA \u4e2d\uff0c\u6a21\u578b\u7531\u5404\u500b\u5c64\u7d44\u6210\uff0c\u6bcf\u500b\u5c64\u90fd\u6703\u905e\u8ff4\u5730\u6839\u64da\u524d\u4e00\u5c64\u5b78\u7fd2\u589e\u91cf\u3002\u7136\u5f8c\u6211\u5011\u5c07 LoRA \u5206\u89e3\u61c9\u7528\u65bc\u589e\u91cf\u3002VLoRA \u904b\u4f5c\u65bc\u57fa\u790e\u6a21\u578b\uff0c\u9019\u8207 LoRA \u662f\u6b63\u4ea4\u7684\uff0c\u610f\u5373\u5b83\u5011\u53ef\u4ee5\u4e00\u8d77\u4f7f\u7528\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u4efb\u52d9\u548c\u6a21\u578b\u9032\u884c\u4e86\u5be6\u9a57\u3002\u7d50\u679c\u8868\u660e\uff0c1) \u4f7f\u7528 VLoRA\uff0cTransformer \u6a21\u578b\u53c3\u6578\u6578\u91cf\u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\uff0c2) \u539f\u59cb\u6a21\u578b\u7684\u6548\u80fd\u5f97\u4ee5\u4fdd\u7559\u3002\u539f\u59cb\u78bc\u53ef\u5728 \\url{https://github.com/neverUseThisName/vlora} \u53d6\u5f97", "author": "Zhuolin Fu et.al.", "authors": "Zhuolin Fu", "id": "2406.09315v1", "paper_url": "http://arxiv.org/abs/2406.09315v1", "repo": "https://github.com/neverUseThisName/vlora"}}