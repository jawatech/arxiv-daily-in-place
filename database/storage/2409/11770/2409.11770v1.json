{"2409.11770": {"publish_time": "2024-09-18", "title": "Knowledge Adaptation Network for Few-Shot Class-Incremental Learning", "paper_summary": "Few-shot class-incremental learning (FSCIL) aims to incrementally recognize\nnew classes using a few samples while maintaining the performance on previously\nlearned classes. One of the effective methods to solve this challenge is to\nconstruct prototypical evolution classifiers. Despite the advancement achieved\nby most existing methods, the classifier weights are simply initialized using\nmean features. Because representations for new classes are weak and biased, we\nargue such a strategy is suboptimal. In this paper, we tackle this issue from\ntwo aspects. Firstly, thanks to the development of foundation models, we employ\na foundation model, the CLIP, as the network pedestal to provide a general\nrepresentation for each class. Secondly, to generate a more reliable and\ncomprehensive instance representation, we propose a Knowledge Adapter (KA)\nmodule that summarizes the data-specific knowledge from training data and fuses\nit into the general representation. Additionally, to tune the knowledge learned\nfrom the base classes to the upcoming classes, we propose a mechanism of\nIncremental Pseudo Episode Learning (IPEL) by simulating the actual FSCIL.\nTaken together, our proposed method, dubbed as Knowledge Adaptation Network\n(KANet), achieves competitive performance on a wide range of datasets,\nincluding CIFAR100, CUB200, and ImageNet-R.", "paper_summary_zh": "\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60 (FSCIL) \u65e8\u5728\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u589e\u91cf\u8bc6\u522b\u65b0\u7c7b\u522b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u5b66\u4e60\u7c7b\u522b\u7684\u6027\u80fd\u3002\u89e3\u51b3\u6b64\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u4e4b\u4e00\u662f\u6784\u5efa\u539f\u578b\u8fdb\u5316\u5206\u7c7b\u5668\u3002\u5c3d\u7ba1\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5206\u7c7b\u5668\u6743\u91cd\u53ea\u662f\u4f7f\u7528\u5e73\u5747\u7279\u5f81\u8fdb\u884c\u521d\u59cb\u5316\u3002\u7531\u4e8e\u65b0\u7c7b\u522b\u7684\u8868\u793a\u5f88\u5f31\u4e14\u6709\u504f\u5dee\uff0c\u6211\u4eec\u8ba4\u4e3a\u8fd9\u79cd\u7b56\u7565\u4e0d\u662f\u6700\u4f18\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u4e24\u4e2a\u65b9\u9762\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u9996\u5148\uff0c\u5f97\u76ca\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u7840\u6a21\u578b CLIP \u4f5c\u4e3a\u7f51\u7edc\u57fa\u5ea7\uff0c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u63d0\u4f9b\u901a\u7528\u8868\u793a\u3002\u5176\u6b21\uff0c\u4e3a\u4e86\u751f\u6210\u66f4\u53ef\u9760\u548c\u5168\u9762\u7684\u5b9e\u4f8b\u8868\u793a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u9002\u914d\u5668 (KA) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u603b\u7ed3\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u4e8e\u6570\u636e\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u878d\u5408\u5230\u901a\u7528\u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5c06\u4ece\u57fa\u7840\u7c7b\u522b\u5b66\u5230\u7684\u77e5\u8bc6\u8c03\u6574\u5230\u5373\u5c06\u5230\u6765\u7684\u7c7b\u522b\uff0c\u6211\u4eec\u901a\u8fc7\u6a21\u62df\u5b9e\u9645 FSCIL \u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u4f2a\u5e8f\u5217\u5b66\u4e60 (IPEL) \u673a\u5236\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u79f0\u4e3a\u77e5\u8bc6\u9002\u5e94\u7f51\u7edc (KANet)\uff0c\u5728\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5305\u62ec CIFAR100\u3001CUB200 \u548c ImageNet-R\u3002", "author": "Ye Wang et.al.", "authors": "Ye Wang, Yaxiong Wang, Guoshuai Zhao, Xueming Qian", "id": "2409.11770v1", "paper_url": "http://arxiv.org/abs/2409.11770v1", "repo": "null"}}