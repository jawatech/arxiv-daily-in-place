{"2502.03009": {"publish_time": "2025-02-05", "title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models", "paper_summary": "Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints.", "paper_summary_zh": "\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u5927\u91cf\u7684\u8cc7\u6e90\uff0c\u5373\u4f7f\u4f7f\u7528\u9ad8\u968e GPU \u96c6\u7fa4\uff0c\u4e5f\u5e38\u5e38\u9700\u8981\u6578\u500b\u6708\u7684\u8a13\u7df4\u6642\u9593\u3002\u6709\u5169\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u6e1b\u8f15\u9019\u7a2e\u904b\u7b97\u9700\u6c42\uff1a\u91cd\u8907\u4f7f\u7528\u8f03\u5c0f\u7684\u6a21\u578b\u4f86\u8a13\u7df4\u8f03\u5927\u7684\u6a21\u578b\uff08\u5347\u7d1a\uff09\uff0c\u4ee5\u53ca\u8a13\u7df4\u904b\u7b97\u6548\u7387\u9ad8\u7684\u6a21\u578b\uff0c\u4f8b\u5982\u5c08\u5bb6\u6df7\u5408\uff08MoE\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86 LLM \u5230 MoE \u6a21\u578b\u7684\u5347\u7d1a\uff0c\u5176\u4e2d\u898f\u6a21\u884c\u70ba\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u627e\u51fa\u7d93\u9a57\u6027\u898f\u6a21\u5b9a\u5f8b\uff0c\u8aaa\u660e\u6548\u80fd\u5982\u4f55\u53d6\u6c7a\u65bc\u8cc7\u6599\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u7d44\u614b\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5728\u64f4\u5c55\u9019\u4e9b\u56e0\u7d20\u6642\u6703\u6539\u5584\u6548\u80fd\uff0c\u4f46\u5bc6\u96c6\u548c\u5347\u7d1a\u8a13\u7df4\u8cc7\u6599\u96c6\u4e4b\u9593\u6709\u4e00\u500b\u65b0\u7684\u4ea4\u4e92\u4f5c\u7528\u9805\uff0c\u6703\u9650\u5236\u5728\u5927\u578b\u904b\u7b97\u9810\u7b97\u4e2d\u5347\u7d1a\u7684\u6548\u7387\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u64f4\u5c55\u5347\u7d1a\u7684\u6307\u5c0e\u65b9\u91dd\uff0c\u4e26\u5efa\u7acb\u4e86\u5728\u9810\u7b97\u9650\u5236\u5167\uff0c\u5347\u7d1a\u512a\u65bc\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u689d\u4ef6\u3002", "author": "Seng Pei Liew et.al.", "authors": "Seng Pei Liew, Takuya Kato, Sho Takase", "id": "2502.03009v1", "paper_url": "http://arxiv.org/abs/2502.03009v1", "repo": "null"}}