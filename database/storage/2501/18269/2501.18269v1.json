{"2501.18269": {"publish_time": "2025-01-30", "title": "MAMS: Model-Agnostic Module Selection Framework for Video Captioning", "paper_summary": "Multi-modal transformers are rapidly gaining attention in video captioning\ntasks. Existing multi-modal video captioning methods typically extract a fixed\nnumber of frames, which raises critical challenges. When a limited number of\nframes are extracted, important frames with essential information for caption\ngeneration may be missed. Conversely, extracting an excessive number of frames\nincludes consecutive frames, potentially causing redundancy in visual tokens\nextracted from consecutive video frames. To extract an appropriate number of\nframes for each video, this paper proposes the first model-agnostic module\nselection framework in video captioning that has two main functions: (1)\nselecting a caption generation module with an appropriate size based on visual\ntokens extracted from video frames, and (2) constructing subsets of visual\ntokens for the selected caption generation module. Furthermore, we propose a\nnew adaptive attention masking scheme that enhances attention on important\nvisual tokens. Our experiments on three different benchmark datasets\ndemonstrate that the proposed framework significantly improves the performance\nof three recent video captioning models.", "paper_summary_zh": "\u591a\u6a21\u6001 Transformer \u5728\u5f71\u7247\u5b57\u5e55\u4efb\u52a1\u4e2d\u8fc5\u901f\u83b7\u5f97\u5173\u6ce8\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5f71\u7247\u5b57\u5e55\u65b9\u6cd5\u901a\u5e38\u4f1a\u8403\u53d6\u56fa\u5b9a\u6570\u91cf\u7684\u5e27\uff0c\u8fd9\u5e26\u6765\u4e86\u5173\u952e\u7684\u6311\u6218\u3002\u5f53\u8403\u53d6\u6709\u9650\u6570\u91cf\u7684\u5e27\u65f6\uff0c\u53ef\u80fd\u4f1a\u6f0f\u6389\u5305\u542b\u5b57\u5e55\u751f\u6210\u5fc5\u8981\u8d44\u8baf\u7684\u91cd\u8981\u5e27\u3002\u76f8\u53cd\u5730\uff0c\u8403\u53d6\u8fc7\u591a\u7684\u5e27\u4f1a\u5305\u542b\u8fde\u7eed\u7684\u5e27\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4ece\u8fde\u7eed\u5f71\u7247\u5e27\u8403\u53d6\u7684\u89c6\u89c9\u6807\u8bb0\u51fa\u73b0\u5197\u4f59\u3002\u4e3a\u4e86\u8403\u53d6\u9002\u5f53\u6570\u91cf\u7684\u5e27\u7ed9\u6bcf\u90e8\u5f71\u7247\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u5f71\u7247\u5b57\u5e55\u4e2d\u7684\u7b2c\u4e00\u4e2a\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6a21\u5757\u9009\u62e9\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6709\u4e24\u4e2a\u4e3b\u8981\u529f\u80fd\uff1a(1) \u6839\u636e\u4ece\u5f71\u7247\u5e27\u8403\u53d6\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u9009\u62e9\u5177\u6709\u9002\u5f53\u5927\u5c0f\u7684\u5b57\u5e55\u751f\u6210\u6a21\u5757\uff0c\u4ee5\u53ca (2) \u4e3a\u9009\u5b9a\u7684\u5b57\u5e55\u751f\u6210\u6a21\u5757\u5efa\u6784\u89c6\u89c9\u6807\u8bb0\u5b50\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u906e\u7f69\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u589e\u5f3a\u4e86\u5bf9\u91cd\u8981\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u663e\u8457\u5730\u6539\u5584\u4e86\u4e09\u4e2a\u8fd1\u671f\u5f71\u7247\u5b57\u5e55\u6a21\u578b\u7684\u6548\u80fd\u3002", "author": "Sangho Lee et.al.", "authors": "Sangho Lee, Il Yong Chun, Hogun Park", "id": "2501.18269v1", "paper_url": "http://arxiv.org/abs/2501.18269v1", "repo": "null"}}