{"2412.09467": {"publish_time": "2024-12-12", "title": "Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio Deepfake Detection", "paper_summary": "With the rapid development of artificial intelligence technology, the\napplication of deepfake technology in the audio field has gradually increased,\nresulting in a wide range of security risks. Especially in the financial and\nsocial security fields, the misuse of deepfake audios has raised serious\nconcerns. To address this challenge, this study proposes an audio deepfake\ndetection method based on multi-frequency channel attention mechanism (MFCA)\nand 2D discrete cosine transform (DCT). By processing the audio signal into a\nmelspectrogram, using MobileNet V2 to extract deep features, and combining it\nwith the MFCA module to weight different frequency channels in the audio\nsignal, this method can effectively capture the fine-grained frequency domain\nfeatures in the audio signal and enhance the Classification capability of fake\naudios. Experimental results show that compared with traditional methods, the\nmodel proposed in this study shows significant advantages in accuracy,\nprecision,recall, F1 score and other indicators. Especially in complex audio\nscenarios, this method shows stronger robustness and generalization\ncapabilities and provides a new idea for audio deepfake detection and has\nimportant practical application value. In the future, more advanced audio\ndetection technologies and optimization strategies will be explored to further\nimprove the accuracy and generalization capabilities of audio deepfake\ndetection.", "paper_summary_zh": "\u96a8\u8457\u4eba\u5de5\u667a\u6167\u6280\u8853\u7684\u5feb\u901f\u767c\u5c55\uff0c\u6df1\u5ea6\u507d\u9020\u6280\u8853\u5728\u97f3\u8a0a\u9818\u57df\u7684\u61c9\u7528\u9010\u6f38\u589e\u591a\uff0c\u5c0e\u81f4\u5b89\u5168\u6027\u98a8\u96aa\u7bc4\u570d\u5ee3\u6cdb\u3002\u7279\u5225\u662f\u5728\u91d1\u878d\u3001\u793e\u6703\u5b89\u5168\u9818\u57df\uff0c\u6df1\u5ea6\u507d\u9020\u97f3\u8a0a\u7684\u6feb\u7528\u5f15\u767c\u4e86\u56b4\u91cd\u7684\u95dc\u6ce8\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u591a\u983b\u9053\u983b\u9053\u6ce8\u610f\u529b\u6a5f\u5236 (MFCA) \u548c 2D \u96e2\u6563\u9918\u5f26\u8b8a\u63db (DCT) \u7684\u97f3\u8a0a\u6df1\u5ea6\u507d\u9020\u6aa2\u6e2c\u65b9\u6cd5\u3002\u901a\u904e\u5c07\u97f3\u8a0a\u8a0a\u865f\u8655\u7406\u6210\u6885\u723e\u983b\u8b5c\u5716\uff0c\u4f7f\u7528 MobileNet V2 \u63d0\u53d6\u6df1\u5ea6\u7279\u5fb5\uff0c\u4e26\u7d50\u5408 MFCA \u6a21\u7d44\u5c0d\u97f3\u8a0a\u8a0a\u865f\u4e2d\u7684\u4e0d\u540c\u983b\u7387\u901a\u9053\u9032\u884c\u52a0\u6b0a\uff0c\u6b64\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u6355\u6349\u97f3\u8a0a\u8a0a\u865f\u4e2d\u7684\u7d30\u7c92\u5ea6\u983b\u57df\u7279\u5fb5\uff0c\u4e26\u589e\u5f37\u507d\u9020\u97f3\u8a0a\u7684\u5206\u985e\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u50b3\u7d71\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6e96\u78ba\u5ea6\u3001\u7cbe\u78ba\u5ea6\u3001\u53ec\u56de\u7387\u3001F1 \u5206\u6578\u7b49\u6307\u6a19\u4e0a\u8868\u73fe\u51fa\u986f\u8457\u7684\u512a\u52e2\u3002\u7279\u5225\u662f\u5728\u8907\u96dc\u7684\u97f3\u8a0a\u5834\u666f\u4e2d\uff0c\u6b64\u65b9\u6cd5\u5c55\u73fe\u51fa\u66f4\u5f37\u7684\u9b6f\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u70ba\u97f3\u8a0a\u6df1\u5ea6\u507d\u9020\u6aa2\u6e2c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u7684\u5be6\u52d9\u61c9\u7528\u50f9\u503c\u3002\u672a\u4f86\u5c07\u63a2\u7d22\u66f4\u591a\u9032\u968e\u7684\u97f3\u8a0a\u6aa2\u6e2c\u6280\u8853\u548c\u6700\u4f73\u5316\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u97f3\u8a0a\u6df1\u5ea6\u507d\u9020\u6aa2\u6e2c\u7684\u6e96\u78ba\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "author": "Yangguang Feng et.al.", "authors": "Yangguang Feng", "id": "2412.09467v1", "paper_url": "http://arxiv.org/abs/2412.09467v1", "repo": "null"}}