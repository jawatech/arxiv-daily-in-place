{"2405.12532": {"publish_time": "2024-05-21", "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference", "paper_summary": "Large Language Models (LLMs) have shown remarkable comprehension abilities\nbut face challenges in GPU memory usage during inference, hindering their\nscalability for real-time applications like chatbots. To accelerate inference,\nwe store computed keys and values (KV cache) in the GPU memory. Existing\nmethods study the KV cache compression to reduce memory by pruning the\npre-computed KV cache. However, they neglect the inter-layer dependency between\nlayers and huge memory consumption in pre-computation. To explore these\ndeficiencies, we find that the number of crucial keys and values that influence\nfuture generations decreases layer by layer and we can extract them by the\nconsistency in attention weights. Based on the findings, we propose\nPyramidInfer, a method that compresses the KV cache by layer-wise retaining\ncrucial context. PyramidInfer saves significant memory by computing fewer keys\nand values without sacrificing performance. Experimental results show\nPyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU\nmemory reduction in KV cache.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u63a8\u8ad6\u904e\u7a0b\u4e2d\u9762\u81e8 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u7387\u7684\u6311\u6230\uff0c\u963b\u7919\u4e86\u5b83\u5011\u5728\u804a\u5929\u6a5f\u5668\u4eba\u7b49\u5373\u6642\u61c9\u7528\u7a0b\u5f0f\u4e2d\u7684\u53ef\u64f4\u5145\u6027\u3002\u70ba\u4e86\u52a0\u901f\u63a8\u8ad6\uff0c\u6211\u5011\u5c07\u8a08\u7b97\u51fa\u7684\u91d1\u9470\u548c\u503c (KV \u5feb\u53d6) \u5132\u5b58\u5728 GPU \u8a18\u61b6\u9ad4\u4e2d\u3002\u73fe\u6709\u65b9\u6cd5\u7814\u7a76 KV \u5feb\u53d6\u58d3\u7e2e\uff0c\u900f\u904e\u4fee\u526a\u9810\u5148\u8a08\u7b97\u7684 KV \u5feb\u53d6\u4f86\u6e1b\u5c11\u8a18\u61b6\u9ad4\u3002\u7136\u800c\uff0c\u5b83\u5011\u5ffd\u7565\u4e86\u5c64\u8207\u5c64\u4e4b\u9593\u7684\u5c64\u9593\u76f8\u4f9d\u6027\uff0c\u4ee5\u53ca\u9810\u5148\u8a08\u7b97\u4e2d\u7684\u9f90\u5927\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u70ba\u4e86\u63a2\u8a0e\u9019\u4e9b\u4e0d\u8db3\u4e4b\u8655\uff0c\u6211\u5011\u767c\u73fe\u5f71\u97ff\u5f8c\u4ee3\u7684\u91d1\u9470\u548c\u503c\u6578\u91cf\u6703\u9010\u5c64\u905e\u6e1b\uff0c\u800c\u4e14\u6211\u5011\u53ef\u4ee5\u900f\u904e\u6ce8\u610f\u529b\u6b0a\u91cd\u7684\u76f8\u5bb9\u6027\u4f86\u8403\u53d6\u5b83\u5011\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa PyramidInfer\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u9010\u5c64\u4fdd\u7559\u95dc\u9375\u5167\u5bb9\u4f86\u58d3\u7e2e KV \u5feb\u53d6\u7684\u65b9\u6cd5\u3002PyramidInfer \u900f\u904e\u8a08\u7b97\u8f03\u5c11\u7684\u91d1\u9470\u548c\u503c\u4f86\u7bc0\u7701\u5927\u91cf\u8a18\u61b6\u9ad4\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207 Accelerate \u76f8\u6bd4\uff0cPyramidInfer \u5c07 KV \u5feb\u53d6\u7684 GPU \u8a18\u61b6\u9ad4\u6e1b\u5c11\u4e86 54% \u4ee5\u4e0a\uff0c\u540c\u6642\u5c07\u8655\u7406\u91cf\u63d0\u5347\u4e86 2.2 \u500d\u3002", "author": "Dongjie Yang et.al.", "authors": "Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, Hai Zhao", "id": "2405.12532v1", "paper_url": "http://arxiv.org/abs/2405.12532v1", "repo": "null"}}