{"2409.11844": {"publish_time": "2024-09-18", "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "paper_summary": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u8a18\u4f4f\u654f\u611f\u8cc7\u8a0a\uff0c\u9019\u5f15\u8d77\u4e86\u4eba\u5011\u5c0d\u6f5b\u5728\u8aa4\u7528\u7684\u64d4\u6182\u3002LLM \u907a\u5fd8\u662f\u4e00\u7a2e\u4e8b\u5f8c\u65b9\u6cd5\uff0c\u7528\u65bc\u5f9e\u8a13\u7df4\u904e\u7684 LLM \u4e2d\u79fb\u9664\u9019\u4e9b\u8cc7\u8a0a\uff0c\u9019\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u6e1b\u8f15\u9019\u4e9b\u98a8\u96aa\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u505a\u6cd5\u9762\u81e8\u4e09\u500b\u95dc\u9375\u6311\u6230\uff1a1. \u5be6\u7528\u6027\uff1a\u6210\u529f\u7684\u907a\u5fd8\u901a\u5e38\u6703\u5c0e\u81f4\u8207\u4efb\u52d9\u7121\u95dc\u7684\u707d\u96e3\u6027\u5d29\u6f70\u30022. \u6548\u7387\uff1a\u8a31\u591a\u65b9\u6cd5\u90fd\u6d89\u53ca\u65b0\u589e\u76f8\u4f3c\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u9019\u6703\u6e1b\u6162\u907a\u5fd8\u6216\u63a8\u7406\u901f\u5ea6\uff0c\u6216\u8005\u9700\u8981\u4fdd\u7559\u96e3\u4ee5\u53d6\u5f97\u7684\u8cc7\u6599\u30023. \u7a69\u5065\u6027\uff1a\u5373\u4f7f\u662f\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e5f\u53ef\u80fd\u900f\u904e\u8403\u53d6\u6280\u8853\u6d29\u6f0f\u8cc7\u6599\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MEOW\uff0c\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u57fa\u65bc\u68af\u5ea6\u4e0b\u964d\u7684\u907a\u5fd8\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u96e2\u7dda LLM \u4f86\u7522\u751f\u4e00\u7d44\u53cd\u5411\u4e8b\u5be6\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u65b0\u7684\u6307\u6a19 MEMO\uff0c\u7528\u65bc\u91cf\u5316 LLM \u4e2d\u7684\u8a18\u61b6\u3002\u6700\u5f8c\uff0c\u6839\u64da MEMO \u63d0\u4f9b\u7684\u4fe1\u865f\uff0c\u6211\u5011\u9078\u64c7\u6700\u5408\u9069\u7684\u53cd\u5411\u4e8b\u5be6\u96c6\uff0c\u4e26\u6839\u64da\u5b83\u5011\u5fae\u8abf\u6a21\u578b\u3002\u6211\u5011\u5728\u5e38\u7528\u7684\u907a\u5fd8\u57fa\u6e96 ToFU \u4e0a\u8a55\u4f30\u4e86 MEOW\uff0c\u5176\u4e2d\u5305\u542b Llama2-7B-Chat \u548c Phi-1.5B\uff0c\u4e26\u5728 NLU \u548c NLG \u4efb\u52d9\u4e0a\u5c0d\u5176\u9032\u884c\u4e86\u6e2c\u8a66\u3002\u7d50\u679c\u8868\u660e\uff0cMEOW \u5728\u907a\u5fd8\u54c1\u8cea\u4e0a\u6709\u4e86\u986f\u8457\u7684\u63d0\u5347\uff0c\u800c\u6a21\u578b\u5be6\u7528\u6027\u6c92\u6709\u986f\u8457\u4e0b\u964d\u3002\u540c\u6642\uff0cMEOW \u5728 NLU \u6216 NLG \u80fd\u529b\u4e0a\u6c92\u6709\u8868\u73fe\u51fa\u986f\u8457\u7684\u4e0b\u964d\uff0c\u751a\u81f3\u5728 NLU \u6027\u80fd\u4e0a\u9084\u7565\u6709\u63d0\u5347\u3002", "author": "Tianle Gu et.al.", "authors": "Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, Yingchun Wang", "id": "2409.11844v1", "paper_url": "http://arxiv.org/abs/2409.11844v1", "repo": "https://github.com/carol-gutianle/meow"}}