{"2409.03905": {"publish_time": "2024-09-05", "title": "CACER: Clinical Concept Annotations for Cancer Events and Relations", "paper_summary": "Clinical notes contain unstructured representations of patient histories,\nincluding the relationships between medical problems and prescription drugs. To\ninvestigate the relationship between cancer drugs and their associated symptom\nburden, we extract structured, semantic representations of medical problem and\ndrug information from the clinical narratives of oncology notes. We present\nClinical Concept Annotations for Cancer Events and Relations (CACER), a novel\ncorpus with fine-grained annotations for over 48,000 medical problems and drug\nevents and 10,000 drug-problem and problem-problem relations. Leveraging CACER,\nwe develop and evaluate transformer-based information extraction (IE) models\nsuch as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context\nlearning (ICL). In event extraction, the fine-tuned BERT and Llama3 models\nachieved the highest performance at 88.2-88.0 F1, which is comparable to the\ninter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the\nfine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at\n61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.\nThe fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the\nimportance of annotated training data and model optimization. Furthermore, the\nBERT models performed similarly to Llama3. For our task, LLMs offer no\nperformance advantage over the smaller BERT models. The results emphasize the\nneed for annotated training data to optimize models. Multiple fine-tuned\ntransformer models achieved performance comparable to IAA for several\nextraction tasks.", "paper_summary_zh": "<paragraph>\u81e8\u5e8a\u7b46\u8a18\u5305\u542b\u60a3\u8005\u75c5\u53f2\u7684\u975e\u7d50\u69cb\u5316\u8868\u793a\uff0c\u5305\u62ec\u91ab\u7642\u554f\u984c\u548c\u8655\u65b9\u85e5\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u70ba\u4e86\u8abf\u67e5\u764c\u75c7\u85e5\u7269\u53ca\u5176\u76f8\u95dc\u75c7\u72c0\u8ca0\u64d4\u4e4b\u9593\u7684\u95dc\u4fc2\uff0c\u6211\u5011\u5f9e\u816b\u7624\u5b78\u7b46\u8a18\u7684\u81e8\u5e8a\u6558\u8ff0\u4e2d\u63d0\u53d6\u4e86\u91ab\u7642\u554f\u984c\u548c\u85e5\u7269\u4fe1\u606f\u7684\u7d50\u69cb\u5316\u8a9e\u7fa9\u8868\u793a\u3002\u6211\u5011\u63d0\u51fa\u4e86\u764c\u75c7\u4e8b\u4ef6\u548c\u95dc\u4fc2\u7684\u81e8\u5e8a\u6982\u5ff5\u8a3b\u91cb (CACER)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u8a9e\u6599\u5eab\uff0c\u5c0d\u8d85\u904e 48,000 \u500b\u91ab\u7642\u554f\u984c\u548c\u85e5\u7269\u4e8b\u4ef6\u4ee5\u53ca 10,000 \u500b\u85e5\u7269\u554f\u984c\u548c\u554f\u984c\u554f\u984c\u95dc\u4fc2\u9032\u884c\u4e86\u7d30\u7c92\u5ea6\u8a3b\u91cb\u3002\u5229\u7528 CACER\uff0c\u6211\u5011\u958b\u767c\u4e26\u8a55\u4f30\u4e86\u57fa\u65bcTransformer\u7684\u4fe1\u606f\u63d0\u53d6 (IE) \u6a21\u578b\uff0c\u4f8b\u5982 BERT\u3001Flan-T5\u3001Llama3 \u548c GPT-4\uff0c\u4f7f\u7528\u5fae\u8abf\u548c\u4e0a\u4e0b\u6587\u5b78\u7fd2 (ICL)\u3002\u5728\u4e8b\u4ef6\u63d0\u53d6\u4e2d\uff0c\u5fae\u8abf\u5f8c\u7684 BERT \u548c Llama3 \u6a21\u578b\u5728 88.2-88.0 F1 \u4e2d\u5be6\u73fe\u4e86\u6700\u9ad8\u6027\u80fd\uff0c\u9019\u8207 88.4 F1 \u7684\u6a19\u8a3b\u9593\u5354\u8b70 (IAA) \u76f8\u7576\u3002\u5728\u95dc\u4fc2\u63d0\u53d6\u4e2d\uff0c\u5fae\u8abf\u5f8c\u7684 BERT\u3001Flan-T5 \u548c Llama3 \u5728 61.8-65.3 F1 \u4e2d\u5be6\u73fe\u4e86\u6700\u9ad8\u6027\u80fd\u3002\u5177\u6709 ICL \u7684 GPT-4 \u5728\u5169\u9805\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u6700\u5dee\u3002\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u5728 ICL \u4e2d\u660e\u986f\u512a\u65bc GPT-4\uff0c\u7a81\u986f\u4e86\u8a3b\u91cb\u8a13\u7df4\u6578\u64da\u548c\u6a21\u578b\u512a\u5316\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0cBERT \u6a21\u578b\u7684\u6027\u80fd\u8207 Llama3 \u985e\u4f3c\u3002\u5c0d\u65bc\u6211\u5011\u7684\u4efb\u52d9\uff0cLLM \u5c0d\u8f03\u5c0f\u7684 BERT \u6a21\u578b\u6c92\u6709\u6027\u80fd\u512a\u52e2\u3002\u7d50\u679c\u5f37\u8abf\u4e86\u5c0d\u8a3b\u91cb\u8a13\u7df4\u6578\u64da\u4ee5\u512a\u5316\u6a21\u578b\u7684\u9700\u6c42\u3002\u591a\u500b\u5fae\u8abf\u5f8c\u7684Transformer\u6a21\u578b\u5be6\u73fe\u4e86\u8207 IAA \u76f8\u7576\u7684\u6027\u80fd\uff0c\u7528\u65bc\u591a\u9805\u63d0\u53d6\u4efb\u52d9\u3002</paragraph>", "author": "Yujuan Fu et.al.", "authors": "Yujuan Fu, Giridhar Kaushik Ramachandran, Ahmad Halwani, Bridget T. McInnes, Fei Xia, Kevin Lybarger, Meliha Yetisgen, \u00d6zlem Uzuner", "id": "2409.03905v1", "paper_url": "http://arxiv.org/abs/2409.03905v1", "repo": "null"}}