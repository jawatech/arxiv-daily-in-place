{"2501.12775": {"publish_time": "2025-01-22", "title": "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation", "paper_summary": "Attention mechanism is contributing to the majority of recent advances in\nmachine learning for natural language processing. Additionally, it results in\nan attention map that shows the proportional influence of each input in its\ndecision. Empirical studies postulate that attention maps can be provided as an\nexplanation for model output. However, it is still questionable to ask whether\nthis explanation helps regular people to understand and accept the model output\n(the plausibility of the explanation). Recent studies show that attention\nweights in the RNN encoders are hardly plausible because they spread on input\ntokens. We thus propose 3 additional constraints to the learning objective\nfunction to improve the plausibility of the attention map: regularization to\nincrease the attention weight sparsity, semi-supervision to supervise the map\nby a heuristic and supervision by human annotation. Results show that all\ntechniques can improve the attention map plausibility at some level. We also\nobserve that specific instructions for human annotation might have a negative\neffect on classification performance. Beyond the attention map, the result of\nexperiments on text classification tasks also shows that no matter how the\nconstraint brings the gain, the contextualization layer plays a crucial role in\nfinding the right space for finding plausible tokens.", "paper_summary_zh": "\u6ce8\u610f\u529b\u673a\u5236\u4fc3\u6210\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6a5f\u5668\u5b78\u7fd2\u6700\u8fd1\u7684\u5927\u90e8\u5206\u9032\u5c55\u3002\u6b64\u5916\uff0c\u5b83\u6703\u7522\u751f\u4e00\u500b\u6ce8\u610f\u529b\u5716\uff0c\u986f\u793a\u6bcf\u500b\u8f38\u5165\u5728\u5176\u6c7a\u7b56\u4e2d\u6240\u4f54\u7684\u6bd4\u4f8b\u5f71\u97ff\u3002\u5be6\u8b49\u7814\u7a76\u5047\u8a2d\u6ce8\u610f\u529b\u5716\u53ef\u4ee5\u4f5c\u70ba\u6a21\u578b\u8f38\u51fa\u7684\u89e3\u91cb\u3002\u7136\u800c\uff0c\u662f\u5426\u9019\u7a2e\u89e3\u91cb\u6709\u52a9\u65bc\u4e00\u822c\u4eba\u7406\u89e3\u4e26\u63a5\u53d7\u6a21\u578b\u8f38\u51fa\uff08\u89e3\u91cb\u7684\u5408\u7406\u6027\uff09\u4ecd\u6709\u5f85\u5546\u69b7\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cRNN \u7de8\u78bc\u5668\u4e2d\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\u5e7e\u4e4e\u4e0d\u5408\u7406\uff0c\u56e0\u70ba\u5b83\u5011\u6563\u4f48\u5728\u8f38\u5165\u7b26\u865f\u4e0a\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5c0d\u5b78\u7fd2\u76ee\u6a19\u51fd\u6578\u63d0\u51fa\u4e86 3 \u500b\u984d\u5916\u7684\u7d04\u675f\uff0c\u4ee5\u63d0\u9ad8\u6ce8\u610f\u529b\u5716\u7684\u5408\u7406\u6027\uff1a\u6b63\u5247\u5316\u4ee5\u589e\u52a0\u6ce8\u610f\u529b\u6b0a\u91cd\u7a00\u758f\u6027\u3001\u534a\u76e3\u7763\u4ee5\u901a\u904e\u555f\u767c\u5f0f\u76e3\u7763\u5716\u4ee5\u53ca\u901a\u904e\u4eba\u5de5\u8a3b\u89e3\u9032\u884c\u76e3\u7763\u3002\u7d50\u679c\u8868\u660e\uff0c\u6240\u6709\u6280\u8853\u90fd\u53ef\u4ee5\u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u6ce8\u610f\u529b\u5716\u7684\u5408\u7406\u6027\u3002\u6211\u5011\u9084\u89c0\u5bdf\u5230\uff0c\u4eba\u5de5\u8a3b\u89e3\u7684\u5177\u9ad4\u8aaa\u660e\u53ef\u80fd\u6703\u5c0d\u5206\u985e\u6027\u80fd\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u9664\u4e86\u6ce8\u610f\u529b\u5716\u4e4b\u5916\uff0c\u6587\u672c\u5206\u985e\u4efb\u52d9\u7684\u5be6\u9a57\u7d50\u679c\u9084\u8868\u660e\uff0c\u7121\u8ad6\u7d04\u675f\u5982\u4f55\u5e36\u4f86\u6536\u76ca\uff0c\u60c5\u5883\u5316\u5c64\u5728\u627e\u5230\u5408\u7406\u7b26\u865f\u7684\u6b63\u78ba\u7a7a\u9593\u4e2d\u90fd\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\u3002", "author": "Duc Hau Nguyen et.al.", "authors": "Duc Hau Nguyen, Cyrielle Mallart, Guillaume Gravier, Pascale S\u00e9billot", "id": "2501.12775v1", "paper_url": "http://arxiv.org/abs/2501.12775v1", "repo": "https://github.com/kihansi95/linkmedia_attentionplausibilitybyconstraint"}}