{"2412.13178": {"publish_time": "2024-12-17", "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "paper_summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6574\u5408\uff0c\u5177\u8c61\u4ee3\u7406\u64c1\u6709\u4ee5\u81ea\u7136\u8a9e\u8a00\u57f7\u884c\u8907\u96dc\u6307\u4ee4\u7684\u5f37\u5927\u80fd\u529b\uff0c\u70ba\u5177\u8c61\u6a5f\u5668\u4eba\u7684\u6f5b\u5728\u90e8\u7f72\u92ea\u5e73\u4e86\u9053\u8def\u3002\u7136\u800c\uff0c\u4e00\u500b\u53ef\u9810\u898b\u7684\u554f\u984c\u662f\uff0c\u9019\u4e9b\u5177\u8c61\u4ee3\u7406\u4e5f\u53ef\u4ee5\u5b8c\u7f8e\u57f7\u884c\u4e00\u4e9b\u5371\u96aa\u7684\u4efb\u52d9\uff0c\u6f5b\u5728\u5730\u5c0d\u73fe\u5be6\u4e16\u754c\u9020\u6210\u640d\u5bb3\u3002\u70ba\u4e86\u7814\u7a76\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SafeAgentBench\u2014\u2014\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u7528\u65bc\u5177\u8c61 LLM \u4ee3\u7406\u7684\u5b89\u5168\u611f\u77e5\u4efb\u52d9\u898f\u5283\u3002SafeAgentBench \u5305\u62ec\uff1a(1) \u4e00\u500b\u64c1\u6709 750 \u500b\u4efb\u52d9\u7684\u65b0\u6578\u64da\u96c6\uff0c\u6db5\u84cb 10 \u500b\u6f5b\u5728\u5371\u5bb3\u548c 3 \u500b\u4efb\u52d9\u985e\u578b\uff1b(2) SafeAgentEnv\uff0c\u4e00\u500b\u5e36\u6709\u4f4e\u7d1a\u63a7\u5236\u5668\u7684\u901a\u7528\u5177\u8c61\u74b0\u5883\uff0c\u652f\u6301\u4f7f\u7528 17 \u500b\u9ad8\u7d1a\u52d5\u4f5c\u5c0d 8 \u500b\u6700\u5148\u9032\u7684\u57fa\u6e96\u9032\u884c\u591a\u4ee3\u7406\u57f7\u884c\uff1b\u4ee5\u53ca (3) \u4f86\u81ea\u57f7\u884c\u548c\u8a9e\u7fa9\u89d2\u5ea6\u7684\u53ef\u9760\u8a55\u4f30\u65b9\u6cd5\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8868\u73fe\u6700\u4f73\u7684\u57fa\u6e96\u5728\u5b89\u5168\u4efb\u52d9\u4e2d\u7372\u5f97\u4e86 69% \u7684\u6210\u529f\u7387\uff0c\u4f46\u5728\u5371\u96aa\u4efb\u52d9\u4e2d\u7684\u62d2\u7d55\u7387\u50c5\u70ba 5%\uff0c\u8868\u660e\u5b58\u5728\u986f\u8457\u7684\u5b89\u5168\u98a8\u96aa\u3002\u66f4\u591a\u8a73\u7d30\u4fe1\u606f\u548c\u4ee3\u78bc\u53ef\u5728 https://github.com/shengyin1224/SafeAgentBench \u4e2d\u627e\u5230\u3002", "author": "Sheng Yin et.al.", "authors": "Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen", "id": "2412.13178v1", "paper_url": "http://arxiv.org/abs/2412.13178v1", "repo": "https://github.com/shengyin1224/safeagentbench"}}