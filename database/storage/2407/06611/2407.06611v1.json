{"2407.06611": {"publish_time": "2024-07-09", "title": "CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding", "paper_summary": "We present CEIA, an effective framework for open-world event-based\nunderstanding. Currently training a large event-text model still poses a huge\nchallenge due to the shortage of paired event-text data. In response to this\nchallenge, CEIA learns to align event and image data as an alternative instead\nof directly aligning event and text data. Specifically, we leverage the rich\nevent-image datasets to learn an event embedding space aligned with the image\nspace of CLIP through contrastive learning. In this way, event and text data\nare naturally aligned via using image data as a bridge. Particularly, CEIA\noffers two distinct advantages. First, it allows us to take full advantage of\nthe existing event-image datasets to make up the shortage of large-scale\nevent-text datasets. Second, leveraging more training data, it also exhibits\nthe flexibility to boost performance, ensuring scalable capability. In\nhighlighting the versatility of our framework, we make extensive evaluations\nthrough a diverse range of event-based multi-modal applications, such as object\nrecognition, event-image retrieval, event-text retrieval, and domain\nadaptation. The outcomes demonstrate CEIA's distinct zero-shot superiority over\nexisting methods on these applications.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa CEIA\uff0c\u4e00\u500b\u958b\u653e\u4e16\u754c\u4e8b\u4ef6\u70ba\u57fa\u790e\u7406\u89e3\u7684\u6709\u6548\u6846\u67b6\u3002\u76ee\u524d\u8a13\u7df4\u4e00\u500b\u5927\u578b\u4e8b\u4ef6\u6587\u5b57\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u500b\u5de8\u5927\u7684\u6311\u6230\uff0c\u56e0\u70ba\u914d\u5c0d\u7684\u4e8b\u4ef6\u6587\u5b57\u8cc7\u6599\u77ed\u7f3a\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0cCEIA \u5b78\u7fd2\u5c07\u4e8b\u4ef6\u548c\u5f71\u50cf\u8cc7\u6599\u5c0d\u9f4a\u4f5c\u70ba\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5c0d\u9f4a\u4e8b\u4ef6\u548c\u6587\u5b57\u8cc7\u6599\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528\u8c50\u5bcc\u7684\u4e8b\u4ef6\u5f71\u50cf\u8cc7\u6599\u96c6\uff0c\u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\uff0c\u5b78\u7fd2\u4e00\u500b\u8207 CLIP \u5f71\u50cf\u7a7a\u9593\u5c0d\u9f4a\u7684\u4e8b\u4ef6\u5d4c\u5165\u7a7a\u9593\u3002\u4ee5\u9019\u7a2e\u65b9\u5f0f\uff0c\u4e8b\u4ef6\u548c\u6587\u5b57\u8cc7\u6599\u900f\u904e\u4f7f\u7528\u5f71\u50cf\u8cc7\u6599\u4f5c\u70ba\u6a4b\u6a11\u800c\u81ea\u7136\u5c0d\u9f4a\u3002\u7279\u5225\u662f\uff0cCEIA \u63d0\u4f9b\u4e86\u5169\u500b\u986f\u8457\u7684\u512a\u9ede\u3002\u9996\u5148\uff0c\u5b83\u8b93\u6211\u5011\u80fd\u5920\u5145\u5206\u5229\u7528\u73fe\u6709\u7684\u4e8b\u4ef6\u5f71\u50cf\u8cc7\u6599\u96c6\uff0c\u4ee5\u5f4c\u88dc\u5927\u898f\u6a21\u4e8b\u4ef6\u6587\u5b57\u8cc7\u6599\u96c6\u7684\u4e0d\u8db3\u3002\u5176\u6b21\uff0c\u5229\u7528\u66f4\u591a\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u5b83\u4e5f\u5c55\u73fe\u4e86\u63d0\u5347\u6548\u80fd\u7684\u9748\u6d3b\u6027\uff0c\u78ba\u4fdd\u53ef\u64f4\u5145\u7684\u80fd\u529b\u3002\u70ba\u4e86\u5f37\u8abf\u6211\u5011\u6846\u67b6\u7684\u591a\u529f\u80fd\u6027\uff0c\u6211\u5011\u900f\u904e\u5404\u7a2e\u4e0d\u540c\u7684\u4e8b\u4ef6\u70ba\u57fa\u790e\u591a\u6a21\u614b\u61c9\u7528\u7a0b\u5f0f\u9032\u884c\u5ee3\u6cdb\u7684\u8a55\u4f30\uff0c\u4f8b\u5982\u7269\u4ef6\u8fa8\u8b58\u3001\u4e8b\u4ef6\u5f71\u50cf\u6aa2\u7d22\u3001\u4e8b\u4ef6\u6587\u5b57\u6aa2\u7d22\u548c\u9818\u57df\u9069\u61c9\u3002\u7d50\u679c\u8b49\u660e CEIA \u5728\u9019\u4e9b\u61c9\u7528\u7a0b\u5f0f\u4e0a\u6bd4\u73fe\u6709\u65b9\u6cd5\u5177\u6709\u986f\u8457\u7684\u96f6\u6b21\u5b78\u7fd2\u512a\u52e2\u3002", "author": "Wenhao Xu et.al.", "authors": "Wenhao Xu, Wenming Weng, Yueyi Zhang, Zhiwei Xiong", "id": "2407.06611v1", "paper_url": "http://arxiv.org/abs/2407.06611v1", "repo": "null"}}