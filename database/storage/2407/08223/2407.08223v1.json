{"2407.08223": {"publish_time": "2024-07-11", "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting", "paper_summary": "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth.", "paper_summary_zh": "\u64b7\u53d6\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u80fd\u529b\u548c\u5916\u90e8\u77e5\u8bc6\u6765\u6e90\uff0c\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u4e14\u6700\u65b0\u7684\u56de\u5e94\u3002\u6700\u8fd1\u7684 RAG \u6539\u8fdb\u91cd\u70b9\u5728\u4e8e\u901a\u8fc7\u8fed\u4ee3 LLM \u4f18\u5316\u6216\u7ecf\u7531 LLM \u7684\u989d\u5916\u6307\u4ee4\u5fae\u8c03\u6240\u83b7\u5f97\u7684\u81ea\u7701\u80fd\u529b\u6765\u6539\u5584\u64b7\u53d6\u7ed3\u679c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u63a8\u6d4b\u6027 RAG\uff0c\u8fd9\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e00\u4e2a\u8f83\u5927\u7684\u901a\u624d LM \u6765\u6709\u6548\u9a8c\u8bc1\u7531\u8f83\u5c0f\u7684\u3001\u7cbe\u70bc\u7684\u4e13\u5bb6 LM \u5e76\u884c\u4ea7\u751f\u7684\u591a\u4e2a RAG \u8349\u7a3f\u3002\u6bcf\u4e2a\u8349\u7a3f\u90fd\u662f\u4ece\u64b7\u53d6\u6587\u4ef6\u7684\u4e00\u4e2a\u4e0d\u540c\u5b50\u96c6\u751f\u6210\u7684\uff0c\u5b83\u63d0\u4f9b\u4e86\u8bc1\u636e\u7684\u4e0d\u540c\u89c2\u70b9\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6bcf\u4e2a\u8349\u7a3f\u7684\u8f93\u5165\u6807\u8bb0\u8ba1\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u589e\u5f3a\u4e86\u5bf9\u6bcf\u4e2a\u5b50\u96c6\u7684\u7406\u89e3\uff0c\u5e76\u51cf\u8f7b\u4e86\u5bf9\u957f\u8bed\u5883\u7684\u6f5c\u5728\u4f4d\u7f6e\u504f\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u8d77\u8349\u59d4\u6d3e\u7ed9\u8f83\u5c0f\u7684\u4e13\u5bb6 LM \u6765\u52a0\u901f RAG\uff0c\u800c\u8f83\u5927\u7684\u901a\u624d LM \u5bf9\u8349\u7a3f\u6267\u884c\u5355\u6b21\u9a8c\u8bc1\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63a8\u6d4b\u6027 RAG \u5728 TriviaQA\u3001MuSiQue\u3001PubHealth \u548c ARC-Challenge \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u5ef6\u8fdf\u66f4\u4f4e\u3002\u4e0e\u4f20\u7edf\u7684 RAG \u7cfb\u7edf\u76f8\u6bd4\uff0c\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u5ea6\uff08\u6700\u9ad8\u63d0\u9ad8 12.97%\uff09\uff0c\u540c\u65f6\u5c06\u5ef6\u8fdf\u964d\u4f4e\u4e86 51%\uff0c\u5c24\u5176\u662f\u5728 PubHealth \u4e0a\u3002", "author": "Zilong Wang et.al.", "authors": "Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, Tomas Pfister", "id": "2407.08223v1", "paper_url": "http://arxiv.org/abs/2407.08223v1", "repo": "null"}}