{"2412.20787": {"publish_time": "2024-12-30", "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity", "paper_summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.", "paper_summary_zh": "\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u65bc\u4e86\u89e3\u5176\u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u7684\u80fd\u529b\u548c\u9650\u5236\u81f3\u95dc\u91cd\u8981\uff0c\u5305\u62ec\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u7a0b\u5f0f\u78bc\u751f\u6210\u3002\u73fe\u6709\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u4f8b\u5982 MMLU\u3001C-Eval \u548c HumanEval\uff0c\u53ef\u4ee5\u8a55\u4f30 LLM \u7684\u4e00\u822c\u6548\u80fd\uff0c\u4f46\u7f3a\u4e4f\u5c0d\u7279\u5b9a\u5c08\u5bb6\u9818\u57df\uff08\u4f8b\u5982\u7db2\u8def\u5b89\u5168\uff09\u7684\u95dc\u6ce8\u3002\u4ee5\u524d\u5efa\u7acb\u7db2\u8def\u5b89\u5168\u8cc7\u6599\u96c6\u7684\u5617\u8a66\u90fd\u9762\u81e8\u9650\u5236\uff0c\u5305\u62ec\u8cc7\u6599\u91cf\u4e0d\u8db3\u548c\u4f9d\u8cf4\u65bc\u591a\u9078\u984c (MCQ)\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa SecBench\uff0c\u9019\u662f\u4e00\u500b\u591a\u7dad\u5ea6\u7684\u57fa\u6e96\u6e2c\u8a66\u8cc7\u6599\u96c6\uff0c\u65e8\u5728\u8a55\u4f30\u7db2\u8def\u5b89\u5168\u9818\u57df\u7684 LLM\u3002SecBench \u5305\u542b\u5404\u7a2e\u683c\u5f0f\uff08MCQ \u548c\u7c21\u7b54\u984c (SAQ)\uff09\u7684\u554f\u984c\uff0c\u5177\u6709\u4e0d\u540c\u7684\u80fd\u529b\u5c64\u7d1a\uff08\u77e5\u8b58\u4fdd\u7559\u548c\u908f\u8f2f\u63a8\u7406\uff09\uff0c\u4f7f\u7528\u591a\u7a2e\u8a9e\u8a00\uff08\u4e2d\u6587\u548c\u82f1\u6587\uff09\uff0c\u4e26\u6db5\u84cb\u5404\u7a2e\u5b50\u9818\u57df\u3002\u8a72\u8cc7\u6599\u96c6\u662f\u900f\u904e\u5f9e\u958b\u653e\u4f86\u6e90\u6536\u96c6\u9ad8\u54c1\u8cea\u8cc7\u6599\u548c\u7d44\u7e54\u7db2\u8def\u5b89\u5168\u554f\u984c\u8a2d\u8a08\u7af6\u8cfd\u800c\u5efa\u69cb\u7684\uff0c\u7522\u751f\u4e86 44,823 \u500b MCQ \u548c 3,087 \u500b SAQ\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u4f7f\u7528\u529f\u80fd\u5f37\u5927\u4e14\u7d93\u6fdf\u5be6\u60e0\u7684 LLM \u4f86 (1)\u3002\u6a19\u8a18\u8cc7\u6599\u548c (2)\u3002\u5efa\u69cb\u4e00\u500b\u8a55\u5206\u4ee3\u7406\uff0c\u7528\u65bc\u81ea\u52d5\u8a55\u4f30 SAQ\u300213 \u500b SOTA LLM \u7684\u57fa\u6e96\u6e2c\u8a66\u7d50\u679c\u8b49\u660e\u4e86 SecBench \u7684\u53ef\u7528\u6027\uff0c\u9019\u53ef\u4ee5\u8aaa\u662f\u7db2\u8def\u5b89\u5168\u9818\u57df LLM \u4e2d\u6700\u5927\u4e14\u6700\u5168\u9762\u7684\u57fa\u6e96\u6e2c\u8a66\u8cc7\u6599\u96c6\u3002\u53ef\u4ee5\u5728\u6211\u5011\u7684\u7db2\u7ad9\u4e0a\u627e\u5230\u6709\u95dc SecBench \u7684\u66f4\u591a\u8cc7\u8a0a\uff0c\u4e26\u4e14\u53ef\u4ee5\u900f\u904e\u4eba\u5de5\u88fd\u54c1\u9023\u7d50\u5b58\u53d6\u8cc7\u6599\u96c6\u3002", "author": "Pengfei Jing et.al.", "authors": "Pengfei Jing, Mengyun Tang, Xiaorong Shi, Xing Zheng, Sen Nie, Shi Wu, Yong Yang, Xiapu Luo", "id": "2412.20787v1", "paper_url": "http://arxiv.org/abs/2412.20787v1", "repo": "null"}}