{"2410.23274": {"publish_time": "2024-10-30", "title": "Multi-student Diffusion Distillation for Better One-step Generators", "paper_summary": "Diffusion models achieve high-quality sample generation at the cost of a\nlengthy multistep inference procedure. To overcome this, diffusion distillation\ntechniques produce student generators capable of matching or surpassing the\nteacher in a single step. However, the student model's inference speed is\nlimited by the size of the teacher architecture, preventing real-time\ngeneration for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher\ndiffusion model into multiple single-step generators. Each student generator is\nresponsible for a subset of the conditioning data, thereby obtaining higher\ngeneration quality for the same capacity. MSD trains multiple distilled\nstudents, allowing smaller sizes and, therefore, faster inference. Also, MSD\noffers a lightweight quality boost over single-student distillation with the\nsame architecture. We demonstrate MSD is effective by training multiple\nsame-sized or smaller students on single-step distillation using distribution\nmatching and adversarial distillation techniques. With smaller students, MSD\ngets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD sets a new state-of-the-art for one-step image\ngeneration: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.", "paper_summary_zh": "\u64f4\u6563\u6a21\u578b\u4ee5\u5197\u9577\u7684\u6b65\u9a5f\u63a8\u8ad6\u7a0b\u5e8f\u70ba\u4ee3\u50f9\uff0c\u9054\u6210\u9ad8\u54c1\u8cea\u7684\u6a23\u672c\u7522\u751f\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e00\u9ede\uff0c\u64f4\u6563\u84b8\u993e\u6280\u8853\u7522\u751f\u5b78\u751f\u7522\u751f\u5668\uff0c\u80fd\u5920\u5728\u55ae\u4e00\u6b65\u9a5f\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u6559\u5e2b\u3002\u7136\u800c\uff0c\u5b78\u751f\u6a21\u578b\u7684\u63a8\u8ad6\u901f\u5ea6\u53d7\u5230\u6559\u5e2b\u67b6\u69cb\u5927\u5c0f\u7684\u9650\u5236\uff0c\u9632\u6b62\u8a08\u7b97\u5bc6\u96c6\u578b\u61c9\u7528\u7a0b\u5f0f\u9032\u884c\u5373\u6642\u7522\u751f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u591a\u5b78\u751f\u84b8\u993e (MSD)\uff0c\u4e00\u500b\u5c07\u689d\u4ef6\u6559\u5e2b\u64f4\u6563\u6a21\u578b\u84b8\u993e\u6210\u591a\u500b\u55ae\u6b65\u9a5f\u7522\u751f\u5668\u7684\u67b6\u69cb\u3002\u6bcf\u500b\u5b78\u751f\u7522\u751f\u5668\u8ca0\u8cac\u689d\u4ef6\u8cc7\u6599\u7684\u5b50\u96c6\uff0c\u56e0\u6b64\u7372\u5f97\u76f8\u540c\u5bb9\u91cf\u7684\u8f03\u9ad8\u7522\u751f\u54c1\u8cea\u3002MSD \u8a13\u7df4\u591a\u500b\u84b8\u993e\u5b78\u751f\uff0c\u5141\u8a31\u8f03\u5c0f\u7684\u5c3a\u5bf8\uff0c\u56e0\u6b64\u63a8\u8ad6\u66f4\u5feb\u3002\u6b64\u5916\uff0cMSD \u5728\u5177\u6709\u76f8\u540c\u67b6\u69cb\u7684\u55ae\u5b78\u751f\u84b8\u993e\u4e0a\u63d0\u4f9b\u8f15\u91cf\u7d1a\u7684\u54c1\u8cea\u63d0\u5347\u3002\u6211\u5011\u793a\u7bc4 MSD \u900f\u904e\u4f7f\u7528\u5206\u4f48\u5339\u914d\u548c\u5c0d\u6297\u84b8\u993e\u6280\u8853\uff0c\u5728\u55ae\u6b65\u9a5f\u84b8\u993e\u4e0a\u8a13\u7df4\u591a\u500b\u76f8\u540c\u5927\u5c0f\u6216\u8f03\u5c0f\u7684\u5b78\u751f\uff0c\u662f\u6709\u6548\u7684\u3002\u5c0d\u65bc\u8f03\u5c0f\u7684\u5b78\u751f\uff0cMSD \u5728\u55ae\u6b65\u9a5f\u7522\u751f\u4e2d\u4ee5\u66f4\u5feb\u7684\u63a8\u8ad6\u7372\u5f97\u7af6\u722d\u7d50\u679c\u3002\u4f7f\u7528 4 \u500b\u76f8\u540c\u5927\u5c0f\u7684\u5b78\u751f\uff0cMSD \u70ba\u55ae\u6b65\u9a5f\u5f71\u50cf\u7522\u751f\u8a2d\u5b9a\u4e86\u65b0\u7684\u6280\u8853\u6c34\u6e96\uff1aImageNet-64x64 \u4e0a\u7684 FID \u70ba 1.20\uff0c\u96f6\u6b21 COCO2014 \u4e0a\u7684 FID \u70ba 8.20\u3002", "author": "Yanke Song et.al.", "authors": "Yanke Song, Jonathan Lorraine, Weili Nie, Karsten Kreis, James Lucas", "id": "2410.23274v1", "paper_url": "http://arxiv.org/abs/2410.23274v1", "repo": "null"}}