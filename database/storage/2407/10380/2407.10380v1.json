{"2407.10380": {"publish_time": "2024-07-15", "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models", "paper_summary": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.", "paper_summary_zh": "\u8a8d\u77e5\u6587\u672c\u8207\u8996\u89ba\u63a8\u7406\u4efb\u52d9\uff0c\u4f8b\u5982\u8b0e\u984c\u3001\u7cfb\u5217\u548c\u985e\u6bd4\uff0c\u9700\u8981\u5feb\u901f\u63a8\u7406\u3001\u89e3\u78bc\u548c\u8a55\u4f30\u6587\u672c\u548c\u7a7a\u9593\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u8996\u89ba\u6a21\u578b (VLM) \u900f\u904e\u5927\u91cf\u4eba\u5de5\u6574\u7406\u8cc7\u6599\u7684\u5ee3\u6cdb\u8a13\u7df4\uff0c\u5728\u4e00\u4e9b\u5e38\u8b58\u63a8\u7406\u4efb\u52d9\u4e2d\u5df2\u9054\u5230\u9ad8\u5ea6\u7684\u985e\u4eba\u985e\u667a\u6167\uff0c\u4f46\u5b83\u5011\u5728\u9700\u8981\u8a8d\u77e5\u7406\u89e3\u7684\u66f4\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u4ecd\u9762\u81e8\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u8cc7\u6599\u96c6 NTSEBench\uff0c\u65e8\u5728\u8a55\u4f30\u5927\u578b\u6a21\u578b\u7684\u8a8d\u77e5\u591a\u6a21\u614b\u63a8\u7406\u548c\u554f\u984c\u89e3\u6c7a\u6280\u80fd\u3002\u8a72\u8cc7\u6599\u96c6\u5305\u542b 2,728 \u500b\u591a\u9078\u984c\uff0c\u7e3d\u5171\u5305\u542b\u4f86\u81ea\u5370\u5ea6\u5168\u570b NTSE \u8003\u8a66\u7684 26 \u500b\u985e\u5225\u7684 4,642 \u5f35\u5716\u7247\uff0c\u5176\u4e2d\u5305\u542b\u8996\u89ba\u548c\u6587\u672c\u7684\u4e00\u822c\u80fd\u529b\u554f\u984c\uff0c\u4e0d\u4f9d\u8cf4\u6b7b\u80cc\u786c\u8a18\u3002\u6211\u5011\u4f7f\u7528\u6700\u5148\u9032\u7684 LLM \u548c VLM \u5728\u8cc7\u6599\u96c6\u4e0a\u5efa\u7acb\u57fa\u6e96\u3002\u70ba\u4e86\u4fbf\u65bc\u5728\u958b\u6e90\u6a21\u578b\u548c\u5c08\u6709\u6a21\u578b\u4e4b\u9593\u9032\u884c\u6bd4\u8f03\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u56db\u7a2e\u4e0d\u540c\u7684\u5efa\u6a21\u7b56\u7565\u4f86\u8655\u7406\u8cc7\u6599\u96c6\u5be6\u4f8b\u4e2d\u7684\u4e0d\u540c\u6a21\u5f0f\uff08\u6587\u5b57\u548c\u5716\u50cf\uff09\u3002", "author": "Pranshu Pandya et.al.", "authors": "Pranshu Pandya, Agney S Talwarr, Vatsal Gupta, Tushar Kataria, Vivek Gupta, Dan Roth", "id": "2407.10380v1", "paper_url": "http://arxiv.org/abs/2407.10380v1", "repo": "null"}}