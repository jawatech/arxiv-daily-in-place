{"2412.09286": {"publish_time": "2024-12-12", "title": "Learning Novel Skills from Language-Generated Demonstrations", "paper_summary": "Current robot learning algorithms for acquiring novel skills often rely on\ndemonstration datasets or environment interactions, resulting in high labor\ncosts and potential safety risks. To address these challenges, this study\nproposes a skill-learning framework that enables robots to acquire novel skills\nfrom natural language instructions. The proposed pipeline leverages\nvision-language models to generate demonstration videos of novel skills, which\nare processed by an inverse dynamics model to extract actions from the\nunlabeled demonstrations. These actions are subsequently mapped to\nenvironmental contexts via imitation learning, enabling robots to learn new\nskills effectively. Experimental evaluations in the MetaWorld simulation\nenvironments demonstrate the pipeline's capability to generate high-fidelity\nand reliable demonstrations. Using the generated demonstrations, various skill\nlearning algorithms achieve an accomplishment rate three times the original on\nnovel tasks. These results highlight a novel approach to robot learning,\noffering a foundation for the intuitive and intelligent acquisition of novel\nrobotic skills.", "paper_summary_zh": "\u7576\u524d\u6a5f\u5668\u4eba\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7528\u65bc\u7372\u53d6\u65b0\u6280\u80fd\u6642\uff0c\u901a\u5e38\u4f9d\u8cf4\u65bc\u793a\u7bc4\u8cc7\u6599\u96c6\u6216\u74b0\u5883\u4e92\u52d5\uff0c\u5c0e\u81f4\u9ad8\u6602\u7684\u4eba\u529b\u6210\u672c\u548c\u6f5b\u5728\u7684\u5b89\u5168\u98a8\u96aa\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u6280\u80fd\u5b78\u7fd2\u67b6\u69cb\uff0c\u4f7f\u6a5f\u5668\u4eba\u80fd\u5920\u5f9e\u81ea\u7136\u8a9e\u8a00\u6307\u4ee4\u4e2d\u7372\u53d6\u65b0\u6280\u80fd\u3002\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4f86\u7522\u751f\u65b0\u6280\u80fd\u7684\u793a\u7bc4\u5f71\u7247\uff0c\u9019\u4e9b\u5f71\u7247\u7531\u9006\u52d5\u529b\u5b78\u6a21\u578b\u8655\u7406\uff0c\u4ee5\u5f9e\u672a\u6a19\u8a18\u7684\u793a\u7bc4\u4e2d\u63d0\u53d6\u52d5\u4f5c\u3002\u9019\u4e9b\u52d5\u4f5c\u96a8\u5f8c\u900f\u904e\u6a21\u4eff\u5b78\u7fd2\u6620\u5c04\u5230\u74b0\u5883\u80cc\u666f\uff0c\u4f7f\u6a5f\u5668\u4eba\u80fd\u5920\u6709\u6548\u5b78\u7fd2\u65b0\u6280\u80fd\u3002\u5728 MetaWorld \u6a21\u64ec\u74b0\u5883\u4e2d\u7684\u5be6\u9a57\u8a55\u4f30\u8b49\u660e\u4e86\u8a72\u7ba1\u9053\u7522\u751f\u9ad8\u4fdd\u771f\u5ea6\u548c\u53ef\u9760\u793a\u7bc4\u7684\u80fd\u529b\u3002\u4f7f\u7528\u7522\u751f\u7684\u793a\u7bc4\uff0c\u5404\u7a2e\u6280\u80fd\u5b78\u7fd2\u6f14\u7b97\u6cd5\u5728\u65b0\u7684\u4efb\u52d9\u4e0a\u5be6\u73fe\u4e86\u4e09\u500d\u65bc\u539f\u4f86\u7684\u5b8c\u6210\u7387\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u6a5f\u5668\u4eba\u5b78\u7fd2\u7684\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u70ba\u76f4\u89ba\u548c\u667a\u6167\u5730\u7372\u53d6\u65b0\u7684\u6a5f\u5668\u4eba\u6280\u80fd\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Ao-Qun Jin et.al.", "authors": "Ao-Qun Jin, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Yue Cao, Sheng-Bin Duan, Fu-Chao Xie, Zeng-Guang Hou", "id": "2412.09286v1", "paper_url": "http://arxiv.org/abs/2412.09286v1", "repo": "null"}}