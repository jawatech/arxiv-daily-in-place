{"2411.04403": {"publish_time": "2024-11-07", "title": "Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers", "paper_summary": "Learned sparse retrieval, which can efficiently perform retrieval through\nmature inverted-index engines, has garnered growing attention in recent years.\nParticularly, the inference-free sparse retrievers are attractive as they\neliminate online model inference in the retrieval phase thereby avoids huge\ncomputational cost, offering reasonable throughput and latency. However, even\nthe state-of-the-art (SOTA) inference-free sparse models lag far behind in\nterms of search relevance when compared to both sparse and dense siamese\nmodels. Towards competitive search relevance for inference-free sparse\nretrievers, we argue that they deserve dedicated training methods other than\nusing same ones with siamese encoders. In this paper, we propose two different\napproaches for performance improvement. First, we introduce the IDF-aware FLOPS\nloss, which introduces Inverted Document Frequency (IDF) to the sparsification\nof representations. We find that it mitigates the negative impact of the FLOPS\nregularization on search relevance, allowing the model to achieve a better\nbalance between accuracy and efficiency. Moreover, we propose a heterogeneous\nensemble knowledge distillation framework that combines siamese dense and\nsparse retrievers to generate supervisory signals during the pre-training\nphase. The ensemble framework of dense and sparse retriever capitalizes on\ntheir strengths respectively, providing a strong upper bound for knowledge\ndistillation. To concur the diverse feedback from heterogeneous supervisors, we\nnormalize and then aggregate the outputs of the teacher models to eliminate\nscore scale differences. On the BEIR benchmark, our model outperforms existing\nSOTA inference-free sparse model by \\textbf{3.3 NDCG@10 score}. It exhibits\nsearch relevance comparable to siamese sparse retrievers and client-side\nlatency only \\textbf{1.1x that of BM25}.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u6765\uff0c\u80fd\u6709\u6548\u901a\u8fc7\u6210\u719f\u7684\u53cd\u5411\u7d22\u5f15\u5f15\u64ce\u6267\u884c\u68c0\u7d22\u7684\u5b66\u4e60\u7a00\u758f\u68c0\u7d22\u5907\u53d7\u5173\u6ce8\u3002\u7279\u522b\u662f\uff0c\u65e0\u63a8\u7406\u7a00\u758f\u68c0\u7d22\u5668\u6781\u5177\u5438\u5f15\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u6d88\u9664\u4e86\u68c0\u7d22\u9636\u6bb5\u7684\u5728\u7ebf\u6a21\u578b\u63a8\u7406\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u4f9b\u4e86\u5408\u7406\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684 (SOTA) \u65e0\u63a8\u7406\u7a00\u758f\u6a21\u578b\u5728\u4e0e\u7a00\u758f\u548c\u5bc6\u96c6\u5b6a\u751f\u6a21\u578b\u76f8\u6bd4\u65f6\uff0c\u5728\u641c\u7d22\u76f8\u5173\u6027\u65b9\u9762\u4e5f\u8fdc\u8fdc\u843d\u540e\u3002\u4e3a\u4e86\u63d0\u9ad8\u65e0\u63a8\u7406\u7a00\u758f\u68c0\u7d22\u5668\u7684\u7ade\u4e89\u6027\u641c\u7d22\u76f8\u5173\u6027\uff0c\u6211\u4eec\u8ba4\u4e3a\u5b83\u4eec\u5e94\u8be5\u6709\u4e13\u95e8\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4e0e\u5b6a\u751f\u7f16\u7801\u5668\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u6027\u80fd\u6539\u8fdb\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u611f\u77e5 IDF \u7684 FLOPS \u635f\u5931\uff0c\u5b83\u5c06\u9006\u5411\u6587\u6863\u9891\u7387 (IDF) \u5f15\u5165\u8868\u793a\u7684\u7a00\u758f\u5316\u3002\u6211\u4eec\u53d1\u73b0\u5b83\u51cf\u8f7b\u4e86 FLOPS \u6b63\u5219\u5316\u5bf9\u641c\u7d22\u76f8\u5173\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f02\u6784\u96c6\u6210\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5b6a\u751f\u5bc6\u96c6\u548c\u7a00\u758f\u68c0\u7d22\u5668\uff0c\u4ee5\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u751f\u6210\u76d1\u7763\u4fe1\u53f7\u3002\u5bc6\u96c6\u548c\u7a00\u758f\u68c0\u7d22\u5668\u7684\u96c6\u6210\u6846\u67b6\u5206\u522b\u5229\u7528\u4e86\u5b83\u4eec\u7684\u4f18\u52bf\uff0c\u4e3a\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4e0a\u9650\u3002\u4e3a\u4e86\u5bf9\u6765\u81ea\u5f02\u6784\u76d1\u7763\u8005\u7684\u4e0d\u540c\u53cd\u9988\u8fbe\u6210\u5171\u8bc6\uff0c\u6211\u4eec\u5bf9\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u7136\u540e\u8fdb\u884c\u805a\u5408\uff0c\u4ee5\u6d88\u9664\u8bc4\u5206\u5c3a\u5ea6\u7684\u5dee\u5f02\u3002\u5728 BEIR \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728 **3.3 NDCG@10 \u5206\u6570** \u4e0a\u4f18\u4e8e\u73b0\u6709\u7684 SOTA \u65e0\u63a8\u7406\u7a00\u758f\u6a21\u578b\u3002\u5b83\u8868\u73b0\u51fa\u7684\u641c\u7d22\u76f8\u5173\u6027\u4e0e\u5b6a\u751f\u7a00\u758f\u68c0\u7d22\u5668\u76f8\u5f53\uff0c\u5ba2\u6237\u7aef\u5ef6\u8fdf\u4ec5\u4e3a **BM25 \u7684 1.1 \u500d**\u3002</paragraph>", "author": "Zhichao Geng et.al.", "authors": "Zhichao Geng, Dongyu Ru, Yang Yang", "id": "2411.04403v1", "paper_url": "http://arxiv.org/abs/2411.04403v1", "repo": "null"}}