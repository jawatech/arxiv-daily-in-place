{"2408.05404": {"publish_time": "2024-08-10", "title": "LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification", "paper_summary": "Metaphor Components Identification (MCI) contributes to enhancing machine\nunderstanding of metaphors, thereby advancing downstream natural language\nprocessing tasks. However, the complexity, diversity, and dependency on context\nand background knowledge pose significant challenges for MCI. Large language\nmodels (LLMs) offer new avenues for accurate comprehension of complex natural\nlanguage texts due to their strong semantic analysis and extensive commonsense\nknowledge. In this research, a new LLM-based framework is proposed, named\nLinguistics-aware In-context Learning with Data Augmentation (LaiDA).\nSpecifically, ChatGPT and supervised fine-tuning are utilized to tailor a\nhigh-quality dataset. LaiDA incorporates a simile dataset for pre-training. A\ngraph attention network encoder generates linguistically rich feature\nrepresentations to retrieve similar examples. Subsequently, LLM is fine-tuned\nwith prompts that integrate linguistically similar examples. LaiDA ranked 2nd\nin Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code\nand data are available at https://github.com/WXLJZ/LaiDA.", "paper_summary_zh": "\u96b1\u55bb\u7d44\u6210\u8fa8\u8b58 (MCI) \u6709\u52a9\u65bc\u63d0\u5347\u6a5f\u5668\u5c0d\u96b1\u55bb\u7684\u7406\u89e3\uff0c\u9032\u800c\u63a8\u52d5\u4e0b\u6e38\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u3002\u4e0d\u904e\uff0c\u8907\u96dc\u6027\u3001\u591a\u6a23\u6027\uff0c\u4ee5\u53ca\u5c0d\u8108\u7d61\u548c\u80cc\u666f\u77e5\u8b58\u7684\u4f9d\u8cf4\u6027\uff0c\u5c0d MCI \u800c\u8a00\u662f\u91cd\u5927\u7684\u6311\u6230\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u5176\u5f37\u5927\u7684\u8a9e\u610f\u5206\u6790\u548c\u5ee3\u6cdb\u7684\u5e38\u8b58\u77e5\u8b58\uff0c\u70ba\u6e96\u78ba\u7406\u89e3\u8907\u96dc\u7684\u81ea\u7136\u8a9e\u8a00\u6587\u672c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f91\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u65bc LLM \u7684\u67b6\u69cb\uff0c\u7a31\u70ba\u5177\u5099\u8cc7\u6599\u64f4\u5145\u529f\u80fd\u7684\u8a9e\u8a00\u611f\u77e5\u60c5\u5883\u5b78\u7fd2 (LaiDA)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cChatGPT \u548c\u76e3\u7763\u5fae\u8abf\u7528\u65bc\u8abf\u6574\u4e00\u500b\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u3002LaiDA \u7d50\u5408\u4e86\u4e00\u500b\u6bd4\u55bb\u8cc7\u6599\u96c6\u9032\u884c\u9810\u8a13\u7df4\u3002\u4e00\u500b\u5716\u5f62\u6ce8\u610f\u529b\u7db2\u8def\u7de8\u78bc\u5668\u7522\u751f\u8a9e\u8a00\u8c50\u5bcc\u7684\u7279\u5fb5\u8868\u793a\uff0c\u4ee5\u64f7\u53d6\u985e\u4f3c\u7684\u7bc4\u4f8b\u3002\u96a8\u5f8c\uff0cLLM \u4f7f\u7528\u6574\u5408\u4e86\u8a9e\u8a00\u76f8\u4f3c\u7bc4\u4f8b\u7684\u63d0\u793a\u9032\u884c\u5fae\u8abf\u3002LaiDA \u5728 NLPCC2024 \u5171\u4eab\u4efb\u52d9 9 \u7684\u5b50\u4efb\u52d9 2 \u4e2d\u6392\u540d\u7b2c 2\uff0c\u8b49\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/WXLJZ/LaiDA \u53d6\u5f97\u3002", "author": "Hongde Liu et.al.", "authors": "Hongde Liu, Chenyuan He, Feiyang Meng, Changyong Niu, Yuxiang Jia", "id": "2408.05404v1", "paper_url": "http://arxiv.org/abs/2408.05404v1", "repo": "null"}}