{"2501.14174": {"publish_time": "2025-01-24", "title": "Dreamweaver: Learning Compositional World Representations from Pixels", "paper_summary": "Humans have an innate ability to decompose their perceptions of the world\ninto objects and their attributes, such as colors, shapes, and movement\npatterns. This cognitive process enables us to imagine novel futures by\nrecombining familiar concepts. However, replicating this ability in artificial\nintelligence systems has proven challenging, particularly when it comes to\nmodeling videos into compositional concepts and generating unseen, recomposed\nfutures without relying on auxiliary data, such as text, masks, or bounding\nboxes. In this paper, we propose Dreamweaver, a neural architecture designed to\ndiscover hierarchical and compositional representations from raw videos and\ngenerate compositional future simulations. Our approach leverages a novel\nRecurrent Block-Slot Unit (RBSU) to decompose videos into their constituent\nobjects and attributes. In addition, Dreamweaver uses a multi-future-frame\nprediction objective to capture disentangled representations for dynamic\nconcepts more effectively as well as static concepts. In experiments, we\ndemonstrate our model outperforms current state-of-the-art baselines for world\nmodeling when evaluated under the DCI framework across multiple datasets.\nFurthermore, we show how the modularized concept representations of our model\nenable compositional imagination, allowing the generation of novel videos by\nrecombining attributes from different objects.", "paper_summary_zh": "\u4eba\u985e\u5177\u6709\u5c07\u4ed6\u5011\u5c0d\u4e16\u754c\u7684\u611f\u77e5\u5206\u89e3\u6210\u7269\u4ef6\u53ca\u5176\u5c6c\u6027\uff08\u4f8b\u5982\u984f\u8272\u3001\u5f62\u72c0\u548c\u904b\u52d5\u6a21\u5f0f\uff09\u7684\u5148\u5929\u80fd\u529b\u3002\u9019\u500b\u8a8d\u77e5\u904e\u7a0b\u8b93\u6211\u5011\u80fd\u5920\u900f\u904e\u91cd\u65b0\u7d44\u5408\u719f\u6089\u7684\u6982\u5ff5\u4f86\u60f3\u50cf\u65b0\u7a4e\u7684\u672a\u4f86\u3002\u7136\u800c\uff0c\u5728\u4eba\u5de5\u667a\u6167\u7cfb\u7d71\u4e2d\u8907\u88fd\u9019\u7a2e\u80fd\u529b\u5df2\u88ab\u8b49\u660e\u5177\u6709\u6311\u6230\u6027\uff0c\u7279\u5225\u662f\u5728\u5c07\u5f71\u7247\u5efa\u6a21\u6210\u7d44\u5408\u6982\u5ff5\u4e26\u7522\u751f\u672a\u898b\u904e\u7684\u3001\u91cd\u65b0\u7d44\u5408\u7684\u672a\u4f86\u6642\uff0c\u4e14\u4e0d\u4f9d\u8cf4\u8f14\u52a9\u8cc7\u6599\uff0c\u4f8b\u5982\u6587\u5b57\u3001\u906e\u7f69\u6216\u908a\u754c\u6846\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Dreamweaver\uff0c\u4e00\u7a2e\u795e\u7d93\u67b6\u69cb\uff0c\u65e8\u5728\u5f9e\u539f\u59cb\u5f71\u7247\u4e2d\u767c\u73fe\u968e\u5c64\u5f0f\u548c\u7d44\u5408\u5f0f\u8868\u793a\uff0c\u4e26\u7522\u751f\u7d44\u5408\u5f0f\u672a\u4f86\u6a21\u64ec\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e00\u7a2e\u65b0\u7a4e\u7684\u905e\u8ff4\u5340\u584a\u69fd\u55ae\u5143 (RBSU) \u5c07\u5f71\u7247\u5206\u89e3\u6210\u5176\u7d44\u6210\u7269\u4ef6\u548c\u5c6c\u6027\u3002\u6b64\u5916\uff0cDreamweaver \u4f7f\u7528\u591a\u672a\u4f86\u5e40\u9810\u6e2c\u76ee\u6a19\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u64f7\u53d6\u52d5\u614b\u6982\u5ff5\u548c\u975c\u614b\u6982\u5ff5\u7684\u89e3\u7cfe\u7e8f\u8868\u793a\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u6a21\u578b\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u6839\u64da DCI \u67b6\u69cb\u8a55\u4f30\u6642\uff0c\u512a\u65bc\u7576\u524d\u4e16\u754c\u5efa\u6a21\u6280\u8853\u7684\u6700\u65b0\u57fa\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u6a21\u578b\u7684\u6a21\u7d44\u5316\u6982\u5ff5\u8868\u793a\u5982\u4f55\u555f\u7528\u7d44\u5408\u5f0f\u60f3\u50cf\u529b\uff0c\u5141\u8a31\u900f\u904e\u91cd\u65b0\u7d44\u5408\u4f86\u81ea\u4e0d\u540c\u7269\u4ef6\u7684\u5c6c\u6027\u4f86\u7522\u751f\u65b0\u7a4e\u7684\u5f71\u7247\u3002", "author": "Junyeob Baek et.al.", "authors": "Junyeob Baek, Yi-Fu Wu, Gautam Singh, Sungjin Ahn", "id": "2501.14174v1", "paper_url": "http://arxiv.org/abs/2501.14174v1", "repo": "null"}}