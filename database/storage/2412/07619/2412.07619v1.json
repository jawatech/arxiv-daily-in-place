{"2412.07619": {"publish_time": "2024-12-10", "title": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models", "paper_summary": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u65b0\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u8fd9\u5f97\u76ca\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL) \u7684\u5e2e\u52a9\u3002\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u7684\u7814\u7a76\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u5728\u5b9e\u65bd ICL \u65f6\u901a\u5e38\u91c7\u7528\u6734\u7d20\u7684\u7b56\u7565\uff0c\u4f8b\u5982\u8de8\u4e0d\u540c\u6837\u672c\u7684\u56fa\u5b9a\u6f14\u793a\uff0c\u6216\u76f4\u63a5\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u9009\u62e9\u6f14\u793a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u80fd\u4fdd\u8bc1\u914d\u7f6e\u7684\u6f14\u793a\u7b26\u5408 LVLMs \u7684\u9700\u6c42\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u73b0\u5728\u63d0\u51fa\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5373\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f14\u793a\u68c0\u7d22\u5668 (DRUM)\uff0c\u5b83\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3 LVLM \u7684\u9700\u6c42\u3002\u9996\u5148\uff0c\u6211\u4eec\u8ba8\u8bba\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u5047\u8bbe\u7ed9\u5b9a\u4e86\u4e00\u4e2a\u5d4c\u5165\u6a21\u578b\u3002\u6211\u4eec\u63d0\u51fa\u5c06\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\u8fde\u63a5\u8d77\u6765\u4ee5\u589e\u5f3a\u68c0\u7d22\u6027\u80fd\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7 LVLM \u7684\u53cd\u9988\u91cd\u65b0\u5bf9\u5d4c\u5165\u6a21\u578b\u68c0\u7d22\u5230\u7684\u6f14\u793a\u8fdb\u884c\u6392\u540d\uff0c\u5e76\u8ba1\u7b97\u4e00\u4e2a\u5217\u8868\u7ea7\u6392\u540d\u635f\u5931\u4ee5\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u6f14\u793a\u6316\u6398\u7b56\u7565\u6765\u6539\u8fdb\u5d4c\u5165\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u901a\u8fc7\u5bf9 3 \u79cd\u7c7b\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u30017 \u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684 DRUM \u6846\u67b6\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u901a\u8fc7\u68c0\u7d22\u66f4\u5408\u9002\u7684\u6f14\u793a\u6765\u63d0\u5347 LVLM \u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u3002</paragraph>", "author": "Ellen Yi-Ge et.al.", "authors": "Ellen Yi-Ge, Jiechao Gao, Wei Han, Wei Zhu", "id": "2412.07619v1", "paper_url": "http://arxiv.org/abs/2412.07619v1", "repo": "null"}}