
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-09**|**Natural Language Processing RELIES on Linguistics**|Juri Opitz et.al.|[2405.05966v1](http://arxiv.org/abs/2405.05966v1)|null|
|**2024-05-09**|**Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask**|Zineb Senane et.al.|[2405.05959v1](http://arxiv.org/abs/2405.05959v1)|[link](https://github.com/eqtpartners/tsde)|
|**2024-05-09**|**OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning**|Dan Qiao et.al.|[2405.05957v1](http://arxiv.org/abs/2405.05957v1)|[link](https://github.com/opennlg/openba-v2)|
|**2024-05-09**|**Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning**|Junzhi Chen et.al.|[2405.05955v1](http://arxiv.org/abs/2405.05955v1)|null|
|**2024-05-09**|**DOLOMITES: Domain-Specific Long-Form Methodical Tasks**|Chaitanya Malaviya et.al.|[2405.05938v1](http://arxiv.org/abs/2405.05938v1)|null|
|**2024-05-09**|**Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness**|Siyuan Li et.al.|[2405.05930v1](http://arxiv.org/abs/2405.05930v1)|null|
|**2024-05-09**|**FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting**|Xiaohui Zhong et.al.|[2405.05925v1](http://arxiv.org/abs/2405.05925v1)|null|
|**2024-05-09**|**Diag2Diag: Multi modal super resolution for physics discovery with application to fusion**|Azarakhsh Jalalvand et.al.|[2405.05908v1](http://arxiv.org/abs/2405.05908v1)|null|
|**2024-05-09**|**Truthful Aggregation of LLMs with an Application to Online Advertising**|Ermis Soumalias et.al.|[2405.05905v1](http://arxiv.org/abs/2405.05905v1)|null|
|**2024-05-09**|**Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?**|Zorik Gekhman et.al.|[2405.05904v1](http://arxiv.org/abs/2405.05904v1)|null|
|**2024-05-09**|**Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons**|Adian Liusie et.al.|[2405.05894v1](http://arxiv.org/abs/2405.05894v1)|null|
|**2024-05-09**|**Safe Exploration Using Bayesian World Models and Log-Barrier Optimization**|Yarden As et.al.|[2405.05890v1](http://arxiv.org/abs/2405.05890v1)|null|
|**2024-05-09**|**Composable Part-Based Manipulation**|Weiyu Liu et.al.|[2405.05876v1](http://arxiv.org/abs/2405.05876v1)|null|
|**2024-05-09**|**ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers**|Liangliang Chen et.al.|[2405.05861v1](http://arxiv.org/abs/2405.05861v1)|null|
|**2024-05-09**|**Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control**|Gunshi Gupta et.al.|[2405.05852v1](http://arxiv.org/abs/2405.05852v1)|[link](https://github.com/ykarmesh/stable-control-representations)|
|**2024-05-09**|**Aequitas Flow: Streamlining Fair ML Experimentation**|Sérgio Jesus et.al.|[2405.05809v1](http://arxiv.org/abs/2405.05809v1)|null|
|**2024-05-09**|**Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference**|Zhihang Lin et.al.|[2405.05803v1](http://arxiv.org/abs/2405.05803v1)|[link](https://github.com/lzhxmu/vtw)|
|**2024-05-09**|**RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**|Sourav Garg et.al.|[2405.05792v1](http://arxiv.org/abs/2405.05792v1)|null|
|**2024-05-09**|**A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties**|A. Noroozi et.al.|[2405.05790v1](http://arxiv.org/abs/2405.05790v1)|null|
|**2024-05-09**|**Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language**|Ronny Paul et.al.|[2405.05777v1](http://arxiv.org/abs/2405.05777v1)|null|
|**2024-05-09**|**Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions**|Polina Tsvilodub et.al.|[2405.05776v1](http://arxiv.org/abs/2405.05776v1)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel Miró-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-09**|**DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models**|Mengxiao Geng et.al.|[2405.05763v1](http://arxiv.org/abs/2405.05763v1)|null|
|**2024-05-09**|**Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media**|Zhizhen Zhang et.al.|[2405.05760v1](http://arxiv.org/abs/2405.05760v1)|null|
|**2024-05-09**|**Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma**|Han Meng et.al.|[2405.05758v1](http://arxiv.org/abs/2405.05758v1)|null|
|**2024-05-09**|**CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks**|Nick et.al.|[2405.05755v1](http://arxiv.org/abs/2405.05755v1)|null|
|**2024-05-09**|**Can large language models understand uncommon meanings of common words?**|Jinyang Wu et.al.|[2405.05741v1](http://arxiv.org/abs/2405.05741v1)|null|
|**2024-05-09**|**Computational lexical analysis of Flamenco genres**|Pablo Rosillo-Rodes et.al.|[2405.05723v1](http://arxiv.org/abs/2405.05723v1)|null|
|**2024-05-09**|**Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution**|Sandrine Chausson et.al.|[2405.05705v1](http://arxiv.org/abs/2405.05705v1)|[link](https://github.com/s-l-chausson/easyclaimsdetection)|
|**2024-05-09**|**Evaluating Dialect Robustness of Language Models via Conversation Understanding**|Dipankar Srirag et.al.|[2405.05688v1](http://arxiv.org/abs/2405.05688v1)|null|
|**2024-05-09**|**Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment**|Yoonsu Kim et.al.|[2405.05678v1](http://arxiv.org/abs/2405.05678v1)|null|
|**2024-05-09**|**SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space**|Zeren Zhang et.al.|[2405.05636v1](http://arxiv.org/abs/2405.05636v1)|null|
|**2024-05-09**|**G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning**|Ruiting Dai et.al.|[2405.05616v1](http://arxiv.org/abs/2405.05616v1)|null|
|**2024-05-09**|**Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning**|Shibo Jie et.al.|[2405.05615v1](http://arxiv.org/abs/2405.05615v1)|null|
|**2024-05-09**|**Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM**|Xikang Yang et.al.|[2405.05610v1](http://arxiv.org/abs/2405.05610v1)|null|
|**2024-05-09**|**Can We Use Large Language Models to Fill Relevance Judgment Holes?**|Zahra Abbasiantaeb et.al.|[2405.05600v1](http://arxiv.org/abs/2405.05600v1)|null|
|**2024-05-09**|**A Survey on Backbones for Deep Video Action Recognition**|Zixuan Tang et.al.|[2405.05584v1](http://arxiv.org/abs/2405.05584v1)|null|
|**2024-05-09**|**OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs**|Yuxia Wang et.al.|[2405.05583v1](http://arxiv.org/abs/2405.05583v1)|[link](https://github.com/yuxiaw/openfactcheck)|
|**2024-05-09**|**One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations**|Yoonjoo Lee et.al.|[2405.05581v1](http://arxiv.org/abs/2405.05581v1)|null|
|**2024-05-09**|**From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences**|Prashant Kodali et.al.|[2405.05572v1](http://arxiv.org/abs/2405.05572v1)|null|
|**2024-05-09**|**Towards Robust Physical-world Backdoor Attacks on Lane Detection**|Xinwei Zhang et.al.|[2405.05553v1](http://arxiv.org/abs/2405.05553v1)|null|
|**2024-05-09**|**Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training**|Sheng Yan et.al.|[2405.05523v1](http://arxiv.org/abs/2405.05523v1)|null|
|**2024-05-09**|**Characteristic Learning for Provable One Step Generation**|Zhao Ding et.al.|[2405.05512v1](http://arxiv.org/abs/2405.05512v1)|[link](https://github.com/burning489/characteristicgenrator)|
|**2024-05-09**|**Redefining Information Retrieval of Structured Database via Large Language Models**|Mingzhu Wang et.al.|[2405.05508v1](http://arxiv.org/abs/2405.05508v1)|null|
|**2024-05-09**|**Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias**|Shan Chen et.al.|[2405.05506v1](http://arxiv.org/abs/2405.05506v1)|[link](https://github.com/shan23chen/cross-care)|
|**2024-05-09**|**Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting**|Feifei Li et.al.|[2405.05499v1](http://arxiv.org/abs/2405.05499v1)|null|
|**2024-05-09**|**Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis**|Xuanwen Ding et.al.|[2405.05496v1](http://arxiv.org/abs/2405.05496v1)|null|
|**2024-05-09**|**Parameter-Efficient Fine-Tuning With Adapters**|Keyu Chen et.al.|[2405.05493v1](http://arxiv.org/abs/2405.05493v1)|null|
|**2024-05-09**|**A logifold structure on measure space**|Inkee Jung et.al.|[2405.05492v1](http://arxiv.org/abs/2405.05492v1)|null|
|**2024-05-09**|**Using Machine Translation to Augment Multilingual Classification**|Adam King et.al.|[2405.05478v1](http://arxiv.org/abs/2405.05478v1)|null|
|**2024-05-08**|**AFEN: Respiratory Disease Classification using Ensemble Learning**|Rahul Nadkarni et.al.|[2405.05467v1](http://arxiv.org/abs/2405.05467v1)|null|
|**2024-05-08**|**Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals**|Joshua Clymer et.al.|[2405.05466v1](http://arxiv.org/abs/2405.05466v1)|null|
|**2024-05-08**|**GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields**|Yuanhao Gong et.al.|[2405.05446v1](http://arxiv.org/abs/2405.05446v1)|null|
|**2024-05-08**|**Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large**|Jussi S. Jauhiainen et.al.|[2405.05444v1](http://arxiv.org/abs/2405.05444v1)|null|
|**2024-05-08**|**How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation**|Joseph A. Vincent et.al.|[2405.05439v1](http://arxiv.org/abs/2405.05439v1)|[link](https://github.com/tri-ml/stochastic_verification)|
|**2024-05-08**|**How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression**|Lucas Kook et.al.|[2405.05429v1](http://arxiv.org/abs/2405.05429v1)|null|
|**2024-05-08**|**Mitigating Exaggerated Safety in Large Language Models**|Ruchi Bhalani et.al.|[2405.05418v1](http://arxiv.org/abs/2405.05418v1)|null|
|**2024-05-08**|**Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models**|Sander Land et.al.|[2405.05417v1](http://arxiv.org/abs/2405.05417v1)|[link](https://github.com/cohere-ai/magikarp)|
|**2024-05-08**|**Interpretability Needs a New Paradigm**|Andreas Madsen et.al.|[2405.05386v1](http://arxiv.org/abs/2405.05386v1)|null|
|**2024-05-08**|**"They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations**|Preetam Prabhu Srikar Dammu et.al.|[2405.05378v1](http://arxiv.org/abs/2405.05378v1)|null|
|**2024-05-08**|**Kreyòl-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages**|Nathaniel R. Robinson et.al.|[2405.05376v1](http://arxiv.org/abs/2405.05376v1)|null|
|**2024-05-08**|**Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models**|Luke Merrick et.al.|[2405.05374v1](http://arxiv.org/abs/2405.05374v1)|null|
|**2024-05-08**|**Offline Model-Based Optimization via Policy-Guided Gradient Search**|Yassine Chemingui et.al.|[2405.05349v1](http://arxiv.org/abs/2405.05349v1)|[link](https://github.com/yassinech/pgs)|
|**2024-05-08**|**The Effect of Model Size on LLM Post-hoc Explainability via LIME**|Henning Heyen et.al.|[2405.05348v1](http://arxiv.org/abs/2405.05348v1)|[link](https://github.com/henningheyen/scalability-of-llm-posthoc-explanations)|
|**2024-05-08**|**Benchmarking Educational Program Repair**|Charles Koutcheme et.al.|[2405.05347v1](http://arxiv.org/abs/2405.05347v1)|[link](https://github.com/koutchemecharles/gaied_nips23)|
|**2024-05-08**|**QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums**|Varun Nagaraj Rao et.al.|[2405.05345v1](http://arxiv.org/abs/2405.05345v1)|null|
|**2024-05-08**|**Joint semi-supervised and contrastive learning enables zero-shot domain-adaptation and multi-domain segmentation**|Alvaro Gomariz et.al.|[2405.05336v1](http://arxiv.org/abs/2405.05336v1)|null|
|**2024-05-08**|**KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation**|Minsik Cho et.al.|[2405.05329v1](http://arxiv.org/abs/2405.05329v1)|null|
|**2024-05-08**|**THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models**|Prannay Kaul et.al.|[2405.05256v1](http://arxiv.org/abs/2405.05256v1)|null|
|**2024-05-08**|**Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge**|Charles Koutcheme et.al.|[2405.05253v1](http://arxiv.org/abs/2405.05253v1)|[link](https://github.com/koutchemecharles/iticse24)|
|**2024-05-08**|**You Only Cache Once: Decoder-Decoder Architectures for Language Models**|Yutao Sun et.al.|[2405.05254v2](http://arxiv.org/abs/2405.05254v2)|null|
|**2024-05-08**|**Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models**|Hongjie Wang et.al.|[2405.05252v1](http://arxiv.org/abs/2405.05252v1)|null|
|**2024-05-08**|**LLMs with Personalities in Multi-issue Negotiation Games**|Sean Noh et.al.|[2405.05248v2](http://arxiv.org/abs/2405.05248v2)|null|
|**2024-05-08**|**SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan**|You Zhang et.al.|[2405.05244v1](http://arxiv.org/abs/2405.05244v1)|[link](https://github.com/svddchallenge/ctrsvdd2024_baseline)|
|**2024-05-08**|**Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers**|Jiuxiang Gu et.al.|[2405.05219v1](http://arxiv.org/abs/2405.05219v1)|null|
|**2024-05-08**|**CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation**|Drew Walker et.al.|[2405.05204v1](http://arxiv.org/abs/2405.05204v1)|null|
|**2024-05-08**|**MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning**|Inderjeet Nair et.al.|[2405.05189v1](http://arxiv.org/abs/2405.05189v1)|null|
|**2024-05-08**|**Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming**|Tommaso Pasini et.al.|[2405.05176v1](http://arxiv.org/abs/2405.05176v1)|null|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175v1](http://arxiv.org/abs/2405.05175v1)|null|
|**2024-05-08**|**Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language**|Julia Krebs et.al.|[2405.05161v1](http://arxiv.org/abs/2405.05161v1)|null|
|**2024-05-08**|**The Potential and Implications of Generative AI on HCI Education**|Ahmed Kharrufa et.al.|[2405.05154v1](http://arxiv.org/abs/2405.05154v1)|null|
|**2024-05-08**|**Hybrid Convolutional Neural Networks with Reliability Guarantee**|Hans Dermot Doran et.al.|[2405.05146v2](http://arxiv.org/abs/2405.05146v2)|null|
|**2024-05-08**|**XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples**|Peiqin Lin et.al.|[2405.05116v1](http://arxiv.org/abs/2405.05116v1)|null|
|**2024-05-08**|**QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs**|Weijia Zhang et.al.|[2405.05109v1](http://arxiv.org/abs/2405.05109v1)|null|
|**2024-05-08**|**Concerns on Bias in Large Language Models when Creating Synthetic Personae**|Helena A. Haxvig et.al.|[2405.05080v1](http://arxiv.org/abs/2405.05080v1)|null|
|**2024-05-08**|**Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology**|Anja Thieme et.al.|[2405.05299v1](http://arxiv.org/abs/2405.05299v1)|null|
|**2024-05-08**|**Designing Skill-Compatible AI: Methodologies and Frameworks in Chess**|Karim Hamade et.al.|[2405.05066v1](http://arxiv.org/abs/2405.05066v1)|null|
|**2024-05-08**|**Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models**|Aylin Gunal et.al.|[2405.05060v1](http://arxiv.org/abs/2405.05060v1)|null|
|**2024-05-08**|**Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources**|Lasse Hyldig Hansen et.al.|[2405.05049v1](http://arxiv.org/abs/2405.05049v1)|null|
|**2024-05-08**|**StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer**|Zijia Wang et.al.|[2405.05027v1](http://arxiv.org/abs/2405.05027v1)|null|
|**2024-05-08**|**ADELIE: Aligning Large Language Models on Information Extraction**|Yunjia Qi et.al.|[2405.05008v1](http://arxiv.org/abs/2405.05008v1)|[link](https://github.com/THU-KEG/ADELIE)|
|**2024-05-08**|**Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning**|Kristupas Bajarunas et.al.|[2405.04990v1](http://arxiv.org/abs/2405.04990v1)|[link](https://github.com/kbaja/unsupervisedhi)|
|**2024-05-08**|**An Artificial Intelligence Approach for Interpreting Creative Combinational Designs**|Liuqing Chen et.al.|[2405.04985v1](http://arxiv.org/abs/2405.04985v1)|null|
|**2024-05-08**|**Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI**|Keqiang Fan et.al.|[2405.04974v1](http://arxiv.org/abs/2405.04974v1)|null|
|**2024-05-08**|**A review on discriminative self-supervised learning methods**|Nikolaos Giakoumoglou et.al.|[2405.04969v1](http://arxiv.org/abs/2405.04969v1)|null|
|**2024-05-08**|**Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers**|Silvan Mertes et.al.|[2405.05295v1](http://arxiv.org/abs/2405.05295v1)|null|
|**2024-05-08**|**P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models**|Guochao Jiang et.al.|[2405.04960v1](http://arxiv.org/abs/2405.04960v1)|null|
|**2024-05-08**|**Improving Long Text Understanding with Knowledge Distilled from Summarization Model**|Yan Liu et.al.|[2405.04955v1](http://arxiv.org/abs/2405.04955v1)|null|
|**2024-05-08**|**VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context**|Yunxin Li et.al.|[2405.04950v1](http://arxiv.org/abs/2405.04950v1)|[link](https://github.com/hitsz-tmg/visiongraph)|
|**2024-05-08**|**Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs**|Eline M. Bovy et.al.|[2405.04941v1](http://arxiv.org/abs/2405.04941v1)|null|

#### Abstracts
##### **Natural Language Processing RELIES on Linguistics**
2405.05966v1 by Juri Opitz, Shira Wein, Nathan Schneider

Large Language Models (LLMs) have become capable of generating highly fluent
text in certain languages, without modules specially designed to capture
grammar or semantic coherence. What does this mean for the future of linguistic
expertise in NLP? We highlight several aspects in which NLP (still) relies on
linguistics, or where linguistic thinking can illuminate new directions. We
argue our case around the acronym $RELIES$ that encapsulates six major facets
where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource
settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This
list is not exhaustive, nor is linguistics the main point of reference for
every effort under these themes; but at a macro level, these facets highlight
the enduring importance of studying machine systems vis-a-vis systems of human
language.

摘要：大型語言模型 (LLM) 已具備產生高度流暢的文本的能力，而無需特別設計來捕捉語法或語義連貫性的模組。這對 NLP 中語言專業的未來有何意義？我們重點介紹了 NLP（仍然）依賴語言學的幾個方面，或語言思維可以照亮新方向的地方。我們圍繞縮寫詞 $RELIES$ 來論證我們的論點，它概括了語言學對 NLP 有所貢獻的六個主要方面：$R$esources（資源）、$E$valuation（評估）、$L$ow-resource settings（低資源設定）、$I$nterpretability（可解釋性）、$E$xplanation（說明）和 $S$tudy of language（語言研究）。此清單並非詳盡無遺，語言學也不是這些主題下每項工作的參考重點；但從巨觀層面來看，這些方面突顯了研究機器系統與人類語言系統之間的持久重要性。

##### **Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask**
2405.05959v1 by Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim

Time Series Representation Learning (TSRL) focuses on generating informative
representations for various Time Series (TS) modeling tasks. Traditional
Self-Supervised Learning (SSL) methods in TSRL fall into four main categories:
reconstructive, adversarial, contrastive, and predictive, each with a common
challenge of sensitivity to noise and intricate data nuances. Recently,
diffusion-based methods have shown advanced generative capabilities. However,
they primarily target specific application scenarios like imputation and
forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our
work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first
diffusion-based SSL TSRL approach. TSDE segments TS data into observed and
masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It
applies a trainable embedding function, featuring dual-orthogonal Transformer
encoders with a crossover mechanism, to the observed part. We train a reverse
diffusion process conditioned on the embeddings, designed to predict noise
added to the masked part. Extensive experiments demonstrate TSDE's superiority
in imputation, interpolation, forecasting, anomaly detection, classification,
and clustering. We also conduct an ablation study, present embedding
visualizations, and compare inference speed, further substantiating TSDE's
efficiency and validity in learning representations of TS data.

摘要：時間序列表示學習 (TSRL) 專注於為各種時間序列 (TS) 建模任務產生有意義的表示。TSRL 中的傳統自監督學習 (SSL) 方法分為四種類型：重建、對抗、對比和預測，每種類型都面臨對雜訊和複雜資料細微差別敏感的共同挑戰。最近，基於擴散的方法已展現出先進的生成能力。然而，它們主要針對特定應用場景，例如填補和預測，在利用擴散模型進行通用 TSRL 方面存在差距。我們的作品時間序列擴散嵌入 (TSDE) 作為第一個基於擴散的 SSL TSRL 方法，彌補了這一差距。TSDE 使用填補-插值-預測 (IIF) 蒙版將 TS 資料分割為觀察部分和遮蔽部分。它將可訓練的嵌入函數應用到觀察部分，該函數具有帶有交叉機制的雙正交 Transformer 編碼器。我們訓練一個反向擴散過程，以嵌入為條件，旨在預測添加到遮蔽部分的雜訊。廣泛的實驗證明了 TSDE 在填補、插值、預測、異常偵測、分類和聚類方面的優越性。我們還進行了消融研究，展示了嵌入視覺化，並比較了推理速度，進一步證實了 TSDE 在學習 TS 資料表示方面的效率和有效性。

##### **OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning**
2405.05957v1 by Dan Qiao, Yi Su, Pinzheng Wang, Jing Ye, Wenjing Xie, Yuechi Zhou, Yuyang Ding, Zecheng Tang, Jikai Wang, Yixin Ji, Yue Wang, Pei Guo, Zechen Sun, Zikang Zhang, Juntao Li, Pingfu Chao, Wenliang Chen, Guohong Fu, Guodong Zhou, Qiaoming Zhu, Min Zhang

Large Language Models (LLMs) have played an important role in many fields due
to their powerful capabilities.However, their massive number of parameters
leads to high deployment requirements and incurs significant inference costs,
which impedes their practical applications. Training smaller models is an
effective way to address this problem. Therefore, we introduce OpenBA-V2, a
3.4B model derived from multi-stage compression and continual pre-training from
the original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible
training objectives, and techniques such as layer pruning, neural pruning, and
vocabulary pruning to achieve a compression rate of 77.3\% with minimal
performance loss. OpenBA-V2 demonstrates competitive performance compared to
other open-source models of similar size, achieving results close to or on par
with the 15B OpenBA model in downstream tasks such as common sense reasoning
and Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be
compressed into smaller ones with minimal performance loss by employing
advanced training objectives and data strategies, which may help deploy LLMs in
resource-limited scenarios.

摘要：大型語言模型 (LLM) 由於其強大的功能，在許多領域中扮演著重要的角色。然而，其龐大的參數量導致了高部署需求，並產生顯著的推論成本，這阻礙了其實際應用。訓練較小的模型是解決此問題的有效方法。因此，我們引入了 OpenBA-V2，一個從原始 15B OpenBA 模型的多階段壓縮和持續預訓練衍生的 3.4B 模型。OpenBA-V2 利用更多數據、更靈活的訓練目標，以及層修剪、神經修剪和詞彙修剪等技術，以最小的效能損失實現 77.3% 的壓縮率。與其他類似規模的開源模型相比，OpenBA-V2 表現出競爭力的效能，在常識推理和命名實體識別 (NER) 等下游任務中，獲得接近或等同於 15B OpenBA 模型的結果。OpenBA-V2 說明了 LLM 可以透過採用進階訓練目標和數據策略，壓縮成較小的模型，且效能損失最小，這有助於在資源受限的情況下部署 LLM。

##### **Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning**
2405.05955v1 by Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces "Smurfs", a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By transforming a conventional LLM
into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and
execution without necessitating extra training. This is achieved through
innovative prompting strategies that allocate distinct roles within the model,
thereby facilitating collaboration among specialized agents. The framework
gives access to external tools to efficiently solve complex tasks. Our
empirical investigation, featuring the mistral-7b-instruct model as a case
study, showcases Smurfs' superior capability in intricate tool utilization
scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and
I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded
performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.

摘要：大型語言模型 (LLM) 的出現為自動化複雜任務開闢了前所未有的可能性，這些任務通常與人類的表現相當。儘管有這些能力，但 LLM 在完成需要高準確度和複雜性的任務時仍會遇到困難，因為它們在單獨處理多方面的問題時有其固有的限制。本文介紹了「Smurfs」，這是一個旨在革新 LLM 應用程序的尖端多代理架構。透過將傳統的 LLM 轉變為協同的多代理整體，Smurfs 增強了任務分解和執行，而無需額外的訓練。這是透過創新的提示策略實現的，這些策略在模型中分配不同的角色，從而促進專業代理之間的協作。該框架允許使用外部工具來有效地解決複雜任務。我們的實證調查以 mistral-7b-instruct 模型為案例研究，展示了 Smurfs 在複雜工具使用場景中的卓越能力。值得注意的是，Smurfs 在 ToolBench I2 和 I3 基準測試中以 84.4% 的勝率擊敗 ChatGPT-ReACT，超越了 GPT-4 模型創下的 73.5% 的最高記錄。此外，透過全面的消融研究，我們剖析了多代理架構的核心組成部分對其整體效能的貢獻。這不僅驗證了該架構的有效性，也為未來探索多代理 LLM 系統設定了一個路線。

##### **DOLOMITES: Domain-Specific Long-Form Methodical Tasks**
2405.05938v1 by Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, Chris Alberti

Experts in various fields routinely perform methodical writing tasks to plan,
organize, and report their work. From a clinician writing a differential
diagnosis for a patient, to a teacher writing a lesson plan for students, these
tasks are pervasive, requiring to methodically generate structured long-form
output for a given input. We develop a typology of methodical tasks structured
in the form of a task objective, procedure, input, and output, and introduce
DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited
from hundreds of experts from across 25 fields. Our benchmark further contains
specific instantiations of methodical tasks with concrete input and output
examples (1,857 in total) which we obtain by collecting expert revisions of up
to 10 model-generated examples of each task. We use these examples to evaluate
contemporary language models highlighting that automating methodical tasks is a
challenging long-form generation problem, as it requires performing complex
inferences, while drawing upon the given context as well as domain knowledge.

摘要：各領域的專家例行執行有條理的寫作任務，以規劃、組織和報告其工作。從臨床醫生為患者撰寫鑑別診斷，到教師為學生撰寫課程計畫，這些任務無所不在，需要有條理地為給定的輸入產生結構化的長篇輸出。我們開發了一種方法任務類型學，以任務目標、程序、輸入和輸出的形式進行結構化，並介紹 DoLoMiTes，這是一個新的基準，其中包含從 25 個領域的數百名專家獲得的 519 項此類任務的規格。我們的基準進一步包含具體方法任務的具體實例，以及具體的輸入和輸出範例（總計 1,857 個），我們透過收集每個任務最多 10 個模型生成範例的專家修訂來取得這些範例。我們使用這些範例來評估當代語言模型，強調自動化方法任務是一個具有挑戰性的長篇生成問題，因為它需要執行複雜的推論，同時利用給定的背景以及領域知識。

##### **Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness**
2405.05930v1 by Siyuan Li, Xi Lin, Yaju Liu, Jianhua Li

AI-generated content (AIGC) models, represented by large language models
(LLM), have brought revolutionary changes to the content generation fields. The
high-speed and extensive 6G technology is an ideal platform for providing
powerful AIGC mobile service applications, while future 6G mobile networks also
need to support intelligent and personalized mobile generation services.
However, the significant ethical and security issues of current AIGC models,
such as adversarial attacks, privacy, and fairness, greatly affect the
credibility of 6G intelligent networks, especially in ensuring secure, private,
and fair AIGC applications. In this paper, we propose TrustGAIN, a novel
paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale
AIGC services in future 6G networks. We first discuss the adversarial attacks
and privacy threats faced by AIGC systems in 6G networks, as well as the
corresponding protection issues. Subsequently, we emphasize the importance of
ensuring the unbiasedness and fairness of the mobile generative service in
future intelligent networks. In particular, we conduct a use case to
demonstrate that TrustGAIN can effectively guide the resistance against
malicious or generated false information. We believe that TrustGAIN is a
necessary paradigm for intelligent and trustworthy 6G networks to support AIGC
services, ensuring the security, privacy, and fairness of AIGC network
services.

摘要：由大型語言模型 (LLM) 所代表的人工智慧生成內容 (AIGC) 模型，為內容生成領域帶來了革命性的變化。高速且廣泛的 6G 技術是提供強大 AIGC 行動服務應用程式的理想平台，而未來的 6G 行動網路也需要支援智慧且個人化的行動生成服務。然而，當前 AIGC 模型的重大道德和安全性問題，例如對抗性攻擊、隱私和公平性，極大地影響了 6G 智慧網路的可信度，特別是在確保安全的、私密的和公平的 AIGC 應用程式方面。在本文中，我們提出了 TrustGAIN，這是一種在 6G 網路中值得信賴的 AIGC 新典範，以確保未來 6G 網路中值得信賴的大規模 AIGC 服務。我們首先討論了 6G 網路中 AIGC 系統面臨的對抗性攻擊和隱私威脅，以及相應的保護問題。隨後，我們強調了確保未來智慧網路中行動生成服務的公正性和公平性的重要性。特別是，我們進行了一個用例，以展示 TrustGAIN 可以有效地指導對抗惡意或生成的虛假資訊。我們相信 TrustGAIN 是智慧且值得信賴的 6G 網路支援 AIGC 服務的必要典範，確保 AIGC 網路服務的安全性、隱私和公平性。

##### **FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting**
2405.05925v1 by Xiaohui Zhong, Lei Chen, Hao Li, Jie Feng, Bo Lu

Ensemble weather forecasting is essential for weather predictions and
mitigating the impacts of extreme weather events. Constructing an ensemble
prediction system (EPS) based on conventional numerical weather prediction
(NWP) models is highly computationally expensive. Machine learning (ML) models
have emerged as valuable tools for deterministic weather forecasts, providing
forecasts with significantly reduced computational requirements and even
surpassing the forecast performance of traditional NWP models. However,
challenges arise when applying ML models to ensemble forecasting. Recent ML
models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data
Assimilations (EDA) or two operational NWP ensemble members for forecast
generation. The spatial resolution of 1{\deg} or 2{\deg} in these models is
often considered too coarse for many applications. To overcome these
limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver
6-hourly global ensemble weather forecasts up to 15 days. This model runs at a
significantly improved spatial resolution of 0.25{\deg}, incorporating 5
upper-air atmospheric variables at 13 pressure levels, along with 13 surface
variables. By leveraging the inherent probabilistic nature of Variational
AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the
continuous ranked probability score (CRPS) and the KL divergence between the
predicted and target distribution. This innovative approach represents an
advancement over the traditional use of L1 loss combined with the KL loss in
standard VAE models when VAE for ensemble weather forecasts. Evaluation results
demonstrate that FuXi-ENS outperforms ensemble forecasts from the European
Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model,
on 98.1% of 360 variable and forecast lead time combinations on CRPS.

摘要：集合天氣預測對於天氣預測和減輕極端天氣事件的影響至關重要。基於傳統數值天氣預測 (NWP) 模型構建集合預測系統 (EPS) 在計算上非常昂貴。機器學習 (ML) 模型已成為確定性天氣預測的寶貴工具，提供計算需求大幅降低的預測，甚至超越傳統 NWP 模型的預測性能。然而，將 ML 模型應用於集合預測時會出現挑戰。最近的 ML 模型，例如 GenCast 和 SEEDS 模型，依賴於 ERA5 資料同化集合 (EDA) 或兩個運算 NWP 集合成員進行預測生成。這些模型中 1{\deg} 或 2{\deg} 的空間解析度通常被認為對於許多應用而言過於粗糙。為了克服這些限制，我們引入了 FuXi-ENS，這是一個先進的 ML 模型，旨在提供長達 15 天的 6 小時全球集合天氣預測。此模型以顯著改善的 0.25{\deg} 空間解析度運行，結合了 13 個壓力層的 5 個高空大氣變數，以及 13 個地表變數。通過利用變分自動編碼器 (VAE) 固有的機率性質，FuXi-ENS 最佳化了結合連續等級機率分數 (CRPS) 和預測分佈與目標分佈之間的 KL 散度的損失函數。這種創新方法代表了在 VAE 用於集合天氣預測時，對標準 VAE 模型中 L1 損失與 KL 損失的傳統使用的一種進步。評估結果表明，FuXi-ENS 在 CRPS 上勝過歐洲中期天氣預報中心 (ECMWF) 的集合預測，後者是世界領先的 NWP 模型，在 360 個變數和預測提前時間組合中的 98.1%。

##### **Diag2Diag: Multi modal super resolution for physics discovery with application to fusion**
2405.05908v1 by Azarakhsh Jalalvand, Max Curie, SangKyeun Kim, Peter Steiner, Jaemin Seo, Qiming Hu, Andrew Oakleigh Nelson, Egemen Kolemen

This paper introduces a groundbreaking multi-modal neural network model
designed for resolution enhancement, which innovatively leverages
inter-diagnostic correlations within a system. Traditional approaches have
primarily focused on uni-modal enhancement strategies, such as pixel-based
image enhancement or heuristic signal interpolation. In contrast, our model
employs a novel methodology by harnessing the diagnostic relationships within
the physics of fusion plasma. Initially, we establish the correlation among
diagnostics within the tokamak. Subsequently, we utilize these correlations to
substantially enhance the temporal resolution of the Thomson Scattering
diagnostic, which assesses plasma density and temperature. By increasing its
resolution from conventional 200Hz to 500kHz, we facilitate a new level of
insight into plasma behavior, previously attainable only through
computationally intensive simulations. This enhancement goes beyond simple
interpolation, offering novel perspectives on the underlying physical phenomena
governing plasma dynamics.

摘要：本文介紹了一種創新的多模態神經網路模型，專為解析度提升而設計，它創新地利用系統內的診斷間相關性。傳統方法主要集中於單模態增強策略，例如基於像素的影像增強或啟發式訊號插值。相反地，我們的模型採用一種新的方法，利用融合電漿物理學中的診斷關係。最初，我們在托卡馬克內建立診斷之間的相關性。隨後，我們利用這些相關性來大幅提升湯姆森散射診斷的時間解析度，該診斷評估電漿密度和溫度。透過將其解析度從傳統的 200Hz 提升至 500kHz，我們促成對電漿行為的新層次見解，以前只能透過計算密集型模擬才能達成。這種增強超越了單純的插值，提供了對支配電漿動態的基礎物理現象的新觀點。

##### **Truthful Aggregation of LLMs with an Application to Online Advertising**
2405.05905v1 by Ermis Soumalias, Michael J. Curry, Sven Seuken

We address the challenge of aggregating the preferences of multiple agents
over LLM-generated replies to user queries, where agents might modify or
exaggerate their preferences. New agents may participate for each new query,
making fine-tuning LLMs on these preferences impractical. To overcome these
challenges, we propose an auction mechanism that operates without fine-tuning
or access to model weights. This mechanism is designed to provably converge to
the ouput of the optimally fine-tuned LLM as computational resources are
increased. The mechanism can also incorporate contextual information about the
agents when avaiable, which significantly accelerates its convergence. A
well-designed payment rule ensures that truthful reporting is the optimal
strategy for all agents, while also promoting an equity property by aligning
each agent's utility with her contribution to social welfare - an essential
feature for the mechanism's long-term viability. While our approach can be
applied whenever monetary transactions are permissible, our flagship
application is in online advertising. In this context, advertisers try to steer
LLM-generated responses towards their brand interests, while the platform aims
to maximize advertiser value and ensure user satisfaction. Experimental results
confirm that our mechanism not only converges efficiently to the optimally
fine-tuned LLM but also significantly boosts advertiser value and platform
revenue, all with minimal computational overhead.

摘要：<paragraph>我們解決了匯總多個代理對 LLM 生成的使用者查詢回覆的偏好的挑戰，其中代理可能會修改或誇大其偏好。新的代理可能會參與每個新的查詢，這使得對這些偏好進行微調 LLM 變得不切實際。為了克服這些挑戰，我們提出了一種拍賣機制，它在不微調或訪問模型權重的條件下運作。此機制旨在隨著計算資源的增加，可證明地收斂到經過最佳微調的 LLM 的輸出。該機制還可以納入有關代理的上下文訊息（如果可用），這會顯著加速其收斂。設計良好的支付規則確保如實報告是所有代理的最佳策略，同時還通過將每個代理的效用與其對社會福利的貢獻相一致來促進公平屬性——這是該機制長期可行性的基本特徵。儘管我們的方法可以在允許金錢交易時應用，但我們的旗艦應用是在線上廣告中。在此背景下，廣告客戶試圖將 LLM 生成的回應引導至其品牌利益，而平台旨在最大化廣告客戶價值並確保使用者滿意度。實驗結果證實，我們的機制不僅有效地收斂到經過最佳微調的 LLM，而且還顯著提升了廣告客戶價值和平台收益，所有這些都以最小的計算開銷實現。</paragraph>

##### **Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?**
2405.05904v1 by Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig

When large language models are aligned via supervised fine-tuning, they may
encounter new factual information that was not acquired through pre-training.
It is often conjectured that this can teach the model the behavior of
hallucinating factually incorrect responses, as the model is trained to
generate facts that are not grounded in its pre-existing knowledge. In this
work, we study the impact of such exposure to new knowledge on the capability
of the fine-tuned model to utilize its pre-existing knowledge. To this end, we
design a controlled setup, focused on closed-book QA, where we vary the
proportion of the fine-tuning examples that introduce new knowledge. We
demonstrate that large language models struggle to acquire new factual
knowledge through fine-tuning, as fine-tuning examples that introduce new
knowledge are learned significantly slower than those consistent with the
model's knowledge. However, we also find that as the examples with new
knowledge are eventually learned, they linearly increase the model's tendency
to hallucinate. Taken together, our results highlight the risk in introducing
new factual knowledge through fine-tuning, and support the view that large
language models mostly acquire factual knowledge through pre-training, whereas
fine-tuning teaches them to use it more efficiently.

摘要：當大型語言模型透過監督式微調來對齊時，可能會遇到在預訓練中未獲得的新事實資訊。人們經常猜測，這可能會教導模型虛構事實上不正確的回應的行為，因為該模型經過訓練可以產生不基於其既有知識的事實。在這項工作中，我們研究了這種接觸新知識對微調模型利用其既有知識的能力的影響。為此，我們設計了一個受控的設定，專注於閉卷問答，其中我們改變了引入新知識的微調範例的比例。我們證明了大型語言模型難以透過微調來獲得新的事實知識，因為引入新知識的微調範例的學習速度顯著低於與模型知識一致的範例。然而，我們也發現，隨著具有新知識的範例最終被學習，它們會線性增加模型虛構的趨勢。綜合起來，我們的結果突顯了透過微調引入新的事實知識的風險，並支持這樣一種觀點，即大型語言模型主要透過預訓練來獲得事實知識，而微調則教導它們更有效地使用它。

##### **Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons**
2405.05894v1 by Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales

LLM-as-a-judge approaches are a practical and effective way of assessing a
range of text tasks, aligning with human judgements especially when applied in
a comparative assessment fashion. However, when using pairwise comparisons to
rank a set of candidates the computational costs scale quadratically with the
number of candidates, which can have practical limitations. This paper
introduces a Product of Expert (PoE) framework for efficient LLM Comparative
Assessment. Here individual comparisons are considered experts that provide
information on a pair's score difference. The PoE framework combines the
information from these experts to yield an expression that can be maximized
with respect to the underlying set of candidates, and is highly flexible where
any form of expert can be assumed. When Gaussian experts are used one can
derive simple closed-form solutions for the optimal candidate ranking, as well
as expressions for selecting which comparisons should be made to maximize the
probability of this ranking. Our approach enables efficient comparative
assessment, where by using only a small subset of the possible comparisons, one
can generate score predictions that correlate as well to human judgements as
the predictions when all comparisons are used. We evaluate the approach on
multiple NLG tasks and demonstrate that our framework can yield considerable
computational savings when performing pairwise comparative assessment. When N
is large, with as few as 2% of comparisons the PoE solution can achieve similar
performance to when all comparisons are used.

摘要：LLM-as-a-judge 方法是一種評估各種文本任務的實用且有效的方法，特別是在以比較評估方式應用時，與人類判斷一致。然而，當使用成對比較對一組候選者進行排名時，計算成本會隨著候選者數量呈二次方擴展，這可能會帶來實際限制。本文介紹了一個專家產品 (PoE) 框架，用於高效的 LLM 比較評估。在此，個別比較被視為專家，提供有關一對分數差異的資訊。PoE 框架結合了這些專家的資訊，產生了一個表達式，可以針對基礎候選者集合進行最大化，並且非常靈活，可以假設任何形式的專家。當使用高斯專家時，可以為最佳候選者排名推導出簡單的閉合形式解，以及用於選擇哪些比較以最大化此排名的機率的表達式。我們的做法可以進行有效的比較評估，只需使用可能比較的一小部分子集，就可以產生與人類判斷相關的評分預測，就像使用所有比較時的預測一樣。我們在多個 NLG 任務上評估了這種方法，並證明了我們的框架在執行成對比較評估時可以產生大量的計算節省。當 N 很大的時候，PoE 解決方案只需 2% 的比較次數，就能達到與使用所有比較次數時類似的效能。

##### **Safe Exploration Using Bayesian World Models and Log-Barrier Optimization**
2405.05890v1 by Yarden As, Bhavya Sukhija, Andreas Krause

A major challenge in deploying reinforcement learning in online tasks is
ensuring that safety is maintained throughout the learning process. In this
work, we propose CERL, a new method for solving constrained Markov decision
processes while keeping the policy safe during learning. Our method leverages
Bayesian world models and suggests policies that are pessimistic w.r.t. the
model's epistemic uncertainty. This makes CERL robust towards model
inaccuracies and leads to safe exploration during learning. In our experiments,
we demonstrate that CERL outperforms the current state-of-the-art in terms of
safety and optimality in solving CMDPs from image observations.

摘要：在線上任務中部署強化學習的一項重大挑戰是確保在整個學習過程中維持安全性。在此工作中，我們提出 CERL，一種在學習過程中保持策略安全性的約束馬可夫決策過程求解新方法。我們的模型利用貝氏世界模型，並提出對模型認識不確定性持悲觀態度的策略。這使得 CERL 對模型不準確性具有魯棒性，並在學習過程中進行安全探索。在我們的實驗中，我們證明 CERL 在從影像觀察中解決 CMDP 的安全性和最佳性方面優於當前的最新技術。

##### **Composable Part-Based Manipulation**
2405.05876v1 by Weiyu Liu, Jiayuan Mao, Joy Hsu, Tucker Hermans, Animesh Garg, Jiajun Wu

In this paper, we propose composable part-based manipulation (CPM), a novel
approach that leverages object-part decomposition and part-part correspondences
to improve learning and generalization of robotic manipulation skills. By
considering the functional correspondences between object parts, we
conceptualize functional actions, such as pouring and constrained placing, as
combinations of different correspondence constraints. CPM comprises a
collection of composable diffusion models, where each model captures a
different inter-object correspondence. These diffusion models can generate
parameters for manipulation skills based on the specific object parts.
Leveraging part-based correspondences coupled with the task decomposition into
distinct constraints enables strong generalization to novel objects and object
categories. We validate our approach in both simulated and real-world
scenarios, demonstrating its effectiveness in achieving robust and generalized
manipulation capabilities.

摘要：在本文中，我們提出可組成部分式操作 (CPM)，這是一種新穎的方法，它利用物件部分分解和部分對應來改善機器人操作技能的學習和概化。透過考量物件部分之間的功能對應，我們將功能動作（例如倒水和受限放置）概念化為不同對應約束的組合。CPM 包含一組可組成的擴散模型，其中每個模型都擷取不同的物件間對應。這些擴散模型可根據特定的物件部分產生操作技能的參數。利用部分對應以及將任務分解為不同約束，能對新物件和物件類別進行強大的概化。我們在模擬和真實世界的場景中驗證了我們的做法，證明了其在達成穩健且概化的操作能力方面的有效性。

##### **ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers**
2405.05861v1 by Liangliang Chen, Shiyu Jin, Haoyu Wang, Liangjun Zhang

Excavators are crucial for diverse tasks such as construction and mining,
while autonomous excavator systems enhance safety and efficiency, address labor
shortages, and improve human working conditions. Different from the existing
modularized approaches, this paper introduces ExACT, an end-to-end autonomous
excavator system that processes raw LiDAR, camera data, and joint positions to
control excavator valves directly. Utilizing the Action Chunking with
Transformers (ACT) architecture, ExACT employs imitation learning to take
observations from multi-modal sensors as inputs and generate actionable
sequences. In our experiment, we build a simulator based on the captured
real-world data to model the relations between excavator valve states and joint
velocities. With a few human-operated demonstration data trajectories, ExACT
demonstrates the capability of completing different excavation tasks, including
reaching, digging and dumping through imitation learning in validations with
the simulator. To the best of our knowledge, ExACT represents the first
instance towards building an end-to-end autonomous excavator system via
imitation learning methods with a minimal set of human demonstrations. The
video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.

摘要：挖掘機對於建築和採礦等各種任務至關重要，而自動化挖掘機系統則可增進安全性與效率，解決勞動力短缺問題，並改善人類的工作條件。與現有的模組化方法不同，本文介紹了 ExACT，這是一個端對端的自動化挖掘機系統，可處理原始 LiDAR、相機資料和接合位置，以直接控制挖掘機閥門。ExACT 利用動作分塊與 Transformer (ACT) 架構，採用模仿學習，將來自多模式感測器的觀察結果作為輸入，並產生可操作的序列。在我們的實驗中，我們根據擷取的真實世界資料建立了一個模擬器，以模擬挖掘機閥門狀態與接合速度之間的關係。透過一些由人類操作的示範資料軌跡，ExACT 證明了其完成不同挖掘任務的能力，包括透過模擬器中的模仿學習來進行觸及、挖掘和傾倒。據我們所知，ExACT 代表了透過模仿學習方法，使用最少的人類示範來建立端對端自動化挖掘機系統的第一個實例。可以在 https://youtu.be/NmzR_Rf-aEk 上觀看有關這項工作的影片。

##### **Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control**
2405.05852v1 by Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner

Embodied AI agents require a fine-grained understanding of the physical world
mediated through visual and language inputs. Such capabilities are difficult to
learn solely from task-specific data. This has led to the emergence of
pre-trained vision-language models as a tool for transferring representations
learned from internet-scale data to downstream tasks and new domains. However,
commonly used contrastively trained representations such as in CLIP have been
shown to fail at enabling embodied agents to gain a sufficiently fine-grained
scene understanding -- a capability vital for control. To address this
shortcoming, we consider representations from pre-trained text-to-image
diffusion models, which are explicitly optimized to generate images from text
prompts and as such, contain text-conditioned representations that reflect
highly fine-grained visuo-spatial information. Using pre-trained text-to-image
diffusion models, we construct Stable Control Representations which allow
learning downstream control policies that generalize to complex, open-ended
environments. We show that policies learned using Stable Control
Representations are competitive with state-of-the-art representation learning
approaches across a broad range of simulated control settings, encompassing
challenging manipulation and navigation tasks. Most notably, we show that
Stable Control Representations enable learning policies that exhibit
state-of-the-art performance on OVMM, a difficult open-vocabulary navigation
benchmark.

摘要：具身人工智慧代理需要透過視覺和語言輸入，對物理世界有細緻的理解。這種能力很難僅從特定任務的資料中學習。這導致了預先訓練的視覺語言模型的出現，作為將從網路規模資料中學習到的表徵轉移到下游任務和新領域的工具。然而，已顯示出像 CLIP 中常用的對比訓練表徵，無法讓具身代理獲得足夠細緻的場景理解，而這對於控制來說是至關重要的能力。為了解決這個缺點，我們考慮了預先訓練的文字到影像擴散模型的表徵，這些表徵經過明確最佳化，以便從文字提示中產生影像，因此包含反映高度細緻視覺空間資訊的文字條件表徵。使用預先訓練的文字到影像擴散模型，我們建構了穩定的控制表徵，允許學習下游控制政策，並推廣到複雜的、開放式的環境。我們展示了使用穩定的控制表徵學習的政策，在廣泛的模擬控制設定中與最先進的表徵學習方法競爭，包括具有挑戰性的操作和導航任務。最值得注意的是，我們展示了穩定的控制表徵能讓學習政策在 OVMM 上展現最先進的效能，OVMM 是一個困難的開放式詞彙導航基準。

##### **Aequitas Flow: Streamlining Fair ML Experimentation**
2405.05809v1 by Sérgio Jesus, Pedro Saleiro, Inês Oliveira e Silva, Beatriz M. Jorge, Rita P. Ribeiro, João Gama, Pedro Bizarro, Rayid Ghani

Aequitas Flow is an open-source framework for end-to-end Fair Machine
Learning (ML) experimentation in Python. This package fills the existing
integration gaps in other Fair ML packages of complete and accessible
experimentation. It provides a pipeline for fairness-aware model training,
hyperparameter optimization, and evaluation, enabling rapid and simple
experiments and result analysis. Aimed at ML practitioners and researchers, the
framework offers implementations of methods, datasets, metrics, and standard
interfaces for these components to improve extensibility. By facilitating the
development of fair ML practices, Aequitas Flow seeks to enhance the adoption
of these concepts in AI technologies.

摘要：Aequitas Flow 是一個開放原始碼架構，用於端對端的公平機器學習 (ML) 實驗，以 Python 編寫。此套件填補了其他公平 ML 套件中現有的完整且可存取實驗整合差距。它提供了一個公平模型訓練、超參數最佳化和評估的管道，能進行快速且簡單的實驗和結果分析。此架構專為 ML 實務工作者和研究人員設計，提供方法、資料集、指標和這些元件的標準介面的實作，以改善可擴充性。透過促進公平 ML 實務的發展，Aequitas Flow 旨在提升這些概念在 AI 技術中的採用率。

##### **Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference**
2405.05803v1 by Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji

Multimodal large language models (MLLMs) demand considerable computations for
inference due to the extensive parameters and the additional input tokens
needed for visual information representation. Herein, we introduce Visual
Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid
inference. Our approach is inspired by two intriguing phenomena we have
observed: (1) the attention sink phenomenon that is prevalent in LLMs also
persists in MLLMs, suggesting that initial tokens and nearest tokens receive
the majority of attention, while middle vision tokens garner minimal attention
in deep layers; (2) the presence of information migration, which implies that
visual information is transferred to subsequent text tokens within the first
few layers of MLLMs. As per our findings, we conclude that vision tokens are
not necessary in the deep layers of MLLMs. Thus, we strategically withdraw them
at a certain layer, enabling only text tokens to engage in subsequent layers.
To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze
a limited set of tiny datasets and choose the first layer that meets the
Kullback-Leibler divergence criterion. Our VTW approach can cut computational
overhead by over 40\% across diverse multimodal tasks while maintaining
performance. Our code is released at https://github.com/lzhxmu/VTW.

摘要：多模态大型语言模型 (MLLM) 由于广泛的参数和视觉信息表示所需的附加输入标记，因此需要大量计算才能进行推理。在此，我们介绍视觉标记撤回 (VTW)，这是一个即插即用模块，用于提升 MLLM 以进行快速推理。我们的方法受到我们观察到的两个有趣的现象的启发：(1) 在 LLM 中普遍存在的注意力汇集现象也存在于 MLLM 中，表明初始标记和最近标记接收了大部分注意力，而中间视觉标记在深层中获得的注意力最少；(2) 信息迁移的存在，这意味着视觉信息在 MLLM 的前几层内被传输到后续文本标记。根据我们的发现，我们得出结论，视觉标记在 MLLM 的深层中是不必要的。因此，我们在某一层对其进行战略性撤回，仅允许文本标记参与后续层。为了精确定位视觉标记撤回的理想层，我们最初分析了一组有限的小型数据集，并选择了满足 Kullback-Leibler 散度准则的第一层。我们的 VTW 方法可以在保持性能的同时，在各种多模态任务中将计算开销减少 40% 以上。我们的代码已在 https://github.com/lzhxmu/VTW 中发布。

##### **RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**
2405.05792v1 by Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, Ian Reid

Mapping is crucial for spatial reasoning, planning and robot navigation.
Existing approaches range from metric, which require precise geometry-based
optimization, to purely topological, where image-as-node based graphs lack
explicit object-level reasoning and interconnectivity. In this paper, we
propose a novel topological representation of an environment based on "image
segments", which are semantically meaningful and open-vocabulary queryable,
conferring several advantages over previous works based on pixel-level
features. Unlike 3D scene graphs, we create a purely topological graph with
segments as nodes, where edges are formed by a) associating segment-level
descriptors between pairs of consecutive images and b) connecting neighboring
segments within an image using their pixel centroids. This unveils a
"continuous sense of a place", defined by inter-image persistence of segments
along with their intra-image neighbours. It further enables us to represent and
update segment-level descriptors through neighborhood aggregation using graph
convolution layers, which improves robot localization based on segment-level
retrieval. Using real-world data, we show how our proposed map representation
can be used to i) generate navigation plans in the form of "hops over segments"
and ii) search for target objects using natural language queries describing
spatial relations of objects. Furthermore, we quantitatively analyze data
association at the segment level, which underpins inter-image connectivity
during mapping and segment-level localization when revisiting the same place.
Finally, we show preliminary trials on segment-level `hopping' based zero-shot
real-world navigation. Project page with supplementary details:
oravus.github.io/RoboHop/

摘要：<paragraph>繪製地圖對於空間推理、規劃和機器人導航至關重要。
現有方法從度量（需要精確的基於幾何的最佳化）到純拓撲（其中以影像為節點的圖形缺乏明確的物件層級推理和互連性）不等。在本文中，我們提出一個基於「影像區段」的新穎拓撲表示，這些區段在語意上是有意義的，並且可以進行開放式詞彙查詢，與基於像素層級特徵的先前工作相比，具有多項優點。與 3D 場景圖形不同，我們使用區段作為節點建立一個純拓撲圖形，其中邊緣是由 a) 將區段層級描述符與連續兩張影像之間的配對關聯起來，以及 b) 使用其像素質心連接影像中的鄰近區段所形成的。這揭示了一個「連續的地方感」，它是由區段在影像之間的持續性及其在影像內的鄰近元素所定義的。它進一步使我們能夠使用圖形卷積層透過鄰域聚合來表示和更新區段層級描述符，這改進了基於區段層級擷取的機器人定位。使用真實世界資料，我們展示了我們提出的地圖表示如何用於 i) 以「區段跳躍」的形式產生導航計畫，以及 ii) 使用描述物件空間關係的自然語言查詢來搜尋目標物件。此外，我們定量分析了區段層級的資料關聯性，這在繪製地圖期間支撐了影像之間的連接性，以及在再次造訪同一個地方時的區段層級定位。最後，我們展示了基於區段層級「跳躍」的零次學習真實世界導航的初步試驗。包含補充詳細資訊的專案頁面：
oravus.github.io/RoboHop/</paragraph>

##### **A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties**
2405.05790v1 by A. Noroozi, M. Ravan, B. Razavi, R. S. Fisher, Y. Law, M. S. Hasan

In this paper, we present a robust version of the well-known exact
low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,
to localize brain sources in the presence of different forward model
uncertainties. Methods: We first assume that the true lead field matrix is a
transformation of the existing lead field matrix distorted by uncertainties and
propose an iterative approach to estimate this transformation accurately. Major
sources of the forward model uncertainties, including differences in geometry,
conductivity, and source space resolution between the real and simulated head
models, and misaligned electrode positions, are then simulated to test the
proposed method. Results: ReLORETA and eLORETA are applied to simulated focal
sources in different regions of the brain and the presence of various noise
levels as well as real data from a patient with focal epilepsy. The results
show that ReLORETA is considerably more robust and accurate than eLORETA in all
cases. Conclusion: Having successfully dealt with the forward model
uncertainties, ReLORETA proved to be a promising method for real-world clinical
applications. Significance: eLORETA is one of the localization techniques that
could be used to study brain activity for medical applications such as
determining the epileptogenic zone in patients with medically refractory
epilepsy. However, the major limitation of eLORETA is sensitivity to the
uncertainties in the forward model. Since this problem can substantially
undermine its performance in real-world applications where the exact lead field
matrix is unknown, developing a more robust method capable of dealing with
these uncertainties is of significant interest.

摘要：<paragraph>在本文中，我們提出了一種強健版的著名精確低解析度電磁斷層掃描 (eLORETA) 技術，名為 ReLORETA，用於定位存在不同正向模型不確定性的腦部來源。方法：我們首先假設真實導線場矩陣是不確定性扭曲的現有導線場矩陣的轉換，並提出了一種反覆運算方法來準確估計此轉換。然後模擬正向模型不確定性的主要來源，包括真實和模擬頭部模型之間的幾何形狀、電導率和源空間解析度的差異，以及電極位置未對齊，以測試所提出的方法。結果：ReLORETA 和 eLORETA 應用於腦部不同區域的模擬焦點來源，以及各種噪聲級別以及來自癲癇患者的真實數據。結果表明，在所有情況下，ReLORETA 都比 eLORETA 更強健且更準確。結論：在成功處理了正向模型不確定性後，ReLORETA 被證明是一種很有前景的真實世界臨床應用方法。意義：eLORETA 是一種定位技術，可用於研究腦部活動，用於醫療應用，例如確定藥物難治性癲癇患者的癲癇發作區。然而，eLORETA 的主要限制是對正向模型的不確定性敏感。由於此問題可能會嚴重損害其在未知確切導線場矩陣的真實世界應用中的性能，因此開發一種能夠應對這些不確定性的更強健的方法具有重大意義。</paragraph>

##### **Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language**
2405.05777v1 by Ronny Paul, Himanshu Buckchash, Shantipriya Parida, Dilip K. Prasad

S\'ami, an indigenous language group comprising multiple languages, faces
digital marginalization due to the limited availability of data and
sophisticated language models designed for its linguistic intricacies. This
work focuses on increasing technological participation for the S\'ami language.
We draw the attention of the ML community towards the language modeling problem
of Ultra Low Resource (ULR) languages. ULR languages are those for which the
amount of available textual resources is very low, and the speaker count for
them is also very low. ULRLs are also not supported by mainstream Large
Language Models (LLMs) like ChatGPT, due to which gathering artificial training
data for them becomes even more challenging. Mainstream AI foundational model
development has given less attention to this category of languages. Generally,
these languages have very few speakers, making it hard to find them. However,
it is important to develop foundational models for these ULR languages to
promote inclusion and the tangible abilities and impact of LLMs. To this end,
we have compiled the available S\'ami language resources from the web to create
a clean dataset for training language models. In order to study the behavior of
modern LLM models with ULR languages (S\'ami), we have experimented with
different kinds of LLMs, mainly at the order of $\sim$ seven billion
parameters. We have also explored the effect of multilingual LLM training for
ULRLs. We found that the decoder-only models under a sequential multilingual
training scenario perform better than joint multilingual training, whereas
multilingual training with high semantic overlap, in general, performs better
than training from scratch.This is the first study on the S\'ami language for
adapting non-statistical language models that use the latest developments in
the field of natural language processing (NLP).

摘要：S'ami 是一種包含多種語言的原住民語言群，由於缺乏資料和針對其語言複雜性而設計的精緻語言模型，面臨數位邊緣化。這項工作專注於增加 S'ami 語言的科技參與。我們將機器學習社群的注意力引導至極低資源 (ULR) 語言的語言建模問題。ULR 語言是指可用文字資源非常少，且說話者數量也很少的語言。ULRL 也不受主流大型語言模型 (LLM)（例如 ChatGPT）支援，因此為其收集人工訓練資料變得更具挑戰性。主流 AI 基礎模型開發較少關注這類語言。一般來說，這些語言的使用者很少，因此很難找到他們。然而，開發這些 ULR 語言的基礎模型非常重要，以促進包容性以及 LLM 的實際能力和影響力。為此，我們已從網路上編譯可用的 S'ami 語言資源，以建立一個用於訓練語言模型的乾淨資料集。為了研究現代 LLM 模型在 ULR 語言（S'ami）中的行為，我們已針對不同類型的 LLM 進行實驗，主要約為 70 億個參數。我們也探討了多語言 LLM 訓練對 ULRL 的影響。我們發現，在順序多語言訓練場景下的僅解碼器模型表現優於聯合多語言訓練，而一般而言，語意重疊度高的多語言訓練表現優於從頭開始訓練。這是第一個針對 S'ami 語言的研究，用於調整非統計語言模型，這些模型使用自然語言處理 (NLP) 領域的最新發展。

##### **Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions**
2405.05776v1 by Polina Tsvilodub, Paul Marty, Sonia Ramotowska, Jacopo Romoli, Michael Franke

Human communication is based on a variety of inferences that we draw from
sentences, often going beyond what is literally said. While there is wide
agreement on the basic distinction between entailment, implicature, and
presupposition, the status of many inferences remains controversial. In this
paper, we focus on three inferences of plain and embedded disjunctions, and
compare them with regular scalar implicatures. We investigate this comparison
from the novel perspective of the predictions of state-of-the-art large
language models, using the same experimental paradigms as recent studies
investigating the same inferences with humans. The results of our best
performing models mostly align with those of humans, both in the large
differences we find between those inferences and implicatures, as well as in
fine-grained distinctions among different aspects of those inferences.

摘要：人類溝通建立在我們從句子中得出的各種推論上，通常超越了字面上的意思。雖然在蘊涵、含義和預設的基本區別上達成廣泛共識，但許多推論的狀態仍然存在爭議。在本文中，我們專注於普通和嵌入式析取的三個推論，並將它們與常規標量含義比較。我們從最先進的大語言模型預測的新穎角度調查了這種比較，使用與最近研究使用人類調查相同推論的相同實驗範例。我們表現最好的模型的結果大多與人類的結果一致，無論是在我們發現的這些推論和含義之間的巨大差異中，還是在這些推論的不同方面的細微區別中。

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel Miró-Nicolau, Gabriel Moyà-Alcover, Antoni Jaume-i-Capó, Manuel González-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

摘要：隨著對深度學習模型依賴性的增加，加上其固有的透明度不足，促使一個新的研究領域發展，稱為可解釋 AI (XAI) 方法。這些方法旨在透過深入了解決策背後的原理，來提升最終使用者對自動化系統的信賴。本文提出了一種衡量使用者對 XAI 系統信賴度的新穎方法，允許對其進行改進。我們提出的指標結合了客觀觀點下的效能指標和信賴指標。為了驗證這個新穎的方法，我們在一個真實的醫療場景中進行了一個案例研究：使用 XAI 系統從 X 光影像中偵測肺炎。

##### **DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models**
2405.05763v1 by Mengxiao Geng, Jiahao Zhu, Xiaolin Zhu, Qiqing Liu, Dong Liang, Qiegen Liu

Detail features of magnetic resonance images play a cru-cial role in accurate
medical diagnosis and treatment, as they capture subtle changes that pose
challenges for doc-tors when performing precise judgments. However, the widely
utilized naive diffusion model has limitations, as it fails to accurately
capture more intricate details. To en-hance the quality of MRI reconstruction,
we propose a comprehensive detail-preserving reconstruction method using
multiple diffusion models to extract structure and detail features in k-space
domain instead of image do-main. Moreover, virtual binary modal masks are
utilized to refine the range of values in k-space data through highly adaptive
center windows, which allows the model to focus its attention more efficiently.
Last but not least, an inverted pyramid structure is employed, where the
top-down image information gradually decreases, ena-bling a cascade
representation. The framework effective-ly represents multi-scale sampled data,
taking into ac-count the sparsity of the inverted pyramid architecture, and
utilizes cascade training data distribution to repre-sent multi-scale data.
Through a step-by-step refinement approach, the method refines the
approximation of de-tails. Finally, the proposed method was evaluated by
con-ducting experiments on clinical and public datasets. The results
demonstrate that the proposed method outper-forms other methods.

摘要：磁共振影像的細節特徵在準確的醫療診斷和治療中扮演著至關重要的角色，因為它們捕捉到細微的變化，對醫生執行精確判斷時構成挑戰。然而，廣泛使用的樸素擴散模型有其局限性，因為它無法準確捕捉到更複雜的細節。為了增強 MRI 重建的品質，我們提出了一種使用多個擴散模型的全面細節保留重建方法，在 k 空間域中提取結構和細節特徵，而不是影像域。此外，虛擬二元模態遮罩被用於透過高度自適應中心視窗來改善 k 空間資料中的值域，這使得模型可以更有效率地集中其注意力。最後但並非最不重要的是，採用了一個倒金字塔結構，其中自上而下的影像資訊逐漸減少，實現了串聯表示。該架構有效地表示多尺度取樣資料，考慮了倒金字塔架構的稀疏性，並利用串聯訓練資料分佈來表示多尺度資料。透過逐步細化的方式，該方法改善了細節的近似值。最後，透過在臨床和公共資料集上進行實驗，對所提出的方法進行了評估。結果表明，所提出的方法優於其他方法。

##### **Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media**
2405.05760v1 by Zhizhen Zhang, Ning Wang, Haojie Li, Zhihui Wang

The purpose of semantic location prediction is to extract relevant semantic
location information from multimodal social media posts, offering a more
contextual understanding of daily activities compared to GPS coordinates.
However, this task becomes challenging due to the presence of noise and
irrelevant information in "text-image" pairs. Existing methods suffer from
insufficient feature representations and fail to consider the comprehensive
integration of similarity at different granularities, making it difficult to
filter out noise and irrelevant information. To address these challenges, we
propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for
predicting social users' semantic locations. First, we utilize a pre-trained
large-scale vision-language model to extract high-quality feature
representations from social media posts. Then, we introduce a Similarity-Guided
Interaction Module (SIM) to alleviate modality heterogeneity and noise
interference by incorporating coarse-grained and fine-grained similarity
guidance for modality interactions. Specifically, we propose a novel
similarity-aware feature interpolation attention mechanism at the coarse level,
leveraging modality-wise similarity to mitigate heterogeneity and reduce noise
within each modality. Meanwhile, we employ a similarity-aware feed-forward
block at the fine level, utilizing element-wise similarity to further mitigate
the impact of modality heterogeneity. Building upon pre-processed features with
minimal noise and modal interference, we propose a Similarity-aware Feature
Fusion Module (SFM) to fuse two modalities with cross-attention mechanism.
Comprehensive experimental results demonstrate the superior performance of our
proposed method in handling modality imbalance while maintaining efficient
fusion effectiveness.

摘要：语义位置预测的目的是从多模态社交媒体帖子中提取相关的语义位置信息，与 GPS 坐标相比，提供对日常活动的更具背景意义的理解。然而，由于“文本-图像”对中存在噪声和无关信息，这项任务变得具有挑战性。现有方法存在特征表示不足的问题，并且无法考虑在不同粒度上的相似性的综合集成，这使得难以滤除噪声和无关信息。为了应对这些挑战，我们提出了一种用于预测社交用户语义位置的相似性引导多模态融合 Transformer (SG-MFT)。首先，我们利用预训练的大规模视觉语言模型从社交媒体帖子中提取高质量的特征表示。然后，我们引入了一个相似性引导交互模块 (SIM)，通过为模态交互纳入粗粒度和细粒度相似性指导来缓解模态异构性和噪声干扰。具体来说，我们在粗粒度上提出了一种新颖的相似性感知特征插值注意力机制，利用模态相似性来减轻异构性并降低每个模态内的噪声。同时，我们在细粒度上采用相似性感知前馈块，利用元素级相似性进一步减轻模态异构性的影响。在经过预处理的、噪声和模态干扰最小的特征之上，我们提出了一种相似性感知特征融合模块 (SFM)，以交叉注意力机制融合两种模态。全面的实验结果证明了我们提出的方法在处理模态不平衡的同时保持高效融合有效性方面的卓越性能。

##### **Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma**
2405.05758v1 by Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee

Qualitative analysis is a challenging, yet crucial aspect of advancing
research in the field of Human-Computer Interaction (HCI). Recent studies show
that large language models (LLMs) can perform qualitative coding within
existing schemes, but their potential for collaborative human-LLM discovery and
new insight generation in qualitative analysis is still underexplored. To
bridge this gap and advance qualitative analysis by harnessing the power of
LLMs, we propose CHALET, a novel methodology that leverages the human-LLM
collaboration paradigm to facilitate conceptualization and empower qualitative
research. The CHALET approach involves LLM-supported data collection,
performing both human and LLM deductive coding to identify disagreements, and
performing collaborative inductive coding on these disagreement cases to derive
new conceptual insights. We validated the effectiveness of CHALET through its
application to the attribution model of mental-illness stigma, uncovering
implicit stigmatization themes on cognitive, emotional and behavioral
dimensions. We discuss the implications for future research, methodology, and
the transdisciplinary opportunities CHALET presents for the HCI community and
beyond.

摘要：質性分析是推進人機互動 (HCI) 領域研究的挑戰性但至關重要的方面。最近的研究表明，大型語言模型 (LLM) 能在現有架構中執行質性編碼，但它們在質性分析中進行協作式人機發現和產生新見解的潛力仍未得到充分探索。為了彌合這一差距，並透過利用 LLM 的力量推進質性分析，我們提出了 CHALET，一種創新的方法論，它利用人機協作範例來促進概念化並賦能質性研究。CHALET 方法涉及 LLM 支持的資料收集，執行人類和 LLM 演繹編碼以找出分歧，並對這些分歧案例執行協作式歸納編碼以得出新的概念見解。我們透過將 CHALET 應用於心理疾病污名歸因模型，驗證了其有效性，揭示了認知、情緒和行為面向的隱性污名化主題。我們討論了 CHALET 為 HCI 社群及其他領域帶來的未來研究、方法論和跨學科機會的影響。

##### **CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks**
2405.05755v1 by Nick, Nikzad, Yongsheng Gao, Jun Zhou

In recent years, convolutional neural networks (CNNs) with channel-wise
feature refining mechanisms have brought noticeable benefits to modelling
channel dependencies. However, current attention paradigms fail to infer an
optimal channel descriptor capable of simultaneously exploiting statistical and
spatial relationships among feature maps. In this paper, to overcome this
shortcoming, we present a novel channel-wise spatially autocorrelated (CSA)
attention mechanism. Inspired by geographical analysis, the proposed CSA
exploits the spatial relationships between channels of feature maps to produce
an effective channel descriptor. To the best of our knowledge, this is the f
irst time that the concept of geographical spatial analysis is utilized in deep
CNNs. The proposed CSA imposes negligible learning parameters and light
computational overhead to the deep model, making it a powerful yet efficient
attention module of choice. We validate the effectiveness of the proposed CSA
networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and
MS COCO benchmark datasets for image classification, object detection, and
instance segmentation. The experimental results demonstrate that CSA-Nets are
able to consistently achieve competitive performance and superior
generalization than several state-of-the-art attention-based CNNs over
different benchmark tasks and datasets.

摘要：近年來，具有通道級特徵精煉機制的卷積神經網路 (CNN) 為通道依賴關係建模帶來了顯著的好處。然而，當前的注意力範例無法推斷出同時利用特徵圖中統計和空間關係的最佳通道描述符。在本文中，為了克服這個缺點，我們提出了一種新的通道級空間自相關 (CSA) 注意力機制。受地理分析的啟發，所提出的 CSA 利用特徵圖通道之間的空間關係來產生有效的通道描述符。據我們所知，這是地理空間分析的概念首次用於深度 CNN。所提出的 CSA 對深度模型施加了可忽略的學習參數和輕量級的運算開銷，使其成為功能強大且高效的注意力模組。我們透過在 ImageNet 和 MS COCO 基準資料集上進行廣泛的實驗和分析，驗證了所提出的 CSA 網路 (CSA-Nets) 的有效性，用於影像分類、物件偵測和實例分割。實驗結果表明，與多種現有注意力式 CNN 相比，CSA-Nets 能在不同的基準任務和資料集上持續達成具競爭力的效能和優越的泛化能力。

##### **Can large language models understand uncommon meanings of common words?**
2405.05741v1 by Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao

Large language models (LLMs) like ChatGPT have shown significant advancements
across diverse natural language understanding (NLU) tasks, including
intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged
testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely
comprehend the world' remains unclear, fostering numerous studies and sparking
heated debates. Prevailing research mainly focuses on surface-level NLU,
neglecting fine-grained explorations. However, such explorations are crucial
for understanding their unique comprehension mechanisms, aligning with human
cognition, and finally enhancing LLMs' general NLU capacities. To address this
gap, our study delves into LLMs' nuanced semantic comprehension capabilities,
particularly regarding common words with uncommon meanings. The idea stems from
foundational principles of human communication within psychology, which
underscore accurate shared understandings of word semantics. Specifically, this
paper presents the innovative construction of a Lexical Semantic Comprehension
(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing
both fine-grained and cross-lingual dimensions. Introducing models of both
open-source and closed-source, varied scales and architectures, our extensive
empirical experiments demonstrate the inferior performance of existing models
in this basic lexical-meaning understanding task. Notably, even the
state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%
and 22.3%, respectively. Additionally, multiple advanced prompting techniques
and retrieval-augmented generation are also introduced to help alleviate this
trouble, yet limitations persist. By highlighting the above critical
shortcomings, this research motivates further investigation and offers novel
insights for developing more intelligent LLMs.

摘要：大型語言模型 (LLM)，例如 ChatGPT，在各種自然語言理解 (NLU) 任務中已展現出顯著的進展，包括智能對話和自主代理。然而，由於缺乏廣泛認可的測試機制，回答「LLM 是隨機鸚鵡還是真正理解世界」的問題仍然不明確，這也促成了許多研究和引發激烈的辯論。現行的研究主要關注於表面層的 NLU，忽略了細緻的探討。然而，此類探討對於理解其獨特的理解機制、與人類認知保持一致，並最終增強 LLM 的一般 NLU 能力至關重要。為了解決這個差距，我們的研究深入探討了 LLM 的細微語義理解能力，特別是關於具有不尋常含義的常見詞彙。這個想法源自心理學中人類溝通的基本原理，它強調準確共享對詞彙語義的理解。具體來說，本文提出了創新的詞彙語義理解 (LeSC) 資料集建構，並採用新穎的評估指標，這是第一個涵蓋細緻和跨語言維度的基準。透過引入開源和閉源、不同規模和架構的模型，我們的廣泛實證實驗證明了現有模型在這個基本的詞彙含義理解任務中的表現較差。值得注意的是，即使是現今最先進的 LLM GPT-4 和 GPT-3.5 也分別落後於 16 歲的人類 3.9% 和 22.3%。此外，也引入了多種進階提示技術和檢索增強生成，以幫助緩解這個問題，但限制仍然存在。透過強調上述關鍵缺點，本研究激勵了進一步的調查，並為開發更智能的 LLM 提供了新穎的見解。

##### **Computational lexical analysis of Flamenco genres**
2405.05723v1 by Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez

Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of
Humanity, is a profound expression of cultural identity rooted in Andalusia,
Spain. However, there is a lack of quantitative studies that help identify
characteristic patterns in this long-lived music tradition. In this work, we
present a computational analysis of Flamenco lyrics, employing natural language
processing and machine learning to categorize over 2000 lyrics into their
respective Flamenco genres, termed as $\textit{palos}$. Using a Multinomial
Naive Bayes classifier, we find that lexical variation across styles enables to
accurately identify distinct $\textit{palos}$. More importantly, from an
automatic method of word usage, we obtain the semantic fields that characterize
each style. Further, applying a metric that quantifies the inter-genre distance
we perform a network analysis that sheds light on the relationship between
Flamenco styles. Remarkably, our results suggest historical connections and
$\textit{palo}$ evolutions. Overall, our work illuminates the intricate
relationships and cultural significance embedded within Flamenco lyrics,
complementing previous qualitative discussions with quantitative analyses and
sparking new discussions on the origin and development of traditional music
genres.

摘要：弗拉門戈舞，被聯合國教科文組織認定為人類非物質文化遺產的一部分，是根植於西班牙安達盧西亞的文化認同的深刻表達。然而，缺乏定量的研究來幫助識別這一悠久的音樂傳統中的特徵模式。在這項工作中，我們提出了弗拉門戈歌詞的計算分析，採用自然語言處理和機器學習將 2000 多首歌詞分類到各自的弗拉門戈流派中，稱為$\textit{palos}$。使用多項式樸素貝葉斯分類器，我們發現不同風格的詞彙變異能夠準確識別不同的$\textit{palos}$。更重要的是，從一種自動化的詞彙使用方式中，我們獲得了表徵每種風格的語義場。此外，應用量化流派間距離的度量標準，我們進行了一項網路分析，闡明了弗拉門戈風格之間的關係。值得注意的是，我們的結果表明了歷史聯繫和$\textit{palo}$的演變。總的來說，我們的作品闡明了弗拉門戈歌詞中嵌入的複雜關係和文化意義，用定量分析補充了先前的定性討論，並引發了對傳統音樂流派起源和發展的新討論。

##### **Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution**
2405.05705v1 by Sandrine Chausson, Björn Ross

Many tasks related to Computational Social Science and Web Content Analysis
involve classifying pieces of text based on the claims they contain.
State-of-the-art approaches usually involve fine-tuning models on large
annotated datasets, which are costly to produce. In light of this, we propose
and release a qualitative and versatile few-shot learning methodology as a
common paradigm for any claim-based textual classification task. This
methodology involves defining the classes as arbitrarily sophisticated
taxonomies of claims, and using Natural Language Inference models to obtain the
textual entailment between these and a corpus of interest. The performance of
these models is then boosted by annotating a minimal sample of data points,
dynamically sampled using the well-established statistical heuristic of
Probabilistic Bisection. We illustrate this methodology in the context of three
tasks: climate change contrarianism detection, topic/stance classification and
depression-relates symptoms detection. This approach rivals traditional
pre-train/fine-tune approaches while drastically reducing the need for data
annotation.

摘要：許多與運算社會科學和網路內容分析相關的任務，涉及根據文字中包含的論點對文字片段進行分類。最先進的方法通常涉及對大型註解資料集進行模型微調，而這些資料集的製作成本很高。有鑑於此，我們提出並發布一種定性且通用的少樣本學習方法論，作為基於論點的文字分類任務的通用範例。此方法論涉及將類別定義為任意複雜的論點分類法，並使用自然語言推論模型來取得這些論點與感興趣的語料庫之間的文字蘊涵。然後，透過註解最少量的資料點來提升這些模型的效能，並使用已建立良好的機率二分統計啟發法動態取樣。我們在以下三個任務的脈絡中說明此方法論：氣候變遷反對論檢測、主題/立場分類和憂鬱相關症狀檢測。此方法與傳統的預訓練/微調方法競爭，同時大幅減少了資料註解的需求。

##### **Evaluating Dialect Robustness of Language Models via Conversation Understanding**
2405.05688v1 by Dipankar Srirag, Aditya Joshi

With an evergrowing number of LLMs reporting superlative performance for
English, their ability to perform equitably for different dialects of English
(i.e., dialect robustness) needs to be ascertained. Specifically, we use
English language (US English or Indian English) conversations between humans
who play the word-guessing game of `taboo'. We formulate two evaluative tasks:
target word prediction (TWP) (i.e.predict the masked target word in a
conversation) and target word selection (TWS) (i.e., select the most likely
masked target word in a conversation, from among a set of candidate words).
Extending MD3, an existing dialectic dataset of taboo-playing conversations, we
introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng
subsets. We add two subsets: AITrans (where dialectic information is removed
from IndEng) and AIGen (where LLMs are prompted to generate conversations). Our
evaluation uses pre-trained and fine-tuned versions of two closed-source
(GPT-4/3.5) and two open-source LLMs (Mistral and Gemma). LLMs perform
significantly better for US English than Indian English for both TWP and TWS,
for all settings. While GPT-based models perform the best, the comparatively
smaller models work more equitably for short conversations (<8 turns). Our
results on AIGen and AITrans (the best and worst-performing subset)
respectively show that LLMs may learn a dialect of their own based on the
composition of the training data, and that dialect robustness is indeed a
challenging task. Our evaluation methodology exhibits a novel way to examine
attributes of language models using pre-existing dialogue datasets.

摘要：隨著越來越多的 LLM 報告英語的卓越效能，需要確定它們對不同英語方言（即方言穩健性）執行公平的效能。具體來說，我們使用英語（美國英語或印度英語）對話，對話者是玩「禁忌」文字猜謎遊戲的人類。我們制定了兩個評估任務：目標詞彙預測（TWP）（即預測對話中被遮蓋的目標詞彙）和目標詞彙選擇（TWS）（即從一組候選詞彙中選擇對話中最可能的被遮蓋目標詞彙）。擴展 MD3，一個現有的禁忌對話方言數據集，我們引入了 M-MD3，一個目標詞彙遮蓋版本的 MD3，包含 USEng 和 IndEng 子集。我們增加了兩個子集：AITrans（從 IndEng 中移除了方言資訊）和 AIGen（提示 LLM 產生對話）。我們的評估使用預先訓練和微調的兩個閉源（GPT-4/3.5）和兩個開源 LLM（Mistral 和 Gemma）。對於 TWP 和 TWS，在所有設定中，LLM 對美國英語的表現明顯優於印度英語。雖然基於 GPT 的模型表現最佳，但較小的模型在較短的對話（<8 回合）中表現得更公平。我們對 AIGen 和 AITrans（表現最佳和最差的子集）的結果分別顯示，LLM 可能根據訓練資料的組成學習自己的方言，並且方言穩健性確實是一項具有挑戰性的任務。我們的評估方法展示了一種使用現有對話數據集檢查語言模型屬性的新方法。

##### **Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment**
2405.05678v1 by Yoonsu Kim, Kihoon Son, Seoyoung Kim, Juho Kim

AI intent alignment, ensuring that AI produces outcomes as intended by users,
is a critical challenge in human-AI interaction. The emergence of generative
AI, including LLMs, has intensified the significance of this problem, as
interactions increasingly involve users specifying desired results for AI
systems. In order to support better AI intent alignment, we aim to explore
human strategies for intent specification in human-human communication. By
studying and comparing human-human and human-LLM communication, we identify key
strategies that can be applied to the design of AI systems that are more
effective at understanding and aligning with user intent. This study aims to
advance toward a human-centered AI system by bringing together human
communication strategies for the design of AI systems.

摘要：人工智能意圖對齊，確保人工智能產生符合使用者意圖的結果，
是人機互動中的關鍵挑戰。生成式人工智能的出現，包括 LLM，加劇了這個問題的重要性，因為互動越來越涉及使用者為人工智能系統指定所需的結果。為了支援更好的 AI 意圖對齊，我們旨在探討人類在人際溝通中意圖規範的策略。透過研究和比較人際溝通和人機溝通，我們找出可以應用於設計 AI 系統的關鍵策略，這些系統更能理解和符合使用者的意圖。本研究旨在透過結合人類溝通策略來設計 AI 系統，朝著以人為中心的 AI 系統邁進。

##### **SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space**
2405.05636v1 by Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma

Combining face swapping with lip synchronization technology offers a
cost-effective solution for customized talking face generation. However,
directly cascading existing models together tends to introduce significant
interference between tasks and reduce video clarity because the interaction
space is limited to the low-level semantic RGB space. To address this issue, we
propose an innovative unified framework, SwapTalk, which accomplishes both face
swapping and lip synchronization tasks in the same latent space. Referring to
recent work on face generation, we choose the VQ-embedding space due to its
excellent editability and fidelity performance. To enhance the framework's
generalization capabilities for unseen identities, we incorporate identity loss
during the training of the face swapping module. Additionally, we introduce
expert discriminator supervision within the latent space during the training of
the lip synchronization module to elevate synchronization quality. In the
evaluation phase, previous studies primarily focused on the self-reconstruction
of lip movements in synchronous audio-visual videos. To better approximate
real-world applications, we expand the evaluation scope to asynchronous
audio-video scenarios. Furthermore, we introduce a novel identity consistency
metric to more comprehensively assess the identity consistency over time series
in generated facial videos. Experimental results on the HDTF demonstrate that
our method significantly surpasses existing techniques in video quality, lip
synchronization accuracy, face swapping fidelity, and identity consistency. Our
demo is available at http://swaptalk.cc.

摘要：結合換臉與唇部同步技術，提供客製化說話人臉生成，具備高成本效益的解決方案。然而，直接將既有模型串接在一起，往往會導致任務之間產生顯著的干擾，並降低影片清晰度，因為互動空間僅限於低階語意 RGB 空間。為了解決此問題，我們提出一個創新的統一架構 SwapTalk，在同一個潛在空間中完成換臉和唇部同步任務。參考人臉生成方面的近期研究，我們選擇 VQ 嵌入空間，因為它具有優異的可編輯性和保真度表現。為了增強架構對未見身分的泛化能力，我們在換臉模組訓練期間納入身分損失。此外，我們在唇部同步模組訓練期間，於潛在空間中引入專家鑑別器監督，以提升同步品質。在評估階段，先前的研究主要著重於同步音訊視訊中唇部動作的自我重建。為了更接近實際應用，我們將評估範圍擴展到非同步音訊視訊場景。此外，我們引入一種新的身分一致性指標，以更全面地評估生成人臉影片中身分隨時間序列的一致性。在 HDTF 上的實驗結果證明，我們的技術在影片品質、唇部同步準確性、換臉保真度和身分一致性方面，都顯著超越現有技術。我們的展示可在 http://swaptalk.cc 找到。

##### **G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning**
2405.05616v1 by Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, Yao Cheng

Commonsense question answering has demonstrated considerable potential across
various applications like assistants and social robots. Although fully
fine-tuned pre-trained Language Models(LM) have achieved remarkable performance
in commonsense reasoning, their tendency to excessively prioritize textual
information hampers the precise transfer of structural knowledge and undermines
interpretability. Some studies have explored combining LMs with Knowledge
Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural
Network(GNN)-based reasoning that lacks a profound interaction between
heterogeneous modalities. In this paper, we propose a novel Graph-based
Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,
aiming to maintain a balance between heterogeneous knowledge and enhance the
cross-modal interaction within the LM+GNNs model. In particular, an evidence
graph is constructed by integrating multiple knowledge sources, i.e.
ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.
Afterward, a structure-aware frozen PLM is employed to fully incorporate the
structured and textual information from the evidence graph, where the
generation of prompts is driven by graph entities and relations. Finally, a
heterogeneous message-passing reasoning module is used to facilitate deep
interaction of knowledge between the LM and graph-based networks. Empirical
validation, conducted through extensive experiments on three benchmark
datasets, demonstrates the notable performance of the proposed model. The
results reveal a significant advancement over the existing models, especially,
with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.

摘要：常識問答在各種應用程式中展現了相當大的潛力，例如助理和社交機器人。儘管經過微調的預訓練語言模型 (LM) 在常識推理中取得了顯著的表現，但它們過度優先考慮文本資訊的傾向會阻礙結構知識的精確傳輸，並損害可解釋性。一些研究探索了將 LM 與知識圖譜 (KG) 結合起來，通過粗略地融合這兩種模式來執行基於圖神經網路 (GNN) 的推理，而這種推理缺乏異質模式之間的深入互動。在本文中，我們提出了一個新的基於圖的結構感知提示學習模型，用於常識推理，稱為 G-SAP，旨在維持異質知識之間的平衡，並增強 LM+GNN 模型內的跨模式互動。特別是，通過整合多個知識來源（即 ConceptNet、Wikipedia 和劍橋詞典）來構建一個證據圖，以提升效能。隨後，採用結構感知凍結 PLM 來充分整合來自證據圖的結構化和文本資訊，其中提示的產生是由圖實體和關係驅動的。最後，使用異質訊息傳遞推理模組來促進 LM 和基於圖的網路之間的深度互動。透過在三個基準資料集上進行廣泛的實驗進行的經驗驗證，證明了所提出的模型的顯著效能。結果顯示出比現有模型有顯著的進步，特別是在 OpenbookQA 資料集上比 SoTA LM+GNN 模型提升了 6.12%。

##### **Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning**
2405.05615v1 by Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, Yunhe Wang

Current solutions for efficiently constructing large vision-language (VL)
models follow a two-step paradigm: projecting the output of pre-trained vision
encoders to the input space of pre-trained language models as visual prompts;
and then transferring the models to downstream VL tasks via end-to-end
parameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits
inefficiency since it significantly increases the input length of the language
models. In this paper, in contrast to integrating visual prompts into inputs,
we regard visual prompts as additional knowledge that facilitates language
models in addressing tasks associated with visual information. Motivated by the
finding that Feed-Forward Network (FFN) of language models acts as "key-value
memory", we introduce a novel approach termed memory-space visual prompting
(MemVP), wherein visual prompts are concatenated with the weights of FFN for
visual knowledge injection. Experimental results across various VL tasks and
language models reveal that MemVP significantly reduces the training time and
inference latency of the finetuned VL models and surpasses the performance of
previous PEFT methods. Code: https://github.com/JieShibo/MemVP

摘要：現今有效建構大型視覺語言 (VL) 模型的解決方案遵循兩階段範例：將預先訓練的視覺編碼器輸出投影到預先訓練語言模型的輸入空間，作為視覺提示；然後透過端對端參數有效微調 (PEFT) 將模型轉移到下游 VL 任務。然而，此範例仍展現出低效率，因為它顯著增加了語言模型的輸入長度。在本文中，與將視覺提示整合到輸入中相反，我們將視覺提示視為額外的知識，有助於語言模型處理與視覺資訊相關的任務。在 Feed-Forward 網路 (FFN) 的語言模型充當「鍵值記憶體」的發現激勵下，我們引入一種稱為記憶空間視覺提示 (MemVP) 的新方法，其中視覺提示與 FFN 的權重串接，以進行視覺知識注入。跨各種 VL 任務和語言模型的實驗結果顯示，MemVP 大幅減少微調 VL 模型的訓練時間和推論延遲，並超越先前的 PEFT 方法的效能。程式碼：https://github.com/JieShibo/MemVP

##### **Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM**
2405.05610v1 by Xikang Yang, Xuehai Tang, Songlin Hu, Jizhong Han

Large language models (LLMs) have achieved remarkable performance in various
natural language processing tasks, especially in dialogue systems. However, LLM
may also pose security and moral threats, especially in multi round
conversations where large models are more easily guided by contextual content,
resulting in harmful or biased responses. In this paper, we present a novel
method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack).
CoA is a semantic-driven contextual multi-turn attack method that adaptively
adjusts the attack policy through contextual feedback and semantic relevance
during multi-turn of dialogue with a large model, resulting in the model
producing unreasonable or harmful content. We evaluate CoA on different LLMs
and datasets, and show that it can effectively expose the vulnerabilities of
LLMs, and outperform existing attack methods. Our work provides a new
perspective and tool for attacking and defending LLMs, and contributes to the
security and ethical assessment of dialogue systems.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中取得了顯著的表現，特別是在對話系統中。然而，LLM 也可能構成安全和道德威脅，特別是在大型模型更容易受到上下文內容引導的多輪對話中，從而導致有害或有偏見的回應。在本文中，我們提出了一種新的方法來攻擊多輪對話中的 LLM，稱為 CoA（攻擊鏈）。CoA 是一種語義驅動的上下文多輪攻擊方法，它通過上下文反饋和語義相關性在與大型模型的多輪對話過程中自適應地調整攻擊策略，從而導致模型產生不合理或有害的內容。我們在不同的 LLM 和數據集上評估了 CoA，並表明它可以有效地揭露 LLM 的漏洞，並且優於現有的攻擊方法。我們的研究為攻擊和防禦 LLM 提供了一個新的視角和工具，並有助於對話系統的安全和道德評估。

##### **Can We Use Large Language Models to Fill Relevance Judgment Holes?**
2405.05600v1 by Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi

Incomplete relevance judgments limit the re-usability of test collections.
When new systems are compared against previous systems used to build the pool
of judged documents, they often do so at a disadvantage due to the ``holes'' in
test collection (i.e., pockets of un-assessed documents returned by the new
system). In this paper, we take initial steps towards extending existing test
collections by employing Large Language Models (LLM) to fill the holes by
leveraging and grounding the method using existing human judgments. We explore
this problem in the context of Conversational Search using TREC iKAT, where
information needs are highly dynamic and the responses (and, the results
retrieved) are much more varied (leaving bigger holes). While previous work has
shown that automatic judgments from LLMs result in highly correlated rankings,
we find substantially lower correlates when human plus automatic judgments are
used (regardless of LLM, one/two/few shot, or fine-tuned). We further find
that, depending on the LLM employed, new runs will be highly favored (or
penalized), and this effect is magnified proportionally to the size of the
holes. Instead, one should generate the LLM annotations on the whole document
pool to achieve more consistent rankings with human-generated labels. Future
work is required to prompt engineering and fine-tuning LLMs to reflect and
represent the human annotations, in order to ground and align the models, such
that they are more fit for purpose.

摘要：不完整的相關性判斷會限制測試集的再利用性。
當新的系統與用於建立已判斷文件池的先前系統進行比較時，它們通常會因測試集中出現的「漏洞」（即新系統返回的未評估文件區塊）而處於劣勢。在本文中，我們採取了初步步驟，利用大型語言模型 (LLM) 來填補漏洞，利用並根據現有人類判斷來擴充現有的測試集。我們在對話式搜尋的背景下探討這個問題，使用 TREC iKAT，其中資訊需求高度動態，而且回應（以及檢索的結果）變化更大（留下更大的漏洞）。雖然先前的研究表明，LLM 的自動判斷會產生高度相關的排名，但我們發現當使用人類加上自動判斷時，相關性會大幅降低（無論是 LLM、一/二/少次嘗試或微調）。我們進一步發現，根據所使用的 LLM，新的執行將受到高度青睞（或懲罰），而且這種效應會隨著漏洞大小成比例地放大。相反地，應該在整個文件池中產生 LLM 注解，以達成與人為產生的標籤更一致的排名。未來的研究需要提示工程和微調 LLM，以反映和表示人類的註解，以便根據並調整模型，使其更適合於目的。

##### **A Survey on Backbones for Deep Video Action Recognition**
2405.05584v1 by Zixuan Tang, Youjun Zhao, Yuhang Wen, Mengyuan Liu

Action recognition is a key technology in building interactive metaverses.
With the rapid development of deep learning, methods in action recognition have
also achieved great advancement. Researchers design and implement the backbones
referring to multiple standpoints, which leads to the diversity of methods and
encountering new challenges. This paper reviews several action recognition
methods based on deep neural networks. We introduce these methods in three
parts: 1) Two-Streams networks and their variants, which, specifically in this
paper, use RGB video frame and optical flow modality as input; 2) 3D
convolutional networks, which make efforts in taking advantage of RGB modality
directly while extracting different motion information is no longer necessary;
3) Transformer-based methods, which introduce the model from natural language
processing into computer vision and video understanding. We offer objective
sights in this review and hopefully provide a reference for future research.

摘要：動作辨識是建立互動式元宇宙的關鍵技術。
隨著深度學習的快速發展，動作辨識方法也取得了很大的進展。研究人員根據多個觀點設計和實作骨幹，這導致方法的多樣性並遇到新的挑戰。本文回顧了基於深度神經網路的幾種動作辨識方法。我們分為三部分介紹這些方法：1) 雙流網路及其變體，特別是在本文中，使用 RGB 影片幀和光流模式作為輸入；2) 3D 卷積網路，努力直接利用 RGB 模式，同時不再需要提取不同的動作資訊；3) 基於 Transformer 的方法，將自然語言處理中的模型引入電腦視覺和影片理解。我們在此回顧中提供客觀的見解，並希望為未來的研究提供參考。

##### **OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs**
2405.05583v1 by Yuxia Wang, Minghan Wang, Hasan Iqbal, Georgi Georgiev, Jiahui Geng, Preslav Nakov

The increased use of large language models (LLMs) across a variety of
real-world applications calls for mechanisms to verify the factual accuracy of
their outputs. Difficulties lie in assessing the factuality of free-form
responses in open domains. Also, different papers use disparate evaluation
benchmarks and measurements, which renders them hard to compare and hampers
future progress. To mitigate these issues, we propose OpenFactCheck, a unified
factuality evaluation framework for LLMs. OpenFactCheck consists of three
modules: (i) CUSTCHECKER allows users to easily customize an automatic
fact-checker and verify the factual correctness of documents and claims, (ii)
LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from
various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution
for gauging the reliability of automatic fact-checkers' verification results
using human-annotated datasets. OpenFactCheck is publicly released at
https://github.com/yuxiaw/OpenFactCheck.

摘要：隨著大型語言模型 (LLM) 在各種實際應用中的使用日益增加，需要有驗證其輸出事實準確性的機制。困難在於評估開放領域中自由形式回應的事實性。此外，不同的論文使用不同的評估基準和測量標準，這使得它們難以比較並阻礙未來的進展。為了減輕這些問題，我們提出了 OpenFactCheck，一個統一的 LLM 事實性評估框架。OpenFactCheck 包含三個模組：(i) CUSTCHECKER 使用戶可以輕鬆自訂自動事實查核器並驗證文件和說法的真實性，(ii) LLMEVAL，一個統一的評估框架從各個角度公平地評估 LLM 的事實性能力，以及 (iii) CHECKEREVAL 是一個可擴充的解決方案，用於使用人工標註的資料集來評估自動事實查核器驗證結果的可靠性。OpenFactCheck 已在 https://github.com/yuxiaw/OpenFactCheck 公開發布。

##### **One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations**
2405.05581v1 by Yoonjoo Lee, Kihoon Son, Tae Soo Kim, Jisu Kim, John Joon Young Chung, Eytan Adar, Juho Kim

As Large Language Models (LLMs) are nondeterministic, the same input can
generate different outputs, some of which may be incorrect or hallucinated. If
run again, the LLM may correct itself and produce the correct answer.
Unfortunately, most LLM-powered systems resort to single results which, correct
or not, users accept. Having the LLM produce multiple outputs may help identify
disagreements or alternatives. However, it is not obvious how the user will
interpret conflicts or inconsistencies. To this end, we investigate how users
perceive the AI model and comprehend the generated information when they
receive multiple, potentially inconsistent, outputs. Through a preliminary
study, we identified five types of output inconsistencies. Based on these
categories, we conducted a study (N=252) in which participants were given one
or more LLM-generated passages to an information-seeking question. We found
that inconsistency within multiple LLM-generated outputs lowered the
participants' perceived AI capacity, while also increasing their comprehension
of the given information. Specifically, we observed that this positive effect
of inconsistencies was most significant for participants who read two passages,
compared to those who read three. Based on these findings, we present design
implications that, instead of regarding LLM output inconsistencies as a
drawback, we can reveal the potential inconsistencies to transparently indicate
the limitations of these models and promote critical LLM usage.

摘要：由於大型語言模型 (LLM) 是非確定性的，相同的輸入可能會產生不同的輸出，其中一些可能是錯誤的或虛構的。如果再次執行，LLM 可能會自行修正並產生正確的答案。不幸的是，大多數由 LLM 驅動的系統都會訴諸單一結果，無論正確與否，使用者都接受。讓 LLM 產生多個輸出可能有助於找出分歧或替代方案。然而，使用者將如何詮釋衝突或不一致並不明顯。為此，我們研究使用者如何感知 AI 模型，以及在收到多個潛在不一致的輸出時如何理解產生的資訊。透過初步研究，我們找出五種類型的輸出不一致。根據這些類別，我們進行了一項研究 (N=252)，其中參與者收到一個或多個由 LLM 產生的段落，以回答尋求資訊的問題。我們發現，多個 LLM 生成的輸出中的不一致性降低了參與者感知的 AI 能力，同時也增加了他們對所提供資訊的理解。具體來說，我們觀察到，這種不一致性的正面影響對於閱讀兩個段落的參與者來說最為顯著，與閱讀三個段落的參與者相比。根據這些發現，我們提出的設計意涵是，不要將 LLM 輸出不一致視為缺點，而是可以揭露潛在的不一致性，以透明地指出這些模型的限制，並促進 LLM 的批判性使用。

##### **From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences**
2405.05572v1 by Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Manish Shrivastava, Ponnurangam Kumaraguru

Current computational approaches for analysing or generating code-mixed
sentences do not explicitly model "naturalness" or "acceptability" of
code-mixed sentences, but rely on training corpora to reflect distribution of
acceptable code-mixed sentences. Modelling human judgement for the
acceptability of code-mixed text can help in distinguishing natural code-mixed
text and enable quality-controlled generation of code-mixed text. To this end,
we construct Cline - a dataset containing human acceptability judgements for
English-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with
16,642 sentences, consisting of samples sourced from two sources: synthetically
generated code-mixed text and samples collected from online social media. Our
analysis establishes that popular code-mixing metrics such as CMI, Number of
Switch Points, Burstines, which are used to filter/curate/compare code-mixed
corpora have low correlation with human acceptability judgements, underlining
the necessity of our dataset. Experiments using Cline demonstrate that simple
Multilayer Perceptron (MLP) models trained solely on code-mixing metrics are
outperformed by fine-tuned pre-trained Multilingual Large Language Models
(MLLMs). Specifically, XLM-Roberta and Bernice outperform IndicBERT across
different configurations in challenging data settings. Comparison with
ChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger
data outperform ChatGPT, providing scope for improvement in code-mixed tasks.
Zero-shot transfer from English-Hindi to English-Telugu acceptability judgments
using our model checkpoints proves superior to random baselines, enabling
application to other code-mixed language pairs and providing further avenues of
research. We publicly release our human-annotated dataset, trained checkpoints,
code-mix corpus, and code for data generation and model training.

摘要：<paragraph>目前的計算方法用於分析或生成混合語言句子，並未明確模擬混合語言句子的「自然性」或「可接受性」，而是依賴訓練語料庫來反映可接受混合語言句子的分佈。模擬人類對混合語言文字可接受性的判斷，有助於區分自然的混合語言文字，並能有品質控管地生成混合語言文字。為此，我們建構了 Cline，一個包含人類對英語-印地語 (en-hi) 混合語言文字可接受性判斷的資料集。Cline 是同類資料集中規模最大的，包含 16,642 個句子，由兩個來源的樣本組成：合成產生的混合語言文字和從線上社群媒體收集的樣本。我們的分析確立了流行的混合語言指標，例如 CMI、切換點數、Burstines，這些指標用於過濾/整理/比較混合語言語料庫，與人類的可接受性判斷相關性很低，強調了我們資料集的必要性。使用 Cline 的實驗證明，僅根據混合語言指標訓練的簡單多層感知器 (MLP) 模型，其表現不如微調後的預訓練多語言大型語言模型 (MLLM)。具體來說，XLM-Roberta 和 Bernice 在具有挑戰性的資料設定中，在不同的配置中都優於 IndicBERT。與 ChatGPT 的零次學習和少次學習能力的比較顯示，經過較大資料微調的 MLLM 優於 ChatGPT，為混合語言任務的改進提供了空間。使用我們的模型檢查點，從英語-印地語到英語-泰盧固語的可接受性判斷的零次學習轉移，證明優於隨機基準，能夠應用於其他混合語言語言對，並提供進一步的研究途徑。我們公開發布我們的人工標註資料集、訓練檢查點、混合語言語料庫，以及用於資料生成和模型訓練的程式碼。</paragraph>

##### **Towards Robust Physical-world Backdoor Attacks on Lane Detection**
2405.05553v1 by Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu

Deep learning-based lane detection (LD) plays a critical role in autonomous
driving systems, such as adaptive cruise control. However, it is vulnerable to
backdoor attacks. Existing backdoor attack methods on LD exhibit limited
effectiveness in dynamic real-world scenarios, primarily because they fail to
consider dynamic scene factors, including changes in driving perspectives
(e.g., viewpoint transformations) and environmental conditions (e.g., weather
or lighting changes). To tackle this issue, this paper introduces BadLANE, a
dynamic scene adaptation backdoor attack for LD designed to withstand changes
in real-world dynamic scene factors. To address the challenges posed by
changing driving perspectives, we propose an amorphous trigger pattern composed
of shapeless pixels. This trigger design allows the backdoor to be activated by
various forms or shapes of mud spots or pollution on the road or lens, enabling
adaptation to changes in vehicle observation viewpoints during driving. To
mitigate the effects of environmental changes, we design a meta-learning
framework to train meta-generators tailored to different environmental
conditions. These generators produce meta-triggers that incorporate diverse
environmental information, such as weather or lighting conditions, as the
initialization of the trigger patterns for backdoor implantation, thus enabling
adaptation to dynamic environments. Extensive experiments on various commonly
used LD models in both digital and physical domains validate the effectiveness
of our attacks, outperforming other baselines significantly (+25.15\% on
average in Attack Success Rate). Our codes will be available upon paper
publication.

摘要：<paragraph>基於深度學習的車道偵測 (LD) 在自駕車系統中扮演關鍵角色，例如自適應巡航控制。然而，它很容易受到後門攻擊。現有的 LD 後門攻擊方法在動態的真實世界場景中效果有限，主要原因是它們未能考慮動態場景因素，包括駕駛視角的變化（例如，視點變換）和環境條件（例如，天氣或光線變化）。為了解決這個問題，本文介紹了 BadLANE，一種動態場景適應後門攻擊，用於 LD，旨在承受真實世界動態場景因素的變化。為了應對駕駛視角變化的挑戰，我們提出了一種由無形像素組成的無定形觸發模式。這種觸發設計允許後門被各種形式或形狀的道路或鏡頭上的泥點或污染物激活，從而適應駕駛過程中車輛觀察視點的變化。為了減輕環境變化的影響，我們設計了一個元學習框架來訓練針對不同環境條件量身定制的元生成器。這些生成器產生元觸發器，其中包含各種環境資訊，例如天氣或光線條件，作為後門植入觸發模式的初始化，從而適應動態環境。在數位和物理領域中對各種常用的 LD 模型進行廣泛的實驗驗證了我們攻擊的有效性，顯著優於其他基準（平均攻擊成功率 +25.15%）。我們的程式碼將在論文發表後提供。</paragraph>

##### **Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training**
2405.05523v1 by Sheng Yan, Xin Du, Zongying Li, Yi Wang, Hongcang Jin, Mengyuan Liu

Temporal grounding is crucial in multimodal learning, but it poses challenges
when applied to animal behavior data due to the sparsity and uniform
distribution of moments. To address these challenges, we propose a novel
Positional Recovery Training framework (Port), which prompts the model with the
start and end times of specific animal behaviors during training. Specifically,
Port enhances the baseline model with a Recovering part to predict flipped
label sequences and align distributions with a Dual-alignment method. This
allows the model to focus on specific temporal regions prompted by ground-truth
information. Extensive experiments on the Animal Kingdom dataset demonstrate
the effectiveness of Port, achieving an IoU@0.3 of 38.52. It emerges as one of
the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.

摘要：時序基礎在多模態學習中至關重要，但由於動物行為資料的稀疏性和均勻分佈，在應用時會帶來挑戰。為了應對這些挑戰，我們提出了一個新穎的位置恢復訓練框架 (Port)，它在訓練期間提示模型具體動物行為的開始和結束時間。具體來說，Port 使用恢復部分增強基準模型，以預測翻轉的標籤序列，並使用雙重對齊方法對齊分佈。這允許模型專注於由基本事實資訊提示的特定時序區域。在動物王國資料集上進行的廣泛實驗證明了 Port 的有效性，達到了 38.52 的 IoU@0.3。它成為 ICME 2024 Grand Challenges 中 MMVRAC 子軌道的頂尖表現者之一。

##### **Characteristic Learning for Provable One Step Generation**
2405.05512v1 by Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang

We propose the characteristic generator, a novel one-step generative model
that combines the efficiency of sampling in Generative Adversarial Networks
(GANs) with the stable performance of flow-based models. Our model is driven by
characteristics, along which the probability density transport can be described
by ordinary differential equations (ODEs). Specifically, We estimate the
velocity field through nonparametric regression and utilize Euler method to
solve the probability flow ODE, generating a series of discrete approximations
to the characteristics. We then use a deep neural network to fit these
characteristics, ensuring a one-step mapping that effectively pushes the prior
distribution towards the target distribution. In the theoretical aspect, we
analyze the errors in velocity matching, Euler discretization, and
characteristic fitting to establish a non-asymptotic convergence rate for the
characteristic generator in 2-Wasserstein distance. To the best of our
knowledge, this is the first thorough analysis for simulation-free one step
generative models. Additionally, our analysis refines the error analysis of
flow-based generative models in prior works. We apply our method on both
synthetic and real datasets, and the results demonstrate that the
characteristic generator achieves high generation quality with just a single
evaluation of neural network.

摘要：<paragraph>我們提出特徵生成器，一種新穎的一步生成模型，它結合了生成對抗網路 (GAN) 中取樣的效率和基於流模型的穩定性能。我們的模型由特徵驅動，沿著特徵，機率密度傳輸可以用常微分方程式 (ODE) 來描述。具體來說，我們透過非參數迴歸估計速度場，並利用歐拉方法來求解機率流 ODE，生成一系列特徵的離散近似值。然後，我們使用深度神經網路來擬合這些特徵，確保一步對應，有效地將先驗分佈推向目標分佈。在理論方面，我們分析速度匹配、歐拉離散化和特徵擬合中的錯誤，以建立特徵生成器在 2-Wasserstein 距離中的非漸近收斂率。據我們所知，這是對無模擬一步生成模型的首次徹底分析。此外，我們的分析改進了先前工作中基於流的生成模型的錯誤分析。我們在合成和真實資料集上應用我們的模型，結果表明特徵生成器僅透過一次神經網路評估就能實現高生成品質。</paragraph>

##### **Redefining Information Retrieval of Structured Database via Large Language Models**
2405.05508v1 by Mingzhu Wang, Yuzhe Zhang, Qihang Zhao, Juanyi Yang, Hong Zhang

Retrieval augmentation is critical when Language Models (LMs) exploit
non-parametric knowledge related to the query through external knowledge bases
before reasoning. The retrieved information is incorporated into LMs as context
alongside the query, enhancing the reliability of responses towards factual
questions. Prior researches in retrieval augmentation typically follow a
retriever-generator paradigm. In this context, traditional retrievers encounter
challenges in precisely and seamlessly extracting query-relevant information
from knowledge bases. To address this issue, this paper introduces a novel
retrieval augmentation framework called ChatLR that primarily employs the
powerful semantic understanding ability of Large Language Models (LLMs) as
retrievers to achieve precise and concise information retrieval. Additionally,
we construct an LLM-based search and question answering system tailored for the
financial domain by fine-tuning LLM on two tasks including Text2API and API-ID
recognition. Experimental results demonstrate the effectiveness of ChatLR in
addressing user queries, achieving an overall information retrieval accuracy
exceeding 98.8\%.

摘要：检索增强对于语言模型 (LM) 在推理之前通过外部知识库利用与查询相关的非参数知识至关重要。检索到的信息与查询一起作为上下文合并到 LM 中，从而增强了对事实问题的响应的可靠性。检索增强中的先前研究通常遵循检索器生成器范例。在这种情况下，传统检索器在从知识库中精确无缝地提取与查询相关的信息时遇到挑战。为了解决这个问题，本文介绍了一种称为 ChatLR 的新型检索增强框架，该框架主要利用大型语言模型 (LLM) 作为检索器强大的语义理解能力来实现精确简洁的信息检索。此外，我们构建了一个基于 LLM 的搜索和问答系统，该系统通过在 Text2API 和 API-ID 识别两个任务上微调 LLM，专门用于金融领域。实验结果证明了 ChatLR 在解决用户查询方面的有效性，实现了超过 98.8% 的整体信息检索准确率。

##### **Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias**
2405.05506v1 by Shan Chen, Jack Gallifant, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony, Leo Anthony Celi, William G. La Cava, Danielle S. Bitterman

Large language models (LLMs) are increasingly essential in processing natural
languages, yet their application is frequently compromised by biases and
inaccuracies originating in their training data. In this study, we introduce
Cross-Care, the first benchmark framework dedicated to assessing biases and
real world knowledge in LLMs, specifically focusing on the representation of
disease prevalence across diverse demographic groups. We systematically
evaluate how demographic biases embedded in pre-training corpora like $ThePile$
influence the outputs of LLMs. We expose and quantify discrepancies by
juxtaposing these biases against actual disease prevalences in various U.S.
demographic groups. Our results highlight substantial misalignment between LLM
representation of disease prevalence and real disease prevalence rates across
demographic subgroups, indicating a pronounced risk of bias propagation and a
lack of real-world grounding for medical applications of LLMs. Furthermore, we
observe that various alignment methods minimally resolve inconsistencies in the
models' representation of disease prevalence across different languages. For
further exploration and analysis, we make all data and a data visualization
tool available at: www.crosscare.net.

摘要：大型語言模型（LLM）在處理自然語言方面日益重要，但它們的應用經常受到訓練資料中偏差和不準確性的影響。在本研究中，我們引入了 Cross-Care，這是第一個專門用於評估 LLM 中偏差和真實世界知識的基準架構，特別關注不同人口群體中疾病流行率的表示。我們系統地評估了嵌入在預訓練語料庫（如 $ThePile$）中的人口統計偏差如何影響 LLM 的輸出。我們通過將這些偏差與美國不同人口群體中的實際疾病流行率並列來揭示和量化差異。我們的結果突出了 LLM 對疾病流行率的表示與不同人口亞組的實際疾病流行率之間存在顯著的不一致，這表明存在明顯的偏差傳播風險以及 LLM 醫療應用缺乏現實依據。此外，我們觀察到，各種對齊方法在不同語言中對模型對疾病流行率的表示的不一致性解決得最少。為了進一步探索和分析，我們在 www.crosscare.net 上提供了所有數據和數據可視化工具。

##### **Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting**
2405.05499v1 by Feifei Li, Suhan Guo, Feng Han, Jian Zhao, Furao Shen

Accurate forecasting of long-term time series has important applications for
decision making and planning. However, it remains challenging to capture the
long-term dependencies in time series data. To better extract long-term
dependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a
method that utilizes a shallow dilated convolution architecture to capture the
period and trend characteristics of long time series. We design different
convolution blocks with exponentially growing dilations and varying kernel
sizes to sample time series data at different scales. Furthermore, we utilize
traditional autoregressive model to capture the linear relationships within the
data. To validate the effectiveness of the proposed approach, we conduct
experiments on eight challenging long-term time series forecasting benchmark
datasets. The experimental results show that our approach outperforms the prior
state-of-the-art approaches and shows significant inference speed improvements
compared to several strong baseline methods.

摘要：準確預測長期時間序列對於決策制定和規劃具有重要的應用。然而，捕捉時間序列資料中的長期依賴關係仍然具有挑戰性。為了更好地提取長期依賴關係，我們提出多尺度膨脹卷積網路 (MSDCN)，一種利用淺層膨脹卷積架構來捕捉長期時間序列的週期和趨勢特徵的方法。我們設計了具有指數級增長的膨脹和不同核心的不同卷積塊，以不同尺度採樣時間序列資料。此外，我們利用傳統的自迴歸模型來捕捉資料中的線性關係。為了驗證所提出方法的有效性，我們對八個具有挑戰性的長期時間序列預測基準資料集進行了實驗。實驗結果表明，與之前的最先進方法相比，我們的模型表現更佳，並且與幾種強大的基準方法相比，推論速度有了顯著的提升。

##### **Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis**
2405.05496v1 by Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He

Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment
analysis, which aims to extract the aspects and predict their sentiments. Most
existing studies focus on improving the performance of the target domain by
fine-tuning domain-specific models (trained on source domains) based on the
target domain dataset. Few works propose continual learning tasks for ABSA,
which aim to learn the target domain's ability while maintaining the history
domains' abilities. In this paper, we propose a Large Language Model-based
Continual Learning (\texttt{LLM-CL}) model for ABSA. First, we design a domain
knowledge decoupling module to learn a domain-invariant adapter and separate
domain-variant adapters dependently with an orthogonal constraint. Then, we
introduce a domain knowledge warmup strategy to align the representation
between domain-invariant and domain-variant knowledge. In the test phase, we
index the corresponding domain-variant knowledge via domain positioning to not
require each sample's domain ID. Extensive experiments over 19 datasets
indicate that our \texttt{LLM-CL} model obtains new state-of-the-art
performance.

摘要：基於面向方面的觀點分析 (ABSA) 是觀點分析中一個重要的子任務，旨在萃取面向並預測其觀點。大多數現有研究專注於透過根據目標領域資料集微調特定領域模型（在來源領域中訓練）來提升目標領域的效能。很少有研究提出 ABSA 的持續學習任務，其目標是在維持歷史領域能力的同時學習目標領域的能力。在本文中，我們提出一個基於大型語言模型的持續學習 (LLM-CL) 模型，用於 ABSA。首先，我們設計一個領域知識解耦模組，以學習一個不變領域的適配器，並透過正交約束分別分離出變異領域的適配器。然後，我們引入一個領域知識熱身策略，以調整不變領域和變異領域知識之間的表徵。在測試階段，我們透過領域定位索引對應的變異領域知識，以不需要每個樣本的領域 ID。在 19 個資料集上的大量實驗表明，我們的 LLM-CL 模型獲得了新的最先進效能。

##### **Parameter-Efficient Fine-Tuning With Adapters**
2405.05493v1 by Keyu Chen, Yuan Pang, Zi Yang

In the arena of language model fine-tuning, the traditional approaches, such
as Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT),
although effective, but computational intensive. This research introduces a
novel adaptation method utilizing the UniPELT framework as a base and added a
PromptTuning Layer, which significantly reduces the number of trainable
parameters while maintaining competitive performance across various benchmarks.
Our method employs adapters, which enable efficient transfer of pretrained
models to new tasks with minimal retraining of the base model parameters. We
evaluate our approach using three diverse datasets: the GLUE benchmark, a
domain-specific dataset comprising four distinct areas, and the Stanford
Question Answering Dataset 1.1 (SQuAD). Our results demonstrate that our
customized adapter-based method achieves performance comparable to full model
fine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or
equivalent amount of parameters. This parameter efficiency not only alleviates
the computational burden but also expedites the adaptation process. The study
underlines the potential of adapters in achieving high performance with
significantly reduced resource consumption, suggesting a promising direction
for future research in parameter-efficient fine-tuning.

摘要：在語言模型微調領域中，傳統方法，例如領域自適應預訓練 (DAPT) 和任務自適應預訓練 (TAPT)，雖然有效，但計算密集。本研究引入了一種新的自適應方法，以 UniPELT 框架為基礎，並添加了 PromptTuning 層，在保持各種基準測試競爭力的同時，顯著減少了可訓練參數的數量。我們的模型採用適配器，它能以最少的基礎模型參數重新訓練，將預訓練模型有效地轉移到新任務。我們使用三個不同的數據集評估我們的模型：GLUE 基準測試、包含四個不同領域的特定領域數據集，以及史丹佛問答數據集 1.1 (SQuAD)。我們的結果表明，我們自訂的基於適配器的模型在執行效能上可與完整的模型微調、DAPT+TAPT 和 UniPELT 策略相媲美，同時所需參數更少或相等。這種參數效率不僅減輕了計算負擔，也加快了自適應過程。本研究強調了適配器在顯著減少資源消耗的情況下實現高執行效能的潛力，為未來在參數有效微調的研究中提出了有希望的方向。

##### **A logifold structure on measure space**
2405.05492v1 by Inkee Jung, Siu-Cheong Lau

In this paper,we develop a local-to-global and measure-theoretical approach
to understand datasets. The idea is to take network models with restricted
domains as local charts of datasets. We develop the mathematical foundations
for these structures, and show in experiments how it can be used to find fuzzy
domains and to improve accuracy in data classification problems.

摘要：在本文中，我們發展了一種局部到全局且基於測量的理論方法來理解資料集。這個想法是將具有受限網域的網路模型視為資料集的局部圖表。我們發展了這些結構的數學基礎，並在實驗中展示了如何使用它來尋找模糊網域，並提升資料分類問題的準確度。

##### **Using Machine Translation to Augment Multilingual Classification**
2405.05478v1 by Adam King

An all-too-present bottleneck for text classification model development is
the need to annotate training data and this need is multiplied for multilingual
classifiers. Fortunately, contemporary machine translation models are both
easily accessible and have dependable translation quality, making it possible
to translate labeled training data from one language into another. Here, we
explore the effects of using machine translation to fine-tune a multilingual
model for a classification task across multiple languages. We also investigate
the benefits of using a novel technique, originally proposed in the field of
image captioning, to account for potential negative effects of tuning models on
translated data. We show that translated data are of sufficient quality to tune
multilingual classifiers and that this novel loss technique is able to offer
some improvement over models tuned without it.

摘要：文本分類模型開發中一個經常出現的瓶頸是
標註訓練資料的需求，而這個需求對於多語言
分類器來說會成倍增加。幸運的是，現代機器翻譯模型既容易取得，
翻譯品質又可靠，這使得將標籤訓練資料從一種語言翻譯成另一種語言成為可能。在這裡，我們
探討使用機器翻譯微調多語言模型以進行跨多種語言的分類任務的效果。我們也研究
使用一種新技術的好處，該技術最初在影像字幕領域中提出，用於考量在翻譯資料上微調模型的潛在負面影響。我們證明翻譯資料品質足夠好，可以微調
多語言分類器，而且這個新損失技術能夠提供比未經微調的模型更好的效果。

##### **AFEN: Respiratory Disease Classification using Ensemble Learning**
2405.05467v1 by Rahul Nadkarni, Emmanouil Nikolakakis, Razvan Marinescu

We present AFEN (Audio Feature Ensemble Learning), a model that leverages
Convolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion
to perform state-of-the-art audio classification for a range of respiratory
diseases. We use a meticulously selected mix of audio features which provide
the salient attributes of the data and allow for accurate classification. The
extracted features are then used as an input to two separate model classifiers
1) a multi-feature CNN classifier and 2) an XGBoost Classifier. The outputs of
the two models are then fused with the use of soft voting. Thus, by exploiting
ensemble learning, we achieve increased robustness and accuracy. We evaluate
the performance of the model on a database of 920 respiratory sounds, which
undergoes data augmentation techniques to increase the diversity of the data
and generalizability of the model. We empirically verify that AFEN sets a new
state-of-the-art using Precision and Recall as metrics, while decreasing
training time by 60%.

摘要：我們提出了 AFEN（音訊特徵集成學習），一種利用卷積神經網路 (CNN) 和 XGBoost 以集成學習方式進行運作的模型，以執行各種呼吸道疾病的最新音訊分類。我們使用精心挑選的音訊特徵組合，提供資料的顯著屬性並允許進行準確分類。提取的特徵接著用作兩個獨立模型分類器的輸入，1) 多特徵 CNN 分類器，以及 2) XGBoost 分類器。接著使用軟投票融合兩個模型的輸出。因此，透過利用集成學習，我們能提升穩健性和準確度。我們在一個包含 920 個呼吸音的資料庫中評估模型的效能，該資料庫經過資料擴充技術處理，以增加資料的多樣性和模型的可概化性。我們實證驗證 AFEN 使用準確度和召回率作為指標，設定了新的最新技術，同時將訓練時間減少了 60%。

##### **Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals**
2405.05466v1 by Joshua Clymer, Caden Juang, Severin Field

Like a criminal under investigation, Large Language Models (LLMs) might
pretend to be aligned while evaluated and misbehave when they have a good
opportunity. Can current interpretability methods catch these 'alignment
fakers?' To answer this question, we introduce a benchmark that consists of 324
pairs of LLMs fine-tuned to select actions in role-play scenarios. One model in
each pair is consistently benign (aligned). The other model misbehaves in
scenarios where it is unlikely to be caught (alignment faking). The task is to
identify the alignment faking model using only inputs where the two models
behave identically. We test five detection strategies, one of which identifies
98% of alignment-fakers.

摘要：大型語言模型 (LLM) 可能就像正在接受調查的罪犯一樣，在接受評估時假裝保持一致，但在有機會時卻會做出不當行為。目前的詮釋方法可以找出這些「假裝一致」的模型嗎？為了回答這個問題，我們引進了一個基準，其中包含 324 對經過微調以在角色扮演情境中選擇動作的 LLM。每對中的其中一個模型始終是良性的（一致的）。另一個模型在不太可能被發現的情境中會做出不當行為（假裝一致）。任務是僅使用兩個模型行為相同時的輸入來找出假裝一致的模型。我們測試了五種偵測策略，其中一種策略識別出 98% 的假裝一致者。

##### **GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields**
2405.05446v1 by Yuanhao Gong

The 3D Gaussian splatting methods are getting popular. However, they work
directly on the signal, leading to a dense representation of the signal. Even
with some techniques such as pruning or distillation, the results are still
dense. In this paper, we propose to model the gradient of the original signal.
The gradients are much sparser than the original signal. Therefore, the
gradients use much less Gaussian splats, leading to the more efficient storage
and thus higher computational performance during both training and rendering.
Thanks to the sparsity, during the view synthesis, only a small mount of pixels
are needed, leading to much higher computational performance ($100\sim
1000\times$ faster). And the 2D image can be recovered from the gradients via
solving a Poisson equation with linear computation complexity. Several
experiments are performed to confirm the sparseness of the gradients and the
computation performance of the proposed method. The method can be applied
various applications, such as human body modeling and indoor environment
modeling.

摘要：3D 高斯散射方法正变得流行。然而，它们直接在信号上工作，从而导致信号的密集表示。即使使用一些技术（如剪枝或蒸馏），结果仍然很密集。在本文中，我们建议对原始信号的梯度进行建模。梯度比原始信号稀疏得多。因此，梯度使用更少的 Gaussian splat，从而在训练和渲染期间导致更有效的存储，从而提高计算性能。由于稀疏性，在视图合成期间，只需要少量像素，从而导致更高的计算性能（快 $100\sim 1000\times$）。并且可以通过求解具有线性计算复杂度的泊松方程从梯度中恢复 2D 图像。执行了几项实验来确认梯度的稀疏性和所提出方法的计算性能。该方法可以应用于各种应用，例如人体建模和室内环境建模。

##### **Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large**
2405.05444v1 by Jussi S. Jauhiainen, Agustín Garagorry Guerra

Evaluating open-ended written examination responses from students is an
essential yet time-intensive task for educators, requiring a high degree of
effort, consistency, and precision. Recent developments in Large Language
Models (LLMs) present a promising opportunity to balance the need for thorough
evaluation with efficient use of educators' time. In our study, we explore the
effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in
assessing university students' open-ended answers to questions made about
reference material they have studied. Each model was instructed to evaluate 54
answers repeatedly under two conditions: 10 times (10-shot) with a temperature
setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of
1,080 evaluations per model and 4,320 evaluations across all models. The RAG
(Retrieval Augmented Generation) framework was used as the framework to make
the LLMs to process the evaluation of the answers. As of spring 2024, our
analysis revealed notable variations in consistency and the grading outcomes
provided by studied LLMs. There is a need to comprehend strengths and
weaknesses of LLMs in educational settings for evaluating open-ended written
responses. Further comparative research is essential to determine the accuracy
and cost-effectiveness of using LLMs for educational assessments.

摘要：評量學生開放式書面考試的回答，對教育工作者來說是一項重要但耗時的任務，需要高度的努力、一致性和準確性。大型語言模型 (LLM) 的最新發展，提供了一個有希望的機會，可以在徹底評量與有效利用教育工作者時間之間取得平衡。在我們的研究中，我們探討了 LLM ChatGPT-3.5、ChatGPT-4、Claude-3 和 Mistral-Large 在評量大學生針對其已研讀的參考資料所提出的開放式問題回答的有效性。每個模型都被指示在兩種條件下重複評量 54 個答案：10 次（10 次嘗試）溫度設定為 0.0，以及 10 次溫度設定為 0.5，預期每個模型共評量 1,080 次，所有模型共評量 4,320 次。RAG（檢索擴充生成）架構被用作讓 LLM 處理答案評量的架構。截至 2024 年春季，我們的分析揭示了所研究的 LLM 在一致性和評分結果方面有顯著的差異。有必要了解 LLM 在教育環境中評量開放式書面回答的優點和缺點。進一步的比較研究對於確定使用 LLM 進行教育評量的準確性和成本效益至關重要。

##### **How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation**
2405.05439v1 by Joseph A. Vincent, Haruki Nishimura, Masha Itkina, Paarth Shah, Mac Schwager, Thomas Kollar

With the rise of stochastic generative models in robot policy learning,
end-to-end visuomotor policies are increasingly successful at solving complex
tasks by learning from human demonstrations. Nevertheless, since real-world
evaluation costs afford users only a small number of policy rollouts, it
remains a challenge to accurately gauge the performance of such policies. This
is exacerbated by distribution shifts causing unpredictable changes in
performance during deployment. To rigorously evaluate behavior cloning
policies, we present a framework that provides a tight lower-bound on robot
performance in an arbitrary environment, using a minimal number of experimental
policy rollouts. Notably, by applying the standard stochastic ordering to robot
performance distributions, we provide a worst-case bound on the entire
distribution of performance (via bounds on the cumulative distribution
function) for a given task. We build upon established statistical results to
ensure that the bounds hold with a user-specified confidence level and
tightness, and are constructed from as few policy rollouts as possible. In
experiments we evaluate policies for visuomotor manipulation in both simulation
and hardware. Specifically, we (i) empirically validate the guarantees of the
bounds in simulated manipulation settings, (ii) find the degree to which a
learned policy deployed on hardware generalizes to new real-world environments,
and (iii) rigorously compare two policies tested in out-of-distribution
settings. Our experimental data, code, and implementation of confidence bounds
are open-source.

摘要：<paragraph>隨著機器人策略學習中隨機生成模型的興起，端到端的視覺運動策略在學習人類示範後，在解決複雜任務方面越來越成功。儘管如此，由於現實世界的評估成本只允許使用者進行少量的策略推出，因此準確評估此類策略的效能仍然是一項挑戰。分佈轉移導致部署期間效能發生不可預測的變化，這加劇了問題。為了嚴格評估行為複製策略，我們提出了一個框架，該框架使用最少的實驗策略推出，為機器人在任意環境中的效能提供了嚴格的下界。值得注意的是，通過將標準隨機排序應用於機器人效能分佈，我們為給定任務提供了效能整個分佈的最壞情況界限（通過累積分佈函數的界限）。我們建立在既定的統計結果之上，以確保界限在使用者指定的信心水準和嚴謹性下成立，並且由儘可能少的策略推出構成。在實驗中，我們評估了視覺運動操作策略在模擬和硬體中的策略。具體來說，我們（i）在模擬操作設置中憑經驗驗證界限的保證，（ii）找出部署在硬體上的學習策略在多大程度上可以推廣到新的現實世界環境，以及（iii）嚴格比較在分佈外設置中測試的兩個策略。我們的實驗資料、程式碼和信心界限的實作是開放原始碼的。</paragraph>

##### **How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression**
2405.05429v1 by Lucas Kook, Chris Kolb, Philipp Schiele, Daniel Dold, Marcel Arpogaus, Cornelius Fritz, Philipp F. Baumann, Philipp Kopper, Tobias Pielok, Emilio Dorigatti, David Rügamer

Neural network representations of simple models, such as linear regression,
are being studied increasingly to better understand the underlying principles
of deep learning algorithms. However, neural representations of distributional
regression models, such as the Cox model, have received little attention so
far. We close this gap by proposing a framework for distributional regression
using inverse flow transformations (DRIFT), which includes neural
representations of the aforementioned models. We empirically demonstrate that
the neural representations of models in DRIFT can serve as a substitute for
their classical statistical counterparts in several applications involving
continuous, ordered, time-series, and survival outcomes. We confirm that models
in DRIFT empirically match the performance of several statistical methods in
terms of estimation of partial effects, prediction, and aleatoric uncertainty
quantification. DRIFT covers both interpretable statistical models and flexible
neural networks opening up new avenues in both statistical modeling and deep
learning.

摘要：神經網路表示簡單模型（例如線性回歸）的原理，正逐漸受到重視，以進一步了解深度學習演算法的基本原理。然而，目前對於分配式回歸模型（例如 Cox 模型）的神經表示，關注度還很低。我們透過提出一個使用逆流轉換（DRIFT）的分配式回歸架構來縮小這個差距，其中包含上述模型的神經表示。我們實證證明，DRIFT 中模型的神經表示，可以在多個涉及連續、有序、時間序列和存活結果的應用中，取代其傳統的統計對應項。我們確認 DRIFT 中的模型在部分效應估計、預測和不確定性量化方面，實證上符合多種統計方法的效能。DRIFT 涵蓋了可解釋的統計模型和靈活的神經網路，為統計建模和深度學習開闢了新的途徑。

##### **Mitigating Exaggerated Safety in Large Language Models**
2405.05418v1 by Ruchi Bhalani, Ruchira Ray

As the popularity of Large Language Models (LLMs) grow, combining model
safety with utility becomes increasingly important. The challenge is making
sure that LLMs can recognize and decline dangerous prompts without sacrificing
their ability to be helpful. The problem of "exaggerated safety" demonstrates
how difficult this can be. To reduce excessive safety behaviours -- which was
discovered to be 26.1% of safe prompts being misclassified as dangerous and
refused -- we use a combination of XSTest dataset prompts as well as
interactive, contextual, and few-shot prompting to examine the decision bounds
of LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot
prompting works best for Llama2, interactive prompting works best Gemma, and
contextual prompting works best for Command R+ and Phi-3. Using a combination
of these prompting strategies, we are able to mitigate exaggerated safety
behaviors by an overall 92.9% across all LLMs. Our work presents a multiple
prompting strategies to jailbreak LLMs' decision-making processes, allowing
them to navigate the tight line between refusing unsafe prompts and remaining
helpful.

摘要：隨著大型語言模型 (LLM) 的普及，模型安全與實用性結合變得越來越重要。挑戰在於確保 LLM 能識別並拒絕危險的提示，同時不犧牲它們的幫助能力。「誇大的安全性」問題說明了這有多麼困難。為了減少過度的安全行為（發現有 26.1% 的安全提示被錯誤分類為危險並被拒絕），我們結合使用 XSTest 資料集提示以及互動式、情境式和少次提示來檢視 LLM（例如 Llama2、Gemma Command R+ 和 Phi-3）的決策界限。我們發現，少次提示最適合 Llama2，互動式提示最適合 Gemma，而情境式提示最適合 Command R+ 和 Phi-3。透過結合這些提示策略，我們能夠在所有 LLM 中將誇大的安全性行為降低 92.9%。我們的研究提出了多種提示策略，以破解 LLM 的決策流程，讓它們能夠在拒絕不安全提示和保持有幫助之間取得平衡。

##### **Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models**
2405.05417v1 by Sander Land, Max Bartolo

The disconnect between tokenizer creation and model training in language
models has been known to allow for certain inputs, such as the infamous
SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch
tokens' that are present in the tokenizer vocabulary, but are nearly or fully
absent in training, have been observed across a variety of different models, a
consistent way of identifying them has been missing. We present a comprehensive
analysis of Large Language Model (LLM) tokenizers, specifically targeting this
issue of detecting untrained and under-trained tokens. Through a combination of
tokenizer analysis, model weight-based indicators, and prompting techniques, we
develop effective methods for automatically detecting these problematic tokens.
Our findings demonstrate the prevalence of such tokens across various models
and provide insights into improving the efficiency and safety of language
models.

摘要：語言模型中斷詞器建立和模型訓練之間的關聯已知允許某些輸入，例如臭名昭著的 SolidGoldMagikarp 斷詞器，誘發不良行為。儘管斷詞器詞彙中存在這種「故障斷詞器」，但在訓練中幾乎或完全不存在，已在各種不同模型中觀察到，但一直缺乏識別它們的一致方法。我們提出大型語言模型 (LLM) 斷詞器的全面分析，特別針對檢測未訓練和訓練不足的斷詞器這個問題。透過結合斷詞器分析、基於模型權重的指標和提示技術，我們開發出自動偵測這些有問題斷詞器的有效方法。我們的發現證明了這些斷詞器在各種模型中的普遍性，並提供了見解來提升語言模型的效率和安全性。

##### **Interpretability Needs a New Paradigm**
2405.05386v1 by Andreas Madsen, Himabindu Lakkaraju, Siva Reddy, Sarath Chandar

Interpretability is the study of explaining models in understandable terms to
humans. At present, interpretability is divided into two paradigms: the
intrinsic paradigm, which believes that only models designed to be explained
can be explained, and the post-hoc paradigm, which believes that black-box
models can be explained. At the core of this debate is how each paradigm
ensures its explanations are faithful, i.e., true to the model's behavior. This
is important, as false but convincing explanations lead to unsupported
confidence in artificial intelligence (AI), which can be dangerous. This
paper's position is that we should think about new paradigms while staying
vigilant regarding faithfulness. First, by examining the history of paradigms
in science, we see that paradigms are constantly evolving. Then, by examining
the current paradigms, we can understand their underlying beliefs, the value
they bring, and their limitations. Finally, this paper presents 3 emerging
paradigms for interpretability. The first paradigm designs models such that
faithfulness can be easily measured. Another optimizes models such that
explanations become faithful. The last paradigm proposes to develop models that
produce both a prediction and an explanation.

摘要：可解釋性是研究如何以人類可以理解的方式來解釋模型。目前，可解釋性分成兩種典範：內在典範，認為只有設計成可以解釋的模型才能被解釋，以及事後典範，認為黑盒模型也可以被解釋。在這個辯論的核心是每個典範如何確保其解釋是忠實的，也就是說，忠於模型的行為。這很重要，因為虛假但令人信服的解釋會導致對人工智慧 (AI) 的信心不足，這可能是危險的。本文的立場是，我們應該思考新的典範，同時對忠實度保持警惕。首先，通過檢視科學中的典範歷史，我們會看到典範不斷演進。然後，通過檢視當前的典範，我們可以了解其背後的信念、它們帶來的價值和它們的限制。最後，本文提出了 3 個可解釋性的新興典範。第一個典範設計模型，以便可以輕鬆衡量忠實度。另一個優化模型，以便解釋變得忠實。最後一個典範提出開發同時產生預測和解釋的模型。

##### **"They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations**
2405.05378v1 by Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra

Large language models (LLMs) have emerged as an integral part of modern
societies, powering user-facing applications such as personal assistants and
enterprise applications like recruitment tools. Despite their utility, research
indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms
predominantly focus on Western concepts like race and gender, often overlooking
cultural concepts from other parts of the world. Additionally, these studies
typically investigate "harm" as a singular dimension, ignoring the various and
subtle forms in which harms manifest. To address this gap, we introduce the
Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in
social science literature. We utilize evaluation models aligned with human
assessments to examine the presence of covert harms in LLM-generated
conversations, particularly in the context of recruitment. Our experiments
reveal that seven out of the eight LLMs included in this study generated
conversations riddled with CHAST, characterized by malign views expressed in
seemingly neutral language unlikely to be detected by existing methods.
Notably, these LLMs manifested more extreme views and opinions when dealing
with non-Western concepts like caste, compared to Western ones such as race.

摘要：大型語言模型 (LLM) 已成為現代社會不可或缺的一部分，為個人助理等面向使用者的應用程式和招募工具等企業應用程式提供支援。儘管它們很實用，但研究顯示 LLM 會延續系統性偏見。然而，先前關於 LLM 危害的研究主要關注種族和性別等西方概念，往往忽略了世界其他地區的文化概念。此外，這些研究通常將「危害」視為單一面向，忽略了危害表現出的各種微妙形式。為了填補這個空白，我們引入了隱蔽危害和社會威脅 (CHAST)，這是一組以社會科學文獻為基礎的七項指標。我們利用與人類評估相符的評估模型來檢查 LLM 生成的對話中是否存在隱蔽危害，特別是在招聘的背景下。我們的實驗顯示，本研究中包含的八個 LLM 中有七個生成了充滿 CHAST 的對話，其特徵是在看似中立的語言中表達惡意觀點，而現有方法不太可能檢測到這些觀點。值得注意的是，與種族等西方概念相比，這些 LLM 在處理種姓等非西方概念時表現出更極端的觀點和意見。

##### **Kreyòl-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages**
2405.05376v1 by Nathaniel R. Robinson, Raj Dabre, Ammon Shurtz, Rasul Dent, Onenamiyi Onesi, Claire Bizon Monroc, Loïc Grobol, Hasan Muhammad, Ashi Garg, Naome A. Etori, Vijay Murari Tiyyala, Olanrewaju Samuel, Matthew Dean Stutzman, Bismarck Bamfo Odoom, Sanjeev Khudanpur, Stephen D. Richardson, Kenton Murray

A majority of language technologies are tailored for a small number of
high-resource languages, while relatively many low-resource languages are
neglected. One such group, Creole languages, have long been marginalized in
academic study, though their speakers could benefit from machine translation
(MT). These languages are predominantly used in much of Latin America, Africa
and the Caribbean. We present the largest cumulative dataset to date for Creole
language MT, including 14.5M unique Creole sentences with parallel translations
-- 11.6M of which we release publicly, and the largest bitexts gathered to date
for 41 languages -- the first ever for 21. In addition, we provide MT models
supporting all 41 Creole languages in 172 translation directions. Given our
diverse dataset, we produce a model for Creole language MT exposed to more
genre diversity than ever before, which outperforms a genre-specific Creole MT
model on its own benchmark for 23 of 34 translation directions.

摘要：大部分語言技術都是為少數高資源語言量身打造，而相對較多的低資源語言則被忽視。克里奧爾語就是這樣的一個群體，長期以來在學術研究中被邊緣化，儘管他們的使用者可以從機器翻譯 (MT) 中受益。這些語言主要用於拉丁美洲、非洲和加勒比海地區的大部分地區。我們提供了迄今為止最大的克里奧爾語 MT 累積數據集，其中包括 1450 萬個帶有並行翻譯的獨特克里奧爾語句子——我們公開發布了其中的 1160 萬個，以及迄今為止為 41 種語言收集的最大雙語語料庫——其中 21 種是首次收集。此外，我們還提供了支持所有 41 種克里奧爾語的 MT 模型，共有 172 個翻譯方向。鑑於我們多樣化的數據集，我們為克里奧爾語 MT 生成了模型，該模型接觸到的類型比以往任何時候都更多，在 34 個翻譯方向中的 23 個方向上，它都優於針對特定類型的克里奧爾語 MT 模型在其自己的基準上。

##### **Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models**
2405.05374v1 by Luke Merrick, Danmei Xu, Gaurav Nuti, Daniel Campos

This report describes the training dataset creation and recipe behind the
family of \texttt{arctic-embed} text embedding models (a set of five models
ranging from 22 to 334 million parameters with weights open-sourced under an
Apache-2 license). At the time of their release, each model achieved
state-of-the-art retrieval accuracy for models of their size on the MTEB
Retrieval leaderboard, with the largest model, arctic-embed-l outperforming
closed source embedding models such as Cohere's embed-v3 and Open AI's
text-embed-3-large. In addition to the details of our training recipe, we have
provided several informative ablation studies, which we believe are the cause
of our model performance.

摘要：這份報告說明了 \texttt{arctic-embed} 文字嵌入模型背後的訓練資料集建立與配方（一組五個模型，參數範圍從 2200 萬到 3.34 億，權重在 Apache-2 授權下開放原始碼）。在這些模型發布時，每個模型在 MTEB 檢索排行榜上都達到了同等規模模型的最新檢索準確度，其中最大的模型 arctic-embed-l 優於封閉原始碼嵌入模型，例如 Cohere 的 embed-v3 和 Open AI 的 text-embed-3-large。除了我們的訓練配方的詳細資訊外，我們還提供了多項具有參考價值的消融研究，我們認為這些研究是我們模型效能的原因。

##### **Offline Model-Based Optimization via Policy-Guided Gradient Search**
2405.05349v1 by Yassine Chemingui, Aryan Deshwal, Trong Nghia Hoang, Janardhan Rao Doppa

Offline optimization is an emerging problem in many experimental engineering
domains including protein, drug or aircraft design, where online
experimentation to collect evaluation data is too expensive or dangerous. To
avoid that, one has to optimize an unknown function given only its offline
evaluation at a fixed set of inputs. A naive solution to this problem is to
learn a surrogate model of the unknown function and optimize this surrogate
instead. However, such a naive optimizer is prone to erroneous overestimation
of the surrogate (possibly due to over-fitting on a biased sample of function
evaluation) on inputs outside the offline dataset. Prior approaches addressing
this challenge have primarily focused on learning robust surrogate models.
However, their search strategies are derived from the surrogate model rather
than the actual offline data. To fill this important gap, we introduce a new
learning-to-search perspective for offline optimization by reformulating it as
an offline reinforcement learning problem. Our proposed policy-guided gradient
search approach explicitly learns the best policy for a given surrogate model
created from the offline data. Our empirical results on multiple benchmarks
demonstrate that the learned optimization policy can be combined with existing
offline surrogates to significantly improve the optimization performance.

摘要：離線最佳化在許多實驗工程領域中是一個新興的問題，包括蛋白質、藥物或飛機設計，其中在線上實驗收集評估資料的成本過於昂貴或危險。為了避免這種情況，必須最佳化一個未知函數，且僅在固定輸入集上提供其離線評估。解決此問題的一個天真方法是學習未知函數的替代模型，並最佳化此替代模型。然而，這種天真的最佳化器容易對離線資料集之外的輸入過度估計替代模型（可能是由於過度擬合函數評估的偏差樣本）。先前解決此挑戰的方法主要集中在學習穩健的替代模型。然而，他們的搜尋策略是從替代模型衍生的，而不是實際的離線資料。為了填補這個重要的差距，我們引入了一個新的學習搜尋觀點，以離線最佳化為離線強化學習問題重新制定。我們提出的策略引導梯度搜尋方法明確學習從離線資料建立的特定替代模型的最佳策略。我們在多個基準上的經驗結果證明，學習的最佳化策略可以與現有的離線替代模型結合，以顯著提升最佳化效能。

##### **The Effect of Model Size on LLM Post-hoc Explainability via LIME**
2405.05348v1 by Henning Heyen, Amy Widdicombe, Noah Y. Siegel, Maria Perez-Ortiz, Philip Treleaven

Large language models (LLMs) are becoming bigger to boost performance.
However, little is known about how explainability is affected by this trend.
This work explores LIME explanations for DeBERTaV3 models of four different
sizes on natural language inference (NLI) and zero-shot classification (ZSC)
tasks. We evaluate the explanations based on their faithfulness to the models'
internal decision processes and their plausibility, i.e. their agreement with
human explanations. The key finding is that increased model size does not
correlate with plausibility despite improved model performance, suggesting a
misalignment between the LIME explanations and the models' internal processes
as model size increases. Our results further suggest limitations regarding
faithfulness metrics in NLI contexts.

摘要：大型語言模型（LLM）正變得更大以提升效能。
然而，鮮少人知道可解釋性如何受到此趨勢影響。
這項工作探索了四種不同大小的 DeBERTaV3 模型的 LIME 解釋，探討自然語言推理 (NLI) 和零次分類 (ZSC) 任務。我們根據解釋對模型內部決策流程的忠實度及其合理性（即與人類解釋的一致性）來評估解釋。關鍵發現是，儘管模型效能有所提升，但模型大小的增加與合理性並無相關性，這表明隨著模型大小的增加，LIME 解釋與模型的內部流程之間存在錯位。我們的結果進一步表明在 NLI 背景下忠實度指標的局限性。

##### **Benchmarking Educational Program Repair**
2405.05347v1 by Charles Koutcheme, Nicola Dainese, Sami Sarsa, Juho Leinonen, Arto Hellas, Paul Denny

The emergence of large language models (LLMs) has sparked enormous interest
due to their potential application across a range of educational tasks. For
example, recent work in programming education has used LLMs to generate
learning resources, improve error messages, and provide feedback on code.
However, one factor that limits progress within the field is that much of the
research uses bespoke datasets and different evaluation metrics, making direct
comparisons between results unreliable. Thus, there is a pressing need for
standardization and benchmarks that facilitate the equitable comparison of
competing approaches. One task where LLMs show great promise is program repair,
which can be used to provide debugging support and next-step hints to students.
In this article, we propose a novel educational program repair benchmark. We
curate two high-quality publicly available programming datasets, present a
unified evaluation procedure introducing a novel evaluation metric rouge@k for
approximating the quality of repairs, and evaluate a set of five recent models
to establish baseline performance.

摘要：大型語言模型 (LLM) 的出現引起了極大的興趣，因為它們在各種教育任務中具有潛在的應用。例如，最近在程式設計教育中的工作已使用 LLM 來產生學習資源、改善錯誤訊息，並提供程式碼回饋。然而，限制該領域進展的一個因素是，許多研究使用客製化資料集和不同的評估指標，這使得結果之間的直接比較不可靠。因此，迫切需要標準化和基準，以促進競爭方法的公平比較。LLM 顯現出巨大潛力的其中一項任務是程式修復，可用於提供除錯支援和下一步提示給學生。在本文中，我們提出了一個新穎的教育程式修復基準。我們策劃了兩個高品質的公開程式設計資料集，提出了一個統一的評估程序，並引入了新的評估指標 Rouge@k 來近似修復品質，並評估了一組五個最近的模型以建立基準效能。

##### **QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums**
2405.05345v1 by Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, Andrés Monroy-Hernández

Online discussion forums provide crucial data to understand the concerns of a
wide range of real-world communities. However, the typical qualitative and
quantitative methods used to analyze those data, such as thematic analysis and
topic modeling, are infeasible to scale or require significant human effort to
translate outputs to human readable forms. This study introduces QuaLLM, a
novel LLM-based framework to analyze and extract quantitative insights from
text data on online forums. The framework consists of a novel prompting
methodology and evaluation strategy. We applied this framework to analyze over
one million comments from two Reddit's rideshare worker communities, marking
the largest study of its type. We uncover significant worker concerns regarding
AI and algorithmic platform decisions, responding to regulatory calls about
worker insights. In short, our work sets a new precedent for AI-assisted
quantitative data analysis to surface concerns from online forums.

摘要：線上討論論壇提供了至關重要的資料，讓我們得以了解廣泛實際社群的疑慮。然而，用來分析這些資料的典型定量和定性方法（例如主題分析和主題建模）難以擴充，或需要大量人力才能將輸出轉換為人類可讀的格式。本研究引入了 QuaLLM，一個基於大型語言模型的全新架構，用來分析和從線上論壇的文字資料中擷取定量見解。這個架構包含一種新穎的提示方法和評估策略。我們將此架構應用於分析來自兩個 Reddit 共乘工作者社群的超過一百萬則留言，標誌著此類研究規模最大的一次。我們揭露了工作者對 AI 和演算法平台決策的重大疑慮，回應了有關工作者見解的法規呼籲。簡而言之，我們的研究為 AI 輔助的定量資料分析樹立了一個新的先例，用於浮現線上論壇的疑慮。

##### **Joint semi-supervised and contrastive learning enables zero-shot domain-adaptation and multi-domain segmentation**
2405.05336v1 by Alvaro Gomariz, Yusuke Kikuchi, Yun Yvonna Li, Thomas Albrecht, Andreas Maunz, Daniela Ferrara, Huanxiang Lu, Orcun Goksel

Despite their effectiveness, current deep learning models face challenges
with images coming from different domains with varying appearance and content.
We introduce SegCLR, a versatile framework designed to segment volumetric
images across different domains, employing supervised and contrastive learning
simultaneously to effectively learn from both labeled and unlabeled data. We
demonstrate the superior performance of SegCLR through a comprehensive
evaluation involving three diverse clinical datasets of retinal fluid
segmentation in 3D Optical Coherence Tomography (OCT), various network
configurations, and verification across 10 different network initializations.
In an unsupervised domain adaptation context, SegCLR achieves results on par
with a supervised upper-bound model trained on the intended target domain.
Notably, we discover that the segmentation performance of SegCLR framework is
marginally impacted by the abundance of unlabeled data from the target domain,
thereby we also propose an effective zero-shot domain adaptation extension of
SegCLR, eliminating the need for any target domain information. This shows that
our proposed addition of contrastive loss in standard supervised training for
segmentation leads to superior models, inherently more generalizable to both
in- and out-of-domain test data. We additionally propose a pragmatic solution
for SegCLR deployment in realistic scenarios with multiple domains containing
labeled data. Accordingly, our framework pushes the boundaries of deep-learning
based segmentation in multi-domain applications, regardless of data
availability - labeled, unlabeled, or nonexistent.

摘要：儘管深度學習模型有效，但目前的模型在處理來自不同領域、外觀和內容各異的影像時會遇到挑戰。
我們引入了 SegCLR，這是一個多功能的架構，旨在分割不同領域的體積影像，同時採用監督式學習和對比式學習，有效地從標籤資料和未標籤資料中學習。我們透過一項全面的評估來展示 SegCLR 的卓越效能，其中包含三個不同的臨床資料集，包括 3D 光學相干斷層掃描 (OCT) 中的視網膜液體分割、各種網路組態以及跨 10 個不同網路初始化的驗證。
在非監督領域適應的背景下，SegCLR 達到的結果與在預期的目標領域中訓練的監督式上限模型相當。
值得注意的是，我們發現 SegCLR 框架的分割效能幾乎不受目標領域中大量未標籤資料的影響，因此我們也提出了 SegCLR 的一個有效的零次域適應延伸，消除了對任何目標領域資訊的需求。這顯示出我們在標準監督式訓練中加入對比損失的提議，可產生卓越的模型，本質上更能概括到領域內和領域外的測試資料。我們還提出了 SegCLR 在包含標籤資料的多個領域的實際場景中部署的務實解決方案。因此，我們的框架推動了基於深度學習的分割在多領域應用中的界限，無論資料可用性如何 - 標籤、未標籤或不存在。

##### **KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation**
2405.05329v1 by Minsik Cho, Mohammad Rastegari, Devang Naik

Large Language Model or LLM inference has two phases, the prompt (or prefill)
phase to output the first token and the extension (or decoding) phase to the
generate subsequent tokens. In this work, we propose an efficient
parallelization scheme, KV-Runahead to accelerate the prompt phase. The key
observation is that the extension phase generates tokens faster than the prompt
phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes
the prompt phase by orchestrating multiple processes to populate the KV-cache
and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache
scheme has two main benefits. Fist, since KV-cache is designed to leverage the
causal attention map, we minimize computation and computation automatically.
Second, since it already exists for the exten- sion phase, KV-Runahead is easy
to implement. We further propose context-level load-balancing to handle uneven
KV-cache generation (due to the causal attention) and to optimize TTFT.
Compared with an existing parallelization scheme such as tensor or sequential
parallelization where keys and values are locally generated and exchanged via
all-gather collectives, our experimental results demonstrate that KV-Runahead
can offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.

摘要：大型語言模型或 LLM 推論有兩個階段，提示（或預填）階段以輸出第一個符號和延伸（或解碼）階段以產生後續符號。在這項工作中，我們提出了一個高效的並行化方案，KV-Runahead 來加速提示階段。主要的觀察結果是，由於快取鍵值（KV 快取），延伸階段比提示階段更快地產生符號。因此，KV-Runahead 通過編排多個處理程序來填充 KV 快取並將首次符號時間（TTFT）最小化，從而並行化提示階段。KV 快取方案的雙重用途有兩個主要好處。首先，由於 KV 快取被設計為利用因果關係注意力圖，我們將計算和計算自動化到最小。其次，由於它已經存在於延伸階段，KV-Runahead 很容易實現。我們進一步提出上下文級負載平衡來處理不均勻的 KV 快取產生（由於因果關係注意力）並優化 TTFT。與現有的並行化方案（例如張量或順序並行化，其中鍵和值是局部生成的，並通過全收集交換）相比，我們的實驗結果表明，KV-Runahead 可以分別為 Llama 7B 和 Falcon 7B 提供超過 1.4 倍和 1.6 倍的加速。

##### **THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models**
2405.05256v1 by Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto

Mitigating hallucinations in large vision-language models (LVLMs) remains an
open problem. Recent benchmarks do not address hallucinations in open-ended
free-form responses, which we term "Type I hallucinations". Instead, they focus
on hallucinations responding to very specific question formats -- typically a
multiple-choice response regarding a particular object or attribute -- which we
term "Type II hallucinations". Additionally, such benchmarks often require
external API calls to models which are subject to change. In practice, we
observe that a reduction in Type II hallucinations does not lead to a reduction
in Type I hallucinations but rather that the two forms of hallucinations are
often anti-correlated. To address this, we propose THRONE, a novel object-based
automatic framework for quantitatively evaluating Type I hallucinations in LVLM
free-form outputs. We use public language models (LMs) to identify
hallucinations in LVLM responses and compute informative metrics. By evaluating
a large selection of recent LVLMs using public datasets, we show that an
improvement in existing metrics do not lead to a reduction in Type I
hallucinations, and that established benchmarks for measuring Type I
hallucinations are incomplete. Finally, we provide a simple and effective data
augmentation method to reduce Type I and Type II hallucinations as a strong
baseline.

摘要：大型视觉语言模型 (LVLMs) 中的幻觉缓解仍然是一个悬而未决的问题。最近的基准没有解决开放式自由形式响应中的幻觉，我们称之为“I 型幻觉”。相反，它们专注于对非常具体的问题格式做出回应的幻觉——通常是关于特定对象或属性的多项选择响应——我们称之为“II 型幻觉”。此外，此类基准通常需要对模型进行外部 API 调用，而这些模型可能会发生变化。在实践中，我们观察到 II 型幻觉的减少并没有导致 I 型幻觉的减少，而是两种形式的幻觉通常是反相关的。为了解决这个问题，我们提出了 THRONE，这是一个基于对象的新型自动框架，用于对 LVLM 自由形式输出中的 I 型幻觉进行定量评估。我们使用公共语言模型 (LM) 来识别 LVLM 响应中的幻觉并计算信息丰富的指标。通过使用公共数据集评估大量最近的 LVLMs，我们表明现有指标的改进并没有导致 I 型幻觉的减少，并且用于测量 I 型幻觉的既定基准是不完整的。最后，我们提供了一种简单有效的 data augmentation 方法来减少 I 型和 II 型幻觉作为强基线。

##### **Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge**
2405.05253v1 by Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, Paul Denny

Large language models (LLMs) have shown great potential for the automatic
generation of feedback in a wide range of computing contexts. However, concerns
have been voiced around the privacy and ethical implications of sending student
work to proprietary models. This has sparked considerable interest in the use
of open source LLMs in education, but the quality of the feedback that such
open models can produce remains understudied. This is a concern as providing
flawed or misleading generated feedback could be detrimental to student
learning. Inspired by recent work that has utilised very powerful LLMs, such as
GPT-4, to evaluate the outputs produced by less powerful models, we conduct an
automated analysis of the quality of the feedback produced by several open
source models using a dataset from an introductory programming course. First,
we investigate the viability of employing GPT-4 as an automated evaluator by
comparing its evaluations with those of a human expert. We observe that GPT-4
demonstrates a bias toward positively rating feedback while exhibiting moderate
agreement with human raters, showcasing its potential as a feedback evaluator.
Second, we explore the quality of feedback generated by several leading
open-source LLMs by using GPT-4 to evaluate the feedback. We find that some
models offer competitive performance with popular proprietary LLMs, such as
ChatGPT, indicating opportunities for their responsible use in educational
settings.

摘要：大型語言模型 (LLM) 已在各種運算環境中展現出自動產生回饋的強大潛力。然而，對於將學生作業傳送至專有模型的隱私和倫理影響，已引起各界關注。這點燃了人們對在教育中使用開放原始碼 LLM 的濃厚興趣，但此類開放模型能產生的回饋品質仍有待深入探討。這是個令人擔憂的問題，因為提供有缺陷或誤導性的產生式回饋可能會對學生的學習造成負面影響。受到近期採用功能強大的 LLM（例如 GPT-4）來評估功能較弱的模型產出之研究啟發，我們使用來自入門程式設計課程的資料集，對幾個開放原始碼模型產生的回饋品質進行自動化分析。首先，我們透過將 GPT-4 的評估結果與人類專家的評估結果進行比較，來探討採用 GPT-4 作為自動化評估器的可行性。我們觀察到，GPT-4 在評分回饋時表現出正向偏誤，同時與人類評分者展現出適度的共識，顯示其作為回饋評估器的潛力。其次，我們使用 GPT-4 來評估回饋，進一步探討幾個領先的開放原始碼 LLM 所產生回饋的品質。我們發現，某些模型提供的效能可與流行的專有 LLM（例如 ChatGPT）相媲美，這表示它們在教育環境中負責任地使用是有機會的。

##### **You Only Cache Once: Decoder-Decoder Architectures for Language Models**
2405.05254v2 by Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu Wei

We introduce a decoder-decoder architecture, YOCO, for large language models,
which only caches key-value pairs once. It consists of two components, i.e., a
cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes
global key-value (KV) caches that are reused by the cross-decoder via
cross-attention. The overall model behaves like a decoder-only Transformer,
although YOCO only caches once. The design substantially reduces GPU memory
demands, yet retains global attention capability. Additionally, the computation
flow enables prefilling to early exit without changing the final output,
thereby significantly speeding up the prefill stage. Experimental results
demonstrate that YOCO achieves favorable performance compared to Transformer in
various settings of scaling up model size and number of training tokens. We
also extend YOCO to 1M context length with near-perfect needle retrieval
accuracy. The profiling results show that YOCO improves inference memory,
prefill latency, and throughput by orders of magnitude across context lengths
and model sizes. Code is available at https://aka.ms/YOCO.

摘要：我們為大型語言模型引入了解碼器-解碼器架構 YOCO，它只快取一次鍵值對。它包含兩個組成部分，即堆疊在自解碼器上的交叉解碼器。自解碼器有效地編碼全局鍵值 (KV) 快取，而交叉解碼器透過交叉注意力重複使用這些快取。整體模型的行為類似於僅解碼器的 Transformer，儘管 YOCO 只快取一次。此設計大幅降低了 GPU 記憶體需求，但仍保留了全局注意力能力。此外，運算流程能夠在不變更最終輸出的情況下進行預先填入以提前退出，從而大幅加快預先填入階段。實驗結果證明，與 Transformer 相比，YOCO 在擴充模型大小和訓練權杖數量的各種設定中都達到了良好的效能。我們也將 YOCO 延伸到 1 百萬文字脈絡長度，並具有接近完美的針頭擷取準確度。分析結果顯示，YOCO 在各種文字脈絡長度和模型大小中，大幅改善了推論記憶體、預先填入延遲和處理量。程式碼可於 https://aka.ms/YOCO 取得。

##### **Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models**
2405.05252v1 by Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu

Diffusion Models (DMs) have exhibited superior performance in generating
high-quality and diverse images. However, this exceptional performance comes at
the cost of expensive architectural design, particularly due to the attention
module heavily used in leading models. Existing works mainly adopt a retraining
process to enhance DM efficiency. This is computationally expensive and not
very scalable. To this end, we introduce the Attention-driven Training-free
Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to
perform run-time pruning of redundant tokens, without the need for any
retraining. Specifically, for single-denoising-step pruning, we develop a novel
ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify
redundant tokens, and a similarity-based recovery method to restore tokens for
the convolution operation. In addition, we propose a Denoising-Steps-Aware
Pruning (DSAP) approach to adjust the pruning budget across different denoising
timesteps for better generation quality. Extensive evaluations show that AT-EDM
performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs
saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining
nearly the same FID and CLIP scores as the full model. Project webpage:
https://atedm.github.io.

摘要：擴散模型 (DM) 在生成高品質且多樣化的圖像方面表現出色的效能。然而，這種卓越的效能是以昂貴的架構設計為代價，特別是因為注意力模組大量用於領先的模型中。現有的作品主要採用再訓練的過程來增強 DM 效率。這在計算上很昂貴，而且擴充性不佳。為此，我們引入了注意力驅動的無訓練高效擴散模型 (AT-EDM) 架構，它利用注意力圖在執行時修剪冗餘的符號，而無需進行任何再訓練。具體來說，對於單一去噪步驟的修剪，我們開發了一種新穎的排名演算法，即廣義加權頁面排名 (G-WPR)，以識別冗餘的符號，並使用基於相似性的復原方法來復原用於卷積運算的符號。此外，我們提出了一種去噪步驟感知修剪 (DSAP) 方法，以調整不同去噪時間步長的修剪預算，以獲得更好的生成品質。廣泛的評估顯示，AT-EDM 在效率方面表現優於先前的技術（例如，節省 38.8% 的 FLOP，並且速度比 Stable Diffusion XL 快 1.53 倍），同時維持與完整模型幾乎相同的 FID 和 CLIP 分數。專案網頁：https://atedm.github.io。

##### **LLMs with Personalities in Multi-issue Negotiation Games**
2405.05248v2 by Sean Noh, Ho-Chun Herbert Chang

Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.

摘要：透過大型語言模型（LLM）的驅動，AI 代理已經能夠執行許多人類任務。使用大五人格特質中最具代表性的定義，我們測量 LLM 在博弈論架構中協商的能力，以及測量公平性和風險概念的方法論挑戰。針對單一議題和多重議題協商的模擬（n=1,500）顯示，隨著議題估值不對稱，領域複雜性會增加，這會改善協議率，但會減少激進協商的盈餘。透過梯度提升迴歸和 Shapley 解釋器，我們發現開放性、盡責性和神經質傾向與公平傾向有關；低宜人性與低開放性與理性傾向有關。低盡責性與高毒性有關。這些結果表明，LLM 可能內建了預設為公平行為的防護機制，但可以被「越獄」以利用合適的對手。我們也提供了關於協商機器人如何設計的務實見解，以及一個基於博弈論和計算社會科學評估協商行為的架構。

##### **SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan**
2405.05244v1 by You Zhang, Yongyi Zang, Jiatong Shi, Ryuichi Yamamoto, Jionghao Han, Yuxun Tang, Tomoki Toda, Zhiyao Duan

The rapid advancement of AI-generated singing voices, which now closely mimic
natural human singing and align seamlessly with musical scores, has led to
heightened concerns for artists and the music industry. Unlike spoken voice,
singing voice presents unique challenges due to its musical nature and the
presence of strong background music, making singing voice deepfake detection
(SVDD) a specialized field requiring focused attention. To promote SVDD
research, we recently proposed the "SVDD Challenge," the very first research
challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and
deepfake singing voice recordings. The challenge will be held in conjunction
with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).

摘要：隨著 AI 生成的歌聲快速進步，現在已能緊密模仿自然的人聲演唱，並與樂譜無縫對齊，這讓藝術家和音樂產業高度關注。與說話的聲音不同，歌聲因其音樂性質和強烈的背景音樂而呈現出獨特的挑戰，使得歌聲深度造假偵測 (SVDD) 成為需要專注關注的專業領域。為了推廣 SVDD 研究，我們最近提出了「SVDD 挑戰」，這是第一個專注於實驗室控制和野外真實與深度造假的歌聲錄音的 SVDD 研究挑戰。此挑戰將與 2024 年 IEEE 口語語言技術研討會 (SLT 2024) 同時舉行。

##### **Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers**
2405.05219v1 by Jiuxiang Gu, Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Junze Yin

Large Language Models (LLMs) have profoundly changed the world. Their
self-attention mechanism is the key to the success of transformers in LLMs.
However, the quadratic computational cost $O(n^2)$ to the length $n$ input
sequence is the notorious obstacle for further improvement and scalability in
the longer context. In this work, we leverage the convolution-like structure of
attention matrices to develop an efficient approximation method for attention
computation using convolution matrices. We propose a $\mathsf{conv}$ basis
system, "similar" to the rank basis, and show that any lower triangular
(attention) matrix can always be decomposed as a sum of $k$ structured
convolution matrices in this basis system. We then design an algorithm to
quickly decompose the attention matrix into $k$ convolution matrices. Thanks to
Fast Fourier Transforms (FFT), the attention {\it inference} can be computed in
$O(knd \log n)$ time, where $d$ is the hidden dimension. In practice, we have $
d \ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd =
n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$.
Furthermore, the attention {\it training forward} and {\it backward gradient}
can be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly
computing the $n \times n$ attention matrix, which may largely alleviate the
quadratic computational complexity. Furthermore, our algorithm works on any
input matrices. This work provides a new paradigm for accelerating attention
computation in transformers to enable their application to longer contexts.

摘要：大型語言模型 (LLM) 深刻地改變了世界。它們的自我注意機制是 LLM 中Transformer的成功的關鍵。然而，對於長度為 n 的輸入序列，二次計算成本 $O(n^2)$ 是進一步改進和擴展到更長語境中的著名障礙。在這項工作中，我們利用注意矩陣的卷積類似結構，開發了一種使用卷積矩陣對注意力計算進行有效近似的方法。我們提出了一個 $\mathsf{conv}$ 基礎系統，它「類似」於秩基礎，並證明任何下三角（注意力）矩陣都可以始終分解為這個基礎系統中 k 個結構化卷積矩陣的和。然後，我們設計了一種演算法，將注意力矩陣快速分解為 k 個卷積矩陣。得益於快速傅立葉轉換 (FFT)，注意力「推論」可以在 $O(knd \log n)$ 時間內計算，其中 d 是隱藏維度。在實務中，我們有 $d \ll n$，即對於 Gemma，$d=3,072$ 且 $n=1,000,000$。因此，當 $kd = n^{o(1)}$ 時，我們的演算法幾乎達到線性時間，即 $n^{1+o(1)}$。此外，注意力「訓練前向」和「反向梯度」也可以在 $n^{1+o(1)}$ 中計算。我們的做法可以避免明確計算 $n \times n$ 注意力矩陣，這可能會大幅減輕二次計算複雜度。此外，我們的演算法適用於任何輸入矩陣。這項工作為加速Transformer中的注意力計算提供了一個新的典範，以使其能夠應用於更長的語境中。

##### **CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation**
2405.05204v1 by Drew Walker, Annie Thorne, Sudeshna Das, Jennifer Love, Hannah LF Cooper, Melvin Livingston III, Abeed Sarker

Objective: To detect and classify features of stigmatizing and biased
language in intensive care electronic health records (EHRs) using natural
language processing techniques. Materials and Methods: We first created a
lexicon and regular expression lists from literature-driven stem words for
linguistic features of stigmatizing patient labels, doubt markers, and scare
quotes within EHRs. The lexicon was further extended using Word2Vec and GPT
3.5, and refined through human evaluation. These lexicons were used to search
for matches across 18 million sentences from the de-identified Medical
Information Mart for Intensive Care-III (MIMIC-III) dataset. For each
linguistic bias feature, 1000 sentence matches were sampled, labeled by expert
clinical and public health annotators, and used to supervised learning
classifiers. Results: Lexicon development from expanded literature stem-word
lists resulted in a doubt marker lexicon containing 58 expressions, and a
stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt
markers and stigmatizing labels had the highest performance, with macro
F1-scores of .84 and .79, positive-label recall and precision values ranging
from .71 to .86, and accuracies aligning closely with human annotator agreement
(.87). Discussion: This study demonstrated the feasibility of supervised
classifiers in automatically identifying stigmatizing labels and doubt markers
in medical text, and identified trends in stigmatizing language use in an EHR
setting. Additional labeled data may help improve lower scare quote model
performance. Conclusions: Classifiers developed in this study showed high model
performance and can be applied to identify patterns and target interventions to
reduce stigmatizing labels and doubt markers in healthcare systems.

摘要：<paragraph>目標：使用自然語言處理技術來偵測和分類加護病房電子病歷（EHR）中污名化和偏見語言的特徵。材料和方法：我們首先從文獻驅動的詞幹字建立詞彙和正規表示式清單，以找出污名化患者標籤、懷疑標記和 EHR 中的恐嚇引號的語言特徵。詞彙進一步使用 Word2Vec 和 GPT 3.5 擴充，並透過人工評估進行精煉。這些詞彙用於在去識別的重症監護醫療資訊市場 III（MIMIC-III）資料集的 1,800 萬個句子中搜尋比對。對於每個語言偏見特徵，抽取 1,000 個句子比對，由臨床和公共衛生專家註解員標籤，並用於監督學習分類器。結果：從擴充的文獻詞幹字清單開發詞彙，產生包含 58 個表達式的懷疑標記詞彙，以及包含 127 個表達式的污名化標籤詞彙。懷疑標記和污名化標籤的分類器效能最高，巨觀 F1 分數為 .84 和 .79，正標籤召回率和精確度值介於 .71 到 .86，準確度與人工註解員的一致性非常接近（.87）。討論：這項研究證明了監督分類器在自動識別醫療文本中的污名化標籤和懷疑標記的可行性，並找出在 EHR 設定中使用污名化語言的趨勢。額外的標籤資料可能有助於改善較低的恐嚇引號模型效能。結論：本研究中開發的分類器顯示出很高的模型效能，可用於識別模式和目標介入措施，以減少醫療保健系統中的污名化標籤和懷疑標記。</paragraph>

##### **MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning**
2405.05189v1 by Inderjeet Nair, Lu Wang

We study the task of conducting structured reasoning as generating a
reasoning graph from natural language input using large language models (LLMs).
Previous approaches have explored various prompting schemes, yet they suffer
from error propagation due to the autoregressive nature and single-pass-based
decoding, which lack error correction capability. Additionally, relying solely
on a single sample may result in the omission of true nodes and edges. To
counter this, we draw inspiration from self-consistency (SC), which involves
sampling a diverse set of reasoning chains and taking the majority vote as the
final answer. To tackle the substantial challenge of applying SC on generated
graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of
Reasoning in Directed acyclic graph) that leverages Minimum Description Length
(MDL)-based formulation to identify consistent properties among the different
graph samples generated by an LLM. This formulation helps reject properties
that appear in only a few samples, which are likely to be erroneous, while
enabling the inclusion of missing elements without compromising precision. Our
method demonstrates superior performance than comparisons across various
structured reasoning tasks, including argument structure extraction,
explanation graph generation, inferring dependency relations among actions for
everyday tasks, and semantic graph generation from natural texts.

摘要：我們研究將結構化推理的任務作為使用大型語言模型 (LLM) 從自然語言輸入產生推理圖。
先前的做法已探討各種提示方案，但由於自迴歸性質和基於單次傳遞的解碼，它們會遭受錯誤傳播，而這缺乏錯誤修正能力。此外，僅依賴單一範例可能會導致遺漏真實節點和邊緣。為了解決此問題，我們從自一致性 (SC) 中汲取靈感，其中涉及取樣多元推理鏈並將多數決作為最終答案。為了應對在生成圖形上應用 SC 的重大挑戰，我們提出了 MIDGARD (無向非循環圖中推理的最小描述長度引導聚合)，它利用基於最小描述長度 (MDL) 的公式來識別 LLM 生成的不同圖形範例之間的一致屬性。此公式有助於拒絕僅出現在少數範例中且可能錯誤的屬性，同時在不損及精準度的前提下納入遺失元素。我們的模型在各種結構化推理任務中表現出優於比較的效能，包括論證結構提取、解釋圖形生成、推論日常任務中動作之間的依賴關係，以及從自然文本生成語義圖形。

##### **Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming**
2405.05176v1 by Tommaso Pasini, Alejo López-Ávila, Husam Quteineh, Gerasimos Lampouras, Jinhua Du, Yubing Wang, Ze Li, Yusen Sun

Composing poetry or lyrics involves several creative factors, but a
challenging aspect of generation is the adherence to a more or less strict
metric and rhyming pattern. To address this challenge specifically, previous
work on the task has mainly focused on reverse language modeling, which brings
the critical selection of each rhyming word to the forefront of each verse. On
the other hand, reversing the word order requires that models be trained from
scratch with this task-specific goal and cannot take advantage of transfer
learning from a Pretrained Language Model (PLM). We propose a novel fine-tuning
approach that prepends the rhyming word at the start of each lyric, which
allows the critical rhyming decision to be made before the model commits to the
content of the lyric (as during reverse language modeling), but maintains
compatibility with the word order of regular PLMs as the lyric itself is still
generated in left-to-right order. We conducted extensive experiments to compare
this fine-tuning against the current state-of-the-art strategies for rhyming,
finding that our approach generates more readable text and better rhyming
capabilities. Furthermore, we furnish a high-quality dataset in English and 12
other languages, analyse the approach's feasibility in a multilingual context,
provide extensive experimental results shedding light on good and bad practices
for lyrics generation, and propose metrics to compare methods in the future.

摘要：創作詩歌或歌詞牽涉到多項創意因素，但生成過程中的一項挑戰性面向在於必須遵守或多或少嚴格的格律和押韻模式。為了特別解決這項挑戰，先前針對這項任務的研究主要著重於反向語言建模，這會讓每個押韻字詞的關鍵選擇成為每節經文的最前要務。另一方面，反轉字詞順序需要模型針對這個特定任務目標從頭訓練，且無法利用預訓練語言模型 (PLM) 的轉移學習。我們提出一個新穎的微調方法，將押韻字詞置於每句歌詞的開頭，這讓關鍵的押韻決定可以在模型承諾歌詞內容之前做出（就像在反向語言建模期間一樣），但維持與一般 PLM 的字詞順序相容，因為歌詞本身仍以由左至右的順序生成。我們進行了廣泛的實驗，將這個微調與目前押韻技術的最新策略進行比較，發現我們的做法能產生更具可讀性的文字和更好的押韻能力。此外，我們提供了英文和其他 12 種語言的高品質資料集，分析了該方法在多語言環境中的可行性，提供了廣泛的實驗結果，闡明了歌詞生成的良好和不良做法，並提出了用於未來比較方法的指標。

##### **Air Gap: Protecting Privacy-Conscious Conversational Agents**
2405.05175v1 by Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, Daniel Ramage

The growing use of large language model (LLM)-based conversational agents to
manage sensitive user data raises significant privacy concerns. While these
agents excel at understanding and acting on context, this capability can be
exploited by malicious actors. We introduce a novel threat model where
adversarial third-party apps manipulate the context of interaction to trick
LLM-based agents into revealing private information not relevant to the task at
hand.
  Grounded in the framework of contextual integrity, we introduce AirGapAgent,
a privacy-conscious agent designed to prevent unintended data leakage by
restricting the agent's access to only the data necessary for a specific task.
Extensive experiments using Gemini, GPT, and Mistral models as agents validate
our approach's effectiveness in mitigating this form of context hijacking while
maintaining core agent functionality. For example, we show that a single-query
context hijacking attack on a Gemini Ultra agent reduces its ability to protect
user data from 94% to 45%, while an AirGapAgent achieves 97% protection,
rendering the same attack ineffective.

摘要：隨著基於大型語言模型 (LLM) 的對話式代理程式日益用於管理敏感的使用者資料，這會引發重大的隱私問題。雖然這些代理程式擅長理解和處理脈絡，但惡意行為者可能會利用此功能。我們引入了一個新的威脅模型，其中惡意的第三方應用程式會操縱互動脈絡，誘騙基於 LLM 的代理程式揭露與手邊任務無關的私人資訊。
  基於脈絡完整性的架構，我們引入了 AirGapAgent，這是一個注重隱私的代理程式，旨在透過限制代理程式僅存取特定任務所需的資料來防止意外的資料外洩。使用 Gemini、GPT 和 Mistral 模型作為代理程式的廣泛實驗驗證了我們的方法在減輕這種形式的脈絡劫持時的有效性，同時維持核心代理程式功能。例如，我們展示了對 Gemini Ultra 代理程式的單一查詢脈絡劫持攻擊，會將其保護使用者資料的能力從 94% 降低到 45%，而 AirGapAgent 達到了 97% 的保護，讓相同的攻擊無效。

##### **Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language**
2405.05161v1 by Julia Krebs, Evie Malaia, Ronnie B. Wilbur, Isabella Fessl, Hans-Peter Wiesinger, Hermann Schwameder, Dietmar Roehm

Across a number of sign languages, temporal and spatial characteristics of
dominant hand articulation are used to express semantic and grammatical
features. In this study of Austrian Sign Language (\"Osterreichische
Geb\"ardensprache, or \"OGS), motion capture data of four Deaf signers is used
to quantitatively characterize the kinematic parameters of sign production in
verbs and adjectives. We investigate (1) the difference in production between
verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking
an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in
intensified vs. non-intensified (plain) forms. Motion capture data analysis
using linear-mixed effects models (LME) indicates that both the endpoint
marking in verbs, as well as marking of intensification in adjectives, are
expressed by movement modulation in \"OGS. While the semantic distinction
between verb types (telic/atelic) is marked by higher peak velocity and shorter
duration for telic signs compared to atelic ones, the grammatical distinction
(intensification) in adjectives is expressed by longer duration for intensified
compared to non-intensified adjectives. The observed individual differences of
signers might be interpreted as personal signing style.

摘要：在許多手語中，慣用手關節的時間和空間特性被用來表達語義和語法特徵。在這項關於奧地利手語（「Osterreichische Geb\"ardensprache」或「OGS」）的研究中，四位聾人手語者的動作捕捉資料被用來量化描述動詞和形容詞中手語製作的運動參數。我們探討（1）涉及自然終點的動詞（完成動詞；例如到達）和沒有終點的動詞（非完成動詞；例如分析）之間的製作差異，以及（2）加強與非加強（普通）形式的形容詞手語。使用線性混合效應模型（LME）的動作捕捉資料分析指出，動詞中的終點標記以及形容詞中的強化標記都由「OGS」中的動作調節表達。雖然動詞類型（完成/非完成）之間的語義區別由與非完成手語相比更高的峰值速度和更短的持續時間標記，但形容詞中的語法區別（強化）由與非強化形容詞相比更長的持續時間表達。手語者觀察到的個別差異可能被解釋為個人的手語風格。

##### **The Potential and Implications of Generative AI on HCI Education**
2405.05154v1 by Ahmed Kharrufa, Ian G Johnson

Generative AI (GAI) is impacting teaching and learning directly or indirectly
across a range of subjects and disciplines. As educators, we need to understand
the potential and limitations of AI in HCI education and ensure our graduating
HCI students are aware of the potential and limitations of AI in HCI. In this
paper, we report on the main pedagogical insights gained from the inclusion of
generative AI into a 10 week undergraduate module. We designed the module to
encourage student experimentation with GAI models as part of the design brief
requirement and planned practical sessions and discussions. Our insights are
based on replies to a survey sent out to the students after completing the
module. Our key findings, for HCI educators, report on the use of AI as a
persona for developing project ideas and creating resources for design, and AI
as a mirror for reflecting students' understanding of key concepts and ideas
and highlighting knowledge gaps. We also discuss potential pitfalls that should
be considered and the need to assess students' literacies and assumptions of
GAIs as pedagogical tools. Finally, we put forward the case for educators to
take the opportunities GAI presents as an educational tool and be experimental,
creative, and courageous in their practice. We end with a discussion of our
findings in relation to the TPACK framework in HCI.

摘要：生成式 AI (GAI) 直接或間接影響各個科目和學科的教學和學習。作為教育工作者，我們需要了解 AI 在 HCI 教育中的潛力和限制，並確保我們畢業的 HCI 學生了解 AI 在 HCI 中的潛力和限制。在本文中，我們報告了將生成式 AI 納入 10 週大學模組中獲得的主要教學見解。我們設計模組以鼓勵學生將 GAI 模型的實驗作為設計簡報要求的一部分，並規劃實務課程和討論。我們的見解基於在完成模組後寄給學生的調查回覆。我們對 HCI 教育工作者的主要發現報告了 AI 作為開發專案構想和建立設計資源的角色，以及 AI 作為反映學生對關鍵概念和想法的理解並突顯知識差距的鏡子。我們也討論應考慮的潛在陷阱，以及評量學生對 GAI 作為教學工具的識字能力和假設的必要性。最後，我們提出教育工作者應將 GAI 視為教育工具所帶來的機會，並在實務中保持實驗、創意和勇氣。我們以討論我們的發現與 HCI 中的 TPACK 架構的關係作為結尾。

##### **Hybrid Convolutional Neural Networks with Reliability Guarantee**
2405.05146v2 by Hans Dermot Doran, Suzana Veljanovska

Making AI safe and dependable requires the generation of dependable models
and dependable execution of those models. We propose redundant execution as a
well-known technique that can be used to ensure reliable execution of the AI
model. This generic technique will extend the application scope of
AI-accelerators that do not feature well-documented safety or dependability
properties. Typical redundancy techniques incur at least double or triple the
computational expense of the original. We adopt a co-design approach,
integrating reliable model execution with non-reliable execution, focusing that
additional computational expense only where it is strictly necessary. We
describe the design, implementation and some preliminary results of a hybrid
CNN.

摘要：要確保 AI 安全且可靠，需要產生可靠的模型以及可靠地執行這些模型。我們建議採用冗餘執行，這是一種眾所周知的技術，可用於確保 AI 模型的可靠執行。此通用技術將擴展沒有完善記錄的安全或可靠性屬性的 AI 加速器的應用範圍。典型的冗餘技術會產生至少兩倍或三倍於原始計算成本的計算開銷。我們採用共同設計方法，將可靠模型執行與非可靠執行整合在一起，僅將額外的計算開銷集中在絕對必要的地方。我們描述了混合 CNN 的設計、實作和一些初步結果。

##### **XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples**
2405.05116v1 by Peiqin Lin, André F. T. Martins, Hinrich Schütze

Recent studies have shown that leveraging off-the-shelf or fine-tuned
retrievers, capable of retrieving high-quality in-context examples,
significantly improves in-context learning of English. However, adapting these
methods to other languages, especially low-resource ones, presents challenges
due to the scarcity of available cross-lingual retrievers and annotated data.
In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method
tailored to tackle the challenge of cross-lingual in-context learning using
only annotated English data. XAMPLER first trains a retriever with
positive/negative English samples, which are constructed based on the
predictions of the multilingual large language model for in-context learning.
Then, the trained retriever is directly employed to retrieve English examples
as few-shot examples for in-context learning of target languages. Experiments
on the massively multilingual text classification benchmark of SIB200 with 176
languages demonstrate that XAMPLER substantially improves the in-context
learning performance across languages. Our code is available at
https://github.com/cisnlp/XAMPLER.

摘要：最近的研究表明，利用現成的或微調過的檢索器，能夠檢索出高品質的語境範例，可以顯著改善英文的語境學習。然而，將這些方法調整到其他語言，尤其是低資源語言，會因為缺乏可用的跨語言檢索器和註解資料而產生挑戰。在本文中，我們介紹了 XAMPLER：跨語言範例檢索，一種方法專門用於解決跨語言語境學習的挑戰，僅使用註解過的英文資料。XAMPLER 首先使用正/負英文範例訓練檢索器，這些範例是根據多語言大型語言模型的預測，針對語境學習所建構的。然後，直接使用訓練好的檢索器來檢索英文範例，作為目標語言語境學習的少樣本範例。在包含 176 種語言的 SIB200 大規模多語言文字分類基準上進行的實驗證明，XAMPLER 大幅改善了跨語言的語境學習效能。我們的程式碼可在 https://github.com/cisnlp/XAMPLER 取得。

##### **QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs**
2405.05109v1 by Weijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos Kanoulas, Maarten de Rijke

Table summarization is a crucial task aimed at condensing information from
tabular data into concise and comprehensible textual summaries. However,
existing approaches often fall short of adequately meeting users' information
and quality requirements and tend to overlook the complexities of real-world
queries. In this paper, we propose a novel method to address these limitations
by introducing query-focused multi-table summarization. Our approach, which
comprises a table serialization module, a summarization controller, and a large
language model (LLM), utilizes textual queries and multiple tables to generate
query-dependent table summaries tailored to users' information needs. To
facilitate research in this area, we present a comprehensive dataset
specifically tailored for this task, consisting of 4909 query-summary pairs,
each associated with multiple tables. Through extensive experiments using our
curated dataset, we demonstrate the effectiveness of our proposed method
compared to baseline approaches. Our findings offer insights into the
challenges of complex table reasoning for precise summarization, contributing
to the advancement of research in query-focused multi-table summarization.

摘要：表格摘要是一項關鍵任務，旨在將來自表格資料的資訊濃縮成簡潔且易於理解的文字摘要。然而，現有的方法通常無法充分滿足使用者的資訊和品質需求，且傾向於忽略真實世界查詢的複雜性。在本文中，我們提出了一種新的方法來解決這些限制，方法是引入以查詢為中心的多分表摘要。我們的做法包含一個表格序列化模組、一個摘要控制器和一個大型語言模型 (LLM)，它利用文字查詢和多個表格來產生依據查詢而定的表格摘要，以滿足使用者的資訊需求。為了促進這方面的研究，我們提出了一個專門針對此任務的全面資料集，其中包含 4909 個查詢摘要對，每個都與多個表格相關聯。透過使用我們整理的資料集進行廣泛的實驗，我們證明了我們提出的方法與基線方法相比的有效性。我們的研究結果提供了對複雜表格推理在精確摘要方面的挑戰的見解，有助於推進以查詢為中心的多分表摘要的研究。

##### **Concerns on Bias in Large Language Models when Creating Synthetic Personae**
2405.05080v1 by Helena A. Haxvig

This position paper explores the benefits, drawbacks, and ethical
considerations of incorporating synthetic personae in HCI research,
particularly focusing on the customization challenges beyond the limitations of
current Large Language Models (LLMs). These perspectives are derived from the
initial results of a sub-study employing vignettes to showcase the existence of
bias within black-box LLMs and explore methods for manipulating them. The study
aims to establish a foundation for understanding the challenges associated with
these models, emphasizing the necessity of thorough testing before utilizing
them to create synthetic personae for HCI research.

摘要：此立場文件探討了在 HCI 研究中納入合成角色的優點、缺點和倫理考量，特別關注當前大型語言模型 (LLM) 的限制之外的客製化挑戰。這些觀點來自於一項子研究的初步結果，該研究採用小故事來說明黑箱 LLM 中存在的偏見，並探討操縱它們的方法。該研究旨在建立一個基礎，以了解與這些模型相關的挑戰，強調在利用它們為 HCI 研究創建合成角色之前徹底測試的必要性。

##### **Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology**
2405.05299v1 by Anja Thieme, Abhijith Rajamohan, Benjamin Cooper, Heather Groombridge, Robert Simister, Barney Wong, Nicholas Woznitza, Mark Ames Pinnock, Maria Teodora Wetscherek, Cecily Morrison, Hannah Richardson, Fernando Pérez-García, Stephanie L. Hyland, Shruthi Bannur, Daniel C. Castro, Kenza Bouzid, Anton Schwaighofer, Mercy Ranjit, Harshita Sharma, Matthew P. Lungren, Ozan Oktay, Javier Alvarez-Valle, Aditya Nori, Stephen Harris, Joseph Jacob

Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose
into the stomach to deliver nutrition or medication. If not placed correctly,
they can cause serious harm, even death to patients. Recent AI developments
demonstrate the feasibility of robustly detecting NGT placement from Chest
X-ray images to reduce risks of sub-optimally or critically placed NGTs being
missed or delayed in their detection, but gaps remain in clinical practice
integration. In this study, we present a human-centered approach to the problem
and describe insights derived following contextual inquiry and in-depth
interviews with 15 clinical stakeholders. The interviews helped understand
challenges in existing workflows, and how best to align technical capabilities
with user needs and expectations. We discovered the trade-offs and complexities
that need consideration when choosing suitable workflow stages, target users,
and design configurations for different AI proposals. We explored how to
balance AI benefits and risks for healthcare staff and patients within broader
organizational and medical-legal constraints. We also identified data issues
related to edge cases and data biases that affect model training and
evaluation; how data documentation practices influence data preparation and
labelling; and how to measure relevant AI outcomes reliably in future
evaluations. We discuss how our work informs design and development of AI
applications that are clinically useful, ethical, and acceptable in real-world
healthcare services.

摘要：鼻胃管 (NGT) 是經由鼻腔插入胃中以提供營養或藥物的餵食管。如果沒有正確放置，可能會對患者造成嚴重傷害，甚至死亡。最近的 AI 發展證明了從胸部 X 光影像中穩健偵測 NGT 放置位置的可行性，以降低遺漏或延遲偵測位置不佳或危急的 NGT 的風險，但臨床實務整合仍有差距。在這項研究中，我們提出以人為中心的解決問題方法，並描述在脈絡探究和與 15 位臨床利益關係人深度訪談後獲得的見解。訪談有助於了解現有工作流程中的挑戰，以及如何最佳地將技術能力與使用者需求和期望結合起來。我們發現了在選擇適合的工作流程階段、目標使用者和不同 AI 提議的設計配置時需要考量的權衡和複雜性。我們探討了如何在更廣泛的組織和醫療法律約束下平衡 AI 對醫療保健人員和患者的益處和風險。我們也找出影響模型訓練和評估的邊緣案例和資料偏差相關的資料問題；資料文件編寫實務如何影響資料準備和標籤；以及如何在未來的評估中可靠地衡量相關的 AI 結果。我們討論了我們的工作如何為在現實世界的醫療保健服務中臨床上有用、符合道德且可接受的 AI 應用程式設計和開發提供資訊。

##### **Designing Skill-Compatible AI: Methodologies and Frameworks in Chess**
2405.05066v1 by Karim Hamade, Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, Ashton Anderson

Powerful artificial intelligence systems are often used in settings where
they must interact with agents that are computationally much weaker, for
example when they work alongside humans or operate in complex environments
where some tasks are handled by algorithms, heuristics, or other entities of
varying computational power. For AI agents to successfully interact in these
settings, however, achieving superhuman performance alone is not sufficient;
they also need to account for suboptimal actions or idiosyncratic style from
their less-skilled counterparts. We propose a formal evaluation framework for
assessing the compatibility of near-optimal AI with interaction partners who
may have much lower levels of skill; we use popular collaborative chess
variants as model systems to study and develop AI agents that can successfully
interact with lower-skill entities. Traditional chess engines designed to
output near-optimal moves prove to be inadequate partners when paired with
engines of various lower skill levels in this domain, as they are not designed
to consider the presence of other agents. We contribute three methodologies to
explicitly create skill-compatible AI agents in complex decision-making
settings, and two chess game frameworks designed to foster collaboration
between powerful AI agents and less-skilled partners. On these frameworks, our
agents outperform state-of-the-art chess AI (based on AlphaZero) despite being
weaker in conventional chess, demonstrating that skill-compatibility is a
tangible trait that is qualitatively and measurably distinct from raw
performance. Our evaluations further explore and clarify the mechanisms by
which our agents achieve skill-compatibility.

摘要：強大的 AI 系統經常在必須與運算能力較弱的代理互動的環境中使用，例如當它們與人類並肩工作或在由演算法、啟發法或其他不同運算能力實體處理某些任務的複雜環境中運作時。然而，對於 AI 代理要在這些環境中成功互動，僅僅達到超人類的表現是不夠的；它們還需要考慮技能較差的對應方的次佳行動或獨特風格。我們提出一個正式的評估架構，用於評估近乎最佳 AI 與技能水平可能低得多的互動夥伴的相容性；我們使用流行的合作西洋棋變體作為模型系統來研究和開發能夠與技能較低的實體成功互動的 AI 代理。傳統的西洋棋引擎旨在輸出近乎最佳的移動，證明在與此領域中各種較低技能等級的引擎配對時，它們並非稱職的夥伴，因為它們並非設計用於考量其他代理的存在。我們貢獻了三種方法論，用於在複雜的決策制定環境中明確建立技能相容的 AI 代理，以及兩個旨在促進強大 AI 代理與技能較差的夥伴之間協作的西洋棋遊戲架構。在這些架構上，我們的代理表現優於最先進的西洋棋 AI（基於 AlphaZero），儘管在傳統西洋棋中較弱，這證明了技能相容性是一個具體的特質，在質量和可衡量性上都與原始表現不同。我們的評估進一步探討並釐清了我們的代理實現技能相容性的機制。

##### **Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models**
2405.05060v1 by Aylin Gunal, Baihan Lin, Djallel Bouneffouf

Given the increasing demand for mental health assistance, artificial
intelligence (AI), particularly large language models (LLMs), may be valuable
for integration into automated clinical support systems. In this work, we
leverage a decision transformer architecture for topic recommendation in
counseling conversations between patients and mental health professionals. The
architecture is utilized for offline reinforcement learning, and we extract
states (dialogue turn embeddings), actions (conversation topics), and rewards
(scores measuring the alignment between patient and therapist) from previous
turns within a conversation to train a decision transformer model. We
demonstrate an improvement over baseline reinforcement learning methods, and
propose a novel system of utilizing our model's output as synthetic labels for
fine-tuning a large language model for the same task. Although our
implementation based on LLaMA-2 7B has mixed results, future work can
undoubtedly build on the design.

摘要：由於對心理健康協助的需求日益增加，人工智慧 (AI)，尤其是大型語言模型 (LLM)，可能對於整合到自動化臨床支援系統中非常有價值。在這項工作中，我們利用決策轉換器架構，針對患者與心理健康專業人員之間諮商對話中的主題推薦。此架構用於離線強化學習，我們從對話中的先前回合中提取狀態（對話回合嵌入）、動作（對話主題）和獎勵（衡量患者與治療師之間一致性的分數），以訓練決策轉換器模型。我們展示了對基準強化學習方法的改進，並提出了一個新穎的系統，利用我們模型的輸出作為合成標籤，以微調大型語言模型以執行相同的任務。儘管我們基於 LLaMA-2 7B 的實作有不同的結果，但未來的研究無疑可以建立在該設計上。

##### **Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources**
2405.05049v1 by Lasse Hyldig Hansen, Nikolaj Andersen, Jack Gallifant, Liam G. McCoy, James K Stone, Nura Izath, Marcela Aguirre-Jerez, Danielle S Bitterman, Judy Gichoya, Leo Anthony Celi

Background Advancements in Large Language Models (LLMs) hold transformative
potential in healthcare, however, recent work has raised concern about the
tendency of these models to produce outputs that display racial or gender
biases. Although training data is a likely source of such biases, exploration
of disease and demographic associations in text data at scale has been limited.
  Methods We conducted a large-scale textual analysis using a dataset
comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl.
The study analyzed the context in which various diseases are discussed
alongside markers of race and gender. Given that LLMs are pre-trained on
similar datasets, this approach allowed us to examine the potential biases that
LLMs may learn and internalize. We compared these findings with actual
demographic disease prevalence as well as GPT-4 outputs in order to evaluate
the extent of bias representation.
  Results Our findings indicate that demographic terms are disproportionately
associated with specific disease concepts in online texts. gender terms are
prominently associated with disease concepts, while racial terms are much less
frequently associated. We find widespread disparities in the associations of
specific racial and gender terms with the 18 diseases analyzed. Most
prominently, we see an overall significant overrepresentation of Black race
mentions in comparison to population proportions.
  Conclusions Our results highlight the need for critical examination and
transparent reporting of biases in LLM pretraining datasets. Our study suggests
the need to develop mitigation strategies to counteract the influence of biased
training data in LLMs, particularly in sensitive domains such as healthcare.

摘要：<paragraph>背景 大型语言模型 (LLM) 的进步在医疗保健领域具有变革性的潜力，然而，最近的研究引起了人们对这些模型产生显示种族或性别偏见的输出的趋势的担忧。虽然训练数据可能是此类偏见的一个可能来源，但对文本数据中疾病和人口统计关联的大规模探索一直受到限制。
方法 我们使用了一个包含各种网络来源（包括 Arxiv、维基百科和 Common Crawl）的数据集进行了大规模文本分析。该研究分析了在讨论各种疾病时种族和性别标记的语境。鉴于 LLM 是在类似的数据集上进行预训练的，这种方法使我们能够检查 LLM 可能学习和内化的潜在偏见。我们将这些发现与实际的人口疾病患病率以及 GPT-4 输出进行了比较，以评估偏见表征的程度。
结果 我们的研究结果表明，人口统计术语与在线文本中的特定疾病概念不成比例地相关。性别术语与疾病概念密切相关，而种族术语的关联频率要低得多。我们发现特定种族和性别术语与所分析的 18 种疾病的关联存在广泛差异。最突出的是，我们看到与人口比例相比，黑人种族的提及总体上明显过高。
结论 我们的研究结果强调了对 LLM 预训练数据集中的偏见进行批判性检查和透明报告的必要性。我们的研究表明需要制定缓解策略来抵消有偏训练数据对 LLM 的影响，特别是在医疗保健等敏感领域。</paragraph>

##### **StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer**
2405.05027v1 by Zijia Wang, Zhi-Song Liu

We present StyleMamba, an efficient image style transfer framework that
translates text prompts into corresponding visual styles while preserving the
content integrity of the original images. Existing text-guided stylization
requires hundreds of training iterations and takes a lot of computing
resources. To speed up the process, we propose a conditional State Space Model
for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that
sequentially aligns the image features to the target text prompts. To enhance
the local and global style consistency between text and image, we propose
masked and second-order directional losses to optimize the stylization
direction to significantly reduce the training iterations by 5 times and the
inference time by 3 times. Extensive experiments and qualitative evaluation
confirm the robust and superior stylization performance of our methods compared
to the existing baselines.

摘要：我們提出 StyleMamba，一個有效率的影像風格轉移架構，它能將文字提示轉換成對應的視覺風格，同時保留原始影像的內容完整性。現有的文字引導風格化需要數百次的訓練反覆運算，並耗費大量的運算資源。為了加速這個過程，我們提出了一個條件式狀態空間模型，用於有效率的文字驅動影像風格轉移，稱為 StyleMamba，它會將影像特徵依序對齊到目標文字提示。為了加強文字和影像之間的局部和整體風格一致性，我們提出遮罩和二階方向損失，以最佳化風格化方向，大幅減少 5 倍的訓練反覆運算和 3 倍的推論時間。廣泛的實驗和定性評估證實了我們的方法與現有的基準相比，具有強健且優異的風格化表現。

##### **ADELIE: Aligning Large Language Models on Information Extraction**
2405.05008v1 by Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li

Large language models (LLMs) usually fall short on information extraction
(IE) tasks and struggle to follow the complex instructions of IE tasks. This
primarily arises from LLMs not being aligned with humans, as mainstream
alignment datasets typically do not include IE data. In this paper, we
introduce ADELIE (Aligning large language moDELs on Information Extraction), an
aligned LLM that effectively solves various IE tasks, including closed IE, open
IE, and on-demand IE. We first collect and construct a high-quality alignment
corpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on
IEInstruct. We further train ADELIE_SFT with direct preference optimization
(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various
held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)
achieve state-of-the-art (SoTA) performance among open-source models. We
further explore the general capabilities of ADELIE, and experimental results
reveal that their general capabilities do not exhibit a noticeable decline. We
will release the code, data, and models to facilitate further research.

摘要：大型語言模型 (LLM) 通常在資訊抽取 (IE) 任務中表現不佳，且難以遵循 IE 任務的複雜指示。這主要是因為 LLM 與人類不一致，因為主流對齊資料集通常不包含 IE 資料。在本文中，我們介紹 ADELIE（對齊大型語言模型以進行資訊抽取），這是一種對齊的 LLM，可有效解決各種 IE 任務，包括封閉式 IE、開放式 IE 和依需求 IE。我們首先收集並建立一個高品質的對齊語料庫 IEInstruct，以進行 IE。然後我們使用 IEInstruct 上的指示微調來訓練 ADELIE_SFT。我們進一步使用直接偏好最佳化 (DPO) 目標來訓練 ADELIE_SFT，產生 ADELIE_DPO。在各種保留 IE 資料集上進行的廣泛實驗表明，我們的模型（ADELIE_SFT 和 ADELIE_DPO）在開源模型中取得了最先進 (SoTA) 的效能。我們進一步探索 ADELIE 的一般能力，實驗結果顯示其一般能力並未出現顯著下降。我們將釋出程式碼、資料和模型，以利進一步研究。

##### **Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning**
2405.04990v1 by Kristupas Bajarunas, Marcia L. Baptista, Kai Goebel, Manuel A. Chao

Accurately estimating a Health Index (HI) from condition monitoring data (CM)
is essential for reliable and interpretable prognostics and health management
(PHM) in complex systems. In most scenarios, complex systems operate under
varying operating conditions and can exhibit different fault modes, making
unsupervised inference of an HI from CM data a significant challenge. Hybrid
models combining prior knowledge about degradation with deep learning models
have been proposed to overcome this challenge. However, previously suggested
hybrid models for HI estimation usually rely heavily on system-specific
information, limiting their transferability to other systems. In this work, we
propose an unsupervised hybrid method for HI estimation that integrates general
knowledge about degradation into the convolutional autoencoder's model
architecture and learning algorithm, enhancing its applicability across various
systems. The effectiveness of the proposed method is demonstrated in two case
studies from different domains: turbofan engines and lithium batteries. The
results show that the proposed method outperforms other competitive
alternatives, including residual-based methods, in terms of HI quality and
their utility for Remaining Useful Life (RUL) predictions. The case studies
also highlight the comparable performance of our proposed method with a
supervised model trained with HI labels.

摘要：從狀態監控資料 (CM) 精確估計健康指數 (HI) 對於複雜系統中的可靠且可解讀的預後和健康管理 (PHM) 至關重要。在大多數情況下，複雜系統在不同的操作條件下運行，並且可能會表現出不同的故障模式，這使得從 CM 資料中進行無監督的 HI 推論成為一項重大挑戰。已經提出結合關於退化的先驗知識與深度學習模型的混合模型來克服這一挑戰。然而，先前建議用於 HI 估計的混合模型通常嚴重依賴系統特定資訊，這限制了它們對其他系統的可傳輸性。在這項工作中，我們提出了一種用於 HI 估計的無監督混合方法，該方法將關於退化的通用知識整合到卷積自動編碼器的模型架構和學習演算法中，增強了其在各種系統中的適用性。所提出的方法的有效性在來自不同領域的兩個案例研究中得到證明：渦扇發動機和鋰電池。結果表明，所提出的方法在 HI 品質和對剩餘使用壽命 (RUL) 預測的效用方面優於其他競爭性替代方案，包括基於殘差的方法。案例研究還強調了我們提出的方法與使用 HI 標籤訓練的監督模型相當的性能。

##### **An Artificial Intelligence Approach for Interpreting Creative Combinational Designs**
2405.04985v1 by Liuqing Chen, Shuhong Xiao, Yunnong Chen, Linyun Sun, Peter R. N. Childs, Ji Han

Combinational creativity, a form of creativity involving the blending of
familiar ideas, is pivotal in design innovation. While most research focuses on
how combinational creativity in design is achieved through blending elements,
this study focuses on the computational interpretation, specifically
identifying the 'base' and 'additive' components that constitute a creative
design. To achieve this goal, the authors propose a heuristic algorithm
integrating computer vision and natural language processing technologies, and
implement multiple approaches based on both discriminative and generative
artificial intelligence architectures. A comprehensive evaluation was conducted
on a dataset created for studying combinational creativity. Among the
implementations of the proposed algorithm, the most effective approach
demonstrated a high accuracy in interpretation, achieving 87.5% for identifying
'base' and 80% for 'additive'. We conduct a modular analysis and an ablation
experiment to assess the performance of each part in our implementations.
Additionally, the study includes an analysis of error cases and bottleneck
issues, providing critical insights into the limitations and challenges
inherent in the computational interpretation of creative designs.

摘要：組合式創意是一種涉及融合熟悉概念的創意形式，在設計創新中至關重要。雖然大多數研究都集中在如何透過融合元素來達成設計中的組合式創意，但本研究專注於計算詮釋，特別是識別構成創意設計的「基礎」和「附加」組成部分。為了達成此目標，作者提出了一種整合電腦視覺和自然語言處理技術的啟發式演算法，並根據判別式和生成式人工智慧架構實作多種方法。針對專門用於研究組合式創意的資料集進行了全面的評估。在所提出的演算法實作中，最有效的方法在詮釋上展現出高準確度，在識別「基礎」時達到 87.5%，在識別「附加」時達到 80%。我們進行模組分析和消融實驗，以評估實作中每個部分的效能。此外，本研究還包含對錯誤案例和瓶頸問題的分析，提供了對創意設計計算詮釋中固有限制和挑戰的重要見解。

##### **Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI**
2405.04974v1 by Keqiang Fan, Xiaohao Cai, Mahesan Niranjan

Diffusion probabilistic models (DPMs) have exhibited significant
effectiveness in computer vision tasks, particularly in image generation.
However, their notable performance heavily relies on labelled datasets, which
limits their application in medical images due to the associated high-cost
annotations. Current DPM-related methods for lesion detection in medical
imaging, which can be categorized into two distinct approaches, primarily rely
on image-level annotations. The first approach, based on anomaly detection,
involves learning reference healthy brain representations and identifying
anomalies based on the difference in inference results. In contrast, the second
approach, resembling a segmentation task, employs only the original brain
multi-modalities as prior information for generating pixel-level annotations.
In this paper, our proposed model - discrepancy distribution medical diffusion
(DDMD) - for lesion detection in brain MRI introduces a novel framework by
incorporating distinctive discrepancy features, deviating from the conventional
direct reliance on image-level annotations or the original brain modalities. In
our method, the inconsistency in image-level annotations is translated into
distribution discrepancies among heterogeneous samples while preserving
information within homogeneous samples. This property retains pixel-wise
uncertainty and facilitates an implicit ensemble of segmentation, ultimately
enhancing the overall detection performance. Thorough experiments conducted on
the BRATS2020 benchmark dataset containing multimodal MRI scans for brain
tumour detection demonstrate the great performance of our approach in
comparison to state-of-the-art methods.

摘要：擴散機率模型 (DPM) 在電腦視覺任務中展現出顯著的效能，特別是在影像生成方面。然而，它們顯著的效能高度仰賴標籤資料集，這會因為相關的高成本註解而限制它們在醫學影像中的應用。目前用於醫學影像病灶偵測的 DPM 相關方法可分為兩種不同的方法，主要仰賴影像層級的註解。第一種方法是基於異常偵測，涉及學習參考健康腦部表徵，並根據推論結果的差異來識別異常。相反地，第二種方法類似於分割任務，僅使用原始腦部多模態作為產生畫素層級註解的先驗資訊。在本文中，我們提出的模型——差異分佈醫學擴散 (DDMD)——用於腦部 MRI 中的病灶偵測，它透過納入獨特的差異特徵來引入一個新穎的架構，偏離對影像層級註解或原始腦部模態的傳統直接依賴。在我們的這種方法中，影像層級註解中的不一致性被轉換為異質樣本之間的分布差異，同時保留同質樣本內的資訊。此特性保留了逐畫素的不確定性，並促進分割的隱含集合，最終提升整體偵測效能。在 BRATS2020 基準資料集上進行的徹底實驗包含用於腦腫瘤偵測的多模態 MRI 掃描，證明了我們的方法與最先進的方法相比具有優異的效能。

##### **A review on discriminative self-supervised learning methods**
2405.04969v1 by Nikolaos Giakoumoglou, Tania Stathaki

In the field of computer vision, self-supervised learning has emerged as a
method to extract robust features from unlabeled data, where models derive
labels autonomously from the data itself, without the need for manual
annotation. This paper provides a comprehensive review of discriminative
approaches of self-supervised learning within the domain of computer vision,
examining their evolution and current status. Through an exploration of various
methods including contrastive, self-distillation, knowledge distillation,
feature decorrelation, and clustering techniques, we investigate how these
approaches leverage the abundance of unlabeled data. Finally, we have
comparison of self-supervised learning methods on the standard ImageNet
classification benchmark.

摘要：在電腦視覺領域中，自我監督學習已經成為從未標籤資料中提取穩健特徵的一種方法，其中模型從資料本身自動推導標籤，而無需手動註解。本文全面回顧了電腦視覺領域內自我監督學習的判別方法，探討了它們的演變和現狀。透過探索各種方法，包括對比、自我蒸餾、知識蒸餾、特徵去相關和聚類技術，我們探討了這些方法如何利用豐富的未標籤資料。最後，我們比較了標準 ImageNet 分類基準上的自我監督學習方法。

##### **Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers**
2405.05295v1 by Silvan Mertes, Tobias Huber, Christina Karle, Katharina Weitz, Ruben Schlagowski, Cristina Conati, Elisabeth André

In this paper, we demonstrate the feasibility of alterfactual explanations
for black box image classifiers. Traditional explanation mechanisms from the
field of Counterfactual Thinking are a widely-used paradigm for Explainable
Artificial Intelligence (XAI), as they follow a natural way of reasoning that
humans are familiar with. However, most common approaches from this field are
based on communicating information about features or characteristics that are
especially important for an AI's decision. However, to fully understand a
decision, not only knowledge about relevant features is needed, but the
awareness of irrelevant information also highly contributes to the creation of
a user's mental model of an AI system. To this end, a novel approach for
explaining AI systems called alterfactual explanations was recently proposed on
a conceptual level. It is based on showing an alternative reality where
irrelevant features of an AI's input are altered. By doing so, the user
directly sees which input data characteristics can change arbitrarily without
influencing the AI's decision. In this paper, we show for the first time that
it is possible to apply this idea to black box models based on neural networks.
To this end, we present a GAN-based approach to generate these alterfactual
explanations for binary image classifiers. Further, we present a user study
that gives interesting insights on how alterfactual explanations can complement
counterfactual explanations.

摘要：<paragraph>在本文中，我們展示了黑盒子影像分類器的反事實解釋的可行性。來自反事實思考領域的傳統解釋機制是廣泛用於可解釋人工智慧 (XAI) 的典範，因為它們遵循人類熟悉的自然推理方式。然而，這個領域最常見的方法是基於傳達關於對 AI 決策特別重要的特徵或特性的資訊。但是，要完全理解一個決策，不僅需要關於相關特徵的知識，而且對無關資訊的認識也有助於建立使用者對 AI 系統的心智模式。為此，最近在概念層面上提出了一種稱為反事實解釋的新方法來解釋 AI 系統。它是基於展示一個替代現實，其中 AI 輸入的無關特徵被改變。藉由這樣做，使用者可以直接看到哪些輸入資料特性可以在不影響 AI 決策的情況下任意改變。在本文中，我們首次展示了將這個想法應用於基於神經網路的黑盒子模型是可行的。為此，我們提出了一個基於 GAN 的方法來為二元影像分類器產生這些反事實解釋。此外，我們提出了一項使用者研究，提供了關於反事實解釋如何補充反事實解釋的有趣見解。</paragraph>

##### **P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models**
2405.04960v1 by Guochao Jiang, Zepeng Ding, Yuchen Shi, Deqing Yang

In recent years, the rise of large language models (LLMs) has made it
possible to directly achieve named entity recognition (NER) without any
demonstration samples or only using a few samples through in-context learning
(ICL). However, standard ICL only helps LLMs understand task instructions,
format and input-label mapping, but neglects the particularity of the NER task
itself. In this paper, we propose a new prompting framework P-ICL to better
achieve NER with LLMs, in which some point entities are leveraged as the
auxiliary information to recognize each entity type. With such significant
information, the LLM can achieve entity classification more precisely. To
obtain optimal point entities for prompting LLMs, we also proposed a point
entity selection method based on K-Means clustering. Our extensive experiments
on some representative NER benchmarks verify the effectiveness of our proposed
strategies in P-ICL and point entity selection.

摘要：近年來，大型語言模型 (LLM) 的興起使得在沒有任何示範範例或僅透過情境學習 (ICL) 使用少數範例的情況下，就能直接達成命名實體辨識 (NER)。然而，標準的 ICL 只協助 LLM 理解任務指示、格式和輸入標籤對應，但忽略了 NER 任務本身的特殊性。在本文中，我們提出一個新的提示架構 P-ICL，以便使用 LLM 更佳地達成 NER，其中一些點實體被用作輔助資訊來辨識每個實體類型。有了這些重要的資訊，LLM 可以更精確地達成實體分類。為了取得用於提示 LLM 的最佳點實體，我們也提出一個基於 K-Means 聚類的點實體選取方法。我們在一些具代表性的 NER 基準上進行的廣泛實驗驗證了我們在 P-ICL 和點實體選取中提出的策略的有效性。

##### **Improving Long Text Understanding with Knowledge Distilled from Summarization Model**
2405.04955v1 by Yan Liu, Yazheng Yang, Xiaokang Chen

Long text understanding is important yet challenging for natural language
processing. A long article or document usually contains many redundant words
that are not pertinent to its gist and sometimes can be regarded as noise. With
recent advances of abstractive summarization, we propose our \emph{Gist
Detector} to leverage the gist detection ability of a summarization model and
integrate the extracted gist into downstream models to enhance their long text
understanding ability. Specifically, Gist Detector first learns the gist
detection knowledge distilled from a summarization model, and then produces
gist-aware representations to augment downstream models. We evaluate our method
on three different tasks: long document classification, distantly supervised
open-domain question answering, and non-parallel text style transfer. The
experimental results show that our method can significantly improve the
performance of baseline models on all tasks.

摘要：長文本理解對自然語言處理來說很重要，但也具有挑戰性。長篇論文或文件通常包含許多與其要旨無關的冗餘字詞，有時可以被視為雜訊。隨著摘要摘要的最新進展，我們提出我們的「要點偵測器」，以利用摘要模型的要點偵測能力，並將提取出的要點整合到下游模型中，以增強其長文本理解能力。具體來說，要點偵測器首先學習從摘要模型中提煉出的要點偵測知識，然後產生要點感知表示，以擴充下游模型。我們對我們的模型進行了三項不同的任務評估：長文分類、遠程監督的開放領域問答和非平行文本風格轉換。實驗結果表明，我們的模型可以在所有任務上顯著提高基線模型的效能。

##### **VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context**
2405.04950v1 by Yunxin Li, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, Min Zhang

Large Multimodal Models (LMMs) have achieved impressive success in visual
understanding and reasoning, remarkably improving the performance of
mathematical reasoning in a visual context. Yet, a challenging type of visual
math lies in the multimodal graph theory problem, which demands that LMMs
understand the graphical structures accurately and perform multi-step reasoning
on the visual graph. Additionally, exploring multimodal graph theory problems
will lead to more effective strategies in fields like biology, transportation,
and robotics planning. To step forward in this direction, we are the first to
design a benchmark named VisionGraph, used to explore the capabilities of
advanced LMMs in solving multimodal graph theory problems. It encompasses eight
complex graph problem tasks, from connectivity to shortest path problems.
Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance
the logical accuracy of reasoning processes through graphical structure
description generation and algorithm-aware multi-step reasoning. Our extensive
study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph
reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical
structures, whether in zero/few-shot settings or with supervised fine-tuning
(SFT), which further affects problem-solving performance; 3) DPR significantly
improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V
(DPR) agent achieves SOTA performance.

摘要：大型多模态模型 (LMM) 在视觉理解和推理方面取得了令人瞩目的成功，显著提高了视觉环境中数学推理的性能。然而，一种具有挑战性的视觉数学存在于多模态图论问题中，它要求 LMM 准确理解图形结构并在视觉图上执行多步推理。此外，探索多模态图论问题将为生物学、交通运输和机器人规划等领域带来更有效的策略。为了朝这个方向迈进，我们率先设计了一个名为 VisionGraph 的基准，用于探索先进 LMM 在解决多模态图论问题中的能力。它包含八个复杂的图问题任务，从连通性到最短路径问题。随后，我们提出了一个描述-程序-推理 (DPR) 链，通过图形结构描述生成和算法感知的多步推理来增强推理过程的逻辑准确性。我们的广泛研究表明：1) GPT-4V 在多步图推理中优于 Gemini Pro；2) 所有 LMM 在图形结构的感知准确性方面表现较差，无论是在零/少样本设置中还是在有监督微调 (SFT) 的情况下，这进一步影响了问题求解性能；3) DPR 显着提高了 LMM 的多步图推理能力，并且 GPT-4V (DPR) 代理实现了 SOTA 性能。

##### **Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs**
2405.04941v1 by Eline M. Bovy, Marnix Suilen, Sebastian Junges, Nils Jansen

Partially observable Markov decision processes (POMDPs) rely on the key
assumption that probability distributions are precisely known. Robust POMDPs
(RPOMDPs) alleviate this concern by defining imprecise probabilities, referred
to as uncertainty sets. While robust MDPs have been studied extensively, work
on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We
expand the theoretical understanding of RPOMDPs by showing that 1) different
assumptions on the uncertainty sets affect optimal policies and values; 2)
RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the
same RPOMDP with different assumptions leads to semantically different POSGs
and, thus, different policies and values. These novel semantics for RPOMDPS
give access to results for the widely studied POSG model; concretely, we show
the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP
literature using our semantics, clarifying under which uncertainty assumptions
these existing works operate.

摘要：部分可觀測馬可夫決策過程 (POMDP) 依賴於一個關鍵假設，即機率分佈是精確已知的。強健 POMDP (RPOMDP) 透過定義不精確的機率（稱為不確定性集合）來減輕這個問題。儘管強健 MDP 已被廣泛研究，但對 RPOMDP 的研究卻很有限，且主要集中於演算法解決方法。我們透過顯示以下內容來擴展對 RPOMDP 的理論理解：1) 不確定性集合的不同假設會影響最佳策略和價值；2) RPOMDP 具有部分可觀測隨機遊戲 (POSG) 語義；以及 3) 具有不同假設的相同 RPOMDP 會導致語義上不同的 POSG，因此會產生不同的策略和價值。這些 RPOMDP 的新語義可以取得廣泛研究的 POSG 模型的結果；具體來說，我們證明了納許均衡的存在性。最後，我們使用我們的語義對現有的 RPOMDP 文獻進行分類，釐清這些現有著作是在哪些不確定性假設下運作的。

