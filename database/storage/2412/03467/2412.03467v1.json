{"2412.03467": {"publish_time": "2024-12-04", "title": "Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning", "paper_summary": "Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks.", "paper_summary_zh": "\u591a\u6a21\u6001\u6a21\u578b\u901a\u5e38\u5c06\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e\u89c6\u89c9\u7f16\u7801\u5668\u7ed3\u5408\u8d77\u6765\uff0c\u7136\u540e\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u867d\u7136\u6b64\u8fc7\u7a0b\u4f7f LLM \u9002\u5e94\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u9002\u5e94\u662f\u5426\u4f1a\u635f\u5bb3\u5176\u539f\u59cb\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6307\u4ee4\u5fae\u8c03\u5bf9\u8bed\u8a00\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e LLaVA\uff0c\u8fd9\u662f\u4e00\u4e2a\u9886\u5148\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5b83\u5c06 Vicuna \u6216 Mistral \u7b49 LLM \u4e0e CLIP \u89c6\u89c9\u7f16\u7801\u5668\u96c6\u6210\u5728\u4e00\u8d77\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u539f\u59cb LLM \u4e0e\u5176\u591a\u6a21\u6001\u9002\u5e94\u5bf9\u5e94\u9879\u5728\u516b\u4e2a\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u4ea7\u751f\u4e86\u51e0\u4e2a\u5173\u952e\u89c1\u89e3\u3002\u9996\u5148\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5f71\u54cd\u5728 Vicuna \u548c Mistral \u4e4b\u95f4\u6709\u6240\u4e0d\u540c\uff1a\u6211\u4eec\u89c2\u5bdf\u5230 Mistral \u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u4f46\u5927\u591a\u6570\u4efb\u52a1\u4e2d Vicuna \u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u90fd\u6709\u6240\u63d0\u9ad8\u3002\u5176\u6b21\uff0c\u867d\u7136\u591a\u6a21\u6001\u6307\u4ee4\u5b66\u4e60\u6301\u7eed\u964d\u4f4e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff08\u4f8b\u5982 GSM8K\uff09\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u589e\u5f3a\u4e86\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\uff08\u4f8b\u5982 CommonsenseQA\uff09\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u6a21\u578b\u5408\u5e76\u6280\u672f\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u5728\u591a\u6a21\u6001\u9002\u5e94\u7684 Mistral \u4e2d\u89c2\u5bdf\u5230\u7684\u8bed\u8a00\u63a8\u7406\u9000\u5316\uff0c\u751a\u81f3\u53ef\u4ee5\u63d0\u9ad8\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "author": "Neale Ratzlaff et.al.", "authors": "Neale Ratzlaff, Man Luo, Xin Su, Vasudev Lal, Phillip Howard", "id": "2412.03467v1", "paper_url": "http://arxiv.org/abs/2412.03467v1", "repo": "null"}}