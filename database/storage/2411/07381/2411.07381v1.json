{"2411.07381": {"publish_time": "2024-11-11", "title": "BeeManc at the PLABA Track of TAC-2024: RoBERTa for task 1 and LLaMA3.1 and GPT-4o for task 2", "paper_summary": "This report is the system description of the BeeManc team for shared task\nPlain Language Adaptation of Biomedical Abstracts (PLABA) 2024. This report\ncontains two sections corresponding to the two sub-tasks in PLABA 2024. In task\none, we applied fine-tuned ReBERTa-Base models to identify and classify the\ndifficult terms, jargon and acronyms in the biomedical abstracts and reported\nthe F1 score. Due to time constraints, we didn't finish the replacement task.\nIn task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot\nprompts to complete the abstract adaptation and reported the scores in BLEU,\nSARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024\non Task 1A and 1B, our \\textbf{much smaller fine-tuned RoBERTa-Base} model\nranked 3rd and 2nd respectively on the two sub-task, and the \\textbf{1st on\naveraged F1 scores across the two tasks} from 9 evaluated systems. Our share\nour fine-tuned models and related resources at\n\\url{https://github.com/HECTA-UoM/PLABA2024}", "paper_summary_zh": "\u9019\u4efd\u5831\u544a\u662f BeeManc \u5718\u968a\u91dd\u5c0d 2024 \u5e74\u751f\u7269\u91ab\u5b78\u6458\u8981\u7684\u901a\u7528\u8a9e\u8a00\u9069\u61c9 (PLABA) \u5171\u4eab\u4efb\u52d9\u6240\u505a\u7684\u7cfb\u7d71\u63cf\u8ff0\u3002\u9019\u4efd\u5831\u544a\u5305\u542b\u5169\u90e8\u5206\uff0c\u5206\u5225\u5c0d\u61c9\u65bc PLABA 2024 \u7684\u5169\u500b\u5b50\u4efb\u52d9\u3002\u5728\u4efb\u52d9\u4e00\u4e2d\uff0c\u6211\u5011\u61c9\u7528\u5fae\u8abf\u5f8c\u7684 ReBERTa-Base \u6a21\u578b\u4f86\u8b58\u5225\u548c\u5206\u985e\u751f\u7269\u91ab\u5b78\u6458\u8981\u4e2d\u7684\u56f0\u96e3\u8853\u8a9e\u3001\u8853\u8a9e\u548c\u7e2e\u5beb\uff0c\u4e26\u5831\u544a F1 \u5206\u6578\u3002\u7531\u65bc\u6642\u9593\u9650\u5236\uff0c\u6211\u5011\u6c92\u6709\u5b8c\u6210\u66ff\u63db\u4efb\u52d9\u3002\u5728\u4efb\u52d9\u4e8c\u4e2d\uff0c\u6211\u5011\u5229\u7528 Llamma3.1-70B-Instruct \u548c GPT-4o \u4ee5\u53ca\u4e00\u6b21\u6027\u63d0\u793a\u4f86\u5b8c\u6210\u6458\u8981\u9069\u61c9\uff0c\u4e26\u5831\u544a\u4e86 BLEU\u3001SARI\u3001BERTScore\u3001LENS \u548c SALSA \u4e2d\u7684\u5206\u6578\u3002\u6839\u64da PLABA-2024 \u5728\u4efb\u52d9 1A \u548c 1B \u4e2d\u7684\u5b98\u65b9\u8a55\u4f30\uff0c\u6211\u5011\u7684\\textbf{\u5fae\u8abf\u5f8c\u7684 RoBERTa-Base \u6a21\u578b\u5c0f\u5f97\u591a}\u5728\u5169\u500b\u5b50\u4efb\u52d9\u4e2d\u5206\u5225\u6392\u540d\u7b2c 3 \u548c\u7b2c 2\uff0c\u4e26\u4e14\u5728 9 \u500b\u8a55\u4f30\u7cfb\u7d71\u4e2d\\textbf{\u5728\u5169\u500b\u4efb\u52d9\u4e2d\u7684\u5e73\u5747 F1 \u5206\u6578\u4e2d\u6392\u540d\u7b2c 1}\u3002\u6211\u5011\u5728\\url{https://github.com/HECTA-UoM/PLABA2024}\u5206\u4eab\u6211\u5011\u7684\u5fae\u8abf\u6a21\u578b\u548c\u76f8\u95dc\u8cc7\u6e90", "author": "Zhidong Ling et.al.", "authors": "Zhidong Ling, Zihao Li, Pablo Romeo, Lifeng Han, Goran Nenadic", "id": "2411.07381v1", "paper_url": "http://arxiv.org/abs/2411.07381v1", "repo": "null"}}