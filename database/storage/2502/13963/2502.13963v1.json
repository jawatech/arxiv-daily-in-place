{"2502.13963": {"publish_time": "2025-02-19", "title": "MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads", "paper_summary": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u5e38\u6703\u56e0\u70ba\u8f38\u5165\u4e2d\u7684\u7121\u95dc\u8cc7\u8a0a\u800c\u8868\u73fe\u51fa\u6ce8\u610f\u529b\u5206\u6563\uff0c\u9019\u56b4\u91cd\u5730\u640d\u5bb3\u4e86\u5b83\u5011\u7684\u9577\u6587\u672c\u80fd\u529b\u3002\u53d7\u5230\u6700\u8fd1\u95dc\u65bc\u6aa2\u7d22\u982d\u5728\u9577\u6587\u672c\u4e8b\u5be6\u6027\u4e2d\u7684\u6709\u6548\u6027\u7684\u7814\u7a76\u555f\u767c\uff0c\u6211\u5011\u65e8\u5728\u900f\u904e\u76f4\u63a5\u6539\u5584\u6b64\u985e\u6aa2\u7d22\u982d\u4f86\u89e3\u6c7a\u9019\u500b\u5206\u5fc3\u554f\u984c\u3002\u6211\u5011\u63d0\u51fa\u591a\u6587\u4ef6\u6ce8\u610f\u529b\u805a\u7126 (MuDAF)\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\u660e\u78ba\u6700\u4f73\u5316\u982d\u90e8\u5c64\u7d1a\u6ce8\u610f\u529b\u5206\u914d\u7684\u65b0\u65b9\u6cd5\u3002\u6839\u64da\u5be6\u9a57\u7d50\u679c\uff0cMuDAF \u53ef\u4ee5\u986f\u8457\u63d0\u5347 LLM \u7684\u9577\u6587\u672c\u554f\u7b54\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u591a\u6587\u4ef6\u554f\u7b54\u4e2d\u3002\u5728\u6aa2\u7d22\u5206\u6578\u548c\u6ce8\u610f\u529b\u8996\u89ba\u5316\u4e0a\u7684\u5ee3\u6cdb\u8a55\u4f30\u986f\u793a\uff0cMuDAF \u5177\u6709\u8b93\u6ce8\u610f\u529b\u982d\u90e8\u66f4\u5c08\u6ce8\u65bc\u76f8\u95dc\u8cc7\u8a0a\u4e26\u6e1b\u5c11\u6ce8\u610f\u529b\u5206\u6563\u7684\u5de8\u5927\u6f5b\u529b\u3002", "author": "Weihao Liu et.al.", "authors": "Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang", "id": "2502.13963v1", "paper_url": "http://arxiv.org/abs/2502.13963v1", "repo": "null"}}