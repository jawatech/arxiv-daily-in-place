{"2412.19139": {"publish_time": "2024-12-26", "title": "PlanLLM: Video Procedure Planning with Refinable Large Language Models", "paper_summary": "Video procedure planning, i.e., planning a sequence of action steps given the\nvideo frames of start and goal states, is an essential ability for embodied AI.\nRecent works utilize Large Language Models (LLMs) to generate enriched action\nstep description texts to guide action step decoding. Although LLMs are\nintroduced, these methods decode the action steps into a closed-set of one-hot\nvectors, limiting the model's capability of generalizing to new steps or tasks.\nAdditionally, fixed action step descriptions based on world-level commonsense\nmay contain noise in specific instances of visual states. In this paper, we\npropose PlanLLM, a cross-modal joint learning framework with LLMs for video\nprocedure planning. We propose an LLM-Enhanced Planning module which fully uses\nthe generalization ability of LLMs to produce free-form planning output and to\nenhance action step decoding. We also propose Mutual Information Maximization\nmodule to connect world-level commonsense of step descriptions and\nsample-specific information of visual states, enabling LLMs to employ the\nreasoning ability to generate step sequences. With the assistance of LLMs, our\nmethod can both closed-set and open vocabulary procedure planning tasks. Our\nPlanLLM achieves superior performance on three benchmarks, demonstrating the\neffectiveness of our designs.", "paper_summary_zh": "\u5f71\u7247\u7a0b\u5e8f\u898f\u5283\uff0c\u5373\u7d66\u5b9a\u958b\u59cb\u548c\u76ee\u6a19\u72c0\u614b\u7684\u5f71\u7247\u756b\u683c\uff0c\u898f\u5283\u4e00\u7cfb\u5217\u52d5\u4f5c\u6b65\u9a5f\uff0c\u662f\u5177\u8c61\u4eba\u5de5\u667a\u6167\u7684\u57fa\u672c\u80fd\u529b\u3002\u6700\u8fd1\u7684\u4f5c\u54c1\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u7522\u751f\u8c50\u5bcc\u7684\u52d5\u4f5c\u6b65\u9a5f\u63cf\u8ff0\u6587\u5b57\uff0c\u4ee5\u5f15\u5c0e\u52d5\u4f5c\u6b65\u9a5f\u89e3\u78bc\u3002\u5118\u7ba1\u5f15\u5165\u4e86 LLM\uff0c\u9019\u4e9b\u65b9\u6cd5\u6703\u5c07\u52d5\u4f5c\u6b65\u9a5f\u89e3\u78bc\u6210\u5c01\u9589\u96c6\u5408\u7684\u4e00\u71b1\u5411\u91cf\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5c0d\u65b0\u6b65\u9a5f\u6216\u4efb\u52d9\u9032\u884c\u6982\u62ec\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u57fa\u65bc\u4e16\u754c\u7d1a\u5e38\u7406\u7684\u56fa\u5b9a\u52d5\u4f5c\u6b65\u9a5f\u63cf\u8ff0\u53ef\u80fd\u5305\u542b\u7279\u5b9a\u8996\u89ba\u72c0\u614b\u5be6\u4f8b\u4e2d\u7684\u96dc\u8a0a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa PlanLLM\uff0c\u4e00\u7a2e\u5177\u6709 LLM \u7684\u8de8\u6a21\u614b\u806f\u5408\u5b78\u7fd2\u67b6\u69cb\uff0c\u7528\u65bc\u5f71\u7247\u7a0b\u5e8f\u898f\u5283\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b LLM \u589e\u5f37\u898f\u5283\u6a21\u7d44\uff0c\u5145\u5206\u5229\u7528 LLM \u7684\u6982\u62ec\u80fd\u529b\u4f86\u7522\u751f\u81ea\u7531\u5f62\u5f0f\u7684\u898f\u5283\u8f38\u51fa\uff0c\u4e26\u589e\u5f37\u52d5\u4f5c\u6b65\u9a5f\u89e3\u78bc\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e92\u8cc7\u8a0a\u6700\u5927\u5316\u6a21\u7d44\uff0c\u5c07\u6b65\u9a5f\u63cf\u8ff0\u7684\u4e16\u754c\u7d1a\u5e38\u7406\u8207\u8996\u89ba\u72c0\u614b\u7684\u6a23\u672c\u7279\u5b9a\u8cc7\u8a0a\u9023\u63a5\u8d77\u4f86\uff0c\u4f7f LLM \u80fd\u5920\u904b\u7528\u63a8\u7406\u80fd\u529b\u4f86\u7522\u751f\u6b65\u9a5f\u5e8f\u5217\u3002\u5728 LLM \u7684\u5354\u52a9\u4e0b\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u540c\u6642\u57f7\u884c\u5c01\u9589\u96c6\u5408\u548c\u958b\u653e\u8a5e\u5f59\u7a0b\u5e8f\u898f\u5283\u4efb\u52d9\u3002\u6211\u5011\u7684 PlanLLM \u5728\u4e09\u500b\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u512a\u7570\u7684\u6548\u80fd\uff0c\u8b49\u660e\u4e86\u6211\u5011\u8a2d\u8a08\u7684\u6709\u6548\u6027\u3002", "author": "Dejie Yang et.al.", "authors": "Dejie Yang, Zijing Zhao, YangLiu", "id": "2412.19139v1", "paper_url": "http://arxiv.org/abs/2412.19139v1", "repo": "https://github.com/idejie/planllm"}}