{"2502.06921": {"publish_time": "2025-02-10", "title": "GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units", "paper_summary": "Graph Neural Networks (GNNs) are vital for learning from graph-structured\ndata, enabling applications in network analysis, recommendation systems, and\nspeech analytics. Deploying them on edge devices like client PCs and laptops\nenhances real-time processing, privacy, and cloud independence. GNNs aid\nRetrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and\nenable event-based vision tasks. However, irregular memory access, sparsity,\nand dynamic structures cause high latency and energy overhead on\nresource-constrained devices. While modern edge processors integrate CPUs,\nGPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular\nGNN computations. We introduce GraNNite, the first hardware-aware framework\noptimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN\naccelerators via a structured three-step methodology: (1) enabling NPU\nexecution, (2) optimizing performance, and (3) trading accuracy for efficiency\ngains. Step 1 employs GraphSplit for workload distribution and StaGr for static\naggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts\nperformance using EffOp for control-heavy tasks and GraSp for sparsity\nexploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce\nredundancy and memory transfers. Step 3 balances quality versus efficiency,\nwhere QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate\nattention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,\nGraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to\n8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher\nperformance than CPUs and GPUs, respectively, across GNN models.", "paper_summary_zh": "\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u5c0d\u65bc\u5f9e\u5716\u5f62\u7d50\u69cb\u8cc7\u6599\u4e2d\u5b78\u7fd2\u81f3\u95dc\u91cd\u8981\uff0c\u80fd\u61c9\u7528\u65bc\u7db2\u8def\u5206\u6790\u3001\u63a8\u85a6\u7cfb\u7d71\u548c\u8a9e\u97f3\u5206\u6790\u3002\u5c07\u5176\u90e8\u7f72\u5728\u908a\u7de3\u88dd\u7f6e\uff08\u4f8b\u5982\u7528\u6236\u7aef\u96fb\u8166\u548c\u7b46\u96fb\uff09\u4e0a\u53ef\u589e\u5f37\u5373\u6642\u8655\u7406\u3001\u96b1\u79c1\u548c\u96f2\u7aef\u7368\u7acb\u6027\u3002GNN \u5354\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG)\uff0c\u4e26\u652f\u63f4\u57fa\u65bc\u4e8b\u4ef6\u7684\u8996\u89ba\u4efb\u52d9\u3002\u7136\u800c\uff0c\u4e0d\u898f\u5247\u7684\u8a18\u61b6\u9ad4\u5b58\u53d6\u3001\u7a00\u758f\u6027\u548c\u52d5\u614b\u7d50\u69cb\u6703\u5c0e\u81f4\u8cc7\u6e90\u53d7\u9650\u88dd\u7f6e\u4e0a\u7684\u9ad8\u5ef6\u9072\u548c\u80fd\u6e90\u8ca0\u64d4\u3002\u5118\u7ba1\u73fe\u4ee3\u908a\u7de3\u8655\u7406\u5668\u6574\u5408\u4e86 CPU\u3001GPU \u548c NPU\uff0c\u4f46\u91dd\u5c0d\u8cc7\u6599\u5e73\u884c\u4efb\u52d9\u6240\u8a2d\u8a08\u7684 NPU \u96e3\u4ee5\u8655\u7406\u4e0d\u898f\u5247\u7684 GNN \u8a08\u7b97\u3002\u6211\u5011\u5f15\u5165\u4e86 GraNNite\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u786c\u9ad4\u611f\u77e5\u6846\u67b6\uff0c\u900f\u904e\u7d50\u69cb\u5316\u7684\u4e09\u6b65\u9a5f\u65b9\u6cd5\u6700\u4f73\u5316\u5546\u7528\u73fe\u6210 (COTS) SOTA DNN \u52a0\u901f\u5668\u4e0a\u7684 GNN \u57f7\u884c\uff1a(1) \u555f\u7528 NPU \u57f7\u884c\uff0c(2) \u6700\u4f73\u5316\u6548\u80fd\uff0c\u4ee5\u53ca (3) \u4ee5\u6e96\u78ba\u5ea6\u63db\u53d6\u6548\u7387\u63d0\u5347\u3002\u6b65\u9a5f 1 \u4f7f\u7528 GraphSplit \u9032\u884c\u5de5\u4f5c\u8ca0\u8f09\u5206\u914d\uff0c\u4e26\u4f7f\u7528 StaGr \u9032\u884c\u975c\u614b\u805a\u5408\uff0c\u800c GrAd \u548c NodePad \u5247\u8655\u7406\u52d5\u614b\u5716\u5f62\u3002\u6b65\u9a5f 2 \u4f7f\u7528 EffOp \u63d0\u5347\u63a7\u5236\u5bc6\u96c6\u578b\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u4e26\u4f7f\u7528 GraSp \u9032\u884c\u7a00\u758f\u6027\u5229\u7528\u3002\u5716\u5f62\u5377\u7a4d\u6700\u4f73\u5316 PreG\u3001SymG \u548c CacheG \u6e1b\u5c11\u4e86\u5197\u9918\u548c\u8a18\u61b6\u9ad4\u50b3\u8f38\u3002\u6b65\u9a5f 3 \u5e73\u8861\u54c1\u8cea\u8207\u6548\u7387\uff0c\u5176\u4e2d QuantGr \u9069\u7528 INT8 \u91cf\u5316\uff0c\u800c GrAx1\u3001GrAx2 \u548c GrAx3 \u5247\u52a0\u901f\u6ce8\u610f\u529b\u3001\u5ee3\u64ad\u52a0\u6cd5\u548c SAGE-max \u805a\u5408\u3002\u5728 Intel Core Ultra AI PC \u4e0a\uff0cGraNNite \u5728\u9810\u8a2d NPU \u6620\u5c04\u4e0a\u5be6\u73fe\u4e86 2.6X \u5230 7.6X \u7684\u52a0\u901f\uff0c\u5728 CPU \u548c GPU \u4e0a\u5be6\u73fe\u4e86\u9ad8\u9054 8.6X \u7684\u80fd\u6e90\u589e\u76ca\uff0c\u5728 GNN \u6a21\u578b\u4e2d\u5206\u5225\u63d0\u4f9b\u4e86\u6bd4 CPU \u548c GPU \u9ad8\u51fa 10.8X \u548c 6.7X \u7684\u6548\u80fd\u3002", "author": "Arghadip Das et.al.", "authors": "Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan", "id": "2502.06921v2", "paper_url": "http://arxiv.org/abs/2502.06921v2", "repo": "null"}}