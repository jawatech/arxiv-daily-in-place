{"2408.05097": {"publish_time": "2024-08-09", "title": "Hyperbolic Learning with Multimodal Large Language Models", "paper_summary": "Hyperbolic embeddings have demonstrated their effectiveness in capturing\nmeasures of uncertainty and hierarchical relationships across various\ndeep-learning tasks, including image segmentation and active learning. However,\ntheir application in modern vision-language models (VLMs) has been limited. A\nnotable exception is MERU, which leverages the hierarchical properties of\nhyperbolic space in the CLIP ViT-large model, consisting of hundreds of\nmillions parameters. In our work, we address the challenges of scaling\nmulti-modal hyperbolic models by orders of magnitude in terms of parameters\n(billions) and training complexity using the BLIP-2 architecture. Although\nhyperbolic embeddings offer potential insights into uncertainty not present in\nEuclidean embeddings, our analysis reveals that scaling these models is\nparticularly difficult. We propose a novel training strategy for a hyperbolic\nversion of BLIP-2, which allows to achieve comparable performance to its\nEuclidean counterpart, while maintaining stability throughout the training\nprocess and showing a meaningful indication of uncertainty with each embedding.", "paper_summary_zh": "\u96d9\u66f2\u5d4c\u5165\u5df2\u8b49\u660e\u5176\u5728\u6355\u6349\u5404\u7a2e\u6df1\u5ea6\u5b78\u7fd2\u4efb\u52d9\u4e2d\u7684\u4e0d\u78ba\u5b9a\u6027\u548c\u5c64\u7d1a\u95dc\u4fc2\u7684\u6e2c\u91cf\u503c\uff08\u5305\u62ec\u5f71\u50cf\u5206\u5272\u548c\u4e3b\u52d5\u5b78\u7fd2\uff09\u65b9\u9762\u6709\u6548\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u73fe\u4ee3\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u61c9\u7528\u53d7\u5230\u9650\u5236\u3002\u4e00\u500b\u503c\u5f97\u6ce8\u610f\u7684\u4f8b\u5916\u662f MERU\uff0c\u5b83\u5229\u7528\u4e86 CLIP ViT-large \u6a21\u578b\u4e2d\u96d9\u66f2\u7a7a\u9593\u7684\u5c64\u7d1a\u5c6c\u6027\uff0c\u8a72\u6a21\u578b\u5305\u542b\u6578\u5104\u500b\u53c3\u6578\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e BLIP-2 \u67b6\u69cb\u89e3\u6c7a\u4e86\u5728\u53c3\u6578\uff08\u6578\u5341\u5104\uff09\u548c\u8a13\u7df4\u8907\u96dc\u6027\u65b9\u9762\u64f4\u5c55\u591a\u6a21\u614b\u96d9\u66f2\u6a21\u578b\u7684\u6311\u6230\u3002\u5118\u7ba1\u96d9\u66f2\u5d4c\u5165\u63d0\u4f9b\u4e86\u6b50\u5e7e\u91cc\u5f97\u5d4c\u5165\u4e2d\u4e0d\u5b58\u5728\u7684\u4e0d\u78ba\u5b9a\u6027\u6f5b\u5728\u898b\u89e3\uff0c\u4f46\u6211\u5011\u7684\u5206\u6790\u8868\u660e\uff0c\u64f4\u5c55\u9019\u4e9b\u6a21\u578b\u7279\u5225\u56f0\u96e3\u3002\u6211\u5011\u70ba BLIP-2 \u7684\u96d9\u66f2\u7248\u672c\u63d0\u51fa\u4e86\u5275\u65b0\u7684\u8a13\u7df4\u7b56\u7565\uff0c\u8a72\u7b56\u7565\u5141\u8a31\u5be6\u73fe\u8207\u5176\u6b50\u5e7e\u91cc\u5f97\u5c0d\u61c9\u9805\u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u4fdd\u6301\u7a69\u5b9a\u6027\uff0c\u4e26\u986f\u793a\u6bcf\u500b\u5d4c\u5165\u7684\u4e0d\u78ba\u5b9a\u6027\u6709\u610f\u7fa9\u7684\u6307\u6a19\u3002", "author": "Paolo Mandica et.al.", "authors": "Paolo Mandica, Luca Franco, Konstantinos Kallidromitis, Suzanne Petryk, Fabio Galasso", "id": "2408.05097v1", "paper_url": "http://arxiv.org/abs/2408.05097v1", "repo": "null"}}