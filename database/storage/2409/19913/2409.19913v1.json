{"2409.19913": {"publish_time": "2024-09-30", "title": "Scaling Optimal LR Across Token Horizon", "paper_summary": "State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via our\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.", "paper_summary_zh": "\u6700\u5148\u8fdb\u7684 LLM \u7531\u4ee5\u4e0b\u65b9\u5f0f\u63d0\u4f9b\u652f\u6301\uff1a\u6269\u5c55\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u96c6\u7fa4\u5927\u5c0f\u3002\u5e7f\u6cdb\u8c03\u6574\u8d85\u53c2\u6570\u4ee5\u8fdb\u884c\u6700\u5927\u8fd0\u884c\u5728\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u884c\u3002\u76f8\u53cd\uff0c\u5fc5\u987b\u4ece\u8f83\u5c0f\u7684\u5b9e\u9a8c\u4e2d\u63a8\u65ad\u6216\u201c\u4f20\u8f93\u201d\u8fd1\u4f3c\u6700\u4f18\u8d85\u53c2\u6570\u3002Yang \u7b49\u4eba\u7814\u7a76\u4e86\u8de8\u6a21\u578b\u5927\u5c0f\u7684\u8d85\u53c2\u6570\u4f20\u8f93\u3002\u7136\u800c\uff0c\u5c1a\u672a\u7814\u7a76\u8de8\u6570\u636e\u96c6\u5927\u5c0f\uff08\u6216\u4ee4\u724c\u8303\u56f4\uff09\u7684\u8d85\u53c2\u6570\u4f20\u8f93\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u6700\u4f18\u5b66\u4e60\u7387 (LR) \u5982\u4f55\u4f9d\u8d56 LLM \u8bad\u7ec3\u4e2d\u7684\u4ee4\u724c\u8303\u56f4\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\u6700\u4f18 LR \u968f\u4ee4\u724c\u8303\u56f4\u800c\u663e\u7740\u53d8\u5316\u2014\u2014\u66f4\u957f\u7684\u8bad\u7ec3\u9700\u8981\u66f4\u5c0f\u7684 LR\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bc1\u660e\u6700\u4f18 LR \u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u6211\u4eec\u7684\u7f29\u653e\u5b9a\u5f8b\u4ece\u8f83\u77ed\u7684\u8303\u56f4\u51c6\u786e\u4f30\u8ba1\u8f83\u957f\u8303\u56f4\u7684\u6700\u4f18 LR\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u6cd5\u5219\uff0c\u7528\u4e8e\u5728\u4ee4\u724c\u8303\u56f4\u4e4b\u95f4\u4f20\u8f93 LR\uff0c\u800c\u65e0\u9700\u5bf9\u5f53\u524d\u5b9e\u8df5\u8fdb\u884c\u4efb\u4f55\u5f00\u9500\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u8bc1\u636e\u8868\u660e LLama-1 \u4f7f\u7528\u4e86\u8fc7\u9ad8\u7684 LR\uff0c\u5e76\u4f30\u8ba1\u4e86\u7531\u6b64\u9020\u6210\u7684\u6027\u80fd\u635f\u5931\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8ba4\u4e3a\u8de8\u6570\u636e\u5927\u5c0f\u7684\u8d85\u53c2\u6570\u4f20\u8f93\u662f LLM \u8bad\u7ec3\u7684\u4e00\u4e2a\u91cd\u8981\u4e14\u88ab\u5ffd\u89c6\u7684\u7ec4\u6210\u90e8\u5206\u3002", "author": "Johan Bjorck et.al.", "authors": "Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song", "id": "2409.19913v1", "paper_url": "http://arxiv.org/abs/2409.19913v1", "repo": "null"}}