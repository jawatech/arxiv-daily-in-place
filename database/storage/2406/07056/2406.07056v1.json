{"2406.07056": {"publish_time": "2024-06-11", "title": "Effectively Compress KV Heads for LLM", "paper_summary": "The advent of pre-trained large language models (LLMs) has revolutionized\nvarious natural language processing tasks. These models predominantly employ an\nauto-regressive decoding mechanism that utilizes Key-Value (KV) caches to\neliminate redundant calculations for previous tokens. Nevertheless, as context\nlengths and batch sizes increase, the linear expansion in memory footprint of\nKV caches becomes a key bottleneck of LLM deployment, which decreases\ngeneration speeds significantly. To mitigate this issue, previous techniques\nlike multi-query attention (MQA) and grouped-query attention (GQA) have been\ndeveloped, in order to reduce KV heads to accelerate inference with comparable\naccuracy to multi-head attention (MHA). Despite their effectiveness, existing\nstrategies for compressing MHA often overlook the intrinsic properties of the\nKV caches. In this work, we explore the low-rank characteristics of the KV\ncaches and propose a novel approach for compressing KV heads. In particular, we\ncarefully optimize the MHA-to-GQA transformation to minimize compression error,\nand to remain compatible with rotary position embeddings (RoPE), we also\nintroduce specialized strategies for key caches with RoPE. We demonstrate that\nour method can compress half or even three-quarters of KV heads while\nmaintaining performance comparable to the original LLMs, which presents a\npromising direction for more efficient LLM deployment in resource-constrained\nenvironments.", "paper_summary_zh": "\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u3002\u9019\u4e9b\u6a21\u578b\u4e3b\u8981\u63a1\u7528\u81ea\u8ff4\u6b78\u89e3\u78bc\u6a5f\u5236\uff0c\u5229\u7528\u9375\u503c (KV) \u5feb\u53d6\u4f86\u6d88\u9664\u5148\u524d\u7b26\u865f\u7684\u91cd\u8907\u8a08\u7b97\u3002\u7136\u800c\uff0c\u96a8\u8457\u5167\u5bb9\u9577\u5ea6\u548c\u6279\u6b21\u5927\u5c0f\u7684\u589e\u52a0\uff0cKV \u5feb\u53d6\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7dda\u6027\u64f4\u5145\uff0c\u6210\u70ba LLM \u90e8\u7f72\u7684\u4e3b\u8981\u74f6\u9838\uff0c\u5927\u5e45\u964d\u4f4e\u7522\u751f\u901f\u5ea6\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u5df2\u958b\u767c\u51fa\u591a\u91cd\u67e5\u8a62\u6ce8\u610f\u529b (MQA) \u548c\u7fa4\u7d44\u67e5\u8a62\u6ce8\u610f\u529b (GQA) \u7b49\u5148\u524d\u7684\u6280\u8853\uff0c\u4ee5\u6e1b\u5c11 KV \u982d\u90e8\uff0c\u85c9\u6b64\u52a0\u901f\u63a8\u8ad6\uff0c\u540c\u6642\u7dad\u6301\u8207\u591a\u982d\u6ce8\u610f\u529b (MHA) \u76f8\u7576\u7684\u6e96\u78ba\u5ea6\u3002\u5118\u7ba1\u9019\u4e9b\u7b56\u7565\u6709\u6548\uff0c\u73fe\u6709\u7684 MHA \u58d3\u7e2e\u7b56\u7565\u5f80\u5f80\u5ffd\u7565 KV \u5feb\u53d6\u7684\u5167\u5728\u7279\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e KV \u5feb\u53d6\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u4e26\u63d0\u51fa\u4e00\u7a2e\u58d3\u7e2e KV \u982d\u90e8\u7684\u5275\u65b0\u65b9\u6cd5\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u4ed4\u7d30\u6700\u4f73\u5316 MHA \u5230 GQA \u7684\u8f49\u63db\uff0c\u4ee5\u6700\u5c0f\u5316\u58d3\u7e2e\u8aa4\u5dee\uff0c\u4e26\u7dad\u6301\u8207\u65cb\u8f49\u4f4d\u7f6e\u5d4c\u5165 (RoPE) \u7684\u76f8\u5bb9\u6027\uff0c\u6211\u5011\u4e5f\u91dd\u5c0d\u4f7f\u7528 RoPE \u7684\u91d1\u9470\u5feb\u53d6\uff0c\u5f15\u9032\u5c08\u9580\u7684\u7b56\u7565\u3002\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u7dad\u6301\u8207\u539f\u59cb LLM \u76f8\u7576\u7684\u6548\u80fd\u4e0b\uff0c\u58d3\u7e2e\u4e00\u534a\u751a\u81f3\u56db\u5206\u4e4b\u4e09\u7684 KV \u982d\u90e8\uff0c\u9019\u70ba\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u66f4\u6709\u6548\u7387\u5730\u90e8\u7f72 LLM\uff0c\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "author": "Hao Yu et.al.", "authors": "Hao Yu, Zelan Yang, Shen Li, Yong Li, Jianxin Wu", "id": "2406.07056v1", "paper_url": "http://arxiv.org/abs/2406.07056v1", "repo": "null"}}