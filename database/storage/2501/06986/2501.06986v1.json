{"2501.06986": {"publish_time": "2025-01-13", "title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models", "paper_summary": "Enhanced visual understanding serves as a cornerstone for multimodal large\nlanguage models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision\nexperts to address the limitations of using a single vision encoder and\nexcessively long visual tokens. Despite the progress of these MLLMs, a research\ngap remains in effectively integrating diverse vision encoders. This work\nexplores fusion strategies of visual tokens for hybrid MLLMs, leading to the\ndesign of LEO, a novel MLLM with a dual-branch vision encoder framework that\nincorporates a post-adaptation fusion strategy and adaptive tiling: for each\nsegmented tile of the input images, LEO sequentially interleaves the visual\ntokens from its two vision encoders. Extensive evaluation across 13\nvision-language benchmarks reveals that LEO outperforms state-of-the-art\nopen-source MLLMs and hybrid MLLMs on the majority of tasks. Furthermore, we\nshow that LEO can be adapted to the specialized domain of autonomous driving\nwithout altering the model architecture or training recipe, achieving\ncompetitive performance compared to existing baselines. The code and model will\nbe publicly available.", "paper_summary_zh": "\u589e\u5f37\u7684\u8996\u89ba\u7406\u89e3\u662f\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u57fa\u77f3\u3002\u6700\u8fd1\u7684\u6df7\u5408 MLLM \u7d50\u5408\u4e86\u8996\u89ba\u5c08\u5bb6\u7684\u6df7\u5408\uff0c\u4ee5\u89e3\u6c7a\u4f7f\u7528\u55ae\u4e00\u8996\u89ba\u7de8\u78bc\u5668\u548c\u904e\u9577\u7684\u8996\u89ba\u7b26\u865f\u7684\u9650\u5236\u3002\u5118\u7ba1\u9019\u4e9b MLLM \u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u6709\u6548\u6574\u5408\u4e0d\u540c\u7684\u8996\u89ba\u7de8\u78bc\u5668\u4ecd\u7136\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002\u9019\u9805\u5de5\u4f5c\u63a2\u8a0e\u4e86\u6df7\u5408 MLLM \u7684\u8996\u89ba\u7b26\u865f\u878d\u5408\u7b56\u7565\uff0c\u5f9e\u800c\u8a2d\u8a08\u51fa LEO\uff0c\u4e00\u7a2e\u5177\u6709\u96d9\u5206\u652f\u8996\u89ba\u7de8\u78bc\u5668\u6846\u67b6\u7684\u65b0\u578b MLLM\uff0c\u5b83\u7d50\u5408\u4e86\u5f8c\u9069\u61c9\u878d\u5408\u7b56\u7565\u548c\u81ea\u9069\u61c9\u5e73\u92ea\uff1a\u5c0d\u65bc\u8f38\u5165\u5f71\u50cf\u7684\u6bcf\u500b\u5206\u5272\u5e73\u92ea\uff0cLEO \u4f9d\u5e8f\u4ea4\u7e54\u5176\u5169\u500b\u8996\u89ba\u7de8\u78bc\u5668\u7684\u8996\u89ba\u7b26\u865f\u3002\u5728 13 \u500b\u8996\u89ba\u8a9e\u8a00\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u8a55\u4f30\u986f\u793a\uff0cLEO \u5728\u5927\u591a\u6578\u4efb\u52d9\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u958b\u6e90 MLLM \u548c\u6df7\u5408 MLLM\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86 LEO \u53ef\u4ee5\u9069\u61c9\u81ea\u52d5\u99d5\u99db\u7684\u5c08\u696d\u9818\u57df\uff0c\u800c\u7121\u9700\u6539\u8b8a\u6a21\u578b\u67b6\u69cb\u6216\u8a13\u7df4\u914d\u65b9\uff0c\u8207\u73fe\u6709\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u5be6\u73fe\u4e86\u7af6\u722d\u529b\u3002\u4ee3\u78bc\u548c\u6a21\u578b\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Mozhgan Nasr Azadani et.al.", "authors": "Mozhgan Nasr Azadani, James Riddell, Sean Sedwards, Krzysztof Czarnecki", "id": "2501.06986v1", "paper_url": "http://arxiv.org/abs/2501.06986v1", "repo": "https://github.com/mozhgan91/leo"}}