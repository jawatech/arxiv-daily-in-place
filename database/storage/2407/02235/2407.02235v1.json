{"2407.02235": {"publish_time": "2024-07-02", "title": "Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation", "paper_summary": "Multi-modal large language models (MLLMs) have been given free rein to\nexplore exciting medical applications with a primary focus on radiology report\ngeneration. Nevertheless, the preliminary success in 2D radiology captioning is\nincompetent to reflect the real-world diagnostic challenge in the volumetric 3D\nanatomy. To mitigate three crucial limitation aspects in the existing\nliterature, including (1) data complexity, (2) model capacity, and (3)\nevaluation metric fidelity, we collected an 18,885 text-scan pairs 3D-BrainCT\ndataset and applied clinical visual instruction tuning (CVIT) to train BrainGPT\nmodels to generate radiology-adherent 3D brain CT reports. Statistically, our\nBrainGPT scored BLEU-1 = 44.35, BLEU-4 = 20.38, METEOR = 30.13, ROUGE-L = 47.6,\nand CIDEr-R = 211.77 during internal testing and demonstrated an accuracy of\n0.91 in captioning midline shifts on the external validation CQ500 dataset. By\nfurther inspecting the captioned report, we reported that the traditional\nmetrics appeared to measure only the surface text similarity and failed to\ngauge the information density of the diagnostic purpose. To close this gap, we\nproposed a novel Feature-Oriented Radiology Task Evaluation (FORTE) to estimate\nthe report's clinical relevance (lesion feature and landmarks). Notably, the\nBrainGPT model scored an average FORTE F1-score of 0.71 (degree=0.661;\nlandmark=0.706; feature=0.693; impression=0.779). To demonstrate that BrainGPT\nmodels possess objective readiness to generate human-like radiology reports, we\nconducted a Turing test that enrolled 11 physician evaluators, and around 74%\nof the BrainGPT-generated captions were indistinguishable from those written by\nhumans. Our work embodies a holistic framework that showcased the first-hand\nexperience of curating a 3D brain CT dataset, fine-tuning anatomy-sensible\nlanguage models, and proposing robust radiology evaluation metrics.", "paper_summary_zh": "<paragraph>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u83b7\u5f97\u81ea\u7531\u63a2\u7d22\u4ee4\u4eba\u5174\u594b\u7684\u533b\u5b66\u5e94\u7528\uff0c\u91cd\u70b9\u653e\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e0a\u3002\u7136\u800c\uff0c2D \u653e\u5c04\u5b66\u6807\u9898\u7684\u521d\u6b65\u6210\u529f\u4e0d\u8db3\u4ee5\u53cd\u6620\u4f53\u79ef 3D \u89e3\u5256\u4e2d\u7684\u771f\u5b9e\u4e16\u754c\u8bca\u65ad\u6311\u6218\u3002\u4e3a\u4e86\u51cf\u8f7b\u73b0\u6709\u6587\u732e\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u9650\u5236\u65b9\u9762\uff0c\u5305\u62ec (1) \u6570\u636e\u590d\u6742\u6027\u3001(2) \u6a21\u578b\u5bb9\u91cf\u548c (3) \u8bc4\u4f30\u6307\u6807\u4fdd\u771f\u5ea6\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b 18,885 \u4e2a\u6587\u672c\u626b\u63cf\u5bf9\u7684 3D-BrainCT \u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528\u4e34\u5e8a\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18 (CVIT) \u6765\u8bad\u7ec3 BrainGPT \u6a21\u578b\u4ee5\u751f\u6210\u7b26\u5408\u653e\u5c04\u5b66\u7684 3D \u8111 CT \u62a5\u544a\u3002\u4ece\u7edf\u8ba1\u5b66\u4e0a\u8bb2\uff0c\u6211\u4eec\u7684 BrainGPT \u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86 BLEU-1 = 44.35\u3001BLEU-4 = 20.38\u3001METEOR = 30.13\u3001ROUGE-L = 47.6 \u548c CIDEr-R = 211.77 \u7684\u5206\u6570\uff0c\u5e76\u5728\u5916\u90e8\u9a8c\u8bc1 CQ500 \u6570\u636e\u96c6\u4e0a\u5bf9\u4e2d\u7ebf\u504f\u79fb\u7684\u6807\u9898\u663e\u793a\u51fa 0.91 \u7684\u51c6\u786e\u5ea6\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u68c0\u67e5\u6807\u9898\u62a5\u544a\uff0c\u6211\u4eec\u62a5\u544a\u8bf4\u4f20\u7edf\u6307\u6807\u4f3c\u4e4e\u53ea\u6d4b\u91cf\u4e86\u8868\u9762\u6587\u672c\u76f8\u4f3c\u6027\uff0c\u800c\u672a\u80fd\u8861\u91cf\u8bca\u65ad\u76ee\u7684\u7684\u4fe1\u606f\u5bc6\u5ea6\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9762\u5411\u7279\u5f81\u7684\u653e\u5c04\u5b66\u4efb\u52a1\u8bc4\u4f30 (FORTE) \u6765\u4f30\u8ba1\u62a5\u544a\u7684\u4e34\u5e8a\u76f8\u5173\u6027\uff08\u75c5\u53d8\u7279\u5f81\u548c\u5730\u6807\uff09\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cBrainGPT \u6a21\u578b\u7684\u5e73\u5747 FORTE F1 \u5206\u6570\u4e3a 0.71\uff08\u7a0b\u5ea6=0.661\uff1b\u5730\u6807=0.706\uff1b\u7279\u5f81=0.693\uff1b\u5370\u8c61=0.779\uff09\u3002\u4e3a\u4e86\u8bc1\u660e BrainGPT \u6a21\u578b\u5177\u5907\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u653e\u5c04\u5b66\u62a5\u544a\u7684\u5ba2\u89c2\u51c6\u5907\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u56fe\u7075\u6d4b\u8bd5\uff0c\u62db\u52df\u4e86 11 \u4f4d\u533b\u751f\u8bc4\u4f30\u5458\uff0c\u5927\u7ea6 74% \u7684 BrainGPT \u751f\u6210\u7684\u6807\u9898\u4e0e\u4eba\u7c7b\u5199\u7684\u6807\u9898\u65e0\u6cd5\u533a\u5206\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4f53\u73b0\u4e86\u4e00\u4e2a\u6574\u4f53\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u7b56\u5212 3D \u8111 CT \u6570\u636e\u96c6\u3001\u5fae\u8c03\u5bf9\u89e3\u5256\u5b66\u654f\u611f\u7684\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u63d0\u51fa\u7a33\u5065\u7684\u653e\u5c04\u5b66\u8bc4\u4f30\u6307\u6807\u7684\u76f4\u63a5\u7ecf\u9a8c\u3002</paragraph>", "author": "Cheng-Yi Li et.al.", "authors": "Cheng-Yi Li, Kao-Jung Chang, Cheng-Fu Yang, Hsin-Yu Wu, Wenting Chen, Hritik Bansal, Ling Chen, Yi-Ping Yang, Yu-Chun Chen, Shih-Pin Chen, Jiing-Feng Lirng, Kai-Wei Chang, Shih-Hwa Chiou", "id": "2407.02235v1", "paper_url": "http://arxiv.org/abs/2407.02235v1", "repo": "https://github.com/charlierabea/FORTE"}}