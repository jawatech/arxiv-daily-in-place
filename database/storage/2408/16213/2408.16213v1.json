{"2408.16213": {"publish_time": "2024-08-29", "title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation", "paper_summary": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.", "paper_summary_zh": "\u4eba\u5de5\u667a\u6167\u7684\u5feb\u901f\u767c\u5c55\uff0c\u7279\u5225\u662f\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\uff0c\u5df2\u5c0d\u5305\u62ec\u91ab\u7642\u4fdd\u5065\u5728\u5167\u7684\u5404\u500b\u9818\u57df\u7522\u751f\u91cd\u5927\u5f71\u97ff\u3002\u5728\u80f8\u90e8 X \u5149 (CXR) \u5206\u6790\u4e2d\uff0c\u5148\u524d\u7684\u7814\u7a76\u5df2\u63a1\u7528 LLM\uff0c\u4f46\u6709\u5176\u9650\u5236\uff1a\u4e0d\u662f\u672a\u80fd\u5145\u5206\u5229\u7528 LLM \u7684\u591a\u4efb\u52d9\u8655\u7406\u80fd\u529b\uff0c\u5c31\u662f\u7f3a\u4e4f\u81e8\u5e8a\u6e96\u78ba\u6027\u3002\u672c\u6587\u63d0\u51fa M4CXR\uff0c\u4e00\u7a2e\u591a\u6a21\u614b LLM\uff0c\u65e8\u5728\u589e\u5f37 CXR \u89e3\u91cb\u3002\u8a72\u6a21\u578b\u8a13\u7df4\u65bc\u8996\u89ba\u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u4ee5\u5c0d\u8a71\u683c\u5f0f\u6574\u5408\u5404\u7a2e\u7279\u5b9a\u4efb\u52d9\u8cc7\u6599\u96c6\u3002\u56e0\u6b64\uff0c\u8a72\u6a21\u578b\u652f\u63f4\u591a\u9805\u4efb\u52d9\uff0c\u4f8b\u5982\u91ab\u7642\u5831\u544a\u7522\u751f (MRG)\u3001\u8996\u89ba\u57fa\u790e\u548c\u8996\u89ba\u554f\u984c\u56de\u7b54 (VQA)\u3002M4CXR \u900f\u904e\u63a1\u7528\u601d\u8003\u93c8\u63d0\u793a\u7b56\u7565\uff0c\u5728 MRG \u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u81e8\u5e8a\u6e96\u78ba\u6027\uff0c\u5176\u4e2d\u5b83\u6703\u8b58\u5225 CXR \u5f71\u50cf\u4e2d\u7684\u767c\u73fe\uff0c\u4e26\u96a8\u5f8c\u7522\u751f\u5c0d\u61c9\u7684\u5831\u544a\u3002\u8a72\u6a21\u578b\u53ef\u6839\u64da\u53ef\u7528\u8f38\u5165\uff08\u4f8b\u5982\u55ae\u4e00\u5f71\u50cf\u3001\u591a\u91cd\u5f71\u50cf\u548c\u591a\u91cd\u7814\u7a76\u8108\u7d61\uff09\u9069\u61c9\u5404\u7a2e MRG \u60c5\u5883\u3002\u9664\u4e86 MRG \u4e4b\u5916\uff0cM4CXR \u4ee5\u8207\u5c08\u9580\u6a21\u578b\u76f8\u7576\u7684\u5c64\u7d1a\u57f7\u884c\u8996\u89ba\u57fa\u790e\uff0c\u4e26\u5728 VQA \u4e2d\u5c55\u73fe\u51fa\u8272\u7684\u6548\u80fd\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u8a55\u4f30\u5747\u986f\u793a\u51fa M4CXR \u5728 MRG\u3001\u8996\u89ba\u57fa\u790e\u548c VQA \u4e2d\u7684\u591a\u529f\u80fd\u6027\uff0c\u540c\u6642\u6301\u7e8c\u7dad\u6301\u81e8\u5e8a\u6e96\u78ba\u6027\u3002", "author": "Jonggwon Park et.al.", "authors": "Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi", "id": "2408.16213v1", "paper_url": "http://arxiv.org/abs/2408.16213v1", "repo": "null"}}