{"2410.12662": {"publish_time": "2024-10-16", "title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models", "paper_summary": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u6210\u529f\u4f7f LLM \u80fd\u591f\u7406\u89e3\u89c6\u89c9\u8f93\u5165\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u5c06 LLM \u4e2d\u6587\u672c\u7684\u73b0\u6709\u5b89\u5168\u673a\u5236\u8f6c\u79fb\u5230\u89c6\u89c9\u4e2d\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6709\u6bd2\u56fe\u50cf\u4e2d\u7684\u6f0f\u6d1e\u3002\u4e3a\u4e86\u63a2\u7d22\u8fd9\u4e2a\u95ee\u9898\u7684\u539f\u56e0\uff0c\u6211\u4eec\u5bf9 LVLMs \u7684\u5b89\u5168\u673a\u5236\u5728\u4f55\u5904\u4ee5\u53ca\u5982\u4f55\u64cd\u4f5c\u7ed9\u51fa\u4e86\u6df1\u523b\u7684\u89e3\u91ca\uff0c\u5e76\u5728\u6587\u672c\u548c\u89c6\u89c9\u4e4b\u95f4\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u7279\u5b9a transformer \u5c42\u7684\u9690\u85cf\u72b6\u6001\u5728\u5b89\u5168\u673a\u5236\u7684\u6210\u529f\u6fc0\u6d3b\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u800c\u5f53\u524d\u65b9\u6cd5\u5728\u9690\u85cf\u72b6\u6001\u7ea7\u522b\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u8fd8\u4e0d\u591f\u3002\u8fd9\u5bfc\u81f4\u9690\u85cf\u72b6\u6001\u4e2d\u8f93\u5165\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u8bed\u4e49\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u56e0\u6b64\u8bef\u5bfc\u4e86\u5b89\u5168\u673a\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4e3a LVLMs \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u6307\u5bfc\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5 (TGA)\u3002TGA \u68c0\u7d22\u4e0e\u8f93\u5165\u89c6\u89c9\u76f8\u5173\u7684\u6587\u672c\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u6765\u6307\u5bfc\u89c6\u89c9\u5728 LLM \u4e2d\u6295\u5f71\u5230\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTGA \u4e0d\u4ec5\u6210\u529f\u5730\u5c06\u57fa\u672c LLM \u4e2d\u6587\u672c\u7684\u5b89\u5168\u673a\u5236\u8f6c\u79fb\u5230 LVLMs \u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u4e2d\u7684\u89c6\u89c9\u4e2d\uff0c\u800c\u65e0\u9700\u5bf9\u89c6\u89c9\u65b9\u5f0f\u8fdb\u884c\u4efb\u4f55\u5b89\u5168\u5fae\u8c03\uff0c\u800c\u4e14\u8fd8\u4fdd\u6301\u4e86\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\uff08\u5b89\u5168\u4e14\u826f\u597d\uff09\u7684\u603b\u4f53\u6027\u80fd\u3002", "author": "Shicheng Xu et.al.", "authors": "Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng", "id": "2410.12662v1", "paper_url": "http://arxiv.org/abs/2410.12662v1", "repo": "null"}}