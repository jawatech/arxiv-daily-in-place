{"2503.05696": {"publish_time": "2025-03-07", "title": "Multi-Fidelity Policy Gradient Algorithms", "paper_summary": "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.", "paper_summary_zh": "\u8a31\u591a\u5f37\u5316\u5b78\u7fd2 (RL) \u6f14\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u7684\u6578\u64da\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u7121\u6cd5\u7528\u65bc\u8207\u64cd\u4f5c\u7cfb\u7d71\u983b\u7e41\u4ea4\u4e92\u4e0d\u53ef\u884c\uff0c\u6216\u8005\u9ad8\u4fdd\u771f\u6a21\u64ec\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u53ef\u7528\u7684\u61c9\u7528\u7a0b\u5f0f\u4e2d\u3002\u540c\u6642\uff0c\u4f4e\u4fdd\u771f\u6a21\u64ec\u5668\u2014\u2014\u4f8b\u5982\u964d\u968e\u6a21\u578b\u3001\u555f\u767c\u5f0f\u734e\u52f5\u51fd\u6578\u6216\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u2014\u2014\u53ef\u4ee5\u5ec9\u50f9\u5730\u63d0\u4f9b\u6709\u7528\u7684 RL \u8a13\u7df4\u6578\u64da\uff0c\u5373\u4f7f\u5b83\u5011\u5c0d\u65bc\u76f4\u63a5\u7684\u6a21\u64ec\u5230\u73fe\u5be6\u7684\u9077\u79fb\u4f86\u8aaa\u904e\u65bc\u7c97\u7cd9\u3002\u6211\u5011\u63d0\u51fa\u4e86\u591a\u4fdd\u771f\u5ea6\u7b56\u7565\u68af\u5ea6 (MFPG)\uff0c\u9019\u662f\u4e00\u500b RL \u6846\u67b6\uff0c\u5b83\u5c07\u4f86\u81ea\u76ee\u6a19\u74b0\u5883\u7684\u5c11\u91cf\u6578\u64da\u8207\u5927\u91cf\u7684\u4f4e\u4fdd\u771f\u6a21\u64ec\u6578\u64da\u6df7\u5408\uff0c\u4ee5\u5f62\u6210\u7528\u65bc\u7b56\u7565\u68af\u5ea6\u7684\u7121\u504f\u5dee\u3001\u4f4e\u65b9\u5dee\u4f30\u8a08\u5668\uff08\u63a7\u5236\u8b8a\u91cf\uff09\u3002\u6211\u5011\u901a\u904e\u958b\u767c\u5169\u7a2e\u7b56\u7565\u68af\u5ea6\u6f14\u7b97\u6cd5\u7684\u591a\u4fdd\u771f\u5ea6\u8b8a\u9ad4\u4f86\u5be6\u4f8b\u5316\u8a72\u6846\u67b6\uff1aREINFORCE \u548c\u8fd1\u7aef\u7b56\u7565\u512a\u5316\u3002\u5728\u4e00\u7cfb\u5217\u6a21\u64ec\u6a5f\u5668\u4eba\u57fa\u6e96\u554f\u984c\u4e2d\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u7576\u76ee\u6a19\u74b0\u5883\u6a23\u672c\u6709\u9650\u6642\uff0c\u8207\u50c5\u4f7f\u7528\u9ad8\u4fdd\u771f\u5ea6\u6578\u64da\u7684\u57fa\u6e96\u76f8\u6bd4\uff0cMFPG \u53ef\u5be6\u73fe\u9ad8\u9054 3.9 \u500d\u7684\u734e\u52f5\uff0c\u4e26\u63d0\u9ad8\u8a13\u7df4\u7a69\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u57fa\u6e96\u7372\u5f97\u66f4\u591a\u7684\u9ad8\u4fdd\u771f\u5ea6\u6a23\u672c\u2014\u2014\u8207\u76ee\u6a19\u74b0\u5883\u7684\u4ea4\u4e92\u6b21\u6578\u591a\u9054 10 \u500d\u2014\u2014MFPG \u4ecd\u7136\u80fd\u5920\u9054\u5230\u6216\u8d85\u904e\u5b83\u5011\u7684\u6027\u80fd\u3002\u6700\u5f8c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5373\u4f7f\u4f4e\u4fdd\u771f\u5ea6\u74b0\u5883\u8207\u76ee\u6a19\u74b0\u5883\u622a\u7136\u4e0d\u540c\uff0cMFPG \u4e5f\u80fd\u5920\u8a13\u7df4\u51fa\u6709\u6548\u7684\u7b56\u7565\u3002\u56e0\u6b64\uff0cMFPG \u4e0d\u50c5\u63d0\u4f9b\u4e86\u4e00\u7a2e\u7528\u65bc\u9ad8\u6548\u6a21\u64ec\u5230\u73fe\u5be6\u9077\u79fb\u7684\u65b0\u7a4e\u7bc4\u4f8b\uff0c\u9084\u63d0\u4f9b\u4e86\u4e00\u7a2e\u7ba1\u7406\u7b56\u7565\u6027\u80fd\u548c\u6578\u64da\u63a1\u96c6\u6210\u672c\u4e4b\u9593\u6b0a\u8861\u7684\u539f\u5247\u6027\u65b9\u6cd5\u3002\n", "author": "Xinjie Liu et.al.", "authors": "Xinjie Liu, Cyrus Neary, Kushagra Gupta, Christian Ellis, Ufuk Topcu, David Fridovich-Keil", "id": "2503.05696v1", "paper_url": "http://arxiv.org/abs/2503.05696v1", "repo": "null"}}