{"2406.09994": {"publish_time": "2024-06-14", "title": "Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models", "paper_summary": "In the realm of multimodal tasks, Visual Question Answering (VQA) plays a\ncrucial role by addressing natural language questions grounded in visual\ncontent. Knowledge-Based Visual Question Answering (KBVQA) advances this\nconcept by adding external knowledge along with images to respond to questions.\nWe introduce an approach for KBVQA, augmenting the existing vision-language\ntransformer encoder-decoder (OFA) model. Our main contribution involves\nenhancing questions by incorporating relevant external knowledge extracted from\nknowledge graphs, using a dynamic triple extraction method. We supply a\nflexible number of triples from the knowledge graph as context, tailored to\nmeet the requirements for answering the question. Our model, enriched with\nknowledge, demonstrates an average improvement of 4.75\\% in Exact Match Score\nover the state-of-the-art on three different KBVQA datasets. Through\nexperiments and analysis, we demonstrate that furnishing variable triples for\neach question improves the reasoning capabilities of the language model in\ncontrast to supplying a fixed number of triples. This is illustrated even for\nrecent large language models. Additionally, we highlight the model's\ngeneralization capability by showcasing its SOTA-beating performance on a small\ndataset, achieved through straightforward fine-tuning.", "paper_summary_zh": "\u5728\u591a\u6a21\u6001\u4efb\u52a1\u9886\u57df\uff0c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u901a\u8fc7\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9\u5185\u5bb9\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u57fa\u4e8e\u77e5\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54\uff08KBVQA\uff09\u901a\u8fc7\u6dfb\u52a0\u5916\u90e8\u77e5\u8bc6\u4ee5\u53ca\u56fe\u50cf\u6765\u56de\u7b54\u95ee\u9898\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u8fd9\u4e00\u6982\u5ff5\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7528\u4e8e KBVQA \u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00 transformer \u7f16\u7801\u5668\u89e3\u7801\u5668 (OFA) \u6a21\u578b\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u6d89\u53ca\u901a\u8fc7\u4f7f\u7528\u52a8\u6001\u4e09\u5143\u7ec4\u63d0\u53d6\u65b9\u6cd5\uff0c\u6574\u5408\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u76f8\u5173\u5916\u90e8\u77e5\u8bc6\u6765\u589e\u5f3a\u95ee\u9898\u3002\u6211\u4eec\u63d0\u4f9b\u6765\u81ea\u77e5\u8bc6\u56fe\u8c31\u7684\u7075\u6d3b\u6570\u91cf\u7684\u4e09\u5143\u7ec4\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u4ee5\u6ee1\u8db3\u56de\u7b54\u95ee\u9898\u7684\u8981\u6c42\u3002\u6211\u4eec\u7ecf\u8fc7\u77e5\u8bc6\u4e30\u5bcc\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u4e0d\u540c\u7684 KBVQA \u6570\u636e\u96c6\u4e0a\uff0c\u5728\u7cbe\u786e\u5339\u914d\u5206\u6570\u65b9\u9762\u5c55\u793a\u4e86\u6bd4\u6700\u5148\u8fdb\u6c34\u5e73\u5e73\u5747\u63d0\u9ad8 4.75%\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u6211\u4eec\u8bc1\u660e\u4e3a\u6bcf\u4e2a\u95ee\u9898\u63d0\u4f9b\u53ef\u53d8\u4e09\u5143\u7ec4\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u4e0e\u63d0\u4f9b\u56fa\u5b9a\u6570\u91cf\u7684\u4e09\u5143\u7ec4\u5f62\u6210\u5bf9\u6bd4\u3002\u5373\u4f7f\u5bf9\u4e8e\u6700\u8fd1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e00\u70b9\u4e5f\u5f97\u5230\u4e86\u8bf4\u660e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5c55\u793a\u6a21\u578b\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u7684 SOTA \u51fb\u8d25\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u662f\u901a\u8fc7\u76f4\u63a5\u5fae\u8c03\u5b9e\u73b0\u7684\u3002", "author": "Manas Jhalani et.al.", "authors": "Manas Jhalani, Annervaz K M, Pushpak Bhattacharyya", "id": "2406.09994v1", "paper_url": "http://arxiv.org/abs/2406.09994v1", "repo": "null"}}