{"2411.10231": {"publish_time": "2024-11-15", "title": "A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image Super-Resolution with Transformers and TaylorShift", "paper_summary": "Transformer-based Super-Resolution (SR) models have recently advanced image\nreconstruction quality, yet challenges remain due to computational complexity\nand an over-reliance on large patch sizes, which constrain fine-grained detail\nenhancement. In this work, we propose TaylorIR to address these limitations by\nutilizing a patch size of 1x1, enabling pixel-level processing in any\ntransformer-based SR model. To address the significant computational demands\nunder the traditional self-attention mechanism, we employ the TaylorShift\nattention mechanism, a memory-efficient alternative based on Taylor series\nexpansion, achieving full token-to-token interactions with linear complexity.\nExperimental results demonstrate that our approach achieves new\nstate-of-the-art SR performance while reducing memory consumption by up to 60%\ncompared to traditional self-attention-based transformers.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u8d85\u89e3\u6790\u5ea6 (SR) \u6a21\u578b\u6700\u8fd1\u63d0\u5347\u4e86\u5f71\u50cf\u91cd\u5efa\u54c1\u8cea\uff0c\u4f46\u4ecd\u56e0\u904b\u7b97\u8907\u96dc\u5ea6\u548c\u904e\u5ea6\u4f9d\u8cf4\u5927\u578b\u5340\u584a\u5927\u5c0f\u800c\u9762\u81e8\u6311\u6230\uff0c\u9019\u6703\u9650\u5236\u7d30\u7dfb\u7d30\u7bc0\u7684\u589e\u5f37\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa TaylorIR \u4f86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u65b9\u6cd5\u662f\u5229\u7528 1x1 \u7684\u5340\u584a\u5927\u5c0f\uff0c\u8b93\u4efb\u4f55\u57fa\u65bc Transformer \u7684 SR \u6a21\u578b\u90fd\u80fd\u9032\u884c\u50cf\u7d20\u5c64\u7d1a\u8655\u7406\u3002\u70ba\u4e86\u6eff\u8db3\u50b3\u7d71\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u4e0b\u7684\u91cd\u5927\u904b\u7b97\u9700\u6c42\uff0c\u6211\u5011\u63a1\u7528 TaylorShift \u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u9019\u662f\u4e00\u7a2e\u57fa\u65bc Taylor \u7d1a\u6578\u5c55\u958b\u7684\u7701\u8a18\u61b6\u9ad4\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u4ee5\u7dda\u6027\u8907\u96dc\u5ea6\u5be6\u73fe\u5b8c\u6574\u7684 token-to-token \u4e92\u52d5\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u9054\u6210\u65b0\u7684\u8d85\u89e3\u6790\u5ea6\u6280\u8853\u6c34\u6e96\uff0c\u540c\u6642\u8207\u50b3\u7d71\u57fa\u65bc\u81ea\u6211\u6ce8\u610f\u529b\u7684 Transformer \u76f8\u6bd4\uff0c\u8a18\u61b6\u9ad4\u6d88\u8017\u91cf\u6700\u591a\u53ef\u6e1b\u5c11 60%\u3002", "author": "Sanath Budakegowdanadoddi Nagaraju et.al.", "authors": "Sanath Budakegowdanadoddi Nagaraju, Brian Bernhard Moser, Tobias Christian Nauen, Stanislav Frolov, Federico Raue, Andreas Dengel", "id": "2411.10231v1", "paper_url": "http://arxiv.org/abs/2411.10231v1", "repo": "null"}}