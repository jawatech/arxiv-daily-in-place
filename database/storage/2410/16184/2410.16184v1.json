{"2410.16184": {"publish_time": "2024-10-21", "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "paper_summary": "Reward models are critical in techniques like Reinforcement Learning from\nHuman Feedback (RLHF) and Inference Scaling Laws, where they guide language\nmodel alignment and select optimal responses. Despite their importance,\nexisting reward model benchmarks often evaluate models by asking them to\ndistinguish between responses generated by models of varying power. However,\nthis approach fails to assess reward models on subtle but critical content\nchanges and variations in style, resulting in a low correlation with policy\nmodel performance. To this end, we introduce RM-Bench, a novel benchmark\ndesigned to evaluate reward models based on their sensitivity to subtle content\ndifferences and resistance to style biases. Extensive experiments demonstrate\nthat RM-Bench strongly correlates with policy model performance, making it a\nreliable reference for selecting reward models to align language models\neffectively. We evaluate nearly 40 reward models on RM-Bench. Our results\nreveal that even state-of-the-art models achieve an average performance of only\n46.6%, which falls short of random-level accuracy (50%) when faced with style\nbias interference. These findings highlight the significant room for\nimprovement in current reward models. Related code and data are available at\nhttps://github.com/THU-KEG/RM-Bench.", "paper_summary_zh": "\u734e\u52f5\u6a21\u578b\u5728\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u548c\u63a8\u8ad6\u898f\u6a21\u5b9a\u5f8b\u7b49\u6280\u8853\u4e2d\u81f3\u95dc\u91cd\u8981\uff0c\u5b83\u5011\u6307\u5c0e\u8a9e\u8a00\u6a21\u578b\u5c0d\u9f4a\u4e26\u9078\u64c7\u6700\u4f73\u56de\u61c9\u3002\u5118\u7ba1\u5b83\u5011\u5f88\u91cd\u8981\uff0c\u4f46\u73fe\u6709\u7684\u734e\u52f5\u6a21\u578b\u57fa\u6e96\u901a\u5e38\u6703\u8981\u6c42\u6a21\u578b\u5340\u5206\u7531\u4e0d\u540c\u80fd\u529b\u6a21\u578b\u7522\u751f\u7684\u56de\u61c9\u4f86\u8a55\u4f30\u6a21\u578b\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u7121\u6cd5\u8a55\u4f30\u734e\u52f5\u6a21\u578b\u5c0d\u5fae\u5999\u4f46\u91cd\u8981\u7684\u5167\u5bb9\u8b8a\u66f4\u548c\u98a8\u683c\u8b8a\u5316\u7684\u5f71\u97ff\uff0c\u5c0e\u81f4\u8207\u653f\u7b56\u6a21\u578b\u6548\u80fd\u76f8\u95dc\u6027\u4f4e\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 RM-Bench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u57fa\u6e96\uff0c\u65e8\u5728\u6839\u64da\u734e\u52f5\u6a21\u578b\u5c0d\u5fae\u5999\u5167\u5bb9\u5dee\u7570\u7684\u654f\u611f\u6027\u548c\u5c0d\u98a8\u683c\u504f\u898b\u7684\u62b5\u6297\u529b\u4f86\u8a55\u4f30\u734e\u52f5\u6a21\u578b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e RM-Bench \u8207\u653f\u7b56\u6a21\u578b\u6548\u80fd\u5bc6\u5207\u76f8\u95dc\uff0c\u4f7f\u5176\u6210\u70ba\u9078\u64c7\u734e\u52f5\u6a21\u578b\u4ee5\u6709\u6548\u5c0d\u9f4a\u8a9e\u8a00\u6a21\u578b\u7684\u53ef\u9760\u53c3\u8003\u3002\u6211\u5011\u5728 RM-Bench \u4e0a\u8a55\u4f30\u4e86\u8fd1 40 \u500b\u734e\u52f5\u6a21\u578b\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u5176\u5e73\u5747\u6548\u80fd\u4e5f\u50c5\u9054\u5230 46.6%\uff0c\u5728\u9762\u5c0d\u98a8\u683c\u504f\u898b\u5e72\u64fe\u6642\uff0c\u4f4e\u65bc\u96a8\u6a5f\u5c64\u7d1a\u6e96\u78ba\u5ea6 (50%)\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u7576\u524d\u734e\u52f5\u6a21\u578b\u6709\u986f\u8457\u7684\u6539\u9032\u7a7a\u9593\u3002\u76f8\u95dc\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u65bc https://github.com/THU-KEG/RM-Bench \u53d6\u5f97\u3002", "author": "Yantao Liu et.al.", "authors": "Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li", "id": "2410.16184v1", "paper_url": "http://arxiv.org/abs/2410.16184v1", "repo": "https://github.com/thu-keg/rm-bench"}}