{"2412.06000": {"publish_time": "2024-12-08", "title": "Does RLHF Scale? Exploring the Impacts From Data, Model, and Method", "paper_summary": "This study explores the scaling properties of Reinforcement Learning from\nHuman Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is\nconsidered an important step in post-training of LLMs, its scaling potential is\nstill largely unknown. We systematically analyze key components in the RLHF\nframework--model size, data composition, and inference budget--and their\nimpacts on performance. Our findings show that increasing data diversity and\nvolume improves reward model performance, helping process-supervision models\nscale better. For policy training, more response samples per prompt boost\nperformance initially but quickly plateau. And larger reward models offer\nmodest gains in policy training. In addition, larger policy models benefit less\nfrom RLHF with a fixed reward model. Overall, RLHF scales less efficiently than\npretraining, with diminishing returns from additional computational resources.\nBased on these observations, we propose strategies to optimize RLHF performance\nwithin computational limits.", "paper_summary_zh": "\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u5f37\u5316\u5b78\u7fd2\u4f86\u81ea\u4eba\u985e\u56de\u994b (RLHF) \u7684\u64f4\u5145\u5c6c\u6027\u3002\u96d6\u7136 RLHF \u88ab\u8a8d\u70ba\u662f LLM \u5f8c\u8a13\u7df4\u7684\u91cd\u8981\u6b65\u9a5f\uff0c\u4f46\u5176\u64f4\u5145\u6f5b\u529b\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u77e5\u3002\u6211\u5011\u7cfb\u7d71\u6027\u5730\u5206\u6790 RLHF \u6846\u67b6\u4e2d\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u2014\u2014\u6a21\u578b\u5927\u5c0f\u3001\u8cc7\u6599\u7d44\u6210\u548c\u63a8\u8ad6\u9810\u7b97\u2014\u2014\u53ca\u5176\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u589e\u52a0\u8cc7\u6599\u7684\u591a\u6a23\u6027\u548c\u6578\u91cf\u53ef\u4ee5\u63d0\u5347\u734e\u52f5\u6a21\u578b\u6548\u80fd\uff0c\u6709\u52a9\u65bc\u6d41\u7a0b\u76e3\u7763\u6a21\u578b\u66f4\u597d\u5730\u64f4\u5145\u3002\u5c0d\u65bc\u7b56\u7565\u8a13\u7df4\uff0c\u6bcf\u500b\u63d0\u793a\u7684\u66f4\u591a\u56de\u61c9\u7bc4\u4f8b\u6703\u5728\u521d\u671f\u63d0\u5347\u6548\u80fd\uff0c\u4f46\u5f88\u5feb\u5c31\u6703\u9054\u5230\u5e73\u7a69\u671f\u3002\u800c\u8f03\u5927\u7684\u734e\u52f5\u6a21\u578b\u5728\u7b56\u7565\u8a13\u7df4\u4e2d\u63d0\u4f9b\u4e86\u9069\u5ea6\u7684\u589e\u76ca\u3002\u6b64\u5916\uff0c\u8f03\u5927\u7684\u7b56\u7565\u6a21\u578b\u5f9e\u5177\u6709\u56fa\u5b9a\u734e\u52f5\u6a21\u578b\u7684 RLHF \u7372\u76ca\u8f03\u5c11\u3002\u7e3d\u7684\u4f86\u8aaa\uff0cRLHF \u7684\u64f4\u5145\u6548\u7387\u4f4e\u65bc\u9810\u8a13\u7df4\uff0c\u984d\u5916\u7684\u904b\u7b97\u8cc7\u6e90\u6703\u5e36\u4f86\u905e\u6e1b\u7684\u56de\u5831\u3002\u6839\u64da\u9019\u4e9b\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5728\u904b\u7b97\u9650\u5236\u5167\u6700\u4f73\u5316 RLHF \u6548\u80fd\u7684\u7b56\u7565\u3002", "author": "Zhenyu Hou et.al.", "authors": "Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong", "id": "2412.06000v1", "paper_url": "http://arxiv.org/abs/2412.06000v1", "repo": "null"}}