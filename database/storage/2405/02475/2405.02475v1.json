{"2405.02475": {"publish_time": "2024-05-03", "title": "Generalizing Orthogonalization for Models with Non-linearities", "paper_summary": "The complexity of black-box algorithms can lead to various challenges,\nincluding the introduction of biases. These biases present immediate risks in\nthe algorithms' application. It was, for instance, shown that neural networks\ncan deduce racial information solely from a patient's X-ray scan, a task beyond\nthe capability of medical experts. If this fact is not known to the medical\nexpert, automatic decision-making based on this algorithm could lead to\nprescribing a treatment (purely) based on racial information. While current\nmethodologies allow for the \"orthogonalization\" or \"normalization\" of neural\nnetworks with respect to such information, existing approaches are grounded in\nlinear models. Our paper advances the discourse by introducing corrections for\nnon-linearities such as ReLU activations. Our approach also encompasses scalar\nand tensor-valued predictions, facilitating its integration into neural network\narchitectures. Through extensive experiments, we validate our method's\neffectiveness in safeguarding sensitive data in generalized linear models,\nnormalizing convolutional neural networks for metadata, and rectifying\npre-existing embeddings for undesired attributes.", "paper_summary_zh": "", "author": "David R\u00fcgamer et.al.", "authors": "David R\u00fcgamer,Chris Kolb,Tobias Weber,Lucas Kook,Thomas Nagler", "id": "2405.02475v1", "paper_url": "http://arxiv.org/abs/2405.02475v1", "repo": "null"}}