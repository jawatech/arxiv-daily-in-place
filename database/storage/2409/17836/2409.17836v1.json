{"2409.17836": {"publish_time": "2024-09-26", "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models", "paper_summary": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10\\% up to 17.2\\% across various\ndatasets and architectures. Additionally, our approach shows promising\ncompatibility with lossy compression techniques such as quantization and\nsparsification. These findings highlight the significant potential of LLMs as a\nmodel for effectively handling gradients. We will release the source code upon\npublication.", "paper_summary_zh": "\u5118\u7ba1\u7d71\u8a08\u5148\u9a57\u6a21\u578b\u5728\u5404\u500b\u9818\u57df\u5ee3\u6cdb\u4f7f\u7528\uff0c\n\u4f46\u9577\u671f\u4ee5\u4f86\uff0c\u795e\u7d93\u7db2\u8def\u68af\u5ea6\u7684\u6a21\u578b\u537b\u4e00\u76f4\u88ab\u5ffd\u7565\u3002\u5176\u56fa\u6709\u7684\u6311\u6230\u5728\u65bc\u5b83\u5011\u7684\u9ad8\u7dad\u5ea6\u7d50\u69cb\u548c\u8907\u96dc\u7684\u76f8\u4e92\u4f9d\u8cf4\u6027\uff0c\u9019\u4f7f\u5f97\u6709\u6548\u7684\u5efa\u6a21\u8b8a\u5f97\u8907\u96dc\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u96f6\u6b21\u5b78\u7fd2\u4e2d\u4f5c\u70ba\u68af\u5ea6\u5148\u9a57\u7684\u6f5b\u529b\u3002\u6211\u5011\u900f\u904e\u8003\u616e\u7121\u5931\u771f\u68af\u5ea6\u58d3\u7e2e\u4f86\u6aa2\u9a57\u6b64\u7279\u6027\uff0c\u7121\u5931\u771f\u68af\u5ea6\u58d3\u7e2e\u662f\u5206\u4f48\u5f0f\u5b78\u7fd2\u4e2d\u7684\u4e00\u9805\u95dc\u9375\u61c9\u7528\uff0c\u5b83\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u7cbe\u78ba\u7684\u6a5f\u7387\u5efa\u6a21\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 LM-GC\uff0c\u9019\u662f\u4e00\u7a2e\u5c07 LLM \u8207\u7b97\u8853\u7de8\u78bc\u6574\u5408\u8d77\u4f86\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u5011\u7684\u6280\u8853\u5c07\u7d14\u7cb9\u7684\u68af\u5ea6\u8f49\u63db\u6210\u985e\u6587\u5b57\u7684\u683c\u5f0f\uff0c\u8207\u5176\u7d14\u7cb9\u7684\u8868\u793a\u76f8\u6bd4\uff0c\u5c07\u4ee3\u78bc\u6548\u7387\u63d0\u9ad8\u4e86 38 \u500d\u3002\u6211\u5011\u78ba\u4fdd\u6b64\u8cc7\u6599\u8f49\u63db\u8207\u7d14\u7cb9\u68af\u5ea6\u7684\u7d50\u69cb\u548c LLM \u5e38\u8b58\u5225\u7684\u7b26\u865f\u4fdd\u6301\u7dca\u5bc6\u7684\u4e00\u81f4\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cLM-GC \u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u6700\u5148\u9032\u7121\u5931\u771f\u58d3\u7e2e\u65b9\u6cd5\uff0c\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u548c\u67b6\u69cb\u4e2d\u5c07\u58d3\u7e2e\u7387\u63d0\u9ad8\u4e86 10% \u81f3 17.2%\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u986f\u793a\u51fa\u8207\u6709\u640d\u58d3\u7e2e\u6280\u8853\uff08\u4f8b\u5982\u91cf\u5316\u548c\u7a00\u758f\u5316\uff09\u6709\u671b\u76f8\u5bb9\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86 LLM \u4f5c\u70ba\u6709\u6548\u8655\u7406\u68af\u5ea6\u7684\u6a21\u578b\u7684\u986f\u8457\u6f5b\u529b\u3002\u6211\u5011\u5c07\u5728\u767c\u8868\u5f8c\u91cb\u51fa\u539f\u59cb\u78bc\u3002", "author": "Hui-Po Wang et.al.", "authors": "Hui-Po Wang, Mario Fritz", "id": "2409.17836v1", "paper_url": "http://arxiv.org/abs/2409.17836v1", "repo": "null"}}