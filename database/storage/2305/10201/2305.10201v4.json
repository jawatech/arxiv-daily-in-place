{"2305.10201": {"publish_time": "2023-05-17", "title": "Echoes of Biases: How Stigmatizing Language Affects AI Performance", "paper_summary": "Electronic health records (EHRs) serve as an essential data source for the\nenvisioned artificial intelligence (AI)-driven transformation in healthcare.\nHowever, clinician biases reflected in EHR notes can lead to AI models\ninheriting and amplifying these biases, perpetuating health disparities. This\nstudy investigates the impact of stigmatizing language (SL) in EHR notes on\nmortality prediction using a Transformer-based deep learning model and\nexplainable AI (XAI) techniques. Our findings demonstrate that SL written by\nclinicians adversely affects AI performance, particularly so for black\npatients, highlighting SL as a source of racial disparity in AI model\ndevelopment. To explore an operationally efficient way to mitigate SL's impact,\nwe investigate patterns in the generation of SL through a clinicians'\ncollaborative network, identifying central clinicians as having a stronger\nimpact on racial disparity in the AI model. We find that removing SL written by\ncentral clinicians is a more efficient bias reduction strategy than eliminating\nall SL in the entire corpus of data. This study provides actionable insights\nfor responsible AI development and contributes to understanding clinician\nbehavior and EHR note writing in healthcare.", "paper_summary_zh": "", "author": "Yizhi Liu et.al.", "authors": "Yizhi Liu,Weiguang Wang,Guodong Gordon Gao,Ritu Agarwal", "id": "2305.10201v4", "paper_url": "http://arxiv.org/abs/2305.10201v4", "repo": "null"}}