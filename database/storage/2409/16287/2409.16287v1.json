{"2409.16287": {"publish_time": "2024-09-24", "title": "Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking", "paper_summary": "Articulated object manipulation requires precise object interaction, where\nthe object's axis must be carefully considered. Previous research employed\ninteractive perception for manipulating articulated objects, but typically,\nopen-loop approaches often suffer from overlooking the interaction dynamics. To\naddress this limitation, we present a closed-loop pipeline integrating\ninteractive perception with online axis estimation from segmented 3D point\nclouds. Our method leverages any interactive perception technique as a\nfoundation for interactive perception, inducing slight object movement to\ngenerate point cloud frames of the evolving dynamic scene. These point clouds\nare then segmented using Segment Anything Model 2 (SAM2), after which the\nmoving part of the object is masked for accurate motion online axis estimation,\nguiding subsequent robotic actions. Our approach significantly enhances the\nprecision and efficiency of manipulation tasks involving articulated objects.\nExperiments in simulated environments demonstrate that our method outperforms\nbaseline approaches, especially in tasks that demand precise axis-based\ncontrol. Project Page:\nhttps://hytidel.github.io/video-tracking-for-axis-estimation/.", "paper_summary_zh": "\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u9700\u8981\u7cbe\u786e\u7684\u7269\u4f53\u4ea4\u4e92\uff0c\u5176\u4e2d\u5fc5\u987b\u4ed4\u7ec6\u8003\u8651\u7269\u4f53\u7684\u8f74\u3002\u5148\u524d\u7684\u7814\u7a76\u5229\u7528\u4ea4\u4e92\u5f0f\u611f\u77e5\u6765\u64cd\u4f5c\u94f0\u63a5\u7269\u4f53\uff0c\u4f46\u901a\u5e38\uff0c\u5f00\u73af\u65b9\u6cd5\u7ecf\u5e38\u4f1a\u5ffd\u7565\u4ea4\u4e92\u52a8\u6001\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ed\u73af\u7ba1\u9053\uff0c\u5c06\u4ea4\u4e92\u5f0f\u611f\u77e5\u4e0e\u5206\u6bb5 3D \u70b9\u4e91\u7684\u5728\u7ebf\u8f74\u4f30\u8ba1\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4efb\u4f55\u4ea4\u4e92\u5f0f\u611f\u77e5\u6280\u672f\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u611f\u77e5\u7684\u57fa\u7840\uff0c\u8bf1\u5bfc\u8f7b\u5fae\u7684\u7269\u4f53\u8fd0\u52a8\u4ee5\u751f\u6210\u4e0d\u65ad\u53d8\u5316\u7684\u52a8\u6001\u573a\u666f\u7684\u70b9\u4e91\u5e27\u3002\u7136\u540e\u4f7f\u7528 Segment Anything Model 2 (SAM2) \u5bf9\u8fd9\u4e9b\u70b9\u4e91\u8fdb\u884c\u5206\u6bb5\uff0c\u4e4b\u540e\u5bf9\u7269\u4f53\u7684\u79fb\u52a8\u90e8\u5206\u8fdb\u884c\u63a9\u7801\u5904\u7406\u4ee5\u8fdb\u884c\u7cbe\u786e\u7684\u8fd0\u52a8\u5728\u7ebf\u8f74\u4f30\u8ba1\uff0c\u6307\u5bfc\u540e\u7eed\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u6d89\u53ca\u94f0\u63a5\u7269\u4f53\u7684\u64cd\u4f5c\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u57fa\u4e8e\u7cbe\u786e\u8f74\u7684\u63a7\u5236\u7684\u4efb\u52a1\u4e2d\u3002\u9879\u76ee\u9875\u9762\uff1a\nhttps://hytidel.github.io/video-tracking-for-axis-estimation/\u3002", "author": "Xi Wang et.al.", "authors": "Xi Wang, Tianxing Chen, Qiaojun Yu, Tianling Xu, Zanxin Chen, Yiting Fu, Cewu Lu, Yao Mu, Ping Luo", "id": "2409.16287v1", "paper_url": "http://arxiv.org/abs/2409.16287v1", "repo": "null"}}