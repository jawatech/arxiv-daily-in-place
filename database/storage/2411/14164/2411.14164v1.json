{"2411.14164": {"publish_time": "2024-11-21", "title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency.", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u85c9\u7531\u8ba9\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u7406\u89e3\u89c6\u89c9\u8f93\u5165\uff0c\u5c55\u73b0\u51fa\u5728\u8fbe\u6210\u5353\u8d8a\u591a\u6a21\u6001\u80fd\u529b\u4e0a\u7684\u4e00\u5927\u8fdb\u6b65\u3002LVLMs \u4e00\u822c\u4f1a\u5229\u7528\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4f8b\u5982 CLIP\uff0c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u89c6\u89c9\u6807\u8bb0\uff0c\u7136\u540e\u5728\u8f93\u5165 LLM \u8fdb\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u900f\u8fc7\u6295\u5f71\u5c42\u4e0e\u6587\u672c\u6807\u8bb0\u5bf9\u9f50\u3002\u867d\u7136\u73b0\u6709\u7684 LVLMs \u5df2\u83b7\u5f97\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u5176\u63a8\u7406\u6548\u7387\u4ecd\u53d7\u5230\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u548c\u6807\u8bb0\u4e4b\u95f4\u6f5c\u5728\u5197\u4f59\u7684\u9650\u5236\u3002\u4e3a\u4e86\u51cf\u8f7b\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7126\u70b9\u4fee\u526a (FoPru)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5b83\u4f1a\u6839\u636e\u89c6\u89c9\u7f16\u7801\u5668\u884d\u751f\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6807\u8bb0\u91cd\u8981\u6027\u6765\u4fee\u526a\u89c6\u89c9\u6807\u8bb0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u66ff\u4ee3\u4fee\u526a\u7b56\u7565\uff1a1) \u6392\u540d\u7b56\u7565\uff0c\u5b83\u5229\u7528\u6240\u6709\u6807\u8bb0\u91cd\u8981\u6027\u5206\u6570\u6765\u5728\u5168\u5c40\u89c6\u56fe\u4e2d\u4fdd\u7559\u66f4\u91cd\u8981\u7684\u6807\u8bb0\uff1b2) \u884c\u7b56\u7565\uff0c\u5b83\u4e13\u6ce8\u4e8e\u4ece\u5c40\u90e8\u89d2\u5ea6\u4fdd\u7559\u56fe\u50cf\u4e2d\u7684\u8fde\u7eed\u5173\u952e\u4fe1\u606f\u3002\u6700\u540e\uff0c\u9009\u5b9a\u7684\u6807\u8bb0\u4f1a\u91cd\u65b0\u6392\u5e8f\u4ee5\u7ef4\u6301\u5176\u539f\u59cb\u4f4d\u7f6e\u5173\u7cfb\u3002\u5728\u5404\u79cd LVLMs \u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4fee\u526a\u5927\u91cf\u5197\u4f59\u6807\u8bb0\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8\u51c6\u786e\u5ea6\uff0c\u4ece\u800c\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "author": "Lei Jiang et.al.", "authors": "Lei Jiang, Weizhe Huang, Tongxuan Liu, Yuting Zeng, Jing Li, Lechao Cheng, Xiaohua Xu", "id": "2411.14164v1", "paper_url": "http://arxiv.org/abs/2411.14164v1", "repo": "null"}}