{"2405.18425": {"publish_time": "2024-05-28", "title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention", "paper_summary": "Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, \\name{}, offers a favorable trade-off in accuracy,\nparameters, and FLOPs on ImageNet and downstream tasks, outperforming popular\nTransformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy\nwhile using only 27\\% of the parameters and 20\\% of the FLOPs, running\n2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution,\n\\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$\nfaster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results\nposition \\name{} as an efficient and scalable solution for visual\nrepresentation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u7ebf\u6027\u590d\u6742\u5ea6\u5e8f\u5217\u5efa\u6a21\u7f51\u7edc\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u89c6\u89c9 Transformer \u7c7b\u4f3c\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u66f4\u5c11\u7684 FLOP \u548c\u66f4\u5c11\u7684\u5185\u5b58\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u65b9\u9762\u7684\u4f18\u52bf\u5e76\u4e0d\u660e\u663e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7528\u4e8e\u89c6\u89c9\u7684 Gate \u7ebf\u6027\u6ce8\u610f\u529b (GLA)\uff0c\u5229\u7528\u5176\u51fa\u8272\u7684\u786c\u4ef6\u611f\u77e5\u548c\u6548\u7387\u3002\u6211\u4eec\u63d0\u51fa\u65b9\u5411\u95e8\u63a7\u6765\u901a\u8fc7\u53cc\u5411\u5efa\u6a21\u6355\u6349 1D \u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7 2D \u95e8\u63a7\u5c40\u90e8\u6ce8\u5165\u5c06 2D \u5c40\u90e8\u7ec6\u8282\u81ea\u9002\u5e94\u5730\u6ce8\u5165 1D \u5168\u5c40\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u6ce8\u91cd\u786c\u4ef6\u7684\u5b9e\u73b0\u8fdb\u4e00\u6b65\u5c06\u6b63\u5411\u548c\u53cd\u5411\u626b\u63cf\u5408\u5e76\u5230\u4e00\u4e2a\u5185\u6838\u4e2d\uff0c\u589e\u5f3a\u4e86\u5e76\u884c\u6027\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u6210\u672c\u548c\u5ef6\u8fdf\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b \\name{} \u5728\u51c6\u786e\u6027\u3001\u53c2\u6570\u548c ImageNet \u53ca\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684 FLOP \u4e0a\u63d0\u4f9b\u4e86\u6709\u5229\u7684\u6743\u8861\uff0c\u4f18\u4e8e\u6d41\u884c\u7684 Transformer \u548c\u57fa\u4e8e CNN \u7684\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\\name{}-S \u5339\u914d\u4e86 DeiT-B \u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u53ea\u4f7f\u7528\u4e86 27% \u7684\u53c2\u6570\u548c 20% \u7684 FLOP\uff0c\u5728 $224\\times224$ \u56fe\u50cf\u4e0a\u8fd0\u884c\u901f\u5ea6\u63d0\u9ad8\u4e86 2 \u500d\u3002\u5728 $1024\\times1024$ \u5206\u8fa8\u7387\u4e0b\uff0c\\name{}-T \u4f7f\u7528\u7684 FLOP \u51cf\u5c11\u4e86 5.2 \u500d\uff0c\u8282\u7701\u4e86 90% \u7684 GPU \u5185\u5b58\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u9ad8\u4e86 4.8 \u500d\uff0c\u5e76\u4e14\u6bd4 DeiT-T \u7684 top-1 \u51c6\u786e\u7387\u63d0\u9ad8\u4e86 20.7%\u3002\u8fd9\u4e9b\u7ed3\u679c\u5c06 \\name{} \u5b9a\u4f4d\u4e3a\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u7684\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/hustvl/ViG} \u83b7\u5f97\u3002</paragraph>", "author": "Bencheng Liao et.al.", "authors": "Bencheng Liao, Xinggang Wang, Lianghui Zhu, Qian Zhang, Chang Huang", "id": "2405.18425v1", "paper_url": "http://arxiv.org/abs/2405.18425v1", "repo": "https://github.com/hustvl/vig"}}