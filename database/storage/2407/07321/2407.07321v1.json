{"2407.07321": {"publish_time": "2024-07-10", "title": "RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension", "paper_summary": "Large Language Models (LLMs) have been applied to many research problems\nacross various domains. One of the applications of LLMs is providing\nquestion-answering systems that cater to users from different fields. The\neffectiveness of LLM-based question-answering systems has already been\nestablished at an acceptable level for users posing questions in popular and\npublic domains such as trivia and literature. However, it has not often been\nestablished in niche domains that traditionally require specialized expertise.\nTo this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering\nquestions originating from Environmental Impact Statements prepared by U.S.\nfederal government agencies in accordance with the National Environmental\nEnvironmental Act (NEPA). We specifically measure the ability of LLMs to\nunderstand the nuances of legal, technical, and compliance-related information\npresent in NEPA documents in different contextual scenarios. For example, we\ntest the LLMs' internal prior NEPA knowledge by providing questions without any\ncontext, as well as assess how LLMs synthesize the contextual information\npresent in long NEPA documents to facilitate the question/answering task. We\ncompare the performance of the long context LLMs and RAG powered models in\nhandling different types of questions (e.g., problem-solving, divergent). Our\nresults suggest that RAG powered models significantly outperform the long\ncontext models in the answer accuracy regardless of the choice of the frontier\nLLM. Our further analysis reveals that many models perform better answering\nclosed questions than divergent and problem-solving questions.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u61c9\u7528\u65bc\u5404\u7a2e\u9818\u57df\u7684\u8a31\u591a\u7814\u7a76\u554f\u984c\u3002LLM \u7684\u61c9\u7528\u4e4b\u4e00\u662f\u63d0\u4f9b\u91dd\u5c0d\u4e0d\u540c\u9818\u57df\u4f7f\u7528\u8005\u7684\u554f\u7b54\u7cfb\u7d71\u3002\u57fa\u65bc LLM \u7684\u554f\u7b54\u7cfb\u7d71\u7684\u6709\u6548\u6027\u5df2\u5728\u71b1\u9580\u548c\u516c\u5171\u9818\u57df\uff08\u4f8b\u5982\u7463\u4e8b\u548c\u6587\u5b78\uff09\u4e2d\u63d0\u51fa\u554f\u984c\u7684\u4f7f\u7528\u8005\u4e2d\u9054\u5230\u53ef\u63a5\u53d7\u7684\u6c34\u5e73\u3002\u7136\u800c\uff0c\u5b83\u4e26\u672a\u7d93\u5e38\u5728\u50b3\u7d71\u4e0a\u9700\u8981\u5c08\u696d\u77e5\u8b58\u7684\u5229\u57fa\u9818\u57df\u4e2d\u5efa\u7acb\u3002\u70ba\u6b64\uff0c\u6211\u5011\u69cb\u5efa\u4e86 NEPAQuAD1.0 \u57fa\u6e96\u4f86\u8a55\u4f30\u4e09\u500b\u524d\u6cbf LLM \u7684\u6027\u80fd\u2014\u2014Claude Sonnet\u3001Gemini \u548c GPT-4\u2014\u2014\u5728\u56de\u7b54\u7531\u7f8e\u570b\u806f\u90a6\u653f\u5e9c\u6a5f\u69cb\u6839\u64da\u570b\u5bb6\u74b0\u5883\u74b0\u5883\u6cd5 (NEPA) \u7de8\u88fd\u7684\u74b0\u5883\u5f71\u97ff\u5831\u544a\u66f8\u4e2d\u7522\u751f\u7684\u554f\u984c\u6642\u3002\u6211\u5011\u7279\u5225\u8861\u91cf\u4e86 LLM \u5728\u4e0d\u540c\u8a9e\u5883\u5834\u666f\u4e2d\u7406\u89e3 NEPA \u6587\u4ef6\u4e2d\u5b58\u5728\u7684\u6cd5\u5f8b\u3001\u6280\u8853\u548c\u5408\u898f\u76f8\u95dc\u8cc7\u8a0a\u7d30\u5fae\u5dee\u5225\u7684\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u6211\u5011\u901a\u904e\u63d0\u4f9b\u6c92\u6709\u4efb\u4f55\u80cc\u666f\u7684\u554f\u984c\u4f86\u6e2c\u8a66 LLM \u7684\u5167\u90e8\u5148\u9a57 NEPA \u77e5\u8b58\uff0c\u4e26\u8a55\u4f30 LLM \u5982\u4f55\u7d9c\u5408 NEPA \u9577\u6587\u4ef6\u4e2d\u5b58\u5728\u7684\u8a9e\u5883\u8cc7\u8a0a\u4ee5\u4fc3\u9032\u554f\u7b54\u4efb\u52d9\u3002\u6211\u5011\u6bd4\u8f03\u4e86\u9577\u8a9e\u5883 LLM \u548c RAG \u9a45\u52d5\u6a21\u578b\u5728\u8655\u7406\u4e0d\u540c\u985e\u578b\u554f\u984c\uff08\u4f8b\u5982\uff0c\u554f\u984c\u89e3\u6c7a\u3001\u767c\u6563\uff09\u65b9\u9762\u7684\u6027\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u7121\u8ad6\u9078\u64c7\u54ea\u500b\u524d\u6cbf LLM\uff0cRAG \u9a45\u52d5\u6a21\u578b\u5728\u56de\u7b54\u6e96\u78ba\u6027\u65b9\u9762\u90fd\u986f\u8457\u512a\u65bc\u9577\u8a9e\u5883\u6a21\u578b\u3002\u6211\u5011\u7684\u9032\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u8a31\u591a\u6a21\u578b\u5728\u56de\u7b54\u5c01\u9589\u5f0f\u554f\u984c\u65b9\u9762\u7684\u8868\u73fe\u512a\u65bc\u767c\u6563\u5f0f\u548c\u554f\u984c\u89e3\u6c7a\u554f\u984c\u3002", "author": "Hung Phan et.al.", "authors": "Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana", "id": "2407.07321v1", "paper_url": "http://arxiv.org/abs/2407.07321v1", "repo": "null"}}