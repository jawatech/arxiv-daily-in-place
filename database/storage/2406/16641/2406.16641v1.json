{"2406.16641": {"publish_time": "2024-06-24", "title": "Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI Generated Image Quality Assessment", "paper_summary": "Recently, textual prompt tuning has shown inspirational performance in\nadapting Contrastive Language-Image Pre-training (CLIP) models to natural image\nquality assessment. However, such uni-modal prompt learning method only tunes\nthe language branch of CLIP models. This is not enough for adapting CLIP models\nto AI generated image quality assessment (AGIQA) since AGIs visually differ\nfrom natural images. In addition, the consistency between AGIs and user input\ntext prompts, which correlates with the perceptual quality of AGIs, is not\ninvestigated to guide AGIQA. In this letter, we propose vision-language\nconsistency guided multi-modal prompt learning for blind AGIQA, dubbed\nCLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in\nlanguage and vision branches of CLIP models, respectively. Moreover, we design\na text-to-image alignment quality prediction task, whose learned\nvision-language consistency knowledge is used to guide the optimization of the\nabove multi-modal prompts. Experimental results on two public AGIQA datasets\ndemonstrate that the proposed method outperforms state-of-the-art quality\nassessment models. The source code is available at\nhttps://github.com/JunFu1995/CLIP-AGIQA.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u6587\u672c\u63d0\u793a\u8c03\u6574\u5728\u9002\u5e94\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u6a21\u578b\u5230\u81ea\u7136\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u663e\u793a\u51fa\u9f13\u821e\u4eba\u5fc3\u7684\u8868\u73b0\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5355\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u53ea\u8c03\u6574\u4e86 CLIP \u6a21\u578b\u7684\u8bed\u8a00\u5206\u652f\u3002\u8fd9\u4e0d\u8db3\u4ee5\u9002\u5e94 CLIP \u6a21\u578b\u5230 AI \u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (AGIQA)\uff0c\u56e0\u4e3a AGI \u5728\u89c6\u89c9\u4e0a\u4e0d\u540c\u4e8e\u81ea\u7136\u56fe\u50cf\u3002\u6b64\u5916\uff0cAGI \u4e0e\u7528\u6237\u8f93\u5165\u6587\u672c\u63d0\u793a\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff08\u4e0e AGI \u7684\u611f\u77e5\u8d28\u91cf\u76f8\u5173\uff09\u5c1a\u672a\u88ab\u8c03\u67e5\u4ee5\u6307\u5bfc AGIQA\u3002\u5728\u8fd9\u5c01\u4fe1\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u89c9\u8bed\u8a00\u4e00\u81f4\u6027\u6307\u5bfc\u7684\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u7528\u4e8e\u76f2 AGIQA\uff0c\u79f0\u4e3a CLIP-AGIQA\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728 CLIP \u6a21\u578b\u7684\u8bed\u8a00\u548c\u89c6\u89c9\u5206\u652f\u4e2d\u5206\u522b\u5f15\u5165\u4e86\u53ef\u5b66\u4e60\u7684\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u5bf9\u9f50\u8d28\u91cf\u9884\u6d4b\u4efb\u52a1\uff0c\u5176\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u4e00\u81f4\u6027\u77e5\u8bc6\u7528\u4e8e\u6307\u5bfc\u4e0a\u8ff0\u591a\u6a21\u6001\u63d0\u793a\u7684\u4f18\u5316\u3002\u5728\u4e24\u4e2a\u516c\u5171 AGIQA \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/JunFu1995/CLIP-AGIQA \u83b7\u5f97\u3002</paragraph>", "author": "Jun Fu et.al.", "authors": "Jun Fu, Wei Zhou, Qiuping Jiang, Hantao Liu, Guangtao Zhai", "id": "2406.16641v1", "paper_url": "http://arxiv.org/abs/2406.16641v1", "repo": "null"}}