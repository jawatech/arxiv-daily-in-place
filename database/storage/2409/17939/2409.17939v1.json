{"2409.17939": {"publish_time": "2024-09-26", "title": "Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods", "paper_summary": "Translation memories (TMs) are the backbone for professional translation\ntools called computer-aided translation (CAT) tools. In order to perform a\ntranslation using a CAT tool, a translator uses the TM to gather translations\nsimilar to the desired segment to translate (s'). Many CAT tools offer a\nfuzzy-match algorithm to locate segments (s) in the TM that are close in\ndistance to s'. After locating two similar segments, the CAT tool will present\nparallel segments (s, t) that contain one segment in the source language along\nwith its translation in the target language. Additionally, CAT tools contain\nfuzzy-match repair (FMR) techniques that will automatically use the parallel\nsegments from the TM to create new TM entries containing a modified version of\nthe original with the idea in mind that it will be the translation of s'. Most\nFMR techniques use machine translation as a way of \"repairing\" those words that\nhave to be modified. In this article, we show that for a large part of those\nwords which are anchored, we can use other techniques that are based on machine\nlearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we\nshow that for anchored words that follow the continuous bag-of-words (CBOW)\nparadigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for\nsome cases, better results than neural machine translation for translating\nanchored words from French to English.", "paper_summary_zh": "\u7ffb\u8b6f\u8a18\u61b6\u9ad4 (TM) \u662f\u7a31\u70ba\u96fb\u8166\u8f14\u52a9\u7ffb\u8b6f (CAT) \u5de5\u5177\u7684\u5c08\u696d\u7ffb\u8b6f\u5de5\u5177\u7684\u9aa8\u5e79\u3002\u70ba\u4e86\u4f7f\u7528 CAT \u5de5\u5177\u9032\u884c\u7ffb\u8b6f\uff0c\u7ffb\u8b6f\u4eba\u54e1\u4f7f\u7528 TM \u4f86\u6536\u96c6\u8207\u8981\u7ffb\u8b6f\u7684\u76ee\u6a19\u7247\u6bb5 (s') \u76f8\u4f3c\u7684\u7ffb\u8b6f\u3002\u8a31\u591a CAT \u5de5\u5177\u63d0\u4f9b\u6a21\u7cca\u6bd4\u5c0d\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5728 TM \u4e2d\u627e\u51fa\u8207 s' \u8ddd\u96e2\u76f8\u8fd1\u7684\u7247\u6bb5 (s)\u3002\u5728\u627e\u5230\u5169\u500b\u76f8\u4f3c\u7684\u7247\u6bb5\u5f8c\uff0cCAT \u5de5\u5177\u6703\u986f\u793a\u5e73\u884c\u7247\u6bb5 (s, t)\uff0c\u5176\u4e2d\u5305\u542b\u539f\u59cb\u8a9e\u8a00\u4e2d\u7684\u7247\u6bb5\u53ca\u5176\u5728\u76ee\u6a19\u8a9e\u8a00\u4e2d\u7684\u7ffb\u8b6f\u3002\u6b64\u5916\uff0cCAT \u5de5\u5177\u5305\u542b\u6a21\u7cca\u6bd4\u5c0d\u4fee\u5fa9 (FMR) \u6280\u8853\uff0c\u8a72\u6280\u8853\u5c07\u81ea\u52d5\u4f7f\u7528 TM \u4e2d\u7684\u5e73\u884c\u7247\u6bb5\uff0c\u5efa\u7acb\u5305\u542b\u539f\u59cb\u7248\u672c\u4fee\u6539\u7248\u672c\u7684\u65b0 TM \u9805\u76ee\uff0c\u4e26\u8003\u616e\u5230\u5b83\u5c07\u662f s' \u7684\u7ffb\u8b6f\u3002\u5927\u591a\u6578 FMR \u6280\u8853\u4f7f\u7528\u6a5f\u5668\u7ffb\u8b6f\u4f86\u300c\u4fee\u5fa9\u300d\u5fc5\u9808\u4fee\u6539\u7684\u90a3\u4e9b\u5b57\u8a5e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u5c0d\u65bc\u90a3\u4e9b\u9328\u5b9a\u7684\u5b57\u8a5e\u7684\u5927\u90e8\u5206\uff0c\u6211\u5011\u53ef\u4ee5\u4f7f\u7528\u5176\u4ed6\u57fa\u65bc\u6a5f\u5668\u5b78\u7fd2\u65b9\u6cd5\u7684\u6280\u8853\uff0c\u4f8b\u5982 Word2Vec\u3001BERT\uff0c\u751a\u81f3 ChatGPT\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c55\u793a\u5c0d\u65bc\u9075\u5faa\u9023\u7e8c\u8a5e\u888b (CBOW) \u5178\u7bc4\u7684\u9328\u5b9a\u5b57\u8a5e\uff0cWord2Vec\u3001BERT \u548c GPT-4 \u53ef\u7528\u65bc\u9054\u6210\u8207\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f\u985e\u4f3c\u7684\u7d50\u679c\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u751a\u81f3\u53ef\u4ee5\u9054\u6210\u66f4\u597d\u7684\u7d50\u679c\uff0c\u7528\u65bc\u5c07\u9328\u5b9a\u5b57\u8a5e\u5f9e\u6cd5\u8a9e\u7ffb\u8b6f\u6210\u82f1\u8a9e\u3002", "author": "Richard Yue et.al.", "authors": "Richard Yue, John E. Ortega", "id": "2409.17939v1", "paper_url": "http://arxiv.org/abs/2409.17939v1", "repo": "null"}}