{"2409.16706": {"publish_time": "2024-09-25", "title": "Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image Translation", "paper_summary": "This paper proposes Pix2Next, a novel image-to-image translation framework\ndesigned to address the challenge of generating high-quality Near-Infrared\n(NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision\nFoundation Model (VFM) within an encoder-decoder architecture, incorporating\ncross-attention mechanisms to enhance feature integration. This design captures\ndetailed global representations and preserves essential spectral\ncharacteristics, treating RGB-to-NIR translation as more than a simple domain\ntransfer problem. A multi-scale PatchGAN discriminator ensures realistic image\ngeneration at various detail levels, while carefully designed loss functions\ncouple global context understanding with local feature preservation. We\nperformed experiments on the RANUS dataset to demonstrate Pix2Next's advantages\nin quantitative metrics and visual quality, improving the FID score by 34.81%\ncompared to existing methods. Furthermore, we demonstrate the practical utility\nof Pix2Next by showing improved performance on a downstream object detection\ntask using generated NIR data to augment limited real NIR datasets. The\nproposed approach enables the scaling up of NIR datasets without additional\ndata acquisition or annotation efforts, potentially accelerating advancements\nin NIR-based computer vision applications.", "paper_summary_zh": "\u672c\u6587\u63d0\u51fa Pix2Next\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u5f71\u50cf\u8f49\u63db\u67b6\u69cb\uff0c\u65e8\u5728\u89e3\u6c7a\u5f9e RGB \u8f38\u5165\u7522\u751f\u9ad8\u54c1\u8cea\u8fd1\u7d05\u5916\u7dda (NIR) \u5f71\u50cf\u7684\u6311\u6230\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u7de8\u78bc\u5668\u89e3\u78bc\u5668\u67b6\u69cb\u4e2d\u5229\u7528\u6700\u5148\u9032\u7684 Vision Foundation Model (VFM)\uff0c\u4e26\u7d50\u5408\u8de8\u6ce8\u610f\u529b\u6a5f\u5236\u4f86\u589e\u5f37\u7279\u5fb5\u6574\u5408\u3002\u9019\u7a2e\u8a2d\u8a08\u64f7\u53d6\u4e86\u8a73\u7d30\u7684\u5168\u5c40\u8868\u793a\uff0c\u4e26\u4fdd\u7559\u4e86\u5fc5\u8981\u7684\u983b\u8b5c\u7279\u6027\uff0c\u5c07 RGB \u8f49 NIR \u8f49\u63db\u8996\u70ba\u4e0d\u53ea\u662f\u55ae\u7d14\u7684\u7db2\u57df\u8f49\u79fb\u554f\u984c\u3002\u591a\u5c3a\u5ea6 PatchGAN \u8fa8\u8b58\u5668\u78ba\u4fdd\u5728\u5404\u7a2e\u7d30\u7bc0\u5c64\u7d1a\u4e2d\u7522\u751f\u903c\u771f\u7684\u5f71\u50cf\uff0c\u540c\u6642\u7cbe\u5fc3\u8a2d\u8a08\u7684\u640d\u5931\u51fd\u6578\u5c07\u5168\u5c40\u8108\u7d61\u7406\u89e3\u8207\u5c40\u90e8\u7279\u5fb5\u4fdd\u7559\u7d50\u5408\u8d77\u4f86\u3002\u6211\u5011\u5728 RANUS \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u4ee5\u5c55\u793a Pix2Next \u5728\u5b9a\u91cf\u6307\u6a19\u548c\u8996\u89ba\u54c1\u8cea\u65b9\u9762\u7684\u512a\u9ede\uff0c\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c07 FID \u5206\u6578\u63d0\u9ad8\u4e86 34.81%\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86 Pix2Next \u7684\u5be6\u969b\u6548\u7528\uff0c\u65b9\u6cd5\u662f\u5728\u4e0b\u6e38\u7269\u4ef6\u5075\u6e2c\u4efb\u52d9\u4e2d\u986f\u793a\u4f7f\u7528\u7522\u751f\u7684 NIR \u8cc7\u6599\u4f86\u64f4\u5145\u6709\u9650\u7684\u771f\u5be6 NIR \u8cc7\u6599\u96c6\uff0c\u5f9e\u800c\u63d0\u9ad8\u6548\u80fd\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u984d\u5916\u53d6\u5f97\u8cc7\u6599\u6216\u6a19\u8a3b\u7684\u60c5\u6cc1\u4e0b\u64f4\u5145 NIR \u8cc7\u6599\u96c6\uff0c\u9019\u6709\u53ef\u80fd\u52a0\u901f\u57fa\u65bc NIR \u7684\u96fb\u8166\u8996\u89ba\u61c9\u7528\u7a0b\u5f0f\u7684\u9032\u5c55\u3002", "author": "Youngwan Jin et.al.", "authors": "Youngwan Jin, Incheol Park, Hanbin Song, Hyeongjin Ju, Yagiz Nalcakan, Shiho Kim", "id": "2409.16706v1", "paper_url": "http://arxiv.org/abs/2409.16706v1", "repo": "null"}}