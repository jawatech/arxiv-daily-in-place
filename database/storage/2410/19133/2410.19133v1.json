{"2410.19133": {"publish_time": "2024-10-24", "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback", "paper_summary": "Learning from human feedback has enabled the alignment of language models\n(LMs) with human preferences. However, directly collecting human preferences\ncan be expensive, time-consuming, and can have high variance. An appealing\nalternative is to distill preferences from LMs as a source of synthetic\nannotations as they are more consistent, cheaper, and scale better than human\nannotation; however, they are also prone to biases and errors. In this work, we\nintroduce a routing framework that combines inputs from humans and LMs to\nachieve better annotation quality, while reducing the total cost of human\nannotation. The crux of our approach is to identify preference instances that\nwill benefit from human annotations. We formulate this as an optimization\nproblem: given a preference dataset and an evaluation metric, we train a\nperformance prediction model to predict a reward model's performance on an\narbitrary combination of human and LM annotations and employ a routing strategy\nthat selects a combination that maximizes predicted performance. We train the\nperformance prediction model on MultiPref, a new preference dataset with 10K\ninstances paired with human and LM labels. We show that the selected hybrid\nmixture of LM and direct human preferences using our routing framework achieves\nbetter reward model performance compared to using either one exclusively. We\nsimulate selective human preference collection on three other datasets and show\nthat our method generalizes well to all three. We analyze features from the\nrouting model to identify characteristics of instances that can benefit from\nhuman feedback, e.g., prompts with a moderate safety concern or moderate intent\ncomplexity. We release the dataset, annotation platform, and source code used\nin this study to foster more efficient and accurate preference collection in\nthe future.", "paper_summary_zh": "<paragraph>\u5f9e\u4eba\u985e\u56de\u994b\u4e2d\u5b78\u7fd2\u5df2\u80fd\u4f7f\u8a9e\u8a00\u6a21\u578b (LM) \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u7136\u800c\uff0c\u76f4\u63a5\u6536\u96c6\u4eba\u985e\u504f\u597d\u53ef\u80fd\u5f88\u6602\u8cb4\u3001\u8017\u6642\u4e14\u8b8a\u7570\u6027\u9ad8\u3002\u4e00\u500b\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u662f\u5f9e LM \u4e2d\u63d0\u53d6\u504f\u597d\uff0c\u4f5c\u70ba\u5408\u6210\u8a3b\u89e3\u7684\u4f86\u6e90\uff0c\u56e0\u70ba\u5b83\u5011\u6bd4\u4eba\u985e\u8a3b\u89e3\u66f4\u4e00\u81f4\u3001\u66f4\u4fbf\u5b9c\u4e14\u64f4\u5c55\u6027\u66f4\u597d\uff1b\u7136\u800c\uff0c\u5b83\u5011\u4e5f\u5bb9\u6613\u51fa\u73fe\u504f\u5dee\u548c\u932f\u8aa4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u8def\u7531\u6846\u67b6\uff0c\u7d50\u5408\u4eba\u985e\u548c LM \u7684\u8f38\u5165\uff0c\u4ee5\u5be6\u73fe\u66f4\u597d\u7684\u8a3b\u89e3\u54c1\u8cea\uff0c\u540c\u6642\u964d\u4f4e\u4eba\u985e\u8a3b\u89e3\u7684\u7e3d\u6210\u672c\u3002\u6211\u5011\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u8b58\u5225\u5c07\u53d7\u76ca\u65bc\u4eba\u985e\u8a3b\u89e3\u7684\u504f\u597d\u5be6\u4f8b\u3002\u6211\u5011\u5c07\u5176\u8868\u8ff0\u70ba\u4e00\u500b\u6700\u4f73\u5316\u554f\u984c\uff1a\u7d66\u5b9a\u4e00\u500b\u504f\u597d\u8cc7\u6599\u96c6\u548c\u4e00\u500b\u8a55\u4f30\u6307\u6a19\uff0c\u6211\u5011\u8a13\u7df4\u4e00\u500b\u6548\u80fd\u9810\u6e2c\u6a21\u578b\uff0c\u4ee5\u9810\u6e2c\u734e\u52f5\u6a21\u578b\u5728\u4eba\u985e\u548c LM \u8a3b\u89e3\u7684\u4efb\u610f\u7d44\u5408\u4e0a\u7684\u6548\u80fd\uff0c\u4e26\u63a1\u7528\u4e00\u7a2e\u8def\u7531\u7b56\u7565\uff0c\u9078\u64c7\u4e00\u500b\u7d44\u5408\uff0c\u4ee5\u6700\u5927\u5316\u9810\u6e2c\u6548\u80fd\u3002\u6211\u5011\u5728 MultiPref \u4e0a\u8a13\u7df4\u6548\u80fd\u9810\u6e2c\u6a21\u578b\uff0cMultiPref \u662f\u500b\u65b0\u7684\u504f\u597d\u8cc7\u6599\u96c6\uff0c\u5305\u542b 10K \u500b\u8207\u4eba\u985e\u548c LM \u6a19\u7c64\u914d\u5c0d\u7684\u5be6\u4f8b\u3002\u6211\u5011\u8b49\u660e\uff0c\u4f7f\u7528\u6211\u5011\u7684\u8def\u7531\u6846\u67b6\u9078\u64c7\u7684 LM \u548c\u76f4\u63a5\u4eba\u985e\u504f\u597d\u7684\u6df7\u5408\u6df7\u5408\uff0c\u8207\u50c5\u4f7f\u7528\u5176\u4e2d\u4e00\u7a2e\u76f8\u6bd4\uff0c\u53ef\u9054\u6210\u66f4\u597d\u7684\u734e\u52f5\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u5728\u5176\u4ed6\u4e09\u500b\u8cc7\u6599\u96c6\u4e0a\u6a21\u64ec\u9078\u64c7\u6027\u4eba\u985e\u504f\u597d\u6536\u96c6\uff0c\u4e26\u8b49\u660e\u6211\u5011\u7684\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u6982\u62ec\u5230\u6240\u6709\u9019\u4e09\u500b\u8cc7\u6599\u96c6\u3002\u6211\u5011\u5206\u6790\u8def\u7531\u6a21\u578b\u4e2d\u7684\u7279\u5fb5\uff0c\u4ee5\u8b58\u5225\u53ef\u5f9e\u4eba\u985e\u56de\u994b\u4e2d\u53d7\u76ca\u7684\u5be6\u4f8b\u7279\u5fb5\uff0c\u4f8b\u5982\uff0c\u5177\u6709\u9069\u5ea6\u5b89\u5168\u7591\u616e\u6216\u9069\u5ea6\u610f\u5716\u8907\u96dc\u6027\u7684\u63d0\u793a\u3002\u6211\u5011\u767c\u5e03\u672c\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\u3001\u8a3b\u89e3\u5e73\u53f0\u548c\u539f\u59cb\u78bc\uff0c\u4ee5\u4fc3\u9032\u672a\u4f86\u66f4\u6709\u6548\u7387\u4e14\u6e96\u78ba\u7684\u504f\u597d\u6536\u96c6\u3002</paragraph>", "author": "Lester James V. Miranda et.al.", "authors": "Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi", "id": "2410.19133v1", "paper_url": "http://arxiv.org/abs/2410.19133v1", "repo": "https://github.com/allenai/hybrid-preferences"}}