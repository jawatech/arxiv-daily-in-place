{"2408.05924": {"publish_time": "2024-08-12", "title": "Adapting a Foundation Model for Space-based Tasks", "paper_summary": "Foundation models, e.g., large language models, possess attributes of\nintelligence which offer promise to endow a robot with the contextual\nunderstanding necessary to navigate complex, unstructured tasks in the wild. In\nthe future of space robotics, we see three core challenges which motivate the\nuse of a foundation model adapted to space-based applications: 1) Scalability\nof ground-in-the-loop operations; 2) Generalizing prior knowledge to novel\nenvironments; and 3) Multi-modality in tasks and sensor data. Therefore, as a\nfirst-step towards building a foundation model for space-based applications, we\nautomatically label the AI4Mars dataset to curate a language annotated dataset\nof visual-question-answer tuples. We fine-tune a pretrained LLaVA checkpoint on\nthis dataset to endow a vision-language model with the ability to perform\nspatial reasoning and navigation on Mars' surface. In this work, we demonstrate\nthat 1) existing vision-language models are deficient visual reasoners in\nspace-based applications, and 2) fine-tuning a vision-language model on\nextraterrestrial data significantly improves the quality of responses even with\na limited training dataset of only a few thousand samples.", "paper_summary_zh": "\u57fa\u790e\u6a21\u578b\uff08\u4f8b\u5982\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u5177\u5099\u667a\u6167\u5c6c\u6027\uff0c\u6709\u671b\u8ce6\u4e88\u6a5f\u5668\u4eba\u5728\u91ce\u5916\u57f7\u884c\u8907\u96dc\u3001\u975e\u7d50\u69cb\u5316\u4efb\u52d9\u6240\u9700\u7684\u8108\u7d61\u7406\u89e3\u529b\u3002\u5728\u592a\u7a7a\u6a5f\u5668\u4eba\u7684\u672a\u4f86\uff0c\u6211\u5011\u770b\u5230\u4e09\u500b\u6838\u5fc3\u6311\u6230\uff0c\u4fc3\u4f7f\u6211\u5011\u4f7f\u7528\u9069\u7528\u65bc\u592a\u7a7a\u61c9\u7528\u7684\u57fa\u790e\u6a21\u578b\uff1a1) \u9589\u74b0\u5730\u9762\u64cd\u4f5c\u7684\u53ef\u64f4\u5145\u6027\uff1b2) \u5c07\u5148\u9a57\u77e5\u8b58\u6982\u62ec\u5230\u65b0\u74b0\u5883\uff1b3) \u4efb\u52d9\u548c\u611f\u6e2c\u5668\u8cc7\u6599\u7684\u591a\u6a21\u614b\u3002\u56e0\u6b64\uff0c\u4f5c\u70ba\u5efa\u7acb\u592a\u7a7a\u61c9\u7528\u57fa\u790e\u6a21\u578b\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u5011\u81ea\u52d5\u6a19\u8a18 AI4Mars \u8cc7\u6599\u96c6\uff0c\u4ee5\u6574\u7406\u4e00\u500b\u5e36\u6709\u8996\u89ba\u554f\u984c\u89e3\u7b54\u5143\u7d44\u7684\u8a9e\u8a00\u8a3b\u89e3\u8cc7\u6599\u96c6\u3002\u6211\u5011\u5fae\u8abf LLaVA \u9810\u8a13\u7df4\u6aa2\u67e5\u9ede\uff0c\u4f7f\u7528\u6b64\u8cc7\u6599\u96c6\u8ce6\u4e88\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u5728\u706b\u661f\u8868\u9762\u57f7\u884c\u7a7a\u9593\u63a8\u7406\u548c\u5c0e\u822a\u7684\u80fd\u529b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8b49\u660e 1) \u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u5728\u592a\u7a7a\u61c9\u7528\u4e2d\u662f\u7f3a\u4e4f\u8996\u89ba\u63a8\u7406\u80fd\u529b\u7684\uff0c\u4ee5\u53ca 2) \u5728\u7570\u661f\u8cc7\u6599\u4e0a\u5fae\u8abf\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u5373\u4f7f\u53ea\u6709\u6578\u5343\u500b\u6a23\u672c\u7684\u6709\u9650\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u4e5f\u80fd\u986f\u8457\u63d0\u5347\u56de\u61c9\u54c1\u8cea\u3002", "author": "Matthew Foutter et.al.", "authors": "Matthew Foutter, Praneet Bhoj, Rohan Sinha, Amine Elhafsi, Somrita Banerjee, Christopher Agia, Justin Kruger, Tommaso Guffanti, Daniele Gammelli, Simone D'Amico, Marco Pavone", "id": "2408.05924v1", "paper_url": "http://arxiv.org/abs/2408.05924v1", "repo": "null"}}