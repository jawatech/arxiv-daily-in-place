{"2406.09272": {"publish_time": "2024-06-13", "title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos", "paper_summary": "Generating realistic audio for human interactions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model\noutperforms an array of existing methods, allows controllable generation of the\nambient sound, and even shows promise for generalizing to computer graphics\ngame clips. Overall, our work is the first to focus video-to-audio generation\nfaithfully on the observed visual content despite training from uncurated clips\nwith natural background sounds.", "paper_summary_zh": "\u751f\u6210\u903c\u771f\u7684\u4eba\u7c7b\u4e92\u52a8\u97f3\u9891\u5bf9\u4e8e\u8bb8\u591a\u5e94\u7528\u975e\u5e38\u91cd\u8981\uff0c\u4f8b\u5982\u4e3a\u7535\u5f71\u6216\u865a\u62df\u73b0\u5b9e\u6e38\u620f\u521b\u5efa\u97f3\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u9690\u5f0f\u5730\u5047\u5b9a\u89c6\u9891\u548c\u97f3\u9891\u4e4b\u95f4\u5b8c\u5168\u5bf9\u5e94\uff0c\u4f46\u8bb8\u591a\u58f0\u97f3\u53d1\u751f\u5728\u5c4f\u5e55\u5916\uff0c\u4e0e\u89c6\u89c9\u6548\u679c\u51e0\u4e4e\u6216\u5b8c\u5168\u6ca1\u6709\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u5bfc\u81f4\u5728\u6d4b\u8bd5\u65f6\u51fa\u73b0\u4e0d\u53d7\u63a7\u5236\u7684\u73af\u5883\u58f0\u97f3\u6216\u5e7b\u89c9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u73af\u5883\u611f\u77e5\u97f3\u9891\u751f\u6210\u6a21\u578b AV-LDM\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u97f3\u9891\u8c03\u8282\u673a\u5236\uff0c\u4ee5\u5b66\u4e60\u4ece\u91ce\u5916\u8bad\u7ec3\u89c6\u9891\u4e2d\u7684\u73af\u5883\u80cc\u666f\u58f0\u97f3\u4e2d\u89e3\u5f00\u524d\u666f\u52a8\u4f5c\u58f0\u97f3\u3002\u7ed9\u5b9a\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u58f0\u89c6\u9891\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u521b\u5efa\u4e0e\u89c6\u89c9\u5185\u5bb9\u5728\u8bed\u4e49\u548c\u65f6\u95f4\u4e0a\u90fd\u5339\u914d\u7684\u97f3\u9891\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u91ce\u5916\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u6570\u636e\u96c6 Ego4D \u548c EPIC-KITCHENS \u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u5141\u8bb8\u53ef\u63a7\u751f\u6210\u73af\u5883\u58f0\u97f3\uff0c\u751a\u81f3\u663e\u793a\u51fa\u63a8\u5e7f\u5230\u8ba1\u7b97\u673a\u56fe\u5f62\u6e38\u620f\u526a\u8f91\u7684\u5e0c\u671b\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u9996\u6b21\u4e13\u6ce8\u4e8e\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\uff0c\u5fe0\u5b9e\u4e8e\u89c2\u5bdf\u5230\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u5c3d\u7ba1\u662f\u4ece\u5177\u6709\u81ea\u7136\u80cc\u666f\u58f0\u97f3\u7684\u672a\u7ecf\u6574\u7406\u7684\u526a\u8f91\u4e2d\u8fdb\u884c\u8bad\u7ec3\u7684\u3002", "author": "Changan Chen et.al.", "authors": "Changan Chen, Puyuan Peng, Ami Baid, Zihui Xue, Wei-Ning Hsu, David Harwarth, Kristen Grauman", "id": "2406.09272v1", "paper_url": "http://arxiv.org/abs/2406.09272v1", "repo": "null"}}