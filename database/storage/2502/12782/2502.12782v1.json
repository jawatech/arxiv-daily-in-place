{"2502.12782": {"publish_time": "2025-02-18", "title": "VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation", "paper_summary": "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps://github.com/VidCapBench/VidCapBench.", "paper_summary_zh": "\u53ef\u63a7\u5236\u6587\u672c\u5230\u5f71\u7247 (T2V) \u6a21\u578b\u7684\u8a13\u7df4\u6975\u5ea6\u4ef0\u8cf4\u5f71\u7247\u548c\u5b57\u5e55\u4e4b\u9593\u7684\u5c0d\u9f4a\uff0c\u4f46\u73fe\u6709\u7814\u7a76\u9bae\u5c11\u5c07\u5f71\u7247\u5b57\u5e55\u8a55\u4f30\u8207 T2V \u751f\u6210\u8a55\u4f30\u9023\u7d50\u8d77\u4f86\u3002\u672c\u6587\u4ecb\u7d39 VidCapBench\uff0c\u9019\u662f\u4e00\u7a2e\u5c08\u9580\u70ba T2V \u751f\u6210\u8a2d\u8a08\u7684\u5f71\u7247\u5b57\u5e55\u8a55\u4f30\u67b6\u69cb\uff0c\u8207\u4efb\u4f55\u7279\u5b9a\u7684\u5b57\u5e55\u683c\u5f0f\u7121\u95dc\u3002VidCapBench \u63a1\u7528\u8cc7\u6599\u6a19\u8a3b\u6d41\u7a0b\uff0c\u7d50\u5408\u5c08\u5bb6\u6a21\u578b\u6a19\u8a18\u548c\u4eba\u5de5\u5fae\u8abf\uff0c\u5c07\u6bcf\u500b\u6536\u96c6\u5230\u7684\u5f71\u7247\u8207\u6db5\u84cb\u5f71\u7247\u7f8e\u5b78\u3001\u5167\u5bb9\u3001\u52d5\u4f5c\u548c\u7269\u7406\u5b9a\u5f8b\u7b49\u95dc\u9375\u8cc7\u8a0a\u95dc\u806f\u8d77\u4f86\u3002VidCapBench \u63a5\u8457\u5c07\u9019\u4e9b\u95dc\u9375\u8cc7\u8a0a\u5c6c\u6027\u5206\u5272\u6210\u53ef\u81ea\u52d5\u8a55\u4f30\u548c\u53ef\u624b\u52d5\u8a55\u4f30\u7684\u5b50\u96c6\uff0c\u4ee5\u6eff\u8db3\u654f\u6377\u958b\u767c\u7684\u5feb\u901f\u8a55\u4f30\u9700\u6c42\u548c\u5168\u9762\u9a57\u8b49\u7684\u6e96\u78ba\u6027\u8981\u6c42\u3002\u900f\u904e\u8a55\u4f30\u8a31\u591a\u6700\u5148\u9032\u7684\u5b57\u5e55\u6a21\u578b\uff0c\u6211\u5011\u8b49\u660e\u4e86 VidCapBench \u8207\u73fe\u6709\u7684\u5f71\u7247\u5b57\u5e55\u8a55\u4f30\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u512a\u7570\u7684\u7a69\u5b9a\u6027\u548c\u5168\u9762\u6027\u3002\u4f7f\u7528\u73fe\u6210\u7684 T2V \u6a21\u578b\u9a57\u8b49\u986f\u793a\uff0cVidCapBench \u5f97\u5206\u8207 T2V \u54c1\u8cea\u8a55\u4f30\u6307\u6a19\u4e4b\u9593\u5b58\u5728\u986f\u8457\u7684\u6b63\u76f8\u95dc\uff0c\u9019\u8868\u793a VidCapBench \u53ef\u4ee5\u70ba\u8a13\u7df4 T2V \u6a21\u578b\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u6307\u5c0e\u3002\u5c08\u6848\u53ef\u65bc https://github.com/VidCapBench/VidCapBench \u53d6\u5f97\u3002", "author": "Xinlong Chen et.al.", "authors": "Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, Tieniu Tan", "id": "2502.12782v1", "paper_url": "http://arxiv.org/abs/2502.12782v1", "repo": "null"}}