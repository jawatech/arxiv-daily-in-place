{"2412.09104": {"publish_time": "2024-12-12", "title": "In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning", "paper_summary": "Offline preference-based reinforcement learning (PbRL) typically operates in\ntwo phases: first, use human preferences to learn a reward model and annotate\nrewards for a reward-free offline dataset; second, learn a policy by optimizing\nthe learned reward via offline RL. However, accurately modeling step-wise\nrewards from trajectory-level preference feedback presents inherent challenges.\nThe reward bias introduced, particularly the overestimation of predicted\nrewards, leads to optimistic trajectory stitching, which undermines the\npessimism mechanism critical to the offline RL phase. To address this\nchallenge, we propose In-Dataset Trajectory Return Regularization (DTR) for\noffline PbRL, which leverages conditional sequence modeling to mitigate the\nrisk of learning inaccurate trajectory stitching under reward bias.\nSpecifically, DTR employs Decision Transformer and TD-Learning to strike a\nbalance between maintaining fidelity to the behavior policy with high\nin-dataset trajectory returns and selecting optimal actions based on high\nreward labels. Additionally, we introduce an ensemble normalization technique\nthat effectively integrates multiple reward models, balancing the tradeoff\nbetween reward differentiation and accuracy. Empirical evaluations on various\nbenchmarks demonstrate the superiority of DTR over other state-of-the-art\nbaselines", "paper_summary_zh": "\u96e2\u7dda\u57fa\u65bc\u504f\u597d\u7684\u5f37\u5316\u5b78\u7fd2 (PbRL) \u901a\u5e38\u5206\u70ba\u5169\u500b\u968e\u6bb5\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u4eba\u985e\u504f\u597d\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\uff0c\u4e26\u70ba\u7121\u734e\u52f5\u96e2\u7dda\u8cc7\u6599\u96c6\u8a3b\u89e3\u734e\u52f5\uff1b\u5176\u6b21\uff0c\u901a\u904e\u96e2\u7dda RL \u6700\u4f73\u5316\u5b78\u7fd2\u7684\u734e\u52f5\u4f86\u5b78\u7fd2\u7b56\u7565\u3002\u7136\u800c\uff0c\u5f9e\u8ecc\u8de1\u7d1a\u5225\u504f\u597d\u56de\u994b\u4e2d\u6e96\u78ba\u5efa\u6a21\u9010\u6b65\u734e\u52f5\u6703\u5e36\u4f86\u56fa\u6709\u7684\u6311\u6230\u3002\u5f15\u5165\u7684\u734e\u52f5\u504f\u5dee\uff0c\u7279\u5225\u662f\u5c0d\u9810\u6e2c\u734e\u52f5\u7684\u9ad8\u4f30\uff0c\u6703\u5c0e\u81f4\u6a02\u89c0\u7684\u8ecc\u8de1\u62fc\u63a5\uff0c\u9019\u6703\u7834\u58de\u5c0d\u96e2\u7dda RL \u968e\u6bb5\u81f3\u95dc\u91cd\u8981\u7684\u60b2\u89c0\u6a5f\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u96e2\u7dda PbRL \u7684\u8cc7\u6599\u96c6\u5167\u8ecc\u8de1\u56de\u5831\u6b63\u5247\u5316 (DTR)\uff0c\u5b83\u5229\u7528\u689d\u4ef6\u5e8f\u5217\u5efa\u6a21\u4f86\u964d\u4f4e\u5728\u734e\u52f5\u504f\u5dee\u4e0b\u5b78\u7fd2\u4e0d\u6e96\u78ba\u8ecc\u8de1\u62fc\u63a5\u7684\u98a8\u96aa\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cDTR \u4f7f\u7528\u6c7a\u7b56\u8f49\u63db\u5668\u548c TD \u5b78\u7fd2\u4f86\u5728\u4fdd\u6301\u5c0d\u884c\u70ba\u7b56\u7565\u7684\u5fe0\u8aa0\u5ea6\u8207\u9ad8\u8cc7\u6599\u96c6\u8ecc\u8de1\u56de\u5831\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\uff0c\u4e26\u6839\u64da\u9ad8\u734e\u52f5\u6a19\u7c64\u9078\u64c7\u6700\u4f73\u884c\u52d5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u96c6\u5408\u6b63\u898f\u5316\u6280\u8853\uff0c\u5b83\u6709\u6548\u5730\u6574\u5408\u4e86\u591a\u500b\u734e\u52f5\u6a21\u578b\uff0c\u5e73\u8861\u4e86\u734e\u52f5\u5340\u5206\u5ea6\u548c\u6e96\u78ba\u6027\u4e4b\u9593\u7684\u6b0a\u8861\u3002\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u7684\u7d93\u9a57\u8a55\u4f30\u8b49\u660e\u4e86 DTR \u512a\u65bc\u5176\u4ed6\u6700\u5148\u9032\u7684\u57fa\u7dda", "author": "Songjun Tu et.al.", "authors": "Songjun Tu, Jingbo Sun, Qichao Zhang, Yaocheng Zhang, Jia Liu, Ke Chen, Dongbin Zhao", "id": "2412.09104v1", "paper_url": "http://arxiv.org/abs/2412.09104v1", "repo": "null"}}