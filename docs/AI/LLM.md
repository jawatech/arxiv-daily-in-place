
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-19**|**Autellix: An Efficient Serving Engine for LLM Agents as General Programs**|Michael Luo et.al.|[2502.13965v1](http://arxiv.org/abs/2502.13965v1)|null|
|**2025-02-19**|**A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects**|Arjun Gupta et.al.|[2502.13964v1](http://arxiv.org/abs/2502.13964v1)|null|
|**2025-02-19**|**MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads**|Weihao Liu et.al.|[2502.13963v1](http://arxiv.org/abs/2502.13963v1)|null|
|**2025-02-19**|**Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering**|William Jurayj et.al.|[2502.13962v1](http://arxiv.org/abs/2502.13962v1)|null|
|**2025-02-19**|**LIDDIA: Language-based Intelligent Drug Discovery Agent**|Reza Averly et.al.|[2502.13959v1](http://arxiv.org/abs/2502.13959v1)|null|
|**2025-02-19**|**RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision**|Guangzhi Xiong et.al.|[2502.13957v1](http://arxiv.org/abs/2502.13957v1)|null|
|**2025-02-19**|**Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition**|Jingwang Huang et.al.|[2502.13954v1](http://arxiv.org/abs/2502.13954v1)|[link](https://github.com/201983290498/lddu_mmer)|
|**2025-02-19**|**Neurosymbolic artificial intelligence via large language models and coherence-driven inference**|Steve Huntsman et.al.|[2502.13953v1](http://arxiv.org/abs/2502.13953v1)|null|
|**2025-02-19**|**Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region**|Chak Tou Leong et.al.|[2502.13946v1](http://arxiv.org/abs/2502.13946v1)|null|
|**2025-02-19**|**AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence**|Yuliang Liu et.al.|[2502.13943v1](http://arxiv.org/abs/2502.13943v1)|null|
|**2025-02-19**|**Continually Learning Structured Visual Representations via Network Refinement with Rerelation**|Zeki Doruk Erden et.al.|[2502.13935v1](http://arxiv.org/abs/2502.13935v1)|null|
|**2025-02-19**|**Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images**|Shengguang Wu et.al.|[2502.13928v1](http://arxiv.org/abs/2502.13928v1)|null|
|**2025-02-19**|**Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?**|Xiaochen Wang et.al.|[2502.13925v1](http://arxiv.org/abs/2502.13925v1)|null|
|**2025-02-19**|**Qwen2.5-VL Technical Report**|Shuai Bai et.al.|[2502.13923v1](http://arxiv.org/abs/2502.13923v1)|null|
|**2025-02-19**|**LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization**|Guanzheng Chen et.al.|[2502.13922v1](http://arxiv.org/abs/2502.13922v1)|[link](https://github.com/DAMO-NLP-SG/LongPO)|
|**2025-02-19**|**Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health**|Xingbo Wang et.al.|[2502.13920v1](http://arxiv.org/abs/2502.13920v1)|null|
|**2025-02-19**|**TESS 2: A Large-Scale Generalist Diffusion Language Model**|Jaesung Tae et.al.|[2502.13917v1](http://arxiv.org/abs/2502.13917v1)|null|
|**2025-02-19**|**How Do LLMs Perform Two-Hop Reasoning in Context?**|Tianyu Guo et.al.|[2502.13913v1](http://arxiv.org/abs/2502.13913v1)|null|
|**2025-02-19**|**Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?**|Sein Kim et.al.|[2502.13909v1](http://arxiv.org/abs/2502.13909v1)|null|
|**2025-02-19**|**Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference**|Saksham Kiroriwal et.al.|[2502.13905v1](http://arxiv.org/abs/2502.13905v1)|null|
|**2025-02-19**|**DataSciBench: An LLM Agent Benchmark for Data Science**|Dan Zhang et.al.|[2502.13897v1](http://arxiv.org/abs/2502.13897v1)|null|
|**2025-02-19**|**PSCon: Toward Conversational Product Search**|Jie Zou et.al.|[2502.13881v1](http://arxiv.org/abs/2502.13881v1)|null|
|**2025-02-19**|**MEX: Memory-efficient Approach to Referring Multi-Object Tracking**|Huu-Thien Tran et.al.|[2502.13875v1](http://arxiv.org/abs/2502.13875v1)|null|
|**2025-02-19**|**NVR: Vector Runahead on NPUs for Sparse Memory Access**|Hui Wang et.al.|[2502.13873v1](http://arxiv.org/abs/2502.13873v1)|null|
|**2025-02-19**|**SPEX: Scaling Feature Interaction Explanations for LLMs**|Justin Singh Kang et.al.|[2502.13870v1](http://arxiv.org/abs/2502.13870v1)|[link](https://github.com/basics-lab/spectral-explain)|
|**2025-02-19**|**DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue**|Feiyuan Zhang et.al.|[2502.13847v1](http://arxiv.org/abs/2502.13847v1)|null|
|**2025-02-19**|**Enhancing LLM-Based Recommendations Through Personalized Reasoning**|Jiahao Liu et.al.|[2502.13845v1](http://arxiv.org/abs/2502.13845v1)|null|
|**2025-02-19**|**Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents**|Jiahao Liu et.al.|[2502.13843v1](http://arxiv.org/abs/2502.13843v1)|null|
|**2025-02-19**|**Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking**|Yilong Chen et.al.|[2502.13842v1](http://arxiv.org/abs/2502.13842v1)|null|
|**2025-02-19**|**Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models**|Peter Carragher et.al.|[2502.13836v1](http://arxiv.org/abs/2502.13836v1)|null|
|**2025-02-19**|**Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning**|Zenan Li et.al.|[2502.13834v1](http://arxiv.org/abs/2502.13834v1)|null|
|**2025-02-19**|**Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning**|Aleksander Ficek et.al.|[2502.13820v1](http://arxiv.org/abs/2502.13820v1)|null|
|**2025-02-19**|**On the Duality between Gradient Transformations and Adapters**|Lucas Torroba-Hennigen et.al.|[2502.13811v1](http://arxiv.org/abs/2502.13811v1)|null|
|**2025-02-19**|**LESA: Learnable LLM Layer Scaling-Up**|Yifei Yang et.al.|[2502.13794v1](http://arxiv.org/abs/2502.13794v1)|null|
|**2025-02-19**|**From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions**|Nathanaël Carraz Rakotonirina et.al.|[2502.13791v1](http://arxiv.org/abs/2502.13791v1)|null|
|**2025-02-19**|**Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics**|Matthew Wood et.al.|[2502.13785v1](http://arxiv.org/abs/2502.13785v1)|null|
|**2025-02-19**|**Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions**|Beatrice Savoldi et.al.|[2502.13780v1](http://arxiv.org/abs/2502.13780v1)|null|
|**2025-02-19**|**Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization**|Jiaqi Li et.al.|[2502.13778v1](http://arxiv.org/abs/2502.13778v1)|null|
|**2025-02-19**|**EHOP: A Dataset of Everyday NP-Hard Optimization Problems**|Alex Duchnowski et.al.|[2502.13776v1](http://arxiv.org/abs/2502.13776v1)|null|
|**2025-02-19**|**VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare**|Anudeex Shetty et.al.|[2502.13775v1](http://arxiv.org/abs/2502.13775v1)|null|
|**2025-02-19**|**A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem**|Juan A. Aledo et.al.|[2502.13769v1](http://arxiv.org/abs/2502.13769v1)|null|
|**2025-02-19**|**AI Software Engineer: Programming with Trust**|Abhik Roychoudhury et.al.|[2502.13767v1](http://arxiv.org/abs/2502.13767v1)|null|
|**2025-02-19**|**GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking**|Florian Schneider et.al.|[2502.13766v1](http://arxiv.org/abs/2502.13766v1)|null|
|**2025-02-19**|**SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning**|Renxi Wang et.al.|[2502.13753v1](http://arxiv.org/abs/2502.13753v1)|null|
|**2025-02-19**|**RobustX: Robust Counterfactual Explanations Made Easy**|Junqi Jiang et.al.|[2502.13751v1](http://arxiv.org/abs/2502.13751v1)|null|
|**2025-02-19**|**Inference of Abstraction for Grounded Predicate Logic**|Hiroyuki Kido et.al.|[2502.13743v1](http://arxiv.org/abs/2502.13743v1)|null|
|**2025-02-19**|**Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding**|Keqin Peng et.al.|[2502.13738v1](http://arxiv.org/abs/2502.13738v1)|null|
|**2025-02-19**|**Robust Counterfactual Inference in Markov Decision Processes**|Jessica Lally et.al.|[2502.13731v1](http://arxiv.org/abs/2502.13731v1)|null|
|**2025-02-19**|**Secure Federated Data Distillation**|Marco Arazzi et.al.|[2502.13728v1](http://arxiv.org/abs/2502.13728v1)|null|
|**2025-02-19**|**Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method**|Juyuan Zhang et.al.|[2502.13725v1](http://arxiv.org/abs/2502.13725v1)|null|
|**2025-02-19**|**Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values**|Hongbo Zhang et.al.|[2502.13723v1](http://arxiv.org/abs/2502.13723v1)|null|
|**2025-02-19**|**Learning Novel Transformer Architecture for Time-series Forecasting**|Juyuan Zhang et.al.|[2502.13721v1](http://arxiv.org/abs/2502.13721v1)|null|
|**2025-02-19**|**TrustRAG: An Information Assistant with Retrieval Augmented Generation**|Yixing Fan et.al.|[2502.13719v1](http://arxiv.org/abs/2502.13719v1)|null|
|**2025-02-19**|**Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis**|Chengyan Wu et.al.|[2502.13718v1](http://arxiv.org/abs/2502.13718v1)|null|
|**2025-02-19**|**Causes and Strategies in Multiagent Systems**|Sylvia S. Kerkhove et.al.|[2502.13701v1](http://arxiv.org/abs/2502.13701v1)|null|
|**2025-02-19**|**Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora**|Tristan Karch et.al.|[2502.13691v1](http://arxiv.org/abs/2502.13691v1)|null|
|**2025-02-19**|**MoM: Linear Sequence Modeling with Mixture-of-Memories**|Jusen Du et.al.|[2502.13685v1](http://arxiv.org/abs/2502.13685v1)|null|
|**2025-02-19**|**An LLM-based Agent for Reliable Docker Environment Configuration**|Ruida Hu et.al.|[2502.13681v1](http://arxiv.org/abs/2502.13681v1)|null|
|**2025-02-19**|**SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation**|Song Duong et.al.|[2502.13674v1](http://arxiv.org/abs/2502.13674v1)|null|
|**2025-02-19**|**PeerQA: A Scientific Question Answering Dataset from Peer Reviews**|Tim Baumgärtner et.al.|[2502.13668v1](http://arxiv.org/abs/2502.13668v1)|null|
|**2025-02-19**|**Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models**|Liyang He et.al.|[2502.13656v1](http://arxiv.org/abs/2502.13656v1)|null|
|**2025-02-19**|**C2T: A Classifier-Based Tree Construction Method in Speculative Decoding**|Feiye Huo et.al.|[2502.13652v1](http://arxiv.org/abs/2502.13652v1)|null|
|**2025-02-19**|**Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs**|Youna Kim et.al.|[2502.13648v1](http://arxiv.org/abs/2502.13648v1)|null|
|**2025-02-19**|**Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh**|Nurkhan Laiyk et.al.|[2502.13647v1](http://arxiv.org/abs/2502.13647v1)|null|
|**2025-02-19**|**D.Va: Validate Your Demonstration First Before You Use It**|Qi Zhang et.al.|[2502.13646v1](http://arxiv.org/abs/2502.13646v1)|null|
|**2025-02-19**|**Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks**|Ori Shapira et.al.|[2502.13645v1](http://arxiv.org/abs/2502.13645v1)|null|
|**2025-02-19**|**Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts**|Maiya Goloburda et.al.|[2502.13640v1](http://arxiv.org/abs/2502.13640v1)|null|
|**2025-02-19**|**Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks**|Julian Vexler et.al.|[2502.13638v1](http://arxiv.org/abs/2502.13638v1)|null|
|**2025-02-19**|**Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization**|Or Raphael Bidusa et.al.|[2502.13632v1](http://arxiv.org/abs/2502.13632v1)|null|
|**2025-02-19**|**Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection**|Darpan Aswal et.al.|[2502.13628v1](http://arxiv.org/abs/2502.13628v1)|null|
|**2025-02-19**|**REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models**|DongGeon Lee et.al.|[2502.13622v1](http://arxiv.org/abs/2502.13622v1)|null|
|**2025-02-19**|**Decentralized Planning Using Probabilistic Hyperproperties**|Francesco Pontiggia et.al.|[2502.13621v1](http://arxiv.org/abs/2502.13621v1)|null|
|**2025-02-19**|**Complex Ontology Matching with Large Language Model Embeddings**|Guilherme Sousa et.al.|[2502.13619v1](http://arxiv.org/abs/2502.13619v1)|null|
|**2025-02-19**|**LaVCa: LLM-assisted Visual Cortex Captioning**|Takuya Matsuyama et.al.|[2502.13606v1](http://arxiv.org/abs/2502.13606v1)|null|
|**2025-02-19**|**BeamLoRA: Beam-Constraint Low-Rank Adaptation**|Naibin Gu et.al.|[2502.13604v1](http://arxiv.org/abs/2502.13604v1)|null|
|**2025-02-19**|**Efficient Safety Retrofitting Against Jailbreaking for LLMs**|Dario Garcia-Gasulla et.al.|[2502.13603v1](http://arxiv.org/abs/2502.13603v1)|null|
|**2025-02-19**|**MMTEB: Massive Multilingual Text Embedding Benchmark**|Kenneth Enevoldsen et.al.|[2502.13595v1](http://arxiv.org/abs/2502.13595v1)|null|
|**2025-02-19**|**Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints**|Nicolò Penzo et.al.|[2502.13592v1](http://arxiv.org/abs/2502.13592v1)|null|
|**2025-02-19**|**Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation**|Peiwen Yuan et.al.|[2502.13576v1](http://arxiv.org/abs/2502.13576v1)|null|
|**2025-02-19**|**Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning**|Yan Yu et.al.|[2502.13569v1](http://arxiv.org/abs/2502.13569v1)|null|
|**2025-02-19**|**LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation**|Xin Li et.al.|[2502.13568v1](http://arxiv.org/abs/2502.13568v1)|null|
|**2025-02-19**|**Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs**|Joonatan Laato et.al.|[2502.13566v1](http://arxiv.org/abs/2502.13566v1)|null|
|**2025-02-19**|**PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models**|Guangwei Li et.al.|[2502.13564v1](http://arxiv.org/abs/2502.13564v1)|null|
|**2025-02-19**|**Are Large Language Models In-Context Graph Learners?**|Jintang Li et.al.|[2502.13562v1](http://arxiv.org/abs/2502.13562v1)|null|
|**2025-02-19**|**Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs**|Yushi Feng et.al.|[2502.13555v1](http://arxiv.org/abs/2502.13555v1)|null|
|**2025-02-19**|**STaR-SQL: Self-Taught Reasoner for Text-to-SQL**|Mingqian He et.al.|[2502.13550v1](http://arxiv.org/abs/2502.13550v1)|null|
|**2025-02-19**|**Detecting Linguistic Bias in Government Documents Using Large language Models**|Milena de Swart et.al.|[2502.13548v1](http://arxiv.org/abs/2502.13548v1)|null|
|**2025-02-19**|**From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN**|Peiwen Yuan et.al.|[2502.13544v1](http://arxiv.org/abs/2502.13544v1)|null|
|**2025-02-19**|**Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference**|Qingfa Xiao et.al.|[2502.13542v1](http://arxiv.org/abs/2502.13542v1)|null|
|**2025-02-19**|**Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models**|Jun Zhang et.al.|[2502.13533v1](http://arxiv.org/abs/2502.13533v1)|null|
|**2025-02-19**|**Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking**|Yanzeng Li et.al.|[2502.13527v1](http://arxiv.org/abs/2502.13527v1)|null|
|**2025-02-19**|**MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis**|Wei Dai et.al.|[2502.13524v1](http://arxiv.org/abs/2502.13524v1)|[link](https://github.com/anthonyweidai/MobileViM_3D)|
|**2025-02-19**|**A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment**|Khalid N. Elmadani et.al.|[2502.13520v1](http://arxiv.org/abs/2502.13520v1)|null|
|**2025-02-19**|**MILE: Model-based Intervention Learning**|Yigit Korkmaz et.al.|[2502.13519v1](http://arxiv.org/abs/2502.13519v1)|null|
|**2025-02-19**|**SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin**|Hao Yi et.al.|[2502.13516v1](http://arxiv.org/abs/2502.13516v1)|null|
|**2025-02-19**|**Shall Your Data Strategy Work? Perform a Swift Study**|Minlong Peng et.al.|[2502.13514v1](http://arxiv.org/abs/2502.13514v1)|null|
|**2025-02-19**|**Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion**|Shuai Niu et.al.|[2502.13509v1](http://arxiv.org/abs/2502.13509v1)|null|
|**2025-02-19**|**PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference**|Burc Gokden et.al.|[2502.13502v1](http://arxiv.org/abs/2502.13502v1)|null|
|**2025-02-19**|**Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in Ecommerce Web Components Generated by LLMs**|Ziwei Chen et.al.|[2502.13499v1](http://arxiv.org/abs/2502.13499v1)|null|
|**2025-02-19**|**Towards Geo-Culturally Grounded LLM Generations**|Piyawat Lertvittayakumjorn et.al.|[2502.13497v1](http://arxiv.org/abs/2502.13497v1)|null|

#### Abstracts
##### **Autellix: An Efficient Serving Engine for LLM Agents as General Programs**
2502.13965v1 by Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica

Large language model (LLM) applications are evolving beyond simple chatbots
into dynamic, general-purpose agentic programs, which scale LLM calls and
output tokens to help AI agents reason, explore, and solve complex tasks.
However, existing LLM serving systems ignore dependencies between programs and
calls, missing significant opportunities for optimization. Our analysis reveals
that programs submitted to LLM serving engines experience long cumulative wait
times, primarily due to head-of-line blocking at both the individual LLM
request and the program. To address this, we introduce Autellix, an LLM serving
system that treats programs as first-class citizens to minimize their
end-to-end latencies. Autellix intercepts LLM calls submitted by programs,
enriching schedulers with program-level context. We propose two scheduling
algorithms-for single-threaded and distributed programs-that preempt and
prioritize LLM calls based on their programs' previously completed calls. Our
evaluation demonstrates that across diverse LLMs and agentic workloads,
Autellix improves throughput of programs by 4-15x at the same latency compared
to state-of-the-art systems, such as vLLM.

摘要：大型語言模型 (LLM) 應用正從簡單的聊天機器人演變成動態、通用的代理程式，它擴展了 LLM 的呼叫，並輸出代碼，以幫助 AI 代理推理、探索和解決複雜任務。
然而，現有的 LLM 服務系統忽視了程式和呼叫之間的依賴關係，錯失了最佳化的重要機會。我們的分析顯示，提交給 LLM 服務引擎的程式會遇到長時間的累積等待時間，這主要是因為在個別 LLM 請求和程式中都會發生隊列頭端封鎖。為了解決這個問題，我們引入了 Autellix，這是一個 LLM 服務系統，它將程式視為一級公民，以最小化其端到端延遲。Autellix 會攔截程式提交的 LLM 呼叫，並使用程式層級的內容豐富排程器。我們提出了兩種排程演算法，分別適用於單執行緒和分散式程式，這些演算法會根據程式先前完成的呼叫，優先處理和預先處理 LLM 呼叫。我們的評估證明，與 vLLM 等最先進的系統相比，在相同的延遲下，Autellix 可將程式的處理量提升 4-15 倍，且適用於各種 LLM 和代理工作負載。

##### **A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects**
2502.13964v1 by Arjun Gupta, Rishik Sathua, Saurabh Gupta

Many everyday mobile manipulation tasks require precise interaction with
small objects, such as grasping a knob to open a cabinet or pressing a light
switch. In this paper, we develop Servoing with Vision Models (SVM), a
closed-loop training-free framework that enables a mobile manipulator to tackle
such precise tasks involving the manipulation of small objects. SVM employs an
RGB-D wrist camera and uses visual servoing for control. Our novelty lies in
the use of state-of-the-art vision models to reliably compute 3D targets from
the wrist image for diverse tasks and under occlusion due to the end-effector.
To mitigate occlusion artifacts, we employ vision models to out-paint the
end-effector thereby significantly enhancing target localization. We
demonstrate that aided by out-painting methods, open-vocabulary object
detectors can serve as a drop-in module to identify semantic targets (e.g.
knobs) and point tracking methods can reliably track interaction sites
indicated by user clicks. This training-free method obtains an 85% zero-shot
success rate on manipulating unseen objects in novel environments in the real
world, outperforming an open-loop control method and an imitation learning
baseline trained on 1000+ demonstrations by an absolute success rate of 50%.

摘要：許多日常行動操作任務需要與小物件精確互動，例如握住旋鈕打開櫃子或按下燈開關。在本文中，我們開發了具備視覺模型的伺服控制 (SVM)，這是一個閉迴路免訓練架構，可讓行動操作器處理此類精確任務，包括操作小物件。SVM 採用 RGB-D 手腕相機，並使用視覺伺服進行控制。我們的創新之處在於使用最先進的視覺模型，從手腕影像中可靠地計算出 3D 目標，以應付各種任務，並在末端執行器造成遮擋的情況下進行。為了減輕遮擋偽影，我們採用視覺模型來對末端執行器進行外繪，從而顯著增強目標定位。我們證明，在輔助外繪方法下，開放式詞彙物件偵測器可用作插入式模組，以識別語義目標 (例如旋鈕)，而點追蹤方法可以可靠地追蹤使用者點擊指示的互動位置。這種免訓練方法在現實世界中，對新環境中的未見物件進行操作，獲得 85% 的零次學習成功率，優於開放迴路控制方法和基於 1000 多次示範進行模仿學習的基準，後者的絕對成功率為 50%。

##### **MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads**
2502.13963v1 by Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang

Large Language Models (LLMs) frequently show distracted attention due to
irrelevant information in the input, which severely impairs their long-context
capabilities. Inspired by recent studies on the effectiveness of retrieval
heads in long-context factutality, we aim at addressing this distraction issue
through improving such retrieval heads directly. We propose Multi-Document
Attention Focusing (MuDAF), a novel method that explicitly optimizes the
attention distribution at the head level through contrastive learning.
According to the experimental results, MuDAF can significantly improve the
long-context question answering performance of LLMs, especially in
multi-document question answering. Extensive evaluations on retrieval scores
and attention visualizations show that MuDAF possesses great potential in
making attention heads more focused on relevant information and reducing
attention distractions.

摘要：大型語言模型 (LLM) 經常會因為輸入中的無關資訊而表現出注意力分散，這嚴重地損害了它們的長文本能力。受到最近關於檢索頭在長文本事實性中的有效性的研究啟發，我們旨在透過直接改善此類檢索頭來解決這個分心問題。我們提出多文件注意力聚焦 (MuDAF)，這是一種透過對比學習明確最佳化頭部層級注意力分配的新方法。根據實驗結果，MuDAF 可以顯著提升 LLM 的長文本問答效能，特別是在多文件問答中。在檢索分數和注意力視覺化上的廣泛評估顯示，MuDAF 具有讓注意力頭部更專注於相關資訊並減少注意力分散的巨大潛力。

##### **Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering**
2502.13962v1 by William Jurayj, Jeffrey Cheng, Benjamin Van Durme

Scaling the test-time compute of large language models has demonstrated
impressive performance on reasoning benchmarks. However, existing evaluations
of test-time scaling make the strong assumption that a reasoning system should
always give an answer to any question provided. This overlooks concerns about
whether a model is confident in its answer, and whether it is appropriate to
always provide a response. To address these concerns, we extract confidence
scores during reasoning for thresholding model responses. We find that
increasing compute budget at inference time not only helps models answer more
questions correctly, but also increases confidence in correct responses. We
then extend the current paradigm of zero-risk responses during evaluation by
considering settings with non-zero levels of response risk, and suggest a
recipe for reporting evaluations under these settings.

摘要：擴充大型語言模型的測試時間運算已在推理基準測試中展現令人印象深刻的效能。然而，現有的測試時間擴充評估會做出強烈的假設，認為推理系統應始終對所提供的任何問題給出答案。這忽略了關於模型是否對其答案有信心，以及是否適當始終提供回應的疑慮。為了解決這些疑慮，我們在推理期間提取信心分數，以設定模型回應的閾值。我們發現，在推論時間增加運算預算不僅有助於模型正確回答更多問題，還能增加對正確回應的信心。然後，我們透過考慮具有非零回應風險層級的設定，擴充評估期間零風險回應的現行範例，並建議在這些設定下報告評估的配方。

##### **LIDDIA: Language-based Intelligent Drug Discovery Agent**
2502.13959v1 by Reza Averly, Frazier N. Baker, Xia Ning

Drug discovery is a long, expensive, and complex process, relying heavily on
human medicinal chemists, who can spend years searching the vast space of
potential therapies. Recent advances in artificial intelligence for chemistry
have sought to expedite individual drug discovery tasks; however, there remains
a critical need for an intelligent agent that can navigate the drug discovery
process. Towards this end, we introduce LIDDiA, an autonomous agent capable of
intelligently navigating the drug discovery process in silico. By leveraging
the reasoning capabilities of large language models, LIDDiA serves as a
low-cost and highly-adaptable tool for autonomous drug discovery. We
comprehensively examine LIDDiA, demonstrating that (1) it can generate
molecules meeting key pharmaceutical criteria on over 70% of 30 clinically
relevant targets, (2) it intelligently balances exploration and exploitation in
the chemical space, and (3) it can identify promising novel drug candidates on
EGFR, a critical target for cancers.

摘要：藥物發現是一個漫長、昂貴且複雜的過程，高度依賴人類醫學化學家，他們可能花費數年時間搜尋潛在療法中的廣大空間。最近人工智慧在化學領域的進展試圖加速個別藥物發現任務；然而，對於能夠引導藥物發現過程的智慧代理人仍有迫切需求。為此，我們引入了 LIDDiA，這是一個自主代理人，能夠在電腦模擬中智慧地引導藥物發現過程。透過利用大型語言模型的推理能力，LIDDiA 成為自主藥物發現的低成本且高度適應性工具。我們全面檢驗 LIDDiA，證明 (1) 它可以在 30 個臨床相關目標中的 70% 以上產生符合關鍵藥物標準的分子，(2) 它在化學空間中智慧地平衡探索和利用，(3) 它可以識別出 EGFR（癌症的重要目標）上的有希望的新藥候選物。

##### **RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision**
2502.13957v1 by Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang

Retrieval-augmented generation (RAG) has shown great potential for
knowledge-intensive tasks, but its traditional architectures rely on static
retrieval, limiting their effectiveness for complex questions that require
sequential information-seeking. While agentic reasoning and search offer a more
adaptive approach, most existing methods depend heavily on prompt engineering.
In this work, we introduce RAG-Gym, a unified optimization framework that
enhances information-seeking agents through fine-grained process supervision at
each search step. We also propose ReSearch, a novel agent architecture that
synergizes answer reasoning and search query generation within the RAG-Gym
framework. Experiments on four challenging datasets show that RAG-Gym improves
performance by up to 25.6\% across various agent architectures, with ReSearch
consistently outperforming existing baselines. Further analysis highlights the
effectiveness of advanced LLMs as process reward judges and the transferability
of trained reward models as verifiers for different LLMs. Additionally, we
examine the scaling properties of training and inference in agentic RAG. The
project homepage is available at https://rag-gym.github.io/.

摘要：檢索增強生成 (RAG) 已在知識密集型任務中展現出極大的潛力，但其傳統架構依賴於靜態檢索，限制了其對需要循序資訊搜尋的複雜問題的有效性。雖然代理推理和搜尋提供了更具適應性的方法，但現有方法大多高度依賴提示工程。在這項工作中，我們引入了 RAG-Gym，一個統一的最佳化架構，它透過在每個搜尋步驟中進行細緻的流程監督來增強資訊搜尋代理。我們還提出了 ReSearch，這是一種新穎的代理架構，它在 RAG-Gym 架構中協同進行答案推理和搜尋查詢生成。在四個具有挑戰性的資料集上進行的實驗表明，RAG-Gym 透過各種代理架構將效能提升了 25.6%，而 ReSearch 則始終優於現有的基線。進一步的分析突顯了進階 LLM 作為流程獎勵評審員的有效性，以及訓練好的獎勵模型作為不同 LLM 驗證器的可轉移性。此外，我們檢視了代理 RAG 中訓練和推論的縮放特性。專案首頁可於 https://rag-gym.github.io/ 取得。

##### **Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition**
2502.13954v1 by Jingwang Huang, Jiang Zhong, Qin Lei, Jinpeng Gao, Yuming Yang, Sirui Wang, Peiguang Li, Kaiwen Wei

Multimodal multi-label emotion recognition (MMER) aims to identify the
concurrent presence of multiple emotions in multimodal data. Existing studies
primarily focus on improving fusion strategies and modeling modality-to-label
dependencies. However, they often overlook the impact of \textbf{aleatoric
uncertainty}, which is the inherent noise in the multimodal data and hinders
the effectiveness of modality fusion by introducing ambiguity into feature
representations. To address this issue and effectively model aleatoric
uncertainty, this paper proposes Latent emotional Distribution Decomposition
with Uncertainty perception (LDDU) framework from a novel perspective of latent
emotional space probabilistic modeling. Specifically, we introduce a
contrastive disentangled distribution mechanism within the emotion space to
model the multimodal data, allowing for the extraction of semantic features and
uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal
method that accounts for the dispersed distribution of uncertainty and
integrates distribution information. Experimental results show that LDDU
achieves state-of-the-art performance on the CMU-MOSEI and M$^3$ED datasets,
highlighting the importance of uncertainty modeling in MMER. Code is available
at https://github.com/201983290498/lddu\_mmer.git.

摘要：多模態多標籤情緒辨識 (MMER) 旨在識別多模態資料中多種情緒的同時存在。現有研究主要集中於改進融合策略和建模模態到標籤的依賴性。然而，他們經常忽視 **隨機不確定性** 的影響，這是多模態資料中的內在雜訊，並且透過在特徵表示中引入模糊性來阻礙模態融合的有效性。為了解決這個問題並有效地建模隨機不確定性，本文從潛在情緒空間機率建模的新觀點提出具有不確定性感知的潛在情緒分佈分解 (LDDU) 架構。具體來說，我們在情緒空間中引入一個對比性的糾纏分佈機制來建模多模態資料，允許提取語意特徵和不確定性。此外，我們設計了一個考量不確定性分散分佈並整合分佈資訊的不確定性感知融合多模態方法。實驗結果顯示，LDDU 在 CMU-MOSEI 和 M$^3$ED 資料集上達到最先進的效能，突顯了不確定性建模在 MMER 中的重要性。程式碼可在 https://github.com/201983290498/lddu\_mmer.git 取得。

##### **Neurosymbolic artificial intelligence via large language models and coherence-driven inference**
2502.13953v1 by Steve Huntsman, Jewell Thomas

We devise an algorithm to generate sets of propositions that objectively
instantiate graphs that support coherence-driven inference. We then benchmark
the ability of large language models (LLMs) to reconstruct coherence graphs
from (a straightforward transformation of) propositions expressed in natural
language, with promising results from a single prompt to models optimized for
reasoning. Combining coherence-driven inference with consistency evaluations by
neural models may advance the state of the art in machine cognition.

摘要：我們設計一種演算法，用來產生命題集合，以客觀地實例化支援連貫性驅動推論的圖形。接著，我們基準化大型語言模型 (LLM) 從以自然語言表達的命題（經過直接轉換）重建連貫性圖形的能力，結果顯示，單一提示就能從最佳化用於推理的模型中獲得有希望的結果。將連貫性驅動推論與神經模型的一致性評估結合起來，可能會提升機器認知的現有技術。

##### **Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region**
2502.13946v1 by Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li

The safety alignment of large language models (LLMs) remains vulnerable, as
their initial behavior can be easily jailbroken by even relatively simple
attacks. Since infilling a fixed template between the input instruction and
initial model output is a common practice for existing LLMs, we hypothesize
that this template is a key factor behind their vulnerabilities: LLMs'
safety-related decision-making overly relies on the aggregated information from
the template region, which largely influences these models' safety behavior. We
refer to this issue as template-anchored safety alignment. In this paper, we
conduct extensive experiments and verify that template-anchored safety
alignment is widespread across various aligned LLMs. Our mechanistic analyses
demonstrate how it leads to models' susceptibility when encountering
inference-time jailbreak attacks. Furthermore, we show that detaching safety
mechanisms from the template region is promising in mitigating vulnerabilities
to jailbreak attacks. We encourage future research to develop more robust
safety alignment techniques that reduce reliance on the template region.

摘要：大型語言模型 (LLM) 的安全性對齊仍然脆弱，因為即使相對簡單的攻擊也可能輕易破解其初始行為。由於在輸入指令和初始模型輸出之間填入一個固定的範本是現有 LLM 的常見做法，我們假設這個範本是其漏洞背後的一個關鍵因素：LLM 與安全性相關的決策制定過度依賴於範本區域的彙總資訊，這在很大程度上影響了這些模型的安全性行為。我們將此問題稱為範本錨定的安全性對齊。在本文中，我們進行了廣泛的實驗，並驗證了範本錨定的安全性對齊在各種對齊的 LLM 中廣泛存在。我們的機制分析說明了當遇到推理時間越獄攻擊時，它是如何導致模型易受攻擊的。此外，我們表明將安全性機制從範本區域分離出來有望減輕越獄攻擊的漏洞。我們鼓勵未來的研究開發更強大的安全性對齊技術，以減少對範本區域的依賴。

##### **AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence**
2502.13943v1 by Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, Chuheng Zhang, Wei Shen, Zhouhan Lin

Current approaches for training Process Reward Models (PRMs) often involve
breaking down responses into multiple reasoning steps using rule-based
techniques, such as using predefined placeholder tokens or setting the
reasoning step's length into a fixed size. These approaches overlook the fact
that specific words do not typically mark true decision points in a text. To
address this, we propose AdaptiveStep, a method that divides reasoning steps
based on the model's confidence in predicting the next word. This division
method provides more decision-making information at each step, enhancing
downstream tasks, such as reward model learning. Moreover, our method does not
require manual annotation. We demonstrate its effectiveness through experiments
with AdaptiveStep-trained PRMs in mathematical reasoning and code generation
tasks. Experimental results indicate that the outcome PRM achieves
state-of-the-art Best-of-N performance, surpassing greedy search strategy with
token-level value-guided decoding, while also reducing construction costs by
over 30% compared to existing open-source PRMs. In addition, we provide a
thorough analysis and case study on the PRM's performance, transferability, and
generalization capabilities.

摘要：目前的訓練流程獎勵模型 (PRM) 方法通常涉及
使用基於規則的技術將回應分解為多個推理步驟，例如使用預定義的佔位符代幣或將
推理步驟的長度設定為固定大小。這些方法忽視了具體的字詞通常不會標記文本中的真實決策點。為了
解決此問題，我們提出了 AdaptiveStep，這是一種基於模型預測下一個字詞的信心來劃分推理步驟的方法。此劃分
方法在每個步驟中提供了更多決策資訊，增強了下游任務，例如獎勵模型學習。此外，我們的模型不需要
手動註解。我們透過使用 AdaptiveStep 訓練的 PRM 在數學推理和程式碼生成任務中進行實驗來證明其有效性。實驗結果表明，成果 PRM 達到了最先進的 N 選一最佳效能，超越了使用代幣層級值引導解碼的貪婪搜尋策略，同時與現有的開源 PRM 相比，建構成本也降低了 30% 以上。此外，我們提供了對 PRM 效能、可移植性和泛化能力的深入分析和個案研究。

##### **Continually Learning Structured Visual Representations via Network Refinement with Rerelation**
2502.13935v1 by Zeki Doruk Erden, Boi Faltings

Current machine learning paradigm relies on continuous representations like
neural networks, which iteratively adjust parameters to approximate outcomes
rather than directly learning the structure of problem. This spreads
information across the network, causing issues like information loss and
incomprehensibility Building on prior work in environment dynamics modeling, we
propose a method that learns visual space in a structured, continual manner.
Our approach refines networks to capture the core structure of objects while
representing significant subvariants in structure efficiently. We demonstrate
this with 2D shape detection, showing incremental learning on MNIST without
overwriting knowledge and creating compact, comprehensible representations.
These results offer a promising step toward a transparent, continually learning
alternative to traditional neural networks for visual processing.

摘要：當前機器學習範例依賴於連續表示，例如神經網路，它會反覆調整參數以近似結果，而不是直接學習問題的結構。這會將資訊散布到整個網路中，導致資訊遺失和難以理解等問題。我們建立在環境動態建模的先前工作上，提出了一種以結構化、連續的方式學習視覺空間的方法。我們的做法會改善網路，以擷取物件的核心結構，同時有效地表示結構中的重大次變異。我們透過 2D 形狀偵測來展示這一點，顯示在 MNIST 上的增量學習，而不會覆寫知識，並建立精簡、易於理解的表示。這些結果為視覺處理的傳統神經網路提供了一個有希望的步驟，朝向透明、持續學習的替代方案。

##### **Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images**
2502.13928v1 by Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber

Recent studies have shown that Large Vision-Language Models (VLMs) tend to
neglect image content and over-rely on language-model priors, resulting in
errors in visually grounded tasks and hallucinations. We hypothesize that this
issue arises because existing VLMs are not explicitly trained to generate texts
that are accurately grounded in fine-grained image details. To enhance visual
feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive
Optimization), a novel finetuning objective that steers the model toward
capturing important visual details and aligning them with corresponding text
tokens. To further facilitate this detailed alignment, we introduce MVC, a
paired image-text dataset built by automatically filtering and augmenting
visual counterfactual data to challenge the model with hard contrastive cases
involving Minimal Visual Contrasts. Experiments show that our method
consistently improves VLM performance across diverse benchmarks covering
various abilities and domains, achieving up to a 22% reduction in
hallucinations, and significant gains in vision-centric and general tasks.
Notably, these improvements become increasingly pronounced in benchmarks with
higher visual dependency. In short, S-VCO offers a significant enhancement of
VLM's visually-dependent task performance while retaining or even improving the
model's general abilities. We opensource our code at https://s-vco.github.io/

摘要：<paragraph>最近的研究表明，大型视觉语言模型 (VLM) 倾向于忽略图像内容，过度依赖语言模型先验，导致视觉基础任务中的错误和幻觉。我们假设这个问题出现是因为现有的 VLM 并未明确训练来生成精确建立在精细图像细节上的文本。为了在 VLM 训练期间增强视觉反馈，我们提出了 S-VCO（对称视觉对比优化），这是一种新颖的微调目标，引导模型捕捉重要的视觉细节，并将其与相应的文本标记对齐。为了进一步促进这种详细对齐，我们引入了 MVC，这是一种成对的图像文本数据集，通过自动过滤和扩充视觉反事实数据构建而成，以使用涉及最小视觉对比的困难对比案例来挑战模型。实验表明，我们的方法在涵盖各种能力和领域的基准测试中持续提升了 VLM 性能，幻觉减少了 22%，并且在以视觉为中心的通用任务中获得了显著提升。值得注意的是，这些改进在视觉依赖性较高的基准测试中变得越来越明显。简而言之，S-VCO 显着提升了 VLM 的视觉依赖型任务性能，同时保留甚至提升了模型的通用能力。我们在 https://s-vco.github.io/ 上开源了我们的代码。</paragraph>

##### **Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?**
2502.13925v1 by Xiaochen Wang, Heming Xia, Jialin Song, Longyu Guan, Yixin Yang, Qingxiu Dong, Weiyao Luo, Yifan Pu, Yiru Wang, Xiangdi Meng, Wenjie Li, Zhifang Sui

Large Multimodal Models (LMMs) have achieved remarkable success across
various visual-language tasks. However, existing benchmarks predominantly focus
on single-image understanding, leaving the analysis of image sequences largely
unexplored. To address this limitation, we introduce StripCipher, a
comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend
and reason over sequential images. StripCipher comprises a human-annotated
dataset and three challenging subtasks: visual narrative comprehension,
contextual frame prediction, and temporal narrative reordering. Our evaluation
of $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a
significant performance gap compared to human capabilities, particularly in
tasks that require reordering shuffled sequential images. For instance, GPT-4o
achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower
than human performance. Further quantitative analysis discuss several factors,
such as input format of images, affecting the performance of LLMs in sequential
understanding, underscoring the fundamental challenges that remain in the
development of LMMs.

摘要：大型多模態模型 (LMM) 已在各種視覺語言任務中取得顯著成功。然而，現有的基準主要集中在單一影像理解上，在很大程度上未探索影像序列的分析。為了解決這個限制，我們引入了 StripCipher，一個全面的基準，旨在評估 LMM 理解和推理順序影像的能力。StripCipher 包含一個人工標註的資料集和三個具有挑戰性的子任務：視覺敘事理解、情境框架預測和時間敘事重新排序。我們對 16 個最先進的 LMM（包括 GPT-4o 和 Qwen2.5VL）的評估顯示，與人類的能力相比，存在顯著的效能差距，特別是在需要重新排序已洗牌的順序影像的任務中。例如，GPT-4o 在重新排序子任務中僅達到 23.93% 的準確度，比人類效能低 56.07%。進一步的量化分析討論了幾個因素，例如影像的輸入格式，影響 LLM 在順序理解中的效能，強調了 LMM 發展中仍然存在的基本挑戰。

##### **Qwen2.5-VL Technical Report**
2502.13923v1 by Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin

We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language
series, which demonstrates significant advancements in both foundational
capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap
forward in understanding and interacting with the world through enhanced visual
recognition, precise object localization, robust document parsing, and
long-video comprehension. A standout feature of Qwen2.5-VL is its ability to
localize objects using bounding boxes or points accurately. It provides robust
structured data extraction from invoices, forms, and tables, as well as
detailed analysis of charts, diagrams, and layouts. To handle complex inputs,
Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding,
enabling it to process images of varying sizes and videos of extended durations
(up to hours) with second-level event localization. This allows the model to
natively perceive spatial scales and temporal dynamics without relying on
traditional normalization techniques. By training a native dynamic-resolution
Vision Transformer (ViT) from scratch and incorporating Window Attention, we
reduce computational overhead while maintaining native resolution. As a result,
Qwen2.5-VL excels not only in static image and document understanding but also
as an interactive visual agent capable of reasoning, tool usage, and task
execution in real-world scenarios such as operating computers and mobile
devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases
from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model
matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly
excelling in document and diagram understanding. Additionally, Qwen2.5-VL
maintains robust linguistic performance, preserving the core language
competencies of the Qwen2.5 LLM.

摘要：<paragraph>我們推出 Qwen2.5-VL，這是 Qwen 視覺語言系列的最新旗艦機型，它在基礎能力和創新功能方面都展現出顯著的進步。Qwen2.5-VL 在理解和與世界互動方面取得重大進展，透過增強的視覺辨識、精確的物件定位、穩健的文件解析和長影片理解來實現。Qwen2.5-VL 的一個顯著特點是它能夠使用邊界框或點準確定位物件。它提供從發票、表格和表格中提取穩健的結構化資料，以及對圖表、圖解和版面的詳細分析。為了處理複雜的輸入，Qwen2.5-VL 引入了動態解析處理和絕對時間編碼，使其能夠處理不同大小的影像和長時影片（長達數小時），並具有秒級事件定位。這允許模型在不依賴傳統正規化技術的情況下，原生感知空間尺度和時間動態。透過從頭訓練原生動態解析視覺轉換器 (ViT) 並整合視窗注意力，我們在維持原生解析度的同時減少了運算開銷。因此，Qwen2.5-VL 不僅在靜態影像和文件理解方面表現出色，還能夠作為一個互動式視覺代理，在操作電腦和行動裝置等真實世界場景中進行推理、工具使用和任務執行。Qwen2.5-VL 有三種尺寸，可滿足從邊緣 AI 到高性能運算的各種使用案例。旗艦 Qwen2.5-VL-72B 機型與 GPT-4o 和 Claude 3.5 Sonnet 等最先進的機型相匹配，特別是在文件和圖解理解方面表現出色。此外，Qwen2.5-VL 維持穩健的語言效能，保留了 Qwen2.5 LLM 的核心語言能力。</paragraph>

##### **LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization**
2502.13922v1 by Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing

Large Language Models (LLMs) have demonstrated remarkable capabilities
through pretraining and alignment. However, superior short-context LLMs may
underperform in long-context scenarios due to insufficient long-context
alignment. This alignment process remains challenging due to the impracticality
of human annotation for extended contexts and the difficulty in balancing
short- and long-context performance. To address these challenges, we introduce
LongPO, that enables short-context LLMs to self-evolve to excel on long-context
tasks by internally transferring short-context capabilities. LongPO harnesses
LLMs to learn from self-generated short-to-long preference data, comprising
paired responses generated for identical instructions with long-context inputs
and their compressed short-context counterparts, respectively. This preference
reveals capabilities and potentials of LLMs cultivated during short-context
alignment that may be diminished in under-aligned long-context scenarios.
Additionally, LongPO incorporates a short-to-long KL constraint to mitigate
short-context performance decline during long-context alignment. When applied
to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully
retains short-context performance and largely outperforms naive SFT and DPO in
both long- and short-context tasks. Specifically, \ourMethod-trained models can
achieve results on long-context benchmarks comparable to, or even surpassing,
those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context
annotation and larger parameter scales.

摘要：大型語言模型 (LLM) 已透過預訓練和比對展現出非凡的能力。然而，卓越的短脈絡 LLM 可能會在長脈絡場景中表現不佳，因為長脈絡比對不足。此比對程序仍然具有挑戰性，原因是針對延伸脈絡進行人工註解不切實際，且難以平衡短脈絡和長脈絡的效能。為了應對這些挑戰，我們引入了 LongPO，它能讓短脈絡 LLM 自我演化，透過內部轉移短脈絡能力，在長脈絡任務中表現出色。LongPO 利用 LLM 從自我產生的短到長偏好資料中學習，包含為具有長脈絡輸入的相同指令產生的配對回應，以及它們壓縮的短脈絡對應項。此偏好揭示了 LLM 在短脈絡比對期間培養的能力和潛力，這些能力和潛力可能會在比對不足的長脈絡場景中減弱。此外，LongPO 結合了短到長的 KL 約束，以減輕長脈絡比對期間的短脈絡效能下降。當應用於 Mistral-7B-Instruct-v0.2，從 128K 到 512K 的脈絡長度時，LongPO 完全保留了短脈絡效能，並且在長脈絡和短脈絡任務中都大幅優於樸素的 SFT 和 DPO。具體來說，經過我們方法訓練的模型可以在長脈絡基準上取得與卓越 LLM（例如，GPT-4-128K）相當甚至超越的結果，而卓越 LLM 涉及廣泛的長脈絡註解和更大的參數規模。

##### **Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health**
2502.13920v1 by Xingbo Wang, Janessa Griffith, Daniel A. Adler, Joey Castillo, Tanzeem Choudhury, Fei Wang

Despite the prevalence of sleep-tracking devices, many individuals struggle
to translate data into actionable improvements in sleep health. Current methods
often provide data-driven suggestions but may not be feasible and adaptive to
real-life constraints and individual contexts. We present HealthGuru, a novel
large language model-powered chatbot to enhance sleep health through
data-driven, theory-guided, and adaptive recommendations with conversational
behavior change support. HealthGuru's multi-agent framework integrates wearable
device data, contextual information, and a contextual multi-armed bandit model
to suggest tailored sleep-enhancing activities. The system facilitates natural
conversations while incorporating data-driven insights and theoretical behavior
change techniques. Our eight-week in-the-wild deployment study with 16
participants compared HealthGuru to a baseline chatbot. Results show improved
metrics like sleep duration and activity scores, higher quality responses, and
increased user motivation for behavior change with HealthGuru. We also identify
challenges and design considerations for personalization and user engagement in
health chatbots.

摘要：儘管有許多睡眠追蹤裝置盛行，但許多人仍難以將資料轉化為改善睡眠健康的實際行動。目前的許多方法提供以資料為基礎的建議，但可能無法在現實生活的限制與個人背景中發揮作用且無法適應。我們提出 HealthGuru，這是一個由大型語言模型驅動的新型聊天機器人，透過以資料為基礎、理論為導向且具適應性的建議，搭配對話式行為改變支援，來改善睡眠健康。HealthGuru 的多重代理架構整合了穿戴式裝置資料、情境資訊，以及情境式多重選擇性賭博機模型，以建議量身打造的助眠活動。此系統促進自然對話，同時納入以資料為基礎的見解和理論行為改變技巧。我們進行為期八週的野外部署研究，有 16 位參與者比較了 HealthGuru 和一個基準聊天機器人。結果顯示 HealthGuru 改善了睡眠時數和活動分數等指標，並獲得較高品質的回應，以及增加使用者改變行為的動機。我們也找出個人化和使用者參與在健康聊天機器人中的挑戰和設計考量。

##### **TESS 2: A Large-Scale Generalist Diffusion Language Model**
2502.13917v1 by Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan

We introduce TESS 2, a general instruction-following diffusion language model
that outperforms contemporary instruction-tuned diffusion models, as well as
matches and sometimes exceeds strong autoregressive (AR) models. We train TESS
2 by first adapting a strong AR model via continued pretraining with the usual
cross-entropy as diffusion loss, and then performing further instruction
tuning. We find that adaptation training as well as the choice of the base
model is crucial for training good instruction-following diffusion models. We
further propose reward guidance, a novel and modular inference-time guidance
procedure to align model outputs without needing to train the underlying model.
Finally, we show that TESS 2 further improves with increased inference-time
compute, highlighting the utility of diffusion LMs in having fine-grained
controllability over the amount of compute used at inference time. Code and
models are available at https://github.com/hamishivi/tess-2.

摘要：我們引入了 TESS 2，一個一般指令追蹤擴散語言模型，它優於當代指令調整擴散模型，以及匹配和有時超過強大的自迴歸 (AR) 模型。我們透過持續預訓練，使用一般交叉熵作為擴散損失，來適應強大的 AR 模型，然後執行進一步的指令調整，來訓練 TESS 2。我們發現適應訓練以及基礎模型的選擇對於訓練良好的指令追蹤擴散模型至關重要。我們進一步提出獎勵引導，這是一種新穎且模組化的推論時間引導程序，用於在不需要訓練基礎模型的情況下對齊模型輸出。最後，我們展示 TESS 2 隨著推論時間運算的增加而進一步改善，突顯了擴散 LM 在推論時間對使用運算量的精細可控性方面的效用。程式碼和模型可在 https://github.com/hamishivi/tess-2 取得。

##### **How Do LLMs Perform Two-Hop Reasoning in Context?**
2502.13913v1 by Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell

"Socrates is human. All humans are mortal. Therefore, Socrates is mortal."
This classical example demonstrates two-hop reasoning, where a conclusion
logically follows from two connected premises. While transformer-based Large
Language Models (LLMs) can make two-hop reasoning, they tend to collapse to
random guessing when faced with distracting premises. To understand the
underlying mechanism, we train a three-layer transformer on synthetic two-hop
reasoning tasks. The training dynamics show two stages: a slow learning phase,
where the 3-layer transformer performs random guessing like LLMs, followed by
an abrupt phase transitions, where the 3-layer transformer suddenly reaches
$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms
for how models learn to randomly guess between distractions initially, and how
they learn to ignore distractions eventually. We further propose a
three-parameter model that supports the causal claims for the mechanisms to the
training dynamics of the transformer. Finally, experiments on LLMs suggest that
the discovered mechanisms generalize across scales. Our methodologies provide
new perspectives for scientific understandings of LLMs and our findings provide
new insights into how reasoning emerges during training.

摘要：「蘇格拉底是人。所有人類都是會死的。因此，蘇格拉底會死。」
這個經典範例展示了兩步推理，結論邏輯地從兩個相關的前提中推導出來。雖然基於轉換器的大型語言模型 (LLM) 可以進行兩步推理，但它們在面對分散注意力的前提時往往會陷入隨機猜測。為了了解其底層機制，我們在合成兩步推理任務上訓練了一個三層轉換器。訓練動態顯示了兩個階段：一個緩慢的學習階段，其中 3 層轉換器像 LLM 一樣進行隨機猜測，然後是一個突然的階段轉換，其中 3 層轉換器突然達到 100% 的準確度。通過逆向工程，我們解釋了模型最初如何在分心中隨機猜測的內部機制，以及它們如何最終學會忽略分心的。我們進一步提出了支持轉換器訓練動態機制的因果主張的三參數模型。最後，針對 LLM 的實驗表明，發現的機制在不同規模上是通用的。我們的技術為科學理解 LLM 提供了新的觀點，我們的發現為推理如何在訓練過程中出現提供了新的見解。

##### **Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?**
2502.13909v1 by Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park

Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.

摘要：大型語言模型 (LLM) 最近因其先進的文本理解能力和情境感知能力而成為推薦的潛力工具。儘管目前在順序推薦情境下訓練和評估 LLM 為基礎的推薦 (LLM4Rec) 模型的作法，我們發現這些模型是否理解使用者項目互動順序中固有的順序資訊在很大程度上被忽略了。在本文中，我們首先透過一系列實驗證明現有的 LLM4Rec 模型在訓練和推論期間並未完全擷取順序資訊。然後，我們提出一個簡單但有效的 LLM 為基礎的順序推薦器，稱為 LLM-SRec，一種透過將從預先訓練的 CF-SRec 模型中萃取的使用者表徵萃取到 LLM 中來增強順序資訊整合到 LLM 的方法。我們廣泛的實驗顯示 LLM-SRec 增強了 LLM 理解使用者項目互動順序的能力，最終導致推薦效能提升。此外，與需要微調 LLM 的現有 LLM4Rec 模型不同，LLM-SRec 透過僅訓練少數輕量級 MLP 來達成最先進的效能，突顯其在真實世界應用中的實用性。我們的程式碼可於 https://github.com/Sein-Kim/LLM-SRec 取得。

##### **Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference**
2502.13905v1 by Saksham Kiroriwal, Julius Pfrommer, Jürgen Beyerer

To reduce the curse of dimensionality for Gaussian processes (GP), they can
be decomposed into a Gaussian Process Network (GPN) of coupled subprocesses
with lower dimensionality. In some cases, intermediate observations are
available within the GPN. However, intermediate observations are often
indirect, noisy, and incomplete in most real-world systems. This work
introduces the Partially Observable Gaussian Process Network (POGPN) to model
real-world process networks. We model a joint distribution of latent functions
of subprocesses and make inferences using observations from all subprocesses.
POGPN incorporates observation lenses (observation likelihoods) into the
well-established inference method of deep Gaussian processes. We also introduce
two training methods for POPGN to make inferences on the whole network using
node observations. The application to benchmark problems demonstrates how
incorporating partial observations during training and inference can improve
the predictive performance of the overall network, offering a promising outlook
for its practical application.

摘要：為了降低高斯程序 (GP) 的維度詛咒，它們可以分解為具有較低維度的耦合子程序的高斯程序網路 (GPN)。在某些情況下，中間觀察可以在 GPN 中使用。然而，在多數真實世界系統中，中間觀察通常是間接、有雜訊且不完整的。這項工作引入了部分可觀察高斯程序網路 (POGPN) 來建模真實世界程序網路。我們建模子程序的潛在函數的聯合分佈，並使用來自所有子程序的觀察進行推論。POGPN 將觀察透鏡 (觀察似然函數) 整合到已建立良好的深度高斯程序推論方法中。我們還引入了兩種 POPGN 訓練方法，以使用節點觀察對整個網路進行推論。應用於基準問題展示了在訓練和推論期間納入部分觀察如何能改善整體網路的預測效能，為其實際應用提供了樂觀的前景。

##### **DataSciBench: An LLM Agent Benchmark for Data Science**
2502.13897v1 by Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue

This paper presents DataSciBench, a comprehensive benchmark for evaluating
Large Language Model (LLM) capabilities in data science. Recent related
benchmarks have primarily focused on single tasks, easily obtainable ground
truth, and straightforward evaluation metrics, which limits the scope of tasks
that can be evaluated. In contrast, DataSciBench is constructed based on a more
comprehensive and curated collection of natural and challenging prompts for
uncertain ground truth and evaluation metrics. We develop a semi-automated
pipeline for generating ground truth (GT) and validating evaluation metrics.
This pipeline utilizes and implements an LLM-based self-consistency and human
verification strategy to produce accurate GT by leveraging collected prompts,
predefined task types, and aggregate functions (metrics). Furthermore, we
propose an innovative Task - Function - Code (TFC) framework to assess each
code execution outcome based on precisely defined metrics and programmatic
rules. Our experimental framework involves testing 6 API-based models, 8
open-source general models, and 9 open-source code generation models using the
diverse set of prompts we have gathered. This approach aims to provide a more
comprehensive and rigorous evaluation of LLMs in data science, revealing their
strengths and weaknesses. Experimental results demonstrate that API-based
models outperform open-sourced models on all metrics and
Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced
models. We release all code and data at https://github.com/THUDM/DataSciBench.

摘要：<paragraph>這篇論文提出了 DataSciBench，一個用於評估大型語言模型 (LLM) 在資料科學能力的綜合基準。最近相關的基準主要集中在單一任務、容易取得的基礎真實值和直接的評估指標上，這限制了可評估任務的範圍。相比之下，DataSciBench 是基於更全面且經過整理的自然且具有挑戰性的提示集合，用於不確定的基礎真實值和評估指標。我們開發了一個半自動化的管道，用於產生基礎真實值 (GT) 和驗證評估指標。此管道利用並實作了基於 LLM 的自我一致性和人工驗證策略，透過利用收集的提示、預定義的任務類型和聚合函數（指標）來產生準確的 GT。此外，我們提出了一個創新的任務 - 函數 - 程式碼 (TFC) 架構，用於根據精確定義的指標和程式規則評估每個程式碼執行結果。我們的實驗架構涉及使用我們收集的多樣化提示，測試 6 個基於 API 的模型、8 個開源通用模型和 9 個開源程式碼生成模型。此方法旨在提供對資料科學中 LLM 的更全面且嚴謹的評估，揭示其優點和缺點。實驗結果表明，基於 API 的模型在所有指標上都優於開源模型，而 Deepseek-Coder-33B-Instruct 在開源模型中獲得最高分。我們在 https://github.com/THUDM/DataSciBench 釋出所有程式碼和資料。</paragraph>

##### **PSCon: Toward Conversational Product Search**
2502.13881v1 by Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen

Conversational Product Search (CPS) is confined to simulated conversations
due to the lack of real-world CPS datasets that reflect human-like language.
Additionally, current conversational datasets are limited to support
cross-market and multi-lingual usage. In this paper, we introduce a new CPS
data collection protocol and present PSCon, a novel CPS dataset designed to
assist product search via human-like conversations. The dataset is constructed
using a coached human-to-human data collection protocol and supports two
languages and dual markets. Also, the dataset enables thorough exploration of
six subtasks of CPS: user intent detection, keyword extraction, system action
prediction, question selection, item ranking, and response generation.
Furthermore, we also offer an analysis of the dataset and propose a benchmark
model on the proposed CPS dataset.

摘要：對話式商品搜尋 (CPS) 由於缺乏反映人類語言的真實世界 CPS 資料集，因此僅限於模擬對話。此外，目前的對話式資料集僅支援跨市場和多語言使用。在本文中，我們介紹一種新的 CPS 資料收集協定，並提出 PSCon，這是一個新穎的 CPS 資料集，旨在透過類似人類的對話協助商品搜尋。該資料集是使用輔導式人對人資料收集協定建構的，並支援兩種語言和兩個市場。此外，該資料集能徹底探索 CPS 的六個子任務：使用者意圖偵測、關鍵字萃取、系統動作預測、問題選擇、商品排名和回應產生。此外，我們還提供對資料集的分析，並在建議的 CPS 資料集上提出一個基準模型。

##### **MEX: Memory-efficient Approach to Referring Multi-Object Tracking**
2502.13875v1 by Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu

Referring Multi-Object Tracking (RMOT) is a relatively new concept that has
rapidly gained traction as a promising research direction at the intersection
of computer vision and natural language processing. Unlike traditional
multi-object tracking, RMOT identifies and tracks objects and incorporates
textual descriptions for object class names, making the approach more
intuitive. Various techniques have been proposed to address this challenging
problem; however, most require the training of the entire network due to their
end-to-end nature. Among these methods, iKUN has emerged as a particularly
promising solution. Therefore, we further explore its pipeline and enhance its
performance. In this paper, we introduce a practical module dubbed
Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be
directly applied to off-the-shelf trackers like iKUN, resulting in significant
architectural improvements. Our method proves effective during inference on a
single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI
dataset, which offers diverse autonomous driving scenes with relevant language
expressions, is particularly useful for studying this problem. Empirically, our
method demonstrates effectiveness and efficiency regarding HOTA tracking
scores, substantially improving memory allocation and processing speed.

摘要：多目標追蹤 (RMOT) 是一個相對較新的概念，它在電腦視覺和自然語言處理的交叉領域中迅速獲得關注，成為一個有前途的研究方向。與傳統多目標追蹤不同，RMOT 識別並追蹤目標，並將目標類別名稱的文字描述納入其中，使這種方法更直觀。已經提出各種技術來解決這個具有挑戰性的問題；然而，由於其端到端特性，大多數技術都需要訓練整個網路。在這些方法中，iKUN 已成為一個特別有前途的解決方案。因此，我們進一步探索其管道並增強其效能。在本文中，我們介紹了一個稱為 Memory-Efficient Cross-modality -- MEX 的實用模組。這種記憶體高效的技術可以直接應用於現成的追蹤器，例如 iKUN，從而帶來顯著的架構改進。我們的技術證明了在單個具有 4 GB 記憶體的 GPU 上進行推理的有效性。在各種基準中，Refer-KITTI 資料集提供了具有相關語言表達的多樣化自動駕駛場景，對於研究這個問題特別有用。根據經驗，我們的技術在 HOTA 追蹤分數方面證明了有效性和效率，顯著改善了記憶體配置和處理速度。

##### **NVR: Vector Runahead on NPUs for Sparse Memory Access**
2502.13873v1 by Hui Wang, Zhengpeng Zhao, Jing Wang, Yushu Du, Yuan Cheng, Bing Guo, He Xiao, Chenhao Ma, Xiaomeng Han, Dean You, Jiapeng Guan, Ran Wei, Dawei Yang, Zhe Jiang

Deep Neural Networks are increasingly leveraging sparsity to reduce the
scaling up of model parameter size. However, reducing wall-clock time through
sparsity and pruning remains challenging due to irregular memory access
patterns, leading to frequent cache misses. In this paper, we present NPU
Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address
cache miss problems in sparse DNN workloads. Rather than optimising memory
patterns with high overhead and poor portability, NVR adapts runahead execution
to the unique architecture of NPUs. NVR provides a general micro-architectural
solution for sparse DNN workloads without requiring compiler or algorithmic
support, operating as a decoupled, speculative, lightweight hardware sub-thread
alongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an
average 90% reduction in cache misses compared to SOTA prefetching in
general-purpose processors, delivering 4x average speedup on sparse workloads
versus NPUs without prefetching. Moreover, we investigate the advantages of
incorporating a small cache (16KB) into the NPU combined with NVR. Our
evaluation shows that expanding this modest cache delivers 5x higher
performance benefits than increasing the L2 cache size by the same amount.

摘要：深度神经网络越来越多地利用稀疏性来减少模型参数大小的扩展。然而，由于不规则的内存访问模式，通过稀疏性和剪枝来减少时钟时间仍然具有挑战性，导致频繁的缓存未命中。在本文中，我们提出了 NPU 矢量超前 (NVR)，这是一种针对 NPU 量身定制的预取机制，以解决稀疏 DNN 工作负载中的缓存未命中问题。NVR 不会通过高开销和较差的可移植性来优化内存模式，而是将超前执行调整为 NPU 的独特架构。NVR 为稀疏 DNN 工作负载提供了一种通用的微架构解决方案，无需编译器或算法支持，作为 NPU 旁路的一个解耦的、推测性的、轻量级的硬件子线程运行，且硬件开销极小（低于 5%）。与通用处理器中的 SOTA 预取相比，NVR 将缓存未命中率平均降低了 90%，在没有预取的 NPU 上，稀疏工作负载的平均加速比为 4 倍。此外，我们研究了将一个小缓存（16KB）与 NVR 结合到 NPU 中的优势。我们的评估表明，扩展此适度缓存提供的性能优势比将 L2 缓存大小增加相同量高 5 倍。

##### **SPEX: Scaling Feature Interaction Explanations for LLMs**
2502.13870v1 by Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu

Large language models (LLMs) have revolutionized machine learning due to
their ability to capture complex interactions between input features. Popular
post-hoc explanation methods like SHAP provide marginal feature attributions,
while their extensions to interaction importances only scale to small input
lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic
interaction attribution algorithm that efficiently scales to large input
lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among
interactions -- common in real-world data -- and applies a sparse Fourier
transform using a channel decoding algorithm to efficiently identify important
interactions. We perform experiments across three difficult long-context
datasets that require LLMs to utilize interactions between inputs to complete
the task. For large inputs, SPEX outperforms marginal attribution methods by up
to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX
successfully identifies key features and interactions that strongly influence
model output. For one of our datasets, HotpotQA, SPEX provides interactions
that align with human annotations. Finally, we use our model-agnostic approach
to generate explanations to demonstrate abstract reasoning in closed-source
LLMs (GPT-4o mini) and compositional reasoning in vision-language models.

摘要：大型語言模型 (LLM) 由於能夠捕捉輸入特徵之間的複雜交互作用，因此徹底改變了機器學習。熱門的後設解釋方法（如 SHAP）提供邊際特徵歸因，而其交互重要性擴充套件僅能擴充套件至較小的輸入長度（≈ 20）。我們提出一種模型不可知互動歸因演算法，稱為 Spectral Explainer (SPEX)，它能有效擴充套件至較大的輸入長度（≈ 1000）。SPEX 利用交互作用之間的自然稀疏性（在現實世界資料中很常見），並使用通道解碼演算法應用稀疏傅立葉轉換，以有效找出重要的交互作用。我們在三個困難的長脈絡資料集上執行實驗，這些資料集需要 LLM 利用輸入之間的交互作用才能完成任務。對於大型輸入，SPEX 在忠實重建 LLM 輸出的方面，比邊際歸因方法高出 20%。此外，SPEX 成功找出強烈影響模型輸出的關鍵特徵和交互作用。對於我們的其中一個資料集 HotpotQA，SPEX 提供與人類註解一致的交互作用。最後，我們使用我們的模型不可知方法來產生解釋，以展示閉源 LLM（GPT-4o mini）中的抽象推理和視覺語言模型中的組合推理。

##### **DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue**
2502.13847v1 by Feiyuan Zhang, Dezhi Zhu, James Ming, Yilun Jin, Di Chai, Liu Yang, Han Tian, Zhaoxin Fan, Kai Chen

Retrieval-Augmented Generation (RAG) systems have shown substantial benefits
in applications such as question answering and multi-turn dialogue
\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging
static knowledge bases, often overlook the potential of dynamic historical
information in ongoing conversations. To bridge this gap, we introduce DH-RAG,
a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for
Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that
utilize both long-term memory and immediate historical context in
conversational responses \citep{stafford1987conversational}. DH-RAG is
structured around two principal components: a History-Learning based Query
Reconstruction Module, designed to generate effective queries by synthesizing
current and prior interactions, and a Dynamic History Information Updating
Module, which continually refreshes historical context throughout the dialogue.
The center of DH-RAG is a Dynamic Historical Information database, which is
further refined by three strategies within the Query Reconstruction Module:
Historical Query Clustering, Hierarchical Matching, and Chain of Thought
Tracking. Experimental evaluations show that DH-RAG significantly surpasses
conventional models on several benchmarks, enhancing response relevance,
coherence, and dialogue quality.

摘要：檢索增強生成 (RAG) 系統已在問答和多輪對話等應用程式中展現顯著的優勢\citep{lewis2020retrieval}。然而，傳統的 RAG 方法雖然利用靜態知識庫，卻常常忽略對話過程中動態歷史資訊的潛力。為了彌補這個差距，我們引進 DH-RAG，一種動態歷史脈絡驅動的檢索增強生成方法，用於多輪對話。DH-RAG 的靈感來自人類認知過程，在對話回應中同時利用長期記憶和即時歷史脈絡\citep{stafford1987conversational}。DH-RAG 的架構圍繞兩個主要元件：一個基於歷史學習的查詢重建模組，旨在透過綜合當前和先前的互動來產生有效的查詢，以及一個動態歷史資訊更新模組，會在對話過程中持續更新歷史脈絡。DH-RAG 的核心是一個動態歷史資訊資料庫，並在查詢重建模組中透過三種策略進一步加以精進：歷史查詢分群、階層式比對和思維鏈追蹤。實驗評估顯示，DH-RAG 在多個基準上顯著超越傳統模型，提升回應相關性、一致性和對話品質。

##### **Enhancing LLM-Based Recommendations Through Personalized Reasoning**
2502.13845v1 by Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu

Current recommendation systems powered by large language models (LLMs) often
underutilize their reasoning capabilities due to a lack of explicit logical
structuring. To address this limitation, we introduce CoT-Rec, a framework that
integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by
incorporating two crucial processes: user preference analysis and item
perception evaluation. CoT-Rec operates in two key phases: (1) personalized
data extraction, where user preferences and item perceptions are identified,
and (2) personalized data application, where this information is leveraged to
refine recommendations. Our experimental analysis demonstrates that CoT-Rec
improves recommendation accuracy by making better use of LLMs' reasoning
potential. The implementation is publicly available at
https://anonymous.4open.science/r/CoT-Rec.

摘要：目前由大型語言模型 (LLM) 驅動的推薦系統通常由於缺乏明確的邏輯結構而無法充分利用其推理能力。為了解決此限制，我們引入了 CoT-Rec，一個將思考鏈 (CoT) 推理整合到 LLM 驅動的推薦中的框架，方法是納入兩個關鍵流程：使用者偏好分析和項目感知評估。CoT-Rec 在兩個關鍵階段中運作：(1) 個人化數據萃取，其中識別使用者偏好和項目感知，以及 (2) 個人化數據應用，其中利用這些資訊來優化推薦。我們的實驗分析證明，CoT-Rec 透過更有效地利用 LLM 的推理潛力來提升推薦準確度。實作已公開於 https://anonymous.4open.science/r/CoT-Rec。

##### **Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents**
2502.13843v1 by Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu

Large Language Model (LLM)-based user agents have emerged as a powerful tool
for improving recommender systems by simulating user interactions. However,
existing methods struggle with cross-domain scenarios due to inefficient memory
structures, leading to irrelevant information retention and failure to account
for social influence factors such as popularity. To address these limitations,
we introduce AgentCF++, a novel framework featuring a dual-layer memory
architecture and a two-step fusion mechanism to filter domain-specific
preferences effectively. Additionally, we propose interest groups with shared
memory, allowing the model to capture the impact of popularity trends on users
with similar interests. Through extensive experiments on multiple cross-domain
datasets, AgentCF++ demonstrates superior performance over baseline models,
highlighting its effectiveness in refining user behavior simulation for
recommender systems. Our code is available at
https://anonymous.4open.science/r/AgentCF-plus.

摘要：大型語言模型 (LLM) 的使用者代理已成為一種強大的工具，可透過模擬使用者互動來改善推薦系統。然而，現有的方法由於記憶體結構效率不彰，導致不相關資訊保留，且無法考量受歡迎度等社會影響因素，因此在跨網域場景中會遇到困難。為了解決這些限制，我們引入了 AgentCF++，這是一個新穎的架構，具備雙層記憶體架構和兩步驟融合機制，可有效過濾特定網域的偏好。此外，我們提出了具有共享記憶體的興趣群組，讓模型能捕捉受歡迎度趨勢對具有相似興趣的使用者造成的影響。透過在多個跨網域資料集上的廣泛實驗，AgentCF++ 展現出優於基準模型的效能，突顯其在為推薦系統精進使用者行為模擬方面的有效性。我們的程式碼可在 https://anonymous.4open.science/r/AgentCF-plus 取得。

##### **Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking**
2502.13842v1 by Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang

Large language models (LLMs) face inherent performance bottlenecks under
parameter constraints, particularly in processing critical tokens that demand
complex reasoning. Empirical analysis reveals challenging tokens induce abrupt
gradient spikes across layers, exposing architectural stress points in standard
Transformers. Building on this insight, we propose Inner Thinking Transformer
(ITT), which reimagines layer computations as implicit thinking steps. ITT
dynamically allocates computation through Adaptive Token Routing, iteratively
refines representations via Residual Thinking Connections, and distinguishes
reasoning phases using Thinking Step Encoding. ITT enables deeper processing of
critical tokens without parameter expansion. Evaluations across 162M-466M
parameter models show ITT achieves 96.5\% performance of a 466M Transformer
using only 162M parameters, reduces training data by 43.2\%, and outperforms
Transformer/Loop variants in 11 benchmarks. By enabling elastic computation
allocation during inference, ITT balances performance and efficiency through
architecture-aware optimization of implicit thinking pathways.

摘要：大型语言模型 (LLM) 在参数限制下会面临固有的性能瓶颈，尤其是在处理需要复杂推理的关键标记时。经验分析表明，具有挑战性的标记会引起跨层梯度急剧上升，暴露了标准 Transformer 中的架构压力点。基于这一见解，我们提出了 Inner Thinking Transformer (ITT)，它将层计算重新构想为隐式思考步骤。ITT 通过自适应标记路由动态分配计算，通过残差思考连接迭代优化表示，并使用思考步骤编码区分推理阶段。ITT 可以在不扩展参数的情况下对关键标记进行更深入的处理。对 162M-466M 参数模型的评估表明，ITT 使用仅 162M 参数就实现了 466M Transformer 的 96.5% 性能，将训练数据减少了 43.2%，并在 11 个基准测试中优于 Transformer/Loop 变体。通过在推理过程中启用弹性计算分配，ITT 通过隐式思考路径的架构感知优化来平衡性能和效率。

##### **Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models**
2502.13836v1 by Peter Carragher, Abhinand Jha, R Raghav, Kathleen M. Carley

Large Language Models (LLMs) demonstrate remarkable capabilities in question
answering (QA), but metrics for assessing their reliance on memorization versus
retrieval remain underdeveloped. Moreover, while finetuned models are
state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o
exhibit strong zero-shot performance. This raises questions about the
trade-offs between memorization, generalization, and retrieval. In this work,
we analyze the extent to which multimodal retrieval-augmented VLMs memorize
training data compared to baseline VLMs. Using the WebQA benchmark, we contrast
finetuned models with baseline VLMs on multihop retrieval and question
answering, examining the impact of finetuning on data memorization. To quantify
memorization in end-to-end retrieval and QA systems, we propose several proxy
metrics by investigating instances where QA succeeds despite retrieval failing.
Our results reveal the extent to which finetuned models rely on memorization.
In contrast, retrieval-augmented VLMs have lower memorization scores, at the
cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a
challenge for future work to reconcile memorization and generalization in both
Open-Domain QA and joint Retrieval-QA tasks.

摘要：大型語言模型（LLM）在問答（QA）任務中展現出驚人的能力，但用於評估其依賴記憶而非檢索的指標仍未發展成熟。此外，雖然微調模型在封閉領域任務中是目前最先進的技術，但像 GPT-4o 這樣的通用模型展現出強大的零次學習效能。這引發了關於記憶、概化和檢索之間權衡取捨的問題。在這項研究中，我們分析了多模態檢索增強 VLM 與基準 VLM 相比，記憶訓練資料的程度。使用 WebQA 基準，我們在多跳檢索和問答方面比較了微調模型和基準 VLM，並探討微調對資料記憶的影響。為了量化端到端檢索和 QA 系統中的記憶，我們透過探討檢索失敗但 QA 成功的情況，提出了幾個代理指標。我們的結果揭示了微調模型依賴記憶的程度。相比之下，檢索增強 VLM 的記憶分數較低，但代價是準確度（在 WebQA 測試集中為 72% 對 52%）。因此，我們的測量對未來的研究提出了挑戰，以調和開放領域 QA 和聯合檢索-QA 任務中的記憶和概化。

##### **Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning**
2502.13834v1 by Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma

Large language models (LLMs) can prove mathematical theorems formally by
generating proof steps (\textit{a.k.a.} tactics) within a proof system.
However, the space of possible tactics is vast and complex, while the available
training data for formal proofs is limited, posing a significant challenge to
LLM-based tactic generation. To address this, we introduce a neuro-symbolic
tactic generator that synergizes the mathematical intuition learned by LLMs
with domain-specific insights encoded by symbolic methods. The key aspect of
this integration is identifying which parts of mathematical reasoning are best
suited to LLMs and which to symbolic methods. While the high-level idea of
neuro-symbolic integration is broadly applicable to various mathematical
problems, in this paper, we focus specifically on Olympiad inequalities
(Figure~1). We analyze how humans solve these problems and distill the
techniques into two types of tactics: (1) scaling, handled by symbolic methods,
and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with
LLMs to prune and rank the proof goals for efficient proof search. We evaluate
our framework on 161 challenging inequalities from multiple mathematics
competitions, achieving state-of-the-art performance and significantly
outperforming existing LLM and symbolic approaches without requiring additional
training data.

摘要：大型語言模型 (LLM) 能透過在證明系統中產生證明步驟（又稱策略）來正式證明數學定理。然而，可能的策略空間浩瀚且複雜，而形式化證明的可用訓練資料有限，對基於 LLM 的策略產生構成重大挑戰。為了解決這個問題，我們引入了神經符號策略產生器，它將 LLM 學到的數學直覺與符號方法編碼的特定領域見解結合起來。這種整合的關鍵面向在於找出數學推理的哪些部分最適合 LLM，哪些部分最適合符號方法。雖然神經符號整合的高階概念廣泛適用於各種數學問題，但在本文中，我們特別關注奧林匹亞不等式（圖 1）。我們分析人類如何解決這些問題，並將技術提煉成兩種策略類型：(1) 縮放，由符號方法處理，以及 (2) 重寫，由 LLM 處理。此外，我們將符號工具與 LLM 結合，以修剪和排列證明目標，以便有效進行證明搜尋。我們在多個數學競賽中針對 161 個具有挑戰性的不等式評估我們的架構，達到了最先進的效能，並且顯著優於現有的 LLM 和符號方法，而無需額外的訓練資料。

##### **Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning**
2502.13820v1 by Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg

Code verification has recently found great success as a critical component in
training large scale reasoning models for coding. Synthetic techniques such as
self-generated test cases and reward models provide a way to enhance code
capabilities beyond predefined tests. Building on these advancements, we
propose new benchmarks designed to systematically evaluate the impact of
synthetic verification methods on assessing solution correctness. We introduce
HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks
into scoring and ranking datasets to evaluate the effectiveness of synthetic
verifiers. Using these benchmarks, we analyze synthetic verification methods in
standard, reasoning-based, and reward-based LLMs. Our results show that recent
reasoning models significantly improve test case generation and that scaling
test cases enhances verification accuracy.

摘要：程式碼驗證最近被發現作為訓練大型推理模型進行編碼的重要組成部分而獲得巨大的成功。自我產生的測試案例和獎勵模型等合成技術提供了一種方法，可以增強程式碼功能，超越預定義的測試。在這些進展的基礎上，我們提出了新的基準，旨在系統性地評估合成驗證方法對評估解決方案正確性的影響。我們介紹了 HE-R、HE-R+、MBPP-R 和 MBPP-R+，它們將現有的編碼基準轉換為評分和排名資料集，以評估合成驗證器的有效性。使用這些基準，我們分析了標準、基於推理和基於獎勵的 LLM 中的合成驗證方法。我們的結果表明，最近的推理模型顯著改進了測試案例生成，並且擴充測試案例增強了驗證準確性。

##### **On the Duality between Gradient Transformations and Adapters**
2502.13811v1 by Lucas Torroba-Hennigen, Hunter Lang, Han Guo, Yoon Kim

We study memory-efficient optimization of neural networks with linear
gradient transformations, where the gradients are linearly mapped to a lower
dimensional space than the full parameter space, thus saving memory required
for gradient accumulation and optimizer state persistence. The model parameters
are updated by first performing an optimization step in the lower dimensional
space and then going back into the original parameter space via the linear
map's transpose. We show that optimizing the model in this transformed space is
equivalent to reparameterizing the original model through a linear adapter that
additively modifies the model parameters, and then only optimizing the
adapter's parameters. When the transformation is Kronecker-factored, this
establishes an equivalence between GaLore and one-sided LoRA. We show that this
duality between gradient transformations and adapter-based reparameterizations
unifies existing approaches to memory-efficient training and suggests new
techniques for improving training efficiency and memory use.

摘要：我們研究具有線性梯度轉換的神經網路的記憶體高效最佳化，其中梯度被線性映射到比完整參數空間更低維度的空間，從而節省了梯度累積和最佳化器狀態持續性所需的記憶體。模型參數會先在低維度空間執行最佳化步驟，再透過線性映射的轉置返回到原始參數空間來進行更新。我們說明在這個轉換空間最佳化模型等於透過線性轉接器重新參數化原始模型，這個轉接器會累加修改模型參數，然後僅最佳化轉接器的參數。當轉換是 Kronecker 分解時，這會在 GaLore 和單邊 LoRA 之間建立等效性。我們說明梯度轉換和基於轉接器的重新參數化之間的這種對偶性統一了現有的記憶體高效訓練方法，並建議新的技術來改善訓練效率和記憶體使用。

##### **LESA: Learnable LLM Layer Scaling-Up**
2502.13794v1 by Yifei Yang, Zouying Cao, Xinbei Ma, Yao Yao, Libo Qin, Zhi Chen, Hai Zhao

Training Large Language Models (LLMs) from scratch requires immense
computational resources, making it prohibitively expensive. Model scaling-up
offers a promising solution by leveraging the parameters of smaller models to
create larger ones. However, existing depth scaling-up methods rely on
empirical heuristic rules for layer duplication, which result in poorer
initialization and slower convergence during continual pre-training. We propose
\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating
parameters from each layer and applying Singular Value Decomposition, we
uncover latent patterns between layers, suggesting that inter-layer parameters
can be learned. LESA uses a neural network to predict the parameters inserted
between adjacent layers, enabling better initialization and faster training.
Experiments show that LESA outperforms existing baselines, achieving superior
performance with less than half the computational cost during continual
pre-training. Extensive analyses demonstrate its effectiveness across different
model sizes and tasks.

摘要：從頭訓練大型語言模型 (LLM) 需要龐大的計算資源，這讓成本高得令人難以負擔。模型擴充提供了有希望的解決方案，透過利用較小模型的參數來建立較大的模型。然而，現有的深度擴充方法依賴於層疊制的經驗啟發式規則，這會導致在持續預訓練期間初始化較差且收斂較慢。我們提出 \textbf{LESA}，一種用於深度擴充的新型可學習方法。透過串接來自每層的參數並應用奇異值分解，我們揭示了層之間的潛在模式，這表明可以學習層間參數。LESA 使用神經網路來預測插入相鄰層之間的參數，從而實現更好的初始化和更快的訓練。實驗表明，LESA 優於現有的基準，在持續預訓練期間以不到一半的計算成本實現了卓越的效能。廣泛的分析證明了其在不同模型大小和任務中的有效性。

##### **From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions**
2502.13791v1 by Nathanaël Carraz Rakotonirina, Mohammed Hamdy, Jon Ander Campos, Lucas Weber, Alberto Testoni, Marzieh Fadaee, Sandro Pezzelle, Marco Del Tredici

Large Language Models (LLMs) are increasingly used in working environments
for a wide range of tasks, excelling at solving individual problems in
isolation. However, are they also able to effectively collaborate over
long-term interactions? To investigate this, we introduce MemoryCode, a
synthetic multi-session dataset designed to test LLMs' ability to track and
execute simple coding instructions amid irrelevant information, simulating a
realistic setting. While all the models we tested handle isolated instructions
well, even the performance of state-of-the-art models like GPT-4o deteriorates
when instructions are spread across sessions. Our analysis suggests this is due
to their failure to retrieve and integrate information over long instruction
chains. Our results highlight a fundamental limitation of current LLMs,
restricting their ability to collaborate effectively in long interactions.

摘要：大型語言模型 (LLM)  zunehmend in Arbeitsumgebungen für eine Vielzahl von Aufgaben verwendet, die sich bei der isolierten Lösung einzelner Probleme auszeichnen. Sind sie jedoch auch in der Lage, bei langfristigen Interaktionen effektiv zusammenzuarbeiten? Um dies zu untersuchen, führen wir MemoryCode ein, einen synthetischen Multi-Session-Datensatz, der entwickelt wurde, um die Fähigkeit von LLMs zu testen, einfache Codierungsanweisungen inmitten irrelevanter Informationen zu verfolgen und auszuführen, und so eine realistische Umgebung zu simulieren. Während alle von uns getesteten Modelle isolierte Anweisungen gut handhaben, verschlechtert sich selbst die Leistung modernster Modelle wie GPT-4o, wenn Anweisungen über Sitzungen verteilt werden. Unsere Analyse legt nahe, dass dies auf ihr Versagen zurückzuführen ist, Informationen über lange Befehlsketten abzurufen und zu integrieren. Unsere Ergebnisse zeigen eine grundlegende Einschränkung aktueller LLMs auf und schränken ihre Fähigkeit ein, bei langen Interaktionen effektiv zusammenzuarbeiten.

##### **Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics**
2502.13785v1 by Matthew Wood, Mathieu Klop, Maxime Allard

mRNA-based vaccines have become a major focus in the pharmaceutical industry.
The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can
strongly influence translation efficiency, stability, degradation, and other
factors that collectively determine a vaccine's effectiveness. However,
optimizing mRNA sequences for those properties remains a complex challenge.
Existing deep learning models often focus solely on coding region optimization,
overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and
attention hybrid model to address these challenges. In addition to a first
pre-training, a second pre-training stage allows us to specialise the model
with high-quality data. We employ single nucleotide tokenization of mRNA
sequences with codon separation, ensuring prior biological and structural
information from the original mRNA sequence is not lost. Our model, Helix-mRNA,
outperforms existing methods in analysing both UTRs and coding region
properties. It can process sequences 6x longer than current approaches while
using only 10% of the parameters of existing foundation models. Its predictive
capabilities extend to all mRNA regions. We open-source the model
(https://github.com/helicalAI/helical) and model weights
(https://huggingface.co/helical-ai/helix-mRNA).

摘要：mRNA 為基礎的疫苗已成為製藥產業的主要焦點。
mRNA 的編碼序列以及非轉譯區 (UTR) 會
強烈影響轉譯效率、穩定性、降解，以及其他
共同決定疫苗有效性的因素。然而，
針對這些特性最佳化 mRNA 序列仍然是一項複雜的挑戰。
現有的深度學習模型通常只專注於編碼區域最佳化，
忽略了 UTR。我們提出 Helix-mRNA，一種結構化狀態空間為基礎且
結合注意力的混合模型來解決這些挑戰。除了第一個
預訓練階段，第二個預訓練階段允許我們使用高品質資料
讓模型專門化。我們使用單核苷酸標記化 mRNA
序列，並分離密碼子，確保不會遺失來自原始 mRNA 序列的
先驗生物和結構資訊。我們的模型 Helix-mRNA，
在分析 UTR 和編碼區域特性方面，表現優於現有方法。它可以
處理比目前方法長 6 倍的序列，同時僅使用現有基礎模型
10% 的參數。其預測能力擴及所有 mRNA 區域。我們開放原始碼
模型 (https://github.com/helicalAI/helical) 和模型權重
(https://huggingface.co/helical-ai/helix-mRNA)。

##### **Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions**
2502.13780v1 by Beatrice Savoldi, Alan Ramponi, Matteo Negri, Luisa Bentivogli

Converging societal and technical factors have transformed language
technologies into user-facing applications employed across languages. Machine
Translation (MT) has become a global tool, with cross-lingual services now also
supported by dialogue systems powered by multilingual Large Language Models
(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,
often with little to no expertise in the languages or the technology itself.
Despite this, the understanding of MT consumed by this diverse group of users
-- their needs, experiences, and interactions with these systems -- remains
limited. This paper traces the shift in MT user profiles, focusing on
non-expert users and how their engagement with these systems may change with
LLMs. We identify three key factors -- usability, trust, and literacy -- that
shape these interactions and must be addressed to align MT with user needs. By
exploring these dimensions, we offer insights to guide future MT with a
user-centered approach.

摘要：社會和技術因素的匯聚，已將語言技術轉變為跨語言應用的使用者介面應用程式。機器翻譯 (MT) 已成為全球性工具，跨語言服務現也獲得由多語言大型語言模型 (LLM) 提供動力的對話系統支援。這種易用性已將 MT 的觸角延伸到廣大的非專業使用者，他們通常對語言或技術本身知之甚少，甚至一無所知。儘管如此，這群多元使用者對 MT 的理解——他們的需求、經驗，以及與這些系統的互動——仍然有限。本文追蹤 MT 使用者輪廓的轉變，著重於非專業使用者，以及他們與這些系統的互動如何隨著 LLM 而改變。我們找出三項關鍵因素——可用性、信任度和識字能力——這些因素形塑了這些互動，且必須加以處理，才能讓 MT 與使用者需求保持一致。透過探討這些面向，我們提供見解，以使用者為中心的方法引導未來的 MT。

##### **Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization**
2502.13778v1 by Jiaqi Li, Xizhong Guo, Yang Zhao, Lvyang Zhang, Lidong Zhai

Rapid industrial digitalization has created intricate cybersecurity demands
that necessitate effective validation methods. While cyber ranges and
simulation platforms are widely deployed, they frequently face limitations in
scenario diversity and creation efficiency. In this paper, we present
SpiderSim, a theoretical cybersecurity simulation platform enabling rapid and
lightweight scenario generation for industrial digitalization security
research. At its core, our platform introduces three key innovations: a
structured framework for unified scenario modeling, a multi-agent collaboration
mechanism for automated generation, and modular atomic security capabilities
for flexible scenario composition. Extensive implementation trials across
multiple industrial digitalization contexts, including marine ranch monitoring
systems, validate our platform's capacity for broad scenario coverage with
efficient generation processes. Built on solid theoretical foundations and
released as open-source software, SpiderSim facilitates broader research and
development in automated security testing for industrial digitalization.

摘要：快速工業數位化創造了複雜的網路安全需求，這需要有效的驗證方法。雖然網路範圍和模擬平台廣泛部署，但它們在場景多樣性和創建效率方面經常面臨限制。在本文中，我們提出了 SpiderSim，這是一個理論網路安全模擬平台，可以為工業數位化安全研究快速且輕鬆地生成場景。從根本上來說，我們的平台引入了三項關鍵創新：統一場景建模的結構化框架、用於自動生成的多代理協作機制，以及用於靈活場景組合的模組化原子安全功能。橫跨多個工業數位化情境的廣泛實作試驗，包括海洋牧場監控系統，驗證了我們的平台在有效率的生成程序下具有廣泛場景涵蓋範圍的能力。SpiderSim 建立在穩固的理論基礎上，並作為開源軟體釋出，促進了工業數位化自動化安全測試的更廣泛研究和開發。

##### **EHOP: A Dataset of Everyday NP-Hard Optimization Problems**
2502.13776v1 by Alex Duchnowski, Ellie Pavlick, Alexander Koller

We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a
collection of NP-hard optimization problems expressed in natural language. EHOP
includes problem formulations that could be found in computer science
textbooks, versions that are dressed up as problems that could arise in real
life, and variants of well-known problems with inverted rules. We find that
state-of-the-art LLMs, across multiple prompting strategies, systematically
solve textbook problems more accurately than their real-life and inverted
counterparts. We argue that this constitutes evidence that LLMs adapt solutions
seen during training, rather than leveraging reasoning abilities that would
enable them to generalize to novel problems.

摘要：我們介紹日常難解最佳化問題 (EHOP) 的資料集，這是以自然語言表示的一系列 NP 難最佳化問題。EHOP 包含可在電腦科學教科書中找到的問題表述、包裝成現實生活中可能出現的問題的版本，以及規則顛倒的知名問題變體。我們發現最先進的 LLM 在多種提示策略中，系統性地比現實生活和顛倒問題更準確地解決教科書問題。我們認為這構成 LLM 會調整在訓練期間看到的解法的證據，而不是利用能讓它們概化到新問題的推理能力。

##### **VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare**
2502.13775v1 by Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem

Alignment techniques have become central to ensuring that Large Language
Models (LLMs) generate outputs consistent with human values. However, existing
alignment paradigms often model an averaged or monolithic preference, failing
to account for the diversity of perspectives across cultures, demographics, and
communities. This limitation is particularly critical in health-related
scenarios, where plurality is essential due to the influence of culture,
religion, personal values, and conflicting opinions. Despite progress in
pluralistic alignment, no prior work has focused on health, likely due to the
unavailability of publicly available datasets. To address this gap, we
introduce VITAL, a new benchmark dataset comprising 13.1K value-laden
situations and 5.4K multiple-choice questions focused on health, designed to
assess and benchmark pluralistic alignment methodologies. Through extensive
evaluation of eight LLMs of varying sizes, we demonstrate that existing
pluralistic alignment techniques fall short in effectively accommodating
diverse healthcare beliefs, underscoring the need for tailored AI alignment in
specific domains. This work highlights the limitations of current approaches
and lays the groundwork for developing health-specific alignment solutions.

摘要：對齊技術已成為確保大型語言模型 (LLM) 產生與人類價值觀一致的輸出的核心。然而，現有的對齊範例通常會建模平均或單一的偏好，無法考量跨文化、人口統計和社群的不同觀點。此限制在與健康相關的場景中特別重要，因為在這種場景中，由於文化、宗教、個人價值觀和相互衝突的意見的影響，多元性是必要的。儘管多元對齊已取得進展，但沒有任何先前的工作專注於健康，這可能是因為缺乏公開可用的資料集。為了解決此差距，我們引入了 VITAL，這是一個新的基準資料集，包含 13.1K 個價值觀念的情境和 5.4K 個選擇題，專注於健康，旨在評估和基準多元對齊方法。透過對八個不同規模的 LLM 進行廣泛評估，我們證明現有的多元對齊技術無法有效適應不同的醫療保健信念，這強調了在特定領域中需要量身打造的 AI 對齊。這項工作突顯了當前方法的限制，並為開發特定於健康的對齊解決方案奠定了基礎。

##### **A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem**
2502.13769v1 by Juan A. Aledo, José A. Gámez, Alejandro Rosete

In rank aggregation problems (RAP), the solution is usually a consensus
ranking that generalizes a set of input orderings. There are different variants
that differ not only in terms of the type of rankings that are used as input
and output, but also in terms of the objective function employed to evaluate
the quality of the desired output ranking. In contrast, in some machine
learning tasks (e.g. subgroup discovery) or multimodal optimization tasks,
attention is devoted to obtaining several models/results to account for the
diversity in the input data or across the search landscape. Thus, in this paper
we propose to provide, as the solution to an RAP, a set of rankings to better
explain the preferences expressed in the input orderings. We exemplify our
proposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists
in finding a single consensus ranking (with ties) that generalizes a set of
input rankings codified as a precedence matrix. To address this, we introduce
the Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP
that aims to produce not a single ranking as output but a set of consensus
rankings. Experimental results are presented to illustrate this proposal,
showing how, by providing a set of consensus rankings, the fitness of the
solution significantly improves with respect to the one of the original OBOP,
without losing comprehensibility.

摘要：在等級彙總問題 (RAP) 中，解決方案通常是共識排名，概括了一組輸入排序。有不同的變體，它們之間的差異不僅在於用作輸入和輸出的排名類型，還在於用於評估所需輸出排名的品質的目標函數。相比之下，在某些機器學習任務（例如子群發現）或多模態優化任務中，注意力集中於獲得多個模型/結果，以說明輸入數據或搜索範圍中的多樣性。因此，在本文中，我們建議提供一組排名作為 RAP 的解決方案，以更好地解釋輸入排序中表達的偏好。我們通過最佳桶序問題 (OBOP) 來例證我們的提案，OBOP 是一個 RAP，它包括找到一個單一的共識排名（帶有聯繫），概括了一組編碼為優先矩陣的輸入排名。為了解決這個問題，我們引入了最佳桶序問題集 (OSBOP)，這是 OBOP 的一個概括，其目的是不產生單一的排名作為輸出，而是產生一組共識排名。提供了實驗結果來說明這個提案，展示了通過提供一組共識排名，解決方案的適應度如何相對於原始 OBOP 的適應度顯著提高，而不會失去可理解性。

##### **AI Software Engineer: Programming with Trust**
2502.13767v1 by Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray

Large Language Models (LLMs) have shown surprising proficiency in generating
code snippets, promising to automate large parts of software engineering via
artificial intelligence (AI). We argue that successfully deploying AI software
engineers requires a level of trust equal to or even greater than the trust
established by human-driven software engineering practices. The recent trend
toward LLM agents offers a path toward integrating the power of LLMs to create
new code with the power of analysis tools to increase trust in the code. This
opinion piece comments on whether LLM agents could dominate software
engineering workflows in the future and whether the focus of programming will
shift from programming at scale to programming with trust.

摘要：大型語言模型 (LLM) 在產生程式碼片段方面展現出驚人的能力，承諾透過人工智慧 (AI) 自動化軟體工程的大部分。我們認為，成功部署 AI 軟體工程師需要與人為驅動的軟體工程實務建立的信任程度相同甚至更高的信任水準。最近朝向 LLM 代理人的趨勢提供了一條路徑，可以整合 LLM 的能力，以建立新的程式碼，並利用分析工具的能力來增加對程式碼的信任。這篇意見文章評論了 LLM 代理人是否能在未來主導軟體工程工作流程，以及程式設計的重點是否會從大規模程式設計轉移到建立信任的程式設計。

##### **GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking**
2502.13766v1 by Florian Schneider, Carolin Holtermann, Chris Biemann, Anne Lauscher

Large Vision-Language Models (LVLMs) have recently gained attention due to
their distinctive performance and broad applicability. While it has been
previously shown that their efficacy in usage scenarios involving non-Western
contexts falls short, existing studies are limited in scope, covering just a
narrow range of cultures, focusing exclusively on a small number of cultural
aspects, or evaluating a limited selection of models on a single task only.
Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive
multimodal benchmark designed to assess a broad spectrum of cultural knowledge
across 144 countries representing six global macro-regions. GIMMICK comprises
six tasks built upon three new datasets that span 728 unique cultural events or
facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary
and 26 open-weight models of all sizes. We systematically examine (1) regional
cultural biases, (2) the influence of model size, (3) input modalities, and (4)
external cues. Our analyses reveal strong biases toward Western cultures across
models and tasks and highlight strong correlations between model size and
performance, as well as the effectiveness of multimodal input and external
geographic cues. We further find that models have more knowledge of tangible
than intangible aspects (e.g., food vs. rituals) and that they excel in
recognizing broad cultural origins but struggle with a more nuanced
understanding.

摘要：大型視覺語言模型 (LVLMs) 最近因其獨特的效能和廣泛的適用性而備受矚目。雖然先前已顯示出它們在涉及非西方語境的使用情境中的功效不足，但現有研究的範圍有限，僅涵蓋少數文化，專注於少數文化面向，或僅針對單一任務評估有限的模型選項。為了促進全球包容性的 LVLM 研究，我們引入了 GIMMICK，這是一個廣泛的多模態基準，旨在評估橫跨六個全球巨型區域的 144 個國家的廣泛文化知識。GIMMICK 包含六項任務，建立在三個新的資料集上，涵蓋 728 個獨特的文化事件或面向，我們在這些面向評估了 20 個 LVLMs 和 11 個 LLM，包括五個專有模型和 26 個各種規模的開放權重模型。我們系統性地檢驗 (1) 區域文化偏見、(2) 模型規模的影響、(3) 輸入模式，以及 (4) 外部提示。我們的分析揭示了所有模型和任務中對西方文化的強烈偏見，並強調了模型規模和效能之間的強烈相關性，以及多模式輸入和外部地理提示的有效性。我們進一步發現，模型對有形面向（例如食物與儀式）的了解多於無形面向，而且它們擅長辨識廣泛的文化起源，但在更細緻的理解上則有困難。

##### **SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning**
2502.13753v1 by Renxi Wang, Honglin Mu, Liqun Ma, Lizhi Lin, Yunlong Feng, Timothy Baldwin, Xudong Han, Haonan Li

Evaluating large language models' (LLMs) long-context understanding
capabilities remains challenging. We present SCALAR (Scientific Citation-based
Live Assessment of Long-context Academic Reasoning), a novel benchmark that
leverages academic papers and their citation networks. SCALAR features
automatic generation of high-quality ground truth labels without human
annotation, controllable difficulty levels, and a dynamic updating mechanism
that prevents data contamination. Using ICLR 2025 papers, we evaluate 8
state-of-the-art LLMs, revealing key insights about their capabilities and
limitations in processing long scientific documents across different context
lengths and reasoning types. Our benchmark provides a reliable and sustainable
way to track progress in long-context understanding as LLM capabilities evolve.

摘要：評估大型語言模型 (LLM) 的長文本理解能力仍然具有挑戰性。我們提出 SCALAR（基於科學引文的長文本學術推理現場評量），這是一個利用學術論文及其引文網路的新基準。SCALAR 的特點是自動生成高品質的真實標籤，無需人工註解、可控的難度等級，以及可防止資料污染的動態更新機制。使用 ICLR 2025 年的論文，我們評估了 8 個最先進的 LLM，揭示了它們在處理不同長度的科學文件和推理類型時的能力和限制方面的關鍵見解。我們的基準提供了一種可靠且可持續的方法來追蹤 LLM 能力演進時長文本理解的進度。

##### **RobustX: Robust Counterfactual Explanations Made Easy**
2502.13751v1 by Junqi Jiang, Luca Marzari, Aaryan Purohit, Francesco Leofante

The increasing use of Machine Learning (ML) models to aid decision-making in
high-stakes industries demands explainability to facilitate trust.
Counterfactual Explanations (CEs) are ideally suited for this, as they can
offer insights into the predictions of an ML model by illustrating how changes
in its input data may lead to different outcomes. However, for CEs to realise
their explanatory potential, significant challenges remain in ensuring their
robustness under slight changes in the scenario being explained. Despite the
widespread recognition of CEs' robustness as a fundamental requirement, a lack
of standardised tools and benchmarks hinders a comprehensive and effective
comparison of robust CE generation methods. In this paper, we introduce
RobustX, an open-source Python library implementing a collection of CE
generation and evaluation methods, with a focus on the robustness property.
RobustX provides interfaces to several existing methods from the literature,
enabling streamlined access to state-of-the-art techniques. The library is also
easily extensible, allowing fast prototyping of novel robust CE generation and
evaluation methods.

摘要：隨著機器學習 (ML) 模型在高風險產業中輔助決策制定日益廣泛，可解釋性需求日增，以利於建立信任。反事實解釋 (CE) 非常適合此目的，因為它們可以透過說明輸入資料的變更如何導致不同的結果，進而提供對 ML 模型預測的見解。然而，要讓 CE 發揮其解釋潛力，在確保其在所解釋情境中對微小變更具有穩健性方面，仍有許多挑戰。儘管 CE 的穩健性廣泛被認為是一項基本要求，但缺乏標準化工具和基準，會阻礙對穩健 CE 生成方法進行全面且有效的比較。在本文中，我們介紹了 RobustX，這是一個開源的 Python 函式庫，實作了一系列 CE 生成和評估方法，並專注於穩健性屬性。RobustX 提供了與文獻中多種現有方法的介面，讓使用者可以簡化存取最先進的技術。此函式庫也容易擴充，可以快速建構原型，以進行新穎的穩健 CE 生成和評估方法。

##### **Inference of Abstraction for Grounded Predicate Logic**
2502.13743v1 by Hiroyuki Kido

An important open question in AI is what simple and natural principle enables
a machine to reason logically for meaningful abstraction with grounded symbols.
This paper explores a conceptually new approach to combining probabilistic
reasoning and predicative symbolic reasoning over data. We return to the era of
reasoning with a full joint distribution before the advent of Bayesian
networks. We then discuss that a full joint distribution over models of
exponential size in propositional logic and of infinite size in predicate logic
should be simply derived from a full joint distribution over data of linear
size. We show that the same process is not only enough to generalise the
logical consequence relation of predicate logic but also to provide a new
perspective to rethink well-known limitations such as the undecidability of
predicate logic, the symbol grounding problem and the principle of explosion.
The reproducibility of this theoretical work is fully demonstrated by the
included proofs.

摘要：在人工智慧中，一個重要的開放性問題是，什麼簡單且自然的原則能讓機器以接地的符號進行有意義的抽象邏輯推理。
本文探討了一種概念上新的方法，用於結合機率推理和資料上的述詞符號推理。我們回到貝氏網路出現之前的，使用完整聯合分配進行推理的時代。接著我們討論，命題邏輯中指數大小的模型和述詞邏輯中無限大小的模型的完整聯合分配，應該可以簡單地從線性大小的資料完整聯合分配中導出。我們展示相同的程序不僅足以概括述詞邏輯的邏輯後果關係，也能提供一個新的觀點來重新思考眾所周知的限制，例如述詞邏輯的不可判定性、符號接地問題和爆炸原理。
包含的證明充分展示了這項理論工作的可複製性。

##### **Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding**
2502.13738v1 by Keqin Peng, Liang Ding, Yuanxin Ouyang, Meng Fang, Yancheng Yuan, Dacheng Tao

Large language models (LLMs) excel at a range of tasks through in-context
learning (ICL), where only a few task examples guide their predictions.
However, prior research highlights that LLMs often overlook input-label mapping
information in ICL, relying more on their pre-trained knowledge. To address
this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method
that emphasizes input-label mapping by contrasting the output distributions
between positive and negative in-context examples. Experiments on 7 natural
language understanding (NLU) tasks show that our ICCD method brings consistent
and significant improvement (up to +2.1 improvement on average) upon 6
different scales of LLMs without requiring additional training. Our approach is
versatile, enhancing performance with various demonstration selection methods,
demonstrating its broad applicability and effectiveness. The code and scripts
will be publicly released.

摘要：大型語言模型 (LLM) 透過情境學習 (ICL) 擅長於各種任務，其中僅有少數任務範例引導其預測。
然而，先前的研究強調，LLM 在 ICL 中經常忽略輸入標籤對應資訊，而更多依賴其預先訓練的知識。
為了解決此問題，我們引進情境對比解碼 (ICCD) 這一種創新方法，透過對比正負情境範例之間的輸出分布，強調輸入標籤對應。
在 7 項自然語言理解 (NLU) 任務上的實驗顯示，我們的 ICCD 方法在 6 種不同規模的 LLM 上帶來一致且顯著的改善（平均改善幅度高達 +2.1），而無需額外訓練。
我們的做法很靈活，透過各種示範選擇方法提升效能，展現其廣泛的適用性和有效性。
程式碼和腳本將公開釋出。

##### **Robust Counterfactual Inference in Markov Decision Processes**
2502.13731v1 by Jessica Lally, Milad Kazemi, Nicola Paoletti

This paper addresses a key limitation in existing counterfactual inference
methods for Markov Decision Processes (MDPs). Current approaches assume a
specific causal model to make counterfactuals identifiable. However, there are
usually many causal models that align with the observational and interventional
distributions of an MDP, each yielding different counterfactual distributions,
so fixing a particular causal model limits the validity (and usefulness) of
counterfactual inference. We propose a novel non-parametric approach that
computes tight bounds on counterfactual transition probabilities across all
compatible causal models. Unlike previous methods that require solving
prohibitively large optimisation problems (with variables that grow
exponentially in the size of the MDP), our approach provides closed-form
expressions for these bounds, making computation highly efficient and scalable
for non-trivial MDPs. Once such an interval counterfactual MDP is constructed,
our method identifies robust counterfactual policies that optimise the
worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate
our method on various case studies, demonstrating improved robustness over
existing methods.

摘要：本文探討了馬可夫決策過程 (MDP) 中現有反事實推論方法的一個關鍵限制。目前的方法假設一個特定的因果模型，以使反事實可識別。然而，通常有許多因果模型與 MDP 的觀察和介入分佈一致，每個模型產生不同的反事實分佈，因此固定一個特定的因果模型會限制反事實推論的有效性（和實用性）。我們提出了一種新穎的非參數方法，它計算所有相容因果模型中反事實轉移機率的緊密界限。與需要解決難以禁止的最佳化問題（其中變數隨著 MDP 的大小呈指數成長）的先前方法不同，我們的做法提供了這些界限的閉合形式表達式，使計算對於非平凡的 MDP 具有高度效率和可擴充性。一旦構造了這樣的區間反事實 MDP，我們的做法便會識別出穩健的反事實策略，以最佳化相對於不確定的區間 MDP 機率的最差獎勵。我們在各種案例研究中評估我們的做法，展示出比現有方法更好的穩健性。

##### **Secure Federated Data Distillation**
2502.13728v1 by Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, Antonino Nocera

Dataset Distillation (DD) is a powerful technique for reducing large datasets
into compact, representative synthetic datasets, accelerating Machine Learning
training. However, traditional DD methods operate in a centralized manner,
which poses significant privacy threats and reduces its applicability. To
mitigate these risks, we propose a Secure Federated Data Distillation framework
(SFDD) to decentralize the distillation process while preserving privacy.Unlike
existing Federated Distillation techniques that focus on training global models
with distilled knowledge, our approach aims to produce a distilled dataset
without exposing local contributions. We leverage the gradient-matching-based
distillation method, adapting it for a distributed setting where clients
contribute to the distillation process without sharing raw data. The central
aggregator iteratively refines a synthetic dataset by integrating client-side
updates while ensuring data confidentiality. To make our approach resilient to
inference attacks perpetrated by the server that could exploit gradient updates
to reconstruct private data, we create an optimized Local Differential Privacy
approach, called LDPO-RLD (Label Differential Privacy Obfuscation via
Randomized Linear Dispersion). Furthermore, we assess the framework's
resilience against malicious clients executing backdoor attacks and demonstrate
robustness under the assumption of a sufficient number of participating
clients. Our experimental results demonstrate the effectiveness of SFDD and
that the proposed defense concretely mitigates the identified vulnerabilities,
with minimal impact on the performance of the distilled dataset. By addressing
the interplay between privacy and federation in dataset distillation, this work
advances the field of privacy-preserving Machine Learning making our SFDD
framework a viable solution for sensitive data-sharing applications.

摘要：資料集萃取 (DD) 是一種強大的技術，用於將大型資料集縮減成精簡、具代表性的合成資料集，加速機器學習訓練。然而，傳統的 DD 方法以集中式方式運作，這會造成重大的隱私威脅，並降低其適用性。為了降低這些風險，我們提出一個安全的聯邦資料萃取架構 (SFDD)，以分散萃取程序，同時保護隱私。與現有的聯邦萃取技術不同，這些技術專注於使用萃取知識訓練全球模型，我們的做法旨在產生萃取資料集，而不會暴露本地貢獻。我們利用基於梯度匹配的萃取方法，將其調整為分散式設定，其中客戶端會貢獻萃取程序，而不會分享原始資料。中央聚合器透過整合客戶端更新，反覆改善合成資料集，同時確保資料機密性。為了讓我們的做法能抵抗伺服器執行的推論攻擊，伺服器可能利用梯度更新重建私人資料，我們建立了一個最佳化的局部差分隱私做法，稱為 LDPO-RLD（標籤差分隱私混淆，透過隨機線性分散）。此外，我們評估架構對執行後門攻擊的惡意客戶端的復原力，並在假設有足夠參與客戶端的情況下證明其穩健性。我們的實驗結果證明了 SFDD 的有效性，而且所提出的防禦措施具體減輕了已識別的漏洞，對萃取資料集的效能影響最小。透過解決資料集萃取中隱私和聯邦之間的交互作用，這項工作推動了隱私保護機器學習的領域，使我們的 SFDD 架構成為敏感資料分享應用程式的可行解決方案。

##### **Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method**
2502.13725v1 by Juyuan Zhang, Wei Zhu, Jiechao Gao

Time series modeling holds significant importance in many real-world
applications and has been extensively studied. While pre-trained foundation
models have made impressive strides in the fields of natural language
processing (NLP) and computer vision (CV), their development in time series
domains has been constrained by data sparsity. A series of recent studies have
demonstrated that large language models (LLMs) possess robust pattern
recognition and reasoning abilities over complex sequences of tokens. However,
the current literature have yet striked a high-quality balance between (a)
effectively aligning the time series and natural language modalities, and (b)
keeping the inference efficiency. To address the above issues, we now propose
the Time-LlaMA framework. Time-LlaMA first converts the time series input into
token embeddings through a linear tokenization mechanism. Second, the time
series token embeddings are aligned with the text prompts. Third, to further
adapt the LLM backbone for time series modeling, we have developed a dynamic
low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most
suitable LoRA modules at each layer of the Transformer backbone for each time
series input, enhancing the model's predictive capabilities. Our experimental
results on an extensive collection of challenging real-world time series tasks
confirm that our proposed method achieves the state-of-the-art (SOTA)
performance.

摘要：時序建模在許多實際應用中具有重要意義，並且已被廣泛研究。雖然預先訓練好的基礎模型在自然語言處理 (NLP) 和電腦視覺 (CV) 領域取得令人印象深刻的進展，但它們在時序領域的發展受到資料稀疏性的限制。一系列最近的研究表明，大型語言模型 (LLM) 對於複雜的代幣序列具備強大的模式辨識和推理能力。然而，目前的文獻尚未在 (a) 有效對齊時序和自然語言模態，以及 (b) 保持推理效率之間取得高品質的平衡。為了解決上述問題，我們現在提出 Time-LlaMA 框架。Time-LlaMA 首先透過線性標記化機制將時序輸入轉換為代幣嵌入。其次，時序代幣嵌入與文字提示對齊。第三，為了進一步調整 LLM 主幹以進行時序建模，我們開發了一種動態低秩適應技術 (D-LoRA)。D-LoRA 會動態選擇 Transformer 主幹中每一層最合適的 LoRA 模組，以增強模型的預測能力。我們在大量具有挑戰性的真實世界時序任務集合中進行的實驗結果證實，我們提出的方法達到了最先進 (SOTA) 的效能。

##### **Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values**
2502.13723v1 by Hongbo Zhang, Han Cui, Guangsheng Bao, Linyi Yang, Jun Wang, Yue Zhang

We introduce Direct Value Optimization (DVO), an innovative reinforcement
learning framework for enhancing large language models in complex reasoning
tasks. Unlike traditional methods relying on preference labels, DVO utilizes
value signals at individual reasoning steps, optimizing models via a mean
squared error loss. The key benefit of DVO lies in its fine-grained
supervision, circumventing the need for labor-intensive human annotations.
Target values within the DVO are estimated using either Monte Carlo Tree Search
or an outcome value model. Our empirical analysis on both mathematical and
commonsense reasoning tasks shows that DVO consistently outperforms existing
offline preference optimization techniques, even with fewer training steps.
These findings underscore the importance of value signals in advancing
reasoning capabilities and highlight DVO as a superior methodology under
scenarios lacking explicit human preference information.

摘要：我們引進直接價值最佳化 (DVO)，一種創新的強化學習架構，用於增強大型語言模型在複雜推理任務中的能力。與依賴偏好標籤的傳統方法不同，DVO 利用個別推理步驟中的價值信號，透過均方誤差損失來最佳化模型。DVO 的主要優點在於其細緻的監督，避免了需要大量人力註解。DVO 中的目標值是使用蒙地卡羅樹狀搜尋或結果值模型來估計的。我們在數學和常識推理任務上的實證分析顯示，DVO 始終優於現有的離線偏好最佳化技術，即使訓練步驟較少。這些發現強調了價值信號在推進推理能力方面的重要性，並強調 DVO 在缺乏明確人類偏好資訊的情況下是一種優越的方法。

##### **Learning Novel Transformer Architecture for Time-series Forecasting**
2502.13721v1 by Juyuan Zhang, Wei Zhu, Jiechao Gao

Despite the success of Transformer-based models in the time-series prediction
(TSP) tasks, the existing Transformer architecture still face limitations and
the literature lacks comprehensive explorations into alternative architectures.
To address these challenges, we propose AutoFormer-TS, a novel framework that
leverages a comprehensive search space for Transformer architectures tailored
to TSP tasks. Our framework introduces a differentiable neural architecture
search (DNAS) method, AB-DARTS, which improves upon existing DNAS approaches by
enhancing the identification of optimal operations within the architecture.
AutoFormer-TS systematically explores alternative attention mechanisms,
activation functions, and encoding operations, moving beyond the traditional
Transformer design. Extensive experiments demonstrate that AutoFormer-TS
consistently outperforms state-of-the-art baselines across various TSP
benchmarks, achieving superior forecasting accuracy while maintaining
reasonable training efficiency.

摘要：儘管 Transformer 模型在時序預測 (TSP) 任務中取得成功，但現有的 Transformer 架構仍面臨限制，且文獻中缺乏對替代架構的全面探討。為了應對這些挑戰，我們提出 AutoFormer-TS，一個新穎的框架，它利用一個全面的搜尋空間，專門針對 TSP 任務設計 Transformer 架構。我們的框架引入了一種可微分神經架構搜尋 (DNAS) 方法 AB-DARTS，它透過增強對架構中最佳運算的識別來改進現有的 DNAS 方法。AutoFormer-TS 系統性地探討了替代注意機制、活化函數和編碼運算，超越了傳統的 Transformer 設計。廣泛的實驗證明，AutoFormer-TS 在各種 TSP 基準測試中始終優於最先進的基準，在維持合理的訓練效率的同時，達到了優異的預測準確度。

##### **TrustRAG: An Information Assistant with Retrieval Augmented Generation**
2502.13719v1 by Yixing Fan, Qiang Yan, Wenshan Wang, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng

\Ac{RAG} has emerged as a crucial technique for enhancing large models with
real-time and domain-specific knowledge. While numerous improvements and
open-source tools have been proposed to refine the \ac{RAG} framework for
accuracy, relatively little attention has been given to improving the
trustworthiness of generated results. To address this gap, we introduce
TrustRAG, a novel framework that enhances \ac{RAG} from three perspectives:
indexing, retrieval, and generation. Specifically, in the indexing stage, we
propose a semantic-enhanced chunking strategy that incorporates hierarchical
indexing to supplement each chunk with contextual information, ensuring
semantic completeness. In the retrieval stage, we introduce a utility-based
filtering mechanism to identify high-quality information, supporting answer
generation while reducing input length. In the generation stage, we propose
fine-grained citation enhancement, which detects opinion-bearing sentences in
responses and infers citation relationships at the sentence-level, thereby
improving citation accuracy. We open-source the TrustRAG framework and provide
a demonstration studio designed for excerpt-based question answering tasks
\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we
aim to help researchers: 1) systematically enhancing the trustworthiness of
\ac{RAG} systems and (2) developing their own \ac{RAG} systems with more
reliable outputs.

摘要：\Ac{RAG} 已成为一种至关重要的技术，用于增强大型模型，使其具有实时且特定于领域的知识。虽然已经提出了许多改进和开源工具来改进 \ac{RAG} 框架的准确性，但对于提高生成结果的可信度却鲜有关注。为了解决这一差距，我们引入了 TrustRAG，这是一个新颖的框架，它从三个角度增强了 \ac{RAG}：索引、检索和生成。具体来说，在索引阶段，我们提出了一种语义增强分块策略，该策略结合了分层索引来为每个块补充上下文信息，从而确保语义完整性。在检索阶段，我们引入了一种基于效用的过滤机制来识别高质量信息，支持答案生成，同时减少输入长度。在生成阶段，我们提出了细粒度引用增强，它检测响应中的观点句，并在句子级别推断引用关系，从而提高引用准确性。我们开源了 TrustRAG 框架，并提供了一个演示工作室，专为基于摘录的问题回答任务而设计\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}。基于这些，我们的目标是帮助研究人员：1) 系统地提高 \ac{RAG} 系统的可信度，以及 (2) 开发他们自己的 \ac{RAG} 系统，并获得更可靠的输出。

##### **Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis**
2502.13718v1 by Chengyan Wu, Bolei Ma, Ningyuan Deng, Yanqing He, Yun Xue

Aspect-based sentiment analysis (ABSA) is a sequence labeling task that has
garnered growing research interest in multilingual contexts. However, recent
studies lack more robust feature alignment and finer aspect-level alignment. In
this paper, we propose a novel framework, Multi-Scale and Multi-Objective
optimization (MSMO) for cross-lingual ABSA. During multi-scale alignment, we
achieve cross-lingual sentence-level and aspect-level alignment, aligning
features of aspect terms in different contextual environments. Specifically, we
introduce code-switched bilingual sentences into the language discriminator and
consistency training modules to enhance the model's robustness. During
multi-objective optimization, we design two optimization objectives: supervised
training and consistency training, aiming to enhance cross-lingual semantic
alignment. To further improve model performance, we incorporate distilled
knowledge of the target language into the model. Results show that MSMO
significantly enhances cross-lingual ABSA by achieving state-of-the-art
performance across multiple languages and models.

摘要：面向方面的观点分析 (ABSA) 是一項序列標籤任務，在多語言環境中引起了越來越多的研究興趣。然而，最近的研究缺乏更強大的特徵對齊和更精細的方面層級對齊。在本文中，我們提出了一個新的框架，即多尺度和多目標優化 (MSMO) 用於跨語言 ABSA。在多尺度對齊期間，我們實現了跨語言句子層級和方面層級對齊，對齊了不同上下文環境中方面條款的特徵。具體來說，我們將代碼轉換的雙語句子引入語言判別器和一致性訓練模組中，以增強模型的健壯性。在多目標優化期間，我們設計了兩個優化目標：監督訓練和一致性訓練，旨在增強跨語言語義對齊。為了進一步提高模型效能，我們將目標語言的蒸餾知識納入模型中。結果表明，MSMO 透過在多種語言和模型中實現最先進的效能，顯著增強了跨語言 ABSA。

##### **Causes and Strategies in Multiagent Systems**
2502.13701v1 by Sylvia S. Kerkhove, Natasha Alechina, Mehdi Dastani

Causality plays an important role in daily processes, human reasoning, and
artificial intelligence. There has however not been much research on causality
in multi-agent strategic settings. In this work, we introduce a systematic way
to build a multi-agent system model, represented as a concurrent game
structure, for a given structural causal model. In the obtained so-called
causal concurrent game structure, transitions correspond to interventions on
agent variables of the given causal model. The Halpern and Pearl framework of
causality is used to determine the effects of a certain value for an agent
variable on other variables. The causal concurrent game structure allows us to
analyse and reason about causal effects of agents' strategic decisions. We
formally investigate the relation between causal concurrent game structures and
the original structural causal models.

摘要：因果關係在日常流程、人類推理和人工智慧中扮演著重要的角色。然而，對於多重代理策略設定中的因果關係，目前的研究並不多。在這項工作中，我們引入了一個系統性的方法，用於建構一個多重代理系統模型，表示為一個並發遊戲結構，用於給定的結構因果模型。在所得的所謂因果並發遊戲結構中，轉換對應於給定因果模型中代理變數的介入。Halpern 和 Pearl 的因果關係架構用於確定代理變數對其他變數的特定值的影響。因果並發遊戲結構允許我們分析和推理代理策略決策的因果關係。我們正式探討因果並發遊戲結構與原始結構因果模型之間的關係。

##### **Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora**
2502.13691v1 by Tristan Karch, Luca Engel, Philippe Schwaller, Frédéric Kaplan

As large language models (LLMs) converge towards similar capabilities, the
key to advancing their performance lies in identifying and incorporating
valuable new information sources. However, evaluating which text collections
are worth the substantial investment required for digitization, preprocessing,
and integration into LLM systems remains a significant challenge. We present a
novel approach to this challenge: an automated pipeline that evaluates the
potential information gain from text collections without requiring model
training or fine-tuning. Our method generates multiple choice questions (MCQs)
from texts and measures an LLM's performance both with and without access to
the source material. The performance gap between these conditions serves as a
proxy for the collection's information potential. We validate our approach
using three strategically selected datasets: EPFL PhD manuscripts (likely
containing novel specialized knowledge), Wikipedia articles (presumably part of
training data), and a synthetic baseline dataset. Our results demonstrate that
this method effectively identifies collections containing valuable novel
information, providing a practical tool for prioritizing data acquisition and
integration efforts.

摘要：隨著大型語言模型 (LLM) 朝著類似的能力收斂，提升其效能的關鍵在於識別和納入有價值的新資訊來源。然而，評估哪些文字集合值得投入數位化、前處理和整合到 LLM 系統中所需的龐大投資，仍然是一項重大的挑戰。我們提出了一種解決此挑戰的新穎方法：一個自動化管道，可評估文字集合的潛在資訊獲取，而無需進行模型訓練或微調。我們的做法會從文字中產生多選題 (MCQ)，並衡量 LLM 在有和沒有取得原始資料的情況下的表現。這些條件之間的表現差距可作為該集合資訊潛力的代理。我們使用三個策略性選定的資料集驗證我們的做法：EPFL 博士論文手稿（可能包含新的專業知識）、維基百科條目（可能是訓練資料的一部分）和一個合成基準資料集。我們的結果證明，此方法有效地找出包含有價值新資訊的集合，提供一個實用的工具來優先處理資料取得和整合工作。

##### **MoM: Linear Sequence Modeling with Mixture-of-Memories**
2502.13685v1 by Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng

Linear sequence modeling methods, such as linear attention, state space
modeling, and linear RNNs, offer significant efficiency improvements by
reducing the complexity of training and inference. However, these methods
typically compress the entire input sequence into a single fixed-size memory
state, which leads to suboptimal performance on recall-intensive downstream
tasks. Drawing inspiration from neuroscience, particularly the brain's ability
to maintain robust long-term memory while mitigating "memory interference", we
introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes
multiple independent memory states, with a router network directing input
tokens to specific memory states. This approach greatly enhances the overall
memory capacity while minimizing memory interference. As a result, MoM performs
exceptionally well on recall-intensive tasks, surpassing existing linear
sequence modeling techniques. Despite incorporating multiple memory states, the
computation of each memory state remains linear in complexity, allowing MoM to
retain the linear-complexity advantage during training, while
constant-complexity during inference. Our experimental results show that MoM
significantly outperforms current linear sequence models on downstream language
tasks, particularly recall-intensive tasks, and even achieves performance
comparable to Transformer models. The code is released at
https://github.com/OpenSparseLLMs/MoM and is also released as a part of
https://github.com/OpenSparseLLMs/Linear-MoE.

摘要：線性序列建模方法，例如線性注意力、狀態空間建模和線性 RNN，透過降低訓練和推論的複雜性，提供了顯著的效率改進。然而，這些方法通常會將整個輸入序列壓縮成一個單一的固定大小記憶體狀態，這會導致召回密集的下游任務的次佳效能。從神經科學中汲取靈感，特別是大腦在減輕「記憶干擾」的同時維持穩健長期記憶的能力，我們引入了一種名為記憶混合 (MoM) 的新架構。MoM 利用多個獨立的記憶體狀態，並透過路由器網路將輸入代碼導向特定的記憶體狀態。這種方法在最大程度減少記憶體干擾的同時，大幅提升了整體記憶體容量。因此，MoM 在召回密集任務上的表現極佳，超越了現有的線性序列建模技術。儘管整合了多個記憶體狀態，但每個記憶體狀態的運算在複雜度上仍然是線性的，這讓 MoM 在訓練期間能保有線性複雜度的優勢，而在推論期間則維持恆定複雜度。我們的實驗結果顯示，MoM 在下游語言任務上明顯優於目前的線性序列模型，特別是召回密集任務，甚至達到了與 Transformer 模型相當的效能。程式碼已發布於 https://github.com/OpenSparseLLMs/MoM，並作為 https://github.com/OpenSparseLLMs/Linear-MoE 的一部分發布。

##### **An LLM-based Agent for Reliable Docker Environment Configuration**
2502.13681v1 by Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao

Environment configuration is a critical yet time-consuming step in software
development, especially when dealing with unfamiliar code repositories. While
Large Language Models (LLMs) demonstrate the potential to accomplish software
engineering tasks, existing methods for environment configuration often rely on
manual efforts or fragile scripts, leading to inefficiencies and unreliable
outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully
automate environment configuration and generate executable Dockerfiles for
arbitrary Python repositories. We address two major challenges: (1) enabling
the LLM agent to configure environments within isolated Docker containers, and
(2) ensuring the successful configuration process is recorded and accurately
transferred to a Dockerfile without error. To achieve this, we propose atomic
configuration synthesis, featuring a dual-environment architecture (internal
and external environment) with a rollback mechanism to prevent environment
"pollution" from failed commands, guaranteeing atomic execution (execute fully
or not at all) and a Dockerfile generator to transfer successful configuration
steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark
of 420 recent Python repositories with unit tests, where it achieves an 86.0%
success rate, outperforming the best baseline by 63.9%.

摘要：環境設定是軟體開發中一個關鍵但耗時的步驟，特別是在處理不熟悉的程式碼儲存庫時。雖然大型語言模型 (LLM) 展示了完成軟體工程任務的潛力，現有的環境設定方法通常依賴手動工作或脆弱的腳本，導致效率低下和結果不可靠。我們介紹了 Repo2Run，這是第一個基於 LLM 的代理，旨在完全自動化環境設定，並為任意 Python 儲存庫產生可執行的 Dockerfile。我們解決了兩個主要挑戰：(1) 使 LLM 代理能夠在隔離的 Docker 容器中設定環境，以及 (2) 確保成功的設定過程被記錄並準確傳輸到 Dockerfile 中而不會出錯。為此，我們提出了原子配置合成，它具有雙環境架構（內部和外部環境）和回滾機制，以防止環境被失敗的命令「污染」，保證原子執行（完全執行或完全不執行）和一個 Dockerfile 產生器，將成功的配置步驟傳輸到可執行的 Dockerfile 中。我們在我們提出的基準測試中評估了 Repo2Run，其中包含 420 個最近的 Python 儲存庫和單元測試，它達到了 86.0% 的成功率，比最佳基準高出 63.9%。

##### **SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation**
2502.13674v1 by Song Duong, Florian Le Bronnec, Alexandre Allauzen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari

Large Language Models (LLMs), when used for conditional text generation,
often produce hallucinations, i.e., information that is unfaithful or not
grounded in the input context. This issue arises in typical conditional text
generation tasks, such as text summarization and data-to-text generation, where
the goal is to produce fluent text based on contextual input. When fine-tuned
on specific domains, LLMs struggle to provide faithful answers to a given
context, often adding information or generating errors. One underlying cause of
this issue is that LLMs rely on statistical patterns learned from their
training data. This reliance can interfere with the model's ability to stay
faithful to a provided context, leading to the generation of ungrounded
information. We build upon this observation and introduce a novel
self-supervised method for generating a training set of unfaithful samples. We
then refine the model using a training process that encourages the generation
of grounded outputs over unfaithful ones, drawing on preference-based training.
Our approach leads to significantly more grounded text generation,
outperforming existing self-supervised techniques in faithfulness, as evaluated
through automatic metrics, LLM-based assessments, and human evaluations.

摘要：大型語言模型 (LLM) 在用於條件式文字產生時，
經常會產生幻覺，即不忠實或不基於輸入內容的資訊。這個問題出現在典型的條件式文字產生任務中，例如文字摘要和資料到文字的產生，其中目標是根據脈絡輸入產生流利的文字。當針對特定領域進行微調時，LLM 難以對給定的脈絡提供忠實的答案，通常會新增資訊或產生錯誤。這個問題的一個根本原因是 LLM 依賴從其訓練資料中學習到的統計模式。這種依賴性會干擾模型忠於所提供脈絡的能力，導致產生不合理的資訊。我們建立在這個觀察之上，並引入一種新穎的自我監督方法來產生一組不忠實的樣本訓練集。然後，我們使用訓練過程改善模型，該過程鼓勵產生合理的輸出，而不是不合理的輸出，並利用基於偏好的訓練。我們的做法導致更合理的文字產生，在忠實度方面優於現有的自我監督技術，透過自動化指標、基於 LLM 的評估和人工評估進行評估。

##### **PeerQA: A Scientific Question Answering Dataset from Peer Reviews**
2502.13668v1 by Tim Baumgärtner, Ted Briscoe, Iryna Gurevych

We present PeerQA, a real-world, scientific, document-level Question
Answering (QA) dataset. PeerQA questions have been sourced from peer reviews,
which contain questions that reviewers raised while thoroughly examining the
scientific article. Answers have been annotated by the original authors of each
paper. The dataset contains 579 QA pairs from 208 academic articles, with a
majority from ML and NLP, as well as a subset of other scientific communities
like Geoscience and Public Health. PeerQA supports three critical tasks for
developing practical QA systems: Evidence retrieval, unanswerable question
classification, and answer generation. We provide a detailed analysis of the
collected dataset and conduct experiments establishing baseline systems for all
three tasks. Our experiments and analyses reveal the need for
decontextualization in document-level retrieval, where we find that even simple
decontextualization approaches consistently improve retrieval performance
across architectures. On answer generation, PeerQA serves as a challenging
benchmark for long-context modeling, as the papers have an average size of 12k
tokens. Our code and data is available at https://github.com/UKPLab/peerqa.

摘要：<paragraph>我們提出 PeerQA，一個真實世界、科學的、文件層級的問答 (QA) 資料集。PeerQA 問題來自於同行評審，其中包含審查者在徹底審查科學文章時提出的問題。答案是由每篇論文的原始作者註解的。此資料集包含來自 208 篇學術文章的 579 個 QA 對，其中大部分來自 ML 和 NLP，以及其他科學社群（例如地球科學和公共衛生）的子集。PeerQA 支援開發實用 QA 系統的三項重要任務：證據檢索、無解答問題分類和答案產生。我們提供收集到的資料集的詳細分析，並進行實驗，為所有三項任務建立基準系統。我們的實驗和分析揭示了在文件層級檢索中去脈絡化的必要性，我們發現即使是簡單的去脈絡化方法也能持續改善跨架構的檢索效能。在答案產生方面，PeerQA 是一個用於長脈絡建模的具挑戰性基準，因為論文的平均大小為 12k 個符號。我們的程式碼和資料可於 https://github.com/UKPLab/peerqa 取得。</paragraph>

##### **Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models**
2502.13656v1 by Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen

Sentence embedding is essential for many NLP tasks, with contrastive learning
methods achieving strong performance using annotated datasets like NLI. Yet,
the reliance on manual labels limits scalability. Recent studies leverage large
language models (LLMs) to generate sentence pairs, reducing annotation
dependency. However, they overlook ranking information crucial for fine-grained
semantic distinctions. To tackle this challenge, we propose a method for
controlling the generation direction of LLMs in the latent space. Unlike
unconstrained generation, the controlled approach ensures meaningful semantic
divergence. Then, we refine exist sentence embedding model by integrating
ranking information and semantic information. Experiments on multiple
benchmarks demonstrate that our method achieves new SOTA performance with a
modest cost in ranking sentence synthesis.

摘要：句子嵌入對於許多 NLP 任務來說至關重要，對比學習方法使用 NLI 等註解資料集來達成強大的效能。然而，依賴人工標籤會限制可擴充性。最近的研究利用大型語言模型 (LLM) 來產生句子對，減少標註的依賴性。然而，他們忽略了對於細緻語義區別至關重要的排名資訊。為了應對這個挑戰，我們提出一個在潛在空間中控制 LLM 產生方向的方法。與不受約束的產生不同，受控方法確保有意義的語義分歧。然後，我們透過整合排名資訊和語義資訊來改善現有的句子嵌入模型。在多個基準測試上的實驗顯示，我們的模型以適度的句子合成排名成本，達到了新的 SOTA 效能。

##### **C2T: A Classifier-Based Tree Construction Method in Speculative Decoding**
2502.13652v1 by Feiye Huo, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Shengli Sun

The growing scale of Large Language Models (LLMs) has exacerbated inference
latency and computational costs. Speculative decoding methods, which aim to
mitigate these issues, often face inefficiencies in the construction of token
trees and the verification of candidate tokens. Existing strategies, including
chain mode, static tree, and dynamic tree approaches, have limitations in
accurately preparing candidate token trees for verification. We propose a novel
method named C2T that adopts a lightweight classifier to generate and prune
token trees dynamically. Our classifier considers additional feature variables
beyond the commonly used joint probability to predict the confidence score for
each draft token to determine whether it is the candidate token for
verification. This method outperforms state-of-the-art (SOTA) methods such as
EAGLE-2 on multiple benchmarks, by reducing the total number of candidate
tokens by 25% while maintaining or even improving the acceptance length.

摘要：大型語言模型 (LLM) 的規模日益擴大，加劇了推論延遲和運算成本。推測性解碼方法旨在緩解這些問題，但通常在令牌樹的建構和候選令牌的驗證過程中會遇到效率低下的問題。現有的策略，包括鏈模式、靜態樹和動態樹方法，在準確準備候選令牌樹以進行驗證方面存在限制。我們提出了一種名為 C2T 的新方法，它採用一個輕量級分類器來動態生成和修剪令牌樹。我們的分類器考慮了除了常用聯合機率之外的其他特徵變數，以預測每個草稿令牌的信心分數，以確定它是否是要驗證的候選令牌。此方法在多個基準測試中優於最先進 (SOTA) 方法，例如 EAGLE-2，它將候選令牌的總數減少了 25%，同時維持或甚至改善了接受長度。

##### **Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs**
2502.13648v1 by Youna Kim, Minjoon Choi, Sungmin Cho, Hyuhng Joon Kim, Sang-goo Lee, Taeuk Kim

Large Language Models (LLMs) enhance their problem-solving capability by
leveraging both parametric and external knowledge. Beyond leveraging external
knowledge to improve response accuracy, they require key capabilities for
reliable knowledge-handling: resolving conflicts between knowledge sources,
avoiding distraction from uninformative external knowledge, and abstaining when
sufficient knowledge is unavailable. Prior studies have examined these
scenarios in isolation or with limited scope. To systematically evaluate these
capabilities, we introduce a comprehensive framework for analyzing
knowledge-handling based on two key dimensions: the presence of parametric
knowledge and the informativeness of external knowledge. Through analysis, we
identify biases in knowledge utilization and examine how the ability to handle
one scenario impacts performance in others. Furthermore, we demonstrate that
training on data constructed based on the knowledge-handling scenarios improves
LLMs' reliability in integrating and utilizing knowledge.

摘要：大型語言模型 (LLM) 透過利用參數化和外部知識，增強它們的解決問題能力。除了利用外部知識來提升回應準確度之外，它們還需要具備可靠的知識處理關鍵能力：解決知識來源之間的衝突、避免因無意義的外部知識而分心，以及在知識不足時保持克制。先前的研究已孤立地或在有限的範圍內探討這些情境。為了系統性地評估這些能力，我們引進一個全面的架構，根據兩個關鍵面向來分析知識處理：參數化知識的存在與外部知識的資訊量。透過分析，我們找出知識利用中的偏誤，並探討處理一個情境的能力如何影響在其他情境中的表現。此外，我們證明在根據知識處理情境建構的資料上進行訓練，可以提升 LLM 在整合和利用知識方面的可靠性。

##### **Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh**
2502.13647v1 by Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto

Instruction tuning in low-resource languages remains underexplored due to
limited text data, particularly in government and cultural domains. To address
this, we introduce and open-source a large-scale (10,600 samples)
instruction-following (IFT) dataset, covering key institutional and cultural
knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of
procedural, legal, and structural governance topics. We employ LLM-assisted
data generation, comparing open-weight and closed-weight models for dataset
construction, and select GPT-4o as the backbone. Each entity of our dataset
undergoes full manual verification to ensure high quality. We also show that
fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent
performance improvements in both multiple-choice and generative tasks,
demonstrating the potential of LLM-assisted instruction tuning for low-resource
languages.

摘要：由於文本資料有限，特別是在政府和文化領域，因此低資源語言的指令調整仍未得到充分探索。為了解決這個問題，我們引入並開放了一個大型（10,600 個範例）指令遵循 (IFT) 資料集，涵蓋與哈薩克相關的主要制度和文化知識。我們的資料集增強了 LLM 對程序、法律和結構治理主題的理解。我們採用 LLM 輔助資料生成，比較開放權重和封閉權重模型以進行資料集建構，並選擇 GPT-4o 作為主幹。我們資料集的每個實體都經過完整的手動驗證，以確保高品質。我們還表明，在我們的資料集上微調 Qwen、Falcon 和 Gemma 會在多重選擇和生成任務中持續改善效能，這證明了 LLM 輔助指令調整在低資源語言中的潛力。

##### **D.Va: Validate Your Demonstration First Before You Use It**
2502.13646v1 by Qi Zhang, Zhiqing Xiao, Ruixuan Xiao, Lirong Gao, Junbo Zhao

In-context learning (ICL) has demonstrated significant potential in enhancing
the capabilities of large language models (LLMs) during inference. It's
well-established that ICL heavily relies on selecting effective demonstrations
to generate outputs that better align with the expected results. As for
demonstration selection, previous approaches have typically relied on intuitive
metrics to evaluate the effectiveness of demonstrations, which often results in
limited robustness and poor cross-model generalization capabilities. To tackle
these challenges, we propose a novel method, \textbf{D}emonstration
\textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration
validation perspective into this field. By introducing the demonstration
validation mechanism, our method effectively identifies demonstrations that are
both effective and highly generalizable. \textbf{D.Va} surpasses all existing
demonstration selection techniques across both natural language understanding
(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate
the robustness and generalizability of our approach across various language
models with different retrieval models.

摘要：情境式學習 (ICL) 已證明在推論期間增強大型語言模型 (LLM) 的能力方面具有顯著潛力。眾所周知，ICL 嚴重依賴於選擇有效的示範來產生更符合預期結果的輸出。至於示範選擇，先前的做法通常依賴於直覺指標來評估示範的有效性，這通常會導致穩健性受限和跨模型泛化能力不佳。為了應對這些挑戰，我們提出了一種新方法，**D**emonstration **VA**lidation（**D.Va**），將示範驗證觀點整合到這個領域。透過引入示範驗證機制，我們的做法有效地找出既有效又高度可泛化的示範。**D.Va** 超越所有現有的示範選擇技術，涵蓋自然語言理解 (NLU) 和自然語言生成 (NLG) 任務。此外，我們展示了我們的方法在具有不同檢索模型的各種語言模型中的穩健性和可泛化性。

##### **Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks**
2502.13645v1 by Ori Shapira, Shlomo E. Chazan, Amir DN Cohen

With the increasing prevalence of recorded human speech, spoken language
understanding (SLU) is essential for its efficient processing. In order to
process the speech, it is commonly transcribed using automatic speech
recognition technology. This speech-to-text transition introduces errors into
the transcripts, which subsequently propagate to downstream NLP tasks, such as
dialogue summarization. While it is known that transcript noise affects
downstream tasks, a systematic approach to analyzing its effects across
different noise severities and types has not been addressed. We propose a
configurable framework for assessing task models in diverse noisy settings, and
for examining the impact of transcript-cleaning techniques. The framework
facilitates the investigation of task model behavior, which can in turn support
the development of effective SLU solutions. We exemplify the utility of our
framework on three SLU tasks and four task models, offering insights regarding
the effect of transcript noise on tasks in general and models in particular.
For instance, we find that task models can tolerate a certain level of noise,
and are affected differently by the types of errors in the transcript.

摘要：隨著錄製人類語音的盛行，口語理解 (SLU) 對於有效處理語音至關重要。為了處理語音，通常使用自動語音辨識技術進行轉錄。這種語音轉文字的轉換會在轉錄過程中產生錯誤，這些錯誤會進一步傳播到下游的 NLP 任務，例如對話摘要。雖然已知轉錄雜訊會影響下游任務，但尚未針對不同雜訊嚴重程度和類型的影響進行系統化分析。我們提出一個可配置的架構，用於評估不同雜訊設定中的任務模型，並檢查轉錄清理技術的影響。此架構有助於調查任務模型行為，進而支援開發有效的 SLU 解决方案。我們在三個 SLU 任務和四個任務模型上展示了我們架構的效用，提供了關於轉錄雜訊對一般任務和特定模型的影響的見解。例如，我們發現任務模型可以容忍一定程度的雜訊，並且會受到轉錄中錯誤類型的不同影響。

##### **Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts**
2502.13640v1 by Maiya Goloburda, Nurkhan Laiyk, Diana Turmakhan, Yuxia Wang, Mukhammed Togmanov, Jonibek Mansurov, Askhat Sametov, Nurdaulet Mukhituly, Minghan Wang, Daniil Orel, Zain Muhammad Mujahid, Fajri Koto, Timothy Baldwin, Preslav Nakov

Large language models (LLMs) are known to have the potential to generate
harmful content, posing risks to users. While significant progress has been
made in developing taxonomies for LLM risks and safety evaluation prompts, most
studies have focused on monolingual contexts, primarily in English. However,
language- and region-specific risks in bilingual contexts are often overlooked,
and core findings can diverge from those in monolingual settings. In this
paper, we introduce Qorgau, a novel dataset specifically designed for safety
evaluation in Kazakh and Russian, reflecting the unique bilingual context in
Kazakhstan, where both Kazakh (a low-resource language) and Russian (a
high-resource language) are spoken. Experiments with both multilingual and
language-specific LLMs reveal notable differences in safety performance,
emphasizing the need for tailored, region-specific datasets to ensure the
responsible and safe deployment of LLMs in countries like Kazakhstan. Warning:
this paper contains example data that may be offensive, harmful, or biased.

摘要：大型語言模型 (LLM) 已知具有產生有害內容的潛力，對使用者構成風險。儘管在開發 LLM 風險分類法和安全評估提示方面已取得重大進展，但大多數研究都集中在單語境語境，主要是英語。然而，在雙語語境中特定於語言和地區的風險通常被忽視，而核心發現可能與單語境設定中的發現不同。在本文中，我們介紹了 Qorgau，這是一個專門為哈薩克語和俄語的安全評估而設計的新穎資料集，反映了哈薩克斯坦獨特的雙語語境，哈薩克語（低資源語言）和俄語（高資源語言）都在此處使用。使用多語言和特定語言 LLM 進行的實驗揭示了安全效能的顯著差異，強調了量身打造特定地區資料集的必要性，以確保在哈薩克斯坦等國家負責任且安全地部署 LLM。警告：本文包含可能令人反感、有害或有偏見的範例資料。

##### **Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks**
2502.13638v1 by Julian Vexler, Björn Vieten, Martin Nelke, Stefan Kramer

We present CavePerception, a framework for the analysis of sparse data from
sensor networks that incorporates elements of inverse modeling and forward
modeling. By integrating machine learning with physical modeling in a
hypotheses space, we aim to improve the interpretability of sparse, noisy, and
potentially incomplete sensor data. The framework assumes data from a
two-dimensional sensor network laid out in a graph structure that detects
certain objects, with certain motion patterns. Examples of such sensors are
magnetometers. Given knowledge about the objects and the way they act on the
sensors, one can develop a data generator that produces data from simulated
motions of the objects across the sensor field. The framework uses the
simulated data to infer object behaviors across the sensor network. The
approach is experimentally tested on real-world data, where magnetometers are
used on an airport to detect and identify aircraft motions. Experiments
demonstrate the value of integrating inverse and forward modeling, enabling
intelligent systems to better understand and predict complex, sensor-driven
events.

摘要：我們提出 CavePerception，一個用於分析來自感測器網路的稀疏資料的架構，其中結合了反向建模和正向建模的元素。透過在假設空間中整合機器學習和物理建模，我們旨在改善稀疏、雜訊和潛在不完整的感測器資料的可解釋性。該架構假設來自二維感測器網路的資料，該網路以圖形結構佈置，用於偵測特定物件，並具備特定動作模式。此類感測器的範例為磁強計。在具備關於物件及其對感測器作用方式的知識下，可以開發一個資料產生器，用於產生來自物件在感測器場中模擬動作的資料。該架構使用模擬資料來推論感測器網路中物件的行為。該方法在真實世界資料上經過實驗測試，其中磁強計用於機場來偵測和識別飛機動作。實驗證明了整合反向和正向建模的價值，讓智慧系統能夠更好地理解和預測複雜的、感測器驅動的事件。

##### **Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization**
2502.13632v1 by Or Raphael Bidusa, Shaul Markovitch

The opaque nature of Large Language Models (LLMs) has led to significant
research efforts aimed at enhancing their interpretability, primarily through
post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck
Models (CBMs), offer both interpretability and intervenability by incorporating
explicit concept representations. However, these methods suffer from key
limitations, including reliance on labeled concept datasets and significant
architectural modifications that challenges re-integration into existing system
pipelines. In this work, we introduce a new methodology for incorporating
interpretability and intervenability into an existing model by integrating
Concept Layers (CLs) into its architecture. Our approach projects the model's
internal vector representations into a conceptual, explainable vector space
before reconstructing and feeding them back into the model. Furthermore, we
eliminate the need for a human-selected concept set by algorithmically
searching an ontology for a set of concepts that can be either task-specific or
task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they
maintain the original model's performance and agreement while enabling
meaningful interventions. Additionally, we present a proof of concept
showcasing an intervenability interface, allowing users to adjust model
behavior dynamically, such as mitigating biases during inference.

摘要：大型語言模型 (LLM) 的不透明本質已導致大量的研究工作，旨在透過後設方法提升其可解釋性。較新的內建方法，例如概念瓶頸模型 (CBM)，透過納入明確的概念表徵，同時提供可解釋性和可介入性。然而，這些方法有其主要的限制，包括依賴標記概念資料集，以及大幅的架構修改，對重新整合到現有的系統管線中構成挑戰。在這項工作中，我們提出了一種新的方法，透過將概念層 (CL) 整合到架構中，將可解釋性和可介入性納入現有模型。我們的做法是將模型的內部向量表徵投影到一個概念性的、可解釋的向量空間，然後再將其重建並回饋到模型中。此外，我們透過演算法搜尋本体，以尋找一組概念（可以是特定於任務或與任務無關），來消除對人類選擇概念集的需求。我們在多項任務中評估 CL，證明它們在維持原始模型的效能和一致性的同時，還能實現有意義的介入。此外，我們提出了一個概念驗證，展示了一個可介入性介面，讓使用者可以動態調整模型行為，例如在推理過程中減輕偏差。

##### **Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection**
2502.13628v1 by Darpan Aswal, Manjira Sinha

Transformer-based models dominate NLP tasks like sentiment analysis, machine
translation, and claim verification. However, their massive computational
demands and lack of interpretability pose challenges for real-world
applications requiring efficiency and transparency. In this work, we explore
Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as
lightweight yet effective alternatives for Environmental Claim Detection,
reframing it as a graph classification problem. We construct dependency parsing
graphs to explicitly model syntactic structures, using simple word embeddings
(word2vec) for node features with dependency relations encoded as edge
features. Our results demonstrate that these graph-based models achieve
comparable or superior performance to state-of-the-art transformers while using
30x fewer parameters. This efficiency highlights the potential of structured,
interpretable, and computationally efficient graph-based approaches.

摘要：基於 Transformer 的模型主導著 NLP 任務，例如情緒分析、機器翻譯和聲明驗證。然而，它們龐大的計算需求和缺乏可解釋性對需要效率和透明度的實際應用構成了挑戰。在這項工作中，我們探討圖神經網路 (GNN) 和雙曲圖神經網路 (HGNN) 作為環境聲明檢測的輕量級且有效的替代方案，並將其重新定義為圖形分類問題。我們構建依賴解析圖形以明確建構語法結構，使用簡單的詞嵌入（word2vec）作為節點特徵，並將依賴關係編碼為邊緣特徵。我們的結果證明，這些基於圖形的模型在使用少 30 倍的參數時，可實現與最先進的 Transformer 相當或更優異的效能。這種效率突顯了結構化、可解釋且計算效率高的基於圖形方法的潛力。

##### **REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models**
2502.13622v1 by DongGeon Lee, Hwanjo Yu

Hallucinations in large language model (LLM) outputs severely limit their
reliability in knowledge-intensive tasks such as question answering. To address
this challenge, we introduce REFIND (Retrieval-augmented Factuality
hallucINation Detection), a novel framework that detects hallucinated spans
within LLM outputs by directly leveraging retrieved documents. As part of the
REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that
quantifies the sensitivity of LLM outputs to retrieved evidence. This
innovative approach enables REFIND to efficiently and accurately detect
hallucinations, setting it apart from existing methods. In the evaluation,
REFIND demonstrated robustness across nine languages, including low-resource
settings, and significantly outperformed baseline models, achieving superior
IoU scores in identifying hallucinated spans. This work highlights the
effectiveness of quantifying context sensitivity for hallucination detection,
thereby paving the way for more reliable and trustworthy LLM applications
across diverse languages.

摘要：大型語言模型 (LLM) 輸出中的幻覺嚴重限制了它們在知識密集型任務（例如問答）中的可靠性。為了應對這一挑戰，我們引入了 REFIND（檢索增強的事實幻覺檢測），這是一個新穎的框架，它通過直接利用檢索到的文件來檢測 LLM 輸出中的幻覺跨度。作為 REFIND 的一部分，我們提出了語境敏感性比率 (CSR)，這是一個新穎的指標，它量化了 LLM 輸出對檢索證據的敏感性。這種創新方法使 REFIND 能夠高效準確地檢測幻覺，使其有別於現有方法。在評估中，REFIND 在九種語言中展示了其穩健性，包括低資源設置，並且顯著優於基線模型，在識別幻覺跨度方面實現了更高的 IoU 分數。這項工作突出了量化語境敏感性對於幻覺檢測的有效性，從而為跨多種語言的更可靠和值得信賴的 LLM 應用鋪平了道路。

##### **Decentralized Planning Using Probabilistic Hyperproperties**
2502.13621v1 by Francesco Pontiggia, Filip Macák, Roman Andriushchenko, Michele Chiari, Milan Češka

Multi-agent planning under stochastic dynamics is usually formalised using
decentralized (partially observable) Markov decision processes ( MDPs) and
reachability or expected reward specifications. In this paper, we propose a
different approach: we use an MDP describing how a single agent operates in an
environment and probabilistic hyperproperties to capture desired temporal
objectives for a set of decentralized agents operating in the environment. We
extend existing approaches for model checking probabilistic hyperproperties to
handle temporal formulae relating paths of different agents, thus requiring the
self-composition between multiple MDPs. Using several case studies, we
demonstrate that our approach provides a flexible and expressive framework to
broaden the specification capabilities with respect to existing planning
techniques. Additionally, we establish a close connection between a subclass of
probabilistic hyperproperties and planning for a particular type of Dec-MDPs,
for both of which we show undecidability. This lays the ground for the use of
existing decentralized planning tools in the field of probabilistic
hyperproperty verification.

摘要：多智能體在隨機動態下的規劃通常使用分散式（部分可觀察）馬可夫決策過程 (MDP) 和可達性或預期獎勵規格進行形式化。在本文中，我們提出不同的方法：我們使用 MDP 來描述單一智能體在環境中運作的方式，並使用機率超屬性來捕捉在環境中運作的一組分散式智能體所期望的時間目標。我們擴充現有方法來檢查機率超屬性，以處理與不同智能體路徑相關的時間公式，因此需要在多個 MDP 之間進行自我組合。使用幾個案例研究，我們證明我們的做法提供了一個靈活且表達力豐富的架構，以擴展相對於現有規劃技術的規格能力。此外，我們建立了機率超屬性的子類別與規劃特定類型 Dec-MDP 之間的密切關聯，我們對這兩者都顯示了不可判定性。這為在機率超屬性驗證領域中使用現有的分散式規劃工具奠定了基礎。

##### **Complex Ontology Matching with Large Language Model Embeddings**
2502.13619v1 by Guilherme Sousa, Rinaldo Lima, Cassia Trojahn

Ontology, and more broadly, Knowledge Graph Matching is a challenging task in
which expressiveness has not been fully addressed. Despite the increasing use
of embeddings and language models for this task, approaches for generating
expressive correspondences still do not take full advantage of these models, in
particular, large language models (LLMs). This paper proposes to integrate LLMs
into an approach for generating expressive correspondences based on alignment
need and ABox-based relation discovery. The generation of correspondences is
performed by matching similar surroundings of instance sub-graphs. The
integration of LLMs results in different architectural modifications, including
label similarity, sub-graph matching, and entity matching. The performance word
embeddings, sentence embeddings, and LLM-based embeddings, was compared. The
results demonstrate that integrating LLMs surpasses all other models, enhancing
the baseline version of the approach with a 45\% increase in F-measure.

摘要：本体论，更广泛地说，知识图谱匹配是一项具有挑战性的任务，其中表达力尚未得到充分解决。尽管越来越多地使用嵌入和语言模型来完成此任务，但生成表达性对应关系的方法仍然没有充分利用这些模型，特别是大型语言模型 (LLM)。本文提出将 LLM 集成到一种基于对齐需求和基于 ABox 的关系发现来生成表达性对应关系的方法中。对应关系的生成是通过匹配实例子图的相似周围环境来执行的。LLM 的集成导致了不同的架构修改，包括标签相似性、子图匹配和实体匹配。比较了单词嵌入、句子嵌入和基于 LLM 的嵌入的性能。结果表明，集成 LLM 超越了所有其他模型，通过 F-measure 提高了 45% 的基准版本的方法。

##### **LaVCa: LLM-assisted Visual Cortex Captioning**
2502.13606v1 by Takuya Matsuyama, Shinji Nishimoto, Yu Takagi

Understanding the property of neural populations (or voxels) in the human
brain can advance our comprehension of human perceptual and cognitive
processing capabilities and contribute to developing brain-inspired computer
models. Recent encoding models using deep neural networks (DNNs) have
successfully predicted voxel-wise activity. However, interpreting the
properties that explain voxel responses remains challenging because of the
black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex
Captioning (LaVCa), a data-driven approach that uses large language models
(LLMs) to generate natural-language captions for images to which voxels are
selective. By applying LaVCa for image-evoked brain activity, we demonstrate
that LaVCa generates captions that describe voxel selectivity more accurately
than the previously proposed method. Furthermore, the captions generated by
LaVCa quantitatively capture more detailed properties than the existing method
at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed
analysis of the voxel-specific properties generated by LaVCa reveals
fine-grained functional differentiation within regions of interest (ROIs) in
the visual cortex and voxels that simultaneously represent multiple distinct
concepts. These findings offer profound insights into human visual
representations by assigning detailed captions throughout the visual cortex
while highlighting the potential of LLM-based methods in understanding brain
representations. Please check out our webpage at
https://sites.google.com/view/lavca-llm/

摘要：理解人類大腦中神經元群（或體素）的特性，有助於我們進一步了解人類知覺和認知處理能力，並有助於開發受大腦啟發的電腦模型。最近使用深度神經網路（DNN）的編碼模型已成功預測體素活動。然而，由於 DNN 的黑箱性質，解釋說明體素反應的特性仍然具有挑戰性。作為解決方案，我們提出 LLM 輔助視覺皮層字幕（LaVCa），這是一種資料驅動的方法，使用大型語言模型（LLM）為體素選擇的影像產生自然語言字幕。透過將 LaVCa 應用於影像引發的大腦活動，我們證明 LaVCa 產生的字幕比先前提出的方法更準確地描述體素選擇性。此外，LaVCa 產生的字幕在體素間和體素內層級上，比現有方法量化捕捉到更多詳細的特性。此外，對 LaVCa 產生的體素特定特性的更詳細分析揭示了視覺皮層中感興趣區域（ROI）內的細緻功能區分，以及同時表示多個不同概念的體素。這些發現透過在整個視覺皮層中指定詳細的字幕，提供了人類視覺表徵的深刻見解，同時強調了基於 LLM 的方法在理解大腦表徵方面的潛力。請查看我們的網頁：
https://sites.google.com/view/lavca-llm/

##### **BeamLoRA: Beam-Constraint Low-Rank Adaptation**
2502.13604v1 by Naibin Gu, Zhenyu Zhang, Xiyu Liu, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang

Due to the demand for efficient fine-tuning of large language models,
Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective
parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves
efficiency, there remains room for improvement in accuracy. Herein, we adopt a
novel perspective to assess the characteristics of LoRA ranks. The results
reveal that different ranks within the LoRA modules not only exhibit varying
levels of importance but also evolve dynamically throughout the fine-tuning
process, which may limit the performance of LoRA. Based on these findings, we
propose BeamLoRA, which conceptualizes each LoRA module as a beam where each
rank naturally corresponds to a potential sub-solution, and the fine-tuning
process becomes a search for the optimal sub-solution combination. BeamLoRA
dynamically eliminates underperforming sub-solutions while expanding the
parameter space for promising ones, enhancing performance with a fixed rank.
Extensive experiments across three base models and 12 datasets spanning math
reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA
consistently enhances the performance of LoRA, surpassing the other baseline
methods.

摘要：由於對大型語言模型進行高效微調的需求，
低階改編 (LoRA) 已被廣泛採用為最有效的
參數高效微調方法之一。儘管如此，雖然 LoRA 提高了
效率，但準確性仍有改進空間。在此，我們採用一
種新穎的觀點來評估 LoRA 等級的特徵。結果
顯示，LoRA 模組內的不同等級不僅表現出不同的
重要性，而且在微調過程中也會動態演變，這可能會限制 LoRA 的效能。根據這些發現，我們
提出 BeamLoRA，將每個 LoRA 模組概念化為一個波束，其中每個
等級自然對應於一個潛在的子解決方案，而微調
過程變成搜尋最佳子解決方案組合。BeamLoRA
動態消除表現不佳的子解決方案，同時擴展有前途的子解決方案的參數空間，以固定等級提升效能。
跨越三個基礎模型和涵蓋數學
推理、程式碼生成和常識推理的 12 個資料集的廣泛實驗證明，BeamLoRA
始終提升 LoRA 的效能，超越其他基準
方法。

##### **Efficient Safety Retrofitting Against Jailbreaking for LLMs**
2502.13603v1 by Dario Garcia-Gasulla, Anna Arias-Duart, Adrian Tormos, Daniel Hinjos, Oscar Molina-Sedano, Ashwin Kumar Gururajan, Maria Eugenia Cardello

Direct Preference Optimization (DPO) is an efficient alignment technique that
steers LLMs towards preferable outputs by training on preference data,
bypassing the need for explicit reward models. Its simplicity enables easy
adaptation to various domains and safety requirements. This paper examines
DPO's effectiveness in model safety against jailbreaking attacks while
minimizing data requirements and training costs. We introduce Egida, a dataset
expanded from multiple sources, which includes 27 different safety topics and
18 different attack styles, complemented with synthetic and human labels. This
data is used to boost the safety of state-of-the-art LLMs
(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack
styles. In addition to safety evaluations, we assess their post-alignment
performance degradation in general purpose tasks, and their tendency to over
refusal. Following the proposed methodology, trained models reduce their Attack
Success Rate by 10%-30%, using small training efforts (2,000 samples) with low
computational cost (3\$ for 8B models, 20\$ for 72B models). Safety aligned
models generalize to unseen topics and attack styles, with the most successful
attack style reaching a success rate around 5%. Size and family are found to
strongly influence model malleability towards safety, pointing at the
importance of pre-training choices. To validate our findings, a large
independent assessment of human preference agreement with Llama-Guard-3-8B is
conducted by the authors and the associated dataset Egida-HSafe is released.
Overall, this study illustrates how affordable and accessible it is to enhance
LLM safety using DPO while outlining its current limitations. All datasets and
models are released to enable reproducibility and further research.

摘要：直接偏好最佳化 (DPO) 是一種有效對齊技術，透過偏好資料訓練，將 LLM 引導至較佳輸出，無須明確回饋模型。其簡潔性讓它易於適應各種領域和安全需求。本文探討 DPO 在模型安全方面對抗越獄攻擊的有效性，同時將資料需求和訓練成本降至最低。我們引進 Egida，一個從多個來源擴展而來的資料集，其中包含 27 個不同的安全主題和 18 種不同的攻擊樣式，並附有合成和人工標籤。這些資料用於提升最先進 LLM（Llama-3.1-8B/70B-Instruct、Qwen-2.5-7B/72B-Instruct）在各種主題和攻擊樣式中的安全性。除了安全評估外，我們評估它們在一般任務中對齊後效能的降低，以及它們過度拒絕的傾向。遵循建議的方法，訓練後的模型將它們的攻擊成功率降低了 10%-30%，且訓練工作量小（2,000 個範例），運算成本低（8B 模型為 3 美元，72B 模型為 20 美元）。安全對齊模型概括至未見主題和攻擊樣式，其中最成功的攻擊樣式成功率約為 5%。發現模型大小和系列會強烈影響模型對安全性的可塑性，這點出預訓練選擇的重要性。為了驗證我們的發現，作者進行了 Llama-Guard-3-8B 人類偏好一致性的廣泛獨立評估，並發布了相關資料集 Egida-HSafe。整體而言，本研究說明了使用 DPO 增強 LLM 安全性的經濟性和可及性，同時概述了其當前的限制。所有資料集和模型皆已發布，以利於重現性和進一步研究。

##### **MMTEB: Massive Multilingual Text Embedding Benchmark**
2502.13595v1 by Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff

Text embeddings are typically evaluated on a limited set of tasks, which are
constrained by language, domain, and task diversity. To address these
limitations and provide a more comprehensive evaluation, we introduce the
Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,
community-driven expansion of MTEB, covering over 500 quality-controlled
evaluation tasks across 250+ languages. MMTEB includes a diverse set of
challenging, novel tasks such as instruction following, long-document
retrieval, and code retrieval, representing the largest multilingual collection
of evaluation tasks for embedding models to date. Using this collection, we
develop several highly multilingual benchmarks, which we use to evaluate a
representative set of models. We find that while large language models (LLMs)
with billions of parameters can achieve state-of-the-art performance on certain
language subsets and task categories, the best-performing publicly available
model is multilingual-e5-large-instruct with only 560 million parameters. To
facilitate accessibility and reduce computational cost, we introduce a novel
downsampling method based on inter-task correlation, ensuring a diverse
selection while preserving relative model rankings. Furthermore, we optimize
tasks such as retrieval by sampling hard negatives, creating smaller but
effective splits. These optimizations allow us to introduce benchmarks that
drastically reduce computational demands. For instance, our newly introduced
zero-shot English benchmark maintains a ranking order similar to the full-scale
version but at a fraction of the computational cost.

摘要：文本嵌入通常在受语言、领域和任务多样性限制的一组有限的任务上进行评估。为了解决这些限制并提供更全面的评估，我们引入了大规模多语言文本嵌入基准（MMTEB）——MTEB 的一项大规模、社区驱动的扩展，涵盖了 250 多种语言中的 500 多项质量受控评估任务。MMTEB 包含了一组具有挑战性的新任务，例如指令遵循、长文档检索和代码检索，代表了迄今为止最大的多语言嵌入模型评估任务集合。使用此集合，我们开发了几个高度多语言的基准，我们使用这些基准来评估一组有代表性的模型。我们发现，虽然具有数十亿个参数的大语言模型 (LLM) 可以对某些语言子集和任务类别实现最先进的性能，但性能最佳的公开可用模型是只有 5.6 亿个参数的多语言 e5 大型指令模型。为了便于访问并降低计算成本，我们引入了一种基于任务间相关性的新降采样方法，确保多样化的选择同时保留相对模型排名。此外，我们通过对困难的负样本进行采样来优化检索等任务，创建更小但有效的拆分。这些优化使我们能够引入大幅降低计算需求的基准。例如，我们新引入的零样本英语基准保持了与全规模版本相似的排名顺序，但计算成本只是其一小部分。

##### **Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints**
2502.13592v1 by Nicolò Penzo, Marco Guerini, Bruno Lepri, Goran Glavaš, Sara Tonelli

Multi-Party Conversations (MPCs) are widely studied across disciplines, with
social media as a primary data source due to their accessibility. However,
these datasets raise privacy concerns and often reflect platform-specific
properties. For example, interactions between speakers may be limited due to
rigid platform structures (e.g., threads, tree-like discussions), which yield
overly simplistic interaction patterns (e.g., as a consequence of ``reply-to''
links). This work explores the feasibility of generating diverse MPCs with
instruction-tuned Large Language Models (LLMs) by providing deterministic
constraints such as dialogue structure and participants' stance. We investigate
two complementary strategies of leveraging LLMs in this context: (i.) LLMs as
MPC generators, where we task the LLM to generate a whole MPC at once and (ii.)
LLMs as MPC parties, where the LLM generates one turn of the conversation at a
time, provided the conversation history. We next introduce an analytical
framework to evaluate compliance with the constraints, content quality, and
interaction complexity for both strategies. Finally, we assess the quality of
obtained MPCs via human annotation and LLM-as-a-judge evaluations. We find
stark differences among LLMs, with only some being able to generate
high-quality MPCs. We also find that turn-by-turn generation yields better
conformance to constraints and higher linguistic variability than generating
MPCs in one pass. Nonetheless, our structural and qualitative evaluation
indicates that both generation strategies can yield high-quality MPCs.

摘要：多方對話 (MPC) 已在各學科廣泛研究，其中社群媒體因其易於取得而成為主要的資料來源。然而，這些資料集引發了隱私問題，且通常反映出特定平台的特性。例如，由於僵化的平台結構（例如，串聯、樹狀討論），發言者之間的互動可能受到限制，這會產生過於簡化的互動模式（例如，由於「回覆」連結的結果）。本研究探討了透過提供確定性的約束條件（例如，對話結構和參與者的立場）來產生多樣化 MPC 的可行性，並調整大型語言模型 (LLM) 的指示。我們研究了在這種情況下利用 LLM 的兩種互補策略：(i.) LLM 作為 MPC 產生器，我們要求 LLM 一次產生整個 MPC，以及 (ii.) LLM 作為 MPC 方，其中 LLM 在提供對話記錄的情況下產生對話的一個回合。接下來，我們引入一個分析架構來評估兩種策略的約束條件、內容品質和互動複雜性。最後，我們透過人工註解和 LLM 作為評審的評估來評估獲得的 MPC 的品質。我們發現 LLM 之間存在顯著差異，只有部分 LLM 能夠產生高品質的 MPC。我們還發現，逐回合產生比一次產生 MPC 更符合約束條件，且語言變異性更高。儘管如此，我們的結構性和品質評估表明，兩種產生策略都可以產生高品質的 MPC。

##### **Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation**
2502.13576v1 by Peiwen Yuan, Yueqi Zhang, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li

Evaluating models on large benchmarks is very resource-intensive, especially
during the period of rapid model evolution. Existing efficient evaluation
methods estimate the performance of target models by testing them only on a
small and static coreset of the benchmark, which is derived from the publicly
available evaluation results of source models. These methods rely on the
assumption that target models have high prediction consistency with source
models. However, we demonstrate that it doesn't generalize well in practice. To
alleviate the inconsistency issue, we present TailoredBench, a method that
conducts customized evaluation tailored to each target model. Specifically, a
Global-coreset is first constructed as a probe to identify the most consistent
source models for each target model with an adaptive source model selection
strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to
extend the Global-coreset to a tailored Native-coreset for each target model.
According to the predictions on Native-coresets, we obtain the performance of
target models on the whole benchmark with a calibrated estimation strategy.
Comprehensive experiments on 5 benchmarks across over 300 models demonstrate
that compared to best performing baselines, TailoredBench achieves an average
reduction of 31.4% in MAE of accuracy estimates under the same inference
budgets, showcasing strong effectiveness and generalizability.

摘要：在大型基准上評估模型非常耗費資源，特別是在模型快速演化的時期。現有的高效評估方法透過僅在基準的小型且靜態核心集上對目標模型進行測試來估計其效能，核心集取自公開的來源模型評估結果。這些方法依賴於目標模型與來源模型具有高度預測一致性的假設。然而，我們證明這在實務中無法很好地概化。為了減輕不一致性問題，我們提出 TailoredBench，這是一種針對每個目標模型進行客製化評估的方法。具體來說，首先建構一個 Global-coreset 作為探針，以識別每個目標模型最一致的來源模型，並採用自適應來源模型選擇策略。之後，提出一個可擴充的 K-Medoids 聚類演算法，將 Global-coreset 延伸為每個目標模型的客製化 Native-coreset。根據 Native-coreset 的預測，我們以校準的估計策略取得目標模型在整個基準上的效能。針對超過 300 個模型的 5 個基準進行的全面實驗證明，與效能最佳的基準線相比，TailoredBench 在相同的推論預算下，準確度估計的 MAE 平均減少 31.4%，展現出強大的有效性和概化能力。

##### **Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning**
2502.13569v1 by Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li

Multi-task reinforcement learning employs a single policy to complete various
tasks, aiming to develop an agent with generalizability across different
scenarios. Given the shared characteristics of tasks, the agent's learning
efficiency can be enhanced through parameter sharing. Existing approaches
typically use a routing network to generate specific routes for each task and
reconstruct a set of modules into diverse models to complete multiple tasks
simultaneously. However, due to the inherent difference between tasks, it is
crucial to allocate resources based on task difficulty, which is constrained by
the model's structure. To this end, we propose a Model Evolution framework with
Genetic Algorithm (MEGA), which enables the model to evolve during training
according to the difficulty of the tasks. When the current model is
insufficient for certain tasks, the framework will automatically incorporate
additional modules, enhancing the model's capabilities. Moreover, to adapt to
our model evolution framework, we introduce a genotype module-level model,
using binary sequences as genotype policies for model reconstruction, while
leveraging a non-gradient genetic algorithm to optimize these genotype
policies. Unlike routing networks with fixed output dimensions, our approach
allows for the dynamic adjustment of the genotype policy length, enabling it to
accommodate models with a varying number of modules. We conducted experiments
on various robotics manipulation tasks in the Meta-World benchmark. Our
state-of-the-art performance demonstrated the effectiveness of the MEGA
framework. We will release our source code to the public.

摘要：多任務強化學習採用單一策略來完成各種任務，旨在培養出在不同場景中具有泛化能力的代理。鑑於任務的共同特徵，代理的學習效率可透過參數共享來提升。現有的方法通常使用路由網路為每個任務生成特定路由，並將一組模組重建成不同的模型，以同時完成多項任務。然而，由於任務之間的固有差異，根據任務難度分配資源至關重要，而這受到模型結構的限制。為此，我們提出了一個具有遺傳演算法 (MEGA) 的模型演化架構，它使模型能夠根據任務的難度在訓練期間演化。當目前的模型不足以應付某些任務時，該架構將自動納入額外的模組，增強模型的能力。此外，為了適應我們的模型演化架構，我們引入了一個基因型模組級別模型，使用二進位序列作為基因型策略來進行模型重建，同時利用非梯度遺傳演算法來最佳化這些基因型策略。與具有固定輸出維度的路由網路不同，我們的做法允許動態調整基因型策略長度，使其能夠容納具有不同數量模組的模型。我們在 Meta-World 基準中的各種機器人操作任務上進行了實驗。我們的最新技術證明了 MEGA 架構的有效性。我們將向公眾發布我們的原始碼。

##### **LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation**
2502.13568v1 by Xin Li, Anand Sarwate

Imposing an effective structural assumption on neural network weight matrices
has been the major paradigm for designing Parameter-Efficient Fine-Tuning
(PEFT) systems for adapting modern large pre-trained models to various
downstream tasks. However, low rank based adaptation has become increasingly
challenging due to the sheer scale of modern large language models. In this
paper, we propose an effective kernelization to further reduce the number of
parameters required for adaptation tasks. Specifically, from the classical idea
in numerical analysis regarding matrix Low-Separation-Rank (LSR)
representations, we develop a kernel using this representation for the low rank
adapter matrices of the linear layers from large networks, named the Low
Separation Rank Adaptation (LSR-Adapt) kernel. With the ultra-efficient kernel
representation of the low rank adapter matrices, we manage to achieve
state-of-the-art performance with even higher accuracy with almost half the
number of parameters as compared to conventional low rank based methods. This
structural assumption also opens the door to further GPU-side optimizations due
to the highly parallelizable nature of Kronecker computations.

摘要：在神经网络权重矩阵上施加有效的结构假设一直是设计参数高效微调 (PEFT) 系统的主要范例，用于将现代大型预训练模型适应到各种下游任务。然而，由于现代大型语言模型的庞大规模，基于低秩的适应变得越来越具有挑战性。在本文中，我们提出了一种有效的核化方法，以进一步减少适应任务所需的​​参数数量。具体来说，从数值分析中关于矩阵低分离秩 (LSR) 表示的经典思想出发，我们使用这种表示为大型网络的线性层的低秩适配器矩阵开发了一个核，称为低分离秩适应 (LSR-Adapt) 核。通过低秩适配器矩阵的超高效核表示，我们设法以与基于传统低秩方法相比几乎一半的参数数量实现了具有更高精度的最先进性能。由于 Kronecker 计算的高度并行化特性，这种结构假设也为进一步的 GPU 端优化打开了大门。

##### **Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs**
2502.13566v1 by Joonatan Laato, Jenna Kanerva, John Loehr, Virpi Lummaa, Filip Ginter

We performed a zero-shot information extraction study on a historical
collection of 89,339 brief Finnish-language interviews of refugee families
relocated post-WWII from Finnish Eastern Karelia. Our research objective is
two-fold. First, we aim to extract social organizations and hobbies from the
free text of the interviews, separately for each family member. These can act
as a proxy variable indicating the degree of social integration of refugees in
their new environment. Second, we aim to evaluate several alternative ways to
approach this task, comparing a number of generative models and a supervised
learning approach, to gain a broader insight into the relative merits of these
different approaches and their applicability in similar studies.
  We find that the best generative model (GPT-4) is roughly on par with human
performance, at an F-score of 88.8%. Interestingly, the best open generative
model (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7%
F-score, demonstrating that open models are becoming a viable alternative for
some practical tasks even on non-English data. Additionally, we test a
supervised learning alternative, where we fine-tune a Finnish BERT model
(FinBERT) using GPT-4 generated training data. By this method, we achieved an
F-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k
interviews. Such an approach would be particularly appealing in cases where the
computational resources are limited, or there is a substantial mass of data to
process.

摘要：<paragraph>我們對 89,339 份芬蘭語簡短訪談的歷史資料進行了零次抽樣資訊萃取研究，這些訪談來自二戰後從芬蘭東卡累利亞遷移的難民家庭。我們的研究目標有兩個面向。首先，我們旨在從訪談的自由文本中萃取出每個家庭成員的社會組織和嗜好。這些資料可以用作代理變數，表示難民在其新環境中的社會整合程度。其次，我們旨在評估處理此任務的幾種替代方法，比較多種生成模型和監督式學習方法，以更廣泛地了解這些不同方法的相對優點及其在類似研究中的適用性。
我們發現最佳的生成模型 (GPT-4) 與人類表現大致相當，F 分數為 88.8%。有趣的是，最佳的開放生成模型 (Llama-3-70B-Instruct) 達到了幾乎相同的表現，F 分數為 87.7%，這證明了開放模型正成為非英語資料中一些實際任務的可行替代方案。此外，我們測試了一種監督式學習替代方案，其中我們使用 GPT-4 生成的訓練資料微調了芬蘭 BERT 模型 (FinBERT)。透過這種方法，我們僅使用 6K 份訪談就達到了 84.1% 的 F 分數，使用 30k 份訪談則達到了 86.3% 的 F 分數。這種方法在計算資源有限或有大量資料需要處理的情況下特別有吸引力。</paragraph>

##### **PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models**
2502.13564v1 by Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei

The rapid development of large language models (LLMs) is redefining the
landscape of human-computer interaction, and their integration into various
user-service applications is becoming increasingly prevalent. However,
transmitting user data to cloud-based LLMs presents significant risks of data
breaches and unauthorized access to personal identification information. In
this paper, we propose a privacy preservation pipeline for protecting privacy
and sensitive information during interactions between users and LLMs in
practical LLM usage scenarios. We construct SensitiveQA, the first privacy
open-ended question-answering dataset. It comprises 57k interactions in Chinese
and English, encompassing a diverse range of user-sensitive information within
the conversations. Our proposed solution employs a multi-stage strategy aimed
at preemptively securing user information while simultaneously preserving the
response quality of cloud-based LLMs. Experimental validation underscores our
method's efficacy in balancing privacy protection with maintaining robust
interaction quality. The code and dataset are available at
https://github.com/ligw1998/PRIV-QA.

摘要：大型語言模型 (LLM) 的快速發展重新定義了人機互動的格局，它們與各種使用者服務應用程式整合的現象也越來越普遍。然而，將使用者資料傳輸到雲端 LLM 會造成資料外洩和個人識別資訊遭到未經授權存取的重大風險。在本文中，我們提出一個隱私保護管線，用於在實際 LLM 使用情境中保護使用者和 LLM 互動時的隱私和敏感資訊。我們建構了 SensitiveQA，這是第一個隱私開放式問答資料集。它包含 57k 個中文和英文互動，涵蓋了對話中各種使用者的敏感資訊。我們提出的解決方案採用多階段策略，旨在預先保護使用者資訊，同時保留雲端 LLM 的回應品質。實驗驗證強調了我們的方法在平衡隱私保護和維持穩健互動品質方面的效能。程式碼和資料集可在 https://github.com/ligw1998/PRIV-QA 取得。

##### **Are Large Language Models In-Context Graph Learners?**
2502.13562v1 by Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Liang Chen, Zibin Zheng

Large language models (LLMs) have demonstrated remarkable in-context
reasoning capabilities across a wide range of tasks, particularly with
unstructured inputs such as language or images. However, LLMs struggle to
handle structured data, such as graphs, due to their lack of understanding of
non-Euclidean structures. As a result, without additional fine-tuning, their
performance significantly lags behind that of graph neural networks (GNNs) in
graph learning tasks. In this paper, we show that learning on graph data can be
conceptualized as a retrieval-augmented generation (RAG) process, where
specific instances (e.g., nodes or edges) act as queries, and the graph itself
serves as the retrieved context. Building on this insight, we propose a series
of RAG frameworks to enhance the in-context learning capabilities of LLMs for
graph learning tasks. Comprehensive evaluations demonstrate that our proposed
RAG frameworks significantly improve LLM performance on graph-based tasks,
particularly in scenarios where a pretrained LLM must be used without
modification or accessed via an API.

摘要：大型語言模型 (LLM) 在廣泛的任務中展示了非凡的語境推理能力，特別是對於語言或影像等非結構化輸入。然而，LLM 難以處理結構化資料，例如圖形，因為它們無法理解非歐幾何結構。因此，在沒有額外微調的情況下，它們在圖形學習任務中的表現遠遠落後於圖形神經網路 (GNN)。在本文中，我們展示了在圖形資料上學習可以被概念化為檢索增強生成 (RAG) 過程，其中特定實例（例如，節點或邊）充當查詢，而圖形本身則作為檢索的語境。基於這個見解，我們提出了一系列 RAG 架構，以增強 LLM 在圖形學習任務中的語境學習能力。全面的評估表明，我們提出的 RAG 架構顯著提升了 LLM 在基於圖形的任務上的表現，特別是在預訓練的 LLM 必須在不修改或透過 API 存取的情況下使用的場景中。

##### **Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs**
2502.13555v1 by Yushi Feng, Tsai Hor Chan, Guosheng Yin, Lequan Yu

Data augmentation is necessary for graph representation learning due to the
scarcity and noise present in graph data. Most of the existing augmentation
methods overlook the context information inherited from the dataset as they
rely solely on the graph structure for augmentation. Despite the success of
some large language model-based (LLM) graph learning methods, they are mostly
white-box which require access to the weights or latent features from the
open-access LLMs, making them difficult to be democratized for everyone as
existing LLMs are mostly closed-source for commercial considerations. To
overcome these limitations, we propose a black-box context-driven graph data
augmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the
text prompt as context-related information, we task the LLM with generating
knowledge graphs (KGs), which allow us to capture the structural interactions
from the text outputs. We then design a dynamic merging schema to
stochastically integrate the LLM-generated KGs into the original graph during
training. To control the sparsity of the augmented graph, we further devise a
granularity-aware prompting strategy and an instruction fine-tuning module,
which seamlessly generates text prompts according to different granularity
levels of the dataset. Extensive experiments on various graph learning tasks
validate the effectiveness of our method over existing graph data augmentation
methods. Notably, our approach excels in scenarios involving electronic health
records (EHRs), which validates its maximal utilization of contextual
knowledge, leading to enhanced predictive performance and interpretability.

摘要：由於圖表資料的稀少性和雜訊，資料擴充對於圖表表示學習來說是必要的。現有的擴充方法大多忽略了從資料集中繼承的背景資訊，因為它們僅依賴於圖表的結構進行擴充。儘管一些大型語言模型 (LLM) 基於圖表學習方法獲得成功，但它們大多是白盒，需要存取開放式 LLM 的權重或潛在特徵，由於現有的 LLM 主要基於商業考量而封閉原始碼，因此難以讓所有人都能使用。為了克服這些限制，我們提出了一個黑盒背景驅動圖表資料擴充方法，在 LLM 的指導下——DemoGraph。利用文字提示作為與背景相關的資訊，我們讓 LLM 產生知識圖譜 (KG)，這讓我們能夠從文字輸出中擷取結構化互動。然後，我們設計了一個動態合併模式，在訓練期間將 LLM 產生的 KG 隨機整合到原始圖表中。為了控制擴充圖表的稀疏性，我們進一步設計了一個粒度感知提示策略和一個指令微調模組，它可以根據資料集的不同粒度層級無縫產生文字提示。在各種圖表學習任務上的大量實驗驗證了我們的方法比現有的圖表資料擴充方法更有效。值得注意的是，我們的做法在涉及電子健康記錄 (EHR) 的場景中表現出色，這驗證了它對上下文知識的最大利用，從而提高了預測效能和可解釋性。

##### **STaR-SQL: Self-Taught Reasoner for Text-to-SQL**
2502.13550v1 by Mingqian He, Yongliang Shen, Wenqi Zhang, Qiuying Peng, Jun Wang, Weiming Lu

Generating step-by-step "chain-of-thought" rationales has proven effective
for improving the performance of large language models on complex reasoning
tasks. However, applying such techniques to structured tasks, such as
text-to-SQL, remains largely unexplored. In this paper, we introduce
Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes
SQL query generation as a reasoning-driven process. Our method prompts the LLM
to produce detailed reasoning steps for SQL queries and fine-tunes it on
rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL
dedicates additional test-time computation to reasoning, thereby positioning
LLMs as spontaneous reasoners rather than mere prompt-based agents. To further
scale the inference process, we incorporate an outcome-supervised reward model
(ORM) as a verifier, which enhances SQL query accuracy. Experimental results on
the challenging Spider benchmark demonstrate that STaR-SQL significantly
improves text-to-SQL performance, achieving an execution accuracy of 86.6%.
This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to
predict answers directly by 18.0%. Additionally, STaR-SQL outperforms
agent-like prompting methods that leverage more powerful yet closed-source
models such as GPT-4. These findings underscore the potential of
reasoning-augmented training for structured tasks and open the door to
extending self-improving reasoning models to text-to-SQL generation and beyond.

摘要：<paragraph>生成逐步的「思維鏈」論述已被證實能有效改善大型語言模型在複雜推理任務中的表現。然而，將此類技術應用於結構化任務（例如文字轉 SQL）仍鮮少被探討。在本文中，我們介紹了文字轉 SQL 的自學推理器 (STaR-SQL)，這是一種新穎的方法，將 SQL 查詢生成重新定義為一個由推理驅動的過程。我們的模型促使 LLM 為 SQL 查詢產生詳細的推理步驟，並針對能產生正確結果的論述進行微調。與傳統方法不同，STaR-SQL 在測試時額外投入運算資源進行推理，因此將 LLM 定位為自發的推理者，而非單純基於提示的代理人。為了進一步擴大推論過程，我們將結果監督回饋模型 (ORM) 納入其中作為驗證器，以提升 SQL 查詢的準確度。在具有挑戰性的 Spider 基準測試中的實驗結果證明，STaR-SQL 大幅提升了文字轉 SQL 的表現，執行準確度達到 86.6%。這比少次學習的基準高出 31.6%，比直接預測答案的微調基準高出 18.0%。此外，STaR-SQL 也優於利用功能更強大但封閉原始碼模型（例如 GPT-4）的代理提示方法。這些發現突顯了推理增強訓練在結構化任務中的潛力，並為將自我提升推理模型擴展到文字轉 SQL 生成及其他領域開啟了大門。</paragraph>

##### **Detecting Linguistic Bias in Government Documents Using Large language Models**
2502.13548v1 by Milena de Swart, Floris den Hengst, Jieying Chen

This paper addresses the critical need for detecting bias in government
documents, an underexplored area with significant implications for governance.
Existing methodologies often overlook the unique context and far-reaching
impacts of governmental documents, potentially obscuring embedded biases that
shape public policy and citizen-government interactions. To bridge this gap, we
introduce the Dutch Government Data for Bias Detection (DGDB), a dataset
sourced from the Dutch House of Representatives and annotated for bias by
experts. We fine-tune several BERT-based models on this dataset and compare
their performance with that of generative language models. Additionally, we
conduct a comprehensive error analysis that includes explanations of the
models' predictions. Our findings demonstrate that fine-tuned models achieve
strong performance and significantly outperform generative language models,
indicating the effectiveness of DGDB for bias detection. This work underscores
the importance of labeled datasets for bias detection in various languages and
contributes to more equitable governance practices.

摘要：本文探討了偵測政府文件中的偏見之重要性，這是個影響治理甚鉅但尚未充分探討的領域。現有方法論常忽略政府文件獨特的脈絡和深遠影響，可能模糊了形塑公共政策和公民與政府互動的內含偏見。為彌補此差距，我們引入了荷蘭國會眾議院提供資料來源，並由專家註解偏見的荷蘭政府偏見偵測資料 (DGDB)。我們針對此資料集微調了多個基於 BERT 的模型，並將其效能與生成語言模型的效能進行比較。此外，我們進行了全面的錯誤分析，其中包含模型預測的說明。我們的研究結果顯示，微調模型達到了強大的效能，且顯著優於生成語言模型，這表示 DGDB 對於偏見偵測是有效的。這項研究強調了標籤資料集對於各種語言的偏見偵測之重要性，並有助於更公平的治理實務。

##### **From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN**
2502.13544v1 by Peiwen Yuan, Chuyi Tan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Boyuan Pan, Yao Hu, Kan Li

Despite the rapid progress of large language models (LLMs), their
length-controllable text generation (LCTG) ability remains below expectations,
posing a major limitation for practical applications. Existing methods mainly
focus on end-to-end training to reinforce adherence to length constraints.
However, the lack of decomposition and targeted enhancement of LCTG
sub-abilities restricts further progress.To bridge this gap, we conduct a
bottom-up decomposition of LCTG sub-abilities with human patterns as reference
and perform a detailed error analysis.On this basis, we propose MarkerGen, a
simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental
deficiencies via external tool integration;(2) conducts explicit length
modeling with dynamically inserted markers;(3) employs a three-stage generation
scheme to better align length constraints while maintaining content
quality.Comprehensive experiments demonstrate that MarkerGen significantly
improves LCTG across various settings, exhibiting outstanding effectiveness and
generalizability.

摘要：儘管大型語言模型 (LLM) 快速進展，它們的長度控制文字生成 (LCTG) 能力仍低於預期，對實際應用造成重大限制。現有方法主要專注於端到端訓練，以加強對長度限制的遵守。然而，缺乏分解和有針對性的增強 LCTG 次能力，限制了進一步的進展。為了彌補這個差距，我們以人類模式為參考，對 LCTG 次能力進行自下而上的分解，並執行詳細的錯誤分析。在此基礎上，我們提出 MarkerGen，這是一種簡單但有效的即插即用方法：(1) 透過外部工具整合，減輕 LLM 基本缺陷；(2) 以動態插入標記進行明確的長度建模；(3) 採用三階段生成方案，在維持內容品質的同時，更好地調整長度限制。綜合實驗表明，MarkerGen 在各種設定中顯著改善 LCTG，展現出傑出的有效性和概括性。

##### **Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference**
2502.13542v1 by Qingfa Xiao, Jiachuan Wang, Haoyang Li, Cheng Deng, Jiaqi Tang, Shuangyin Li, Yongqi Zhang, Jun Wang, Lei Chen

Recent advances in large language models (LLMs) have showcased exceptional
performance in long-context tasks, while facing significant inference
efficiency challenges with limited GPU memory. Existing solutions first
proposed the sliding-window approach to accumulate a set of historical
\textbf{key-value} (KV) pairs for reuse, then further improvements selectively
retain its subsets at each step. However, due to the sparse attention
distribution across a long context, it is hard to identify and recall relevant
KV pairs, as the attention is distracted by massive candidate pairs.
Additionally, we found it promising to select representative tokens as
probe-Query in each sliding window to effectively represent the entire context,
which is an approach overlooked by existing methods. Thus, we propose
\textbf{ActQKV}, a training-free, \textbf{Act}ivation-aware approach that
dynamically determines probe-\textbf{Q}uery and leverages it to retrieve the
relevant \textbf{KV} pairs for inference. Specifically, ActQKV monitors a
token-level indicator, Activation Bias, within each context window, enabling
the proper construction of probe-Query for retrieval at pre-filling stage. To
accurately recall the relevant KV pairs and minimize the irrelevant ones, we
design a dynamic KV cut-off mechanism guided by information density across
layers at the decoding stage. Experiments on the Long-Bench and $\infty$
Benchmarks demonstrate its state-of-the-art performance with competitive
inference quality and resource efficiency.

摘要：<paragraph>大型語言模型 (LLM) 最近的進展在長語境任務中展現出非凡的效能，同時也面臨有限 GPU 記憶體所造成的重大推論效率挑戰。現有的解決方案首先提出滑動視窗方法來累積一組歷史性的「鍵值」(KV) 對以供重複使用，然後進一步的改進在每個步驟中選擇性地保留其子集。然而，由於在長語境中稀疏的注意力分佈，很難辨識和召回相關的 KV 對，因為注意力被大量的候選對分散了。此外，我們發現有望在每個滑動視窗中選擇具代表性的符號作為探測查詢，以有效地表示整個語境，這是一種現有方法所忽略的方法。因此，我們提出 ActQKV，一種無需訓練的、基於活化的方法，它動態地決定探測查詢並利用它來擷取相關的 KV 對以進行推論。具體來說，ActQKV 會在每個語境視窗中監控符號層級的指標，也就是活化偏差，以便在預先填入階段適當地建構探測查詢以進行擷取。為了準確地召回相關的 KV 對並將不相關的 KV 對減到最少，我們設計了一個動態的 KV 截斷機制，在解碼階段由跨層級資訊密度引導。在 Long-Bench 和 $\infty$ Benchmarks 上的實驗證明了其最先進的效能，具有競爭力的推論品質和資源效率。</paragraph>

##### **Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models**
2502.13533v1 by Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou

Large Language Models (LLMs) have significantly advanced natural language
processing with exceptional task generalization capabilities. Low-Rank Adaption
(LoRA) offers a cost-effective fine-tuning solution, freezing the original
model parameters and training only lightweight, low-rank adapter matrices.
However, the memory footprint of LoRA is largely dominated by the original
model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA
training scheme founded on the intuition that many neurons in
over-parameterized LLMs have low training utility but are essential for
inference. LoRAM presents a unique twist: it trains on a pruned (small) model
to obtain pruned low-rank matrices, which are then recovered and utilized with
the original (large) model for inference. Additionally, minimal-cost continual
pre-training, performed by the model publishers in advance, aligns the
knowledge discrepancy between pruned and original models. Our extensive
experiments demonstrate the efficacy of LoRAM across various pruning strategies
and downstream tasks. For a model with 70 billion parameters, LoRAM enables
training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA
training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by
structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B
(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory
usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while
achieving dominant performance gains over both the original LLaMA-3.1-70B
(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).

摘要：大型語言模型 (LLM) 透過卓越的任務泛化能力大幅提升自然語言處理。低階適應 (LoRA) 提供一種經濟實惠的微調解決方案，凍結原始模型參數，只訓練輕量級的低階適配器矩陣。然而，LoRA 的記憶體佔用量在很大程度上取決於原始模型參數。為了減輕這種情況，我們提出了 LoRAM，這是一種以直覺為基礎的省記憶體 LoRA 訓練方案，直覺認為過度參數化的 LLM 中許多神經元在訓練中效用低，但在推論中卻是必要的。LoRAM 呈現出一個獨特的轉折：它在一個修剪過的（小）模型上進行訓練，以取得修剪過的低階矩陣，然後將這些矩陣復原並與原始的（大）模型一起用於推論。此外，由模型發布者預先執行的最小成本持續預訓練，調整了修剪模型和原始模型之間的知識差異。我們廣泛的實驗證明了 LoRAM 在各種修剪策略和下游任務中的功效。對於一個擁有 700 億個參數的模型，LoRAM 能夠在只有 20G HBM 的 GPU 上進行訓練，取代了 LoRA 訓練的 A100-80G GPU 和微調的 15 個 GPU。具體來說，由結構化修剪與 4 位量化結合執行的 QLoRAM，針對 LLaMA-3.1-70B（LLaMA-2-70B），將在低階矩陣訓練中佔據記憶體使用量主導地位的參數儲存成本降低了 15.81 倍（16.95 倍），同時在原始 LLaMA-3.1-70B（LLaMA-2-70B）和 LoRA 訓練的 LLaMA-3.1-8B（LLaMA-2-13B）上均取得了顯著的效能提升。

##### **Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking**
2502.13527v1 by Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, Lei Zou

The rise of Large Language Models (LLMs) has led to significant applications
but also introduced serious security threats, particularly from jailbreak
attacks that manipulate output generation. These attacks utilize prompt
engineering and logit manipulation to steer models toward harmful content,
prompting LLM providers to implement filtering and safety alignment strategies.
We investigate LLMs' safety mechanisms and their recent applications, revealing
a new threat model targeting structured output interfaces, which enable
attackers to manipulate the inner logit during LLM generation, requiring only
API access permissions. To demonstrate this threat model, we introduce a
black-box attack framework called AttackPrefixTree (APT). APT exploits
structured output interfaces to dynamically construct attack patterns. By
leveraging prefixes of models' safety refusal response and latent harmful
outputs, APT effectively bypasses safety measures. Experiments on benchmark
datasets indicate that this approach achieves higher attack success rate than
existing methods. This work highlights the urgent need for LLM providers to
enhance security protocols to address vulnerabilities arising from the
interaction between safety patterns and structured outputs.

摘要：大型語言模型 (LLM) 的興起帶來了重要的應用，但也引入了嚴重的安全威脅，特別是來自操縱輸出生成的越獄攻擊。這些攻擊利用提示工程和 logit 操縱將模型導向有害內容，促使 LLM 提供者實施過濾和安全對齊策略。我們調查了 LLM 的安全機制及其最近的應用，揭示了一個針對結構化輸出介面的新威脅模型，它使攻擊者能夠在 LLM 生成期間操縱內部 logit，只需要 API 訪問權限。為了展示這個威脅模型，我們引入了一個名為 AttackPrefixTree (APT) 的黑盒攻擊框架。APT 利用結構化輸出介面動態構建攻擊模式。通過利用模型安全拒絕回應和潛在有害輸出的前綴，APT 有效地繞過了安全措施。在基準資料集上的實驗表明，與現有方法相比，這種方法實現了更高的攻擊成功率。這項工作強調了 LLM 提供者迫切需要增強安全協議，以解決安全模式和結構化輸出之間交互產生的漏洞。

##### **MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis**
2502.13524v1 by Wei Dai, Steven Wang, Jun Liu

Efficient evaluation of three-dimensional (3D) medical images is crucial for
diagnostic and therapeutic practices in healthcare. Recent years have seen a
substantial uptake in applying deep learning and computer vision to analyse and
interpret medical images. Traditional approaches, such as convolutional neural
networks (CNNs) and vision transformers (ViTs), face significant computational
challenges, prompting the need for architectural advancements. Recent efforts
have led to the introduction of novel architectures like the ``Mamba'' model as
alternative solutions to traditional CNNs or ViTs. The Mamba model excels in
the linear processing of one-dimensional data with low computational demands.
However, Mamba's potential for 3D medical image analysis remains underexplored
and could face significant computational challenges as the dimension increases.
This manuscript presents MobileViM, a streamlined architecture for efficient
segmentation of 3D medical images. In the MobileViM network, we invent a new
dimension-independent mechanism and a dual-direction traversing approach to
incorporate with a vision-Mamba-based framework. MobileViM also features a
cross-scale bridging technique to improve efficiency and accuracy across
various medical imaging modalities. With these enhancements, MobileViM achieves
segmentation speeds exceeding 90 frames per second (FPS) on a single graphics
processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster
than the state-of-the-art deep learning models for processing 3D images with
the same computational resources. In addition, experimental evaluations
demonstrate that MobileViM delivers superior performance, with Dice similarity
scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,
ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses
existing models.

摘要：<paragraph>有效評估三維 (3D) 醫學影像對於醫療保健中的診斷和治療實務至關重要。近年來，將深度學習和電腦視覺應用於分析和詮釋醫學影像的應用大幅增加。傳統方法，例如卷積神經網路 (CNN) 和視覺Transformer (ViT)，面臨重大的運算挑戰，促使需要架構上的進步。最近的努力已導致引進創新的架構，例如「Mamba」模型，作為傳統 CNN 或 ViT 的替代解決方案。Mamba 模型擅長以低運算需求進行一維資料的線性處理。然而，Mamba 在 3D 醫學影像分析方面的潛力仍未被充分探索，並且隨著維度的增加可能會面臨重大的運算挑戰。本手稿提出 MobileViM，這是一種簡化的架構，可有效分割 3D 醫學影像。在 MobileViM 網路中，我們發明了一種新的與維度無關的機制和雙向遍歷方法，以與基於視覺 Mamba 的架構結合。MobileViM 還具備跨尺度橋接技術，以提高各種醫學影像模式的效率和準確性。透過這些增強功能，MobileViM 在單一顯示卡 (即 NVIDIA RTX 4090) 上達到了每秒超過 90 幀 (FPS) 的分割速度。此效能比現有最先進的深度學習模型快了超過 24 FPS，這些模型使用相同的運算資源處理 3D 影像。此外，實驗評估證明 MobileViM 提供了卓越的效能，Dice 相似性評分對於 PENGWIN、BraTS2024、ATLAS 和 Toothfairy2 資料集分別達到 92.72%、86.69%、80.46% 和 77.43%，顯著超越現有模型。</paragraph>

##### **A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment**
2502.13520v1 by Khalid N. Elmadani, Nizar Habash, Hanada Taha-Thomure

This paper introduces the Balanced Arabic Readability Evaluation Corpus
BAREC, a large-scale, fine-grained dataset for Arabic readability assessment.
BAREC consists of 68,182 sentences spanning 1+ million words, carefully curated
to cover 19 readability levels, from kindergarten to postgraduate
comprehension. The corpus balances genre diversity, topical coverage, and
target audiences, offering a comprehensive resource for evaluating Arabic text
complexity. The corpus was fully manually annotated by a large team of
annotators. The average pairwise inter-annotator agreement, measured by
Quadratic Weighted Kappa, is 81.3%, reflecting a high level of substantial
agreement. Beyond presenting the corpus, we benchmark automatic readability
assessment across different granularity levels, comparing a range of
techniques. Our results highlight the challenges and opportunities in Arabic
readability modeling, demonstrating competitive performance across various
methods. To support research and education, we will make BAREC openly
available, along with detailed annotation guidelines and benchmark results.

摘要：本文介紹了平衡阿拉伯語可讀性評估語料庫 (BAREC)，這是一個針對阿拉伯語可讀性評估的大規模、細粒度的資料集。
BAREC 包含 68,182 個句子，跨越 100 多萬個字詞，經過仔細策劃，涵蓋從幼稚園到研究所程度的 19 個可讀性等級。語料庫平衡了體裁的多樣性、主題的涵蓋範圍和目標受眾，提供了一個全面的資源，用於評估阿拉伯語文本的複雜性。語料庫由一個龐大的註解小組完全手動註解。由二次加權 Kappa 測量的平均成對註解者間一致性為 81.3%，反映出高度的實質性一致性。除了呈現語料庫之外，我們還比較了一系列技術，對不同粒度層級的自動可讀性評估進行基準測試。我們的結果突出了阿拉伯語可讀性建模中的挑戰和機會，展示了各種方法的競爭性能。為了支持研究和教育，我們將開放 BAREC，並提供詳細的註解指南和基準測試結果。

##### **MILE: Model-based Intervention Learning**
2502.13519v1 by Yigit Korkmaz, Erdem Bıyık

Imitation learning techniques have been shown to be highly effective in
real-world control scenarios, such as robotics. However, these approaches not
only suffer from compounding error issues but also require human experts to
provide complete trajectories. Although there exist interactive methods where
an expert oversees the robot and intervenes if needed, these extensions usually
only utilize the data collected during intervention periods and ignore the
feedback signal hidden in non-intervention timesteps. In this work, we create a
model to formulate how the interventions occur in such cases, and show that it
is possible to learn a policy with just a handful of expert interventions. Our
key insight is that it is possible to get crucial information about the quality
of the current state and the optimality of the chosen action from expert
feedback, regardless of the presence or the absence of intervention. We
evaluate our method on various discrete and continuous simulation environments,
a real-world robotic manipulation task, as well as a human subject study.
Videos and the code can be found at https://liralab.usc.edu/mile .

摘要：模仿學習技術已被證明在現實世界的控制場景中非常有效，例如機器人技術。然而，這些方法不僅會產生複合錯誤問題，還需要人類專家提供完整的軌跡。儘管存在交互式方法，專家可以在需要時監督機器人和進行干預，但這些擴展通常只利用干預期間收集的數據，而忽略非干預時間步長中隱藏的回饋信號。在這項工作中，我們創建了一個模型來制定在這種情況下如何進行干預，並表明僅憑藉少數專家干預就可以學習一項策略。我們的關鍵見解是，無論是否存在干預，都可以從專家回饋中獲得有關當前狀態質量和所選動作最優性的關鍵信息。我們在各種離散和連續模擬環境、一個真實世界的機器人操作任務以及一項人類主體研究中評估了我們的的方法。可以在 https://liralab.usc.edu/mile 找到影片和程式碼。

##### **SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin**
2502.13516v1 by Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu

Recently, enhancing the numerical and logical reasoning capability of Large
Language Models (LLMs) has emerged as a research hotspot. Existing methods face
several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely
on prompt selection and the pretrained knowledge; sentence-level Supervised
Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with
step-wise mathematical correctness and depend on stronger models distillation
or human annotations; while Reinforcement Learning (RL) approaches incur high
GPU memory costs and unstable training. To address these, we propose
\textbf{S}elf-training framework integrating \textbf{P}rocess
\textbf{P}reference learning using \textbf{D}ynamic value margin (SPPD). SPPD
leverages a process-based Markov Decision Process (MDP) and Bellman optimality
equation to derive \textbf{dynamic value margin} on step-level preference
optimization, which employs tree-based self-sampling on model responses
\textbf{without any distillation} from other models. Furthermore, we
theoretically prove that SPPD is \textbf{equivalent to on-policy policy
gradient methods} under reward constraints. Experiments on 7B-scale models
demonstrate superior performance across in-domain and out-domain mathematical
benchmarks. We open-source our code at
\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}.

摘要：<paragraph>近期，增强大型语言模型 (LLM) 的数字和逻辑推理能力已成为研究热点。现有方法面临着一些限制：推理阶段技术（例如思想链）依赖于提示选择和预训练知识；句子级别的监督微调 (SFT) 和直接偏好优化 (DPO) 难以实现逐步的数学正确性，并且依赖于更强的模型蒸馏或人工注释；而强化学习 (RL) 方法会产生高 GPU 内存成本和不稳定的训练。为了解决这些问题，我们提出了将基于过程的偏好学习与动态值边际 (SPPD) 相结合的自我训练框架。SPPD 利用基于过程的马尔可夫决策过程 (MDP) 和贝尔曼最优性方程，在步骤级偏好优化中推导出动态值边际，该边际在模型响应上采用基于树的自采样，而无需从其他模型中进行任何蒸馏。此外，我们从理论上证明了 SPPD 在奖励约束下等效于策略梯度法。在 7B 级模型上的实验表明，在域内和域外数学基准测试中都取得了卓越的性能。我们在
\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD} 上开源了我们的代码。</paragraph>

##### **Shall Your Data Strategy Work? Perform a Swift Study**
2502.13514v1 by Minlong Peng, Jingyi Yang, Zhongjun He, Hua Wu

This work presents a swift method to assess the efficacy of particular types
of instruction-tuning data, utilizing just a handful of probe examples and
eliminating the need for model retraining. This method employs the idea of
gradient-based data influence estimation, analyzing the gradient projections of
probe examples from the chosen strategy onto evaluation examples to assess its
advantages. Building upon this method, we conducted three swift studies to
investigate the potential of Chain-of-thought (CoT) data, query clarification
data, and response evaluation data in enhancing model generalization.
Subsequently, we embarked on a validation study to corroborate the findings of
these swift studies. In this validation study, we developed training datasets
tailored to each studied strategy and compared model performance with and
without the use of these datasets. The results of the validation study aligned
with the findings of the swift studies, validating the efficacy of our proposed
method.

摘要：本研究提出了一種快速的方法來評估特定類型指令調整資料的效能，僅使用少數探測範例，並消除重新訓練模型的需要。此方法採用基於梯度的資料影響估計概念，分析從所選策略中探測範例的梯度投影，到評估範例，以評估其優點。在此方法的基礎上，我們進行了三項快速研究，以探討思想鏈 (CoT) 資料、查詢澄清資料和回應評估資料在增強模型概括化中的潛力。隨後，我們著手進行驗證研究，以驗證這些快速研究的發現。在驗證研究中，我們開發了針對每個研究策略量身定制的訓練資料集，並比較了使用和不使用這些資料集的模型效能。驗證研究的結果與快速研究的發現一致，驗證了我們提出的方法的有效性。

##### **Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion**
2502.13509v1 by Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang

Large language models (LLMs) have shown remarkable performance in
vision-language tasks, but their application in the medical field remains
underexplored, particularly for integrating structured time series data with
unstructured clinical notes. In clinical practice, dynamic time series data
such as lab test results capture critical temporal patterns, while clinical
notes provide rich semantic context. Merging these modalities is challenging
due to the inherent differences between continuous signals and discrete text.
To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal
framework that employs prompt-guided learning to unify these heterogeneous data
types. Our approach leverages lightweight anomaly detection to generate anomaly
captions that serve as prompts, guiding the encoding of raw time series data
into informative embeddings. These embeddings are aligned with textual
representations in a shared latent space, preserving fine-grained temporal
nuances alongside semantic insights. Furthermore, our framework incorporates
tailored self-supervised objectives to enhance both intra- and inter-modal
alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world
datasets, and the results demonstrate that our method consistently outperforms
state-of-the-art approaches.

摘要：大型語言模型（LLM）在視覺語言任務中表現出色，但其在醫療領域的應用仍未得到充分探索，特別是在將結構化時間序列數據與非結構化臨床筆記整合方面。在臨床實務中，動態時間序列數據（例如實驗室檢驗結果）會擷取關鍵的時間模式，而臨床筆記則提供豐富的語意脈絡。由於連續訊號與離散文字之間的固有差異，合併這些方式具有挑戰性。為了彌補這個差距，我們引入了 ProMedTS，這是一個新穎的自監督多模態框架，採用提示引導學習來統一這些異質化的數據類型。我們的做法利用輕量級異常偵測來產生異常標題，作為提示，引導將原始時間序列數據編碼成資訊性的嵌入。這些嵌入與共享潛在空間中的文字表示對齊，同時保留細微的時間差異和語意見解。此外，我們的框架納入了客製化的自監督目標，以增強模態內和模態間對齊。我們在疾病診斷任務中使用真實世界的數據集評估 ProMedTS，結果表明，我們的模型始終優於最先進的方法。

##### **PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference**
2502.13502v1 by Burc Gokden

We show that Large Language Model from Power Law Decoder Representations
(PLDR-LLM) is a foundational model whose deductive outputs are invariant
tensors up to a small perturbation. PLDR-LLM learns a singularity condition for
the deductive outputs that enable the once-inferred energy-curvature tensor
$\mathbf{G}_{LM}$ to replace the deep neural network of power law graph
attention (PLGA) generating the deductive outputs at inference. We demonstrate
that a cache for $\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in
a straightforward manner to improve the inference time. The invariance and
generalizable nature of deductive outputs is at a very high fidelity where
deductive outputs have same RMSE and determinant values up to 15 decimal places
after caching, and zero-shot benchmark scores remain unchanged. Ablation
studies show that learned deductive outputs have distinct loss and accuracy
characteristics from models pretrained with transferred, randomly initialized
or identity tensors as a constant tensor operator and an LLM with scaled-dot
product attention (SDPA) is a special case of PLDR-LLM where $\mathbf{G}_{LM}$
is predefined as identity. The observed invariance characteristic introduces a
novel asymmetry between training and inference phases with caching. We outline
observed common characteristics of the deductive outputs for the learned
singularity condition. We provide an implementation of a training and inference
framework for PLDR-LLM with KV-cache and G-cache.

摘要：<paragraph>我們展示了來自冪律解碼器表示 (PLDR-LLM) 的大型語言模型是一個基礎模型，其演繹輸出是直到一個小擾動的不變張量。PLDR-LLM 學習演繹輸出的奇異條件，使曾經推斷出的能量曲率張量 $\mathbf{G}_{LM}$ 能夠取代產生演繹輸出的冪律圖注意力 (PLGA) 深度神經網路，進行推論。我們證明了 $\mathbf{G}_{LM}$ 快取 (G 快取) 和 KV 快取能夠以一種直接的方式實作，以改善推論時間。演繹輸出的不變性和可概化性質具有非常高的保真度，其中演繹輸出在快取後具有相同的 RMSE 和行列式值，直到小數點後 15 位，且零次學習基準分數保持不變。消融研究表明，學習的演繹輸出具有與使用轉移、隨機初始化或恆等張量作為常數張量算子和具有縮放點積注意力的 LLM 預先訓練的模型不同的損失和準確性特徵，並且 $\mathbf{G}_{LM}$ 被預先定義為恆等的 PLDR-LLM 的一個特例，其中 $\mathbf{G}_{LM}$ 被預先定義為恆等。觀察到的不變特徵引入了訓練和推論階段之間一個新的不對稱性，並帶有快取。我們概述了學習的奇異條件演繹輸出的觀察到的共同特徵。我們提供了一個具有 KV 快取和 G 快取的 PLDR-LLM 訓練和推論框架的實作。</paragraph>

##### **Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in Ecommerce Web Components Generated by LLMs**
2502.13499v1 by Ziwei Chen, Jiawen Shen, Luna, Kristen Vaccaro

Recent work has highlighted the risks of LLM-generated content for a wide
range of harmful behaviors, including incorrect and harmful code. In this work,
we extend this by studying whether LLM-generated web design contains dark
patterns. This work evaluated designs of ecommerce web components generated by
four popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used
ecommerce components (e.g., search, product reviews) and used them as prompts
to generate a total of 312 components across all models. Over one-third of
generated components contain at least one dark pattern. The majority of dark
pattern strategies involve hiding crucial information, limiting users' actions,
and manipulating them into making decisions through a sense of urgency. Dark
patterns are also more frequently produced in components that are related to
company interests. These findings highlight the need for interventions to
prevent dark patterns during front-end code generation with LLMs and emphasize
the importance of expanding ethical design education to a broader audience.

摘要：近期研究強調出 LLM 生成的內容對於各種有害行為的風險，包括不正確且有害的程式碼。在這項研究中，我們透過探討 LLM 生成的網頁設計是否包含惡意模式來延伸這項研究。這項研究評估了由四種常見 LLM 生成的電子商務網頁元件設計：Claude、GPT、Gemini 和 Llama。我們測試了 13 個常用的電子商務元件（例如搜尋、產品評論），並將它們用作提示，在所有模型中總共產生了 312 個元件。超過三分之一的已生成元件包含至少一個惡意模式。大多數惡意模式策略涉及隱藏關鍵資訊、限制使用者的操作，以及透過急迫感操縱他們做出決定。惡意模式也更常出現在與公司利益相關的元件中。這些發現強調了在 LLM 前端程式碼產生期間預防惡意模式的干預措施的必要性，並強調將道德設計教育擴展到更廣泛受眾的重要性。

##### **Towards Geo-Culturally Grounded LLM Generations**
2502.13497v1 by Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin, Sunipa Dev

Generative large language models (LLMs) have been demonstrated to have gaps
in diverse, cultural knowledge across the globe. We investigate the effect of
retrieval augmented generation and search-grounding techniques on the ability
of LLMs to display familiarity with a diverse range of national cultures.
Specifically, we compare the performance of standard LLMs, LLMs augmented with
retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs
augmented with retrievals from a web search (i.e., search grounding) on a
series of cultural familiarity benchmarks. We find that search grounding
significantly improves the LLM performance on multiple-choice benchmarks that
test propositional knowledge (e.g., the norms, artifacts, and institutions of
national cultures), while KB grounding's effectiveness is limited by inadequate
knowledge base coverage and a suboptimal retriever. However, search grounding
also increases the risk of stereotypical judgments by language models, while
failing to improve evaluators' judgments of cultural familiarity in a human
evaluation with adequate statistical power. These results highlight the
distinction between propositional knowledge about a culture and open-ended
cultural fluency when it comes to evaluating the cultural familiarity of
generative LLMs.

摘要：生成式大型语言模型（LLM）已被证明在全球范围内存在文化知识方面的差距。我们调查了检索增强生成和搜索基础技术对 LLM 展示对各种国家文化熟悉程度的能力的影响。具体来说，我们比较了标准 LLM、使用来自定制知识库（即 KB 基础）的检索增强 LLM 和使用来自网络搜索（即搜索基础）的检索增强 LLM 在一系列文化熟悉基准上的性能。我们发现，搜索基础显着提高了 LLM 在测试命题知识（例如，国家文化的规范、人工制品和制度）的多项选择基准上的性能，而 KB 基础的有效性受到知识库覆盖不足和检索器欠佳的限制。然而，搜索基础也增加了语言模型产生刻板印象判断的风险，同时未能提高评估者在具有足够统计能力的人类评估中对文化熟悉程度的判断。这些结果突出了在评估生成式 LLM 的文化熟悉度时，关于文化的命题知识和开放式文化流畅性之间的区别。

