{"2410.05077": {"publish_time": "2024-10-07", "title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "paper_summary": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.", "paper_summary_zh": "\u7576\u524d\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5e38\u8b58\u554f\u7b54\u57fa\u6e96\u4e2d\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6210\u529f\u80cc\u5f8c\u7684\u904e\u7a0b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u4e0d\u900f\u660e\u3002\u56e0\u6b64\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u70ba LLM \u914d\u5099\u4e86\u77e5\u8b58\u6aa2\u7d22\u3001\u63a8\u7406\u548c\u5167\u7701\u7684\u6a5f\u5236\uff0c\u4e0d\u50c5\u53ef\u4ee5\u63d0\u5347\u5176\u80fd\u529b\uff0c\u9084\u80fd\u589e\u5f37\u5176\u8f38\u51fa\u7684\u53ef\u89e3\u91cb\u6027\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u9700\u8981\u984d\u5916\u7684\u8a13\u7df4\u3001\u624b\u5de5\u88fd\u4f5c\u7684\u6a21\u677f\u6216\u4eba\u70ba\u64b0\u5beb\u7684\u8aaa\u660e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 ZEBRA\uff0c\u9019\u662f\u4e00\u500b\u96f6\u6b21\u5b78\u7fd2\u554f\u984c\u56de\u7b54\u6846\u67b6\uff0c\u5b83\u7d50\u5408\u4e86\u6aa2\u7d22\u3001\u57fa\u65bc\u6848\u4f8b\u7684\u63a8\u7406\u548c\u5167\u7701\uff0c\u4e26\u6d88\u9664\u4e86\u5c0d LLM \u9032\u884c\u984d\u5916\u8a13\u7df4\u7684\u9700\u6c42\u3002\u7d66\u5b9a\u4e00\u500b\u8f38\u5165\u554f\u984c\uff0cZEBRA \u6703\u5f9e\u77e5\u8b58\u5eab\u4e2d\u6aa2\u7d22\u76f8\u95dc\u7684\u554f\u984c\u77e5\u8b58\u5c0d\uff0c\u4e26\u901a\u904e\u63a8\u7406\u9019\u4e9b\u5c0d\u4e2d\u7684\u95dc\u4fc2\u4f86\u7522\u751f\u65b0\u7684\u77e5\u8b58\u3002\u7136\u5f8c\u4f7f\u7528\u9019\u500b\u7522\u751f\u7684\u77e5\u8b58\u4f86\u56de\u7b54\u8f38\u5165\u554f\u984c\uff0c\u5f9e\u800c\u63d0\u5347\u6a21\u578b\u7684\u6548\u80fd\u548c\u53ef\u89e3\u91cb\u6027\u3002\u6211\u5011\u5728 8 \u500b\u5b8c\u5584\u7684\u5e38\u8b58\u63a8\u7406\u57fa\u6e96\u4e2d\u8a55\u4f30\u4e86\u6211\u5011\u7684\u505a\u6cd5\uff0c\u8b49\u660e ZEBRA \u6301\u7e8c\u512a\u65bc\u5f37\u5927\u7684 LLM \u548c\u5148\u524d\u7684\u77e5\u8b58\u6574\u5408\u65b9\u6cd5\uff0c\u5e73\u5747\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86 4.5 \u500b\u767e\u5206\u9ede\u3002", "author": "Francesco Maria Molfese et.al.", "authors": "Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli", "id": "2410.05077v1", "paper_url": "http://arxiv.org/abs/2410.05077v1", "repo": "null"}}