
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-27**|**The Mamba in the Llama: Distilling and Accelerating Hybrid Models**|Junxiong Wang et.al.|[2408.15237v1](http://arxiv.org/abs/2408.15237v1)|[link](https://github.com/jxiw/mambainllama)|
|**2024-08-27**|**Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations**|Yucheng Jiang et.al.|[2408.15232v1](http://arxiv.org/abs/2408.15232v1)|null|
|**2024-08-27**|**LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet**|Nathaniel Li et.al.|[2408.15221v1](http://arxiv.org/abs/2408.15221v1)|null|
|**2024-08-27**|**Classifying populist language in American presidential and governor speeches using automatic text analysis**|Olaf van der Veen et.al.|[2408.15213v1](http://arxiv.org/abs/2408.15213v1)|null|
|**2024-08-27**|**Can Unconfident LLM Annotations Be Used for Confident Conclusions?**|Kristina Gligorić et.al.|[2408.15204v1](http://arxiv.org/abs/2408.15204v1)|null|
|**2024-08-27**|**Infusing Acoustic Pause Context into Text-Based Dementia Assessment**|Franziska Braun et.al.|[2408.15188v1](http://arxiv.org/abs/2408.15188v1)|null|
|**2024-08-27**|**PoseWatch: A Transformer-based Architecture for Human-centric Video Anomaly Detection Using Spatio-temporal Pose Tokenization**|Ghazal Alinezhad Noghre et.al.|[2408.15185v1](http://arxiv.org/abs/2408.15185v1)|null|
|**2024-08-27**|**Unlocking Potential in Pre-Trained Music Language Models for Versatile Multi-Track Music Arrangement**|Longshen Ou et.al.|[2408.15176v1](http://arxiv.org/abs/2408.15176v1)|null|
|**2024-08-27**|**X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation**|Hanjia Lyu et.al.|[2408.15172v1](http://arxiv.org/abs/2408.15172v1)|null|
|**2024-08-27**|**Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation**|N. E. Kriman et.al.|[2408.15171v1](http://arxiv.org/abs/2408.15171v1)|null|
|**2024-08-27**|**How transformers learn structured data: insights from hierarchical filtering**|Jerome Garnier-Brun et.al.|[2408.15138v1](http://arxiv.org/abs/2408.15138v1)|null|
|**2024-08-27**|**Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments**|Charlotte Rodriguez et.al.|[2408.15128v1](http://arxiv.org/abs/2408.15128v1)|null|
|**2024-08-27**|**Urdu Digital Text Word Optical Character Recognition Using Permuted Auto Regressive Sequence Modeling**|Ahmed Mustafa et.al.|[2408.15119v2](http://arxiv.org/abs/2408.15119v2)|null|
|**2024-08-27**|**Evaluating Stability of Unreflective Alignment**|James Lucassen et.al.|[2408.15116v1](http://arxiv.org/abs/2408.15116v1)|null|
|**2024-08-27**|**MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders**|Baijiong Lin et.al.|[2408.15101v1](http://arxiv.org/abs/2408.15101v1)|[link](https://github.com/envision-research/mtmamba)|
|**2024-08-27**|**Post-processing fairness with minimal changes**|Federico Di Gennaro et.al.|[2408.15096v1](http://arxiv.org/abs/2408.15096v1)|null|
|**2024-08-27**|**Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models**|Xiyu Liu et.al.|[2408.15091v1](http://arxiv.org/abs/2408.15091v1)|null|
|**2024-08-27**|**BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline**|Guosheng Dong et.al.|[2408.15079v1](http://arxiv.org/abs/2408.15079v1)|null|
|**2024-08-27**|**MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of Children with Autism Spectrum Disorder**|Pavan Uttej Ravva et.al.|[2408.15077v1](http://arxiv.org/abs/2408.15077v1)|[link](https://github.com/pavanravva/enhanced-mmasd)|
|**2024-08-27**|**Interactive dense pixel visualizations for time series and model attribution explanations**|Udo Schlegel et.al.|[2408.15073v1](http://arxiv.org/abs/2408.15073v1)|null|
|**2024-08-27**|**Causal Rule Forest: Toward Interpretable and Precise Treatment Effect Estimation**|Chan Hsu et.al.|[2408.15055v1](http://arxiv.org/abs/2408.15055v1)|null|
|**2024-08-27**|**Self-supervised Topic Taxonomy Discovery in the Box Embedding Space**|Yuyin Lu et.al.|[2408.15050v1](http://arxiv.org/abs/2408.15050v1)|null|
|**2024-08-27**|**A Survey of Large Language Models for European Languages**|Wazir Ali et.al.|[2408.15040v2](http://arxiv.org/abs/2408.15040v2)|null|
|**2024-08-27**|**Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering**|Haowei Du et.al.|[2408.15037v1](http://arxiv.org/abs/2408.15037v1)|null|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|null|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Speech Recognition Transformers: Topological-lingualism Perspective**|Shruti Singh et.al.|[2408.14991v1](http://arxiv.org/abs/2408.14991v1)|null|
|**2024-08-27**|**Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for Long-Tailed Continual Learning**|Lei Liu et.al.|[2408.14976v1](http://arxiv.org/abs/2408.14976v1)|null|
|**2024-08-27**|**AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems**|Chi-Min Chan et.al.|[2408.14972v1](http://arxiv.org/abs/2408.14972v1)|null|
|**2024-08-27**|**CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task**|Lingyun Huang et.al.|[2408.14961v1](http://arxiv.org/abs/2408.14961v1)|null|
|**2024-08-27**|**Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress**|Ayomide Odumakinde et.al.|[2408.14960v1](http://arxiv.org/abs/2408.14960v1)|null|
|**2024-08-27**|**NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework**|Shuangchen Zhao et.al.|[2408.14950v1](http://arxiv.org/abs/2408.14950v1)|null|
|**2024-08-27**|**Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures**|Tomi Silander et.al.|[2408.14935v1](http://arxiv.org/abs/2408.14935v1)|null|
|**2024-08-27**|**Distance-Forward Learning: Enhancing the Forward-Forward Algorithm Towards High-Performance On-Chip Learning**|Yujie Wu et.al.|[2408.14925v1](http://arxiv.org/abs/2408.14925v1)|null|
|**2024-08-27**|**SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models**|Shuaijie Shen et.al.|[2408.14909v1](http://arxiv.org/abs/2408.14909v1)|null|
|**2024-08-27**|**Triplètoile: Extraction of Knowledge from Microblogging Text**|Vanni Zavarella et.al.|[2408.14908v1](http://arxiv.org/abs/2408.14908v1)|null|
|**2024-08-27**|**Writing in the Margins: Better Inference Pattern for Long Context Retrieval**|Melisa Russak et.al.|[2408.14906v1](http://arxiv.org/abs/2408.14906v1)|[link](https://github.com/writer/writing-in-the-margins)|
|**2024-08-27**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.|[2408.14895v2](http://arxiv.org/abs/2408.14895v2)|null|
|**2024-08-27**|**Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures**|Pooja Krishan et.al.|[2408.14875v1](http://arxiv.org/abs/2408.14875v1)|null|
|**2024-08-27**|**Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data**|Han Xia et.al.|[2408.14874v1](http://arxiv.org/abs/2408.14874v1)|null|
|**2024-08-27**|**Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models**|Hongfu Liu et.al.|[2408.14866v1](http://arxiv.org/abs/2408.14866v1)|null|
|**2024-08-27**|**Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL**|Jihwan Lee et.al.|[2408.14855v1](http://arxiv.org/abs/2408.14855v1)|null|
|**2024-08-27**|**Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models**|Yuhao Du et.al.|[2408.14853v1](http://arxiv.org/abs/2408.14853v1)|null|
|**2024-08-27**|**Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing**|Hanna Abi Akl et.al.|[2408.14849v1](http://arxiv.org/abs/2408.14849v1)|null|
|**2024-08-27**|**AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark**|Abhay Gupta et.al.|[2408.14845v1](http://arxiv.org/abs/2408.14845v1)|null|
|**2024-08-27**|**Diffusion based Semantic Outlier Generation via Nuisance Awareness for Out-of-Distribution Detection**|Suhee Yoon et.al.|[2408.14841v1](http://arxiv.org/abs/2408.14841v1)|null|
|**2024-08-27**|**CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding**|Yang Liu et.al.|[2408.14840v1](http://arxiv.org/abs/2408.14840v1)|null|
|**2024-08-27**|**Diffusion Models Are Real-Time Game Engines**|Dani Valevski et.al.|[2408.14837v1](http://arxiv.org/abs/2408.14837v1)|null|
|**2024-08-27**|**Strategic Optimization and Challenges of Large Language Models in Object-Oriented Programming**|Zinan Wang et.al.|[2408.14834v1](http://arxiv.org/abs/2408.14834v1)|null|
|**2024-08-27**|**PolicyLR: A Logic Representation For Privacy Policies**|Ashish Hooda et.al.|[2408.14830v1](http://arxiv.org/abs/2408.14830v1)|null|
|**2024-08-27**|**From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation**|Nada Shahin et.al.|[2408.14825v1](http://arxiv.org/abs/2408.14825v1)|null|
|**2024-08-27**|**A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets**|Assaf Shmuel et.al.|[2408.14817v1](http://arxiv.org/abs/2408.14817v1)|null|
|**2024-08-27**|**Brain-inspired Artificial Intelligence: A Comprehensive Review**|Jing Ren et.al.|[2408.14811v1](http://arxiv.org/abs/2408.14811v1)|null|
|**2024-08-27**|**Poly2Vec: Polymorphic Encoding of Geospatial Objects for Spatial Reasoning with Deep Neural Networks**|Maria Despoina Siampou et.al.|[2408.14806v1](http://arxiv.org/abs/2408.14806v1)|null|
|**2024-08-27**|**GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks**|Nisal Ranasinghe et.al.|[2408.14780v2](http://arxiv.org/abs/2408.14780v2)|null|
|**2024-08-27**|**MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Semantic Segmentation**|Yuanbing Zhu et.al.|[2408.14776v1](http://arxiv.org/abs/2408.14776v1)|null|
|**2024-08-27**|**Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning**|Simran Kaur et.al.|[2408.14774v1](http://arxiv.org/abs/2408.14774v1)|null|
|**2024-08-27**|**A global AI community requires language-diverse publishing**|Haley Lepp et.al.|[2408.14772v1](http://arxiv.org/abs/2408.14772v1)|null|
|**2024-08-27**|**CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy Concerns**|Anbai Jiang et.al.|[2408.14753v1](http://arxiv.org/abs/2408.14753v1)|null|
|**2024-08-27**|**LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models**|Haven Kim et.al.|[2408.14750v1](http://arxiv.org/abs/2408.14750v1)|[link](https://github.com/havenpersona/lycon)|
|**2024-08-27**|**Benchmarking Reinforcement Learning Methods for Dexterous Robotic Manipulation with a Three-Fingered Gripper**|Elizabeth Cutler et.al.|[2408.14747v1](http://arxiv.org/abs/2408.14747v1)|null|
|**2024-08-27**|**RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models**|Junyao Ge et.al.|[2408.14744v1](http://arxiv.org/abs/2408.14744v1)|[link](https://github.com/slytheringe/rsteller)|
|**2024-08-27**|**PAT: Pruning-Aware Tuning for Large Language Models**|Yijiang Liu et.al.|[2408.14721v1](http://arxiv.org/abs/2408.14721v1)|[link](https://github.com/kriskrisliu/pat_pruning-aware-tuning)|
|**2024-08-27**|**Residual-based Adaptive Huber Loss (RAHL) -- Design of an improved Huber loss for CQI prediction in 5G networks**|Mina Kaviani et.al.|[2408.14718v1](http://arxiv.org/abs/2408.14718v1)|null|
|**2024-08-27**|**Text2SQL is Not Enough: Unifying AI and Databases with TAG**|Asim Biswal et.al.|[2408.14717v1](http://arxiv.org/abs/2408.14717v1)|[link](https://github.com/tag-research/tag-bench)|
|**2024-08-27**|**StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech**|Haowei Lou et.al.|[2408.14713v1](http://arxiv.org/abs/2408.14713v1)|null|
|**2024-08-26**|**Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express**|Cherag Aroraa et.al.|[2408.14698v1](http://arxiv.org/abs/2408.14698v1)|null|
|**2024-08-26**|**Training-Free Activation Sparsity in Large Language Models**|James Liu et.al.|[2408.14690v1](http://arxiv.org/abs/2408.14690v1)|null|
|**2024-08-26**|**Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems**|Nikhil Khani et.al.|[2408.14678v1](http://arxiv.org/abs/2408.14678v1)|null|
|**2024-08-26**|**Emergent Language in Open-Ended Environments**|Cornelius Wolff et.al.|[2408.14649v1](http://arxiv.org/abs/2408.14649v1)|null|
|**2024-08-26**|**Hybrid Deep Convolutional Neural Networks Combined with Autoencoders And Augmented Data To Predict The Look-Up Table 2006**|Messaoud Djeddou et.al.|[2408.14626v1](http://arxiv.org/abs/2408.14626v1)|null|
|**2024-08-26**|**MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval and Text Generation Functions**|Yingqiang Gao et.al.|[2408.14623v1](http://arxiv.org/abs/2408.14623v1)|null|
|**2024-08-26**|**What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation**|Dingyi Yang et.al.|[2408.14622v1](http://arxiv.org/abs/2408.14622v1)|null|
|**2024-08-26**|**Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models**|Ian Stewart et.al.|[2408.14595v1](http://arxiv.org/abs/2408.14595v1)|null|
|**2024-08-26**|**How to build trust in answers given by Generative AI for specific, and vague, financial questions**|Alex Zarifis et.al.|[2408.14593v1](http://arxiv.org/abs/2408.14593v1)|null|
|**2024-08-26**|**DIAGen: Diverse Image Augmentation with Generative Models**|Tobias Lingenberg et.al.|[2408.14584v1](http://arxiv.org/abs/2408.14584v1)|null|
|**2024-08-26**|**EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics and Information Theory**|Edward Y. Chang et.al.|[2408.14575v1](http://arxiv.org/abs/2408.14575v1)|null|
|**2024-08-26**|**CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation**|Muhammad Fawi et.al.|[2408.14572v1](http://arxiv.org/abs/2408.14572v1)|[link](https://github.com/mnoorfawi/curlora)|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**A Survey of Camouflaged Object Detection and Beyond**|Fengyang Xiao et.al.|[2408.14562v1](http://arxiv.org/abs/2408.14562v1)|null|
|**2024-08-26**|**Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization**|Nicholas Moratelli et.al.|[2408.14547v1](http://arxiv.org/abs/2408.14547v1)|[link](https://github.com/aimagelab/dico)|
|**2024-08-26**|**Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning**|Xinyang Gu et.al.|[2408.14472v1](http://arxiv.org/abs/2408.14472v1)|null|
|**2024-08-26**|**A Practitioner's Guide to Continual Multimodal Pretraining**|Karsten Roth et.al.|[2408.14471v1](http://arxiv.org/abs/2408.14471v1)|[link](https://github.com/explainableml/fomo_in_flux)|
|**2024-08-26**|**Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**|Aradhye Agarwal et.al.|[2408.14470v2](http://arxiv.org/abs/2408.14470v2)|[link](https://github.com/Aradhye2002/selective-peft-toolkit)|
|**2024-08-26**|**K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences**|Zhikai Li et.al.|[2408.14468v1](http://arxiv.org/abs/2408.14468v1)|null|
|**2024-08-26**|**Explicit Inductive Inference using Large Language Models**|Tianyang Liu et.al.|[2408.14467v1](http://arxiv.org/abs/2408.14467v1)|null|
|**2024-08-26**|**Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification**|Mahrukh Awan et.al.|[2408.14441v1](http://arxiv.org/abs/2408.14441v1)|null|
|**2024-08-26**|**Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**|Liuchang Xu et.al.|[2408.14438v2](http://arxiv.org/abs/2408.14438v2)|null|
|**2024-08-26**|**Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview**|Ilkin Aliyev et.al.|[2408.14437v1](http://arxiv.org/abs/2408.14437v1)|null|
|**2024-08-26**|**Social perception of faces in a vision-language model**|Carina I. Hausladen et.al.|[2408.14435v1](http://arxiv.org/abs/2408.14435v1)|[link](https://github.com/carinahausladen/clip-face-bias)|
|**2024-08-26**|**Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications**|Luyue Xu et.al.|[2408.14432v2](http://arxiv.org/abs/2408.14432v2)|null|
|**2024-08-26**|**CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**|Shubham Bharti et.al.|[2408.14419v1](http://arxiv.org/abs/2408.14419v1)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v1](http://arxiv.org/abs/2408.14418v1)|null|
|**2024-08-26**|**Language-specific Calibration for Pruning Multilingual Language Models**|Simon Kurz et.al.|[2408.14398v2](http://arxiv.org/abs/2408.14398v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**|Sakhinana Sagar Srinivas et.al.|[2408.14387v1](http://arxiv.org/abs/2408.14387v1)|null|
|**2024-08-26**|**Probing Causality Manipulation of Large Language Models**|Chenyang Zhang et.al.|[2408.14380v1](http://arxiv.org/abs/2408.14380v1)|[link](https://github.com/tongjinlp/llm-causality-probing)|
|**2024-08-26**|**SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery**|Sarah Rastegar et.al.|[2408.14371v1](http://arxiv.org/abs/2408.14371v1)|[link](https://github.com/sarahrastegar/selex)|
|**2024-08-26**|**GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy**|Peiyan Li et.al.|[2408.14368v1](http://arxiv.org/abs/2408.14368v1)|[link](https://github.com/bytedance/GR-MG)|
|**2024-08-26**|**SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**|Daoguang Zan et.al.|[2408.14354v1](http://arxiv.org/abs/2408.14354v1)|[link](https://github.com/multi-swe-bench/multi-swe-bench-env)|

#### Abstracts
##### **The Mamba in the Llama: Distilling and Accelerating Hybrid Models**
2408.15237v1 by Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao

Linear RNN architectures, like Mamba, can be competitive with Transformer
models in language modeling while having advantageous deployment
characteristics. Given the focus on training large-scale Transformer models, we
consider the challenge of converting these pretrained models for deployment. We
demonstrate that it is feasible to distill large Transformers into linear RNNs
by reusing the linear projection weights from attention layers with academic
GPU resources. The resulting hybrid model, which incorporates a quarter of the
attention layers, achieves performance comparable to the original Transformer
in chat benchmarks and outperforms open-source hybrid Mamba models trained from
scratch with trillions of tokens in both chat benchmarks and general
benchmarks. Moreover, we introduce a hardware-aware speculative decoding
algorithm that accelerates the inference speed of Mamba and hybrid models.
Overall we show how, with limited computation resources, we can remove many of
the original attention layers and generate from the resulting model more
efficiently. Our top-performing model, distilled from Llama3-8B-Instruct,
achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and
7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

摘要：線性 RNN 架構，例如 Mamba，在語言模型中可以與 Transformer 模型競爭，同時具有有利的部署特性。鑑於重點在訓練大型 Transformer 模型，我們考慮將這些預訓練模型轉換為部署的挑戰。我們證明了透過重複使用注意力層的線性投影權重，將大型 Transformer 萃取到線性 RNN 中是可行的，並具備學術 GPU 資源。所產生的混合模型包含四分之一的注意力層，在聊天基準測試中實現與原始 Transformer 相當的效能，並優於從頭開始訓練的開源混合 Mamba 模型，且聊天基準測試和一般基準測試中都有數兆個代幣。此外，我們引入了硬體感知推測解碼演算法，可加速 Mamba 和混合模型的推論速度。總的來說，我們展示了如何透過有限的運算資源，移除許多原始的注意力層，並更有效率地從產生的模型中生成。我們從 Llama3-8B-Instruct 萃取出的效能最佳模型，在 AlpacaEval 2 中對上 GPT-4 和 MT-Bench 中的獲勝率分別達到 29.61 和 7.35，超越了最佳的指令調整線性 RNN 模型。

##### **Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations**
2408.15232v1 by Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, Monica S. Lam

While language model (LM)-powered chatbots and generative search engines
excel at answering concrete queries, discovering information in the terrain of
unknown unknowns remains challenging for users. To emulate the common
educational scenario where children/students learn by listening to and
participating in conversations of their parents/teachers, we create
Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all
the questions, Co-STORM lets users observe and occasionally steer the discourse
among several LM agents. The agents ask questions on the user's behalf,
allowing the user to discover unknown unknowns serendipitously. To facilitate
user interaction, Co-STORM assists users in tracking the discourse by
organizing the uncovered information into a dynamic mind map, ultimately
generating a comprehensive report as takeaways. For automatic evaluation, we
construct the WildSeek dataset by collecting real information-seeking records
with user goals. Co-STORM outperforms baseline methods on both discourse trace
and report quality. In a further human evaluation, 70% of participants prefer
Co-STORM over a search engine, and 78% favor it over a RAG chatbot.

摘要：<paragraph>雖然語言模型 (LM) 驅動的聊天機器人和生成式搜尋引擎
擅長回答具體的查詢，但在未知的未知領域中發現資訊對使用者來說仍然具有挑戰性。為了模擬常見的教育場景，讓兒童/學生透過聆聽和參與父母/老師的對話來學習，我們建立了協作式 STORM (Co-STORM)。與要求使用者提出所有問題的問答系統不同，Co-STORM 使用者可以觀察並偶爾引導多個 LM 代理之間的討論。代理會代表使用者提出問題，讓使用者可以意外地發現未知的未知。為了促進使用者互動，Co-STORM 透過將發現的資訊組織成動態心智圖來協助使用者追蹤討論，最終產生一份全面的報告作為重點。為了自動評估，我們透過收集包含使用者目標的真實資訊搜尋記錄來建構 WildSeek 資料集。Co-STORM 在討論記錄和報告品質上都優於基準方法。在進一步的人工評估中，70% 的參與者偏好 Co-STORM 而非搜尋引擎，而 78% 的參與者偏好 Co-STORM 而非 RAG 聊天機器人。</paragraph>

##### **LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet**
2408.15221v1 by Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue

Recent large language model (LLM) defenses have greatly improved models'
ability to refuse harmful queries, even when adversarially attacked. However,
LLM defenses are primarily evaluated against automated adversarial attacks in a
single turn of conversation, an insufficient threat model for real-world
malicious use. We demonstrate that multi-turn human jailbreaks uncover
significant vulnerabilities, exceeding 70% attack success rate (ASR) on
HarmBench against defenses that report single-digit ASRs with automated
single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine
unlearning defenses, successfully recovering dual-use biosecurity knowledge
from unlearned models. We compile these results into Multi-Turn Human
Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.
We publicly release MHJ alongside a compendium of jailbreak tactics developed
across dozens of commercial red teaming engagements, supporting research
towards stronger LLM defenses.

摘要：近期的大型语言模型（LLM）防御功能已大幅提升模型拒绝有害查询的能力，即使在对抗性攻击下也能如此。然而，LLM 防御功能主要针对对话中的单回合自动化对抗性攻击进行评估，这对于现实世界中的恶意使用而言是不够的威胁模型。我们证明，多回合人类越狱揭示了严重的漏洞，在 HarmBench 上对报告单回合自动化攻击时具有个位数 ASR 的防御措施，其攻击成功率 (ASR) 超过 70%。人类越狱还揭示了机器取消学习防御中的漏洞，成功地从未学习的模型中恢复了双重用途的生物安全知识。我们将这些结果汇编成多回合人类越狱 (MHJ)，这是一个包含 537 个多回合越狱的 2,912 个提示的数据集。我们在几十次商业红队参与行动中开发的越狱策略汇编中公开发布了 MHJ，支持针对更强大的 LLM 防御功能的研究。

##### **Classifying populist language in American presidential and governor speeches using automatic text analysis**
2408.15213v1 by Olaf van der Veen, Semir Dzebo, Levi Littvay, Kirk Hawkins, Oren Dar

Populism is a concept that is often used but notoriously difficult to
measure. Common qualitative measurements like holistic grading or content
analysis require great amounts of time and labour, making it difficult to
quickly scope out which politicians should be classified as populist and which
should not, while quantitative methods show mixed results when it comes to
classifying populist rhetoric. In this paper, we develop a pipeline to train
and validate an automated classification model to estimate the use of populist
language. We train models based on sentences that were identified as populist
and pluralist in 300 US governors' speeches from 2010 to 2018 and in 45
speeches of presidential candidates in 2016. We find that these models classify
most speeches correctly, including 84% of governor speeches and 89% of
presidential speeches. These results extend to different time periods (with 92%
accuracy on more recent American governors), different amounts of data (with as
few as 70 training sentences per category achieving similar results), and when
classifying politicians instead of individual speeches. This pipeline is thus
an effective tool that can optimise the systematic and swift classification of
the use of populist language in politicians' speeches.

摘要：民粹主義是一個經常被使用，但難以測量的概念。常見的定性測量，例如整體評分或內容分析，需要大量時間和勞力，這使得快速找出哪些政治人物應被歸類為民粹主義者，哪些不應被歸類為民粹主義者變得困難，而定量方法在對民粹主義言論進行分類時則顯示出不同的結果。在本文中，我們開發了一個管道來訓練和驗證一個自動分類模型，以估計民粹主義語言的使用情況。我們根據在 2010 年至 2018 年間 300 位美國州長演講中被確定為民粹主義和多元主義的句子，以及 2016 年 45 位總統候選人的演講來訓練模型。我們發現這些模型正確地對大多數演講進行了分類，包括 84% 的州長演講和 89% 的總統演講。這些結果延伸到不同的時間段（對最近的美國州長有 92% 的準確度）、不同的數據量（每類別少至 70 個訓練句子即可獲得類似的結果），以及在對政治人物進行分類而不是對個別演講進行分類時。因此，這個管道是一個有效的工具，可以優化對政治人物演講中民粹主義語言使用情況的系統且快速的分類。

##### **Can Unconfident LLM Annotations Be Used for Confident Conclusions?**
2408.15204v1 by Kristina Gligorić, Tijana Zrnic, Cinoo Lee, Emmanuel J. Candès, Dan Jurafsky

Large language models (LLMs) have shown high agreement with human raters
across a variety of tasks, demonstrating potential to ease the challenges of
human data collection. In computational social science (CSS), researchers are
increasingly leveraging LLM annotations to complement slow and expensive human
annotations. Still, guidelines for collecting and using LLM annotations,
without compromising the validity of downstream conclusions, remain limited. We
introduce Confidence-Driven Inference: a method that combines LLM annotations
and LLM confidence indicators to strategically select which human annotations
should be collected, with the goal of producing accurate statistical estimates
and provably valid confidence intervals while reducing the number of human
annotations needed. Our approach comes with safeguards against LLM annotations
of poor quality, guaranteeing that the conclusions will be both valid and no
less accurate than if we only relied on human annotations. We demonstrate the
effectiveness of Confidence-Driven Inference over baselines in statistical
estimation tasks across three CSS settings--text politeness, stance, and
bias--reducing the needed number of human annotations by over 25% in each.
Although we use CSS settings for demonstration, Confidence-Driven Inference can
be used to estimate most standard quantities across a broad range of NLP
problems.

摘要：大型語言模型 (LLM) 在各種任務中已展現出與人類評分者高度一致，證明了其減輕人類資料收集挑戰的潛力。在計算社會科學 (CSS) 中，研究人員正日益利用 LLM 標註來補充緩慢且昂貴的人類標註。儘管如此，在不損害下游結論的有效性的前提下，收集和使用 LLM 標註的準則仍然有限。我們引入了信心驅動推論：一種結合 LLM 標註和 LLM 信心指標的方法，以策略性地選擇應收集哪些人類標註，目標是在減少所需人類標註數量的同時產生準確的統計估計和可證明有效的信心區間。我們的做法帶有針對品質不佳的 LLM 標註的保障措施，確保結論既有效又不會比我們僅依賴人類標註時不準確。我們在三個 CSS 設定（文字禮貌、立場和偏見）中，證明了信心驅動推論優於基線的有效性，在每個設定中將所需的人類標註數量減少了 25% 以上。儘管我們使用 CSS 設定進行示範，但信心驅動推論可用於估計廣泛 NLP 問題中的大多數標準量。

##### **Infusing Acoustic Pause Context into Text-Based Dementia Assessment**
2408.15188v1 by Franziska Braun, Sebastian P. Bayerl, Florian Hönig, Hartmut Lehfeld, Thomas Hillemacher, Tobias Bocklet, Korbinian Riedhammer

Speech pauses, alongside content and structure, offer a valuable and
non-invasive biomarker for detecting dementia. This work investigates the use
of pause-enriched transcripts in transformer-based language models to
differentiate the cognitive states of subjects with no cognitive impairment,
mild cognitive impairment, and Alzheimer's dementia based on their speech from
a clinical assessment. We address three binary classification tasks: Onset,
monitoring, and dementia exclusion. The performance is evaluated through
experiments on a German Verbal Fluency Test and a Picture Description Test,
comparing the model's effectiveness across different speech production
contexts. Starting from a textual baseline, we investigate the effect of
incorporation of pause information and acoustic context. We show the test
should be chosen depending on the task, and similarly, lexical pause
information and acoustic cross-attention contribute differently.

摘要：語音停頓與內容和結構並列，為偵測失智症提供了有價值且非侵入性的生物標記。這項研究探討在基於轉換器的語言模型中使用富含停頓的轉錄，以根據臨床評估中受試者的語言區分出無認知障礙、輕度認知障礙和阿茲海默症失智症的認知狀態。我們處理三項二元分類任務：發病、監測和排除失智症。透過對德語流利口語測驗和圖片描述測驗進行實驗，評估效能，比較模型在不同語言產生脈絡中的有效性。從文字基準開始，我們探討加入停頓資訊和音響脈絡的效果。我們顯示測試應根據任務選擇，同樣地，語彙停頓資訊和音響交叉注意力的貢獻也不同。

##### **PoseWatch: A Transformer-based Architecture for Human-centric Video Anomaly Detection Using Spatio-temporal Pose Tokenization**
2408.15185v1 by Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi

Video Anomaly Detection (VAD) presents a significant challenge in computer
vision, particularly due to the unpredictable and infrequent nature of
anomalous events, coupled with the diverse and dynamic environments in which
they occur. Human-centric VAD, a specialized area within this domain, faces
additional complexities, including variations in human behavior, potential
biases in data, and substantial privacy concerns related to human subjects.
These issues complicate the development of models that are both robust and
generalizable. To address these challenges, recent advancements have focused on
pose-based VAD, which leverages human pose as a high-level feature to mitigate
privacy concerns, reduce appearance biases, and minimize background
interference. In this paper, we introduce PoseWatch, a novel transformer-based
architecture designed specifically for human-centric pose-based VAD. PoseWatch
features an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)
tokenization method that enhances the representation of human motion over time,
which is also beneficial for broader human behavior analysis tasks. The
architecture's core, a Unified Encoder Twin Decoders (UETD) transformer,
significantly improves the detection of anomalous behaviors in video data.
Extensive evaluations across multiple benchmark datasets demonstrate that
PoseWatch consistently outperforms existing methods, establishing a new
state-of-the-art in pose-based VAD. This work not only demonstrates the
efficacy of PoseWatch but also highlights the potential of integrating Natural
Language Processing techniques with computer vision to advance human behavior
analysis.

摘要：影片異常偵測 (VAD) 在電腦視覺中是一項重大挑戰，特別是因為異常事件的不可預測性和不頻繁性，加上它們發生的環境多樣且動態。以人為中心的 VAD 是此領域中的專門領域，面臨額外的複雜性，包括人類行為的變化、資料中的潛在偏差，以及與人類受試者相關的重大隱私問題。這些問題使得開發既強健又可概化的模型變得複雜。為了應對這些挑戰，最近的進展專注於基於姿勢的 VAD，它利用人類姿勢作為高級特徵來減輕隱私問題、減少外觀偏差，並將背景干擾降至最低。在本文中，我們介紹 PoseWatch，這是一種新穎的基於Transformer的架構，專門設計用於以人為中心的基於姿勢的 VAD。PoseWatch 採用創新的時空姿勢和相對姿勢 (ST-PRP) 標記化方法，增強了人類動作隨時間的表示，這也有利於更廣泛的人類行為分析任務。該架構的核心，一個統一編碼器雙解碼器 (UETD) Transformer，顯著改善了影片資料中異常行為的偵測。跨多個基準資料集的廣泛評估證明，PoseWatch 持續優於現有方法，在基於姿勢的 VAD 中建立了新的技術水準。這項工作不僅證明了 PoseWatch 的效能，也突出了將自然語言處理技術與電腦視覺整合以推進人類行為分析的潛力。

##### **Unlocking Potential in Pre-Trained Music Language Models for Versatile Multi-Track Music Arrangement**
2408.15176v1 by Longshen Ou, Jingwei Zhao, Ziyu Wang, Gus Xia, Ye Wang

Large language models have shown significant capabilities across various
domains, including symbolic music generation. However, leveraging these
pre-trained models for controllable music arrangement tasks, each requiring
different forms of musical information as control, remains a novel challenge.
In this paper, we propose a unified sequence-to-sequence framework that enables
the fine-tuning of a symbolic music language model for multiple multi-track
arrangement tasks, including band arrangement, piano reduction, drum
arrangement, and voice separation. Our experiments demonstrate that the
proposed approach consistently achieves higher musical quality compared to
task-specific baselines across all four tasks. Furthermore, through additional
experiments on probing analysis, we show the pre-training phase equips the
model with essential knowledge to understand musical conditions, which is hard
to acquired solely through task-specific fine-tuning.

摘要：大型語言模型已在各種領域展現出顯著的能力，包括符號音樂生成。然而，要利用這些預訓練模型進行可控音樂編排任務，每個任務都需要不同的音樂資訊作為控制，這仍然是一個新挑戰。在本文中，我們提出了一個統一的序列到序列框架，可以微調符號音樂語言模型以進行多個多軌編排任務，包括樂隊編排、鋼琴簡化、鼓編排和人聲分離。我們的實驗表明，與所有四項任務中特定於任務的基準相比，所提出的方法始終實現更高的音樂品質。此外，透過探測分析的額外實驗，我們展示了預訓練階段為模型提供了理解音樂條件的基本知識，而僅透過特定於任務的微調很難獲得這些知識。

##### **X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation**
2408.15172v1 by Hanjia Lyu, Ryan Rossi, Xiang Chen, Md Mehrab Tanjim, Stefano Petrangeli, Somdeb Sarkhel, Jiebo Luo

Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been
shown to enhance the effectiveness of enriching item descriptions, thereby
improving the accuracy of recommendation systems. However, most existing
approaches either rely on text-only prompting or employ basic multimodal
strategies that do not fully exploit the complementary information available
from both textual and visual modalities. This paper introduces a novel
framework, Cross-Reflection Prompting, termed X-Reflect, designed to address
these limitations by prompting LMMs to explicitly identify and reconcile
supportive and conflicting information between text and images. By capturing
nuanced insights from both modalities, this approach generates more
comprehensive and contextually richer item representations. Extensive
experiments conducted on two widely used benchmarks demonstrate that our method
outperforms existing prompting baselines in downstream recommendation accuracy.
Additionally, we evaluate the generalizability of our framework across
different LMM backbones and the robustness of the prompting strategies,
offering insights for optimization. This work underscores the importance of
integrating multimodal information and presents a novel solution for improving
item understanding in multimodal recommendation systems.

摘要：大型語言模型 (LLM) 和大型多模態模型 (LMM) 已被證明可以提高豐富商品說明的有效性，從而提高推薦系統的準確性。然而，現有的方法大多依賴於純文字提示，或採用基本的模態策略，無法充分利用文本和視覺模態中可用的互補資訊。本文介紹了一個創新的框架，稱為交叉反射提示 (X-Reflect)，旨在透過提示 LMM 明確找出和調和文本和影像之間的支持和衝突資訊來解決這些限制。透過擷取兩種模態的細微見解，此方法產生更全面且內容更豐富的商品表示。在兩個廣泛使用的基準上進行的廣泛實驗證明，我們的模型在下游推薦準確度方面優於現有的提示基準。此外，我們評估了我們的框架在不同 LMM 主幹中的概括性以及提示策略的穩健性，為最佳化提供見解。這項研究強調了整合多模態資訊的重要性，並提出了一種創新的解決方案，用於改善多模態推薦系統中的商品理解。

##### **Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation**
2408.15171v1 by N. E. Kriman

The use of large language models (LLMs) has significantly increased since the
introduction of ChatGPT in 2022, demonstrating their value across various
applications. However, a major challenge for enterprise and commercial adoption
of LLMs is their tendency to generate inaccurate information, a phenomenon
known as "hallucination." This project proposes a method for estimating the
factuality of a summary generated by LLMs when compared to a source text. Our
approach utilizes Naive Bayes classification to assess the accuracy of the
content produced.

摘要：自 2022 年 ChatGPT 推出以來，大型語言模型 (LLM) 的使用大幅增加，證明它們在各種應用中的價值。然而，企業和商業採用 LLM 的一個主要挑戰是它們傾向於產生不準確的資訊，這種現象稱為「幻覺」。本專案提出了一種方法，用於評估 LLM 生成的摘要與原始文字相比的事實性。我們的做法利用朴素貝氏分類來評估所產生內容的準確性。

##### **How transformers learn structured data: insights from hierarchical filtering**
2408.15138v1 by Jerome Garnier-Brun, Marc Mézard, Emanuele Moscato, Luca Saglietti

We introduce a hierarchical filtering procedure for generative models of
sequences on trees, enabling control over the range of positional correlations
in the data. Leveraging this controlled setting, we provide evidence that
vanilla encoder-only transformer architectures can implement the optimal Belief
Propagation algorithm on both root classification and masked language modeling
tasks. Correlations at larger distances corresponding to increasing layers of
the hierarchy are sequentially included as the network is trained. We analyze
how the transformer layers succeed by focusing on attention maps from models
trained with varying degrees of filtering. These attention maps show clear
evidence for iterative hierarchical reconstruction of correlations, and we can
relate these observations to a plausible implementation of the exact inference
algorithm for the network sizes considered.

摘要：我們引入了一個分層過濾程序，用於樹狀結構序列的生成模型，能夠控制資料中位置相關性的範圍。利用這個受控設定，我們提供了證據，表明僅編碼器的香草Transformer架構可以在根分類和遮罩語言建模任務中實作最佳的信念傳播演算法。隨著網路受訓，與層級中遞增層數對應的較大距離相關性會依序納入。我們分析Transformer層如何成功，重點關注使用不同程度過濾訓練的模型的注意力圖。這些注意力圖清楚地證明了相關性的迭代式分層重建，而且我們可以將這些觀察與所考慮的網路大小的精確推論演算法的合理實作相關聯。

##### **Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments**
2408.15128v1 by Charlotte Rodriguez, Laura Degioanni, Laetitia Kameni, Richard Vidal, Giovanni Neglia

Monitoring, understanding, and optimizing the energy consumption of Machine
Learning (ML) are various reasons why it is necessary to evaluate the energy
usage of ML. However, there exists no universal tool that can answer this
question for all use cases, and there may even be disagreement on how to
evaluate energy consumption for a specific use case. Tools and methods are
based on different approaches, each with their own advantages and drawbacks,
and they need to be mapped out and explained in order to select the most
suitable one for a given situation. We address this challenge through two
approaches. First, we conduct a systematic literature review of all tools and
methods that permit to evaluate the energy consumption of ML (both at training
and at inference), irrespective of whether they were originally designed for
machine learning or general software. Second, we develop and use an
experimental protocol to compare a selection of these tools and methods. The
comparison is both qualitative and quantitative on a range of ML tasks of
different nature (vision, language) and computational complexity. The
systematic literature review serves as a comprehensive guide for understanding
the array of tools and methods used in evaluating energy consumption of ML, for
various use cases going from basic energy monitoring to consumption
optimization. Two open-source repositories are provided for further
exploration. The first one contains tools that can be used to replicate this
work or extend the current review. The second repository houses the
experimental protocol, allowing users to augment the protocol with new ML
computing tasks and additional energy evaluation tools.

摘要：<paragraph>監控、了解和優化機器學習 (ML) 的能源消耗是評估 ML 能源使用的各種原因。然而，目前尚無萬能工具可以針對所有使用案例回答這個問題，甚至可能對於如何評估特定使用案例的能源消耗存在分歧。工具和方法基於不同的方法，各自有其優點和缺點，並且需要將它們繪製出來並加以說明，才能為特定情況選擇最合適的方法。我們透過兩種方法來解決這個挑戰。首先，我們對所有允許評估 ML 能源消耗（在訓練和推理時）的工具和方法進行系統性的文獻回顧，無論它們最初是為機器學習或一般軟體設計的。其次，我們開發並使用實驗協定來比較這些工具和方法的選項。比較在性質不同（視覺、語言）和計算複雜性範圍的 ML 任務上是定性和定量的。系統性的文獻回顧作為一個全面的指南，用於了解評估 ML 能源消耗所使用的工具和方法陣列，適用於從基本能源監控到消耗優化的各種使用案例。提供兩個開源儲存庫以供進一步探索。第一個包含可複製這項工作或擴展目前回顧的工具。第二個儲存庫容納實驗協定，允許使用者使用新的 ML 計算任務和額外的能源評估工具來擴充協定。</paragraph>

##### **Urdu Digital Text Word Optical Character Recognition Using Permuted Auto Regressive Sequence Modeling**
2408.15119v2 by Ahmed Mustafa, Muhammad Tahir Rafique, Muhammad Ijlal Baig, Hasan Sajid, Muhammad Jawad Khan, Karam Dad Kallu

This research paper presents a novel word-level Optical Character Recognition
(OCR) model developed specifically for digital Urdu text. The model utilizes
transformer-based architectures and attention mechanisms to address the unique
challenges of recognizing Urdu script, which includes handling a diverse range
of text styles, fonts, and variations. Trained on a comprehensive dataset of
approximately 160,000 Urdu text images, the model incorporates a permuted
autoregressive sequence (PARSeq) architecture. This design enables
context-aware inference and iterative refinement by leveraging bidirectional
context information, significantly enhancing its ability to accurately
recognize Urdu characters. The model achieves a character error rate (CER) of
0.178, highlighting its effectiveness and precision in real-world applications.
However, the model has some limitations, such as difficulties with blurred
images, non-horizontal orientations, and the presence of trailing punctuation
marks, which can introduce noise into the recognition process. Addressing these
challenges will be a key focus of future work. Future research will aim to
further refine the model through advanced data augmentation techniques,
optimization of hyperparameters, and the integration of context-aware language
models, ultimately enhancing the model's performance and robustness in Urdu
text recognition.

摘要：這篇研究論文提出了一個新穎的單字層級光學字元辨識 (OCR) 模型，專為數位烏爾都語文字而開發。此模型利用基於轉換器的架構和注意力機制來解決辨識烏爾都語文字的獨特挑戰，包括處理各種文字樣式、字體和變化。在約 160,000 張烏爾都語文字影像的綜合資料集上訓練後，此模型結合了一個排列的自動迴歸序列 (PARSeq) 架構。此設計透過利用雙向脈絡資訊，實現了脈絡感知推論和反覆精煉，大幅提升了其準確辨識烏爾都語字元的能。此模型達到了 0.178 的字元錯誤率 (CER)，突顯了其在實際應用中的效能和精準度。然而，此模型有一些限制，例如難以處理模糊影像、非水平方向和存在尾隨標點符號，這些因素會在辨識過程中產生雜訊。解決這些挑戰將是未來工作的重點。未來的研究將旨在透過進階資料擴充技術、超參數最佳化和整合脈絡感知語言模型，進一步精煉此模型，最終提升此模型在烏爾都語文字辨識中的效能和穩健性。

##### **Evaluating Stability of Unreflective Alignment**
2408.15116v1 by James Lucassen, Mark Henry, Philippa Wright, Owen Yeung

Many theoretical obstacles to AI alignment are consequences of reflective
stability - the problem of designing alignment mechanisms that the AI would not
disable if given the option. However, problems stemming from reflective
stability are not obviously present in current LLMs, leading to disagreement
over whether they will need to be solved to enable safe delegation of cognitive
labor. In this paper, we propose Counterfactual Priority Change (CPC)
destabilization as a mechanism by which reflective stability problems may arise
in future LLMs. We describe two risk factors for CPC-destabilization: 1)
CPC-based stepping back and 2) preference instability. We develop preliminary
evaluations for each of these risk factors, and apply them to frontier LLMs.
Our findings indicate that in current LLMs, increased scale and capability are
associated with increases in both CPC-based stepping back and preference
instability, suggesting that CPC-destabilization may cause reflective stability
problems in future LLMs.

摘要：許多理論上 AI 對齊的障礙是反射穩定性的後果，即設計 AI 在有選項時不會停用的對齊機制的難題。然而，源自反射穩定性的問題在目前的 LLM 中並不明顯，導致對是否需要解決這些問題以實現認知勞動的安全委派產生分歧。在本文中，我們提出反事實優先變更 (CPC) 去穩定化，作為反射穩定性問題可能在未來 LLM 中產生的機制。我們描述了 CPC 去穩定的兩個風險因素：1) 基於 CPC 的後退和 2) 偏好不穩定。我們針對這些風險因素中的每一個開發初步評估，並將它們應用於前沿 LLM。我們的研究結果表明，在目前的 LLM 中，規模和能力的增加與基於 CPC 的後退和偏好不穩定的增加有關，這表明 CPC 去穩定化可能會在未來的 LLM 中導致反射穩定性問題。

##### **MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders**
2408.15101v1 by Baijiong Lin, Weisen Jiang, Pengguang Chen, Shu Liu, Ying-Cong Chen

Multi-task dense scene understanding, which trains a model for multiple dense
prediction tasks, has a wide range of application scenarios. Capturing
long-range dependency and enhancing cross-task interactions are crucial to
multi-task dense prediction. In this paper, we propose MTMamba++, a novel
architecture for multi-task scene understanding featuring with a Mamba-based
decoder. It contains two types of core blocks: self-task Mamba (STM) block and
cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging
state-space models, while CTM explicitly models task interactions to facilitate
information exchange across tasks. We design two types of CTM block, namely
F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic
perspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and
Cityscapes datasets demonstrate the superior performance of MTMamba++ over
CNN-based and Transformer-based methods. The code is available at
https://github.com/EnVision-Research/MTMamba.

摘要：多任務稠密場景理解訓練模型執行多項稠密預測任務，具備廣泛的應用場景。捕捉長程依賴性和增強跨任務互動對於多任務稠密預測至關重要。在本文中，我們提出 MTMamba++，一種採用基於 Mamba 的解碼器，用於多任務場景理解的新穎架構。它包含兩種核心區塊：自任務 Mamba (STM) 區塊和跨任務 Mamba (CTM) 區塊。STM 透過利用狀態空間模型來處理長程依賴性，而 CTM 則明確建模任務互動以促進跨任務資訊交換。我們設計了兩種 CTM 區塊，分別是 F-CTM 和 S-CTM，以分別從特徵和語義角度增強跨任務互動。在 NYUDv2、PASCAL-Context 和 Cityscapes 資料集上的實驗證明了 MTMamba++ 優於基於 CNN 和基於 Transformer 的方法。程式碼可在 https://github.com/EnVision-Research/MTMamba 取得。

##### **Post-processing fairness with minimal changes**
2408.15096v1 by Federico Di Gennaro, Thibault Laugel, Vincent Grari, Xavier Renard, Marcin Detyniecki

In this paper, we introduce a novel post-processing algorithm that is both
model-agnostic and does not require the sensitive attribute at test time. In
addition, our algorithm is explicitly designed to enforce minimal changes
between biased and debiased predictions; a property that, while highly
desirable, is rarely prioritized as an explicit objective in fairness
literature. Our approach leverages a multiplicative factor applied to the logit
value of probability scores produced by a black-box classifier. We demonstrate
the efficacy of our method through empirical evaluations, comparing its
performance against other four debiasing algorithms on two widely used datasets
in fairness research.

摘要：在本文中，我們介紹了一種新穎的後處理演算法，它既與模型無關，也不需要在測試時提供敏感屬性。此外，我們的演算法明確設計用於強制偏差預測和非偏差預測之間的最小變更；這是一個特性，儘管非常需要，但很少在公平性文獻中作為明確目標優先考慮。我們的做法利用乘法因子，應用於黑箱分類器產生的機率分數的 logit 值。我們透過實證評估來證明我們方法的效力，比較它在公平性研究中兩個廣泛使用的資料集上，與其他四種去偏差演算法的效能。

##### **Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models**
2408.15091v1 by Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, Weiping Wang

The storage and recall of factual associations in auto-regressive transformer
language models (LMs) have drawn a great deal of attention, inspiring knowledge
editing by directly modifying the located model weights. Most editing works
achieve knowledge editing under the guidance of existing interpretations of
knowledge recall that mainly focus on subject knowledge. However, these
interpretations are seriously flawed, neglecting relation information and
leading to the over-generalizing problem for editing. In this work, we discover
a novel relation-focused perspective to interpret the knowledge recall of
transformer LMs during inference and apply it on knowledge editing to avoid
over-generalizing. Experimental results on the dataset supplemented with a new
R-Specificity criterion demonstrate that our editing approach significantly
alleviates over-generalizing while remaining competitive on other criteria,
breaking the domination of subject-focused editing for future research.

摘要：自回归转换器语言模型 (LM) 中事实关联的存储和召回引起了极大的关注，激发了通过直接修改定位的模型权重来编辑知识。大多数编辑工作在主要关注主体知识的现有知识召回解释的指导下实现知识编辑。然而，这些解释存在严重缺陷，忽略了关系信息并导致编辑的过度概括问题。在这项工作中，我们发现了一个新的以关系为中心的视角来解释转换器 LM 在推理过程中的知识召回，并将其应用于知识编辑以避免过度概括。在补充了新的 R 特异性标准的数据集上进行的实验结果表明，我们的编辑方法显着减轻了过度概括，同时在其他标准上保持竞争力，打破了以主题为中心的编辑对未来研究的主导地位。

##### **BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline**
2408.15079v1 by Guosheng Dong, Da Pan, Yiding Sun, Shusen Zhang, Zheng Liang, Xin Wu, Yanjun Shen, Fan Yang, Haoze Sun, Tianpeng Li, Mingan Lin, Jianhua Xu, Yufan Zhang, Xiaonan Nie, Lei Su, Bingning Wang, Wentao Zhang, Jiaxin Mao, Zenan Zhou, Weipeng Chen

The general capabilities of Large Language Models (LLM) highly rely on the
composition and selection on extensive pretraining datasets, treated as
commercial secrets by several institutions. To mitigate this issue, we
open-source the details of a universally applicable data processing pipeline
and validate its effectiveness and potential by introducing a competitive LLM
baseline. Specifically, the data processing pipeline consists of broad
collection to scale up and reweighting to improve quality. We then pretrain a
7B model BaichuanSEED with 3T tokens processed by our pipeline without any
deliberate downstream task-related optimization, followed by an easy but
effective supervised fine-tuning stage. BaichuanSEED demonstrates consistency
and predictability throughout training and achieves comparable performance on
comprehensive benchmarks with several commercial advanced large language
models, such as Qwen1.5 and Llama3. We also conduct several heuristic
experiments to discuss the potential for further optimization of downstream
tasks, such as mathematics and coding.

摘要：大型語言模型 (LLM) 的一般功能高度依賴於廣泛預訓練數據集的組成和選擇，而這些數據集被多家機構視為商業機密。為了減輕此問題，我們公開了普遍適用的資料處理管線的詳細資訊，並透過引入有競爭力的 LLM 基準來驗證其有效性和潛力。具體來說，資料處理管線包括廣泛收集以擴大規模和重新加權以提高品質。然後，我們使用我們的管線處理的 3T 個代幣，預訓練一個 7B 模型 BaichuanSEED，而沒有任何故意的下游任務相關最佳化，接著是一個簡單但有效的監督微調階段。BaichuanSEED 在整個訓練過程中展現了一致性和可預測性，並在全面的基準測試中達到了與多個商業進階大型語言模型（例如 Qwen1.5 和 Llama3）相當的效能。我們還進行了多項啟發式實驗，以探討進一步最佳化下游任務（例如數學和編碼）的潛力。

##### **MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of Children with Autism Spectrum Disorder**
2408.15077v1 by Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki

Autism spectrum disorder (ASD) is characterized by significant challenges in
social interaction and comprehending communication signals. Recently,
therapeutic interventions for ASD have increasingly utilized Deep learning
powered-computer vision techniques to monitor individual progress over time.
These models are trained on private, non-public datasets from the autism
community, creating challenges in comparing results across different models due
to privacy-preserving data-sharing issues. This work introduces MMASD+. MMASD+
consists of diverse data modalities, including 3D-Skeleton, 3D Body Mesh, and
Optical Flow data. It integrates the capabilities of Yolov8 and Deep SORT
algorithms to distinguish between the therapist and children, addressing a
significant barrier in the original dataset. Additionally, a Multimodal
Transformer framework is proposed to predict 11 action types and the presence
of ASD. This framework achieves an accuracy of 95.03% for predicting action
types and 96.42% for predicting ASD presence, demonstrating over a 10%
improvement compared to models trained on single data modalities. These
findings highlight the advantages of integrating multiple data modalities
within the Multimodal Transformer framework.

摘要：自閉症譜系障礙 (ASD) 的特徵在於社交互動和理解溝通信號方面有顯著的挑戰。最近，自閉症譜系障礙的治療干預措施越來越多地利用深度學習驅動的電腦視覺技術來監控個人隨著時間的進展。這些模型是在自閉症社群的私人非公開數據集上訓練的，由於保護隱私的數據共享問題，這造成比較不同模型的結果時出現挑戰。這項工作引入了 MMASD+。MMASD+ 包含多樣化的數據模式，包括 3D 骨架、3D 身體網格和光流數據。它整合了 Yolov8 和 Deep SORT 演算法的能力，以區分治療師和兒童，解決原始數據集中的一個重大障礙。此外，還提出了多模式轉換器架構來預測 11 種動作類型和自閉症譜系障礙的存在。此架構在預測動作類型方面達到了 95.03% 的準確度，在預測自閉症譜系障礙的存在方面達到了 96.42%，與在單一數據模式上訓練的模型相比，準確度提高了 10% 以上。這些發現突顯了在多模式轉換器架構中整合多個數據模式的優點。

##### **Interactive dense pixel visualizations for time series and model attribution explanations**
2408.15073v1 by Udo Schlegel, Daniel A. Keim

The field of Explainable Artificial Intelligence (XAI) for Deep Neural
Network models has developed significantly, offering numerous techniques to
extract explanations from models. However, evaluating explanations is often not
trivial, and differences in applied metrics can be subtle, especially with
non-intelligible data. Thus, there is a need for visualizations tailored to
explore explanations for domains with such data, e.g., time series. We propose
DAVOTS, an interactive visual analytics approach to explore raw time series
data, activations of neural networks, and attributions in a dense-pixel
visualization to gain insights into the data, models' decisions, and
explanations. To further support users in exploring large datasets, we apply
clustering approaches to the visualized data domains to highlight groups and
present ordering strategies for individual and combined data exploration to
facilitate finding patterns. We visualize a CNN trained on the FordA dataset to
demonstrate the approach.

摘要：可解釋人工智慧 (XAI) 領域已針對深度神經網路模型大幅發展，提供多種技術從模型中提取解釋。然而，評估解釋通常並非易事，而所套用的指標之間的差異可能很細微，尤其是在處理不可理解的資料時。因此，需要量身打造視覺化功能，以探索此類資料的領域之解釋，例如時間序列。我們提出 DAVOTS，這是一種互動式視覺分析方法，用於探索原始時間序列資料、神經網路的激活，以及密集像素視覺化中的歸因，以深入了解資料、模型決策和解釋。為了進一步協助使用者探索大型資料集，我們將群集方法套用至視覺化資料領域，以突顯群組，並提出個人和合併資料探索的排序策略，以利於尋找模式。我們視覺化在 FordA 資料集上訓練的 CNN，以展示此方法。

##### **Causal Rule Forest: Toward Interpretable and Precise Treatment Effect Estimation**
2408.15055v1 by Chan Hsu, Jun-Ting Wu, Yihuang Kang

Understanding and inferencing Heterogeneous Treatment Effects (HTE) and
Conditional Average Treatment Effects (CATE) are vital for developing
personalized treatment recommendations. Many state-of-the-art approaches
achieve inspiring performance in estimating HTE on benchmark datasets or
simulation studies. However, the indirect predicting manner and complex model
architecture reduce the interpretability of these approaches. To mitigate the
gap between predictive performance and heterogeneity interpretability, we
introduce the Causal Rule Forest (CRF), a novel approach to learning hidden
patterns from data and transforming the patterns into interpretable multi-level
Boolean rules. By training the other interpretable causal inference models with
data representation learned by CRF, we can reduce the predictive errors of
these models in estimating HTE and CATE, while keeping their interpretability
for identifying subgroups that a treatment is more effective. Our experiments
underscore the potential of CRF to advance personalized interventions and
policies, paving the way for future research to enhance its scalability and
application across complex causal inference challenges.

摘要：了解與推論異質處理效果 (HTE) 和條件平均處理效果 (CATE) 對開發個人化處理建議至關重要。許多最先進的方法在基準資料集或模擬研究中估計 HTE 時都能達到令人振奮的效能。然而，間接預測方式和複雜的模型架構降低了這些方法的可解釋性。為了縮小預測效能和異質性可解釋性之間的差距，我們引入了因果規則森林 (CRF)，這是一種從資料中學習隱藏模式並將模式轉換為可解釋的多層布林規則的新方法。透過使用 CRF 學習的資料表示訓練其他可解釋的因果推論模型，我們可以減少這些模型在估計 HTE 和 CATE 時的預測誤差，同時保留其可解釋性以識別治療更有效的子群。我們的實驗強調了 CRF 在推進個人化干預和政策方面的潛力，為未來研究增強其可擴充性和在複雜因果推論挑戰中的應用鋪平了道路。

##### **Self-supervised Topic Taxonomy Discovery in the Box Embedding Space**
2408.15050v1 by Yuyin Lu, Hegang Chen, Pengbo Mao, Yanghui Rao, Haoran Xie, Fu Lee Wang, Qing Li

Topic taxonomy discovery aims at uncovering topics of different abstraction
levels and constructing hierarchical relations between them. Unfortunately,
most of prior work can hardly model semantic scopes of words and topics by
holding the Euclidean embedding space assumption. What's worse, they infer
asymmetric hierarchical relations by symmetric distances between topic
embeddings. As a result, existing methods suffer from problems of low-quality
topics at high abstraction levels and inaccurate hierarchical relations. To
alleviate these problems, this paper develops a Box embedding-based Topic Model
(BoxTM) that maps words and topics into the box embedding space, where the
asymmetric metric is defined to properly infer hierarchical relations among
topics. Additionally, our BoxTM explicitly infers upper-level topics based on
correlation between specific topics through recursive clustering on topic
boxes. Finally, extensive experiments validate high-quality of the topic
taxonomy learned by BoxTM.

摘要：主題分類發現旨在揭示不同抽象層級的主題，並建構它們之間的階層關係。不幸的是，大多數先前的研究難以透過維持歐幾里得嵌入空間假設來建模詞彙和主題的語義範圍。更糟糕的是，它們透過主題嵌入之間的對稱距離來推斷非對稱的階層關係。因此，現有方法存在高抽象層級低品質主題和不準確階層關係的問題。為了減輕這些問題，本文開發了一個基於方塊嵌入的主題模型（BoxTM），將詞彙和主題對應到方塊嵌入空間中，其中定義了非對稱量度以適當地推斷主題之間的階層關係。此外，我們的 BoxTM 透過主題方塊上的遞迴聚類，根據特定主題之間的關聯性明確推斷上層主題。最後，廣泛的實驗驗證了 BoxTM 所學習的主題分類的高品質。

##### **A Survey of Large Language Models for European Languages**
2408.15040v2 by Wazir Ali, Sampo Pyysalo

Large Language Models (LLMs) have gained significant attention due to their
high performance on a wide range of natural language tasks since the release of
ChatGPT. The LLMs learn to understand and generate language by training
billions of model parameters on vast volumes of text data. Despite being a
relatively new field, LLM research is rapidly advancing in various directions.
In this paper, we present an overview of LLM families, including LLaMA, PaLM,
GPT, and MoE, and the methods developed to create and enhance LLMs for official
European Union (EU) languages. We provide a comprehensive summary of common
monolingual and multilingual datasets used for pretraining large language
models.

摘要：大型語言模型（LLM）自 ChatGPT 發布以來，由於在廣泛自然語言任務上的高性能而備受關注。LLM 透過在大量文本數據上訓練數十億個模型參數，學習理解和生成語言。儘管 LLM 研究是一個相對較新的領域，但它正在各個方向迅速推進。在本文中，我們概述了 LLM 家族，包括 LLaMA、PaLM、GPT 和 MoE，以及為歐盟（EU）官方語言創建和增強 LLM 而開發的方法。我們提供了對用於預訓練大型語言模型的常見單語和多語數據集的全面摘要。

##### **Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering**
2408.15037v1 by Haowei Du, Huishuai Zhang, Dongyan Zhao

To address the hallucination in generative question answering (GQA) where the
answer can not be derived from the document, we propose a novel
evidence-enhanced triplet generation framework, EATQA, encouraging the model to
predict all the combinations of (Question, Evidence, Answer) triplet by
flipping the source pair and the target label to understand their logical
relationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a
QE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap
to distill the knowledge from evidence in inference stage. Our framework
ensures the model to learn the logical relation between query, evidence and
answer, which simultaneously improves the evidence generation and query
answering. In this paper, we apply EATQA to LLama and it outperforms other
LLMs-based methods and hallucination mitigation approaches on two challenging
GQA benchmarks. Further analysis shows that our method not only keeps prior
knowledge within LLM, but also mitigates hallucination and generates faithful
answers.

摘要：為了解決生成式問答 (GQA) 中的幻覺問題，其中無法從文件中推導出答案，我們提出了一個新穎的證據增強三元組生成框架 EATQA，鼓勵模型通過翻轉源對和目標標籤來預測所有 (問題、證據、答案) 三元組的組合，以了解它們的邏輯關係，即預測給定 QE、EA 和 QA 對的答案 (A)、問題 (Q) 和證據 (E)。此外，我們彌合了分佈差距，以在推理階段從證據中提取知識。我們的框架確保模型學習查詢、證據和答案之間的邏輯關係，這同時改進了證據生成和查詢回答。在本文中，我們將 EATQA 應用於 LLama，它在兩個具有挑戰性的 GQA 基準上優於其他基於 LLM 的方法和幻覺緩解方法。進一步的分析表明，我們的模型不僅保留了 LLM 中的先驗知識，而且還減輕了幻覺並生成了真實的答案。

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

摘要：<paragraph>計算病理學 (CPath) 已顯著提升病理學的臨床實務。儘管已有進展，作為 CPath 中一個有前途的範例，多重實例學習 (MIL) 持續面臨挑戰，特別是與不完整資訊使用有關。現有的架構，例如基於卷積神經網路 (CNN)、注意力和選擇性掃描空間狀態序列模型 (SSM) 的架構，在融合各種特徵時缺乏足夠的彈性和可擴充性，且無法有效融合各種特徵。此外，目前的作法並未充分利用與順序相關和與順序無關的特徵，導致序列資訊使用率不佳。為了解決這些限制，我們提出一個名為 Mamba2MIL 的新 MIL 架構。我們的架構利用狀態空間對偶模型 (SSD) 來建模全幻燈片影像 (WSI) 的長序列貼片，這與加權特徵選取結合使用，支援更多分支特徵的融合處理，且可根據特定應用需求進行延伸。此外，我們引入一種針對不同 WSI 大小量身打造的序列轉換方法，這增強了與序列無關的特徵，同時保留了局部序列資訊，進而改善序列資訊使用率。廣泛的實驗證明 Mamba2MIL 超越了最先進的 MIL 方法。我們在多個資料集上進行廣泛的實驗，在幾乎所有效能指標上均獲得改善。特別是在 NSCLC 資料集上，Mamba2MIL 達到 0.9533 的二元腫瘤分類 AUC 和 0.8794 的準確度。在 BRACS 資料集上，它達到 0.7986 的多類別分類 AUC 和 0.4981 的準確度。程式碼可在 https://github.com/YuqiZhang-Buaa/Mamba2MIL 取得。</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

摘要：<paragraph>心臟超音波探頭引導旨在幫助新手調整 6-DOF 探頭姿勢，以取得高品質的斷面影像。心臟超音波面臨兩項主要挑戰：(1) 心臟結構複雜且固有，以及 (2) 個體差異顯著。先前的研究僅學習了整體平均的 2D 和 3D 心臟結構，而非個人化的解剖特徵，導致效能瓶頸。臨床上，我們觀察到超音波技師會根據先前的掃描序列調整他們對患者心臟結構的理解，進而修改他們的掃描策略。受到此啟發，我們提出一個具序列感知的自監督預訓練方法。具體來說，我們的做法透過預測掃描序列中遮罩的影像和動作，來學習個人化的 2D 和 3D 心臟解剖特徵。我們假設，如果模型可以預測遺漏的內容，它便對個人化的解剖結構有了良好的理解。在下游的探頭引導任務中，我們也導入一個序列建模方法，該方法根據歷史掃描資料中的影像和動作，模擬個別心臟解剖資訊，進而做出更精確的導航決策。在有 136 萬個樣本的大規模資料集上進行的實驗證明，我們提出的具序列感知的典範可以大幅減少導航錯誤，其中平移錯誤減少了 15.90% 至 36.87%，旋轉錯誤減少了 11.13% 至 20.77%，與最先進的方法相比。</paragraph>

##### **Speech Recognition Transformers: Topological-lingualism Perspective**
2408.14991v1 by Shruti Singh, Muskaan Singh, Virender Kadyan

Transformers have evolved with great success in various artificial
intelligence tasks. Thanks to our recent prevalence of self-attention
mechanisms, which capture long-term dependency, phenomenal outcomes in speech
processing and recognition tasks have been produced. The paper presents a
comprehensive survey of transformer techniques oriented in speech modality. The
main contents of this survey include (1) background of traditional ASR,
end-to-end transformer ecosystem, and speech transformers (2) foundational
models in a speech via lingualism paradigm, i.e., monolingual, bilingual,
multilingual, and cross-lingual (3) dataset and languages, acoustic features,
architecture, decoding, and evaluation metric from a specific topological
lingualism perspective (4) popular speech transformer toolkit for building
end-to-end ASR systems. Finally, highlight the discussion of open challenges
and potential research directions for the community to conduct further research
in this domain.

摘要：變形金剛在各種人工智慧任務中已成功演進。感謝我們最近盛行的自注意力機制，它能擷取長期依賴性，已產生語音處理和辨識任務中的驚人成果。本文提供以語音模式為導向的變形金剛技術全面調查。此調查的主要內容包括：(1) 傳統 ASR 的背景、端對端變形金剛生態系統和語音變形金剛 (2) 語言學範例中的基礎模型，即單語、雙語、多語和跨語言 (3) 特定拓撲語言學觀點的資料集和語言、音響特徵、架構、解碼和評估指標 (4) 用於建立端對端 ASR 系統的熱門語音變形金剛工具包。最後，重點討論開放挑戰和潛在的研究方向，供社群在這個領域進行進一步研究。

##### **Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for Long-Tailed Continual Learning**
2408.14976v1 by Lei Liu, Li Liu, Yawen Cui

Even in the era of large models, one of the well-known issues in continual
learning (CL) is catastrophic forgetting, which is significantly challenging
when the continual data stream exhibits a long-tailed distribution, termed as
Long-Tailed Continual Learning (LTCL). Existing LTCL solutions generally
require the label distribution of the data stream to achieve re-balance
training. However, obtaining such prior information is often infeasible in real
scenarios since the model should learn without pre-identifying the majority and
minority classes. To this end, we propose a novel Prior-free Balanced Replay
(PBR) framework to learn from long-tailed data stream with less forgetting.
Concretely, motivated by our experimental finding that the minority classes are
more likely to be forgotten due to the higher uncertainty, we newly design an
uncertainty-guided reservoir sampling strategy to prioritize rehearsing
minority data without using any prior information, which is based on the mutual
dependence between the model and samples. Additionally, we incorporate two
prior-free components to further reduce the forgetting issue: (1) Boundary
constraint is to preserve uncertain boundary supporting samples for continually
re-estimating task boundaries. (2) Prototype constraint is to maintain the
consistency of learned class prototypes along with training. Our approach is
evaluated on three standard long-tailed benchmarks, demonstrating superior
performance to existing CL methods and previous SOTA LTCL approach in both
task- and class-incremental learning settings, as well as ordered- and
shuffled-LTCL settings.

摘要：<paragraph>即使在大模型時代，持續學習 (CL) 中一個眾所周知的問題是災難性遺忘，當持續數據流呈現長尾分佈（稱為長尾持續學習 (LTCL)）時，這將帶來極大的挑戰。現有的 LTCL 解决方案通常需要數據流的標籤分佈來實現重新平衡訓練。然而，在實際場景中，由於模型應該在不預先識別多數類和少數類的情況下學習，因此通常無法獲得此類先驗信息。為此，我們提出了一種新穎的無先驗平衡重播 (PBR) 框架，以從長尾數據流中學習，減少遺忘。具體來說，受我們的實驗發現的啟發，即少數類由於不確定性較高而更有可能被遺忘，我們新設計了一種不確定性引導的儲存器採樣策略，以優先排練少數數據，而無需使用任何先驗信息，這基於模型和樣本之間的相互依賴性。此外，我們整合了兩個無先驗組件以進一步減少遺忘問題：(1) 邊界約束是用於保留不確定的邊界支持樣本，以持續重新估計任務邊界。(2) 原型約束是用於維護學習的類原型在訓練過程中的連貫性。我們的做法在三個標準長尾基準上進行了評估，證明了其在任務和類增量學習設置以及有序和混洗 LTCL 設置中均優於現有的 CL 方法和先前的 SOTA LTCL 方法的卓越性能。</paragraph>

##### **AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems**
2408.14972v1 by Chi-Min Chan, Jianxuan Yu, Weize Chen, Chunyang Jiang, Xinyu Liu, Weijie Shi, Zhiyuan Liu, Wei Xue, Yike Guo

The rapid advancement of large language models (LLMs) has led to the rise of
LLM-based agents. Recent research shows that multi-agent systems (MAS), where
each agent plays a specific role, can outperform individual LLMs. However,
configuring an MAS for a task remains challenging, with performance only
observable post-execution. Inspired by scaling laws in LLM development, we
investigate whether MAS performance can be predicted beforehand. We introduce
AgentMonitor, a framework that integrates at the agent level to capture inputs
and outputs, transforming them into statistics for training a regression model
to predict task performance. Additionally, it can further apply real-time
corrections to address security risks posed by malicious agents, mitigating
negative impacts and enhancing MAS security. Experiments demonstrate that an
XGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in
more challenging scenarios. Furthermore, using AgentMonitor reduces harmful
content by 6.2% and increases helpful content by 1.8% on average, enhancing
safety and reliability. Code is available at
\url{https://github.com/chanchimin/AgentMonitor}.

摘要：大型語言模型 (LLM) 的快速進展導致了基於 LLM 的代理的興起。最近的研究表明，多代理系統 (MAS)，其中每個代理扮演特定角色，可以勝過個別的 LLM。然而，為任務配置 MAS 仍然具有挑戰性，因為只有在執行後才能觀察到效能。受 LLM 開發中規模定律的啟發，我們研究了 MAS 效能是否可以事先預測。我們引入了 AgentMonitor，一個整合在代理層級的架構，用於擷取輸入和輸出，將它們轉換成用於訓練回歸模型以預測任務效能的統計資料。此外，它還可以進一步應用即時修正，以解決惡意代理所造成的安全風險，減輕負面影響並增強 MAS 安全性。實驗表明，XGBoost 模型在域內達到 0.89 的 Spearman 相關性，在更具挑戰性的場景中達到 0.58。此外，使用 AgentMonitor 平均減少了 6.2% 的有害內容，增加了 1.8% 的有用內容，增強了安全性與可靠性。程式碼可在 \url{https://github.com/chanchimin/AgentMonitor} 取得。

##### **CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task**
2408.14961v1 by Lingyun Huang, Jianxu Mao, Yaonan Wang, Junfei Yi, Ziming Tao

In recent years, the rapid expansion of model sizes has led to large-scale
pre-trained models demonstrating remarkable capabilities. Consequently, there
has been a trend towards increasing the scale of models. However, this trend
introduces significant challenges, including substantial computational costs of
training and transfer to downstream tasks. To address these issues,
Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced. These
methods optimize large-scale pre-trained models for specific tasks by
fine-tuning a select group of parameters. Among these PEFT methods,
adapter-based and prompt-based methods are the primary techniques.
Specifically, in the field of visual fine-tuning, adapters gain prominence over
prompts because of the latter's relatively weaker performance and efficiency.
Under the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)
method, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates
cross-attention between the prompt tokens and the embedded tokens, which allows
us to compute the semantic relationship between them and conduct the
fine-tuning of models exactly to adapt visual tasks better. Furthermore, we
introduce the weight-sharing mechanism to initialize the parameters of
cross-attention, which avoids massive learnable parameters from cross-attention
and enhances the representative capability of cross-attention. We conduct
comprehensive testing across 25 datasets and the result indicates that CVPT
significantly improves VPT's performance and efficiency in visual tasks. For
example, on the VTAB-1K benchmark, CVPT outperforms VPT over 4% in average
accuracy, rivaling the advanced adapter-based methods in performance and
efficiency. Our experiments confirm that prompt-based methods can achieve
exceptional results in visual fine-tuning.

摘要：近年来，模型规模的快速扩展导致大规模预训练模型展现出非凡的能力。因此，出现了扩大模型规模的趋势。然而，这一趋势带来了重大挑战，包括训练和迁移到下游任务的巨大计算成本。为了解决这些问题，引入了参数高效微调 (PEFT) 方法。这些方法通过微调选定的参数组来优化特定任务的大规模预训练模型。在这些 PEFT 方法中，基于适配器和基于提示的方法是主要技术。具体来说，在视觉微调领域，适配器比提示更突出，因为后者的性能和效率相对较弱。在这种情况下，我们改进了广泛使用的视觉提示微调 (VPT) 方法，提出了交叉视觉提示微调 (CVPT)。CVPT 计算提示标记和嵌入标记之间的交叉注意力，这使我们能够计算它们之间的语义关系并准确地进行模型微调以更好地适应视觉任务。此外，我们引入了权重共享机制来初始化交叉注意力的参数，这避免了交叉注意力的大量可学习参数，并增强了交叉注意力的表示能力。我们在 25 个数据集上进行了全面测试，结果表明 CVPT 显着提高了 VPT 在视觉任务中的性能和效率。例如，在 VTAB-1K 基准测试中，CVPT 在平均准确度上比 VPT 高出 4%，在性能和效率上与基于高级适配器的方法相媲美。我们的实验证实，基于提示的方法可以在视觉微调中取得非凡的成果。

##### **Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress**
2408.14960v1 by Ayomide Odumakinde, Daniel D'souza, Pat Verga, Beyza Ermis, Sara Hooker

The use of synthetic data has played a critical role in recent state-of-art
breakthroughs. However, overly relying on a single oracle teacher model to
generate data has been shown to lead to model collapse and invite propagation
of biases. These limitations are particularly evident in multilingual settings,
where the absence of a universally effective teacher model that excels across
all languages presents significant challenges. In this work, we address these
extreme difference by introducing "multilingual arbitrage", which capitalizes
on performance variations between multiple models for a given language. To do
so, we strategically route samples through a diverse pool of models, each with
unique strengths in different languages. Across exhaustive experiments on
state-of-art models, our work suggests that arbitrage techniques allow for
spectacular gains in performance that far outperform relying on a single
teacher. In particular, compared to the best single teacher, we observe gains
of up to 56.5% improvement in win rates averaged across all languages when
switching to multilingual arbitrage. We observe the most significant gains for
the least resourced languages in our pool.

摘要：合成資料的使用在最近的技術突破中扮演了關鍵角色。然而，過度依賴單一的預言家教師模型來產生資料已被證明會導致模型崩潰並引發偏差的傳播。這些限制在多語言環境中特別明顯，在多語言環境中，缺乏一個普遍有效的教師模型，無法在所有語言中表現出色，這構成了重大挑戰。在這項工作中，我們通過引入「多語言套利」來解決這些極端的差異，該套利利用了針對特定語言的多個模型之間的效能差異。為此，我們策略性地透過一個多樣化的模型池路由樣本，每個模型在不同的語言中都有獨特的優勢。在對最先進模型進行的詳盡實驗中，我們的研究表明，套利技術可以顯著提升效能，遠遠優於依賴單一教師。特別是，與最佳單一教師相比，當切換到多語言套利時，我們觀察到在所有語言的平均勝率中獲得了高達 56.5% 的提升。我們在我們的池中資源最少的語言中觀察到了最顯著的收益。

##### **NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework**
2408.14950v1 by Shuangchen Zhao, Changde Du, Hui Li, Huiguang He

Deep Neural Networks (DNNs) have demonstrated exceptional recognition
capabilities in traditional computer vision (CV) tasks. However, existing CV
models often suffer a significant decrease in accuracy when confronted with
out-of-distribution (OOD) data. In contrast to these DNN models, human can
maintain a consistently low error rate when facing OOD scenes, partly
attributed to the rich prior cognitive knowledge stored in the human brain.
Previous OOD generalization researches only focus on the single modal,
overlooking the advantages of multimodal learning method. In this paper, we
utilize the multimodal learning method to improve the OOD generalization and
propose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the
cross-attention mechanism to fuse the visual knowledge from CV model and prior
cognitive knowledge from the human brain. Specially, we employ a pre-trained
visual neural encoding model to predict the functional Magnetic Resonance
Imaging (fMRI) from visual features which eliminates the need for the fMRI data
collection and pre-processing, effectively reduces the workload associated with
conventional BMFL methods. Furthermore, we construct a brain transformer to
facilitate the extraction of knowledge inside the fMRI data. Moreover, we
introduce the Pearson correlation coefficient maximization regularization
method into the training process, which improves the fusion capability with
better constrains. Our model outperforms the DINOv2 and baseline models on the
ImageNet-1k validation dataset as well as six curated OOD datasets, showcasing
its superior performance in diverse scenarios.

摘要：深度神经網路 (DNN) 已在傳統電腦視覺 (CV) 任務中展現出非凡的辨識能力。然而，現有的 CV 模型在面對分布外 (OOD) 資料時，其準確度通常會大幅降低。與這些 DNN 模型相反的是，人類在面對 OOD 場景時，可以維持持續低錯誤率，這部分歸功於儲存在人腦中的豐富先驗認知知識。先前的 OOD 泛化研究僅專注於單一模式，而忽略了多模態學習方法的優點。在本文中，我們利用多模態學習方法來改善 OOD 泛化，並提出了一種新穎的大腦機器融合學習 (BMFL) 架構。我們採用交叉注意力機制，融合來自 CV 模型的視覺知識和來自人腦的先驗認知知識。特別是，我們採用預先訓練好的視覺神經編碼模型，從視覺特徵預測功能性磁振造影 (fMRI)，這消除了對 fMRI 資料收集和預處理的需求，有效地減少了與傳統 BMFL 方法相關的工作負擔。此外，我們建構了一個大腦轉換器，以利於從 fMRI 資料中萃取知識。此外，我們將皮爾森相關係數最大化正則化方法引入訓練過程中，這透過更好的約束改善了融合能力。我們的模型在 ImageNet-1k 驗證資料集以及六個策展的 OOD 資料集上優於 DINOv2 和基線模型，展示了其在不同場景中的優異效能。

##### **Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures**
2408.14935v1 by Tomi Silander, Janne Leppä-aho, Elias Jääsaari, Teemu Roos

We introduce an information theoretic criterion for Bayesian network
structure learning which we call quotient normalized maximum likelihood (qNML).
In contrast to the closely related factorized normalized maximum likelihood
criterion, qNML satisfies the property of score equivalence. It is also
decomposable and completely free of adjustable hyperparameters. For practical
computations, we identify a remarkably accurate approximation proposed earlier
by Szpankowski and Weinberger. Experiments on both simulated and real data
demonstrate that the new criterion leads to parsimonious models with good
predictive accuracy.

摘要：<paragraph>我們引入一個資訊理論準則，用於貝氏網路結構學習，我們稱之為商數標準化最大似然（qNML）。
與密切相關的因式化標準化最大似然準則相比，qNML 滿足分數等價的特性。它也是可分解的，並且完全沒有可調整的超參數。對於實際計算，我們識別出 Szpankowski 和 Weinberger 早先提出的非常準確的近似值。對模擬和真實數據的實驗證明，新的準則會產生具有良好預測精度的簡約模型。</paragraph>

##### **Distance-Forward Learning: Enhancing the Forward-Forward Algorithm Towards High-Performance On-Chip Learning**
2408.14925v1 by Yujie Wu, Siyuan Xu, Jibin Wu, Lei Deng, Mingkun Xu, Qinghao Wen, Guoqi Li

The Forward-Forward (FF) algorithm was recently proposed as a local learning
method to address the limitations of backpropagation (BP), offering biological
plausibility along with memory-efficient and highly parallelized computational
benefits. However, it suffers from suboptimal performance and poor
generalization, largely due to inadequate theoretical support and a lack of
effective learning strategies. In this work, we reformulate FF using distance
metric learning and propose a distance-forward algorithm (DF) to improve FF
performance in supervised vision tasks while preserving its local computational
properties, making it competitive for efficient on-chip learning. To achieve
this, we reinterpret FF through the lens of centroid-based metric learning and
develop a goodness-based N-pair margin loss to facilitate the learning of
discriminative features. Furthermore, we integrate layer-collaboration local
update strategies to reduce information loss caused by greedy local parameter
updates. Our method surpasses existing FF models and other advanced local
learning approaches, with accuracies of 99.7\% on MNIST, 88.2\% on CIFAR-10,
59\% on CIFAR-100, 95.9\% on SVHN, and 82.5\% on ImageNette, respectively.
Moreover, it achieves comparable performance with less than 40\% memory cost
compared to BP training, while exhibiting stronger robustness to multiple types
of hardware-related noise, demonstrating its potential for online learning and
energy-efficient computation on neuromorphic chips.

摘要：前饋-前饋 (FF) 演算法最近被提出作為一種局部學習方法，用於解決反向傳播 (BP) 的限制，提供生物學上的合理性，以及記憶體使用率高且高度平行化的運算優點。然而，它會出現次佳效能和差勁的概化能力，這在很大程度上是因為理論支援不足，以及缺乏有效的學習策略。在這項工作中，我們使用距離度量學習重新制定 FF，並提出距離前饋演算法 (DF) 以改善 FF 在監督式視覺任務中的效能，同時保留其局部運算特性，使其具有競爭力，可進行高效的晶片上學習。為達成此目的，我們透過基於質心的度量學習重新詮釋 FF，並開發出一個基於優良度的 N 對邊界損失，以促進判別特徵的學習。此外，我們整合了層協作局部更新策略，以減少由貪婪的局部參數更新所造成的資訊損失。我們的模型超越了現有的 FF 模型和其他進階的局部學習方法，在 MNIST 上的準確度為 99.7%，在 CIFAR-10 上為 88.2%，在 CIFAR-100 上為 59%，在 SVHN 上為 95.9%，在 ImageNette 上為 82.5%。此外，與 BP 訓練相比，它以不到 40% 的記憶體成本達到了相當的效能，同時對多種類型的硬體相關雜訊展現出更強的穩健性，證明了它在神經型態晶片上進行線上學習和能源效率運算的潛力。

##### **SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models**
2408.14909v1 by Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo, Zhichao Lu, Jianguo Zhang, Luziwei Leng

Known as low energy consumption networks, spiking neural networks (SNNs) have
gained a lot of attention within the past decades. While SNNs are increasing
competitive with artificial neural networks (ANNs) for vision tasks, they are
rarely used for long sequence tasks, despite their intrinsic temporal dynamics.
In this work, we develop spiking state space models (SpikingSSMs) for long
sequence learning by leveraging on the sequence learning abilities of state
space models (SSMs). Inspired by dendritic neuron structure, we hierarchically
integrate neuronal dynamics with the original SSM block, meanwhile realizing
sparse synaptic computation. Furthermore, to solve the conflict of event-driven
neuronal dynamics with parallel computing, we propose a light-weight surrogate
dynamic network which accurately predicts the after-reset membrane potential
and compatible to learnable thresholds, enabling orders of acceleration in
training speed compared with conventional iterative methods. On the long range
arena benchmark task, SpikingSSM achieves competitive performance to
state-of-the-art SSMs meanwhile realizing on average 90\% of network sparsity.
On language modeling, our network significantly surpasses existing spiking
large language models (spikingLLMs) on the WikiText-103 dataset with only a
third of the model size, demonstrating its potential as backbone architecture
for low computation cost LLMs.

摘要：<paragraph>尖峰神經網路（SNN）以低能耗網路聞名，在過去的幾十年中備受關注。雖然 SNN 在視覺任務方面與人工神經網路（ANN）競爭力越來越高，但儘管它們具有內在的時間動態，卻很少用於長序列任務。在這項工作中，我們透過利用狀態空間模型（SSM）的序列學習能力，開發了尖峰狀態空間模型（SpikingSSM）用於長序列學習。受樹突神經元結構啟發，我們將神經元動態與原始 SSM 區塊分層整合，同時實現稀疏突觸計算。此外，為了解決事件驅動的神經元動態與並行計算的衝突，我們提出了一個輕量級的代理動態網路，它可以準確預測重置後膜電位，並與可學習閾值相容，與傳統的迭代方法相比，能夠將訓練速度提升好幾個數量級。在長程競技場基準任務中，SpikingSSM 達到了與最先進的 SSM 相當的效能，同時平均實現了 90% 的網路稀疏性。在語言建模中，我們的網路在 WikiText-103 資料集上顯著超越了現有的尖峰大型語言模型（spikingLLM），而模型大小只有三分之一，證明了其作為低運算成本 LLM 的主幹架構的潛力。</paragraph>

##### **Triplètoile: Extraction of Knowledge from Microblogging Text**
2408.14908v1 by Vanni Zavarella, Sergio Consoli, Diego Reforgiato Recupero, Gianni Fenu, Simone Angioni, Davide Buscaldi, Danilo Dessì, Francesco Osborne

Numerous methods and pipelines have recently emerged for the automatic
extraction of knowledge graphs from documents such as scientific publications
and patents. However, adapting these methods to incorporate alternative text
sources like micro-blogging posts and news has proven challenging as they
struggle to model open-domain entities and relations, typically found in these
sources. In this paper, we propose an enhanced information extraction pipeline
tailored to the extraction of a knowledge graph comprising open-domain entities
from micro-blogging posts on social media platforms. Our pipeline leverages
dependency parsing and classifies entity relations in an unsupervised manner
through hierarchical clustering over word embeddings. We provide a use case on
extracting semantic triples from a corpus of 100 thousand tweets about digital
transformation and publicly release the generated knowledge graph. On the same
dataset, we conduct two experimental evaluations, showing that the system
produces triples with precision over 95% and outperforms similar pipelines of
around 5% in terms of precision, while generating a comparatively higher number
of triples.

摘要：近來出現許多方法和管線，可自動從科學出版物和專利等文件萃取知識圖譜。然而，將這些方法調整為納入微網誌貼文和新聞等其他文字來源已證明具有挑戰性，因為這些方法難以建構在這些來源中常見的開放領域實體和關係模型。在本文中，我們提出了一個增強型資訊萃取管線，專門用於從社群媒體平台上的微網誌貼文中萃取包含開放領域實體的知識圖譜。我們的管線利用依賴剖析，並透過詞嵌入的階層式分群，以非監督的方式分類實體關係。我們提供了一個使用案例，說明如何從 10 萬則關於數位轉型的推文語料中萃取語意三元組，並公開發布產生的知識圖譜。在同一個資料集上，我們進行了兩次實驗評估，結果顯示系統產生的三元組準確率超過 95%，且在準確率方面優於類似的管線約 5%，同時產生了數量相對較高的三元組。

##### **Writing in the Margins: Better Inference Pattern for Long Context Retrieval**
2408.14906v1 by Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh

In this paper, we introduce Writing in the Margins (WiM), a new inference
pattern for Large Language Models designed to optimize the handling of long
input sequences in retrieval-oriented tasks. This approach leverages the
chunked prefill of the key-value cache to perform segment-wise inference, which
enables efficient processing of extensive contexts along with the generation
and classification of intermediate information ("margins") that guide the model
towards specific tasks. This method increases computational overhead marginally
while significantly enhancing the performance of off-the-shelf models without
the need for fine-tuning. Specifically, we observe that WiM provides an average
enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)
and more than a 30.0% increase in the F1-score for aggregation tasks (CWE).
Additionally, we show how the proposed pattern fits into an interactive
retrieval design that provides end-users with ongoing updates about the
progress of context processing, and pinpoints the integration of relevant
information into the final response. We release our implementation of WiM using
Hugging Face Transformers library at
https://github.com/writer/writing-in-the-margins.

摘要：在本文中，我們介紹了邊緣寫入 (WiM)，這是一種大型語言模型的新推論模式，旨在優化在檢索導向任務中處理長輸入序列。此方法利用關鍵值快取的分塊預填充來執行分段推論，這使得能夠有效率地處理廣泛的上下文，並生成和分類中間資訊（「邊緣」），以引導模型執行特定任務。此方法會稍微增加運算負擔，同時大幅提升現成模型的效能，而無需微調。具體來說，我們觀察到 WiM 為推理技能（HotpotQA、MultiHop-RAG）提供了平均 7.5% 的準確度提升，並且聚合任務（CWE）的 F1 分數提升了 30.0% 以上。此外，我們展示了所提出的模式如何融入互動式檢索設計中，該設計可為最終使用者提供有關處理進度的持續更新，並精確指出將相關資訊整合到最終回應中。我們在 https://github.com/writer/writing-in-the-margins 中使用 Hugging Face Transformers 函式庫發布了我們的 WiM 實作。

##### **VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**
2408.14895v2 by Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda

Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.

摘要：多模態知識圖（MMKG）將各種非符號數據（例如，影像和影片）轉換為符號，成為一種資源，能讓跨模態的知識處理和機器學習成為可能。然而，對於包含多個事件（例如日常生活活動）的影片，其 MMKG 的建構仍處於早期階段。在本文中，我們基於每日活動的同步多視角模擬影片，建構了一個 MMKG。除了將日常生活影片的內容表示為以事件為中心的知識外，我們的 MMKG 也包含逐幀的細微變化，例如影片幀中的邊界框。此外，我們還提供了用於查詢 MMKG 的支援工具。作為應用範例，我們展示了我們的 MMKG 如何透過提供特定任務所需的視覺語言資料集，來促進視覺語言模型的基準測試。

##### **Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures**
2408.14875v1 by Pooja Krishan, Rohan Mohapatra, Saptarshi Sengupta

The emergence of deep learning models has revolutionized various industries
over the last decade, leading to a surge in connected devices and
infrastructures. However, these models can be tricked into making incorrect
predictions with high confidence, leading to disastrous failures and security
concerns. To this end, we explore the impact of adversarial attacks on
multivariate time-series forecasting and investigate methods to counter them.
Specifically, we employ untargeted white-box attacks, namely the Fast Gradient
Sign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs
to the training process, effectively misleading the model. We also illustrate
the subtle modifications to the inputs after the attack, which makes detecting
the attack using the naked eye quite difficult. Having demonstrated the
feasibility of these attacks, we develop robust models through adversarial
training and model hardening. We are among the first to showcase the
transferability of these attacks and defenses by extrapolating our work from
the benchmark electricity data to a larger, 10-year real-world data used for
predicting the time-to-failure of hard disks. Our experimental results confirm
that the attacks and defenses achieve the desired security thresholds, leading
to a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk
datasets respectively after implementing the adversarial defenses.

摘要：深度學習模型的出現徹底改變了各個產業
在過去十年中，導致連接設備和基礎設施激增。然而，這些模型可能會被誘騙做出不正確的預測，並具有高度的信心，從而導致災難性的失敗和安全問題。為此，我們探討了對抗性攻擊對多變數時間序列預測的影響，並研究了應對它們的方法。具體來說，我們採用無目標白盒攻擊，即快速梯度符號方法 (FGSM) 和基本迭代方法 (BIM)，以毒害訓練過程的輸入，有效地誤導模型。我們還說明了攻擊後對輸入的細微修改，這使得用肉眼檢測攻擊非常困難。在證明了這些攻擊的可行性後，我們通過對抗性訓練和模型硬化來開發強大的模型。我們是最早展示這些攻擊和防禦的可傳遞性的人之一，通過將我們的研究從基準電力數據推廣到用於預測硬碟故障時間的更大的 10 年現實世界數據中。我們的實驗結果證實，在實施對抗性防禦後，攻擊和防禦達到了所需的安全性閾值，分別導致電力和硬碟數據集的 RMSE 下降了 72.41% 和 94.81%。

##### **Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data**
2408.14874v1 by Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang

Reinforcement Learning from Human Feedback (RLHF) has proven effective in
aligning large language models with human intentions, yet it often relies on
complex methodologies like Proximal Policy Optimization (PPO) that require
extensive hyper-parameter tuning and present challenges in sample efficiency
and stability. In this paper, we introduce Inverse-Q*, an innovative framework
that transcends traditional RL methods by optimizing token-level reinforcement
learning without the need for additional reward or value models. Inverse-Q*
leverages direct preference optimization techniques but extends them by
estimating the conditionally optimal policy directly from the model's
responses, facilitating more granular and flexible policy shaping. Our approach
reduces reliance on human annotation and external supervision, making it
especially suitable for low-resource settings. We present extensive
experimental results demonstrating that Inverse-Q* not only matches but
potentially exceeds the effectiveness of PPO in terms of convergence speed and
the alignment of model responses with human preferences. Our findings suggest
that Inverse-Q* offers a practical and robust alternative to conventional RLHF
approaches, paving the way for more efficient and adaptable model training
approaches.

摘要：強化學習來自人類回饋 (RLHF) 已被證明有效地將大型語言模型與人類意圖保持一致，但它通常依賴於複雜的方法，例如近端策略最佳化 (PPO)，這需要廣泛的超參數調整，並在範例效率和穩定性方面提出挑戰。在本文中，我們介紹了 Inverse-Q*，這是一個創新的架構，它透過最佳化代幣級別的強化學習來超越傳統的 RL 方法，而不需要額外的獎勵或價值模型。Inverse-Q* 利用直接偏好最佳化技術，但透過直接從模型的回應中估計條件最佳策略來擴展它們，促進更精細且靈活的策略塑造。我們的做法減少了對人類註解和外部監督的依賴，使其特別適合低資源環境。我們提出了廣泛的實驗結果，證明 Inverse-Q* 不僅匹配，而且在收斂速度和模型回應與人類偏好的對齊方面可能超過 PPO 的有效性。我們的發現表明，Inverse-Q* 為傳統 RLHF 方法提供了一個實用且強大的替代方案，為更有效率且更適應性的模型訓練方法鋪平了道路。

##### **Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models**
2408.14866v1 by Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh

Language Language Models (LLMs) face safety concerns due to potential misuse
by malicious users. Recent red-teaming efforts have identified adversarial
suffixes capable of jailbreaking LLMs using the gradient-based search algorithm
Greedy Coordinate Gradient (GCG). However, GCG struggles with computational
inefficiency, limiting further investigations regarding suffix transferability
and scalability across models and data. In this work, we bridge the connection
between search efficiency and suffix transferability. We propose a two-stage
transfer learning framework, DeGCG, which decouples the search process into
behavior-agnostic pre-searching and behavior-relevant post-searching.
Specifically, we employ direct first target token optimization in pre-searching
to facilitate the search process. We apply our approach to cross-model,
cross-data, and self-transfer scenarios. Furthermore, we introduce an
interleaved variant of our approach, i-DeGCG, which iteratively leverages
self-transferability to accelerate the search process. Experiments on HarmBench
demonstrate the efficiency of our approach across various models and domains.
Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of
$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.
Further analysis on cross-model transfer indicates the pivotal role of first
target token optimization in leveraging suffix transferability for efficient
searching.

摘要：語言語言模型 (LLM) 因惡意使用者可能濫用而面臨安全問題。最近的紅隊行動已找出對抗性字尾，可使用基於梯度的搜尋演算法貪婪座標梯度 (GCG) 破解 LLM。然而，GCG 飽受運算效率不彰所苦，限制進一步調查字尾的可移植性和跨模型及資料的可擴充性。在這項研究中，我們搭起搜尋效率和字尾可移植性之間的橋樑。我們提出一個兩階段轉移學習架構 DeGCG，它將搜尋程序解耦成與行為無關的前置搜尋和與行為相關的後置搜尋。具體來說，我們在預先搜尋中採用直接的第一個目標符號最佳化，以利搜尋程序。我們將我們的做法應用於跨模型、跨資料和自我轉移場景。此外，我們還引入了我們做法的交錯變體 i-DeGCG，它反覆利用自我可轉移性來加速搜尋程序。在 HarmBench 上的實驗證明了我們的方法在各種模型和領域中的效率。值得注意的是，我們的 i-DeGCG 在 Llama2-chat-7b 上優於基準，在驗證和測試集上的 ASR 分別為 43.9 (+22.2) 和 39.0 (+19.5)。跨模型轉移的進一步分析表明，第一個目標符號最佳化在利用字尾可移植性以進行有效搜尋中扮演關鍵角色。

##### **Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL**
2408.14855v1 by Jihwan Lee, Woochang Sim, Sejin Kim, Sundong Kim

This paper demonstrates that model-based reinforcement learning (model-based
RL) is a suitable approach for the task of analogical reasoning. We hypothesize
that model-based RL can solve analogical reasoning tasks more efficiently
through the creation of internal models. To test this, we compared DreamerV3, a
model-based RL method, with Proximal Policy Optimization, a model-free RL
method, on the Abstraction and Reasoning Corpus (ARC) tasks. Our results
indicate that model-based RL not only outperforms model-free RL in learning and
generalizing from single tasks but also shows significant advantages in
reasoning across similar tasks.

摘要：本文證明基於模型的強化學習（基於模型的 RL）是類比推理任務的適當方法。我們假設基於模型的 RL 可以透過建立內部模型更有效率地解決類比推理任務。為了測試這一點，我們比較了基於模型的 RL 方法 DreamerV3 和無模型 RL 方法近端策略最佳化，在抽象與推理語料庫 (ARC) 任務上。我們的結果表明，基於模型的 RL 不僅在單一任務的學習和概括上優於無模型 RL，而且在類似任務的推理中也顯示出顯著優勢。

##### **Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models**
2408.14853v1 by Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao

Large Language Models (LLMs) have become a focal point in the rapidly
evolving field of artificial intelligence. However, a critical concern is the
presence of toxic content within the pre-training corpus of these models, which
can lead to the generation of inappropriate outputs. Investigating methods for
detecting internal faults in LLMs can help us understand their limitations and
improve their security. Existing methods primarily focus on jailbreaking
attacks, which involve manually or automatically constructing adversarial
content to prompt the target LLM to generate unexpected responses. These
methods rely heavily on prompt engineering, which is time-consuming and usually
requires specially designed questions. To address these challenges, this paper
proposes a target-driven attack paradigm that focuses on directly eliciting the
target response instead of optimizing the prompts. We introduce the use of
another LLM as the detector for toxic content, referred to as ToxDet. Given a
target toxic response, ToxDet can generate a possible question and a
preliminary answer to provoke the target model into producing desired toxic
responses with meanings equivalent to the provided one. ToxDet is trained by
interacting with the target LLM and receiving reward signals from it, utilizing
reinforcement learning for the optimization process. While the primary focus of
the target models is on open-source LLMs, the fine-tuned ToxDet can also be
transferred to attack black-box models such as GPT-4o, achieving notable
results. Experimental results on AdvBench and HH-Harmless datasets demonstrate
the effectiveness of our methods in detecting the tendencies of target LLMs to
generate harmful responses. This algorithm not only exposes vulnerabilities but
also provides a valuable resource for researchers to strengthen their models
against such attacks.

摘要：大型語言模型 (LLM) 已成為快速發展的人工智慧領域的焦點。然而，一個關鍵問題是這些模型的預訓練語料庫中存在有毒內容，這可能會導致產生不適當的輸出。探討用於偵測 LLM 內部故障的方法，有助於我們了解其限制並提升其安全性。現有方法主要專注於越獄攻擊，其中涉及手動或自動建構對抗性內容，以提示目標 LLM 產生意外的回應。這些方法高度依賴提示工程，這非常耗時，而且通常需要特別設計的問題。為了應對這些挑戰，本文提出了一種目標導向的攻擊範例，專注於直接引發目標回應，而不是最佳化提示。我們引入了使用另一個 LLM 作為有毒內容的偵測器，稱為 ToxDet。給定一個目標有毒回應，ToxDet 可以產生一個可能的問題和一個初步答案，以激發目標模型產生具有等同於所提供內容意義的所需有毒回應。ToxDet 是透過與目標 LLM 互動並從中接收獎勵訊號，利用強化學習進行最佳化過程來訓練的。雖然目標模型的主要焦點是開源 LLM，但微調後的 ToxDet 也可以轉移到攻擊黑盒模型，例如 GPT-4o，並獲得顯著的成果。在 AdvBench 和 HH-Harmless 資料集上的實驗結果證明了我們的方法在偵測目標 LLM 產生有害回應的傾向方面非常有效。此演算法不僅揭露了漏洞，還為研究人員提供了寶貴的資源，可讓他們強化其模型以抵禦此類攻擊。

##### **Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing**
2408.14849v1 by Hanna Abi Akl

We introduce SHADOW, a fine-tuned language model trained on an intermediate
task using associative deductive reasoning, and measure its performance on a
knowledge base construction task using Wikidata triple completion. We evaluate
SHADOW on the LM-KBC 2024 challenge and show that it outperforms the baseline
solution by 20% with a F1 score of 68.72%.

摘要：我們引入了 SHADOW，這是一個經過微調的語言模型，使用聯想演繹推理在一個中介任務上進行訓練，並使用 Wikidata 三元組完成來衡量它在知識庫構建任務上的表現。我們在 LM-KBC 2024 挑戰中評估了 SHADOW，並表明它以 68.72% 的 F1 分數優於基準解法 20%。

##### **AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark**
2408.14845v1 by Abhay Gupta, Philip Meng, Ece Yurtseven, Sean O'Brien, Kevin Zhu

Detecting biases in natural language understanding (NLU) for African American
Vernacular English (AAVE) is crucial to developing inclusive natural language
processing (NLP) systems. To address dialect-induced performance discrepancies,
we introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),
a benchmark for evaluating large language model (LLM) performance on NLU tasks
in AAVE and Standard American English (SAE). AAVENUE builds upon and extends
existing benchmarks like VALUE, replacing deterministic syntactic and
morphological transformations with a more flexible methodology leveraging
LLM-based translation with few-shot prompting, improving performance across our
evaluation metrics when translating key tasks from the GLUE and SuperGLUE
benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs
and a comprehensive set of metrics including fluency, BARTScore, quality,
coherence, and understandability. Additionally, we recruit fluent AAVE speakers
to validate our translations for authenticity. Our evaluations reveal that LLMs
consistently perform better on SAE tasks than AAVE-translated versions,
underscoring inherent biases and highlighting the need for more inclusive NLP
models. We have open-sourced our source code on GitHub and created a website to
showcase our work at https://aavenue.live.

摘要：檢測非裔美國人白話英語 (AAVE) 中自然語言理解 (NLU) 的偏差對於開發包容性的自然語言處理 (NLP) 系統至關重要。為了解決方言引起的效能差異，我們引入了 AAVENUE（{AAVE} {N}atural Language {U}nderstanding {E}valuation），這是一個用於評估大型語言模型 (LLM) 在 AAVE 和標準美式英語 (SAE) 的 NLU 任務上效能的基準。AAVENUE 建立在現有基準（如 VALUE）的基礎上並予以擴充，用更靈活的方法取代確定性的句法和形態轉換，並利用基於 LLM 的翻譯和少量提示，改善了我們在翻譯 GLUE 和 SuperGLUE 基準中的關鍵任務時的效能評估指標。我們使用五種流行的 LLM 和一組全面的指標（包括流暢度、BARTScore、品質、連貫性和可理解性）來比較 AAVENUE 和 VALUE 翻譯。此外，我們招募了流利的 AAVE 說話者來驗證我們翻譯的真實性。我們的評估顯示，LLM 在 SAE 任務上的表現始終優於 AAVE 翻譯版本，這凸顯了內在偏差並強調了對更具包容性的 NLP 模型的需求。我們已在 GitHub 上開源了我們的原始碼，並建立了一個網站來展示我們的作品，網址為 https://aavenue.live。

##### **Diffusion based Semantic Outlier Generation via Nuisance Awareness for Out-of-Distribution Detection**
2408.14841v1 by Suhee Yoon, Sanghyu Yoon, Hankook Lee, Ye Seul Sim, Sungik Choi, Kyungeun Lee, Hye-Seung Cho, Woohyung Lim

Out-of-distribution (OOD) detection, which determines whether a given sample
is part of the in-distribution (ID), has recently shown promising results
through training with synthetic OOD datasets. Nonetheless, existing methods
often produce outliers that are considerably distant from the ID, showing
limited efficacy for capturing subtle distinctions between ID and OOD. To
address these issues, we propose a novel framework, Semantic Outlier generation
via Nuisance Awareness (SONA), which notably produces challenging outliers by
directly leveraging pixel-space ID samples through diffusion models. Our
approach incorporates SONA guidance, providing separate control over semantic
and nuisance regions of ID samples. Thereby, the generated outliers achieve two
crucial properties: (i) they present explicit semantic-discrepant information,
while (ii) maintaining various levels of nuisance resemblance with ID.
Furthermore, the improved OOD detector training with SONA outliers facilitates
learning with a focus on semantic distinctions. Extensive experiments
demonstrate the effectiveness of our framework, achieving an impressive AUROC
of 88% on near-OOD datasets, which surpasses the performance of baseline
methods by a significant margin of approximately 6%.

摘要：<paragraph>異常分佈 (OOD) 偵測會判斷給定的範例是否為常態分佈 (ID) 的一部分，最近已透過使用合成 OOD 資料集進行訓練，展現出有前途的成果。儘管如此，現有方法經常會產生與 ID 相距甚遠的異常值，顯示出在捕捉 ID 和 OOD 之間的細微差異時效能有限。為了解決這些問題，我們提出一個創新的架構，稱為透過雜訊感知 (SONA) 進行語意異常值產生，它特別透過擴散模型直接利用像素空間 ID 範例來產生具挑戰性的異常值。我們的做法納入了 SONA 指導，對 ID 範例的語意和雜訊區域提供個別控制。因此，產生的異常值達到了兩個關鍵特性：(i) 它們呈現明確的語意差異資訊，同時 (ii) 保持與 ID 不同程度的雜訊相似性。此外，使用 SONA 異常值進行改良的 OOD 偵測器訓練，有助於學習專注於語意差異。廣泛的實驗證明了我們架構的有效性，在近 OOD 資料集上達到了令人印象深刻的 88% AUROC，比基準方法的效能高出約 6%，差距顯著。</paragraph>

##### **CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding**
2408.14840v1 by Yang Liu, Chuan Zhou, Peng Zhang, Yanan Cao, Yongchao Liu, Zhao Li, Hongyang Chen

Knowledge graph embedding (KGE) constitutes a foundational task, directed
towards learning representations for entities and relations within knowledge
graphs (KGs), with the objective of crafting representations comprehensive
enough to approximate the logical and symbolic interconnections among entities.
In this paper, we define a metric Z-counts to measure the difficulty of
training each triple ($<$head entity, relation, tail entity$>$) in KGs with
theoretical analysis. Based on this metric, we propose \textbf{CL4KGE}, an
efficient \textbf{C}urriculum \textbf{L}earning based training strategy for
\textbf{KGE}. This method includes a difficulty measurer and a training
scheduler that aids in the training of KGE models. Our approach possesses the
flexibility to act as a plugin within a wide range of KGE models, with the
added advantage of adaptability to the majority of KGs in existence. The
proposed method has been evaluated on popular KGE models, and the results
demonstrate that it enhances the state-of-the-art methods. The use of Z-counts
as a metric has enabled the identification of challenging triples in KGs, which
helps in devising effective training strategies.

摘要：知識圖譜嵌入 (KGE) 構成一項基礎任務，旨在學習知識圖譜 (KG) 內實體和關係的表示，目標是建立足夠全面的表示，以近似實體之間的邏輯和符號互連。在本文中，我們定義了一個指標 Z 計數，以測量在 KG 中訓練每個三元組（<$頭實體，關係，尾實體$>）的難度，並進行理論分析。基於此指標，我們提出 \textbf{CL4KGE}，一種用於 \textbf{KGE} 的基於訓練策略的有效 \textbf{C}urriculum \textbf{L}earning。此方法包括一個難度測量器和一個訓練排程器，有助於訓練 KGE 模型。我們的做法具有靈活性，可以在各種 KGE 模型中作為外掛程式運作，並具有適應現有大多數 KG 的優點。已針對熱門 KGE 模型評估所提出的方法，結果證明它增強了最先進的方法。使用 Z 計數作為指標，可以識別 KG 中具有挑戰性的三元組，這有助於制定有效的訓練策略。

##### **Diffusion Models Are Real-Time Game Engines**
2408.14837v1 by Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter

We present GameNGen, the first game engine powered entirely by a neural model
that enables real-time interaction with a complex environment over long
trajectories at high quality. GameNGen can interactively simulate the classic
game DOOM at over 20 frames per second on a single TPU. Next frame prediction
achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are
only slightly better than random chance at distinguishing short clips of the
game from clips of the simulation. GameNGen is trained in two phases: (1) an
RL-agent learns to play the game and the training sessions are recorded, and
(2) a diffusion model is trained to produce the next frame, conditioned on the
sequence of past frames and actions. Conditioning augmentations enable stable
auto-regressive generation over long trajectories.

摘要：我們展示 GameNGen，第一個完全由神經模型驅動的遊戲引擎
它能以高品質在長時間軌跡上與複雜環境進行實時互動。GameNGen 能在單個 TPU 上以每秒 20 幀的速率互動模擬經典遊戲 DOOM。下一幀預測達到了 29.4 的 PSNR，這與有損 JPEG 壓縮相當。人類評分者僅略優於隨機機會，可以區分遊戲的短片和模擬的短片。GameNGen 分兩個階段訓練：(1) 一個 RL 代理學習玩遊戲，並記錄訓練課程，以及 (2) 一個擴散模型訓練以產生下一幀，條件取決於過去幀和動作的順序。條件擴充能使長期軌跡上的穩定自回歸生成。

##### **Strategic Optimization and Challenges of Large Language Models in Object-Oriented Programming**
2408.14834v1 by Zinan Wang

In the area of code generation research, the emphasis has transitioned from
crafting individual functions to developing class-level method code that
integrates contextual information. This shift has brought several benchmarks
such as ClassEval and CoderEval, which consider class-level contexts.
Nevertheless, the influence of specific contextual factors at the method level
remains less explored.
  This research focused on method-level code generation within the
Object-Oriented Programming (OOP) framework. Based on CoderEval, we devised
experiments that varied the extent of contextual information in the prompts,
ranging from method-specific to project-wide details. We introduced the
innovative metric of "Prompt-Token Cost-Effectiveness" to evaluate the economic
viability of incorporating additional contextual layers. Our findings indicate
that prompts enriched with method invocation details yield the highest
cost-effectiveness. Additionally, our study revealed disparities among Large
Language Models (LLMs) regarding error type distributions and the level of
assistance they provide to developers. Notably, larger LLMs do not invariably
perform better. We also observed that tasks with higher degrees of coupling
present more substantial challenges, suggesting that the choice of LLM should
be tailored to the task's coupling degree. For example, GPT-4 exhibited
improved performance in low-coupling scenarios, whereas GPT-3.5 seemed better
suited for tasks with high coupling. By meticulously curating prompt content
and selecting the appropriate LLM, developers can optimize code quality while
maximizing cost-efficiency during the development process.

摘要：<paragraph>在程式碼產生研究領域中，重點已從製作個別函數轉移到開發整合背景資訊的類別層級方法程式碼。此轉變帶來了幾個基準，例如考慮類別層級背景的 ClassEval 和 CoderEval。儘管如此，特定背景因素在方法層級的影響仍較少探討。
本研究專注於物件導向程式設計 (OOP) 架構中的方法層級程式碼產生。根據 CoderEval，我們設計了實驗，改變提示中背景資訊的範圍，從特定於方法的詳細資訊到專案範圍的詳細資訊。我們引進創新的「提示代幣成本效益」指標，以評估加入額外背景層的經濟可行性。我們的研究結果顯示，豐富方法呼叫詳細資訊的提示產生最高的成本效益。此外，我們的研究揭示了大型語言模型 (LLM) 在錯誤類型分佈和它們提供給開發人員的協助層級方面存在差異。值得注意的是，較大的 LLM 並非總是表現得更好。我們還觀察到，耦合程度較高的任務會帶來更重大的挑戰，這表明 LLM 的選擇應根據任務的耦合程度進行調整。例如，GPT-4 在低耦合場景中表現出更好的效能，而 GPT-3.5 似乎更適合高耦合的任務。透過仔細策劃提示內容並選擇適當的 LLM，開發人員可以在開發過程中最佳化程式碼品質，同時最大化成本效益。</paragraph>

##### **PolicyLR: A Logic Representation For Privacy Policies**
2408.14830v1 by Ashish Hooda, Rishabh Khandelwal, Prasad Chalasani, Kassem Fawaz, Somesh Jha

Privacy policies are crucial in the online ecosystem, defining how services
handle user data and adhere to regulations such as GDPR and CCPA. However,
their complexity and frequent updates often make them difficult for
stakeholders to understand and analyze. Current automated analysis methods,
which utilize natural language processing, have limitations. They typically
focus on individual tasks and fail to capture the full context of the policies.
We propose PolicyLR, a new paradigm that offers a comprehensive
machine-readable representation of privacy policies, serving as an all-in-one
solution for multiple downstream tasks. PolicyLR converts privacy policies into
a machine-readable format using valuations of atomic formulae, allowing for
formal definitions of tasks like compliance and consistency. We have developed
a compiler that transforms unstructured policy text into this format using
off-the-shelf Large Language Models (LLMs). This compiler breaks down the
transformation task into a two-stage translation and entailment procedure. This
procedure considers the full context of the privacy policy to infer a complex
formula, where each formula consists of simpler atomic formulae. The advantage
of this model is that PolicyLR is interpretable by design and grounded in
segments of the privacy policy. We evaluated the compiler using ToS;DR, a
community-annotated privacy policy entailment dataset. Utilizing open-source
LLMs, our compiler achieves precision and recall values of 0.91 and 0.88,
respectively. Finally, we demonstrate the utility of PolicyLR in three privacy
tasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison
Shopping.

摘要：<paragraph>隱私政策在線上生態系統中至關重要，定義服務如何處理使用者資料，並遵守 GDPR 和 CCPA 等法規。然而，其複雜性和頻繁更新通常讓利害關係人難以理解和分析。目前利用自然語言處理的自動化分析方法有其限制。它們通常專注於個別任務，無法掌握政策的完整脈絡。我們提出 PolicyLR，這是一個新的典範，提供隱私政策的全面機器可讀表示，作為多個下游任務的一體化解決方案。PolicyLR 使用原子公式的估值將隱私政策轉換為機器可讀格式，允許正式定義合規性和一致性等任務。我們開發了一個編譯器，使用現成的巨量語言模型 (LLM) 將非結構化的政策文字轉換為這種格式。此編譯器將轉換任務分解為兩階段翻譯和蘊含程序。此程序考慮隱私政策的完整脈絡，以推論一個複雜的公式，其中每個公式都包含更簡單的原子公式。此模型的優點在於 PolicyLR 在設計上是可解釋的，並奠基於隱私政策的部分。我們使用 ToS;DR 這個社群註解隱私政策蘊含資料集，評估了編譯器。利用開源 LLM，我們的編譯器分別達到 0.91 和 0.88 的準確度和召回率。最後，我們展示了 PolicyLR 在三個隱私任務中的效用：政策合規性、不一致性偵測和隱私比較購物。</paragraph>

##### **From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation**
2408.14825v1 by Nada Shahin, Leila Ismail

With the growing Deaf and Hard of Hearing population worldwide and the
persistent shortage of certified sign language interpreters, there is a
pressing need for an efficient, signs-driven, integrated end-to-end translation
system, from sign to gloss to text and vice-versa. There has been a wealth of
research on machine translations and related reviews. However, there are few
works on sign language machine translation considering the particularity of the
language being continuous and dynamic. This paper aims to address this void,
providing a retrospective analysis of the temporal evolution of sign language
machine translation algorithms and a taxonomy of the Transformers
architectures, the most used approach in language translation. We also present
the requirements of a real-time Quality-of-Service sign language ma-chine
translation system underpinned by accurate deep learning algorithms. We propose
future research directions for sign language translation systems.

摘要：隨著全球聾啞人口的增加以及認證手語翻譯員的持續短缺，迫切需要一個有效率、以手語為主的整合式端到端翻譯系統，從手語到手語符號再到文字，反之亦然。機器翻譯和相關評論的研究成果豐富。然而，考量到手語的連續性和動態性，關於手語機器翻譯的研究卻很少。本文旨在解決這個問題，回顧手語機器翻譯演算法的時間演進，並對語言翻譯中最常用的方法變形金剛架構進行分類。我們也提出了一個由精準深度學習演算法支撐的即時服務品質手語機器翻譯系統的要求。我們提出手語翻譯系統的未來研究方向。

##### **A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets**
2408.14817v1 by Assaf Shmuel, Oren Glickman, Teddy Lazebnik

The analysis of tabular datasets is highly prevalent both in scientific
research and real-world applications of Machine Learning (ML). Unlike many
other ML tasks, Deep Learning (DL) models often do not outperform traditional
methods in this area. Previous comparative benchmarks have shown that DL
performance is frequently equivalent or even inferior to models such as
Gradient Boosting Machines (GBMs). In this study, we introduce a comprehensive
benchmark aimed at better characterizing the types of datasets where DL models
excel. Although several important benchmarks for tabular datasets already
exist, our contribution lies in the variety and depth of our comparison: we
evaluate 111 datasets with 20 different models, including both regression and
classification tasks. These datasets vary in scale and include both those with
and without categorical variables. Importantly, our benchmark contains a
sufficient number of datasets where DL models perform best, allowing for a
thorough analysis of the conditions under which DL models excel. Building on
the results of this benchmark, we train a model that predicts scenarios where
DL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We
present insights derived from this characterization and compare these findings
to previous benchmarks.

摘要：表格資料集的分析在科學研究和機器學習 (ML) 的實際應用中非常普遍。與許多其他 ML 任務不同，深度學習 (DL) 模型通常無法在此領域勝過傳統方法。先前的比較基準顯示，DL 效能經常等同於或甚至低於梯度提升機 (GBM) 等模型。在本研究中，我們介紹了一個全面的基準，旨在更佳地描述 DL 模型表現優異的資料集類型。儘管已經有幾個表格資料集的重要基準，但我們的貢獻在於比較的多樣性和深度：我們評估了 111 個資料集，使用 20 個不同的模型，包括回歸和分類任務。這些資料集的規模各不相同，包括有和沒有類別變數的資料集。重要的是，我們的基準包含足夠數量的 DL 模型表現最佳的資料集，允許徹底分析 DL 模型表現優異的條件。根據此基準的結果，我們訓練了一個模型，該模型預測 DL 模型優於其他方法的情境，準確率為 86.1%（AUC 0.78）。我們提出從此描述中得出的見解，並將這些發現與先前的基準進行比較。

##### **Brain-inspired Artificial Intelligence: A Comprehensive Review**
2408.14811v1 by Jing Ren, Feng Xia

Current artificial intelligence (AI) models often focus on enhancing
performance through meticulous parameter tuning and optimization techniques.
However, the fundamental design principles behind these models receive
comparatively less attention, which can limit our understanding of their
potential and constraints. This comprehensive review explores the diverse
design inspirations that have shaped modern AI models, i.e., brain-inspired
artificial intelligence (BIAI). We present a classification framework that
categorizes BIAI approaches into physical structure-inspired and human
behavior-inspired models. We also examine the real-world applications where
different BIAI models excel, highlighting their practical benefits and
deployment challenges. By delving into these areas, we provide new insights and
propose future research directions to drive innovation and address current gaps
in the field. This review offers researchers and practitioners a comprehensive
overview of the BIAI landscape, helping them harness its potential and expedite
advancements in AI development.

摘要：現今的人工智慧（AI）模型通常專注於透過細緻的參數調整和最佳化技術來提升效能。然而，這些模型背後的基礎設計原則卻較少受到重視，這可能會限制我們對其潛力和限制的理解。這份全面的評論探討了塑造現代 AI 模型的多元設計靈感，亦即大腦啟發的人工智慧（BIAI）。我們提出了一個分類架構，將 BIAI 方法分類為受物理結構啟發的模型和受人類行為啟發的模型。我們也檢視了不同 BIAI 模型在現實世界應用中的優點，強調其實際效益和部署挑戰。透過深入探討這些領域，我們提供了新的見解，並提出未來的研究方向，以推動創新並解決該領域目前的差距。這份評論為研究人員和從業人員提供了 BIAI 領域的全面概觀，協助他們發揮其潛力並加速 AI 開發的進展。

##### **Poly2Vec: Polymorphic Encoding of Geospatial Objects for Spatial Reasoning with Deep Neural Networks**
2408.14806v1 by Maria Despoina Siampou, Jialiang Li, John Krumm, Cyrus Shahabi, Hua Lu

Encoding geospatial data is crucial for enabling machine learning (ML) models
to perform tasks that require spatial reasoning, such as identifying the
topological relationships between two different geospatial objects. However,
existing encoding methods are limited as they are typically customized to
handle only specific types of spatial data, which impedes their applicability
across different downstream tasks where multiple data types coexist. To address
this, we introduce Poly2Vec, an encoding framework that unifies the modeling of
different geospatial objects, including 2D points, polylines, and polygons,
irrespective of the downstream task. We leverage the power of the 2D Fourier
transform to encode useful spatial properties, such as shape and location, from
geospatial objects into fixed-length vectors. These vectors are then inputted
into neural network models for spatial reasoning tasks.This unified approach
eliminates the need to develop and train separate models for each distinct
spatial type. We evaluate Poly2Vec on both synthetic and real datasets of mixed
geometry types and verify its consistent performance across several downstream
spatial reasoning tasks.

摘要：對地理空間資料進行編碼對於讓機器學習 (ML) 模型執行需要空間推理的任務至關重要，例如識別兩個不同地理空間物件之間的拓撲關係。然而，現有的編碼方法受到限制，因為它們通常是客製化來僅處理特定類型的空間資料，這阻礙了它們在有多種資料類型並存的不同下游任務中的適用性。為了解決這個問題，我們引入了 Poly2Vec，這是一個編碼框架，統一了不同地理空間物件的建模，包括 2D 點、折線和多邊形，而與下游任務無關。我們利用 2D 傅立葉轉換的力量，從地理空間物件編碼有用的空間屬性，例如形狀和位置，轉換成固定長度的向量。然後將這些向量輸入到神經網路模型中，用於空間推理任務。這種統一的方法消除了為每個不同的空間類型開發和訓練單獨模型的需要。我們在混合幾何類型的合成和真實資料集上評估 Poly2Vec，並驗證了它在幾個下游空間推理任務中的一致效能。

##### **GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks**
2408.14780v2 by Nisal Ranasinghe, Yu Xia, Sachith Seneviratne, Saman Halgamuge

Neural networks are powerful function approximators, yet their ``black-box"
nature often renders them opaque and difficult to interpret. While many
post-hoc explanation methods exist, they typically fail to capture the
underlying reasoning processes of the networks. A truly interpretable neural
network would be trained similarly to conventional models using techniques such
as backpropagation, but additionally provide insights into the learned
input-output relationships. In this work, we introduce the concept of
interpretability pipelineing, to incorporate multiple interpretability
techniques to outperform each individual technique. To this end, we first
evaluate several architectures that promise such interpretability, with a
particular focus on two recent models selected for their potential to
incorporate interpretability into standard neural network architectures while
still leveraging backpropagation: the Growing Interpretable Neural Network
(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations and
strengths of each and introduce a novel interpretable neural network GINN-KAN
that synthesizes the advantages of both models. When tested on the Feynman
symbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.
To highlight the capabilities and the generalizability of this approach, we
position GINN-KAN as an alternative to conventional black-box networks in
Physics-Informed Neural Networks (PINNs). We expect this to have far-reaching
implications in the application of deep learning pipelines in the natural
sciences. Our experiments with this interpretable PINN on 15 different partial
differential equations demonstrate that GINN-KAN augmented PINNs outperform
PINNs with black-box networks in solving differential equations and surpass the
capabilities of both GINN and KAN.

摘要：<paragraph>神經網路是強大的函數逼近器，但其「黑盒子」性質通常使它們不透明且難以解釋。雖然有許多事後解釋方法，但它們通常無法捕捉網路的底層推理過程。一個真正可解釋的神經網路將與使用反向傳播等技術的傳統模型類似地進行訓練，但此外還提供對學習輸入輸出關係的見解。在這項工作中，我們引入了可解釋性管道概念，以整合多種可解釋性技術，以優於每種個別技術。為此，我們首先評估了幾種承諾這種可解釋性的架構，特別關注兩個最近的模型，這些模型被選中是因為它們有可能將可解釋性納入標準神經網路架構，同時仍然利用反向傳播：成長可解釋神經網路 (GINN) 和 Kolmogorov Arnold 網路 (KAN)。我們分析了每個模型的限制和優勢，並引入了一個新穎的可解釋神經網路 GINN-KAN，它綜合了兩個模型的優點。在 Feynman 符號迴歸基準資料集上進行測試時，GINN-KAN 優於 GINN 和 KAN。為了強調這種方法的能力和普遍性，我們將 GINN-KAN 定位為物理訊息神經網路 (PINN) 中傳統黑盒子網路的替代方案。我們預計這將對自然科學中深度學習管道的應用產生深遠影響。我們使用這個可解釋的 PINN 對 15 個不同的偏微分方程式進行的實驗表明，增強的 GINN-KAN PINN 在求解微分方程式方面優於具有黑盒子網路的 PINN，並超越了 GINN 和 KAN 的能力。</paragraph>

##### **MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Semantic Segmentation**
2408.14776v1 by Yuanbing Zhu, Bingke Zhu, Zhen Chen, Huan Xu, Ming Tang, Jinqiao Wang

Open-vocabulary semantic segmentation aims to segment and recognize
semantically meaningful regions based on text-based descriptions during
inference. A typical solution to address this task is to leverage powerful
vision-language models (VLMs), such as CLIP, to bridge the gap between open-
and close-vocabulary recognition. As VLMs are usually pretrained with
low-resolution images (e.g. $224\times224$), most previous methods operate only
on downscaled images. We question this design as low resolution features often
fail to preserve fine details. Although employing additional image backbones
for high-resolution inputs can mitigate this issue, it may also introduce
significant computation overhead. Therefore, we propose MROVSeg, a
multi-resolution training framework for open-vocabulary semantic segmentation
with a single pretrained CLIP backbone, that uses sliding windows to slice the
high-resolution input into uniform patches, each matching the input size of the
well-trained image encoder. Its key components include a Multi-Res Adapter,
which restores the spatial geometry and grasps local-global correspondences
across patches by learnable convolutional and scale attention layers. To
achieve accurate segmentation, we introduce Multi-grained Masked Attention
scheme to aggregate multi-grained semantics by performing cross-attention
between object queries and multi-resolution CLIP features within the region of
interests. Through comprehensive experiments, we demonstrate the superiority of
MROVSeg on well-established open-vocabulary semantic segmentation benchmarks,
particularly for high-resolution inputs, establishing new standards for
open-vocabulary semantic segmentation.

摘要：開放詞彙語意分割旨在根據推理期間的文字描述，分割和辨識語意有意義的區域。解決此任務的典型方法是利用強大的視覺語言模型 (VLM)，例如 CLIP，來彌合開放和封閉詞彙辨識之間的差距。由於 VLM 通常使用低解析度影像 (例如 $224\times224$) 進行預訓練，因此大多數先前的方法僅在縮小的影像上執行。我們質疑此設計，因為低解析度特徵通常無法保留精細的細節。儘管採用額外的影像主幹進行高解析度輸入可以減輕此問題，但它也可能會引入大量的運算負擔。因此，我們提出 MROVSeg，這是一個單一預訓練 CLIP 主幹的開放詞彙語意分割多解析度訓練架構，它使用滑動視窗將高解析度輸入切成均勻的區塊，每個區塊都與訓練良好的影像編碼器的輸入大小相符。它的關鍵組成部分包括多解析度適配器，它透過可學習的卷積和縮放注意層，恢復空間幾何形狀並掌握區塊間的局部和全局對應關係。為了達成精確的分割，我們引入了多粒度遮罩注意機制，透過在興趣區域內執行物件查詢和多解析度 CLIP 特徵之間的交叉注意，來彙總多粒度語意。透過全面的實驗，我們證明了 MROVSeg 在建立良好的開放詞彙語意分割基準上的優越性，特別是對於高解析度輸入，為開放詞彙語意分割建立了新的標準。

##### **Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning**
2408.14774v1 by Simran Kaur, Simon Park, Anirudh Goyal, Sanjeev Arora

We introduce Instruct-SkillMix, an automated approach for creating diverse,
high quality SFT data. The Instruct-SkillMix pipeline involves two stages, each
leveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to
extract core "skills" for instruction-following, either from existing datasets,
or by directly prompting the model; (2) Data generation: uses the powerful LLM
to generate (instruction, response) data that exhibit a randomly chosen pair of
these skills. Here, the use of random skill combinations promotes diversity and
difficulty.
  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from
Instruct-SkillMix leads to strong gains on instruction following benchmarks
such as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,
LLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.
To our knowledge, this achieves state-of-the-art performance among all models
that have only undergone SFT (no RL methods) and competes with proprietary
models such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.
  Ablation studies also suggest plausible reasons for why creating open
instruction-tuning datasets via naive crowd-sourcing has proved difficult.
Introducing low quality answers ("shirkers") in $20\%$ of Instruct-SkillMix
examples causes performance to plummet, sometimes catastrophically.
  The Instruct-SkillMix pipeline is flexible and is adaptable to other
settings.

摘要：<paragraph>我們推出 Instruct-SkillMix，一種用於建立多元化、高品質 SFT 資料的自動化方法。Instruct-SkillMix 管線包含兩個階段，每個階段都利用現有的強大 LLM：(1) 技能萃取：使用 LLM 從現有資料集或直接提示模型萃取指導遵循的核心「技能」；(2) 資料產生：使用強大的 LLM 產生展現隨機選擇的技能對的 (指導、回應) 資料。在此，隨機技能組合的使用促进了多樣性和難度。
  來自 Instruct-SkillMix 產生資料的 Vanilla SFT (即，沒有 PPO、DPO 或 RL 方法) 導致在指導遵循基準（例如 AlpacaEval 2.0、MT-Bench 和 WildBench）上獲得顯著收益。僅使用 4K 個範例，LLaMA-3-8B-Base 在 AlpacaEval 2.0 上達到 42.76% 的長度控制獲勝率。
  據我們所知，這在所有僅經過 SFT（沒有 RL 方法）的模型中獲得了最先進的效能，並且與專有模型（例如 Claude 3 Opus 和 LLaMA-3.1-405B-Instruct）競爭。
  消融研究也提出了合理的理由，說明為什麼透過天真的群眾外包建立開放式指導調整資料集被證明很困難。
  在 20% 的 Instruct-SkillMix 範例中引入低品質答案（「偷懶者」）會導致效能大幅下降，有時甚至會造成災難性的後果。
  Instruct-SkillMix 管線具有彈性，並且可以適應其他設定。</paragraph>

##### **A global AI community requires language-diverse publishing**
2408.14772v1 by Haley Lepp, Parth Sarin

In this provocation, we discuss the English dominance of the AI research
community, arguing that the requirement for English language publishing upholds
and reinforces broader regimes of extraction in AI. While large language models
and machine translation have been celebrated as a way to break down barriers,
we regard their use as a symptom of linguistic exclusion of scientists and
potential readers. We propose alternative futures for a healthier publishing
culture, organized around three themes: administering conferences in the
languages of the country in which they are held, instructing peer reviewers not
to adjudicate the language appropriateness of papers, and offering
opportunities to publish and present in multiple languages. We welcome new
translations of this piece. Please contact the authors if you would like to
contribute one.

摘要：在這項挑釁中，我們討論了人工智慧研究社群中英語的支配地位，並主張英語語言出版的要求維護並強化了人工智慧中更廣泛的提取制度。雖然大型語言模型和機器翻譯被譽為打破藩籬的方法，但我們認為它們的使用是科學家和潛在讀者語言排斥的症狀。我們提出了一個更健康的出版文化的替代未來，圍繞三個主題組織：以舉辦會議的國家的語言管理會議、指示同行評審員不要裁決論文的語言適當性，並提供以多種語言發表和展示的機會。我們歡迎這篇文章的新翻譯。如果您有興趣貢獻一份，請聯繫作者。

##### **CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy Concerns**
2408.14753v1 by Anbai Jiang, Yuchen Shi, Pingyi Fan, Wei-Qiang Zhang, Jia Liu

Machine anomalous sound detection (ASD) has emerged as one of the most
promising applications in the Industrial Internet of Things (IIoT) due to its
unprecedented efficacy in mitigating risks of malfunctions and promoting
production efficiency. Previous works mainly investigated the machine ASD task
under centralized settings. However, developing the ASD system under
decentralized settings is crucial in practice, since the machine data are
dispersed in various factories and the data should not be explicitly shared due
to privacy concerns. To enable these factories to cooperatively develop a
scalable ASD model while preserving their privacy, we propose a novel framework
named CoopASD, where each factory trains an ASD model on its local dataset, and
a central server aggregates these local models periodically. We employ a
pre-trained model as the backbone of the ASD model to improve its robustness
and develop specialized techniques to stabilize the model under a completely
non-iid and domain shift setting. Compared with previous state-of-the-art
(SOTA) models trained in centralized settings, CoopASD showcases competitive
results with negligible degradation of 0.08%. We also conduct extensive
ablation studies to demonstrate the effectiveness of CoopASD.

摘要：機器異常聲音偵測 (ASD) 已成為工業物聯網 (IIoT) 中最有前途的應用之一，因為它在減輕故障風險和促進生產效率方面具有前所未有的功效。先前的研究主要在集中式設定下研究機器 ASD 任務。然而，在分散式設定下開發 ASD 系統在實務上至關重要，因為機器資料分散在各個工廠中，且由於隱私問題，不應明確共享資料。為了讓這些工廠能夠在保護隱私的同時，合作開發可擴充的 ASD 模型，我們提出了一個名為 CoopASD 的新架構，其中每個工廠都在其本地資料集上訓練 ASD 模型，而中央伺服器會定期彙總這些本地模型。我們使用預訓練模型作為 ASD 模型的主幹，以提高其穩健性，並開發專門技術，在完全非 iid 和領域轉移設定下穩定模型。與先前在集中式設定下訓練的最新技術 (SOTA) 模型相比，CoopASD 展示了具有競爭力的結果，效能降低幅度僅為 0.08%。我們還進行了廣泛的消融研究，以證明 CoopASD 的有效性。

##### **LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models**
2408.14750v1 by Haven Kim, Kahyun Choi

This paper addresses the unique challenge of conducting research in lyric
studies, where direct use of lyrics is often restricted due to copyright
concerns. Unlike typical data, internet-sourced lyrics are frequently protected
under copyright law, necessitating alternative approaches. Our study introduces
a novel method for generating copyright-free lyrics from publicly available
Bag-of-Words (BoW) datasets, which contain the vocabulary of lyrics but not the
lyrics themselves. Utilizing metadata associated with BoW datasets and large
language models, we successfully reconstructed lyrics. We have compiled and
made available a dataset of reconstructed lyrics, LyCon, aligned with metadata
from renowned sources including the Million Song Dataset, Deezer Mood Detection
Dataset, and AllMusic Genre Dataset, available for public access. We believe
that the integration of metadata such as mood annotations or genres enables a
variety of academic experiments on lyrics, such as conditional lyric
generation.

摘要：本文探討在歌詞研究中進行研究的獨特挑戰，由於版權問題，通常會限制直接使用歌詞。與一般資料不同的是，網路上取得的歌詞經常受著作權法保護，因此需要另闢蹊徑。我們的研究提出了一種創新的方法，從公開的詞彙袋 (BoW) 資料集產生無版權的歌詞，這些資料集包含歌詞的詞彙，但並非歌詞本身。我們利用與 BoW 資料集相關的元資料和大型語言模型，成功重建歌詞。我們已經編譯並提供了一個重建歌詞的資料集 LyCon，與來自知名來源的元資料相符，包括百萬歌曲資料集、Deezer 情緒偵測資料集和 AllMusic 類型資料集，可供公眾存取。我們相信，整合元資料（例如情緒註解或類型）可以進行各種關於歌詞的學術實驗，例如條件式歌詞產生。

##### **Benchmarking Reinforcement Learning Methods for Dexterous Robotic Manipulation with a Three-Fingered Gripper**
2408.14747v1 by Elizabeth Cutler, Yuning Xing, Tony Cui, Brendan Zhou, Koen van Rijnsoever, Ben Hart, David Valencia, Lee Violet C. Ong, Trevor Gee, Minas Liarokapis, Henry Williams

Reinforcement Learning (RL) training is predominantly conducted in
cost-effective and controlled simulation environments. However, the transfer of
these trained models to real-world tasks often presents unavoidable challenges.
This research explores the direct training of RL algorithms in controlled yet
realistic real-world settings for the execution of dexterous manipulation. The
benchmarking results of three RL algorithms trained on intricate in-hand
manipulation tasks within practical real-world contexts are presented. Our
study not only demonstrates the practicality of RL training in authentic
real-world scenarios, facilitating direct real-world applications, but also
provides insights into the associated challenges and considerations.
Additionally, our experiences with the employed experimental methods are
shared, with the aim of empowering and engaging fellow researchers and
practitioners in this dynamic field of robotics.

摘要：強化學習（RL）訓練主要在具成本效益且受控的模擬環境中進行。然而，將這些訓練好的模型轉移到現實世界的任務中，通常會出現無法避免的挑戰。本研究探討了在受控但貼近現實的真實世界環境中，直接訓練 RL 演算法，以執行靈巧的操控。本文呈現了在實際的真實世界情境中，針對複雜的徒手操作任務訓練的三種 RL 演算法的基準測試結果。我們的研究不僅證明了 RL 訓練在真實世界場景中的實用性，促成了直接的真實世界應用，還提供了對相關挑戰和考量的見解。此外，我們分享了使用實驗方法的經驗，目的是讓機器人領域的研究人員和實務工作者能獲得知識並參與其中。

##### **RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models**
2408.14744v1 by Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang

Abundant, well-annotated multimodal data in remote sensing are pivotal for
aligning complex visual remote sensing (RS) scenes with human language,
enabling the development of specialized vision language models across diverse
RS interpretation tasks. However, annotating RS images with rich linguistic
semantics at scale demands expertise in RS and substantial human labor, making
it costly and often impractical. In this study, we propose a workflow that
leverages large language models (LLMs) to generate multimodal datasets with
semantically rich captions at scale from plain OpenStreetMap (OSM) data for
images sourced from the Google Earth Engine (GEE) platform. This approach
facilitates the generation of paired remote sensing data and can be readily
scaled up using openly available data. Within this framework, we present
RSTeller, a multimodal dataset comprising over 1 million RS images, each
accompanied by multiple descriptive captions. Extensive experiments demonstrate
that RSTeller enhances the performance of multiple existing vision language
models for RS scene understanding through continual pre-training. Our
methodology significantly reduces the manual effort and expertise needed for
annotating remote sensing imagery while democratizing access to high-quality
annotated data. This advancement fosters progress in visual language modeling
and encourages broader participation in remote sensing research and
applications. The RSTeller dataset is available at
https://github.com/SlytherinGe/RSTeller.

摘要：豐富且標註完善的遙測多模態資料對於將複雜的視覺遙測 (RS) 場景與人類語言對齊至關重要，讓跨不同 RS 解釋任務的專門視覺語言模型得以發展。然而，大規模使用豐富語言語義標註 RS 影像是需要 RS 專業知識和大量人力，這使得成本高昂且通常不切實際。在本研究中，我們提出一個工作流程，利用大型語言模型 (LLM) 從 Google Earth Engine (GEE) 平台取得的影像中，產生來自一般 OpenStreetMap (OSM) 資料的語義豐富標題，以大規模產生多模態資料集。此方法有助於產生配對的遙測資料，並且可以使用公開取得的資料輕鬆擴充。在這個架構中，我們提出 RSTeller，一個包含超過 100 萬張 RS 影像的多模態資料集，每張影像都附有多個描述性標題。廣泛的實驗證明 RSTeller 透過持續預訓練，增強了多個現有視覺語言模型在 RS 場景理解方面的效能。我們的技術大幅減少了標註遙測影像所需的手動工作和專業知識，同時讓高品質標註資料的取得更為民主化。這項進展促進了視覺語言建模，並鼓勵更廣泛地參與遙測研究和應用。RSTeller 資料集可在 https://github.com/SlytherinGe/RSTeller 取得。

##### **PAT: Pruning-Aware Tuning for Large Language Models**
2408.14721v1 by Yijiang Liu, Huanrui Yang, Youxin Chen, Rongyu Zhang, Miao Wang, Yuan Du, Li Du

Large language models (LLMs) excel in language tasks, especially with
supervised fine-tuning after pre-training. However, their substantial memory
and computational requirements hinder practical applications. Structural
pruning, which reduces less significant weight dimensions, is one solution.
Yet, traditional post-hoc pruning often leads to significant performance loss,
with limited recovery from further fine-tuning due to reduced capacity. Since
the model fine-tuning refines the general and chaotic knowledge in pre-trained
models, we aim to incorporate structural pruning with the fine-tuning, and
propose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy
while preserving the model performance to the maximum extend. Specifically, we
insert the innovative Hybrid Sparsification Modules (HSMs) between the
Attention and FFN components to accordingly sparsify the upstream and
downstream linear modules. The HSM comprises a lightweight operator and a
globally shared trainable mask. The lightweight operator maintains a training
overhead comparable to that of LoRA, while the trainable mask unifies the
channels to be sparsified, ensuring structural pruning. Additionally, we
propose the Identity Loss which decouples the transformation and scaling
properties of the HSMs to enhance training robustness. Extensive experiments
demonstrate that PAT excels in both performance and efficiency. For example,
our Llama2-7b model with a 25\% pruning ratio achieves 1.33$\times$ speedup
while outperforming the LoRA-finetuned model by up to 1.26\% in accuracy with a
similar training cost. Code:
https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning

摘要：大型語言模型 (LLM) 在語言任務中表現出色，特別是在預訓練後的監督微調。然而，它們龐大的記憶體和運算需求阻礙了實際應用。結構性剪枝（減少較不重要的權重維度）是一種解決方案。然而，傳統的後設剪枝通常會導致顯著的效能損失，由於容量減少，進一步微調的恢復能力有限。由於模型微調改進了預訓練模型中的一般性和混亂知識，我們旨在將結構性剪枝與微調結合起來，並提出修剪感知調整 (PAT) 典範，以消除模型冗餘，同時最大程度地保留模型效能。具體來說，我們在注意力和 FFN 組件之間插入創新的混合稀疏化模組 (HSM)，以相應地稀疏化上游和下游線性模組。HSM 包含一個輕量級運算子，以及一個全域共享的可訓練遮罩。輕量級運算子維持與 LoRA 相當的訓練開銷，而可訓練遮罩統一了要稀疏化的通道，確保結構性剪枝。此外，我們提出了身分損失，它解耦了 HSM 的轉換和縮放屬性，以增強訓練的穩健性。大量的實驗證明，PAT 在效能和效率方面都表現出色。例如，我們的 Llama2-7b 模型以 25% 的剪枝率實現了 1.33 倍的加速，同時在準確度上比 LoRA 微調模型高出 1.26%，且訓練成本類似。程式碼：
https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning

##### **Residual-based Adaptive Huber Loss (RAHL) -- Design of an improved Huber loss for CQI prediction in 5G networks**
2408.14718v1 by Mina Kaviani, Jurandy Almeida, Fabio L. Verdi

The Channel Quality Indicator (CQI) plays a pivotal role in 5G networks,
optimizing infrastructure dynamically to ensure high Quality of Service (QoS).
Recent research has focused on improving CQI estimation in 5G networks using
machine learning. In this field, the selection of the proper loss function is
critical for training an accurate model. Two commonly used loss functions are
Mean Squared Error (MSE) and Mean Absolute Error (MAE). Roughly speaking, MSE
put more weight on outliers, MAE on the majority. Here, we argue that the Huber
loss function is more suitable for CQI prediction, since it combines the
benefits of both MSE and MAE. To achieve this, the Huber loss transitions
smoothly between MSE and MAE, controlled by a user-defined hyperparameter
called delta. However, finding the right balance between sensitivity to small
errors (MAE) and robustness to outliers (MSE) by manually choosing the optimal
delta is challenging. To address this issue, we propose a novel loss function,
named Residual-based Adaptive Huber Loss (RAHL). In RAHL, a learnable residual
is added to the delta, enabling the model to adapt based on the distribution of
errors in the data. Our approach effectively balances model robustness against
outliers while preserving inlier data precision. The widely recognized Long
Short-Term Memory (LSTM) model is employed in conjunction with RAHL, showcasing
significantly improved results compared to the aforementioned loss functions.
The obtained results affirm the superiority of RAHL, offering a promising
avenue for enhanced CQI prediction in 5G networks.

摘要：信道品質指標 (CQI) 在 5G 網路中扮演著舉足輕重的角色，動態地最佳化基礎建設以確保高品質的服務 (QoS)。最近的研究已專注於使用機器學習來改善 5G 網路中的 CQI 估計。在此領域中，適當損失函數的選擇對於訓練準確的模型至關重要。兩個常用的損失函數是均方誤差 (MSE) 和平均絕對誤差 (MAE)。粗略來說，MSE 較重視異常值，MAE 則重視多數值。在此，我們主張 Huber 損失函數更適合 CQI 預測，因為它結合了 MSE 和 MAE 的優點。為達成此目的，Huber 損失在 MSE 和 MAE 之間平滑轉換，並由稱為 delta 的使用者定義的超參數控制。然而，透過手動選擇最佳 delta 來找出對小誤差的敏感性 (MAE) 和對異常值的穩健性 (MSE) 之間的適當平衡是一項挑戰。為了解決此問題，我們提出了一種新的損失函數，稱為基於殘差的自適應 Huber 損失 (RAHL)。在 RAHL 中，一個可學習的殘差被加到 delta 中，使模型能夠根據資料中誤差的分布進行調整。我們的做法有效地平衡了模型對異常值的穩健性，同時保留了內點資料的精確度。廣受認可的長期短期記憶 (LSTM) 模型與 RAHL 結合使用，與前述損失函數相比，展示出顯著改善的結果。所獲得的結果肯定了 RAHL 的優越性，為增強 5G 網路中的 CQI 預測提供了有希望的途徑。

##### **Text2SQL is Not Enough: Unifying AI and Databases with TAG**
2408.14717v1 by Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E. Gonzalez, Carlos Guestrin, Matei Zaharia

AI systems that serve natural language questions over databases promise to
unlock tremendous value. Such systems would allow users to leverage the
powerful reasoning and knowledge capabilities of language models (LMs)
alongside the scalable computational power of data management systems. These
combined capabilities would empower users to ask arbitrary natural language
questions over custom data sources. However, existing methods and benchmarks
insufficiently explore this setting. Text2SQL methods focus solely on natural
language questions that can be expressed in relational algebra, representing a
small subset of the questions real users wish to ask. Likewise,
Retrieval-Augmented Generation (RAG) considers the limited subset of queries
that can be answered with point lookups to one or a few data records within the
database. We propose Table-Augmented Generation (TAG), a unified and
general-purpose paradigm for answering natural language questions over
databases. The TAG model represents a wide range of interactions between the LM
and database that have been previously unexplored and creates exciting research
opportunities for leveraging the world knowledge and reasoning capabilities of
LMs over data. We systematically develop benchmarks to study the TAG problem
and find that standard methods answer no more than 20% of queries correctly,
confirming the need for further research in this area. We release code for the
benchmark at https://github.com/TAG-Research/TAG-Bench.

摘要：人工智能系統可透過資料庫提供自然語言問題的解答，承諾能釋放出巨大的價值。此類系統將允許使用者運用語言模型 (LM) 的強大推理和知識能力，以及資料管理系統的可擴充運算能力。這些結合的能力將賦予使用者詢問自訂資料來源的任意自然語言問題。然而，現有的方法和基準並未充分探索此設定。Text2SQL 方法僅專注於可透過關聯代數表達的自然語言問題，只代表實際使用者想要詢問的問題的一小部分。同樣地，檢索增強生成 (RAG) 考慮到有限的查詢子集，這些查詢子集可以用於資料庫中一個或幾個資料記錄的點查詢來回答。我們提出表格增強生成 (TAG)，一種用於回答資料庫中自然語言問題的統一且通用的範例。TAG 模型表示 LM 和資料庫之間廣泛的互動，這些互動以前未經探索，並為利用 LM 的世界知識和推理能力來處理資料創造了令人興奮的研究機會。我們系統性地開發基準來研究 TAG 問題，並發現標準方法正確回答的查詢不超過 20%，證實了進一步研究此領域的必要性。我們在 https://github.com/TAG-Research/TAG-Bench 上釋出基準的程式碼。

##### **StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech**
2408.14713v1 by Haowei Lou, Helen Paik, Wen Hu, Lina Yao

This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that
enhances the naturalness and accuracy of synthesized speech. Building upon
existing TTS technologies, StyleSpeech incorporates a unique Style Decorator
structure that enables deep learning models to simultaneously learn style and
phoneme features, improving adaptability and efficiency through the principles
of Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style
features in pre-trained models. Additionally, we introduce a novel automatic
evaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs
large language models to offer an objective and robust protocol for
automatically assessing TTS system performance. Extensive testing on benchmark
datasets shows that our approach markedly outperforms existing state-of-the-art
baseline methods in producing natural, accurate, and high-quality speech. These
advancements not only pushes the boundaries of current TTS system capabilities,
but also facilitate the application of TTS system in more dynamic and
specialized, such as interactive virtual assistants, adaptive audiobooks, and
customized voice for gaming. Speech samples can be found in
https://style-speech.vercel.app

摘要：這篇論文介紹 StyleSpeech，一個新穎的文字轉語音~(TTS) 系統，它增強了合成語音的自然性和準確性。StyleSpeech 建立在現有的 TTS 技術之上，並結合了獨特的 Style Decorator 結構，使深度學習模型能夠同時學習風格和音素特徵，透過低秩適應 (LoRA) 的原理來提升適應性和效率。LoRA 允許在預訓練模型中有效適應風格特徵。此外，我們還引入了一個新穎的自動評估指標，LLM 引導平均意見分數 (LLM-MOS)，它採用大型語言模型來提供一個客觀且穩健的協定，用於自動評估 TTS 系統效能。在基準資料集上的廣泛測試顯示，我們的做法明顯優於現有的最先進基準方法，可產生自然、準確且高品質的語音。這些進展不僅突破了當前 TTS 系統能力的界限，也促進了 TTS 系統在更動態和更專業化的應用，例如互動式虛擬助理、自適應有聲書，以及遊戲中的自訂聲音。語音範例可以在 https://style-speech.vercel.app 中找到

##### **Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express**
2408.14698v1 by Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan

As user content and queries become increasingly multi-modal, the need for
effective multi-modal search systems has grown. Traditional search systems
often rely on textual and metadata annotations for indexed images, while
multi-modal embeddings like CLIP enable direct search using text and image
embeddings. However, embedding-based approaches face challenges in integrating
contextual features such as user locale and recency. Building a scalable
multi-modal search system requires fine-tuning several components. This paper
presents a multi-modal search architecture and a series of AB tests that
optimize embeddings and multi-modal technologies in Adobe Express template
search. We address considerations such as embedding model selection, the roles
of embeddings in matching and ranking, and the balance between dense and sparse
embeddings. Our iterative approach demonstrates how utilizing sparse, dense,
and contextual features enhances short and long query search, significantly
reduces null rates (over 70\%), and increases click-through rates (CTR). Our
findings provide insights into developing robust multi-modal search systems,
thereby enhancing relevance for complex queries.

摘要：隨著使用者內容和查詢變得越來越多元，對有效的多元搜尋系統的需求也隨之增加。傳統的搜尋系統經常依賴索引圖像的文字和元資料註解，而像 CLIP 這樣的多元嵌入則能使用文字和圖像嵌入進行直接搜尋。然而，基於嵌入的方法在整合上下文特徵（例如使用者的語言環境和最近性）時會面臨挑戰。建構可擴充的多元搜尋系統需要微調多個元件。本文提出一個多元搜尋架構和一系列的 AB 測試，在 Adobe Express 範本搜尋中最佳化嵌入和多元技術。我們探討了考量因素，例如嵌入模型選擇、嵌入在配對和排名中的角色，以及稠密和稀疏嵌入之間的平衡。我們的迭代方法示範了如何利用稀疏、稠密和上下文特徵來增強短查詢和長查詢搜尋，大幅降低空值率（超過 70%），並提高點擊率 (CTR)。我們的研究結果提供了開發強健多元搜尋系統的見解，進而提升複雜查詢的相關性。

##### **Training-Free Activation Sparsity in Large Language Models**
2408.14690v1 by James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben Athiwaratkun

Activation sparsity can enable practical inference speedups in large language
models (LLMs) by reducing the compute and memory-movement required for matrix
multiplications during the forward pass. However, existing methods face
limitations that inhibit widespread adoption. Some approaches are tailored
towards older models with ReLU-based sparsity, while others require extensive
continued pre-training on up to hundreds of billions of tokens. This paper
describes TEAL, a simple training-free method that applies magnitude-based
activation sparsity to hidden states throughout the entire model. TEAL achieves
40-50% model-wide sparsity with minimal performance degradation across Llama-2,
Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve
existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to
1.53$\times$ and 1.8$\times$ at 40% and 50% model-wide sparsity. TEAL is
compatible with weight quantization, enabling further efficiency gains.

摘要：激活稀疏性可通过减少正向传递期间矩阵乘法所需的计算和内存移动，在大型语言模型 (LLM) 中实现实际推理加速。然而，现有方法面临着阻碍广泛采用的局限性。一些方法针对基于 ReLU 稀疏性的旧模型进行定制，而另一些方法则需要在高达数百亿个标记上进行广泛的持续预训练。本文描述了 TEAL，这是一种简单的无训练方法，它将基于幅度的激活稀疏性应用于整个模型中的隐藏状态。TEAL 在 Llama-2、Llama-3 和 Mistral 系列中实现了 40-50% 的模型范围稀疏性，同时性能下降最小，其大小从 7B 到 70B 不等。我们改进了现有的稀疏内核，并展示了在 40% 和 50% 的模型范围稀疏性下，墙时解码速度分别提高了 1.53 倍和 1.8 倍。TEAL 与权重量化兼容，从而实现进一步的效率提升。

##### **Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems**
2408.14678v1 by Nikhil Khani, Shuo Yang, Aniruddh Nath, Yang Liu, Pendo Abbo, Li Wei, Shawn Andrews, Maciej Kula, Jarrod Kahn, Zhe Zhao, Lichan Hong, Ed Chi

Knowledge Distillation (KD) is a powerful approach for compressing a large
model into a smaller, more efficient model, particularly beneficial for
latency-sensitive applications like recommender systems. However, current KD
research predominantly focuses on Computer Vision (CV) and NLP tasks,
overlooking unique data characteristics and challenges inherent to recommender
systems. This paper addresses these overlooked challenges, specifically: (1)
mitigating data distribution shifts between teacher and student models, (2)
efficiently identifying optimal teacher configurations within time and
budgetary constraints, and (3) enabling computationally efficient and rapid
sharing of teacher labels to support multiple students. We present a robust KD
system developed and rigorously evaluated on multiple large-scale personalized
video recommendation systems within Google. Our live experiment results
demonstrate significant improvements in student model performance while
ensuring consistent and reliable generation of high quality teacher labels from
a continuous data stream of data.

摘要：知識蒸餾 (KD) 是一種強大的方法，可用於將大型模型壓縮成較小、更有效率的模型，特別有利於對延遲敏感的應用程式，例如推薦系統。然而，目前的 KD 研究主要集中在電腦視覺 (CV) 和 NLP 任務，忽略了推薦系統固有的獨特資料特性和挑戰。本文探討了這些被忽視的挑戰，具體來說：(1) 減輕教師和學生模型之間的資料分佈轉移，(2) 在時間和預算限制內有效率地找出最佳教師配置，以及 (3) 啟用計算效率高且快速的教師標籤分享，以支援多個學生。我們展示了一個健全的 KD 系統，該系統是在 Google 內多個大型個人化影片推薦系統上開發和嚴格評估的。我們的現場實驗結果證明，學生模型效能顯著提升，同時確保從資料的連續資料串流中持續產生高品質的教師標籤。

##### **Emergent Language in Open-Ended Environments**
2408.14649v1 by Cornelius Wolff, Julius Mayer, Elia Bruni, Xenia Ohmer

Emergent language research has made significant progress in recent years, but
still largely fails to explore how communication emerges in more complex and
situated multi-agent systems. Existing setups often employ a reference game,
which limits the range of language emergence phenomena that can be studied, as
the game consists of a single, purely language-based interaction between the
agents. In this paper, we address these limitations and explore the emergence
and utility of token-based communication in open-ended multi-agent
environments, where situated agents interact with the environment through
movement and communication over multiple time-steps. Specifically, we introduce
two novel cooperative environments: Multi-Agent Pong and Collectors. These
environments are interesting because optimal performance requires the emergence
of a communication protocol, but moderate success can be achieved without one.
By employing various methods from explainable AI research, such as saliency
maps, perturbation, and diagnostic classifiers, we are able to track and
interpret the agents' language channel use over time. We find that the emerging
communication is sparse, with the agents only generating meaningful messages
and acting upon incoming messages in states where they cannot succeed without
coordination.

摘要：近年來，新興的語言研究取得了重大進展，但
在探索更複雜且情境化的多代理系統中，溝通是如何出現的方面，仍然有很大的不足。現有的設置通常採用參考遊戲，這限制了可以研究的語言出現現象的範圍，因為該遊戲包含代理之間單一的、純粹基於語言的互動。在本文中，我們解決了這些限制，並探討了在開放式多代理環境中基於代幣的溝通的出現和效用，在這種環境中，情境代理通過運動和跨多個時間步長的溝通與環境互動。具體來說，我們引入了兩個新穎的合作環境：多代理 Pong 和收集器。這些環境很有趣，因為最佳性能需要溝通協議的出現，但可以在沒有協議的情況下取得中等成功。通過採用可解釋 AI 研究中的各種方法，例如顯著性映射、擾動和診斷分類器，我們能夠追蹤和解釋代理隨著時間推移的語言管道使用情況。我們發現新興的溝通是稀疏的，代理只在沒有協調就無法成功的情況下生成有意義的訊息並對接收到的訊息採取行動。

##### **Hybrid Deep Convolutional Neural Networks Combined with Autoencoders And Augmented Data To Predict The Look-Up Table 2006**
2408.14626v1 by Messaoud Djeddou, Aouatef Hellal, Ibrahim A. Hameed, Xingang Zhao, Djehad Al Dallal

This study explores the development of a hybrid deep convolutional neural
network (DCNN) model enhanced by autoencoders and data augmentation techniques
to predict critical heat flux (CHF) with high accuracy. By augmenting the
original input features using three different autoencoder configurations, the
model's predictive capabilities were significantly improved. The hybrid models
were trained and tested on a dataset of 7225 samples, with performance metrics
including the coefficient of determination (R2), Nash-Sutcliffe efficiency
(NSE), mean absolute error (MAE), and normalized root-mean-squared error
(NRMSE) used for evaluation. Among the tested models, the DCNN_3F-A2
configuration demonstrated the highest accuracy, achieving an R2 of 0.9908
during training and 0.9826 during testing, outperforming the base model and
other augmented versions. These results suggest that the proposed hybrid
approach, combining deep learning with feature augmentation, offers a robust
solution for CHF prediction, with the potential to generalize across a wider
range of conditions.

摘要：本研究探討了透過自編碼器和資料擴充技術增強的混合深度卷積神經網路 (DCNN) 模型，以高準確度預測臨界熱通量 (CHF) 的發展。透過使用三種不同的自編碼器組態來擴充原始輸入特徵，顯著改善了模型的預測能力。混合模型在包含 7225 個樣本的資料集上進行訓練和測試，並使用決定係數 (R2)、Nash-Sutcliffe 效率 (NSE)、平均絕對誤差 (MAE) 和正規化均方根誤差 (NRMSE) 等效能指標進行評估。在測試的模型中，DCNN_3F-A2 組態表現出最高的準確度，在訓練期間達到 R2 為 0.9908，在測試期間達到 0.9826，優於基礎模型和其他擴充版本。這些結果表明，所提出的結合深度學習與特徵擴充的混合方法，提供了一個穩健的 CHF 預測解決方案，並具有在更廣泛的條件下進行概化的潛力。

##### **MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval and Text Generation Functions**
2408.14623v1 by Yingqiang Gao, Jhony Prada, Nianlong Gu, Jessica Lam, Richard H. R. Hahnloser

Large Language Models (LLMs) produce eloquent texts but often the content
they generate needs to be verified. Traditional information retrieval systems
can assist with this task, but most systems have not been designed with
LLM-generated queries in mind. As such, there is a compelling need for
integrated systems that provide both retrieval and generation functionality
within a single user interface.
  We present MODOC, a modular user interface that leverages the capabilities of
LLMs and provides assistance with detecting their confabulations, promoting
integrity in scientific writing. MODOC represents a significant step forward in
scientific writing assistance. Its modular architecture supports flexible
functions for retrieving information and for writing and generating text in a
single, user-friendly interface.

摘要：大型語言模型 (LLM) 會產生流暢的文字，但其產生的內容通常需要驗證。傳統資訊檢索系統可以協助執行這項任務，但大多數系統並未考量 LLM 產生的查詢。因此，非常需要整合系統，在單一使用者介面中提供檢索和產生功能。
我們提出 MODOC，這是一個模組化使用者介面，它能利用 LLM 的功能，並協助偵測其虛構內容，提升科學寫作的誠信度。MODOC 代表科學寫作輔助的重大進展。其模組化架構支援彈性的資訊檢索功能，以及在單一、使用者友善的介面中撰寫和產生文字。

##### **What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation**
2408.14622v1 by Dingyi Yang, Qin Jin

With the development of artificial intelligence, particularly the success of
Large Language Models (LLMs), the quantity and quality of automatically
generated stories have significantly increased. This has led to the need for
automatic story evaluation to assess the generative capabilities of computing
systems and analyze the quality of both automatic-generated and human-written
stories. Evaluating a story can be more challenging than other generation
evaluation tasks. While tasks like machine translation primarily focus on
assessing the aspects of fluency and accuracy, story evaluation demands complex
additional measures such as overall coherence, character development,
interestingness, etc. This requires a thorough review of relevant research. In
this survey, we first summarize existing storytelling tasks, including
text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation
challenges, identify various human criteria to measure stories, and present
existing benchmark datasets. Then, we propose a taxonomy to organize evaluation
metrics that have been developed or can be adopted for story evaluation. We
also provide descriptions of these metrics, along with the discussion of their
merits and limitations. Later, we discuss the human-AI collaboration for story
evaluation and generation. Finally, we suggest potential future research
directions, extending from story evaluation to general evaluations.

摘要：隨著人工智慧的發展，特別是大型語言模型 (LLM) 的成功，自動生成的故事數量和品質顯著提升。這導致了自動故事評估的需求，以評估運算系統的生成能力，並分析自動生成和人類撰寫故事的品質。評估故事可能比其他生成評估任務更具挑戰性。機器翻譯等任務主要專注於評估流暢性和準確性，而故事評估則需要複雜的額外措施，例如整體連貫性、角色發展、趣味性等。這需要對相關研究進行徹底的回顧。在這項調查中，我們首先總結現有的說故事任務，包括文字轉文字、視覺轉文字和文字轉視覺。我們強調了他們的評估挑戰，找出衡量故事的各種人類標準，並提出現有的基準資料集。然後，我們提出了一個分類法來組織評估指標，這些指標已經開發或可以採用於故事評估。我們還提供這些指標的描述，並討論它們的優點和限制。稍後，我們將討論人類與人工智慧在故事評估和生成方面的合作。最後，我們建議潛在的未來研究方向，從故事評估擴展到一般評估。

##### **Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models**
2408.14595v1 by Ian Stewart, Sameera Horawalavithana, Brendan Kennedy, Sai Munikoti, Karl Pazdernik

Multimodal foundation models (MFMs) such as OFASys show the potential to
unlock analysis of complex data such as images, videos, and audio data via text
prompts alone. However, their performance may suffer in the face of text input
that differs even slightly from their training distribution, which is
surprising considering the use of modality-specific data to "ground" the text
input. This study demonstrates that prompt instability is a major concern for
MFMs, leading to a consistent drop in performance across all modalities, but
that instability can be mitigated with additional training with augmented data.
We evaluate several methods for grounded prompt perturbation, where we generate
perturbations and filter based on similarity to text and/or modality data.
After re-training the models on the augmented data, we find improved accuracy
and more stable performance on the perturbed test data regardless of
perturbation condition, suggesting that the data augmentation strategy helps
the models handle domain shifts more effectively. In error analysis, we find
consistent patterns of performance improvement across domains, suggesting that
retraining on prompt perturbations tends to help general reasoning capabilities
in MFMs.

摘要：多模态基础模型（MFM）如 OFASys 显示出仅通过文本提示解锁对图像、视频和音频数据等复杂数据进行分析的潜力。然而，它们在面对与训练分布略有不同的文本输入时，性能可能会下降，考虑到使用特定于模态的数据来“接地”文本输入，这是令人惊讶的。本研究表明，提示不稳定性是 MFM 的一个主要问题，导致所有模态的性能持续下降，但可以通过使用扩充数据进行额外训练来减轻这种不稳定性。我们评估了几种接地提示扰动的方法，其中我们生成扰动并根据与文本和/或模态数据的相似性进行过滤。在扩充数据上重新训练模型后，我们发现无论扰动条件如何，扰动测试数据的准确性和性能都得到了提高，这表明数据扩充策略帮助模型更有效地处理域偏移。在错误分析中，我们发现跨域的性能改进模式一致，这表明对提示扰动的重新训练往往有助于 MFM 中的一般推理能力。

##### **How to build trust in answers given by Generative AI for specific, and vague, financial questions**
2408.14593v1 by Alex Zarifis, Xusen Cheng

Purpose: Generative artificial intelligence (GenAI) has progressed in its
ability and has seen explosive growth in adoption. However, the consumer's
perspective on its use, particularly in specific scenarios such as financial
advice, is unclear. This research develops a model of how to build trust in the
advice given by GenAI when answering financial questions.
Design/methodology/approach: The model is tested with survey data using
structural equation modelling (SEM) and multi-group analysis (MGA). The MGA
compares two scenarios, one where the consumer makes a specific question and
one where a vague question is made. Findings: This research identifies that
building trust for consumers is different when they ask a specific financial
question in comparison to a vague one. Humanness has a different effect in the
two scenarios. When a financial question is specific, human-like interaction
does not strengthen trust, while (1) when a question is vague, humanness builds
trust. The four ways to build trust in both scenarios are (2) human oversight
and being in the loop, (3) transparency and control, (4) accuracy and
usefulness and finally (5) ease of use and support. Originality/value: This
research contributes to a better understanding of the consumer's perspective
when using GenAI for financial questions and highlights the importance of
understanding GenAI in specific contexts from specific stakeholders.

摘要：目的：生成式人工智慧（GenAI）在能力方面進步，並且在採用方面呈現爆炸性成長。然而，消費者對其使用的觀點，特別是在特定場景（例如財務建議）中，尚不清楚。本研究建立了一個模型，說明在回答財務問題時如何建立對 GenAI 所提供建議的信任。
設計/方法論/方法：該模型使用結構方程模型（SEM）和多群體分析（MGA）對調查資料進行測試。MGA 比較了兩種情況，一種是消費者提出具體問題，另一種是提出模糊問題。研究發現：本研究發現，當消費者提出具體的財務問題與提出模糊問題時，建立信任的方式不同。在兩種情況下，人性化具有不同的影響。當財務問題具體時，類人互動並不會增強信任，而（1）當問題模糊時，人性化會建立信任。在兩種情況下建立信任的四種方法是（2）人為監督和參與其中，（3）透明度和控制，（4）準確性和有用性，最後（5）易用性和支援。原創性/價值：本研究有助於更深入了解消費者在財務問題上使用 GenAI 時的觀點，並強調從特定利害關係人了解特定脈絡中 GenAI 的重要性。

##### **DIAGen: Diverse Image Augmentation with Generative Models**
2408.14584v1 by Tobias Lingenberg, Markus Reuter, Gopika Sudhakaran, Dominik Gojny, Stefan Roth, Simone Schaub-Meyer

Simple data augmentation techniques, such as rotations and flips, are widely
used to enhance the generalization power of computer vision models. However,
these techniques often fail to modify high-level semantic attributes of a
class. To address this limitation, researchers have explored generative
augmentation methods like the recently proposed DA-Fusion. Despite some
progress, the variations are still largely limited to textural changes, thus
falling short on aspects like varied viewpoints, environment, weather
conditions, or even class-level semantic attributes (eg, variations in a dog's
breed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.
First, we apply Gaussian noise to the embeddings of an object learned with
Textual Inversion to diversify generations using a pre-trained diffusion
model's knowledge. Second, we exploit the general knowledge of a text-to-text
generative model to guide the image generation of the diffusion model with
varied class-specific prompts. Finally, we introduce a weighting mechanism to
mitigate the impact of poorly generated samples. Experimental results across
various datasets show that DIAGen not only enhances semantic diversity but also
improves the performance of subsequent classifiers. The advantages of DIAGen
over standard augmentations and the DA-Fusion baseline are particularly
pronounced with out-of-distribution samples.

摘要：簡單的資料擴充技術，例如旋轉和翻轉，廣泛用於增強電腦視覺模型的泛化能力。然而，這些技術通常無法修改類別的高階語義屬性。為了解決這個限制，研究人員探索了生成擴充方法，例如最近提出的 DA-Fusion。儘管取得了一些進展，但變異仍然很大程度上僅限於紋理變化，因此在視角、環境、天氣條件甚至類別級語義屬性（例如，狗品種的變化）等方面存在不足。為了克服這個挑戰，我們提出了 DIAGen，並建立在 DA-Fusion 之上。首先，我們將高斯噪聲應用於使用文字反轉學習的物件嵌入，以使用預先訓練的擴散模型的知識來多樣化生成。其次，我們利用文字轉文字生成模型的一般知識來指導擴散模型的影像生成，並使用不同的類別特定提示。最後，我們引入了一個加權機制來減輕生成不良樣本的影響。跨各種資料集的實驗結果表明，DIAGen 不僅增強了語義多樣性，而且還改善了後續分類器的性能。DIAGen 相對於標準擴充和 DA-Fusion 基線的優勢在分佈外樣本中尤為明顯。

##### **EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics and Information Theory**
2408.14575v1 by Edward Y. Chang

This paper introduces EVINCE (Entropy and Variation IN Conditional
Exchanges), a dialogue framework advancing Artificial General Intelligence
(AGI) by enhancing versatility, adaptivity, and reasoning in large language
models (LLMs). Leveraging adversarial debate and a novel dual entropy theory,
EVINCE improves prediction accuracy, robustness, and stability in LLMs by
integrating statistical modeling, information theory, and machine learning to
balance diverse perspective exploration with strong prior exploitation. The
framework's effectiveness is demonstrated through consistent convergence of
information-theoretic metrics, particularly improved mutual information,
fostering productive LLM collaboration. We apply EVINCE to healthcare, showing
improved disease diagnosis, and discuss its broader implications for
decision-making across domains. This work provides theoretical foundations and
empirical validation for EVINCE, paving the way for advancements in LLM
collaboration and AGI development.

摘要：本文介紹 EVINCE（條件交換中的熵和變異），一種透過增強大型語言模型（LLM）的多功能性、適應性和推理來促進人工通用智慧（AGI）的對話框架。EVINCE 利用對抗性辯論和創新的雙重熵理論，透過整合統計建模、資訊理論和機器學習，在 LLM 中改善了預測準確度、穩健性和穩定性，以平衡多樣化的觀點探索和強有力的先驗利用。該框架的有效性透過資訊理論指標的一致收斂得到證明，特別是改善了互資訊，促進了富有成效的 LLM 協作。我們將 EVINCE 應用於醫療保健，顯示出改善的疾病診斷，並討論其對跨領域決策制定的更廣泛影響。這項工作為 EVINCE 提供了理論基礎和經驗驗證，為 LLM 協作和 AGI 開發的進步鋪平了道路。

##### **CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation**
2408.14572v1 by Muhammad Fawi

This paper introduces CURLoRA, a novel approach to fine-tuning large language
models (LLMs) that leverages CUR matrix decomposition in the context of
Low-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM
fine-tuning: mitigating catastrophic forgetting during continual learning and
reducing the number of trainable parameters. We propose a unique modification
to the CUR decomposition process, utilizing inverted probabilities for column
and row selection which acts as an implicit regularization, and initializing
the $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate
through experiments on multiple datasets that CURLoRA outperforms standard LoRA
in mitigating catastrophic forgetting. It maintains model stability and
performance across tasks while significantly reducing the number of trainable
parameters. Our results show that CURLoRA achieves very good and stable task
accuracy while maintaining base model's perplexity scores fixed compared to
LoRA upon continual fine-tuning, particularly in scenarios with limited data.

摘要：本文介紹 CURLoRA，一種微調大型語言模型 (LLM) 的新方法，它在低秩適應 (LoRA) 的背景下利用 CUR 矩陣分解。我們的模型解決了 LLM 微調中的兩個關鍵挑戰：減輕持續學習過程中的災難性遺忘，以及減少可訓練參數的數量。我們提出了一個對 CUR 分解過程的獨特修改，利用倒置機率進行欄和列選擇，作為隱式正則化，並將 $U$ 矩陣初始化為零矩陣，並僅對其進行微調。我們通過在多個資料集上進行的實驗證明，CURLoRA 在減輕災難性遺忘方面優於標準 LoRA。它在任務中保持模型穩定性和效能，同時顯著減少可訓練參數的數量。我們的結果表明，與持續微調的 LoRA 相比，CURLoRA 在保持基本模型困惑度分數固定的同時，實現了非常良好且穩定的任務準確度，特別是在資料有限的情況下。

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

摘要：撰寫臨床筆記和記錄醫療檢查是醫療保健專業人員的一項重要任務，是患者照護文件中的重要組成部分。然而，手動撰寫這些筆記很耗時，並且會影響臨床醫生花在直接患者互動和其他任務上的時間。因此，自動化臨床筆記生成系統的開發已成為 AI 在健康領域中具有臨床意義的研究領域。在本文中，我們提出了使用大型語言模型 (LLM) 進行臨床筆記生成的領域的 3 項關鍵貢獻。首先，我們介紹了 CliniKnote，這是一個綜合性數據集，包含 1,200 個複雜的醫患對話及其完整的臨床筆記。此數據集由醫學專家在現代神經網路的幫助下創建和策劃，為臨床筆記生成任務中的模型訓練和評估提供了寶貴的資源。其次，我們提出了 K-SOAP（關鍵字、主觀、客觀、評估和計畫）筆記格式，它通過在頂部添加一個關鍵字部分來增強傳統的 SOAP~\cite{podder2023soap}（主觀、客觀、評估和計畫）筆記，以便快速識別基本資訊。第三，我們開發了一個自動化管道，從醫患對話中生成 K-SOAP 筆記，並使用各種指標對各種現代 LLM 進行基準測試。我們的結果表明，與標準 LLM 微調方法相比，效率和性能有了顯著的提升。

##### **A Survey of Camouflaged Object Detection and Beyond**
2408.14562v1 by Fengyang Xiao, Sujie Hu, Yuqi Shen, Chengyu Fang, Jinfa Huang, Chunming He, Longxiang Tang, Ziyun Yang, Xiu Li

Camouflaged Object Detection (COD) refers to the task of identifying and
segmenting objects that blend seamlessly into their surroundings, posing a
significant challenge for computer vision systems. In recent years, COD has
garnered widespread attention due to its potential applications in
surveillance, wildlife conservation, autonomous systems, and more. While
several surveys on COD exist, they often have limitations in terms of the
number and scope of papers covered, particularly regarding the rapid
advancements made in the field since mid-2023. To address this void, we present
the most comprehensive review of COD to date, encompassing both theoretical
frameworks and practical contributions to the field. This paper explores
various COD methods across four domains, including both image-level and
video-level solutions, from the perspectives of traditional and deep learning
approaches. We thoroughly investigate the correlations between COD and other
camouflaged scenario methods, thereby laying the theoretical foundation for
subsequent analyses. Beyond object-level detection, we also summarize extended
methods for instance-level tasks, including camouflaged instance segmentation,
counting, and ranking. Additionally, we provide an overview of commonly used
benchmarks and evaluation metrics in COD tasks, conducting a comprehensive
evaluation of deep learning-based techniques in both image and video domains,
considering both qualitative and quantitative performance. Finally, we discuss
the limitations of current COD models and propose 9 promising directions for
future research, focusing on addressing inherent challenges and exploring
novel, meaningful technologies. For those interested, a curated list of
COD-related techniques, datasets, and additional resources can be found at
https://github.com/ChunmingHe/awesome-concealed-object-segmentation

摘要：偽裝物體偵測 (COD) 指的是識別與分割與周圍環境完美融合的物體的任務，對電腦視覺系統來說是一項重大的挑戰。近年來，由於 COD 在監視、野生動物保育、自主系統等領域的潛在應用，因此廣受關注。雖然有許多關於 COD 的調查，但它們在涵蓋的論文數量和範圍方面通常有其限制，特別是對於自 2023 年中期以來該領域的快速進展。為了填補這個空白，我們提出了迄今為止最全面的 COD 評論，涵蓋了理論架構和對該領域的實際貢獻。本文從傳統和深度學習方法的角度，探討了跨越四個領域的各種 COD 方法，包括影像層級和影片層級的解決方案。我們徹底調查了 COD 與其他偽裝場景方法之間的關聯性，從而為後續分析奠定理論基礎。除了物體層級的偵測之外，我們也總結了實例層級任務的延伸方法，包括偽裝實例分割、計數和排名。此外，我們概述了 COD 任務中常用的基準和評估指標，對影像和影片領域中基於深度學習的技術進行全面的評估，考量質化和量化的表現。最後，我們討論了當前 COD 模型的限制，並提出了 9 個有希望的未來研究方向，專注於解決固有的挑戰並探索新穎且有意義的技術。對於有興趣的人，可以在 https://github.com/ChunmingHe/awesome-concealed-object-segmentation 找到經過整理的 COD 相關技術、資料集和額外資源清單。

##### **Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization**
2408.14547v1 by Nicholas Moratelli, Davide Caffagni, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara

The conventional training approach for image captioning involves pre-training
a network using teacher forcing and subsequent fine-tuning with Self-Critical
Sequence Training to maximize hand-crafted captioning metrics. However, when
attempting to optimize modern and higher-quality metrics like CLIP-Score and
PAC-Score, this training method often encounters instability and fails to
acquire the genuine descriptive capabilities needed to produce fluent and
informative captions. In this paper, we propose a new training paradigm termed
Direct CLIP-Based Optimization (DiCO). Our approach jointly learns and
optimizes a reward model that is distilled from a learnable captioning
evaluator with high human correlation. This is done by solving a weighted
classification problem directly inside the captioner. At the same time, DiCO
prevents divergence from the original model, ensuring that fluency is
maintained. DiCO not only exhibits improved stability and enhanced quality in
the generated captions but also aligns more closely with human preferences
compared to existing methods, especially in modern metrics. Additionally, it
maintains competitive performance in traditional metrics. Our source code and
trained models are publicly available at https://github.com/aimagelab/DiCO.

摘要：傳統的影像標題訓練方法包括使用教師強制和後續微調與自我批判序列訓練來預訓練網路，以最大化手工製作的標題度量。然而，當嘗試最佳化現代和更高品質的度量（例如 CLIP-Score 和 PAC-Score）時，此訓練方法經常遇到不穩定性，並且無法獲得產生流暢且具資訊性的標題所需的真實描述能力。在本文中，我們提出一個新的訓練範例，稱為直接基於 CLIP 的最佳化 (DiCO)。我們的做法共同學習和最佳化從具有高度人類關聯性的可學習標題評估器中提取的獎勵模型。這透過直接在標題產生器內解決加權分類問題來完成。同時，DiCO 可防止與原始模型產生差異，確保維持流暢度。與現有方法相比，DiCO 不僅展現出生成的標題的穩定性和品質提升，而且更符合人類偏好，特別是在現代度量方面。此外，它在傳統度量中維持競爭力。我們的原始碼和訓練模型可在 https://github.com/aimagelab/DiCO 公開取得。

##### **Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning**
2408.14472v1 by Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, Jianyu Chen

Humanoid robots, with their human-like skeletal structure, are especially
suited for tasks in human-centric environments. However, this structure is
accompanied by additional challenges in locomotion controller design,
especially in complex real-world environments. As a result, existing humanoid
robots are limited to relatively simple terrains, either with model-based
control or model-free reinforcement learning. In this work, we introduce
Denoising World Model Learning (DWL), an end-to-end reinforcement learning
framework for humanoid locomotion control, which demonstrates the world's first
humanoid robot to master real-world challenging terrains such as snowy and
inclined land in the wild, up and down stairs, and extremely uneven terrains.
All scenarios run the same learned neural network with zero-shot sim-to-real
transfer, indicating the superior robustness and generalization capability of
the proposed method.

摘要：類人機器人擁有類似人類的骨骼結構，特別適合在以人類為中心環境中的任務。然而，此結構伴隨著運動控制器設計中的額外挑戰，特別是在複雜的真實世界環境中。因此，現有的類人機器人僅限於相對簡單的地形，無論是基於模型的控制或無模型的強化學習。在這項工作中，我們引入了去噪世界模型學習 (DWL)，這是一種用於類人運動控制的端對端強化學習框架，展示了世界上第一個類人機器人，它能掌握真實世界中具有挑戰性的地形，例如野外白雪皚皚和傾斜的土地、上下樓梯以及極不平坦的地形。所有場景都使用零次學習神經網路運作，並進行模擬到真實的轉移，這表明所提出方法具有優異的穩健性和泛化能力。

##### **A Practitioner's Guide to Continual Multimodal Pretraining**
2408.14471v1 by Karsten Roth, Vishaal Udandarao, Sebastian Dziadzio, Ameya Prabhu, Mehdi Cherti, Oriol Vinyals, Olivier Hénaff, Samuel Albanie, Matthias Bethge, Zeynep Akata

Multimodal foundation models serve numerous applications at the intersection
of vision and language. Still, despite being pretrained on extensive data, they
become outdated over time. To keep models updated, research into continual
pretraining mainly explores scenarios with either (1) infrequent,
indiscriminate updates on large-scale new data, or (2) frequent, sample-level
updates. However, practical model deployment often operates in the gap between
these two limit cases, as real-world applications often demand adaptation to
specific subdomains, tasks or concepts -- spread over the entire, varying life
cycle of a model. In this work, we complement current perspectives on continual
pretraining through a research test bed as well as provide comprehensive
guidance for effective continual model updates in such scenarios. We first
introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with
realistic compute constraints and practical deployment requirements,
constructed over 63 datasets with diverse visual and semantic coverage. Using
FoMo-in-Flux, we explore the complex landscape of practical continual
pretraining through multiple perspectives: (1) A data-centric investigation of
data mixtures and stream orderings that emulate real-world deployment
situations, (2) a method-centric investigation ranging from simple fine-tuning
and traditional continual learning strategies to parameter-efficient updates
and model merging, (3) meta learning rate schedules and mechanistic design
choices, and (4) the influence of model and compute scaling. Together, our
insights provide a practitioner's guide to continual multimodal pretraining for
real-world deployment. Our benchmark and code is here:
https://github.com/ExplainableML/fomo_in_flux.

摘要：<paragraph>多模態基礎模型在視覺和語言的交會點上服務於許多應用程式。儘管經過大量資料的預訓練，它們仍會隨著時間而過時。為了保持模型的更新，對持續預訓練的研究主要探討以下場景：(1) 對大規模新資料進行不頻繁、無差別的更新，或 (2) 頻繁、樣本層級的更新。然而，實際的模型部署通常在兩個限制案例之間運作，因為現實世界的應用程式通常需要適應特定的子網域、任務或概念，這些概念遍佈於模型的整個、多變的生命週期。在這項工作中，我們透過研究測試平台補充了目前對持續預訓練的觀點，並針對此類場景中的有效持續模型更新提供了全面的指導。我們首先介紹 FoMo-in-Flux，這是一個持續多模態預訓練基準，具有實際的運算限制和實際的部署需求，建構在 63 個具有多樣化視覺和語義涵蓋範圍的資料集上。使用 FoMo-in-Flux，我們從多個角度探討了實際持續預訓練的複雜環境：(1) 模擬現實世界部署情況的資料混合和串流排序的以資料為中心的調查，(2) 從簡單的微調和傳統的持續學習策略到參數有效更新和模型合併的方法中心調查，(3) 元學習率排程和機械設計選擇，以及 (4) 模型和運算縮放的影響。我們的見解共同為實務工作者提供了持續多模態預訓練的指南，以便進行實際世界的部署。我們的基準和程式碼在此處：https://github.com/ExplainableML/fomo_in_flux。</paragraph>

##### **Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**
2408.14470v2 by Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty

Fine-tuning large language models (LLMs) on downstream tasks requires
substantial computational resources. A class of parameter-efficient fine-tuning
(PEFT) aims to mitigate these computational challenges by selectively
fine-tuning only a small fraction of the model parameters. Although
computationally efficient, these techniques often fail to match the performance
of fully fine-tuned models, primarily due to inherent biases introduced during
parameter selection. Traditional selective PEFT techniques use a fixed set of
parameters based on a predefined budget (a process also known as unmasking),
failing to capture parameter importance dynamically and often ending up
exceeding the budget. We introduce $\text{ID}^3$, a novel selective PEFT method
that calculates parameter importance continually and dynamically unmasks
parameters by balancing exploration and exploitation in parameter selection.
Our empirical study on 15 tasks spanning natural language understanding and
generative tasks demonstrates the effectiveness of our method compared to
fixed-masking-based PEFT techniques. We analytically show that $\text{ID}^3$
reduces the number of gradient updates by a factor of two, enhancing
computational efficiency. $\text{ID}^3$ is robust to random initialization of
neurons and, therefore, can be seamlessly integrated into existing additive and
reparametrization-based PEFT modules such as adapters and LoRA for dynamic
sparsification.

摘要：微调下游任务中的大型语言模型 (LLM) 需要大量的计算资源。一类参数高效微调 (PEFT) 旨在通过仅选择性微调模型参数的一小部分来减轻这些计算挑战。尽管计算效率高，但这些技术通常无法与经过完全微调的模型的性能相匹配，这主要是由于在参数选择过程中引入的固有偏差。传统的选择性 PEFT 技术使用基于预定义预算的一组固定参数（也称为取消掩码的过程），无法动态捕捉参数重要性，并且经常超出预算。我们引入了 $\text{ID}^3$，这是一种新颖的选择性 PEFT 方法，它不断计算参数重要性，并通过平衡参数选择中的探索和利用来动态取消参数掩码。我们对涵盖自然语言理解和生成任务的 15 项任务进行的实证研究证明了我们方法与基于固定掩码的 PEFT 技术相比的有效性。我们通过分析表明，$\text{ID}^3$ 将梯度更新次数减少了一半，从而提高了计算效率。$\text{ID}^3}$ 对神经元的随机初始化具有鲁棒性，因此可以无缝集成到现有的基于加法和重新参数化的 PEFT 模块中，例如用于动态稀疏化的适配器和 LoRA。

##### **K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences**
2408.14468v1 by Zhikai Li, Xuewen Liu, Dongrong Fu, Jianquan Li, Qingyi Gu, Kurt Keutzer, Zhen Dong

The rapid advancement of visual generative models necessitates efficient and
reliable evaluation methods. Arena platform, which gathers user votes on model
comparisons, can rank models with human preferences. However, traditional Arena
methods, while established, require an excessive number of comparisons for
ranking to converge and are vulnerable to preference noise in voting,
suggesting the need for better approaches tailored to contemporary evaluation
challenges. In this paper, we introduce K-Sort Arena, an efficient and reliable
platform based on a key insight: images and videos possess higher perceptual
intuitiveness than texts, enabling rapid evaluation of multiple samples
simultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing
K models to engage in free-for-all competitions, which yield much richer
information than pairwise comparisons. To enhance the robustness of the system,
we leverage probabilistic modeling and Bayesian updating techniques. We propose
an exploration-exploitation-based matchmaking strategy to facilitate more
informative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster
convergence compared to the widely used ELO algorithm. To further validate the
superiority and obtain a comprehensive leaderboard, we collect human feedback
via crowdsourced evaluations of numerous cutting-edge text-to-image and
text-to-video models. Thanks to its high efficiency, K-Sort Arena can
continuously incorporate emerging models and update the leaderboard with
minimal votes. Our project has undergone several months of internal testing and
is now available at https://huggingface.co/spaces/ksort/K-Sort-Arena

摘要：<paragraph>視覺生成模型的快速進展需要有效且可靠的評估方法。Arena 平台收集使用者對模型比較中的投票，可以根據人類偏好對模型進行排名。然而，傳統的 Arena 方法雖然已經建立，但需要大量的比較才能收斂排名，而且容易受到投票中的偏好雜訊影響，這表明需要更好的方法來應對當前的評估挑戰。在本文中，我們介紹了 K-Sort Arena，這是一個基於一個關鍵見解的有效且可靠的平台：圖像和影片比文字具有更高的感知直觀性，可以快速評估多個樣本。因此，K-Sort Arena 採用 K 次比較，允許 K 個模型進行混戰，這比成對比較產生了更豐富的資訊。為了增強系統的穩健性，我們利用機率模型和貝氏更新技術。我們提出了一個基於探索-開發的配對策略，以促進更多有意義的比較。在我們的實驗中，與廣泛使用的 ELO 演算法相比，K-Sort Arena 的收斂速度快了 16.3 倍。為了進一步驗證其優越性並獲得一個全面的排行榜，我們通過眾包評估大量最先進的文字轉圖像和文字轉影片模型來收集人類回饋。由於其高效率，K-Sort Arena 可以持續納入新興模型，並以最少的投票更新排行榜。我們的專案已經歷數個月的內部測試，現在可以在 https://huggingface.co/spaces/ksort/K-Sort-Arena 取得。</paragraph>

##### **Explicit Inductive Inference using Large Language Models**
2408.14467v1 by Tianyang Liu, Tianyi Li, Liang Cheng, Mark Steedman

Large Language Models (LLMs) are reported to hold undesirable attestation
bias on inference tasks: when asked to predict if a premise P entails a
hypothesis H, instead of considering H's conditional truthfulness entailed by
P, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In
this paper, we propose a pipeline that exploits this bias to do explicit
inductive inference. Our pipeline uses an LLM to transform a premise into a set
of attested alternatives, and then aggregate answers of the derived new
entailment inquiries to support the original inference prediction. On a
directional predicate entailment benchmark, we demonstrate that by applying
this simple pipeline, we can improve the overall performance of LLMs on
inference and substantially alleviate the impact of their attestation bias.

摘要：大型語言模型 (LLM) 被報導在推論任務中存在不良的證明偏見：當被要求預測前提 P 是否蘊含假設 H 時，LLM 傾向於使用 H 的語境外真實標籤作為一個脆弱的代理，而不是考慮 H 的條件真實性由 P 蘊含。在本文中，我們提出了一個利用這種偏見來進行明確歸納推理的管道。我們的管道使用 LLM 將前提轉換為一組經過證實的替代方案，然後彙總衍生的新蘊含查詢的答案來支持原始推理預測。在一個方向謂詞蘊含基準上，我們證明通過應用這個簡單的管道，我們可以提高 LLM 在推理上的整體性能，並大幅減輕其證明偏見的影響。

##### **Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification**
2408.14441v1 by Mahrukh Awan, Asmar Nadeem, Muhammad Junaid Awan, Armin Mustafa, Syed Sameed Husain

Exploiting both audio and visual modalities for video classification is a
challenging task, as the existing methods require large model architectures,
leading to high computational complexity and resource requirements. Smaller
architectures, on the other hand, struggle to achieve optimal performance. In
this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that
introduces a compact model architecture specifically designed to capture
intricate audio-visual relationships in video data. Through extensive
experiments on the challenging YouTube-8M dataset, we demonstrate that
Attend-Fusion achieves an F1 score of 75.64\% with only 72M parameters, which
is comparable to the performance of larger baseline models such as
Fully-Connected Late Fusion (75.96\% F1 score, 341M parameters). Attend-Fusion
achieves similar performance to the larger baseline model while reducing the
model size by nearly 80\%, highlighting its efficiency in terms of model
complexity. Our work demonstrates that the Attend-Fusion model effectively
combines audio and visual information for video classification, achieving
competitive performance with significantly reduced model size. This approach
opens new possibilities for deploying high-performance video understanding
systems in resource-constrained environments across various applications.

摘要：利用音訊和視覺兩種方式進行影片分類是一項具有挑戰性的任務，因為現有方法需要大型模型架構，導致運算複雜度和資源需求高。另一方面，較小的架構難以達到最佳效能。在本文中，我們提出 Attend-Fusion，這是一種音訊視覺 (AV) 融合方法，它引進了一個專門設計用於擷取影片資料中複雜音訊視覺關係的精簡模型架構。透過在具有挑戰性的 YouTube-8M 資料集上進行廣泛的實驗，我們證明 Attend-Fusion 只使用 72M 個參數就能達到 75.64% 的 F1 分數，這與較大的基準模型（例如全連接後融合（75.96% F1 分數，341M 個參數））的效能相當。Attend-Fusion 達到與較大的基準模型相似的效能，同時將模型大小減少了將近 80%，突顯了它在模型複雜度方面的效率。我們的研究證明 Attend-Fusion 模型有效地結合了音訊和視覺資訊進行影片分類，在顯著減少模型大小的情況下達到具有競爭力的效能。這種方法為在資源受限的環境中部署高性能影片理解系統開啟了新的可能性，適用於各種應用程式。

##### **Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**
2408.14438v2 by Liuchang Xu, Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, Zhenhong Du

The advent of large language models such as ChatGPT, Gemini, and others has
underscored the importance of evaluating their diverse capabilities, ranging
from natural language understanding to code generation. However, their
performance on spatial tasks has not been comprehensively assessed. This study
addresses this gap by introducing a novel multi-task spatial evaluation
dataset, designed to systematically explore and compare the performance of
several advanced models on spatial tasks. The dataset encompasses twelve
distinct task types, including spatial understanding and path planning, each
with verified, accurate answers. We evaluated multiple models, including
OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase
testing approach. Initially, we conducted zero-shot testing, followed by
categorizing the dataset by difficulty and performing prompt tuning tests.
Results indicate that gpt-4o achieved the highest overall accuracy in the first
phase, with an average of 71.3%. Although moonshot-v1-8k slightly
underperformed overall, it surpassed gpt-4o in place name recognition tasks.
The study also highlights the impact of prompt strategies on model performance
in specific tasks. For example, the Chain-of-Thought (COT) strategy increased
gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot
strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to
76.3%.

摘要：大型語言模型，例如 ChatGPT、Gemini 等的出現，
突顯了評估其多樣化功能的重要性，從自然語言理解到程式碼生成。
然而，它們在空間任務上的表現尚未得到全面評估。
本研究通過引入一個新穎的多任務空間評估資料集來解決這個差距，
旨在系統地探索和比較幾個先進模型在空間任務上的表現。
該資料集包含十二個不同的任務類型，包括空間理解和路徑規劃，
每個任務都有經過驗證的準確答案。
我們評估了多個模型，包括 OpenAI 的 gpt-3.5-turbo、gpt-4o 和 ZhipuAI 的 glm-4，
採用兩階段測試方法。最初，我們進行了零次學習測試，
然後按難度對資料集進行分類並執行提示調整測試。
結果表明，gpt-4o 在第一階段中獲得了最高的整體準確度，平均為 71.3%。
儘管 moonshot-v1-8k 的整體表現略遜一籌，但它在地名識別任務中超越了 gpt-4o。
該研究還強調了提示策略對特定任務中模型效能的影響。
例如，思考鏈 (COT) 策略將 gpt-4o 在路徑規劃中的準確度從 12.4% 提高到 87.5%，
而一次性策略將 moonshot-v1-8k 在映射任務中的準確度從 10.1% 提高到 76.3%。

##### **Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview**
2408.14437v1 by Ilkin Aliyev, Kama Svoboda, Tosiron Adegbija, Jean-Marc Fellous

Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven
nature of biological neural processing, and offer the potential for
ultra-low-power artificial intelligence. However, realizing their efficiency
benefits requires specialized hardware and a co-design approach that
effectively leverages sparsity. We explore the hardware-software co-design of
sparse SNNs, examining how sparsity representation, hardware architectures, and
training techniques influence hardware efficiency. We analyze the impact of
static and dynamic sparsity, discuss the implications of different neuron
models and encoding schemes, and investigate the need for adaptability in
hardware designs. Our work aims to illuminate the path towards embedded
neuromorphic systems that fully exploit the computational advantages of sparse
SNNs.

摘要：尖峰神經網路 (SNN) 受到生物神經處理的稀疏和事件驅動特性的啟發，並提供超低功耗人工智慧的潛力。然而，實現其效率優勢需要專用硬體和一種協同設計方法，該方法有效利用稀疏性。我們探討稀疏 SNN 的硬體軟體協同設計，探討稀疏表示、硬體架構和訓練技術如何影響硬體效率。我們分析靜態和動態稀疏性的影響，討論不同神經元模型和編碼方案的含義，並探討硬體設計中對適應性的需求。我們的研究工作旨在闡明通往嵌入式類腦系統的道路，該系統充分利用稀疏 SNN 的計算優勢。

##### **Social perception of faces in a vision-language model**
2408.14435v1 by Carina I. Hausladen, Manuel Knott, Colin F. Camerer, Pietro Perona

We explore social perception of human faces in CLIP, a widely used
open-source vision-language model. To this end, we compare the similarity in
CLIP embeddings between different textual prompts and a set of face images. Our
textual prompts are constructed from well-validated social psychology terms
denoting social perception. The face images are synthetic and are
systematically and independently varied along six dimensions: the legally
protected attributes of age, gender, and race, as well as facial expression,
lighting, and pose. Independently and systematically manipulating face
attributes allows us to study the effect of each on social perception and
avoids confounds that can occur in wild-collected data due to uncontrolled
systematic correlations between attributes. Thus, our findings are experimental
rather than observational. Our main findings are three. First, while CLIP is
trained on the widest variety of images and texts, it is able to make
fine-grained human-like social judgments on face images. Second, age, gender,
and race do systematically impact CLIP's social perception of faces, suggesting
an undesirable bias in CLIP vis-a-vis legally protected attributes. Most
strikingly, we find a strong pattern of bias concerning the faces of Black
women, where CLIP produces extreme values of social perception across different
ages and facial expressions. Third, facial expression impacts social perception
more than age and lighting as much as age. The last finding predicts that
studies that do not control for unprotected visual attributes may reach the
wrong conclusions on bias. Our novel method of investigation, which is founded
on the social psychology literature and on the experiments involving the
manipulation of individual attributes, yields sharper and more reliable
observations than previous observational methods and may be applied to study
biases in any vision-language model.

摘要：<paragraph>我們在 CLIP 中探索人類臉部的社會感知，CLIP 是一個廣泛使用的開源視覺語言模型。為此，我們比較了不同文字提示和一系列臉部影像在 CLIP 嵌入中的相似性。我們的文字提示是由經過充分驗證的社會心理學術語建構而成，用於表示社會感知。臉部影像為合成影像，並沿著六個面向進行系統且獨立的變化：受法律保護的年齡、性別和種族屬性，以及面部表情、光線和姿勢。獨立且系統地操作臉部屬性，讓我們得以研究每個屬性對社會感知的影響，並避免在野外收集的資料中會出現的混淆因素，原因是屬性之間會出現不受控的系統性關聯。因此，我們的發現是實驗性的，而非觀察性的。我們的發現主要有三個。首先，儘管 CLIP 是在各種影像和文字上進行訓練，但它能夠對臉部影像做出細微的人類社會判斷。其次，年齡、性別和種族確實系統性地影響 CLIP 對臉部的社會感知，這表明 CLIP 在受法律保護的屬性方面存在不良偏見。最令人驚訝的是，我們發現了一個關於黑人女性臉部的強烈偏見模式，其中 CLIP 在不同的年齡和面部表情中產生了極端的社會感知值。第三，面部表情對社會感知的影響大於年齡，對光線的影響則與年齡相當。最後一項發現預測，未控制不受保護視覺屬性的研究可能會對偏見得出錯誤的結論。我們新穎的研究方法建立在社會心理學文獻和涉及操作個別屬性的實驗之上，比先前的觀察方法產生更清晰且更可靠的觀察結果，並可應用於研究任何視覺語言模型中的偏見。</paragraph>

##### **Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications**
2408.14432v2 by Luyue Xu, Liming Wang, Hong Xie, Mingqiang Zhou

Contextual bandits serve as a fundamental algorithmic framework for
optimizing recommendation decisions online. Though extensive attention has been
paid to tailoring contextual bandits for recommendation applications, the
"herding effects" in user feedback have been ignored. These herding effects
bias user feedback toward historical ratings, breaking down the assumption of
unbiased feedback inherent in contextual bandits. This paper develops a novel
variant of the contextual bandit that is tailored to address the feedback bias
caused by the herding effects. A user feedback model is formulated to capture
this feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)
algorithm, which employs posterior sampling to balance the exploration and
exploitation tradeoff. We prove an upper bound for the regret of the algorithm,
revealing the impact of herding effects on learning speed. Extensive
experiments on datasets demonstrate that TS-Conf outperforms four benchmark
algorithms. Analysis reveals that TS-Conf effectively mitigates the negative
impact of herding effects, resulting in faster learning and improved
recommendation accuracy.

摘要：情境式多臂老虎機作為一種基本演算法架構，用於線上最佳化推薦決策。儘管已廣泛關注客製化情境式多臂老虎機以進行推薦應用，但使用者回饋中的「從眾效應」已被忽略。這些從眾效應會讓使用者回饋偏向歷史評分，打破情境式多臂老虎機中固有的無偏回饋假設。本文開發出一種新穎的情境式多臂老虎機變體，專門用於解決由從眾效應造成的回饋偏差。制定使用者回饋模型以捕捉此回饋偏差。我們設計了 TS-Conf（從眾下的湯普森取樣）演算法，它採用後驗取樣來平衡探索和利用的取捨。我們證明了演算法遺憾的上限，揭示了從眾效應對學習速度的影響。對資料集進行的大量實驗證明，TS-Conf 優於四種基準演算法。分析顯示，TS-Conf 有效減輕了從眾效應的負面影響，從而加快了學習速度並提高了推薦準確度。

##### **CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**
2408.14419v1 by Shubham Bharti, Shiyun Cheng, Jihyun Rho, Martina Rao, Xiaojin Zhu

We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large
language models. CHARTOM consists of specially designed data visualizing
charts. Given a chart, a language model needs to not only correctly comprehend
the chart (the FACT question) but also judge if the chart will be misleading to
a human reader (the MIND question). Both questions have significant societal
benefits. We detail the construction of the CHARTOM benchmark including its
calibration on human performance.

摘要：我們介紹 CHARTOM，這是一個針對多模態大型語言模型的視覺心智理論基準。CHARTOM 包含特別設計的資料視覺化圖表。給定一個圖表，語言模型不僅需要正確理解圖表（事實問題），還需要判斷該圖表是否會對人類讀者造成誤導（心智問題）。這兩個問題都具有重大的社會效益。我們詳細說明了 CHARTOM 基準的建構，包括對人類表現的校準。

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v1 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

摘要：自動語音辨識 (ASR) 系統在將語音轉錄為文字方面至關重要，但它們造成的錯誤可能會顯著降低摘要等下游任務的效能。這個問題在臨床對話摘要中特別明顯，這是一個低資源的領域，其中用於微調的監督式資料很稀少，因此必須使用 ASR 模型作為黑盒解決方案。由於無法取得足夠的醫療對話音訊錄音和對應的 ASR 轉錄，因此採用傳統資料擴充來增強摘要模型的抗雜訊性也是不可行的。為了應對這個挑戰，我們提出了 MEDSAGE，這是一種使用大型語言模型 (LLM) 為資料擴充產生合成樣本的方法。具體來說，我們利用 LLM 的語境學習能力，並指示它們根據少數有音訊錄音的可用醫療對話範例產生類似的 ASR 錯誤。實驗結果顯示，LLM 能有效地模擬 ASR 雜訊，並且將這些雜訊資料納入訓練過程中，可以顯著改善醫療對話摘要系統的穩健性和準確性。這種方法應對了關鍵應用中 ASR 輸出有雜訊的挑戰，提供了一個穩健的解決方案來增強臨床對話摘要的可靠性。

##### **Language-specific Calibration for Pruning Multilingual Language Models**
2408.14398v2 by Simon Kurz, Jian-Jia Chen, Lucie Flek, Zhixue Zhao

Recent advances in large language model (LLM) pruning have shown
state-of-the-art compression results in post-training and retraining-free
settings while maintaining high predictive performance. However, such research
mainly considers calibrating pruning using English text, despite the
multilingual nature of modern LLMs and their frequent uses in non-English
languages. In this paper, we set out to explore effective strategies for
calibrating the pruning of multilingual language models. We present the first
comprehensive empirical study, comparing different calibration languages for
pruning multilingual models across diverse tasks, models, and state-of-the-art
pruning techniques. Our results present practical suggestions, for example,
calibrating in the target language can efficiently yield lower perplexity, but
does not necessarily benefit downstream tasks. Our further analysis experiments
unveil that calibration in the target language mainly contributes to preserving
language-specific features related to fluency and coherence, but might not
contribute to capturing language-agnostic features such as language
understanding and reasoning. Last, we provide practical recommendations for
future practitioners.

摘要：大型語言模型 (LLM) 剪枝的最新進展顯示，在訓練後和不重新訓練的設定中，最先進的壓縮結果，同時維持高預測效能。然而，此類研究主要考慮使用英文文字校準剪枝，儘管現代 LLM 的多語言性質以及它們在非英文語言中的頻繁使用。在本文中，我們著手探討校準多語言語言模型剪枝的有效策略。我們提出第一個全面的實證研究，比較不同校準語言，用於在不同的任務、模型和最先進的剪枝技術中剪枝多語言模型。我們的結果提出實用的建議，例如，在目標語言中校準可以有效降低困惑度，但未必有利於下游任務。我們進一步的分析實驗揭示，目標語言中的校準主要有助於保留與流暢性和一致性相關的語言特定特徵，但可能無助於擷取與語言無關的特徵，例如語言理解和推理。最後，我們為未來的從業人員提供實用的建議。

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, Julián N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

摘要：近期人工智能的進展顯著改善了放射報告的自動生成。然而，現有的評估方法無法揭示模型對放射影像的理解，以及它們在描述中達到人類層級精細度的能力。為了彌補這個差距，我們引進一個名為 ReXKG 的系統，它從處理過的報告中萃取出結構化的資訊，以建構一個全面的放射知識圖譜。接著，我們提出三個指標來評估各種知識圖譜中節點的相似性 (ReXKG-NSC)、邊緣的分布 (ReXKG-AMS) 和子圖的涵蓋範圍 (ReXKG-SCS)。我們對 AI 生成的和人類撰寫的放射報告進行深入的比較分析，評估專家和通才模型的效能。我們的研究提供對目前 AI 模型在放射報告生成中的能力和限制更深入的理解，並提供有價值的見解來改善模型效能和臨床應用。

##### **Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**
2408.14387v1 by Sakhinana Sagar Srinivas, Chidaksh Ravuru, Geethan Sannidhi, Venkataramana Runkana

Spatio-temporal forecasting plays a crucial role in various sectors such as
transportation systems, logistics, and supply chain management. However,
existing methods are limited by their ability to handle large, complex
datasets. To overcome this limitation, we introduce a hybrid approach that
combines the strengths of open-source large and small-scale language models
(LLMs and LMs) with traditional forecasting methods. We augment traditional
methods with dynamic prompting and a grouped-query, multi-head attention
mechanism to more effectively capture both intra-series and inter-series
dependencies in evolving nonlinear time series data. In addition, we facilitate
on-premises customization by fine-tuning smaller open-source LMs for time
series trend analysis utilizing descriptions generated by open-source large LMs
on consumer-grade hardware using Low-Rank Adaptation with Activation Memory
Reduction (LoRA-AMR) technique to reduce computational overhead and activation
storage memory demands while preserving inference latency. We combine language
model processing for time series trend analysis with traditional time series
representation learning method for cross-modal integration, achieving robust
and accurate forecasts. The framework effectiveness is demonstrated through
extensive experiments on various real-world datasets, outperforming existing
methods by significant margins in terms of forecast accuracy.

摘要：時空預測在各種領域中扮演著至關重要的角色，例如運輸系統、物流和供應鏈管理。然而，現有方法受到處理大型複雜資料集的能力限制。為了克服這個限制，我們引入了一個混合方法，結合了開放原始碼大型和小型語言模型（LLM 和 LM）的優勢以及傳統預測方法。我們使用動態提示和分組查詢、多頭注意力機制來擴充傳統方法，以更有效地捕捉不斷變化的非線性時間序列資料中的序列內和序列間依賴性。此外，我們透過微調較小的開放原始碼 LM 來進行內部部署自訂，以利用開放原始碼大型 LM 在消費級硬體上產生的描述來進行時間序列趨勢分析，使用低階適應和激活記憶體減少（LoRA-AMR）技術來降低運算負擔和激活儲存記憶體需求，同時保持推論延遲。我們將語言模型處理與時間序列趨勢分析結合傳統的時間序列表示學習方法進行跨模式整合，實現穩健且準確的預測。透過在各種真實世界資料集上進行廣泛的實驗，證明了這個架構的有效性，在預測準確性方面大幅優於現有方法。

##### **Probing Causality Manipulation of Large Language Models**
2408.14380v1 by Chenyang Zhang, Haibo Tong, Bin Zhang, Dongyu Zhang

Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.

摘要：大型語言模型 (LLM) 在自然語言處理中展現了各種能力，包括因果關係問題。對於 LLM 來說，理解因果關係並非直覺，因為預訓練模型通常建立於統計關聯，並不專注於句子中的原因和結果。因此，探討 LLM 對因果關係的內部操作是必要的。本文提出了一種新穎的方法，透過提供不同的捷徑給模型並觀察行為，來分層探討因果關係操作。我們利用檢索擴增生成 (RAG) 和情境內學習 (ICL) 來處理模型在設計的因果關係分類任務上。我們對主流 LLM 進行實驗，包括 GPT-4 和一些較小且特定於領域的模型。我們的結果表明，LLM 可以偵測與因果關係相關的實體，並辨識直接的因果關係。然而，LLM 缺乏對因果關係的專門認知，僅將其視為句子整體語意的部分。

##### **SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery**
2408.14371v1 by Sarah Rastegar, Mohammadreza Salehi, Yuki M. Asano, Hazel Doughty, Cees G. M. Snoek

In this paper, we address Generalized Category Discovery, aiming to
simultaneously uncover novel categories and accurately classify known ones.
Traditional methods, which lean heavily on self-supervision and contrastive
learning, often fall short when distinguishing between fine-grained categories.
To address this, we introduce a novel concept called `self-expertise', which
enhances the model's ability to recognize subtle differences and uncover
unknown categories. Our approach combines unsupervised and supervised
self-expertise strategies to refine the model's discernment and generalization.
Initially, hierarchical pseudo-labeling is used to provide `soft supervision',
improving the effectiveness of self-expertise. Our supervised technique differs
from traditional methods by utilizing more abstract positive and negative
samples, aiding in the formation of clusters that can generalize to novel
categories. Meanwhile, our unsupervised strategy encourages the model to
sharpen its category distinctions by considering within-category examples as
`hard' negatives. Supported by theoretical insights, our empirical results
showcase that our method outperforms existing state-of-the-art techniques in
Generalized Category Discovery across several fine-grained datasets. Our code
is available at: https://github.com/SarahRastegar/SelEx.

摘要：在本文中，我们探讨廣義類別發現，旨在同時發現新類別並準確分類已知類別。傳統方法過度依賴自我監督和對比學習，在區分細粒度類別時常常力不從心。為了解決此問題，我們引入了一個稱為「自我專業知識」的新概念，它增強了模型識別細微差異和發現未知類別的能力。我們的做法結合了無監督和監督自我專業知識策略，以優化模型的辨識和概化能力。最初，層次偽標籤用於提供「軟監督」，提高自我專業知識的有效性。我們的監督技術不同於傳統方法，它利用更抽象的正負樣本，協助形成可以概化到新類別的群集。同時，我們的無監督策略鼓勵模型將類別內部範例視為「硬」負例，藉此強化其類別區分能力。在理論見解的支援下，我們的實證結果顯示，我們的模型在廣義類別發現方面優於現有的最先進技術，且涵蓋多個細粒度資料集。我們的程式碼可在 https://github.com/SarahRastegar/SelEx 取得。

##### **GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy**
2408.14368v1 by Peiyan Li, Hongtao Wu, Yan Huang, Chilam Cheang, Liang Wang, Tao Kong

The robotics community has consistently aimed to achieve generalizable robot
manipulation with flexible natural language instructions. One of the primary
challenges is that obtaining robot data fully annotated with both actions and
texts is time-consuming and labor-intensive. However, partially annotated data,
such as human activity videos without action labels and robot play data without
language labels, is much easier to collect. Can we leverage these data to
enhance the generalization capability of robots? In this paper, we propose
GR-MG, a novel method which supports conditioning on both a language
instruction and a goal image. During training, GR-MG samples goal images from
trajectories and conditions on both the text and the goal image or solely on
the image when text is unavailable. During inference, where only the text is
provided, GR-MG generates the goal image via a diffusion-based image-editing
model and condition on both the text and the generated image. This approach
enables GR-MG to leverage large amounts of partially annotated data while still
using language to flexibly specify tasks. To generate accurate goal images, we
propose a novel progress-guided goal image generation model which injects task
progress information into the generation process, significantly improving the
fidelity and the performance. In simulation experiments, GR-MG improves the
average number of tasks completed in a row of 5 from 3.35 to 4.04. In
real-robot experiments, GR-MG is able to perform 47 different tasks and
improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and
generalization settings, respectively. Code and checkpoints will be available
at the project page: https://gr-mg.github.io/.

摘要：機器人社群持續致力於透過靈活的自然語言指令，達成可概化的機器人操作。其中一項主要的挑戰在於，要取得同時標註動作與文字的機器人資料，非常耗時且費力。然而，部分標註的資料，例如沒有動作標籤的人類活動影片，以及沒有語言標籤的機器人遊戲資料，就容易取得許多。我們能利用這些資料來提升機器人的概化能力嗎？在本文中，我們提出 GR-MG，這是一種新穎的方法，支援以語言指令和目標影像為條件。在訓練期間，GR-MG 從軌跡中取樣目標影像，並以文字和目標影像為條件，或是在沒有文字時，僅以影像為條件。在推理期間，僅提供文字時，GR-MG 會透過基於擴散的影像編輯模型生成目標影像，並以文字和生成的影像為條件。這個方法讓 GR-MG 能夠利用大量的部分標註資料，同時仍然使用語言來靈活地指定任務。為了產生精確的目標影像，我們提出一個新穎的進度引導目標影像生成模型，將任務進度資訊注入到生成過程中，大幅提升保真度和效能。在模擬實驗中，GR-MG 將連續完成任務的平均次數從 3.35 提升至 4.04。在真實機器人實驗中，GR-MG 能執行 47 項不同的任務，並將成功率從 62.5% 提升至 75.0%，以及在簡單和概化設定中從 42.4% 提升至 57.6%。程式碼和檢查點將會在專案頁面提供：https://gr-mg.github.io/。

##### **SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**
2408.14354v1 by Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, Qianxiang Wang

GitHub issue resolving is a critical task in software engineering, recently
gaining significant attention in both industry and academia. Within this task,
SWE-bench has been released to evaluate issue resolving capabilities of large
language models (LLMs), but has so far only focused on Python version. However,
supporting more programming languages is also important, as there is a strong
demand in industry. As a first step toward multilingual support, we have
developed a Java version of SWE-bench, called SWE-bench-java. We have publicly
released the dataset, along with the corresponding Docker-based evaluation
environment and leaderboard, which will be continuously maintained and updated
in the coming months. To verify the reliability of SWE-bench-java, we implement
a classic method SWE-agent and test several powerful LLMs on it. As is well
known, developing a high-quality multi-lingual benchmark is time-consuming and
labor-intensive, so we welcome contributions through pull requests or
collaboration to accelerate its iteration and refinement, paving the way for
fully automated programming.

摘要：GitHub 議題解決在軟體工程中是一項關鍵任務，最近在產業和學術界都獲得了顯著的關注。在此任務中，SWE-bench 已被釋出，用於評估大型語言模型 (LLM) 的議題解決能力，但到目前為止僅專注於 Python 版本。然而，支援更多程式語言也很重要，因為產業中有強烈的需求。作為邁向多語言支援的第一步，我們已經開發了 SWE-bench 的 Java 版本，稱為 SWE-bench-java。我們已經公開發布了資料集，以及對應的基於 Docker 的評估環境和排行榜，這些資料將在未來幾個月持續維護和更新。為了驗證 SWE-bench-java 的可靠性，我們實作了一個經典方法 SWE-agent，並在它上面測試了幾個功能強大的 LLM。眾所周知，開發一個高品質的多語言基準是耗時且費力的，因此我們歡迎透過 pull request 或合作來加速它的反覆運算和精進，為全自動化程式設計鋪路。

