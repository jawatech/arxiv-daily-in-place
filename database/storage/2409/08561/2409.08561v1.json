{"2409.08561": {"publish_time": "2024-09-13", "title": "Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u900f\u904e\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u9032\u884c\u63a8\u7406\u548c\u591a\u6b65\u9a5f\u554f\u984c\u89e3\u6c7a\u7b49\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u751f\u6210\u5b8c\u6574\u7684 CoT \u7a0b\u5e8f\u6703\u7522\u751f\u986f\u8457\u66f4\u9577\u7684\u8f38\u51fa\u5e8f\u5217\uff0c\u5c0e\u81f4\u5728\u63a8\u7406\u671f\u9593\u589e\u52a0\u904b\u7b97\u6210\u672c\u548c\u5ef6\u9072\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u900f\u904e\u8a9e\u7fa9\u5c0d\u9f4a\u4f86\u58d3\u7e2e CoT \u7a0b\u5e8f\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u7559 CoT \u63a8\u7406\u512a\u9ede\u7684\u540c\u6642\u5be6\u73fe\u66f4\u6709\u6548\u7387\u7684\u89e3\u78bc\u3002\u6211\u5011\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u8f14\u52a9 CoT \u6a21\u578b\uff0c\u53ef\u5b78\u7fd2\u5c07\u5b8c\u6574\u7684\u601d\u8003\u904e\u7a0b\u751f\u6210\u4e26\u58d3\u7e2e\u6210\u8207\u539f\u59cb CoT \u8f38\u51fa\u8a9e\u7fa9\u5c0d\u9f4a\u7684\u7279\u6b8a\u7c21\u6f54\u4ee3\u5e63\u8868\u793a\u3002\u7136\u5f8c\u5c07\u6b64\u58d3\u7e2e\u8868\u793a\u6574\u5408\u5230\u96b1\u85cf\u601d\u8003\u93c8 (HCoT) \u6a21\u578b\u7684\u8f38\u5165\u4e2d\u3002\u8a13\u7df4\u7a0b\u5e8f\u9075\u5faa\u4e00\u500b\u5169\u968e\u6bb5\u7a0b\u5e8f\uff1a\u9996\u5148\uff0c\u6700\u4f73\u5316 CoT \u6a21\u578b\uff0c\u4ee5\u4f7f\u7528\u5c0d\u6bd4\u640d\u5931\u4f86\u751f\u6210\u8207\u57fa\u672c\u4e8b\u5be6 CoT \u8f38\u51fa\u5c0d\u9f4a\u7684\u58d3\u7e2e\u4ee3\u5e63\u8868\u793a\u3002\u96a8\u5f8c\uff0c\u5728\u51cd\u7d50 CoT \u6a21\u578b\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\uff0c\u5fae\u8abf HCoT \u6a21\u578b\uff0c\u4ee5\u6839\u64da\u524d\u7f6e\u6307\u4ee4\u548c\u4f86\u81ea CoT \u6a21\u578b\u7684\u58d3\u7e2e CoT \u8868\u793a\u751f\u6210\u6e96\u78ba\u7684\u5f8c\u7e8c\u9810\u6e2c\u3002\u5728\u4e09\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u9818\u57df\uff08\u6578\u5b78\u63a8\u7406\u3001\u4ee3\u7406\u547c\u53eb\u548c\u554f\u984c\u56de\u7b54\uff09\u4e2d\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u5b8c\u6574\u7684 CoT \u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u8a9e\u7fa9\u58d3\u7e2e\u65b9\u6cd5\u53ef\u5be6\u73fe\u7af6\u722d\u529b\u6216\u66f4\u9ad8\u7684\u6548\u80fd\uff0c\u540c\u6642\u5728\u89e3\u78bc\u6642\u9593\u4e0a\u63d0\u4f9b\u81f3\u5c11 1.5 \u500d\u7684\u986f\u8457\u52a0\u901f\u3002\u6b64\u5916\uff0c\u7d0d\u5165\u5c0d\u6bd4\u5b78\u7fd2\u76ee\u6a19\u9032\u4e00\u6b65\u63d0\u5347\u4e86\u58d3\u7e2e\u8868\u793a\u7684\u54c1\u8cea\uff0c\u9032\u800c\u6539\u5584\u4e86 CoT \u63d0\u793a\u548c\u4efb\u52d9\u6e96\u78ba\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u70ba\u5728\u5ee3\u6cdb\u61c9\u7528\u4e2d\u66f4\u6709\u6548\u7387\u5730\u5229\u7528 LLM \u4e2d\u7684\u591a\u6b65\u9a5f\u63a8\u7406\u80fd\u529b\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Tianqiao Liu et.al.", "authors": "Tianqiao Liu, Zui Chen, Zitao Liu, Mi Tian, Weiqi Luo", "id": "2409.08561v1", "paper_url": "http://arxiv.org/abs/2409.08561v1", "repo": "null"}}