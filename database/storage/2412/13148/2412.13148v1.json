{"2412.13148": {"publish_time": "2024-12-17", "title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction", "paper_summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen.", "paper_summary_zh": "\u81ea\u9069\u61c9\u512a\u5316\u5668\uff0c\u4f8b\u5982 Adam (Kingma & Ba, 2015) \u4e00\u76f4\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6210\u529f\u7684\u95dc\u9375\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u7dad\u8b77\u984d\u5916\u7684\u79fb\u52d5\u5e73\u5747\u72c0\u614b\uff0c\u9019\u5c0e\u81f4\u8a18\u61b6\u9ad4\u9700\u6c42\u6bd4\u6a21\u578b\u5927\u597d\u5e7e\u500d\u3002\u9019\u7a2e\u8ca0\u64d4\u5c0d\u53ef\u64f4\u5145\u6027\u548c\u8a08\u7b97\u6548\u7387\u9020\u6210\u9650\u5236\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u96d6\u7136\u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d (SGD) \u5728\u8a18\u61b6\u9ad4\u6548\u7387\u65b9\u9762\u662f\u6700\u4f73\u7684\uff0c\u4f46\u5176\u5728 LLM \u8a13\u7df4\u4e2d\u7684\u80fd\u529b\u6709\u9650 (Zhao \u7b49\u4eba\uff0c2024b)\u3002\n\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5169\u96e3\u56f0\u5883\uff0c\u6211\u5011\u8868\u660e\u9810\u8655\u7406 SGD \u8db3\u4ee5\u5728 LLM \u4e0a\u9054\u5230 Adam \u7d1a\u5225\u7684\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u5169\u500b\u7c21\u55ae\u7684\u904b\u7b97\u5b50\u5c0d\u77ac\u6642\u96a8\u6a5f\u68af\u5ea6\u9032\u884c\u9810\u8655\u7406\uff1a\n$\\mathtt{GradNorm}$ \u548c $\\mathtt{GradWhitening}$\u3002$\\mathtt{GradNorm}$ \u7a69\u5b9a\u68af\u5ea6\u5206\u4f48\uff0c\u800c $\\mathtt{GradWhitening}$ \u5206\u5225\u62b5\u6d88\u640d\u5931\u666f\u89c0\u7684\u5c40\u90e8\u66f2\u7387\u3002\u9019\u7522\u751f\u4e86 SWAN\uff08\u5177\u6709\u767d\u5316\u548c\u6b63\u898f\u5316\u7684 SGD\uff09\uff0c\u9019\u662f\u4e00\u500b\u96a8\u6a5f\u512a\u5316\u5668\uff0c\u6d88\u9664\u4e86\u5132\u5b58\u4efb\u4f55\u7d2f\u7a4d\u72c0\u614b\u8b8a\u6578\u7684\u9700\u8981\u3002\u6839\u64da\u7d93\u9a57\uff0cSWAN \u8207 SGD \u5177\u6709\u76f8\u540c\u7684\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\uff0c\u8207 Adam \u76f8\u6bd4\uff0c\u7e3d\u7aef\u5230\u7aef\u8a18\u61b6\u9ad4\u6e1b\u5c11\u4e86\u5927\u7d04 50%\u3002\u5728\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u4e2d\uff0cSWAN \u5c55\u793a\u51fa\u8207 Adam \u76f8\u540c\u751a\u81f3\u5927\u5e45\u6539\u5584\u7684\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5728\u4f7f\u7528 350M \u548c 1.3B \u53c3\u6578\u9810\u8a13\u7df4 LLaMa \u6a21\u578b\u6642\uff0cSWAN \u5728\u89c0\u5bdf\u5230\u7684\u7b26\u865f\u5c11\u65bc\u4e00\u534a\u7684\u60c5\u6cc1\u4e0b\u9054\u5230\u76f8\u540c\u7684\u8a55\u4f30\u56f0\u60d1\u5ea6\uff0c\u5f9e\u800c\u5be6\u73fe\u4e86 2 \u500d\u7684\u52a0\u901f\u3002", "author": "Chao Ma et.al.", "authors": "Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds", "id": "2412.13148v1", "paper_url": "http://arxiv.org/abs/2412.13148v1", "repo": "null"}}