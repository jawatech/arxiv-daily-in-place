{"2409.04185": {"publish_time": "2024-09-06", "title": "Residual Stream Analysis with Multi-Layer SAEs", "paper_summary": "Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, standard SAEs\nare trained separately on each transformer layer, making it difficult to use\nthem to study how information flows across layers. To solve this problem, we\nintroduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer simultaneously. The\nresidual stream is usually understood as preserving information across layers,\nso we expected to, and did, find individual SAE features that are active at\nmultiple layers. Interestingly, while a single SAE feature is active at\ndifferent layers for different prompts, for a single prompt, we find that a\nsingle feature is far more likely to be active at a single layer. For larger\nunderlying models, we find that the cosine similarities between adjacent layers\nin the residual stream are higher, so we expect more features to be active at\nmultiple layers. These results show that MLSAEs are a promising method to study\ninformation flow in transformers. We release our code to train and analyze\nMLSAEs at https://github.com/tim-lawson/mlsae.", "paper_summary_zh": "\u7a00\u758f\u81ea\u52d5\u7de8\u78bc\u5668 (SAE) \u662f\u4e00\u7a2e\u89e3\u91cbTransformer\u8a9e\u8a00\u6a21\u578b\u5167\u90e8\u8868\u793a\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6a19\u6e96 SAE \u5728\u6bcf\u500bTransformer\u5c64\u4e0a\u5206\u958b\u8a13\u7df4\uff0c\u9019\u4f7f\u5f97\u4f7f\u7528\u5b83\u5011\u4f86\u7814\u7a76\u8cc7\u8a0a\u5982\u4f55\u5728\u5c64\u4e4b\u9593\u6d41\u52d5\u8b8a\u5f97\u56f0\u96e3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u5c64 SAE (MLSAE)\uff1a\u4e00\u500b\u540c\u6642\u5728\u6bcf\u500bTransformer\u5c64\u7684\u6b98\u5dee\u4e32\u6d41\u555f\u7528\u5411\u91cf\u4e0a\u8a13\u7df4\u7684\u55ae\u4e00 SAE\u3002\u6b98\u5dee\u4e32\u6d41\u901a\u5e38\u88ab\u7406\u89e3\u70ba\u8de8\u5c64\u4fdd\u7559\u8cc7\u8a0a\uff0c\u56e0\u6b64\u6211\u5011\u9810\u671f\u6703\u627e\u5230\uff0c\u800c\u4e14\u78ba\u5be6\u627e\u5230\u4e86\uff0c\u5728\u591a\u500b\u5c64\u4e0a\u90fd\u8655\u65bc\u6d3b\u52d5\u72c0\u614b\u7684\u500b\u5225 SAE \u7279\u5fb5\u3002\u6709\u8da3\u7684\u662f\uff0c\u5118\u7ba1\u5c0d\u65bc\u4e0d\u540c\u7684\u63d0\u793a\uff0c\u55ae\u4e00 SAE \u7279\u5fb5\u5728\u4e0d\u540c\u7684\u5c64\u4e0a\u8655\u65bc\u6d3b\u52d5\u72c0\u614b\uff0c\u4f46\u5c0d\u65bc\u55ae\u4e00\u63d0\u793a\uff0c\u6211\u5011\u767c\u73fe\u55ae\u4e00\u7279\u5fb5\u5728\u55ae\u4e00\u5c64\u4e0a\u8655\u65bc\u6d3b\u52d5\u72c0\u614b\u7684\u53ef\u80fd\u6027\u8981\u9ad8\u5f97\u591a\u3002\u5c0d\u65bc\u8f03\u5927\u7684\u5e95\u5c64\u6a21\u578b\uff0c\u6211\u5011\u767c\u73fe\u6b98\u5dee\u4e32\u6d41\u4e2d\u76f8\u9130\u5c64\u4e4b\u9593\u7684\u9918\u5f26\u76f8\u4f3c\u6027\u8f03\u9ad8\uff0c\u56e0\u6b64\u6211\u5011\u9810\u671f\u6703\u6709\u66f4\u591a\u7279\u5fb5\u5728\u591a\u500b\u5c64\u4e0a\u8655\u65bc\u6d3b\u52d5\u72c0\u614b\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e MLSAE \u662f\u4e00\u7a2e\u7814\u7a76Transformer\u4e2d\u8cc7\u8a0a\u6d41\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002\u6211\u5011\u5728 https://github.com/tim-lawson/mlsae \u4e0a\u767c\u5e03\u4e86\u6211\u5011\u7528\u65bc\u8a13\u7df4\u548c\u5206\u6790 MLSAE \u7684\u7a0b\u5f0f\u78bc\u3002", "author": "Tim Lawson et.al.", "authors": "Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison", "id": "2409.04185v1", "paper_url": "http://arxiv.org/abs/2409.04185v1", "repo": "https://github.com/tim-lawson/mlsae"}}