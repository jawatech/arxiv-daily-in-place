{"2412.03800": {"publish_time": "2024-12-05", "title": "ELEMENT: Episodic and Lifelong Exploration via Maximum Entropy", "paper_summary": "This paper proposes \\emph{Episodic and Lifelong Exploration via Maximum\nENTropy} (ELEMENT), a novel, multiscale, intrinsically motivated reinforcement\nlearning (RL) framework that is able to explore environments without using any\nextrinsic reward and transfer effectively the learned skills to downstream\ntasks. We advance the state of the art in three ways. First, we propose a\nmultiscale entropy optimization to take care of the fact that previous maximum\nstate entropy, for lifelong exploration with millions of state observations,\nsuffers from vanishing rewards and becomes very expensive computationally\nacross iterations. Therefore, we add an episodic maximum entropy over each\nepisode to speedup the search further. Second, we propose a novel intrinsic\nreward for episodic entropy maximization named \\emph{average episodic state\nentropy} which provides the optimal solution for a theoretical upper bound of\nthe episodic state entropy objective. Third, to speed the lifelong entropy\nmaximization, we propose a $k$ nearest neighbors ($k$NN) graph to organize the\nestimation of the entropy and updating processes that reduces the computation\nsubstantially. Our ELEMENT significantly outperforms state-of-the-art intrinsic\nrewards in both episodic and lifelong setups. Moreover, it can be exploited in\ntask-agnostic pre-training, collecting data for offline reinforcement learning,\netc.", "paper_summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u3001\u5185\u5728\u52a8\u673a\u5f3a\u5316\u5b66\u4e60 (RL) \u6846\u67b6\uff0c\u540d\u4e3a\u201c\u901a\u8fc7\u6700\u5927\u71b5\u8fdb\u884c\u60c5\u666f\u548c\u7ec8\u8eab\u63a2\u7d22\u201d(ELEMENT)\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u5916\u5728\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u63a2\u7d22\u73af\u5883\uff0c\u5e76\u6709\u6548\u5730\u5c06\u6240\u5b66\u6280\u80fd\u8f6c\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u65b9\u9762\u63d0\u5347\u4e86\u6280\u672f\u6c34\u5e73\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u71b5\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u4ee5\u4e0b\u4e8b\u5b9e\uff1a\u5148\u524d\u7684\u6700\u5927\u72b6\u6001\u71b5\u5728\u8fdb\u884c\u6570\u767e\u4e07\u6b21\u72b6\u6001\u89c2\u5bdf\u7684\u7ec8\u8eab\u63a2\u7d22\u65f6\uff0c\u4f1a\u906d\u53d7\u5956\u52b1\u6d88\u5931\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u90fd\u4f1a\u53d8\u5f97\u975e\u5e38\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5728\u6bcf\u4e2a\u60c5\u666f\u4e2d\u6dfb\u52a0\u4e86\u4e00\u4e2a\u60c5\u666f\u6700\u5927\u71b5\uff0c\u4ee5\u8fdb\u4e00\u6b65\u52a0\u5feb\u641c\u7d22\u901f\u5ea6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u5728\u5956\u52b1\uff0c\u7528\u4e8e\u60c5\u666f\u71b5\u6700\u5927\u5316\uff0c\u540d\u4e3a\u201c\u5e73\u5747\u60c5\u666f\u72b6\u6001\u71b5\u201d\uff0c\u5b83\u4e3a\u60c5\u666f\u72b6\u6001\u71b5\u76ee\u6807\u7684\u7406\u8bba\u4e0a\u9650\u63d0\u4f9b\u4e86\u6700\u4f18\u89e3\u3002\u7b2c\u4e09\uff0c\u4e3a\u4e86\u52a0\u5feb\u7ec8\u8eab\u71b5\u6700\u5927\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a $k$ \u8fd1\u90bb ($k$NN) \u56fe\uff0c\u7528\u4e8e\u7ec4\u7ec7\u71b5\u7684\u4f30\u8ba1\u548c\u66f4\u65b0\u8fc7\u7a0b\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u3002\u6211\u4eec\u7684 ELEMENT \u5728\u60c5\u666f\u548c\u7ec8\u8eab\u8bbe\u7f6e\u4e2d\u90fd\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5185\u5728\u5956\u52b1\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u9884\u8bad\u7ec3\u3001\u6536\u96c6\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u7b49\u3002", "author": "Hongming Li et.al.", "authors": "Hongming Li, Shujian Yu, Bin Liu, Jose C. Principe", "id": "2412.03800v1", "paper_url": "http://arxiv.org/abs/2412.03800v1", "repo": "null"}}