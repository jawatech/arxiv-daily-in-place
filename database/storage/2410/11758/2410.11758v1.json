{"2410.11758": {"publish_time": "2024-10-15", "title": "Latent Action Pretraining from Videos", "paper_summary": "We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u901a\u7528\u52d5\u4f5c\u6a21\u578b\u7684\u6f5b\u5728\u52d5\u4f5c\u9810\u8a13\u7df4 (LAPA)\uff0c\u9019\u662f\u4e00\u7a2e\u975e\u76e3\u7763\u5f0f\u65b9\u6cd5\uff0c\u7528\u65bc\u9810\u8a13\u7df4\u8996\u89ba\u8a9e\u8a00\u52d5\u4f5c (VLA) \u6a21\u578b\uff0c\u800c\u7121\u9700\u5730\u9762\u771f\u5be6\u6a5f\u5668\u4eba\u52d5\u4f5c\u6a19\u7c64\u3002\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u52d5\u4f5c\u6a21\u578b\u9700\u8981\u5728\u9810\u8a13\u7df4\u671f\u9593\u7531\u4eba\u985e\u9059\u63a7\u64cd\u4f5c\u54e1\u6536\u96c6\u7684\u52d5\u4f5c\u6a19\u7c64\uff0c\u9019\u986f\u8457\u9650\u5236\u4e86\u53ef\u80fd\u7684\u6578\u64da\u4f86\u6e90\u548c\u898f\u6a21\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5f9e\u6c92\u6709\u6a5f\u5668\u4eba\u52d5\u4f5c\u6a19\u7c64\u7684\u7db2\u8def\u898f\u6a21\u5f71\u7247\u4e2d\u5b78\u7fd2\u7684\u65b9\u6cd5\u3002\u6211\u5011\u9996\u5148\u8a13\u7df4\u4e00\u500b\u52d5\u4f5c\u91cf\u5316\u6a21\u578b\uff0c\u5229\u7528\u57fa\u65bc VQ-VAE \u7684\u76ee\u6a19\u4f86\u5b78\u7fd2\u5f71\u50cf\u5e40\u4e4b\u9593\u7684\u96e2\u6563\u6f5b\u5728\u52d5\u4f5c\uff0c\u7136\u5f8c\u9810\u8a13\u7df4\u4e00\u500b\u6f5b\u5728 VLA \u6a21\u578b\uff0c\u5f9e\u89c0\u5bdf\u548c\u4efb\u52d9\u63cf\u8ff0\u4e2d\u9810\u6e2c\u9019\u4e9b\u6f5b\u5728\u52d5\u4f5c\uff0c\u6700\u5f8c\u5c0d VLA \u9032\u884c\u5fae\u8abf\uff0c\u5728\u5c0f\u898f\u6a21\u6a5f\u5668\u4eba\u64cd\u4f5c\u6578\u64da\u4e0a\u5f9e\u6f5b\u5728\u52d5\u4f5c\u6620\u5c04\u5230\u6a5f\u5668\u4eba\u52d5\u4f5c\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u8457\u512a\u65bc\u5f9e\u5927\u898f\u6a21\u5f71\u7247\u4e2d\u8a13\u7df4\u6a5f\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u73fe\u6709\u6280\u8853\u3002\u6b64\u5916\uff0c\u5b83\u512a\u65bc\u5728\u73fe\u5be6\u4e16\u754c\u64cd\u4f5c\u4efb\u52d9\u4e0a\u4f7f\u7528\u6a5f\u5668\u4eba\u52d5\u4f5c\u6a19\u7c64\u8a13\u7df4\u7684\u6700\u65b0 VLA \u6a21\u578b\uff0c\u9019\u4e9b\u4efb\u52d9\u9700\u8981\u8a9e\u8a00\u689d\u4ef6\u3001\u5c0d\u672a\u898b\u7269\u9ad4\u7684\u6982\u62ec\u4ee5\u53ca\u5c0d\u672a\u898b\u6307\u4ee4\u7684\u8a9e\u7fa9\u6982\u62ec\u3002\u50c5\u91dd\u5c0d\u4eba\u985e\u64cd\u4f5c\u5f71\u7247\u9032\u884c\u8a13\u7df4\u4e5f\u986f\u793a\u51fa\u6b63\u5411\u8f49\u79fb\uff0c\u9019\u70ba\u5229\u7528\u7db2\u8def\u898f\u6a21\u6578\u64da\u9032\u884c\u6a5f\u5668\u4eba\u57fa\u790e\u6a21\u578b\u958b\u95e2\u4e86\u53ef\u80fd\u6027\u3002", "author": "Seonghyeon Ye et.al.", "authors": "Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo", "id": "2410.11758v1", "paper_url": "http://arxiv.org/abs/2410.11758v1", "repo": "null"}}