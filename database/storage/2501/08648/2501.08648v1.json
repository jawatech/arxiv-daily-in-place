{"2501.08648": {"publish_time": "2025-01-15", "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities", "paper_summary": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining.", "paper_summary_zh": "\u5118\u7ba1\u6700\u521d\u662f\u70ba\u55ae\u5411\u751f\u6210\u5f0f\u5efa\u6a21\u800c\u8a2d\u8a08\uff0c\n\u50c5\u89e3\u78bc\u5668\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u537b\u65e5\u76ca\u88ab\u6539\u7de8\u70ba\n\u96d9\u5411\u5efa\u6a21\u3002\u7136\u800c\uff0c\u55ae\u5411\u548c\u96d9\u5411\u6a21\u578b\n\u901a\u5e38\u6703\u4ee5\u4e0d\u540c\u7684\u76ee\u6a19\uff08\u5206\u5225\u70ba\u7522\u751f\u548c\n\u8868\u793a\u5b78\u7fd2\uff09\u5206\u958b\u8a13\u7df4\u3002\u9019\u7a2e\u5206\u96e2\u5ffd\u8996\u4e86\n\u958b\u767c\u66f4\u901a\u7528\u8a9e\u8a00\u6a21\u578b\u7684\u6a5f\u6703\uff0c\u4ee5\u53ca\u9019\u4e9b\n\u76ee\u6a19\u76f8\u4e92\u88dc\u5145\u7684\u6a5f\u6703\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 MAGNET\uff0c\u4e00\u7a2e\u50c5\u89e3\u78bc\u5668 LLM \u7684\u6539\u7de8\uff0c\u5b83\u589e\u5f37\u4e86\u5b83\u5011\u751f\u6210\u7a69\u5065\n\u8868\u793a\u548c\u586b\u88dc\u7f3a\u5931\u6587\u5b57\u5340\u6bb5\u7684\u80fd\u529b\uff0c\u540c\u6642\u4fdd\u7559\u5b83\u5011\u7684\u77e5\u8b58\n\u548c\u6587\u5b57\u751f\u6210\u80fd\u529b\u3002MAGNET \u63a1\u7528\u4e09\u500b\u81ea\u6211\u76e3\u7763\u8a13\u7df4\n\u76ee\u6a19\uff0c\u4e26\u5f15\u5165\u4e00\u7a2e\u7d50\u5408\u96d9\u5411\n\u548c\u56e0\u679c\u6ce8\u610f\u529b\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u8b93\u6240\u6709\u76ee\u6a19\u90fd\u80fd\u9032\u884c\u7d71\u4e00\u8a13\u7df4\u3002\u6211\u5011\u7684\n\u7d50\u679c\u8b49\u660e\u4f7f\u7528 MAGNET \u6539\u7de8\u7684 LLM (1) \u5728\u6a19\u8a18\u5c64\u7d1a\u548c\u53e5\u5b50\u5c64\u7d1a\u8868\u793a\u5b78\u7fd2\u4efb\u52d9\u4e2d\u8d85\u8d8a\u5f37\u5927\u7684\u6587\u5b57\n\u7de8\u78bc\u5668\uff0c(2)\n\u5229\u7528\u672a\u4f86\u5167\u5bb9\u7522\u751f\u9069\u7576\u7684\u6587\u5b57\u586b\u88dc\uff0c\n(3) \u4fdd\u7559\u9032\u884c\u958b\u653e\u5f0f\u6587\u5b57\u751f\u6210\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u6703\u51fa\u73fe\n\u91cd\u8907\u7684\u554f\u984c\uff0c\u4ee5\u53ca (4) \u4fdd\u7559 LLM \u5728\n\u9810\u8a13\u7df4\u671f\u9593\u7372\u5f97\u7684\u77e5\u8b58\u3002", "author": "Savya Khosla et.al.", "authors": "Savya Khosla, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi", "id": "2501.08648v1", "paper_url": "http://arxiv.org/abs/2501.08648v1", "repo": "null"}}