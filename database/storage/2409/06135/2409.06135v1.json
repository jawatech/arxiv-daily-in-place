{"2409.06135": {"publish_time": "2024-09-10", "title": "Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis", "paper_summary": "Foley is a term commonly used in filmmaking, referring to the addition of\ndaily sound effects to silent films or videos to enhance the auditory\nexperience. Video-to-Audio (V2A), as a particular type of automatic foley task,\npresents inherent challenges related to audio-visual synchronization. These\nchallenges encompass maintaining the content consistency between the input\nvideo and the generated audio, as well as the alignment of temporal and\nloudness properties within the video. To address these issues, we construct a\ncontrollable video-to-audio synthesis model, termed Draw an Audio, which\nsupports multiple input instructions through drawn masks and loudness signals.\nTo ensure content consistency between the synthesized audio and target video,\nwe introduce the Mask-Attention Module (MAM), which employs masked video\ninstruction to enable the model to focus on regions of interest. Additionally,\nwe implement the Time-Loudness Module (TLM), which uses an auxiliary loudness\nsignal to ensure the synthesis of sound that aligns with the video in both\nloudness and temporal dimensions. Furthermore, we have extended a large-scale\nV2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive\nexperiments on challenging benchmarks across two large-scale V2A datasets\nverify Draw an Audio achieves the state-of-the-art. Project page:\nhttps://yannqi.github.io/Draw-an-Audio/.", "paper_summary_zh": "\u798f\u5229\u662f\u96fb\u5f71\u88fd\u4f5c\u4e2d\u5e38\u7528\u7684\u8853\u8a9e\uff0c\u6307\u7684\u662f\u5728\u9ed8\u7247\u6216\u5f71\u7247\u4e2d\u52a0\u5165\u65e5\u5e38\u97f3\u6548\uff0c\u4ee5\u589e\u5f37\u807d\u89ba\u9ad4\u9a57\u3002\u5f71\u7247\u8f49\u97f3\u8a0a (V2A) \u662f\u4e00\u7a2e\u7279\u6b8a\u7684\u81ea\u52d5\u798f\u5229\u4efb\u52d9\uff0c\u5728\u97f3\u8a0a\u8996\u89ba\u540c\u6b65\u65b9\u9762\u5b58\u5728\u56fa\u6709\u7684\u6311\u6230\u3002\u9019\u4e9b\u6311\u6230\u5305\u62ec\u5728\u8f38\u5165\u5f71\u7247\u548c\u7522\u751f\u7684\u97f3\u8a0a\u4e4b\u9593\u7dad\u6301\u5167\u5bb9\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5f71\u7247\u4e2d\u6642\u9593\u548c\u97ff\u5ea6\u7684\u5c6c\u6027\u5c0d\u9f4a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u53ef\u63a7\u7684\u5f71\u7247\u8f49\u97f3\u8a0a\u5408\u6210\u6a21\u578b\uff0c\u7a31\u70ba\u7e6a\u88fd\u97f3\u8a0a\uff0c\u5b83\u652f\u63f4\u900f\u904e\u7e6a\u88fd\u906e\u7f69\u548c\u97ff\u5ea6\u8a0a\u865f\u63d0\u4f9b\u591a\u91cd\u8f38\u5165\u6307\u4ee4\u3002\u70ba\u4e86\u78ba\u4fdd\u5408\u6210\u97f3\u8a0a\u548c\u76ee\u6a19\u5f71\u7247\u4e4b\u9593\u7684\u5167\u5bb9\u4e00\u81f4\u6027\uff0c\u6211\u5011\u5f15\u5165\u4e86\u906e\u7f69\u6ce8\u610f\u6a21\u7d44 (MAM)\uff0c\u5b83\u63a1\u7528\u906e\u7f69\u5f71\u7247\u6307\u4ee4\uff0c\u8b93\u6a21\u578b\u80fd\u5920\u5c08\u6ce8\u65bc\u611f\u8208\u8da3\u7684\u5340\u57df\u3002\u6b64\u5916\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u6642\u9593\u97ff\u5ea6\u6a21\u7d44 (TLM)\uff0c\u5b83\u4f7f\u7528\u8f14\u52a9\u97ff\u5ea6\u8a0a\u865f\u4f86\u78ba\u4fdd\u5408\u6210\u97f3\u8a0a\u5728\u97ff\u5ea6\u548c\u6642\u9593\u7dad\u5ea6\u4e0a\u8207\u5f71\u7247\u5c0d\u9f4a\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u8a3b\u89e3\u5b57\u5e55\u63d0\u793a\uff0c\u64f4\u5145\u4e86\u4e00\u500b\u540d\u70ba VGGSound-Caption \u7684\u5927\u578b V2A \u8cc7\u6599\u96c6\u3002\u5728\u5169\u500b\u5927\u578b V2A \u8cc7\u6599\u96c6\u4e2d\u7684\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u9a57\u8b49\uff0c\u7e6a\u88fd\u97f3\u8a0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6280\u8853\u6c34\u6e96\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://yannqi.github.io/Draw-an-Audio/\u3002", "author": "Qi Yang et.al.", "authors": "Qi Yang, Binjie Mao, Zili Wang, Xing Nie, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang", "id": "2409.06135v1", "paper_url": "http://arxiv.org/abs/2409.06135v1", "repo": "null"}}