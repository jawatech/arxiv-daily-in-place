{"2410.06625": {"publish_time": "2024-10-09", "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time", "paper_summary": "Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\u5df2\u6210\u70ba\u591a\u6a21\u614b\u667a\u6167\u7684\u95dc\u9375\u9aa8\u5e79\uff0c\u4f46\u56b4\u91cd\u7684\u5b89\u5168\u6027\u6311\u6230\u9650\u5236\u4e86\u5b83\u5011\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u7684\u61c9\u7528\u3002\u96d6\u7136\u6587\u5b57\u8f38\u5165\u901a\u5e38\u53d7\u5230\u6709\u6548\u4fdd\u969c\uff0c\u4f46\u5c0d\u6297\u6027\u7684\u8996\u89ba\u8f38\u5165\u53ef\u4ee5\u8f15\u6613\u7e5e\u904e VLM \u9632\u79a6\u6a5f\u5236\u3002\u73fe\u6709\u7684\u9632\u79a6\u65b9\u6cd5\u4e0d\u662f\u8cc7\u6e90\u5bc6\u96c6\u578b\uff08\u9700\u8981\u5927\u91cf\u7684\u6578\u64da\u548c\u904b\u7b97\uff09\uff0c\u5c31\u662f\u7121\u6cd5\u540c\u6642\u78ba\u4fdd\u56de\u61c9\u7684\u5b89\u5168\u6027\u8207\u5be6\u7528\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5169\u968e\u6bb5\u63a8\u8ad6\u6642\u9593\u5c0d\u9f4a\u67b6\u69cb\uff0c\u5373\u5148\u8a55\u4f30\u518d\u5c0d\u9f4a\uff08ETA\uff09\uff1a1\uff09\u8a55\u4f30\u8f38\u5165\u8996\u89ba\u5167\u5bb9\u548c\u8f38\u51fa\u56de\u61c9\uff0c\u4ee5\u5728\u591a\u6a21\u614b\u8a2d\u7f6e\u4e2d\u5efa\u7acb\u7a69\u5065\u7684\u5b89\u5168\u610f\u8b58\uff0c\u4ee5\u53ca 2\uff09\u900f\u904e\u4f7f\u7528\u5e72\u64fe\u524d\u7db4\u8abf\u6574 VLM \u7684\u751f\u6210\u5206\u4f48\uff0c\u4e26\u57f7\u884c\u53e5\u5b50\u5c64\u7d1a\u7684\u6700\u4f73 N \u9078\u64c7\uff0c\u4ee5\u641c\u5c0b\u6700\u7121\u5bb3\u4e14\u6709\u7528\u7684\u751f\u6210\u8def\u5f91\uff0c\u5f9e\u800c\u8abf\u6574\u6dfa\u5c64\u548c\u6df1\u5c64\u7684\u4e0d\u5b89\u5168\u884c\u70ba\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u986f\u793a\uff0cETA \u5728\u7121\u5bb3\u6027\u3001\u6709\u7528\u6027\u548c\u6548\u7387\u65b9\u9762\u512a\u65bc\u57fa\u7dda\u65b9\u6cd5\uff0c\u5728\u8de8\u6a21\u5f0f\u653b\u64ca\u4e2d\u5c07\u4e0d\u5b89\u5168\u7387\u964d\u4f4e\u4e86 87.5%\uff0c\u4e26\u5728 GPT-4 \u6709\u7528\u6027\u8a55\u4f30\u4e2d\u5be6\u73fe\u4e86 96.6% \u7684\u7372\u52dd\u5e73\u5c40\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u767c\u5e03\u65bc https://github.com/DripNowhy/ETA\u3002", "author": "Yi Ding et.al.", "authors": "Yi Ding, Bolian Li, Ruqi Zhang", "id": "2410.06625v1", "paper_url": "http://arxiv.org/abs/2410.06625v1", "repo": "https://github.com/dripnowhy/eta"}}