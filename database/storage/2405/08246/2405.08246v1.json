{"2405.08246": {"publish_time": "2024-05-14", "title": "Compositional Text-to-Image Generation with Dense Blob Representations", "paper_summary": "Existing text-to-image models struggle to follow complex text prompts,\nraising the need for extra grounding inputs for better controllability. In this\nwork, we propose to decompose a scene into visual primitives - denoted as dense\nblob representations - that contain fine-grained details of the scene while\nbeing modular, human-interpretable, and easy-to-construct. Based on blob\nrepresentations, we develop a blob-grounded text-to-image diffusion model,\ntermed BlobGEN, for compositional generation. Particularly, we introduce a new\nmasked cross-attention module to disentangle the fusion between blob\nrepresentations and visual features. To leverage the compositionality of large\nlanguage models (LLMs), we introduce a new in-context learning approach to\ngenerate blob representations from text prompts. Our extensive experiments show\nthat BlobGEN achieves superior zero-shot generation quality and better\nlayout-guided controllability on MS-COCO. When augmented by LLMs, our method\nexhibits superior numerical and spatial correctness on compositional image\ngeneration benchmarks. Project page: https://blobgen-2d.github.io.", "paper_summary_zh": "\u73fe\u6709\u7684\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u96e3\u4ee5\u9075\u5faa\u8907\u96dc\u7684\u6587\u5b57\u63d0\u793a\uff0c\u56e0\u6b64\u9700\u8981\u984d\u5916\u7684\u57fa\u790e\u8f38\u5165\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u53ef\u63a7\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u5c07\u5834\u666f\u5206\u89e3\u70ba\u8996\u89ba\u57fa\u5143 - \u8868\u793a\u70ba\u5bc6\u96c6\u7684\u6591\u9ede\u8868\u793a - \u5176\u4e2d\u5305\u542b\u5834\u666f\u7684\u7d30\u7dfb\u7d30\u7bc0\uff0c\u540c\u6642\u5177\u6709\u6a21\u7d44\u5316\u3001\u4eba\u985e\u53ef\u89e3\u8b80\u4e14\u6613\u65bc\u5efa\u69cb\u7684\u7279\u6027\u3002\u57fa\u65bc\u6591\u9ede\u8868\u793a\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u6591\u9ede\u57fa\u790e\u7684\u6587\u5b57\u8f49\u5716\u50cf\u64f4\u6563\u6a21\u578b\uff0c\u7a31\u70ba BlobGEN\uff0c\u7528\u65bc\u5408\u6210\u751f\u6210\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u906e\u7f69\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u7d44\uff0c\u4ee5\u89e3\u958b\u6591\u9ede\u8868\u793a\u548c\u8996\u89ba\u7279\u5fb5\u4e4b\u9593\u7684\u878d\u5408\u3002\u70ba\u4e86\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7d44\u5408\u6027\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u60c5\u5883\u5167\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4ee5\u5f9e\u6587\u5b57\u63d0\u793a\u4e2d\u751f\u6210\u6591\u9ede\u8868\u793a\u3002\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cBlobGEN \u5728 MS-COCO \u4e0a\u5be6\u73fe\u4e86\u51fa\u8272\u7684\u96f6\u6b21\u751f\u6210\u54c1\u8cea\u548c\u66f4\u597d\u7684\u4f48\u5c40\u5f15\u5c0e\u53ef\u63a7\u6027\u3002\u7576\u7531 LLM \u589e\u5f37\u6642\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5408\u6210\u5716\u50cf\u751f\u6210\u57fa\u6e96\u4e0a\u5c55\u73fe\u51fa\u512a\u7570\u7684\u6578\u5b57\u548c\u7a7a\u9593\u6b63\u78ba\u6027\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://blobgen-2d.github.io\u3002", "author": "Weili Nie et.al.", "authors": "Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, Arash Vahdat", "id": "2405.08246v1", "paper_url": "http://arxiv.org/abs/2405.08246v1", "repo": "null"}}