{"2501.12033": {"publish_time": "2025-01-21", "title": "Harnessing Generative Pre-Trained Transformer for Datacenter Packet Trace Generation", "paper_summary": "Today, the rapid growth of applications reliant on datacenters calls for new\nadvancements to meet the increasing traffic and computational demands. Traffic\ntraces from datacenters are essential for further development and optimization\nof future datacenters. However, traces are rarely released to the public.\nResearchers often use simplified mathematical models that lack the depth needed\nto recreate intricate traffic patterns and, thus, miss optimization\nopportunities found in realistic traffic. In this preliminary work, we\nintroduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on\nthe generative pre-trained transformer (GPT) architecture used by many\nstate-of-the-art large language models. We train our model on a small set of\navailable traffic traces from different domains and offer a simple methodology\nto evaluate the fidelity of the generated traces to their original\ncounterparts. We show that DTG-GPT can synthesize novel traces that mimic the\nspatiotemporal patterns found in real traffic traces. We further demonstrate\nthat DTG-GPT can generate traces for networks of different scales while\nmaintaining fidelity. Our findings indicate the potential that, in the future,\nsimilar models to DTG-GPT will allow datacenter operators to release traffic\ninformation to the research community via trained GPT models.", "paper_summary_zh": "<paragraph>\u5982\u4eca\uff0c\u4ef0\u8cf4\u8cc7\u6599\u4e2d\u5fc3\u7684\u61c9\u7528\u7a0b\u5f0f\u5feb\u901f\u6210\u9577\uff0c\u547c\u7c72\u65b0\u7684\u9032\u5c55\u4ee5\u6eff\u8db3\u65e5\u76ca\u589e\u52a0\u7684\u6d41\u91cf\u548c\u8a08\u7b97\u9700\u6c42\u3002\u8cc7\u6599\u4e2d\u5fc3\u7684\u6d41\u91cf\u8ffd\u8e64\u5c0d\u65bc\u672a\u4f86\u8cc7\u6599\u4e2d\u5fc3\u7684\u9032\u4e00\u6b65\u958b\u767c\u548c\u6700\u4f73\u5316\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u8ffd\u8e64\u5f88\u5c11\u516c\u958b\u767c\u5e03\u3002\u7814\u7a76\u4eba\u54e1\u7d93\u5e38\u4f7f\u7528\u7c21\u5316\u7684\u6578\u5b78\u6a21\u578b\uff0c\u800c\u9019\u4e9b\u6a21\u578b\u7f3a\u4e4f\u91cd\u65b0\u5efa\u7acb\u8907\u96dc\u6d41\u91cf\u6a21\u5f0f\u6240\u9700\u7684\u6df1\u5ea6\uff0c\u56e0\u6b64\u932f\u5931\u4e86\u5728\u5be6\u969b\u6d41\u91cf\u4e2d\u767c\u73fe\u7684\u6700\u4f73\u5316\u6a5f\u6703\u3002\u5728\u9019\u9805\u521d\u6b65\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 DTG-GPT\uff0c\u4e00\u7a2e\u57fa\u65bc\u8a31\u591a\u6700\u5148\u9032\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u6240\u4f7f\u7528\u7684\u751f\u6210\u5f0f\u9810\u5148\u8a13\u7df4Transformer (GPT) \u67b6\u69cb\u7684\u5c01\u5305\u7b49\u7d1a\u8cc7\u6599\u4e2d\u5fc3\u6d41\u91cf\u7522\u751f\u5668 (DTG)\u3002\u6211\u5011\u5728\u4f86\u81ea\u4e0d\u540c\u7db2\u57df\u7684\u4e00\u5c0f\u7d44\u53ef\u7528\u6d41\u91cf\u8ffd\u8e64\u4e0a\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u4e26\u63d0\u4f9b\u4e00\u500b\u7c21\u55ae\u7684\u65b9\u6cd5\u4f86\u8a55\u4f30\u6240\u7522\u751f\u8ffd\u8e64\u8207\u5176\u539f\u59cb\u5c0d\u61c9\u9805\u7684\u4fdd\u771f\u5ea6\u3002\u6211\u5011\u5c55\u793a DTG-GPT \u53ef\u4ee5\u5408\u6210\u65b0\u7684\u8ffd\u8e64\uff0c\u6a21\u64ec\u5728\u5be6\u969b\u6d41\u91cf\u8ffd\u8e64\u4e2d\u767c\u73fe\u7684\u6642\u7a7a\u6a21\u5f0f\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c55\u793a DTG-GPT \u53ef\u4ee5\u70ba\u4e0d\u540c\u898f\u6a21\u7684\u7db2\u8def\u7522\u751f\u8ffd\u8e64\uff0c\u540c\u6642\u7dad\u6301\u4fdd\u771f\u5ea6\u3002\u6211\u5011\u7684\u767c\u73fe\u6307\u51fa\uff0c\u5728\u672a\u4f86\uff0c\u985e\u4f3c\u65bc DTG-GPT \u7684\u6a21\u578b\u5c07\u5141\u8a31\u8cc7\u6599\u4e2d\u5fc3\u71df\u904b\u5546\u900f\u904e\u8a13\u7df4\u904e\u7684 GPT \u6a21\u578b\u5411\u7814\u7a76\u793e\u7fa4\u767c\u5e03\u6d41\u91cf\u8cc7\u8a0a\u3002</paragraph>", "author": "Chen Griner et.al.", "authors": "Chen Griner", "id": "2501.12033v1", "paper_url": "http://arxiv.org/abs/2501.12033v1", "repo": "null"}}