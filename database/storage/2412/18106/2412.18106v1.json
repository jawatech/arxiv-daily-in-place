{"2412.18106": {"publish_time": "2024-12-24", "title": "Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels", "paper_summary": "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well.", "paper_summary_zh": "<paragraph>\u4e3a\u4e86\u6ee1\u8db3\u751f\u4ea7\u7ea7\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u670d\u52a1\u7cfb\u7edf\u5bf9\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u6548\u7387\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\uff0c\u9700\u8981\u96c6\u6210\u5148\u8fdb\u7684\u4f18\u5316\u6280\u672f\u3002\u7136\u800c\uff0cLLM \u52a8\u6001\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\uff0c\u52a0\u4e0a\u8fd9\u4e9b\u4f18\u5316\uff0c\u52a0\u5267\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u53d8\u6027\u7684\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u5728 AI \u52a0\u901f\u5668\u4e0a\u4fdd\u6301\u9ad8\u6548\u7387\uff0c\u5c24\u5176\u662f\u5177\u6709\u57fa\u4e8e\u56fe\u5757\u7684\u7f16\u7a0b\u6a21\u578b\u7684 DSA\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 XY-Serve\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u3001Ascend \u539f\u751f\u7684\u7aef\u5230\u7aef\u751f\u4ea7 LLM \u670d\u52a1\u7cfb\u7edf\u3002\u6838\u5fc3\u601d\u60f3\u662f\u4e00\u79cd\u62bd\u8c61\u673a\u5236\uff0c\u5b83\u901a\u8fc7\u5c06\u8ba1\u7b97\u5206\u89e3\u4e3a\u7edf\u4e00\u7684\u3001\u786c\u4ef6\u53cb\u597d\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u5143\u57fa\u5143\u6765\u5e73\u6ed1\u5de5\u4f5c\u8d1f\u8f7d\u7684\u53ef\u53d8\u6027\u3002\u5bf9\u4e8e\u6ce8\u610f\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u5185\u6838\uff0c\u5b83\u8ba1\u7b97\u5177\u6709\u67b6\u6784\u611f\u77e5\u56fe\u5757\u5927\u5c0f\u7684 matmul-softmax-matmul \u7684\u57fa\u672c\u6a21\u5f0f\u3002\u5bf9\u4e8e GEMM\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u865a\u62df\u586b\u5145\u65b9\u6848\uff0c\u5b83\u53ef\u4ee5\u9002\u5e94\u52a8\u6001\u5f62\u72b6\u53d8\u5316\uff0c\u540c\u65f6\u4f7f\u7528\u5177\u6709\u5404\u79cd\u56fa\u5b9a\u56fe\u5757\u5927\u5c0f\u7684\u9ad8\u6548 GEMM \u57fa\u5143\u3002XY-Serve \u4e0e vLLM \u5b8c\u7f8e\u5951\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e Ascend NPU \u4e0a\u5f53\u524d\u516c\u5f00\u53ef\u7528\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 89%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684 GEMM\uff08\u5e73\u5747\u5feb 14.6%\uff09\u548c\u6ce8\u610f\u529b\uff08\u5e73\u5747\u5feb 21.5%\uff09\u5185\u6838\uff0c\u76f8\u5bf9\u4e8e\u73b0\u6709\u7684\u5e93\u800c\u8a00\u3002\u867d\u7136\u8fd9\u9879\u5de5\u4f5c\u662f Ascend \u539f\u751f\u7684\uff0c\u4f46\u6211\u4eec\u76f8\u4fe1\u8be5\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5e94\u7528\u4e8e SIMT \u67b6\u6784\u3002</paragraph>", "author": "Mingcong Song et.al.", "authors": "Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long", "id": "2412.18106v1", "paper_url": "http://arxiv.org/abs/2412.18106v1", "repo": "null"}}