{"2408.06150": {"publish_time": "2024-08-12", "title": "LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library", "paper_summary": "In this study, we generate and maintain a database of 10 million virtual\nlipids through METiS's in-house de novo lipid generation algorithms and lipid\nvirtual screening techniques. These virtual lipids serve as a corpus for\npre-training, lipid representation learning, and downstream task knowledge\ntransfer, culminating in state-of-the-art LNP property prediction performance.\nWe propose LipidBERT, a BERT-like model pre-trained with the Masked Language\nModel (MLM) and various secondary tasks. Additionally, we compare the\nperformance of embeddings generated by LipidBERT and PhatGPT, our GPT-like\nlipid generation model, on downstream tasks. The proposed bilingual LipidBERT\nmodel operates in two languages: the language of ionizable lipid pre-training,\nusing in-house dry-lab lipid structures, and the language of LNP fine-tuning,\nutilizing in-house LNP wet-lab data. This dual capability positions LipidBERT\nas a key AI-based filter for future screening tasks, including new versions of\nMETiS de novo lipid libraries and, more importantly, candidates for in vivo\ntesting for orgran-targeting LNPs. To the best of our knowledge, this is the\nfirst successful demonstration of the capability of a pre-trained language\nmodel on virtual lipids and its effectiveness in downstream tasks using web-lab\ndata. This work showcases the clever utilization of METiS's in-house de novo\nlipid library as well as the power of dry-wet lab integration.", "paper_summary_zh": "\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e METiS \u5167\u90e8\u7684\u65b0\u751f\u8102\u8cea\u751f\u6210\u6f14\u7b97\u6cd5\u548c\u8102\u8cea\u865b\u64ec\u7be9\u9078\u6280\u8853\uff0c\u7522\u751f\u4e26\u7dad\u8b77\u4e00\u500b\u5305\u542b 1000 \u842c\u7a2e\u865b\u64ec\u8102\u8cea\u7684\u8cc7\u6599\u5eab\u3002\u9019\u4e9b\u865b\u64ec\u8102\u8cea\u4f5c\u70ba\u9810\u5148\u8a13\u7df4\u3001\u8102\u8cea\u8868\u5fb5\u5b78\u7fd2\u548c\u4e0b\u6e38\u4efb\u52d9\u77e5\u8b58\u8f49\u79fb\u7684\u8a9e\u6599\u5eab\uff0c\u6700\u7d42\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684 LNP \u5c6c\u6027\u9810\u6e2c\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa\u4e86 LipidBERT\uff0c\u9019\u662f\u4e00\u500b\u4f7f\u7528\u906e\u7f69\u8a9e\u8a00\u6a21\u578b (MLM) \u548c\u5404\u7a2e\u6b21\u8981\u4efb\u52d9\u9810\u5148\u8a13\u7df4\u7684\u985e BERT \u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u6bd4\u8f03\u4e86 LipidBERT \u548c PhatGPT\uff08\u6211\u5011\u7684\u985e GPT \u8102\u8cea\u751f\u6210\u6a21\u578b\uff09\u7522\u751f\u7684\u5d4c\u5165\u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002\u6240\u63d0\u51fa\u7684\u96d9\u8a9e LipidBERT \u6a21\u578b\u4f7f\u7528\u5169\u7a2e\u8a9e\u8a00\uff1a\u53ef\u96fb\u96e2\u8102\u8cea\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\uff08\u4f7f\u7528\u5167\u90e8\u4e7e\u5f0f\u5be6\u9a57\u5ba4\u8102\u8cea\u7d50\u69cb\uff09\u548c LNP \u5fae\u8abf\u7684\u8a9e\u8a00\uff08\u4f7f\u7528\u5167\u90e8 LNP \u6fd5\u5f0f\u5be6\u9a57\u5ba4\u8cc7\u6599\uff09\u3002\u9019\u7a2e\u96d9\u91cd\u80fd\u529b\u5c07 LipidBERT \u5b9a\u4f4d\u70ba\u672a\u4f86\u7be9\u9078\u4efb\u52d9\uff08\u5305\u62ec METiS \u65b0\u751f\u8102\u8cea\u5eab\u7684\u65b0\u7248\u672c\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u91dd\u5c0d\u5668\u5b98\u9776\u5411 LNP \u7684\u9ad4\u5167\u6e2c\u8a66\u5019\u9078\u85e5\u7269\uff09\u7684\u95dc\u9375 AI \u7be9\u9078\u5668\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u9810\u5148\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u5728\u865b\u64ec\u8102\u8cea\u4e0a\u7684\u80fd\u529b\u9996\u6b21\u6210\u529f\u5c55\u793a\uff0c\u4ee5\u53ca\u5b83\u5728\u4f7f\u7528\u7db2\u8def\u5be6\u9a57\u5ba4\u8cc7\u6599\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u7684\u6709\u6548\u6027\u3002\u9019\u9805\u5de5\u4f5c\u5c55\u793a\u4e86\u5de7\u5999\u5229\u7528 METiS \u5167\u90e8\u7684\u65b0\u751f\u8102\u8cea\u5eab\uff0c\u4ee5\u53ca\u4e7e\u6fd5\u5be6\u9a57\u5ba4\u6574\u5408\u7684\u529b\u91cf\u3002", "author": "Tianhao Yu et.al.", "authors": "Tianhao Yu, Cai Yao, Zhuorui Sun, Feng Shi, Lin Zhang, Kangjie Lyu, Xuan Bai, Andong Liu, Xicheng Zhang, Jiali Zou, Wenshou Wang, Chris Lai, Kai Wang", "id": "2408.06150v1", "paper_url": "http://arxiv.org/abs/2408.06150v1", "repo": "null"}}