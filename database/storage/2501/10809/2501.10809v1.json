{"2501.10809": {"publish_time": "2025-01-18", "title": "Efficient Auto-Labeling of Large-Scale Poultry Datasets (ALPD) Using Semi-Supervised Models, Active Learning, and Prompt-then-Detect Approach", "paper_summary": "The rapid growth of AI in poultry farming has highlighted the challenge of\nefficiently labeling large, diverse datasets. Manual annotation is\ntime-consuming, making it impractical for modern systems that continuously\ngenerate data. This study explores semi-supervised auto-labeling methods,\nintegrating active learning, and prompt-then-detect paradigm to develop an\nefficient framework for auto-labeling of large poultry datasets aimed at\nadvancing AI-driven behavior and health monitoring. Viideo data were collected\nfrom broilers and laying hens housed at the University of Arkansas and the\nUniversity of Georgia. The collected videos were converted into images,\npre-processed, augmented, and labeled. Various machine learning models,\nincluding zero-shot models like Grounding DINO, YOLO-World, and CLIP, and\nsupervised models like YOLO and Faster-RCNN, were utilized for broilers, hens,\nand behavior detection. The results showed that YOLOv8s-World and YOLOv9s\nperformed better when compared performance metrics for broiler and hen\ndetection under supervised learning, while among the semi-supervised model,\nYOLOv8s-ALPD achieved the highest precision (96.1%) and recall (99.0%) with an\nRMSE of 1.9. The hybrid YOLO-World model, incorporating the optimal YOLOv8s\nbackbone, demonstrated the highest overall performance. It achieved a precision\nof 99.2%, recall of 99.4%, and an F1 score of 98.7% for breed detection,\nalongside a precision of 88.4%, recall of 83.1%, and an F1 score of 84.5% for\nindividual behavior detection. Additionally, semi-supervised models showed\nsignificant improvements in behavior detection, achieving up to 31% improvement\nin precision and 16% in F1-score. The semi-supervised models with minimal\nactive learning reduced annotation time by over 80% compared to full manual\nlabeling. Moreover, integrating zero-shot models enhanced detection and\nbehavior identification.", "paper_summary_zh": "<paragraph>\u5bb6\u79bd\u517b\u6b96\u4e2d\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u589e\u957f\u51f8\u663e\u4e86\u9ad8\u6548\u6807\u6ce8\u5927\u578b\u3001\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u6311\u6218\u3002\u624b\u52a8\u6807\u6ce8\u975e\u5e38\u8017\u65f6\uff0c\u5bf9\u4e8e\u6301\u7eed\u751f\u6210\u6570\u636e\u7684\u73b0\u4ee3\u7cfb\u7edf\u800c\u8a00\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u534a\u76d1\u7763\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u63d0\u793a\u518d\u68c0\u6d4b\u8303\u5f0f\uff0c\u4ee5\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u6807\u6ce8\u5927\u578b\u5bb6\u79bd\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u884c\u4e3a\u548c\u5065\u5eb7\u76d1\u6d4b\u3002\u89c6\u9891\u6570\u636e\u662f\u4ece\u963f\u80af\u8272\u5927\u5b66\u548c\u4f50\u6cbb\u4e9a\u5927\u5b66\u9972\u517b\u7684\u8089\u9e21\u548c\u86cb\u9e21\u4e2d\u6536\u96c6\u7684\u3002\u6536\u96c6\u7684\u89c6\u9891\u88ab\u8f6c\u6362\u6210\u56fe\u50cf\uff0c\u7ecf\u8fc7\u9884\u5904\u7406\u3001\u589e\u5f3a\u548c\u6807\u6ce8\u3002\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec Grounding DINO\u3001YOLO-World \u548c CLIP \u7b49\u96f6\u6837\u672c\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u53ca YOLO \u548c Faster-RCNN \u7b49\u76d1\u7763\u6a21\u578b\uff0c\u88ab\u7528\u4e8e\u8089\u9e21\u3001\u6bcd\u9e21\u548c\u884c\u4e3a\u68c0\u6d4b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u76d1\u7763\u5b66\u4e60\u4e0b\uff0cYOLOv8s-World \u548c YOLOv9s \u5728\u8089\u9e21\u548c\u6bcd\u9e21\u68c0\u6d4b\u7684\u6027\u80fd\u6307\u6807\u6bd4\u8f83\u4e2d\u8868\u73b0\u5f97\u66f4\u597d\uff0c\u800c\u5728\u534a\u76d1\u7763\u6a21\u578b\u4e2d\uff0cYOLOv8s-ALPD \u4ee5 1.9 \u7684 RMSE \u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u7cbe\u5ea6 (96.1%) \u548c\u53ec\u56de\u7387 (99.0%)\u3002\u7ed3\u5408\u4e86\u6700\u4f73 YOLOv8s \u4e3b\u5e72\u7f51\u7edc\u7684\u6df7\u5408 YOLO-World \u6a21\u578b\u5c55\u793a\u4e86\u6700\u9ad8\u7684\u6574\u4f53\u6027\u80fd\u3002\u5b83\u5728\u54c1\u79cd\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86 99.2% \u7684\u7cbe\u5ea6\u300199.4% \u7684\u53ec\u56de\u7387\u548c 98.7% \u7684 F1 \u5206\u6570\uff0c\u5728\u4e2a\u4f53\u884c\u4e3a\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86 88.4% \u7684\u7cbe\u5ea6\u300183.1% \u7684\u53ec\u56de\u7387\u548c 84.5% \u7684 F1 \u5206\u6570\u3002\u6b64\u5916\uff0c\u534a\u76d1\u7763\u6a21\u578b\u5728\u884c\u4e3a\u68c0\u6d4b\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5728\u7cbe\u5ea6\u4e0a\u63d0\u9ad8\u4e86 31%\uff0c\u5728 F1 \u5206\u6570\u4e0a\u63d0\u9ad8\u4e86 16%\u3002\u4e0e\u5b8c\u5168\u624b\u52a8\u6807\u6ce8\u76f8\u6bd4\uff0c\u5177\u6709\u6700\u5c11\u4e3b\u52a8\u5b66\u4e60\u7684\u534a\u76d1\u7763\u6a21\u578b\u5c06\u6807\u6ce8\u65f6\u95f4\u51cf\u5c11\u4e86 80% \u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u96c6\u6210\u96f6\u6837\u672c\u5b66\u4e60\u6a21\u578b\u589e\u5f3a\u4e86\u68c0\u6d4b\u548c\u884c\u4e3a\u8bc6\u522b\u3002</paragraph>", "author": "Ramesh Bahadur Bist et.al.", "authors": "Ramesh Bahadur Bist, Lilong Chai, Shawna Weimer, Hannah Atungulua, Chantel Pennicott, Xiao Yang, Sachin Subedi, Chaitanya Pallerla, Yang Tian, Dongyi Wang", "id": "2501.10809v1", "paper_url": "http://arxiv.org/abs/2501.10809v1", "repo": "null"}}