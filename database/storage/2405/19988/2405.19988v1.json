{"2405.19988": {"publish_time": "2024-05-30", "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics", "paper_summary": "Natural language is often the easiest and most convenient modality for humans\nto specify tasks for robots. However, learning to ground language to behavior\ntypically requires impractical amounts of diverse, language-annotated\ndemonstrations collected on each target robot. In this work, we aim to separate\nthe problem of what to accomplish from how to accomplish it, as the former can\nbenefit from substantial amounts of external observation-only data, and only\nthe latter depends on a specific robot embodiment. To this end, we propose\nVideo-Language Critic, a reward model that can be trained on readily available\ncross-embodiment data using contrastive learning and a temporal ranking\nobjective, and use it to score behavior traces from a separate reinforcement\nlearning actor. When trained on Open X-Embodiment data, our reward model\nenables 2x more sample-efficient policy training on Meta-World tasks than a\nsparse reward only, despite a significant domain gap. Using in-domain data but\nin a challenging task generalization setting on Meta-World, we further\ndemonstrate more sample-efficient training than is possible with prior\nlanguage-conditioned reward models that are either trained with binary\nclassification, use static images, or do not leverage the temporal information\npresent in video data.", "paper_summary_zh": "\u81ea\u7136\u8a9e\u8a00\u901a\u5e38\u662f\u4eba\u985e\u70ba\u6a5f\u5668\u4eba\u6307\u5b9a\u4efb\u52d9\u6700\u5bb9\u6613\u3001\u6700\u65b9\u4fbf\u7684\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u5b78\u7fd2\u5c07\u8a9e\u8a00\u8f49\u5316\u70ba\u884c\u70ba\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u3001\u591a\u6a23\u5316\u7684\u3001\u5728\u6bcf\u500b\u76ee\u6a19\u6a5f\u5668\u4eba\u4e0a\u6536\u96c6\u7684\u8a9e\u8a00\u8a3b\u91cb\u793a\u7bc4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7684\u76ee\u6a19\u662f\u5c07\u5982\u4f55\u5b8c\u6210\u4efb\u52d9\u7684\u554f\u984c\u8207\u5982\u4f55\u5b8c\u6210\u4efb\u52d9\u7684\u554f\u984c\u5206\u958b\uff0c\u56e0\u70ba\u524d\u8005\u53ef\u4ee5\u5f9e\u5927\u91cf\u7684\u5916\u90e8\u89c0\u5bdf\u6578\u64da\u4e2d\u53d7\u76ca\uff0c\u800c\u5f8c\u8005\u50c5\u4f9d\u8cf4\u65bc\u5177\u9ad4\u6a5f\u5668\u4eba\u7684\u5177\u9ad4\u5be6\u65bd\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8996\u983b\u8a9e\u8a00\u6279\u8a55\u5bb6\uff0c\u9019\u662f\u4e00\u7a2e\u734e\u52f5\u6a21\u578b\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c0d\u6bd4\u5b78\u7fd2\u548c\u6642\u9593\u6392\u5e8f\u76ee\u6a19\u5c0d\u73fe\u6210\u7684\u8de8\u5177\u9ad4\u5be6\u65bd\u6578\u64da\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u4f7f\u7528\u5b83\u4f86\u8a55\u5206\u4f86\u81ea\u55ae\u7368\u5f37\u5316\u5b78\u7fd2\u53c3\u8207\u8005\u7684\u884c\u70ba\u8ecc\u8de1\u3002\u5728 Open X-Embodiment \u6578\u64da\u4e0a\u9032\u884c\u8a13\u7df4\u6642\uff0c\u6211\u5011\u7684\u734e\u52f5\u6a21\u578b\u4f7f Meta-World \u4efb\u52d9\u4e0a\u7684\u7b56\u7565\u8a13\u7df4\u6bd4\u50c5\u4f7f\u7528\u7a00\u758f\u734e\u52f5\u7684\u7b56\u7565\u8a13\u7df4\u6548\u7387\u63d0\u9ad8\u4e86 2 \u500d\uff0c\u5118\u7ba1\u5b58\u5728\u986f\u8457\u7684\u9818\u57df\u5dee\u8ddd\u3002\u5728 Meta-World \u4e0a\u4f7f\u7528\u9818\u57df\u5167\u6578\u64da\u4f46\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u6cdb\u5316\u8a2d\u7f6e\u4e2d\uff0c\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\u4e86\u6bd4\u4f7f\u7528\u4e8c\u5143\u5206\u985e\u8a13\u7df4\u7684\u3001\u4f7f\u7528\u975c\u614b\u5716\u50cf\u7684\u6216\u4e0d\u5229\u7528\u8996\u983b\u6578\u64da\u4e2d\u5b58\u5728\u7684\u6642\u9593\u4fe1\u606f\u7684\u5148\u524d\u8a9e\u8a00\u689d\u4ef6\u734e\u52f5\u6a21\u578b\u66f4\u6709\u6548\u7684\u8a13\u7df4\u3002", "author": "Minttu Alakuijala et.al.", "authors": "Minttu Alakuijala, Reginald McLean, Isaac Woungang, Nariman Farsad, Samuel Kaski, Pekka Marttinen, Kai Yuan", "id": "2405.19988v1", "paper_url": "http://arxiv.org/abs/2405.19988v1", "repo": "null"}}