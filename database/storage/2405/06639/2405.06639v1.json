{"2405.06639": {"publish_time": "2024-05-10", "title": "Value Augmented Sampling for Language Model Alignment and Personalization", "paper_summary": "Aligning Large Language Models (LLMs) to cater to different human\npreferences, learning new skills, and unlearning harmful behavior is an\nimportant problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree\nSearch, are performant, but impractical for LLM adaptation due to their high\ninference cost. On the other hand, using Reinforcement Learning (RL) for\nadaptation is computationally efficient, but performs worse due to the\noptimization challenges in co-training the value function and the policy. We\npresent a new framework for reward optimization, Value Augmented Sampling\n(VAS), that can maximize different reward functions using data sampled from\nonly the initial, frozen LLM. VAS solves for the optimal reward-maximizing\npolicy without co-training the policy and the value function, making the\noptimization stable, outperforming established baselines, such as PPO and DPO,\non standard benchmarks, and achieving comparable results to Best-of-128 with\nlower inference cost. Unlike existing RL methods that require changing the\nweights of the LLM, VAS does not require access to the weights of the\npre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are\navailable only as APIs. In addition, our algorithm unlocks the new capability\nof composing several rewards and controlling the extent of each one during\ndeployment time, paving the road ahead for the future of aligned, personalized\nLLMs.", "paper_summary_zh": "\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u8fce\u5408\u4e0d\u540c\u7684\u4eba\u985e\u504f\u597d\u3001\u5b78\u7fd2\u65b0\u6280\u80fd\u548c\u53d6\u6d88\u6709\u5bb3\u884c\u70ba\u662f\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\u3002\u57fa\u65bc\u641c\u5c0b\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982 Best-of-N \u6216\u8499\u5730\u5361\u7f85\u6a39\u641c\u5c0b\uff0c\u6548\u80fd\u826f\u597d\uff0c\u4f46\u7531\u65bc\u5176\u63a8\u8ad6\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u4e0d\u5207\u5be6\u969b\u7528\u65bc LLM \u9069\u61c9\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4f7f\u7528\u5f37\u5316\u5b78\u7fd2 (RL) \u4f86\u9069\u61c9\u5728\u904b\u7b97\u4e0a\u5f88\u6709\u6548\u7387\uff0c\u4f46\u7531\u65bc\u5728\u5171\u540c\u8a13\u7df4\u50f9\u503c\u51fd\u6578\u548c\u7b56\u7565\u6642\u6709\u6700\u4f73\u5316\u6311\u6230\uff0c\u56e0\u6b64\u8868\u73fe\u8f03\u5dee\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u734e\u52f5\u6700\u4f73\u5316\u67b6\u69cb\uff0c\u5373\u50f9\u503c\u589e\u5f37\u53d6\u6a23 (VAS)\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528\u50c5\u5f9e\u521d\u59cb\u51cd\u7d50 LLM \u53d6\u6a23\u7684\u8cc7\u6599\u4f86\u6700\u5927\u5316\u4e0d\u540c\u7684\u734e\u52f5\u51fd\u6578\u3002VAS \u89e3\u6c7a\u4e86\u6700\u4f73\u5316\u734e\u52f5\u6700\u5927\u5316\u7684\u7b56\u7565\uff0c\u800c\u7121\u9700\u5171\u540c\u8a13\u7df4\u7b56\u7565\u548c\u50f9\u503c\u51fd\u6578\uff0c\u4f7f\u5f97\u6700\u4f73\u5316\u7a69\u5b9a\uff0c\u5728\u6a19\u6e96\u57fa\u6e96\u4e0a\u512a\u65bc\u5df2\u5efa\u7acb\u7684\u57fa\u6e96\uff0c\u4f8b\u5982 PPO \u548c DPO\uff0c\u4e26\u4ee5\u8f03\u4f4e\u7684\u63a8\u8ad6\u6210\u672c\u9054\u5230\u8207 Best-of-128 \u76f8\u7576\u7684\u7d50\u679c\u3002\u8207\u9700\u8981\u6539\u8b8a LLM \u6b0a\u91cd\u7684\u73fe\u6709 RL \u65b9\u6cd5\u4e0d\u540c\uff0cVAS \u4e0d\u9700\u8981\u5b58\u53d6\u9810\u5148\u8a13\u7df4\u7684 LLM \u7684\u6b0a\u91cd\u3002\u56e0\u6b64\uff0c\u5b83\u751a\u81f3\u53ef\u4ee5\u8abf\u6574\u50c5\u4f5c\u70ba API \u53ef\u7528\u7684 LLM\uff08\u4f8b\u5982 ChatGPT\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u89e3\u9396\u4e86\u7d44\u5408\u591a\u500b\u734e\u52f5\u4e26\u5728\u90e8\u7f72\u6642\u63a7\u5236\u6bcf\u500b\u734e\u52f5\u7a0b\u5ea6\u7684\u65b0\u529f\u80fd\uff0c\u70ba\u672a\u4f86\u5c0d\u9f4a\u3001\u500b\u4eba\u5316\u7684 LLM \u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Seungwook Han et.al.", "authors": "Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, Pulkit Agrawal", "id": "2405.06639v1", "paper_url": "http://arxiv.org/abs/2405.06639v1", "repo": "null"}}