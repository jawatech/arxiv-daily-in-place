{"2412.07405": {"publish_time": "2024-12-10", "title": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "paper_summary": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability.", "paper_summary_zh": "<paragraph>\u5728\u767c\u5c55\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6642\uff0c\u5c0d\u66f4\u5927\u898f\u6a21\u6a21\u578b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u5c0d\u5728\u6709\u9650\u7684\u904b\u7b97\u8cc7\u6e90\u5167\u9032\u884c\u6709\u6548\u8a13\u7df4\u69cb\u6210\u4e86\u6311\u6230\u3002\u50b3\u7d71\u7684\u5fae\u8abf\u65b9\u6cd5\u901a\u5e38\u5728\u591a\u4efb\u52d9\u5b78\u7fd2\u4e2d\u8868\u73fe\u51fa\u4e0d\u7a69\u5b9a\u6027\uff0c\u4e14\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u5927\u91cf\u7684\u8a13\u7df4\u8cc7\u6e90\u3002\u5728\u6b64\uff0c\u6211\u5011\u63d0\u51fa MoDULA\uff08\u9818\u57df\u7279\u5b9a\u548c\u901a\u7528 LoR \u7684\u6df7\u5408\uff09\uff0c\u4e00\u7a2e\u7528\u65bc\u591a\u4efb\u52d9\u5b78\u7fd2\u7684\u5275\u65b0\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u5c08\u5bb6\u6df7\u5408 (MoE) \u5178\u7bc4\uff0c\u4ee5\u6539\u5584\u5fae\u8abf\u548c\u53c3\u6578\u6548\u7387\u3002\u8a72\u5178\u7bc4\u900f\u904e\u5206\u5225\u8a13\u7df4\u901a\u7528\u5c08\u5bb6\u3001\u9818\u57df\u7279\u5b9a\u5c08\u5bb6\u548c\u8def\u7531\u5668\uff0c\u6709\u6548\u5730\u63d0\u5347\u6a21\u578b\u7684\u591a\u4efb\u52d9\u80fd\u529b\u3002MoDULA-Res \u662f MoDULA \u5178\u7bc4\u4e2d\u7684\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5b83\u900f\u904e\u6b98\u5dee\u9023\u7dda\u5c07\u901a\u7528\u5c08\u5bb6\u548c\u4efb\u52d9\u7279\u5b9a\u5c08\u5bb6\u9023\u63a5\u8d77\u4f86\uff0c\u4ee5\u7dad\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cMoDULA-Flan \u548c MoDULA-Res \u65b9\u6cd5\u7684\u6574\u9ad4\u6548\u80fd\u8d85\u8d8a\u4e86\u5404\u7a2e LLM \u4e0a\u73fe\u6709\u7684\u5fae\u8abf\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMoDULA-Res \u5728\u591a\u9805\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86\u66f4\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u540c\u6642\u5c07\u8a13\u7df4\u6210\u672c\u964d\u4f4e\u4e86 80% \u4ee5\u4e0a\uff0c\u4e14\u4e0d\u640d\u5931\u4e00\u822c\u80fd\u529b\u3002\u6b64\u5916\uff0cMoDULA \u5c55\u73fe\u51fa\u9748\u6d3b\u7684\u53ef\u63d2\u5165\u6027\uff0c\u5141\u8a31\u6709\u6548\u5730\u65b0\u589e\u4efb\u52d9\uff0c\u800c\u7121\u9700\u5f9e\u982d\u91cd\u65b0\u8a13\u7df4\u73fe\u6709\u7684\u5c08\u5bb6\u3002\u9019\u7a2e\u6f38\u9032\u5f0f\u8a13\u7df4\u5178\u7bc4\u8ff4\u907f\u4e86\u8cc7\u6599\u5e73\u8861\u554f\u984c\uff0c\u589e\u5f37\u4e86\u8a13\u7df4\u6548\u7387\u548c\u6a21\u578b\u7a69\u5b9a\u6027\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cMoDULA \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u3001\u7d93\u6fdf\u9ad8\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u5fae\u8abf LLM\uff0c\u4e26\u5177\u5099\u589e\u5f37\u7684\u53c3\u6578\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002</paragraph>", "author": "Yufei Ma et.al.", "authors": "Yufei Ma, Zihan Liang, Huangyu Dai, Ben Chen, Dehong Gao, Zhuoran Ran, Wang Zihan, Linbo Jin, Wen Jiang, Guannan Zhang, Xiaoyan Cai, Libin Yang", "id": "2412.07405v1", "paper_url": "http://arxiv.org/abs/2412.07405v1", "repo": "null"}}