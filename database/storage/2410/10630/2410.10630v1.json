{"2410.10630": {"publish_time": "2024-10-14", "title": "Thinking LLMs: General Instruction Following with Thought Generation", "paper_summary": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.", "paper_summary_zh": "LLM \u901a\u5e38\u88ab\u8bad\u7ec3\u6210\u56de\u7b54\u7528\u6237\u7684\u63d0\u95ee\u6216\u9075\u5faa\u6307\u4ee4\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u5982\u4f55\u56de\u5e94\u3002\u7136\u800c\uff0c\u5728\u6807\u51c6\u5bf9\u9f50\u6846\u67b6\u4e2d\uff0c\u5b83\u4eec\u7f3a\u4e4f\u5728\u56de\u7b54\u4e4b\u524d\u8fdb\u884c\u660e\u786e\u601d\u8003\u7684\u57fa\u672c\u80fd\u529b\u3002\u601d\u8003\u5bf9\u4e8e\u9700\u8981\u63a8\u7406\u548c\u89c4\u5212\u7684\u590d\u6742\u95ee\u9898\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5b83\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u73b0\u6709\u7684 LLM \u63d0\u4f9b\u8fd9\u79cd\u601d\u8003\u80fd\u529b\uff0c\u4ee5\u4fbf\u5728\u6ca1\u6709\u4f7f\u7528\u989d\u5916\u4eba\u7c7b\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9075\u5faa\u4e00\u822c\u6307\u4ee4\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u79cd\u8fed\u4ee3\u641c\u7d22\u548c\u4f18\u5316\u7a0b\u5e8f\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u8be5\u7a0b\u5e8f\u63a2\u7d22\u53ef\u80fd\u7684\u601d\u60f3\u751f\u6210\u7a7a\u95f4\uff0c\u5141\u8bb8\u6a21\u578b\u5b66\u4e60\u5982\u4f55\u5728\u6ca1\u6709\u76f4\u63a5\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u601d\u8003\u3002\u5bf9\u4e8e\u6bcf\u6761\u6307\u4ee4\uff0c\u601d\u60f3\u5019\u9009\u8005\u4f7f\u7528\u8bc4\u5224\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\uff0c\u4ec5\u8bc4\u4f30\u5176\u54cd\u5e94\uff0c\u7136\u540e\u901a\u8fc7\u504f\u597d\u4f18\u5316\u8fdb\u884c\u4f18\u5316\u3002\u6211\u4eec\u8868\u660e\uff0c\u6b64\u7a0b\u5e8f\u5728 AlpacaEval \u548c Arena-Hard \u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u9664\u4e86\u66f4\u4f20\u7edf\u7684\u63a8\u7406\u548c\u89e3\u51b3\u95ee\u9898\u4efb\u52a1\u4e4b\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u975e\u63a8\u7406\u7c7b\u522b\uff08\u4f8b\u5982\u8425\u9500\u3001\u5065\u5eb7\u548c\u4e00\u822c\u77e5\u8bc6\uff09\u4e0a\u7684\u601d\u8003\u6536\u76ca\u3002", "author": "Tianhao Wu et.al.", "authors": "Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar", "id": "2410.10630v1", "paper_url": "http://arxiv.org/abs/2410.10630v1", "repo": "null"}}