{"2409.11055": {"publish_time": "2024-09-17", "title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B", "paper_summary": "Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs.", "paper_summary_zh": "\u5148\u524d\u7684\u7814\u7a76\u5de5\u4f5c\u5df2\u4f7f\u7528\u6709\u9650\u7684\u6307\u6a19\uff0c\u4f8b\u5982\u56f0\u60d1\u5ea6\u6216\u4e00\u4e9b\u57fa\u672c\u7684\u77e5\u8b58\u4efb\u52d9\u548c\u820a\u7684\u8cc7\u6599\u96c6\uff0c\u4f86\u8a55\u4f30\u91cf\u5316\u7684 LLM\u3002\u6b64\u5916\uff0c\u50cf Llama 3.1 \u9019\u6a23\u9ad8\u9054 405B \u7684\u6700\u65b0\u5927\u578b\u6a21\u578b\u5c1a\u672a\u7d93\u904e\u5fb9\u5e95\u6aa2\u67e5\u3002\u672c\u6587\u8a55\u4f30\u4e86\u5f9e 7B \u5230 405B \u7684\u6a21\u578b\u5728\u5404\u7a2e\u91cf\u5316\u65b9\u6cd5\uff08GPTQ\u3001AWQ\u3001SmoothQuant \u548c FP8\uff09\u4e0a\uff0c\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684 LLM \u7684\u6548\u80fd\u3002\u4f7f\u7528 13 \u500b\u57fa\u6e96\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u516d\u7a2e\u4efb\u52d9\u985e\u578b\u7684\u6548\u80fd\uff1a\u5e38\u8b58\u554f\u7b54\u3001\u77e5\u8b58\u548c\u8a9e\u8a00\u7406\u89e3\u3001\u6307\u4ee4\u9075\u5faa\u3001\u5e7b\u89ba\u6aa2\u6e2c\u3001\u6578\u5b78\u548c\u5c0d\u8a71\u3002\u6211\u5011\u7684\u95dc\u9375\u767c\u73fe\u986f\u793a\uff0c\uff081\uff09\u5c07\u8f03\u5927\u7684 LLM \u91cf\u5316\u70ba\u8207\u8f03\u5c0f\u7684 FP16 LLM \u76f8\u4f3c\u7684\u5c3a\u5bf8\uff0c\u901a\u5e38\u5728\u9664\u5e7b\u89ba\u6aa2\u6e2c\u548c\u6307\u4ee4\u9075\u5faa\u4e4b\u5916\u7684\u5927\u591a\u6578\u57fa\u6e96\u4e0a\u8868\u73fe\u5f97\u66f4\u597d\uff1b\uff082\uff09\u6548\u80fd\u6703\u96a8\u8457\u4e0d\u540c\u7684\u91cf\u5316\u65b9\u6cd5\u3001\u6a21\u578b\u5927\u5c0f\u548c\u4f4d\u5143\u5bec\u5ea6\u800c\u6709\u986f\u8457\u5dee\u7570\uff0c\u50c5\u6b0a\u91cd\u7684\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u5728\u8f03\u5927\u7684\u6a21\u578b\u4e2d\u7522\u751f\u66f4\u597d\u7684\u7d50\u679c\uff1b\uff083\uff09\u4efb\u52d9\u96e3\u5ea6\u4e0d\u6703\u986f\u8457\u5f71\u97ff\u91cf\u5316\u9020\u6210\u7684\u6e96\u78ba\u5ea6\u4e0b\u964d\uff1b\uff084\uff09MT-Bench \u8a55\u4f30\u65b9\u6cd5\u5728\u6700\u8fd1\u6548\u80fd\u826f\u597d\u7684 LLM \u4e2d\u5177\u6709\u6709\u9650\u7684\u5340\u5206\u80fd\u529b\u3002", "author": "Jemin Lee et.al.", "authors": "Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon", "id": "2409.11055v1", "paper_url": "http://arxiv.org/abs/2409.11055v1", "repo": "null"}}