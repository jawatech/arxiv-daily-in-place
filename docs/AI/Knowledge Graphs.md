
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**|Yukang Lin et.al.|[2409.11147v1](http://arxiv.org/abs/2409.11147v1)|[link](https://github.com/yukang-lin/rger)|
|**2024-09-17**|**Semformer: Transformer Language Models with Semantic Planning**|Yongjing Yin et.al.|[2409.11143v1](http://arxiv.org/abs/2409.11143v1)|null|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|Yanbei Jiang et.al.|[2409.10921v1](http://arxiv.org/abs/2409.10921v1)|[link](https://github.com/yanbei-jiang/artwork-interpretation)|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**|Shanshan Wang et.al.|[2409.10294v1](http://arxiv.org/abs/2409.10294v1)|null|
|**2024-09-16**|**LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**|Le Xiao et.al.|[2409.10077v1](http://arxiv.org/abs/2409.10077v1)|null|
|**2024-09-16**|**On the Diagram of Thought**|Yifan Zhang et.al.|[2409.10038v1](http://arxiv.org/abs/2409.10038v1)|[link](https://github.com/diagram-of-thought/diagram-of-thought)|
|**2024-09-14**|**Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**|Yuanjie Lyu et.al.|[2409.09362v1](http://arxiv.org/abs/2409.09362v1)|null|
|**2024-09-14**|**ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**|Yahan Tu et.al.|[2409.09318v1](http://arxiv.org/abs/2409.09318v1)|null|
|**2024-09-13**|**Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**|Florian Gr√∂tschla et.al.|[2409.09026v1](http://arxiv.org/abs/2409.09026v1)|null|
|**2024-09-13**|**Contri(e)ve: Context + Retrieve for Scholarly Question Answering**|Kanchan Shivashankar et.al.|[2409.09010v1](http://arxiv.org/abs/2409.09010v1)|null|
|**2024-09-13**|**SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**|Qitian Wu et.al.|[2409.09007v1](http://arxiv.org/abs/2409.09007v1)|[link](https://github.com/qitianwu/sgformer)|
|**2024-09-13**|**Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**|Zhiqiang Zhong et.al.|[2409.08864v1](http://arxiv.org/abs/2409.08864v1)|null|
|**2024-09-13**|**A RAG Approach for Generating Competency Questions in Ontology Engineering**|Xueli Pan et.al.|[2409.08820v1](http://arxiv.org/abs/2409.08820v1)|null|
|**2024-09-13**|**ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**|Zezheng Qin et.al.|[2409.08543v1](http://arxiv.org/abs/2409.08543v1)|null|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202v1](http://arxiv.org/abs/2409.08202v1)|null|
|**2024-09-12**|**Towards a graph-based foundation model for network traffic analysis**|Louis Van Langendonck et.al.|[2409.08111v1](http://arxiv.org/abs/2409.08111v1)|null|
|**2024-09-12**|**Learning Rules from KGs Guided by Language Models**|Zihang Peng et.al.|[2409.07869v1](http://arxiv.org/abs/2409.07869v1)|[link](https://github.com/pzh97/learning-rules-from-kgs-guided-by-language-models)|
|**2024-09-12**|**Multi-object event graph representation learning for Video Question Answering**|Yanan Wang et.al.|[2409.07747v1](http://arxiv.org/abs/2409.07747v1)|null|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368v2](http://arxiv.org/abs/2409.07368v2)|null|
|**2024-09-11**|**Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**|Daehee Kim et.al.|[2409.07088v1](http://arxiv.org/abs/2409.07088v1)|[link](https://github.com/daehuikim/WikiOFGraph)|
|**2024-09-11**|**Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**|Jiun-Ting Li et.al.|[2409.07064v1](http://arxiv.org/abs/2409.07064v1)|null|
|**2024-09-11**|**FreeRide: Harvesting Bubbles in Pipeline Parallelism**|Jiashu Zhang et.al.|[2409.06941v1](http://arxiv.org/abs/2409.06941v1)|null|
|**2024-09-10**|**Generative Hierarchical Materials Search**|Sherry Yang et.al.|[2409.06762v1](http://arxiv.org/abs/2409.06762v1)|null|
|**2024-09-10**|**Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**|Gollam Rabby et.al.|[2409.06433v1](http://arxiv.org/abs/2409.06433v1)|null|
|**2024-09-09**|**Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**|Dongyue Li et.al.|[2409.06091v1](http://arxiv.org/abs/2409.06091v1)|[link](https://github.com/virtuosoresearch/scalablemtl)|
|**2024-09-09**|**OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**|Ningyu Zhang et.al.|[2409.07497v1](http://arxiv.org/abs/2409.07497v1)|[link](https://github.com/zjunlp/oneedit)|
|**2024-09-09**|**SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**|Alireza Ghafarollahi et.al.|[2409.05556v1](http://arxiv.org/abs/2409.05556v1)|[link](https://github.com/lamm-mit/SciAgentsDiscovery)|
|**2024-09-09**|**Assessing SPARQL capabilities of Large Language Models**|Lars-Peter Meyer et.al.|[2409.05925v1](http://arxiv.org/abs/2409.05925v1)|[link](https://github.com/aksw/llm-kg-bench)|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-07**|**Action is the primary key: a categorical framework for episode description and logical reasoning**|Yoshiki Fukada et.al.|[2409.04793v1](http://arxiv.org/abs/2409.04793v1)|null|
|**2024-09-06**|**Accelerating Training with Neuron Interaction and Nowcasting Networks**|Boris Knyazev et.al.|[2409.04434v1](http://arxiv.org/abs/2409.04434v1)|[link](https://github.com/samsungsailmontreal/nino)|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286v1](http://arxiv.org/abs/2409.04286v1)|null|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183v1](http://arxiv.org/abs/2409.04183v1)|null|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181v1](http://arxiv.org/abs/2409.04181v1)|null|
|**2024-09-06**|**Refining Wikidata Taxonomy using Large Language Models**|Yiwen Peng et.al.|[2409.04056v1](http://arxiv.org/abs/2409.04056v1)|[link](https://github.com/peng-yiwen/WiKC)|
|**2024-09-06**|**Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**|Miao Fan et.al.|[2409.04009v1](http://arxiv.org/abs/2409.04009v1)|null|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440v1](http://arxiv.org/abs/2409.03440v1)|null|
|**2024-09-05**|**iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**|Yassir Lairgi et.al.|[2409.03284v1](http://arxiv.org/abs/2409.03284v1)|[link](https://github.com/AuvaLab/itext2kg)|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258v1](http://arxiv.org/abs/2409.03258v1)|null|
|**2024-09-05**|**Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**|Jie Ma et.al.|[2409.03155v1](http://arxiv.org/abs/2409.03155v1)|[link](https://github.com/reml-group/dog)|
|**2024-09-04**|**Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**|Junyoung Lee et.al.|[2409.02481v1](http://arxiv.org/abs/2409.02481v1)|null|
|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|Xiongkun Linghu et.al.|[2409.02389v1](http://arxiv.org/abs/2409.02389v1)|null|
|**2024-09-02**|**Grounding Language Models in Autonomous Loco-manipulation Tasks**|Jin Wang et.al.|[2409.01326v1](http://arxiv.org/abs/2409.01326v1)|null|
|**2024-09-02**|**LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**|Haoran Yang et.al.|[2409.01145v1](http://arxiv.org/abs/2409.01145v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**|Yuxiang Wang et.al.|[2409.00727v1](http://arxiv.org/abs/2409.00727v1)|null|
|**2024-08-31**|**WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**|Oktie Hassanzadeh et.al.|[2409.00331v1](http://arxiv.org/abs/2409.00331v1)|[link](https://github.com/IBM/wikicausal)|
|**2024-08-29**|**HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**|Rishi Kalra et.al.|[2409.09046v1](http://arxiv.org/abs/2409.09046v1)|null|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|Jingyi Wang et.al.|[2408.16224v2](http://arxiv.org/abs/2408.16224v2)|null|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903v1](http://arxiv.org/abs/2408.15903v1)|null|
|**2024-08-27**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.|[2408.14895v2](http://arxiv.org/abs/2408.14895v2)|[link](https://github.com/aistairc/virtualhome_aist)|
|**2024-08-27**|**XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**|Yasir Ali Farrukh et.al.|[2408.16021v1](http://arxiv.org/abs/2408.16021v1)|[link](https://github.com/yasir-ali-farrukh/gnn4id)|
|**2024-08-26**|**Process Trace Querying using Knowledge Graphs and Notation3**|William Van Woensel et.al.|[2409.04452v1](http://arxiv.org/abs/2409.04452v1)|null|
|**2024-08-26**|**PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**|Runtao Ren et.al.|[2409.00092v1](http://arxiv.org/abs/2409.00092v1)|null|
|**2024-08-26**|**DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**|Ziai Zhou et.al.|[2408.14185v1](http://arxiv.org/abs/2408.14185v1)|null|
|**2024-08-26**|**Exploring the Potential of Large Language Models for Heterophilic Graphs**|Yuxia Wu et.al.|[2408.14134v1](http://arxiv.org/abs/2408.14134v1)|null|
|**2024-08-26**|**Towards Graph Prompt Learning: A Survey and Beyond**|Qingqing Long et.al.|[2408.14520v2](http://arxiv.org/abs/2408.14520v2)|null|
|**2024-08-25**|**CodeGraph: Enhancing Graph Reasoning of LLMs with Code**|Qiaolong Cai et.al.|[2408.13863v1](http://arxiv.org/abs/2408.13863v1)|null|
|**2024-08-25**|**LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**|Duo Wang et.al.|[2408.14512v1](http://arxiv.org/abs/2408.14512v1)|null|
|**2024-08-24**|**Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**|Sakhinana Sagar Srinivas et.al.|[2408.13661v1](http://arxiv.org/abs/2408.13661v1)|null|
|**2024-08-24**|**GNN: Graph Neural Network and Large Language Model for Data Discovery**|Thomas Hoang et.al.|[2408.13609v2](http://arxiv.org/abs/2408.13609v2)|null|
|**2024-08-24**|**HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**|Azmine Toushik Wasi et.al.|[2408.13521v1](http://arxiv.org/abs/2408.13521v1)|[link](https://github.com/azminewasi/hrgraph)|
|**2024-08-24**|**Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**|Yi-Hui Chen et.al.|[2408.13432v1](http://arxiv.org/abs/2408.13432v1)|null|
|**2024-08-23**|**CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**|Ekaterina Trofimova et.al.|[2408.13366v1](http://arxiv.org/abs/2408.13366v1)|null|
|**2024-08-23**|**Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**|Sakhinana Sagar Srinivas et.al.|[2408.14494v1](http://arxiv.org/abs/2408.14494v1)|null|
|**2024-08-22**|**A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**|Ekdeep Singh Lubana et.al.|[2408.12578v2](http://arxiv.org/abs/2408.12578v2)|[link](https://github.com/ekdeepslubana/conceptpercolation)|
|**2024-08-22**|**Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**|Arief Purnama Muharram et.al.|[2409.00061v1](http://arxiv.org/abs/2409.00061v1)|null|
|**2024-08-22**|**Cell-ontology guided transcriptome foundation model**|Xinyu Yuan et.al.|[2408.12373v1](http://arxiv.org/abs/2408.12373v1)|null|
|**2024-08-22**|**Graph Retrieval Augmented Trustworthiness Reasoning**|Ying Zhu et.al.|[2408.12333v2](http://arxiv.org/abs/2408.12333v2)|[link](https://github.com/EvoNexusX/Graph-Retrieval-Augmented-Trustworthiness-Reasoning)|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**|Junlin He et.al.|[2408.12116v1](http://arxiv.org/abs/2408.12116v1)|null|
|**2024-08-21**|**Enabling Small Models for Zero-Shot Classification through Model Label Learning**|Jia Zhang et.al.|[2408.11449v1](http://arxiv.org/abs/2408.11449v1)|null|
|**2024-08-20**|**Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**|Zhilong Wang et.al.|[2408.11182v1](http://arxiv.org/abs/2408.11182v1)|null|
|**2024-08-20**|**Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**|Thomas Hoang et.al.|[2408.11133v1](http://arxiv.org/abs/2408.11133v1)|null|
|**2024-08-20**|**Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**|Rui Yang et.al.|[2408.10819v1](http://arxiv.org/abs/2408.10819v1)|null|
|**2024-08-20**|**Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**|Litian Huang et.al.|[2408.10592v1](http://arxiv.org/abs/2408.10592v1)|[link](https://github.com/ferretdoll/hgr)|
|**2024-08-19**|**Query languages for neural networks**|Martin Grohe et.al.|[2408.10362v2](http://arxiv.org/abs/2408.10362v2)|null|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124v1](http://arxiv.org/abs/2408.10124v1)|[link](https://github.com/zhangtia16/molgraph-lardo)|
|**2024-08-19**|**Geometry Informed Tokenization of Molecules for Language Model Generation**|Xiner Li et.al.|[2408.10120v1](http://arxiv.org/abs/2408.10120v1)|null|
|**2024-08-19**|**GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**|Ran Liu et.al.|[2408.10115v1](http://arxiv.org/abs/2408.10115v1)|[link](https://github.com/oswald1997/glimmer)|
|**2024-08-19**|**SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**|Pengjie Liu et.al.|[2408.09717v1](http://arxiv.org/abs/2408.09717v1)|null|
|**2024-08-18**|**Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**|Xinnan Dai et.al.|[2408.09529v1](http://arxiv.org/abs/2408.09529v1)|null|
|**2024-08-18**|**Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**|Geethan Sannidhi et.al.|[2408.13273v1](http://arxiv.org/abs/2408.13273v1)|null|
|**2024-08-18**|**Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**|Kening Zheng et.al.|[2408.09429v1](http://arxiv.org/abs/2408.09429v1)|null|
|**2024-08-16**|**ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**|Debashis Gupta et.al.|[2408.08972v1](http://arxiv.org/abs/2408.08972v1)|null|
|**2024-08-16**|**EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**|Chenwei Wan et.al.|[2408.08782v1](http://arxiv.org/abs/2408.08782v1)|[link](https://github.com/cw-wan/EmoDynamiX-v2)|
|**2024-08-16**|**Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**|Zhongjian Zhang et.al.|[2408.08685v1](http://arxiv.org/abs/2408.08685v1)|null|
|**2024-08-16**|**RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**|Meng Chen et.al.|[2408.08933v1](http://arxiv.org/abs/2408.08933v1)|[link](https://github.com/matchyc/RoarGraph)|
|**2024-08-16**|**Handling abort commands for household kitchen robots**|Darius Has et.al.|[2408.14480v1](http://arxiv.org/abs/2408.14480v1)|null|
|**2024-08-16**|**CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**|Rong-Ching Chang et.al.|[2408.08535v1](http://arxiv.org/abs/2408.08535v1)|null|
|**2024-08-15**|**VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**|Chia-Tung Ho et.al.|[2408.08927v1](http://arxiv.org/abs/2408.08927v1)|null|
|**2024-08-15**|**Graph Retrieval-Augmented Generation: A Survey**|Boci Peng et.al.|[2408.08921v2](http://arxiv.org/abs/2408.08921v2)|[link](https://github.com/pengboci/graphrag-survey)|
|**2024-08-14**|**Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**|Jiri Hron et.al.|[2408.07852v1](http://arxiv.org/abs/2408.07852v1)|null|
|**2024-08-14**|**ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**|Xuanqing Yu et.al.|[2408.07840v1](http://arxiv.org/abs/2408.07840v1)|null|
|**2024-08-14**|**WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**|Weijian Xie et.al.|[2408.07611v2](http://arxiv.org/abs/2408.07611v2)|null|
|**2024-08-14**|**Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**|Tobias A. Opsahl et.al.|[2408.07453v1](http://arxiv.org/abs/2408.07453v1)|[link](https://github.com/tobias-opsahl/fact-or-fiction)|
|**2024-08-13**|**LLMs can Schedule**|Henrik Abgaryan et.al.|[2408.06993v1](http://arxiv.org/abs/2408.06993v1)|[link](https://github.com/starjob42/datasetjsp)|
|**2024-08-13**|**Causal Agent based on Large Language Model**|Kairong Han et.al.|[2408.06849v1](http://arxiv.org/abs/2408.06849v1)|[link](https://github.com/kairong-han/causal_agent)|

#### Abstracts
##### **Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**
2409.11147v1 by Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen

Large language models(LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning(ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÂ∞ëÈáèÂ≠∏ÁøíËÉΩÂäõÔºå‰∏¶ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÊäÄË°ìÁµ±‰∏Ä‰∫Ü NLP ‰ªªÂãôÁöÑÁØÑ‰æã„ÄÇÂÑòÁÆ° ICL Â∑≤ÊàêÂäüÔºåÁØÑ‰æãÁ§∫ÁØÑÁöÑÂìÅË≥™ÊúÉÈ°ØËëóÂΩ±Èüø LLM ÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÁØÑ‰æãÈÅ∏ÊìáÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñºÊü•Ë©¢ËàáÂÄôÈÅ∏ÁØÑ‰æã‰πãÈñìÁöÑË™ûÊÑèÁõ∏‰ººÊÄß„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÊé®ÁêÜÊ≠•È©ü‰πãÈñìÁöÑÈÇèËºØÈÄ£ÁµêÊúâÂä©ÊñºÊèèÁπ™ÂïèÈ°åËß£Ê±∫ÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êé®ÁêÜÂúñÂ¢ûÂº∑ÁØÑ‰æãÊ™¢Á¥¢ (RGER) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇRGER È¶ñÂÖàË¶ÅÊ±Ç LLM Áî¢Áîü‰∏ÄÂÄãÂàùÂßãÂõûÊáâÔºåÁÑ∂ÂæåÂ∞á‰∏≠ÈñìÂïèÈ°åËß£Ê±∫Ê≠•È©üË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁµêÊßã„ÄÇ‰πãÂæåÔºåÂÆÉÊé°Áî®ÂúñÂΩ¢Ê†∏ÈÅ∏ÂèñÂÖ∑ÊúâË™ûÊÑèÂíåÁµêÊßãÁõ∏‰ººÊÄßÁöÑÁØÑ‰æã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÁµêÊßãÈóú‰øÇÊúâÂä©ÊñºÊü•Ë©¢ÂíåÂÄôÈÅ∏ÁØÑ‰æãÁöÑÂ∞çÈΩä„ÄÇRGER Âú®Êï∏Â≠∏ÂíåÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏äÁöÑÂäüÊïàÂ±ïÁ§∫‰∫ÜÂÆÉÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÁôºÂ∏ÉÊñº https://github.com/Yukang-Lin/RGER„ÄÇ

##### **Semformer: Transformer Language Models with Semantic Planning**
2409.11143v1 by Yongjing Yin, Junran Ding, Kai Song, Yue Zhang

Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.

ÊëòË¶ÅÔºöÂú®Áï∂ÂâçÁöÑË™ûË®ÄÊ®°Âûã‰∏≠Ôºå‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨ÊòØ‰∏ªÂ∞éÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂú®Ë®ìÁ∑¥ÈöéÊÆµÔºåÊ®°ÂûãÊé°Áî®ÊïôÂ∏´Âº∑Âà∂Ê≥ïÔºåÊ†πÊìöÊâÄÊúâÂâç‰∏ÄÂÄãÁöÑÁúüÂØ¶Ë©ûÂΩô‰æÜÈ†êÊ∏¨Ë©ûÂΩô„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÈÄôÁ®ÆÊñπÊ≥ïÊúÉÁî¢ÁîüÊç∑ÂæëÔºåÂà©Áî®Â∑≤Êè≠Èú≤ÁöÑÂâçÁ∂¥‰æÜËôõÂÅáÂú∞Á¨¶ÂêàÂæåÁ∫åÁöÑË©ûÂΩôÔºåÊΩõÂú®ÊúÉÂç±ÂÆ≥‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨Âô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π SemformerÔºå‰∏ÄÁ®ÆË®ìÁ∑¥ Transformer Ë™ûË®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ïÔºåÊòéÁ¢∫Âú∞Âª∫ÊßãÂõûÊáâÁöÑË™ûÊÑèË¶èÂäÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàóË¶èÂäÉË©ûÂΩôÁ¥çÂÖ•ÂâçÁ∂¥ÔºåÂºïÂ∞éË¶èÂäÉË©ûÂΩôÁöÑË°®ÂæµÂéªÈ†êÊ∏¨ÂõûÊáâÁöÑÊΩõÂú®Ë™ûÊÑèË°®ÂæµÔºåÈÄô‰∫õË°®ÂæµÊòØÁî±Ëá™ÂãïÁ∑®Á¢ºÂô®Ë™òÂ∞éÁöÑ„ÄÇÂú®‰∏ÄÂÄãÊúÄÂ∞èÁöÑË¶èÂäÉ‰ªªÂãôÔºàÂç≥ÂúñÂΩ¢Ë∑ØÂæëÂ∞ãÊâæÔºâ‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæÂá∫Êé•ËøëÂÆåÁæéÁöÑÊïàËÉΩÔºå‰∏¶ÊúâÊïàÂú∞Ê∏õËºïÊç∑ÂæëÂ≠∏ÁøíÔºåÈÄôÊòØÊ®ôÊ∫ñË®ìÁ∑¥ÊñπÊ≥ïÂíåÂü∫Á∑öÊ®°ÂûãÁÑ°Ê≥ïÈÅîÊàêÁöÑÂ£ØËàâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÈ†≠ÈñãÂßã‰ΩøÁî® 1.25 ÂÑÑÂÄãÂèÉÊï∏È†êË®ìÁ∑¥ SemformerÔºåÈÄèÈÅéÂõ∞ÊÉëÂ∫¶„ÄÅË™ûÂ¢ÉÂ≠∏ÁøíÂíåÂú®ÊëòË¶Å‰ªªÂãô‰∏äÁöÑÂæÆË™ø‰æÜË≠âÊòéÂÖ∂ÂäüÊïà„ÄÇ

##### **KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**
2409.10921v1 by Yanbei Jiang, Krista A. Ehinger, Jey Han Lau

Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.

ÊëòË¶ÅÔºöÊé¢Á¥¢Áî±ÁæéÊúØÁªòÁîª‰º†ËææÁöÑÂèô‰∫ãÊòØÂõæÂÉèÂ≠óÂπï‰∏≠ÁöÑÊåëÊàòÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏ç‰ªÖÂáÜÁ°ÆÂú∞Ë°®Á§∫ËßÜËßâÂÜÖÂÆπËÄå‰∏îËøòÊèê‰æõÂØπËâ∫ÊúØÂìÅÂê´‰πâÁöÑÊ∑±ÂÖ•Ëß£ÈáäÁöÑÊèèËø∞„ÄÇÁî±‰∫éÂÖ∂‰∏çÂêåÁöÑËß£ÈáäÂíåË∑®‰∏çÂêåËâ∫ÊúØÊµÅÊ¥æÂíåÈ£éÊ†ºÁöÑ‰∏çÂêåÁæéÂ≠¶ÂéüÂàôÔºåËøôÈ°π‰ªªÂä°ÂØπ‰∫éËâ∫ÊúØÂìÅÂõæÂÉèÊù•ËØ¥Â∞§ÂÖ∂Â§çÊùÇ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü KALE Áü•ËØÜÂ¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁî®‰∫éËâ∫ÊúØÂìÅÈòêÈáäÔºå‰∏ÄÁßçÈÄöËøáÂ∞ÜËâ∫ÊúØÂìÅÂÖÉÊï∞ÊçÆ‰Ωú‰∏∫ÈôÑÂä†Áü•ËØÜÊù•Â¢ûÂº∫Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇKALE ‰ª•‰∏§ÁßçÊñπÂºèÂêàÂπ∂ÂÖÉÊï∞ÊçÆÔºöÈ¶ñÂÖà‰Ωú‰∏∫Áõ¥Êé•ÊñáÊú¨ËæìÂÖ•ÔºåÂÖ∂Ê¨°ÈÄöËøáÂ§öÊ®°ÊÄÅÂºÇÊûÑÁü•ËØÜÂõæ„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÂõæË°®ÁöÑÂ≠¶‰π†Ë°®Á§∫ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÊçüÂ§±ÔºåÂÆÉÊúÄÂ§ßÂåñÂõæÂÉè‰∏éÂÖ∂ÂØπÂ∫îÂÖÉÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKALE Âú®‰ΩøÁî® CIDEr ËØÑ‰º∞Êó∂ÔºåÂú®Âá†‰∏™Ëâ∫ÊúØÂìÅÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩÔºàÁâπÂà´ÊòØ‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂ∑•‰ΩúÁõ∏ÊØîÔºâ„ÄÇËØ•È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/Yanbei-Jiang/Artwork-Interpretation Ëé∑Âæó„ÄÇ

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊèêÁ§∫Â≠¶‰π†Ê°ÜÊû∂ÁöÑÁü•ËØÜÂ¢ûÂº∫ÁñæÁóÖËØäÊñ≠ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ï‰ªé‰∏é‰∏¥Â∫äÁóÖ‰æãÁõ∏ÂÖ≥ÁöÑÂ§ñÈÉ®Áü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢ÁªìÊûÑÂåñÁü•ËØÜÔºåÂØπÂÖ∂ËøõË°åÁºñÁ†ÅÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞ÊèêÁ§∫Ê®°Êùø‰∏≠Ôºå‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÂØπ‰ªªÂä°ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®‰∏â‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºöCHIP-CTC„ÄÅIMCS-V2-NER Âíå KUAKE-QTR„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÊòéÊòæ‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÂú® CHIP-CTC Êï∞ÊçÆÈõÜ‰∏äÁöÑ F1 ÂæóÂàÜÊèêÈ´ò‰∫Ü 2.4%ÔºåÂú® IMCS-V2-NER Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 3.1%ÔºåÂú® KUAKE-QTR Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 4.2%„ÄÇÊ≠§Â§ñÔºåÊ∂àËûçÁ†îÁ©∂ËØÅÂÆû‰∫ÜÁü•ËØÜÊ≥®ÂÖ•Ê®°ÂùóÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂõ†‰∏∫ÁßªÈô§Ê≠§Ê®°Âùó‰ºöÂØºËá¥ F1 ÂæóÂàÜÊòæÁùÄ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏ç‰ªÖÊúâÊïàÊèêÈ´ò‰∫ÜÁñæÁóÖËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÂ¢ûÂº∫‰∫ÜÈ¢ÑÊµãÁöÑÂèØËß£ÈáäÊÄßÔºå‰∏∫‰∏¥Â∫äËØäÊñ≠Êèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑÊîØÊåÅÂíåËØÅÊçÆ„ÄÇ

##### **MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**
2409.10294v1 by Shanshan Wang, Chun Zhang, Ning Zhang

The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÂà∞ÊñáÂ≠óÁîüÊàê‰ªªÂãôÊó®Âú®Â∞áÁµêÊßãÂåñÁü•Ë≠òÂúñË≠úËΩâÊèõÁÇ∫ÈÄ£Ë≤´‰∏î‰∫∫È°ûÂèØËÆÄÁöÑËá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÊúÄËøëÂú®ÈÄôÂÄãÈ†òÂüüÁöÑÁ†îÁ©∂ÈõÜ‰∏≠ÊñºÈÄèÈÅéÁ¥çÂÖ•ÂúñÂΩ¢ÁµêÊßãË≥áË®ä‰æÜÂ¢ûÂº∑È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM)Ôºå‰ª•Êì∑ÂèñÁü•Ë≠òÂúñË≠úÁöÑË§áÈõúÁµêÊßãÁ¥∞ÁØÄ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂ§ßÂ§öÂÇæÂêëÊñºÂÉÖÊì∑ÂèñÂñÆ‰∏ÄÁ≤íÂ∫¶ÁöÑÁµêÊßãË≥áË®äÔºåÂ∞àÊ≥®ÊñºÂéüÂßãÂúñÂΩ¢‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇÊàñÂêå‰∏ÄÂÄãÂØ¶È´îÊàñ‰∏çÂêåÂØ¶È´î‰πãÈñìÁöÑÂñÆÂ≠óÈóú‰øÇ„ÄÇÈÄôÁ®ÆÁãπÈöòÁöÑÁÑ¶ÈªûÂ∞éËá¥‰∏ÄÂÄãÈ°ØËëóÁöÑÈôêÂà∂ÔºöÂÉÖÂ∞àÊ≥®ÊñºÂØ¶È´îÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÁÑ°Ê≥ïÊì∑ÂèñÂñÆÂ≠ó‰πãÈñìÁ¥∞ÂæÆÁöÑË™ûÁæ©Èóú‰øÇÔºåËÄåÂÉÖÂ∞àÊ≥®ÊñºÂñÆÂ≠óÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÂâáÂøΩÁï•‰∫ÜÂéüÂßãÊï¥ÂÄãÂØ¶È´î‰πãÈñìÁöÑÊõ¥Âª£Ê≥õÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÂºïÂÖ•‰∫ÜÂü∫Êñº PLM ÁöÑÂ§öÁ≤íÂ∫¶ÂúñÂΩ¢ÁµêÊßãÊ≥®ÊÑèÂäõ (MGSA)„ÄÇÊ®°ÂûãÊû∂ÊßãÁöÑÁ∑®Á¢ºÂô®ÂÖ∑ÊúâÂØ¶È´îÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑ„ÄÅÂñÆÂ≠óÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑÂíå‰∏ÄÂÄãÂæûÂÖ©ÂÄãÁµêÊßã‰∏≠Á∂úÂêàË≥áË®äÁöÑËÅöÂêàÊ®°ÁµÑ„ÄÇÈÄôÁ®ÆÂ§öÁ≤íÂ∫¶ÁµêÊßãÁ∑®Á¢ºÊñπÊ≥ïÂÖÅË®±Ê®°ÂûãÂêåÊôÇÊì∑ÂèñÂØ¶È´îÂ±§Á¥öÂíåÂñÆÂ≠óÂ±§Á¥öÁµêÊßãË≥áË®äÔºåÊèê‰æõÂ∞çÁü•Ë≠òÂúñË≠úÁµêÊßãË≥áË®äÊõ¥ÂÖ®Èù¢ÁöÑÁêÜËß£ÔºåÂæûËÄåÈ°ØËëóÊèêÂçáÁîüÊàêÊñáÂ≠óÁöÑÂìÅË≥™„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÂª£Ê≥õË™çÂèØÁöÑ KG Âà∞ÊñáÂ≠óÁîüÊàêÂü∫Ê∫ñË≥áÊñôÈõÜ WebNLG Âíå EventNarrative Â∞ç MGSA Ê®°ÂûãÈÄ≤Ë°åÂª£Ê≥õË©ï‰º∞ÔºåÂú®ÈÄô‰∫õË≥áÊñôÈõÜ‰∏äÔºåÂÆÉÂßãÁµÇÂÑ™ÊñºÂÉÖ‰æùË≥¥ÂñÆ‰∏ÄÁ≤íÂ∫¶ÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**
2409.10077v1 by Le Xiao, Yunfei Xu, Jing Zhao

Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.

ÊëòË¶ÅÔºö<paragraph>È†òÂüüÁâπÂÆöÂëΩÂêçÂØ¶È´îËæ®Ë≠òÔºàNERÔºâÔºåÂÖ∂ÁõÆÊ®ôÊòØËæ®Ë≠òÈ†òÂüüÁâπÂÆöÂØ¶È´îÂèäÂÖ∂È°ûÂà•ÔºåÁÇ∫Âª∫ÊßãÈ†òÂüüÁü•Ë≠òÂúñË≠úÊèê‰æõÈáçË¶ÅÁöÑÊîØÊè¥„ÄÇÁõÆÂâçÔºåÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂª£Ê≥õÁî®Êñº NER ‰ªªÂãô‰∏îÂçÅÂàÜÊúâÊïàÔºå‰ΩÜÁî±Êñº‰æùË≥¥ÊñºÂ§ßË¶èÊ®°Ê®ôË®òË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÁâπÂÆöÈ†òÂüü‰∏≠Ê®ôË®òË≥áÊñôÁöÑÁ®ÄÂ∞ëÊúÉÈôêÂà∂ÂÖ∂ÊáâÁî®„ÄÇÂõ†Ê≠§ÔºåË®±Â§öÁ†îÁ©∂ÈñãÂßãÂºïÂÖ•Â∞ëÈáèÊ®£Êú¨ÊñπÊ≥ï‰∏¶Áç≤Âæó‰∏Ä‰∫õÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÁâπÂÆöÈ†òÂüü‰∏≠ÁöÑÂØ¶È´îÁµêÊßãÈÄöÂ∏∏ÂæàË§áÈõúÔºåËÄåÁõÆÂâçÁöÑÂ∞ëÈáèÊ®£Êú¨ÊñπÊ≥ïÈõ£‰ª•ÈÅ©ÊáâÂÖ∑ÊúâË§áÈõúÁâπÂæµÁöÑ NER ‰ªªÂãô„ÄÇ‰ª•‰∏≠ÂúãÁÖ§ÂåñÂ∑•Áî¢Ê•≠È†òÂüüÁÇ∫‰æãÔºåÂ≠òÂú®Â§öÂÄãÂØ¶È´îÂÖ±Áî®ÂñÆ‰∏ÄÂØ¶È´îÁöÑË§áÈõúÁµêÊßãÔºå‰ª•ÂèäÂêå‰∏ÄÂ∞çÂØ¶È´îÊúâÂ§öÈáçÈóú‰øÇÔºåÈÄôÊúÉÂΩ±ÈüøÊ®£Êú¨ËºÉÂ∞ëÊ¢ù‰ª∂‰∏ãÁöÑ NER ‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂØ¶È´îËæ®Ë≠òÊû∂Êßã LLM-DERÔºåÁî®Êñº‰∏≠ÊñáÈ†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠òÂïèÈ°åÔºåÈÄöÈÅé LLM ÁîüÊàêÂåÖÂê´ÂØ¶È´îÈ°ûÂûãÁöÑÈóú‰øÇÊ∏ÖÂñÆÔºå‰∏¶Ë®≠Ë®à‰∏ÄÂÄãÂêàÁêÜÊÄßÂíå‰∏ÄËá¥ÊÄßË©ï‰º∞ÊñπÊ≥ï‰æÜÁßªÈô§Ëæ®Ë≠òÈåØË™§ÁöÑÂØ¶È´îÔºåÂæûËÄåÂèØ‰ª•ÊúâÊïàËß£Ê±∫ÁâπÂÆöÈ†òÂüü‰∏≠Ë§áÈõúÁµêÊßãÂØ¶È´îËæ®Ë≠òÂïèÈ°å„ÄÇÊú¨ÊñáÂú® Resume Ë≥áÊñôÈõÜÂíåËá™Âª∫ÁÖ§ÂåñÂ∑•Ë≥áÊñôÈõÜ Coal ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLM-DER Âú®È†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠ò‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏çÂÉÖÂÑ™ÊñºÁèæÊúâÁöÑ GPT-3.5-turbo Âü∫Ê∫ñÔºåÈÇÑË∂ÖÈÅé‰∫ÜÂÆåÂÖ®Áõ£Áù£ÁöÑÂü∫Á∑öÔºåÈ©óË≠â‰∫ÜÂÖ∂Âú®ÂØ¶È´îËæ®Ë≠ò‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ</paragraph>

##### **On the Diagram of Thought**
2409.10038v1 by Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao

We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫ÜÊÄùÊÉ≥ÂúñÔºàDoTÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂÆÉÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑËø≠‰ª£Êé®ÁêÜÂª∫Ê®°ÁÇ∫Âú®ÂñÆ‰∏ÄÊ®°ÂûãÂÖßÂª∫Êßã‰∏ÄÂÄãÊúâÂêëÁÑ°Áí∞ÂúñÔºàDAGÔºâ„ÄÇËàáÂ∞áÊé®ÁêÜË°®Á§∫ÁÇ∫Á∑öÊÄßÈèàÊàñÊ®πÁöÑÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåDoT Â∞áÂëΩÈ°å„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÂíåÈ©óË≠âÁµÑÁπîÊàê‰∏ÄÂÄãÊúâÂáùËÅöÂäõÁöÑ DAG ÁµêÊßãÔºåÂÖÅË®±Ê®°ÂûãÊé¢Á¥¢Ë§áÈõúÁöÑÊé®ÁêÜË∑ØÂæëÔºåÂêåÊôÇ‰øùÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄß„ÄÇÂúñË°®‰∏≠ÁöÑÊØèÂÄãÁØÄÈªûÂ∞çÊáâÊñº‰∏ÄÂÄãÂ∑≤Ë¢´ÊèêÂá∫„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÊàñÈ©óË≠âÁöÑÂëΩÈ°åÔºå‰Ωø LLM ËÉΩÂ§†ÈÄöÈÅéËá™ÁÑ∂Ë™ûË®ÄÂõûÈ•ãËø≠‰ª£Âú∞ÊîπÈÄ≤ÂÖ∂Êé®ÁêÜ„ÄÇÈÄöÈÅéÂà©Áî®ÂÖ∑ÊúâËßíËâ≤ÁâπÂÆöÊ®ôË®òÁöÑËá™ÂãïÂõûÊ≠∏‰∏ã‰∏ÄÂÄãÊ®ôË®òÈ†êÊ∏¨ÔºåDoT ‰øÉÈÄ≤‰∫ÜÊèêÂá∫ÊÉ≥Ê≥ïÂíåÊâπÂà§ÊÄßË©ï‰º∞ÂÆÉÂÄë‰πãÈñìÁöÑÁÑ°Á∏´ÈÅéÊ∏°ÔºåÊèê‰æõ‰∫ÜÊØî‰∫åÂÖÉ‰ø°ËôüÊõ¥Ë±êÂØåÁöÑÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî®ÊãìÊí≤ÁêÜË´ñÂΩ¢ÂºèÂåñ‰∫Ü DoT Ê°ÜÊû∂ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊï∏Â≠∏Âü∫Á§éÔºå‰ª•Á¢∫‰øùÊé®ÁêÜÈÅéÁ®ã‰∏≠ÁöÑÈÇèËºØ‰∏ÄËá¥ÊÄßÂíåÂÅ•ÂÖ®ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÂñÆ‰∏Ä LLM ÂÖßÁöÑË®ìÁ∑¥ÂíåÊé®ÁêÜÈÅéÁ®ãÔºåÊ∂àÈô§‰∫ÜÂ∞çÂ§öÂÄãÊ®°ÂûãÊàñÂ§ñÈÉ®ÊéßÂà∂Ê©üÂà∂ÁöÑÈúÄË¶Å„ÄÇDoT ÁÇ∫Ë®≠Ë®à‰∏ã‰∏Ä‰ª£Êé®ÁêÜÂ∞àÁî®Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊ¶ÇÂøµÊ°ÜÊû∂ÔºåÂº∑Ë™øË®ìÁ∑¥ÊïàÁéá„ÄÅÂº∑Â§ßÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁêÜË´ñÂü∫Á§é„ÄÇ‰ª£Á¢ºÂèØÂú® https://github.com/diagram-of-thought/diagram-of-thought Áç≤Âæó„ÄÇ

##### **Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**
2409.09362v1 by Yuanjie Lyu, Tong Xu, Zihan Niu, Bo Peng, Jing Ke, Enhong Chen

The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑËì¨ÂãÉÁôºÂ±ïÔºåÊèêÂçá‰∫ÜÂ∞çË™ûÊÑèË±êÂØåÊúçÂãôÔºà‰æãÂ¶Ç‰∫ã‰ª∂ÂíåÊïÖ‰∫ãÁ∑öÊ≠∏Âõ†ÔºâÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂Â§ßÂ§öËëóÈáçÊñºÁâáÊÆµÂ±§Á¥öÁöÑ‰∫ã‰ª∂ÁêÜËß£Ôºå‰∏ªË¶ÅÊòØÈÄèÈÅéÂü∫Á§éÁöÑÂ≠óÂπï‰ªªÂãôÔºåËÄåÊú™ÂàÜÊûêÊï¥ÈÉ®ÈõªÂΩ±‰∏≠‰∫ã‰ª∂ÁôºÁîüÁöÑÂéüÂõ†„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫Âç≥‰ΩøÊòØÈÄ≤ÈöéÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰πüÊúÉÂõ†ÁÇ∫ÂèóÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ËÄåÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑÂ§öÊ®°ÊÖãË≥áË®ä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÂâçÁΩÆË©ûÂ¢ûÂº∑ MLLM (TSPE) ÊñπÊ≥ïÔºåÁî®ÊñºÈõªÂΩ±ÂΩ±Áâá‰∏≠ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†Ôºå‰πüÂ∞±ÊòØÂ∞áÁõ∏Èóú‰∫ã‰ª∂ËàáÂÖ∂Âõ†ÊûúË™ûÊÑèÈÄ£ÁµêËµ∑‰æÜ„ÄÇÂú®Â±ÄÈÉ®ÈöéÊÆµÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰∫íÂãïÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁâáÊÆµ‰∏≠ÁöÑÁõ∏ÈóúÂ§öÊ®°ÊÖãË≥áË®äÔºåÁ∞°Ë¶ÅÂú∞Á∏ΩÁµêÂñÆ‰∏Ä‰∫ã‰ª∂„ÄÇÁõ∏ÊáâÂú∞ÔºåÂú®Êï¥È´îÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî®Êé®ÁêÜÁü•Ë≠òÂúñË≠úÂº∑ÂåñÁõ∏Èóú‰∫ã‰ª∂‰πãÈñìÁöÑÈÄ£ÁµêÔºå‰∏¶Ë®≠Ë®à‰∫Ü‰∏ÄÂÄã‰∫ã‰ª∂ÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÁõ∏Èóú‰∫ã‰ª∂ÔºåËÄåÈùûÊâÄÊúâÂâçÁΩÆÁâáÊÆµÔºåÈÄ≤ËÄåÁî¢ÁîüÊ∫ñÁ¢∫ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†„ÄÇÂ∞çÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÁöÑÂÖ®Èù¢Ë©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇ

##### **ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**
2409.09318v1 by Yahan Tu, Rui Hu, Jitao Sang

Hallucination poses a significant challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
static, which can lead to potential data contamination. This paper introduces
ODE, an open-set, dynamic protocol for evaluating object existence
hallucinations in MLLMs. Our framework employs graph structures to model
associations between real-word concepts and generates novel samples for both
general and domain-specific scenarios. The dynamic combination of concepts,
along with various combination principles, ensures a broad sample distribution.
Experimental results show that MLLMs exhibit higher hallucination rates with
ODE-generated samples, effectively avoiding data contamination. Moreover, these
samples can also be used for fine-tuning to improve MLLM performance on
existing benchmarks.

ÊëòË¶ÅÔºöÂπªË¶∫Â∞çÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË©ï‰º∞ÂπªË¶∫Âü∫Ê∫ñÊòØÈùúÊÖãÁöÑÔºåÈÄôÂèØËÉΩÂ∞éËá¥ÊΩõÂú®ÁöÑË≥áÊñôÊ±°Êüì„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü ODEÔºå‰∏ÄÁ®ÆÈñãÊîæÂºè„ÄÅÂãïÊÖãÁöÑÂçîÂÆöÔºåÁî®ÊñºË©ï‰º∞ MLLM ‰∏≠ÁöÑÁâ©‰ª∂Â≠òÂú®ÂπªË¶∫„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊé°Áî®ÂúñÂΩ¢ÁµêÊßã‰æÜÂª∫Ê®°ÁúüÂØ¶‰∏ñÁïåÊ¶ÇÂøµ‰πãÈñìÁöÑÈóúËÅØÔºå‰∏¶ÁÇ∫‰∏ÄËà¨ÂíåÁâπÂÆöÈ†òÂüüÊÉÖÂ¢ÉÁî¢ÁîüÊñ∞ÁöÑÁØÑ‰æã„ÄÇÊ¶ÇÂøµÁöÑÂãïÊÖãÁµÑÂêàÔºå‰ª•ÂèäÂêÑÁ®ÆÁµÑÂêàÂéüÂâáÔºåÁ¢∫‰øù‰∫ÜÂª£Ê≥õÁöÑÁØÑ‰æãÂàÜ‰Ωà„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåMLLM Âú® ODE ÁîüÊàêÁöÑÁØÑ‰æã‰∏≠Ë°®ÁèæÂá∫ËºÉÈ´òÁöÑÂπªË¶∫ÁéáÔºåÊúâÊïàÈÅøÂÖç‰∫ÜË≥áÊñôÊ±°Êüì„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÁØÑ‰æã‰πüÂèØË¢´Áî®ÊñºÂæÆË™øÔºå‰ª•ÊîπÂñÑ MLLM Âú®ÁèæÊúâÂü∫Ê∫ñ‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**
2409.09026v1 by Florian Gr√∂tschla, Luca Str√§ssle, Luca A. Lanzend√∂rfer, Roger Wattenhofer

Music recommender systems frequently utilize network-based models to capture
relationships between music pieces, artists, and users. Although these
relationships provide valuable insights for predictions, new music pieces or
artists often face the cold-start problem due to insufficient initial
information. To address this, one can extract content-based information
directly from the music to enhance collaborative-filtering-based methods. While
previous approaches have relied on hand-crafted audio features for this
purpose, we explore the use of contrastively pretrained neural audio embedding
models, which offer a richer and more nuanced representation of music. Our
experiments demonstrate that neural embeddings, particularly those generated
with the Contrastive Language-Audio Pretraining (CLAP) model, present a
promising approach to enhancing music recommendation tasks within graph-based
frameworks.

ÊëòË¶ÅÔºöÈü≥Ê®ÇÊé®Ëñ¶Á≥ªÁµ±Á∂ìÂ∏∏‰ΩøÁî®Âü∫ÊñºÁ∂≤Ë∑ØÁöÑÊ®°Âûã‰æÜÊì∑ÂèñÈü≥Ê®Ç‰ΩúÂìÅ„ÄÅËóùË°ìÂÆ∂Âíå‰ΩøÁî®ËÄÖ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÈóú‰øÇÁÇ∫È†êÊ∏¨Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ΩÜÁî±ÊñºÂàùÂßãË≥áË®ä‰∏çË∂≥ÔºåÊñ∞ÁöÑÈü≥Ê®Ç‰ΩúÂìÅÊàñËóùË°ìÂÆ∂Á∂ìÂ∏∏Èù¢Ëá®ÂÜ∑ÂïüÂãïÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂèØ‰ª•ÂæûÈü≥Ê®Ç‰∏≠Áõ¥Êé•Êì∑ÂèñÂü∫ÊñºÂÖßÂÆπÁöÑË≥áË®äÔºå‰ª•Â¢ûÂº∑Âü∫ÊñºÂçîÂêåÈÅéÊøæÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰æùË≥¥ÊâãÂ∑•Ë£Ω‰ΩúÁöÑÈü≥Ë®äÁâπÂæµ‰æÜÈÅîÊàêÊ≠§ÁõÆÁöÑÔºå‰ΩÜÊàëÂÄëÊé¢Á¥¢‰ΩøÁî®Â∞çÊØîÈ†êË®ìÁ∑¥Á•ûÁ∂ìÈü≥Ë®äÂµåÂÖ•Ê®°ÂûãÔºåÈÄôÊèê‰æõ‰∫ÜÊõ¥Ë±êÂØå‰∏îÊõ¥Á¥∞Á∑ªÁöÑÈü≥Ê®ÇË°®Á§∫„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÁ•ûÁ∂ìÂµåÂÖ•ÔºåÁâπÂà•ÊòØ‰ΩøÁî®Â∞çÊØîË™ûË®ÄÈü≥Ë®äÈ†êË®ìÁ∑¥ (CLAP) Ê®°ÂûãÁî¢ÁîüÁöÑÂµåÂÖ•ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Áî®ÊñºÂ¢ûÂº∑ÂúñÂΩ¢ÂåñÊ°ÜÊû∂‰∏≠ÁöÑÈü≥Ê®ÇÊé®Ëñ¶‰ªªÂãô„ÄÇ

##### **Contri(e)ve: Context + Retrieve for Scholarly Question Answering**
2409.09010v1 by Kanchan Shivashankar, Nadine Steinmetz

Scholarly communication is a rapid growing field containing a wealth of
knowledge. However, due to its unstructured and document format, it is
challenging to extract useful information from them through conventional
document retrieval methods. Scholarly knowledge graphs solve this problem, by
representing the documents in a semantic network, providing, hidden insights,
summaries and ease of accessibility through queries. Naturally, question
answering for scholarly graphs expands the accessibility to a wider audience.
But some of the knowledge in this domain is still presented as unstructured
text, thus requiring a hybrid solution for question answering systems. In this
paper, we present a two step solution using open source Large Language
Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the
context pertaining to the question from different structured and unstructured
data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,
we implement prompt engineering to improve the information retrieval
performance of the LLM. Our approach achieved an F1 score of 40% and also
observed some anomalous responses from the LLM, that are discussed in the final
part of the paper.

ÊëòË¶ÅÔºöÂ≠∏Ë°ì‰∫§ÊµÅÊòØ‰∏ÄÂÄãÂø´ÈÄüÊàêÈï∑ÁöÑÈ†òÂüüÔºåÂåÖÂê´‰∫ÜË±êÂØåÁöÑÁü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÖ∂ÈùûÁµêÊßãÂåñÂíåÊñá‰ª∂Ê†ºÂºèÔºåÈÄèÈÅéÂÇ≥Áµ±ÁöÑÊñá‰ª∂Ê™¢Á¥¢ÊñπÊ≥ïÂæàÈõ£Âæû‰∏≠ËêÉÂèñÂá∫ÊúâÁî®ÁöÑË≥áË®ä„ÄÇÂ≠∏Ë°ìÁü•Ë≠òÂúñË≠úËß£Ê±∫‰∫ÜÈÄôÂÄãÂïèÈ°åÔºåÂÆÉ‰ª•Ë™ûÁæ©Á∂≤Ë∑ØÂëàÁèæÊñá‰ª∂ÔºåÊèê‰æõÈö±ËóèÁöÑË¶ãËß£„ÄÅÊëòË¶ÅÂíåÈÄèÈÅéÊü•Ë©¢ËºïÈ¨ÜÂ≠òÂèñ„ÄÇËá™ÁÑ∂Âú∞ÔºåÂ≠∏Ë°ìÂúñË≠úÁöÑÂïèÁ≠îÊì¥Â±ï‰∫ÜÂ∞çÊõ¥Âª£Ê≥õÂèóÁúæÁöÑÂ≠òÂèñÊÄß„ÄÇ‰ΩÜÈÄôÂÄãÈ†òÂüü‰∏≠ÁöÑ‰∏Ä‰∫õÁü•Ë≠ò‰ªçÁÑ∂‰ª•ÈùûÁµêÊßãÂåñÊñáÂ≠óÂëàÁèæÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÂÄãÊ∑∑ÂêàËß£Ê±∫ÊñπÊ°à‰æÜÈÄ≤Ë°åÂïèÁ≠îÁ≥ªÁµ±„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÖ©Ê≠•È©üËß£Ê±∫ÊñπÊ°àÔºöLlama3.1 for Scholarly-QALD Ë≥áÊñôÈõÜ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæû‰∏çÂêåÁöÑÁµêÊßãÂåñÂíåÈùûÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫ê‰∏≠ËêÉÂèñËàáÂïèÈ°åÁõ∏ÈóúÁöÑËÑàÁµ°ÔºöDBLP„ÄÅSemOpenAlex Áü•Ë≠òÂúñË≠úÂíåÁ∂≠Âü∫ÁôæÁßëÊñáÂ≠ó„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂØ¶‰ΩúÊèêÁ§∫Â∑•Á®ã‰ª•ÊîπÂñÑ LLM ÁöÑË≥áË®äÊ™¢Á¥¢ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫Ü 40% ÁöÑ F1 ÂàÜÊï∏Ôºå‰∏¶‰∏î‰πüËßÄÂØüÂà∞ LLM ÁöÑ‰∏Ä‰∫õÁï∞Â∏∏ÂõûÊáâÔºåÈÄô‰∫õÂõûÊáâÂú®Êú¨ÊñáÁöÑÊúÄÂæå‰∏ÄÈÉ®ÂàÜ‰∏≠ÈÄ≤Ë°å‰∫ÜË®éË´ñ„ÄÇ

##### **SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**
2409.09007v1 by Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan

Learning representations on large graphs is a long-standing challenge due to
the inter-dependence nature. Transformers recently have shown promising
performance on small graphs thanks to its global attention for capturing
all-pair interactions beyond observed structures. Existing approaches tend to
inherit the spirit of Transformers in language and vision tasks, and embrace
complicated architectures by stacking deep attention-based propagation layers.
In this paper, we attempt to evaluate the necessity of adopting multi-layer
attentions in Transformers on graphs, which considerably restricts the
efficiency. Specifically, we analyze a generic hybrid propagation layer,
comprised of all-pair attention and graph-based propagation, and show that
multi-layer propagation can be reduced to one-layer propagation, with the same
capability for representation learning. It suggests a new technical path for
building powerful and efficient Transformers on graphs, particularly through
simplifying model architectures without sacrificing expressiveness. As
exemplified by this work, we propose a Simplified Single-layer Graph
Transformers (SGFormer), whose main component is a single-layer global
attention that scales linearly w.r.t. graph sizes and requires none of any
approximation for accommodating all-pair interactions. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M, yielding
orders-of-magnitude inference acceleration over peer Transformers on
medium-sized graphs, and demonstrates competitiveness with limited labeled
data.

ÊëòË¶ÅÔºöÂú®Â§ßÂûãÂúñË°®‰∏äÂ≠∏ÁøíË°®ÂæµÁî±ÊñºÁõ∏‰∫í‰æùË≥¥ÁöÑÊÄßË≥™ËÄåÊàêÁÇ∫‰∏ÄÈ†ÖÈï∑ÊúüÁöÑÊåëÊà∞„ÄÇÁî±Êñº Transfomer ËÉΩÂ§†ÈáùÂ∞çÊâÄÊúâÊàêÂ∞ç‰∫íÂãïÈÄ≤Ë°åÂÖ®Â±ÄÈóúÊ≥®ÔºåË∂ÖË∂äËßÄÊ∏¨ÁµêÊßãÔºåÂõ†Ê≠§ÊúÄËøëÂú®Â∞èÂûãÂúñË°®‰∏äÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÂÇæÂêëÊñºÁπºÊâø Transformer Âú®Ë™ûË®ÄÂíåË¶ñË¶∫‰ªªÂãô‰∏≠ÁöÑÁ≤æÁ•ûÔºå‰∏¶ÈÄöÈÅéÂ†ÜÁñäÂü∫ÊñºÊ∑±Â∫¶ÈóúÊ≥®ÁöÑÂÇ≥Êí≠Â±§‰æÜÊé°Áî®Ë§áÈõúÁöÑÊû∂Êßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Ë©ï‰º∞Âú®ÂúñË°®‰∏äÊé°Áî®Â§öÂ±§Ê≥®ÊÑèÂäõ Transformer ÁöÑÂøÖË¶ÅÊÄßÔºåÈÄôÊ•µÂ§ßÂú∞ÈôêÂà∂‰∫ÜÊïàÁéá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü‰∏ÄÂÄãÈÄöÁî®ÁöÑÊ∑∑ÂêàÂÇ≥Êí≠Â±§ÔºåÂÆÉÂåÖÂê´ÊâÄÊúâÊàêÂ∞çÊ≥®ÊÑèÂäõÂíåÂü∫ÊñºÂúñË°®ÁöÑÂÇ≥Êí≠Ôºå‰∏¶Ë°®ÊòéÂ§öÂ±§ÂÇ≥Êí≠ÂèØ‰ª•Á∞°ÂåñÁÇ∫ÂñÆÂ±§ÂÇ≥Êí≠ÔºåÂÖ∑ÊúâÁõ∏ÂêåÁöÑË°®ÂæµÂ≠∏ÁøíËÉΩÂäõ„ÄÇÈÄôÁÇ∫Âú®ÂúñË°®‰∏äÊßãÂª∫Âº∑Â§ßËÄåÈ´òÊïàÁöÑ Transformer Êèê‰æõ‰∫Ü‰∏ÄÊ¢ùÊñ∞ÁöÑÊäÄË°ìË∑ØÂæëÔºåÁâπÂà•ÊòØÈÄöÈÅéÁ∞°ÂåñÊ®°ÂûãÊû∂ÊßãÔºåËÄåÁÑ°ÈúÄÁäßÁâ≤Ë°®ÈÅîËÉΩÂäõ„ÄÇÊ≠£Â¶ÇÈÄôÈ†ÖÂ∑•‰ΩúÊâÄ‰æãË≠âÁöÑÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂåñÁöÑÂñÆÂ±§ÂúñÂΩ¢ Transformer (SGFormer)ÔºåÂÖ∂‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÊòØ‰∏ÄÂÄãÂñÆÂ±§ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºåÂÆÉËàáÂúñÂΩ¢Â§ßÂ∞èÊàêÁ∑öÊÄßÊØî‰æãÔºå‰∏¶‰∏î‰∏çÈúÄË¶Å‰ªª‰ΩïËøë‰ºº‰æÜÈÅ©ÊáâÊâÄÊúâÊàêÂ∞ç‰∫íÂãï„ÄÇÊ†πÊìöÁ∂ìÈ©óÔºåSGFormer ÊàêÂäüÂú∞Êì¥Â±ïÂà∞Á∂≤Ë∑ØË¶èÊ®°ÁöÑÂúñË°® ogbn-papers100MÔºåÂú®‰∏≠Á≠âÂ§ßÂ∞èÁöÑÂúñË°®‰∏äÁî¢Áîü‰∫ÜÊØîÂêåÂÑï Transformer Âø´ÂπæÂÄãÊï∏ÈáèÁ¥öÁöÑÊé®Ë´ñÂä†ÈÄüÔºå‰∏¶Ë≠âÊòé‰∫ÜÂú®Ê®ôÁ±§Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇ

##### **Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**
2409.08864v1 by Zhiqiang Zhong, Davide Mottin

Large Language Models (LLMs) have shown remarkable capabilities in processing
various data structures, including graphs. While previous research has focused
on developing textual encoding methods for graph representation, the emergence
of multimodal LLMs presents a new frontier for graph comprehension. These
advanced models, capable of processing both text and images, offer potential
improvements in graph understanding by incorporating visual representations
alongside traditional textual data. This study investigates the impact of graph
visualisations on LLM performance across a range of benchmark tasks at node,
edge, and graph levels. Our experiments compare the effectiveness of multimodal
approaches against purely textual graph representations. The results provide
valuable insights into both the potential and limitations of leveraging visual
graph modalities to enhance LLMs' graph structure comprehension abilities.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËôïÁêÜÂêÑÁ®ÆÊï∏ÊìöÁµêÊßãÔºàÂåÖÊã¨ÂúñÂΩ¢ÔºâÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑÁ†îÁ©∂ËëóÈáçÊñºÈñãÁôºÂúñÂΩ¢Ë°®Á§∫ÁöÑÊñáÊú¨Á∑®Á¢ºÊñπÊ≥ïÔºå‰ΩÜÂ§öÊ®°ÊÖã LLM ÁöÑÂá∫ÁèæÁÇ∫ÂúñÂΩ¢ÁêÜËß£Êèê‰æõ‰∫ÜÊñ∞ÁöÑÈ†òÂüü„ÄÇÈÄô‰∫õÂÖàÈÄ≤ÁöÑÊ®°ÂûãËÉΩÂ§†ËôïÁêÜÊñáÊú¨ÂíåÂúñÂÉèÔºåÈÄèÈÅéÁµêÂêàË¶ñË¶∫Ë°®Á§∫ËàáÂÇ≥Áµ±ÊñáÊú¨Ë≥áÊñôÔºåÊèê‰æõÂúñÂΩ¢ÁêÜËß£ÁöÑÊΩõÂú®ÊîπÈÄ≤„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂúñÂΩ¢Ë¶ñË¶∫ÂåñÂ∞ç LLM Âú®ÁØÄÈªû„ÄÅÈÇäÁ∑£ÂíåÂúñÂΩ¢Â±§Á¥ö‰∏ÄÁ≥ªÂàóÂü∫Ê∫ñ‰ªªÂãôÁöÑÊïàËÉΩÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊØîËºÉ‰∫ÜÂ§öÊ®°ÊÖãÊñπÊ≥ïËàáÁ¥îÊñáÊú¨ÂúñÂΩ¢Ë°®Á§∫ÁöÑÊúâÊïàÊÄß„ÄÇÁµêÊûúÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£Âà©Áî®Ë¶ñË¶∫ÂúñÂΩ¢Ê®°ÊÖã‰æÜÂ¢ûÂº∑ LLM ÂúñÂΩ¢ÁµêÊßãÁêÜËß£ËÉΩÂäõÁöÑÊΩõÂäõËàáÈôêÂà∂„ÄÇ

##### **A RAG Approach for Generating Competency Questions in Ontology Engineering**
2409.08820v1 by Xueli Pan, Jacco van Ossenbruggen, Victor de Boer, Zhisheng Huang

Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.

ÊëòË¶ÅÔºöËÉΩÂäõÂïèÈ°å (CQ) ÁöÑÂà∂ÂÆöÊòØÂπæÂÄãÊú¨‰ΩìË´ñÁôºÂ±ïÂíåË©ï‰º∞ÊñπÊ≥ïÁöÑ‰∏≠ÂøÉ„ÄÇÂÇ≥Áµ±‰∏äÔºåÂà∂ÂÆöÈÄô‰∫õËÉΩÂäõÂïèÈ°åÁöÑ‰ªªÂãôÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÈ†òÂüüÂ∞àÂÆ∂ÂíåÁü•Ë≠òÂ∑•Á®ãÂ∏´ÁöÑÂä™ÂäõÔºåÈÄôÈÄöÂ∏∏ÊòØËÄóÊôÇ‰∏îÂãûÂäõÂØÜÈõÜÁöÑ„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåËá™ÂãïÂåñÂíåÂ¢ûÂº∑Ê≠§ÈÅéÁ®ãÁöÑÂèØËÉΩÊÄßÂá∫Áèæ‰∫Ü„ÄÇËàáÂÖ∂‰ªñ‰ΩøÁî®ÁèæÊúâÊú¨‰ΩìË´ñÊàñÁü•Ë≠òÂúñË≠ú‰ΩúÁÇ∫ LLM Ëº∏ÂÖ•ÁöÑÈ°û‰ººÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî® LLM Ëá™ÂãïÁîüÊàêË¢´Ë™çÁÇ∫ÊòØÈ†òÂüüÁü•Ë≠òÂ∫´ÁöÑ‰∏ÄÁµÑÁßëÂ≠∏Ë´ñÊñáÁöÑ CQ„ÄÇÊàëÂÄëÁ†îÁ©∂ÂÖ∂ÊÄßËÉΩÔºåÁâπÂà•ÊòØÊàëÂÄëÁ†îÁ©∂‰∏çÂêåÊï∏ÈáèÁöÑË´ñÊñáÂ∞ç RAG ÁöÑÂΩ±ÈüøÂíå LLM ÁöÑ‰∏çÂêåÊ∫´Â∫¶Ë®≠ÁΩÆ„ÄÇÊàëÂÄë‰ΩøÁî® GPT-4 Â∞çÂÖ©ÂÄãÈ†òÂüüÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÈÄ≤Ë°åÂØ¶È©óÔºå‰∏¶Â∞áÁµêÊûúËàáÁî±È†òÂüüÂ∞àÂÆ∂ÊßãÈÄ†ÁöÑÁúüÂØ¶ CQ ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂà©Áî®Ë©ï‰º∞ÊåáÊ®ôÔºàÁ≤æÁ¢∫Â∫¶Âíå‰∏ÄËá¥ÊÄßÔºâÂ∞çÁµêÊûúÈÄ≤Ë°åÁöÑÂØ¶Ë≠âË©ï‰º∞Ë°®ÊòéÔºåËàáÈõ∂Ê¨°ÊèêÁ§∫Áõ∏ÊØîÔºåÂ∞áÁõ∏ÈóúÈ†òÂüüÁü•Ë≠òÊ∑ªÂä†Âà∞ RAG ÂèØ‰ª•ÊèêÈ´ò LLM Âú®ÁÇ∫ÂÖ∑È´îÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÁîüÊàê CQ ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ

##### **ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**
2409.08543v1 by Zezheng Qin

Recommender Systems (RS) play a pivotal role in boosting user satisfaction by
providing personalized product suggestions in domains such as e-commerce and
entertainment. This study examines the integration of multimodal data text and
audio into large language models (LLMs) with the aim of enhancing
recommendation performance. Traditional text and audio recommenders encounter
limitations such as the cold-start problem, and recent advancements in LLMs,
while promising, are computationally expensive. To address these issues,
Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without
compromising performance. The ATFLRec framework is proposed to integrate audio
and text modalities into a multimodal recommendation system, utilizing various
LoRA configurations and modality fusion techniques. Results indicate that
ATFLRec outperforms baseline models, including traditional and graph neural
network-based approaches, achieving higher AUC scores. Furthermore, separate
fine-tuning of audio and text data with distinct LoRA modules yields optimal
performance, with different pooling methods and Mel filter bank numbers
significantly impacting performance. This research offers valuable insights
into optimizing multimodal recommender systems and advancing the integration of
diverse data modalities in LLMs.

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶‰∏≠ÊâÆÊºîËëóËàâË∂≥ËºïÈáçÁöÑËßíËâ≤ÔºåÂÆÉÂú®ÈõªÂ≠êÂïÜÂãôÂíåÂ®õÊ®ÇÁ≠âÈ†òÂüüÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÁî¢ÂìÅÂª∫Ë≠∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂ∞áÂ§öÊ®°ÊÖãË≥áÊñôÊñáÂ≠óÂíåÈü≥Ë®äÊï¥ÂêàÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ôºå‰ª•Â¢ûÂº∑Êé®Ëñ¶ÊïàËÉΩ„ÄÇÂÇ≥Áµ±ÁöÑÊñáÂ≠óÂíåÈü≥Ë®äÊé®Ëñ¶Âô®ÊúÉÈÅáÂà∞ÂÜ∑ÂïüÂãïÂïèÈ°åÁ≠âÈôêÂà∂ÔºåËÄå LLM ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÈõñÁÑ∂ÂæàÊúâÂâçÊôØÔºå‰ΩÜË®àÁÆóÊàêÊú¨ÂæàÈ´ò„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂºïÂÖ•‰∫Ü‰ΩéÁß©ÈÅ©Êáâ (LoRA)ÔºåÂÆÉÂú®‰∏çÂΩ±ÈüøÊïàËÉΩÁöÑÊÉÖÊ≥Å‰∏ãÊèêÂçá‰∫ÜÊïàÁéá„ÄÇATFLRec Ê°ÜÊû∂Ë¢´ÊèêÂá∫‰æÜÂ∞áÈü≥Ë®äÂíåÊñáÂ≠óÊ®°ÊÖãÊï¥ÂêàÂà∞Â§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÔºåÂà©Áî®ÂêÑÁ®Æ LoRA ÈÖçÁΩÆÂíåÊ®°ÊÖãËûçÂêàÊäÄË°ì„ÄÇÁµêÊûúË°®ÊòéÔºåATFLRec ÂÑ™ÊñºÂü∫Á∑öÊ®°ÂûãÔºåÂåÖÊã¨ÂÇ≥Áµ±ÂíåÂü∫ÊñºÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊñπÊ≥ïÔºåÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑ AUC ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®‰∏çÂêåÁöÑ LoRA Ê®°ÁµÑÂ∞çÈü≥Ë®äÂíåÊñáÂ≠óË≥áÊñôÈÄ≤Ë°åÂñÆÁç®ÂæÆË™øÊúÉÁî¢ÁîüÊúÄ‰Ω≥ÊïàËÉΩÔºå‰∏çÂêåÁöÑÊ±†ÂåñÊñπÊ≥ïÂíå Mel ÊøæÊ≥¢Âô®ÁµÑÊï∏ÊúÉÂ∞çÊïàËÉΩÁî¢ÁîüÈ°ØËëóÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£ÔºåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂ§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±Ôºå‰∏¶Êé®ÂãïÂ∞á‰∏çÂêåÁöÑË≥áÊñôÊ®°ÊÖãÊï¥ÂêàÂà∞ LLM ‰∏≠„ÄÇ

##### **What Makes a Maze Look Like a Maze?**
2409.08202v1 by Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu

A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.

ÊëòË¶ÅÔºö‰∫∫È°ûË¶ñË¶∫ÁêÜËß£ÁöÑÁç®ÁâπÈù¢ÂêëÂú®ÊñºÈùàÊ¥ªË©ÆÈáãÊäΩË±°Ê¶ÇÂøµÁöÑËÉΩÂäõÔºöÁç≤ÂèñËß£ÈáãÂÖ∂Ë±°ÂæµÊÑèÁæ©ÁöÑÊèêÂçáË¶èÂâáÔºåÂú®ÁÜüÊÇâÂíå‰∏çÁÜüÊÇâÁöÑËÉåÊôØ‰∏ãÂ•†ÂÆöÂÖ∂Âü∫Á§éÔºå‰∏¶Â∞çÂÖ∂ÈÄ≤Ë°åÈ†êÊ∏¨ÊàñÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÁèæÊàêÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÊìÖÈï∑Â∞çÂΩ±ÂÉèÈÄ≤Ë°åÂ≠óÈù¢Ë©ÆÈáãÔºà‰æãÂ¶ÇËæ®Ë≠òÊ®πÊûùÁ≠âÁâ©È´îÈ°ûÂà•ÔºâÔºå‰ΩÜÂÆÉÂÄëÂú®ÁêÜËß£Ê≠§È°ûË¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÊôÇ‰ªçÊúâÂõ∞Èõ£Ôºà‰æãÂ¶ÇÊ®πÊûùÁöÑÊéíÂàóÂ¶Ç‰ΩïÂΩ¢ÊàêËø∑ÂÆÆÁöÑÁâÜÂ£ÅÔºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊ∑±Â∫¶Ê®°ÂºèÂü∫Á§éÔºàDSGÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂà©Áî®Ë¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÁöÑÊòéÁ¢∫ÁµêÊßãÂåñË°®Á§∫‰æÜÈÄ≤Ë°åÂü∫Á§éÂíåÊé®ÁêÜ„ÄÇDSG ÁöÑÊ†∏ÂøÉÊòØÊ®°Âºè‚Äî‚ÄîÊäΩË±°Ê¶ÇÂøµÁöÑ‰æùË≥¥ÂúñÊèèËø∞ÔºåÂ∞áÂÖ∂ÂàÜËß£ÁÇ∫Êõ¥ÂéüÂßãÂ±§Á¥öÁöÑÁ¨¶Ëôü„ÄÇDSG ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÊèêÂèñÊ®°ÂºèÔºåÁÑ∂ÂæåÂ∞áÊ®°ÂºèÁöÑÂÖ∑È´îÁµÑÊàêÈÉ®ÂàÜÂàÜÂ±§Âü∫Á§éÂà∞ÂΩ±ÂÉè‰∏äÔºå‰∏¶‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã„ÄÇÂü∫Á§éÊ®°ÂºèÁî®ÊñºÊì¥ÂÖÖË¶ñË¶∫ÊäΩË±°ÁêÜËß£„ÄÇÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫Ü DSG ÂíåÊàëÂÄëÁöÑÊñ∞Ë¶ñË¶∫ÊäΩË±°Ë≥áÊñôÈõÜ‰∏äÁöÑ‰∏çÂêåÊé®ÁêÜÊñπÊ≥ïÔºåË©≤Ë≥áÊñôÈõÜÂåÖÂê´ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåÁöÑÊäΩË±°Ê¶ÇÂøµÂΩ±ÂÉèÔºå‰ª•ÂèäÁî±‰∫∫È°ûÊ®ôË®òÁöÑÂ∞çÊáâÂïèÈ°åËß£Á≠îÂ∞ç„ÄÇÊàëÂÄëË≠âÊòé DSG Â§ßÂπÖÊèêÂçá‰∫ÜË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÊäΩË±°Ë¶ñË¶∫Êé®ÁêÜÊïàËÉΩÔºå‰∏¶‰∏îÊúùËëóËàá‰∫∫È°û‰∏ÄËá¥ÁöÑË¶ñË¶∫ÊäΩË±°ÁêÜËß£ÈÇÅÈÄ≤‰∏ÄÊ≠•„ÄÇ

##### **Towards a graph-based foundation model for network traffic analysis**
2409.08111v1 by Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros

Foundation models have shown great promise in various fields of study. A
potential application of such models is in computer network traffic analysis,
where these models can grasp the complexities of network traffic dynamics and
adapt to any specific task or network environment with minimal fine-tuning.
Previous approaches have used tokenized hex-level packet data and the model
architecture of large language transformer models. We propose a new, efficient
graph-based alternative at the flow-level. Our approach represents network
traffic as a dynamic spatio-temporal graph, employing a self-supervised link
prediction pretraining task to capture the spatial and temporal dynamics in
this network graph framework. To evaluate the effectiveness of our approach, we
conduct a few-shot learning experiment for three distinct downstream network
tasks: intrusion detection, traffic classification, and botnet classification.
Models finetuned from our pretrained base achieve an average performance
increase of 6.87\% over training from scratch, demonstrating their ability to
effectively learn general network traffic dynamics during pretraining. This
success suggests the potential for a large-scale version to serve as an
operational foundational model.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤Âú®ÂêÑÂÄãÁ†îÁ©∂È†òÂüü‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÂâçÊôØ„ÄÇÊ≠§È°ûÊ®°ÂûãÁöÑÊΩõÂú®ÊáâÁî®‰πã‰∏ÄÂú®ÊñºÈõªËÖ¶Á∂≤Ë∑ØÊµÅÈáèÂàÜÊûêÔºåÂÖ∂‰∏≠ÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÊéåÊè°Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑË§áÈõúÊÄßÔºå‰∏¶‰ª•ÊúÄÂ∞èÁöÑÂæÆË™øÈÅ©Êáâ‰ªª‰ΩïÁâπÂÆö‰ªªÂãôÊàñÁ∂≤Ë∑ØÁí∞Â¢É„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰ΩøÁî®Ê®ôË®òÂåñÂçÅÂÖ≠ÈÄ≤‰ΩçÂ±§Á¥öÂ∞ÅÂåÖË≥áÊñôÂíåÂ§ßÂûãË™ûË®ÄËΩâÊèõÂô®Ê®°ÂûãÁöÑÊ®°ÂûãÊû∂Êßã„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑ„ÄÅÊúâÊïàÁöÑÊµÅÁ®ãÂ±§Á¥öÂúñÂΩ¢ÂåñÊõø‰ª£ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÁ∂≤Ë∑ØÊµÅÈáèË°®Á§∫ÁÇ∫ÂãïÊÖãÊôÇÁ©∫ÂúñÂΩ¢ÔºåÊé°Áî®Ëá™ÊàëÁõ£Áù£ÈÄ£ÁµêÈ†êÊ∏¨È†êË®ìÁ∑¥‰ªªÂãô‰æÜÊçïÊçâÊ≠§Á∂≤Ë∑ØÂúñÂΩ¢Êû∂Êßã‰∏≠ÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÂãïÊÖã„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂ∞ç‰∏âÂÄã‰∏çÂêåÁöÑ‰∏ãÊ∏∏Á∂≤Ë∑Ø‰ªªÂãôÔºàÂÖ•‰æµÂÅµÊ∏¨„ÄÅÊµÅÈáèÂàÜÈ°ûÂíåÊÆ≠Â±çÁ∂≤Ë∑ØÂàÜÈ°ûÔºâÈÄ≤Ë°åÂ∞ëÈáèÂ≠∏ÁøíÂØ¶È©ó„ÄÇÂæûÊàëÂÄëÁöÑÈ†êË®ìÁ∑¥Âü∫Á§éÂæÆË™øÁöÑÊ®°ÂûãÔºåÂÖ∂Âπ≥ÂùáÊïàËÉΩÊèêÂçá 6.87%ÔºåÈ´òÊñºÂæûÈ†≠Ë®ìÁ∑¥ÔºåÈÄôË≠âÊòé‰∫ÜÂÆÉÂÄëÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂ≠∏Áøí‰∏ÄËà¨Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑËÉΩÂäõ„ÄÇÈÄôÈ†ÖÊàêÂäüÈ°ØÁ§∫Âá∫Â§ßË¶èÊ®°ÁâàÊú¨ÊúâÊΩõÂäõ‰ΩúÁÇ∫ÈÅã‰ΩúÂü∫Á§éÊ®°Âûã„ÄÇ

##### **Learning Rules from KGs Guided by Language Models**
2409.07869v1 by Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott

Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.

ÊëòË¶ÅÔºöË≥áË®äËêÉÂèñÁöÑÈÄ≤Â±ïÂ∑≤ËÉΩËá™ÂãïÂª∫ÊßãÂ§ßÂûãÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶Ç Yago„ÄÅWikidata Êàñ Google KGÔºâÔºåÈÄô‰∫õÁü•Ë≠òÂúñË≠úÂª£Ê≥õÁî®ÊñºË®±Â§öÊáâÁî®Á®ãÂºèÔºå‰æãÂ¶ÇË™ûÊÑèÊêúÂ∞ãÊàñË≥áÊñôÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÄô‰∫õÁü•Ë≠òÂúñË≠úÊòØÂçäËá™ÂãïÂª∫ÊßãÁöÑÔºåÂõ†Ê≠§ÈÄöÂ∏∏‰∏¶‰∏çÂÆåÊï¥„ÄÇË¶èÂâáÂ≠∏ÁøíÊñπÊ≥ïËëóÈáçÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñÈ†ªÁπÅÊ®°ÂºèÔºå‰∏¶Â∞áÂÆÉÂÄëËΩâÊèõÁÇ∫Ë¶èÂâáÔºåÂèØÊáâÁî®ÊñºÈ†êÊ∏¨ÊΩõÂú®ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÊ≠•È©üÊòØË¶èÂâáÊéíÂ∫è„ÄÇË¶èÂâáÊéíÂ∫èÂú®È´òÂ∫¶‰∏çÂÆåÊï¥ÊàñÊúâÂÅèÂ∑ÆÁöÑÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶ÇÔºå‰∏ªË¶ÅÂÑ≤Â≠òÂêç‰∫∫‰∫ãÂØ¶ÁöÑÁü•Ë≠òÂúñË≠úÔºâ‰∏≠ÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Âú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÊúâÂÅèÂ∑ÆÁöÑË¶èÂâáÂèØËÉΩÊúÄÁ¨¶ÂêàË≥áÊñôÔºå‰∏¶Ê†πÊìöÊ®ôÊ∫ñÁµ±Ë®àÈáèÂ∫¶Ôºà‰æãÂ¶ÇË¶èÂâá‰ø°ÂøÉÔºâÊéíÂú®ÊúÄÂâçÈù¢„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÊèêÂá∫‰∏çÂè™‰æùË≥¥ÂéüÂßãÁü•Ë≠òÂúñË≠úÔºåÈÇÑË¶Å‰æùË≥¥Áü•Ë≠òÂúñË≠úÂµåÂÖ•Ê®°ÂûãÈ†êÊ∏¨ÁöÑ‰∫ãÂØ¶‰æÜÂ∞çË¶èÂâáÈÄ≤Ë°åÊéíÂ∫è„ÄÇÂêåÊôÇÔºåÈö®ËëóË™ûË®ÄÊ®°Âûã (LM) ÁöÑËààËµ∑Ôºå‰∏Ä‰∫õÁ†îÁ©∂ËÅ≤Á®± LM ÂèØÁî®‰ΩúÁü•Ë≠òÂúñË≠úÂÆåÊàêÁöÑÊõø‰ª£ÊñπÊ≥ï„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÈ©óË≠âÂà©Áî® LM Âú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÊúâÂä©ÊñºÊèêÂçáË¶èÂâáÂ≠∏ÁøíÁ≥ªÁµ±ÁöÑÂìÅË≥™„ÄÇ

##### **Multi-object event graph representation learning for Video Question Answering**
2409.07747v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.

ÊëòË¶ÅÔºöÂΩ±ÁâáÂïèÁ≠î (VideoQA) ÊòØ‰∏ÄÈ†Ö‰ªªÂãôÔºåÁî®ÊñºÈ†êÊ∏¨ÈáùÂ∞çÁµ¶ÂÆöÂΩ±ÁâáÊèêÂá∫ÁöÑÂïèÈ°åÁöÑÊ≠£Á¢∫Á≠îÊ°à„ÄÇÁ≥ªÁµ±ÂøÖÈ†à‰∫ÜËß£ÂæûÂΩ±Áâá‰∏≠ÊèêÂèñÁöÑÁâ©‰ª∂‰πãÈñìÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÈóú‰øÇÔºåÊâçËÉΩÂü∑Ë°åÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂ÈõÜ‰∏≠Êñº‰ΩøÁî®Âü∫ÊñºTransformerÁöÑÊ®°Âûã‰æÜÂª∫Ê®°ÂÄãÂà•Áâ©‰ª∂ÁöÑÂãï‰ΩúÔºå‰ΩÜÂú®ÊçïÊçâÊ∂âÂèäÂ§öÂÄãÁâ©‰ª∂ÁöÑË§áÈõúÂ†¥ÊôØÔºà‰æãÂ¶Ç„Äå‰∏ÄÂÄãÁî∑Â≠©Ê≠£Âú®Â∞áÁêÉÊäïÈÄ≤Á±ÉÊ°Ü„ÄçÔºâÊôÇÔºåÂÆÉÂÄëÊúÉÂá∫ÁèæÂïèÈ°å„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çÊØîÂºèË™ûË®Ä‰∫ã‰ª∂ÂúñË°®Ë°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºåÁ®±ÁÇ∫ CLanGÔºå‰ª•Ëß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊçïÊçâËàáÂ§öÂÄãÁâ©‰ª∂Áõ∏ÈóúÁöÑ‰∫ã‰ª∂Ë°®Á§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî®Â§öÂ±§ GNN ÈõÜÁæ§Ê®°ÁµÑÈÄ≤Ë°åÂ∞çÊäóÂºèÂúñË°®Ë°®Á§∫Â≠∏ÁøíÔºå‰ΩøÂïèÈ°åÊñáÂ≠óÂèäÂÖ∂Áõ∏ÈóúÁöÑÂ§öÁâ©‰ª∂‰∫ã‰ª∂ÂúñË°®‰πãÈñìËÉΩÂ§†ÈÄ≤Ë°åÂ∞çÊØîÂºèÂ≠∏Áøí„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂº∑Â§ßÁöÑÂü∫Ê∫ñÔºåÂú®ÂÖ©ÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ VideoQA Ë≥áÊñôÈõÜ NExT-QA Âíå TGIF-QA-R ‰∏äÈÅîÂà∞‰∫ÜÈ´òÈÅî 2.2% ÁöÑÊõ¥È´òÊ∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÂú®ËôïÁêÜÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÂïèÈ°åÊñπÈù¢ÊØîÂü∫Ê∫ñÈ´òÂá∫ 2.8%ÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÂú®Êé®ÁêÜÂ§öÂÄãÂü∫ÊñºÁâ©‰ª∂ÁöÑ‰∫ã‰ª∂ÊñπÈù¢ÁöÑÂÑ™Âã¢„ÄÇ

##### **Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**
2409.07368v2 by Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

This paper introduces SGCode, a flexible prompt-optimizing system to generate
secure code with large language models (LLMs). SGCode integrates recent
prompt-optimization approaches with LLMs in a unified system accessible through
front-end and back-end APIs, enabling users to 1) generate secure code, which
is free of vulnerabilities, 2) review and share security analysis, and 3)
easily switch from one prompt optimization approach to another, while providing
insights on model and system performance. We populated SGCode on an AWS server
with PromSec, an approach that optimizes prompts by combining an LLM and
security tools with a lightweight generative adversarial graph neural network
to detect and fix security vulnerabilities in the generated code. Extensive
experiments show that SGCode is practical as a public tool to gain insights
into the trade-offs between model utility, secure code generation, and system
cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is
available at: http://3.131.141.63:8501/.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñá‰ªãÁ¥π‰∫Ü SGCodeÔºå‰∏ÄÂÄãÈùàÊ¥ªÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÁ≥ªÁµ±ÔºåÁî®ÊñºÁîüÊàêÂÖ∑ÂÇôÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÆâÂÖ®Á®ãÂºèÁ¢º„ÄÇSGCode Â∞áÊúÄËøëÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïËàá LLM Êï¥ÂêàÂú®‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÁ≥ªÁµ±‰∏≠ÔºåÂèØÈÄèÈÅéÂâçÁ´ØÂíåÂæåÁ´Ø API Â≠òÂèñÔºå‰ΩøÁî®Êà∂ËÉΩÂ§† 1) ÁîüÊàêÂÆâÂÖ®ÁöÑÁ®ãÂºèÁ¢ºÔºåÊ≤íÊúâÊºèÊ¥ûÔºå2) Ê™¢Èñ±ÂíåÂàÜ‰∫´ÂÆâÂÖ®ÊÄßÂàÜÊûêÔºå‰ª•Âèä 3) ËºïÈ¨ÜÂæû‰∏ÄÁ®ÆÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÂàáÊèõÂà∞Âè¶‰∏ÄÁ®ÆÔºåÂêåÊôÇÊèê‰æõÊ®°ÂûãÂíåÁ≥ªÁµ±ÊïàËÉΩÁöÑË¶ãËß£„ÄÇÊàëÂÄëÂú® AWS ‰º∫ÊúçÂô®‰∏ä‰ΩøÁî® PromSec Â°´ÂÖÖ SGCodeÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÁµêÂêà LLM ÂíåÂÆâÂÖ®ÊÄßÂ∑•ÂÖ∑ËàáËºïÈáèÁ¥öÁîüÊàêÂºèÂ∞çÊäóÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰æÜÊúÄ‰Ω≥ÂåñÊèêÁ§∫ÁöÑÊñπÊ≥ïÔºå‰ª•ÂÅµÊ∏¨‰∏¶‰øÆÂæ©ÁîüÊàêÁ®ãÂºèÁ¢º‰∏≠ÁöÑÂÆâÂÖ®ÊÄßÊºèÊ¥û„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåSGCode ‰ΩúÁÇ∫‰∏ÄÂÄãÂÖ¨Áî®Â∑•ÂÖ∑ÊòØÂØ¶Áî®ÁöÑÔºåÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÂØ¶Áî®ÊÄß„ÄÅÂÆâÂÖ®Á®ãÂºèÁ¢ºÁîüÊàêÂíåÁ≥ªÁµ±ÊàêÊú¨‰πãÈñìÁöÑÂèñÊç®„ÄÇËàáÊèêÁ§∫ LLM Áõ∏ÊØîÔºåSGCode ÂÉÖÊúâÂæÆÂ∞èÁöÑÊàêÊú¨„ÄÇSGCode ÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttp://3.131.141.63:8501/„ÄÇ

##### **Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**
2409.07088v1 by Daehee Kim, Deokhyung Kang, Sangwon Ryu, Gary Geunbae Lee

Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÂà∞ÊñáÂ≠ó (G2T) ÁîüÊàêÊ∂âÂèäÂ∞áÁµêÊßãÂåñÁü•Ë≠òÂúñË≠úË°®ÈÅîÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊîπÂñÑ‰∫Ü G2T ÁöÑÊïàËÉΩÔºå‰ΩÜÂÖ∂ÊúâÊïàÊÄßÂèñÊ±∫ÊñºÂÖ∑ÊúâÁ≤æÁ¢∫ÂúñÂΩ¢ÊñáÂ≠óÂ∞çÈΩäÁöÑË≥áÊñôÈõÜ„ÄÇÁÑ∂ËÄåÔºåÈ´òÂìÅË≥™„ÄÅ‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÊÄßÈôêÂà∂‰∫Ü‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêÁ†îÁ©∂ÁöÑÈÄ≤Â±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ∂≠Âü∫ÁôæÁßëÊú¨‰ΩìÂÖçË≤ªÂúñÂΩ¢ÊñáÂ≠óË≥áÊñôÈõÜ (WikiOFGraph)ÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå Data-QuestEval ÁöÑÊñ∞ÊñπÊ≥ïÁîüÊàêÁöÑÊñ∞Â§ßÂûã G2T Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñ∞Ë≥áÊñôÈõÜÂåÖÂê´ 585 Ëê¨ÂÄã‰∏ÄËà¨È†òÂüüÁöÑÂúñÂΩ¢ÊñáÂ≠óÂ∞çÔºåÊèê‰æõÈ´òÂúñÂΩ¢ÊñáÂ≠ó‰∏ÄËá¥ÊÄßÔºåËÄå‰∏ç‰æùË≥¥ÊñºÂ§ñÈÉ®Êú¨‰Ωì„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂú® WikiOFGraph ‰∏äÂæÆË™øÁöÑ PLM Âú®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏äÂÑ™ÊñºÂú®ÂÖ∂‰ªñË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ PLM„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñπÊ≥ïË¢´Ë≠âÊòéÊòØ‰∏ÄÂÄãÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÁîüÊàêÈ´òÂìÅË≥™ÁöÑ G2T Ë≥áÊñôÔºåÈ°ØËëóÊé®Âãï‰∫Ü G2T ÁîüÊàêÈ†òÂüüÁöÑÁôºÂ±ï„ÄÇ

##### **Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**
2409.07064v1 by Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen

Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.

ÊëòË¶ÅÔºöËá™ÂãïÂ∞çË©±Ë©ïÈáè‰∏≠ÁöÑËá™ÂãïÂåñÂè£Ë™™Ë©ïÈáèÔºàASACÔºâÊó®Âú®Ë©ï‰º∞ L2ÔºàÁ¨¨‰∫åË™ûË®ÄÔºâË©±ËÄÖÂú®Ëàá‰∏Ä‰ΩçÊàñÂ§ö‰ΩçÊáâË©¶ËÄÖ‰∫íÂãïÁöÑÁí∞Â¢É‰∏≠ÔºåÊï¥È´îÁöÑÂè£Ë™™ËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑ ASAC ÊñπÊ≥ïÂú®ÂÖ∂ÂêÑËá™ÁöÑË≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫ÊúâÂâçÈÄîÁöÑË°®ÁèæÔºå‰ΩÜ‰ªçÁº∫‰πèÂ∞àÊ≥®ÊñºÂ∞áÂ∞çË©±‰∏≠ÈÇèËºØÊµÅÁ®ãÁöÑÈÄ£Ë≤´ÊÄßÁ¥çÂÖ•Ë©ïÂàÜÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÈóúÈçµÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈöéÂ±§ÂºèÂúñÂΩ¢Ê®°ÂûãÔºåÂÆÉÈÅ©Áï∂Âú∞ÁµêÂêà‰∫ÜÂª£Ê≥õÁöÑÂõûÊáâÈñì‰∫íÂãïÔºà‰æãÂ¶ÇÔºöË™ûÁØáÈóú‰øÇÔºâÂíåÁ¥∞ÂæÆÁöÑË™ûÁæ©Ë≥áË®äÔºà‰æãÂ¶ÇÔºöË™ûÁæ©Â≠óË©ûÂíåË™™Ë©±ËÄÖÊÑèÂúñÔºâÔºåÈö®ÂæåËàáËÑàÁµ°Ë≥áË®äËûçÂêàÔºå‰ª•ÈÄ≤Ë°åÊúÄÁµÇÈ†êÊ∏¨„ÄÇÂú® NICT-JLE Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàá‰∏Ä‰∫õÂº∑Â§ßÁöÑÂü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÂª∫Ê®°ÊñπÊ≥ïÂèØ‰ª•È°ØËëóÊèêÂçáÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®ÂêÑÁ®ÆË©ïÈáèÊåáÊ®ôÊñπÈù¢„ÄÇÈÄô‰πüÈó°Êòé‰∫ÜÂú® ASAC ‰∏≠Êé¢Ë®éÂè£Ë™ûÂõûÊáâÁöÑÈÄ£Ë≤´ÊÄßÁõ∏ÈóúÈù¢ÂêëÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **FreeRide: Harvesting Bubbles in Pipeline Parallelism**
2409.06941v1 by Jiashu Zhang, Zihan Pan, Molly, Xu, Khuzaima Daudjee, Sihang Liu

The occurrence of bubbles in pipeline parallelism is an inherent limitation
that can account for more than 40% of the large language model (LLM) training
time and is one of the main reasons for the underutilization of GPU resources
in LLM training. Harvesting these bubbles for GPU side tasks can increase
resource utilization and reduce training costs but comes with challenges.
First, because bubbles are discontinuous with various shapes, programming side
tasks becomes difficult while requiring excessive engineering effort. Second, a
side task can compete with pipeline training for GPU resources and incur
significant overhead. To address these challenges, we propose FreeRide, a
system designed to harvest bubbles in pipeline parallelism for side tasks.
FreeRide provides programmers with interfaces to implement side tasks easily,
manages bubbles and side tasks during pipeline training, and controls access to
GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide
achieves 7.8% average cost savings with a negligible overhead of about 1% in
training LLMs while serving model training, graph analytics, and image
processing side tasks.

ÊëòË¶ÅÔºöÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁôºÁîüÊ∞£Ê≥°ÊòØ‰∏ÄÂÄãÂõ∫ÊúâÈôêÂà∂ÔºåÂèØËÉΩ‰ΩîÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë®ìÁ∑¥ÊôÇÈñìÁöÑ 40% ‰ª•‰∏äÔºå‰∏¶‰∏îÊòØ LLM Ë®ìÁ∑¥‰∏≠ GPU Ë≥áÊ∫êÂà©Áî®‰∏çË∂≥ÁöÑ‰∏ªË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇÊî∂ÈõÜÈÄô‰∫õÊ∞£Ê≥°‰ª•ÈÄ≤Ë°å GPU ÂÅ¥Èù¢‰ªªÂãôÂèØ‰ª•ÊèêÈ´òË≥áÊ∫êÂà©Áî®Áéá‰∏¶Èôç‰ΩéË®ìÁ∑¥ÊàêÊú¨Ôºå‰ΩÜÊúÉÂ∏∂‰æÜÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÁî±ÊñºÊ∞£Ê≥°ÊòØ‰∏çÈÄ£Á∫åÁöÑ‰∏îÂΩ¢ÁãÄÂêÑÁï∞ÔºåÂõ†Ê≠§Á∑®ÂØ´Á®ãÂºèÂÅ¥Èù¢‰ªªÂãôËÆäÂæóÂõ∞Èõ£ÔºåÂêåÊôÇÈúÄË¶ÅÈÅéÂ§öÁöÑÂ∑•Á®ãÂ∑•‰Ωú„ÄÇÂÖ∂Ê¨°ÔºåÂÅ¥Èù¢‰ªªÂãôÂèØËÉΩÊúÉËàáÁÆ°Á∑öË®ìÁ∑¥Á´∂Áà≠ GPU Ë≥áÊ∫êÔºå‰∏¶ÈÄ†ÊàêÈ°ØËëóÁöÑÈñãÈä∑„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü FreeRideÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®Êî∂ÈõÜÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁöÑÊ∞£Ê≥°‰ª•ÈÄ≤Ë°åÂÅ¥Èù¢‰ªªÂãôÁöÑÁ≥ªÁµ±„ÄÇFreeRide ÁÇ∫Á®ãÂºèË®≠Ë®àÂ∏´Êèê‰æõ‰∫ÜËºïÈ¨ÜÂØ¶‰ΩúÂÅ¥Èù¢‰ªªÂãôÁöÑ‰ªãÈù¢ÔºåÂú®ÁÆ°Á∑öË®ìÁ∑¥ÊúüÈñìÁÆ°ÁêÜÊ∞£Ê≥°ÂíåÂÅ¥Èù¢‰ªªÂãôÔºå‰∏¶ÊéßÂà∂ÂÅ¥Èù¢‰ªªÂãôÂ∞ç GPU Ë≥áÊ∫êÁöÑÂ≠òÂèñ‰ª•Ê∏õÂ∞ëÈñãÈä∑„ÄÇÊàëÂÄëË≠âÊòé FreeRide Âú®Ë®ìÁ∑¥ LLM ÊôÇÂèØÁØÄÁúÅ 7.8% ÁöÑÂπ≥ÂùáÊàêÊú¨ÔºåÂêåÊôÇÂú®Âü∑Ë°åÊ®°ÂûãË®ìÁ∑¥„ÄÅÂúñÂΩ¢ÂàÜÊûêÂíåÂΩ±ÂÉèËôïÁêÜÂÅ¥Èù¢‰ªªÂãôÊôÇÔºåÈñãÈä∑ÂèØÂøΩÁï•‰∏çË®àÔºåÁ¥ÑÁÇ∫ 1%„ÄÇ

##### **Generative Hierarchical Materials Search**
2409.06762v1 by Sherry Yang, Simon Batzner, Ruiqi Gao, Muratahan Aykol, Alexander L. Gaunt, Brendan McMorrow, Danilo J. Rezende, Dale Schuurmans, Igor Mordatch, Ekin D. Cubuk

Generative models trained at scale can now produce text, video, and more
recently, scientific data such as crystal structures. In applications of
generative approaches to materials science, and in particular to crystal
structures, the guidance from the domain expert in the form of high-level
instructions can be essential for an automated system to output candidate
crystals that are viable for downstream research. In this work, we formulate
end-to-end language-to-structure generation as a multi-objective optimization
problem, and propose Generative Hierarchical Materials Search (GenMS) for
controllable generation of crystal structures. GenMS consists of (1) a language
model that takes high-level natural language as input and generates
intermediate textual information about a crystal (e.g., chemical formulae), and
(2) a diffusion model that takes intermediate information as input and
generates low-level continuous value crystal structures. GenMS additionally
uses a graph neural network to predict properties (e.g., formation energy) from
the generated crystal structures. During inference, GenMS leverages all three
components to conduct a forward tree search over the space of possible
structures. Experiments show that GenMS outperforms other alternatives of
directly using language models to generate structures both in satisfying user
request and in generating low-energy structures. We confirm that GenMS is able
to generate common crystal structures such as double perovskites, or spinels,
solely from natural language input, and hence can form the foundation for more
complex structure generation in near future.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°Ë®ìÁ∑¥ÁöÑÁîüÊàêÊ®°ÂûãÁèæÂú®ÂèØ‰ª•Áî¢ÁîüÊñáÂ≠ó„ÄÅÂΩ±ÁâáÔºå‰ª•ÂèäÊúÄËøëÁöÑÁßëÂ≠∏Ë≥áÊñôÔºå‰æãÂ¶ÇÊô∂È´îÁµêÊßã„ÄÇÂú®ÁîüÊàêÊñπÊ≥ïÊáâÁî®ÊñºÊùêÊñôÁßëÂ≠∏ÔºåÂ∞§ÂÖ∂ÊòØÊô∂È´îÁµêÊßãÊôÇÔºåÈ†òÂüüÂ∞àÂÆ∂ÁöÑÊåáÂ∞éÔºå‰ª•È´òÈöéÊåá‰ª§ÁöÑÂΩ¢ÂºèÔºåÂ∞çÊñºËá™ÂãïÂåñÁ≥ªÁµ±Ëº∏Âá∫ÂèØË°åÊñº‰∏ãÊ∏∏Á†îÁ©∂ÁöÑÂÄôÈÅ∏Êô∂È´îËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞áÁ´ØÂ∞çÁ´ØË™ûË®ÄÂà∞ÁµêÊßãÁîüÊàêÂà∂ÂÆöÁÇ∫Â§öÁõÆÊ®ôÊúÄ‰Ω≥ÂåñÂïèÈ°åÔºå‰∏¶ÊèêÂá∫ÁîüÊàêÂàÜÂ±§ÊùêÊñôÊêúÂ∞ã (GenMS) ‰ª•ÊéßÂà∂Êô∂È´îÁµêÊßãÁöÑÁîüÊàê„ÄÇGenMS ÂåÖÂê´ (1) ‰∏ÄÂÄãË™ûË®ÄÊ®°ÂûãÔºåÂÆÉÂ∞áÈ´òÈöéËá™ÁÑ∂Ë™ûË®Ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàêÊúâÈóúÊô∂È´îÁöÑ‰∏≠ÈñìÊñáÂ≠óË≥áË®äÔºà‰æãÂ¶ÇÂåñÂ≠∏ÂÖ¨ÂºèÔºâÔºå‰ª•Âèä (2) ‰∏ÄÂÄãÊì¥Êï£Ê®°ÂûãÔºåÂÆÉÂ∞á‰∏≠ÈñìË≥áË®ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàê‰ΩéÈöéÈÄ£Á∫åÂÄºÊô∂È´îÁµêÊßã„ÄÇGenMS Ê≠§Â§ñ‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂæûÁîüÊàêÁöÑÊô∂È´îÁµêÊßãÈ†êÊ∏¨Â±¨ÊÄßÔºà‰æãÂ¶ÇÂΩ¢ÊàêËÉΩÔºâ„ÄÇÂú®Êé®ÁêÜÊúüÈñìÔºåGenMS Âà©Áî®ÊâÄÊúâ‰∏âÂÄãÁµÑ‰ª∂Â∞çÂèØËÉΩÁöÑÁµêÊßãÁ©∫ÈñìÈÄ≤Ë°åÂâçÂêëÊ®πÁãÄÊêúÂ∞ã„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåGenMS ÂÑ™ÊñºÁõ¥Êé•‰ΩøÁî®Ë™ûË®ÄÊ®°Âûã‰æÜÁîüÊàêÁµêÊßãÁöÑÂÖ∂‰ªñÊõø‰ª£ÊñπÊ°àÔºåÁÑ°Ë´ñÊòØÂú®ÊªøË∂≥‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊàñÁîüÊàê‰ΩéËÉΩÁµêÊßãÊñπÈù¢„ÄÇÊàëÂÄëÁ¢∫Ë™ç GenMS ËÉΩÂ§†ÂÉÖÂæûËá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•ÁîüÊàêÂ∏∏Ë¶ãÁöÑÊô∂È´îÁµêÊßãÔºå‰æãÂ¶ÇÈõôÈà£Èà¶Á§¶ÊàñÂ∞ñÊô∂Áü≥ÔºåÂõ†Ê≠§ÂèØ‰ª•Âú®‰∏ç‰πÖÁöÑÂ∞á‰æÜÂΩ¢ÊàêÊõ¥Ë§áÈõúÁµêÊßãÁîüÊàêÁöÑÂü∫Á§é„ÄÇ</paragraph>

##### **Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**
2409.06433v1 by Gollam Rabby, S√∂ren Auer, Jennifer D'Souza, Allard Oelen

The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.

ÊëòË¶ÅÔºö<paragraph>ÊØèÂπ¥Ë∂ÖÈÅé 250 Ëê¨ÁØáÁöÑÂ≠∏Ë°ìÊñáÁ´†ÁôºË°®Êï∏ÈáèÊåÅÁ∫åÂ¢ûÂä†ÔºåÂ∞çÁ†îÁ©∂‰∫∫Âì°ËøΩËπ§ÁßëÂ≠∏ÈÄ≤Â±ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂ∞áÂ≠∏Ë°ìÊñáÁ´†ÁöÑË≤¢ÁçªÊï¥ÂêàÂà∞Êñ∞ÂûãÊÖãÁöÑË™çÁü•Áü•Ë≠òÂúñË≠ú (CKG) ‰∏≠ÔºåÂ∞áÊàêÁÇ∫Â≠òÂèñÂíåÁµÑÁπîÂ≠∏Ë°ìÁü•Ë≠òÁöÑÈóúÈçµË¶ÅÁ¥†ÔºåË∂ÖË∂äÊ®ôÈ°åÂíåÊëòË¶ÅÊèê‰æõÁöÑË¶ãËß£„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÊúâÊïàÂÇ≥ÈÅîÁµêÊßãÂåñÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂàÜÈ°ûÂ≠∏Ë°ìÊñáÁ´†Ôºå‰∏¶‰ª•ÁµêÊßãÂåñ‰∏îÂèØÊØîËºÉÁöÑÂΩ¢ÂºèÊèèËø∞ÂÖ∂Ë≤¢Áçª„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂Âú®ÁâπÂÆöÁ†îÁ©∂È†òÂüü‰∏≠Êé¢Á¥¢Ë™ûË®ÄÊ®°ÂûãÔºå‰ΩÜ LLM ÊçïÊçâÂà∞ÁöÑÂª£Ê≥õÈ†òÂüüÁÑ°ÈóúÁü•Ë≠òÔºåÁÇ∫Áî¢ÁîüÁµêÊßãÂåñÁöÑË≤¢ÁçªÊèèËø∞Êèê‰æõ‰∫ÜÂØ¶Ë≥™Ê©üÊúÉÔºå‰æãÂ¶Ç CKG„ÄÇÊ≠§Â§ñÔºåLLM ÈÄèÈÅéÊèêÁ§∫Â∑•Á®ãÊàñÂæÆË™øÊèê‰æõÂèØËá™Ë®ÇË∑ØÂæëÔºåÂæûËÄå‰øÉÈÄ≤Âà©Áî®‰ª•ÊïàÁéá„ÄÅÊàêÊú¨ÊïàÁõäÂíåÁí∞Â¢ÉËÄÉÈáèËÅûÂêçÁöÑËºÉÂ∞èÂûã LLM„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Âà©Áî® LLM Áü•Ë≠òÔºå‰∏¶ÈÄèÈÅé CKG ‰æÜÊ∫êÁöÑÈ†òÂüüÂ∞àÂÆ∂È©óË≠âÂ≠∏Ë°ìË≥áÊñô‰æÜË£úÂÖÖ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•ËûçÂêàÈ°ØËëóÊèêÂçá LLM ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®Â≠∏Ë°ìÊñáÁ´†ÂàÜÈ°ûÂíåË¨ÇË©ûÊé®Ëñ¶Á≠â‰ªªÂãô‰∏≠„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂåÖÊã¨‰ª• CKG Áü•Ë≠òÂæÆË™ø LLMÔºå‰∏¶ÈÄèÈÅéÊñ∞ÁöÑÊèêÁ§∫ÊäÄË°ìÊ≥®ÂÖ• CKG ÁöÑÁü•Ë≠òÔºåÈ°ØËëóÊèêÂçáÂ≠∏Ë°ìÁü•Ë≠òËêÉÂèñÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÂÅöÊ≥ïÊï¥ÂêàÂà∞ÈñãÊîæÁ†îÁ©∂Áü•Ë≠òÂúñË≠ú (ORKG) ‰∏≠ÔºåÂæûËÄåËÉΩÁ≤æÊ∫ñÂ≠òÂèñÂ∑≤ÁµÑÁπîÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÈÄôÂ∞çÊîøÁ≠ñÂà∂ÂÆöËÄÖ„ÄÅÁî¢Ê•≠ÂæûÊ•≠‰∫∫Âì°Âíå‰∏ÄËà¨Â§ßÁúæ‰πãÈñìÁöÑÈ†òÂüüÁÑ°ÈóúÂ≠∏Ë°ìÁü•Ë≠ò‰∫§ÊµÅÂíåÂÇ≥Êí≠Ëá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

##### **Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**
2409.06091v1 by Dongyue Li, Aneesh Sharma, Hongyang R. Zhang

Multitask learning is a widely used paradigm for training models on diverse
tasks, with applications ranging from graph neural networks to language model
fine-tuning. Since tasks may interfere with each other, a key notion for
modeling their relationships is task affinity. This includes pairwise task
affinity, computed among pairs of tasks, and higher-order affinity, computed
among subsets of tasks. Naively computing either of them requires repeatedly
training on data from various task combinations, which is computationally
intensive. We present a new algorithm Grad-TAG that can estimate task
affinities without this repeated training.
  The key idea of Grad-TAG is to train a "base" model for all tasks and then
use a linearization technique to estimate the loss of the model for a specific
task combination. The linearization works by computing a gradient-based
approximation of the loss, using low-dimensional projections of gradients as
features in a logistic regression to predict labels for the task combination.
We show that the linearized model can provably approximate the loss when the
gradient-based approximation is accurate, and also empirically verify that on
several large models. Then, given the estimated task affinity, we design a
semi-definite program for clustering similar tasks by maximizing the average
density of clusters.
  We evaluate Grad-TAG's performance across seven datasets, including
multi-label classification on graphs, and instruction fine-tuning of language
models. Our task affinity estimates are within 2.7% distance to the true
affinities while needing only 3% of FLOPs in full training. On our largest
graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates
within 5% distance to the true affinities, using only 112 GPU hours. Our
results show that Grad-TAG achieves excellent performance and runtime tradeoffs
compared to existing approaches.

ÊëòË¶ÅÔºöÂ§ö‰ªªÂãôÂ≠∏ÁøíÊòØ‰∏ÄÁ®ÆÂª£Ê≥õ‰ΩøÁî®ÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂú®‰∏çÂêåÁöÑ‰ªªÂãô‰∏äË®ìÁ∑¥Ê®°ÂûãÔºåÂÖ∂ÊáâÁî®ÁØÑÂúçÂæûÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂà∞Ë™ûË®ÄÊ®°ÂûãÂæÆË™ø„ÄÇÁî±Êñº‰ªªÂãôÂèØËÉΩÊúÉÁõ∏‰∫íÂπ≤ÊìæÔºåÂõ†Ê≠§Âª∫Ê®°ÂÆÉÂÄëÈóú‰øÇÁöÑ‰∏ÄÂÄãÈóúÈçµÊ¶ÇÂøµÊòØ‰ªªÂãôË¶™ÂíåÊÄß„ÄÇÈÄôÂåÖÊã¨ÊàêÂ∞ç‰ªªÂãôË¶™ÂíåÊÄßÔºåÂú®ÊàêÂ∞ç‰ªªÂãô‰πãÈñìË®àÁÆóÔºå‰ª•ÂèäÈ´òÈöéË¶™ÂíåÊÄßÔºåÂú®‰ªªÂãôÂ≠êÈõÜ‰πãÈñìË®àÁÆó„ÄÇÂ§©ÁúüÂú∞Ë®àÁÆóÂÖ∂‰∏≠‰ªª‰Ωï‰∏ÄÂÄãÈÉΩÈúÄË¶ÅÈáçË§áË®ìÁ∑¥‰æÜËá™ÂêÑÁ®Æ‰ªªÂãôÁµÑÂêàÁöÑË≥áÊñôÔºåÈÄôÂú®Ë®àÁÆó‰∏äÂæàÂØÜÈõÜ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊºîÁÆóÊ≥ï Grad-TAGÔºåÂÆÉÂèØ‰ª•Âú®Ê≤íÊúâÈáçË§áË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ã‰º∞Ë®à‰ªªÂãôË¶™ÂíåÊÄß„ÄÇ
Grad-TAG ÁöÑÈóúÈçµÊÄùÊÉ≥ÊòØÁÇ∫ÊâÄÊúâ‰ªªÂãôË®ìÁ∑¥‰∏ÄÂÄã„ÄåÂü∫Á§é„ÄçÊ®°ÂûãÔºåÁÑ∂Âæå‰ΩøÁî®Á∑öÊÄßÂåñÊäÄË°ì‰æÜ‰º∞Ë®àÊ®°ÂûãÂ∞çÁâπÂÆö‰ªªÂãôÁµÑÂêàÁöÑÊêçÂ§±„ÄÇÁ∑öÊÄßÂåñÈÄöÈÅéË®àÁÆóÊêçÂ§±ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄº‰æÜÂ∑•‰ΩúÔºå‰ΩøÁî®Ê¢ØÂ∫¶ÁöÑ‰ΩéÁ∂≠ÊäïÂΩ±‰ΩúÁÇ∫ÁâπÂæµÔºåÂú®ÈÇèËºØËø¥Ê≠∏‰∏≠È†êÊ∏¨‰ªªÂãôÁµÑÂêàÁöÑÊ®ôÁ±§„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÁï∂Âü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄºÊ∫ñÁ¢∫ÊôÇÔºåÁ∑öÊÄßÂåñÊ®°ÂûãÂèØ‰ª•Ë≠âÊòéÂú∞Ëøë‰ººÊêçÂ§±Ôºå‰∏¶‰∏îÂú®ÂπæÂÄãÂ§ßÂûãÊ®°Âûã‰∏äÁ∂ìÈ©óÈ©óË≠â‰∫ÜÈÄô‰∏ÄÈªû„ÄÇÁÑ∂ÂæåÔºåÁµ¶ÂÆö‰º∞Ë®àÁöÑ‰ªªÂãôË¶™ÂíåÊÄßÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂçäÂÆöÁ®ãÂºèÔºåÈÄöÈÅéÊúÄÂ§ßÂåñÂè¢ÈõÜÁöÑÂπ≥ÂùáÂØÜÂ∫¶‰æÜÂ∞çÈ°û‰ººÁöÑ‰ªªÂãôÈÄ≤Ë°åÂè¢ÈõÜ„ÄÇ
ÊàëÂÄëË©ï‰º∞‰∫Ü Grad-TAG Âú®‰∏ÉÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºåÂåÖÊã¨ÂúñÂΩ¢‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÔºå‰ª•ÂèäË™ûË®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÂæÆË™ø„ÄÇÊàëÂÄëÁöÑ‰ªªÂãôË¶™ÂíåÊÄß‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 2.7% ‰ª•ÂÖßÔºåÂêåÊôÇÂè™ÈúÄË¶Å 3% ÁöÑ FLOP ÈÄ≤Ë°åÂÆåÊï¥Ë®ìÁ∑¥„ÄÇÂú®ÊàëÂÄëÊúÄÂ§ßÁöÑÂúñÂΩ¢ÔºàÊúâ 2100 Ëê¨Ê¢ùÈÇäÂíå 500 ÂÄãÊ®ôÁ±§‰ªªÂãôÔºâ‰∏äÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ïÊèê‰æõÁöÑ‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 5% ‰ª•ÂÖßÔºåÂè™‰ΩøÁî® 112 ÂÄã GPU Â∞èÊôÇ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGrad-TAG Âú®ÊïàËÉΩÂíåÂü∑Ë°åÊôÇÈñìÊ¨äË°°ÊñπÈù¢ÂèñÂæó‰∫ÜÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇ

##### **OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**
2409.07497v1 by Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.

ÊëòË¶ÅÔºöÁü•Ë≠òË°®ÂæµËá™‰∫∫Â∑•Êô∫ÊÖßË™ïÁîü‰ª•‰æÜ‰∏ÄÁõ¥ÊòØÂÖ∂Ê†∏ÂøÉÁõÆÊ®ô„ÄÇ
Á¨¶ËôüÁü•Ë≠òÂúñË≠ú (KG) ÂíåÁ•ûÁ∂ìË™ûË®ÄÂ§ßÊ®°Âûã (LLM) ÈÉΩÂèØ‰ª•Ë°®ÂæµÁü•Ë≠ò„ÄÇKG Êèê‰æõÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÊòéÁ¢∫ÁöÑÁü•Ë≠òË°®ÂæµÔºå‰ΩÜÈù¢Ëá®ÂèØÊì¥ÂÖÖÊÄßÁöÑÂïèÈ°åÔºõËÄå LLM Êèê‰æõÂª£Ê≥õÁöÑÁü•Ë≠òÊ∂µËìãÁØÑÂúçÔºå‰ΩÜÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑË®ìÁ∑¥ÊàêÊú¨Ôºå‰∏¶‰∏îÂú®Á≤æÁ¢∫‰∏îÂèØÈù†ÁöÑÁü•Ë≠òÊìç‰ΩúÊñπÈù¢ÈÅáÂà∞Âõ∞Èõ£„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü OneEditÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°åÂçî‰ΩúÁü•Ë≠òÁ∑®ËºØÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÂéüÂûãÁ≥ªÁµ±ÔºåÂÆÉ‰øÉÈÄ≤‰∫Ü‰ΩøÁî® KG Âíå LLM ÈÄ≤Ë°åÊòìÊñº‰ΩøÁî®ÁöÑÁü•Ë≠òÁÆ°ÁêÜ„ÄÇOneEdit ÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö1) Ëß£Ë≠ØÂô®Áî®Êñº‰ΩøÁî®ËÄÖÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°å‰∫íÂãïÔºõ2) ÊéßÂà∂Âô®ÁÆ°ÁêÜ‰æÜËá™‰∏çÂêå‰ΩøÁî®ËÄÖÁöÑÁ∑®ËºØË´ãÊ±ÇÔºåÂà©Áî® KG ÂíåÂõûÊªæ‰æÜËôïÁêÜÁü•Ë≠òË°ùÁ™Å‰∏¶Èò≤Ê≠¢ÊúâÊØíÁöÑÁü•Ë≠òÊîªÊìäÔºõ3) Á∑®ËºØÂô®Âà©Áî®‰æÜËá™ÊéßÂà∂Âô®ÁöÑÁü•Ë≠ò‰æÜÁ∑®ËºØ KG Âíå LLM„ÄÇÊàëÂÄëÂ∞çÂÖ©ÂÄãÂÖ∑Êúâ KG ÁöÑÊñ∞Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåË≠âÊòé OneEdit ÂèØ‰ª•ÂØ¶ÁèæÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ

##### **SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**
2409.05556v1 by Alireza Ghafarollahi, Markus J. Buehler

A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ‰∏≠Ôºå‰∏ÄÂÄãÈóúÈçµÁöÑÊåëÊà∞ÊòØÂâµÈÄ†Âá∫ÊúâËÉΩÂäõÈÄèÈÅéÊé¢Á¥¢Êñ∞È†òÂüü„ÄÅË≠òÂà•Ë§áÈõúÊ®°ÂºèÔºå‰ª•ÂèäÂú®Â§ßÈáèÁöÑÁßëÂ≠∏Êï∏Êìö‰∏≠ÁôºÁèæÂâçÊâÄÊú™Ë¶ãÁöÑÈóúËÅØÔºå‰æÜËá™‰∏ªÊé®ÈÄ≤ÁßëÂ≠∏ÁêÜËß£ÁöÑÁ≥ªÁµ±„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SciAgentsÔºå‰∏ÄÁ®ÆÂà©Áî®‰∏âÂÄãÊ†∏ÂøÉÊ¶ÇÂøµÁöÑÊñπÊ≥ïÔºö(1) ‰ΩøÁî®Â§ßË¶èÊ®°ÁöÑÊú¨‰ΩìÁü•Ë≠òÂúñË≠ú‰æÜÊï¥ÁêÜÂíåÈÄ£Áµê‰∏çÂêåÁöÑÁßëÂ≠∏Ê¶ÇÂøµÔºå(2) ‰∏ÄÂ•óÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊï∏ÊìöÊ™¢Á¥¢Â∑•ÂÖ∑Ôºå‰ª•Âèä (3) ÂÖ∑ÊúâÂéü‰ΩçÂ≠∏ÁøíËÉΩÂäõÁöÑÂ§ö‰ª£ÁêÜÁ≥ªÁµ±„ÄÇÊáâÁî®ÊñºÁîüÁâ©ÂïüÁôºÊùêÊñôÔºåSciAgents Êè≠Á§∫‰∫Ü‰ª•ÂâçË¢´Ë™çÁÇ∫ÁÑ°ÈóúÁöÑÈö±ËóèË∑®Â≠∏ÁßëÈóú‰øÇÔºåÈÅîÂà∞‰∫ÜË∂ÖË∂äÂÇ≥Áµ±‰∫∫ÁÇ∫Á†îÁ©∂ÊñπÊ≥ïÁöÑË¶èÊ®°„ÄÅÁ≤æÁ¢∫Â∫¶ÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇË©≤Ê°ÜÊû∂Ëá™‰∏ªÁîüÊàêÂíåÂÑ™ÂåñÁ†îÁ©∂ÂÅáË®≠ÔºåÈó°ÊòéÂü∫Á§éÊ©üÂà∂„ÄÅË®≠Ë®àÂéüÁêÜÂíåÊÑèÂ§ñÁöÑÊùêÊñôÁâπÊÄß„ÄÇÈÄèÈÅé‰ª•Ê®°ÁµÑÂåñÊñπÂºèÊï¥ÂêàÈÄô‰∫õËÉΩÂäõÔºåÊô∫ËÉΩÁ≥ªÁµ±Áî¢ÁîüÊùêÊñôÁôºÁèæ„ÄÅÊâπÂà§ÂíåÊîπÈÄ≤ÁèæÊúâÂÅáË®≠„ÄÅÊ™¢Á¥¢ÈóúÊñºÁèæÊúâÁ†îÁ©∂ÁöÑÊúÄÊñ∞Êï∏ÊìöÔºå‰∏¶Âº∑Ë™øÂÆÉÂÄëÁöÑÂÑ™ÈªûÂíåÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÁµêÂêàÁîüÊàêÂºè AI„ÄÅÊú¨‰ΩìË°®Á§∫ÂíåÂ§ö‰ª£ÁêÜÂª∫Ê®°ÁöÑÂèØÊì¥ÂÖÖËÉΩÂäõÔºåÂà©Áî®È°û‰ººÊñºÁîüÁâ©Á≥ªÁµ±ÁöÑ„ÄåÊô∫ÊÖßÁæ§È´î„Äç„ÄÇÈÄôÁÇ∫ÊùêÊñôÁôºÁèæÊèê‰æõ‰∫ÜÊñ∞ÈÄîÂæëÔºå‰∏¶ÈÄèÈÅéËß£ÈéñÂ§ßËá™ÁÑ∂ÁöÑË®≠Ë®àÂéüÁêÜ‰æÜÂä†ÈÄüÂÖàÈÄ≤ÊùêÊñôÁöÑÈñãÁôº„ÄÇ

##### **Assessing SPARQL capabilities of Large Language Models**
2409.05925v1 by Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊï¥ÂêàÁÇ∫Áü•Ë≠òÈ©ÖÂãïÊáâÁî®Á®ãÂºèÊèê‰æõ‰∫ÜÈ°ØËëóÁöÑÁ∂úÊïàÊΩõÂäõ„ÄÇ‰∏ÄÁ®ÆÂèØËÉΩÁöÑÊï¥ÂêàÊòØËß£ÈáãÂíåÁî¢ÁîüÂΩ¢ÂºèÂåñË™ûË®ÄÔºå‰æãÂ¶ÇË™ûÁæ©Á∂≤Ë∑Ø‰∏≠‰ΩøÁî®ÁöÑË™ûË®ÄÔºåËÄå SPARQL ÊòØÂ≠òÂèñ KG ÁöÑÊ†∏ÂøÉÊäÄË°ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºË°°Èáè LLM ÈñãÁÆ±Âç≥Áî®ÁöÑËÉΩÂäõÔºå‰ª•‰ΩøÁî® SPARQLÔºåÊõ¥ÂÖ∑È´îÂú∞Ë™™Ôºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢ÊáâÁî®ÈáèÂåñÊñπÊ≥ï„ÄÇ
  ÊàëÂÄëÂú® LLM-KG-Bench Êû∂Êßã‰∏≠ÂØ¶‰Ωú‰∫ÜÂêÑÁ®ÆÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºå‰ª•Ëá™ÂãïÂü∑Ë°åÂíåË©ï‰º∞Â§öÂÄã LLM„ÄÇÈÄô‰∫õ‰ªªÂãôË©ï‰º∞‰∫ÜË™ûÊ≥ï„ÄÅË™ûÁæ©ËÆÄÂèñ„ÄÅË™ûÁæ©Âª∫Á´ãÂíåÁü•Ë≠òÂúñË≠úÊèêÁ§∫ÂåÖÂê´ÁöÑËßíËâ≤Á≠âÈù¢ÂêëÁöÑËÉΩÂäõ„ÄÇ
  Êúâ‰∫ÜÈÄô‰∫õÊñ∞ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºåÊàëÂÄëË©ï‰º∞‰∫Ü GPT„ÄÅGemini Âíå Claude Ê®°ÂûãÁöÑÈÅ∏È†Ö„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢Â∞çÊñº LLM ‰æÜË™™‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏¶‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÂÖ∑È´îÁöÑ LLM ‰ª•Âèä‰ªªÂãôÁöÑË§áÈõúÊÄß„ÄÇÂÑòÁÆ°‰øÆÂæ©Âü∫Êú¨ÁöÑË™ûÊ≥ïÈåØË™§‰ºº‰πéÂ∞çÁõÆÂâçË©ï‰º∞ÁöÑÊúÄ‰Ω≥ LLM ‰æÜË™™‰∏çÊàêÂïèÈ°åÔºå‰ΩÜÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÂª∫Á´ãË™ûÁæ©Ê≠£Á¢∫ÁöÑ SPARQL SELECT Êü•Ë©¢ÂæàÂõ∞Èõ£„ÄÇ

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âº∑Â§ßÁöÑÂäüËÉΩÔºåÈÄ≤Ë°åÊïò‰∫ãÁîüÊàê„ÄÅÈÇèËºØÊé®ÁêÜÂíåÂ∏∏Ë≠òÁü•Ë≠òÊï¥ÂêàÔºåÊú¨Á†îÁ©∂Ê∑±ÂÖ•Êé¢Ë®éÂà©Áî® LLM ‰æÜÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàê (R2Gen)„ÄÇÂÑòÁÆ° LLM ÊìÅÊúâË±êÂØåÁöÑÁü•Ë≠òÔºå‰ΩÜË¶ÅÊúâÊïàËß∏ÁôºÈÄô‰∫õÂ§ßÂûãÊ®°Âûã‰∏≠ËàáÁâπÂÆö‰ªªÂãôÔºàÂ¶Ç R2GenÔºâÁõ∏ÈóúÁöÑÁü•Ë≠òÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü KARGENÔºå‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÁü•Ë≠òÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàêÊ°ÜÊû∂„ÄÇÂà©Áî®ÂáçÁµêÁöÑ LLM ‰æÜÁîüÊàêÂ†±ÂëäÔºåË©≤Ê°ÜÊû∂Êï¥Âêà‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂúñË≠úÔºå‰ª•Ëß£Èéñ LLM ‰∏≠ËàáËÉ∏ÈÉ®ÁñæÁóÖÁõ∏ÈóúÁöÑÁü•Ë≠òÔºå‰ª•Â¢ûÂº∑ÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊïàÁî®„ÄÇÈÄôÊòØÈÄèÈÅéÂà©Áî®Áü•Ë≠òÂúñË≠ú‰ª•Ë®≠Ë®àÁöÑÊñπÂºèÊèêÂèñËàáÁñæÁóÖÁõ∏ÈóúÁöÑÁâπÂæµ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÁî±ÊñºÊîæÂ∞ÑÂ†±ÂëäÂåÖÂê´Ê≠£Â∏∏ÂíåÁñæÁóÖÁõ∏ÈóúÁöÑÁôºÁèæÔºåÂõ†Ê≠§ÊèêÂèñÁöÑÂúñÂΩ¢Â¢ûÂº∑ÁñæÁóÖÁõ∏ÈóúÁâπÂæµËàáÂçÄÂüüÂΩ±ÂÉèÁâπÂæµÊï¥ÂêàÔºåÂÖºÈ°ßÂÖ©ÂÄãÊñπÈù¢„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫ÜÂÖ©Á®ÆËûçÂêàÊñπÊ≥ïÔºå‰ª•Ëá™ÂãïÂÑ™ÂÖàÊéíÂ∫èÂíåÈÅ∏ÊìáÊúÄÁõ∏ÈóúÁöÑÁâπÂæµ„ÄÇËûçÂêàÁöÑÁâπÂæµÁî± LLM ‰ΩøÁî®Ôºå‰ª•ÁîüÊàêÂ∞çÁñæÁóÖÊõ¥ÊïèÊÑü‰∏îÂìÅË≥™Êõ¥È´òÁöÑÂ†±Âëä„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® MIMIC-CXR Âíå IU-Xray Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ</paragraph>

##### **Action is the primary key: a categorical framework for episode description and logical reasoning**
2409.04793v1 by Yoshiki Fukada

This research presents a computational framework for describing and
recognizing episodes and for logical reasoning. This framework, named
cognitive-logs, consists of a set of relational and graph databases.
Cognitive-logs record knowledge, particularly in episodes that consist of
"actions" represented by verbs in natural languages and "participants" who
perform the actions. These objects are connected by arrows (morphisms) that
link each action to its participant and link cause to effect. Operations based
on category theory enable comparisons between episodes and deductive
inferences, including abstractions of stories. One of the goals of this study
is to develop a database-driven artificial intelligence. This artificial
intelligence thinks like a human but possesses the accuracy and rigour of a
machine. The vast capacities of databases (up to petabyte scales in current
technologies) enable the artificial intelligence to store a greater volume of
knowledge than neural-network based artificial intelligences. Cognitive-logs
serve as a model of human cognition and designed with references to cognitive
linguistics. Cognitive-logs also have the potential to model various human mind
activities.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãË®àÁÆóÊ°ÜÊû∂ÔºåÁî®‰æÜÊèèËø∞ÂíåËæ®Ë≠ò‰∫ã‰ª∂‰ª•ÂèäÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜ„ÄÇÈÄôÂÄãÊ°ÜÊû∂ÂêçÁÇ∫Ë™çÁü•Êó•Ë™åÔºåÂåÖÂê´‰∏ÄÁµÑÈóúËÅØÂºèÂíåÂúñÂΩ¢Ë≥áÊñôÂ∫´„ÄÇË™çÁü•Êó•Ë™åË®òÈåÑÁü•Ë≠òÔºåÁâπÂà•ÊòØÂåÖÂê´Áî±Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÂãïË©ûË°®Á§∫ÁöÑ„ÄåÂãï‰Ωú„ÄçÂíåÂü∑Ë°åÂãï‰ΩúÁöÑ„ÄåÂèÉËàáËÄÖ„ÄçÁöÑ‰∫ã‰ª∂„ÄÇÈÄô‰∫õÁâ©‰ª∂Áî±ÁÆ≠È†≠ÔºàÊÖãÂ∞ÑÔºâÈÄ£Êé•ÔºåÂ∞áÊØèÂÄãÂãï‰ΩúÈÄ£ÁµêÂà∞ÂÖ∂ÂèÉËàáËÄÖÔºå‰∏¶Â∞áÂéüÂõ†ÈÄ£ÁµêÂà∞ÁµêÊûú„ÄÇÂü∫ÊñºÁØÑÁñáË´ñÁöÑÈÅãÁÆóÂèØÊØîËºÉ‰∫ã‰ª∂ÂíåÊºîÁππÊé®Ë´ñÔºåÂåÖÊã¨ÊïÖ‰∫ãÁöÑÊäΩË±°Âåñ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ô‰πã‰∏ÄÊòØÈñãÁôº‰∏ÄÂÄãË≥áÊñôÂ∫´È©ÖÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÈÄôÂÄã‰∫∫Â∑•Êô∫ÊÖßÊÄùËÄÉÊñπÂºèÂÉè‰∫∫È°ûÔºå‰ΩÜÊìÅÊúâÊ©üÂô®Ëà¨ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂö¥Ë¨πÊÄß„ÄÇË≥áÊñôÂ∫´ÁöÑÈæêÂ§ßÂÆπÈáèÔºàÂú®ÁõÆÂâçÁöÑÊäÄË°ì‰∏≠ÂèØÈÅîÁöÆ‰ΩçÂÖÉÁµÑÁ≠âÁ¥öÔºâ‰Ωø‰∫∫Â∑•Êô∫ÊÖßËÉΩÂ§†ÂÑ≤Â≠òÊØîÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊõ¥Â§ßÁöÑÁü•Ë≠òÈáè„ÄÇË™çÁü•Êó•Ë™å‰ΩúÁÇ∫‰∫∫È°ûË™çÁü•ÁöÑÊ®°ÂûãÔºå‰∏¶ÂèÉËÄÉË™çÁü•Ë™ûË®ÄÂ≠∏ÈÄ≤Ë°åË®≠Ë®à„ÄÇË™çÁü•Êó•Ë™å‰πüÊúâÊΩõÂäõÊ®°Êì¨ÂêÑÁ®Æ‰∫∫È°ûÂøÉÊô∫Ê¥ªÂãï„ÄÇ

##### **Accelerating Training with Neuron Interaction and Nowcasting Networks**
2409.04434v1 by Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien

Neural network training can be accelerated when a learnable update rule is
used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable
update rules can be costly and unstable to train and use. A simpler recently
proposed approach to accelerate training is to use Adam for most of the
optimization steps and periodically, only every few steps, nowcast (predict
future) parameters. We improve this approach by Neuron interaction and
Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural
networks to more accurately nowcast parameters by learning in a supervised way
from a set of training trajectories over multiple tasks. We show that in some
networks, such as Transformers, neuron connectivity is non-trivial. By
accurately modeling neuron connectivity, we allow NiNo to accelerate Adam
training by up to 50\% in vision and language tasks.

ÊëòË¶ÅÔºöÁ•ûÁªèÁΩëÁªúËÆ≠ÁªÉÂèØ‰ª•Âä†ÈÄüÔºåÂΩì‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôË¢´Áî®Êù•‰ª£ÊõøÁªèÂÖ∏ÁöÑËá™ÈÄÇÂ∫î‰ºòÂåñÂô®Ôºà‰æãÂ¶Ç AdamÔºâ„ÄÇÁÑ∂ËÄåÔºåÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôÂèØËÉΩÊòØÊòÇË¥µ‰∏î‰∏çÁ®≥ÂÆöÁöÑÔºåÈúÄË¶ÅËÆ≠ÁªÉÂíå‰ΩøÁî®„ÄÇ‰∏ÄÁßçÊúÄËøëÊèêÂá∫ÁöÑÊõ¥ÁÆÄÂçïÁöÑÂä†ÈÄüËÆ≠ÁªÉÁöÑÊñπÊ≥ïÊòØÔºåÂØπ‰∫éÂ§ßÂ§öÊï∞ÁöÑ‰ºòÂåñÊ≠•È™§‰ΩøÁî® AdamÔºåÂπ∂‰∏îÂÆöÊúüÂú∞Ôºå‰ªÖÊØèÈöîÂá†Ê≠•ÔºåÈ¢ÑÊµãÔºàÈ¢ÑÊµãÊú™Êù•ÔºâÂèÇÊï∞„ÄÇÊàë‰ª¨ÈÄöËøáÁ•ûÁªèÂÖÉ‰∫§‰∫íÂíåÈ¢ÑÊµãÔºàNiNoÔºâÁΩëÁªúÊù•ÊîπËøõËøôÁßçÊñπÊ≥ï„ÄÇNiNo Âà©Áî®Á•ûÁªèÂÖÉËøûÊé•ÂíåÂõæÁ•ûÁªèÁΩëÁªúÔºåÈÄöËøá‰ªéÂ§ö‰∏™‰ªªÂä°‰∏≠ÁöÑ‰∏ÄÁªÑËÆ≠ÁªÉËΩ®Ëøπ‰∏≠‰ª•ÁõëÁù£ÊñπÂºèÂ≠¶‰π†ÔºåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµãÂèÇÊï∞„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÂú®‰∏Ä‰∫õÁΩëÁªú‰∏≠Ôºå‰æãÂ¶Ç TransformerÔºåÁ•ûÁªèÂÖÉËøûÊé•ÊòØÈùûÂπ≥Âá°ÁöÑ„ÄÇÈÄöËøáÂáÜÁ°ÆÂú∞Âª∫Ê®°Á•ûÁªèÂÖÉËøûÊé•ÔºåÊàë‰ª¨ÂÖÅËÆ∏ NiNo Â∞Ü Adam ËÆ≠ÁªÉÂä†ÈÄüÈ´òËææ 50%ÔºåÁî®‰∫éËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°„ÄÇ

##### **Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**
2409.04286v1 by Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel

Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÂâçÂÖ¨ÈñãÂèØÁî®ÁöÑÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôËíêÈõÜÁº∫‰πèÂ§öÂÖÉÊÄß„ÄÅÂª£Ê≥õË®ªËß£Âíå‰ΩøÁî®ËÄÖÂèäÂÖ∂Êñá‰ª∂ËÉåÊôØË≥áË®ä„ÄÇÈÄô‰∫õÂïèÈ°åÈòªÁ§ô‰∫ÜÂÆ¢ËßÄ‰∏îÂèØÊØîËºÉÁöÑË≥áÊñôÈ©ÖÂãïË©ï‰º∞Ôºå‰ª•ÂèäÁü•Ë≠òÂ∑•‰ΩúÂçîÂä©Á≥ªÁµ±ÁöÑÊúÄ‰Ω≥Âåñ„ÄÇÁî±ÊñºÂú®ÁèæÂØ¶ÁîüÊ¥ª‰∏≠ËíêÈõÜÊ≠§È°ûË≥áÊñôÈúÄË¶ÅÂ§ßÈáèË≥áÊ∫êÔºåËÄå‰∏îÂøÖÈ†àÂØ©Êü•Ë≥áÊñôÔºåËíêÈõÜÊ≠§È°ûË≥áÊñôÁµÑÈ°ØÁÑ∂Âπæ‰πé‰∏çÂèØËÉΩ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèØË®≠ÂÆöÁöÑÂ§öÈáç‰ª£ÁêÜÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôÁµÑÁî¢ÁîüÂô®„ÄÇÊ≠§Á≥ªÁµ±Ê®°Êì¨‰ª£ÁêÜ‰πãÈñìÁöÑÂçî‰ΩúÁü•Ë≠òÂ∑•‰ΩúÔºåÁî¢ÁîüÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁöÑÊñá‰ª∂ÂíåÈö®ÈôÑÁöÑË≥áÊñôËøΩËπ§„ÄÇÊ≠§Â§ñÔºåÁî¢ÁîüÂô®ÊúÉÊì∑ÂèñÊâÄÊúâËÉåÊôØË≥áË®äÔºåÂú®ÁµÑÊÖã‰∏≠Êèê‰æõÊàñÂú®Ê®°Êì¨ÈÅéÁ®ã‰∏≠Âª∫Á´ãÔºå‰∏¶Â∞áÂÖ∂ÂÑ≤Â≠òÂú®Áü•Ë≠òÂúñË≠ú‰∏≠„ÄÇÊúÄÂæåÔºåÁî¢ÁîüÁöÑË≥áÊñôÁµÑÂèØ‰ª•‰ΩøÁî®ÂíåÂàÜ‰∫´ÔºåÁÑ°È†àÊìîÂøÉÈö±ÁßÅÊàñÊ©üÂØÜÊÄß„ÄÇ
Êú¨Êñá‰ªãÁ¥πÊàëÂÄëÊñπÊ≥ïÁöÑË®≠Ë®àÂíåÈ°òÊôØÔºå‰∏¶Â∞àÊ≥®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁúüÂØ¶ÁöÑÁü•Ë≠òÂ∑•‰ΩúÊñá‰ª∂„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ê∂âÂèä‰∫∫È°ûË©ïÂàÜÂì°Ôºå‰ªñÂÄëË©ï‰º∞‰∫Ü 53% ÁöÑÁî¢ÁîüÊñá‰ª∂Âíå 74% ÁöÑÁúüÂØ¶Êñá‰ª∂ÁÇ∫ÁúüÂØ¶ÔºåÈÄôË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûêÂèÉËàáËÄÖË©ïË´ñ‰∏≠ÊèêÂà∞ÁöÑÁúüÂØ¶ÊÄßÊ®ôÊ∫ñÔºå‰∏¶Ë©≥Á¥∞Ë™™ÊòéÂ∑≤Ë≠òÂà•Â∏∏Ë¶ãÂïèÈ°åÁöÑÊΩõÂú®ÊîπÂñÑÊñπÊ≥ï„ÄÇ</paragraph>

##### **GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**
2409.04183v1 by Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang

Programming languages possess rich semantic information such as data flow
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with four different baseline LLMs ranging in size from 350M to 8B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3.

ÊëòË¶ÅÔºöÁ®ãÂºèË™ûË®ÄÊìÅÊúâË±êÂØåÁöÑË™ûÊÑèË≥áË®äÔºå‰æãÂ¶ÇÁî±ÂúñÂΩ¢Ë°®Á§∫‰∏îÁÑ°Ê≥ïÂæûÂéüÂßãÁ¢ºË°®Èù¢ÂΩ¢ÂºèÂèñÂæóÁöÑË≥áÊñôÊµÅÁ®ã„ÄÇÊúÄËøëÁöÑÁ®ãÂºèÁ¢ºË™ûË®ÄÊ®°ÂûãÂ∑≤Êì¥ÂÖÖËá≥Êï∏ÂçÅÂÑÑÂÄãÂèÉÊï∏Ôºå‰ΩÜÊ®°ÂûãÂéüÂßãÁ¢ºÂÉÖ‰ΩúÁÇ∫ÊñáÂ≠óÁ¨¶ËôüÔºåËÄåÂøΩÁï•‰ªª‰ΩïÂÖ∂‰ªñÁµêÊßãË≥áË®ä„ÄÇÂèç‰πãÔºåÁ∑®Á¢ºÁ®ãÂºèÁ¢ºÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÊúÉ‰øÆÊîπ Transformer Êû∂ÊßãÔºåÈôêÂà∂ÂÖ∂Ë¶èÊ®°ÂíåËàáÈ†êÂÖàË®ìÁ∑¥ÁöÑ LLM ÁöÑÁõ∏ÂÆπÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé°Áî® GALLaÔºàÂúñÂΩ¢Â∞çÈΩäÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºâÊì∑ÂèñÂÖ©ÂÖ®ÂÖ∂ÁæéÁöÑÂÑ™Èªû„ÄÇGALLa Âà©Áî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåË∑®Ê®°ÊÖãÂ∞çÈΩäÊäÄË°ìÔºåÂú®ÂæÆË™øÊúüÈñìÂ∞áÁ®ãÂºèÁ¢ºÁöÑÁµêÊßãË≥áË®äÊ≥®ÂÖ• LLM ‰ΩúÁÇ∫ËºîÂä©‰ªªÂãô„ÄÇÊ≠§Êû∂ÊßãÂêåÊôÇ‰∏ç‰æùË≥¥Ê®°ÂûãÂíå‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ÊáâÁî®Êñº‰ªª‰ΩïÁ®ãÂºèÁ¢º LLM ÁöÑ‰ªª‰ΩïÁ®ãÂºèÁ¢º‰∏ãÊ∏∏‰ªªÂãôÔºå‰∏¶‰∏îÂÉÖÂú®Ë®ìÁ∑¥ÊúüÈñìÂæûËàáÂæÆË™øË≥áÊñôÁÑ°ÈóúÁöÑË™ûÊñôÂ∫´ÂèñÂæóÁµêÊßãÂúñÂΩ¢Ë≥áÊñôÔºåÂêåÊôÇÂú®Êé®Ë´ñÊúüÈñì‰∏çÁî¢ÁîüÊØîÂü∫Ê∫ñ LLM Êõ¥È´òÁöÑÊàêÊú¨„ÄÇÂú®‰∫îÂÄãÁ®ãÂºèÁ¢º‰ªªÂãô‰∏≠ÈÄ≤Ë°åÂØ¶È©óÔºå‰ΩøÁî®ÂõõÂÄã‰∏çÂêåÁöÑÂü∫Ê∫ñ LLMÔºåË¶èÊ®°Âæû 350M Âà∞ 8BÔºåÈ©óË≠â GALLa ÁöÑÊúâÊïàÊÄßÔºåË≠âÊòéÂç≥‰ΩøÂ∞çÊñº LLaMA3 Á≠âÂº∑Â§ßÊ®°ÂûãÔºå‰πüËÉΩÊåÅÁ∫åÂÑ™ÊñºÂü∫Ê∫ñ„ÄÇ

##### **Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**
2409.04181v1 by Larissa Pusch, Tim O. F. Conrad

Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÊï∏‰ΩçË≥áË®äÁ≥ªÁµ±Ôºà‰æãÂ¶ÇË≥áÊñôÂ∫´Ôºâ‰∫íÂãïÁöÑÊñπÂºèÔºåËÆìÈÄô‰∫õÁ≥ªÁµ±ËÆäÂæóÊõ¥ÊòìÊñºÂ≠òÂèñ„ÄÇÁÑ∂ËÄåÔºåÊåëÊà∞‰ªçÁÑ∂Â≠òÂú®ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∫ñÁ¢∫ÊÄßËá≥ÈóúÈáçË¶ÅÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰æãÂ¶ÇÂú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüü„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØÂπªË¶∫ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê®°ÂûãÊúÉÁî¢ÁîüÊú™Á∂ìÂü∫Á§éË≥áÊñôÈ©óË≠âÁöÑË≥áË®äÔºåÂèØËÉΩÂ∞éËá¥Âç±Èö™ÁöÑÈåØË™§Ë≥áË®ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄèÈÅéÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºå‰ª•ÊèêÈ´òÁîüÁâ©ÈÜ´Â≠∏ KG ‰∏≠ÂïèÁ≠îÁ≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂª∫Á´ãÂú® LangChain Ê°ÜÊû∂‰∏äÔºåÁµêÂêà‰∫Ü‰∏ÄÂÄãÊü•Ë©¢Ê™¢Êü•Âô®ÔºåÂèØÁ¢∫‰øù LLM ÁîüÊàêÁöÑÊü•Ë©¢Âú®Ë™ûÊ≥ïÂíåË™ûÊÑè‰∏äÊúâÊïàÔºåÁÑ∂ÂæåÁî®ÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñË≥áË®äÔºåÂ§ßÂπÖÊ∏õÂ∞ëÂπªË¶∫Á≠âÈåØË™§„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãÊñ∞ÁöÑ 50 ÂÄãÁîüÁâ©ÈÜ´Â≠∏ÂïèÈ°åÂü∫Ê∫ñË≥áÊñôÈõÜË©ï‰º∞‰∫ÜÊï¥È´îÊïàËÉΩÔºåÊ∏¨Ë©¶‰∫ÜÂåÖÊã¨ GPT-4 Turbo Âíå llama3:70b Âú®ÂÖßÁöÑÂπæÂÄã LLM„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÈõñÁÑ∂ GPT-4 Turbo Âú®Áî¢ÁîüÊ∫ñÁ¢∫Êü•Ë©¢ÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºå‰ΩÜÂÉè llama3:70b ÈÄôÊ®£ÁöÑÈñãÊ∫êÊ®°ÂûãÂú®ÈÅ©Áï∂ÁöÑÊèêÁ§∫Â∑•Á®ã‰∏ãÈ°ØÁ§∫Âá∫ÂâçÊôØ„ÄÇÁÇ∫‰∫ÜËÆìÈÄôÁ®ÆÊñπÊ≥ïÊòìÊñº‰ΩøÁî®ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÁ∂≤Ë∑Ø‰ªãÈù¢ÔºåËÆì‰ΩøÁî®ËÄÖÂèØ‰ª•Ëº∏ÂÖ•Ëá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢„ÄÅÊ™¢Ë¶ñÁî¢ÁîüÂíåÊõ¥Ê≠£ÁöÑ Cypher Êü•Ë©¢Ôºå‰∏¶È©óË≠âÁµêÊûúË∑ØÂæëÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÈÄôÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜË≥áÊñôÂ∑ÆË∑ùÂíåÂπªË¶∫Á≠âÂ∏∏Ë¶ãÂïèÈ°åÔºåÁÇ∫ÂïèÁ≠îÁ≥ªÁµ±Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÈù†‰∏îÁõ¥ËßÄÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊú¨ÊñáÁµêÊûúÁî¢ÁîüÁöÑÂéüÂßãÁ¢ºÂíå‰ΩøÁî®ËÄÖ‰ªãÈù¢ÁöÑÂéüÂßãÁ¢ºÂèØ‰ª•Âú®ÊàëÂÄëÁöÑ Git ÂÑ≤Â≠òÂ∫´‰∏≠ÊâæÂà∞Ôºöhttps://git.zib.de/lpusch/cyphergenkg-gui

##### **Refining Wikidata Taxonomy using Large Language Models**
2409.04056v1 by Yiwen Peng, Thomas Bonald, Mehwish Alam

Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.

ÊëòË¶ÅÔºöÁî±ÊñºÂÖ∂Âçî‰ΩúÊÄßË≥™ÔºåWikidata Â∑≤Áü•ÂÖ∑ÊúâË§áÈõúÁöÑÂàÜÈ°ûÊ≥ïÔºå‰∏¶ÊúâÈáçË§áÁôºÁîüÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂØ¶‰æãÂíåÈ°ûÂà•‰πãÈñìÁöÑÊ≠ßÁæ©„ÄÅÊüê‰∫õÂàÜÈ°ûË∑ØÂæëÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄß„ÄÅÂæ™Áí∞ÁöÑÂ≠òÂú®Ôºå‰ª•ÂèäÈ°ûÂà•‰πãÈñìÁöÑÈ´òÂÜóÈ§ò„ÄÇÊâãÂãïÊ∏ÖÁêÜÊ≠§ÂàÜÈ°ûÊ≥ïÁöÑÂ∑•‰ΩúÊó¢ËÄóÊôÇÂèàÂÆπÊòìÂá∫ÁèæÈåØË™§Êàñ‰∏ªËßÄÂà§Êñ∑„ÄÇÊàëÂÄëÊèêÂá∫ WiKCÔºåÈÄôÊòØ Wikidata ÂàÜÈ°ûÊ≥ïÁöÑÊñ∞ÁâàÊú¨Ôºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂúñÂΩ¢ÊåñÊéòÊäÄË°ìËá™ÂãïÊ∏ÖÁêÜ„ÄÇÂàÜÈ°ûÊ≥ï‰∏äÁöÑÊìç‰ΩúÔºå‰æãÂ¶ÇÂâ™ÂàáÈèàÊé•ÊàñÂêà‰ΩµÈ°ûÂà•ÔºåÊòØÂú®ÈñãÊ∫ê LLM ‰∏äÂÄüÂä©Èõ∂Ê¨°ÊèêÁ§∫ÁöÑÂπ´Âä©‰∏ãÂü∑Ë°åÁöÑ„ÄÇÁ≤æÁÖâÂàÜÈ°ûÊ≥ïÁöÑÂìÅË≥™ÂæûÂÖßÂú®ÂíåÂ§ñÂú®ÁöÑËßÄÈªûÈÄ≤Ë°åË©ï‰º∞ÔºåÂú®ÂæåËÄÖÁöÑÂØ¶È´îÂàÜÂûã‰ªªÂãô‰∏äÔºåÈ°ØÁ§∫‰∫Ü WiKC ÁöÑÂØ¶ÈöõËààË∂£„ÄÇ

##### **Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**
2409.04009v1 by Miao Fan, Yeqi Bai, Mingming Sun, Ping Li

Relation classification (RC) plays a pivotal role in both natural language
understanding and knowledge graph completion. It is generally formulated as a
task to recognize the relationship between two entities of interest appearing
in a free-text sentence. Conventional approaches on RC, regardless of feature
engineering or deep learning based, can obtain promising performance on
categorizing common types of relation leaving a large proportion of
unrecognizable long-tail relations due to insufficient labeled instances for
training. In this paper, we consider few-shot learning is of great practical
significance to RC and thus improve a modern framework of metric learning for
few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained
features, expecting they can generalize well on long-tail relations. Extensive
experiments were conducted by FewRel, a large-scale supervised few-shot RC
dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate
that it can achieve substantial improvements over many baseline approaches.

ÊëòË¶ÅÔºöÈóú‰øÇÂàÜÈ°û (RC) Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁü•Ë≠òÂúñË≠úÂÆåÊàê‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÆÉÈÄöÂ∏∏Ë¢´Ë°®Ëø∞ÁÇ∫‰∏ÄÂÄã‰ªªÂãôÔºåÁî®ÊñºËæ®Ë≠òÂá∫ÁèæÂú®Ëá™Áî±ÊñáÂ≠óÂè•Â≠ê‰∏≠ÁöÑÂÖ©ÂÄãÊÑüËààË∂£ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÁÑ°Ë´ñÊòØÂü∫ÊñºÁâπÂæµÂ∑•Á®ãÈÇÑÊòØÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÇ≥Áµ± RC ÊñπÊ≥ïÔºåÈÉΩÂèØ‰ª•Â∞çÂ∏∏Ë¶ãÁöÑÈóú‰øÇÈ°ûÂûãÈÄ≤Ë°åÂàÜÈ°ûÔºåÂæûËÄåÁç≤ÂæóÊúâÂ∏åÊúõÁöÑÊïàËÉΩÔºå‰ΩÜÁî±ÊñºË®ìÁ∑¥Ê®ôÁ±§ÂØ¶‰æã‰∏çË∂≥ÔºåÂõ†Ê≠§ÁÑ°Ê≥ïËæ®Ë≠òÂá∫Â§ßÈáèÁöÑÈï∑Â∞æÈóú‰øÇ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™çÁÇ∫Â∞ëÊ®£Êú¨Â≠∏ÁøíÂ∞ç RC ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂØ¶Áî®ÊÑèÁæ©ÔºåÂõ†Ê≠§ÊîπÈÄ≤‰∫ÜÂ∫¶ÈáèÂ≠∏ÁøíÁöÑÁèæ‰ª£Ê°ÜÊû∂Ôºå‰ª•ÈÄ≤Ë°åÂ∞ëÊ®£Êú¨ RC„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®ÂÖ∑ÊúâÁ¥∞Á≤íÂ∫¶ÁâπÂæµÁöÑÂ§ßÈÇäË∑ù ProtoNetÔºåÊúüÊúõÂÆÉÂÄëËÉΩÂú®Èï∑Â∞æÈóú‰øÇ‰∏äÂæàÂ•ΩÂú∞Ê¶ÇÊã¨„ÄÇÊàëÂÄë‰ΩøÁî®Â§ßÂûãÁõ£Áù£Â∞ëÊ®£Êú¨ RC Ë≥áÊñôÈõÜ FewRel ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Ë©ï‰º∞ÊàëÂÄëÁöÑÊ°ÜÊû∂ÔºöLM-ProtoNet (FGF)„ÄÇÁµêÊûúË°®ÊòéÔºåÂÆÉÂèØ‰ª•ÊØîË®±Â§öÂü∫Á∑öÊñπÊ≥ïÁç≤ÂæóÈ°ØËëóÊîπÈÄ≤„ÄÇ

##### **Rx Strategist: Prescription Verification using LLM Agents System**
2409.03440v1 by Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh

To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.

ÊëòË¶ÅÔºöÁÇ∫‰∫Ü‰øùË≠∑ÊÇ£ËÄÖÂÆâÂÖ®ÔºåÁèæ‰ª£Ëó•ÂìÅË§áÈõúÊÄßË¶ÅÊ±ÇÂö¥Ê†ºÁöÑËôïÊñπÈ©óË≠â„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï - Rx Strategist - ÂÆÉÂà©Áî®Áü•Ë≠òÂúñË≠úÂíå‰∏çÂêåÁöÑÊêúÂ∞ãÁ≠ñÁï•‰æÜÂ¢ûÂº∑‰ª£ÁêÜÊû∂ÊßãÂÖßÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂäüËÉΩ„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÊäÄË°ìÂÖÅË®±Â§öÈöéÊÆµÁöÑ LLM ÁÆ°Á∑öÂíåÂæûËá™Ë®Ç‰∏ªÂãïÊàêÂàÜË≥áÊñôÂ∫´‰∏≠ÂèØÈù†Âú∞Êì∑ÂèñË≥áË®ä„ÄÇËôïÊñπÈ©óË≠âÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰æãÂ¶ÇÈÅ©ÊáâÁóá„ÄÅÂäëÈáèÂíåÂèØËÉΩÁöÑËó•Áâ©‰∫§‰∫í‰ΩúÁî®ÔºåÈÉΩÂú®ÁÆ°Á∑öÁöÑÊØèÂÄãÈöéÊÆµ‰∏≠Ê∂µËìã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊé®ÁêÜÂàÜÊï£Âú®ÈÄô‰∫õÈöéÊÆµ‰æÜÊ∏õËºïÂñÆ‰∏Ä LLM ÊäÄË°ìÁöÑÁº∫ÈªûÔºåÂêåÊôÇÊèêÈ´òÊ≠£Á¢∫ÊÄßÂíåÂèØÈù†ÊÄßÔºå‰∏¶Ê∏õÂ∞ëË®òÊÜ∂È´îÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåRx Strategist Ë∂ÖË∂äË®±Â§öÁèæÊúâÁöÑ LLMÔºåÈÅîÂà∞ËàáÁ∂ìÈ©óË±êÂØåÁöÑËá®Â∫äËó•ÂäëÂ∏´Áõ∏Áï∂ÁöÑË°®Áèæ„ÄÇÂú®Áèæ‰ª£Ëó•Áâ©Ë§áÈõúÁöÑ‰∏ñÁïå‰∏≠ÔºåÈÄôÁ®ÆÂ∞á LLM ËàáÊúâÁµÑÁπîÁöÑÁü•Ë≠òÂíåÂÖàÈÄ≤ÊêúÂ∞ãÊñπÊ≥ïÁõ∏ÁµêÂêàÔºåÁÇ∫Ê∏õÂ∞ëËôïÊñπÈåØË™§ÂíåÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåÊèê‰æõ‰∫ÜÂèØË°åÁöÑÈÄîÂæë„ÄÇ

##### **iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**
2409.03284v1 by Yassir Lairgi, Ludovic Moncla, R√©my Cazabet, Khalid Benabdeslem, Pierre Cl√©au

Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.

ÊëòË¶ÅÔºöÂ§ßÈÉ®ÂàÜÂèØÁî®Ë≥áÊñôÁÇ∫ÈùûÁµêÊßãÂåñÔºåÈÄô‰ΩøÂæóÂ≠òÂèñÊúâÂÉπÂÄºÁöÑË≥áË®äËÆäÂæóÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇËá™ÂãïÂª∫Á´ãÁü•Ë≠òÂúñË≠ú (KG) Â∞çÊñºÁµêÊßãÂåñË≥áÊñôÂíåËÆìË≥áÊñôÊòìÊñºÂ≠òÂèñËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÂ§†ÊúâÊïàÂú∞ÊêúÂ∞ãË≥áË®ä„ÄÇKG ‰πü‰øÉÈÄ≤Ë¶ãËß£„ÄÅÊé®Ë´ñÂíåÊé®ÁêÜ„ÄÇÂÇ≥Áµ±ÁöÑ NLP ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂëΩÂêçÂØ¶È´îËæ®Ë≠òÂíåÈóú‰øÇËêÉÂèñÔºåÂú®Ë≥áË®äÊ™¢Á¥¢‰∏≠ÊòØÈóúÈçµÔºå‰ΩÜÈù¢Ëá®ÈôêÂà∂ÔºåÂåÖÊã¨‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÂØ¶È´îÈ°ûÂûãÂíåÈúÄË¶ÅÁõ£Áù£ÂºèÂ≠∏Áøí„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÈõ∂Ê¨°ÊàñÂ∞ëÊ¨°Â≠∏Áøí„ÄÇÁÑ∂ËÄåÔºåÊú™Ëß£Ê±∫ÂíåË™ûÁæ©ÈáçË§áÁöÑÂØ¶È´îÂíåÈóú‰øÇ‰ªçÁÑ∂ÊßãÊàêÊåëÊà∞ÔºåÂ∞éËá¥ÂúñÂΩ¢‰∏ç‰∏ÄËá¥ÔºåÈúÄË¶ÅÂª£Ê≥õÁöÑÂæåËôïÁêÜ„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩ‰æùË≥¥Êñº‰∏ªÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ iText2KGÔºå‰∏ÄÁ®ÆÁî®ÊñºÊº∏ÈÄ≤Âºè„ÄÅËàá‰∏ªÈ°åÁÑ°ÈóúÁöÑ KG Âª∫ÊßãÊñπÊ≥ïÔºåÁÑ°ÈúÄÂæåËôïÁêÜ„ÄÇÈÄôÁ®ÆÂç≥ÊèíÂç≥Áî®„ÄÅÈõ∂Ê¨°ÁöÑÊñπÊ≥ïÈÅ©Áî®ÊñºÂª£Ê≥õÁöÑ KG Âª∫ÊßãÂ†¥ÊôØÔºå‰∏¶ÂåÖÂê´ÂõõÂÄãÊ®°ÁµÑÔºöÊñá‰ª∂Á≤æÈ§æÂô®„ÄÅÊº∏ÈÄ≤ÂºèÂØ¶È´îËêÉÂèñÂô®„ÄÅÊº∏ÈÄ≤ÂºèÈóú‰øÇËêÉÂèñÂô®Ôºå‰ª•ÂèäÂúñÂΩ¢Êï¥ÂêàÂô®ÂíåË¶ñË¶∫ÂåñÂô®„ÄÇËàáÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®‰∏âÁ®ÆÂ†¥ÊôØ‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩÔºöÂ∞áÁßëÂ≠∏Ë´ñÊñáËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÅÁ∂≤Á´ôËΩâÊèõÁÇ∫ÂúñÂΩ¢Ôºå‰ª•ÂèäÂ±•Ê≠∑ËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÇ

##### **GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**
2409.03258v1 by Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou

Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ËôïÁêÜÂúñÂΩ¢ÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉÂÄëÂú®ÈÄèÈÅéÂúñÂΩ¢ÊèèËø∞Â∫èÂàóÊèêÁ§∫ÁêÜËß£ÂúñÂΩ¢ÁµêÊßãË≥áË®äÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÂúñÂΩ¢Â§ßÂ∞èÂ¢ûÂä†ÊôÇ„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊåëÊà∞Ê≠∏Âõ†Êñº LLM Âú®ÂúñÂΩ¢ÊèèËø∞Â∫èÂàó‰∏≠‰∏çÂêå‰ΩçÁΩÆÁöÑË®òÊÜ∂ÂäõË°®Áèæ‰∏çÂùáÔºåÁ®±ÁÇ∫„Äå‰ΩçÁΩÆÂÅèË™§„Äç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphInsightÔºå‰∏ÄÂÄãÊó®Âú®ÊîπÂñÑ LLM Â∞çÂ∑®ËßÄÂíåÂæÆËßÄÂ±§Á¥öÂúñÂΩ¢Ë≥áË®äÁêÜËß£ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇGraphInsight ‰ª•ÂÖ©ÂÄãÈóúÈçµÁ≠ñÁï•ÁÇ∫Âü∫Á§éÔºö1) Â∞áÈóúÈçµÂúñÂΩ¢Ë≥áË®äÊîæÁΩÆÂú® LLM Â±ïÁèæËºÉÂº∑Ë®òÊÜ∂ÂäõË°®ÁèæÁöÑ‰ΩçÁΩÆÔºå‰ª•Âèä 2) Ë™øÊü•‰∏ÄÂÄãÂèóÂà∞Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÂïüÁôºÁöÑ„ÄÅÈáùÂ∞çË®òÊÜ∂ÂäõË°®ÁèæËºÉÂº±ÂçÄÂüüÁöÑËºïÈáèÁ¥öÂ§ñÈÉ®Áü•Ë≠òÂ∫´„ÄÇÊ≠§Â§ñÔºåGraphInsight Êé¢Á¥¢Â∞áÈÄôÂÖ©ÂÄãÁ≠ñÁï•Êï¥ÂêàÂà∞ LLM ‰ª£ÁêÜÁ®ãÂ∫è‰∏≠Ôºå‰ª•ËôïÁêÜÈúÄË¶ÅÂ§öÊ≠•È©üÊé®ÁêÜÁöÑË§áÂêàÂúñÂΩ¢‰ªªÂãô„ÄÇÂú®ÂÖ∑ÊúâÂª£Ê≥õË©ïÈáè‰ªªÂãôÁöÑÂü∫Ê∫ñ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶Ë≠âÁ†îÁ©∂È°ØÁ§∫ÔºåGraphInsight Âú®ÁêÜËß£ÂêÑÁ®ÆÂ§ßÂ∞èÁöÑÂúñÂΩ¢ÁµêÊßãÊñπÈù¢ÔºåÊòéÈ°ØÂÑ™ÊñºÊâÄÊúâÂÖ∂‰ªñÂúñÂΩ¢ÊèèËø∞ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊèêÁ§∫ÊäÄÂ∑ßÂíåÈáçÊñ∞ÊéíÂ∫èÁ≠ñÁï•Ôºâ„ÄÇ

##### **Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**
2409.03155v1 by Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui

Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÁº∫‰πèÁõ∏ÈóúÁü•Ë≠òÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØËÉΩÊúÉÁî¢ÁîüÂπªË¶∫„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåÁü•Ë≠òÂúñË≠úÂåÖÂê´Âª£Ê≥õÁöÑÂ§öÈáçÈóú‰øÇÁµêÊßãÔºåÂÑ≤Â≠òÂ§ßÈáèÁ¨¶Ëôü‰∫ãÂØ¶„ÄÇÂõ†Ê≠§ÔºåÂ∞á LLM ËàáÁü•Ë≠òÂúñË≠úÊï¥ÂêàÂ∑≤Âª£Ê≥õÊé¢Ë®éÔºåÂÖ∂‰∏≠Áü•Ë≠òÂúñË≠úÂïèÈ°åËß£Á≠î (KGQA) ÊàêÁÇ∫Êï¥ÂêàÁöÑÈáçË¶ÅË©¶ÈáëÁü≥„ÄÇÊ≠§‰ªªÂãôË¶ÅÊ±Ç LLM ÈÄèÈÅéÂæûÁü•Ë≠òÂúñË≠ú‰∏≠Êì∑ÂèñÁõ∏Èóú‰∏âÂÖÉÁµÑ‰æÜÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈù¢Ëá®ÂÖ©È†ÖÈáçÂ§ßÊåëÊà∞Ôºö\textit{ÈÅéÈï∑ÁöÑÊé®ÁêÜË∑ØÂæëÊúÉÂàÜÊï£ÂõûÁ≠îÁî¢Áîü}Ôºå‰ª•Âèä\textit{ÈåØË™§Ê≠£ÂêëÈóú‰øÇÈòªÁ§ôË∑ØÂæëÁ≤æÁÖâ}„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèçË¶Ü‰∫íÂãïÁöÑ KGQA Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ÁöÑ‰∫íÂãïÂ≠∏ÁøíËÉΩÂäõ‰æÜÂü∑Ë°åÊé®ÁêÜÂíåÂúñÂΩ¢ËæØË´ñ (DoG)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåDoG Êé°Áî®Â≠êÂúñËÅöÁÑ¶Ê©üÂà∂ÔºåÂÖÅË®± LLM Âú®ÊØèÂÄãÊé®ÁêÜÊ≠•È©üÂæåÂü∑Ë°åÁ≠îÊ°àÂòóË©¶ÔºåÂæûËÄåÊ∏õËºïÂÜóÈï∑Êé®ÁêÜË∑ØÂæëÁöÑÂΩ±Èüø„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåDoG Âà©Áî®Â§öËßíËâ≤ËæØË´ñÂ∞èÁµÑÈÄêÊº∏Á∞°ÂåñË§áÈõúÂïèÈ°åÔºåÊ∏õÂ∞ëÈåØË™§Ê≠£ÂêëÈóú‰øÇÁöÑÂΩ±Èüø„ÄÇÈÄôÁ®ÆËæØË´ñÊ©üÂà∂Á¢∫‰øù‰∫ÜÊé®ÁêÜÈÅéÁ®ãÁöÑÂèØÈù†ÊÄß„ÄÇÂú®‰∫îÂÄãÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÂíåÂÑ™Ë∂äÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåDoG Âú® WebQuestions Âíå GrailQA ‰∏äÁöÑÊ∫ñÁ¢∫Â∫¶ÂàÜÂà•ÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï ToG È´òÂá∫ 23.7% Âíå 9.1%„ÄÇÊ≠§Â§ñÔºåÂú®‰∏äËø∞Êï∏ÊìöÈõÜ‰∏äËàáÂêÑÁ®Æ LLM ÁöÑÊï¥ÂêàÂØ¶È©óÁ™ÅÈ°Ø‰∫Ü DoG ÁöÑÈùàÊ¥ªÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®\url{https://github.com/reml-group/DoG}ÂèñÂæó„ÄÇ</paragraph>

##### **Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**
2409.02481v1 by Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja

Effective question classification is crucial for AI-driven educational tools,
enabling adaptive learning systems to categorize questions by skill area,
difficulty level, and competence. This classification not only supports
educational diagnostics and analytics but also enhances complex tasks like
information retrieval and question answering by associating questions with
relevant categories. Traditional methods, often based on word embeddings and
conventional classifiers, struggle to capture the nuanced relationships in
natural language, leading to suboptimal performance. To address this, we
propose a novel approach leveraging graph convolutional networks (GCNs), named
Phrase Question-Graph Convolutional Network (PQ-GCN) to better model the
inherent structure of questions. By representing questions as graphs -- where
nodes signify words or phrases and edges denote syntactic or semantic
relationships -- our method allows GCNs to learn from the interconnected nature
of language more effectively. Additionally, we explore the incorporation of
phrase-based features to enhance classification accuracy, especially in
low-resource settings. Our findings demonstrate that GCNs, augmented with these
features, offer a promising solution for more accurate and context-aware
question classification, bridging the gap between graph neural network research
and practical educational applications.

ÊëòË¶ÅÔºöÊúâÊïàÁöÑÂïèÈ°åÂàÜÈ°ûÂ∞çÊñº AI È©ÖÂãïÁöÑÊïôËÇ≤Â∑•ÂÖ∑Ëá≥ÈóúÈáçË¶ÅÔºå
ËÆìÈÅ©ÊáâÊÄßÂ≠∏ÁøíÁ≥ªÁµ±ËÉΩ‰æùÊìöÊäÄËÉΩÈ†òÂüü„ÄÅ
Èõ£Â∫¶Á≠âÁ¥öÂíåËÉΩÂäõÂ∞çÂïèÈ°åÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÁ®ÆÂàÜÈ°û‰∏çÂÉÖÊîØÊè¥
ÊïôËÇ≤Ë®∫Êñ∑ÂíåÂàÜÊûêÔºåÈÇÑËÉΩÈÄèÈÅéÂ∞áÂïèÈ°åËàá
Áõ∏ÈóúÈ°ûÂà•ÈóúËÅØËµ∑‰æÜÔºåÂ¢ûÂº∑Ë≥áË®äÊ™¢Á¥¢ÂíåÂïèÈ°åËß£Á≠îÁ≠âË§áÈõú‰ªªÂãô„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏Âª∫Á´ãÂú®Ë©ûÂµåÂÖ•Âíå
ÂÇ≥Áµ±ÂàÜÈ°ûÂô®‰∏äÔºåÈõ£‰ª•ÊçïÊçâËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÁ¥∞ÂæÆÈóú‰øÇÔºåÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄë
ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑Ø (GCN)ÔºåÁ®±ÁÇ∫
Phrase Question-Graph Convolutional Network (PQ-GCN) ‰æÜÊõ¥Â•ΩÂú∞Âª∫Ê®°ÂïèÈ°åÁöÑÂÖßÂú®ÁµêÊßã„ÄÇÈÄèÈÅéÂ∞áÂïèÈ°åË°®Á§∫ÁÇ∫ÂúñÂΩ¢‚Äî‚ÄîÂÖ∂‰∏≠
ÁØÄÈªûË°®Á§∫Ë©ûÊàñË©ûÁµÑÔºåÈÇäÁ∑£Ë°®Á§∫Ë™ûÊ≥ïÊàñË™ûÁæ©Èóú‰øÇ‚Äî‚ÄîÊàëÂÄëÁöÑÊ®°ÂûãÂÖÅË®± GCN Êõ¥ÊúâÊïàÂú∞ÂæûË™ûË®ÄÁöÑÁõ∏‰∫íÈÄ£ÁµêÊÄßË≥™‰∏≠Â≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÊï¥Âêà
Âü∫ÊñºË©ûÁµÑÁöÑÁâπÂæµ‰ª•Â¢ûÂº∑ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®
‰ΩéË≥áÊ∫êË®≠ÂÆö‰∏≠„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåGCN Âú®ÈÄô‰∫õ
ÁâπÂæµÁöÑÂ¢ûÂº∑‰∏ãÔºåÁÇ∫Êõ¥Ê∫ñÁ¢∫‰∏îÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ËÉΩÂäõÁöÑÂïèÈ°åÂàÜÈ°ûÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁ∏ÆÂ∞è‰∫ÜÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁ†îÁ©∂
ËàáÂØ¶ÈöõÊïôËÇ≤ÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Multi-modal Situated Reasoning in 3D Scenes**
2409.02389v1 by Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang

Situation awareness is essential for understanding and reasoning about 3D
scenes in embodied AI agents. However, existing datasets and benchmarks for
situated understanding are limited in data modality, diversity, scale, and task
scope. To address these limitations, we propose Multi-modal Situated Question
Answering (MSQA), a large-scale multi-modal situated reasoning dataset,
scalably collected leveraging 3D scene graphs and vision-language models (VLMs)
across a diverse range of real-world 3D scenes. MSQA includes 251K situated
question-answering pairs across 9 distinct question categories, covering
complex scenarios within 3D scenes. We introduce a novel interleaved
multi-modal input setting in our benchmark to provide text, image, and point
cloud for situation and question description, resolving ambiguity in previous
single-modality convention (e.g., text). Additionally, we devise the
Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'
situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN
highlight the limitations of existing vision-language models and underscore the
importance of handling multi-modal interleaved inputs and situation modeling.
Experiments on data scaling and cross-domain transfer further demonstrate the
efficacy of leveraging MSQA as a pre-training dataset for developing more
powerful situated reasoning models.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÊÑüÁü•Â∞çÊñºÁêÜËß£ÂíåÊé®ÁêÜÂÖ∑Ë∫´ AI ‰ª£ÁêÜ‰∏≠ÁöÑ 3D Â†¥ÊôØËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË≥áÊñôÈõÜÂíåÂü∫Ê∫ñÂú®Ë≥áÊñôÊ®°ÊÖã„ÄÅÂ§öÊ®£ÊÄß„ÄÅË¶èÊ®°Âíå‰ªªÂãôÁØÑÂúçÊñπÈù¢Â∞çÊñºÊÉÖÂ¢ÉÁêÜËß£‰æÜË™™ÊòØÊúâÈôêÁöÑ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÂïèÁ≠î (MSQA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§ßÂûãÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÊé®ÁêÜË≥áÊñôÈõÜÔºåÂèØÈÄèÈÅéÂà©Áî® 3D Â†¥ÊôØÂúñÂíåË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Âú®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïå 3D Â†¥ÊôØ‰∏≠ÈÄ≤Ë°åÂèØÊì¥ÂÖÖÊî∂ÈõÜ„ÄÇMSQA ÂåÖÂê´ 251K ÂÄãÊÉÖÂ¢ÉÂïèÁ≠îÂ∞çÔºåÊ∂µËìã 9 ÂÄã‰∏çÂêåÁöÑÂïèÈ°åÈ°ûÂà•ÔºåÊ∂µËìã 3D Â†¥ÊôØ‰∏≠ÁöÑË§áÈõúÂ†¥ÊôØ„ÄÇÊàëÂÄëÂú®Âü∫Ê∫ñ‰∏≠ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ‰∫§ÈåØÂ§öÊ®°ÊÖãËº∏ÂÖ•Ë®≠ÂÆöÔºå‰ª•Êèê‰æõÊñáÂ≠ó„ÄÅÂΩ±ÂÉèÂíåÈªûÈõ≤ÔºåÁî®ÊñºÊÉÖÂ¢ÉÂíåÂïèÈ°åÊèèËø∞ÔºåËß£Ê±∫‰ª•ÂâçÂñÆ‰∏ÄÊ®°ÊÖãÊÖ£‰æãÔºà‰æãÂ¶ÇÊñáÂ≠óÔºâ‰∏≠ÁöÑÊ≠ßÁæ©„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢É‰∏ã‰∏ÄÊ≠•Â∞éËà™ (MSNN) Âü∫Ê∫ñÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãÁöÑÂ∞éËà™ÊÉÖÂ¢ÉÊé®ÁêÜ„ÄÇMSQA Âíå MSNN ÁöÑÁ∂úÂêàË©ï‰º∞Á™ÅÈ°Ø‰∫ÜÁèæÊúâË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÈôêÂà∂Ôºå‰∏¶Âº∑Ë™ø‰∫ÜËôïÁêÜÂ§öÊ®°ÊÖã‰∫§ÈåØËº∏ÂÖ•ÂíåÊÉÖÂ¢ÉÂª∫Ê®°ÁöÑÈáçË¶ÅÊÄß„ÄÇË≥áÊñôÊì¥ÂÖÖÂíåË∑®È†òÂüüËΩâÁßªÁöÑÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Ë≠âÊòé‰∫ÜÂà©Áî® MSQA ‰ΩúÁÇ∫È†êË®ìÁ∑¥Ë≥áÊñôÈõÜ‰æÜÈñãÁôºÊõ¥Âº∑Â§ßÁöÑÊÉÖÂ¢ÉÊé®ÁêÜÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Grounding Language Models in Autonomous Loco-manipulation Tasks**
2409.01326v1 by Jin Wang, Nikos Tsagarakis

Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.

ÊëòË¶ÅÔºöÂÖ∑ÊúâË°åÁÇ∫Ëá™‰∏ªÊ¨äÁöÑ‰∫∫ÂΩ¢Ê©üÂô®‰∫∫‰∏ÄÁõ¥Ë¢´Ë¶ñÁÇ∫ÊàëÂÄëÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁêÜÊÉ≥ÁöÑÂêà‰ΩúËÄÖÔºå‰πüÊòØÂÖ∑È´îÊô∫ËÉΩÁöÑÊúâÂ∏åÊúõÁöÑ‰ª£Ë°®„ÄÇËàáÂõ∫ÂÆöÂºèÊ©üÂô®ÊâãËáÇÁõ∏ÊØîÔºå‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Êèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÊìç‰ΩúÁ©∫ÈñìÔºåÂêåÊôÇÈ°ØËëóÂ¢ûÂä†‰∫ÜÊéßÂà∂ÂíåË¶èÂäÉÁöÑÈõ£Â∫¶„ÄÇÂÑòÁÆ°ÊúùËëóÈÄöÁî®‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Âø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∏Á†îÁ©∂‰ªçÁÑ∂ÈõÜ‰∏≠Âú®ÈÅãÂãïËÉΩÂäõ‰∏äÔºåÂæàÂ∞ëÁ†îÁ©∂ÂÖ®Ë∫´ÂçîË™øÂíå‰ªªÂãôË¶èÂäÉÔºåÂæûËÄåÈôêÂà∂‰∫ÜÂ±ïÁ§∫Ê∂âÂèäÁßªÂãïÂíåÊìç‰ΩúÁöÑÈï∑Êúü‰ªªÂãôÁöÑÊΩõÂäõÔºåÂêåÊôÇÈÇÑËÉΩÊé•ÂèóÈñãÊîæÂºèÂè£È†≠Êåá‰ª§„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂ÂèØ‰ª•Ê†πÊìö‰∏çÂêåÂ†¥ÊôØ‰∏≠ÁöÑ‰ªªÂãôÂ≠∏Áøí„ÄÅÈÅ∏ÊìáÂíåË¶èÂäÉË°åÁÇ∫„ÄÇÊàëÂÄëÂ∞áÂº∑ÂåñÂ≠∏Áøí (RL) ËàáÂÖ®Ë∫´ÂÑ™ÂåñÁõ∏ÁµêÂêàÔºå‰ª•ÁîüÊàêÊ©üÂô®‰∫∫Âãï‰Ωú‰∏¶Â∞áÂÖ∂Â≠òÂÑ≤Âà∞Âãï‰ΩúÂ∫´‰∏≠„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË¶èÂäÉÂíåÊé®ÁêÜÂäüËÉΩÔºåÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂàÜÂ±§‰ªªÂãôÂúñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÈÅãÂãïÂéüË™ûÔºå‰ª•Ê©ãÊé•‰ΩéÁ¥öÂü∑Ë°åÂíåÈ´òÁ¥öË¶èÂäÉ„ÄÇÂú®Ê®°Êì¨Âíå‰ΩøÁî® CENTAURO Ê©üÂô®‰∫∫ÁöÑÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÂØ¶È©óË°®ÊòéÔºåÂü∫ÊñºË™ûË®ÄÊ®°ÂûãÁöÑË¶èÂäÉÂô®ÂèØ‰ª•ÊúâÊïàÈÅ©ÊáâÊñ∞ÁöÑÈÅãÂãïÊìç‰Ωú‰ªªÂãôÔºåË≠âÊòé‰∫ÜÂú®ÈùûÁµêÊßãÂåñÂ†¥ÊôØ‰∏≠ÂæûËá™Áî±ÊñáÊú¨ÂëΩ‰ª§‰∏≠Áç≤ÂæóÁöÑÈ´òÂ∫¶Ëá™‰∏ªÊÄß„ÄÇ

##### **LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**
2409.01145v1 by Haoran Yang, Xiangyu Zhao, Sirui Huang, Qing Li, Guandong Xu

Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised
graph learning that has attracted attention across various application
scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet
to be explored. Because conventional augmentation techniques like feature
embedding masking cannot directly process textual attributes on TAGs. A naive
strategy for applying GCL to TAGs is to encode the textual attributes into
feature embeddings via a language model and then feed the embeddings into the
following GCL module for processing. Such a strategy faces three key
challenges: I) failure to avoid information loss, II) semantic loss during the
text encoding phase, and III) implicit augmentation constraints that lead to
uncontrollable and incomprehensible results. In this paper, we propose a novel
GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to
produce textual augmentations and LLMs' powerful natural language processing
(NLP) abilities to address the three limitations aforementioned to pave the way
for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG
datasets illustrate the superiority of the proposed LATEX-GCL method. The
source codes and datasets are released to ease the reproducibility, which can
be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.

ÊëòË¶ÅÔºöÂúñÂΩ¢Â∞çÊØîÂ≠∏Áøí (GCL) ÊòØËá™Áõ£Áù£ÂúñÂΩ¢Â≠∏ÁøíÁöÑÂº∑Â§ßÁØÑ‰æãÔºåÂ∑≤Âú®ÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØ‰∏≠ÂºïËµ∑ÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåGCL Â∞çÊñºÂú®ÊñáÊú¨Ë®ªËß£ÂúñÂΩ¢ (TAG) ‰∏äÂ≠∏ÁøíÂ∞öÊú™Ë¢´Êé¢Ë®é„ÄÇÂõ†ÁÇ∫ÁâπÂæµÂµåÂÖ•ÈÅÆÁΩ©Á≠âÂÇ≥Áµ±Êì¥ÂÖÖÊäÄË°ìÁÑ°Ê≥ïÁõ¥Êé•ËôïÁêÜ TAG ‰∏äÁöÑÊñáÊú¨Â±¨ÊÄß„ÄÇÂ∞á GCL ÊáâÁî®Êñº TAG ÁöÑ‰∏ÄÁ®ÆÂ§©ÁúüÁ≠ñÁï•ÊòØÈÄöÈÅéË™ûË®ÄÊ®°ÂûãÂ∞áÊñáÊú¨Â±¨ÊÄßÁ∑®Á¢ºÂà∞ÁâπÂæµÂµåÂÖ•‰∏≠ÔºåÁÑ∂ÂæåÂ∞áÂµåÂÖ•Ëº∏ÂÖ•ÂæåÁ∫åÁöÑ GCL Ê®°ÁµÑÈÄ≤Ë°åËôïÁêÜ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•Èù¢Ëá®‰∏âÂÄãÈóúÈçµÊåëÊà∞ÔºöI) ÁÑ°Ê≥ïÈÅøÂÖçË≥áË®äÈÅ∫Â§±ÔºåII) Âú®ÊñáÊú¨Á∑®Á¢ºÈöéÊÆµÁôºÁîüË™ûÁæ©ÈÅ∫Â§±Ôºå‰ª•Âèä III) Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂‰∏îÈõ£‰ª•ÁêÜËß£ÁµêÊûúÁöÑÈö±ÂºèÊì¥ÂÖÖÁ¥ÑÊùü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ LATEX-GCL ÁöÑÊñ∞Á©é GCL Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÊñáÊú¨Êì¥ÂÖÖÔºå‰ª•Âèä LLM Âº∑Â§ßÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ËÉΩÂäõ‰æÜËß£Ê±∫‰∏äËø∞‰∏âÂÄãÈôêÂà∂ÔºåÁÇ∫Â∞á GCL ÊáâÁî®Êñº TAG ‰ªªÂãôÈã™Âπ≥ÈÅìË∑Ø„ÄÇÂú®ÂõõÂÄãÈ´òÂìÅË≥™ TAG Ë≥áÊñôÈõÜ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óË™™Êòé‰∫ÜÊâÄÊèêÂá∫ÁöÑ LATEX-GCL ÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄß„ÄÇÂéüÂßãÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤ÁôºÂ∏É‰ª•Á∞°ÂåñÂèØÈáçË£ΩÊÄßÔºåÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂ≠òÂèñÔºöhttps://anonymous.4open.science/r/LATEX-GCL-0712„ÄÇ

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÔºåÂç≥‰ΩøÁ∂ìÈÅéÂæÆË™øÁöÑÊ®°Âûã‰πüÂÆπÊòìÁî¢ÁîüÂπªË¶∫„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÊõ¥Â§öÂèØÈù†ÁöÑÊ®°Âûã‰æÜÁ¥çÂÖ•Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊµÅÁ®ã 4StepFocusÔºåÁâπÂà•ÊòØÈ†êËôïÁêÜÊ≠•È©üÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM ÁöÑÁ≠îÊ°à„ÄÇÈÄôÊòØÈÄèÈÅéÊèê‰æõÂèóÂºïÂ∞éÁöÑÂ§ñÈÉ®Áü•Ë≠òÂ≠òÂèñÔºåÂà©Áî®Ê®°ÂûãËá™Ë°åÊì∑ÂèñÈóúËÅØÊÄßËÑàÁµ°ÂíåÈÄ≤Ë°åÂü∫Êú¨Êé®ÁêÜÁöÑËÉΩÂäõ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÂú®ÂçäÁµêÊßãÂåñÁü•Ë≠òÂ∫´‰∏≠ÈÄ≤Ë°åÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑÊêúÂ∞ãÔºå‰ª•Áõ¥Êé•‰∏îÂèØËøΩËπ§ÁöÑÊñπÂºèÁ∏ÆÂ∞èÊΩõÂú®Ê≠£Á¢∫Á≠îÊ°àÁöÑÁØÑÂúçÔºåÁÑ∂ÂæåÂÜçÂàáÊèõÂà∞ÊΩõÂú®Ë°®ÂæµÔºåÊ†πÊìöÈùûÁµêÊßãÂåñË≥áÊñôÂ∞çÈÄô‰∫õÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêç„ÄÇÈÄôËàáÁ¥îÁ≤πÂü∫ÊñºÊΩõÂú®Ë°®ÂæµÁöÑÁõ∏ÈóúÊñπÊ≥ïÊúâÊâÄÂçÄÂà•„ÄÇ4StepFocus ÂåÖÂê´‰ª•‰∏ãÊ≠•È©üÔºö1) Áî± LLM ÈÄ≤Ë°å‰∏âÂÖÉÁµÑÁî¢Áîü‰ª•Êì∑ÂèñÈóúËÅØË≥áÊñôÔºå2) Âú®ÈÄô‰∫õ‰∏âÂÖÉÁµÑ‰∏≠ÊõøÊèõËÆäÊï∏Ôºå‰ª•Êé°Áî®Áü•Ë≠òÂúñË°®Á∏ÆÂ∞èÁ≠îÊ°àÂÄôÈÅ∏ÁØÑÂúçÔºå3) ‰ΩøÁî®Ê∂âÂèäÈóúËÅØÈùûÁµêÊßãÂåñË≥áÊñôÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÂ∞ãÂ∞çÂâ©È§òÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂ∫èÔºå4) Áî± LLM ÈáçÊñ∞Â∞çÊúÄ‰Ω≥ÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêçÔºå‰∏¶Êèê‰æõËÉåÊôØË≥áÊñô„ÄÇÂú®ÈÜ´ÁôÇ„ÄÅÁî¢ÂìÅÊé®Ëñ¶ÂíåÂ≠∏Ë°ìË´ñÊñáÊêúÂ∞ãÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫ÂØ¶ÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊì¥ÂÖÖ„ÄÇÂÆÉ‰∏çÂÉÖÂ¢ûÂä†‰∫Ü‰æÜËá™Ë≥áË®äÊ™¢Á¥¢ÁöÑÁõ∏ÂÖ≥ÂèØËøΩËπ§ËÉåÊôØË≥áË®äÔºåËÄå‰∏îËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºå‰πüÂ§ßÂπÖÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈÆÆÂ∞ëÊé¢Á¥¢ÁöÑÊñπÂêëÔºåÂõ†Ê≠§Êèê‰æõ‰∫ÜÂª£Ê≥õÁöÑÊú™‰æÜÂ∑•‰ΩúÊ©üÊúÉ„ÄÇ‰ΩøÁî®ÁöÑÂéüÂßãÁ¢ºÂèØÂú® https://github.com/kramerlab/4StepFocus ÂèñÂæó„ÄÇ

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂ∑•Á®ãÂíåÂ§öË™ûË®ÄË™ûÁæ©Êé®ÁêÜÊäÄË°ìÁöÑÊú¨‰ΩìË®≠Ë®àÔºåÁî®ÊñºÂª∫Á´ã‰∏ÄÂÄãËá™ÂãïÂåñÁ≥ªÁµ±Ôºå‰ª•Áü•Ë≠òÂúñË≠úÁöÑÂΩ¢ÂºèÂê∏Êî∂Âç∞Â∫¶ÊñôÁêÜÁöÑÁÉπÈ£™Ë≥áË®ä„ÄÇÈáçÈªûÂú®ÊñºË®≠Ë®àÊô∫ÊÖßÊñπÊ≥ïÔºå‰ª•Êé®Â∞éÊú¨‰ΩìË®≠Ë®àÔºå‰∏¶ÂÖ®Èù¢Êì∑ÂèñÈóúÊñºÈ£üÁâ©„ÄÅÈ£üË≠ú„ÄÅÈ£üÊùê„ÄÅÁÉπÈ£™ÁâπÊÄßÔºå‰ª•ÂèäÊúÄÈáçË¶ÅÁöÑÁáüÈ§äÁöÑÁü•Ë≠òÔºå‰∏¶Êì¥Â§ßË¶èÊ®°„ÄÇÊàëÂÄëÂú®ÈÄôÂÄãÁ†îË®éÊúÉË´ñÊñá‰∏≠‰ªãÁ¥π‰∫ÜÊàëÂÄëÊ≠£Âú®ÈÄ≤Ë°åÁöÑÂ∑•‰ΩúÔºåË©≥Á¥∞ÊèèËø∞‰∫ÜÊï¥ÁêÜÂç∞Â∫¶ÊñôÁêÜÁü•Ë≠òÁõ∏ÈóúÁöÑÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊàëÂÄëÁöÑÈ´òÈöéÊú¨‰ΩìË®≠Ë®à„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÆÉ‰ΩøÁî® AI„ÄÅLLM ÂíåË™ûË®ÄÊäÄË°ìÔºåÂæûÂÖ¨ÂÖ±È†òÂüüÁöÑÈ£üË≠úÈÉ®ËêΩÊ†ºÁ∂≤Á´ô‰∏≠Êï¥ÁêÜË≥áË®äÔºå‰ª•Âª∫Á´ãÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑÁü•Ë≠òÊï¥ÁêÜÊñπÊ≥ïÊòØÈÄöÁî®ÁöÑÔºåÂèØ‰ª•Ë§áË£ΩÂà∞‰ªª‰ΩïÈ†òÂüü„ÄÇË®≠Ë®àËàáÊáâÁî®ÁÑ°ÈóúÔºåÂèØÁî®Êñº AI È©ÖÂãïÁöÑÊô∫ÊÖßÂàÜÊûê„ÄÅÂª∫Á´ãÂÄã‰∫∫ÂåñÊï∏‰ΩçÂÅ•Â∫∑Êé®Ëñ¶Á≥ªÁµ±Ôºå‰ª•Âèä‰ΩøÁî®‰ΩøÁî®ËÄÖË≥áË®ä„ÄÅÈ£üÁâ©ÁîüÁâ©ÂåñÂ≠∏„ÄÅÂú∞ÁêÜË≥áË®ä„ÄÅËæ≤Ê•≠Ë≥áË®äÁ≠âËÑàÁµ°Ë≥áË®äÔºå‰æÜË£úÂÖÖÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇ

##### **Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**
2409.00727v1 by Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuanhui Yang, Yuanyuan Zhu, Chuang Hu, Bo Du, Jiawei Jiang

Text-attributed graph (TAG) is an important type of graph structured data
with text descriptions for each node. Few- and zero-shot node classification on
TAGs have many applications in fields such as academia and social networks.
However, the two tasks are challenging due to the lack of supervision signals,
and existing methods only use the contrastive loss to align graph-based node
embedding and language-based text embedding. In this paper, we propose Hound to
improve accuracy by introducing more supervision signals, and the core idea is
to go beyond the node-text pairs that come with data. Specifically, we design
three augmentation techniques, i.e., node perturbation, text matching, and
semantics negation to provide more reference nodes for each text and vice
versa. Node perturbation adds/drops edges to produce diversified node
embeddings that can be matched with a text. Text matching retrieves texts with
similar embeddings to match with a node. Semantics negation uses a negative
prompt to construct a negative text with the opposite semantics, which is
contrasted with the original node and text. We evaluate Hound on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that Hound
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

ÊëòË¶ÅÔºöÊñáÂ≠óÂ±ûÊÄßÂúñ (TAG) ÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑÂúñÂΩ¢ÁµêÊßãÂåñË≥áÊñôÈ°ûÂûãÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÈÉΩÊúâÊñáÂ≠óÊèèËø∞„ÄÇTAG ‰∏äÁöÑÂ∞ëÊ®£Êú¨ÂíåÈõ∂Ê®£Êú¨ÁØÄÈªûÂàÜÈ°ûÂú®Â≠∏Ë°ìÁïåÂíåÁ§æ‰∫§Á∂≤Ë∑ØÁ≠âÈ†òÂüüÊúâË®±Â§öÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÁõ£Áù£Ë®äËôüÔºåÈÄôÂÖ©ÂÄã‰ªªÂãôÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÁèæÊúâÊñπÊ≥ïÂÉÖ‰ΩøÁî®Â∞çÊØîÊêçÂ§±‰æÜÂ∞çÈΩäÂü∫ÊñºÂúñÂΩ¢ÁØÄÈªûÁöÑÂµåÂÖ•ÂíåÂü∫ÊñºË™ûË®ÄÁöÑÊñáÂ≠óÂµåÂÖ•„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Hound ‰æÜÈÄèÈÅéÂºïÂÖ•Êõ¥Â§öÁõ£Áù£Ë®äËôü‰æÜÊîπÂñÑÊ∫ñÁ¢∫Â∫¶ÔºåÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØË∂ÖË∂äË≥áÊñô‰∏≠ÈôÑÂ∏∂ÁöÑÁØÄÈªûÊñáÂ≠óÂ∞ç„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏âÁ®ÆÊì¥ÂÖÖÊäÄË°ìÔºåÂç≥ÁØÄÈªûÊìæÂãï„ÄÅÊñáÂ≠óÈÖçÂ∞çÂíåË™ûÁæ©Âê¶ÂÆöÔºåÁÇ∫ÊØèÂÄãÊñáÂ≠óÊèê‰æõÊõ¥Â§öÂèÉËÄÉÁØÄÈªûÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÁØÄÈªûÊìæÂãïÊñ∞Â¢û/Âà™Èô§ÈÇäÁ∑£‰ª•Áî¢ÁîüÂèØ‰ª•ËàáÊñáÂ≠óÈÖçÂ∞çÁöÑÂ§öÊ®£ÂåñÁØÄÈªûÂµåÂÖ•„ÄÇÊñáÂ≠óÈÖçÂ∞çÊì∑ÂèñÂÖ∑ÊúâÈ°û‰ººÂµåÂÖ•ÁöÑÊñáÂ≠ó‰ª•ËàáÁØÄÈªûÈÖçÂ∞ç„ÄÇË™ûÁæ©Âê¶ÂÆö‰ΩøÁî®Ë≤†Èù¢ÊèêÁ§∫‰æÜÂª∫ÊßãÂÖ∑ÊúâÁõ∏ÂèçË™ûÁæ©ÁöÑË≤†Èù¢ÊñáÂ≠óÔºåËàáÂéüÂßãÁØÄÈªûÂíåÊñáÂ≠óÂΩ¢ÊàêÂ∞çÊØî„ÄÇÊàëÂÄëÂú® 5 ÂÄãË≥áÊñôÈõÜ‰∏äË©ï‰º∞ HoundÔºå‰∏¶Ëàá 13 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÂü∫Á∑öÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúË°®ÊòéÔºåHound Âú®ÊâÄÊúâÂü∫Á∑ö‰∏äÂßãÁµÇË°®ÁèæÂÑ™Áï∞ÔºåÂÖ∂Ê∫ñÁ¢∫Â∫¶ÈÄöÂ∏∏ÊØîÊïàËÉΩÊúÄ‰Ω≥ÁöÑÂü∫Á∑öÊèêÈ´ò‰∫Ü 5% ‰ª•‰∏ä„ÄÇ

##### **WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**
2409.00331v1 by Oktie Hassanzadeh

Recently, there has been an increasing interest in the construction of
general-domain and domain-specific causal knowledge graphs. Such knowledge
graphs enable reasoning for causal analysis and event prediction, and so have a
range of applications across different domains. While great progress has been
made toward automated construction of causal knowledge graphs, the evaluation
of such solutions has either focused on low-level tasks (e.g., cause-effect
phrase extraction) or on ad hoc evaluation data and small manual evaluations.
In this paper, we present a corpus, task, and evaluation framework for causal
knowledge graph construction. Our corpus consists of Wikipedia articles for a
collection of event-related concepts in Wikidata. The task is to extract causal
relations between event concepts from the corpus. The evaluation is performed
in part using existing causal relations in Wikidata to measure recall, and in
part using Large Language Models to avoid the need for manual or crowd-sourced
evaluation. We evaluate a pipeline for causal knowledge graph construction that
relies on neural models for question answering and concept linking, and show
how the corpus and the evaluation framework allow us to effectively find the
right model for each task. The corpus and the evaluation framework are publicly
available.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºå‰∫∫ÂÄëÂ∞çÈÄöÁî®È†òÂüüÂíåÁâπÂÆöÈ†òÂüüÂõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãË∂ä‰æÜË∂äÊÑüËààË∂£„ÄÇÊ≠§È°ûÁü•Ë≠òÂúñË≠úËÉΩÂ§†Êé®ÁêÜÂõ†ÊûúÂàÜÊûêÂíå‰∫ã‰ª∂È†êÊ∏¨ÔºåÂõ†Ê≠§Âú®‰∏çÂêåÈ†òÂüü‰∏≠ÊúâÂª£Ê≥õÁöÑÊáâÁî®„ÄÇÈõñÁÑ∂Âú®Âõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑËá™ÂãïÂª∫ÊßãÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜÊ≠§È°ûËß£Ê±∫ÊñπÊ°àÁöÑË©ï‰º∞Ë¶ÅÂòõËëóÈáçÊñº‰ΩéÈöé‰ªªÂãôÔºà‰æãÂ¶ÇÂõ†ÊûúÈóú‰øÇÁü≠Ë™ûÊì∑ÂèñÔºâÔºåË¶ÅÂòõËëóÈáçÊñºËá®ÊôÇË©ï‰º∞Ë≥áÊñôÂíåÂ∞èÂûãÊâãÂãïË©ï‰º∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË™ûÊñôÂ∫´„ÄÅ‰ªªÂãôÂíåÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãË©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑË™ûÊñôÂ∫´ÂåÖÂê´Á∂≠Âü∫ÁôæÁßëÊñáÁ´†ÔºåÂÖ∂‰∏≠ÂåÖÂê´ Wikidata ‰∏≠‰∏ÄÁ≥ªÂàó‰∫ã‰ª∂Áõ∏ÈóúÊ¶ÇÂøµ„ÄÇ‰ªªÂãôÊòØÂæûË™ûÊñôÂ∫´‰∏≠Êì∑Âèñ‰∫ã‰ª∂Ê¶ÇÂøµ‰πãÈñìÁöÑÂõ†ÊûúÈóú‰øÇ„ÄÇË©ï‰º∞ÈÉ®ÂàÜ‰ΩøÁî® Wikidata ‰∏≠ÁèæÊúâÁöÑÂõ†ÊûúÈóú‰øÇ‰æÜË°°ÈáèÂè¨ÂõûÁéáÔºåÈÉ®ÂàÜ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÈÅøÂÖçÊâãÂãïÊàñÁæ§ÁúæÂ§ñÂåÖË©ï‰º∞ÁöÑÈúÄË¶Å„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÂÄãÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãÁÆ°ÈÅìÔºåË©≤ÁÆ°ÈÅì‰æùË≥¥ÊñºÁî®ÊñºÂïèÁ≠îÂíåÊ¶ÇÂøµÈÄ£ÁµêÁöÑÁ•ûÁ∂ìÊ®°ÂûãÔºå‰∏¶Â±ïÁ§∫‰∫ÜË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂ¶Ç‰ΩïËÆìÊàëÂÄëÊúâÊïàÂú∞ÁÇ∫ÊØèÂÄã‰ªªÂãôÊâæÂà∞ÂêàÈÅ©ÁöÑÊ®°Âûã„ÄÇË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂÖ¨ÈñãÊèê‰æõ„ÄÇ</paragraph>

##### **HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**
2409.09046v1 by Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven

While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈõñÁÑ∂Âú®ÊñáÂ≠óÁî¢ÁîüÂíåÂïèÁ≠îÊñπÈù¢Ë°®ÁèæÂÑ™Áï∞Ôºå‰ΩÜÂÖ∂Âú® AI Ê≥ïÂæãÂíåÊîøÁ≠ñ‰∏≠ÁöÑÊïàËÉΩÂçªÂèóÂà∞ÈÅéÊôÇÁü•Ë≠ò„ÄÅÂπªË¶∫‰ª•ÂèäÂú®Ë§áÈõúËÑàÁµ°‰∏≠Êé®ÁêÜ‰∏çË∂≥ÁöÑÈôêÂà∂„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Áü•Ë≠ò‰æÜÊîπÂñÑÂõûÊáâÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÂçªÂú®Ê™¢Á¥¢ÈåØË™§„ÄÅËÑàÁµ°Êï¥Âêà‰∏çËâØ‰ª•ÂèäÊàêÊú¨È´òÊòÇÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®Ë©ÆÈáãÂÆöÊÄßÂíåÂÆöÈáèÁöÑ AI Ê≥ïÂæãÊñáÊú¨ÊôÇ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂ∞àÁÇ∫ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÈáèË∫´ÊâìÈÄ†ÁöÑÊ∑∑ÂêàÂèÉÊï∏Ëá™ÈÅ©Êáâ RAG (HyPA-RAG) Á≥ªÁµ±Ôºå‰ª•Á¥êÁ¥ÑÂ∏ÇÂú∞ÊñπÊ≥ïÂæã 144 (LL144) ÁÇ∫‰æã„ÄÇHyPA-RAG ‰ΩøÁî®Êü•Ë©¢Ë§áÈõúÂ∫¶ÂàÜÈ°ûÂô®ÈÄ≤Ë°åËá™ÈÅ©ÊáâÂèÉÊï∏Ë™øÊï¥ÔºåÁµêÂêàÁ®†ÂØÜ„ÄÅÁ®ÄÁñèÂíåÁü•Ë≠òÂúñË°®ÊñπÊ≥ïÁöÑÊ∑∑ÂêàÊ™¢Á¥¢Á≠ñÁï•Ôºå‰ª•ÂèäÂåÖÂê´ÁâπÂÆöÂïèÈ°åÈ°ûÂûãÂíåÊåáÊ®ôÁöÑË©ï‰º∞Êû∂Êßã„ÄÇÈÄèÈÅéÂãïÊÖãË™øÊï¥ÂèÉÊï∏ÔºåHyPA-RAG Â§ßÂπÖÊîπÂñÑ‰∫ÜÊ™¢Á¥¢Ê∫ñÁ¢∫ÊÄßÂíåÂõûÊáâ‰øùÁúüÂ∫¶„ÄÇÂú® LL144 ‰∏äÁöÑÊ∏¨Ë©¶È°ØÁ§∫Âá∫Â¢ûÂº∑ÁöÑÊ≠£Á¢∫ÊÄß„ÄÅÂø†ÂØ¶Â∫¶ÂíåËÑàÁµ°Ê∫ñÁ¢∫Â∫¶ÔºåÊªøË∂≥‰∫ÜÂú®Ë§áÈõú„ÄÅÈ´òÈ¢®Èö™ÁöÑ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÊáâÁî®‰∏≠Â∞çÂèØÈÅ©Êáâ NLP Á≥ªÁµ±ÁöÑÈúÄÊ±Ç„ÄÇ

##### **LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**
2408.16224v2 by Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng

Recent advances in large vision-language models (VLMs) typically employ
vision encoders based on the Vision Transformer (ViT) architecture. The
division of the images into patches by ViT results in a fragmented perception,
thereby hindering the visual understanding capabilities of VLMs. In this paper,
we propose an innovative enhancement to address this limitation by introducing
a Scene Graph Expression (SGE) module in VLMs. This module extracts and
structurally expresses the complex semantic information within images, thereby
improving the foundational perception and understanding abilities of VLMs.
Extensive experiments demonstrate that integrating our SGE module significantly
enhances the VLM's performance in vision-language tasks, indicating its
effectiveness in preserving intricate semantic details and facilitating better
visual understanding.

ÊëòË¶ÅÔºöËøë‰æÜÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÈÄ≤Â±ïÈÄöÂ∏∏Êé°Áî®Âü∫ÊñºË¶ñË¶∫ËΩâÊèõÂô® (ViT) Êû∂ÊßãÁöÑË¶ñË¶∫Á∑®Á¢ºÂô®„ÄÇViT Â∞áÂΩ±ÂÉèÂàÜÂâ≤ÊàêÂçÄÂ°äÊúÉÈÄ†ÊàêÁ†¥Á¢éÁöÑÊÑüÁü•ÔºåÂæûËÄåÈòªÁ§ô VLM ÁöÑË¶ñË¶∫ÁêÜËß£ËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂâµÊñ∞ÁöÑÂ¢ûÂº∑ÂäüËÉΩÔºåÈÄèÈÅéÂú® VLM ‰∏≠ÂºïÂÖ•Â†¥ÊôØÂúñË°®ÈÅî (SGE) Ê®°ÁµÑ‰æÜËß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÊ≠§Ê®°ÁµÑÊúÉËêÉÂèñÂΩ±ÂÉè‰∏≠ÁöÑË§áÈõúË™ûÊÑèË≥áË®ä‰∏¶‰ª•ÁµêÊßãÂåñÁöÑÊñπÂºèË°®ÈÅîÔºåÂæûËÄåÊîπÂñÑ VLM ÁöÑÂü∫Á§éÊÑüÁü•ÂíåÁêÜËß£ËÉΩÂäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊï¥ÂêàÊàëÂÄëÁöÑ SGE Ê®°ÁµÑËÉΩÈ°ØËëóÊèêÂçá VLM Âú®Ë¶ñË¶∫Ë™ûË®Ä‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºåË°®Á§∫ÂÆÉÂú®‰øùÁïôË§áÈõúÁöÑË™ûÊÑèÁ¥∞ÁØÄÂíå‰øÉÈÄ≤Êõ¥Â•ΩÁöÑË¶ñË¶∫ÁêÜËß£ÊñπÈù¢ÂæàÊúâÊïà„ÄÇ

##### **LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**
2408.15903v1 by Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

The rapid obsolescence of information in Large Language Models (LLMs) has
driven the development of various techniques to incorporate new facts. However,
existing methods for knowledge editing still face difficulties with multi-hop
questions that require accurate fact identification and sequential logical
reasoning, particularly among numerous fact updates. To tackle these
challenges, this paper introduces Graph Memory-based Editing for Large Language
Models (GMeLLo), a straitforward and effective method that merges the explicit
knowledge representation of Knowledge Graphs (KGs) with the linguistic
flexibility of LLMs. Beyond merely leveraging LLMs for question answering,
GMeLLo employs these models to convert free-form language into structured
queries and fact triples, facilitating seamless interaction with KGs for rapid
updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art knowledge editing methods in
the multi-hop question answering benchmark, MQuAKE, especially in scenarios
with extensive knowledge edits.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ë≥áË®äÂø´ÈÄüÈÅéÊôÇÔºå‰øÉ‰ΩøÂêÑÁ®ÆÊäÄË°ìÁôºÂ±ï‰ª•Á¥çÂÖ•Êñ∞‰∫ãÂØ¶„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÂú®ÈúÄË¶ÅÊ∫ñÁ¢∫‰∫ãÂØ¶Ëæ®Ë≠òÂíåÈ†ÜÂ∫èÈÇèËºØÊé®ÁêÜÁöÑÂ§öË∑≥ÂïèÈ°å‰∏ä‰ªçÈù¢Ëá®Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÁúæÂ§ö‰∫ãÂØ¶Êõ¥Êñ∞‰∏≠„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Êñá‰ªãÁ¥π‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂúñË®òÊÜ∂Á∑®ËºØ (GMeLLo)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰∏îÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊòéÁ¢∫Áü•Ë≠òË°®Á§∫Ëàá LLM ÁöÑË™ûË®ÄÈùàÊ¥ªÊÄß„ÄÇGMeLLo ‰∏çÂÉÖÂà©Áî® LLM ‰æÜÂõûÁ≠îÂïèÈ°åÔºåÈÇÑ‰ΩøÁî®ÈÄô‰∫õÊ®°ÂûãÂ∞áËá™Áî±ÂΩ¢ÂºèÁöÑË™ûË®ÄËΩâÊèõÁÇ∫ÁµêÊßãÂåñÊü•Ë©¢Âíå‰∫ãÂØ¶‰∏âÂÖÉÁµÑÔºå‰øÉÈÄ≤Ëàá KG ÁöÑÁÑ°Á∏´‰∫íÂãïÔºå‰ª•‰æøÂø´ÈÄüÊõ¥Êñ∞ÂíåÁ≤æÁ¢∫ÁöÑÂ§öË∑≥Êé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂú®Â§öË∑≥ÂïèÈ°åÂõûÁ≠îÂü∫Ê∫ñ MQuAKE ‰∏≠ÔºåGMeLLo ÊòéÈ°ØË∂ÖË∂ä‰∫ÜÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Âª£Ê≥õÁü•Ë≠òÁ∑®ËºØÁöÑÂ†¥ÊôØ‰∏≠„ÄÇ

##### **VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**
2408.14895v2 by Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda

Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÁü•Ë≠òÂúñÔºàMMKGÔºâÂ∞áÂêÑÁ®ÆÈùûÁ¨¶ËôüÊï∏ÊìöÔºà‰æãÂ¶ÇÔºåÂΩ±ÂÉèÂíåÂΩ±ÁâáÔºâËΩâÊèõÁÇ∫Á¨¶ËôüÔºåÊàêÁÇ∫‰∏ÄÁ®ÆË≥áÊ∫êÔºåËÉΩËÆìË∑®Ê®°ÊÖãÁöÑÁü•Ë≠òËôïÁêÜÂíåÊ©üÂô®Â≠∏ÁøíÊàêÁÇ∫ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂåÖÂê´Â§öÂÄã‰∫ã‰ª∂Ôºà‰æãÂ¶ÇÊó•Â∏∏ÁîüÊ¥ªÊ¥ªÂãïÔºâÁöÑÂΩ±ÁâáÔºåÂÖ∂ MMKG ÁöÑÂª∫Êßã‰ªçËôïÊñºÊó©ÊúüÈöéÊÆµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂü∫ÊñºÊØèÊó•Ê¥ªÂãïÁöÑÂêåÊ≠•Â§öË¶ñËßíÊ®°Êì¨ÂΩ±ÁâáÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄã MMKG„ÄÇÈô§‰∫ÜÂ∞áÊó•Â∏∏ÁîüÊ¥ªÂΩ±ÁâáÁöÑÂÖßÂÆπË°®Á§∫ÁÇ∫‰ª•‰∫ã‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁü•Ë≠òÂ§ñÔºåÊàëÂÄëÁöÑ MMKG ‰πüÂåÖÂê´ÈÄêÂπÄÁöÑÁ¥∞ÂæÆËÆäÂåñÔºå‰æãÂ¶ÇÂΩ±ÁâáÂπÄ‰∏≠ÁöÑÈÇäÁïåÊ°Ü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÊèê‰æõ‰∫ÜÁî®ÊñºÊü•Ë©¢ MMKG ÁöÑÊîØÊè¥Â∑•ÂÖ∑„ÄÇ‰ΩúÁÇ∫ÊáâÁî®ÁØÑ‰æãÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑ MMKG Â¶Ç‰ΩïÈÄèÈÅéÊèê‰æõÁâπÂÆö‰ªªÂãôÊâÄÈúÄÁöÑË¶ñË¶∫Ë™ûË®ÄË≥áÊñôÈõÜÔºå‰æÜ‰øÉÈÄ≤Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇ

##### **XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**
2408.16021v1 by Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian

In the rapidly evolving field of cybersecurity, the integration of flow-level
and packet-level information for real-time intrusion detection remains a
largely untapped area of research. This paper introduces "XG-NID," a novel
framework that, to the best of our knowledge, is the first to fuse flow-level
and packet-level data within a heterogeneous graph structure, offering a
comprehensive analysis of network traffic. Leveraging a heterogeneous graph
neural network (GNN) with graph-level classification, XG-NID uniquely enables
real-time inference while effectively capturing the intricate relationships
between flow and packet payload data. Unlike traditional GNN-based
methodologies that predominantly analyze historical data, XG-NID is designed to
accommodate the heterogeneous nature of network traffic, providing a robust and
real-time defense mechanism. Our framework extends beyond mere classification;
it integrates Large Language Models (LLMs) to generate detailed, human-readable
explanations and suggest potential remedial actions, ensuring that the insights
produced are both actionable and comprehensible. Additionally, we introduce a
new set of flow features based on temporal information, further enhancing the
contextual and explainable inferences provided by our model. To facilitate
practical application and accessibility, we developed "GNN4ID," an open-source
tool that enables the extraction and transformation of raw network traffic into
the proposed heterogeneous graph structure, seamlessly integrating flow and
packet-level data. Our comprehensive quantitative comparative analysis
demonstrates that XG-NID achieves an F1 score of 97\% in multi-class
classification, outperforming existing baseline and state-of-the-art methods.
This sets a new standard in Network Intrusion Detection Systems by combining
innovative data fusion with enhanced interpretability and real-time
capabilities.

ÊëòË¶ÅÔºö<paragraph>Âú®Âø´ÈÄüÁôºÂ±ïÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®È†òÂüü‰∏≠ÔºåÊï¥ÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áË®ä‰ª•ÈÄ≤Ë°åÂç≥ÊôÇÂÖ•‰æµÂÅµÊ∏¨Ôºå‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂ∞öÊú™ÈñãÁôºÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÊú¨Êñá‰ªãÁ¥π„ÄåXG-NID„ÄçÔºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊû∂ÊßãÔºåÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂú®Áï∞Ë≥™ÂúñÂΩ¢ÁµêÊßã‰∏≠ËûçÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñôÁöÑÊû∂ÊßãÔºåÊèê‰æõÂ∞çÁ∂≤Ë∑ØÊµÅÈáèÁöÑÂÖ®Èù¢ÂàÜÊûê„ÄÇÈÄèÈÅéÂà©Áî®Áï∞Ë≥™ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÔºåXG-NID Áç®ÁâπÂú∞ÂØ¶ÁèæÂç≥ÊôÇÊé®Ë´ñÔºåÂêåÊôÇÊúâÊïàÊì∑ÂèñÊµÅÂíåÂ∞ÅÂåÖÈÖ¨ËºâË≥áÊñô‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇËàáÂÇ≥Áµ±Âü∫Êñº GNN ÁöÑÊñπÊ≥ïÔºà‰∏ªË¶ÅÂàÜÊûêÊ≠∑Âè≤Ë≥áÊñôÔºâ‰∏çÂêåÔºåXG-NID Ë¢´Ë®≠Ë®àÊàêÈÅ©ÊáâÁ∂≤Ë∑ØÊµÅÈáèÁöÑÁï∞Ë≥™ÊÄßÔºåÊèê‰æõÂº∑Â§ß‰∏îÂç≥ÊôÇÁöÑÈò≤Á¶¶Ê©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂Êßã‰∏çÂÉÖÈôêÊñºÂàÜÈ°ûÔºõÂÆÉÊï¥ÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•Áî¢ÁîüË©≥Á¥∞„ÄÅ‰∫∫È°ûÂèØËÆÄÁöÑËß£Èáã‰∏¶Âª∫Ë≠∞ÊΩõÂú®ÁöÑË£úÊïëÊé™ÊñΩÔºåÁ¢∫‰øùÁî¢ÁîüÁöÑË¶ãËß£Êó¢ÂèØÊìç‰ΩúÂèàÊòìÊñºÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ†πÊìöÊôÇÈñìË≥áË®äÂºïÂÖ•‰∏ÄÁµÑÊñ∞ÁöÑÊµÅÁâπÂæµÔºåÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ê®°ÂûãÊèê‰æõÁöÑËÑàÁµ°ÂíåÂèØËß£ÈáãÊé®Ë´ñ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂØ¶ÈöõÊáâÁî®ÂíåÂèØÂèäÊÄßÔºåÊàëÂÄëÈñãÁôº‰∫Ü„ÄåGNN4ID„ÄçÔºå‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Â∞áÂéüÂßãÁ∂≤Ë∑ØÊµÅÈáèÊèêÂèñ‰∏¶ËΩâÊèõÁÇ∫Âª∫Ë≠∞ÁöÑÁï∞Ë≥™ÂúñÂΩ¢ÁµêÊßãÔºåÁÑ°Á∏´Êï¥ÂêàÊµÅÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñô„ÄÇÊàëÂÄëÂÖ®Èù¢ÁöÑÂÆöÈáèÊØîËºÉÂàÜÊûêË°®ÊòéÔºåXG-NID Âú®Â§öÈ°ûÂà•ÂàÜÈ°û‰∏≠ÈÅîÂà∞ 97% ÁöÑ F1 ÂàÜÊï∏ÔºåÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñÂíåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÈÄôÈÄèÈÅéÁµêÂêàÂâµÊñ∞ÁöÑË≥áÊñôËûçÂêà„ÄÅÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÂíåÂç≥ÊôÇÂäüËÉΩÔºåÂú®Á∂≤Ë∑ØÂÖ•‰æµÂÅµÊ∏¨Á≥ªÁµ±‰∏≠Ê®πÁ´ã‰∫ÜÊñ∞ÁöÑÊ®ôÊ∫ñ„ÄÇ</paragraph>

##### **Process Trace Querying using Knowledge Graphs and Notation3**
2409.04452v1 by William Van Woensel

In process mining, a log exploration step allows making sense of the event
traces; e.g., identifying event patterns and illogical traces, and gaining
insight into their variability. To support expressive log exploration, the
event log can be converted into a Knowledge Graph (KG), which can then be
queried using general-purpose languages. We explore the creation of semantic KG
using the Resource Description Framework (RDF) as a data model, combined with
the general-purpose Notation3 (N3) rule language for querying. We show how
typical trace querying constraints, inspired by the state of the art, can be
implemented in N3. We convert case- and object-centric event logs into a
trace-based semantic KG; OCEL2 logs are hereby "flattened" into traces based on
object paths through the KG. This solution offers (a) expressivity, as queries
can instantiate constraints in multiple ways and arbitrarily constrain
attributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2
event logs can be serialized as traces in arbitrary ways based on the KG; and
(c) extensibility, as others can extend our library by leveraging the same
implementation patterns.

ÊëòË¶ÅÔºöÂú®ÊµÅÁ®ãÊåñÊéò‰∏≠ÔºåÊó•ÂøóÊé¢Á¥¢Ê≠•È™§ÂèØ‰ª•ÁêÜËß£‰∫ã‰ª∂ËΩ®ËøπÔºõ‰æãÂ¶ÇÔºåËØÜÂà´‰∫ã‰ª∂Ê®°ÂºèÂíåÈùûÈÄªËæëËΩ®ËøπÔºåÂπ∂Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÂèØÂèòÊÄß„ÄÇ‰∏∫‰∫ÜÊîØÊåÅË°®ËææÊÄßÊó•ÂøóÊé¢Á¥¢Ôºå‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•ËΩ¨Êç¢‰∏∫Áü•ËØÜÂõæ (KG)ÔºåÁÑ∂ÂêéÂèØ‰ª•‰ΩøÁî®ÈÄöÁî®ËØ≠Ë®ÄÂØπÂÖ∂ËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Êé¢Á¥¢‰ΩøÁî®ËµÑÊ∫êÊèèËø∞Ê°ÜÊû∂ (RDF) ‰Ωú‰∏∫Êï∞ÊçÆÊ®°ÂûãÂàõÂª∫ËØ≠‰πâ KGÔºåÂπ∂ÁªìÂêàÈÄöÁî® Notation3 (N3) ËßÑÂàôËØ≠Ë®ÄËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® N3 ÂÆûÁé∞ÂèóÁé∞ÊúâÊäÄÊúØÂêØÂèëÁöÑÂÖ∏ÂûãËΩ®ËøπÊü•ËØ¢Á∫¶Êùü„ÄÇÊàë‰ª¨Â∞ÜÊ°à‰æãÂíåÂØπË±°‰∏≠ÂøÉ‰∫ã‰ª∂Êó•ÂøóËΩ¨Êç¢‰∏∫Âü∫‰∫éËΩ®ËøπÁöÑËØ≠‰πâ KGÔºõOCEL2 Êó•ÂøóÂú®Ê≠§Ë¢´‚ÄúÊâÅÂπ≥Âåñ‚Äù‰∏∫Âü∫‰∫éÈÄöËøá KG ÁöÑÂØπË±°Ë∑ØÂæÑÁöÑËΩ®Ëøπ„ÄÇÊ≠§Ëß£ÂÜ≥ÊñπÊ°àÊèê‰æõ (a) Ë°®ËææÂäõÔºåÂõ†‰∏∫Êü•ËØ¢ÂèØ‰ª•‰ª•Â§öÁßçÊñπÂºèÂÆû‰æãÂåñÁ∫¶ÊùüÂπ∂‰ªªÊÑèÁ∫¶ÊùüÂ±ûÊÄßÂíåÂÖ≥Á≥ªÔºà‰æãÂ¶ÇÔºåÂèÇ‰∏éËÄÖ„ÄÅËµÑÊ∫êÔºâÔºõ(b) ÁÅµÊ¥ªÔºåÂõ†‰∏∫ OCEL2 ‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•Âü∫‰∫é KG ‰ª•‰ªªÊÑèÊñπÂºèÂ∫èÂàóÂåñ‰∏∫ËΩ®ËøπÔºõ‰ª•Âèä (c) ÂèØÊâ©Â±ïÊÄßÔºåÂõ†‰∏∫ÂÖ∂‰ªñ‰∫∫ÂèØ‰ª•ÈÄöËøáÂà©Áî®Áõ∏ÂêåÁöÑÂÆûÁé∞Ê®°ÂºèÊù•Êâ©Â±ïÊàë‰ª¨ÁöÑÂ∫ì„ÄÇ

##### **PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**
2409.00092v1 by Runtao Ren, Jian Ma

As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.

ÊëòË¶ÅÔºö<paragraph>Èö®Ëëó‰∫∫È°ûÈÇÅÂÖ•ÁßëÊäÄÂâµÊñ∞ÁöÑÊñ∞Á¥ÄÂÖÉÔºåËøÖÈÄüÂ∞áÂâµÊÑèÈªûÂ≠êËΩâÂåñÁÇ∫Âèó‰øùË≠∑ÁöÑÊô∫ÊÖßË≤°Áî¢ÔºàIPÔºâÁöÑËÉΩÂäõÊØî‰ª•ÂæÄ‰ªª‰ΩïÊôÇÂÄôÈÉΩÊõ¥Âä†ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂ∞àÂà©Ëµ∑ËçâÁ®ãÂ∫èÂÖÖÊªøÊåëÊà∞ÔºåÈúÄË¶ÅÂ∞çÂÖàÈÄ≤È†òÂüüÁü•Ë≠òÂíåÊäÄË°ìÊ¶ÇÂøµÊúâÁ¥∞Á∑ªÂÖ•ÂæÆÁöÑ‰∫ÜËß£„ÄÇÁèæÊúâÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÈõñÁÑ∂Âº∑Â§ßÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÁî¢ÁîüÊäÄË°ì‰∏äÊ∫ñÁ¢∫ÁöÑÂ∞àÂà©Êñá‰ª∂ÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÊÉÖÂ¢ÉÊÑèË≠òÔºåÂõ†Ê≠§Â∏∏Â∏∏ÁÑ°Ê≥ïÊªøË∂≥Ê≠§ IP Ââµ‰ΩúÈ†òÂüüÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÈóúÈçµÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑ LLM Áü•Ë≠òÂæÆË™ø (KFT) Êû∂ÊßãÔºåÊó®Âú®Ë≥¶‰∫à AI Ëá™‰∏ªÊåñÊéò„ÄÅÁêÜËß£ÂíåÊáâÁî®ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã PatentGPT ÂÖÖÂàÜÂà©Áî®‰∫ÜÂü∫ÊñºÁü•Ë≠òÂúñË°®ÁöÑÈ†êË®ìÁ∑¥„ÄÅÁâπÂÆöÈ†òÂüüÁöÑÁõ£Áù£ÂºèÂæÆË™ø (SFT) Âíå‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF) ÁöÑÁç®ÁâπÁµÑÂêà„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑË©ï‰º∞ÔºåPatentGPT Â∑≤Â±ïÁèæÂá∫ÂÇëÂá∫ÁöÑË°®ÁèæÔºåÂú®ËàáÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏ÊØîÁöÑÂ∞àÂà©Áõ∏ÈóúÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂæóÂàÜÈ´òÂá∫Á¥Ñ 400%„ÄÇÈÄèÈÅé KFT ÊñπÊ≥ïÔºåÊ≠§Ê®°Âûã‰∏çÂÉÖËÉΩÂ§†ÂçîÂä©ÔºåÈÇÑËÉΩÊì¥Â¢û‰∫∫È°ûÁöÑÂâµÈÄ†ÂäõÂíåÂâµÊñ∞ÂäõÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁÇ∫ AI È©ÖÂãïÁöÑÊô∫ÊÖßË≤°Áî¢ÁîüÊàêÊ®πÁ´ã‰∫ÜÊñ∞Ê®ôÊ∫ñÔºåÁÇ∫Êõ¥ÊúâÊïàÁéá‰∏îÊõ¥ÊúâÊïàÁöÑÁôºÊòéÊµÅÁ®ãÈã™Ë∑Ø„ÄÇ</paragraph>

##### **DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**
2408.14185v1 by Ziai Zhou, Bin Zhou, Hao Liu

Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning, balancing global and local optimality. We
first use the static Dijkstra algorithm to compute a globally optimal baseline
path. A distributed control strategy then guides vehicles along this path. At
intersections, DynamicRouteGPT performs real-time decision-making for local
path selection, considering real-time traffic, driving preferences, and
unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian
inference, and large-scale pretrained language models like Llama3 8B to provide
an efficient path planning solution. It dynamically adjusts to traffic
scenarios and driver preferences and requires no pre-training, offering broad
applicability across road networks. A key innovation is the construction of
causal graphs for counterfactual reasoning, optimizing path decisions.
Experimental results show that our method achieves state-of-the-art performance
in real-time dynamic path planning for multiple vehicles while providing
explainable path selections, offering a novel and efficient solution for
complex traffic environments.

ÊëòË¶ÅÔºöÂú®Ë§áÈõú‰∫§ÈÄöÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÊúÉÈù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶Ç‰∫§ÈÄöÊµÅÈáèËÆäÂåñÂíå‰ø°ËôüÁ≠âÂæÖÊôÇÈñì„ÄÇÂÇ≥Áµ±ÁöÑÈùúÊÖãË∑ØÁî±ÊºîÁÆóÊ≥ïÔºå‰æãÂ¶Ç Dijkstra Âíå A*ÔºåÊúÉË®àÁÆóÊúÄÁü≠Ë∑ØÂæëÔºå‰ΩÜÈÄöÂ∏∏Âú®ÂãïÊÖãÊ¢ù‰ª∂‰∏ãÊúÉÂ§±Êïó„ÄÇÊúÄËøëÁöÑÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊîπÈÄ≤Ôºå‰ΩÜÂÇæÂêëÊñºÈóúÊ≥®Â±ÄÈÉ®ÊúÄÂÑ™ÔºåÂÜíËëóÈô∑ÂÖ•Ê≠ªËÉ°ÂêåÊàñÈÇäÁïåÂïèÈ°åÁöÑÈ¢®Èö™„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂõ†ÊûúÊé®Ë´ñÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÁî®ÊñºÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÔºåÂπ≥Ë°°ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÊúÄÂÑ™ÊÄß„ÄÇÊàëÂÄëÈ¶ñÂÖà‰ΩøÁî®ÈùúÊÖã Dijkstra ÊºîÁÆóÊ≥ïË®àÁÆóÂÖ®Â±ÄÊúÄÂÑ™Âü∫Á∑öË∑ØÂæë„ÄÇÁÑ∂ÂæåÔºå‰∏ÄÂÄãÂàÜÂ∏ÉÂºèÊéßÂà∂Á≠ñÁï•Ê≤øËëóÈÄôÊ¢ùË∑ØÂæëÂºïÂ∞éËªäËºõ„ÄÇÂú®‰∫§ÂèâË∑ØÂè£ÔºåDynamicRouteGPT ÈáùÂ∞çÂ±ÄÈÉ®Ë∑ØÂæëÈÅ∏ÊìáÂü∑Ë°åÂØ¶ÊôÇÊ±∫Á≠ñÔºåËÄÉÈáèÂØ¶ÊôÇ‰∫§ÈÄö„ÄÅÈßïÈßõÂÅèÂ•ΩÂíåÊÑèÂ§ñ‰∫ã‰ª∂„ÄÇDynamicRouteGPT Êï¥Âêà‰∫ÜÈ¶¨ÂèØÂ§´Èèà„ÄÅË≤ùÊ∞èÊé®Ë´ñÂíå Llama3 8B Á≠âÂ§ßË¶èÊ®°È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºå‰ª•Êèê‰æõÊúâÊïàÁöÑË∑ØÂæëË¶èÂäÉËß£Ê±∫ÊñπÊ°à„ÄÇÂÆÉÊúÉÂãïÊÖãË™øÊï¥Âà∞‰∫§ÈÄöÁãÄÊ≥ÅÂíåÈßïÈßõÂÅèÂ•ΩÔºå‰∏¶‰∏î‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÔºåÂú®ÈÅìË∑ØÁ∂≤Ë∑Ø‰∏äÊèê‰æõÂª£Ê≥õÁöÑÈÅ©Áî®ÊÄß„ÄÇ‰∏ÄÂÄãÈóúÈçµÂâµÊñ∞ÊòØÂª∫Á´ãÂèç‰∫ãÂØ¶Êé®ÁêÜÁöÑÂõ†ÊûúÂúñÔºå‰ª•ÊúÄ‰Ω≥ÂåñË∑ØÂæëÊ±∫Á≠ñ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â§öËºõËªäËºõÁöÑÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉ‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇÊèê‰æõÂèØËß£ÈáãÁöÑË∑ØÂæëÈÅ∏ÊìáÔºåÁÇ∫Ë§áÈõúÁöÑ‰∫§ÈÄöÁí∞Â¢ÉÊèê‰æõ‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Exploring the Potential of Large Language Models for Heterophilic Graphs**
2408.14134v1 by Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi

Graph Neural Networks (GNNs) are essential for various graph-based learning
tasks. Notably, classical GNN architectures operate under the assumption of
homophily, which posits that connected nodes are likely to share similar
features. However, this assumption limits the effectiveness of GNNs in handling
heterophilic graphs where connected nodes often exhibit dissimilar
characteristics. Existing approaches for homophily graphs such as non-local
neighbor extension and architectural refinement overlook the rich textual data
associated with nodes, which could unlock deeper insights into these
heterophilic contexts. With advancements in Large Language Models (LLMs), there
is significant promise to enhance GNNs by leveraging the extensive open-world
knowledge within LLMs to more effectively interpret and utilize textual data
for characterizing heterophilic graphs. In this work, we explore the potential
of LLMs for modeling heterophilic graphs and propose a novel two-stage
framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.
Specifically, in the first stage, we fine-tune the LLM to better identify
homophilic and heterophilic edges based on the textual information of their
nodes. In the second stage, we adaptively manage message propagation in GNNs
for different edge types based on node features, structures, and heterophilic
or homophilic characteristics. To cope with the computational demands when
deploying LLMs in practical scenarios, we further explore model distillation
techniques to fine-tune smaller, more efficient models that maintain
competitive performance. Extensive experiments validate the effectiveness of
our framework, demonstrating the feasibility of using LLMs to enhance GNNs for
node classification on heterophilic graphs.

ÊëòË¶ÅÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞çÊñºÂêÑÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÂ≠∏Áøí‰ªªÂãôËá≥ÈóúÈáçË¶Å„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂÇ≥Áµ±ÁöÑ GNN Êû∂ÊßãÂú®ÂêåË≥™ÊÄßÁöÑÂÅáË®≠‰∏ãÈÅã‰ΩúÔºåË©≤ÂÅáË®≠Ë™çÁÇ∫ÈÄ£Êé•ÁöÑÁØÄÈªûÂèØËÉΩÂÖ±‰∫´È°û‰ººÁöÑÁâπÂæµ„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÂÅáË®≠ÈôêÂà∂‰∫Ü GNN Âú®ËôïÁêÜÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏≠ÁöÑÊïàËÉΩÔºåÂÖ∂‰∏≠ÈÄ£Êé•ÁöÑÁØÄÈªûÈÄöÂ∏∏Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÁâπÂæµ„ÄÇÁèæÊúâÁöÑÂêåË≥™ÊÄßÂúñÂΩ¢ÊñπÊ≥ïÔºà‰æãÂ¶ÇÈùûÂ±ÄÈÉ®ÈÑ∞ÂüüÂª∂‰º∏ÂíåÊû∂ÊßãÊîπÈÄ≤ÔºâÂøΩÁï•‰∫ÜËàáÁØÄÈªûÁõ∏ÈóúÁöÑË±êÂØåÊñáÊú¨Ë≥áÊñôÔºåÈÄôÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£ÈÄô‰∫õÁï∞Ë≥™ÊÄßËÑàÁµ°„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÈÄèÈÅéÂà©Áî® LLM ‰∏≠Âª£Ê≥õÁöÑÈñãÊîæ‰∏ñÁïåÁü•Ë≠ò‰æÜÂ¢ûÂº∑ GNNÔºåÂ∞çÊñºÊõ¥ÊúâÊïàÂú∞Ë©ÆÈáãÂíåÂà©Áî®ÊñáÊú¨Ë≥áÊñô‰æÜË°®ÂæµÁï∞Ë≥™ÊÄßÂúñÂΩ¢ÊúâÂæàÂ§ßÁöÑÂ∏åÊúõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®Áï∞Ë≥™ÊÄßÂúñÂΩ¢Âª∫Ê®°‰∏≠ÁöÑÊΩõÂäõÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ©ÈöéÊÆµÊû∂ÊßãÔºöLLM Â¢ûÂº∑ÈÇäÁ∑£Âà§Âà•Âô®Âíå LLM ÂºïÂ∞éÈÇäÁ∑£ÈáçÊñ∞Âä†Ê¨ä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂú®Á¨¨‰∏ÄÈöéÊÆµÔºåÊàëÂÄëÂæÆË™ø LLM ‰ª•Ê†πÊìöÂÖ∂ÁØÄÈªûÁöÑÊñáÊú¨Ë≥áË®äÔºåÊõ¥Â•ΩÂú∞Ë≠òÂà•ÂêåË≥™ÊÄßÂíåÁï∞Ë≥™ÊÄßÈÇäÁ∑£„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊàëÂÄëÊ†πÊìöÁØÄÈªûÁâπÂæµ„ÄÅÁµêÊßãÂíåÁï∞Ë≥™ÊÄßÊàñÂêåË≥™ÊÄßÁâπÂæµÔºåËá™ÈÅ©ÊáâÂú∞ÁÆ°ÁêÜ GNN ‰∏≠‰∏çÂêåÈÇäÁ∑£È°ûÂûãÁöÑË®äÊÅØÂÇ≥ÈÅû„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂú®ÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÈÉ®ÁΩ≤ LLM ÊôÇÁöÑË®àÁÆóÈúÄÊ±ÇÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÊ®°ÂûãËêÉÂèñÊäÄË°ìÔºå‰ª•ÂæÆË™øËºÉÂ∞è„ÄÅÊõ¥ÊúâÊïàÁéáÁöÑÊ®°ÂûãÔºå‰ª•Á∂≠ÊåÅÁ´∂Áà≠Âäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫Ü‰ΩøÁî® LLM ‰æÜÂ¢ûÂº∑ GNN ‰ª•ÈÄ≤Ë°åÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏äÁöÑÁØÄÈªûÂàÜÈ°ûÁöÑÂèØË°åÊÄß„ÄÇ

##### **Towards Graph Prompt Learning: A Survey and Beyond**
2408.14520v2 by Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou

Large-scale "pre-train and prompt learning" paradigms have demonstrated
remarkable adaptability, enabling broad applications across diverse domains
such as question answering, image recognition, and multimodal retrieval. This
approach fully leverages the potential of large-scale pre-trained models,
reducing downstream data requirements and computational costs while enhancing
model applicability across various tasks. Graphs, as versatile data structures
that capture relationships between entities, play pivotal roles in fields such
as social network analysis, recommender systems, and biological graphs. Despite
the success of pre-train and prompt learning paradigms in Natural Language
Processing (NLP) and Computer Vision (CV), their application in graph domains
remains nascent. In graph-structured data, not only do the node and edge
features often have disparate distributions, but the topological structures
also differ significantly. This diversity in graph data can lead to
incompatible patterns or gaps between pre-training and fine-tuning on
downstream graphs. We aim to bridge this gap by summarizing methods for
alleviating these disparities. This includes exploring prompt design
methodologies, comparing related techniques, assessing application scenarios
and datasets, and identifying unresolved problems and challenges. This survey
categorizes over 100 relevant works in this field, summarizing general design
principles and the latest applications, including text-attributed graphs,
molecules, proteins, and recommendation systems. Through this extensive review,
we provide a foundational understanding of graph prompt learning, aiming to
impact not only the graph mining community but also the broader Artificial
General Intelligence (AGI) community.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°„ÄåÈ†êË®ìÁ∑¥ÂíåÊèêÁ§∫Â≠∏Áøí„ÄçÁØÑ‰æãÂ∑≤Â±ïÁèæÂá∫ÈùûÂá°ÁöÑÈÅ©ÊáâÂäõÔºåËÉΩÂª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®ÆÈ†òÂüüÔºå‰æãÂ¶ÇÂïèÁ≠î„ÄÅÂΩ±ÂÉèËæ®Ë≠òÂíåÂ§öÊ®°ÊÖãÊ™¢Á¥¢„ÄÇÊ≠§ÊñπÊ≥ïÂÖÖÂàÜÁôºÊèÆÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÊΩõÂäõÔºåÊ∏õÂ∞ë‰∏ãÊ∏∏Ë≥áÊñôÈúÄÊ±ÇÂíåÈÅãÁÆóÊàêÊú¨ÔºåÂêåÊôÇÊèêÂçáÊ®°ÂûãÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÂúñÂΩ¢‰ΩúÁÇ∫ËÉΩÊçïÊçâÂØ¶È´î‰πãÈñìÈóú‰øÇÁöÑÂ§öÂäüËÉΩË≥áÊñôÁµêÊßãÔºåÂú®Á§æÁæ§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÊé®Ëñ¶Á≥ªÁµ±ÂíåÁîüÁâ©ÂúñÂΩ¢Á≠âÈ†òÂüüÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÑòÁÆ°È†êË®ìÁ∑¥ÂíåÊèêÁ§∫Â≠∏ÁøíÁØÑ‰æãÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÈõªËÖ¶Ë¶ñË¶∫ (CV) ‰∏≠Áç≤ÂæóÊàêÂäüÔºå‰ΩÜÂÆÉÂÄëÂú®ÂúñÂΩ¢È†òÂüüÁöÑÊáâÁî®‰ªçËôïÊñºËµ∑Ê≠•ÈöéÊÆµ„ÄÇÂú®ÂúñÂΩ¢ÁµêÊßãÂåñË≥áÊñô‰∏≠ÔºåÁØÄÈªûÂíåÈÇäÁ∑£ÁâπÂæµ‰∏çÂÉÖÂ∏∏Êúâ‰∏çÂêåÁöÑÂàÜ‰ΩàÔºåÊãìÊí≤ÁµêÊßã‰πüÂ∑ÆÁï∞ÂæàÂ§ß„ÄÇÂúñÂΩ¢Ë≥áÊñô‰∏≠ÁöÑÈÄôÁ®ÆÂ§öÊ®£ÊÄßÂèØËÉΩÂ∞éËá¥È†êË®ìÁ∑¥ÂíåÂæÆË™ø‰πãÈñìÂá∫Áèæ‰∏çÁõ∏ÂÆπÁöÑÊ®°ÂºèÊàñÂ∑ÆË∑ù„ÄÇÊàëÂÄëÊó®Âú®ÈÄèÈÅéÁ∏ΩÁµêÊ∏õËºïÈÄô‰∫õÂ∑ÆÁï∞ÁöÑÊñπÊ≥ï‰æÜÂΩåË£úÊ≠§Â∑ÆË∑ù„ÄÇÈÄôÂåÖÊã¨Êé¢Á¥¢ÊèêÁ§∫Ë®≠Ë®àÊñπÊ≥ï„ÄÅÊØîËºÉÁõ∏ÈóúÊäÄË°ì„ÄÅË©ï‰º∞ÊáâÁî®Â†¥ÊôØÂíåË≥áÊñôÈõÜÔºå‰ª•ÂèäÊâæÂá∫Êú™Ëß£Ê±∫ÁöÑÂïèÈ°åÂíåÊåëÊà∞„ÄÇÊú¨Ë™øÊü•Ê≠∏È°û‰∫ÜÊ≠§È†òÂüü‰∏≠Ë∂ÖÈÅé 100 ÁØáÁõ∏Èóú‰ΩúÂìÅÔºåÁ∏ΩÁµê‰∫Ü‰∏ÄËà¨Ë®≠Ë®àÂéüÂâáÂíåÊúÄÊñ∞ÊáâÁî®ÔºåÂåÖÊã¨ÊñáÂ≠óÂ±¨ÊÄßÂúñÂΩ¢„ÄÅÂàÜÂ≠ê„ÄÅËõãÁôΩË≥™ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÈÄèÈÅéÈÄôÈ†ÖÂª£Ê≥õÁöÑÂõûÈ°ßÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁöÑÂü∫Êú¨ÁêÜËß£ÔºåÊó®Âú®‰∏çÂÉÖÂΩ±ÈüøÂúñÂΩ¢ÊåñÊéòÁ§æÁæ§Ôºå‰πüÂΩ±ÈüøÊõ¥Âª£Ê≥õÁöÑ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) Á§æÁæ§„ÄÇ</paragraph>

##### **CodeGraph: Enhancing Graph Reasoning of LLMs with Code**
2408.13863v1 by Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song

With the increasing popularity of large language models (LLMs), reasoning on
basic graph algorithm problems is an essential intermediate step in assessing
their abilities to process and infer complex graph reasoning tasks. Existing
methods usually convert graph-structured data to textual descriptions and then
use LLMs for reasoning and computation. However, LLMs often produce computation
errors on arithmetic parts in basic graph algorithm problems, such as counting
number of edges. In addition, they struggle to control or understand the output
of the reasoning process, raising concerns about whether LLMs are simply
guessing. In this paper, we introduce CodeGraph, a method that encodes graph
problem solutions as code. The methods solve new graph problems by learning
from exemplars, generating programs, and executing them via a program
interpreter. Using the few-shot setting, we evaluate CodeGraph with the base
LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and
Mixtral-8x7B Instruct. Experimental results on six tasks with six graph
encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost
performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on
the task. Compared to the existing methods, CodeGraph demonstrates strong
performance on arithmetic problems in graph tasks and offers a more
controllable and interpretable approach to the reasoning process.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊó•Êº∏ÊôÆÂèäÔºåÂ∞çÂü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°åÈÄ≤Ë°åÊé®ÁêÜÊòØË©ï‰º∞ÂÆÉÂÄëËôïÁêÜÂíåÊé®Ë´ñË§áÈõúÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑËÉΩÂäõ‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑ‰∏≠ÈñìÊ≠•È©ü„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂ∞áÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñôËΩâÊèõÊàêÊñáÂ≠óÊèèËø∞ÔºåÁÑ∂Âæå‰ΩøÁî® LLM ÈÄ≤Ë°åÊé®ÁêÜÂíåÈÅãÁÆó„ÄÇÁÑ∂ËÄåÔºåLLM ÈÄöÂ∏∏ÊúÉÂú®Âü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°å‰∏≠Ôºå‰æãÂ¶ÇË®àÁÆóÈÇäÁ∑£Êï∏ÈáèÔºåÂ∞çÁÆóË°ìÈÉ®ÂàÜÁî¢ÁîüÈÅãÁÆóÈåØË™§„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÈõ£‰ª•ÊéßÂà∂ÊàñÁêÜËß£Êé®ÁêÜÈÅéÁ®ãÁöÑËº∏Âá∫ÔºåÈÄôÂºïÁôº‰∫Ü LLM ÊòØÂê¶Âè™ÊòØÂú®ÁåúÊ∏¨ÁöÑÁñëÊÖÆ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CodeGraphÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞áÂúñÂΩ¢ÂïèÈ°åËß£Ê±∫ÊñπÊ°àÁ∑®Á¢ºÁÇ∫Á®ãÂºèÁ¢ºÁöÑÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÈÄèÈÅéÂ≠∏ÁøíÁØÑ‰æã„ÄÅÁî¢ÁîüÁ®ãÂºèÔºå‰∏¶ÈÄèÈÅéÁ®ãÂºèÁ¢ºÁõ¥Ë≠ØÂô®Âü∑Ë°åÂÆÉÂÄë‰æÜËß£Ê±∫Êñ∞ÁöÑÂúñÂΩ¢ÂïèÈ°å„ÄÇ‰ΩøÁî®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆöÔºåÊàëÂÄë‰ΩøÁî®Âü∫Á§é LLM ÁÇ∫ GPT-3.5 Turbo„ÄÅLlama3-70B Instruct„ÄÅMixtral-8x22B Instruct Âíå Mixtral-8x7B Instruct ‰æÜË©ï‰º∞ CodeGraph„ÄÇÂú® GraphQA Ë≥áÊñôÈõÜ‰∏≠‰ΩøÁî®ÂÖ≠Á®ÆÂúñÂΩ¢Á∑®Á¢ºÊñπÊ≥ïÂ∞çÂÖ≠È†Ö‰ªªÂãôÈÄ≤Ë°åÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåCodeGraph ÂèØ‰ª•Â∞á LLM ‰∏≠ÁöÑÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩÊèêÂçá 1.3% Âà∞ 58.6%ÔºåÂÖ∑È´îÂèñÊ±∫Êñº‰ªªÂãô„ÄÇËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCodeGraph Âú®ÂúñÂΩ¢‰ªªÂãô‰∏≠ÁöÑÁÆóË°ìÂïèÈ°å‰∏äË°®ÁèæÂá∫Âº∑ÂãÅÁöÑÊïàËÉΩÔºå‰∏¶ÁÇ∫Êé®ÁêÜÈÅéÁ®ãÊèê‰æõÊõ¥ÂÖ∑ÂèØÊéßÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇ

##### **LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**
2408.14512v1 by Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.

ÊëòË¶ÅÔºöÈõ∂ÁØÑ‰æãÂúñÂΩ¢Ê©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÁî±ÊñºÁ®ÄÊúâÊ®ôÁ±§Ë≥áÊñôÁöÑÊåëÊà∞ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÈõñÁÑ∂Ëá™Áõ£Áù£ÂºèÂ≠∏ÁøíÂíåÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁ≠âÊñπÊ≥ïÂ∑≤Ë¢´Âª£Ê≥õÊé¢Á¥¢Ôºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥Êñº‰ªªÂãôÁâπÂÆöÊ®ôÁ±§ÁöÑÂæÆË™øÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èõ∂ÁØÑ‰æãÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂèóÂà∞Êåá‰ª§ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈõ∂ÁØÑ‰æãÂäüËÉΩÁöÑÂïüÁôºÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ Token Embedding-Aligned Graph Language Model (TEA-GLM) ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ‰ΩúÁÇ∫Ë∑®Ë≥áÊñôÈõÜÂíåË∑®‰ªªÂãôÁöÑÈõ∂ÁØÑ‰æãÂ≠∏ÁøíÂô®ÔºåÁî®ÊñºÂúñÂΩ¢Ê©üÂô®Â≠∏Áøí„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ†êË®ìÁ∑¥‰∏ÄÂÄã GNNÔºåÂ∞áÂÖ∂Ë°®Á§∫Ëàá LLM ÁöÑ token embedding Â∞çÈΩä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®ìÁ∑¥‰∏ÄÂÄãÁ∑öÊÄßÊäïÂΩ±Ê©üÔºåÂ∞á GNN ÁöÑË°®Á§∫ËΩâÊèõÁÇ∫Âõ∫ÂÆöÊï∏ÈáèÁöÑÂúñÂΩ¢ token embeddingÔºåËÄåÁÑ°ÈúÄË™øÊï¥ LLM„ÄÇÁµ±‰∏ÄÁöÑÊåá‰ª§ÊòØÁÇ∫‰∏çÂêåÂ±§Á¥öÁöÑÂêÑÁ®ÆÂúñÂΩ¢‰ªªÂãôË®≠Ë®àÁöÑÔºå‰æãÂ¶ÇÁØÄÈªûÂàÜÈ°ûÔºàÁØÄÈªûÂ±§Á¥öÔºâÂíåÈÄ£ÁµêÈ†êÊ∏¨ÔºàÈÇäÁ∑£Â±§Á¥öÔºâ„ÄÇÈÄô‰∫õË®≠Ë®àÈÅ∏ÊìáÂÖ±ÂêåÂ¢ûÂº∑‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Èõ∂ÁØÑ‰æãÂ≠∏Áøí‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰ΩøÂÖ∂ÊúâÂà•ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÂØ¶È©óË°®ÊòéÔºåËàá‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ÁöÑÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂúñÂΩ¢ token embedding Âπ´Âä© LLM È†êÊ∏¨Âô®Âú®Êú™Ë¶ãÈÅéÁöÑË≥áÊñôÈõÜÂíå‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**
2408.13661v1 by Sakhinana Sagar Srinivas, Geethan Sannidhi, Venkataramana Runkana

Characterizing materials with electron micrographs is a crucial task in
fields such as semiconductors and quantum materials. The complex hierarchical
structure of micrographs often poses challenges for traditional classification
methods. In this study, we propose an innovative backbone architecture for
analyzing electron micrographs. We create multi-modal representations of the
micrographs by tokenizing them into patch sequences and, additionally,
representing them as vision graphs, commonly referred to as patch attributed
graphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered
network structure architecture that facilitates information exchange between
the multi-modal representations and knowledge integration across different
patch resolutions. Furthermore, we leverage large language models (LLMs) to
generate detailed technical descriptions of nanomaterials as auxiliary
information to assist in the downstream task. We utilize a cross-modal
attention mechanism for knowledge fusion across cross-domain
representations(both image-based and linguistic insights) to predict the
nanomaterial category. This multi-faceted approach promises a more
comprehensive and accurate representation and classification of micrographs for
nanomaterial identification. Our framework outperforms traditional methods,
overcoming challenges posed by distributional shifts, and facilitating
high-throughput screening.

ÊëòË¶ÅÔºöÂà©Áî®ÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá‰æÜË°®ÂæµÊùêÊñôÔºåÂú®ÂçäÂ∞éÈ´îÂíåÈáèÂ≠êÊùêÊñôÁ≠âÈ†òÂüü‰∏≠ÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÈ°ØÂæÆÁÖßÁâáË§áÈõúÁöÑÂàÜÂ±§ÁµêÊßãÈÄöÂ∏∏ÊúÉÂ∞çÂÇ≥Áµ±ÂàÜÈ°ûÊñπÊ≥ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑ‰∏ªÂππÊû∂ÊßãÔºåÁî®ÊñºÂàÜÊûêÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÈ°ØÂæÆÁÖßÁâá‰ª£ÊèõÊàêÂçÄÂ°äÂ∫èÂàó‰æÜÂª∫Á´ãÂÖ∂Â§öÊ®°ÊÖãË°®Á§∫ÔºåÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂ∞áÂÖ∂Ë°®Á§∫ÁÇ∫Ë¶ñË¶∫ÂúñÂΩ¢ÔºåÈÄöÂ∏∏Á®±ÁÇ∫ÂçÄÂ°äÂ±¨ÊÄßÂúñÂΩ¢„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂàÜÂ±§Á∂≤Ë∑ØËûçÂêà (HNF)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ§öÂ±§Á∂≤Ë∑ØÁµêÊßãÊû∂ÊßãÔºåÊúâÂä©ÊñºÂ§öÊ®°ÊÖãË°®Á§∫‰πãÈñìÁöÑË≥áË®ä‰∫§ÊèõÔºå‰ª•Âèä‰∏çÂêåÂçÄÂ°äËß£ÊûêÂ∫¶‰πãÈñìÁöÑÁü•Ë≠òÊï¥Âêà„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÂ•àÁ±≥ÊùêÊñôÁöÑË©≥Á¥∞ÊäÄË°ìË™™ÊòéÔºå‰ΩúÁÇ∫ËºîÂä©Ë≥áË®äÔºå‰ª•ÂçîÂä©‰∏ãÊ∏∏‰ªªÂãô„ÄÇÊàëÂÄëÂà©Áî®Ë∑®Ê®°ÊÖãÊ≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂú®Ë∑®È†òÂüüË°®Á§∫ÔºàÂü∫ÊñºÂΩ±ÂÉèÂíåË™ûË®ÄÊ¥ûÂØüÂäõÔºâ‰∏≠ÈÄ≤Ë°åÁü•Ë≠òËûçÂêàÔºå‰ª•È†êÊ∏¨Â•àÁ±≥ÊùêÊñôÈ°ûÂà•„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÂÅöÊ≥ïÊúâÊúõÁÇ∫Â•àÁ±≥ÊùêÊñôË≠òÂà•Êèê‰æõÊõ¥ÂÖ®Èù¢‰∏îÊ∫ñÁ¢∫ÁöÑË°®Á§∫ÂíåÂàÜÈ°û„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÖãÊúç‰∫ÜÂàÜ‰ΩàËΩâÁßªÂ∏∂‰æÜÁöÑÊåëÊà∞Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÈ´òÈÄöÈáèÁØ©ÈÅ∏„ÄÇ

##### **GNN: Graph Neural Network and Large Language Model for Data Discovery**
2408.13609v2 by Thomas Hoang

Our algorithm GNN: Graph Neural Network and Large Language Model for Data
Discovery inherit the benefits of \cite{hoang2024plod} (PLOD: Predictive
Learning Optimal Data Discovery), \cite{Hoang2024BODBO} (BOD: Blindly Optimal
Data Discovery) in terms of overcoming the challenges of having to predefine
utility function and the human input for attribute ranking, which helps prevent
the time-consuming loop process. In addition to these previous works, our
algorithm GNN leverages the advantages of graph neural networks and large
language models to understand text type values that cannot be understood by
PLOD and MOD, thus making the task of predicting outcomes more reliable. GNN
could be seen as an extension of PLOD in terms of understanding the text type
value and the user's preferences, not only numerical values but also text
values, making the promise of data science and analytics purposes.

ÊëòË¶ÅÔºöÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNNÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÔºåÁî®ÊñºË≥áÊñôÊé¢Á¥¢ÔºåÁπºÊâø‰∫Ü \cite{hoang2024plod}ÔºàPLODÔºöÈ†êÊ∏¨ÊÄßÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢Ôºâ„ÄÅ\cite{Hoang2024BODBO}ÔºàBODÔºöÁõ≤ÁõÆÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢ÔºâÁöÑÂÑ™ÈªûÔºåÂú®ÊñºÂÖãÊúçÂøÖÈ†àÈ†êÂÖàÂÆöÁæ©ÊïàÁî®ÂáΩÊï∏Âíå‰∫∫È°ûËº∏ÂÖ•Â±¨ÊÄßÊéíÂêçÁöÑÊåëÊà∞ÔºåÈÄôÊúâÂä©ÊñºÈò≤Ê≠¢ËÄóÊôÇÁöÑËø¥ÂúàËôïÁêÜ„ÄÇÈô§‰∫ÜÈÄô‰∫õÂÖàÂâçÁöÑ‰ΩúÂìÅÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNN Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁöÑÂÑ™ÈªûÔºå‰æÜÁêÜËß£ PLOD Âíå MOD ÁÑ°Ê≥ïÁêÜËß£ÁöÑÊñáÂ≠óÈ°ûÂûãÂÄºÔºåÂæûËÄå‰ΩøÈ†êÊ∏¨ÁµêÊûúÁöÑ‰ªªÂãôÊõ¥ÂèØÈù†„ÄÇGNN ÂèØ‰ª•Ë¶ñÁÇ∫ PLOD Âú®ÁêÜËß£ÊñáÂ≠óÈ°ûÂûãÂÄºÂíå‰ΩøÁî®ËÄÖÂÅèÂ•ΩÊñπÈù¢ÁöÑÂª∂‰º∏Ôºå‰∏çÂÉÖÊòØÊï∏ÂÄºÔºåÈÇÑÊúâÊñáÂ≠óÂÄºÔºåÈÄôÂØ¶Áèæ‰∫ÜË≥áÊñôÁßëÂ≠∏ÂíåÂàÜÊûêÁõÆÁöÑÁöÑÊâøË´æ„ÄÇ

##### **HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**
2408.13521v1 by Azmine Toushik Wasi

Knowledge Graphs (KGs) serving as semantic networks, prove highly effective
in managing complex interconnected data in different domains, by offering a
unified, contextualized, and structured representation with flexibility that
allows for easy adaptation to evolving knowledge. Processing complex Human
Resources (HR) data, KGs can help in different HR functions like recruitment,
job matching, identifying learning gaps, and enhancing employee retention.
Despite their potential, limited efforts have been made to implement practical
HR knowledge graphs. This study addresses this gap by presenting a framework
for effectively developing HR knowledge graphs from documents using Large
Language Models. The resulting KG can be used for a variety of downstream
tasks, including job matching, identifying employee skill gaps, and many more.
In this work, we showcase instances where HR KGs prove instrumental in precise
job matching, yielding advantages for both employers and employees. Empirical
evidence from experiments with information propagation in KGs and Graph Neural
Nets, along with case studies underscores the effectiveness of KGs in tasks
such as job and employee recommendations and job area classification. Code and
data are available at : https://github.com/azminewasi/HRGraph

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Ë™ûÁæ©Á∂≤Ë∑ØÔºåË≠âÊòéÂú®ÁÆ°ÁêÜ‰∏çÂêåÈ†òÂüü‰∏≠Ë§áÈõúÁöÑ‰∫íÈÄ£Ë≥áÊñôÊñπÈù¢ÈùûÂ∏∏ÊúâÊïàÔºåÈÄèÈÅéÊèê‰æõÁµ±‰∏Ä„ÄÅËÑàÁµ°Âåñ‰∏îÁµêÊßãÂåñÁöÑË°®Á§∫Ôºå‰∏¶ÂÖ∑ÂÇôÈùàÊ¥ªÊÄßÔºåÂèØËºïÈ¨ÜÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑÁü•Ë≠ò„ÄÇKG ËôïÁêÜË§áÈõúÁöÑ‰∫∫ÂäõË≥áÊ∫ê (HR) Ë≥áÊñôÔºåÊúâÂä©Êñº‰∏çÂêåÁöÑ HR ÂäüËÉΩÔºå‰æãÂ¶ÇÊãõÂãü„ÄÅÂ∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Â≠∏ÁøíÂ∑ÆË∑ùÂíåÊèêÂçáÂì°Â∑•ÁïôÂ≠òÁéá„ÄÇÂÑòÁÆ°ÊúâÂÖ∂ÊΩõÂäõÔºå‰ΩÜÂØ¶‰ΩúÂØ¶Áî®ÁöÑ HR Áü•Ë≠òÂúñË≠úÁöÑÂä™ÂäõÊúâÈôê„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄãÊû∂ÊßãÔºåÂæûÊñá‰ª∂‰∏≠‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊúâÊïàÈñãÁôº HR Áü•Ë≠òÂúñË≠úÔºå‰æÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÁî¢ÁîüÁöÑ KG ÂèØÁî®ÊñºÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãôÔºåÂåÖÊã¨Â∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Âì°Â∑•ÊäÄËÉΩÂ∑ÆË∑ùÁ≠â„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü HR KG Âú®Á≤æÁ¢∫Â∑•‰ΩúÂåπÈÖç‰∏≠Ë≠âÊòéÊúâÁî®ÁöÑÁØÑ‰æãÔºåÁÇ∫Èõá‰∏ªÂíåÂì°Â∑•Â∏∂‰æÜÂÑ™Âã¢„ÄÇÈÄèÈÅé KG ÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠Ë≥áË®äÂÇ≥Êí≠ÁöÑÂØ¶È©óÊâÄÂæóÁöÑÂØ¶Ë≠âÔºå‰ª•ÂèäÊ°à‰æãÁ†îÁ©∂ÔºåÂº∑Ë™ø‰∫Ü KG Âú®Â∑•‰ΩúÂíåÂì°Â∑•Êé®Ëñ¶‰ª•ÂèäÂ∑•‰ΩúÈ†òÂüüÂàÜÈ°ûÁ≠â‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/azminewasi/HRGraph

##### **Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**
2408.13432v1 by Yi-Hui Chen, Eric Jui-Lin Lu, Kwan-Ho Cheng

The main task of the KGQA system (Knowledge Graph Question Answering) is to
convert user input questions into query syntax (such as SPARQL). With the rise
of modern popular encoders and decoders like Transformer and ConvS2S, many
scholars have shifted the research direction of SPARQL generation to the Neural
Machine Translation (NMT) architecture or the generative AI field of
Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query
syntax as a language. It uses NMT-based translation models to translate natural
language questions into query syntax. Scholars use popular architectures
equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to
train translation models for query syntax. To achieve better query results,
this paper improved the ConvS2S encoder and added multi-head attention from the
Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the
n-gram language model. The principle is to use convolutional layers to capture
local hidden features in the input sequence with different receptive fields,
using multi-head attention to calculate dependencies between them. Ultimately,
we found that the translation model based on the Multi-Head Conv encoder
achieved better performance than other encoders, obtaining 76.52\% and 83.37\%
BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0
datasets, respectively. Additionally, in the end-to-end system experiments on
the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other
KGQA systems, with Macro F1-measures reaching 52\% and 66\%, respectively.
Moreover, the experimental results show that with limited computational
resources, if one possesses an excellent encoder-decoder architecture and
cross-attention, experts and scholars can achieve outstanding performance
equivalent to large pre-trained models using only general embeddings.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË°®ÂïèÁ≠îÁ≥ªÁµ± (KGQA) ÁöÑ‰∏ªË¶Å‰ªªÂãôÊòØÂ∞á‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï (‰æãÂ¶Ç SPARQL)„ÄÇÈö®Ëëó Transformer Âíå ConvS2S Á≠âÁèæ‰ª£ÊµÅË°åÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÁöÑÂ¥õËµ∑ÔºåË®±Â§öÂ≠∏ËÄÖÂ∑≤Â∞á SPARQL ÁîüÊàêÁöÑÁ†îÁ©∂ÊñπÂêëËΩâÁßªÂà∞Á•ûÁ∂ìÊ©üÂô®ÁøªË≠Ø (NMT) Êû∂ÊßãÊàñÊñáÂ≠óËΩâ SPARQL ÁöÑÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüü„ÄÇÂú®Âü∫Êñº NMT ÁöÑÂïèÁ≠îÁ≥ªÁµ±‰∏≠ÔºåÁ≥ªÁµ±Â∞áÁü•Ë≠òÂ∫´Êü•Ë©¢Ë™ûÊ≥ïË¶ñÁÇ∫‰∏ÄÁ®ÆË™ûË®Ä„ÄÇÂÆÉ‰ΩøÁî®Âü∫Êñº NMT ÁöÑÁøªË≠ØÊ®°ÂûãÂ∞áËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï„ÄÇÂ≠∏ËÄÖ‰ΩøÁî®ÈÖçÂÇôË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÁÜ±ÈñÄÊû∂ÊßãÔºå‰æãÂ¶Ç Transformer„ÄÅConvS2S Âíå BiLSTMÔºå‰æÜË®ìÁ∑¥Êü•Ë©¢Ë™ûÊ≥ïÁöÑÁøªË≠ØÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÁç≤ÂæóÊõ¥Â•ΩÁöÑÊü•Ë©¢ÁµêÊûúÔºåÊú¨ÊñáÊîπÈÄ≤‰∫Ü ConvS2S Á∑®Á¢ºÂô®Ôºå‰∏¶Âæû Transformer ‰∏≠Âä†ÂÖ•Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº n-gram Ë™ûË®ÄÊ®°ÂûãÁöÑÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô® (MHC Á∑®Á¢ºÂô®)„ÄÇÂÖ∂ÂéüÁêÜÊòØ‰ΩøÁî®Âç∑Á©çÂ±§‰ª•‰∏çÂêåÁöÑÊÑüÂèóÈáéÊì∑ÂèñËº∏ÂÖ•Â∫èÂàó‰∏≠ÁöÑÂ±ÄÈÉ®Èö±ËóèÁâπÂæµÔºå‰∏¶‰ΩøÁî®Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂Ë®àÁÆóÂÆÉÂÄë‰πãÈñìÁöÑ‰æùË≥¥Èóú‰øÇ„ÄÇÊúÄÁµÇÔºåÊàëÂÄëÁôºÁèæÂü∫ÊñºÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô®ÁöÑÁøªË≠ØÊ®°ÂûãÊØîÂÖ∂‰ªñÁ∑®Á¢ºÂô®Áç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩÔºåÂàÜÂà•Âú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜ‰∏äÁç≤Âæó 76.52% Âíå 83.37% ÁöÑ BLEU-1ÔºàÈõôË™ûË©ï‰º∞Á†îÁ©∂ÔºâÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÂú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜÁöÑÁ´ØÂà∞Á´ØÁ≥ªÁµ±ÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÂú®ÂÖ∂‰ªñ KGQA Á≥ªÁµ±‰∏≠ÂèñÂæó‰∫ÜÈ†òÂÖàÁöÑÁµêÊûúÔºåÂ∑®ËßÄ F1 Ê∏¨ÈáèÂÄºÂàÜÂà•ÈÅîÂà∞ 52% Âíå 66%„ÄÇÊ≠§Â§ñÔºåÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂ¶ÇÊûúÊìÅÊúâÂá∫Ëâ≤ÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂ÊßãÂíåË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂç≥‰ΩøÂú®ÈÅãÁÆóË≥áÊ∫êÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞àÂÆ∂ÂíåÂ≠∏ËÄÖ‰ªçÂèØ‰ª•‰ΩøÁî®‰∏ÄËà¨ÁöÑÂµåÂÖ•‰æÜÁç≤ÂæóÁ≠âÂêåÊñºÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇëÂá∫ÊïàËÉΩ„ÄÇ

##### **CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**
2408.13366v1 by Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari

This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.

ÊëòË¶ÅÔºöÊú¨ÁØáË´ñÊñáÊèêÂá∫ CodeRefineÔºå‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÁ†îÁ©∂Ë´ñÊñáÊñπÊ≥ïËá™ÂãïËΩâÊèõÁÇ∫ÂäüËÉΩÁ®ãÂºèÁ¢ºÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÂ§öÊ≠•È©üÊñπÊ≥ïÈ¶ñÂÖàÂæûË´ñÊñá‰∏≠ËêÉÂèñ‰∏¶ÊëòË¶ÅÂá∫ÈóúÈçµÊñáÂ≠óÂçÄÂ°äÔºåÂàÜÊûêÂÖ∂Á®ãÂºèÁ¢ºÁõ∏ÈóúÊÄßÔºå‰∏¶‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÊú¨‰ΩìÂª∫Á´ãÁü•Ë≠òÂúñË≠ú„ÄÇÊé•ËëóÂæûÈÄôÂÄãÁµêÊßãÂåñË°®Á§∫Áî¢ÁîüÁ®ãÂºèÁ¢ºÔºå‰∏¶ÈÄèÈÅéÊèêÂá∫ÁöÑÂõûÊ∫ØÂºèÊ™¢Á¥¢Â¢ûÂº∑Áî¢ÁîüÊñπÊ≥ïÈÄ≤Ë°åÂº∑Âåñ„ÄÇCodeRefine Ëß£Ê±∫‰∫ÜÁêÜË´ñÁ†îÁ©∂ËàáÂØ¶ÈöõÂØ¶‰Ωú‰πãÈñìÁöÑÈ¥ªÊ∫ùÔºåÊèê‰æõÊØî LLM Èõ∂Ê¨°ÊèêÁ§∫Êõ¥Á≤æÁ¢∫ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂú®ÂêÑÁ®ÆÁßëÂ≠∏Ë´ñÊñá‰∏äÁöÑË©ï‰º∞Ë≠âÊòé‰∫Ü CodeRefine ÂæûË´ñÊñáÊîπÂñÑÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÁöÑËÉΩÂäõÔºåÈÄôÊúâÊΩõÂäõÂä†ÈÄüÂ∞ñÁ´ØÊºîÁÆóÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊé°Áî®„ÄÇ

##### **Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**
2408.14494v1 by Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana

We present the Process Engineering Operations Assistant (PEOA), an AI-driven
framework designed to solve complex problems in the chemical and process
industries. The framework employs a modular architecture orchestrated by a
meta-agent, which serves as the central coordinator, managing an action
generator and instruction-tuned small-scale language models (expert models).
The action generator decomposes complex problems into sub-tasks and identifies
suitable expert models to execute each, delivering precise solutions for
multi-step problem-solving. Key techniques include advanced knowledge modeling
using property graphs for improved information retrieval, facilitating more
accurate and contextually relevant solutions. Additionally, the framework
utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to
fine-tune the action generator and expert models for domain adaptation,
alongside an iterative problem-solving mechanism with sophisticated error
handling. Custom datasets were developed to evaluate the framework against
leading proprietary language models on various engineering tasks. The results
demonstrate the framework effectiveness in automating calculations,
accelerating prototyping, and providing AI-augmented decision support for
industrial processes, marking a significant advancement in process engineering
capabilities.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜË£ΩÁ®ãÂ∑•Á®ã‰ΩúÊ•≠Âä©ÁêÜ (PEOA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± AI È©ÖÂãïÁöÑÊû∂ÊßãÔºåÊó®Âú®Ëß£Ê±∫ÂåñÂ≠∏ÂíåË£ΩÁ®ãÁî¢Ê•≠‰∏≠ÁöÑË§áÈõúÂïèÈ°å„ÄÇË©≤Êû∂ÊßãÊé°Áî®Ê®°ÁµÑÂåñÊû∂ÊßãÔºåÁî±‰∏ÄÂÄãÂÖÉ‰ª£ÁêÜÁ®ãÂºèÂçîË™øÔºåË©≤‰ª£ÁêÜÁ®ãÂºè‰ΩúÁÇ∫‰∏≠Â§ÆÂçîË™øÂô®ÔºåÁÆ°ÁêÜÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÊåá‰ª§Ë™øÊï¥ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÊ®°Âûã (Â∞àÂÆ∂Ê®°Âûã)„ÄÇÂãï‰ΩúÁî¢ÁîüÂô®Â∞áË§áÈõúÁöÑÂïèÈ°åÂàÜËß£ÁÇ∫Â≠ê‰ªªÂãôÔºå‰∏¶Ë≠òÂà•ÂêàÈÅ©ÁöÑÂ∞àÂÆ∂Ê®°Âûã‰æÜÂü∑Ë°åÊØèÂÄã‰ªªÂãôÔºåÁÇ∫Â§öÊ≠•È©üÂïèÈ°åËß£Ê±∫Êèê‰æõÁ≤æÁ¢∫ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÈóúÈçµÊäÄË°ìÂåÖÊã¨‰ΩøÁî®Â±¨ÊÄßÂúñÈÄ≤Ë°åÈÄ≤ÈöéÁü•Ë≠òÂª∫Ê®°Ôºå‰ª•ÊîπÂñÑË≥áË®äÊ™¢Á¥¢ÔºåÊèê‰æõÊõ¥Ê∫ñÁ¢∫‰∏îËàáËÑàÁµ°Áõ∏ÈóúÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊ≠§Â§ñÔºåË©≤Êû∂ÊßãÊé°Áî®ÊïôÂ∏´-Â≠∏ÁîüÂÇ≥Ëº∏Â≠∏ÁøíÊñπÊ≥ïÔºå‰ΩøÁî® GPT-4 (Omni) ‰æÜÂæÆË™øÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÂ∞àÂÆ∂Ê®°ÂûãÔºå‰ª•ÈÄ≤Ë°åÈ†òÂüüÈÅ©ÊáâÔºå‰ª•ÂèäÂÖ∑ÂÇôÁ≤æÁ∑ªÈåØË™§ËôïÁêÜÂäüËÉΩÁöÑËø≠‰ª£ÂïèÈ°åËß£Ê±∫Ê©üÂà∂„ÄÇÈñãÁôº‰∫ÜËá™Ë®ÇË≥áÊñôÈõÜÔºå‰ª•ÈáùÂ∞çÂêÑÁ®ÆÂ∑•Á®ã‰ªªÂãôË©ï‰º∞Ë©≤Êû∂ÊßãËàáÈ†òÂÖàÁöÑÂ∞àÊúâË™ûË®ÄÊ®°Âûã„ÄÇÁµêÊûúË≠âÊòé‰∫ÜË©≤Êû∂ÊßãÂú®Ëá™ÂãïÂåñË®àÁÆó„ÄÅÂä†ÈÄüÂª∫Ê®°ÂíåÊèê‰æõ AI Â¢ûÂº∑Ê±∫Á≠ñÊîØÊè¥ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË™åËëóË£ΩÁ®ãÂ∑•Á®ãËÉΩÂäõÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇ

##### **A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**
2408.12578v2 by Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka

Increase in data, size, or compute can lead to sudden learning of specific
capabilities by a neural network -- a phenomenon often called "emergence''.
Beyond scientific understanding, establishing the causal factors underlying
such emergent capabilities is crucial to enable risk regulation frameworks for
AI. In this work, we seek inspiration from study of emergent properties in
other fields and propose a phenomenological definition for the concept in the
context of neural networks. Our definition implicates the acquisition of
general structures underlying the data-generating process as a cause of sudden
performance growth for specific, narrower tasks. We empirically investigate
this definition by proposing an experimental system grounded in a
context-sensitive formal language and find that Transformers trained to perform
tasks on top of strings from this language indeed exhibit emergent
capabilities. Specifically, we show that once the language's underlying grammar
and context-sensitivity inducing structures are learned by the model,
performance on narrower tasks suddenly begins to improve. We then analogize our
network's learning dynamics with the process of percolation on a bipartite
graph, establishing a formal phase transition model that predicts the shift in
the point of emergence observed in our experiments when changing the data
structure. Overall, our experimental and theoretical frameworks yield a step
towards better defining, characterizing, and predicting emergence in neural
networks.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñô„ÄÅË¶èÊ®°ÊàñÈÅãÁÆóÁöÑÂ¢ûÂä†ÔºåÂèØËÉΩÊúÉÂ∞éËá¥Á•ûÁ∂ìÁ∂≤Ë∑ØÁ™ÅÁÑ∂Â≠∏ÊúÉÁâπÂÆöËÉΩÂäõ‚Äî‚ÄîÈÄôÁ®ÆÁèæË±°Â∏∏Á®±ÁÇ∫„ÄåÊπßÁèæ„Äç„ÄÇÈô§‰∫ÜÁßëÂ≠∏ÁêÜËß£‰πãÂ§ñÔºåÁ¢∫Á´ãÈÄôÁ®ÆÊπßÁèæËÉΩÂäõËÉåÂæåÁöÑÂü∫Êú¨ÂéüÂõ†ÔºåÂ∞çÊñºÁÇ∫ AI Âª∫Á´ãÈ¢®Èö™Ê≥ïË¶èÊ°ÜÊû∂Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂæûÂÖ∂‰ªñÈ†òÂüü‰∏≠Â∞çÊπßÁèæÁâπÊÄßÁöÑÁ†îÁ©∂‰∏≠Â∞ãÊ±ÇÈùàÊÑüÔºå‰∏¶ÈáùÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊ¶ÇÂøµÊèêÂá∫ÁèæË±°Â≠∏ÂÆöÁæ©„ÄÇÊàëÂÄëÁöÑÂÆöÁæ©ÊöóÁ§∫ÔºåÂèñÂæóË≥áÊñôÁî¢ÁîüÁ®ãÂ∫èËÉåÂæåÁöÑÈÄöÁî®ÁµêÊßãÔºåÊòØÁâπÂÆö„ÄÅËºÉÁãπÈöò‰ªªÂãôÁ™ÅÁÑ∂ÊïàËÉΩÊèêÂçáÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄã‰ª•ÊÉÖÂ¢ÉÊïèÊÑüÂΩ¢ÂºèË™ûË®ÄÁÇ∫Âü∫Á§éÁöÑÂØ¶È©óÁ≥ªÁµ±ÔºåÂ∞çÈÄôÂÄãÂÆöÁæ©ÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂ÔºåÁôºÁèæÁ∂ìÈÅéË®ìÁ∑¥‰ª•Âü∑Ë°åÈÄôÂÄãË™ûË®Ä‰∏≠Â≠ó‰∏≤È†ÇÈÉ®‰ªªÂãôÁöÑ TransformerÔºåÁ¢∫ÂØ¶Â±ïÁèæÂá∫ÊπßÁèæËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ±ïÁ§∫Âá∫Ê®°Âûã‰∏ÄÊó¶Â≠∏ÊúÉË™ûË®ÄÁöÑÂ∫ïÂ±§ÊñáÊ≥ïÂíåÊÉÖÂ¢ÉÊïèÊÑüË™òÂ∞éÁµêÊßãÔºåÂ∞çËºÉÁãπÈöò‰ªªÂãôÁöÑÊïàËÉΩÂ∞±ÊúÉÁ™ÅÁÑ∂ÈñãÂßãÊèêÂçá„ÄÇÊé•ËëóÊàëÂÄëÂ∞áÁ∂≤Ë∑ØÁöÑÂ≠∏ÁøíÂãïÊÖãÈ°ûÊØîÁÇ∫‰∫åÈÉ®Âúñ‰∏äÁöÑÊª≤ÊµÅÈÅéÁ®ãÔºåÂª∫Á´ã‰∏ÄÂÄãÊ≠£ÂºèÁöÑÁõ∏ËÆäÊ®°ÂûãÔºåÁî®ÊñºÈ†êÊ∏¨Âú®ÊîπËÆäË≥áÊñôÁµêÊßãÊôÇÔºåÊàëÂÄëÂú®ÂØ¶È©ó‰∏≠ËßÄÂØüÂà∞ÁöÑÊπßÁèæÈªû‰ΩçÁßª„ÄÇÊï¥È´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂØ¶È©óÂíåÁêÜË´ñÊ°ÜÊû∂ÊúùËëóÊõ¥ÂÆåÂñÑÂú∞ÂÆöÁæ©„ÄÅÊèèËø∞ÂíåÈ†êÊ∏¨Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊπßÁèæÈÇÅÈÄ≤‰∫Ü‰∏ÄÊ≠•„ÄÇ</paragraph>

##### **Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**
2409.00061v1 by Arief Purnama Muharram, Ayu Purwarianti

Automated fact-checking is a key strategy to overcome the spread of COVID-19
misinformation on the internet. These systems typically leverage deep learning
approaches through Natural Language Inference (NLI) to verify the truthfulness
of information based on supporting evidence. However, one challenge that arises
in deep learning is performance stagnation due to a lack of knowledge during
training. This study proposes using a Knowledge Graph (KG) as external
knowledge to enhance NLI performance for automated COVID-19 fact-checking in
the Indonesian language. The proposed model architecture comprises three
modules: a fact module, an NLI module, and a classifier module. The fact module
processes information from the KG, while the NLI module handles semantic
relationships between the given premise and hypothesis. The representation
vectors from both modules are concatenated and fed into the classifier module
to produce the final result. The model was trained using the generated
Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.
Our study demonstrates that incorporating KGs can significantly improve NLI
performance in fact-checking, achieving the best accuracy of 0,8616. This
suggests that KGs are a valuable component for enhancing NLI performance in
automated fact-checking.

ÊëòË¶ÅÔºöËá™Âãï‰∫ãÂØ¶Êü•Ê†∏ÊòØÂÖãÊúçÁ∂≤Ë∑Ø‰∏ä COVID-19 ÈåØË™§Ë≥áË®äÊï£Êí≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµÁ≠ñÁï•„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÈÄöÂ∏∏ÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) ‰æÜÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÊ†πÊìöÊîØÊè¥Ë≠âÊìöÈ©óË≠âË≥áË®äÁöÑÁúüÂØ¶ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂú®Ê∑±Â∫¶Â≠∏Áøí‰∏≠ÊúÉÂá∫Áèæ‰∏ÄÂÄãÊåëÊà∞ÔºåÈÇ£Â∞±ÊòØÂú®Ë®ìÁ∑¥ÊúüÈñìÂõ†Áº∫‰πèÁü•Ë≠òËÄåÂ∞éËá¥ÊïàËÉΩÂÅúÊªØ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®Áü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Â§ñÈÉ®Áü•Ë≠òÔºå‰ª•Â¢ûÂº∑Ëá™ÂãïÂåñ COVID-19 ‰∫ãÂØ¶Êü•Ê†∏ÁöÑ NLI ÊïàËÉΩÔºå‰∏¶‰ª•Âç∞Â∞ºË™ûÈÄ≤Ë°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊû∂ÊßãÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö‰∫ãÂØ¶Ê®°ÁµÑ„ÄÅNLI Ê®°ÁµÑÂíåÂàÜÈ°ûÂô®Ê®°ÁµÑ„ÄÇ‰∫ãÂØ¶Ê®°ÁµÑËôïÁêÜ‰æÜËá™ KG ÁöÑË≥áË®äÔºåËÄå NLI Ê®°ÁµÑÂâáËôïÁêÜÁµ¶ÂÆöÂâçÊèêÂíåÂÅáË®≠‰πãÈñìÁöÑË™ûÁæ©Èóú‰øÇ„ÄÇ‰æÜËá™ÂÖ©ÂÄãÊ®°ÁµÑÁöÑË°®Á§∫ÂêëÈáèÊúÉ‰∏≤Êé•Ëµ∑‰æÜÔºå‰∏¶Ëº∏ÂÖ•ÂàÜÈ°ûÂô®Ê®°ÁµÑ‰ª•Áî¢ÁîüÊúÄÁµÇÁµêÊûú„ÄÇÊ≠§Ê®°Âûã‰ΩøÁî®Áî¢ÁîüÁöÑÂç∞Â∞ºË™û COVID-19 ‰∫ãÂØ¶Êü•Ê†∏Ë≥áÊñôÈõÜÂíå COVID-19 KG Bahasa Indonesia ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë≠âÊòéÔºåÁ¥çÂÖ• KG ÂèØ‰ª•È°ØËëóÊîπÂñÑ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ÁöÑ NLI ÊïàËÉΩÔºåÈÅîÂà∞ 0.8616 ÁöÑÊúÄ‰Ω≥Ê∫ñÁ¢∫Â∫¶„ÄÇÈÄôË°®Á§∫ KG ÊòØÂ¢ûÂº∑Ëá™ÂãïÂåñ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ NLI ÊïàËÉΩÁöÑÂØ∂Ë≤¥ÁµÑÊàêÈÉ®ÂàÜ„ÄÇ

##### **Cell-ontology guided transcriptome foundation model**
2408.12373v1 by Xinyu Yuan, Zhihao Zhan, Zuobai Zhang, Manqi Zhou, Jianan Zhao, Boyu Han, Yue Li, Jian Tang

Transcriptome foundation models TFMs hold great promises of deciphering the
transcriptomic language that dictate diverse cell functions by self-supervised
learning on large-scale single-cell gene expression data, and ultimately
unraveling the complex mechanisms of human diseases. However, current TFMs
treat cells as independent samples and ignore the taxonomic relationships
between cell types, which are available in cell ontology graphs. We argue that
effectively leveraging this ontology information during the TFM pre-training
can improve learning biologically meaningful gene co-expression patterns while
preserving TFM as a general purpose foundation model for downstream zero-shot
and fine-tuning tasks. To this end, we present \textbf{s}ingle \textbf{c}ell,
\textbf{Cell}-\textbf{o}ntology guided TFM scCello. We introduce cell-type
coherence loss and ontology alignment loss, which are minimized along with the
masked gene expression prediction loss during the pre-training. The novel loss
component guide scCello to learn the cell-type-specific representation and the
structural relation between cell types from the cell ontology graph,
respectively. We pre-trained scCello on 22 million cells from CellxGene
database leveraging their cell-type labels mapped to the cell ontology graph
from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates
competitive generalization and transferability performance over the existing
TFMs on biologically important tasks including identifying novel cell types of
unseen cells, prediction of cell-type-specific marker genes, and cancer drug
responses.

ÊëòË¶ÅÔºö<paragraph>ËΩâÈåÑÁµÑÂü∫Á§éÊ®°Âûã TFM ÊâøË´æËß£Á¢ºËΩâÈåÑÁµÑË™ûË®ÄÔºåÂÆÉÈÄèÈÅéÂú®Â§ßÂûãÂñÆÁ¥∞ËÉûÂü∫Âõ†Ë°®ÁèæË≥áÊñô‰∏äÈÄ≤Ë°åËá™ÊàëÁõ£Áù£Â≠∏ÁøíÔºå‰æÜÊ±∫ÂÆö‰∏çÂêåÁöÑÁ¥∞ËÉûÂäüËÉΩÔºå‰∏¶ÊúÄÁµÇËß£Èñã‰∫∫È°ûÁñæÁóÖÁöÑË§áÈõúÊ©üÂà∂„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ TFM Â∞áÁ¥∞ËÉûË¶ñÁÇ∫Áç®Á´ãÊ®£Êú¨Ôºå‰∏¶ÂøΩÁï•Á¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÂàÜÈ°ûÈóú‰øÇÔºåËÄåÈÄôÂú®Á¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠ÊòØÂèØÁî®ÁöÑ„ÄÇÊàëÂÄëË™çÁÇ∫Âú® TFM È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂà©Áî®Ê≠§Êú¨È´îË´ñË≥áË®äÔºåÂèØ‰ª•ÊîπÂñÑÂ≠∏ÁøíÁîüÁâ©Â≠∏‰∏äÊúâÊÑèÁæ©ÁöÑÂü∫Âõ†ÂÖ±Ë°®ÁèæÊ®°ÂºèÔºåÂêåÊôÇ‰øùÁïô TFM ‰ΩúÁÇ∫‰∏ãÊ∏∏Èõ∂Ê¨°Â≠∏ÁøíÂíåÂæÆË™ø‰ªªÂãôÁöÑ‰∏ÄËà¨Áî®ÈÄîÂü∫Á§éÊ®°Âûã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ÂñÆÁ¥∞ËÉû„ÄÅÁ¥∞ËÉûÊú¨È´îË´ñÂºïÂ∞éÁöÑ TFM scCello„ÄÇÊàëÂÄëÂºïÂÖ•Á¥∞ËÉûÈ°ûÂûã‰∏ÄËá¥ÊÄßÊêçÂ§±ÂíåÊú¨È´îË´ñÂ∞çÈΩäÊêçÂ§±ÔºåÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúÉÂ∞áÂÖ∂ËàáÈÅÆÁΩ©Âü∫Âõ†Ë°®ÁèæÈ†êÊ∏¨ÊêçÂ§±‰∏ÄËµ∑ÊúÄÂ∞èÂåñ„ÄÇÈÄôÂÄãÊñ∞Á©éÁöÑÊêçÂ§±ÁµÑ‰ª∂ÂºïÂ∞é scCello ÂàÜÂà•ÂæûÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠Â≠∏ÁøíÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöË°®Á§∫ÂíåÁ¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÁµêÊßãÈóú‰øÇ„ÄÇÊàëÂÄëÂú® CellxGene Ë≥áÊñôÂ∫´‰∏≠Â∞ç 2200 Ëê¨ÂÄãÁ¥∞ËÉûÈÄ≤Ë°å scCello È†êË®ìÁ∑¥ÔºåÂà©Áî®ÂÖ∂Á¥∞ËÉûÈ°ûÂûãÊ®ôÁ±§Â∞çÊáâÂà∞ÈñãÊîæÁîüÁâ©ÂíåÁîüÁâ©ÈÜ´Â≠∏Êú¨È´îÈëÑÈÄ†Âª†ÁöÑÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®„ÄÇÊàëÂÄëÁöÑ TFM Âú®ÁîüÁâ©Â≠∏‰∏äÈáçË¶ÅÁöÑ‰ªªÂãô‰∏äÂ±ïÁ§∫‰∫ÜÊØîÁèæÊúâ TFM Êõ¥ÂÖ∑Á´∂Áà≠ÂäõÁöÑÊ≥õÂåñÂíåÂèØËΩâÁßªÊÄßÔºåÂåÖÊã¨Ë≠òÂà•Êú™Ë¶ãÁ¥∞ËÉûÁöÑÊñ∞Á¥∞ËÉûÈ°ûÂûã„ÄÅÈ†êÊ∏¨Á¥∞ËÉûÈ°ûÂûãÁâπÂÆöÊ®ôË®òÂü∫Âõ†ÂíåÁôåÁóáËó•Áâ©ÂèçÊáâ„ÄÇ</paragraph>

##### **Graph Retrieval Augmented Trustworthiness Reasoning**
2408.12333v2 by Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.

ÊëòË¶ÅÔºö<paragraph>Âú®‰ø°ÊÅØ‰∏çÂÆåÊï¥ÁöÑÂ§ö‰∫∫ÈÅäÊà≤‰∏≠ÔºåÂèØ‰ø°Â∫¶Êé®ÁêÜËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ª£ÁêÜ‰∫∫ËÉΩÂ§†Ë≠òÂà•ÊΩõÂú®ÁöÑÁõüÂèãÂíåÊïµ‰∫∫ÔºåÂæûËÄåÂ¢ûÂº∑Êé®ÁêÜÂíåÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ã„ÄÇ‰æùË≥¥È†êÂÖàË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑÁâπÂÆöÈ†òÂüüÊï∏ÊìöÂíåÂ§ßÈáèÁöÑÁçéÂãµÂõûÈ•ãÔºåËÄåÂÆÉÂÄëÁº∫‰πèÂØ¶ÊôÇÈÅ©ÊáâÊÄßÊúÉÈòªÁ§ôÂÆÉÂÄëÂú®ÂãïÊÖãÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂúñÂΩ¢Ê™¢Á¥¢Â¢ûÂº∑Êé®ÁêÜ (GRATR) Ê°ÜÊû∂ÔºåÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰æÜÂä†Âº∑‰ª£ÁêÜ‰∫∫ÁöÑÂèØ‰ø°Â∫¶Êé®ÁêÜ„ÄÇGRATR ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂãïÊÖãÂèØ‰ø°Â∫¶ÂúñÂΩ¢Ôºå‰∏¶‰ΩøÁî®Ë≠âÊìö‰ø°ÊÅØÂØ¶ÊôÇÊõ¥Êñ∞ÂÆÉÔºå‰∏¶Ê™¢Á¥¢Áõ∏ÈóúÁöÑ‰ø°‰ªªÊï∏Êìö‰ª•Â¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÈÄöÈÅéÂ§ö‰∫∫ÈÅäÊà≤„ÄåÁãº‰∫∫„ÄçÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂ∞á GRATR ËàáÂü∫Ê∫ñ LLM Âíå‰ΩøÁî® Native RAG Âíå Rerank RAG Â¢ûÂº∑ÁöÑ LLM ÈÄ≤Ë°å‰∫ÜÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåGRATR Âú®Áç≤ÂãùÁéá‰∏äÊØîÂü∫Ê∫ñÊñπÊ≥ïÈ´òÂá∫ 30%ÔºåÂÖ∑ÊúâÂçìË∂äÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåGRATR ÊúâÊïàÂú∞Ê∏õËºï‰∫Ü LLM ÁöÑÂπªË¶∫Ôºå‰æãÂ¶ÇË∫´‰ªΩÂíåÁõÆÊ®ôÂÅ•ÂøòÁóáÔºåÊúÄÈáçË¶ÅÁöÑÊòØÔºåÂÆÉÈÄöÈÅé‰ΩøÁî®ÂèØ‰ø°Â∫¶ÂúñÂΩ¢‰ΩøÊé®ÁêÜÈÅéÁ®ãÊõ¥ÈÄèÊòé‰∏îÂèØËøΩËπ§„ÄÇ</paragraph>

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÊïôËÇ≤È´òÂ∫¶‰æùË≥¥Ê®°Êì¨ÁóÖ‰∫∫ (SP) Êèê‰æõ‰∏ÄÂÄãÂÆâÂÖ®ÁöÑÁí∞Â¢ÉÔºåËÆìÂ≠∏ÁîüÁ∑¥ÁøíËá®Â∫äÊäÄËÉΩÔºåÂåÖÊã¨ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÊãõÂãüÂêàÊ†º SP ÁöÑÈ´òÊàêÊú¨ÂíåÁº∫‰πèÂ§öÊ®£ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÂ∑≤ÈÄ†ÊàêÈ°ØËëóÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π MedDiTÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÁü•Ë≠òÊéßÂà∂Â∞çË©±Êû∂ÊßãÔºåÂÆÉÂèØ‰ª•ÂãïÊÖãÁî¢ÁîüÁ¨¶ÂêàÊ®°Êì¨ÁóÖ‰∫∫ÁóáÁãÄÁöÑÂêàÁêÜÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂØ¶ÁèæÂ§öÊ®£ÁöÑË®∫Êñ∑ÊäÄËÉΩË®ìÁ∑¥„ÄÇÂÖ∑È´î‰æÜË™™ÔºåMedDiT Êï¥Âêà‰∫ÜÂêÑÁ®ÆÁóÖ‰∫∫Áü•Ë≠òÂúñË≠ú (KG)ÔºåÊèèËø∞ÁóÖ‰∫∫ÁöÑÂ±¨ÊÄßÂíåÁóáÁãÄÔºå‰ª•ÂãïÊÖãÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË°åÁÇ∫Ôºå‰∏¶ÊéßÂà∂ÁóÖ‰∫∫ÁâπÂæµÔºåÊ∏õËºïÈÜ´Â≠∏Â∞çË©±‰∏≠ÁöÑÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¥çÂÖ•‰∏ÄÂÄãÁ∂ìÈÅéÂæÆË™øÁöÑÊì¥Êï£Transformer (DiT) Ê®°ÂûãÔºåÊ†πÊìö KG ‰∏≠ÊåáÂÆöÁöÑÁóÖ‰∫∫Â±¨ÊÄßÁî¢ÁîüÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂØ¶ÈöõÁ§∫ÁØÑÂ±ïÁ§∫ MedDiT ÁöÑÂäüËÉΩÔºåÂ±ïÁ§∫ÂÆÉÂú®‰∏çÂêåÊ®°Êì¨ÁóÖ‰∫∫Ê°à‰æã‰∏≠‰ΩúÁî®‰∏¶Áî¢ÁîüÁõ∏ÊáâÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑËÉΩÂäõ„ÄÇÈÄôÂèØ‰ª•ÁÇ∫Â≠∏ÁîüÊèê‰æõË±êÂØå‰∏î‰∫íÂãïÁöÑÂ≠∏ÁøíÈ´îÈ©óÔºåÈÄèÈÅéÊèê‰æõË∫´Ê≠∑ÂÖ∂Â¢ÉÁöÑÊ®°Êì¨Âπ≥Âè∞ÔºåÊèêÂçáÈÜ´Â≠∏ÊïôËÇ≤ÔºåÈÄ†Á¶èÊú™‰æÜÁöÑÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈó°Êòé‰∫ÜÂú®ÊïôËÇ≤ÊáâÁî®‰∏≠Êï¥Âêà LLM„ÄÅKG Âíå DiT Á≠âÂÖàÈÄ≤ÊäÄË°ìÁöÑÂèØË°åÊÄßÔºåÁ™ÅÈ°ØÂÆÉÂÄëÂú®Ëß£Ê±∫Ê®°Êì¨ÁóÖ‰∫∫ÁÇ∫Âü∫Á§éÁöÑÈÜ´Â≠∏ÊïôËÇ≤ÊâÄÈù¢Ëá®ÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ

##### **Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**
2408.12116v1 by Junlin He, Tong Nie, Wei Ma

In the geospatial domain, universal representation models are significantly
less prevalent than their extensive use in natural language processing and
computer vision. This discrepancy arises primarily from the high costs
associated with the input of existing representation models, which often
require street views and mobility data. To address this, we develop a novel,
training-free method that leverages large language models (LLMs) and auxiliary
map data from OpenStreetMap to derive geolocation representations (LLMGeovec).
LLMGeovec can represent the geographic semantics of city, country, and global
scales, which acts as a generic enhancer for spatio-temporal learning.
Specifically, by direct feature concatenation, we introduce a simple yet
effective paradigm for enhancing multiple spatio-temporal tasks including
geographic prediction (GP), long-term time series forecasting (LTSF), and
graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly
integrate into a wide spectrum of spatio-temporal learning models, providing
immediate enhancements. Experimental results demonstrate that LLMGeovec
achieves global coverage and significantly boosts the performance of leading
GP, LTSF, and GSTF models.

ÊëòË¶ÅÔºöÂú®Á©∫ÈñìÂú∞ÁêÜÈ†òÂüüÔºåÈÄöÁî®Ë°®Á§∫Ê®°ÂûãÈ°ØËëóÂ∞ëÊñºÂÆÉÂÄëÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫‰∏≠ÁöÑÂª£Ê≥õ‰ΩøÁî®„ÄÇÈÄôÁ®ÆÂ∑ÆÁï∞‰∏ªË¶ÅÊ∫êÊñºÁèæÊúâË°®Á§∫Ê®°ÂûãÁöÑËº∏ÂÖ•ÊàêÊú¨È´òÔºåÈÄôÈÄöÂ∏∏ÈúÄË¶ÅË°óÊôØÂíåÊµÅÂãïÊÄßË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂÖçË®ìÁ∑¥ÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå OpenStreetMap ÁöÑËºîÂä©Âú∞ÂúñË≥áÊñô‰æÜÊé®Â∞éÂú∞ÁêÜ‰ΩçÁΩÆË°®Á§∫ (LLMGeovec)„ÄÇLLMGeovec ÂèØ‰ª•Ë°®Á§∫ÂüéÂ∏Ç„ÄÅÂúãÂÆ∂ÂíåÂÖ®ÁêÉË¶èÊ®°ÁöÑÂú∞ÁêÜË™ûÁæ©Ôºå‰ΩúÁÇ∫ÊôÇÁ©∫Â≠∏ÁøíÁöÑÈÄöÁî®Â¢ûÂº∑Âô®„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈÄöÈÅéÁõ¥Êé•ÁâπÂæµ‰∏≤Êé•ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂ¢ûÂº∑Â§öÂÄãÊôÇÁ©∫‰ªªÂãôÔºåÂåÖÊã¨Âú∞ÁêÜÈ†êÊ∏¨ (GP)„ÄÅÈï∑ÊúüÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ (LTSF) ÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑÊôÇÁ©∫È†êÊ∏¨ (GSTF)„ÄÇLLMGeovec ÂèØ‰ª•ÁÑ°Á∏´Êï¥ÂêàÂà∞Âª£Ê≥õÁöÑÊôÇÁ©∫Â≠∏ÁøíÊ®°Âûã‰∏≠ÔºåÊèê‰æõÁ´ãÂç≥ÁöÑÂ¢ûÂº∑„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLMGeovec ÈÅîÂà∞‰∫ÜÂÖ®ÁêÉË¶ÜËìãÁéáÔºå‰∏¶È°ØËëóÊèêÂçá‰∫ÜÈ†òÂÖàÁöÑ GP„ÄÅLTSF Âíå GSTF Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇ

##### **Enabling Small Models for Zero-Shot Classification through Model Label Learning**
2408.11449v1 by Jia Zhang, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li

Vision-language models (VLMs) like CLIP have demonstrated impressive
zero-shot ability in image classification tasks by aligning text and images but
suffer inferior performance compared with task-specific expert models. On the
contrary, expert models excel in their specialized domains but lack zero-shot
ability for new tasks. How to obtain both the high performance of expert models
and zero-shot ability is an important research direction. In this paper, we
attempt to demonstrate that by constructing a model hub and aligning models
with their functionalities using model labels, new tasks can be solved in a
zero-shot manner by effectively selecting and reusing models in the hub. We
introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap
between models and their functionalities through a Semantic Directed Acyclic
Graph (SDAG) and leverages an algorithm, Classification Head Combination
Optimization (CHCO), to select capable models for new tasks. Compared with the
foundation model paradigm, it is less costly and more scalable, i.e., the
zero-shot ability grows with the sizes of the model hub. Experiments on seven
real-world datasets validate the effectiveness and efficiency of MLL,
demonstrating that expert models can be effectively reused for zero-shot tasks.
Our code will be released publicly.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºàVLMÔºâÔºå‰æãÂ¶Ç CLIPÔºåÂ∑≤Âú®ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰∏≠Â±ïÁèæ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊñπÊ≥ïÊòØÂ∞çÈΩäÊñáÂ≠óÂíåÂΩ±ÂÉèÔºå‰ΩÜËàáÁâπÂÆö‰ªªÂãôÁöÑÂ∞àÂÆ∂Ê®°ÂûãÁõ∏ÊØîÔºåÂÖ∂ÊïàËÉΩËºÉÂ∑Æ„ÄÇÁõ∏ÂèçÂú∞ÔºåÂ∞àÂÆ∂Ê®°ÂûãÂú®ÂÖ∂Â∞àÊ•≠È†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂ∞çÊñºÊñ∞‰ªªÂãôÁº∫‰πèÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÂ¶Ç‰ΩïÂêåÊôÇÁç≤ÂæóÂ∞àÂÆ∂Ê®°ÂûãÁöÑÈ´òÊïàËÉΩÂíåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶ÈÄèÈÅéÂª∫Á´ãÊ®°Âûã‰∏≠ÂøÉÔºå‰∏¶‰ΩøÁî®Ê®°ÂûãÊ®ôÁ±§Â∞áÊ®°ÂûãËàáÂÖ∂ÂäüËÉΩÂ∞çÈΩäÔºåË≠âÊòéÂèØ‰ª•ÈÄèÈÅéÊúâÊïàÈÅ∏ÊìáÂíåÈáçË§á‰ΩøÁî®‰∏≠ÂøÉ‰∏≠ÁöÑÊ®°ÂûãÔºå‰ª•Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊñπÂºèËß£Ê±∫Êñ∞‰ªªÂãô„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÁØÑ‰æãÔºåÂç≥Ê®°ÂûãÊ®ôÁ±§Â≠∏ÁøíÔºàMLLÔºâÔºåÂÆÉÈÄèÈÅéË™ûÁæ©Â∞éÂêëÈùûÂæ™Áí∞ÂúñÔºàSDAGÔºâÂΩåÂêàÊ®°ÂûãÂèäÂÖ∂ÂäüËÉΩ‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰∏¶Âà©Áî®‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÂç≥ÂàÜÈ°ûÈ†≠ÁµÑÂêàÊúÄ‰Ω≥ÂåñÔºàCHCOÔºâÔºåÁÇ∫Êñ∞‰ªªÂãôÈÅ∏ÊìáÊúâËÉΩÂäõÁöÑÊ®°Âûã„ÄÇËàáÂü∫Á§éÊ®°ÂûãÁØÑ‰æãÁõ∏ÊØîÔºåÂÆÉÁöÑÊàêÊú¨ËºÉ‰Ωé‰∏îÊõ¥ÂÖ∑ÂèØÊì¥ÂÖÖÊÄßÔºå‰πüÂ∞±ÊòØË™™ÔºåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÊúÉÈö®ËëóÊ®°Âûã‰∏≠ÂøÉË¶èÊ®°ÁöÑÊì¥Â§ßËÄåÂ¢ûÈï∑„ÄÇÂú®‰∏ÉÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü MLL ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºåË≠âÊòé‰∫ÜÂ∞àÂÆ∂Ê®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞ÈáçË§áÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÁôºÂ∏É„ÄÇ

##### **Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**
2408.11182v1 by Zhilong Wang, Haizhou Wang, Nanqing Luo, Lan Zhang, Xiaoyan Sun, Yebo Cao, Peng Liu

Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts
aimed at exploiting the models to generate malicious content. This paper
proposes a new type of jailbreak attacks which shift the attention of the LLM
by inserting a prohibited query into a carrier article. The proposed attack
leverage the knowledge graph and a composer LLM to automatically generating a
carrier article that is similar to the topic of the prohibited query but does
not violate LLM's safeguards. By inserting the malicious query to the carrier
article, the assembled attack payload can successfully jailbreak LLM. To
evaluate the effectiveness of our method, we leverage 4 popular categories of
``harmful behaviors'' adopted by related researches to attack 6 popular LLMs.
Our experiment results show that the proposed attacking method can successfully
jailbreak all the target LLMs which high success rate, except for Claude-3.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°ÂûãÊ®°ÂûãÔºàLLMÔºâÁöÑË∂äÁçÑÊîªÊìäÊ∂âÂèäË£Ω‰ΩúÊèêÁ§∫ÔºåÊó®Âú®Âà©Áî®Ê®°Âûã‰æÜÁî¢ÁîüÊÉ°ÊÑèÂÖßÂÆπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÂûãÁöÑË∂äÁçÑÊîªÊìäÔºåÂÆÉÈÄöÈÅéÂú®ËºâÈ´îÊñáÁ´†‰∏≠ÊèíÂÖ•Á¶ÅÊ≠¢Êü•Ë©¢‰æÜËΩâÁßª LLM ÁöÑÊ≥®ÊÑèÂäõ„ÄÇÊèêË≠∞ÁöÑÊîªÊìäÂà©Áî®Áü•Ë≠òÂúñË≠úÂíå‰ΩúÊõ≤ÂÆ∂ LLM Ëá™ÂãïÁîüÊàêËàáÁ¶ÅÊ≠¢Êü•Ë©¢ÁöÑ‰∏ªÈ°åÁõ∏‰ºº‰ΩÜ‰∏çÊúÉÈÅïÂèç LLM ‰øùÈöúÊé™ÊñΩÁöÑËºâÈ´îÊñáÁ´†„ÄÇÈÄöÈÅéÂ∞áÊÉ°ÊÑèÊü•Ë©¢ÊèíÂÖ•ËºâÈ´îÊñáÁ´†‰∏≠ÔºåÁµÑË£ùÁöÑÊîªÊìäÊúâÊïàËºâËç∑ÂèØ‰ª•ÊàêÂäüË∂äÁçÑ LLM„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂà©Áî®Áõ∏ÈóúÁ†îÁ©∂Êé°Áî®ÁöÑ 4 È°ûÊµÅË°åÁöÑ„ÄåÊúâÂÆ≥Ë°åÁÇ∫„Äç‰æÜÊîªÊìä 6 ÂÄãÊµÅË°åÁöÑ LLM„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊîªÊìäÊñπÊ≥ïÂèØ‰ª•ÊàêÂäüË∂äÁçÑÊâÄÊúâÁõÆÊ®ô LLMÔºåÊàêÂäüÁéáÂæàÈ´òÔºåÈô§‰∫Ü Claude-3„ÄÇ

##### **Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**
2408.11133v1 by Thomas Hoang, Quynh Anh Nguyen, Long Nguyen

Countless disasters have resulted from climate change, causing severe damage
to infrastructure and the economy. These disasters have significant societal
impacts, necessitating mental health services for the millions affected. To
prepare for and respond effectively to such events, it is important to
understand people's emotions and the life incidents they experience before and
after a disaster strikes. In this case study, we collected a dataset of
approximately 400,000 public tweets related to the storm. Using a BERT-based
model, we predicted the emotions associated with each tweet. To efficiently
identify these topics, we utilized the Latent Dirichlet Allocation (LDA)
technique for topic modeling, which allowed us to bypass manual content
analysis and extract meaningful patterns from the data. However, rather than
stopping at topic identification like previous methods \cite{math11244910}, we
further refined our analysis by integrating Graph Neural Networks (GNN) and
Large Language Models (LLM). The GNN was employed to generate embeddings and
construct a similarity graph of the tweets, which was then used to optimize
clustering. Subsequently, we used an LLM to automatically generate descriptive
names for each event cluster, offering critical insights for disaster
preparedness and response strategies.

ÊëòË¶ÅÔºöÁÑ°Êï∏ÁöÑÁÅΩÈõ£ÊòØÁî±ÊñºÊ∞£ÂÄôËÆäÈÅ∑ÊâÄÈÄ†ÊàêÁöÑÔºåÂ∞çÂü∫Á§éÂª∫Ë®≠ÂíåÁ∂ìÊøüÈÄ†ÊàêÂö¥ÈáçÁöÑÊêçÂÆ≥„ÄÇÈÄô‰∫õÁÅΩÈõ£Â∞çÁ§æÊúÉÈÄ†ÊàêÈáçÂ§ßÁöÑÂΩ±ÈüøÔºåÈúÄË¶ÅÁÇ∫Êï∏ÁôæËê¨ÂèóÁÅΩÊ∞ëÁúæÊèê‰æõÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãô„ÄÇÁÇ∫‰∫ÜÊúâÊïàÂú∞ÁÇ∫Ê≠§È°û‰∫ã‰ª∂ÂÅöÂ•ΩÊ∫ñÂÇô‰∏¶‰ΩúÂá∫ÂõûÊáâÔºå‰∫ÜËß£‰∫∫ÂÄëÁöÑÊÉÖÁ∑í‰ª•Âèä‰ªñÂÄëÂú®ÁÅΩÈõ£ÁôºÁîüÂâçÂæåÊâÄÁ∂ìÊ≠∑ÁöÑÁîüÊ¥ª‰∫ã‰ª∂ÈùûÂ∏∏ÈáçË¶Å„ÄÇÂú®Êú¨Ê°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÂåÖÂê´Á¥Ñ 400,000 ÂâáËàáÈ¢®Êö¥Áõ∏ÈóúÁöÑÂÖ¨ÈñãÊé®ÊñáÁöÑË≥áÊñôÈõÜ„ÄÇ‰ΩøÁî®Âü∫Êñº BERT ÁöÑÊ®°ÂûãÔºåÊàëÂÄëÈ†êÊ∏¨‰∫ÜËàáÊØèÂâáÊé®ÊñáÁõ∏ÈóúÁöÑÊÉÖÁ∑í„ÄÇÁÇ∫‰∫ÜÊúâÊïàÁéáÂú∞ÊâæÂá∫ÈÄô‰∫õ‰∏ªÈ°åÔºåÊàëÂÄëÂà©Áî®‰∫ÜÊΩõÂú®ÁãÑÂà©ÂÖãÈõ∑ÈÖçÁΩÆ (LDA) ÊäÄË°ìÈÄ≤Ë°å‰∏ªÈ°åÂª∫Ê®°ÔºåÈÄôËÆìÊàëÂÄëËÉΩÂ§†ÁπûÈÅéÊâãÂãïÂÖßÂÆπÂàÜÊûêÔºåÂæûË≥áÊñô‰∏≠ËêÉÂèñÂá∫ÊúâÊÑèÁæ©ÁöÑÊ®°Âºè„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰∏¶Êú™ÂÉèÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ï \cite{math11244910} ÈÇ£Ê®£ÂÉÖÊ≠¢Êñº‰∏ªÈ°åËæ®Ë≠òÔºåËÄåÊòØÈÄ≤‰∏ÄÊ≠•Êï¥ÂêàÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂÑ™ÂåñÊàëÂÄëÁöÑÂàÜÊûê„ÄÇGNN Ë¢´Áî®ÊñºÁî¢ÁîüÂµåÂÖ•ÂíåÂª∫ÊßãÊé®ÊñáÁöÑÁõ∏‰ººÊÄßÂúñÔºåÁÑ∂ÂæåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂàÜÁæ§„ÄÇÈö®ÂæåÔºåÊàëÂÄë‰ΩøÁî® LLM ÁÇ∫ÊØèÂÄã‰∫ã‰ª∂Áæ§ÈõÜËá™ÂãïÁî¢ÁîüÊèèËø∞ÊÄßÂêçÁ®±ÔºåÁÇ∫ÁÅΩÂÆ≥Èò≤ÁØÑÂíåÊáâËÆäÁ≠ñÁï•Êèê‰æõÈáçË¶ÅÁöÑË¶ãËß£„ÄÇ

##### **Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**
2408.10819v1 by Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou

Knowledge graph completion (KGC) aims to identify missing triples in a
knowledge graph (KG). This is typically achieved through tasks such as link
prediction and instance completion. However, these methods often focus on
either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),
addressing only within-scope triples. This paper introduces a new generative
completion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC
employs a question-answering format to directly generate target entities,
addressing the challenge of questions having multiple possible answers. We
propose a strategy that extracts subgraphs centered on entities and
relationships within the KG, from which negative samples and neighborhood
information are separately obtained to address the one-to-many problem. Our
method generates negative samples using known facts to facilitate the discovery
of new information. Furthermore, we collect and refine neighborhood path data
of known entities, providing contextual information to enhance reasoning in
large language models (LLMs). Our experiments evaluated the proposed method on
four SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five
datasets. Analysis of the results shows that GS-KGC can discover new triples
within existing KGs and generate new facts beyond the closed KG, effectively
bridging the gap between closed-world and open-world KGC.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úË£úÂÖ® (KGC) ÁöÑÁõÆÊ®ôÊòØË≠òÂà•Áü•Ë≠òÂúñË≠ú (KG) ‰∏≠ÈÅ∫Â§±ÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÈÄôÈÄöÂ∏∏ÈÄèÈÅéÈÄ£ÁµêÈ†êÊ∏¨ÂíåÂØ¶‰æãË£úÂÖ®Á≠â‰ªªÂãôÈÅîÊàê„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Â∞àÊ≥®ÊñºÈùúÊÖãÁü•Ë≠òÂúñË≠ú (SKG) ÊàñÊôÇÂ∫èÁü•Ë≠òÂúñË≠ú (TKG)ÔºåÂÉÖËôïÁêÜÁØÑÂúçÂÖßÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∏ÄÂÄãÂêçÁÇ∫ÁîüÊàêÂ≠êÂúñÁÇ∫Âü∫Á§éÁöÑ KGC (GS-KGC) ÁöÑÊñ∞ÁîüÊàêË£úÂÖ®Êû∂Êßã„ÄÇGS-KGC ‰ΩøÁî®ÂïèÁ≠îÊ†ºÂºèÁõ¥Êé•ÁîüÊàêÁõÆÊ®ôÂØ¶È´îÔºå‰ª•Ëß£Ê±∫ÂïèÈ°åÊúâÂ§öÂÄãÂèØËÉΩÁ≠îÊ°àÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ≠ñÁï•ÔºåÂæûÁü•Ë≠òÂúñË≠ú‰∏≠‰ª•ÂØ¶È´îÂíåÈóú‰øÇÁÇ∫‰∏≠ÂøÉÁöÑÂ≠êÂúñÔºåÂæû‰∏≠ÂàÜÂà•ÂèñÂæóË≤†Èù¢Ê®£Êú¨ÂíåÈÑ∞ÂüüË≥áË®äÔºå‰ª•Ëß£Ê±∫‰∏ÄÂ∞çÂ§öÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÊ®°Âûã‰ΩøÁî®Â∑≤Áü•‰∫ãÂØ¶ÁîüÊàêË≤†Èù¢Ê®£Êú¨Ôºå‰ª•Âà©ÁôºÁèæÊñ∞Ë≥áË®ä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊî∂ÈõÜ‰∏¶Á≤æÁÖâÂ∑≤Áü•ÂØ¶È´îÁöÑÈÑ∞ÂüüË∑ØÂæëË≥áÊñôÔºåÊèê‰æõËÉåÊôØË≥áË®ä‰ª•Â¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂú®ÂõõÂÄã SKG ÂíåÂÖ©ÂÄã TKG ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÂú®‰∫îÂÄãË≥áÊñôÈõÜ‰∏äÈÅîÊàêÊúÄÂÖàÈÄ≤ÁöÑ Hits@1 ÊåáÊ®ô„ÄÇÁµêÊûúÂàÜÊûêÈ°ØÁ§∫ÔºåGS-KGC ËÉΩÂ§†Âú®ÁèæÊúâÁöÑ KG ‰∏≠ÁôºÁèæÊñ∞ÁöÑ‰∏âÂÖÉÁµÑÔºå‰∏¶ÁîüÊàêÂ∞ÅÈñâ KG ‰ª•Â§ñÁöÑÊñ∞‰∫ãÂØ¶ÔºåÊúâÊïàÂú∞Á∏ÆÂ∞èÂ∞ÅÈñâ‰∏ñÁïåÂíåÈñãÊîæ‰∏ñÁïå KGC ‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**
2408.10592v1 by Litian Huang, Xinguo Yu, Feng Xiong, Bin He, Shengbing Tang, Jiawen Fu

Solving Algebra Problems with Geometry Diagrams (APGDs) is still a
challenging problem because diagram processing is not studied as intensively as
language processing. To work against this challenge, this paper proposes a
hologram reasoning scheme and develops a high-performance method for solving
APGDs by using this scheme. To reach this goal, it first defines a hologram,
being a kind of graph, and proposes a hologram generator to convert a given
APGD into a hologram, which represents the entire information of APGD and the
relations for solving the problem can be acquired from it by a uniform way.
Then HGR, a hologram reasoning method employs a pool of prepared graph models
to derive algebraic equations, which is consistent with the geometric theorems.
This method is able to be updated by adding new graph models into the pool.
Lastly, it employs deep reinforcement learning to enhance the efficiency of
model selection from the pool. The entire HGR not only ensures high solution
accuracy with fewer reasoning steps but also significantly enhances the
interpretability of the solution process by providing descriptions of all
reasoning steps. Experimental results demonstrate the effectiveness of HGR in
improving both accuracy and interpretability in solving APGDs.

ÊëòË¶ÅÔºöÂà©Áî®Âπæ‰ΩïÂúñÂΩ¢ÂúñÔºàAPGDÔºâËß£Ê±∫‰ª£Êï∏ÂïèÈ°å‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÔºåÂõ†ÁÇ∫ÂúñÂΩ¢ËôïÁêÜÁöÑÁ†îÁ©∂‰∏çÂ¶ÇË™ûË®ÄËôïÁêÜÈÇ£È∫ºÊ∑±ÂÖ•„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÖ®ÊÅØÊé®ÁêÜÊñπÊ°àÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ë©≤ÊñπÊ°àËß£Ê±∫ APGD ÁöÑÈ´òÊÄßËÉΩÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞ÈÄôÂÄãÁõÆÊ®ôÔºåÂÆÉÈ¶ñÂÖàÂÆöÁæ©‰∫Ü‰∏ÄÂÄãÂÖ®ÊÅØÂúñÔºå‰ΩúÁÇ∫‰∏ÄÁ®ÆÂúñÂΩ¢Ôºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ®ÊÅØÂúñÁîüÊàêÂô®ÔºåÂ∞áÁµ¶ÂÆöÁöÑ APGD ËΩâÊèõÁÇ∫‰∏ÄÂÄãÂÖ®ÊÅØÂúñÔºåÂÆÉË°®Á§∫ APGD ÁöÑÂÖ®ÈÉ®‰ø°ÊÅØÔºå‰∏¶‰∏îÂèØ‰ª•ÈÄöÈÅéÁµ±‰∏ÄÁöÑÊñπÂºèÂæû‰∏≠Áç≤ÂèñËß£Ê±∫ÂïèÈ°åÁöÑÈóú‰øÇ„ÄÇÁÑ∂ÂæåÔºåHGRÔºå‰∏ÄÁ®ÆÂÖ®ÊÅØÊé®ÁêÜÊñπÊ≥ïÔºåÊé°Áî®‰∏ÄÁµÑÊ∫ñÂÇôÂ•ΩÁöÑÂúñÂΩ¢Ê®°Âûã‰æÜÊé®Â∞é‰ª£Êï∏ÊñπÁ®ãÂºèÔºåÈÄôËàáÂπæ‰ΩïÂÆöÁêÜÊòØ‰∏ÄËá¥ÁöÑ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•ÈÄöÈÅéÂêëÊ±†‰∏≠Ê∑ªÂä†Êñ∞ÁöÑÂúñÂΩ¢Ê®°Âûã‰æÜÊõ¥Êñ∞„ÄÇÊúÄÂæåÔºåÂÆÉÊé°Áî®Ê∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí‰æÜÊèêÈ´òÂæûÊ±†‰∏≠ÈÅ∏ÊìáÊ®°ÂûãÁöÑÊïàÁéá„ÄÇÊï¥ÂÄã HGR ‰∏çÂÉÖÁ¢∫‰øù‰∫ÜËºÉÂ∞ëÁöÑÊé®ÁêÜÊ≠•È©üÂç≥ÂèØÁç≤ÂæóËºÉÈ´òÁöÑÊ±ÇËß£Á≤æÂ∫¶ÔºåËÄå‰∏îÈÇÑÈÄöÈÅéÊèê‰æõÊâÄÊúâÊé®ÁêÜÊ≠•È©üÁöÑÊèèËø∞‰æÜÈ°ØËëóÂ¢ûÂº∑‰∫ÜËß£Ê±∫ÈÅéÁ®ãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü HGR Âú®ÊèêÈ´òÊ±ÇËß£ APGD ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Query languages for neural networks**
2408.10362v2 by Martin Grohe, Christoph Standke, Juno Steegmans, Jan Van den Bussche

We lay the foundations for a database-inspired approach to interpreting and
understanding neural network models by querying them using declarative
languages. Towards this end we study different query languages, based on
first-order logic, that mainly differ in their access to the neural network
model. First-order logic over the reals naturally yields a language which views
the network as a black box; only the input--output function defined by the
network can be queried. This is essentially the approach of constraint query
languages. On the other hand, a white-box language can be obtained by viewing
the network as a weighted graph, and extending first-order logic with summation
over weight terms. The latter approach is essentially an abstraction of SQL. In
general, the two approaches are incomparable in expressive power, as we will
show. Under natural circumstances, however, the white-box approach can subsume
the black-box approach; this is our main result. We prove the result concretely
for linear constraint queries over real functions definable by feedforward
neural networks with a fixed number of hidden layers and piecewise linear
activation functions.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂ•†ÂÆö‰∫Ü‰∏ÄÂÄãÂèóË≥áÊñôÂ∫´ÂïüÁôºÁöÑÂü∫Á§éÔºåÁî®ÊñºÈÄèÈÅé‰ΩøÁî®ÂÆ£ÂëäÂºèË™ûË®ÄÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÈÄ≤Ë°åË©ÆÈáãÂíåÁêÜËß£„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞ÈÄôÂÄãÁõÆÁöÑÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂü∫Êñº‰∏ÄÈöéÈÇèËºØÁöÑ‰∏çÂêåÊü•Ë©¢Ë™ûË®ÄÔºåÂÆÉÂÄë‰∏ªË¶ÅÂú®ÊñºÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑÂ≠òÂèñÊñπÂºè‰∏çÂêå„ÄÇ‰∏ÄÈöéÂØ¶Êï∏ÈÇèËºØËá™ÁÑ∂ÊúÉÁî¢Áîü‰∏ÄÁ®ÆË™ûË®ÄÔºåÂ∞áÁ∂≤Ë∑ØË¶ñÁÇ∫‰∏ÄÂÄãÈªëÁõíÂ≠êÔºõÂè™ËÉΩÊü•Ë©¢Á∂≤Ë∑ØÂÆöÁæ©ÁöÑËº∏ÂÖ•Ëº∏Âá∫ÂáΩÊï∏„ÄÇÈÄôÂü∫Êú¨‰∏äÊòØÁ¥ÑÊùüÊü•Ë©¢Ë™ûË®ÄÁöÑÊñπÊ≥ï„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂèØ‰ª•ÈÄèÈÅéÂ∞áÁ∂≤Ë∑ØË¶ñÁÇ∫‰∏ÄÂÄãÂä†Ê¨äÂúñÔºå‰∏¶Â∞á‰∏ÄÈöéÈÇèËºØÂª∂‰º∏Âà∞Ê¨äÈáçÈ†Ö‰∏äÁöÑÁ∏ΩÂíåÔºå‰æÜÂèñÂæó‰∏ÄÂÄãÁôΩÁõíË™ûË®Ä„ÄÇÂæåËÄÖÊñπÊ≥ïÂü∫Êú¨‰∏äÊòØ SQL ÁöÑÊäΩË±°„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ïÂú®Ë°®ÈÅîËÉΩÂäõ‰∏äÁÑ°Ê≥ïÁõ∏Êèê‰∏¶Ë´ñÔºåÊàëÂÄëÂ∞áÊúÉË≠âÊòéÈÄô‰∏ÄÈªû„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá™ÁÑ∂ÊÉÖÊ≥Å‰∏ãÔºåÁôΩÁõíÊñπÊ≥ïÂèØ‰ª•ÂåÖÂê´ÈªëÁõíÊñπÊ≥ïÔºõÈÄôÊòØÊàëÂÄëÁöÑÈáçÈªû„ÄÇÊàëÂÄëÂÖ∑È´îË≠âÊòé‰∫ÜÁ∑öÊÄßÁ¥ÑÊùüÊü•Ë©¢Â∞çÊñºÁî±ÂÖ∑ÊúâÂõ∫ÂÆöÊï∏ÈáèÈö±ËóèÂ±§ÂíåÂàÜÊÆµÁ∑öÊÄßÊøÄÊ¥ªÂáΩÊï∏ÁöÑÂâçÈ•ãÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèØÂÆöÁæ©ÁöÑÂØ¶ÂáΩÊï∏„ÄÇ</paragraph>

##### **Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**
2408.10124v1 by Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang

Molecular property prediction is a crucial foundation for drug discovery. In
recent years, pre-trained deep learning models have been widely applied to this
task. Some approaches that incorporate prior biological domain knowledge into
the pre-training framework have achieved impressive results. However, these
methods heavily rely on biochemical experts, and retrieving and summarizing
vast amounts of domain knowledge literature is both time-consuming and
expensive. Large Language Models (LLMs) have demonstrated remarkable
performance in understanding and efficiently providing general knowledge.
Nevertheless, they occasionally exhibit hallucinations and lack precision in
generating domain-specific knowledge. Conversely, Domain-specific Small Models
(DSMs) possess rich domain knowledge and can accurately calculate molecular
domain-related metrics. However, due to their limited model size and singular
functionality, they lack the breadth of knowledge necessary for comprehensive
representation learning. To leverage the advantages of both approaches in
molecular property prediction, we propose a novel Molecular Graph
representation learning framework that integrates Large language models and
Domain-specific small models (MolGraph-LarDo). Technically, we design a
two-stage prompt strategy where DSMs are introduced to calibrate the knowledge
provided by LLMs, enhancing the accuracy of domain-specific information and
thus enabling LLMs to generate more precise textual descriptions for molecular
samples. Subsequently, we employ a multi-modal alignment method to coordinate
various modalities, including molecular graphs and their corresponding
descriptive texts, to guide the pre-training of molecular representations.
Extensive experiments demonstrate the effectiveness of the proposed method.

ÊëòË¶ÅÔºöÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨ÊòØËó•Áâ©ÁôºÁèæÁöÑÈóúÈçµÂü∫Á§é„ÄÇËøëÂπ¥‰æÜÔºåÈ†êË®ìÁ∑¥Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂ∑≤Âª£Ê≥õÊáâÁî®ÊñºÊ≠§‰ªªÂãô„ÄÇ‰∏Ä‰∫õÂ∞áÂÖàÈ©óÁîüÁâ©È†òÂüüÁü•Ë≠òÁ¥çÂÖ•È†êË®ìÁ∑¥Êû∂ÊßãÁöÑÊñπÊ≥ïÂ∑≤ÂèñÂæó‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂö¥Èáç‰æùË≥¥ÊñºÁîüÁâ©ÂåñÂ≠∏Â∞àÂÆ∂Ôºå‰∏¶‰∏îÊ™¢Á¥¢ÂíåÁ∏ΩÁµêÂ§ßÈáèÁöÑÈ†òÂüüÁü•Ë≠òÊñáÁçªÊó¢ËÄóÊôÇÂèàÊòÇË≤¥„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÁêÜËß£ÂíåÊúâÊïàÊèê‰æõ‰∏ÄËà¨Áü•Ë≠òÊñπÈù¢Â±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÂÄëÂÅ∂ÁàæÊúÉÂá∫ÁèæÂπªË¶∫Ôºå‰∏¶‰∏îÂú®ÁîüÊàêÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÊôÇÁº∫‰πèÁ≤æÁ¢∫ÊÄß„ÄÇÁõ∏ÂèçÔºåÁâπÂÆöÈ†òÂüüÁöÑÂ∞èÊ®°Âûã (DSM) ÊìÅÊúâË±êÂØåÁöÑÈ†òÂüüÁü•Ë≠òÔºå‰∏¶‰∏îÂèØ‰ª•Ê∫ñÁ¢∫Ë®àÁÆóËàáÂàÜÂ≠êÈ†òÂüüÁõ∏ÈóúÁöÑÊåáÊ®ô„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÆÉÂÄëÊúâÈôêÁöÑÊ®°ÂûãÂ§ßÂ∞èÂíåÂñÆ‰∏ÄÂäüËÉΩÔºåÂÆÉÂÄëÁº∫‰πèÂÖ®Èù¢Ë°®Á§∫Â≠∏ÁøíÊâÄÈúÄÁöÑÁü•Ë≠òÂª£Â∫¶„ÄÇÁÇ∫‰∫ÜÂú®ÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨‰∏≠Âà©Áî®ÈÄôÂÖ©Á®ÆÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÂ≠êÂúñË°®Á§∫Â≠∏ÁøíÊ°ÜÊû∂ÔºåÂÆÉÈõÜÊàê‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂíåÁâπÂÆöÈ†òÂüüÁöÑÂ∞èÊ®°Âûã (MolGraph-LarDo)„ÄÇÂú®ÊäÄË°ì‰∏äÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÊèêÁ§∫Á≠ñÁï•ÔºåÂÖ∂‰∏≠ÂºïÂÖ• DSM ‰æÜÊ†°Ê∫ñ LLM Êèê‰æõÁöÑÁü•Ë≠òÔºåÊèêÈ´òÁâπÂÆöÈ†òÂüü‰ø°ÊÅØÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂæûËÄå‰Ωø LLM ËÉΩÂ§†ÁÇ∫ÂàÜÂ≠êÊ®£Êú¨ÁîüÊàêÊõ¥Á≤æÁ¢∫ÁöÑÊñáÊú¨ÊèèËø∞„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊé°Áî®Â§öÊ®°ÊÖãÂ∞çÈΩäÊñπÊ≥ï‰æÜÂçîË™øÂêÑÁ®ÆÊ®°ÊÖãÔºåÂåÖÊã¨ÂàÜÂ≠êÂúñÂèäÂÖ∂Â∞çÊáâÁöÑÊèèËø∞ÊÄßÊñáÊú¨Ôºå‰ª•ÊåáÂ∞éÂàÜÂ≠êË°®Á§∫ÁöÑÈ†êË®ìÁ∑¥„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Geometry Informed Tokenization of Molecules for Language Model Generation**
2408.10120v1 by Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji

We consider molecule generation in 3D space using language models (LMs),
which requires discrete tokenization of 3D molecular geometries. Although
tokenization of molecular graphs exists, that for 3D geometries is largely
unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which
converts molecular geometries into $SE(3)$-invariant 1D discrete sequences.
Geo2Seq consists of canonical labeling and invariant spherical representation
steps, which together maintain geometric and atomic fidelity in a format
conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various
LMs excel in molecular geometry generation, especially in controlled generation
tasks.

ÊëòË¶ÅÔºöÊàëÂÄëËÄÉÊÖÆ‰ΩøÁî®Ë™ûË®ÄÊ®°Âûã (LM) Âú® 3D Á©∫Èñì‰∏≠ÁîüÊàêÂàÜÂ≠êÔºåÈÄôÈúÄË¶ÅÂ∞ç 3D ÂàÜÂ≠êÂπæ‰ΩïÁµêÊßãÈÄ≤Ë°åÈõ¢Êï£ÁöÑÊ®ôË®òÂåñ„ÄÇÂÑòÁÆ°Â≠òÂú®ÂàÜÂ≠êÂúñÁöÑÊ®ôË®òÂåñÔºå‰ΩÜÂ∞ç 3D Âπæ‰ΩïÁµêÊßãÁöÑÊ®ôË®òÂåñÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂ∞öÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂòóË©¶ÈÄöÈÅéÊèêÂá∫ Geo2Seq ‰æÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåË©≤ÊñπÊ≥ïÂ∞áÂàÜÂ≠êÂπæ‰ΩïÁµêÊßãËΩâÊèõÁÇ∫ $SE(3)$ ‰∏çËÆäÁöÑ 1D Èõ¢Êï£Â∫èÂàó„ÄÇGeo2Seq ÂåÖÂê´Ë¶èÁØÑÊ®ôÁ±§Âíå‰∏çËÆäÁêÉÈù¢Ë°®Á§∫Ê≠•È©üÔºåÂÆÉÂÄëÂÖ±Âêå‰ª•ÊúâÂà©Êñº LM ÁöÑÊ†ºÂºè‰øùÊåÅÂπæ‰ΩïÂíåÂéüÂ≠ê‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÁï∂Ëàá Geo2Seq ÁµêÂêà‰ΩøÁî®ÊôÇÔºåÂêÑÁ®Æ LM Âú®ÂàÜÂ≠êÂπæ‰ΩïÁîüÊàêÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÁâπÂà•ÊòØÂú®ÂèóÊéßÁîüÊàê‰ªªÂãô‰∏≠„ÄÇ

##### **GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**
2408.10115v1 by Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang

Pre-trained language models are increasingly being used in multi-document
summarization tasks. However, these models need large-scale corpora for
pre-training and are domain-dependent. Other non-neural unsupervised
summarization approaches mostly rely on key sentence extraction, which can lead
to information loss. To address these challenges, we propose a lightweight yet
effective unsupervised approach called GLIMMER: a Graph and LexIcal features
based unsupervised Multi-docuMEnt summaRization approach. It first constructs a
sentence graph from the source documents, then automatically identifies
semantic clusters by mining low-level features from raw texts, thereby
improving intra-cluster correlation and the fluency of generated sentences.
Finally, it summarizes clusters into natural sentences. Experiments conducted
on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach
outperforms existing unsupervised approaches. Furthermore, it surpasses
state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS
and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,
human evaluations indicate that summaries generated by GLIMMER achieve high
readability and informativeness scores. Our code is available at
https://github.com/Oswald1997/GLIMMER.

ÊëòË¶ÅÔºöÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊñá‰ª∂ÊëòË¶Å‰ªªÂä°‰∏≠Ë¢´Ë∂äÊù•Ë∂äÂ§öÂú∞‰ΩøÁî®„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÈúÄË¶ÅÂ§ßËßÑÊ®°ËØ≠ÊñôÂ∫ìËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂‰∏î‰æùËµñ‰∫éÈ¢ÜÂüü„ÄÇÂÖ∂‰ªñÈùûÁ•ûÁªèÊó†ÁõëÁù£ÊëòË¶ÅÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂÖ≥ÈîÆÂè•Â≠êÊèêÂèñÔºåËøôÂèØËÉΩÂØºËá¥‰ø°ÊÅØ‰∏¢Â§±„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ß‰ΩÜÊúâÊïàÁöÑÊó†ÁõëÁù£ÊñπÊ≥ïÔºåÁß∞‰∏∫ GLIMMERÔºö‰∏ÄÁßçÂü∫‰∫éÂõæÂíåËØçÊ±áÁâπÂæÅÁöÑÊó†ÁõëÁù£Â§öÊñáÊ°£ÊëòË¶ÅÊñπÊ≥ï„ÄÇÂÆÉÈ¶ñÂÖà‰ªéÊ∫êÊñáÊ°£ÊûÑÂª∫‰∏Ä‰∏™Âè•Â≠êÂõæÔºåÁÑ∂ÂêéÈÄöËøá‰ªéÂéüÂßãÊñáÊú¨‰∏≠ÊåñÊéò‰ΩéÁ∫ßÁâπÂæÅËá™Âä®ËØÜÂà´ËØ≠‰πâÁ∞áÔºå‰ªéËÄåÊèêÈ´òÁ∞áÂÜÖÁõ∏ÂÖ≥ÊÄßÂíåÁîüÊàêÂè•Â≠êÁöÑÊµÅÁïÖÊÄß„ÄÇÊúÄÂêéÔºåÂÆÉÂ∞ÜÁ∞áÊÄªÁªì‰∏∫Ëá™ÁÑ∂Âè•Â≠ê„ÄÇÂú® Multi-News„ÄÅMulti-XScience Âíå DUC-2004 ‰∏äËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ºò‰∫éÁé∞ÊúâÁöÑÊó†ÁõëÁù£ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂú®Èõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÔºåÂÆÉÂú® ROUGE ÂæóÂàÜÊñπÈù¢Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÈ¢ÑËÆ≠ÁªÉÂ§öÊñáÊ°£ÊëòË¶ÅÊ®°ÂûãÔºà‰æãÂ¶Ç PEGASUS Âíå PRIMERAÔºâ„ÄÇÊ≠§Â§ñÔºå‰∫∫Á±ªËØÑ‰º∞Ë°®ÊòéÔºåGLIMMER ÁîüÊàêÁöÑÊëòË¶ÅËé∑Âæó‰∫ÜÂæàÈ´òÁöÑÂèØËØªÊÄßÂíå‰ø°ÊÅØÊÄßÂæóÂàÜ„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂèØÂú® https://github.com/Oswald1997/GLIMMER Ëé∑Âæó„ÄÇ

##### **SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**
2408.09717v1 by Pengjie Liu, Wang Zhang, Yulong Ding, Xuefeng Zhang, Shuang-Hua Yang

Legal Judgment Prediction (LJP) aims to form legal judgments based on the
criminal fact description. However, researchers struggle to classify confusing
criminal cases, such as robbery and theft, which requires LJP models to
distinguish the nuances between similar crimes. Existing methods usually design
handcrafted features to pick up necessary semantic legal clues to make more
accurate legal judgment predictions. In this paper, we propose a Semantic-Aware
Dual Encoder Model (SEMDR), which designs a novel legal clue tracing mechanism
to conduct fine-grained semantic reasoning between criminal facts and
instruments. Our legal clue tracing mechanism is built from three reasoning
levels: 1) Lexicon-Tracing, which aims to extract criminal facts from criminal
descriptions; 2) Sentence Representation Learning, which contrastively trains
language models to better represent confusing criminal facts; 3) Multi-Fact
Reasoning, which builds a reasons graph to propagate semantic clues among fact
nodes to capture the subtle difference among criminal facts. Our legal clue
tracing mechanism helps SEMDR achieve state-of-the-art on the CAIL2018 dataset
and shows its advance in few-shot scenarios. Our experiments show that SEMDR
has a strong ability to learn more uniform and distinguished representations
for criminal facts, which helps to make more accurate predictions on confusing
criminal cases and reduces the model uncertainty during making judgments. All
codes will be released via GitHub.

ÊëòË¶ÅÔºöÊ≥ïÂæãÂà§Ê±∫È†êÊ∏¨ (LJP) Êó®Âú®Ê†πÊìöÁäØÁΩ™‰∫ãÂØ¶ÊèèËø∞ÂΩ¢ÊàêÊ≥ïÂæãÂà§Ê±∫„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂‰∫∫Âì°Èõ£‰ª•Â∞çÊê∂Âä´ÂíåÁõúÁ´äÁ≠â‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂàë‰∫ãÊ°à‰ª∂ÈÄ≤Ë°åÂàÜÈ°ûÔºåÈÄôÈúÄË¶Å LJP Ê®°ÂûãÂçÄÂàÜÈ°û‰ººÁäØÁΩ™‰πãÈñìÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏Ë®≠Ë®àÊâãÂ∑•ÁâπÂæµ‰ª•Áç≤ÂèñÂøÖË¶ÅÁöÑË™ûÁæ©Ê≥ïÂæãÁ∑öÁ¥¢Ôºå‰ª•ÂÅöÂá∫Êõ¥Ê∫ñÁ¢∫ÁöÑÊ≥ïÂæãÂà§Ê±∫È†êÊ∏¨„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË™ûÁæ©ÊÑüÁü•ÈõôÁ∑®Á¢ºÂô®Ê®°Âûã (SEMDR)ÔºåÂÆÉË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Ôºå‰ª•Âú®ÁäØÁΩ™‰∫ãÂØ¶ÂíåÂ∑•ÂÖ∑‰πãÈñìÈÄ≤Ë°åÁ¥∞Á≤íÂ∫¶ÁöÑË™ûÁæ©Êé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Âª∫Á´ãÂú®‰∏âÂÄãÊé®ÁêÜÂ±§Á¥ö‰πã‰∏äÔºö1) Ë©ûÂΩôËøΩËπ§ÔºåÊó®Âú®ÂæûÁäØÁΩ™ÊèèËø∞‰∏≠ÊèêÂèñÁäØÁΩ™‰∫ãÂØ¶Ôºõ2) Âè•Â≠êË°®Á§∫Â≠∏ÁøíÔºåÂ∞çÊØîË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã‰ª•Êõ¥Â•ΩÂú∞Ë°®Á§∫‰ª§‰∫∫Âõ∞ÊÉëÁöÑÁäØÁΩ™‰∫ãÂØ¶Ôºõ3) Â§ö‰∫ãÂØ¶Êé®ÁêÜÔºåÊßãÂª∫‰∏ÄÂÄãÂéüÂõ†ÂúñÔºåÂú®‰∫ãÂØ¶ÁØÄÈªû‰πãÈñìÂÇ≥Êí≠Ë™ûÁæ©Á∑öÁ¥¢Ôºå‰ª•ÊçïÊçâÁäØÁΩ™‰∫ãÂØ¶‰πãÈñìÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•„ÄÇÊàëÂÄëÁöÑÊ≥ïÂæãÁ∑öÁ¥¢ËøΩËπ§Ê©üÂà∂Âπ´Âä© SEMDR Âú® CAIL2018 Ë≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊäÄË°ìÔºå‰∏¶Â±ïÁ§∫‰∫ÜÂÖ∂Âú®Â∞ëÈè°È†≠Â†¥ÊôØ‰∏≠ÁöÑÈÄ≤Ê≠•„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåSEMDR ÂÖ∑ÊúâÂ≠∏ÁøíÊõ¥Áµ±‰∏ÄÂíåÂçÄÂà•ÁöÑÁäØÁΩ™‰∫ãÂØ¶Ë°®Á§∫ÁöÑÂº∑Â§ßËÉΩÂäõÔºåÈÄôÊúâÂä©ÊñºÂ∞ç‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂàë‰∫ãÊ°à‰ª∂ÂÅöÂá∫Êõ¥Ê∫ñÁ¢∫ÁöÑÈ†êÊ∏¨Ôºå‰∏¶Âú®ÂÅöÂá∫Âà§Ê±∫ÊôÇÊ∏õÂ∞ëÊ®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÊâÄÊúâ‰ª£Á¢ºÈÉΩÂ∞áÈÄöÈÅé GitHub ÁôºÂ∏É„ÄÇ

##### **Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**
2408.09529v1 by Xinnan Dai, Qihao Wen, Yifei Shen, Hongzhi Wen, Dongsheng Li, Jiliang Tang, Caihua Shan

Large Language Models (LLMs) have achieved great success in various reasoning
tasks. In this work, we focus on the graph reasoning ability of LLMs. Although
theoretical studies proved that LLMs are capable of handling graph reasoning
tasks, empirical evaluations reveal numerous failures. To deepen our
understanding on this discrepancy, we revisit the ability of LLMs on three
fundamental graph tasks: graph description translation, graph connectivity, and
the shortest-path problem. Our findings suggest that LLMs can fail to
understand graph structures through text descriptions and exhibit varying
performance for all these three fundamental tasks. Meanwhile, we perform a
real-world investigation on knowledge graphs and make consistent observations
with our findings. The codes and datasets are available.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÊé®ÁêÜ‰ªªÂãô‰∏≠Â∑≤ÂèñÂæóÂ∑®Â§ßÁöÑÊàêÂäü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº LLM ÁöÑÂúñÂΩ¢Êé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ°ÁêÜË´ñÁ†îÁ©∂Ë≠âÊòé LLM ÊúâËÉΩÂäõËôïÁêÜÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÔºå‰ΩÜÁ∂ìÈ©óË©ï‰º∞È°ØÁ§∫Âá∫Ë®±Â§öÂ§±Êïó„ÄÇÁÇ∫‰∫ÜÂä†Ê∑±ÊàëÂÄëÂ∞çÈÄôÁ®ÆÂ∑ÆÁï∞ÁöÑÁêÜËß£ÔºåÊàëÂÄëÈáçÊñ∞Êé¢Ë®é LLM Âú®‰∏âÂÄãÂü∫Êú¨ÂúñÂΩ¢‰ªªÂãô‰∏äÁöÑËÉΩÂäõÔºöÂúñÂΩ¢ÊèèËø∞ÁøªË≠Ø„ÄÅÂúñÂΩ¢ÈÄ£ÈÄöÊÄßÂíåÊúÄÁü≠Ë∑ØÂæëÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåLLM ÂèØËÉΩÁÑ°Ê≥ïÈÄöÈÅéÊñáÊú¨ÊèèËø∞ÁêÜËß£ÂúñÂΩ¢ÁµêÊßãÔºå‰∏¶‰∏îÂú®ÊâÄÊúâÈÄô‰∏âÂÄãÂü∫Êú¨‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÊÄßËÉΩ„ÄÇÂêåÊôÇÔºåÊàëÂÄëÂ∞çÁü•Ë≠òÂúñË≠úÈÄ≤Ë°å‰∫ÜÁèæÂØ¶‰∏ñÁïåÁöÑË™øÊü•Ôºå‰∏¶Â∞çÊàëÂÄëÁöÑÁôºÁèæÈÄ≤Ë°å‰∫Ü‰∏ÄËá¥ÁöÑËßÄÂØü„ÄÇ‰ª£Á¢ºÂíåÊï∏ÊìöÈõÜÂèØÁî®„ÄÇ

##### **Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**
2408.13273v1 by Geethan Sannidhi, Sagar Srinivas Sakhinana, Venkataramana Runkana

Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàPLLMÔºâÔºå‰æãÂ¶Ç OpenAI ChatGPT Âíå Google
Gemini Èù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶Ç‰∏çÊ∫ñÁ¢∫ÁöÑ‰∫ãÂØ¶ÂõûÊÜ∂„ÄÅÂπªË¶∫„ÄÅ
ÂÅèË¶ãÂíåÊôÇÈñìÁü•Ë≠òÂúñÔºàtKGÔºâÈ†êÊ∏¨ÁöÑÊú™‰æÜÊï∏ÊìöÊ¥©Êºè„ÄÇ
ÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü sLA-tKGFÔºàtKG È†êÊ∏¨ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÂä©ÁêÜÔºâÔºåÂÆÉÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâ
ËºîÂä©ÔºåÂæûÈ†≠ÈñãÂßãÈÄöÈÅéÁôΩÊùøÊ≥ïËá™Ë®ÇË®ìÁ∑¥ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÊ®°ÂûãÔºå‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑ tKG È†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂª∫Êßã
Ê≥®ÂÖ•Áü•Ë≠òÁöÑÊèêÁ§∫ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ tKG„ÄÅÁ∂≤Ë∑ØÊêúÂ∞ã
ÁµêÊûúÂíå PLLM ÁîüÊàêÁöÑÊñáÂ≠óÊèèËø∞Ôºå‰ª•‰∫ÜËß£ÁõÆÊ®ôÊôÇÈñì‰πãÂâçÁöÑÊ≠∑Âè≤ÂØ¶È´îÈóú‰øÇ„ÄÇÂÆÉÂà©Áî®ÈÄô‰∫õÂ§ñÈÉ®
Ê≥®ÂÖ•Áü•Ë≠òÁöÑÊèêÁ§∫Ôºå‰ª•Êõ¥Ê∑±ÂÖ•Âú∞ÁêÜËß£ÂíåÊé®ÁêÜ
ÁâπÂÆöÊñºËÑàÁµ°ÁöÑË™ûÁæ©ÂíåÊôÇÈñìË≥áË®äÔºå‰ª•Èõ∂Ê¨°ÊèêÁ§∫Â∞èË¶èÊ®°Ë™ûË®ÄÊ®°ÂûãÔºå‰ª•Êõ¥Ê∫ñÁ¢∫Âú∞È†êÊ∏¨ tKG ‰∏≠ÁöÑÊú™‰æÜ‰∫ã‰ª∂„ÄÇÂÆÉÊ∏õÂ∞ëÂπªË¶∫‰∏¶ÈÄèÈÅé‰∫ÜËß£Èö®ÊôÇÈñìËÆäÂåñÁöÑË∂®Âã¢‰æÜÊ∏õËºïÂàÜ‰ΩàËΩâÁßªÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÂÆÉ
ËÉΩÊõ¥Ê∫ñÁ¢∫‰∏îÊúâËÑàÁµ°Âú∞È†êÊ∏¨Êú™‰æÜ‰∫ã‰ª∂ÔºåÂêåÊôÇÂ∞áÈÅãÁÆóÈúÄÊ±ÇÈôçËá≥ÊúÄ‰Ωé„ÄÇÂö¥Ë¨πÁöÑÂØ¶Ë≠âÁ†îÁ©∂Ë≠âÊòé
ÊàëÂÄëÁöÑÊû∂ÊßãÂÖ∑ÊúâÁ©©ÂÅ•ÊÄß„ÄÅÂèØÊì¥ÂÖÖÊÄßÂíåÊúÄÂÖàÈÄ≤ÔºàSOTAÔºâÊïàËÉΩ
Âú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂèØËß£Èáã‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑ tKG È†êÊ∏¨„ÄÇ

##### **Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**
2408.09429v1 by Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu

Hallucination issues persistently plagued current multimodal large language
models (MLLMs). While existing research primarily focuses on object-level or
attribute-level hallucinations, sidelining the more sophisticated relation
hallucinations that necessitate advanced reasoning abilities from MLLMs.
Besides, recent benchmarks regarding relation hallucinations lack in-depth
evaluation and effective mitigation. Moreover, their datasets are typically
derived from a systematic annotation process, which could introduce inherent
biases due to the predefined process. To handle the aforementioned challenges,
we introduce Reefknot, a comprehensive benchmark specifically targeting
relation hallucinations, consisting of over 20,000 samples derived from
real-world scenarios. Specifically, we first provide a systematic definition of
relation hallucinations, integrating perspectives from perceptive and cognitive
domains. Furthermore, we construct the relation-based corpus utilizing the
representative scene graph dataset Visual Genome (VG), from which semantic
triplets follow real-world distributions. Our comparative evaluation across
three distinct tasks revealed a substantial shortcoming in the capabilities of
current MLLMs to mitigate relation hallucinations. Finally, we advance a novel
confidence-based mitigation strategy tailored to tackle the relation
hallucinations problem. Across three datasets, including Reefknot, we observed
an average reduction of 9.75% in the hallucination rate. We believe our paper
sheds valuable insights into achieving trustworthy multimodal intelligence. Our
dataset and code will be released upon paper acceptance.

ÊëòË¶ÅÔºöÂπªË¶∫ÂïèÈ°åÊåÅÁ∫åÂõ∞ÊìæËëóÁï∂ÂâçÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM)„ÄÇÈõñÁÑ∂ÁèæÊúâÁ†îÁ©∂‰∏ªË¶ÅÈóúÊ≥®Áâ©‰ª∂Â±§Á¥öÊàñÂ±¨ÊÄßÂ±§Á¥öÁöÑÂπªË¶∫Ôºå‰ΩÜÂçªÂøΩË¶ñ‰∫ÜÈúÄË¶Å MLLM ÂÖ∑ÂÇôÈÄ≤ÈöéÊé®ÁêÜËÉΩÂäõÁöÑÊõ¥Ë§áÈõúÈóú‰øÇÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÈóúÊñºÈóú‰øÇÂπªË¶∫ÁöÑÊúÄÊñ∞Âü∫Ê∫ñÁº∫‰πèÊ∑±ÂÖ•Ë©ï‰º∞ÂíåÊúâÊïàÁöÑÁ∑©Ëß£Êé™ÊñΩ„ÄÇËÄå‰∏îÔºå‰ªñÂÄëÁöÑË≥áÊñôÈõÜÈÄöÂ∏∏‰æÜËá™Á≥ªÁµ±ÂåñÁöÑË®ªÈáãÈÅéÁ®ãÔºåÈÄôÂèØËÉΩÊúÉÂõ†ÁÇ∫È†êÂÖàÂÆöÁæ©ÁöÑÈÅéÁ®ãËÄåÂºïÂÖ•Âõ∫ÊúâÁöÑÂÅèÂ∑Æ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç‰∏äËø∞ÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ReefknotÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÈñÄÈáùÂ∞çÈóú‰øÇÂπªË¶∫ÁöÑÁ∂úÂêàÂü∫Ê∫ñÔºåÂåÖÂê´Ë∂ÖÈÅé 20,000 ÂÄã‰æÜËá™ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÁöÑÁØÑ‰æã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊèê‰æõÈóú‰øÇÂπªË¶∫ÁöÑÁ≥ªÁµ±ÊÄßÂÆöÁæ©ÔºåÊï¥Âêà‰æÜËá™Áü•Ë¶∫ÂíåË™çÁü•È†òÂüüÁöÑËßÄÈªû„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÂ†¥ÊôØÂúñÂΩ¢Ë≥áÊñôÈõÜ Visual Genome (VG) Âª∫ÊßãÂü∫ÊñºÈóú‰øÇÁöÑË™ûÊñôÂ∫´ÔºåË™ûÁæ©‰∏âÂÖÉÁµÑÈÅµÂæ™ÁúüÂØ¶‰∏ñÁïåÁöÑÂàÜ‰Ωà„ÄÇÊàëÂÄëÂú®‰∏âÂÄã‰∏çÂêåÁöÑ‰ªªÂãô‰∏≠ÈÄ≤Ë°åÊØîËºÉË©ï‰º∞ÔºåÊè≠Á§∫‰∫ÜÁï∂Ââç MLLM Âú®Ê∏õËºïÈóú‰øÇÂπªË¶∫ÊñπÈù¢ÁöÑËÉΩÂäõÂ≠òÂú®ÈáçÂ§ßÁº∫Èô∑„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫Êñº‰ø°ÂøÉÁöÑÁ∑©Ëß£Á≠ñÁï•ÔºåÂ∞àÈñÄÁî®ÊñºËß£Ê±∫Èóú‰øÇÂπªË¶∫ÂïèÈ°å„ÄÇÂú®ÂåÖÊã¨ Reefknot Âú®ÂÖßÁöÑ‰∏âÂÄãË≥áÊñôÈõÜ‰∏≠ÔºåÊàëÂÄëËßÄÂØüÂà∞ÂπªË¶∫ÁéáÂπ≥ÂùáÈôç‰Ωé‰∫Ü 9.75%„ÄÇÊàëÂÄëÁõ∏‰ø°ÊàëÂÄëÁöÑË´ñÊñáÂ∞çÂØ¶ÁèæÂÄºÂæó‰ø°Ë≥¥ÁöÑÂ§öÊ®°ÊÖãÊô∫ÊÖßÊèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÂ∞áÂú®Ë´ñÊñáË¢´Êé•ÂèóÂæåÁôºÂ∏É„ÄÇ

##### **ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**
2408.08972v1 by Debashis Gupta, Aditi Golder, Luis Fernendez, Miles Silman, Greg Lersen, Fan Yang, Bob Plemmons, Sarra Alqahtani, Paul Victor Pauca

Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly
destructive mining practice, leading to environmental disasters across the
world's tropical watersheds. The topic of ASGM spans multiple domains of
research and information, including natural and social systems, and knowledge
is often atomized across a diversity of media and documents. We therefore
introduce a knowledge graph (ASGM-KG) that consolidates and provides crucial
information about ASGM practices and their environmental effects. The current
version of ASGM-KG consists of 1,899 triples extracted using a large language
model (LLM) from documents and reports published by both non-governmental and
governmental organizations. These documents were carefully selected by a group
of tropical ecologists with expertise in ASGM. This knowledge graph was
validated using two methods. First, a small team of ASGM experts reviewed and
labeled triples as factual or non-factual. Second, we devised and applied an
automated factual reduction framework that relies on a search engine and an LLM
for labeling triples. Our framework performs as well as five baselines on a
publicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG
validated by domain experts. ASGM-KG demonstrates an advancement in knowledge
aggregation and representation for complex, interdisciplinary environmental
crises such as ASGM.

ÊëòË¶ÅÔºöÊâãÂ∑•ÂíåÂ∞èÂûãÊé°ÈáëÔºàASGMÔºâÊòØ‰∏ÄÁ®Æ‰ΩéÊàêÊú¨‰ΩÜÈ´òÂ∫¶Á†¥Â£ûÊÄßÁöÑÊé°Á§¶ÂØ¶ÂãôÔºåÂ∞éËá¥ÂÖ®ÁêÉÁÜ±Â∏∂ÊµÅÂüüÁôºÁîüÁí∞Â¢ÉÁÅΩÈõ£„ÄÇASGM ÁöÑ‰∏ªÈ°åÊ∂µËìãÂ§öÂÄãÁ†îÁ©∂ÂíåË≥áË®äÈ†òÂüüÔºåÂåÖÊã¨Ëá™ÁÑ∂ÂíåÁ§æÊúÉÁ≥ªÁµ±ÔºåËÄåÁü•Ë≠òÈÄöÂ∏∏ÂàÜÊï£Âú®ÂêÑÁ®ÆÂ™íÈ´îÂíåÊñá‰ª∂‰∏≠„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•Áü•Ë≠òÂúñË≠ú (ASGM-KG)ÔºåÂÆÉÊï¥Âêà‰∏¶Êèê‰æõÊúâÈóú ASGM ÂØ¶ÂãôÂèäÂÖ∂Áí∞Â¢ÉÂΩ±ÈüøÁöÑÈáçË¶ÅË≥áË®ä„ÄÇÁõÆÂâçÁâàÊú¨ÁöÑ ASGM-KG ÂåÖÂê´ 1,899 ÂÄã‰∏âÂÖÉÁµÑÔºåÈÄô‰∫õ‰∏âÂÖÉÁµÑÊòØ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂæûÈùûÊîøÂ∫úÁµÑÁπîÂíåÊîøÂ∫úÁµÑÁπîÁôºÂ∏ÉÁöÑÊñá‰ª∂ÂíåÂ†±Âëä‰∏≠ÊèêÂèñÂá∫‰æÜÁöÑ„ÄÇÈÄô‰∫õÊñá‰ª∂ÊòØÁî±‰∏ÄÁæ§ÂÖ∑Êúâ ASGM Â∞àÊ•≠Áü•Ë≠òÁöÑÁÜ±Â∏∂ÁîüÊÖãÂ≠∏ÂÆ∂‰ªîÁ¥∞ÊåëÈÅ∏ÁöÑ„ÄÇÈÄôÂÄãÁü•Ë≠òÂúñË≠ú‰ΩøÁî®ÂÖ©Á®ÆÊñπÊ≥ïÈ©óË≠â„ÄÇÈ¶ñÂÖàÔºå‰∏ÄÂ∞èÁµÑ ASGM Â∞àÂÆ∂ÂØ©Êü•‰∏¶Â∞á‰∏âÂÖÉÁµÑÊ®ôË®òÁÇ∫‰∫ãÂØ¶ÊàñÈùû‰∫ãÂØ¶„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëË®≠Ë®à‰∏¶ÊáâÁî®‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñÁöÑ‰∫ãÂØ¶Á∞°ÂåñÊû∂ÊßãÔºåË©≤Êû∂Êßã‰æùË≥¥ÊñºÊêúÂ∞ãÂºïÊìéÂíå LLM ‰æÜÊ®ôË®ò‰∏âÂÖÉÁµÑ„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®ÂÖ¨ÈñãÁöÑÁü•Ë≠òÂúñË≠ú‰∏äÂü∑Ë°åÂæóËàá‰∫îÂÄãÂü∫Ê∫ñ‰∏ÄÊ®£Â•ΩÔºå‰∏¶Âú®ÊàëÂÄëÁî±È†òÂüüÂ∞àÂÆ∂È©óË≠âÁöÑ ASGM-KG ‰∏äÈÅîÂà∞Ë∂ÖÈÅé 90 ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇASGM-KG Â±ïÁ§∫‰∫ÜË§áÈõúÁöÑË∑®Â≠∏ÁßëÁí∞Â¢ÉÂç±Ê©üÔºà‰æãÂ¶Ç ASGMÔºâÁöÑÁü•Ë≠òÂΩôÊï¥ÂíåË°®Á§∫ÊñπÈù¢ÁöÑ‰∏ÄÈ†ÖÈÄ≤Â±ï„ÄÇ

##### **EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**
2408.08782v1 by Chenwei Wan, Matthieu Labeau, Chlo√© Clavel

Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Previous efforts have focused on developing modular dialogue systems that treat
socio-emotional strategy prediction as an auxiliary task and generate
strategy-conditioned responses with customized decoders. Recently, with
advancements in large language models (LLMs), end-to-end dialogue agents
without explicit socio-emotional strategy prediction steps have become
prevalent. However, despite their excellence in language generation, recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy predictor,
EmoDynamiX, which models the discourse dynamics between user emotions and
system strategies using a heterogeneous graph. Additionally, we make use of the
Emotion Recognition in Conversations (ERC) task and design a flexible
mixed-emotion module to capture fine-grained emotional states of the user.
Experimental results on two ESC datasets show EmoDynamiX outperforms previous
state-of-the-art methods with a significant margin.

ÊëòË¶ÅÔºöË®≠Ë®àÊÉÖÁ∑íÊô∫ËÉΩÂ∞çË©±Á≥ªÁµ±‰ª•Êèê‰æõÂÆâÊÖ∞ÂíåÂª∫Ë≠∞Áµ¶Á∂ìÊ≠∑ÁóõËã¶ÁöÑ‰∫∫ÊòØ‰∏ÄÂÄãÂºï‰∫∫ÂÖ•ÂãùÁöÑÁ†îÁ©∂È†òÂüü„ÄÇ
ÂÖàÂâçÁöÑÂä™ÂäõÈõÜ‰∏≠ÊñºÈñãÁôºÊ®°ÁµÑÂåñÂ∞çË©±Á≥ªÁµ±ÔºåÂ∞áÁ§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•È†êÊ∏¨Ë¶ñÁÇ∫ËºîÂä©‰ªªÂãôÔºå‰∏¶‰ΩøÁî®Ëá™Ë®ÇËß£Á¢ºÂô®Áî¢ÁîüÁ≠ñÁï•Ê¢ù‰ª∂ÂåñÁöÑÂõûÊáâ„ÄÇÊúÄËøëÔºåÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÊ≤íÊúâÊòéÁ¢∫Á§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•È†êÊ∏¨Ê≠•È©üÁöÑÁ´ØÂà∞Á´ØÂ∞çË©±‰ª£ÁêÜÂ∑≤ËÆäÂæóÊôÆÈÅç„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°ÂÆÉÂÄëÂú®Ë™ûË®ÄÁîüÊàêÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLLM Â∞çÊüê‰∫õÁ§æÊúÉÊÉÖÁ∑íÁ≠ñÁï•ÁöÑÂõ∫ÊúâÂÅèÂ•ΩÊúÉÈòªÁ§ôÊèê‰æõÈ´òÂìÅË≥™ÁöÑÊÉÖÁ∑íÊîØÊåÅ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÂª∫Ë≠∞Â∞áÁ≠ñÁï•È†êÊ∏¨ËàáË™ûË®ÄÁîüÊàêËß£ËÄ¶Ôºå‰∏¶ÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ∞çË©±Á≠ñÁï•È†êÊ∏¨Âô® EmoDynamiXÔºåÂÆÉ‰ΩøÁî®Áï∞Ë≥™ÂúñÂΩ¢Â∞ç‰ΩøÁî®ËÄÖÊÉÖÁ∑íÂíåÁ≥ªÁµ±Á≠ñÁï•‰πãÈñìÁöÑË©±Ë™ûÂãïÊÖãÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Â∞çË©±‰∏≠ÁöÑÊÉÖÁ∑íËæ®Ë≠ò (ERC) ‰ªªÂãô‰∏¶Ë®≠Ë®à‰∫Ü‰∏ÄÂÄãÈùàÊ¥ªÁöÑÊ∑∑ÂêàÊÉÖÁ∑íÊ®°ÁµÑ‰æÜÊçïÊçâ‰ΩøÁî®ËÄÖÁöÑÁ¥∞Á∑ªÊÉÖÁ∑íÁãÄÊÖã„ÄÇÂú®ÂÖ©ÂÄã ESC Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåEmoDynamiX ‰ª•È°ØËëóÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ

##### **Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**
2408.08685v1 by Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi

Graph neural networks (GNNs) are vulnerable to adversarial perturbations,
especially for topology attacks, and many methods that improve the robustness
of GNNs have received considerable attention. Recently, we have witnessed the
significant success of large language models (LLMs), leading many to explore
the great potential of LLMs on GNNs. However, they mainly focus on improving
the performance of GNNs by utilizing LLMs to enhance the node features.
Therefore, we ask: Will the robustness of GNNs also be enhanced with the
powerful understanding and inference capabilities of LLMs? By presenting the
empirical results, we find that despite that LLMs can improve the robustness of
GNNs, there is still an average decrease of 23.1% in accuracy, implying that
the GNNs remain extremely vulnerable against topology attack. Therefore,
another question is how to extend the capabilities of LLMs on graph adversarial
robustness. In this paper, we propose an LLM-based robust graph structure
inference framework, LLM4RGNN, which distills the inference capabilities of
GPT-4 into a local LLM for identifying malicious edges and an LM-based edge
predictor for finding missing important edges, so as to recover a robust graph
structure. Extensive experiments demonstrate that LLM4RGNN consistently
improves the robustness across various GNNs. Even in some cases where the
perturbation ratio increases to 40%, the accuracy of GNNs is still better than
that on the clean graph.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊìæÂãïÁöÑÂΩ±ÈüøÔºå
ÁâπÂà•ÊòØÊãìÊí≤ÊîªÊìäÔºåË®±Â§öÊîπÂñÑ GNN È≠ØÊ£íÊÄßÁöÑÊñπÊ≥ïÈÉΩÂÇôÂèóÈóúÊ≥®„ÄÇÊúÄËøëÔºåÊàëÂÄëË¶ãË≠â‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈ°ØËëóÊàêÂäüÔºåÂ∞éËá¥Ë®±Â§ö‰∫∫Êé¢Á¥¢ LLM Âú® GNN ‰∏äÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄë‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂà©Áî® LLM Â¢ûÂº∑ÁØÄÈªûÁâπÂæµ‰æÜÊîπÂñÑ GNN ÁöÑÊïàËÉΩ„ÄÇ
Âõ†Ê≠§ÔºåÊàëÂÄëÂïèÔºöLLM Âº∑Â§ßÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÊòØÂê¶‰πüÊúÉÂ¢ûÂº∑ GNN ÁöÑÈ≠ØÊ£íÊÄßÔºüÈÄèÈÅéÂëàÁèæÂØ¶Ë≠âÁµêÊûúÔºåÊàëÂÄëÁôºÁèæÂÑòÁÆ° LLM ÂèØ‰ª•ÊîπÂñÑ GNN ÁöÑÈ≠ØÊ£íÊÄßÔºå‰ΩÜÊ∫ñÁ¢∫Â∫¶‰ªçÂπ≥Âùá‰∏ãÈôç 23.1%ÔºåÈÄôË°®Á§∫ GNN ‰ªçÁÑ∂Ê•µÂÆπÊòìÂèóÂà∞ÊãìÊí≤ÊîªÊìä„ÄÇÂõ†Ê≠§ÔºåÂè¶‰∏ÄÂÄãÂïèÈ°åÊòØÂ¶Ç‰ΩïÊì¥Â±ï LLM Âú®ÂúñÂΩ¢Â∞çÊäóÈ≠ØÊ£íÊÄß‰∏äÁöÑËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÈ≠ØÊ£íÂúñÂΩ¢ÁµêÊßãÊé®ÁêÜÊ°ÜÊû∂ LLM4RGNNÔºåÂÆÉÂ∞á GPT-4 ÁöÑÊé®ÁêÜËÉΩÂäõÊèêÁÖâÊàê‰∏ÄÂÄãÁî®ÊñºË≠òÂà•ÊÉ°ÊÑèÈÇäÁ∑£ÁöÑÊú¨Âú∞ LLMÔºå‰ª•Âèä‰∏ÄÂÄãÁî®ÊñºÂ∞ãÊâæÈÅ∫Â§±ÈáçË¶ÅÈÇäÁ∑£ÁöÑÂü∫Êñº LM ÁöÑÈÇäÁ∑£È†êÊ∏¨Âô®Ôºå‰ª•‰æøÊÅ¢Âæ©‰∏ÄÂÄãÈ≠ØÊ£íÁöÑÂúñÂΩ¢ÁµêÊßã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåLLM4RGNN ÊåÅÁ∫åÊîπÂñÑÂêÑÁ®Æ GNN ÁöÑÈ≠ØÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êüê‰∫õÊìæÂãïÁéáÂ¢ûÂä†Âà∞ 40% ÁöÑÊÉÖÊ≥Å‰∏ãÔºåGNN ÁöÑÊ∫ñÁ¢∫Â∫¶‰ªçÁÑ∂ÂÑ™Êñº‰πæÊ∑®ÂúñÂΩ¢„ÄÇ

##### **RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**
2408.08933v1 by Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, X. Sean Wang

Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical
component in many applications, including recommendation systems and large
language model-based applications. With the advancement of multimodal neural
models, which transform data from different modalities into a shared
high-dimensional space as feature vectors, cross-modal ANNS aims to use the
data vector from one modality (e.g., texts) as the query to retrieve the most
similar items from another (e.g., images or videos). However, there is an
inherent distribution gap between embeddings from different modalities, and
cross-modal queries become Out-of-Distribution (OOD) to the base data.
Consequently, state-of-the-art ANNS approaches suffer poor performance for OOD
workloads. In this paper, we quantitatively analyze the properties of the OOD
workloads to gain an understanding of their ANNS efficiency. Unlike
single-modal workloads, we reveal OOD queries spatially deviate from base data,
and the k-nearest neighbors of an OOD query are distant from each other in the
embedding space. The property breaks the assumptions of existing ANNS
approaches and mismatches their design for efficient search. With insights from
the OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an
efficient ANNS graph index built under the guidance of query distribution.
Extensive experiments show that RoarGraph significantly outperforms
state-of-the-art approaches on modern cross-modal datasets, achieving up to
3.56x faster search speed at a 90% recall rate for OOD queries.

ÊëòË¶ÅÔºöËøë‰ººÊúÄËøëÈÇªÊêúÁ¥¢ (ANNS) ÊòØËÆ∏Â§öÂ∫îÁî®Á®ãÂ∫è‰∏≠ÁöÑÂü∫Êú¨ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåÂåÖÊã¨Êé®ËçêÁ≥ªÁªüÂíåÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇÈöèÁùÄÂ§öÊ®°ÊÄÅÁ•ûÁªèÊ®°ÂûãÁöÑÂèëÂ±ïÔºåÂÆÉÂ∞ÜÊù•Ëá™‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆËΩ¨Êç¢‰∏∫ÂÖ±‰∫´ÁöÑÈ´òÁª¥Á©∫Èó¥‰Ωú‰∏∫ÁâπÂæÅÂêëÈáèÔºåË∑®Ê®°ÊÄÅ ANNS Êó®Âú®‰ΩøÁî®Êù•Ëá™‰∏Ä‰∏™Ê®°ÊÄÅÔºà‰æãÂ¶ÇÊñáÊú¨ÔºâÁöÑÊï∞ÊçÆÂêëÈáè‰Ωú‰∏∫Êü•ËØ¢Ôºå‰ª•Ê£ÄÁ¥¢Êù•Ëá™Âè¶‰∏Ä‰∏™Ê®°ÊÄÅÔºà‰æãÂ¶ÇÂõæÂÉèÊàñËßÜÈ¢ëÔºâÊúÄÁõ∏‰ººÁöÑÈ°πÁõÆ„ÄÇ‰ΩÜÊòØÔºå‰∏çÂêåÊ®°ÊÄÅÁöÑÂµåÂÖ•‰πãÈó¥Â≠òÂú®Âõ∫ÊúâÁöÑÂàÜÂ∏ÉÂ∑ÆË∑ùÔºåÂπ∂‰∏îË∑®Ê®°ÊÄÅÊü•ËØ¢ÂØπ‰∫éÂü∫Á°ÄÊï∞ÊçÆËÄåË®ÄÊàê‰∏∫ÂàÜÂ∏ÉÂ§ñ (OOD)„ÄÇÂõ†Ê≠§ÔºåÊúÄÂÖàËøõÁöÑ ANNS ÊñπÊ≥ïÂØπ‰∫é OOD Â∑•‰ΩúË¥üËΩΩÁöÑÊÄßËÉΩÂæàÂ∑Æ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂÆöÈáèÂàÜÊûê‰∫Ü OOD Â∑•‰ΩúË¥üËΩΩÁöÑÂ±ûÊÄßÔºå‰ª•‰∫ÜËß£ÂÖ∂ ANNS ÊïàÁéá„ÄÇ‰∏éÂçïÊ®°ÊÄÅÂ∑•‰ΩúË¥üËΩΩ‰∏çÂêåÔºåÊàë‰ª¨Êè≠Á§∫‰∫Ü OOD Êü•ËØ¢Âú®Á©∫Èó¥‰∏äÂÅèÁ¶ªÂü∫Á°ÄÊï∞ÊçÆÔºåÂπ∂‰∏î OOD Êü•ËØ¢ÁöÑ k ‰∏™ÊúÄËøëÈÇªÂú®ÂµåÂÖ•Á©∫Èó¥‰∏≠ÂΩºÊ≠§Áõ∏Ë∑ùÁîöËøú„ÄÇËØ•Â±ûÊÄßÊâìÁ†¥‰∫ÜÁé∞Êúâ ANNS ÊñπÊ≥ïÁöÑÂÅáËÆæÔºåÂπ∂‰∏î‰∏çÂåπÈÖçÂÆÉ‰ª¨‰∏∫È´òÊïàÊêúÁ¥¢ËÄåËÆæËÆ°ÁöÑÂÅáËÆæ„ÄÇÈÄöËøáÂØπ OOD Â∑•‰ΩúË¥üËΩΩÁöÑËßÅËß£ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü pRojected ‰∫åÂàÜÂõæ (RoarGraph)ÔºåËøôÊòØ‰∏ÄÁßçÂú®Êü•ËØ¢ÂàÜÂ∏ÉÊåáÂØº‰∏ãÊûÑÂª∫ÁöÑÈ´òÊïà ANNS ÂõæÂΩ¢Á¥¢Âºï„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåRoarGraph Âú®Áé∞‰ª£Ë∑®Ê®°ÊÄÅÊï∞ÊçÆÈõÜ‰∏äÊòéÊòæ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú® OOD Êü•ËØ¢ÁöÑ 90% Âè¨ÂõûÁéá‰∏ãÂÆûÁé∞‰∫ÜÈ´òËææ 3.56 ÂÄçÁöÑÊõ¥Âø´ÊêúÁ¥¢ÈÄüÂ∫¶„ÄÇ

##### **Handling abort commands for household kitchen robots**
2408.14480v1 by Darius Has, Adrian Groza, Mihai Pomarlan

We propose a solution for handling abort commands given to robots. The
solution is exemplified with a running scenario with household kitchen robots.
The robot uses planning to find sequences of actions that must be performed in
order to gracefully cancel a previously received command. The Planning Domain
Definition Language (PDDL) is used to write a domain to model kitchen
activities and behaviours, and this domain is enriched with knowledge from
online ontologies and knowledge graphs, like DBPedia. We discuss the results
obtained in different scenarios.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËôïÁêÜÁôºÈÄÅÁµ¶Ê©üÂô®‰∫∫ÁöÑ‰∏≠Ê≠¢ÂëΩ‰ª§ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ
ÈÄôÂÄãËß£Ê±∫ÊñπÊ°à‰ª•ÂÆ∂Áî®ÂªöÊàøÊ©üÂô®‰∫∫ÁöÑÂü∑Ë°åÊÉÖÂ¢ÉÁÇ∫‰æã„ÄÇ
Ê©üÂô®‰∫∫‰ΩøÁî®Ë¶èÂäÉ‰æÜÂ∞ãÊâæÂøÖÈ†àÂü∑Ë°åÁöÑÂãï‰ΩúÂ∫èÂàóÔºå‰ª•‰æøÂÑ™ÈõÖÂú∞ÂèñÊ∂àÂÖàÂâçÊé•Êî∂ÁöÑÂëΩ‰ª§„ÄÇË¶èÂäÉÈ†òÂüüÂÆöÁæ©Ë™ûË®Ä (PDDL) Áî®ÊñºÊí∞ÂØ´‰∏ÄÂÄãÁ∂≤Âüü‰æÜÂª∫Ê®°ÂªöÊàøÊ¥ªÂãïÂíåË°åÁÇ∫ÔºåËÄåÈÄôÂÄãÁ∂≤ÂüüÂâáÈÄèÈÅéÁ∑ö‰∏äÊú¨‰ΩìÂíåÁü•Ë≠òÂúñË°®Ôºà‰æãÂ¶Ç DBPediaÔºâÁöÑÁü•Ë≠ò‰æÜË±êÂØå„ÄÇÊàëÂÄëË®éË´ñÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠Áç≤ÂæóÁöÑÁµêÊûú„ÄÇ

##### **CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**
2408.08535v1 by Rong-Ching Chang, Jiawei Zhang

Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÊúâÈÄ≤Ê≠•Ôºå‰ΩÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÁ∂ìÂ∏∏ÂèóÂà∞Áº∫‰πèËàáÂØ¶È´îÈóú‰øÇÂíåÁ§æÁæ§ÁµêÊßãÊï¥ÂêàÁöÑÈòªÁ§ôÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÊèê‰æõËÑàÁµ°Ë±êÂØå‰∏îÊ∫ñÁ¢∫ÁöÑË≥áË®äÊ™¢Á¥¢‰ª•ÈÄ≤Ë°å‰∫ãÂØ¶Êü•Ê†∏ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄë‰ªãÁ¥π CommunityKG-RAGÔºàÁ§æÁæ§Áü•Ë≠òÂúñË≠úÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÂ∞áÁü•Ë≠òÂúñË≠ú (KG) ÂÖßÁöÑÁ§æÁæ§ÁµêÊßãËàá RAG Á≥ªÁµ±Êï¥ÂêàÔºå‰ª•Â¢ûÂº∑‰∫ãÂØ¶Êü•Ê†∏ÊµÅÁ®ã„ÄÇCommunityKG-RAG ÁÑ°ÈúÄÈ°çÂ§ñË®ìÁ∑¥Â∞±ËÉΩÈÅ©ÊáâÊñ∞ÁöÑÈ†òÂüüÂíåÊü•Ë©¢ÔºåÂÆÉÂà©Áî® KG ÂÖßÁ§æÁæ§ÁµêÊßãÁöÑÂ§öË∑≥ÁâπÊÄßÔºåÂ§ßÂπÖÊèêÂçáË≥áË®äÊ™¢Á¥¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé CommunityKG-RAG ÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºå‰ª£Ë°®Ëëó‰∫ãÂØ¶Êü•Ê†∏ÁöÑÈáçÂ§ßÈÄ≤Ê≠•ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑ÂÅ•„ÄÅÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁéáÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**
2408.08927v1 by Chia-Tung Ho, Haoxing Ren, Brucek Khailany

Due to the growing complexity of modern Integrated Circuits (ICs), automating
hardware design can prevent a significant amount of human error from the
engineering process and result in less errors. Verilog is a popular hardware
description language for designing and modeling digital systems; thus, Verilog
generation is one of the emerging areas of research to facilitate the design
process. In this work, we propose VerilogCoder, a system of multiple Artificial
Intelligence (AI) agents for Verilog code generation, to autonomously write
Verilog code and fix syntax and functional errors using collaborative Verilog
tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we
propose a task planner that utilizes a novel Task and Circuit Relation Graph
retrieval method to construct a holistic plan based on module descriptions. To
debug and fix functional errors, we develop a novel and efficient abstract
syntax tree (AST)-based waveform tracing tool, which is integrated within the
autonomous Verilog completion flow. The proposed methodology successfully
generates 94.2% syntactically and functionally correct Verilog code, surpassing
the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.

ÊëòË¶ÅÔºöÁî±ÊñºÁèæ‰ª£Êï¥ÂêàÈõªË∑Ø (IC) ÁöÑË§áÈõúÊÄßÊó•ÁõäÂ¢ûÂä†ÔºåËá™ÂãïÂåñÁ°¨È´îË®≠Ë®àÂèØ‰ª•Èò≤Ê≠¢Â∑•Á®ãÈÅéÁ®ã‰∏≠Âá∫ÁèæÂ§ßÈáèÁöÑ‰∫∫ÁÇ∫ÈåØË™§Ôºå‰∏¶Ê∏õÂ∞ëÈåØË™§„ÄÇVerilog ÊòØ‰∏ÄÁ®ÆÊµÅË°åÁöÑÁ°¨È´îÊèèËø∞Ë™ûË®ÄÔºåÁî®ÊñºË®≠Ë®àÂíåÂª∫Ê®°Êï∏‰ΩçÁ≥ªÁµ±ÔºõÂõ†Ê≠§ÔºåVerilog Áî¢ÁîüÊòØÊñ∞ËààÁöÑÁ†îÁ©∂È†òÂüü‰πã‰∏ÄÔºåÊó®Âú®‰øÉÈÄ≤Ë®≠Ë®àÈÅéÁ®ã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ VerilogCoderÔºå‰∏ÄÂÄãÁî±Â§öÂÄã‰∫∫Â∑•Êô∫ÊÖß (AI) ‰ª£ÁêÜÁµÑÊàêÁöÑÁ≥ªÁµ±ÔºåÁî®Êñº Verilog Á®ãÂºèÁ¢ºÁî¢ÁîüÔºå‰ª•Ëá™‰∏ªÊí∞ÂØ´ Verilog Á®ãÂºèÁ¢º‰∏¶‰ΩøÁî®Âçî‰ΩúÂºè Verilog Â∑•ÂÖ∑Ôºà‰æãÂ¶ÇÔºåË™ûÊ≥ïÊ™¢Êü•Âô®„ÄÅÊ®°Êì¨Âô®ÂíåÊ≥¢ÂΩ¢ËøΩËπ§Âô®Ôºâ‰øÆÂæ©Ë™ûÊ≥ïÂíåÂäüËÉΩÈåØË™§„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ªªÂãôË¶èÂäÉÂô®ÔºåÂÆÉÂà©Áî®Êñ∞Á©éÁöÑ‰ªªÂãôÂíåÈõªË∑ØÈóú‰øÇÂúñÊì∑ÂèñÊñπÊ≥ïÔºåÊ†πÊìöÊ®°ÁµÑÊèèËø∞Âª∫Êßã‰∏ÄÂÄãÊï¥È´îË®àÁï´„ÄÇÁÇ∫‰∫ÜÈô§ÈåØÂíå‰øÆÂæ©ÂäüËÉΩÈåØË™§ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈ´òÊïàÁöÑÂü∫ÊñºÊäΩË±°Ë™ûÊ≥ïÊ®π (AST) ÁöÑÊ≥¢ÂΩ¢ËøΩËπ§Â∑•ÂÖ∑ÔºåÂÆÉÊï¥ÂêàÂú®Ëá™‰∏ª Verilog ÂÆåÊàêÊµÅÁ®ã‰∏≠„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊàêÂäüÁî¢Áîü‰∫Ü 94.2% Ë™ûÊ≥ïÂíåÂäüËÉΩÊ≠£Á¢∫ÁöÑ Verilog Á®ãÂºèÁ¢ºÔºåÂú® VerilogEval-Human v2 Âü∫Ê∫ñ‰∏äÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÈ´òÂá∫ 33.9%„ÄÇ

##### **Graph Retrieval-Augmented Generation: A Survey**
2408.08921v2 by Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang

Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.

ÊëòË¶ÅÔºöÊúÄËøëÔºåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊåëÊàòÊñπÈù¢ÂèñÂæó‰∫ÜÊòæÁùÄÊàêÂäüÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÈÄöËøáÂèÇËÄÉÂ§ñÈÉ®Áü•ËØÜÂ∫ìÔºåRAG ÊîπËøõ‰∫Ü LLM ÁöÑËæìÂá∫ÔºåÊúâÊïàÂú∞ÂáèËΩª‰∫ÜËØ∏Â¶Ç„ÄåÂπªËßâ„Äç„ÄÅÁº∫‰πèÁâπÂÆöÈ¢ÜÂüüÁü•ËØÜÂíå‰ø°ÊÅØËøáÊó∂Á≠âÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºåÊï∞ÊçÆÂ∫ì‰∏≠‰∏çÂêåÂÆû‰Ωì‰πãÈó¥ÂÖ≥Á≥ªÁöÑÂ§çÊùÇÁªìÊûÑÁªô RAG Á≥ªÁªüÂ∏¶Êù•‰∫ÜÊåëÊàò„ÄÇ‰Ωú‰∏∫ÂõûÂ∫îÔºåGraphRAG Âà©Áî®ÂÆû‰Ωì‰πãÈó¥ÁöÑÁªìÊûÑ‰ø°ÊÅØÊù•ÂÆûÁé∞Êõ¥Á≤æÁ°ÆÂíåÂÖ®Èù¢ÁöÑÊ£ÄÁ¥¢ÔºåÊçïËé∑ÂÖ≥Á≥ªÁü•ËØÜÂπ∂‰øÉËøõÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÂÖ∑‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂìçÂ∫î„ÄÇÈâ¥‰∫é GraphRAG ÁöÑÊñ∞È¢ñÊÄßÂíåÊΩúÂäõÔºåÂØπÂΩìÂâçÊäÄÊúØËøõË°åÁ≥ªÁªüÂÆ°Êü•ÂäøÂú®ÂøÖË°å„ÄÇÊú¨ÊñáÊèê‰æõ‰∫Ü GraphRAG ÊñπÊ≥ïÁöÑÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÊàë‰ª¨ÂΩ¢ÂºèÂåñ‰∫Ü GraphRAG Â∑•‰ΩúÊµÅÔºåÂåÖÊã¨Âü∫‰∫éÂõæÁöÑÁ¥¢Âºï„ÄÅÂõæÂºïÂØºÁöÑÊ£ÄÁ¥¢ÂíåÂõæÂ¢ûÂº∫ÁöÑÁîüÊàê„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Âú®ÊØè‰∏™Èò∂ÊÆµÊ¶ÇËø∞‰∫ÜÊ†∏ÂøÉÊäÄÊúØÂíåËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÁ†îÁ©∂‰∫Ü GraphRAG ÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÅÂ∫îÁî®È¢ÜÂüü„ÄÅËØÑ‰º∞ÊñπÊ≥ïÂíåÂ∑•‰∏öÁî®‰æã„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Êé¢ËÆ®‰∫ÜÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÔºå‰ª•ÊøÄÂèëËøõ‰∏ÄÊ≠•ÁöÑÊé¢Á©∂Âπ∂Êé®ËøõËØ•È¢ÜÂüüÁöÑËøõÂ±ï„ÄÇ‰∏∫‰∫ÜËøΩË∏™ËØ•È¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÊàë‰ª¨Âú® \url{https://github.com/pengboci/GraphRAG-Survey} ‰∏äÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Â≠òÂÇ®Â∫ì„ÄÇ

##### **Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**
2408.07852v1 by Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, Simon Kornblith

While many capabilities of language models (LMs) improve with increased
training budget, the influence of scale on hallucinations is not yet fully
understood. Hallucinations come in many forms, and there is no universally
accepted definition. We thus focus on studying only those hallucinations where
a correct answer appears verbatim in the training set. To fully control the
training data content, we construct a knowledge graph (KG)-based dataset, and
use it to train a set of increasingly large LMs. We find that for a fixed
dataset, larger and longer-trained LMs hallucinate less. However, hallucinating
on $\leq5$% of the training data requires an order of magnitude larger model,
and thus an order of magnitude more compute, than Hoffmann et al. (2022)
reported was optimal. Given this costliness, we study how hallucination
detectors depend on scale. While we see detector size improves performance on
fixed LM's outputs, we find an inverse relationship between the scale of the LM
and the detectability of its hallucinations.

ÊëòË¶ÅÔºöÈõñÁÑ∂Ë™ûË®ÄÊ®°Âûã (LM) ÁöÑË®±Â§öËÉΩÂäõÊúÉÈö®ËëóË®ìÁ∑¥È†êÁÆóÁöÑÂ¢ûÂä†ËÄåÊúâÊâÄÊèêÂçáÔºå‰ΩÜË¶èÊ®°Â∞çÂπªË¶∫ÁöÑÂΩ±ÈüøÂ∞öÊú™ÂÆåÂÖ®‰∫ÜËß£„ÄÇÂπªË¶∫ÊúâË®±Â§öÂΩ¢ÂºèÔºå‰∏îÊ≤íÊúâÊôÆÈÅçÊé•ÂèóÁöÑÂÆöÁæ©„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂè™Â∞àÊ≥®ÊñºÁ†îÁ©∂Ë®ìÁ∑¥ÈõÜ‰∏≠Âá∫ÁèæÊ≠£Á¢∫Á≠îÊ°àÁöÑÂπªË¶∫„ÄÇÁÇ∫‰∫ÜÂÆåÂÖ®ÊéßÂà∂Ë®ìÁ∑¥Ë≥áÊñôÂÖßÂÆπÔºåÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñË≠ú (KG) ÁöÑË≥áÊñôÈõÜÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜË®ìÁ∑¥‰∏ÄÁµÑË∂ä‰æÜË∂äÂ§ßÁöÑ LM„ÄÇÊàëÂÄëÁôºÁèæÂ∞çÊñºÂõ∫ÂÆöÁöÑË≥áÊñôÈõÜÔºåË¶èÊ®°ËºÉÂ§ß‰∏îË®ìÁ∑¥ÊôÇÈñìËºÉÈï∑ÁöÑ LM Áî¢ÁîüÁöÑÂπªË¶∫ËºÉÂ∞ë„ÄÇÁÑ∂ËÄåÔºåÂú® $\leq5$% ÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏äÁî¢ÁîüÂπªË¶∫ÈúÄË¶ÅË¶èÊ®°Â§ß‰∏ÄÂÄãÊï∏ÈáèÁ¥öÁöÑÊ®°ÂûãÔºåÂõ†Ê≠§ÊØî Hoffmann Á≠â‰∫∫ (2022) ÊâÄÂ†±ÂëäÁöÑÊúÄ‰Ω≥Ë¶èÊ®°Â§ö‰∏ÄÂÄãÊï∏ÈáèÁ¥öÁöÑÈÅãÁÆóÊàêÊú¨„ÄÇËÄÉÈáèÂà∞ÈÄôÁ®ÆÊàêÊú¨ÔºåÊàëÂÄëÁ†îÁ©∂ÂπªË¶∫ÂÅµÊ∏¨Âô®Â¶Ç‰ΩïÂèñÊ±∫ÊñºË¶èÊ®°„ÄÇÈõñÁÑ∂ÊàëÂÄëÁúãÂà∞ÂÅµÊ∏¨Âô®Ë¶èÊ®°ÊúÉÊèêÂçáÂ∞çÂõ∫ÂÆö LM Ëº∏Âá∫ÁöÑÊïàËÉΩÔºå‰ΩÜÊàëÂÄëÁôºÁèæ LM ÁöÑË¶èÊ®°ËàáÂÖ∂ÂπªË¶∫ÁöÑÂèØÂÅµÊ∏¨ÊÄß‰πãÈñìÂ≠òÂú®ÂèçÊØîÈóú‰øÇ„ÄÇ

##### **ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**
2408.07840v1 by Xuanqing Yu, Wangtao Sun, Jingwei Li, Kang Liu, Chengbao Liu, Jie Tan

In the realm of event prediction, temporal knowledge graph forecasting (TKGF)
stands as a pivotal technique. Previous approaches face the challenges of not
utilizing experience during testing and relying on a single short-term history,
which limits adaptation to evolving data. In this paper, we introduce the
Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by
integrating dynamic causal rule mining (DCRM) and dual history augmented
generation (DHAG). DCRM dynamically constructs causal rules from real-time
data, allowing for swift adaptation to new causal relationships. In parallel,
DHAG merges short-term and long-term historical contexts, leveraging a
bi-branch approach to enrich event prediction. Our framework demonstrates
notable performance enhancements across diverse datasets, with significant
Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language
models (LLMs) for event prediction without necessitating extensive retraining.
The ONSEP framework not only advances the field of TKGF but also underscores
the potential of neural-symbolic approaches in adapting to dynamic data
environments.

ÊëòË¶ÅÔºöÂú®‰∫ã‰ª∂È†êÊ∏¨È†òÂüü‰∏≠ÔºåÊôÇÂ∫èÁü•Ë≠òÂúñË≠úÈ†êÊ∏¨ (TKGF) ÊòØ‰∏ÄÂÄãÈóúÈçµÊäÄË°ì„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÈù¢Ëá®Âú®Ê∏¨Ë©¶ÊúüÈñì‰∏çÂà©Áî®Á∂ìÈ©ó‰ª•Âèä‰æùË≥¥ÂñÆ‰∏ÄÁü≠ÊúüÊ≠∑Âè≤ÁöÑÊåëÊà∞ÔºåÈÄôÈôêÂà∂‰∫ÜÂ∞çÊºîÂåñË≥áÊñôÁöÑÈÅ©ÊáâÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÁ∑ö‰∏äÁ•ûÁ∂ìÁ¨¶Ëôü‰∫ã‰ª∂È†êÊ∏¨ (ONSEP) Êû∂ÊßãÔºåÂÆÉÈÄèÈÅéÊï¥ÂêàÂãïÊÖãÂõ†ÊûúË¶èÂâáÊåñÊéò (DCRM) ÂíåÈõôÈáçÊ≠∑Âè≤Êì¥ÂÖÖÁîüÊàê (DHAG) ‰æÜÂâµÊñ∞„ÄÇDCRM ÂæûÂç≥ÊôÇË≥áÊñô‰∏≠ÂãïÊÖãÂª∫ÊßãÂõ†ÊûúË¶èÂâáÔºåÂÖÅË®±Âø´ÈÄüÈÅ©ÊáâÊñ∞ÁöÑÂõ†ÊûúÈóú‰øÇ„ÄÇÂêåÊôÇÔºåDHAG Âêà‰ΩµÁü≠ÊúüÂíåÈï∑ÊúüÊ≠∑Âè≤ËÑàÁµ°ÔºåÂà©Áî®ÈõôÂàÜÊîØÊñπÊ≥ï‰æÜË±êÂØå‰∫ã‰ª∂È†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫Âá∫È°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºåHit@k (k=1,3,10) ÊúâÈ°ØËëóÁöÑÊîπÂñÑÔºåÂ±ïÁ§∫‰∫ÜÂÆÉÂú®ÁÑ°ÈúÄÂª£Ê≥õÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÊì¥ÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•ÈÄ≤Ë°å‰∫ã‰ª∂È†êÊ∏¨ÁöÑËÉΩÂäõ„ÄÇONSEP Êû∂Êßã‰∏çÂÉÖÊé®Âãï‰∫Ü TKGF È†òÂüüÔºå‰πüÂº∑Ë™ø‰∫ÜÁ•ûÁ∂ìÁ¨¶ËôüÊñπÊ≥ïÂú®ÈÅ©ÊáâÂãïÊÖãË≥áÊñôÁí∞Â¢É‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**
2408.07611v2 by Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu

Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce "phantom" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a "Retrieval-Augmented
Generation (RAG)" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê•µÂ§ßÂú∞‰øÉËøõ‰∫ÜËá™ÈÅ©ÊáâÊô∫ËÉΩ‰ª£ÁêÜÁöÑÈñãÁôºÔºå‰∏¶Ë¢´ÂÆö‰ΩçÁÇ∫ÂØ¶Áèæ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) ÁöÑÈáçË¶ÅÈÄîÂæë„ÄÇÁÑ∂ËÄåÔºåLLM ÂÆπÊòìÁî¢Áîü‰∫ãÂØ¶‰∏ä‰∏çÊ≠£Á¢∫ÁöÑË≥áË®äÔºåËÄå‰∏îÂ∏∏Â∏∏Áî¢Áîü„ÄåÂπªÂΩ±„ÄçÂÖßÂÆπÔºåÈÄôÊúÉÁ†¥Â£ûÂÖ∂ÂèØÈù†ÊÄßÔºåÂ∞çÂÖ∂Âú®ÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÊßãÊàêÂö¥Â≥ªÊåëÊà∞„ÄÇÁµêÂêàÂ§ñÈÉ®Ë≥áÊñôÂ∫´ÂíåË≥áË®äÊ™¢Á¥¢Ê©üÂà∂‰æÜÂ¢ûÂº∑ LLM ÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç‰∏äËø∞ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ WeKnow-RAG ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂ∞áÁ∂≤Ë∑ØÊêúÂ∞ãÂíåÁü•Ë≠òÂúñË≠úÊï¥ÂêàÂà∞„ÄåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)„ÄçÁ≥ªÁµ±‰∏≠„ÄÇÈ¶ñÂÖàÔºåÈÄèÈÅéÁµêÂêàÁü•Ë≠òÂúñË≠úÁöÑÁµêÊßãÂåñË°®Á§∫ÂíåÁ®†ÂØÜÂêëÈáèÊ™¢Á¥¢ÁöÑÈùàÊ¥ªÊÄßÔºå‰æÜÊèêÂçá LLM ÂõûÊáâÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇWeKnow-RAG Êé•ËëóÂà©Áî®ÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÂúñË≠ú‰æÜÊªøË∂≥ÂêÑÁ®ÆÊü•Ë©¢ÂíåÈ†òÂüüÔºåÂæûËÄåÈÄèÈÅé‰ΩøÁî®Á®ÄÁñèÂíåÁ®†ÂØÜÊ™¢Á¥¢ÊñπÊ≥ïÁöÑÂ§öÈöéÊÆµÁ∂≤È†ÅÊ™¢Á¥¢ÊäÄË°ìÔºå‰æÜÊèêÂçá‰∫ãÂØ¶Ë≥áË®äÂíåË§áÈõúÊé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊúâÊïàÂú∞Âπ≥Ë°°‰∫ÜË≥áË®äÊ™¢Á¥¢ÁöÑÊïàÁéáÂíåÊ∫ñÁ¢∫ÊÄßÔºåÈÄ≤ËÄåÊîπÂñÑÊï¥È´îÊ™¢Á¥¢ÊµÅÁ®ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÇÑÊï¥Âêà‰∫Ü‰∏ÄÂÄã LLM Ëá™ÊàëË©ï‰º∞Ê©üÂà∂Ôºå‰ª•Ë©ï‰º∞ÂÖ∂ÊâÄÁî¢ÁîüÁ≠îÊ°àÁöÑÂèØ‰ø°Â∫¶„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Âª£Ê≥õÁöÑÈõ¢Á∑öÂØ¶È©óÂíåÁ∑ö‰∏äÊèê‰∫§‰∏≠Ë≠âÊòé‰∫ÜÂÖ∂ÂÇëÂá∫ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**
2408.07453v1 by Tobias A. Opsahl

Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠Áç≤ÂæóËøëÊúüÊàêÂäüÔºå‰∫ãÂØ¶È©óË≠â‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖËâ±Èõ£ÁöÑ‰ªªÂãô„ÄÇÁî±ÊñºÈåØË™§Ë≥áË®äÂÇ≥Êí≠ÂæóË∂ä‰æÜË∂äÂø´ÔºåÊ≥®ÊÑèÂäõÂ∑≤ËΩâÂêëËá™ÂãïÈ©óË≠âËÅ≤ÊòéÁöÑÊ≠£Á¢∫ÊÄß„ÄÇÂú® NLP È†òÂüü‰∏≠ÔºåÈÄôÈÄöÂ∏∏ÈÄèÈÅéË®ìÁ∑¥Áõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÂÆåÊàêÔºåÈÄô‰∫õÊ®°ÂûãÂà©Áî®‰æÜËá™ÂèØ‰ø°Ë≥¥Ë™ûÊñôÂ∫´ÁöÑË≠âÊìö‰æÜÈ©óË≠âËÅ≤Êòé„ÄÇÊàëÂÄëÊèêÂá∫ÊúâÊïàÁöÑÊñπÊ≥ï‰æÜÈ©óË≠âË≥áÊñôÈõÜ‰∏≠ÁöÑËÅ≤ÊòéÔºåÂÖ∂‰∏≠Ë≠âÊìöÊòØ‰ª•ÁµêÊßãÂåñÁü•Ë≠òÂúñË°®ÁöÑÂΩ¢ÂºèÂëàÁèæ„ÄÇÊàëÂÄë‰ΩøÁî® FactKG Ë≥áÊñôÈõÜÔºåÂÆÉÊòØÁî±ÂæûÁ∂≠Âü∫ÁôæÁßë‰∏≠ËêÉÂèñÁöÑ DBpedia Áü•Ë≠òÂúñË°®ÊâÄÂª∫Êßã„ÄÇÈÄèÈÅéÁ∞°ÂåñË≠âÊìöÊì∑ÂèñÊµÅÁ®ãÔºåÂæûÂæÆË™øË™ûË®ÄÊ®°ÂûãÂà∞Á∞°ÂñÆÁöÑÈÇèËºØÊì∑ÂèñÔºåÊàëÂÄëËÉΩÂ§†Âª∫ÊßãÊó¢ÈúÄË¶ÅËºÉÂ∞ëË®àÁÆóË≥áÊ∫êÔºåÂèàËÉΩÈÅîÂà∞ËºÉ‰Ω≥Ê∏¨Ë©¶ÈõÜÊ∫ñÁ¢∫Â∫¶ÁöÑÊ®°Âûã„ÄÇ

##### **LLMs can Schedule**
2408.06993v1 by Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave

The job shop scheduling problem (JSSP) remains a significant hurdle in
optimizing production processes. This challenge involves efficiently allocating
jobs to a limited number of machines while minimizing factors like total
processing time or job delays. While recent advancements in artificial
intelligence have yielded promising solutions, such as reinforcement learning
and graph neural networks, this paper explores the potential of Large Language
Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset
specifically designed to train LLMs for JSSP. Surprisingly, our findings
demonstrate that LLM-based scheduling can achieve performance comparable to
other neural approaches. Furthermore, we propose a sampling method that
enhances the effectiveness of LLMs in tackling JSSP.

ÊëòË¶ÅÔºö‰ΩúÊ•≠ËªäÈñìÊéíÁ®ãÂïèÈ°å (JSSP) ‰ªçÁÑ∂ÊòØÊúÄ‰Ω≥ÂåñÁîüÁî¢ÊµÅÁ®ã‰∏≠ÁöÑ‰∏ÄÂ§ßÈöúÁ§ô„ÄÇÈÄôÈ†ÖÊåëÊà∞Ê∂âÂèäÂ∞á‰ΩúÊ•≠ÊúâÊïàÂàÜÈÖçÂà∞Êï∏ÈáèÊúâÈôêÁöÑÊ©üÂô®ÔºåÂêåÊôÇÂ∞áÁ∏ΩËôïÁêÜÊôÇÈñìÊàñ‰ΩúÊ•≠Âª∂ÈÅ≤Á≠âÂõ†Á¥†ÈôçËá≥ÊúÄ‰Ωé„ÄÇÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖßÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤Áî¢ÁîüÊúâÂ∏åÊúõÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰æãÂ¶ÇÂº∑ÂåñÂ≠∏ÁøíÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰ΩÜÊú¨ÊñáÊé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú® JSSP ‰∏≠ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÁ¨¨‰∏ÄÂÄãÁõ£Áù£Âºè 120k Ë≥áÊñôÈõÜÔºåÂ∞àÈñÄÁî®ÊñºË®ìÁ∑¥ JSSP ÁöÑ LLM„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂü∫Êñº LLM ÁöÑÊéíÁ®ãÂèØ‰ª•ÈÅîÂà∞ËàáÂÖ∂‰ªñÁ•ûÁ∂ìÊñπÊ≥ïÁõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊäΩÊ®£ÊñπÊ≥ïÔºåÂèØÂ¢ûÂº∑ LLM Âú®ËôïÁêÜ JSSP ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Causal Agent based on Large Language Model**
2408.06849v1 by Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye, Fei Wu

Large language models (LLMs) have achieved significant success across various
domains. However, the inherent complexity of causal problems and causal theory
poses challenges in accurately describing them in natural language, making it
difficult for LLMs to comprehend and use them effectively. Causal methods are
not easily conveyed through natural language, which hinders LLMs' ability to
apply them accurately. Additionally, causal datasets are typically tabular,
while LLMs excel in handling natural language data, creating a structural
mismatch that impedes effective reasoning with tabular data. This lack of
causal reasoning capability limits the development of LLMs. To address these
challenges, we have equipped the LLM with causal tools within an agent
framework, named the Causal Agent, enabling it to tackle causal problems. The
causal agent comprises tools, memory, and reasoning modules. In the tools
module, the causal agent applies causal methods to align tabular data with
natural language. In the reasoning module, the causal agent employs the ReAct
framework to perform reasoning through multiple iterations with the tools. In
the memory module, the causal agent maintains a dictionary instance where the
keys are unique names and the values are causal graphs. To verify the causal
ability of the causal agent, we established a benchmark consisting of four
levels of causal problems: variable level, edge level, causal graph level, and
causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for
these four levels of issues and tested the causal agent on the datasets. Our
methodology demonstrates remarkable efficacy on the four-level causal problems,
with accuracy rates all above 80%. For further insights and implementation
details, our code is accessible via the GitHub repository
https://github.com/Kairong-Han/Causal_Agent.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÂÄãÈ†òÂüüÂèñÂæóÈáçÂ§ßÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂõ†ÊûúÂïèÈ°åÂíåÂõ†ÊûúÁêÜË´ñÁöÑÂÖßÂú®Ë§áÈõúÊÄßÔºåÂú®Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠Ê∫ñÁ¢∫ÊèèËø∞ÂÆÉÂÄëÊôÇÊßãÊàêÊåëÊà∞ÔºåÈÄô‰ΩøÂæó LLM Èõ£‰ª•ÁêÜËß£‰∏¶ÊúâÊïà‰ΩøÁî®ÂÆÉÂÄë„ÄÇÂõ†ÊûúÊñπÊ≥ï‰∏çÊòìÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÂÇ≥ÈÅîÔºåÈÄôÈòªÁ§ô‰∫Ü LLM Ê∫ñÁ¢∫ÊáâÁî®ÂÆÉÂÄëÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂõ†ÊûúË≥áÊñôÈõÜÈÄöÂ∏∏ÊòØË°®Ê†ºÂåñÁöÑÔºåËÄå LLM ÊìÖÈï∑ËôïÁêÜËá™ÁÑ∂Ë™ûË®ÄË≥áÊñôÔºåÈÄôÈÄ†Êàê‰∫ÜÁµêÊßã‰∏äÁöÑ‰∏çÂåπÈÖçÔºåÈòªÁ§ô‰∫ÜÂ∞çË°®Ê†ºË≥áÊñôÈÄ≤Ë°åÊúâÊïàÁöÑÊé®ÁêÜ„ÄÇÈÄôÁ®ÆÁº∫‰πèÂõ†ÊûúÊé®ÁêÜËÉΩÂäõÈôêÂà∂‰∫Ü LLM ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂú®‰∏ÄÂÄã‰ª£ÁêÜÊ°ÜÊû∂‰∏≠ÁÇ∫ LLM ÈÖçÂÇô‰∫ÜÂõ†ÊûúÂ∑•ÂÖ∑ÔºåÁ®±ÁÇ∫Âõ†Êûú‰ª£ÁêÜÔºå‰ΩøÂÆÉËÉΩÂ§†Ëß£Ê±∫Âõ†ÊûúÂïèÈ°å„ÄÇÂõ†Êûú‰ª£ÁêÜÂåÖÂê´Â∑•ÂÖ∑„ÄÅË®òÊÜ∂È´îÂíåÊé®ÁêÜÊ®°ÁµÑ„ÄÇÂú®Â∑•ÂÖ∑Ê®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÊáâÁî®Âõ†ÊûúÊñπÊ≥ïÂ∞áË°®Ê†ºË≥áÊñôËàáËá™ÁÑ∂Ë™ûË®ÄÂ∞çÈΩä„ÄÇÂú®Êé®ÁêÜÊ®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÊé°Áî® ReAct Ê°ÜÊû∂ÔºåÈÄèÈÅéËàáÂ∑•ÂÖ∑ÈÄ≤Ë°åÂ§öÊ¨°ÂèçË¶ÜÈÅãÁÆó‰æÜÂü∑Ë°åÊé®ÁêÜ„ÄÇÂú®Ë®òÊÜ∂È´îÊ®°ÁµÑ‰∏≠ÔºåÂõ†Êûú‰ª£ÁêÜÁ∂≠Ë≠∑‰∏ÄÂÄãÂ≠óÂÖ∏ÂØ¶‰æãÔºåÂÖ∂‰∏≠ÈçµÊòØÂîØ‰∏ÄÂêçÁ®±ÔºåËÄåÂÄºÊòØÂõ†ÊûúÂúñ„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÂõ†Êûú‰ª£ÁêÜÁöÑÂõ†ÊûúËÉΩÂäõÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂü∫Ê∫ñÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõõÂÄãÂ±§Á¥öÁöÑÂõ†ÊûúÂïèÈ°åÔºöËÆäÊï∏Â±§Á¥ö„ÄÅÈÇäÂ±§Á¥ö„ÄÅÂõ†ÊûúÂúñÂ±§Á¥öÂíåÂõ†ÊûúÊïàÊáâÂ±§Á¥ö„ÄÇÊàëÂÄë‰ΩøÁî® ChatGPT-3.5 ÁÇ∫ÈÄôÂõõÂÄãÂ±§Á¥öÁöÑÂïèÈ°åÁî¢Áîü‰∫Ü 1.3K ÁöÑÊ∏¨Ë©¶Ë≥áÊñôÈõÜÔºå‰∏¶Âú®Ë≥áÊñôÈõÜ‰∏äÊ∏¨Ë©¶‰∫ÜÂõ†Êûú‰ª£ÁêÜ„ÄÇÊàëÂÄëÁöÑÈÄôÂ•óÊñπÊ≥ïÂú®ÂõõÂÄãÂ±§Á¥öÁöÑÂõ†ÊûúÂïèÈ°å‰∏äÂ±ïÁèæ‰∫ÜÈ°ØËëóÁöÑÂäüÊïàÔºåÊ∫ñÁ¢∫ÁéáÈÉΩÈ´òÊñº 80%„ÄÇÊúâÈóúÈÄ≤‰∏ÄÊ≠•ÁöÑË¶ãËß£ÂíåÂØ¶‰ΩúÁ¥∞ÁØÄÔºåÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÈÄèÈÅé GitHub ÂÑ≤Â≠òÂ∫´ https://github.com/Kairong-Han/Causal_Agent ÂèñÂæó„ÄÇ

