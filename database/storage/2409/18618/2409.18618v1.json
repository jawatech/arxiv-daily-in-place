{"2409.18618": {"publish_time": "2024-09-27", "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "paper_summary": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.", "paper_summary_zh": "\u5728\u62bd\u8c61\u6458\u8981\u4e2d\uff0c\u7522\u751f\u7c21\u6f54\u4e14\u6e96\u78ba\u7684\u6458\u8981\u7684\u6311\u6230\u4f86\u81ea\u65bc\u539f\u59cb\u6587\u4ef6\u4e2d\u5305\u542b\u7684\u9f90\u5927\u8cc7\u8a0a\u91cf\u3002\u56e0\u6b64\uff0c\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u7522\u751f\u6d41\u66a2\u7684\u6587\u5b57\uff0c\u4f46\u5b83\u5011\u5e38\u5e38\u6703\u900f\u904e\u5e7b\u89ba\u5316\u539f\u59cb\u4f86\u6e90\u4e2d\u627e\u4e0d\u5230\u7684\u5167\u5bb9\u4f86\u5f15\u5165\u4e0d\u6e96\u78ba\u6027\u3002\u96d6\u7136\u6700\u5927\u5316\u53ef\u80fd\u6027\u6709\u52a9\u65bc\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u76e3\u7763\u5fae\u8abf\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u4e26\u672a\u6301\u7e8c\u589e\u5f37\u6458\u8981\u7684\u5fe0\u5be6\u5ea6\u3002\u57fa\u65bc\u504f\u597d\u7684\u6700\u4f73\u5316\u65b9\u6cd5\uff08\u4f8b\u5982\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff09\u53ef\u4ee5\u9032\u4e00\u6b65\u6539\u5584\u6a21\u578b\uff0c\u4ee5\u7b26\u5408\u4eba\u985e\u7684\u504f\u597d\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4ecd\u7136\u9ad8\u5ea6\u4f9d\u8cf4\u6602\u8cb4\u7684\u4eba\u985e\u56de\u994b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u9032\u4e00\u7a2e\u7a31\u70ba\u57fa\u65bc\u6a21\u578b\u7684\u504f\u597d\u6700\u4f73\u5316 (MPO) \u7684\u65b0\u7a4e\u4e14\u76f4\u63a5\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u5fae\u8abf LLM\uff0c\u4ee5\u5728\u6c92\u6709\u4efb\u4f55\u4eba\u985e\u56de\u994b\u7684\u60c5\u6cc1\u4e0b\u6539\u5584\u6458\u8981\u80fd\u529b\u3002\u900f\u904e\u5229\u7528\u6a21\u578b\u56fa\u6709\u7684\u6458\u8981\u80fd\u529b\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u504f\u597d\u8cc7\u6599\u96c6\uff0c\u8a72\u8cc7\u6599\u96c6\u5b8c\u5168\u7531\u6a21\u578b\u4f7f\u7528\u4e0d\u540c\u7684\u89e3\u78bc\u7b56\u7565\u7522\u751f\u3002\u6211\u5011\u5728\u6a19\u6e96\u6458\u8981\u8cc7\u6599\u96c6\u548c\u5404\u7a2e\u6307\u6a19\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684 MPO \u5927\u5e45\u63d0\u5347\u4e86\u7522\u751f\u6458\u8981\u7684\u54c1\u8cea\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u4eba\u985e\u56de\u994b\u3002", "author": "Jaepill Choi et.al.", "authors": "Jaepill Choi, Kyubyung Chae, Jiwoo Song, Yohan Jo, Taesup Kim", "id": "2409.18618v1", "paper_url": "http://arxiv.org/abs/2409.18618v1", "repo": "null"}}