{"2410.01651": {"publish_time": "2024-10-02", "title": "Efficient Long-range Language Modeling with Self-supervised Causal Retrieval", "paper_summary": "Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u57fa\u4e8e\u68c0\u7d22\u7684\u8bed\u8a00\u6a21\u578b (RLM) \u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5927\u591a\u6570\u5229\u7528\u5177\u6709\u56fa\u5b9a\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u7ec4\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u5bf9\u68c0\u7d22\u5668\u548c\u56e0\u679c LM \u8fdb\u884c\u8054\u5408\u9884\u8bad\u7ec3\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u8f93\u5165\u5e8f\u5217\uff0c\u6211\u4eec\u5c06\u5b83\u5206\u6210\u5757\uff0c\u5e76\u4f7f\u7528\u5f53\u524d\u5757\u6765\u68c0\u7d22\u8fc7\u53bb\u5757\u4ee5\u8fdb\u884c\u540e\u7eed\u6587\u672c\u751f\u6210\u3002\u6211\u4eec\u7684\u521b\u65b0\u5141\u8bb8\u68c0\u7d22\u5668\u5b66\u4e60\u5982\u4f55\u68c0\u7d22\u8fc7\u53bb\u5757\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u6700\u5c0f\u5316\u540e\u7eed\u4ee4\u724c\u7684\u7aef\u5230\u7aef\u81ea\u56de\u5f52\u635f\u5931\u3002\u901a\u8fc7\u96c6\u6210\u524d $k$ \u68c0\u7d22\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u6709\u6548\u5730\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u9ad8\u8fbe 64K \u4e2a\u4ee4\u724c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u957f\u7a0b LM \u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u53ef\u6bd4\u8f83\u6216\u66f4\u4f4e\u7684\u9884\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3002", "author": "Xiang Hu et.al.", "authors": "Xiang Hu, Zhihao Teng, Wei Wu, Kewei Tu", "id": "2410.01651v1", "paper_url": "http://arxiv.org/abs/2410.01651v1", "repo": "null"}}