{"2410.14198": {"publish_time": "2024-10-18", "title": "Supervised Chain of Thought", "paper_summary": "Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u4e26\u5177\u5099\u4fc3\u9032\u4eba\u5de5\u667a\u6167\u767c\u5c55\u7684\u5de8\u5927\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u4e3b\u6d41 LLM \u7684\u6838\u5fc3\u67b6\u69cb\uff08Transformer\uff09\u5728\u8a08\u7b97\u6df1\u5ea6\u65b9\u9762\u6709\u5176\u5167\u5728\u9650\u5236\uff0c\u7406\u8ad6\u4e0a\u7121\u6cd5\u89e3\u6c7a\u8a31\u591a\u9700\u8981\u8d8a\u4f86\u8d8a\u6df1\u5165\u8a08\u7b97\u7684\u63a8\u7406\u4efb\u52d9\u3002\u601d\u7dad\u93c8 (CoT) \u63d0\u793a\u5df2\u6210\u70ba\u89e3\u6c7a\u9019\u4e9b\u67b6\u69cb\u9650\u5236\u7684\u4e00\u7a2e\u6280\u8853\uff0c\u9019\u5df2\u7531\u5e7e\u9805\u7406\u8ad6\u7814\u7a76\u8b49\u5be6\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u6cd5\u4f86\u89e3\u6c7a\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\uff0c\u9019\u4e9b\u4efb\u52d9\u4ee5\u524d\u8d85\u51fa\u4e86\u9019\u4e9b\u6a21\u578b\u7684\u80fd\u529b\u3002\u5118\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0cCoT \u53ca\u5176\u8b8a\u9ad4\uff08\u4f8b\u5982\u601d\u7dad\u6a39\u3001\u601d\u7dad\u5716\u7b49\uff09\u4f9d\u8cf4\u65bc\u300c\u4e00\u63d0\u793a\u9069\u7528\u6240\u6709\u300d\u7684\u65b9\u6cd5\uff0c\u5c0d\u5404\u7a2e\u4efb\u52d9\uff08\u5f9e\u8a08\u6578\u548c\u6392\u5e8f\u5230\u89e3\u6c7a\u6578\u5b78\u548c\u6f14\u7b97\u6cd5\u554f\u984c\uff09\u4f7f\u7528\u55ae\u4e00\u7684\u63d0\u793a\u7d50\u69cb\uff08\u4f8b\u5982\uff0c\u300c\u9010\u6b65\u601d\u8003\u300d\uff09\u3002\u9019\u7a2e\u65b9\u6cd5\u5c0d\u6a21\u578b\u7522\u751f\u6b63\u78ba\u7684\u63a8\u7406\u6b65\u9a5f\u69cb\u6210\u4e86\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u6a21\u578b\u5fc5\u9808\u5728\u5ee3\u6cdb\u7684\u63d0\u793a\u7bc4\u672c\u7a7a\u9593\u4e2d\u5c0e\u822a\uff0c\u624d\u80fd\u70ba\u6bcf\u500b\u4efb\u52d9\u627e\u5230\u9069\u7576\u7684\u7bc4\u672c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5efa\u7acb\u5728 CoT \u5148\u524d\u7684\u7406\u8ad6\u5206\u6790\u4e4b\u4e0a\uff0c\u8aaa\u660e\u300c\u4e00\u63d0\u793a\u9069\u7528\u6240\u6709\u300d\u7684\u65b9\u6cd5\u5982\u4f55\u5c0d LLM \u7684\u53ef\u8a08\u7b97\u6027\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u6211\u5011\u5c07\u89e3\u7684\u641c\u5c0b\u7a7a\u9593\u5206\u70ba\u5169\u90e8\u5206\uff1a\u63d0\u793a\u7a7a\u9593\u548c\u7b54\u6848\u7a7a\u9593\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u76e3\u7763\u5c0d\u65bc\u6e96\u78ba\u5c0e\u822a\u63d0\u793a\u7a7a\u9593\u4e26\u5be6\u73fe\u6700\u4f73\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u900f\u904e\u4f7f\u7528\u6700\u5148\u9032\u7684 LLM \u9032\u884c\u5be6\u9a57\uff0c\u6211\u5011\u63ed\u793a\u4e86\u5728\u61c9\u7528\u76e3\u7763\u8207\u672a\u61c9\u7528\u76e3\u7763\u6642\u63a8\u7406\u6548\u80fd\u7684\u5dee\u8ddd\u3002", "author": "Xiang Zhang et.al.", "authors": "Xiang Zhang, Dujian Ding", "id": "2410.14198v1", "paper_url": "http://arxiv.org/abs/2410.14198v1", "repo": "null"}}