{"2409.13312": {"publish_time": "2024-09-20", "title": "GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification", "paper_summary": "Pretrained transformer-based Language Models (LMs) are well-known for their\nability to achieve significant improvement on text classification tasks with\ntheir powerful word embeddings, but their black-box nature, which leads to a\nlack of interpretability, has been a major concern. In this work, we introduce\nGAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical\nNetwork designed to explain the decisions of text classification models built\nwith LM encoders. In our approach, the input vector and prototypes are regarded\nas nodes within a graph, and we utilize multi-head graph attention to\nselectively construct edges between the input node and prototype nodes to learn\nan interpretable prototypical representation. During inference, the model makes\ndecisions based on a linear combination of activated prototypes weighted by the\nattention score assigned for each prototype, allowing its choices to be\ntransparently explained by the attention weights and the prototypes projected\ninto the closest matching training examples. Experiments on multiple public\ndatasets show our approach achieves superior results without sacrificing the\naccuracy of the original black-box LMs. We also compare with four alternative\nprototypical network variations and our approach achieves the best accuracy and\nF1 among all. Our case study and visualization of prototype clusters also\ndemonstrate the efficiency in explaining the decisions of black-box models\nbuilt with LMs.", "paper_summary_zh": "\u9810\u8a13\u7df4\u7684 Transformer \u57fa\u65bc\u8a9e\u8a00\u6a21\u578b (LM) \u4ee5\u5176\u5f37\u5927\u7684\u8a5e\u5d4c\u5165\u529f\u80fd\u800c\u805e\u540d\uff0c\u80fd\u5920\u5728\u6587\u672c\u5206\u985e\u4efb\u52d9\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6539\u9032\uff0c\u4f46\u5176\u9ed1\u76d2\u6027\u8cea\u5c0e\u81f4\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\uff0c\u4e00\u76f4\u662f\u4e00\u500b\u4e3b\u8981\u554f\u984c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 GAProtoNet\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u767d\u76d2\u591a\u982d\u5716\u6ce8\u610f\u529b\u539f\u578b\u7db2\u8def\uff0c\u65e8\u5728\u89e3\u91cb\u4f7f\u7528 LM \u7de8\u78bc\u5668\u5efa\u7acb\u7684\u6587\u672c\u5206\u985e\u6a21\u578b\u7684\u6c7a\u7b56\u3002\u5728\u6211\u5011\u7684\u505a\u6cd5\u4e2d\uff0c\u8f38\u5165\u5411\u91cf\u548c\u539f\u578b\u88ab\u8996\u70ba\u5716\u5f62\u4e2d\u7684\u7bc0\u9ede\uff0c\u6211\u5011\u5229\u7528\u591a\u982d\u5716\u6ce8\u610f\u529b\u5728\u8f38\u5165\u7bc0\u9ede\u548c\u539f\u578b\u7bc0\u9ede\u4e4b\u9593\u6709\u9078\u64c7\u5730\u69cb\u9020\u908a\u7de3\uff0c\u4ee5\u5b78\u7fd2\u53ef\u89e3\u91cb\u7684\u539f\u578b\u8868\u793a\u3002\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\uff0c\u6a21\u578b\u6839\u64da\u6fc0\u6d3b\u539f\u578b\u7684\u7dda\u6027\u7d44\u5408\u505a\u51fa\u6c7a\u7b56\uff0c\u8a72\u7d44\u5408\u7531\u5206\u914d\u7d66\u6bcf\u500b\u539f\u578b\u7684\u6ce8\u610f\u529b\u5206\u6578\u52a0\u6b0a\uff0c\u5f9e\u800c\u5141\u8a31\u901a\u904e\u6ce8\u610f\u529b\u6b0a\u91cd\u548c\u6295\u5f71\u5230\u6700\u63a5\u8fd1\u5339\u914d\u8a13\u7df4\u7bc4\u4f8b\u7684\u539f\u578b\u4f86\u900f\u660e\u5730\u89e3\u91cb\u5176\u9078\u64c7\u3002\u5728\u591a\u500b\u516c\u5171\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u4e0d\u72a7\u7272\u539f\u59cb\u9ed1\u76d2 LM \u7684\u6e96\u78ba\u6027\u7684\u60c5\u6cc1\u4e0b\uff0c\u53d6\u5f97\u4e86\u512a\u7570\u7684\u7d50\u679c\u3002\u6211\u5011\u9084\u8207\u56db\u7a2e\u66ff\u4ee3\u539f\u578b\u7db2\u8def\u8b8a\u9ad4\u9032\u884c\u4e86\u6bd4\u8f03\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u6240\u6709\u8b8a\u9ad4\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u6e96\u78ba\u6027\u548c F1\u3002\u6211\u5011\u7684\u6848\u4f8b\u7814\u7a76\u548c\u539f\u578b\u7fa4\u96c6\u7684\u53ef\u8996\u5316\u4e5f\u8b49\u660e\u4e86\u4f7f\u7528 LM \u5efa\u7acb\u7684\u9ed1\u76d2\u6a21\u578b\u6c7a\u7b56\u7684\u89e3\u91cb\u6548\u7387\u3002", "author": "Ximing Wen et.al.", "authors": "Ximing Wen, Wenjuan Tan, Rosina O. Weber", "id": "2409.13312v1", "paper_url": "http://arxiv.org/abs/2409.13312v1", "repo": "null"}}