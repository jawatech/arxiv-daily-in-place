{"2410.07103": {"publish_time": "2024-10-09", "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context", "paper_summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.", "paper_summary_zh": "\u591a\u8df3\u63a8\u7406\u9700\u8981\u6839\u64da\u7d66\u5b9a\u8108\u7d61\u4e2d\u7684\u652f\u63f4\u6587\u4ef6\u9032\u884c\u591a\u6b65\u9a5f\u63a8\u7406\uff0c\u9019\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8aaa\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002LLM \u7d93\u5e38\u96e3\u4ee5\u5728\u8108\u7d61\u4e2d\u904e\u6ffe\u6389\u7121\u95dc\u7684\u6587\u4ef6\uff0c\u800c\u4e14\u5b83\u5011\u7684\u6548\u80fd\u6703\u53d7\u5230\u652f\u63f4\u6587\u4ef6\u5728\u8a72\u8108\u7d61\u4e2d\u7684\u4f4d\u7f6e\u5f71\u97ff\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u627e\u51fa\u53e6\u4e00\u500b\u6311\u6230\uff1aLLM \u7684\u6548\u80fd\u4e5f\u6703\u53d7\u5230\u652f\u63f4\u6587\u4ef6\u5448\u73fe\u9806\u5e8f\u7684\u5f71\u97ff\u3002\u6211\u5011\u5c07\u6b64\u7a31\u70ba\u9806\u5e8f\u932f\u8aa4\u7684\u8108\u7d61\u554f\u984c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba\u8108\u7d61\u91cd\u8907 (CoRe)\uff0c\u5176\u4e2d\u6d89\u53ca\u900f\u904e\u91cd\u8907\u5448\u73fe\u8108\u7d61\u4f86\u63d0\u793a\u6a21\u578b\uff0c\u4ee5\u78ba\u4fdd\u652f\u63f4\u6587\u4ef6\u4ee5\u6a21\u578b\u7684\u6700\u4f73\u9806\u5e8f\u5448\u73fe\u3002\u4f7f\u7528 CoRe\uff0c\u6211\u5011\u5c07\u591a\u8df3\u554f\u7b54\u4efb\u52d9\u7684 F1 \u5206\u6578\u63d0\u9ad8\u4e86 30%\uff0c\u4e26\u5728\u5408\u6210\u4efb\u52d9\u4e2d\u5c07\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 70%\u3002\u6b64\u5916\uff0cCoRe \u6709\u52a9\u65bc\u6e1b\u8f15 LLM \u4e2d\u773e\u6240\u5468\u77e5\u7684\u300c\u8ff7\u5931\u5728\u4e2d\u9593\u300d\u554f\u984c\uff0c\u4e26\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u8207\u5229\u7528\u601d\u60f3\u93c8 (CoT) \u63a8\u7406\u7684\u57fa\u65bc\u6aa2\u7d22\u7684\u65b9\u6cd5\u7d50\u5408\u4f7f\u7528\u3002", "author": "Sangwon Yu et.al.", "authors": "Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon", "id": "2410.07103v1", "paper_url": "http://arxiv.org/abs/2410.07103v1", "repo": "null"}}