{"2412.00251": {"publish_time": "2024-11-29", "title": "Fine Tuning Large Language Models to Deliver CBT for Depression", "paper_summary": "Cognitive Behavioral Therapy (CBT) is a well-established, evidence-based\ntreatment for Major Depressive Disorder. Unfortunately, there exist significant\nbarriers to individuals accessing CBT, including cost, scarcity of therapists\nand stigma. This study explores the feasibility of fine-tuning small open\nweight large language models (LLMs) to deliver CBT for depression. Using 58\nsets of synthetic CBT transcripts generated by the Nous Research fine-tune of\nLlama 3.1 405b, we fine-tuned three models: Mistral 7b v0.3, Qwen 2.5 7b, and\nLlama 3.1 8b. CBT fidelity was evaluated through a modified Cognitive Therapy\nRating Scale (CTRS). All fine-tuned models were compared against each other, as\nwell as their instruct-tuned variants. Simulated patient transcripts were\ngenerated for the purpose of evaluating model performance, with the instruct\nand CBT-tuned models acting as the therapist and DeepSeek-V2.5 acting as the\npatient. These simulated transcripts were evaluated on a modified CTRS by\nGemini 1.5 Pro-002. Our findings demonstrated that the CBT-tuned models\nsignificantly outperformed their instruct-tuned counterparts, with an average\nimprovement of 11.33 points (p < 0.001) on total CTRS score. Llama 3.1 8b had\nthe strongest performance (mean CTRS score 67.86 +/- 7.24), followed by Qwen\n2.5 7b (64.28 +/- 9.55) and Mistral 7b v0.3 (64.17 +/- 9.79), with these\ndifferences between models being statistically significant. The CBT-tuned\nmodels were competent in implementing core CBT techniques and providing\nempathetic responses, however, there were limitations observed in agenda\nadherence, exploration depth and long-context coherence. This study establishes\nthat CBT specific fine-tuning can effectively encode therapeutic competencies\nin small LLMs, though significant technical and ethical considerations must be\nresolved prior to clinical deployment.", "paper_summary_zh": "<paragraph>\u8a8d\u77e5\u884c\u70ba\u7642\u6cd5 (CBT) \u662f\u4e00\u7a2e\u6cbb\u7642\u91cd\u5ea6\u6182\u9b31\u75c7\u7684\u5b8c\u5584\u4e14\u6709\u5be6\u8b49\u57fa\u790e\u7684\u7642\u6cd5\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u500b\u4eba\u63a5\u53d7 CBT \u4ecd\u5b58\u5728\u91cd\u5927\u969c\u7919\uff0c\u5305\u62ec\u8cbb\u7528\u3001\u6cbb\u7642\u5e2b\u7a00\u7f3a\u548c\u6c59\u540d\u5316\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u5fae\u8abf\u5c0f\u578b\u958b\u653e\u5f0f\u6b0a\u91cd\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u63d0\u4f9b CBT \u6cbb\u7642\u6182\u9b31\u75c7\u7684\u53ef\u884c\u6027\u3002\u4f7f\u7528 Nous Research \u5fae\u8abf Llama 3.1 405b \u6240\u7522\u751f\u7684 58 \u7d44\u5408\u6210 CBT \u8b04\u672c\uff0c\u6211\u5011\u5fae\u8abf\u4e86\u4e09\u500b\u6a21\u578b\uff1aMistral 7b v0.3\u3001Qwen 2.5 7b \u548c Llama 3.1 8b\u3002CBT \u4fdd\u771f\u5ea6\u900f\u904e\u4fee\u6b63\u5f8c\u7684\u8a8d\u77e5\u6cbb\u7642\u8a55\u5206\u91cf\u8868 (CTRS) \u9032\u884c\u8a55\u4f30\u3002\u6240\u6709\u5fae\u8abf\u6a21\u578b\u5f7c\u6b64\u6bd4\u8f03\uff0c\u4ee5\u53ca\u5b83\u5011\u7684\u6307\u4ee4\u5fae\u8abf\u8b8a\u9ad4\u3002\u6a21\u64ec\u60a3\u8005\u8b04\u672c\u662f\u70ba\u4e86\u8a55\u4f30\u6a21\u578b\u6548\u80fd\u800c\u7522\u751f\u7684\uff0c\u6307\u4ee4\u548c CBT \u5fae\u8abf\u6a21\u578b\u626e\u6f14\u6cbb\u7642\u5e2b\uff0c\u800c DeepSeek-V2.5 \u626e\u6f14\u60a3\u8005\u3002\u9019\u4e9b\u6a21\u64ec\u8b04\u672c\u7531 Gemini 1.5 Pro-002 \u4f7f\u7528\u4fee\u6b63\u5f8c\u7684 CTRS \u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cCBT \u5fae\u8abf\u6a21\u578b\u986f\u8457\u512a\u65bc\u5176\u6307\u4ee4\u5fae\u8abf\u6a21\u578b\uff0cCTRS \u7e3d\u5206\u5e73\u5747\u63d0\u5347 11.33 \u5206 (p < 0.001)\u3002Llama 3.1 8b \u6548\u80fd\u6700\u5f37 (CTRS \u5e73\u5747\u5206\u6578 67.86 +/- 7.24)\uff0c\u5176\u6b21\u662f Qwen 2.5 7b (64.28 +/- 9.55) \u548c Mistral 7b v0.3 (64.17 +/- 9.79)\uff0c\u9019\u4e9b\u6a21\u578b\u4e4b\u9593\u7684\u5dee\u7570\u5177\u6709\u7d71\u8a08\u986f\u8457\u6027\u3002CBT \u5fae\u8abf\u6a21\u578b\u5728\u5be6\u65bd\u6838\u5fc3 CBT \u6280\u8853\u548c\u63d0\u4f9b\u540c\u7406\u56de\u61c9\u65b9\u9762\u8868\u73fe\u5f97\u5f88\u597d\uff0c\u7136\u800c\u5728\u8b70\u7a0b\u9075\u5faa\u3001\u63a2\u7d22\u6df1\u5ea6\u548c\u9577\u8108\u7d61\u9023\u8cab\u6027\u65b9\u9762\u4ecd\u6709\u89c0\u5bdf\u5230\u7684\u9650\u5236\u3002\u672c\u7814\u7a76\u8b49\u5be6\uff0c\u7279\u5b9a\u65bc CBT \u7684\u5fae\u8abf\u53ef\u4ee5\u6709\u6548\u5730\u5c07\u6cbb\u7642\u80fd\u529b\u7de8\u78bc\u5230\u5c0f\u578b LLM \u4e2d\uff0c\u5118\u7ba1\u5728\u81e8\u5e8a\u90e8\u7f72\u4e4b\u524d\u5fc5\u9808\u89e3\u6c7a\u91cd\u5927\u7684\u6280\u8853\u548c\u502b\u7406\u8003\u91cf\u3002</paragraph>", "author": "Talha Tahir et.al.", "authors": "Talha Tahir", "id": "2412.00251v1", "paper_url": "http://arxiv.org/abs/2412.00251v1", "repo": "null"}}