{"2409.06595": {"publish_time": "2024-09-10", "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering", "paper_summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u5df2\u6210\u70ba\u4e00\u7a2e\u5e38\u898b\u7bc4\u4f8b\uff0c\u53ef\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u79c1\u4eba\u4e14\u6700\u65b0\u7684\u77e5\u8b58\u5eab\u4e00\u8d77\u4f7f\u7528\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5728\u8a55\u4f30 RAG \u7cfb\u7d71\u751f\u6210\u7684\u57fa\u790e\u7b54\u6848\u6642\uff0c\u63a2\u8a0e\u4e86\u4f7f\u7528 LLM \u4f5c\u70ba\u8a55\u5be9\u6642\u6240\u9762\u81e8\u7684\u6311\u6230\u3002\u70ba\u4e86\u8a55\u4f30\u8a55\u5be9\u6a21\u578b\u7684\u6821\u6e96\u548c\u5340\u5206\u80fd\u529b\uff0c\u6211\u5011\u627e\u51fa 7 \u7a2e\u751f\u6210\u5668\u6545\u969c\u6a21\u5f0f\uff0c\u4e26\u5f15\u5165\u4e86 GroUSE\uff08\u8a55\u4f30\u4eba\u54e1\u7684\u57fa\u790e\u554f\u7b54\u55ae\u5143\u8a55\u5206\uff09\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 144 \u500b\u55ae\u5143\u6e2c\u8a66\u7684\u5143\u8a55\u4f30\u57fa\u6e96\u3002\u6b64\u57fa\u6e96\u63ed\u793a\u4e86\u73fe\u6709\u7684\u81ea\u52d5\u5316 RAG \u8a55\u4f30\u67b6\u69cb\u901a\u5e38\u6703\u5ffd\u7565\u91cd\u8981\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u5373\u4f7f\u4f7f\u7528 GPT-4 \u4f5c\u70ba\u8a55\u5be9\u4e5f\u662f\u5982\u6b64\u3002\u70ba\u4e86\u6539\u5584\u81ea\u52d5\u5316 RAG \u8a55\u4f30\u67b6\u69cb\u7684\u7576\u524d\u8a2d\u8a08\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u7ba1\u9053\uff0c\u4e26\u767c\u73fe\u5c01\u9589\u6a21\u578b\u5728 GroUSE \u4e0a\u8868\u73fe\u826f\u597d\uff0c\u4f46\u6700\u5148\u9032\u7684\u958b\u6e90\u8a55\u5be9\u4e26\u672a\u6982\u62ec\u5230\u6211\u5011\u63d0\u51fa\u7684\u6a19\u6e96\uff0c\u5118\u7ba1\u8207 GPT-4 \u7684\u5224\u65b7\u6709\u5f88\u5f37\u7684\u76f8\u95dc\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u8207 GPT-4 \u7684\u76f8\u95dc\u6027\u662f\u8a55\u5be9\u6a21\u578b\u5be6\u969b\u6548\u80fd\u7684\u4e0d\u5b8c\u6574\u4ee3\u7406\uff0c\u61c9\u88dc\u5145\u55ae\u5143\u6e2c\u8a66\u7684\u8a55\u4f30\uff0c\u4ee5\u9032\u884c\u7cbe\u78ba\u7684\u6545\u969c\u6a21\u5f0f\u5075\u6e2c\u3002\u6211\u5011\u9032\u4e00\u6b65\u8868\u660e\uff0c\u5728 GPT-4 \u7684\u63a8\u7406\u8ecc\u8de1\u4e0a\u5fae\u8abf Llama-3 \u53ef\u986f\u8457\u63d0\u5347\u5176\u8a55\u4f30\u80fd\u529b\uff0c\u540c\u6642\u6539\u5584\u8207 GPT-4 \u8a55\u4f30\u7684\u76f8\u95dc\u6027\uff0c\u4e26\u6821\u6e96\u53c3\u8003\u60c5\u6cc1\u3002", "author": "Sacha Muller et.al.", "authors": "Sacha Muller, Ant\u00f3nio Loison, Bilel Omrani, Gautier Viaud", "id": "2409.06595v1", "paper_url": "http://arxiv.org/abs/2409.06595v1", "repo": "null"}}