{"2501.07766": {"publish_time": "2025-01-14", "title": "Large Language Models for Knowledge Graph Embedding Techniques, Methods, and Challenges: A Survey", "paper_summary": "Large Language Models (LLMs) have attracted a lot of attention in various\nfields due to their superior performance, aiming to train hundreds of millions\nor more parameters on large amounts of text data to understand and generate\nnatural language. As the superior performance of LLMs becomes apparent, they\nare increasingly being applied to knowledge graph embedding (KGE) related tasks\nto improve the processing results. As a deep learning model in the field of\nNatural Language Processing (NLP), it learns a large amount of textual data to\npredict the next word or generate content related to a given text. However,\nLLMs have recently been invoked to varying degrees in different types of KGE\nrelated scenarios such as multi-modal KGE and open KGE according to their task\ncharacteristics. In this paper, we investigate a wide range of approaches for\nperforming LLMs-related tasks in different types of KGE scenarios. To better\ncompare the various approaches, we summarize each KGE scenario in a\nclassification. In addition to the categorization methods, we provide a tabular\noverview of the methods and their source code links for a more direct\ncomparison. In the article we also discuss the applications in which the\nmethods are mainly used and suggest several forward-looking directions for the\ndevelopment of this new research area.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u5176\u512a\u7570\u7684\u6027\u80fd\uff0c\u5728\u5404\u500b\u9818\u57df\u4e2d\u5f15\u8d77\u4e86\u8a31\u591a\u95dc\u6ce8\uff0c\u76ee\u6a19\u662f\u8a13\u7df4\u6578\u5104\u6216\u66f4\u591a\u53c3\u6578\uff0c\u4ee5\u7406\u89e3\u548c\u7522\u751f\u5927\u91cf\u6587\u672c\u8cc7\u6599\u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u3002\u96a8\u8457 LLM \u512a\u7570\u6027\u80fd\u7684\u986f\u73fe\uff0c\u5b83\u5011\u6b63\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u5730\u61c9\u7528\u65bc\u77e5\u8b58\u5716\u8b5c\u5d4c\u5165 (KGE) \u76f8\u95dc\u4efb\u52d9\uff0c\u4ee5\u6539\u5584\u8655\u7406\u7d50\u679c\u3002\u4f5c\u70ba\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u4e2d\u7684\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u5b83\u5b78\u7fd2\u5927\u91cf\u7684\u6587\u672c\u8cc7\u6599\uff0c\u4ee5\u9810\u6e2c\u4e0b\u4e00\u500b\u55ae\u5b57\u6216\u7522\u751f\u8207\u7d66\u5b9a\u6587\u672c\u76f8\u95dc\u7684\u5167\u5bb9\u3002\u7136\u800c\uff0c\u6839\u64da\u4efb\u52d9\u7279\u6027\uff0cLLM \u6700\u8fd1\u5df2\u5728\u4e0d\u540c\u985e\u578b\u7684 KGE \u76f8\u95dc\u5834\u666f\uff08\u4f8b\u5982\u591a\u6a21\u614b KGE \u548c\u958b\u653e\u5f0f KGE\uff09\u4e2d\u4ee5\u4e0d\u540c\u7a0b\u5ea6\u88ab\u63a1\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5728\u4e0d\u540c\u985e\u578b\u7684 KGE \u5834\u666f\u4e2d\u57f7\u884c\u8207 LLM \u76f8\u95dc\u4efb\u52d9\u7684\u5404\u7a2e\u65b9\u6cd5\u3002\u70ba\u4e86\u66f4\u597d\u5730\u6bd4\u8f03\u5404\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u5728\u5206\u985e\u4e2d\u7e3d\u7d50\u4e86\u6bcf\u500b KGE \u5834\u666f\u3002\u9664\u4e86\u5206\u985e\u65b9\u6cd5\u4e4b\u5916\uff0c\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u65b9\u6cd5\u53ca\u5176\u539f\u59cb\u78bc\u9023\u7d50\u7684\u8868\u683c\u6982\u89c0\uff0c\u4ee5\u4fbf\u9032\u884c\u66f4\u76f4\u63a5\u7684\u6bd4\u8f03\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9084\u8a0e\u8ad6\u4e86\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u7528\u65bc\u54ea\u4e9b\u61c9\u7528\uff0c\u4e26\u5efa\u8b70\u4e86\u5e7e\u500b\u9019\u500b\u65b0\u7814\u7a76\u9818\u57df\u767c\u5c55\u7684\u524d\u77bb\u6027\u65b9\u5411\u3002", "author": "Bingchen Liu et.al.", "authors": "Bingchen Liu, Xin Li", "id": "2501.07766v1", "paper_url": "http://arxiv.org/abs/2501.07766v1", "repo": "null"}}