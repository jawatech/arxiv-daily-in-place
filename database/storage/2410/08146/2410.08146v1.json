{"2410.08146": {"publish_time": "2024-10-10", "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning", "paper_summary": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs.", "paper_summary_zh": "\u4e00\u7a2e\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u80fd\u529b\u7684 promising \u65b9\u6cd5\u662f\u4f7f\u7528\u904e\u7a0b\u734e\u52f5\u6a21\u578b (PRM)\u3002PRM \u5728\u591a\u6b65\u9a5f\u63a8\u7406\u8ffd\u8e64\u7684\u6bcf\u4e00\u6b65\u63d0\u4f9b\u56de\u994b\uff0c\u6f5b\u5728\u6539\u5584\u4e86\u5c0d\u7d50\u679c\u734e\u52f5\u6a21\u578b (ORM) \u7684\u4fe1\u7528\u5206\u914d\uff0c\u800c\u5f8c\u8005\u50c5\u5728\u6700\u5f8c\u4e00\u6b65\u63d0\u4f9b\u56de\u994b\u3002\n\u7136\u800c\uff0c\u6536\u96c6\u5bc6\u96c6\u7684\u3001\u6309\u6b65\u9a5f\u9032\u884c\u7684\u4eba\u985e\u6a19\u7c64\u4e26\u975e\u53ef\u64f4\u5145\u7684\uff0c\u800c\u5f9e\u81ea\u52d5\u6a19\u7c64\u8cc7\u6599\u8a13\u7df4 PRM \u81f3\u4eca\u50c5\u7372\u5f97\u6709\u9650\u7684\u9032\u5c55\u3002\u70ba\u4e86\u900f\u904e\u91dd\u5c0d PRM \u57f7\u884c\u641c\u5c0b\u6216\u5c07\u5176\u7528\u4f5c\u5f37\u5316\u5b78\u7fd2 (RL) \u7684\u5bc6\u96c6\u734e\u52f5\u4f86\u6539\u5584\u57fa\u790e\u653f\u7b56\uff0c\u6211\u5011\u63d0\u51fa\u7591\u554f\uff1a\u300c\u6211\u5011\u61c9\u5982\u4f55\u8a2d\u8a08\u904e\u7a0b\u734e\u52f5\uff1f\u300d\u6211\u5011\u7684\u95dc\u9375\u898b\u89e3\u5728\u65bc\uff0c\u904e\u7a0b\u734e\u52f5\u82e5\u8981\u6709\u6548\uff0c\u5c0d\u65bc\u67d0\u500b\u6b65\u9a5f\u800c\u8a00\uff0c\u61c9\u8861\u91cf\u9032\u5ea6\uff1a\u5728\u63a1\u53d6\u6b65\u9a5f\u4e4b\u524d\u548c\u4e4b\u5f8c\uff0c\u7522\u751f\u6b63\u78ba\u56de\u61c9\u7684\u53ef\u80fd\u6027\u767c\u751f\u8b8a\u5316\uff0c\u9019\u5c0d\u61c9\u65bc RL \u4e2d\u6b65\u9a5f\u5c64\u7d1a\u512a\u52e2\u7684\u6982\u5ff5\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u61c9\u5728\u8207\u57fa\u790e\u653f\u7b56\u4e0d\u540c\u7684\u8b49\u660e\u8005\u653f\u7b56\u4e0b\u8861\u91cf\u6b64\u9032\u5ea6\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u63cf\u8ff0\u4e86\u826f\u597d\u7684\u8b49\u660e\u8005\u7d44\u5408\uff0c\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u6700\u4f73\u5316\u4f86\u81ea\u6b64\u985e\u8b49\u660e\u8005\u7684\u904e\u7a0b\u734e\u52f5\u6703\u6539\u5584\u6e2c\u8a66\u6642\u9593\u641c\u5c0b\u548c\u7dda\u4e0a RL \u4e2d\u7684\u63a2\u7d22\u3002\u4e8b\u5be6\u4e0a\uff0c\u6211\u5011\u7684\u63cf\u8ff0\u986f\u793a\uff0c\u5f31\u8b49\u660e\u8005\u653f\u7b56\u53ef\u4ee5\u5927\u5e45\u6539\u5584\u8f03\u5f37\u7684\u57fa\u790e\u653f\u7b56\uff0c\u9019\u4e5f\u662f\u6211\u5011\u5728\u7d93\u9a57\u4e0a\u89c0\u5bdf\u5230\u7684\u3002\u6211\u5011\u900f\u904e\u8a13\u7df4\u904e\u7a0b\u512a\u52e2\u9a57\u8b49\u5668 (PAV) \u4f86\u9a57\u8b49\u6211\u5011\u7684\u8aaa\u6cd5\uff0c\u4ee5\u9810\u6e2c\u5728\u9019\u4e9b\u8b49\u660e\u8005\u4e0b\u7684\u9032\u5ea6\uff0c\u4e26\u986f\u793a\u8207 ORM \u76f8\u6bd4\uff0c\u91dd\u5c0d PAV \u7684\u6e2c\u8a66\u6642\u9593\u641c\u5c0b\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 $>8\\%$\uff0c\u8a08\u7b97\u6548\u7387\u63d0\u9ad8\u4e86 $1.5-5\\times$\u3002\u900f\u904e PAV \u7684\u5bc6\u96c6\u734e\u52f5\u9032\u884c\u7dda\u4e0a RL\uff0c\u7372\u5f97\u4e86\u6a23\u672c\u6548\u7387\u63d0\u9ad8 $5-6\\times$\u3001\u6e96\u78ba\u5ea6\u63d0\u9ad8 $>6\\%$ \u7684\u9996\u6279\u7d50\u679c\uff0c\u512a\u65bc ORM\u3002", "author": "Amrith Setlur et.al.", "authors": "Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar", "id": "2410.08146v1", "paper_url": "http://arxiv.org/abs/2410.08146v1", "repo": "null"}}