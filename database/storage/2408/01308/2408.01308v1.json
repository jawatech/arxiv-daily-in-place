{"2408.01308": {"publish_time": "2024-08-02", "title": "Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models", "paper_summary": "Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out the distribution of learned embeddings\ndegenerates into anisotropy, and even pre-trained language models (PLMs) suffer\nfrom a loss of semantics-related information in embeddings for low-frequency\ntokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,\nand demonstrates its robustness against degeneration. On the basis of this\nfinding, we propose DefinitionEMB, a method that utilizes definitions to\nconstruct isotropically distributed and semantics-related token embeddings for\nPLMs while maintaining original robustness during fine-tuning. Our experiments\ndemonstrate the effectiveness of leveraging definitions from Wiktionary to\nconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, the\nconstructed embeddings for low-frequency tokens improve the performance of\nthese models across various GLUE and four text summarization datasets.", "paper_summary_zh": "\u5b78\u7fd2\u57fa\u65bc\u6a19\u8a18\u5171\u73fe\u7d71\u8a08\u7684\u6a19\u8a18\u5d4c\u5165\u5df2\u8b49\u660e\u5c0d\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u7684\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u90fd\u5f88\u6709\u6548\u3002\n\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u6307\u51fa\uff0c\u5b78\u7fd2\u5230\u7684\u5d4c\u5165\u5206\u4f48\u6703\u9000\u5316\u70ba\u5404\u5411\u7570\u6027\uff0c\u751a\u81f3\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u4e5f\u6703\u56e0\u70ba\u4f4e\u983b\u7387\u6a19\u8a18\u7684\u5d4c\u5165\u800c\u55aa\u5931\u8a9e\u7fa9\u76f8\u95dc\u8cc7\u8a0a\u3002\u672c\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86 PLM BART-large \u7684\u5fae\u8abf\u52d5\u614b\uff0c\u4e26\u5c55\u793a\u5176\u5c0d\u9000\u5316\u7684\u7a69\u5065\u6027\u3002\u6839\u64da\u6b64\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DefinitionEMB\uff0c\u9019\u662f\u4e00\u7a2e\u5229\u7528\u5b9a\u7fa9\u4f86\u5efa\u69cb\u5404\u5411\u540c\u6027\u5206\u4f48\u4e14\u8a9e\u7fa9\u76f8\u95dc\u7684 PLM \u6a19\u8a18\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u540c\u6642\u5728\u5fae\u8abf\u671f\u9593\u7dad\u6301\u539f\u59cb\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u5229\u7528\u7dad\u57fa\u8a5e\u5178\u4e2d\u7684\u5b9a\u7fa9\u4f86\u5efa\u69cb RoBERTa-base \u548c BART-large \u7684\u6b64\u985e\u5d4c\u5165\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u70ba\u4f4e\u983b\u7387\u6a19\u8a18\u5efa\u69cb\u7684\u5d4c\u5165\u6539\u5584\u4e86\u9019\u4e9b\u6a21\u578b\u5728\u5404\u7a2e GLUE \u548c\u56db\u500b\u6587\u672c\u6458\u8981\u8cc7\u6599\u96c6\u4e2d\u7684\u6548\u80fd\u3002", "author": "Ying Zhang et.al.", "authors": "Ying Zhang, Dongyuan Li, Manabu Okumura", "id": "2408.01308v1", "paper_url": "http://arxiv.org/abs/2408.01308v1", "repo": "null"}}