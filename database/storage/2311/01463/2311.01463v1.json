{"2311.01463": {"publish_time": "2023-09-26", "title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI", "paper_summary": "Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.", "paper_summary_zh": "", "author": "Muhammad Aurangzeb Ahmad et.al.", "authors": "Muhammad Aurangzeb Ahmad,Ilker Yaramis,Taposh Dutta Roy", "id": "2311.01463v1", "paper_url": "http://arxiv.org/abs/2311.01463v1", "repo": "null"}}