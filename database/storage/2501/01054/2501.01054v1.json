{"2501.01054": {"publish_time": "2025-01-02", "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling", "paper_summary": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus).", "paper_summary_zh": "<paragraph>\u7576\u524d\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9996\u6b21\u5617\u8a66\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\uff08\u4f8b\u5982\u7a0b\u5f0f\u78bc\u751f\u6210\uff09\u6642\uff0c\u901a\u5e38\u96e3\u4ee5\u7522\u751f\u6e96\u78ba\u7684\u56de\u61c9\u3002\u5148\u524d\u7684\u7814\u7a76\u900f\u904e\u7522\u751f\u591a\u500b\u5019\u9078\u89e3\uff0c\u4e26\u4f7f\u7528 LLM \u751f\u6210\u7684\u55ae\u5143\u6e2c\u8a66\u9a57\u8b49\u5b83\u5011\u4f86\u61c9\u5c0d\u6b64\u6311\u6230\u3002\u55ae\u5143\u6e2c\u8a66\u7684\u57f7\u884c\u7d50\u679c\u7528\u4f5c\u734e\u52f5\u8a0a\u865f\uff0c\u4ee5\u8b58\u5225\u6b63\u78ba\u7684\u89e3\u3002\u7531\u65bc LLM \u7e3d\u662f\u81ea\u4fe1\u5730\u72af\u932f\uff0c\u56e0\u6b64\u9019\u4e9b\u55ae\u5143\u6e2c\u8a66\u4e26\u4e0d\u53ef\u9760\uff0c\u5f9e\u800c\u964d\u4f4e\u4e86\u734e\u52f5\u8a0a\u865f\u7684\u54c1\u8cea\u3002\u53d7\u5230\u64f4\u5145\u89e3\u7684\u6578\u91cf\u6703\u6539\u5584 LLM \u6548\u80fd\u7684\u89c0\u5bdf\u6240\u6fc0\u52f5\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u64f4\u5145\u55ae\u5143\u6e2c\u8a66\u4ee5\u63d0\u5347\u734e\u52f5\u8a0a\u865f\u54c1\u8cea\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u5148\u9a45\u5be6\u9a57\u63ed\u793a\u4e86\u55ae\u5143\u6e2c\u8a66\u6578\u91cf\u8207\u734e\u52f5\u8a0a\u865f\u54c1\u8cea\u4e4b\u9593\u7684\u6b63\u76f8\u95dc\uff0c\u5728\u66f4\u5177\u6311\u6230\u6027\u7684\u554f\u984c\u4e2d\u89c0\u5bdf\u5230\u66f4\u5927\u7684\u597d\u8655\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CodeRM-8B\uff0c\u9019\u662f\u4e00\u500b\u8f15\u91cf\u7d1a\u4f46\u6709\u6548\u7684\u55ae\u5143\u6e2c\u8a66\u7522\u751f\u5668\uff0c\u53ef\u5be6\u73fe\u9ad8\u6548\u4e14\u9ad8\u54c1\u8cea\u7684\u55ae\u5143\u6e2c\u8a66\u64f4\u5145\u3002\u6b64\u5916\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u500b\u52d5\u614b\u64f4\u5145\u6a5f\u5236\uff0c\u6839\u64da\u554f\u984c\u96e3\u5ea6\u8abf\u6574\u55ae\u5143\u6e2c\u8a66\u7684\u6578\u91cf\uff0c\u9032\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u4e09\u500b\u57fa\u6e96\u4e0a\u986f\u8457\u63d0\u5347\u4e86\u5404\u7a2e\u6a21\u578b\u7684\u6548\u80fd\uff08\u4f8b\u5982\uff0c\u5728 HumanEval Plus \u4e0a\uff0cLlama3-8B \u63d0\u5347\u4e86 18.43%\uff0cGPT-4o-mini \u63d0\u5347\u4e86 3.42%\uff09\u3002</paragraph>", "author": "Zeyao Ma et.al.", "authors": "Zeyao Ma, Xiaokang Zhang, Jing Zhang, Jifan Yu, Sijia Luo, Jie Tang", "id": "2501.01054v1", "paper_url": "http://arxiv.org/abs/2501.01054v1", "repo": "null"}}