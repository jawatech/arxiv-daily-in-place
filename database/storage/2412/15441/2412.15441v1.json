{"2412.15441": {"publish_time": "2024-12-19", "title": "Energy consumption of code small language models serving with runtime engines and execution providers", "paper_summary": "Background. The rapid growth of Language Models (LMs), particularly in code\ngeneration, requires substantial computational resources, raising concerns\nabout energy consumption and environmental impact. Optimizing LMs inference for\nenergy efficiency is crucial, and Small Language Models (SLMs) offer a\npromising solution to reduce resource demands.\n  Aim. Our goal is to analyze the impact of deep learning runtime engines and\nexecution providers on energy consumption, execution time, and\ncomputing-resource utilization from the point of view of software engineers\nconducting inference in the context of code SLMs.\n  Method. We conducted a technology-oriented, multi-stage experimental pipeline\nusing twelve code generation SLMs to investigate energy consumption, execution\ntime, and computing-resource utilization across the configurations.\n  Results. Significant differences emerged across configurations. CUDA\nexecution provider configurations outperformed CPU execution provider\nconfigurations in both energy consumption and execution time. Among the\nconfigurations, TORCH paired with CUDA demonstrated the greatest energy\nefficiency, achieving energy savings from 37.99% up to 89.16% compared to other\nserving configurations. Similarly, optimized runtime engines like ONNX with the\nCPU execution provider achieved from 8.98% up to 72.04% energy savings within\nCPU-based configurations. Also, TORCH paired with CUDA exhibited efficient\ncomputing-resource utilization.\n  Conclusions. Serving configuration choice significantly impacts energy\nefficiency. While further research is needed, we recommend the above\nconfigurations best suited to software engineers' requirements for enhancing\nserving efficiency in energy and performance.", "paper_summary_zh": "<paragraph>\u80cc\u666f\u3002\u8a9e\u8a00\u6a21\u578b\uff08LM\uff09\u7684\u5feb\u901f\u767c\u5c55\uff0c\u7279\u5225\u662f\u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\uff0c\u9700\u8981\u5927\u91cf\u7684\u8a08\u7b97\u8cc7\u6e90\uff0c\u5f15\u767c\u4e86\u5c0d\u80fd\u6e90\u6d88\u8017\u548c\u74b0\u5883\u5f71\u97ff\u7684\u64d4\u6182\u3002\u512a\u5316 LM \u63a8\u8ad6\u4ee5\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u81f3\u95dc\u91cd\u8981\uff0c\u800c\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\uff08SLM\uff09\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u964d\u4f4e\u8cc7\u6e90\u9700\u6c42\u3002\n\u76ee\u6a19\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u5206\u6790\u6df1\u5ea6\u5b78\u7fd2\u57f7\u884c\u5f15\u64ce\u548c\u57f7\u884c\u63d0\u4f9b\u8005\u5c0d\u80fd\u6e90\u6d88\u8017\u3001\u57f7\u884c\u6642\u9593\u548c\u8a08\u7b97\u8cc7\u6e90\u5229\u7528\u7387\u7684\u5f71\u97ff\uff0c\u5f9e\u8edf\u9ad4\u5de5\u7a0b\u5e2b\u5728\u7a0b\u5f0f\u78bc SLM \u7684\u80cc\u666f\u4e0b\u9032\u884c\u63a8\u8ad6\u7684\u89d2\u5ea6\u4f86\u770b\u3002\n\u65b9\u6cd5\u3002\u6211\u5011\u4f7f\u7528\u5341\u4e8c\u500b\u7a0b\u5f0f\u78bc\u751f\u6210 SLM \u9032\u884c\u4e86\u4e00\u9805\u4ee5\u6280\u8853\u70ba\u5c0e\u5411\u7684\u591a\u968e\u6bb5\u5be6\u9a57\u7ba1\u9053\uff0c\u4ee5\u8abf\u67e5\u914d\u7f6e\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u3001\u57f7\u884c\u6642\u9593\u548c\u8a08\u7b97\u8cc7\u6e90\u5229\u7528\u7387\u3002\n\u7d50\u679c\u3002\u914d\u7f6e\u4e4b\u9593\u51fa\u73fe\u4e86\u986f\u8457\u5dee\u7570\u3002CUDA \u57f7\u884c\u63d0\u4f9b\u8005\u914d\u7f6e\u5728\u80fd\u6e90\u6d88\u8017\u548c\u57f7\u884c\u6642\u9593\u4e0a\u90fd\u512a\u65bc CPU \u57f7\u884c\u63d0\u4f9b\u8005\u914d\u7f6e\u3002\u5728\u9019\u4e9b\u914d\u7f6e\u4e2d\uff0c\u8207 CUDA \u914d\u5c0d\u7684 TORCH \u8868\u73fe\u51fa\u6700\u9ad8\u7684\u80fd\u6e90\u6548\u7387\uff0c\u8207\u5176\u4ed6\u670d\u52d9\u914d\u7f6e\u76f8\u6bd4\uff0c\u7bc0\u80fd\u5f9e 37.99% \u9054\u5230 89.16%\u3002\u540c\u6a23\u5730\uff0c\u8207 CPU \u57f7\u884c\u63d0\u4f9b\u8005\u914d\u5c0d\u7684 ONNX \u7b49\u512a\u5316\u7684\u904b\u884c\u6642\u5f15\u64ce\u5728\u57fa\u65bc CPU \u7684\u914d\u7f6e\u4e2d\u5be6\u73fe\u4e86 8.98% \u5230 72.04% \u7684\u7bc0\u80fd\u3002\u6b64\u5916\uff0c\u8207 CUDA \u914d\u5c0d\u7684 TORCH \u8868\u73fe\u51fa\u9ad8\u6548\u7684\u8a08\u7b97\u8cc7\u6e90\u5229\u7528\u7387\u3002\n\u7d50\u8ad6\u3002\u670d\u52d9\u914d\u7f6e\u7684\u9078\u64c7\u986f\u8457\u5f71\u97ff\u80fd\u6e90\u6548\u7387\u3002\u96d6\u7136\u9700\u8981\u9032\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u4f46\u6211\u5011\u5efa\u8b70\u4e0a\u8ff0\u914d\u7f6e\u6700\u9069\u5408\u8edf\u9ad4\u5de5\u7a0b\u5e2b\u5c0d\u63d0\u9ad8\u80fd\u6e90\u548c\u6548\u80fd\u670d\u52d9\u6548\u7387\u7684\u8981\u6c42\u3002</paragraph>", "author": "Francisco Dur\u00e1n et.al.", "authors": "Francisco Dur\u00e1n, Matias Martinez, Patricia Lago, Silverio Mart\u00ednez-Fern\u00e1ndez", "id": "2412.15441v1", "paper_url": "http://arxiv.org/abs/2412.15441v1", "repo": "null"}}