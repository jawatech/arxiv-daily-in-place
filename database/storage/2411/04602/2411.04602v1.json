{"2411.04602": {"publish_time": "2024-11-07", "title": "Self-Calibrated Listwise Reranking with Large Language Models", "paper_summary": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5177\u6709\u5148\u9032\u7684\u8a9e\u8a00\u80fd\u529b\uff0c\u5df2\u900f\u904e\u5e8f\u5217\u5c0d\u5e8f\u5217\u65b9\u6cd5\u7528\u65bc\u91cd\u65b0\u6392\u5e8f\u4efb\u52d9\u3002\u5728\u6b64\u7bc4\u4f8b\u4e2d\uff0c\u591a\u500b\u6bb5\u843d\u6703\u4ee5\u5217\u8868\u65b9\u5f0f\u91cd\u65b0\u6392\u5e8f\uff0c\u4e26\u7522\u751f\u6587\u5b57\u91cd\u65b0\u6392\u5e8f\u6392\u5217\u3002\u4f46\u662f\uff0c\u7531\u65bc LLM \u7684\u5167\u5bb9\u8996\u7a97\u6709\u9650\uff0c\u6b64\u91cd\u65b0\u6392\u5e8f\u7bc4\u4f8b\u9700\u8981\u6ed1\u52d5\u8996\u7a97\u7b56\u7565\u4f86\u53cd\u8986\u8655\u7406\u8f03\u5927\u7684\u5019\u9078\u96c6\u3002\u9019\u4e0d\u50c5\u6703\u589e\u52a0\u904b\u7b97\u6210\u672c\uff0c\u9084\u6703\u9650\u5236 LLM \u5b8c\u6574\u64f7\u53d6\u6240\u6709\u5019\u9078\u96c6\u7684\u6240\u6709\u6bd4\u8f03\u8cc7\u8a0a\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u81ea\u6821\u6e96\u5217\u8868\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u65e8\u5728\u5229\u7528 LLM \u7522\u751f\u7528\u65bc\u6392\u540d\u7684\u6574\u9ad4\u76f8\u95dc\u6027\u5206\u6578\u3002\u70ba\u9054\u6210\u6b64\u76ee\u6a19\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u8207\u76f8\u95dc\u6027\u76f8\u95dc\u7684\u5217\u8868\u91cd\u65b0\u6392\u5e8f\u67b6\u69cb\uff0c\u5176\u4e2d\u5305\u542b\u660e\u78ba\u7684\u5217\u8868\u6aa2\u8996\u76f8\u95dc\u6027\u5206\u6578\uff0c\u4ee5\u63d0\u9ad8\u91cd\u65b0\u6392\u5e8f\u6548\u7387\u4e26\u91dd\u5c0d\u6574\u500b\u5019\u9078\u96c6\u555f\u7528\u6574\u9ad4\u6bd4\u8f03\u3002\u5176\u6b21\uff0c\u70ba\u4e86\u78ba\u4fdd\u8a08\u7b97\u5206\u6578\u7684\u53ef\u6bd4\u8f03\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528 LLM \u672c\u8eab\u5167\u90e8\u7522\u751f\u7684\u89c0\u9ede\u76f8\u95dc\u6027\u8a55\u4f30\u7684\u81ea\u6821\u6e96\u8a13\u7df4\uff0c\u4ee5\u6821\u6e96\u5217\u8868\u6aa2\u8996\u76f8\u95dc\u6027\u8a55\u4f30\u3002\u5728 BEIR \u57fa\u6e96\u548c TREC \u6df1\u5ea6\u5b78\u7fd2\u8ecc\u9053\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u548c\u5168\u9762\u5206\u6790\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "author": "Ruiyang Ren et.al.", "authors": "Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, Tat-Seng Chua", "id": "2411.04602v1", "paper_url": "http://arxiv.org/abs/2411.04602v1", "repo": "null"}}