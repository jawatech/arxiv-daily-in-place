{"2410.12341": {"publish_time": "2024-10-16", "title": "A linguistic analysis of undesirable outcomes in the era of generative AI", "paper_summary": "Recent research has focused on the medium and long-term impacts of generative\nAI, posing scientific and societal challenges mainly due to the detection and\nreliability of machine-generated information, which is projected to form the\nmajor content on the Web soon. Prior studies show that LLMs exhibit a lower\nperformance in generation tasks (model collapse) as they undergo a fine-tuning\nprocess across multiple generations on their own generated content\n(self-consuming loop). In this paper, we present a comprehensive simulation\nframework built upon the chat version of LLama2, focusing particularly on the\nlinguistic aspects of the generated content, which has not been fully examined\nin existing studies. Our results show that the model produces less lexical rich\ncontent across generations, reducing diversity. The lexical richness has been\nmeasured using the linguistic measures of entropy and TTR as well as\ncalculating the POSTags frequency. The generated content has also been examined\nwith an $n$-gram analysis, which takes into account the word order, and\nsemantic networks, which consider the relation between different words. These\nfindings suggest that the model collapse occurs not only by decreasing the\ncontent diversity but also by distorting the underlying linguistic patterns of\nthe generated text, which both highlight the critical importance of carefully\nchoosing and curating the initial input text, which can alleviate the model\ncollapse problem. Furthermore, we conduct a qualitative analysis of the\nfine-tuned models of the pipeline to compare their performances on generic NLP\ntasks to the original model. We find that autophagy transforms the initial\nmodel into a more creative, doubtful and confused one, which might provide\ninaccurate answers and include conspiracy theories in the model responses,\nspreading false and biased information on the Web.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u7814\u7a76\u4e13\u6ce8\u4e8e\u751f\u6210\u5f0f AI \u7684\u4e2d\u671f\u548c\u957f\u671f\u5f71\u54cd\uff0c\u4e3b\u8981\u7531\u4e8e\u673a\u5668\u751f\u6210\u4fe1\u606f\u7684\u68c0\u6d4b\u548c\u53ef\u9760\u6027\uff0c\u9884\u8ba1\u8fd9\u5c06\u5f88\u5feb\u5f62\u6210\u7f51\u7edc\u4e0a\u7684\u4e3b\u8981\u5185\u5bb9\uff0c\u4ece\u800c\u5e26\u6765\u79d1\u5b66\u548c\u793e\u4f1a\u6311\u6218\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0cLLM \u5728\u751f\u6210\u4efb\u52a1\uff08\u6a21\u578b\u5d29\u6e83\uff09\u4e2d\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u81ea\u5df1\u7684\u751f\u6210\u5185\u5bb9\u4e0a\u8de8\u591a\u4e2a\u4e16\u4ee3\u8fdb\u884c\u5fae\u8c03\u8fc7\u7a0b\uff08\u81ea\u8017\u5faa\u73af\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e LLama2 \u804a\u5929\u7248\u672c\u7684\u7efc\u5408\u6a21\u62df\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u751f\u6210\u5185\u5bb9\u7684\u8bed\u8a00\u65b9\u9762\uff0c\u8fd9\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u68c0\u9a8c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5404\u4e2a\u4e16\u4ee3\u4e2d\u4ea7\u751f\u7684\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u8f83\u4f4e\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u591a\u6837\u6027\u3002\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u5df2\u4f7f\u7528\u71b5\u548c TTR \u7684\u8bed\u8a00\u5ea6\u91cf\u4ee5\u53ca\u8ba1\u7b97 POSTags \u9891\u7387\u6765\u8861\u91cf\u3002\u8fd8\u4f7f\u7528\u8003\u8651\u5355\u8bcd\u987a\u5e8f\u7684 n-gram \u5206\u6790\u548c\u8003\u8651\u4e0d\u540c\u5355\u8bcd\u4e4b\u95f4\u5173\u7cfb\u7684\u8bed\u4e49\u7f51\u7edc\u68c0\u67e5\u4e86\u751f\u6210\u7684\u5185\u5bb9\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u6a21\u578b\u5d29\u6e83\u4e0d\u4ec5\u901a\u8fc7\u964d\u4f4e\u5185\u5bb9\u591a\u6837\u6027\u53d1\u751f\uff0c\u8fd8\u901a\u8fc7\u626d\u66f2\u751f\u6210\u6587\u672c\u7684\u5e95\u5c42\u8bed\u8a00\u6a21\u5f0f\u53d1\u751f\uff0c\u8fd9\u4e24\u8005\u90fd\u7a81\u51fa\u4e86\u4ed4\u7ec6\u9009\u62e9\u548c\u7b56\u5212\u521d\u59cb\u8f93\u5165\u6587\u672c\u7684\u5173\u952e\u91cd\u8981\u6027\uff0c\u8fd9\u53ef\u4ee5\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u6d41\u6c34\u7ebf\u7684\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u4ee5\u5c06\u5176\u5728\u901a\u7528 NLP \u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e\u539f\u59cb\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u53d1\u73b0\u81ea\u566c\u5c06\u521d\u59cb\u6a21\u578b\u8f6c\u5316\u4e3a\u66f4\u5177\u521b\u9020\u529b\u3001\u6000\u7591\u548c\u56f0\u60d1\u7684\u6a21\u578b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u63d0\u4f9b\u4e0d\u51c6\u786e\u7684\u7b54\u6848\uff0c\u5e76\u5728\u6a21\u578b\u54cd\u5e94\u4e2d\u5305\u542b\u9634\u8c0b\u8bba\uff0c\u5728\u7f51\u7edc\u4e0a\u6563\u5e03\u865a\u5047\u548c\u6709\u504f\u5dee\u7684\u4fe1\u606f\u3002</paragraph>", "author": "Daniele Gambetta et.al.", "authors": "Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo", "id": "2410.12341v1", "paper_url": "http://arxiv.org/abs/2410.12341v1", "repo": "null"}}