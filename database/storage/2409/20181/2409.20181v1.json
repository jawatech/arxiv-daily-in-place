{"2409.20181": {"publish_time": "2024-09-30", "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for Large Language Models", "paper_summary": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5feb\u901f\u9032\u6b65\u4e26\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u60c5\u5883\u5b78\u7fd2 (ICL) \u548c\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u76ee\u524d\u662f\u64f4\u5145 LLM \u4ee5\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u7684\u5169\u7a2e\u4e3b\u6d41\u65b9\u6cd5\u3002ICL \u901a\u5e38\u6703\u5efa\u69cb\u4e00\u500b\u5c0f\u6a23\u672c\u5b78\u7fd2\u60c5\u5883\uff0c\u53ef\u4ee5\u624b\u52d5\u6216\u900f\u904e\u8a2d\u5b9a\u6aa2\u7d22\u64f4\u5145\u751f\u6210 (RAG) \u7cfb\u7d71\u4f86\u57f7\u884c\uff0c\u5354\u52a9\u6a21\u578b\u5feb\u901f\u638c\u63e1\u9818\u57df\u77e5\u8b58\u6216\u554f\u7b54\u6a21\u5f0f\uff0c\u800c\u7121\u9700\u8b8a\u66f4\u6a21\u578b\u53c3\u6578\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u6703\u6d89\u53ca\u53d6\u6368\uff0c\u4f8b\u5982\u63a8\u8ad6\u901f\u5ea6\u8b8a\u6162\u548c\u7a7a\u9593\u4f54\u7528\u91cf\u589e\u52a0\u3002PEFT \u5354\u52a9\u6a21\u578b\u900f\u904e\u6700\u5c11\u7684\u53c3\u6578\u4fee\u6539\u4f86\u9069\u61c9\u4efb\u52d9\uff0c\u4f46\u8a13\u7df4\u904e\u7a0b\u4ecd\u9700\u8981\u5f88\u9ad8\u7684\u786c\u9ad4\u9700\u6c42\uff0c\u5373\u4f7f\u6d89\u53ca\u7684\u53c3\u6578\u6578\u91cf\u5f88\u5c11\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u53c3\u8003\u53ef\u4fe1\u89e3\u78bc (RTD)\uff0c\u9019\u662f\u4e00\u7a2e\u5178\u7bc4\uff0c\u8b93\u6a21\u578b\u80fd\u5920\u5feb\u901f\u9069\u61c9\u65b0\u4efb\u52d9\uff0c\u800c\u7121\u9700\u5fae\u8abf\uff0c\u4e26\u7dad\u6301\u4f4e\u63a8\u8ad6\u6210\u672c\u3002RTD \u5f9e\u63d0\u4f9b\u7684\u8a13\u7df4\u7bc4\u4f8b\u5efa\u69cb\u53c3\u8003\u8cc7\u6599\u5132\u5b58\u5eab\uff0c\u4e26\u900f\u904e\u6839\u64da\u8f38\u5165\u9748\u6d3b\u5730\u9078\u64c7\u5408\u9069\u7684\u53c3\u8003\u4f86\u6700\u4f73\u5316 LLM \u7684\u6700\u7d42\u8a5e\u5f59\u5206\u4f48\uff0c\u9032\u800c\u7522\u751f\u66f4\u53ef\u4fe1\u7684\u56de\u61c9\uff0c\u4e26\u8b93\u6a21\u578b\u80fd\u5920\u4ee5\u4f4e\u6210\u672c\u9069\u61c9\u4e0b\u6e38\u4efb\u52d9\u3002\u4f7f\u7528\u4e0d\u540c\u57fa\u6e96\u5c0d\u5404\u7a2e LLM \u9032\u884c\u7684\u5be6\u9a57\u8a55\u4f30\u986f\u793a\uff0cRTD \u70ba\u64f4\u5145\u6a21\u578b\u4ee5\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u5efa\u7acb\u4e86\u4e00\u500b\u65b0\u7684\u5178\u7bc4\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5c55\u73fe\u51fa\u8207\u50b3\u7d71\u65b9\u6cd5\u7684\u5f37\u5927\u6b63\u4ea4\u6027\uff0c\u5141\u8a31\u540c\u6642\u4f7f\u7528\u3002", "author": "Luohe Shi et.al.", "authors": "Luohe Shi, Yao Yao, Zuchao Li, Lefei Zhang, Hai Zhao", "id": "2409.20181v1", "paper_url": "http://arxiv.org/abs/2409.20181v1", "repo": "null"}}