{"2410.05021": {"publish_time": "2024-10-07", "title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "paper_summary": "Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u5f97\u76ca\u65bc\u66f4\u5ee3\u6cdb\u7684\u8cc7\u6599\u7d44\u5408\uff0c\u4ee5\u63d0\u5347\u8de8\u9818\u57df\u548c\u8de8\u8a9e\u8a00\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5728\u5982\u6b64\u7570\u8cea\u7684\u6587\u5b57\u8a9e\u6599\u5eab\u4e0a\u9032\u884c\u8a13\u7df4\u5f88\u8907\u96dc\uff0c\u9700\u8981\u5ee3\u6cdb\u4e14\u6210\u672c\u5bc6\u96c6\u7684\u52aa\u529b\u3002\u7531\u65bc\u9019\u4e9b\u8cc7\u6599\u4f86\u6e90\u5728\u8a5e\u5f59\u3001\u53e5\u6cd5\u548c\u8a9e\u7fa9\u65b9\u9762\u6709\u6240\u4e0d\u540c\uff0c\u56e0\u6b64\u6703\u9020\u6210\u8ca0\u9762\u5e72\u64fe\u6216\u300c\u591a\u8a9e\u8a00\u7684\u8a5b\u5492\u300d\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u9810\u8a13\u7df4\u67b6\u69cb\u4f86\u6e1b\u8f15\u9019\u500b\u8a5b\u5492\u3002\u6211\u5011\u7684 DEPT \u65b9\u6cd5\u5c07\u5d4c\u5165\u5c64\u8207 Transformer \u4e3b\u9ad4\u5206\u958b\uff0c\u540c\u6642\u5728\u591a\u91cd\u8108\u7d61\u4e2d\u8a13\u7df4\u5f8c\u8005\u3002DEPT \u4f7f\u6a21\u578b\u80fd\u5920\u5728\u4e0d\u53d7\u9650\u65bc\u5171\u4eab\u5168\u5c40\u8a5e\u5f59\u8868\u7684\u60c5\u6cc1\u4e0b\u9032\u884c\u8a13\u7df4\u3002DEPT\uff1a(1) \u80fd\u5728\u986f\u8457\u7684\u8cc7\u6599\u7570\u8cea\u6027\u4e0b\u7a69\u5065\u4e14\u6709\u6548\u5730\u8a13\u7df4\uff0c(2) \u5c07\u6b0a\u6756\u5d4c\u5165\u7684\u53c3\u6578\u6578\u91cf\u6e1b\u5c11\u591a\u9054 80%\uff0c\u4e26\u5c07\u5341\u5104\u898f\u6a21\u6a21\u578b\u7684\u901a\u8a0a\u6210\u672c\u964d\u4f4e 675 \u500d\uff0c(3) \u589e\u5f37\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9069\u61c9\u65b0\u8a9e\u8a00\u548c\u9818\u57df\u7684\u53ef\u5851\u6027\uff0c\u4ee5\u53ca (4) \u5141\u8a31\u4f7f\u7528\u91dd\u5c0d\u6bcf\u500b\u8cc7\u6599\u4f86\u6e90\u6700\u4f73\u5316\u904e\u7684\u81ea\u8a02\u8a5e\u5f59\u8868\u9032\u884c\u8a13\u7df4\u3002\u6211\u5011\u900f\u904e\u57f7\u884c\u7b2c\u4e00\u500b\u8207\u8a5e\u5f59\u7121\u95dc\u7684 13 \u5104\u53c3\u6578\u6a21\u578b\u806f\u90a6\u591a\u8a9e\u8a00\u9810\u8a13\u7df4\uff0c\u8de8\u8d8a\u9ad8\u8cc7\u6e90\u548c\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff0c\u8b49\u660e\u4e86 DEPT \u7684\u6f5b\u529b\uff0c\u5c07\u5176\u53c3\u6578\u6578\u91cf\u6e1b\u5c11\u4e86 4.09 \u5104\u3002", "author": "Alex Iacob et.al.", "authors": "Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane", "id": "2410.05021v1", "paper_url": "http://arxiv.org/abs/2410.05021v1", "repo": "null"}}