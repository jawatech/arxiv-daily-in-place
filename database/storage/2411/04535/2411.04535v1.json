{"2411.04535": {"publish_time": "2024-11-07", "title": "Meta-Reasoning Improves Tool Use in Large Language Models", "paper_summary": "External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets.", "paper_summary_zh": "\u5916\u90e8\u5de5\u5177\u53ef\u5354\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5176\u539f\u672c\u901a\u5e38\u6703\u5931\u6557\u7684\u4efb\u52d9\u4e2d\u7372\u5f97\u6210\u529f\u3002\u5728\u73fe\u6709\u7684\u67b6\u69cb\u4e2d\uff0cLLM \u900f\u904e\u60c5\u5883\u4e2d\u7684\u793a\u7bc4\u6216\u900f\u904e\u91dd\u5c0d\u8a3b\u89e3\u8cc7\u6599\u9032\u884c\u5b8c\u6574\u7684\u6a21\u578b\u5fae\u8abf\u4f86\u5b78\u7fd2\u4f7f\u7528\u5de5\u5177\u3002\u7531\u65bc\u9019\u4e9b\u65b9\u6cd5\u4e0d\u6613\u64f4\u5c55\uff0c\u56e0\u6b64\u6700\u8fd1\u7684\u8da8\u52e2\u662f\u653e\u68c4\u5b83\u5011\uff0c\u8f49\u800c\u63a1\u7528\u8f15\u91cf\u7d1a\u3001\u53c3\u6578\u9ad8\u6548\u7684\u5fae\u8abf\u7bc4\u4f8b\u3002\u9019\u4e9b\u65b9\u6cd5\u5141\u8a31\u900f\u904e\u555f\u7528\u6216\u505c\u7528\u5c11\u6578\u984d\u5916\u7684\u81ea\u8a02\u53c3\u6578\uff0c\u5728\u51cd\u7d50\u7684 LLM \u548c\u5176\u7d93\u904e\u5c08\u9580\u5fae\u8abf\u7684\u7248\u672c\u4e4b\u9593\u5feb\u901f\u4ea4\u66ff\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5047\u8a2d\u53ef\u4ee5\u5229\u7528\u51cd\u7d50\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4f86\u6539\u5584\u5de5\u5177\u9078\u64c7\u3002\u6211\u5011\u63d0\u51fa\u4e86\u900f\u904e\u5143\u63a8\u7406\u9032\u884c\u5de5\u5177\u9078\u64c7 (TECTON)\uff0c\u9019\u662f\u4e00\u500b\u5206\u70ba\u5169\u500b\u968e\u6bb5\u7684\u7cfb\u7d71\uff0c\u5b83\u6703\u5148\u4f7f\u7528\u81ea\u8a02\u5fae\u8abf\u7684 LM \u982d\u91dd\u5c0d\u4efb\u52d9\u9032\u884c\u63a8\u7406\uff0c\u4e26\u8f38\u51fa\u5019\u9078\u5de5\u5177\u3002\u7136\u5f8c\uff0c\u5728\u505c\u7528\u81ea\u8a02\u982d\u7684\u60c5\u6cc1\u4e0b\uff0c\u5b83\u6703\u9032\u884c\u5143\u63a8\u7406\uff08\u5373\u5c0d\u5148\u524d\u7684\u63a8\u7406\u904e\u7a0b\u9032\u884c\u63a8\u7406\uff09\u4ee5\u505a\u51fa\u6700\u7d42\u9078\u64c7\u3002\u6211\u5011\u8868\u660e TECTON \u5728\u4e00\u7cfb\u5217\u6578\u5b78\u63a8\u7406\u8cc7\u6599\u96c6\u4e0a\u7522\u751f\u4e86\u5be6\u8cea\u7684\u6536\u76ca\uff0c\u7121\u8ad6\u662f\u5728\u5206\u4f48\u5167\u9084\u662f\u5206\u4f48\u5916\u3002", "author": "Lisa Alazraki et.al.", "authors": "Lisa Alazraki, Marek Rei", "id": "2411.04535v1", "paper_url": "http://arxiv.org/abs/2411.04535v1", "repo": "https://github.com/lisaalaz/tecton"}}