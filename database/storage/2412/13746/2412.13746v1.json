{"2412.13746": {"publish_time": "2024-12-18", "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment", "paper_summary": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.", "paper_summary_zh": "\u5118\u7ba1\u73fe\u6709\u7684\u6aa2\u7d22\u589e\u5f37\u8a9e\u8a00\u6a21\u578b (RALM) \u5728\u63d0\u4f9b\u53ef\u4fe1\u8cf4\u7684\u56de\u61c9\u548c\u4f9d\u64da\u53ef\u4fe1\u8cf4\u7684\u4f86\u6e90\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5f80\u5f80\u5ffd\u7565\u8207\u4eba\u985e\u504f\u597d\u9032\u884c\u6709\u6548\u7684\u5c0d\u9f4a\u3002\u5728\u5c0d\u9f4a\u904e\u7a0b\u4e2d\uff0c\u734e\u52f5\u6a21\u578b (RM) \u4f5c\u70ba\u4eba\u985e\u50f9\u503c\u89c0\u7684\u95dc\u9375\u4ee3\u7406\uff0c\u7528\u65bc\u5f15\u5c0e\u6700\u4f73\u5316\u3002\u7136\u800c\uff0c\u5982\u4f55\u8a55\u4f30\u548c\u9078\u64c7\u4e00\u500b\u53ef\u9760\u7684 RM \u4ee5\u7528\u65bc RALM \u4e2d\u7684\u504f\u597d\u5c0d\u9f4a\uff0c\u76ee\u524d\u4ecd\u4e0d\u6e05\u695a\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa RAG-RewardBench\uff0c\u9019\u662f\u8a55\u4f30 RAG \u8a2d\u5b9a\u4e2d RM \u7684\u7b2c\u4e00\u500b\u57fa\u6e96\u3002\u9996\u5148\uff0c\u6211\u5011\u8a2d\u8a08\u56db\u500b\u95dc\u9375\u4e14\u5177\u6709\u6311\u6230\u6027\u7684 RAG \u7279\u5b9a\u5834\u666f\u4f86\u8a55\u4f30 RM\uff0c\u5305\u62ec\u591a\u8df3\u63a8\u7406\u3001\u7d30\u7c92\u5ea6\u5f15\u6587\u3001\u9069\u7576\u7684\u68c4\u6b0a\u548c\u885d\u7a81\u7a69\u5065\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u7d0d\u5165 18 \u500b RAG \u5b50\u96c6\u3001\u516d\u500b\u6aa2\u7d22\u5668\u548c 24 \u500b RALM\uff0c\u4ee5\u589e\u52a0\u6578\u64da\u4f86\u6e90\u7684\u591a\u6a23\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a1\u7528 LLM \u4f5c\u70ba\u8a55\u5224\u8005\u65b9\u6cd5\u4f86\u63d0\u9ad8\u504f\u597d\u6a19\u8a3b\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u4e26\u8868\u73fe\u51fa\u8207\u4eba\u985e\u6a19\u8a3b\u7684\u5f37\u76f8\u95dc\u6027\u3002\u6839\u64da RAG-RewardBench\uff0c\u6211\u5011\u5c0d 45 \u500b RM \u9032\u884c\u4e86\u5168\u9762\u7684\u8a55\u4f30\uff0c\u4e26\u63ed\u793a\u4e86\u5b83\u5011\u5728 RAG \u5834\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u63ed\u793a\u73fe\u6709\u7684\u5df2\u8a13\u7df4 RALM \u5728\u504f\u597d\u5c0d\u9f4a\u65b9\u9762\u5e7e\u4e4e\u6c92\u6709\u6539\u9032\uff0c\u9019\u51f8\u986f\u4e86\u9700\u8981\u8f49\u5411\u504f\u597d\u5c0d\u9f4a\u8a13\u7df4\u3002\u6211\u5011\u5728 https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ \u4e0a\u516c\u958b\u767c\u5e03\u6211\u5011\u7684\u57fa\u6e96\u548c\u4ee3\u78bc\uff0c\u4f9b\u5c07\u4f86\u7684\u5de5\u4f5c\u4f7f\u7528\u3002", "author": "Zhuoran Jin et.al.", "authors": "Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao", "id": "2412.13746v1", "paper_url": "http://arxiv.org/abs/2412.13746v1", "repo": "https://github.com/jinzhuoran/rag-rewardbench"}}