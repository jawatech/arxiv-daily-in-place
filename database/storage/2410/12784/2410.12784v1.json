{"2410.12784": {"publish_time": "2024-10-16", "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges", "paper_summary": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .", "paper_summary_zh": "<paragraph>\u57fa\u65bc LLM \u7684\u8a55\u5be9\u5df2\u6210\u70ba\u4eba\u985e\u8a55\u4f30\u7684\u53ef\u64f4\u5145\u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u6108\u4f86\u6108\u5e38\u88ab\u7528\u65bc\u8a55\u4f30\u3001\u6bd4\u8f03\u548c\u6539\u5584\u6a21\u578b\u3002\u7136\u800c\uff0c\u57fa\u65bc LLM \u7684\u8a55\u5be9\u672c\u8eab\u7684\u53ef\u9760\u6027\u537b\u9bae\u5c11\u53d7\u5230\u5be9\u67e5\u3002\u96a8\u8457 LLM \u8b8a\u5f97\u66f4\u5148\u9032\uff0c\u5176\u56de\u61c9\u4e5f\u8b8a\u5f97\u66f4\u8907\u96dc\uff0c\u9700\u8981\u66f4\u5f37\u5927\u7684\u8a55\u5be9\u4f86\u8a55\u4f30\u5b83\u5011\u3002\u73fe\u6709\u7684\u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u8a55\u5be9\u8207\u4eba\u985e\u504f\u597d\u7684\u543b\u5408\u5ea6\uff0c\u4f46\u901a\u5e38\u7121\u6cd5\u8003\u91cf\u66f4\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u5176\u4e2d\u7fa4\u773e\u5916\u5305\u7684\u4eba\u985e\u504f\u597d\u662f\u4e8b\u5be6\u548c\u908f\u8f2f\u6b63\u78ba\u6027\u7684\u4e0d\u826f\u6307\u6a19\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u8a55\u4f30\u67b6\u69cb\uff0c\u4ee5\u5ba2\u89c0\u8a55\u4f30\u57fa\u65bc LLM \u7684\u8a55\u5be9\u3002\u6839\u64da\u9019\u500b\u67b6\u69cb\uff0c\u6211\u5011\u63d0\u51fa JudgeBench\uff0c\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u57fa\u65bc LLM \u7684\u8a55\u5be9\u5728\u8de8\u8d8a\u77e5\u8b58\u3001\u63a8\u7406\u3001\u6578\u5b78\u548c\u7de8\u78bc\u7684\u5177\u6311\u6230\u6027\u56de\u61c9\u914d\u5c0d\u4e0a\u7684\u57fa\u6e96\u3002JudgeBench \u85c9\u7531\u4e00\u500b\u65b0\u7a4e\u7684\u7ba1\u9053\uff0c\u5c07\u73fe\u6709\u7684\u56f0\u96e3\u8cc7\u6599\u96c6\u8f49\u63db\u6210\u5177\u6311\u6230\u6027\u7684\u56de\u61c9\u914d\u5c0d\uff0c\u5176\u4e2d\u504f\u597d\u6a19\u7c64\u53cd\u6620\u51fa\u5ba2\u89c0\u6b63\u78ba\u6027\u3002\u6211\u5011\u5c0d\u4e00\u7cfb\u5217\u63d0\u793a\u5f0f\u8a55\u5be9\u3001\u5fae\u8abf\u8a55\u5be9\u3001\u591a\u91cd\u4ee3\u7406\u8a55\u5be9\u548c\u734e\u52f5\u6a21\u578b\u9032\u884c\u7684\u5168\u9762\u8a55\u4f30\u986f\u793a\uff0cJudgeBench \u69cb\u6210\u7684\u6311\u6230\u986f\u8457\u5927\u65bc\u5148\u524d\u7684\u57fa\u6e96\uff0c\u8a31\u591a\u5f37\u5927\u7684\u6a21\u578b\uff08\u4f8b\u5982 GPT-4o\uff09\u7684\u8868\u73fe\u50c5\u6bd4\u96a8\u6a5f\u731c\u6e2c\u597d\u4e00\u9ede\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cJudgeBench \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u9760\u7684\u5e73\u53f0\uff0c\u7528\u65bc\u8a55\u4f30\u6108\u4f86\u6108\u5148\u9032\u7684\u57fa\u65bc LLM \u7684\u8a55\u5be9\u3002\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/ScalerLab/JudgeBench \u53d6\u5f97\u3002</paragraph>", "author": "Sijun Tan et.al.", "authors": "Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, Ion Stoica", "id": "2410.12784v1", "paper_url": "http://arxiv.org/abs/2410.12784v1", "repo": "null"}}