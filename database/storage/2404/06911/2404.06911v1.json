{"2404.06911": {"publish_time": "2024-04-10", "title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism", "paper_summary": "Pretrained Language Models (PLMs) benefit from external knowledge stored in\ngraph structures for various downstream tasks. However, bridging the modality\ngap between graph structures and text remains a significant challenge.\nTraditional methods like linearizing graphs for PLMs lose vital graph\nconnectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes\nfor integration into PLMs. In this work, we propose a novel graph-guided\nself-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level\nstructural information into PLMs without necessitating additional alignment or\nconcatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME\nfollows a multi-task learning strategy and effectively bridges the gap between\ngraph and textual modalities, facilitating dynamic interactions between GNNs\nand PLMs. Our experiments on the graph-to-text generation task demonstrate that\nGraSAME outperforms baseline models and achieves results comparable to\nstate-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to\nSOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust\ngraph inputs and reduces the number of trainable parameters by over 100\nmillion.", "paper_summary_zh": "\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u53d7\u76ca\u65bc\u5132\u5b58\u5728\u5716\u5f62\u7d50\u69cb\u4e2d\u7684\u5916\u90e8\u77e5\u8b58\uff0c\u4ee5\u9032\u884c\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5f4c\u5408\u5716\u5f62\u7d50\u69cb\u8207\u6587\u5b57\u4e4b\u9593\u7684\u6a21\u614b\u5dee\u8ddd\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u7dda\u6027\u5316\u5716\u5f62\u4f9b PLM \u4f7f\u7528\u7b49\u50b3\u7d71\u65b9\u6cd5\u6703\u5931\u53bb\u91cd\u8981\u7684\u5716\u5f62\u9023\u901a\u6027\uff0c\u800c\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u9700\u8981\u7e41\u7463\u7684\u7a0b\u5e8f\u624d\u80fd\u6574\u5408\u5230 PLM \u4e2d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5716\u5f62\u5c0e\u5411\u81ea\u6ce8\u610f\u529b\u6a5f\u5236 GraSAME\u3002GraSAME \u5c07\u4ee4\u724c\u7d1a\u5225\u7684\u7d50\u69cb\u8cc7\u8a0a\u7121\u7e2b\u6574\u5408\u5230 PLM \u4e2d\uff0c\u800c\u7121\u9700\u984d\u5916\u7684\u5c0d\u9f4a\u6216\u4e32\u806f\u5de5\u4f5c\u3002\u4f5c\u70ba\u4e00\u500b\u7aef\u5230\u7aef\u7684\u8f15\u91cf\u7d1a\u591a\u6a21\u7d44\uff0cGraSAME \u9075\u5faa\u591a\u4efb\u52d9\u5b78\u7fd2\u7b56\u7565\uff0c\u6709\u6548\u5730\u5f4c\u5408\u4e86\u5716\u5f62\u548c\u6587\u672c\u6a21\u614b\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u4fc3\u9032\u4e86 GNN \u548c PLM \u4e4b\u9593\u7684\u52d5\u614b\u4e92\u52d5\u3002\u6211\u5011\u5728\u5716\u5f62\u5230\u6587\u5b57\u751f\u6210\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cGraSAME \u512a\u65bc\u57fa\u6e96\u6a21\u578b\uff0c\u4e26\u5728 WebNLG \u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86\u8207\u6700\u5148\u9032 (SOTA) \u6a21\u578b\u76f8\u7576\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u8207 SOTA \u6a21\u578b\u76f8\u6bd4\uff0cGraSAME \u6d88\u9664\u4e86\u8abf\u6574\u5716\u5f62\u8f38\u5165\u6240\u9700\u7684\u984d\u5916\u9810\u8a13\u7df4\u4efb\u52d9\uff0c\u4e26\u5c07\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u6578\u91cf\u6e1b\u5c11\u4e86 1 \u5104\u4ee5\u4e0a\u3002", "author": "Shuzhou Yuan et.al.", "authors": "Shuzhou Yuan, Michael F\u00e4rber", "id": "2404.06911v1", "paper_url": "http://arxiv.org/abs/2404.06911v1", "repo": "null"}}