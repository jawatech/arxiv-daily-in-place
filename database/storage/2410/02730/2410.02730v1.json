{"2410.02730": {"publish_time": "2024-10-03", "title": "DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects", "paper_summary": "Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent.", "paper_summary_zh": "\u5728\u672a\u77e5\u74b0\u5883\u4e2d\u9032\u884c\u7269\u9ad4\u5c0e\u822a\u5c0d\u65bc\u5728\u771f\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\u90e8\u7f72\u5177\u8eab\u4ee3\u7406\u81f3\u95dc\u91cd\u8981\u3002\u5118\u7ba1\u6211\u5011\u898b\u8b49\u4e86\u7531\u65bc\u5927\u898f\u6a21\u5834\u666f\u6578\u64da\u96c6\u3001\u66f4\u5feb\u7684\u6a21\u64ec\u5668\u548c\u66f4\u5f37\u5927\u7684\u6a21\u578b\u800c\u53d6\u5f97\u7684\u5de8\u5927\u9032\u5c55\uff0c\u4f46\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6709\u9650\u7684\u5834\u666f\u985e\u578b\u548c\u76ee\u6a19\u7269\u9ad4\u4e0a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4e00\u9805\u65b0\u4efb\u52d9\uff0c\u5373\u5728\u5927\u91cf\u5834\u666f\u985e\u578b\u4e2d\u5c0e\u822a\u5230\u4e0d\u540c\u7684\u76ee\u6a19\u7269\u9ad4\u3002\u70ba\u4e86\u5c0d\u554f\u984c\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5927\u578b\u5834\u666f\u6578\u64da\u96c6 DivScene\uff0c\u5176\u4e2d\u5305\u542b 81 \u7a2e\u4e0d\u540c\u985e\u578b\u7684 4,614 \u500b\u5834\u666f\u3002\u5229\u7528\u8a72\u6578\u64da\u96c6\uff0c\u6211\u5011\u901a\u904e\u6a21\u4eff\u5b78\u7fd2\u5fae\u8abf\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLM) \u4f86\u69cb\u5efa\u7aef\u5230\u7aef\u7684\u5177\u8eab\u4ee3\u7406 NatVLM\u3002LVLM \u7d93\u904e\u8a13\u7df4\uff0c\u53ef\u4ee5\u5f9e\u74b0\u5883\u4e2d\u7372\u53d6\u5148\u524d\u7684\u89c0\u5bdf\u7d50\u679c\u4e26\u751f\u6210\u5f8c\u7e8c\u52d5\u4f5c\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u52d5\u4f5c\u9810\u6e2c\u7684 CoT \u89e3\u91cb\u8ecc\u8de1\uff0c\u4ee5\u4fbf\u5728\u8abf\u6574 LVLMs \u6642\u7372\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u9a57\u767c\u73fe\uff0c\u6211\u5011\u53ef\u4ee5\u5728\u6c92\u6709\u4efb\u4f55\u4eba\u5de5\u76e3\u7763\u7684\u60c5\u6cc1\u4e0b\uff0c\u901a\u904e\u5c0d BFS \u898f\u5283\u5668\u69cb\u9020\u7684\u6700\u77ed\u8def\u5f91\u9032\u884c\u6a21\u4eff\u5b78\u7fd2\uff0c\u69cb\u5efa\u4e00\u500b\u57fa\u65bc LVLM \u7684\u9ad8\u6027\u80fd\u4ee3\u7406\u3002\u6211\u5011\u7684\u4ee3\u7406\u5be6\u73fe\u7684\u6210\u529f\u7387\u6bd4 GPT-4o \u9ad8\u51fa 20% \u4ee5\u4e0a\u3002\u540c\u6642\uff0c\u6211\u5011\u9032\u884c\u4e86\u5404\u7a2e\u5206\u6790\uff0c\u5c55\u793a\u4e86\u6211\u5011\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u3002", "author": "Zhaowei Wang et.al.", "authors": "Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu", "id": "2410.02730v1", "paper_url": "http://arxiv.org/abs/2410.02730v1", "repo": "null"}}