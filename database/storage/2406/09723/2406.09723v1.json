{"2406.09723": {"publish_time": "2024-06-14", "title": "When Will Gradient Regularization Be Harmful?", "paper_summary": "Gradient regularization (GR), which aims to penalize the gradient norm atop\nthe loss function, has shown promising results in training modern\nover-parameterized deep neural networks. However, can we trust this powerful\ntechnique? This paper reveals that GR can cause performance degeneration in\nadaptive optimization scenarios, particularly with learning rate warmup. Our\nempirical and theoretical analyses suggest this is due to GR inducing\ninstability and divergence in gradient statistics of adaptive optimizers at the\ninitial training stage. Inspired by the warmup heuristic, we propose three GR\nwarmup strategies, each relaxing the regularization effect to a certain extent\nduring the warmup course to ensure the accurate and stable accumulation of\ngradients. With experiments on Vision Transformer family, we confirm the three\nGR warmup strategies can effectively circumvent these issues, thereby largely\nimproving the model performance. Meanwhile, we note that scalable models tend\nto rely more on the GR warmup, where the performance can be improved by up to\n3\\% on Cifar10 compared to baseline GR. Code is available at\n\\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.", "paper_summary_zh": "\u68af\u5ea6\u6b63\u5247\u5316 (GR) \u65e8\u5728\u61f2\u7f70\u640d\u5931\u51fd\u6578\u4e0a\u7684\u68af\u5ea6\u7bc4\u6578\uff0c\u5728\u8a13\u7df4\u73fe\u4ee3\u904e\u5ea6\u53c3\u6578\u5316\u7684\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u65b9\u9762\u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u6211\u5011\u53ef\u4ee5\u76f8\u4fe1\u9019\u7a2e\u5f37\u5927\u7684\u6280\u8853\u55ce\uff1f\u672c\u6587\u63ed\u793a\u4e86 GR \u6703\u5728\u9069\u61c9\u6027\u6700\u4f73\u5316\u60c5\u5883\u4e2d\u5c0e\u81f4\u6548\u80fd\u9000\u5316\uff0c\u7279\u5225\u662f\u5728\u5b78\u7fd2\u7387\u71b1\u8eab\u7684\u60c5\u6cc1\u4e0b\u3002\u6211\u5011\u7684\u7d93\u9a57\u548c\u7406\u8ad6\u5206\u6790\u8868\u660e\uff0c\u9019\u662f\u56e0\u70ba GR \u5728\u9069\u61c9\u6027\u6700\u4f73\u5316\u5668\u7684\u521d\u59cb\u8a13\u7df4\u968e\u6bb5\u4e2d\u6703\u5c0e\u81f4\u68af\u5ea6\u7d71\u8a08\u7684\u4e0d\u7a69\u5b9a\u548c\u767c\u6563\u3002\u53d7\u5230\u71b1\u8eab\u555f\u767c\u6cd5\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u7a2e GR \u71b1\u8eab\u7b56\u7565\uff0c\u6bcf\u7a2e\u7b56\u7565\u90fd\u5728\u71b1\u8eab\u904e\u7a0b\u4e2d\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u653e\u5bec\u6b63\u5247\u5316\u6548\u679c\uff0c\u4ee5\u78ba\u4fdd\u68af\u5ea6\u6e96\u78ba\u4e14\u7a69\u5b9a\u5730\u7d2f\u7a4d\u3002\u900f\u904e\u5728 Vision Transformer \u5bb6\u65cf\u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u6211\u5011\u78ba\u8a8d\u9019\u4e09\u7a2e GR \u71b1\u8eab\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u898f\u907f\u9019\u4e9b\u554f\u984c\uff0c\u5f9e\u800c\u5927\u5e45\u63d0\u5347\u6a21\u578b\u6548\u80fd\u3002\u540c\u6642\uff0c\u6211\u5011\u6ce8\u610f\u5230\u53ef\u64f4\u5145\u6a21\u578b\u5f80\u5f80\u66f4\u4f9d\u8cf4 GR \u71b1\u8eab\uff0c\u5176\u4e2d\u5728 Cifar10 \u4e0a\u7684\u6548\u80fd\u53ef\u4ee5\u6bd4\u57fa\u7dda GR \u63d0\u5347\u591a\u9054 3%\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc\\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}\u53d6\u5f97\u3002", "author": "Yang Zhao et.al.", "authors": "Yang Zhao, Hao Zhang, Xiuyuan Hu", "id": "2406.09723v1", "paper_url": "http://arxiv.org/abs/2406.09723v1", "repo": "https://github.com/zhaoyang-0204/gnp"}}