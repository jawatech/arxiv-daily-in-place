{"2406.07012": {"publish_time": "2024-06-11", "title": "Bridging Language Gaps in Audio-Text Retrieval", "paper_summary": "Audio-text retrieval is a challenging task, requiring the search for an audio\nclip or a text caption within a database. The predominant focus of existing\nresearch on English descriptions poses a limitation on the applicability of\nsuch models, given the abundance of non-English content in real-world data. To\naddress these linguistic disparities, we propose a language enhancement (LE),\nusing a multilingual text encoder (SONAR) to encode the text data with\nlanguage-specific information. Additionally, we optimize the audio encoder\nthrough the application of consistent ensemble distillation (CED), enhancing\nsupport for variable-length audio-text retrieval. Our methodology excels in\nEnglish audio-text retrieval, demonstrating state-of-the-art (SOTA) performance\non commonly used datasets such as AudioCaps and Clotho. Simultaneously, the\napproach exhibits proficiency in retrieving content in seven other languages\nwith only 10% of additional language-enhanced training data, yielding promising\nresults. The source code is publicly available\nhttps://github.com/zyyan4/ml-clap.", "paper_summary_zh": "\u97f3\u8a0a\u6587\u5b57\u6aa2\u7d22\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u9700\u8981\u5728\u8cc7\u6599\u5eab\u4e2d\u641c\u5c0b\u97f3\u8a0a\u7247\u6bb5\u6216\u6587\u5b57\u6a19\u984c\u3002\u73fe\u6709\u82f1\u6587\u8aaa\u660e\u7684\u7814\u7a76\u91cd\u9ede\uff0c\u9650\u5236\u4e86\u6b64\u985e\u6a21\u578b\u7684\u9069\u7528\u6027\uff0c\u56e0\u70ba\u73fe\u5be6\u4e16\u754c\u8cc7\u6599\u4e2d\u5b58\u5728\u5927\u91cf\u975e\u82f1\u6587\u5167\u5bb9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u8a9e\u8a00\u5dee\u7570\uff0c\u6211\u5011\u63d0\u51fa\u8a9e\u8a00\u589e\u5f37 (LE)\uff0c\u4f7f\u7528\u591a\u8a9e\u8a00\u6587\u5b57\u7de8\u78bc\u5668 (SONAR) \u7de8\u78bc\u5177\u6709\u8a9e\u8a00\u7279\u5b9a\u8cc7\u8a0a\u7684\u6587\u5b57\u8cc7\u6599\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u61c9\u7528\u4e00\u81f4\u7684\u6574\u9ad4\u84b8\u993e (CED) \u4f86\u6700\u4f73\u5316\u97f3\u8a0a\u7de8\u78bc\u5668\uff0c\u52a0\u5f37\u5c0d\u53ef\u8b8a\u9577\u5ea6\u97f3\u8a0a\u6587\u5b57\u6aa2\u7d22\u7684\u652f\u63f4\u3002\u6211\u5011\u7684\u6280\u8853\u5728\u82f1\u6587\u97f3\u8a0a\u6587\u5b57\u6aa2\u7d22\u4e2d\u8868\u73fe\u512a\u7570\uff0c\u5728 AudioCaps \u548c Clotho \u7b49\u5e38\u7528\u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u6700\u5148\u9032 (SOTA) \u7684\u6548\u80fd\u3002\u540c\u6642\uff0c\u6b64\u65b9\u6cd5\u50c5\u4f7f\u7528 10% \u7684\u984d\u5916\u8a9e\u8a00\u589e\u5f37\u8a13\u7df4\u8cc7\u6599\uff0c\u5c31\u80fd\u719f\u7df4\u5730\u6aa2\u7d22\u4e03\u7a2e\u5176\u4ed6\u8a9e\u8a00\u7684\u5167\u5bb9\uff0c\u7522\u751f\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u539f\u59cb\u7a0b\u5f0f\u78bc\u516c\u958b\u65bc https://github.com/zyyan4/ml-clap\u3002", "author": "Zhiyong Yan et.al.", "authors": "Zhiyong Yan, Heinrich Dinkel, Yongqing Wang, Jizhong Liu, Junbo Zhang, Yujun Wang, Bin Wang", "id": "2406.07012v1", "paper_url": "http://arxiv.org/abs/2406.07012v1", "repo": "null"}}