{"2412.08117": {"publish_time": "2024-12-11", "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation", "paper_summary": "Diffusion-based Generative AI gains significant attention for its superior\nperformance over other generative techniques like Generative Adversarial\nNetworks and Variational Autoencoders. While it has achieved notable\nadvancements in fields such as computer vision and natural language processing,\ntheir application in speech generation remains under-explored. Mainstream\nText-to-Speech systems primarily map outputs to Mel-Spectrograms in the\nspectral space, leading to high computational loads due to the sparsity of\nMelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS\ngeneration approach utilizing latent diffusion models. By using latent\nembeddings as the intermediate representation, LatentSpeech reduces the target\ndimension to 5% of what is required for MelSpecs, simplifying the processing\nfor the TTS encoder and vocoder and enabling efficient high-quality speech\ngeneration. This study marks the first integration of latent diffusion models\nin TTS, enhancing the accuracy and naturalness of generated speech.\nExperimental results on benchmark datasets demonstrate that LatentSpeech\nachieves a 25% improvement in Word Error Rate and a 24% improvement in Mel\nCepstral Distortion compared to existing models, with further improvements\nrising to 49.5% and 26%, respectively, with additional training data. These\nfindings highlight the potential of LatentSpeech to advance the\nstate-of-the-art in TTS technology", "paper_summary_zh": "\u57fa\u65bc\u64f4\u6563\u7684\u751f\u6210\u5f0f AI \u56e0\u5176\u512a\u65bc\u5176\u4ed6\u751f\u6210\u5f0f\u6280\u8853\uff08\u4f8b\u5982\u751f\u6210\u5c0d\u6297\u7db2\u8def\u548c\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668\uff09\u7684\u5353\u8d8a\u6548\u80fd\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u96d6\u7136\u5b83\u5728\u96fb\u8166\u8996\u89ba\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7b49\u9818\u57df\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u5176\u5728\u8a9e\u97f3\u751f\u6210\u7684\u61c9\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3b\u6d41\u7684\u6587\u5b57\u8f49\u8a9e\u97f3\u7cfb\u7d71\u4e3b\u8981\u5c07\u8f38\u51fa\u6620\u5c04\u5230\u983b\u8b5c\u7a7a\u9593\u4e2d\u7684\u6885\u723e\u983b\u8b5c\u5716\uff0c\u7531\u65bc\u6885\u723e\u983b\u8b5c\u5716\u7684\u7a00\u758f\u6027\uff0c\u5c0e\u81f4\u9ad8\u904b\u7b97\u8ca0\u8f09\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LatentSpeech\uff0c\u4e00\u7a2e\u5229\u7528\u6f5b\u5728\u64f4\u6563\u6a21\u578b\u7684\u65b0\u578b TTS \u751f\u6210\u65b9\u6cd5\u3002\u901a\u904e\u4f7f\u7528\u6f5b\u5728\u5d4c\u5165\u4f5c\u70ba\u4e2d\u9593\u8868\u793a\uff0cLatentSpeech \u5c07\u76ee\u6a19\u7dad\u5ea6\u6e1b\u5c11\u5230\u6885\u723e\u983b\u8b5c\u5716\u6240\u9700\u7dad\u5ea6\u7684 5%\uff0c\u7c21\u5316\u4e86 TTS \u7de8\u78bc\u5668\u548c\u8a9e\u97f3\u7de8\u78bc\u5668\u7684\u8655\u7406\uff0c\u4e26\u5be6\u73fe\u4e86\u9ad8\u6548\u7684\u9ad8\u54c1\u8cea\u8a9e\u97f3\u751f\u6210\u3002\u9019\u9805\u7814\u7a76\u6a19\u8a8c\u8457\u6f5b\u5728\u64f4\u6563\u6a21\u578b\u9996\u6b21\u6574\u5408\u5230 TTS \u4e2d\uff0c\u589e\u5f37\u4e86\u751f\u6210\u8a9e\u97f3\u7684\u6e96\u78ba\u6027\u548c\u81ea\u7136\u6027\u3002\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u73fe\u6709\u6a21\u578b\u76f8\u6bd4\uff0cLatentSpeech \u7684\u5b57\u5143\u932f\u8aa4\u7387\u63d0\u9ad8\u4e86 25%\uff0c\u6885\u723e\u5012\u983b\u8b5c\u5931\u771f\u63d0\u9ad8\u4e86 24%\uff0c\u800c\u96a8\u8457\u8a13\u7df4\u8cc7\u6599\u7684\u589e\u52a0\uff0c\u9032\u4e00\u6b65\u7684\u6539\u5584\u5206\u5225\u9054\u5230 49.5% \u548c 26%\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86 LatentSpeech \u63a8\u52d5 TTS \u6280\u8853\u9032\u6b65\u7684\u6f5b\u529b\u3002", "author": "Haowei Lou et.al.", "authors": "Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao", "id": "2412.08117v1", "paper_url": "http://arxiv.org/abs/2412.08117v1", "repo": "null"}}