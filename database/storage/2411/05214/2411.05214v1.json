{"2411.05214": {"publish_time": "2024-11-07", "title": "STAND-Guard: A Small Task-Adaptive Content Moderation Model", "paper_summary": "Content moderation, the process of reviewing and monitoring the safety of\ngenerated content, is important for development of welcoming online platforms\nand responsible large language models. Content moderation contains various\ntasks, each with its unique requirements tailored to specific scenarios.\nTherefore, it is crucial to develop a model that can be easily adapted to novel\nor customized content moderation tasks accurately without extensive model\ntuning. This paper presents STAND-GUARD, a Small Task-Adaptive coNtent\nmoDeration model. The basic motivation is: by performing instruct tuning on\nvarious content moderation tasks, we can unleash the power of small language\nmodels (SLMs) on unseen (out-of-distribution) content moderation tasks. We also\ncarefully study the effects of training tasks and model size on the efficacy of\ncross-task fine-tuning mechanism. Experiments demonstrate STAND-Guard is\ncomparable to GPT-3.5-Turbo across over 40 public datasets, as well as\nproprietary datasets derived from real-world business scenarios. Remarkably,\nSTAND-Guard achieved nearly equivalent results to GPT-4-Turbo on unseen English\nbinary classification tasks", "paper_summary_zh": "\u5167\u5bb9\u5be9\u6838\uff0c\u6aa2\u95b1\u548c\u76e3\u63a7\u751f\u6210\u5167\u5bb9\u5b89\u5168\u6027\u7684\u904e\u7a0b\uff0c\u5c0d\u65bc\u958b\u767c\u6b61\u8fce\u7684\u7dda\u4e0a\u5e73\u53f0\u548c\u8ca0\u8cac\u4efb\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u81f3\u95dc\u91cd\u8981\u3002\u5167\u5bb9\u5be9\u6838\u5305\u542b\u5404\u7a2e\u4efb\u52d9\uff0c\u6bcf\u500b\u4efb\u52d9\u90fd\u6709\u5176\u7368\u7279\u7684\u8981\u6c42\uff0c\u6839\u64da\u7279\u5b9a\u5834\u666f\u91cf\u8eab\u5b9a\u5236\u3002\u56e0\u6b64\uff0c\u958b\u767c\u4e00\u500b\u6a21\u578b\u81f3\u95dc\u91cd\u8981\uff0c\u8a72\u6a21\u578b\u53ef\u4ee5\u8f15\u9b06\u9069\u61c9\u65b0\u7a4e\u6216\u81ea\u8a02\u7684\u5167\u5bb9\u5be9\u6838\u4efb\u52d9\uff0c\u800c\u7121\u9700\u5ee3\u6cdb\u7684\u6a21\u578b\u8abf\u6574\u3002\u672c\u6587\u4ecb\u7d39 STAND-GUARD\uff0c\u4e00\u500b\u5c0f\u578b\u4efb\u52d9\u9069\u61c9\u6027\u5167\u5bb9\u5be9\u6838\u6a21\u578b\u3002\u57fa\u672c\u52d5\u6a5f\u662f\uff1a\u901a\u904e\u5c0d\u5404\u7a2e\u5167\u5bb9\u5be9\u6838\u4efb\u52d9\u57f7\u884c\u6307\u4ee4\u8abf\u6574\uff0c\u6211\u5011\u53ef\u4ee5\u91cb\u653e\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u5728\u672a\u898b\uff08\u5206\u4f48\u5916\uff09\u5167\u5bb9\u5be9\u6838\u4efb\u52d9\u4e0a\u7684\u80fd\u529b\u3002\u6211\u5011\u9084\u4ed4\u7d30\u7814\u7a76\u4e86\u8a13\u7df4\u4efb\u52d9\u548c\u6a21\u578b\u5927\u5c0f\u5c0d\u8de8\u4efb\u52d9\u5fae\u8abf\u6a5f\u5236\u7684\u529f\u6548\u7684\u5f71\u97ff\u3002\u5be6\u9a57\u8868\u660e\uff0cSTAND-Guard \u5728 40 \u591a\u500b\u516c\u5171\u6578\u64da\u96c6\u4ee5\u53ca\u6e90\u81ea\u73fe\u5be6\u4e16\u754c\u696d\u52d9\u5834\u666f\u7684\u5c08\u6709\u6578\u64da\u96c6\u4e0a\u8207 GPT-3.5-Turbo \u76f8\u7576\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSTAND-Guard \u5728\u672a\u898b\u7684\u82f1\u8a9e\u4e8c\u5143\u5206\u985e\u4efb\u52d9\u4e0a\u53d6\u5f97\u4e86\u8207 GPT-4-Turbo \u8fd1\u4e4e\u76f8\u7576\u7684\u7d50\u679c", "author": "Minjia Wang et.al.", "authors": "Minjia Wang, Pingping Lin, Siqi Cai, Shengnan An, Shengjie Ma, Zeqi Lin, Congrui Huang, Bixiong Xu", "id": "2411.05214v1", "paper_url": "http://arxiv.org/abs/2411.05214v1", "repo": "null"}}