{"2502.11946": {"publish_time": "2025-02-17", "title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction", "paper_summary": "Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.", "paper_summary_zh": "<paragraph>\u5373\u6642\u8a9e\u97f3\u4e92\u52d5\u4f5c\u70ba\u4eba\u6a5f\u5354\u4f5c\u7684\u57fa\u672c\u4ecb\u9762\uff0c\u860a\u542b\u8457\u5de8\u5927\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u958b\u6e90\u6a21\u578b\u9762\u81e8\u8457\u8a9e\u97f3\u6578\u64da\u6536\u96c6\u6210\u672c\u9ad8\u3001\u52d5\u614b\u63a7\u5236\u80fd\u529b\u5f31\u3001\u667a\u6167\u6709\u9650\u7b49\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u6587\u4ecb\u7d39\u4e86 Step-Audio\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u53ef\u6295\u5165\u751f\u7522\u7684\u958b\u6e90\u89e3\u6c7a\u65b9\u6848\u3002\u4e3b\u8981\u8ca2\u737b\u5305\u62ec\uff1a1) \u4e00\u500b 130B \u53c3\u6578\u7684\u7d71\u4e00\u8a9e\u97f3\u6587\u5b57\u591a\u6a21\u614b\u6a21\u578b\uff0c\u5be6\u73fe\u4e86\u7d71\u4e00\u7684\u7406\u89e3\u548c\u751f\u6210\uff0c\u5176\u4e2d Step-Audio-Chat \u7248\u672c\u5df2\u958b\u6e90\uff1b2) \u4e00\u500b\u751f\u6210\u5f0f\u8a9e\u97f3\u6578\u64da\u5f15\u64ce\uff0c\u5efa\u7acb\u4e86\u4e00\u500b\u7d93\u6fdf\u5be6\u60e0\u7684\u8a9e\u97f3\u514b\u9686\u6846\u67b6\uff0c\u4e26\u901a\u904e\u84b8\u993e\u6280\u8853\u7522\u751f\u4e86\u958b\u6e90\u7684\u8f15\u91cf\u7d1a Step-Audio-TTS-3B \u6a21\u578b\uff1b3) \u4e00\u500b\u6307\u4ee4\u9a45\u52d5\u7684\u7cbe\u7d30\u63a7\u5236\u7cfb\u7d71\uff0c\u5be6\u73fe\u4e86\u8de8\u65b9\u8a00\u3001\u60c5\u7dd2\u3001\u5531\u6b4c\u548c\u9952\u820c\u7684\u52d5\u614b\u8abf\u6574\uff1b4) \u4e00\u500b\u589e\u5f37\u7684\u8a8d\u77e5\u67b6\u69cb\uff0c\u589e\u52a0\u4e86\u5de5\u5177\u547c\u53eb\u548c\u89d2\u8272\u626e\u6f14\u7684\u80fd\u529b\uff0c\u4ee5\u6709\u6548\u5730\u7ba1\u7406\u8907\u96dc\u7684\u4efb\u52d9\u3002\u6839\u64da\u6211\u5011\u65b0\u7684 StepEval-Audio-360 \u8a55\u4f30\u57fa\u6e96\uff0cStep-Audio \u5728\u4eba\u985e\u8a55\u4f30\u4e2d\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u7279\u5225\u662f\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u3002\u5728 LLaMA Question \u7b49\u958b\u6e90\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u8868\u73fe\u51fa\u5e73\u5747\u63d0\u5347\u4e86 9.3%\uff0c\u8b49\u660e\u4e86\u6211\u5011\u81f4\u529b\u65bc\u63a8\u9032\u958b\u6e90\u591a\u6a21\u614b\u8a9e\u8a00\u6280\u8853\u7684\u767c\u5c55\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u5728 https://github.com/stepfun-ai/Step-Audio \u53d6\u5f97\u3002</paragraph>", "author": "Ailin Huang et.al.", "authors": "Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu", "id": "2502.11946v1", "paper_url": "http://arxiv.org/abs/2502.11946v1", "repo": "null"}}