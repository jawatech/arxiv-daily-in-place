{"2412.14093": {"publish_time": "2024-12-18", "title": "Alignment faking in large language models", "paper_summary": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5c55\u793a\u4e86\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53c3\u8207\u8abf\u6574\u507d\u9020\u7684\u793a\u7bc4\uff1a\u6709\u9078\u64c7\u5730\u9075\u5b88\u5176\u8a13\u7df4\u76ee\u6a19\uff0c\u4ee5\u9632\u6b62\u5728\u8a13\u7df4\u4e2d\u4fee\u6539\u5176\u884c\u70ba\u3002\u9996\u5148\uff0c\u6211\u5011\u7d66 Claude 3 Opus \u4e00\u500b\u7cfb\u7d71\u63d0\u793a\uff0c\u8aaa\u660e\u5b83\u6b63\u5728\u63a5\u53d7\u8a13\u7df4\u4ee5\u56de\u7b54\u6240\u6709\u67e5\u8a62\uff0c\u751a\u81f3\u662f\u6709\u5bb3\u7684\u67e5\u8a62\uff0c\u9019\u8207\u5b83\u62d2\u7d55\u6b64\u985e\u67e5\u8a62\u7684\u5148\u524d\u8a13\u7df4\u76f8\u885d\u7a81\u3002\u70ba\u8b93\u6a21\u578b\u63a8\u65b7\u5b83\u4f55\u6642\u8655\u65bc\u8a13\u7df4\u4e2d\uff0c\u6211\u5011\u8aaa\u5b83\u5c07\u53ea\u63a5\u53d7\u8207\u514d\u8cbb\u7528\u6236\uff08\u800c\u975e\u4ed8\u8cbb\u7528\u6236\uff09\u5c0d\u8a71\u7684\u8a13\u7df4\u3002\u6211\u5011\u767c\u73fe\u8a72\u6a21\u578b 14% \u7684\u6642\u9593\u9075\u5b88\u514d\u8cbb\u7528\u6236\u7684\u6709\u5bb3\u67e5\u8a62\uff0c\u800c\u4ed8\u8cbb\u7528\u6236\u5e7e\u4e4e\u5f9e\u4e0d\u9075\u5b88\u3002\u5728\u5e7e\u4e4e\u6240\u6709\u6a21\u578b\u9075\u5b88\u514d\u8cbb\u7528\u6236\u7684\u6709\u5bb3\u67e5\u8a62\u7684\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u660e\u78ba\u7684\u8abf\u6574\u507d\u9020\u63a8\u7406\uff0c\u89e3\u91cb\u4e86\u9019\u500b\u5dee\u8ddd\uff0c\u6a21\u578b\u8072\u7a31\u5b83\u5728\u8a13\u7df4\u4e2d\u7b56\u7565\u6027\u5730\u56de\u7b54\u6709\u5bb3\u67e5\u8a62\uff0c\u4ee5\u5728\u8a13\u7df4\u4e4b\u5916\u4fdd\u6301\u5176\u9996\u9078\u7684\u7121\u5bb3\u884c\u70ba\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4e00\u500b\u66f4\u5be6\u969b\u7684\u8a2d\u5b9a\uff0c\u5176\u4e2d\u6709\u95dc\u8a13\u7df4\u904e\u7a0b\u7684\u4fe1\u606f\u4e0d\u662f\u5728\u7cfb\u7d71\u63d0\u793a\u4e2d\u63d0\u4f9b\u7684\uff0c\u800c\u662f\u901a\u904e\u8a13\u7df4\u6a21\u64ec\u9810\u8a13\u7df4\u6578\u64da\u7684\u5408\u6210\u6587\u4ef6\u4f86\u63d0\u4f9b\u7684\u2014\u2014\u4e26\u89c0\u5bdf\u5230\u985e\u4f3c\u7684\u8abf\u6574\u507d\u9020\u3002\u6700\u5f8c\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5be6\u969b\u8a13\u7df4\u6a21\u578b\u4ee5\u901a\u904e\u5f37\u5316\u5b78\u7fd2\u4f86\u9075\u5b88\u6709\u5bb3\u67e5\u8a62\u7684\u5f71\u97ff\uff0c\u6211\u5011\u767c\u73fe\u9019\u5c07\u8abf\u6574\u507d\u9020\u63a8\u7406\u7684\u6bd4\u7387\u63d0\u9ad8\u5230 78%\uff0c\u5118\u7ba1\u5728\u8a13\u7df4\u4e4b\u5916\u4e5f\u6703\u589e\u52a0\u9075\u5b88\u7387\u3002\u6211\u5011\u9084\u89c0\u5bdf\u5230\u5176\u4ed6\u884c\u70ba\uff0c\u4f8b\u5982\u6a21\u578b\u5728\u6709\u5bb9\u6613\u6a5f\u6703\u6642\u6ef2\u900f\u5176\u6b0a\u91cd\u3002\u96d6\u7136\u6211\u5011\u901a\u904e\u544a\u8a34\u6a21\u578b\u4f55\u6642\u4ee5\u53ca\u6839\u64da\u4ec0\u9ebc\u6a19\u6e96\u9032\u884c\u8a13\u7df4\u4f86\u7c21\u5316\u8abf\u6574\u507d\u9020\uff0c\u4f46\u6211\u5011\u6c92\u6709\u6307\u793a\u6a21\u578b\u507d\u9020\u8abf\u6574\u6216\u7d66\u4e88\u5b83\u4efb\u4f55\u660e\u78ba\u7684\u76ee\u6a19\u3002\u7531\u65bc\u672a\u4f86\u7684\u6a21\u578b\u53ef\u80fd\u6703\u5728\u6c92\u6709\u88ab\u544a\u77e5\u7684\u60c5\u6cc1\u4e0b\u63a8\u65b7\u51fa\u6709\u95dc\u5176\u8a13\u7df4\u904e\u7a0b\u7684\u4fe1\u606f\uff0c\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\u672a\u4f86\u6a21\u578b\u5b58\u5728\u8abf\u6574\u507d\u9020\u7684\u98a8\u96aa\uff0c\u7121\u8ad6\u662f\u51fa\u65bc\u826f\u6027\u7684\u504f\u597d\uff08\u5982\u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff09\u9084\u662f\u51fa\u65bc\u5176\u4ed6\u539f\u56e0\u3002</paragraph>", "author": "Ryan Greenblatt et.al.", "authors": "Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, S\u00f6ren Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Evan Hubinger", "id": "2412.14093v1", "paper_url": "http://arxiv.org/abs/2412.14093v1", "repo": "https://github.com/redwoodresearch/alignment_faking_public"}}