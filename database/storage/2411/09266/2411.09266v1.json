{"2411.09266": {"publish_time": "2024-11-14", "title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception", "paper_summary": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks.", "paper_summary_zh": "\u591a\u6a21\u614b\u6df1\u5ea6\u507d\u9020\u6d89\u53ca\u8996\u807d\u64cd\u7e31\uff0c\u662f\u4e00\u500b\u65e5\u76ca\u56b4\u91cd\u7684\u5a01\u8105\uff0c\u56e0\u70ba\u7528\u8089\u773c\u6216\u4f7f\u7528\u55ae\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u507d\u9020\u5075\u6e2c\u65b9\u6cd5\u5f88\u96e3\u5075\u6e2c\u5230\u3002\u8996\u807d\u9451\u8b58\u6a21\u578b\u96d6\u7136\u6bd4\u55ae\u6a21\u614b\u6a21\u578b\u529f\u80fd\u66f4\u5f37\u5927\uff0c\u4f46\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u800c\u4e14\u8a13\u7df4\u548c\u63a8\u8ad6\u7684\u8a08\u7b97\u6210\u672c\u5f88\u9ad8\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\uff0c\u800c\u4e14\u901a\u5e38\u7121\u6cd5\u5f88\u597d\u5730\u6982\u62ec\u5230\u672a\u898b\u904e\u7684\u64cd\u7e31\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u6aa2\u9a57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff08\u5373 ChatGPT\uff09\u7684\u5075\u6e2c\u80fd\u529b\uff0c\u4ee5\u8b58\u5225\u548c\u8aaa\u660e\u8996\u807d\u6df1\u5ea6\u507d\u9020\u5167\u5bb9\u4e2d\u7684\u4efb\u4f55\u53ef\u80fd\u7684\u8996\u89ba\u548c\u807d\u89ba\u4eba\u5de5\u88fd\u54c1\u548c\u64cd\u7e31\u3002\u5728\u4e00\u500b\u57fa\u6e96\u591a\u6a21\u614b\u6df1\u5ea6\u507d\u9020\u8cc7\u6599\u96c6\u7684\u5f71\u7247\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u8a55\u4f30 ChatGPT \u7684\u5075\u6e2c\u6548\u80fd\uff0c\u4e26\u5c07\u5176\u8207\u6700\u5148\u9032\u7684\u591a\u6a21\u614b\u9451\u8b58\u6a21\u578b\u548c\u4eba\u985e\u7684\u5075\u6e2c\u80fd\u529b\u9032\u884c\u6bd4\u8f03\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u9818\u57df\u77e5\u8b58\u548c\u63d0\u793a\u5de5\u7a0b\u5c0d\u65bc\u4f7f\u7528 LLM \u9032\u884c\u5f71\u7247\u507d\u9020\u5075\u6e2c\u4efb\u52d9\u7684\u91cd\u8981\u6027\u3002\u8207\u57fa\u65bc\u7aef\u5230\u7aef\u5b78\u7fd2\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cChatGPT \u53ef\u4ee5\u8aaa\u660e\u53ef\u80fd\u5b58\u5728\u65bc\u6a21\u614b\u5167\u6216\u8de8\u6a21\u614b\u7684\u7a7a\u9593\u548c\u6642\u7a7a\u4eba\u5de5\u88fd\u54c1\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86 ChatGPT \u5728\u591a\u5a92\u9ad4\u9451\u8b58\u4efb\u52d9\u4e2d\u7684\u9650\u5236\u3002", "author": "Sahibzada Adil Shahzad et.al.", "authors": "Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang", "id": "2411.09266v1", "paper_url": "http://arxiv.org/abs/2411.09266v1", "repo": "null"}}