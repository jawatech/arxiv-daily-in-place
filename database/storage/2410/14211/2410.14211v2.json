{"2410.14211": {"publish_time": "2024-10-18", "title": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model Reasoning", "paper_summary": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89ba\u554f\u984c\u548c\u7f3a\u4e4f\u76f8\u95dc\u77e5\u8b58\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u8907\u96dc\u63a8\u7406\u548c\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u4e2d\u3002\u77e5\u8b58\u5716\u8b5c (KG) \u4ee5\u7d50\u69cb\u5316\u683c\u5f0f\u64f7\u53d6\u5927\u91cf\u4e8b\u5be6\uff0c\u70ba\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u77e5\u8b58\u4f86\u6e90\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u65bc KG \u7684 LLM \u63a8\u7406\u65b9\u6cd5\u9762\u81e8\u8655\u7406\u591a\u8df3\u63a8\u7406\u3001\u591a\u5be6\u9ad4\u554f\u984c\u548c\u6709\u6548\u5229\u7528\u5716\u7d50\u69cb\u7b49\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5716\u4e0a\u8def\u5f91 (PoG)\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u904e\u6574\u5408\u4f86\u81ea KG \u7684\u77e5\u8b58\u63a8\u7406\u8def\u5f91\u4f86\u589e\u5f37 LLM \u63a8\u7406\uff0c\u63d0\u9ad8 LLM \u8f38\u51fa\u7684\u53ef\u89e3\u91cb\u6027\u548c\u4fdd\u771f\u6027\u3002PoG \u901a\u904e\u4e09\u968e\u6bb5\u52d5\u614b\u591a\u8df3\u8def\u5f91\u63a2\u7d22\u4f86\u89e3\u6c7a\u591a\u8df3\u548c\u591a\u5be6\u9ad4\u554f\u984c\uff0c\u5c07 LLM \u7684\u56fa\u6709\u77e5\u8b58\u8207\u4f86\u81ea KG \u7684\u4e8b\u5be6\u77e5\u8b58\u76f8\u7d50\u5408\u3002\u70ba\u4e86\u63d0\u9ad8\u6548\u7387\uff0cPoG \u9996\u5148\u5f9e\u5716\u63a2\u7d22\u4e2d\u526a\u9664\u7121\u95dc\u4fe1\u606f\uff0c\u4e26\u5f15\u5165\u4e86\u4e09\u6b65\u526a\u679d\u6280\u8853\uff0c\u9019\u4e9b\u6280\u8853\u7d50\u5408\u4e86\u5716\u7d50\u69cb\u3001LLM \u63d0\u793a\u548c\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982\uff0cSBERT\uff09\u4f86\u6709\u6548\u7e2e\u5c0f\u63a2\u7d22\u7684\u5019\u9078\u8def\u5f91\u3002\u9019\u78ba\u4fdd\u4e86\u6240\u6709\u63a8\u7406\u8def\u5f91\u90fd\u5305\u542b\u5f9e KG \u64f7\u53d6\u7684\u9ad8\u5ea6\u76f8\u95dc\u4fe1\u606f\uff0c\u5f9e\u800c\u4f7f\u63a8\u7406\u5728\u554f\u984c\u89e3\u6c7a\u4e2d\u5177\u6709\u4fdd\u771f\u6027\u548c\u53ef\u89e3\u91cb\u6027\u3002PoG \u5275\u65b0\u5730\u5229\u7528\u5716\u7d50\u69cb\u4f86\u526a\u9664\u7121\u95dc\u566a\u8072\uff0c\u4e26\u4ee3\u8868\u4e86\u5728 KG \u4e0a\u5be6\u73fe LLM \u63a8\u7406\u4efb\u52d9\u7684\u591a\u5be6\u9ad4\u6df1\u5ea6\u8def\u5f91\u6aa2\u6e2c\u7684\u7b2c\u4e00\u7a2e\u65b9\u6cd5\u3002\u5728\u4e94\u500b\u57fa\u6e96 KGQA \u6578\u64da\u96c6\u4e0a\u7684\u7d9c\u5408\u5be6\u9a57\u8868\u660e\uff0cPoG \u5728 GPT-3.5-Turbo \u548c GPT-4 \u4e0a\u7684\u8868\u73fe\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5 ToG\uff0c\u5e73\u5747\u6e96\u78ba\u7387\u63d0\u9ad8\u4e86 18.9%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528 GPT-3.5-Turbo \u7684 PoG \u6bd4\u4f7f\u7528 GPT-4 \u7684 ToG \u9ad8\u51fa 23.9%\u3002", "author": "Xingyu Tan et.al.", "authors": "Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang", "id": "2410.14211v2", "paper_url": "http://arxiv.org/abs/2410.14211v2", "repo": "null"}}