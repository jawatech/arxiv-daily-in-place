{"2411.13405": {"publish_time": "2024-11-20", "title": "On the Way to LLM Personalization: Learning to Remember User Conversations", "paper_summary": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u8fc5\u901f\u6210\u70ba\u5404\u7a2e\u4efb\u52d9\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u52a9\u624b\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u6709\u6548\u6027\u53d7\u5230\u5b83\u5011\u901a\u904e\u500b\u4eba\u5316\u8abf\u6574\u56de\u61c9\u4ee5\u7b26\u5408\u4eba\u985e\u504f\u597d\u548c\u884c\u70ba\u7684\u80fd\u529b\u7684\u9650\u5236\u3002LLM \u500b\u4eba\u5316\u7684\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u6a23\u5f0f\u8f49\u79fb\u6216\u7d0d\u5165\u6709\u95dc\u4f7f\u7528\u8005\u7684\u5c11\u91cf\u4e8b\u5be6\uff0c\u56e0\u70ba\u77e5\u8b58\u6ce8\u5165\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u6c7a\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5c07\u5148\u524d\u5c0d\u8a71\u7684\u77e5\u8b58\u6ce8\u5165 LLM\uff0c\u4ee5\u652f\u6301\u672a\u4f86\u5728\u5197\u9918\u8f03\u5c11\u3001\u500b\u6027\u5316\u7684\u5c0d\u8a71\u65b9\u9762\u7684\u5de5\u4f5c\u3002\u6211\u5011\u78ba\u5b9a\u4e86\u5169\u500b\u73fe\u5be6\u4e16\u754c\u7684\u9650\u5236\uff1a(1) \u5c0d\u8a71\u5728\u6642\u9593\u4e0a\u662f\u9023\u7e8c\u7684\uff0c\u4e26\u4e14\u5728\u8a13\u7df4\u671f\u9593\u5fc5\u9808\u9019\u6a23\u8655\u7406\uff0c\u4ee5\u53ca (2) \u6bcf\u7528\u6236\u500b\u4eba\u5316\u50c5\u5728\u53c3\u6578\u6709\u6548\u7387\u7684\u8a2d\u5b9a\u4e2d\u53ef\u884c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 PLUM\uff0c\u9019\u662f\u4e00\u500b\u7ba1\u9053\uff0c\u57f7\u884c\u8cc7\u6599\u64f4\u5145\u4ee5\u5c0d\u8a71\u4f5c\u70ba\u554f\u7b54\u5c0d\u9032\u884c\u4e0a\u63a1\u6a23\uff0c\u7136\u5f8c\u7528\u65bc\u5fae\u8abf\u4f4e\u79e9\u9069\u61c9\u9069\u914d\u5668\uff0c\u4e26\u4f7f\u7528\u52a0\u6b0a\u4ea4\u53c9\u71b5\u640d\u5931\u3002\u5373\u4f7f\u5728\u5c0d\u554f\u984c\u7684\u9996\u6b21\u63a2\u8a0e\u4e2d\uff0c\u6211\u5011\u7684\u8868\u73fe\u4e5f\u80fd\u8207 RAG \u7b49\u57fa\u6e96\u7af6\u722d\uff0c\u5728 100 \u6b21\u5c0d\u8a71\u4e2d\u9054\u5230 81.5% \u7684\u6e96\u78ba\u5ea6\u3002", "author": "Lucie Charlotte Magister et.al.", "authors": "Lucie Charlotte Magister, Katherine Metcalf, Yizhe Zhang, Maartje ter Hoeve", "id": "2411.13405v1", "paper_url": "http://arxiv.org/abs/2411.13405v1", "repo": "null"}}