{"2410.15116": {"publish_time": "2024-10-19", "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models", "paper_summary": "Generation of plausible but incorrect factual information, often termed\nhallucination, has attracted significant research interest. Retrieval-augmented\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\nemerges as a promising method to reduce hallucination. However, existing RALMs\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\naddress this challenge, we propose COFT, a novel\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\nknowledge graph to extract potential key entities in a given context. Second,\n\\textit{scorer} measures the importance of each entity by calculating its\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\nentities with a dynamic threshold algorithm and highlights the corresponding\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\nexperiments on the knowledge hallucination benchmark demonstrate the\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\nscore metric. Moreover, COFT also exhibits remarkable versatility across\nvarious long-form tasks, such as reading comprehension and question answering.", "paper_summary_zh": "\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u4e0a\u4e0d\u6b63\u786e\u7684\u5b9e\u9645\u4fe1\u606f\uff08\u901a\u5e38\u79f0\u4e3a\u5e7b\u89c9\uff09\u5f15\u8d77\u4e86\u91cd\u8981\u7684\u7814\u7a76\u5174\u8da3\u3002\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b (RALM) \u901a\u8fc7\u4e3a\u6a21\u578b\u63d0\u4f9b\u6700\u65b0\u7684\u77e5\u8bc6\u6765\u589e\u5f3a\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 RALM \u5728\u68c0\u7d22\u5197\u957f\u7684\u4e0a\u4e0b\u6587\u65f6\u53ef\u80fd\u4f1a\u52a0\u5267\u5e7b\u89c9\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 COFT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\\textbf{\u7c97}\u5230\\textbf{\u7ec6}\u9ad8\u4eae\\textbf{T}ing \u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u7684\u5173\u952e\u6587\u672c\uff0c\u4ece\u800c\u907f\u514d\u5728\u5197\u957f\u7684\u4e0a\u4e0b\u6587\u4e2d\u8ff7\u5931\u3002\u5177\u4f53\u6765\u8bf4\uff0cCOFT \u7531\u4e09\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a\\textit{recaller}\u3001\\textit{scorer} \u548c \\textit{selector}\u3002\u9996\u5148\uff0c\\textit{recaller} \u5e94\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u63d0\u53d6\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u6f5c\u5728\u7684\u5173\u952e\u5b9e\u4f53\u3002\u5176\u6b21\uff0c\\textit{scorer} \u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u5b9e\u4f53\u7684\u4e0a\u4e0b\u6587\u6743\u91cd\u6765\u8861\u91cf\u5176\u91cd\u8981\u6027\u3002\u6700\u540e\uff0c\\textit{selector} \u4f7f\u7528\u52a8\u6001\u9608\u503c\u7b97\u6cd5\u9009\u62e9\u5177\u6709\u9ad8\u4e0a\u4e0b\u6587\u6743\u91cd\u7684\u5b9e\u4f53\uff0c\u5e76\u4ee5\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u7a81\u51fa\u663e\u793a\u76f8\u5e94\u7684\u6bb5\u843d\u3001\u53e5\u5b50\u6216\u5355\u8bcd\u3002\u5728\u77e5\u8bc6\u5e7b\u89c9\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86 COFT \u7684\u6709\u6548\u6027\uff0c\u5728 F1 \u5206\u6570\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc7 30% \u7684\u5353\u8d8a\u6027\u80fd\u3002\u6b64\u5916\uff0cCOFT \u5728\u5404\u79cd\u957f\u7bc7\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u591a\u529f\u80fd\u6027\uff0c\u4f8b\u5982\u9605\u8bfb\u7406\u89e3\u548c\u95ee\u9898\u89e3\u7b54\u3002", "author": "Qitan Lv et.al.", "authors": "Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, Feng Wu", "id": "2410.15116v1", "paper_url": "http://arxiv.org/abs/2410.15116v1", "repo": "null"}}