{"2405.03279": {"publish_time": "2024-05-06", "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning", "paper_summary": "Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed.", "paper_summary_zh": "", "author": "Qizhou Chen et.al.", "authors": "Qizhou Chen,Taolin Zhang,Dongyang Li,Longtao Huang,Hui Xue,Chengyu Wang,Xiaofeng He", "id": "2405.03279v1", "paper_url": "http://arxiv.org/abs/2405.03279v1", "repo": "null"}}