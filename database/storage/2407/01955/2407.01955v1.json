{"2407.01955": {"publish_time": "2024-07-02", "title": "S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models", "paper_summary": "Deployment of autoregressive large language models (LLMs) is costly, and as\nthese models increase in size, the associated costs will become even more\nconsiderable. Consequently, different methods have been proposed to accelerate\nthe token generation process and reduce costs. Speculative decoding (SD) is\namong the most promising approaches to speed up the LLM decoding process by\nverifying multiple tokens in parallel and using an auxiliary smaller draft\nmodel to generate the possible tokens. In SD, usually, one draft model is used\nto serve a specific target model; however, in practice, LLMs are diverse, and\nwe might need to deal with many target models or more than one target model\nsimultaneously. In this scenario, it is not clear which draft model should be\nused for which target model, and searching among different draft models or\ntraining customized draft models can further increase deployment costs. In this\npaper, we first introduce a novel multi-target scenario for the deployment of\ndraft models for faster inference. Then, we present a novel, more efficient\nsorted speculative decoding mechanism that outperforms regular baselines in\nmulti-target settings. We evaluated our method on Spec-Bench in different\nsettings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our\nresults suggest that our draft models perform better than baselines for\nmultiple target models at the same time.", "paper_summary_zh": "\u81ea\u8ff4\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u90e8\u7f72\u6210\u672c\u5f88\u9ad8\uff0c\u96a8\u8457\u9019\u4e9b\u6a21\u578b\u7684\u898f\u6a21\u8d8a\u4f86\u8d8a\u5927\uff0c\u76f8\u95dc\u6210\u672c\u5c07\u8b8a\u5f97\u66f4\u52a0\u53ef\u89c0\u3002\u56e0\u6b64\uff0c\u4eba\u5011\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u65b9\u6cd5\u4f86\u52a0\u901f\u8a18\u865f\u7522\u751f\u904e\u7a0b\u4e26\u964d\u4f4e\u6210\u672c\u3002\u63a8\u6e2c\u6027\u89e3\u78bc (SD) \u662f\u52a0\u901f LLM \u89e3\u78bc\u904e\u7a0b\u6700\u6709\u524d\u9014\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5b83\u901a\u904e\u4e26\u884c\u9a57\u8b49\u591a\u500b\u8a18\u865f\u4e26\u4f7f\u7528\u8f14\u52a9\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u4f86\u751f\u6210\u53ef\u80fd\u7684\u8a18\u865f\u3002\u5728 SD \u4e2d\uff0c\u901a\u5e38\u4f7f\u7528\u4e00\u500b\u8349\u7a3f\u6a21\u578b\u4f86\u670d\u52d9\u4e00\u500b\u7279\u5b9a\u7684\u76ee\u6a19\u6a21\u578b\uff1b\u7136\u800c\uff0c\u5728\u5be6\u52d9\u4e2d\uff0cLLM \u662f\u591a\u6a23\u5316\u7684\uff0c\u6211\u5011\u53ef\u80fd\u9700\u8981\u540c\u6642\u8655\u7406\u591a\u500b\u76ee\u6a19\u6a21\u578b\u6216\u591a\u65bc\u4e00\u500b\u76ee\u6a19\u6a21\u578b\u3002\u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff0c\u5c1a\u4e0d\u6e05\u695a\u61c9\u70ba\u54ea\u500b\u76ee\u6a19\u6a21\u578b\u4f7f\u7528\u54ea\u500b\u8349\u7a3f\u6a21\u578b\uff0c\u4e26\u4e14\u5728\u4e0d\u540c\u7684\u8349\u7a3f\u6a21\u578b\u4e2d\u641c\u5c0b\u6216\u8a13\u7df4\u81ea\u8a02\u8349\u7a3f\u6a21\u578b\u53ef\u80fd\u6703\u9032\u4e00\u6b65\u589e\u52a0\u90e8\u7f72\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u4ecb\u7d39\u4e86\u4e00\u500b\u7528\u65bc\u90e8\u7f72\u8349\u7a3f\u6a21\u578b\u4ee5\u52a0\u5feb\u63a8\u7406\u7684\u591a\u76ee\u6a19\u5834\u666f\u7684\u65b0\u7a4e\u65b9\u6cd5\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u3001\u66f4\u6709\u6548\u7387\u7684\u6392\u5e8f\u63a8\u6e2c\u6027\u89e3\u78bc\u6a5f\u5236\uff0c\u5b83\u5728\u591a\u76ee\u6a19\u8a2d\u5b9a\u4e2d\u512a\u65bc\u5e38\u898f\u57fa\u6e96\u3002\u6211\u5011\u5728\u4e0d\u540c\u7684\u8a2d\u5b9a\u4e2d\u5c0d Spec-Bench \u8a55\u4f30\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u790e\u6a21\u578b\uff0c\u4f8b\u5982 Vicuna 7B\u300113B \u548c LLama Chat 70B\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u8349\u7a3f\u6a21\u578b\u540c\u6642\u91dd\u5c0d\u591a\u500b\u76ee\u6a19\u6a21\u578b\u7684\u8868\u73fe\u512a\u65bc\u57fa\u6e96\u3002", "author": "Parsa Kavehzadeh et.al.", "authors": "Parsa Kavehzadeh, Mohammadreza Pourreza, Mojtaba Valipour, Tinashu Zhu, Haoli Bai, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh", "id": "2407.01955v1", "paper_url": "http://arxiv.org/abs/2407.01955v1", "repo": "null"}}