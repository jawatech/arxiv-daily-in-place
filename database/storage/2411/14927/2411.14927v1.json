{"2411.14927": {"publish_time": "2024-11-22", "title": "LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation", "paper_summary": "Temporal perception, the ability to detect and track objects over time, is\ncritical in autonomous driving for maintaining a comprehensive understanding of\ndynamic environments. However, this task is hindered by significant challenges,\nincluding incomplete perception caused by occluded objects and observational\nblind spots, which are common in single-vehicle perception systems. To address\nthese issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework\nfor Vehicle-Infrastructure Cooperation (VIC). LET-VIC leverages\nVehicle-to-Everything (V2X) communication to enhance temporal perception by\nfusing spatial and temporal data from both vehicle and infrastructure sensors.\nFirst, it spatially integrates Bird's Eye View (BEV) features from vehicle-side\nand infrastructure-side LiDAR data, creating a comprehensive view that\nmitigates occlusions and compensates for blind spots. Second, LET-VIC\nincorporates temporal context across frames, allowing the model to leverage\nhistorical data for enhanced tracking stability and accuracy. To further\nimprove robustness, LET-VIC includes a Calibration Error Compensation (CEC)\nmodule to address sensor misalignments and ensure precise feature alignment.\nExperiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly\noutperforms baseline models, achieving at least a 13.7% improvement in mAP and\na 13.1% improvement in AMOTA without considering communication delays. This\nwork offers a practical solution and a new research direction for advancing\ntemporal perception in autonomous driving through vehicle-infrastructure\ncooperation.", "paper_summary_zh": "\u6642\u9593\u611f\u77e5\uff0c\u5373\u5728\u6642\u9593\u4e2d\u5075\u6e2c\u548c\u8ffd\u8e64\u7269\u4ef6\u7684\u80fd\u529b\uff0c\u5c0d\u65bc\u81ea\u52d5\u99d5\u99db\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u80fd\u7dad\u6301\u5c0d\u52d5\u614b\u74b0\u5883\u7684\u5168\u9762\u7406\u89e3\u3002\u7136\u800c\uff0c\u9019\u9805\u4efb\u52d9\u53d7\u5230\u91cd\u5927\u6311\u6230\u7684\u963b\u7919\uff0c\u5305\u62ec\u7531\u906e\u64cb\u7269\u9ad4\u548c\u89c0\u6e2c\u76f2\u9ede\u9020\u6210\u7684\u611f\u77e5\u4e0d\u5b8c\u6574\uff0c\u9019\u4e9b\u5728\u55ae\u4e00\u8eca\u8f1b\u611f\u77e5\u7cfb\u7d71\u4e2d\u5f88\u5e38\u898b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LET-VIC\uff0c\u4e00\u500b\u57fa\u65bc LiDAR \u7684\u7aef\u5230\u7aef\u8ffd\u8e64\u67b6\u69cb\uff0c\u7528\u65bc\u8eca\u8f1b\u57fa\u790e\u8a2d\u65bd\u5408\u4f5c (VIC)\u3002LET-VIC \u5229\u7528\u8eca\u806f\u842c\u7269 (V2X) \u901a\u8a0a\uff0c\u900f\u904e\u878d\u5408\u8eca\u8f1b\u548c\u57fa\u790e\u8a2d\u65bd\u611f\u6e2c\u5668\u7684\u7a7a\u9593\u548c\u6642\u9593\u8cc7\u6599\u4f86\u589e\u5f37\u6642\u9593\u611f\u77e5\u3002\u9996\u5148\uff0c\u5b83\u5c07\u8eca\u8f1b\u5074\u548c\u57fa\u790e\u8a2d\u65bd\u5074 LiDAR \u8cc7\u6599\u4e2d\u7684\u9ce5\u77b0\u5716 (BEV) \u7279\u5fb5\u9032\u884c\u7a7a\u9593\u6574\u5408\uff0c\u5275\u9020\u4e86\u4e00\u500b\u5168\u9762\u7684\u8996\u5716\uff0c\u53ef\u6e1b\u8f15\u906e\u64cb\u4e26\u88dc\u511f\u76f2\u9ede\u3002\u5176\u6b21\uff0cLET-VIC \u6574\u5408\u4e86\u8de8\u5e40\u7684\u6642\u9593\u80cc\u666f\uff0c\u8b93\u6a21\u578b\u80fd\u5229\u7528\u6b77\u53f2\u8cc7\u6599\u4f86\u589e\u5f37\u8ffd\u8e64\u7a69\u5b9a\u6027\u548c\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u9ad8\u7a69\u5065\u6027\uff0cLET-VIC \u5305\u542b\u4e86\u4e00\u500b\u6821\u6b63\u8aa4\u5dee\u88dc\u511f (CEC) \u6a21\u7d44\uff0c\u7528\u65bc\u8655\u7406\u611f\u6e2c\u5668\u672a\u5c0d\u6e96\u7684\u554f\u984c\uff0c\u4e26\u78ba\u4fdd\u7cbe\u78ba\u7684\u7279\u5fb5\u5c0d\u9f4a\u3002\u5728 V2X-Seq-SPD \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0cLET-VIC \u660e\u986f\u512a\u65bc\u57fa\u6e96\u6a21\u578b\uff0c\u5728 mAP \u4e0a\u7684\u6539\u9032\u81f3\u5c11\u70ba 13.7%\uff0c\u5728 AMOTA \u4e0a\u7684\u6539\u9032\u70ba 13.1%\uff0c\u4e14\u672a\u8003\u616e\u901a\u8a0a\u5ef6\u9072\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u500b\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u548c\u4e00\u500b\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u900f\u904e\u8eca\u8f1b\u57fa\u790e\u8a2d\u65bd\u5408\u4f5c\u4f86\u63d0\u5347\u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u6642\u9593\u611f\u77e5\u3002", "author": "Zhenwei Yang et.al.", "authors": "Zhenwei Yang, Jilei Mao, Wenxian Yang, Yibo Ai, Yu Kong, Haibao Yu, Weidong Zhang", "id": "2411.14927v1", "paper_url": "http://arxiv.org/abs/2411.14927v1", "repo": "null"}}