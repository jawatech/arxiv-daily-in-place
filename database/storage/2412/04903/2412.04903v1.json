{"2412.04903": {"publish_time": "2024-12-06", "title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation", "paper_summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u5404\u79cd\u89c6\u89c9\u95ee\u9898\u89e3\u7b54\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5229\u7528\u6307\u4ee4\u5fae\u8c03\u7279\u5b9a\u6570\u636e\u96c6\u3002\u5b83\u4eec\u8fd8\u53ef\u4ee5\u4ece\u4eba\u7c7b\u6807\u6ce8\u7684\u9996\u9009\u9879\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\u5e76\u51cf\u8f7b\u5e7b\u89c9\u3002\u5927\u591a\u6570\u9996\u9009\u9879\u6570\u636e\u90fd\u662f\u7531\u6a21\u578b\u672c\u8eab\u751f\u6210\u7684\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u5173\u952e\u6807\u7b7e\uff0c\u8fd9\u65e2\u6602\u8d35\u53c8\u4f9d\u8d56\u4e8e GPT-4V \u7b49\u4eba\u7c7b\u6216\u4e13\u6709\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u8fc7\u5173\u952e\u89c2\u5bdf\uff08EACO\uff09\u589e\u5f3a MLLM \u4e2d\u7684\u5bf9\u9f50\uff0c\u5b83\u4ec5\u4f7f\u7528 5k \u5f20\u56fe\u50cf\u7ecf\u6d4e\u5730\u901a\u8fc7\u81ea\u751f\u6210\u7684\u504f\u597d\u6570\u636e\u5bf9\u9f50 MLLM\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u4ece\u6536\u96c6\u548c\u4f18\u5316\u8bc4\u5206\u8bc4\u4f30\u6307\u4ee4\u8c03\u6574\u6570\u636e\u96c6\u5f00\u59cb\uff0c\u4ee5\u8bad\u7ec3\u4e00\u4e2a\u5173\u952e\u8bc4\u4f30\u6a21\u578b\uff0c\u79f0\u4e3a Critic\u3002\u8be5 Critic \u89c2\u5bdf\u8de8\u591a\u4e2a\u7ef4\u5ea6\u7684\u6a21\u578b\u54cd\u5e94\uff0c\u4e3a\u6539\u8fdb\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u8c03\u6574\u9009\u62e9\u9996\u9009\u548c\u975e\u9996\u9009\u8f93\u51fa\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u6211\u4eec\u5728\u504f\u597d\u8c03\u6574\u540e\u91c7\u7528\u4e86\u989d\u5916\u7684\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u3002EACO \u5728 HallusionBench \u4e0a\u5c06\u6574\u4f53\u5e7b\u89c9\u51cf\u5c11\u4e86 65.6%\uff0c\u5e76\u5728 MME-Cognition \u4e0a\u5c06\u63a8\u7406\u80fd\u529b\u63d0\u9ad8\u4e86 21.8%\u3002EACO \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4 LLaVA-v1.6-Mistral-7B \u63d0\u9ad8\u4e86 8.5%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cEACO \u4e5f\u663e\u793a\u4e86\u5f00\u6e90 MLLM \u4e2d\u6f5c\u5728\u7684\u5173\u952e\u80fd\u529b\uff0c\u8fd9\u8868\u660e EACO \u662f\u63d0\u5347 MLLM \u80fd\u529b\u7684\u53ef\u884c\u9014\u5f84\u3002", "author": "Yongxin Wang et.al.", "authors": "Yongxin Wang, Meng Cao, Haokun Lin, Mingfei Han, Liang Ma, Jin Jiang, Yuhao Cheng, Xiaodan Liang", "id": "2412.04903v1", "paper_url": "http://arxiv.org/abs/2412.04903v1", "repo": "null"}}