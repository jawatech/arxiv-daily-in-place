{"2501.09608": {"publish_time": "2025-01-16", "title": "Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning", "paper_summary": "Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t", "paper_summary_zh": "\u5ea6\u91cf\u5b66\u4e60\u9879\u76ee\u6837\u672c\u5230\u4e00\u4e2a\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u5176\u4e2d\u76f8\u4f3c\u6027\u548c\u76f8\u5f02\u6027\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u8868\u793a\u8fdb\u884c\u91cf\u5316\u3002\n\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6807\u7b7e\u6307\u5bfc\u8868\u793a\u5b66\u4e60\uff0c\u5176\u4e2d\u4e0d\u540c\u6a21\u6001\uff08\u5982\u97f3\u9891\u548c\u89c6\u89c9\u6570\u636e\uff09\u7684\u8868\u793a\u57fa\u4e8e\u6ce8\u91ca\u6807\u7b7e\u5bf9\u9f50\u3002\u8fd9\u79cd\u65b9\u6cd5\u5f80\u5f80\u4f4e\u4f30\u4e86\u6f5c\u5728\u7684\u590d\u6742\u7279\u5f81\u548c\u97f3\u9891\u548c\u89c6\u89c9\u6570\u636e\u5206\u5e03\u4e2d\u56fa\u6709\u7684\u6f5c\u5728\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u4e0e\u6807\u7b7e\u6ca1\u6709\u76f4\u63a5\u8054\u7cfb\uff0c\u5bfc\u81f4\u97f3\u9891\u89c6\u89c9\u5d4c\u5165\u5b66\u4e60\u7684\u6027\u80fd\u4e0d\u4f73\u3002\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u5b83\u5c06\u8de8\u6a21\u6001\u4e09\u5143\u635f\u5931\u4e0e\u6e10\u8fdb\u5f0f\u81ea\u84b8\u998f\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u56fa\u6709\u5206\u5e03\u548c\u52a8\u6001\u7ec6\u5316\u8f6f\u97f3\u9891\u89c6\u89c9\u5bf9\u9f50\u6765\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u2014\u2014\u97f3\u9891\u548c\u89c6\u89c9\u6570\u636e\u4e4b\u95f4\u7684\u6982\u7387\u5bf9\u9f50\uff0c\u6355\u83b7\u4e86\u8d85\u51fa\u663e\u5f0f\u6807\u7b7e\u7684\u56fa\u6709\u5173\u7cfb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u6a21\u578b\u4ece\u6bcf\u4e2a\u6279\u6b21\u7684\u4e00\u4e2a\u5b50\u96c6\u4e2d\u7684\u6ce8\u91ca\u6807\u7b7e\u4e2d\u63d0\u53d6\u57fa\u4e8e\u97f3\u9891\u89c6\u89c9\u5206\u5e03\u7684\u77e5\u8bc6\u3002\u8fd9\u79cd\u81ea\u84b8\u998f\u77e5\u8bc6\u7528\u4e8e t", "author": "Donghuo Zeng et.al.", "authors": "Donghuo Zeng, Kazushi Ikeda", "id": "2501.09608v1", "paper_url": "http://arxiv.org/abs/2501.09608v1", "repo": "null"}}