{"2412.18573": {"publish_time": "2024-12-24", "title": "How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation", "paper_summary": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u7531\u4ee3\u7801 LLM \u63d0\u4f9b\u652f\u6301\u7684\u8d8a\u6765\u8d8a\u591a\u7684 AI \u9a71\u52a8\u7684\u7f16\u7a0b\u52a9\u624b\u5df2\u96c6\u6210\u5230\u5404\u79cd\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5f00\u53d1\u4eba\u5458\u7684\u751f\u4ea7\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u573a\u666f\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86 LLM \u5728\u7279\u5b9a\u5e94\u7528\u7a0b\u5e8f\u9886\u57df\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6 MultiCodeBench \u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002MultiCodeBench \u5305\u542b 2,400 \u4e2a\u7f16\u7a0b\u4efb\u52a1\uff0c\u6db5\u76d6 12 \u4e2a\u6d41\u884c\u7684\u8f6f\u4ef6\u5f00\u53d1\u9886\u57df\u548c 15 \u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u7814\u7a76\u4ee5\u8bc6\u522b\u8fd9 12 \u4e2a\u5e94\u7528\u7a0b\u5e8f\u9886\u57df\u3002\u9274\u4e8e\u6bcf\u4e2a\u9886\u57df\u53ef\u80fd\u6d89\u53ca\u591a\u4e2a\u6280\u672f\u6846\u67b6\uff0c\u5e76\u4e14\u4e0d\u540c\u7684\u6846\u67b6\u5728\u7f16\u7801\u8fc7\u7a0b\u4e2d\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u6311\u6218\uff0c\u56e0\u6b64\u6211\u4eec\u5bf9\u6bcf\u4e2a\u9886\u57df\u4e2d\u5e38\u7528\u7684\u6846\u67b6\u548c\u5e73\u53f0\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u7136\u540e\uff0c\u6211\u4eec\u4ece\u4e0e\u8fd9\u4e9b\u5b50\u57df\u76f8\u5173\u7684 GitHub \u5b58\u50a8\u5e93\u4e2d\u62bd\u53d6\u7f16\u7a0b\u95ee\u9898\u3002\u4e3a\u4e86\u786e\u4fdd\u4efb\u52a1\u7684\u8d28\u91cf\u5e76\u51cf\u8f7b\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u6211\u4eec\u9080\u8bf7\u6ce8\u91ca\u5458\u91cd\u5199 MultiCodeBench \u4e2d\u6bcf\u4e2a\u4efb\u52a1\u7684\u6587\u6863\u5b57\u7b26\u4e32\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u4f9d\u8d56\u9879\u89e3\u6790\u5de5\u5177\uff0c\u4ee5\u63d0\u53d6\u6bcf\u4e2a\u4efb\u52a1\u7684\u57fa\u672c\u4e8b\u5b9e\u4e2d\u7684\u4f9d\u8d56\u9879\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6df1\u5165\u7684\u6027\u80fd\u5206\u6790\u3002\u901a\u8fc7\u5728 MultiCodeBench \u4e0a\u4f7f\u7528 11 \u4e2a\u6709\u4ee3\u8868\u6027\u7684\u4e3b\u6d41 LLM \u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63ed\u793a\u4e86 LLM \u5728\u4e0d\u540c\u5e94\u7528\u7a0b\u5e8f\u9886\u57df\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u4e0b\u6e38\u9886\u57df\u7684\u5f00\u53d1\u4eba\u5458\u5728\u9009\u62e9 LLM \u65f6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5206\u6790\u4e86\u6a21\u578b\u5728\u5b8c\u6210\u8f6f\u4ef6\u5e94\u7528\u7a0b\u5e8f\u5f00\u53d1\u4efb\u52a1\u65f6\u5931\u8d25\u7684\u539f\u56e0\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u4e86\u589e\u5f3a\u7279\u5b9a\u9886\u57df\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u6307\u5bfc\u3002</paragraph>", "author": "Dewu Zheng et.al.", "authors": "Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, Zibin Zheng", "id": "2412.18573v1", "paper_url": "http://arxiv.org/abs/2412.18573v1", "repo": "https://github.com/deepsoftwareanalytics/multicodebench"}}