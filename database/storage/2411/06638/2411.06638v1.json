{"2411.06638": {"publish_time": "2024-11-11", "title": "Model Editing for LLMs4Code: How Far are We?", "paper_summary": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4ee3\u78bc (LLMs4Code) \u5df2\u88ab\u767c\u73fe\u53ef\u5728\u8edf\u9ad4\u5de5\u7a0b\u9818\u57df\u5c55\u73fe\u5091\u51fa\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u7de8\u78bc\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u5148\u9032\u7684 LLMs4Code \u4e5f\u96e3\u514d\u5305\u542b\u4e0d\u6b63\u78ba\u6216\u904e\u6642\u7684\u7a0b\u5f0f\u78bc\u77e5\u8b58\u3002\u7531\u65bc\u8a13\u7df4 LLMs4Code \u7684\u6210\u672c\u5f88\u9ad8\uff0c\u56e0\u6b64\u4e0d\u5207\u5be6\u969b\u5730\u91cd\u65b0\u8a13\u7df4\u6a21\u578b\u4f86\u4fee\u6b63\u9019\u4e9b\u6709\u554f\u984c\u7684\u7a0b\u5f0f\u78bc\u77e5\u8b58\u3002\u6a21\u578b\u7de8\u8f2f\u662f\u4e00\u500b\u65b0\u7684\u6280\u8853\u9818\u57df\uff0c\u7528\u65bc\u6709\u6548\u4e14\u9ad8\u6548\u5730\u4fee\u6b63\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u932f\u8aa4\u77e5\u8b58\uff0c\u6700\u8fd1\u5df2\u63d0\u51fa\u5404\u7a2e\u6a21\u578b\u7de8\u8f2f\u6280\u8853\u548c\u57fa\u6e96\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u4e00\u500b\u5168\u9762\u7814\u7a76\uff0c\u5fb9\u5e95\u6bd4\u8f03\u548c\u5206\u6790\u6700\u5148\u9032\u7684\u6a21\u578b\u7de8\u8f2f\u6280\u8853\u5728\u5404\u7a2e\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u4efb\u52d9\u4e2d\u8abf\u6574 LLMs4Code \u5167\u90e8\u77e5\u8b58\u7684\u6548\u80fd\uff0c\u986f\u8457\u5730\u4e0d\u5b58\u5728\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5c0d\u61c9\u7528\u6700\u5148\u9032\u7684\u6a21\u578b\u7de8\u8f2f\u65b9\u6cd5\u4f86\u4fee\u5fa9 LLMs4Code \u7684\u4e0d\u6e96\u78ba\u6027\u9032\u884c\u4e86\u7b2c\u4e00\u500b\u7cfb\u7d71\u6027\u7814\u7a76\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u540d\u70ba CLMEEval \u7684\u57fa\u6e96\uff0c\u5b83\u5305\u542b\u5169\u500b\u8cc7\u6599\u96c6\uff0c\u5373\u5305\u542b 21K+ \u7a0b\u5f0f\u78bc\u751f\u6210\u7bc4\u4f8b\u7684 CoNaLa-Edit (CNLE) \u548c\u5305\u542b 16K+ \u7a0b\u5f0f\u78bc\u6458\u8981\u7bc4\u4f8b\u7684 CodeSearchNet-Edit (CSNE)\u3002\u5728 CLMEEval \u7684\u5e6b\u52a9\u4e0b\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u516d\u7a2e\u5148\u9032\u7684\u6a21\u578b\u7de8\u8f2f\u6280\u8853\u5728\u4e09\u500b LLMs4Code \u4e0a\u7684\u8868\u73fe\uff1aCodeLlama (7B)\u3001CodeQwen1.5 (7B) \u548c Stable-Code (3B)\u3002\u6211\u5011\u7684\u767c\u73fe\u5305\u62ec\u57fa\u65bc\u5916\u90e8\u8a18\u61b6\u7684 GRACE \u65b9\u6cd5\u5be6\u73fe\u4e86\u6700\u4f73\u7684\u77e5\u8b58\u7de8\u8f2f\u6548\u80fd\u548c\u7279\u7570\u6027\uff08\u7de8\u8f2f\u4e0d\u6703\u5f71\u97ff\u672a\u9396\u5b9a\u7684\u77e5\u8b58\uff09\uff0c\u800c\u6cdb\u5316\uff08\u7de8\u8f2f\u662f\u5426\u53ef\u4ee5\u6cdb\u5316\u5230\u5176\u4ed6\u8a9e\u7fa9\u76f8\u540c\u7684\u8f38\u5165\uff09\u662f\u73fe\u6709\u6280\u8853\u7684\u666e\u904d\u6311\u6230\u3002\u6b64\u5916\uff0c\u5728\u6df1\u5165\u6848\u4f8b\u5206\u6790\u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u5f15\u5165\u4e86 GRACE \u7684\u589e\u5f37\u7248\u672c\uff0c\u7a31\u70ba A-GRACE\uff0c\u5b83\u7d50\u5408\u4e86\u5c0d\u6bd4\u5b78\u7fd2\u4ee5\u66f4\u597d\u5730\u6355\u6349\u8f38\u5165\u7684\u8a9e\u7fa9\u3002</paragraph>", "author": "Xiaopeng Li et.al.", "authors": "Xiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Bin Ji, Weimin Zhang", "id": "2411.06638v1", "paper_url": "http://arxiv.org/abs/2411.06638v1", "repo": "https://github.com/xpq-tech/code-llmedit"}}