{"2411.11739": {"publish_time": "2024-11-18", "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou", "paper_summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u96a8\u8457\u591a\u6a21\u614b\u5927\u578b\u6a21\u578b\u7684\u986f\u8457\u6f14\u9032\uff0c\n\u8a31\u591a\u63a8\u85a6\u7814\u7a76\u8005\u9ad4\u8a8d\u5230\u591a\u6a21\u614b\u8cc7\u8a0a\u5728\u4f7f\u7528\u8005\u8208\u8da3\u5efa\u6a21\u4e0a\u7684\u6f5b\u529b\u3002\u5728\u7522\u696d\u4e2d\uff0c\u4e00\u500b\u5ee3\u70ba\u4f7f\u7528\u7684\u5efa\u6a21\u67b6\u69cb\u70ba\n\u4e32\u63a5\u5f0f\u7bc4\u4f8b\uff1a(1) \u9996\u5148\u9810\u8a13\u7df4\u4e00\u500b\u591a\u6a21\u614b\u6a21\u578b\uff0c\u4ee5\u63d0\u4f9b\u4e0b\u6e38\u670d\u52d9\u7684\u5168\u80fd\u8868\u793a\uff1b(2) \u4e0b\u6e38\u63a8\u85a6\u6a21\u578b\u5c07\u591a\u6a21\u614b\u8868\u793a\u4f5c\u70ba\u984d\u5916\u8f38\u5165\uff0c\u4ee5\u7b26\u5408\u771f\u5be6\u4f7f\u7528\u8005-\u9805\u76ee\u884c\u70ba\u3002\u5118\u7ba1\u6b64\u7bc4\u4f8b\u7372\u5f97\u986f\u8457\u7684\u9032\u6b65\uff0c\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u5169\u500b\u9650\u5236\u6a21\u578b\u6548\u80fd\u7684\u554f\u984c\uff1a(1) \u8868\u793a\u4e0d\u5339\u914d\uff1a\u9810\u8a13\u7df4\u7684\u591a\u6a21\u614b\u6a21\u578b\u7e3d\u662f\u53d7\u5230\u50b3\u7d71 NLP/CV \u4efb\u52d9\u7684\u76e3\u7763\uff0c\u800c\u63a8\u85a6\u6a21\u578b\u5247\u53d7\u5230\u771f\u5be6\u4f7f\u7528\u8005-\u9805\u76ee\u4e92\u52d5\u7684\u76e3\u7763\u3002\u56e0\u6b64\uff0c\u9019\u5169\u500b\u6839\u672c\u4e0d\u540c\u7684\u4efb\u52d9\u76ee\u6a19\u76f8\u5c0d\u7368\u7acb\uff0c\u4e26\u4e14\u5b83\u5011\u7684\u8868\u793a\u7f3a\u4e4f\u4e00\u81f4\u7684\u76ee\u6a19\uff1b(2) \u8868\u793a\u907a\u5fd8\uff1a\u7522\u751f\u7684\u591a\u6a21\u614b\u8868\u793a\u7e3d\u662f\u5132\u5b58\u5728\u5feb\u53d6\u5132\u5b58\u5340\u4e2d\uff0c\u4e26\u4f5c\u70ba\u63a8\u85a6\u6a21\u578b\u7684\u984d\u5916\u56fa\u5b9a\u8f38\u5165\uff0c\u56e0\u6b64\u7121\u6cd5\u7531\u63a8\u85a6\u6a21\u578b\u68af\u5ea6\u66f4\u65b0\uff0c\u9032\u4e00\u6b65\u4e0d\u5229\u65bc\u4e0b\u6e38\u8a13\u7df4\u3002\u53d7\u5230\u4e0b\u6e38\u4efb\u52d9\u4f7f\u7528\u4e2d\u7684\u5169\u500b\u56f0\u96e3\u6311\u6230\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u91cf\u5316\u7684\u591a\u6a21\u614b\u6846\u67b6\uff0c\u4ee5\u81ea\u8a02\u7279\u5b9a\u4e14\u53ef\u8a13\u7df4\u7684\u591a\u6a21\u614b\u8cc7\u8a0a\uff0c\u4f9b\u4e0d\u540c\u7684\u4e0b\u6e38\u6a21\u578b\u4f7f\u7528\u3002", "author": "Xinchen Luo et.al.", "authors": "Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, Changqing Qiu, Jiaqi Zhang, Xu Zhang, Zhiheng Yan, Jingming Zhang, Simin Zhang, Mingxing Wen, Zhaojie Liu, Kun Gai, Guorui Zhou", "id": "2411.11739v1", "paper_url": "http://arxiv.org/abs/2411.11739v1", "repo": "null"}}