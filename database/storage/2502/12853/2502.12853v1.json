{"2502.12853": {"publish_time": "2025-02-18", "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning", "paper_summary": "Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\u4e86 LLM \u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7684\u6709\u6548\u6027\u3002\n\u7136\u800c\uff0c\u73b0\u6709\u6fc0\u52b1 LLM \u6df1\u5ea6\u601d\u8003\u80fd\u529b\u7684\u65b9\u6cd5\n\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u6216\u5927\u91cf\u7684\u8bad\u7ec3\u5de5\u4f5c\u3002\u540c\u65f6\uff0c\n\u5982\u4f55\u63d0\u9ad8\u8f83\u5f31\u57fa\u7840\u6a21\u578b\u7684\u601d\u8003\u80fd\u529b\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 S$^2$R\uff0c\u4e00\u4e2a\u901a\u8fc7\u6559\u5bfc\u6a21\u578b\u5728\n\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u7ea0\u6b63\u6765\u589e\u5f3a LLM \u63a8\u7406\u7684\u6709\u6548\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5bf9\u7cbe\u5fc3\u6574\u7406\u7684\u6570\u636e\u6765\u521d\u59cb\u5316\u5177\u6709\u8fed\u4ee3\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u7ea0\u6b63\u884c\u4e3a\u7684 LLM\u3002\u7136\u540e\u901a\u8fc7\u7ed3\u679c\u7ea7\u522b\u548c\u8fc7\u7a0b\u7ea7\u522b\u7684\u5f3a\u5316\n\u5b66\u4e60\u8fdb\u4e00\u6b65\u52a0\u5f3a\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u7ea0\u6b63\u6280\u80fd\uff0c\u540c\u65f6\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\n\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u4f18\u5316\u5176\u63a8\u7406\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u7ed3\u679c\n\u8868\u660e\uff0c\u4ec5\u4f7f\u7528 3.1k \u4e2a\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u7ea0\u6b63\u884c\u4e3a\n\u521d\u59cb\u5316\u6837\u672c\uff0cQwen2.5-math-7B \u7684\u51c6\u786e\u7387\u4ece\n51.0% \u63d0\u9ad8\u5230 81.6%\uff0c\u4f18\u4e8e\u5728\u7b49\u91cf\u957f CoT \u84b8\u998f\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u57fa\u4e8e\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\u5728\u57df\u5185\u548c\u57df\u5916\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u5206\u6790\u9a8c\u8bc1\u4e86\nS$^2$R \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u53ef\u4ee5\u5728\nhttps://github.com/NineAbyss/S2R \u83b7\u5f97\u3002</paragraph>", "author": "Ruotian Ma et.al.", "authors": "Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, Jia Li", "id": "2502.12853v1", "paper_url": "http://arxiv.org/abs/2502.12853v1", "repo": "null"}}