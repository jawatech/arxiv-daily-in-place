{"2405.20131": {"publish_time": "2024-05-30", "title": "Language Models Need Inductive Biases to Count Inductively", "paper_summary": "Counting is a fundamental example of generalization, whether viewed through\nthe mathematical lens of Peano's axioms defining the natural numbers or the\ncognitive science literature for children learning to count. The argument holds\nfor both cases that learning to count means learning to count infinitely. While\nfew papers have tried to distill transformer \"reasoning\" to the simplest case\nof counting, investigating length generalization does occur throughout the\nliterature. In the \"train short, test long\" paradigm of NLP, length refers to\nthe training sentence length. In formal language recognition, length refers to\nthe input sequence length, or the maximum stack size induced by a pushdown\nautomata. In general problem solving, length refers to the number of hops in a\ndeductive reasoning chain or the recursion depth. For all cases, counting is\ncentral to task success. And crucially, generalizing counting inductively is\ncentral to success on OOD instances. This work provides extensive empirical\nresults on training language models to count. We experiment with architectures\nranging from RNNs, Transformers, State-Space Models and RWKV. We present\ncarefully-designed task formats, auxiliary tasks and positional embeddings to\navoid limitations in generalization with OOD-position and OOD-vocabulary. We\nfind that while traditional RNNs trivially achieve inductive counting,\nTransformers have to rely on positional embeddings to count out-of-domain. As\ncounting is the basis for many arguments concerning the expressivity of\nTransformers, our finding calls for the community to reexamine the application\nscope of primitive functions defined in formal characterizations. Finally,\nmodern RNNs also largely underperform traditional RNNs in generalizing counting\ninductively. We discuss how design choices that enable parallelized training of\nmodern RNNs cause them to lose merits of a recurrent nature.", "paper_summary_zh": "<paragraph>\u7121\u8ad6\u662f\u5f9e\u5b9a\u7fa9\u81ea\u7136\u6578\u7684\u76ae\u4e9e\u8afe\u516c\u7406\u7684\u6578\u5b78\u89d2\u5ea6\u4f86\u770b\uff0c\u9084\u662f\u5f9e\u5152\u7ae5\u5b78\u7fd2\u6578\u6578\u7684\u8a8d\u77e5\u79d1\u5b78\u6587\u737b\u4f86\u770b\uff0c\u6578\u6578\u90fd\u662f\u6982\u62ec\u5316\u7684\u57fa\u672c\u7bc4\u4f8b\u3002\u9019\u5169\u65b9\u9762\u7684\u8ad6\u9ede\u90fd\u8a8d\u70ba\uff0c\u5b78\u7fd2\u6578\u6578\u610f\u5473\u8457\u5b78\u7fd2\u7121\u9650\u6578\u6578\u3002\u5118\u7ba1\u5f88\u5c11\u6709\u8ad6\u6587\u8a66\u5716\u5c07Transformer\u7684\u300c\u63a8\u7406\u300d\u7c21\u5316\u70ba\u6700\u7c21\u55ae\u7684\u6578\u6578\u6848\u4f8b\uff0c\u4f46\u5728\u6574\u500b\u6587\u737b\u4e2d\u78ba\u5be6\u6703\u63a2\u8a0e\u9577\u5ea6\u6982\u62ec\u5316\u3002\u5728 NLP \u7684\u300c\u77ed\u8a13\u7df4\uff0c\u9577\u6e2c\u8a66\u300d\u7bc4\u4f8b\u4e2d\uff0c\u9577\u5ea6\u662f\u6307\u8a13\u7df4\u53e5\u5b50\u7684\u9577\u5ea6\u3002\u5728\u5f62\u5f0f\u8a9e\u8a00\u8b58\u5225\u4e2d\uff0c\u9577\u5ea6\u662f\u6307\u8f38\u5165\u5e8f\u5217\u7684\u9577\u5ea6\uff0c\u6216\u7531\u4e0b\u63a8\u81ea\u52d5\u6a5f\u7522\u751f\u7684\u6700\u5927\u5806\u758a\u5927\u5c0f\u3002\u5728\u4e00\u822c\u554f\u984c\u89e3\u6c7a\u4e2d\uff0c\u9577\u5ea6\u662f\u6307\u6f14\u7e79\u63a8\u7406\u93c8\u4e2d\u7684\u8df3\u8e8d\u6b21\u6578\u6216\u905e\u8ff4\u6df1\u5ea6\u3002\u5c0d\u65bc\u6240\u6709\u60c5\u6cc1\uff0c\u6578\u6578\u5c0d\u65bc\u4efb\u52d9\u6210\u529f\u81f3\u95dc\u91cd\u8981\u3002\u800c\u4e14\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6b78\u7d0d\u6982\u62ec\u6578\u6578\u5c0d\u65bc OOD \u5be6\u4f8b\u7684\u6210\u529f\u81f3\u95dc\u91cd\u8981\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5ee3\u6cdb\u7684\u5be6\u8b49\u7d50\u679c\uff0c\u8aaa\u660e\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u6578\u6578\u3002\u6211\u5011\u5be6\u9a57\u4e86\u5f9e RNN\u3001Transformer\u3001\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u548c RWKV \u7b49\u67b6\u69cb\u3002\u6211\u5011\u63d0\u51fa\u4e86\u7cbe\u5fc3\u8a2d\u8a08\u7684\u4efb\u52d9\u683c\u5f0f\u3001\u8f14\u52a9\u4efb\u52d9\u548c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4ee5\u907f\u514d\u5728 OOD \u4f4d\u7f6e\u548c OOD \u8a5e\u5f59\u4e2d\u6982\u62ec\u7684\u9650\u5236\u3002\u6211\u5011\u767c\u73fe\uff0c\u96d6\u7136\u50b3\u7d71\u7684 RNN \u53ef\u4ee5\u8f15\u800c\u6613\u8209\u5730\u5be6\u73fe\u6b78\u7d0d\u6578\u6578\uff0c\u4f46Transformer\u5fc5\u9808\u4f9d\u8cf4\u4f4d\u7f6e\u5d4c\u5165\u624d\u80fd\u9032\u884c\u57df\u5916\u6578\u6578\u3002\u7531\u65bc\u6578\u6578\u662f\u8a31\u591a\u95dc\u65bcTransformer\u8868\u9054\u80fd\u529b\u7684\u8ad6\u9ede\u7684\u57fa\u790e\uff0c\u6211\u5011\u7684\u767c\u73fe\u8981\u6c42\u793e\u7fa4\u91cd\u65b0\u5be9\u8996\u5f62\u5f0f\u8868\u5fb5\u4e2d\u5b9a\u7fa9\u7684\u539f\u59cb\u51fd\u6578\u7684\u61c9\u7528\u7bc4\u570d\u3002\u6700\u5f8c\uff0c\u73fe\u4ee3 RNN \u5728\u6b78\u7d0d\u6982\u62ec\u6578\u6578\u65b9\u9762\u4e5f\u9060\u9060\u4e0d\u5982\u50b3\u7d71 RNN\u3002\u6211\u5011\u8a0e\u8ad6\u4e86\u4f7f\u73fe\u4ee3 RNN \u80fd\u5920\u9032\u884c\u4e26\u884c\u8a13\u7df4\u7684\u8a2d\u8a08\u9078\u64c7\u5982\u4f55\u5c0e\u81f4\u5b83\u5011\u5931\u53bb\u905e\u8ff4\u7279\u6027\u7684\u512a\u9ede\u3002</paragraph>", "author": "Yingshan Chang et.al.", "authors": "Yingshan Chang, Yonatan Bisk", "id": "2405.20131v1", "paper_url": "http://arxiv.org/abs/2405.20131v1", "repo": "https://github.com/zdxdsw/inductive_counting_with_lms"}}