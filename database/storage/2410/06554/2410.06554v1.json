{"2410.06554": {"publish_time": "2024-10-09", "title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "paper_summary": "Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at\n[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).", "paper_summary_zh": "\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2\uff0c\u80fd\u986f\u8457\u589e\u5f37\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u8b93\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u671f\u671b\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u6b64\u4e00\u81f4\u6027\u4e2d\uff0c\u4e00\u500b\u95dc\u9375\u7684\u56e0\u7d20\u662f\u8a13\u7df4\u671f\u9593\u4f7f\u7528\u7684\u734e\u52f5\u6a21\u578b\u5f37\u5ea6\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u5f37\u5ea6\u8f03\u9ad8\u7684\u734e\u52f5\u6a21\u578b\u662f\u5426\u4e00\u5b9a\u6703\u7522\u751f\u66f4\u597d\u7684\u8a9e\u8a00\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u900f\u904e\u5c0d QA-FEEDBACK \u8cc7\u6599\u96c6\u548c\u57fa\u65bc Longformer \u7684\u734e\u52f5\u6a21\u578b\u9032\u884c\u76f8\u95dc\u6027\u3001\u4e8b\u5be6\u6027\u548c\u5b8c\u6574\u6027\u4efb\u52d9\u7684\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\u4e86\u4e00\u500b\u9a5a\u4eba\u7684\u6096\u8ad6\uff1a\u4f7f\u7528\u4e2d\u7b49\u7cbe\u78ba\u5ea6\u734e\u52f5\u6a21\u578b\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u8868\u73fe\u512a\u65bc\u90a3\u4e9b\u7531\u9ad8\u7cbe\u78ba\u5ea6\u734e\u52f5\u6a21\u578b\u5f15\u5c0e\u7684\u8a9e\u8a00\u6a21\u578b\u3002\u9019\u6311\u6230\u4e86\u5ee3\u6cdb\u5b58\u5728\u7684\u4fe1\u5ff5\uff0c\u5373\u5f37\u5ea6\u8f03\u9ad8\u7684\u734e\u52f5\u6a21\u578b\u7e3d\u662f\u6703\u7522\u751f\u66f4\u597d\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u70ba\u672a\u4f86\u7814\u7a76\u9a45\u52d5\u6a21\u578b\u6548\u80fd\u548c\u5982\u4f55\u9078\u64c7\u6700\u5408\u9069\u7684\u734e\u52f5\u6a21\u578b\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u958b\u555f\u4e86\u65b0\u7684\u9014\u5f91\u3002\u7a0b\u5f0f\u78bc\u548c\u5176\u4ed6\u8a73\u7d30\u8cc7\u8a0a\u53ef\u5728 [https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF) \u53d6\u5f97\u3002", "author": "Yanjun Chen et.al.", "authors": "Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen", "id": "2410.06554v1", "paper_url": "http://arxiv.org/abs/2410.06554v1", "repo": "null"}}