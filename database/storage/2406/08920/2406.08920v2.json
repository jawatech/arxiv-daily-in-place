{"2406.08920": {"publish_time": "2024-06-13", "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis", "paper_summary": "Novel view acoustic synthesis (NVAS) aims to render binaural audio at any\ntarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.\nExisting methods have proposed NeRF-based implicit models to exploit visual\ncues as a condition for synthesizing binaural audio. However, in addition to\nlow efficiency originating from heavy NeRF rendering, these methods all have a\nlimited ability of characterizing the entire scene environment such as room\ngeometry, material properties, and the spatial relation between the listener\nand sound source. To address these issues, we propose a novel Audio-Visual\nGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware\ncondition for audio synthesis, we learn an explicit point-based scene\nrepresentation with an audio-guidance parameter on locally initialized Gaussian\npoints, taking into account the space relation from the listener and sound\nsource. To make the visual scene model audio adaptive, we propose a point\ndensification and pruning strategy to optimally distribute the Gaussian points,\nwith the per-point contribution in sound propagation (e.g., more points needed\nfor texture-less wall surfaces as they affect sound path diversion). Extensive\nexperiments validate the superiority of our AV-GS over existing alternatives on\nthe real-world RWAS and simulation-based SoundSpaces datasets.", "paper_summary_zh": "\u65b0\u7a4e\u89c0\u9ede\u8072\u5b78\u5408\u6210\uff08NVAS\uff09\u65e8\u5728\u5448\u73fe\u4efb\u4f55\u76ee\u6a19\u8996\u9ede\u7684\u96d9\u8033\u97f3\u8a0a\uff0c\u4e26\u63d0\u4f9b 3D \u5834\u666f\u4e2d\u8072\u6e90\u767c\u51fa\u7684\u55ae\u8072\u9053\u97f3\u8a0a\u3002\u73fe\u6709\u65b9\u6cd5\u5df2\u63d0\u51fa\u57fa\u65bc NeRF \u7684\u96b1\u5f0f\u6a21\u578b\uff0c\u4ee5\u5229\u7528\u8996\u89ba\u7dda\u7d22\u4f5c\u70ba\u5408\u6210\u96d9\u8033\u97f3\u8a0a\u7684\u689d\u4ef6\u3002\u7136\u800c\uff0c\u9664\u4e86\u6e90\u81ea\u7e41\u91cd NeRF \u6e32\u67d3\u7684\u4f4e\u6548\u7387\u4e4b\u5916\uff0c\u9019\u4e9b\u65b9\u6cd5\u5728\u8868\u5fb5\u6574\u500b\u5834\u666f\u74b0\u5883\uff08\u4f8b\u5982\u623f\u9593\u5e7e\u4f55\u5f62\u72c0\u3001\u6750\u6599\u5c6c\u6027\u548c\u807d\u773e\u8207\u8072\u6e90\u4e4b\u9593\u7684\u7a7a\u9593\u95dc\u4fc2\uff09\u7684\u80fd\u529b\u4e0a\u4e5f\u6709\u9650\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u97f3\u8996\u9ad8\u65af\u6f51\u9ede\uff08AV-GS\uff09\u6a21\u578b\u3002\u70ba\u4e86\u7372\u5f97\u5c0d\u6750\u6599\u548c\u5e7e\u4f55\u5f62\u72c0\u611f\u77e5\u7684\u97f3\u8a0a\u5408\u6210\u689d\u4ef6\uff0c\u6211\u5011\u5b78\u7fd2\u4e86\u4e00\u500b\u660e\u78ba\u7684\u57fa\u65bc\u9ede\u7684\u5834\u666f\u8868\u793a\uff0c\u5176\u4e2d\u5305\u542b\u5c40\u90e8\u521d\u59cb\u5316\u7684\u9ad8\u65af\u9ede\u4e0a\u7684\u97f3\u8a0a\u5f15\u5c0e\u53c3\u6578\uff0c\u4e26\u8003\u616e\u4e86\u807d\u773e\u8207\u8072\u6e90\u7684\u7a7a\u9593\u95dc\u4fc2\u3002\u70ba\u4e86\u4f7f\u8996\u89ba\u5834\u666f\u6a21\u578b\u9069\u61c9\u97f3\u8a0a\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u9ede\u589e\u5bc6\u548c\u526a\u679d\u7b56\u7565\uff0c\u4ee5\u6700\u4f73\u65b9\u5f0f\u5206\u4f48\u9ad8\u65af\u9ede\uff0c\u4e26\u5728\u8072\u97f3\u50b3\u64ad\u4e2d\u6309\u9ede\u8ca2\u737b\uff08\u4f8b\u5982\uff0c\u7d0b\u7406\u8f03\u5c11\u7684\u7246\u9762\u9700\u8981\u66f4\u591a\u9ede\uff0c\u56e0\u70ba\u5b83\u5011\u6703\u5f71\u97ff\u8072\u97f3\u8def\u5f91\u8f49\u79fb\uff09\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684 AV-GS \u5728\u771f\u5be6\u4e16\u754c RWAS \u548c\u57fa\u65bc\u6a21\u64ec\u7684 SoundSpaces \u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u73fe\u6709\u66ff\u4ee3\u65b9\u6848\u3002", "author": "Swapnil Bhosale et.al.", "authors": "Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu", "id": "2406.08920v2", "paper_url": "http://arxiv.org/abs/2406.08920v2", "repo": "null"}}