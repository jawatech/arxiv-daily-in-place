{"2501.01424": {"publish_time": "2025-01-02", "title": "Object-level Visual Prompts for Compositional Image Generation", "paper_summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5728\u6587\u672c\u5230\u5f71\u50cf\u64f4\u6563\u6a21\u578b\u4e2d\u7d44\u6210\u7269\u4ef6\u5c64\u7d1a\u8996\u89ba\u63d0\u793a\u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u505a\u6cd5\u8655\u7406\u5728\u4e0d\u540c\u7684\u5834\u666f\u548c\u98a8\u683c\u4e2d\u7522\u751f\u8a9e\u610f\u9023\u8cab\u7684\u7d44\u5408\u4efb\u52d9\uff0c\u985e\u4f3c\u65bc\u6587\u5b57\u63d0\u793a\u6240\u63d0\u4f9b\u7684\u591a\u6a23\u6027\u548c\u8868\u73fe\u529b\u3002\u6b64\u4efb\u52d9\u4e2d\u7684\u95dc\u9375\u6311\u6230\u662f\u4fdd\u7559\u8f38\u5165\u8996\u89ba\u63d0\u793a\u4e2d\u6240\u63cf\u7e6a\u7269\u4ef6\u7684\u8eab\u4efd\uff0c\u540c\u6642\u5728\u4e0d\u540c\u7684\u5f71\u50cf\u4e2d\u7522\u751f\u591a\u6a23\u5316\u7684\u7d44\u5408\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684 KV \u6df7\u5408\u4ea4\u53c9\u6ce8\u610f\u6a5f\u5236\uff0c\u5176\u4e2d\u9375\u548c\u503c\u5f9e\u4e0d\u540c\u7684\u8996\u89ba\u8868\u793a\u4e2d\u5b78\u7fd2\u3002\u9375\u6e90\u81ea\u5177\u6709\u5c0f\u578b\u74f6\u9838\u7684\u7de8\u78bc\u5668\uff0c\u7528\u65bc\u5e03\u5c40\u63a7\u5236\uff0c\u800c\u503c\u5247\u4f86\u81ea\u64f7\u53d6\u7d30\u7dfb\u5916\u89c0\u7d30\u7bc0\u7684\u8f03\u5927\u578b\u74f6\u9838\u7de8\u78bc\u5668\u3002\u900f\u904e\u6df7\u5408\u4f86\u81ea\u9019\u4e9b\u4e92\u88dc\u4f86\u6e90\u7684\u9375\u548c\u503c\uff0c\u6211\u5011\u7684\u6a21\u578b\u4fdd\u7559\u4e86\u8996\u89ba\u63d0\u793a\u7684\u8eab\u4efd\uff0c\u540c\u6642\u652f\u63f4\u7269\u4ef6\u6392\u5217\u3001\u59ff\u52e2\u548c\u7d44\u5408\u7684\u9748\u6d3b\u8b8a\u5316\u3002\u5728\u63a8\u7406\u671f\u9593\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u7269\u4ef6\u5c64\u7d1a\u7684\u7d44\u5408\u6307\u5c0e\uff0c\u4ee5\u6539\u5584\u65b9\u6cd5\u7684\u8eab\u4efd\u4fdd\u7559\u548c\u5e03\u5c40\u6b63\u78ba\u6027\u3002\u7d50\u679c\u986f\u793a\u6211\u5011\u7684\u6280\u8853\u7522\u751f\u4e86\u591a\u6a23\u7684\u5834\u666f\u7d44\u5408\uff0c\u4fdd\u7559\u4e86\u6bcf\u500b\u8996\u89ba\u63d0\u793a\u7684\u7368\u7279\u7279\u5fb5\uff0c\u64f4\u5c55\u4e86\u6587\u5b57\u5230\u5f71\u50cf\u751f\u6210\u7684\u5275\u610f\u6f5b\u529b\u3002", "author": "Gaurav Parmar et.al.", "authors": "Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman", "id": "2501.01424v1", "paper_url": "http://arxiv.org/abs/2501.01424v1", "repo": "null"}}