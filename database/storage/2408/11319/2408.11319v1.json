{"2408.11319": {"publish_time": "2024-08-21", "title": "Towards Evaluating Large Language Models on Sarcasm Understanding", "paper_summary": "In the era of large language models (LLMs), the task of ``System I''~-~the\nfast, unconscious, and intuitive tasks, e.g., sentiment analysis, text\nclassification, etc., have been argued to be successfully solved. However,\nsarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices\nlike hyperbole and figuration to convey true sentiments and intentions,\ninvolving a higher level of abstraction than sentiment analysis. There is\ngrowing concern that the argument about LLMs' success may not be fully tenable\nwhen considering sarcasm understanding. To address this question, we select\neleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present\ncomprehensive evaluations on six widely used benchmark datasets through\ndifferent prompting approaches, i.e., zero-shot input/output (IO) prompting,\nfew-shot IO prompting, chain of thought (CoT) prompting. Our results highlight\nthree key findings: (1) current LLMs underperform supervised PLMs based sarcasm\ndetection baselines across six sarcasm benchmarks. This suggests that\nsignificant efforts are still required to improve LLMs' understanding of human\nsarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across\nvarious prompting methods, with an average improvement of 14.0\\%$\\uparrow$.\nClaude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3)\nFew-shot IO prompting method outperforms the other two methods: zero-shot IO\nand few-shot CoT. The reason is that sarcasm detection, being a holistic,\nintuitive, and non-rational cognitive process, is argued not to adhere to\nstep-by-step logical reasoning, making CoT less effective in understanding\nsarcasm compared to its effectiveness in mathematical reasoning tasks.", "paper_summary_zh": "<paragraph>\u5728\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u6642\u4ee3\uff0c``\u7cfb\u7d71 I'' \u7684\u4efb\u52d9~-~\u5feb\u901f\u3001\u7121\u610f\u8b58\u548c\u76f4\u89ba\u6027\u7684\u4efb\u52d9\uff0c\u4f8b\u5982\u60c5\u7dd2\u5206\u6790\u3001\u6587\u5b57\u5206\u985e\u7b49\uff0c\u5df2\u88ab\u8a8d\u70ba\u5df2\u6210\u529f\u89e3\u6c7a\u3002\u7136\u800c\uff0c\u8af7\u523a\u4f5c\u70ba\u4e00\u7a2e\u5fae\u5999\u7684\u8a9e\u8a00\u73fe\u8c61\uff0c\u901a\u5e38\u63a1\u7528\u8a87\u98fe\u548c\u6bd4\u55bb\u7b49\u4fee\u8fad\u624b\u6cd5\u4f86\u50b3\u9054\u771f\u5be6\u7684\u60c5\u611f\u548c\u610f\u5716\uff0c\u6d89\u53ca\u6bd4\u60c5\u7dd2\u5206\u6790\u66f4\u9ad8\u7684\u62bd\u8c61\u5c64\u6b21\u3002\u8d8a\u4f86\u8d8a\u591a\u4eba\u64d4\u5fc3\uff0c\u5728\u8003\u616e\u8af7\u523a\u7406\u89e3\u6642\uff0c\u95dc\u65bc LLM \u6210\u529f\u7684\u4e3b\u5f35\u53ef\u80fd\u7121\u6cd5\u5b8c\u5168\u6210\u7acb\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u9078\u64c7\u4e86\u5341\u4e00\u7a2e SOTA LLM \u548c\u516b\u7a2e SOTA \u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM)\uff0c\u4e26\u901a\u904e\u4e0d\u540c\u7684\u63d0\u793a\u65b9\u6cd5\u5c0d\u516d\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u6e96\u6578\u64da\u96c6\u9032\u884c\u4e86\u5168\u9762\u7684\u8a55\u4f30\uff0c\u5373\u96f6\u6b21\u8f38\u5165/\u8f38\u51fa (IO) \u63d0\u793a\u3001\u5c11\u6b21 IO \u63d0\u793a\u3001\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u3002\u6211\u5011\u7684\u7d50\u679c\u7a81\u51fa\u4e86\u4e09\u500b\u95dc\u9375\u767c\u73fe\uff1a(1) \u7576\u524d LLM \u5728\u516d\u500b\u8af7\u523a\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8868\u73fe\u4e0d\u5982\u57fa\u65bc\u76e3\u7763 PLM \u7684\u8af7\u523a\u6aa2\u6e2c\u57fa\u6e96\u3002\u9019\u8868\u660e\uff0c\u4ecd\u9700\u8981\u4ed8\u51fa\u5de8\u5927\u7684\u52aa\u529b\u4f86\u63d0\u9ad8 LLM \u5c0d\u4eba\u985e\u8af7\u523a\u7684\u7406\u89e3\u3002(2) GPT-4 \u5728\u5404\u7a2e\u63d0\u793a\u65b9\u6cd5\u4e2d\u59cb\u7d42\u986f\u8457\u512a\u65bc\u5176\u4ed6 LLM\uff0c\u5e73\u5747\u6539\u9032\u4e86 14.0%$\\uparrow$\u3002Claude 3 \u548c ChatGPT \u5728 GPT-4 \u4e4b\u5f8c\u8868\u73fe\u51fa\u6b21\u4f73\u7684\u6027\u80fd\u3002(3) \u5c11\u6b21 IO \u63d0\u793a\u65b9\u6cd5\u512a\u65bc\u5176\u4ed6\u5169\u7a2e\u65b9\u6cd5\uff1a\u96f6\u6b21 IO \u548c\u5c11\u6b21 CoT\u3002\u539f\u56e0\u662f\u8af7\u523a\u6aa2\u6e2c\u662f\u4e00\u500b\u6574\u9ad4\u7684\u3001\u76f4\u89ba\u7684\u548c\u975e\u7406\u6027\u7684\u8a8d\u77e5\u904e\u7a0b\uff0c\u64da\u8a8d\u70ba\u4e0d\u9075\u5faa\u5faa\u5e8f\u6f38\u9032\u7684\u908f\u8f2f\u63a8\u7406\uff0c\u9019\u4f7f\u5f97 CoT \u5728\u7406\u89e3\u8af7\u523a\u65b9\u9762\u7684\u6548\u679c\u4e0d\u5982\u5728\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u6548\u679c\u3002</paragraph>", "author": "Yazhou Zhang et.al.", "authors": "Yazhou Zhang, Chunwang Zou, Zheng Lian, Prayag Tiwari, Jing Qin", "id": "2408.11319v1", "paper_url": "http://arxiv.org/abs/2408.11319v1", "repo": "null"}}