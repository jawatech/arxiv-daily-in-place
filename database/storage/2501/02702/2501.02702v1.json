{"2501.02702": {"publish_time": "2025-01-06", "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance", "paper_summary": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics.", "paper_summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u5efa\u69cb\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u7cfb\u7d71\uff0c\u4ee5\u6539\u5584\u76ee\u6a19\u8a9e\u6599\u5eab\u4e2d\u7684\u554f\u7b54 (QA) \u4efb\u52d9\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u4eba\u985e\u8a9e\u8a00\u7684\u5206\u6790\u548c\u751f\u6210\u65b9\u5f0f\u3002\u9019\u4e9b\u6a21\u578b\u4f9d\u8cf4\u65bc\u9810\u5148\u8a13\u7df4\u7684\u8cc7\u6599\uff0c\u9664\u975e\u8207\u5373\u6642\u8cc7\u6599\u5de5\u5177\u6574\u5408\uff0c\u5426\u5247\u7f3a\u4e4f\u5373\u6642\u66f4\u65b0\u3002RAG \u900f\u904e\u6574\u5408\u7dda\u4e0a\u8cc7\u6e90\u548c\u8cc7\u6599\u5eab\uff0c\u589e\u5f37 LLM\uff0c\u4ee5\u7522\u751f\u7b26\u5408\u8108\u7d61\u7684\u56de\u61c9\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684 RAG \u5728\u8655\u7406\u5927\u91cf\u8cc7\u6599\u6642\u4ecd\u6703\u9047\u5230\u8cc7\u8a0a\u7a00\u91cb\u548c\u5e7b\u89ba\u7b49\u6311\u6230\u3002\u6211\u5011\u7684\u505a\u6cd5\u900f\u904e\u5c07\u8a9e\u6599\u5eab\u8f49\u63db\u6210\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u96c6\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u4e26\u5efa\u69cb RAG \u67b6\u69cb\uff0c\u4ee5\u5f9e\u76ee\u6a19\u6587\u4ef6\u7522\u751f\u56de\u61c9\u3002\u6211\u5011\u5f15\u5165\u4e86 QuIM-RAG\uff08\u554f\u984c\u5c0d\u554f\u984c\u53cd\u5411\u7d22\u5f15\u6bd4\u5c0d\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u6211\u5011\u7cfb\u7d71\u4e2d\u6aa2\u7d22\u6a5f\u5236\u7684\u5275\u65b0\u65b9\u6cd5\u3002\u6b64\u7b56\u7565\u5f9e\u6587\u4ef6\u5340\u584a\u4e2d\u7522\u751f\u6f5b\u5728\u554f\u984c\uff0c\u4e26\u5c07\u9019\u4e9b\u554f\u984c\u8207\u4f7f\u7528\u8005\u67e5\u8a62\u9032\u884c\u6bd4\u5c0d\uff0c\u4ee5\u627e\u51fa\u6700\u76f8\u95dc\u7684\u6587\u5b57\u5340\u584a\uff0c\u7528\u65bc\u7522\u751f\u6e96\u78ba\u7684\u7b54\u6848\u3002\u6211\u5011\u5df2\u5728 Meta Inc. \u958b\u6e90\u7684 Meta-LLaMA3-8B-instruct \u6a21\u578b\u4e0a\u5be6\u4f5c\u6211\u5011\u7684 RAG \u7cfb\u7d71\uff0c\u8a72\u6a21\u578b\u53ef\u5728 Hugging Face \u4e0a\u53d6\u5f97\u3002\u6211\u5011\u5f9e\u4e00\u500b\u6bcf\u65e5\u5b58\u53d6\u6578\u5343\u6b21\u7684\u6d41\u91cf\u6975\u9ad8\u7684\u7db2\u7ad9\u5efa\u69cb\u4e86\u4e00\u500b\u5305\u542b 500 \u591a\u9801\u7684\u5ba2\u88fd\u5316\u8a9e\u6599\u5eab\uff0c\u7528\u65bc\u56de\u7b54\u8907\u96dc\u7684\u554f\u984c\uff0c\u4e26\u624b\u52d5\u6e96\u5099\u4e86\u5730\u9762\u5be6\u6cc1 QA \u4ee5\u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u4f7f\u7528 BERT-Score \u548c RAGAS\uff08\u8a55\u4f30 LLM \u61c9\u7528\u7a0b\u5f0f\u7684\u6700\u65b0\u6307\u6a19\uff09\u5c07\u6211\u5011\u7684\u65b9\u6cd5\u8207\u50b3\u7d71\u7684 RAG \u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u7684\u8a55\u4f30\u8b49\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u5169\u500b\u6307\u6a19\u4e0a\u90fd\u512a\u65bc\u50b3\u7d71\u7684 RAG \u67b6\u69cb\u3002", "author": "Binita Saha et.al.", "authors": "Binita Saha, Utsha Saha, Muhammad Zubair Malik", "id": "2501.02702v1", "paper_url": "http://arxiv.org/abs/2501.02702v1", "repo": "null"}}