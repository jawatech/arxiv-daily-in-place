
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-11-25**|**Privacy-Preserving Federated Foundation Model for Generalist Ultrasound Artificial Intelligence**|Yuncheng Jiang et.al.|[2411.16380v1](http://arxiv.org/abs/2411.16380v1)|null|
|**2024-11-25**|**Med-PerSAM: One-Shot Visual Prompt Tuning for Personalized Segment Anything Model in Medical Domain**|Hangyul Yoon et.al.|[2411.16123v1](http://arxiv.org/abs/2411.16123v1)|null|
|**2024-11-25**|**Why the Agent Made that Decision: Explaining Deep Reinforcement Learning with Vision Masks**|Rui Zuo et.al.|[2411.16120v1](http://arxiv.org/abs/2411.16120v1)|null|
|**2024-11-24**|**DRIVE: Dual-Robustness via Information Variability and Entropic Consistency in Source-Free Unsupervised Domain Adaptation**|Ruiqiang Xiao et.al.|[2411.15976v1](http://arxiv.org/abs/2411.15976v1)|null|
|**2024-11-24**|**Medical Slice Transformer: Improved Diagnosis and Explainability on 3D Medical Images with DINOv2**|Gustav Müller-Franzes et.al.|[2411.15802v1](http://arxiv.org/abs/2411.15802v1)|[link](https://github.com/mueller-franzes/mst)|
|**2024-11-24**|**Enhancing the automatic segmentation and analysis of 3D liver vasculature models**|Yassine Machta et.al.|[2411.15778v1](http://arxiv.org/abs/2411.15778v1)|null|
|**2024-11-24**|**RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language Models on Dietary Supplements**|Zaifu Zhan et.al.|[2411.15700v1](http://arxiv.org/abs/2411.15700v1)|null|
|**2024-11-23**|**Ontology-Constrained Generation of Domain-Specific Clinical Summaries**|Gaya Mehenni et.al.|[2411.15666v1](http://arxiv.org/abs/2411.15666v1)|[link](https://github.com/lama-west/ontology-based-decoding_ekaw2024)|
|**2024-11-23**|**A Survey on LLM-as-a-Judge**|Jiawei Gu et.al.|[2411.15594v1](http://arxiv.org/abs/2411.15594v1)|null|
|**2024-11-23**|**Large Language Model with Region-guided Referring and Grounding for CT Report Generation**|Zhixuan Chen et.al.|[2411.15539v1](http://arxiv.org/abs/2411.15539v1)|null|
|**2024-11-23**|**GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning**|Yunlei Liang et.al.|[2411.15428v1](http://arxiv.org/abs/2411.15428v1)|[link](https://github.com/geods/region2vec-gat)|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396v1](http://arxiv.org/abs/2411.15396v1)|null|
|**2024-11-22**|**Regulator-Manufacturer AI Agents Modeling: Mathematical Feedback-Driven Multi-Agent LLM Framework**|Yu Han et.al.|[2411.15356v1](http://arxiv.org/abs/2411.15356v1)|null|
|**2024-11-22**|**Health AI Developer Foundations**|Atilla P. Kiraly et.al.|[2411.15128v1](http://arxiv.org/abs/2411.15128v1)|null|
|**2024-11-22**|**ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation**|Xiaoman Zhang et.al.|[2411.15122v1](http://arxiv.org/abs/2411.15122v1)|null|
|**2024-11-22**|**Feature-interactive Siamese graph encoder-based image analysis to predict STAS from histopathology images in lung cancer**|Liangrui Pan et.al.|[2411.15274v1](http://arxiv.org/abs/2411.15274v1)|null|
|**2024-11-22**|**Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot**|Linqi Lu et.al.|[2411.14925v1](http://arxiv.org/abs/2411.14925v1)|null|
|**2024-11-22**|**Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation**|Yuheng Xu et.al.|[2411.14883v1](http://arxiv.org/abs/2411.14883v1)|null|
|**2024-11-22**|**AI-Driven Real-Time Monitoring of Ground-Nesting Birds: A Case Study on Curlew Detection Using YOLOv10**|Carl Chalmers et.al.|[2411.15263v1](http://arxiv.org/abs/2411.15263v1)|null|
|**2024-11-22**|**Optimized Vessel Segmentation: A Structure-Agnostic Approach with Small Vessel Enhancement and Morphological Correction**|Dongning Song et.al.|[2411.15251v1](http://arxiv.org/abs/2411.15251v1)|null|
|**2024-11-22**|**Adversarial Prompt Distillation for Vision-Language Models**|Lin Luo et.al.|[2411.15244v1](http://arxiv.org/abs/2411.15244v1)|null|
|**2024-11-22**|**Is Attention All You Need For Actigraphy? Foundation Models of Wearable Accelerometer Data for Mental Health Research**|Franklin Y. Ruan et.al.|[2411.15240v1](http://arxiv.org/abs/2411.15240v1)|[link](https://github.com/njacobsonlab/pretrained-actigraphy-transformer)|
|**2024-11-21**|**Uterine Ultrasound Image Captioning Using Deep Learning Techniques**|Abdennour Boulesnane et.al.|[2411.14039v1](http://arxiv.org/abs/2411.14039v1)|null|
|**2024-11-21**|**Logic Augmented Generation**|Aldo Gangemi et.al.|[2411.14012v1](http://arxiv.org/abs/2411.14012v1)|null|
|**2024-11-21**|**AmpliNetECG12: A lightweight SoftMax-based relativistic amplitude amplification architecture for 12 lead ECG classification**|Shreya Srivastava et.al.|[2411.13903v1](http://arxiv.org/abs/2411.13903v1)|null|
|**2024-11-21**|**PIORS: Personalized Intelligent Outpatient Reception based on Large Language Model with Multi-Agents Medical Scenario Simulation**|Zhijie Bao et.al.|[2411.13902v1](http://arxiv.org/abs/2411.13902v1)|null|
|**2024-11-20**|**Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models**|Chanseo Lee et.al.|[2411.13518v1](http://arxiv.org/abs/2411.13518v1)|null|
|**2024-11-20**|**Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint**|Guangkun Nie et.al.|[2411.15216v1](http://arxiv.org/abs/2411.15216v1)|null|
|**2024-11-20**|**SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers**|Hojjat Karami et.al.|[2411.13428v1](http://arxiv.org/abs/2411.13428v1)|null|
|**2024-11-20**|**S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning**|Mingze Yin et.al.|[2411.15215v1](http://arxiv.org/abs/2411.15215v1)|null|
|**2024-11-20**|**Are Large Language Models Memorizing Bug Benchmarks?**|Daniel Ramos et.al.|[2411.13323v1](http://arxiv.org/abs/2411.13323v1)|null|
|**2024-11-20**|**Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training**|Ameera Bawazir et.al.|[2411.15207v1](http://arxiv.org/abs/2411.15207v1)|null|
|**2024-11-20**|**GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation**|Mengzhu Wang et.al.|[2411.13147v2](http://arxiv.org/abs/2411.13147v2)|null|
|**2024-11-20**|**Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine**|Yifan Yang et.al.|[2411.14487v1](http://arxiv.org/abs/2411.14487v1)|null|
|**2024-11-20**|**Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI**|Yaşar Utku Alçalar et.al.|[2411.13022v1](http://arxiv.org/abs/2411.13022v1)|null|
|**2024-11-20**|**Automating Sonologists USG Commands with AI and Voice Interface**|Emad Mohamed et.al.|[2411.13006v1](http://arxiv.org/abs/2411.13006v1)|null|
|**2024-11-20**|**DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback**|Mahsa Sheikholeslami et.al.|[2411.14157v1](http://arxiv.org/abs/2411.14157v1)|[link](https://github.com/mahsasheikh/DrugGen)|
|**2024-11-19**|**Deep Learning-Based Classification of Hyperkinetic Movement Disorders in Children**|Nandika Ramamurthy et.al.|[2411.15200v1](http://arxiv.org/abs/2411.15200v1)|null|
|**2024-11-19**|**Efficient Medicinal Image Transmission and Resolution Enhancement via GAN**|Rishabh Kumar Sharma et.al.|[2411.12833v1](http://arxiv.org/abs/2411.12833v1)|null|
|**2024-11-19**|**Conversational Medical AI: Ready for Practice**|Antoine Lizée et.al.|[2411.12808v1](http://arxiv.org/abs/2411.12808v1)|null|
|**2024-11-19**|**Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs**|Ahmed Akib Jawad Karim et.al.|[2411.12712v1](http://arxiv.org/abs/2411.12712v1)|null|
|**2024-11-19**|**AI Guided Early Screening of Cervical Cancer**|Dharanidharan S I et.al.|[2411.12681v1](http://arxiv.org/abs/2411.12681v1)|null|
|**2024-11-19**|**Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers**|Devakumar GR et.al.|[2411.12678v1](http://arxiv.org/abs/2411.12678v1)|null|
|**2024-11-19**|**DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Image Segmentation**|Bingli Wang et.al.|[2411.12350v1](http://arxiv.org/abs/2411.12350v1)|null|
|**2024-11-19**|**Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment**|Shuoling Liu et.al.|[2411.13599v1](http://arxiv.org/abs/2411.13599v1)|null|
|**2024-11-19**|**StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model**|Zongrong Li et.al.|[2411.14476v1](http://arxiv.org/abs/2411.14476v1)|null|
|**2024-11-19**|**Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification**|Mingsen Du et.al.|[2411.12222v1](http://arxiv.org/abs/2411.12222v1)|null|
|**2024-11-19**|**CCIS-Diff: A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis**|Yifan Xie et.al.|[2411.12198v1](http://arxiv.org/abs/2411.12198v1)|null|
|**2024-11-18**|**Leveraging Gene Expression Data and Explainable Machine Learning for Enhanced Early Detection of Type 2 Diabetes**|Aurora Lithe Roy et.al.|[2411.14471v1](http://arxiv.org/abs/2411.14471v1)|null|
|**2024-11-18**|**Medical Video Generation for Disease Progression Simulation**|Xu Cao et.al.|[2411.11943v1](http://arxiv.org/abs/2411.11943v1)|null|
|**2024-11-18**|**Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion**|Meng Zhou et.al.|[2411.11799v1](http://arxiv.org/abs/2411.11799v1)|[link](https://github.com/simonzhou86/en_dran)|
|**2024-11-18**|**Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care**|Jeffrey N. Clark et.al.|[2411.11774v1](http://arxiv.org/abs/2411.11774v1)|null|
|**2024-11-18**|**SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation**|Shiman Li et.al.|[2411.11636v1](http://arxiv.org/abs/2411.11636v1)|null|
|**2024-11-18**|**HistoEncoder: a digital pathology foundation model for prostate cancer**|Joona Pohjonen et.al.|[2411.11458v2](http://arxiv.org/abs/2411.11458v2)|[link](https://github.com/jopo666/HistoEncoder)|
|**2024-11-18**|**TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation**|Ranmin Wang et.al.|[2411.11305v2](http://arxiv.org/abs/2411.11305v2)|null|
|**2024-11-18**|**Deep learning waterways for rural infrastructure development**|Matthew Pierson et.al.|[2411.13590v1](http://arxiv.org/abs/2411.13590v1)|null|
|**2024-11-18**|**Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development**|Ranjan Sapkota et.al.|[2411.11285v1](http://arxiv.org/abs/2411.11285v1)|null|
|**2024-11-18**|**Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction**|Yucong Meng et.al.|[2411.11282v1](http://arxiv.org/abs/2411.11282v1)|null|
|**2024-11-17**|**F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics**|Pramit Saha et.al.|[2411.11912v1](http://arxiv.org/abs/2411.11912v1)|null|
|**2024-11-17**|**MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records**|Eric Yang et.al.|[2411.11161v1](http://arxiv.org/abs/2411.11161v1)|null|
|**2024-11-17**|**Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks**|Deepa Anand et.al.|[2411.11105v1](http://arxiv.org/abs/2411.11105v1)|null|
|**2024-11-17**|**BianCang: A Traditional Chinese Medicine Large Language Model**|Sibo Wei et.al.|[2411.11027v1](http://arxiv.org/abs/2411.11027v1)|[link](https://github.com/qlu-nlp/biancang)|
|**2024-11-16**|**MpoxVLM: A Vision-Language Model for Diagnosing Skin Lesions from Mpox Virus Infection**|Xu Cao et.al.|[2411.10888v1](http://arxiv.org/abs/2411.10888v1)|[link](https://github.com/IrohXu/MpoxVLM)|
|**2024-11-16**|**Towards Next-Generation Medical Agent: How o1 is Reshaping Decision-Making in Medical Scenarios**|Shaochen Xu et.al.|[2411.14461v1](http://arxiv.org/abs/2411.14461v1)|null|
|**2024-11-16**|**A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks**|Pandiyaraju V et.al.|[2411.10843v1](http://arxiv.org/abs/2411.10843v1)|null|
|**2024-11-16**|**Decentralizing Test-time Adaptation under Heterogeneous Data Streams**|Zixian Su et.al.|[2411.15173v1](http://arxiv.org/abs/2411.15173v1)|null|
|**2024-11-16**|**MRI Parameter Mapping via Gaussian Mixture VAE: Breaking the Assumption of Independent Pixels**|Moucheng Xu et.al.|[2411.10772v1](http://arxiv.org/abs/2411.10772v1)|[link](https://github.com/moucheng2017/MRI-GMM-VAE)|
|**2024-11-16**|**Integrated Machine Learning and Survival Analysis Modeling for Enhanced Chronic Kidney Disease Risk Stratification**|Zachary Dana et.al.|[2411.10754v1](http://arxiv.org/abs/2411.10754v1)|null|
|**2024-11-16**|**LTCXNet: Advancing Chest X-Ray Analysis with Solutions for Long-Tailed Multi-Label Classification and Fairness Challenges**|Chin-Wei Huang et.al.|[2411.10746v1](http://arxiv.org/abs/2411.10746v1)|null|
|**2024-11-15**|**Can Artificial Intelligence Generate Quality Research Topics Reflecting Patient Concerns?**|Jiyeong Kim et.al.|[2411.14456v1](http://arxiv.org/abs/2411.14456v1)|null|
|**2024-11-15**|**Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment**|Andrew Konya et.al.|[2411.10534v1](http://arxiv.org/abs/2411.10534v1)|null|
|**2024-11-15**|**Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization**|Fatahlla Moreh et.al.|[2411.10389v1](http://arxiv.org/abs/2411.10389v1)|null|
|**2024-11-15**|**Artificial Intelligence in Pediatric Echocardiography: Exploring Challenges, Opportunities, and Clinical Applications with Explainable AI and Federated Learning**|Mohammed Yaseen Jabarulla et.al.|[2411.10255v1](http://arxiv.org/abs/2411.10255v1)|null|
|**2024-11-15**|**FedCL-Ensemble Learning: A Framework of Federated Continual Learning with Ensemble Transfer Learning Enhanced for Alzheimer's MRI Classifications while Preserving Privacy**|Rishit Kapoor et.al.|[2411.12756v1](http://arxiv.org/abs/2411.12756v1)|null|
|**2024-11-15**|**Evaluating the role of `Constitutions' for learning from AI feedback**|Saskia Redgate et.al.|[2411.10168v1](http://arxiv.org/abs/2411.10168v1)|null|
|**2024-11-15**|**PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse**|Einari Vaaras et.al.|[2411.10087v1](http://arxiv.org/abs/2411.10087v1)|[link](https://github.com/SPEECHCOG/PFML)|
|**2024-11-15**|**Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion**|Dan He et.al.|[2411.10036v1](http://arxiv.org/abs/2411.10036v1)|null|
|**2024-11-15**|**JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging**|Kaito Baba et.al.|[2411.09933v1](http://arxiv.org/abs/2411.09933v1)|null|
|**2024-11-15**|**A Hybrid Artificial Intelligence System for Automated EEG Background Analysis and Report Generation**|Chin-Sung Tung et.al.|[2411.09874v1](http://arxiv.org/abs/2411.09874v1)|[link](https://github.com/tcs211/ai_eeeg_report)|
|**2024-11-14**|**A Benchmark for Long-Form Medical Question Answering**|Pedram Hosseini et.al.|[2411.09834v2](http://arxiv.org/abs/2411.09834v2)|null|
|**2024-11-14**|**A Self-Supervised Model for Multi-modal Stroke Risk Prediction**|Camille Delgrange et.al.|[2411.09822v1](http://arxiv.org/abs/2411.09822v1)|[link](https://github.com/CamilleDelgrange/SSMSRPM)|
|**2024-11-14**|**Deep Learning for Fetal Inflammatory Response Diagnosis in the Umbilical Cord**|Marina A. Ayad et.al.|[2411.09767v1](http://arxiv.org/abs/2411.09767v1)|null|
|**2024-11-14**|**Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information**|Ahan Bhatt et.al.|[2411.09648v1](http://arxiv.org/abs/2411.09648v1)|null|
|**2024-11-14**|**An Explainable Attention Model for Cervical Precancer Risk Classification using Colposcopic Images**|Smith K. Khare et.al.|[2411.09469v1](http://arxiv.org/abs/2411.09469v1)|null|
|**2024-11-14**|**Script-centric behavior understanding for assisted autism spectrum disorder diagnosis**|Wenxing Liu et.al.|[2411.09413v1](http://arxiv.org/abs/2411.09413v1)|null|
|**2024-11-14**|**NFRs in Medical Imaging**|Amanda Vallentin et.al.|[2411.09718v1](http://arxiv.org/abs/2411.09718v1)|null|
|**2024-11-14**|**Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering**|Nghia Trung Ngo et.al.|[2411.09213v1](http://arxiv.org/abs/2411.09213v1)|null|
|**2024-11-14**|**Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance**|Md Fahim Anjum et.al.|[2411.09174v1](http://arxiv.org/abs/2411.09174v1)|null|
|**2024-11-14**|**Artificial Intelligence for Infectious Disease Prediction and Prevention: A Comprehensive Review**|Selestine Melchane et.al.|[2411.10486v1](http://arxiv.org/abs/2411.10486v1)|null|
|**2024-11-13**|**The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models**|Daniel P. Jeong et.al.|[2411.08870v1](http://arxiv.org/abs/2411.08870v1)|null|
|**2024-11-13**|**MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification**|Shan Cong et.al.|[2411.08703v1](http://arxiv.org/abs/2411.08703v1)|null|
|**2024-11-13**|**TRACE: Transformer-based Risk Assessment for Clinical Evaluation**|Dionysis Christopoulos et.al.|[2411.08701v1](http://arxiv.org/abs/2411.08701v1)|null|
|**2024-11-13**|**Rethinking negative sampling in content-based news recommendation**|Miguel Ângelo Rebelo et.al.|[2411.08700v1](http://arxiv.org/abs/2411.08700v1)|null|
|**2024-11-13**|**A Survey on Vision Autoregressive Model**|Kai Jiang et.al.|[2411.08666v2](http://arxiv.org/abs/2411.08666v2)|null|
|**2024-11-13**|**Optimizing Automatic Summarization of Long Clinical Records Using Dynamic Context Extension:Testing and Evaluation of the NBCE Method**|Guoqing Zhang et.al.|[2411.08586v2](http://arxiv.org/abs/2411.08586v2)|null|
|**2024-11-13**|**A Heterogeneous Graph Neural Network Fusing Functional and Structural Connectivity for MCI Diagnosis**|Feiyu Yin et.al.|[2411.08424v1](http://arxiv.org/abs/2411.08424v1)|null|
|**2024-11-13**|**A Fuzzy Reinforcement LSTM-based Long-term Prediction Model for Fault Conditions in Nuclear Power Plants**|Siwei Li et.al.|[2411.08370v1](http://arxiv.org/abs/2411.08370v1)|null|
|**2024-11-13**|**TowerDebias: A Novel Debiasing Method based on the Tower Property**|Norman Matloff et.al.|[2411.08297v1](http://arxiv.org/abs/2411.08297v1)|null|
|**2024-11-12**|**Scaling Properties of Diffusion Models for Perceptual Tasks**|Rahul Ravishankar et.al.|[2411.08034v3](http://arxiv.org/abs/2411.08034v3)|null|
|**2024-11-12**|**Investigating the Effectiveness of Explainability Methods in Parkinson's Detection from Speech**|Eleonora Mancini et.al.|[2411.08013v2](http://arxiv.org/abs/2411.08013v2)|null|

#### Abstracts
##### **Privacy-Preserving Federated Foundation Model for Generalist Ultrasound Artificial Intelligence**
2411.16380v1 by Yuncheng Jiang, Chun-Mei Feng, Jinke Ren, Jun Wei, Zixun Zhang, Yiwen Hu, Yunbi Liu, Rui Sun, Xuemei Tang, Juan Du, Xiang Wan, Yong Xu, Bo Du, Xin Gao, Guangyu Wang, Shaohua Zhou, Shuguang Cui, Rick Siow Mong Goh, Yong Liu, Zhen Li

Ultrasound imaging is widely used in clinical diagnosis due to its
non-invasive nature and real-time capabilities. However, conventional
ultrasound diagnostics face several limitations, including high dependence on
physician expertise and suboptimal image quality, which complicates
interpretation and increases the likelihood of diagnostic errors. Artificial
intelligence (AI) has emerged as a promising solution to enhance clinical
diagnosis, particularly in detecting abnormalities across various biomedical
imaging modalities. Nonetheless, current AI models for ultrasound imaging face
critical challenges. First, these models often require large volumes of labeled
medical data, raising concerns over patient privacy breaches. Second, most
existing models are task-specific, which restricts their broader clinical
utility. To overcome these challenges, we present UltraFedFM, an innovative
privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively
pre-trained using federated learning across 16 distributed medical institutions
in 9 countries, leveraging a dataset of over 1 million ultrasound images
covering 19 organs and 10 ultrasound modalities. This extensive and diverse
data, combined with a secure training framework, enables UltraFedFM to exhibit
strong generalization and diagnostic capabilities. It achieves an average area
under the receiver operating characteristic curve of 0.927 for disease
diagnosis and a dice similarity coefficient of 0.878 for lesion segmentation.
Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level
ultrasonographers and matches the performance of expert-level sonographers in
the joint diagnosis of 8 common systemic diseases. These findings indicate that
UltraFedFM can significantly enhance clinical diagnostics while safeguarding
patient privacy, marking an advancement in AI-driven ultrasound imaging for
future clinical applications.

摘要：超音波影像因其非侵入性與即時性廣泛應用於臨床診斷。然而，傳統超音波診斷面臨數項限制，包括高度依賴醫師專業知識和次佳影像品質，這使得影像判讀更為複雜，並增加診斷錯誤的可能性。人工智慧 (AI) 已成為增強臨床診斷的潛在解決方案，特別是在偵測各種生物醫學影像模式中的異常。儘管如此，目前用於超音波影像的 AI 模型面臨嚴峻挑戰。首先，這些模型通常需要大量的標籤醫學資料，這引發了對病患隱私遭侵犯的疑慮。其次，現有的大部分模型都是針對特定任務而設計，這限制了它們在更廣泛的臨床應用。為了解決這些挑戰，我們提出了 UltraFedFM，一個創新的隱私保護超音波基礎模型。UltraFedFM 透過 9 個國家/地區的 16 個分散式醫療機構的聯合學習進行協作預訓練，利用包含超過 100 萬張超音波影像的資料集，涵蓋 19 個器官和 10 種超音波模式。這些廣泛且多樣化的資料，結合安全的訓練架構，使 UltraFedFM 能夠展現強大的概化和診斷能力。在疾病診斷方面，其受試者工作特徵曲線下的平均面積達到 0.927，在病灶分割方面，其 Dice 相似係數為 0.878。值得注意的是，UltraFedFM 超越了中階超音波檢查員的診斷準確性，並在 8 種常見全身性疾病的聯合診斷中達到專家級超音波檢查員的水準。這些發現表明，UltraFedFM 可以顯著增強臨床診斷，同時保護病患隱私，這標誌著 AI 驅動超音波影像在未來臨床應用中的一項進步。

##### **Med-PerSAM: One-Shot Visual Prompt Tuning for Personalized Segment Anything Model in Medical Domain**
2411.16123v1 by Hangyul Yoon, Doohyuk Jang, Jungeun Kim, Eunho Yang

Leveraging pre-trained models with tailored prompts for in-context learning
has proven highly effective in NLP tasks. Building on this success, recent
studies have applied a similar approach to the Segment Anything Model (SAM)
within a ``one-shot" framework, where only a single reference image and its
label are employed. However, these methods face limitations in the medical
domain, primarily due to SAM's essential requirement for visual prompts and the
over-reliance on pixel similarity for generating them. This dependency may lead
to (1) inaccurate prompt generation and (2) clustering of point prompts,
resulting in suboptimal outcomes. To address these challenges, we introduce
\textbf{Med-PerSAM}, a novel and straightforward one-shot framework designed
for the medical domain. Med-PerSAM uses only visual prompt engineering and
eliminates the need for additional training of the pretrained SAM or human
intervention, owing to our novel automated prompt generation process. By
integrating our lightweight warping-based prompt tuning model with SAM, we
enable the extraction and iterative refinement of visual prompts, enhancing the
performance of the pre-trained SAM. This advancement is particularly meaningful
in the medical domain, where creating visual prompts poses notable challenges
for individuals lacking medical expertise. Our model outperforms various
foundational models and previous SAM-based approaches across diverse 2D medical
imaging datasets.

摘要：利用預先訓練的模型，並針對特定提示進行情境學習，已證明在自然語言處理任務中非常有效。在此成功基礎上，最近的研究已將類似方法應用於「片段任何模型」(SAM)，採用「一次性」架構，其中僅使用單一參考影像及其標籤。然而，這些方法在醫療領域面臨限制，主要是由於 SAM 對視覺提示的基本需求，以及過度依賴像素相似性來產生它們。這種依賴性可能會導致 (1) 提示產生不準確，以及 (2) 點提示群集，導致結果次佳。為了應對這些挑戰，我們引入了 \textbf{Med-PerSAM}，這是一個專為醫療領域設計的新穎且直接的一次性架構。Med-PerSAM 僅使用視覺提示工程，並消除了對預訓練 SAM 或人為干預的額外訓練需求，這要歸功於我們新穎的自動化提示產生流程。透過將我們輕量級基於變形的提示調整模型與 SAM 整合，我們能夠提取和反覆改善視覺提示，增強預訓練 SAM 的效能。這項進展在醫療領域特別有意義，因為對於缺乏醫療專業知識的人來說，建立視覺提示會構成顯著的挑戰。我們的模型在各種 2D 醫學影像資料集上優於各種基礎模型和先前的基於 SAM 的方法。

##### **Why the Agent Made that Decision: Explaining Deep Reinforcement Learning with Vision Masks**
2411.16120v1 by Rui Zuo, Zifan Wang, Simon Khan, Garrett Ethan Katz, Qinru Qiu

Due to the inherent lack of transparency in deep neural networks, it is
challenging for deep reinforcement learning (DRL) agents to gain trust and
acceptance from users, especially in safety-critical applications such as
medical diagnosis and military operations. Existing methods for explaining an
agent's decision either require to retrain the agent using models that support
explanation generation or rely on perturbation-based techniques to reveal the
significance of different input features in the decision making process.
However, retraining the agent may compromise its integrity and performance,
while perturbation-based methods have limited performance and lack knowledge
accumulation or learning capabilities. Moreover, since each perturbation is
performed independently, the joint state of the perturbed inputs may not be
physically meaningful. To address these challenges, we introduce
$\textbf{VisionMask}$, a standalone explanation model trained end-to-end to
identify the most critical regions in the agent's visual input that can explain
its actions. VisionMask is trained in a self-supervised manner without relying
on human-generated labels. Importantly, its training does not alter the agent
model, hence preserving the agent's performance and integrity. We evaluate
VisionMask on Super Mario Bros (SMB) and three Atari games. Compared to
existing methods, VisionMask achieves a 14.9% higher insertion accuracy and a
30.08% higher F1-Score in reproducing original actions from the selected visual
explanations. We also present examples illustrating how VisionMask can be used
for counterfactual analysis.

摘要：<paragraph>由於深度神經網路缺乏透明度，深度強化學習 (DRL) 代理程式要獲得使用者的信任和認可是一項挑戰，特別是在安全關鍵的應用程式中，例如醫療診斷和軍事行動。現有的方法用於解釋代理程式的決策，需要使用支援解釋產生的模型重新訓練代理程式，或依賴於基於擾動的技術來揭示不同輸入特徵在決策制定過程中的重要性。然而，重新訓練代理程式可能會損害其完整性和效能，而基於擾動的方法效能有限，且缺乏知識累積或學習能力。此外，由於每個擾動都是獨立執行的，因此擾動輸入的聯合狀態可能沒有實際意義。為了應對這些挑戰，我們引入了 $\textbf{VisionMask}$，這是一個獨立的解釋模型，經過端對端的訓練，以識別代理程式視覺輸入中最關鍵的區域，這些區域可以解釋其動作。VisionMask 以自監督的方式進行訓練，而不依賴於人為產生的標籤。重要的是，其訓練不會改變代理程式模型，因此可以保留代理程式的效能和完整性。我們在 Super Mario Bros (SMB) 和三款 Atari 遊戲上評估了 VisionMask。與現有方法相比，VisionMask 在根據所選的視覺解釋複製原始動作時，插入準確率提高了 14.9%，F1 分數提高了 30.08%。我們還提供了範例來說明如何使用 VisionMask 進行反事實分析。</paragraph>

##### **DRIVE: Dual-Robustness via Information Variability and Entropic Consistency in Source-Free Unsupervised Domain Adaptation**
2411.15976v1 by Ruiqiang Xiao, Songning Lai, Yijun Yang, Jiemin Wu, Yutao Yue, Lei Zhu

Adapting machine learning models to new domains without labeled data,
especially when source data is inaccessible, is a critical challenge in
applications like medical imaging, autonomous driving, and remote sensing. This
task, known as Source-Free Unsupervised Domain Adaptation (SFUDA), involves
adapting a pre-trained model to a target domain using only unlabeled target
data, which can lead to issues such as overfitting, underfitting, and poor
generalization due to domain discrepancies and noise. Existing SFUDA methods
often rely on single-model architectures, struggling with uncertainty and
variability in the target domain. To address these challenges, we propose DRIVE
(Dual-Robustness through Information Variability and Entropy), a novel SFUDA
framework leveraging a dual-model architecture. The two models, initialized
with identical weights, work in parallel to capture diverse target domain
characteristics. One model is exposed to perturbations via projection gradient
descent (PGD) guided by mutual information, focusing on high-uncertainty
regions. We also introduce an entropy-aware pseudo-labeling strategy that
adjusts label weights based on prediction uncertainty, ensuring the model
focuses on reliable data while avoiding noisy regions. The adaptation process
has two stages: the first aligns the models on stable features using a mutual
information consistency loss, and the second dynamically adjusts the
perturbation level based on the loss from the first stage, encouraging the
model to explore a broader range of the target domain while preserving existing
performance. This enhances generalization capabilities and robustness against
interference. Evaluations on standard SFUDA benchmarks show that DRIVE
consistently outperforms previous methods, delivering improved adaptation
accuracy and stability across complex target domains.

摘要：<paragraph>在沒有標籤資料的情況下將機器學習模型調整到新的領域，特別是在無法取得原始資料時，是醫療影像、自動駕駛和遙測等應用中的一項關鍵挑戰。這項任務稱為無來源非監督領域適應 (SFUDA)，涉及使用僅有的未標籤目標資料將預先訓練的模型調整到目標領域，這可能會導致過度擬合、欠擬合和因領域差異和雜訊而導致的概化不良等問題。現有的 SFUDA 方法通常依賴於單一模型架構，難以應對目標領域中的不確定性和變異性。為了應對這些挑戰，我們提出了 DRIVE（透過資訊變異性和熵的雙重穩健性），一種利用雙模型架構的新穎 SFUDA 架構。這兩個模型以相同的權重初始化，並行工作以擷取不同的目標領域特徵。其中一個模型透過由互資訊引導的投影梯度下降 (PGD) 暴露於擾動，重點在於高度不確定的區域。我們還引入了一種熵感知偽標籤策略，該策略根據預測不確定性調整標籤權重，確保模型專注於可靠的資料，同時避免雜訊區域。適應過程分為兩個階段：第一個階段使用互資訊一致性損失在穩定特徵上對齊模型，第二個階段根據第一個階段的損失動態調整擾動級別，鼓勵模型探索目標領域的更廣泛範圍，同時保留現有的效能。這增強了概化能力和對干擾的穩健性。在標準 SFUDA 基準上的評估顯示，DRIVE 持續優於先前的各種方法，在複雜的目標領域中提供改善的適應準確性和穩定性。</paragraph>

##### **Medical Slice Transformer: Improved Diagnosis and Explainability on 3D Medical Images with DINOv2**
2411.15802v1 by Gustav Müller-Franzes, Firas Khader, Robert Siepmann, Tianyu Han, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn

MRI and CT are essential clinical cross-sectional imaging techniques for
diagnosing complex conditions. However, large 3D datasets with annotations for
deep learning are scarce. While methods like DINOv2 are encouraging for 2D
image analysis, these methods have not been applied to 3D medical images.
Furthermore, deep learning models often lack explainability due to their
"black-box" nature. This study aims to extend 2D self-supervised models,
specifically DINOv2, to 3D medical imaging while evaluating their potential for
explainable outcomes. We introduce the Medical Slice Transformer (MST)
framework to adapt 2D self-supervised models for 3D medical image analysis. MST
combines a Transformer architecture with a 2D feature extractor, i.e., DINOv2.
We evaluate its diagnostic performance against a 3D convolutional neural
network (3D ResNet) across three clinical datasets: breast MRI (651 patients),
chest CT (722 patients), and knee MRI (1199 patients). Both methods were tested
for diagnosing breast cancer, predicting lung nodule dignity, and detecting
meniscus tears. Diagnostic performance was assessed by calculating the Area
Under the Receiver Operating Characteristic Curve (AUC). Explainability was
evaluated through a radiologist's qualitative comparison of saliency maps based
on slice and lesion correctness. P-values were calculated using Delong's test.
MST achieved higher AUC values compared to ResNet across all three datasets:
breast (0.94$\pm$0.01 vs. 0.91$\pm$0.02, P=0.02), chest (0.95$\pm$0.01 vs.
0.92$\pm$0.02, P=0.13), and knee (0.85$\pm$0.04 vs. 0.69$\pm$0.05, P=0.001).
Saliency maps were consistently more precise and anatomically correct for MST
than for ResNet. Self-supervised 2D models like DINOv2 can be effectively
adapted for 3D medical imaging using MST, offering enhanced diagnostic accuracy
and explainability compared to convolutional neural networks.

摘要：<paragraph>MRI 和 CT 是诊断复杂疾病的重要临床横断面成像技术。然而，用于深度学习的大型 3D 数据集和注释却很稀缺。虽然诸如 DINOv2 之类的方法对 2D 图像分析很有帮助，但这些方法尚未应用于 3D 医学图像。此外，深度学习模型通常缺乏可解释性，因为它们具有“黑匣子”的性质。本研究旨在将 2D 自监督模型（特别是 DINOv2）扩展到 3D 医学成像，同时评估其对可解释结果的潜力。我们引入了医学切片转换器 (MST) 框架，以将 2D 自监督模型用于 3D 医学图像分析。MST 将 Transformer 架构与 2D 特征提取器（即 DINOv2）相结合。我们评估了其针对三个临床数据集（乳腺 MRI（651 名患者）、胸部 CT（722 名患者）和膝部 MRI（1199 名患者））的诊断性能，与 3D 卷积神经网络 (3D ResNet) 进行了对比。两种方法均经过测试，用于诊断乳腺癌、预测肺结节性质和检测半月板撕裂。通过计算受试者工作特征曲线下面积 (AUC) 来评估诊断性能。可解释性通过放射科医生对基于切片和病变正确性的显着性图的定性比较来评估。P 值使用 Delong 的检验计算。与所有三个数据集中的 ResNet 相比，MST 获得了更高的 AUC 值：乳腺（0.94±0.01 vs. 0.91±0.02，P=0.02）、胸部（0.95±0.01 vs. 0.92±0.02，P=0.13）和膝部（0.85±0.04 vs. 0.69±0.05，P=0.001）。与 ResNet 相比，MST 的显着性图始终更加精确且解剖学上更正确。诸如 DINOv2 之类的自监督 2D 模型可以使用 MST 有效地适应 3D 医学成像，与卷积神经网络相比，提供了增强的诊断准确性和可解释性。</paragraph>

##### **Enhancing the automatic segmentation and analysis of 3D liver vasculature models**
2411.15778v1 by Yassine Machta, Omar Ali, Kevin Hakkakian, Ana Vlascenau, Amaury Facque, Nicolas Golse, Irene Vignon-Clementel

Surgical assessment of liver cancer patients requires identification of the
vessel trees from medical images. Specifically, the venous trees - the portal
(perfusing) and the hepatic (draining) trees are important for understanding
the liver anatomy and disease state, and perform surgery planning. This
research aims to improve the 3D segmentation, skeletonization, and subsequent
analysis of vessel trees, by creating an automatic pipeline based on deep
learning and image processing techniques.
  The first part of this work explores the impact of differentiable
skeletonization methods such as ClDice and morphological skeletonization loss,
on the overall liver vessel segmentation performance. To this aim, it studies
how to improve vessel tree connectivity.
  The second part of this study converts a single class vessel segmentation
into multi-class ones, separating the two venous trees. It builds on the
previous two-class vessel segmentation model, which vessel tree outputs might
be entangled, and on connected components and skeleton analyses of the trees.
  After providing sub-labeling of the specific anatomical branches of each
venous tree, these algorithms also enable a morphometric analysis of the vessel
trees by extracting various geometrical markers.
  In conclusion, we propose a method that successfully improves current
skeletonization methods, for extensive vascular trees that contain vessels of
different calibers. The separation algorithm creates a clean multi-class
segmentation of the vessels, validated by surgeons to provide low error. A new,
publicly shared high-quality liver vessel dataset of 77 cases is thus created.
Finally a method to annotate vessel trees according to anatomy is provided,
enabling a unique liver vessel morphometry analysis.

摘要：肝癌病患的外科評估需要從醫學影像中辨識血管樹。具體來說，靜脈樹 - 門靜脈（灌流）和肝靜脈（引流）樹對於了解肝臟解剖結構和疾病狀態以及進行手術規劃非常重要。這項研究旨在透過建立基於深度學習和影像處理技術的自動化流程，改善血管樹的 3D 分割、骨架化和後續分析。
這項工作的首要部分探討可微分骨架化方法（例如 ClDice 和形態骨架化損失）對整體肝臟血管分割效能的影響。為此，它研究如何改善血管樹連通性。
這項研究的第二部分將單一類別的血管分割轉換為多類別，以區分兩個靜脈樹。它建立在前一個雙類別血管分割模型的基礎上，該模型的血管樹輸出可能糾纏在一起，並建立在樹的連通元件和骨架分析的基礎上。
在對每個靜脈樹的特定解剖分支進行次標記後，這些演算法還能透過萃取各種幾何標記來對血管樹進行形態計量分析。
結論來說，我們提出了一種方法，可成功改善目前的骨架化方法，以適用於包含不同口徑血管的廣泛血管樹。分離演算法會建立血管的乾淨多類別分割，經由外科醫生驗證以提供低誤差。因此建立了一個新的、公開共享的高品質 77 個病例肝臟血管資料集。最後提供了一種根據解剖結構註解血管樹的方法，可進行獨特的肝臟血管形態計量分析。

##### **RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language Models on Dietary Supplements**
2411.15700v1 by Zaifu Zhan, Shuang Zhou, Mingchen Li, Rui Zhang

\textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.

摘要：<paragraph>**目的：**我們旨在開發一個先進的多任務大型語言模型 (LLM) 架構，從臨床記錄中提取多種類型的膳食補充品 (DS) 資訊。
**方法：**我們使用了四項核心 DS 資訊提取任務，即命名實體識別 (NER：2,949 個臨床句子)、關係提取 (RE：4,892 個句子)、三元組提取 (TE：2,949 個句子) 和使用分類 (UC：2,460 個句子) 作為我們的多任務。我們引入了一個新的檢索增強多任務資訊提取 (RAMIE) 架構，包括：1) 使用任務特定提示的指令微調技術，2) 以更高的儲存效率和更低的訓練成本訓練多任務的 LLM，以及 3) 通過從訓練集中檢索類似範例，整合檢索增強生成 (RAG) 技術。我們將 RAMIE 的效能與僅使用指令微調的 LLM 進行比較，並進行消融研究，評估多任務學習和 RAG 對改善多任務效能的貢獻。
**結果：**在 RAMIE 架構的幫助下，Llama2-13B 在 NER 任務上取得了 87.39 的 F1 分數（提升了 3.51%），並在 RE 任務上表現出色，F1 分數為 93.74（提升了 1.15%）。對於 TE 任務，Llama2-7B 得分為 79.45（提升了 14.26%），而 MedAlpaca-7B 在 UC 任務上取得了最高的 F1 分數 93.45（提升了 0.94%）。消融研究表明，儘管 MTL 以略微犧牲效能為代價提高了效率，但 RAG 顯著提升了整體準確度。
**結論：**本研究提出了一個新的 RAMIE 架構，展示了從臨床記錄中提取與 DS 相關資料的多任務資訊的顯著改進。我們的架構有可能應用於其他領域。</paragraph>

##### **Ontology-Constrained Generation of Domain-Specific Clinical Summaries**
2411.15666v1 by Gaya Mehenni, Amal Zouaq

Large Language Models (LLMs) offer promising solutions for text
summarization. However, some domains require specific information to be
available in the summaries. Generating these domain-adapted summaries is still
an open challenge. Similarly, hallucinations in generated content is a major
drawback of current approaches, preventing their deployment. This study
proposes a novel approach that leverages ontologies to create domain-adapted
summaries both structured and unstructured. We employ an ontology-guided
constrained decoding process to reduce hallucinations while improving
relevance. When applied to the medical domain, our method shows potential in
summarizing Electronic Health Records (EHRs) across different specialties,
allowing doctors to focus on the most relevant information to their domain.
Evaluation on the MIMIC-III dataset demonstrates improvements in generating
domain-adapted summaries of clinical notes and hallucination reduction.

摘要：大型語言模型 (LLM) 為文字摘要提供了有前景的解決方案。然而，某些領域需要摘要中提供特定資訊。產生這些領域適應型摘要仍然是一項公開挑戰。同樣地，產生式內容中的幻覺是當前方法的一大缺點，阻礙了它們的部署。本研究提出了一種新穎的方法，利用本体論來建立結構化和非結構化的領域適應型摘要。我們採用本体論引導約束式解碼程序，以減少幻覺，同時提高相關性。當應用於醫學領域時，我們的這項方法顯示了跨不同專業領域摘要電子健康記錄 (EHR) 的潛力，讓醫生可以專注於與其領域最相關的資訊。在 MIMIC-III 資料集上的評估證明，在產生臨床筆記的領域適應型摘要和減少幻覺方面都有所改進。

##### **A Survey on LLM-as-a-Judge**
2411.15594v1 by Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, Jian Guo

Accurate and consistent evaluation is crucial for decision-making across
numerous fields, yet it remains a challenging task due to inherent
subjectivity, variability, and scale. Large Language Models (LLMs) have
achieved remarkable success across diverse domains, leading to the emergence of
"LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With
their ability to process diverse data types and provide scalable,
cost-effective, and consistent assessments, LLMs present a compelling
alternative to traditional expert-driven evaluations. However, ensuring the
reliability of LLM-as-a-Judge systems remains a significant challenge that
requires careful design and standardization. This paper provides a
comprehensive survey of LLM-as-a-Judge, addressing the core question: How can
reliable LLM-as-a-Judge systems be built? We explore strategies to enhance
reliability, including improving consistency, mitigating biases, and adapting
to diverse assessment scenarios. Additionally, we propose methodologies for
evaluating the reliability of LLM-as-a-Judge systems, supported by a novel
benchmark designed for this purpose. To advance the development and real-world
deployment of LLM-as-a-Judge systems, we also discussed practical applications,
challenges, and future directions. This survey serves as a foundational
reference for researchers and practitioners in this rapidly evolving field.

摘要：<paragraph>準確且一致的評估對於各領域的決策制定至關重要，但由於固有的主觀性、變異性和規模，這仍然是一項具有挑戰性的任務。大型語言模型 (LLM) 已在不同領域取得顯著成功，導致「LLM 作為評審」的出現，其中 LLM 被用作複雜任務的評估者。憑藉處理各種數據類型並提供可擴充、經濟高效且一致的評估的能力，LLM 為傳統的專家驅動評估提供了令人信服的替代方案。然而，確保 LLM 作為評審系統的可靠性仍然是一個重大挑戰，需要仔細設計和標準化。本文對 LLM 作為評審進行了全面的調查，解決了核心問題：如何構建可靠的 LLM 作為評審系統？我們探討了提高可靠性的策略，包括提高一致性、減輕偏差和適應不同的評估場景。此外，我們提出了評估 LLM 作為評審系統可靠性的方法，並由為此目的設計的新基準測試提供支持。為了推進 LLM 作為評審系統的開發和實際部署，我們還討論了實際應用、挑戰和未來方向。本調查作為該快速發展領域的研究人員和從業人員的基本參考。</paragraph>

##### **Large Language Model with Region-guided Referring and Grounding for CT Report Generation**
2411.15539v1 by Zhixuan Chen, Yequan Bie, Haibo Jin, Hao Chen

Computed tomography (CT) report generation is crucial to assist radiologists
in interpreting CT volumes, which can be time-consuming and labor-intensive.
Existing methods primarily only consider the global features of the entire
volume, making it struggle to focus on specific regions and potentially missing
abnormalities. To address this issue, we propose Reg2RG, the first
region-guided referring and grounding framework for CT report generation, which
enhances diagnostic performance by focusing on anatomical regions within the
volume. Specifically, we utilize masks from a universal segmentation module to
capture local features for each referring region. A local feature decoupling
(LFD) strategy is proposed to preserve the local high-resolution details with
little computational overhead. Then the local features are integrated with
global features to capture inter-regional relationships within a cohesive
context. Moreover, we propose a novel region-report alignment (RRA) training
strategy. It leverages the recognition of referring regions to guide the
generation of region-specific reports, enhancing the model's referring and
grounding capabilities while also improving the report's interpretability. A
large language model (LLM) is further employed as the language decoder to
generate reports from integrated visual features, facilitating region-level
comprehension. Extensive experiments on two large-scale chest CT-report
datasets demonstrate the superiority of our method, which outperforms several
state-of-the-art methods in terms of both natural language generation and
clinical efficacy metrics while preserving promising interpretability. The code
will be made publicly available.

摘要：電腦斷層掃描 (CT) 報告生成對於協助放射科醫師判讀 CT 影像體積至關重要，而這項工作可能耗時且費力。現有方法主要只考量整個影像體積的全局特徵，導致難以聚焦於特定區域，並可能遺漏異常。為了解決這個問題，我們提出 Reg2RG，這是第一個針對 CT 報告生成的區域引導參照和基礎架構，透過聚焦於影像體積內的解剖區域，來增強診斷效能。具體來說，我們利用通用分割模組中的遮罩，來擷取每個參照區域的局部特徵。我們提出局部特徵解耦 (LFD) 策略，以在運算負擔不大的情況下保留局部高解析度細節。接著，局部特徵與全局特徵整合，以在一個有凝聚力的脈絡中擷取區域間的關係。此外，我們提出一個新穎的區域報告對齊 (RRA) 訓練策略。它利用參照區域的辨識來引導區域特定報告的生成，同時增強模型的參照和基礎能力，並改善報告的可解讀性。大型語言模型 (LLM) 進一步用作語言解碼器，以從整合的視覺特徵中生成報告，促進區域層級的理解。在兩個大規模胸部 CT 報告資料集上進行的廣泛實驗，證明了我們方法的優越性，在自然語言生成和臨床效能指標方面都優於多種最先進的方法，同時保留了良好的可解讀性。此程式碼將公開提供。

##### **GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning**
2411.15428v1 by Yunlei Liang, Jiawei Zhu, Wen Ye, Song Gao

Spatial networks are useful for modeling geographic phenomena where spatial
interaction plays an important role. To analyze the spatial networks and their
internal structures, graph-based methods such as community detection have been
widely used. Community detection aims to extract strongly connected components
from the network and reveal the hidden relationships between nodes, but they
usually do not involve the attribute information. To consider edge-based
interactions and node attributes together, this study proposed a family of
GeoAI-enhanced unsupervised community detection methods called region2vec based
on Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN). The
region2vec methods generate node neural embeddings based on attribute
similarity, geographic adjacency and spatial interactions, and then extract
network communities based on node embeddings using agglomerative clustering.
The proposed GeoAI-based methods are compared with multiple baselines and
perform the best when one wants to maximize node attribute similarity and
spatial interaction intensity simultaneously within the spatial network
communities. It is further applied in the shortage area delineation problem in
public health and demonstrates its promise in regionalization problems.

摘要：空間網路對於建模空間互動扮演重要角色的地理現象很有用。為了分析空間網路及其內部結構，基於圖形的方法，例如社群偵測已廣泛使用。社群偵測旨在從網路中提取強連結元件，並揭示節點之間的隱藏關係，但它們通常不涉及屬性資訊。為了同時考慮基於邊緣的互動和節點屬性，本研究提出了一系列稱為 region2vec 的 GeoAI 增強式非監督式社群偵測方法，該方法基於圖形注意力網路 (GAT) 和圖形卷積網路 (GCN)。region2vec 方法根據屬性相似性、地理鄰接性和空間互動產生節點神經嵌入，然後使用凝聚式分群根據節點嵌入提取網路社群。將提出的基於 GeoAI 的方法與多個基線進行比較，並在希望同時最大化空間網路社群中的節點屬性相似性和空間互動強度時執行最佳。進一步應用於公共衛生的短缺區域描繪問題中，並證明其在區域化問題中的前景。

##### **The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**
2411.15396v1 by Jiqun Liu, Jiangen He

Can AI be cognitively biased in automated information judgment tasks? Despite
recent progresses in measuring and mitigating social and algorithmic biases in
AI and large language models (LLMs), it is not clear to what extent LLMs behave
"rationally", or if they are also vulnerable to human cognitive bias triggers.
To address this open problem, our study, consisting of a crowdsourcing user
experiment and a LLM-enabled simulation experiment, compared the credibility
assessments by LLM and human judges under potential decoy effects in an
information retrieval (IR) setting, and empirically examined the extent to
which LLMs are cognitively biased in COVID-19 medical (mis)information
assessment tasks compared to traditional human assessors as a baseline. The
results, collected from a between-subject user experiment and a LLM-enabled
replicate experiment, demonstrate that 1) Larger and more recent LLMs tend to
show a higher level of consistency and accuracy in distinguishing credible
information from misinformation. However, they are more likely to give higher
ratings for misinformation due to the presence of a more salient, decoy
misinformation result; 2) While decoy effect occurred in both human and LLM
assessments, the effect is more prevalent across different conditions and
topics in LLM judgments compared to human credibility ratings. In contrast to
the generally assumed "rationality" of AI tools, our study empirically confirms
the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on
LLMs against human credibility assessments, and thereby highlights the
complexity and importance of debiasing AI agents and developing
psychology-informed AI audit techniques and policies for automated judgment
tasks and beyond.

摘要：大型語言模型 (LLM) 是否會在自動化資訊判斷任務中產生認知偏差？儘管最近在衡量和減輕 AI 和大型語言模型 (LLM) 中的社會和演算法偏差方面取得了進展，但尚不清楚 LLM 在何種程度上表現出「理性」，或者它們是否也容易受到人類認知偏差觸發因素的影響。為了解決這個開放性問題，我們的研究包含群眾外包使用者實驗和 LLM 啟用的模擬實驗，比較了 LLM 和人類評審員在資訊檢索 (IR) 設定中的潛在誘餌效應下的可信度評估，並實證檢驗了 LLM 在 COVID-19 醫療 (錯誤) 資訊評估任務中與傳統人類評估員相比在認知偏差的程度。從受試者之間的使用者實驗和 LLM 啟用的複製實驗中收集的結果顯示，1) 較大且較新的 LLM 往往在區分可信資訊和錯誤資訊時表現出較高程度的一致性和準確性。然而，由於存在更顯著的誘餌錯誤資訊結果，它們更有可能對錯誤資訊給予較高的評分；2) 雖然誘餌效應發生在人類和 LLM 評估中，但與人類可信度評分相比，該效應在 LLM 判斷的不同條件和主題中更為普遍。與一般假設的 AI 工具「理性」相反，我們的研究實證證實了 LLM 代理中嵌入的認知偏差風險，評估了誘餌對 LLM 的影響，並根據人類可信度評估，從而突顯了消除 AI 代理偏差和開發心理學資訊 AI 稽核技術和政策的複雜性和重要性，以用於自動化判斷任務及其他任務。

##### **Regulator-Manufacturer AI Agents Modeling: Mathematical Feedback-Driven Multi-Agent LLM Framework**
2411.15356v1 by Yu Han, Zekun Guo

The increasing complexity of regulatory updates from global authorities
presents significant challenges for medical device manufacturers, necessitating
agile strategies to sustain compliance and maintain market access.
Concurrently, regulatory bodies must effectively monitor manufacturers'
responses and develop strategic surveillance plans. This study employs a
multi-agent modeling approach, enhanced with Large Language Models (LLMs), to
simulate regulatory dynamics and examine the adaptive behaviors of key actors,
including regulatory bodies, manufacturers, and competitors. These agents
operate within a simulated environment governed by regulatory flow theory,
capturing the impacts of regulatory changes on compliance decisions, market
adaptation, and innovation strategies. Our findings illuminate the influence of
regulatory shifts on industry behaviour and identify strategic opportunities
for improving regulatory practices, optimizing compliance, and fostering
innovation. By leveraging the integration of multi-agent systems and LLMs, this
research provides a novel perspective and offers actionable insights for
stakeholders navigating the evolving regulatory landscape of the medical device
industry.

摘要：隨著全球主管機關法規更新的日益複雜，醫療器材製造商面臨重大挑戰，需要靈活的策略來維持合規並保持市場准入。同時，法規機構必須有效監控製造商的回應，並制定策略性監控計畫。本研究採用多重代理人建模方法，並透過大型語言模型 (LLM) 加以強化，以模擬法規動態並檢視主要參與者（包括法規機構、製造商和競爭者）的適應行為。這些代理人運作在受法規流動理論支配的模擬環境中，捕捉法規變更對合規決策、市場適應和創新策略的影響。我們的研究結果闡明法規變動對產業行為的影響，並找出改善法規實務、最佳化合規和促進創新的策略性機會。透過整合多重代理人系統和 LLM，本研究提供一個新穎觀點，並為利害關係人提供可行的見解，以因應醫療器材產業不斷變遷的法規環境。

##### **Health AI Developer Foundations**
2411.15128v1 by Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak

Robust medical Machine Learning (ML) models have the potential to
revolutionize healthcare by accelerating clinical research, improving workflows
and outcomes, and producing novel insights or capabilities. Developing such ML
models from scratch is cost prohibitive and requires substantial compute, data,
and time (e.g., expert labeling). To address these challenges, we introduce
Health AI Developer Foundations (HAI-DEF), a suite of pre-trained,
domain-specific foundation models, tools, and recipes to accelerate building ML
for health applications. The models cover various modalities and domains,
including radiology (X-rays and computed tomography), histopathology,
dermatological imaging, and audio. These models provide domain specific
embeddings that facilitate AI development with less labeled data, shorter
training times, and reduced computational costs compared to traditional
approaches. In addition, we utilize a common interface and style across these
models, and prioritize usability to enable developers to integrate HAI-DEF
efficiently. We present model evaluations across various tasks and conclude
with a discussion of their application and evaluation, covering the importance
of ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and
specifically the foundation models lower the barrier to entry for ML in
healthcare, we emphasize the importance of validation with problem- and
population-specific data for each desired usage setting. This technical report
will be updated over time as more modalities and features are added.

摘要：強大的醫療機器學習 (ML) 模型有潛力透過加速臨床研究、改善工作流程和成果，以及產生新見解或能力，從而徹底改變醫療保健。從頭開始開發此類 ML 模型在成本上過於昂貴，且需要大量的運算、資料和時間 (例如，專家標籤)。為了應對這些挑戰，我們推出了 Health AI Developer Foundations (HAI-DEF)，這是一套預先訓練好的、特定於領域的基礎模型、工具和範例，用於加速建立醫療保健應用的 ML。這些模型涵蓋各種模式和領域，包括放射學 (X 光和電腦斷層掃描)、組織病理學、皮膚影像學和音訊。這些模型提供特定於領域的嵌入，與傳統方法相比，它們有助於使用較少的標記資料、縮短訓練時間和降低運算成本來進行 AI 開發。此外，我們在這些模型中使用通用介面和樣式，並優先考慮可用性，以使開發人員能夠有效整合 HAI-DEF。我們針對各種任務展示模型評估，並以討論其應用和評估作為結論，涵蓋確保效能、公平性和公正性的重要性。最後，儘管 HAI-DEF 和特別是基礎模型降低了醫療保健中 ML 的進入門檻，但我們強調驗證對於每個所需的用法設定來說具有問題和特定於人群資料的重要性。隨著更多模式和功能的加入，這份技術報告將會隨著時間更新。

##### **ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation**
2411.15122v1 by Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar

AI-driven models have demonstrated significant potential in automating
radiology report generation for chest X-rays. However, there is no standardized
benchmark for objectively evaluating their performance. To address this, we
present ReXrank, https://rexrank.ai, a public leaderboard and challenge for
assessing AI-powered radiology report generation. Our framework incorporates
ReXGradient, the largest test dataset consisting of 10,000 studies, and three
public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation
assessment. ReXrank employs 8 evaluation metrics and separately assesses models
capable of generating only findings sections and those providing both findings
and impressions sections. By providing this standardized evaluation framework,
ReXrank enables meaningful comparisons of model performance and offers crucial
insights into their robustness across diverse clinical settings. Beyond its
current focus on chest X-rays, ReXrank's framework sets the stage for
comprehensive evaluation of automated reporting across the full spectrum of
medical imaging.

摘要：人工智能驅動的模型已證明在自動化胸部 X 射線放射報告生成方面具有顯著的潛力。然而，沒有標準化的基準來客觀評估其性能。為了解決這個問題，我們提出了 ReXrank，https://rexrank.ai，一個公共排行榜和挑戰，用於評估 AI 驅動的放射報告生成。我們的框架包含 ReXGradient，這是由 10,000 項研究組成的最大測試數據集，以及三個公共數據集（MIMIC-CXR、IU-Xray、CheXpert Plus），用於報告生成評估。ReXrank 採用 8 項評估指標，並分別評估只能生成結果部分的模型和同時提供結果和印象部分的模型。通過提供這個標準化的評估框架，ReXrank 能夠對模型性能進行有意義的比較，並提供對其在不同臨床環境中穩健性的關鍵見解。除了目前關注胸部 X 射線之外，ReXrank 的框架還為跨越整個醫學影像範圍的自動化報告的全面評估奠定了基礎。

##### **Feature-interactive Siamese graph encoder-based image analysis to predict STAS from histopathology images in lung cancer**
2411.15274v1 by Liangrui Pan, Qingchun Liang, Wenwu Zeng, Yijun Peng, Zhenyu Zhao, Yiyi Liang, Jiadi Luo, Xiang Wang, Shaoliang Peng

Spread through air spaces (STAS) is a distinct invasion pattern in lung
cancer, crucial for prognosis assessment and guiding surgical decisions.
Histopathology is the gold standard for STAS detection, yet traditional methods
are subjective, time-consuming, and prone to misdiagnosis, limiting large-scale
applications. We present VERN, an image analysis model utilizing a
feature-interactive Siamese graph encoder to predict STAS from lung cancer
histopathological images. VERN captures spatial topological features with
feature sharing and skip connections to enhance model training. Using 1,546
histopathology slides, we built a large single-cohort STAS lung cancer dataset.
VERN achieved an AUC of 0.9215 in internal validation and AUCs of 0.8275 and
0.8829 in frozen and paraffin-embedded test sections, respectively,
demonstrating clinical-grade performance. Validated on a single-cohort and
three external datasets, VERN showed robust predictive performance and
generalizability, providing an open platform (http://plr.20210706.xyz:5000/) to
enhance STAS diagnosis efficiency and accuracy.

摘要：<paragraph>經由空氣腔（STAS）擴散是一種肺癌中獨特的侵襲模式，對於預後評估和引導手術決策至關重要。組織病理學是 STAS 檢測的黃金標準，但傳統方法主觀、耗時且容易誤診，限制了大規模應用。我們提出 VERN，一種利用特徵互動式連體圖編碼器從肺癌組織病理學影像預測 STAS 的影像分析模型。VERN 通過特徵共享和跳躍連接捕獲空間拓撲特徵，以增強模型訓練。使用 1,546 張組織病理學切片，我們建立了一個大型單一隊列 STAS 肺癌數據集。VERN 在內部驗證中達到 0.9215 的 AUC，在冷凍和石蠟包埋的試驗切片中分別達到 0.8275 和 0.8829 的 AUC，證明了臨床級的性能。在單一隊列和三個外部數據集上進行驗證，VERN 表現出穩健的預測性能和泛化能力，提供了一個開放平台 (http://plr.20210706.xyz:5000/)，以提高 STAS 診斷效率和準確性。</paragraph>

##### **Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot**
2411.14925v1 by Linqi Lu, Yifan Deng, Chuan Tian, Sijia Yang, Dhavan Shah

This study introduces Purrfessor, an innovative AI chatbot designed to
provide personalized dietary guidance through interactive, multimodal
engagement. Leveraging the Large Language-and-Vision Assistant (LLaVA) model
fine-tuned with food and nutrition data and a human-in-the-loop approach,
Purrfessor integrates visual meal analysis with contextual advice to enhance
user experience and engagement. We conducted two studies to evaluate the
chatbot's performance and user experience: (a) simulation assessments and human
validation were conducted to examine the performance of the fine-tuned model;
(b) a 2 (Profile: Bot vs. Pet) by 3 (Model: GPT-4 vs. LLaVA vs. Fine-tuned
LLaVA) experiment revealed that Purrfessor significantly enhanced users'
perceptions of care ($\beta = 1.59$, $p = 0.04$) and interest ($\beta = 2.26$,
$p = 0.01$) compared to the GPT-4 bot. Additionally, user interviews
highlighted the importance of interaction design details, emphasizing the need
for responsiveness, personalization, and guidance to improve user engagement.

摘要：本研究介紹了 Purrfessor，這是一種創新的 AI 聊天機器人，旨在透過互動式多模式參與提供個人化的飲食指導。Purrfessor 採用經過食物和營養資料微調的大語言和視覺助理 (LLaVA) 模型，以及人工介入的方式，將視覺化餐點分析與情境建議整合，以增強使用者體驗和參與度。我們進行了兩項研究，以評估聊天機器人的效能和使用者體驗：(a) 進行模擬評估和人工驗證，以檢驗微調模型的效能；(b) 一項 2 (個人資料：機器人與寵物) x 3 (模型：GPT-4 與 LLaVA 與微調 LLaVA) 實驗顯示，與 GPT-4 機器人相比，Purrfessor 大幅提升了使用者對於關懷 ($\beta = 1.59$，$p = 0.04$) 和興趣 ($\beta = 2.26$，$p = 0.01$) 的觀感。此外，使用者訪談強調了互動設計細節的重要性，強調需要回應性、個人化和指導，以提升使用者參與度。

##### **Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation**
2411.14883v1 by Yuheng Xu, Taiping Zhang

Domain-invariant representation learning is a powerful method for domain
generalization. Previous approaches face challenges such as high computational
demands, training instability, and limited effectiveness with high-dimensional
data, potentially leading to the loss of valuable features. To address these
issues, we hypothesize that an ideal generalized representation should exhibit
similar pattern responses within the same channel across cross-domain images.
Based on this hypothesis, we use deep features from the source domain as
queries, and deep features from the generated domain as keys and values.
Through a cross-channel attention mechanism, the original deep features are
reconstructed into robust regularization representations, forming an explicit
constraint that guides the model to learn domain-invariant representations.
Additionally, style augmentation is another common method. However, existing
methods typically generate new styles through convex combinations of source
domains, which limits the diversity of training samples by confining the
generated styles to the original distribution. To overcome this limitation, we
propose an Adaptive Feature Blending (AFB) method that generates
out-of-distribution samples while exploring the in-distribution space,
significantly expanding the domain range. Extensive experimental results
demonstrate that our proposed methods achieve superior performance on two
standard domain generalization benchmarks for medical image segmentation.

摘要：領域不變表示學習是領域泛化的一種強大方法。先前的做法面臨諸如高計算需求、訓練不穩定以及對高維數據的有效性有限等挑戰，可能會導致有價值特徵的遺失。為了解決這些問題，我們假設一個理想的泛化表示應在跨領域影像中展現出相同通道內相似的模式反應。基於此假設，我們使用來自來源領域的深度特徵作為查詢，並使用來自生成領域的深度特徵作為鍵和值。透過跨通道注意力機制，原始深度特徵被重建為穩健的正則化表示，形成一個引導模型學習領域不變表示的明確約束。此外，樣式擴充是另一種常見方法。然而，現有方法通常透過來源領域的凸組合來生成新的樣式，這會將生成的樣式限制在原始分佈中，進而限制訓練樣本的多樣性。為了克服此限制，我們提出了一種自適應特徵混合 (AFB) 方法，該方法在探索分佈內空間的同時生成分佈外樣本，大幅擴展了領域範圍。廣泛的實驗結果證明，我們提出的方法在兩個標準醫學影像分割領域泛化基準上取得了卓越的效能。

##### **AI-Driven Real-Time Monitoring of Ground-Nesting Birds: A Case Study on Curlew Detection Using YOLOv10**
2411.15263v1 by Carl Chalmers, Paul Fergus, Serge Wich, Steven N Longmore, Naomi Davies Walsh, Lee Oliver, James Warrington, Julieanne Quinlan, Katie Appleby

Effective monitoring of wildlife is critical for assessing biodiversity and
ecosystem health, as declines in key species often signal significant
environmental changes. Birds, particularly ground-nesting species, serve as
important ecological indicators due to their sensitivity to environmental
pressures. Camera traps have become indispensable tools for monitoring nesting
bird populations, enabling data collection across diverse habitats. However,
the manual processing and analysis of such data are resource-intensive, often
delaying the delivery of actionable conservation insights. This study presents
an AI-driven approach for real-time species detection, focusing on the curlew
(Numenius arquata), a ground-nesting bird experiencing significant population
declines. A custom-trained YOLOv10 model was developed to detect and classify
curlews and their chicks using 3/4G-enabled cameras linked to the Conservation
AI platform. The system processes camera trap data in real-time, significantly
enhancing monitoring efficiency. Across 11 nesting sites in Wales, the model
achieved high performance, with a sensitivity of 90.56%, specificity of 100%,
and F1-score of 95.05% for curlew detections, and a sensitivity of 92.35%,
specificity of 100%, and F1-score of 96.03% for curlew chick detections. These
results demonstrate the capability of AI-driven monitoring systems to deliver
accurate, timely data for biodiversity assessments, facilitating early
conservation interventions and advancing the use of technology in ecological
research.

摘要：<paragraph>有效監測野生動物對於評估生物多樣性和生態系統健康至關重要，因為關鍵物種的數量減少通常表示重大的環境變化。鳥類，尤其是築巢於地面的物種，由於對環境壓力的敏感性，因此是重要的生態指標。相機陷阱已成為監測築巢鳥類種群不可或缺的工具，可以在不同的棲息地收集資料。然而，此類資料的手動處理和分析需要大量資源，通常會延遲提供可行的保育見解。本研究提出了一種由人工智慧驅動的即時物種偵測方法，重點關注杓鷸（Numenius arquata），一種築巢於地面的鳥類，其種群數量大幅下降。開發了一個客製化訓練的 YOLOv10 模型，以使用連結到保育人工智慧平台的 3/4G 相機偵測和分類杓鷸及其雛鳥。此系統即時處理相機陷阱資料，大幅提升監測效率。在威爾斯的 11 個築巢地點中，此模型表現優異，杓鷸偵測的敏感度為 90.56%，特異度為 100%，F1 分數為 95.05%，杓鷸雛鳥偵測的敏感度為 92.35%，特異度為 100%，F1 分數為 96.03%。這些結果證明了由人工智慧驅動的監測系統能夠提供準確、及時的資料，以進行生物多樣性評估，促進早期保育介入，並推進技術在生態研究中的應用。</paragraph>

##### **Optimized Vessel Segmentation: A Structure-Agnostic Approach with Small Vessel Enhancement and Morphological Correction**
2411.15251v1 by Dongning Song, Weijian Huang, Jiarun Liu, Md Jahidul Islam, Hao Yang, Shanshan Wang

Accurate segmentation of blood vessels is essential for various clinical
assessments and postoperative analyses. However, the inherent challenges of
vascular imaging, such as sparsity, fine granularity, low contrast, data
distribution variability, and the critical need for preserving topological
structure, making generalized vessel segmentation particularly complex. While
specialized segmentation methods have been developed for specific anatomical
regions, their over-reliance on tailored models hinders broader applicability
and generalization. General-purpose segmentation models introduced in medical
imaging often fail to address critical vascular characteristics, including the
connectivity of segmentation results. To overcome these limitations, we propose
an optimized vessel segmentation framework: a structure-agnostic approach
incorporating small vessel enhancement and morphological correction for
multi-modality vessel segmentation. To train and validate this framework, we
compiled a comprehensive multi-modality dataset spanning 17 datasets and
benchmarked our model against six SAM-based methods and 17 expert models. The
results demonstrate that our approach achieves superior segmentation accuracy,
generalization, and a 34.6% improvement in connectivity, underscoring its
clinical potential. An ablation study further validates the effectiveness of
the proposed improvements. We will release the code and dataset at github
following the publication of this work.

摘要：準確分割血管對於各種臨床評估和術後分析至關重要。然而，血管影像的固有挑戰，例如稀疏性、細緻顆粒度、低對比度、資料分佈變異性，以及保留拓撲結構的關鍵需求，使得廣義血管分割特別複雜。雖然已針對特定解剖區域開發了專門的分割方法，但它們過度依賴於客製化模型，阻礙了更廣泛的適用性和概括性。醫學影像中引入的通用分割模型通常無法解決關鍵的血管特徵，包括分割結果的連接性。為了克服這些限制，我們提出了一個最佳化的血管分割框架：一種結構不可知的方法，結合小血管增強和形態校正，用於多模態血管分割。為了訓練和驗證這個框架，我們編制了一個綜合的多模態資料集，涵蓋 17 個資料集，並將我們的模型與六種基於 SAM 的方法和 17 種專家模型進行了基準測試。結果表明，我們的做法實現了優異的分割準確度、概括性，以及連接性提升了 34.6%，突顯了其臨床潛力。消融研究進一步驗證了所提出改進的有效性。我們將在發表這項工作後於 github 上發布程式碼和資料集。

##### **Adversarial Prompt Distillation for Vision-Language Models**
2411.15244v1 by Lin Luo, Xin Wang, Bojia Zi, Shihao Zhao, Xingjun Ma

Large pre-trained Vision-Language Models (VLMs) such as Contrastive
Language-Image Pre-Training (CLIP) have been shown to be susceptible to
adversarial attacks, raising concerns about their deployment in safety-critical
scenarios like autonomous driving and medical diagnosis. One promising approach
for improving the robustness of pre-trained VLMs is Adversarial Prompt Tuning
(APT), which combines adversarial training with prompt tuning. However,
existing APT methods are mostly single-modal methods that design prompt(s) for
only the visual or textual modality, limiting their effectiveness in either
robustness or clean accuracy. In this work, we propose a novel method called
Adversarial Prompt Distillation (APD) that combines APT with knowledge
distillation to boost the adversarial robustness of CLIP. Specifically, APD is
a bimodal method that adds prompts for both the visual and textual modalities
while leveraging a cleanly pre-trained teacher CLIP model to distill and boost
the performance of the student CLIP model on downstream tasks. Extensive
experiments on multiple benchmark datasets demonstrate the superiority of our
APD over the current state-of-the-art APT methods in terms of both natural and
adversarial performances. The effectiveness of our APD method validates the
possibility of using a non-robust teacher to improve the generalization and
robustness of VLMs.

摘要：大型預訓練視覺語言模型 (VLM)，例如對比語言影像預訓練 (CLIP)，已被證明容易受到對抗性攻擊，這引起了人們對其在自動駕駛和醫療診斷等安全關鍵場景中部署的擔憂。一種有希望的方法是對抗提示調整 (APT)，它結合了對抗訓練和提示調整。然而，現有的 APT 方法大多是單模態方法，僅為視覺或文本模態設計提示，這限制了它們在魯棒性或乾淨準確性方面的有效性。在這項工作中，我們提出了一種稱為對抗提示蒸餾 (APD) 的新方法，它將 APT 與知識蒸餾相結合，以提高 CLIP 的對抗魯棒性。具體來說，APD 是一種雙模態方法，它為視覺和文本模態添加提示，同時利用預先訓練好的乾淨教師 CLIP 模型來蒸餾和提升學生 CLIP 模型在下游任務上的性能。在多個基準數據集上進行的廣泛實驗證明了我們的 APD 在自然和對抗性能方面優於當前最先進的 APT 方法。我們的 APD 方法的有效性驗證了使用非魯棒教師來提高 VLM 的泛化性和魯棒性的可能性。

##### **Is Attention All You Need For Actigraphy? Foundation Models of Wearable Accelerometer Data for Mental Health Research**
2411.15240v1 by Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C Jacobson

Wearable accelerometry (actigraphy) has provided valuable data for clinical
insights since the 1970s and is increasingly important as wearable devices
continue to become widespread. The effectiveness of actigraphy in research and
clinical contexts is heavily dependent on the modeling architecture utilized.
To address this, we developed the Pretrained Actigraphy Transformer (PAT)--the
first pretrained and fully attention-based model designed specifically to
handle actigraphy. PAT was pretrained on actigraphy from 29,307 participants in
NHANES, enabling it to deliver state-of-the-art performance when fine-tuned
across various actigraphy prediction tasks in the mental health domain, even in
data-limited scenarios. For example, when trained to predict benzodiazepine
usage using actigraphy from only 500 labeled participants, PAT achieved an 8.8
percentage-point AUC improvement over the best baseline. With fewer than 2
million parameters and built-in model explainability, PAT is robust yet easy to
deploy in health research settings.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/

摘要：可穿戴式加速度計（活動描記）自 1970 年代以來，為臨床見解提供了有價值的數據，並且隨著可穿戴式裝置日益普及，其重要性也與日俱增。活動描記在研究和臨床環境中的有效性，在很大程度上取決於所使用的建模架構。為了解決這個問題，我們開發了預訓練活動描記轉換器 (PAT)，這是第一個預訓練且完全基於注意力的模型，專門設計用於處理活動描記。PAT 是根據 29,307 位 NHANES 參與者的活動描記進行預訓練，使其能夠在針對心理健康領域的各種活動描記預測任務進行微調時，提供最先進的效能，即使在資料有限的情況下也是如此。例如，當訓練使用僅來自 500 位標記參與者的活動描記來預測苯二氮䓬類藥物使用時，PAT 在最佳基準上實現了 8.8 個百分點的 AUC 改進。PAT 具有不到 200 萬個參數和內建模型可解釋性，因此在健康研究設定中既強大又容易部署。
GitHub：https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/

##### **Uterine Ultrasound Image Captioning Using Deep Learning Techniques**
2411.14039v1 by Abdennour Boulesnane, Boutheina Mokhtari, Oumnia Rana Segueni, Slimane Segueni

Medical imaging has significantly revolutionized medical diagnostics and
treatment planning, progressing from early X-ray usage to sophisticated methods
like MRIs, CT scans, and ultrasounds. This paper investigates the use of deep
learning for medical image captioning, with a particular focus on uterine
ultrasound images. These images are vital in obstetrics and gynecology for
diagnosing and monitoring various conditions across different age groups.
However, their interpretation is often challenging due to their complexity and
variability. To address this, a deep learning-based medical image captioning
system was developed, integrating Convolutional Neural Networks with a
Bidirectional Gated Recurrent Unit network. This hybrid model processes both
image and text features to generate descriptive captions for uterine ultrasound
images. Our experimental results demonstrate the effectiveness of this approach
over baseline methods, with the proposed model achieving superior performance
in generating accurate and informative captions, as indicated by higher BLEU
and ROUGE scores. By enhancing the interpretation of uterine ultrasound images,
our research aims to assist medical professionals in making timely and accurate
diagnoses, ultimately contributing to improved patient care.

摘要：醫學影像大幅革新了醫療診斷和治療計畫，從早期的 X 光使用進展到 MRI、電腦斷層掃描和超音波等精密方法。這篇論文探討深度學習在醫學影像標題中的應用，特別著重於子宮超音波影像。這些影像在婦產科中對於診斷和追蹤不同年齡層的各種疾病至關重要。然而，由於其複雜性和變異性，它們的詮釋通常具有挑戰性。為了解決這個問題，開發了一個基於深度學習的醫學影像標題系統，將卷積神經網路與雙向門控循環單元網路整合在一起。這個混合模型處理影像和文字特徵，為子宮超音波影像產生描述性標題。我們的實驗結果證明了此方法優於基線方法的有效性，所提出的模型在產生準確且有意義的標題方面達到了卓越的效能，這由較高的 BLEU 和 ROUGE 分數所證明。透過增強子宮超音波影像的詮釋，我們的研究旨在協助醫療專業人員進行及時且準確的診斷，最終有助於改善病患照護。

##### **Logic Augmented Generation**
2411.14012v1 by Aldo Gangemi, Andrea Giovanni Nuzzolese

Semantic Knowledge Graphs (SKG) face challenges with scalability,
flexibility, contextual understanding, and handling unstructured or ambiguous
information. However, they offer formal and structured knowledge enabling
highly interpretable and reliable results by means of reasoning and querying.
Large Language Models (LLMs) overcome those limitations making them suitable in
open-ended tasks and unstructured environments. Nevertheless, LLMs are neither
interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we
envision Logic Augmented Generation (LAG) that combines the benefits of the two
worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate
potentially infinite relations and tacit knowledge on-demand. SKGs are key for
injecting a discrete heuristic dimension with clear logical and factual
boundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,
medical diagnostics and climate projections. Understanding the properties and
limitations of LAG, which are still mostly unknown, is of utmost importance for
enabling a variety of tasks involving tacit knowledge in order to provide
interpretable and effective results.

摘要：語意知識圖（SKG）在可擴充性、靈活性、情境理解以及處理非結構化或含糊資訊方面面臨挑戰。然而，它們提供正式且結構化的知識，能透過推理和查詢提供高度可解釋且可靠的結果。大型語言模型（LLM）克服了這些限制，使其適用於開放式任務和非結構化環境。儘管如此，LLM 既不可解釋也不可靠。為了解決 LLM 和 SKG 之間的二分法，我們設想了邏輯增強生成（LAG），它結合了兩個世界的優點。LAG 使用 LLM 作為反應式連續知識圖，它可以按需產生潛在的無限關係和默會知識。SKG 是注入離散啟發式維度（具有明確邏輯和事實邊界）的關鍵。我們在集體智慧的兩個任務中舉例說明 LAG，即醫療診斷和氣候預測。理解 LAG 的特性和限制（目前仍然大多數未知）對於啟用涉及默會知識的各種任務以提供可解釋且有效的結果至關重要。

##### **AmpliNetECG12: A lightweight SoftMax-based relativistic amplitude amplification architecture for 12 lead ECG classification**
2411.13903v1 by Shreya Srivastava

The urgent need to promptly detect cardiac disorders from 12-lead
Electrocardiograms using limited computations is motivated by the heart's fast
and complex electrical activity and restricted computational power of portable
devices. Timely and precise diagnoses are crucial since delays might
significantly impact patient health outcomes. This research presents a novel
deep-learning architecture that aims to diagnose heart abnormalities quickly
and accurately. We devised a new activation function called aSoftMax, designed
to improve the visibility of ECG deflections. The proposed activation function
is used with Convolutional Neural Network architecture to includes kernel
weight sharing across the ECG's various leads. This innovative method
thoroughly generalizes the global 12-lead ECG features and minimizes the
model's complexity by decreasing the trainable parameters. aSoftMax, combined
with enhanced CNN architecture yielded AmpliNetECG12, we obtain exceptional
accuracy of 84% in diagnosing cardiac disorders. AmpliNetECG12 shows
outstanding prediction ability when used with the CPSC2018 dataset for
arrhythmia classification. The model attains an F1-score of 80.71% and a
ROC-AUC score of 96.00%, with 280,000 trainable parameters which signifies the
lightweight yet efficient nature of AmpliNetECG12. The stochastic
characteristics of aSoftMax, a fundamental element of AmpliNetECG12, improve
prediction accuracy and also increasse the model's interpretability. This
feature enhances comprehension of important ECG segments in different forms of
arrhythmias, establishing a new standard of explainable architecture for
cardiac disorder classification.

摘要：<paragraph>由於心臟的快速且複雜的電氣活動和攜帶式裝置受限的運算能力，因此迫切需要使用 12 導程心電圖來快速偵測心臟疾病。及時且精確的診斷至關重要，因為延誤可能會對患者的健康狀況產生重大影響。本研究提出了一種新穎的深度學習架構，旨在快速且準確地診斷心臟異常。我們設計了一個稱為 aSoftMax 的新激活函數，旨在提高心電圖偏轉的可見度。所提出的激活函數與卷積神經網路架構一起使用，以在心電圖的各種導程之間包含核權重共享。這種創新方法徹底概括了全球 12 導程心電圖特徵，並通過減少可訓練參數來最小化模型的複雜性。aSoftMax 結合增強的 CNN 架構產生了 AmpliNetECG12，我們在診斷心臟疾病方面獲得了 84% 的出色準確度。AmpliNetECG12 在與 CPSC2018 資料集一起用於心律不整分類時顯示出出色的預測能力。該模型以 280,000 個可訓練參數獲得 80.71% 的 F1 分數和 96.00% 的 ROC-AUC 分數，這表明 AmpliNetECG12 的輕量級且高效的本質。aSoftMax 的隨機特徵是 AmpliNetECG12 的基本要素，它提高了預測準確度，也增加了模型的可解釋性。此功能增強了對不同形式心律不整中重要心電圖區段的理解，為心臟疾病分類建立了一個新的可解釋架構標準。</paragraph>

##### **PIORS: Personalized Intelligent Outpatient Reception based on Large Language Model with Multi-Agents Medical Scenario Simulation**
2411.13902v1 by Zhijie Bao, Qingyun Liu, Ying Guo, Zhengqiang Ye, Jun Shen, Shirong Xie, Jiajie Peng, Xuanjing Huang, Zhongyu Wei

In China, receptionist nurses face overwhelming workloads in outpatient
settings, limiting their time and attention for each patient and ultimately
reducing service quality. In this paper, we present the Personalized
Intelligent Outpatient Reception System (PIORS). This system integrates an
LLM-based reception nurse and a collaboration between LLM and hospital
information system (HIS) into real outpatient reception setting, aiming to
deliver personalized, high-quality, and efficient reception services.
Additionally, to enhance the performance of LLMs in real-world healthcare
scenarios, we propose a medical conversational data generation framework named
Service Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM
to the real-world environments and PIORS settings. We evaluate the
effectiveness of PIORS and SFMSS through automatic and human assessments
involving 15 users and 15 clinical experts. The results demonstrate that
PIORS-Nurse outperforms all baselines, including the current state-of-the-art
model GPT-4o, and aligns with human preferences and clinical needs. Further
details and demo can be found at https://github.com/FudanDISC/PIORS

摘要：<paragraph>在中国，接待护士在门诊环境中面临着繁重的工作量，限制了他们对每位患者的时间和注意力，最终降低了服务质量。在本文中，我们提出了个性化智能门诊接待系统 (PIORS)。该系统将基于 LLM 的接待护士和 LLM 与医院信息系统 (HIS) 之间的协作整合到真实的门诊接待环境中，旨在提供个性化、高质量和高效的接待服务。此外，为了提高 LLM 在真实医疗保健场景中的性能，我们提出了一个名为服务流感知医疗场景模拟 (SFMSS) 的医疗会话数据生成框架，旨在使 LLM 适应真实世界环境和 PIORS 设置。我们通过涉及 15 名用户和 15 名临床专家的自动和人工评估来评估 PIORS 和 SFMSS 的有效性。结果表明，PIORS-Nurse 优于所有基线，包括当前最先进的模型 GPT-4o，并且符合人类偏好和临床需求。更多详细信息和演示可在 https://github.com/FudanDISC/PIORS 中找到</paragraph>

##### **Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models**
2411.13518v1 by Chanseo Lee, Sonu Kumar, Kimon A. Vogt, Sam Meraj, Antonia Vogt

The increasing demand for multilingual capabilities in healthcare underscores
the need for AI models adept at processing diverse languages, particularly in
clinical documentation and decision-making. Arabic, with its complex
morphology, syntax, and diglossia, poses unique challenges for natural language
processing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a
language model tailored for Arabic clinical documentation, against JAIS, the
leading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics
modified ourselves for the purposes of assessing model performances in a
different language. The study assessed the models' performance in summarizing
patient-physician interactions, focusing on accuracy, comprehensiveness,
clinical utility, and linguistic-cultural competence.
  Results indicate that Sporo AraSum significantly outperforms JAIS in
AI-centric quantitative metrics and all qualitative attributes measured in our
modified version of the PDQI-9. AraSum's architecture enables precise and
culturally sensitive documentation, addressing the linguistic nuances of Arabic
while mitigating risks of AI hallucinations. These findings suggest that Sporo
AraSum is better suited to meet the demands of Arabic-speaking healthcare
environments, offering a transformative solution for multilingual clinical
workflows. Future research should incorporate real-world data to further
validate these findings and explore broader integration into healthcare
systems.

摘要：醫療保健領域對多語言能力的需求日益增加，這凸顯了對善於處理各種語言的 AI 模型的需求，特別是在臨床文件和決策制定中。阿拉伯語具有複雜的形態、語法和雙語現象，這對醫療環境中的自然語言處理 (NLP) 構成了獨特的挑戰。本案例研究評估了 Sporo AraSum（一種專為阿拉伯語臨床文件量身打造的語言模型）和阿拉伯語 NLP 模型的領導者 JAIS。我們使用合成資料集和修改後的 PDQI-9 指標（我們自行修改，以評估模型在不同語言中的表現）。本研究評估了模型在總結患者與醫師互動時的表現，重點在於準確性、全面性、臨床效用和語言文化能力。
結果表明，在以 AI 為中心的定量指標和我們修改後的 PDQI-9 版本中測量的所有定性屬性中，Sporo AraSum 明顯優於 JAIS。AraSum 的架構能產生精確且具有文化敏感度的文件，它能處理阿拉伯語的語言差異，同時降低 AI 產生幻覺的風險。這些發現表明，Sporo AraSum 更適合滿足講阿拉伯語的醫療保健環境的需求，為多語言臨床工作流程提供了一個變革性的解決方案。未來的研究應納入真實世界的資料，以進一步驗證這些發現，並探索更廣泛地整合到醫療保健系統中。

##### **Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint**
2411.15216v1 by Guangkun Nie, Gongzheng Tang, Shenda Hong

Imbalanced data distributions are prevalent in real-world scenarios, posing
significant challenges in both imbalanced classification and imbalanced
regression tasks. They often cause deep learning models to overfit in areas of
high sample density (many-shot regions) while underperforming in areas of low
sample density (few-shot regions). This characteristic restricts the utility of
deep learning models in various sectors, notably healthcare, where areas with
few-shot data hold greater clinical relevance. While recent studies have shown
the benefits of incorporating distribution information in imbalanced
classification tasks, such strategies are rarely explored in imbalanced
regression. In this paper, we address this issue by introducing a novel loss
function, termed Dist Loss, designed to minimize the distribution distance
between the model's predictions and the target labels in a differentiable
manner, effectively integrating distribution information into model training.
Dist Loss enables deep learning models to regularize their output distribution
during training, effectively enhancing their focus on few-shot regions. We have
conducted extensive experiments across three datasets spanning computer vision
and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results
demonstrate that Dist Loss effectively mitigates the negative impact of
imbalanced data distribution on model performance, achieving state-of-the-art
results in sparse data regions. Furthermore, Dist Loss is easy to integrate,
complementing existing methods.

摘要：<paragraph>不平衡資料分佈在實際情況中很常見，對不平衡分類和不平衡回歸任務構成重大挑戰。它們通常導致深度學習模型在樣本密度高（多樣本區域）的區域過度擬合，而在樣本密度低（少樣本區域）的區域表現不佳。這種特性限制了深度學習模型在各個領域的實用性，特別是在醫療保健領域，其中少樣本資料區域具有更大的臨床相關性。儘管最近的研究顯示了在不平衡分類任務中納入分佈資訊的好處，但此類策略在不平衡回歸中很少被探討。在本文中，我們透過引入一種稱為 Dist Loss 的新損失函數來解決此問題，該函數旨在以可微分的方式最小化模型預測與目標標籤之間的分佈距離，有效地將分佈資訊整合到模型訓練中。Dist Loss 能讓深度學習模型在訓練期間調整其輸出分佈，有效地加強它們對少樣本區域的關注。我們跨越電腦視覺和醫療保健領域的三個資料集進行了廣泛的實驗：IMDB-WIKI-DIR、AgeDB-DIR 和 ECG-Ka-DIR。結果表明，Dist Loss 有效地減輕了不平衡資料分佈對模型效能的負面影響，在稀疏資料區域中取得了最先進的結果。此外，Dist Loss 易於整合，可補充現有方法。</paragraph>

##### **SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers**
2411.13428v1 by Hojjat Karami, David Atienza, Anisoara Ionescu

Generating synthetic Electronic Health Records (EHRs) offers significant
potential for data augmentation, privacy-preserving data sharing, and improving
machine learning model training. We propose a novel tokenization strategy
tailored for structured EHR data, which encompasses diverse data types such as
covariates, ICD codes, and irregularly sampled time series. Using a GPT-like
decoder-only transformer model, we demonstrate the generation of high-quality
synthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we
benchmark the fidelity, utility, and privacy of the generated data against
state-of-the-art models.

摘要：生成合成電子病歷 (EHR) 提供了顯著的數據擴充、隱私保護數據共享以及改進機器學習模型訓練的潛力。我們提出了一種針對結構化電子病歷數據量身打造的新型標記化策略，它包含了各種數據類型，例如協變量、ICD 代碼和不規則採樣的時序。使用類似 GPT 的僅解碼器Transformer模型，我們展示了高品質合成電子病歷的生成。我們的做法使用 MIMIC-III 數據集進行評估，我們根據最先進的模型對生成數據的保真度、實用性和隱私性進行基準測試。

##### **S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning**
2411.15215v1 by Mingze Yin, Hanjing Zhou, Jialu Wu, Yiheng Zhu, Yuxuan Zhan, Zitai Kong, Hongxia Xu, Chang-Yu Hsieh, Jintai Chen, Tingjun Hou, Jian Wu

Antibodies safeguard our health through their precise and potent binding to
specific antigens, demonstrating promising therapeutic efficacy in the
treatment of numerous diseases, including COVID-19. Recent advancements in
biomedical language models have shown the great potential to interpret complex
biological structures and functions. However, existing antibody specific models
have a notable limitation that they lack explicit consideration for antibody
structural information, despite the fact that both 1D sequence and 3D structure
carry unique and complementary insights into antibody behavior and
functionality. This paper proposes Sequence-Structure multi-level pre-trained
Antibody Language Model (S$^2$ALM), combining holistic sequential and
structural information in one unified, generic antibody foundation model. We
construct a hierarchical pre-training paradigm incorporated with two customized
multi-level training objectives to facilitate the modeling of comprehensive
antibody representations. S$^2$ALM's representation space uncovers inherent
functional binding mechanisms, biological evolution properties and structural
interaction patterns. Pre-trained over 75 million sequences and 11.7 million
structures, S$^2$ALM can be adopted for diverse downstream tasks: accurately
predicting antigen-antibody binding affinities, precisely distinguishing B cell
maturation stages, identifying antibody crucial binding positions, and
specifically designing novel coronavirus-binding antibodies. Remarkably,
S$^2$ALM outperforms well-established and renowned baselines and sets new
state-of-the-art performance across extensive antibody specific understanding
and generation tasks. S$^2$ALM's ability to model comprehensive and generalized
representations further positions its potential to advance real-world
therapeutic antibody development, potentially addressing unmet academic,
industrial, and clinical needs.

摘要：<paragraph>抗體透過精準且強而有力的結合，針對特定抗原來保護我們的健康，在包括 COVID-19 在內的許多疾病的治療中，展現出有希望的治療功效。生物醫學語言模型的最新進展，已顯示出詮釋複雜生物結構和功能的巨大潛力。然而，現有的抗體特定模型有一個顯著的限制，就是它們缺乏對抗體結構資訊的明確考量，儘管 1D 序列和 3D 結構都能提供獨特且互補的見解，深入了解抗體行為和功能。本文提出序列結構多層級預訓練抗體語言模型 (S$^2$ALM)，將整體序列和結構資訊結合在一個統一的通用抗體基礎模型中。我們建構了一個分層預訓練範例，並結合兩個客製化多層級訓練目標，以利於建模全面的抗體表示。S$^2$ALM 的表示空間揭示了內在的功能結合機制、生物演化特性和結構互動模式。S$^2$ALM 預先訓練超過 7500 萬個序列和 1170 萬個結構，可採用於各種下游任務：準確預測抗原抗體結合親和力、精確區分 B 細胞成熟階段、識別抗體關鍵結合位置，以及特別設計新型冠狀病毒結合抗體。值得注意的是，S$^2$ALM 優於既有且知名的基線，並在廣泛的抗體特定理解和生成任務中，樹立新的最先進效能。S$^2$ALM 建模全面且廣泛表示的能力，進一步奠定了其在推進現實世界治療性抗體開發的潛力，有可能滿足未滿足的學術、產業和臨床需求。</paragraph>

##### **Are Large Language Models Memorizing Bug Benchmarks?**
2411.13323v1 by Daniel Ramos, Claudia Mamede, Kush Jain, Paulo Canelas, Catarina Gamboa, Claire Le Goues

Large Language Models (LLMs) have become integral to various software
engineering tasks, including code generation, bug detection, and repair. To
evaluate model performance in these domains, numerous bug benchmarks containing
real-world bugs from software projects have been developed. However, a growing
concern within the software engineering community is that these benchmarks may
not reliably reflect true LLM performance due to the risk of data leakage.
Despite this concern, limited research has been conducted to quantify the
impact of potential leakage.
  In this paper, we systematically evaluate popular LLMs to assess their
susceptibility to data leakage from widely used bug benchmarks. To identify
potential leakage, we use multiple metrics, including a study of benchmark
membership within commonly used training datasets, as well as analyses of
negative log-likelihood and n-gram accuracy. Our findings show that certain
models, in particular codegen-multi, exhibit significant evidence of
memorization in widely used benchmarks like Defects4J, while newer models
trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.
These results highlight the need for careful benchmark selection and the
adoption of robust metrics to adequately assess models capabilities.

摘要：大型語言模型 (LLM) 已成為各種軟體工程任務中不可或缺的一部分，包括程式碼產生、錯誤偵測和修復。為了評估這些領域中的模型效能，已開發出許多包含軟體專案中真實錯誤的錯誤基準測試。然而，軟體工程社群中日益關注的是，由於資料外洩的風險，這些基準測試可能無法可靠地反映真正的 LLM 效能。儘管有此疑慮，但針對量化潛在外洩影響的研究卻十分有限。
在本文中，我們系統性地評估熱門 LLM，以評估它們對廣泛使用的錯誤基準測試中資料外洩的敏感性。為了識別潛在的外洩，我們使用多種指標，包括研究基準測試成員資格在常用的訓練資料集中的情況，以及對負對數似然和 n-gram 精確度的分析。我們的研究結果顯示，某些模型，特別是 codegen-multi，在 Defects4J 等廣泛使用的基準測試中展現出顯著的記憶證據，而訓練於較大型資料集（例如 LLaMa 3.1）的較新模型則展現出有限的外洩跡象。這些結果突顯了仔細選擇基準測試和採用穩健指標以充分評估模型功能的必要性。

##### **Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training**
2411.15207v1 by Ameera Bawazir, Kebin Wu, Wenbin Li

Recent advancements in vision-language pre-training via contrastive learning
have significantly improved performance across computer vision tasks. However,
in the medical domain, obtaining multimodal data is often costly and
challenging due to privacy, sensitivity, and annotation complexity. To mitigate
data scarcity while boosting model performance, we introduce \textbf{Uni-Mlip},
a unified self-supervision framework specifically designed to enhance medical
vision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,
uni-modality, and fused-modality self-supervision techniques at the data-level
and the feature-level. Additionally, Uni-Mlip tailors uni-modal image
self-supervision to accommodate the unique characteristics of medical images.
Our experiments across datasets of varying scales demonstrate that Uni-Mlip
significantly surpasses current state-of-the-art methods in three key
downstream tasks: image-text retrieval, image classification, and visual
question answering (VQA).

摘要：透過對比學習進行視覺語言預訓練的最新進展，已顯著提升電腦視覺任務的效能。然而，在醫學領域中，取得多模態資料通常成本高昂且具挑戰性，原因在於隱私、敏感性和標註的複雜性。為了在提升模型效能的同時減輕資料稀少的問題，我們引入了 \textbf{Uni-Mlip}，這是一個統一的自我監督架構，專門設計用於增強醫學視覺語言預訓練。Uni-Mlip 在資料層級和特徵層級無縫整合跨模態、單模態和融合模態的自我監督技術。此外，Uni-Mlip 針對單模態影像自我監督進行調整，以適應醫學影像的獨特特性。我們在不同規模的資料集上進行的實驗證明，Uni-Mlip 在三個關鍵的下游任務中顯著超越目前最先進的方法：影像文字檢索、影像分類和視覺問答 (VQA)。

##### **GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation**
2411.13147v2 by Mengzhu Wang, Jiao Li, Houcheng Su, Nan Yin, Liang Yang, Shen Li

Semi-supervised learning (SSL) has made notable advancements in medical image
segmentation (MIS), particularly in scenarios with limited labeled data and
significantly enhancing data utilization efficiency. Previous methods primarily
focus on complex training strategies to utilize unlabeled data but neglect the
importance of graph structural information. Different from existing methods, we
propose a graph-based clustering for semi-supervised medical image segmentation
(GraphCL) by jointly modeling graph data structure in a unified deep model. The
proposed GraphCL model enjoys several advantages. Firstly, to the best of our
knowledge, this is the first work to model the data structure information for
semi-supervised medical image segmentation (SSMIS). Secondly, to get the
clustered features across different graphs, we integrate both pairwise
affinities between local image features and raw features as inputs. Extensive
experimental results on three standard benchmarks show that the proposed
GraphCL algorithm outperforms state-of-the-art semi-supervised medical image
segmentation methods.

摘要：半監督學習 (SSL) 已在醫學影像分割 (MIS) 中取得顯著進展，特別是在標籤資料有限的情況下，並顯著提升資料利用效率。先前的研究方法主要專注於複雜的訓練策略以利用未標籤資料，卻忽略了圖形結構資訊的重要性。有別於現有方法，我們提出一個基於圖形聚類的半監督醫學影像分割 (GraphCL)，透過在一個統一深度模型中聯合建模圖形資料結構。所提出的 GraphCL 模型享有許多優點。首先，據我們所知，這是第一個為半監督醫學影像分割 (SSMIS) 建模資料結構資訊的研究。其次，為了取得跨不同圖形的聚類特徵，我們將局部影像特徵和原始特徵之間的成對相似性都整合為輸入。在三個標準基準上的廣泛實驗結果顯示，所提出的 GraphCL 演算法優於現有最先進的半監督醫學影像分割方法。

##### **Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine**
2411.14487v1 by Yifan Yang, Qiao Jin, Robert Leaman, Xiaoyu Liu, Guangzhi Xiong, Maame Sarfo-Gyamfi, Changlin Gong, Santiago Ferrière-Steinert, W. John Wilbur, Xiaojun Li, Jiaxin Yuan, Bang An, Kelvin S. Castro, Francisco Erramuspe Álvarez, Matías Stockle, Aidong Zhang, Furong Huang, Zhiyong Lu

The remarkable capabilities of Large Language Models (LLMs) make them
increasingly compelling for adoption in real-world healthcare applications.
However, the risks associated with using LLMs in medical applications have not
been systematically characterized. We propose using five key principles for
safe and trustworthy medical AI: Truthfulness, Resilience, Fairness,
Robustness, and Privacy, along with ten specific aspects. Under this
comprehensive framework, we introduce a novel MedGuard benchmark with 1,000
expert-verified questions. Our evaluation of 11 commonly used LLMs shows that
the current language models, regardless of their safety alignment mechanisms,
generally perform poorly on most of our benchmarks, particularly when compared
to the high performance of human physicians. Despite recent reports indicate
that advanced LLMs like ChatGPT can match or even exceed human performance in
various medical tasks, this study underscores a significant safety gap,
highlighting the crucial need for human oversight and the implementation of AI
safety guardrails.

摘要：大型語言模型 (LLM) 擁有非凡的能力，使其在實際醫療保健應用中越來越引人注目。
然而，在醫療應用中使用 LLM 相關的風險尚未得到系統性的描述。我們建議使用五項關鍵原則，以確保醫療 AI 的安全和可信賴：真實性、彈性、公平性、穩健性和隱私，以及十個具體方面。在這個全面的架構下，我們引入了一個新的 MedGuard 基準，其中包含 1,000 個由專家驗證的問題。我們對 11 個常用 LLM 的評估表明，無論其安全對齊機制如何，當前的語言模型在我們大多數基準測試中表現普遍不佳，特別是與人類醫生的高表現相比時。儘管最近的報告表明，像 ChatGPT 這樣的先進 LLM 在各種醫療任務中可以達到或甚至超過人類的表現，但這項研究強調了一個重大的安全差距，突顯了人類監督和實施 AI 安全護欄的關鍵需求。

##### **Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI**
2411.13022v1 by Yaşar Utku Alçalar, Merve Gülle, Mehmet Akçakaya

Physics-driven deep learning (PD-DL) approaches have become popular for
improved reconstruction of fast magnetic resonance imaging (MRI) scans. Even
though PD-DL offers higher acceleration rates compared to existing clinical
fast MRI techniques, their use has been limited outside specialized MRI
centers. One impediment for their deployment is the difficulties with
generalization to pathologies or population groups that are not
well-represented in training sets. This has been noted in several studies, and
fine-tuning on target populations to improve reconstruction has been suggested.
However, current approaches for PD-DL training require access to raw k-space
measurements, which is typically only available at specialized MRI centers that
have research agreements for such data access. This is especially an issue for
rural and underserved areas, where commercial MRI scanners only provide access
to a final reconstructed image. To tackle these challenges, we propose
Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity
(CUPID) for high-quality PD-DL training, using only routine clinical
reconstructed images exported from an MRI scanner. CUPID evaluates the goodness
of the output with a compressibility-based approach, while ensuring that the
output stays consistent with the clinical parallel imaging reconstruction
through well-designed perturbations. Our results show that CUPID achieves
similar quality compared to well-established PD-DL training strategies that
require raw k-space data access, while outperforming conventional compressed
sensing (CS) and state-of-the-art generative methods. We also demonstrate its
effectiveness in a zero-shot training setup for retrospectively and
prospectively sub-sampled acquisitions, attesting to its minimal training
burden.

摘要：<paragraph>物理驅動深度學習 (PD-DL) 方法已廣受歡迎，用於改善快速磁振造影 (MRI) 掃描的重建。儘管與現有的臨床快速 MRI 技術相比，PD-DL 提供了更高的加速率，但其使用已被限制在專門的 MRI 中心之外。部署它們的一個障礙是難以推廣到訓練集中未充分呈現的病理或人群。這已在多項研究中註明，並且建議對目標人群進行微調以改善重建。然而，當前 PD-DL 訓練方法需要存取原始 k-space 量測，而這通常僅在具有此類資料存取研究協議的專門 MRI 中心才可取得。這對於鄉村和服務不足的地區來說尤其成問題，因為商業 MRI 掃描儀僅提供存取最終重建影像。為了應對這些挑戰，我們提出透過平行影像保真度 (CUPID) 進行受壓縮性啟發的非監督式學習，僅使用從 MRI 掃描儀輸出的例行臨床重建影像，以進行高品質 PD-DL 訓練。CUPID 使用基於壓縮性的方法評估輸出的優劣，同時確保輸出透過精心設計的擾動與臨床平行影像重建保持一致。我們的結果顯示，與需要原始 k-space 資料存取的完善 PD-DL 訓練策略相比，CUPID 達到了類似的品質，同時優於傳統的壓縮感測 (CS) 和最先進的生成方法。我們還展示了它在零次學習設定中對回溯性和前瞻性子抽樣擷取的有效性，證明了它的訓練負擔很小。</paragraph>

##### **Automating Sonologists USG Commands with AI and Voice Interface**
2411.13006v1 by Emad Mohamed, Shruti Tiwari, Sheena Christabel Pravin

This research presents an advanced AI-powered ultrasound imaging system that
incorporates real-time image processing, organ tracking, and voice commands to
enhance the efficiency and accuracy of diagnoses in clinical practice.
Traditional ultrasound diagnostics often require significant time and introduce
a degree of subjectivity due to user interaction. The goal of this innovative
solution is to provide Sonologists with a more predictable and productive
imaging procedure utilizing artificial intelligence, computer vision, and voice
technology. The functionality of the system employs computer vision and deep
learning algorithms, specifically adopting the Mask R-CNN model from Detectron2
for semantic segmentation of organs and key landmarks. This automation improves
diagnostic accuracy by enabling the extraction of valuable information with
minimal human input. Additionally, it includes a voice recognition feature that
allows for hands-free operation, enabling users to control the system with
commands such as freeze or liver, all while maintaining their focus on the
patient. The architecture comprises video processing and real-time segmentation
modules that prepare the system to perform essential imaging functions, such as
freezing and zooming in on frames. The liver histopathology module, optimized
for detecting fibrosis, achieved an impressive accuracy of 98.6%. Furthermore,
the organ segmentation module produces output confidence levels between 50% and
95%, demonstrating its efficacy in organ detection.

摘要：本研究提出了一個進階的人工智慧超音波影像系統，它結合了即時影像處理、器官追蹤和語音指令，以增強臨床實務中診斷的效率和準確性。傳統的超音波診斷通常需要大量的時間，並由於使用者的互動而引入了一定的主觀性。這個創新解決方案的目標是為超音波檢查醫師提供一個更可預測且更具生產力的影像程序，利用人工智慧、電腦視覺和語音技術。該系統的功能採用電腦視覺和深度學習演算法，特別是採用 Detectron2 中的 Mask R-CNN 模型來進行器官和關鍵地標的語意分割。此自動化透過以最少的人工輸入提取有價值的資訊來提高診斷準確性。此外，它還包括一個語音辨識功能，允許免持操作，使用戶能夠使用凍結或肝臟等指令來控制系統，同時將注意力集中在患者身上。架構包含視訊處理和即時分割模組，準備系統執行必要的影像功能，例如凍結和縮放畫面。針對纖維化偵測而最佳化的肝臟組織病理學模組，達到了令人印象深刻的 98.6% 準確度。此外，器官分割模組產生的輸出信心水準在 50% 到 95% 之間，證明了其在器官偵測中的效能。

##### **DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback**
2411.14157v1 by Mahsa Sheikholeslami, Navid Mazrouei, Yousof Gheisari, Afshin Fasihi, Matin Irajpour, Ali Motahharynia

Traditional drug design faces significant challenges due to inherent chemical
and biological complexities, often resulting in high failure rates in clinical
trials. Deep learning advancements, particularly generative models, offer
potential solutions to these challenges. One promising algorithm is DrugGPT, a
transformer-based model, that generates small molecules for input protein
sequences. Although promising, it generates both chemically valid and invalid
structures and does not incorporate the features of approved drugs, resulting
in time-consuming and inefficient drug discovery. To address these issues, we
introduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is
fine-tuned on approved drug-target interactions and optimized with proximal
policy optimization. By giving reward feedback from protein-ligand binding
affinity prediction using pre-trained transformers (PLAPT) and a customized
invalid structure assessor, DrugGen significantly improves performance.
Evaluation across multiple targets demonstrated that DrugGen achieves 100%
valid structure generation compared to 95.5% with DrugGPT and produced
molecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared
to DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking
simulations further validate its ability to generate molecules targeting
binding sites effectively. For example, in the case of fatty acid-binding
protein 5 (FABP5), DrugGen generated molecules with superior docking scores
(FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule
(Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows
potential for drug repositioning and creating novel pharmacophores for existing
targets. By producing high-quality small molecules, DrugGen provides a
high-performance medium for advancing pharmaceutical research and drug
discovery.

摘要：傳統藥物設計因其化學和生物複雜性而面臨重大挑戰，這通常會導致臨床試驗的高失敗率。深度學習的進展，特別是生成模型，為這些挑戰提供了潛在的解決方案。一種有前途的演算法是 DrugGPT，這是一種基於轉換器的模型，它為輸入蛋白質序列生成小分子。儘管有前景，但它既產生化學有效結構，也產生無效結構，並且不包含已核准藥物的特徵，導致耗時且低效的藥物發現。為了解決這些問題，我們引入了 DrugGen，這是一種基於 DrugGPT 結構的增強模型。DrugGen 在已核准的藥物目標交互作用上進行微調，並使用近端策略最佳化進行最佳化。通過使用預先訓練的轉換器 (PLAPT) 和自訂無效結構評估器，從蛋白質配體結合親和力預測中提供獎勵回饋，DrugGen 大幅提升了效能。多個目標的評估顯示，與 DrugGPT 的 95.5% 相比，DrugGen 達到了 100% 的有效結構生成，並且產生了預測結合親和力較高的分子 (7.22 [6.30-8.07])，而 DrugGPT 為 (5.81 [4.97-6.63])，同時保持了多樣性和新穎性。對接模擬進一步驗證了其有效生成分子以針對結合位點的能力。例如，在脂肪酸結合蛋白 5 (FABP5) 的情況下，與參考分子（棕櫚酸，-6.177）相比，DrugGen 生成了對接評分較高的分子 (FABP5/11，-9.537 和 FABP5/5，-8.399)。除了先導化合物生成之外，DrugGen 還顯示了藥物重新定位和為現有目標建立新型藥效團的潛力。通過產生高品質的小分子，DrugGen 為推進製藥研究和藥物發現提供了一個高性能的媒介。

##### **Deep Learning-Based Classification of Hyperkinetic Movement Disorders in Children**
2411.15200v1 by Nandika Ramamurthy, Dr Daniel Lumsden, Dr Rachel Sparks

Hyperkinetic movement disorders (HMDs) in children, including dystonia
(abnormal twisting) and chorea (irregular, random movements), pose significant
diagnostic challenges due to overlapping clinical features. The prevalence of
dystonia ranges from 2 to 50 per million, and chorea from 5 to 10 per 100,000.
These conditions are often diagnosed with delays averaging 4.75 to 7.83 years.
Traditional diagnostic methods depend on clinical history and expert physical
examinations, but specialized tests are ineffective due to the complex
pathophysiology of these disorders. This study develops a neural network model
to differentiate between dystonia and chorea from video recordings of
paediatric patients performing motor tasks. The model integrates a Graph
Convolutional Network (GCN) to capture spatial relationships and Long
Short-Term Memory (LSTM) networks to account for temporal dynamics. Attention
mechanisms were incorporated to improve model interpretability. The model was
trained and validated on a dataset of 50 videos (31 chorea-predominant, 19
dystonia-predominant) collected under regulatory approval from Guy's and St
Thomas' NHS Foundation Trust. The model achieved 85% accuracy, 81% sensitivity,
and 88% specificity at 15 frames per second. Attention maps highlighted the
model's ability to correctly identify involuntary movement patterns, with
misclassifications often due to occluded body parts or subtle movement
variations. This work demonstrates the potential of deep learning to improve
the accuracy and efficiency of HMD diagnosis and could contribute to more
reliable, interpretable clinical tools.

摘要：<paragraph>兒童的運動過動症 (HMDs)，包括肌張力不全（異常扭動）和舞蹈症（不規則、隨機的動作），由於臨床特徵重疊，因此在診斷上具有重大挑戰。肌張力不全的盛行率為每百萬人 2 至 50 人，舞蹈症的盛行率為每 10 萬人 5 至 10 人。這些疾病通常在平均 4.75 至 7.83 年後才被診斷出來。傳統的診斷方法依賴於病史和專家身體檢查，但由於這些疾病的複雜病理生理，專門的檢查並無效。本研究開發了一種神經網路模型，用於區分舞蹈症和肌張力不全，方法是透過錄製兒童患者執行運動任務的影片。該模型整合了圖形卷積網路 (GCN) 以擷取空間關係，以及長短期記憶 (LSTM) 網路以考量時間動態。注意力機制被納入以改善模型的可解釋性。該模型在一個由 50 個影片組成的資料集上進行訓練和驗證（31 個以舞蹈症為主，19 個以肌張力不全為主），這些影片是在取得 Guy's and St Thomas' NHS Foundation Trust 的法規核准後收集的。該模型在每秒 15 幀時達到 85% 的準確度、81% 的靈敏度和 88% 的特異性。注意力圖突顯了該模型正確識別非自主運動模式的能力，而誤分類通常是因身體部位被遮擋或細微的運動變化所致。這項工作證明了深度學習在改善 HMD 診斷的準確性和效率方面的潛力，並可能有助於開發更可靠、可解釋的臨床工具。</paragraph>

##### **Efficient Medicinal Image Transmission and Resolution Enhancement via GAN**
2411.12833v1 by Rishabh Kumar Sharma, Mukund Sharma, Pushkar Sharma, Jeetashree Aparjeeta

While X-ray imaging is indispensable in medical diagnostics, it inherently
carries with it those noises and limitations on resolution that mask the
details necessary for diagnosis. B/W X-ray images require a careful balance
between noise suppression and high-detail preservation to ensure clarity in
soft-tissue structures and bone edges. While traditional methods, such as CNNs
and early super-resolution models like ESRGAN, have enhanced image resolution,
they often perform poorly regarding high-frequency detail preservation and
noise control for B/W imaging. We are going to present one efficient approach
that improves the quality of an image with the optimization of network
transmission in the following paper. The pre-processing of X-ray images into
low-resolution files by Real-ESRGAN, a version of ESRGAN elucidated and
improved, helps reduce the server load and transmission bandwidth.
Lower-resolution images are upscaled at the receiving end using Real-ESRGAN,
fine-tuned for real-world image degradation. The model integrates
Residual-in-Residual Dense Blocks with perceptual and adversarial loss
functions for high-quality upscaled images with low noise. We further fine-tune
Real-ESRGAN by adapting it to the specific B/W noise and contrast
characteristics. This suppresses noise artifacts without compromising detail.
The comparative evaluation conducted shows that our approach achieves superior
noise reduction and detail clarity compared to state-of-the-art CNN-based and
ESRGAN models, apart from reducing network bandwidth requirements. These
benefits are confirmed both by quantitative metrics, including Peak
Signal-to-Noise Ratio and Structural Similarity Index, and by qualitative
assessments, which indicate the potential of Real-ESRGAN for diagnostic-quality
X-ray imaging and for efficient medical data transmission.

摘要：儘管 X 光影像在醫療診斷中不可或缺，但它本身就帶有那些會遮蔽診斷所需細節的雜訊和解析度限制。黑白 X 光影像需要在雜訊抑制和高細節保留之間取得仔細的平衡，以確保軟組織結構和骨骼邊緣的清晰度。儘管 CNN 和 ESRGAN 等傳統方法和早期超解析度模型已增強影像解析度，但它們在高頻率細節保留和黑白影像的雜訊控制方面通常表現不佳。我們將在以下論文中提出一種有效的方法，該方法透過最佳化網路傳輸來提升影像品質。將 X 光影像預處理成低解析度檔案，透過經過闡明和改良的 ESRGAN 版本 Real-ESRGAN，有助於降低伺服器負載和傳輸頻寬。低解析度影像在接收端使用針對真實世界影像劣化進行微調的 Real-ESRGAN 升級。該模型整合了殘差中殘差密集區塊與感知和對抗損失函數，以產生雜訊低的高品質升級影像。我們進一步微調 Real-ESRGAN，使其適應特定黑白雜訊和對比特徵。這抑制了雜訊偽影，同時不影響細節。進行的比較評估顯示，除了降低網路頻寬需求外，我們的做法在雜訊降低和細節清晰度方面都優於最先進的基於 CNN 和 ESRGAN 的模型。這些優點已透過定量指標（包括峰值信噪比和結構相似性指標）和定性評估得到證實，這表明 Real-ESRGAN 具有診斷品質 X 光影像和有效醫療資料傳輸的潛力。

##### **Conversational Medical AI: Ready for Practice**
2411.12808v1 by Antoine Lizée, Pierre-Auguste Beaucoté, James Whitbeck, Marion Doumeingts, Anaël Beaugnon, Isabelle Feldhaus

The shortage of doctors is creating a critical squeeze in access to medical
expertise. While conversational Artificial Intelligence (AI) holds promise in
addressing this problem, its safe deployment in patient-facing roles remains
largely unexplored in real-world medical settings. We present the first
large-scale evaluation of a physician-supervised LLM-based conversational agent
in a real-world medical setting.
  Our agent, Mo, was integrated into an existing medical advice chat service.
Over a three-week period, we conducted a randomized controlled experiment with
926 cases to evaluate patient experience and satisfaction. Among these, Mo
handled 298 complete patient interactions, for which we report
physician-assessed measures of safety and medical accuracy.
  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <
0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with
AI-assisted conversations compared to standard care, while showing equivalent
levels of trust and perceived empathy. The high opt-in rate (81% among
respondents) exceeded previous benchmarks for AI acceptance in healthcare.
Physician oversight ensured safety, with 95% of conversations rated as "good"
or "excellent" by general practitioners experienced in operating a medical
advice chat service.
  Our findings demonstrate that carefully implemented AI medical assistants can
enhance patient experience while maintaining safety standards through physician
supervision. This work provides empirical evidence for the feasibility of AI
deployment in healthcare communication and insights into the requirements for
successful integration into existing healthcare services.

摘要：<paragraph>醫生短缺正在造成取得醫療專業知識的嚴重擠壓。儘管對話式人工智慧 (AI) 有望解決此問題，但在現實世界的醫療環境中，其在面對患者的角色中的安全部署仍未得到充分探討。我們提出在現實世界的醫療環境中，對基於 LLM 的醫師監督對話代理進行首次大規模評估。
我們的代理 Mo 已整合到現有的醫療諮詢聊天服務中。在三週的時間裡，我們進行了一項隨機對照實驗，包含 926 個案例，以評估患者體驗和滿意度。其中，Mo 處理了 298 次完整的患者互動，我們報告了醫師評估的安全性和醫療準確性指標。
與標準照護相比，患者報告了更高的資訊清晰度（4 分中的 3.73 對 3.62，p < 0.05）和整體滿意度（5 分中的 4.58 對 4.42，p < 0.05），同時顯示出同等的信任度和同理心。高選擇參與率（受訪者中為 81%）超過了醫療保健中 AI 接受度的先前基準。醫師監督確保了安全性，95% 的對話被經驗豐富的醫療諮詢聊天服務操作員評為「良好」或「極佳」。
我們的研究結果表明，仔細實施的 AI 醫療助理可以在維持醫師監督下的安全標準的同時，提升患者體驗。這項工作為 AI 部署在醫療保健溝通中的可行性提供了實證，並深入了解了成功整合到現有醫療保健服務中的要求。</paragraph>

##### **Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs**
2411.12712v1 by Ahmed Akib Jawad Karim, Muhammad Zawad Mahmud, Samiha Islam, Aznur Azam

In this research, we explored the improvement in terms of multi-class disease
classification via pre-trained language models over Medical-Abstracts-TC-Corpus
that spans five medical conditions. We excluded non-cancer conditions and
examined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and
BERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained
on medical data, demonstrated superior performance in medical text
classification (97% accuracy). Surprisingly, XLNet followed closely (96%
accuracy), demonstrating its generalizability across domains even though it was
not pre-trained on medical data. LastBERT, a custom model based on the lighter
version of BERT, also proved competitive with 87.10% accuracy (just under
BERT's 89.33%). Our findings confirm the importance of specialized models such
as BioBERT and also support impressions around more general solutions like
XLNet and well-tuned transformer architectures with fewer parameters (in this
case, LastBERT) in medical domain tasks.

摘要：在這項研究中，我們探討了透過預先訓練的語言模型在跨越五種醫療疾病的 Medical-Abstracts-TC-Corpus 上，多類疾病分類的改進。我們排除了非癌症疾病，並檢查了四種特定疾病。我們評估了四個 LLM，BioBERT、XLNet 和 BERT，以及一個新的基礎模型 (Last-BERT)。在醫學文本分類中，經過醫學資料預先訓練的 BioBERT 表現出優異的效能（97% 準確度）。令人驚訝的是，XLNet 緊隨其後（96% 準確度），展示了它在不同領域的概括能力，即使它不是在醫學資料上預先訓練的。LastBERT 是一個基於較輕版本的 BERT 的自訂模型，也證明了競爭力，準確度為 87.10%（僅低於 BERT 的 89.33%）。我們的發現證實了 BioBERT 等專用模型的重要性，也支持了對更通用的解決方案的印象，例如 XLNet 和在醫學領域任務中具有較少參數的微調Transformer架構（在本例中為 LastBERT）。

##### **AI Guided Early Screening of Cervical Cancer**
2411.12681v1 by Dharanidharan S I, Suhitha Renuka S V, Ajishi Singh, Sheena Christabel Pravin

In order to support the creation of reliable machine learning models for
anomaly detection, this project focuses on preprocessing, enhancing, and
organizing a medical imaging dataset. There are two classifications in the
dataset: normal and abnormal, along with extra noise fluctuations. In order to
improve the photographs' quality, undesirable artifacts, including visible
medical equipment at the edges, were eliminated using central cropping.
Adjusting the brightness and contrast was one of the additional preprocessing
processes. Normalization was then performed to normalize the data. To make
classification jobs easier, the dataset was methodically handled by combining
several image subsets into two primary categories: normal and pathological. To
provide a strong training set that adapts well to real-world situations,
sophisticated picture preprocessing techniques were used, such as contrast
enhancement and real-time augmentation (including rotations, zooms, and
brightness modifications). To guarantee efficient model evaluation, the data
was subsequently divided into training and testing subsets. In order to create
precise and effective machine learning models for medical anomaly detection,
high-quality input data is ensured via this thorough approach. Because of the
project pipeline's flexible and scalable design, it can be easily integrated
with bigger clinical decision-support systems.

摘要：<paragraph>為了支持建立用於異常偵測的可靠機器學習模型，此專案專注於預處理、增強和組織醫學影像資料集。資料集中有兩個分類：正常和異常，以及額外的雜訊波動。為了提高照片的品質，包括邊緣可見的醫療設備在內的不可取的人工製品已使用中央裁切予以消除。調整亮度和對比度是額外預處理程序之一。然後執行正規化以正規化資料。為了使分類工作更輕鬆，資料集透過將多個影像子集組合成兩個主要類別（正常和病理）來有條理地處理。為了提供一個能良好適應真實世界情況的強大訓練集，使用了先進的圖片預處理技術，例如對比增強和即時擴充（包括旋轉、縮放和亮度修改）。為了保證有效的模型評估，資料隨後被分為訓練和測試子集。為了建立用於醫學異常偵測的精確且有效的機器學習模型，透過此徹底的方法確保了高品質的輸入資料。由於專案管線的靈活且可擴充的設計，它可以輕鬆地整合到更大的臨床決策支援系統中。</paragraph>

##### **Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers**
2411.12678v1 by Devakumar GR, JB Kaarthikeyan, Dominic Immanuel T, Sheena Christabel Pravin

Understanding the appropriate skin layer thickness in wounded sites is an
important tool to move forward on wound healing practices and treatment
protocols. Methods to measure depth often are invasive and less specific. This
paper introduces a novel method that is non-invasive with deep learning
techniques using classifying of skin layers that helps in measurement of wound
depth through heatmap analysis. A set of approximately 200 labeled images of
skin allows five classes to be distinguished: scars, wounds, and healthy skin,
among others. Each image has annotated key layers, namely the stratum cornetum,
the epidermis, and the dermis, in the software Roboflow. In the preliminary
stage, the Heatmap generator VGG16 was used to enhance the visibility of tissue
layers, based upon which their annotated images were used to train ResNet18
with early stopping techniques. It ended up at a very high accuracy rate of
97.67%. To do this, the comparison of the models ResNet18, VGG16, DenseNet121,
and EfficientNet has been done where both EfficientNet and ResNet18 have
attained accuracy rates of almost 95.35%. For further hyperparameter tuning,
EfficientNet and ResNet18 were trained at six different learning rates to
determine the best model configuration. It has been noted that the accuracy has
huge variations with different learning rates. In the case of EfficientNet, the
maximum achievable accuracy was 95.35% at the rate of 0.0001. The same was true
for ResNet18, which also attained its peak value of 95.35% at the same rate.
These facts indicate that the model can be applied and utilized in actual-time,
non-invasive wound assessment, which holds a great promise to improve clinical
diagnosis and treatment planning.

摘要：了解傷口部位適當的皮膚層厚度，是推動傷口癒合實務和治療方案的重要工具。測量深度的方法通常具有侵入性且不夠具體。本文介紹一種非侵入性的新方法，使用深度學習技術對皮膚層進行分類，有助於透過熱圖分析測量傷口深度。一組約 200 張標記的皮膚影像，可區分為五類：疤痕、傷口和健康皮膚等。每張影像在 Roboflow 軟體中都標註了關鍵層，即角質層、表皮和真皮。在初步階段，使用熱圖產生器 VGG16 來增強組織層的可見度，根據其標註的影像用於訓練 ResNet18，並採用早期停止技術。最終達到非常高的準確率 97.67%。為此，對 ResNet18、VGG16、DenseNet121 和 EfficientNet 進行了模型比較，其中 EfficientNet 和 ResNet18 都達到了接近 95.35% 的準確率。為了進一步調整超參數，以六種不同的學習率訓練 EfficientNet 和 ResNet18，以確定最佳模型配置。已注意到準確率會隨著不同的學習率而有很大的變化。在 EfficientNet 的情況下，在 0.0001 的速率下，可達到的最大準確率為 95.35%。ResNet18 也是如此，在相同的速率下也達到了 95.35% 的峰值。這些事實表明，該模型可以應用於實際時間的非侵入性傷口評估中，這對改善臨床診斷和治療計畫具有很大的前景。

##### **DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Image Segmentation**
2411.12350v1 by Bingli Wang, Houcheng Su, Nan Yin, Mengzhu Wang, Li Shen

As a technique to alleviate the pressure of data annotation, semi-supervised
learning (SSL) has attracted widespread attention. In the specific domain of
medical image segmentation, semi-supervised methods (SSMIS) have become a
research hotspot due to their ability to reduce the need for large amounts of
precisely annotated data. SSMIS focuses on enhancing the model's generalization
performance by leveraging a small number of labeled samples and a large number
of unlabeled samples. The latest sharpness-aware optimization (SAM) technique,
which optimizes the model by reducing the sharpness of the loss function, has
shown significant success in SSMIS. However, SAM and its variants may not fully
account for the distribution differences between different datasets. To address
this issue, we propose a sharpness-aware optimization method based on
$f$-divergence minimization (DiM) for semi-supervised medical image
segmentation. This method enhances the model's stability by fine-tuning the
sensitivity of model parameters and improves the model's adaptability to
different datasets through the introduction of $f$-divergence. By reducing
$f$-divergence, the DiM method not only improves the performance balance
between the source and target datasets but also prevents performance
degradation due to overfitting on the source dataset.

摘要：作為一種減輕資料標註壓力的技術，半監督式學習 (SSL) 已廣受關注。在醫學影像分割的特定領域中，半監督式方法 (SSMIS) 由於能夠減少對大量精確標註資料的需求而成為研究熱點。SSMIS 專注於透過利用少數標籤樣本和大量未標籤樣本來增強模型的泛化效能。最新的銳利度感知最佳化 (SAM) 技術透過降低損失函數的銳利度來最佳化模型，已在 SSMIS 中展現顯著的成功。然而，SAM 及其變體可能無法完全考量不同資料集之間的分布差異。為了解決此問題，我們提出了一種基於 $f$-散度最小化 (DiM) 的銳利度感知最佳化方法，用於半監督式醫學影像分割。此方法透過微調模型參數的敏感度並透過引入 $f$-散度來改善模型對不同資料集的適應性，進而增強模型的穩定性。透過降低 $f$-散度，DiM 方法不僅改善了來源資料集和目標資料集之間的效能平衡，還防止了因過度擬合來源資料集而導致的效能下降。

##### **Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment**
2411.13599v1 by Shuoling Liu, Gaoguo Jia, Yuhang Jiang, Liyuan Chen, Qiang Yang

Large Language Models (LLMs) have achieved remarkable success recently,
displaying exceptional capabilities in creating understandable and organized
text. These LLMs have been utilized in diverse fields, such as clinical
research, where domain-specific models like Med-Palm have achieved human-level
performance. Recently, researchers have employed advanced prompt engineering to
enhance the general reasoning ability of LLMs. Despite the remarkable success
of zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the
potential of these methods still remains paid limited attention in the
financial reasoning task.To address this issue, we explore multiple prompt
strategies and incorporated semantic news information to improve LLMs'
performance on financial reasoning tasks.To the best of our knowledge, we are
the first to explore this important issue by applying ChatGPT to the gold
investment.In this work, our aim is to investigate the financial reasoning
capabilities of LLMs and their capacity to generate logical and persuasive
investment opinions. We will use ChatGPT, one of the most powerful LLMs
recently, and prompt engineering to achieve this goal. Our research will focus
on understanding the ability of LLMs in sophisticated analysis and reasoning
within the context of investment decision-making. Our study finds that ChatGPT
with CoT prompt can provide more explainable predictions and overcome
behavioral biases, which is crucial in finance-related tasks and can achieve
higher investment returns.

摘要：大型語言模型 (LLM) 近期已取得顯著成功，在建立可理解且有條理的文本方面展現出非凡的能力。這些 LLM 已運用於不同的領域，例如臨床研究，其中特定領域的模型（例如 Med-Palm）已達到人類等級的表現。最近，研究人員採用進階提示工程來提升 LLM 的一般推理能力。儘管零次學習思考鏈 (CoT) 在解決一般推理任務方面取得顯著成功，但這些方法的潛力在財務推理任務中仍未受到足夠的關注。為了解決這個問題，我們探討多種提示策略，並納入語義新聞資訊來提升 LLM 在財務推理任務上的表現。據我們所知，我們是第一個透過將 ChatGPT 應用於黃金投資來探討這個重要問題的人。在這項工作中，我們的目標是調查 LLM 的財務推理能力，以及它們產生合乎邏輯且有說服力的投資意見的能力。我們將使用 ChatGPT（最近最强大的 LLM 之一）和提示工程來達成這個目標。我們的研究將專注於了解 LLM 在投資決策制定脈絡中進行複雜分析和推理的能力。我們的研究發現，具備 CoT 提示的 ChatGPT 可以提供更具說明性的預測，並克服行為偏差（這在與財務相關的任務中至關重要），並能獲得更高的投資報酬。

##### **StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model**
2411.14476v1 by Zongrong Li, Junhao Xu, Siqin Wang, Yifan Wu, Haiyang Li

Geospatial predictions are crucial for diverse fields such as disaster
management, urban planning, and public health. Traditional machine learning
methods often face limitations when handling unstructured or multi-modal data
like street view imagery. To address these challenges, we propose
StreetViewLLM, a novel framework that integrates a large language model with
the chain-of-thought reasoning and multimodal data sources. By combining street
view imagery with geographic coordinates and textual data, StreetViewLLM
improves the precision and granularity of geospatial predictions. Using
retrieval-augmented generation techniques, our approach enhances geographic
information extraction, enabling a detailed analysis of urban environments. The
model has been applied to seven global cities, including Hong Kong, Tokyo,
Singapore, Los Angeles, New York, London, and Paris, demonstrating superior
performance in predicting urban indicators, including population density,
accessibility to healthcare, normalized difference vegetation index, building
height, and impervious surface. The results show that StreetViewLLM
consistently outperforms baseline models, offering improved predictive accuracy
and deeper insights into the built environment. This research opens new
opportunities for integrating the large language model into urban analytics,
decision-making in urban planning, infrastructure management, and environmental
monitoring.

摘要：地理空間預測對於各個領域至關重要，例如災害管理、都市規劃和公共衛生。傳統機器學習方法在處理非結構化或多模態資料（例如街景影像）時，通常會面臨限制。為了應對這些挑戰，我們提出了 StreetViewLLM，這是一個創新的架構，它將大型語言模型與思考鏈推理和多模態資料來源整合在一起。透過結合街景影像、地理座標和文字資料，StreetViewLLM 提升了地理空間預測的精準度和詳細程度。我們的做法使用了檢索增強生成技術，增強了地理資訊萃取，能詳細分析都市環境。這個模型已經應用於七個全球城市，包括香港、東京、新加坡、洛杉磯、紐約、倫敦和巴黎，在預測都市指標（包括人口密度、醫療保健可及性、正規化差異植被指數、建築物高度和不透水表面）方面展現出優異的效能。結果顯示 StreetViewLLM 持續優於基準模型，提供了更好的預測準確度，並對已建成的環境有更深入的見解。這項研究為將大型語言模型整合到都市分析、都市規劃的決策制定、基礎設施管理和環境監控中，開啟了新的契機。

##### **Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification**
2411.12222v1 by Mingsen Du, Meng Chen, Yongjian Li, Xiuxin Zhang, Jiahui Gao, Cun Ji, Shoushui Wei

Multivariate time series (MTS) data is generated through multiple sensors
across various domains such as engineering application, health monitoring, and
the internet of things, characterized by its temporal changes and high
dimensional characteristics. Over the past few years, many studies have
explored the long-range dependencies and similarities in MTS. However,
long-range dependencies are difficult to model due to their temporal changes
and high dimensionality makes it difficult to obtain similarities effectively
and efficiently. Thus, to address these issues, we propose contrast
similarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).
Firstly, to obtain the dynamic similarity of each sample, we initially use
temporal contrast learning module to acquire MTS representations. And then we
construct a similarity matrix between MTS representations using Fast Dynamic
Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the
bidirectional nature of MTS, allowing us to better capture long-range and
short-range dependencies within the data. Finally, we utilize the
Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the
information interaction in the matrix and MTS node classification task. By
comprehensively considering the long-range dependencies and dynamic similarity
features, we achieved precise MTS node classification. We conducted experiments
on multiple University of East Anglia (UEA) MTS datasets, which encompass
diverse application scenarios. Our results demonstrate the superiority of our
method through both supervised and semi-supervised experiments on the MTS
classification task.

摘要：多變量時間序列 (MTS) 資料是透過多個感測器在各種領域中產生的，例如工程應用、健康監測和物聯網，其特徵在於其時間變化和高維度特徵。在過去幾年中，許多研究探索了 MTS 中的長程依賴性和相似性。然而，由於時間變化，長程依賴性難以建模，而高維度性使得難以有效且有效地取得相似性。因此，為了解決這些問題，我們提出對比相似度感知雙路徑 Mamba 進行 MTS 節點分類 (CS-DPMamba)。首先，為了取得每個樣本的動態相似度，我們最初使用時間對比學習模組來取得 MTS 表徵。然後，我們使用快速動態時間扭曲 (FastDTW) 在 MTS 表徵之間建立相似矩陣。其次，我們應用 DPMamba 來考量 MTS 的雙向性質，讓我們能夠在資料中更好地擷取長程和短程依賴性。最後，我們利用 Kolmogorov-Arnold 網路增強圖同構網路來完成矩陣中的資訊互動和 MTS 節點分類任務。透過全面考量長程依賴性和動態相似性特徵，我們達到了精確的 MTS 節點分類。我們在多個東英格蘭大學 (UEA) MTS 資料集上進行了實驗，這些資料集涵蓋了不同的應用場景。我們的結果透過 MTS 分類任務上的監督式和半監督式實驗證明了我們方法的優越性。

##### **CCIS-Diff: A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis**
2411.12198v1 by Yifan Xie, Jingge Wang, Tao Feng, Fei Ma, Yang Li

Colonoscopy is crucial for identifying adenomatous polyps and preventing
colorectal cancer. However, developing robust models for polyp detection is
challenging by the limited size and accessibility of existing colonoscopy
datasets. While previous efforts have attempted to synthesize colonoscopy
images, current methods suffer from instability and insufficient data
diversity. Moreover, these approaches lack precise control over the generation
process, resulting in images that fail to meet clinical quality standards. To
address these challenges, we propose CCIS-DIFF, a Controlled generative model
for high-quality Colonoscopy Image Synthesis based on a Diffusion architecture.
Our method offers precise control over both the spatial attributes (polyp
location and shape) and clinical characteristics of polyps that align with
clinical descriptions. Specifically, we introduce a blur mask weighting
strategy to seamlessly blend synthesized polyps with the colonic mucosa, and a
text-aware attention mechanism to guide the generated images to reflect
clinical characteristics. Notably, to achieve this, we construct a new
multi-modal colonoscopy dataset that integrates images, mask annotations, and
corresponding clinical text descriptions. Experimental results demonstrate that
our method generates high-quality, diverse colonoscopy images with fine control
over both spatial constraints and clinical consistency, offering valuable
support for downstream segmentation and diagnostic tasks.

摘要：結腸鏡檢查對於腺瘤性息肉的辨識與預防大腸直腸癌至關重要。然而，由於現有結腸鏡檢查資料集的規模與取得不易，開發穩健的息肉偵測模型極具挑戰性。雖然先前的研究已嘗試合成結腸鏡影像，但目前的方法存在不穩定與資料多樣性不足的問題。此外，這些方法對於生成過程缺乏精確的控制，導致影像無法達到臨床品質標準。為了應對這些挑戰，我們提出 CCIS-DIFF，一種基於擴散架構的高品質結腸鏡影像合成受控生成模型。我們的方法能精確控制息肉的空間屬性（息肉位置與形狀）與臨床特徵，並與臨床描述相符。具體來說，我們引入模糊遮罩加權策略，以無縫地將合成的息肉與結腸黏膜融合，並使用文字感知注意力機制來引導生成的影像反映臨床特徵。值得注意的是，為了達成此目標，我們建構了一個新的多模式結腸鏡檢查資料集，其中整合了影像、遮罩標註與對應的臨床文字描述。實驗結果證明，我們的方法能生成高品質、多樣化的結腸鏡影像，並能精細地控制空間約束與臨床一致性，為下游分割與診斷任務提供有價值的支援。

##### **Leveraging Gene Expression Data and Explainable Machine Learning for Enhanced Early Detection of Type 2 Diabetes**
2411.14471v1 by Aurora Lithe Roy, Md Kamrul Siam, Nuzhat Noor Islam Prova, Sumaiya Jahan, Abdullah Al Maruf

Diabetes, particularly Type 2 diabetes (T2D), poses a substantial global
health burden, compounded by its associated complications such as
cardiovascular diseases, kidney failure, and vision impairment. Early detection
of T2D is critical for improving healthcare outcomes and optimizing resource
allocation. In this study, we address the gap in early T2D detection by
leveraging machine learning (ML) techniques on gene expression data obtained
from T2D patients. Our primary objective was to enhance the accuracy of early
T2D detection through advanced ML methodologies and increase the model's
trustworthiness using the explainable artificial intelligence (XAI) technique.
Analyzing the biological mechanisms underlying T2D through gene expression
datasets represents a novel research frontier, relatively less explored in
previous studies. While numerous investigations have focused on utilizing
clinical and demographic data for T2D prediction, the integration of molecular
insights from gene expression datasets offers a unique and promising avenue for
understanding the pathophysiology of the disease. By employing six ML
classifiers on data sourced from NCBI's Gene Expression Omnibus (GEO), we
observed promising performance across all models. Notably, the XGBoost
classifier exhibited the highest accuracy, achieving 97%. Our study addresses a
notable gap in early T2D detection methodologies, emphasizing the importance of
leveraging gene expression data and advanced ML techniques.

摘要：糖尿病，尤其是 2 型糖尿病 (T2D)，對全球健康造成重大負擔，其相關併發症（如心血管疾病、腎衰竭和視力受損）更使情況雪上加霜。及早發現 T2D 對於改善醫療保健成果和優化資源配置至關重要。在本研究中，我們透過對從 T2D 患者取得的基因表現數據應用機器學習 (ML) 技術，來解決 T2D 早期檢測的差距。我們的首要目標是透過進階 ML 方法提升 T2D 早期檢測的準確度，並使用可解釋人工智慧 (XAI) 技術來提升模型的可信度。透過基因表現數據集分析 T2D 背後的生物機制，代表了一項新的研究領域，在過去的研究中較少被探討。雖然許多研究專注於利用臨床和人口數據來預測 T2D，但整合來自基因表現數據集的分子見解，為了解疾病的病理生理學提供了獨特且有希望的途徑。我們對來自 NCBI 基因表現總線 (GEO) 的數據採用六種 ML 分類器，觀察到所有模型都有令人滿意的表現。值得注意的是，XGBoost 分類器表現出最高的準確度，達到 97%。我們的研究解決了 T2D 早期檢測方法中一個顯著的差距，強調了利用基因表現數據和進階 ML 技術的重要性。

##### **Medical Video Generation for Disease Progression Simulation**
2411.11943v1 by Xu Cao, Kaizhao Liang, Kuei-Da Liao, Tianren Gao, Wenqian Ye, Jintai Chen, Zhiguang Ding, Jianguo Cao, James M. Rehg, Jimeng Sun

Modeling disease progression is crucial for improving the quality and
efficacy of clinical diagnosis and prognosis, but it is often hindered by a
lack of longitudinal medical image monitoring for individual patients. To
address this challenge, we propose the first Medical Video Generation (MVG)
framework that enables controlled manipulation of disease-related image and
video features, allowing precise, realistic, and personalized simulations of
disease progression. Our approach begins by leveraging large language models
(LLMs) to recaption prompt for disease trajectory. Next, a controllable
multi-round diffusion model simulates the disease progression state for each
patient, creating realistic intermediate disease state sequence. Finally, a
diffusion-based video transition generation model interpolates disease
progression between these states. We validate our framework across three
medical imaging domains: chest X-ray, fundus photography, and skin image. Our
results demonstrate that MVG significantly outperforms baseline models in
generating coherent and clinically plausible disease trajectories. Two user
studies by veteran physicians, provide further validation and insights into the
clinical utility of the generated sequences. MVG has the potential to assist
healthcare providers in modeling disease trajectories, interpolating missing
medical image data, and enhancing medical education through realistic, dynamic
visualizations of disease progression.

摘要：疾病進程建模對於提升臨床診斷和預後的品質和效能至關重要，但通常會受到缺乏針對個別患者的縱向醫學影像監測的阻礙。為了應對此挑戰，我們提出第一個醫學影片生成 (MVG) 架構，它能控制操作與疾病相關的影像和影片特徵，允許精確、逼真且客製化的疾病進程模擬。我們的做法首先利用大型語言模型 (LLM) 來重新標記疾病軌跡的提示。接下來，可控制的多輪擴散模型會模擬每個患者的疾病進程狀態，建立逼真的中間疾病狀態序列。最後，基於擴散的影片轉換生成模型會內插這些狀態之間的疾病進程。我們在三個醫學影像領域驗證了我們的架構：胸部 X 光、眼底攝影和皮膚影像。我們的結果證明，MVG 在生成連貫且臨床上合理的疾病軌跡方面顯著優於基準模型。兩項由資深醫師進行的使用者研究進一步驗證並深入探討了生成序列的臨床效用。MVG 有潛力協助醫療保健提供者建模疾病軌跡、內插遺失的醫學影像資料，並透過逼真、動態的疾病進程視覺化來加強醫學教育。

##### **Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion**
2411.11799v1 by Meng Zhou, Yuxuan Zhang, Xiaolan Xu, Jiayi Wang, Farzad Khalvati

Multimodal medical image fusion is a crucial task that combines complementary
information from different imaging modalities into a unified representation,
thereby enhancing diagnostic accuracy and treatment planning. While deep
learning methods, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly advanced fusion performance, some of the
existing CNN-based methods fall short in capturing fine-grained multiscale and
edge features, leading to suboptimal feature integration. Transformer-based
models, on the other hand, are computationally intensive in both the training
and fusion stages, making them impractical for real-time clinical use.
Moreover, the clinical application of fused images remains unexplored. In this
paper, we propose a novel CNN-based architecture that addresses these
limitations by introducing a Dilated Residual Attention Network Module for
effective multiscale feature extraction, coupled with a gradient operator to
enhance edge detail learning. To ensure fast and efficient fusion, we present a
parameter-free fusion strategy based on the weighted nuclear norm of softmax,
which requires no additional computations during training or inference.
Extensive experiments, including a downstream brain tumor classification task,
demonstrate that our approach outperforms various baseline methods in terms of
visual quality, texture preservation, and fusion speed, making it a possible
practical solution for real-world clinical applications. The code will be
released at https://github.com/simonZhou86/en_dran.

摘要：多模态医学图像融合是一项至关重要的任务，它将来自不同成像方式的互补信息融合到一个统一的表示中，从而提高诊断准确性和治疗计划。虽然深度学习方法，尤其是卷积神经网络 (CNN) 和 Transformer，已经显著提升了融合性能，但一些现有的基于 CNN 的方法在捕捉细粒度多尺度和边缘特征方面存在不足，导致次优特征集成。另一方面，基于 Transformer 的模型在训练和融合阶段计算量很大，这使得它们不适用于实时临床使用。此外，融合图像的临床应用仍未得到探索。在本文中，我们提出了一种新颖的基于 CNN 的架构，通过引入膨胀残差注意力网络模块来解决这些限制，以进行有效的多分辨率特征提取，并结合梯度算子来增强边缘细节学习。为了确保快速而高效的融合，我们提出了一种基于 softmax 的加权核范数的参数化融合策略，它在训练或推理过程中不需要额外的计算。广泛的实验，包括下游脑肿瘤分类任务，表明我们的方法在视觉质量、纹理保留和融合速度方面优于各种基准方法，使其成为现实世界临床应用的可能实用解决方案。代码将在 https://github.com/simonZhou86/en_dran 发布。

##### **Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care**
2411.11774v1 by Jeffrey N. Clark, Matthew Wragg, Emily Nielsen, Miquel Perello-Nieto, Nawid Keshtmand, Michael Ambler, Shiv Sharma, Christopher P. Bourdeaux, Amberly Brigden, Raul Santos-Rodriguez

There is a growing need to understand how digital systems can support
clinical decision-making, particularly as artificial intelligence (AI) models
become increasingly complex and less human-interpretable. This complexity
raises concerns about trustworthiness, impacting safe and effective adoption of
such technologies. Improved understanding of decision-making processes and
requirements for explanations coming from decision support tools is a vital
component in providing effective explainable solutions. This is particularly
relevant in the data-intensive, fast-paced environments of intensive care units
(ICUs). To explore these issues, group interviews were conducted with seven ICU
clinicians, representing various roles and experience levels. Thematic analysis
revealed three core themes: (T1) ICU decision-making relies on a wide range of
factors, (T2) the complexity of patient state is challenging for shared
decision-making, and (T3) requirements and capabilities of AI decision support
systems. We include design recommendations from clinical input, providing
insights to inform future AI systems for intensive care.

摘要：隨著人工智慧 (AI) 模型變得越來越複雜，且越來越難以被人理解，了解數位系統如何支援臨床決策的需求也日益增加。這種複雜性引發了對可信度的疑慮，影響了此類技術的安全且有效採用。改善對決策制定流程的理解，以及對決策支援工具所提供說明的要求，是提供有效可解釋解決方案的重要組成部分。這在資料密集、快節奏的加護病房 (ICU) 環境中特別相關。為了探討這些問題，對七位 ICU 臨床醫師進行了小組訪談，這些醫師代表了不同的角色和經驗層級。主題分析揭露了三個核心主題：(T1) ICU 決策制定依賴於廣泛的因素，(T2) 病患狀態的複雜性對共同決策制定構成挑戰，以及 (T3) AI 決策支援系統的要求和能力。我們納入了臨床輸入的設計建議，提供見解以提供資訊給未來用於加護的 AI 系統。

##### **SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation**
2411.11636v1 by Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song

Deep learning-based medical image segmentation helps assist diagnosis and
accelerate the treatment process while the model training usually requires
large-scale dense annotation datasets. Weakly semi-supervised medical image
segmentation is an essential application because it only requires a small
amount of scribbles and a large number of unlabeled data to train the model,
which greatly reduces the clinician's effort to fully annotate images. To
handle the inadequate supervisory information challenge in weakly
semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label
(SP${}^3$) learning method is proposed, using the structural information
contained in superpixel for supplemental information. Specifically, the
annotation of scribbles is propagated to superpixels and thus obtains a dense
annotation for supervised training. Since the quality of pseudo-labels is
limited by the low-quality annotation, the beneficial superpixels selected by
dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to
alleviate the negative impact of noise in pseudo-label, superpixel-level
uncertainty is incorporated to guide the pseudo-label supervision for stable
learning. Our method achieves state-of-the-art performance on both tumor and
organ segmentation datasets under the WSSS setting, using only 3\% of the
annotation workload compared to fully supervised methods and attaining
approximately 80\% Dice score. Additionally, our method outperforms eight
weakly and semi-supervised methods under both weakly supervised and
semi-supervised settings. Results of extensive experiments validate the
effectiveness and annotation efficiency of our weakly semi-supervised
segmentation, which can assist clinicians in achieving automated segmentation
for organs or tumors quickly and ultimately benefit patients.

摘要：<paragraph>基於深度學習的醫學影像分割有助於診斷並加速治療過程，而模型訓練通常需要大規模密集標註的資料集。弱半監督醫學影像分割是一項重要的應用，因為它只需要少量的塗鴉和大量的未標註資料來訓練模型，這大大減少了臨床醫生完全標註影像的工作量。為了應對弱半監督分割 (WSSS) 中監督資訊不足的挑戰，提出了一種超像素傳播偽標籤 (SP${}^3$) 學習方法，利用超像素中包含的結構資訊作為補充資訊。具體來說，將塗鴉的標註傳播到超像素，從而獲得用於監督訓練的密集標註。由於偽標籤的品質受到低品質標註的限制，因此使用動態閾值選取的有利超像素來精緻偽標籤。此外，為了減輕偽標籤中雜訊的負面影響，將超像素層級的不確定性納入其中，以指導偽標籤監督以進行穩定的學習。我們的模型在 WSSS 設定下，在腫瘤和器官分割資料集上都達到了最先進的效能，與完全監督的方法相比，只使用了 3% 的標註工作量，並達到了約 80% 的 Dice 分數。此外，我們的模型在弱監督和半監督設定下都優於八種弱監督和半監督方法。廣泛實驗的結果驗證了我們弱半監督分割的有效性和標註效率，這可以協助臨床醫生快速實現器官或腫瘤的自動分割，並最終使患者受益。</paragraph>

##### **HistoEncoder: a digital pathology foundation model for prostate cancer**
2411.11458v2 by Joona Pohjonen, Abderrahim-Oussama Batouche, Antti Rannikko, Kevin Sandeman, Andrew Erickson, Esa Pitkanen, Tuomas Mirtti

Foundation models are trained on massive amounts of data to distinguish
complex patterns and can be adapted to a wide range of downstream tasks with
minimal computational resources. Here, we develop a foundation model for
prostate cancer digital pathology called HistoEncoder by pre-training on 48
million prostate tissue tile images. We demonstrate that HistoEncoder features
extracted from tile images with similar histological patterns map closely
together in the feature space. HistoEncoder outperforms models pre-trained with
natural images, even without fine-tuning or with 1000 times less training data.
We describe two use cases that leverage the capabilities of HistoEncoder by
fine-tuning the model with a limited amount of data and computational
resources. First, we show how HistoEncoder can be used to automatically
annotate large-scale datasets with high accuracy. Second, we combine histomics
with commonly used clinical nomograms, significantly improving prostate
cancer-specific death survival models. Foundation models such as HistoEncoder
can allow organizations with limited resources to build effective clinical
software tools without needing extensive datasets or significant amounts of
computing.

摘要：基礎模型會在大量資料上訓練，以區分複雜模式，並能以最少的運算資源適應各種下游任務。在此，我們開發了一個名為 HistoEncoder 的攝護腺癌數位病理基礎模型，方法是在 4800 萬個攝護腺組織切片影像上進行預訓練。我們示範了從具有類似組織學模式的切片影像中萃取的 HistoEncoder 特徵，會在特徵空間中緊密地相互對應。即使沒有微調或訓練資料少 1000 倍，HistoEncoder 的表現也優於使用自然影像進行預訓練的模型。我們描述了兩個使用案例，它們利用 HistoEncoder 的功能，以有限的資料和運算資源微調模型。首先，我們展示了如何使用 HistoEncoder 自動為大型資料集加上註解，並達到很高的準確度。其次，我們將組織學與常用的臨床列線圖結合，大幅改善了攝護腺癌特定死亡存活模型。像 HistoEncoder 這樣的基礎模型，能讓資源有限的組織建立有效的臨床軟體工具，而不需要廣泛的資料集或大量的運算。

##### **TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation**
2411.11305v2 by Ranmin Wang, Limin Zhuang, Hongkun Chen, Boyan Xu, Ruichu Cai

The advancement of medical image segmentation techniques has been propelled
by the adoption of deep learning techniques, particularly UNet-based
approaches, which exploit semantic information to improve the accuracy of
segmentations. However, the order of organs in scanned images has been
disregarded by current medical image segmentation approaches based on UNet.
Furthermore, the inherent network structure of UNet does not provide direct
capabilities for integrating temporal information. To efficiently integrate
temporal information, we propose TP-UNet that utilizes temporal prompts,
encompassing organ-construction relationships, to guide the segmentation UNet
model. Specifically, our framework is featured with cross-attention and
semantic alignment based on unsupervised contrastive learning to combine
temporal prompts and image features effectively. Extensive evaluations on two
medical image segmentation datasets demonstrate the state-of-the-art
performance of TP-UNet. Our implementation will be open-sourced after
acceptance.

摘要：醫療影像分割技術的進步已受到深度學習技術的採用所推動，特別是基於 UNet 的方法，它利用語義資訊來提高分割的準確性。然而，當前基於 UNet 的醫學影像分割方法忽略了掃描影像中器官的順序。此外，UNet 的固有網路結構無法直接整合時間資訊。為了有效整合時間資訊，我們提出了 TP-UNet，它利用時間提示，包含器官建構關係，來引導分割 UNet 模型。具體來說，我們的框架以無監督對比學習為基礎，具有交叉注意和語義對齊，以有效結合時間提示和影像特徵。在兩個醫學影像分割資料集上的廣泛評估證明了 TP-UNet 的最先進效能。我們的實作將在接受後開源。

##### **Deep learning waterways for rural infrastructure development**
2411.13590v1 by Matthew Pierson, Zia Mehrabi

Surprisingly a number of Earth's waterways remain unmapped, with a
significant number in low and middle income countries. Here we build a computer
vision model (WaterNet) to learn the location of waterways in the United
States, based on high resolution satellite imagery and digital elevation
models, and then deploy this in novel environments in the African continent.
Our outputs provide detail of waterways structures hereto unmapped. When
assessed against community needs requests for rural bridge building related to
access to schools, health care facilities and agricultural markets, we find
these newly generated waterways capture on average 93% (country range: 88-96%)
of these requests whereas Open Street Map, and the state of the art data from
TDX-Hydro, capture only 36% (5-72%) and 62% (37%-85%), respectively. Because
these new machine learning enabled maps are built on public and operational
data acquisition this approach offers promise for capturing humanitarian needs
and planning for social development in places where cartographic efforts have
so far failed to deliver. The improved performance in identifying community
needs missed by existing data suggests significant value for rural
infrastructure development and better targeting of development interventions.

摘要：令人驚訝的是，地球上的許多水道仍然未繪製，其中有大量低收入和中等收入國家。在這裡，我們建立了一個電腦視覺模型（WaterNet）來學習美國水道的所在位置，該模型基於高解析度衛星影像和數位高程模型，然後將其部署到非洲大陸的新環境中。我們的輸出提供了迄今未繪製的水道結構的詳細資訊。在根據社區需求評估與學校、醫療保健設施和農業市場的通路相關的農村橋樑建設請求時，我們發現這些新生成的航道平均涵蓋了這些請求的 93%（國家範圍：88-96%），而 Open Street Map 和 TDX-Hydro 中最先進的資料分別只涵蓋了 36%（5-72%）和 62%（37%-85%）。因為這些新的機器學習啟用地圖建立在公開和運作的資料採集中，所以這種方法有望捕捉人道主義需求，並為製圖工作迄今未能提供的地區的社會發展進行規劃。在識別現有資料遺漏的社區需求方面表現的改善，顯示出對農村基礎設施開發和更佳的發展干預目標具有重大價值。

##### **Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development**
2411.11285v1 by Ranjan Sapkota, Achyut Paudel, Manoj Karkee

Currently, deep learning-based instance segmentation for various applications
(e.g., Agriculture) is predominantly performed using a labor-intensive process
involving extensive field data collection using sophisticated sensors, followed
by careful manual annotation of images, presenting significant logistical and
financial challenges to researchers and organizations. The process also slows
down the model development and training process. In this study, we presented a
novel method for deep learning-based instance segmentation of apples in
commercial orchards that eliminates the need for labor-intensive field data
collection and manual annotation. Utilizing a Large Language Model (LLM), we
synthetically generated orchard images and automatically annotated them using
the Segment Anything Model (SAM) integrated with a YOLO11 base model. This
method significantly reduces reliance on physical sensors and manual data
processing, presenting a major advancement in "Agricultural AI". The synthetic,
auto-annotated dataset was used to train the YOLO11 model for Apple instance
segmentation, which was then validated on real orchard images. The results
showed that the automatically generated annotations achieved a Dice Coefficient
of 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask
annotations. All YOLO11 configurations, trained solely on these synthetic
datasets with automated annotations, accurately recognized and delineated
apples, highlighting the method's efficacy. Specifically, the YOLO11m-seg
configuration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on
test images collected from a commercial orchard. Additionally, the YOLO11l-seg
configuration outperformed other models in validation on 40 LLM-generated
images, achieving the highest mask precision and mAP@50 metrics.
  Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM

摘要：<paragraph>目前，針對各種應用（例如農業）的深度學習實例分割，主要透過勞力密集的程序執行，包括使用精密感測器廣泛收集現場資料，接著仔細手動標註影像，對研究人員和組織而言，這會造成顯著的後勤和財務挑戰。此程序也會減緩模型開發和訓練的過程。在此研究中，我們提出了一種創新的方法，可針對商業果園中的蘋果執行深度學習實例分割，無需勞力密集的現場資料收集和手動標註。我們利用大型語言模型 (LLM) 合成產生果園影像，並使用與 YOLO11 基礎模型整合的 Segment Anything Model (SAM) 自動標註這些影像。這種方法大幅降低對實體感測器和手動資料處理的依賴性，代表「農業 AI」的一大進步。合成自動標註的資料集用於訓練 YOLO11 模型，以進行蘋果實例分割，接著在真實果園影像中驗證。結果顯示，自動產生的標註達到了 0.9513 的 Dice 係數和 0.9303 的 IoU，驗證了遮罩標註的準確性和重疊性。所有 YOLO11 組態僅使用這些具有自動化標註的合成資料集進行訓練，就能準確辨識和描繪蘋果，突顯了此方法的效能。具體來說，YOLO11m-seg 組態在從商業果園收集的測試影像上達到了 0.902 的遮罩準確度和 0.833 的遮罩 mAP@50。此外，YOLO11l-seg 組態在針對 40 張 LLM 生成的影像進行驗證時，優於其他模型，達到了最高的遮罩準確度和 mAP@50 指標。
關鍵字：YOLO、SAM、SAMv2、YOLO11、YOLOv11、Segment Anything、YOLO-SAM</paragraph>

##### **Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction**
2411.11282v1 by Yucong Meng, Zhiwei Yang, Minghong Duan, Yonghong Shi, Zhijian Song

Magnetic resonance imaging (MRI) is a crucial tool for clinical diagnosis
while facing the challenge of long scanning time. To reduce the acquisition
time, fast MRI reconstruction aims to restore high-quality images from the
undersampled k-space. Existing methods typically train deep learning models to
map the undersampled data to artifact-free MRI images. However, these studies
often overlook the unique properties of k-space and directly apply general
networks designed for image processing to k-space recovery, leaving the precise
learning of k-space largely underexplored. In this work, we propose a
continuous k-space recovery network from a new perspective of implicit neural
representation with image domain guidance, which boosts the performance of MRI
reconstruction. Specifically, (1) an implicit neural representation based
encoder-decoder structure is customized to continuously query unsampled
k-values. (2) an image guidance module is designed to mine the semantic
information from the low-quality MRI images to further guide the k-space
recovery. (3) a multi-stage training strategy is proposed to recover dense
k-space progressively. Extensive experiments conducted on CC359, fastMRI, and
IXI datasets demonstrate the effectiveness of our method and its superiority
over other competitors.

摘要：磁共振成像 (MRI) 對於臨床診斷至關重要，但卻面臨掃描時間長的問題。為了縮短擷取時間，快速 MRI 重建旨在從欠採樣 k 空間恢復高品質影像。現有方法通常訓練深度學習模型，將欠採樣資料對應到沒有偽影的 MRI 影像。然而，這些研究常常忽略 k 空間的獨特屬性，並直接套用設計用於影像處理的一般網路到 k 空間重建，導致 k 空間的精確學習在很大程度上仍未被探索。在這項工作中，我們從隱式神經表徵與影像網域引導的新觀點提出一個連續的 k 空間重建網路，提升 MRI 重建的效能。具體來說，(1) 根據隱式神經表徵設計編碼器-解碼器結構，用於連續查詢未採樣的 k 值。(2) 設計一個影像引導模組，從低品質的 MRI 影像中挖掘語義資訊，進一步引導 k 空間重建。(3) 提出一個多階段訓練策略，用於逐步重建密集的 k 空間。在 CC359、fastMRI 和 IXI 資料集上進行的大量實驗證明了我們方法的有效性，以及其優於其他競爭對手的優越性。

##### **F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics**
2411.11912v1 by Pramit Saha, Felix Wagner, Divyanshu Mishra, Can Peng, Anshul Thakur, David Clifton, Konstantinos Kamnitsas, J. Alison Noble

Effective training of large Vision-Language Models (VLMs) on
resource-constrained client devices in Federated Learning (FL) requires the
usage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we
demonstrate the impact of two factors \textit{viz.}, client-specific layer
importance score that selects the most important VLM layers for fine-tuning and
inter-client layer diversity score that encourages diverse layer selection
across clients for optimal VLM layer selection. We first theoretically motivate
and leverage the principal eigenvalue magnitude of layerwise Neural Tangent
Kernels and show its effectiveness as client-specific layer importance score.
Next, we propose a novel layer updating strategy dubbed F$^3$OCUS that jointly
optimizes the layer importance and diversity factors by employing a data-free,
multi-objective, meta-heuristic optimization on the server. We explore 5
different meta-heuristic algorithms and compare their effectiveness for
selecting model layers and adapter layers towards PEFT-FL. Furthermore, we
release a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9
modality-specific clients and utilize it to train and evaluate our method.
Overall, we conduct more than 10,000 client-level experiments on 6
Vision-Language FL task settings involving 58 medical image datasets and 4
different VLM architectures of varying sizes to demonstrate the effectiveness
of the proposed method.

摘要：<paragraph>在聯合學習 (FL) 中，在資源受限的用戶端裝置上有效訓練大型視覺語言模型 (VLM)，需要使用參數有效微調 (PEFT) 策略。為此，我們展示了兩個因素的影響，即客戶端特定層重要性評分，它選擇了最重要的 VLM 層進行微調，以及客戶端間層多樣性評分，它鼓勵在客戶端之間進行不同的層選擇，以實現最佳的 VLM 層選擇。我們首先從理論上激勵並利用層級神經切線核的主特徵值大小，並展示其作為客戶端特定層重要性評分的有效性。接下來，我們提出了一種名為 F$^3$OCUS 的新穎層更新策略，它通過在伺服器上使用無數據、多目標、元啟發式優化，共同優化層重要性和多樣性因素。我們探索了 5 種不同的元啟發式演算法，並比較了它們在選擇模型層和適配器層以進行 PEFT-FL 的有效性。此外，我們發布了一個新的 MedVQA-FL 資料集，其中包含 707,962 個 VQA 三元組和 9 個特定於模式的用戶端，並利用它來訓練和評估我們的模型。總體而言，我們在 6 個視覺語言 FL 任務設置上進行了 10,000 多個用戶端級別的實驗，涉及 58 個醫學影像資料集和 4 個不同大小的 VLM 架構，以證明所提出方法的有效性。</paragraph>

##### **MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records**
2411.11161v1 by Eric Yang, Pengfei Hu, Xiaoxue Han, Yue Ning

The adoption of digital systems in healthcare has resulted in the
accumulation of vast electronic health records (EHRs), offering valuable data
for machine learning methods to predict patient health outcomes. However,
single-visit records of patients are often neglected in the training process
due to the lack of annotations of next-visit information, thereby limiting the
predictive and expressive power of machine learning models. In this paper, we
present a novel framework MPLite that utilizes Multi-aspect Pretraining with
Lab results through a light-weight neural network to enhance medical concept
representation and predict future health outcomes of individuals. By
incorporating both structured medical data and additional information from lab
results, our approach fully leverages patient admission records. We design a
pretraining module that predicts medical codes based on lab results, ensuring
robust prediction by fusing multiple aspects of features. Our experimental
evaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements
over existing models in diagnosis prediction and heart failure prediction
tasks, achieving a higher weighted-F1 and recall with MPLite. This work reveals
the potential of integrating diverse aspects of data to advance predictive
modeling in healthcare.

摘要：數位系統在醫療保健中的採用導致了大量電子健康記錄 (EHR) 的累積，這些記錄提供了有價值的資料，可供機器學習方法用來預測患者的健康結果。然而，由於缺乏下次就診資訊的註解，患者的單次就診記錄在訓練過程中常常被忽略，因此限制了機器學習模型的預測和表達能力。在本文中，我們提出了一個創新的框架 MPLite，它利用透過輕量級神經網路進行多面向預訓練與實驗室結果，來增強醫療概念的表徵並預測個人的未來健康結果。透過結合結構化的醫療資料和來自實驗室結果的額外資訊，我們的做法充分利用了患者的入院記錄。我們設計了一個預訓練模組，根據實驗室結果預測醫療代碼，確保透過融合特徵的各個面向來進行穩健的預測。我們使用 MIMIC-III 和 MIMIC-IV 資料集進行的實驗評估證明，在診斷預測和心臟衰竭預測任務中，我們的模型優於現有的模型，使用 MPLite 達到了更高的加權 F1 和召回率。這項工作揭示了整合資料中不同面向的潛力，以推進醫療保健中的預測建模。

##### **Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks**
2411.11105v1 by Deepa Anand, Bipul Das, Vyshnav Dangeti, Antony Jerald, Rakesh Mullick, Uday Patil, Pakhi Sharma, Prasad Sudhakar

In a setting where segmentation models have to be built for multiple
datasets, each with its own corresponding label set, a straightforward way is
to learn one model for every dataset and its labels. Alternatively, multi-task
architectures with shared encoders and multiple segmentation heads or shared
weights with compound labels can also be made use of. This work proposes a
novel label sharing framework where a shared common label space is constructed
and each of the individual label sets are systematically mapped to the common
labels. This transforms multiple datasets with disparate label sets into a
single large dataset with shared labels, and therefore all the segmentation
tasks can be addressed by learning a single model. This eliminates the need for
task specific adaptations in network architectures and also results in
parameter and data efficient models. Furthermore, label sharing framework is
naturally amenable for incremental learning where segmentations for new
datasets can be easily learnt. We experimentally validate our method on various
medical image segmentation datasets, each involving multi-label segmentation.
Furthermore, we demonstrate the efficacy of the proposed method in terms of
performance and incremental learning ability vis-a-vis alternative methods.

摘要：在必須為多個資料集建立分割模型的設定中，每個資料集都有自己對應的標籤集，一個直接的方法是為每個資料集及其標籤學習一個模型。或者，也可以利用具有共享編碼器和多個分割頭或具有複合標籤的共享權重的多任務架構。這項工作提出了一個新穎的標籤共享框架，其中構建了一個共享的共同標籤空間，並且每個單獨的標籤集都系統性地映射到共同標籤。這將具有不同標籤集的多個資料集轉換為具有共享標籤的單一大型資料集，因此所有分割任務都可以通過學習單一模型來解決。這消除了對網路架構中特定任務適應的需求，並且還產生了參數和資料有效率的模型。此外，標籤共享框架自然適用於增量學習，其中可以輕鬆學習新資料集的分割。我們在涉及多標籤分割的各種醫學影像分割資料集上對我們的模型進行實驗驗證。此外，我們根據效能和增量學習能力證明了所提出模型的有效性，相對於其他方法。

##### **BianCang: A Traditional Chinese Medicine Large Language Model**
2411.11027v1 by Sibo Wei, Xueping Peng, Yi-fei Wang, Jiasheng Si, Weiyu Zhang, Wenpeng Lu, Xiaoming Wu, Yinglong Wang

The rise of large language models (LLMs) has driven significant progress in
medical applications, including traditional Chinese medicine (TCM). However,
current medical LLMs struggle with TCM diagnosis and syndrome differentiation
due to substantial differences between TCM and modern medical theory, and the
scarcity of specialized, high-quality corpora. This paper addresses these
challenges by proposing BianCang, a TCM-specific LLM, using a two-stage
training process that first injects domain-specific knowledge and then aligns
it through targeted stimulation. To enhance diagnostic and differentiation
capabilities, we constructed pre-training corpora, instruction-aligned datasets
based on real hospital records, and the ChP-TCM dataset derived from the
Pharmacopoeia of the People's Republic of China. We compiled extensive TCM and
medical corpora for continuous pre-training and supervised fine-tuning,
building a comprehensive dataset to refine the model's understanding of TCM.
Evaluations across 11 test sets involving 29 models and 4 tasks demonstrate the
effectiveness of BianCang, offering valuable insights for future research.
Code, datasets, and models are available at
https://github.com/QLU-NLP/BianCang.

摘要：大型語言模型 (LLM) 的興起推動了醫療應用領域的重大進展，包括中醫學 (TCM)。然而，由於中醫學與現代醫學理論之間存在著實質性的差異，以及缺乏專業、高品質的語料庫，當前的醫學 LLM 在中醫診斷和證候鑑別方面遇到了困難。本文通過提出 BianCang，一種特定於中醫學的 LLM，來應對這些挑戰，使用一個兩階段訓練過程，首先注入特定領域的知識，然後通過有針對性的刺激來對齊它。為了增強診斷和鑑別能力，我們構建了預訓練語料庫、基於真實醫院記錄的指令對齊數據集，以及源自中華人民共和國藥典的 ChP-TCM 數據集。我們編譯了大量的 TCM 和醫學語料庫，用於持續的預訓練和監督微調，構建了一個全面的數據集來完善模型對 TCM 的理解。涉及 29 個模型和 4 項任務的 11 個測試集的評估證明了 BianCang 的有效性，為未來的研究提供了有價值的見解。程式碼、數據集和模型可在 https://github.com/QLU-NLP/BianCang 獲得。

##### **MpoxVLM: A Vision-Language Model for Diagnosing Skin Lesions from Mpox Virus Infection**
2411.10888v1 by Xu Cao, Wenqian Ye, Kenny Moise, Megan Coffee

In the aftermath of the COVID-19 pandemic and amid accelerating climate
change, emerging infectious diseases, particularly those arising from zoonotic
spillover, remain a global threat. Mpox (caused by the monkeypox virus) is a
notable example of a zoonotic infection that often goes undiagnosed, especially
as its rash progresses through stages, complicating detection across diverse
populations with different presentations. In August 2024, the WHO
Director-General declared the mpox outbreak a public health emergency of
international concern for a second time. Despite the deployment of deep
learning techniques for detecting diseases from skin lesion images, a robust
and publicly accessible foundation model for mpox diagnosis is still lacking
due to the unavailability of open-source mpox skin lesion images, multimodal
clinical data, and specialized training pipelines. To address this gap, we
propose MpoxVLM, a vision-language model (VLM) designed to detect mpox by
analyzing both skin lesion images and patient clinical information. MpoxVLM
integrates the CLIP visual encoder, an enhanced Vision Transformer (ViT)
classifier for skin lesions, and LLaMA-2-7B models, pre-trained and fine-tuned
on visual instruction-following question-answer pairs from our newly released
mpox skin lesion dataset. Our work achieves 90.38% accuracy for mpox detection,
offering a promising pathway to improve early diagnostic accuracy in combating
mpox.

摘要：在 COVID-19 大流行之後，在加速的氣候變遷和新興傳染病中，特別是那些源自人畜共通傳染病的疾病，仍然是一個全球性的威脅。猴痘（由猴痘病毒引起）是一個顯著的人畜共通傳染病感染範例，通常未被診斷出來，特別是隨著其皮疹進展到各個階段，使得在具有不同表現形式的多元族群中進行偵測變得複雜。2024 年 8 月，世界衛生組織總幹事第二次宣布猴痘疫情為國際關注的公共衛生緊急事件。儘管已部署深度學習技術用於從皮膚病灶影像中偵測疾病，但由於缺乏開放原始碼的猴痘皮膚病灶影像、多模式臨床資料和專業的訓練管道，因此仍然缺乏一個穩健且公開可用的猴痘診斷基礎模型。為了解決這個差距，我們提出 MpoxVLM，這是一個視覺語言模型 (VLM)，旨在透過分析皮膚病灶影像和患者臨床資訊來偵測猴痘。MpoxVLM 整合了 CLIP 視覺編碼器、一個針對皮膚病灶增強的視覺轉換器 (ViT) 分類器，以及 LLaMA-2-7B 模型，這些模型經過預先訓練和微調，並根據我們新發布的猴痘皮膚病灶資料集中的視覺指令遵循問題解答配對。我們的研究在猴痘偵測方面達到了 90.38% 的準確度，為提高早期診斷準確度以對抗猴痘提供了有希望的途徑。

##### **Towards Next-Generation Medical Agent: How o1 is Reshaping Decision-Making in Medical Scenarios**
2411.14461v1 by Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, Jin Lu, Wei Zhang, Tuo Zhang, Lu Zhang, Dajiang Zhu, Xiang Li, Wei Liu, Quanzheng Li, Andrea Sikora, Xiaoming Zhai, Zhen Xiang, Tianming Liu

Artificial Intelligence (AI) has become essential in modern healthcare, with
large language models (LLMs) offering promising advances in clinical
decision-making. Traditional model-based approaches, including those leveraging
in-context demonstrations and those with specialized medical fine-tuning, have
demonstrated strong performance in medical language processing but struggle
with real-time adaptability, multi-step reasoning, and handling complex medical
tasks. Agent-based AI systems address these limitations by incorporating
reasoning traces, tool selection based on context, knowledge retrieval, and
both short- and long-term memory. These additional features enable the medical
AI agent to handle complex medical scenarios where decision-making should be
built on real-time interaction with the environment. Therefore, unlike
conventional model-based approaches that treat medical queries as isolated
questions, medical AI agents approach them as complex tasks and behave more
like human doctors. In this paper, we study the choice of the backbone LLM for
medical AI agents, which is the foundation for the agent's overall reasoning
and action generation. In particular, we consider the emergent o1 model and
examine its impact on agents' reasoning, tool-use adaptability, and real-time
information retrieval across diverse clinical scenarios, including high-stakes
settings such as intensive care units (ICUs). Our findings demonstrate o1's
ability to enhance diagnostic accuracy and consistency, paving the way for
smarter, more responsive AI tools that support better patient outcomes and
decision-making efficacy in clinical practice.

摘要：人工智能 (AI) 已成為現代醫療保健中不可或缺的一部分，其中大型語言模型 (LLM) 為臨床決策制定提供了有希望的進展。傳統的基於模型的方法，包括利用情境中的示範和具有專業醫療微調的方法，已證明在醫學語言處理中具有強勁的性能，但在實時適應性、多步驟推理和處理複雜的醫療任務方面存在困難。基於代理的 AI 系統通過整合推理追蹤、基於情境的工具選擇、知識檢索以及短期和長期記憶來解決這些限制。這些額外的功能使醫療 AI 代理能夠處理複雜的醫療場景，在這些場景中，決策制定應建立在與環境的實時互動之上。因此，與將醫療查詢視為孤立問題的傳統基於模型的方法不同，醫療 AI 代理將它們視為複雜的任務，並且更像人類醫生。在本文中，我們研究了醫療 AI 代理的骨幹 LLM 的選擇，這是代理整體推理和動作生成的基礎。特別是，我們考慮了新興的 o1 模型，並檢查了它對代理推理、工具使用適應性和跨不同臨床場景的實時信息檢索的影響，包括重症監護病房 (ICU) 等高風險環境。我們的研究結果證明了 o1 提高診斷準確性和一致性的能力，為更智能、更靈敏的 AI 工具鋪平了道路，這些工具支持更好的患者預後和臨床實踐中的決策制定效率。

##### **A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks**
2411.10843v1 by Pandiyaraju V, Santhosh Malarvannan, Shravan Venkatraman, Abeshek A, Priyadarshini B, Kannan A

Diabetic retinopathy is a leading cause of blindness around the world and
demands precise AI-based diagnostic tools. Traditional loss functions in
multi-class classification, such as Categorical Cross-Entropy (CCE), are very
common but break down with class imbalance, especially in cases with inherently
challenging or overlapping classes, which leads to biased and less sensitive
models. Since a heavy imbalance exists in the number of examples for higher
severity stage 4 diabetic retinopathy, etc., classes compared to those very
early stages like class 0, achieving class balance is key. For this purpose, we
propose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of
focal loss and entropy loss with adaptive weighting in order to focus on
minority classes and highlight the challenging samples. The state-of-the art
models applied for diabetic retinopathy detection with AHFE revealed good
performance improvements, indicating the top performances of ResNet50 at
99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and
InceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes
enhancement in AI-driven diagnostics for complex and imbalanced medical
datasets.

摘要：糖尿病視網膜病變是全球失明的主要原因，需要精準的 AI 診斷工具。多類別分類中的傳統損失函數，例如分類交叉熵 (CCE)，非常常見，但會隨著類別失衡而失效，特別是在類別本身具有挑戰性或重疊的情況下，這會導致有偏差且不敏感的模型。由於嚴重度較高的第 4 期糖尿病視網膜病變等類別的範例數量嚴重失衡，與第 0 期等非常早期的類別相比，達成類別平衡至關重要。為此，我們提出自適應混合焦點熵損失，它結合了焦點損失和熵損失的概念，並採用自適應加權，以專注於少數類別並突顯具有挑戰性的範例。應用於糖尿病視網膜病變偵測的最新模型搭配 AHFE，顯示出良好的效能提升，表示 ResNet50 的最高效能為 99.79%、DenseNet121 為 98.86%、Xception 為 98.92%、MobileNetV2 為 97.84%，以及 InceptionV3 的準確度為 93.62%。這揭示了 AHFE 如何促進以 AI 為主的診斷，以應對複雜且不平衡的醫療資料集。

##### **Decentralizing Test-time Adaptation under Heterogeneous Data Streams**
2411.15173v1 by Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Kaizhu Huang

While Test-Time Adaptation (TTA) has shown promise in addressing distribution
shifts between training and testing data, its effectiveness diminishes with
heterogeneous data streams due to uniform target estimation. As previous
attempts merely stabilize model fine-tuning over time to handle continually
changing environments, they fundamentally assume a homogeneous target domain at
any moment, leaving the intrinsic real-world data heterogeneity unresolved.
This paper delves into TTA under heterogeneous data streams, moving beyond
current model-centric limitations. By revisiting TTA from a data-centric
perspective, we discover that decomposing samples into Fourier space
facilitates an accurate data separation across different frequency levels.
Drawing from this insight, we propose a novel Frequency-based Decentralized
Adaptation (FreDA) framework, which transitions data from globally
heterogeneous to locally homogeneous in Fourier space and employs decentralized
adaptation to manage diverse distribution shifts.Interestingly, we devise a
novel Fourier-based augmentation strategy to assist in decentralizing
adaptation, which individually enhances sample quality for capturing each type
of distribution shifts. Extensive experiments across various settings
(corrupted, natural, and medical environments) demonstrate the superiority of
our proposed framework over the state-of-the-arts.

摘要：雖然測試時間適應 (TTA) 已展現出解決訓練和測試資料之間分佈轉移的潛力，但由於均勻目標估計，其效能會隨著異質資料串流而降低。由於先前的嘗試僅僅穩定模型微調以隨著時間推移處理持續變化的環境，因此它們在任何時刻都假設同質目標網域，讓內在的真實世界資料異質性無法解決。本文深入探討異質資料串流下的 TTA，超越目前以模型為中心的限制。透過從以資料為中心的觀點重新檢視 TTA，我們發現將樣本分解成傅立葉空間有助於在不同頻率層級中精確區分資料。根據此見解，我們提出一個新的基於頻率的分散式適應 (FreDA) 架構，該架構將資料從全球異質轉變為傅立葉空間中的局部同質，並採用分散式適應來管理不同的分佈轉移。有趣的是，我們設計了一個新的基於傅立葉的擴充策略，以協助分散適應，個別增強樣本品質以擷取每種類型的分佈轉移。在各種設定（受損、自然和醫療環境）中進行的廣泛實驗證明了我們提出的架構優於現有技術。

##### **MRI Parameter Mapping via Gaussian Mixture VAE: Breaking the Assumption of Independent Pixels**
2411.10772v1 by Moucheng Xu, Yukun Zhou, Tobias Goodwin-Allcock, Kimia Firoozabadi, Joseph Jacob, Daniel C. Alexander, Paddy J. Slator

We introduce and demonstrate a new paradigm for quantitative parameter
mapping in MRI. Parameter mapping techniques, such as diffusion MRI and
quantitative MRI, have the potential to robustly and repeatably measure
biologically-relevant tissue maps that strongly relate to underlying
microstructure. Quantitative maps are calculated by fitting a model to multiple
images, e.g. with least-squares or machine learning. However, the overwhelming
majority of model fitting techniques assume that each voxel is independent,
ignoring any co-dependencies in the data. This makes model fitting sensitive to
voxelwise measurement noise, hampering reliability and repeatability. We
propose a self-supervised deep variational approach that breaks the assumption
of independent pixels, leveraging redundancies in the data to effectively
perform data-driven regularisation of quantitative maps. We demonstrate that
our approach outperforms current model fitting techniques in dMRI simulations
and real data. Especially with a Gaussian mixture prior, our model enables
sharper quantitative maps, revealing finer anatomical details that are not
presented in the baselines. Our approach can hence support the clinical
adoption of parameter mapping methods such as dMRI and qMRI.

摘要：<paragraph>我們介紹並展示一種新的範例，用於 MRI 中的定量參數對應。參數對應技術，例如擴散 MRI 和定量 MRI，具有強健且可重複測量與底層微結構密切相關的生物相關組織對應的能力。定量對應是透過將模型套用到多個影像來計算，例如使用最小平方或機器學習。然而，絕大多數的模型擬合技術假設每個體素都是獨立的，忽略資料中的任何共依賴性。這使得模型擬合容易受到體素測量雜訊的影響，阻礙了可靠性和可重複性。我們提出一個自我監督的深度變異方法，打破了獨立像素的假設，利用資料中的冗餘來有效執行定量對應的資料驅動正規化。我們證明我們的模型在 dMRI 模擬和真實資料中優於目前的模型擬合技術。特別是使用高斯混合先驗，我們的模型能產生更清晰的定量對應，揭示出基準線中未呈現的更精細解剖細節。因此，我們的模型可以支援 dMRI 和 qMRI 等參數對應方法的臨床採用。</paragraph>

##### **Integrated Machine Learning and Survival Analysis Modeling for Enhanced Chronic Kidney Disease Risk Stratification**
2411.10754v1 by Zachary Dana, Ahmed Ammar Naseer, Botros Toro, Sumanth Swaminathan

Chronic kidney disease (CKD) is a significant public health challenge, often
progressing to end-stage renal disease (ESRD) if not detected and managed
early. Early intervention, warranted by silent disease progression, can
significantly reduce associated morbidity, mortality, and financial burden. In
this study, we propose a novel approach to modeling CKD progression using a
combination of machine learning techniques and classical statistical models.
Building on the work of Liu et al. (2023), we evaluate linear models,
tree-based methods, and deep learning models to extract novel predictors for
CKD progression, with feature importance assessed using Shapley values. These
newly identified predictors, integrated with established clinical features from
the Kidney Failure Risk Equation, are then applied within the framework of Cox
proportional hazards models to predict CKD progression.

摘要：慢性腎臟病 (CKD) 是一項重大的公共衛生挑戰，如果未及早發現和管理，通常會進展到末期腎臟疾病 (ESRD)。無聲疾病進程所致的早期介入，可以顯著降低相關的發病率、死亡率和財務負擔。在這項研究中，我們提出了一種使用機器學習技術和經典統計模型相結合來建模 CKD 進程的新方法。在 Liu 等人 (2023) 的研究基礎上，我們評估了線性模型、基於樹的方法和深度學習模型，以提取 CKD 進程的新預測因子，並使用 Shapley 值評估特徵重要性。這些新識別的預測因子與腎臟衰竭風險方程式中已建立的臨床特徵相結合，然後應用於 Cox 比例風險模型的框架中以預測 CKD 進程。

##### **LTCXNet: Advancing Chest X-Ray Analysis with Solutions for Long-Tailed Multi-Label Classification and Fairness Challenges**
2411.10746v1 by Chin-Wei Huang, Mu-Yi Shen, Kuan-Chang Shih, Shih-Chih Lin, Chi-Yu Chen, Po-Chih Kuo

Chest X-rays (CXRs) often display various diseases with disparate class
frequencies, leading to a long-tailed, multi-label data distribution. In
response to this challenge, we explore the Pruned MIMIC-CXR-LT dataset, a
curated collection derived from the MIMIC-CXR dataset, specifically designed to
represent a long-tailed and multi-label data scenario. We introduce LTCXNet, a
novel framework that integrates the ConvNeXt model, ML-Decoder, and strategic
data augmentation, further enhanced by an ensemble approach. We demonstrate
that LTCXNet improves the performance of CXR interpretation across all classes,
especially enhancing detection in rarer classes like `Pneumoperitoneum' and
`Pneumomediastinum' by 79\% and 48\%, respectively. Beyond performance metrics,
our research extends into evaluating fairness, highlighting that some methods,
while improving model accuracy, could inadvertently affect fairness across
different demographic groups negatively. This work contributes to advancing the
understanding and management of long-tailed, multi-label data distributions in
medical imaging, paving the way for more equitable and effective diagnostic
tools.

摘要：胸部 X 光片 (CXR) 通常會顯示各種疾病，且各類別的頻率不同，導致長尾的多標籤資料分佈。為了應對這項挑戰，我們探討了精簡的 MIMIC-CXR-LT 資料集，這是一個從 MIMIC-CXR 資料集衍生的精選集合，專門設計用於表示長尾和多標籤資料情境。我們引入了 LTCXNet，這是一個整合了 ConvNeXt 模型、ML-Decoder 和策略性資料擴充的新架構，並透過整體方法進一步增強。我們證明 LTCXNet 可提升所有類別的 CXR 解釋效能，特別是將較罕見類別（如「氣腹」和「縱膈氣腫」）的偵測功能分別提升了 79% 和 48%。除了效能指標之外，我們的研究還擴展到評估公平性，強調一些方法在提升模型準確度的同時，可能會無意間對不同人口群體的公平性產生負面影響。這項工作有助於促進對醫學影像中長尾、多標籤資料分佈的理解和管理，為更公平、有效的診斷工具鋪路。

##### **Can Artificial Intelligence Generate Quality Research Topics Reflecting Patient Concerns?**
2411.14456v1 by Jiyeong Kim, Michael L. Chen, Shawheen J. Rezaei, Mariana Ramirez-Posada, Jennifer L. Caswell-Jin, Allison W. Kurian, Fauzia Riaz, Kavita Y. Sarin, Jean Y. Tang, Steven M. Asch, Eleni Linos

Patient-centered research is increasingly important in narrowing the gap
between research and patient care, yet incorporating patient perspectives into
health research has been inconsistent. We propose an automated framework
leveraging innovative natural language processing (NLP) and artificial
intelligence (AI) with patient portal messages to generate research ideas that
prioritize important patient issues. We further quantified the quality of
AI-generated research topics. To define patient clinical concerns, we analyzed
614,464 patient messages from 25,549 individuals with breast or skin cancer
obtained from a large academic hospital (2013 to 2024), constructing a 2-staged
unsupervised NLP topic model. Then, we generated research topics to resolve the
defined issues using a widely used AI (ChatGPT-4o, OpenAI Inc, April 2024
version) with prompt-engineering strategies. We guided AI to perform
multi-level tasks: 1) knowledge interpretation and summarization (e.g.,
interpreting and summarizing the NLP-defined topics), 2) knowledge generation
(e.g., generating research ideas corresponding to patients issues), 3)
self-reflection and correction (e.g., ensuring and revising the research ideas
after searching for scientific articles), and 4) self-reassurance (e.g.,
confirming and finalizing the research ideas). Six highly experienced breast
oncologists and dermatologists assessed the significance and novelty of
AI-generated research topics using a 5-point Likert scale (1-exceptional,
5-poor). One-third of the AI-suggested research topics were highly significant
and novel when both scores were lower than the average. Two-thirds of the
AI-suggested topics were novel in both cancers. Our findings demonstrate that
AI-generated research topics reflecting patient perspectives via a large volume
of patient messages can meaningfully guide future directions in
patient-centered health research.

摘要：<paragraph>以患者為中心的研究所對於縮小研究與患者照護之間的差距越來越重要，然而將患者觀點納入健康研究一直不一致。我們提出一個自動化架構，利用創新的自然語言處理 (NLP) 和人工智慧 (AI) 與患者入口網站訊息，以產生研究想法，優先考慮重要的患者問題。我們進一步量化了 AI 生成的研究主題的品質。為了定義患者的臨床問題，我們分析了從一個大型教學醫院 (2013 年至 2024 年) 取得的 25,549 位乳癌或皮膚癌患者的 614,464 則患者訊息，建構了一個兩階段無監督的 NLP 主題模型。然後，我們使用廣泛使用的 AI (ChatGPT-4o，OpenAI Inc，2024 年 4 月版本) 和提示工程策略，產生研究主題以解決定義的問題。我們引導 AI 執行多層級任務：1) 知識詮釋與摘要 (例如，詮釋和摘要 NLP 定義的主題)，2) 知識產生 (例如，產生與患者問題相應的研究想法)，3) 自我反省和修正 (例如，在搜尋科學文章後確保和修改研究想法)，以及 4) 自我保證 (例如，確認和最後確定研究想法)。六位經驗豐富的乳癌腫瘤科醫師和皮膚科醫師使用 5 點李克特量表 (1-傑出，5-差) 評估 AI 生成的研究主題的重要性和新穎性。當兩個分數都低於平均值時，三分之一的 AI 建議研究主題非常重要且新穎。三分之二的 AI 建議主題在兩種癌症中都是新穎的。我們的發現證明了透過大量患者訊息反映患者觀點的 AI 生成的研究主題，可以有意義地引導以患者為中心的健康研究的未來方向。</paragraph>

##### **Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment**
2411.10534v1 by Andrew Konya, Aviv Ovadya, Kevin Feng, Quan Ze Chen, Lisa Schirch, Colin Irwin, Amy X. Zhang

We introduce a method to measure the alignment between public will and
language model (LM) behavior that can be applied to fine-tuning, online
oversight, and pre-release safety checks. Our `chain of alignment' (CoA)
approach produces a rule based reward (RBR) by creating model behavior
$\textit{rules}$ aligned to normative $\textit{objectives}$ aligned to
$\textit{public will}$. This factoring enables a nonexpert public to directly
specify their will through the normative objectives, while expert intelligence
is used to figure out rules entailing model behavior that best achieves those
objectives. We validate our approach by applying it across three different
domains of LM prompts related to mental health. We demonstrate a public input
process built on collective dialogues and bridging-based ranking that reliably
produces normative objectives supported by at least $96\% \pm 2\%$ of the US
public. We then show that rules developed by mental health experts to achieve
those objectives enable a RBR that evaluates an LM response's alignment with
the objectives similarly to human experts (Pearson's $r=0.841$, $AUC=0.964$).
By measuring alignment with objectives that have near unanimous public support,
these CoA RBRs provide an approximate measure of alignment between LM behavior
and public will.

摘要：<paragraph>我們提出了一種方法來衡量公眾意願與語言模型 (LM) 行為之間的一致性，這種方法可以應用於微調、線上監督和預發佈安全檢查。我們的「一致性鏈」(CoA) 方法透過建立模型行為 $\textit{規則}$，使其與規範性 $\textit{目標}$ 保持一致，進而與 $\textit{公眾意願}$ 保持一致，從而產生基於規則的獎勵 (RBR)。這種分解使非專家公眾能夠透過規範性目標直接表達他們的意願，而專家智慧則用於找出蘊含模型行為的規則，這些規則最能實現這些目標。我們透過將方法應用在與心理健康相關的 LM 提示的三個不同領域來驗證我們的做法。我們展示了一個建立在集體對話和基於橋接的排名上的公眾意見輸入流程，該流程可靠地產生至少有 $96\% \pm 2\%$ 的美國公眾支持的規範性目標。然後我們展示由心理健康專家制定以實現這些目標的規則，使 RBR 能夠評估 LM 回應與目標的一致性，其方式與人類專家類似（Pearson's $r=0.841$，$AUC=0.964$）。透過衡量與獲得近乎一致公眾支持的目標的一致性，這些 CoA RBR 提供了 LM 行為與公眾意願之間一致性的近似衡量標準。</paragraph>

##### **Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization**
2411.10389v1 by Fatahlla Moreh, Yusuf Hasan, Bilal Zahid Hussain, Mohammad Ammar, Sven Tomforde

Internal crack detection has been a subject of focus in structural health
monitoring. By focusing on crack detection in structural datasets, it is
demonstrated that deep learning (DL) methods can effectively analyze seismic
wave fields interacting with micro-scale cracks, which are beyond the
resolution of conventional visual inspection. This work explores a novel
application of DL-based key point detection technique, where cracks are
localized by predicting the coordinates of four key points that define a
bounding region of the crack. The study not only opens new research directions
for non-visual applications but also effectively mitigates the impact of
imbalanced data which poses a challenge for previous DL models, as it can be
biased toward predicting the majority class (non-crack regions). Popular DL
techniques, such as the Inception blocks, are used and investigated. The model
shows an overall reduction in loss when applied to micro-scale crack detection
and is reflected in the lower average deviation between the location of actual
and predicted cracks, with an average Intersection over Union (IoU) being 0.511
for all micro cracks (greater than 0.00 micrometers) and 0.631 for larger micro
cracks (greater than 4 micrometers).

摘要：內部裂縫偵測一直是結構健康監測的重點。透過專注於結構資料集中的裂縫偵測，證明深度學習 (DL) 方法可以有效分析與微尺度裂縫交互作用的地震波場，這超出了傳統目視檢查的解析度。這項工作探索了基於 DL 的關鍵點偵測技術的一項新應用，其中透過預測定義裂縫邊界區域的四個關鍵點的座標來定位裂縫。這項研究不僅為非視覺應用開啟了新的研究方向，還能有效減輕不平衡資料的影響，而這對先前的 DL 模型構成挑戰，因為它可能偏向於預測多數類別（非裂縫區域）。使用並研究了流行的 DL 技術，例如 Inception 區塊。該模型在應用於微尺度裂縫偵測時顯示出整體損失減少，並且反映在實際裂縫和預測裂縫的位置之間的較低平均偏差中，所有微裂縫（大於 0.00 微米）的平均交集比聯合（IoU）為 0.511，而較大的微裂縫（大於 4 微米）則為 0.631。

##### **Artificial Intelligence in Pediatric Echocardiography: Exploring Challenges, Opportunities, and Clinical Applications with Explainable AI and Federated Learning**
2411.10255v1 by Mohammed Yaseen Jabarulla, Theodor Uden, Thomas Jack, Philipp Beerbaum, Steffen Oeltze-Jafra

Pediatric heart diseases present a broad spectrum of congenital and acquired
diseases. More complex congenital malformations require a differentiated and
multimodal decision-making process, usually including echocardiography as a
central imaging method. Artificial intelligence (AI) offers considerable
promise for clinicians by facilitating automated interpretation of pediatric
echocardiography data. However, adapting AI technologies for pediatric
echocardiography analysis has challenges such as limited public data
availability, data privacy, and AI model transparency. Recently, researchers
have focused on disruptive technologies, such as federated learning (FL) and
explainable AI (XAI), to improve automatic diagnostic and decision support
workflows. This study offers a comprehensive overview of the limitations and
opportunities of AI in pediatric echocardiography, emphasizing the synergistic
workflow and role of XAI and FL, identifying research gaps, and exploring
potential future developments. Additionally, three relevant clinical use cases
demonstrate the functionality of XAI and FL with a focus on (i) view
recognition, (ii) disease classification, (iii) segmentation of cardiac
structures, and (iv) quantitative assessment of cardiac function.

摘要：小兒心臟疾病呈現先天性與後天性疾病的廣泛光譜。較複雜的先天性畸形需要一個差異化且多模式的決策過程，通常包括超音波檢查作為主要的影像方法。人工智慧 (AI) 為臨床醫生提供了相當大的希望，因為它可以促進小兒超音波檢查資料的自動化解讀。然而，將人工智慧技術應用於小兒超音波檢查分析有許多挑戰，例如有限的公開資料可用性、資料隱私和人工智慧模型透明度。最近，研究人員專注於破壞性技術，例如聯合學習 (FL) 和可解釋人工智慧 (XAI)，以改善自動診斷和決策支援工作流程。本研究提供了人工智慧在小兒超音波檢查中的限制和機會的全面概述，強調了 XAI 和 FL 的協同工作流程和角色，找出研究差距並探討潛在的未來發展。此外，三個相關的臨床使用案例展示了 XAI 和 FL 的功能，重點在於 (i) 檢視辨識、(ii) 疾病分類、(iii) 心臟結構分割和 (iv) 心臟功能的量化評估。

##### **FedCL-Ensemble Learning: A Framework of Federated Continual Learning with Ensemble Transfer Learning Enhanced for Alzheimer's MRI Classifications while Preserving Privacy**
2411.12756v1 by Rishit Kapoor, Jesher Joshua, Muralidharan Vijayarangan, Natarajan B

This research work introduces a novel approach to the classification of
Alzheimer's disease by using the advanced deep learning techniques combined
with secure data processing methods. This research work primary uses transfer
learning models such as ResNet, ImageNet, and VNet to extract high-level
features from medical image data. Thereafter, these pre-trained models were
fine-tuned for Alzheimer's related subtle patterns such that the model is
capable of robust feature extraction over varying data sources. Further, the
federated learning approaches were incorporated to tackle a few other
challenges related to classification, aimed to provide better prediction
performance and protect data privacy. The proposed model was built using
federated learning without sharing sensitive patient data. This way, the
decentralized model benefits from the large and diversified dataset that it is
trained upon while ensuring confidentiality. The cipher-based encryption
mechanism is added that allows us to secure the transportation of data and
further ensure the privacy and integrity of patient information throughout
training and classification. The results of the experiments not only help to
improve the accuracy of the classification of Alzheimer's but at the same time
provides a framework for secure and collaborative analysis of health care data.

摘要：本研究工作提出了一種新的阿茲海默症分類方法，該方法結合了先進的深度學習技術和安全的數據處理方法。本研究工作主要使用 ResNet、ImageNet 和 VNet 等遷移學習模型從醫學影像數據中提取高級特徵。隨後，這些預訓練模型針對阿茲海默症相關的細微模式進行微調，使模型能夠對不同數據源中的特徵進行穩健提取。此外，聯邦學習方法被納入以應對與分類相關的幾個其他挑戰，旨在提供更好的預測性能和保護數據隱私。所提出的模型是使用聯邦學習構建的，而無需共享敏感的患者數據。這樣，分散式模型可以從其訓練的大型且多樣化的數據集中受益，同時確保機密性。添加了基於密碼的加密機制，它使我們能夠確保數據傳輸的安全性，並進一步確保患者信息在整個訓練和分類過程中的隱私和完整性。實驗結果不僅有助於提高阿茲海默症分類的準確性，同時還為醫療保健數據的安全和協作分析提供了一個框架。

##### **Evaluating the role of `Constitutions' for learning from AI feedback**
2411.10168v1 by Saskia Redgate, Andrew M. Bean, Adam Mahdi

The growing capabilities of large language models (LLMs) have led to their
use as substitutes for human feedback for training and assessing other LLMs.
These methods often rely on `constitutions', written guidelines which a critic
model uses to provide feedback and improve generations. We investigate how the
choice of constitution affects feedback quality by using four different
constitutions to improve patient-centered communication in medical interviews.
In pairwise comparisons conducted by 215 human raters, we found that detailed
constitutions led to better results regarding emotive qualities. However, none
of the constitutions outperformed the baseline in learning more
practically-oriented skills related to information gathering and provision. Our
findings indicate that while detailed constitutions should be prioritised,
there are possible limitations to the effectiveness of AI feedback as a reward
signal in certain areas.

摘要：大型語言模型（LLM）功能不斷增強，促使它們被用作人類回饋的替代品，以訓練和評估其他 LLM。這些方法通常依賴於「憲法」，也就是評論模型用來提供回饋和改進生成的書面準則。我們探討憲法選擇如何影響回饋品質，方法是使用四種不同的憲法來改善醫療訪談中的以患者為中心的溝通。在 215 位人類評分員進行的成對比較中，我們發現詳細的憲法在情緒品質方面帶來更好的結果。然而，沒有任何憲法在學習與資訊收集和提供相關的更實用技能方面優於基準。我們的研究結果表明，雖然應優先考慮詳細的憲法，但 AI 回饋作為特定領域的獎勵訊號的有效性可能有限。

##### **PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse**
2411.10087v1 by Einari Vaaras, Manu Airaksinen, Okko Räsänen

Self-supervised learning (SSL) is a data-driven learning approach that
utilizes the innate structure of the data to guide the learning process. In
contrast to supervised learning, which depends on external labels, SSL utilizes
the inherent characteristics of the data to produce its own supervisory signal.
However, one frequent issue with SSL methods is representation collapse, where
the model outputs a constant input-invariant feature representation. This issue
hinders the potential application of SSL methods to new data modalities, as
trying to avoid representation collapse wastes researchers' time and effort.
This paper introduces a novel SSL algorithm for time-series data called
Prediction of Functionals from Masked Latents (PFML). Instead of predicting
masked input signals or their latent representations directly, PFML operates by
predicting statistical functionals of the input signal corresponding to masked
embeddings, given a sequence of unmasked embeddings. The algorithm is designed
to avoid representation collapse, rendering it straightforwardly applicable to
different time-series data domains, such as novel sensor modalities in clinical
data. We demonstrate the effectiveness of PFML through complex, real-life
classification tasks across three different data modalities: infant posture and
movement classification from multi-sensor inertial measurement unit data,
emotion recognition from speech data, and sleep stage classification from EEG
data. The results show that PFML is superior to a conceptually similar
pre-existing SSL method and competitive against the current state-of-the-art
SSL method, while also being conceptually simpler and without suffering from
representation collapse.

摘要：自监督学习 (SSL) 是一种数据驱动的学习方法，它利用数据的内在结构来指导学习过程。与依赖外部标签的监督学习相反，SSL 利用数据本身的固有特征来产生自己的监督信号。然而，SSL 方法的一个常见问题是表示坍塌，其中模型输出一个常数输入不变特征表示。这个问题阻碍了 SSL 方法在新的数据模式中的潜在应用，因为试图避免表示坍塌会浪费研究人员的时间和精力。本文介绍了一种针对时间序列数据的新型 SSL 算法，称为掩码潜在变量的功能预测 (PFML)。PFML 不是直接预测掩码输入信号或其潜在表示，而是通过预测输入信号的统计函数（对应于掩码嵌入）来操作，给定一系列未掩码嵌入。该算法旨在避免表示坍塌，使其可以直接应用于不同的时间序列数据域，例如临床数据中的新型传感器模式。我们通过三个不同数据模式的复杂现实生活分类任务展示了 PFML 的有效性：多传感器惯性测量单元数据的婴儿姿势和运动分类、语音数据的语音识别以及脑电图数据的睡眠阶段分类。结果表明，PFML 优于概念上相似的现有 SSL 方法，并且与当前最先进的 SSL 方法具有竞争力，同时在概念上更简单，并且不会出现表示坍塌。

##### **Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion**
2411.10036v1 by Dan He, Guofen Wang, Weisheng Li, Yucheng Shu, Wenbo Li, Lijian Yang, Yuping Huang, Feiyan Li

Multimodal image fusion (MMIF) aims to integrate information from different
modalities to obtain a comprehensive image, aiding downstream tasks. However,
existing methods tend to prioritize natural image fusion and focus on
information complementary and network training strategies. They ignore the
essential distinction between natural and medical image fusion and the
influence of underlying components. This paper dissects the significant
differences between the two tasks regarding fusion goals, statistical
properties, and data distribution. Based on this, we rethink the suitability of
the normalization strategy and convolutional kernels for end-to-end
MMIF.Specifically, this paper proposes a mixture of instance normalization and
group normalization to preserve sample independence and reinforce intrinsic
feature correlation.This strategy promotes the potential of enriching feature
maps, thus boosting fusion performance. To this end, we further introduce the
large kernel convolution, effectively expanding receptive fields and enhancing
the preservation of image detail. Moreover, the proposed multipath adaptive
fusion module recalibrates the decoder input with features of various scales
and receptive fields, ensuring the transmission of crucial information.
Extensive experiments demonstrate that our method exhibits state-of-the-art
performance in multiple fusion tasks and significantly improves downstream
applications. The code is available at https://github.com/HeDan-11/LKC-FUNet.

摘要：多模態影像融合 (MMIF) 旨在整合來自不同模態的資訊，以取得全面的影像，協助下游任務。然而，現有方法傾向於優先考慮自然影像融合，並專注於資訊互補和網路訓練策略。它們忽略了自然影像融合與醫學影像融合之間的本質區別，以及底層組成的影響。本文剖析了這兩個任務在融合目標、統計性質和資料分佈方面的顯著差異。基於此，我們重新思考正規化策略和捲積核對端到端 MMIF 的適用性。具體而言，本文提出實例正規化和群組正規化的混合，以保留樣本獨立性並加強內在特徵關聯。此策略提升了豐富特徵圖的潛力，從而提升融合效能。為此，我們進一步引入了大核卷積，有效地擴展感受野並增強影像細節的保留。此外，提出的多路徑自適應融合模組重新校準解碼器輸入，其特徵具有不同的比例和感受野，確保關鍵資訊的傳輸。廣泛的實驗證明，我們的模型在多個融合任務中展現出最先進的效能，並顯著改善下游應用。程式碼可在 https://github.com/HeDan-11/LKC-FUNet 取得。

##### **JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging**
2411.09933v1 by Kaito Baba, Ryota Yagi, Junichiro Takahashi, Risa Kishikawa, Satoshi Kodera

With the rapid advancement of large language models (LLMs), foundational
models (FMs) have seen significant advancements. Healthcare is one of the most
crucial application areas for these FMs, given the significant time and effort
required for physicians to analyze large volumes of patient data. Recent
efforts have focused on adapting multimodal FMs to the medical domain through
techniques like instruction-tuning, leading to the development of medical
foundation models (MFMs). However, these approaches typically require large
amounts of training data to effectively adapt models to the medical field.
Moreover, most existing models are trained on English datasets, limiting their
practicality in non-English-speaking regions where healthcare professionals and
patients are not always fluent in English. The need for translation introduces
additional costs and inefficiencies. To address these challenges, we propose a
\textbf{J}apanese \textbf{Radi}ology report generation model enhanced by
\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the
first attempt to extend a non-medical vision-language foundation model to the
medical domain through evolutionary optimization of model merging. We
successfully created a model that generates accurate Japanese reports from
X-ray images using only 50 translated samples from publicly available data.
This model, developed with highly efficient use of limited data, outperformed
leading models from recent research trained on much larger datasets.
Additionally, with only 8 billion parameters, this relatively compact
foundation model can be deployed locally within hospitals, making it a
practical solution for environments where APIs and other external services
cannot be used due to strict privacy and security requirements.

摘要：<paragraph>隨著大型語言模型 (LLM) 的快速進展，基礎模型 (FM) 已經獲得顯著的進步。醫療保健是這些 FM 最重要的應用領域之一，因為醫生需要花費大量時間和精力來分析大量的患者資料。最近的研究重點在於透過指令微調等技術將多模態 FM 適應到醫療領域，從而開發出醫療基礎模型 (MFM)。然而，這些方法通常需要大量的訓練資料才能有效地將模型適應到醫療領域。此外，大多數現有模型都是針對英語資料集進行訓練，這限制了它們在非英語地區的實用性，那裡的醫療專業人員和患者並不總是精通英語。翻譯的需求引入了額外的成本和低效率。為了應對這些挑戰，我們提出了一個由模型合併的進化優化增強的**J**apanese **Radi**ology 報告生成模型 (JRadiEvo)。這是首次嘗試透過模型合併的進化優化將非醫療視覺語言基礎模型擴展到醫療領域。我們成功地建立了一個模型，僅使用來自公開資料的 50 個翻譯範例，就能從 X 光影像中產生準確的日文報告。這個模型使用有限資料進行高效率的開發，其效能優於最近研究中在更大資料集上訓練出來的領先模型。此外，這個相對精簡的基礎模型只有 80 億個參數，可以在醫院內部進行本地部署，使其成為在嚴格的隱私和安全要求下無法使用 API 和其他外部服務的環境中的實用解決方案。</paragraph>

##### **A Hybrid Artificial Intelligence System for Automated EEG Background Analysis and Report Generation**
2411.09874v1 by Chin-Sung Tung, Sheng-Fu Liang, Shu-Feng Chang, Chung-Ping Young

Electroencephalography (EEG) plays a crucial role in the diagnosis of various
neurological disorders. However, small hospitals and clinics often lack
advanced EEG signal analysis systems and are prone to misinterpretation in
manual EEG reading. This study proposes an innovative hybrid artificial
intelligence (AI) system for automatic interpretation of EEG background
activity and report generation. The system combines deep learning models for
posterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and
expert-designed algorithms for abnormality detection. For PDR prediction, 1530
labeled EEGs were used, and the best ensemble model achieved a mean absolute
error (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of
91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI
system significantly outperformed neurologists in detecting generalized
background slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated
improved focal abnormality detection, although not statistically significant (p
= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset
and the Temple University Abnormal EEG Corpus showed consistent performance
(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.
The use of large language models (LLMs) for report generation demonstrated 100%
accuracy, verified by three other independent LLMs. This hybrid AI system
provides an easily scalable and accurate solution for EEG interpretation in
resource-limited settings, assisting neurologists in improving diagnostic
accuracy and reducing misdiagnosis rates.

摘要：腦電圖（EEG）在診斷各種神經疾病中扮演至關重要的角色。然而，小型醫院和診所通常缺乏進階的 EEG 訊號分析系統，且容易在手動判讀 EEG 時產生誤解。本研究提出一個創新的混合人工智慧（AI）系統，用於自動判讀 EEG 背景活動並產生報告。此系統結合深度學習模型，用於預測後優勢律動（PDR）、非監督人工製品移除，以及專家設計的異常偵測演算法。在 PDR 預測中，使用了 1530 個標記 EEG，最佳的整體模型達到了 0.237 的平均絕對誤差（MAE）、0.359 的均方根誤差（RMSE）、91.8% 的 0.6Hz 誤差準確度，以及 99% 的 1.2Hz 誤差準確度。AI 系統在偵測廣泛背景減慢方面明顯優於神經學家（p = 0.02；F1：AI 0.93，神經學家 0.82），並展現出改善的局部異常偵測，儘管沒有統計意義（p = 0.79；F1：AI 0.71，神經學家 0.55）。在內部資料集和 Temple University 異常 EEG 語料庫上的驗證顯示了一致的效能（F1：分別為 0.884 和 0.835；p = 0.66），證明了其普遍性。使用大型語言模型（LLM）來產生報告證明了 100% 的準確度，並由其他三個獨立的 LLM 驗證。此混合 AI 系統提供了一個易於擴充且準確的解決方案，用於在資源有限的環境中進行 EEG 判讀，協助神經學家提高診斷準確度並降低誤診率。

##### **A Benchmark for Long-Form Medical Question Answering**
2411.09834v2 by Pedram Hosseini, Jessica M. Sin, Bing Ren, Bryceton G. Thomas, Elnaz Nouri, Ali Farahanchi, Saeed Hassanpour

There is a lack of benchmarks for evaluating large language models (LLMs) in
long-form medical question answering (QA). Most existing medical QA evaluation
benchmarks focus on automatic metrics and multiple-choice questions. While
valuable, these benchmarks fail to fully capture or assess the complexities of
real-world clinical applications where LLMs are being deployed. Furthermore,
existing studies on evaluating long-form answer generation in medical QA are
primarily closed-source, lacking access to human medical expert annotations,
which makes it difficult to reproduce results and enhance existing baselines.
In this work, we introduce a new publicly available benchmark featuring
real-world consumer medical questions with long-form answer evaluations
annotated by medical doctors. We performed pairwise comparisons of responses
from various open and closed-source medical and general-purpose LLMs based on
criteria such as correctness, helpfulness, harmfulness, and bias. Additionally,
we performed a comprehensive LLM-as-a-judge analysis to study the alignment
between human judgments and LLMs. Our preliminary results highlight the strong
potential of open LLMs in medical QA compared to leading closed models. Code &
Data: https://github.com/lavita-ai/medical-eval-sphere

摘要：大型語言模型 (LLM) 在長篇醫療問題解答 (QA) 中的評估基準有所不足。現有的醫療 QA 評估基準大多集中在自動化指標和多選題上。儘管有價值，但這些基準無法完全捕捉或評估 LLM 部署的現實臨床應用的複雜性。此外，現有的關於評估醫療 QA 中長篇答案生成的研究所主要是閉源的，缺乏對人類醫學專家註釋的訪問權限，這使得難以重現結果並增強現有的基準。在這項工作中，我們引入了一個新的公開基準，其中包含實際的消費者醫療問題，以及由醫生註釋的長篇答案評估。我們根據正確性、有用性、有害性和偏差等標準，對來自各種開放和閉源醫療和通用 LLM 的回應進行了成對比較。此外，我們進行了一項全面的 LLM 作為評審的分析，以研究人類判斷和 LLM 之間的一致性。我們的初步結果突出了開放式 LLM 在醫療 QA 中的強大潛力，與領先的閉源模型相比。程式碼和資料：https://github.com/lavita-ai/medical-eval-sphere

##### **A Self-Supervised Model for Multi-modal Stroke Risk Prediction**
2411.09822v1 by Camille Delgrange, Olga Demler, Samia Mora, Bjoern Menze, Ezequiel de la Rosa, Neda Davoudi

Predicting stroke risk is a complex challenge that can be enhanced by
integrating diverse clinically available data modalities. This study introduces
a self-supervised multimodal framework that combines 3D brain imaging, clinical
data, and image-derived features to improve stroke risk prediction prior to
onset. By leveraging large unannotated clinical datasets, the framework
captures complementary and synergistic information across image and tabular
data modalities. Our approach is based on a contrastive learning framework that
couples contrastive language-image pretraining with an image-tabular matching
module, to better align multimodal data representations in a shared latent
space. The model is trained on the UK Biobank, which includes structural brain
MRI and clinical data. We benchmark its performance against state-of-the-art
unimodal and multimodal methods using tabular, image, and image-tabular
combinations under diverse frozen and trainable model settings. The proposed
model outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in
ROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6%
increase in balanced accuracy compared to the best multimodal supervised model.
Through interpretable tools, our approach demonstrated better integration of
tabular and image data, providing richer and more aligned embeddings.
Gradient-weighted Class Activation Mapping heatmaps further revealed activated
brain regions commonly associated in the literature with brain aging, stroke
risk, and clinical outcomes. This robust self-supervised multimodal framework
surpasses state-of-the-art methods for stroke risk prediction and offers a
strong foundation for future studies integrating diverse data modalities to
advance clinical predictive modelling.

摘要：<paragraph>預測中風風險是一項複雜的挑戰，可以透過整合多樣化的臨床可用數據模式來加強。本研究介紹了一個自監督多模式架構，結合 3D 大腦影像、臨床數據和影像衍生特徵，以在發作前改善中風風險預測。透過利用大量的未標記臨床數據集，該架構擷取了影像和表格數據模式之間的互補和協同資訊。我們的做法基於對比學習架構，將對比語言影像預訓練與影像表格匹配模組結合，以在共享潛在空間中更好地對齊多模式數據表示。該模型是在英國生物銀行中訓練的，其中包括結構性腦部 MRI 和臨床數據。我們使用表格、影像和影像表格組合，在不同的凍結和可訓練模型設定下，根據最先進的單模式和多模式方法對其效能進行基準測試。所提出的模型在 ROC-AUC 中比自監督表格（影像）方法高出 2.6%（2.6%），在平衡準確度中高出 3.3%（5.6%）。此外，與最佳多模式監督模型相比，它的平衡準確度提高了 7.6%。透過可解釋的工具，我們的做法證明了表格和影像數據的整合性更好，提供了更豐富且更一致的嵌入。梯度加權類別啟用對應熱圖進一步揭示了文獻中通常與腦部老化、中風風險和臨床結果相關的活化腦區。這個強健的自監督多模式架構超越了中風風險預測的最新方法，並為整合不同數據模式以推進臨床預測建模的未來研究提供了堅實的基礎。</paragraph>

##### **Deep Learning for Fetal Inflammatory Response Diagnosis in the Umbilical Cord**
2411.09767v1 by Marina A. Ayad, Ramin Nateghi, Abhishek Sharma, Lawrence Chillrud, Tilly Seesillapachai, Lee A. D. Cooper, Jeffery A. Goldstein

Inflammation of the umbilical cord can be seen as a result of ascending
intrauterine infection or other inflammatory stimuli. Acute fetal inflammatory
response (FIR) is characterized by infiltration of the umbilical cord by fetal
neutrophils, and can be associated with neonatal sepsis or fetal inflammatory
response syndrome. Recent advances in deep learning in digital pathology have
demonstrated favorable performance across a wide range of clinical tasks, such
as diagnosis and prognosis. In this study we classified FIR from whole slide
images (WSI). We digitized 4100 histological slides of umbilical cord stained
with hematoxylin and eosin(H&E) and extracted placental diagnoses from the
electronic health record. We build models using attention-based whole slide
learning models. We compared strategies between features extracted by a model
(ConvNeXtXLarge) pretrained on non-medical images (ImageNet), and one
pretrained using histopathology images (UNI). We trained multiple iterations of
each model and combined them into an ensemble. The predictions from the
ensemble of models trained using UNI achieved an overall balanced accuracy of
0.836 on the test dataset. In comparison, the ensembled predictions using
ConvNeXtXLarge had a lower balanced accuracy of 0.7209. Heatmaps generated from
top accuracy model appropriately highlighted arteritis in cases of FIR 2. In
FIR 1, the highest performing model assigned high attention to areas of
activated-appearing stroma in Wharton's Jelly. However, other high-performing
models assigned attention to umbilical vessels. We developed models for
diagnosis of FIR from placental histology images, helping reduce interobserver
variability among pathologists. Future work may examine the utility of these
models for identifying infants at risk of systemic inflammatory response or
early onset neonatal sepsis.

摘要：臍帶發炎可視為上行性子宮內感染或其他發炎刺激所致。急性胎兒發炎反應 (FIR) 的特徵是胎兒中性球浸潤臍帶，可能與新生兒敗血症或胎兒發炎反應症候群有關。數位病理學中深度學習的最新進展已證明在廣泛的臨床任務中表現良好，例如診斷和預後。在這項研究中，我們從全切片影像 (WSI) 中分類 FIR。我們將 4100 張用蘇木精和曙紅 (H&E) 染色的臍帶組織切片數位化，並從電子病歷中提取胎盤診斷。我們使用基於注意力的全切片學習模型建立模型。我們比較了非醫療影像 (ImageNet) 預訓練模型 (ConvNeXtXLarge) 和使用組織病理學影像 (UNI) 預訓練模型提取的特徵之間的策略。我們訓練了每個模型的多次迭代，並將它們組合成一個整體。使用 UNI 訓練的模型整體的預測在測試資料集上達到 0.836 的整體平衡準確度。相比之下，使用 ConvNeXtXLarge 的整體預測的平衡準確度較低，為 0.7209。從準確度最高的模型產生的熱圖適當地突出了 FIR 2 病例中的動脈炎。在 FIR 1 中，表現最好的模型將高度關注分配給沃頓氏膠中的活化外觀基質區域。然而，其他表現良好的模型將注意力分配給臍帶血管。我們開發了從胎盤組織學影像診斷 FIR 的模型，有助於減少病理學家之間的觀察者間變異性。未來的研究可能會探討這些模型在識別有全身性發炎反應風險或早期發作新生兒敗血症的嬰兒方面的效用。

##### **Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information**
2411.09648v1 by Ahan Bhatt, Nandan Vaghela

This paper introduces Med-Bot, an AI-powered chatbot designed to provide
users with accurate and reliable medical information. Utilizing advanced
libraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,
Med-Bot is built to handle the complexities of natural language understanding
in a healthcare context. The integration of llamaassisted data processing and
AutoGPT-Q provides enhanced performance in processing and responding to queries
based on PDFs of medical literature, ensuring that users receive precise and
trustworthy information. This research details the methodologies employed in
developing Med-Bot and evaluates its effectiveness in disseminating healthcare
information.

摘要：本文介紹 Med-Bot，一個由人工智慧驅動的聊天機器人，旨在為使用者提供準確且可靠的醫療資訊。Med-Bot 利用進階函式庫和框架，例如 PyTorch、Chromadb、Langchain 和 Autogptq，建構來處理醫療保健環境中自然語言理解的複雜性。整合了 Llama 輔助資料處理和 AutoGPT-Q，在處理和回應基於醫學文獻 PDF 的查詢時提供了增強的效能，確保使用者收到精確且可信賴的資訊。本研究詳細說明了開發 Med-Bot 所採用的方法，並評估其在傳播醫療保健資訊方面的有效性。

##### **An Explainable Attention Model for Cervical Precancer Risk Classification using Colposcopic Images**
2411.09469v1 by Smith K. Khare, Berit Bargum Booth, Victoria Blanes-Vidal, Lone Kjeld Petersen, Esmaeil S. Nadimi

Cervical cancer remains a major worldwide health issue, with early
identification and risk assessment playing critical roles in effective
preventive interventions. This paper presents the Cervix-AID-Net model for
cervical precancer risk classification. The study designs and evaluates the
proposed Cervix-AID-Net model based on patients colposcopy images. The model
comprises a Convolutional Block Attention Module (CBAM) and convolutional
layers that extract interpretable and representative features of colposcopic
images to distinguish high-risk and low-risk cervical precancer. In addition,
the proposed Cervix-AID-Net model integrates four explainable techniques,
namely gradient class activation maps, Local Interpretable Model-agnostic
Explanations, CartoonX, and pixel rate distortion explanation based on output
feature maps and input features. The evaluation using holdout and ten-fold
cross-validation techniques yielded a classification accuracy of 99.33\% and
99.81\%. The analysis revealed that CartoonX provides meticulous explanations
for the decision of the Cervix-AID-Net model due to its ability to provide the
relevant piece-wise smooth part of the image. The effect of Gaussian noise and
blur on the input shows that the performance remains unchanged up to Gaussian
noise of 3\% and blur of 10\%, while the performance reduces thereafter. A
comparison study of the proposed model's performance compared to other deep
learning approaches highlights the Cervix-AID-Net model's potential as a
supplemental tool for increasing the effectiveness of cervical precancer risk
assessment. The proposed method, which incorporates the CBAM and explainable
artificial integration, has the potential to influence cervical cancer
prevention and early detection, improving patient outcomes and lowering the
worldwide burden of this preventable disease.

摘要：子宮頸癌仍然是全球主要的健康議題，早期辨識和風險評估在有效的預防性干預措施中扮演著關鍵性的角色。本文提出子宮頸輔助網路模型，用於子宮頸癌前病變風險分類。本研究基於病患的陰道鏡影像設計並評估所提出的子宮頸輔助網路模型。該模型包含卷積區塊注意力模組 (CBAM) 和卷積層，用於萃取可解釋且具代表性的陰道鏡影像特徵，以區分高風險和低風險的子宮頸癌前病變。此外，所提出的子宮頸輔助網路模型整合了四種可解釋的技術，分別為梯度類別激活圖、局部可解釋模型不可知解釋、CartoonX 和基於輸出特徵圖和輸入特徵的像素率失真解釋。使用留存法和十倍交叉驗證技術進行評估，得到 99.33% 和 99.81% 的分類準確率。分析顯示，CartoonX 能夠提供影像中相關的分段平滑部分，因此能為子宮頸輔助網路模型的決策提供細緻的解釋。高斯噪聲和模糊對輸入的影響顯示，在高斯噪聲低於 3% 和模糊低於 10% 的情況下，效能保持不變，但之後效能便會下降。所提出的模型效能與其他深度學習方法的比較研究，突顯了子宮頸輔助網路模型作為補充工具的潛力，用於提高子宮頸癌前病變風險評估的有效性。所提出的方法結合了 CBAM 和可解釋的人工整合，有潛力影響子宮頸癌的預防和早期偵測，改善病患的預後並降低這種可預防疾病在全球的負擔。

##### **Script-centric behavior understanding for assisted autism spectrum disorder diagnosis**
2411.09413v1 by Wenxing Liu, Yueran Pan, Ming Li

Observing and analyzing children's social behaviors is crucial for the early
diagnosis of Autism Spectrum Disorders (ASD). This work focuses on
automatically detecting ASD using computer vision techniques and large language
models (LLMs). Existing methods typically rely on supervised learning. However,
the scarcity of ASD diagnostic datasets and the lack of interpretability in
diagnostic results significantly limits its clinical application. To address
these challenges, we introduce a novel unsupervised approach based on
script-centric behavior understanding. Our pipeline converts video content into
scripts that describe the behavior of characters, leveraging the
generalizability of large language models to detect ASD in a zero-shot or
few-shot manner. Specifically, we propose a scripts transcription module for
multimodal behavior data textualization and a domain prompts module to bridge
LLMs. Our method achieves an accuracy of 92.00\% in diagnosing ASD in children
with an average age of 24 months, surpassing the performance of supervised
learning methods by 3.58\% absolutely. Extensive experiments confirm the
effectiveness of our approach and suggest its potential for advancing ASD
research through LLMs.

摘要：觀察和分析兒童的社交行為對於自閉症譜系障礙 (ASD) 的早期診斷至關重要。這項工作著重於使用電腦視覺技術和大語言模型 (LLM) 自動偵測 ASD。現有方法通常依賴於監督式學習。然而，ASD 診斷資料集的稀少性和診斷結果缺乏可解釋性，顯著地限制了其臨床應用。為了應對這些挑戰，我們引入了一種基於以腳本為中心的行為理解的新型非監督式方法。我們的管道將影片內容轉換成描述角色行為的腳本，利用大語言模型的泛化性以零次或少次學習的方式偵測 ASD。具體來說，我們提出了一個腳本轉錄模組用於多模態行為資料文字化，以及一個網域提示模組來橋接 LLM。我們的模型在診斷平均年齡為 24 個月的兒童 ASD 時，達到了 92.00% 的準確率，絕對優於監督式學習方法 3.58%。大量的實驗證實了我們方法的有效性，並表明其通過 LLM 推動 ASD 研究的潛力。

##### **NFRs in Medical Imaging**
2411.09718v1 by Amanda Vallentin

The diagnostic imaging departments are under great pressure due to a growing
workload. The number of required scans is growing and there is a shortage of
qualified labor. AI solutions for medical imaging applications have shown great
potential. However, very few diagnostic imaging models have been approved for
hospital use and even fewer are being implemented at the hospitals. The most
common reason why software projects fail is poor requirement engineering,
especially non-functional requirements (NFRs) can be detrimental to a project.
Research shows that machine learning professionals struggle to work with NFRs
and that there is a need to adapt NFR frameworks to machine learning, AI-based,
software. This study uses qualitative methods to interact with key stakeholders
to identify which types of NFRs are important for medical imaging applications.
The study was done on a single Danish hospital and found that NFRs of type
Efficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability,
and Fairness were important to the stakeholders. Especially Efficiency since
the diagnostic imaging department is trying to spend as little time as possible
on each scan.

摘要：由於工作負載增加，診斷影像部門承受著極大的壓力。所需掃描的數量正在增加，而且合格的勞動力短缺。醫療影像應用的人工智慧解決方案顯示出巨大的潛力。然而，很少有診斷影像模型被批准用於醫院，甚至更少被實施於醫院。軟體專案失敗的最常見原因是需求工程不佳，特別是非功能性需求 (NFR) 可能對專案有害。研究顯示，機器學習專業人員難以使用 NFR，並且需要將 NFR 框架調整為機器學習、基於 AI 的軟體。本研究使用定性方法與主要利害關係人互動，以找出哪些類型的 NFR 對醫學影像應用程式很重要。該研究在一家丹麥醫院進行，發現效率、準確性、互操作性、可靠性、可用性、適應性和公平性類型的 NFR 對利害關係人很重要。特別是效率，因為診斷影像部門正努力盡可能減少每項掃描所花的時間。

##### **Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering**
2411.09213v1 by Nghia Trung Ngo, Chien Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen

Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.

摘要：檢索增強生成（RAG）已成為一種有前途的方法，可增強大型語言模型（LLM）在知識密集型任務中的效能，例如醫療領域的任務。然而，醫療領域的敏感性質需要一個完全準確且值得信賴的系統。雖然現有的 RAG 評量基準主要著重於標準檢索回答設定，但它們忽略了許多衡量可靠醫療系統關鍵面向的實際情境。本文透過提供一個全面的評量架構來解決這個差距，這個架構適用於這些情境的 RAG 設定中的醫療問答（QA）系統，包括充足性、整合性與穩健性。我們引入了醫療檢索增強生成評量基準（MedRGB），它為四個醫療 QA 資料集提供了各種補充元素，以測試 LLM 處理這些特定情境的的能力。利用 MedRGB，我們對多種檢索條件下的最新商業 LLM 和開源模型進行了廣泛的評量。我們的實驗結果顯示，目前模型處理檢索文件中的雜訊和錯誤資訊的能力有限。我們進一步分析了 LLM 的推理程序，為在這個關鍵的醫療領域開發 RAG 系統提供了寶貴的見解和未來的方向。

##### **Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance**
2411.09174v1 by Md Fahim Anjum

Recent advances in image generation, particularly via diffusion models, have
led to impressive improvements in image synthesis quality. Despite this,
diffusion models are still challenged by model-induced artifacts and limited
stability in image fidelity. In this work, we hypothesize that the primary
cause of this issue is the improper resampling operation that introduces
aliasing in the diffusion model and a careful alias-free resampling dictated by
image processing theory can improve the model's performance in image synthesis.
We propose the integration of alias-free resampling layers into the UNet
architecture of diffusion models without adding extra trainable parameters,
thereby maintaining computational efficiency. We then assess whether these
theory-driven modifications enhance image quality and rotational equivariance.
Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and
MNIST-M, reveal consistent gains in image quality, particularly in terms of FID
and KID scores. Furthermore, we propose a modified diffusion process that
enables user-controlled rotation of generated images without requiring
additional training. Our findings highlight the potential of theory-driven
enhancements such as alias-free resampling in generative models to improve
image quality while maintaining model efficiency and pioneer future research
directions to incorporate them into video-generating diffusion models, enabling
deeper exploration of the applications of alias-free resampling in generative
modeling.

摘要：影像生成技術的最新進展，特別是透過擴散模型，已大幅提升影像合成品質。儘管如此，擴散模型仍受限於模型引發的人工製品，且影像保真度穩定性有限。在這項工作中，我們假設此問題的主要原因是不適當的重新取樣運算，這會在擴散模型中引入混疊，而由影像處理理論指導的仔細無混疊重新取樣可以提升模型在影像合成的效能。我們建議將無混疊重新取樣層整合到擴散模型的 UNet 架構中，而無須增加額外的可訓練參數，進而維持運算效率。接著我們評估這些理論驅動的修改是否能提升影像品質和旋轉等變性。我們在基準資料集（包括 CIFAR-10、MNIST 和 MNIST-M）上的實驗結果顯示，影像品質獲得一致的提升，特別是在 FID 和 KID 分數方面。此外，我們提出一個修改過的擴散程序，它能讓使用者控制生成影像的旋轉，而無需額外訓練。我們的發現突顯了理論驅動的強化（例如無混疊重新取樣）在生成模型中的潛力，它能在維持模型效率的同時提升影像品質，並為未來研究開拓方向，將其納入生成影片的擴散模型中，進一步探索無混疊重新取樣在生成模型中的應用。

##### **Artificial Intelligence for Infectious Disease Prediction and Prevention: A Comprehensive Review**
2411.10486v1 by Selestine Melchane, Youssef Elmir, Farid Kacimi, Larbi Boubchir

Artificial Intelligence (AI) and infectious diseases prediction have recently
experienced a common development and advancement. Machine learning (ML)
apparition, along with deep learning (DL) emergence, extended many approaches
against diseases apparition and their spread. And despite their outstanding
results in predicting infectious diseases, conflicts appeared regarding the
types of data used and how they can be studied, analyzed, and exploited using
various emerging methods. This has led to some ongoing discussions in the
field. This research aims not only to provide an overview of what has been
accomplished, but also to highlight the difficulties related to the types of
data used, and the learning methods applied for each research objective. It
categorizes these contributions into three areas: predictions using Public
Health Data to prevent the spread of a transmissible disease within a region;
predictions using Patients' Medical Data to detect whether a person is infected
by a transmissible disease; and predictions using both Public and patient
medical data to estimate the extent of disease spread in a population. The
paper also critically assesses the potential of AI and outlines its limitations
in infectious disease management.

摘要：人工智能 (AI) 和傳染病預測最近經歷了一段共同發展和進步。機器學習 (ML) 的出現，以及深度學習 (DL) 的興起，擴展了許多對抗疾病出現及其傳播的方法。儘管它們在預測傳染病方面取得了傑出的成果，但對於所使用數據的類型以及如何使用各種新興方法對其進行研究、分析和利用，出現了爭議。這導致了該領域的一些持續討論。本研究不僅旨在概述已取得的成果，還旨在強調與所使用數據類型相關的難點，以及針對每個研究目標應用的學習方法。它將這些貢獻歸類為三個領域：使用公共衛生數據來預防傳染病在一個地區內傳播的預測；使用患者醫療數據來檢測一個人是否感染了傳染病的預測；以及使用公共和患者醫療數據來估計疾病在人群中傳播程度的預測。本文還批判性地評估了人工智能的潛力，並概述了它在傳染病管理中的局限性。

##### **The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models**
2411.08870v1 by Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst

Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare ten
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting and supervised fine-tuning regimes for medical question-answering
(QA). For instance, across all tasks and model pairs we consider in the 3-shot
setting, medical LLMs only outperform their base models in 22.7% of cases,
reach a (statistical) tie in 36.8% of cases, and are significantly worse than
their base models in the remaining 40.5% of cases. Our conclusions are based on
(i) comparing each medical model head-to-head, directly against the
corresponding base model; (ii) optimizing the prompts for each model separately
in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty
in comparisons. While these basic practices are not consistently adopted in the
literature, our ablations show that they substantially impact conclusions.
Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs
can show performance improvements, but the benefits do not carry over to tasks
based on clinical notes. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.

摘要：<paragraph>最近有許多研究專門開發醫療應用基礎模型，透過持續預訓練公開的生物醫學語料庫，改編通用大型語言模型 (LLM) 和視覺語言模型 (VLM)。這些研究通常聲稱此類領域自適應預訓練 (DAPT) 能提升下游醫療任務的效能，例如回答醫療執照考試題目。在本文中，我們比較了十個公開的「醫療」LLM 和兩個 VLM，並將其與對應的基本模型進行比較，得出了不同的結論：所有醫療 VLM 和幾乎所有醫療 LLM 都無法在醫療問題解答 (QA) 的零次/小樣本提示和監督微調機制中持續優於其基本模型。例如，在我們在 3 次取樣設定中考量的所有任務和模型配對中，醫療 LLM 僅在 22.7% 的案例中優於其基本模型，在 36.8% 的案例中達到（統計）平手，而在其餘 40.5% 的案例中則顯著低於其基本模型。我們的結論基於 (i) 將每個醫療模型與對應的基本模型進行一對一比較；(ii) 在零次/小樣本提示中分別針對每個模型最佳化提示；以及 (iii) 在比較中考量統計不確定性。儘管這些基本做法並未在文獻中一致採用，但我們的消融研究顯示，它們對結論有重大影響。同時，我們發現，在針對特定 QA 任務進行微調後，醫療 LLM 可以展現效能提升，但這些好處並未延續到基於臨床筆記的任務。我們的研究結果表明，最先進的通用領域模型可能已經展現出強大的醫療知識和推理能力，並提供建議以強化未來研究的結論。</paragraph>

##### **MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification**
2411.08703v1 by Shan Cong, Zhiling Sang, Hongwei Liu, Haoran Luo, Xin Wang, Hong Liang, Jie Hao, Xiaohui Yao

The distinct characteristics of multiomics data, including complex
interactions within and across biological layers and disease heterogeneity
(e.g., heterogeneity in etiology and clinical symptoms), drive us to develop
novel designs to address unique challenges in multiomics prediction. In this
paper, we propose the multi-view knowledge transfer learning (MVKTrans)
framework, which transfers intra- and inter-omics knowledge in an adaptive
manner by reviewing data heterogeneity and suppressing bias transfer, thereby
enhancing classification performance. Specifically, we design a graph
contrastive module that is trained on unlabeled data to effectively learn and
transfer the underlying intra-omics patterns to the supervised task. This
unsupervised pretraining promotes learning general and unbiased representations
for each modality, regardless of the downstream tasks. In light of the varying
discriminative capacities of modalities across different diseases and/or
samples, we introduce an adaptive and bi-directional cross-omics distillation
module. This module automatically identifies richer modalities and facilitates
dynamic knowledge transfer from more informative to less informative omics,
thereby enabling a more robust and generalized integration. Extensive
experiments on four real biomedical datasets demonstrate the superior
performance and robustness of MVKTrans compared to the state-of-the-art. Code
and data are available at https://github.com/Yaolab-fantastic/MVKTrans.

摘要：多組學資料的獨特特徵，包括生物層內和層間的複雜交互作用和疾病異質性（例如，病因和臨床症狀的異質性），促使我們開發新穎的設計來應對多組學預測中的獨特挑戰。在本文中，我們提出了多視角知識遷移學習 (MVKTrans) 框架，它通過審查數據異質性和抑制偏差轉移，以自適應的方式傳遞組內和組間知識，從而增強分類性能。具體來說，我們設計了一個圖對比模組，在未標記數據上進行訓練，以有效地學習和將組內模式轉移到監督任務中。這種無監督的預訓練促進了學習一般且無偏差的表示，適用於每個模態，無論下游任務為何。鑑於不同疾病和/或樣本中模態的不同辨別能力，我們引入了一個自適應且雙向的組間知識蒸餾模組。此模組自動識別更豐富的模態，並促進從更有資訊性的組學到資訊較少的組學的動態知識轉移，從而實現更強健且更廣泛的整合。對四個真實生物醫學資料集的廣泛實驗證明了 MVKTrans 與最先進技術相比具有優異的性能和強健性。程式碼和資料可在 https://github.com/Yaolab-fantastic/MVKTrans 取得。

##### **TRACE: Transformer-based Risk Assessment for Clinical Evaluation**
2411.08701v1 by Dionysis Christopoulos, Sotiris Spanos, Valsamis Ntouskos, Konstantinos Karantzalos

We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),
a novel method for clinical risk assessment based on clinical data, leveraging
the self-attention mechanism for enhanced feature interaction and result
interpretation. Our approach is able to handle different data modalities,
including continuous, categorical and multiple-choice (checkbox) attributes.
The proposed architecture features a shared representation of the clinical data
obtained by integrating specialized embeddings of each data modality, enabling
the detection of high-risk individuals using Transformer encoder layers. To
assess the effectiveness of the proposed method, a strong baseline based on
non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method
outperforms various baselines widely used in the domain of clinical risk
assessment, while effectively handling missing values. In terms of
explainability, our Transformer-based method offers easily interpretable
results via attention weights, further enhancing the clinicians'
decision-making process.

摘要：我們提出 TRACE（臨床評估的基於 Transformer 的風險評估），這是一種基於臨床數據的臨床風險評估新方法，利用自注意力機制增強特徵交互和結果解讀。我們的做法能夠處理不同的數據模式，包括連續、分類和多選（核取方塊）屬性。提議的架構特徵是通過整合每個數據模式的專用嵌入來獲得臨床數據的共享表示，從而能夠使用 Transformer 編碼器層檢測高風險個體。為了評估所提出方法的有效性，引入了基於非負多層感知器 (MLP) 的強大基線。所提出的方法優於臨床風險評估領域中廣泛使用的各種基線，同時有效地處理缺失值。在可解釋性方面，我們基於 Transformer 的方法通過注意力權重提供了易於解釋的結果，進一步增強了臨床醫生的決策制定過程。

##### **Rethinking negative sampling in content-based news recommendation**
2411.08700v1 by Miguel Ângelo Rebelo, João Vinagre, Ivo Pereira, Álvaro Figueira

News recommender systems are hindered by the brief lifespan of articles, as
they undergo rapid relevance decay. Recent studies have demonstrated the
potential of content-based neural techniques in tackling this problem. However,
these models often involve complex neural architectures and often lack
consideration for negative examples. In this study, we posit that the careful
sampling of negative examples has a big impact on the model's outcome. We
devise a negative sampling technique that not only improves the accuracy of the
model but also facilitates the decentralization of the recommendation system.
The experimental results obtained using the MIND dataset demonstrate that the
accuracy of the method under consideration can compete with that of
State-of-the-Art models. The utilization of the sampling technique is essential
in reducing model complexity and accelerating the training process, while
maintaining a high level of accuracy. Finally, we discuss how decentralized
models can help improve privacy and scalability.

摘要：新聞推薦系統受到文章生命週期短暫的阻礙，因為它們會快速衰退。最近的研究已證明基於內容的神經技術在解決這個問題上的潛力。然而，這些模型通常涉及複雜的神經架構，而且常常缺乏對負面範例的考量。在這項研究中，我們假設負面範例的仔細抽樣對模型的結果有很大的影響。我們設計了一種負面抽樣技術，它不僅提高了模型的準確性，還促进了推薦系統的分散化。使用 MIND 資料集獲得的實驗結果證明，所考慮方法的準確性可以與最先進的模型相媲美。抽樣技術的使用對於降低模型複雜性和加速訓練過程至關重要，同時保持高準確度。最後，我們討論了分散式模型如何有助於改善隱私和可擴展性。

##### **A Survey on Vision Autoregressive Model**
2411.08666v2 by Kai Jiang, Jiaxing Huang

Autoregressive models have demonstrated great performance in natural language
processing (NLP) with impressive scalability, adaptability and
generalizability. Inspired by their notable success in NLP field,
autoregressive models have been intensively investigated recently for computer
vision, which perform next-token predictions by representing visual data as
visual tokens and enables autoregressive modelling for a wide range of vision
tasks, ranging from visual generation and visual understanding to the very
recent multimodal generation that unifies visual generation and understanding
with a single autoregressive model. This paper provides a systematic review of
vision autoregressive models, including the development of a taxonomy of
existing methods and highlighting their major contributions, strengths, and
limitations, covering various vision tasks such as image generation, video
generation, image editing, motion generation, medical image analysis, 3D
generation, robotic manipulation, unified multimodal generation, etc. Besides,
we investigate and analyze the latest advancements in autoregressive models,
including thorough benchmarking and discussion of existing methods across
various evaluation datasets. Finally, we outline key challenges and promising
directions for future research, offering a roadmap to guide further
advancements in vision autoregressive models.

摘要：自回归模型在自然语言处理 (NLP) 中展现了极佳的性能，具有令人印象深刻的可扩展性、适应性和泛化性。受其在 NLP 领域的显著成功启发，自回归模型最近在计算机视觉领域得到了深入研究，通过将视觉数据表示为视觉标记来执行下一个标记预测，并为广泛的视觉任务启用自回归建模，从视觉生成和视觉理解到最近将视觉生成和理解统一到一个自回归模型中的多模态生成。本文对视觉自回归模型进行了系统性综述，包括对现有方法进行分类，并重点介绍了它们的主要贡献、优点和局限性，涵盖了图像生成、视频生成、图像编辑、动作生成、医学图像分析、3D 生成、机器人操作、统一多模态生成等各种视觉任务。此外，我们还调查和分析了自回归模型的最新进展，包括对现有方法在各种评估数据集上的全面基准测试和讨论。最后，我们概述了未来研究的关键挑战和有希望的方向，为视觉自回归模型的进一步发展提供了路线图。

##### **Optimizing Automatic Summarization of Long Clinical Records Using Dynamic Context Extension:Testing and Evaluation of the NBCE Method**
2411.08586v2 by Guoqing Zhang, Keita Fukuyama, Kazumasa Kishimoto, Tomohiro Kuroda

Summarizing patient clinical notes is vital for reducing documentation
burdens. Current manual summarization makes medical staff struggle. We propose
an automatic method using LLMs, but long inputs cause LLMs to lose context,
reducing output quality especially in small size model. We used a 7B model,
open-calm-7b, enhanced with Native Bayes Context Extend and a redesigned
decoding mechanism to reference one sentence at a time, keeping inputs within
context windows, 2048 tokens. Our improved model achieved near parity with
Google's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating
strong performance using less resources, enhancing automated EMR summarization
feasibility.

摘要：摘要病患臨床筆記對於減輕文件負擔至關重要。目前的摘要手冊讓醫療人員難以應付。我們提出使用大型語言模型 (LLM) 的自動化方法，但過長的輸入會導致大型語言模型失去上下文，降低輸出品質，特別是在小型模型中。我們使用一個 7B 模型，open-calm-7b，並搭配 Native Bayes Context Extend 和重新設計的解碼機制，一次參考一個句子，將輸入保持在上下文視窗中，2048 個符號。我們改進的模型在 ROUGE-L 指標上達到接近 Google 超過 175B 的 Gemini，有 200 個範本，表示使用較少資源就能有強大的效能，提升自動化電子病歷摘要的可行性。

##### **A Heterogeneous Graph Neural Network Fusing Functional and Structural Connectivity for MCI Diagnosis**
2411.08424v1 by Feiyu Yin, Yu Lei, Siyuan Dai, Wenwen Zeng, Guoqing Wu, Liang Zhan, Jinhua Yu

Brain connectivity alternations associated with brain disorders have been
widely reported in resting-state functional imaging (rs-fMRI) and diffusion
tensor imaging (DTI). While many dual-modal fusion methods based on graph
neural networks (GNNs) have been proposed, they generally follow homogenous
fusion ways ignoring rich heterogeneity of dual-modal information. To address
this issue, we propose a novel method that integrates functional and structural
connectivity based on heterogeneous graph neural networks (HGNNs) to better
leverage the rich heterogeneity in dual-modal images. We firstly use blood
oxygen level dependency and whiter matter structure information provided by
rs-fMRI and DTI to establish homo-meta-path, capturing node relationships
within the same modality. At the same time, we propose to establish
hetero-meta-path based on structure-function coupling and brain community
searching to capture relations among cross-modal nodes. Secondly, we further
introduce a heterogeneous graph pooling strategy that automatically balances
homo- and hetero-meta-path, effectively leveraging heterogeneous information
and preventing feature confusion after pooling. Thirdly, based on the
flexibility of heterogeneous graphs, we propose a heterogeneous graph data
augmentation approach that can conveniently address the sample imbalance issue
commonly seen in clinical diagnosis. We evaluate our method on ADNI-3 dataset
for mild cognitive impairment (MCI) diagnosis. Experimental results indicate
the proposed method is effective and superior to other algorithms, with a mean
classification accuracy of 93.3%.

摘要：腦部連通性變化與腦部疾病的關聯性已在靜態功能性影像 (rs-fMRI) 和擴散張量影像 (DTI) 中廣泛報導。雖然已經提出許多基於圖形神經網路 (GNN) 的雙模態融合方法，但它們通常遵循同質融合方式，忽略了雙模態資訊的豐富異質性。為了解決這個問題，我們提出了一種新方法，它整合了基於異質圖形神經網路 (HGNN) 的功能性和結構性連通性，以更好地利用雙模態影像中的豐富異質性。我們首先使用 rs-fMRI 和 DTI 提供的血氧濃度依賴性和白質結構資訊來建立同質元路徑，捕捉同一模式內的節點關係。同時，我們提議建立基於結構功能耦合和腦部社群搜尋的異質元路徑，以捕捉跨模式節點之間的關係。其次，我們進一步引入了一個異質圖形池化策略，該策略自動平衡同質和異質元路徑，有效利用異質資訊並防止池化後特徵混淆。第三，基於異質圖形的靈活性，我們提出了一種異質圖形資料擴充方法，可以方便地解決臨床診斷中常見的樣本不平衡問題。我們在 ADNI-3 資料集上評估了我們的方法，用於輕度認知障礙 (MCI) 診斷。實驗結果表明，所提出的方法有效且優於其他演算法，平均分類準確率為 93.3%。

##### **A Fuzzy Reinforcement LSTM-based Long-term Prediction Model for Fault Conditions in Nuclear Power Plants**
2411.08370v1 by Siwei Li, Jiayan Fang, Yichun Wua, Wei Wang, Chengxin Li, Jiangwen Chen

Early fault detection and timely maintenance scheduling can significantly
mitigate operational risks in NPPs and enhance the reliability of operator
decision-making. Therefore, it is necessary to develop an efficient Prognostics
and Health Management (PHM) multi-step prediction model for predicting of
system health status and prompt execution of maintenance operations. In this
study, we propose a novel predictive model that integrates reinforcement
learning with Long Short-Term Memory (LSTM) neural networks and the Expert
Fuzzy Evaluation Method. The model is validated using parameter data for 20
different breach sizes in the Main Steam Line Break (MSLB) accident condition
of the CPR1000 pressurized water reactor simulation model and it demonstrates a
remarkable capability in accurately forecasting NPP parameter changes up to 128
steps ahead (with a time interval of 10 seconds per step, i.e., 1280 seconds),
thereby satisfying the temporal advance requirement for fault prognostics in
NPPs. Furthermore, this method provides an effective reference solution for PHM
applications such as anomaly detection and remaining useful life prediction.

摘要：早期故障偵測和及時維護排程可以顯著降低核能電廠的營運風險，並提升操作人員決策的可靠性。因此，有必要開發一個高效的預測與健康管理 (PHM) 多步驟預測模型，用於預測系統健康狀態和及時執行維護作業。在此研究中，我們提出一個創新的預測模型，它整合了強化學習與長期短期記憶 (LSTM) 神經網路和專家模糊評估方法。該模型使用 CPR1000 加壓水反應爐模擬模型中主蒸汽管破裂 (MSLB) 事故條件下 20 種不同破裂尺寸的參數資料進行驗證，它展現出準確預測核能電廠參數變化長達 128 個步驟（每個步驟的時間間隔為 10 秒，即 1280 秒）的卓越能力，從而滿足核能電廠故障預測的時間提前需求。此外，此方法為異常偵測和剩餘使用壽命預測等 PHM 應用提供了一個有效的參考解決方案。

##### **TowerDebias: A Novel Debiasing Method based on the Tower Property**
2411.08297v1 by Norman Matloff, Aditya Mittal

Decision-making processes have increasingly come to rely on sophisticated
machine learning tools, raising concerns about the fairness of their
predictions with respect to any sensitive groups. The widespread use of
commercial black-box machine learning models necessitates careful consideration
of their legal and ethical implications on consumers. In situations where users
have access to these "black-box" models, a key question emerges: how can we
mitigate or eliminate the influence of sensitive attributes, such as race or
gender? We propose towerDebias (tDB), a novel approach designed to reduce the
influence of sensitive variables in predictions made by black-box models. Using
the Tower Property from probability theory, tDB aims to improve prediction
fairness during the post-processing stage in a manner amenable to the
Fairness-Utility Tradeoff. This method is highly flexible, requiring no prior
knowledge of the original model's internal structure, and can be extended to a
range of different applications. We provide a formal improvement theorem for
tDB and demonstrate its effectiveness in both regression and classification
tasks, underscoring its impact on the fairness-utility tradeoff.

摘要：決策制定過程越來越依賴於先進機器學習工具，這引起了人們對其預測的公平性是否會對任何敏感群體造成影響的擔憂。商業黑盒機器學習模型的廣泛使用需要仔細考慮其對消費者的法律和道德影響。在使用者能夠使用這些「黑盒」模型的情況下，一個關鍵問題浮現：我們如何減輕或消除敏感屬性（例如種族或性別）的影響？我們提出 towerDebias (tDB)，這是一種新穎的方法，旨在減少黑盒模型所做預測中敏感變數的影響。tDB 使用機率論中的 Tower 屬性，旨在以有利於公平性-效用權衡的方式在後處理階段改善預測公平性。此方法非常靈活，不需要事先了解原始模型的內部結構，並且可以擴展到各種不同的應用程式。我們為 tDB 提供了正式的改進定理，並展示了它在迴歸和分類任務中的有效性，強調了它對公平性-效用權衡的影響。

##### **Scaling Properties of Diffusion Models for Perceptual Tasks**
2411.08034v3 by Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, Jitendra Malik

In this paper, we argue that iterative computation with diffusion models
offers a powerful paradigm for not only generation but also visual perception
tasks. We unify tasks such as depth estimation, optical flow, and amodal
segmentation under the framework of image-to-image translation, and show how
diffusion models benefit from scaling training and test-time compute for these
perceptual tasks. Through a careful analysis of these scaling properties, we
formulate compute-optimal training and inference recipes to scale diffusion
models for visual perception tasks. Our models achieve competitive performance
to state-of-the-art methods using significantly less data and compute. To
access our code and models, see https://scaling-diffusion-perception.github.io .

摘要：在本文中，我們論證使用擴散模型進行迭代計算提供了一個強大的範例，不僅適用於生成任務，也適用於視覺感知任務。我們將深度估計、光流和非模態分割等任務統一在圖像到圖像轉換的框架下，並展示擴散模型如何從擴展訓練和測試時間計算中受益，以執行這些感知任務。透過仔細分析這些擴展屬性，我們制定了計算最佳的訓練和推論範例，以擴展用於視覺感知任務的擴散模型。我們的模型使用明顯較少資料和運算，便能達到與最先進方法相當的效能。若要存取我們的程式碼和模型，請參閱 https://scaling-diffusion-perception.github.io 。

##### **Investigating the Effectiveness of Explainability Methods in Parkinson's Detection from Speech**
2411.08013v2 by Eleonora Mancini, Francesco Paissan, Paolo Torroni, Mirco Ravanelli, Cem Subakan

Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.

摘要：帕金森氏症 (PD) 的言語障礙提供了重要的早期診斷指標。雖然基於言語的 PD 檢測模型已顯示出強勁的效能，但其可解釋性仍未得到充分探討。本研究系統性地評估了幾種可解釋性方法，以識別 PD 特有的言語特徵，旨在支援開發準確、可解釋的模型，用於 PD 診斷和監測中的臨床決策。我們的研究方法包括：(i) 使用主流可解釋性技術獲得歸因和顯著性圖，(ii) 定量評估這些圖及其通過聯集和交集獲得的組合的忠實度，通過一系列已建立的指標，以及 (iii) 評估顯著性圖傳達的資訊，用於輔助分類器的 PD 檢測。我們的結果表明，雖然解釋與分類器一致，但它們通常無法為領域專家提供有價值的資訊。

