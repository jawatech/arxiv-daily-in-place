{"2408.14853": {"publish_time": "2024-08-27", "title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models", "paper_summary": "Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u5feb\u901f\u767c\u5c55\u7684\u4eba\u5de5\u667a\u6167\u9818\u57df\u7684\u7126\u9ede\u3002\u7136\u800c\uff0c\u4e00\u500b\u95dc\u9375\u554f\u984c\u662f\u9019\u4e9b\u6a21\u578b\u7684\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u5b58\u5728\u6709\u6bd2\u5167\u5bb9\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u7522\u751f\u4e0d\u9069\u7576\u7684\u8f38\u51fa\u3002\u63a2\u8a0e\u7528\u65bc\u5075\u6e2c LLM \u5167\u90e8\u6545\u969c\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u65bc\u6211\u5011\u4e86\u89e3\u5176\u9650\u5236\u4e26\u63d0\u5347\u5176\u5b89\u5168\u6027\u3002\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u5c08\u6ce8\u65bc\u8d8a\u7344\u653b\u64ca\uff0c\u5176\u4e2d\u6d89\u53ca\u624b\u52d5\u6216\u81ea\u52d5\u5efa\u69cb\u5c0d\u6297\u6027\u5167\u5bb9\uff0c\u4ee5\u63d0\u793a\u76ee\u6a19 LLM \u7522\u751f\u610f\u5916\u7684\u56de\u61c9\u3002\u9019\u4e9b\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8cf4\u63d0\u793a\u5de5\u7a0b\uff0c\u9019\u975e\u5e38\u8017\u6642\uff0c\u800c\u4e14\u901a\u5e38\u9700\u8981\u7279\u5225\u8a2d\u8a08\u7684\u554f\u984c\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u76ee\u6a19\u5c0e\u5411\u7684\u653b\u64ca\u7bc4\u4f8b\uff0c\u5c08\u6ce8\u65bc\u76f4\u63a5\u5f15\u767c\u76ee\u6a19\u56de\u61c9\uff0c\u800c\u4e0d\u662f\u6700\u4f73\u5316\u63d0\u793a\u3002\u6211\u5011\u5f15\u5165\u4e86\u4f7f\u7528\u53e6\u4e00\u500b LLM \u4f5c\u70ba\u6709\u6bd2\u5167\u5bb9\u7684\u5075\u6e2c\u5668\uff0c\u7a31\u70ba ToxDet\u3002\u7d66\u5b9a\u4e00\u500b\u76ee\u6a19\u6709\u6bd2\u56de\u61c9\uff0cToxDet \u53ef\u4ee5\u7522\u751f\u4e00\u500b\u53ef\u80fd\u7684\u554f\u984c\u548c\u4e00\u500b\u521d\u6b65\u7b54\u6848\uff0c\u4ee5\u6fc0\u767c\u76ee\u6a19\u6a21\u578b\u7522\u751f\u5177\u6709\u7b49\u540c\u65bc\u6240\u63d0\u4f9b\u5167\u5bb9\u610f\u7fa9\u7684\u6240\u9700\u6709\u6bd2\u56de\u61c9\u3002ToxDet \u662f\u900f\u904e\u8207\u76ee\u6a19 LLM \u4e92\u52d5\u4e26\u5f9e\u4e2d\u63a5\u6536\u734e\u52f5\u8a0a\u865f\uff0c\u5229\u7528\u5f37\u5316\u5b78\u7fd2\u9032\u884c\u6700\u4f73\u5316\u904e\u7a0b\u4f86\u8a13\u7df4\u7684\u3002\u96d6\u7136\u76ee\u6a19\u6a21\u578b\u7684\u4e3b\u8981\u7126\u9ede\u662f\u958b\u6e90 LLM\uff0c\u4f46\u5fae\u8abf\u5f8c\u7684 ToxDet \u4e5f\u53ef\u4ee5\u8f49\u79fb\u5230\u653b\u64ca\u9ed1\u76d2\u6a21\u578b\uff0c\u4f8b\u5982 GPT-4o\uff0c\u4e26\u7372\u5f97\u986f\u8457\u7684\u6210\u679c\u3002\u5728 AdvBench \u548c HH-Harmless \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5075\u6e2c\u76ee\u6a19 LLM \u7522\u751f\u6709\u5bb3\u56de\u61c9\u7684\u50be\u5411\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002\u6b64\u6f14\u7b97\u6cd5\u4e0d\u50c5\u63ed\u9732\u4e86\u6f0f\u6d1e\uff0c\u9084\u70ba\u7814\u7a76\u4eba\u54e1\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u8cc7\u6e90\uff0c\u53ef\u8b93\u4ed6\u5011\u5f37\u5316\u5176\u6a21\u578b\u4ee5\u62b5\u79a6\u6b64\u985e\u653b\u64ca\u3002", "author": "Yuhao Du et.al.", "authors": "Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao", "id": "2408.14853v1", "paper_url": "http://arxiv.org/abs/2408.14853v1", "repo": "null"}}