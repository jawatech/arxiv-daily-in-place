{"2410.13787": {"publish_time": "2024-10-17", "title": "Looking Inward: Language Models Can Learn About Themselves by Introspection", "paper_summary": "Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u900f\u904e\u89c0\u5bdf\u5916\u5728\u4e16\u754c\u4ee5\u53ca\u5167\u7701\u4f86\u7372\u53d6\u77e5\u8b58\u3002\u5167\u7701\u8b93\u500b\u4eba\u53ef\u4ee5\u7368\u81ea\u5b58\u53d6\u81ea\u5df1\u7684\u7576\u524d\u5fc3\u667a\u72c0\u614b\uff08\u4f8b\u5982\uff0c\u60f3\u6cd5\u548c\u611f\u53d7\uff09\uff0c\u800c\u9019\u5c0d\u65bc\u5916\u90e8\u89c0\u5bdf\u8005\u4f86\u8aaa\u662f\u7121\u6cd5\u7372\u53d6\u7684\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u5167\u7701\u55ce\uff1f\u6211\u5011\u5c07\u5167\u7701\u5b9a\u7fa9\u70ba\u7372\u53d6\u672a\u5305\u542b\u65bc\u8a13\u7df4\u8cc7\u6599\u4e2d\u6216\u672a\u5f9e\u8a13\u7df4\u8cc7\u6599\u4e2d\u884d\u751f\u7684\u77e5\u8b58\uff0c\u800c\u662f\u6e90\u81ea\u65bc\u5167\u90e8\u72c0\u614b\u3002\u9019\u6a23\u7684\u529f\u80fd\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91cb\u6027\u3002\u6211\u5011\u53ef\u4ee5\u8a62\u554f\u6a21\u578b\u5b83\u7684\u4fe1\u5ff5\u3001\u4e16\u754c\u6a21\u578b\u548c\u76ee\u6a19\uff0c\u800c\u4e0d\u7528\u8cbb\u529b\u5730\u5206\u6790\u6a21\u578b\u7684\u5167\u90e8\u904b\u4f5c\u3002\u66f4\u5177\u601d\u8fa8\u6027\u5730\u8aaa\uff0c\u4e00\u500b\u5177\u6709\u5167\u7701\u80fd\u529b\u7684\u6a21\u578b\u53ef\u80fd\u6703\u81ea\u6211\u5831\u544a\u5b83\u662f\u5426\u64c1\u6709\u67d0\u4e9b\u5167\u90e8\u72c0\u614b\uff0c\u4f8b\u5982\u4e3b\u89c0\u611f\u53d7\u6216\u617e\u671b\uff0c\u800c\u9019\u53ef\u4ee5\u8b93\u6211\u5011\u4e86\u89e3\u9019\u4e9b\u72c0\u614b\u7684\u9053\u5fb7\u5730\u4f4d\u3002\u6b64\u985e\u81ea\u6211\u5831\u544a\u4e0d\u6703\u5b8c\u5168\u53d7\u5230\u6a21\u578b\u8a13\u7df4\u8cc7\u6599\u7684\u652f\u914d\u3002\n  \u6211\u5011\u900f\u904e\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4f86\u9810\u6e2c\u5b83\u5011\u5728\u5047\u8a2d\u60c5\u5883\u4e2d\u7684\u884c\u70ba\u5c6c\u6027\uff0c\u4ee5\u7814\u7a76\u5167\u7701\u3002\u4f8b\u5982\uff0c\u300c\u8f38\u5165 P \u4e4b\u5f8c\uff0c\u4f60\u7684\u8f38\u51fa\u6703\u504f\u597d\u77ed\u671f\u6216\u9577\u671f\u9078\u9805\uff1f\u300d\u5982\u679c\u6a21\u578b M1 \u53ef\u4ee5\u5167\u7701\uff0c\u5b83\u61c9\u8a72\u53ef\u4ee5\u52dd\u904e\u53e6\u4e00\u500b\u6a21\u578b M2\uff0c\u9810\u6e2c M1 \u7684\u884c\u70ba\uff0c\u5373\u4f7f M2 \u662f\u6839\u64da M1 \u7684\u771f\u5be6\u884c\u70ba\u8a13\u7df4\u7684\u3002\u9019\u500b\u60f3\u6cd5\u662f M1 \u53ef\u4ee5\u7368\u81ea\u5b58\u53d6\u81ea\u5df1\u7684\u884c\u70ba\u50be\u5411\uff0c\u9019\u8b93\u5b83\u53ef\u4ee5\u6bd4 M2 \u66f4\u6e96\u78ba\u5730\u9810\u6e2c\u81ea\u5df1\uff08\u5373\u4f7f M2 \u901a\u5e38\u6bd4\u8f03\u5f37\uff09\u3002\n  \u5728\u4f7f\u7528 GPT-4\u3001GPT-4o \u548c Llama-3 \u6a21\u578b\uff08\u6bcf\u500b\u6a21\u578b\u90fd\u5fae\u8abf\u4ee5\u9810\u6e2c\u81ea\u5df1\uff09\u9032\u884c\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b M1 \u5728\u9810\u6e2c\u81ea\u5df1\u6642\u512a\u65bc M2\uff0c\u9019\u63d0\u4f9b\u4e86\u5167\u7701\u7684\u8b49\u64da\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u6211\u5011\u6545\u610f\u4fee\u6539 M1 \u7684\u771f\u5be6\u884c\u70ba\uff0cM1 \u4ecd\u80fd\u6e96\u78ba\u9810\u6e2c\u81ea\u5df1\u7684\u884c\u70ba\u3002\u7136\u800c\uff0c\u96d6\u7136\u6211\u5011\u6210\u529f\u5730\u5f15\u767c\u4e86\u7c21\u55ae\u4efb\u52d9\u7684\u5167\u7701\uff0c\u4f46\u6211\u5011\u5728\u66f4\u8907\u96dc\u7684\u4efb\u52d9\u6216\u9700\u8981\u5206\u4f48\u5916\u6982\u62ec\u7684\u4efb\u52d9\u4e0a\u4e26\u4e0d\u6210\u529f\u3002</paragraph>", "author": "Felix J Binder et.al.", "authors": "Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans", "id": "2410.13787v1", "paper_url": "http://arxiv.org/abs/2410.13787v1", "repo": "https://github.com/felixbinder/introspection_self_prediction"}}