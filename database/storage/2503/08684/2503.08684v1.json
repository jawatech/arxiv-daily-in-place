{"2503.08684": {"publish_time": "2025-03-11", "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents", "paper_summary": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.", "paper_summary_zh": "<paragraph>\u5148\u524d\u7684\u7814\u7a76\u767c\u73fe\uff0c\u57fa\u65bc PLM \u7684\u6aa2\u7d22\u6a21\u578b\u6703\u8868\u73fe\u51fa\u5c0d LLM \u751f\u6210\u5167\u5bb9\u7684\u504f\u597d\uff0c\u5373\u4f7f\u9019\u4e9b\u6587\u4ef6\u7684\u8a9e\u7fa9\u54c1\u8cea\u8207\u4eba\u5de5\u7de8\u5beb\u7684\u6587\u4ef6\u76f8\u7576\uff0c\u4e5f\u6703\u7d66\u4e88\u5b83\u5011\u66f4\u9ad8\u7684\u76f8\u95dc\u6027\u5206\u6578\u3002\u9019\u7a2e\u73fe\u8c61\u88ab\u7a31\u70ba\u4f86\u6e90\u504f\u5dee\uff0c\u5a01\u8105\u8457\u8cc7\u8a0a\u6aa2\u7d22\u751f\u614b\u7cfb\u7d71\u7684\u53ef\u6301\u7e8c\u767c\u5c55\u3002\u7136\u800c\uff0c\u4f86\u6e90\u504f\u5dee\u7684\u6839\u672c\u539f\u56e0\u4ecd\u672a\u88ab\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7528\u56e0\u679c\u5716\u4f86\u89e3\u91cb\u8cc7\u8a0a\u6aa2\u7d22\u7684\u904e\u7a0b\uff0c\u4e26\u767c\u73fe\u57fa\u65bc PLM \u7684\u6aa2\u7d22\u5668\u6703\u5b78\u7fd2\u56f0\u60d1\u5ea6\u7279\u5fb5\u4f86\u4f30\u8a08\u76f8\u95dc\u6027\uff0c\u900f\u904e\u5c07\u4f4e\u56f0\u60d1\u5ea6\u7684\u6587\u4ef6\u6392\u5728\u66f4\u9ad8\u7684\u4f4d\u7f6e\u800c\u5c0e\u81f4\u4f86\u6e90\u504f\u5dee\u3002\u7406\u8ad6\u5206\u6790\u9032\u4e00\u6b65\u63ed\u793a\uff0c\u9019\u7a2e\u73fe\u8c61\u6e90\u65bc\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u548c\u6aa2\u7d22\u4efb\u52d9\u4e2d\u640d\u5931\u51fd\u6578\u68af\u5ea6\u7684\u6b63\u76f8\u95dc\u6027\u3002\u57fa\u65bc\u6b64\u5206\u6790\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u56e0\u679c\u555f\u767c\u7684\u63a8\u7406\u6642\u53bb\u504f\u5dee\u65b9\u6cd5\uff0c\u7a31\u70ba\u56e0\u679c\u8a3a\u65b7\u548c\u6821\u6b63 (CDC)\u3002CDC \u9996\u5148\u8a3a\u65b7\u56f0\u60d1\u5ea6\u7684\u504f\u5dee\u6548\u61c9\uff0c\u7136\u5f8c\u5c07\u504f\u5dee\u6548\u61c9\u5f9e\u6574\u9ad4\u4f30\u8a08\u7684\u76f8\u95dc\u6027\u5206\u6578\u4e2d\u5206\u96e2\u51fa\u4f86\u3002\u8de8\u4e09\u500b\u9818\u57df\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 CDC \u512a\u8d8a\u7684\u53bb\u504f\u5dee\u6548\u679c\uff0c\u5f37\u8abf\u4e86\u6211\u5011\u63d0\u51fa\u7684\u89e3\u91cb\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u539f\u59cb\u78bc\u53ef\u5728 https://github.com/WhyDwelledOnAi/Perplexity-Trap \u7372\u5f97\u3002</paragraph>\n", "author": "Haoyu Wang et.al.", "authors": "Haoyu Wang, Sunhao Dai, Haiyuan Zhao, Liang Pang, Xiao Zhang, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen", "id": "2503.08684v1", "paper_url": "http://arxiv.org/abs/2503.08684v1", "repo": "null"}}