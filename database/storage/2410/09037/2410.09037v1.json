{"2410.09037": {"publish_time": "2024-10-11", "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "paper_summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5229\u7528\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u5728\u5404\u7a2e\u8907\u96dc\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u8868\u73fe\u3002\u6700\u8fd1\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u77e5\u8b58\u84b8\u993e (KD) \u65b9\u6cd5\uff0c\u5373\u63a8\u7406\u84b8\u993e\uff0c\u5b83\u900f\u904e\u5fae\u8abf LLM \u6559\u5e2b\u7522\u751f\u7684\u591a\u6b65\u9a5f\u4f9d\u64da\u8a9e\u8a00\u6a21\u578b\u4f86\u50b3\u905e LLM \u7684\u9019\u7a2e\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728 LLM \u6559\u5e2b\u6a21\u578b\u4e2d\u8003\u91cf\u5169\u500b\u6311\u6230\u6642\u4e26\u4e0d\u5145\u5206\uff0c\u9019\u5169\u500b\u6311\u6230\u5206\u5225\u662f 1) \u8cc7\u6599\u54c1\u8cea\u548c 2) \u8edf\u6a19\u7c64\u63d0\u4f9b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa Mentor-KD\uff0c\u5b83\u6709\u6548\u5730\u5c07 LLM \u7684\u591a\u6b65\u9a5f\u63a8\u7406\u80fd\u529b\u84b8\u993e\u5230\u8f03\u5c0f\u7684 LLM\uff0c\u540c\u6642\u89e3\u6c7a\u4e0a\u8ff0\u6311\u6230\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528\u5c0e\u5e2b\u3001\u4e2d\u578b\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u5fae\u8abf\u6a21\u578b\uff0c\u4f86\u64f4\u5145\u984d\u5916\u7684 CoT \u6a19\u8a3b\u4e26\u5728\u63a8\u7406\u84b8\u993e\u671f\u9593\u70ba\u5b78\u751f\u6a21\u578b\u63d0\u4f9b\u8edf\u6a19\u7c64\u3002\u6211\u5011\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u4e26\u78ba\u8a8d Mentor-KD \u5728\u5404\u7a2e\u6a21\u578b\u548c\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u3002", "author": "Hojae Lee et.al.", "authors": "Hojae Lee, Junho Kim, SangKeun Lee", "id": "2410.09037v1", "paper_url": "http://arxiv.org/abs/2410.09037v1", "repo": "https://github.com/2hojae/mentor-kd"}}