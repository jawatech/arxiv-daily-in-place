{"2502.04077": {"publish_time": "2025-02-06", "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference", "paper_summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u767c\u5c55\uff0c\u900f\u904e Key-Value\uff08KV\uff09\u5feb\u53d6\u58d3\u7e2e\u9032\u884c\u7684\u6709\u6548\u63a8\u8ad6\u5099\u53d7\u95dc\u6ce8\uff0c\u7279\u5225\u662f\u9577\u8a9e\u5883\u751f\u6210\u3002\u70ba\u4e86\u58d3\u7e2e KV \u5feb\u53d6\uff0c\u8fd1\u671f\u65b9\u6cd5\u900f\u904e\u6ce8\u610f\u529b\u5206\u6578\u9032\u884c\u555f\u767c\u5f0f\u6392\u5e8f\uff0c\u4f86\u8b58\u5225\u95dc\u9375 KV \u6a19\u8a18\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u96e3\u4ee5\u6e96\u78ba\u5730\u5224\u65b7\u95dc\u9375\u6a19\u8a18\uff0c\u56e0\u70ba\u5b83\u5011\u5ffd\u7565\u4e86\u6ce8\u610f\u529b\u5206\u6578\u4e2d\u7684\u300c\u6642\u9593\u6a21\u5f0f\u300d\uff0c\u5c0e\u81f4 LLM \u6548\u80fd\u986f\u8457\u4e0b\u964d\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa AttentionPredictor\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u57fa\u65bc\u5b78\u7fd2\u7684\u95dc\u9375\u6a19\u8a18\u8b58\u5225\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cAttentionPredictor \u5b78\u7fd2\u4e00\u500b\u8f15\u91cf\u7d1a\u5377\u7a4d\u6a21\u578b\u4f86\u64f7\u53d6\u6642\u7a7a\u6a21\u5f0f\uff0c\u4e26\u9810\u6e2c\u4e0b\u4e00\u500b\u6a19\u8a18\u7684\u6ce8\u610f\u529b\u5206\u6578\u3002AttentionPredictor \u7684\u4e00\u500b\u5438\u5f15\u4eba\u7279\u9ede\u662f\u5b83\u5728\u6d88\u8017\u6975\u5c11\u8a18\u61b6\u9ad4\u7684\u60c5\u6cc1\u4e0b\u6e96\u78ba\u9810\u6e2c\u6ce8\u610f\u529b\u5206\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8de8\u6a19\u8a18\u95dc\u9375\u5feb\u53d6\u9810\u53d6\u67b6\u69cb\uff0c\u5b83\u96b1\u85cf\u4e86\u6a19\u8a18\u4f30\u8a08\u6642\u9593\u958b\u92b7\uff0c\u4ee5\u52a0\u901f\u89e3\u78bc\u968e\u6bb5\u3002\u900f\u904e\u4fdd\u7559\u5927\u90e8\u5206\u6ce8\u610f\u529b\u8cc7\u8a0a\uff0cAttentionPredictor \u9054\u5230 16 \u500d KV \u5feb\u53d6\u58d3\u7e2e\uff0c\u4e26\u5177\u6709\u76f8\u7576\u7684 LLM \u6548\u80fd\uff0c\u986f\u8457\u512a\u65bc\u73fe\u6709\u6280\u8853\u3002", "author": "Qingyue Yang et.al.", "authors": "Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li", "id": "2502.04077v1", "paper_url": "http://arxiv.org/abs/2502.04077v1", "repo": "null"}}