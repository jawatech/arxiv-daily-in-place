{"2502.14476": {"publish_time": "2025-02-20", "title": "Argument-Based Comparative Question Answering Evaluation Benchmark", "paper_summary": "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u89e3\u6c7a\u963b\u7919\u81ea\u52d5\u6bd4\u8f03\u6027\u554f\u984c\u89e3\u7b54\u7684\u96e3\u984c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8a55\u4f30\u6846\u67b6\uff0c\u7528\u65bc\u8a55\u4f30\u6bd4\u8f03\u6027\u554f\u984c\u89e3\u7b54\u6458\u8981\u7684\u54c1\u8cea\u3002\u6211\u5011\u5236\u5b9a\u4e86 15 \u9805\u6e96\u5247\uff0c\u7528\u65bc\u8a55\u4f30\u4f7f\u7528\u624b\u52d5\u6a19\u8a3b\u548c\u4f86\u81ea 6 \u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u5169\u500b\u6bd4\u8f03\u6027\u554f\u984c\u89e3\u7b54\u8cc7\u6599\u96c6\u7684\u6a19\u8a3b\u6240\u5efa\u7acb\u7684\u6bd4\u8f03\u6027\u7b54\u6848\u3002\u6211\u5011\u5728\u4e0d\u540c\u7684\u8a2d\u5b9a\u4e0b\u4f7f\u7528\u5e7e\u500b LLM \u548c\u624b\u52d5\u6a19\u8a3b\u57f7\u884c\u6e2c\u8a66\uff0c\u4e26\u5c55\u793a\u5169\u7a2e\u8a55\u4f30\u7684\u7d44\u6210\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cLlama-3 70B Instruct \u6a21\u578b\u5728\u6458\u8981\u8a55\u4f30\u4e2d\u8868\u73fe\u6700\u4f73\uff0c\u800c GPT-4 \u5728\u56de\u7b54\u6bd4\u8f03\u6027\u554f\u984c\u65b9\u9762\u8868\u73fe\u6700\u4f73\u3002\u6240\u6709\u4f7f\u7528\u904e\u7684\u8cc7\u6599\u3001\u7a0b\u5f0f\u78bc\u548c\u8a55\u4f30\u7d50\u679c\u5747\u516c\u958b\u53ef\u7528\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}\u3002", "author": "Irina Nikishina et.al.", "authors": "Irina Nikishina, Saba Anwar, Nikolay Dolgov, Maria Manina, Daria Ignatenko, Viktor Moskvoretskii, Artem Shelmanov, Tim Baldwin, Chris Biemann", "id": "2502.14476v1", "paper_url": "http://arxiv.org/abs/2502.14476v1", "repo": "null"}}