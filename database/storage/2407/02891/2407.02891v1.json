{"2407.02891": {"publish_time": "2024-07-03", "title": "GPTQT: Quantize Large Language Models Twice to Push the Efficiency", "paper_summary": "Due to their large size, generative Large Language Models (LLMs) require\nsignificant computing and storage resources. This paper introduces a new\npost-training quantization method, GPTQT, to reduce memory usage and enhance\nprocessing speed by expressing the weight of LLM in 3bit/2bit. Practice has\nshown that minimizing the quantization error of weights is ineffective, leading\nto overfitting. Therefore, GPTQT employs a progressive two-step approach:\ninitially quantizing weights using Linear quantization to a relatively high\nbit, followed by converting obtained int weight to lower bit binary coding. A\nre-explore strategy is proposed to optimize initial scaling factor. During\ninference, these steps are merged into pure binary coding, enabling efficient\ncomputation. Testing across various models and datasets confirms GPTQT's\neffectiveness. Compared to the strong 3-bit quantization baseline, GPTQT\nfurther reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times\non opt-30b. The results on Llama2 show that GPTQT is currently the best binary\ncoding quantization method for such kind of LLMs.", "paper_summary_zh": "\u7531\u65bc\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u898f\u6a21\u9f90\u5927\uff0c\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u548c\u5132\u5b58\u8cc7\u6e90\u3002\u672c\u6587\u4ecb\u7d39\u4e00\u7a2e\u65b0\u7684\u8a13\u7df4\u5f8c\u91cf\u5316\u65b9\u6cd5 GPTQT\uff0c\u900f\u904e\u5c07 LLM \u7684\u6b0a\u91cd\u8868\u793a\u70ba 3 \u4f4d\u5143/2 \u4f4d\u5143\uff0c\u4ee5\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u4e26\u63d0\u5347\u8655\u7406\u901f\u5ea6\u3002\u5be6\u52d9\u4e0a\u5df2\u8b49\u660e\uff0c\u6700\u5c0f\u5316\u6b0a\u91cd\u7684\u91cf\u5316\u8aa4\u5dee\u7121\u6548\uff0c\u6703\u5c0e\u81f4\u904e\u5ea6\u64ec\u5408\u3002\u56e0\u6b64\uff0cGPTQT \u63a1\u7528\u6f38\u9032\u7684\u5169\u6b65\u9a5f\u65b9\u6cd5\uff1a\u6700\u521d\u4f7f\u7528\u7dda\u6027\u91cf\u5316\u5c07\u6b0a\u91cd\u91cf\u5316\u5230\u76f8\u5c0d\u8f03\u9ad8\u7684\u4f4d\u5143\uff0c\u7136\u5f8c\u5c07\u53d6\u5f97\u7684\u6574\u6578\u6b0a\u91cd\u8f49\u63db\u70ba\u8f03\u4f4e\u7684\u4f4d\u5143\u4e8c\u9032\u4f4d\u7de8\u78bc\u3002\u63d0\u51fa\u4e86\u4e00\u7a2e\u91cd\u65b0\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u6700\u4f73\u5316\u521d\u59cb\u7e2e\u653e\u56e0\u5b50\u3002\u5728\u63a8\u8ad6\u671f\u9593\uff0c\u9019\u4e9b\u6b65\u9a5f\u6703\u5408\u4f75\u6210\u7d14\u4e8c\u9032\u4f4d\u7de8\u78bc\uff0c\u5be6\u73fe\u9ad8\u6548\u904b\u7b97\u3002\u8de8\u5404\u7a2e\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u7684\u6e2c\u8a66\u8b49\u5be6\u4e86 GPTQT \u7684\u6709\u6548\u6027\u3002\u8207\u5f37\u5927\u7684 3 \u4f4d\u5143\u91cf\u5316\u57fa\u6e96\u76f8\u6bd4\uff0cGPTQT \u9032\u4e00\u6b65\u5c07 opt-66B \u4e0a\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e86 4.01\uff0c\u4e26\u5c07 opt-30b \u4e0a\u7684\u901f\u5ea6\u63d0\u9ad8\u4e86 1.24 \u500d\u3002\u5728 Llama2 \u4e0a\u7684\u7d50\u679c\u986f\u793a\uff0cGPTQT \u76ee\u524d\u662f\u6b64\u985e LLM \u6700\u4f73\u7684\u4e8c\u9032\u4f4d\u7de8\u78bc\u91cf\u5316\u65b9\u6cd5\u3002", "author": "Yipin Guo et.al.", "authors": "Yipin Guo, Yilin Lang, Qinyuan Ren", "id": "2407.02891v1", "paper_url": "http://arxiv.org/abs/2407.02891v1", "repo": "null"}}