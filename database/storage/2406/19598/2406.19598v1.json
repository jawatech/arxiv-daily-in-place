{"2406.19598": {"publish_time": "2024-06-28", "title": "Mixture of In-Context Experts Enhance LLMs' Long Context Awareness", "paper_summary": "Many studies have revealed that large language models (LLMs) exhibit uneven\nawareness of different contextual positions.Their limited context awareness can\nlead to overlooking critical information and subsequent task failures. While\nseveral approaches have been proposed to enhance LLMs' context awareness,\nachieving both effectiveness and efficiency remains challenging.In this paper,\nfor LLMs utilizing RoPE as position embeddings, we introduce a novel method\ncalled ``Mixture of In-Context Experts'' (MoICE) to address this challenge.\nMoICE comprises two key components: a router integrated into each attention\nhead within LLMs and a lightweight router-only training optimization strategy:\n(1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be\ncapable of directing the attention of a head to specific contextual positions.\nConsequently, each attention head flexibly processes tokens using multiple RoPE\nangles dynamically selected by the router to attend to the needed positions.\nThis approach mitigates the risk of overlooking essential contextual\ninformation. (2) The router-only training strategy entails freezing LLM\nparameters and exclusively updating routers for only a few steps. When applied\nto open-source LLMs including Llama and Mistral, MoICE surpasses prior methods\nacross multiple tasks on long context understanding and generation, all while\nmaintaining commendable inference efficiency.", "paper_summary_zh": "\u8a31\u591a\u7814\u7a76\u5df2\u63ed\u793a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u5c0d\u4e0d\u540c\u8108\u7d61\u4f4d\u7f6e\u7684\u4e0d\u5747\u7b49\u611f\u77e5\u3002\u5b83\u5011\u53d7\u9650\u7684\u8108\u7d61\u611f\u77e5\u53ef\u80fd\u6703\u5c0e\u81f4\u5ffd\u7565\u95dc\u9375\u8cc7\u8a0a\u548c\u5f8c\u7e8c\u7684\u4efb\u52d9\u5931\u6557\u3002\u96d6\u7136\u5df2\u63d0\u51fa\u591a\u7a2e\u65b9\u6cd5\u4f86\u589e\u5f37 LLM \u7684\u8108\u7d61\u611f\u77e5\uff0c\u4f46\u540c\u6642\u9054\u6210\u6709\u6548\u6027\u548c\u6548\u7387\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u5c0d\u65bc\u4f7f\u7528 RoPE \u4f5c\u70ba\u4f4d\u7f6e\u5d4c\u5165\u7684 LLM\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u300c\u8108\u7d61\u4e2d\u5c08\u5bb6\u6df7\u5408\u300d(MoICE) \u7684\u65b0\u65b9\u6cd5\u4f86\u61c9\u5c0d\u6b64\u6311\u6230\u3002MoICE \u5305\u542b\u5169\u500b\u95dc\u9375\u7d44\u6210\u90e8\u5206\uff1a\u6574\u5408\u5230 LLM \u5167\u6bcf\u500b\u6ce8\u610f\u529b\u982d\u4e2d\u7684\u8def\u7531\u5668\u548c\u8f15\u91cf\u7d1a\u7684\u50c5\u8def\u7531\u5668\u8a13\u7df4\u6700\u4f73\u5316\u7b56\u7565\uff1a(1) MoICE \u5c07\u6bcf\u500b RoPE \u89d2\u5ea6\u8996\u70ba\u300c\u8108\u7d61\u4e2d\u300d\u7684\u5c08\u5bb6\uff0c\u8b49\u660e\u6709\u80fd\u529b\u5c07\u982d\u90e8\u7684\u6ce8\u610f\u529b\u5c0e\u5411\u7279\u5b9a\u7684\u8108\u7d61\u4f4d\u7f6e\u3002\u56e0\u6b64\uff0c\u6bcf\u500b\u6ce8\u610f\u529b\u982d\u4f7f\u7528\u8def\u7531\u5668\u52d5\u614b\u9078\u53d6\u7684\u591a\u500b RoPE \u89d2\u5ea6\u9748\u6d3b\u5730\u8655\u7406\u6b0a\u6756\uff0c\u4ee5\u95dc\u6ce8\u6240\u9700\u7684\u8077\u4f4d\u3002\u6b64\u65b9\u6cd5\u6e1b\u8f15\u4e86\u5ffd\u7565\u91cd\u8981\u8108\u7d61\u8cc7\u8a0a\u7684\u98a8\u96aa\u3002(2) \u50c5\u8def\u7531\u5668\u8a13\u7df4\u7b56\u7565\u9700\u8981\u51cd\u7d50 LLM \u53c3\u6578\uff0c\u4e26\u50c5\u91dd\u5c0d\u5e7e\u500b\u6b65\u9a5f\u7368\u5bb6\u66f4\u65b0\u8def\u7531\u5668\u3002\u7576\u61c9\u7528\u65bc\u5305\u62ec Llama \u548c Mistral \u5728\u5167\u7684\u958b\u6e90 LLM \u6642\uff0cMoICE \u5728\u9577\u8108\u7d61\u7406\u89e3\u548c\u751f\u6210\u7684\u591a\u9805\u4efb\u52d9\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u591a\u7a2e\u65b9\u6cd5\uff0c\u540c\u6642\u7dad\u6301\u61c9\u6709\u7684\u63a8\u8ad6\u6548\u7387\u3002", "author": "Hongzhan Lin et.al.", "authors": "Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, Rui Yan", "id": "2406.19598v1", "paper_url": "http://arxiv.org/abs/2406.19598v1", "repo": "null"}}