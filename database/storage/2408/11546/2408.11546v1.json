{"2408.11546": {"publish_time": "2024-08-21", "title": "Memorization In In-Context Learning", "paper_summary": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?", "paper_summary_zh": "\u60c5\u5883\u5b78\u7fd2 (ICL) \u5df2\u88ab\u8b49\u660e\u662f\u4e00\u7a2e\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4e0d\u9032\u884c\u984d\u5916\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6548\u80fd\u63d0\u5347\u80cc\u5f8c\u7684\u78ba\u5207\u6a5f\u5236\u4ecd\u4e0d\u660e\u78ba\u3002\u672c\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86 ICL \u5982\u4f55\u6d6e\u73fe\u8a18\u61b6\u5316\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u4e26\u63a2\u7d22\u6b64\u8a18\u61b6\u5316\u8207\u5404\u7a2e ICL \u5236\u5ea6\uff08\u96f6\u6b21\u5b78\u7fd2\u3001\u5c11\u6b21\u5b78\u7fd2\u548c\u591a\u6b21\u5b78\u7fd2\uff09\u4e4b\u9593\u7684\u95dc\u806f\u6027\u3002\u6211\u5011\u6700\u986f\u8457\u7684\u767c\u73fe\u5305\u62ec\uff1a(1) \u5728\u5927\u591a\u6578\u60c5\u6cc1\u4e0b\uff0c\u8207\u96f6\u6b21\u5b78\u7fd2\u76f8\u6bd4\uff0cICL \u6703\u986f\u8457\u6d6e\u73fe\u8a18\u61b6\u5316\uff1b(2) \u793a\u7bc4\uff08\u4e0d\u542b\u6a19\u7c64\uff09\u662f\u6d6e\u73fe\u8a18\u61b6\u5316\u6700\u6709\u6548\u7684\u5143\u7d20\uff1b(3) \u7576\u5c11\u6b21\u5b78\u7fd2\u5236\u5ea6\u4e2d\u7684\u6d6e\u73fe\u8a18\u61b6\u5316\u9054\u5230\u9ad8\u6c34\u6e96\uff08\u7d04 40%\uff09\u6642\uff0cICL \u6703\u63d0\u5347\u6548\u80fd\uff1b(4) \u7576 ICL \u52dd\u904e\u96f6\u6b21\u5b78\u7fd2\u6642\uff0c\u6548\u80fd\u8207\u8a18\u61b6\u5316\u4e4b\u9593\u5b58\u5728\u975e\u5e38\u5f37\u70c8\u7684\u95dc\u806f\u6027\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u63ed\u9732\u4e86\u4e00\u500b\u96b1\u85cf\u7684\u73fe\u8c61\u2014\u2014\u8a18\u61b6\u5316\u2014\u2014\u9019\u662f ICL \u7684\u6838\u5fc3\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u500b\u91cd\u8981\u554f\u984c\uff1aLLM \u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u771f\u6b63\u5f9e ICL \u4e2d\u7684\u793a\u7bc4\u4e2d\u6982\u62ec\uff0c\u800c\u5b83\u5011\u7684\u6210\u529f\u6709\u591a\u5c11\u6b78\u529f\u65bc\u8a18\u61b6\u5316\uff1f", "author": "Shahriar Golchin et.al.", "authors": "Shahriar Golchin, Mihai Surdeanu, Steven Bethard, Eduardo Blanco, Ellen Riloff", "id": "2408.11546v1", "paper_url": "http://arxiv.org/abs/2408.11546v1", "repo": "null"}}