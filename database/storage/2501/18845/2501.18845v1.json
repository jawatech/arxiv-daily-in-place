{"2501.18845": {"publish_time": "2025-01-31", "title": "Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities", "paper_summary": "The increasing size and complexity of pre-trained language models have\ndemonstrated superior performance in many applications, but they usually\nrequire large training datasets to be adequately trained. Insufficient training\nsets could unexpectedly make the model overfit and fail to cope with complex\ntasks. Large language models (LLMs) trained on extensive corpora have prominent\ntext generation capabilities, which improve the quality and quantity of data\nand play a crucial role in data augmentation. Specifically, distinctive prompt\ntemplates are given in personalised tasks to guide LLMs in generating the\nrequired content. Recent promising retrieval-based techniques further improve\nthe expressive performance of LLMs in data augmentation by introducing external\nknowledge to enable them to produce more grounded-truth data. This survey\nprovides an in-depth analysis of data augmentation in LLMs, classifying the\ntechniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based\nAugmentation and Hybrid Augmentation. We summarise the post-processing\napproaches in data augmentation, which contributes significantly to refining\nthe augmented data and enabling the model to filter out unfaithful content.\nThen, we provide the common tasks and evaluation metrics. Finally, we introduce\nexisting challenges and future opportunities that could bring further\nimprovement to data augmentation.", "paper_summary_zh": "\u96a8\u8457\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u898f\u6a21\u548c\u8907\u96dc\u6027\u7684\u63d0\u5347\uff0c\u5df2\u5728\u8a31\u591a\u61c9\u7528\u4e2d\u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u9700\u8981\u9f90\u5927\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u624d\u80fd\u7372\u5f97\u5145\u5206\u7684\u8a13\u7df4\u3002\u4e0d\u8db3\u7684\u8a13\u7df4\u96c6\u53ef\u80fd\u610f\u5916\u5730\u4f7f\u6a21\u578b\u904e\u5ea6\u64ec\u5408\uff0c\u800c\u7121\u6cd5\u61c9\u4ed8\u8907\u96dc\u7684\u4efb\u52d9\u3002\u5728\u5ee3\u6cdb\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u7a81\u51fa\u7684\u6587\u5b57\u751f\u6210\u80fd\u529b\uff0c\u9019\u80fd\u63d0\u5347\u8cc7\u6599\u7684\u54c1\u8cea\u548c\u6578\u91cf\uff0c\u4e26\u5728\u8cc7\u6599\u64f4\u5145\u4e2d\u626e\u6f14\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5728\u500b\u4eba\u5316\u4efb\u52d9\u4e2d\u7d66\u4e88\u7368\u7279\u7684\u63d0\u793a\u7bc4\u672c\uff0c\u4ee5\u5f15\u5c0e LLM \u7522\u751f\u6240\u9700\u7684\u5167\u5bb9\u3002\u6700\u8fd1\u6709\u524d\u9014\u7684\u57fa\u65bc\u6aa2\u7d22\u7684\u6280\u8853\u9032\u4e00\u6b65\u63d0\u5347\u4e86 LLM \u5728\u8cc7\u6599\u64f4\u5145\u4e2d\u7684\u8868\u9054\u6548\u80fd\uff0c\u900f\u904e\u5f15\u5165\u5916\u90e8\u77e5\u8b58\uff0c\u8b93\u5b83\u5011\u80fd\u7522\u751f\u66f4\u591a\u771f\u5be6\u7684\u8cc7\u6599\u3002\u9019\u4efd\u8abf\u67e5\u63d0\u4f9b\u4e86 LLM \u4e2d\u8cc7\u6599\u64f4\u5145\u7684\u6df1\u5165\u5206\u6790\uff0c\u5c07\u6280\u8853\u5206\u985e\u70ba\uff1a\u7c21\u55ae\u64f4\u5145\u3001\u57fa\u65bc\u63d0\u793a\u7684\u64f4\u5145\u3001\u57fa\u65bc\u6aa2\u7d22\u7684\u64f4\u5145\u548c\u6df7\u5408\u64f4\u5145\u3002\u6211\u5011\u7e3d\u7d50\u4e86\u8cc7\u6599\u64f4\u5145\u4e2d\u7684\u5f8c\u8655\u7406\u65b9\u6cd5\uff0c\u9019\u5c0d\u7cbe\u7149\u64f4\u5145\u8cc7\u6599\u548c\u8b93\u6a21\u578b\u904e\u6ffe\u6389\u4e0d\u5fe0\u5be6\u7684\u5167\u5bb9\u6709\u986f\u8457\u7684\u8ca2\u737b\u3002\u63a5\u8457\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5e38\u898b\u7684\u4efb\u52d9\u548c\u8a55\u4f30\u6307\u6a19\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u73fe\u6709\u7684\u6311\u6230\u548c\u672a\u4f86\u7684\u6a5f\u6703\uff0c\u9019\u4e9b\u90fd\u6709\u53ef\u80fd\u9032\u4e00\u6b65\u6539\u5584\u8cc7\u6599\u64f4\u5145\u3002", "author": "Yaping Chai et.al.", "authors": "Yaping Chai, Haoran Xie, Joe S. Qin", "id": "2501.18845v1", "paper_url": "http://arxiv.org/abs/2501.18845v1", "repo": "null"}}