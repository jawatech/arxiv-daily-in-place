{"2406.08903": {"publish_time": "2024-06-13", "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models", "paper_summary": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.", "paper_summary_zh": "\u5fae\u8abf\u662f\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u7684\u91cd\u8981\u904e\u7a0b\u3002\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u4f8b\u5982\u591a\u79df\u6236\u670d\u52d9\uff0c\u90e8\u7f72\u591a\u500b LLM \u4ee5\u6eff\u8db3\u8907\u96dc\u7684\u9700\u6c42\u8b8a\u5f97\u6709\u5fc5\u8981\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\u5c07\u5fae\u8abf\u7684 LLM \u5206\u89e3\u6210\u57fa\u790e\u6a21\u578b\u548c\u5c0d\u61c9\u7684 delta \u6b0a\u91cd\uff0c\u7136\u5f8c\u4f7f\u7528\u4f4e\u79e9\u6216\u4f4e\u4f4d\u5143\u65b9\u6cd5\u58d3\u7e2e\u5b83\u5011\u4ee5\u964d\u4f4e\u6210\u672c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u73fe\u6709\u7684\u4f4e\u79e9\u548c\u4f4e\u4f4d\u5143\u58d3\u7e2e\u65b9\u6cd5\u53ef\u80fd\u6703\u56b4\u91cd\u640d\u5bb3\u4efb\u52d9\u7279\u5b9a\u7684\u5fae\u8abf LLM\uff08\u4f8b\u5982\uff0c\u7528\u65bc\u6578\u5b78\u554f\u984c\u7684 WizardMath\uff09\u7684\u6a21\u578b\u6548\u80fd\u3002\u53d7\u5230 delta \u6b0a\u91cd\u4e2d\u5947\u7570\u503c\u9577\u5c3e\u5206\u4f48\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u7684 delta \u91cf\u5316\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u63a1\u7528\u5c0d\u61c9\u65bc\u8f03\u5927\u5947\u7570\u503c\u7684\u5947\u7570\u5411\u91cf\u7684\u8f03\u9ad8\u4f4d\u5143\u8868\u793a\u3002\u6211\u5011\u5728\u5404\u7a2e\u5fae\u8abf\u7684 LLM \u4e0a\u8a55\u4f30\u6211\u5011\u7684\u4f5c\u6cd5\uff0c\u5305\u62ec\u6578\u5b78 LLM\u3001\u7a0b\u5f0f\u78bc LLM\u3001\u804a\u5929 LLM\uff0c\u751a\u81f3\u662f VLM\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u4f5c\u6cd5\u57f7\u884c\u8d77\u4f86\u53ef\u8207\u5b8c\u6574\u7684\u5fae\u8abf LLM \u76f8\u5ab2\u7f8e\uff0c\u5728\u76f8\u7576\u5927\u7684\u5e45\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u4f4e\u79e9\u548c\u4f4e\u4f4d\u5143\u7684\u57fa\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684\u4f5c\u6cd5\u8207\u5404\u7a2e\u4e3b\u5e79 LLM \u76f8\u5bb9\uff0c\u4f8b\u5982 Llama-2\u3001Llama-3 \u548c Mistral\uff0c\u7a81\u986f\u4e86\u5b83\u7684\u901a\u7528\u6027\u3002", "author": "Bowen Ping et.al.", "authors": "Bowen Ping, Shuo Wang, Hanqing Wang, Xu Han, Yuzhuang Xu, Yukun Yan, Yun Chen, Baobao Chang, Zhiyuan Liu, Maosong Sun", "id": "2406.08903v1", "paper_url": "http://arxiv.org/abs/2406.08903v1", "repo": "null"}}