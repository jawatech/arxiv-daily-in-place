{"2406.12036": {"publish_time": "2024-06-17", "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations", "paper_summary": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.", "paper_summary_zh": "\u8207\u8a55\u4f30\u8a08\u7b97\u548c\u57fa\u65bc\u908f\u8f2f\u7684\u63a8\u7406\u4e0d\u540c\uff0c\u76ee\u524d\u7528\u65bc\u8a55\u4f30\u91ab\u5b78\u4e2d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u6d89\u53ca\u9818\u57df\u77e5\u8b58\u548c\u63cf\u8ff0\u6027\u63a8\u7406\u7684\u554f\u7b54\u4e0a\u3002\u96d6\u7136\u6b64\u985e\u5b9a\u6027\u80fd\u529b\u5c0d\u65bc\u91ab\u7642\u8a3a\u65b7\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u5728\u5be6\u969b\u60c5\u6cc1\u4e2d\uff0c\u91ab\u751f\u7d93\u5e38\u4f7f\u7528\u9075\u5faa\u5b9a\u91cf\u65b9\u7a0b\u5f0f\u548c\u57fa\u65bc\u898f\u5247\u7684\u63a8\u7406\u7bc4\u4f8b\u7684\u81e8\u5e8a\u8a08\u7b97\u5668\u4f86\u9032\u884c\u57fa\u65bc\u8b49\u64da\u7684\u6c7a\u7b56\u652f\u6301\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MedCalc-Bench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5c08\u6ce8\u65bc\u8a55\u4f30 LLM \u91ab\u5b78\u8a08\u7b97\u80fd\u529b\u7684\u540c\u985e\u6578\u64da\u96c6\u3002MedCalc-Bench \u5305\u542b\u4e00\u500b\u8a55\u4f30\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 55 \u500b\u4e0d\u540c\u91ab\u5b78\u8a08\u7b97\u4efb\u52d9\u7684 1000 \u591a\u500b\u624b\u52d5\u5be9\u67e5\u7684\u5be6\u4f8b\u3002MedCalc-Bench \u4e2d\u7684\u6bcf\u500b\u5be6\u4f8b\u90fd\u5305\u542b\u4e00\u500b\u60a3\u8005\u5099\u8a3b\u3001\u4e00\u500b\u8acb\u6c42\u8a08\u7b97\u7279\u5b9a\u91ab\u5b78\u503c\u7684\u63d0\u554f\u3001\u4e00\u500b\u57fa\u672c\u4e8b\u5be6\u7b54\u6848\u4ee5\u53ca\u4e00\u500b\u9010\u6b65\u8aaa\u660e\u5982\u4f55\u7372\u5f97\u7b54\u6848\u7684\u8aaa\u660e\u3002\u96d6\u7136\u6211\u5011\u7684\u8a55\u4f30\u7d50\u679c\u986f\u793a\u4e86 LLM \u5728\u6b64\u9818\u57df\u7684\u6f5b\u529b\uff0c\u4f46\u6c92\u6709\u4efb\u4f55\u4e00\u500b LLM \u8db3\u5920\u6709\u6548\u5730\u7528\u65bc\u81e8\u5e8a\u74b0\u5883\u3002\u5e38\u898b\u554f\u984c\u5305\u62ec\u63d0\u53d6\u4e0d\u6b63\u78ba\u7684\u5be6\u9ad4\u3001\u672a\u91dd\u5c0d\u8a08\u7b97\u4efb\u52d9\u4f7f\u7528\u6b63\u78ba\u7684\u65b9\u7a0b\u5f0f\u6216\u898f\u5247\uff0c\u6216\u932f\u8aa4\u5730\u57f7\u884c\u8a08\u7b97\u7684\u7b97\u8853\u904b\u7b97\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u7814\u7a76\u80fd\u7a81\u986f LLM \u5728\u91ab\u7642\u74b0\u5883\u4e2d\u7684\u5b9a\u91cf\u77e5\u8b58\u548c\u63a8\u7406\u5dee\u8ddd\uff0c\u4e26\u9f13\u52f5\u672a\u4f86\u6539\u9032 LLM \u4ee5\u61c9\u4ed8\u5404\u7a2e\u81e8\u5e8a\u8a08\u7b97\u4efb\u52d9\u3002", "author": "Nikhil Khandekar et.al.", "authors": "Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu", "id": "2406.12036v3", "paper_url": "http://arxiv.org/abs/2406.12036v3", "repo": "https://github.com/ncbi-nlp/medcalc-bench"}}