{"2411.11235": {"publish_time": "2024-11-18", "title": "MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis", "paper_summary": "Artificial Intelligence (AI) has demonstrated significant capabilities in\nvarious fields, and in areas such as human-computer interaction (HCI), embodied\nintelligence, and the design and animation of virtual digital humans, both\npractitioners and users are increasingly concerned with AI's ability to\nunderstand and express emotion. Consequently, the question of whether AI can\naccurately interpret human emotions remains a critical challenge. To date, two\nprimary classes of AI models have been involved in human emotion analysis:\ngenerative models and Multimodal Large Language Models (MLLMs). To assess the\nemotional capabilities of these two classes of models, this study introduces\nMEMO-Bench, a comprehensive benchmark consisting of 7,145 portraits, each\ndepicting one of six different emotions, generated by 12 Text-to-Image (T2I)\nmodels. Unlike previous works, MEMO-Bench provides a framework for evaluating\nboth T2I models and MLLMs in the context of sentiment analysis. Additionally, a\nprogressive evaluation approach is employed, moving from coarse-grained to\nfine-grained metrics, to offer a more detailed and comprehensive assessment of\nthe sentiment analysis capabilities of MLLMs. The experimental results\ndemonstrate that existing T2I models are more effective at generating positive\nemotions than negative ones. Meanwhile, although MLLMs show a certain degree of\neffectiveness in distinguishing and recognizing human emotions, they fall short\nof human-level accuracy, particularly in fine-grained emotion analysis. The\nMEMO-Bench will be made publicly available to support further research in this\narea.", "paper_summary_zh": "\u4eba\u5de5\u667a\u6167 (AI) \u5df2\u5728\u5404\u9818\u57df\u5c55\u73fe\u986f\u8457\u80fd\u529b\uff0c\u800c\u5728\u4eba\u6a5f\u4e92\u52d5 (HCI)\u3001\u5177\u8eab\u667a\u6167\uff0c\u4ee5\u53ca\u865b\u64ec\u6578\u4f4d\u4eba\u985e\u7684\u8a2d\u8a08\u8207\u52d5\u756b\u7b49\u9818\u57df\uff0c\u5be6\u52d9\u5de5\u4f5c\u8005\u548c\u4f7f\u7528\u8005\u6108\u4f86\u6108\u95dc\u6ce8 AI \u7406\u89e3\u548c\u8868\u9054\u60c5\u7dd2\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0cAI \u662f\u5426\u80fd\u6e96\u78ba\u8a6e\u91cb\u4eba\u985e\u60c5\u7dd2\u7684\u554f\u984c\uff0c\u4ecd\u662f\u95dc\u9375\u6311\u6230\u3002\u8fc4\u4eca\u70ba\u6b62\uff0c\u5169\u5927\u985e\u578b\u7684 AI \u6a21\u578b\u5df2\u53c3\u8207\u4eba\u985e\u60c5\u7dd2\u5206\u6790\uff1a\u751f\u6210\u6a21\u578b\u548c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MMLM)\u3002\u70ba\u4e86\u8a55\u4f30\u9019\u5169\u985e\u6a21\u578b\u7684\u60c5\u7dd2\u80fd\u529b\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86 MEMO-Bench\uff0c\u9019\u662f\u4e00\u500b\u7531 7,145 \u5f35\u8096\u50cf\u7d44\u6210\u7684\u7d9c\u5408\u57fa\u6e96\u6e2c\u8a66\uff0c\u6bcf\u4e00\u5f35\u90fd\u63cf\u7e6a\u4e86\u516d\u7a2e\u4e0d\u540c\u60c5\u7dd2\u4e4b\u4e00\uff0c\u7531 12 \u500b\u6587\u5b57\u8f49\u5716\u50cf (T2I) \u6a21\u578b\u7522\u751f\u3002\u8207\u5148\u524d\u7684\u7814\u7a76\u4e0d\u540c\uff0cMEMO-Bench \u63d0\u4f9b\u4e86\u4e00\u500b\u8a55\u4f30 T2I \u6a21\u578b\u548c MMLM \u5728\u60c5\u7dd2\u5206\u6790\u80cc\u666f\u4e0b\u7684\u67b6\u69cb\u3002\u6b64\u5916\uff0c\u63a1\u7528\u6f38\u9032\u5f0f\u8a55\u4f30\u65b9\u6cd5\uff0c\u5f9e\u7c97\u7565\u6307\u6a19\u8f49\u5411\u7d30\u7dfb\u6307\u6a19\uff0c\u4ee5\u63d0\u4f9b\u66f4\u8a73\u7d30\u4e14\u5168\u9762\u7684 MMLM \u60c5\u7dd2\u5206\u6790\u80fd\u529b\u8a55\u4f30\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u73fe\u6709\u7684 T2I \u6a21\u578b\u5728\u7522\u751f\u6b63\u9762\u60c5\u7dd2\u65b9\u9762\u6bd4\u8ca0\u9762\u60c5\u7dd2\u66f4\u6709\u6548\u3002\u540c\u6642\uff0c\u5118\u7ba1 MMLM \u5728\u5340\u5206\u548c\u8fa8\u8b58\u4eba\u985e\u60c5\u7dd2\u65b9\u9762\u5c55\u73fe\u4e00\u5b9a\u7a0b\u5ea6\u7684\u6709\u6548\u6027\uff0c\u4f46\u4ecd\u672a\u9054\u5230\u4eba\u985e\u7b49\u7d1a\u7684\u6e96\u78ba\u5ea6\uff0c\u7279\u5225\u662f\u5728\u7d30\u7dfb\u7684\u60c5\u7dd2\u5206\u6790\u65b9\u9762\u3002MEMO-Bench \u5c07\u516c\u958b\u63d0\u4f9b\uff0c\u4ee5\u652f\u6301\u6b64\u9818\u57df\u7684\u9032\u4e00\u6b65\u7814\u7a76\u3002", "author": "Yingjie Zhou et.al.", "authors": "Yingjie Zhou, Zicheng Zhang, Jiezhang Cao, Jun Jia, Yanwei Jiang, Farong Wen, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai", "id": "2411.11235v1", "paper_url": "http://arxiv.org/abs/2411.11235v1", "repo": "null"}}