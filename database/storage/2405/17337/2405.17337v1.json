{"2405.17337": {"publish_time": "2024-05-27", "title": "Cost-efficient Knowledge-based Question Answering with Large Language Models", "paper_summary": "Knowledge-based question answering (KBQA) is widely used in many scenarios\nthat necessitate domain knowledge. Large language models (LLMs) bring\nopportunities to KBQA, while their costs are significantly higher and absence\nof domain-specific knowledge during pre-training. We are motivated to combine\nLLMs and prior small models on knowledge graphs (KGMs) for both inferential\naccuracy and cost saving. However, it remains challenging since accuracy and\ncost are not readily combined in the optimization as two distinct metrics. It\nis also laborious for model selection since different models excel in diverse\nknowledge. To this end, we propose Coke, a novel cost-efficient strategy for\nKBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize\ncalls to LLMs within limited budgets. We first formulate the accuracy\nexpectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A\ncontext-aware policy is optimized to further distinguish the expert model\nsubject to the question semantics. The overall decision is bounded by the cost\nregret according to historical expenditure on failures. Extensive experiments\nshowcase the superior performance of Coke, which moves the Pareto frontier with\nup to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on\nthe benchmark datasets.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u77e5\u8b58\u7684\u554f\u7b54 (KBQA) \u5ee3\u6cdb\u61c9\u7528\u65bc\u8a31\u591a\u9700\u8981\u9818\u57df\u77e5\u8b58\u7684\u5834\u666f\u4e2d\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba KBQA \u5e36\u4f86\u4e86\u6a5f\u6703\uff0c\u4f46\u5176\u6210\u672c\u986f\u8457\u63d0\u9ad8\uff0c\u4e14\u5728\u9810\u8a13\u7df4\u671f\u9593\u7f3a\u4e4f\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\u3002\u6211\u5011\u6709\u52d5\u529b\u5c07 LLM \u548c\u5148\u524d\u7684\u77e5\u8b58\u5716\u8b5c (KGM) \u4e0a\u7684\u5c0f\u6a21\u578b\u7d50\u5408\u8d77\u4f86\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u6e96\u78ba\u6027\u548c\u7bc0\u7701\u6210\u672c\u3002\u7136\u800c\uff0c\u7531\u65bc\u6e96\u78ba\u6027\u548c\u6210\u672c\u7121\u6cd5\u5728\u512a\u5316\u4e2d\u4f5c\u70ba\u5169\u500b\u4e0d\u540c\u7684\u6307\u6a19\u8f15\u6613\u5730\u7d50\u5408\u5728\u4e00\u8d77\uff0c\u56e0\u6b64\u9019\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u7531\u65bc\u4e0d\u540c\u7684\u6a21\u578b\u64c5\u9577\u65bc\u4e0d\u540c\u7684\u77e5\u8b58\uff0c\u56e0\u6b64\u6a21\u578b\u9078\u64c7\u4e5f\u5f88\u8cbb\u529b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Coke\uff0c\u4e00\u7a2e\u91dd\u5c0d LLM \u7684\u65b0\u7a4e\u4e14\u5177\u6709\u6210\u672c\u6548\u76ca\u7684 KBQA \u7b56\u7565\uff0c\u5b83\u88ab\u5efa\u6a21\u70ba\u4e00\u500b\u5b9a\u5236\u7684\u591a\u81c2\u8ced\u535a\u6a5f\u554f\u984c\uff0c\u4ee5\u5728\u6709\u9650\u7684\u9810\u7b97\u5167\u6700\u5927\u7a0b\u5ea6\u5730\u6e1b\u5c11\u5c0d LLM \u7684\u547c\u53eb\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528\u91dd\u5c0d KGM \u6216 LLM \u7684\u7fa4\u96c6\u7d1a Thompson \u63a1\u6a23\u4f86\u5236\u5b9a\u6e96\u78ba\u6027\u671f\u671b\u3002\u512a\u5316\u4e86\u4e00\u500b\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u5340\u5206\u554f\u984c\u8a9e\u7fa9\u7684\u4e3b\u984c\u6a21\u578b\u3002\u6839\u64da\u5931\u6557\u7684\u6b77\u53f2\u652f\u51fa\uff0c\u7e3d\u9ad4\u6c7a\u7b56\u53d7\u5230\u6210\u672c\u907a\u61be\u7684\u7d04\u675f\u3002\u5927\u91cf\u7684\u5be6\u9a57\u5c55\u793a\u4e86 Coke \u7684\u512a\u8d8a\u6027\u80fd\uff0c\u5b83\u5c07\u5e15\u7d2f\u6258\u524d\u6cbf\u79fb\u52d5\u4e86\u591a\u9054 20.89%\uff0c\u540c\u6642\u5728\u57fa\u6e96\u6578\u64da\u96c6\u4e0a\u5be6\u73fe\u4e86 2.74% \u7684\u66f4\u9ad8\u6e96\u78ba\u6027\u3002</paragraph>", "author": "Junnan Dong et.al.", "authors": "Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, Xiao Huang", "id": "2405.17337v1", "paper_url": "http://arxiv.org/abs/2405.17337v1", "repo": "null"}}