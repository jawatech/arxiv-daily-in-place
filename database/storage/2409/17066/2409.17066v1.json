{"2409.17066": {"publish_time": "2024-09-25", "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "paper_summary": "Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA.", "paper_summary_zh": "<paragraph>\u5927\u5e45\u7e2e\u653e\u6a21\u578b\u5927\u5c0f\u6703\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u90e8\u7f72\u548c\u63a8\u8ad6\u5e36\u4f86\u56b4\u5cfb\u6311\u6230\u3002\u7531\u65bc LLM \u6b0a\u91cd\u4e2d\u7684\u5197\u9918\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u5c08\u6ce8\u65bc\u5c07\u50c5\u6b0a\u91cd\u91cf\u5316\u63a8\u81f3\u6975\u4f4e\u4f4d\u5143\uff08\u751a\u81f3\u4f4e\u81f3 2 \u4f4d\u5143\uff09\u3002\u9019\u6e1b\u5c11\u4e86\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u6700\u4f73\u5316\u5132\u5b58\u6210\u672c\uff0c\u4e26\u5728\u63a8\u8ad6\u671f\u9593\u964d\u4f4e\u8a18\u61b6\u9ad4\u983b\u5bec\u9700\u6c42\u3002\u7136\u800c\uff0c\u7531\u65bc\u6578\u503c\u8868\u793a\u9650\u5236\uff0c\u50b3\u7d71\u7684\u57fa\u65bc\u7d14\u91cf\u6b0a\u91cd\u91cf\u5316\u96e3\u4ee5\u9054\u5230\u5982\u6b64\u6975\u4f4e\u7684\u4f4d\u5143\u3002\u6700\u8fd1\u91dd\u5c0d LLM \u7684\u5411\u91cf\u91cf\u5316 (VQ) \u7814\u7a76\u5df2\u8b49\u660e\u4e86\u6975\u4f4e\u4f4d\u5143\u6a21\u578b\u91cf\u5316\u7684\u6f5b\u529b\uff0c\u65b9\u6cd5\u662f\u4f7f\u7528\u67e5\u627e\u8868\u5c07\u5411\u91cf\u58d3\u7e2e\u6210\u7d22\u5f15\u3002\n\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u6975\u4f4e\u4f4d\u5143 LLM \u91cf\u5316\u7684\u5411\u91cf\u5f8c\u8a13\u7df4\u91cf\u5316 (VPTQ)\u3002\u6211\u5011\u4f7f\u7528\u4e8c\u968e\u6700\u4f73\u5316\u4f86\u5236\u5b9a LLM VQ \u554f\u984c\uff0c\u4e26\u900f\u904e\u6c42\u89e3\u6700\u4f73\u5316\u4f86\u5f15\u5c0e\u6211\u5011\u7684\u91cf\u5316\u6f14\u7b97\u6cd5\u8a2d\u8a08\u3002\u6211\u5011\u9032\u4e00\u6b65\u4f7f\u7528\u901a\u9053\u7121\u95dc\u4e8c\u968e\u6700\u4f73\u5316\u4f86\u6539\u5584\u6b0a\u91cd\uff0c\u4ee5\u9032\u884c\u7d30\u7dfb\u7684 VQ\u3002\u6b64\u5916\uff0c\u900f\u904e\u5206\u89e3\u6700\u4f73\u5316\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u77ed\u4e14\u6709\u6548\u7684\u78bc\u672c\u521d\u59cb\u5316\u6f14\u7b97\u6cd5\u3002\u6211\u5011\u4e5f\u64f4\u5145 VPTQ \u4ee5\u652f\u63f4\u6b98\u5dee\u548c\u96e2\u7fa4\u503c\u91cf\u5316\uff0c\u9019\u80fd\u63d0\u5347\u6a21\u578b\u6e96\u78ba\u5ea6\u4e26\u9032\u4e00\u6b65\u58d3\u7e2e\u6a21\u578b\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cVPTQ \u5728 2 \u4f4d\u5143\u6642\uff0c\u5c07 LLaMA-2 \u7684\u6a21\u578b\u91cf\u5316\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e86 $0.01$-$0.34$\uff0cMistral-7B \u964d\u4f4e\u4e86 $0.38$-$0.68$\uff0cLLaMA-3 \u964d\u4f4e\u4e86 $4.41$-$7.34$\uff0c\u512a\u65bc SOTA\uff0c\u5e73\u5747\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86 LLaMA-2 \u7684 $0.79$-$1.5\\%$\uff0cMistral-7B \u7684 $1\\%$\uff0cLLaMA-3 \u7684 $11$-$22\\%$\uff08\u5e73\u5747\u800c\u8a00\uff09\u3002\u6211\u5011\u50c5\u5229\u7528\u4e86\u91cf\u5316\u6f14\u7b97\u6cd5\u57f7\u884c\u6642\u9593\u7684 $10.4$-$18.6\\%$\uff0c\u8207 SOTA \u76f8\u6bd4\uff0c\u63a8\u8ad6\u541e\u5410\u91cf\u589e\u52a0\u4e86 $1.6$-$1.8\\times$\u3002</paragraph>", "author": "Yifei Liu et.al.", "authors": "Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang", "id": "2409.17066v1", "paper_url": "http://arxiv.org/abs/2409.17066v1", "repo": "https://github.com/microsoft/vptq"}}