{"2408.10159": {"publish_time": "2024-08-19", "title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation", "paper_summary": "Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy.", "paper_summary_zh": "\u5e8f\u5217\u63a8\u85a6\u7cfb\u7d71\u900f\u904e\u5206\u6790\u904e\u5f80\u4e92\u52d5\uff0c\u6bd4\u5c0d\u63a8\u85a6\u8207\u500b\u4eba\u504f\u597d\u4f86\u9810\u6e2c\u4f7f\u7528\u8005\u4e0b\u4e00\u500b\u611f\u8208\u8da3\u7684\u9805\u76ee\u3002\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u77e5\u8b58\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u512a\u52e2\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5df2\u900f\u904e\u8a9e\u8a00\u751f\u6210\u7bc4\u4f8b\u5c07 LLM \u61c9\u7528\u65bc\u5e8f\u5217\u63a8\u85a6\u3002\u9019\u4e9b\u65b9\u6cd5\u5c07\u4f7f\u7528\u8005\u884c\u70ba\u5e8f\u5217\u8f49\u63db\u70ba LLM \u5fae\u8abf\u63d0\u793a\uff0c\u4e26\u5229\u7528\u4f4e\u79e9\u9069\u61c9 (LoRA) \u6a21\u7d44\u4f86\u6539\u5584\u63a8\u85a6\u3002\u7136\u800c\uff0c\u5728\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u884c\u70ba\u4e2d\u7d71\u4e00\u61c9\u7528 LoRA \u6709\u6642\u7121\u6cd5\u6355\u6349\u500b\u5225\u8b8a\u7570\u6027\uff0c\u5c0e\u81f4\u6b21\u4f73\u6548\u80fd\u548c\u4e0d\u540c\u5e8f\u5217\u4e4b\u9593\u7684\u8ca0\u9762\u8f49\u79fb\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u5be6\u4f8b LoRA (iLoRA)\uff0c\u5c07 LoRA \u8207\u5c08\u5bb6\u6df7\u5408 (MoE) \u67b6\u69cb\u6574\u5408\u3002iLoRA \u5efa\u7acb\u4e86\u4e00\u7cfb\u5217\u4e0d\u540c\u7684\u5c08\u5bb6\uff0c\u6bcf\u500b\u5c08\u5bb6\u90fd\u80fd\u6355\u6349\u4f7f\u7528\u8005\u504f\u597d\u7684\u7279\u5b9a\u9762\u5411\uff0c\u4e26\u5f15\u5165\u7531\u5e8f\u5217\u8868\u793a\u5f15\u5c0e\u7684\u9598\u9580\u51fd\u6578\u3002\u6b64\u9598\u9580\u51fd\u6578\u8655\u7406\u6b77\u53f2\u4e92\u52d5\u5e8f\u5217\u4ee5\u7522\u751f\u8c50\u5bcc\u7684\u8868\u793a\uff0c\u5f15\u5c0e\u9598\u63a7\u7db2\u8def\u8f38\u51fa\u5ba2\u88fd\u5316\u7684\u5c08\u5bb6\u53c3\u8207\u6b0a\u91cd\u3002\u6b64\u91cf\u8eab\u6253\u9020\u7684\u65b9\u6cd5\u53ef\u6e1b\u8f15\u8ca0\u9762\u8f49\u79fb\uff0c\u4e26\u52d5\u614b\u8abf\u6574\u81f3\u4e0d\u540c\u7684\u884c\u70ba\u6a21\u5f0f\u3002\u5728\u4e09\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 iLoRA \u7684\u6709\u6548\u6027\uff0c\u7a81\u986f\u51fa\u5176\u5728\u6355\u6349\u4f7f\u7528\u8005\u7279\u5b9a\u504f\u597d\u548c\u6539\u5584\u63a8\u85a6\u6e96\u78ba\u5ea6\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u7684\u5353\u8d8a\u6548\u80fd\u3002", "author": "Xiaoyu Kong et.al.", "authors": "Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He", "id": "2408.10159v1", "paper_url": "http://arxiv.org/abs/2408.10159v1", "repo": "null"}}