{"2406.11823": {"publish_time": "2024-06-17", "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "paper_summary": "Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva .", "paper_summary_zh": "\u8a9e\u8a00\u548c\u8996\u89ba\u52a9\u7406\u7684\u6700\u65b0\u9032\u5c55\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u66f4\u5ee3\u6cdb\u7684\u7814\u7a76\u548c\u53ef\u8907\u88fd\u6027\u3002\u96d6\u7136\u958b\u6e90\u6a21\u578b\u6709\u6548\u5730\u8655\u7406\u4e00\u822c\u5f71\u50cf\u4efb\u52d9\uff0c\u4f46\u5b83\u5011\u5728\u8907\u96dc\u7684\u8996\u89ba\u60c5\u5883\u6587\u672c\u7406\u89e3\u7684\u9ad8\u8a08\u7b97\u9700\u6c42\u65b9\u9762\u9762\u81e8\u6311\u6230\u3002\u6b64\u985e\u4efb\u52d9\u901a\u5e38\u9700\u8981\u589e\u52a0\u7684\u6a19\u8a18\u8f38\u5165\u548c\u5927\u578b\u8996\u89ba\u6a21\u7d44\uff0c\u4ee5\u5229\u7528\u9ad8\u89e3\u6790\u5ea6\u8cc7\u8a0a\u3002\u5728\u6a21\u578b\u5927\u5c0f\u548c\u8cc7\u6599\u91cd\u8981\u6027\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u6c7a\u7684\u554f\u984c\u3002\u672c\u7814\u7a76\u65e8\u5728\u900f\u904e\u8b58\u5225\u95dc\u9375\u7d44\u6210\u90e8\u5206\u548c\u5efa\u7acb\u5177\u6709\u53d7\u9650\u63a8\u8ad6\u6210\u672c\u7684\u6709\u6548\u7387\u6a21\u578b\uff0c\u91cd\u65b0\u5b9a\u7fa9\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u8a2d\u8a08\u3002\u900f\u904e\u7b56\u7565\u6027\u5730\u5236\u5b9a\u8cc7\u6599\u96c6\u3001\u6700\u4f73\u5316\u8996\u89ba\u6a21\u7d44\u548c\u589e\u5f37\u76e3\u7763\u6280\u8853\uff0c\u6211\u5011\u5728\u7dad\u6301\u9ad8\u6027\u80fd\u7684\u540c\u6642\uff0c\u986f\u8457\u6539\u5584\u4e86\u63a8\u8ad6\u8655\u7406\u91cf\u3002\u5f9e 160M \u5230 13B \u53c3\u6578\u7684\u6a21\u578b\u7684\u5ee3\u6cdb\u5be6\u9a57\u63d0\u4f9b\u4e86\u6a21\u578b\u6700\u4f73\u5316\u7684\u898b\u89e3\u3002\u6211\u5011\u5c07\u5728 https://github.com/naver-ai/elva \u5b8c\u5168\u958b\u653e\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5eab\u3001\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u3002", "author": "Geewook Kim et.al.", "authors": "Geewook Kim, Minjoon Seo", "id": "2406.11823v1", "paper_url": "http://arxiv.org/abs/2406.11823v1", "repo": "https://github.com/naver-ai/elva"}}