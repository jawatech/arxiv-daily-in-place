{"2411.19668": {"publish_time": "2024-11-29", "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information", "paper_summary": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0", "paper_summary_zh": "<paragraph>\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f00\u53d1\u4e2d\uff0c\u9884\u8bad\u7ec3\u6570\u636e\u5728\u5851\u9020 LLM \u80fd\u529b\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u8fd1\u5e74\u6765\uff0c\u5df2\u7ecf\u53d1\u5e03\u4e86\u51e0\u4e2a\u5927\u89c4\u6a21\u4e14\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u52a0\u901f LLM \u7684\u7814\u7a76\uff0c\u5305\u62ec ChineseWebText1.0\u3001C4\u3001Pile\u3001WanJuan\u3001MAPCC \u7b49\u3002\u7136\u800c\uff0c\u968f\u7740 LLM \u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u91cd\u70b9\u5df2\u9010\u6e10\u8f6c\u5411\u7279\u5b9a\u9886\u57df\u7684\u8bed\u8a00\u80fd\u529b\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u4ee5\u524d\u90a3\u4e9b\u7c92\u5ea6\u8f83\u7c97\u7684\u6587\u672c\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42\u3002\u6b64\u5916\uff0c\u7cbe\u7ec6\u7684\u4fe1\u606f\uff08\u5982\u8d28\u91cf\u3001\u9886\u57df\u548c\u6bd2\u6027\uff09\u5728\u4e3a\u5404\u79cd\u573a\u666f\u6784\u5efa\u5f3a\u5927\u4e14\u53ef\u9760\u7684 LLM \u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MDFG-tool \u7684\u65b0\u5de5\u5177\u94fe\uff0c\u7528\u4e8e\u6784\u5efa\u5177\u6709\u591a\u7ef4\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u5927\u89c4\u6a21\u548c\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u6570\u636e\u96c6\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528\u4eba\u5de5\u5236\u4f5c\u7684\u89c4\u5219\u4ece\u539f\u59cb\u5185\u5bb9\u4e2d\u4e22\u5f03\u660e\u663e\u7684\u566a\u58f0\u6587\u672c\u3002\u5176\u6b21\uff0c\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u3001\u9886\u57df\u5206\u7c7b\u5668\u548c\u6bd2\u6027\u8bc4\u4f30\u6a21\u578b\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u5206\u522b\u8bc4\u4f30\u5269\u4f59\u7684\u5df2\u6e05\u7406\u6570\u636e\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u96c6\u6210\u5230\u6bcf\u6bb5\u6587\u672c\u4e2d\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u6700\u5927\u3001\u9ad8\u8d28\u91cf\u548c\u7ec6\u7c92\u5ea6\u7684\u4e2d\u6587\u6587\u672c ChineseWebText2.0\uff0c\u5b83\u5305\u542b 3.8TB\uff0c\u5e76\u4e14\u6bcf\u6bb5\u6587\u672c\u90fd\u4e0e\u8d28\u91cf\u5206\u6570\u3001\u9886\u57df\u6807\u7b7e\u3001\u6bd2\u6027\u6807\u7b7e\u548c\u6bd2\u6027\u5206\u6570\u76f8\u5173\u8054\uff0c\u4ece\u800c\u65b9\u4fbf LLM \u7814\u7a76\u4eba\u5458\u6839\u636e\u5404\u79cd\u7c7b\u578b\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u9009\u62e9\u6570\u636e\u3002\u6570\u636e\u3001\u4ee3\u7801\u548c\u5de5\u5177\u94fe\u53ef\u5728\u4ee5\u4e0b\u7f51\u7ad9\u4e0a\u83b7\u5f97 https://github.com/CASIA-LM/ChineseWebText-2.0</paragraph>", "author": "Wanyue Zhang et.al.", "authors": "Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang", "id": "2411.19668v1", "paper_url": "http://arxiv.org/abs/2411.19668v1", "repo": "https://github.com/casia-lm/chinesewebtext-2.0"}}