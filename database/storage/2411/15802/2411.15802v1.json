{"2411.15802": {"publish_time": "2024-11-24", "title": "Medical Slice Transformer: Improved Diagnosis and Explainability on 3D Medical Images with DINOv2", "paper_summary": "MRI and CT are essential clinical cross-sectional imaging techniques for\ndiagnosing complex conditions. However, large 3D datasets with annotations for\ndeep learning are scarce. While methods like DINOv2 are encouraging for 2D\nimage analysis, these methods have not been applied to 3D medical images.\nFurthermore, deep learning models often lack explainability due to their\n\"black-box\" nature. This study aims to extend 2D self-supervised models,\nspecifically DINOv2, to 3D medical imaging while evaluating their potential for\nexplainable outcomes. We introduce the Medical Slice Transformer (MST)\nframework to adapt 2D self-supervised models for 3D medical image analysis. MST\ncombines a Transformer architecture with a 2D feature extractor, i.e., DINOv2.\nWe evaluate its diagnostic performance against a 3D convolutional neural\nnetwork (3D ResNet) across three clinical datasets: breast MRI (651 patients),\nchest CT (722 patients), and knee MRI (1199 patients). Both methods were tested\nfor diagnosing breast cancer, predicting lung nodule dignity, and detecting\nmeniscus tears. Diagnostic performance was assessed by calculating the Area\nUnder the Receiver Operating Characteristic Curve (AUC). Explainability was\nevaluated through a radiologist's qualitative comparison of saliency maps based\non slice and lesion correctness. P-values were calculated using Delong's test.\nMST achieved higher AUC values compared to ResNet across all three datasets:\nbreast (0.94$\\pm$0.01 vs. 0.91$\\pm$0.02, P=0.02), chest (0.95$\\pm$0.01 vs.\n0.92$\\pm$0.02, P=0.13), and knee (0.85$\\pm$0.04 vs. 0.69$\\pm$0.05, P=0.001).\nSaliency maps were consistently more precise and anatomically correct for MST\nthan for ResNet. Self-supervised 2D models like DINOv2 can be effectively\nadapted for 3D medical imaging using MST, offering enhanced diagnostic accuracy\nand explainability compared to convolutional neural networks.", "paper_summary_zh": "<paragraph>MRI \u548c CT \u662f\u8bca\u65ad\u590d\u6742\u75be\u75c5\u7684\u91cd\u8981\u4e34\u5e8a\u6a2a\u65ad\u9762\u6210\u50cf\u6280\u672f\u3002\u7136\u800c\uff0c\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5927\u578b 3D \u6570\u636e\u96c6\u548c\u6ce8\u91ca\u5374\u5f88\u7a00\u7f3a\u3002\u867d\u7136\u8bf8\u5982 DINOv2 \u4e4b\u7c7b\u7684\u65b9\u6cd5\u5bf9 2D \u56fe\u50cf\u5206\u6790\u5f88\u6709\u5e2e\u52a9\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u672a\u5e94\u7528\u4e8e 3D \u533b\u5b66\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u201c\u9ed1\u5323\u5b50\u201d\u7684\u6027\u8d28\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06 2D \u81ea\u76d1\u7763\u6a21\u578b\uff08\u7279\u522b\u662f DINOv2\uff09\u6269\u5c55\u5230 3D \u533b\u5b66\u6210\u50cf\uff0c\u540c\u65f6\u8bc4\u4f30\u5176\u5bf9\u53ef\u89e3\u91ca\u7ed3\u679c\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u533b\u5b66\u5207\u7247\u8f6c\u6362\u5668 (MST) \u6846\u67b6\uff0c\u4ee5\u5c06 2D \u81ea\u76d1\u7763\u6a21\u578b\u7528\u4e8e 3D \u533b\u5b66\u56fe\u50cf\u5206\u6790\u3002MST \u5c06 Transformer \u67b6\u6784\u4e0e 2D \u7279\u5f81\u63d0\u53d6\u5668\uff08\u5373 DINOv2\uff09\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5176\u9488\u5bf9\u4e09\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\uff08\u4e73\u817a MRI\uff08651 \u540d\u60a3\u8005\uff09\u3001\u80f8\u90e8 CT\uff08722 \u540d\u60a3\u8005\uff09\u548c\u819d\u90e8 MRI\uff081199 \u540d\u60a3\u8005\uff09\uff09\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u4e0e 3D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc (3D ResNet) \u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bca\u65ad\u4e73\u817a\u764c\u3001\u9884\u6d4b\u80ba\u7ed3\u8282\u6027\u8d28\u548c\u68c0\u6d4b\u534a\u6708\u677f\u6495\u88c2\u3002\u901a\u8fc7\u8ba1\u7b97\u53d7\u8bd5\u8005\u5de5\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef (AUC) \u6765\u8bc4\u4f30\u8bca\u65ad\u6027\u80fd\u3002\u53ef\u89e3\u91ca\u6027\u901a\u8fc7\u653e\u5c04\u79d1\u533b\u751f\u5bf9\u57fa\u4e8e\u5207\u7247\u548c\u75c5\u53d8\u6b63\u786e\u6027\u7684\u663e\u7740\u6027\u56fe\u7684\u5b9a\u6027\u6bd4\u8f83\u6765\u8bc4\u4f30\u3002P \u503c\u4f7f\u7528 Delong \u7684\u68c0\u9a8c\u8ba1\u7b97\u3002\u4e0e\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u7684 ResNet \u76f8\u6bd4\uff0cMST \u83b7\u5f97\u4e86\u66f4\u9ad8\u7684 AUC \u503c\uff1a\u4e73\u817a\uff080.94\u00b10.01 vs. 0.91\u00b10.02\uff0cP=0.02\uff09\u3001\u80f8\u90e8\uff080.95\u00b10.01 vs. 0.92\u00b10.02\uff0cP=0.13\uff09\u548c\u819d\u90e8\uff080.85\u00b10.04 vs. 0.69\u00b10.05\uff0cP=0.001\uff09\u3002\u4e0e ResNet \u76f8\u6bd4\uff0cMST \u7684\u663e\u7740\u6027\u56fe\u59cb\u7ec8\u66f4\u52a0\u7cbe\u786e\u4e14\u89e3\u5256\u5b66\u4e0a\u66f4\u6b63\u786e\u3002\u8bf8\u5982 DINOv2 \u4e4b\u7c7b\u7684\u81ea\u76d1\u7763 2D \u6a21\u578b\u53ef\u4ee5\u4f7f\u7528 MST \u6709\u6548\u5730\u9002\u5e94 3D \u533b\u5b66\u6210\u50cf\uff0c\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002</paragraph>", "author": "Gustav M\u00fcller-Franzes et.al.", "authors": "Gustav M\u00fcller-Franzes, Firas Khader, Robert Siepmann, Tianyu Han, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn", "id": "2411.15802v1", "paper_url": "http://arxiv.org/abs/2411.15802v1", "repo": "https://github.com/mueller-franzes/mst"}}