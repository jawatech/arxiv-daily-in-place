{"2406.03855": {"publish_time": "2024-06-06", "title": "Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As", "paper_summary": "Clinical problem-solving requires processing of semantic medical knowledge\nsuch as illness scripts and numerical medical knowledge of diagnostic tests for\nevidence-based decision-making. As large language models (LLMs) show promising\nresults in many aspects of language-based clinical practice, their ability to\ngenerate non-language evidence-based answers to clinical questions is\ninherently limited by tokenization. Therefore, we evaluated LLMs' performance\non two question types: numeric (correlating findings) and semantic\n(differentiating entities) while examining differences within and between LLMs\nin medical aspects and comparing their performance to humans. To generate\nstraightforward multi-choice questions and answers (QAs) based on\nevidence-based medicine (EBM), we used a comprehensive medical knowledge graph\n(encompassed data from more than 50,00 peer-reviewed articles) and created the\n\"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics\nand classified into numerical or semantic questions. We benchmarked this\ndataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and\nClaude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question\ntypes and according to sub-labeled topics. For validation, six medical experts\nwere tested on 100 numerical EBMQA questions. We found that both LLMs excelled\nmore in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical\nQAs. However, both LLMs showed inter and intra gaps in different medical\naspects and remained inferior to humans. Thus, their medical advice should be\naddressed carefully.", "paper_summary_zh": "<paragraph>\u81e8\u5e8a\u554f\u984c\u89e3\u6c7a\u9700\u8981\u8655\u7406\u8a9e\u7fa9\u91ab\u5b78\u77e5\u8b58\n\u4f8b\u5982\u75be\u75c5\u8173\u672c\u548c\u57fa\u65bc\u8b49\u64da\u7684\u6c7a\u7b56\u5236\u5b9a\u4e4b\u8a3a\u65b7\u6e2c\u8a66\u7684\u6578\u503c\u91ab\u5b78\u77e5\u8b58\u3002\u7531\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u57fa\u790e\u81e8\u5e8a\u5be6\u52d9\u7684\u8a31\u591a\u65b9\u9762\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6210\u679c\uff0c\u5b83\u5011\u7522\u751f\u975e\u8a9e\u8a00\u3001\u57fa\u65bc\u8b49\u64da\u7684\u81e8\u5e8a\u554f\u984c\u89e3\u7b54\u7684\u80fd\u529b\u672c\u8cea\u4e0a\u53d7\u5230\u6a19\u8a18\u5316\u7684\u9650\u5236\u3002\u56e0\u6b64\uff0c\u6211\u5011\u8a55\u4f30\u4e86 LLM \u5728\u5169\u7a2e\u554f\u984c\u985e\u578b\u4e0a\u7684\u8868\u73fe\uff1a\u6578\u503c\uff08\u76f8\u95dc\u767c\u73fe\uff09\u548c\u8a9e\u7fa9\uff08\u5340\u5206\u5be6\u9ad4\uff09\uff0c\u540c\u6642\u6aa2\u8996 LLM \u5728\u91ab\u5b78\u65b9\u9762\u7684\u5dee\u7570\uff0c\u4e26\u5c07\u5176\u8868\u73fe\u8207\u4eba\u985e\u9032\u884c\u6bd4\u8f03\u3002\u70ba\u4e86\u6839\u64da\u5faa\u8b49\u91ab\u5b78 (EBM) \u7522\u751f\u76f4\u63a5\u7684\u591a\u91cd\u9078\u64c7\u984c\u548c\u89e3\u7b54 (QA)\uff0c\u6211\u5011\u4f7f\u7528\u4e86\u4e00\u500b\u5168\u9762\u7684\u91ab\u5b78\u77e5\u8b58\u5716\u8b5c\uff08\u5305\u542b\u4f86\u81ea 50,000 \u591a\u7bc7\u540c\u884c\u8a55\u5be9\u6587\u7ae0\u7684\u8cc7\u6599\uff09\uff0c\u4e26\u5efa\u7acb\u4e86\u300cEBMQA\u300d\u3002EBMQA \u5305\u542b 105,000 \u500b QA\uff0c\u6a19\u8a18\u6709\u91ab\u5b78\u548c\u975e\u91ab\u5b78\u4e3b\u984c\uff0c\u4e26\u5206\u985e\u70ba\u6578\u503c\u6216\u8a9e\u7fa9\u554f\u984c\u3002\u6211\u5011\u4f7f\u7528\u8d85\u904e 24,500 \u500b QA \u5728\u5169\u500b\u6700\u5148\u9032\u7684 LLM \u4e0a\u5c0d\u6b64\u8cc7\u6599\u96c6\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff1aChat-GPT4 \u548c Claude3-Opus\u3002\u6211\u5011\u6839\u64da\u8a9e\u7fa9\u548c\u6578\u503c\u554f\u984c\u985e\u578b\u4ee5\u53ca\u6b21\u6a19\u8a18\u4e3b\u984c\u8a55\u4f30\u4e86 LLM \u7684\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u9a57\u8b49\uff0c\u516d\u4f4d\u91ab\u5b78\u5c08\u5bb6\u63a5\u53d7\u4e86 100 \u500b\u6578\u503c EBMQA \u554f\u984c\u7684\u6e2c\u8a66\u3002\u6211\u5011\u767c\u73fe\uff0c\u9019\u5169\u500b LLM \u5728\u8a9e\u7fa9 QA \u4e2d\u7684\u8868\u73fe\u90fd\u6bd4\u5728\u6578\u503c QA \u4e2d\u7684\u8868\u73fe\u51fa\u8272\uff0c\u800c Claude3 \u5728\u6578\u503c QA \u4e2d\u7684\u8868\u73fe\u512a\u65bc GPT4\u3002\u7136\u800c\uff0c\u9019\u5169\u500b LLM \u5728\u4e0d\u540c\u7684\u91ab\u5b78\u65b9\u9762\u90fd\u986f\u793a\u51fa\u5167\u90e8\u548c\u5916\u90e8\u7684\u5dee\u8ddd\uff0c\u4e26\u4e14\u4ecd\u7136\u905c\u65bc\u4eba\u985e\u3002\u56e0\u6b64\uff0c\u61c9\u8b39\u614e\u5c0d\u5f85\u4ed6\u5011\u7684\u91ab\u7642\u5efa\u8b70\u3002</paragraph>", "author": "Eden Avnat et.al.", "authors": "Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E. Abdulnour", "id": "2406.03855v2", "paper_url": "http://arxiv.org/abs/2406.03855v2", "repo": "null"}}