{"2406.06385": {"publish_time": "2024-06-10", "title": "Low-Rank Quantization-Aware Training for LLMs", "paper_summary": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model\nfamilies and validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7121\u6240\u4e0d\u5728\uff0c\u4f46\u7531\u65bc\u5176\u4e0d\u65b7\u589e\u52a0\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u5b83\u5011\u7684\u5be6\u969b\u90e8\u7f72\u5177\u6709\u6311\u6230\u6027\u3002\u91cf\u5316\u662f\u8b93\u5b83\u5011\u66f4\u5177\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u7684\u6700\u6709\u6548\u65b9\u6cd5\u4e4b\u4e00\u3002\u91cf\u5316\u611f\u77e5\u8a13\u7df4 (QAT) \u65b9\u6cd5\u901a\u5e38\u6703\u7522\u751f\u6700\u4f73\u7684\u91cf\u5316\u6548\u80fd\uff0c\u4f46\u4ee3\u50f9\u662f\u6f5b\u5728\u7684\u8a13\u7df4\u6642\u9593\u9577\u548c\u904e\u5ea6\u4f7f\u7528\u8a18\u61b6\u9ad4\uff0c\u9019\u4f7f\u5f97\u5176\u5728\u61c9\u7528\u65bc LLM \u6642\u4e0d\u5207\u5be6\u969b\u3002\u53d7\u5230\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u548c\u4f4e\u79e9\u9069\u61c9 (LoRA) \u6587\u737b\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa LR-QAT \u2014\u2014\u4e00\u7a2e\u9069\u7528\u65bc LLM \u7684\u8f15\u91cf\u7d1a\u4e14\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684 QAT \u6f14\u7b97\u6cd5\u3002LR-QAT \u4f7f\u7528\u591a\u500b\u7d44\u4ef6\u4f86\u7bc0\u7701\u8a18\u61b6\u9ad4\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u9810\u6e2c\u6548\u80fd\uff1a(a) \u4e86\u89e3\u91cf\u5316\u7db2\u683c\u7684\u4f4e\u79e9\u8f14\u52a9\u6b0a\u91cd\uff1b(b) \u4f7f\u7528\u5b9a\u9ede\u6216\u96d9\u5c01\u88dd\u6574\u6578\u7684\u4e0b\u8f49\u7b97\u5143\uff1b\u4ee5\u53ca (c) \u6aa2\u67e5\u9ede\u3002\u8207\u5927\u591a\u6578\u76f8\u95dc\u5de5\u4f5c\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5 (i) \u5177\u6709\u63a8\u7406\u6548\u7387\uff0c\u8207\u50b3\u7d71 PTQ \u76f8\u6bd4\u6c92\u6709\u984d\u5916\u7684\u958b\u92b7\uff1b(ii) \u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u7a2e\u901a\u7528\u7684\u5ef6\u4f38\u9810\u8a13\u7df4\u67b6\u69cb\uff0c\u9019\u610f\u5473\u8457\u7522\u751f\u7684\u6a21\u578b\u4ecd\u7136\u53ef\u4ee5\u5728\u4e4b\u5f8c\u7528\u65bc\u4efb\u4f55\u4e0b\u6e38\u4efb\u52d9\uff1b(iii) \u53ef\u4ee5\u61c9\u7528\u65bc\u5ee3\u6cdb\u7684\u91cf\u5316\u8a2d\u5b9a\uff0c\u4f8b\u5982\u4e0d\u540c\u7684\u91cf\u5316\u7c92\u5ea6\u9078\u64c7\u3001\u6fc0\u6d3b\u91cf\u5316\uff0c\u4e26\u8207\u8a31\u591a PTQ \u6280\u8853\u7121\u7e2b\u7d50\u5408\u3002\u6211\u5011\u5c07 LR-QAT \u61c9\u7528\u65bc LLaMA-2/3 \u548c Mistral \u6a21\u578b\u7cfb\u5217\uff0c\u4e26\u5728\u591a\u9805\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9a57\u8b49\u5176\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u5e38\u898b\u7684\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u65b9\u6cd5\uff0c\u4e26\u5728\u4f7f\u7528\u8a18\u61b6\u9ad4\u7684\u4e00\u5c0f\u90e8\u5206\u6642\u9054\u5230\u8207\u5168\u6a21\u578b QAT \u76f8\u540c\u7684\u6a21\u578b\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u53ef\u4ee5\u5728\u55ae\u500b\u6d88\u8cbb\u8005\u7d1a GPU \u4e0a\u8a13\u7df4\u4e00\u500b 7B LLM\uff0c\u8a18\u61b6\u9ad4\u70ba 24GB\u3002", "author": "Yelysei Bondarenko et.al.", "authors": "Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel", "id": "2406.06385v1", "paper_url": "http://arxiv.org/abs/2406.06385v1", "repo": "null"}}