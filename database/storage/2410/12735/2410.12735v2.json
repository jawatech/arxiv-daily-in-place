{"2410.12735": {"publish_time": "2024-10-16", "title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "paper_summary": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u81ea\u734e\u52f5\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u529f\u61c9\u7528 LLM-as-a-Judge \u8fed\u4ee3\u6539\u5584\u6bd4\u5c0d\u6548\u80fd\uff0c\u7121\u9700\u4eba\u5de5\u6a19\u8a3b\u504f\u597d\u8cc7\u6599\u3002\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u5229\u7528\u76f8\u540c\u7684 LLM \u4f5c\u70ba\u653f\u7b56\u6a21\u578b (\u7522\u751f\u56de\u61c9) \u548c\u734e\u52f5\u6a21\u578b (\u8a55\u5206\u548c\u6392\u540d\u9019\u4e9b\u56de\u61c9)\u3002\u6392\u540d\u5f8c\u7684\u56de\u61c9\u63a5\u8457\u7528\u4f5c\u504f\u597d\u5c0d\uff0c\u900f\u904e\u76f4\u63a5\u6bd4\u5c0d\u6280\u8853 (\u4f8b\u5982 DPO) \u4f86\u8a13\u7df4 LLM\u3002\u7136\u800c\uff0c\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u6574\u500b\u904e\u7a0b\u4e2d\uff0c\u734e\u52f5\u548c\u6392\u540d\u4e26\u7121\u4fdd\u8b49\u6e96\u78ba\uff0c\u9019\u5c0d\u65bc\u78ba\u4fdd\u6e96\u78ba\u7684\u734e\u52f5\u548c\u9ad8\u54c1\u8cea\u7684\u504f\u597d\u8cc7\u6599\u81f3\u95dc\u91cd\u8981\u3002\u4f86\u81ea\u76f8\u5c0d\u8f03\u5c0f LLM (\u4f8b\u5982 7B \u53c3\u6578) \u7684\u7d93\u9a57\u7d50\u679c\u4e5f\u986f\u793a\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u7d93\u904e\u5e7e\u6b21\u8fed\u4ee3\u5f8c\uff0c\u81ea\u734e\u52f5\u7684\u6539\u5584\u53ef\u80fd\u6e1b\u5f31\uff0c\u6211\u5011\u5047\u8a2d\u9019\u662f\u7531\u65bc\u734e\u52f5\u7cfb\u7d71\u4e2d\u7d2f\u7a4d\u7684\u504f\u5dee\u3002\u6b64\u504f\u5dee\u53ef\u80fd\u5c0e\u81f4\u7528\u65bc\u8a13\u7df4 LLM \u7684\u504f\u597d\u8cc7\u6599\u4e0d\u53ef\u9760\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u5236\u5b9a\u4e26\u5206\u6790\u81ea\u734e\u52f5\u8a9e\u8a00\u6a21\u578b\u7684\u5ee3\u7fa9\u8fed\u4ee3\u504f\u597d\u5fae\u8abf\u67b6\u69cb\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u6b63\u5247\u5316\u5f15\u5165\u6b64\u5ee3\u7fa9\u67b6\u69cb\uff0c\u4ee5\u6e1b\u8f15\u81ea\u734e\u52f5\u904e\u7a0b\u4e2d\u904e\u5ea6\u81ea\u4fe1\u7684\u504f\u597d\u6a19\u7c64\u3002\u6839\u64da\u6b64\u7406\u8ad6\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u81f4\u6027\u6b63\u5247\u5316\u81ea\u6211\u734e\u52f5\u8a9e\u8a00\u6a21\u578b (CREAM)\uff0c\u5b83\u5229\u7528\u4e0d\u540c\u8fed\u4ee3\u9593\u7684\u734e\u52f5\u4e00\u81f4\u6027\u4f86\u6b63\u5247\u5316\u81ea\u734e\u52f5\u8a13\u7df4\uff0c\u5e6b\u52a9\u6a21\u578b\u5f9e\u66f4\u53ef\u9760\u7684\u504f\u597d\u8cc7\u6599\u4e2d\u5b78\u7fd2\u3002\u900f\u904e\u6b64\u660e\u78ba\u7684\u6b63\u5247\u5316\uff0c\u6211\u5011\u7684\u7d93\u9a57\u7d50\u679c\u8b49\u660e CREAM \u5728\u6539\u5584\u734e\u52f5\u4e00\u81f4\u6027\u548c\u6bd4\u5c0d\u6548\u80fd\u65b9\u9762\u5177\u6709\u512a\u8d8a\u6027\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc https://github.com/Raibows/CREAM\u3002</paragraph>", "author": "Zhaoyang Wang et.al.", "authors": "Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao", "id": "2410.12735v2", "paper_url": "http://arxiv.org/abs/2410.12735v2", "repo": "null"}}