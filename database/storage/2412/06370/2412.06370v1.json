{"2412.06370": {"publish_time": "2024-12-09", "title": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit", "paper_summary": "Copyright infringement in frontier LLMs has received much attention recently\ndue to the New York Times v. OpenAI lawsuit, filed in December 2023. The New\nYork Times claims that GPT-4 has infringed its copyrights by reproducing\narticles for use in LLM training and by memorizing the inputs, thereby publicly\ndisplaying them in LLM outputs. Our work aims to measure the propensity of\nOpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other\nLLMs, specifically focusing on news articles. We discover that both GPT and\nClaude models use refusal training and output filters to prevent verbatim\noutput of the memorized articles. We apply a basic prompt template to bypass\nthe refusal training and show that OpenAI models are currently less prone to\nmemorization elicitation than models from Meta, Mistral, and Anthropic. We find\nthat as models increase in size, especially beyond 100 billion parameters, they\ndemonstrate significantly greater capacity for memorization. Our findings have\npractical implications for training: more attention must be placed on\npreventing verbatim memorization in very large models. Our findings also have\nlegal significance: in assessing the relative memorization capacity of OpenAI's\nLLMs, we probe the strength of The New York Times's copyright infringement\nclaims and OpenAI's legal defenses, while underscoring issues at the\nintersection of generative AI, law, and policy.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u908a\u754c LLM \u7684\u8457\u4f5c\u6b0a\u4fb5\u6b0a\u554f\u984c\u5099\u53d7\u95dc\u6ce8\uff0c\u539f\u56e0\u662f\u7d10\u7d04\u6642\u5831\u65bc 2023 \u5e74 12 \u6708\u63d0\u8d77\u7684\u7d10\u7d04\u6642\u5831\u8a34 OpenAI \u8a34\u8a1f\u3002\u7d10\u7d04\u6642\u5831\u8072\u7a31\uff0cGPT-4 \u8907\u88fd\u6587\u7ae0\u4ee5\u4f9b LLM \u8a13\u7df4\u4f7f\u7528\uff0c\u4e26\u8a18\u4f4f\u8f38\u5165\uff0c\u5f9e\u800c\u516c\u958b\u5c55\u793a\u5b83\u5011\u5728 LLM \u8f38\u51fa\u4e2d\uff0c\u4fb5\u72af\u4e86\u5176\u8457\u4f5c\u6b0a\u3002\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u8861\u91cf OpenAI \u7684 LLM \u76f8\u5c0d\u65bc\u5176\u4ed6 LLM \u5728\u5176\u8f38\u51fa\u4e2d\u8868\u73fe\u51fa\u9010\u5b57\u8a18\u61b6\u7684\u50be\u5411\uff0c\u7279\u5225\u95dc\u6ce8\u65b0\u805e\u6587\u7ae0\u3002\u6211\u5011\u767c\u73fe\uff0cGPT \u548c Claude \u6a21\u578b\u90fd\u4f7f\u7528\u62d2\u7d55\u8a13\u7df4\u548c\u8f38\u51fa\u904e\u6ffe\u5668\u4f86\u9632\u6b62\u8a18\u61b6\u6587\u7ae0\u7684\u9010\u5b57\u8f38\u51fa\u3002\u6211\u5011\u61c9\u7528\u4e86\u4e00\u500b\u57fa\u672c\u7684\u63d0\u793a\u7bc4\u672c\u4f86\u7e5e\u904e\u62d2\u7d55\u8a13\u7df4\uff0c\u4e26\u8868\u660e OpenAI \u6a21\u578b\u76ee\u524d\u6bd4 Meta\u3001Mistral \u548c Anthropic \u7684\u6a21\u578b\u66f4\u4e0d\u5bb9\u6613\u5f15\u767c\u8a18\u61b6\u3002\u6211\u5011\u767c\u73fe\uff0c\u96a8\u8457\u6a21\u578b\u898f\u6a21\u7684\u589e\u52a0\uff0c\u7279\u5225\u662f\u8d85\u904e 1000 \u5104\u500b\u53c3\u6578\u6642\uff0c\u5b83\u5011\u8868\u73fe\u51fa\u986f\u8457\u66f4\u5927\u7684\u8a18\u61b6\u5bb9\u91cf\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5c0d\u8a13\u7df4\u6709\u5be6\u969b\u610f\u7fa9\uff1a\u5fc5\u9808\u66f4\u52a0\u8a3b\u91cd\u9632\u6b62\u5728\u975e\u5e38\u5927\u7684\u6a21\u578b\u4e2d\u9010\u5b57\u8a18\u61b6\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u4e5f\u5177\u6709\u6cd5\u5f8b\u610f\u7fa9\uff1a\u5728\u8a55\u4f30 OpenAI \u7684 LLM \u7684\u76f8\u5c0d\u8a18\u61b6\u5bb9\u91cf\u6642\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u7d10\u7d04\u6642\u5831\u8457\u4f5c\u6b0a\u4fb5\u6b0a\u4e3b\u5f35\u548c OpenAI \u7684\u6cd5\u5f8b\u6297\u8faf\u7684\u5f37\u5ea6\uff0c\u540c\u6642\u5f37\u8abf\u4e86\u751f\u6210\u5f0f AI\u3001\u6cd5\u5f8b\u548c\u653f\u7b56\u4ea4\u53c9\u9ede\u4e0a\u7684\u554f\u984c\u3002", "author": "Joshua Freeman et.al.", "authors": "Joshua Freeman, Chloe Rippe, Edoardo Debenedetti, Maksym Andriushchenko", "id": "2412.06370v1", "paper_url": "http://arxiv.org/abs/2412.06370v1", "repo": "null"}}