{"2408.14874": {"publish_time": "2024-08-27", "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) has proven effective in\naligning large language models with human intentions, yet it often relies on\ncomplex methodologies like Proximal Policy Optimization (PPO) that require\nextensive hyper-parameter tuning and present challenges in sample efficiency\nand stability. In this paper, we introduce Inverse-Q*, an innovative framework\nthat transcends traditional RL methods by optimizing token-level reinforcement\nlearning without the need for additional reward or value models. Inverse-Q*\nleverages direct preference optimization techniques but extends them by\nestimating the conditionally optimal policy directly from the model's\nresponses, facilitating more granular and flexible policy shaping. Our approach\nreduces reliance on human annotation and external supervision, making it\nespecially suitable for low-resource settings. We present extensive\nexperimental results demonstrating that Inverse-Q* not only matches but\npotentially exceeds the effectiveness of PPO in terms of convergence speed and\nthe alignment of model responses with human preferences. Our findings suggest\nthat Inverse-Q* offers a practical and robust alternative to conventional RLHF\napproaches, paving the way for more efficient and adaptable model training\napproaches.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\u4f86\u81ea\u4eba\u985e\u56de\u994b (RLHF) \u5df2\u88ab\u8b49\u660e\u6709\u6548\u5730\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u610f\u5716\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u5b83\u901a\u5e38\u4f9d\u8cf4\u65bc\u8907\u96dc\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316 (PPO)\uff0c\u9019\u9700\u8981\u5ee3\u6cdb\u7684\u8d85\u53c3\u6578\u8abf\u6574\uff0c\u4e26\u5728\u7bc4\u4f8b\u6548\u7387\u548c\u7a69\u5b9a\u6027\u65b9\u9762\u63d0\u51fa\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 Inverse-Q*\uff0c\u9019\u662f\u4e00\u500b\u5275\u65b0\u7684\u67b6\u69cb\uff0c\u5b83\u900f\u904e\u6700\u4f73\u5316\u4ee3\u5e63\u7d1a\u5225\u7684\u5f37\u5316\u5b78\u7fd2\u4f86\u8d85\u8d8a\u50b3\u7d71\u7684 RL \u65b9\u6cd5\uff0c\u800c\u4e0d\u9700\u8981\u984d\u5916\u7684\u734e\u52f5\u6216\u50f9\u503c\u6a21\u578b\u3002Inverse-Q* \u5229\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\u6280\u8853\uff0c\u4f46\u900f\u904e\u76f4\u63a5\u5f9e\u6a21\u578b\u7684\u56de\u61c9\u4e2d\u4f30\u8a08\u689d\u4ef6\u6700\u4f73\u7b56\u7565\u4f86\u64f4\u5c55\u5b83\u5011\uff0c\u4fc3\u9032\u66f4\u7cbe\u7d30\u4e14\u9748\u6d3b\u7684\u7b56\u7565\u5851\u9020\u3002\u6211\u5011\u7684\u505a\u6cd5\u6e1b\u5c11\u4e86\u5c0d\u4eba\u985e\u8a3b\u89e3\u548c\u5916\u90e8\u76e3\u7763\u7684\u4f9d\u8cf4\uff0c\u4f7f\u5176\u7279\u5225\u9069\u5408\u4f4e\u8cc7\u6e90\u74b0\u5883\u3002\u6211\u5011\u63d0\u51fa\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\uff0c\u8b49\u660e Inverse-Q* \u4e0d\u50c5\u5339\u914d\uff0c\u800c\u4e14\u5728\u6536\u6582\u901f\u5ea6\u548c\u6a21\u578b\u56de\u61c9\u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\u65b9\u9762\u53ef\u80fd\u8d85\u904e PPO \u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\uff0cInverse-Q* \u70ba\u50b3\u7d71 RLHF \u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u5be6\u7528\u4e14\u5f37\u5927\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u70ba\u66f4\u6709\u6548\u7387\u4e14\u66f4\u9069\u61c9\u6027\u7684\u6a21\u578b\u8a13\u7df4\u65b9\u6cd5\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Han Xia et.al.", "authors": "Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang", "id": "2408.14874v1", "paper_url": "http://arxiv.org/abs/2408.14874v1", "repo": "null"}}