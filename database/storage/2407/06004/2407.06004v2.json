{"2407.06004": {"publish_time": "2024-07-08", "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models", "paper_summary": "While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.", "paper_summary_zh": "\u5118\u7ba1\u4eba\u985e\u81ea\u7136\u800c\u7136\u5730\u767c\u5c55\u51fa\u5fc3\u667a\u7406\u8ad6 (ToM)\uff0c\u5373\u7406\u89e3\u4ed6\u4eba\u5fc3\u7406\u72c0\u614b\u548c\u4fe1\u5ff5\u7684\u80fd\u529b\uff0c\u4f46\u6700\u5148\u9032\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7c21\u55ae\u7684 ToM \u57fa\u6e96\u4e0a\u8868\u73fe\u4e0d\u4f73\u3002\u6211\u5011\u5047\u8a2d\u6211\u5011\u53ef\u4ee5\u900f\u904e\u8a55\u4f30\u4eba\u985e ToM \u7684\u91cd\u8981\u524d\u5146\uff08\u77e5\u89ba\u63a8\u8ad6\u548c\u77e5\u89ba\u5230\u4fe1\u5ff5\u7684\u63a8\u8ad6\uff09\u4f86\u64f4\u5c55\u6211\u5011\u5c0d LLM \u7684 ToM \u80fd\u529b\u7684\u7406\u89e3\u3002\u6211\u5011\u5f15\u5165\u4e86\u5169\u500b\u8cc7\u6599\u96c6 Percept-ToMi \u548c Percept-FANToM\uff0c\u5206\u5225\u91dd\u5c0d ToMi \u548c FANToM \u4e0a\u7684\u89d2\u8272\u77e5\u89ba\u9032\u884c\u8a3b\u89e3\uff0c\u4ee5\u8a55\u4f30 LLM \u4e2d\u9019\u4e9b\u5148\u9a45\u63a8\u8ad6\u7684 ToM\u3002\u6211\u5011\u5c0d\u516b\u500b\u6700\u5148\u9032\u7684 LLM \u9032\u884c\u8a55\u4f30\uff0c\u7d50\u679c\u986f\u793a\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u5728\u77e5\u89ba\u63a8\u8ad6\u65b9\u9762\u8868\u73fe\u826f\u597d\uff0c\u4f46\u5728\u77e5\u89ba\u5230\u4fe1\u5ff5\u7684\u63a8\u8ad6\uff08\u4f8b\u5982\u7f3a\u4e4f\u6291\u5236\u63a7\u5236\uff09\u65b9\u9762\u8868\u73fe\u51fa\u7684\u80fd\u529b\u6709\u9650\u3002\u6839\u64da\u9019\u4e9b\u7d50\u679c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 PercepToM\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684 ToM \u65b9\u6cd5\uff0c\u5b83\u5229\u7528 LLM \u5f37\u5927\u7684\u77e5\u89ba\u63a8\u8ad6\u80fd\u529b\uff0c\u540c\u6642\u88dc\u5145\u5176\u6709\u9650\u7684\u77e5\u89ba\u5230\u4fe1\u5ff5\u7684\u63a8\u8ad6\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cPercepToM \u5927\u5e45\u63d0\u5347\u4e86 LLM \u7684\u6548\u80fd\uff0c\u5c24\u5176\u662f\u5728\u932f\u8aa4\u4fe1\u5ff5\u5834\u666f\u4e2d\u3002", "author": "Chani Jung et.al.", "authors": "Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh, Hyunwoo Kim", "id": "2407.06004v2", "paper_url": "http://arxiv.org/abs/2407.06004v2", "repo": "null"}}