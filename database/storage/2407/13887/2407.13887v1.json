{"2407.13887": {"publish_time": "2024-07-18", "title": "Learning Goal-Conditioned Representations for Language Reward Models", "paper_summary": "Techniques that learn improved representations via offline data or\nself-supervised objectives have shown impressive results in traditional\nreinforcement learning (RL). Nevertheless, it is unclear how improved\nrepresentation learning can benefit reinforcement learning from human feedback\n(RLHF) on language models (LMs). In this work, we propose training reward\nmodels (RMs) in a contrastive, $\\textit{goal-conditioned}$ fashion by\nincreasing the representation similarity of future states along sampled\npreferred trajectories and decreasing the similarity along randomly sampled\ndispreferred trajectories. This objective significantly improves RM performance\nby up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k.\nThese findings extend to general alignment as well -- on the Helpful-Harmless\ndataset, we observe $2.3\\%$ increase in accuracy. Beyond improving reward model\nperformance, we show this way of training RM representations enables improved\n$\\textit{steerability}$ because it allows us to evaluate the likelihood of an\naction achieving a particular goal-state (e.g., whether a solution is correct\nor helpful). Leveraging this insight, we find that we can filter up to $55\\%$\nof generated tokens during majority voting by discarding trajectories likely to\nend up in an \"incorrect\" state, which leads to significant cost savings. We\nadditionally find that these representations can perform fine-grained control\nby conditioning on desired future goal-states. For example, we show that\nsteering a Llama 3 model towards helpful generations with our approach improves\nhelpfulness by $9.6\\%$ over a supervised-fine-tuning trained baseline.\nSimilarly, steering the model towards complex generations improves complexity\nby $21.6\\%$ over the baseline. Overall, we find that training RMs in this\ncontrastive, goal-conditioned fashion significantly improves performance and\nenables model steerability.", "paper_summary_zh": "<paragraph>\u900f\u904e\u96e2\u7dda\u8cc7\u6599\u6216\u81ea\u6211\u76e3\u7763\u76ee\u6a19\u5b78\u7fd2\u5230\u6539\u826f\u8868\u5fb5\u7684\u6280\u8853\uff0c\u5728\u50b3\u7d71\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u4e2d\u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u6539\u826f\u7684\u8868\u5fb5\u5b78\u7fd2\u5982\u4f55\u80fd\u5f9e\u8a9e\u8a00\u6a21\u578b (LM) \u4e0a\u7684\u4eba\u985e\u56de\u994b (RLHF) \u4e2d\u53d7\u76ca\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u4ee5\u5c0d\u6bd4\u3001$\\textit{\u76ee\u6a19\u5236\u7d04}$\u7684\u65b9\u5f0f\u8a13\u7df4\u734e\u52f5\u6a21\u578b (RM)\uff0c\u65b9\u6cd5\u662f\u589e\u52a0\u6cbf\u8457\u53d6\u6a23\u504f\u597d\u8ecc\u8de1\u7684\u672a\u4f86\u72c0\u614b\u7684\u8868\u5fb5\u76f8\u4f3c\u6027\uff0c\u4e26\u6e1b\u5c11\u6cbf\u8457\u96a8\u6a5f\u53d6\u6a23\u975e\u504f\u597d\u8ecc\u8de1\u7684\u76f8\u4f3c\u6027\u3002\u6b64\u76ee\u6a19\u986f\u8457\u6539\u5584 RM \u6548\u80fd\uff0c\u5728 MATH \u548c GSM8k \u7b49\u5177\u6311\u6230\u6027\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u63d0\u5347\u4e86\u9ad8\u9054 0.09 AUROC\u3002\u9019\u4e9b\u767c\u73fe\u4e5f\u5ef6\u4f38\u5230\u4e00\u822c\u5c0d\u9f4a\u2014\u2014\u5728 Helpful-Harmless \u8cc7\u6599\u96c6\u4e0a\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u6e96\u78ba\u5ea6\u589e\u52a0\u4e86 $2.3\\%$\u3002\u9664\u4e86\u6539\u5584\u734e\u52f5\u6a21\u578b\u6548\u80fd\u4e4b\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9019\u7a2e\u8a13\u7df4 RM \u8868\u5fb5\u7684\u65b9\u5f0f\u80fd\u6539\u5584$\\textit{\u53ef\u63a7\u6027}$\uff0c\u56e0\u70ba\u5b83\u8b93\u6211\u5011\u80fd\u5920\u8a55\u4f30\u52d5\u4f5c\u9054\u6210\u7279\u5b9a\u76ee\u6a19\u72c0\u614b\u7684\u53ef\u80fd\u6027\uff08\u4f8b\u5982\uff0c\u67d0\u500b\u89e3\u662f\u5426\u6b63\u78ba\u6216\u6709\u5e6b\u52a9\uff09\u3002\u5229\u7528\u6b64\u6d1e\u898b\uff0c\u6211\u5011\u767c\u73fe\u6211\u5011\u53ef\u4ee5\u5728\u591a\u6578\u6295\u7968\u671f\u9593\u904e\u6ffe\u6389\u9ad8\u9054 $55\\%$ \u7684\u5df2\u7522\u751f\u6b0a\u6756\uff0c\u65b9\u6cd5\u662f\u6368\u68c4\u53ef\u80fd\u6703\u7d50\u675f\u5728\u300c\u4e0d\u6b63\u78ba\u300d\u72c0\u614b\u7684\u8ecc\u8de1\uff0c\u9019\u80fd\u5927\u5e45\u7bc0\u7701\u6210\u672c\u3002\u6211\u5011\u53e6\u5916\u767c\u73fe\uff0c\u9019\u4e9b\u8868\u5fb5\u53ef\u4ee5\u900f\u904e\u5236\u7d04\u5728\u7406\u60f3\u7684\u672a\u4f86\u76ee\u6a19\u72c0\u614b\u4e0a\u4f86\u57f7\u884c\u7d30\u5fae\u63a7\u5236\u3002\u4f8b\u5982\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4f7f\u7528\u6211\u5011\u7684\u505a\u6cd5\u5c07 Llama 3 \u6a21\u578b\u5c0e\u5411\u6709\u5e6b\u52a9\u7684\u751f\u6210\uff0c\u80fd\u6bd4\u7d93\u904e\u76e3\u7763\u5fae\u8abf\u8a13\u7df4\u7684\u57fa\u6e96\u7dda\u6539\u5584 $9.6\\%$ \u7684\u6709\u5e6b\u52a9\u6027\u3002\u985e\u4f3c\u5730\uff0c\u5c07\u6a21\u578b\u5c0e\u5411\u8907\u96dc\u7684\u751f\u6210\u80fd\u6bd4\u57fa\u6e96\u7dda\u6539\u5584 $21.6\\%$ \u7684\u8907\u96dc\u6027\u3002\u6574\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u767c\u73fe\u4ee5\u9019\u7a2e\u5c0d\u6bd4\u3001\u76ee\u6a19\u5236\u7d04\u7684\u65b9\u5f0f\u8a13\u7df4 RM \u80fd\u986f\u8457\u6539\u5584\u6548\u80fd\uff0c\u4e26\u8b93\u6a21\u578b\u5177\u5099\u53ef\u63a7\u6027\u3002</paragraph>", "author": "Vaskar Nath et.al.", "authors": "Vaskar Nath, Dylan Slack, Jeff Da, Yuntao Ma, Hugh Zhang, Spencer Whitehead, Sean Hendryx", "id": "2407.13887v1", "paper_url": "http://arxiv.org/abs/2407.13887v1", "repo": "https://github.com/vaskarnathscale/goal-conditioned-rm"}}