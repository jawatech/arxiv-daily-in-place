{"2406.17755": {"publish_time": "2024-06-25", "title": "Accelerating Clinical Evidence Synthesis with Large Language Models", "paper_summary": "Automatic medical discovery by AI is a dream of many. One step toward that\ngoal is to create an AI model to understand clinical studies and synthesize\nclinical evidence from the literature. Clinical evidence synthesis currently\nrelies on systematic reviews of clinical trials and retrospective analyses from\nmedical literature. However, the rapid expansion of publications presents\nchallenges in efficiently identifying, summarizing, and updating evidence. We\nintroduce TrialMind, a generative AI-based pipeline for conducting medical\nsystematic reviews, encompassing study search, screening, and data extraction\nphases. We utilize large language models (LLMs) to drive each pipeline\ncomponent while incorporating human expert oversight to minimize errors. To\nfacilitate evaluation, we also create a benchmark dataset TrialReviewBench, a\ncustom dataset with 870 annotated clinical studies from 25 meta-analysis papers\nacross various medical treatments. Our results demonstrate that TrialMind\nsignificantly improves the literature review process, achieving high recall\nrates (0.897-1.000) in study searching from over 20 million PubMed studies and\noutperforming traditional language model embeddings-based methods in screening\n(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses\ndirect GPT-4 performance in result extraction, with accuracy ranging from 0.65\nto 0.84. We also support clinical evidence synthesis in forest plots, as\nvalidated by eight human annotators who preferred TrialMind over the GPT-4\nbaseline with a winning rate of 62.5%-100% across the involved reviews. Our\nfindings suggest that an LLM-based clinical evidence synthesis approach, such\nas TrialMind, can enable reliable and high-quality clinical evidence synthesis\nto improve clinical research efficiency.", "paper_summary_zh": "<paragraph>\u8a31\u591a\u4eba\u5922\u60f3\u8457\u7531 AI \u81ea\u52d5\u9032\u884c\u91ab\u5b78\u767c\u73fe\u3002\u671d\u6b64\u76ee\u6a19\u9081\u9032\u7684\u4e00\u6b65\u662f\u5efa\u7acb\u4e00\u500b AI \u6a21\u578b\u4f86\u7406\u89e3\u81e8\u5e8a\u7814\u7a76\uff0c\u4e26\u5f9e\u6587\u737b\u4e2d\u7d9c\u5408\u81e8\u5e8a\u8b49\u64da\u3002\u81e8\u5e8a\u8b49\u64da\u7d9c\u5408\u76ee\u524d\u4f9d\u8cf4\u65bc\u81e8\u5e8a\u8a66\u9a57\u7684\u7cfb\u7d71\u6027\u56de\u9867\u548c\u91ab\u5b78\u6587\u737b\u7684\u56de\u6eaf\u6027\u5206\u6790\u3002\u7136\u800c\uff0c\u51fa\u7248\u7269\u7684\u5feb\u901f\u64f4\u5f35\u5728\u6709\u6548\u5730\u8b58\u5225\u3001\u7e3d\u7d50\u548c\u66f4\u65b0\u8b49\u64da\u65b9\u9762\u63d0\u51fa\u4e86\u6311\u6230\u3002\u6211\u5011\u4ecb\u7d39\u4e86 TrialMind\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u751f\u6210\u5f0f AI \u7684\u7ba1\u9053\uff0c\u7528\u65bc\u9032\u884c\u91ab\u5b78\u7cfb\u7d71\u6027\u56de\u9867\uff0c\u5305\u62ec\u7814\u7a76\u641c\u5c0b\u3001\u7be9\u9078\u548c\u6578\u64da\u8403\u53d6\u968e\u6bb5\u3002\u6211\u5011\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u9a45\u52d5\u6bcf\u500b\u7ba1\u9053\u7d44\u4ef6\uff0c\u540c\u6642\u7d50\u5408\u4eba\u985e\u5c08\u5bb6\u76e3\u7763\u4ee5\u6700\u5927\u7a0b\u5ea6\u5730\u6e1b\u5c11\u932f\u8aa4\u3002\u70ba\u4e86\u4fc3\u9032\u8a55\u4f30\uff0c\u6211\u5011\u9084\u5efa\u7acb\u4e86\u4e00\u500b\u57fa\u6e96\u6578\u64da\u96c6 TrialReviewBench\uff0c\u9019\u662f\u4e00\u500b\u81ea\u8a02\u6578\u64da\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 25 \u7bc7\u95dc\u65bc\u5404\u7a2e\u91ab\u7642\u6cbb\u7642\u7684\u5143\u5206\u6790\u8ad6\u6587\u7684 870 \u9805\u8a3b\u89e3\u81e8\u5e8a\u7814\u7a76\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cTrialMind \u5927\u5e45\u6539\u5584\u4e86\u6587\u737b\u56de\u9867\u6d41\u7a0b\uff0c\u5728\u8d85\u904e 2000 \u842c\u7bc7 PubMed \u7814\u7a76\u4e2d\u9054\u5230\u4e86\u5f88\u9ad8\u7684\u53ec\u56de\u7387 (0.897-1.000)\uff0c\u4e26\u5728\u7be9\u9078\u65b9\u9762\u512a\u65bc\u57fa\u65bc\u50b3\u7d71\u8a9e\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u65b9\u6cd5\uff08\u53ec\u56de\u7387 @20 \u70ba 0.227-0.246\uff0c\u76f8\u8f03\u65bc 0.000-0.102\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u7d50\u679c\u8403\u53d6\u65b9\u9762\u8d85\u8d8a\u4e86\u76f4\u63a5\u7684 GPT-4 \u6548\u80fd\uff0c\u6e96\u78ba\u5ea6\u7bc4\u570d\u5f9e 0.65 \u5230 0.84\u3002\u6211\u5011\u9084\u652f\u63f4\u68ee\u6797\u5716\u4e2d\u7684\u81e8\u5e8a\u8b49\u64da\u7d9c\u5408\uff0c\u9019\u9ede\u5df2\u7372\u5f97\u516b\u4f4d\u4eba\u985e\u8a3b\u89e3\u8005\u7684\u9a57\u8b49\uff0c\u4ed6\u5011\u5728\u76f8\u95dc\u56de\u9867\u4e2d\u4ee5 62.5%-100% \u7684\u7372\u52dd\u7387\u504f\u597d TrialMind\uff0c\u52dd\u904e GPT-4 \u57fa\u6e96\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u57fa\u65bc LLM \u7684\u81e8\u5e8a\u8b49\u64da\u7d9c\u5408\u65b9\u6cd5\uff08\u4f8b\u5982 TrialMind\uff09\u53ef\u4ee5\u5be6\u73fe\u53ef\u9760\u4e14\u9ad8\u54c1\u8cea\u7684\u81e8\u5e8a\u8b49\u64da\u7d9c\u5408\uff0c\u4ee5\u63d0\u9ad8\u81e8\u5e8a\u7814\u7a76\u6548\u7387\u3002</paragraph>", "author": "Zifeng Wang et.al.", "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun", "id": "2406.17755v1", "paper_url": "http://arxiv.org/abs/2406.17755v1", "repo": "null"}}