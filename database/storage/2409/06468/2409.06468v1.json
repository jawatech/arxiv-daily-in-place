{"2409.06468": {"publish_time": "2024-09-10", "title": "An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech Recognition", "paper_summary": "End-to-end (E2E) automatic speech recognition (ASR) models have become\nstandard practice for various commercial applications. However, in real-world\nscenarios, the long-tailed nature of word distribution often leads E2E ASR\nmodels to perform well on common words but fall short in recognizing uncommon\nones. Recently, the notion of a contextual adapter (CA) was proposed to infuse\nexternal knowledge represented by a context word list into E2E ASR models.\nAlthough CA can improve recognition performance on rare words, two crucial data\nimbalance problems remain. First, when using low-frequency words as context\nwords during training, since these words rarely occur in the utterance, CA\nbecomes prone to overfit on attending to the <no-context> token due to\nhigher-frequency words not being present in the context list. Second, the\nlong-tailed distribution within the context list itself still causes the model\nto perform poorly on low-frequency context words. In light of this, we explore\nin-depth the impact of altering the context list to have words with different\nfrequency distributions on model performance, and meanwhile extend CA with a\nsimple yet effective context-balanced learning objective. A series of\nexperiments conducted on the AISHELL-1 benchmark dataset suggests that using\nall vocabulary words from the training corpus as the context list and pairing\nthem with our balanced objective yields the best performance, demonstrating a\nsignificant reduction in character error rate (CER) by up to 1.21% and a more\npronounced 9.44% reduction in the error rate of zero-shot words.", "paper_summary_zh": "\u7aef\u5c0d\u7aef (E2E) \u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6a21\u578b\u5df2\u6210\u70ba\u5404\u7a2e\u5546\u696d\u61c9\u7528\u4e2d\u7684\u6a19\u6e96\u5be6\u52d9\u3002\u7136\u800c\uff0c\u5728\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u8a5e\u5f59\u5206\u4f48\u7684\u9577\u5c3e\u7279\u6027\u5e38\u5e38\u5c0e\u81f4 E2E ASR \u6a21\u578b\u5728\u5e38\u898b\u8a5e\u5f59\u4e0a\u8868\u73fe\u826f\u597d\uff0c\u4f46\u5728\u8fa8\u8b58\u4e0d\u5e38\u898b\u8a5e\u5f59\u6642\u537b\u8868\u73fe\u4e0d\u4f73\u3002\u6700\u8fd1\uff0c\u6709\u4eba\u63d0\u51fa\u60c5\u5883\u5f0f\u9069\u914d\u5668 (CA) \u7684\u6982\u5ff5\uff0c\u5c07\u7531\u60c5\u5883\u8a5e\u5f59\u6e05\u55ae\u6240\u4ee3\u8868\u7684\u5916\u90e8\u77e5\u8b58\u6ce8\u5165 E2E ASR \u6a21\u578b\u4e2d\u3002\u5118\u7ba1 CA \u80fd\u5920\u63d0\u5347\u7f55\u898b\u8a5e\u5f59\u7684\u8fa8\u8b58\u6548\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u5169\u500b\u95dc\u9375\u7684\u8cc7\u6599\u4e0d\u5e73\u8861\u554f\u984c\u3002\u9996\u5148\uff0c\u5728\u8a13\u7df4\u671f\u9593\u4f7f\u7528\u4f4e\u983b\u7387\u8a5e\u5f59\u4f5c\u70ba\u60c5\u5883\u8a5e\u5f59\u6642\uff0c\u7531\u65bc\u9019\u4e9b\u8a5e\u5f59\u5f88\u5c11\u51fa\u73fe\u5728\u8a9e\u53e5\u4e2d\uff0c\u56e0\u6b64 CA \u5bb9\u6613\u904e\u5ea6\u64ec\u5408\uff0c\u5c08\u6ce8\u65bc <no-context> \u6b0a\u6756\uff0c\u56e0\u70ba\u8f03\u9ad8\u983b\u7387\u7684\u8a5e\u5f59\u4e26\u672a\u51fa\u73fe\u5728\u60c5\u5883\u6e05\u55ae\u4e2d\u3002\u5176\u6b21\uff0c\u60c5\u5883\u6e05\u55ae\u672c\u8eab\u5167\u7684\u9577\u5c3e\u5206\u4f48\u4ecd\u6703\u5c0e\u81f4\u6a21\u578b\u5728\u4f4e\u983b\u7387\u60c5\u5883\u8a5e\u5f59\u4e0a\u7684\u8868\u73fe\u4e0d\u4f73\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u6539\u8b8a\u60c5\u5883\u6e05\u55ae\u4ee5\u7d0d\u5165\u5177\u6709\u4e0d\u540c\u983b\u7387\u5206\u4f48\u7684\u8a5e\u5f59\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u540c\u6642\u4f7f\u7528\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u5e73\u8861\u60c5\u5883\u5b78\u7fd2\u76ee\u6a19\u4f86\u5ef6\u4f38 CA\u3002\u5728 AISHELL-1 \u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u4e00\u7cfb\u5217\u5be6\u9a57\u986f\u793a\uff0c\u4f7f\u7528\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u7684\u6240\u6709\u8a5e\u5f59\u4f5c\u70ba\u60c5\u5883\u6e05\u55ae\uff0c\u4e26\u5c07\u5176\u8207\u6211\u5011\u7684\u5e73\u8861\u76ee\u6a19\u914d\u5c0d\uff0c\u53ef\u7522\u751f\u6700\u4f73\u6548\u80fd\uff0c\u8b49\u660e\u5b57\u5143\u932f\u8aa4\u7387 (CER) \u5927\u5e45\u964d\u4f4e\u4e86 1.21%\uff0c\u800c\u96f6\u6b21\u5b78\u7fd2\u8a5e\u5f59\u7684\u932f\u8aa4\u7387\u5247\u5927\u5e45\u964d\u4f4e\u4e86 9.44%\u3002", "author": "Yi-Cheng Wang et.al.", "authors": "Yi-Cheng Wang, Li-Ting Pai, Bi-Cheng Yan, Hsin-Wei Wang, Chi-Han Lin, Berlin Chen", "id": "2409.06468v1", "paper_url": "http://arxiv.org/abs/2409.06468v1", "repo": "null"}}