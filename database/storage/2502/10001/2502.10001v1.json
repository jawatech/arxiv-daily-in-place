{"2502.10001": {"publish_time": "2025-02-14", "title": "EmbBERT-Q: Breaking Memory Barriers in Embedded NLP", "paper_summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u5728\u5ee3\u6cdb\u7684\u61c9\u7528\u4e2d\u6a39\u7acb\u4e86\u65b0\u7684\u6a19\u6e96\u3002\u7136\u800c\uff0c\u5b83\u5011\u76f8\u95dc\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u4f7f\u5f97\u5b83\u5011\u4e0d\u5207\u5be6\u969b\u5730\u90e8\u7f72\u5728\u6280\u8853\u53d7\u9650\u7684\u5c0f\u578b\u8a2d\u5099\u4e0a\uff0c\u4f8b\u5982\u7a7f\u6234\u5f0f\u8a2d\u5099\u548c\u7269\u806f\u7db2\u55ae\u5143\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 EmbBERT-Q\uff0c\u9019\u662f\u4e00\u500b\u5c08\u70ba\u5177\u6709\u56b4\u683c\u8a18\u61b6\u9ad4\u9650\u5236\u7684\u5c0f\u578b\u8a2d\u5099\u8a2d\u8a08\u7684\u65b0\u578b\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u3002EmbBERT-Q \u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\u9054\u5230\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u7684\u6700\u65b0\u6280\u8853 (SotA) \u6e96\u78ba\u5ea6\uff0c\u7e3d\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff08\u6b0a\u91cd\u548c\u6fc0\u6d3b\uff09\u50c5\u70ba 781 kB\uff0c\u8207 SotA \u6a21\u578b\u76f8\u6bd4\uff0c\u5c3a\u5bf8\u7e2e\u5c0f\u4e86 25 \u500d\u3002\u901a\u904e\u5c07\u67b6\u69cb\u5275\u65b0\u8207\u786c\u9ad4\u76f8\u5bb9\u7684 8 \u4f4d\u91cf\u5316\u76f8\u7d50\u5408\uff0cEmbBERT-Q \u6301\u7e8c\u512a\u65bc\u7e2e\u5c0f\u5230 2 MB \u8a18\u61b6\u9ad4\u9810\u7b97\u7684\u5e7e\u500b\u57fa\u6e96\u6a21\u578b\uff08\u5373\uff0c\u5c0f\u578b\u8a2d\u5099\u4e2d\u901a\u5e38\u53ef\u7528\u7684\u6700\u5927\u8a18\u61b6\u9ad4\uff09\uff0c\u5305\u62ec BERT \u548c MAMBA \u7684\u9ad8\u5ea6\u58d3\u7e2e\u7248\u672c\u3002\u5728\u7cbe\u9078\u57fa\u6e96\u8cc7\u6599\u96c6 TinyNLP\uff08\u5c08\u9580\u7528\u65bc\u8a55\u4f30 NLP \u4efb\u52d9\u4e2d\u7684\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u5be6\u969b\u5834\u666f\uff09\u548c GLUE \u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8a55\u4f30\uff0c\u8b49\u660e\u4e86 EmbBERT-Q \u5728\u73fe\u6709\u65b9\u6cd5\u65b9\u9762\u63d0\u4f9b\u7af6\u722d\u6e96\u78ba\u5ea6\u7684\u80fd\u529b\uff0c\u5728\u8a18\u61b6\u9ad4\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u4e86\u7121\u8207\u502b\u6bd4\u7684\u5e73\u8861\u3002\u70ba\u4e86\u78ba\u4fdd\u6240\u6709\u7d50\u679c\u7684\u5b8c\u6574\u4e14\u7acb\u5373\u91cd\u73fe\u6027\uff0c\u6211\u5011\u5728 https://github.com/RiccardoBravin/tiny-LLM \u4e0a\u91cb\u51fa\u6240\u6709\u7a0b\u5f0f\u78bc\u3001\u6307\u4ee4\u78bc\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\u3002", "author": "Riccardo Bravin et.al.", "authors": "Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri", "id": "2502.10001v1", "paper_url": "http://arxiv.org/abs/2502.10001v1", "repo": "null"}}