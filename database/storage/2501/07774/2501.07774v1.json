{"2501.07774": {"publish_time": "2025-01-14", "title": "Transforming Indoor Localization: Advanced Transformer Architecture for NLOS Dominated Wireless Environments with Distributed Sensors", "paper_summary": "Indoor localization in challenging non-line-of-sight (NLOS) environments\noften leads to mediocre accuracy with traditional approaches. Deep learning\n(DL) has been applied to tackle these challenges; however, many DL approaches\noverlook computational complexity, especially for floating-point operations\n(FLOPs), making them unsuitable for resource-limited devices. Transformer-based\nmodels have achieved remarkable success in natural language processing (NLP)\nand computer vision (CV) tasks, motivating their use in wireless applications.\nHowever, their use in indoor localization remains nascent, and directly\napplying Transformers for indoor localization can be both computationally\nintensive and exhibit limitations in accuracy. To address these challenges, in\nthis work, we introduce a novel tokenization approach, referred to as Sensor\nSnapshot Tokenization (SST), which preserves variable-specific representations\nof power delay profile (PDP) and enhances attention mechanisms by effectively\ncapturing multi-variate correlation. Complementing this, we propose a\nlightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer)\nmodel, designed to reduce computational complexity without compromising\nlocalization accuracy. Together, these contributions mitigate the computational\nburden and dependency on large datasets, making Transformer models more\nefficient and suitable for resource-constrained scenarios. The proposed\ntokenization method enables the Vanilla Transformer to achieve a 90th\npercentile positioning error of 0.388 m in a highly NLOS indoor factory,\nsurpassing conventional tokenization methods. The L-SwiGLU ViT further reduces\nthe error to 0.355 m, achieving an 8.51% improvement. Additionally, the\nproposed model outperforms a 14.1 times larger model with a 46.13% improvement,\nunderscoring its computational efficiency.", "paper_summary_zh": "\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u975e\u8996\u7dda\uff08NLOS\uff09\u74b0\u5883\u4e2d\u9032\u884c\u5ba4\u5167\u5b9a\u4f4d\uff0c\u901a\u5e38\u6703\u5c0e\u81f4\u50b3\u7d71\u65b9\u6cd5\u7684\u6e96\u78ba\u5ea6\u5e73\u5eb8\u3002\u6df1\u5ea6\u5b78\u7fd2\uff08DL\uff09\u5df2\u88ab\u61c9\u7528\u65bc\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff1b\u7136\u800c\uff0c\u8a31\u591a DL \u65b9\u6cd5\u5ffd\u8996\u4e86\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u6d6e\u9ede\u904b\u7b97\uff08FLOP\uff09\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u4e0d\u9069\u5408\u8cc7\u6e90\u53d7\u9650\u7684\u8a2d\u5099\u3002\u57fa\u65bc Transformer \u7684\u6a21\u578b\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff08NLP\uff09\u548c\u8a08\u7b97\u6a5f\u8996\u89ba\uff08CV\uff09\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6210\u529f\uff0c\u6fc0\u52f5\u4e86\u5b83\u5011\u5728\u7121\u7dda\u61c9\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u5ba4\u5167\u5b9a\u4f4d\u4e2d\u7684\u4f7f\u7528\u4ecd\u7136\u8655\u65bc\u840c\u82bd\u968e\u6bb5\uff0c\u4e26\u4e14\u76f4\u63a5\u61c9\u7528 Transformer \u9032\u884c\u5ba4\u5167\u5b9a\u4f4d\u65e2\u6703\u8a08\u7b97\u5bc6\u96c6\uff0c\u53c8\u6703\u8868\u73fe\u51fa\u6e96\u78ba\u6027\u65b9\u9762\u7684\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u6a19\u8a18\u5316\u65b9\u6cd5\uff0c\u7a31\u70ba\u50b3\u611f\u5668\u5feb\u7167\u6a19\u8a18\u5316\uff08SST\uff09\uff0c\u5b83\u4fdd\u7559\u4e86\u529f\u7387\u5ef6\u9072\u8f2a\u5ed3\uff08PDP\uff09\u7684\u7279\u5b9a\u65bc\u8b8a\u91cf\u7684\u8868\u793a\uff0c\u4e26\u901a\u904e\u6709\u6548\u5730\u6355\u7372\u591a\u8b8a\u91cf\u76f8\u95dc\u6027\u4f86\u589e\u5f37\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u8207\u6b64\u76f8\u8f14\u76f8\u6210\u7684\u662f\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u7684 Swish-Gated \u7dda\u6027\u55ae\u5143 Transformer\uff08L-SwiGLU Transformer\uff09\u6a21\u578b\uff0c\u65e8\u5728\u964d\u4f4e\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u9019\u4e9b\u8ca2\u737b\u5171\u540c\u6e1b\u8f15\u4e86\u8a08\u7b97\u8ca0\u64d4\u548c\u5c0d\u5927\u578b\u6578\u64da\u96c6\u7684\u4f9d\u8cf4\u6027\uff0c\u4f7f Transformer \u6a21\u578b\u66f4\u6709\u6548\u7387\uff0c\u66f4\u9069\u5408\u8cc7\u6e90\u53d7\u9650\u7684\u5834\u666f\u3002\u6240\u63d0\u51fa\u7684\u6a19\u8a18\u5316\u65b9\u6cd5\u4f7f Vanilla Transformer \u80fd\u5920\u5728\u9ad8\u5ea6 NLOS \u5ba4\u5167\u5de5\u5ee0\u4e2d\u5be6\u73fe 0.388 m \u7684\u7b2c 90 \u500b\u767e\u5206\u4f4d\u5b9a\u4f4d\u8aa4\u5dee\uff0c\u8d85\u904e\u4e86\u50b3\u7d71\u7684\u6a19\u8a18\u5316\u65b9\u6cd5\u3002L-SwiGLU ViT \u9032\u4e00\u6b65\u5c07\u8aa4\u5dee\u964d\u4f4e\u5230 0.355 m\uff0c\u5be6\u73fe\u4e86 8.51% \u7684\u6539\u9032\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u512a\u65bc\u4e00\u500b\u5927 14.1 \u500d\u7684\u6a21\u578b\uff0c\u6539\u9032\u4e86 46.13%\uff0c\u7a81\u986f\u4e86\u5b83\u7684\u8a08\u7b97\u6548\u7387\u3002", "author": "Saad Masrur et.al.", "authors": "Saad Masrur, Jung-Fu, Cheng, Atieh R. Khamesi, Ismail Guvenc", "id": "2501.07774v1", "paper_url": "http://arxiv.org/abs/2501.07774v1", "repo": "null"}}