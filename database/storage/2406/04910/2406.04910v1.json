{"2406.04910": {"publish_time": "2024-06-07", "title": "PolyLUT-Add: FPGA-based LUT Inference with Wide Inputs", "paper_summary": "FPGAs have distinct advantages as a technology for deploying deep neural\nnetworks (DNNs) at the edge. Lookup Table (LUT) based networks, where neurons\nare directly modelled using LUTs, help maximize this promise of offering\nultra-low latency and high area efficiency on FPGAs. Unfortunately, LUT\nresource usage scales exponentially with the number of inputs to the LUT,\nrestricting PolyLUT to small LUT sizes. This work introduces PolyLUT-Add, a\ntechnique that enhances neuron connectivity by combining $A$ PolyLUT\nsub-neurons via addition to improve accuracy. Moreover, we describe a novel\narchitecture to improve its scalability. We evaluated our implementation over\nthe MNIST, Jet Substructure classification and Network Intrusion Detection\nbenchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT\nreduction of $1.3-7.7\\times$ with a $1.2-2.2\\times$ decrease in latency.", "paper_summary_zh": "FPGA \u5177\u6709\u4f5c\u70ba\u6280\u8853\u7684\u986f\u8457\u512a\u52e2\uff0c\u53ef\u5c07\u6df1\u5ea6\u795e\u7d93\u7db2\u8def (DNN) \u90e8\u7f72\u5728\u908a\u7de3\u3002\u57fa\u65bc\u67e5\u8a62\u8868 (LUT) \u7684\u7db2\u8def\uff0c\u5176\u4e2d\u795e\u7d93\u5143\u76f4\u63a5\u4f7f\u7528 LUT \u5efa\u6a21\uff0c\u6709\u52a9\u65bc\u6700\u5927\u5316\u6b64\u627f\u8afe\uff0c\u5728 FPGA \u4e0a\u63d0\u4f9b\u8d85\u4f4e\u5ef6\u9072\u548c\u9ad8\u9762\u7a4d\u6548\u7387\u3002\u4e0d\u5e78\u7684\u662f\uff0cLUT \u8cc7\u6e90\u4f7f\u7528\u6703\u96a8\u8457 LUT \u8f38\u5165\u6578\u91cf\u5448\u6307\u6578\u7d1a\u589e\u9577\uff0c\u5c07 PolyLUT \u9650\u5236\u5728\u8f03\u5c0f\u7684 LUT \u5927\u5c0f\u3002\u9019\u9805\u5de5\u4f5c\u5f15\u5165\u4e86 PolyLUT-Add\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u52a0\u6cd5\u7d50\u5408 $A$ PolyLUT \u5b50\u795e\u7d93\u5143\u4f86\u589e\u5f37\u795e\u7d93\u5143\u9023\u63a5\u6027\u7684\u6280\u8853\uff0c\u4ee5\u63d0\u9ad8\u6e96\u78ba\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\u4f86\u6539\u5584\u5176\u53ef\u64f4\u5145\u6027\u3002\u6211\u5011\u5728 MNIST\u3001Jet \u5b50\u7d50\u69cb\u5206\u985e\u548c\u7db2\u8def\u5165\u4fb5\u5075\u6e2c\u57fa\u6e96\u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u5be6\u4f5c\uff0c\u767c\u73fe\u5c0d\u65bc\u985e\u4f3c\u7684\u6e96\u78ba\u5ea6\uff0cPolyLUT-Add \u53ef\u5c07 LUT \u6e1b\u5c11 $1.3-7.7\\times$\uff0c\u800c\u5ef6\u9072\u6e1b\u5c11 $1.2-2.2\\times$\u3002", "author": "Binglei Lou et.al.", "authors": "Binglei Lou, Richard Rademacher, David Boland, Philip H. W. Leong", "id": "2406.04910v1", "paper_url": "http://arxiv.org/abs/2406.04910v1", "repo": "https://github.com/bingleilou/PolyLUT-Add"}}