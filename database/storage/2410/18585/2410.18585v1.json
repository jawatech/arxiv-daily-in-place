{"2410.18585": {"publish_time": "2024-10-24", "title": "Aligning CodeLLMs with Direct Preference Optimization", "paper_summary": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval.", "paper_summary_zh": "\u53bb\u5e74\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u500b\u9818\u57df\u90fd\u53d6\u5f97\u4e86\u5feb\u901f\u9032\u5c55\u3002\u5176\u4e2d\uff0cCodeLLM \u7279\u5225\u53d7\u5230\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5011\u4e0d\u50c5\u53ef\u4ee5\u5354\u52a9\u5b8c\u6210\u5404\u7a2e\u7a0b\u5f0f\u8a2d\u8a08\u4efb\u52d9\uff0c\u9084\u53ef\u4ee5\u4ee3\u8868 LLM \u7684\u6c7a\u7b56\u5236\u5b9a\u548c\u908f\u8f2f\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684 CodeLLM \u4e3b\u8981\u5c08\u6ce8\u65bc\u9810\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf\u5834\u666f\uff0c\u800c\u5c0d\u8a13\u7df4\u5f8c LLM \u5f88\u91cd\u8981\u7684\u5c0d\u9f4a\u968e\u6bb5\u5247\u63a2\u7d22\u4e0d\u8db3\u3002\u9019\u9805\u5de5\u4f5c\u9996\u5148\u767c\u73fe\uff0c\u5e38\u7528\u7684 PPO \u6f14\u7b97\u6cd5\u53ef\u80fd\u4e0d\u9069\u5408 CodeLLM \u7684\u5c0d\u9f4a\uff0c\u56e0\u70ba\u6240\u6d89\u53ca\u7684\u734e\u52f5\u898f\u5247\u901a\u5e38\u662f\u7c97\u7565\u7684\uff0c\u800c\u4e14\u53ef\u80fd\u5b58\u5728\u7f3a\u9677\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u5021\u4f7f\u7528 DPO \u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002DPO \u50c5\u57fa\u65bc\u504f\u597d\u8cc7\u6599\u5c0d\uff0c\u5c31\u53ef\u4ee5\u81ea\u52d5\u5448\u73fe\u6a21\u578b\u6392\u540d\u8cc7\u6599\uff0c\u5f9e\u800c\u7522\u751f\u6bd4\u4eba\u5de5\u5e72\u9810\u66f4\u7a69\u5065\u7684\u7d30\u7dfb\u734e\u52f5\u6a21\u5f0f\u3002\u6211\u5011\u9084\u70ba CodeLLM \u4e0a\u7684 DPO \u504f\u597d\u5c0d\u6536\u96c6\u63d0\u4f9b\u4e86\u4e00\u500b\u7ba1\u9053\u3002\u7814\u7a76\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u986f\u8457\u63d0\u5347\u4e86\u73fe\u6709 CodeLLM \u5728 MBPP \u548c HumanEval \u7b49\u57fa\u6e96\u4e0a\u7684\u6548\u80fd\u3002", "author": "Yibo Miao et.al.", "authors": "Yibo Miao, Bofei Gao, Shanghaoran Quan, Junyang Lin, Daoguang Zan, Jiaheng Liu, Jian Yang, Tianyu Liu, Zhijie Deng", "id": "2410.18585v1", "paper_url": "http://arxiv.org/abs/2410.18585v1", "repo": "null"}}