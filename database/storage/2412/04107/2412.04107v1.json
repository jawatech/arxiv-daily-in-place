{"2412.04107": {"publish_time": "2024-12-05", "title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models", "paper_summary": "Sequential recommendation (SR) aims to model the sequential dependencies in\nusers' historical interactions to better capture their evolving interests.\nHowever, existing SR approaches primarily rely on collaborative data, which\nleads to limitations such as the cold-start problem and sub-optimal\nperformance. Meanwhile, despite the success of large language models (LLMs),\ntheir application in industrial recommender systems is hindered by high\ninference latency, inability to capture all distribution statistics, and\ncatastrophic forgetting. To this end, we propose a novel Pre-train, Align, and\nDisentangle (PAD) paradigm to empower recommendation models with LLMs.\nSpecifically, we first pre-train both the SR and LLM models to get\ncollaborative and textual embeddings. Next, a characteristic\nrecommendation-anchored alignment loss is proposed using multi-kernel maximum\nmean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,\nconsisting aligned and modality-specific experts with disentangled embeddings,\nis fine-tuned in a frequency-aware manner. Experiments conducted on three\npublic datasets demonstrate the effectiveness of PAD, showing significant\nimprovements and compatibility with various SR backbone models, especially on\ncold items. The implementation code and datasets will be publicly available.", "paper_summary_zh": "\u5e8f\u5217\u63a8\u85a6 (SR) \u7684\u76ee\u6a19\u662f\u6a21\u64ec\u4f7f\u7528\u8005\u6b77\u53f2\u4e92\u52d5\u4e2d\u7684\u9806\u5e8f\u4f9d\u8cf4\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4ed6\u5011\u4e0d\u65b7\u8b8a\u5316\u7684\u8208\u8da3\u3002\n\u7136\u800c\uff0c\u73fe\u6709\u7684 SR \u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u5354\u4f5c\u8cc7\u6599\uff0c\u9019\u6703\u5c0e\u81f4\u51b7\u555f\u52d5\u554f\u984c\u548c\u6b21\u4f73\u6548\u80fd\u7b49\u9650\u5236\u3002\u540c\u6642\uff0c\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f88\u6210\u529f\uff0c\u4f46\u5b83\u5011\u5728\u7522\u696d\u63a8\u85a6\u7cfb\u7d71\u4e2d\u7684\u61c9\u7528\u53d7\u5230\u9ad8\u63a8\u8ad6\u5ef6\u9072\u3001\u7121\u6cd5\u6355\u6349\u6240\u6709\u5206\u4f48\u7d71\u8a08\u8cc7\u6599\u548c\u707d\u96e3\u6027\u907a\u5fd8\u7684\u963b\u7919\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u9810\u8a13\u7df4\u3001\u5c0d\u9f4a\u548c\u89e3\u958b (PAD) \u7bc4\u4f8b\uff0c\u4ee5 LLM \u8ce6\u80fd\u63a8\u85a6\u6a21\u578b\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u9810\u8a13\u7df4 SR \u548c LLM \u6a21\u578b\uff0c\u4ee5\u53d6\u5f97\u5354\u4f5c\u548c\u6587\u5b57\u5d4c\u5165\u3002\u63a5\u4e0b\u4f86\uff0c\u5efa\u8b70\u4f7f\u7528\u5177\u6709\u9ad8\u65af\u6838\u7684\u591a\u6838\u6700\u5927\u5e73\u5747\u5dee\u7570\uff0c\u63d0\u51fa\u4e00\u500b\u7279\u5fb5\u63a8\u85a6\u9328\u5b9a\u5c0d\u9f4a\u640d\u5931\u3002\u6700\u5f8c\uff0c\u4e00\u500b\u7531\u5c0d\u9f4a\u548c\u5177\u6709\u89e3\u958b\u5d4c\u5165\u7684\u7279\u5b9a\u6a21\u5f0f\u5c08\u5bb6\u7d44\u6210\u7684\u4e09\u91cd\u5c08\u5bb6\u67b6\u69cb\uff0c\u4ee5\u983b\u7387\u611f\u77e5\u7684\u65b9\u5f0f\u9032\u884c\u5fae\u8abf\u3002\u5728\u4e09\u500b\u516c\u958b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86 PAD \u7684\u6709\u6548\u6027\uff0c\u986f\u793a\u51fa\u986f\u8457\u7684\u6539\u9032\uff0c\u4e26\u8207\u5404\u7a2e SR \u4e3b\u5e79\u6a21\u578b\u76f8\u5bb9\uff0c\u7279\u5225\u662f\u5728\u51b7\u9580\u9805\u76ee\u4e0a\u3002\u5be6\u4f5c\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Yuhao Wang et.al.", "authors": "Yuhao Wang, Junwei Pan, Xiangyu Zhao, Pengyue Jia, Wanyu Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang", "id": "2412.04107v1", "paper_url": "http://arxiv.org/abs/2412.04107v1", "repo": "null"}}