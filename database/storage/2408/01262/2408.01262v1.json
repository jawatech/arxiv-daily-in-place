{"2408.01262": {"publish_time": "2024-08-02", "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework", "paper_summary": "Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.", "paper_summary_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u5df2\u8bc1\u660e\u5176\u5728\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5e7b\u89c9\u65b9\u9762\u7684\u4f18\u52bf\u3002\u73b0\u6709\u7684 RAG \u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc4\u4f30 LLM \u662f\u5426\u53ef\u4ee5\u6b63\u786e\u56de\u7b54\u5e38\u8bc6\u3002\u7136\u800c\uff0c\u5b83\u4eec\u65e0\u6cd5\u8bc4\u4f30 RAG \u7cfb\u7edf\u5728\u5904\u7406\u6765\u81ea\u4e0d\u540c\u5782\u76f4\u9886\u57df\u7684\u7684\u6570\u636e\u65f6\u7684\u6709\u6548\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 RAGEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8bc4\u4f30\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u4e0d\u540c LLM \u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u4f7f\u7528\u80fd\u529b\u7684\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0cRAGEval \u4ece\u79cd\u5b50\u6587\u6863\u4e2d\u603b\u7ed3\u51fa\u4e00\u4e2a\u6a21\u5f0f\uff0c\u5e94\u7528\u914d\u7f6e\u6765\u751f\u6210\u4e0d\u540c\u7684\u6587\u6863\uff0c\u5e76\u6839\u636e\u6587\u7ae0\u548c\u914d\u7f6e\u6784\u5efa\u95ee\u7b54\u5bf9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u65b0\u9896\u7684\u6307\u6807\uff1a\u5b8c\u6574\u6027\u3001\u5e7b\u89c9\u548c\u4e0d\u76f8\u5173\u6027\uff0c\u4ee5\u4ed4\u7ec6\u8bc4\u4f30 LLM \u751f\u6210\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u5bf9\u5782\u76f4\u9886\u57df\u7684 RAG \u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0cRAGEval \u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30 LLM \u7684\u77e5\u8bc6\u4f7f\u7528\u80fd\u529b\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5728\u73b0\u6709\u95ee\u7b54\u6570\u636e\u96c6\u4e2d\u7684\u56de\u7b54\u95ee\u9898\u65f6\u7684\u77e5\u8bc6\u6765\u6e90\u7684\u56f0\u60d1\u2014\u2014\u5b83\u662f\u5426\u6765\u81ea\u53c2\u6570\u5316\u8bb0\u5fc6\u6216\u68c0\u7d22\u3002", "author": "Kunlun Zhu et.al.", "authors": "Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun", "id": "2408.01262v1", "paper_url": "http://arxiv.org/abs/2408.01262v1", "repo": "null"}}