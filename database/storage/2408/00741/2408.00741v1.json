{"2408.00741": {"publish_time": "2024-08-01", "title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency", "paper_summary": "The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.", "paper_summary_zh": "\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u53d1\u5c55\u548c\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f7f\u5176\u6210\u4e3a\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u5173\u952e\u5de5\u4f5c\u8d1f\u8f7d\u3002\u5982\u4eca\uff0cLLM \u63a8\u7406\u96c6\u7fa4\u4f1a\u6536\u5230\u5927\u91cf\u5177\u6709\u4e25\u683c\u670d\u52a1\u7ea7\u522b\u76ee\u6807 (SLO) \u7684\u67e5\u8be2\u3002\u4e3a\u4e86\u5b9e\u73b0\u6240\u9700\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8017\u7535\u7684 GPU \u4e0a\u6267\u884c\uff0c\u5bfc\u81f4\u63a8\u7406\u96c6\u7fa4\u6d88\u8017\u5927\u91cf\u7684\u80fd\u91cf\uff0c\u4ece\u800c\u5bfc\u81f4\u8fc7\u5ea6\u7684\u78b3\u6392\u653e\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u6709\u5f88\u5927\u7684\u673a\u4f1a\u5229\u7528\u63a8\u7406\u8ba1\u7b97\u7279\u6027\u548c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6ce2\u52a8\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u80fd\u6548\u3002\u7136\u800c\uff0c\u5982\u6b64\u591a\u6837\u5316\u548c\u52a8\u6001\u7684\u73af\u5883\u521b\u9020\u4e86\u4e00\u4e2a\u5de8\u5927\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u7cfb\u7edf\u914d\u7f6e\uff08\u4f8b\u5982\uff0c\u5b9e\u4f8b\u6570\u91cf\u3001\u6a21\u578b\u5e76\u884c\u6027\u548c GPU \u9891\u7387\uff09\u8f6c\u5316\u4e3a\u4e0d\u540c\u7684\u80fd\u6548\u6743\u8861\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DynamoLLM\uff0c\u8fd9\u662f LLM \u63a8\u7406\u73af\u5883\u7684\u7b2c\u4e00\u4e2a\u80fd\u6e90\u7ba1\u7406\u6846\u67b6\u3002DynamoLLM \u4f1a\u81ea\u52a8\u52a8\u6001\u5730\u91cd\u65b0\u914d\u7f6e\u63a8\u7406\u96c6\u7fa4\uff0c\u4ee5\u4f18\u5316 LLM \u670d\u52a1\u7684\u80fd\u6e90\u548c\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u670d\u52a1\u7684\u6027\u80fd SLO\u3002\u6211\u4eec\u8868\u660e\uff0c\u5728\u670d\u52a1\u7ea7\u522b\uff0cDynamoLLM \u8282\u7701\u4e86 53% \u7684\u80fd\u6e90\u548c 38% \u7684\u8fd0\u8425\u78b3\u6392\u653e\uff0c\u5e76\u4e3a\u5ba2\u6237\u8282\u7701\u4e86 61% \u7684\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u5ef6\u8fdf SLO\u3002", "author": "Jovan Stojkovic et.al.", "authors": "Jovan Stojkovic, Chaojie Zhang, \u00cd\u00f1igo Goiri, Josep Torrellas, Esha Choukse", "id": "2408.00741v1", "paper_url": "http://arxiv.org/abs/2408.00741v1", "repo": "null"}}