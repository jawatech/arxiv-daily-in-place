
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-29**|**SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners**|Ziyu Guo et.al.|[2408.16768v1](http://arxiv.org/abs/2408.16768v1)|[link](https://github.com/ziyuguo99/sam2point)|
|**2024-08-29**|**ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model**|Fangfu Liu et.al.|[2408.16767v1](http://arxiv.org/abs/2408.16767v1)|null|
|**2024-08-29**|**A Score-Based Density Formula, with Applications in Diffusion Generative Models**|Gen Li et.al.|[2408.16765v1](http://arxiv.org/abs/2408.16765v1)|null|
|**2024-08-29**|**Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks**|Hongjun Wang et.al.|[2408.16757v2](http://arxiv.org/abs/2408.16757v2)|[link](https://github.com/visual-ai/dissect-ood-osr)|
|**2024-08-29**|**How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models**|Jiyue Jiang et.al.|[2408.16756v1](http://arxiv.org/abs/2408.16756v1)|null|
|**2024-08-29**|**Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models**|Alec Solway et.al.|[2408.16753v1](http://arxiv.org/abs/2408.16753v1)|null|
|**2024-08-29**|**A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models**|Yi-Lin Tuan et.al.|[2408.16751v1](http://arxiv.org/abs/2408.16751v1)|null|
|**2024-08-29**|**Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge**|Beidi Dong et.al.|[2408.16749v1](http://arxiv.org/abs/2408.16749v1)|null|
|**2024-08-29**|**Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models**|Jiří Milička et.al.|[2408.16740v1](http://arxiv.org/abs/2408.16740v1)|null|
|**2024-08-29**|**Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling**|Hritik Bansal et.al.|[2408.16737v1](http://arxiv.org/abs/2408.16737v1)|null|
|**2024-08-29**|**Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming**|Zhifei Xie et.al.|[2408.16725v2](http://arxiv.org/abs/2408.16725v2)|[link](https://github.com/gpt-omni/mini-omni)|
|**2024-08-29**|**A GREAT Architecture for Edge-Based Graph Problems Like TSP**|Attila Lischka et.al.|[2408.16717v1](http://arxiv.org/abs/2408.16717v1)|null|
|**2024-08-29**|**Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever**|Rohan Jha et.al.|[2408.16672v1](http://arxiv.org/abs/2408.16672v1)|null|
|**2024-08-29**|**Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity**|Ziniu Li et.al.|[2408.16673v1](http://arxiv.org/abs/2408.16673v1)|null|
|**2024-08-29**|**Iterative Graph Alignment**|Fangyuan Yu et.al.|[2408.16667v1](http://arxiv.org/abs/2408.16667v1)|null|
|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|Yongjie Fu et.al.|[2408.16647v1](http://arxiv.org/abs/2408.16647v1)|null|
|**2024-08-29**|**RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model**|Zhuan Shi et.al.|[2408.16634v1](http://arxiv.org/abs/2408.16634v1)|null|
|**2024-08-29**|**Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning**|Keqin Li et.al.|[2408.16633v1](http://arxiv.org/abs/2408.16633v1)|null|
|**2024-08-29**|**Maelstrom Networks**|Matthew Evanusa et.al.|[2408.16632v1](http://arxiv.org/abs/2408.16632v1)|null|
|**2024-08-29**|**LLMs generate structurally realistic social networks but overestimate political homophily**|Serina Chang et.al.|[2408.16629v1](http://arxiv.org/abs/2408.16629v1)|[link](https://github.com/snap-stanford/llm-social-network)|
|**2024-08-29**|**Towards Infusing Auxiliary Knowledge for Distracted Driver Detection**|Ishwar B Balappanawar et.al.|[2408.16621v1](http://arxiv.org/abs/2408.16621v1)|[link](https://github.com/ishwarbb/kid3)|
|**2024-08-29**|**Hyperdimensional Vector Tsetlin Machines with Applications to Sequence Learning and Generation**|Christian D. Blakely et.al.|[2408.16620v1](http://arxiv.org/abs/2408.16620v1)|null|
|**2024-08-29**|**Examination of Code generated by Large Language Models**|Robin Beer et.al.|[2408.16601v1](http://arxiv.org/abs/2408.16601v1)|[link](https://github.com/t-muras/ai-code-analysis)|
|**2024-08-29**|**Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies**|Zhiyang Qi et.al.|[2408.16586v1](http://arxiv.org/abs/2408.16586v1)|null|
|**2024-08-29**|**Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning**|Boyu Chen et.al.|[2408.16577v1](http://arxiv.org/abs/2408.16577v1)|null|
|**2024-08-29**|**Predictability maximization and the origins of word order harmony**|Ramon Ferrer-i-Cancho et.al.|[2408.16570v1](http://arxiv.org/abs/2408.16570v1)|null|
|**2024-08-29**|**SALSA: Speedy ASR-LLM Synchronous Aggregation**|Ashish Mittal et.al.|[2408.16542v1](http://arxiv.org/abs/2408.16542v1)|null|
|**2024-08-29**|**SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks**|Xing Ai et.al.|[2408.16537v1](http://arxiv.org/abs/2408.16537v1)|null|
|**2024-08-29**|**CNIMA: A Universal Evaluation Framework and Automated Approach for Assessing Second Language Dialogues**|Rena Gao et.al.|[2408.16518v1](http://arxiv.org/abs/2408.16518v1)|null|
|**2024-08-29**|**Adaptive Variational Continual Learning via Task-Heuristic Modelling**|Fan Yang et.al.|[2408.16517v1](http://arxiv.org/abs/2408.16517v1)|null|
|**2024-08-29**|**LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?**|Jan Cegin et.al.|[2408.16502v1](http://arxiv.org/abs/2408.16502v1)|null|
|**2024-08-29**|**On-device AI: Quantization-aware Training of Transformers in Time-Series**|Tianheng Ling et.al.|[2408.16495v1](http://arxiv.org/abs/2408.16495v1)|null|
|**2024-08-29**|**Learning from Negative Samples in Generative Biomedical Entity Linking**|Chanhwi Kim et.al.|[2408.16493v1](http://arxiv.org/abs/2408.16493v1)|[link](https://github.com/dmis-lab/angel)|
|**2024-08-29**|**Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning**|Rochelle Choenni et.al.|[2408.16482v1](http://arxiv.org/abs/2408.16482v1)|null|
|**2024-08-29**|**Is text normalization relevant for classifying medieval charters?**|Florian Atzenhofer-Baumgartner et.al.|[2408.16446v1](http://arxiv.org/abs/2408.16446v1)|null|
|**2024-08-29**|**Integrating Features for Recognizing Human Activities through Optimized Parameters in Graph Convolutional Networks and Transformer Architectures**|Mohammad Belal et.al.|[2408.16442v1](http://arxiv.org/abs/2408.16442v1)|null|
|**2024-08-29**|**Instruction-tuned Large Language Models for Machine Translation in the Medical Domain**|Miguel Rios et.al.|[2408.16440v1](http://arxiv.org/abs/2408.16440v1)|null|
|**2024-08-29**|**Gradient-free variational learning with conditional mixture networks**|Conor Heins et.al.|[2408.16429v1](http://arxiv.org/abs/2408.16429v1)|[link](https://github.com/versestech/cavi-cmn)|
|**2024-08-29**|**COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation**|Jiefeng Li et.al.|[2408.16426v1](http://arxiv.org/abs/2408.16426v1)|null|
|**2024-08-29**|**MQM-Chat: Multidimensional Quality Metrics for Chat Translation**|Yunmeng Li et.al.|[2408.16390v1](http://arxiv.org/abs/2408.16390v1)|null|
|**2024-08-29**|**DetectBERT: Towards Full App-Level Representation Learning to Detect Android Malware**|Tiezhu Sun et.al.|[2408.16353v1](http://arxiv.org/abs/2408.16353v1)|[link](https://github.com/trustworthy-software/detectbert)|
|**2024-08-29**|**The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization**|Luka Borec et.al.|[2408.16345v1](http://arxiv.org/abs/2408.16345v1)|[link](https://github.com/lukaborec/memorization-nucleus-sampling)|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Self-Improving Diffusion Models with Synthetic Data**|Sina Alemohammad et.al.|[2408.16333v1](http://arxiv.org/abs/2408.16333v1)|null|
|**2024-08-29**|**Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic**|Xin Zheng et.al.|[2408.16326v1](http://arxiv.org/abs/2408.16326v1)|null|
|**2024-08-29**|**FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm Based On FMDS and AGMF Modules**|Yukang Huo et.al.|[2408.16313v1](http://arxiv.org/abs/2408.16313v1)|null|
|**2024-08-29**|**Safe Bayesian Optimization for High-Dimensional Control Systems via Additive Gaussian Processes**|Hongxuan Wang et.al.|[2408.16307v1](http://arxiv.org/abs/2408.16307v1)|null|
|**2024-08-29**|**Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems**|Tian Ye et.al.|[2408.16293v1](http://arxiv.org/abs/2408.16293v1)|null|
|**2024-08-29**|**Measuring the Accuracy of Automatic Speech Recognition Solutions**|Korbinian Kuhn et.al.|[2408.16287v1](http://arxiv.org/abs/2408.16287v1)|[link](https://github.com/shuffle-project/asr-comparison)|
|**2024-08-29**|**Enhancing AI-Driven Psychological Consultation: Layered Prompts with Large Language Models**|Rafael Souza et.al.|[2408.16276v1](http://arxiv.org/abs/2408.16276v1)|null|
|**2024-08-29**|**Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding**|Kaijing Ma et.al.|[2408.16272v1](http://arxiv.org/abs/2408.16272v1)|null|
|**2024-08-29**|**LoraMap: Harnessing the Power of LoRA Connections**|Hyeryun Park et.al.|[2408.16264v1](http://arxiv.org/abs/2408.16264v1)|null|
|**2024-08-29**|**Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models**|Sekitoshi Kanai et.al.|[2408.16261v1](http://arxiv.org/abs/2408.16261v1)|null|
|**2024-08-29**|**Making the Most of your Model: Methods for Finetuning and Applying Pretrained Transformers**|Davis Yoshida et.al.|[2408.16241v1](http://arxiv.org/abs/2408.16241v1)|null|
|**2024-08-29**|**Enhancing Conditional Image Generation with Explainable Latent Space Manipulation**|Kshitij Pathania et.al.|[2408.16232v1](http://arxiv.org/abs/2408.16232v1)|[link](https://github.com/kshitij79/CS-7476-Improvements-in-Diffusion-Model)|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|Jingyi Wang et.al.|[2408.16224v2](http://arxiv.org/abs/2408.16224v2)|null|
|**2024-08-29**|**SSDM: Scalable Speech Dysfluency Modeling**|Jiachen Lian et.al.|[2408.16221v1](http://arxiv.org/abs/2408.16221v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-29**|**From cart to truck: meaning shift through words in English in the last two centuries**|Esteban Rodríguez Betancourt et.al.|[2408.16209v1](http://arxiv.org/abs/2408.16209v1)|null|
|**2024-08-29**|**ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology Report Generation Metrics**|Oishi Banerjee et.al.|[2408.16208v1](http://arxiv.org/abs/2408.16208v1)|null|
|**2024-08-29**|**Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey**|Qi Dong et.al.|[2408.16202v1](http://arxiv.org/abs/2408.16202v1)|null|
|**2024-08-29**|**Benchmarking Japanese Speech Recognition on ASR-LLM Setups with Multi-Pass Augmented Generative Error Correction**|Yuka Ko et.al.|[2408.16180v1](http://arxiv.org/abs/2408.16180v1)|null|
|**2024-08-28**|**LLM-assisted Labeling Function Generation for Semantic Type Detection**|Chenjie Li et.al.|[2408.16173v1](http://arxiv.org/abs/2408.16173v1)|null|
|**2024-08-28**|**FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench**|Aman Priyanshu et.al.|[2408.16163v1](http://arxiv.org/abs/2408.16163v1)|null|
|**2024-08-28**|**Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation**|Ke Chen et.al.|[2408.16126v1](http://arxiv.org/abs/2408.16126v1)|null|
|**2024-08-28**|**Data Formulator 2: Iteratively Creating Rich Visualizations with AI**|Chenglong Wang et.al.|[2408.16119v1](http://arxiv.org/abs/2408.16119v1)|[link](https://github.com/microsoft/data-formulator)|
|**2024-08-28**|**Structured Event Reasoning with Large Language Models**|Li Zhang et.al.|[2408.16098v1](http://arxiv.org/abs/2408.16098v1)|null|
|**2024-08-28**|**Logic-Enhanced Language Model Agents for Trustworthy Social Simulations**|Agnieszka Mensfelt et.al.|[2408.16081v1](http://arxiv.org/abs/2408.16081v1)|[link](https://github.com/dicelab-rhul/lelma)|
|**2024-08-28**|**Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings**|Leo Yeykelis et.al.|[2408.16073v1](http://arxiv.org/abs/2408.16073v1)|null|
|**2024-08-28**|**Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders**|Min Shi et.al.|[2408.15998v1](http://arxiv.org/abs/2408.15998v1)|[link](https://github.com/nvlabs/eagle)|
|**2024-08-28**|**Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need**|Sijia Peng et.al.|[2408.15997v1](http://arxiv.org/abs/2408.15997v1)|[link](https://github.com/lunaaa95/mou)|
|**2024-08-28**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|Wei-Jhe Huang et.al.|[2408.15996v2](http://arxiv.org/abs/2408.15996v2)|null|
|**2024-08-28**|**CoGen: Learning from Feedback with Coupled Comprehension and Generation**|Mustafa Omer Gul et.al.|[2408.15992v1](http://arxiv.org/abs/2408.15992v1)|[link](https://github.com/lil-lab/cogen)|
|**2024-08-28**|**In-Context Imitation Learning via Next-Token Prediction**|Letian Fu et.al.|[2408.15980v1](http://arxiv.org/abs/2408.15980v1)|null|
|**2024-08-28**|**WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration**|Yao Zhang et.al.|[2408.15978v1](http://arxiv.org/abs/2408.15978v1)|null|
|**2024-08-28**|**BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems**|Wei Wang et.al.|[2408.15971v1](http://arxiv.org/abs/2408.15971v1)|null|
|**2024-08-28**|**More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding**|Yuan Tang et.al.|[2408.15966v1](http://arxiv.org/abs/2408.15966v1)|null|
|**2024-08-28**|**Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games**|Nicholas R. Waytowich et.al.|[2408.15950v1](http://arxiv.org/abs/2408.15950v1)|null|
|**2024-08-28**|**Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning**|Bingchen Yan et.al.|[2408.15924v1](http://arxiv.org/abs/2408.15924v1)|null|
|**2024-08-28**|**Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models**|Yuncheng Yang et.al.|[2408.15915v1](http://arxiv.org/abs/2408.15915v1)|null|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903v1](http://arxiv.org/abs/2408.15903v1)|null|
|**2024-08-28**|**Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts**|Nikolas Gritsch et.al.|[2408.15901v1](http://arxiv.org/abs/2408.15901v1)|null|
|**2024-08-28**|**Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation**|Reid Graves et.al.|[2408.15898v1](http://arxiv.org/abs/2408.15898v1)|null|
|**2024-08-28**|**A New Method for Cross-Lingual-based Semantic Role Labeling**|Mohammad Ebrahimi et.al.|[2408.15896v1](http://arxiv.org/abs/2408.15896v1)|null|
|**2024-08-28**|**Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models**|Sebastian Vallejo Vera et.al.|[2408.15895v1](http://arxiv.org/abs/2408.15895v1)|null|
|**2024-08-28**|**Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks**|Amar Amouri et.al.|[2408.15886v2](http://arxiv.org/abs/2408.15886v2)|null|
|**2024-08-28**|**Persuasion Games using Large Language Models**|Ganesh Prasath Ramani et.al.|[2408.15879v1](http://arxiv.org/abs/2408.15879v1)|null|
|**2024-08-28**|**GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model**|Yongjie Fu et.al.|[2408.15868v1](http://arxiv.org/abs/2408.15868v1)|null|
|**2024-08-28**|**Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection**|Sagar Srinivas Sakhinana et.al.|[2408.15866v1](http://arxiv.org/abs/2408.15866v1)|null|
|**2024-08-28**|**Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature**|Uri Katz et.al.|[2408.15836v1](http://arxiv.org/abs/2408.15836v1)|null|
|**2024-08-28**|**Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification**|Abu Adnan Sadi et.al.|[2408.15827v1](http://arxiv.org/abs/2408.15827v1)|null|
|**2024-08-28**|**Object Detection for Vehicle Dashcams using Transformers**|Osama Mustafa et.al.|[2408.15809v1](http://arxiv.org/abs/2408.15809v1)|null|
|**2024-08-28**|**ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation**|Tiantian Feng et.al.|[2408.15803v1](http://arxiv.org/abs/2408.15803v1)|null|
|**2024-08-28**|**Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization**|Léo Hemamou et.al.|[2408.15801v1](http://arxiv.org/abs/2408.15801v1)|null|
|**2024-08-28**|**Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing**|Kenneth Stewart et.al.|[2408.15800v1](http://arxiv.org/abs/2408.15800v1)|[link](https://github.com/nmi-lab/snn_maml)|
|**2024-08-28**|**Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models**|Hédi Zhegidi et.al.|[2408.15796v1](http://arxiv.org/abs/2408.15796v1)|[link](https://github.com/geode-project/ner-llm)|
|**2024-08-28**|**Language Adaptation on a Tight Academic Compute Budget: Tokenizer Swapping Works and Pure bfloat16 Is Enough**|Konstantin Dobler et.al.|[2408.15793v1](http://arxiv.org/abs/2408.15793v1)|[link](https://github.com/konstantinjdobler/tight-budget-llm-adaptation)|
|**2024-08-28**|**Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions**|Huachuan Qiu et.al.|[2408.15787v1](http://arxiv.org/abs/2408.15787v1)|[link](https://github.com/qiuhuachuan/interactive-agents)|
|**2024-08-28**|**LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models**|Jiayi Gui et.al.|[2408.15778v1](http://arxiv.org/abs/2408.15778v1)|null|
|**2024-08-28**|**Easy, Interpretable, Effective: openSMILE for voice deepfake detection**|Octavian Pascu et.al.|[2408.15775v2](http://arxiv.org/abs/2408.15775v2)|null|

#### Abstracts
##### **SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners**
2408.16768v1 by Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng

We introduce SAM2Point, a preliminary exploration adapting Segment Anything
Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point
interprets any 3D data as a series of multi-directional videos, and leverages
SAM 2 for 3D-space segmentation, without further training or 2D-3D projection.
Our framework supports various prompt types, including 3D points, boxes, and
masks, and can generalize across diverse scenarios, such as 3D objects, indoor
scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple
3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight
the robust generalization capabilities of SAM2Point. To our best knowledge, we
present the most faithful implementation of SAM in 3D, which may serve as a
starting point for future research in promptable 3D segmentation. Online Demo:
https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:
https://github.com/ZiyuGuo99/SAM2Point .

摘要：我們介紹 SAM2Point，這是一種初步探索，將 Segment Anything Model 2 (SAM 2) 改編為零次學習和可提示的 3D 分割。SAM2Point 將任何 3D 資料詮釋為一系列多方向影片，並利用 SAM 2 進行 3D 空間分割，而無需進一步訓練或 2D-3D 投影。我們的架構支援各種提示類型，包括 3D 點、方塊和遮罩，並且可以在不同的場景中泛化，例如 3D 物件、室內場景、戶外環境和原始稀疏 LiDAR。在多個 3D 資料集上的示範，例如 Objaverse、S3DIS、ScanNet、Semantic3D 和 KITTI，突出了 SAM2Point 的強大泛化能力。據我們所知，我們展示了 SAM 在 3D 中最忠實的實作，這可以作為未來可提示 3D 分割研究的起點。線上示範：https://huggingface.co/spaces/ZiyuG/SAM2Point。程式碼：https://github.com/ZiyuGuo99/SAM2Point。

##### **ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model**
2408.16767v1 by Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan

Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.

摘要：3D 場景重建的進展已將現實世界的 2D 影像轉換為 3D 模型，從數百張輸入照片產生逼真的 3D 結果。儘管在密集視圖重建場景中取得巨大成功，但從不足的擷取視圖中渲染出詳細場景仍然是一個欠約束的最佳化問題，通常會在未見區域中產生偽像和失真。在本文中，我們提出 ReconX，這是一個新穎的 3D 場景重建範例，將模稜兩可的重建挑戰重新定義為一個時間生成任務。關鍵見解是釋放預先訓練好的影片擴散模型的強大生成先驗，以進行稀疏視圖重建。然而，3D 視圖一致性難以直接從預先訓練好的模型中生成的影片格中準確保留。為了解決這個問題，在給定有限的輸入視圖下，提議的 ReconX 首先建構一個全域點雲，並將其編碼到一個脈絡空間中作為 3D 結構條件。在條件的引導下，影片擴散模型接著合成既保留細節又展現高度 3D 一致性的影片格，確保場景從各種角度的一致性。最後，我們透過一個具有信心感知的 3D 高斯噴繪最佳化架構，從生成的影片中恢復 3D 場景。在各種真實世界資料集上的大量實驗顯示，我們的 ReconX 在品質和概括性方面優於最先進的方法。

##### **A Score-Based Density Formula, with Applications in Diffusion Generative Models**
2408.16765v1 by Gen Li, Yuling Yan

Score-based generative models (SGMs) have revolutionized the field of
generative modeling, achieving unprecedented success in generating realistic
and diverse content. Despite empirical advances, the theoretical basis for why
optimizing the evidence lower bound (ELBO) on the log-likelihood is effective
for training diffusion generative models, such as DDPMs, remains largely
unexplored. In this paper, we address this question by establishing a density
formula for a continuous-time diffusion process, which can be viewed as the
continuous-time limit of the forward process in an SGM. This formula reveals
the connection between the target density and the score function associated
with each step of the forward process. Building on this, we demonstrate that
the minimizer of the optimization objective for training DDPMs nearly coincides
with that of the true objective, providing a theoretical foundation for
optimizing DDPMs using the ELBO. Furthermore, we offer new insights into the
role of score-matching regularization in training GANs, the use of ELBO in
diffusion classifiers, and the recently proposed diffusion loss.

摘要：基於分數的生成模型 (SGM) 已徹底改變生成模型領域，在生成逼真且多樣化的內容方面取得了前所未有的成功。儘管有經驗上的進步，但優化對數似然上的證據下界 (ELBO) 對於訓練擴散生成模型（例如 DDPM）有效的原因，其理論基礎在很大程度上仍未得到探索。在本文中，我們通過建立連續時間擴散過程的密度公式來解決這個問題，該公式可以視為 SGM 中前向過程的連續時間極限。這個公式揭示了目標密度與與前向過程的每一步相關的分數函數之間的聯繫。在此基礎上，我們證明了訓練 DDPM 的優化目標的極小值幾乎與真實目標的極小值一致，為使用 ELBO 優化 DDPM 提供了理論基礎。此外，我們對生成對抗網路 (GAN) 訓練中分數匹配正則化的作用、擴散分類器中 ELBO 的使用以及最近提出的擴散損失提供了新的見解。

##### **Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks**
2408.16757v2 by Hongjun Wang, Sagar Vaze, Kai Han

Detecting test-time distribution shift has emerged as a key capability for
safely deployed machine learning models, with the question being tackled under
various guises in recent years. In this paper, we aim to provide a consolidated
view of the two largest sub-fields within the community: out-of-distribution
(OOD) detection and open-set recognition (OSR). In particular, we aim to
provide rigorous empirical analysis of different methods across settings and
provide actionable takeaways for practitioners and researchers. Concretely, we
make the following contributions: (i) We perform rigorous cross-evaluation
between state-of-the-art methods in the OOD detection and OSR settings and
identify a strong correlation between the performances of methods for them;
(ii) We propose a new, large-scale benchmark setting which we suggest better
disentangles the problem tackled by OOD detection and OSR, re-evaluating
state-of-the-art OOD detection and OSR methods in this setting; (iii) We
surprisingly find that the best performing method on standard benchmarks
(Outlier Exposure) struggles when tested at scale, while scoring rules which
are sensitive to the deep feature magnitude consistently show promise; and (iv)
We conduct empirical analysis to explain these phenomena and highlight
directions for future research. Code:
https://github.com/Visual-AI/Dissect-OOD-OSR

摘要：<paragraph>偵測測試時間的分配轉移已成為安全部署機器學習模型的一項關鍵能力，這個問題在近年來以各種形式被解決。在本文中，我們旨在提供社群中兩個最大的子領域的統合觀點：分佈外 (OOD) 偵測和開放式識別 (OSR)。特別是，我們旨在提供在不同設定中對不同方法進行嚴謹的實證分析，並為實務工作者和研究人員提供可行的外賣。具體來說，我們做出以下貢獻：(i) 我們在 OOD 偵測和 OSR 設定中對最先進的方法進行嚴謹的交叉評估，並找出它們方法的效能之間有很強的相關性；(ii) 我們提出一個新的、大規模的基準設定，我們建議它能更好地解開 OOD 偵測和 OSR 所解決的問題，重新評估在此設定中的最先進的 OOD 偵測和 OSR 方法；(iii) 我們驚訝地發現，在標準基準上表現最好的方法（異常值暴露）在擴大規模時會遇到困難，而對深度特徵量大小敏感的計分規則則持續表現出前景；以及 (iv) 我們進行實證分析來解釋這些現象，並重點說明未來研究的方向。程式碼：
https://github.com/Visual-AI/Dissect-OOD-OSR</paragraph>

##### **How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models**
2408.16756v1 by Jiyue Jiang, Liheng Chen, Pengan Chen, Sheng Wang, Qinghang Bao, Lingpeng Kong, Yu Li, Chuan Wu

The rapid evolution of large language models (LLMs) has transformed the
competitive landscape in natural language processing (NLP), particularly for
English and other data-rich languages. However, underrepresented languages like
Cantonese, spoken by over 85 million people, face significant development gaps,
which is particularly concerning given the economic significance of the
Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial
Cantonese-speaking populations in places like Singapore and North America.
Despite its wide use, Cantonese has scant representation in NLP research,
especially compared to other languages from similarly developed regions. To
bridge these gaps, we outline current Cantonese NLP methods and introduce new
benchmarks designed to evaluate LLM performance in factual generation,
mathematical logic, complex reasoning, and general knowledge in Cantonese,
which aim to advance open-source Cantonese LLM technology. We also propose
future research directions and recommended models to enhance Cantonese LLM
development.

摘要：大型語言模型 (LLM) 的快速演進已經改變了自然語言處理 (NLP) 的競爭格局，特別是對於英語和其他資料豐富的語言。然而，像廣東話這樣被低估的語言，擁有超過 8500 萬人口使用，卻面臨著重大的發展差距，這特別令人擔憂，因為粵港澳大灣區的經濟重要性，以及像新加坡和北美這樣擁有大量廣東話人口的地方。儘管廣東話被廣泛使用，但在 NLP 研究中卻鮮有代表性，特別是與來自類似發達地區的其他語言相比。為了彌合這些差距，我們概述了目前的廣東話 NLP 方法，並引入了新的基準，旨在評估 LLM 在廣東話的事實生成、數學邏輯、複雜推理和一般知識方面的表現，目標是推進開源廣東話 LLM 技術。我們還提出了未來的研究方向和建議的模型，以增強廣東話 LLM 的發展。

##### **Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models**
2408.16753v1 by Alec Solway

Reinforcement learning is used to align language models with human preference
signals after first pre-training the model to predict the next token of text
within a large corpus using likelihood maximization. Before being deployed in a
specific domain, models are often further fine-tuned on task specific data.
Since human preferences are often unavailable for the last step, it is
performed using likelihood maximization as that is the typical default method.
However, reinforcement learning has other advantages besides facilitating
alignment to a human derived reward function. For one, whereas likelihood
maximization is a form of imitation learning in which the model is trained on
what to do under ideal conditions, reinforcement learning is not limited to
demonstrating actions just for optimally reached states and trains a model what
to do under a range of scenarios as it explores the policy space. In addition,
it also trains a model what not to do, suppressing competitive but poor
actions. This work develops a framework for last-mile fine-tuning using
reinforcement learning and tests whether it garners performance gains. The
experiments center on abstractive summarization, but the framework is general
and broadly applicable. Use of the procedure produced significantly better
results than likelihood maximization when comparing raw predictions. For the
specific data tested, the gap could be bridged by employing post-processing of
the maximum likelihood outputs. Nonetheless, the framework offers a new avenue
for model optimization in situations where post-processing may be less
straightforward or effective, and it can be extended to include more complex
classes of undesirable outputs to penalize and train against, such as
hallucinations.

摘要：強化學習被用於在使用似然最大化預測語料中下一個文本符號後，將語言模型與人類偏好信號對齊。在特定領域部署之前，模型通常會進一步針對特定任務的數據進行微調。由於在最後一步中通常無法獲得人類偏好，因此使用似然最大化進行，這是典型的默認方法。然而，強化學習除了促進與人類衍生的獎勵函數對齊之外，還有其他優點。首先，儘管似然最大化是一種模仿學習形式，其中模型在理想條件下訓練要執行什麼操作，但強化學習不僅限於展示僅針對最佳狀態達成的動作，並訓練模型在探索策略空間時在各種場景下要執行什麼操作。此外，它還訓練模型不要執行什麼操作，抑制有競爭力但效果不佳的動作。這項工作開發了一個使用強化學習進行最後一英里微調的框架，並測試它是否能獲得性能提升。實驗集中在抽象摘要上，但該框架是通用的，廣泛適用的。與比較原始預測時，使用該程序產生的結果顯著優於似然最大化。對於測試的具體數據，可以使用最大似然輸出後處理來彌合差距。儘管如此，該框架為模型優化提供了一個新的途徑，在這種情況下，後處理可能不太直接或有效，並且可以擴展到包括更多類型的不可取輸出以進行懲罰和訓練，例如幻覺。

##### **A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models**
2408.16751v1 by Yi-Lin Tuan, William Yang Wang

Beyond maximum likelihood estimation (MLE), the standard objective of a
language model (LM) that optimizes good examples probabilities, many studies
have explored ways that also penalize bad examples for enhancing the quality of
output distribution, including unlikelihood training, exponential maximizing
average treatment effect (ExMATE), and direct preference optimization (DPO). To
systematically compare these methods and further provide a unified recipe for
LM optimization, in this paper, we present a unique angle of gradient analysis
of loss functions that simultaneously reward good examples and penalize bad
ones in LMs. Through both mathematical results and experiments on
CausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional
characteristics among these methods. We find that ExMATE serves as a superior
surrogate for MLE, and that combining DPO with ExMATE instead of MLE further
enhances both the statistical (5-7%) and generative (+18% win rate)
performance.

摘要：除了最大似然估计 (MLE) 之外，语言模型 (LM) 的标准目标是优化好例子的概率，许多研究探索了同时惩罚坏例子以提高输出分布质量的方法，包括非似然训练、指数最大化平均处理效果 (ExMATE) 和直接偏好优化 (DPO)。为了系统地比较这些方法并进一步为 LM 优化提供统一的配方，在本文中，我们提出了对损失函数进行梯度分析的独特角度，该损失函数同时奖励好例子并惩罚 LM 中的坏例子。通过对 CausalDialogue 和 Anthropic HH-RLHF 数据集的数学结果和实验，我们识别了这些方法之间不同的功能特征。我们发现 ExMATE 作为 MLE 的优秀替代品，并且将 DPO 与 ExMATE（而不是 MLE）结合使用进一步提高了统计（5-7%）和生成（+18% 获胜率）性能。

##### **Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge**
2408.16749v1 by Beidi Dong, Jin R. Lee, Ziwei Zhu, Balassubramanian Srinivasan

The United States has experienced a significant increase in violent
extremism, prompting the need for automated tools to detect and limit the
spread of extremist ideology online. This study evaluates the performance of
Bidirectional Encoder Representations from Transformers (BERT) and Generative
Pre-Trained Transformers (GPT) in detecting and classifying online domestic
extremist posts. We collected social media posts containing "far-right" and
"far-left" ideological keywords and manually labeled them as extremist or
non-extremist. Extremist posts were further classified into one or more of five
contributing elements of extremism based on a working definitional framework.
The BERT model's performance was evaluated based on training data size and
knowledge transfer between categories. We also compared the performance of GPT
3.5 and GPT 4 models using different prompts: na\"ive, layperson-definition,
role-playing, and professional-definition. Results showed that the best
performing GPT models outperformed the best performing BERT models, with more
detailed prompts generally yielding better results. However, overly complex
prompts may impair performance. Different versions of GPT have unique
sensitives to what they consider extremist. GPT 3.5 performed better at
classifying far-left extremist posts, while GPT 4 performed better at
classifying far-right extremist posts. Large language models, represented by
GPT models, hold significant potential for online extremism classification
tasks, surpassing traditional BERT models in a zero-shot setting. Future
research should explore human-computer interactions in optimizing GPT models
for extremist detection and classification tasks to develop more efficient
(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)
methods for identifying extremist content.

摘要：<paragraph>美國經歷了暴力極端主義的顯著增加，促使需要自動化工具來偵測和限制極端主義意識形態在網路上散布。本研究評估了來自 Transformer 的雙向編碼器表徵 (BERT) 和生成式預訓練 Transformer (GPT) 在偵測和分類線上國內極端主義貼文中的表現。我們收集了包含「極右派」和「極左派」意識形態關鍵字的社群媒體貼文，並手動標記它們為極端主義或非極端主義。極端主義貼文進一步根據一個工作定義架構分類為極端主義的五個促成元素中的其中一個或多個。BERT 模型的表現根據訓練資料大小和類別之間的知識轉移進行評估。我們也比較了使用不同提示的 GPT 3.5 和 GPT 4 模型的表現：天真的、外行人的定義、角色扮演和專業定義。結果顯示表現最好的 GPT 模型優於表現最好的 BERT 模型，而更詳細的提示通常會產生更好的結果。然而，過於複雜的提示可能會損害表現。不同版本的 GPT 對它們認為極端的內容有獨特的敏感性。GPT 3.5 在分類極左派極端主義貼文方面表現得更好，而 GPT 4 在分類極右派極端主義貼文方面表現得更好。以 GPT 模型為代表的大語言模型在線上極端主義分類任務中具有顯著的潛力，在零次學習設置中超越了傳統的 BERT 模型。未來的研究應探索人機互動，以最佳化 GPT 模型，用於極端主義偵測和分類任務，以開發更有效率（例如，更快、更省力）和有效（例如，更少的錯誤或失誤）的方法來識別極端主義內容。</paragraph>

##### **Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models**
2408.16740v1 by Jiří Milička

This paper addresses the conceptual, methodological and technical challenges
in studying large language models (LLMs) and the texts they produce from a
quantitative linguistics perspective. It builds on a theoretical framework that
distinguishes between the LLM as a substrate and the entities the model
simulates. The paper advocates for a strictly non-anthropomorphic approach to
models while cautiously applying methodologies used in studying human
linguistic behavior to the simulated entities. While natural language
processing researchers focus on the models themselves, their architecture,
evaluation, and methods for improving performance, we as quantitative linguists
should strive to build a robust theory concerning the characteristics of texts
produced by LLMs, how they differ from human-produced texts, and the properties
of simulated entities. Additionally, we should explore the potential of LLMs as
an instrument for studying human culture, of which language is an integral
part.

摘要：本文探討從量化語言學的角度研究大型語言模型 (LLM) 及其產生的文本時，在概念、方法和技術上會遇到的挑戰。本文建立在一個理論架構上，將 LLM 視為一個基底，以及模型模擬的實體。本文主張對模型採取嚴格非擬人化的方式，同時謹慎地將用於研究人類語言行為的方法應用於模擬實體。雖然自然語言處理研究人員專注於模型本身、其架構、評估和改善效能的方法，但我們作為量化語言學家應致力於建立一個強健的理論，探討 LLM 產生的文本特徵、它們與人類產生的文本有何不同，以及模擬實體的特性。此外，我們應探索 LLM 作為研究人類文化的工具的潛力，而語言是其中不可或缺的一部分。

##### **Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling**
2408.16737v1 by Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi

Training on high-quality synthetic data from strong language models (LMs) is
a common strategy to improve the reasoning performance of LMs. In this work, we
revisit whether this strategy is compute-optimal under a fixed inference budget
(e.g., FLOPs). To do so, we investigate the trade-offs between generating
synthetic data using a stronger but more expensive (SE) model versus a weaker
but cheaper (WC) model. We evaluate the generated data across three key
metrics: coverage, diversity, and false positive rate, and show that the data
from WC models may have higher coverage and diversity, but also exhibit higher
false positive rates. We then finetune LMs on data from SE and WC models in
different settings: knowledge distillation, self-improvement, and a novel
weak-to-strong improvement setup where a weaker LM teaches reasoning to a
stronger LM. Our findings reveal that models finetuned on WC-generated data
consistently outperform those trained on SE-generated data across multiple
benchmarks and multiple choices of WC and SE models. These results challenge
the prevailing practice of relying on SE models for synthetic data generation,
suggesting that WC may be the compute-optimal approach for training advanced LM
reasoners.

摘要：訓練強大的語言模型 (LM) 中的高品質合成資料是改善 LM 推論效能的常見策略。在這項工作中，我們重新探討此策略在固定推論預算（例如 FLOP）下是否為運算最佳化。為此，我們研究使用較強但較昂貴 (SE) 模型與較弱但較便宜 (WC) 模型產生合成資料之間的取捨。我們根據三個關鍵指標（涵蓋範圍、多樣性和誤報率）評估所產生的資料，並顯示來自 WC 模型的資料可能具有較高的涵蓋範圍和多樣性，但也會顯示較高的誤報率。然後，我們在不同的設定中對來自 SE 和 WC 模型的資料進行微調 LM：知識萃取、自我改善，以及一個新的弱轉強的改善設定，其中較弱的 LM 教導較強的 LM 推理。我們的研究結果顯示，在 WC 產生的資料上進行微調的模型在多個基準和多種 WC 和 SE 模型的選擇中，始終優於在 SE 產生的資料上訓練的模型。這些結果挑戰了依賴 SE 模型進行合成資料產生的普遍做法，表明 WC 可能是在訓練進階 LM 推論器時運算最佳化的途徑。

##### **Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming**
2408.16725v2 by Zhifei Xie, Changqiao Wu

Recent advances in language models have achieved significant progress.
GPT-4o, as a new milestone, has enabled real-time conversations with humans,
demonstrating near-human natural fluency. Such human-computer interaction
necessitates models with the capability to perform reasoning directly with the
audio modality and generate output in streaming. However, this remains beyond
the reach of current academic models, as they typically depend on extra TTS
systems for speech synthesis, resulting in undesirable latency. This paper
introduces the Mini-Omni, an audio-based end-to-end conversational model,
capable of real-time speech interaction. To achieve this capability, we propose
a text-instructed speech generation method, along with batch-parallel
strategies during inference to further boost the performance. Our method also
helps to retain the original model's language capabilities with minimal
degradation, enabling other works to establish real-time interaction
capabilities. We call this training method "Any Model Can Talk". We also
introduce the VoiceAssistant-400K dataset to fine-tune models optimized for
speech output. To our best knowledge, Mini-Omni is the first fully end-to-end,
open-source model for real-time speech interaction, offering valuable potential
for future research.

摘要：語言模型的最新進展取得重大進展。
GPT-4o 作為一個新的里程碑，實現了與人類的即時對話，
展示了接近人類的自然流暢度。這種人機互動
需要模型具備直接使用音訊模式進行推理並生成串流輸出的能力。然而，這仍然超出了當前學術模型的範圍，因為它們通常依賴於額外的 TTS 系統進行語音合成，從而導致不希望的延遲。本文介紹了 Mini-Omni，一個基於音訊的端到端對話模型，能夠進行實時語音互動。為了實現這一能力，我們提出
一種文字指導的語音生成方法，以及在推理過程中採用批次並行策略以進一步提升效能。我們的技術也有助於保留原始模型的語言能力，同時將退化降至最低，使其他作品能夠建立實時互動能力。我們將這種訓練方法稱為「任何模型都能說話」。我們還引入了 VoiceAssistant-400K 資料集，以微調針對語音輸出最佳化的模型。據我們所知，Mini-Omni 是第一個完全端到端、開放原始碼的實時語音互動模型，為未來的研究提供了寶貴的潛力。

##### **A GREAT Architecture for Edge-Based Graph Problems Like TSP**
2408.16717v1 by Attila Lischka, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár

In the last years, many neural network-based approaches have been proposed to
tackle combinatorial optimization problems such as routing problems. Many of
these approaches are based on graph neural networks (GNNs) or related
transformers, operating on the Euclidean coordinates representing the routing
problems. However, GNNs are inherently not well suited to operate on dense
graphs, such as in routing problems. Furthermore, models operating on Euclidean
coordinates cannot be applied to non-Euclidean versions of routing problems
that are often found in real-world settings. To overcome these limitations, we
propose a novel GNN-related edge-based neural model called Graph Edge Attention
Network (GREAT). We evaluate the performance of GREAT in the
edge-classification task to predict optimal edges in the Traveling Salesman
Problem (TSP). We can use such a trained GREAT model to produce sparse TSP
graph instances, keeping only the edges GREAT finds promising. Compared to
other, non-learning-based methods to sparsify TSP graphs, GREAT can produce
very sparse graphs while keeping most of the optimal edges. Furthermore, we
build a reinforcement learning-based GREAT framework which we apply to
Euclidean and non-Euclidean asymmetric TSP. This framework achieves
state-of-the-art results.

摘要：在过去几年中，许多基于神经网络的方法已被提出用于解决组合优化问题，例如路径问题。其中许多方法基于图神经网络 (GNN) 或相关转换器，它们在表示路径问题的欧几里得坐标上运行。然而，GNN 本质上不适合在稠密图上运行，例如在路径问题中。此外，在欧几里得坐标上运行的模型不能应用于在现实世界环境中经常发现的非欧几里得版本的路径问题。为了克服这些限制，我们提出了一种新颖的 GNN 相关边基神经模型，称为图边注意力网络 (GREAT)。我们在边分类任务中评估了 GREAT 的性能，以预测旅行商问题 (TSP) 中的最佳边。我们可以使用这种经过训练的 GREAT 模型来生成稀疏 TSP 图实例，只保留 GREAT 发现有希望的边。与其他基于非学习的方法来稀疏化 TSP 图相比，GREAT 可以生成非常稀疏的图，同时保留大部分最佳边。此外，我们构建了一个基于强化学习的 GREAT 框架，我们将其应用于欧几里得和非欧几里得非对称 TSP。该框架实现了最先进的结果。

##### **Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever**
2408.16672v1 by Rohan Jha, Bo Wang, Michael Günther, Saba Sturua, Mohammad Kalim Akram, Han Xiao

Multi-vector dense models, such as ColBERT, have proven highly effective in
information retrieval. ColBERT's late interaction scoring approximates the
joint query-document attention seen in cross-encoders while maintaining
inference efficiency closer to traditional dense retrieval models, thanks to
its bi-encoder architecture and recent optimizations in indexing and search. In
this paper, we introduce several improvements to the ColBERT model architecture
and training pipeline, leveraging techniques successful in the more established
single-vector embedding model paradigm, particularly those suited for
heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates
strong performance across a range of English and multilingual retrieval tasks,
while also cutting storage requirements by up to 50% compared to previous
models.

摘要：多向量稠密模型，例如 ColBERT，已被证明在信息检索中非常有效。ColBERT 的后期交互评分近似于跨编码器中看到的联合查询文档注意力，同时由于其双编码器架构和索引和搜索中的最新优化，推理效率接近于传统的稠密检索模型。在本文中，我们对 ColBERT 模型架构和训练管道进行了多项改进，利用了在更成熟的单向量嵌入模型范例中成功的技术，特别是那些适用于异构多语言数据的方法。我们的新模型 Jina-ColBERT-v2 在一系列英语和多语言检索任务中展示了强大的性能，同时与之前的模型相比，还将存储需求减少了 50%。

##### **Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity**
2408.16673v1 by Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo

Large language models rely on Supervised Fine-Tuning (SFT) to specialize in
downstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it
often leads to overfitting and limited output diversity due to its aggressive
updates to the data distribution. This paper aim to address these issues by
introducing the maximum entropy principle, which favors models with flatter
distributions that still effectively capture the data. Specifically, we develop
a new distribution matching method called GEM, which solves reverse
Kullback-Leibler divergence minimization with an entropy regularizer.
  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.
First, when applied to the UltraFeedback dataset to develop general
instruction-following abilities, GEM exhibits reduced overfitting, evidenced by
lower perplexity and better performance on the IFEval benchmark. Furthermore,
GEM enhances output diversity, leading to performance gains of up to 7 points
on math reasoning and code generation tasks using best-of-n sampling, even
without domain-specific data. Second, when fine-tuning with domain-specific
datasets for math reasoning and code generation, GEM also shows less
overfitting and improvements of up to 10 points compared with CE.

摘要：大型語言模型依賴受控微調 (SFT) 來專精於下游任務。交叉熵 (CE) 損失是 SFT 中的實際選擇，但由於其對資料分佈的激進更新，它經常導致過度擬合和輸出多樣性受限。本文旨在透過引入最大熵原理來解決這些問題，該原理有利於具有較平坦分佈且仍能有效擷取資料的模型。具體來說，我們開發了一種稱為 GEM 的新分佈匹配方法，它以熵正則化求解反向 Kullback-Leibler 散度最小化。對於 Llama-3-8B 模型的 SFT，GEM 在幾個方面優於 CE。首先，當應用於 UltraFeedback 資料集以開發一般的指令遵循能力時，GEM 展現出減少的過度擬合，這由較低的困惑度和在 IFEval 基準上的更好效能所證明。此外，GEM 增強了輸出多樣性，即使沒有特定領域的資料，也能在數學推理和程式碼產生任務中使用最佳 n 採樣獲得高達 7 分的效能提升。其次，當使用特定領域的資料集進行微調以進行數學推理和程式碼產生時，與 CE 相比，GEM 也顯示出較少的過度擬合和高達 10 分的改進。

##### **Iterative Graph Alignment**
2408.16667v1 by Fangyuan Yu, Hardeep Singh Arora, Matt Johnson

By compressing diverse narratives, LLMs go beyond memorization, achieving
intelligence by capturing generalizable causal relationships. However, they
suffer from local 'representation gaps' due to insufficient training data
diversity, limiting their real-world utility, especially in tasks requiring
strict alignment to rules. Traditional alignment methods relying on heavy human
annotations are inefficient and unscalable. Recent self-alignment techniques
also fall short, as they often depend on self-selection based prompting and
memorization-based learning. To address these issues, we introduce Iterative
Graph Alignment (IGA), an annotation-free rule-based alignment algorithm. A
teacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical
graphs and reference answers. The student model (LLM) identifies local
knowledge gaps by attempting to align its responses with these references,
collaborating with helper models to generate diverse answers. These aligned
responses are then used for iterative supervised fine-tuning (SFT). Our
evaluations across five rule-based scenarios demonstrate IGP's effectiveness,
with a 73.12\% alignment improvement in Claude Sonnet 3.5, and
Llama3-8B-Instruct achieving an 86.20\% improvement, outperforming Claude
Sonnet 3.5 in rule-based alignment.

摘要：透過壓縮各種敘述，LLM 不再只是記憶，而是透過擷取可概括的因果關係來實現智能。然而，由於訓練資料的多樣性不足，它們會出現區域性的「表示差距」，這限制了它們在現實世界中的效用，特別是在需要嚴格遵守規則的任務中。傳統的比對方法仰賴大量的人工標註，既沒有效率又無法擴展。最近的自我比對技術也做得不夠好，因為它們通常依賴於基於提示的自我選擇和基於記憶的學習。為了解決這些問題，我們引入了迭代圖形比對 (IGA)，這是一個無需標註的基於規則的比對演算法。教師模型 (VLM) 使用迭代圖形提示 (IGP) 來建立邏輯圖形和參考答案。學生模型 (LLM) 嘗試將其回應與這些參考內容比對，並與輔助模型合作產生多樣化的答案，藉此找出局部知識差距。接著，這些比對後的回應會用於反覆的監督微調 (SFT)。我們在五種基於規則的情境中進行評估，證明了 IGP 的有效性，Claude Sonnet 3.5 的比對改善了 73.12%，而 Llama3-8B-Instruct 的改善達到 86.20%，在基於規則的比對中優於 Claude Sonnet 3.5。

##### **DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**
2408.16647v1 by Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, Zhaobin Mo

The advancement of autonomous driving technologies necessitates increasingly
sophisticated methods for understanding and predicting real-world scenarios.
Vision language models (VLMs) are emerging as revolutionary tools with
significant potential to influence autonomous driving. In this paper, we
propose the DriveGenVLM framework to generate driving videos and use VLMs to
understand them. To achieve this, we employ a video generation framework
grounded in denoising diffusion probabilistic models (DDPM) aimed at predicting
real-world video sequences. We then explore the adequacy of our generated
videos for use in VLMs by employing a pre-trained model known as Efficient
In-context Learning on Egocentric Videos (EILEV). The diffusion model is
trained with the Waymo open dataset and evaluated using the Fr\'echet Video
Distance (FVD) score to ensure the quality and realism of the generated videos.
Corresponding narrations are provided by EILEV for these generated videos,
which may be beneficial in the autonomous driving domain. These narrations can
enhance traffic scene understanding, aid in navigation, and improve planning
capabilities. The integration of video generation with VLMs in the DriveGenVLM
framework represents a significant step forward in leveraging advanced AI
models to address complex challenges in autonomous driving.

摘要：自動駕駛技術的進展需要越來越精密的方法來理解和預測真實世界的場景。視覺語言模型 (VLM) 正在成為革命性的工具，具有影響自動駕駛的巨大潛力。在本文中，我們提出 DriveGenVLM 架構來生成駕駛影片，並使用 VLM 來理解它們。為此，我們採用了一個基於去噪擴散機率模型 (DDPM) 的影片生成架構，旨在預測真實世界的影片序列。然後，我們透過採用一種稱為自視影片上高效情境學習 (EILEV) 的預訓練模型，來探討我們生成的影片是否足以用於 VLM。擴散模型使用 Waymo 開放資料集進行訓練，並使用 Fr\'echet 影片距離 (FVD) 分數進行評估，以確保生成影片的品質和真實性。EILEV 為這些生成的影片提供了對應的旁白，這可能有助於自動駕駛領域。這些旁白可以增強對交通場景的理解、協助導航，並改善規劃能力。在 DriveGenVLM 架構中整合影片生成和 VLM，代表了在利用進階 AI 模型來解決自動駕駛中的複雜挑戰方面向前邁進了一大步。

##### **RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model**
2408.16634v1 by Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings

The increasing sophistication of text-to-image generative models has led to
complex challenges in defining and enforcing copyright infringement criteria
and protection. Existing methods, such as watermarking and dataset
deduplication, fail to provide comprehensive solutions due to the lack of
standardized metrics and the inherent complexity of addressing copyright
infringement in diffusion models. To deal with these challenges, we propose a
Reinforcement Learning-based Copyright Protection(RLCP) method for
Text-to-Image Diffusion Model, which minimizes the generation of
copyright-infringing content while maintaining the quality of the
model-generated dataset. Our approach begins with the introduction of a novel
copyright metric grounded in copyright law and court precedents on
infringement. We then utilize the Denoising Diffusion Policy Optimization
(DDPO) framework to guide the model through a multi-step decision-making
process, optimizing it using a reward function that incorporates our proposed
copyright metric. Additionally, we employ KL divergence as a regularization
term to mitigate some failure modes and stabilize RL fine-tuning. Experiments
conducted on 3 mixed datasets of copyright and non-copyright images demonstrate
that our approach significantly reduces copyright infringement risk while
maintaining image quality.

摘要：文本到图像生成模型日益复杂，导致在定义和执行版权侵权标准和保护方面面临复杂的挑战。现有的方法（例如水印和数据集去重）由于缺乏标准化指标和解决扩散模型中版权侵权的固有复杂性而无法提供全面的解决方案。为了应对这些挑战，我们提出了一种基于强化学习的文本到图像扩散模型版权保护（RLCP）方法，该方法最大程度地减少了侵犯版权内容的生成，同时保持了模型生成数据集的质量。我们的方法首先引入了一种新的版权度量，该度量基于版权法和侵权方面的法院判例。然后，我们利用去噪扩散策略优化（DDPO）框架指导模型完成多步骤决策过程，使用包含我们提出的版权度量的奖励函数对其进行优化。此外，我们采用 KL 散度作为正则化项，以减轻一些故障模式并稳定 RL 微调。在 3 个包含版权和非版权图像的混合数据集上进行的实验表明，我们的方法显著降低了版权侵权风险，同时保持了图像质量。

##### **Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning**
2408.16633v1 by Keqin Li, Jin Wang, Xubo Wu, Xirui Peng, Runmian Chang, Xiaoyu Deng, Yiwen Kang, Yue Yang, Fanghao Ni, Bo Hong

With the rapid growth of global e-commerce, the demand for automation in the
logistics industry is increasing. This study focuses on automated picking
systems in warehouses, utilizing deep learning and reinforcement learning
technologies to enhance picking efficiency and accuracy while reducing system
failure rates. Through empirical analysis, we demonstrate the effectiveness of
these technologies in improving robot picking performance and adaptability to
complex environments. The results show that the integrated machine learning
model significantly outperforms traditional methods, effectively addressing the
challenges of peak order processing, reducing operational errors, and improving
overall logistics efficiency. Additionally, by analyzing environmental factors,
this study further optimizes system design to ensure efficient and stable
operation under variable conditions. This research not only provides innovative
solutions for logistics automation but also offers a theoretical and empirical
foundation for future technological development and application.

摘要：隨著全球電子商務的快速發展，物流產業對自動化的需求與日俱增。本研究著重於倉庫中的自動化揀貨系統，利用深度學習與強化學習技術，以提升揀貨效率與準確度，同時降低系統故障率。透過實證分析，我們證明了這些技術在提升機器人揀貨效能與適應複雜環境的有效性。結果顯示，整合機器學習模型顯著優於傳統方法，有效解決尖峰訂單處理的挑戰，減少作業錯誤，並提升整體物流效率。此外，本研究透過分析環境因素，進一步最佳化系統設計，以確保在變動的條件下能有效且穩定的運作。本研究不僅為物流自動化提供了創新的解決方案，也為未來的技術發展與應用奠定了理論與實證基礎。

##### **Maelstrom Networks**
2408.16632v1 by Matthew Evanusa, Cornelia Fermüller, Yiannis Aloimonos

Artificial Neural Networks has struggled to devise a way to incorporate
working memory into neural networks. While the ``long term'' memory can be seen
as the learned weights, the working memory consists likely more of dynamical
activity, that is missing from feed-forward models. Current state of the art
models such as transformers tend to ``solve'' this by ignoring working memory
entirely and simply process the sequence as an entire piece of data; however
this means the network cannot process the sequence in an online fashion, and
leads to an immense explosion in memory requirements. Here, inspired by a
combination of controls, reservoir computing, deep learning, and recurrent
neural networks, we offer an alternative paradigm that combines the strength of
recurrent networks, with the pattern matching capability of feed-forward neural
networks, which we call the \textit{Maelstrom Networks} paradigm. This paradigm
leaves the recurrent component - the \textit{Maelstrom} - unlearned, and
offloads the learning to a powerful feed-forward network. This allows the
network to leverage the strength of feed-forward training without unrolling the
network, and allows for the memory to be implemented in new neuromorphic
hardware. It endows a neural network with a sequential memory that takes
advantage of the inductive bias that data is organized causally in the temporal
domain, and imbues the network with a state that represents the agent's
``self'', moving through the environment. This could also lead the way to
continual learning, with the network modularized and ``'protected'' from
overwrites that come with new data. In addition to aiding in solving these
performance problems that plague current non-temporal deep networks, this also
could finally lead towards endowing artificial networks with a sense of
``self''.

摘要：人工神经网络一直努力想办法将工作记忆纳入神经网络中。虽然「长期」记忆可以看作是学习到的权重，但工作记忆可能更多地由动态活动组成，而这在前馈模型中是不存在的。目前最先进的模型（例如变压器）倾向于通过完全忽略工作记忆并简单地将序列处理为整个数据块来「解决」这个问题；然而，这意味着网络无法在线处理序列，并导致内存需求激增。在这里，受控制、储层计算、深度学习和循环神经网络的组合启发，我们提供了一种替代范例，它结合了循环网络的优势和前馈神经网络的模式匹配能力，我们称之为\textit{Maelstrom Networks}范例。此范例将循环组件（即\textit{Maelstrom}）保持未学习状态，并将学习卸载到强大的前馈网络。这使网络能够利用前馈训练的优势，而无需展开网络，并允许在新的神经形态硬件中实现内存。它赋予神经网络一种顺序记忆，该记忆利用了数据在时间域中因果组织的归纳偏置，并赋予网络一种状态，该状态表示代理的「自我」，在环境中移动。这还可以为持续学习铺平道路，网络模块化并「受保护」，免受新数据带来的覆盖。除了帮助解决困扰当前非时间深度网络的这些性能问题之外，这最终还可以赋予人工网络一种「自我」意识。

##### **LLMs generate structurally realistic social networks but overestimate political homophily**
2408.16629v1 by Serina Chang, Alicja Chaszczewicz, Emma Wang, Maya Josifovska, Emma Pierson, Jure Leskovec

Generating social networks is essential for many applications, such as
epidemic modeling and social simulations. Prior approaches either involve deep
learning models, which require many observed networks for training, or stylized
models, which are limited in their realism and flexibility. In contrast, LLMs
offer the potential for zero-shot and flexible network generation. However, two
key questions are: (1) are LLM's generated networks realistic, and (2) what are
risks of bias, given the importance of demographics in forming social ties? To
answer these questions, we develop three prompting methods for network
generation and compare the generated networks to real social networks. We find
that more realistic networks are generated with "local" methods, where the LLM
constructs relations for one persona at a time, compared to "global" methods
that construct the entire network at once. We also find that the generated
networks match real networks on many characteristics, including density,
clustering, community structure, and degree. However, we find that LLMs
emphasize political homophily over all other types of homophily and
overestimate political homophily relative to real-world measures.

摘要：生成社交網路對於許多應用程式來說至關重要，例如流行病建模和社交模擬。先前的做法包括深度學習模型，這需要許多已觀察到的網路進行訓練，或樣式化模型，其真實性和靈活性受到限制。相比之下，LLM 提供了零次學習和靈活網路生成的潛力。然而，有兩個關鍵問題：(1) LLM 生成的網路是否真實，以及 (2) 鑑於人口統計資料在形成社交關係中的重要性，有哪些偏見風險？為了回答這些問題，我們開發了三種提示方法來生成網路，並將生成的網路與真實的社交網路進行比較。我們發現使用「局部」方法生成了更真實的網路，其中 LLM 一次為一個角色建立關係，相比之下，「全域」方法一次建立整個網路。我們還發現生成的網路在許多特徵上與真實網路相符，包括密度、群集、社群結構和程度。然而，我們發現 LLM 強調政治同質性勝過所有其他類型的同質性，並且高估了政治同質性相對於真實世界的衡量標準。

##### **Towards Infusing Auxiliary Knowledge for Distracted Driver Detection**
2408.16621v1 by Ishwar B Balappanawar, Ashmit Chamoli, Ruwan Wickramarachchi, Aditya Mishra, Ponnurangam Kumaraguru, Amit P. Sheth

Distracted driving is a leading cause of road accidents globally.
Identification of distracted driving involves reliably detecting and
classifying various forms of driver distraction (e.g., texting, eating, or
using in-car devices) from in-vehicle camera feeds to enhance road safety. This
task is challenging due to the need for robust models that can generalize to a
diverse set of driver behaviors without requiring extensive annotated datasets.
In this paper, we propose KiD3, a novel method for distracted driver detection
(DDD) by infusing auxiliary knowledge about semantic relations between entities
in a scene and the structural configuration of the driver's pose. Specifically,
we construct a unified framework that integrates the scene graphs, and driver
pose information with the visual cues in video frames to create a holistic
representation of the driver's actions.Our results indicate that KiD3 achieves
a 13.64% accuracy improvement over the vision-only baseline by incorporating
such auxiliary knowledge with visual information.

摘要：分心駕駛是全球道路事故的主要原因。
分心駕駛的識別涉及從車載相機饋送中可靠地檢測和分類各種形式的駕駛分心（例如發簡訊、進食或使用車載設備），以增強道路安全。由於需要強大的模型才能概括到各種駕駛行為，而無需大量的註釋數據集，因此這項任務具有挑戰性。
在本文中，我們提出了 KiD3，這是一種新的分心駕駛檢測 (DDD) 方法，通過注入場景中實體之間語義關係和駕駛員姿勢結構配置的輔助知識。具體來說，我們構建了一個統一的框架，將場景圖、駕駛員姿勢信息與視頻幀中的視覺線索集成在一起，以創建駕駛員動作的整體表示。我們的結果表明，KiD3 通過將這種輔助知識與視覺信息相結合，比僅視覺基準提高了 13.64% 的準確率。

##### **Hyperdimensional Vector Tsetlin Machines with Applications to Sequence Learning and Generation**
2408.16620v1 by Christian D. Blakely

We construct a two-layered model for learning and generating sequential data
that is both computationally fast and competitive with vanilla Tsetlin
machines, adding numerous advantages. Through the use of hyperdimensional
vector computing (HVC) algebras and Tsetlin machine clause structures, we
demonstrate that the combination of both inherits the generality of data
encoding and decoding of HVC with the fast interpretable nature of Tsetlin
machines to yield a powerful machine learning model. We apply the approach in
two areas, namely in forecasting, generating new sequences, and classification.
For the latter, we derive results for the entire UCR Time Series Archive and
compare with the standard benchmarks to see how well the method competes in
time series classification.

摘要：我們建構了一個兩層模型來學習和生成順序資料，它在計算上既快速又與香草 Tsetlin 機器競爭，並增加了許多優點。透過使用超維度向量運算 (HVC) 代數和 Tsetlin 機器子句結構，我們證明了兩者的結合繼承了 HVC 資料編碼和解碼的普遍性，以及 Tsetlin 機器的快速可解釋性質，產生了一個強大的機器學習模型。我們在兩個領域應用此方法，即預測、產生新序列和分類。對於後者，我們導出整個 UCR 時間序列檔案的結果，並與標準基準進行比較，以了解該方法在時間序列分類中的競爭程度。

##### **Examination of Code generated by Large Language Models**
2408.16601v1 by Robin Beer, Alexander Feix, Tim Guttzeit, Tamara Muras, Vincent Müller, Maurice Rauscher, Florian Schäffler, Welf Löwe

Large language models (LLMs), such as ChatGPT and Copilot, are transforming
software development by automating code generation and, arguably, enable rapid
prototyping, support education, and boost productivity. Therefore, correctness
and quality of the generated code should be on par with manually written code.
To assess the current state of LLMs in generating correct code of high quality,
we conducted controlled experiments with ChatGPT and Copilot: we let the LLMs
generate simple algorithms in Java and Python along with the corresponding unit
tests and assessed the correctness and the quality (coverage) of the generated
(test) codes. We observed significant differences between the LLMs, between the
languages, between algorithm and test codes, and over time. The present paper
reports these results together with the experimental methods allowing repeated
and comparable assessments for more algorithms, languages, and LLMs over time.

摘要：大型語言模型 (LLM)，例如 ChatGPT 和 Copilot，透過自動化程式碼產生，並可快速建構原型、支援教育，以及提升生產力，轉變軟體開發。因此，產生的程式碼的正確性和品質應與手寫程式碼相當。為評估 LLM 在產生高品質正確程式碼的現況，我們對 ChatGPT 和 Copilot 進行受控實驗：我們讓 LLM 產生 Java 和 Python 中的簡單演算法以及對應的單元測試，並評估產生的 (測試) 程式碼的正確性和品質 (涵蓋範圍)。我們觀察到 LLM 之間、語言之間、演算法和測試程式碼之間，以及隨著時間推移的顯著差異。本文報告了這些結果以及實驗方法，允許隨著時間推移對更多演算法、語言和 LLM 進行重複且可比較的評估。

##### **Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies**
2408.16586v1 by Zhiyang Qi, Michimasa Inaba

Recent advancements in natural language processing, particularly with large
language models (LLMs) like GPT-4, have significantly enhanced dialogue
systems, enabling them to generate more natural and fluent conversations.
Despite these improvements, challenges persist, such as managing continuous
dialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024
addresses these challenges by employing the Werewolf Game, an incomplete
information game, to test the capabilities of LLMs in complex interactive
environments. This paper introduces a LLM-based Werewolf Game AI, where each
role is supported by situation analysis to aid response generation.
Additionally, for the werewolf role, various persuasion strategies, including
logical appeal, credibility appeal, and emotional appeal, are employed to
effectively persuade other players to align with its actions.

摘要：近年來自然語言處理領域的進展，特別是大語言模型 (LLM) 如 GPT-4，大幅提升了對話系統，讓它們能夠產生更自然且流暢的對話。儘管有這些進展，挑戰依然存在，例如管理連續對話、記憶保留和最小化幻覺。AIWolfDial2024 透過採用狼人遊戲，一種不完全資訊遊戲，來解決這些挑戰，以測試 LLM 在複雜互動環境中的能力。本文介紹了一款基於 LLM 的狼人遊戲 AI，其中每個角色都透過情境分析來協助回應產生。此外，對於狼人角色，採用各種說服策略，包括邏輯訴求、信譽訴求和情緒訴求，以有效說服其他玩家配合其行動。

##### **Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning**
2408.16577v1 by Boyu Chen, Junjie Liu, Zhu Li, Mengyue yang

Learning representations with a high Probability of Necessary and Sufficient
Causes (PNS) has been shown to enhance deep learning models' ability. This task
involves identifying causal features that are both sufficient (guaranteeing the
outcome) and necessary (without which the outcome cannot occur). However,
current research predominantly focuses on unimodal data, and extending PNS
learning to multimodal settings presents significant challenges. The challenges
arise as the conditions for PNS identifiability, Exogeneity and Monotonicity,
need to be reconsidered in a multimodal context, where sufficient and necessary
causal features are distributed across different modalities. To address this,
we first propose conceptualizing multimodal representations as comprising
modality-invariant and modality-specific components. We then analyze PNS
identifiability for each component, while ensuring non-trivial PNS estimation.
Finally, we formulate tractable optimization objectives that enable multimodal
models to learn high-PNS representations, thereby enhancing their predictive
performance. Experiments demonstrate the effectiveness of our method on both
synthetic and real-world data.

摘要：學習具有必要的和充分原因 (PNS) 的高機率表示法已被證明可以增強深度學習模型的能力。此任務涉及識別既充分（保證結果）又必要（沒有它結果無法發生）的因果特徵。然而，目前的研究所主要集中於單模態資料，將 PNS 學習擴展到多模態設定會產生重大挑戰。挑戰在於 PNS 可識別性、外生性和單調性的條件需要在多模態上下文中重新考慮，其中充分且必要的因果特徵分布在不同的模態中。為了解決這個問題，我們首先提出將多模態表示概念化為包含模態不變和模態特定組成部分。然後，我們分析每個組成部分的 PNS 可識別性，同時確保非平凡的 PNS 估計。最後，我們制定可行的最佳化目標，使多模態模型能夠學習高 PNS 表示，從而增強其預測性能。實驗證明了我們的方法在合成和真實世界資料上的有效性。

##### **Predictability maximization and the origins of word order harmony**
2408.16570v1 by Ramon Ferrer-i-Cancho

We address the linguistic problem of the sequential arrangement of a head and
its dependents from an information theoretic perspective. In particular, we
consider the optimal placement of a head that maximizes the predictability of
the sequence. We assume that dependents are statistically independent given a
head, in line with the open-choice principle and the core assumptions of
dependency grammar. We demonstrate the optimality of harmonic order, i.e.,
placing the head last maximizes the predictability of the head whereas placing
the head first maximizes the predictability of dependents. We also show that
postponing the head is the optimal strategy to maximize its predictability
while bringing it forward is the optimal strategy to maximize the
predictability of dependents. We unravel the advantages of the strategy of
maximizing the predictability of the head over maximizing the predictability of
dependents. Our findings shed light on the placements of the head adopted by
real languages or emerging in different kinds of experiments.

摘要：我們從資訊理論的角度探討頭部及其依賴項的順序排列的語言學問題。特別是，我們考慮頭部的最佳位置，以最大化序列的可預測性。我們假設依賴項在給定頭部時在統計上是獨立的，這符合開放選擇原則和依賴語法的核心假設。我們證明了諧波順序的最優性，即把頭部放在最後可以最大化頭部的可預測性，而把頭部放在第一個可以最大化依賴項的可預測性。我們還表明，延後頭部是最大化其可預測性的最佳策略，而提前頭部是最大化依賴項可預測性的最佳策略。我們揭示了最大化頭部可預測性策略相對於最大化依賴項可預測性的優點。我們的發現揭示了實際語言採用的頭部位置或在不同類型的實驗中出現的頭部位置。

##### **SALSA: Speedy ASR-LLM Synchronous Aggregation**
2408.16542v1 by Ashish Mittal, Darshan Prabhu, Sunita Sarawagi, Preethi Jyothi

Harnessing pre-trained LLMs to improve ASR systems, particularly for
low-resource languages, is now an emerging area of research. Existing methods
range from using LLMs for ASR error correction to tightly coupled systems that
replace the ASR decoder with the LLM. These approaches either increase decoding
time or require expensive training of the cross-attention layers. We propose
SALSA, which couples the decoder layers of the ASR to the LLM decoder, while
synchronously advancing both decoders. Such coupling is performed with a simple
projection of the last decoder state, and is thus significantly more training
efficient than earlier approaches. A challenge of our proposed coupling is
handling the mismatch between the tokenizers of the LLM and ASR systems. We
handle this mismatch using cascading tokenization with respect to the LLM and
ASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS
benchmark, yielding substantial WER reductions of up to 38%.

摘要：利用預先訓練的 LLM 來改善 ASR 系統，特別針對低資源語言，現在是一個新興的研究領域。現有方法從使用 LLM 進行 ASR 錯誤修正到使用 LLM 取代 ASR 解碼器的緊密耦合系統。這些方法會增加解碼時間或需要對交叉注意層進行昂貴的訓練。我們提出 SALSA，它將 ASR 的解碼器層與 LLM 解碼器耦合，同時同步推進兩個解碼器。這種耦合是通過對最後解碼器狀態進行簡單投影來執行，因此比早期的訓練方法顯著提高了訓練效率。我們提出的耦合面臨的挑戰是處理 LLM 和 ASR 系統的標記化器之間的不匹配。我們使用針對 LLM 和 ASR 詞彙表進行串聯標記化來處理這種不匹配。我們在 FLEURS 基準測試中的 8 種低資源語言上評估 SALSA，獲得了高達 38% 的 WER 顯著降低。

##### **SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks**
2408.16537v1 by Xing Ai, Guanyu Zhu, Yulin Zhu, Yu Zheng, Gaolei Li, Jianhua Li, Kai Zhou

Graph Neural Networks (GNNs) have demonstrated commendable performance for
graph-structured data. Yet, GNNs are often vulnerable to adversarial structural
attacks as embedding generation relies on graph topology. Existing efforts are
dedicated to purifying the maliciously modified structure or applying adaptive
aggregation, thereby enhancing the robustness against adversarial structural
attacks. It is inevitable for a defender to consume heavy computational costs
due to lacking prior knowledge about modified structures. To this end, we
propose an efficient defense method, called Simple and Fast Robust Graph Neural
Network (SFR-GNN), supported by mutual information theory. The SFR-GNN first
pre-trains a GNN model using node attributes and then fine-tunes it over the
modified graph in the manner of contrastive learning, which is free of
purifying modified structures and adaptive aggregation, thus achieving great
efficiency gains. Consequently, SFR-GNN exhibits a 24%--162% speedup compared
to advanced robust models, demonstrating superior robustness for node
classification tasks.

摘要：圖形神經網路（GNN）已證明在圖形結構資料中具有值得稱道的效能。然而，由於嵌入式生成依賴於圖形拓撲，因此 GNN 常常容易受到對抗性結構攻擊。現有的努力專注於淨化惡意修改的結構或應用適應性聚合，從而增強對抗對抗性結構攻擊的穩健性。由於缺乏關於修改結構的先驗知識，因此防禦者不可避免地會消耗大量的運算成本。為此，我們提出了一種稱為簡單且快速的穩健圖形神經網路（SFR-GNN）的有效防禦方法，並得到互資訊理論的支持。SFR-GNN 首先使用節點屬性預訓練 GNN 模型，然後以對比學習的方式對修改後的圖形進行微調，無需淨化修改後的結構和自適應聚合，從而實現了巨大的效率提升。因此，與先進的穩健模型相比，SFR-GNN 表現出 24%--162% 的加速，證明了節點分類任務的卓越穩健性。

##### **CNIMA: A Universal Evaluation Framework and Automated Approach for Assessing Second Language Dialogues**
2408.16518v1 by Rena Gao, Jingxuan Wu, Carsten Roever, Xuetong Wu, Jing Wu, Long Lv, Jey Han Lau

We develop CNIMA (Chinese Non-Native Interactivity Measurement and
Automation), a Chinese-as-a-second-language labelled dataset with 10K
dialogues. We annotate CNIMA using an evaluation framework -- originally
introduced for English-as-a-second-language dialogues -- that assesses
micro-level features (e.g.\ backchannels) and macro-level interactivity labels
(e.g.\ topic management) and test the framework's transferability from English
to Chinese. We found the framework robust across languages and revealed
universal and language-specific relationships between micro-level and
macro-level features. Next, we propose an approach to automate the evaluation
and find strong performance, creating a new tool for automated second language
assessment. Our system can be adapted to other languages easily as it uses
large language models and as such does not require large-scale annotated
training data.

摘要：我們開發了 CNIMA（華語非母語互動測量與自動化），這是一個標有 10K 對話的華語作為第二語言的標記資料集。我們使用原本用於英語作為第二語言對話的評估架構來標記 CNIMA，該架構評估微觀層級特徵（例如反向頻道）和巨觀層級互動標籤（例如主題管理），並測試該架構從英語到華語的可移植性。我們發現該架構在各語言間具有強健性，並揭示了微觀層級和巨觀層級特徵之間的通用和特定於語言的關係。接下來，我們提出了一種自動化評估的方法，並發現強大的效能，創造了一個用於自動化第二語言評估的新工具。我們的系統可以輕鬆地調整到其他語言，因為它使用大型語言模型，因此不需要大量標記的訓練資料。

##### **Adaptive Variational Continual Learning via Task-Heuristic Modelling**
2408.16517v1 by Fan Yang

Variational continual learning (VCL) is a turn-key learning algorithm that
has state-of-the-art performance among the best continual learning models. In
our work, we explore an extension of the generalized variational continual
learning (GVCL) model, named AutoVCL, which combines task heuristics for
informed learning and model optimization. We demonstrate that our model
outperforms the standard GVCL with fixed hyperparameters, benefiting from the
automatic adjustment of the hyperparameter based on the difficulty and
similarity of the incoming task compared to the previous tasks.

摘要：變異持續學習 (VCL) 是一種交鑰匙學習演算法，在最佳持續學習模型中具有最先進的效能。在我們的研究中，我們探討廣義變異持續學習 (GVCL) 模型的延伸，稱為 AutoVCL，它結合任務啟發法，用於明智的學習和模型最佳化。我們證明我們的模型優於具有固定超參數的標準 GVCL，受益於根據與前一個任務相比的新進任務的難度和相似性自動調整超參數。

##### **LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?**
2408.16502v1 by Jan Cegin, Jakub Simko, Peter Brusilovsky

The generative large language models (LLMs) are increasingly being used for
data augmentation tasks, where text samples are LLM-paraphrased and then used
for classifier fine-tuning. However, a research that would confirm a clear
cost-benefit advantage of LLMs over more established augmentation methods is
largely missing. To study if (and when) is the LLM-based augmentation
advantageous, we compared the effects of recent LLM augmentation methods with
established ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We
also varied the number of seeds and collected samples to better explore the
downstream model accuracy space. Finally, we performed a cost-benefit analysis
and show that LLM-based methods are worthy of deployment only when very small
number of seeds is used. Moreover, in many cases, established methods lead to
similar or better model accuracies.

摘要：生成式大型語言模型（LLM）正越來越多地用於資料擴充任務，其中文字範例經過 LLM 重新詮釋，然後用於分類器微調。然而，一項研究將確認 LLM 相較於更既定的擴充方法具有明顯的成本效益優勢，這在很大程度上是缺失的。為了研究 LLM 基於擴充是否（以及何時）具有優勢，我們比較了近期 LLM 擴充方法與既定方法在 6 個資料集、3 個分類器和 2 個微調方法上的效果。我們也改變了種子數和收集的範例，以更好地探索下游模型準確性空間。最後，我們執行了一個成本效益分析，並顯示基於 LLM 的方法只有在使用極少數的種子時才值得部署。此外，在許多情況下，既定方法會導致相似或更好的模型準確性。

##### **On-device AI: Quantization-aware Training of Transformers in Time-Series**
2408.16495v1 by Tianheng Ling, Gregor Schiele

Artificial Intelligence (AI) models for time-series in pervasive computing
keep getting larger and more complicated. The Transformer model is by far the
most compelling of these AI models. However, it is difficult to obtain the
desired performance when deploying such a massive model on a sensor device with
limited resources. My research focuses on optimizing the Transformer model for
time-series forecasting tasks. The optimized model will be deployed as hardware
accelerators on embedded Field Programmable Gate Arrays (FPGAs). I will
investigate the impact of applying Quantization-aware Training to the
Transformer model to reduce its size and runtime memory footprint while
maximizing the advantages of FPGAs.

摘要：人工智慧（AI）模型在普遍運算中的時間序列持續變大且更複雜。Transformer 模型是目前最引人注目的 AI 模型。然而，在資源有限的感測器裝置上部署如此龐大的模型時，難以獲得理想的效能。我的研究重點在於最佳化 Transformer 模型，以進行時間序列預測任務。最佳化的模型將部署為嵌入式現場可編程閘陣列（FPGA）上的硬體加速器。我將探討將量化感知訓練應用於 Transformer 模型的影響，以縮小其大小和執行時期記憶體佔用空間，同時最大化 FPGA 的優勢。

##### **Learning from Negative Samples in Generative Biomedical Entity Linking**
2408.16493v1 by Chanhwi Kim, Hyunjae Kim, Sihyeon Park, Jiwoo Lee, Mujeen Sung, Jaewoo Kang

Generative models have become widely used in biomedical entity linking
(BioEL) due to their excellent performance and efficient memory usage. However,
these models are usually trained only with positive samples--entities that
match the input mention's identifier--and do not explicitly learn from hard
negative samples, which are entities that look similar but have different
meanings. To address this limitation, we introduce ANGEL (Learning from
Negative Samples in Generative Biomedical Entity Linking), the first framework
that trains generative BioEL models using negative samples. Specifically, a
generative model is initially trained to generate positive samples from the
knowledge base for given input entities. Subsequently, both correct and
incorrect outputs are gathered from the model's top-k predictions. The model is
then updated to prioritize the correct predictions through direct preference
optimization. Our models fine-tuned with ANGEL outperform the previous best
baseline models by up to an average top-1 accuracy of 1.4% on five benchmarks.
When incorporating our framework into pre-training, the performance improvement
further increases to 1.7%, demonstrating its effectiveness in both the
pre-training and fine-tuning stages. Our code is available at
https://github.com/dmis-lab/ANGEL.

摘要：生成模型因其出色的性能和高效的内存使用而被广泛用于生物医学实体链接 (BioEL)。然而，这些模型通常仅使用正面样本（与输入提及的标识符匹配的实体）进行训练，并且不会明确地从硬负面样本（看起来相似但具有不同含义的实体）中学习。为了解决这一限制，我们引入了 ANGEL（生成生物医学实体链接中的负样本学习），这是第一个使用负样本训练生成 BioEL 模型的框架。具体来说，最初训练一个生成模型，以便为给定的输入实体从知识库中生成正样本。随后，从模型的 top-k 预测中收集正确和不正确的输出。然后更新模型以通过直接偏好优化来优先考虑正确的预测。我们用 ANGEL 微调的模型在五个基准上将之前的最佳基线模型的平均 top-1 准确率提高了 1.4%。当将我们的框架纳入预训练时，性能提升进一步提高到 1.7%，这证明了其在预训练和微调阶段的有效性。我们的代码可在 https://github.com/dmis-lab/ANGEL 获得。

##### **Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning**
2408.16482v1 by Rochelle Choenni, Ekaterina Shutova

Improving the alignment of Large Language Models (LLMs) with respect to the
cultural values that they encode has become an increasingly important topic. In
this work, we study whether we can exploit existing knowledge about cultural
values at inference time to adjust model responses to cultural value probes. We
present a simple and inexpensive method that uses a combination of in-context
learning (ICL) and human survey data, and show that we can improve the
alignment to cultural values across 5 models that include both English-centric
and multilingual LLMs. Importantly, we show that our method could prove useful
in test languages other than English and can improve alignment to the cultural
values that correspond to a range of culturally diverse countries.

摘要：改善大型語言模型 (LLM) 與其編碼的文化價值觀之間的一致性已成為越來越重要的主題。在本文中，我們研究是否能利用現有的文化價值觀知識在推論時間調整模型回應以符合文化價值觀探測。我們提出一個簡單且便宜的方法，它結合了情境中學習 (ICL) 和人類調查數據，並展示我們可以改善跨越 5 個模型（包括以英語為中心和多語言的 LLM）的文化價值觀一致性。重要的是，我們展示了我們的方法可能對英語以外的測試語言有用，並且可以改善與各種文化多元國家對應的文化價值觀的一致性。

##### **Is text normalization relevant for classifying medieval charters?**
2408.16446v1 by Florian Atzenhofer-Baumgartner, Tamás Kovács

This study examines the impact of historical text normalization on the
classification of medieval charters, specifically focusing on document dating
and locating. Using a data set of Middle High German charters from a digital
archive, we evaluate various classifiers, including traditional and
transformer-based models, with and without normalization. Our results indicate
that the given normalization minimally improves locating tasks but reduces
accuracy for dating, implying that original texts contain crucial features that
normalization may obscure. We find that support vector machines and gradient
boosting outperform other models, questioning the efficiency of transformers
for this use case. Results suggest a selective approach to historical text
normalization, emphasizing the significance of preserving some textual
characteristics that are critical for classification tasks in document
analysis.

摘要：本研究探討了歷史文本正規化對中世紀憲章分類的影響，特別關注文件的年代測定和定位。我們使用來自數位檔案館的中部高地德語憲章資料集，評估了各種分類器，包括傳統和基於轉換器的模型，是否使用正規化。我們的結果表明，給定的正規化最小程度地改進了定位任務，但降低了年代測定的準確性，這意味著原始文本包含正規化可能會掩蓋的重要特徵。我們發現支持向量機和梯度提升優於其他模型，質疑了轉換器在此用例中的效率。結果表明對歷史文本正規化採取選擇性方法，強調保留某些文本特徵的重要性，這些特徵對於文件分析中的分類任務至關重要。

##### **Integrating Features for Recognizing Human Activities through Optimized Parameters in Graph Convolutional Networks and Transformer Architectures**
2408.16442v1 by Mohammad Belal, Taimur Hassan, Abdelfatah Hassan, Nael Alsheikh, Noureldin Elhendawi, Irfan Hussain

Human activity recognition is a major field of study that employs computer
vision, machine vision, and deep learning techniques to categorize human
actions. The field of deep learning has made significant progress, with
architectures that are extremely effective at capturing human dynamics. This
study emphasizes the influence of feature fusion on the accuracy of activity
recognition. This technique addresses the limitation of conventional models,
which face difficulties in identifying activities because of their limited
capacity to understand spatial and temporal features. The technique employs
sensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD,
LARa, and TUG. The accuracy and F1-score of two deep learning models,
specifically a Transformer model and a Parameter-Optimized Graph Convolutional
Network (PO-GCN), were evaluated using these datasets. The feature fusion
technique integrated the final layer features from both models and inputted
them into a classifier. Empirical evidence demonstrates that PO-GCN outperforms
standard models in activity recognition. HuGaDB demonstrated a 2.3% improvement
in accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in
accuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD
achieved lower accuracies of 64% and 69% respectively. This indicates that the
integration of features enhanced the performance of both the Transformer model
and PO-GCN.

摘要：人類活動識別是運用電腦視覺、機器視覺和深度學習技術來分類人類動作的主要研究領域。深度學習領域已取得顯著進展，其架構在捕捉人類動態方面非常有效。本研究強調特徵融合對活動識別準確性的影響。此技術解決了傳統模型的限制，傳統模型由於理解空間和時間特徵的能力有限，在識別活動方面面臨困難。此技術採用從四個公開可用的數據集取得的感測資料：HuGaDB、PKU-MMD、LARa 和 TUG。使用這些數據集評估了兩個深度學習模型（特別是 Transformer 模型和參數優化的圖形卷積網路 (PO-GCN)）的準確性和 F1 分數。特徵融合技術整合了來自兩個模型的最終層特徵，並將它們輸入分類器。實證證據表明，PO-GCN 在活動識別方面優於標準模型。HuGaDB 的準確性提高了 2.3%，F1 分數提高了 2.2%。TUG 的準確性提高了 5%，F1 分數提高了 0.5%。另一方面，LARa 和 PKU-MMD 分別達到了較低的準確度 64% 和 69%。這表明特徵的整合增強了 Transformer 模型和 PO-GCN 的效能。

##### **Instruction-tuned Large Language Models for Machine Translation in the Medical Domain**
2408.16440v1 by Miguel Rios

Large Language Models (LLMs) have shown promising results on machine
translation for high resource language pairs and domains. However, in
specialised domains (e.g. medical) LLMs have shown lower performance compared
to standard neural machine translation models. The consistency in the machine
translation of terminology is crucial for users, researchers, and translators
in specialised domains. In this study, we compare the performance between
baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we
introduce terminology from specialised medical dictionaries into the
instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs
significantly outperform the baseline models with automatic metrics.

摘要：大型語言模型 (LLM) 在資源豐富的語言對和領域中已展現出機器翻譯的亮眼成果。然而，在專業領域（例如醫學），LLM 的表現與標準神經機器翻譯模型相比，顯得遜色。在專業領域中，機器翻譯術語的一致性對使用者、研究人員和翻譯人員至關重要。在這項研究中，我們比較了基礎 LLM 和針對醫學領域調整指令的 LLM 之間的表現。此外，我們將來自專業醫學詞典的術語引入調整 LLM 的指令格式化資料集中。調整指令的 LLM 在自動評量指標上明顯優於基礎模型。

##### **Gradient-free variational learning with conditional mixture networks**
2408.16429v1 by Conor Heins, Hao Wu, Dimitrije Markovic, Alexander Tschantz, Jeff Beck, Christopher Buckley

Balancing computational efficiency with robust predictive performance is
crucial in supervised learning, especially for critical applications. Standard
deep learning models, while accurate and scalable, often lack probabilistic
features like calibrated predictions and uncertainty quantification. Bayesian
methods address these issues but can be computationally expensive as model and
data complexity increase. Previous work shows that fast variational methods can
reduce the compute requirements of Bayesian methods by eliminating the need for
gradient computation or sampling, but are often limited to simple models. We
demonstrate that conditional mixture networks (CMNs), a probabilistic variant
of the mixture-of-experts (MoE) model, are suitable for fast, gradient-free
inference and can solve complex classification tasks. CMNs employ linear
experts and a softmax gating network. By exploiting conditional conjugacy and
P\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of
both the linear experts and the gating network. This enables efficient
variational updates using coordinate ascent variational inference (CAVI),
avoiding traditional gradient-based optimization. We validate this approach by
training two-layer CMNs on standard benchmarks from the UCI repository. Our
method, CAVI-CMN, achieves competitive and often superior predictive accuracy
compared to maximum likelihood estimation (MLE) with backpropagation, while
maintaining competitive runtime and full posterior distributions over all model
parameters. Moreover, as input size or the number of experts increases,
computation time scales competitively with MLE and other gradient-based
solutions like black-box variational inference (BBVI), making CAVI-CMN a
promising tool for deep, fast, and gradient-free Bayesian networks.

摘要：<paragraph>在監督式學習中，平衡計算效率與穩健的預測效能至關重要，特別是對於關鍵應用程式。標準深度學習模型雖然準確且可擴充，但通常缺乏機率性特徵，例如校準預測和不確定性量化。貝氏方法解決了這些問題，但隨著模型和資料複雜性的增加，在計算上可能很昂貴。先前的研究表明，快速的變異方法可以透過消除對梯度計算或抽樣的需要來降低貝氏方法的運算需求，但通常僅限於簡單的模型。我們證明了條件混合網路 (CMN)，一種混合專家 (MoE) 模型的機率變體，適用於快速、無梯度的推論，並且可以解決複雜的分類任務。CMN 使用線性專家和 softmax 閘控網路。透過利用條件共軛和 P\'olya-Gamma 擴充，我們為線性專家和閘控網路的權重提供高斯似然函數。這可以使用座標上升變異推論 (CAVI) 實現有效的變異更新，避免傳統的基於梯度的最佳化。我們透過在 UCI 儲存庫中的標準基準上訓練兩層 CMN 來驗證此方法。與使用反向傳播的最大似然估計 (MLE) 相比，我們的 CAVI-CMN 方法實現了具有競爭力且通常優異的預測準確度，同時維持所有模型參數的競爭執行時間和完整的後驗分配。此外，隨著輸入大小或專家數量的增加，計算時間與 MLE 和其他基於梯度的解決方案（例如黑盒變異推論 (BBVI)）具有競爭力，這使得 CAVI-CMN 成為深度、快速且無梯度的貝氏網路的有前途的工具。</paragraph>

##### **COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation**
2408.16426v1 by Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, Umar Iqbal

Estimating global human motion from moving cameras is challenging due to the
entanglement of human and camera motions. To mitigate the ambiguity, existing
methods leverage learned human motion priors, which however often result in
oversmoothed motions with misaligned 2D projections. To tackle this problem, we
propose COIN, a control-inpainting motion diffusion prior that enables
fine-grained control to disentangle human and camera motions. Although
pre-trained motion diffusion models encode rich motion priors, we find it
non-trivial to leverage such knowledge to guide global motion estimation from
RGB videos. COIN introduces a novel control-inpainting score distillation
sampling method to ensure well-aligned, consistent, and high-quality motion
from the diffusion prior within a joint optimization framework. Furthermore, we
introduce a new human-scene relation loss to alleviate the scale ambiguity by
enforcing consistency among the humans, camera, and scene. Experiments on three
challenging benchmarks demonstrate the effectiveness of COIN, which outperforms
the state-of-the-art methods in terms of global human motion estimation and
camera motion estimation. As an illustrative example, COIN outperforms the
state-of-the-art method by 33% in world joint position error (W-MPJPE) on the
RICH dataset.

摘要：由於人類和相機動作的糾纏，從移動的相機估計全球人類運動具有挑戰性。為了減輕這種模糊性，現有方法利用學習的人類運動先驗，但這通常會導致 2D 投影錯位的過度平滑運動。為了解決這個問題，我們提出了 COIN，這是一種控制內插運動擴散先驗，可以進行細粒度控制以解開人類和相機運動。儘管預先訓練的運動擴散模型編碼了豐富的運動先驗，但我們發現利用這種知識來指導從 RGB 影片中估計全局運動並非易事。COIN 導入了一種新穎的控制內插分數蒸餾採樣方法，以確保在聯合優化框架內從擴散先驗中獲得對齊良好、一致且高品質的運動。此外，我們引入了一種新的人類場景關係損失，以通過強制人類、相機和場景之間的一致性來緩解規模模糊性。在三個具有挑戰性的基準測試上的實驗證明了 COIN 的有效性，它在全局人類運動估計和相機運動估計方面優於最先進的方法。作為一個說明性範例，COIN 在 RICH 資料集上的世界關節位置誤差 (W-MPJPE) 方面比最先進的方法高出 33%。

##### **MQM-Chat: Multidimensional Quality Metrics for Chat Translation**
2408.16390v1 by Yunmeng Li, Jun Suzuki, Makoto Morishita, Kaori Abe, Kentaro Inui

The complexities of chats pose significant challenges for machine translation
models. Recognizing the need for a precise evaluation metric to address the
issues of chat translation, this study introduces Multidimensional Quality
Metrics for Chat Translation (MQM-Chat). Through the experiments of five models
using MQM-Chat, we observed that all models generated certain fundamental
errors, while each of them has different shortcomings, such as omission, overly
correcting ambiguous source content, and buzzword issues, resulting in the loss
of stylized information. Our findings underscore the effectiveness of MQM-Chat
in evaluating chat translation, emphasizing the importance of stylized content
and dialogue consistency for future studies.

摘要：聊天對話的複雜性對機器翻譯模型構成重大挑戰。為了滿足對精確評估指標的需求以解決聊天翻譯的問題，本研究引入了聊天翻譯的多維品質指標 (MQM-Chat)。透過使用 MQM-Chat 對五個模型進行實驗，我們觀察到所有模型都產生某些基本錯誤，而每個模型都有不同的缺點，例如遺漏、過度修正模稜兩可的原始內容和流行語問題，導致風格化資訊遺失。我們的研究結果強調了 MQM-Chat 在評估聊天翻譯中的有效性，強調了風格化內容和對話一致性對未來研究的重要性。

##### **DetectBERT: Towards Full App-Level Representation Learning to Detect Android Malware**
2408.16353v1 by Tiezhu Sun, Nadia Daoudi, Kisub Kim, Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein

Recent advancements in ML and DL have significantly improved Android malware
detection, yet many methodologies still rely on basic static analysis,
bytecode, or function call graphs that often fail to capture complex malicious
behaviors. DexBERT, a pre-trained BERT-like model tailored for Android
representation learning, enriches class-level representations by analyzing
Smali code extracted from APKs. However, its functionality is constrained by
its inability to process multiple Smali classes simultaneously. This paper
introduces DetectBERT, which integrates correlated Multiple Instance Learning
(c-MIL) with DexBERT to handle the high dimensionality and variability of
Android malware, enabling effective app-level detection. By treating
class-level features as instances within MIL bags, DetectBERT aggregates these
into a comprehensive app-level representation. Our evaluation demonstrates that
DetectBERT not only surpasses existing state-of-the-art detection methods but
also adapts to evolving malware threats. Moreover, the versatility of the
DetectBERT framework holds promising potential for broader applications in
app-level analysis and other software engineering tasks, offering new avenues
for research and development.

摘要：最近在機器學習和深度學習的進展已顯著改善 Android 惡意軟體偵測，但許多方法仍依賴於基本的靜態分析、位元組碼或函式呼叫圖，這些方法通常無法擷取複雜的惡意行為。DexBERT 是一個針對 Android 表徵學習量身打造的預訓練 BERT 類似模型，透過分析從 APK 提取的 Smali 程式碼來豐富類別層級的表徵。然而，其功能受到無法同時處理多個 Smali 類別的限制。本文介紹 DetectBERT，它將相關的多重實例學習 (c-MIL) 與 DexBERT 整合，以處理 Android 惡意軟體的高維度和變異性，進而實現有效的應用程式層級偵測。透過將類別層級特徵視為 MIL 袋中的實例，DetectBERT 將這些特徵彙總成一個全面的應用程式層級表徵。我們的評估證明，DetectBERT 不僅超越現有的最先進偵測方法，還能適應不斷變化的惡意軟體威脅。此外，DetectBERT 框架的多功能性在應用程式層級分析和其他軟體工程任務中具有廣泛應用的潛力，為研究和開發提供了新的途徑。

##### **The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization**
2408.16345v1 by Luka Borec, Philipp Sadler, David Schlangen

This work analyses the text memorization behavior of large language models
(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like
nucleus sampling are typically applied to overcome issues such as monotonous
and repetitive text generation, which are often observed with
maximization-based decoding techniques. We hypothesize that nucleus sampling
might also reduce the occurrence of memorization patterns, because it could
lead to the selection of tokens outside the memorized sequence. To test this
hypothesis we create a diagnostic dataset with a known distribution of
duplicates that gives us some control over the likelihood of memorization of
certain parts of the training data. Our analysis of two GPT-Neo models
fine-tuned on this dataset interestingly shows that (i) an increase of the
nucleus size reduces memorization only modestly, and (ii) even when models do
not engage in "hard" memorization -- a verbatim reproduction of training
samples -- they may still display "soft" memorization whereby they generate
outputs that echo the training data but without a complete one-by-one
resemblance.

摘要：本研究分析了大型語言模型 (LLM) 在接受核取樣時，其文字記憶行為。隨機解碼方法（如核取樣）通常用於克服單調且重複的文字生成等問題，而這些問題通常在基於最大化的解碼技術中可見。我們假設核取樣也可能減少記憶模式的發生，因為它可能導致選擇記憶序列之外的符號。為了測試這個假設，我們建立了一個診斷資料集，其中包含已知的重複分佈，讓我們可以控制訓練資料特定部分的記憶可能性。我們對兩個在這個資料集上進行微調的 GPT-Neo 模型的分析有趣地顯示出：(i) 增加核取樣大小僅能適度減少記憶，以及 (ii) 即使模型沒有從事「硬」記憶（訓練樣本的逐字重現），它們仍然可能顯示「軟」記憶，即它們生成的輸出呼應訓練資料，但沒有完全逐一相似。

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

摘要：阿茲海默症 (AD) 是一種複雜的神經退化性疾病，特徵是記憶力喪失、執行功能障礙和人格改變。由於症狀微妙且表現形式多樣，早期診斷具有挑戰性，通常由於傳統單模態診斷方法的範圍有限而導致誤診。本研究引入了一個先進的多模態分類模型，它整合了臨床、認知、神經影像和腦電圖數據，以提高診斷準確性。該模型結合了一個具有表格數據編碼架構的特徵標籤器，並利用 TimesBlock 模組來捕捉腦電圖 (EEG) 數據中的複雜時間模式。通過採用跨模態注意力聚合模組，該模型有效地融合了磁共振成像 (MRI) 空間資訊和腦電圖時間數據，顯著改善了 AD、輕度認知障礙和正常認知之間的區別。同時，我們構建了第一個 AD 分類數據集，其中包含三種模態：腦電圖、磁共振成像和表格數據。我們的創新方法旨在促進早期診斷和干預，潛在地減緩 AD 的進展。原始碼和我們的私人 ADMC 數據集可在 https://github.com/JustlfC03/MSTNet 獲得。

##### **Self-Improving Diffusion Models with Synthetic Data**
2408.16333v1 by Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti Agarwal, John Collomosse, Richard Baraniuk

The artificial intelligence (AI) world is running out of real data for
training increasingly large generative models, resulting in accelerating
pressure to train on synthetic data. Unfortunately, training new generative
models with synthetic data from current or past generation models creates an
autophagous (self-consuming) loop that degrades the quality and/or diversity of
the synthetic data in what has been termed model autophagy disorder (MAD) and
model collapse. Current thinking around model autophagy recommends that
synthetic data is to be avoided for model training lest the system deteriorate
into MADness. In this paper, we take a different tack that treats synthetic
data differently from real data. Self-IMproving diffusion models with Synthetic
data (SIMS) is a new training concept for diffusion models that uses
self-synthesized data to provide negative guidance during the generation
process to steer a model's generative process away from the non-ideal synthetic
data manifold and towards the real data distribution. We demonstrate that SIMS
is capable of self-improvement; it establishes new records based on the
Fr\'echet inception distance (FID) metric for CIFAR-10 and ImageNet-64
generation and achieves competitive results on FFHQ-64 and ImageNet-512.
Moreover, SIMS is, to the best of our knowledge, the first prophylactic
generative AI algorithm that can be iteratively trained on self-generated
synthetic data without going MAD. As a bonus, SIMS can adjust a diffusion
model's synthetic data distribution to match any desired in-domain target
distribution to help mitigate biases and ensure fairness.

摘要：人工智慧（AI）的世界正逐漸耗盡用於訓練越來越大型生成模型的真實資料，導致對合成資料訓練的需求急遽增加。不幸的是，使用現有或過往世代模型的合成資料訓練新的生成模型，會產生一種自我消耗的迴圈，進而降低合成資料的品質和/或多樣性，這種現象稱為模型自噬症（MAD）和模型崩潰。目前關於模型自噬症的觀點建議避免在模型訓練中使用合成資料，以免系統惡化為 MAD。在本文中，我們採取不同的策略，將合成資料與真實資料區別對待。使用合成資料的自改善擴散模型（SIMS）是一種新的擴散模型訓練概念，它使用自我合成的資料在生成過程中提供負面指導，以引導模型的生成過程遠離非理想的合成資料流形，並朝向真實資料分佈。我們證明了 SIMS 具備自我改善的能力；它根據 CIFAR-10 和 ImageNet-64 生成建立了基於 Fréchet 起始距離（FID）指標的新紀錄，並在 FFHQ-64 和 ImageNet-512 上取得了具有競爭力的結果。此外，據我們所知，SIMS 是第一個可以反覆訓練在自我生成的合成資料上而不會發瘋的預防性生成式 AI 演算法。作為一個額外的優點，SIMS 可以調整擴散模型的合成資料分佈，以匹配任何所需的域內目標分佈，以幫助減輕偏差並確保公平性。

##### **Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic**
2408.16326v1 by Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun

Self-critic has become an important mechanism for enhancing the reasoning
performance of LLMs. However, current approaches mainly involve basic prompts
without further training, which tend to be over-simplified, leading to limited
accuracy.Moreover, there is a lack of in-depth investigation of the
relationship between LLM's ability to criticism and its task-solving
performance.To address these issues, we propose Critic-CoT, a novel framework
that pushes LLMs toward System-2-like critic capability, via step-wise CoT
reasoning format and distant-supervision data construction, without the need
for human annotation. Experiments on GSM8K and MATH show that via filtering out
invalid solutions or iterative refinement, our enhanced model boosts
task-solving performance, which demonstrates the effectiveness of our method.
Further, we find that training on critique and refinement alone improves the
generation. We hope our work could shed light on future research on improving
the reasoning and critic ability of LLMs.

摘要：自我批評已成為增強 LLM 推理效能的重要機制。然而，目前的做法主要涉及基本提示，而無需進一步訓練，這往往過於簡化，導致準確性有限。此外，對於 LLM 批評能力與其任務解決效能之間的關係，缺乏深入探討。為了解決這些問題，我們提出了 Critic-CoT，一個新穎的框架，透過逐步 CoT 推理格式和遠程監督數據建構，將 LLM 推向類似系統 2 的批評能力，而無需人工註解。在 GSM8K 和 MATH 上的實驗表明，透過過濾無效解或迭代改進，我們增強的模型提升了任務解決效能，這證明了我們方法的有效性。此外，我們發現僅針對批評和改進進行訓練就能改善生成。我們希望我們的研究成果能為未來關於改善 LLM 推理和批評能力的研究提供啟發。

##### **FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm Based On FMDS and AGMF Modules**
2408.16313v1 by Yukang Huo, Mingyuan Yao, Qingbin Tian, Tonghao Wang, Ruifeng Wang, Haihua Wang

Over the past few years, the YOLO series of models has emerged as one of the
dominant methodologies in the realm of object detection. Many studies have
advanced these baseline models by modifying their architectures, enhancing data
quality, and developing new loss functions. However, current models still
exhibit deficiencies in processing feature maps, such as overlooking the fusion
of cross-scale features and a static fusion approach that lacks the capability
for dynamic feature adjustment. To address these issues, this paper introduces
an efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module),
which applies a more effective dynamic feature selection and fusion method on
fine-grained multi-scale feature maps, significantly enhancing the detection
accuracy of small, medium, and large-sized targets in complex environments.
Furthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion
Module (AGMF Module), which utilizes multiple parallel branches to perform
complementary fusion of various features captured by the gated unit branch,
FMDS Module branch, and TripletAttention branch. This approach further enhances
the comprehensiveness, diversity, and integrity of feature fusion. This paper
has integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel
object detection model named FA-YOLO. Extensive experimental results show that
under identical experimental conditions, FA-YOLO achieves an outstanding 66.1%
mean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0%
improvement over YOLOv9's 65.1%. Additionally, the detection accuracies of
FA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%,
respectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's
42.1%, 51.5%, and 69.9%.

摘要：<paragraph>在過去幾年中，YOLO 系列模型已成為目標偵測領域中其中一種主要方法。許多研究透過修改架構、增強資料品質以及開發新的損失函數來提升這些基準模型。然而，目前的模型在處理特徵圖時仍存在缺陷，例如忽略跨尺度特徵的融合，以及缺乏動態特徵調整能力的靜態融合方法。為了解決這些問題，本文提出了一個高效的細粒度多尺度動態選擇模組（FMDS 模組），它在細粒度的多尺度特徵圖上應用更有效的動態特徵選擇和融合方法，大幅提升了在複雜環境中偵測小型、中型和大型目標的準確度。此外，本文提出了一個自適應閘控多支線焦點融合模組（AGMF 模組），它利用多個並行支線來執行閘控單元支線、FMDS 模組支線和 TripletAttention 支線所擷取的各種特徵的互補融合。這種方法進一步增強了特徵融合的全面性、多樣性和完整性。本文已將 FMDS 模組、AGMF 模組整合到 Yolov9 中，開發了一個名為 FA-YOLO 的新目標偵測模型。廣泛的實驗結果顯示，在相同的實驗條件下，FA-YOLO 在 PASCAL VOC 2007 資料集上達到了 66.1% 的出色平均準確度（mAP），比 YOLOv9 的 65.1% 提升了 1.0%。此外，FA-YOLO 對小型、中型和大型目標的偵測準確度分別為 44.1%、54.6% 和 70.8%，與 YOLOv9 的 42.1%、51.5% 和 69.9% 相比，分別提升了 2.0%、3.1% 和 0.9%。</paragraph>

##### **Safe Bayesian Optimization for High-Dimensional Control Systems via Additive Gaussian Processes**
2408.16307v1 by Hongxuan Wang, Xiaocong Li, Adrish Bhaumik, Prahlad Vadakkepat

Controller tuning and optimization have been among the most fundamental
problems in robotics and mechatronic systems. The traditional methodology is
usually model-based, but its performance heavily relies on an accurate
mathematical model of the system. In control applications with complex
dynamics, obtaining a precise model is often challenging, leading us towards a
data-driven approach. While optimizing a single controller has been explored by
various researchers, it remains a challenge to obtain the optimal controller
parameters safely and efficiently when multiple controllers are involved. In
this paper, we propose a high-dimensional safe Bayesian optimization method
based on additive Gaussian processes to optimize multiple controllers
simultaneously and safely. Additive Gaussian kernels replace the traditional
squared-exponential kernels or Mat\'ern kernels, enhancing the efficiency with
which Gaussian processes update information on unknown functions. Experimental
results on a permanent magnet synchronous motor (PMSM) demonstrate that
compared to existing safe Bayesian optimization algorithms, our method can
obtain optimal parameters more efficiently while ensuring safety.

摘要：控制器調校和最佳化一直是機器人和機電系統中最基本的
問題。傳統方法通常是基於模型，但其效能嚴重依賴於系統的準確
數學模型。在控制應用中，動態複雜，取得精確模型通常具有挑戰性，
導致我們朝向資料驅動的方法。雖然最佳化單一控制器已由各研究人員
探索，但當涉及多個控制器時，安全且有效地取得最佳控制器參數仍
然是一項挑戰。在本文中，我們提出一個基於加法高斯過程的高維度安全
貝氏最佳化方法，以同時且安全地最佳化多個控制器。加法高斯核取代
傳統的平方指數核或 Mat\'ern 核，增強高斯過程更新未知函數資訊的
效率。永磁同步電動機 (PMSM) 的實驗結果證明，與現有的安全貝氏最佳
化演算法相比，我們的演算法可以在確保安全性的同時更有效地取得最佳
參數。

##### **Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems**
2408.16293v1 by Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu

Language models have demonstrated remarkable performance in solving reasoning
tasks; however, even the strongest models still occasionally make reasoning
mistakes. Recently, there has been active research aimed at improving reasoning
accuracy, particularly by using pretrained language models to "self-correct"
their mistakes via multi-round prompting. In this paper, we follow this line of
work but focus on understanding the usefulness of incorporating
"error-correction" data directly into the pretraining stage. This data consists
of erroneous solution steps immediately followed by their corrections. Using a
synthetic math dataset, we show promising results: this type of pretrain data
can help language models achieve higher reasoning accuracy directly (i.e.,
through simple auto-regression, without multi-round prompting) compared to
pretraining on the same amount of error-free data. We also delve into many
details, such as (1) how this approach differs from beam search, (2) how such
data can be prepared, (3) whether masking is needed on the erroneous tokens,
(4) the amount of error required, (5) whether such data can be deferred to the
fine-tuning stage, and many others.

摘要：語言模型在解決推理任務方面展示了卓越的性能；然而，即使是最強大的模型偶爾也會犯推理錯誤。最近，有積極的研究旨在提高推理準確性，特別是通過使用預訓練語言模型通過多輪提示來「自我修正」錯誤。在本文中，我們遵循這一思路，但重點在於了解將「錯誤更正」資料直接納入預訓練階段的有用性。此資料包含錯誤的解題步驟，其後緊接著是更正。使用合成數學資料集，我們展示了有希望的結果：與在相同數量的無錯誤資料上進行預訓練相比，此類型的預訓練資料可以幫助語言模型直接（即通過簡單的自動迴歸，無需多輪提示）實現更高的推理準確性。我們還深入探討了許多細節，例如 (1) 此方法與波束搜尋有何不同，(2) 如何準備此類資料，(3) 是否需要對錯誤的標記進行遮罩，(4) 所需的錯誤量，(5) 此類資料是否可以推遲到微調階段，以及許多其他方面。

##### **Measuring the Accuracy of Automatic Speech Recognition Solutions**
2408.16287v1 by Korbinian Kuhn, Verena Kersken, Benedikt Reuter, Niklas Egger, Gottfried Zimmermann

For d/Deaf and hard of hearing (DHH) people, captioning is an essential
accessibility tool. Significant developments in artificial intelligence (AI)
mean that Automatic Speech Recognition (ASR) is now a part of many popular
applications. This makes creating captions easy and broadly available - but
transcription needs high levels of accuracy to be accessible. Scientific
publications and industry report very low error rates, claiming AI has reached
human parity or even outperforms manual transcription. At the same time the DHH
community reports serious issues with the accuracy and reliability of ASR.
There seems to be a mismatch between technical innovations and the real-life
experience for people who depend on transcription. Independent and
comprehensive data is needed to capture the state of ASR. We measured the
performance of eleven common ASR services with recordings of Higher Education
lectures. We evaluated the influence of technical conditions like streaming,
the use of vocabularies, and differences between languages. Our results show
that accuracy ranges widely between vendors and for the individual audio
samples. We also measured a significant lower quality for streaming ASR, which
is used for live events. Our study shows that despite the recent improvements
of ASR, common services lack reliability in accuracy.

摘要：對於失聰/聽障人士 (DHH)，字幕是必要的輔助工具。人工智慧 (AI) 的重大發展，代表自動語音辨識 (ASR) 已成為許多熱門應用程式的一部分。這使得建立字幕變得容易且廣泛可用，但要能被存取，轉錄需要高度準確性。科學刊物和產業報告指出極低的錯誤率，聲稱 AI 已達到人類同等水平，甚至優於人工轉錄。同時，DHH 社群報告 ASR 的準確性和可靠性有嚴重問題。對於依賴轉錄的人來說，技術創新與實際體驗之間似乎存在落差。需要獨立且全面的資料來掌握 ASR 的狀態。我們使用高等教育課程的錄音測量了 11 項常見 ASR 服務的效能。我們評估了串流等技術條件、詞彙的使用以及語言之間的差異。我們的結果顯示，準確性在供應商和個別音訊樣本之間差異很大。我們還測量了串流 ASR 的品質顯著較低，而這用於現場活動。我們的研究顯示，儘管 ASR 近期有進步，但常見服務在準確性方面缺乏可靠性。

##### **Enhancing AI-Driven Psychological Consultation: Layered Prompts with Large Language Models**
2408.16276v1 by Rafael Souza, Jia-Hao Lim, Alexander Davis

Psychological consultation is essential for improving mental health and
well-being, yet challenges such as the shortage of qualified professionals and
scalability issues limit its accessibility. To address these challenges, we
explore the use of large language models (LLMs) like GPT-4 to augment
psychological consultation services. Our approach introduces a novel layered
prompting system that dynamically adapts to user input, enabling comprehensive
and relevant information gathering. We also develop empathy-driven and
scenario-based prompts to enhance the LLM's emotional intelligence and
contextual understanding in therapeutic settings. We validated our approach
through experiments using a newly collected dataset of psychological
consultation dialogues, demonstrating significant improvements in response
quality. The results highlight the potential of our prompt engineering
techniques to enhance AI-driven psychological consultation, offering a scalable
and accessible solution to meet the growing demand for mental health support.

摘要：心理諮詢對於提升心理健康和幸福至關重要，但合格專業人員短缺和可擴充性問題等挑戰限制了其可及性。為了應對這些挑戰，我們探討了利用 GPT-4 等大型語言模型 (LLM) 來擴充心理諮詢服務。我們的做法引入了一個新穎的分層提示系統，可動態適應使用者輸入，實現全面且相關的資訊收集。我們還開發了同理心驅動和基於情境的提示，以增強 LLM 在治療環境中的情緒智力和情境理解。我們透過使用新收集的心理諮詢對話資料集進行實驗驗證了我們的做法，證明了回應品質有顯著改善。結果突顯了我們的提示工程技術在增強 AI 驅動的心理諮詢方面的潛力，提供了一個可擴充且可存取的解決方案，以滿足日益增長的心理健康支援需求。

##### **Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding**
2408.16272v1 by Kaijing Ma, Haojian Huang, Jin Chen, Haodong Chen, Pengliang Ji, Xianghao Zang, Han Fang, Chao Ban, Hao Sun, Mulin Chen, Xuelong Li

Existing Video Temporal Grounding (VTG) models excel in accuracy but often
overlook open-world challenges posed by open-vocabulary queries and untrimmed
videos. This leads to unreliable predictions for noisy, corrupted, and
out-of-distribution data. Adapting VTG models to dynamically estimate
uncertainties based on user input can address this issue. To this end, we
introduce SRAM, a robust network module that benefits from a two-stage
cross-modal alignment task. More importantly, it integrates Deep Evidential
Regression (DER) to explicitly and thoroughly quantify uncertainty during
training, thus allowing the model to say "I do not know" in scenarios beyond
its handling capacity. However, the direct application of traditional DER
theory and its regularizer reveals structural flaws, leading to unintended
constraints in VTG tasks. In response, we develop a simple yet effective
Geom-regularizer that enhances the uncertainty learning framework from the
ground up. To the best of our knowledge, this marks the first successful
attempt of DER in VTG. Our extensive quantitative and qualitative results
affirm the effectiveness, robustness, and interpretability of our modules and
the uncertainty learning paradigm in VTG tasks. The code will be made
available.

摘要：現有的影片時序定位 (VTG) 模型在準確度方面表現出色，但通常會忽略開放詞彙查詢和未修剪影片所帶來的開放世界挑戰。這會導致對雜訊、損毀和分佈外資料進行不可靠的預測。根據使用者輸入動態估計不確定性，以適應 VTG 模型可以解決此問題。為此，我們引入了 SRAM，這是一個健全的網路模組，受益於兩階段跨模態對齊任務。更重要的是，它整合了深度證據回歸 (DER)，以在訓練期間明確且徹底地量化不確定性，從而允許模型在超出其處理能力的情況下說「我不知道」。然而，傳統 DER 理論及其正則化的直接應用揭示了結構缺陷，導致 VTG 任務中出現意外的約束。為了解決此問題，我們開發了一個簡單但有效的幾何正則化器，從頭開始增強不確定性學習架構。據我們所知，這是 DER 在 VTG 中首次成功嘗試。我們廣泛的量化和定性結果肯定了我們的模組和 VTG 任務中不確定性學習範例的有效性、健全性和可解釋性。程式碼將會提供。

##### **LoraMap: Harnessing the Power of LoRA Connections**
2408.16264v1 by Hyeryun Park, Jeongwon Kwak, Dongsuk Jang, Sumin Park, Jinwook Choi

Large Language Models (LLMs) can benefit from mitigating hallucinations
through fact-checking and overcoming substantial computational overhead with
parameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some
studies have explored the parallel integration of multiple LoRAs, these
approaches need attention to the connections between them. This paper
investigates methods to establish connections among multiple LoRAs. We create
three reasoning datasets tailored to fact-checking and fine-tune individual
LoRAs, allowing them to view and reason from diverse perspectives. Then, we
explore strategies for allocating these reasoning LoRAs and introduce LoraMap,
an approach to map connections between them. The results on the fact-checking
task demonstrate that the performance of LoraMap is superior to LoraHub, an
existing LoRA composition method. LoraMap also outperforms with significantly
fewer parameters than LoraConcat, which concatenates LoRAs and further
fine-tunes them.

摘要：大型语言模型 (LLM) 可受益于通过事实检查来减轻幻觉，并通过参数高效技术（例如低秩适应 (LoRA)）克服大量的计算开销。虽然一些研究探索了多个 LoRA 的并行集成，但这些方法需要关注它们之间的连接。本文调查了建立多个 LoRA 之间连接的方法。我们创建了三个针对事实检查量身定制的推理数据集，并对各个 LoRA 进行微调，使它们能够从不同的角度查看和推理。然后，我们探索分配这些推理 LoRA 的策略，并介绍 LoraMap，这是一种映射它们之间连接的方法。事实检查任务的结果表明，LoraMap 的性能优于 LoraHub，这是一种现有的 LoRA 组合方法。LoraMap 也优于 LoraConcat，后者连接 LoRA 并进一步对它们进行微调，而参数却少得多。

##### **Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models**
2408.16261v1 by Sekitoshi Kanai, Yasutoshi Ida, Kazuki Adachi, Mihiro Uchida, Tsukasa Yoshida, Shin'ya Yamaguchi

This study investigates a method to evaluate time-series datasets in terms of
the performance of deep neural networks (DNNs) with state space models (deep
SSMs) trained on the dataset. SSMs have attracted attention as components
inside DNNs to address time-series data. Since deep SSMs have powerful
representation capacities, training datasets play a crucial role in solving a
new task. However, the effectiveness of training datasets cannot be known until
deep SSMs are actually trained on them. This can increase the cost of data
collection for new tasks, as a trial-and-error process of data collection and
time-consuming training are needed to achieve the necessary performance. To
advance the practical use of deep SSMs, the metric of datasets to estimate the
performance early in the training can be one key element. To this end, we
introduce the concept of data evaluation methods used in system identification.
In system identification of linear dynamical systems, the effectiveness of
datasets is evaluated by using the spectrum of input signals. We introduce this
concept to deep SSMs, which are nonlinear dynamical systems. We propose the
K-spectral metric, which is the sum of the top-K spectra of signals inside deep
SSMs, by focusing on the fact that each layer of a deep SSM can be regarded as
a linear dynamical system. Our experiments show that the K-spectral metric has
a large absolute value of the correlation coefficient with the performance and
can be used to evaluate the quality of training datasets.

摘要：本研究探討一種方法，以狀態空間模型 (deep SSM) 訓練資料集，來評估時間序列資料集在深度神經網路 (DNN) 上的效能。SSMs 作為 DNN 中的元件，在處理時間序列資料時備受關注。由於深度 SSM 具有強大的表示能力，因此訓練資料集在解決新任務時扮演著至關重要的角色。然而，在深度 SSM 實際訓練資料集之前，無法得知訓練資料集的有效性。這可能會增加新任務資料收集的成本，因為需要透過資料收集的試錯過程和耗時的訓練，才能達到必要的效能。為了促進深度 SSM 的實際應用，資料集的指標可以作為一個關鍵元素，在訓練早期估計效能。為此，我們引入了系統辨識中使用的資料評估方法的概念。在線性動態系統的系統辨識中，資料集的有效性是透過輸入訊號的頻譜來評估。我們將此概念引入非線性動態系統的深度 SSM。我們提出了 K 頻譜指標，這是深度 SSM 內訊號的前 K 個頻譜的總和，重點在於深度 SSM 的每一層都可以視為線性動態系統。我們的實驗顯示，K 頻譜指標與效能之間具有很大的相關係數絕對值，可用來評估訓練資料集的品質。

##### **Making the Most of your Model: Methods for Finetuning and Applying Pretrained Transformers**
2408.16241v1 by Davis Yoshida

This thesis provides methods and analysis of models which make progress on
this goal. The techniques outlined are task agnostic, and should provide
benefit when used with nearly any transformer LM. We introduce two new
finetuning methods which add new capabilities to the models they are used on.
The first adds a recurrence mechanism, which removes the fixed-window sized
constraint and improves the efficiency of a transformer decoder. The second
allows masked language models (MLMs) to be used for initialization of both the
encoder and decoder of a non-autoregressive sequence-to-sequence transformer,
opening up generative applications of models which were previously only used
for natural language understanding tasks.
  We also introduce two new techniques for improving the quality of predictions
of any transformer decoder without additional finetuning. One, hidden state
optimization, can be applied to any transformer decoder to improve the quality
of predictions at inference time, especially for few-shot classification. The
other, conditional beam search, allows practitioners to search for natural
language generation (NLG) model outputs with high likelihood while conditioning
on the event that the output is not degenerate (e.g. empty, repetitive, etc.).
  Finally, we provide theoretical and empirical insights on the divergence of
model-likelihood and output quality which has widely been observed in prior
work. These insights apply to any model which represents a distribution over
text, and apply to language models which are not transformers or even
autoregressive. We argue that the NLP community has, to some extent,
misunderstood the implications of these findings, and encourage a point of view
which has more nuance.

摘要：<paragraph>本論文提供了方法並分析了在這個目標上取得進展的模型。所概述的技術與任務無關，且在與幾乎任何Transformer LM 搭配使用時，應可帶來好處。我們引入了兩種新的微調方法，可為其所使用的模型添加新功能。第一種方法添加了遞迴機制，消除了固定視窗大小的約束，並提升了Transformer解碼器的效率。第二種方法允許將遮蔽語言模型 (MLM) 用於非自迴歸序列對序列Transformer的編碼器和解碼器的初始化，開啟了先前僅用於自然語言理解任務的模型的生成應用。
  我們還引入了兩種新技術，用於提升任何Transformer解碼器的預測品質，而無需額外的微調。一種是隱藏狀態最佳化，可應用於任何Transformer解碼器，以提升推論時間的預測品質，特別是對於小樣本分類。另一種是條件光束搜尋，允許實務工作者在輸出不簡約（例如，空、重複等）的事件條件下，搜尋具有高可能性的自然語言生成 (NLG) 模型輸出。
  最後，我們提供了關於模型似然性與輸出品質差異的理論和經驗見解，這在先前的研究中已被廣泛觀察到。這些見解適用於任何表示文字分佈的模型，並適用於非Transformer，甚至是非自迴歸的語言模型。我們認為，NLP 社群在某種程度上誤解了這些發現的含意，並鼓勵採取觀點，讓觀點更具細微差別。</paragraph>

##### **Enhancing Conditional Image Generation with Explainable Latent Space Manipulation**
2408.16232v1 by Kshitij Pathania

In the realm of image synthesis, achieving fidelity to a reference image
while adhering to conditional prompts remains a significant challenge. This
paper proposes a novel approach that integrates a diffusion model with latent
space manipulation and gradient-based selective attention mechanisms to address
this issue. Leveraging Grad-SAM (Gradient-based Selective Attention
Manipulation), we analyze the cross attention maps of the cross attention
layers and gradients for the denoised latent vector, deriving importance scores
of elements of denoised latent vector related to the subject of interest. Using
this information, we create masks at specific timesteps during denoising to
preserve subjects while seamlessly integrating the reference image features.
This approach ensures the faithful formation of subjects based on conditional
prompts, while concurrently refining the background for a more coherent
composition. Our experiments on places365 dataset demonstrate promising
results, with our proposed model achieving the lowest mean and median Frechet
Inception Distance (FID) scores compared to baseline models, indicating
superior fidelity preservation. Furthermore, our model exhibits competitive
performance in aligning the generated images with provided textual
descriptions, as evidenced by high CLIP scores. These results highlight the
effectiveness of our approach in both fidelity preservation and textual context
preservation, offering a significant advancement in text-to-image synthesis
tasks.

摘要：在图像合成领域中，在遵守条件提示的同时实现对参考图像的保真度仍然是一项重大挑战。本文提出了一种新颖的方法，该方法将扩散模型与潜在空间操作和基于梯度的选择性注意机制相结合来解决此问题。利用 Grad-SAM（基于梯度的选择性注意操作），我们分析了去噪潜在向量的交叉注意力层和梯度的交叉注意力图，得出与感兴趣主题相关的去噪潜在向量元素的重要性分数。利用此信息，我们在去噪过程中在特定时间步长创建掩码，以在无缝集成参考图像特征的同时保留主题。这种方法确保了基于条件提示对主题进行忠实地形成，同时还细化了背景以获得更连贯的构图。我们在 places365 数据集上的实验展示了有希望的结果，与基线模型相比，我们提出的模型实现了最低的平均和中值 Fréchet 起始距离 (FID) 分数，表明了卓越的保真度保留。此外，我们的模型在使生成的图像与提供的文本描述保持一致方面表现出了竞争力，正如高 CLIP 分数所证明的那样。这些结果突出了我们方法在保真度保留和文本上下文保留方面的有效性，为文本到图像合成任务提供了一项重大进展。

##### **LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**
2408.16224v2 by Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng

Recent advances in large vision-language models (VLMs) typically employ
vision encoders based on the Vision Transformer (ViT) architecture. The
division of the images into patches by ViT results in a fragmented perception,
thereby hindering the visual understanding capabilities of VLMs. In this paper,
we propose an innovative enhancement to address this limitation by introducing
a Scene Graph Expression (SGE) module in VLMs. This module extracts and
structurally expresses the complex semantic information within images, thereby
improving the foundational perception and understanding abilities of VLMs.
Extensive experiments demonstrate that integrating our SGE module significantly
enhances the VLM's performance in vision-language tasks, indicating its
effectiveness in preserving intricate semantic details and facilitating better
visual understanding.

摘要：近來大型視覺語言模型 (VLM) 的進展通常採用基於視覺轉換器 (ViT) 架構的視覺編碼器。ViT 將影像分割成區塊會造成破碎的感知，從而阻礙 VLM 的視覺理解能力。在本文中，我們提出了一項創新的增強功能，透過在 VLM 中引入場景圖表達 (SGE) 模組來解決此限制。此模組會萃取影像中的複雜語意資訊並以結構化的方式表達，從而改善 VLM 的基礎感知和理解能力。廣泛的實驗證明，整合我們的 SGE 模組能顯著提升 VLM 在視覺語言任務中的效能，表示它在保留複雜的語意細節和促進更好的視覺理解方面很有效。

##### **SSDM: Scalable Speech Dysfluency Modeling**
2408.16221v1 by Jiachen Lian, Xuanru Zhou, Zoe Ezzes, Jet Vonk, Brittany Morin, David Baquirin, Zachary Mille, Maria Luisa Gorno Tempini, Gopala Anumanchipalli

Speech dysfluency modeling is the core module for spoken language learning,
and speech therapy. However, there are three challenges. First, current
state-of-the-art solutions suffer from poor scalability. Second, there is a
lack of a large-scale dysfluency corpus. Third, there is not an effective
learning framework. In this paper, we propose \textit{SSDM: Scalable Speech
Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced
alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve
dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus
called Libri-Dys; and (4) develops an end-to-end system by leveraging the power
of large language models (LLMs). We expect SSDM to serve as a standard in the
area of dysfluency modeling. Demo is available at
\url{https://eureka235.github.io}.

摘要：語音不流暢建模是口語學習和語言治療的核心模組，然而，有三個挑戰。第一，目前的最新解決方案可擴充性不佳。第二，缺乏大規模的不流暢語料庫。第三，沒有有效的學習架構。在本文中，我們提出「可擴充式語音不流暢建模 (SSDM)」，它 (1) 採用發音手勢作為可擴充式強制對齊；(2) 導入連接主義子序列對齊器 (CSA) 以達成不流暢對齊；(3) 導入名為 Libri-Dys 的大規模模擬不流暢語料庫；(4) 透過利用大型語言模型 (LLM) 的功能來開發端到端系統。我們預期 SSDM 將成為不流暢建模領域的標準。示範可在網址 \url{https://eureka235.github.io} 取得。

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

摘要：人工智慧的快速發展，特別是在大型語言模型 (LLM) 中，已對包括醫療保健在內的各個領域產生重大影響。在胸部 X 光 (CXR) 分析中，先前的研究已採用 LLM，但有其限制：不是未能充分利用 LLM 的多任務處理能力，就是缺乏臨床準確性。本文提出 M4CXR，一種多模態 LLM，旨在增強 CXR 解釋。該模型訓練於視覺指令遵循資料集，其中以對話格式整合各種特定任務資料集。因此，該模型支援多項任務，例如醫療報告產生 (MRG)、視覺基礎和視覺問題回答 (VQA)。M4CXR 透過採用思考鏈提示策略，在 MRG 中達成最先進的臨床準確性，其中它會識別 CXR 影像中的發現，並隨後產生對應的報告。該模型可根據可用輸入（例如單一影像、多重影像和多重研究脈絡）適應各種 MRG 情境。除了 MRG 之外，M4CXR 以與專門模型相當的層級執行視覺基礎，並在 VQA 中展現出色的效能。定量和定性評估均顯示出 M4CXR 在 MRG、視覺基礎和 VQA 中的多功能性，同時持續維持臨床準確性。

##### **From cart to truck: meaning shift through words in English in the last two centuries**
2408.16209v1 by Esteban Rodríguez Betancourt, Edgar Casasola Murillo

This onomasiological study uses diachronic word embeddings to explore how
different words represented the same concepts over time, using historical word
data from 1800 to 2000. We identify shifts in energy, transport, entertainment,
and computing domains, revealing connections between language and societal
changes.
  Our approach consisted in using diachronic word embeddings trained using
word2vec with skipgram and aligning them using orthogonal Procrustes. We
discuss possible difficulties linked to the relationships the method
identifies. Moreover, we look at the ethical aspects of interpreting results,
highlighting the need for expert insights to understand the method's
significance.

摘要：這項語義學研究使用歷時詞嵌入探討不同字詞如何隨著時間推移表達相同的概念，使用 1800 年至 2000 年的歷史字詞資料。我們識別出能源、運輸、娛樂和運算領域的轉變，揭示語言與社會變遷之間的關聯。
我們的做法包括使用 word2vec 使用跳躍語法訓練歷時詞嵌入，並使用正交 Procrustes 將它們對齊。我們討論該方法識別的關係可能存在的困難。此外，我們探討了解釋結果的倫理面向，強調需要專家見解來了解該方法的重要性。

##### **ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology Report Generation Metrics**
2408.16208v1 by Oishi Banerjee, Agustina Saenz, Kay Wu, Warren Clements, Adil Zia, Dominic Buensalido, Helen Kavnoudias, Alain S. Abi-Ghanem, Nour El Ghawi, Cibele Luna, Patricia Castillo, Khaled Al-Surimi, Rayyan A. Daghistani, Yuh-Min Chen, Heng-sheng Chao, Lars Heiliger, Moon Kim, Johannes Haubold, Frederic Jonske, Pranav Rajpurkar

Given the rapidly expanding capabilities of generative AI models for
radiology, there is a need for robust metrics that can accurately measure the
quality of AI-generated radiology reports across diverse hospitals. We develop
ReXamine-Global, a LLM-powered, multi-site framework that tests metrics across
different writing styles and patient populations, exposing gaps in their
generalization. First, our method tests whether a metric is undesirably
sensitive to reporting style, providing different scores depending on whether
AI-generated reports are stylistically similar to ground-truth reports or not.
Second, our method measures whether a metric reliably agrees with experts, or
whether metric and expert scores of AI-generated report quality diverge for
some sites. Using 240 reports from 6 hospitals around the world, we apply
ReXamine-Global to 7 established report evaluation metrics and uncover serious
gaps in their generalizability. Developers can apply ReXamine-Global when
designing new report evaluation metrics, ensuring their robustness across
sites. Additionally, our analysis of existing metrics can guide users of those
metrics towards evaluation procedures that work reliably at their sites of
interest.

摘要：隨著放射科生成式 AI 模型功能的快速擴展，需要健全的指標來準確衡量不同醫院中 AI 生成的放射科報告品質。我們開發了 ReXamine-Global，一個由 LLM 驅動的多站點架構，它會在不同的寫作風格和患者族群中測試指標，找出它們概括性中的差距。首先，我們的測試方法會測試指標是否對報告風格過度敏感，並提供不同的分數，具體取決於 AI 生成的報告在風格上是否與真實報告類似。其次，我們的測試方法會衡量指標是否可靠地與專家意見一致，或者指標和專家對 AI 生成的報告品質的分數是否在某些站點上有所不同。我們將 ReXamine-Global 套用在來自全球 6 家醫院的 240 份報告上，並對 7 項既有的報告評估指標進行評估，發現它們的概括性存在嚴重差距。開發人員在設計新的報告評估指標時可以套用 ReXamine-Global，以確保它們在不同站點之間的健全性。此外，我們對既有指標的分析可以引導這些指標的使用者採用在他們感興趣的站點上可靠運作的評估程序。

##### **Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey**
2408.16202v1 by Qi Dong, Rubing Huang, Chenhui Cui, Dave Towey, Ling Zhou, Jinyu Tian, Jianzhou Wang

Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of
the immediate demand (in the next few hours to several days) for the power
system. Various external factors, such as weather changes and the emergence of
new electricity consumption scenarios, can impact electricity demand, causing
load data to fluctuate and become non-linear, which increases the complexity
and difficulty of STELF. In the past decade, deep learning has been applied to
STELF, modeling and predicting electricity demand with high accuracy, and
contributing significantly to the development of STELF. This paper provides a
comprehensive survey on deep-learning-based STELF over the past ten years. It
examines the entire forecasting process, including data pre-processing, feature
extraction, deep-learning modeling and optimization, and results evaluation.
This paper also identifies some research challenges and potential research
directions to be further investigated in future work.

摘要：短期電力負載預測（STELF）是指對電力系統在未來幾個小時至數天的即時需求（電力需求）進行預測。天氣變化和新的電力消耗情境出現等各種外部因素會影響電力需求，導致負載數據波動並變得非線性，這增加了 STELF 的複雜性和難度。在過去的十年中，深度學習已應用於 STELF，對電力需求進行建模和預測，準確度很高，並為 STELF 的發展做出了重大貢獻。本文對過去十年基於深度學習的 STELF 進行了全面的調查。它審查了整個預測過程，包括數據預處理、特徵提取、深度學習建模和優化以及結果評估。本文還確定了一些研究挑戰和潛在的研究方向，以便在未來的研究工作中進一步研究。

##### **Benchmarking Japanese Speech Recognition on ASR-LLM Setups with Multi-Pass Augmented Generative Error Correction**
2408.16180v1 by Yuka Ko, Sheng Li, Chao-Han Huck Yang, Tatsuya Kawahara

With the strong representational power of large language models (LLMs),
generative error correction (GER) for automatic speech recognition (ASR) aims
to provide semantic and phonetic refinements to address ASR errors. This work
explores how LLM-based GER can enhance and expand the capabilities of Japanese
language processing, presenting the first GER benchmark for Japanese ASR with
0.9-2.6k text utterances. We also introduce a new multi-pass augmented
generative error correction (MPA GER) by integrating multiple system hypotheses
on the input side with corrections from multiple LLMs on the output side and
then merging them. To the best of our knowledge, this is the first
investigation of the use of LLMs for Japanese GER, which involves second-pass
language modeling on the output transcriptions generated by the ASR system
(e.g., N-best hypotheses). Our experiments demonstrated performance improvement
in the proposed methods of ASR quality and generalization both in SPREDS-U1-ja
and CSJ data.

摘要：利用大型語言模型 (LLM) 強大的表示能力，自動語音辨識 (ASR) 的生成錯誤修正 (GER) 旨在提供語意和語音修正，以解決 ASR 錯誤。本研究探討了基於 LLM 的 GER 如何增強和擴展日語處理的能力，並展示了第一個針對日語 ASR 的 GER 基準，其中包含 0.9-2.6k 個文字語句。我們還引入了一種新的多重通過擴增生成錯誤修正 (MPA GER)，方法是在輸入端整合多個系統假設，並在輸出端使用多個 LLM 的修正，然後將它們合併在一起。據我們所知，這是首次研究使用 LLM 進行日語 GER，其中涉及對 ASR 系統生成的輸出轉錄進行第二次語言建模（例如，N-best 假設）。我們的實驗證明了在 SPREDS-U1-ja 和 CSJ 數據中，所提出的 ASR 品質和泛化方法的效能有所提升。

##### **LLM-assisted Labeling Function Generation for Semantic Type Detection**
2408.16173v1 by Chenjie Li, Dan Zhang, Jin Wang

Detecting semantic types of columns in data lake tables is an important
application. A key bottleneck in semantic type detection is the availability of
human annotation due to the inherent complexity of data lakes. In this paper,
we propose using programmatic weak supervision to assist in annotating the
training data for semantic type detection by leveraging labeling functions. One
challenge in this process is the difficulty of manually writing labeling
functions due to the large volume and low quality of the data lake table
datasets. To address this issue, we explore employing Large Language Models
(LLMs) for labeling function generation and introduce several prompt
engineering strategies for this purpose. We conduct experiments on real-world
web table datasets. Based on the initial results, we perform extensive analysis
and provide empirical insights and future directions for researchers in this
field.

摘要：偵測資料湖泊表格中欄位的語意類型是一項重要的應用。語意類型偵測中一個主要的瓶頸是，由於資料湖泊的複雜性，會影響到人為標註的可用性。在本文中，我們建議使用程式化弱監督，協助標註語意類型偵測的訓練資料，利用標籤功能。這個過程中的一個挑戰是，由於資料湖泊表格資料集的數量龐大且品質低落，導致手動撰寫標籤功能的難度很高。為了解決這個問題，我們探討採用大型語言模型 (LLM) 進行標籤功能產生，並為此目的引入數種提示工程策略。我們在真實世界的網頁表格資料集上進行實驗。根據初步結果，我們進行廣泛分析，並為這個領域的研究人員提供實證見解和未來方向。

##### **FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench**
2408.16163v1 by Aman Priyanshu, Supriti Vijay

This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the
safety of Large Language Models (LLMs) against multi-turn conversational
attacks. Building upon the SORRY-Bench dataset, we propose a simple yet
effective method for generating adversarial prompts by breaking down harmful
queries into seemingly innocuous sub-questions. Our approach achieves a maximum
increase of +46.22\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,
GPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We
demonstrate that this technique poses a challenge to current LLM safety
measures and highlights the need for more robust defenses against subtle,
multi-turn attacks.

摘要：本文介紹 FRACTURED-SORRY-Bench，一個評估大型語言模型 (LLM) 對多輪對話攻擊的安全性框架。在 SORRY-Bench 資料集的基礎上，我們提出了一種簡單但有效的方法來產生對抗提示，方法是將有害查詢分解成看似無害的子問題。我們的做法在 GPT-4、GPT-4o、GPT-4o-mini 和 GPT-3.5-Turbo 模型中攻擊成功率 (ASR) 最高提升了 +46.22%，相比於基線方法。我們證明了這種技術對當前的 LLM 安全措施構成挑戰，並強調需要更強大的防禦措施來應對微妙的多輪攻擊。

##### **Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation**
2408.16126v1 by Ke Chen, Jiaqi Su, Taylor Berg-Kirkpatrick, Shlomo Dubnov, Zeyu Jin

Achieving robust speech separation for overlapping speakers in various
acoustic environments with noise and reverberation remains an open challenge.
Although existing datasets are available to train separators for specific
scenarios, they do not effectively generalize across diverse real-world
scenarios. In this paper, we present a novel data simulation pipeline that
produces diverse training data from a range of acoustic environments and
content, and propose new training paradigms to improve quality of a general
speech separation model. Specifically, we first introduce AC-SIM, a data
simulation pipeline that incorporates broad variations in both content and
acoustics. Then we integrate multiple training objectives into the permutation
invariant training (PIT) to enhance separation quality and generalization of
the trained model. Finally, we conduct comprehensive objective and human
listening experiments across separation architectures and benchmarks to
validate our methods, demonstrating substantial improvement of generalization
on both non-homologous and real-world test sets.

摘要：在各種有噪音和混響的聲學環境中，針對重疊說話者實現穩健的語音分離仍然是一個未解決的挑戰。儘管現有的數據集可供訓練特定場景的分離器，但它們無法有效地概括到不同的現實世界場景。在本文中，我們提出了一個新穎的數據模擬管道，它可以從一系列聲學環境和內容中產生多樣的訓練數據，並提出新的訓練範例來提高通用語音分離模型的質量。具體來說，我們首先介紹 AC-SIM，這是一個數據模擬管道，它包含了內容和聲學的廣泛變化。然後，我們將多個訓練目標整合到排列不變訓練 (PIT) 中，以增強訓練模型的分離質量和泛化能力。最後，我們對分離架構和基準進行了全面的客觀和人類聆聽實驗，以驗證我們的模型，證明了在非同源和現實世界測試集上泛化能力的顯著提高。

##### **Data Formulator 2: Iteratively Creating Rich Visualizations with AI**
2408.16119v1 by Chenglong Wang, Bongshin Lee, Steven Drucker, Dan Marshall, Jianfeng Gao

To create rich visualizations, data analysts often need to iterate back and
forth among data processing and chart specification to achieve their goals. To
achieve this, analysts need not only proficiency in data transformation and
visualization tools but also efforts to manage the branching history consisting
of many different versions of data and charts. Recent LLM-powered AI systems
have greatly improved visualization authoring experiences, for example by
mitigating manual data transformation barriers via LLMs' code generation
ability. However, these systems do not work well for iterative visualization
authoring, because they often require analysts to provide, in a single turn, a
text-only prompt that fully describes the complex visualization task to be
performed, which is unrealistic to both users and models in many cases. In this
paper, we present Data Formulator 2, an LLM-powered visualization system to
address these challenges. With Data Formulator 2, users describe their
visualization intent with blended UI and natural language inputs, and data
transformation are delegated to AI. To support iteration, Data Formulator 2
lets users navigate their iteration history and reuse previous designs towards
new ones so that they don't need to start from scratch every time. In a user
study with eight participants, we observed that Data Formulator 2 allows
participants to develop their own iteration strategies to complete challenging
data exploration sessions.

摘要：為了建立豐富的視覺化效果，資料分析師經常需要在資料處理和圖表規格中反覆運算，才能達成目標。為了達成此目標，分析師不僅需要熟練使用資料轉換和視覺化工具，還需要努力管理包含許多不同資料和圖表版本的分支記錄。最近由 LLM 驅動的 AI 系統大幅改善了視覺化創作體驗，例如透過 LLM 的程式碼產生能力來減輕手動資料轉換的障礙。然而，這些系統並不適用於反覆的視覺化創作，因為它們通常要求分析師一次提供一個純文字提示，完整描述要執行的複雜視覺化任務，在許多情況下對使用者和模型來說這是不切實際的。在本文中，我們提出 Data Formulator 2，一個由 LLM 驅動的視覺化系統，用來解決這些挑戰。使用 Data Formulator 2，使用者可以使用混合式 UI 和自然語言輸入來描述其視覺化意圖，而資料轉換則委派給 AI。為了支援反覆運算，Data Formulator 2 讓使用者可以瀏覽其反覆運算記錄，並將先前的設計重新用於新的設計，這樣他們就不必每次都從頭開始。在一個有八位參與者的使用者研究中，我們觀察到 Data Formulator 2 讓參與者可以開發自己的反覆運算策略，以完成具有挑戰性的資料探索工作階段。

##### **Structured Event Reasoning with Large Language Models**
2408.16098v1 by Li Zhang

Reasoning about real-life events is a unifying challenge in AI and NLP that
has profound utility in a variety of domains, while fallacy in high-stake
applications could be catastrophic. Able to work with diverse text in these
domains, large language models (LLMs) have proven capable of answering
questions and solving problems. However, I show that end-to-end LLMs still
systematically fail to reason about complex events, and they lack
interpretability due to their black-box nature. To address these issues, I
propose three general approaches to use LLMs in conjunction with a structured
representation of events. The first is a language-based representation
involving relations of sub-events that can be learned by LLMs via fine-tuning.
The second is a semi-symbolic representation involving states of entities that
can be predicted and leveraged by LLMs via few-shot prompting. The third is a
fully symbolic representation that can be predicted by LLMs trained with
structured data and be executed by symbolic solvers. On a suite of event
reasoning tasks spanning common-sense inference and planning, I show that each
approach greatly outperforms end-to-end LLMs with more interpretability. These
results suggest manners of synergy between LLMs and structured representations
for event reasoning and beyond.

摘要：推理現實生活中的事件是人工智慧和自然語言處理中一個統一的挑戰，在各種領域中具有深遠的效用，而高風險應用中的謬誤可能會造成災難性的後果。大型語言模型 (LLM) 能夠處理這些領域中的各種文字，已證明它們有能力回答問題和解決問題。然而，我發現端到端 LLM 仍然系統性地無法推理複雜事件，而且由於它們的黑盒子性質而缺乏可解釋性。為了解決這些問題，我提出了三種通用方法，將 LLM 與事件的結構化表示結合使用。第一個是一種基於語言的表示，涉及子事件的關係，LLM 可以通過微調來學習這些關係。第二個是一種半符號表示，涉及實體的狀態，LLM 可以通過少量提示來預測和利用這些狀態。第三個是一種完全符號化的表示，可以由使用結構化數據訓練的 LLM 預測，並由符號求解器執行。在一系列涵蓋常識推理和規劃的事件推理任務中，我發現每種方法都比端到端 LLM 具有更高的可解釋性，並且表現出色。這些結果表明了 LLM 與結構化表示之間在事件推理及其他領域中協同作用的方式。

##### **Logic-Enhanced Language Model Agents for Trustworthy Social Simulations**
2408.16081v1 by Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi

We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a
novel approach to enhance the trustworthiness of social simulations that
utilize large language models (LLMs). While LLMs have gained attention as
agents for simulating human behaviour, their applicability in this role is
limited by issues such as inherent hallucinations and logical inconsistencies.
LELMA addresses these challenges by integrating LLMs with symbolic AI, enabling
logical verification of the reasoning generated by LLMs. This verification
process provides corrective feedback, refining the reasoning output. The
framework consists of three main components: an LLM-Reasoner for producing
strategic reasoning, an LLM-Translator for mapping natural language reasoning
to logic queries, and a Solver for evaluating these queries. This study focuses
on decision-making in game-theoretic scenarios as a model of human interaction.
Experiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt
highlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0
Pro, in producing correct reasoning in these contexts. LELMA demonstrates high
accuracy in error detection and improves the reasoning correctness of LLMs via
self-refinement, particularly in GPT-4 Omni.

摘要：我們介紹邏輯增強語言模型代理 (LELMA) 架構，一種增強使用大型語言模型 (LLM) 的社會模擬的可信度的創新方法。儘管 LLM 已獲得關注作為模擬人類行為的代理，但它們在這個角色中的適用性受到固有幻覺和邏輯不一致等問題的限制。LELMA 透過將 LLM 與符號 AI 整合，來解決這些挑戰，讓 LLM 產生的推理進行邏輯驗證。此驗證程序提供修正回饋，精煉推理輸出。此架構包含三個主要組成部分：用於產生策略推理的 LLM 推理引擎、用於將自然語言推理對應到邏輯查詢的 LLM 翻譯器，以及用於評估這些查詢的求解器。本研究專注於博弈論情境中的決策制定，作為人類互動的模型。涉及鷹鴿博弈、囚犯困境和麋鹿獵的實驗突顯了最先進的 LLM、GPT-4 Omni 和 Gemini 1.0 Pro 在這些情境中產生正確推理的限制。LELMA 證明了在錯誤偵測中的高準確度，並透過自我精煉改善了 LLM 的推理正確性，特別是在 GPT-4 Omni 中。

##### **Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings**
2408.16073v1 by Leo Yeykelis, Kaavya Pichai, James J. Cummings, Byron Reeves

This report analyzes the potential for large language models (LLMs) to
expedite accurate replication of published message effects studies. We tested
LLM-powered participants (personas) by replicating 133 experimental findings
from 14 papers containing 45 recent studies in the Journal of Marketing
(January 2023-May 2024). We used a new software tool, Viewpoints AI
(https://viewpoints.ai/), that takes study designs, stimuli, and measures as
input, automatically generates prompts for LLMs to act as a specified sample of
unique personas, and collects their responses to produce a final output in the
form of a complete dataset and statistical analysis. The underlying LLM used
was Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to replicate
these studies with the exact same sample attributes, study designs, stimuli,
and measures reported in the original human research. Our LLM replications
successfully reproduced 76% of the original main effects (84 out of 111),
demonstrating strong potential for AI-assisted replication of studies in which
people respond to media stimuli. When including interaction effects, the
overall replication rate was 68% (90 out of 133). The use of LLMs to replicate
and accelerate marketing research on media effects is discussed with respect to
the replication crisis in social science, potential solutions to
generalizability problems in sampling subjects and experimental conditions, and
the ability to rapidly test consumer responses to various media stimuli. We
also address the limitations of this approach, particularly in replicating
complex interaction effects in media response studies, and suggest areas for
future research and improvement in AI-assisted experimental replication of
media effects.

摘要：<paragraph>本報告分析大型語言模型 (LLM) 促進準確複製已發表的訊息效果研究的潛力。我們透過複製《行銷期刊》(2023 年 1 月至 2024 年 5 月) 中 14 篇論文中的 45 項近期研究的 133 項實驗發現，來測試由 LLM 驅動的參與者 (角色)。我們使用一個新的軟體工具 Viewpoints AI (https://viewpoints.ai/)，它將研究設計、刺激和測量作為輸入，自動為 LLM 生成提示，讓 LLM 作為指定樣本的獨特角色，並收集它們的回應，以產生一個完整的資料集和統計分析作為最終輸出。所使用的基礎 LLM 是 Anthropic 的 Claude Sonnet 3.5。我們生成了 19,447 個 AI 角色，以複製這些研究，這些研究具有與原始人類研究中報告的完全相同的樣本屬性、研究設計、刺激和測量。我們的 LLM 複製成功複製了 76% 的原始主要效果 (111 個中的 84 個)，證明了 AI 輔助複製人們對媒體刺激做出回應的研究具有強大的潛力。在納入交互作用時，整體複製率為 68% (133 個中的 90 個)。使用 LLM 來複製和加速媒體效果的行銷研究，會針對社會科學中的複製危機、抽樣主體和實驗條件的概化問題的潛在解決方案，以及快速測試消費者對各種媒體刺激的反應的能力進行討論。我們也探討了這種方法的限制，特別是在複製媒體反應研究中的複雜交互作用時，並建議未來在 AI 輔助的實驗複製媒體效果方面進行研究和改進。</paragraph>

##### **Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders**
2408.15998v1 by Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu

The ability to accurately interpret complex visual information is a crucial
topic of multimodal large language models (MLLMs). Recent work indicates that
enhanced visual perception significantly reduces hallucinations and improves
performance on resolution-sensitive tasks, such as optical character
recognition and document analysis. A number of recent MLLMs achieve this goal
using a mixture of vision encoders. Despite their success, there is a lack of
systematic comparisons and detailed ablation studies addressing critical
aspects, such as expert selection and the integration of multiple vision
experts. This study provides an extensive exploration of the design space for
MLLMs using a mixture of vision encoders and resolutions. Our findings reveal
several underlying principles common to various existing strategies, leading to
a streamlined yet effective design approach. We discover that simply
concatenating visual tokens from a set of complementary vision encoders is as
effective as more complex mixing architectures or strategies. We additionally
introduce Pre-Alignment to bridge the gap between vision-focused encoders and
language tokens, enhancing model coherence. The resulting family of MLLMs,
Eagle, surpasses other leading open-source models on major MLLM benchmarks.
Models and code: https://github.com/NVlabs/Eagle

摘要：準確地詮釋複雜的視覺資訊的能力是多模態大型語言模型 (MLLM) 的一個關鍵議題。最近的研究指出，增強的視覺感知能顯著減少幻覺，並改善對解析度敏感任務的執行，例如光學字元辨識和文件分析。許多最近的 MLLM 使用視覺編碼器的混合來達成此目標。儘管它們成功，但缺乏系統性的比較和詳細的消融研究來探討關鍵面向，例如專家選擇和整合多個視覺專家。本研究針對使用視覺編碼器和解析度的 MLLM 混合提供了廣泛的設計空間探討。我們的發現揭示了各種現有策略中幾個共同的底層原則，進而引導出簡化但有效的設計方法。我們發現，僅僅連結來自一組互補視覺編碼器的視覺代碼，就和更複雜的混合架構或策略一樣有效。此外，我們引入了預對齊，以彌合以視覺為中心的編碼器和語言代碼之間的差距，增強模型的相干性。由此產生的 MLLM 家族 Eagle，在主要的 MLLM 基準上超越了其他領先的開源模型。模型和程式碼：https://github.com/NVlabs/Eagle

##### **Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need**
2408.15997v1 by Sijia Peng, Yun Xiong, Yangyong Zhu, Zhiqiang Shen

Time series forecasting requires balancing short-term and long-term
dependencies for accurate predictions. Existing methods mainly focus on
long-term dependency modeling, neglecting the complexities of short-term
dynamics, which may hinder performance. Transformers are superior in modeling
long-term dependencies but are criticized for their quadratic computational
cost. Mamba provides a near-linear alternative but is reported less effective
in time series longterm forecasting due to potential information loss. Current
architectures fall short in offering both high efficiency and strong
performance for long-term dependency modeling. To address these challenges, we
introduce Mixture of Universals (MoU), a versatile model to capture both
short-term and long-term dependencies for enhancing performance in time series
forecasting. MoU is composed of two novel designs: Mixture of Feature
Extractors (MoF), an adaptive method designed to improve time series patch
representations for short-term dependency, and Mixture of Architectures (MoA),
which hierarchically integrates Mamba, FeedForward, Convolution, and
Self-Attention architectures in a specialized order to model long-term
dependency from a hybrid perspective. The proposed approach achieves
state-of-the-art performance while maintaining relatively low computational
costs. Extensive experiments on seven real-world datasets demonstrate the
superiority of MoU. Code is available at https://github.com/lunaaa95/mou/.

摘要：時間序列預測需要平衡短期和長期依賴性，才能做出準確的預測。現有方法主要專注於長期依賴性建模，忽略了短期動態的複雜性，這可能會阻礙效能。Transformer 在建模長期依賴性方面表現優異，但因其二次計算成本而受到批評。Mamba 提供了一個接近線性的替代方案，但據報導，由於潛在資訊遺失，在時間序列長期預測中效果較差。目前的架構無法同時提供高效率和強大的效能，以進行長期依賴性建模。為了應對這些挑戰，我們引入了 Mixture of Universals (MoU)，這是一個通用模型，可以捕捉短期和長期依賴性，以增強時間序列預測的效能。MoU 由兩個新穎的設計組成：特徵萃取器混合 (MoF)，一種適應方法，旨在改善時間序列區塊表示，以適應短期依賴性；以及架構混合 (MoA)，它以一種專業的順序分層整合 Mamba、前饋、卷積和自注意力架構，以從混合觀點對長期依賴性進行建模。所提出的方法在保持相對較低計算成本的同時，達到了最先進的效能。在七個真實世界資料集上進行的廣泛實驗證明了 MoU 的優越性。程式碼可在 https://github.com/lunaaa95/mou/ 獲得。

##### **Spatio-Temporal Context Prompting for Zero-Shot Action Detection**
2408.15996v2 by Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai

Spatio-temporal action detection encompasses the tasks of localizing and
classifying individual actions within a video. Recent works aim to enhance this
process by incorporating interaction modeling, which captures the relationship
between people and their surrounding context. However, these approaches have
primarily focused on fully-supervised learning, and the current limitation lies
in the lack of generalization capability to recognize unseen action categories.
In this paper, we aim to adapt the pretrained image-language models to detect
unseen actions. To this end, we propose a method which can effectively leverage
the rich knowledge of visual-language models to perform Person-Context
Interaction. Meanwhile, our Context Prompting module will utilize contextual
information to prompt labels, thereby enhancing the generation of more
representative text features. Moreover, to address the challenge of recognizing
distinct actions by multiple people at the same timestamp, we design the
Interest Token Spotting mechanism which employs pretrained visual knowledge to
find each person's interest context tokens, and then these tokens will be used
for prompting to generate text features tailored to each individual. To
evaluate the ability to detect unseen actions, we propose a comprehensive
benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our
method achieves superior results compared to previous approaches and can be
further extended to multi-action videos, bringing it closer to real-world
applications. The code and data can be found in
https://webber2933.github.io/ST-CLIP-project-page.

摘要：時空動作偵測涵蓋了在影片中定位和分類個別動作的任務。最近的研究旨在透過納入互動建模來增強此程序，這能捕捉人與其周遭環境之間的關係。然而，這些方法主要著重於全監督式學習，而目前的限制在於缺乏辨識未見動作類別的概化能力。在本文中，我們旨在調整預訓練的影像語言模型，以偵測未見的動作。為此，我們提出了一種方法，它能有效利用視覺語言模型豐富的知識來執行人情境互動。同時，我們的脈絡提示模組將利用脈絡資訊來提示標籤，從而增強更具代表性的文字特徵的產生。此外，為了應對在同一個時間戳記中辨識多人不同動作的挑戰，我們設計了興趣標記點選機制，它採用預訓練的視覺知識來尋找每個人的興趣脈絡標記，然後這些標記將用於提示，以產生針對每個個體量身打造的文字特徵。為了評估偵測未見動作的能力，我們在 J-HMDB、UCF101-24 和 AVA 資料集上提出了全面的基準。實驗顯示，與先前的做法相比，我們的方法獲得了優異的結果，並且可以進一步擴展到多動作影片，使其更接近於真實世界的應用。程式碼和資料可以在 https://webber2933.github.io/ST-CLIP-project-page 中找到。

##### **CoGen: Learning from Feedback with Coupled Comprehension and Generation**
2408.15992v1 by Mustafa Omer Gul, Yoav Artzi

Systems with both language comprehension and generation capabilities can
benefit from the tight connection between the two. This work studies coupling
comprehension and generation with focus on continually learning from
interaction with users. We propose techniques to tightly integrate the two
capabilities for both learning and inference. We situate our studies in
two-player reference games, and deploy various models for thousands of
interactions with human users, while learning from interaction feedback
signals. We show dramatic improvements in performance over time, with
comprehension-generation coupling leading to performance improvements up to 26%
in absolute terms and up to 17% higher accuracies compared to a non-coupled
system. Our analysis also shows coupling has substantial qualitative impact on
the system's language, making it significantly more human-like.

摘要：具備語言理解與產生能力的系統能從兩者之間的緊密連結中獲益。這項工作研究了將理解與產生結合，並專注於從與使用者的互動中持續學習。我們提出了一些技術，用於在學習和推理中緊密整合這兩種能力。我們將研究置於雙人參考遊戲中，並部署各種模型，與人類使用者進行數千次互動，同時從互動回饋訊號中學習。我們展示了隨著時間推移而大幅提升的效能，理解產生結合可帶來高達 26% 的絕對效能提升，以及與非結合系統相比，準確度提升高達 17%。我們的分析也顯示，結合對系統語言有實質性的質化影響，使其更接近人類語言。

##### **In-Context Imitation Learning via Next-Token Prediction**
2408.15980v1 by Letian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg

We explore how to enhance next-token prediction models to perform in-context
imitation learning on a real robot, where the robot executes new tasks by
interpreting contextual information provided during the input phase, without
updating its underlying policy parameters. We propose In-Context Robot
Transformer (ICRT), a causal transformer that performs autoregressive
prediction on sensorimotor trajectories without relying on any linguistic data
or reward function. This formulation enables flexible and training-free
execution of new tasks at test time, achieved by prompting the model with
sensorimotor trajectories of the new task composing of image observations,
actions and states tuples, collected through human teleoperation. Experiments
with a Franka Emika robot demonstrate that the ICRT can adapt to new tasks
specified by prompts, even in environment configurations that differ from both
the prompt and the training data. In a multitask environment setup, ICRT
significantly outperforms current state-of-the-art next-token prediction models
in robotics on generalizing to unseen tasks. Code, checkpoints and data are
available on https://icrt.dev/

摘要：我們探討如何增強下一個代幣預測模型，以便在真實機器人上執行情境內模仿學習，其中機器人在輸入階段解釋提供的背景資訊來執行新任務，而不會更新其基礎策略參數。我們提出情境內機器人轉換器 (ICRT)，這是一個因果轉換器，對感測運動軌跡執行自迴歸預測，而不依賴任何語言資料或獎勵函數。此公式化能靈活且無需訓練地執行新任務於測試時間，這是透過提示模型包含影像觀察、動作和狀態元組的新任務感測運動軌跡，經由人為遙控收集而達成。使用 Franka Emika 機器的實驗證明，即使在提示和訓練資料不同的環境組態中，ICRT 也能適應由提示指定的任務。在多任務環境設定中，ICRT 在機器人學上顯著優於目前最先進的下一個代幣預測模型，並推廣到未見任務。程式碼、檢查點和資料可於 https://icrt.dev/ 取得

##### **WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration**
2408.15978v1 by Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp

LLM-based autonomous agents often fail to execute complex web tasks that
require dynamic interaction due to the inherent uncertainty and complexity of
these environments. Existing LLM-based web agents typically rely on rigid,
expert-designed policies specific to certain states and actions, which lack the
flexibility and generalizability needed to adapt to unseen tasks. In contrast,
humans excel by exploring unknowns, continuously adapting strategies, and
resolving ambiguities through exploration. To emulate human-like adaptability,
web agents need strategic exploration and complex decision-making. Monte Carlo
Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with
vast action spaces, unpredictable state transitions, and incomplete information
in web tasks. In light of this, we develop WebPilot, a multi-agent system with
a dual optimization strategy that improves MCTS to better handle complex web
environments. Specifically, the Global Optimization phase involves generating a
high-level plan by breaking down tasks into manageable subtasks and
continuously refining this plan, thereby focusing the search process and
mitigating the challenges posed by vast action spaces in classical MCTS.
Subsequently, the Local Optimization phase executes each subtask using a
tailored MCTS designed for complex environments, effectively addressing
uncertainties and managing incomplete information. Experimental results on
WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on
WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%
relative increase in success rate over the concurrent tree search-based method.
WebPilot marks a significant advancement in general autonomous agent
capabilities, paving the way for more advanced and reliable decision-making in
practical environments.

摘要：<paragraph>基於 LLM 的自主代理通常無法執行複雜的網路任務，因為這些環境具有內在的不確定性和複雜性，需要動態互動。現有的基於 LLM 的網路代理通常依賴於針對特定狀態和動作設計的僵化專家政策，缺乏適應未知任務所需的靈活性與概括性。相反地，人類擅長探索未知、持續調整策略，並透過探索解決模棱兩可的問題。為了模擬類人適應力，網路代理需要策略性探索和複雜的決策制定。蒙地卡羅樹狀搜尋 (MCTS) 非常適合這一點，但傳統的 MCTS 難以應付網路任務中廣大的動作空間、不可預測的狀態轉換和不完整資訊。有鑑於此，我們開發了 WebPilot，一個具有雙重最佳化策略的多代理系統，可改善 MCTS 以更好地處理複雜的網路環境。具體來說，全球最佳化階段涉及透過將任務分解成可管理的子任務並持續優化此計畫來產生高階計畫，從而集中搜尋流程並減輕傳統 MCTS 中廣大動作空間所帶來的挑戰。隨後，局部最佳化階段使用專為複雜環境設計的客製化 MCTS 來執行每個子任務，有效地解決不確定性並管理不完整資訊。在 WebArena 和 MiniWoB++ 上的實驗結果證明了 WebPilot 的有效性。值得注意的是，在 WebArena 上，WebPilot 以 GPT-4 達到了 SOTA 效能，相較於並行的基於樹狀搜尋的方法，成功率增加了 93%。WebPilot 標誌著一般自主代理能力的重大進步，為在實際環境中進行更進階且可靠的決策制定鋪路。</paragraph>

##### **BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems**
2408.15971v1 by Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang

Large Language Models (LLMs) are becoming increasingly powerful and capable
of handling complex tasks, e.g., building single agents and multi-agent
systems. Compared to single agents, multi-agent systems have higher
requirements for the collaboration capabilities of language models. Many
benchmarks are proposed to evaluate their collaborative abilities. However,
these benchmarks lack fine-grained evaluations of LLM collaborative
capabilities. Additionally, multi-agent collaborative and competitive scenarios
are ignored in existing works. To address these two problems, we propose a
benchmark, called BattleAgentBench, which defines seven sub-stages of three
varying difficulty levels and conducts a fine-grained evaluation of language
models in terms of single-agent scenario navigation capabilities, paired-agent
task execution abilities, and multi-agent collaboration and competition
capabilities. We conducted extensive evaluations on leading four closed-source
and seven open-source models. Experimental results indicate that API-based
models perform excellently on simple tasks but open-source small models
struggle with simple tasks. Regarding difficult tasks that require
collaborative and competitive abilities, although API-based models have
demonstrated some collaborative capabilities, there is still enormous room for
improvement.

摘要：大型語言模型（LLM）變得越來越強大，並且能夠處理複雜的任務，例如，建立單一代理和多代理系統。與單一代理相比，多代理系統對語言模型的協作能力有更高的要求。許多基準被提出用於評估它們的協作能力。然而，這些基準缺乏對 LLM 協作能力的細粒度評估。此外，現有工作中忽略了多代理協作和競爭場景。為了解決這兩個問題，我們提出了一個基準，稱為 BattleAgentBench，它定義了三個不同難度級別的七個子階段，並對語言模型在單一代理場景導航能力、配對代理任務執行能力以及多代理協作和競爭能力方面進行了細粒度評估。我們對領先的四個閉源模型和七個開源模型進行了廣泛的評估。實驗結果表明，基於 API 的模型在簡單任務上表現出色，但開源小模型在簡單任務上表現不佳。對於需要協作和競爭能力的困難任務，儘管基於 API 的模型已經展示出了一些協作能力，但仍然有很大的改進空間。

##### **More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding**
2408.15966v1 by Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen

Enabling Large Language Models (LLMs) to comprehend the 3D physical world
remains a significant challenge. Due to the lack of large-scale 3D-text pair
datasets, the success of LLMs has yet to be replicated in 3D understanding. In
this paper, we rethink this issue and propose a new task: 3D Data-Efficient
Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D
object understanding with minimal 3D point cloud and text data pairs. To
address this task, we introduce GreenPLM, which leverages more text data to
compensate for the lack of 3D data. First, inspired by using CLIP to align
images and text, we utilize a pre-trained point cloud-text encoder to map the
3D point cloud space to the text space. This mapping leaves us to seamlessly
connect the text space with LLMs. Once the point-text-LLM connection is
established, we further enhance text-LLM alignment by expanding the
intermediate text space, thereby reducing the reliance on 3D point cloud data.
Specifically, we generate 6M free-text descriptions of 3D objects, and design a
three-stage training strategy to help LLMs better explore the intrinsic
connections between different modalities. To achieve efficient modality
alignment, we design a zero-parameter cross-attention module for token pooling.
Extensive experimental results show that GreenPLM requires only 12% of the 3D
training data used by existing state-of-the-art models to achieve superior 3D
understanding. Remarkably, GreenPLM also achieves competitive performance using
text-only data. The code and weights are available at:
https://github.com/TangYuan96/GreenPLM.

摘要：<paragraph>讓大型語言模型 (LLM) 理解 3D 物理世界仍然是一項重大挑戰。由於缺乏大規模的 3D-文字對資料集，LLM 的成功尚未在 3D 理解中複製。在本文中，我們重新思考這個問題並提出一個新任務：3D 資料有效點語言理解。目標是讓 LLM 能夠在最少的 3D 點雲和文字資料對的情況下實現穩健的 3D 物件理解。為了解決這個任務，我們引入了 GreenPLM，它利用更多文字資料來彌補 3D 資料的不足。首先，受使用 CLIP 將影像和文字對齊的啟發，我們利用預先訓練好的點雲文字編碼器將 3D 點雲空間映射到文字空間。這個對應讓我們可以無縫地將文字空間與 LLM 連接起來。一旦建立了點文字 LLM 連接，我們進一步透過擴展中間文字空間來增強文字 LLM 對齊，從而減少對 3D 點雲資料的依賴。具體來說，我們生成了 600 萬個 3D 物件的自由文字描述，並設計了一個三階段訓練策略來幫助 LLM 更深入地探索不同模態之間的內在聯繫。為了實現有效的模態對齊，我們設計了一個零參數跨注意力模組進行代碼池化。大量的實驗結果顯示，GreenPLM 只需要現有最先進模型所使用 3D 訓練資料的 12%，就能實現優異的 3D 理解。值得注意的是，GreenPLM 還使用純文字資料達到了有競爭力的效能。程式碼和權重可在以下網址取得：https://github.com/TangYuan96/GreenPLM。</paragraph>

##### **Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games**
2408.15950v1 by Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks

Recent advancements in large language models (LLMs) have expanded their
capabilities beyond traditional text-based tasks to multimodal domains,
integrating visual, auditory, and textual data. While multimodal LLMs have been
extensively explored for high-level planning in domains like robotics and
games, their potential as low-level controllers remains largely untapped. This
paper explores the application of multimodal LLMs as low-level controllers in
the domain of Atari video games, introducing Atari game performance as a new
benchmark for evaluating the ability of multimodal LLMs to perform low-level
control tasks. Unlike traditional reinforcement learning (RL) and imitation
learning (IL) methods that require extensive computational resources as well as
reward function specification, these LLMs utilize pre-existing multimodal
knowledge to directly engage with game environments. Our study assesses
multiple multimodal LLMs performance against traditional RL agents, human
players, and random agents, focusing on their ability to understand and
interact with complex visual scenes and formulate strategic responses.
Additionally, we examine the impact of In-Context Learning (ICL) by
incorporating human-demonstrated game-play trajectories to enhance the models
contextual understanding. Through this investigation, we aim to determine the
extent to which multimodal LLMs can leverage their extensive training to
effectively function as low-level controllers, thereby redefining potential
applications in dynamic and visually complex environments. Additional results
and videos are available at our project webpage:
https://sites.google.com/view/atari-gpt/.

摘要：大型語言模型 (LLM) 的最新進展已將其功能從傳統的基於文本任務擴展到多模態領域，整合了視覺、聽覺和文本數據。雖然多模態 LLM 已被廣泛探索用於機器人和遊戲等領域的高階規劃，但它們作為低階控制器仍有很大的潛力尚未開發。本文探討了多模態 LLM 在 Atari 視頻遊戲領域中作為低階控制器的應用，引入了 Atari 遊戲效能作為評估多模態 LLM 執行低階控制任務能力的新基準。與需要大量計算資源和獎勵功能規範的傳統強化學習 (RL) 和模仿學習 (IL) 方法不同，這些 LLM 利用預先存在的多模態知識直接與遊戲環境互動。我們的研究評估了多個多模態 LLM 的效能，並針對傳統 RL 代理、人類玩家和隨機代理進行比較，重點關注它們理解和與複雜視覺場景互動並制定策略反應的能力。此外，我們透過納入人類展示的遊戲軌跡來增強模型的上下文理解，檢視情境學習 (ICL) 的影響。透過這項調查，我們旨在確定多模態 LLM 在多大程度上可以利用其廣泛的訓練來有效地作為低階控制器，從而重新定義在動態和視覺複雜環境中的潛在應用。其他結果和影片可在我們的專案網頁中取得：
https://sites.google.com/view/atari-gpt/。

##### **Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning**
2408.15924v1 by Bingchen Yan

Few-shot image classification is a challenging task in the field of machine
learning, involving the identification of new categories using a limited number
of labeled samples. In recent years, methods based on local descriptors have
made significant progress in this area. However, the key to improving
classification accuracy lies in effectively filtering background noise and
accurately selecting critical local descriptors highly relevant to image
category information.
  To address this challenge, we propose an innovative weighted adaptive
threshold filtering (WATF) strategy for local descriptors. This strategy can
dynamically adjust based on the current task and image context, thereby
selecting local descriptors most relevant to the image category. This enables
the model to better focus on category-related information while effectively
mitigating interference from irrelevant background regions.
  To evaluate the effectiveness of our method, we adopted the N-way K-shot
experimental framework. Experimental results show that our method not only
improves the clustering effect of selected local descriptors but also
significantly enhances the discriminative ability between image categories.
Notably, our method maintains a simple and lightweight design philosophy
without introducing additional learnable parameters. This feature ensures
consistency in filtering capability during both training and testing phases,
further enhancing the reliability and practicality of the method.

摘要：小样本图像分类是机器学习领域的一项具有挑战性的任务，涉及使用有限数量的标记样本识别新类别。近年来，基于局部描述符的方法在这个领域取得了重大进展。然而，提高分类精度的关键在于有效地过滤背景噪声，并准确地选择与图像类别信息高度相关的关键局部描述符。
为了解决这一挑战，我们提出了一种针对局部描述符的创新加权自适应阈值过滤（WATF）策略。该策略可以根据当前任务和图像上下文动态调整，从而选择与图像类别最相关的局部描述符。这使模型能够更好地关注与类别相关的信息，同时有效地减轻无关背景区域的干扰。
为了评估我们方法的有效性，我们采用了 N 路 K 镜头实验框架。实验结果表明，我们的方法不仅提高了所选局部描述符的聚类效果，而且还显着增强了图像类别之间的判别能力。值得注意的是，我们的方法保持了简单轻量的设计理念，没有引入额外的可学习参数。此功能确保了在训练和测试阶段过滤能力的一致性，进一步增强了该方法的可靠性和实用性。

##### **Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models**
2408.15915v1 by Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu

The cultivation of expertise for large language models (LLMs) to solve tasks
of specific areas often requires special-purpose tuning with calibrated
behaviors on the expected stable outputs. To avoid huge cost brought by manual
preparation of instruction datasets and training resources up to hundreds of
hours, the exploitation of open knowledge including a wealth of low rank
adaptation (LoRA) models and instruction datasets serves as a good starting
point. However, existing methods on model and data selection focus on the
performance of general-purpose capabilities while neglecting the knowledge gap
exposed in domain-specific deployment. In the present study, we propose to
bridge such gap by introducing few human-annotated samples (i.e., K-shot) for
advancing task expertise of LLMs with open knowledge. Specifically, we develop
an efficient and scalable pipeline to cost-efficiently produce task experts
where K-shot data intervene in selecting the most promising expert candidates
and the task-relevant instructions. A mixture-of-expert (MoE) system is built
to make the best use of individual-yet-complementary knowledge between multiple
experts. We unveil the two keys to the success of a MoE system, 1) the abidance
by K-shot, and 2) the insistence on diversity. For the former, we ensure that
models that truly possess problem-solving abilities on K-shot are selected
rather than those blind guessers. Besides, during data selection, instructions
that share task-relevant contexts with K-shot are prioritized. For the latter,
we highlight the diversity of constituting experts and that of the fine-tuning
instructions throughout the model and data selection process. Extensive
experimental results confirm the superiority of our approach over existing
methods on utilization of open knowledge across various tasks. Codes and models
will be released later.

摘要：<paragraph>為了解決特定領域的任務，培養大型語言模型 (LLM) 的專業知識通常需要透過針對預期穩定輸出進行校準行為的特殊用途微調。為了避免手動準備指令資料集和訓練資源長達數百小時所帶來的龐大成本，利用開放知識（包括大量的低秩適應 (LoRA) 模型和指令資料集）作為一個良好的起點。然而，現有關於模型和資料選擇的方法著重於通用功能的效能，同時忽略了特定領域部署中暴露的知識差距。在本研究中，我們提出透過引入少量的經由人類註解的樣本（即，K-shot）來彌補此差距，藉此提升 LLM 的任務專業知識，並運用開放知識。具體來說，我們開發了一個高效且可擴充的管線，以經濟有效的方式產生任務專家，其中 K-shot 資料介入以選出最有希望的專家候選人和與任務相關的指令。建立一個混合專家 (MoE) 系統，以最佳利用多個專家之間個別且互補的知識。我們揭示了 MoE 系統成功的兩個關鍵：1) 遵守 K-shot，以及 2) 堅持多元性。對於前者，我們確保選擇真正具備 K-shot 問題解決能力的模型，而不是那些盲目猜測的模型。此外，在資料選擇期間，會優先考慮與 K-shot 共享與任務相關脈絡的指令。對於後者，我們強調構成專家的多元性，以及在模型和資料選擇過程中微調指令的多元性。廣泛的實驗結果證實了我們的方法在各種任務中利用開放知識的優越性，優於現有方法。程式碼和模型將稍後釋出。</paragraph>

##### **LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**
2408.15903v1 by Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

The rapid obsolescence of information in Large Language Models (LLMs) has
driven the development of various techniques to incorporate new facts. However,
existing methods for knowledge editing still face difficulties with multi-hop
questions that require accurate fact identification and sequential logical
reasoning, particularly among numerous fact updates. To tackle these
challenges, this paper introduces Graph Memory-based Editing for Large Language
Models (GMeLLo), a straitforward and effective method that merges the explicit
knowledge representation of Knowledge Graphs (KGs) with the linguistic
flexibility of LLMs. Beyond merely leveraging LLMs for question answering,
GMeLLo employs these models to convert free-form language into structured
queries and fact triples, facilitating seamless interaction with KGs for rapid
updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art knowledge editing methods in
the multi-hop question answering benchmark, MQuAKE, especially in scenarios
with extensive knowledge edits.

摘要：大型語言模型 (LLM) 中資訊快速過時，促使各種技術發展以納入新事實。然而，現有的知識編輯方法在需要準確事實辨識和順序邏輯推理的多跳問題上仍面臨困難，特別是在眾多事實更新中。為了應對這些挑戰，本文介紹了大型語言模型的圖記憶編輯 (GMeLLo)，這是一種直接且有效的方法，結合了知識圖譜 (KG) 的明確知識表示與 LLM 的語言靈活性。GMeLLo 不僅利用 LLM 來回答問題，還使用這些模型將自由形式的語言轉換為結構化查詢和事實三元組，促進與 KG 的無縫互動，以便快速更新和精確的多跳推理。我們的結果表明，在多跳問題回答基準 MQuAKE 中，GMeLLo 明顯超越了當前最先進的知識編輯方法，特別是在廣泛知識編輯的場景中。

##### **Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts**
2408.15901v1 by Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet Üstün

Efficiency, specialization, and adaptability to new data distributions are
qualities that are hard to combine in current Large Language Models. The
Mixture of Experts (MoE) architecture has been the focus of significant
research because its inherent conditional computation enables such desirable
properties. In this work, we focus on "upcycling" dense expert models into an
MoE, aiming to improve specialization while also adding the ability to adapt to
new tasks easily. We introduce Nexus, an enhanced MoE architecture with
adaptive routing where the model learns to project expert embeddings from
domain representations. This approach allows Nexus to flexibly add new experts
after the initial upcycling through separately trained dense models, without
requiring large-scale MoE training for unseen data domains. Our experiments
show that Nexus achieves a relative gain of up to 2.1% over the baseline for
initial upcycling, and a 18.8% relative gain for extending the MoE with a new
expert by using limited finetuning data. This flexibility of Nexus is crucial
to enable an open-source ecosystem where every user continuously assembles
their own MoE-mix according to their needs.

摘要：效率、專精度以及適應新資料分佈是當前大型語言模型中難以結合的品質。專家混合 (MoE) 架構一直是重要的研究重點，因為其內在的條件式運算可實現這些理想的特性。在這項研究中，我們專注於將密集的專家模型「升級」至 MoE，目標是改善專精度的同時，也能輕鬆地加入適應新任務的能力。我們引入了 Nexus，一種具備自適應路由的增強式 MoE 架構，其中模型學習從網域表示中投射專家嵌入。這種方法讓 Nexus 能夠在最初的升級後，透過個別訓練的密集模型靈活地加入新的專家，而無需針對未見的資料網域進行大規模的 MoE 訓練。我們的實驗顯示，Nexus 在最初的升級中取得了比基準高達 2.1% 的相對增益，以及透過使用有限的微調資料來擴充 MoE 的新專家時，取得了 18.8% 的相對增益。Nexus 的這種靈活性對於建立一個開放原始碼生態系統至關重要，讓每個使用者都能根據自己的需求持續組裝自己的 MoE 組合。

##### **Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation**
2408.15898v1 by Reid Graves, Amir Barati Farimani

The design of aerodynamic shapes, such as airfoils, has traditionally
required significant computational resources and relied on predefined design
parameters, which limit the potential for novel shape synthesis. In this work,
we introduce a data-driven methodology for airfoil generation using a diffusion
model. Trained on a dataset of preexisting airfoils, our model can generate an
arbitrary number of new airfoils from random vectors, which can be conditioned
on specific aerodynamic performance metrics such as lift and drag, or geometric
criteria. Our results demonstrate that the diffusion model effectively produces
airfoil shapes with realistic aerodynamic properties, offering substantial
improvements in efficiency, flexibility, and the potential for discovering
innovative airfoil designs. This approach significantly expands the design
space, facilitating the synthesis of high-performance aerodynamic shapes that
transcend the limitations of traditional methods.

摘要：空氣動力形狀，例如機翼的設計，傳統上需要大量的計算資源，並依賴於預先定義的設計參數，這限制了新形狀合成的可能性。在這項工作中，我們引入了一種使用擴散模型進行翼型的資料驅動方法。根據現有翼型的資料集進行訓練，我們的模型可以從隨機向量中產生任意數量的新的翼型，這些向量可以根據特定的空氣動力效能指標（例如升力和阻力）或幾何標準進行調整。我們的結果表明，擴散模型有效地產生具有實際空氣動力特性的翼型形狀，在效率、靈活性以及發現創新的翼型設計的潛力方面提供了顯著的改進。這種方法顯著擴展了設計空間，促進了超越傳統方法限制的高效能空氣動力形狀的合成。

##### **A New Method for Cross-Lingual-based Semantic Role Labeling**
2408.15896v1 by Mohammad Ebrahimi, Behrouz Minaei Bidgoli, Nasim Khozouei

Semantic role labeling is a crucial task in natural language processing,
enabling better comprehension of natural language. However, the lack of
annotated data in multiple languages has posed a challenge for researchers. To
address this, a deep learning algorithm based on model transfer has been
proposed. The algorithm utilizes a dataset consisting of the English portion of
CoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency
of training, only ten percent of the educational data from each language is
used. The results of the proposed model demonstrate significant improvements
compared to Niksirt et al.'s model. In monolingual mode, the proposed model
achieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,
the improvement was even more substantial, reaching 6.23 percent. Worth noting
is that the compared model only trained two of the four stages of semantic role
labeling and employed golden data for the remaining two stages. This suggests
that the actual superiority of the proposed model surpasses the reported
numbers by a significant margin. The development of cross-lingual methods for
semantic role labeling holds promise, particularly in addressing the scarcity
of annotated data for various languages. These advancements pave the way for
further research in understanding and processing natural language across
different linguistic contexts.

摘要：語意角色標籤是自然語言處理中的一項重要任務，
能讓電腦更好地理解自然語言。然而，多語種標註資料的缺乏對研究人員來說是一個挑戰。為了解決這個問題，有人提出了一種基於模型轉移的深度學習演算法。該演算法利用了一個由 CoNLL2009 的英文部分和波斯語語意角色語料庫組成的資料集。為了優化訓練效率，只使用了每種語言中 10% 的教育資料。所提出的模型的結果顯示出與 Niksirt 等人的模型相比有顯著的改進。在單語模式下，所提出的模型在 F1 分數上提高了 2.05%，而在跨語言模式下，改進幅度更大，達到 6.23%。值得注意的是，所比較模型只訓練了語意角色標籤的四個階段中的兩個階段，並在剩下的兩個階段中使用了金資料。這表明所提出的模型的實際優越性比報告的數字高出很多。跨語言語意角色標籤方法的發展很有前景，特別是在解決各種語言標註資料稀缺的問題上。這些進展為在不同的語言環境中理解和處理自然語言的進一步研究鋪平了道路。

##### **Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models**
2408.15895v1 by Sebastian Vallejo Vera, Hunter Driggers

Human coders are biased. We test similar biases in Large Language Models
(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and
Meyer (2018), we find evidence that LLMs use political information, and
specifically party cues, to judge political statements. Not only do LLMs use
relevant information to contextualize whether a statement is positive,
negative, or neutral based on the party cue, they also reflect the biases of
the human-generated data upon which they have been trained. We also find that
unlike humans, who are only biased when faced with statements from extreme
parties, LLMs exhibit significant bias even when prompted with statements from
center-left and center-right parties. The implications of our findings are
discussed in the conclusion.

摘要：人類編碼器有偏見。我們在大型語言模型 (LLM) 中測試了作為註解者的類似偏見。通過複製 Ennser-Jedenastik 和 Meyer (2018) 執行的實驗，我們發現證據表明 LLM 使用政治資訊，特別是政黨線索，來判斷政治聲明。LLM 不僅使用相關資訊根據政黨線索來判斷陳述是正面、負面還是中立，它們還反映了他們接受訓練的人類生成資料的偏見。我們還發現，與只在面對極端政黨的聲明時才會出現偏見的人類不同，即使提示來自中間偏左和中間偏右的政黨的聲明，LLM 仍會表現出顯著的偏見。我們發現的意義在結論中進行了討論。

##### **Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks**
2408.15886v2 by Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub

In recent years, the evolution of machine learning techniques has
significantly impacted the field of intrusion detection, particularly within
the context of the Internet of Things (IoT). As IoT networks expand, the need
for robust security measures to counteract potential threats has become
increasingly critical. This paper introduces a hybrid Intrusion Detection
System (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)
with the XGBoost algorithm. Our proposed IDS leverages the unique capabilities
of KANs, which utilize learnable activation functions to model complex
relationships within data, alongside the powerful ensemble learning techniques
of XGBoost, known for its high performance in classification tasks. This hybrid
approach not only enhances the detection accuracy but also improves the
interpretability of the model, making it suitable for dynamic and intricate IoT
environments. Experimental evaluations demonstrate that our hybrid IDS achieves
an impressive detection accuracy exceeding 99% in distinguishing between benign
and malicious activities. Additionally, we were able to achieve F1 scores,
precision, and recall that exceeded 98%. Furthermore, we conduct a comparative
analysis against traditional Multi-Layer Perceptron (MLP) networks, assessing
performance metrics such as Precision, Recall, and F1-score. The results
underscore the efficacy of integrating KANs with XGBoost, highlighting the
potential of this innovative approach to significantly strengthen the security
framework of IoT networks.

摘要：近年來，機器學習技術的演進對入侵偵測領域產生重大影響，特別是在物聯網 (IoT) 的脈絡中。隨著物聯網網路的擴展，對抗潛在威脅的強大安全措施的需求變得越來越關鍵。本文介紹了一種混合入侵偵測系統 (IDS)，它協同結合了 Kolmogorov-Arnold 網路 (KAN) 和 XGBoost 演算法。我們提出的 IDS 利用了 KAN 的獨特功能，它利用可學習的激活函數來模擬資料中的複雜關係，以及 XGBoost 強大的整合學習技術，XGBoost 以其在分類任務中的高性能而聞名。這種混合方法不僅增強了偵測準確性，也改善了模型的可解釋性，使其適用於動態且複雜的物聯網環境。實驗評估表明，我們的混合 IDS 在區分良性和惡意活動方面達到了令人印象深刻的 99% 以上的偵測準確度。此外，我們能夠達到超過 98% 的 F1 分數、精確度和召回率。此外，我們對傳統的多層感知器 (MLP) 網路進行了比較分析，評估了精確度、召回率和 F1 分數等效能指標。結果強調了將 KAN 與 XGBoost 整合的效能，突顯了這種創新方法在顯著強化物聯網網路安全架構方面的潛力。

##### **Persuasion Games using Large Language Models**
2408.15879v1 by Ganesh Prasath Ramani, Shirish Karande, Santhosh V, Yash Bhatia

Large Language Models (LLMs) have emerged as formidable instruments capable
of comprehending and producing human-like text. This paper explores the
potential of LLMs, to shape human perspectives and subsequently influence their
decisions on particular tasks. This capability finds applications in diverse
domains such as Investment, Credit cards and Insurance, wherein they assist
users in selecting appropriate insurance policies, investment plans, Credit
cards, Retail, as well as in Behavioral Change Support Systems (BCSS).
  We present a sophisticated multi-agent framework wherein a consortium of
agents operate in collaborative manner. The primary agent engages directly with
users through persuasive dialogue, while the auxiliary agents perform tasks
such as information retrieval, response analysis, development of persuasion
strategies, and validation of facts. Empirical evidence from our experiments
demonstrates that this collaborative methodology significantly enhances the
persuasive efficacy of the LLM. We analyze user resistance to persuasive
efforts continuously and counteract it by employing a combination of rule-based
and LLM-based resistance-persuasion mapping techniques.
  We employ simulated personas and generate conversations in insurance,
banking, and retail domains to evaluate the proficiency of large language
models (LLMs) in recognizing, adjusting to, and influencing various personality
types. Concurrently, we examine the resistance mechanisms employed by LLM
simulated personas. Persuasion is quantified via measurable surveys before and
after interaction, LLM-generated scores on conversation, and user decisions
(purchase or non-purchase).

摘要：大型語言模型 (LLM) 已成為強大的工具，能夠理解並產生類似人類的文字。本文探討了 LLM 的潛力，以塑造人類觀點，並進而影響他們對特定任務的決策。此功能可在投資、信用卡和保險等不同領域中找到應用，在這些領域中，它們協助使用者選擇適當的保險保單、投資計畫、信用卡、零售，以及行為改變支持系統 (BCSS)。
我們提出了一個複雜的多代理架構，其中一組代理以協作方式運作。主要代理透過有說服力的對話直接與使用者互動，而輔助代理則執行任務，例如資訊檢索、回應分析、說服策略的發展以及事實驗證。我們實驗的實證證據表明，這種協作方法顯著增強了 LLM 的說服力。我們持續分析使用者對說服工作的抵制，並透過結合基於規則和基於 LLM 的抗拒說服對應技術來加以應對。
我們採用模擬角色，並在保險、銀行和零售領域產生對話，以評估大型語言模型 (LLM) 在識別、調整和影響各種人格類型的熟練度。同時，我們檢查 LLM 模擬角色所採用的抵制機制。說服力透過互動前後的可衡量調查、LLM 對對話產生的分數以及使用者決策（購買或不購買）來量化。

##### **GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model**
2408.15868v1 by Yongjie Fu, Yunlong Li, Xuan Di

Autonomous driving training requires a diverse range of datasets encompassing
various traffic conditions, weather scenarios, and road types. Traditional data
augmentation methods often struggle to generate datasets that represent rare
occurrences. To address this challenge, we propose GenDDS, a novel approach for
generating driving scenarios generation by leveraging the capabilities of
Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology
involves the use of descriptive prompts to guide the synthesis process, aimed
at producing realistic and diverse driving scenarios. With the power of the
latest computer vision techniques, such as ControlNet and Hotshot-XL, we have
built a complete pipeline for video generation together with SDXL. We employ
the KITTI dataset, which includes real-world driving videos, to train the
model. Through a series of experiments, we demonstrate that our model can
generate high-quality driving videos that closely replicate the complexity and
variability of real-world driving scenarios. This research contributes to the
development of sophisticated training data for autonomous driving systems and
opens new avenues for creating virtual environments for simulation and
validation purposes.

摘要：自動駕駛訓練需要各種各樣涵蓋各種交通狀況、天氣情境和道路類型的資料集。傳統資料擴充方法通常難以產生表示罕見事件的資料集。為了應對這個挑戰，我們提出了 GenDDS，這是一種透過利用先進潛在擴散模型 Stable Diffusion XL (SDXL) 的功能來產生駕駛場景生成的新方法。我們的做法涉及使用描述性提示來引導合成過程，目的是產生逼真且多樣化的駕駛場景。透過運用最新的電腦視覺技術，例如 ControlNet 和 Hotshot-XL，我們已經建立了一個完整的影片生成管道與 SDXL。我們採用包含真實世界駕駛影片的 KITTI 資料集來訓練模型。透過一系列實驗，我們證明我們的模型可以產生高品質的駕駛影片，這些影片可以緊密複製真實世界駕駛場景的複雜性和可變性。這項研究有助於開發用於自動駕駛系統的精密訓練資料，並為建立用於模擬和驗證目的的虛擬環境開啟新的途徑。

##### **Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection**
2408.15866v1 by Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana

The current technology landscape lacks a foundational AI model for solving
process engineering calculations. In this work, we introduce a novel autonomous
agent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to
enhance open, customizable small code language models (SLMs) for these
calculations. By combining instruction tuned code SLMs with Retrieval-Augmented
Code Generation (RACG) using external tools, the agent generates, debugs, and
optimizes code from natural language specifications. Our approach addresses the
limitations of the current lack of a foundational AI model for specialized
process engineering tasks and offers benefits of explainability, knowledge
editing, and cost-effectiveness. Additionally, we curate custom datasets of
chemical and process engineering problems and solutions to overcome data
scarcity. Experimental results show that our framework matches the performance
of large-scale proprietary models on benchmark datasets, proving its
effectiveness and usability.

摘要：當前的技術領域缺乏一個基礎 AI 模型來解決製程工程計算。在此工作中，我們介紹一個新穎的自主代理架構，利用檢索增強指令調整 (RAIT) 來增強開放且可自訂的小型程式碼語言模型 (SLM)，以進行這些計算。透過結合指令調整程式碼 SLM 與使用外部工具的檢索增強程式碼產生 (RACG)，代理會根據自然語言規格產生、除錯和最佳化程式碼。我們的做法解決了當前缺乏基礎 AI 模型來執行專業製程工程任務的限制，並提供可解釋性、知識編輯和成本效益的優點。此外，我們策劃化學和製程工程問題與解決方案的客製化資料集，以克服資料稀少的問題。實驗結果顯示，我們的架構在基準資料集上與大型專有模型的效能相符，證明了其有效性和可用性。

##### **Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature**
2408.15836v1 by Uri Katz, Mosh Levy, Yoav Goldberg

The exponential growth of scientific literature necessitates advanced tools
for effective knowledge exploration. We present Knowledge Navigator, a system
designed to enhance exploratory search abilities by organizing and structuring
the retrieved documents from broad topical queries into a navigable, two-level
hierarchy of named and descriptive scientific topics and subtopics. This
structured organization provides an overall view of the research themes in a
domain, while also enabling iterative search and deeper knowledge discovery
within specific subtopics by allowing users to refine their focus and retrieve
additional relevant documents. Knowledge Navigator combines LLM capabilities
with cluster-based methods to enable an effective browsing method. We
demonstrate our approach's effectiveness through automatic and manual
evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,
prompts, and benchmarks are made publicly available.

摘要：科學文獻的指數成長需要進階工具，以有效探索知識。我們提出知識導航器，一個系統，旨在透過將從廣泛主題查詢中擷取的文件組織和建構到可導航的兩層級命名和描述性科學主題和子主題中，來增強探索性搜尋能力。此結構化組織提供特定領域研究主題的整體概觀，同時也透過讓使用者精煉其焦點和擷取額外的相關文件，在特定子主題中啟用反覆搜尋和更深入的知識發現。知識導航器結合 LLM 能力和基於群集的方法，以啟用有效的瀏覽方法。我們透過在兩個新基準 CLUSTREC-COVID 和 SCITOC 上進行自動和手動評估，來證明我們方法的有效性。我們的程式碼、提示和基準已公開提供。

##### **Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification**
2408.15827v1 by Abu Adnan Sadi, Mohammad Ashrafuzzaman Khan, Lubaba Binte Saber

As the field of artificial intelligence progresses, assistive technologies
are becoming more widely used across all industries. The healthcare industry is
no different, with numerous studies being done to develop assistive tools for
healthcare professionals. Automatic diagnostic systems are one such beneficial
tool that can assist with a variety of tasks, including collecting patient
information, analyzing test results, and diagnosing patients. However, the idea
of developing systems that can provide a differential diagnosis has been
largely overlooked in most of these research studies. In this study, we propose
a transformer-based approach for providing differential diagnoses based on a
patient's age, sex, medical history, and symptoms. We use the DDXPlus dataset,
which provides differential diagnosis information for patients based on 49
disease types. Firstly, we propose a method to process the tabular patient data
from the dataset and engineer them into patient reports to make them suitable
for our research. In addition, we introduce two data modification modules to
diversify the training data and consequently improve the robustness of the
models. We approach the task as a multi-label classification problem and
conduct extensive experiments using four transformer models. All the models
displayed promising results by achieving over 97% F1 score on the held-out test
set. Moreover, we design additional behavioral tests to get a broader
understanding of the models. In particular, for one of our test cases, we
prepared a custom test set of 100 samples with the assistance of a doctor. The
results on the custom set showed that our proposed data modification modules
improved the model's generalization capabilities. We hope our findings will
provide future researchers with valuable insights and inspire them to develop
reliable systems for automatic differential diagnosis.

摘要：隨著人工智慧領域的進展，輔助技術在各行各業的應用日益廣泛。醫療保健產業也不例外，有許多研究致力於為醫療保健專業人員開發輔助工具。自動診斷系統就是一種有益的工具，可以協助執行各種任務，包括收集患者資訊、分析檢驗結果和診斷患者。然而，在這些研究中，開發可提供鑑別診斷的系統的想法在很大程度上被忽視了。在本研究中，我們提出了一種基於 Transformer 的方法，根據患者的年齡、性別、病史和症狀提供鑑別診斷。我們使用 DDXPlus 資料集，該資料集根據 49 種疾病類型提供患者的鑑別診斷資訊。首先，我們提出了一種方法來處理資料集中的表格患者資料，並將其設計成患者報告，使其適合我們的研究。此外，我們引入了兩個資料修改模組，以使訓練資料多樣化，進而提高模型的穩健性。我們將此任務視為多標籤分類問題，並使用四個 Transformer 模型進行廣泛的實驗。所有模型在保留的測試集中都達到了 97% 以上的 F1 分數，表現出令人滿意的結果。此外，我們設計了額外的行為測試，以更廣泛地了解這些模型。特別是，對於我們的其中一個測試案例，我們在一位醫生的協助下準備了一個包含 100 個樣本的客製化測試集。客製化集上的結果顯示，我們提出的資料修改模組改善了模型的泛化能力。我們希望我們的發現能為未來的研究人員提供有價值的見解，並激勵他們開發可靠的自動鑑別診斷系統。

##### **Object Detection for Vehicle Dashcams using Transformers**
2408.15809v1 by Osama Mustafa, Khizer Ali, Anam Bibi, Imran Siddiqi, Momina Moetesum

The use of intelligent automation is growing significantly in the automotive
industry, as it assists drivers and fleet management companies, thus increasing
their productivity. Dash cams are now been used for this purpose which enables
the instant identification and understanding of multiple objects and
occurrences in the surroundings. In this paper, we propose a novel approach for
object detection in dashcams using transformers. Our system is based on the
state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong
performance in a variety of conditions, including different weather and
illumination scenarios. The use of transformers allows for the consideration of
contextual information in decisionmaking, improving the accuracy of object
detection. To validate our approach, we have trained our DETR model on a
dataset that represents real-world conditions. Our results show that the use of
intelligent automation through transformers can significantly enhance the
capabilities of dashcam systems. The model achieves an mAP of 0.95 on
detection.

摘要：智能自動化在汽車產業中正大幅成長，因為它能協助駕駛和車隊管理公司，進而提升他們的生產力。行車記錄器現在被用於此目的，能立即辨識並理解周遭的各種物體和事件。在本文中，我們提出了一個使用Transformer在行車記錄器中進行物體偵測的新穎方法。我們的系統是基於最先進的檢測Transformer (DETR)，它已在各種條件下展現強大的效能，包括不同的天氣和光照場景。Transformer的使用允許在決策中考量脈絡資訊，進而提升物體偵測的準確度。為了驗證我們的做法，我們已在一個代表真實世界條件的資料集上訓練我們的 DETR 模型。我們的結果顯示，使用Transformer進行的智能自動化能大幅提升行車記錄器系統的能力。該模型在偵測上達到 0.95 的 mAP。

##### **ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation**
2408.15803v1 by Tiantian Feng, Tuo Zhang, Salman Avestimehr, Shrikanth S. Narayanan

Multimodal Federated Learning frequently encounters challenges of client
modality heterogeneity, leading to undesired performances for secondary
modality in multimodal learning. It is particularly prevalent in audiovisual
learning, with audio is often assumed to be the weaker modality in recognition
tasks. To address this challenge, we introduce ModalityMirror to improve audio
model performance by leveraging knowledge distillation from an audiovisual
federated learning model. ModalityMirror involves two phases: a modality-wise
FL stage to aggregate uni-modal encoders; and a federated knowledge
distillation stage on multi-modality clients to train an unimodal student
model. Our results demonstrate that ModalityMirror significantly improves the
audio classification compared to the state-of-the-art FL methods such as
Harmony, particularly in audiovisual FL facing video missing. Our approach
unlocks the potential for exploiting the diverse modality spectrum inherent in
multi-modal FL.

摘要：多模态联邦学习经常遇到客户端模态异质性的挑战，导致多模态学习中次要模态的性能不佳。这在视听学习中尤为普遍，音频通常被认为是识别任务中较弱的模态。为了应对这一挑战，我们引入了 ModalityMirror，通过利用来自视听联邦学习模型的知识蒸馏来提高音频模型性能。ModalityMirror 涉及两个阶段：一个模态明智的 FL 阶段来聚合单模态编码器；以及一个在多模态客户端上进行联邦知识蒸馏的阶段，以训练单模态学生模型。我们的结果表明，与 Harmony 等最先进的 FL 方法相比，ModalityMirror 显着改进了音频分类，尤其是在面对视频缺失的视听 FL 中。我们的方法释放了利用多模态 FL 中固有的多样化模态频谱的潜力。

##### **Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization**
2408.15801v1 by Léo Hemamou, Mehdi Debiane

In an era where digital text is proliferating at an unprecedented rate,
efficient summarization tools are becoming indispensable. While Large Language
Models (LLMs) have been successfully applied in various NLP tasks, their role
in extractive text summarization remains underexplored. This paper introduces
EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive
Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and
ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of
abstractive methods, which often suffer from issues like factual inaccuracies
and hallucinations, EYEGLAXS focuses on extractive summarization to ensure
factual and grammatical integrity. Utilizing state-of-the-art techniques such
as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS
addresses the computational and resource challenges typically associated with
LLMs. The system sets new performance benchmarks on well-known datasets like
PubMed and ArXiv. Furthermore, we extend our research through additional
analyses that explore the adaptability of LLMs in handling different sequence
lengths and their efficiency in training on smaller datasets. These
contributions not only set a new standard in the field but also open up
promising avenues for future research in extractive text summarization.

摘要：在數位文字以空前速度激增的時代，高效的摘要工具正變得不可或缺。儘管大型語言模型 (LLM) 已成功應用於各種自然語言處理任務中，但它們在萃取式文字摘要中的角色仍未得到充分探討。本文介紹 EYEGLAXS（用於萃取式摘要的簡易高效大型語言模型），一個利用 LLM（特別是 LLAMA2-7B 和 ChatGLM2-6B）的架構，針對冗長的文字文件進行萃取式摘要。EYEGLAXS 專注於萃取式摘要，以確保事實和語法的完整性，而非抽象方法，後者通常會出現事實不正確和幻覺等問題。EYEGLAXS 利用了最先進的技術，例如 Flash Attention 和參數高效微調 (PEFT)，解決了通常與 LLM 相關的運算和資源挑戰。該系統在 PubMed 和 ArXiv 等知名資料集上設定了新的效能基準。此外，我們透過額外的分析擴展了我們的研究，探討 LLM 在處理不同序列長度時的適應性，以及它們在較小資料集上訓練的效率。這些貢獻不僅為該領域樹立了新的標準，也為萃取式文字摘要的未來研究開闢了有前景的途徑。

##### **Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing**
2408.15800v1 by Kenneth Stewart, Michael Neumeier, Sumit Bam Shrestha, Garrick Orchard, Emre Neftci

Achieving personalized intelligence at the edge with real-time learning
capabilities holds enormous promise in enhancing our daily experiences and
helping decision making, planning, and sensing. However, efficient and reliable
edge learning remains difficult with current technology due to the lack of
personalized data, insufficient hardware capabilities, and inherent challenges
posed by online learning.
  Over time and across multiple developmental stages, the brain has evolved to
efficiently incorporate new knowledge by gradually building on previous
knowledge. In this work, we emulate the multiple stages of learning with
digital neuromorphic technology that simulates the neural and synaptic
processes of the brain using two stages of learning. First, a meta-training
stage trains the hyperparameters of synaptic plasticity for one-shot learning
using a differentiable simulation of the neuromorphic hardware. This
meta-training process refines a hardware local three-factor synaptic plasticity
rule and its associated hyperparameters to align with the trained task domain.
In a subsequent deployment stage, these optimized hyperparameters enable fast,
data-efficient, and accurate learning of new classes. We demonstrate our
approach using event-driven vision sensor data and the Intel Loihi neuromorphic
processor with its plasticity dynamics, achieving real-time one-shot learning
of new classes that is vastly improved over transfer learning. Our methodology
can be deployed with arbitrary plasticity models and can be applied to
situations demanding quick learning and adaptation at the edge, such as
navigating unfamiliar environments or learning unexpected categories of data
through user engagement.

摘要：透過即時學習能力在邊緣實現個人化智能，在提升我們的日常體驗以及協助決策制定、規劃和感測方面極具前景。然而，由於缺乏個人化資料、硬體功能不足以及線上學習所帶來的固有挑戰，使得使用現有技術進行有效且可靠的邊緣學習仍然困難。
隨著時間推移和經歷多個發展階段，大腦已演化為能有效整合新知識，並逐漸建立在先前知識的基礎上。在這項工作中，我們模擬學習的多個階段，使用數位神經型態技術，利用兩個學習階段模擬大腦的神經和突觸過程。首先，元訓練階段使用神經型態硬體的可微分模擬，針對一次性學習訓練突觸可塑性的超參數。這個元訓練過程精進了硬體局部三因子突觸可塑性規則及其相關超參數，以與訓練後的任務領域相符。在後續的部署階段，這些最佳化的超參數能快速、資料有效且準確地學習新類別。我們使用事件驅動的視覺感測器資料和 Intel Loihi 神經型態處理器及其可塑性動態，展示了我們的做法，在一次性學習新類別方面大幅優於遷移學習，達到了即時性。我們的做法可以用於任意可塑性模型，並可應用於需要在邊緣進行快速學習和適應的情況，例如導航不熟悉的環境，或透過使用者參與學習資料的意外類別。

##### **Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models**
2408.15796v1 by Hédi Zhegidi, Ludovic Moncla

This paper evaluates Few-Shot Prompting with Large Language Models for Named
Entity Recognition (NER). Traditional NER systems rely on extensive labeled
datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or
in-context learning enables models to recognize entities with minimal examples.
We assess state-of-the-art models like GPT-4 in NER tasks, comparing their
few-shot performance to fully supervised benchmarks. Results show that while
there is a performance gap, large models excel in adapting to new entity types
and domains with very limited data. We also explore the effects of prompt
engineering, guided output format and context length on performance. This study
underscores Few-Shot Learning's potential to reduce the need for large labeled
datasets, enhancing NER scalability and accessibility.

摘要：本文評估了使用大型語言模型進行命名實體辨識 (NER) 的少樣本提示。傳統的 NER 系統依賴於廣泛標記的資料集，而這些資料集的取得既昂貴又費時。少樣本提示或情境學習讓模型能夠以最少範例辨識實體。我們評估了 GPT-4 等最先進的模型在 NER 任務中的表現，並將它們的少樣本表現與完全監督的基準進行比較。結果顯示，儘管存在效能差距，但大型模型在適應極少資料的新實體類型和領域方面表現出色。我們還探討了提示工程、引導輸出格式和情境長度對效能的影響。這項研究強調了少樣本學習在減少對大型標記資料集需求的潛力，並增強了 NER 的可擴充性和可及性。

##### **Language Adaptation on a Tight Academic Compute Budget: Tokenizer Swapping Works and Pure bfloat16 Is Enough**
2408.15793v1 by Konstantin Dobler, Gerard de Melo

We investigate continued pretraining of LLMs for language adaptation on a
tight academic budget: a setting in which only a few GPUs can be used in
parallel, for a heavily constrained duration. We focus on adapting Mistral-7B
to German or Arabic and evaluate several techniques to improve efficiency and
effectiveness in this setting. Our German models adapted on this tight compute
budget underperform compared to the base Mistral-7B, while our Arabic models
outperform several baselines, showing that for sufficiently well-represented
languages, continued pretraining for specialization is not always helpful. Our
main findings focus on training precision and tokenizer swapping. Our results
show that pure bfloat16 training is a viable alternative to mixed-precision
training, while being much faster when only using a few GPUs. Swapping the
tokenizer for a specialized one yields more efficient tokenization and is
competitive with the original tokenizer, which already contains some German
tokens, but did not significantly increase performance for German. Code and
model weights are available at on GitHub.

摘要：我們研究了在嚴格的學術預算中，持續預訓練 LLM 以適應語言：一種只能並行使用少數 GPU，且時間受到嚴格限制的設定。我們專注於將 Mistral-7B 適應為德語或阿拉伯語，並評估了幾種技術，以提高此設定中的效率和效能。我們在嚴格的運算預算下適應的德語模型，與基礎的 Mistral-7B 相比表現不佳，而我們的阿拉伯語模型則優於幾個基準，顯示對於充分代表的語言，持續預訓練以進行專業化並非總是會有幫助。我們的重點發現集中於訓練精度和 tokenizer 交換。我們的結果顯示，純 bfloat16 訓練是混合精度訓練的可行替代方案，同時在僅使用少數 GPU 時速度快很多。將 tokenizer 交換為專門的 tokenizer 可產生更有效率的 tokenization，並且與原始 tokenizer（其中已包含一些德語 token）具有競爭力，但並未顯著提升德語的效能。程式碼和模型權重可在 GitHub 上取得。

##### **Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions**
2408.15787v1 by Huachuan Qiu, Zhenzhong Lan

Virtual counselors powered by large language models (LLMs) aim to create
interactive support systems that effectively assist clients struggling with
mental health challenges. To replicate counselor-client conversations,
researchers have built an online mental health platform that allows
professional counselors to provide clients with text-based counseling services
for about an hour per session. Notwithstanding its effectiveness, challenges
exist as human annotation is time-consuming, cost-intensive, privacy-protected,
and not scalable. To address this issue and investigate the applicability of
LLMs in psychological counseling conversation simulation, we propose a
framework that employs two LLMs via role-playing for simulating
counselor-client interactions. Our framework involves two LLMs, one acting as a
client equipped with a specific and real-life user profile and the other
playing the role of an experienced counselor, generating professional responses
using integrative therapy techniques. We implement both the counselor and the
client by zero-shot prompting the GPT-4 model. In order to assess the
effectiveness of LLMs in simulating counselor-client interactions and
understand the disparities between LLM- and human-generated conversations, we
evaluate the synthetic data from various perspectives. We begin by assessing
the client's performance through automatic evaluations. Next, we analyze and
compare the disparities between dialogues generated by the LLM and those
generated by professional counselors. Furthermore, we conduct extensive
experiments to thoroughly examine the performance of our LLM-based counselor
trained with synthetic interactive dialogues by benchmarking against
state-of-the-art models for mental health.

摘要：<paragraph>由大型語言模型 (LLM) 支援的虛擬諮商師，旨在建立互動式支援系統，有效協助有心理健康問題的客戶。為了複製諮商師與客戶的對話，研究人員建立了一個線上心理健康平台，讓專業諮商師能為客戶提供約一小時的文字諮商服務。儘管有效，但仍存在挑戰，因為人工標註耗時、成本高昂、受隱私保護且無法擴展。為了解決這個問題，並探討 LLM 在心理諮商對話模擬中的適用性，我們提出了一個架構，採用兩個 LLM 透過角色扮演來模擬諮商師與客戶的互動。我們的架構包含兩個 LLM，一個扮演具備特定且真實使用者個人資料的客戶，另一個扮演經驗豐富的諮商師的角色，使用整合治療技術產生專業的回應。我們透過零次提示 GPT-4 模型來實作諮商師和客戶。為了評估 LLM 在模擬諮商師與客戶互動中的有效性，並了解 LLM 和人類產生的對話之間的差異，我們從各種角度評估合成資料。我們從評估客戶的表現透過自動評估開始。接下來，我們分析並比較 LLM 產生的對話和專業諮商師產生的對話之間的差異。此外，我們進行廣泛的實驗，透過對心理健康領域最先進的模型進行基準測試，來徹底檢查我們基於 LLM 的諮商師在經過合成互動對話訓練後的表現。</paragraph>

##### **LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models**
2408.15778v1 by Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

Large Language Models (LLMs) have demonstrated notable capabilities across
various tasks, showcasing complex problem-solving abilities. Understanding and
executing complex rules, along with multi-step planning, are fundamental to
logical reasoning and critical for practical LLM agents and decision-making
systems. However, evaluating LLMs as effective rule-based executors and
planners remains underexplored. In this paper, we introduce LogicGame, a novel
benchmark designed to evaluate the comprehensive rule understanding, execution,
and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame
provides diverse games that contain a series of rules with an initial state,
requiring models to comprehend and apply predefined regulations to solve
problems. We create simulated scenarios in which models execute or plan
operations to achieve specific outcomes. These game scenarios are specifically
designed to distinguish logical reasoning from mere knowledge by relying
exclusively on predefined rules. This separation allows for a pure assessment
of rule-based reasoning capabilities. The evaluation considers not only final
outcomes but also intermediate steps, providing a comprehensive assessment of
model performance. Moreover, these intermediate steps are deterministic and can
be automatically verified. LogicGame defines game scenarios with varying
difficulty levels, from simple rule applications to complex reasoning chains,
in order to offer a precise evaluation of model performance on rule
understanding and multi-step execution. Utilizing LogicGame, we test various
LLMs and identify notable shortcomings in their rule-based logical reasoning
abilities.

摘要：大型語言模型 (LLM) 已在各種任務中展現出顯著的能力，展示出複雜的問題解決能力。理解和執行複雜的規則，以及多步驟規劃，是邏輯推理的基礎，對於實用的 LLM 代理和決策系統至關重要。然而，將 LLM 評估為有效的基於規則的執行者和規劃者仍未得到充分探討。在本文中，我們介紹了 LogicGame，這是一個新穎的基準，旨在評估 LLM 全面的規則理解、執行和規劃能力。與傳統基準不同，LogicGame 提供了包含一系列規則和初始狀態的不同遊戲，要求模型理解並應用預定義法規來解決問題。我們創建了模擬場景，其中模型執行或規劃操作以實現特定結果。這些遊戲場景經過專門設計，通過完全依賴預定義規則來區分邏輯推理和僅有的知識。這種分離允許對基於規則的推理能力進行純粹的評估。評估不僅考慮最終結果，還考慮中間步驟，對模型性能進行全面評估。此外，這些中間步驟是確定性的，可以自動驗證。LogicGame 定義了具有不同難度級別的遊戲場景，從簡單的規則應用到複雜的推理鏈，以便對模型在規則理解和多步驟執行方面的性能進行精確評估。利用 LogicGame，我們測試了各種 LLM，並發現它們在基於規則的邏輯推理能力方面存在顯著的缺陷。

##### **Easy, Interpretable, Effective: openSMILE for voice deepfake detection**
2408.15775v2 by Octavian Pascu, Dan Oneata, Horia Cucu, Nicolas M. Müller

In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --
a de facto standard in the field of voice authenticity and deepfake detection
-- can be identified with surprising accuracy using a small subset of very
simplistic features. These are derived from the openSMILE library, and are
scalar-valued, easy to compute, and human interpretable. For example, attack
A10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fide
instances have a mean length of 0.18 +- 0.07. Using this feature alone, a
threshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack
A10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall
EER of 15.7 +- 6.0%. We explore the generalization capabilities of these
features and find that some of them transfer effectively between attacks,
primarily when the attacks originate from similar Text-to-Speech (TTS)
architectures. This finding may indicate that voice anti-spoofing is, in part,
a problem of identifying and remembering signatures or fingerprints of
individual TTS systems. This allows to better understand anti-spoofing models
and their challenges in real-world application.

摘要：在本文中，我們證明了在最新的 ASVspoof5 資料集中攻擊 --
語音真實性和深度偽造偵測領域的既定標準
-- 可以使用非常簡單特徵的子集識別出驚人的準確度。這些特徵來自 openSMILE 函式庫，是標量值，易於計算，且人類可以理解。例如，攻擊 A10 的無聲區段平均長度為 0.09 +- 0.02，而真實案例的平均長度為 0.18 +- 0.07。單獨使用此特徵，閾值分類器可為攻擊 A10 達到 10.3% 的等錯誤率 (EER)。類似地，在所有攻擊中，我們達到 0.8% 的 EER，整體 EER 為 15.7 +- 6.0%。我們探討了這些特徵的泛化能力，發現其中一些特徵在攻擊之間有效轉移，特別是在攻擊源自於類似的文字轉語音 (TTS) 架構時。此發現可能表明語音防偽在某種程度上是識別和記憶個別 TTS 系統的簽章或指紋的問題。這有助於我們進一步了解防偽模型及其在現實世界應用中的挑戰。

