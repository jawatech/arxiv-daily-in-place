{"2501.18596": {"publish_time": "2025-01-30", "title": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights", "paper_summary": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86 DeltaLLM\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u8a13\u7df4\u5f8c\u58d3\u7e2e\u6280\u8853\uff0c\u7528\u65bc\u6e1b\u5c11 LLM \u7684\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7d50\u69cb\u5316 LLM \u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5728\u5f8c\u7e8c Transformer \u5340\u584a\u4e2d\u7684\u5c64\u4e4b\u9593\u5171\u4eab\u6b0a\u91cd\uff0c\u4e26\u5728\u5b83\u5011\u4e4b\u9593\u6dfb\u52a0\u4f4e\u79e9\u5dee\u5206\u77e9\u9663\u3002\u5728\u8a13\u7df4\u65b9\u9762\uff0c\u6211\u5011\u63a1\u7528\u4e86\u9010\u6f38\u66ff\u63db\u6a21\u7d44\u7684\u65b9\u6cd5\uff0c\u4e26\u8868\u660e\u4f4e\u79e9\u6a21\u7d44\u7684\u8f15\u91cf\u5316\u8a13\u7df4\uff0c\u5927\u7d04\u6709 30M-40M \u500b\u7b26\u865f\uff0c\u8db3\u4ee5\u9054\u5230\u8207\u5f9e\u982d\u8a13\u7df4\u7684\u540c\u7b49\u5927\u5c0f\u7684 LLM \u76f8\u7576\u7684\u6548\u80fd\u3002\u6211\u5011\u767c\u5e03\u4e86\u7d50\u679c\u6a21\u578b DeltaLLAMA \u548c DeltaPHI\uff0c\u5b83\u5011\u7684\u53c3\u6578\u6e1b\u5c11\u4e86 12%\uff0c\u5728\u5e38\u898b\u77e5\u8b58\u548c\u63a8\u7406\u57fa\u6e96\u4e0a\u4fdd\u7559\u4e86 Llama \u548c Phi \u57fa\u790e\u6a21\u578b 90% \u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6280\u8853\u4e5f\u512a\u65bc\u58d3\u7e2e\u6280\u8853 JointDrop\u3001LaCo\u3001ShortGPT \u548c SliceGPT\uff0c\u800c\u79fb\u9664\u7684\u53c3\u6578\u6578\u91cf\u76f8\u540c\u3002\u4f8b\u5982\uff0cDeltaPhi 2.9B \u6e1b\u5c0f\u4e86 24%\uff0c\u9054\u5230\u4e86\u8207\u4f7f\u7528 12% \u6e1b\u5c0f\u7684 SlicedPhi 3.3B \u9032\u884c\u6062\u5fa9\u5fae\u8abf\u5f8c\u985e\u4f3c\u7684\u5e73\u5747\u96f6\u6b21\u5b78\u7fd2\u6e96\u78ba\u7387\uff0c\u5118\u7ba1\u5b83\u5927\u7d04\u5c0f\u4e86 400M \u53c3\u6578\u4e14\u672a\u5957\u7528\u5fae\u8abf\u3002\u7576\u5132\u5b58\u7a7a\u9593\u81f3\u95dc\u91cd\u8981\u6642\uff0c\u9019\u9805\u5de5\u4f5c\u70ba LLM \u67b6\u69cb\u8a2d\u8a08\u548c\u58d3\u7e2e\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002", "author": "Liana Mikaelyan et.al.", "authors": "Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz", "id": "2501.18596v1", "paper_url": "http://arxiv.org/abs/2501.18596v1", "repo": "null"}}