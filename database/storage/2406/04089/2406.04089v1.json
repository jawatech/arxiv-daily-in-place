{"2406.04089": {"publish_time": "2024-06-06", "title": "On Limitation of Transformer for Learning HMMs", "paper_summary": "Despite the remarkable success of Transformer-based architectures in various\nsequential modeling tasks, such as natural language processing, computer\nvision, and robotics, their ability to learn basic sequential models, like\nHidden Markov Models (HMMs), is still unclear. This paper investigates the\nperformance of Transformers in learning HMMs and their variants through\nextensive experimentation and compares them to Recurrent Neural Networks\n(RNNs). We show that Transformers consistently underperform RNNs in both\ntraining speed and testing accuracy across all tested HMM models. There are\neven challenging HMM instances where Transformers struggle to learn, while RNNs\ncan successfully do so. Our experiments further reveal the relation between the\ndepth of Transformers and the longest sequence length it can effectively learn,\nbased on the types and the complexity of HMMs. To address the limitation of\ntransformers in modeling HMMs, we demonstrate that a variant of the\nChain-of-Thought (CoT), called $\\textit{block CoT}$ in the training phase, can\nhelp transformers to reduce the evaluation error and to learn longer sequences\nat a cost of increasing the training time. Finally, we complement our empirical\nfindings by theoretical results proving the expressiveness of transformers in\napproximating HMMs with logarithmic depth.", "paper_summary_zh": "\u5118\u7ba1 Transformer \u57fa\u790e\u67b6\u69cb\u5728\u5404\u7a2e\u5e8f\u5217\u5efa\u6a21\u4efb\u52d9\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4f8b\u5982\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3001\u96fb\u8166\u8996\u89ba\u548c\u6a5f\u5668\u4eba\u6280\u8853\uff0c\u4f46\u5b83\u5011\u5b78\u7fd2\u57fa\u672c\u5e8f\u5217\u6a21\u578b\uff08\u4f8b\u5982\u96b1\u85cf\u99ac\u53ef\u592b\u6a21\u578b (HMM)\uff09\u7684\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u6587\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\u8abf\u67e5 Transformer \u5728\u5b78\u7fd2 HMM \u53ca\u5176\u8b8a\u9ad4\u65b9\u9762\u7684\u6548\u80fd\uff0c\u4e26\u5c07\u5b83\u5011\u8207\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN) \u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u8868\u660e\uff0c\u5728\u6240\u6709\u6e2c\u8a66\u7684 HMM \u6a21\u578b\u4e2d\uff0cTransformer \u5728\u8a13\u7df4\u901f\u5ea6\u548c\u6e2c\u8a66\u6e96\u78ba\u5ea6\u65b9\u9762\u90fd\u6301\u7e8c\u8868\u73fe\u4e0d\u5982 RNN\u3002\u751a\u81f3\u6709\u4e00\u4e9b\u5177\u6709\u6311\u6230\u6027\u7684 HMM \u5be6\u4f8b\u8b93 Transformer \u96e3\u4ee5\u5b78\u7fd2\uff0c\u800c RNN \u53ef\u4ee5\u6210\u529f\u505a\u5230\u3002\u6211\u5011\u7684\u5be6\u9a57\u9032\u4e00\u6b65\u63ed\u793a\u4e86 Transformer \u7684\u6df1\u5ea6\u8207\u5b83\u53ef\u4ee5\u6709\u6548\u5b78\u7fd2\u7684\u6700\u9577\u5e8f\u5217\u9577\u5ea6\u4e4b\u9593\u7684\u95dc\u4fc2\uff0c\u9019\u53d6\u6c7a\u65bc HMM \u7684\u985e\u578b\u548c\u8907\u96dc\u6027\u3002\u70ba\u4e86\u89e3\u6c7a Transformer \u5728\u5efa\u6a21 HMM \u65b9\u9762\u7684\u9650\u5236\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5728\u8a13\u7df4\u968e\u6bb5\u7a31\u70ba $\\textit{\u5340\u584a CoT}$ \u7684\u601d\u7dad\u93c8 (CoT) \u8b8a\u9ad4\uff0c\u53ef\u4ee5\u5e6b\u52a9 Transformer \u6e1b\u5c11\u8a55\u4f30\u8aa4\u5dee\u4e26\u5728\u589e\u52a0\u8a13\u7df4\u6642\u9593\u7684\u4ee3\u50f9\u4e0b\u5b78\u7fd2\u66f4\u9577\u7684\u5e8f\u5217\u3002\u6700\u5f8c\uff0c\u6211\u5011\u900f\u904e\u8b49\u660e Transformer \u5728\u4f7f\u7528\u5c0d\u6578\u6df1\u5ea6\u8fd1\u4f3c HMM \u6642\u7684\u8868\u73fe\u529b\uff0c\u4ee5\u7406\u8ad6\u7d50\u679c\u88dc\u5145\u6211\u5011\u7684\u7d93\u9a57\u767c\u73fe\u3002", "author": "Jiachen Hu et.al.", "authors": "Jiachen Hu, Qinghua Liu, Chi Jin", "id": "2406.04089v1", "paper_url": "http://arxiv.org/abs/2406.04089v1", "repo": "null"}}