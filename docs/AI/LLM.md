
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217v1](http://arxiv.org/abs/2405.12217v1)|[link](https://github.com/jameszhou-gl/icl-distribution-shift)|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209v1](http://arxiv.org/abs/2405.12209v1)|[link](https://github.com/open-compass/mathbench)|
|**2024-05-20**|**Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models**|Tong Zeng et.al.|[2405.12206v1](http://arxiv.org/abs/2405.12206v1)|[link](https://github.com/sciosci/cite-worthiness)|
|**2024-05-20**|**Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving**|Aniket Didolkar et.al.|[2405.12205v1](http://arxiv.org/abs/2405.12205v1)|null|
|**2024-05-20**|**Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution**|Xihaier Luo et.al.|[2405.12202v1](http://arxiv.org/abs/2405.12202v1)|null|
|**2024-05-20**|**Multi-order Graph Clustering with Adaptive Node-level Weight Learning**|Ye Liu et.al.|[2405.12183v1](http://arxiv.org/abs/2405.12183v1)|[link](https://github.com/scutft-ml/mogc)|
|**2024-05-20**|**Building Temporal Kernels with Orthogonal Polynomials**|Yan Ru Pei et.al.|[2405.12179v1](http://arxiv.org/abs/2405.12179v1)|[link](https://github.com/peabrane/pleiades)|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174v1](http://arxiv.org/abs/2405.12174v1)|null|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163v1](http://arxiv.org/abs/2405.12163v1)|[link](https://github.com/dropreg/fennec)|
|**2024-05-20**|**Bangladeshi Native Vehicle Detection in Wild**|Bipin Saha et.al.|[2405.12150v1](http://arxiv.org/abs/2405.12150v1)|[link](https://github.com/bipin-saha/bnvd)|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147v1](http://arxiv.org/abs/2405.12147v1)|null|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130v1](http://arxiv.org/abs/2405.12130v1)|[link](https://github.com/kongds/mora)|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119v1](http://arxiv.org/abs/2405.12119v1)|null|
|**2024-05-20**|**Linguistic Structure from a Bottleneck on Sequential Information Processing**|Richard Futrell et.al.|[2405.12109v1](http://arxiv.org/abs/2405.12109v1)|null|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107v1](http://arxiv.org/abs/2405.12107v1)|[link](https://github.com/milvlg/imp)|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100v1](http://arxiv.org/abs/2405.12100v1)|null|
|**2024-05-20**|**Distributional Semantics, Holism, and the Instability of Meaning**|Jumbly Grindrod et.al.|[2405.12084v1](http://arxiv.org/abs/2405.12084v1)|null|
|**2024-05-20**|**Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model**|Chen Huang et.al.|[2405.12081v1](http://arxiv.org/abs/2405.12081v1)|null|
|**2024-05-20**|**AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements**|Calvin Yeung et.al.|[2405.12070v1](http://arxiv.org/abs/2405.12070v1)|[link](https://github.com/calvinyeungck/3d-shot-posture-dataset)|
|**2024-05-20**|**CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models**|Tong Zhang et.al.|[2405.12063v1](http://arxiv.org/abs/2405.12063v1)|[link](https://github.com/zt991211/clamber)|
|**2024-05-20**|**STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents**|Yue Chen et.al.|[2405.12059v1](http://arxiv.org/abs/2405.12059v1)|null|
|**2024-05-20**|**Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics**|Olga Kellert et.al.|[2405.12055v1](http://arxiv.org/abs/2405.12055v1)|null|
|**2024-05-20**|**KG-RAG: Bridging the Gap Between Knowledge and Creativity**|Diego Sanmartin et.al.|[2405.12035v1](http://arxiv.org/abs/2405.12035v1)|null|
|**2024-05-20**|**Can AI Relate: Testing Large Language Model Response for Mental Health Support**|Saadia Gabriel et.al.|[2405.12021v1](http://arxiv.org/abs/2405.12021v1)|null|
|**2024-05-20**|**Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning**|Hai Zhang et.al.|[2405.12001v1](http://arxiv.org/abs/2405.12001v1)|null|
|**2024-05-20**|**A review on the use of large language models as virtual tutors**|Silvia García-Méndez et.al.|[2405.11983v1](http://arxiv.org/abs/2405.11983v1)|null|
|**2024-05-20**|**Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space**|Qianmei Liu et.al.|[2405.11982v1](http://arxiv.org/abs/2405.11982v1)|null|
|**2024-05-20**|**SM-DTW: Stability Modulated Dynamic Time Warping for signature verification**|Antonio Parziale et.al.|[2405.11978v1](http://arxiv.org/abs/2405.11978v1)|null|
|**2024-05-20**|**Conditional Shift-Robust Conformal Prediction for Graph Neural Network**|S. Akansha et.al.|[2405.11968v1](http://arxiv.org/abs/2405.11968v1)|null|
|**2024-05-20**|**Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home**|Tatiana V. Afanasieva et.al.|[2405.11967v1](http://arxiv.org/abs/2405.11967v1)|null|
|**2024-05-20**|**Multiple-Choice Questions are Efficient and Robust LLM Evaluators**|Ziyin Zhang et.al.|[2405.11966v2](http://arxiv.org/abs/2405.11966v2)|[link](https://github.com/geralt-targaryen/mc-evaluation)|
|**2024-05-20**|**WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles**|Tabea M. G. Pakull et.al.|[2405.11950v1](http://arxiv.org/abs/2405.11950v1)|null|
|**2024-05-20**|**FAME-MT Dataset: Formality Awareness Made Easy for Machine Translation Purposes**|Dawid Wiśniewski et.al.|[2405.11942v1](http://arxiv.org/abs/2405.11942v1)|[link](https://github.com/laniqo-public/fame-mt)|
|**2024-05-20**|**Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT Model on an Automatically Generated Wikipedia Corpus**|Fons Hartendorp et.al.|[2405.11941v1](http://arxiv.org/abs/2405.11941v1)|[link](https://github.com/fonshartendorp/dutch_biomedical_entity_linking)|
|**2024-05-20**|**Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation**|Kamil Guttmann et.al.|[2405.11937v1](http://arxiv.org/abs/2405.11937v1)|null|
|**2024-05-20**|**"Set It Up!": Functional Object Arrangement with Compositional Generative Models**|Yiqing Xu et.al.|[2405.11928v1](http://arxiv.org/abs/2405.11928v1)|null|
|**2024-05-20**|**On Efficient and Statistical Quality Estimation for Data Annotation**|Jan-Christoph Klie et.al.|[2405.11919v1](http://arxiv.org/abs/2405.11919v1)|null|
|**2024-05-20**|**ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation**|Chen Huang et.al.|[2405.11912v1](http://arxiv.org/abs/2405.11912v1)|null|
|**2024-05-20**|**A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers**|Tom Roth et.al.|[2405.11904v1](http://arxiv.org/abs/2405.11904v1)|null|
|**2024-05-20**|**CReMa: Crisis Response through Computational Identification and Matching of Cross-Lingual Requests and Offers Shared on Social Media**|Rabindra Lamsal et.al.|[2405.11897v1](http://arxiv.org/abs/2405.11897v1)|null|
|**2024-05-20**|**Unveiling and Manipulating Prompt Influence in Large Language Models**|Zijian Feng et.al.|[2405.11891v1](http://arxiv.org/abs/2405.11891v1)|[link](https://github.com/zijian678/tdd)|
|**2024-05-20**|**Out-of-Distribution Detection with a Single Unconditional Diffusion Model**|Alvin Heng et.al.|[2405.11881v1](http://arxiv.org/abs/2405.11881v1)|[link](https://github.com/clear-nus/diffpath)|
|**2024-05-20**|**Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs**|Siyu Lou et.al.|[2405.11880v1](http://arxiv.org/abs/2405.11880v1)|null|
|**2024-05-20**|**A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus**|Eduard Poesina et.al.|[2405.11877v2](http://arxiv.org/abs/2405.11877v2)|[link](https://github.com/eduard6421/ronli)|
|**2024-05-20**|**xFinder: Robust and Pinpoint Answer Extraction for Large Language Models**|Qingchen Yu et.al.|[2405.11874v1](http://arxiv.org/abs/2405.11874v1)|null|
|**2024-05-20**|**Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process**|Ermo Hua et.al.|[2405.11870v1](http://arxiv.org/abs/2405.11870v1)|null|
|**2024-05-20**|**Towards Graph Contrastive Learning: A Survey and Beyond**|Wei Ju et.al.|[2405.11868v1](http://arxiv.org/abs/2405.11868v1)|null|
|**2024-05-20**|**CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English**|Andrew Rueda et.al.|[2405.11865v1](http://arxiv.org/abs/2405.11865v1)|null|
|**2024-05-20**|**Alternators For Sequence Modeling**|Mohammad Reza Rezaei et.al.|[2405.11848v1](http://arxiv.org/abs/2405.11848v1)|null|
|**2024-05-20**|**Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities**|Junqi Wang et.al.|[2405.11841v1](http://arxiv.org/abs/2405.11841v1)|[link](https://github.com/bigai-ai/evaluate-n-model-social-intelligence)|
|**2024-05-20**|**Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model**|Mounes Zaval et.al.|[2405.11837v1](http://arxiv.org/abs/2405.11837v1)|null|
|**2024-05-20**|**Beyond MLE: Investigating SEARNN for Low-Resourced Neural Machine Translation**|Chris Emezue et.al.|[2405.11819v1](http://arxiv.org/abs/2405.11819v1)|null|
|**2024-05-20**|**Systematic Review on Healthcare Systems Engineering utilizing ChatGPT**|Jungwoo Kim et.al.|[2405.11817v1](http://arxiv.org/abs/2405.11817v1)|null|
|**2024-05-20**|**Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning**|Anastasios Foliadis et.al.|[2405.11816v1](http://arxiv.org/abs/2405.11816v1)|null|
|**2024-05-20**|**Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling**|Masato Sakai et.al.|[2405.11814v1](http://arxiv.org/abs/2405.11814v1)|null|
|**2024-05-20**|**Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices**|Baiyu Pan et.al.|[2405.11809v1](http://arxiv.org/abs/2405.11809v1)|null|
|**2024-05-20**|**(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts**|Minghao Wu et.al.|[2405.11804v1](http://arxiv.org/abs/2405.11804v1)|null|
|**2024-05-20**|**Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines**|Yueqiao Jin et.al.|[2405.11800v1](http://arxiv.org/abs/2405.11800v1)|null|
|**2024-05-20**|**Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing**|Shinyoung Kang et.al.|[2405.11783v1](http://arxiv.org/abs/2405.11783v1)|null|
|**2024-05-20**|**Efficient Multi-agent Reinforcement Learning by Planning**|Qihan Liu et.al.|[2405.11778v1](http://arxiv.org/abs/2405.11778v1)|[link](https://github.com/liuqh16/mazero)|
|**2024-05-20**|**Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques**|Siva Rajesh Kasa et.al.|[2405.11775v1](http://arxiv.org/abs/2405.11775v1)|null|
|**2024-05-20**|**From SHAP Scores to Feature Importance Scores**|Olivier Letoffe et.al.|[2405.11766v1](http://arxiv.org/abs/2405.11766v1)|null|
|**2024-05-20**|**Fed-Credit: Robust Federated Learning with Credibility Management**|Jiayan Chen et.al.|[2405.11758v1](http://arxiv.org/abs/2405.11758v1)|null|
|**2024-05-20**|**Contactless Polysomnography: What Radio Waves Tell Us about Sleep**|Hao He et.al.|[2405.11739v1](http://arxiv.org/abs/2405.11739v1)|null|
|**2024-05-20**|**Token-wise Influential Training Data Retrieval for Large Language Models**|Huawei Lin et.al.|[2405.11724v1](http://arxiv.org/abs/2405.11724v1)|[link](https://github.com/huawei-lin/rapidin)|
|**2024-05-20**|**Semantic Trajectory Data Mining with LLM-Informed POI Classification**|Yifan Liu et.al.|[2405.11715v1](http://arxiv.org/abs/2405.11715v1)|null|
|**2024-05-20**|**OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework**|Jian Hu et.al.|[2405.11143v1](http://arxiv.org/abs/2405.11143v1)|[link](https://github.com/OpenLLMAI/OpenRLHF)|
|**2024-05-20**|**Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!**|Dean Allemang et.al.|[2405.11706v1](http://arxiv.org/abs/2405.11706v1)|null|
|**2024-05-20**|**Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks**|Taiyuan Mei et.al.|[2405.11704v1](http://arxiv.org/abs/2405.11704v1)|null|
|**2024-05-19**|**ColorFoil: Investigating Color Blindness in Large Vision and Language Models**|Ahnaf Mozib Samin et.al.|[2405.11685v1](http://arxiv.org/abs/2405.11685v1)|[link](https://github.com/samin9796/colorfoil)|
|**2024-05-19**|**Deep Ensemble Art Style Recognition**|Orfeas Menis-Mastromichalakis et.al.|[2405.11675v1](http://arxiv.org/abs/2405.11675v1)|null|
|**2024-05-19**|**On the Expressivity of Recurrent Neural Cascades with Identity**|Nadezda A. Knorozova et.al.|[2405.11657v1](http://arxiv.org/abs/2405.11657v1)|null|
|**2024-05-19**|**URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images**|Zoey Chen et.al.|[2405.11656v1](http://arxiv.org/abs/2405.11656v1)|null|
|**2024-05-19**|**Track Anything Rapter(TAR)**|Tharun V. Puthanveettil et.al.|[2405.11655v1](http://arxiv.org/abs/2405.11655v1)|[link](https://github.com/tvpian/project-tar)|
|**2024-05-19**|**Hummer: Towards Limited Competitive Preference Dataset**|Li Jiang et.al.|[2405.11647v2](http://arxiv.org/abs/2405.11647v2)|null|
|**2024-05-19**|**Inquire, Interact, and Integrate: A Proactive Agent Collaborative Framework for Zero-Shot Multimodal Medical Reasoning**|Zishan Gu et.al.|[2405.11640v1](http://arxiv.org/abs/2405.11640v1)|null|
|**2024-05-19**|**Zero-Shot Stance Detection using Contextual Data Generation with LLMs**|Ghazaleh Mahmoudi et.al.|[2405.11637v1](http://arxiv.org/abs/2405.11637v1)|[link](https://github.com/Babakbehkamkia/GPT-Stance-Detection)|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records**|Mireia Hernandez Caralt et.al.|[2405.11622v1](http://arxiv.org/abs/2405.11622v1)|null|
|**2024-05-19**|**Novel Interpretable and Robust Web-based AI Platform for Phishing Email Detection**|Abdulla Al-Subaiey et.al.|[2405.11619v1](http://arxiv.org/abs/2405.11619v1)|null|
|**2024-05-19**|**Transcriptomics-guided Slide Representation Learning in Computational Pathology**|Guillaume Jaume et.al.|[2405.11618v1](http://arxiv.org/abs/2405.11618v1)|[link](https://github.com/mahmoodlab/tangle)|
|**2024-05-19**|**Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts**|Baolong Bi et.al.|[2405.11613v2](http://arxiv.org/abs/2405.11613v2)|null|
|**2024-05-19**|**AI-Assisted Diagnosis for Covid-19 CXR Screening: From Data Collection to Clinical Validation**|Carlo Alberto Barbano et.al.|[2405.11598v1](http://arxiv.org/abs/2405.11598v1)|null|
|**2024-05-19**|**Language Reconstruction with Brain Predictive Coding from fMRI Data**|Congchi Yin et.al.|[2405.11597v1](http://arxiv.org/abs/2405.11597v1)|null|
|**2024-05-19**|**SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization**|Jialong Guo et.al.|[2405.11582v1](http://arxiv.org/abs/2405.11582v1)|[link](https://github.com/xinghaochen/slab)|
|**2024-05-19**|**Exploring the Capabilities of Prompted Large Language Models in Educational and Assessment Applications**|Subhankar Maity et.al.|[2405.11579v1](http://arxiv.org/abs/2405.11579v1)|null|
|**2024-05-19**|**A Multi-Perspective Analysis of Memorization in Large Language Models**|Bowen Chen et.al.|[2405.11577v1](http://arxiv.org/abs/2405.11577v1)|null|
|**2024-05-19**|**SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks**|Xuanli He et.al.|[2405.11575v1](http://arxiv.org/abs/2405.11575v1)|null|
|**2024-05-19**|**Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification**|Manan Shah et.al.|[2405.11574v1](http://arxiv.org/abs/2405.11574v1)|[link](https://github.com/cs-mshah/CDUL)|
|**2024-05-19**|**An Invisible Backdoor Attack Based On Semantic Feature**|Yangming Chen et.al.|[2405.11551v1](http://arxiv.org/abs/2405.11551v1)|null|
|**2024-05-19**|**VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications**|Mikhail Konenkov et.al.|[2405.11537v1](http://arxiv.org/abs/2405.11537v1)|null|
|**2024-05-19**|**Knowledge Graph Pruning for Recommendation**|Fake Lin et.al.|[2405.11531v1](http://arxiv.org/abs/2405.11531v1)|null|
|**2024-05-19**|**Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors**|Chun-Yin Huang et.al.|[2405.11525v1](http://arxiv.org/abs/2405.11525v1)|null|
|**2024-05-19**|**Simple-Sampling and Hard-Mixup with Prototypes to Rebalance Contrastive Learning for Text Classification**|Mengyu Li et.al.|[2405.11524v1](http://arxiv.org/abs/2405.11524v1)|null|
|**2024-05-19**|**MSNER: A Multilingual Speech Dataset for Named Entity Recognition**|Quentin Meeus et.al.|[2405.11519v1](http://arxiv.org/abs/2405.11519v1)|null|
|**2024-05-19**|**NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation**|Zhiyu Xu et.al.|[2405.11476v1](http://arxiv.org/abs/2405.11476v1)|null|
|**2024-05-19**|**FIFO-Diffusion: Generating Infinite Videos from Text without Training**|Jihwan Kim et.al.|[2405.11473v1](http://arxiv.org/abs/2405.11473v1)|null|
|**2024-05-19**|**VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting**|Yingnan Yang et.al.|[2405.11470v1](http://arxiv.org/abs/2405.11470v1)|null|
|**2024-05-19**|**Effective In-Context Example Selection through Data Compression**|Zhongxiang Sun et.al.|[2405.11465v1](http://arxiv.org/abs/2405.11465v1)|null|
|**2024-05-19**|**Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion**|Pengxiang Lan et.al.|[2405.11464v1](http://arxiv.org/abs/2405.11464v1)|null|

#### Abstracts
##### **Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**
2405.12217v1 by Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Salman Khan, Xin Gao, Lina Yao

Recent studies indicate that large multimodal models (LMMs) are highly robust
against natural distribution shifts, often surpassing previous baselines.
Despite this, domain-specific adaptation is still necessary, particularly in
specialized areas like healthcare. Due to the impracticality of fine-tuning
LMMs given their vast parameter space, this work investigates in-context
learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We
find that the success of ICL heavily relies on the choice of demonstration,
mirroring challenges seen in large language models but introducing unique
complexities for LMMs facing distribution shifts. Our study addresses this by
evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context
examples through a nearest example search based on feature similarity. We
uncover that its effectiveness is limited by the deficiencies of pre-trained
vision encoders under distribution shift scenarios. To address these
challenges, we propose InvariantSelectPR, a novel method leveraging
Class-conditioned Contrastive Invariance (CCI) for more robust demonstration
selection. Specifically, CCI enhances pre-trained vision encoders by improving
their discriminative capabilities across different classes and ensuring
invariance to domain-specific variations. This enhancement allows the encoders
to effectively identify and retrieve the most informative examples, which are
then used to guide LMMs in adapting to new query samples under varying
distributions. Our experiments show that InvariantSelectPR substantially
improves the adaptability of LMMs, achieving significant performance gains on
benchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot on
Camelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to the
baseline zero-shot performance.

摘要：<paragraph>最近的研究表明，大型多模态模型 (LMM) 对于自然分布位移具有高度鲁棒性，通常超越之前的基准。
尽管如此，领域特定适应仍然是必要的，尤其是在医疗保健等专业领域。由于 LMM 参数空间巨大，微调不切实际，因此这项工作研究了语境学习 (ICL) 作为增强 LMM 适应性的有效替代方案。我们
发现 ICL 的成功在很大程度上取决于演示的选择，这反映了在大语言模型中看到的挑战，但为面临分布位移的 LMM 引入了独特的复杂性。我们的研究通过评估无监督 ICL 方法 TopKNearestPR 来解决这个问题，该方法通过基于特征相似性的最近示例搜索来选择语境示例。我们
发现其有效性受到预训练视觉编码器在分布位移场景下的缺陷的限制。为了解决这些
挑战，我们提出了 InvariantSelectPR，这是一种利用类条件对比不变性 (CCI) 进行更稳健的演示选择的新方法。具体来说，CCI 通过提高预训练视觉编码器在不同类别的判别能力并确保
对特定领域的变化保持不变性来增强它们。这种增强使编码器能够有效识别和检索信息最丰富的示例，然后用于指导 LMM 在不同分布下适应新的查询样本。我们的实验表明，InvariantSelectPR 大大提高了 LMM 的适应性，在基准数据集上实现了显著的性能提升，在 Camelyon17 上的 7 次拍摄准确率提高了 34.2%，在 HAM10000 上的 7 次拍摄准确率提高了 16.9%，与
基线零次拍摄性能相比。</paragraph>

##### **MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**
2405.12209v1 by Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, Kai Chen

Recent advancements in large language models (LLMs) have showcased
significant improvements in mathematics. However, traditional math benchmarks
like GSM8k offer a unidimensional perspective, falling short in providing a
holistic assessment of the LLMs' math capabilities. To address this gap, we
introduce MathBench, a new benchmark that rigorously assesses the mathematical
capabilities of large language models. MathBench spans a wide range of
mathematical disciplines, offering a detailed evaluation of both theoretical
understanding and practical problem-solving skills. The benchmark progresses
through five distinct stages, from basic arithmetic to college mathematics, and
is structured to evaluate models at various depths of knowledge. Each stage
includes theoretical questions and application problems, allowing us to measure
a model's mathematical proficiency and its ability to apply concepts in
practical scenarios. MathBench aims to enhance the evaluation of LLMs'
mathematical abilities, providing a nuanced view of their knowledge
understanding levels and problem solving skills in a bilingual context. The
project is released at https://github.com/open-compass/MathBench .

摘要：大型語言模型 (LLM) 近期進展已展示出數學方面的重大改進。然而，傳統數學基準（例如 GSM8k）提供了一種單向度觀點，無法全面評估 LLM 的數學能力。為了解決這個差距，我們引入了 MathBench，這是一個新的基準，可以嚴格評估大型語言模型的數學能力。MathBench 涵蓋了廣泛的數學學科，對理論理解和實際問題解決技能進行了詳細評估。基準分為五個不同的階段，從基礎算術到大學數學，並旨在評估模型在不同知識深度下的表現。每個階段都包含理論問題和應用問題，讓我們可以衡量模型的數學能力及其在實際場景中應用概念的能力。MathBench 旨在加強對 LLM 數學能力的評估，在雙語環境中提供其知識理解層次和問題解決技能的細緻觀點。該專案已在 https://github.com/open-compass/MathBench 發布。

##### **Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models**
2405.12206v1 by Tong Zeng, Daniel E. Acuna

Scientist learn early on how to cite scientific sources to support their
claims. Sometimes, however, scientists have challenges determining where a
citation should be situated -- or, even worse, fail to cite a source
altogether. Automatically detecting sentences that need a citation (i.e.,
citation worthiness) could solve both of these issues, leading to more robust
and well-constructed scientific arguments. Previous researchers have applied
machine learning to this task but have used small datasets and models that do
not take advantage of recent algorithmic developments such as attention
mechanisms in deep learning. We hypothesize that we can develop significantly
accurate deep learning architectures that learn from large supervised datasets
constructed from open access publications. In this work, we propose a
Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism
and contextual information to detect sentences that need citations. We also
produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,
which is orders of magnitude larger than previous datasets. Our experiments
show that our architecture achieves state of the art performance on the
standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance
($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer
learning across these datasets. We further use interpretable models to
illuminate how specific language is used to promote and inhibit citations. We
discover that sections and surrounding sentences are crucial for our improved
predictions. We further examined purported mispredictions of the model, and
uncovered systematic human mistakes in citation behavior and source data. This
opens the door for our model to check documents during pre-submission and
pre-archival procedures. We make this new dataset, the code, and a web-based
tool available to the community.

摘要：<paragraph>科學家很早就學會如何引用科學文獻來支持他們的說法。然而，有時候，科學家在決定引文應置於何處時會遇到挑戰，或者更糟的是，根本沒有引用來源。自動偵測需要引用的句子（即引用的價值）可以解決這兩個問題，從而產生更強大且結構良好的科學論證。先前的研究人員已將機器學習應用於此任務，但使用了小型資料集和模型，這些模型並未利用深度學習中的最新演算法發展，例如注意力機制。我們假設我們可以開發出從由開放獲取出版物建構的大型監督式資料集中學習的顯著準確深度學習架構。在這項工作中，我們提出了一個具有注意力機制和上下文資訊的雙向長短期記憶 (BiLSTM) 網路，以偵測需要引用的句子。我們還根據 PubMed 開放獲取子集製作了一個新的大型資料集 (PMOA-CITE)，其數量級大於先前的資料集。我們的實驗表明，我們的架構在標準 ACL-ARC 資料集 ($F_{1}=0.507$) 上達到了最先進的效能，並在新的 PMOA-CITE 上表現出高效能 ($F_{1}=0.856$)。此外，我們展示了它可以在這些資料集之間轉移學習。我們進一步使用可解釋模型來說明如何使用特定語言來促進和抑制引用。我們發現章節和周圍的句子對於我們改進的預測至關重要。我們進一步檢查了模型的假定錯誤預測，並發現了引用行為和來源資料中的人為系統性錯誤。這為我們的模型在提交前和歸檔前程序中檢查文件開啟了大門。我們將這個新資料集、程式碼和一個基於網路的工具提供給社群使用。</paragraph>

##### **Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving**
2405.12205v1 by Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora

Metacognitive knowledge refers to humans' intuitive knowledge of their own
thinking and reasoning processes. Today's best LLMs clearly possess some
reasoning processes. The paper gives evidence that they also have metacognitive
knowledge, including ability to name skills and procedures to apply given a
task. We explore this primarily in context of math reasoning, developing a
prompt-guided interaction procedure to get a powerful LLM to assign sensible
skill labels to math questions, followed by having it perform semantic
clustering to obtain coarser families of skill labels. These coarse skill
labels look interpretable to humans.
  To validate that these skill labels are meaningful and relevant to the LLM's
reasoning processes we perform the following experiments. (a) We ask GPT-4 to
assign skill labels to training questions in math datasets GSM8K and MATH. (b)
When using an LLM to solve the test questions, we present it with the full list
of skill labels and ask it to identify the skill needed. Then it is presented
with randomly selected exemplar solved questions associated with that skill
label. This improves accuracy on GSM8k and MATH for several strong LLMs,
including code-assisted models. The methodology presented is domain-agnostic,
even though this article applies it to math problems.

摘要：元认知知识是指人类对自身思考和推理过程的直觉知识。当今最好的 LLM 显然拥有一些推理过程。本文提供了证据表明它们也具有元认知知识，包括根据给定的任务来命名技能和应用程序的能力。我们主要在数学推理的背景下探索这一点，开发了一个提示引导的交互程序，让一个强大的 LLM 为数学问题分配明智的技能标签，然后对其执行语义聚类以获得更粗略的技能标签族。这些粗略的技能标签看起来对人类来说是可解释的。
为了验证这些技能标签对 LLM 的推理过程有意义且相关，我们进行了以下实验。(a) 我们要求 GPT-4 为数学数据集 GSM8K 和 MATH 中的训练问题分配技能标签。(b) 当使用 LLM 来解决测试问题时，我们向其提供完整的技能标签列表，并要求其识别所需的技能。然后向其展示与该技能标签相关联的随机选择的示例求解问题。这提高了 GSM8k 和 MATH 对几个强大的 LLM 的准确性，包括代码辅助模型。所提出的方法论与领域无关，尽管本文将其应用于数学问题。

##### **Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution**
2405.12202v1 by Xihaier Luo, Xiaoning Qian, Byung-Jun Yoon

In this work, we present an arbitrary-scale super-resolution (SR) method to
enhance the resolution of scientific data, which often involves complex
challenges such as continuity, multi-scale physics, and the intricacies of
high-frequency signals. Grounded in operator learning, the proposed method is
resolution-invariant. The core of our model is a hierarchical neural operator
that leverages a Galerkin-type self-attention mechanism, enabling efficient
learning of mappings between function spaces. Sinc filters are used to
facilitate the information transfer across different levels in the hierarchy,
thereby ensuring representation equivalence in the proposed neural operator.
Additionally, we introduce a learnable prior structure that is derived from the
spectral resizing of the input data. This loss prior is model-agnostic and is
designed to dynamically adjust the weighting of pixel contributions, thereby
balancing gradients effectively across the model. We conduct extensive
experiments on diverse datasets from different domains and demonstrate
consistent improvements compared to strong baselines, which consist of various
state-of-the-art SR methods.

摘要：在這項工作中，我們提出了一種任意尺度的超解析度 (SR) 方法，以增強科學數據的分辨率，這通常涉及連續性、多尺度物理以及高頻率訊號的複雜性等挑戰。該方法以算子學習為基礎，具有解析度不變性。我們模型的核心是一個分層神經算子，它利用了伽遼金類型自注意力機制，實現了函數空間之間對應關係的有效學習。Sinc 濾波器用於促進層次結構中不同層級之間的資訊傳遞，從而確保了所提出的神經算子中的表示等價性。此外，我們還引入了一個可學習的先驗結構，它來自輸入數據的頻譜調整。此損失先驗與模型無關，旨在動態調整像素貢獻的權重，從而有效地平衡整個模型中的梯度。我們對來自不同領域的不同數據集進行了廣泛的實驗，並展示了與強基線相比的一致改進，這些基線包括各種最先進的 SR 方法。

##### **Multi-order Graph Clustering with Adaptive Node-level Weight Learning**
2405.12183v1 by Ye Liu, Xuelei Lin, Yejia Chen, Reynold Cheng

Current graph clustering methods emphasize individual node and edge con
nections, while ignoring higher-order organization at the level of motif. Re
cently, higher-order graph clustering approaches have been designed by motif
based hypergraphs. However, these approaches often suffer from hypergraph
fragmentation issue seriously, which degrades the clustering performance
greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes
participating in multiple motifs. A key challenge is how to achieve precise
clustering results by integrating information from multiple motifs at the node
level. In this paper, we propose a multi-order graph clustering model (MOGC) to
integrate multiple higher-order structures and edge connections at node level.
MOGC employs an adaptive weight learning mechanism to au tomatically adjust the
contributions of different motifs for each node. This not only tackles
hypergraph fragmentation issue but enhances clustering accuracy. MOGC is
efficiently solved by an alternating minimization algo rithm. Experiments on
seven real-world datasets illustrate the effectiveness of MOGC.

摘要：目前圖形聚類方法強調個別節點和邊緣連接，而忽略了母題層級的高階組織。最近，高階圖形聚類方法已由基於母題的超圖設計。然而，這些方法通常會遭受超圖碎片化問題的嚴重影響，這會大幅降低聚類效能。此外，真實世界的圖形通常包含多種母題，且節點參與多個母題。一項關鍵挑戰是如何透過整合節點層級來自多個母題的資訊，來達成精確的聚類結果。在本文中，我們提出一個多階圖形聚類模型 (MOGC)，以整合節點層級的多個高階結構和邊緣連接。MOGC 採用自適應權重學習機制，自動調整每個節點不同母題的貢獻。這不僅解決了超圖碎片化問題，也增強了聚類準確度。MOGC 可透過交替最小化演算法有效率地解決。在七個真實世界資料集上的實驗說明了 MOGC 的有效性。

##### **Building Temporal Kernels with Orthogonal Polynomials**
2405.12179v1 by Yan Ru Pei, Olivier Coenen

We introduce a class of models named PLEIADES (PoLynomial Expansion In
Adaptive Distributed Event-based Systems), which contains temporal convolution
kernels generated from orthogonal polynomial basis functions. We focus on
interfacing these networks with event-based data to perform online
spatiotemporal classification and detection with low latency. By virtue of
using structured temporal kernels and event-based data, we have the freedom to
vary the sample rate of the data along with the discretization step-size of the
network without additional finetuning. We experimented with three event-based
benchmarks and obtained state-of-the-art results on all three by large margins
with significantly smaller memory and compute costs. We achieved: 1) 99.59%
accuracy with 192K parameters on the DVS128 hand gesture recognition dataset
and 100% with a small additional output filter; 2) 99.58% test accuracy with
277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with
576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.

摘要：<paragraph>我们引入了一类名为 PLEIADES（自适应分布式事件驱动系统中的多项式扩展）的模型，其中包含由正交多项式基函数生成的时序卷积核。我们专注于将这些网络与基于事件的数据进行接口，以执行具有低延迟的在线时空分类和检测。由于使用了结构化的时序核和基于事件的数据，我们可以在不进行额外微调的情况下自由改变数据的采样率以及网络的离散化步长。我们对三个基于事件的基准进行了实验，在所有三个基准上都获得了最先进的结果，并且显著降低了内存和计算成本。我们实现了：1）在 DVS128 手势识别数据集上，使用 192K 参数实现了 99.59% 的准确度，使用一个小的附加输出滤波器实现了 100% 的准确度；2）在 AIS 2024 眼动追踪挑战中，使用 277K 参数实现了 99.58% 的测试准确度；3）在 PROPHESEE 1 百万像素汽车检测数据集中，使用 576k 参数实现了 0.556 mAP。</paragraph>

##### **CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**
2405.12174v1 by Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai

Text-to-Table aims to generate structured tables to convey the key
information from unstructured documents. Existing text-to-table datasets are
typically oriented English, limiting the research in non-English languages.
Meanwhile, the emergence of large language models (LLMs) has shown great
success as general task solvers in multi-lingual settings (e.g., ChatGPT),
theoretically enabling text-to-table in other languages. In this paper, we
propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this
task. Our preliminary analysis of English text-to-table datasets highlights two
key factors for dataset construction: data diversity and data hallucination.
Inspired by this, the CT-Eval dataset selects a popular Chinese
multidisciplinary online encyclopedia as the source and covers 28 domains to
ensure data diversity. To minimize data hallucination, we first train an LLM to
judge and filter out the task samples with hallucination, then employ human
annotators to clean the hallucinations in the validation and testing sets.
After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we
evaluate the performance of open-source and closed-source LLMs. Our results
reveal that zero-shot LLMs (including GPT-4) still have a significant
performance gap compared with human judgment. Furthermore, after fine-tuning,
open-source LLMs can significantly improve their text-to-table ability,
outperforming GPT-4 by a large margin. In short, CT-Eval not only helps
researchers evaluate and quickly understand the Chinese text-to-table ability
of existing LLMs but also serves as a valuable resource to significantly
improve the text-to-table performance of LLMs.

摘要：文本到表格旨在生成结构化表格来传达非结构化文档中的关键信息。现有的文本到表格数据集通常面向英语，限制了非英语语言的研究。与此同时，大型语言模型（LLM）的出现已显示出作为多语言环境中的通用任务解决者的巨大成功（例如 ChatGPT），理论上可以在其他语言中实现文本到表格。在本文中，我们提出了一个中文文本到表格数据集 CT-Eval，以对 LLM 在此任务上进行基准测试。我们对英语文本到表格数据集的初步分析突出了数据集构建的两个关键因素：数据多样性和数据幻觉。受此启发，CT-Eval 数据集选择了一个流行的中文多学科在线百科全书作为来源，并涵盖 28 个领域以确保数据多样性。为了最大程度地减少数据幻觉，我们首先训练一个 LLM 来判断并过滤掉带有幻觉的任务样本，然后雇用人工注释员来清理验证和测试集中的幻觉。经过这个过程，CT-Eval 包含 88.6K 个任务样本。使用 CT-Eval，我们评估了开源和闭源 LLM 的性能。我们的结果表明，零样本 LLM（包括 GPT-4）与人类判断相比仍然存在显着的性能差距。此外，在微调之后，开源 LLM 可以显着提高其文本到表格的能力，大幅优于 GPT-4。简而言之，CT-Eval 不仅帮助研究人员评估和快速了解现有 LLM 的中文文本到表格能力，而且还作为一种宝贵的资源来显着提高 LLM 的文本到表格性能。

##### **Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**
2405.12163v1 by Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, Min Zhang

The rapid advancement of large language models has given rise to a plethora
of applications across a myriad of real-world tasks, mainly centered on
aligning with human intent. However, the complexities inherent in human intent
necessitate a dependence on labor-intensive and time-consuming human
evaluation. To alleviate this constraint, we delve into the paradigm of
employing open-source large language models as evaluators, aligning with the
prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step
evaluation framework: \textbf{Fennec}, capable of \textbf{F}ine-grained
\textbf{E}valuatio\textbf{N} and correctio\textbf{N} \textbf{E}xtended through
bran\textbf{C}hing and bridging. Specifically, the branching operation dissects
the evaluation task into various dimensions and granularities, thereby
alleviating the challenges associated with evaluation. Concurrently, the
bridging operation amalgamates diverse training datasets, augmenting the
variety of evaluation tasks. In experimental trials, our 7B model consistently
outperforms open-source larger-scale evaluation models across various widely
adopted benchmarks in terms of both \textit{Agreement} and
\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ
the fine-grained correction capabilities induced by the evaluation model to
refine multiple model responses, and the results show that the refinement
elevates the quality of responses, leading to an improvement of 1-2 points on
the MT-Bench. Our code is available at
Github\footnote{\url{https://github.com/dropreg/Fennec}}.

摘要：<paragraph>大型語言模型的快速進展已產生大量應用，橫跨無數真實世界的任務，主要集中在與人類意圖保持一致。然而，人類意圖中固有的複雜性需要依賴於勞動密集且耗時的評估。為了緩解這種限制，我們深入探討採用開放原始碼大型語言模型作為評估者的範例，與利用 GPT-4 的盛行趨勢保持一致。特別是，我們提出了一個逐步評估框架：**Fennec**，能夠進行**F**ine-grained**E**valuatio**N** and correctio**N** **E**xtended through bran**C**hing and bridging。具體來說，分支操作將評估任務剖析成各種維度和粒度，從而緩解與評估相關的挑戰。同時，橋接操作合併了不同的訓練資料集，增加了評估任務的多樣性。在實驗試驗中，我們的 7B 模型在各種廣泛採用的基準測試中，無論在**一致性**和**一致性**方面，都持續優於開放原始碼的大規模評估模型，接近 GPT-4 的能力。我們採用評估模型誘導的細粒度校正能力來改善多個模型回應，結果表明，這種改善提升了回應的品質，導致 MT-Bench 上提高了 1-2 分。我們的程式碼可以在 Github\footnote{\url{https://github.com/dropreg/Fennec}} 中取得。</paragraph>

##### **Bangladeshi Native Vehicle Detection in Wild**
2405.12150v1 by Bipin Saha, Md. Johirul Islam, Shaikh Khaled Mostaque, Aditya Bhowmik, Tapodhir Karmakar Taton, Md. Nakib Hayat Chowdhury, Mamun Bin Ibne Reaz

The success of autonomous navigation relies on robust and precise vehicle
recognition, hindered by the scarcity of region-specific vehicle detection
datasets, impeding the development of context-aware systems. To advance
terrestrial object detection research, this paper proposes a native vehicle
detection dataset for the most commonly appeared vehicle classes in Bangladesh.
17 distinct vehicle classes have been taken into account, with fully annotated
81542 instances of 17326 images. Each image width is set to at least 1280px.
The dataset's average vehicle bounding box-to-image ratio is 4.7036. This
Bangladesh Native Vehicle Dataset (BNVD) has accounted for several
geographical, illumination, variety of vehicle sizes, and orientations to be
more robust on surprised scenarios. In the context of examining the BNVD
dataset, this work provides a thorough assessment with four successive You Only
Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's
effectiveness is methodically evaluated and contrasted with other vehicle
datasets already in use. The BNVD dataset exhibits mean average precision(mAP)
at 50% intersection over union (IoU) is 0.848 corresponding precision and
recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643
at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset
serves as a reliable representation of vehicle distribution and presents
considerable complexities.

摘要：<paragraph>自主導航的成功有賴於強健且精確的車輛辨識，而這卻受到特定區域車輛偵測資料集的匱乏所阻礙，進而妨礙了情境感知系統的發展。為了推動地面物體偵測的研究，本篇論文提出了一個針對孟加拉國最常見車輛類別的原生車輛偵測資料集。我們考慮了 17 個不同的車輛類別，並針對 17326 張影像進行了完整的標註，共標註了 81542 個實例。每張影像的寬度都設定為至少 1280 像素。該資料集的平均車輛邊界框與影像比例為 4.7036。這個孟加拉國原生車輛資料集 (BNVD) 考慮了多種地理、光照、車輛大小和方向，以便在意外的情況下更為強健。在檢查 BNVD 資料集的脈絡下，本研究提供了對四個連續的 You Only Look Once (YOLO) 模型的徹底評估，分別是 YOLO v5、v6、v7 和 v8。我們有系統地評估了這些資料集的有效性，並與其他現有車輛資料集進行對比。BNVD 資料集在 50% 交集並集 (IoU) 上展現出 0.848 的平均準確度 (mAP)，對應的準確率和召回率分別為 0.841 和 0.774。研究結果顯示，在 0.5 到 0.95 的 IoU 範圍內，mAP 為 0.643。實驗顯示，BNVD 資料集可作為車輛分佈的可靠表示，並呈現出相當的複雜性。</paragraph>

##### **Eliciting Problem Specifications via Large Language Models**
2405.12147v1 by Robert E. Wray, James R. Kirk, John E. Laird

Cognitive systems generally require a human to translate a problem definition
into some specification that the cognitive system can use to attempt to solve
the problem or perform the task. In this paper, we illustrate that large
language models (LLMs) can be utilized to map a problem class, defined in
natural language, into a semi-formal specification that can then be utilized by
an existing reasoning and learning system to solve instances from the problem
class. We present the design of LLM-enabled cognitive task analyst agent(s).
Implemented with LLM agents, this system produces a definition of problem
spaces for tasks specified in natural language. LLM prompts are derived from
the definition of problem spaces in the AI literature and general
problem-solving strategies (Polya's How to Solve It). A cognitive system can
then use the problem-space specification, applying domain-general problem
solving strategies ("weak methods" such as search), to solve multiple instances
of problems from the problem class. This result, while preliminary, suggests
the potential for speeding cognitive systems research via disintermediation of
problem formulation while also retaining core capabilities of cognitive
systems, such as robust inference and online learning.

摘要：認知系統通常需要人類將問題定義轉換為認知系統可利用來嘗試解決問題或執行任務的某些規格。在本文中，我們說明大型語言模型 (LLM) 可用於將以自然語言定義的問題類別對應到半正式規格，然後現有的推理和學習系統可利用該規格來解決問題類別中的實例。我們提出 LLM 啟用的認知任務分析代理的設計。此系統利用 LLM 代理實作，可產生以自然語言指定的任務問題空間定義。LLM 提示來自 AI 文獻中問題空間的定義和一般問題解決策略（波利亞的如何解題）。認知系統接著可以使用問題空間規格，套用領域通用的問題解決策略（「弱方法」，例如搜尋），來解決問題類別中多個問題實例。此結果雖然初步，但顯示出透過問題表述的去中介化來加速認知系統研究的潛力，同時也保留認知系統的核心功能，例如強健的推論和線上學習。

##### **MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**
2405.12130v1 by Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang

Low-rank adaptation is a popular parameter-efficient fine-tuning method for
large language models. In this paper, we analyze the impact of low-rank
updating, as implemented in LoRA. Our findings suggest that the low-rank
updating mechanism may limit the ability of LLMs to effectively learn and
memorize new knowledge. Inspired by this observation, we propose a new method
called MoRA, which employs a square matrix to achieve high-rank updating while
maintaining the same number of trainable parameters. To achieve it, we
introduce the corresponding non-parameter operators to reduce the input
dimension and increase the output dimension for the square matrix. Furthermore,
these operators ensure that the weight can be merged back into LLMs, which
makes our method can be deployed like LoRA. We perform a comprehensive
evaluation of our method across five tasks: instruction tuning, mathematical
reasoning, continual pretraining, memory and pretraining. Our method
outperforms LoRA on memory-intensive tasks and achieves comparable performance
on other tasks.

摘要：低秩適應是一種廣受歡迎且參數效率高的微調方法，適用於大型語言模型。在本文中，我們分析了低秩更新的影響，就像在 LoRA 中實作的那樣。我們的研究結果表明，低秩更新機制可能會限制大型語言模型有效學習和記憶新知識的能力。受到這一觀察結果的啟發，我們提出了一種名為 MoRA 的新方法，它採用一個方陣來實現高秩更新，同時保持相同數量的可訓練參數。為此，我們引入了相應的非參數運算子，以降低輸入維度並增加方陣的輸出維度。此外，這些運算子確保權重可以合併回大型語言模型，這使得我們的模型可以像 LoRA 一樣部署。我們對我們的模型進行了全面的評估，涵蓋了五項任務：指令微調、數學推理、持續預訓練、記憶和預訓練。我們的模型在記憶密集型任務上優於 LoRA，並在其他任務上實現了可比的效能。

##### **Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**
2405.12119v1 by Zhankui He, Zhouhang Xie, Harald Steck, Dawen Liang, Rahul Jha, Nathan Kallus, Julian McAuley

Large language models (LLMs) are revolutionizing conversational recommender
systems by adeptly indexing item content, understanding complex conversational
contexts, and generating relevant item titles. However, controlling the
distribution of recommended items remains a challenge. This leads to suboptimal
performance due to the failure to capture rapidly changing data distributions,
such as item popularity, on targeted conversational recommendation platforms.
In conversational recommendation, LLMs recommend items by generating the titles
(as multiple tokens) autoregressively, making it difficult to obtain and
control the recommendations over all items. Thus, we propose a
Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into
single tokens within LLMs, and then adjusts the probability distributions over
these single-token item titles accordingly. The RTA framework marries the
benefits of both LLMs and traditional recommender systems (RecSys):
understanding complex queries as LLMs do; while efficiently controlling the
recommended item distributions in conversational recommendations as traditional
RecSys do. Our framework demonstrates improved accuracy metrics across three
different conversational recommendation datasets and two adaptation settings

摘要：大型語言模型 (LLM) 透過靈活地索引項目內容、理解複雜的對話脈絡，以及產生相關的項目標題，徹底革新對話推薦系統。然而，控制推薦項目的分佈仍然是一項挑戰。這會導致次佳效能，因為無法擷取快速變化的資料分佈，例如目標對話推薦平台上的項目熱門程度。在對話推薦中，LLM 透過自迴歸產生標題（作為多個符號），這使得難以取得和控制所有項目的推薦。因此，我們提出一個重新索引再調整 (RTA) 架構，將多符號項目標題轉換為 LLM 中的單一符號，然後相應地調整這些單一符號項目標題的機率分佈。RTA 架構結合了 LLM 和傳統推薦系統 (RecSys) 的優點：像 LLM 一樣理解複雜的查詢；同時像傳統 RecSys 一樣有效控制對話推薦中的推薦項目分佈。我們的架構在三個不同的對話推薦資料集和兩個調整設定中展示了改善的準確度指標

##### **Linguistic Structure from a Bottleneck on Sequential Information Processing**
2405.12109v1 by Richard Futrell, Michael Hahn

Human language is a unique form of communication in the natural world,
distinguished by its structured nature. Most fundamentally, it is systematic,
meaning that signals can be broken down into component parts that are
individually meaningful -- roughly, words -- which are combined in a regular
way to form sentences. Furthermore, the way in which these parts are combined
maintains a kind of locality: words are usually concatenated together, and they
form contiguous phrases, keeping related parts of sentences close to each
other. We address the challenge of understanding how these basic properties of
language arise from broader principles of efficient communication under
information processing constraints. Here we show that natural-language-like
systematicity arises from minimization of excess entropy, a measure of
statistical complexity that represents the minimum amount of information
necessary for predicting the future of a sequence based on its past. In
simulations, we show that codes that minimize excess entropy factorize their
source distributions into approximately independent components, and then
express those components systematically and locally. Next, in a series of
massively cross-linguistic corpus studies, we show that human languages are
structured to have low excess entropy at the level of phonology, morphology,
syntax, and semantics. Our result suggests that human language performs a
sequential generalization of Independent Components Analysis on the statistical
distribution over meanings that need to be expressed. It establishes a link
between the statistical and algebraic structure of human language, and
reinforces the idea that the structure of human language may have evolved to
minimize cognitive load while maximizing communicative expressiveness.

摘要：人類語言是自然界中一種獨特的溝通形式，
以其結構化的性質為特色。最根本的是，它是系統性的，
這表示符號可以分解成個別有意義的組成部分——大致上，
就是單字——這些單字以規則的方式組合成句子。此外，這些部分組合的方式
維持著一種局部性：單字通常會串聯在一起，並形成連續的片語，
讓句子的相關部分緊密相連。我們探討了如何理解這些語言基本屬性
在資訊處理限制下，如何從有效溝通的更廣泛原則中產生。在這裡，我們展示了
類自然語言的系統性來自於過剩熵的最小化，一種統計複雜性的測量，
它表示根據序列的過去預測其未來的必要資訊最小量。在
模擬中，我們展示了最小化過剩熵的碼將其來源分佈分解成
近似獨立的組成部分，然後以系統且局部的形式表達這些組成部分。接下來，
在一系列的大規模跨語言語料庫研究中，我們展示了人類語言的結構
在音韻、形態、句法和語義的層級上具有低過剩熵。我們的結果表明，
人類語言對需要表達的意義的統計分佈執行獨立成分分析的順序概化。它建立了
人類語言的統計和代數結構之間的連結，並強化了人類語言的結構可能演化為
最小化認知負擔，同時最大化溝通表達力的想法。

##### **Imp: Highly Capable Large Multimodal Models for Mobile Devices**
2405.12107v1 by Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, Jiajun Ding

By harnessing the capabilities of large language models (LLMs), recent large
multimodal models (LMMs) have shown remarkable versatility in open-world
multimodal understanding. Nevertheless, they are usually parameter-heavy and
computation-intensive, thus hindering their applicability in
resource-constrained scenarios. To this end, several lightweight LMMs have been
proposed successively to maximize the capabilities under constrained scale
(e.g., 3B). Despite the encouraging results achieved by these methods, most of
them only focus on one or two aspects of the design space, and the key design
choices that influence model capability have not yet been thoroughly
investigated. In this paper, we conduct a systematic study for lightweight LMMs
from the aspects of model architecture, training strategy, and training data.
Based on our findings, we obtain Imp -- a family of highly capable LMMs at the
2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing
lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs
at the 13B scale. With low-bit quantization and resolution reduction
techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile
chip with a high inference speed of about 13 tokens/s.

摘要：<paragraph>透過利用大型語言模型 (LLM) 的能力，最近的大型多模態模型 (LMM) 在開放世界多模態理解中展現了驚人的多功能性。儘管如此，它們通常參數繁多且計算密集，因此阻礙了它們在資源受限場景中的應用。為了解決這個問題，已經陸續提出了幾種輕量級 LMM，以最大化受限規模（例如 3B）下的能力。儘管這些方法取得了令人鼓舞的成果，但它們大多只關注設計空間的一個或兩個方面，而影響模型能力的關鍵設計選擇尚未得到徹底研究。在本文中，我們從模型架構、訓練策略和訓練數據等方面對輕量級 LMM 進行了系統性研究。根據我們的研究結果，我們獲得了 Imp——一系列在 2B-4B 規模上具有高度能力的 LMM。值得注意的是，我們的 Imp-3B 模型穩定地優於所有現有類似規模的輕量級 LMM，甚至超越了 13B 規模的最新 LMM。透過低位元量化和解析度降低技術，我們的 Imp 模型可以部署在 Qualcomm Snapdragon 8Gen3 行動晶片上，具有約 13 個符號/秒的高推論速度。</paragraph>

##### **DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**
2405.12100v1 by Hao Chen, Biaojie Zeng, Xin Lin, Liang He, Aimin Zhou

Math world problems correction(MWPC) is a novel task dedicated to rectifying
reasoning errors in the process of solving mathematical problems. In this
paper, leveraging the advancements in large language models (LLMs), we address
two key objectives:(1) Distinguishing between mathematical reasoning and error
correction; (2) Exploring strategies to enhance the error correction
capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in
real-time education,assisting students in recognizing their mistakes is more
crucial than simply providing correct answers. However, current research tends
to prioritize obtaining accurate solutions to math problems rather than
correcting potentially incorrect ones. Therefore, we modify the research
paradigm, demonstrating that improving mathematical reasoning abilities does
not equate to mastery in error correction. Meanwhile, we propose a novel method
called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in
error correction. In experiments, DOP has shown outstanding performance,
highlighting its significant impact. We argue that in mathematical education,
the demand for outstanding correctors surpasses that for proficient reasoners.
Codes and data are available on
https://github.com/ChenhaoEcnuCS/Reason-Correct.

摘要：數學世界問題修正 (MWPC) 是一項新穎的任務，專門用於糾正解決數學問題過程中的推理錯誤。在本文中，利用大型語言模型 (LLM) 的進步，我們解決了兩個關鍵目標：(1) 區分數學推理和錯誤修正；(2) 探索增強 LLM 在數學中錯誤修正能力以解決 MWPC 任務的策略。我們注意到，在實時教育中，協助學生認識他們的錯誤比僅提供正確答案更為關鍵。然而，目前的研究所傾向於優先取得數學問題的準確解，而不是糾正潛在的錯誤解。因此，我們修改了研究範例，證明改進數學推理能力並不等於精通錯誤修正。同時，我們提出了一種稱為診斷導向提示 (DOP) 的新方法，旨在促進 LLM 在錯誤修正方面表現出色。在實驗中，DOP 已展現出傑出的效能，突顯其顯著影響。我們認為在數學教育中，對傑出修正者的需求超越了對熟練推理者的需求。程式碼和資料可在 https://github.com/ChenhaoEcnuCS/Reason-Correct 取得。

##### **Distributional Semantics, Holism, and the Instability of Meaning**
2405.12084v1 by Jumbly Grindrod, J. D. Porter, Nat Hansen

Current language models are built on the so-called distributional semantic
approach to linguistic meaning that has the distributional hypothesis at its
core. The distributional hypothesis involves a holistic conception of word
meaning: the meaning of a word depends upon its relations to other words in the
model. A standard objection to meaning holism is the charge of instability: any
change in the meaning properties of a linguistic system (a human speaker, for
example) would lead to many changes or possibly a complete change in the entire
system. When the systems in question are trying to communicate with each other,
it has been argued that instability of this kind makes communication impossible
(Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether the
instability objection poses a problem for distributional models of meaning.
First, we distinguish between distinct forms of instability that these models
could exhibit, and we argue that only one such form is relevant for
understanding the relation between instability and communication: what we call
differential instability. Differential instability is variation in the relative
distances between points in a space, rather than variation in the absolute
position of those points. We distinguish differential and absolute instability
by constructing two of our own models, a toy model constructed from the text of
two novels, and a more sophisticated model constructed using the Word2vec
algorithm from a combination of Wikipedia and SEP articles. We demonstrate the
two forms of instability by showing how these models change as the corpora they
are constructed from increase in size.

摘要：當前語言模型建立在所謂的分配語義方法上，以語言意義為核心，分配假設為其核心。分配假設涉及單詞意義的整體概念：單詞的意義取決於它與模型中其他單詞的關係。對意義整體論的標準反對意見是不穩定性的指控：語言系統（例如人類說話者）的意義屬性中的任何變化都會導致許多變化，或者可能導致整個系統的完全變化。當有問題的系統嘗試相互通信時，有人認為這種不穩定性使得通信變得不可能（Fodor and Lepore 1992, 1996, 1999）。在本文中，我們探討了不穩定性反對意見是否對意義的分配模型構成問題。首先，我們區分這些模型可能表現出的不同形式的不穩定性，我們認為只有一種形式與理解不穩定性和通信之間的關係相關：我們稱之為差異不穩定性。差異不穩定性是空間中各點之間相對距離的變化，而不是這些點絕對位置的變化。我們通過構建我們自己的兩個模型來區分差異和絕對不穩定性，一個玩具模型由兩部小說的文本構建，一個更精緻的模型由 Word2vec 演算法使用維基百科和 SEP 文章的組合構建。我們通過展示這些模型如何隨著構建它們的語料庫大小的增加而改變，來證明這兩種形式的不穩定性。

##### **Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model**
2405.12081v1 by Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan

To obtain high-quality annotations under limited budget, semi-automatic
annotation methods are commonly used, where a portion of the data is annotated
by experts and a model is then trained to complete the annotations for the
remaining data. However, these methods mainly focus on selecting informative
data for expert annotations to improve the model predictive ability (i.e.,
triage-to-human data), while the rest of the data is indiscriminately assigned
to model annotation (i.e., triage-to-model data). This may lead to
inefficiencies in budget allocation for annotations, as easy data that the
model could accurately annotate may be unnecessarily assigned to the expert,
and hard data may be misclassified by the model. As a result, the overall
annotation quality may be compromised. To address this issue, we propose a
selective annotation framework called SANT. It effectively takes advantage of
both the triage-to-human and triage-to-model data through the proposed
error-aware triage and bi-weighting mechanisms. As such, informative or hard
data is assigned to the expert for annotation, while easy data is handled by
the model. Experimental results show that SANT consistently outperforms other
baselines, leading to higher-quality annotation through its proper allocation
of data to both expert and model workers. We provide pioneering work on data
annotation within budget constraints, establishing a landmark for future
triage-based annotation studies.

摘要：為了在有限的預算下取得高品質的註解，通常會使用半自動註解方法，其中一部分資料由專家註解，然後訓練模型來完成其餘資料的註解。然而，這些方法主要著重於選擇具資訊性的資料進行專家註解，以提升模型的預測能力（亦即，分流到人類的資料），而其餘資料則不加區別地分配給模型註解（亦即，分流到模型的資料）。這可能會導致註解預算分配的低效率，因為模型可以準確註解的容易資料可能會不必要地分配給專家，而困難的資料可能會被模型錯誤分類。因此，整體註解品質可能會受到影響。為了解決這個問題，我們提出了一個稱為 SANT 的選擇性註解架構。它有效地利用了分流到人類和分流到模型的資料，透過提出的錯誤感知分流和雙重加權機制。因此，具資訊性或困難的資料會分配給專家進行註解，而容易的資料則由模型處理。實驗結果顯示，SANT 持續優於其他基準，透過適當地將資料分配給專家和模型工作人員，產生更高品質的註解。我們提供了在預算限制下進行資料註解的開創性工作，為未來的基於分流的註解研究建立了一個里程碑。

##### **AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements**
2405.12070v1 by Calvin Yeung, Kenjiro Ide, Keisuke Fujii

Image understanding is a foundational task in computer vision, with recent
applications emerging in soccer posture analysis. However, existing publicly
available datasets lack comprehensive information, notably in the form of
posture sequences and 2D pose annotations. Moreover, current analysis models
often rely on interpretable linear models (e.g., PCA and regression), limiting
their capacity to capture non-linear spatiotemporal relationships in complex
and diverse scenarios. To address these gaps, we introduce the 3D Shot Posture
(3DSP) dataset in soccer broadcast videos, which represents the most extensive
sports image dataset with 2D pose annotations to our knowledge. Additionally,
we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear
approach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,
a pipeline aimed at semi-automating 2D and 3D pose estimation and posture
analysis. While achieving full automation proved challenging, we provide a
foundational baseline, extending its utility beyond the scope of annotated
data. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present
posture analysis results based on 3DSP. The dataset, code, and models are
available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.

摘要：影像理解是電腦視覺中的基礎任務，最近在足球姿勢分析中出現了新的應用。然而，現有的公開可用資料集缺乏全面的資訊，特別是姿勢序列和 2D 姿勢註解的形式。此外，目前的分析模型通常依賴於可解釋的線性模型（例如，PCA 和回歸），這限制了它們在複雜且多樣化的場景中捕捉非線性時空關係的能力。為了解決這些差距，我們在足球廣播影片中引入了 3D 射門姿勢 (3DSP) 資料集，它代表了我們所知最廣泛的具有 2D 姿勢註解的運動影像資料集。此外，我們提出了 3DSP-GRAE（圖形遞迴自動編碼器）模型，這是一種用於嵌入姿勢序列的非線性方法。此外，我們提出了 AutoSoccerPose，這是一個旨在半自動化 2D 和 3D 姿勢估計和姿勢分析的管道。儘管實現完全自動化被證明具有挑戰性，但我們提供了一個基礎基準，將其效用擴展到註解資料的範圍之外。我們在 SoccerNet 和 3DSP 資料集上驗證了 AutoSoccerPose，並根據 3DSP 呈現姿勢分析結果。資料集、程式碼和模型可在以下位置取得：https://github.com/calvinyeungck/3D-Shot-Posture-Dataset。

##### **CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models**
2405.12063v1 by Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua

Large language models (LLMs) are increasingly used to meet user information
needs, but their effectiveness in dealing with user queries that contain
various types of ambiguity remains unknown, ultimately risking user trust and
satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating
LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct
~12K high-quality data to assess the strengths, weaknesses, and potential risks
of various off-the-shelf LLMs. Our findings indicate the limited practical
utility of current LLMs in identifying and clarifying ambiguous user queries,
even enhanced by chain-of-thought (CoT) and few-shot prompting. These
techniques may result in overconfidence in LLMs and yield only marginal
enhancements in identifying ambiguity. Furthermore, current LLMs fall short in
generating high-quality clarifying questions due to a lack of conflict
resolution and inaccurate utilization of inherent knowledge. In this paper,
CLAMBER presents a guidance and promotes further research on proactive and
trustworthy LLMs. Our dataset is available at
https://github.com/zt991211/CLAMBER

摘要：大型語言模型 (LLM) 愈來愈常被用於滿足使用者的資訊需求，但它們在處理包含各種歧義的使用者查詢時的有效性仍不得而知，最終會影響使用者的信任和滿意度。為此，我們引入了 CLAMBER，一個用於使用井然有序的分類法評估 LLM 的基準。在分類法的基礎上，我們建構了約 12K 個高品質的資料，以評估各種現成的 LLM 的優點、缺點和潛在風險。我們的研究結果顯示，現有 LLM 在識別和釐清有歧義的使用者查詢方面的實際效用有限，即使透過思考鏈 (CoT) 和少量提示加以增強。這些技術可能會導致對 LLM 過度自信，而且在識別歧義方面僅能帶來邊際改善。此外，現有 LLM 在產生高品質的澄清問題方面也有所不足，原因是缺乏衝突解決和不準確利用內在知識。在本文中，CLAMBER 提供了一個指導方針，並促進對主動且值得信賴的 LLM 的進一步研究。我們的資料集可在 https://github.com/zt991211/CLAMBER 取得

##### **STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents**
2405.12059v1 by Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, Tat-Seng Chua

Equipping a conversational search engine with strategies regarding when to
ask clarification questions is becoming increasingly important across various
domains. Attributing to the context understanding capability of LLMs and their
access to domain-specific sources of knowledge, LLM-based clarification
strategies feature rapid transfer to various domains in a post-hoc manner.
However, they still struggle to deliver promising performance on unseen
domains, struggling to achieve effective domain transferability. We take the
first step to investigate this issue and existing methods tend to produce
one-size-fits-all strategies across diverse domains, limiting their search
effectiveness. In response, we introduce a novel method, called Style, to
achieve effective domain transferability. Our experimental results indicate
that Style bears strong domain transferability, resulting in an average search
performance improvement of ~10% on four unseen domains.

摘要：隨著對話式搜尋引擎在各種領域中日益重要，因此需要為其配備有關何時提出澄清問題的策略。基於 LLM 對脈絡理解的能力及其存取特定領域知識來源的能力，基於 LLM 的澄清策略在事後方式中具備快速轉移至各種領域的功能。然而，它們仍然難以在未見過的領域中提供有前途的效能，難以達成有效的領域可轉移性。我們採取第一步來探討這個問題，現有方法傾向於在各種領域中產生一體適用的策略，限制其搜尋效果。為了解決這個問題，我們引進一種稱為 Style 的新方法，以達成有效的領域可轉移性。我們的實驗結果顯示，Style 具有強大的領域可轉移性，導致在四個未見過的領域中平均搜尋效能提升約 10%。

##### **Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics**
2405.12055v1 by Olga Kellert, Carlos Gómez-Rodríguez, Mahmud Uz Zaman

TripAdvisor reviews and comparable data sources play an important role in
many tasks in Natural Language Processing (NLP), providing a data basis for the
identification and classification of subjective judgments, such as hotel or
restaurant reviews, into positive or negative polarities. This study explores
three important factors influencing variation in crowdsourced polarity
judgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are
tested: the role of Part Of Speech (POS), the impact of sentiment words such as
"tasty", and the influence of neutral words like "ok" on judgment variation.
The study's methodology employs one-word titles, demonstrating their efficacy
in studying polarity variation of words. Statistical tests on mean equality are
performed on word groups of our interest. The results of this study reveal that
adjectives in one-word titles tend to result in lower judgment variation
compared to other word types or POS. Sentiment words contribute to lower
judgment variation as well, emphasizing the significance of sentiment words in
research on polarity judgments, and neutral words are associated with higher
judgment variation as expected. However, these effects cannot be always
reproduced in longer titles, which suggests that longer titles do not represent
the best data source for testing the ambiguity of single words due to the
influence on word polarity by other words like negation in longer titles. This
empirical investigation contributes valuable insights into the factors
influencing polarity variation of words, providing a foundation for NLP
practitioners that aim to capture and predict polarity judgments in Spanish and
for researchers that aim to understand factors influencing judgment variation.

摘要：<paragraph>TripAdvisor 評論和類似資料來源在自然語言處理 (NLP) 的許多任務中扮演重要角色，提供資料基礎，用於識別和分類主觀判斷，例如飯店或餐廳評論，為正向或負向極性。此研究探討影響群眾外包極性判斷變化的三個重要因素，重點在於西班牙文的 TripAdvisor 評論。測試了三個假設：詞性 (POS) 的角色、美味等情感詞的影響，以及像「ok」等中性詞對判斷變化的影響。此研究的方法採用單字標題，證明其在研究詞彙極性變化的效能。對我們感興趣的詞群執行平均相等性統計檢定。此研究的結果顯示，單字標題中的形容詞傾向於造成較低的判斷變化，相較於其他詞類或詞性。情感詞也有助於降低判斷變化，強調情感詞在極性判斷研究中的重要性，而中性詞則預期會與較高的判斷變化相關。然而，這些效應無法總是在較長的標題中重現，這表示較長的標題並非測試單字歧義的最佳資料來源，因為在較長的標題中，否定等其他詞彙會影響詞彙極性。這項實證研究提供了有價值的見解，了解影響詞彙極性變化的因素，為 NLP 從業人員奠定基礎，目標在於擷取和預測西班牙文的極性判斷，以及為研究人員奠定基礎，目標在於了解影響判斷變化的因素。</paragraph>

##### **KG-RAG: Bridging the Gap Between Knowledge and Creativity**
2405.12035v1 by Diego Sanmartin

Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.

摘要：在確保事實準確性的同時，保持大型語言模型代理（LMA）的創建能力，對智慧型代理系統的開發構成了重大的挑戰。LMA 面臨普遍的問題，例如資訊幻覺、災難性遺忘，以及在處理知識密集型任務時處理長脈絡的限制。本文介紹了 KG-RAG（知識圖譜檢索增強生成）管道，這是一個新穎的框架，旨在透過將結構化的知識圖譜（KG）與 LLM 的功能整合在一起，來增強 LMA 的知識能力，從而顯著減少對 LLM 潛在知識的依賴。KG-RAG 管道從非結構化文字中建構一個 KG，然後對新建立的圖譜執行資訊檢索，以執行 KGQA（知識圖譜問答）。檢索方法利用了一種稱為探索鏈（CoE）的新演算法，該演算法受益於 LLM 推理，以循序探索 KG 中的節點和關係。在 ComplexWebQuestions 資料集上的初步實驗證明了在減少幻覺內容方面有顯著的改進，並暗示了一條有希望的道路，朝著開發擅長處理知識密集型任務的智慧型系統邁進。

##### **Can AI Relate: Testing Large Language Model Response for Mental Health Support**
2405.12021v1 by Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, Marzyeh Ghassemi

Large language models (LLMs) are already being piloted for clinical use in
hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed
deployment use case is psychotherapy, where a LLM-powered chatbot can treat a
patient undergoing a mental health crisis. Deployment of LLMs for mental health
response could hypothetically broaden access to psychotherapy and provide new
possibilities for personalizing care. However, recent high-profile failures,
like damaging dieting advice offered by the Tessa chatbot to patients with
eating disorders, have led to doubt about their reliability in high-stakes and
safety-critical settings.
  In this work, we develop an evaluation framework for determining whether LLM
response is a viable and ethical path forward for the automation of mental
health treatment. Using human evaluation with trained clinicians and automatic
quality-of-care metrics grounded in psychology research, we compare the
responses provided by peer-to-peer responders to those provided by a
state-of-the-art LLM.
  We show that LLMs like GPT-4 use implicit and explicit cues to infer patient
demographics like race. We then show that there are statistically significant
discrepancies between patient subgroups: Responses to Black posters
consistently have lower empathy than for any other demographic group (2%-13%
lower than the control group). Promisingly, we do find that the manner in which
responses are generated significantly impacts the quality of the response. We
conclude by proposing safety guidelines for the potential deployment of LLMs
for mental health response.

摘要：大型語言模型 (LLM) 已在紐約大學朗格尼、達納法伯和英國國家醫療服務體系等醫院系統中試行用於臨床用途。建議的部署用例是心理治療，其中由 LLM 驅動的聊天機器人可以治療正在經歷心理健康危機的患者。假設 LLM 部署於心理健康應對，可以擴大心理治療的普及度，並提供新的可能性來實現個性化護理。然而，最近備受矚目的失敗，例如 Tessa 聊天機器人為飲食失調患者提供的有害飲食建議，讓人們對其在高風險和安全關鍵環境中的可靠性產生了懷疑。
  在這項工作中，我們開發了一個評估框架，用於確定 LLM 回應是否是一個可行且合乎道德的途徑，用於自動化心理健康治療。我們利用經過培訓的臨床醫生進行的人類評估和基於心理學研究的自動質量護理指標，比較了點對點回應者提供的回應與最先進的 LLM 提供的回應。
  我們展示了 GPT-4 等 LLM 使用隱式和顯式線索來推斷患者人口統計資料，例如種族。然後我們展示了患者亞組之間存在統計上顯著的差異：對黑人海報的回應始終低於任何其他人口統計組（比對照組低 2%-13%）。可喜的是，我們確實發現回應的生成方式會顯著影響回應的品質。最後，我們提出 LLM 在心理健康應對中潛在部署的安全指南。

##### **Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning**
2405.12001v1 by Hai Zhang, Boyuan Zheng, Anqi Guo, Tianying Ji, Pheng-Ann Heng, Junqiao Zhao, Lanqing Li

Offline meta reinforcement learning (OMRL) has emerged as a promising
approach for interaction avoidance and strong generalization performance by
leveraging pre-collected data and meta-learning techniques. Previous
context-based approaches predominantly rely on the intuition that maximizing
the mutual information between the task and the task representation ($I(Z;M)$)
can lead to performance improvements. Despite achieving attractive results, the
theoretical justification of performance improvement for such intuition has
been lacking. Motivated by the return discrepancy scheme in the model-based RL
field, we find that maximizing $I(Z;M)$ can be interpreted as consistently
raising the lower bound of the expected return for a given policy conditioning
on the optimal task representation. However, this optimization process ignores
the task representation shift between two consecutive updates, which may lead
to performance improvement collapse. To address this problem, we turn to use
the framework of performance difference bound to consider the impacts of task
representation shift explicitly. We demonstrate that by reining the task
representation shift, it is possible to achieve monotonic performance
improvements, thereby showcasing the advantage against previous approaches. To
make it practical, we design an easy yet highly effective algorithm RETRO
(\underline{RE}ining \underline{T}ask \underline{R}epresentation shift in
context-based \underline{O}ffline meta reinforcement learning) with only adding
one line of code compared to the backbone. Empirical results validate its
state-of-the-art (SOTA) asymptotic performance, training stability and
training-time consumption on MuJoCo and MetaWorld benchmarks.

摘要：離線元增強學習 (OMRL) 已成為一種有前途的方法，它透過利用預先收集的資料和元學習技術，來避免互動並提升強大的泛化效能。先前的基於脈絡的方法主要依賴於一個直覺，即最大化任務與任務表示之間的互信息 ($I(Z;M)$) 可以提升效能。儘管獲得了令人滿意的結果，但對於這種直覺的效能提升，卻缺乏理論依據。在基於模型的 RL 領域的回報差異方案的啟發下，我們發現最大化 $I(Z;M)$ 可以解釋為一致地提高給定策略在最佳任務表示上的條件預期回報的下限。然而，這個最佳化程序忽略了兩個連續更新之間的任務表示轉移，這可能會導致效能提升崩潰。為了解決這個問題，我們轉而使用效能差異界限的架構，來明確考慮任務表示轉移的影響。我們證明，透過約束任務表示轉移，可以實現單調的效能提升，從而展示出相較於先前方法的優勢。為了使其更實用，我們設計了一個簡單但高效率的演算法 RETRO（在基於脈絡的離線元增強學習中約束任務表示轉移），它與主幹相比只增加了程式碼的一行。經驗結果驗證了其在 MuJoCo 和 MetaWorld 基準上的最先進 (SOTA) 的漸近效能、訓練穩定性和訓練時間消耗。

##### **A review on the use of large language models as virtual tutors**
2405.11983v1 by Silvia García-Méndez, Francisco de Arriba-Pérez, María del Carmen Somoza-López

Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.

摘要：Transformer 架構有助於管理自然語言處理的長期依賴性，代表該領域最新變革之一。這些架構是創新、尖端的巨量語言模型 (LLM) 的基礎，這些模型在多個領域和產業部門引起極大迴響，其中教育領域尤為突出。因此，這些基於生成式人工智慧的解決方案引導了技術變革，以及教育方法和內容的演進，以及網路基礎設施，朝向高品質學習。鑑於 LLM 的普及性，本篇評論旨在提供對這些解決方案的全面概述，這些解決方案專門設計用於產生和評估教育材料，並讓學生和教師參與其設計或實驗計畫。據我們所知，這是第一篇 LLM 教育應用（例如學生評量）的評論。不出所料，這些系統最常見的角色是作為自動產生問題的虛擬導師。此外，最受歡迎的模型是 GTP-3 和 BERT。然而，由於持續推出新的生成式模型，預計不久後將會發表新的作品。

##### **Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space**
2405.11982v1 by Qianmei Liu, Yufei Kuang, Jie Wang

Deep reinforcement learning (DRL) algorithms can suffer from modeling errors
between the simulation and the real world. Many studies use adversarial
learning to generate perturbation during training process to model the
discrepancy and improve the robustness of DRL. However, most of these
approaches use a fixed parameter to control the intensity of the adversarial
perturbation, which can lead to a trade-off between average performance and
robustness. In fact, finding the optimal parameter of the perturbation is
challenging, as excessive perturbations may destabilize training and compromise
agent performance, while insufficient perturbations may not impart enough
information to enhance robustness. To keep the training stable while improving
robustness, we propose a simple but effective method, namely, Adaptive
Adversarial Perturbation (A2P), which can dynamically select appropriate
adversarial perturbations for each sample. Specifically, we propose an adaptive
adversarial coefficient framework to adjust the effect of the adversarial
perturbation during training. By designing a metric for the current intensity
of the perturbation, our method can calculate the suitable perturbation levels
based on the current relative performance. The appealing feature of our method
is that it is simple to deploy in real-world applications and does not require
accessing the simulator in advance. The experiments in MuJoCo show that our
method can improve the training stability and learn a robust policy when
migrated to different test environments. The code is available at
https://github.com/Lqm00/A2P-SAC.

摘要：深度強化學習 (DRL) 演算法可能會受到模擬與真實世界之間的建模誤差影響。許多研究使用對抗學習在訓練過程中產生擾動，以模擬差異並改善 DRL 的穩健性。然而，這些方法大多使用固定參數來控制對抗擾動的強度，這可能導致平均效能和穩健性之間的取捨。事實上，找到擾動的最佳參數具有挑戰性，因為過度的擾動可能會破壞訓練並損害代理效能，而不足的擾動可能無法提供足夠的資訊來增強穩健性。為了在改善穩健性的同時保持訓練穩定，我們提出了一個簡單但有效的方法，即自適應對抗擾動 (A2P)，它可以動態地為每個樣本選擇適當的對抗擾動。具體來說，我們提出了一個自適應對抗係數框架，以調整訓練過程中對抗擾動的影響。透過設計一個用於測量擾動當前強度的指標，我們的模型可以根據當前的相對效能計算適當的擾動級別。我們的方法具有吸引力的特點是，它易於部署在實際應用中，並且不需要事先存取模擬器。MuJoCo 中的實驗表明，當我們的模型轉移到不同的測試環境時，它可以改善訓練穩定性並學習穩健的政策。程式碼可在 https://github.com/Lqm00/A2P-SAC 取得。

##### **SM-DTW: Stability Modulated Dynamic Time Warping for signature verification**
2405.11978v1 by Antonio Parziale, Moises Diaz, Miguel A. Ferrer, Angelo Marcelli

Building upon findings in computational model of handwriting learning and
execution, we introduce the concept of stability to explain the difference
between the actual movements performed during multiple execution of the
subject's signature, and conjecture that the most stable parts of the signature
should play a paramount role in evaluating the similarity between a questioned
signature and the reference ones during signature verification. We then
introduce the Stability Modulated Dynamic Time Warping algorithm for
incorporating the stability regions, i.e. the most similar parts between two
signatures, into the distance measure between a pair of signatures computed by
the Dynamic Time Warping for signature verification. Experiments were conducted
on two datasets largely adopted for performance evaluation. Experimental
results show that the proposed algorithm improves the performance of the
baseline system and compares favourably with other top performing signature
verification systems.

摘要：建立在手寫學習和執行運算模型的發現上，我們引入穩定性的概念來解釋在主體簽名多次執行期間實際執行的動作之間的差異，並推測簽名中最穩定的部分應在簽名驗證期間評估有疑問的簽名與參考簽名之間的相似性時發揮至關重要的作用。然後，我們引入穩定性調製動態時間規整演算法，以將穩定區域（即兩個簽名之間最相似的部分）納入由動態時間規整演算法計算出的簽名對之間的距離測量中，以進行簽名驗證。在兩個廣泛採用於效能評估的資料集上進行了實驗。實驗結果顯示，所提出的演算法改善了基準系統的效能，並且與其他頂尖的簽名驗證系統相比之下表現良好。

##### **Conditional Shift-Robust Conformal Prediction for Graph Neural Network**
2405.11968v1 by S. Akansha

Graph Neural Networks (GNNs) have emerged as potent tools for predicting
outcomes in graph-structured data. Despite their efficacy, a significant
drawback of GNNs lies in their limited ability to provide robust uncertainty
estimates, posing challenges to their reliability in contexts where errors
carry significant consequences. Moreover, GNNs typically excel in
in-distribution settings, assuming that training and test data follow identical
distributions: a condition often unmet in real-world graph data scenarios. In
this article, we leverage conformal prediction, a widely recognized statistical
technique for quantifying uncertainty by transforming predictive model outputs
into prediction sets, to address uncertainty quantification in GNN predictions
amidst conditional shift \footnote{Representing the change in conditional
probability distribution $P(label |input)$ from source domain to target
domain.} in graph-based semi-supervised learning (SSL). Additionally, we
propose a novel loss function aimed at refining model predictions by minimizing
conditional shift in latent stages. Termed Conditional Shift Robust (CondSR)
conformal prediction for GNNs, our approach CondSR is model-agnostic and
adaptable to various classification models. We validate the effectiveness of
our method on standard graph benchmark datasets, integrating it with
state-of-the-art GNNs in node classification tasks. The code implementation is
publicly available for further exploration and experimentation.

摘要：圖形神經網路 (GNN) 已成為預測圖形結構資料中結果的強大工具。儘管其有效，但 GNN 的一個重大缺點在於其提供穩健不確定性估計的能力有限，這對其在錯誤會造成重大後果的環境中的可靠性構成挑戰。此外，GNN 通常在同分佈設定中表現出色，假設訓練和測試資料遵循相同的分配：這是一個在現實世界圖形資料場景中經常無法滿足的條件。在本文中，我們利用共形預測，這是一種廣泛認可的統計技術，透過將預測模型輸出轉換為預測集合來量化不確定性，以解決在圖形半監督學習 (SSL) 中條件轉移下 GNN 預測中的不確定性量化問題\footnote{表示條件機率分佈 $P(label |input)$ 從來源網域到目標網域的變化。}。此外，我們提出了一種新的損失函數，旨在透過最小化潛在階段中的條件轉移來改善模型預測。我們的 CondSR 方法稱為條件轉移穩健 (CondSR) 共形預測，適用於 GNN，它與模型無關，且適用於各種分類模型。我們在標準圖形基準資料集上驗證了我們方法的有效性，並將其與節點分類任務中的最先進 GNN 整合。程式碼實作已公開，可供進一步探索和實驗。

##### **Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home**
2405.11967v1 by Tatiana V. Afanasieva, Pavel V. Platov, Anastasia I. Medvedeva

One of the new trends in the development of recommendation algorithms is the
dissemination of their capabilities to support the population in managing their
health. This article focuses on the problem of improving the effectiveness of
cardiovascular diseases (CVD) prevention, since CVD is the leading cause of
death worldwide. To address this issue, a knowledge-based recommendation
algorithm was proposed to support self-management of CVD risk factors in adults
at home. The proposed algorithm is based on the original multidimensional
recommendation model and on a new user profile model, which includes predictive
assessments of CVD health in addition to its current ones as outlined in
official guidelines. The main feature of the proposed algorithm is the
combination of rule-based logic with the capabilities of a large language model
in generating human-like text for explanatory component of multidimensional
recommendation. The verification and evaluation of the proposed algorithm
showed the usefulness of the proposed recommendation algorithm for supporting
adults in self-management of their CVD risk factors at home. As follows from
the comparison with similar knowledge-based recommendation algorithms, the
proposed algorithm evaluates a larger number of CVD risk factors and has a
greater information and semantic capacity of the generated recommendations.

摘要：推薦演算法發展的新趨勢之一是傳播其能力，以協助民眾管理自身健康。本文重點探討改善心血管疾病（CVD）預防的有效性，因為 CVD 是全球主要的死亡原因。為了解決這個問題，提出了一種基於知識的推薦演算法，以在家中支援成人自我管理 CVD 風險因子。所提出的演算法基於原始的多維度推薦模型和新的使用者輪廓模型，其中除了官方指南中概述的現有評估外，還包括 CVD 健康的預測評估。所提出的演算法主要特色是將基於規則的邏輯與大型語言模型的能力相結合，以產生類人文字，作為多維度推薦的說明性組成部分。所提出的演算法的驗證和評估顯示，所提出的推薦演算法在協助成人在家中自我管理其 CVD 風險因子方面很有用。從與類似的基於知識的推薦演算法的比較中得知，所提出的演算法評估了更多 CVD 風險因子，並且產生的建議具有更大的資訊和語義容量。

##### **Multiple-Choice Questions are Efficient and Robust LLM Evaluators**
2405.11966v2 by Ziyin Zhang, Lizhen Xu, Zhaokun Jiang, Hongkun Hao, Rui Wang

We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed
by collecting answers and incorrect predictions on GSM8K and MATH from over 50
open-source models. Through extensive experiments, we show that LLMs'
performance on the MC versions of these two popular benchmarks is strongly
correlated with their performance on the original versions, and is quite robust
to distractor choices and option orders, while the evaluation time is reduced
by a factor of up to 30. Following a similar procedure, we also introduce
PythonIO, a new program output prediction MC dataset constructed from two other
popular LLM evaluation benchmarks HumanEval and MBPP. Our data and code are
available at https://github.com/Geralt-Targaryen/MC-Evaluation.

摘要：我們提出 GSM-MC 和 MATH-MC，這兩個多選題 (MC) 資料集是透過收集超過 50 個開放原始碼模型在 GSM8K 和 MATH 上的答案和錯誤預測而建構的。透過廣泛的實驗，我們證明 LLM 在這兩個熱門基準測試的多選題版本上的表現與其在原始版本上的表現有很強的相關性，並且對於干擾選項和選項順序相當穩健，同時評估時間減少了多達 30 倍。遵循類似的程序，我們還引入了 PythonIO，這是一個新的程式輸出預測多選題資料集，建構自另外兩個熱門的 LLM 評估基準 HumanEval 和 MBPP。我們的資料和程式碼可以在 https://github.com/Geralt-Targaryen/MC-Evaluation 取得。

##### **WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles**
2405.11950v1 by Tabea M. G. Pakull, Hendrik Damm, Ahmad Idrissi-Yaghir, Henning Schäfer, Peter A. Horn, Christoph M. Friedrich

This paper details the efforts of the WisPerMed team in the BioLaySumm2024
Shared Task on automatic lay summarization in the biomedical domain, aimed at
making scientific publications accessible to non-specialists. Large language
models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned
and employed to create lay summaries from complex scientific texts. The
summarization performance was enhanced through various approaches, including
instruction tuning, few-shot learning, and prompt variations tailored to
incorporate specific context information. The experiments demonstrated that
fine-tuning generally led to the best performance across most evaluated
metrics. Few-shot learning notably improved the models' ability to generate
relevant and factually accurate texts, particularly when using a well-crafted
prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize
the selection of text outputs based on readability and factuality metrics was
developed. Out of 54 participants, the WisPerMed team reached the 4th place,
measured by readability, factuality, and relevance. Determined by the overall
score, our approach improved upon the baseline by approx. 5.5 percentage points
and was only approx 1.5 percentage points behind the first place.

摘要：這篇論文詳細說明了 WisPerMed 團隊在 BioLaySumm2024 生物醫學領域自動化摘要共享任務中的努力，旨在讓非專家也能理解科學出版物。大型語言模型 (LLM)，特別是 BioMistral 和 Llama3 模型經過微調，並用於從複雜的科學文本中建立摘要。摘要的表現透過各種方法得到提升，包括指令微調、少量學習，以及針對特定脈絡資訊而調整的提示變化。實驗證明，微調通常會在大部分評估指標中帶來最佳表現。少量學習顯著改善了模型產生相關且事實正確的文字的能力，特別是在使用精心設計的提示時。此外，還開發了一種動態專家選擇 (DES) 機制，用於根據可讀性和事實性指標來最佳化文字輸出的選擇。在 54 位參與者中，WisPerMed 團隊獲得第 4 名，衡量標準為可讀性、事實性和相關性。根據整體評分，我們的做法比基準線進步了約 5.5 個百分點，僅比第一名低了約 1.5 個百分點。

##### **FAME-MT Dataset: Formality Awareness Made Easy for Machine Translation Purposes**
2405.11942v1 by Dawid Wiśniewski, Zofia Rostek, Artur Nowakowski

People use language for various purposes. Apart from sharing information,
individuals may use it to express emotions or to show respect for another
person. In this paper, we focus on the formality level of machine-generated
translations and present FAME-MT -- a dataset consisting of 11.2 million
translations between 15 European source languages and 8 European target
languages classified to formal and informal classes according to target
sentence formality. This dataset can be used to fine-tune machine translation
models to ensure a given formality level for each European target language
considered. We describe the dataset creation procedure, the analysis of the
dataset's quality showing that FAME-MT is a reliable source of language
register information, and we present a publicly available proof-of-concept
machine translation model that uses the dataset to steer the formality level of
the translation. Currently, it is the largest dataset of formality annotations,
with examples expressed in 112 European language pairs. The dataset is
published online: https://github.com/laniqo-public/fame-mt/ .

摘要：人們使用語言的目的是多種多樣的。除了共享資訊之外，個人可能會使用語言來表達情緒或表示對他人的尊重。在本文中，我們專注於機器產生的翻譯的正式程度，並提出 FAME-MT -- 一個由 1120 萬個翻譯組成的資料集，其中包含 15 種歐洲原始語言與 8 種歐洲目標語言之間的翻譯，並根據目標句子的正式程度分類為正式和非正式類別。此資料集可用於微調機器翻譯模型，以確保每個歐洲目標語言都有既定的正式程度。我們描述了資料集建立程序、資料集品質分析，顯示 FAME-MT 是語言註冊資訊的可靠來源，並提出了一個公開可用的概念驗證機器翻譯模型，該模型使用資料集來引導翻譯的正式程度。目前，它是最大的正式註解資料集，範例使用 112 種歐洲語言對表達。資料集已在線上發布：https://github.com/laniqo-public/fame-mt/。

##### **Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT Model on an Automatically Generated Wikipedia Corpus**
2405.11941v1 by Fons Hartendorp, Tom Seinen, Erik van Mulligen, Suzan Verberne

Biomedical entity linking, a main component in automatic information
extraction from health-related texts, plays a pivotal role in connecting
textual entities (such as diseases, drugs and body parts mentioned by patients)
to their corresponding concepts in a structured biomedical knowledge base. The
task remains challenging despite recent developments in natural language
processing. This paper presents the first evaluated biomedical entity linking
model for the Dutch language. We use MedRoBERTa.nl as base model and perform
second-phase pretraining through self-alignment on a Dutch biomedical ontology
extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of
ontology-linked Dutch biomedical entities in context and fine-tune our model on
this dataset. We evaluate our model on the Dutch portion of the Mantra
GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance
accuracy. We then perform a case study on a collection of unlabeled,
patient-support forum data and show that our model is hampered by the limited
quality of the preceding entity recognition step. Manual evaluation of small
sample indicates that of the correctly extracted entities, around 65% is linked
to the correct concept in the ontology. Our results indicate that biomedical
entity linking in a language other than English remains challenging, but our
Dutch model can be used to for high-level analysis of patient-generated text.

摘要：生物醫學實體連結，是從健康相關文本中自動提取資訊的主要組成部分，在將文本實體（例如患者提到的疾病、藥物和身體部位）連結到結構化生物醫學知識庫中的對應概念方面發揮著關鍵作用。儘管自然語言處理最近有發展，但此任務仍然具有挑戰性。本文提出了第一個針對荷蘭語的生物醫學實體連結模型評估。我們使用 MedRoBERTa.nl 作為基礎模型，並透過從 UMLS 和荷蘭 SNOMED 中萃取的荷蘭生物醫學本体進行自我對齊，執行第二階段預訓練。我們從維基百科中衍生了一個語境中的本体連結荷蘭生物醫學實體語料庫，並針對此資料集微調我們的模型。我們在 Mantra GSC 語料庫的荷蘭語部分對我們的模型進行評估，並達到 54.7% 的分類準確率和 69.8% 的 1 距離準確率。然後，我們對一組未標記的患者支援論壇資料進行案例研究，並顯示我們的模型受到前一個實體辨識步驟品質有限的阻礙。對小型樣本的手動評估表明，在正確萃取的實體中，約有 65% 連結到本体中的正確概念。我們的結果表明，以英語以外的語言進行生物醫學實體連結仍然具有挑戰性，但我們的荷蘭語模型可用於對患者產生的文本進行高階分析。

##### **Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation**
2405.11937v1 by Kamil Guttmann, Mikołaj Pokrywka, Adrian Charkiewicz, Artur Nowakowski

This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in
machine translation (MT), particularly for domain adaptation and low-resource
languages. We implement the self-improvement process by fine-tuning the model
on its MBR-decoded forward translations. By employing COMET as the MBR utility
metric, we aim to achieve the reranking of translations that better aligns with
human preferences. The paper explores the iterative application of this
approach and the potential need for language-specific MBR utility metrics. The
results demonstrate significant enhancements in translation quality for all
examined language pairs, including successful application to domain-adapted
models and generalisation to low-resource settings. This highlights the
potential of COMET-guided MBR for efficient MT self-improvement in various
scenarios.

摘要：本篇論文探討了機器翻譯（MT）中用於自我提升的最小貝氏風險（MBR）解碼，特別是針對領域適應和低資源語言。我們透過對模型進行微調，使其 MBR 解碼的前向翻譯，來實作自我提升的過程。透過採用 COMET 作為 MBR 實用程式量度，我們旨在達成更符合人類偏好的翻譯重新排序。本文探討了此方法的迭代應用，以及對特定語言的 MBR 實用程式量度潛在需求。結果顯示，所有檢驗的語言對都顯著提升了翻譯品質，包括成功應用於領域適應模型，以及推廣至低資源設定。這突顯了 COMET 引導的 MBR 在各種情境中，進行有效 MT 自我提升的潛力。

##### **"Set It Up!": Functional Object Arrangement with Compositional Generative Models**
2405.11928v1 by Yiqing Xu, Jiayuan Mao, Yilun Du, Tomas Lozáno-Pérez, Leslie Pack Kaebling, David Hsu

This paper studies the challenge of developing robots capable of
understanding under-specified instructions for creating functional object
arrangements, such as "set up a dining table for two"; previous arrangement
approaches have focused on much more explicit instructions, such as "put object
A on the table." We introduce a framework, SetItUp, for learning to interpret
under-specified instructions. SetItUp takes a small number of training examples
and a human-crafted program sketch to uncover arrangement rules for specific
scene types. By leveraging an intermediate graph-like representation of
abstract spatial relationships among objects, SetItUp decomposes the
arrangement problem into two subproblems: i) learning the arrangement patterns
from limited data and ii) grounding these abstract relationships into object
poses. SetItUp leverages large language models (LLMs) to propose the abstract
spatial relationships among objects in novel scenes as the constraints to be
satisfied; then, it composes a library of diffusion models associated with
these abstract relationships to find object poses that satisfy the constraints.
We validate our framework on a dataset comprising study desks, dining tables,
and coffee tables, with the results showing superior performance in generating
physically plausible, functional, and aesthetically pleasing object
arrangements compared to existing models.

摘要：這篇論文探討了開發機器人的挑戰，這些機器人能夠理解未明確規定的指示，以建立功能性的物件排列，例如「為兩個人擺好餐桌」；先前的排列方法著重於更明確的指示，例如「將物件 A 放在桌上」。我們引進一個架構 SetItUp，用於學習詮釋未明確規定的指示。SetItUp 採用少數訓練範例和人類製作的程式草圖，以找出特定場景類型的排列規則。透過利用物件之間抽象空間關係的中間圖形化表示，SetItUp 將排列問題分解成兩個子問題：i) 從有限資料中學習排列模式，以及 ii) 將這些抽象關係基礎化為物件姿勢。SetItUp 利用大型語言模型 (LLM) 來提出新場景中物件之間的抽象空間關係，作為要滿足的約束；然後，它編寫一個與這些抽象關係相關的擴散模型庫，以找出滿足約束的物件姿勢。我們在包含書桌、餐桌和咖啡桌的資料集上驗證我們的架構，結果顯示在產生物理上合理、功能性和美觀的物件排列方面，與現有模型相比具有優異的效能。

##### **On Efficient and Statistical Quality Estimation for Data Annotation**
2405.11919v1 by Jan-Christoph Klie, Rahul Nair, Juan Haladjian, Marc Kirchner

Annotated datasets are an essential ingredient to train, evaluate, compare
and productionalize supervised machine learning models. It is therefore
imperative that annotations are of high quality. For their creation, good
quality management and thereby reliable quality estimates are needed. Then, if
quality is insufficient during the annotation process, rectifying measures can
be taken to improve it. Quality estimation is often performed by having experts
manually label instances as correct or incorrect. But checking all annotated
instances tends to be expensive. Therefore, in practice, usually only subsets
are inspected; sizes are chosen mostly without justification or regard to
statistical power and more often than not, are relatively small. Basing
estimates on small sample sizes, however, can lead to imprecise values for the
error rate. Using unnecessarily large sample sizes costs money that could be
better spent, for instance on more annotations. Therefore, we first describe in
detail how to use confidence intervals for finding the minimal sample size
needed to estimate the annotation error rate. Then, we propose applying
acceptance sampling as an alternative to error rate estimation We show that
acceptance sampling can reduce the required sample sizes up to 50% while
providing the same statistical guarantees.

摘要：標註過的資料集是訓練、評量、比較和生產監督式機器學習模型的必要元素。因此，標註的品質必須很高。為了建立標註，需要良好的品質管理和可靠的品質估計。接著，如果在標註過程中品質不足，可以採取補救措施來改善品質。品質估計通常由專家手動標示實例為正確或不正確來執行。但是，檢查所有標註的實例往往很昂貴。因此，在實務上，通常只檢查子集；大小的選擇大多沒有依據或考量統計功效，而且通常很小。然而，根據小樣本大小來估計可能會導致錯誤率的值不精確。使用不必要的大樣本大小會花費金錢，而這些金錢可以花在更好的地方，例如更多標註。因此，我們首先詳細說明如何使用信賴區間來找出估計標註錯誤率所需的最小樣本大小。接著，我們建議採用驗收抽樣作為錯誤率估計的替代方案。我們顯示，驗收抽樣可以將所需的樣本大小減少多達 50%，同時提供相同的統計保證。

##### **ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation**
2405.11912v1 by Chen Huang, Yiping Jin, Ilija Ilievski, Wenqiang Lei, Jiancheng Lv

Human annotation is a time-consuming task that requires a significant amount
of effort. To address this issue, interactive data annotation utilizes an
annotation model to provide suggestions for humans to approve or correct.
However, annotation models trained with limited labeled data are prone to
generating incorrect suggestions, leading to extra human correction effort. To
tackle this challenge, we propose Araida, an analogical reasoning-based
approach that enhances automatic annotation accuracy in the interactive data
annotation setting and reduces the need for human corrections. Araida involves
an error-aware integration strategy that dynamically coordinates an annotation
model and a k-nearest neighbors (KNN) model, giving more importance to KNN's
predictions when predictions from the annotation model are deemed inaccurate.
Empirical studies demonstrate that Araida is adaptable to different annotation
tasks and models. On average, it reduces human correction labor by 11.02%
compared to vanilla interactive data annotation methods.

摘要：人工註解是一項耗時的任務，需要大量的精力。為了解決這個問題，互動式資料註解利用註解模型提供人類批准或更正的建議。然而，使用受限標記資料訓練的註解模型容易產生不正確的建議，導致額外的人工更正工作。為了應對這個挑戰，我們提出了 Araida，一種基於類比推理的方法，它增強了互動式資料註解設定中的自動註解準確性，並減少了人工更正的需求。Araida 涉及一個錯誤感知整合策略，它動態地協調一個註解模型和一個 k 最近鄰 (KNN) 模型，當註解模型的預測被認為不準確時，賦予 KNN 的預測更多重要性。實證研究表明，Araida 適用於不同的註解任務和模型。平均而言，與香草互動式資料註解方法相比，它將人工更正工作量減少了 11.02%。

##### **A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers**
2405.11904v1 by Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi

Text classifiers are vulnerable to adversarial examples --
correctly-classified examples that are deliberately transformed to be
misclassified while satisfying acceptability constraints. The conventional
approach to finding adversarial examples is to define and solve a combinatorial
optimisation problem over a space of allowable transformations. While
effective, this approach is slow and limited by the choice of transformations.
An alternate approach is to directly generate adversarial examples by
fine-tuning a pre-trained language model, as is commonly done for other
text-to-text tasks. This approach promises to be much quicker and more
expressive, but is relatively unexplored. For this reason, in this work we
train an encoder-decoder paraphrase model to generate a diverse range of
adversarial examples. For training, we adopt a reinforcement learning algorithm
and propose a constraint-enforcing reward that promotes the generation of valid
adversarial examples. Experimental results over two text classification
datasets show that our model has achieved a higher success rate than the
original paraphrase model, and overall has proved more effective than other
competitive attacks. Finally, we show how key design choices impact the
generated examples and discuss the strengths and weaknesses of the proposed
approach.

摘要：文本分类器容易受到对抗性示例的影响——经过蓄意转换以被错误分类，同时满足可接受性约束的正确分类示例。寻找对抗性示例的传统方法是在允许的转换空间上定义和解决组合优化问题。虽然有效，但这种方法很慢，并且受到转换选择的影响。另一种方法是通过微调预训练的语言模型来直接生成对抗性示例，就像通常对其他文本到文本任务所做的那样。这种方法有望更快、更具表现力，但相对来说尚未得到探索。出于这个原因，在这项工作中，我们训练了一个编码器-解码器释义模型来生成各种对抗性示例。在训练中，我们采用了一种强化学习算法，并提出了一种强制约束的奖励，以促进生成有效的对抗性示例。在两个文本分类数据集上的实验结果表明，我们的模型比原始释义模型获得了更高的成功率，并且总体上比其他竞争性攻击更有效。最后，我们展示了关键设计选择如何影响生成的示例，并讨论了所提出方法的优点和缺点。

##### **CReMa: Crisis Response through Computational Identification and Matching of Cross-Lingual Requests and Offers Shared on Social Media**
2405.11897v1 by Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera, Muhammad Imran

During times of crisis, social media platforms play a vital role in
facilitating communication and coordinating resources. Amidst chaos and
uncertainty, communities often rely on these platforms to share urgent pleas
for help, extend support, and organize relief efforts. However, the sheer
volume of conversations during such periods, which can escalate to
unprecedented levels, necessitates the automated identification and matching of
requests and offers to streamline relief operations. This study addresses the
challenge of efficiently identifying and matching assistance requests and
offers on social media platforms during emergencies. We propose CReMa (Crisis
Response Matcher), a systematic approach that integrates textual, temporal, and
spatial features for multi-lingual request-offer matching. By leveraging
CrisisTransformers, a set of pre-trained models specific to crises, and a
cross-lingual embedding space, our methodology enhances the identification and
matching tasks while outperforming strong baselines such as RoBERTa, MPNet, and
BERTweet, in classification tasks, and Universal Sentence Encoder, Sentence
Transformers in crisis embeddings generation tasks. We introduce a novel
multi-lingual dataset that simulates scenarios of help-seeking and offering
assistance on social media across the 16 most commonly used languages in
Australia. We conduct comprehensive cross-lingual experiments across these 16
languages, also while examining trade-offs between multiple vector search
strategies and accuracy. Additionally, we analyze a million-scale geotagged
global dataset to comprehend patterns in relation to seeking help and offering
assistance on social media. Overall, these contributions advance the field of
crisis informatics and provide benchmarks for future research in the area.

摘要：<paragraph>在危機時刻，社群媒體平台在促進溝通和協調資源方面扮演著至關重要的角色。在混亂和不確定的情況下，社群經常依賴這些平台分享緊急求助、提供支援和組織救助工作。然而，在這些時期，對話量極大，可能會升高到前所未有的程度，這需要自動化辨識和配對請求和提供，以簡化救助行動。本研究探討在緊急情況下，在社群媒體平台上有效辨識和配對求助和提供請求的挑戰。我們提出 CReMa（危機回應配對器），這是一種系統化方法，整合了文字、時間和空間特徵，用於多語言請求-提供配對。透過利用 CrisisTransformers，這是一組專門針對危機的預訓練模型，以及跨語言嵌入空間，我們的技術增強了辨識和配對任務，同時在分類任務中優於 RoBERTa、MPNet 和 BERTweet 等強大的基線，以及通用句子編碼器、句子轉換器在危機嵌入生成任務中。我們引入了一個新穎的多語言資料集，模擬了在澳洲 16 種最常用的語言中，在社群媒體上尋求幫助和提供協助的場景。我們在這些 16 種語言中進行了全面的跨語言實驗，同時也探討了多種向量搜尋策略和準確性之間的取捨。此外，我們分析了一個百萬規模的地理標記全球資料集，以了解在社群媒體上尋求幫助和提供協助的模式。總體而言，這些貢獻推動了危機資訊學領域的發展，並為該領域的未來研究提供了基準。</paragraph>

##### **Unveiling and Manipulating Prompt Influence in Large Language Models**
2405.11891v1 by Zijian Feng, Hanzhang Zhou, Zixiao Zhu, Junlang Qian, Kezhi Mao

Prompts play a crucial role in guiding the responses of Large Language Models
(LLMs). However, the intricate role of individual tokens in prompts, known as
input saliency, in shaping the responses remains largely underexplored.
Existing saliency methods either misalign with LLM generation objectives or
rely heavily on linearity assumptions, leading to potential inaccuracies. To
address this, we propose Token Distribution Dynamics (TDD), a
\textcolor{black}{simple yet effective} approach to unveil and manipulate the
role of prompts in generating LLM outputs. TDD leverages the robust
interpreting capabilities of the language model head (LM head) to assess input
saliency. It projects input tokens into the embedding space and then estimates
their significance based on distribution dynamics over the vocabulary. We
introduce three TDD variants: forward, backward, and bidirectional, each
offering unique insights into token relevance. Extensive experiments reveal
that the TDD surpasses state-of-the-art baselines with a big margin in
elucidating the causal relationships between prompts and LLM outputs. Beyond
mere interpretation, we apply TDD to two prompt manipulation tasks for
controlled text generation: zero-shot toxic language suppression and sentiment
steering. Empirical results underscore TDD's proficiency in identifying both
toxic and sentimental cues in prompts, subsequently mitigating toxicity or
modulating sentiment in the generated content.

摘要：提示在引导大型语言模型 (LLM) 的响应中扮演着至关重要的角色。然而，提示中各个标记的复杂作用，即输入显着性，在塑造响应方面仍然很大程度上未被探索。现有的显着性方法要么与 LLM 生成目标不一致，要么严重依赖线性假设，从而导致潜在的不准确性。为了解决这个问题，我们提出了标记分布动态 (TDD)，这是一种揭示和操纵提示在生成 LLM 输出中的作用的简单而有效的方法。TDD 利用语言模型头 (LM 头) 强大的解释能力来评估输入显着性。它将输入标记投影到嵌入空间，然后根据词汇表上的分布动态估计它们的显着性。我们引入了三种 TDD 变体：向前、向后和双向，每种变体都提供了对标记相关性的独特见解。广泛的实验表明，TDD 以很大的优势超越了最先进的基线，阐明了提示和 LLM 输出之间的因果关系。除了简单的解释之外，我们还将 TDD 应用于两个提示操作任务，用于受控文本生成：零样本有毒语言抑制和情感引导。经验结果强调了 TDD 在识别提示中的有毒和情感线索方面的能力，随后减轻了生成的文本中的毒性或调节了情感。

##### **Out-of-Distribution Detection with a Single Unconditional Diffusion Model**
2405.11881v1 by Alvin Heng, Alexandre H. Thiery, Harold Soh

Out-of-distribution (OOD) detection is a critical task in machine learning
that seeks to identify abnormal samples. Traditionally, unsupervised methods
utilize a deep generative model for OOD detection. However, such approaches
necessitate a different model when evaluating abnormality against a new
distribution. With the emergence of foundational generative models, this paper
explores whether a single generalist model can also perform OOD detection
across diverse tasks. To that end, we introduce our method, Diffusion Paths,
(DiffPath) in this work. DiffPath proposes to utilize a single diffusion model
originally trained to perform unconditional generation for OOD detection.
Specifically, we introduce a novel technique of measuring the rate-of-change
and curvature of the diffusion paths connecting samples to the standard normal.
Extensive experiments show that with a single model, DiffPath outperforms prior
work on a variety of OOD tasks involving different distributions. Our code is
publicly available at https://github.com/clear-nus/diffpath.

摘要：離散佈偵測 (OOD) 是機器學習中一項關鍵任務，旨在辨識異常樣本。傳統上，非監督式方法會利用深度生成模型來進行 OOD 偵測。然而，此類方法在針對新分佈評估異常時需要採用不同的模型。隨著基礎生成模型的出現，本文探討單一泛用模型是否也能在各種任務中執行 OOD 偵測。為此，我們在此工作中介紹了我們的 Diffusion Paths (DiffPath) 方法。DiffPath 提議利用單一擴散模型，該模型最初訓練為執行無條件生成，以進行 OOD 偵測。具體來說，我們介紹了一種新的技術，用於測量連接樣本與標準常態分佈的擴散路徑的變化率和曲率。廣泛的實驗顯示，DiffPath 使用單一模型即可超越先前針對涉及不同分佈的各種 OOD 任務所做的工作。我們的程式碼已於 https://github.com/clear-nus/diffpath 公開。

##### **Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs**
2405.11880v1 by Siyu Lou, Yuntian Chen, Xiaodan Liang, Liang Lin, Quanshi Zhang

In this study, we propose an axiomatic system to define and quantify the
precise memorization and in-context reasoning effects used by the large
language model (LLM) for language generation. These effects are formulated as
non-linear interactions between tokens/words encoded by the LLM. Specifically,
the axiomatic system enables us to categorize the memorization effects into
foundational memorization effects and chaotic memorization effects, and further
classify in-context reasoning effects into enhanced inference patterns,
eliminated inference patterns, and reversed inference patterns. Besides, the
decomposed effects satisfy the sparsity property and the universal matching
property, which mathematically guarantee that the LLM's confidence score can be
faithfully decomposed into the memorization effects and in-context reasoning
effects. Experiments show that the clear disentanglement of memorization
effects and in-context reasoning effects enables a straightforward examination
of detailed inference patterns encoded by LLMs.

摘要：在本研究中，我們提出一個公理系統，用來定義和量化大型語言模型 (LLM) 用於語言生成的精確記憶和語境推理效果。這些效果被表述為 LLM 編碼的記號/字詞之間的非線性交互。具體來說，公理系統使我們能夠將記憶效果分類為基礎記憶效果和混亂記憶效果，並進一步將語境推理效果分類為增強推理模式、消除推理模式和反向推理模式。此外，分解的效果滿足稀疏性屬性和通用匹配屬性，這在數學上保證了 LLM 的置信度分數可以忠實地分解為記憶效果和語境推理效果。實驗表明，記憶效果和語境推理效果的清楚區分，能夠對 LLM 編碼的詳細推理模式進行直接檢查。

##### **A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus**
2405.11877v2 by Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu

Natural language inference (NLI), the task of recognizing the entailment
relationship in sentence pairs, is an actively studied topic serving as a proxy
for natural language understanding. Despite the relevance of the task in
building conversational agents and improving text classification, machine
translation and other NLP tasks, to the best of our knowledge, there is no
publicly available NLI corpus for the Romanian language. To this end, we
introduce the first Romanian NLI corpus (RoNLI) comprising 58K training
sentence pairs, which are obtained via distant supervision, and 6K validation
and test sentence pairs, which are manually annotated with the correct labels.
We conduct experiments with multiple machine learning methods based on distant
learning, ranging from shallow models based on word embeddings to
transformer-based neural networks, to establish a set of competitive baselines.
Furthermore, we improve on the best model by employing a new curriculum
learning strategy based on data cartography. Our dataset and code to reproduce
the baselines are available at https://github.com/Eduard6421/RONLI.

摘要：自然語言推理 (NLI) 是識別句子對中蘊含關係的任務，是作為自然語言理解的代理而積極研究的主題。儘管此任務與建構對話代理和改善文字分類、機器翻譯和其他 NLP 任務相關，但據我們所知，羅馬尼亞語並沒有公開可用的 NLI 語料庫。為此，我們引入了第一個羅馬尼亞語 NLI 語料庫 (RoNLI)，包含 58K 個透過遠程監督獲得的訓練句子對，以及 6K 個透過人工標註正確標籤的驗證和測試句子對。我們使用多種基於遠程學習的機器學習方法進行實驗，從基於詞嵌入的淺層模型到基於轉換器的類神經網路，以建立一組競爭基準。此外，我們透過採用基於資料製圖的新課程學習策略來改進最佳模型。我們的資料集和重現基準的程式碼可在 https://github.com/Eduard6421/RONLI 取得。

##### **xFinder: Robust and Pinpoint Answer Extraction for Large Language Models**
2405.11874v1 by Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, Ding Chen

The continuous advancement of large language models (LLMs) has brought
increasing attention to the critical issue of developing fair and reliable
methods for evaluating their performance. Particularly, the emergence of
subjective or non-subjective cheating phenomena, such as test set leakage and
prompt format overfitting, poses significant challenges to the reliable
evaluation of LLMs. Since evaluation frameworks often utilize Regular
Expression (RegEx) for answer extraction, some models may adjust their
responses to comply with specific formats that are easily extractable by RegEx.
Nevertheless, the key answer extraction module based on RegEx frequently
suffers from extraction errors. This paper conducts a comprehensive analysis of
the entire LLM evaluation chain, demonstrating that optimizing the key answer
extraction module can improve extraction accuracy, reduce LLMs' reliance on
specific answer formats, and enhance the reliability of LLM evaluation. To
address these issues, we propose xFinder, a model specifically designed for key
answer extraction. As part of this process, we create a specialized dataset,
the Key Answer Finder (KAF) dataset, to ensure effective model training and
evaluation. Through generalization testing and evaluation in real-world
scenarios, the results demonstrate that the smallest xFinder model with only
500 million parameters achieves an average answer extraction accuracy of
93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%.
xFinder exhibits stronger robustness and higher accuracy compared to existing
evaluation frameworks. All resources for xFinder are available at
\url{https://github.com/IAAR-Shanghai/xFinder}.

摘要：大型語言模型（LLM）的持續進步引起了人們對開發公平且可靠的方法來評估其性能的關鍵問題的越來越多的關注。特別是主觀或非主觀作弊現象的出現，例如測試集洩漏和提示格式過度擬合，對 LLM 的可靠評估構成了重大挑戰。由於評估框架通常利用正則表達式（RegEx）進行答案提取，一些模型可能會調整其響應以符合 RegEx 容易提取的特定格式。儘管如此，基於 RegEx 的關鍵答案提取模塊經常會出現提取錯誤。本文對整個 LLM 評估鏈進行了全面分析，證明優化關鍵答案提取模塊可以提高提取準確度，減少 LLM 對特定答案格式的依賴，並增強 LLM 評估的可靠性。為了解決這些問題，我們提出了 xFinder，這是一個專門設計用於關鍵答案提取的模型。作為此過程的一部分，我們創建了一個專門的數據集，即關鍵答案查找器 (KAF) 數據集，以確保有效的模型訓練和評估。通過在現實世界場景中進行泛化測試和評估，結果表明，只有 5 億個參數的最小的 xFinder 模型達到了 93.42% 的平均答案提取準確度。相比之下，在最好的評估框架中，RegEx 的準確度為 74.38%。與現有的評估框架相比，xFinder 表現出更強的魯棒性和更高的準確性。xFinder 的所有資源均可在 https://github.com/IAAR-Shanghai/xFinder 獲得。

##### **Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process**
2405.11870v1 by Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, Bowen Zhou

Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback
(RLHF) are two fundamental processes for enhancing the capabilities of Language
Models (LMs) post pre-training, aligning them better with human preferences.
Although SFT advances in training efficiency, RLHF delivers better alignment,
thus they are often combined. However, common practices simply apply them
sequentially without unifying their optimization targets, resulting in a
trade-off between fitting different objectives, and ignoring the opportunities
to bridge the paradigm gap and take the strength from both. To obtain a unified
understanding, we interpret SFT and RLHF using two sub-processes -- Preference
Estimation and Transition Optimization -- defined at token level within the
Markov Decision Process (MDP) framework. This modeling shows that SFT is only a
specialized case of RLHF with inferior estimation and optimization. RLHF
evaluates the quality of model's entire generated answer, whereas SFT only
scores predicted tokens based on preceding tokens from target answers.
Therefore, SFT overestimates the ability of model, leading to inferior
optimization. Building on this view, we introduce Intuitive Fine-tuning (IFT)
to integrate SFT and RLHF into a single process. IFT captures LMs' intuitive
sense of the entire answers through a temporal residual connection, while using
a single policy and the same volume of non-preference-labeled data as SFT. Our
experiments show that IFT performs comparably or even superiorly to sequential
recipes of SFT and some typical alignment methods across several tasks,
particularly those requires generation, reasoning, and fact-following
abilities. An explainable Frozen Lake game further validates the effectiveness
of IFT.

摘要：監督式微調 (SFT) 和人類回饋強化學習 (RLHF) 是兩種用於增強語言模型 (LM) 能力的基本流程，在預訓練後讓它們更符合人類偏好。儘管 SFT 在訓練效率方面取得進展，但 RLHF 提供了更好的比對，因此它們經常結合使用。然而，常見做法只是將它們按順序應用，而沒有統一其最佳化目標，導致在符合不同目標之間進行權衡，並忽略了彌合典範差距和利用兩者優勢的機會。為了獲得統一的理解，我們使用兩個子流程來詮釋 SFT 和 RLHF——偏好估計和轉換最佳化——這些子流程在馬可夫決策過程 (MDP) 框架中在令牌層級定義。這種建模顯示，SFT 只是 RLHF 的一種特殊情況，具有較差的估計和最佳化。RLHF 評估模型整個生成答案的品質，而 SFT 僅根據目標答案中的前置令牌對預測令牌進行評分。因此，SFT 高估了模型的能力，導致最佳化較差。基於此觀點，我們引入了直觀微調 (IFT) 來將 SFT 和 RLHF 整合到單一流程中。IFT 通過暫態殘差連接捕捉 LM 對整個答案的直觀感覺，同時使用單一策略和與 SFT 相同體積的非偏好標籤數據。我們的實驗表明，IFT 在多項任務中表現出與 SFT 和一些典型比對方法的順序配方相當甚至優越，特別是那些需要生成、推理和事實遵循能力的任務。一個可解釋的 Frozen Lake 遊戲進一步驗證了 IFT 的有效性。

##### **Towards Graph Contrastive Learning: A Survey and Beyond**
2405.11868v1 by Wei Ju, Yifan Wang, Yifang Qin, Zhengyang Mao, Zhiping Xiao, Junyu Luo, Junwei Yang, Yiyang Gu, Dongjie Wang, Qingqing Long, Siyu Yi, Xiao Luo, Ming Zhang

In recent years, deep learning on graphs has achieved remarkable success in
various domains. However, the reliance on annotated graph data remains a
significant bottleneck due to its prohibitive cost and time-intensive nature.
To address this challenge, self-supervised learning (SSL) on graphs has gained
increasing attention and has made significant progress. SSL enables machine
learning models to produce informative representations from unlabeled graph
data, reducing the reliance on expensive labeled data. While SSL on graphs has
witnessed widespread adoption, one critical component, Graph Contrastive
Learning (GCL), has not been thoroughly investigated in the existing
literature. Thus, this survey aims to fill this gap by offering a dedicated
survey on GCL. We provide a comprehensive overview of the fundamental
principles of GCL, including data augmentation strategies, contrastive modes,
and contrastive optimization objectives. Furthermore, we explore the extensions
of GCL to other aspects of data-efficient graph learning, such as weakly
supervised learning, transfer learning, and related scenarios. We also discuss
practical applications spanning domains such as drug discovery, genomics
analysis, recommender systems, and finally outline the challenges and potential
future directions in this field.

摘要：近年來，圖形上的深度學習在各種領域中取得了顯著的成功。然而，依賴註釋圖形數據仍然是一個重大的瓶頸，因為它的成本過高且耗時。為了應對這一挑戰，圖形上的自我監督學習 (SSL) 獲得了越來越多的關注，並取得了顯著的進展。SSL 使機器學習模型能夠從未標記的圖形數據中產生有意義的表示，從而減少對昂貴的標記數據的依賴。雖然圖形上的 SSL 已被廣泛採用，但一個關鍵組成部分圖形對比學習 (GCL) 尚未在現有文獻中得到徹底研究。因此，本調查旨在通過提供專門的 GCL 調查來填補這一空白。我們對 GCL 的基本原理提供了全面的概述，包括數據擴充策略、對比模式和對比優化目標。此外，我們探討了 GCL 在數據高效圖形學習的其他方面的擴展，例如弱監督學習、遷移學習和相關場景。我們還討論了跨越藥物發現、基因組學分析、推薦系統等領域的實際應用，最後概述了該領域的挑戰和潛在的未來方向。

##### **CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English**
2405.11865v1 by Andrew Rueda, Elena Álvarez Mellado, Constantine Lignos

Modern named entity recognition systems have steadily improved performance in
the age of larger and more powerful neural models. However, over the past
several years, the state-of-the-art has seemingly hit another plateau on the
benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into
the test outputs of the highest-performing NER models, conducting a
fine-grained evaluation of their performance by introducing new document-level
annotations on the test set. We go beyond F1 scores by categorizing errors in
order to interpret the true state of the art for NER and guide future work. We
review previous attempts at correcting the various flaws of the test set and
introduce CoNLL#, a new corrected version of the test set that addresses its
systematic and most prevalent errors, allowing for low-noise, interpretable
error analysis.

摘要：現代命名實體辨識系統在大型且更強大的神經模型時代，其效能穩步提升。然而，在過去幾年，最先進的技術似乎在基準 CoNLL-03 英語資料集上又遇到了另一個停滯期。在本文中，我們深入探討效能最高的 NER 模型的測試輸出，並透過在測試集上引入新的文件層級註解，對其效能進行細緻的評估。我們超越 F1 分數，透過分類錯誤來詮釋 NER 的最新真實狀態，並引導後續的工作。我們檢視先前修正測試集各種缺陷的嘗試，並提出 CoNLL#，這是測試集的一個新的修正版本，它解決了測試集的系統性和最普遍的錯誤，允許進行低雜訊且可詮釋的錯誤分析。

##### **Alternators For Sequence Modeling**
2405.11848v1 by Mohammad Reza Rezaei, Adji Bousso Dieng

This paper introduces alternators, a novel family of non-Markovian dynamical
models for sequences. An alternator features two neural networks: the
observation trajectory network (OTN) and the feature trajectory network (FTN).
The OTN and the FTN work in conjunction, alternating between outputting samples
in the observation space and some feature space, respectively, over a cycle.
The parameters of the OTN and the FTN are not time-dependent and are learned
via a minimum cross-entropy criterion over the trajectories. Alternators are
versatile. They can be used as dynamical latent-variable generative models or
as sequence-to-sequence predictors. When alternators are used as generative
models, the FTN produces interpretable low-dimensional latent variables that
capture the dynamics governing the observations. When alternators are used as
sequence-to-sequence predictors, the FTN learns to predict the observed
features. In both cases, the OTN learns to produce sequences that match the
data. Alternators can uncover the latent dynamics underlying complex sequential
data, accurately forecast and impute missing data, and sample new trajectories.
We showcase the capabilities of alternators in three applications. We first
used alternators to model the Lorenz equations, often used to describe chaotic
behavior. We then applied alternators to Neuroscience, to map brain activity to
physical activity. Finally, we applied alternators to Climate Science, focusing
on sea-surface temperature forecasting. In all our experiments, we found
alternators are stable to train, fast to sample from, yield high-quality
generated samples and latent variables, and outperform strong baselines such as
neural ODEs and diffusion models in the domains we studied.

摘要：<paragraph>本文介紹交替器，一種非馬可夫動態序列模型的新穎系列。交替器具有兩個神經網路：觀測軌跡網路 (OTN) 和特徵軌跡網路 (FTN)。OTN 和 FTN 協同工作，在一個週期內，輪流輸出觀測空間和一些特徵空間中的樣本。OTN 和 FTN 的參數與時間無關，並透過軌跡上的最小交叉熵準則學習。交替器具有多功能性。它們可以用作動態潛在變量生成模型或序列對序列預測器。當交替器用作生成模型時，FTN 會產生可解釋的低維潛在變量，以捕捉控制觀測的動態。當交替器用作序列對序列預測器時，FTN 會學習預測觀測到的特徵。在兩種情況下，OTN 都會學習產生與資料相符的序列。交替器可以揭示複雜序列資料背後的潛在動態，準確預測和填補遺失的資料，以及取樣新的軌跡。我們在三個應用程式中展示了交替器的功能。我們首先使用交替器對洛倫茲方程式建模，該方程式通常用於描述混沌行為。然後我們將交替器應用於神經科學，將大腦活動對應到身體活動。最後，我們將交替器應用於氣候科學，重點關注海面溫度預測。在我們所有的實驗中，我們發現交替器在訓練時很穩定，取樣速度很快，產生的樣本和潛在變量品質很高，並且在我們研究的領域中優於強大的基準，例如神經 ODE 和擴散模型。</paragraph>

##### **Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities**
2405.11841v1 by Junqi Wang, Chunhui Zhang, Jiapeng Li, Yuxi Ma, Lixing Niu, Jiaheng Han, Yujia Peng, Yixin Zhu, Lifeng Fan

Facing the current debate on whether Large Language Models (LLMs) attain
near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023;
Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study
introduces a benchmark for evaluating social intelligence, one of the most
distinctive aspects of human cognition. We developed a comprehensive
theoretical framework for social dynamics and introduced two evaluation tasks:
Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also
encompassed a computational model based on recursive Bayesian inference, adept
at elucidating diverse human behavioral patterns. Extensive experiments and
detailed analyses revealed that humans surpassed the latest GPT models in
overall performance, zero-shot learning, one-shot generalization, and
adaptability to multi-modalities. Notably, GPT models demonstrated social
intelligence only at the most basic order (order = 0), in stark contrast to
human social intelligence (order >= 2). Further examination indicated a
propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt
on their possession of authentic human-level social intelligence. Our codes,
dataset, appendix and human data are released at
https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.

摘要：<paragraph>面對大型語言模型 (LLM) 是否達到接近人類智慧水準的當前爭論（Mitchell & Krakauer，2023；Bubeck et al.，2023；Kosinski，2023；Shiffrin & Mitchell，2023；Ullman，2023），本研究提出了評估社交智慧的基準，這是人類認知最顯著的特徵之一。我們為社會動態開發了一個全面的理論框架，並引入了兩個評估任務：逆向推理 (IR) 和逆向逆向規劃 (IIP)。我們的做法還包括一個基於遞迴貝氏推論的計算模型，擅長闡明各種人類行為模式。廣泛的實驗和詳細的分析表明，人類在整體表現、零次學習、一次性概括和對多模態的適應性方面都超越了最新的 GPT 模型。值得注意的是，GPT 模型僅在最基本的順序（順序 = 0）表現出社交智慧，這與人類社交智慧（順序 >= 2）形成鮮明對比。進一步的檢查表明，LLM 傾向於依賴模式識別來尋找捷徑，讓人懷疑它們是否具備真正的、人類水準的社交智慧。我們的程式碼、資料集、附錄和人類資料已於 https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence 發布。</paragraph>

##### **Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model**
2405.11837v1 by Mounes Zaval, Sedat Ozer

In the evolving field of Explainable AI (XAI), interpreting the decisions of
deep neural networks (DNNs) in computer vision tasks is an important process.
While pixel-based XAI methods focus on identifying significant pixels, existing
concept-based XAI methods use pre-defined or human-annotated concepts. The
recently proposed Segment Anything Model (SAM) achieved a significant step
forward to prepare automatic concept sets via comprehensive instance
segmentation. Building upon this, the Explain Any Concept (EAC) model emerged
as a flexible method for explaining DNN decisions. EAC model is based on using
a surrogate model which has one trainable linear layer to simulate the target
model. In this paper, by introducing an additional nonlinear layer to the
original surrogate model, we show that we can improve the performance of the
EAC model. We compare our proposed approach to the original EAC model and
report improvements obtained on both ImageNet and MS COCO datasets.

摘要：在可解釋 AI (XAI) 的演進領域中，解釋電腦視覺任務中深度神經網路 (DNN) 的決策是一個重要的過程。雖然基於像素的 XAI 方法著重於識別顯著像素，但現有的基於概念的 XAI 方法使用預先定義或人工標註的概念。最近提出的 Segment Anything Model (SAM) 在透過全面的實例分割準備自動概念集方面取得了顯著的進展。在此基礎上，Explain Any Concept (EAC) 模型成為解釋 DNN 決策的靈活方法。EAC 模型基於使用一個可訓練線性層來模擬目標模型的代理模型。在本文中，透過在原始代理模型中引入一個額外的非線性層，我們展示了我們可以改善 EAC 模型的效能。我們將我們提出的方法與原始 EAC 模型進行比較，並報告在 ImageNet 和 MS COCO 資料集上獲得的改進。

##### **Beyond MLE: Investigating SEARNN for Low-Resourced Neural Machine Translation**
2405.11819v1 by Chris Emezue

Structured prediction tasks, like machine translation, involve learning
functions that map structured inputs to structured outputs. Recurrent Neural
Networks (RNNs) have historically been a popular choice for such tasks,
including in natural language processing (NLP) applications. However, training
RNNs using Maximum Likelihood Estimation (MLE) has its limitations, including
exposure bias and a mismatch between training and testing metrics. SEARNN,
based on the learning to search (L2S) framework, has been proposed as an
alternative to MLE for RNN training. This project explored the potential of
SEARNN to improve machine translation for low-resourced African languages -- a
challenging task characterized by limited training data availability and the
morphological complexity of the languages. Through experiments conducted on
translation for English to Igbo, French to \ewe, and French to \ghomala
directions, this project evaluated the efficacy of SEARNN over MLE in
addressing the unique challenges posed by these languages. With an average BLEU
score improvement of $5.4$\% over the MLE objective, we proved that SEARNN is
indeed a viable algorithm to effectively train RNNs on machine translation for
low-resourced languages.

摘要：結構化預測任務，如機器翻譯，涉及學習將結構化輸入映射到結構化輸出的函數。遞迴神經網路 (RNN) 歷來一直是此類任務的熱門選擇，包括在自然語言處理 (NLP) 應用中。然而，使用最大似然估計 (MLE) 訓練 RNN 有其局限性，包括暴露偏差以及訓練和測試指標之間的不匹配。基於學習搜尋 (L2S) 框架的 SEARNN 已被提議作為 RNN 訓練的 MLE 替代方案。這個專案探討了 SEARNN 在改善低資源非洲語言機器翻譯的潛力——這是一項具有訓練資料可用性有限和語言形態複雜性特徵的挑戰性任務。透過對英語翻譯成伊博語、法語翻譯成埃維語和法語翻譯成戈馬拉語方向進行的實驗，這個專案評估了 SEARNN 在解決這些語言帶來的獨特挑戰方面優於 MLE 的效能。透過在 MLE 目標上平均 BLEU 分數提高 5.4%，我們證明了 SEARNN 確實是有效訓練 RNN 以進行低資源語言機器翻譯的可行演算法。

##### **Systematic Review on Healthcare Systems Engineering utilizing ChatGPT**
2405.11817v1 by Jungwoo Kim, Ji-Su Lee, Huijae Kim, Taesik Lee

This paper presents an analytical framework for conducting academic reviews
in the field of Healthcare Systems Engineering, employing ChatGPT, a
state-of-the-art tool among recent language models. We utilized 9,809 abstract
paragraphs from conference presentations to systematically review the field.
The framework comprises distinct analytical processes, each employing tailored
prompts and the systematic use of the ChatGPT API. Through this framework, we
organized the target field into 11 topic categories and conducted a
comprehensive analysis covering quantitative yearly trends and detailed
sub-categories. This effort explores the potential for leveraging ChatGPT to
alleviate the burden of academic reviews. Furthermore, it provides valuable
insights into the dynamic landscape of Healthcare Systems Engineering research.

摘要：本文提出了一個分析框架，用於在醫療系統工程領域進行學術審查，採用了 ChatGPT，這是一個近期語言模型中最先進的工具。我們利用了 9,809 篇來自會議簡報的摘要段落，對該領域進行了系統性審查。該框架包含不同的分析流程，每個流程都採用了量身定制的提示和系統化地使用 ChatGPT API。通過這個框架，我們將目標領域組織成 11 個主題類別，並進行了涵蓋數量化年度趨勢和詳細子類別的全面分析。這項工作探索了利用 ChatGPT 減輕學術審查負擔的潛力。此外，它還提供了對醫療系統工程研究的動態格局的寶貴見解。

##### **Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning**
2405.11816v1 by Anastasios Foliadis, Mario H. Castañeda, Richard A. Stirling-Gallacher, Reiner S. Thomä

Utilizing deep learning (DL) techniques for radio-based positioning of user
equipment (UE) through channel state information (CSI) fingerprints has
demonstrated significant potential. DL models can extract complex
characteristics from the CSI fingerprints of a particular environment and
accurately predict the position of a UE. Nonetheless, the effectiveness of the
DL model trained on CSI fingerprints is highly dependent on the particular
training environment, limiting the trained model's applicability across
different environments. This paper proposes a novel DL model structure
consisting of two parts, where the first part aims at identifying features that
are independent from any specific environment, while the second part combines
those features in an environment specific way with the goal of positioning. To
train such a two-part model, we propose the multi-environment meta-learning
(MEML) approach for the first part to facilitate training across various
environments, while the second part of the model is trained solely on data from
a specific environment. Our findings indicate that employing the MEML approach
for initializing the weights of the DL model for a new unseen environment
significantly boosts the accuracy of UE positioning in the new target
environment as well the reliability of its uncertainty estimation. This method
outperforms traditional transfer learning methods, whether direct transfer
learning (DTL) between environments or completely training from scratch with
data from a new environment. The proposed approach is verified with real
measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.

摘要：利用深度學習（DL）技術透過信道狀態資訊（CSI）指紋進行無線電定位的使用者端設備（UE），已展現出顯著的潛力。DL 模型可以從特定環境的 CSI 指紋中萃取出複雜的特徵，並準確預測 UE 的位置。儘管如此，在 CSI 指紋上訓練的 DL 模型的有效性高度依賴於特定的訓練環境，這限制了訓練模型在不同環境中的適用性。本文提出了一個新穎的 DL 模型結構，由兩部分組成，其中第一部分旨在識別獨立於任何特定環境的特徵，而第二部分則以環境特定方式結合這些特徵，以定位為目標。為了訓練這樣的兩部分模型，我們提出了多環境元學習（MEML）方法，用於第一部分，以促進在各種環境中進行訓練，而模型的第二部分僅在來自特定環境的資料上進行訓練。我們的研究結果表明，採用 MEML 方法來初始化 DL 模型的權重，以適應新的未知環境，顯著提高了在新的目標環境中 UE 定位的準確性，以及其不確定性估計的可靠性。此方法優於傳統的遷移學習方法，無論是在環境之間的直接遷移學習（DTL），或完全從新環境的資料中從頭開始訓練。所提出的方法已透過實際測量，驗證了適用於視線（LOS）和非視線（NLOS）環境。

##### **Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling**
2405.11814v1 by Masato Sakai, Marcus Freitag, Akihisa Sakurai, Conrad M Albrecht, Hendrik F Hamann

Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru
is urgent as natural and human impact accelerates. More frequent weather
extremes such as flashfloods threaten Nasca artifacts. We demonstrate that
runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data
can highlight AI-detected geoglyphs that are in danger of erosion. We recommend
measures of mitigation to protect the famous "lizard", "tree", and "hand"
geoglyphs located close by, or even cut by the Pan-American Highway.

摘要：秘魯聯合國教科文組織世界遺產納斯卡巨型地畫的保護刻不容緩，因為自然和人為影響正在加速。更頻繁的極端天氣，例如山洪暴發，威脅著納斯卡文物。我們證明，基於（次）公尺級別、LiDAR 衍生的數位高程資料的徑流模型可以突顯出 AI 偵測到的、有侵蝕危險的巨型地畫。我們建議採取緩解措施，保護位於附近、甚至被泛美公路切斷的著名的「蜥蜴」、「樹」和「手」巨型地畫。

##### **Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices**
2405.11809v1 by Baiyu Pan, Jichao Jiao, Jianxing Pang, Jun Cheng

In recent years, numerous real-time stereo matching methods have been
introduced, but they often lack accuracy. These methods attempt to improve
accuracy by introducing new modules or integrating traditional methods.
However, the improvements are only modest. In this paper, we propose a novel
strategy by incorporating knowledge distillation and model pruning to overcome
the inherent trade-off between speed and accuracy. As a result, we obtained a
model that maintains real-time performance while delivering high accuracy on
edge devices. Our proposed method involves three key steps. Firstly, we review
state-of-the-art methods and design our lightweight model by removing redundant
modules from those efficient models through a comparison of their
contributions. Next, we leverage the efficient model as the teacher to distill
knowledge into the lightweight model. Finally, we systematically prune the
lightweight model to obtain the final model. Through extensive experiments
conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform
ablation studies to analyze the effectiveness of each module and present our
state-of-the-art results.

摘要：近年來，雖然有許多即時立體匹配方法被提出，但往往缺乏準確性。這些方法嘗試透過引入新的模組或整合傳統方法來提升準確性，然而提升幅度有限。在本文中，我們提出一個新的策略，結合知識蒸餾和模型剪枝，以克服速度和準確性之間的固有權衡。因此，我們獲得了一個在邊緣裝置上維持即時效能，同時提供高準確性的模型。我們提出的方法包含三個關鍵步驟。首先，我們回顧最先進的方法，並透過比較其貢獻，從那些有效率的模型中移除多餘的模組來設計我們的輕量化模型。接下來，我們利用有效率的模型作為教師，將知識蒸餾到輕量化模型中。最後，我們系統性地剪枝輕量化模型，以獲得最終模型。透過在兩個廣泛使用的基準 Sceneflow 和 KITTI 上進行廣泛的實驗，我們執行消融研究，以分析每個模組的有效性，並展示我們最先進的成果。

##### **(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts**
2405.11804v1 by Minghao Wu, Yulin Yuan, Gholamreza Haffari, Longyue Wang

Recent advancements in machine translation (MT) have significantly enhanced
translation quality across various domains. However, the translation of
literary texts remains a formidable challenge due to their complex language,
figurative expressions, and cultural nuances. In this work, we introduce a
novel multi-agent framework based on large language models (LLMs) for literary
translation, implemented as a company called TransAgents, which mirrors
traditional translation publication process by leveraging the collective
capabilities of multiple agents, to address the intricate demands of
translating literary works. To evaluate the effectiveness of our system, we
propose two innovative evaluation strategies: Monolingual Human Preference
(MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the
perspective of monolingual readers of the target language, while BLP uses
advanced LLMs to compare translations directly with the original texts.
Empirical findings indicate that despite lower d-BLEU scores, translations from
TransAgents are preferred by both human evaluators and LLMs over human-written
references, particularly in genres requiring domain-specific knowledge. We also
highlight the strengths and limitations of TransAgents through case studies and
suggests directions for future research.

摘要：機器翻譯 (MT) 近期的進展大幅提升了各種領域的翻譯品質。然而，文學文本的翻譯由於其複雜的語言、比喻表達和文化細微差別，仍然是一項艱鉅的挑戰。在這項工作中，我們介紹了一個基於大型語言模型 (LLM) 的創新多重代理架構，用於文學翻譯，並實作為一家名為 TransAgents 的公司，透過利用多個代理的集體能力，模擬傳統的翻譯出版流程，以滿足翻譯文學作品的複雜需求。為了評估我們系統的有效性，我們提出了兩種創新的評估策略：單語人類偏好 (MHP) 和雙語 LLM 偏好 (BLP)。MHP 從目標語言的單語讀者的角度評估翻譯，而 BLP 使用進階 LLM 直接將翻譯與原始文本進行比較。實證結果顯示，儘管 d-BLEU 分數較低，但 TransAgents 的翻譯在需要特定領域知識的類型中，特別受到人類評估者和 LLM 的青睞，優於人類撰寫的參考。我們也透過案例研究強調 TransAgents 的優點和限制，並建議未來的研究方向。

##### **Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines**
2405.11800v1 by Yueqiao Jin, Lixiang Yan, Vanessa Echeverria, Dragan Gašević, Roberto Martinez-Maldonado

Integrating generative AI (GAI) into higher education is crucial for
preparing a future generation of GAI-literate students. Yet a thorough
understanding of the global institutional adoption policy remains absent, with
most of the prior studies focused on the Global North and the promises and
challenges of GAI, lacking a theoretical lens. This study utilizes the
Diffusion of Innovations Theory to examine GAI adoption strategies in higher
education across 40 universities from six global regions. It explores the
characteristics of GAI innovation, including compatibility, trialability, and
observability, and analyses the communication channels and roles and
responsibilities outlined in university policies and guidelines. The findings
reveal a proactive approach by universities towards GAI integration,
emphasizing academic integrity, teaching and learning enhancement, and equity.
Despite a cautious yet optimistic stance, a comprehensive policy framework is
needed to evaluate the impacts of GAI integration and establish effective
communication strategies that foster broader stakeholder engagement. The study
highlights the importance of clear roles and responsibilities among faculty,
students, and administrators for successful GAI integration, supporting a
collaborative model for navigating the complexities of GAI in education. This
study contributes insights for policymakers in crafting detailed strategies for
its integration.

摘要：將生成式 AI（GAI）整合到高等教育中，對於培養未來一代具備 GAI 素養的學生至關重要。然而，對於全球機構採用政策的透徹了解仍付之闕如，大多數先前的研究都集中在全球北方以及 GAI 的承諾與挑戰上，缺乏理論觀點。本研究利用創新擴散理論來探討 40 所來自六個全球區域的大學中 GAI 採用策略。它探討了 GAI 創新的特徵，包括相容性、試驗性和可觀察性，並分析了大學政策和指南中概述的溝通管道、角色和責任。研究結果揭示了大學對 GAI 整合採取積極主動的方法，強調學術誠信、教學與學習的提升以及公平性。儘管採取謹慎但樂觀的立場，仍需要一個全面的政策框架來評估 GAI 整合的影響，並建立有效的溝通策略，以促進更廣泛的利害關係人參與。該研究強調了教職員、學生和管理人員在角色和責任明確的情況下，對於成功整合 GAI 的重要性，並支持一種協作模式，以應對 GAI 在教育中的複雜性。本研究為政策制定者提供了見解，以便制定詳細的整合策略。

##### **Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing**
2405.11783v1 by Shinyoung Kang, Jihan Kim

In this study, we explore the potential of using quantum natural language
processing (QNLP) to inverse design metal-organic frameworks (MOFs) with
targeted properties. Specifically, by analyzing 150 hypothetical MOF structures
consisting of 10 metal nodes and 15 organic ligands, we categorize these
structures into four distinct classes for pore volume and $H_{2}$ uptake
values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat
(Distributional Compositional Categorical), and sequence-based models) to
identify the most effective approach to process the MOF dataset. Using a
classical simulator provided by the IBM Qiskit, the bag-of-words model is
identified to be the optimum model, achieving validation accuracies of 85.7%
and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake,
respectively. Further, we developed multi-class classification models tailored
to the probabilistic nature of quantum circuits, with average test accuracies
of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake
datasets. Finally, the performance of generating MOF with target properties
showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake,
respectively. Although our investigation covers only a fraction of the vast MOF
search space, it marks a promising first step towards using quantum computing
for materials design, offering a new perspective through which to explore the
complex landscape of MOFs.

摘要：<paragraph>在本研究中，我們探討了使用量子自然語言處理 (QNLP) 來反向設計具有目標特性的金屬有機骨架 (MOF) 的可能性。具體來說，通過分析由 10 個金屬節點和 15 個有機配體組成的 150 個假設的 MOF 結構，我們將這些結構分類為四個不同的類別，用於孔隙體積和 $H_{2}$ 吸收值。然後，我們比較各種 QNLP 模型（即詞袋、DisCoCat（分佈式組合範疇）和基於序列的模型），以確定處理 MOF 數據集的最有效方法。使用 IBM Qiskit 提供的經典模擬器，詞袋模型被確定為最佳模型，在孔隙體積和 $H_{2}$ 吸收的二元分類任務中分別實現了 85.7% 和 86.7% 的驗證準確度。此外，我們開發了針對量子電路概率特性的多類分類模型，在孔隙體積和 $H_{2}$ 吸收數據集的不同類別中，平均測試準確度分別為 88.4% 和 80.7%。最後，生成具有目標特性的 MOF 的性能顯示，孔隙體積的準確度為 93.5%，而 $H_{2}$ 吸收的準確度為 89%。儘管我們的調查僅涵蓋了廣闊的 MOF 搜索空間的一小部分，但它標誌著朝著使用量子計算進行材料設計邁出的有希望的第一步，提供了一個新的視角來探索 MOF 的複雜領域。</paragraph>

##### **Efficient Multi-agent Reinforcement Learning by Planning**
2405.11778v1 by Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang

Multi-agent reinforcement learning (MARL) algorithms have accomplished
remarkable breakthroughs in solving large-scale decision-making tasks.
Nonetheless, most existing MARL algorithms are model-free, limiting sample
efficiency and hindering their applicability in more challenging scenarios. In
contrast, model-based reinforcement learning (MBRL), particularly algorithms
integrating planning, such as MuZero, has demonstrated superhuman performance
with limited data in many tasks. Hence, we aim to boost the sample efficiency
of MARL by adopting model-based approaches. However, incorporating planning and
search methods into multi-agent systems poses significant challenges. The
expansive action space of multi-agent systems often necessitates leveraging the
nearly-independent property of agents to accelerate learning. To tackle this
issue, we propose the MAZero algorithm, which combines a centralized model with
Monte Carlo Tree Search (MCTS) for policy search. We design a novel network
structure to facilitate distributed execution and parameter sharing. To enhance
search efficiency in deterministic environments with sizable action spaces, we
introduce two novel techniques: Optimistic Search Lambda (OS($\lambda$)) and
Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the
SMAC benchmark demonstrate that MAZero outperforms model-free approaches in
terms of sample efficiency and provides comparable or better performance than
existing model-based methods in terms of both sample and computational
efficiency. Our code is available at https://github.com/liuqh16/MAZero.

摘要：多智能體強化學習 (MARL) 演算法在解決大規模決策制定任務方面取得了顯著的突破。儘管如此，現有的 MARL 演算法大多是無模型的，限制了樣本效率，並阻礙了它們在更具挑戰性的場景中的應用。相比之下，基於模型的強化學習 (MBRL)，特別是整合規劃的演算法，例如 MuZero，已經在許多任務中以有限的數據展示出超人的表現。因此，我們旨在通過採用基於模型的方法來提高 MARL 的樣本效率。然而，將規劃和搜尋方法整合到多智能體系統中會帶來重大的挑戰。多智能體系統的廣泛動作空間通常需要利用智能體的近乎獨立屬性來加速學習。為了解決這個問題，我們提出了 MAZero 演算法，它結合了集中式模型與蒙地卡羅樹搜尋 (MCTS) 進行策略搜尋。我們設計了一種新穎的網路結構，以促進分散式執行和參數共享。為了提高具有相當大動作空間的確定性環境中的搜尋效率，我們引入了兩種新技術：樂觀搜尋 Lambda (OS($\lambda$)) 和優勢加權策略最佳化 (AWPO)。在 SMAC 基準上的大量實驗表明，MAZero 在樣本效率方面優於無模型方法，並且在樣本和計算效率方面提供了與現有的基於模型的方法相當或更好的效能。我們的程式碼可在 https://github.com/liuqh16/MAZero 獲得。

##### **Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques**
2405.11775v1 by Siva Rajesh Kasa, Aniket Goel, Karan Gupta, Sumegh Roychowdhury, Anish Bhanushali, Nikhil Pattisapu, Prasanna Srinivasa Murthy

Ordinal Classification (OC) is a widely encountered challenge in Natural
Language Processing (NLP), with applications in various domains such as
sentiment analysis, rating prediction, and more. Previous approaches to tackle
OC have primarily focused on modifying existing or creating novel loss
functions that \textbf{explicitly} account for the ordinal nature of labels.
However, with the advent of Pretrained Language Models (PLMs), it became
possible to tackle ordinality through the \textbf{implicit} semantics of the
labels as well. This paper provides a comprehensive theoretical and empirical
examination of both these approaches. Furthermore, we also offer strategic
recommendations regarding the most effective approach to adopt based on
specific settings.

摘要：序數分類 (OC) 是自然語言處理 (NLP) 中廣泛遇到的挑戰，在情緒分析、評分預測等各種領域都有應用。處理 OC 的先前方法主要集中在修改現有損失函數或建立新的損失函數，以**明確**考慮標籤的序數性質。然而，隨著預訓練語言模型 (PLM) 的出現，也可以透過標籤的**隱含**語義來處理序數性。本文對這兩種方法提供了全面的理論和實證檢驗。此外，我們還根據特定設定提供了關於採用最有效方法的策略性建議。

##### **From SHAP Scores to Feature Importance Scores**
2405.11766v1 by Olivier Letoffe, Xuanxiang Huang, Nicholas Asher, Joao Marques-Silva

A central goal of eXplainable Artificial Intelligence (XAI) is to assign
relative importance to the features of a Machine Learning (ML) model given some
prediction. The importance of this task of explainability by feature
attribution is illustrated by the ubiquitous recent use of tools such as SHAP
and LIME. Unfortunately, the exact computation of feature attributions, using
the game-theoretical foundation underlying SHAP and LIME, can yield manifestly
unsatisfactory results, that tantamount to reporting misleading relative
feature importance. Recent work targeted rigorous feature attribution, by
studying axiomatic aggregations of features based on logic-based definitions of
explanations by feature selection. This paper shows that there is an essential
relationship between feature attribution and a priori voting power, and that
those recently proposed axiomatic aggregations represent a few instantiations
of the range of power indices studied in the past. Furthermore, it remains
unclear how some of the most widely used power indices might be exploited as
feature importance scores (FISs), i.e. the use of power indices in XAI, and
which of these indices would be the best suited for the purposes of XAI by
feature attribution, namely in terms of not producing results that could be
deemed as unsatisfactory. This paper proposes novel desirable properties that
FISs should exhibit. In addition, the paper also proposes novel FISs exhibiting
the proposed properties. Finally, the paper conducts a rigorous analysis of the
best-known power indices in terms of the proposed properties.

摘要：可解釋人工智慧 (XAI) 的主要目標是針對機器學習 (ML) 模型的特徵，給予相對的重要性，並給出一些預測。特徵歸屬說明了此可解釋性任務的重要性，這從近期廣泛使用的工具（例如 SHAP 和 LIME）中可見一斑。遺憾的是，使用 SHAP 和 LIME 背後博弈論基礎的精確特徵歸屬計算，可能會產生明顯不令人滿意的結果，等同於報告具有誤導性的相對特徵重要性。近期針對嚴謹的特徵歸屬進行研究，透過基於特徵選擇的邏輯定義，研究特徵的公理聚合來進行說明。本文顯示，特徵歸屬與先驗投票權之間存在著必要的關係，而近期提出的公理聚合，代表過去所研究的權力指數範圍中的一些實例。此外，目前仍不清楚一些最廣泛使用的權力指數，如何能被用作特徵重要性分數 (FIS)，亦即在 XAI 中使用權力指數，以及在特徵歸屬的 XAI 目的方面，哪一個指數最適合，也就是說，在不產生可能被視為不令人滿意的結果方面。本文提出了 FIS 應具備的新穎理想特性。此外，本文也提出了具備所提出特性的新穎 FIS。最後，本文針對最知名的權力指數，根據所提出的特性進行嚴謹的分析。

##### **Fed-Credit: Robust Federated Learning with Credibility Management**
2405.11758v1 by Jiayan Chen, Zhirong Qian, Tianhui Meng, Xitong Gao, Tian Wang, Weijia Jia

Aiming at privacy preservation, Federated Learning (FL) is an emerging
machine learning approach enabling model training on decentralized devices or
data sources. The learning mechanism of FL relies on aggregating parameter
updates from individual clients. However, this process may pose a potential
security risk due to the presence of malicious devices. Existing solutions are
either costly due to the use of compute-intensive technology, or restrictive
for reasons of strong assumptions such as the prior knowledge of the number of
attackers and how they attack. Few methods consider both privacy constraints
and uncertain attack scenarios. In this paper, we propose a robust FL approach
based on the credibility management scheme, called Fed-Credit. Unlike previous
studies, our approach does not require prior knowledge of the nodes and the
data distribution. It maintains and employs a credibility set, which weighs the
historical clients' contributions based on the similarity between the local
models and global model, to adjust the global model update. The subtlety of
Fed-Credit is that the time decay and attitudinal value factor are incorporated
into the dynamic adjustment of the reputation weights and it boasts a
computational complexity of O(n) (n is the number of the clients). We conducted
extensive experiments on the MNIST and CIFAR-10 datasets under 5 types of
attacks. The results exhibit superior accuracy and resilience against
adversarial attacks, all while maintaining comparatively low computational
complexity. Among these, on the Non-IID CIFAR-10 dataset, our algorithm
exhibited performance enhancements of 19.5% and 14.5%, respectively, in
comparison to the state-of-the-art algorithm when dealing with two types of
data poisoning attacks.

摘要：<paragraph>為了保護隱私，聯合學習 (FL) 是一種新興機器學習方法，可針對分散式裝置或資料來源進行模型訓練。FL 的學習機制仰賴彙總來自個別用戶端的參數更新。然而，此程序可能會因為惡意裝置而構成潛在安全風險。現有解決方案不是因為使用運算密集技術而成本高昂，就是因為過於依賴強假設（例如事先知道攻擊者數量及其攻擊方式）而受到限制。很少有方法同時考量隱私限制和不確定的攻擊情境。在本文中，我們提出了一種基於信譽管理機制的穩健 FL 方法，稱為 Fed-Credit。與先前的研究不同，我們的做法不需要事先了解節點和資料分佈。它維護並使用一個信譽集合，根據本地模型和全域模型之間的相似性對歷史用戶端的貢獻進行加權，以調整全域模型更新。Fed-Credit 的精妙之處在於，時間衰減和態度價值因子已納入信譽權重的動態調整中，並且其運算複雜度為 O(n)（n 為用戶端數量）。我們針對 MNIST 和 CIFAR-10 資料集進行了廣泛的實驗，涵蓋 5 種類型的攻擊。結果展現出優異的準確度和對抗攻擊的韌性，同時維持相對較低的運算複雜度。其中，在非 IID CIFAR-10 資料集上，我們的演算法在處理兩種資料中毒攻擊時，分別展現出 19.5% 和 14.5% 的效能提升，相較於最先進的演算法。</paragraph>

##### **Contactless Polysomnography: What Radio Waves Tell Us about Sleep**
2405.11739v1 by Hao He, Chao Li, Wolfgang Ganglberger, Kaileigh Gallagher, Rumen Hristov, Michail Ouroutzoglou, Haoqi Sun, Jimeng Sun, Brandon Westover, Dina Katabi

The ability to assess sleep at home, capture sleep stages, and detect the
occurrence of apnea (without on-body sensors) simply by analyzing the radio
waves bouncing off people's bodies while they sleep is quite powerful. Such a
capability would allow for longitudinal data collection in patients' homes,
informing our understanding of sleep and its interaction with various diseases
and their therapeutic responses, both in clinical trials and routine care. In
this article, we develop an advanced machine learning algorithm for passively
monitoring sleep and nocturnal breathing from radio waves reflected off people
while asleep. Validation results in comparison with the gold standard (i.e.,
polysomnography) (n=849) demonstrate that the model captures the sleep
hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake,
Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and
measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]).
Notably, the model exhibits equitable performance across race, sex, and age.
Moreover, the model uncovers informative interactions between sleep stages and
a range of diseases including neurological, psychiatric, cardiovascular, and
immunological disorders. These findings not only hold promise for clinical
practice and interventional trials but also underscore the significance of
sleep as a fundamental component in understanding and managing various
diseases.

摘要：<paragraph>僅透過分析人們睡覺時從身體反射的無線電波，就能評估居家睡眠、捕捉睡眠階段並偵測呼吸中止（無需配戴身體感測器）的能力非常強大。這種能力能讓患者在家中進行縱向資料收集，有助於我們了解睡眠及其與各種疾病的交互作用，以及在臨床試驗和例行照護中的治療反應。在本文中，我們開發了一種先進的機器學習演算法，可被動監控人們睡覺時從身體反射的無線電波中的睡眠和夜間呼吸。與黃金標準（即多重睡眠生理檢查）（n=849）相比的驗證結果顯示，該模型捕捉到了睡眠腦波圖（30 秒時間區段分類為清醒、淺眠、深眠或快速動眼期，準確度為 81%），可偵測睡眠呼吸中止（AUROC = 0.88），並測量患者的呼吸中止低通氣指數（ICC=0.95；95% CI = [0.93, 0.97]）。值得注意的是，該模型在種族、性別和年齡上表現出公平的效能。此外，該模型揭示了睡眠階段與神經、精神、心血管和免疫疾病等一系列疾病之間的資訊性交互作用。這些發現不僅對臨床實務和介入試驗有望，也強調了睡眠作為理解和管理各種疾病的基本組成部分的重要性。</paragraph>

##### **Token-wise Influential Training Data Retrieval for Large Language Models**
2405.11724v1 by Huawei Lin, Jikai Long, Zhaozhuo Xu, Weijie Zhao

Given a Large Language Model (LLM) generation, how can we identify which
training data led to this generation? In this paper, we proposed RapidIn, a
scalable framework adapting to LLMs for estimating the influence of each
training data. The proposed framework consists of two stages: caching and
retrieval. First, we compress the gradient vectors by over 200,000x, allowing
them to be cached on disk or in GPU/CPU memory. Then, given a generation,
RapidIn efficiently traverses the cached gradients to estimate the influence
within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports
multi-GPU parallelization to substantially accelerate caching and retrieval.
Our empirical result confirms the efficiency and effectiveness of RapidIn.

摘要：給定一個大型語言模型 (LLM) 生成，我們如何識別哪些訓練資料導致這個生成？在本文中，我們提出 RapidIn，一個可擴充的框架，適用於 LLM 以估計每個訓練資料的影響。所提出的框架包含兩個階段：快取和檢索。首先，我們將梯度向量壓縮超過 200,000 倍，允許它們快取在磁碟或 GPU/CPU 記憶體中。然後，給定一個生成，RapidIn 有效地遍歷快取的梯度，在幾分鐘內估計影響，達到超過 6,326 倍的加速。此外，RapidIn 支援多 GPU 並行處理，以大幅加速快取和檢索。我們的實證結果證實了 RapidIn 的效率和有效性。

##### **Semantic Trajectory Data Mining with LLM-Informed POI Classification**
2405.11715v1 by Yifan Liu, Chenchen Kuai, Haoxuan Ma, Xishun Liao, Brian Yueshuai He, Jiaqi Ma

Human travel trajectory mining is crucial for transportation systems,
enhancing route optimization, traffic management, and the study of human travel
patterns. Previous rule-based approaches without the integration of semantic
information show a limitation in both efficiency and accuracy. Semantic
information, such as activity types inferred from Points of Interest (POI)
data, can significantly enhance the quality of trajectory mining. However,
integrating these insights is challenging, as many POIs have incomplete feature
information, and current learning-based POI algorithms require the integrity of
datasets to do the classification. In this paper, we introduce a novel pipeline
for human travel trajectory mining. Our approach first leverages the strong
inferential and comprehension capabilities of large language models (LLMs) to
annotate POI with activity types and then uses a Bayesian-based algorithm to
infer activity for each stay point in a trajectory. In our evaluation using the
OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a
96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1
score in activity inference.

摘要：人類旅遊軌跡挖掘對於交通系統至關重要，能強化路線最佳化、交通管理以及人類旅遊模式的研究。先前的基於規則方法在沒有整合語義資訊的情況下，在效率和準確性方面都顯示出限制。語義資訊，例如從興趣點 (POI) 資料推論出的活動類型，可以顯著提升軌跡挖掘的品質。然而，整合這些見解具有挑戰性，因為許多 POI 具有不完整的特徵資訊，而且當前的基於學習的 POI 演算法需要資料集的完整性才能進行分類。在本文中，我們提出了一個用於人類旅遊軌跡挖掘的新穎管線。我們的做法首先利用大型語言模型 (LLM) 強大的推論和理解能力，以活動類型註解 POI，然後使用基於貝氏的演算法推論軌跡中每個停留點的活動。在我們使用 OpenStreetMap (OSM) POI 資料集進行評估時，我們的做法在 POI 分類中達到 93.4% 的準確度和 96.1% 的 F-1 分數，在活動推論中達到 91.7% 的準確度和 92.3% 的 F-1 分數。

##### **OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework**
2405.11143v1 by Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, Yu Cao

As large language models (LLMs) continue to grow by scaling laws,
reinforcement learning from human feedback (RLHF) has gained significant
attention due to its outstanding performance. However, unlike pretraining or
fine-tuning a single model, scaling reinforcement learning from human feedback
(RLHF) for training large language models poses coordination challenges across
four models. We present OpenRLHF, an open-source framework enabling efficient
RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the
same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters
using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and
diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF
provides an out-of-the-box solution with optimized algorithms and launch
scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,
rejection sampling, and other alignment techniques. Empowering state-of-the-art
LLM development, OpenRLHF's code is available at
https://github.com/OpenLLMAI/OpenRLHF.

摘要：隨著大型語言模型 (LLM) 繼續透過規模法則成長，
從人類回饋中進行強化學習 (RLHF) 因為其傑出的表現而受到廣泛關注。
然而，與預訓練或微調單一模型不同，針對大型語言模型訓練而從人類回饋中進行強化學習 (RLHF) 會在四個模型間造成協調上的挑戰。我們提出 OpenRLHF，一個開放原始碼架構，可實現高效能的 RLHF 擴充。與將四個模型共置於同一 GPU 上的現有 RLHF 架構不同，OpenRLHF 透過 Ray、vLLM 和 DeepSpeed 重新設計超過 70B 參數的模型排程，善用改善的資源利用率和多樣化的訓練方法。OpenRLHF 與 Hugging Face 無縫整合，提供開箱即用的解決方案，包含最佳化演算法和啟動指令碼，確保使用者友善。OpenRLHF 實作 RLHF、DPO、拒絕採樣和其他對齊技術。OpenRLHF 的程式碼賦能最先進的 LLM 開發，可於 https://github.com/OpenLLMAI/OpenRLHF 取得。

##### **Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!**
2405.11706v1 by Dean Allemang, Juan Sequeda

There is increasing evidence that question-answering (QA) systems with Large
Language Models (LLMs), which employ a knowledge graph/semantic representation
of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy
compared to systems that answer questions directly on SQL databases (i.e.
Text-to-SQL). Our previous benchmark research showed that by using a knowledge
graph, the accuracy improved from 16% to 54%. The question remains: how can we
further improve the accuracy and reduce the error rate? Building on the
observations of our previous research where the inaccurate LLM-generated SPARQL
queries followed incorrect paths, we present an approach that consists of 1)
Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of
the knowledge graph to check if the LLM-generated SPARQL query matches the
semantic of ontology and 2) LLM Repair: use the error explanations with an LLM
to repair the SPARQL query. Using the chat with the data benchmark, our primary
finding is that our approach increases the overall accuracy to 72% including an
additional 8% of "I don't know" unknown results. Thus, the overall error rate
is 20%. These results provide further evidence that investing knowledge graphs,
namely the ontology, provides higher accuracy for LLM powered question
answering systems.

摘要：有越來越多的證據顯示，使用大型語言模型 (LLM) 的問答 (QA) 系統，它採用企業 SQL 資料庫的知識圖譜/語義表示（即 Text-to-SPARQL），與直接在 SQL 資料庫上回答問題的系統（即 Text-to-SQL）相比，能達到更高的準確度。我們先前的基準研究顯示，透過使用知識圖譜，準確度從 16% 提升至 54%。問題仍然存在：我們如何進一步提升準確度並降低錯誤率？根據我們先前研究的觀察，其中不準確的 LLM 生成的 SPARQL 查詢遵循不正確的路徑，我們提出了一種方法，其中包含 1) 基於本体的查詢檢查 (OBQC)：利用知識圖譜的本体來檢查 LLM 生成的 SPARQL 查詢是否符合本体的語義，從而偵測錯誤，以及 2) LLM 修復：使用帶有 LLM 的錯誤說明來修復 SPARQL 查詢。使用與資料基準的聊天，我們的初步發現是，我們的做法將整體準確度提升至 72%，包括額外的 8% 的「我不知道」未知結果。因此，整體錯誤率為 20%。這些結果進一步證明投資知識圖譜，即本体，能為 LLM 驅動的問答系統提供更高的準確度。

##### **Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks**
2405.11704v1 by Taiyuan Mei, Yun Zi, Xiaohan Cheng, Zijun Gao, Qi Wang, Haowei Yang

The internal structure and operation mechanism of large-scale language models
are analyzed theoretically, especially how Transformer and its derivative
architectures can restrict computing efficiency while capturing long-term
dependencies. Further, we dig deep into the efficiency bottleneck of the
training phase, and evaluate in detail the contribution of adaptive
optimization algorithms (such as AdamW), massively parallel computing
techniques, and mixed precision training strategies to accelerate convergence
and reduce memory footprint. By analyzing the mathematical principles and
implementation details of these algorithms, we reveal how they effectively
improve training efficiency in practice. In terms of model deployment and
inference optimization, this paper systematically reviews the latest advances
in model compression techniques, focusing on strategies such as quantification,
pruning, and knowledge distillation. By comparing the theoretical frameworks of
these techniques and their effects in different application scenarios, we
demonstrate their ability to significantly reduce model size and inference
delay while maintaining model prediction accuracy. In addition, this paper
critically examines the limitations of current efficiency optimization methods,
such as the increased risk of overfitting, the control of performance loss
after compression, and the problem of algorithm generality, and proposes some
prospects for future research. In conclusion, this study provides a
comprehensive theoretical framework for understanding the efficiency
optimization of large-scale language models.

摘要：<paragraph>從理論層面分析大型語言模型的內部結構與運作機制，特別是探討 Transformer 及其衍生架構如何在捕捉長期依賴關係的同時，受限於運算效率。進一步深入探討訓練階段的效率瓶頸，並詳細評估自適應優化演算法（如 AdamW）、大規模平行運算技術與混合精度訓練策略在加速收斂與降低記憶體佔用量方面的貢獻。透過分析這些演算法的數學原理與實作細節，揭示其在實務上有效提升訓練效率的方式。在模型部署與推理優化方面，本文系統性地回顧模型壓縮技術的最新進展，重點探討量化、剪枝與知識蒸餾等策略。透過比較這些技術的理論架構及其在不同應用場景中的效果，驗證其在大幅降低模型大小與推理延遲的同時，仍能維持模型預測準確度。此外，本文批判性地審視當前效率優化方法的限制，例如過擬合風險的增加、壓縮後效能損失的控制以及演算法通用性的問題，並提出未來研究的一些展望。綜上所述，本研究提供了一個全面的理論架構，用於理解大型語言模型的效率最佳化。</paragraph>

##### **ColorFoil: Investigating Color Blindness in Large Vision and Language Models**
2405.11685v1 by Ahnaf Mozib Samin, M. Firoz Ahmed, Md. Mushtaq Shahriyar Rafee

With the utilization of Transformer architecture, large Vision and Language
(V&L) models have shown promising performance in even zero-shot settings.
Several studies, however, indicate a lack of robustness of the models when
dealing with complex linguistics and visual attributes. In this work, we
introduce a novel V&L benchmark - ColorFoil, by creating color-related foils to
assess the models' perception ability to detect colors like red, white, green,
etc. We evaluate seven state-of-the-art V&L models including CLIP, ViLT,
GroupViT, and BridgeTower, etc. in a zero-shot setting and present intriguing
findings from the V&L models. The experimental evaluation indicates that ViLT
and BridgeTower demonstrate much better color perception capabilities compared
to CLIP and its variants and GroupViT. Moreover, CLIP-based models and GroupViT
struggle to distinguish colors that are visually distinct to humans with normal
color perception ability.

摘要：透過 Transformer 架構的運用，大型視覺與語言 (V&L) 模型即使在零次學習的設定下，也展現出令人滿意的表現。然而，一些研究指出，當處理複雜的語言學和視覺屬性時，這些模型缺乏穩健性。在這項工作中，我們透過建立與顏色相關的誘餌，來評估模型偵測紅色、白色、綠色等顏色的感知能力，進而提出一個新穎的 V&L 基準測試 ColorFoil。我們在零次學習的設定下，評估了七個最先進的 V&L 模型，包括 CLIP、ViLT、GroupViT 和 BridgeTower 等，並提出 V&L 模型中引人入勝的發現。實驗評估指出，與 CLIP 及其變體和 GroupViT 相比，ViLT 和 BridgeTower 展現出更好的色彩感知能力。此外，基於 CLIP 的模型和 GroupViT 難以區分對具有正常色彩感知能力的人類而言在視覺上截然不同的顏色。

##### **Deep Ensemble Art Style Recognition**
2405.11675v1 by Orfeas Menis-Mastromichalakis, Natasa Sofou, Giorgos Stamou

The massive digitization of artworks during the last decades created the need
for categorization, analysis, and management of huge amounts of data related to
abstract concepts, highlighting a challenging problem in the field of computer
science. The rapid progress of artificial intelligence and neural networks has
provided tools and technologies that seem worthy of the challenge. Recognition
of various art features in artworks has gained attention in the deep learning
society. In this paper, we are concerned with the problem of art style
recognition using deep networks. We compare the performance of 8 different deep
architectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121,
DenseNet201 and Inception-ResNet-V2), on two different art datasets, including
3 architectures that have never been used on this task before, leading to
state-of-the-art performance. We study the effect of data preprocessing prior
to applying a deep learning model. We introduce a stacking ensemble method
combining the results of first-stage classifiers through a meta-classifier,
with the innovation of a versatile approach based on multiple models that
extract and recognize different characteristics of the input, creating a more
consistent model compared to existing works and achieving state-of-the-art
accuracy on the largest art dataset available (WikiArt - 68,55%). We also
discuss the impact of the data and art styles themselves on the performance of
our models forming a manifold perspective on the problem.

摘要：<paragraph>在過去幾十年中，藝術品的大量數位化產生了對抽象概念相關的大量資料進行分類、分析和管理的需求，突顯了電腦科學領域中一個具有挑戰性的問題。人工智慧和神經網路的快速進展提供了看似值得挑戰的工具和技術。在深度學習社群中，對藝術品中各種藝術特徵的辨識備受關注。在本文中，我們關注使用深度網路進行藝術風格辨識的問題。我們比較了 8 種不同的深度架構（VGG16、VGG19、ResNet50、ResNet152、Inception-V3、DenseNet121、DenseNet201 和 Inception-ResNet-V2）在兩個不同的藝術資料集上的效能，包括 3 種以前從未用於此任務的架構，從而達到最先進的效能。我們研究了在應用深度學習模型之前進行資料前處理的影響。我們引入了一種堆疊集成方法，通過元分類器將第一階段分類器的結果進行組合，並創新地採用基於多個模型的多功能方法，這些模型會萃取和辨識輸入的不同特徵，創造出比現有作品更一致的模型，並在最大的可用藝術資料集（WikiArt - 68,55%）上達到最先進的準確度。我們還討論了資料和藝術風格本身對我們模型效能的影響，從多面向的角度探討這個問題。</paragraph>

##### **On the Expressivity of Recurrent Neural Cascades with Identity**
2405.11657v1 by Nadezda A. Knorozova, Alessandro Ronca

Recurrent Neural Cascades (RNC) are the class of recurrent neural networks
with no cyclic dependencies among recurrent neurons. Their subclass RNC+ with
positive recurrent weights has been shown to be closely connected to the
star-free regular languages, which are the expressivity of many
well-established temporal logics. The existing expressivity results show that
the regular languages captured by RNC+ are the star-free ones, and they leave
open the possibility that RNC+ may capture languages beyond regular. We exclude
this possibility for languages that include an identity element, i.e., an input
that can occur an arbitrary number of times without affecting the output.
Namely, in the presence of an identity element, we show that the languages
captured by RNC+ are exactly the star-free regular languages. Identity elements
are ubiquitous in temporal patterns, and hence our results apply to a large
number of applications. The implications of our results go beyond expressivity.
At their core, we establish a close structural correspondence between RNC+ and
semiautomata cascades, showing that every neuron can be equivalently captured
by a three-state semiautomaton. A notable consequence of this result is that
RNC+ are no more succinct than cascades of three-state semiautomata.

摘要：遞迴神經串級 (RNC) 是一類遞迴神經網路，在遞迴神經元之間沒有循環依賴。其子類 RNC+ 具有正遞迴權重，已顯示與無星正則語言密切相關，而無星正則語言是許多既定時序邏輯的表達能力。現有的表達能力結果顯示 RNC+ 擷取的正則語言是無星語言，且它們留下了 RNC+ 可能擷取超越正則語言的可能性的可能性。我們排除了包含恆等元素的語言的可能性，亦即，一個輸入可以在不影響輸出的情況下出現任意次數。亦即，在恆等元素存在的情況下，我們顯示 RNC+ 擷取的語言正是無星正則語言。恆等元素在時序模式中無所不在，因此我們的結果適用於大量的應用程式。我們的結果的含意超越了表達能力。在它們的核心，我們建立了 RNC+ 與半自動串級之間緊密的結構對應，顯示每個神經元都可以等效地由三態半自動擷取。這個結果的一個顯著後果是，RNC+ 沒有比三態半自動的串級更簡潔。

##### **URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images**
2405.11656v1 by Zoey Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, Abhishek Gupta

Constructing simulation scenes that are both visually and physically
realistic is a problem of practical interest in domains ranging from robotics
to computer vision. This problem has become even more relevant as researchers
wielding large data-hungry learning methods seek new sources of training data
for physical decision-making systems. However, building simulation models is
often still done by hand. A graphic designer and a simulation engineer work
with predefined assets to construct rich scenes with realistic dynamic and
kinematic properties. While this may scale to small numbers of scenes, to
achieve the generalization properties that are required for data-driven robotic
control, we require a pipeline that is able to synthesize large numbers of
realistic scenes, complete with 'natural' kinematic and dynamic structures. To
attack this problem, we develop models for inferring structure and generating
simulation scenes from natural images, allowing for scalable scene generation
from web-scale datasets. To train these image-to-simulation models, we show how
controllable text-to-image generative models can be used in generating paired
training data that allows for modeling of the inverse problem, mapping from
realistic images back to complete scene models. We show how this paradigm
allows us to build large datasets of scenes in simulation with semantic and
physical realism. We present an integrated end-to-end pipeline that generates
simulation scenes complete with articulated kinematic and dynamic structures
from real-world images and use these for training robotic control policies. We
then robustly deploy in the real world for tasks like articulated object
manipulation. In doing so, our work provides both a pipeline for large-scale
generation of simulation environments and an integrated system for training
robust robotic control policies in the resulting environments.

摘要：<paragraph>構建視覺上和物理上皆逼真的模擬場景，是從機器人技術到電腦視覺等領域中，一個具有實際意義的問題。這個問題變得更加重要，因為使用大量資料學習方法的研究人員，正在尋找新的訓練資料來源，以進行物理決策系統的訓練。然而，建立模擬模型通常仍然是手動完成的。一位平面設計師和一位模擬工程師會利用預先定義的資產，來構建具有逼真的動態和運動特性的豐富場景。雖然這可能會擴展到少數場景，但為了實現資料驅動機器人控制所需的泛化特性，我們需要一個管道，能夠合成大量的逼真場景，並具備「自然」的運動和動態結構。為了解決這個問題，我們開發了用於推論結構和從自然影像產生模擬場景的模型，允許從網路規模的資料集進行可擴充的場景產生。為了訓練這些影像到模擬的模型，我們展示了可控文字到影像的生成模型，如何用於產生配對的訓練資料，允許對反向問題進行建模，將逼真的影像對應回完整的場景模型。我們展示了這個範例如何讓我們在模擬中建立具有語意和物理真實性的大型場景資料集。我們提出了一個整合式的端到端管道，從真實世界的影像產生完整的模擬場景，並具備關節運動和動態結構，並使用這些場景來訓練機器人控制策略。然後，我們在真實世界中穩健地部署，以執行關節物件操作等任務。在這樣做的過程中，我們的研究提供了用於大規模產生模擬環境的管道，以及一個整合系統，用於在產生的環境中訓練穩健的機器人控制策略。</paragraph>

##### **Track Anything Rapter(TAR)**
2405.11655v1 by Tharun V. Puthanveettil, Fnu Obaid ur Rahman

Object tracking is a fundamental task in computer vision with broad practical
applications across various domains, including traffic monitoring, robotics,
and autonomous vehicle tracking. In this project, we aim to develop a
sophisticated aerial vehicle system known as Track Anything Raptor (TAR),
designed to detect, segment, and track objects of interest based on
user-provided multimodal queries, such as text, images, and clicks. TAR
utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate
the relative pose of the queried object. The tracking problem is approached as
a Visual Servoing task, enabling the UAV to consistently focus on the object
through advanced motion planning and control algorithms. We showcase how the
integration of these foundational models with a custom high-level control
algorithm results in a highly stable and precise tracking system deployed on a
custom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking
algorithm's performance, we compare it against Vicon-based ground truth.
Additionally, we evaluate the reliability of the foundational models in aiding
tracking in scenarios involving occlusions. Finally, we test and validate the
model's ability to work seamlessly with multiple modalities, such as click,
bounding box, and image templates.

摘要：物體追蹤是電腦視覺中的一項基本任務，在包括交通監控、機器人和自動車輛追蹤等多種領域都有廣泛的實際應用。在這個專案中，我們的目標是開發一個稱為 Track Anything Raptor (TAR) 的先進空中載具系統，旨在根據使用者提供的多模態查詢（例如文字、影像和點擊）來偵測、分割和追蹤感興趣的物體。TAR 利用 DINO、CLIP 和 SAM 等先進的預先訓練模型來估計查詢物體的相對位姿。追蹤問題被視為一個視覺伺服任務，讓無人機能夠透過先進的運動規劃和控制演算法持續對焦在物體上。我們展示了這些基礎模型與自訂高階控制演算法整合後，如何產生一個部署在自訂 PX4 Autopilot 支援的 Voxl2 M500 無人機上的高度穩定且精確的追蹤系統。為了驗證追蹤演算法的效能，我們將其與基於 Vicon 的地面實況進行比較。此外，我們評估了基礎模型在協助追蹤遮擋情境中的可靠性。最後，我們測試並驗證了模型與多種模態（例如點擊、邊界框和影像範本）無縫運作的能力。

##### **Hummer: Towards Limited Competitive Preference Dataset**
2405.11647v2 by Li Jiang, Yusen Wu, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, Zujie Wen, Jun Zhou, Xiaotie Deng

Preference datasets are essential for incorporating human preferences into
pre-trained language models, playing a key role in the success of Reinforcement
Learning from Human Feedback. However, these datasets often demonstrate
conflicting alignment objectives, leading to increased vulnerability to
jailbreak attacks and challenges in adapting downstream tasks to prioritize
specific alignment objectives without negatively impacting others. In this
work, we introduce a novel statistical metric, Alignment Dimension Conflict, to
quantify the degree of conflict within preference datasets. We then present
\texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovative
pairwise preference datasets with reduced-conflict alignment objectives.
\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback
from GPT-4, marking as the first preference dataset aimed at reducing the
competition between alignment objectives. Furthermore, we develop reward
models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to
balance diverse alignment objectives effectively. This sampling method
positions HummerRM as an ideal model for domain-specific further fine-tuning
and reducing vulnerabilities to attacks.

摘要：偏好数据集对于将人类偏好纳入预先训练的语言模型至关重要，在人类反馈强化学习的成功中发挥着关键作用。然而，这些数据集通常表现出相互冲突的对齐目标，导致更容易受到越狱攻击，并且在调整下游任务以优先考虑特定对齐目标时面临挑战，而不会对其他目标产生负面影响。在这项工作中，我们引入了一种新颖的统计指标，对齐维度冲突，以量化偏好数据集中的冲突程度。然后，我们提出了 Hummer 及其细粒度变体 Hummer-F，作为具有减少冲突对齐目标的创新成对偏好数据集。Hummer 基于 UltraFeedback 构建，并通过 GPT-4 的 AI 反馈进行了增强，标志着第一个旨在减少对齐目标之间竞争的偏好数据集。此外，我们开发了奖励模型 HummerRM 和 HummerRM-F，它们采用混合采样方法来有效平衡不同的对齐目标。这种采样方法将 HummerRM 定位为特定领域进一步微调和减少攻击漏洞的理想模型。

##### **Inquire, Interact, and Integrate: A Proactive Agent Collaborative Framework for Zero-Shot Multimodal Medical Reasoning**
2405.11640v1 by Zishan Gu, Fenglin Liu, Changchang Yin, Ping Zhang

The adoption of large language models (LLMs) in healthcare has attracted
significant research interest. However, their performance in healthcare remains
under-investigated and potentially limited, due to i) they lack rich
domain-specific knowledge and medical reasoning skills; and ii) most
state-of-the-art LLMs are unimodal, text-only models that cannot directly
process multimodal inputs. To this end, we propose a multimodal medical
collaborative reasoning framework \textbf{MultiMedRes}, which incorporates a
learner agent to proactively gain essential information from domain-specific
expert models, to solve medical multimodal reasoning problems. Our method
includes three steps: i) \textbf{Inquire}: The learner agent first decomposes
given complex medical reasoning problems into multiple domain-specific
sub-problems; ii) \textbf{Interact}: The agent then interacts with
domain-specific expert models by repeating the ``ask-answer'' process to
progressively obtain different domain-specific knowledge; iii)
\textbf{Integrate}: The agent finally integrates all the acquired
domain-specific knowledge to accurately address the medical reasoning problem.
We validate the effectiveness of our method on the task of difference visual
question answering for X-ray images. The experiments demonstrate that our
zero-shot prediction achieves state-of-the-art performance, and even
outperforms the fully supervised methods. Besides, our approach can be
incorporated into various LLMs and multimodal LLMs to significantly boost their
performance.

摘要：大型語言模型（LLM）在醫療保健領域的採用引起了
顯著的研究興趣。然而，由於 i) 它們缺乏豐富的特定領域知識和醫療推理技能；以及 ii) 大多數
最先進的 LLM 是單模態、純文字模型，無法直接
處理多模態輸入，因此它們在醫療保健中的表現仍未得到充分研究，且潛在受限。為此，我們提出了一個多模態醫療
協作推理框架\textbf{MultiMedRes}，它包含一個學習代理，可主動從特定領域的
專家模型中獲取基本資訊，以解決醫療多模態推理問題。我們的
方法包括三個步驟：i) \textbf{詢問}：學習代理首先將
給定的複雜醫療推理問題分解為多個特定領域的
子問題；ii) \textbf{互動}：然後，代理透過重複``詢問-回答''程序與
特定領域的專家模型互動，以逐步獲取不同的特定領域知識；iii)
\textbf{整合}：最後，代理整合所有獲得的
特定領域知識，以準確解決醫療推理問題。
我們在 X 光影像的差異視覺問題解答任務上驗證了我們方法的有效性。實驗表明，我們的
零次學習預測達到了最先進的性能，甚至
優於完全監督的方法。此外，我們的
方法可以整合到各種 LLM 和多模態 LLM 中，以顯著提升其
性能。

##### **Zero-Shot Stance Detection using Contextual Data Generation with LLMs**
2405.11637v1 by Ghazaleh Mahmoudi, Babak Behkamkia, Sauleh Eetemadi

Stance detection, the classification of attitudes expressed in a text towards
a specific topic, is vital for applications like fake news detection and
opinion mining. However, the scarcity of labeled data remains a challenge for
this task. To address this problem, we propose Dynamic Model Adaptation with
Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and
Large Language Models. In this approach, we aim to fine-tune an existing model
at test time. We achieve this by generating new topic-specific data using
GPT-3. This method could enhance performance by allowing the adaptation of the
model to new topics. However, the results did not increase as we expected.
Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset,
which extends VAST using GPT-3. In this dataset, each context is associated
with multiple topics, allowing the model to understand the relationship between
contexts and various potential topics

摘要：立場偵測，將文本中對特定主題表達的態度進行分類，對於假新聞偵測和意見探勘等應用至關重要。然而，標記資料的稀少性仍然是這項任務的挑戰。為了解決這個問題，我們提出結合少量學習和大語言模型的動態模型適應與脈絡資料生成（DyMoAdapt）。在這種方法中，我們旨在在測試時微調現有模型。我們通過使用 GPT-3 生成新的主題特定資料來實現這一點。這種方法可以通過允許模型適應新主題來提高性能。然而，結果並未如我們預期的那樣增加。此外，我們引入了多生成主題 VAST（MGT-VAST）資料集，它使用 GPT-3 擴充了 VAST。在這個資料集中，每個脈絡都與多個主題相關聯，允許模型了解脈絡與各種潛在主題之間的關係

##### **Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**
2405.11629v1 by Shengxiang Sun, Shenzhe Zhu

Numerous studies on adversarial attacks targeting self-driving policies fail
to incorporate realistic-looking adversarial objects, limiting real-world
applicability. Building upon prior research that facilitated the transition of
adversarial objects from simulations to practical applications, this paper
discusses a modified gradient-based texture optimization method to discover
realistic-looking adversarial objects. While retaining the core architecture
and techniques of the prior research, the proposed addition involves an entity
termed the 'Judge'. This agent assesses the texture of a rendered object,
assigning a probability score reflecting its realism. This score is integrated
into the loss function to encourage the NeRF object renderer to concurrently
learn realistic and adversarial textures. The paper analyzes four strategies
for developing a robust 'Judge': 1) Leveraging cutting-edge vision-language
models. 2) Fine-tuning open-sourced vision-language models. 3) Pretraining
neurosymbolic systems. 4) Utilizing traditional image processing techniques.
Our findings indicate that strategies 1) and 4) yield less reliable outcomes,
pointing towards strategies 2) or 3) as more promising directions for future
research.

摘要：許多針對自駕策略的對抗性攻擊研究未能納入看起來逼真的對抗性物體，限制了實際應用的可能性。本文建立在促進對抗性物體從模擬轉變為實際應用的先前研究之上，探討了一種修改後的基於梯度的紋理優化方法，以發現看起來逼真的對抗性物體。在保留先前研究的核心架構和技術的同時，建議的附加功能涉及一個稱為「評審」的實體。這個代理評估渲染物體的紋理，並指派一個反映其真實性的機率分數。此分數整合到損失函數中，以鼓勵 NeRF 物體渲染器同時學習逼真和對抗性的紋理。本文分析了四種策略，用於開發強健的「評審」：1) 槓桿尖端的視覺語言模型。2) 微調開源的視覺語言模型。3) 預訓練神經符號系統。4) 利用傳統的影像處理技術。我們的研究結果表明，策略 1) 和 4) 產生的結果較不可靠，指出策略 2) 或 3) 是未來研究更有前途的方向。

##### **Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records**
2405.11622v1 by Mireia Hernandez Caralt, Clarence Boon Liang Ng, Marek Rei

Electronic Health Records (EHR) serve as a valuable source of patient
information, offering insights into medical histories, treatments, and
outcomes. Previous research has developed systems for detecting applicable ICD
codes that should be assigned while writing a given EHR document, mainly
focusing on discharge summaries written at the end of a hospital stay. In this
work, we investigate the potential of predicting these codes for the whole
patient stay at different time points during their stay, even before they are
officially assigned by clinicians. The development of methods to predict
diagnoses and treatments earlier in advance could open opportunities for
predictive medicine, such as identifying disease risks sooner, suggesting
treatments, and optimizing resource allocation. Our experiments show that
predictions regarding final ICD codes can be made already two days after
admission and we propose a custom model that improves performance on this early
prediction task.

摘要：電子健康記錄 (EHR) 是有價值的病人資訊來源，提供醫療病史、治療和結果的見解。先前的研究已開發出系統，用於偵測在撰寫特定 EHR 文件時應指派的適用的 ICD 代碼，主要著重於在醫院停留結束時撰寫的出院摘要。在這項工作中，我們調查預測這些代碼在病人停留期間不同時間點的潛力，甚至在臨床醫生正式指派之前。開發方法來預測診斷和治療，可以為預測醫學開啟機會，例如及早識別疾病風險、建議治療和最佳化資源配置。我們的實驗顯示，關於最終 ICD 代碼的預測可以在入院後兩天進行，我們提出一個自訂模型，以改善此早期預測任務的效能。

##### **Novel Interpretable and Robust Web-based AI Platform for Phishing Email Detection**
2405.11619v1 by Abdulla Al-Subaiey, Mohammed Al-Thani, Naser Abdullah Alam, Kaniz Fatema Antora, Amith Khandakar, SM Ashfaq Uz Zaman

Phishing emails continue to pose a significant threat, causing financial
losses and security breaches. This study addresses limitations in existing
research, such as reliance on proprietary datasets and lack of real-world
application, by proposing a high-performance machine learning model for email
classification. Utilizing a comprehensive and largest available public dataset,
the model achieves a f1 score of 0.99 and is designed for deployment within
relevant applications. Additionally, Explainable AI (XAI) is integrated to
enhance user trust. This research offers a practical and highly accurate
solution, contributing to the fight against phishing by empowering users with a
real-time web-based application for phishing email detection.

摘要：網路釣魚郵件持續構成重大威脅，造成金錢損失和安全漏洞。本研究探討現有研究的限制，例如依賴專有資料集和缺乏實際應用，提出一個用於電子郵件分類的高效能機器學習模型。利用全面且最大的可用公開資料集，該模型達到 0.99 的 f1 分數，並設計用於在相關應用程式中部署。此外，整合可解釋 AI (XAI) 以增強使用者信任。本研究提供一個實用且高度準確的解決方案，透過提供使用者即時網路釣魚郵件偵測網頁應用程式，協助對抗網路釣魚。

##### **Transcriptomics-guided Slide Representation Learning in Computational Pathology**
2405.11618v1 by Guillaume Jaume, Lukas Oldenburg, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Thomas Peeters, Andrew H. Song, Faisal Mahmood

Self-supervised learning (SSL) has been successful in building patch
embeddings of small histology images (e.g., 224x224 pixels), but scaling these
models to learn slide embeddings from the entirety of giga-pixel whole-slide
images (WSIs) remains challenging. Here, we leverage complementary information
from gene expression profiles to guide slide representation learning using
multimodal pre-training. Expression profiles constitute highly detailed
molecular descriptions of a tissue that we hypothesize offer a strong
task-agnostic training signal for learning slide embeddings. Our slide and
expression (S+E) pre-training strategy, called Tangle, employs
modality-specific encoders, the outputs of which are aligned via contrastive
learning. Tangle was pre-trained on samples from three different organs: liver
(n=6,597 S+E pairs), breast (n=1,020), and lung (n=1,012) from two different
species (Homo sapiens and Rattus norvegicus). Across three independent test
datasets consisting of 1,265 breast WSIs, 1,946 lung WSIs, and 4,584 liver
WSIs, Tangle shows significantly better few-shot performance compared to
supervised and SSL baselines. When assessed using prototype-based
classification and slide retrieval, Tangle also shows a substantial performance
improvement over all baselines. Code available at
https://github.com/mahmoodlab/TANGLE.

摘要：<paragraph>自监督学习 (SSL) 已成功构建小组织图像 (例如，224x224 像素) 的补丁嵌入，但将这些模型扩展为从全幻灯片全像素图像 (WSI) 的整体学习幻灯片嵌入仍然具有挑战性。在这里，我们利用基因表达谱中的互补信息来指导幻灯片表示学习，使用多模态预训练。表达谱构成组织的高度详细的分子描述，我们假设它为学习幻灯片嵌入提供了强大的与任务无关的训练信号。我们的幻灯片和表达 (S+E) 预训练策略称为 Tangle，它采用特定于模态的编码器，其输出通过对比学习进行对齐。Tangle 在来自三个不同器官的样本上进行预训练：来自两个不同物种（智人和大鼠）的肝脏 (n=6,597 S+E 对)、乳腺 (n=1,020) 和肺 (n=1,012)。在包括 1,265 个乳腺 WSI、1,946 个肺 WSI 和 4,584 个肝 WSI 在内的三个独立测试数据集中，与监督和 SSL 基线相比，Tangle 显示出明显更好的小样本性能。在使用基于原型的分类和幻灯片检索进行评估时，Tangle 也显示出比所有基线都有大幅的性能提升。代码可从 https://github.com/mahmoodlab/TANGLE 获得。</paragraph>

##### **Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts**
2405.11613v2 by Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, Xueqi Cheng

The knowledge within large language models (LLMs) may become outdated
quickly. While in-context editing (ICE) is currently the most effective method
for knowledge editing (KE), it is constrained by the black-box modeling of LLMs
and thus lacks interpretability. Our work aims to elucidate the superior
performance of ICE on the KE by analyzing the impacts of in-context new
knowledge on token-wise distributions. We observe that despite a significant
boost in logits of the new knowledge, the performance of is still hindered by
stubborn knowledge. Stubborn knowledge refers to as facts that have gained
excessive confidence during pretraining, making it hard to edit effectively. To
address this issue and further enhance the performance of ICE, we propose a
novel approach termed $\textbf{De}$coding by $\textbf{C}$ontrasting
$\textbf{K}$nowledge (DeCK). DeCK derives the distribution of the next token by
contrasting the logits obtained from the newly edited knowledge guided by ICE
with those from the unedited parametric knowledge. Our experiments consistently
demonstrate that DeCK enhances the confidence of LLMs in edited facts. For
instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to
219%, demonstrating its capability to strengthen ICE in the editing of stubborn
knowledge. Our work paves the way to develop the both effective and accountable
KE methods for LLMs. (The source code is available at:
https://deck-llm.meirtz.com)

摘要：大型語言模型 (LLM) 中的知識可能會快速過時。雖然目前情境編輯 (ICE) 是知識編輯 (KE) 最有效的方法，但它受到 LLM 黑盒建模的限制，因此缺乏可解釋性。我們的研究旨在通過分析情境新知識對代幣級分布的影響，闡明 ICE 在 KE 上的優異性能。我們觀察到，儘管新知識的對數幾率顯著提升，但頑固知識仍然阻礙了性能。頑固知識是指在預訓練期間獲得過度信心的事實，這使得有效編輯變得困難。為了解決這個問題並進一步提升 ICE 的性能，我們提出了一種稱為「對比知識解碼」（DeCK）的新方法。DeCK 通過對比由 ICE 引導的新編輯知識獲得的對數幾率和未編輯參數知識獲得的對數幾率，推導出下一個代幣的分布。我們的實驗持續證明，DeCK 提升了 LLM 對編輯事實的信心。例如，它將 LLaMA3-8B-instruct 在 MQuAKE 上的性能提升了 219%，證明了它加強 ICE 編輯頑固知識的能力。我們的研究為開發 LLM 的有效且負責任的 KE 方法鋪平了道路。（原始碼可在以下位置取得：https://deck-llm.meirtz.com）

##### **AI-Assisted Diagnosis for Covid-19 CXR Screening: From Data Collection to Clinical Validation**
2405.11598v1 by Carlo Alberto Barbano, Riccardo Renzulli, Marco Grosso, Domenico Basile, Marco Busso, Marco Grangetto

In this paper, we present the major results from the Covid Radiographic
imaging System based on AI (Co.R.S.A.) project, which took place in Italy. This
project aims to develop a state-of-the-art AI-based system for diagnosing
Covid-19 pneumonia from Chest X-ray (CXR) images. The contributions of this
work are manyfold: the release of the public CORDA dataset, a deep learning
pipeline for Covid-19 detection, and the clinical validation of the developed
solution by expert radiologists. The proposed detection model is based on a
two-step approach that, paired with state-of-the-art debiasing, provides
reliable results. Most importantly, our investigation includes the actual usage
of the diagnosis aid tool by radiologists, allowing us to assess the real
benefits in terms of accuracy and time efficiency. Project homepage:
https://corsa.di.unito.it/

摘要：在本文中，我們將介紹在義大利進行的 Covid 放射影像系統基於 AI（Co.R.S.A.）計畫的主要結果。此計畫的目標是開發一個最先進的基於 AI 的系統，用於根據胸部 X 光（CXR）影像診斷 Covid-19 肺炎。此研究的貢獻包括：釋出公開的 CORDA 資料集、一個用於偵測 Covid-19 的深度學習管道，以及由放射科專家對開發的解決方案進行臨床驗證。所提出的偵測模型基於一個兩步驟方法，搭配最先進的去偏，提供可靠的結果。最重要的是，我們的調查包括放射科醫師實際使用診斷輔助工具，讓我們能夠評估在準確性和時間效率方面的實際效益。計畫首頁：https://corsa.di.unito.it/

##### **Language Reconstruction with Brain Predictive Coding from fMRI Data**
2405.11597v1 by Congchi Yin, Ziyi Ye, Piji Li

Many recent studies have shown that the perception of speech can be decoded
from brain signals and subsequently reconstructed as continuous language.
However, there is a lack of neurological basis for how the semantic information
embedded within brain signals can be used more effectively to guide language
reconstruction. The theory of predictive coding suggests that human brain
naturally engages in continuously predicting future word representations that
span multiple timescales. This implies that the decoding of brain signals could
potentially be associated with a predictable future. To explore the predictive
coding theory within the context of language reconstruction, this paper
proposes a novel model \textsc{PredFT} for jointly modeling neural decoding and
brain prediction. It consists of a main decoding network for language
reconstruction and a side network for predictive coding. The side network
obtains brain predictive coding representation from related brain regions of
interest with a multi-head self-attention module. This representation is fused
into the main decoding network with cross-attention to facilitate the language
models' generation process. Experiments are conducted on the largest
naturalistic language comprehension fMRI dataset Narratives. \textsc{PredFT}
achieves current state-of-the-art decoding performance with a maximum BLEU-1
score of $27.8\%$.

摘要：許多近期研究顯示，可以從腦部訊號解碼言語感知，並進一步重建為連續的語言。
然而，缺乏神經學基礎來解釋如何更有效地使用嵌入在腦部訊號中的語義資訊，以引導語言重建。
預測編碼理論表明，人類大腦會自然地持續預測跨越多個時間尺度的未來詞彙表徵。
這表示腦部訊號的解碼可能與可預測的未來有關。
為了在語言重建的脈絡中探索預測編碼理論，本文提出一個新的模型 \textsc{PredFT}，用於聯合建模神經解碼和腦部預測。
它包含一個用於語言重建的主要解碼網路，以及一個用於預測編碼的輔助網路。
輔助網路從相關的感興趣腦區取得腦部預測編碼表徵，並使用多頭自我注意力模組。
此表徵與主要解碼網路融合，並透過交叉注意力來促進語言模型的產生過程。
實驗是在最大的自然語言理解 fMRI 資料集 Narratives 上進行。\textsc{PredFT} 達到目前最先進的解碼效能，其 BLEU-1 最高分為 $27.8\%$。

##### **SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization**
2405.11582v1 by Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang

Transformers have become foundational architectures for both natural language
and computer vision tasks. However, the high computational cost makes it quite
challenging to deploy on resource-constraint devices. This paper investigates
the computational bottleneck modules of efficient transformer, i.e.,
normalization layers and attention modules. LayerNorm is commonly used in
transformer architectures but is not computational friendly due to statistic
calculation during inference. However, replacing LayerNorm with more efficient
BatchNorm in transformer often leads to inferior performance and collapse in
training. To address this problem, we propose a novel method named PRepBN to
progressively replace LayerNorm with re-parameterized BatchNorm in training.
Moreover, we propose a simplified linear attention (SLA) module that is simple
yet effective to achieve strong performance. Extensive experiments on image
classification as well as object detection demonstrate the effectiveness of our
proposed method. For example, our SLAB-Swin obtains $83.6\%$ top-1 accuracy on
ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of
Flatten-Swin with $0.1\%$ higher accuracy. We also evaluated our method for
language modeling task and obtain comparable performance and lower
latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and
https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.

摘要：Transformer 已成为自然語言和電腦視覺任務的基礎架構。然而，高運算成本使其在資源受限裝置上部署極具挑戰性。本文探討了高效 Transformer 的運算瓶頸模組，即正規化層和注意力模組。LayerNorm 常用於 Transformer 架構中，但由於推論期間的統計計算，它並非運算友善。然而，在 Transformer 中用更有效的 BatchNorm 取代 LayerNorm 往往會導致效能低落和訓練崩潰。為了解決這個問題，我們提出了一種名為 PRepBN 的新方法，在訓練中逐步用重新參數化的 BatchNorm 取代 LayerNorm。此外，我們提出了一個簡化的線性注意力 (SLA) 模組，它簡單但有效，可實現強大的效能。在影像分類和物件偵測上的大量實驗證明了我們所提出方法的有效性。例如，我們的 SLAB-Swin 在 ImageNet-1K 上獲得 83.6% 的 top-1 精確度，延遲時間為 16.2 毫秒，比 Flatten-Swin 少 2.4 毫秒，但精確度高出 0.1%。我們也評估了我們在語言模型任務中的方法，並獲得可比較的效能和更低的延遲時間。程式碼已公開於 https://github.com/xinghaochen/SLAB 和 https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB。

##### **Exploring the Capabilities of Prompted Large Language Models in Educational and Assessment Applications**
2405.11579v1 by Subhankar Maity, Aniket Deroy, Sudeshna Sarkar

In the era of generative artificial intelligence (AI), the fusion of large
language models (LLMs) offers unprecedented opportunities for innovation in the
field of modern education. We embark on an exploration of prompted LLMs within
the context of educational and assessment applications to uncover their
potential. Through a series of carefully crafted research questions, we
investigate the effectiveness of prompt-based techniques in generating
open-ended questions from school-level textbooks, assess their efficiency in
generating open-ended questions from undergraduate-level technical textbooks,
and explore the feasibility of employing a chain-of-thought inspired
multi-stage prompting approach for language-agnostic multiple-choice question
(MCQ) generation. Additionally, we evaluate the ability of prompted LLMs for
language learning, exemplified through a case study in the low-resource Indian
language Bengali, to explain Bengali grammatical errors. We also evaluate the
potential of prompted LLMs to assess human resource (HR) spoken interview
transcripts. By juxtaposing the capabilities of LLMs with those of human
experts across various educational tasks and domains, our aim is to shed light
on the potential and limitations of LLMs in reshaping educational practices.

摘要：在生成式人工智能 (AI) 時代，大型語言模型 (LLM) 的融合為現代教育領域的創新提供了前所未有的機會。我們在教育和評量應用範疇內探討提示式 LLM，以發掘其潛力。透過一系列精心設計的研究問題，我們探討基於提示的技術在從學校教科書產生開放式問題中的有效性，評估其在從大學程度技術教科書產生開放式問題中的效率，並探討採用受思考鏈啟發的多階段提示方法來產生與語言無關的多重選擇題 (MCQ) 的可行性。此外，我們評估提示式 LLM 在語言學習中的能力，透過低資源印度語言孟加拉語的案例研究來說明孟加拉語的語法錯誤。我們也評估提示式 LLM 評量人力資源 (HR) 口語面試紀錄的潛力。透過將 LLM 的能力與人類專家在各種教育任務和領域中的能力並列，我們的目標是闡明 LLM 在重塑教育實務中的潛力與限制。

##### **A Multi-Perspective Analysis of Memorization in Large Language Models**
2405.11577v1 by Bowen Chen, Namgi Han, Yusuke Miyao

Large Language Models (LLMs), trained on massive corpora with billions of
parameters, show unprecedented performance in various fields. Though surprised
by their excellent performances, researchers also noticed some special
behaviors of those LLMs. One of those behaviors is memorization, in which LLMs
can generate the same content used to train them. Though previous research has
discussed memorization, the memorization of LLMs still lacks explanation,
especially the cause of memorization and the dynamics of generating them. In
this research, we comprehensively discussed memorization from various
perspectives and extended the discussion scope to not only just the memorized
content but also less and unmemorized content. Through various studies, we
found that: (1) Through experiments, we revealed the relation of memorization
between model size, continuation size, and context size. Further, we showed how
unmemorized sentences transition to memorized sentences. (2) Through embedding
analysis, we showed the distribution and decoding dynamics across model size in
embedding space for sentences with different memorization scores. The n-gram
statistics analysis presents d (3) An analysis over n-gram and entropy decoding
dynamics discovered a boundary effect when the model starts to generate
memorized sentences or unmemorized sentences. (4)We trained a Transformer model
to predict the memorization of different models, showing that it is possible to
predict memorizations by context.

摘要：大型語言模型 (LLM) 在數十億個參數的龐大語料庫上訓練，在各個領域展現出前所未有的效能。儘管對其卓越的表現感到驚訝，研究人員也注意到這些 LLM 的一些特殊行為。其中一種行為是記憶，LLM 可以產生用於訓練它們的相同內容。儘管先前的研究討論過記憶，但 LLM 的記憶仍然缺乏解釋，特別是記憶的原因和產生它們的動力。在本研究中，我們從各種角度全面討論記憶，並將討論範圍擴展到不僅僅是記憶的內容，還包括更少和未記憶的內容。透過各種研究，我們發現：(1) 透過實驗，我們揭示了模型大小、延續大小和上下文大小之間的記憶關係。此外，我們展示了未記憶句子如何轉換為記憶句子。(2) 透過嵌入分析，我們展示了不同記憶分數句子的嵌入空間中，模型大小的分布和解碼動態。n-gram 統計分析呈現 d (3) 對 n-gram 和熵解碼動態的分析發現，當模型開始產生記憶句子或未記憶句子時，會產生邊界效應。(4) 我們訓練了一個 Transformer 模型來預測不同模型的記憶，表明可以通過上下文來預測記憶。

##### **SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks**
2405.11575v1 by Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn

Modern NLP models are often trained on public datasets drawn from diverse
sources, rendering them vulnerable to data poisoning attacks. These attacks can
manipulate the model's behavior in ways engineered by the attacker. One such
tactic involves the implantation of backdoors, achieved by poisoning specific
training instances with a textual trigger and a target class label. Several
strategies have been proposed to mitigate the risks associated with backdoor
attacks by identifying and removing suspected poisoned examples. However, we
observe that these strategies fail to offer effective protection against
several advanced backdoor attacks. To remedy this deficiency, we propose a
novel defensive mechanism that first exploits training dynamics to identify
poisoned samples with high precision, followed by a label propagation step to
improve recall and thus remove the majority of poisoned instances. Compared
with recent advanced defense methods, our method considerably reduces the
success rates of several backdoor attacks while maintaining high classification
accuracy on clean test sets.

摘要：現代 NLP 模型通常在從不同來源擷取的公開資料集上訓練，使其容易受到資料中毒攻擊。這些攻擊可以透過攻擊者設計的方式來操縱模型的行為。其中一種策略涉及後門的植入，透過在特定訓練實例中使用文字觸發器和目標類別標籤來達成中毒。已經提出多種策略來減輕與後門攻擊相關的風險，方法是識別並移除可疑的中毒範例。然而，我們觀察到這些策略無法對抗多種進階後門攻擊提供有效的防護。為了補救這個缺點，我們提出了一種新穎的防禦機制，它首先利用訓練動態來識別中毒樣本並具備高準確度，接著透過標籤傳播步驟來改善召回率，從而移除大部分中毒實例。與近期進階防禦方法相比，我們的模型大幅降低了多種後門攻擊的成功率，同時在乾淨測試集上維持高分類準確度。

##### **Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification**
2405.11574v1 by Manan Shah, Yash Bhalgat

This report is a reproducibility study of the paper "CDUL: CLIP-Driven
Unsupervised Learning for Multi-Label Image Classification" (Abdelfattah et al,
ICCV 2023). Our report makes the following contributions: (1) We provide a
reproducible, well commented and open-sourced code implementation for the
entire method specified in the original paper. (2) We try to verify the
effectiveness of the novel aggregation strategy which uses the CLIP model to
initialize the pseudo labels for the subsequent unsupervised multi-label image
classification task. (3) We try to verify the effectiveness of the
gradient-alignment training method specified in the original paper, which is
used to update the network parameters and pseudo labels. The code can be found
at https://github.com/cs-mshah/CDUL

摘要：本報告是論文「CDUL：CLIP 驅動的無監督學習，用於多標籤影像分類」（Abdelfattah 等人，ICCV 2023）的可重製性研究。我們的報告做出了以下貢獻：(1) 我們提供原始論文中所述的完整方法的可重製、有良好註解和開源的程式碼實作。(2) 我們嘗試驗證使用 CLIP 模型初始化後續無監督多標籤影像分類任務的偽標籤的新穎聚合策略的有效性。(3) 我們嘗試驗證原始論文中所述的梯度對齊訓練方法的有效性，該方法用於更新網路參數和偽標籤。程式碼可在 https://github.com/cs-mshah/CDUL 找到

##### **An Invisible Backdoor Attack Based On Semantic Feature**
2405.11551v1 by Yangming Chen

Backdoor attacks have severely threatened deep neural network (DNN) models in
the past several years. These attacks can occur in almost every stage of the
deep learning pipeline. Although the attacked model behaves normally on benign
samples, it makes wrong predictions for samples containing triggers. However,
most existing attacks use visible patterns (e.g., a patch or image
transformations) as triggers, which are vulnerable to human inspection. In this
paper, we propose a novel backdoor attack, making imperceptible changes.
Concretely, our attack first utilizes the pre-trained victim model to extract
low-level and high-level semantic features from clean images and generates
trigger pattern associated with high-level features based on channel attention.
Then, the encoder model generates poisoned images based on the trigger and
extracted low-level semantic features without causing noticeable feature loss.
We evaluate our attack on three prominent image classification DNN across three
standard datasets. The results demonstrate that our attack achieves high attack
success rates while maintaining robustness against backdoor defenses.
Furthermore, we conduct extensive image similarity experiments to emphasize the
stealthiness of our attack strategy.

摘要：在過去幾年中，後門攻擊嚴重威脅到深度神經網路（DNN）模型。這些攻擊幾乎可以在深度學習管線的每個階段發生。儘管被攻擊的模型對良性樣本表現正常，但它對包含觸發器的樣本做出錯誤的預測。然而，大多數現有的攻擊使用可見模式（例如，修補程式或影像轉換）作為觸發器，這些觸發器容易受到人為檢查。在本文中，我們提出了一種新穎的後門攻擊，進行難以察覺的改變。具體來說，我們的攻擊首先利用預訓練的受害者模型從乾淨的影像中提取低階和高階語義特徵，並根據通道注意力生成與高階特徵相關的觸發模式。然後，編碼器模型根據觸發器和提取的低階語義特徵生成中毒影像，而不會造成明顯的特徵損失。我們在三個標準資料集上對三個著名的影像分類 DNN 評估我們的攻擊。結果表明，我們的攻擊實現了很高的攻擊成功率，同時保持了對後門防禦的魯棒性。此外，我們進行了廣泛的影像相似度實驗，以強調我們的攻擊策略的隱蔽性。

##### **VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications**
2405.11537v1 by Mikhail Konenkov, Artem Lykov, Daria Trinitatova, Dzmitry Tsetserukou

The advent of immersive Virtual Reality applications has transformed various
domains, yet their integration with advanced artificial intelligence
technologies like Visual Language Models remains underexplored. This study
introduces a pioneering approach utilizing VLMs within VR environments to
enhance user interaction and task efficiency. Leveraging the Unity engine and a
custom-developed VLM, our system facilitates real-time, intuitive user
interactions through natural language processing, without relying on visual
text instructions. The incorporation of speech-to-text and text-to-speech
technologies allows for seamless communication between the user and the VLM,
enabling the system to guide users through complex tasks effectively.
Preliminary experimental results indicate that utilizing VLMs not only reduces
task completion times but also improves user comfort and task engagement
compared to traditional VR interaction methods.

摘要：沉浸式虛擬實境應用程式的出現改變了各種領域，但它們與視覺語言模型等先進的人工智慧技術的整合仍未被充分探討。本研究提出了一種開創性的方法，利用虛擬實境環境中的 VLM 來增強使用者互動和任務效率。我們的系統利用 Unity 引擎和自訂開發的 VLM，透過自然語言處理來促進即時、直覺的使用者互動，而無需依賴視覺文字說明。整合語音轉文字和文字轉語音技術，讓使用者與 VLM 之間能順暢溝通，使系統能有效地引導使用者完成複雜的任務。初步的實驗結果表明，利用 VLM 不僅可以縮短任務完成時間，還能提升使用者的舒適度和任務參與度，優於傳統的 VR 互動方式。

##### **Knowledge Graph Pruning for Recommendation**
2405.11531v1 by Fake Lin, Xi Zhu, Ziwei Zhao, Deqiang Huang, Yu Yu, Xueying Li, Tong Xu, Enhong Chen

Recent years have witnessed the prosperity of knowledge graph based
recommendation system (KGRS), which enriches the representation of users,
items, and entities by structural knowledge with striking improvement.
Nevertheless, its unaffordable computational cost still limits researchers from
exploring more sophisticated models. We observe that the bottleneck for
training efficiency arises from the knowledge graph, which is plagued by the
well-known issue of knowledge explosion. Recently, some works have attempted to
slim the inflated KG via summarization techniques. However, these summarized
nodes may ignore the collaborative signals and deviate from the facts that
nodes in knowledge graph represent symbolic abstractions of entities from the
real-world. To this end, in this paper, we propose a novel approach called
KGTrimmer for knowledge graph pruning tailored for recommendation, to remove
the unessential nodes while minimizing performance degradation. Specifically,
we design an importance evaluator from a dual-view perspective. For the
collective view, we embrace the idea of collective intelligence by extracting
community consensus based on abundant collaborative signals, i.e. nodes are
considered important if they attract attention of numerous users. For the
holistic view, we learn a global mask to identify the valueless nodes from
their inherent properties or overall popularity. Next, we build an end-to-end
importance-aware graph neural network, which injects filtered knowledge to
enhance the distillation of valuable user-item collaborative signals.
Ultimately, we generate a pruned knowledge graph with lightweight, stable, and
robust properties to facilitate the following-up recommendation task. Extensive
experiments are conducted on three publicly available datasets to prove the
effectiveness and generalization ability of KGTrimmer.

摘要：<paragraph>近年來，基於知識圖譜的推薦系統（KGRS）蓬勃發展，透過結構化知識豐富了使用者、項目和實體的表示，並獲得顯著的提升。然而，其昂貴的計算成本仍然限制了研究人員探索更精密的模型。我們觀察到，訓練效率的瓶頸來自於知識圖譜，而知識圖譜飽受眾所周知的知識爆炸問題所困擾。最近，一些研究嘗試透過摘要技術來精簡膨脹的知識圖譜。然而，這些經過摘要的節點可能會忽略協作訊號，並偏離知識圖譜中的節點表示實體的象徵性抽象。為此，在本文中，我們提出了一種名為 KGTrimmer 的新方法，用於知識圖譜的修剪，專門用於推薦，以移除不必要的節點，同時將效能下降降到最低。具體來說，我們從雙視角設計了一個重要性評估器。對於集體觀點，我們採用集體智慧的概念，透過基於豐富的協作訊號來提取社群共識，亦即節點如果吸引了許多使用者的注意力，則被視為重要。對於整體觀點，我們學習一個全域遮罩，從其固有屬性或整體熱門程度來識別無價值的節點。接下來，我們建構了一個端到端的重視重要性的圖神經網路，注入經過濾的知識，以增強有價值的使用者項目協作訊號的萃取。最後，我們產生一個經過修剪的知識圖譜，具有輕量、穩定和強健的屬性，以利於後續的推薦任務。我們在三個公開可用的資料集上進行了廣泛的實驗，以證明 KGTrimmer 的有效性和泛化能力。</paragraph>

##### **Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors**
2405.11525v1 by Chun-Yin Huang, Kartik Srinivas, Xin Zhang, Xiaoxiao Li

Conventional Federated Learning (FL) involves collaborative training of a
global model while maintaining user data privacy. One of its branches,
decentralized FL, is a serverless network that allows clients to own and
optimize different local models separately, which results in saving management
and communication resources. Despite the promising advancements in
decentralized FL, it may reduce model generalizability due to lacking a global
model. In this scenario, managing data and model heterogeneity among clients
becomes a crucial problem, which poses a unique challenge that must be
overcome: How can every client's local model learn generalizable representation
in a decentralized manner? To address this challenge, we propose a novel
Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA.
Based on the theory of domain adaptation and Knowledge Distillation (KD), we
theoretically and empirically show that synthesizing global anchors based on
raw data distribution facilitates mutual knowledge transfer. We further design
two effective regularization terms for local training: 1) REG loss that
regularizes the distribution of the client's latent embedding with the anchors
and 2) KD loss that enables clients to learn from others. Through extensive
experiments on diverse client data distributions, we showcase the effectiveness
of DeSA in enhancing both inter- and intra-domain accuracy of each client.

摘要：傳統的聯合學習 (FL) 涉及在維護用戶資料隱私的同時，對全球模型進行協作訓練。其中一個分支，去中心化 FL，是一個無伺服器網路，允許客戶分別擁有和最佳化不同的本地模型，這有助於節省管理和通訊資源。儘管去中心化 FL 有著令人振奮的進展，但由於缺乏全球模型，它可能會降低模型的概括性。在這種情況下，管理客戶之間的資料和模型異質性成為一個關鍵問題，這提出了必須克服的獨特挑戰：每個客戶的本地模型如何在去中心化的方式下學習可概括的表示？為了應對這一挑戰，我們提出了一種新的去中心化 FL 技術，方法是引入合成錨點，稱為 DeSA。根據領域適應和知識蒸餾 (KD) 的理論，我們在理論上和經驗上證明，基於原始資料分佈合成全球錨點有助於相互知識轉移。我們進一步設計了兩個有效的正規化術語，用於本地訓練：1) REG 損失，它規範了客戶潛在嵌入與錨點的分布，以及 2) KD 損失，它使客戶能夠從他人那裡學習。透過在不同的客戶資料分佈上進行廣泛的實驗，我們展示了 DeSA 在提升每個客戶的域間和域內準確性方面的有效性。

##### **Simple-Sampling and Hard-Mixup with Prototypes to Rebalance Contrastive Learning for Text Classification**
2405.11524v1 by Mengyu Li, Yonghao Liu, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan

Text classification is a crucial and fundamental task in natural language
processing. Compared with the previous learning paradigm of pre-training and
fine-tuning by cross entropy loss, the recently proposed supervised contrastive
learning approach has received tremendous attention due to its powerful feature
learning capability and robustness. Although several studies have incorporated
this technique for text classification, some limitations remain. First, many
text datasets are imbalanced, and the learning mechanism of supervised
contrastive learning is sensitive to data imbalance, which may harm the model
performance. Moreover, these models leverage separate classification branch
with cross entropy and supervised contrastive learning branch without explicit
mutual guidance. To this end, we propose a novel model named SharpReCL for
imbalanced text classification tasks. First, we obtain the prototype vector of
each class in the balanced classification branch to act as a representation of
each class. Then, by further explicitly leveraging the prototype vectors, we
construct a proper and sufficient target sample set with the same size for each
class to perform the supervised contrastive learning procedure. The empirical
results show the effectiveness of our model, which even outperforms popular
large language models across several datasets.

摘要：文本分类是自然语言处理中一项关键且基础的任务。与之前通过交叉熵损失进行预训练和微调的学习范例相比，最近提出的监督对比学习方法因其强大的特征学习能力和鲁棒性而受到极大关注。虽然有几项研究将此技术应用于文本分类，但仍存在一些局限性。首先，许多文本数据集是不平衡的，而监督对比学习的学习机制对数据不平衡很敏感，这可能会损害模型性能。此外，这些模型利用带有交叉熵的单独分类分支和带有监督对比学习分支，而没有明确的相互指导。为此，我们提出了一种名为 SharpReCL 的新模型，用于不平衡文本分类任务。首先，我们在平衡分类分支中获取每个类的原型向量，作为每个类的表示。然后，通过进一步明确利用原型向量，我们为每个类构建一个大小相同的适当且充分的目标样本集，以执行监督对比学习过程。实证结果表明了我们模型的有效性，它甚至在几个数据集上优于流行的大语言模型。

##### **MSNER: A Multilingual Speech Dataset for Named Entity Recognition**
2405.11519v1 by Quentin Meeus, Marie-Francine Moens, Hugo Van hamme

While extensively explored in text-based tasks, Named Entity Recognition
(NER) remains largely neglected in spoken language understanding. Existing
resources are limited to a single, English-only dataset. This paper addresses
this gap by introducing MSNER, a freely available, multilingual speech corpus
annotated with named entities. It provides annotations to the VoxPopuli dataset
in four languages (Dutch, French, German, and Spanish). We have also releasing
an efficient annotation tool that leverages automatic pre-annotations for
faster manual refinement. This results in 590 and 15 hours of silver-annotated
speech for training and validation, alongside a 17-hour, manually-annotated
evaluation set. We further provide an analysis comparing silver and gold
annotations. Finally, we present baseline NER models to stimulate further
research on this newly available dataset.

摘要：儘管在基於文字的任務中已廣泛探討，但命名實體辨識 (NER) 在口語理解中仍被大量忽略。現有資源僅限於單一的、僅限英語的資料集。本文透過介紹 MSNER 來解決此差距，MSNER 是一個免費提供的多語言語音語料庫，其中標註了命名實體。它為 VoxPopuli 資料集提供四種語言 (荷蘭語、法語、德語和西班牙語) 的標註。我們也發布了一個高效的標註工具，它利用自動預標註來加快手動精煉。這產生了 590 小時和 15 小時的銀標註語音，用於訓練和驗證，以及一個 17 小時的手動標註評估集。我們進一步提供了一個分析，比較銀標註和金標註。最後，我們提出基準 NER 模型，以激勵對這個新資料集的進一步研究。

##### **NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation**
2405.11476v1 by Zhiyu Xu, Qingliang Chen

Driven by large data trained segmentation models, such as SAM , research in
one-shot segmentation has experienced significant advancements. Recent
contributions like PerSAM and MATCHER , presented at ICLR 2024, utilize a
similar approach by leveraging SAM with one or a few reference images to
generate high quality segmentation masks for target images. Specifically, they
utilize raw encoded features to compute cosine similarity between patches
within reference and target images along the channel dimension, effectively
generating prompt points or boxes for the target images a technique referred to
as the matching strategy. However, relying solely on raw features might
introduce biases and lack robustness for such a complex task. To address this
concern, we delve into the issues of feature interaction and uneven
distribution inherent in raw feature based matching. In this paper, we propose
a simple and training-free method to enhance the validity and robustness of the
matching strategy at no additional computational cost (NubbleDrop). The core
concept involves randomly dropping feature channels (setting them to zero)
during the matching process, thereby preventing models from being influenced by
channels containing deceptive information. This technique mimics discarding
pathological nubbles, and it can be seamlessly applied to other similarity
computing scenarios. We conduct a comprehensive set of experiments, considering
a wide range of factors, to demonstrate the effectiveness and validity of our
proposed method. Our results showcase the significant improvements achieved
through this simmple and straightforward approach.

摘要：<paragraph>在 SAM 等大型数据训练分割模型的推动下，一次性分割的研究取得了重大进展。最近在 ICLR 2024 上提出的 PerSAM 和 MATCHER 等贡献，利用了类似的方法，利用 SAM 和一张或几张参考图像为目标图像生成高质量的分割掩模。具体来说，它们利用原始编码特征来计算参考图像和目标图像中补丁之间的通道维度余弦相似性，有效地为目标图像生成提示点或框，这是一种被称为匹配策略的技术。然而，仅依赖原始特征可能会引入偏差，并且对于如此复杂的任务来说缺乏鲁棒性。为了解决这个问题，我们深入研究了原始特征匹配中固有的特征交互和不均匀分布问题。在本文中，我们提出了一种简单且无需训练的方法来增强匹配策略的有效性和鲁棒性，而无需额外的计算成本（NubbleDrop）。核心概念涉及在匹配过程中随机丢弃特征通道（将它们设置为零），从而防止模型受到包含欺骗性信息的通道的影响。这种技术模仿了丢弃病理性小块，并且可以无缝应用于其他相似性计算场景。我们进行了一组全面的实验，考虑了广泛的因素，以证明我们提出的方法的有效性和有效性。我们的结果展示了通过这种简单直接的方法取得的重大改进。</paragraph>

##### **FIFO-Diffusion: Generating Infinite Videos from Text without Training**
2405.11473v1 by Jihwan Kim, Junoh Kang, Jinyoung Choi, Bohyung Han

We propose a novel inference technique based on a pretrained diffusion model
for text-conditional video generation. Our approach, called FIFO-Diffusion, is
conceptually capable of generating infinitely long videos without training.
This is achieved by iteratively performing diagonal denoising, which
concurrently processes a series of consecutive frames with increasing noise
levels in a queue; our method dequeues a fully denoised frame at the head while
enqueuing a new random noise frame at the tail. However, diagonal denoising is
a double-edged sword as the frames near the tail can take advantage of cleaner
ones by forward reference but such a strategy induces the discrepancy between
training and inference. Hence, we introduce latent partitioning to reduce the
training-inference gap and lookahead denoising to leverage the benefit of
forward referencing. We have demonstrated the promising results and
effectiveness of the proposed methods on existing text-to-video generation
baselines.

摘要：我們提出一個新的推論技術，基於預訓練的擴散模型進行基於文字條件的影片生成。我們的做法稱為 FIFO-Diffusion，在概念上能夠在沒有訓練的情況下產生無限長的影片。這是透過反覆執行對角去噪來達成，同時處理一系列連續的影格，並在佇列中以增加的雜訊等級處理；我們的做法在佇列頭部將一個完全去噪的影格出列，同時在佇列尾部加入一個新的隨機雜訊影格。然而，對角去噪是一把雙面刃，因為接近佇列尾部的影格可以透過前向參考利用較乾淨的影格，但這樣的策略會導致訓練和推論之間的差異。因此，我們引入潛在分割以縮小訓練推論差距，並使用前瞻去噪來利用前向參考的好處。我們已經在現有的文字到影片生成基準上展示了所提出方法的潛力結果和有效性。

##### **VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting**
2405.11470v1 by Yingnan Yang, Qingling Zhu, Jianyong Chen

Multivariate time series (MTS) forecasting has been extensively applied
across diverse domains, such as weather prediction and energy consumption.
However, current studies still rely on the vanilla point-wise self-attention
mechanism to capture cross-variable dependencies, which is inadequate in
extracting the intricate cross-correlation implied between variables. To fill
this gap, we propose Variable Correlation Transformer (VCformer), which
utilizes Variable Correlation Attention (VCA) module to mine the correlations
among variables. Specifically, based on the stochastic process theory, VCA
calculates and integrates the cross-correlation scores corresponding to
different lags between queries and keys, thereby enhancing its ability to
uncover multivariate relationships. Additionally, inspired by Koopman dynamics
theory, we also develop Koopman Temporal Detector (KTD) to better address the
non-stationarity in time series. The two key components enable VCformer to
extract both multivariate correlations and temporal dependencies. Our extensive
experiments on eight real-world datasets demonstrate the effectiveness of
VCformer, achieving top-tier performance compared to other state-of-the-art
baseline models. Code is available at this repository:
https://github.com/CSyyn/VCformer.

摘要：多變量時間序列 (MTS) 預測已被廣泛應用於各種領域，例如天氣預測和能源消耗。然而，目前的研究仍依賴於香草點式自我注意機制來捕捉跨變量依賴性，這不足以提取變量之間隱含的複雜互相關。為了填補這一空白，我們提出了變量相關變換器 (VCformer)，它利用變量相關注意 (VCA) 模組來挖掘變量之間的相關性。具體來說，基於隨機過程理論，VCA 計算和整合與查詢和鍵之間的不同滯後相應的互相關分數，從而增強其發現多變量關係的能力。此外，受庫普曼動力學理論的啟發，我們還開發了庫普曼時間檢測器 (KTD) 來更好地解決時間序列中的非平穩性。這兩個關鍵組件使 VCformer 能夠提取多變量相關性和時間依賴性。我們在八個真實世界資料集上的大量實驗證明了 VCformer 的有效性，與其他最先進的基準模型相比，實現了頂級效能。程式碼可在這個儲存庫中取得：https://github.com/CSyyn/VCformer。

##### **Effective In-Context Example Selection through Data Compression**
2405.11465v1 by Zhongxiang Sun, Kepu Zhang, Haoyu Wang, Xiao Zhang, Jun Xu

In-context learning has been extensively validated in large language models.
However, the mechanism and selection strategy for in-context example selection,
which is a crucial ingredient in this approach, lacks systematic and in-depth
research. In this paper, we propose a data compression approach to the
selection of in-context examples. We introduce a two-stage method that can
effectively choose relevant examples and retain sufficient information about
the training dataset within the in-context examples. Our method shows a
significant improvement of an average of 5.90% across five different real-world
datasets using four language models.

摘要：語境學習已在大型語言模型中得到廣泛驗證。
然而，語境範例選擇的機制和選擇策略，
是此方法中至關重要的成分，卻缺乏系統性和深入
的研究。在本文中，我們提出一個資料壓縮方法來
選擇語境範例。我們引入一種兩階段方法，可以
有效地選擇相關範例並在語境範例中保留訓練資料集的足夠資訊。我們的
方法顯示在使用四種語言模型的五個不同的真實世界
資料集上平均有 5.90% 的顯著提升。

##### **Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion**
2405.11464v1 by Pengxiang Lan, Enneng Yang, Yuting Liu, Guibing Guo, Linying Jiang, Jianzhe Zhao, Xingwei Wang

Prompt tuning is a promising method to fine-tune a pre-trained language model
without retraining its large-scale parameters. Instead, it attaches a soft
prompt to the input text, whereby downstream tasks can be well adapted by
merely learning the embeddings of prompt tokens. Nevertheless, existing methods
still suffer from two challenges: (i) they are hard to balance accuracy and
efficiency. A longer (shorter) soft prompt generally leads to a better (worse)
accuracy but at the cost of more (less) training time. (ii) The performance may
not be consistent when adapting to different downstream tasks. We attribute it
to the same embedding space but responsible for different requirements of
downstream tasks. To address these issues, we propose an Efficient Prompt
Tuning method (EPT) by multi-space projection and prompt fusion. Specifically,
it decomposes a given soft prompt into a shorter prompt and two low-rank
matrices, whereby the number of parameters is greatly reduced as well as the
training time. The accuracy is also enhanced by leveraging low-rank matrices
and the short prompt as additional knowledge sources to enrich the semantics of
the original short prompt. In addition, we project the soft prompt into
multiple subspaces to improve the performance consistency, and then adaptively
learn the combination weights of different spaces through a gating network.
Experimental experiments on 13 natural language processing downstream tasks
show that our method significantly and consistently outperforms 11 comparison
methods with the relative percentage of improvements up to 28.8%, and training
time decreased by 14%.

摘要：提示微调是一种很有前途的方法，可以微调预训练语言模型，而无需重新训练其大规模参数。相反，它将一个软提示附加到输入文本，从而可以通过仅仅学习提示标记的嵌入来很好地适应下游任务。然而，现有方法仍然面临两大挑战：(i) 难以平衡准确性和效率。较长（较短）的软提示通常会导致较好（较差）的准确性，但代价是训练时间更多（更少）。(ii) 在适应不同的下游任务时，性能可能不一致。我们将其归因于相同的嵌入空间，但负责不同的下游任务需求。为了解决这些问题，我们通过多空间投影和提示融合提出了一种高效提示微调方法 (EPT)。具体来说，它将给定的软提示分解为一个较短的提示和两个低秩矩阵，从而大大减少了参数的数量以及训练时间。通过利用低秩矩阵和短提示作为附加知识源来丰富原始短提示的语义，还可以提高准确性。此外，我们将软提示投影到多个子空间以提高性能一致性，然后通过门控网络自适应地学习不同空间的组合权重。在 13 个自然语言处理下游任务上的实验表明，我们的方法明显且持续地优于 11 种比较方法，相对改进百分比高达 28.8%，训练时间减少了 14%。

