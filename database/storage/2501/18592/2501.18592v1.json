{"2501.18592": {"publish_time": "2025-01-30", "title": "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models", "paper_summary": "In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.", "paper_summary_zh": "\u5728\u5be6\u969b\u5834\u666f\u4e2d\uff0c\u5be6\u73fe\u9818\u57df\u9069\u61c9\u548c\u6cdb\u5316\u6703\u9020\u6210\n\u986f\u8457\u7684\u6311\u6230\uff0c\u56e0\u70ba\u6a21\u578b\u5fc5\u9808\u9069\u61c9\u6216\u6cdb\u5316\u5230\u672a\u77e5\n\u76ee\u6a19\u5206\u4f48\u3002\u5c07\u9019\u4e9b\u80fd\u529b\u64f4\u5c55\u5230\u672a\u898b\u7684\u591a\u6a21\u5f0f\n\u5206\u4f48\uff0c\u5373\u591a\u6a21\u5f0f\u9818\u57df\u9069\u61c9\u548c\u6cdb\u5316\uff0c\u7531\u65bc\u4e0d\u540c\u6a21\u5f0f\u7684\n\u4e0d\u540c\u7279\u5fb5\uff0c\u56e0\u6b64\u66f4\u5177\u6311\u6230\u6027\u3002\u591a\u5e74\u4f86\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\n\u61c9\u7528\u7bc4\u570d\u5f9e\u52d5\u4f5c\u8fa8\u8b58\u5230\u8a9e\u7fa9\u5206\u5272\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u51fa\u73fe\n\u4e86 CLIP \u7b49\u5927\u578b\u9810\u8a13\u7df4\u591a\u6a21\u5f0f\u57fa\u790e\u6a21\u578b\uff0c\u555f\u767c\u4e86\u5229\u7528\n\u9019\u4e9b\u6a21\u578b\u4f86\u589e\u5f37\u9069\u61c9\u548c\u6cdb\u5316\u6548\u80fd\u6216\u5c07\u5b83\u5011\u9069\u61c9\u5230\u4e0b\u6e38\n\u4efb\u52d9\u3002\u9019\u9805\u8abf\u67e5\u63d0\u4f9b\u4e86\u5c0d\u5f9e\u50b3\u7d71\u65b9\u6cd5\u5230\n\u57fa\u790e\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u7684\u9996\u6b21\u5168\u9762\u56de\u9867\uff0c\u6db5\u84cb\uff1a(1) \u591a\u6a21\u5f0f\n\u9818\u57df\u9069\u61c9\uff1b(2) \u591a\u6a21\u5f0f\u6e2c\u8a66\u6642\u9593\u9069\u61c9\uff1b(3) \u591a\u6a21\u5f0f\u9818\u57df\u6cdb\u5316\uff1b\n(4) \u5728\u591a\u6a21\u5f0f\u57fa\u790e\u6a21\u578b\u7684\u5e6b\u52a9\u4e0b\u9032\u884c\u9818\u57df\u9069\u61c9\u548c\u6cdb\u5316\uff1b\n\u4ee5\u53ca (5) \u591a\u6a21\u5f0f\u57fa\u790e\u6a21\u578b\u7684\u9069\u61c9\u3002\u5c0d\u65bc\u6bcf\u500b\u4e3b\u984c\uff0c\u6211\u5011\u6b63\u5f0f\n\u5b9a\u7fa9\u554f\u984c\u4e26\u5fb9\u5e95\u6aa2\u8996\u73fe\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\n\u5206\u6790\u76f8\u95dc\u7684\u8cc7\u6599\u96c6\u548c\u61c9\u7528\uff0c\u5f37\u8abf\u958b\u653e\u7684\u6311\u6230\u548c\n\u6f5b\u5728\u7684\u672a\u4f86\u7814\u7a76\u65b9\u5411\u3002\u6211\u5011\u7dad\u8b77\u4e00\u500b\u6d3b\u8e8d\u7684\u5b58\u653e\u5eab\uff0c\u5176\u4e2d\n\u5305\u542b https://github.com/donghao51/Awesome-Multimodal-Adaptation\n\u7684\u6700\u65b0\u6587\u737b\u3002", "author": "Hao Dong et.al.", "authors": "Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink", "id": "2501.18592v1", "paper_url": "http://arxiv.org/abs/2501.18592v1", "repo": "https://github.com/donghao51/awesome-multimodal-adaptation"}}