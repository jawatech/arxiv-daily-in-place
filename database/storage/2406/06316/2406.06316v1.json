{"2406.06316": {"publish_time": "2024-06-10", "title": "Tx-LLM: A Large Language Model for Therapeutics", "paper_summary": "Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.", "paper_summary_zh": "\u958b\u767c\u6cbb\u7642\u85e5\u7269\u662f\u4e00\u500b\u6f2b\u9577\u4e14\u6602\u8cb4\u7684\u904e\u7a0b\uff0c\u9700\u8981\u6eff\u8db3\u8a31\u591a\u4e0d\u540c\u7684\u6a19\u6e96\uff0c\u800c\u80fd\u5920\u52a0\u901f\u9019\u500b\u904e\u7a0b\u7684 AI \u6a21\u578b\u5c07\u6703\u975e\u5e38\u6709\u50f9\u503c\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u591a\u6578\u7684 AI \u65b9\u6cd5\u50c5\u80fd\u8655\u7406\u5b9a\u7fa9\u72f9\u9698\u7684\u4e00\u7d44\u4efb\u52d9\uff0c\u901a\u5e38\u4fb7\u9650\u5728\u7279\u5b9a\u9818\u57df\u5167\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 Tx-LLM\uff0c\u4e00\u500b\u5f9e PaLM-2 \u5fae\u8abf\u800c\u4f86\u7684\u901a\u624d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u5b83\u7de8\u78bc\u4e86\u95dc\u65bc\u5404\u7a2e\u6cbb\u7642\u65b9\u5f0f\u7684\u77e5\u8b58\u3002Tx-LLM \u662f\u4f7f\u7528 709 \u500b\u8cc7\u6599\u96c6\u8a13\u7df4\u7684\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u91dd\u5c0d 66 \u500b\u4efb\u52d9\uff0c\u6db5\u84cb\u85e5\u7269\u767c\u73fe\u7ba1\u7dda\u7684\u5404\u500b\u968e\u6bb5\u3002Tx-LLM \u4f7f\u7528\u55ae\u4e00\u7d44\u6b0a\u91cd\uff0c\u540c\u6642\u8655\u7406\u5404\u7a2e\u5316\u5b78\u6216\u751f\u7269\u5be6\u9ad4\uff08\u5c0f\u5206\u5b50\u3001\u86cb\u767d\u8cea\u3001\u6838\u9178\u3001\u7d30\u80de\u7cfb\u3001\u75be\u75c5\uff09\u7a7f\u63d2\u81ea\u7531\u6587\u5b57\uff0c\u8b93\u5b83\u80fd\u5920\u9810\u6e2c\u5ee3\u6cdb\u7684\u95dc\u806f\u5c6c\u6027\uff0c\u5728 66 \u500b\u4efb\u52d9\u4e2d\u6709 43 \u500b\u4efb\u52d9\u9054\u5230\u8207\u6700\u5148\u9032 (SOTA) \u6280\u8853\u540c\u7b49\u7684\u6548\u80fd\uff0c\u4e26\u5728 22 \u500b\u4efb\u52d9\u4e2d\u8d85\u8d8a SOTA\u3002\u5176\u4e2d\uff0cTx-LLM \u7279\u5225\u5f37\u5927\uff0c\u5728\u7d50\u5408\u5206\u5b50 SMILES \u8868\u793a\u6cd5\u8207\u6587\u5b57\uff08\u4f8b\u5982\u7d30\u80de\u7cfb\u540d\u7a31\u6216\u75be\u75c5\u540d\u7a31\uff09\u7684\u4efb\u52d9\u4e2d\uff0c\u5e73\u5747\u6548\u80fd\u8d85\u8d8a\u540c\u985e\u6700\u4f73\u6548\u80fd\uff0c\u9019\u53ef\u80fd\u662f\u7531\u65bc\u5728\u9810\u8a13\u7df4\u671f\u9593\u5b78\u7fd2\u5230\u7684\u8108\u7d61\u3002\u6211\u5011\u89c0\u5bdf\u5230\u4e0d\u540c\u85e5\u7269\u985e\u578b\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u6d89\u53ca\u5c0f\u5206\u5b50\u7684\u4efb\u52d9\u548c\u6d89\u53ca\u86cb\u767d\u8cea\u7684\u4efb\u52d9\uff09\u4e4b\u9593\u5177\u6709\u6b63\u5411\u8f49\u79fb\u7684\u8b49\u64da\uff0c\u4e26\u4e14\u6211\u5011\u7814\u7a76\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u9818\u57df\u5fae\u8abf\u548c\u63d0\u793a\u7b56\u7565\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6211\u5011\u76f8\u4fe1 Tx-LLM \u4ee3\u8868\u4e86 LLM \u7de8\u78bc\u751f\u5316\u77e5\u8b58\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e26\u4e14\u53ef\u4ee5\u5728\u85e5\u7269\u767c\u73fe\u958b\u767c\u7ba1\u7dda\u4e2d\u4f5c\u70ba\u7aef\u5230\u7aef\u5de5\u5177\u767c\u63ee\u672a\u4f86\u7684\u4f5c\u7528\u3002", "author": "Juan Manuel Zambrano Chaves et.al.", "authors": "Juan Manuel Zambrano Chaves, Eric Wang, Tao Tu, Eeshit Dhaval Vaishnav, Byron Lee, S. Sara Mahdavi, Christopher Semturs, David Fleet, Vivek Natarajan, Shekoofeh Azizi", "id": "2406.06316v1", "paper_url": "http://arxiv.org/abs/2406.06316v1", "repo": "null"}}