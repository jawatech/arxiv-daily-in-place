{"2410.19317": {"publish_time": "2024-10-25", "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs", "paper_summary": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u804a\u5929\u6a5f\u5668\u4eba\u7684\u4f7f\u7528\u65e5\u76ca\u5ee3\u6cdb\uff0c\u9019\u5f15\u767c\u4e86\u4eba\u5011\u5c0d\u516c\u5e73\u6027\u7684\u64d4\u6182\u3002LLM \u4e2d\u7684\u516c\u5e73\u6027\u554f\u984c\u53ef\u80fd\u6703\u5c0e\u81f4\u56b4\u91cd\u7684\u5f8c\u679c\uff0c\u4f8b\u5982\u504f\u898b\u64f4\u5927\u3001\u6b67\u8996\u548c\u5c0d\u908a\u7de3\u5316\u793e\u7fa4\u9020\u6210\u50b7\u5bb3\u3002\u73fe\u6709\u7684\u516c\u5e73\u6027\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u55ae\u8f2a\u5c0d\u8a71\u4e0a\uff0c\u800c\u591a\u8f2a\u5c0d\u8a71\u5834\u666f\u5be6\u969b\u4e0a\u66f4\u80fd\u53cd\u6620\u771f\u5be6\u4e16\u754c\u7684\u5c0d\u8a71\uff0c\u7531\u65bc\u5c0d\u8a71\u7684\u8907\u96dc\u6027\u548c\u6f5b\u5728\u7684\u504f\u898b\u7d2f\u7a4d\uff0c\u56e0\u6b64\u5e36\u4f86\u4e86\u66f4\u5927\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u70ba\u591a\u8f2a\u5c0d\u8a71\u5834\u666f\u4e2d\u7684 LLM \u63d0\u51fa\u4e86\u4e00\u500b\u5168\u9762\u7684\u516c\u5e73\u6027\u57fa\u6e96\uff0c\\textbf{FairMT-Bench}\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5236\u5b9a\u4e86\u4e00\u500b\u4efb\u52d9\u5206\u985e\u6cd5\uff0c\u91dd\u5c0d LLM \u5728\u4e09\u500b\u968e\u6bb5\u7684\u516c\u5e73\u6027\u80fd\u529b\uff1a\u80cc\u666f\u7406\u89e3\u3001\u4f7f\u7528\u8005\u4e92\u52d5\u548c\u6307\u4ee4\u6b0a\u8861\uff0c\u6bcf\u500b\u968e\u6bb5\u5305\u542b\u5169\u500b\u4efb\u52d9\u3002\u70ba\u4e86\u78ba\u4fdd\u6db5\u84cb\u591a\u7a2e\u504f\u898b\u985e\u578b\u548c\u5c6c\u6027\uff0c\u6211\u5011\u5f9e\u73fe\u6709\u7684\u516c\u5e73\u6027\u8cc7\u6599\u96c6\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u4e26\u4f7f\u7528\u6211\u5011\u7684\u7bc4\u672c\u4f86\u69cb\u5efa\u591a\u8f2a\u5c0d\u8a71\u8cc7\u6599\u96c6\uff0c\\texttt{FairMT-10K}\u3002\u70ba\u4e86\u9032\u884c\u8a55\u4f30\uff0c\u6211\u5011\u61c9\u7528 GPT-4\uff0c\u4ee5\u53ca\u5305\u62ec Llama-Guard-3 \u5728\u5167\u7684\u504f\u898b\u5206\u985e\u5668\u548c\u4eba\u5de5\u9a57\u8b49\uff0c\u4ee5\u78ba\u4fdd\u5065\u5168\u6027\u3002\u5728 \\texttt{FairMT-10K} \u4e0a\u9032\u884c\u7684\u5be6\u9a57\u548c\u5206\u6790\u8868\u660e\uff0c\u5728\u591a\u8f2a\u5c0d\u8a71\u5834\u666f\u4e2d\uff0c\u7576\u524d\u7684 LLM \u66f4\u6709\u53ef\u80fd\u7522\u751f\u6709\u504f\u898b\u7684\u56de\u61c9\uff0c\u4e26\u4e14\u4e0d\u540c\u4efb\u52d9\u548c\u6a21\u578b\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u7570\u5f88\u5927\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u7b56\u5283\u4e86\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u8cc7\u6599\u96c6\uff0c\\texttt{FairMT-1K}\uff0c\u4e26\u5728\u6b64\u8cc7\u6599\u96c6\u4e0a\u6e2c\u8a66\u4e86 15 \u500b\u7576\u524d\u6700\u5148\u9032 (SOTA) \u7684 LLM\u3002\u7d50\u679c\u986f\u793a\u4e86 LLM \u4e2d\u516c\u5e73\u6027\u7684\u73fe\u72c0\uff0c\u4e26\u5c55\u793a\u4e86\u9019\u7a2e\u65b0\u65b9\u6cd5\u5728\u66f4\u771f\u5be6\u7684\u591a\u8f2a\u5c0d\u8a71\u4e0a\u4e0b\u6587\u4e2d\u8a55\u4f30\u516c\u5e73\u6027\u7684\u6548\u7528\uff0c\u547c\u7c72\u672a\u4f86\u7684\u7814\u7a76\u91cd\u9ede\u95dc\u6ce8 LLM \u516c\u5e73\u6027\u7684\u6539\u9032\uff0c\u4e26\u5728\u9019\u4e9b\u5de5\u4f5c\u4e2d\u63a1\u7528 \\texttt{FairMT-1K}\u3002</paragraph>", "author": "Zhiting Fan et.al.", "authors": "Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu", "id": "2410.19317v1", "paper_url": "http://arxiv.org/abs/2410.19317v1", "repo": "null"}}