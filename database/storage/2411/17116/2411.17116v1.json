{"2411.17116": {"publish_time": "2024-11-26", "title": "Star Attention: Efficient LLM Inference over Long Sequences", "paper_summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u9577\u5e8f\u5217\u9032\u884c\u63a8\u8ad6\uff0c\u7531\u65bc\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\uff0c\u65e2\u6602\u8cb4\u53c8\u7de9\u6162\u3002\u6211\u5011\u5f15\u5165\u4e86 Star Attention\uff0c\u9019\u662f\u4e00\u7a2e\u5169\u968e\u6bb5\u5340\u584a\u7a00\u758f\u8fd1\u4f3c\uff0c\u900f\u904e\u5728\u591a\u500b\u4e3b\u6a5f\u4e0a\u5206\u7247\u6ce8\u610f\u529b\uff0c\u540c\u6642\u6700\u5927\u7a0b\u5ea6\u5730\u6e1b\u5c11\u901a\u8a0a\u958b\u92b7\uff0c\u4f86\u63d0\u5347\u904b\u7b97\u6548\u7387\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u4f7f\u7528\u5340\u584a\u5c40\u90e8\u6ce8\u610f\u529b\u8de8\u4e3b\u6a5f\u4e26\u884c\u8655\u7406\u5167\u5bb9\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u67e5\u8a62\u548c\u56de\u61c9\u6b0a\u6756\u900f\u904e\u5e8f\u5217\u5168\u5c40\u6ce8\u610f\u529b\u95dc\u6ce8\u6240\u6709\u5148\u524d\u5feb\u53d6\u7684\u6b0a\u6756\u3002Star Attention \u53ef\u8207\u5927\u591a\u6578\u4f7f\u7528\u5168\u5c40\u6ce8\u610f\u529b\u8a13\u7df4\u7684\u57fa\u65bc Transformer \u7684 LLM \u7121\u7e2b\u6574\u5408\uff0c\u5c07\u8a18\u61b6\u9ad4\u9700\u6c42\u548c\u63a8\u8ad6\u6642\u9593\u6e1b\u5c11\u591a\u9054 11 \u500d\uff0c\u540c\u6642\u4fdd\u7559 95-100% \u7684\u6e96\u78ba\u5ea6\u3002", "author": "Shantanu Acharya et.al.", "authors": "Shantanu Acharya, Fei Jia, Boris Ginsburg", "id": "2411.17116v1", "paper_url": "http://arxiv.org/abs/2411.17116v1", "repo": "https://github.com/NVIDIA/Star-Attention"}}