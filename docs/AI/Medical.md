
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-05**|**Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**|Sarah Al-Shareeda et.al.|[2502.03396v1](http://arxiv.org/abs/2502.03396v1)|null|
|**2025-02-05**|**RadVLM: A Multitask Conversational Vision-Language Model for Radiology**|Nicolas Deperrois et.al.|[2502.03333v1](http://arxiv.org/abs/2502.03333v1)|null|
|**2025-02-05**|**MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**|Amin Dada et.al.|[2502.03298v1](http://arxiv.org/abs/2502.03298v1)|null|
|**2025-02-05**|**Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**|Matthias Schwab et.al.|[2502.03272v1](http://arxiv.org/abs/2502.03272v1)|null|
|**2025-02-05**|**Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**|Li Pan et.al.|[2502.03238v1](http://arxiv.org/abs/2502.03238v1)|null|
|**2025-02-05**|**MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**|Seonok Kim et.al.|[2502.03004v1](http://arxiv.org/abs/2502.03004v1)|null|
|**2025-02-04**|**3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**|Weicheng Zhu et.al.|[2502.02779v1](http://arxiv.org/abs/2502.02779v1)|null|
|**2025-02-04**|**Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**|Obed Korshie Dzikunu et.al.|[2502.02756v1](http://arxiv.org/abs/2502.02756v1)|null|
|**2025-02-04**|**MedRAX: Medical Reasoning Agent for Chest X-ray**|Adibvafa Fallahpour et.al.|[2502.02673v1](http://arxiv.org/abs/2502.02673v1)|null|
|**2025-02-04**|**Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**|Shayan Kiyani et.al.|[2502.02561v1](http://arxiv.org/abs/2502.02561v1)|null|
|**2025-02-04**|**A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**|Edward Ellis et.al.|[2502.02489v1](http://arxiv.org/abs/2502.02489v1)|null|
|**2025-02-04**|**Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**|Yaling Shen et.al.|[2502.02438v1](http://arxiv.org/abs/2502.02438v1)|null|
|**2025-02-04**|**Test Time Training for 4D Medical Image Interpolation**|Qikang Zhang et.al.|[2502.02341v1](http://arxiv.org/abs/2502.02341v1)|null|
|**2025-02-04**|**Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**|Atharva Mangeshkumar Agrawal et.al.|[2502.02249v1](http://arxiv.org/abs/2502.02249v1)|null|
|**2025-02-04**|**Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**|F. Xavier Gaya-Morey et.al.|[2502.02618v1](http://arxiv.org/abs/2502.02618v1)|null|
|**2025-02-04**|**Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**|Yuxiao Cheng et.al.|[2502.02109v1](http://arxiv.org/abs/2502.02109v1)|null|
|**2025-02-03**|**An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**|Jiazi Tian et.al.|[2502.01789v1](http://arxiv.org/abs/2502.01789v1)|null|
|**2025-02-03**|**Improving Transformer World Models for Data-Efficient RL**|Antoine Dedieu et.al.|[2502.01591v1](http://arxiv.org/abs/2502.01591v1)|null|
|**2025-02-03**|**Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**|Zhi Zhang et.al.|[2502.01377v1](http://arxiv.org/abs/2502.01377v1)|null|
|**2025-02-03**|**OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**|Chengfeng Zhou et.al.|[2502.01243v1](http://arxiv.org/abs/2502.01243v1)|null|
|**2025-02-03**|**MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**|Alejandro Guerra-Manzanares et.al.|[2502.01158v1](http://arxiv.org/abs/2502.01158v1)|null|
|**2025-02-03**|**Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**|Qian Chen et.al.|[2502.01141v1](http://arxiv.org/abs/2502.01141v1)|null|
|**2025-02-03**|**Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**|Mithun Saha et.al.|[2502.01108v1](http://arxiv.org/abs/2502.01108v1)|null|
|**2025-02-02**|**Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**|Hadas Ben-Atya et.al.|[2502.01691v1](http://arxiv.org/abs/2502.01691v1)|null|
|**2025-02-02**|**Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**|Si-Ioi Ng et.al.|[2502.01685v1](http://arxiv.org/abs/2502.01685v1)|null|
|**2025-02-02**|**Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**|Shengtian Sang et.al.|[2502.00712v1](http://arxiv.org/abs/2502.00712v1)|null|
|**2025-02-02**|**TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**|Linglong Wu et.al.|[2502.00695v1](http://arxiv.org/abs/2502.00695v1)|null|
|**2025-02-02**|**Enhanced Convolutional Neural Networks for Improved Image Classification**|Xiaoran Yang et.al.|[2502.00663v1](http://arxiv.org/abs/2502.00663v1)|null|
|**2025-02-02**|**Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**|Yujin Oh et.al.|[2502.00619v1](http://arxiv.org/abs/2502.00619v1)|null|
|**2025-02-01**|**Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**|Samiran Dey et.al.|[2502.00568v1](http://arxiv.org/abs/2502.00568v1)|null|
|**2025-02-01**|**Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**|Mohammad Saleh Torkestani et.al.|[2502.00495v1](http://arxiv.org/abs/2502.00495v1)|null|
|**2025-02-01**|**Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**|Aishik Mandal et.al.|[2502.00451v1](http://arxiv.org/abs/2502.00451v1)|null|
|**2025-01-31**|**EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**|Omar H. Khater et.al.|[2502.00205v1](http://arxiv.org/abs/2502.00205v1)|null|
|**2025-01-31**|**DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**|Abdurrahim Yilmaz et.al.|[2502.00196v1](http://arxiv.org/abs/2502.00196v1)|null|
|**2025-01-31**|**Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**|Hassan Jahanandish et.al.|[2502.00146v1](http://arxiv.org/abs/2502.00146v1)|null|
|**2025-01-31**|**AIN: The Arabic INclusive Large Multimodal Model**|Ahmed Heakl et.al.|[2502.00094v2](http://arxiv.org/abs/2502.00094v2)|null|
|**2025-01-31**|**Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**|Misha P. T Kaandorp et.al.|[2501.19338v1](http://arxiv.org/abs/2501.19338v1)|null|
|**2025-01-31**|**Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**|Halil Ibrahim Aysel et.al.|[2501.19271v1](http://arxiv.org/abs/2501.19271v1)|null|
|**2025-01-31**|**Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**|Aurora Rofena et.al.|[2501.19176v1](http://arxiv.org/abs/2501.19176v1)|null|
|**2025-01-31**|**Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**|Xiangyu Sun et.al.|[2501.19086v1](http://arxiv.org/abs/2501.19086v1)|null|
|**2025-01-30**|**Survey and Improvement Strategies for Gene Prioritization with Large Language Models**|Matthew Neeley et.al.|[2501.18794v1](http://arxiv.org/abs/2501.18794v1)|null|
|**2025-01-30**|**Synthetic Data Generation for Augmenting Small Samples**|Dan Liu et.al.|[2501.18741v1](http://arxiv.org/abs/2501.18741v1)|null|
|**2025-01-30**|**A Multi-Layered Large Language Model Framework for Disease Prediction**|Malak Mohamed et.al.|[2502.00063v1](http://arxiv.org/abs/2502.00063v1)|null|
|**2025-01-30**|**A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**|Yifan Wang et.al.|[2501.18367v1](http://arxiv.org/abs/2501.18367v1)|null|
|**2025-01-30**|**MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**|Yuxin Zuo et.al.|[2501.18362v1](http://arxiv.org/abs/2501.18362v1)|null|
|**2025-01-30**|**CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**|Yicheng Wu et.al.|[2501.18328v1](http://arxiv.org/abs/2501.18328v1)|null|
|**2025-01-30**|**A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**|Shayli Farshchiha et.al.|[2501.18294v1](http://arxiv.org/abs/2501.18294v1)|null|
|**2025-01-30**|**The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**|Anup Saha et.al.|[2501.18270v1](http://arxiv.org/abs/2501.18270v1)|null|
|**2025-01-30**|**Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**|Malte TÃ¶lle et.al.|[2501.18237v1](http://arxiv.org/abs/2501.18237v1)|null|
|**2025-01-30**|**Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**|Min Hun Lee et.al.|[2501.18108v1](http://arxiv.org/abs/2501.18108v1)|null|
|**2025-01-30**|**Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**|Pratik S. Sachdeva et.al.|[2501.18081v1](http://arxiv.org/abs/2501.18081v1)|null|
|**2025-01-30**|**Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**|Pir Bakhsh Khokhar et.al.|[2501.18071v1](http://arxiv.org/abs/2501.18071v1)|null|
|**2025-01-29**|**Current Pathology Foundation Models are unrobust to Medical Center Differences**|Edwin D. de Jong et.al.|[2501.18055v2](http://arxiv.org/abs/2501.18055v2)|null|
|**2025-01-29**|**Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**|Zijie Liu et.al.|[2501.17860v1](http://arxiv.org/abs/2501.17860v1)|null|
|**2025-01-29**|**GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**|Ziang Liu et.al.|[2501.17855v1](http://arxiv.org/abs/2501.17855v1)|null|
|**2025-01-29**|**Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**|Ljubisa Bojic et.al.|[2502.00055v1](http://arxiv.org/abs/2502.00055v1)|null|
|**2025-01-29**|**Tonguescape: Exploring Language Models Understanding of Vowel Articulation**|Haruki Sakajo et.al.|[2501.17643v1](http://arxiv.org/abs/2501.17643v1)|[link](https://github.com/sj-h4/tonguescape-builder)|
|**2025-01-29**|**Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**|Manish Sanwal et.al.|[2501.18645v2](http://arxiv.org/abs/2501.18645v2)|null|
|**2025-01-29**|**An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**|Wenqi Li et.al.|[2501.17555v1](http://arxiv.org/abs/2501.17555v1)|null|
|**2025-01-29**|**LLM Assistance for Pediatric Depression**|Mariia Ignashina et.al.|[2501.17510v1](http://arxiv.org/abs/2501.17510v1)|null|
|**2025-01-28**|**Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**|Gonzalo IÃ±aki Quintana et.al.|[2502.00052v1](http://arxiv.org/abs/2502.00052v1)|null|
|**2025-01-28**|**Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**|Chongyu Qu et.al.|[2501.17343v1](http://arxiv.org/abs/2501.17343v1)|null|
|**2025-01-28**|**Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**|Mingyu Derek Ma et.al.|[2501.17338v1](http://arxiv.org/abs/2501.17338v1)|null|
|**2025-01-28**|**Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**|Mingyu Derek Ma et.al.|[2501.17326v1](http://arxiv.org/abs/2501.17326v1)|null|
|**2025-01-28**|**Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**|Peilong Wang et.al.|[2501.17286v1](http://arxiv.org/abs/2501.17286v1)|null|
|**2025-01-28**|**ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**|Mohammadreza Saraei et.al.|[2501.17260v1](http://arxiv.org/abs/2501.17260v1)|[link](https://github.com/mrsaraei/vit-2spn)|
|**2025-01-28**|**A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**|Suresh Babu Nettur et.al.|[2501.17160v1](http://arxiv.org/abs/2501.17160v1)|null|
|**2025-01-28**|**Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**|Reza Ghorbani et.al.|[2501.17152v1](http://arxiv.org/abs/2501.17152v1)|null|
|**2025-01-28**|**Irony Detection, Reasoning and Understanding in Zero-shot Learning**|Peiling Yi et.al.|[2501.16884v1](http://arxiv.org/abs/2501.16884v1)|null|
|**2025-01-28**|**Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**|Keqi Han et.al.|[2501.17207v1](http://arxiv.org/abs/2501.17207v1)|[link](https://github.com/learningkeqi/rethinkingbca)|
|**2025-01-28**|**Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**|Fengpei Yuan et.al.|[2501.17206v1](http://arxiv.org/abs/2501.17206v1)|null|
|**2025-01-28**|**Efficient Knowledge Distillation of SAM for Medical Image Segmentation**|Kunal Dasharath Patil et.al.|[2501.16740v1](http://arxiv.org/abs/2501.16740v1)|null|
|**2025-01-28**|**VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**|Philip Chung et.al.|[2501.16672v1](http://arxiv.org/abs/2501.16672v1)|[link](https://github.com/philipchung/verifact)|
|**2025-01-28**|**Vision-based autonomous structural damage detection using data-driven methods**|Seyyed Taghi Ataei et.al.|[2501.16662v2](http://arxiv.org/abs/2501.16662v2)|null|
|**2025-01-28**|**Molecular-driven Foundation Model for Oncologic Pathology**|Anurag Vaidya et.al.|[2501.16652v1](http://arxiv.org/abs/2501.16652v1)|null|
|**2025-01-27**|**Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**|Yi Mao et.al.|[2502.00045v1](http://arxiv.org/abs/2502.00045v1)|null|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282v1](http://arxiv.org/abs/2501.16282v1)|null|
|**2025-01-27**|**Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**|Huayu Li et.al.|[2501.16215v1](http://arxiv.org/abs/2501.16215v1)|[link](https://github.com/HuayuLiArizona/Conformalized-Multiple-Instance-Learning-For-MedTS)|
|**2025-01-27**|**Atla Selene Mini: A General Purpose Evaluation Model**|Andrei Alexandru et.al.|[2501.17195v1](http://arxiv.org/abs/2501.17195v1)|[link](https://huggingface.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B)|
|**2025-01-27**|**An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**|Shaheer Ahmad Khan et.al.|[2501.15969v1](http://arxiv.org/abs/2501.15969v1)|null|
|**2025-01-27**|**Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**|Taymaz Akan et.al.|[2501.15733v1](http://arxiv.org/abs/2501.15733v1)|null|
|**2025-01-27**|**A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**|Dong Li et.al.|[2501.15724v1](http://arxiv.org/abs/2501.15724v1)|null|
|**2025-01-26**|**Beyond Benchmarks: On The False Promise of AI Regulation**|Gabriel Stanovsky et.al.|[2501.15693v1](http://arxiv.org/abs/2501.15693v1)|null|
|**2025-01-26**|**Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**|Mahshid shiri et.al.|[2501.15572v1](http://arxiv.org/abs/2501.15572v1)|null|
|**2025-01-26**|**AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**|Muhammad Aftab et.al.|[2501.15489v1](http://arxiv.org/abs/2501.15489v1)|null|
|**2025-01-26**|**Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**|Solha Kang et.al.|[2501.15452v1](http://arxiv.org/abs/2501.15452v1)|null|
|**2025-01-25**|**An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**|Arya Rahgozar et.al.|[2501.17181v1](http://arxiv.org/abs/2501.17181v1)|null|
|**2025-01-25**|**Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**|Harshita Chopra et.al.|[2501.15056v1](http://arxiv.org/abs/2501.15056v1)|null|
|**2025-01-24**|**Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**|Md. Kamrul Hasan et.al.|[2501.14929v1](http://arxiv.org/abs/2501.14929v1)|null|
|**2025-01-24**|**Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**|Hang Luo et.al.|[2501.14892v1](http://arxiv.org/abs/2501.14892v1)|null|
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719v1](http://arxiv.org/abs/2501.14719v1)|null|
|**2025-01-24**|**GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**|Ziwen Li et.al.|[2501.16382v1](http://arxiv.org/abs/2501.16382v1)|[link](https://github.com/aaronli43/grappi)|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693v1](http://arxiv.org/abs/2501.14693v1)|null|
|**2025-01-24**|**Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**|Dmitry Ryabtsev et.al.|[2501.14689v1](http://arxiv.org/abs/2501.14689v1)|null|
|**2025-01-24**|**Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**|Fuping Wu et.al.|[2501.14685v1](http://arxiv.org/abs/2501.14685v1)|null|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654v1](http://arxiv.org/abs/2501.14654v1)|[link](https://github.com/stanfordmlgroup/medagentbench)|
|**2025-01-24**|**Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**|Xu Chen et.al.|[2501.18614v1](http://arxiv.org/abs/2501.18614v1)|null|
|**2025-01-24**|**Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**|Walid Yassine et.al.|[2501.14483v1](http://arxiv.org/abs/2501.14483v1)|null|
|**2025-01-24**|**Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**|Taehan Kim et.al.|[2501.14469v1](http://arxiv.org/abs/2501.14469v1)|null|
|**2025-01-24**|**ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**|Yoni Schirris et.al.|[2501.14379v1](http://arxiv.org/abs/2501.14379v1)|[link](https://github.com/nki-ai/ectil)|

#### Abstracts
##### **Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin**
2502.03396v1 by Sarah Al-Shareeda, Yasar Celik, Bilge Bilgili, Ahmed Al-Dubai, Berk Canberk

Creating a Digital Twin (DT) for Healthcare Intelligent Transportation
Systems (HITS) is a hot research trend focusing on enhancing HITS management,
particularly in emergencies where ambulance vehicles must arrive at the crash
scene on time and track their real-time location is crucial to the medical
authorities. Despite the claim of real-time representation, a temporal
misalignment persists between the physical and virtual domains, leading to
discrepancies in the ambulance's location representation. This study proposes
integrating AI predictive models, specifically Support Vector Regression (SVR)
and Deep Neural Networks (DNN), within a constructed mock DT data pipeline
framework to anticipate the medical vehicle's next location in the virtual
world. These models align virtual representations with their physical
counterparts, i.e., metaphorically offsetting the synchronization delay between
the two worlds. Trained meticulously on a historical geospatial dataset, SVR
and DNN exhibit exceptional prediction accuracy in MATLAB and Python
environments. Through various testing scenarios, we visually demonstrate the
efficacy of our methodology, showcasing SVR and DNN's key role in significantly
reducing the witnessed gap within the HITS's DT. This transformative approach
enhances real-time synchronization in emergency HITS by approximately 88% to
93%.

æè¦ï¼å»ºç«é«çæºæ§äº¤éç³»çµ±ï¼HITSï¼çæ¸ä½åèº«ï¼DTï¼æ¯ç±éçç ç©¶è¶¨å¢ï¼å¶éé»å¨æ¼æå HITS ç®¡çï¼ç¹å¥æ¯å¨æè­·è»å¿é æºææµéè»ç¦ç¾å ´çç·æ¥ææ³ä¸­ï¼è¿½è¹¤å¶å³æä½ç½®å°æ¼é«çå®ä½è³ééè¦ãåç®¡è²ç¨±å³æåç¾ï¼ä½å¯¦é«åèæ¬é åä¹éä»å­å¨æéä¸çé¯ä½ï¼å°è´æè­·è»ä½ç½®åç¾ä¸çå·®ç°ãæ¬ç ç©¶å»ºè­°å¨å»ºæ§çèæ¬ DT è³æç®¡éæ¶æ§ä¸­æ´åäººå·¥æºæ§é æ¸¬æ¨¡åï¼ç¹å¥æ¯æ¯æ´åéåæ­¸ï¼SVRï¼åæ·±åº¦ç¥ç¶ç¶²è·¯ï¼DNNï¼ï¼ä»¥é æ¸¬é«çè»è¼å¨èæ¬ä¸ççä¸ä¸åä½ç½®ãéäºæ¨¡åå°èæ¬åç¾èå¶å¯¦é«å°æç©å°é½ï¼ä¹å°±æ¯èªªï¼å¨å©åä¸çä¹éæ¯å»æ§å°æµé·åæ­¥å»¶é²ãå¨æ­·å²å°çç©ºéè³æéä¸ç¶éä»ç´°è¨ç·´ï¼SVR å DNN å¨ MATLAB å Python ç°å¢ä¸­å±ç¾åºåè¶çé æ¸¬æºç¢ºæ§ãééåç¨®æ¸¬è©¦æå¢ï¼æåè¦è¦ºåå±ç¤ºäºæåæ¹æ³è«çæè½ï¼å±ç¤ºäº SVR å DNN å¨é¡¯èç¸®å° HITS ç DT ä¸­è¦è­å°çå·®è·æ¹é¢çééµä½ç¨ãéç¨®è®é©æ§çæ¹æ³å°ç·æ¥ HITS ä¸­çå³æåæ­¥æåäºå¤§ç´ 88% å° 93%ã

##### **RadVLM: A Multitask Conversational Vision-Language Model for Radiology**
2502.03333v1 by Nicolas Deperrois, Hidetoshi Matsuo, Samuel RuipÃ©rez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas M. Sutter, Julia E. Vogt, Jonas Kluckert, Thomas Frauenfelder, Christian BlÃ¼thgen, Farhad Nooralahzadeh, Michael Krauthammer

The widespread use of chest X-rays (CXRs), coupled with a shortage of
radiologists, has driven growing interest in automated CXR analysis and
AI-assisted reporting. While existing vision-language models (VLMs) show
promise in specific tasks such as report generation or abnormality detection,
they often lack support for interactive diagnostic capabilities. In this work
we present RadVLM, a compact, multitask conversational foundation model
designed for CXR interpretation. To this end, we curate a large-scale
instruction dataset comprising over 1 million image-instruction pairs
containing both single-turn tasks -- such as report generation, abnormality
classification, and visual grounding -- and multi-turn, multi-task
conversational interactions. After fine-tuning RadVLM on this instruction
dataset, we evaluate it across different tasks along with re-implemented
baseline VLMs. Our results show that RadVLM achieves state-of-the-art
performance in conversational capabilities and visual grounding while remaining
competitive in other radiology tasks. Ablation studies further highlight the
benefit of joint training across multiple tasks, particularly for scenarios
with limited annotated data. Together, these findings highlight the potential
of RadVLM as a clinically relevant AI assistant, providing structured CXR
interpretation and conversational capabilities to support more effective and
accessible diagnostic workflows.

æè¦ï¼è¸é¨ X å (CXR) çå¹¿æ³ä½¿ç¨ï¼å ä¸æ¾å°ç§é«å¸«ç­ç¼ºï¼ä¿ä½¿äººåå°èªåå CXR åæå AI è¼å©å ±åç¢çè¶ä¾è¶æ¿åçèè¶£ãéç¶ç¾æçè¦è¦ºèªè¨æ¨¡å (VLM) å¨ç¹å®ä»»åä¸­é¡¯ç¤ºåºåæ¯ï¼ä¾å¦å ±åçææç°å¸¸åµæ¸¬ï¼ä½å®åéå¸¸ç¼ºä¹å°äºåå¼è¨ºæ·åè½çæ¯æãå¨éé å·¥ä½ä¸­ï¼æåæåº RadVLMï¼éæ¯ä¸åç·æ¹çå¤ä»»åå°è©±å¼åºç¤æ¨¡åï¼å°çº CXR è§£éèè¨­è¨ãçºæ­¤ï¼æåç­åäºä¸åå¤§åæä»¤è³æéï¼åå«è¶é 100 è¬åå½±åæä»¤å°ï¼å¶ä¸­åå«å®è¼ªä»»åï¼ä¾å¦å ±åçæãç°å¸¸åé¡åè¦è¦ºåºç¤ï¼ï¼ä»¥åå¤è¼ªãå¤ä»»åå°è©±äºåãå¨å°éåæä»¤è³æéé²è¡å¾®èª¿å¾ï¼æåå° RadVLM é²è¡è©ä¼°ï¼ä¸¦èéæ°å¯¦ä½çåºæº VLM ä¸èµ·å·è¡ä¸åçä»»åãæåççµæé¡¯ç¤ºï¼RadVLM å¨å°è©±è½ååè¦è¦ºåºç¤æ¹é¢åå¾äºæåé²çæè½ï¼åæå¨å¶ä»æ¾å°å­¸ä»»åä¸­ä»å·æç«¶ç­åãæ¶èç ç©¶é²ä¸æ­¥çªé¡¯äºè·¨å¤åä»»åé²è¡è¯åè¨ç·´çå¥½èï¼ç¹å¥æ¯å°æ¼å¸¶ææ¨è¨»è³ææéçå ´æ¯ãéäºç¼ç¾å±åçªé¡¯äº RadVLM ä½çºè¨åºç¸é AI å©ççæ½åï¼æä¾çµæ§åç CXR è§£éåå°è©±è½åï¼ä»¥æ¯æ´æ´ææä¸å¯å­åçè¨ºæ·å·¥ä½æµç¨ã

##### **MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters**
2502.03298v1 by Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich

While increasing patients' access to medical documents improves medical care,
this benefit is limited by varying health literacy levels and complex medical
terminology. Large language models (LLMs) offer solutions by simplifying
medical information. However, evaluating LLMs for safe and patient-friendly
text generation is difficult due to the lack of standardized evaluation
resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset
created from MIMIC-IV discharge summaries through an automated pipeline
combining LLM-based question-answer generation with manual quality checks. We
use this dataset to evaluate various LLMs on patient-oriented
question-answering. Our findings reveal that general-purpose LLMs frequently
surpass biomedical-adapted models, while automated metrics correlate with human
judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the
development of LLMs to enhance patient understanding and ultimately improve
care outcomes.

æè¦ï¼åç®¡è®æ£èæ´è½åå¾é«çæä»¶æå©æ¼æ¹åé«çç§è­·ï¼
ä½æ­¤åªé»åå°ä¸åçå¥åº·ç´ é¤ç¨åº¦åè¤éçé«çè¡èªæéå¶ãå¤§åèªè¨æ¨¡å (LLM) æä¾äºç°¡åé«çè³è¨çè§£æ±ºæ¹æ¡ãç¶èï¼ç±æ¼ç¼ºä¹æ¨æºåçè©ä¼°è³æºï¼å æ­¤é£ä»¥è©ä¼° LLM ä»¥ç¢ºä¿å¶å®å¨ä¸å°æ£èååçæå­ç¢çãçºäºå¡«è£æ­¤ç¼ºå£ï¼æåéç¼äº MeDiSumQAãMeDiSumQA æ¯ééèªååæµç¨å¾ MIMIC-IV åºé¢æè¦ä¸­å»ºç«çè³æéï¼çµåäºåºæ¼ LLM çåç­ç¢çåæååè³ªæª¢æ¥ãæåä½¿ç¨æ­¤è³æéä¾è©ä¼°åç¨® LLM å¨ä»¥æ£èçºå°åçåç­ä¸­ãæåçç¼ç¾é¡¯ç¤ºï¼éç¨ LLM ç¶å¸¸è¶è¶çç©é«å­¸é©ææ¨¡åï¼èèªååææ¨èäººé¡å¤æ·ç¸éãééå¨ PhysioNet ä¸ç¼å¸ MeDiSumQAï¼æåæ¨å¨æ¨å LLM çç¼å±ï¼ä»¥å¢é²æ£èçè§£ï¼ä¸¦æçµæ¹åç§è­·ææã

##### **Deep Learning Pipeline for Fully Automated Myocardial Infarct Segmentation from Clinical Cardiac MR Scans**
2502.03272v1 by Matthias Schwab, Mathias Pamminger, Christian Kremser, Agnes Mayr

Purpose: To develop and evaluate a deep learning-based method that allows to
perform myocardial infarct segmentation in a fully-automated way.
  Materials and Methods: For this retrospective study, a cascaded framework of
two and three-dimensional convolutional neural networks (CNNs), specialized on
identifying ischemic myocardial scars on late gadolinium enhancement (LGE)
cardiac magnetic resonance (CMR) images, was trained on an in-house training
dataset consisting of 144 examinations. On a separate test dataset from the
same institution, including images from 152 examinations obtained between 2021
and 2023, a quantitative comparison between artificial intelligence (AI)-based
segmentations and manual segmentations was performed. Further, qualitative
assessment of segmentation accuracy was evaluated for both human and
AI-generated contours by two CMR experts in a blinded experiment.
  Results: Excellent agreement could be found between manually and
automatically calculated infarct volumes ($\rho_c$ = 0.9). The qualitative
evaluation showed that compared to human-based measurements, the experts rated
the AI-based segmentations to better represent the actual extent of infarction
significantly (p < 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On
the contrary, for segmentation of microvascular obstruction (MVO), manual
measurements were still preferred (11.3% AI, 55.6% human, 33.1% equal).
  Conclusion: This fully-automated segmentation pipeline enables CMR infarct
size to be calculated in a very short time and without requiring any
pre-processing of the input images while matching the segmentation quality of
trained human observers. In a blinded experiment, experts preferred automated
infarct segmentations more often than manual segmentations, paving the way for
a potential clinical application.

æè¦ï¼<paragraph>ç®çï¼éç¼åè©ä¼°ä¸ç¨®åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³ï¼åè¨±ä»¥å¨èªåçæ¹å¼å·è¡å¿èæ¢å¡åå²ã
ææåæ¹æ³ï¼å°æ¼éé åé¡§æ§ç ç©¶ï¼ä¸åç±äºç¶­åä¸ç¶­å·ç©ç¥ç¶ç¶²è·¯ (CNN) çµæçä¸²è¯æ¶æ§ï¼å°éç¨æ¼è­å¥ææéå¢å¼· (LGE) å¿èç£æ¯é å½± (CMR) å½±åä¸çç¼ºè¡æ§å¿èç¤çï¼ä¸¦å¨åå« 144 é æª¢æ¥çå§é¨è¨ç·´è³æéä¸åè¨ãå¨ä¾èªåä¸å®¶æ©æ§çç¨ç«æ¸¬è©¦è³æéä¸ï¼åæ¬ 2021 å¹´è³ 2023 å¹´éç²å¾ç 152 é æª¢æ¥çå½±åï¼å·è¡åºæ¼äººå·¥æºæ§ (AI) çåå²åæååå²ä¹éçå®éæ¯è¼ãæ­¤å¤ï¼ç±å©ä½ CMR å°å®¶å¨ç²æ¸¬å¯¦é©ä¸­è©ä¼°äººé¡å AI çæçè¼ªå»çåå²æºç¢ºåº¦ã
çµæï¼å¨æååèªåè¨ç®çæ¢å¡é«ç©ä¹éå¯ä»¥ç¼ç¾æ¥µä½³çä¸è´æ§ï¼Ï_c = 0.9ï¼ãå®æ§è©ä¼°é¡¯ç¤ºï¼èåºæ¼äººé¡çæ¸¬éç¸æ¯ï¼å°å®¶è©ä¼° AI åºæ¼åå²è½æ´è½ä»£è¡¨æ¢å¡çå¯¦éç¯åï¼é¡¯èï¼p < 0.001ï¼æ´å¸¸ç¼çï¼33.4% AIï¼25.1% äººé¡ï¼41.5% ç¸ç­ï¼ãç¸åï¼å°æ¼å¾®è¡ç®¡é»å¡ (MVO) çåå²ï¼æåæ¸¬éä»ç¶è¼åéçï¼11.3% AIï¼55.6% äººé¡ï¼33.1% ç¸ç­ï¼ã
çµè«ï¼éåå¨èªååå²ç®¡éå¯ä»¥å¨å¾ç­çæéå§è¨ç® CMR æ¢å¡å¤§å°ï¼èä¸ç¡éå°è¼¸å¥å½±åé²è¡ä»»ä½åèçï¼åæå¹éåéè¨ç·´çäººé¡è§å¯èçåå²åè³ªãå¨ç²æ¸¬å¯¦é©ä¸­ï¼å°å®¶æ¯æååå²æ´å¸¸åå¥½èªåæ¢å¡åå²ï¼çºæ½å¨çè¨åºæç¨éªå¹³äºéè·¯ã</paragraph>

##### **Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration**
2502.03238v1 by Li Pan, Yupei Zhang, Qiushi Yang, Tan Li, Zhen Chen

Recently computer-aided diagnosis has demonstrated promising performance,
effectively alleviating the workload of clinicians. However, the inherent
sample imbalance among different diseases leads algorithms biased to the
majority categories, leading to poor performance for rare categories. Existing
works formulated this challenge as a long-tailed problem and attempted to
tackle it by decoupling the feature representation and classification. Yet, due
to the imbalanced distribution and limited samples from tail classes, these
works are prone to biased representation learning and insufficient classifier
calibration. To tackle these problems, we propose a new Long-tailed Medical
Diagnosis (LMD) framework for balanced medical image classification on
long-tailed datasets. In the initial stage, we develop a Relation-aware
Representation Learning (RRL) scheme to boost the representation ability by
encouraging the encoder to capture intrinsic semantic features through
different data augmentations. In the subsequent stage, we propose an Iterative
Classifier Calibration (ICC) scheme to calibrate the classifier iteratively.
This is achieved by generating a large number of balanced virtual features and
fine-tuning the encoder using an Expectation-Maximization manner. The proposed
ICC compensates for minority categories to facilitate unbiased classifier
optimization while maintaining the diagnostic knowledge in majority classes.
Comprehensive experiments on three public long-tailed medical datasets
demonstrate that our LMD framework significantly surpasses state-of-the-art
approaches. The source code can be accessed at
https://github.com/peterlipan/LMD.

æè¦ï¼<paragraph>è¿æï¼è®¡ç®æºè¾å©è¯æ­å·²å±ç°åºè¯å¥½çæ§è½ï¼ææåè½»äºä¸´åºå»å¸çå·¥ä½éãç¶èï¼ä¸åç¾çä¹é´åºæçæ ·æ¬ä¸å¹³è¡¡å¯¼è´ç®æ³ååäºå¤æ°ç±»å«ï¼ä»èå¯¼è´ç½è§ç±»å«æ§è½ä¸ä½³ãç°æå·¥ä½å°è¿ä¸ææè¡¨è¿°ä¸ºé¿å°¾é®é¢ï¼å¹¶å°è¯éè¿è§£è¦ç¹å¾è¡¨ç¤ºååç±»æ¥è§£å³å®ãç¶èï¼ç±äºä¸å¹³è¡¡åå¸åå°¾é¨ç±»å«çæ ·æ¬æéï¼è¿äºå·¥ä½å®¹æåºç°åå·®çè¡¨ç¤ºå­¦ä¹ åä¸è¶³çåç±»å¨æ ¡åãä¸ºäºè§£å³è¿äºé®é¢ï¼æä»¬æåºäºä¸ç§æ°çé¿å°¾å»å­¦è¯æ­ (LMD) æ¡æ¶ï¼ç¨äºå¯¹é¿å°¾æ°æ®éè¿è¡å¹³è¡¡çå»å­¦å¾ååç±»ãå¨åå§é¶æ®µï¼æä»¬å¼åäºä¸ç§å³ç³»æç¥è¡¨ç¤ºå­¦ä¹  (RRL) æ¹æ¡ï¼éè¿é¼å±ç¼ç å¨éè¿ä¸åçæ°æ®å¢å¼ºæ¥æè·åå¨è¯­ä¹ç¹å¾ï¼ä»èæåè¡¨ç¤ºè½åãå¨åç»­é¶æ®µï¼æä»¬æåºäºä¸ç§è¿­ä»£åç±»å¨æ ¡å (ICC) æ¹æ¡ï¼ä»¥è¿­ä»£æ ¡ååç±»å¨ãè¿æ¯éè¿çæå¤§éå¹³è¡¡çèæç¹å¾å¹¶ä½¿ç¨æææå¤§åæ¹å¼å¾®è°ç¼ç å¨æ¥å®ç°çãææåºç ICC è¡¥å¿äºå°æ°ç±»å«ï¼ä»¥ä¿è¿æ åå·®çåç±»å¨ä¼åï¼åæ¶ä¿æå¤æ°ç±»å«çè¯æ­ç¥è¯ãå¨ä¸ä¸ªå¬å¼çé¿å°¾å»å­¦æ°æ®éä¸çç»¼åå®éªè¡¨æï¼æä»¬ç LMD æ¡æ¶ææ¾ä¼äºæåè¿çæ¹æ³ãæºä»£ç å¯å¨ https://github.com/peterlipan/LMD è·åã</paragraph>

##### **MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation**
2502.03004v1 by Seonok Kim

Large Language Models (LLMs) have demonstrated impressive capabilities across
natural language processing tasks. However, their application to specialized
domains such as medicine and biology requires further optimization to ensure
factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a
domain-adapted biomedical question-answering model designed to enhance both
short-form and long-form queries. By integrating fine-tuning and
retrieval-augmented generation (RAG), MedBioLM dynamically incorporates
domain-specific knowledge, improving reasoning abilities and factual accuracy.
To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA
datasets, covering structured multiple-choice assessments and complex clinical
reasoning tasks. Fine-tuning significantly improves accuracy on benchmark
datasets, while RAG enhances factual consistency. These results highlight the
potential of domain-optimized LLMs in advancing biomedical research, medical
education, and clinical decision support.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å±ç¾åºå¨èªç¶èªè¨èçä»»åä¸­ä»¤äººå°è±¡æ·±å»çè½åãç¶èï¼è¦å°å¶æç¨æ¼é«å­¸åçç©å­¸ç­ç¹å®é åï¼éè¦é²ä¸æ­¥æä½³åï¼ä»¥ç¢ºä¿äºå¯¦çæºç¢ºæ§ãå¯é æ§ä»¥åèçµ¡çæ·±åº¦ãæåå¼é²äº MedBioLMï¼éæ¯ä¸åé©æé åççç©é«å­¸åç­æ¨¡åï¼æ¨å¨å¢å¼·ç­å¼åé·å¼æ¥è©¢ãééæ´åå¾®èª¿åæª¢ç´¢å¢å¼·çæ (RAG)ï¼MedBioLM è½åæå°ç´å¥é åç¹å®çç¥è­ï¼å¾èæåæ¨çè½ååäºå¯¦æºç¢ºæ§ãçºäºè©ä¼°å¶æææ§ï¼æåå°æ¨¡åé²è¡å¾®èª¿ï¼ä½¿å¶æ¶µèçµæ§åçå¤éé¸æè©éåè¤éçè¨åºæ¨çä»»åç­å¤æ¨£åççç©é«å­¸åç­è³æéãå¾®èª¿é¡¯èæåäºåºæºè³æéçæºç¢ºæ§ï¼è RAG åå¢å¼·äºäºå¯¦çä¸è´æ§ãéäºçµæçªé¡¯äºé åæä½³åç LLM å¨æ¨é²çç©é«å­¸ç ç©¶ãé«å­¸æè²åè¨åºæ±ºç­æ¯æ´æ¹é¢çæ½åã

##### **3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography**
2502.02779v1 by Weicheng Zhu, Haoxu Huang, Huanze Tang, Rushabh Musthyala, Boyang Yu, Long Chen, Emilio Vega, Thomas O'Donnell, Seena Dehkharghani, Jennifer A. Frontera, Arjun V. Masurkar, Kara Melmed, Narges Razavian

Head computed tomography (CT) imaging is a widely-used imaging modality with
multitudes of medical indications, particularly in assessing pathology of the
brain, skull, and cerebrovascular system. It is commonly the first-line imaging
in neurologic emergencies given its rapidity of image acquisition, safety,
cost, and ubiquity. Deep learning models may facilitate detection of a wide
range of diseases. However, the scarcity of high-quality labels and
annotations, particularly among less common conditions, significantly hinders
the development of powerful models. To address this challenge, we introduce
FM-CT: a Foundation Model for Head CT for generalizable disease detection,
trained using self-supervised learning. Our approach pre-trains a deep learning
model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans
without the need for manual annotations, enabling the model to learn robust,
generalizable features. To investigate the potential of self-supervised
learning in head CT, we employed both discrimination with self-distillation and
masked image modeling, and we construct our model in 3D rather than at the
slice level (2D) to exploit the structure of head CT scans more comprehensively
and efficiently. The model's downstream classification performance is evaluated
using internal and three external datasets, encompassing both in-distribution
(ID) and out-of-distribution (OOD) data. Our results demonstrate that the
self-supervised foundation model significantly improves performance on
downstream diagnostic tasks compared to models trained from scratch and
previous 3D CT foundation models on scarce annotated datasets. This work
highlights the effectiveness of self-supervised learning in medical imaging and
sets a new benchmark for head CT image analysis in 3D, enabling broader use of
artificial intelligence for head CT-based diagnosis.

æè¦ï¼é ­é¨é»è¦æ·å±¤ææï¼CTï¼å½±åæ¯ä¸ç¨®å»£æ³ä½¿ç¨çå½±åæ¨¡å¼ï¼å·æ
å¤§éçé«çé©æçï¼ç¹å¥æ¯å¨è©ä¼°è¦é¨ãé ­éª¨åè¦è¡ç®¡ç³»çµ±çççæãç±æ¼å¶å½±åæ·åéåº¦å¿«ãå®å¨æ§ãææ¬ä½åæ®éæ§ï¼éå¸¸æ¯ç¥ç¶ç·æ¥ææ³ä¸çç¬¬ä¸ç·å½±åãæ·±åº¦å­¸ç¿æ¨¡åå¯ä»¥ä¿é²å°åç¨®ç¾ççæª¢æ¸¬ãç¶èï¼é«åè³ªæ¨ç±¤åè¨»éçç¨ç¼ºï¼ç¹å¥æ¯å¨è¼ä¸å¸¸è¦çç¾çä¸­ï¼é¡¯èå°é»ç¤äºå¼·å¤§æ¨¡åçç¼å±ãçºäºæå°éä¸ææ°ï¼æåå¼å¥äº FM-CTï¼ä¸åç¨æ¼é ­é¨ CT çåºç¤æ¨¡åï¼ç¨æ¼å¯æ¦åçç¾çæª¢æ¸¬ï¼ä¸¦ä½¿ç¨èªæç£ç£å­¸ç¿é²è¡è¨ç·´ãæåçåæ³å¨ä¸ååå« 361,663 åéå°æ¯ 3D é ­é¨ CT ææçå¤§åãå¤æ¨£åçæ¸æéä¸é è¨ç·´ä¸åæ·±åº¦å­¸ç¿æ¨¡åï¼èç¡éæåè¨»éï¼ä½¿æ¨¡åè½å¤ å­¸ç¿å¼·å¥ãå¯æ¦åçç¹å¾µãçºäºæ¢è¨èªæç£ç£å­¸ç¿å¨é ­é¨ CT ä¸­çæ½åï¼æååææ¡ç¨äºå¸¶æèªæè¸é¤¾çå¤å¥åé®ç½©å½±åå»ºæ¨¡ï¼ä¸¦ä¸æåä»¥ 3D èä¸æ¯åçå±¤ç´ï¼2Dï¼æ§å»ºæåçæ¨¡åï¼ä»¥æ´å¨é¢ãææå°å©ç¨é ­é¨ CT ææççµæ§ãè©²æ¨¡åçä¸æ¸¸åé¡æè½ä½¿ç¨å§é¨åä¸åå¤é¨æ¸æéé²è¡è©ä¼°ï¼åæ¬åä½å§ (ID) ååä½å¤ (OOD) è³æãæåççµæè¡¨æï¼èå¾é ­éå§è¨ç·´çæ¨¡ååååå¨ç¨çè¨»éæ¸æéä¸è¨ç·´ç 3D CT åºç¤æ¨¡åç¸æ¯ï¼èªæç£ç£åºç¤æ¨¡åé¡¯èæ¹åäºä¸æ¸¸è¨ºæ·ä»»åçæè½ãéé å·¥ä½çªé¡¯äºèªæç£ç£å­¸ç¿å¨é«å­¸å½±åä¸­çæææ§ï¼ä¸¦çº 3D é ­é¨ CT å½±ååæè¨­å®äºä¸åæ°çåºæºï¼è®äººå·¥æºæ§è½å¤ æ´å»£æ³å°ç¨æ¼åºæ¼é ­é¨ CT çè¨ºæ·ã

##### **Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images**
2502.02756v1 by Obed Korshie Dzikunu, Shadab Ahamed, Amirhossein Toosi, Xiaoxiao Li, Arman Rahmim

This study proposes a new loss function for deep neural networks, L1-weighted
Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of
voxels based on their classification difficulty, towards automated detection
and segmentation of metastatic prostate cancer lesions in PET/CT scans. We
obtained 380 PSMA [18-F] DCFPyL PET/CT scans of patients diagnosed with
biochemical recurrence metastatic prostate cancer. We trained two 3D
convolutional neural networks, Attention U-Net and SegResNet, and concatenated
the PET and CT volumes channel-wise as input. The performance of our custom
loss function was evaluated against the Dice and Dice Focal Loss functions. For
clinical significance, we considered a detected region of interest (ROI) as a
true positive if at least the voxel with the maximum standardized uptake value
falls within the ROI. We assessed the models' performance based on the number
of lesions in an image, tumour volume, activity, and extent of spread. The
L1DFL outperformed the comparative loss functions by at least 13% on the test
set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were
lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal
Loss yielded more false positives, whereas the Dice Loss was more sensitive to
smaller volumes and struggled to segment larger lesions accurately. They also
exhibited network-specific variations and yielded declines in segmentation
accuracy with increased tumour spread. Our results demonstrate the potential of
L1DFL to yield robust segmentation of metastatic prostate cancer lesions in
PSMA PET/CT images. The results further highlight potential complexities
arising from the variations in lesion characteristics that may influence
automated prostate cancer tumour detection and segmentation. The code is
publicly available at: https://github.com/ObedDzik/pca_segment.git.

æè¦ï¼<paragraph>æ¬ç ç©¶éå°æ·±åº¦ç¥ç¶ç¶²è·¯æåºä¸åæ°çæå¤±å½æ¸ï¼L1 å æ¬ Dice ç¦é»æå¤± (L1DFL)ï¼å®å©ç¨ L1 ç¯æ¸æ ¹æé«ç´ çåé¡é£åº¦é²è¡èªé©æå æ¬ï¼ç¨æ¼èªååµæ¸¬ååå² PET/CT ææä¸­è½ç§»æ§ååèºççç¶ãæååå¾ 380 åç¶è¨ºæ·çºçåå¾©ç¼è½ç§»æ§ååèºççæ£èç PSMA [18-F] DCFPyL PET/CT ææãæåè¨ç·´äºå©å 3D æ²ç©ç¥ç¶ç¶²è·¯ï¼Attention U-Net å SegResNetï¼ä¸¦å° PET å CT é«ç©æééé£æ¥ä½çºè¼¸å¥ãæåèªè¨çæå¤±å½æ¸çæè½è Dice å Dice ç¦é»æå¤±å½æ¸é²è¡è©ä¼°ãçºäºè¨åºæç¾©ï¼æåå°ä¸ååµæ¸¬å°çæèè¶£åå (ROI) è¦çºçé½æ§ï¼å¦æè³å°å·ææå¤§æ¨æºæåå¼çé«ç´ è½å¨ ROI å§ãæåæ ¹æå½±åä¸­ççç¶æ¸éãè«ç¤é«ç©ãæ´»æ§ï¼ä»¥åæ´æ£ç¨åº¦è©ä¼°æ¨¡åçæè½ãL1DFL å¨æ¸¬è©¦çµä¸­è³å°æ¯æ¯è¼æå¤±å½æ¸é«åº 13%ãæ­¤å¤ï¼Dice æå¤±å Dice ç¦é»æå¤±ç F1 åæ¸åå¥æ¯ L1DFL ä½è³å° 6% å 34%ãDice ç¦é»æå¤±ç¢çæ´å¤åé½æ§ï¼è Dice æå¤±å°è¼å°é«ç©è¼çºææï¼ä¸é£ä»¥æºç¢ºåå²è¼å¤§çç¶ãå®åä¹å±ç¾åºç¶²è·¯ç¹å®çè®åï¼ä¸¦é¨èè«ç¤æ´æ£èå°è´åå²æºç¢ºåº¦ä¸éãæåççµæè­æ L1DFL å·æå¨ PSMA PET/CT å½±åä¸­ç¢çè½ç§»æ§ååèºççç¶çå¼·å¥åå²çæ½åãçµæé²ä¸æ­¥å¼·èª¿ç±çç¶ç¹å¾µè®åæç¢ççæ½å¨è¤éæ§ï¼éå¯è½æå½±é¿èªååååèºçè«ç¤åµæ¸¬ååå²ãç¨å¼ç¢¼å¬éæ¼ï¼https://github.com/ObedDzik/pca_segment.gitã</paragraph>

##### **MedRAX: Medical Reasoning Agent for Chest X-ray**
2502.02673v1 by Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang

Chest X-rays (CXRs) play an integral role in driving critical decisions in
disease management and patient care. While recent innovations have led to
specialized models for various CXR interpretation tasks, these solutions often
operate in isolation, limiting their practical utility in clinical practice. We
present MedRAX, the first versatile AI agent that seamlessly integrates
state-of-the-art CXR analysis tools and multimodal large language models into a
unified framework. MedRAX dynamically leverages these models to address complex
medical queries without requiring additional training. To rigorously evaluate
its capabilities, we introduce ChestAgentBench, a comprehensive benchmark
containing 2,500 complex medical queries across 7 diverse categories. Our
experiments demonstrate that MedRAX achieves state-of-the-art performance
compared to both open-source and proprietary models, representing a significant
step toward the practical deployment of automated CXR interpretation systems.
Data and code have been publicly available at
https://github.com/bowang-lab/MedRAX

æè¦ï¼è¸é¨ X åç (CXR) å¨ç¾çç®¡çåæ£èç§è­·ä¸­æ®æ¼èä¸å¯æç¼ºçè§è²ï¼æ¨åèééµæ±ºç­çå¶å®ãåç®¡è¿æçåµæ°å·²éå°åç¨® CXR è§£è®ä»»åéç¼åºå°éçæ¨¡åï¼ä½éäºè§£æ±ºæ¹æ¡éå¸¸ç¨ç«éä½ï¼éå¶äºå®åå¨è¨åºå¯¦åä¸­çå¯¦éæç¨ãæåæåº MedRAXï¼éæ¯ä¸æ¬¾é¦åµçå¤åè½ AI ä»£çï¼å®å°æåé²ç CXR åæå·¥å·åå¤æ¨¡æå¤§åèªè¨æ¨¡åç¡ç¸«æ´åå°ä¸åçµ±ä¸çæ¶æ§ä¸­ãMedRAX åæéç¨éäºæ¨¡åä¾è§£æ±ºè¤éçé«çæ¥è©¢ï¼èç¡éé¡å¤çè¨ç·´ãçºäºå´æ ¼è©ä¼°å¶åè½ï¼æåå¼å¥äº ChestAgentBenchï¼éæ¯ä¸åå¨é¢çåºæºï¼åå« 7 åä¸åé¡å¥ç 2,500 åè¤éé«çæ¥è©¢ãæåçå¯¦é©è­æï¼èéæºåå°ææ¨¡åç¸æ¯ï¼MedRAX éå°äºæåé²çæè½ï¼éä»£è¡¨äºèªåå CXR è§£è®ç³»çµ±å¯¦éé¨ç½²çéè¦ä¸æ­¥ãè³æåç¨å¼ç¢¼å·²å¬éæ¼ https://github.com/bowang-lab/MedRAX

##### **Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents**
2502.02561v1 by Shayan Kiyani, George Pappas, Aaron Roth, Hamed Hassani

A fundamental question in data-driven decision making is how to quantify the
uncertainty of predictions in ways that can usefully inform downstream action.
This interface between prediction uncertainty and decision-making is especially
important in risk-sensitive domains, such as medicine. In this paper, we
develop decision-theoretic foundations that connect uncertainty quantification
using prediction sets with risk-averse decision-making. Specifically, we answer
three fundamental questions: (1) What is the correct notion of uncertainty
quantification for risk-averse decision makers? We prove that prediction sets
are optimal for decision makers who wish to optimize their value at risk. (2)
What is the optimal policy that a risk averse decision maker should use to map
prediction sets to actions? We show that a simple max-min decision policy is
optimal for risk-averse decision makers. Finally, (3) How can we derive
prediction sets that are optimal for such decision makers? We provide an exact
characterization in the population regime and a distribution free finite-sample
construction. Answering these questions naturally leads to an algorithm,
Risk-Averse Calibration (RAC), which follows a provably optimal design for
deriving action policies from predictions. RAC is designed to be both
practical-capable of leveraging the quality of predictions in a black-box
manner to enhance downstream utility-and safe-adhering to a user-defined risk
threshold and optimizing the corresponding risk quantile of the user's
downstream utility. Finally, we experimentally demonstrate the significant
advantages of RAC in applications such as medical diagnosis and recommendation
systems. Specifically, we show that RAC achieves a substantially improved
trade-off between safety and utility, offering higher utility compared to
existing methods while maintaining the safety guarantee.

æè¦ï¼<paragraph>å¨è³æé©åæ±ºç­ä¸­ï¼ä¸ååºæ¬åé¡æ¯ï¼å¦ä½éåé æ¸¬çä¸ç¢ºå®æ§ï¼ä»¥è½æç¨å°åç¥ä¸æ¸¸è¡åã
é æ¸¬ä¸ç¢ºå®æ§åæ±ºç­å¶å®ä¹éçéç¨®ä»é¢ï¼å¨é¢¨éªææé åä¸­ç¹å¥éè¦ï¼ä¾å¦é«å­¸ãå¨æ¬æä¸­ï¼æå
ç¼å±äºæ±ºç­çè«åºç¤ï¼å®å©ç¨é æ¸¬éåå°ä¸ç¢ºå®æ§éåèé¢¨éªè¦é¿æ±ºç­å¶å®è¯ç¹«èµ·ä¾ãå·é«ä¾èªªï¼æååç­
äºä¸ååºæ¬åé¡ï¼(1) å°æ¼é¢¨éªè¦é¿æ±ºç­èä¾èªªï¼ä¸ç¢ºå®æ§éåçæ­£ç¢ºæ¦å¿µæ¯ä»éº¼ï¼æåè­æï¼å°æ¼å¸ææä½³åå¶é¢¨éªå¹å¼çæ±ºç­èä¾èªªï¼é æ¸¬éåæ¯æä½³çã(2)
é¢¨éªè¦é¿æ±ºç­èæä½¿ç¨ä»éº¼æä½³æ¿ç­ï¼å°é æ¸¬éåæ å°å°è¡åï¼æåè¡¨æï¼å°æ¼é¢¨éªè¦é¿æ±ºç­èä¾èªªï¼ä¸åç°¡å®çæå¤§æå°æ±ºç­æ¿ç­æ¯æä½³çãæå¾ï¼(3) æåå¦ä½æ¨å°åºå°æ­¤é¡æ±ºç­èä¾èªªæä½³çé æ¸¬éåï¼æåå¨ç¸½é«ç¯åå§æä¾äºä¸åç¢ºåçè¡¨å¾µï¼ä¸¦æä¾äºä¸åä¸ä¾è³´åä½çæéæ¨£æ¬å»ºæ§ãåç­éäºåé¡èªç¶æå°è´ä¸åæ¼ç®æ³ï¼é¢¨éªè¦é¿æ ¡æº (RAC)ï¼å®éµå¾ªä¸åå¯è­ææä½³çè¨­è¨ï¼å¾é æ¸¬ä¸­æ¨å°åºè¡åæ¿ç­ãRAC è¢«è¨­è¨çºæ¢å¯¦ç¨ââè½å¤ ä»¥é»çæ¹å¼å©ç¨é æ¸¬çåè³ªä¾å¢å¼·ä¸æ¸¸æç¨ââåå®å¨ââéµå®ä½¿ç¨èå®ç¾©çé¢¨éªé¾å¼ï¼ä¸¦æä½³åä½¿ç¨èçä¸æ¸¸æç¨çå°æé¢¨éªåä½æ¸ãæå¾ï¼æåå¨é«å­¸è¨ºæ·åæ¨è¦ç³»çµ±ç­æç¨ä¸­ï¼ä»¥å¯¦é©æ¹å¼è­æäº RAC çé¡¯èåªé»ãå·é«ä¾èªªï¼æåè¡¨æï¼èç¾ææ¹æ³ç¸æ¯ï¼RAC å¨å®å¨æ§åæç¨ä¹éå¯¦ç¾äºé¡¯èæ¹åçæè¡·ï¼å¨ç¶­æå®å¨ä¿è­çåæï¼æä¾äºæ´é«çæç¨ã</paragraph>

##### **A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation**
2502.02489v1 by Edward Ellis, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali

Ultrasound (US) imaging is clinically invaluable due to its noninvasive and
safe nature. However, interpreting US images is challenging, requires
significant expertise, and time, and is often prone to errors. Deep learning
offers assistive solutions such as segmentation. Supervised methods rely on
large, high-quality, and consistently labeled datasets, which are challenging
to curate. Moreover, these methods tend to underperform on out-of-distribution
data, limiting their clinical utility. Self-supervised learning (SSL) has
emerged as a promising alternative, leveraging unlabeled data to enhance model
performance and generalisability. We introduce a contrastive SSL approach
tailored for B-mode US images, incorporating a novel Relation Contrastive Loss
(RCL). RCL encourages learning of distinct features by differentiating positive
and negative sample pairs through a learnable metric. Additionally, we propose
spatial and frequency-based augmentation strategies for the representation
learning on US images. Our approach significantly outperforms traditional
supervised segmentation methods across three public breast US datasets,
particularly in data-limited scenarios. Notable improvements on the Dice
similarity metric include a 4% increase on 20% and 50% of the BUSI dataset,
nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4%
and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.
Furthermore, we demonstrate superior generalisability on the
out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6%
compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST
training data, respectively. Our research highlights that domain-inspired SSL
can improve US segmentation, especially under data-limited conditions.

æè¦ï¼è¶é³æ³¢ (US) å½±åç±æ¼å¶éä¾µå¥æ§ä¸å®å¨çç¹æ§ï¼å¨è¨åºä¸æ¥µå·å¹å¼ãç¶èï¼è§£è®è¶é³æ³¢å½±åå·æææ°æ§ï¼éè¦å¤§éçå°æ¥­ç¥è­åæéï¼èä¸ç¶å¸¸å®¹æåºé¯ãæ·±åº¦å­¸ç¿æä¾äºè¼å©è§£æ±ºæ¹æ¡ï¼ä¾å¦åå²ãç£ç£å¼æ¹æ³ä¾è³´æ¼å¤§éãé«åè³ªä¸æ¨ç±¤ä¸è´çè³æéï¼èéå¨ç­åä¸å·æææ°æ§ãæ­¤å¤ï¼éäºæ¹æ³å¨åä½å¤è³æä¸çè¡¨ç¾å¾å¾ä¸ä½³ï¼ééå¶äºå®åçè¨åºæç¨ãèªç£ç£å­¸ç¿ (SSL) å·²æçºä¸ç¨®æåéçæ¿ä»£æ¹æ¡ï¼å®å©ç¨æªæ¨ç±¤è³æä¾å¢å¼·æ¨¡åæè½åæ³åè½åãæåæåºäºä¸ç¨®å°æ¯å¼ SSL æ¹æ³ï¼å°ééå° B æ¨¡å¼è¶é³æ³¢å½±åï¼ä¸¦ç´å¥äºæ°ç©çéä¿å°æ¯æå¤± (RCL)ãRCL ééä¸åå¯å­¸ç¿çææ¨ååæ­£è² æ¨£æ¬å°ï¼ä¾é¼åµå­¸ç¿ä¸åçç¹å¾µãæ­¤å¤ï¼æåæåºäºç¨æ¼è¶é³æ³¢å½±åä¸è¡¨å¾µå­¸ç¿çç©ºéåé »çå¢å¼·ç­ç¥ãæåçåæ³å¨ä¸åå¬éçä¹³æ¿è¶é³æ³¢è³æéä¸é¡¯èåªæ¼å³çµ±çç£ç£å¼åå²æ¹æ³ï¼ç¹å¥æ¯å¨è³ææéçææ³ä¸ãå¨ Dice ç¸ä¼¼æ§ææ¨ä¸çé¡¯èæ¹é²åæ¬å¨ BUSI è³æéç 20% å 50% ä¸å¢å äº 4%ï¼å¨ BrEaST è³æéç 20% å 50% ä¸å¢å äºè¿ 6% å 9%ï¼ä»¥åå¨ UDIAT è³æéç 20% å 50% ä¸åå¥å¢å äº 6.4% å 3.7%ãæ­¤å¤ï¼æåå¨åä½å¤ç UDIAT è³æéä¸å±ç¤ºäºåè¶çæ³åè½åï¼èä½¿ç¨ BUSI å BrEaST è¨ç·´è³æç 20% å 50% çç£ç£å¼åºæºç¸æ¯ï¼æè½åå¥æåäº 20.6% å 13.6%ãæåçç ç©¶å¼·èª¿ï¼é ååç¼ç SSL å¯ä»¥æ¹åè¶é³æ³¢åå²ï¼ç¹å¥æ¯å¨è³ææéçæ¢ä»¶ä¸ã

##### **Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment**
2502.02438v1 by Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz

Medical multimodal large language models (MLLMs) are becoming an instrumental
part of healthcare systems, assisting medical personnel with decision making
and results analysis. Models for radiology report generation are able to
interpret medical imagery, thus reducing the workload of radiologists. As
medical data is scarce and protected by privacy regulations, medical MLLMs
represent valuable intellectual property. However, these assets are potentially
vulnerable to model stealing, where attackers aim to replicate their
functionality via black-box access. So far, model stealing for the medical
domain has focused on classification; however, existing attacks are not
effective against MLLMs. In this paper, we introduce Adversarial Domain
Alignment (ADA-STEAL), the first stealing attack against medical MLLMs.
ADA-STEAL relies on natural images, which are public and widely available, as
opposed to their medical counterparts. We show that data augmentation with
adversarial noise is sufficient to overcome the data distribution gap between
natural images and the domain-specific distribution of the victim MLLM.
Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that
Adversarial Domain Alignment enables attackers to steal the medical MLLM
without any access to medical data.

æè¦ï¼é«çå¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) æ­£å¨æçºé«çä¿å¥ç³»çµ±ä¸­ä¸å¯æç¼ºçä¸é¨åï¼åå©é«çäººå¡é²è¡æ±ºç­åçµæåæãæ¾å°å ±åçæçæ¨¡åè½å¤ è§£éé«å­¸å½±åï¼å¾èæ¸è¼æ¾å°ç§é«å¸«çå·¥ä½è² æãç±æ¼é«çè³æç¨å°ä¸åé±ç§æ³è¦ä¿è­·ï¼é«ç MLLM ä»£è¡¨äºæå¹å¼çæºæ§è²¡ç¢ãç¶èï¼éäºè³ç¢æ½å¨å°å®¹æåå°æ¨¡åç«åçæ»æï¼æ»æèæ¨å¨ééé»çå­åä¾è¤è£½å¶åè½ãå°ç®åçºæ­¢ï¼éå°é«çé åçæ¨¡åç«åä¸ç´å°æ³¨æ¼åé¡ï¼ç¶èï¼ç¾æçæ»æå° MLLM æ²ææãå¨æ¬æä¸­ï¼æåä»ç´¹äºå°æåå°é½ (ADA-STEAL)ï¼éæ¯éå°é«ç MLLM çç¬¬ä¸åç«åæ»æãèé«çå°æç©ç¸åï¼ADA-STEAL ä¾è³´æ¼å¬éä¸å»£æ³å¯ç¨çèªç¶å½±åãæåè¡¨æï¼å°æéè¨çè³ææ´åè¶³ä»¥åæèªç¶å½±åèåå®³è MLLM çç¹å®é ååä½ä¹éçè³æåä½å·®è·ãå¨ IU X-RAY å MIMIC-CXR æ¾å°å­¸è³æéä¸é²è¡çå¯¦é©è¡¨æï¼å°æåå°é½ä½¿æ»æèè½å¤ å¨ä¸å­åä»»ä½é«çè³æçææ³ä¸ç«åé«ç MLLMã

##### **Test Time Training for 4D Medical Image Interpolation**
2502.02341v1 by Qikang Zhang, Yingjie Lei, Zihao Zheng, Ziyang Chen, Zhonghao Xie

4D medical image interpolation is essential for improving temporal resolution
and diagnostic precision in clinical applications. Previous works ignore the
problem of distribution shifts, resulting in poor generalization under
different distribution. A natural solution would be to adapt the model to a new
test distribution, but this cannot be done if the test input comes without a
ground truth label. In this paper, we propose a novel test time training
framework which uses self-supervision to adapt the model to a new distribution
without requiring any labels. Indeed, before performing frame interpolation on
each test video, the model is trained on the same instance using a
self-supervised task, such as rotation prediction or image reconstruction. We
conduct experiments on two publicly available 4D medical image interpolation
datasets, Cardiac and 4D-Lung. The experimental results show that the proposed
method achieves significant performance across various evaluation metrics on
both datasets. It achieves higher peak signal-to-noise ratio values, 33.73dB on
Cardiac and 34.02dB on 4D-Lung. Our method not only advances 4D medical image
interpolation but also provides a template for domain adaptation in other
fields such as image segmentation and image registration.

æè¦ï¼4D é«å­¸å½±åæå¼å°æ¼æåæéè§£æåº¦åè¨åºæç¨ä¸­çè¨ºæ·ç²¾æºåº¦è³ééè¦ãéå¾çç ç©¶å¿½ç¥äºåä½è½ç§»åé¡ï¼å°è´å¨ä¸ååä½ä¸æ³åè½åä¸ä½³ãä¸åèªç¶çè§£æ±ºæ¹æ¡æ¯å°æ¨¡åé©æå°æ°çæ¸¬è©¦åä½ï¼ä½å¦ææ¸¬è©¦è¼¸å¥æ²æçå¯¦æ¨ç±¤ï¼å°±ç¡æ³åå°éä¸é»ãå¨æ¬æä¸­ï¼æåæåºäºä¸åæ°çæ¸¬è©¦æéè¨ç·´æ¶æ§ï¼å®ä½¿ç¨èªæç£ç£ä¾é©ææ¨¡åå°ä¸åæ°çåä½ï¼èä¸éè¦ä»»ä½æ¨ç±¤ãäºå¯¦ä¸ï¼å¨å°æ¯åæ¸¬è©¦å½±çå·è¡å¹æå¼ä¹åï¼ä½¿ç¨èªæç£ç£ä»»åï¼ä¾å¦æè½é æ¸¬æå½±åéå»ºï¼å¨åä¸åå¯¦ä¾ä¸è¨ç·´æ¨¡åãæåå¨å©åå¬éç 4D é«å­¸å½±åæå¼è³æéï¼Cardiac å 4D-Lungï¼ä¸é²è¡å¯¦é©ãå¯¦é©çµæè¡¨æï¼ææåºçæ¹æ³å¨å©åè³æéä¸çåç¨®è©ä¼°ææ¨ä¸­é½åå¾äºé¡¯èçæè½ãå®éå°äºæ´é«çå³°å¼ä¿¡åªæ¯å¼ï¼å¨ Cardiac ä¸çº 33.73dBï¼å¨ 4D-Lung ä¸çº 34.02dBãæåçæè¡ä¸åæ¨åäº 4D é«å­¸å½±åæå¼ï¼éçºå¶ä»é åï¼ä¾å¦å½±ååå²åå½±åéæºï¼ä¸­çé åé©ææä¾äºä¸åç¯æ¬ã

##### **Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation**
2502.02249v1 by Atharva Mangeshkumar Agrawal, Rutika Pandurang Shinde, Vasanth Kumar Bhukya, Ashmita Chakraborty, Sagar Bharat Shah, Tanmay Shukla, Sree Pradeep Kumar Relangi, Nilesh Mutyam

Large language models (LLMs) have shown impressive capabilities in natural
language processing tasks, including dialogue generation. This research aims to
conduct a novel comparative analysis of two prominent techniques, fine-tuning
with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)
framework, in the context of doctor-patient chat conversations with multiple
datasets of mixed medical domains. The analysis involves three state-of-the-art
models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient
dialogues, we comprehensively evaluate the performance of models, assessing key
metrics such as language quality (perplexity, BLEU score), factual accuracy
(fact-checking against medical knowledge bases), adherence to medical
guidelines, and overall human judgments (coherence, empathy, safety). The
findings provide insights into the strengths and limitations of each approach,
shedding light on their suitability for healthcare applications. Furthermore,
the research investigates the robustness of the models in handling diverse
patient queries, ranging from general health inquiries to specific medical
conditions. The impact of domain-specific knowledge integration is also
explored, highlighting the potential for enhancing LLM performance through
targeted data augmentation and retrieval strategies.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èçä»»åä¸­å±ç¾äºä»¤äººå°è±¡æ·±å»çè½åï¼åæ¬å°è©±çæãæ¬ç ç©¶æ¨å¨å°å©ç¨®èåçæè¡é²è¡æ°ç©çæ¯è¼åæï¼å³å¾®èª¿ LoRA (ä½ç§©é©æ) åæª¢ç´¢å¢å¼·çæ (RAG) æ¡æ¶ï¼å¨å·ææ··åé«çé åçå¤åè³æéçé«æ£èå¤©å°è©±ä¸­ãåææ¶åä¸åæåé²çæ¨¡åï¼Llama-2ãGPT å LSTM æ¨¡åãæ¡ç¨çå¯¦ä¸ççé«æ£å°è©±ï¼æåå¨é¢è©ä¼°æ¨¡åçæ§è½ï¼è©ä¼°èªè¨åè³ªï¼å°æåº¦ãBLEU åæ¸ï¼ãäºå¯¦æºç¢ºæ§ï¼å°ç§é«å­¸ç¥è­åº«é²è¡äºå¯¦æ¥æ ¸ï¼ãéµå®é«çæåä»¥åæ´é«äººé¡å¤æ·ï¼é£è²«æ§ãåçå¿ãå®å¨æ§ï¼ç­ééµææ¨ãç ç©¶çµææ·±å¥äºè§£äºæ¯ç¨®æ¹æ³çåªé»åéå¶ï¼é¡æäºå®åé©ç¨æ¼é«çä¿å¥æç¨çé©ç¶æ§ãæ­¤å¤ï¼è©²ç ç©¶èª¿æ¥äºæ¨¡åå¨èçå¤æ¨£åæ£èæ¥è©¢æçç©©å¥æ§ï¼ç¯åå¾ä¸è¬å¥åº·è©¢åå°ç¹å®é«ççæ³ãéæ¢è¨äºç¹å®é åç¥è­æ´åçå½±é¿ï¼å¼·èª¿äºééæéå°æ§çè³ææ´ååæª¢ç´¢ç­ç¥ä¾å¢å¼· LLM æ§è½çæ½åã

##### **Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review**
2502.02618v1 by F. Xavier Gaya-Morey, Jose M. Buades-Rubio, Philippe Palanque, Raquel Lacuesta, Cristina Manresa-Yee

The rapid aging of the global population has highlighted the need for
technologies to support elderly, particularly in healthcare and emotional
well-being. Facial expression recognition (FER) systems offer a non-invasive
means of monitoring emotional states, with applications in assisted living,
mental health support, and personalized care. This study presents a systematic
review of deep learning-based FER systems, focusing on their applications for
the elderly population. Following a rigorous methodology, we analyzed 31
studies published over the last decade, addressing challenges such as the
scarcity of elderly-specific datasets, class imbalances, and the impact of
age-related facial expression differences. Our findings show that convolutional
neural networks remain dominant in FER, and especially lightweight versions for
resource-constrained environments. However, existing datasets often lack
diversity in age representation, and real-world deployment remains limited.
Additionally, privacy concerns and the need for explainable artificial
intelligence emerged as key barriers to adoption. This review underscores the
importance of developing age-inclusive datasets, integrating multimodal
solutions, and adopting XAI techniques to enhance system usability,
reliability, and trustworthiness. We conclude by offering recommendations for
future research to bridge the gap between academic progress and real-world
implementation in elderly care.

æè¦ï¼å¨çäººå£å¿«éèé¾åçªæ¾äºå¯¹ææ¯çéæ±ï¼ä»¥æ¯æèå¹´äººï¼å°¤å¶æ¯å¨å»çä¿å¥åæç»ªå¥åº·æ¹é¢ãé¢é¨è¡¨æè¯å« (FER) ç³»ç»æä¾äºä¸ç§éä¾µå¥æ§çæç»ªç¶æçæµææ®µï¼å¨è¾å©çæ´»ãå¿çå¥åº·æ¯æåä¸ªæ§åæ¤çä¸­å¾å°åºç¨ãæ¬ç ç©¶å¯¹åºäºæ·±åº¦å­¦ä¹ ç FER ç³»ç»è¿è¡äºç³»ç»çåé¡¾ï¼éç¹å³æ³¨å®ä»¬å¨èå¹´äººç¾¤ä¸­çåºç¨ãéµå¾ªä¸¥æ ¼çæ¹æ³ï¼æä»¬åæäºå¨è¿å»åå¹´ä¸­åè¡¨ç 31 é¡¹ç ç©¶ï¼è§£å³äºè¯¸å¦èå¹´äººç¹å®æ°æ®éçç¨ç¼ºæ§ãç±»å«ä¸å¹³è¡¡ä»¥åä¸å¹´é¾ç¸å³çé¢é¨è¡¨æå·®å¼çå½±åç­ææãæä»¬çç ç©¶ç»æè¡¨æï¼å·ç§¯ç¥ç»ç½ç»å¨ FER ä¸­ä»ç¶å ä¸»å¯¼å°ä½ï¼ç¹å«æ¯éå¯¹èµæºåéç¯å¢çè½»éçº§çæ¬ãç¶èï¼ç°ææ°æ®éå¾å¾ç¼ºä¹å¹´é¾ä»£è¡¨æ§çå¤æ ·æ§ï¼å¹¶ä¸ç°å®ä¸ççé¨ç½²ä»ç¶æéãæ­¤å¤ï¼éç§é®é¢åå¯¹å¯è§£éäººå·¥æºè½çéæ±å·²æä¸ºéç¨è¿ç¨ä¸­çä¸»è¦éç¢ãæ¬æ¬¡å®¡æ¥å¼ºè°äºå¼ååå®¹å¹´é¾çæ°æ®éãæ´åå¤æ¨¡å¼è§£å³æ¹æ¡ä»¥åéç¨ XAI ææ¯ä»¥å¢å¼ºç³»ç»å¯ç¨æ§ãå¯é æ§åå¯ä¿¡åº¦çéè¦æ§ãæåï¼æä»¬æåºäºæªæ¥ç ç©¶çå»ºè®®ï¼ä»¥å¼¥åå­¦æ¯è¿å±ä¸èå¹´æ¤çä¸­çç°å®ä¸çå®æ½ä¹é´çå·®è·ã

##### **Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care**
2502.02109v1 by Yuxiao Cheng, Xinxin Song, Ziqian Wang, Qin Zhong, Kunlun He, Jinli Suo

Recent advances in deep learning (DL) have prompted the development of
high-performing early warning score (EWS) systems, predicting clinical
deteriorations such as acute kidney injury, acute myocardial infarction, or
circulatory failure. DL models have proven to be powerful tools for various
tasks but come with the cost of lacking interpretability and limited
generalizability, hindering their clinical applications. To develop a practical
EWS system applicable to various outcomes, we propose causally-informed
explainable early prediction model, which leverages causal discovery to
identify the underlying causal relationships of prediction and thus owns two
unique advantages: demonstrating the explicit interpretation of the prediction
while exhibiting decent performance when applied to unfamiliar environments.
Benefiting from these features, our approach achieves superior accuracy for 6
different critical deteriorations and achieves better generalizability across
different patient groups, compared to various baseline algorithms. Besides, we
provide explicit causal pathways to serve as references for assistant clinical
diagnosis and potential interventions. The proposed approach enhances the
practical application of deep learning in various medical scenarios.

æè¦ï¼æ·±åº¦å­¸ç¿ (DL) çææ°é²å±ä¿ä½¿éç¼åºé«æ§è½æ©æé è­¦è©å (EWS) ç³»çµ±ï¼é æ¸¬æ¥æ§èèæå·ãæ¥æ§å¿èæ¢å¡æå¾ªç°è¡°ç«­ç­è¨åºæ¡åãDL æ¨¡åå·²è¢«è­ææ¯åç¨®ä»»åçå¼·å¤§å·¥å·ï¼ä½ä»£å¹æ¯ç¼ºä¹å¯è§£éæ§åæéçæ¦æ¬æ§ï¼é»ç¤äºå¶è¨åºæç¨ãçºäºéç¼é©ç¨æ¼åç¨®çµæçå¯¦ç¨ EWS ç³»çµ±ï¼æåæåºäºå æéä¿è§£éæ§æ©æé æ¸¬æ¨¡åï¼å®å©ç¨å æç¼ç¾ä¾è­å¥é æ¸¬çæ½å¨å æéä¿ï¼å¾èææå©åç¨ç¹çåªé»ï¼å±ç¤ºé æ¸¬çæç¢ºè§£éï¼åæå¨æç¨æ¼ä¸çæçç°å¢æè¡¨ç¾åºè¯å¥½çæ§è½ãå¾çæ¼éäºç¹æ§ï¼èåç¨®åºç·æ¼ç®æ³ç¸æ¯ï¼æåçæ¨¡åå¨ 6 ç¨®ä¸åçå±éæ¡åä¸­å¯¦ç¾äºæ´é«çæºç¢ºåº¦ï¼ä¸¦å¨ä¸åçæ£èç¾¤é«ä¸­å¯¦ç¾äºæ´å¥½çæ¦æ¬æ§ãæ­¤å¤ï¼æåæä¾äºæç¢ºçå æéå¾ï¼ä½çºè¼å©è¨åºè¨ºæ·åæ½å¨å¹²é æªæ½çåèãææåºçæ¹æ³å¢å¼·äºæ·±åº¦å­¸ç¿å¨åç¨®é«çå ´æ¯ä¸­çå¯¦éæç¨ã

##### **An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data**
2502.01789v1 by Jiazi Tian, Liqin Wang, Pedram Fard, Valdery Moura Junior, Deborah Blacker, Jennifer S. Haas, Chirag Patel, Shawn N. Murphy, Lidia M. V. R. Moura, Hossein Estiri

Early identification of cognitive concerns is critical but often hindered by
subtle symptom presentation. This study developed and validated a fully
automated, multi-agent AI workflow using LLaMA 3 8B to identify cognitive
concerns in 3,338 clinical notes from Mass General Brigham. The agentic
workflow, leveraging task-specific agents that dynamically collaborate to
extract meaningful insights from clinical notes, was compared to an
expert-driven benchmark. Both workflows achieved high classification
performance, with F1-scores of 0.90 and 0.91, respectively. The agentic
workflow demonstrated improved specificity (1.00) and achieved prompt
refinement in fewer iterations. Although both workflows showed reduced
performance on validation data, the agentic workflow maintained perfect
specificity. These findings highlight the potential of fully automated
multi-agent AI workflows to achieve expert-level accuracy with greater
efficiency, offering a scalable and cost-effective solution for detecting
cognitive concerns in clinical settings.

æè¦ï¼åæ©è¾¨è­èªç¥åé¡è³ééè¦ï¼ä½å¸¸å¸¸åå°ççåç¾éæ¼ç´°å¾®çé»ç¤ãæ¬ç ç©¶éç¼ä¸¦é©è­äºä¸åå¨èªååãå¤éä»£çç AI å·¥ä½æµç¨ï¼ä½¿ç¨ LLaMA 3 8B ä¾è¾¨è­ä¾èªéº»çç¸½é«é¢å¸èæ ¹åé¢ç 3,338 åè¨åºç­è¨ä¸­çèªç¥åé¡ãéåä»£çå·¥ä½æµç¨å©ç¨äºç¹å®ä»»åçä»£çï¼éäºä»£çæåæåä½å¾è¨åºç­è¨ä¸­èååºææç¾©çè¦è§£ï¼ä¸¦èå°å®¶é©åçåºæºé²è¡æ¯è¼ãéå©åå·¥ä½æµç¨é½éå°äºå¾é«çåé¡æè½ï¼F1 åæ¸åå¥çº 0.90 å 0.91ãä»£çå·¥ä½æµç¨å±ç¾åºæ´å¥½çç¹ç°æ§ï¼1.00ï¼ï¼ä¸¦ä¸å¨æ´å°çåè¦éç®ä¸­éå°äºæç¤ºç²¾çãåç®¡éå©åå·¥ä½æµç¨å¨é©è­è³æä¸çæè½é½éä½äºï¼ä½ä»£çå·¥ä½æµç¨ç¶­æäºå®ç¾çç¹ç°æ§ãéäºç¼ç¾çªé¡¯äºå¨èªååå¤éä»£ç AI å·¥ä½æµç¨çæ½åï¼å®åè½ä»¥æ´é«çæçéå°å°å®¶ç´çæºç¢ºåº¦ï¼çºå¨è¨åºç°å¢ä¸­åµæ¸¬èªç¥åé¡æä¾äºä¸åå¯æ´åä¸å·ææ¬æççè§£æ±ºæ¹æ¡ã

##### **Improving Transformer World Models for Data-Efficient RL**
2502.01591v1 by Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy

We present an approach to model-based RL that achieves a new state of the art
performance on the challenging Craftax-classic benchmark, an open-world 2D
survival game that requires agents to exhibit a wide range of general abilities
-- such as strong generalization, deep exploration, and long-term reasoning.
With a series of careful design choices aimed at improving sample efficiency,
our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps,
significantly outperforming DreamerV3, which achieves 53.2%, and, for the first
time, exceeds human performance of 65.0%. Our method starts by constructing a
SOTA model-free baseline, using a novel policy architecture that combines CNNs
and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna
with warmup", which trains the policy on real and imaginary data, (b) "nearest
neighbor tokenizer" on image patches, which improves the scheme to create the
transformer world model (TWM) inputs, and (c) "block teacher forcing", which
allows the TWM to reason jointly about the future tokens of the next timestep.

æè¦ï¼æåæåºäºä¸ååºæ¼æ¨¡åç RL æ¹æ³ï¼å¨å·æææ°æ§ç Craftax-classic åºæºä¸å¯¦ç¾äºæ°çæè¡æ°´æºï¼éæ¯ä¸åéæ¾ä¸çç 2D çå­éæ²ï¼è¦æ±ä»£çäººå±ç¾å»£æ³çä¸è¬è½åï¼ä¾å¦å¼·å¤§çæ¦æ¬è½åãæ·±å¥æ¢ç´¢åé·ææ¨çãééä¸ç³»åæ¨å¨æé«æ¨£æ¬æççä»ç´°è¨­è¨é¸æï¼æåç MBRL æ¼ç®æ³å¨å 1M ç°å¢æ­¥é©å¾å°±å¯¦ç¾äº 67.4% ççåµï¼é¡¯èåªæ¼ DreamerV3ï¼å¯¦ç¾ 53.2%ï¼ï¼ä¸¦ä¸é¦æ¬¡è¶éäºäººé¡ç 65.0% çè¡¨ç¾ãæåçæ¼ç®æ³é¦åééä½¿ç¨çµå CNN å RNN çæ°ç©ç­ç¥æ¶æ§ä¾å»ºæ§ä¸å SOTA ç¡æ¨¡ååºç·ãç¶å¾ï¼æåå°æ¨æº MBRL è¨­å®æ°å¢äºä¸é æ¹é²ï¼(a)ãå¸¶ç±èº«ç Dynaãï¼å®å¨çå¯¦ååæ³è³æä¸è¨ç·´ç­ç¥ï¼(b) å½±åè²¼ççãæè¿é°ä»£ç¢¼åå¨ãï¼å®æ¹é²äºå»ºç«è½æå¨ä¸çæ¨¡å (TWM) è¼¸å¥çæ¹æ¡ï¼ä»¥å (c)ãåå¡æå¸«å¼·å¶ãï¼å®åè¨± TWM å±åæ¨çä¸ä¸åæéæ­¥é·çæªä¾ä»£ç¢¼ã

##### **Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data**
2502.01377v1 by Zhi Zhang, Yan Liu, Mengxia Gao, Yu Yang, Jiannong Cao, Wai Kai Hou, Shirley Li, Sonata Yau, Yun Kwok Wing, Tatia M. C. Lee

Psychological resilience, defined as the ability to rebound from adversity,
is crucial for mental health. Compared with traditional resilience assessments
through self-reported questionnaires, resilience assessments based on
neurological data offer more objective results with biological markers, hence
significantly enhancing credibility. This paper proposes a novel data-efficient
model to address the scarcity of neurological data. We employ Neuro
Kolmogorov-Arnold Networks as the structure of the prediction model. In the
training stage, a new trait-informed multimodal representation algorithm with a
smart chunk technique is proposed to learn the shared latent space with limited
data. In the test stage, a new noise-informed inference algorithm is proposed
to address the low signal-to-noise ratio of the neurological data. The proposed
model not only shows impressive performance on both public datasets and
self-constructed datasets but also provides some valuable psychological
hypotheses for future research.

æè¦ï¼å¿çéæ§ï¼å®ç¾©çºå¾éå¢ä¸­åå½çè½åï¼å°å¿çå¥åº·è³ééè¦ãèééèªæå ±ååå·çå³çµ±éæ§è©ä¼°ç¸æ¯ï¼åºæ¼ç¥ç¶æ¸æçéæ§è©ä¼°æä¾äºæ´å®¢è§ççµæåçç©æ¨è¨ï¼å¾èé¡¯èæé«äºå¯ä¿¡åº¦ãæ¬ææåºäºä¸åæ°ç©çæ¸æé«ææ¨¡åä¾è§£æ±ºç¥ç¶æ¸æçç¨ç¼ºæ§ãæåæ¡ç¨ç¥ç¶ç§ç¾è«å¥ç¾å¤«-é¿è«¾å¾·ç¶²è·¯ä½çºé æ¸¬æ¨¡åççµæ§ãå¨è¨ç·´éæ®µï¼æåºäºä¸ç¨®æ°çç¹å¾µä¿¡æ¯å¤æ¨¡æè¡¨ç¤ºç®æ³ï¼æ¡ç¨æºè½å¡æè¡ï¼ä»¥æéçæ¸æå­¸ç¿å±äº«æ½å¨ç©ºéãå¨æ¸¬è©¦éæ®µï¼æåºäºä¸ç¨®æ°çåªè²ä¿¡æ¯æ¨çç®æ³ï¼ä»¥è§£æ±ºç¥ç¶æ¸æçä¿¡åªæ¯ä½çåé¡ãææåºçæ¨¡åä¸åå¨å¬å±æ¸æéåèªæ§æ¸æéä¸é½é¡¯ç¤ºåºä»¤äººå°è±¡æ·±å»çæ§è½ï¼éçºæªä¾çç ç©¶æä¾äºä¸äºæå¹å¼çå¿çåè¨­ã

##### **OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology**
2502.01243v1 by Chengfeng Zhou, Ji Wang, Juanjuan Qin, Yining Wang, Ling Sun, Weiwei Dai

Large language models (LLMs) have shown significant promise across various
medical applications, with ophthalmology being a notable area of focus. Many
ophthalmic tasks have shown substantial improvement through the integration of
LLMs. However, before these models can be widely adopted in clinical practice,
evaluating their capabilities and identifying their limitations is crucial. To
address this research gap and support the real-world application of LLMs, we
introduce the OphthBench, a specialized benchmark designed to assess LLM
performance within the context of Chinese ophthalmic practices. This benchmark
systematically divides a typical ophthalmic clinical workflow into five key
scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each
scenario, we developed multiple tasks featuring diverse question types,
resulting in a comprehensive benchmark comprising 9 tasks and 591 questions.
This comprehensive framework allows for a thorough assessment of LLMs'
capabilities and provides insights into their practical application in Chinese
ophthalmology. Using this benchmark, we conducted extensive experiments and
analyzed the results from 39 popular LLMs. Our evaluation highlights the
current gap between LLM development and its practical utility in clinical
settings, providing a clear direction for future advancements. By bridging this
gap, we aim to unlock the potential of LLMs and advance their development in
ophthalmology.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨åç¨®é«çæç¨ä¸­å·²å±ç¾åºé¡¯èçæ½åï¼å¶ä¸­ç¼ç§æ¯ä¸åå¼å¾éæ³¨çéè¦é åãè¨±å¤ç¼ç§ä»»åå·²ééæ´å LLM èå¤§å¹é²æ­¥ãç¶èï¼å¨éäºæ¨¡åè½å»£æ³æç¨æ¼è¨åºå¯¦åä¹åï¼è©ä¼°å¶è½åä¸¦æ¾åºå¶éå¶è³ééè¦ãçºäºè§£æ±ºéåç ç©¶å·®è·ä¸¦æ¯æ´ LLM çå¯¦éæç¨ï¼æåå¼å¥äº OphthBenchï¼éæ¯ä¸åå°éçåºæºæ¸¬è©¦ï¼æ¨å¨è©ä¼° LLM å¨ä¸­åç¼ç§å¯¦åä¸­çè¡¨ç¾ãæ­¤åºæºæ¸¬è©¦ç³»çµ±æ§å°å°å¸åç¼ç§è¨åºå·¥ä½æµç¨ååçºäºåééµæå¢ï¼æè²ãåæµãè¨ºæ·ãæ²»çåé å¾ãå°æ¼æ¯åæå¢ï¼æåéç¼äºå¤é ä»»åï¼åå«å¤æ¨£åçåé¡é¡åï¼æå¾çµæä¸ååå« 9 é ä»»åå 591 ååé¡çç¶ååºæºæ¸¬è©¦ãæ­¤ç¶åæ¶æ§å¯å¾¹åºè©ä¼° LLM çè½åï¼ä¸¦æä¾å¶å¨ä¸­åç¼ç§çå¯¦éæç¨è¦è§£ãä½¿ç¨æ­¤åºæºæ¸¬è©¦ï¼æåé²è¡äºå»£æ³çå¯¦é©ï¼ä¸¦åæäºä¾èª 39 åç±é LLM ççµæãæåçè©ä¼°å¼·èª¿äº LLM éç¼èå¶å¨è¨åºç°å¢ä¸­çå¯¦éæç¨ä¹éçå·®è·ï¼çºæªä¾çé²å±æä¾äºæç¢ºçæ¹åãééå½åæ­¤å·®è·ï¼æåæ¨å¨éæ¾ LLM çæ½åï¼ä¸¦ä¿é²å¶å¨ç¼ç§çç¼å±ã

##### **MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks**
2502.01158v1 by Alejandro Guerra-Manzanares, Farah E. Shamout

Multimodal fusion leverages information across modalities to learn better
feature representations with the goal of improving performance in fusion-based
tasks. However, multimodal datasets, especially in medical settings, are
typically smaller than their unimodal counterparts, which can impede the
performance of multimodal models. Additionally, the increase in the number of
modalities is often associated with an overall increase in the size of the
multimodal network, which may be undesirable in medical use cases. Utilizing
smaller unimodal encoders may lead to sub-optimal performance, particularly
when dealing with high-dimensional clinical data. In this paper, we propose the
Modality-INformed knowledge Distillation (MIND) framework, a multimodal model
compression approach based on knowledge distillation that transfers knowledge
from ensembles of pre-trained deep neural networks of varying sizes into a
smaller multimodal student. The teacher models consist of unimodal networks,
allowing the student to learn from diverse representations. MIND employs
multi-head joint fusion models, as opposed to single-head models, enabling the
use of unimodal encoders in the case of unimodal samples without requiring
imputation or masking of absent modalities. As a result, MIND generates an
optimized multimodal model, enhancing both multimodal and unimodal
representations. It can also be leveraged to balance multimodal learning during
training. We evaluate MIND on binary and multilabel clinical prediction tasks
using time series data and chest X-ray images. Additionally, we assess the
generalizability of the MIND framework on three non-medical multimodal
multiclass datasets. Experimental results demonstrate that MIND enhances the
performance of the smaller multimodal network across all five tasks, as well as
various fusion methods and multimodal architectures, compared to
state-of-the-art baselines.

æè¦ï¼å¤æ¨¡æèåå©ç¨è·¨æ¨¡æçä¿¡æ¯æ¥å­¦ä¹ æ´å¥½çç¹å¾è¡¨ç¤ºï¼ç®æ æ¯æååºäºèåçä»»å¡çæ§è½ãç¶èï¼å¤æ¨¡ææ°æ®éï¼å°¤å¶æ¯å¨å»çç¯å¢ä¸­ï¼éå¸¸æ¯å®ä»¬çåæ¨¡æå¯¹åºæ°æ®éå°ï¼è¿ä¼é»ç¢å¤æ¨¡ææ¨¡åçæ§è½ãæ­¤å¤ï¼æ¨¡ææ°éçå¢å éå¸¸ä¸å¤æ¨¡æç½ç»å°ºå¯¸çæ´ä½å¢å ç¸å³ï¼è¿å¨å»çç¨ä¾ä¸­å¯è½æ¯ä¸å¯åçãå©ç¨è¾å°çåæ¨¡æç¼ç å¨å¯è½ä¼å¯¼è´æ¬¡ä¼æ§è½ï¼å°¤å¶æ¯å¨å¤çé«ç»´ä¸´åºæ°æ®æ¶ãå¨æ¬æä¸­ï¼æä»¬æåºäºæ¨¡æä¿¡æ¯ç¥è¯è¸é¦ (MIND) æ¡æ¶ï¼è¿æ¯ä¸ç§åºäºç¥è¯è¸é¦çå¤æ¨¡ææ¨¡ååç¼©æ¹æ³ï¼å®å°æ¥èªä¸åå¤§å°çé¢è®­ç»æ·±åº¦ç¥ç»ç½ç»çéåä¸­çç¥è¯è½¬ç§»å°ä¸ä¸ªè¾å°çå¤æ¨¡æå­¦çä¸­ãæå¸æ¨¡åç±åæ¨¡æç½ç»ç»æï¼åè®¸å­¦çä»ä¸åçè¡¨ç¤ºä¸­å­¦ä¹ ãMIND éç¨å¤å¤´èåèåæ¨¡åï¼èä¸æ¯åå¤´æ¨¡åï¼ä»èè½å¤å¨åæ¨¡ææ ·æ¬çæåµä¸ä½¿ç¨åæ¨¡æç¼ç å¨ï¼èä¸éè¦ç¼ºå¤±æ¨¡æçæè¡¥ææ©è½ãå æ­¤ï¼MIND çæäºä¸ä¸ªç»è¿ä¼åçå¤æ¨¡ææ¨¡åï¼å¢å¼ºäºå¤æ¨¡æååæ¨¡æè¡¨ç¤ºãå®è¿å¯ä»¥ç¨æ¥å¨è®­ç»æé´å¹³è¡¡å¤æ¨¡æå­¦ä¹ ãæä»¬ä½¿ç¨æ¶é´åºåæ°æ®åè¸é¨ X å°çº¿å¾åå¯¹äºååå¤æ ç­¾ä¸´åºé¢æµä»»å¡è¯ä¼°äº MINDãæ­¤å¤ï¼æä»¬è¯ä¼°äº MIND æ¡æ¶å¨ä¸ä¸ªéå»çå¤æ¨¡æå¤åç±»æ°æ®éä¸çæ³åæ§ãå®éªç»æè¡¨æï¼ä¸æåè¿çåºçº¿ç¸æ¯ï¼MIND å¢å¼ºäºè¾å°çå¤æ¨¡æç½ç»å¨ææäºä¸ªä»»å¡ä»¥ååç§èåæ¹æ³åå¤æ¨¡ææ¶æä¸­çæ§è½ã

##### **Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations**
2502.01141v1 by Qian Chen, Stefanie Rinderle-Ma, Lijie Wen

Most existing process compliance monitoring approaches detect compliance
violations in an ex post manner. Only predicate prediction focuses on
predicting them. However, predicate prediction provides a binary yes/no notion
of compliance, lacking the ability to measure to which extent an ongoing
process instance deviates from the desired state as specified in constraints.
Here, being able to quantify the magnitude of violation would provide
organizations with deeper insights into their operational performance, enabling
informed decision making to reduce or mitigate the risk of non-compliance.
Thus, we propose two predictive compliance monitoring approaches to close this
research gap. The first approach reformulates the binary classification problem
as a hybrid task that considers both classification and regression, while the
second employs a multi-task learning method to explicitly predict the
compliance status and the magnitude of violation for deviant cases
simultaneously. In this work, we focus on temporal constraints as they are
significant in almost any application domain, e.g., health care. The evaluation
on synthetic and real-world event logs demonstrates that our approaches are
capable of quantifying the magnitude of violations while maintaining comparable
performance for compliance predictions achieved by state-of-the-art approaches.

æè¦ï¼ç¾æçæµç¨åè¦ç£æ§æ¹æ³å¤§å¤æå¨äºå¾åµæ¸¬å°åè¦éè¦ãåªæè¬è©é æ¸¬å°æ³¨æ¼é æ¸¬éäºéè¦ãç¶èï¼è¬è©é æ¸¬æä¾çæ¯åè¦èå¦çäºåæ¦å¿µï¼ç¡æ³è¡¡éæ­£å¨é²è¡çæµç¨å¯¦ä¾åé¢ç´æä¸­ææå®ä¹çæ³çæçç¨åº¦ãå¨æ­¤ï¼è½å¤ éåéè¦çå´éç¨åº¦ï¼å°è½è®çµç¹æ·±å¥äºè§£å¶çéç¸¾æï¼ä¸¦è½ææ­¤ååºææºçæ±ºç­ï¼ä»¥éä½ææ¸è¼ä¸åè¦çé¢¨éªãå æ­¤ï¼æåæåºå©ç¨®é æ¸¬åè¦ç£æ§æ¹æ³ä¾å¡«è£æ­¤ç ç©¶ç©ºç½ãç¬¬ä¸ç¨®æ¹æ³å°äºååé¡åé¡éæ°è¡¨è¿°çºåæèéåé¡ååæ­¸çæ··åä»»åï¼èç¬¬äºç¨®æ¹æ³åæ¡ç¨å¤ä»»åå­¸ç¿æ¹æ³ï¼åææç¢ºé æ¸¬åè¦çæååå·®æ¡ä¾çéè¦å´éç¨åº¦ãå¨éé å·¥ä½ä¸­ï¼æåå°æ³¨æ¼æéç´æï¼å çºå®åå¹¾ä¹å¨ä»»ä½æç¨é åï¼ä¾å¦é«çä¿å¥ï¼ä¸­é½å¾éè¦ãå¨åæåçå¯¦ä¸çäºä»¶è¨éä¸çè©ä¼°é¡¯ç¤ºï¼æåçåæ³è½å¤ éåéè¦çå´éç¨åº¦ï¼åæç¶­æèç¾ææ¹æ³æéæçåè¦é æ¸¬ç¸ç¶çç¸¾æã

##### **Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings**
2502.01108v1 by Mithun Saha, Maxwell A. Xu, Wanting Mao, Sameer Neupane, James M. Rehg, Santosh Kumar

Photoplethysmography (PPG)-based foundation models are gaining traction due
to the widespread use of PPG in biosignal monitoring and their potential to
generalize across diverse health applications. In this paper, we introduce
Pulse-PPG, the first open-source PPG foundation model trained exclusively on
raw PPG data collected over a 100-day field study with 120 participants.
Existing PPG foundation models are either open-source but trained on clinical
data or closed-source, limiting their applicability in real-world settings. We
evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its
performance against a state-of-the-art foundation model trained on clinical
data. Our results demonstrate that Pulse-PPG, trained on uncurated field data,
exhibits superior generalization across clinical and mobile health applications
in both lab and field settings. This suggests that exposure to real-world
variability enables the model to learn fine-grained representations, making it
more adaptable across tasks. Furthermore, pre-training on field data
surprisingly outperforms its pre-training on clinical data in many tasks,
reinforcing the importance of training on real-world, diverse datasets. To
encourage further advancements in robust foundation models leveraging field
data, we plan to release Pulse-PPG, providing researchers with a powerful
resource for developing more generalizable PPG-based models.

æè¦ï¼åºæ¼åé»å®¹ç©æè¨è¡ (PPG) çåºç¤æ¨¡åç±æ¼ PPG å¨çç©è¨èç£æ§ä¸­çå»£æ³ä½¿ç¨åå¶å¨åç¨®å¥åº·æç¨ä¸­æ¨å»£çæ½åèååéæ³¨ãå¨æ¬æä¸­ï¼æåä»ç´¹ Pulse-PPGï¼éæ¯ç¬¬ä¸åéæ¾åå§ç¢¼ PPG åºç¤æ¨¡åï¼å°ééå°å¨çºæ 100 å¤©çç¾å ´ç ç©¶ä¸­æ¶éç 120 ä½åèèçåå§ PPG è³æé²è¡è¨ç·´ãç¾æç PPG åºç¤æ¨¡åè¦ä¸æ¯éæ¾åå§ç¢¼ï¼ä½è¨ç·´æ¼è¨åºè³æï¼ä¸ç¶å°±æ¯éæºï¼ééå¶äºå®åå¨çå¯¦ä¸çä¸­çæç¨æ§ãæåè©ä¼°äº Pulse-PPG å¨å¤åè³æéåä¸æ¸¸ä»»åä¸­çè¡¨ç¾ï¼ä¸¦å°å¶æè½èè¨ç·´æ¼è¨åºè³æçææ°åºç¤æ¨¡åé²è¡æ¯è¼ãæåççµæè¡¨æï¼è¨ç·´æ¼æªæ´çç¾å ´è³æç Pulse-PPG å¨å¯¦é©å®¤åç¾å ´ç°å¢ä¸­ï¼å¨è¨åºåè¡åå¥åº·æç¨ä¸­å±ç¾åºåªç°çæ³åè½åãéè¡¨ææ¥è§¸çå¯¦ä¸ççè®ç°æ§ä½¿æ¨¡åè½å¤ å­¸ç¿ç´°ç²åº¦çè¡¨ç¤ºï¼ä½¿å¶æ´è½é©æåç¨®ä»»åãæ­¤å¤ï¼ä»¤äººé©è¨çæ¯ï¼ç¾å ´è³æçé è¨ç·´å¨è¨±å¤ä»»åä¸­åªæ¼è¨åºè³æçé è¨ç·´ï¼éå¼·åäºå¨çå¯¦ä¸çãå¤æ¨£åçè³æéä¸è¨ç·´çéè¦æ§ãçºäºé¼åµå¨å©ç¨ç¾å ´è³æçå¼·å¥åºç¤æ¨¡åæ¹é¢é²ä¸æ­¥ç¼å±ï¼æåè¨ç«ç¼å¸ Pulse-PPGï¼çºç ç©¶äººå¡æä¾ä¸åå¼·å¤§çè³æºï¼ç¨æ¼éç¼æ´å·æ³åæ§çåºæ¼ PPG çæ¨¡åã

##### **Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model**
2502.01691v1 by Hadas Ben-Atya, Naama Gavrielov, Zvi Badash, Gili Focht, Ruth Cytter-Kuint, Talar Hagopian, Dan Turner, Moti Freiman

Reliable extraction of structured data from radiology reports using Large
Language Models (LLMs) remains challenging, especially for complex, non-English
texts like Hebrew. This study introduces an agent-based uncertainty-aware
approach to improve the trustworthiness of LLM predictions in medical
applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease
patients (from 2010 to 2023) across three medical centers. A subset of 512
reports was manually annotated for six gastrointestinal organs and 15
pathological findings, while the remaining reports were automatically annotated
using HSMP-BERT. Structured data extraction was performed using Llama 3.1
(Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed
six semantically equivalent prompts to estimate uncertainty. An Agent-Based
Decision Model integrated multiple prompt outputs into five confidence levels
for calibrated uncertainty and was compared against three entropy-based models.
Performance was evaluated using accuracy, F1 score, precision, recall, and
Cohen's Kappa before and after filtering high-uncertainty cases. The
agent-based model outperformed the baseline across all metrics, achieving an F1
score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering
high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to
0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated
clear separation between correct and incorrect predictions, with the
agent-based model providing the most well-calibrated uncertainty estimates. By
incorporating uncertainty-aware prompt ensembles and an agent-based decision
model, this approach enhances the performance and reliability of LLMs in
structured data extraction from radiology reports, offering a more
interpretable and trustworthy solution for high-stakes medical applications.

æè¦ï¼<paragraph>ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) å¾æ¾å°ç§å ±åä¸­å¯é å°æåçµæ§åæ¸æä»ç¶å·æææ°æ§ï¼å°¤å¶æ¯å°æ¼å¸ä¼¯ä¾èªç­è¤éçéè±èªææ¬ãæ¬ç ç©¶å¼å¥äºä¸ç¨®åºæ¼ä»£ççä¸ç¢ºå®æ§æç¥æ¹æ³ï¼ä»¥æé« LLM é æ¸¬å¨é«çæç¨ä¸­çå¯ä¿¡åº¦ãæååæäºä¾èªä¸åé«çä¸­å¿ç 9,683 ä»½åéæ°çæ£èçå¸ä¼¯ä¾èªæ¾å°ç§å ±åï¼å¾ 2010 å¹´å° 2023 å¹´ï¼ãå¶ä¸­ 512 ä»½å ±åçæåè¨»éåæ¬å­åèè¸å¨å®å 15 åççç¼ç¾ï¼èå¶é¤å ±ååä½¿ç¨ HSMP-BERT èªåè¨»éãçµæ§åæ¸ææåä½¿ç¨ Llama 3.1ï¼Llama 3-8b-instructï¼èè²èæ¯æç¤ºéåï¼BayesPEï¼é²è¡ï¼å®æ¡ç¨å­åèªç¾©ç­ææç¤ºä¾ä¼°è¨ä¸ç¢ºå®æ§ãåºæ¼ä»£ççæ±ºç­æ¨¡åå°å¤åæç¤ºè¼¸åºæ´åå°äºåç½®ä¿¡åº¦ç´å¥ä¸­ä»¥æ ¡æºä¸ç¢ºå®æ§ï¼ä¸¦èä¸ååºæ¼çµçæ¨¡åé²è¡æ¯è¼ãå¨éæ¿¾æé«åº¦ä¸ç¢ºå®æ§çææ³ä¹ååä¹å¾ï¼ä½¿ç¨æºç¢ºåº¦ãF1 åæ¸ãç²¾ç¢ºåº¦ãå¬åçå Cohen's Kappa è©ä¼°æ§è½ãåºæ¼ä»£ççæ¨¡åå¨ææææ¨ä¸é½åªæ¼åºç·ï¼F1 åæ¸éå° 0.3967ï¼å¬åçéå° 0.6437ï¼Cohen's Kappa éå° 0.3006ãå¨éæ¿¾æé«åº¦ä¸ç¢ºå®æ§çææ³ï¼å¤§æ¼æç­æ¼ 0.5ï¼å¾ï¼F1 åæ¸æé«å° 0.4787ï¼Kappa æé«å° 0.4258ãä¸ç¢ºå®æ§ç´æ¹åé¡¯ç¤ºäºæ­£ç¢ºé æ¸¬åä¸æ­£ç¢ºé æ¸¬ä¹éçæé¡¯åå¥ï¼åºæ¼ä»£ççæ¨¡åæä¾äºæ ¡æºæå¥½çä¸ç¢ºå®æ§ä¼°è¨ãééçµåä¸ç¢ºå®æ§æç¥æç¤ºéåååºæ¼ä»£ççæ±ºç­æ¨¡åï¼éç¨®æ¹æ³å¢å¼·äº LLM å¨æ¾å°ç§å ±åä¸­çµæ§åæ¸ææåä¸­çæ§è½åå¯é æ§ï¼çºé«é¢¨éªé«çæç¨æä¾äºæ´å·å¯è§£éæ§åå¯ä¿¡åº¦çè§£æ±ºæ¹æ¡ã</paragraph>

##### **Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment**
2502.01685v1 by Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha

Existing methods for analyzing linguistic content from picture descriptions
for assessment of cognitive-linguistic impairment often overlook the
participant's visual narrative path, which typically requires eye tracking to
assess. Spatio-semantic graphs are a useful tool for analyzing this narrative
path from transcripts alone, however they are limited by the need for manual
tagging of content information units (CIUs). In this paper, we propose an
automated approach for estimation of spatio-semantic graphs (via automated
extraction of CIUs) from the Cookie Theft picture commonly used in
cognitive-linguistic analyses. The method enables the automatic
characterization of the visual semantic path during picture description.
Experiments demonstrate that the automatic spatio-semantic graphs effectively
differentiate between cognitively impaired and unimpaired speakers. Statistical
analyses reveal that the features derived by the automated method produce
comparable results to the manual method, with even greater group differences
between clinical groups of interest. These results highlight the potential of
the automated approach for extracting spatio-semantic features in developing
clinical speech models for cognitive impairment assessment.

æè¦ï¼ç¾æçç¨æ¼åæååæè¿°ä¸­çèªè¨å§å®¹çæ¹æ³ï¼ç¨æ¼è©ä¼°èªç¥èªè¨éç¤ï¼éå¸¸æå¿½ç¥åèèçè¦è¦ºæäºè·¯å¾ï¼ééå¸¸éè¦ç¼çè¿½è¹¤ä¾è©ä¼°ãæç©ºèªç¾©åæ¯ä¸ç¨®æç¨çå·¥å·ï¼å¯ä»¥åå¾è½éæ¬ä¸­åææ­¤æäºè·¯å¾ï¼ä½æ¯å®ååå°æåæ¨è¨å§å®¹è³è¨å®å (CIU) çéæ±æéå¶ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®èªååæ¹æ³ï¼ç¨æ¼å¾èªç¥èªè¨åæä¸­å¸¸ç¨ç Cookie Theft ååä¼°è¨æç©ºèªç¾©åï¼ééèªåæå CIUï¼ãè©²æ¹æ³è½å¤ èªåè¡¨å¾µåçæè¿°æéçè¦è¦ºèªç¾©è·¯å¾ãå¯¦é©è¡¨æï¼èªåæç©ºèªç¾©åææå°ååäºèªç¥åæåæªåæçèªªè©±èãçµ±è¨åæè¡¨æï¼èªååæ¹æ³è¡ççç¹å¾µç¢çäºèæåæ¹æ³ç¸ç¶ççµæï¼çè³å¨æèè¶£çè¨åºçµä¹éç¢çäºæ´å¤§ççµå·®ç°ãéäºçµæçªåºäºèªååæ¹æ³å¨æåæç©ºèªç¾©ç¹å¾µä»¥éç¼ç¨æ¼èªç¥éç¤è©ä¼°çè¨åºèªé³æ¨¡åæ¹é¢çæ½åã

##### **Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images**
2502.00712v1 by Shengtian Sang, Hassan Jahanandish, Cynthia Xinran Li, Indrani Bhattachary, Jeong Hoon Lee, Lichun Zhang, Sulaiman Vesal, Pejman Ghanouni, Richard Fan, Geoffrey A. Sonn, Mirabela Rusu

Prostate cancer is a major cause of cancer-related deaths in men, where early
detection greatly improves survival rates. Although MRI-TRUS fusion biopsy
offers superior accuracy by combining MRI's detailed visualization with TRUS's
real-time guidance, it is a complex and time-intensive procedure that relies
heavily on manual annotations, leading to potential errors. To address these
challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation
method that identifies prostate tumors directly in TRUS images without
requiring manual annotations. Unlike traditional multimodal fusion approaches
that rely on naive data concatenation, our method integrates a
registration-segmentation framework to align and leverage spatial information
between MRI and TRUS modalities. This alignment enhances segmentation accuracy
and reduces reliance on manual effort. Our approach was validated on a dataset
of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient
of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132)
methods, with significant improvements (p $<$ 0.01). This framework
demonstrates the potential for reducing the complexity of prostate cancer
diagnosis and provides a flexible architecture applicable to other multimodal
medical imaging tasks.

æè¦ï¼ååèºçæ¯ç·æ§ççç¸éæ­»äº¡çä¸»è¦åå ï¼æ©æç¼ç¾å¯å¤§å¹æåå­æ´»çãåç®¡ MRI-TRUS èååçæª¢æ¥çµåäº MRI çè©³ç´°è¦è¦ºåè TRUS çå³æå°å¼ï¼å¯æä¾æ´é«çæºç¢ºåº¦ï¼ä½å®æ¯ä¸ç¨®ä»°è³´å¤§éæåè¨»è§£çè¤éä¸èæçç¨åºï¼å®¹æå°è´é¯èª¤ãçºäºè§£æ±ºéäºææ°ï¼æåæåºäºä¸ç¨®å¨èªåç MRI-TRUS èåå¼åå²æ¹æ³ï¼å®å¯ä»¥å¨ TRUS å½±åä¸­ç´æ¥è¾¨è­åºååèºè«ç¤ï¼èä¸éè¦æåè¨»è§£ãèä¾è³´æ¼å¤©çè³æä¸²æ¥çå³çµ±å¤æ¨¡æèåæ¹æ³ä¸åï¼æåçæ¹æ³æ´åäºä¸åéæºåå²æ¶æ§ï¼ä»¥å°é½ä¸¦å©ç¨ MRI è TRUS æ¨¡æä¹éçç©ºéè³è¨ãéç¨®å°é½æåäºåå²æºç¢ºåº¦ï¼ä¸¦æ¸å°äºå°æåä½æ¥­çä¾è³´ãæåçæ¹æ³å·²ééä¾èª Stanford é«é¢ç 1,747 ä½æ£èçè³æéé²è¡é©è­ï¼éå°äº 0.212 çå¹³å Dice ä¿æ¸ï¼åªæ¼åä½¿ç¨ TRUS (0.117) åå¤©çç MRI-TRUS èå (0.132) æ¹æ³ï¼ä¸¦æé¡¯èçæ¹åï¼p < 0.01ï¼ãéåæ¶æ§è­æäºéä½ååèºçè¨ºæ·è¤éæ§çæ½åï¼ä¸¦æä¾äºä¸åé©ç¨æ¼å¶ä»å¤æ¨¡æé«å­¸å½±åä»»åçå½æ§æ¶æ§ã

##### **TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion**
2502.00695v1 by Linglong Wu, Xuhao Shan, Ruiquan Ge, Ruoyu Liang, Chi Zhang, Yonghong Li, Ahmed Elazab, Huoling Luo, Yunbi Liu, Changmiao Wang

Chronic liver disease represents a significant health challenge worldwide and
accurate prognostic evaluations are essential for personalized treatment plans.
Recent evidence suggests that integrating multimodal data, such as computed
tomography imaging, radiomic features, and clinical information, can provide
more comprehensive prognostic information. However, modalities have an inherent
heterogeneity, and incorporating additional modalities may exacerbate the
challenges of heterogeneous data fusion. Moreover, existing multimodal fusion
methods often struggle to adapt to richer medical modalities, making it
difficult to capture inter-modal relationships. To overcome these limitations,
We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).
Specifically, we develop an Intra-Modality Aggregation module and a
Triple-Modal Cross-Attention Fusion module, which are designed to eliminate
intra-modality redundancy and extract cross-modal information, respectively.
Furthermore, we design a Triple-Modal Feature Fusion loss function to align
feature representations across modalities. Extensive experiments on the liver
prognosis dataset demonstrate that our approach significantly outperforms
existing state-of-the-art unimodal models and other multi-modal techniques. Our
code is available at https://github.com/Mysterwll/liver.git.

æè¦ï¼æ¢æ§èçå¨å¨çèå´åä»£è¡¨èéå¤§çå¥åº·ææ°ï¼èæºç¢ºçé å¾è©ä¼°å°æ¼åäººåæ²»çè¨ç«è³ééè¦ãæè¿çè­æè¡¨æï¼æ´åå¤æ¨¡æè³æï¼ä¾å¦é»è¦æ·å±¤å½±åãæ¾å°ç¹å¾µåè¨åºè³è¨ï¼å¯ä»¥æä¾æ´å¨é¢çé å¾è³è¨ãç¶èï¼æ¨¡æå·æå§å¨ç°è³ªæ§ï¼èç´å¥é¡å¤çæ¨¡æå¯è½æå åç°è³ªåè³æèåçææ°ãæ­¤å¤ï¼ç¾æçå¤æ¨¡æèåæ¹æ³éå¸¸é£ä»¥é©ææ´è±å¯çé«çæ¨¡æï¼éä½¿å¾é£ä»¥æææ¨¡æéçéä¿ãçºäºåæéäºéå¶ï¼æåæåºäºä¸æ¨¡æäº¤äºæ¢æ§èèç¶²è·¯ (TMI-CLNet)ãå·é«ä¾èªªï¼æåéç¼äºä¸åæ¨¡æå§èåæ¨¡çµåä¸åä¸æ¨¡æäº¤åæ³¨æåèåæ¨¡çµï¼å®ååå¥æ¨å¨æ¶é¤æ¨¡æå§åé¤åæåè·¨æ¨¡æè³è¨ãæ­¤å¤ï¼æåè¨­è¨äºä¸åä¸æ¨¡æç¹å¾µèåæå¤±å½æ¸ï¼ä»¥å°é½è·¨æ¨¡æçç¹å¾µè¡¨ç¤ºãå¨èèé å¾è³æéä¸çå»£æ³å¯¦é©è¡¨æï¼æåçåæ³é¡¯èåªæ¼ç¾æçæåé²å®æ¨¡ææ¨¡ååå¶ä»å¤æ¨¡ææè¡ãæåçç¨å¼ç¢¼å¯ä»¥å¨ https://github.com/Mysterwll/liver.git ä¸åå¾ã

##### **Enhanced Convolutional Neural Networks for Improved Image Classification**
2502.00663v1 by Xiaoran Yang, Shuhan Yu, Wenxi Xu

Image classification is a fundamental task in computer vision with diverse
applications, ranging from autonomous systems to medical imaging. The CIFAR-10
dataset is a widely used benchmark to evaluate the performance of
classification models on small-scale, multi-class datasets. Convolutional
Neural Networks (CNNs) have demonstrated state-of-the-art results; however,
they often suffer from overfitting and suboptimal feature representation when
applied to challenging datasets like CIFAR-10. In this paper, we propose an
enhanced CNN architecture that integrates deeper convolutional blocks, batch
normalization, and dropout regularization to achieve superior performance. The
proposed model achieves a test accuracy of 84.95%, outperforming baseline CNN
architectures. Through detailed ablation studies, we demonstrate the
effectiveness of the enhancements and analyze the hierarchical feature
representations. This work highlights the potential of refined CNN
architectures for tackling small-scale image classification problems
effectively.

æè¦ï¼å½±ååé¡æ¯é»è¦è¦è¦ºä¸­çä¸é åºæ¬ä»»åï¼æç¨ç¯åå»£æ³ï¼å¾èªåç³»çµ±å°é«å­¸å½±åçæãCIFAR-10 è³æéæ¯ä¸åå»£æ³ä½¿ç¨çåºæºï¼ç¨æ¼è©ä¼°åé¡æ¨¡åå¨å°è¦æ¨¡ãå¤é¡å¥è³æéä¸çæè½ãå·ç©ç¥ç¶ç¶²è·¯ (CNN) å·²å±ç¾åºæåé²çææï¼ç¶èï¼ç¶æç¨æ¼ CIFAR-10 ç­å·ææ°æ§çè³æéæï¼å®åå¸¸å¸¸æç¼çéåº¦æ¬ååæ¬¡ä½³ç¹å¾µè¡¨ç¤ºçåé¡ãå¨æ¬æä¸­ï¼æåæåºä¸åå¢å¼·ç CNN æ¶æ§ï¼å®æ´åäºæ´æ·±çå·ç©åå¡ãæ¹æ¬¡æ­£è¦ååä¸­æ·æ­£è¦åï¼ä»¥éæåè¶çæè½ãææåºçæ¨¡åéå°äº 84.95% çæ¸¬è©¦æºç¢ºåº¦ï¼åªæ¼åºæº CNN æ¶æ§ãééè©³ç´°çæ¶èç ç©¶ï¼æåè­æäºéäºå¢å¼·åè½çæææ§ï¼ä¸¦åæäºéå±¤å¼ç¹å¾µè¡¨ç¤ºãéé å·¥ä½çªé¡¯äºç²¾é²ç CNN æ¶æ§å¨ææè§£æ±ºå°è¦æ¨¡å½±ååé¡åé¡ä¸çæ½åã

##### **Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective**
2502.00619v1 by Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon, Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li

Ensuring fairness in medical image segmentation is critical due to biases in
imbalanced clinical data acquisition caused by demographic attributes (e.g.,
age, sex, race) and clinical factors (e.g., disease severity). To address these
challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired
by optimal control theory. We provide a comprehensive analysis of its
underlying mechanisms and clarify dMoE's role in adapting to heterogeneous
distributions in medical image segmentation. Furthermore, we integrate dMoE
into multiple network architectures, demonstrating its broad applicability
across diverse medical image analysis tasks. By incorporating demographic and
clinical factors, dMoE achieves state-of-the-art performance on two 2D
benchmark datasets and a 3D in-house dataset. Our results highlight the
effectiveness of dMoE in mitigating biases from imbalanced distributions,
offering a promising approach to bridging control theory and medical image
segmentation within fairness learning paradigms. The source code will be made
available.

æè¦ï¼å¨å»å­¦å½±ååå²ä¸­ï¼ç±æ¼äººå£å±¬æ§ï¼ä¾å¦å¹´é½¡ãæ§å¥ãç¨®æï¼åè¨åºå ç´ ï¼ä¾å¦ç¾çå´éç¨åº¦ï¼å°è´ä¸å¹³è¡¡çè¨åºæ¸ææ¡éä¸­å­å¨åå·®ï¼å æ­¤ç¢ºä¿å¬å¹³æ§è³ééè¦ãçºäºæå°éäºææ°ï¼æåå¼å¥äºåæåªæ§å¶çè«åç¼çæç¥æ··åå°å®¶ (dMoE)ãæåå°å¶åºå±¤æ©å¶é²è¡äºå¨é¢åæï¼ä¸¦éæ¸äº dMoE å¨é©æé«å­¸å½±ååå²ä¸­çç°è³ªåä½ä¸­çä½ç¨ãæ­¤å¤ï¼æåå° dMoE æ´åå°å¤åç¶²è·¯æ¶æ§ä¸­ï¼å±ç¤ºäºå¶å¨åç¨®é«å­¸å½±ååæä»»åä¸­çå»£æ³é©ç¨æ§ãééç´å¥äººå£çµ±è¨åè¨åºå ç´ ï¼dMoE å¨å©å 2D åºæºæ¸æéåä¸å 3D å§é¨æ¸æéä¸å¯¦ç¾äºæåé²çæ§è½ãæåççµæçªåºäº dMoE å¨æ¸è¼ä¸å¹³è¡¡åä½çåå·®æ¹é¢çæææ§ï¼çºå¨å¬å¹³æ§å­¸ç¿ç¯ä¾ä¸­æ©æ¥æ§å¶çè«åé«å­¸å½±ååå²æä¾äºä¸åæåæ¯çæ¹æ³ãåå§ç¢¼å°æå¬éã

##### **Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions**
2502.00568v1 by Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti

Emerging research has highlighted that artificial intelligence based
multimodal fusion of digital pathology and transcriptomic features can improve
cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.
However, such direct fusion for joint decision is impractical in real clinical
settings, where histopathology is still the gold standard for diagnosis and
transcriptomic tests are rarely requested, at least in the public healthcare
system. With our novel diffusion based crossmodal generative AI model PathoGen,
we show that genomic expressions synthesized from digital histopathology
jointly predicts cancer grading and patient survival risk with high accuracy
(state-of-the-art performance), certainty (through conformal coverage
guarantee) and interpretability (through distributed attention maps). PathoGen
code is available for open use by the research community through GitHub at
https://github.com/Samiran-Dey/PathoGen.

æè¦ï¼æ°èç ç©¶å¼·èª¿ï¼åºæ¼äººå·¥æºæ§çå¤æ¨¡æèåæ¸ä½ççå­¸åè½éçµç¹å¾µï¼å¯ä»¥æ¹åççè¨ºæ·ï¼åç´/ååï¼åé å¾ï¼å­æ´»é¢¨éªï¼é æ¸¬ã
ç¶èï¼éç¨®ç´æ¥èåå°æ¼è¯åæ±ºç­å¨å¯¦éè¨åºç°å¢ä¸­æ¯ä¸åå¯¦éçï¼å çºå¨å¯¦éè¨åºç°å¢ä¸­ï¼çµç¹ççå­¸ä»ç¶æ¯è¨ºæ·çé»éæ¨æºï¼èè½éçµæª¢æ¸¬å¾å°è¢«è¦æ±ï¼è³å°å¨å¬å±é«çç³»çµ±ä¸­æ¯å¦æ­¤ãééæåæ°ç©çåºæ¼æ´æ£çè·¨æ¨¡æçæå¼ AI æ¨¡å PathoGenï¼æåå±ç¤ºäºå¾æ¸ä½çµç¹ççå­¸åæçåºå é«è¡¨éï¼å¯ä»¥å±åé æ¸¬ççåç´åæ£èå­æ´»é¢¨éªï¼å·æé«æºç¢ºåº¦ï¼æåé²çæè½ï¼ãç¢ºå®æ§ï¼ééä¿å½¢è¦èä¿è­ï¼åå¯è§£éæ§ï¼ééåä½å¼æ³¨æååï¼ãPathoGen ç¨å¼ç¢¼å¯éé GitHub ä¸ç https://github.com/Samiran-Dey/PathoGenï¼éæ¾ä¾ç ç©¶ç¤¾ç¾¤ä½¿ç¨ã

##### **Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?**
2502.00495v1 by Mohammad Saleh Torkestani, Robert Davis, Abdolhossein Sarrafzadeh

Time constraints on doctor patient interaction and restricted access to
specialists under the managed care system led to increasingly referring to
computers as a medical information source and a self-health-care management
tool. However, research show that less than 40% of information seekers
indicated that online information helped them to make a decision about their
health. Searching multiple web sites that need basic computer skills, lack of
interaction and no face to face interaction in most search engines and some
social issues, led us to develop a specialized life-like agent that would
overcome mentioned problems.

æè¦ï¼ç±æ¼ç®¡çå¼é«çä¿å¥ç³»çµ±ä¸­é«å¸«èçæ£äºåæéæéï¼ä¸å°ç§é«å¸«çåå¾åéï¼å æ­¤è¶ä¾è¶å¤äººå°é»è¦è¦çºé«çè³è¨ä¾æºåèªæä¿å¥ç®¡çå·¥å·ãç¶èï¼ç ç©¶é¡¯ç¤ºï¼ä¸å° 40% çè³è¨å°æ±èè¡¨ç¤ºï¼ç·ä¸è³è¨æå©æ¼ä»åååºå¥åº·æ±ºç­ãæå°éè¦åºæ¬é»è¦æè½çè¨±å¤ç¶²ç«ãç¼ºä¹äºåï¼ä»¥åå¤§å¤æ¸æå°å¼æåä¸äºç¤¾äº¤è­°é¡ä¸­æ²æé¢å°é¢çäºåï¼éäºåå ä¿ä½¿æåéç¼åºä¸åç¹æ®ä¸é¼ççä»£çäººï¼ä»¥åæä¸è¿°åé¡ã

##### **Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities**
2502.00451v1 by Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych

Mental illness is a widespread and debilitating condition with substantial
societal and personal costs. Traditional diagnostic and treatment approaches,
such as self-reported questionnaires and psychotherapy sessions, often impose
significant burdens on both patients and clinicians, limiting accessibility and
efficiency. Recent advances in Artificial Intelligence (AI), particularly in
Natural Language Processing and multimodal techniques, hold great potential for
recognizing and addressing conditions such as depression, anxiety, bipolar
disorder, schizophrenia, and post-traumatic stress disorder. However, privacy
concerns, including the risk of sensitive data leakage from datasets and
trained models, remain a critical barrier to deploying these AI systems in
real-world clinical settings. These challenges are amplified in multimodal
methods, where personal identifiers such as voice and facial data can be
misused. This paper presents a critical and comprehensive study of the privacy
challenges associated with developing and deploying AI models for mental
health. We further prescribe potential solutions, including data anonymization,
synthetic data generation, and privacy-preserving model training, to strengthen
privacy safeguards in practical applications. Additionally, we discuss
evaluation frameworks to assess the privacy-utility trade-offs in these
approaches. By addressing these challenges, our work aims to advance the
development of reliable, privacy-aware AI tools to support clinical
decision-making and improve mental health outcomes.

æè¦ï¼ç²¾ç¥ç¾çæ¯ä¸ç¨®å»£æ³ä¸æä½¿äººè¡°å¼±çç¾çï¼æé æéå¤§çç¤¾æååäººææ¬ãå³çµ±çè¨ºæ·èæ²»çæ¹æ³ï¼ä¾å¦èªæå ±ååå·åå¿çæ²»ççç¨ï¼éå¸¸æå°æ£èåè¨åºé«çé æéå¤§è² æï¼éå¶äºå¯åæ§åæçãäººå·¥æºæ§ (AI) çææ°é²å±ï¼ç¹å¥æ¯å¨èªç¶èªè¨èçåå¤æ¨¡å¼æè¡æ¹é¢ï¼å¨è¾¨è­åèçæé¬±çãç¦æ®çãèºé¬±çãç²¾ç¥åè£çååµå·å¾å£åçåç¾¤ç­ç¾çæ¹é¢å·ææ¥µå¤§çæ½åãç¶èï¼é±ç§åé¡ï¼åæ¬è³æéåè¨ç·´æ¨¡åä¸­ææè³æå¤æ´©çé¢¨éªï¼ä»ç¶æ¯éäº AI ç³»çµ±å¨çå¯¦è¨åºç°å¢ä¸­é¨ç½²çä¸é ééµéç¤ãéäºææ°å¨å¤æ¨¡å¼æ¹æ³ä¸­æè¢«æ¾å¤§ï¼å çºèªé³åé¢é¨è³æç­åäººè­å¥è³æå¯è½æè¢«æ¿«ç¨ãæ¬æå°èéç¼åé¨ç½²ç¨æ¼å¿çå¥åº·ç AI æ¨¡åç¸éçé±ç§ææ°é²è¡äºä¸é æ¹å¤ä¸å¨é¢çç ç©¶ãæåé²ä¸æ­¥æåºäºæ½å¨çè§£æ±ºæ¹æ¡ï¼åæ¬è³æå¿ååãåæè³æç¢çåé±ç§ä¿è­·æ¨¡åè¨ç·´ï¼ä»¥å å¼·å¯¦éæç¨ä¸­çé±ç§ä¿éãæ­¤å¤ï¼æåè¨è«äºè©ä¼°éäºæ¹æ³ä¸­é±ç§èæç¨åæ¨çè©ä¼°æ¶æ§ãééè§£æ±ºéäºææ°ï¼æåçç ç©¶æ¨å¨æ¨é²å¯é ãéè¦é±ç§ç AI å·¥å·çéç¼ï¼ä»¥æ¯æè¨åºæ±ºç­å¶å®ä¸¦æ¹åå¿çå¥åº·ææã

##### **EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics**
2502.00205v1 by Omar H. Khater, Abdul Jabbar Siddiqui, M. Shamim Hossain

Sustainable agriculture plays a crucial role in ensuring world food security
for consumers. A critical challenge faced by sustainable precision agriculture
is weed growth, as weeds share essential resources with the crops, such as
water, soil nutrients, and sunlight, which notably affect crop yields. The
traditional methods employed to combat weeds include the usage of chemical
herbicides and manual weed removal methods. However, these could damage the
environment and pose health hazards. The adoption of automated computer vision
technologies and ground agricultural consumer electronic vehicles in precision
agriculture offers sustainable, low-carbon solutions. However, prior works
suffer from issues such as low accuracy and precision and high computational
expense. This work proposes EcoWeedNet, a novel model with enhanced weed
detection performance without adding significant computational complexity,
aligning with the goals of low-carbon agricultural practices. Additionally, our
model is lightweight and optimal for deployment on ground-based consumer
electronic agricultural vehicles and robots. The effectiveness of the proposed
model is demonstrated through comprehensive experiments on the CottonWeedDet12
benchmark dataset reflecting real-world scenarios. EcoWeedNet achieves
performance close to that of large models yet with much fewer parameters.
(approximately 4.21% of the parameters and 6.59% of the GFLOPs of YOLOv4). This
work contributes effectively to the development of automated weed detection
methods for next-generation agricultural consumer electronics featuring lower
energy consumption and lower carbon footprint. This work paves the way forward
for sustainable agricultural consumer technologies.

æè¦ï¼æ°¸çºè¾²æ¥­å¨ç¢ºä¿ä¸çç³§é£å®å¨æ¹é¢æ®æ¼èè³ééè¦çè§è²
å°æ¼æ°¸çºç²¾æºè¾²æ¥­ä¾èªªï¼éèççé·æ¯ä¸åéå¤§çææ°ï¼å çºéèèè¾²ä½ç©å±äº«æ°´ãåå£¤é¤ååé½åç­åºæ¬è³æºï¼éæé¡¯èå½±é¿è¾²ä½ç©çç¢éãå³çµ±ä¸ç¨æ¼å°æéèçæ¹æ³åæ¬ä½¿ç¨åå­¸é¤èååäººå·¥é¤èæ¹æ³ãç¶èï¼éäºæ¹æ³å¯è½ææå®³ç°å¢ä¸¦é æå¥åº·å±å®³ãå¨ç²¾æºè¾²æ¥­ä¸­æ¡ç¨èªååé»è¦è¦è¦ºæè¡åå°é¢è¾²æ¥­ç¨æ¶è²»é»å­è»è¼æä¾äºæ°¸çºçä½ç¢³è§£æ±ºæ¹æ¡ãç¶èï¼ååçç ç©¶å­å¨æºç¢ºåº¦åç²¾ç¢ºåº¦ä½ä»¥åè¨ç®ææ¬é«ç­åé¡ãéé ç ç©¶æåºäº EcoWeedNetï¼éæ¯ä¸åæ°çæ¨¡åï¼å·æå¢å¼·çéèåµæ¸¬æè½ï¼èä¸æå¢å é¡¯èçè¨ç®è¤éåº¦ï¼ç¬¦åä½ç¢³è¾²æ¥­å¯¦åçç®æ¨ãæ­¤å¤ï¼æåçæ¨¡åè¼å·§ï¼æé©æ¼é¨ç½²å¨å°é¢æ¶è²»é»å­è¾²æ¥­è»è¼åæ©å¨äººä¸ãææåºçæ¨¡åçæææ§å·²ééå¨åæ çå¯¦ä¸çå ´æ¯ç CottonWeedDet12 åºæºè³æéä¸é²è¡çå¨é¢å¯¦é©å¾å°è­æãEcoWeedNet çæè½æ¥è¿å¤§åæ¨¡åï¼ä½åæ¸å»å°å¾å¤ã(å¤§ç´æ¯ YOLOv4 åæ¸ç 4.21% å GFLOP ç 6.59%)ãéé ç ç©¶ææå°ä¿é²äºä¸ä¸ä»£è¾²æ¥­æ¶è²»é»å­ç¢åçèªååéèåµæ¸¬æ¹æ³çéç¼ï¼éäºç¢åçç¹é»æ¯è½èæ´ä½ãç¢³è¶³è·¡æ´ä½ãéé ç ç©¶çºæ°¸çºè¾²æ¥­æ¶è²»æè¡éªå¹³äºéè·¯ã

##### **DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets**
2502.00196v1 by Abdurrahim Yilmaz, Furkan Yuceyalcin, Ece Gokyayla, Donghee Choi, Ozan Erdem Ali Anil Demircali, Rahmetullah Varol, Ufuk Gorkem Kirabali, Gulsum Gencoglan, Joram M. Posma, Burak Temelkuran

A major barrier to developing vision large language models (LLMs) in
dermatology is the lack of large image--text pairs dataset. We introduce
DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated
from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for
dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using
Gemini 2.0, we used clinically related prompts and self-instruct method to
generate diverse and rich synthetic texts. Metadata of the datasets were
incorporated into the input prompts by targeting to reduce potential
hallucinations. The resulting dataset builds upon open access dermatological
image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have
permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary
Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We
anticipate this dataset to support and accelerate AI research in dermatology.
Data and code underlying this work are accessible at
https://github.com/abdurrahimyilmaz/DermaSynth.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼å¨ç®èç§ç¼å±çä¸å¤§éç¤æ¯ç¼ºä¹å¤§éçå½±åæå­å°æè³æéãæåå¼é² DermaSynthï¼éæ¯ä¸åç± 92,020 ååæå½±åæå­å°æè³æçµæçè³æéï¼éäºè³æå°æè³ææ¯å¾ 45,205 åå½±åï¼13,568 åè¨åºå½±åå 35,561 åç®èé¡å½±åï¼ä¸­ç­åèä¾çï¼ç¨æ¼ç®èç§ç¸éçè¨åºä»»åãå©ç¨æåé²ç LLMï¼ä½¿ç¨ Gemini 2.0ï¼æåä½¿ç¨èè¨åºç¸éçæç¤ºåèªææå°æ¹æ³ä¾ç¢çå¤æ¨£ä¸è±å¯çåææå­ãè³æéçåè³ææç´å¥è¼¸å¥æç¤ºä¸­ï¼ç®æ¨æ¯æ¸å°æ½å¨çå¹»è¦ºãç¢ççè³æéå»ºç«å¨éæ¾åç¨çç®èç§å½±åå²å­åº«ï¼DERM12345ãBCN20000ãPAD-UFES-20ãSCIN å HIBAï¼ä¹ä¸ï¼éäºå²å­åº«ææå¯¬é¬ç CC-BY-4.0 licensesãæåéå°ä¸ååæ­¥ç Llama-3.2-11B-Vision-Instruct æ¨¡åï¼DermatoLlama 1.0ï¼å¨ 5,000 åæ¨£æ¬ä¸é²è¡å¾®èª¿ãæåé è¨éåè³æéå°æ¯æ´åå éç®èç§ç AI ç ç©¶ãéé å·¥ä½çè³æåç¨å¼ç¢¼å¯å¨ https://github.com/abdurrahimyilmaz/DermaSynth åå¾ã

##### **Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study**
2502.00146v1 by Hassan Jahanandish, Shengtian Sang, Cynthia Xinran Li, Sulaiman Vesal, Indrani Bhattacharya, Jeong Hoon Lee, Richard Fan, Geoffrey A. Sonna, Mirabela Rusu

Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target
suspicious prostate lesions. This has led to artificial intelligence (AI)
applications improving MRI-based detection of clinically significant prostate
cancer (CsPCa). However, MRI-detected lesions must still be mapped to
transrectal ultrasound (TRUS) images during biopsy, which results in missing
CsPCa. This study systematically evaluates a multimodal AI framework
integrating MRI and TRUS image sequences to enhance CsPCa identification. The
study included 3110 patients from three cohorts across two institutions who
underwent prostate biopsy. The proposed framework, based on the 3D UNet
architecture, was evaluated on 1700 test cases, comparing performance to
unimodal AI models that use either MRI or TRUS alone. Additionally, the
proposed model was compared to radiologists in a cohort of 110 patients. The
multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice
(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared
to radiologists, the multimodal model showed higher specificity (88% vs. 78%)
and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings
demonstrate the potential of multimodal AI to improve CsPCa lesion targeting
during biopsy and treatment planning, surpassing current unimodal models and
radiologists; ultimately improving outcomes for prostate cancer patients.

æè¦ï¼<paragraph>å¨æ´»æª¢åï¼ç£æ¯é å½±ï¼MRIï¼æ­£è¶ä¾è¶å¸¸è¢«ç¨æ¼éå®å¯ççæè­·èºçç¶ãéå°è´äººå·¥æºæ§ï¼AIï¼æç¨ç¨å¼æ¹åäºä»¥ MRI çºåºç¤çè¨åºé¡¯èæè­·èºçï¼CsPCaï¼æª¢æ¸¬ãç¶èï¼å¨æ´»æª¢æéï¼ç± MRI åµæ¸¬å°ççç¶ä»å¿é å°æå°ç¶ç´è¸è¶é³æ³¢ï¼TRUSï¼å½±åï¼éå°è´é¯å¤± CsPCaãæ¬ç ç©¶ç³»çµ±æ§å°è©ä¼°äºä¸åå¤æ¨¡æ AI æ¶æ§ï¼æ´å MRI å TRUS å½±ååºåï¼ä»¥å¢å¼· CsPCa è­å¥ãéé ç ç©¶ç´å¥äºä¾èªå©å®¶æ©æ§çä¸åç¾¤çµä¸­ç 3110 åæ£èï¼ä»åæ¥åäºæè­·èºæ´»æª¢ãææåºçæ¶æ§åºæ¼ 3D UNet æ¶æ§ï¼å¨ 1700 åæ¸¬è©¦æ¡ä¾ä¸­é²è¡è©ä¼°ï¼ä¸¦å°å¶æè½èåä½¿ç¨ MRI æ TRUS çå®æ¨¡æ AI æ¨¡åé²è¡æ¯è¼ãæ­¤å¤ï¼ææåºçæ¨¡åèä¸åç± 110 åæ£èçµæçç¾¤çµä¸­çæ¾å°ç§é«å¸«é²è¡æ¯è¼ãèå®æ¨¡æ MRIï¼73%ã30%ï¼å TRUS æ¨¡åï¼49%ã27%ï¼ç¸æ¯ï¼å¤æ¨¡æ AI æ¹æ³éå°äºæ´é«çææåº¦ï¼80%ï¼åçç¶ Diceï¼42%ï¼ãèæ¾å°ç§é«å¸«ç¸æ¯ï¼å¤æ¨¡ææ¨¡åé¡¯ç¤ºåºæ´é«çç¹ç°æ§ï¼88% å° 78%ï¼åçç¶ Diceï¼38% å° 33%ï¼ï¼ä¸ææåº¦ç¸ç¶ï¼79%ï¼ãæåçç ç©¶çµæè­æäºå¤æ¨¡æ AI å¨æ´»æª¢åæ²»çè¨ç«æéæ¹å CsPCa çç¶éå®çæ½åï¼è¶è¶äºç®åçå®æ¨¡ææ¨¡ååæ¾å°ç§é«å¸«ï¼æçµæ¹åäºæè­·èºçæ£èçæ²»çææã</paragraph>

##### **AIN: The Arabic INclusive Large Multimodal Model**
2502.00094v2 by Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan

Amid the swift progress of large language models (LLMs) and their evolution
into large multimodal models (LMMs), significant strides have been made in
high-resource languages such as English and Chinese. While Arabic LLMs have
seen notable progress, Arabic LMMs remain largely unexplored, often narrowly
focusing on a few specific aspects of the language and visual understanding. To
bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal
Model-designed to excel across diverse domains. AIN is an English-Arabic
bilingual LMM designed to excel in English and Arabic, leveraging carefully
constructed 3.6 million high-quality Arabic-English multimodal data samples.
AIN demonstrates state-of-the-art Arabic performance, while also possessing
strong English-language visual capabilities. On the recent CAMEL-Bench
benchmark comprising 38 sub-domains including, multi-image understanding,
complex visual perception, handwritten document understanding, video
understanding, medical imaging, plant diseases, and remote sensing-based land
use understanding, our AIN demonstrates strong performance with the 7B model
outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains
and 38 sub-domains. AIN's superior capabilities position it as a significant
step toward empowering Arabic speakers with advanced multimodal generative AI
tools across diverse applications.

æè¦ï¼å¨å¤§åèªè¨æ¨¡å (LLM) å¿«éç¼å±ï¼ä¸¦æ¼è®æå¤§åå¤æ¨¡ææ¨¡å (LMM) çéç¨ä¸­ï¼è±èªåä¸­æç­é«è³æºèªè¨å·²åå¾éå¤§é²å±ãéç¶é¿æä¼¯èª LLM å·²åå¾é¡¯èé²å±ï¼ä½é¿æä¼¯èª LMM ä»æªè¢«å»£æ³æ¢ç´¢ï¼éå¸¸åªç¹éå°éæ³¨èªè¨åè¦è¦ºçè§£çå¹¾åç¹å®æ¹é¢ãçºäºå½åéé å·®è·ï¼æåæ¨åºäº AINï¼å³é¿æä¼¯èªåå®¹æ§å¤æ¨¡ææ¨¡åï¼æ¨å¨å¨ä¸åé åä¸­è¡¨ç¾åºè²ãAIN æ¯ä¸åè±èª-é¿æä¼¯èªéèª LMMï¼æ¨å¨ç²¾éè±èªåé¿æä¼¯èªï¼å©ç¨ç²¾å¿å»ºæ§ç 360 è¬åé«åè³ªé¿æä¼¯èª-è±èªå¤æ¨¡ææ¸ææ¨£æ¬ãAIN å±ç¤ºäºæåé²çé¿æä¼¯èªæè½ï¼åæä¹å·åå¼·å¤§çè±èªè¦è¦ºè½åãå¨æè¿ç CAMEL-Bench åºæºæ¸¬è©¦ä¸­ï¼åå« 38 åå­é åï¼åæ¬å¤å½±åçè§£ãè¤éè¦è¦ºæç¥ãæå¯«æä»¶çè§£ãå½±ççè§£ãé«å­¸å½±åãæ¤ç©ç¾çååºæ¼éæ¸¬çåå°ä½¿ç¨çè§£ï¼æåç AIN è¡¨ç¾åºè²ï¼å¶ä¸­ 7B æ¨¡åå¨å«åé åå 38 åå­é åçå¹³åçµå°å¢ççº 3.4%ï¼åªæ¼ GPT-4oãAIN çåè¶è½åä½¿å¶æçºæèè³¦äºé¿æä¼¯èªä½¿ç¨èé²éå¤æ¨¡æçæå¼ AI å·¥å·éåºçéè¦ä¸æ­¥ï¼å¯ç¨æ¼åç¨®æç¨ã

##### **Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**
2501.19338v1 by Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente IstvÃ¡n LÃ¡nczi, Andras Jakab

Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.

æè¦ï¼<paragraph>éç¼ç¨æ¼èªååæè¨åºèååæ°çå MRI è³æçæ°æ¹æ³åå°æ¨è¨»ççè³æéç¨å°åé±ç§åé¡çéå¶ï¼éäºåé¡éå¸¸æéå¶è³æå±äº«ï¼å¾èé»ç¤æ·±åº¦å­¸ç¿æ¨¡åçæææ§ãæåä»¥å©ç¨®æ¹å¼è§£æ±ºéååé¡ãé¦åï¼æåå¼å¥äº Fetal&Neonatal-DDPMï¼éæ¯ä¸åæ°ç©çæ´æ£æ¨¡åæ¶æ§ï¼æ¨å¨å¾èªç¾©æ¨ç±¤å½±åçæé«åè³ªçåæççèååæ°çå MRIãå¶æ¬¡ï¼æåééå½¢ææ¹è®ä¾ä¿®æ¹å¥åº·çæ¨ç±¤å½±åï¼ä»¥æ¨¡æ¬è¦å®¤æ´å¤§ãå°è¦åæ©è¦å°è¦ç¼è²ä¸å¨ä»¥åå°é ­ç¸å½¢ç­ææ³ï¼å¾èå¢å¼·è¨ç·´è³æãééå©ç¨ Fetal&Neonatal-DDPMï¼æåå¾éäºä¿®æ¹å¾çççæ¨ç±¤å½±åä¸­åæäºé¼çççç MRIãæ¾å°ç§é«å¸«è©ä¼°åæ MRI çåè³ªåè¨ºæ·å¹å¼é¡¯èåªæ¼çå¯¦ MRIï¼p < 0.05ï¼ï¼å±ç¤ºäºè¡ç®¡åèçµ¡å¢ç­ç¹å¾µï¼ä¸¦æ¹åäºèæ¨ç±¤è¨»è§£çä¸è´æ§ãåæççè³æå¢å¼·äºæåé²ç nnUNet åå²æè½ï¼ç¹å¥æ¯å°æ¼å´éçè¦å®¤æ´å¤§çä¾ï¼å¶ä¸­è¦å®¤åå²ï¼Dice åæ¸ï¼0.9253 å° 0.7317ï¼çæ¹åæå¤§ãéé ç ç©¶å¼·èª¿äºçæå¼ AI ä½çºè³ææ´åè½åå·¥å·çæ½åï¼å¨çççä¾ä¸­æä¾äºæ¹åçåå²æè½ãéé ç¼å±ä»£è¡¨äºæ¹åç¢åå½±ååæååå²æºç¢ºæ§çéè¦ä¸æ­¥ï¼ä¹çºééçæççå½±åè³æä¾é²è¡è³æå¿ååæä¾äºæ°æ¹æ³ã</paragraph>

##### **Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**
2501.19271v1 by Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett

Concept-based explanation methods, such as concept bottleneck models (CBMs),
aim to improve the interpretability of machine learning models by linking their
decisions to human-understandable concepts, under the critical assumption that
such concepts can be accurately attributed to the network's feature space.
However, this foundational assumption has not been rigorously validated, mainly
because the field lacks standardised metrics and benchmarks to assess the
existence and spatial alignment of such concepts. To address this, we propose
three metrics: the concept global importance metric, the concept existence
metric, and the concept location metric, including a technique for visualising
concept activations, i.e., concept activation mapping. We benchmark post-hoc
CBMs to illustrate their capabilities and challenges. Through qualitative and
quantitative experiments, we demonstrate that, in many cases, even the most
important concepts determined by post-hoc CBMs are not present in input images;
moreover, when they are present, their saliency maps fail to align with the
expected regions by either activating across an entire object or misidentifying
relevant concept-specific regions. We analyse the root causes of these
limitations, such as the natural correlation of concepts. Our findings
underscore the need for more careful application of concept-based explanation
techniques especially in settings where spatial interpretability is critical.

æè¦ï¼åºæ¼æ¦å¿µçè§£éæ¹æ³ï¼ä¾å¦æ¦å¿µç¶é ¸æ¨¡å (CBM)ï¼æ¨å¨ééå°æ©å¨å­¸ç¿æ¨¡åçæ±ºç­èäººé¡å¯çè§£çæ¦å¿µé£çµï¼ä¾æåæ©å¨å­¸ç¿æ¨¡åçå¯è§£éæ§ï¼å¶ééµåè¨­çºæ­¤é¡æ¦å¿µå¯ä»¥æºç¢ºå°æ­¸å æ¼ç¶²è·¯çç¹å¾µç©ºéãç¶èï¼æ­¤é åºç¤åè¨­å°æªç¶éå´æ ¼é©è­ï¼ä¸»è¦æ¯å çºè©²é åç¼ºä¹æ¨æºåææ¨ååºæºä¾è©ä¼°æ­¤é¡æ¦å¿µçå­å¨åç©ºéå°é½ãçºäºè§£æ±ºéååé¡ï¼æåæåºä¸é ææ¨ï¼æ¦å¿µæ´é«éè¦æ§ææ¨ãæ¦å¿µå­å¨ææ¨åæ¦å¿µä½ç½®ææ¨ï¼åæ¬ä¸ç¨®ç¨æ¼è¦è¦ºåæ¦å¿µæ´»åï¼å³æ¦å¿µæ´»åå°æçæè¡ãæåå°äºå¾ CBM é²è¡åºæºæ¸¬è©¦ï¼ä»¥èªªæå®åçè½ååææ°ãééå®æ§åå®éå¯¦é©ï¼æåè­æï¼å¨è¨±å¤ææ³ä¸ï¼å³ä½¿æ¯ç±äºå¾ CBM ç¢ºå®çæéè¦æ¦å¿µä¹ä¸å­å¨æ¼è¼¸å¥å½±åä¸­ï¼æ­¤å¤ï¼ç¶å®åå­å¨æï¼å®åçé¡¯èæ§åç¡æ³èé æçååå°é½ï¼åå å¯è½æ¯å®åå¨æ´åç©ä»¶ä¸­æ´»åï¼æé¯èª¤è¾¨è­åºç¸éçæ¦å¿µç¹å®ååãæååæäºéäºéå¶çæ ¹æ¬åå ï¼ä¾å¦æ¦å¿µçèªç¶ç¸éæ§ãæåçç ç©¶çµæå¼·èª¿éè¦æ´å°å¿å°æç¨åºæ¼æ¦å¿µçè§£éæè¡ï¼ç¹å¥æ¯å¨ç©ºéå¯è§£éæ§è³ééè¦çè¨­å®ä¸­ã

##### **Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**
2501.19176v1 by Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi

Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.

æè¦ï¼å¨è¦éæ¸ä½ä¹³æ¿æå½± (FFDM) æ¯å¸¸è¦ä¹³çç¯©æª¢çä¸»è¦å½±åæ¨¡å¼ï¼ç¶èï¼å°æ¼ä¹³æ¿çµç¹ç·»å¯æçºç¶­åè«çè®çæ£èï¼å¶æææ§åå°éå¶ãå°æ¯å¢å¼·åè­ä¹³æ¿æå½± (CESM) æ¯ä¸ç¨®äºç´å½±åæè¡ï¼å¯æåè«ç¤åµæ¸¬çæºç¢ºåº¦ãåç®¡å¦æ­¤ï¼ç±æ¼è¼é«çè¼»å°æé²ãå°æ¯åçä½¿ç¨åæéçå¯åæ§ï¼éå¶äºå¶æç¨ãå æ­¤ï¼CESM éå¸¸åä¿çå¨ç¹å®ææ³ä¸ä½¿ç¨ï¼åç®¡ CESM çè¨ºæ·æè½è¼ä½³ï¼ä½è¨±å¤æ£èä»åªè½ä¾è³´ FFDMãéç¶åçæª¢æ¥ä»ç¶æ¯æç¢ºè¨ºæ·çé»éæ¨æºï¼ä½éæ¯ä¸ç¨®ä¾µå¥æ§ç¨åºï¼å¯è½æè®æ£èæå°ä¸é©ãæåå¼å¥ä¸ç¨®å¤æ¨¡å¼ãå¤è¦åçæ·±åº¦å­¸ç¿æ¹æ³é²è¡èæ¬åçæª¢æ¥ï¼å° FFDM å CESM æ¨¡å¼æ´åå¨é ­å°¾ååå§å¤å´æè¦åä¸­ï¼ä»¥å°çç¶åé¡çºæ¡æ§æè¯æ§ãçºäºè§£æ±º CESM è³æç¼ºå¤±çææ°ï¼æåå©ç¨çæå¼äººå·¥æºæ§å¾ FFDM ææä¸­æ¨ç® CESM å½±åãå¯¦é©çµæè­æï¼æ´å CESM æ¨¡å¼å°æ¼æåèæ¬åçæª¢æ¥çæè½è³ééè¦ãç¶çå¯¦ CESM è³æç¼ºå¤±æï¼åæ CESM å½±åè¢«è­ææ¯ææçï¼å¶æè½åªæ¼å®ç¨ä½¿ç¨ FFDMï¼ç¹å¥æ¯å¨çµå FFDM å CESM æ¨¡å¼çå¤æ¨¡å¼éç½®ä¸­ãææåºçæ¹æ³ææ½åæ¹åè¨ºæ·å·¥ä½æµç¨ï¼çºè¨åºé«å¸«æä¾å¢å¼·çæºæ§å·¥å·ï¼ä»¥æé«è¨ºæ·æºç¢ºåº¦åæ£èç§è­·ãæ­¤å¤ï¼ä½çºå°ç ç©¶ç¤¾ç¾¤çè²¢ç»ï¼æåå¬éç¼å¸å¨å¯¦é©ä¸­ä½¿ç¨çè³æéï¼ä»¥ä¿é²æ­¤é åçé²ä¸æ­¥é²å±ã

##### **Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**
2501.19086v1 by Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang

X-ray imaging is pivotal in medical diagnostics, offering non-invasive
insights into a range of health conditions. Recently, vision-language models,
such as the Contrastive Language-Image Pretraining (CLIP) model, have
demonstrated potential in improving diagnostic accuracy by leveraging
large-scale image-text datasets. However, since CLIP was not initially designed
for medical images, several CLIP-like models trained specifically on medical
images have been developed. Despite their enhanced performance, issues of
fairness - particularly regarding demographic attributes - remain largely
unaddressed. In this study, we perform a comprehensive fairness analysis of
CLIP-like models applied to X-ray image classification. We assess their
performance and fairness across diverse patient demographics and disease
categories using zero-shot inference and various fine-tuning techniques,
including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation
(LoRA), and full fine-tuning. Our results indicate that while fine-tuning
improves model accuracy, fairness concerns persist, highlighting the need for
further fairness interventions in these foundational models.

æè¦ï¼X åå½±åå¨é«çè¨ºæ·ä¸­è³ééè¦ï¼è½æä¾åç¨®å¥åº·çæ³çéä¾µå¥æ§è¦è§£ãæè¿ï¼è¦è¦ºèªè¨æ¨¡åï¼ä¾å¦å°æ¯èªè¨å½±åé è¨ç·´ (CLIP) æ¨¡åï¼å·²è­æææ½åééå©ç¨å¤§è¦æ¨¡å½±åæå­è³æéä¾æ¹åè¨ºæ·æºç¢ºæ§ãç¶èï¼ç±æ¼ CLIP æåä¸¦éè¨­è¨ç¨æ¼é«çå½±åï¼å æ­¤å·²ç¶éç¼äºæ¸åç¹å¥éå°é«çå½±åè¨ç·´çé¡ä¼¼ CLIP æ¨¡åãåç®¡å®åçæè½æææåï¼ä½å¬å¹³æ§çåé¡ï¼ç¹å¥æ¯éæ¼äººå£çµ±è¨å±¬æ§ï¼ä»å¤§å¤æªç²è§£æ±ºãå¨æ¬ç ç©¶ä¸­ï¼æåå°æç¨æ¼ X åå½±ååé¡çé¡ä¼¼ CLIP æ¨¡åå·è¡å¨é¢çå¬å¹³æ§åæãæåä½¿ç¨é¶æ¬¡å­¸ç¿æ¨è«ååç¨®å¾®èª¿æè¡ï¼åæ¬ç·æ§æ¢æ¥ãå¤å±¤æç¥å¨ (MLP)ãä½ç§©é©æ (LoRA) åå®æ´å¾®èª¿ï¼ä¾è©ä¼°å®åå¨ä¸åæ£èäººå£çµ±è¨åç¾çé¡å¥ä¸­çæè½åå¬å¹³æ§ãæåççµæè¡¨æï¼éç¶å¾®èª¿ææ¹åæ¨¡åæºç¢ºæ§ï¼ä½å¬å¹³æ§åé¡ä»ç¶å­å¨ï¼å¼·èª¿éè¦å¨éäºåºç¤æ¨¡åä¸­é²ä¸æ­¥æ¡åå¬å¹³æ§å¹²é æªæ½ã

##### **Survey and Improvement Strategies for Gene Prioritization with Large Language Models**
2501.18794v1 by Matthew Neeley, Guantong Qi, Guanchu Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, Fan Xia, Pengfei Liu, Zhandong Liu, Xia Hu

Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.

æè¦ï¼ç½è¦ç¾çç±æ¼æ£èæ¸ææéåéºå³å¤æ¨£æ§ï¼è¨ºæ·èµ·ä¾å·æææ°æ§ãåç®¡è®ç°åªåç´æåºæè¡é²æ­¥ï¼ä½è¨±å¤çä¾ä»æªå¾å°è¨ºæ·ãåç®¡å¤§åèªè¨æ¨¡å (LLM) å¨é«å­¸èè©¦ä¸­è¡¨ç¾è¯å¥½ï¼ä½å®åå¨è¨ºæ·ç½è¦éºå³ç¾çæ¹é¢çæææ§å°æªå¾å°è©ä¼°ãçºäºè­å¥è´çåºå ï¼æåå°åç¨® LLM é²è¡äºåºå åªåç´æåºåºæºæ¸¬è©¦ãä½¿ç¨å¤æºè½é«åäººé¡è¡¨åæ¬ä½ (HPO) åé¡ï¼æåæ ¹æè¡¨ååå¯è§£æ±ºæ§å°æ£èé²è¡äºåé¡ãé¨èåºå çµå¤§å°çå¢å ï¼LLM æ§è½ä¸éï¼å æ­¤æåä½¿ç¨åèæ²»ä¹ç­ç¥å°ä»»ååè§£çºæ´å°çå­éãå¨åºç·ä¸­ï¼GPT-4 åªæ¼å¶ä» LLMï¼å¨æ­£ç¢ºæåºè´çåºå æ¹é¢éå°è¿ 30% çæºç¢ºåº¦ãå¤æºè½é«å HPO æ¹æ³æå©æ¼ååè§£æ±ºæä¿¡å¿ççä¾åå·æææ°æ§ççä¾ï¼å¼·èª¿å·²ç¥åºå -è¡¨åéè¯åè¡¨åç¹ç°æ§çéè¦æ§ãæåç¼ç¾å·æç¹å®è¡¨åææç¢ºéè¯ççä¾å¾å°æ´æºç¢ºçè§£æ±ºãç¶èï¼æåè§å¯å°å°ç ç©¶ååçåºå åè¼¸å¥é åºæææ§çåå·®ï¼éé»ç¤äºåºå åªåç´æåºãæåçåèæ²»ä¹ç­ç¥ééåæéäºåå·®ä¾æé«æºç¢ºæ§ãééå©ç¨ HPO åé¡ãæ°ç©çå¤æºè½é«æè¡åæåç LLM ç­ç¥ï¼æåèæåçåºç·è©ä¼°ç¸æ¯æé«äºè´çåºå è­å¥æºç¢ºæ§ãéç¨®æ¹æ³ç°¡åäºç½è¦ç¾ççè¨ºæ·ï¼ä¿é²äºå°æªè§£æ±ºçä¾çéæ°åæï¼ä¸¦å éäºåºå ç¼ç¾ï¼æ¯æäºé¶åè¨ºæ·åæ²»ççéç¼ã

##### **Synthetic Data Generation for Augmenting Small Samples**
2501.18741v1 by Dan Liu, Samer El Kababji, Nicholas Mitsakakis, Lisa Pilgram, Thomas Walters, Mark Clemons, Greg Pond, Alaa El-Hussuna, Khaled El Emam

Small datasets are common in health research. However, the generalization
performance of machine learning models is suboptimal when the training datasets
are small. To address this, data augmentation is one solution. Augmentation
increases sample size and is seen as a form of regularization that increases
the diversity of small datasets, leading them to perform better on unseen data.
We found that augmentation improves prognostic performance for datasets that:
have fewer observations, with smaller baseline AUC, have higher cardinality
categorical variables, and have more balanced outcome variables. No specific
generative model consistently outperformed the others. We developed a decision
support model that can be used to inform analysts if augmentation would be
useful. For seven small application datasets, augmenting the existing data
results in an increase in AUC between 4.31% (AUC from 0.71 to 0.75) and 43.23%
(AUC from 0.51 to 0.73), with an average 15.55% relative improvement,
demonstrating the nontrivial impact of augmentation on small datasets
(p=0.0078). Augmentation AUC was higher than resampling only AUC (p=0.016). The
diversity of augmented datasets was higher than the diversity of resampled
datasets (p=0.046).

æè¦ï¼å¨å¥åº·ç ç©¶ä¸­ï¼å°åæ°æ®éå¾å¸¸è§ãç¶èï¼å½è®­ç»æ°æ®éè¾å°æ¶ï¼æºå¨å­¦ä¹ æ¨¡åçæ³åæ§è½å¹¶ä¸çæ³ãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æ°æ®å¢å¼ºæ¯ä¸ç§è§£å³æ¹æ¡ãå¢å¼ºå¢å äºæ ·æ¬éï¼å¹¶è¢«è§ä¸ºä¸ç§æ­£ååå½¢å¼ï¼å®å¢å äºå°åæ°æ®éçå¤æ ·æ§ï¼ä»èä½¿å¶å¨æªè§æ°æ®ä¸è¡¨ç°å¾æ´å¥½ãæä»¬åç°ï¼å¢å¼ºæé«äºä»¥ä¸æ°æ®éçé¢æµæ§è½ï¼å·æè¾å°çè§æµå¼ãè¾å°çåºçº¿ AUCãè¾é«çåºæ°åç±»åéä»¥åæ´å¹³è¡¡çç»æåéãæ²¡æç¹å®ççææ¨¡åå§ç»ä¼äºå¶ä»æ¨¡åãæä»¬å¼åäºä¸ä¸ªå³ç­æ¯ææ¨¡åï¼å¯ç¨äºåç¥åæå¸å¢å¼ºæ¯å¦æç¨ãå¯¹äºä¸ä¸ªå°ååºç¨ç¨åºæ°æ®éï¼å¢å¼ºç°ææ°æ®å¯¼è´ AUC å¢å  4.31%ï¼AUC ä» 0.71 å¢å å° 0.75ï¼å 43.23%ï¼AUC ä» 0.51 å¢å å° 0.73ï¼ï¼å¹³åç¸å¯¹æ¹è¿ 15.55%ï¼è¿è¡¨æäºå¢å¼ºå¯¹å°åæ°æ®éçéå¹³å¡å½±åï¼p=0.0078ï¼ãå¢å¼º AUC é«äºä»éæ°éæ ·ç AUCï¼p=0.016ï¼ãå¢å¼ºæ°æ®éçå¤æ ·æ§é«äºéæ°éæ ·æ°æ®éçå¤æ ·æ§ï¼p=0.046ï¼ã

##### **A Multi-Layered Large Language Model Framework for Disease Prediction**
2502.00063v1 by Malak Mohamed, Rokaia Emad, Ali Hamdi

Social telehealth has revolutionized healthcare by enabling patients to share
symptoms and receive medical consultations remotely. Users frequently post
symptoms on social media and online health platforms, generating a vast
repository of medical data that can be leveraged for disease classification and
symptom severity assessment. Large language models (LLMs), such as LLAMA3,
GPT-3.5 Turbo, and BERT, process complex medical data to enhance disease
classification. This study explores three Arabic medical text preprocessing
techniques: text summarization, text refinement, and Named Entity Recognition
(NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best
performance was achieved using CAMeL-BERT with NER-augmented text (83% type
classification, 69% severity assessment). Non-fine-tuned models performed
poorly (13%-20% type classification, 40%-49% severity assessment). Integrating
LLMs into social telehealth systems enhances diagnostic accuracy and treatment
outcomes.

æè¦ï¼ç¤¾äº¤é è·é«çééè®æ£èå¯ä»¥é è·åäº«ççä¸¦æ¥åé«çè«®è©¢ï¼å¾¹åºæ¹è®äºé«çä¿å¥ãä½¿ç¨èç¶å¸¸å¨ç¤¾ç¾¤åªé«åç·ä¸å¥åº·å¹³å°ä¸ç¼å¸ççï¼ç¢çäºé¾å¤§çé«çè³æåº«ï¼å¯å©ç¨æ¼ç¾çåé¡åççå´éæ§è©ä¼°ãå¤§åèªè¨æ¨¡å (LLM)ï¼ä¾å¦ LLAMA3ãGPT-3.5 Turbo å BERTï¼èçè¤éçé«çè³æä»¥å¢å¼·ç¾çåé¡ãæ¬ç ç©¶æ¢è¨äºä¸ç¨®é¿æä¼¯èªé«çæå­åèçæè¡ï¼æå­æè¦ãæå­ç²¾çåå½åå¯¦é«è¾¨è­ (NER)ãä½¿ç¨ LoRA è©ä¼° CAMeL-BERTãAraBERT å Asafaya-BERTï¼ä½¿ç¨å· NER å¢å¼·æå­ç CAMeL-BERT ç²å¾æä½³æè½ï¼83% é¡ååé¡ï¼69% å´éæ§è©ä¼°ï¼ãæªç¶éå¾®èª¿çæ¨¡åæè½ä¸ä½³ï¼13%-20% é¡ååé¡ï¼40%-49% å´éæ§è©ä¼°ï¼ãå° LLM æ´åå°ç¤¾äº¤é è·é«çç³»çµ±ä¸­å¯å¢å¼·è¨ºæ·æºç¢ºæ§åæ²»ççµæã

##### **A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**
2501.18367v1 by Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li

In medical time series disease diagnosis, two key challenges are
identified.First, the high annotation cost of medical data leads to overfitting
in models trained on label-limited, single-center datasets. To address this, we
propose incorporating external data from related tasks and leveraging AE-GAN to
extract prior knowledge,providing valuable references for downstream tasks.
Second, many existing studies employ contrastive learning to derive more
generalized medical sequence representations for diagnostic tasks, usually
relying on manually designed diverse positive and negative sample
pairs.However, these approaches are complex, lack generalizability, and fail to
adaptively capture disease-specific features across different conditions.To
overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework),
a framework that integrates a multi-head attention mechanism and adaptively
learns representations from different views through inter-view and intra-view
contrastive learning strategies.Additionally, the pre-trained AE-GAN is used to
reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process.Experiments on three
target datasets demonstrate that our method consistently outperforms seven
other baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease.

æè¦ï¼å¨å»çæ¶é´åºåç¾çè¯æ­ä¸­ï¼ç¡®å®äºä¸¤ä¸ªå³é®ææãé¦åï¼å»çæ°æ®çæ æ³¨ææ¬é«ï¼å¯¼è´å¨æ ç­¾åéçåä¸­å¿æ°æ®éä¸è®­ç»çæ¨¡ååºç°è¿æåãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æä»¬å»ºè®®åå¹¶æ¥èªç¸å³ä»»å¡çå¤é¨æ°æ®ï¼å¹¶å©ç¨ AE-GAN æååéªç¥è¯ï¼ä¸ºä¸æ¸¸ä»»å¡æä¾æä»·å¼çåèãå¶æ¬¡ï¼è®¸å¤ç°æçç ç©¶éç¨å¯¹æ¯å­¦ä¹ æ¥æ¨å¯¼åºæ´éç¨çå»çåºåè¡¨ç¤ºï¼ç¨äºè¯æ­ä»»å¡ï¼éå¸¸ä¾èµäºæå¨è®¾è®¡çåç§æ­£è´æ ·æ¬å¯¹ãç¶èï¼è¿äºæ¹æ³å¤æï¼ç¼ºä¹éç¨æ§ï¼å¹¶ä¸æ æ³èªéåºå°æè·ä¸åæ¡ä»¶ä¸çç¹å®ç¾çç¹å¾ãä¸ºäºåæè¿ä¸ªé®é¢ï¼æä»¬å¼å¥äº LMCFï¼å¯å­¦ä¹ çå¤è§å¾å¯¹æ¯æ¡æ¶ï¼ï¼è¿æ¯ä¸ä¸ªéæäºå¤å¤´æ³¨ææºå¶çæ¡æ¶ï¼å¹¶éè¿è§å¾é´åè§å¾åå¯¹æ¯å­¦ä¹ ç­ç¥èªéåºå°å­¦ä¹ æ¥èªä¸åè§å¾çè¡¨ç¤ºãæ­¤å¤ï¼é¢è®­ç»ç AE-GAN ç¨äºéå»ºç®æ æ°æ®ä¸­çå·®å¼ä½ä¸ºç¾çæ¦çï¼ç¶åå°å¶éæå°å¯¹æ¯å­¦ä¹ è¿ç¨ä¸­ãå¨ä¸ä¸ªç®æ æ°æ®éä¸çå®éªè¡¨æï¼æä»¬çæ¹æ³å§ç»ä¼äºå¶ä»ä¸ä¸ªåºçº¿ï¼çªåºäºå¶å¯¹å»çä¿å¥åºç¨ï¼å¦å¿èæ¢å¡ãé¿å°è¨æµ·é»çåå¸éæ£®ççè¯æ­ï¼çéå¤§å½±åã

##### **MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**
2501.18362v1 by Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou

We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.

æè¦ï¼æåæ¨åºäº MedXpertQAï¼éæ¯ä¸åæ¥µå·ææ°æ§ä¸å¨é¢çåºæºï¼ç¨æ¼è©ä¼°å°å®¶ç´çé«å­¸ç¥è­ååé²çæ¨çè½åãMedXpertQA åå« 4,460 ååé¡ï¼æ¶µè 17 åå°ç§å 11 åèº«é«ç³»çµ±ãå®åå«å©åå­éï¼ææ¬ç¨æ¼ææ¬è©ä¼°ï¼MM ç¨æ¼å¤æ¨¡å¼è©ä¼°ãå¼å¾æ³¨æçæ¯ï¼MM å¼å¥äºå°å®¶ç´èè©¦é¡ç®ï¼å¶ä¸­åå«å¤æ¨£åçå½±ååè±å¯çè¨åºè³è¨ï¼åæ¬æ£èè¨éåæª¢æ¥çµæï¼éè®å®æå¥æ¼å³çµ±çé«å­¸å¤æ¨¡å¼åºæºï¼å¾èæ¯å¾å½±åæ¨é¡ä¸­ç¢ççç°¡å®åç­å°ãMedXpertQA æ¡ç¨å´æ ¼çéæ¿¾åæ´åï¼ä»¥è§£æ±º MedQA ç­ç¾æåºæºçé£åº¦ä¸è¶³åé¡ï¼ä¸¦ç´å¥å°ç§å§å¡æåé¡ä»¥æé«è¨åºç¸éæ§åå¨é¢æ§ãæåå·è¡è³æåæä»¥éä½è³æå¤æ´©é¢¨éªï¼ä¸¦é²è¡å¤è¼ªå°å®¶å¯©æ¥ä»¥ç¢ºä¿æºç¢ºæ§åå¯é æ§ãæåå¨ MedXpertQA ä¸è©ä¼°äº 16 åé åçæ¨¡åãæ­¤å¤ï¼é«å­¸èç¾å¯¦ä¸ççæ±ºç­å¶å®æå¯åçè¯ç¹«ï¼æä¾äºè±å¯ä¸å·ä»£è¡¨æ§çç°å¢ï¼ç¨æ¼è©ä¼°è¶è¶æ¸å­¸åç¨å¼ç¢¼çæ¨çè½åãçºæ­¤ï¼æåéç¼äºä¸åä»¥æ¨ççºå°åçå­éï¼ä»¥å©æ¼è©ä¼°é¡ o1 çæ¨¡åã

##### **CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**
2501.18328v1 by Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai

MRI imputation aims to synthesize the missing modality from one or more
available ones, which is highly desirable since it reduces scanning costs and
delivers comprehensive MRI information to enhance clinical diagnosis. In this
paper, we propose a unified model, CodeBrain, designed to adapt to various
brain MRI imputation scenarios. The core design lies in casting various
inter-modality transformations as a full-modality code prediction task. To this
end, CodeBrain is trained in two stages: Reconstruction and Code Prediction.
First, in the Reconstruction stage, we reconstruct each MRI modality, which is
mapped into a shared latent space followed by a scalar quantization. Since such
quantization is lossy and the code is low dimensional, another MRI modality
belonging to the same subject is randomly selected to generate common features
to supplement the code and boost the target reconstruction. In the second
stage, we train another encoder by a customized grading loss to predict the
full-modality codes from randomly masked MRI samples, supervised by the
corresponding quantized codes generated from the first stage. In this way, the
inter-modality transformation is achieved by mapping the instance-specific
codes in a finite scalar space. We evaluated the proposed CodeBrain model on
two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments
demonstrate that our CodeBrain model achieves superior imputation performance
compared to four existing methods, establishing a new state of the art for
unified brain MRI imputation. Codes will be released.

æè¦ï¼MRI è£å®æ¨å¨å¾ä¸åæå¤åå¯ç¨æ¹å¼ä¸­åæéºå¤±çæ¨¡æï¼éæ¯éå¸¸çæ³çï¼å çºå®éä½äºææææ¬ï¼ä¸¦æä¾äºå¨é¢ç MRI è³è¨ä»¥å¢å¼·è¨åºè¨ºæ·ãå¨æ¬æä¸­ï¼æåæåºäºä¸åçµ±ä¸æ¨¡å CodeBrainï¼æ¨å¨é©æåç¨®è¦é¨ MRI è£å®å ´æ¯ãæ ¸å¿è¨­è¨å¨æ¼å°åç¨®æ¨¡æéè½æè½æçºå¨æ¨¡æç¢¼é æ¸¬ä»»åãçºæ­¤ï¼CodeBrain åå©åéæ®µé²è¡è¨ç·´ï¼éå»ºåç¢¼é æ¸¬ãé¦åï¼å¨éå»ºéæ®µï¼æåéå»ºæ¯å MRI æ¨¡æï¼å®è¢«æ å°å°ä¸åå±äº«æ½å¨ç©ºéï¼ç¶å¾é²è¡æ¨ééåãç±æ¼éç¨®éåæ¯ææçï¼ä¸¦ä¸ç¢¼çç¶­åº¦å¾ä½ï¼å æ­¤é¨æ©é¸æå±¬æ¼åä¸ååè©¦èçå¦ä¸å MRI æ¨¡æä¾ç¢çå±åç¹å¾µä»¥è£åç¢¼ä¸¦æåç®æ¨éå»ºãå¨ç¬¬äºéæ®µï¼æåééèªè¨åç´æå¤±è¨ç·´å¦ä¸åç·¨ç¢¼å¨ï¼å¾é¨æ©é®ç½©ç MRI æ¨£æ¬é æ¸¬å¨æ¨¡æç¢¼ï¼ä¸¦ç±ç¬¬ä¸éæ®µç¢ççå°æéåç¢¼é²è¡ç£ç£ãéæ¨£ï¼æ¨¡æéè½ææ¯ééå°ç¹å®æ¼ä¾é çç¢¼æ å°å°ä¸åæéçæ¨éç©ºéä¾å¯¦ç¾çãæåå¨å©åå¬éçè¦é¨ MRI è³æéï¼å³ IXI å BraTS 2023ï¼ä¸è©ä¼°äºææåºç CodeBrain æ¨¡åãå¤§éçå¯¦é©è­æï¼èåç¨®ç¾ææ¹æ³ç¸æ¯ï¼æåç CodeBrain æ¨¡åå¯¦ç¾äºåªç°çè£å®æè½ï¼çºçµ±ä¸çè¦é¨ MRI è£å®å»ºç«äºæ°çæè¡æ°´æºãç¢¼å°æéåºã

##### **A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**
2501.18294v1 by Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki

Lung cancer is a major issue in worldwide public health, requiring early
diagnosis using stable techniques. This work begins a thorough investigation of
the use of machine learning (ML) methods for precise classification of lung
cancer stages. A cautious analysis is performed to overcome overfitting issues
in model performance, taking into account minimum child weight and learning
rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM,
Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF),
CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted.
Furthermore, the correlation between features and targets is examined using the
deep neural network (DNN) model and thus their capability in detecting complex
patternsis established. It is argued that several ML models can be capable of
classifying lung cancer stages with great accuracy. In spite of the complexity
of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic
Regression excel with superior performance. The models perform better than the
others in lung cancer prediction on the complete set of comparative metrics
like accuracy, precision, recall, and F-1 score

æè¦ï¼èºçæ¯å¨çå¬å±è¡ççä¸å¤§åé¡ï¼éè¦ä½¿ç¨ç©©å®çæè¡é²è¡æ©æè¨ºæ·ãéé å·¥ä½éå§å¾¹åºèª¿æ¥ä½¿ç¨æ©å¨å­¸ç¿ (ML) æ¹æ³ç²¾ç¢ºåé¡èºçåæçä½¿ç¨ææ³ãå·è¡è¬¹æçåæä»¥åææ¨¡åæè½ä¸­çéåº¦æ¬ååé¡ï¼ä¸¦èæ®æå°å­æ¬éåå­¸ç¿çãä¸çµæ©å¨å­¸ç¿ (ML) æ¨¡åï¼åæ¬ XGBoost (XGB)ãLGBMãAdaboostãéè¼¯è¿´æ­¸ (LR)ãæ±ºç­æ¨¹ (DT)ãé¨æ©æ£®æ (RF)ãCatBoost å k æè¿é° (k-NN)ï¼ä»¥ææ¢ççæ¹å¼å·è¡ä¸¦é²è¡å°æ¯ãæ­¤å¤ï¼ä½¿ç¨æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) æ¨¡åæª¢æ¥ç¹å¾µåç®æ¨ä¹éçéè¯æ§ï¼å¾èå»ºç«å®åå¨æª¢æ¸¬è¤éæ¨¡å¼ä¸­çè½åãæäººèªçºï¼å¤å ML æ¨¡åè½å¤ ä»¥å¾é«çæºç¢ºåº¦å°èºçåæé²è¡åé¡ãåç®¡ DNN æ¶æ§å¾è¤éï¼ä½å³çµ± ML æ¨¡åï¼å¦ XGBoostãLGBM åéè¼¯è¿´æ­¸ï¼è¡¨ç¾åºè²ï¼æè½åªç°ãéäºæ¨¡åå¨èºçé æ¸¬ä¸­è¡¨ç¾åªæ¼å¶ä»æ¨¡åï¼å¨æºç¢ºåº¦ãç²¾ç¢ºåº¦ãå¬åçå F-1 åæ¸ç­å®æ´çæ¯è¼ææ¨ä¸­è¡¨ç¾åºè²ã

##### **The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**
2501.18270v1 by Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, SÃ©raphin Gaborit, Brian D'Alessandro, James Hudson, Gyula SzabÃ³, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia

Artificial intelligence has significantly advanced skin cancer diagnosis by
enabling rapid and accurate detection of malignant lesions. In this domain,
most publicly available image datasets consist of single, isolated skin lesions
positioned at the center of the image. While these lesion-centric datasets have
been fundamental for developing diagnostic algorithms, they lack the context of
the surrounding skin, which is critical for improving lesion detection. The
iToBoS dataset was created to address this challenge. It includes 16,954 images
of skin regions from 100 participants, captured using 3D total body
photography. Each image roughly corresponds to a $7 \times 9$ cm section of
skin with all suspicious lesions annotated using bounding boxes. Additionally,
the dataset provides metadata such as anatomical location, age group, and sun
damage score for each image. This dataset aims to facilitate training and
benchmarking of algorithms, with the goal of enabling early detection of skin
cancer and deployment of this technology in non-clinical environments.

æè¦ï¼äººå·¥æºæ§ééå¿«éä¸æºç¢ºåµæ¸¬æ¡æ§çç¶ï¼å¤§å¹æåç®èççè¨ºæ·ãå¨éåé åä¸­ï¼å¤§å¤æ¸å¬éçå½±åè³æéé½åå«å®ä¸ãå­¤ç«çç®èçç¶ï¼ç½®æ¼å½±åçä¸­å¤®ãåç®¡éäºä»¥çç¶çºä¸­å¿çè³æéå°æ¼éç¼è¨ºæ·æ¼ç®æ³è³ééè¦ï¼ä½å®åå»ç¼ºä¹å¨åç®èçèæ¯ï¼éå°æ¼æ¹åçç¶åµæ¸¬è³ééè¦ãiToBoS è³æéçå»ºç«å°±æ¯çºäºæå°éåææ°ãå®åå« 100 ä½åèèç 16,954 å¼µç®èååå½±åï¼ä½¿ç¨ 3D å¨èº«æå½±æè¡æ·åãæ¯å¼µå½±åå¤§è´å°ææ¼ $7 \times 9$ å¬åçç®èååï¼ææå¯ççç¶é½ä½¿ç¨éçæ¡æ¨è¨»ãæ­¤å¤ï¼è©²è³æééæä¾æ¯å¼µå½±åçåè³æï¼ä¾å¦è§£åä½ç½®ãå¹´é½¡çµåæ¥æ¬æå·è©åãæ­¤è³æéæ¨å¨ä¿é²æ¼ç®æ³çè¨ç·´ååºæºæ¸¬è©¦ï¼ç®æ¨æ¯å¯¦ç¾ç®èççæ©æåµæ¸¬ï¼ä¸¦å°æ­¤æè¡é¨ç½²å¨éè¨åºç°å¢ä¸­ã

##### **Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**
2501.18237v1 by Malte TÃ¶lle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt

A patient undergoes multiple examinations in each hospital stay, where each
provides different facets of the health status. These assessments include
temporal data with varying sampling rates, discrete single-point measurements,
therapeutic interventions such as medication administration, and images. While
physicians are able to process and integrate diverse modalities intuitively,
neural networks need specific modeling for each modality complicating the
training procedure. We demonstrate that this complexity can be significantly
reduced by visualizing all information as images along with unstructured text
and subsequently training a conventional vision-text transformer. Our approach,
Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not
only simplifies data preprocessing and modeling but also outperforms current
state-of-the-art methods in predicting in-hospital mortality and phenotyping,
as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities
include patient's clinical measurements, medications, X-ray images, and
electrocardiography scans. We hope our work inspires advancements in
multi-modal medical AI by reducing the training complexity to (visual) prompt
engineering, thus lowering entry barriers and enabling no-code solutions for
training. The source code will be made publicly available.

æè¦ï¼å¨æ¯æ¬¡ä½é¢æéï¼æ£èææ¥åå¤é æª¢æ¥ï¼æ¯ä¸é æª¢æ¥é½è½æä¾å¥åº·çæçä¸åé¢åãéäºè©ä¼°åæ¬å·æä¸ååæ¨£ççæéè³æãé¢æ£å®é»æ¸¬éå¼ãæ²»çä»å¥ï¼å¦è¥ç©ç®¡çï¼åå½±åãéç¶é«çè½å¤ ç´è§å°èçåæ´åä¸åçæ¨¡å¼ï¼ä½ç¥ç¶ç¶²è·¯éè¦éå°æ¯ç¨®æ¨¡å¼é²è¡ç¹å®çå»ºæ¨¡ï¼éä½¿å¾è¨ç·´ç¨åºè®å¾è¤éãæåè­æï¼ééå°ææè³è¨è¦è¦ºåçºå½±åï¼ä¸¦çµåéçµæ§åæå­ï¼é¨å¾è¨ç·´ä¸åå³çµ±çè¦è¦ºæå­è½æå¨ï¼å¯ä»¥å¤§å¹éä½éç¨®è¤éæ§ãæåçåæ³ï¼å³ç¨æ¼ä¸è¦åæ¡æ¨£å¤æ¨¡å¼æ¸¬éçè¦è¦ºè½æå¨ (ViTiMM)ï¼ä¸åç°¡åäºè³æé èçåå»ºæ¨¡ï¼èä¸å¨é æ¸¬é¢å§æ­»äº¡çåè¡¨åæ¹é¢ä¹åªæ¼ç®åçææ°æ¹æ³ï¼éæ¯æ ¹æ MIMIC-IV è³æéä¸­ç 6,175 åæ£èè©ä¼°çãéäºæ¨¡å¼åæ¬æ£èçè¨åºæ¸¬éå¼ãè¥ç©ãX åå½±ååå¿é»åææãæåå¸ææåçå·¥ä½è½éééä½è¨ç·´è¤éåº¦å°ï¼è¦è¦ºï¼æç¤ºå·¥ç¨ï¼å¾èéä½é²å¥éæª»ï¼ä¸¦çºè¨ç·´åç¨ç¡ç¨å¼ç¢¼è§£æ±ºæ¹æ¡ï¼é²èæ¿åµå¤æ¨¡å¼é«ç AI çé²æ­¥ãåå§ç¨å¼ç¢¼å°å¬éæä¾ã

##### **Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**
2501.18108v1 by Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino

Despite the growing potential of older adult care technologies, the adoption
of these technologies remains challenging. In this work, we conducted a
focus-group session with family caregivers to scope designs of the older adult
care technology. We then developed a high-fidelity prototype and conducted its
qualitative study with professional caregivers and older adults to understand
their perspectives on the system functionalities. This system monitors abnormal
activity patterns of older adults using wireless motion sensors and machine
learning models and supports interactive dialogue responses to explain abnormal
activity patterns of older adults to caregivers and allow older adults
proactively sharing their status with caregivers for an adequate intervention.
Both older adults and professional caregivers appreciated that our system can
provide a faster, personalized service while proactively controlling what
information is to be shared through interactive dialogue responses. We further
discuss other considerations to realize older adult technology in practice.

æè¦ï¼åç®¡èå¹´äººç§è­·æè¡çæ½åæ¥çå¢é·ï¼ä½æ¡ç¨éäºæè¡ä»å·æææ°æ§ãå¨éé ç ç©¶ä¸­ï¼æåèå®¶åº­ç§è­·èé²è¡ç¦é»å°çµæè­°ï¼ä»¥çå®èå¹´äººç§è­·æè¡çè¨­è¨ç¯åãæ¥èï¼æåéç¼äºä¸åé«ä¿çååï¼ä¸¦èå°æ¥­ç§è­·èåèå¹´äººé²è¡è³ªæ§ç ç©¶ï¼ä»¥äºè§£ä»åå°ç³»çµ±åè½çè§é»ãæ­¤ç³»çµ±ä½¿ç¨ç¡ç·åä½ææ¸¬å¨åæ©å¨å­¸ç¿æ¨¡åç£æ§èå¹´äººçç°å¸¸æ´»åæ¨¡å¼ï¼ä¸¦æ¯æ´äºåå¼å°è©±åæï¼åç§è­·èè§£éèå¹´äººçç°å¸¸æ´»åæ¨¡å¼ï¼ä¸¦è®èå¹´äººä¸»åèç§è­·èåäº«ä»åççæï¼ä»¥é²è¡é©ç¶çä»å¥ãèå¹´äººåå°æ¥­ç§è­·èé½è®è³æåçç³»çµ±è½æä¾æ´å¿«éãåäººåçæåï¼åæééäºåå¼å°è©±åæä¸»åæ§å¶è¦åäº«åªäºè³è¨ãæåé²ä¸æ­¥è¨è«å¶ä»èéå ç´ ï¼ä»¥å¨å¯¦åä¸­å¯¦ç¾èå¹´äººæè¡ã

##### **Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**
2501.18081v1 by Pratik S. Sachdeva, Tom van Nuenen

The rapid adoption of large language models (LLMs) has spurred extensive
research into their encoded moral norms and decision-making processes. Much of
this research relies on prompting LLMs with survey-style questions to assess
how well models are aligned with certain demographic groups, moral beliefs, or
political ideologies. While informative, the adherence of these approaches to
relatively superficial constructs tends to oversimplify the complexity and
nuance underlying everyday moral dilemmas. We argue that auditing LLMs along
more detailed axes of human interaction is of paramount importance to better
assess the degree to which they may impact human beliefs and actions. To this
end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am
I the Asshole" (AITA) community on Reddit, where users seek moral judgments on
everyday conflicts from other community members. We prompted seven LLMs to
assign blame and provide explanations for over 10,000 AITA moral dilemmas. We
then compared the LLMs' judgments and explanations to those of Redditors and to
each other, aiming to uncover patterns in their moral reasoning. Our results
demonstrate that large language models exhibit distinct patterns of moral
judgment, varying substantially from human evaluations on the AITA subreddit.
LLMs demonstrate moderate to high self-consistency but low inter-model
agreement. Further analysis of model explanations reveals distinct patterns in
how models invoke various moral principles. These findings highlight the
complexity of implementing consistent moral reasoning in artificial systems and
the need for careful evaluation of how different models approach ethical
judgment. As LLMs continue to be used in roles requiring ethical
decision-making such as therapists and companions, careful evaluation is
crucial to mitigate potential biases and limitations.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå¿«éæ¡ç¨å·²ä¿ä½¿äººåæ·±å¥ç ç©¶å¶ç·¨ç¢¼çéå¾·è¦ç¯åæ±ºç­éç¨ãè¨±å¤éé¡ç ç©¶ä¾è³´æ¼ä»¥èª¿æ¥å¼åé¡æç¤º LLMï¼ä»¥è©ä¼°æ¨¡åèç¹å®äººå£ç¾¤é«ãéå¾·ä¿¡å¿µææ¿æ²»æè­å½¢æçå¥åç¨åº¦ãåç®¡ææä¾è³è¨ï¼ä½éäºæ¹æ³å°ç¸å°èæ·ºççµæ§çå æå¾åæ¼éåº¦ç°¡åæ¥å¸¸éå¾·å°å¢èå¾çè¤éæ§åç´°å¾®å·®å¥ãæåèªçºï¼æ²¿èæ´è©³ç´°çäººé¡äºåè»¸ç·å¯©æ¥ LLM å°æ¼æ´å¥½å°è©ä¼°å®åå¯è½å½±é¿äººé¡ä¿¡å¿µåè¡çºçç¨åº¦è³ééè¦ãçºæ­¤ï¼æåæ ¹æ Reddit ä¸ãææ¯æ··èåã(AITA) ç¤¾ç¾¤è©ä¼° LLM å¨è¤éçæ¥å¸¸éå¾·å°å¢ä¸­ï¼ä½¿ç¨èå¨å¶ä¸­å°æ±å¶ä»ç¤¾ç¾¤æå¡å°æ¥å¸¸è¡çªçéå¾·å¤æ·ãæåæç¤ºä¸å LLM å°è¶é 10,000 å AITA éå¾·å°å¢åéè²¬ä»»ä¸¦æä¾è§£éãç¶å¾ï¼æåå° LLM çå¤æ·åè§£éè Reddit ä½¿ç¨èçå¤æ·åè§£éä»¥åå½¼æ­¤é²è¡æ¯è¼ï¼æ¨å¨æ­ç¤ºå¶éå¾·æ¨çä¸­çæ¨¡å¼ãæåççµæè¡¨æï¼å¤§åèªè¨æ¨¡åå±ç¾åºä¸åçéå¾·å¤æ·æ¨¡å¼ï¼è AITA å­çå¡ä¸çäººé¡è©ä¼°æå¾å¤§å·®ç°ãLLM è¡¨ç¾åºä¸­åº¦å°é«åº¦çèªæä¸è´æ§ï¼ä½æ¨¡åéåè­°ä½ãé²ä¸æ­¥åææ¨¡åè§£éæ­ç¤ºäºæ¨¡åå¦ä½æ´å¼åç¨®éå¾·ååçä¸åæ¨¡å¼ãéäºç¼ç¾çªé¡¯äºå¨äººå·¥ç³»çµ±ä¸­å¯¦æ½ä¸è´çéå¾·æ¨ççè¤éæ§ï¼ä»¥åä»ç´°è©ä¼°ä¸åæ¨¡åå¦ä½é²è¡éå¾·å¤æ·çå¿è¦æ§ãé¨è LLM æçºç¨æ¼éè¦éå¾·æ±ºç­çè§è²ï¼ä¾å¦æ²»çå¸«åä¼´ä¾¶ï¼ä»ç´°è©ä¼°å°æ¼æ¸è¼æ½å¨åè¦åéå¶è³ééè¦ã

##### **Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**
2501.18071v1 by Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino

Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.

æè¦ï¼ç³å°¿ç (DM) æ¯ä¸é éè¦çå¨çå¥åº·è­°é¡ï¼å¿é ç¡æ©è¨ºæ·ä¸¦å¦¥åç®¡çãæ¬ç ç©¶æåºä¸åç³å°¿çé æ¸¬æ¶æ§ï¼ä½¿ç¨æ©å¨å­¸ç¿ (ML) æ¨¡åï¼ä¸¦æ­éå¯è§£éäººå·¥æºæ§ (XAI) å·¥å·ï¼ä¾æ¢è¨ ML æ¨¡åé æ¸¬çæºç¢ºåº¦åå¯è§£éæ§ãè³æåèçåºæ¼åæå°æ¸éæ¡æ¨£æè¡ (SMOTE) åç¹å¾µç¸®æ¾ï¼ç¨æ¼ç³å°¿çäºåå¥åº·ææ¨è³æéï¼ä»¥èçé¡å¥ä¸å¹³è¡¡åè¨åºç¹å¾µçå¯è®æ§ãæ´åæ¨¡åæä¾äºé«æºç¢ºåº¦ï¼æ¸¬è©¦æºç¢ºåº¦çº 92.50%ï¼ROC-AUC çº 0.975ãæ ¹ææ¨¡åè§£éï¼BMIãå¹´é½¡ãä¸è¬å¥åº·çæ³ãæ¶å¥åèº«é«æ´»åæ¯æå·å½±é¿åçé æ¸¬å å­ãæ¬ç ç©¶çµæè¡¨æï¼ML çµå XAI æ¯ä¸ç¨®æåéçæ¹å¼ï¼å¯ä»¥éç¼åºæºç¢ºä¸å¨éç®ä¸éæçå·¥å·ï¼ç¨æ¼é«çä¿å¥ç³»çµ±ã

##### **Current Pathology Foundation Models are unrobust to Medical Center Differences**
2501.18055v2 by Edwin D. de Jong, Eric Marcus, Jonas Teuwen

Pathology Foundation Models (FMs) hold great promise for healthcare. Before
they can be used in clinical practice, it is essential to ensure they are
robust to variations between medical centers. We measure whether pathology FMs
focus on biological features like tissue and cancer type, or on the well known
confounding medical center signatures introduced by staining procedure and
other differences. We introduce the Robustness Index. This novel robustness
metric reflects to what degree biological features dominate confounding
features. Ten current publicly available pathology FMs are evaluated. We find
that all current pathology foundation models evaluated represent the medical
center to a strong degree. Significant differences in the robustness index are
observed. Only one model so far has a robustness index greater than one,
meaning biological features dominate confounding features, but only slightly. A
quantitative approach to measure the influence of medical center differences on
FM-based prediction performance is described. We analyze the impact of
unrobustness on classification performance of downstream models, and find that
cancer-type classification errors are not random, but specifically attributable
to same-center confounders: images of other classes from the same medical
center. We visualize FM embedding spaces, and find these are more strongly
organized by medical centers than by biological factors. As a consequence, the
medical center of origin is predicted more accurately than the tissue source
and cancer type. The robustness index introduced here is provided with the aim
of advancing progress towards clinical adoption of robust and reliable
pathology FMs.

æè¦ï¼ççåºç¤æ¨¡å (FM) å°é«çä¿å¥èè¨æ¥µå·æ½åãå¨è¨åºå¯¦åä¸­ä½¿ç¨ä¹åï¼å¿é ç¢ºä¿å®åè½æå°é«çä¸­å¿ä¹éçå·®ç°ãæåè¡¡éçç FM æ¯å¦èéæ¼çµç¹åççé¡åç­çç©ç¹å¾µï¼æèéæ¼æè²ç¨åºåå¶ä»å·®ç°æå°è´çç¾æå¨ç¥æ··æ·é«çä¸­å¿ç¹å¾µãæåå¼é²äºç©©å¥æ§ææ¨ãéé æ°ç©çç©©å¥æ§ææ¨åæ äºçç©ç¹å¾µä¸»å°æ··æ·ç¹å¾µçç¨åº¦ãè©ä¼°äºåé ç®åå¬éæä¾ççç FMãæåç¼ç¾ï¼ææç®åè©ä¼°çççåºç¤æ¨¡åå¨å¾å¤§ç¨åº¦ä¸ä»£è¡¨äºé«çä¸­å¿ãè§å¯å°ç©©å¥æ§ææ¨æé¡¯èå·®ç°ãå°ç®åçºæ­¢ï¼åªæä¸é æ¨¡åçç©©å¥æ§ææ¨å¤§æ¼ä¸ï¼è¡¨ç¤ºçç©ç¹å¾µä¸»å°æ··æ·ç¹å¾µï¼ä½åç¥å¾®ä¸»å°ãæè¿°äºè¡¡éé«çä¸­å¿å·®ç°å°åºæ¼ FM çé æ¸¬æè½å½±é¿çéåæ¹æ³ãæååæäºä¸ç©©å¥æ§å°ä¸æ¸¸æ¨¡ååé¡æè½çå½±é¿ï¼ç¼ç¾ççé¡ååé¡é¯èª¤ä¸¦éé¨æ©ï¼èæ¯ç¹å¥æ­¸å æ¼åä¸­å¿æ··æ·å å­ï¼ä¾èªåä¸é«çä¸­å¿çå¶ä»é¡å¥å½±åãæåè¦è¦ºå FM åµå¥ç©ºéï¼ç¼ç¾éäºç©ºéæ¯ç±é«çä¸­å¿èéçç©å ç´ æ´å¼·æåå°çµç¹ãå æ­¤ï¼æ¯çµç¹ä¾æºåççé¡åæ´æºç¢ºå°é æ¸¬äºé«çä¸­å¿çä¾æºãå¨æ­¤å¼å¥ç©©å¥æ§ææ¨ï¼ç®çæ¯æ¨é²æèè¨åºæ¡ç¨ç©©å¥ä¸å¯é ççç FM çé²å±ã

##### **Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**
2501.17860v1 by Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen

Current medical AI systems often fail to replicate real-world clinical
reasoning, as they are predominantly trained and evaluated on static text and
question-answer tasks. These tuning methods and benchmarks overlook critical
aspects like evidence-based reasoning and handling distracting information. To
bridge this gap, we introduce a novel benchmark that simulates real-world
diagnostic scenarios, integrating noise and difficulty levels aligned with
USMLE standards. Moreover, we explore dialogue-based fine-tuning, which
transforms static datasets into conversational formats to better capture
iterative reasoning processes. Experiments show that dialogue-tuned models
outperform traditional methods, with improvements of $9.64\%$ in multi-round
reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our
findings highlight dialogue tuning as a promising approach for advancing
clinically aligned and robust medical AI systems.

æè¦ï¼ç®åçé«ç AI ç³»çµ±å¸¸ç¡æ³è¤è£½çå¯¦ä¸ççè¨åºæ¨çï¼å çºå®åä¸»è¦å¨éææå­ååç­ä»»åä¸åè¨åè©ä¼°ãéäºèª¿æ´æ¹æ³ååºæºå¿½ç¥äºåºæ¼è­æçæ¨çåèçåæ£è³è¨ç­ééµé¢åãçºäºå½è£éåå·®è·ï¼æåæåºä¸åæ¨¡æ¬çå¯¦ä¸çè¨ºæ·æå¢çå¨æ°åºæºï¼æ´åè USMLE æ¨æºä¸è´çéè¨åé£åº¦ç­ç´ãæ­¤å¤ï¼æåæ¢ç´¢ä»¥å°è©±çºåºç¤çå¾®èª¿ï¼å°éæè³æéè½æçºå°è©±æ ¼å¼ï¼ä»¥æ´å¥½å°ææåè¦çæ¨çéç¨ãå¯¦é©é¡¯ç¤ºï¼å°è©±å¾®èª¿æ¨¡ååªæ¼å³çµ±æ¹æ³ï¼å¨å¤è¼ªæ¨çæå¢ä¸­æåäº 9.64%ï¼å¨æéè¨çç°å¢ä¸­æåäº 6.18% çæºç¢ºåº¦ãæåçç¼ç¾å¼·èª¿å°è©±å¾®èª¿æ¯ä¸ç¨®æææ¨é²èè¨åºç¸ç¬¦ä¸å¼·å¥çé«ç AI ç³»çµ±çæ¹æ³ã

##### **GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**
2501.17855v1 by Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee

Robot caregiving should be personalized to meet the diverse needs of care
recipients -- assisting with tasks as needed, while taking user agency in
action into account. In physical tasks such as handover, bathing, dressing, and
rehabilitation, a key aspect of this diversity is the functional range of
motion (fROM), which can vary significantly between individuals. In this work,
we learn to predict personalized fROM as a way to generalize robot
decision-making in a wide range of caregiving tasks. We propose a novel
data-driven method for predicting personalized fROM using functional assessment
scores from occupational therapy. We develop a neural model that learns to
embed functional assessment scores into a latent representation of the user's
physical function. The model is trained using motion capture data collected
from users with emulated mobility limitations. After training, the model
predicts personalized fROM for new users without motion capture. Through
simulated experiments and a real-robot user study, we show that the
personalized fROM predictions from our model enable the robot to provide
personalized and effective assistance while improving the user's agency in
action. See our website for more visualizations:
https://emprise.cs.cornell.edu/grace/.

æè¦ï¼æ©å¨äººç§è­·ææ ¹æç§è­·å°è±¡çä¸åéæ±é²è¡å®¢è£½åï¼å¨éè¦æåå©å·è¡ä»»åï¼åæèéä½¿ç¨èçèªä¸»è¡åãå¨ç§»äº¤ãæ²æµ´ãç©¿è¡£åå¾©å¥ç­èº«é«ä»»åä¸­ï¼éç¨®å¤æ¨£æ§çééµé¢åæ¯åè½æ§åä½ç¯å (fROM)ï¼èéå¨ä¸ååé«ä¹éå¯è½å·®ç°å¾å¤§ãå¨éé å·¥ä½ä¸­ï¼æåå­¸ç¿é æ¸¬å®¢è£½å fROMï¼ä½çºå¨å»£æ³ç§è­·ä»»åä¸­æ¦åæ©å¨äººæ±ºç­å¶å®çä¸ç¨®æ¹å¼ãæåæåºäºä¸ç¨®ä½¿ç¨è·è½æ²»çåè½è©ä¼°åæ¸ä¾é æ¸¬å®¢è£½å fROM çæ°ç©è³æé©åæ¹æ³ãæåéç¼äºä¸åç¥ç¶æ¨¡åï¼å­¸ç¿å°åè½è©ä¼°åæ¸åµå¥å°ä½¿ç¨èçèº«é«åè½æ½å¨è¡¨å¾µä¸­ãè©²æ¨¡åä½¿ç¨å¾å·ææ¨¡æ¬è¡åéå¶çä½¿ç¨èæ¶éçåä½æ·åè³æé²è¡è¨ç·´ãè¨ç·´å¾ï¼è©²æ¨¡åæçºæ²æåä½æ·åçæ°ä½¿ç¨èé æ¸¬å®¢è£½å fROMãééæ¨¡æ¬å¯¦é©åçå¯¦æ©å¨äººä½¿ç¨èç ç©¶ï¼æåå±ç¤ºäºæåæ¨¡åçå®¢è£½å fROM é æ¸¬ä½¿æ©å¨äººè½å¤ æä¾å®¢è£½åä¸ææçåå©ï¼åææé«ä½¿ç¨èçèªä¸»è¡åãè«åé±æåçç¶²ç«ä»¥åå¾æ´å¤è¦è¦ºåè³æï¼https://emprise.cs.cornell.edu/grace/ã

##### **Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks**
2502.00055v1 by Ljubisa Bojic, Zorica Dodevska, Yashar Deldjoo, Nenad Pantelic

Given the exponential advancement in AI technologies and the potential
escalation of harmful effects from recommendation systems, it is crucial to
simulate and evaluate these effects early on. Doing so can help prevent
possible damage to both societies and technology companies. This paper
introduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel
simulation framework leveraging Large Language Models (LLMs) to explore the
impacts of different content recommendation setups on user engagement and
polarization in social networks. By creating diverse AI agents (AgentPrompts)
with descriptive, static, and dynamic attributes, we assess their autonomous
behaviour across three scenarios: Plurality, Balanced, and Similarity. Our
findings reveal that the Similarity Scenario, which aligns content with user
preferences, maximizes engagement while potentially fostering echo chambers.
Conversely, the Plurality Scenario promotes diverse interactions but produces
mixed engagement results. Our study emphasizes the need for a careful balance
in recommender system designs to enhance user satisfaction while mitigating
societal polarization. It underscores the unique value and challenges of
incorporating LLMs into simulation environments. The benefits of RecSysLLMsP
lie in its potential to calculate polarization effects, which is crucial for
assessing societal impacts and determining user engagement levels with diverse
recommender system setups. This advantage is essential for developing and
maintaining a successful business model for social media companies. However,
the study's limitations revolve around accurately emulating reality. Future
efforts should validate the similarity in behaviour between real humans and
AgentPrompts and establish metrics for measuring polarization scores.

æè¦ï¼<paragraph>é¨è AI æè¡çææ¸ç´é²æ­¥ï¼ä»¥åæ¨è¦ç³»çµ±é æçæå®³å½±é¿çæ½å¨åç´ï¼ææ©æ¨¡æ¬åè©ä¼°éäºå½±é¿è³ééè¦ãééº¼åæå©æ¼é²æ­¢å°ç¤¾æåç§æå¬å¸é æçæ½å¨æå®³ãéç¯è«æä»ç´¹äºæ¨è¦ç³»çµ± LLM éæ¨å ´ (RecSysLLMsP)ï¼ä¸ååµæ°çæ¨¡æ¬æ¶æ§ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾æ¢ç´¢ä¸åå§å®¹æ¨è¦è¨­å®å°ç¤¾ç¾¤ç¶²è·¯ä¸­çä½¿ç¨èåèåº¦åæ¥µåç¾è±¡çå½±é¿ãèç±åµé å·ææè¿°æ§ãéæååæå±¬æ§çå¤æ¨£å AI ä»£çç¨å¼ (AgentPrompts)ï¼æåè©ä¼°å®åå¨ä¸ç¨®æå¢ä¸­çèªä¸»è¡çºï¼å¤åæ§ãå¹³è¡¡æ§åç¸ä¼¼æ§ãæåçç ç©¶çµæé¡¯ç¤ºï¼èä½¿ç¨èåå¥½ä¸è´çç¸ä¼¼æ§æå¢ï¼æå¤§åäºåèåº¦ï¼åææ½å¨ä¿é²äºåæº«å±¤ãç¸åå°ï¼å¤åæ§æå¢ä¿é²äºå¤æ¨£åçäºåï¼ä½ç¢çäºåå·®ä¸é½çåèåº¦çµæãæåçç ç©¶å¼·èª¿äºå¨æ¨è¦ç³»çµ±è¨­è¨ä¸­ä»ç´°åå¾å¹³è¡¡çå¿è¦æ§ï¼ä»¥æåä½¿ç¨èæ»¿æåº¦ï¼åææ¸è¼ç¤¾ææ¥µåç¾è±¡ãå®å¼·èª¿äºå° LLM ç´å¥æ¨¡æ¬ç°å¢çç¨ç¹å¹å¼åææ°ãRecSysLLMsP çåªå¢å¨æ¼å®è¨ç®æ¥µåææçæ½åï¼éå°æ¼è©ä¼°ç¤¾æå½±é¿åç¢ºå®ä½¿ç¨èåèåº¦èå¤æ¨£åæ¨è¦ç³»çµ±è¨­å®çå±¤ç´è³ééè¦ãéååªå¢å°æ¼ç¤¾ç¾¤åªé«å¬å¸éç¼åç¶­ææåçåæ¥­æ¨¡å¼è³ééè¦ãç¶èï¼éé ç ç©¶çéå¶å¨æ¼ç²¾ç¢ºå°æ¨¡æ¬ç¾å¯¦ãæªä¾çåªåæè©²é©è­çå¯¦äººé¡å AgentPrompts ä¹éè¡çºçç¸ä¼¼æ§ï¼ä¸¦å»ºç«è¡¡éæ¥µååæ¸çææ¨ã</paragraph>

##### **Tonguescape: Exploring Language Models Understanding of Vowel Articulation**
2501.17643v1 by Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe

Vowels are primarily characterized by tongue position. Humans have discovered
these features of vowel articulation through their own experience and explicit
objective observation such as using MRI. With this knowledge and our
experience, we can explain and understand the relationship between tongue
positions and vowels, and this knowledge is helpful for language learners to
learn pronunciation. Since language models (LMs) are trained on a large amount
of data that includes linguistic and medical fields, our preliminary studies
indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align
textual information with visual information. One question arises: do LMs
associate real tongue positions with vowel articulation? In this study, we
created video and image datasets from the existing real-time MRI dataset and
investigated whether LMs can understand vowel articulation based on tongue
positions using vision-based information. Our findings suggest that LMs exhibit
potential for understanding vowels and tongue positions when reference examples
are provided while they have difficulties without them. Our code for dataset
building is available on GitHub.

æè¦ï¼åé³ä¸»è¦ç±èé ­ä½ç½®æ±ºå®ãäººé¡ééèªå·±çç¶é©åæç¢ºçå®¢è§è§å¯ï¼ä¾å¦ä½¿ç¨ MRIï¼ç¼ç¾äºåé³ç¼é³çéäºç¹å¾µãæäºéäºç¥è­åç¶é©ï¼æåå¯ä»¥è§£éåçè§£èé ­ä½ç½®ååé³ä¹éçéä¿ï¼èéäºç¥è­å°èªè¨å­¸ç¿èå­¸ç¿ç¼é³å¾æå¹«å©ãç±æ¼èªè¨æ¨¡å (LM) æ¯å¨åå«èªè¨å­¸åé«å­¸é åçå¤§éè³æä¸è¨ç·´çï¼æåçåæ­¥ç ç©¶è¡¨æï¼LM è½å¤ è§£éåé³çç¼é³æ©å¶ãç¶èï¼å°ä¸æ¸æ¥å¤æ¨¡æ LMï¼ä¾å¦è¦è¦º LMï¼æ¯å¦å°æå­è³è¨èè¦è¦ºè³è¨å°é½ãä¸ååé¡ç¢çäºï¼LM æ¯å¦å°çå¯¦çèé ­ä½ç½®èåé³ç¼é³è¯ç¹«èµ·ä¾ï¼å¨éé ç ç©¶ä¸­ï¼æåå¾ç¾æçå³æ MRI è³æéä¸­å»ºç«äºå½±çåå½±åè³æéï¼ä¸¦æ¢è¨ LM æ¯å¦è½æ ¹æèé ­ä½ç½®ä½¿ç¨åºæ¼è¦è¦ºçè³è¨ä¾çè§£åé³ç¼é³ãæåçç ç©¶çµæè¡¨æï¼ç¶æä¾åèç¯ä¾æï¼LM å·æçè§£åé³åèé ­ä½ç½®çæ½åï¼èæ²æåèç¯ä¾æåæå°é£ãæåç¨æ¼å»ºç«è³æéçç¨å¼ç¢¼å¯å¨ GitHub ä¸åå¾ã

##### **Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**
2501.18645v2 by Manish Sanwal

Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to
provide step-by-step rationales, improving performance on complex tasks.
Despite its benefits, vanilla CoT often fails to fully verify intermediate
inferences and can produce misleading explanations. In this work, we propose
Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that
systematically segments the reasoning process into multiple layers, each
subjected to external checks and optional user feedback. We expand on the key
concepts, present three scenarios -- medical triage, financial risk assessment,
and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT
in terms of transparency, correctness, and user engagement. By integrating
references from recent arXiv papers on interactive explainability, multi-agent
frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves
the way for more reliable and grounded explanations in high-stakes domains.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼å©ç¨æèéï¼CoTï¼æç¤ºæä¾éæ­¥ççç±ï¼æåè¤éä»»åçè¡¨ç¾ãåç®¡æå¶å¥½èï¼ä½é¦è CoT å¸¸å¸¸ç¡æ³å®å¨é©è­ä¸­éæ¨è«ï¼ä¸å¯è½æç¢çèª¤å°æ§çè§£éãå¨éé ç ç©¶ä¸­ï¼æåæåºåå±¤æèéï¼åå±¤ CoTï¼æç¤ºï¼ä¸åæ°ç©çæ¶æ§ï¼å®æç³»çµ±å°å°æ¨çéç¨åéæå¤åå±¤ç´ï¼æ¯åå±¤ç´é½ç¶éå¤é¨æª¢æ¥åå¯é¸æçä½¿ç¨èåé¥ãæåæ´å±ééµæ¦å¿µï¼æåºä¸åå ´æ¯ââé«çåæµãè²¡åé¢¨éªè©ä¼°åææ·å·¥ç¨ââä¸¦å±ç¤ºåå±¤ CoT å¨éæåº¦ãæ­£ç¢ºæ§åä½¿ç¨èåèåº¦æ¹é¢å¦ä½è¶è¶é¦è CoTãééæ´åä¾èªè¿æ arXiv è«æä¸­éæ¼äºåå¯è§£éæ§ãå¤éä»£çæ¶æ§ååºæ¼ä»£ççåä½çåèæç»ï¼æåèªªæåå±¤ CoT å¦ä½çºé«é¢¨éªé åä¸­æ´å¯é ä¸ææ ¹æçè§£ééªè·¯ã

##### **An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**
2501.17555v1 by Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian

Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms
that account for less than 5% of all pancreatic malignancies, with an incidence
of only 1-1.5 cases per 100,000. Early detection of pNETs is critical for
improving patient survival, but the rarity of pNETs makes segmenting them from
CT a very challenging problem. So far, there has not been a dataset
specifically for pNETs available to researchers. To address this issue, we
propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography
(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,
containing data from 469 patients. This is the first dataset solely dedicated
to pNETs, distinguishing it from previous collections. Additionally, we provide
the baseline detection networks with a new slice-wise weight loss function
designed for the UNet-based model, improving the overall pNET segmentation
performance. We hope that our dataset can enhance the understanding and
diagnosis of pNET Tumors within the medical community, facilitate the
development of more accurate diagnostic tools, and ultimately improve patient
outcomes and advance the field of oncology.

æè¦ï¼è°èç¥ç¶å§åæ³è«ç¤ (pNETs) æ¯éå¸¸ç½è¦çå§åæ³è«ç¤ï¼åä½ææè°èæ¡æ§è«ç¤çä¸å° 5%ï¼æ¯ 100,000 äººä¸­åç¼ç 1-1.5 åçä¾ãæ©æç¼ç¾ pNETs å°æ¹åæ£èå­æ´»çè³ééè¦ï¼ä½ pNETs çç½è¦æ§ä½¿å¾å¾ CT ä¸­åå²å®åæçºä¸åéå¸¸å·æææ°æ§çåé¡ãå°ç®åçºæ­¢ï¼éæ²æå°ééå° pNETs çæ¸æéå¯ä¾ç ç©¶äººå¡ä½¿ç¨ãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸å pNETs æ¸æéï¼ä¸åå°æ³¨æ¼è°èç¥ç¶å§åæ³è«ç¤çæ¨è¨»è¯å¥½çå°æ¯å¢å¼·é»è¦æ·å±¤ææ (CECT) æ¸æéï¼åå«ä¾èª 469 åæ£èçæ¸æãéæ¯ç¬¬ä¸åå°ééå° pNETs çæ¸æéï¼éä½¿å¶æå¥æ¼ä¹åçæ¶éãæ­¤å¤ï¼æåçºåºç·æª¢æ¸¬ç¶²è·¯æä¾äºä¸åæ°çåºæ¼ UNet æ¨¡åè¨­è¨çåçå æ¬æå¤±å½æ¸ï¼æ¹åäºæ´é« pNET åå²æ§è½ãæåå¸ææåçæ¸æéè½å¤ å¢å¼·é«å­¸çå° pNET è«ç¤ççè§£åè¨ºæ·ï¼ä¿é²æ´æºç¢ºçè¨ºæ·å·¥å·çéç¼ï¼æçµæ¹åæ£èçé å¾ä¸¦æ¨é²è«ç¤å­¸é åã

##### **LLM Assistance for Pediatric Depression**
2501.17510v1 by Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive

Traditional depression screening methods, such as the PHQ-9, are particularly
challenging for children in pediatric primary care due to practical
limitations. AI has the potential to help, but the scarcity of annotated
datasets in mental health, combined with the computational costs of training,
highlights the need for efficient, zero-shot approaches. In this work, we
investigate the feasibility of state-of-the-art LLMs for depressive symptom
extraction in pediatric settings (ages 6-24). This approach aims to complement
traditional screening and minimize diagnostic errors.
  Our findings show that all LLMs are 60% more efficient than word match, with
Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the
extraction of more rare symptoms like "sleep problems" (F1: 0.92) and
"self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and
recall (0.60), performing well in categories like "Feeling depressed" (0.69)
and "Weight change" (0.78). Llama 3, with the highest recall (0.90),
overgeneralizes symptoms, making it less suitable for this type of analysis.
Challenges include the complexity of clinical notes and overgeneralization from
PHQ-9 scores. The main challenges faced by LLMs include navigating the complex
structure of clinical notes with content from different times in the patient
trajectory, as well as misinterpreting elevated PHQ-9 scores.
  We finally demonstrate the utility of symptom annotations provided by Flan as
features in an ML algorithm, which differentiates depression cases from
controls with high precision of 0.78, showing a major performance boost
compared to a baseline that does not use these features.

æè¦ï¼<paragraph>å³çµ±çæé¬±çç¯©æª¢æ¹æ³ï¼ä¾å¦ PHQ-9ï¼ç±æ¼å¯¦ééå¶ï¼å°æ¼å°åç§åç´ç§è­·ä¸­çåç«¥ä¾èªªç¹å¥å·æææ°æ§ãAI æå¯è½æä¾å¹«å©ï¼ä½å¿çå¥åº·ä¸­è¨»è§£è³æéçç¨å°ï¼å ä¸è¨ç·´çéç®ææ¬ï¼çªé¡¯äºå°ææççé¶æ¬¡å­¸ç¿æ¹æ³çéæ±ãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨äºæåé²ç LLM å¨å°åç§ç°å¢ï¼6-24 æ­²ï¼ä¸­æåæé¬±çççå¯è¡æ§ãéç¨®æ¹æ³æ¨å¨è£åå³çµ±ç¯©æª¢ä¸¦å°è¨ºæ·é¯èª¤éè³æä½ãæåçç ç©¶çµæé¡¯ç¤ºï¼ææ LLM çæçé½æ¯å­è©æ¯å°é«åº 60%ï¼è Flan å¨ç²¾ç¢ºåº¦æ¹é¢é åï¼å¹³å F1ï¼0.65ï¼ç²¾ç¢ºåº¦ï¼0.78ï¼ï¼å¨æåè¼ç½è¦çççæ¹é¢è¡¨ç¾åºè²ï¼ä¾å¦ãç¡ç åé¡ãï¼F1ï¼0.92ï¼åãèªæå­æ¡ãï¼F1ï¼0.8ï¼ãPhi å¨ç²¾ç¢ºåº¦ï¼0.44ï¼åå¬åçï¼0.60ï¼ä¹éåå¾å¹³è¡¡ï¼å¨ãæå°æ²®åªãï¼0.69ï¼åãé«éæ¹è®ãï¼0.78ï¼ç­é¡å¥ä¸­è¡¨ç¾è¯å¥½ãæææé«å¬åçï¼0.90ï¼ç Llama 3 æéåº¦æ¦æ¬ççï¼ä½¿å¶ä¸å¤ªé©åæ­¤é¡åæãææ°åæ¬è¨åºç­è¨çè¤éæ§å PHQ-9 åæ¸çéåº¦æ¦æ¬ãLLM é¢è¨çä¸»è¦ææ°åæ¬å¨æ£èæ­·ç¨ä¸­ä¸åæéçå§å®¹ä¸­å°èªè¨åºç­è¨çè¤éçµæ§ï¼ä»¥åèª¤è§£ PHQ-9 åæ¸åé«ãæåæå¾å±ç¤ºäº Flan æä¾çççè¨»è§£ä½çºæ©å¨å­¸ç¿æ¼ç®æ³ä¸­ç¹å¾µçæç¨ï¼å®ä»¥ 0.78 çé«ç²¾ç¢ºåº¦å°æé¬±ççä¾èå°ç§çµååéä¾ï¼èä¸ä½¿ç¨éäºç¹å¾µçåºæºç¸æ¯ï¼é¡¯ç¤ºåºä¸»è¦çæè½æåã</paragraph>

##### **Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application**
2502.00052v1 by Gonzalo IÃ±aki Quintana, Laurence Vancamberg, Vincent Jugnon, AgnÃ¨s Desolneux, Mathilde Mougeot

This work studies the relationship between Contrastive Learning and Domain
Adaptation from a theoretical perspective. The two standard contrastive losses,
NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to
the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely
used for Domain Adaptation. Our work shows that minimizing the contrastive
losses decreases the CMMD and simultaneously improves class-separability,
laying the theoretical groundwork for the use of Contrastive Learning in the
context of Domain Adaptation. Due to the relevance of Domain Adaptation in
medical imaging, we focused the experiments on mammography images. Extensive
experiments on three mammography datasets - synthetic patches, clinical (real)
patches, and clinical (real) images - show improved Domain Adaptation,
class-separability, and classification performance, when minimizing the
Supervised Contrastive loss.

æè¦ï¼æ¬ç ç©¶å¾çè«è§åº¦æ¢è¨å°æ¯å­¸ç¿èé åé©æä¹éçéä¿ãå©ç¨®æ¨æºå°æ¯æå¤±ï¼NT-Xent æå¤±ï¼èªæç£ç£ï¼åç£ç£å°æ¯æå¤±ï¼èå»£æ³ç¨æ¼é åé©æçå·®ç°æ§æ¸¬éæ¨æºé¡å¥å¹³åæå¤§å·®ç°ï¼CMMDï¼æéãæåçç ç©¶è¡¨æï¼æå°åå°æ¯æå¤±æéä½ CMMDï¼åææé«é¡å¥å¯åé¢æ§ï¼çºå¨é åé©æä¸­ä½¿ç¨å°æ¯å­¸ç¿å¥ å®çè«åºç¤ãç±æ¼é åé©æå¨é«å­¸å½±åä¸­çç¸éæ§ï¼æåå°å¯¦é©éé»æ¾å¨ä¹³æ¿æå½±ååä¸ãå¨ä¸åä¹³æ¿æå½±æ¸æéï¼åæè²¼çãè¨åºï¼çå¯¦ï¼è²¼çåè¨åºï¼çå¯¦ï¼ååï¼ä¸é²è¡çå»£æ³å¯¦é©è¡¨æï¼å¨æå°åç£ç£å°æ¯æå¤±æï¼é åé©æãé¡å¥å¯åé¢æ§ååé¡æ§è½å¾å°æ¹åã

##### **Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**
2501.17343v1 by Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo

Quantizing deep neural networks ,reducing the precision (bit-width) of their
computations, can remarkably decrease memory usage and accelerate processing,
making these models more suitable for large-scale medical imaging applications
with limited computational resources. However, many existing methods studied
"fake quantization", which simulates lower precision operations during
inference, but does not actually reduce model size or improve real-world
inference speed. Moreover, the potential of deploying real 3D low-bit
quantization on modern GPUs is still unexplored. In this study, we introduce a
real post-training quantization (PTQ) framework that successfully implements
true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation
models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,
ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use
TensorRT to perform fake quantization for both weights and activations with
unlabeled calibration dataset. Second, we convert this fake quantization into
real quantization via TensorRT engine on real GPUs, resulting in real-world
reductions in model size and inference latency. Extensive experiments
demonstrate that our framework effectively performs 8-bit quantization on GPUs
without sacrificing model performance. This advancement enables the deployment
of efficient deep learning models in medical imaging applications where
computational resources are constrained. The code and models have been
released, including U-Net, TransUNet pretrained on the BTCV dataset for
abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset
for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and
VISTA3D pretrained on TotalSegmentator V2 for full body (104-label)
segmentation. https://github.com/hrlblab/PTQ.

æè¦ï¼<paragraph>éåæ·±åº¦ç¥ç»ç½ç»ï¼éä½å¶è®¡ç®çç²¾åº¦ï¼ä½å®½ï¼ï¼å¯ä»¥æ¾èåå°åå­ä½¿ç¨éå¹¶å éå¤çï¼ä½¿è¿äºæ¨¡åæ´éåäºå·ææéè®¡ç®èµæºçå¤§è§æ¨¡å»å­¦å½±ååºç¨ãç¶èï¼è®¸å¤ç°ææ¹æ³ç ç©¶äºâä¼ªéåâï¼å®å¨æ¨çæé´æ¨¡æè¾ä½ç²¾åº¦çæä½ï¼ä½å®éä¸å¹¶æ²¡æåå°æ¨¡åå¤§å°ææé«å®éæ¨çéåº¦ãæ­¤å¤ï¼å¨ç°ä»£ GPU ä¸é¨ç½²çæ­£ç 3D ä½ä½éåçæ½åä»æªå¾å°æ¢ç´¢ãå¨è¿é¡¹ç ç©¶ä¸­ï¼æä»¬å¼å¥äºä¸ä¸ªçæ­£çè®­ç»åéå (PTQ) æ¡æ¶ï¼è¯¥æ¡æ¶æåå°å¨æåè¿ç (SOTA) 3D å»å­¦åå²æ¨¡åï¼å³ U-NetãSegResNetãSwinUNETRãnnU-NetãUNesTãTransUNetãST-UNet å VISTA3Dï¼ä¸å®ç°äºçæ­£ç 8 ä½éåãæä»¬çæ¹æ³æ¶åä¸¤ä¸ªä¸»è¦æ­¥éª¤ãé¦åï¼æä»¬ä½¿ç¨ TensorRT å¯¹æéåæ¿æ´»è¿è¡ä¼ªéåï¼å¹¶ä½¿ç¨æªæ è®°çæ ¡åæ°æ®éãå¶æ¬¡ï¼æä»¬å°è¿ç§ä¼ªéåéè¿çå® GPU ä¸ç TensorRT å¼æè½¬æ¢ä¸ºçæ­£çéåï¼ä»èå¨æ¨¡åå¤§å°åæ¨çå»¶è¿æ¹é¢å®ç°äºå®éçåå°ãå¤§éçå®éªè¡¨æï¼æä»¬çæ¡æ¶å¨ GPU ä¸ææå°æ§è¡ 8 ä½éåï¼èä¸ä¼çºç²æ¨¡åæ§è½ãè¿ä¸è¿æ­¥ä½¿å¾å¨è®¡ç®èµæºåéçå»å­¦å½±ååºç¨ä¸­é¨ç½²é«æçæ·±åº¦å­¦ä¹ æ¨¡åæä¸ºå¯è½ãä»£ç åæ¨¡åå·²ç»åå¸ï¼åæ¬ U-NetãTransUNETï¼å¨ BTCV æ°æ®éä¸é¢è®­ç»ç¨äºè¹é¨ï¼13 æ ç­¾ï¼åå²ï¼UNesT å¨ Whole Brain æ°æ®éä¸é¢è®­ç»ç¨äºå¨èï¼133 æ ç­¾ï¼åå²ï¼ä»¥å nnU-NetãSegResNetãSwinUNETR å VISTA3D å¨ TotalSegmentator V2 ä¸é¢è®­ç»ç¨äºå¨èº«ï¼104 æ ç­¾ï¼åå²ãhttps://github.com/hrlblab/PTQã</paragraph>

##### **Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**
2501.17338v1 by Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang

Generative Language Models rely on autoregressive decoding to produce the
output sequence token by token. Many tasks such as preference optimization,
require the model to produce task-level output consisting of multiple tokens
directly by selecting candidates from a pool as predictions. Determining a
task-level prediction from candidates using the ordinary token-level decoding
mechanism is constrained by time-consuming decoding and interrupted gradients
by discrete token selection. Existing works have been using decoding-free
candidate selection methods to obtain candidate probability from initial output
logits over vocabulary. Though these estimation methods are widely used, they
are not systematically evaluated, especially on end tasks. We introduce an
evaluation of a comprehensive collection of decoding-free candidate selection
approaches on a comprehensive set of tasks, including five multiple-choice QA
tasks with a small candidate pool and four clinical decision tasks with a
massive amount of candidates, some with 10k+ options. We evaluate the
estimation methods paired with a wide spectrum of foundation LMs covering
different architectures, sizes and training paradigms. The results and insights
from our analysis inform the future model design.

æè¦ï¼çæèªè¨æ¨¡åä¾é èªè¿´æ­¸è§£ç¢¼ä¾éåç¬¦èç¢çè¼¸åºåºåãè¨±å¤ä»»åï¼å¦åå¥½æä½³åï¼è¦æ±æ¨¡åç´æ¥å¾åé¸æ± ä¸­é¸æé æ¸¬ï¼ç¢çç±å¤åç¬¦èçµæçä»»åç´å¥è¼¸åºãä½¿ç¨ä¸è¬çç¬¦èç´å¥è§£ç¢¼æ©å¶å¾åé¸èä¸­ç¢ºå®ä»»åç´å¥é æ¸¬åå°èæçè§£ç¢¼åé¢æ£ç¬¦èé¸æä¸­æ·çæ¢¯åº¦çç´æãç¾æå·¥ä½ä¸ç´ä½¿ç¨ç¡è§£ç¢¼åé¸èé¸ææ¹æ³å¾åå§è¼¸åºéè¼¯å¼ä¸­ç²å¾åé¸èæ©çãåç®¡éäºä¼°è¨æ¹æ³è¢«å»£æ³ä½¿ç¨ï¼ä½å®åä¸¦æªç¶éç³»çµ±è©ä¼°ï¼ç¹å¥æ¯å¨æçµä»»åä¸ãæåéå°å¨é¢çä»»åéï¼åæ¬äºåå·æå°ååé¸èæ± çå¤é¸é¡åç­ä»»ååååå·æå¤§éåé¸èçè¨åºæ±ºç­ä»»åï¼å¶ä¸­ä¸äºæ 10k+ é¸é ï¼ï¼å°å¨é¢çç¡è§£ç¢¼åé¸èé¸ææ¹æ³é²è¡è©ä¼°ãæåè©ä¼°èå»£æ³åºç¤èªè¨æ¨¡åéå°çä¼°è¨æ¹æ³ï¼éäºæ¨¡åæ¶µèä¸åçæ¶æ§ãå¤§å°åè¨ç·´ç¯ä¾ãæååæççµæåè¦è§£çºæªä¾çæ¨¡åè¨­è¨æä¾äºè³è¨ã

##### **Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**
2501.17326v1 by Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang

Clinical diagnosis prediction models, when provided with a patient's medical
history, aim to detect potential diseases early, facilitating timely
intervention and improving prognostic outcomes. However, the inherent scarcity
of patient data and large disease candidate space often pose challenges in
developing satisfactory models for this intricate task. The exploration of
leveraging Large Language Models (LLMs) for encapsulating clinical decision
processes has been limited. We introduce MERA, a clinical diagnosis prediction
model that bridges pertaining natural language knowledge with medical practice.
We apply hierarchical contrastive learning on a disease candidate ranking list
to alleviate the large decision space issue. With concept memorization through
fine-tuning, we bridge the natural language clinical knowledge with medical
codes. Experimental results on MIMIC-III and IV datasets show that MERA
achieves the state-of-the-art diagnosis prediction performance and dramatically
elevates the diagnosis prediction capabilities of generative LMs.

æè¦ï¼è¨åºè¨ºæ·é æ¸¬æ¨¡åå¨æä¾æ£èçæ­·çåæï¼æ¨å¨åæ©ç¼ç¾æ½å¨ç¾çï¼ä¿é²åæå¹²é ä¸¦æ¹åé å¾çµæãç¶èï¼æ£èæ¸æçåºæç¨ç¼ºæ§åå¤§éçç¾çåé¸ç©ºééå¸¸å°éç¼ä»¤äººæ»¿æçæ¨¡åä»¥æå°éé è¤éçä»»åæ§æææ°ãå©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾å°è£è¨åºæ±ºç­æµç¨çæ¢ç´¢åå°éå¶ãæåå¼å¥äº MERAï¼éæ¯ä¸åè¨åºè¨ºæ·é æ¸¬æ¨¡åï¼å®å°ç¸éçèªç¶èªè¨ç¥è­èé«çå¯¦è¸è¯ç¹«èµ·ä¾ãæåå¨ç¾çåé¸æåæ¸å®ä¸æç¨åå±¤å°æ¯å­¸ç¿ï¼ä»¥ç·©è§£å¤§åæ±ºç­ç©ºéåé¡ãééå¾®èª¿æ¦å¿µè¨æ¶ï¼æåå°èªç¶èªè¨è¨åºç¥è­èé«çä»£ç¢¼è¯ç¹«èµ·ä¾ãå¨ MIMIC-III å IV æ¸æéä¸çå¯¦é©çµæè¡¨æï¼MERA éå°äºæåé²çè¨ºæ·é æ¸¬æ§è½ï¼ä¸¦é¡¯èæåäºçæå¼ LM çè¨ºæ·é æ¸¬è½åã

##### **Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**
2501.17286v1 by Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu

Background: The radiation oncology clinical practice involves many steps
relying on the dynamic interplay of abundant text data. Large language models
have displayed remarkable capabilities in processing complex text information.
But their direct applications in specific fields like radiation oncology remain
underexplored.
  Purpose: This study aims to investigate whether fine-tuning LLMs with domain
knowledge can improve the performance on Task (1) treatment regimen generation,
Task (2) treatment modality selection (photon, proton, electron, or
brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.
  Methods: Data for 15,724 patient cases were extracted. Cases where patients
had a single diagnostic record, and a clearly identifiable primary treatment
plan were selected for preprocessing and manual annotation to have 7,903 cases
of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.
Each case was used to construct a pair consisting of patient diagnostics
details and an answer (treatment regimen, treatment modality, or ICD-10 code
respectively) for the supervised fine-tuning of these three tasks. Open source
LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the
Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for
the fine-tuned models and original models. Clinical evaluation was performed on
Task (1) by radiation oncologists, while precision, recall, and F-1 score were
evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used
to statistically analyze the results.
  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with
p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the
fine-tuned LLMs-generated treatment regimens were clinically acceptable.
Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.

æè¦ï¼<paragraph>èæ¯ï¼æ¾å°è¿ç¤ä¸´åºå®è·µæ¶åè®¸å¤æ­¥éª¤ï¼è¿äºæ­¥éª¤ä¾èµäºä¸°å¯ææ¬æ°æ®çå¨æäº¤äºãå¤§åè¯­è¨æ¨¡åå¨å¤çå¤æçææ¬ä¿¡æ¯æ¹é¢è¡¨ç°åºäºåè¶çè½åãä½å®ä»¬å¨æ¾å°è¿ç¤ç­ç¹å®é¢åçç´æ¥åºç¨ä»æªå¾å°ååæ¢ç´¢ã
ç®çï¼æ¬ç ç©¶æ¨å¨è°æ¥éè¿é¢åç¥è¯å¾®è° LLM æ¯å¦å¯ä»¥æé«ä»»å¡ (1) æ²»çæ¹æ¡çæãä»»å¡ (2) æ²»çæ¹å¼éæ©ï¼åå­ãè´¨å­ãçµå­æè¿è·ç¦»æ¾å°æ²»çï¼åä»»å¡ (3) æ¾å°è¿ç¤ä¸­ ICD-10 ä»£ç é¢æµçæ§è½ã
æ¹æ³ï¼æåäº 15,724 ä¾æ£èçä¾çæ°æ®ãéæ©äºæ£èæåä¸è¯æ­è®°å½ä¸ææç¡®å¯è¯å«çä¸»è¦æ²»çè®¡åççä¾ï¼è¿è¡é¢å¤çåæå¨æ³¨éï¼å¾å° 7,903 ä¾æ£èè¯æ­ãæ²»çè®¡åãæ²»çæ¹å¼å ICD-10 ä»£ç ãæ¯ä¸ªçä¾é½ç¨äºæå»ºä¸å¯¹ï¼åæ¬æ£èè¯æ­è¯¦æåç­æ¡ï¼åå«æ¯æ²»çæ¹æ¡ãæ²»çæ¹å¼æ ICD-10 ä»£ç ï¼ï¼ç¨äºè¿ä¸ä¸ªä»»å¡ççç£å¾®è°ãå¼æº LLaMA2-7B å Mistral-7B æ¨¡åè¢«ç¨äºä½¿ç¨ä½ç§©é¼è¿æ¹æ³è¿è¡å¾®è°ãæ¥åäºå¾®è°æ¨¡åååå§æ¨¡åçåç¡®æ§å ROUGE-1 åæ°ãä»»å¡ (1) ç±æ¾å°è¿ç¤ç§å»å¸è¿è¡ä¸´åºè¯ä¼°ï¼èä»»å¡ (2) å (3) åè¯ä¼°äºç²¾ç¡®åº¦ãå¬åçå F-1 åæ°ãåä¾§ Wilcoxon ç¬¦å·ç§©æ£éªç¨äºå¯¹ç»æè¿è¡ç»è®¡åæã
ç»æï¼å¾®è°åç LLM å¨ææä»»å¡ä¸­é½ä¼äºåå§ LLMï¼p å¼ <= 0.001ãä¸´åºè¯ä¼°è¡¨æï¼è¶è¿ 60% çå¾®è° LLM çæçæ²»çæ¹æ¡å¨ä¸´åºä¸æ¯å¯æ¥åçãç²¾ç¡®åº¦ãå¬åçå F1 åæ°æ¾ç¤ºå¾®è°åç LLM æ§è½å¾å°æ¹åã</paragraph>

##### **ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**
2501.17260v1 by Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee

Optical Coherence Tomography (OCT) is a non-invasive imaging modality
essential for diagnosing various eye diseases. Despite its clinical
significance, developing OCT-based diagnostic tools faces challenges, such as
limited public datasets, sparse annotations, and privacy concerns. Although
deep learning has made progress in automating OCT analysis, these challenges
remain unresolved. To address these limitations, we introduce the Vision
Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a
novel framework designed to enhance feature extraction and improve diagnostic
accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,
Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining
phase leverages the OCTMNIST dataset (97,477 unlabeled images across four
disease classes) with data augmentation to create dual-augmented views. A
Vision Transformer (ViT-Base) backbone extracts features, while a negative
cosine similarity loss aligns feature representations. Pretraining is conducted
over 50 epochs with a learning rate of 0.0001 and momentum of 0.999.
Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using
10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of
0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming
existing SSP-based methods.

æè¦ï¼åå­¸ç¸å¹²æ·å±¤ææï¼OCTï¼æ¯ä¸ç¨®éä¾µå¥å¼å½±åæ¨¡å¼ï¼å°æ¼è¨ºæ·åç¨®ç¼ç¾è³ééè¦ãåç®¡å¶è¨åºæç¾©éå¤§ï¼ä½éç¼åºæ¼ OCT çè¨ºæ·å·¥å·é¢è¨ææ°ï¼ä¾å¦å¬å±æ¸æéæéãè¨»è§£ç¨çåé±ç§åé¡ãåç®¡æ·±åº¦å­¸ç¿å¨èªåå OCT åææ¹é¢åå¾äºé²å±ï¼ä½éäºææ°ä»ç¶æ²æè§£æ±ºãçºäºæå°éäºéå¶ï¼æåå¼å¥äºåºæ¼ Vision Transformer çéæµèªç£ç£é è¨ç·´ç¶²è·¯ï¼ViT-2SPNï¼ï¼éæ¯ä¸åæ°ç©çæ¡æ¶ï¼æ¨å¨å¢å¼·ç¹å¾µæåä¸¦æé«è¨ºæ·æºç¢ºæ§ãViT-2SPN æ¡ç¨ä¸éæ®µå·¥ä½æµç¨ï¼ç£ç£é è¨ç·´ãèªç£ç£é è¨ç·´ï¼SSPï¼åç£ç£å¾®èª¿ãé è¨ç·´éæ®µå©ç¨ OCTMNIST æ¸æéï¼è·¨è¶ååç¾çé¡å¥ç 97,477 å¼µæªæ¨è¨å½±åï¼åæ¸ææ´åä¾å»ºç«ééæ´åçæª¢è¦ãè¦è¦ºè½æå¨ï¼ViT-Baseï¼ä¸»å¹¹æåç¹å¾µï¼èè² é¤å¼¦ç¸ä¼¼åº¦æå¤±åæ ¡æºç¹å¾µè¡¨ç¤ºãé è¨ç·´å¨ 50 åä¸ä»£ä¸­é²è¡ï¼å­¸ç¿ççº 0.0001ï¼åè½çº 0.999ãå¾®èª¿å¨ OCTMNIST çåå±¤ 5.129% å­éä¸å·è¡ï¼ä½¿ç¨ 10 åäº¤åé©è­ãViT-2SPN éå°äº 0.93 çå¹³å AUCã0.77 çæºç¢ºçã0.81 çç²¾ç¢ºåº¦ã0.75 çå¬åçå 0.76 ç F1 åæ¸ï¼åªæ¼ç¾æçåºæ¼ SSP çæ¹æ³ã

##### **A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**
2501.17160v1 by Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham

Early detection of COVID-19 is crucial for effective treatment and
controlling its spread. This study proposes a novel hybrid deep learning model
for detecting COVID-19 from CT scan images, designed to assist overburdened
medical professionals. Our proposed model leverages the strengths of VGG16,
DenseNet121, and MobileNetV2 to extract features, followed by Principal
Component Analysis (PCA) for dimensionality reduction, after which the features
are stacked and classified using a Support Vector Classifier (SVC). We
conducted comparative analysis between the proposed hybrid model and individual
pre-trained CNN models, using a dataset of 2,108 training images and 373 test
images comprising both COVID-positive and non-COVID images. Our proposed hybrid
model achieved an accuracy of 98.93%, outperforming the individual models in
terms of precision, recall, F1 scores, and ROC curve performance.

æè¦ï¼æ©æåµæ¸¬ COVID-19 å°æææ²»çåæ§å¶å¶å³æ­è³ééè¦ãæ¬ç ç©¶æåºä¸åæ°ç©çæ·±åº¦å­¸ç¿æ··åæ¨¡åï¼ç¨æ¼å¾é»è¦æ·å±¤ææå½±åä¸­åµæ¸¬ COVID-19ï¼æ¨å¨åå©è² æééçé«çå°æ¥­äººå¡ãæåæåºçæ¨¡åå©ç¨ VGG16ãDenseNet121 å MobileNetV2 çåªé»ä¾èåç¹å¾µï¼æ¥èé²è¡ä¸»æååæ (PCA) ä»¥é²è¡éç¶­ï¼ç¶å¾å°ç¹å¾µå çä¸¦ä½¿ç¨æ¯æåéåé¡å¨ (SVC) é²è¡åé¡ãæåå°æåºçæ··åæ¨¡åååå¥é è¨ç·´ç CNN æ¨¡åé²è¡æ¯è¼åæï¼ä½¿ç¨åå« 2,108 å¼µè¨ç·´å½±åå 373 å¼µæ¸¬è©¦å½±åçè³æéï¼å¶ä¸­åå« COVID-19 é½æ§å½±ååé COVID-19 å½±åãæåæåºçæ··åæ¨¡åéå°äº 98.93% çæºç¢ºåº¦ï¼å¨ç²¾æºåº¦ãå¬åçãF1 åæ¸å ROC æ²ç·æè½æ¹é¢åªæ¼åå¥æ¨¡åã

##### **Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**
2501.17152v1 by Reza Ghorbani, Jyothi Rikhab Chand, Chu-Yu Lee, Mathews Jacob, Merry Mani

Three-dimensional (3D) multi-slab acquisition is a technique frequently
employed in high-resolution diffusion-weighted MRI in order to achieve the best
signal-to-noise ratio (SNR) efficiency. However, this technique is limited by
slab boundary artifacts that cause intensity fluctuations and aliasing between
slabs which reduces the accuracy of anatomical imaging. Addressing this issue
is crucial for advancing diffusion MRI quality and making high-resolution
imaging more feasible for clinical and research applications. In this work, we
propose a regularized slab profile encoding (PEN) method within a Plug-and-Play
ADMM framework, incorporating multi-scale energy (MuSE) regularization to
effectively improve the slab combined reconstruction. Experimental results
demonstrate that the proposed method significantly improves image quality
compared to non-regularized and TV-regularized PEN approaches. The regularized
PEN framework provides a more robust and efficient solution for high-resolution
3D diffusion MRI, potentially enabling clearer, more reliable anatomical
imaging across various applications.

æè¦ï¼ä¸ç¶­ (3D) å¤å±¤æ¿æ·åæ¯ä¸ç¨®æè¡ï¼ç¶å¸¸ä½¿ç¨æ¼é«è§£æåº¦æ´æ£å æ¬ MRIï¼ä»¥éå°æä½³çè¨èéè¨æ¯ (SNR) æçãç¶èï¼æ­¤æè¡åå°å±¤æ¿éçå½å½±çéå¶ï¼æé æå¼·åº¦æ³¢ååå±¤æ¿ä¹éçæ··çï¼éä½è§£åå½±åçæºç¢ºåº¦ãè§£æ±ºéååé¡å°æ¼æåæ´æ£ MRI åè³ªè³ééè¦ï¼ä¸¦ä½¿é«è§£æåº¦å½±åæ´é©ç¨æ¼è¨åºåç ç©¶æç¨ãå¨éé å·¥ä½ä¸­ï¼æåå¨ Plug-and-Play ADMM æ¶æ§å§æåºæ­£è¦åçå±¤æ¿è¼ªå»ç·¨ç¢¼ (PEN) æ¹æ³ï¼ä¸¦çµåå¤å°ºåº¦è½é (MuSE) æ­£è¦åï¼ä»¥æææ¹åå±¤æ¿çµåéå»ºãå¯¦é©çµæè­æï¼èéæ­£è¦åå TV æ­£è¦å PEN æ¹æ³ç¸æ¯ï¼ææåºçæ¹æ³é¡¯èæåäºå½±ååè³ªãæ­£è¦åç PEN æ¶æ§çºé«è§£æåº¦ 3D æ´æ£ MRI æä¾æ´å¼·åºä¸ææççè§£æ±ºæ¹æ¡ï¼æ½å¨å¯å¯¦ç¾æ´æ¸æ°ãæ´å¯é çè§£åå½±åï¼é©ç¨æ¼åç¨®æç¨ã

##### **Irony Detection, Reasoning and Understanding in Zero-shot Learning**
2501.16884v1 by Peiling Yi, Yuhan Xia

Irony is a powerful figurative language (FL) on social media that can
potentially mislead various NLP tasks, such as recommendation systems,
misinformation checks, and sentiment analysis. Understanding the implicit
meaning of this kind of subtle language is essential to mitigate irony's
negative impact on NLP tasks. However, building models to understand irony
presents a unique set of challenges, because irony is a complex form of
language that often relies on context, tone, and subtle cues to convey meaning
that is opposite or different from the literal interpretation. Large language
models, such as ChatGPT, are increasingly able to capture implicit and
contextual information. In this study, we investigate the generalization,
reasoning and understanding ability of ChatGPT on irony detection across six
different genre irony detection datasets. Our findings suggest that ChatGPT
appears to show an enhanced language understanding and reasoning ability. But
it needs to be very careful in prompt engineering design. Thus, we propose a
prompt engineering design framework IDADP to achieve higher irony detection
accuracy, improved understanding of irony, and more effective explanations
compared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain
via experiments that the practice generated under the framework is likely to be
the promised solution to resolve the generalization issues of LLMs.

æè¦ï¼åè«·æ¯ä¸ç¨®å¼·å¤§çç¤¾äº¤åªé«æ¯å»èªè¨ (FL)ï¼å¯è½æèª¤å°åç¨® NLP ä»»åï¼ä¾å¦æ¨è¦ç³»çµ±ãé¯èª¤è¨æ¯æª¢æ¥åæç·åæãçè§£éç¨®å¾®å¦èªè¨çé±å«å«ç¾©å°æ¼æ¸è¼åè«·å° NLP ä»»åçè² é¢å½±é¿è³ééè¦ãç¶èï¼å»ºç«æ¨¡åä¾çè§£åè«·æå¸¶ä¾ä¸ç³»åç¨ç¹çææ°ï¼å çºåè«·æ¯ä¸ç¨®è¤éçèªè¨å½¢å¼ï¼éå¸¸ä¾è³´æ¼ä¸ä¸æãèªæ°£åå¾®å¦çç·ç´¢ä¾å³éèå­é¢è§£éç¸åæä¸åçå«ç¾©ãå¤§åèªè¨æ¨¡åï¼ä¾å¦ ChatGPTï¼è¶ä¾è¶è½å¤ ææé±å«åä¸ä¸æä¿¡æ¯ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨äº ChatGPT å¨å­åä¸åé¡ååè«·æª¢æ¸¬æ¸æéä¸çåè«·æª¢æ¸¬çæ¦æ¬ãæ¨çåçè§£è½åãæåçç ç©¶çµæè¡¨æï¼ChatGPT ä¼¼ä¹è¡¨ç¾åºå¢å¼·çèªè¨çè§£åæ¨çè½åãä½å®éè¦å¨æç¤ºå·¥ç¨è¨­è¨ä¸­éå¸¸å°å¿ãå æ­¤ï¼æåæåºäºä¸åæç¤ºå·¥ç¨è¨­è¨æ¡æ¶ IDADPï¼ä»¥å¯¦ç¾æ´é«çåè«·æª¢æ¸¬æºç¢ºåº¦ãæ¹é²çåè«·çè§£ä»¥åèå¶ä»æåé²ç ChatGPT é¶æ¬¡å­¸ç¿æ¹æ³ç¸æ¯æ´ææçè§£éãä¸¦ééå¯¦é©ç¢ºå®å¨è©²æ¡æ¶ä¸ç¢ççå¯¦è¸å¾å¯è½æ¯è§£æ±º LLM æ¦æ¬åé¡çæ¿è«¾è§£æ±ºæ¹æ¡ã

##### **Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**
2501.17207v1 by Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang

Functional brain connectome is crucial for deciphering the neural mechanisms
underlying cognitive functions and neurological disorders. Graph deep learning
models have recently gained tremendous popularity in this field. However, their
actual effectiveness in modeling the brain connectome remains unclear. In this
study, we re-examine graph deep learning models based on four large-scale
neuroimaging studies encompassing diverse cognitive and clinical outcomes.
Surprisingly, we find that the message aggregation mechanism, a hallmark of
graph deep learning models, does not help with predictive performance as
typically assumed, but rather consistently degrades it. To address this issue,
we propose a hybrid model combining a linear model with a graph attention
network through dual pathways, achieving robust predictions and enhanced
interpretability by revealing both localized and global neural connectivity
patterns. Our findings urge caution in adopting complex deep learning models
for functional brain connectome analysis, emphasizing the need for rigorous
experimental designs to establish tangible performance gains and perhaps more
importantly, to pursue improvements in model interpretability.

æè¦ï¼åè½æ§è¦é£æ¥é«å°æ¼ç ´è­¯èªç¥åè½åç¥ç¶ç¾çèå¾çæ©å¶è³ééè¦ãåå½¢æ·±åº¦å­¸ç¿æ¨¡åæè¿å¨éåé åç²å¾æ¥µå¤§çæ­¡è¿ãç¶èï¼å®åå¨å»ºæ¨¡è¦é£æ¥é«çå¯¦éæè½ä»ä¸æç¢ºãå¨éé ç ç©¶ä¸­ï¼æåæ ¹æåé æ¶µèä¸åèªç¥åè¨åºçµæçå¤§è¦æ¨¡ç¥ç¶å½±åç ç©¶ï¼éæ°æª¢è¦åå½¢æ·±åº¦å­¸ç¿æ¨¡åãä»¤äººé©è¨çæ¯ï¼æåç¼ç¾è¨æ¯èåæ©å¶ï¼åå½¢æ·±åº¦å­¸ç¿æ¨¡åçæ¨èªï¼ä¸¦ä¸åéå¸¸åè¨­çé£æ¨£æå©æ¼é æ¸¬æè½ï¼åèæçºéä½æè½ãçºäºè§£æ±ºéååé¡ï¼æåæåºä¸åæ··åæ¨¡åï¼éééè·¯å¾çµåç·æ§æ¨¡åèåå½¢æ³¨æåç¶²è·¯ï¼éæç©©å¥çé æ¸¬åå¢å¼·çå¯è§£éæ§ï¼æ¹æ³æ¯æ­é²å±é¨åæ´é«çç¥ç¶é£æ¥æ¨¡å¼ãæåçç¼ç¾æ¦ä¿å¨æ¡ç¨è¤éçæ·±åº¦å­¸ç¿æ¨¡åé²è¡åè½æ§è¦é£æ¥é«åææä¿æè¬¹æï¼å¼·èª¿éè¦å´è¬¹çå¯¦é©è¨­è¨ï¼ä»¥å»ºç«å·é«çæè½å¢çï¼æè¨±æ´éè¦çæ¯ï¼è¿½æ±æ¨¡åå¯è§£éæ§çæ¹é²ã

##### **Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**
2501.17206v1 by Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao

This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.

æè¦ï¼æ¬ç ç©¶æ¢ç´¢ä¸ç¨®åµæ°çæ¹æ³ï¼ééæ´åç¤¾æè¼å©æ©å¨äººãå¼·åå­¸ç¿ (RL)ãå¤§åèªè¨æ¨¡å (LLM) åè¨åºé åå°æ¥­ç¥è­æ¼æ¨¡æ¬ç°å¢ä¸­ï¼ä»¥æ¨é²å¤±æºçç§è­·ãéç¨®æ´åè§£æ±ºäºå¤±æºçç§è­·ä¸­ç¤¾æè¼å©æ©å¨äººå¯¦é©æ¸ææéçéå¤§ææ°ï¼æä¾äºä¸ååæçæ¨¡æ¬ç°å¢ï¼çå¯¦å°æ¨¡æ¬å¤±æºçæ£è (PLWD) åæ©å¨äººç§è­·èä¹éçäºåãææåºçæ¶æ§å¼å¥äºæ©çæ¨¡åä¾è¡¨ç¤º PLWD çèªç¥åæç·çæï¼ä¸¦çµåäºåºæ¼ LLM çè¡çºæ¨¡æ¬ä¾æ¨¡æ¬ä»åçåæãæåé²ä¸æ­¥éç¼ä¸¦è¨ç·´äºä¸åé©ææ§ RL ç³»çµ±ï¼ä½¿ Pepper ç­äººå½¢æ©å¨äººè½å¤ æ ¹æ PLWD çèªç¥åæç·çææä¾æå¢æç¥ååäººåçäºåååå©ãè©²æ¶æ§ä¹æ¦æ¬å°é»è¦ä»£çï¼çªé¡¯äºå®çå¤åè½æ§ãçµæè¡¨æï¼ç± LLM å¢å¼·ç RL ç³»çµ±ææå°è§£éååæ PLWD çè¤ééæ±ï¼æä¾éèº«æé çç§è­·ç­ç¥ãéé ç ç©¶ééæä¾ä¸åå¯èªè¨ç AI é©åç§è­·å¹³å°ï¼ä¿é²å°å¤±æºçç¸éææ°çäºè§£ï¼ä¸¦ä¿é²è¼å©æè¡çåä½åµæ°ï¼çºäººæ©äºååäººæ©äºåååºè²¢ç»ãææåºçæ¹æ³æå¯è½æé« PLWD çç¨ç«æ§åçæ´»åè³ªï¼åææ¸è¼ç§è­·èçè² æï¼å¼·èª¿äºäºåå°å AI ç³»çµ±å¨å¤±æºçç§è­·ä¸­çè½åä½ç¨ã

##### **Efficient Knowledge Distillation of SAM for Medical Image Segmentation**
2501.16740v1 by Kunal Dasharath Patil, Gowthamaan Palani, Ganapathy Krishnamurthi

The Segment Anything Model (SAM) has set a new standard in interactive image
segmentation, offering robust performance across various tasks. However, its
significant computational requirements limit its deployment in real-time or
resource-constrained environments. To address these challenges, we propose a
novel knowledge distillation approach, KD SAM, which incorporates both encoder
and decoder optimization through a combination of Mean Squared Error (MSE) and
Perceptual Loss. This dual-loss framework captures structural and semantic
features, enabling the student model to maintain high segmentation accuracy
while reducing computational complexity. Based on the model evaluation on
datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast
Ultrasound, we demonstrate that KD SAM achieves comparable or superior
performance to the baseline models, with significantly fewer parameters. KD SAM
effectively balances segmentation accuracy and computational efficiency, making
it well-suited for real-time medical image segmentation applications in
resource-constrained environments.

æè¦ï¼åæ®µä»»ä½æ¨¡å (SAM) å·²å¨äºåå¼å½±ååå²ä¸­æ¨¹ç«æ°æ¨æºï¼å¨åé ä»»åä¸­çè½æä¾ç©©å¥çæè½ãç¶èï¼å¶é¾å¤§çéç®éæ±éå¶äºå®å¨å³ææè³æºåéç°å¢ä¸­çé¨ç½²ãçºäºæå°éäºææ°ï¼æåæåºäºä¸ç¨®æ°ç©çç¥è­è¸é¤¾æ¹æ³ï¼KD SAMï¼å®ééçµååæ¹èª¤å·® (MSE) åæç¥æå¤±ï¼å°ç·¨ç¢¼å¨åè§£ç¢¼å¨æä½³åç´å¥å¶ä¸­ãæ­¤ééæå¤±æ¶æ§æ·åçµæ§åèªç¾©ç¹å¾µï¼è®å­¸çæ¨¡åè½å¤ å¨éä½éç®è¤éåº¦çåæï¼ç¶­æé«åå²æºç¢ºåº¦ãæ ¹æå¨ Kvasir-SEGãISIC 2017ãèåé ­é¨è¶é³æ³¢åä¹³æ¿è¶é³æ³¢ç­è³æéä¸çæ¨¡åè©ä¼°ï¼æåè­æ KD SAM éå°äºèåºæºæ¨¡åç¸ç¶ææ´åªç°çæè½ï¼ä¸åæ¸æé¡¯æ´å°ãKD SAM ææå°å¹³è¡¡äºåå²æºç¢ºåº¦åéç®æçï¼ä½¿å¶éå¸¸é©åå¨è³æºåéç°å¢ä¸­çå³æé«å­¸å½±ååå²æç¨ã

##### **VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**
2501.16672v1 by Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour

Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨è¨åºé«å­¸ä¸­çæææ¬çäºå¯¦æºç¢ºæ§ï¼ç¼ºä¹ç¢ºä¿çæ¹æ³ãVeriFact æ¯ä¸ç¨®äººå·¥æºæ§ç³»çµ±ï¼çµåäºæª¢ç´¢å¢å¼·çæå LLM-as-a-Judgeï¼ç¨æ¼é©è­ LLM çæçææ¬æ¯å¦åºæ¼çäººçé»å­å¥åº·è¨é (EHR) ç²å¾çäººççæ­·äºå¯¦æ¯æãçºäºè©ä¼°éåç³»çµ±ï¼æåå¼å¥äº VeriFact-BHCï¼éæ¯ä¸åæ°çè³æéï¼å°åºé¢æè¦ä¸­çç°¡è¦ä½é¢çç¨åè§£æä¸çµç°¡å®çé³è¿°ï¼ä¸¦ç±è¨åºé«çè¨»è§£æ¯ä¸åé³è¿°æ¯å¦ç²å¾çäººç EHR çæ­·æè¦æ¯æãåç®¡è¨åºé«çä¹éçæé«ä¸è´æ§çº 88.5%ï¼ä½èå»åªåè£æ±ºçå¹³åäººé¡è¨åºé«çåºæ¬äºå¯¦ç¸æ¯ï¼VeriFact çä¸è´æ§é«é 92.7%ï¼éè¡¨æ VeriFact è¶è¶äºå¹³åè¨åºé«çæ ¹æçäººççæ­·æª¢æ¥ææ¬äºå¯¦çè½åãVeriFact å¯è½æééç§»é¤ç®åçè©ä¼°ç¶é ¸ï¼å éåºæ¼ LLM ç EHR æç¨ç¨å¼çéç¼ã

##### **Vision-based autonomous structural damage detection using data-driven methods**
2501.16662v2 by Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei

This study addresses the urgent need for efficient and accurate damage
detection in wind turbine structures, a crucial component of renewable energy
infrastructure. Traditional inspection methods, such as manual assessments and
non-destructive testing (NDT), are often costly, time-consuming, and prone to
human error. To tackle these challenges, this research investigates advanced
deep learning algorithms for vision-based structural health monitoring (SHM). A
dataset of wind turbine surface images, featuring various damage types and
pollution, was prepared and augmented for enhanced model training. Three
algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to
detect and classify surface damage. The models were trained and evaluated on a
dataset split into training, testing, and evaluation subsets (80%-10%-10%).
Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50
and high processing speed, making it suitable for real-time inspections. By
optimizing hyperparameters like learning rate and batch size, the models'
accuracy and efficiency improved further. YOLOv7 demonstrated significant
advancements in detection precision and execution speed, especially for
real-time applications. However, challenges such as dataset limitations and
environmental variability were noted, suggesting future work on segmentation
methods and larger datasets. This research underscores the potential of
vision-based deep learning techniques to transform SHM practices by reducing
costs, enhancing safety, and improving reliability, thus contributing to the
sustainable maintenance of critical infrastructure and supporting the longevity
of wind energy systems.

æè¦ï¼æ¬ç ç©¶è§£æ±ºäºé¢¨åæ¸¦è¼ªæ©çµæ§ä¸­è¿«åéè¦çææä¸æºç¢ºçæå·æª¢æ¸¬ï¼éæ¯å¯åçè½æºåºç¤è¨­æ½çééµçµæé¨åãå³çµ±çæª¢æ¥æ¹æ³ï¼ä¾å¦æåè©ä¼°åéç ´å£æ§æª¢æ¸¬ (NDT)ï¼éå¸¸ææ¬é«æãèæä¸å®¹æåºé¯ãçºäºæå°éäºææ°ï¼æ¬ç ç©¶èª¿æ¥äºç¨æ¼åºæ¼è¦è¦ºççµæ§å¥åº·ç£æ¸¬ (SHM) çåé²æ·±åº¦å­¸ç¿æ¼ç®æ³ãæºåäºä¸çµé¢¨åæ¸¦è¼ªæ©è¡¨é¢å½±åçè³æéï¼å¶ä¸­åå«åç¨®æå£é¡ååæ±¡æï¼ä¸¦æ´åäºå¢å¼·æ¨¡åè¨ç·´ãæ¡ç¨äºä¸ç¨®æ¼ç®æ³ââYOLOv7ãå¶è¼éç´è®é«å Faster R-CNNââä¾æª¢æ¸¬ååé¡è¡¨é¢æå£ãéäºæ¨¡åå¨åå²æè¨ç·´ãæ¸¬è©¦åè©ä¼°å­éï¼80%-10%-10%ï¼çè³æéä¸é²è¡è¨ç·´åè©ä¼°ãçµæè¡¨æï¼YOLOv7 åªæ¼å¶ä»æ¼ç®æ³ï¼å¯¦ç¾äº 82.4% ç mAP@50 åè¼é«çèçéåº¦ï¼ä½¿å¶é©ç¨æ¼å³ææª¢æ¥ãééæä½³åå­¸ç¿çåæ¹æ¬¡å¤§å°ç­è¶åæ¸ï¼æ¨¡åçæºç¢ºæ§åæçé²ä¸æ­¥æé«ãYOLOv7 å¨æª¢æ¸¬ç²¾åº¦åå·è¡éåº¦æ¹é¢è¡¨ç¾åºé¡¯èçé²æ­¥ï¼ç¹å¥æ¯å°æ¼å³ææç¨ç¨å¼ãç¶èï¼æ³¨æå°è³æééå¶åç°å¢è®ç°æ§ç­ææ°ï¼éè¡¨ææªä¾å¨åå²æ¹æ³åæ´å¤§çè³æéæ¹é¢çå·¥ä½ãæ¬ç ç©¶å¼·èª¿äºåºæ¼è¦è¦ºçæ·±åº¦å­¸ç¿æè¡å¨è½æ SHM å¯¦åæ¹é¢çæ½åï¼æ¹æ³æ¯éä½ææ¬ãå¢å¼·å®å¨æ§ä¸¦æé«å¯é æ§ï¼å¾èæå©æ¼ç¶­è­·ééµåºç¤è¨­æ½çå¯æçºæ§ä¸¦æ¯æé¢¨è½ç³»çµ±çé·å£½å½ã

##### **Molecular-driven Foundation Model for Oncologic Pathology**
2501.16652v1 by Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood

Foundation models are reshaping computational pathology by enabling transfer
learning, where models pre-trained on vast datasets can be adapted for
downstream diagnostic, prognostic, and therapeutic response tasks. Despite
these advances, foundation models are still limited in their ability to encode
the entire gigapixel whole-slide images without additional training and often
lack complementary multimodal data. Here, we introduce Threads, a slide-level
foundation model capable of generating universal representations of whole-slide
images of any size. Threads was pre-trained using a multimodal learning
approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained
tissue sections, paired with corresponding genomic and transcriptomic profiles
- the largest such paired dataset to be used for foundation model development
to date. This unique training paradigm enables Threads to capture the tissue's
underlying molecular composition, yielding powerful representations applicable
to a wide array of downstream tasks. In extensive benchmarking across 54
oncology tasks, including clinical subtyping, grading, mutation prediction,
immunohistochemistry status determination, treatment response prediction, and
survival prediction, Threads outperformed all baselines while demonstrating
remarkable generalizability and label efficiency. It is particularly well
suited for predicting rare events, further emphasizing its clinical utility. We
intend to make the model publicly available for the broader community.

æè¦ï¼åºç¤æ¨¡åééåç¨è½ç§»å­¸ç¿ä¾éå¡è¨ç®ççå­¸ï¼å¶ä¸­é åå¨é¾å¤§è³æéä¸è¨ç·´çæ¨¡åå¯é©ææ¼ä¸æ¸¸è¨ºæ·ãé å¾åæ²»çåæä»»åãåç®¡æéäºé²å±ï¼åºç¤æ¨¡åå¨ç·¨ç¢¼æ´ååååç´ å¨å¹»ççå½±åçè½åä¸ä»æéï¼ä¸ç¶å¸¸ç¼ºä¹è£åå¤æ¨¡å¼è³æãå¨æ­¤ï¼æåä»ç´¹ Threadsï¼éæ¯ä¸åå¹»ççå±¤ç´åºç¤æ¨¡åï¼è½å¤ ç¢çä»»ä½å¤§å°çå¨å¹»ççå½±åçéç¨è¡¨ç¤ºãThreads ä½¿ç¨å¤æ¨¡å¼å­¸ç¿æ¹æ³é åè¨ç·´ï¼ä¸¦éå° 47,171 åèæ¨ç²¾åæç´ (H&E) æè²ççµç¹åççå¤åç¾¤çµé²è¡è¨ç·´ï¼ä¸¦æ­éå°æçåºå é«åè½éçµç¹å¾µæªï¼éæ¯è¿ä»çºæ­¢ç¨æ¼åºç¤æ¨¡åéç¼çæå¤§æ­¤é¡éå°è³æéãéç¨®ç¨ç¹çè¨ç·´ç¯ä¾ä½¿ Threads è½å¤ æ·åçµç¹çåºç¤åå­çµæï¼ç¢çå¼·å¤§çè¡¨ç¤ºï¼é©ç¨æ¼å»£æ³çä¸æ¸¸ä»»åãå¨æ¶µè 54 åè«ç¤å­¸ä»»åçå»£æ³åºæºæ¸¬è©¦ä¸­ï¼åæ¬è¨åºååãåç´ãçªè®é æ¸¬ãåç«çµç¹åå­¸çæå¤å®ãæ²»çåæé æ¸¬åå­æ´»é æ¸¬ï¼Threads åªæ¼ææåºæºï¼åæå±ç¾åºé¡¯èçæ¦æ¬æ§åæ¨ç±¤æçãå®ç¹å¥é©åé æ¸¬ç½è¦äºä»¶ï¼é²ä¸æ­¥å¼·èª¿å¶è¨åºæç¨ãæåæç®è®è©²æ¨¡åå¬éï¼ä¾æ´å»£æ³çç¤¾ç¾¤ä½¿ç¨ã

##### **Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections**
2502.00045v1 by Yi Mao, Andrew Perrault

Municipal inspections are an important part of maintaining the quality of
goods and services. In this paper, we approach the problem of intelligently
scheduling service inspections to maximize their impact, using the case of food
establishment inspections in Chicago as a case study. The Chicago Department of
Public Health (CDPH) inspects thousands of establishments each year, with a
substantial fail rate (over 3,000 failed inspection reports in 2023). To
balance the objectives of ensuring adherence to guidelines, minimizing
disruption to establishments, and minimizing inspection costs, CDPH assigns
each establishment an inspection window every year and guarantees that they
will be inspected exactly once during that window. These constraints create a
challenge for a restless multi-armed bandit (RMAB) approach, for which there
are no existing methods. We develop an extension to Whittle index-based systems
for RMABs that can guarantee action window constraints and frequencies, and
furthermore can be leveraged to optimize action window assignments themselves.
Briefly, we combine MDP reformulation and integer programming-based lookahead
to maximize the impact of inspections subject to constraints. A neural
network-based supervised learning model is developed to model state transitions
of real Chicago establishments using public CDPH inspection records, which
demonstrates 10\% AUC improvements compared with directly predicting
establishments' failures. Our experiments not only show up to 24\% (in
simulation) or 33\% (on real data) reward improvements resulting from our
approach but also give insight into the impact of scheduling constraints.

æè¦ï¼å¸æ¿æª¢æ¥æ¯ç¶­æåååæååè³ªçéè¦ä¸ç°ãå¨æ¬æä¸­ï¼æåä»¥èå å¥çé£åæ©æ§æª¢æ¥çºæ¡ä¾ç ç©¶ï¼æ¢è¨å¦ä½ééæºæ§æç¨æåæª¢æ¥ä»¥æå¤§åå¶å½±é¿åãèå å¥å¬å±è¡çé¨ (CDPH) æ¯å¹´æª¢æ¥æ¸åå®¶æ©æ§ï¼ä¸æç¸ç¶é«çä¸åæ ¼çï¼2023 å¹´æè¶é 3,000 ä»½ä¸åæ ¼æª¢æ¥å ±åï¼ãçºäºå¹³è¡¡ç¢ºä¿éµå¾ªæºåãæå¤§ç¨åº¦æ¸å°å°æ©æ§çå¹²æ¾åæå¤§ç¨åº¦æ¸å°æª¢æ¥ææ¬çç®æ¨ï¼CDPH æ¯å¹´çºæ¯å®¶æ©æ§åéä¸åæª¢æ¥ææ®µï¼ä¸¦ä¿è­å¨è©²ææ®µå§åªæª¢æ¥ä¸æ¬¡ãéäºéå¶å°ä¸éæ·å¤èè³­åæ© (RMAB) æ¹æ³æ§æææ°ï¼ç®åå°ç¡ç¾ææ¹æ³ãæåéå° RMAB ç Whittle ææ¸ç³»çµ±éç¼äºä¸åå»¶ä¼¸ï¼å¯ä»¥ä¿è­åä½ææ®µéå¶åé »çï¼æ­¤å¤éå¯ä»¥å©ç¨å®ä¾åªååä½ææ®µåéæ¬èº«ãç°¡èè¨ä¹ï¼æåçµå MDP éæ°å¶å®ååºæ¼æ´æ¸è¦åçåç»æ§ï¼ä»¥æå¤§åç¬¦åéå¶æ¢ä»¶çæª¢æ¥å½±é¿ãä½¿ç¨å¬éç CDPH æª¢æ¥è¨ééç¼äºä¸ååºæ¼ç¥ç¶ç¶²è·¯çç£ç£å¼å­¸ç¿æ¨¡åï¼ç¨æ¼æ¨¡æ¬èå å¥å¯¦éæ©æ§ççæè½æï¼èç´æ¥é æ¸¬æ©æ§çå¤±æç¸æ¯ï¼é¡¯ç¤ºåº AUC æåäº 10%ãæåçå¯¦é©ä¸åé¡¯ç¤ºæåçåæ³å¸¶ä¾ççåµæåé«é 24%ï¼æ¨¡æ¬ä¸­ï¼æ 33%ï¼å¯¦éè³æä¸­ï¼ï¼éæ·±å¥äºè§£äºæç¨éå¶çå½±é¿ã

##### **Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**
2501.16282v1 by Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu

Understanding brain disorders is crucial for accurate clinical diagnosis and
treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a
promising approach to interpreting medical images with the support of text
descriptions. However, previous research has primarily focused on 2D medical
images, leaving richer spatial information of 3D images under-explored, and
single-modality-based methods are limited by overlooking the critical clinical
information contained in other modalities. To address this issue, this paper
proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck
layer to learn new knowledge and instill it into the original pre-trained
knowledge. The major idea is to incorporate a lightweight bottleneck layer to
train fewer parameters while capturing essential information and utilize a
Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal
data within a unified representation space. Extensive experiments demonstrated
the effectiveness of our approach in integrating multimodal data to
significantly improve the diagnosis accuracy without high computational costs,
highlighting the potential to enhance real-world diagnostic workflows.

æè¦ï¼äºè§£è¦é¨ç¾çå°æ¼æºç¢ºçè¨åºè¨ºæ·åæ²»çè³ééè¦ãå¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) çææ°é²å±æä¾äºä¸åæåéçæ¹æ³ï¼å¯ä»¥å¨ææ¬æè¿°çæ¯æ´ä¸è©®éé«å­¸å½±åãç¶èï¼ååçç ç©¶ä¸»è¦éä¸­å¨ 2D é«å­¸å½±åï¼å¿½ç¥äº 3D å½±åæ´è±å¯çç©ºéè³è¨ï¼èå®ä¸æ¨¡ææ¹æ³åå°å¿½è¦å¶ä»æ¨¡æä¸­ééµè¨åºè³è¨çéå¶ãçºäºè§£æ±ºéååé¡ï¼æ¬ææåºäº Brain-Adapterï¼éæ¯ä¸ç¨®æ°çæ¹æ³ï¼å®çµåäºä¸åé¡å¤çç¶é ¸å±¤ä¾å­¸ç¿æ°ç¥è­ä¸¦å°å¶çè¼¸å°åå§é è¨ç·´çç¥è­ä¸­ãä¸»è¦çæ³æ³æ¯çµåä¸åè¼éç´ç¶é ¸å±¤ï¼å¨æ·åå¿è¦è³è¨çåæè¨ç·´è¼å°çåæ¸ï¼ä¸¦å©ç¨å°æ¯èªè¨å½±åé è¨ç·´ (CLIP) ç­ç¥å¨çµ±ä¸çè¡¨ç¤ºç©ºéä¸­å°é½å¤æ¨¡æè³æãå»£æ³çå¯¦é©è­æäºæåçæ¹æ³å¨æ´åå¤æ¨¡æè³æä»¥é¡¯èæé«è¨ºæ·æºç¢ºæ§æ¹é¢çæææ§ï¼èä¸æé æé«éç®ææ¬ï¼çªé¡¯äºå¢å¼·çå¯¦ä¸çè¨ºæ·å·¥ä½æµç¨çæ½åã

##### **Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**
2501.16215v1 by Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li

Large language models (LLMs) exhibit remarkable capabilities in visual
inspection of medical time-series data, achieving proficiency comparable to
human clinicians. However, their broad scope limits domain-specific precision,
and proprietary weights hinder fine-tuning for specialized datasets. In
contrast, small specialized models (SSMs) excel in targeted tasks but lack the
contextual reasoning required for complex clinical decision-making. To address
these challenges, we propose ConMIL (Conformalized Multiple Instance Learning),
a decision-support SSM that integrates seamlessly with LLMs. By using Multiple
Instance Learning (MIL) to identify clinically significant signal segments and
conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'
interpretative capabilities for medical time-series analysis. Experimental
results demonstrate that ConMIL significantly improves the performance of
state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,
\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for
confident samples in arrhythmia detection and sleep staging, compared to
standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the
potential of ConMIL to bridge task-specific precision and broader contextual
reasoning, enabling more reliable and interpretable AI-driven clinical decision
support.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çæéåºåè³æçè¦è¦ºæª¢æ¥ä¸­å±ç¾åºéå¡çè½åï¼éå°äºèäººé¡è¨åºé«çç¸ç¶ççç·´åº¦ãç¶èï¼å®åçå»£æ³ç¯åéå¶äºç¹å®é åçç²¾ç¢ºåº¦ï¼èå°ææ¬éé»ç¤äºéå°ç¹å®è³æéçå¾®èª¿ãç¸æ¯ä¹ä¸ï¼å°åå°ç¨æ¨¡å (SSM) å¨ç®æ¨ä»»åä¸­è¡¨ç¾åºè²ï¼ä½ç¼ºä¹è¤éè¨åºæ±ºç­å¶å®æéçèæ¯æ¨çãçºäºæå°éäºææ°ï¼æåæåºäº ConMILï¼å±å½¢å¤å¯¦ä¾å­¸ç¿ï¼ï¼éæ¯ä¸åè LLM ç¡ç¸«æ´åçæ±ºç­æ¯æ´ SSMãééä½¿ç¨å¤å¯¦ä¾å­¸ç¿ (MIL) ä¾è­å¥è¨åºé¡¯èè¨èåæ®µï¼ä¸¦å°æ ¡æºçéåå¼è¼¸åºé²è¡å±å½¢é æ¸¬ï¼ConMIL å¢å¼·äº LLM å°é«çæéåºååæçè§£éè½åãå¯¦é©çµæè¡¨æï¼ConMIL æé¡¯æ¹åäºæåé² LLM çæè½ï¼ä¾å¦ ChatGPT4.0 å Qwen2-VL-7Bãå·é«ä¾èªªï¼\ConMIL{}- æ¯æ´ç Qwen2-VL-7B å¨å¿å¾ä¸æ´åµæ¸¬åç¡ç åæä¸­ï¼å°æ¼æä¿¡å¿çæ¨£æ¬éå°äº 94.92% å 96.82% çç²¾ç¢ºåº¦ï¼èç¨ç« LLM çæºç¢ºåº¦åçº 46.13% å 13.16%ãéäºç¼ç¾çªé¡¯äº ConMIL å¨æ©æ¥ç¹å®ä»»åçç²¾ç¢ºåº¦åæ´å»£æ³çèæ¯æ¨çæ¹é¢çæ½åï¼å¾èå¯¦ç¾æ´å¯é ä¸å¯è§£éç AI é©åè¨åºæ±ºç­æ¯æ´ã

##### **Atla Selene Mini: A General Purpose Evaluation Model**
2501.17195v1 by Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park

We introduce Atla Selene Mini, a state-of-the-art small language
model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that
outperforms the best SLMJs and GPT-4o-mini on overall performance across 11
out-of-distribution benchmarks, spanning absolute scoring, classification, and
pairwise preference tasks. It is the highest-scoring 8B generative model on
RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To
achieve this, we develop a principled data curation strategy that augments
public datasets with synthetically generated critiques and ensures high quality
through filtering and dataset ablations. We train our model on a combined
direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and
produce a highly promptable evaluator that excels in real-world scenarios.
Selene Mini shows dramatically improved zero-shot agreement with human expert
evaluations on financial and medical industry datasets. It is also robust to
variations in prompt format. Preliminary results indicate that Selene Mini is
the top-ranking evaluator in a live, community-driven Judge Arena. We release
the model weights on HuggingFace
(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage
widespread community adoption.

æè¦ï¼æåä»ç´¹ Atla Selene Miniï¼éæ¯ä¸åæåé²çå°åèªè¨æ¨¡åè©å¯© (SLMJ)ãSelene Mini æ¯ä¸åéç¨è©ä¼°å¨ï¼å¨ 11 ååä½å¤åºæºæ¸¬è©¦ï¼åå«çµå°è©åãåé¡åæå°åå¥½ä»»åï¼çæ´é«æè½ä¸åªæ¼æä½³ç SLMJ å GPT-4o-miniãå®æ¯ RewardBench ä¸å¾åæé«ç 8B çææ¨¡åï¼è¶è¶äº GPT-4o åå°æ¥­è©å¯©ç­å¼·å¤§çåºæºãçºäºéææ­¤ç®æ¨ï¼æåéç¼äºä¸ç¨®æååçè³æç­å±ç­ç¥ï¼å©ç¨åæç¢ççè©è«æ´åå¬éè³æéï¼ä¸¦ééç¯©é¸åè³æéæ¶èç¢ºä¿é«åè³ªãæåå¨çµåç´æ¥åå¥½æä½³å (DPO) åç£ç£å¾®èª¿ (SFT) æå¤±çè³æéä¸è¨ç·´æåçæ¨¡åï¼ä¸¦ç¢çä¸åé«åº¦å¯æç¤ºçè©ä¼°å¨ï¼å¨å¯¦éå ´æ¯ä¸­è¡¨ç¾åºè²ãSelene Mini å¨è²¡ååé«çç¢æ¥­è³æéä¸é¡¯ç¤ºåºé¡¯èæ¹åçé¶æ¬¡å­¸ç¿èäººé¡å°å®¶è©ä¼°çä¸è´æ§ãå®ä¹å°æç¤ºæ ¼å¼çè®åå·æç©©å¥æ§ãåæ­¥çµæé¡¯ç¤º Selene Mini æ¯ç¤¾ç¾¤é©åç Judge Arena ä¸­æåç¬¬ä¸çè©ä¼°å¨ãæåå¨ HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) å Ollama ä¸éåºæ¨¡åæ¬éï¼ä»¥é¼åµå»£æ³çç¤¾ç¾¤æ¡ç¨ã

##### **An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**
2501.15969v1 by Shaheer Ahmad Khan, Muhammad Usamah Shahid, Ahmad Abdullah, Ibrahim Hashmat, Muddassar Farooq

This study addresses a critical gap in the healthcare system by developing a
clinically meaningful, practical, and explainable disease surveillance system
for multiple chronic diseases, utilizing routine EHR data from multiple U.S.
practices integrated with CureMD's EMR/EHR system. Unlike traditional
systems--using AI models that rely on features from patients' labs--our
approach focuses on routinely available data, such as medical history, vitals,
diagnoses, and medications, to preemptively assess the risks of chronic
diseases in the next year. We trained three distinct models for each chronic
disease: prediction models that forecast the risk of a disease 3, 6, and 12
months before a potential diagnosis. We developed Random Forest models, which
were internally validated using F1 scores and AUROC as performance metrics and
further evaluated by a panel of expert physicians for clinical relevance based
on inferences grounded in medical knowledge. Additionally, we discuss our
implementation of integrating these models into a practical EMR system. Beyond
using Shapley attributes and surrogate models for explainability, we also
introduce a new rule-engineering framework to enhance the intrinsic
explainability of Random Forests.

æè¦ï¼æ¬ç ç©¶éééç¼ä¸åè¨åºææç¾©ãå¯¦ç¨ä¸å¯è§£éçå¤éæ¢æ§ç¾çç¾çç£æ¸¬ç³»çµ±ï¼ä¾è§£æ±ºé«çä¿å¥ç³»çµ±ä¸­çéå¤§ç¼ºå£ï¼å©ç¨æ´å CureMD ç EMR/EHR ç³»çµ±ï¼ä¾èªå¤åç¾åå¯¦åçä¾è¡ EHR è³æãèå³çµ±ç³»çµ±ä¸åçæ¯ï¼æåçåæ³èéå¨ä¾è¡å¯å¾çè³æï¼ä¾å¦çæ­·ãçå½å¾µè±¡ãè¨ºæ·åè¥ç©ï¼ä»¥é åè©ä¼°æªä¾ä¸å¹´æ¢æ§ç¾ççé¢¨éªï¼èéä»°è³´çæ£å¯¦é©å®¤ç¹å¾µç AI æ¨¡åãæåéå°æ¯ç¨®æ¢æ§ç¾çè¨ç·´äºä¸åä¸åçæ¨¡åï¼é æ¸¬æ¨¡åï¼ç¨ä»¥é æ¸¬å¨æ½å¨è¨ºæ·å 3ã6 å 12 åæçç¾çé¢¨éªãæåéç¼äºé¨æ©æ£®ææ¨¡åï¼ä¸¦ä½¿ç¨ F1 åæ¸å AUROC ä½çºæè½ææ¨ï¼é²è¡å§é¨é©è­ï¼ä¸¦é²ä¸æ­¥ç±å°å®¶é«å¸«å°çµæ ¹ææ¤åºæ¼é«å­¸ç¥è­çæ¨è«ï¼è©ä¼°å¶è¨åºç¸éæ§ãæ­¤å¤ï¼æåè¨è«äºå°éäºæ¨¡åæ´åå°å¯¦ç¨ EMR ç³»çµ±ä¸­çå¯¦ä½æ¹å¼ãé¤äºä½¿ç¨ Shapley å±¬æ§åä»£çæ¨¡åä¾è§£éå¤ï¼æåéå¼é²äºä¸åæ°çè¦åå·¥ç¨æ¶æ§ï¼ä»¥å¢å¼·é¨æ©æ£®æçå§å¨å¯è§£éæ§ã

##### **Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**
2501.15733v1 by Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan

Alzheimer's disease (AD) is a neurodegenerative disorder affecting millions
worldwide, necessitating early and accurate diagnosis for optimal patient
management. In recent years, advancements in deep learning have shown
remarkable potential in medical image analysis. Methods In this study, we
present "ViTranZheimer," an AD diagnosis approach which leverages video vision
transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as
videos, we exploit the temporal dependencies between slices to capture
intricate structural relationships. The video vision transformer's
self-attention mechanisms enable the model to learn long-range dependencies and
identify subtle patterns that may indicate AD progression. Our proposed deep
learning framework seeks to enhance the accuracy and sensitivity of AD
diagnosis, empowering clinicians with a tool for early detection and
intervention. We validate the performance of the video vision transformer using
the ADNI dataset and conduct comparative analyses with other relevant models.
Results The proposed ViTranZheimer model is compared with two hybrid models,
CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional
neural network (CNN) and a bidirectional long-short-term memory network
(BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT)
with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and
ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer
demonstrated the highest accuracy at 98.6%, outperforming other models in this
evaluation metric, indicating its superior performance in this specific
evaluation metric. Conclusion This research advances the understanding of
applying deep learning techniques in neuroimaging and Alzheimer's disease
research, paving the way for earlier and less invasive clinical diagnosis.

æè¦ï¼é¿è²æµ·é»ç (AD) æ¯ä¸ç¨®ç¥ç¶éåæ§ç¾çï¼å½±é¿èå¨çæ¸ç¾è¬äººï¼å æ­¤éè¦æ©ææºç¢ºè¨ºæ·ä»¥é²è¡æä½³æ£èç®¡çãè¿å¹´ä¾ï¼æ·±åº¦å­¸ç¿çé²æ­¥å¨é«å­¸å½±ååæä¸­å±ç¾äºéå¡çæ½åãæ¹æ³å¨éåç ç©¶ä¸­ï¼æåæåºãViTranZheimerãï¼ä¸ç¨® AD è¨ºæ·æ¹æ³ï¼å®å©ç¨å½±çè¦è¦ºè½æå¨ä¾åæ 3D å¤§è¦ MRI è³æãééå° 3D MRI é«ç©è¦çºå½±çï¼æåå©ç¨åçä¹éçæéä¾è³´æ§ä¾ææè¤éççµæ§éä¿ãå½±çè¦è¦ºè½æå¨çèªæ³¨æåæ©å¶ä½¿æ¨¡åè½å¤ å­¸ç¿é·ç¨ä¾è³´æ§ï¼ä¸¦è­å¥å¯è½è¡¨ç¤º AD é²å±çç´°å¾®æ¨¡å¼ãæåæåºçæ·±åº¦å­¸ç¿æ¶æ§æ¨å¨æé« AD è¨ºæ·çæºç¢ºæ§åæææ§ï¼çºè¨åºé«çæä¾æ©ææª¢æ¸¬åå¹²é çå·¥å·ãæåä½¿ç¨ ADNI è³æéé©è­å½±çè¦è¦ºè½æå¨çæè½ï¼ä¸¦èå¶ä»ç¸éæ¨¡åé²è¡æ¯è¼åæãçµæææåºç ViTranZheimer æ¨¡åèå©åæ··åæ¨¡å CNN-BiLSTM å ViT-BiLSTM é²è¡æ¯è¼ãCNN-BiLSTM æ¯å·ç©ç¥ç¶ç¶²è·¯ (CNN) åéåé·ç­æè¨æ¶ç¶²è·¯ (BiLSTM) ççµåï¼è ViT-BiLSTM æ¯è¦è¦ºè½æå¨ (ViT) è BiLSTM ççµåãå¨ ViTranZheimerãCNN-BiLSTM å ViT-BiLSTM æ¨¡åä¸­éå°çæºç¢ºåº¦åå¥çº 98.6%ã96.479% å 97.465%ãViTranZheimer ä»¥ 98.6% çæºç¢ºåº¦è¡¨ç¾æä½³ï¼å¨éåè©ä¼°ææ¨ä¸­åªæ¼å¶ä»æ¨¡åï¼é¡¯ç¤ºåºå¶å¨éåç¹å®è©ä¼°ææ¨ä¸­çåè¶æè½ãçµè«éé ç ç©¶ä¿é²äºå¨ç¥ç¶å½±ååé¿è²æµ·é»çç ç©¶ä¸­æç¨æ·±åº¦å­¸ç¿æè¡ççè§£ï¼çºæ´æ©ãæ´ä½ä¾µå¥æ§çè¨åºè¨ºæ·éªè·¯ã

##### **A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**
2501.15724v1 by Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Ajit J. Nirmal, Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao

Computational pathology foundation models (CPathFMs) have emerged as a
powerful approach for analyzing histopathological data, leveraging
self-supervised learning to extract robust feature representations from
unlabeled whole-slide images. These models, categorized into uni-modal and
multi-modal frameworks, have demonstrated promise in automating complex
pathology tasks such as segmentation, classification, and biomarker discovery.
However, the development of CPathFMs presents significant challenges, such as
limited data accessibility, high variability across datasets, the necessity for
domain-specific adaptation, and the lack of standardized evaluation benchmarks.
This survey provides a comprehensive review of CPathFMs in computational
pathology, focusing on datasets, adaptation strategies, and evaluation tasks.
We analyze key techniques, such as contrastive learning and multi-modal
integration, and highlight existing gaps in current research. Finally, we
explore future directions from four perspectives for advancing CPathFMs. This
survey serves as a valuable resource for researchers, clinicians, and AI
practitioners, guiding the advancement of CPathFMs toward robust and clinically
applicable AI-driven pathology solutions.

æè¦ï¼è¨ç®ççåºç¤æ¨¡å (CPathFM) å·²æçºåæçµç¹ççå­¸æ¸æçä¸ç¨®å¼·å¤§æ¹æ³ï¼å©ç¨èªæç£ç£å­¸ç¿å¾æªæ¨è¨çå¨å¹»ççå½±åä¸­æåç©©å¥çç¹è²è¡¨å¾µãéäºæ¨¡ååçºå®æ¨¡æåå¤æ¨¡ææ¡æ¶ï¼å·²è­æææèªååè¤éçççä»»åï¼ä¾å¦åå²ãåé¡åçç©æ¨è¨ç¼ç¾ãç¶èï¼CPathFM çéç¼æåºäºéå¤§ææ°ï¼ä¾å¦æ¸æå¯åæ§æéãæ¸æéä¹éè®ç°æ§é«ãéè¦ç¹å®é åçé©ææ§ï¼ä»¥åç¼ºä¹æ¨æºåçè©ä¼°åºæºãéé èª¿æ¥å°è¨ç®ççå­¸ä¸­ç CPathFM é²è¡äºå¨é¢çåé¡§ï¼éé»éæ³¨æ¸æéãé©æç­ç¥åè©ä¼°ä»»åãæååæäºå°æ¯å­¸ç¿åå¤æ¨¡ææ´åç­ééµæè¡ï¼ä¸¦å¼·èª¿äºç¶åç ç©¶ä¸­å­å¨çå·®è·ãæå¾ï¼æåå¾ååè§åº¦æ¢è¨äºæ¨é² CPathFM çæªä¾æ¹åãéé èª¿æ¥ä½çºç ç©¶äººå¡ãè¨åºé«çåäººå·¥æºè½å¾æ¥­äººå¡çå¯¶è²´è³æºï¼æå° CPathFM æèç©©å¥ä¸è¨åºä¸é©ç¨ç AI é©åççè§£æ±ºæ¹æ¡éé²ã

##### **Beyond Benchmarks: On The False Promise of AI Regulation**
2501.15693v1 by Gabriel Stanovsky, Renana Keydar, Gadi Perl, Eliya Habba

The rapid advancement of artificial intelligence (AI) systems in critical
domains like healthcare, justice, and social services has sparked numerous
regulatory initiatives aimed at ensuring their safe deployment. Current
regulatory frameworks, exemplified by recent US and EU efforts, primarily focus
on procedural guidelines while presuming that scientific benchmarking can
effectively validate AI safety, similar to how crash tests verify vehicle
safety or clinical trials validate drug efficacy. However, this approach
fundamentally misunderstands the unique technical challenges posed by modern AI
systems. Through systematic analysis of successful technology regulation case
studies, we demonstrate that effective scientific regulation requires a causal
theory linking observable test outcomes to future performance - for instance,
how a vehicle's crash resistance at one speed predicts its safety at lower
speeds. We show that deep learning models, which learn complex statistical
patterns from training data without explicit causal mechanisms, preclude such
guarantees. This limitation renders traditional regulatory approaches
inadequate for ensuring AI safety. Moving forward, we call for regulators to
reckon with this limitation, and propose a preliminary two-tiered regulatory
framework that acknowledges these constraints: mandating human oversight for
high-risk applications while developing appropriate risk communication
strategies for lower-risk uses. Our findings highlight the urgent need to
reconsider fundamental assumptions in AI regulation and suggest a concrete path
forward for policymakers and researchers.

æè¦ï¼äººå·¥æºæ§ (AI) ç³»çµ±å¨é«çä¿å¥ãå¸æ³åç¤¾ææåç­ééµé åçå¿«éé²å±ï¼å¼ç¼äºè¨±å¤æ³è¦å¡è­°ï¼æ¨å¨ç¢ºä¿å¶å®å¨é¨ç½²ãä»¥æè¿ç¾ååæ­ççåªåçºä¾ï¼ç¶åçæ³è¦æ¡æ¶ä¸»è¦éæ³¨ç¨åºæåï¼åæåè¨­ç§å­¸åºæºæ¸¬è©¦å¯ä»¥ææé©è­ AI å®å¨æ§ï¼é¡ä¼¼æ¼ç¢°ææ¸¬è©¦é©è­è»è¼å®å¨æ§æè¨åºè©¦é©é©è­è¥ç©åæãç¶èï¼éç¨®æ¹æ³æ ¹æ¬èª¤è§£äºç¾ä»£ AI ç³»çµ±å¸¶ä¾çç¨ç¹æè¡ææ°ãééå°æåçæè¡æ³è¦æ¡ä¾ç ç©¶é²è¡ç³»çµ±åæï¼æåè­æææçç§å­¸æ³è¦éè¦ä¸åå æçè«ï¼å°å¯è§å¯çæ¸¬è©¦çµæèæªä¾çæè½è¯ç¹«èµ·ä¾ââä¾å¦ï¼è»è¼å¨æä¸éåº¦ä¸çæææ§å¦ä½é æ¸¬å¶å¨è¼ä½éåº¦ä¸çå®å¨æ§ãæåè¡¨ææ·±åº¦å­¸ç¿æ¨¡åå¾è¨ç·´è³æä¸­å­¸ç¿è¤éççµ±è¨æ¨¡å¼ï¼èæ²ææç¢ºçå ææ©å¶ï¼æé¤äºéæ¨£çä¿è­ãéç¨®éå¶ä½¿å¾å³çµ±çæ³è¦æ¹æ³ä¸è¶³ä»¥ç¢ºä¿ AI å®å¨æ§ãå±ææªä¾ï¼æåå¼ç±²ç£ç®¡æ©æ§æ­£è¦éä¸éå¶ï¼ä¸¦æåºä¸ååæ­¥çå©å±¤æ³è¦æ¡æ¶ï¼æ¿èªéäºç´æï¼å°é«é¢¨éªæç¨å¼·å¶äººå·¥ç£ç£ï¼åæçºä½é¢¨éªç¨éå¶å®é©ç¶çé¢¨éªæºéç­ç¥ãæåçç¼ç¾å¼·èª¿äºéæ°èæ® AI æ³è¦ä¸­åºæ¬åè¨­çè¿«åéè¦ï¼ä¸¦çºæ¿ç­å¶å®èåç ç©¶äººå¡æåºäºå·é«çé²å±éè·¯ã

##### **Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**
2501.15572v1 by Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, Lorenzo Preda

Introduction: Generative Adversarial Networks (GANs) are increasingly used to
generate synthetic medical images, addressing the critical shortage of
annotated data for training Artificial Intelligence (AI) systems. This study
introduces a novel memory-efficient GAN architecture, incorporating Conditional
Random Fields (CRFs) to generate high-resolution 3D medical images and
evaluates its performance against the state-of-the-art hierarchical (HA)-GAN
model.
  Materials and Methods: The CRF-GAN was trained using the open-source lung CT
LUNA16 dataset. The architecture was compared to HA-GAN through a quantitative
evaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy
(MMD) metrics, and a qualitative evaluation, through a two-alternative forced
choice (2AFC) test completed by a pool of 12 resident radiologists, in order to
assess the realism of the generated images.
  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD
(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test
showed a significant preference for images generated by CRF-Gan over those
generated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN
demonstrated 9.34% lower memory usage at 256 resolution and achieved up to
14.6% faster training speeds, offering substantial computational savings.
  Discussion: CRF-GAN model successfully generates high-resolution 3D medical
images with non-inferior quality to conventional models, while being more
memory-efficient and faster. Computational power and time saved can be used to
improve the spatial resolution and anatomical accuracy of generated images,
which is still a critical factor limiting their direct clinical applicability.

æè¦ï¼<paragraph>å¼è¨ï¼çæå°æç¶²è·¯ (GAN) æä¾æå¸¸è¢«ç¨æ¼çæåæé«å­¸å½±åï¼ä»¥è§£æ±ºäººå·¥æºæ§ (AI) ç³»çµ±è¨ç·´ä¸­æ¨è¨»è³æå´éç­ç¼ºçåé¡ãæ¬ç ç©¶æåºäºä¸ç¨®æ°ç©çè¨æ¶é«é«æ GAN æ¶æ§ï¼çµåæ¢ä»¶é¨æ©å ´ (CRF) ä¾çæé«è§£æåº¦ 3D é«å­¸å½±åï¼ä¸¦è©ä¼°å¶ç¸å°æ¼æåé²çåå±¤ (HA)-GAN æ¨¡åçæè½ã
ææåæ¹æ³ï¼CRF-GAN ä½¿ç¨éæºèºé¨é»è¦æ·å±¤ææ LUNA16 è³æéé²è¡è¨ç·´ãééä½¿ç¨ FrÃ©chet Inception Distance (FID) å Maximum Mean Discrepancy (MMD) ææ¨é²è¡éåè©ä¼°ï¼ä»¥åç± 12 ä½ä½é¢æ¾å°ç§é«å¸«çµæçåéå®æçå©æä¸å¼·å¶é¸æ (2AFC) æ¸¬è©¦é²è¡å®æ§è©ä¼°ï¼å°è©²æ¶æ§è HA-GAN é²è¡æ¯è¼ï¼ä»¥è©ä¼°çæå½±åççå¯¦æ§ã
çµæï¼CRF-GAN ä»¥è¼ä½ç FID (0.047 å° 0.061) å MMD (0.084 å° 0.086) åæ¸åªæ¼ HA-GANï¼è¡¨ç¤ºå½±åä¿çåº¦è¼ä½³ã2AFC æ¸¬è©¦é¡¯ç¤ºï¼åè©¦èé¡¯èåå¥½ç± CRF-GAN çæçå½±åï¼èéç± HA-GAN çæçå½±åï¼p å¼çº 1.93e-05ãæ­¤å¤ï¼CRF-GAN å¨è§£æåº¦çº 256 æçè¨æ¶é«ä½¿ç¨ééä½äº 9.34%ï¼ä¸¦å°è¨ç·´éåº¦æåäº 14.6%ï¼æä¾äºå¤§éçéç®ç¯çã
è¨è«ï¼CRF-GAN æ¨¡åæåå°çæäºé«è§£æåº¦ 3D é«å­¸å½±åï¼å¶åè³ªä¸éæ¼å³çµ±æ¨¡åï¼åææ´çè¨æ¶é«ä¸éåº¦æ´å¿«ãç¯ççéç®è½ååæéå¯ç¨æ¼æåçæå½±åçç©ºéè§£æåº¦åè§£åç²¾ç¢ºåº¦ï¼éä»ç¶æ¯éå¶å¶ç´æ¥è¨åºæç¨æ§çééµå ç´ ã</paragraph>

##### **AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**
2501.15489v1 by Muhammad Aftab, Faisal Mehmood, Chengjuan Zhang, Alishba Nadeem, Zigang Dong, Yanan Jiang, Kangdongs Liu

Artificial intelligence (AI) has potential to revolutionize the field of
oncology by enhancing the precision of cancer diagnosis, optimizing treatment
strategies, and personalizing therapies for a variety of cancers. This review
examines the limitations of conventional diagnostic techniques and explores the
transformative role of AI in diagnosing and treating cancers such as lung,
breast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate,
and skin cancers. The primary objective of this paper is to highlight the
significant advancements that AI algorithms have brought to oncology within the
medical industry. By enabling early cancer detection, improving diagnostic
accuracy, and facilitating targeted treatment delivery, AI contributes to
substantial improvements in patient outcomes. The integration of AI in medical
imaging, genomic analysis, and pathology enhances diagnostic precision and
introduces a novel, less invasive approach to cancer screening. This not only
boosts the effectiveness of medical facilities but also reduces operational
costs. The study delves into the application of AI in radiomics for detailed
cancer characterization, predictive analytics for identifying associated risks,
and the development of algorithm-driven robots for immediate diagnosis.
Furthermore, it investigates the impact of AI on addressing healthcare
challenges, particularly in underserved and remote regions. The overarching
goal of this platform is to support the development of expert recommendations
and to provide universal, efficient diagnostic procedures. By reviewing
existing research and clinical studies, this paper underscores the pivotal role
of AI in improving the overall cancer care system. It emphasizes how AI-enabled
systems can enhance clinical decision-making and expand treatment options,
thereby underscoring the importance of AI in advancing precision oncology

æè¦ï¼äººå·¥æºè½ï¼AIï¼ææ½åééæåççè¨ºæ·çæºç¢ºæ§ãåªåæ²»çç­ç¥ï¼ä»¥åçºåç¨®ççæä¾åäººåçæ³ï¼ä¾å¾¹åºæ¹è®è«ç¤å­¸é åãæ¬ç¯è©è«æ¢è¨å³çµ±è¨ºæ·æè¡çéå¶ï¼ä¸¦æ¢è¨ AI å¨è¨ºæ·åæ²»çèºçãä¹³çãå¤§è¸ç´è¸çãèçãèçãé£éçãå­å®®é ¸çãç²çèºçãæè­·èºçåç®èçç­ççä¸­è½è®æ§çè§è²ãæ¬æçä¸»è¦ç®çæ¯å¼·èª¿ AI æ¼ç®æ³å¨é«çç¢æ¥­è«ç¤å­¸é åå¸¶ä¾çéå¤§é²å±ãééå¯¦ç¾æ©æççåµæ¸¬ãæåè¨ºæ·æºç¢ºæ§ï¼ä»¥åä¿é²æ¨é¶æ²»ççæä¾ï¼AI æå©æ¼å¤§å¹æ¹åçæ£çæ²»ççµæãAI æ´åå¨é«å­¸å½±åãåºå é«åæåççå­¸ä¸­ï¼æåäºè¨ºæ·çæºç¢ºæ§ï¼ä¸¦å¼é²äºä¸ç¨®åµæ°ãä¾µå¥æ§è¼ä½çæ¹æ³ä¾é²è¡ççç¯©æª¢ãéä¸åæåäºé«çæ©æ§çæçï¼ä¹éä½äºçéææ¬ãæ¬ç ç©¶æ¢è¨äº AI å¨æ¾å°ç·çµå­¸ä¸­çæç¨ï¼ä»¥é²è¡è©³ç´°çççç¹å¾µåæãé æ¸¬åæä»¥æ¾åºç¸éé¢¨éªï¼ä»¥åéç¼æ¼ç®æ³é©åçæ©å¨äººé²è¡ç«å³è¨ºæ·ãæ­¤å¤ï¼å®éæ¢è¨äº AI å¨è§£æ±ºé«çä¿å¥ææ°ä¸­çå½±é¿ï¼ç¹å¥æ¯å¨æåä¸è¶³ååé å°åãæ­¤å¹³å°çç¸½é«ç®æ¨æ¯æ¯æ´å°å®¶å»ºè­°çç¼å±ï¼ä¸¦æä¾æ®éãææçè¨ºæ·ç¨åºãééæª¢è¦ç¾æçç ç©¶åè¨åºè©¦é©ï¼æ¬æå¼·èª¿äº AI å¨æ¹åæ´é«ççç§è­·ç³»çµ±ä¸­çééµä½ç¨ãå®å¼·èª¿äº AI é©åçç³»çµ±å¦ä½è½æåè¨åºæ±ºç­å¶å®ï¼ä¸¦æ´å±æ²»çé¸é ï¼å¾èå¼·èª¿äº AI å¨æ¨é²ç²¾æºè«ç¤å­¸ä¸­çéè¦æ§ã

##### **Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**
2501.15452v1 by Solha Kang, Joris Vankerschaver, Utku Ozbulak

With the advancements in self-supervised learning (SSL), transformer-based
computer vision models have recently demonstrated superior results compared to
convolutional neural networks (CNNs) and are poised to dominate the field of
artificial intelligence (AI)-based medical imaging in the upcoming years.
Nevertheless, similar to CNNs, unveiling the decision-making process of
transformer-based models remains a challenge. In this work, we take a step
towards demystifying the decision-making process of transformer-based medical
imaging models and propose Token Insight, a novel method that identifies the
critical tokens that contribute to the prediction made by the model. Our method
relies on the principled approach of token discarding native to
transformer-based models, requires no additional module, and can be applied to
any transformer model. Using the proposed approach, we quantify the importance
of each token based on its contribution to the prediction and enable a more
nuanced understanding of the model's decisions. Our experimental results which
are showcased on the problem of colonic polyp identification using both
supervised and self-supervised pretrained vision transformers indicate that
Token Insight contributes to a more transparent and interpretable
transformer-based medical imaging model, fostering trust and facilitating
broader adoption in clinical settings.

æè¦ï¼é¨èèªç£ç£å­¸ç¿ (SSL) çé²å±ï¼åºæ¼ Transformer çé»è¦è¦è¦ºæ¨¡åæè¿å·²å±ç¤ºåºåªæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN) çåè¶ææï¼ä¸¦æºåå¨æªä¾å¹¾å¹´ä¸»å°åºæ¼äººå·¥æºæ§ (AI) çé«å­¸å½±åé åãåç®¡å¦æ­¤ï¼è CNN é¡ä¼¼ï¼æ­ç¤ºåºæ¼ Transformer çæ¨¡åçæ±ºç­éç¨ä»ç¶æ¯ä¸é ææ°ãå¨éé å·¥ä½ä¸­ï¼æåæèæ­éåºæ¼ Transformer çé«å­¸å½±åæ¨¡åçæ±ºç­éç¨éåºäºä¸æ­¥ï¼ä¸¦æåºäº Token Insightï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼å¯ä»¥è­å¥å°æ¨¡åæåçé æ¸¬æè²¢ç»çééµ Tokenãæåçæ¨¡åä¾è³´æ¼åºæ¼ Transformer çæ¨¡åçåç Token æ¨æ£åçåæ¹æ³ï¼ä¸éè¦é¡å¤çæ¨¡çµï¼ä¸¦ä¸å¯ä»¥æç¨æ¼ä»»ä½ Transformer æ¨¡åãä½¿ç¨ææåºçæ¹æ³ï¼æåæ ¹ææ¯å Token å°é æ¸¬çè²¢ç»éåå¶éè¦æ§ï¼ä¸¦è½æ´ç´°ç·»å°äºè§£æ¨¡åçæ±ºç­ãæåçå¯¦é©çµæå±ç¤ºå¨ä½¿ç¨æç£ç£åèªç£ç£é è¨ç·´è¦è¦º Transformer ççµè¸æ¯èè­å¥åé¡ä¸ï¼è¡¨æ Token Insight æå©æ¼å»ºç«æ´éæä¸å¯è§£éçåºæ¼ Transformer çé«å­¸å½±åæ¨¡åï¼å¹é¤ä¿¡ä»»ä¸¦ä¿é²å¨è¨åºç°å¢ä¸­çæ´å»£æ³æ¡ç¨ã

##### **An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**
2501.17181v1 by Arya Rahgozar, Pouria Mortezaagha, Jodi Edwards, Douglas Manuel, Jessie McGowen, Merrick Zwarenstein, Dean Fergusson, Andrea Tricco, Kelly Cobey, Margaret Sampson, Malcolm King, Dawn Richards, Alexandra Bodnaruc, David Moher

The Brain-Heart Interconnectome (BHI) combines neurology and cardiology but
is hindered by inefficiencies in evidence synthesis, poor adherence to quality
standards, and research waste. To address these challenges, we developed an
AI-driven system to enhance systematic reviews in the BHI domain. The system
integrates automated detection of Population, Intervention, Comparator,
Outcome, and Study design (PICOS), semantic search using vector embeddings,
graph-based querying, and topic modeling to identify redundancies and
underexplored areas. Core components include a Bi-LSTM model achieving 87%
accuracy for PICOS compliance, a study design classifier with 95.7% accuracy,
and Retrieval-Augmented Generation (RAG) with GPT-3.5, which outperformed GPT-4
for graph-based and topic-driven queries. The system provides real-time
updates, reducing research waste through a living database and offering an
interactive interface with dashboards and conversational AI. While initially
developed for BHI, the system's adaptable architecture enables its application
across various biomedical fields, supporting rigorous evidence synthesis,
efficient resource allocation, and informed clinical decision-making.

æè¦ï¼è¦å¿äº¤äºçµå­¸ (BHI) çµåç¥ç¶å­¸åå¿èå­¸ï¼ä½åå°è­æåææçä½ãå°åè³ªæ¨æºçéµå®åº¦ä¸ä½³åç ç©¶æµªè²»çé»ç¤ãçºäºæå°éäºææ°ï¼æåéç¼äºä¸å AI é©åç³»çµ±ï¼ä»¥å¢å¼· BHI é åä¸­çç³»çµ±æ§åé¡§ãæ­¤ç³»çµ±æ´åäºäººå£ãå¹²é ãæ¯è¼ãçµæåç ç©¶è¨­è¨ (PICOS) çèªåæª¢æ¸¬ï¼ä½¿ç¨åéåµå¥çèªç¾©æå°ãåºæ¼åå½¢æ¥è©¢åä¸»é¡å»ºæ¨¡ï¼ä»¥è­å¥åé¤åæªååæ¢ç´¢çé åãæ ¸å¿çµæåæ¬ä¸åéå LSTM æ¨¡åï¼PICOS åè¦æ§éå° 87% çæºç¢ºåº¦ï¼ç ç©¶è¨­è¨åé¡å¨æºç¢ºåº¦çº 95.7%ï¼ä»¥åä½¿ç¨ GPT-3.5 çæª¢ç´¢å¢å¼·çæ (RAG)ï¼å¶å¨åºæ¼åå½¢åä¸»é¡é©åçæ¥è©¢æ¹é¢åªæ¼ GPT-4ãè©²ç³»çµ±æä¾å³ææ´æ°ï¼ééåæè³æåº«æ¸å°ç ç©¶æµªè²»ï¼ä¸¦æä¾å·æåè¡¨æ¿åå°è©±å¼ AI çäºåä»é¢ãåç®¡æåæ¯çº BHI éç¼ï¼ä½è©²ç³»çµ±çå¯é©ææ¶æ§ä½¿å¶è½å¤ æç¨æ¼åç¨®çç©é«å­¸é åï¼æ¯æ´å´è¬¹çè­æåæãææçè³æºéç½®åææºçè¨åºæ±ºç­å¶å®ã

##### **Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**
2501.15056v1 by Harshita Chopra, Chirag Shah

The ability to identify and acquire missing information is a critical
component of effective decision making and problem solving. With the rise of
conversational artificial intelligence (AI) systems, strategically formulating
information-seeking questions becomes crucial and demands efficient methods to
guide the search process. We introduce a novel approach to adaptive
question-asking through a combination of Large Language Models (LLM) for
generating questions that maximize information gain, Monte Carlo Tree Search
(MCTS) for constructing and leveraging a decision tree across multiple samples,
and a hierarchical feedback mechanism to learn from past interactions. We
present two key innovations: (1) an adaptive MCTS algorithm that balances
exploration and exploitation for efficient search over potential questions; and
(2) a clustering-based feedback algorithm that leverages prior experience to
guide future interactions. Each incoming sample is assigned to a cluster based
on its semantic similarity with previously observed samples. Our UCT (Upper
Confidence bound for Trees) formulation selects optimal questions by combining
expected rewards, a function of information gain, with a cluster-specific bonus
that decays with depth, to emphasize the importance of early-stage questions
that have proven effective for narrowing the solution space in similar samples.
Experiments across three domains, including medical diagnosis and
troubleshooting, demonstrate that our method leads to an average of 12%
improvement in success rates and a 10x reduction in the average number of LLM
calls made per conversation for the search process, in comparison to the state
of the art.

æè¦ï¼å·åè¾¨è­èåå¾éºæ¼è³è¨çè½åæ¯æææ±ºç­ååé¡è§£æ±ºçééµçµæé¨åãé¨èå°è©±å¼äººå·¥æºæ§ (AI) ç³»çµ±çå´èµ·ï¼ç­ç¥æ§å°æ¬å®å°æ±è³è¨çåé¡è®å¾è³ééè¦ï¼ä¸¦éè¦ææççæ¹æ³ä¾å¼å°æå°æµç¨ãæåééçµåå¤§åèªè¨æ¨¡å (LLM) ä¾ç¢çæå¤§åè³è¨ç²åçåé¡ãèå°å¡ç¾æ¨¹çæå° (MCTS) ä¾å»ºæ§ä¸¦å©ç¨å¤åæ¨£æ¬çæ±ºç­æ¨¹ï¼ä»¥ååå±¤åé¥æ©å¶ä¾å¾éå»çäºåä¸­å­¸ç¿ï¼æåºäºä¸ç¨®èªé©ææåçæ°æ¹æ³ãæåæåºäºå©é ééµåµæ°ï¼(1) ä¸ç¨®èªé©æ MCTS æ¼ç®æ³ï¼ç¨æ¼å¹³è¡¡æ¢ç´¢åå©ç¨ï¼ä»¥æææå°æ½å¨åé¡ï¼ä»¥å (2) ä¸ç¨®åºæ¼èé¡çåé¥æ¼ç®æ³ï¼ç¨æ¼å©ç¨ååçç¶é©ä¾å¼å°æªä¾çäºåãæ¯åè¼¸å¥æ¨£æ¬ææ ¹æå¶èååè§å¯å°çæ¨£æ¬çèªç¾©ç¸ä¼¼æ§åéå°ä¸åç¾¤éãæåç UCT (æ¨¹ççµæ§çä¸ç½®ä¿¡ç) å¬å¼ééçµåé æçåé¥ï¼è³è¨ç²åå½æ¸ï¼åé¨èæ·±åº¦è¡°æ¸çç¹å®ç¾¤éå æï¼ä¾é¸ææä½³åé¡ï¼ä»¥å¼·èª¿æ©æéæ®µåé¡å°æ¼ç¸®å°é¡ä¼¼æ¨£æ¬ä¸­çè§£ç©ºéçéè¦æ§ãå¨åæ¬é«çè¨ºæ·åæéæé¤å¨å§çä¸åé åä¸­çå¯¦é©è­æï¼èç¾ææè¡ç¸æ¯ï¼æåçæè¡å¯å°æåçå¹³åæå 12%ï¼ä¸¦å°æå°æµç¨ä¸­æ¯æ¬¡å°è©±æé²è¡ç LLM å¼å«å¹³åæ¬¡æ¸æ¸å° 10 åã

##### **Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**
2501.14929v1 by Md. Kamrul Hasan, Guang Yang, Choon Hwai Yap

Cardiac anatomy segmentation is essential for clinical assessment of cardiac
function and disease diagnosis to inform treatment and intervention. In
performing segmentation, deep learning (DL) algorithms improved accuracy
significantly compared to traditional image processing approaches. More
recently, studies showed that enhancing DL segmentation with motion information
can further improve it. A range of methods for injecting motion information has
been proposed, but many of them increase the dimensionality of input images
(which is computationally expensive) or have not used an optimal method to
insert motion information, such as non-DL registration, non-attention-based
networks or single-headed attention. Here, we present a novel,
computation-efficient alternative where a novel, scalable temporal attention
module (TAM) extracts temporal feature interactions multiple times and where
TAM has a multi-headed, KQV projection cross-attention architecture. The module
can be seamlessly integrated into a wide range of existing CNN- or
Transformer-based networks, providing novel flexibility for inclusion in future
implementations. Extensive evaluations on different cardiac datasets, 2D
echocardiography (CAMUS), and 3D echocardiography (MITEA) demonstrate the
model's effectiveness when integrated into well-established backbone networks
like UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that
the optimized TAM-enhanced FCN8s network performs well compared to contemporary
alternatives. Our results confirm TAM's robustness, scalability, and
generalizability across diverse datasets and backbones.

æè¦ï¼<paragraph>å¿èè§£ååå²å°æ¼è©ä¼°å¿èåè½åç¾çè¨ºæ·ä»¥æä¾æ²»çåä»å¥è³è¨è³ééè¦ãå¨å·è¡åå²æï¼æ·±åº¦å­¸ç¿ (DL) æ¼ç®æ³èå³çµ±å½±åèçæ¹æ³ç¸æ¯ï¼æºç¢ºåº¦é¡¯èæåãæè¿çç ç©¶é¡¯ç¤ºï¼ééåä½è³è¨å¼·å DL åå²å¯ä»¥é²ä¸æ­¥æååå²ææãå·²ç¶æåºè¨±å¤ç¨æ¼æ³¨å¥åä½è³è¨çæ¹æ³ï¼ä½å¶ä¸­è¨±å¤æ¹æ³æå¢å è¼¸å¥å½±åçç¶­åº¦ï¼å¨éç®ä¸å¾èè²»ææ¬ï¼ï¼ææªä½¿ç¨æä½³æ¹æ³ä¾æå¥åä½è³è¨ï¼ä¾å¦é DL éæºãéåºæ¼æ³¨æåçç¶²è·¯æå®é ­æ³¨æåãå¨æ­¤ï¼æåæåºä¸åæ°ç©ä¸éç®æçé«çæ¿ä»£æ¹æ¡ï¼å¶ä¸­ä¸åæ°ç©ä¸å¯æ´åçæåºæ³¨æåæ¨¡çµ (TAM) å¤æ¬¡æåæåºç¹å¾µäºåï¼ä¸ TAM å·æå¤é ­ãKQV æå½±äº¤åæ³¨æåçæ¶æ§ãæ­¤æ¨¡çµå¯ä»¥ç¡ç¸«æ´åå°åç¨®ç¾æçåºæ¼ CNN æ Transformer çç¶²è·¯ä¸­ï¼çºæªä¾å¯¦ä½çç´å¥æä¾æ°ç©çéæ´»æ§ãå¨ä¸åçå¿èè³æéã2D è¶é³æ³¢å¿åå (CAMUS) å 3D è¶é³æ³¢å¿åå (MITEA) ä¸é²è¡çå»£æ³è©ä¼°è­æäºæ­¤æ¨¡åå¨æ´åå° UNetãFCN8sãUNetRãSwinUNetR åæè¿ç I2UNet ç­å®åçä¸»å¹¹ç¶²è·¯ä¸­çæè½ãæåé²ä¸æ­¥ç¼ç¾ï¼ç¶éæä½³åç TAM å¢å¼· FCN8s ç¶²è·¯èç¶ä»£æ¿ä»£æ¹æ¡ç¸æ¯è¡¨ç¾è¯å¥½ãæåççµæè­å¯¦äº TAM å¨ä¸åè³æéåä¸»å¹¹ä¸­çç©©å¥æ§ãå¯æ´åæ§åæ³åæ§ã</paragraph>

##### **Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**
2501.14892v1 by Hang Luo, Jian Zhang, Chujun Li

In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.

æè¦ï¼å¨ç¥è­å¯éåä»»åä¸­ï¼ç¹å¥æ¯å¨é«å­¸åæ³å¾ç­é«é¢¨éªé åï¼ä¸åæª¢ç´¢ç¸éè³è¨è³ééè¦ï¼éå¿é æä¾å ææ¨çåå¯è§£éæ§ãå¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨çè§£åçæä»»åä¸­åå¾äºé¡¯èçè¡¨ç¾ãç¶èï¼å®åéå¸¸æéå°ä¸äºéå¶ï¼ä¾å¦é£ä»¥ç´å¥æ°ç¥è­ãç¢çå¹»è¦ºï¼ä»¥åè§£éå¶æ¨çéç¨ãçºäºæå°éäºææ°ï¼å°ç¥è­åèåå½¢æª¢ç´¢å¢å¼·çæ (Graph RAG) æ´åå¨ä¸èµ·å·²æçºä¸ç¨®ææçè§£æ±ºæ¹æ¡ãå³çµ±ç Graph RAG æ¹æ³éå¸¸ä¾è³´æ¼ç°¡å®çåå½¢éæ­·æèªç¾©ç¸ä¼¼æ§ï¼éç¡æ³ææå æéä¿æèæ¨¡åçå§é¨æ¨çæ­¥é©å¾å¥½å°å°é½ãæ¬ææåºäºä¸åæ°ç©çç®¡éï¼è©²ç®¡ééæ¿¾å¤§åç¥è­åä»¥å¼·èª¿å æéç·£ï¼å°æª¢ç´¢éç¨èæ¨¡åçææ³é (CoT) å°é½ï¼ä¸¦ééå¤éæ®µè·¯å¾æ¹é²ä¾å¢å¼·æ¨çãå¨é«çåé¡è§£ç­ä»»åä¸çå¯¦é©é¡¯ç¤ºåºä¸è´çæ¶çï¼å¨å¤åå¤§åèªè¨æ¨¡å (LLM) ä¸­çµå°æ¹é²å¹åº¦é«é 10%ãéç¨®æ¹æ³å±ç¤ºäºå°å ææ¨çèéæ­¥æª¢ç´¢ç¸çµåçå¹å¼ï¼å¾èçºè¤éæ¥è©¢æä¾æ´å·å¯è§£éæ§åéè¼¯ä¾æçè§£æ±ºæ¹æ¡ã

##### **Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**
2501.14719v1 by Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso

Equitable access to reliable health information is vital for public health,
but the quality of online health resources varies by language, raising concerns
about inconsistencies in Large Language Models (LLMs) for healthcare. In this
study, we examine the consistency of responses provided by LLMs to
health-related questions across English, German, Turkish, and Chinese. We
largely expand the HealthFC dataset by categorizing health-related questions by
disease type and broadening its multilingual scope with Turkish and Chinese
translations. We reveal significant inconsistencies in responses that could
spread healthcare misinformation. Our main contributions are 1) a multilingual
health-related inquiry dataset with meta-information on disease categories, and
2) a novel prompt-based evaluation workflow that enables sub-dimensional
comparisons between two languages through parsing. Our findings highlight key
challenges in deploying LLM-based tools in multilingual contexts and emphasize
the need for improved cross-lingual alignment to ensure accurate and equitable
healthcare information.

æè¦ï¼å¯é çå¥åº·è³è¨çå¬å¹³åå¾å°å¬å±è¡çè³ééè¦ï¼
ä½ç¶²è·¯å¥åº·è³æºçåè³ªå èªè¨èç°ï¼éå¼ç¼äºå°å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æ¹é¢çä¸ä¸è´æ§çææãå¨éé ç ç©¶ä¸­ï¼æåæ¢è¨äº LLM å°è±èªãå¾·èªãåè³å¶èªåä¸­æçå¥åº·ç¸éåé¡ææä¾åæçä¸è´æ§ãæåééä¾ç¾çé¡ååé¡å¥åº·ç¸éåé¡ï¼ä¸¦ééåè³å¶èªåä¸­æç¿»è­¯æ´å±å¶å¤èªè¨ç¯åï¼å¤§å¹æ´å±äº HealthFC è³æéãæåæ­é²äºåæä¸­å­å¨é¡¯èçä¸ä¸è´æ§ï¼éå¯è½ææ£å¸é«çä¿å¥é¯èª¤è³è¨ãæåçè²¢ç»ä¸»è¦æ 1) ä¸ååå«ç¾çé¡å¥åè³è¨çå¤èªè¨å¥åº·ç¸éæ¥è©¢è³æéï¼ä»¥å 2) ä¸åæ°ç©çæç¤ºå¼è©ä¼°å·¥ä½æµç¨ï¼å®è½ééè§£æå¨å©ç¨®èªè¨ä¹éé²è¡æ¬¡ç¶­åº¦æ¯è¼ãæåçç ç©¶çµæçªé¡¯äºå¨å¤èªè¨ç°å¢ä¸­é¨ç½²åºæ¼ LLM çå·¥å·çä¸»è¦ææ°ï¼ä¸¦å¼·èª¿éè¦æ¹åè·¨èªè¨å°é½ä»¥ç¢ºä¿æºç¢ºä¸å¬å¹³çé«çä¿å¥è³è¨ã

##### **GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**
2501.16382v1 by Ziwen Li, Xiang 'Anthony' Chen, Youngseung Jeon

Drug discovery (DD) has tremendously contributed to maintaining and improving
public health. Hypothesizing that inhibiting protein misfolding can slow
disease progression, researchers focus on target identification (Target ID) to
find protein structures for drug binding. While Large Language Models (LLMs)
and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug
discovery, integrating models into cohesive workflows remains challenging. We
conducted a user study with drug discovery researchers to identify the
applicability of LLMs and RAGs in Target ID. We identified two main findings:
1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on
an initial protein and protein candidates that have a therapeutic impact; 2)
the model must provide the PPI and relevant explanations for better
understanding. Based on these observations, we identified three limitations in
previous approaches for Target ID: 1) semantic ambiguity, 2) lack of
explainability, and 3) short retrieval units. To address these issues, we
propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve
agent pipeline RAG framework to support large-scale PPI signaling pathway
exploration in understanding therapeutic impacts by decomposing the analysis of
entire PPI pathways into sub-tasks focused on the analysis of PPI edges.

æè¦ï¼è¯ç©åç° (DD) æå¤§å°ä¿è¿äºå¬å±å«ççç»´æ¤åæ¹åãç ç©¶äººååè®¾æå¶èç½è´¨éè¯¯æå å¯ä»¥åç¼ç¾çè¿å±ï¼å æ­¤ä¸æ³¨äºé¶ç¹è¯å« (Target ID) ä»¥æ¾å°ç¨äºè¯ç©ç»åçèç½è´¨ç»æãè½ç¶å¤§åè¯­è¨æ¨¡å (LLM) åæ£ç´¢å¢å¼ºçæ (RAG) æ¡æ¶å éäºè¯ç©åç°ï¼ä½å°æ¨¡åæ´åå°åèå·¥ä½æµä¸­ä»ç¶å·ææææ§ãæä»¬ä¸è¯ç©åç°ç ç©¶äººåè¿è¡äºä¸é¡¹ç¨æ·ç ç©¶ï¼ä»¥ç¡®å® LLM å RAG å¨ Target ID ä¸­çéç¨æ§ãæä»¬ç¡®å®äºä¸¤ä¸ªä¸»è¦åç°ï¼1) LLM åºè¯¥åºäºåå§èç½è´¨åå·ææ²»çä½ç¨çèç½è´¨åéç©æä¾å¤ä¸ªèç½è´¨-èç½è´¨ç¸äºä½ç¨ (PPI)ï¼2) è¯¥æ¨¡åå¿é¡»æä¾ PPI åç¸å³è§£éä»¥æ´å¥½å°çè§£ãåºäºè¿äºè§å¯ï¼æä»¬åç°äºåå Target ID æ¹æ³ä¸­çä¸ä¸ªå±éæ§ï¼1) è¯­ä¹æ­§ä¹ï¼2) ç¼ºä¹å¯è§£éæ§ï¼3) æ£ç´¢ååç­ãä¸ºäºè§£å³è¿äºé®é¢ï¼æä»¬æåºäº GraPPIï¼è¿æ¯ä¸ç§åºäºå¤§è§æ¨¡ç¥è¯å¾ (KG) çæ£ç´¢-åè§£-æ±è§£ä»£çç®¡é RAG æ¡æ¶ï¼ä»¥æ¯æå¤§è§æ¨¡ PPI ä¿¡å·éè·¯æ¢ç´¢ï¼éè¿å°æ´ä¸ª PPI éè·¯çåæåè§£ä¸ºä¸æ³¨äº PPI è¾¹ç¼åæçå­ä»»å¡æ¥çè§£æ²»çå½±åã

##### **Rethinking Table Instruction Tuning**
2501.14693v1 by Naihao Deng, Rada Mihalcea

Recent advances in table understanding have focused on instruction-tuning
large language models (LLMs) for table-related tasks. However, existing
research has overlooked the impact of hyperparameter choices and lacks a
comprehensive evaluation of the out-of-domain table understanding ability and
the general capabilities of these table LLMs. In this paper, we evaluate these
abilities in existing table LLMs, and reveal significant declines in both
out-of-domain table understanding and general capabilities compared to their
base models. Through systematic analysis, we show that hyperparameters, such as
learning rate, can significantly influence both table-specific and general
capabilities. Contrary to the existing table instruction-tuning works, we
demonstrate that smaller learning rates and fewer training instances can
enhance table understanding while preserving general capabilities. Based on our
findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B
Instruct, which achieves performance on par with, or surpassing GPT-3.5 and
GPT-4 on table tasks, while maintaining strong out-of-domain generalization and
general capabilities. Our findings highlight the potential for reduced data
annotation costs and more efficient model development through careful
hyperparameter selection.

æè¦ï¼æè¿è¡¨çè§£çé²å±éä¸­å¨æä»¤èª¿æ ¡å¤§åèªè¨æ¨¡å (LLM) ä»¥å·è¡èè¡¨æ ¼ç¸éçä»»åãç¶èï¼ç¾æçç ç©¶å¿½ç¥äºè¶åæ¸é¸æçå½±é¿ï¼ä¸¦ä¸ç¼ºä¹å°é åå¤è¡¨æ ¼çè§£è½ååéäºè¡¨æ ¼ LLM çä¸è¬è½åçå¨é¢è©ä¼°ãå¨æ¬æä¸­ï¼æåè©ä¼°äºç¾æè¡¨æ ¼ LLM ä¸­çéäºè½åï¼ä¸¦æ­ç¤ºäºèå¶åºç¤æ¨¡åç¸æ¯ï¼é åå¤è¡¨æ ¼çè§£åä¸è¬è½åé½æé¡¯èä¸éãééç³»çµ±åæï¼æåè¡¨æè¶åæ¸ï¼ä¾å¦å­¸ç¿çï¼å¯ä»¥é¡¯èå½±é¿ç¹å®è¡¨æ ¼åä¸è¬è½åãèç¾æè¡¨æ ¼æä»¤èª¿æ ¡å·¥ä½ç¸åï¼æåè­æè¼å°çå­¸ç¿çåè¼å°çè¨ç·´å¯¦ä¾å¯ä»¥å¨ä¿çä¸è¬è½åçåæå¢å¼·è¡¨æ ¼çè§£ãæ ¹ææåçç¼ç¾ï¼æåå¼å¥äº TAMAï¼éæ¯ä¸åå¾ LLaMA 3.1 8B Instruct èª¿æ ¡çè¡¨æ ¼ LLMï¼å®å¨è¡¨æ ¼ä»»åä¸å¯¦ç¾äºè GPT-3.5 å GPT-4 ç¸ç¶æè¶è¶çæè½ï¼åæä¿æå¼·å¤§çé åå¤æ¦ååä¸è¬è½åãæåçç¼ç¾å¼·èª¿äºééä»ç´°é¸æè¶åæ¸ï¼éä½è³ææ¨è¨»ææ¬åæ´ææççæ¨¡åéç¼çå¯è½æ§ã

##### **Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**
2501.14689v1 by Dmitry Ryabtsev, Boris Vasilyev, Sergey Shershakov

This paper introduces an innovative software system for fundus image analysis
that deliberately diverges from the conventional screening approach, opting not
to predict specific diagnoses. Instead, our methodology mimics the diagnostic
process by thoroughly analyzing both normal and pathological features of fundus
structures, leaving the ultimate decision-making authority in the hands of
healthcare professionals. Our initiative addresses the need for objective
clinical analysis and seeks to automate and enhance the clinical workflow of
fundus image examination. The system, from its overarching architecture to the
modular analysis design powered by artificial intelligence (AI) models, aligns
seamlessly with ophthalmological practices. Our unique approach utilizes a
combination of state-of-the-art deep learning methods and traditional computer
vision algorithms to provide a comprehensive and nuanced analysis of fundus
structures. We present a distinctive methodology for designing medical
applications, using our system as an illustrative example. Comprehensive
verification and validation results demonstrate the efficacy of our approach in
revolutionizing fundus image analysis, with potential applications across
various medical domains.

æè¦ï¼æ¬è«æä»ç´¹äºä¸ç¨®åµæ°çè»é«ç³»çµ±ï¼ç¨æ¼ç¼åºå½±ååæï¼å®å»æåé¢å³çµ±çç¯©æª¢æ¹æ³ï¼é¸æä¸é æ¸¬å·é«çè¨ºæ·ãç¸åå°ï¼æåçåææ¹æ³æ¨¡æ¬è¨ºæ·éç¨ï¼å¾¹åºåæç¼åºçµæ§çæ­£å¸¸åççç¹å¾µï¼å°æçµçæ±ºç­æ¬äº¤å°é«çä¿å¥å°æ¥­äººå¡æä¸­ãæåçè¨ç«æ¨å¨æ»¿è¶³å®¢è§è¨åºåæçéæ±ï¼ä¸¦å°æ±èªåååå¼·åç¼åºå½±åæª¢æ¥çè¨åºå·¥ä½æµç¨ãè©²ç³»çµ±å¾å¶æ´é«æ¶æ§å°ç±äººå·¥æºæ§ (AI) æ¨¡åé©åçæ¨¡çµååæè¨­è¨ï¼é½èç¼ç§å¯¦åç¡ç¸«å°é½ãæåç¨ç¹çæ¹æ³çµåäºæåé²çæ·±åº¦å­¸ç¿æ¹æ³åå³çµ±çé»è¦è¦è¦ºæ¼ç®æ³ï¼æä¾ç¼åºçµæ§çå¨é¢ä¸ç´°ç·»çåæãæåæåºäºä¸ç¨®ç¨ç¹çè¨­è¨é«çæç¨æ¹æ³ï¼ä¸¦ä»¥æåçç³»çµ±ä½çºèªªæç¯ä¾ãå¨é¢çé©è­åé©è­çµæè­æäºæåçæ¹æ³å¨é©æ°ç¼åºå½±ååææ¹é¢çæåï¼ä¸¦å·æå¨åç¨®é«çé åçæ½å¨æç¨ã

##### **Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**
2501.14685v1 by Fuping Wu, Bartlomiej W. Papiez

Foundation models are widely employed in medical image analysis, due to their
high adaptability and generalizability for downstream tasks. With the
increasing number of foundation models being released, model selection has
become an important issue. In this work, we study the capabilities of
foundation models in medical image classification tasks by conducting a
benchmark study on the MedMNIST dataset. Specifically, we adopt various
foundation models ranging from convolutional to Transformer-based models and
implement both end-to-end training and linear probing for all classification
tasks. The results demonstrate the significant potential of these pre-trained
models when transferred for medical image classification. We further conduct
experiments with different image sizes and various sizes of training data. By
analyzing all the results, we provide preliminary, yet useful insights and
conclusions on this topic.

æè¦ï¼åºç¤æ¨¡åå»£æ³ç¨æ¼é«å­¸å½±ååæï¼å çºå®åå°ä¸æ¸¸ä»»åå·æé«åº¦çé©ææ§åæ¦æ¬æ§ãé¨èç¼å¸çåºç¤æ¨¡åæ¸éè¶ä¾è¶å¤ï¼æ¨¡åé¸æå·²æçºä¸åéè¦åé¡ãå¨éé å·¥ä½ä¸­ï¼æåééå° MedMNIST è³æéé²è¡åºæºç ç©¶ä¾ç ç©¶åºç¤æ¨¡åå¨é«å­¸å½±ååé¡ä»»åä¸­çè½åãå·é«ä¾èªªï¼æåæ¡ç¨äºå¾å·ç©å°åºæ¼ Transformer çæ¨¡åç­åç¨®åºç¤æ¨¡åï¼ä¸¦å°ææåé¡ä»»åå¯¦æ½ç«¯å°ç«¯è¨ç·´åç·æ§æ¢æ¸¬ãçµæè­æäºéäºé è¨ç·´æ¨¡åå¨è½ç§»å°é«å­¸å½±ååé¡æå·æé¡¯èçæ½åãæåé²ä¸æ­¥é²è¡äºä¸åå½±åå¤§å°ååç¨®è¨ç·´è³æå¤§å°çå¯¦é©ãééåæææçµæï¼æåå°æ­¤ä¸»é¡æä¾äºåæ­¥ä½æç¨çè¦è§£åçµè«ã

##### **MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**
2501.14654v1 by Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen

Recent large language models (LLMs) have demonstrated significant
advancements, particularly in their ability to serve as agents thereby
surpassing their traditional role as chatbots. These agents can leverage their
planning and tool utilization capabilities to address tasks specified at a high
level. However, a standardized dataset to benchmark the agent capabilities of
LLMs in medical applications is currently lacking, making the evaluation of
LLMs on complex tasks in interactive healthcare environments challenging. To
address this gap, we introduce MedAgentBench, a broad evaluation suite designed
to assess the agent capabilities of large language models within medical
records contexts. MedAgentBench encompasses 100 patient-specific
clinically-derived tasks from 10 categories written by human physicians,
realistic profiles of 100 patients with over 700,000 data elements, a
FHIR-compliant interactive environment, and an accompanying codebase. The
environment uses the standard APIs and communication infrastructure used in
modern EMR systems, so it can be easily migrated into live EMR systems.
MedAgentBench presents an unsaturated agent-oriented benchmark that current
state-of-the-art LLMs exhibit some ability to succeed at. The best model
(GPT-4o) achieves a success rate of 72%. However, there is still substantial
space for improvement to give the community a next direction to optimize.
Furthermore, there is significant variation in performance across task
categories. MedAgentBench establishes this and is publicly available at
https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable
framework for model developers to track progress and drive continuous
improvements in the agent capabilities of large language models within the
medical domain.

æè¦ï¼<paragraph>æè¿çå¤§åè¯­è¨æ¨¡å (LLM) å·²å±ç¤ºåºæ¾èçè¿æ­¥ï¼ç¹å«æ¯å¨å¶ä½ä¸ºä»£ççè½åæ¹é¢ï¼ä»èè¶è¶äºå¶ä½ä¸ºèå¤©æºå¨äººçä¼ ç»è§è²ãè¿äºä»£çå¯ä»¥å©ç¨å¶è§ååå·¥å·å©ç¨è½åæ¥è§£å³å¨é«å±æå®çä»»å¡ãç¶èï¼ç®åç¼ºä¹ç¨äºå¯¹å»çåºç¨ä¸­ LLM çä»£çè½åè¿è¡åºåæµè¯çæ ååæ°æ®éï¼è¿ä½¿å¾å¨äº¤äºå¼å»çä¿å¥ç¯å¢ä¸­å¯¹ LLM å¨å¤æä»»å¡ä¸çè¯ä¼°å·ææææ§ãä¸ºäºè§£å³è¿ä¸å·®è·ï¼æä»¬å¼å¥äº MedAgentBenchï¼è¿æ¯ä¸ä¸ªå¹¿æ³çè¯ä¼°å¥ä»¶ï¼æ¨å¨è¯ä¼°å¤§åè¯­è¨æ¨¡åå¨å»çè®°å½èæ¯ä¸çä»£çè½åãMedAgentBench åå« 100 ä¸ªç±äººç±»å»çç¼åçæ¥èª 10 ä¸ªç±»å«çç¹å®äºæ£èçä¸´åºä»»å¡ã100 ä¸ªæ£èççå®ä¸ªäººèµæï¼åå«è¶è¿ 700,000 ä¸ªæ°æ®åç´ ï¼ãä¸ä¸ªç¬¦å FHIR çäº¤äºå¼ç¯å¢ä»¥åä¸ä¸ªéå¥çä»£ç åºãè¯¥ç¯å¢ä½¿ç¨ç°ä»£ EMR ç³»ç»ä¸­ä½¿ç¨çæ å API åéä¿¡åºç¡è®¾æ½ï¼å æ­¤å¯ä»¥è½»æ¾å°è¿ç§»å°å®æ¶ EMR ç³»ç»ä¸­ãMedAgentBench åç°äºä¸ä¸ªæªé¥±åçä»¥ä»£çä¸ºå¯¼åçåºåï¼å½åæåè¿ç LLM è¡¨ç°åºä¸å®ç¨åº¦çæåè½åãæå¥½çæ¨¡å (GPT-4o) çæåçè¾¾å° 72%ãç¶èï¼ä»ç¶æå¾å¤§çæ¹è¿ç©ºé´ï¼å¯ä»¥ä¸ºç¤¾åºæä¾ä¼åæ¹åãæ­¤å¤ï¼ä¸åä»»å¡ç±»å«ä¹é´çæ§è½å·®å¼å¾å¤§ãMedAgentBench å»ºç«äºè¿ä¸ç¹ï¼å¹¶å¨ https://github.com/stanfordmlgroup/MedAgentBench å¬å¼æä¾ï¼ä¸ºæ¨¡åå¼åèæä¾äºä¸ä¸ªæä»·å¼çæ¡æ¶ï¼ç¨äºè·è¸ªè¿åº¦å¹¶æ¨å¨å¤§åè¯­è¨æ¨¡åå¨å»çé¢åçä»£çè½åçæç»­æ¹è¿ã</paragraph>

##### **Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**
2501.18614v1 by Xu Chen, Yuan Huang, Benn Jessney, Jason Sangha, Sophie Gu, Carola-Bibiane SchÃ¶nlieb, Martin Bennett, Michael Roberts

Artificial intelligence (AI) methodologies hold great promise for the rapid
and accurate diagnosis of coronary artery disease (CAD) from intravascular
optical coherent tomography (IVOCT) images. Numerous papers have been published
describing AI-based models for different diagnostic tasks, yet it remains
unclear which models have potential clinical utility and have been properly
validated. This systematic review considered published literature between
January 2015 and February 2023 describing AI-based diagnosis of CAD using
IVOCT. Our search identified 5,576 studies, with 513 included after initial
screening and 35 studies included in the final systematic review after quality
screening. Our findings indicate that most of the identified models are not
currently suitable for clinical use, primarily due to methodological flaws and
underlying biases. To address these issues, we provide recommendations to
improve model quality and research practices to enhance the development of
clinically useful AI products.

æè¦ï¼äººå·¥æºæ§ (AI) æ¹æ³è«å°æ¼å¾è¡ç®¡å§åå­¸ç¸å¹²æ·å±¤ææ (IVOCT) å½±åå¿«éä¸æºç¢ºè¨ºæ·å çåèç¾ç (CAD) èè¨ï¼å·æå¾å¤§çåæ¯ãè¨±å¤è«æå·²ç¼è¡¨ï¼èªªæäºç¨æ¼ä¸åè¨ºæ·ä»»åç AI åºç¤æ¨¡åï¼ä½ä»ä¸æ¸æ¥åªäºæ¨¡åå·ææ½å¨çè¨åºç¨éï¼ä¸å·²ç²å¾é©ç¶é©è­ãéé ç³»çµ±æ§åé¡§èéäº 2015 å¹´ 1 æè³ 2023 å¹´ 2 æéç¼è¡¨çæç»ï¼èªªæäºä½¿ç¨ IVOCT ç CAD AI åºç¤è¨ºæ·ãæåçæå°è¾¨è­åº 5,576 é ç ç©¶ï¼å¨åæ­¥ç¯©é¸å¾ç´å¥ 513 é ï¼å¨åè³ªç¯©é¸å¾ç´å¥ 35 é ç ç©¶é²è¡æçµçç³»çµ±æ§åé¡§ãæåçç¼ç¾é¡¯ç¤ºï¼å¤§å¤æ¸å·²è¾¨è­åºçæ¨¡åç®åä¸é©åç¨æ¼è¨åºï¼ä¸»è¦æ¯å çºæ¹æ³è«ä¸çç¼ºé·åæ½å¨åå·®ãçºäºè§£æ±ºéäºåé¡ï¼æåæä¾å»ºè­°ä¾æ¹åæ¨¡ååè³ªåç ç©¶å¯¦åï¼ä»¥å¢å¼·è¨åºæç¨ AI ç¢åçéç¼ã

##### **Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**
2501.14483v1 by Walid Yassine, Martin Charachon, CÃ©line Hudelot, Roberto Ardon

Assessing cancer progression in liver CT scans is a clinical challenge,
requiring a comparison of scans at different times for the same patient.
Practitioners must identify existing tumors, compare them with prior exams,
identify new tumors, and evaluate overall disease evolution. This process is
particularly complex in liver examinations due to misalignment between exams
caused by several factors. Indeed, longitudinal liver examinations can undergo
different non-pathological and pathological changes due to non-rigid
deformations, the appearance or disappearance of pathologies, and other
variations. In such cases, existing registration approaches, mainly based on
intrinsic features may distort tumor regions, biasing the tumor progress
evaluation step and the corresponding diagnosis. This work proposes a
registration method based only on geometrical and anatomical information from
liver segmentation, aimed at aligning longitudinal liver images for aided
diagnosis. The proposed method is trained and tested on longitudinal liver CT
scans, with 317 patients for training and 53 for testing. Our experimental
results support our claims by showing that our method is better than other
registration techniques by providing a smoother deformation while preserving
the tumor burden (total volume of tissues considered as tumor) within the
volume. Qualitative results emphasize the importance of smooth deformations in
preserving tumor appearance.

æè¦ï¼è©ä¼°èèé»è¦æ·å±¤ææä¸­çççé²ç¨æ¯ä¸é è¨åºä¸çææ°ï¼
éè¦æ¯è¼åä¸çæ£å¨ä¸åæéé»çææçµæã
å¾æ¥­äººå¡å¿é è¾¨è­ç¾æçè«ç¤ï¼å°å¶èååçæª¢æ¥çµæé²è¡æ¯è¼ï¼
è¾¨è­æ°çè«ç¤ï¼ä¸¦è©ä¼°æ´é«ç¾ççæ¼è®ãç±æ¼ç¨®ç¨®å ç´ é ææª¢æ¥çµæä¹éçé¯ä½ï¼éåéç¨å¨èèæª¢æ¥ä¸­ç¹å¥è¤éãäºå¯¦ä¸ï¼ç¸±åçèèæª¢æ¥å¯è½æå çºéåæ§è®å½¢ãçççåºç¾ææ¶å¤±ï¼ä»¥åå¶ä»è®åèç¢çä¸åçéççæ§åççæ§çè®åãå¨éç¨®ææ³ä¸ï¼ç¾æçéæºæ¹æ³ï¼ä¸»è¦åºæ¼å§å¨ç¹å¾µï¼å¯è½ææ­æ²è«ç¤ååï¼é æè«ç¤é²ç¨è©ä¼°æ­¥é©åç¸æè¨ºæ·çåå·®ãæ¬ç ç©¶æåºäºä¸ç¨®ååºæ¼èèåå²çå¹¾ä½åè§£åè³è¨çéæºæ¹æ³ï¼æ¨å¨å°ç¸±åèèå½±åé²è¡éæºï¼ä»¥åå©è¨ºæ·ãææåºçæ¹æ³å¨ç¸±åèèé»è¦æ·å±¤ææä¸é²è¡è¨ç·´åæ¸¬è©¦ï¼è¨ç·´è³ææ 317 ä½çæ£ï¼æ¸¬è©¦è³ææ 53 ä½ãæåçå¯¦é©çµææ¯ææåçèªªæ³ï¼è­ææåçéæºæ¹æ³æ¯å¶ä»éæºæè¡æ´å¥½ï¼å çºå®å¨ä¿çè«ç¤è² æï¼è¢«è¦çºè«ç¤ççµç¹ç¸½é«ç©ï¼çåæï¼æä¾äºæ´å¹³æ»çè®å½¢ãå®æ§çµæå¼·èª¿äºå¹³æ»è®å½¢å¨ä¿çè«ç¤å¤è§æ¹é¢çéè¦æ§ã

##### **Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**
2501.14469v1 by Taehan Kim, Wonduk Seo

Global climate change has reduced crop resilience and pesticide efficacy,
making reliance on synthetic pesticides inevitable, even though their
widespread use poses significant health and environmental risks. While these
pesticides remain a key tool in pest management, previous machine-learning
applications in pesticide and agriculture have focused on classification or
regression, leaving the fundamental challenge of generating new molecular
structures or designing novel candidates unaddressed. In this paper, we propose
Pesti-Gen, a novel generative model based on variational auto-encoders,
designed to create pesticide candidates with optimized properties for the first
time. Specifically, Pesti-Gen leverages a two-stage learning process: an
initial pre-training phase that captures a generalized chemical structure
representation, followed by a fine-tuning stage that incorporates
toxicity-specific information. The model simultaneously optimizes over multiple
toxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to
generate environmentally friendly pesticide candidates. Notably, Pesti-Gen
achieves approximately 68\% structural validity in generating new molecular
structures, demonstrating the model's effectiveness in producing optimized and
feasible pesticide candidates, thereby providing a new way for safer and more
sustainable pest management solutions.

æè¦ï¼å¨çæ°£åè®é·éä½äºä½ç©çå¾©ååèæ®ºè²åçæåï¼
ä½¿å¾ä»°è³´åææ®ºè²åæçºç¡å¯é¿åçè¶¨å¢ï¼åç®¡å®åçå»£æ³ä½¿ç¨æå¸¶ä¾éå¤§çå¥åº·åç°å¢é¢¨éªãåç®¡éäºæ®ºè²åä»ç¶æ¯è²å®³ç®¡çä¸­çééµå·¥å·ï¼éå»å¨æ®ºè²ååè¾²æ¥­æ¹é¢çæ©å¨å­¸ç¿æç¨é½èéæ¼åé¡æè¿´æ­¸ï¼èæªè§£æ±ºç¢çæ°çåå­çµæ§æè¨­è¨æ°åé¸è¥åçåºæ¬ææ°ãå¨æ¬æä¸­ï¼æåæåº Pesti-Genï¼ä¸ç¨®åºæ¼è®ç°èªåç·¨ç¢¼å¨çåµæ°çææ¨¡åï¼æ¨å¨é¦æ¬¡å»ºç«å·ææä½³åç¹æ§çæ®ºè²ååé¸è¥åãå·é«ä¾èªªï¼Pesti-Gen æ¡ç¨å©éæ®µå­¸ç¿æµç¨ï¼ä¸åæ·åå»£ç¾©åå­¸çµæ§è¡¨ç¤ºçåå§é è¨ç·´éæ®µï¼æ¥èæ¯ä¸åç´å¥æ¯æ§ç¹å®è³è¨çå¾®èª¿éæ®µãæ­¤æ¨¡ååæéå°å¤ç¨®æ¯æ§ææ¨é²è¡æä½³åï¼ä¾å¦ (1) ç²çæ¯æ§å (2) æ°´çæ¯æ§ï¼ä»¥ç¢çå°ç°å¢ååçæ®ºè²ååé¸è¥åãå¼å¾æ³¨æçæ¯ï¼Pesti-Gen å¨ç¢çæ°çåå­çµæ§æ¹é¢éå°äºç´ 68% ççµæ§æåº¦ï¼è­æäºæ­¤æ¨¡åå¨ç¢çæä½³åä¸å¯è¡çæ®ºè²ååé¸è¥åæ¹é¢çæè½ï¼é²èçºæ´å®å¨ä¸æ´æ°¸çºçè²å®³ç®¡çè§£æ±ºæ¹æ¡æä¾äºä¸ç¨®æ°æ¹æ³ã

##### **ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**
2501.14379v1 by Yoni Schirris, Rosie Voorthuis, Mark Opdam, Marte Liefaard, Gabe S Sonke, Gwen Dackus, Vincent de Jong, Yuwei Wang, Annelot Van Rossum, Tessa G Steenbruggen, Lars C Steggink, Liesbeth G. E. de Vries, Marc van de Vijver, Roberto Salgado, Efstratios Gavves, Paul J van Diest, Sabine C Linn, Jonas Teuwen, Renee Menezes, Marleen Kok, Hugo Horlings

The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor
for patients with (triple-negative) breast cancer (BC). Computational TIL
assessment (CTA) has the potential to assist pathologists in this
labour-intensive task, but current CTA models rely heavily on many detailed
annotations. We propose and validate a fundamentally simpler deep learning
based CTA that can be trained in only ten minutes on hundredfold fewer
pathologist annotations. We collected whole slide images (WSIs) with TILs
scores and clinical data of 2,340 patients with BC from six cohorts including
three randomised clinical trials. Morphological features were extracted from
whole slide images (WSIs) using a pathology foundation model. Our
label-efficient Computational stromal TIL assessment model (ECTIL) directly
regresses the TILs score from these features. ECTIL trained on only a few
hundred samples (ECTIL-TCGA) showed concordance with the pathologist over five
heterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all
slides of five cohorts (ECTIL-combined) improved results on a held-out test set
(r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that
every 10% increase of ECTIL scores was associated with improved overall
survival independent of clinicopathological variables (HR 0.86, p<0.01),
similar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL
is highly concordant with an expert pathologist and obtains a similar hazard
ratio. ECTIL has a fundamentally simpler design than existing methods and can
be trained on orders of magnitude fewer annotations. Such a CTA may be used to
pre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a
tool to assist clinicians in the diagnostic work-up of patients with BC. Our
model is available under an open source licence
(https://github.com/nki-ai/ectil).

æè¦ï¼è¿ç¤æµ¸æ¶¦æ·å·´ç»è (TIL) çæ°´å¹³æ¯ (ä¸é´æ§) ä¹³èºç (BC) æ£èçé¢åå ç´ ãè®¡ç® TIL è¯ä¼° (CTA) æå¯è½åå©ççå­¦å®¶å®æè¿é¡¹å³å¨å¯éåä»»å¡ï¼ä½ç®åç CTA æ¨¡åä¸¥éä¾èµäºè®¸å¤è¯¦ç»çæ³¨éãæä»¬æåºå¹¶éªè¯äºä¸ä¸ªåºäºæ·±åº¦å­¦ä¹ ç CTAï¼å®å¯ä»¥å¨å ç¾åæ´å°çççå­¦å®¶æ³¨éä¸ä»å¨ååéåè¿è¡è®­ç»ãæä»¬ä»å­ä¸ªéåä¸­æ¶éäº 2,340 å BC æ£èç TILs è¯ååä¸´åºæ°æ®çå¨ç»çå¾å (WSI)ï¼å¶ä¸­åæ¬ä¸é¡¹éæºä¸´åºè¯éªãä½¿ç¨ççåºç¡æ¨¡åä»å¨ç»çå¾å (WSI) ä¸­æåå½¢æå­¦ç¹å¾ãæä»¬çæ ç­¾é«æè®¡ç®åºè´¨ TIL è¯ä¼°æ¨¡å (ECTIL) ç´æ¥ä»è¿äºç¹å¾ä¸­åå½ TILs è¯åãä»å¨å ç¾ä¸ªæ ·æ¬ä¸è¿è¡è®­ç»ç ECTILï¼ECTIL-TCGAï¼æ¾ç¤ºåºä¸ççå­¦å®¶å¨äºä¸ªå¼è´¨å¤é¨éåä¸­çä¸è´æ§ï¼r=0.54-0.74ï¼AUROC=0.80-0.94ï¼ãå¨äºä¸ªéåçææç»çä¸è¿è¡è®­ç»ï¼ECTIL-combinedï¼æ¹åäºä¿çæµè¯éä¸çç»æï¼r=0.69ï¼AUROC=0.85ï¼ãå¤åé Cox åå½åæè¡¨æï¼ECTIL è¯åæ¯å¢å  10%ï¼ä¸ä¸´åºççå­¦åéæ å³çæ»ä½çå­çå°±ä¼æé«ï¼HR 0.86ï¼p<0.01ï¼ï¼ç±»ä¼¼äºççå­¦å®¶è¯åï¼HR 0.87ï¼p<0.001ï¼ãæä»¬è¯æ ECTIL ä¸ä¸å®¶ççå­¦å®¶é«åº¦ä¸è´ï¼å¹¶è·å¾äºç±»ä¼¼çé£é©æ¯ãECTIL çè®¾è®¡æ¯ç°ææ¹æ³ä»æ ¹æ¬ä¸æ´ç®åï¼å¹¶ä¸å¯ä»¥å¨æ°éçº§æ´å°çæ³¨éä¸è¿è¡è®­ç»ãè¿ç§ CTA å¯ç¨äºå¯¹æ£èè¿è¡é¢ç­éï¼ä¾å¦åç«æ²»çä¸´åºè¯éªçº³å¥ï¼æä½ä¸ºä¸ç§å·¥å·æ¥å¸®å©ä¸´åºå»çå¯¹ BC æ£èè¿è¡è¯æ­æ£æ¥ãæä»¬çæ¨¡åå¯å¨å¼æ¾æºä»£ç è®¸å¯ä¸è·å¾ (https://github.com/nki-ai/ectil)ã

