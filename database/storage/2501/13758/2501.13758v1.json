{"2501.13758": {"publish_time": "2025-01-23", "title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings", "paper_summary": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task.", "paper_summary_zh": "\u6709\u6548\u7684\u53e5\u5b50\u5d4c\u5165\u53ef\u4ee5\u6355\u6349\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e2d\u5f88\u597d\u5730\u6982\u62ec\uff0c\u8fd9\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\n\u6211\u4eec\u901a\u8fc7\u5e94\u7528 SimCSE\uff08\u53e5\u5b50\u5d4c\u5165\u7684\u7b80\u5355\u5bf9\u6bd4\u5b66\u4e60\uff09\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9 minBERT \u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u884c\u60c5\u611f\u5206\u6790\u3001\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027 (STS) \u548c\u91ca\u4e49\u68c0\u6d4b\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\u5c1d\u8bd5\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684 dropout \u6280\u672f\uff0c\u5373\u6807\u51c6 dropout\u3001\u8bfe\u7a0b dropout \u548c\u81ea\u9002\u5e94 dropout\uff0c\u4ee5\u89e3\u51b3\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 2 \u5c42 SimCSE \u5fae\u8c03\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86 STS \u4efb\u52a1\u4e2d\u7684\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763 SimCSE\uff0c\u5e76\u63a2\u7d22\u4e86\u91ca\u4e49\u548c SST \u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u6f5c\u529b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86 SimCSE \u7684\u6709\u6548\u6027\uff0c2 \u5c42\u6a21\u578b\u5728 STS \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e73\u5747\u6d4b\u8bd5\u5f97\u5206\u4e3a 0.742\u3002\u9519\u8bef\u5206\u6790\u7684\u7ed3\u679c\u63ed\u793a\u4e86\u5728\u5904\u7406\u590d\u6742\u60c5\u611f\u548c\u4f9d\u8d56\u8bcd\u6cd5\u91cd\u53e0\u8fdb\u884c\u91ca\u4e49\u68c0\u6d4b\u65b9\u9762\u7684\u6311\u6218\uff0c\u7a81\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u9886\u57df\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5728\u5355\u4efb\u52a1\u65e0\u76d1\u7763 SimCSE \u6a21\u578b\u4e2d\u79fb\u9664\u81ea\u9002\u5e94 Dropout \u4f1a\u63d0\u9ad8 STS \u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u7531\u4e8e\u6dfb\u52a0\u4e86\u53c2\u6570\u800c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u3002\u4ece SimCSE \u6a21\u578b\u5230\u91ca\u4e49\u548c SST \u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u5e76\u6ca1\u6709\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u4ece STS \u4efb\u52a1\u4e2d\u83b7\u53d6\u77e5\u8bc6\u7684\u53ef\u8f6c\u79fb\u6027\u6709\u9650\u3002", "author": "Yumeng Wang et.al.", "authors": "Yumeng Wang, Ziran Zhou, Junjin Wang", "id": "2501.13758v1", "paper_url": "http://arxiv.org/abs/2501.13758v1", "repo": "null"}}