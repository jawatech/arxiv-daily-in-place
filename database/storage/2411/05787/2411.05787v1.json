{"2411.05787": {"publish_time": "2024-11-08", "title": "Recycled Attention: Efficient inference for long-context language models", "paper_summary": "Generating long sequences of tokens given a long-context input imposes a\nheavy computational burden for large language models (LLMs). One of the\ncomputational bottleneck comes from computing attention over a long sequence of\ninput at each generation step. In this paper, we propose Recycled Attention, an\ninference-time method which alternates between full context attention and\nattention over a subset of input tokens. When performing partial attention, we\nrecycle the attention pattern of a previous token that has performed full\nattention and attend only to the top K most attended tokens, reducing the cost\nof data movement and attention computation. Compared to previously proposed\ninference-time acceleration method which attends only to local context or\ntokens with high accumulative attention scores, our approach flexibly chooses\ntokens that are relevant to the current decoding step. We evaluate our methods\non RULER, a suite of tasks designed to comprehensively evaluate long-context\nabilities, and long-context language modeling tasks. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to baselines which only consider\nlocal context while improving the performance by 2x. We further explore two\nideas to improve performance-efficiency trade-offs: (1) dynamically decide when\nto perform recycled or full attention step based on the query similarities and\n(2) continued pre-training the model with Recycled Attention.", "paper_summary_zh": "\u5728\u63d0\u4f9b\u9577\u80cc\u666f\u8f38\u5165\u7684\u60c5\u6cc1\u4e0b\u7522\u751f\u9577\u5e8f\u5217\u7684\u7b26\u865f\u6703\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9020\u6210\u6c89\u91cd\u7684\u8a08\u7b97\u8ca0\u64d4\u3002\u5176\u4e2d\u4e00\u500b\u8a08\u7b97\u74f6\u9838\u4f86\u81ea\u65bc\u5728\u6bcf\u500b\u7522\u751f\u6b65\u9a5f\u4e2d\u8a08\u7b97\u9577\u8f38\u5165\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u5faa\u74b0\u6ce8\u610f\u529b\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u5b8c\u6574\u80cc\u666f\u6ce8\u610f\u529b\u548c\u8f38\u5165\u7b26\u865f\u5b50\u96c6\u7684\u6ce8\u610f\u529b\u4e4b\u9593\u4ea4\u66ff\u7684\u63a8\u8ad6\u6642\u9593\u65b9\u6cd5\u3002\u5728\u57f7\u884c\u90e8\u5206\u6ce8\u610f\u529b\u6642\uff0c\u6211\u5011\u5faa\u74b0\u5229\u7528\u5df2\u57f7\u884c\u5b8c\u6574\u6ce8\u610f\u529b\u7684\u524d\u4e00\u500b\u7b26\u865f\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4e26\u50c5\u6ce8\u610f\u6700\u53d7\u95dc\u6ce8\u7684 K \u500b\u7b26\u865f\uff0c\u5f9e\u800c\u964d\u4f4e\u4e86\u6578\u64da\u79fb\u52d5\u548c\u6ce8\u610f\u529b\u8a08\u7b97\u7684\u6210\u672c\u3002\u8207\u5148\u524d\u63d0\u51fa\u7684\u50c5\u95dc\u6ce8\u5c40\u90e8\u80cc\u666f\u6216\u7d2f\u7a4d\u6ce8\u610f\u529b\u5206\u6578\u9ad8\u7684\u7b26\u865f\u7684\u63a8\u8ad6\u6642\u9593\u52a0\u901f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u9748\u6d3b\u5730\u9078\u64c7\u8207\u7576\u524d\u89e3\u78bc\u6b65\u9a5f\u76f8\u95dc\u7684\u7b26\u865f\u3002\u6211\u5011\u5728 RULER \u4e0a\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0cRULER \u662f\u4e00\u7d44\u65e8\u5728\u5168\u9762\u8a55\u4f30\u9577\u80cc\u666f\u80fd\u529b\u7684\u4efb\u52d9\uff0c\u4ee5\u53ca\u9577\u80cc\u666f\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u3002\u5c07\u6211\u5011\u7684\u6a21\u578b\u61c9\u7528\u65bc\u73fe\u6210\u7684 LLM\uff0c\u53ef\u5be6\u73fe\u8207\u50c5\u8003\u616e\u5c40\u90e8\u80cc\u666f\u7684\u57fa\u7dda\u76f8\u7576\u7684\u52a0\u901f\uff0c\u540c\u6642\u5c07\u6548\u80fd\u63d0\u5347 2 \u500d\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u5169\u7a2e\u63d0\u5347\u6548\u80fd\u6548\u7387\u6298\u8877\u7684\u69cb\u60f3\uff1a(1) \u6839\u64da\u67e5\u8a62\u76f8\u4f3c\u6027\u52d5\u614b\u6c7a\u5b9a\u4f55\u6642\u57f7\u884c\u5faa\u74b0\u6216\u5b8c\u6574\u6ce8\u610f\u529b\u6b65\u9a5f\uff0c\u4ee5\u53ca (2) \u6301\u7e8c\u4f7f\u7528\u5faa\u74b0\u6ce8\u610f\u529b\u9810\u8a13\u7df4\u6a21\u578b\u3002", "author": "Fangyuan Xu et.al.", "authors": "Fangyuan Xu, Tanya Goyal, Eunsol Choi", "id": "2411.05787v1", "paper_url": "http://arxiv.org/abs/2411.05787v1", "repo": "null"}}