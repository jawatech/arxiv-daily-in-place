{"2502.14502": {"publish_time": "2025-02-20", "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?", "paper_summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u53d7\u5230\u9810\u8a13\u7df4\u671f\u9593\u5b78\u5230\u7684\u77e5\u8b58\u548c\u5132\u5b58\u5728\u6a21\u578b\u53c3\u6578\u4e2d\u7684\u77e5\u8b58\u7684\u6975\u5927\u9650\u5236\u3002\u4f4e\u968e\u9069\u61c9 (LoRA) \u662f\u4e00\u7a2e\u6d41\u884c\u4e14\u6709\u6548\u7684\u8a13\u7df4\u6280\u8853\uff0c\u7528\u65bc\u66f4\u65b0\u6216 LLM \u7684\u7279\u5b9a\u9818\u57df\u9069\u61c9\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u4f7f\u7528 LoRA \u5c07\u65b0\u4e8b\u5be6\u7d0d\u5165 LLM\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u5148\u524d\u5b78\u5230\u7684\u77e5\u8b58\u3002\u6211\u5011\u4f7f\u7528\u4e0d\u540c\u6578\u91cf\u7684\u77e5\u8b58\u5fae\u8abf Llama-3.1-8B-instruct\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u7576\u8a13\u7df4\u8cc7\u6599\u5305\u542b\u5df2\u77e5\u548c\u65b0\u4e8b\u5be6\u7684\u6df7\u5408\u6642\uff0c\u6703\u7372\u5f97\u6700\u4f73\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u4ecd\u7136\u5177\u6709\u6f5b\u5728\u7684\u5371\u5bb3\u6027\uff0c\u56e0\u70ba\u6a21\u578b\u5728\u5916\u90e8\u554f\u7b54\u57fa\u6e96\u4e0a\u7684\u8868\u73fe\u6703\u5728\u9019\u7a2e\u5fae\u8abf\u5f8c\u4e0b\u964d\u3002\u7576\u8a13\u7df4\u8cc7\u6599\u504f\u5411\u65bc\u67d0\u4e9b\u5be6\u9ad4\u6642\uff0c\u6a21\u578b\u50be\u5411\u65bc\u56de\u6b78\u5230\u5c11\u6578\u904e\u5ea6\u8868\u793a\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b\u8b8a\u5f97\u66f4\u6709\u4fe1\u5fc3\uff0c\u4e26\u4e14\u5728\u6975\u5c11\u6578\u60c5\u6cc1\u4e0b\u62d2\u7d55\u63d0\u4f9b\u7b54\u6848\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u57fa\u65bc LoRA \u7684 LLM \u66f4\u65b0\u7684\u6f5b\u5728\u7f3a\u9ede\uff0c\u4e26\u5f37\u8abf\u4e86\u8a13\u7df4\u8cc7\u6599\u7d44\u6210\u548c\u8abf\u6574\u53c3\u6578\u4ee5\u5e73\u8861\u65b0\u77e5\u8b58\u6574\u5408\u548c\u4e00\u822c\u6a21\u578b\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "author": "Sergey Pletenev et.al.", "authors": "Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, Mikhail Salnikov", "id": "2502.14502v1", "paper_url": "http://arxiv.org/abs/2502.14502v1", "repo": "null"}}