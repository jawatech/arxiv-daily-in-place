{"2502.06669": {"publish_time": "2025-02-10", "title": "Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations", "paper_summary": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u5df2\u7372\u5f97\u986f\u8457\u6539\u5584\u3002\u7531\u65bc\u5176\u5c0d\u8f38\u5165\u7684\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u7814\u7a76\u8d8a\u4f86\u8d8a\u5c08\u6ce8\u65bc\u900f\u904e\u76f4\u63a5\u4e14\u7c21\u55ae\u7684\u63d0\u793a\u5de5\u7a0b\u4f86\u589e\u5f37 LLM \u7684\u6548\u80fd\uff0c\u800c\u975e\u8907\u96dc\u7684\u9818\u57df\u9069\u61c9\u3002\u7814\u7a76\u8868\u660e\uff0cLLM \u8868\u73fe\u51fa\u60c5\u7dd2\u667a\u529b\uff0c\u6b63\u9762\u548c\u8ca0\u9762\u60c5\u7dd2\u90fd\u53ef\u80fd\u589e\u5f37\u4efb\u52d9\u8868\u73fe\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u4e92\u52d5\u63d0\u793a\u4e3b\u8981\u96c6\u4e2d\u65bc\u55ae\u4e00\u523a\u6fc0\u985e\u578b\uff0c\u5ffd\u7565\u4e86\u6bd4\u8f03\u4e0d\u540c\u7684\u523a\u6fc0\u6548\u679c\u3001\u6aa2\u67e5\u4e0d\u540c\u4efb\u52d9\u96e3\u5ea6\u7684\u5f71\u97ff\uff0c\u6216\u63a2\u7d22\u57fa\u790e\u6a5f\u5236\u3002\u672c\u6587\u53d7\u5230\u793e\u6703\u8a8d\u77e5\u7406\u8ad6\u4e2d\u81ea\u6211\u6548\u80fd\u611f\u548c\u4efb\u52d9\u8868\u73fe\u4e4b\u9593\u7684\u6b63\u76f8\u95dc\u555f\u767c\uff0c\u5f15\u5165\u4e86\u8a00\u8a9e\u6548\u80fd\u523a\u6fc0 (VES)\u3002\u6211\u5011\u7684 VES \u5305\u542b\u4e09\u7a2e\u985e\u578b\u7684\u8a00\u8a9e\u63d0\u793a\uff1a\u9f13\u52f5\u3001\u6311\u91c1\u548c\u6279\u8a55\uff0c\u6d89\u53ca\u516d\u500b\u65b9\u9762\uff0c\u4f8b\u5982\u6a02\u65bc\u52a9\u4eba\u548c\u80fd\u529b\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c0d\u4efb\u52d9\u96e3\u5ea6\u9032\u884c\u5206\u985e\uff0c\u65e8\u5728\u5ee3\u6cdb\u7814\u7a76\u4e0d\u540c\u7684 VES \u5982\u4f55\u5f71\u97ff\u4e0d\u540c\u96e3\u5ea6\u7d1a\u5225\u7684\u8a9e\u8a00\u6a21\u578b\u7684\u81ea\u6211\u6548\u80fd\u611f\u548c\u4efb\u52d9\u6210\u5c31\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u4e09\u7a2e\u985e\u578b\u7684 VES \u63d0\u9ad8\u4e86 LLM \u5728\u5927\u591a\u6578\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u6700\u6709\u6548\u7684 VES \u56e0\u4e0d\u540c\u7684\u6a21\u578b\u800c\u7570\u3002\u5728\u5ee3\u6cdb\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u7372\u5f97\u4e86\u4e00\u4e9b\u8207\u5fc3\u7406\u5b78\u7406\u8ad6\u4e00\u81f4\u7684\u767c\u73fe\uff0c\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002", "author": "Rui Chen et.al.", "authors": "Rui Chen, Tailai Peng, Xinran Xie, Dekun Lin, Zhe Cui, Zheng Chen", "id": "2502.06669v1", "paper_url": "http://arxiv.org/abs/2502.06669v1", "repo": "null"}}