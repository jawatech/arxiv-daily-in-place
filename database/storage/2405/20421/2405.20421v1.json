{"2405.20421": {"publish_time": "2024-05-30", "title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA", "paper_summary": "Large Multimodal Models (LMMs) have shown remarkable progress in the field of\nmedical Visual Question Answering (Med-VQA), achieving high accuracy on\nexisting benchmarks. However, their reliability under robust evaluation is\nquestionable. This study reveals that state-of-the-art models, when subjected\nto simple probing evaluation, perform worse than random guessing on medical\ndiagnosis questions. To address this critical evaluation problem, we introduce\nthe Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously\nassess LMM performance in medical imaging through probing evaluation and\nprocedural diagnosis. Particularly, probing evaluation features pairing\noriginal questions with negation questions with hallucinated attributes, while\nprocedural diagnosis requires reasoning across various diagnostic dimensions\nfor each image, including modality recognition, organ identification, clinical\nfindings, abnormalities, and positional grounding. Our evaluation reveals that\ntop-performing models like GPT-4V and Gemini Pro perform worse than random\nguessing on specialized diagnostic questions, indicating significant\nlimitations in handling fine-grained medical inquiries. Besides, models like\nLLaVA-Med struggle even with more general questions, and results from CheXagent\ndemonstrate the transferability of expertise across different modalities of the\nsame organ, showing that specialized domain knowledge is still crucial for\nimproving performance. This study underscores the urgent need for more robust\nevaluation to ensure the reliability of LMMs in critical fields like medical\ndiagnosis, and current LMMs are still far from applicable to those fields.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54 (Med-VQA) \u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u5ea6\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u7a33\u5065\u8bc4\u4f30\u4e0b\u7684\u53ef\u9760\u6027\u503c\u5f97\u6000\u7591\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5f53\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7ecf\u8fc7\u7b80\u5355\u7684\u63a2\u6d4b\u8bc4\u4f30\u65f6\uff0c\u5728\u533b\u5b66\u8bca\u65ad\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u6bd4\u968f\u673a\u731c\u6d4b\u8fd8\u8981\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5173\u952e\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u533b\u5b66\u8bca\u65ad\u63a2\u6d4b\u8bc4\u4f30 (ProbMed) \u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63a2\u6d4b\u8bc4\u4f30\u548c\u7a0b\u5e8f\u8bca\u65ad\u4e25\u683c\u8bc4\u4f30 LMM \u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\uff0c\u63a2\u6d4b\u8bc4\u4f30\u7684\u7279\u70b9\u662f\u5c06\u539f\u59cb\u95ee\u9898\u4e0e\u5e26\u6709\u5e7b\u89c9\u5c5e\u6027\u7684\u5426\u5b9a\u95ee\u9898\u914d\u5bf9\uff0c\u800c\u7a0b\u5e8f\u8bca\u65ad\u5219\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u56fe\u50cf\u63a8\u7406\u5404\u79cd\u8bca\u65ad\u7ef4\u5ea6\uff0c\u5305\u62ec\u6a21\u6001\u8bc6\u522b\u3001\u5668\u5b98\u8bc6\u522b\u3001\u4e34\u5e8a\u53d1\u73b0\u3001\u5f02\u5e38\u548c\u4f4d\u7f6e\u57fa\u7840\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u50cf GPT-4V \u548c Gemini Pro \u8fd9\u6837\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u4e13\u95e8\u7684\u8bca\u65ad\u95ee\u9898\u4e0a\u8868\u73b0\u6bd4\u968f\u673a\u731c\u6d4b\u8fd8\u8981\u5dee\uff0c\u8868\u660e\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u7684\u533b\u5b66\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u50cf LLaVA-Med \u8fd9\u6837\u7684\u6a21\u578b\u751a\u81f3\u5728\u66f4\u4e00\u822c\u7684\u95ee\u9898\u4e0a\u4e5f\u96be\u4ee5\u5e94\u4ed8\uff0c\u800c CheXagent \u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u4e13\u4e1a\u77e5\u8bc6\u5728\u540c\u4e00\u5668\u5b98\u7684\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u5177\u6709\u53ef\u8f6c\u79fb\u6027\uff0c\u8868\u660e\u4e13\u95e8\u7684\u9886\u57df\u77e5\u8bc6\u5bf9\u4e8e\u63d0\u9ad8\u6027\u80fd\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u786e\u4fdd LMM \u5728\u533b\u5b66\u8bca\u65ad\u7b49\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\uff0c\u800c\u5f53\u524d\u7684 LMM \u4ecd\u7136\u8fdc\u4e0d\u80fd\u5e94\u7528\u4e8e\u8fd9\u4e9b\u9886\u57df\u3002", "author": "Qianqi Yan et.al.", "authors": "Qianqi Yan, Xuehai He, Xiang Yue, Xin Eric Wang", "id": "2405.20421v1", "paper_url": "http://arxiv.org/abs/2405.20421v1", "repo": "https://github.com/eric-ai-lab/probmed"}}