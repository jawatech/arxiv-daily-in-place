{"2502.10202": {"publish_time": "2025-02-14", "title": "Can Post-Training Quantization Benefit from an Additional QLoRA Integration?", "paper_summary": "Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u7d93\u8f49\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u4f46\u5c0d\u771f\u5be6\u4e16\u754c\u7684\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u7684\u6311\u6230\u3002\u9019\u4e9b\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u9019\u53ef\u80fd\u6703\u5f88\u6602\u8cb4\u4e14\u7d93\u5e38\u4e0d\u53ef\u7528\u3002\u6a21\u578b\u58d3\u7e2e\u6280\u8853\uff08\u4f8b\u5982\u91cf\u5316\uff09\u901a\u5e38\u7528\u65bc\u6e1b\u8f15\u8cc7\u6e90\u9700\u6c42\uff0c\u4f46\u5b83\u5011\u53ef\u80fd\u6703\u5c0d\u751f\u6210\u54c1\u8cea\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5c07 4 \u4f4d\u5143\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u8207 QLoRA \u6574\u5408\u4ee5\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u6211\u5011\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u9019\u7a2e\u6574\u5408\u512a\u65bc\u6a19\u6e96 PTQ\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\u751a\u81f3\u512a\u65bc LLM \u4e0a\u7684 16 \u4f4d\u5143\u5168\u53c3\u6578\u5fae\u8abf\uff0c\u4e26\u5728\u4f7f\u7528\u4e0d\u540c\u91cf\u5316\u6f14\u7b97\u6cd5\u7684\u5c08\u6709\u548c\u516c\u958b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u9a57\u8b49\u3002\u7d50\u679c\u8b49\u660e\u4e86 PTQ-QLoRA \u6574\u5408\u7684\u6548\u80fd\uff0c\u70ba\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u90e8\u7f72\u5f37\u5927\u7684 LLM \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u540c\u6642\u4e0d\u5f71\u97ff\u6548\u80fd\u3002", "author": "Xiliang Zhu et.al.", "authors": "Xiliang Zhu, Elena Khasanova, Cheng Chen", "id": "2502.10202v1", "paper_url": "http://arxiv.org/abs/2502.10202v1", "repo": "null"}}